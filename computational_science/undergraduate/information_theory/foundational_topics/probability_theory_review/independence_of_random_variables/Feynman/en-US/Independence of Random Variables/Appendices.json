{
    "hands_on_practices": [
        {
            "introduction": "Understanding the independence of random variables begins with a fundamental test. For discrete variables, independence holds if and only if their joint probability, $P(X=x, Y=y)$, is the product of their individual (marginal) probabilities, $P(X=x)P(Y=y)$. This exercise  provides a direct, hands-on opportunity to apply this core principle by working with a joint probability table to find the specific condition that ensures independence.",
            "id": "9067",
            "problem": "Consider two discrete random variables, $X$ and $Y$. The variable $X$ can take values from the set $\\{0, 1\\}$, and the variable $Y$ can take values from the set $\\{0, 1\\}$. Their joint probability mass function (PMF), denoted by $p(x, y) = P(X=x, Y=y)$, is given by the following table, where $c$ is a real-valued parameter.\n\n|       | Y=0   | Y=1   |\n| :---- | :---- | :---- |\n| **X=0** | $1/8$ | $c$   |\n| **X=1** | $1/4$ | $5/8 - c$ |\n\nRecall that for two discrete random variables $X$ and $Y$ to be statistically independent, their joint PMF must be equal to the product of their marginal PMFs for all possible values of $x$ and $y$. That is, $P(X=x, Y=y) = P(X=x) P(Y=y)$.\n\nDetermine the value of the parameter $c$ that makes the random variables $X$ and $Y$ statistically independent.",
            "solution": "Compute the marginal PMFs. First, sum over $y$ to get $P(X=x)$:\n$$P(X=0)=p(0,0)+p(0,1)=\\tfrac18+c,\\qquad\nP(X=1)=p(1,0)+p(1,1)=\\tfrac14+\\Bigl(\\tfrac58-c\\Bigr)=\\tfrac78-c.$$\nNext, sum over $x$ to get $P(Y=y)$:\n$$P(Y=0)=p(0,0)+p(1,0)=\\tfrac18+\\tfrac14=\\tfrac38,\\qquad\nP(Y=1)=p(0,1)+p(1,1)=c+\\Bigl(\\tfrac58-c\\Bigr)=\\tfrac58.$$\nIndependence requires \n$$p(0,1)=P(X=0)\\,P(Y=1)\n\\quad\\Longrightarrow\\quad\nc=\\Bigl(\\tfrac18+c\\Bigr)\\tfrac58.$$\nSolve for $c$:\n$$c=\\tfrac58\\cdot\\tfrac18+\\tfrac58\\,c\n\\quad\\Longrightarrow\\quad\nc-\\tfrac58\\,c=\\tfrac5{64}\n\\quad\\Longrightarrow\\quad\n\\tfrac38\\,c=\\tfrac5{64}\n\\quad\\Longrightarrow\\quad\nc=\\tfrac5{64}\\cdot\\tfrac83=\\tfrac5{24}.$$\nOne may check $p(1,0)=P(X=1)P(Y=0)$ yields the same value, confirming independence.",
            "answer": "$$\\boxed{\\frac{5}{24}}$$"
        },
        {
            "introduction": "When we move from discrete to continuous random variables, summations are replaced by integrals, but the core ideas of independence remain. This practice  challenges you to work with a continuous joint probability density function, or PDF. You will not only check for independence but also calculate the covariance, $\\text{Cov}(X,Y)$, a key statistical measure that helps quantify how two variables vary together.",
            "id": "9073",
            "problem": "Two continuous random variables, $X$ and $Y$, are described by a joint probability density function (PDF) given by\n$$\nf_{X,Y}(x,y) = \n\\begin{cases} \nk(x+y) & \\text{for } 0 \\le x \\le 1 \\text{ and } 0 \\le y \\le 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nwhere $k$ is a normalization constant.\n\nFor a joint PDF to be valid, it must satisfy the normalization condition:\n$$\n\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\,dx\\,dy = 1\n$$\nThe marginal PDF for a single variable, say $X$, is found by integrating the joint PDF over the other variable:\n$$\nf_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\,dy\n$$\nThe expectation of a function $g(X,Y)$ is given by:\n$$\nE[g(X,Y)] = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} g(x,y)f_{X,Y}(x,y) \\,dx\\,dy\n$$\nThe covariance between $X$ and $Y$, which measures their joint variability, is defined as:\n$$\n\\text{Cov}(X,Y) = E[XY] - E[X]E[Y]\n$$\nTwo random variables are independent if and only if their joint PDF is the product of their marginal PDFs, i.e., $f_{X,Y}(x,y) = f_X(x)f_Y(y)$. A consequence of independence is that $\\text{Cov}(X,Y) = 0$.\n\nYour task is to derive the value of the covariance, $\\text{Cov}(X,Y)$, for the given random variables $X$ and $Y$.",
            "solution": "The derivation proceeds in several steps: First, we find the normalization constant $k$. Second, we compute the marginal PDFs $f_X(x)$ and $f_Y(y)$. Third, we calculate the expectations $E[X]$ and $E[Y]$. Fourth, we calculate the expectation of the product, $E[XY]$. Finally, we combine these results to find the covariance $\\text{Cov}(X,Y)$.\n\n**Step 1: Determine the normalization constant $k$.**\nWe use the normalization condition for the PDF. Since the function is non-zero only on the unit square, the integral simplifies:\n$$\n\\int_0^1 \\int_0^1 k(x+y) \\,dx\\,dy = 1\n$$\nWe perform the inner integral with respect to $x$:\n$$\nk \\int_0^1 \\left[ \\frac{x^2}{2} + yx \\right]_{x=0}^{x=1} \\,dy = k \\int_0^1 \\left( \\frac{1^2}{2} + y(1) - 0 \\right) \\,dy = k \\int_0^1 \\left(\\frac{1}{2} + y\\right) \\,dy\n$$\nNow, we perform the outer integral with respect to $y$:\n$$\nk \\left[ \\frac{y}{2} + \\frac{y^2}{2} \\right]_{y=0}^{y=1} = k \\left( \\left(\\frac{1}{2} + \\frac{1^2}{2}\\right) - 0 \\right) = k \\left(\\frac{1}{2} + \\frac{1}{2}\\right) = k\n$$\nSetting this result equal to 1 gives $k=1$. Thus, the joint PDF is $f_{X,Y}(x,y) = x+y$ for $x, y \\in [0, 1]$.\n\n**Step 2: Calculate the marginal PDFs.**\nThe marginal PDF for $X$, $f_X(x)$, is found by integrating $f_{X,Y}(x,y)$ over all possible values of $y$:\n$$\nf_X(x) = \\int_0^1 (x+y) \\,dy = \\left[ xy + \\frac{y^2}{2} \\right]_{y=0}^{y=1} = x(1) + \\frac{1^2}{2} - 0 = x + \\frac{1}{2}\n$$\nThis is valid for $x \\in [0, 1]$. By symmetry, the marginal PDF for $Y$ is:\n$$\nf_Y(y) = \\int_0^1 (x+y) \\,dx = \\left[ \\frac{x^2}{2} + yx \\right]_{x=0}^{x=1} = \\frac{1^2}{2} + y(1) - 0 = y + \\frac{1}{2}\n$$\nThis is valid for $y \\in [0, 1]$.\n\n**Step 3: Calculate the individual expectations $E[X]$ and $E[Y]$.**\nUsing the marginal PDF $f_X(x)$, we find $E[X]$:\n$$\nE[X] = \\int_0^1 x f_X(x) \\,dx = \\int_0^1 x \\left(x + \\frac{1}{2}\\right) \\,dx = \\int_0^1 \\left(x^2 + \\frac{x}{2}\\right) \\,dx\n$$\n$$\nE[X] = \\left[ \\frac{x^3}{3} + \\frac{x^2}{4} \\right]_0^1 = \\left( \\frac{1^3}{3} + \\frac{1^2}{4} \\right) - 0 = \\frac{1}{3} + \\frac{1}{4} = \\frac{4+3}{12} = \\frac{7}{12}\n$$\nDue to the symmetry of the problem, $E[Y] = E[X]$, so $E[Y] = \\frac{7}{12}$.\n\n**Step 4: Calculate the expectation of the product, $E[XY]$.**\nWe compute this using the joint PDF:\n$$\nE[XY] = \\int_0^1 \\int_0^1 (xy) f_{X,Y}(x,y) \\,dx\\,dy = \\int_0^1 \\int_0^1 xy(x+y) \\,dx\\,dy\n$$\n$$\nE[XY] = \\int_0^1 \\int_0^1 (x^2y + xy^2) \\,dx\\,dy\n$$\nIntegrating with respect to $x$ first:\n$$\nE[XY] = \\int_0^1 \\left[ \\frac{x^3y}{3} + \\frac{x^2y^2}{2} \\right]_{x=0}^{x=1} \\,dy = \\int_0^1 \\left( \\frac{y}{3} + \\frac{y^2}{2} \\right) \\,dy\n$$\nNow integrating with respect to $y$:\n$$\nE[XY] = \\left[ \\frac{y^2}{6} + \\frac{y^3}{6} \\right]_0^1 = \\left( \\frac{1^2}{6} + \\frac{1^3}{6} \\right) - 0 = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6} = \\frac{1}{3}\n$$\n\n**Step 5: Calculate the covariance.**\nFinally, we use the definition of covariance:\n$$\n\\text{Cov}(X,Y) = E[XY] - E[X]E[Y]\n$$\nSubstituting the calculated values:\n$$\n\\text{Cov}(X,Y) = \\frac{1}{3} - \\left(\\frac{7}{12}\\right)\\left(\\frac{7}{12}\\right) = \\frac{1}{3} - \\frac{49}{144}\n$$\nTo subtract, we find a common denominator, which is 144:\n$$\n\\text{Cov}(X,Y) = \\frac{1 \\times 48}{3 \\times 48} - \\frac{49}{144} = \\frac{48}{144} - \\frac{49}{144} = -\\frac{1}{144}\n$$",
            "answer": "$$\n\\boxed{-\\frac{1}{144}}\n$$"
        },
        {
            "introduction": "Beyond algebraic formulas, a powerful intuition for independence can be developed through geometry. A crucial, and often visual, condition for the independence of continuous variables is that the region of possible outcomes—the support of the joint distribution—must be rectangular. This thought experiment  explores this concept, prompting you to determine if the Cartesian coordinates of a point chosen randomly from a circular disk are independent, thereby shifting the focus from pure calculation to conceptual reasoning.",
            "id": "1308140",
            "problem": "A point is selected uniformly at random from the interior of a flat, circular disk of radius $R$ centered at the origin. Let the random variables $X$ and $Y$ represent the Cartesian coordinates of the selected point. Which of the following statements correctly describes the relationship between the random variables $X$ and $Y$?\n\nA. $X$ and $Y$ are dependent because the support of their joint probability distribution is not a rectangular region.\n\nB. $X$ and $Y$ are independent because the point is chosen with a uniform probability distribution over the disk.\n\nC. $X$ and $Y$ are dependent because their marginal probability distributions are not uniform.\n\nD. $X$ and $Y$ are independent because the geometric shape of the disk is symmetric.\n\nE. The relationship cannot be determined; we would need to calculate the correlation coefficient, and if it is zero, then they are independent.",
            "solution": "To determine if the random variables $X$ and $Y$ are independent, we must check if their joint probability density function (PDF), $f_{X,Y}(x,y)$, can be factored into the product of their individual marginal PDFs, $f_X(x)$ and $f_Y(y)$. That is, we must check if $f_{X,Y}(x,y) = f_X(x)f_Y(y)$ for all $x$ and $y$.\n\nFirst, let's define the joint PDF, $f_{X,Y}(x,y)$. The point $(X,Y)$ is chosen uniformly from a circular disk of radius $R$. The area of this disk is $A = \\pi R^2$. Since the distribution is uniform, the PDF must be a constant, $c$, over the disk and zero elsewhere. The value of this constant is determined by the normalization condition that the total probability must be 1.\n$$ \\iint_{x^2+y^2 \\le R^2} f_{X,Y}(x,y) \\,dx\\,dy = 1 $$\n$$ \\iint_{x^2+y^2 \\le R^2} c \\,dx\\,dy = 1 $$\n$$ c \\cdot (\\text{Area of disk}) = 1 $$\n$$ c \\cdot (\\pi R^2) = 1 \\implies c = \\frac{1}{\\pi R^2} $$\nSo, the joint PDF is:\n$$ f_{X,Y}(x,y) = \\begin{cases} \\frac{1}{\\pi R^2} & \\text{if } x^2+y^2 \\le R^2 \\\\ 0 & \\text{otherwise} \\end{cases} $$\nThe set of points where the PDF is non-zero, $\\{(x,y) | x^2+y^2 \\le R^2\\}$, is called the support of the distribution.\n\nA necessary condition for two continuous random variables to be independent is that the support of their joint distribution must be a Cartesian product of their individual supports. In other words, the support region must be a rectangle (possibly infinite) with sides parallel to the coordinate axes.\n\nLet's find the support for $X$ and $Y$ individually.\nThe possible values for $X$ range from $-R$ to $R$. So, the support of $X$ is the interval $[-R, R]$.\nThe possible values for $Y$ also range from $-R$ to $R$. So, the support of $Y$ is the interval $[-R, R]$.\n\nThe Cartesian product of these individual supports is the set of all points $(x,y)$ such that $x \\in [-R, R]$ and $y \\in [-R, R]$. This region is a square with vertices at $(R,R)$, $(R,-R)$, $(-R,-R)$, and $(-R,R)$.\n\nThe support of the joint distribution $f_{X,Y}(x,y)$ is the circular disk defined by $x^2+y^2 \\le R^2$.\n\nSince the support of the joint distribution (a disk) is not the same as the Cartesian product of the marginal supports (a square), the random variables $X$ and $Y$ cannot be independent.\n\nTo illustrate this dependence, consider a specific value for $X$. If we know that $X = 0.9R$, the possible values of $Y$ are constrained by the inequality $(0.9R)^2 + y^2 \\le R^2$, which means $y^2 \\le R^2 - 0.81R^2 = 0.19R^2$. So, $Y$ must be in the interval $[-\\sqrt{0.19}R, \\sqrt{0.19}R]$. If we know $X=R$, then $Y$ must be $0$. Since knowledge of the value of $X$ changes the possible range of values for $Y$, the variables are dependent.\n\nLet's analyze the given options:\nA. $X$ and $Y$ are dependent because the support of their joint probability distribution is not a rectangular region. This is the correct reason for their dependence, as explained above.\nB. Independence is not guaranteed by a uniform distribution. The geometry of the support region is critical. This statement is false.\nC. While it is true that the marginal distributions are not uniform (for instance, $f_X(x) = \\frac{2\\sqrt{R^2-x^2}}{\\pi R^2}$ for $x \\in [-R,R]$), this is a consequence of the geometry, not the fundamental reason for dependence. The core reason is the non-rectangular support.\nD. Symmetry of the support region does not imply independence. This statement is false.\nE. This statement confuses independence with correlation. Two variables can be uncorrelated (correlation coefficient is zero) but still be dependent. For $X$ and $Y$ in this problem, their covariance and thus correlation coefficient is indeed zero due to symmetry, but as we've shown, they are dependent. Therefore, a zero correlation coefficient does not imply independence. This statement is false.\n\nThe correct conclusion is that $X$ and $Y$ are dependent, and the fundamental reason is that the domain of their joint distribution is a disk, which is not a rectangular region.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}