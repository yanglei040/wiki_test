{
    "hands_on_practices": [
        {
            "introduction": "Let's begin by exploring the expectation of a discrete random variable, a fundamental building block in probability. This first exercise  requires us to determine the probability distribution of a new variable—the maximum of two sensor readings—before we can apply the definition of expected value. This practice sharpens our skills in working with probability mass functions and demonstrates a common technique for analyzing the combined output of multiple independent sources.",
            "id": "1622946",
            "problem": "In a simplified model of a fault detection system, two independent sensors, Sensor 1 and Sensor 2, monitor a machine. Each sensor can report an integer \"severity level\" from the set $\\{1, 2, \\dots, N\\}$, where $N$ is a positive integer representing the maximum possible severity level. For each sensor, any given severity level is equally likely to be reported. The system's \"overall severity level\" is defined as the maximum of the two levels reported by the sensors. Let $X_1$ be the severity level reported by Sensor 1 and $X_2$ be the severity level reported by Sensor 2. The overall severity level is thus $Y = \\max(X_1, X_2)$.\n\nDetermine the expected value of the overall severity level, $E[Y]$, as a function of $N$.",
            "solution": "Let $X_1$ and $X_2$ be the random variables representing the severity levels reported by Sensor 1 and Sensor 2, respectively. The problem states that $X_1$ and $X_2$ are independent and uniformly distributed on the set of integers $\\{1, 2, \\dots, N\\}$. The probability mass function (PMF) for each sensor is therefore:\n$$P(X_i = k) = \\frac{1}{N} \\quad \\text{for } k \\in \\{1, 2, \\dots, N\\} \\text{ and } i \\in \\{1, 2\\}$$\nWe are asked to find the expected value of the random variable $Y = \\max(X_1, X_2)$. The expectation of a discrete random variable is given by $E[Y] = \\sum_{k} k \\cdot P(Y=k)$. The possible values for $Y$ are also in the set $\\{1, 2, \\dots, N\\}$.\n\nTo find the PMF of $Y$, $P(Y=k)$, it is often easier to first find its cumulative distribution function (CDF), $F_Y(k) = P(Y \\le k)$.\nThe event $Y \\le k$ is equivalent to the event $\\max(X_1, X_2) \\le k$. The maximum of two values is less than or equal to $k$ if and only if both values are less than or equal to $k$. Therefore:\n$$F_Y(k) = P(Y \\le k) = P(\\max(X_1, X_2) \\le k) = P(X_1 \\le k \\text{ and } X_2 \\le k)$$\nSince $X_1$ and $X_2$ are independent, we can write:\n$$F_Y(k) = P(X_1 \\le k) \\cdot P(X_2 \\le k)$$\nFor a discrete uniform distribution on $\\{1, 2, \\dots, N\\}$, the probability $P(X_i \\le k)$ is the sum of the probabilities of outcomes from 1 to $k$. Since there are $k$ such outcomes, each with probability $1/N$, we have:\n$$P(X_i \\le k) = \\sum_{j=1}^{k} P(X_i = j) = \\sum_{j=1}^{k} \\frac{1}{N} = \\frac{k}{N}$$\nSubstituting this back into the expression for the CDF of $Y$:\n$$F_Y(k) = \\left(\\frac{k}{N}\\right) \\cdot \\left(\\frac{k}{N}\\right) = \\frac{k^2}{N^2}$$\nThis CDF is valid for $k \\in \\{1, 2, \\dots, N\\}$.\n\nNow we can find the PMF, $P(Y=k)$, using the relation $P(Y=k) = F_Y(k) - F_Y(k-1)$ for $k \\ge 1$. For $k=1$, $P(Y=1) = F_Y(1) = 1/N^2$. For $k > 1$:\n$$P(Y=k) = \\frac{k^2}{N^2} - \\frac{(k-1)^2}{N^2} = \\frac{k^2 - (k^2 - 2k + 1)}{N^2} = \\frac{2k - 1}{N^2}$$\nNote that this formula also works for $k=1$, since $(2(1)-1)/N^2 = 1/N^2$. Thus, the PMF is $P(Y=k) = \\frac{2k-1}{N^2}$ for $k \\in \\{1, 2, \\dots, N\\}$.\n\nFinally, we compute the expected value of $Y$ using its definition:\n$$E[Y] = \\sum_{k=1}^{N} k \\cdot P(Y=k) = \\sum_{k=1}^{N} k \\cdot \\frac{2k-1}{N^2}$$\n$$E[Y] = \\frac{1}{N^2} \\sum_{k=1}^{N} (2k^2 - k) = \\frac{1}{N^2} \\left( 2\\sum_{k=1}^{N} k^2 - \\sum_{k=1}^{N} k \\right)$$\nWe use the standard formulas for the sum of the first $N$ integers and the sum of the first $N$ squares:\n$$\\sum_{k=1}^{N} k = \\frac{N(N+1)}{2}$$\n$$\\sum_{k=1}^{N} k^2 = \\frac{N(N+1)(2N+1)}{6}$$\nSubstituting these into the expression for $E[Y]$:\n$$E[Y] = \\frac{1}{N^2} \\left( 2 \\cdot \\frac{N(N+1)(2N+1)}{6} - \\frac{N(N+1)}{2} \\right)$$\n$$E[Y] = \\frac{1}{N^2} \\left( \\frac{N(N+1)(2N+1)}{3} - \\frac{N(N+1)}{2} \\right)$$\nFactor out the common term $\\frac{N(N+1)}{N^2} = \\frac{N+1}{N}$:\n$$E[Y] = \\frac{N+1}{N} \\left( \\frac{2N+1}{3} - \\frac{1}{2} \\right)$$\nFind a common denominator for the terms in the parenthesis:\n$$E[Y] = \\frac{N+1}{N} \\left( \\frac{2(2N+1) - 3(1)}{6} \\right) = \\frac{N+1}{N} \\left( \\frac{4N+2 - 3}{6} \\right)$$\n$$E[Y] = \\frac{N+1}{N} \\left( \\frac{4N-1}{6} \\right)$$\nExpanding the numerator gives the final expression:\n$$E[Y] = \\frac{(N+1)(4N-1)}{6N} = \\frac{4N^2 - N + 4N - 1}{6N} = \\frac{4N^2 + 3N - 1}{6N}$$",
            "answer": "$$\\boxed{\\frac{4N^{2} + 3N - 1}{6N}}$$"
        },
        {
            "introduction": "Having tackled a discrete scenario, we now turn our attention to the continuous domain. This problem  challenges us to find the expected distance of a randomly placed particle from the center of a circular area. The key step here is to translate a uniform distribution over an area into a probability density function for the radial distance, providing excellent practice with the integral-based definition of expectation for continuous variables.",
            "id": "1301055",
            "problem": "A particle is deposited onto a circular silicon wafer of radius $R$. The position of the particle on the wafer is random, with its location following a uniform probability distribution over the entire area of the circular wafer. Let the random variable $D$ represent the distance of the particle from the center of the wafer. Determine the expected value of $D$. Express your answer as an analytic expression in terms of $R$.",
            "solution": "Let the wafer be a disk of radius $R$ in the plane, and let the particle’s position be uniformly distributed over its area. The uniformity over area implies the joint probability density over the disk is constant and equal to $1/(\\pi R^{2})$ on $\\{(x,y): x^{2}+y^{2} \\leq R^{2}\\}$ and zero otherwise.\n\nDefine $D$ as the distance from the center, so $D = \\sqrt{X^{2}+Y^{2}}$. The cumulative distribution function of $D$ for $0 \\leq d \\leq R$ is the probability that the point lies within a concentric disk of radius $d$, which equals the ratio of areas:\n$$\nF_{D}(d) = \\mathbb{P}(D \\leq d) = \\frac{\\pi d^{2}}{\\pi R^{2}} = \\frac{d^{2}}{R^{2}}, \\quad 0 \\leq d \\leq R.\n$$\nDifferentiating gives the probability density function of $D$:\n$$\nf_{D}(d) = \\frac{\\mathrm{d}}{\\mathrm{d}d}F_{D}(d) = \\frac{2d}{R^{2}}, \\quad 0 \\leq d \\leq R.\n$$\nThe expected value is computed by the definition of expectation for a continuous random variable:\n$$\n\\mathbb{E}[D] = \\int_{0}^{R} d \\, f_{D}(d) \\, \\mathrm{d}d = \\int_{0}^{R} d \\cdot \\frac{2d}{R^{2}} \\, \\mathrm{d}d = \\frac{2}{R^{2}} \\int_{0}^{R} d^{2} \\, \\mathrm{d}d.\n$$\nEvaluating the integral,\n$$\n\\int_{0}^{R} d^{2} \\, \\mathrm{d}d = \\left. \\frac{d^{3}}{3} \\right|_{0}^{R} = \\frac{R^{3}}{3},\n$$\nso\n$$\n\\mathbb{E}[D] = \\frac{2}{R^{2}} \\cdot \\frac{R^{3}}{3} = \\frac{2R}{3}.\n$$\nTherefore, the expected distance from the center is $\\frac{2R}{3}$.",
            "answer": "$$\\boxed{\\frac{2R}{3}}$$"
        },
        {
            "introduction": "Beyond applying its basic definition, the real power of expectation is revealed when we utilize its elegant mathematical properties. This final practice  introduces a problem from signal processing, where we analyze the \"roughness\" of a digital signal. Instead of deriving a complex probability distribution, we will see how leveraging the linearity of expectation and the relationship between variance and mean provides a direct and powerful path to the solution.",
            "id": "1622947",
            "problem": "In the design of a digital communication system, a signal is modeled as a sequence of measurements, $\\{X_i\\}_{i=1}^{\\infty}$. These measurements are treated as independent and identically distributed (i.i.d.) random variables. Each random variable $X_i$ in the sequence has a mean $E[X_i] = \\mu$ and a variance $\\text{Var}(X_i) = \\sigma^2$.\n\nA common technique to analyze the high-frequency content or \"roughness\" of such a signal is to examine the statistical properties of the difference between successive samples. This analysis is fundamental in areas like data compression and predictive coding.\n\nDetermine the expected value of the squared difference between any two adjacent samples, $E[(X_{i+1} - X_i)^2]$. Express your answer as a closed-form expression in terms of $\\mu$ and $\\sigma^2$.",
            "solution": "We seek $E[(X_{i+1} - X_{i})^{2}]$ for i.i.d. random variables $\\{X_{i}\\}$ with $E[X_{i}] = \\mu$ and $\\text{Var}(X_{i}) = \\sigma^{2}$. Expand the square and take expectation:\n$$\nE[(X_{i+1} - X_{i})^{2}] = E[X_{i+1}^{2}] + E[X_{i}^{2}] - 2E[X_{i+1}X_{i}].\n$$\nUse the identity $E[X_{i}^{2}] = \\text{Var}(X_{i}) + (E[X_{i}])^{2} = \\sigma^{2} + \\mu^{2}$, which also holds for $X_{i+1}$ since the variables are identically distributed:\n$$\nE[X_{i+1}^{2}] = \\sigma^{2} + \\mu^{2}, \\quad E[X_{i}^{2}] = \\sigma^{2} + \\mu^{2}.\n$$\nBecause $X_{i+1}$ and $X_{i}$ are independent, $E[X_{i+1}X_{i}] = E[X_{i+1}]E[X_{i}] = \\mu^{2}$. Substitute these into the expansion:\n$$\nE[(X_{i+1} - X_{i})^{2}] = (\\sigma^{2} + \\mu^{2}) + (\\sigma^{2} + \\mu^{2}) - 2\\mu^{2} = 2\\sigma^{2}.\n$$\nAlternatively, note that $E[X_{i+1} - X_{i}] = \\mu - \\mu = 0$ and $\\text{Var}(X_{i+1} - X_{i}) = \\text{Var}(X_{i+1}) + \\text{Var}(X_{i}) - 2\\text{Cov}(X_{i+1}, X_{i}) = \\sigma^{2} + \\sigma^{2} - 0 = 2\\sigma^{2}$ by independence, and thus $E[(X_{i+1} - X_{i})^{2}] = \\text{Var}(X_{i+1} - X_{i}) + (E[X_{i+1} - X_{i}])^{2} = 2\\sigma^{2}$.",
            "answer": "$$\\boxed{2\\sigma^{2}}$$"
        }
    ]
}