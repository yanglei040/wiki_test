{
    "hands_on_practices": [
        {
            "introduction": "To begin, we'll explore the fundamental definition of conditional entropy with a clear and simple example involving a fair die. This exercise helps solidify the core idea: how much uncertainty about an outcome remains *after* we receive partial information. By calculating the remaining uncertainty in a die roll once we know its parity (even or odd), we can directly quantify the value of that partial information .",
            "id": "1612385",
            "problem": "Consider a standard, fair six-sided die. Let the random variable $X$ represent the outcome of a single roll of this die, so its set of possible values is $\\{1, 2, 3, 4, 5, 6\\}$, with each outcome being equally likely. A second random variable, $Y$, is determined by the parity of $X$. Specifically, $Y$ takes the value 0 if the outcome $X$ is an even number, and $Y$ takes the value 1 if the outcome $X$ is an odd number.\n\nAn experiment is performed where the die is rolled, but you are only told the value of $Y$ (i.e., you are only told whether the outcome was even or odd). We are interested in the amount of uncertainty that remains about the specific outcome $X$ after the parity information $Y$ has been revealed.\n\nCalculate this remaining average uncertainty about the outcome $X$. Express your answer in units of bits, which implies the use of a base-2 logarithm for all entropy calculations. Present your answer as a single, closed-form analytic expression.",
            "solution": "Let $X$ be uniformly distributed on $\\{1,2,3,4,5,6\\}$, so $P(X=k)=\\frac{1}{6}$ for each $k$. Let $Y$ be the parity of $X$, with $Y=0$ if $X\\in\\{2,4,6\\}$ and $Y=1$ if $X\\in\\{1,3,5\\}$. Since each of the three even and three odd outcomes are equally likely, we have\n$$\nP(Y=0)=\\sum_{x\\in\\{2,4,6\\}}P(X=x)=3\\cdot\\frac{1}{6}=\\frac{1}{2},\\quad P(Y=1)=\\frac{1}{2}.\n$$\nBecause $Y$ is a deterministic function of $X$, for $y\\in\\{0,1\\}$ and $x$ consistent with $y$,\n$$\nP(X=x\\mid Y=y)=\\frac{P(X=x, Y=y)}{P(Y=y)}=\\frac{P(X=x)}{P(Y=y)}=\\frac{\\frac{1}{6}}{\\frac{1}{2}}=\\frac{1}{3},\n$$\nand $P(X=x\\mid Y=y)=0$ otherwise. Thus, conditional on $Y=y$, $X$ is uniform over a set of three outcomes.\n\nThe conditional entropy given $Y=y$ is\n$$\nH(X\\mid Y=y)=-\\sum_{x}P(X=x\\mid Y=y)\\log_{2}\\bigl(P(X=x\\mid Y=y)\\bigr)\n=-3\\cdot\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)=\\log_{2}(3).\n$$\nThe average remaining uncertainty is the conditional entropy\n$$\nH(X\\mid Y)=\\sum_{y\\in\\{0,1\\}}P(Y=y)\\,H(X\\mid Y=y)\n=\\frac{1}{2}\\log_{2}(3)+\\frac{1}{2}\\log_{2}(3)=\\log_{2}(3).\n$$\nTherefore, the remaining average uncertainty about $X$ after observing $Y$ is $\\log_{2}(3)$ bits.",
            "answer": "$$\\boxed{\\log_{2}(3)}$$"
        },
        {
            "introduction": "Building on the basics, this next practice introduces a more nuanced scenario where the amount of information gained depends on the specific observation made. We will analyze two independent binary sources and calculate the uncertainty about one source given the sum of both. This problem highlights the important fact that conditional entropy, $H(X|Y)$, is an *average* of the uncertainty over all possible outcomes of $Y$ .",
            "id": "1612398",
            "problem": "Consider two independent binary sources, Source 1 and Source 2. Source 1 produces a random variable $X$, and Source 2 produces a random variable $Y$. Both $X$ and $Y$ can take values in the set $\\{0, 1\\}$, with $P(X=1) = P(X=0) = \\frac{1}{2}$ and $P(Y=1) = P(Y=0) = \\frac{1}{2}$.\n\nAn observer does not have access to the individual outputs $X$ and $Y$. Instead, the observer only knows the value of their sum, $S = X+Y$.\n\nCalculate the average uncertainty about the value of $X$ that remains after the value of $S$ is revealed. Express your final answer as a single real number in units of bits.",
            "solution": "We are asked to compute the average uncertainty about $X$ after observing $S=X+Y$, which is the conditional entropy $H(X|S)$ measured in bits. Throughout, we use base-2 logarithms, denoted $\\log_{2}$.\n\nGiven $X,Y \\in \\{0,1\\}$, independent with $P(X=1)=P(X=0)=\\frac{1}{2}$ and $P(Y=1)=P(Y=0)=\\frac{1}{2}$, the sum $S=X+Y$ takes values in $\\{0,1,2\\}$ with\n$$\nP(S=0)=P(X=0,Y=0)=\\frac{1}{4},\\quad P(S=2)=P(X=1,Y=1)=\\frac{1}{4},\\quad P(S=1)=\\frac{1}{2}.\n$$\nWe compute $H(X|S)$ via\n$$\nH(X|S)=\\sum_{s\\in\\{0,1,2\\}} P(S=s)\\,H(X|S=s).\n$$\nFor $s=0$, the event $S=0$ implies $(X,Y)=(0,0)$, hence $P(X=0|S=0)=1$ and\n$$\nH(X|S=0)=-\\left(1\\cdot \\log_{2}1\\right)=0.\n$$\nFor $s=2$, the event $S=2$ implies $(X,Y)=(1,1)$, hence $P(X=1|S=2)=1$ and\n$$\nH(X|S=2)=0.\n$$\nFor $s=1$, the event $S=1$ corresponds to $(X,Y)=(1,0)$ or $(0,1)$. Using independence,\n$$\nP(X=1,S=1)=P(X=1,Y=0)=\\frac{1}{4},\\quad P(S=1)=\\frac{1}{2},\n$$\nso\n$$\nP(X=1|S=1)=\\frac{P(X=1,S=1)}{P(S=1)}=\\frac{\\frac{1}{4}}{\\frac{1}{2}}=\\frac{1}{2},\\quad P(X=0|S=1)=\\frac{1}{2}.\n$$\nThus\n$$\nH(X|S=1)=-\\left(\\frac{1}{2}\\log_{2}\\frac{1}{2}+\\frac{1}{2}\\log_{2}\\frac{1}{2}\\right)=1.\n$$\nTherefore,\n$$\nH(X|S)=P(S=0)\\cdot 0+P(S=1)\\cdot 1+P(S=2)\\cdot 0=\\frac{1}{2}\\ \\text{bits}.\n$$",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "Finally, let's apply our understanding of conditional entropy to a famous and often counter-intuitive puzzle: the Monty Hall problem. By framing the problem in terms of information theory, we can precisely measure the remaining uncertainty about the car's location after the host reveals a goat. This practice demonstrates the power of conditional entropy to analyze complex scenarios and provide clear, quantitative answers to questions about information .",
            "id": "1612388",
            "problem": "Consider a variation of a classic game show problem designed to explore the flow of information. There are three identical, closed doors: Door 1, Door 2, and Door 3. Behind one door is a valuable car, and behind the other two are goats. The placement of the car is random, with each door having an equal probability of hiding the car.\n\nYou, the contestant, make an initial choice and select Door 1.\n\nThe host, who knows the location of the car, then opens one of the two remaining doors (Door 2 or Door 3) to reveal a goat. The host's strategy is as follows: if the car is behind your chosen door (Door 1), the host opens one of the other two doors (Door 2 or Door 3) with equal probability. If the car is not behind your chosen door, the host is constrained to open the only other door that hides a goat.\n\nLet $C$ be the random variable representing the door number hiding the car, with possible outcomes $\\{1, 2, 3\\}$. Let $H$ be the random variable representing the door number opened by the host, with possible outcomes $\\{2, 3\\}$.\n\nCalculate the conditional entropy $H(C|H)$, which quantifies the average remaining uncertainty about the car's location after you have observed the host's action. Express your answer as a single analytic expression in bits. Note that information theory calculations use base-2 logarithms.",
            "solution": "Let $C \\in \\{1,2,3\\}$ be the car’s door with $P(C=c)=\\frac{1}{3}$ for each $c$, and let $H \\in \\{2,3\\}$ be the host’s opened door. The host policy implies the conditional probabilities:\n$$\nP(H=2 \\mid C=1)=\\frac{1}{2}, \\quad P(H=3 \\mid C=1)=\\frac{1}{2}, \\quad P(H=2 \\mid C=2)=0, \\quad P(H=3 \\mid C=2)=1,\n$$\n$$\nP(H=2 \\mid C=3)=1, \\quad P(H=3 \\mid C=3)=0.\n$$\nBy the law of total probability,\n$$\nP(H=2)=\\sum_{c=1}^{3} P(H=2 \\mid C=c)P(C=c)=\\frac{1}{2}\\cdot \\frac{1}{3}+0\\cdot \\frac{1}{3}+1\\cdot \\frac{1}{3}=\\frac{1}{2},\n$$\nand similarly $P(H=3)=\\frac{1}{2}$.\n\nUsing Bayes’ rule, for $H=2$,\n$$\nP(C=1 \\mid H=2)=\\frac{P(H=2 \\mid C=1)P(C=1)}{P(H=2)}=\\frac{\\frac{1}{2}\\cdot \\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{3}, \\quad\nP(C=3 \\mid H=2)=\\frac{1\\cdot \\frac{1}{3}}{\\frac{1}{2}}=\\frac{2}{3}, \\quad\nP(C=2 \\mid H=2)=0.\n$$\nBy symmetry, for $H=3$,\n$$\nP(C=1 \\mid H=3)=\\frac{1}{3}, \\quad P(C=2 \\mid H=3)=\\frac{2}{3}, \\quad P(C=3 \\mid H=3)=0.\n$$\n\nThe conditional entropy is\n$$\nH(C \\mid H)=\\sum_{h \\in \\{2,3\\}} P(H=h)\\left(-\\sum_{c \\in \\{1,2,3\\}} P(C=c \\mid H=h)\\log_{2} P(C=c \\mid H=h)\\right),\n$$\nwith the convention $0\\log_{2} 0=0$ by continuity. For each $h$, the posterior over $C$ is $\\left(\\frac{1}{3},\\frac{2}{3},0\\right)$ up to permutation, hence\n$$\nH(C \\mid H)=\\frac{1}{2}\\left(-\\frac{1}{3}\\log_{2}\\frac{1}{3}-\\frac{2}{3}\\log_{2}\\frac{2}{3}\\right)+\\frac{1}{2}\\left(-\\frac{1}{3}\\log_{2}\\frac{1}{3}-\\frac{2}{3}\\log_{2}\\frac{2}{3}\\right)\n=-\\frac{1}{3}\\log_{2}\\frac{1}{3}-\\frac{2}{3}\\log_{2}\\frac{2}{3}.\n$$\nSimplifying using $\\log_{2}\\left(\\frac{1}{3}\\right)=-\\log_{2} 3$ and $\\log_{2}\\left(\\frac{2}{3}\\right)=1-\\log_{2} 3$,\n$$\n-\\frac{1}{3}\\left(-\\log_{2} 3\\right)-\\frac{2}{3}\\left(1-\\log_{2} 3\\right)=\\log_{2} 3-\\frac{2}{3}.\n$$\nTherefore, the conditional entropy in bits is $\\log_{2}(3)-\\frac{2}{3}$.",
            "answer": "$$\\boxed{\\log_{2}(3)-\\frac{2}{3}}$$"
        }
    ]
}