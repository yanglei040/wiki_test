{
    "hands_on_practices": [
        {
            "introduction": "The Data Processing Inequality is a cornerstone of information theory, stating that one cannot create new information by post-processing data. This first practice problem provides a direct and intuitive illustration of this principle using a cascade of two noisy channels, forming a Markov chain $X \\to Y \\to Z$ . By analyzing this simple system, you will formalize the idea that each stage of processing can only degrade the signal's information content about the original source, leading to the fundamental result $I(X;Z) \\le I(X;Y)$.",
            "id": "1613405",
            "problem": "A binary signal $X$ is drawn from a distribution where $P(X=0) = \\alpha$ and $P(X=1) = 1-\\alpha$, with $0 < \\alpha < 1$. The signal is sent through a cascade of two independent noisy channels. The first channel is a Binary Symmetric Channel (BSC), which flips the transmitted bit with a crossover probability of $p_1$, producing an intermediate signal $Y$. This signal $Y$ is then immediately fed as input into a second, independent BSC with a crossover probability of $p_2$, producing the final signal $Z$. Assume that the channels are neither perfect nor completely random, so their crossover probabilities are in the range $0 < p_1 < 1$ and $0 < p_2 < 1$.\n\nLet $I(X; Y)$ denote the mutual information between the source $X$ and the intermediate signal $Y$, and let $I(X; Z)$ denote the mutual information between the source $X$ and the final signal $Z$. All mutual information values are calculated in units of bits. Which of the following relationships is universally true for the described system under the given conditions?\n\nA. $I(X; Z) > I(X; Y)$\n\nB. $I(X; Z) = I(X; Y)$\n\nC. $I(X; Z) < I(X; Y)$\n\nD. $I(X; Z) \\le I(X; Y)$\n\nE. $I(X; Z) \\ge I(X; Y)$",
            "solution": "The problem describes a system where a signal $X$ is processed to create $Y$, which is then further processed to create $Z$. The key insight is to recognize the relationship between the three random variables $X$, $Y$, and $Z$.\n\nThe output of the first channel, $Y$, depends only on its input, $X$. The output of the second channel, $Z$, depends only on its input, $Y$. This means that if we know the intermediate signal $Y$, the final signal $Z$ is conditionally independent of the original signal $X$. This structure is known as a Markov chain, which is denoted as $X \\to Y \\to Z$. The joint probability distribution for such a chain factorizes as $p(x, y, z) = p(x) p(y|x) p(z|y)$.\n\nWe can analyze the relationship between $I(X;Y)$ and $I(X;Z)$ using the chain rule for mutual information. The mutual information between $X$ and the pair $(Y, Z)$ can be expanded in two ways:\n\n1.  $I(X; Y, Z) = I(X; Y) + I(X; Z|Y)$\n2.  $I(X; Y, Z) = I(X; Z) + I(X; Y|Z)$\n\nFrom the definition of the Markov chain $X \\to Y \\to Z$, we know that $X$ and $Z$ are conditionally independent given $Y$. This property implies that the conditional mutual information $I(X; Z|Y)$ is zero.\n$$I(X; Z|Y) = 0$$\n\nSubstituting this result into the first expansion gives:\n$$I(X; Y, Z) = I(X; Y) + 0 = I(X; Y)$$\n\nNow we can equate the two expressions for $I(X; Y, Z)$:\n$$I(X; Y) = I(X; Z) + I(X; Y|Z)$$\n\nA fundamental property of mutual information is that it is always non-negative. This applies to conditional mutual information as well. Therefore:\n$$I(X; Y|Z) \\ge 0$$\n\nThis term, $I(X; Y|Z)$, represents the remaining uncertainty about $Y$ that is resolved by knowing $X$, even after $Z$ is already known. Intuitively, it is the information \"lost\" in the second channel.\n\nCombining our equation with the non-negativity property, we get:\n$$I(X; Y) \\ge I(X; Z)$$\nor equivalently,\n$$I(X; Z) \\le I(X; Y)$$\n\nThis result is a famous theorem in information theory called the Data Processing Inequality. It states that post-processing of data (in this case, passing $Y$ through the second BSC to get $Z$) cannot increase the mutual information with respect to the original source $X$.\n\nLet's analyze the given options based on this inequality:\n- Option A ($I(X; Z) > I(X; Y)$) and Option E ($I(X; Z) \\ge I(X; Y)$) are incorrect as they contradict the Data Processing Inequality (unless equality holds).\n- Option B ($I(X; Z) = I(X; Y)$) is not always true. Equality holds if and only if $I(X; Y|Z) = 0$. This would mean that $Y$ can be perfectly determined from $Z$. For a BSC with crossover probability $p_2 \\in (0, 1)$, the mapping from $Y$ to $Z$ is not one-to-one, so in general $Y$ cannot be recovered from $Z$ without uncertainty. For example, if $p_1=0.1, p_2=0.1, \\alpha=0.5$, a direct calculation would show a strict inequality.\n- Option C ($I(X; Z) < I(X; Y)$) is also not always true. While it holds for most non-trivial cases, it is possible for equality to occur. For example, if the first channel is such that $I(X; Y) = 0$ (e.g., if $p_1=0.5$ and $\\alpha=0.5$), then since $Z$ is a further processed version of $Y$, we must also have $I(X; Z) = 0$. In this case, $I(X; Z) = I(X; Y) = 0$.\n- Option D ($I(X; Z) \\le I(X; Y)$) is the only relationship that is universally true for any choice of $\\alpha$, $p_1$, and $p_2$ within the specified ranges, as it correctly captures the Data Processing Inequality, including the possibility of equality in specific edge cases.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "We now move from discrete bits to continuous signals, a scenario frequently encountered in engineering and signal processing. This exercise models a signal corrupted by successive stages of additive Gaussian noise, a fundamental model for communication channels . You will use the tools of differential entropy to explicitly calculate and compare the mutual information before and after the second noise stage, thereby providing a concrete verification and deeper intuition for the Data Processing Inequality in a continuous setting.",
            "id": "1613384",
            "problem": "Consider a simplified model for a signal cascade in a communication system. An initial continuous signal, modeled by a random variable $X$, is a zero-mean Gaussian random variable with variance $\\sigma_X^2$. This signal passes through a first noisy channel, which adds independent, zero-mean Gaussian noise $N_1$ with variance $\\sigma_{N_1}^2$. The resulting signal is $Y = X + N_1$. This signal $Y$ is then fed into a second channel, which adds further independent, zero-mean Gaussian noise $N_2$ with variance $\\sigma_{N_2}^2$. The final output signal is $Z = Y + N_2$. The random variables $X$, $N_1$, and $N_2$ are all mutually independent. Assume that all variances, $\\sigma_X^2$, $\\sigma_{N_1}^2$, and $\\sigma_{N_2}^2$, are strictly positive.\n\nAll information-theoretic quantities are to be measured in nats, which means all logarithms are natural logarithms ($\\ln$). Let $I(X;Y)$ be the mutual information between the original signal $X$ and the signal after the first stage, $Y$. Let $I(X;Z)$ be the mutual information between the original signal $X$ and the final signal, $Z$.\n\nWhich of the following statements correctly describes the relationship between $I(X;Y)$ and $I(X;Z)$?\n\nA. $I(X;Y) < I(X;Z)$\n\nB. $I(X;Y) > I(X;Z)$\n\nC. $I(X;Y) = I(X;Z)$\n\nD. The relationship cannot be determined without knowing the specific numerical values of the variances.\n\nE. $I(X;Y)$ and $I(X;Z)$ are both infinite.",
            "solution": "We are given $X \\sim \\mathcal{N}(0,\\sigma_{X}^{2})$, $N_{1} \\sim \\mathcal{N}(0,\\sigma_{N_{1}}^{2})$, $N_{2} \\sim \\mathcal{N}(0,\\sigma_{N_{2}}^{2})$, all mutually independent and with strictly positive variances. The channels are $Y = X + N_{1}$ and $Z = Y + N_{2} = X + (N_{1}+N_{2})$. Since $N_{1}$ and $N_{2}$ are independent Gaussians, $N_{1}+N_{2} \\sim \\mathcal{N}(0,\\sigma_{N_{1}}^{2}+\\sigma_{N_{2}}^{2})$ and is independent of $X$.\n\nWe compute mutual informations using $I(U;V) = h(V) - h(V|U)$ and the differential entropy of a zero-mean Gaussian $W \\sim \\mathcal{N}(0,\\sigma^{2})$, namely $h(W) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,\\sigma^{2}\\big)$.\n\nFirst stage:\n- $Y = X + N_{1}$ is Gaussian with variance $\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2}$, so\n$$\nh(Y) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,(\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2})\\big).\n$$\n- Given $X$, $Y|X = X + N_{1}$ has the same distribution as $N_{1}$, so\n$$\nh(Y|X) = h(N_{1}) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,\\sigma_{N_{1}}^{2}\\big).\n$$\nThus\n$$\nI(X;Y) = h(Y) - h(Y|X) = \\frac{1}{2} \\ln\\!\\left(\\frac{\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2}}{\\sigma_{N_{1}}^{2}}\\right) = \\frac{1}{2} \\ln\\!\\left(1 + \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}}\\right).\n$$\n\nSecond stage:\n- $Z = X + (N_{1}+N_{2})$ is Gaussian with variance $\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}$, so\n$$\nh(Z) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,(\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2})\\big).\n$$\n- Given $X$, $Z|X$ has the same distribution as $N_{1}+N_{2}$, so\n$$\nh(Z|X) = h(N_{1}+N_{2}) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,(\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2})\\big).\n$$\nThus\n$$\nI(X;Z) = h(Z) - h(Z|X) = \\frac{1}{2} \\ln\\!\\left(\\frac{\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}}{\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}}\\right) = \\frac{1}{2} \\ln\\!\\left(1 + \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}}\\right).\n$$\n\nComparison:\nSince $\\sigma_{N_{2}}^{2} > 0$, we have $\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2} > \\sigma_{N_{1}}^{2}$, which implies\n$$\n\\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}} < \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}}.\n$$\nBecause $\\ln$ is strictly increasing,\n$$\n\\frac{1}{2} \\ln\\!\\left(1 + \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}}\\right) < \\frac{1}{2} \\ln\\!\\left(1 + \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}}\\right),\n$$\nso $I(X;Z) < I(X;Y)$.\n\nThis also aligns with the data processing inequality for the Markov chain $X \\to Y \\to Z$, which guarantees $I(X;Z) \\leq I(X;Y)$; strict inequality holds here because additional independent Gaussian noise is added in the second stage and all variances are strictly positive.\n\nTherefore, the correct choice is B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Having established that processing generally leads to information loss, a crucial theoretical question arises: when exactly is no information lost? This advanced problem challenges you to derive the necessary and sufficient conditions for the equality case of the Data Processing Inequality, $I(X;Z) = I(X;Y)$, to hold for a Markov chain $X \\to Y \\to Z$ . Successfully solving this requires moving beyond calculation to a deep, structural understanding of what constitutes an \"information-preserving\" transformation.",
            "id": "1613387",
            "problem": "Consider a communication system modeled by three discrete random variables $X$, $Y$, and $Z$ with finite alphabets $\\mathcal{X}$, $\\mathcal{Y}$, and $\\mathcal{Z}$, respectively. The variables form a Markov chain $X \\to Y \\to Z$, which implies that their joint probability distribution is given by $P(x,y,z) = P(x)P(y|x)P(z|y)$. The initial joint distribution of the source $X$ and the first stage output $Y$, denoted by $P_{XY}(x,y)$, is fixed. The second stage is a memoryless channel described by the conditional probability distribution $P_{Z|Y}(z|y)$, which maps an input from $\\mathcal{Y}$ to an output in $\\mathcal{Z}$.\n\nThe Data Processing Inequality states that $I(X;Z) \\le I(X;Y)$, where $I(\\cdot;\\cdot)$ denotes the mutual information. This inequality means that no processing of $Y$ can increase the information it contains about $X$. Your task is to analyze the specific case where this inequality is met with equality, i.e., $I(X;Z) = I(X;Y)$.\n\nWhich of the following statements provides the necessary and sufficient condition on the channel $P_{Z|Y}$ for the equality $I(X;Z) = I(X;Y)$ to hold for the given $P_{XY}$?\n\nA. The channel $P_{Z|Y}$ must be deterministic and injective. That is, for each $y \\in \\mathcal{Y}$, there is a unique $z \\in \\mathcal{Z}$ such that $P_{Z|Y}(z|y) = 1$, and different $y$ values map to different $z$ values.\n\nB. The channel $P_{Z|Y}$ must be such that $Z$ is a deterministic function of $Y$. That is, for each $y \\in \\mathcal{Y}$, there is a specific $z \\in \\mathcal{Z}$ for which $P_{Z|Y}(z|y) = 1$.\n\nC. For any two inputs to the channel, $y_1 \\in \\mathcal{Y}$ and $y_2 \\in \\mathcal{Y}$, if the conditional probability distributions of $X$ given these inputs are different (i.e., $P_{X|Y}(\\cdot|y_1) \\neq P_{X|Y}(\\cdot|y_2)$), then the sets of their possible outputs must be disjoint.\n\nD. The mutual information between the channel's input and output must be maximized, i.e., $I(Y;Z) = \\min(\\log|\\mathcal{Y}|, \\log|\\mathcal{Z}|)$.\n\nE. The output alphabet $\\mathcal{Z}$ must be a partition of the input alphabet $\\mathcal{Y}$.\n\nF. The output $Z$ must be statistically independent of the input $Y$, i.e., $I(Y;Z) = 0$.",
            "solution": "We are given a Markov chain $X \\to Y \\to Z$ with joint law $P(x,y,z)=P(x)P(y|x)P(z|y)$, a fixed $P_{XY}$, and a memoryless channel $P_{Z|Y}$. The Data Processing Inequality states $I(X;Z) \\leq I(X;Y)$. We seek necessary and sufficient conditions on $P_{Z|Y}$ for the equality $I(X;Z)=I(X;Y)$ to hold for the given $P_{XY}$.\n\nStart from the chain rule for mutual information:\n$$\nI(X;Y,Z)=I(X;Z)+I(X;Y|Z)=I(X;Y)+I(X;Z|Y).\n$$\nBecause $X \\to Y \\to Z$ is a Markov chain, we have $I(X;Z|Y)=0$. Hence\n$$\nI(X;Y)=I(X;Z)+I(X;Y|Z).\n$$\nTherefore $I(X;Z)=I(X;Y)$ if and only if\n$$\nI(X;Y|Z)=0.\n$$\nThe condition $I(X;Y|Z)=0$ is equivalent to $X$ and $Y$ being conditionally independent given $Z$, namely\n$$\nP_{X|Y,Z}(x|y,z)=P_{X|Z}(x|z) \\quad \\text{for all } x,y,z \\text{ with } P(y,z)>0.\n$$\nUnder the Markov structure $X \\to Y \\to Z$, we can compute\n$$\nP(x|y,z)=\\frac{P(x,y,z)}{P(y,z)}=\\frac{P(x)P(y|x)P(z|y)}{\\sum_{x'}P(x')P(y|x')P(z|y)}=\\frac{P(x)P(y|x)}{\\sum_{x'}P(x')P(y|x')}=P(x|y).\n$$\nThus the equality condition reduces to\n$$\nP(x|y)=P(x|z) \\quad \\text{for all } x \\text{ and for all } y,z \\text{ with } P(y)P(z|y)>0.\n$$\nEquivalently, for each $z$, all $y$ that can produce $z$ with positive probability must share the same posterior $P_{X|Y=\\!y}$. Define the support sets\n$$\n\\mathcal{S}(y)\\triangleq\\{z \\in \\mathcal{Z}: P_{Z|Y}(z|y)>0\\}.\n$$\nThen the above condition says: if there exists $z$ such that $z \\in \\mathcal{S}(y_{1}) \\cap \\mathcal{S}(y_{2})$, then $P_{X|Y}(\\cdot|y_{1})=P_{X|Y}(\\cdot|y_{2})$. Taking the contrapositive, if $P_{X|Y}(\\cdot|y_{1}) \\neq P_{X|Y}(\\cdot|y_{2})$, then $\\mathcal{S}(y_{1}) \\cap \\mathcal{S}(y_{2})=\\varnothing$, i.e., the sets of possible outputs must be disjoint.\n\nNecessity: Suppose $I(X;Z)=I(X;Y)$. Then $I(X;Y|Z)=0$, which implies that for any $z$ and any $y$ with $P(z|y)>0$, $P_{X|Y=y}$ must be equal to $P_{X|Z=z}$. Hence, if two inputs $y_{1}$ and $y_{2}$ have different $P_{X|Y}$, they cannot both yield the same $z$ with positive probability; thus $\\mathcal{S}(y_{1})$ and $\\mathcal{S}(y_{2})$ are disjoint.\n\nSufficiency: Suppose that whenever $P_{X|Y}(\\cdot|y_{1}) \\neq P_{X|Y}(\\cdot|y_{2})$, we have $\\mathcal{S}(y_{1}) \\cap \\mathcal{S}(y_{2})=\\varnothing$. Then for any fixed $z$, all $y$ with $P(z|y)>0$ have the same posterior $P_{X|Y=y}$; call this common distribution $q_{z}$. Then\n$$\nP(x|z)=\\sum_{y}P(x|y,z)P(y|z)=\\sum_{y}P(x|y)P(y|z)=q_{z}(x),\n$$\nso $P(x|y,z)=P(x|y)=P(x|z)$ and thus $I(X;Y|Z)=0$. Therefore $I(X;Z)=I(X;Y)$.\n\nThis condition is exactly option C. Option A (deterministic injective) is sufficient but not necessary. Option B (deterministic function) is neither necessary nor sufficient in general, unless the partition aligns with equal posteriors. Option D is unrelated to the equality condition. Option E is ill-posed as stated and, if interpreted as a deterministic partition, is again only sufficient under alignment with posterior equivalence classes. Option F would force $I(X;Z)=0$, which only matches $I(X;Y)$ in the trivial case $I(X;Y)=0$.\n\nTherefore, the necessary and sufficient condition on $P_{Z|Y}$ for the given $P_{XY}$ is the disjoint-support condition stated in option C.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}