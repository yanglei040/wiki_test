{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of data compression is minimizing the average number of bits needed to represent information from a source. For any source with known symbol probabilities, Huffman's algorithm provides a direct and elegant method for constructing a prefix code that achieves this minimum possible expected length. This first exercise  will guide you through the step-by-step application of the Huffman procedure, giving you foundational practice in this essential data compression technique.",
            "id": "1623278",
            "problem": "A deep-space probe is designed to monitor and classify five distinct types of cosmic ray events, labeled A, B, C, D, and E. The probe operates autonomously and, due to power limitations for its transmitter, must encode the classification of each detected event into a sequence of binary digits (bits) using the most efficient scheme possible. From extensive prior observations, the long-term probabilities for the detection of each event type within a standard observation window have been determined. Events A and B each occur with a probability of $1/3$. Events C, D, and E each occur with a probability of $1/9$.\n\nAssuming the goal is to minimize the average number of bits transmitted per event, what is this minimum possible average length? Express your answer as a single fraction.",
            "solution": "We model the problem as constructing a binary prefix code to minimize the expected codeword length. Let the symbol probabilities be $p_{A}=p_{B}=\\frac{1}{3}$ and $p_{C}=p_{D}=p_{E}=\\frac{1}{9}$. For optimality among binary prefix codes, we apply Huffman coding.\n\nStep-by-step Huffman merges using the smallest probabilities:\n1. Merge two $\\frac{1}{9}$ symbols to form a node of weight $\\frac{2}{9}$. This increases the codeword lengths of those two symbols by $1$.\n2. Merge $\\frac{2}{9}$ with the remaining $\\frac{1}{9}$ to form a node of weight $\\frac{3}{9}=\\frac{1}{3}$. This increases the codeword lengths of the three $\\frac{1}{9}$ symbols by $1$ (cumulative so far: the first two have increased by $2$, the last by $1$).\n3. Now the multiset of weights is $\\left\\{\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}\\right\\}$. Merge any two $\\frac{1}{3}$ to get a node of weight $\\frac{2}{3}$. This increases the codeword lengths of those two symbols by $1$.\n4. Merge $\\frac{2}{3}$ with the remaining $\\frac{1}{3}$ to form the root. This increases the codeword lengths of all three symbols under these nodes by $1$.\n\nTracing depths (codeword lengths):\n- Symbols $A$ and $B$ (both with probability $\\frac{1}{3}$) are merged together in step 3 and then merged with the other $\\frac{1}{3}$ node in step 4, so $l_{A}=l_{B}=2$.\n- The symbol among $\\{C,D,E\\}$ that did not participate in the first merge (call it $E$) is merged in steps 2 and 4, so $l_{E}=2$.\n- The two symbols that were merged first (call them $C$ and $D$) are merged in steps 1, 2, and 4, so $l_{C}=l_{D}=3$.\n\nThus the expected codeword length is\n$$\nL=\\frac{1}{3}\\cdot l_{A}+\\frac{1}{3}\\cdot l_{B}+\\frac{1}{9}\\cdot l_{C}+\\frac{1}{9}\\cdot l_{D}+\\frac{1}{9}\\cdot l_{E}\n= \\frac{1}{3}\\cdot 2+\\frac{1}{3}\\cdot 2+\\frac{1}{9}\\cdot 3+\\frac{1}{9}\\cdot 3+\\frac{1}{9}\\cdot 2.\n$$\nCompute:\n$$\nL=\\frac{2}{3}+\\frac{2}{3}+\\frac{3}{9}+\\frac{3}{9}+\\frac{2}{9}=\\frac{4}{3}+\\frac{8}{9}=\\frac{12}{9}+\\frac{8}{9}=\\frac{20}{9}.\n$$\nBy the optimality of Huffman coding, this is the minimum possible average number of bits per event. As a consistency check, the source entropy in bits is\n$$\nH=-2\\cdot \\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)-3\\cdot \\frac{1}{9}\\log_{2}\\!\\left(\\frac{1}{9}\\right)=\\frac{4}{3}\\log_{2}(3),\n$$\nand indeed $H \\leq \\frac{20}{9}$, consistent with the coding bound.",
            "answer": "$$\\boxed{\\frac{20}{9}}$$"
        },
        {
            "introduction": "While Huffman's algorithm guarantees an optimal expected code length, the resulting code itself is not always unique. When probabilities are tied during the merging process, different choices can lead to different code trees, all of which are optimal in terms of average length. This practice  delves into this nuance by asking you to construct two distinct optimal codes for the same source and compare them using a secondary metric: the variance of codeword length, a crucial factor in designing systems with fixed-rate data buffers.",
            "id": "1623280",
            "problem": "A discrete memoryless source emits symbols from the alphabet $\\mathcal{S} = \\{S_1, S_2, S_3, S_4, S_5\\}$. The probabilities of these symbols are given by:\n$P(S_1) = 1/3$\n$P(S_2) = 1/3$\n$P(S_3) = 1/9$\n$P(S_4) = 1/9$\n$P(S_5) = 1/9$\n\nFor this particular probability distribution, it is possible to construct more than one optimal prefix code using the Huffman algorithm, where the resulting codes are distinguished by having different sets of codeword lengths.\n\nConstruct two such distinct Huffman codes. For each code, calculate the variance of the codeword length. The variance $\\sigma^2$ of the codeword length $l$ is defined as $\\sigma^2 = E[l^2] - (E[l])^2$, where $E[\\cdot]$ denotes the expectation with respect to the source probabilities.\n\nWhat is the absolute difference between the two calculated variances? Express your final answer as a decimal rounded to four significant figures.",
            "solution": "Order the source probabilities in nondecreasing order: three symbols with probability $1/9$ and two with probability $1/3$. Apply the binary Huffman algorithm. The ties allow different choices at one merge, leading to two distinct optimal trees with different sets of codeword lengths.\n\nConstruct Code A (merge the two $1/3$ symbols together at the third step):\n- Merge two of the three $1/9$ to get $2/9$.\n- Merge $1/9$ and $2/9$ to get $1/3$.\n- Merge the two $1/3$ source symbols to get $2/3$, then merge with the $1/3$ compound node.\n\nOne explicit assignment consistent with this tree is:\nS1: 00, S2: 01, S3: 10, S4: 110, S5: 111.\nThis yields codeword lengths $\\{2,2,2,3,3\\}$ assigned to probabilities $\\{1/3,1/3,1/9,1/9,1/9\\}$ respectively.\n\nCompute expectations for Code A:\n$$E[l]=\\frac{1}{3}\\cdot 2+\\frac{1}{3}\\cdot 2+\\frac{1}{9}\\cdot 2+\\frac{1}{9}\\cdot 3+\\frac{1}{9}\\cdot 3=\\frac{20}{9}.$$\n$$E[l^{2}]=\\frac{1}{3}\\cdot 4+\\frac{1}{3}\\cdot 4+\\frac{1}{9}\\cdot 4+\\frac{1}{9}\\cdot 9+\\frac{1}{9}\\cdot 9=\\frac{46}{9}.$$\nTherefore\n$$\\sigma^{2}_{A}=E[l^{2}]-(E[l])^{2}=\\frac{46}{9}-\\left(\\frac{20}{9}\\right)^{2}=\\frac{46}{9}-\\frac{400}{81}=\\frac{14}{81}.$$\n\nConstruct Code B (merge one $1/3$ symbol with the $1/3$ compound node at the third step):\n- Merge two of the three $1/9$ to get $2/9$.\n- Merge $1/9$ and $2/9$ to get $1/3$.\n- Merge one $1/3$ source symbol with the $1/3$ compound node to get $2/3$, then merge with the remaining $1/3$ source symbol.\n\nOne explicit assignment consistent with this tree is:\nS2: 0, S1: 10, S3: 110, S4: 1110, S5: 1111.\nThis yields codeword lengths $\\{1,2,3,4,4\\}$ assigned to probabilities $\\{1/3,1/3,1/9,1/9,1/9\\}$ respectively.\n\nCompute expectations for Code B:\n$$E[l]=\\frac{1}{3}\\cdot 1+\\frac{1}{3}\\cdot 2+\\frac{1}{9}\\cdot 3+\\frac{1}{9}\\cdot 4+\\frac{1}{9}\\cdot 4=\\frac{20}{9}.$$\n$$E[l^{2}]=\\frac{1}{3}\\cdot 1+\\frac{1}{3}\\cdot 4+\\frac{1}{9}\\cdot 9+\\frac{1}{9}\\cdot 16+\\frac{1}{9}\\cdot 16=\\frac{56}{9}.$$\nTherefore\n$$\\sigma^{2}_{B}=E[l^{2}]-(E[l])^{2}=\\frac{56}{9}-\\left(\\frac{20}{9}\\right)^{2}=\\frac{56}{9}-\\frac{400}{81}=\\frac{104}{81}.$$\n\nThe absolute difference between the two variances is\n$$\\left|\\sigma^{2}_{B}-\\sigma^{2}_{A}\\right|=\\left|\\frac{104}{81}-\\frac{14}{81}\\right|=\\frac{90}{81}=\\frac{10}{9}.$$\nAs a decimal rounded to four significant figures, this is $1.111$.",
            "answer": "$$\\boxed{1.111}$$"
        },
        {
            "introduction": "The optimality of Huffman coding is unparalleled for unconstrained prefix codes, but real-world systems often impose additional requirements. A common constraint, especially in data retrieval and indexing, is that the code must be 'alphabetic'â€”meaning the lexicographical order of codewords must match the natural order of the source symbols. This exercise  challenges you to move beyond standard Huffman coding and find the optimal code under this alphabetic constraint, illustrating how problem requirements can fundamentally change the optimal solution and the method needed to find it.",
            "id": "1623266",
            "problem": "A data acquisition system monitors a physical process that can be in one of five discrete states, denoted by the ordered alphabet $S = \\{s_1, s_2, s_3, s_4, s_5\\}$, where the index implies a natural ordering ($s_1 < s_2 < s_3 < s_4 < s_5$). After a long observation period, the stationary probability distribution for these states has been determined as follows:\n$p(s_1) = 0.05$\n$p(s_2) = 0.30$\n$p(s_3) = 0.05$\n$p(s_4) = 0.40$\n$p(s_5) = 0.20$\n\nFor efficient storage, the stream of state symbols must be encoded into a binary prefix code. A critical requirement for the system's data-retrieval algorithm is that the lexicographical order of the binary codewords must correspond to the intrinsic order of the states. That is, if $C(s_i)$ is the codeword for state $s_i$, the condition $C(s_i) < C(s_j)$ must hold for all $i < j$, where the comparison is lexicographical.\n\nDetermine the expected codeword length for an optimal binary prefix code that satisfies this alphabetic constraint. Express your answer as a decimal rounded to three significant figures.",
            "solution": "We seek an optimal binary alphabetic (ordered) prefix code for ordered states $s_{1}<s_{2}<s_{3}<s_{4}<s_{5}$ with probabilities $w_{1}=0.05$, $w_{2}=0.30$, $w_{3}=0.05$, $w_{4}=0.40$, $w_{5}=0.20$. For an alphabetic code, the leaves must appear in order in a full binary tree. The expected codeword length is the weighted external path length $L=\\sum_{i=1}^{5}w_{i}d_{i}$, where $d_{i}$ is the depth of leaf $i$.\n\nUse the optimal alphabetic coding dynamic programming. For any interval $[i,j]$, let $W(i,j)=\\sum_{k=i}^{j}w_{k}$ and let $F(i,j)$ be the minimal weighted external path length within that subinterval, measured with the subtree root at depth zero. The recurrence is\n$$\nF(i,i)=0,\\quad F(i,j)=\\min_{r\\in\\{i,\\dots,j-1\\}}\\big(F(i,r)+F(r+1,j)\\big)+W(i,j).\n$$\nCompute $W(i,j)$ where needed: $W(1,2)=0.35$, $W(2,3)=0.35$, $W(3,4)=0.45$, $W(4,5)=0.60$, $W(1,3)=0.40$, $W(2,4)=0.75$, $W(3,5)=0.65$, $W(1,4)=0.80$, $W(2,5)=0.95$, $W(1,5)=1.00$.\n\nLength 2 intervals:\n$$\nF(1,2)=0.35,\\quad F(2,3)=0.35,\\quad F(3,4)=0.45,\\quad F(4,5)=0.60.\n$$\n\nLength 3 intervals:\n$$\nF(1,3)=0.40+\\min\\{0+0.35,\\,0.35+0\\}=0.40+0.35=0.75,\n$$\n$$\nF(2,4)=0.75+\\min\\{0+0.45,\\,0.35+0\\}=0.75+0.35=1.10,\n$$\n$$\nF(3,5)=0.65+\\min\\{0+0.60,\\,0.45+0\\}=0.65+0.45=1.10.\n$$\n\nLength 4 intervals:\n$$\nF(1,4)=0.80+\\min\\{0+1.10,\\,0.35+0.45,\\,0.75+0\\}=0.80+0.75=1.55,\n$$\n$$\nF(2,5)=0.95+\\min\\{0+1.10,\\,0.35+0.60,\\,1.10+0\\}=0.95+0.95=1.90.\n$$\n\nLength 5 interval:\n$$\nF(1,5)=1.00+\\min\\{0+1.90,\\,0.35+1.10,\\,0.75+0.60,\\,1.55+0\\}=1.00+1.35=2.35.\n$$\n\nTherefore, the minimal expected codeword length for an optimal alphabetic binary prefix code is $2.35$. One optimal code structure achieving this is to split at the root into $\\{s_{1},s_{2},s_{3}\\}$ and $\\{s_{4},s_{5}\\}$, with an optimal split inside the left subset, yielding depths, for example, $(2,3,3,2,2)$ and expected length $0.05\\cdot 2+0.30\\cdot 3+0.05\\cdot 3+0.40\\cdot 2+0.20\\cdot 2=2.35$.",
            "answer": "$$\\boxed{2.35}$$"
        }
    ]
}