{
    "hands_on_practices": [
        {
            "introduction": "The first step in understanding data compression is to quantify the amount of information a source produces. Shannon's Source Coding Theorem introduces entropy as this fundamental measure, representing the irreducible limit on how much a source can be compressed. This exercise provides direct practice in calculating the entropy, which is the theoretical minimum average number of bits required to represent each symbol from a source .",
            "id": "1657634",
            "problem": "An interstellar probe, the \"Argo,\" is designed to explore a distant star system. Its communication system transmits data packets back to Earth. Each packet is classified into one of three categories based on its scientific importance and system status: \"Nominal\" (representing routine environmental readings), \"Alert\" (indicating an unexpected but non-critical scientific find), or \"Error\" (signaling a malfunction in one of the probe's subsystems).\n\nBased on long-term operational statistics from similar missions, the probabilities of these packet types are as follows:\n- The probability of a \"Nominal\" packet is $P_N = 0.80$.\n- The probability of an \"Alert\" packet is $P_A = 0.15$.\n- The probability of an \"Error\" packet is $P_E = 0.05$.\n\nBefore transmission, these packets are encoded into a binary stream. According to the principles of information theory, there is a fundamental limit on the efficiency of any possible lossless compression scheme for this data source. Determine the theoretical minimum average number of bits required to represent one such data packet. Express your answer in bits per packet, rounded to four significant figures.",
            "solution": "The theoretical minimum average number of bits per packet for a discrete memoryless source is given by the Shannon entropy in bits,\n$$\nH = -\\sum_{i} p_{i}\\log_{2}(p_{i}).\n$$\nFor the three packet types with probabilities $P_{N} = 0.80$, $P_{A} = 0.15$, and $P_{E} = 0.05$, the entropy is\n$$\nH = -\\left(0.80\\log_{2}(0.80) + 0.15\\log_{2}(0.15) + 0.05\\log_{2}(0.05)\\right).\n$$\nEvaluating each term numerically,\n$$\n\\log_{2}(0.80) \\approx -0.3219280949,\\quad \\log_{2}(0.15) \\approx -2.736965594,\\quad \\log_{2}(0.05) \\approx -4.3219280949,\n$$\nso\n$$\n-0.80\\log_{2}(0.80) \\approx 0.2575424759,\\quad -0.15\\log_{2}(0.15) \\approx 0.4105448391,\\quad -0.05\\log_{2}(0.05) \\approx 0.2160964047.\n$$\nSumming gives\n$$\nH \\approx 0.2575424759 + 0.4105448391 + 0.2160964047 = 0.8841837198 \\text{ bits per packet}.\n$$\nRounding to four significant figures yields $0.8842$ bits per packet.",
            "answer": "$$\\boxed{0.8842}$$"
        },
        {
            "introduction": "While entropy defines the theoretical gold standard for compression, practical coding schemes often fall slightly short of this ideal limit. To measure how well a real-world algorithm performs, we use the concept of coding efficiency, which compares the average length of the implemented code against the entropy. This practice allows you to calculate this crucial metric and understand the performance gap between theory and application .",
            "id": "1657617",
            "problem": "A data science team is developing a compression algorithm for a stream of symbols generated by a discrete memoryless source. Through extensive statistical analysis, they have determined that the entropy of this source, denoted as $H(S)$, is $3.15$ bits per symbol. This value represents the fundamental lower bound on the average number of bits required to encode each symbol from this source.\n\nThe team's implementation of a new prefix-free coding scheme results in an average codeword length, $L$, of $3.24$ bits per symbol. The coding efficiency is a measure of how close an actual code's performance is to the theoretical optimum, and it is defined as the ratio of the theoretical minimum average length to the actual average length.\n\nCalculate the coding efficiency of the team's algorithm. Express your answer as a decimal rounded to four significant figures.",
            "solution": "The coding efficiency for a prefix-free code relative to a discrete memoryless source with entropy $H(S)$ and achieved average codeword length $L$ is defined as the ratio of the theoretical minimum average length to the actual average length:\n$$\\eta = \\frac{H(S)}{L}.$$\nWith $H(S) = 3.15$ bits per symbol and $L = 3.24$ bits per symbol, the efficiency is\n$$\\eta = \\frac{3.15}{3.24}.$$\nTo compute this exactly, clear decimals and simplify:\n$$\\eta = \\frac{315}{324} = \\frac{35}{36}.$$\nIts decimal expansion is\n$$\\eta = 0.972222\\ldots.$$\nRounding to four significant figures gives\n$$\\eta = 0.9722.$$",
            "answer": "$$\\boxed{0.9722}$$"
        },
        {
            "introduction": "Shannon's theorem suggests that we can approach the entropy limit more closely by encoding larger blocks of symbols at a time, rather than one by one. This powerful strategy, known as block or joint encoding, averages out the inefficiencies of assigning integer-length codewords to fractional-bit probabilities. This problem demonstrates the practical advantage of this method by comparing the efficiency of encoding symbols separately versus jointly .",
            "id": "1657629",
            "problem": "An information system processes data from two independent discrete memoryless sources, Source A and Source B.\n\nSource A produces symbols from the set {Red, Green, Blue} with the following probabilities:\n- $P(\\text{Red}) = \\frac{1}{2}$\n- $P(\\text{Green}) = \\frac{1}{4}$\n- $P(\\text{Blue}) = \\frac{1}{4}$\n\nSource B produces symbols from the set {0, 1} with the following probabilities:\n- $P(1) = \\frac{3}{4}$\n- $P(0) = \\frac{1}{4}$\n\nAn engineer is tasked with designing a binary encoding scheme to transmit pairs of symbols, one from Source A and one from Source B. Two strategies are being considered, both of which use optimal prefix codes, such as those generated by the Huffman algorithm.\n\n1.  **Separate Encoding:** An optimal prefix code is designed for Source A, and a separate optimal prefix code is designed for Source B. To encode a pair of symbols $(a, b)$, where $a$ is from Source A and $b$ is from Source B, the codeword for $a$ is concatenated with the codeword for $b$.\n2.  **Joint Encoding:** The pairs of symbols $(a, b)$ are treated as single symbols from a new, combined source. An optimal prefix code is designed for this joint source.\n\nCalculate the improvement in efficiency gained by using joint encoding instead of separate encoding. Specifically, compute the difference: (Average number of bits per original source symbol for Separate Encoding) - (Average number of bits per original source symbol for Joint Encoding).\n\nExpress your answer in units of bits per original source symbol, rounded to three significant figures.",
            "solution": "Let the sources be independent. A single transmitted object is a pair $(a,b)$ where $a$ is from Source A and $b$ is from Source B.\n\nSeparate encoding uses two optimal prefix codes applied independently and concatenates the codewords; the expected length per pair equals the sum of the individual expected lengths. Joint encoding builds a single optimal prefix code for the six pair symbols.\n\nFor Source A, with probabilities $P(\\text{Red})=\\frac{1}{2}$, $P(\\text{Green})=\\frac{1}{4}$, $P(\\text{Blue})=\\frac{1}{4}$, an optimal (Huffman) code has lengths $1,2,2$, giving expected length\n$$\nL_{A}=\\frac{1}{2}\\cdot 1+\\frac{1}{4}\\cdot 2+\\frac{1}{4}\\cdot 2=\\frac{3}{2}.\n$$\nFor Source B, with probabilities $P(1)=\\frac{3}{4}$ and $P(0)=\\frac{1}{4}$, any optimal prefix code for two symbols assigns length $1$ to each, so\n$$\nL_{B}=1.\n$$\nThus, the separate-encoding expected length per pair is\n$$\nL_{\\text{sep,pair}}=L_{A}+L_{B}=\\frac{3}{2}+1=\\frac{5}{2}.\n$$\nPer original source symbol (there are two symbols per pair), this is\n$$\nL_{\\text{sep,sym}}=\\frac{L_{\\text{sep,pair}}}{2}=\\frac{5}{4}.\n$$\n\nFor joint encoding, list the six pair probabilities using independence:\n$$\n\\begin{aligned}\nP(\\text{R},1)=\\frac{3}{8}, P(\\text{R},0)=\\frac{1}{8},\\\\\nP(\\text{G},1)=\\frac{3}{16}, P(\\text{G},0)=\\frac{1}{16},\\\\\nP(\\text{B},1)=\\frac{3}{16}, P(\\text{B},0)=\\frac{1}{16}.\n\\end{aligned}\n$$\nApply the Huffman algorithm by successively merging the two smallest probabilities:\n- Merge $\\frac{1}{16}$ and $\\frac{1}{16}$ to get $\\frac{1}{8}$.\n- Merge $\\frac{1}{8}$ (original) and $\\frac{1}{8}$ (merged) to get $\\frac{1}{4}$.\n- Merge $\\frac{3}{16}$ and $\\frac{3}{16}$ to get $\\frac{3}{8}$.\n- Merge $\\frac{1}{4}$ and $\\frac{3}{8}$ to get $\\frac{5}{8}$.\n- Merge $\\frac{3}{8}$ and $\\frac{5}{8}$ to get $1$.\n\nTracing depth increases from these merges yields code lengths:\n$$\n\\ell(\\tfrac{3}{8})=1,\\quad \\ell(\\tfrac{1}{8})=3,\\quad \\ell(\\tfrac{3}{16})=3,\\quad \\ell(\\tfrac{1}{16})=4,\n$$\nassigned respectively to the probabilities $\\frac{3}{8}$, $\\frac{1}{8}$, the two $\\frac{3}{16}$’s, and the two $\\frac{1}{16}$’s. The expected length per pair is\n$$\nL_{\\text{joint,pair}}=\\frac{3}{8}\\cdot 1+\\frac{1}{8}\\cdot 3+2\\cdot\\frac{3}{16}\\cdot 3+2\\cdot\\frac{1}{16}\\cdot 4=\\frac{19}{8}.\n$$\nPer original source symbol this is\n$$\nL_{\\text{joint,sym}}=\\frac{L_{\\text{joint,pair}}}{2}=\\frac{19}{16}.\n$$\n\nThe requested improvement (separate minus joint, per original source symbol) is\n$$\n\\Delta=L_{\\text{sep,sym}}-L_{\\text{joint,sym}}=\\frac{5}{4}-\\frac{19}{16}=\\frac{1}{16}=0.0625,\n$$\nwhich to three significant figures is $0.0625$ bits per original source symbol.",
            "answer": "$$\\boxed{0.0625}$$"
        }
    ]
}