{
    "hands_on_practices": [
        {
            "introduction": "To build our understanding from the ground up, we first consider a degenerate case: a source with no randomness. This exercise explores what rate-distortion theory tells us about compressing a signal that is constant and entirely predictable. By analyzing this simple scenario, we can clarify the fundamental link between a source's inherent uncertainty, measured by its entropy, and the resources required for its compression. ",
            "id": "1652578",
            "problem": "In the field of data compression, rate-distortion theory provides the theoretical limits for lossy compression. Consider a scenario involving a faulty environmental sensor. This sensor is supposed to monitor a stable chemical process, but it has become stuck, consistently outputting a single, constant real value, $c$. This system can be modeled as a discrete memoryless source $X$ with a source alphabet $\\mathcal{X} = \\{c\\}$ and a corresponding probability mass function $p(X=c) = 1$.\n\nWe want to compress the signal from this sensor. The quality of the compressed-and-reconstructed signal, denoted by $\\hat{X}$, is assessed using a non-negative distortion measure, $d(x, \\hat{x})$. This measure has the fundamental property that a perfect reconstruction yields zero distortion, i.e., $d(c, c) = 0$.\n\nThe rate-distortion function, $R(D)$, defines the minimum achievable data rate (in bits per symbol) for a given maximum tolerable average distortion, $D$. It is formally given by:\n$$R(D) = \\min_{p(\\hat{x}|x) : \\mathbb{E}[d(X, \\hat{X})] \\le D} I(X; \\hat{X})$$\nwhere $I(X; \\hat{X})$ is the mutual information between the source $X$ and the reconstruction $\\hat{X}$. The minimization is performed over all conditional probability distributions $p(\\hat{x}|x)$ that satisfy the distortion constraint.\n\nFor this \"stuck\" sensor, determine the rate-distortion function $R(D)$ for any non-negative distortion level $D \\ge 0$. Select the option that correctly describes this function.\n\nA. $R(D) = 0$ for all $D \\geq 0$.\n\nB. $R(D) = 1$ for $D = 0$ and $R(D) = 0$ for $D  0$.\n\nC. $R(D)$ is a strictly decreasing positive function for $D \\geq 0$.\n\nD. The function cannot be determined without knowing the specific form of the distortion measure $d(x, \\hat{x})$ for $\\hat{x} \\neq c$.",
            "solution": "We model the source as $X=c$ with $p(X=c)=1$. For any conditional distribution $p(\\hat{x}|x)$, the joint distribution is $p(x,\\hat{x})=\\mathbf{1}\\{x=c\\}\\,p(\\hat{x}|c)$ and the marginal of $\\hat{X}$ is $p(\\hat{x})=p(\\hat{x}|c)$.\n\nUsing the definition of mutual information,\n$$\nI(X;\\hat{X})=\\sum_{x,\\hat{x}} p(x,\\hat{x}) \\ln \\frac{p(x,\\hat{x})}{p(x)p(\\hat{x})}.\n$$\nSince only $x=c$ has positive probability,\n$$\nI(X;\\hat{X})=\\sum_{\\hat{x}} p(\\hat{x}|c)\\,\\ln \\frac{p(\\hat{x}|c)}{p(X=c)\\,p(\\hat{x})}\n=\\sum_{\\hat{x}} p(\\hat{x}|c)\\,\\ln \\frac{p(\\hat{x}|c)}{1\\cdot p(\\hat{x})}.\n$$\nBut $p(\\hat{x})=p(\\hat{x}|c)$, so the ratio inside the logarithm is $1$, hence $I(X;\\hat{X})=0$ for any choice of $p(\\hat{x}|x)$.\n\nThe distortion constraint is\n$$\n\\mathbb{E}[d(X,\\hat{X})]=\\mathbb{E}[d(c,\\hat{X})].\n$$\nChoosing $\\hat{X}=c$ almost surely (i.e., $p(\\hat{x}|c)=\\mathbf{1}\\{\\hat{x}=c\\}$) yields\n$$\n\\mathbb{E}[d(c,\\hat{X})]=d(c,c)=0\\le D\n$$\nfor every $D\\ge 0$, so the feasible set is nonempty for all $D\\ge 0$.\n\nThe rate-distortion function is\n$$\nR(D)=\\min_{p(\\hat{x}|x):\\,\\mathbb{E}[d(X,\\hat{X})]\\le D} I(X;\\hat{X}).\n$$\nSince $I(X;\\hat{X})=0$ for any $p(\\hat{x}|x)$ and mutual information is nonnegative, the minimum over the feasible set is exactly $0$ for all $D\\ge 0$.\n\nTherefore, the correct description is $R(D)=0$ for all $D\\ge 0$, which corresponds to option A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Having examined a source with no uncertainty, we now turn to the opposite extreme: what is the best we can do with no information transmission ($R=0$)? This practice calculates the minimum possible distortion for a Gaussian source when the decoder must use a single, constant value for every reconstruction. Understanding this zero-rate limit is crucial as it defines the maximum distortion, $D_{max}$, which corresponds to the source's own variance and serves as a fundamental benchmark for any compression system. ",
            "id": "1652592",
            "problem": "In the field of data compression, rate-distortion theory provides the fundamental limits on the trade-off between the compression rate and the fidelity of the reconstructed data.\n\nConsider a memoryless continuous source that generates independent and identically distributed random variables $X$. Each variable follows a Gaussian (normal) distribution with a mean of zero and a variance of $\\sigma^2$, denoted as $X \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nA simple compression system is designed to represent this source. The performance of this system is evaluated using the squared-error distortion measure, where the average distortion $D$ is given by the expected value of the squared difference between the original signal and its reconstruction: $D = E[(X - \\hat{X})^2]$.\n\nDue to extreme bandwidth constraints, the system is designed to operate at a rate of zero bits per source symbol. This \"zero-rate\" encoding means that no information about the specific value of any given sample $x$ can be transmitted. Consequently, the decoder must produce the same constant estimate, let's call it $\\hat{x}_c$, for every source symbol it is asked to reconstruct.\n\nTo achieve the best possible performance under this severe constraint, the constant estimate $\\hat{x}_c$ must be chosen to minimize the average distortion $D$.\n\nCalculate this minimum achievable average distortion for the zero-rate system. Express your final answer as an analytic expression in terms of the source parameter $\\sigma$.",
            "solution": "The problem asks for the minimum average distortion for a zero-rate encoder, which uses a constant reconstruction value $\\hat{x}_c$ for a source $X \\sim \\mathcal{N}(0, \\sigma^2)$. The distortion measure is the mean squared error, $D = E[(X - \\hat{x}_c)^2]$. Our goal is to find the optimal constant $\\hat{x}_c$ that minimizes this distortion, and then calculate the value of this minimum distortion.\n\nFirst, let's express the average distortion $D$ as a function of the constant estimate $\\hat{x}_c$.\n$$D(\\hat{x}_c) = E[(X - \\hat{x}_c)^2]$$\nWe can expand the squared term inside the expectation:\n$$D(\\hat{x}_c) = E[X^2 - 2X\\hat{x}_c + \\hat{x}_c^2]$$\nUsing the linearity of the expectation operator, we can write:\n$$D(\\hat{x}_c) = E[X^2] - E[2X\\hat{x}_c] + E[\\hat{x}_c^2]$$\nSince $\\hat{x}_c$ is a constant, we can pull it out of the expectations:\n$$D(\\hat{x}_c) = E[X^2] - 2\\hat{x}_c E[X] + \\hat{x}_c^2$$\nTo find the value of $\\hat{x}_c$ that minimizes $D$, we can treat $D$ as a function of $\\hat{x}_c$ and find its minimum by taking the derivative with respect to $\\hat{x}_c$ and setting it to zero.\n$$\\frac{dD(\\hat{x}_c)}{d\\hat{x}_c} = \\frac{d}{d\\hat{x}_c} (E[X^2] - 2\\hat{x}_c E[X] + \\hat{x}_c^2)$$\nThe term $E[X^2]$ is a constant with respect to $\\hat{x}_c$, so its derivative is zero.\n$$\\frac{dD(\\hat{x}_c)}{d\\hat{x}_c} = 0 - 2E[X] + 2\\hat{x}_c$$\nSetting the derivative to zero to find the minimum:\n$$-2E[X] + 2\\hat{x}_c = 0$$\n$$\\hat{x}_c = E[X]$$\nThis shows that the optimal constant estimate that minimizes the mean squared error is the mean (expected value) of the source random variable.\n\nFor the given source, $X \\sim \\mathcal{N}(0, \\sigma^2)$, the mean is $E[X] = 0$. Therefore, the optimal constant reconstruction is $\\hat{x}_c = 0$.\n\nNow we substitute this optimal value back into the distortion formula to find the minimum achievable distortion:\n$$D_{min} = E[(X - 0)^2] = E[X^2]$$\nWe need to find the value of $E[X^2]$. We can use the definition of variance:\n$$\\text{Var}(X) = E[X^2] - (E[X])^2$$\nRearranging this to solve for $E[X^2]$:\n$$E[X^2] = \\text{Var}(X) + (E[X])^2$$\nFor our source, we are given that the variance is $\\text{Var}(X) = \\sigma^2$ and the mean is $E[X] = 0$.\nSubstituting these values:\n$$E[X^2] = \\sigma^2 + 0^2 = \\sigma^2$$\nThus, the minimum achievable average distortion for the zero-rate system is $\\sigma^2$. This is the distortion obtained by always guessing the mean of the source, and it represents the inherent uncertainty or variance of the source itself. No amount of compression can result in an average distortion greater than this value, as this level of distortion can be achieved without transmitting any information.",
            "answer": "$$\\boxed{\\sigma^{2}}$$"
        },
        {
            "introduction": "Theoretical limits are essential, but how do they translate into engineering design? This problem bridges the gap by tasking you with the design of a simple yet optimal two-level quantizer for a continuous source. You will apply the principles of distortion minimization to find the ideal decision boundaries and reconstruction levels, providing a concrete example of how rate-distortion concepts are put into practice in signal processing. ",
            "id": "1652563",
            "problem": "In the design of a simple lossy compression system, a continuous information source is modeled by a random variable $X$ with a probability density function given by $p(x) = 2x$ for $x \\in [0, 1]$ and $p(x)=0$ otherwise.\n\nThe goal is to represent the output of this source using a two-level scalar quantizer. This quantizer is defined by a single decision boundary $b$, where $0  b  1$, and two reconstruction levels, $q_1$ and $q_2$. For any source output $x$, the quantizer produces a representation $\\hat{x}$ according to the following rule:\n- If $0 \\le x  b$, then $\\hat{x} = q_1$.\n- If $b \\le x \\le 1$, then $\\hat{x} = q_2$.\n\nThe quality of the compression is measured by the mean squared error (MSE) distortion, defined as $D = E[(X-\\hat{X})^2]$. Your task is to determine the values of the parameters $b$, $q_1$, and $q_2$ that minimize this distortion.\n\nCalculate the resulting minimum possible mean squared error distortion, $D_{min}$. Express your answer as an exact analytic expression.",
            "solution": "The source has density $p(x)=2x$ on $[0,1]$ and zero otherwise. A two-level scalar quantizer is defined by a boundary $b$ with reconstruction levels $q_{1}$ on $[0,b)$ and $q_{2}$ on $[b,1]$. For mean squared error minimization (Lloyd-Max conditions), the necessary conditions are:\n- Centroid conditions: $q_{1} = E[X \\mid 0 \\leq X  b]$ and $q_{2} = E[X \\mid b \\leq X \\leq 1]$.\n- Nearest-neighbor condition: the decision boundary is the midpoint, $b = \\frac{q_{1} + q_{2}}{2}$.\n\nFirst compute the region probabilities and conditional means:\n$$\nP_{1} = \\int_{0}^{b} 2x \\, dx = b^{2}, \n\\quad\nP_{2} = 1 - b^{2}.\n$$\nThen\n$$\nq_{1} = \\frac{1}{P_{1}} \\int_{0}^{b} x \\cdot 2x \\, dx \n= \\frac{1}{b^{2}} \\cdot \\frac{2 b^{3}}{3}\n= \\frac{2b}{3},\n$$\n$$\nq_{2} = \\frac{1}{P_{2}} \\int_{b}^{1} x \\cdot 2x \\, dx \n= \\frac{1}{1-b^{2}} \\cdot \\frac{2(1-b^{3})}{3}\n= \\frac{2(1-b^{3})}{3(1-b^{2})}.\n$$\nImpose the midpoint condition $b = \\frac{q_{1} + q_{2}}{2}$:\n$$\nb = \\frac{1}{2}\\left(\\frac{2b}{3} + \\frac{2(1-b^{3})}{3(1-b^{2})}\\right)\n= \\frac{1}{3}\\left(b + \\frac{1-b^{3}}{1-b^{2}}\\right).\n$$\nThis gives\n$$\n3b = b + \\frac{1-b^{3}}{1-b^{2}}\n\\;\\;\\Rightarrow\\;\\;\n2b(1-b^{2}) = 1 - b^{3}\n\\;\\;\\Rightarrow\\;\\;\n2b - 2b^{3} = 1 - b^{3}\n\\;\\;\\Rightarrow\\;\\;\nb^{3} - 2b + 1 = 0.\n$$\nFactor the cubic:\n$$\nb^{3} - 2b + 1 = (b-1)(b^{2} + b - 1).\n$$\nSince $0  b  1$, the valid root is\n$$\nb = \\frac{-1 + \\sqrt{5}}{2}.\n$$\nFrom $b^{2} + b - 1 = 0$ it follows that $b^{2} = 1 - b$, $b^{3} = 2b - 1$, and $b^{4} = 2 - 3b$. The corresponding reconstruction levels are\n$$\nq_{1} = \\frac{2b}{3} = \\frac{\\sqrt{5} - 1}{3}, \n\\quad\nq_{2} = \\frac{2(1 - b^{3})}{3(1 - b^{2})}\n= \\frac{4b}{3} = \\frac{2(\\sqrt{5} - 1)}{3}.\n$$\n\nNow compute the minimum distortion. For an MSE-optimal quantizer with centroid reconstruction values,\n$$\nD = E[(X - \\hat{X})^{2}] \n= E[X^{2}] - \\left(P_{1} q_{1}^{2} + P_{2} q_{2}^{2}\\right),\n$$\nbecause $E[X \\hat{X}] = E[\\hat{X}^{2}] = P_{1} q_{1}^{2} + P_{2} q_{2}^{2}$ when $q_{k} = E[X \\mid \\text{region } k]$. First,\n$$\nE[X^{2}] = \\int_{0}^{1} x^{2} \\cdot 2x \\, dx = 2 \\int_{0}^{1} x^{3} \\, dx = \\frac{1}{2}.\n$$\nNext,\n$$\nP_{1} q_{1}^{2} = b^{2} \\left(\\frac{2b}{3}\\right)^{2} = \\frac{4 b^{4}}{9}, \n\\quad\nP_{2} q_{2}^{2} = (1 - b^{2}) \\left(\\frac{4b}{3}\\right)^{2} = \\frac{16 b^{2}(1 - b^{2})}{9}.\n$$\nUsing $1 - b^{2} = b$ and $b^{4} = 2 - 3b$,\n$$\nP_{1} q_{1}^{2} + P_{2} q_{2}^{2} \n= \\frac{4(2 - 3b)}{9} + \\frac{16 b^{3}}{9}\n= \\frac{8 - 12b + 16(2b - 1)}{9}\n= \\frac{20b - 8}{9}.\n$$\nTherefore\n$$\nD_{\\min} = \\frac{1}{2} - \\frac{20b - 8}{9}\n= \\frac{25}{18} - \\frac{20}{9} b.\n$$\nSubstitute $b = \\frac{\\sqrt{5} - 1}{2}$:\n$$\nD_{\\min} \n= \\frac{25}{18} - \\frac{20}{9} \\cdot \\frac{\\sqrt{5} - 1}{2}\n= \\frac{25}{18} - \\frac{10}{9} \\sqrt{5} + \\frac{10}{9}\n= \\frac{5}{2} - \\frac{10}{9} \\sqrt{5}.\n$$\nThis is the exact minimal mean squared error distortion.",
            "answer": "$$\\boxed{\\frac{5}{2} - \\frac{10}{9} \\sqrt{5}}$$"
        }
    ]
}