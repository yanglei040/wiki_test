{
    "hands_on_practices": [
        {
            "introduction": "At the heart of vector quantization lies the nearest neighbor principle. This exercise grounds you in the fundamental encoding step: for any given input vector, you must find the 'closest' codevector from a predefined codebook. By doing so, you are effectively determining which Voronoi cell the vector belongs to, providing a concrete example of how VQ partitions a multi-dimensional space. ",
            "id": "1667384",
            "problem": "In a simplified model for lossy data compression, a 2-dimensional vector quantizer is used to represent input data points. The quantizer has a fixed codebook containing four representative vectors (codewords). The process involves mapping any given input vector to the closest codeword in the codebook, where \"closeness\" is determined by the standard Euclidean distance. The region of space containing all input vectors that are closer to a particular codeword than to any other is known as that codeword's Voronoi cell.\n\nThe codebook for this system is defined by the following four vectors:\n$\\mathbf{c}_1 = (-2.0, -3.0)$\n$\\mathbf{c}_2 = (4.0, 1.0)$\n$\\mathbf{c}_3 = (-1.0, 5.0)$\n$\\mathbf{c}_4 = (3.0, -4.0)$\n\nAn input data vector $\\mathbf{x} = (1.5, 2.0)$ is received. To which codeword's Voronoi cell does this input vector belong?\n\nA. $\\mathbf{c}_1 = (-2.0, -3.0)$\n\nB. $\\mathbf{c}_2 = (4.0, 1.0)$\n\nC. $\\mathbf{c}_3 = (-1.0, 5.0)$\n\nD. $\\mathbf{c}_4 = (3.0, -4.0)$",
            "solution": "We determine the nearest codeword using the Euclidean distance. For a codeword $\\mathbf{c}_{i} = (c_{i1}, c_{i2})$ and input $\\mathbf{x} = (x_{1}, x_{2})$, the Euclidean distance is\n$$\nd(\\mathbf{x}, \\mathbf{c}_{i}) = \\sqrt{(x_{1} - c_{i1})^{2} + (x_{2} - c_{i2})^{2}}.\n$$\nSince the square root is strictly increasing, we can compare squared distances:\n$$\nd^{2}(\\mathbf{x}, \\mathbf{c}_{i}) = (x_{1} - c_{i1})^{2} + (x_{2} - c_{i2})^{2}.\n$$\nCompute for each codeword with $\\mathbf{x} = (1.5, 2.0)$:\n- For $\\mathbf{c}_{1} = (-2.0, -3.0)$:\n$$\nd^{2}(\\mathbf{x}, \\mathbf{c}_{1}) = (1.5 - (-2.0))^{2} + (2.0 - (-3.0))^{2} = 3.5^{2} + 5.0^{2} = 12.25 + 25 = 37.25.\n$$\n- For $\\mathbf{c}_{2} = (4.0, 1.0)$:\n$$\nd^{2}(\\mathbf{x}, \\mathbf{c}_{2}) = (1.5 - 4.0)^{2} + (2.0 - 1.0)^{2} = (-2.5)^{2} + 1.0^{2} = 6.25 + 1 = 7.25.\n$$\n- For $\\mathbf{c}_{3} = (-1.0, 5.0)$:\n$$\nd^{2}(\\mathbf{x}, \\mathbf{c}_{3}) = (1.5 - (-1.0))^{2} + (2.0 - 5.0)^{2} = 2.5^{2} + (-3.0)^{2} = 6.25 + 9 = 15.25.\n$$\n- For $\\mathbf{c}_{4} = (3.0, -4.0)$:\n$$\nd^{2}(\\mathbf{x}, \\mathbf{c}_{4}) = (1.5 - 3.0)^{2} + (2.0 - (-4.0))^{2} = (-1.5)^{2} + 6.0^{2} = 2.25 + 36 = 38.25.\n$$\nThe smallest squared distance is $7.25$, corresponding to $\\mathbf{c}_{2} = (4.0, 1.0)$. Therefore, $\\mathbf{x}$ lies in the Voronoi cell of $\\mathbf{c}_{2}$, which is option B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "While the previous practice used a fixed codebook, a crucial question is how these optimal codebooks are designed in the first place. The Linde-Buzo-Gray (LBG) algorithm provides an iterative solution to this problem, forming the basis of many real-world clustering and quantization applications. This hands-on problem walks you through a single, complete iteration of the LBG algorithm, illustrating the two key steps of data partitioning and centroid updating. ",
            "id": "1667388",
            "problem": "In the field of data compression and clustering, the Linde-Buzo-Gray (LBG) algorithm is a classic method for designing a vector quantizer. It iteratively refines a set of representative points, called a codebook, to better represent a dataset.\n\nConsider a simplified two-dimensional dataset consisting of three points: $\\mathbf{x}_1 = (2, 2)$, $\\mathbf{x}_2 = (3, 1)$, and $\\mathbf{x}_3 = (8, 9)$. We want to quantize these points using a codebook of size two. The initial codebook consists of two centroids: $\\mathbf{c}_1 = (1, 1)$ and $\\mathbf{c}_2 = (10, 10)$.\n\nPerform one full iteration of the LBG algorithm, which consists of two steps:\n1.  **Partitioning:** Assign each data point $\\mathbf{x}_i$ to the closest centroid $\\mathbf{c}_j$ in the current codebook. The distance is measured using the squared Euclidean distance, defined as $d^2(\\mathbf{x}, \\mathbf{c}) = \\|\\mathbf{x} - \\mathbf{c}\\|^2$. This creates two disjoint sets of data points, one for each initial centroid.\n2.  **Centroid Update:** For each partition, calculate a new centroid by finding the arithmetic mean (the center of mass) of all the data points assigned to it.\n\nWhich of the following pairs represents the new centroids after this single iteration? The pair of new centroids is presented as $(\\mathbf{c}'_a, \\mathbf{c}'_b)$, ordered such that the x-coordinate of $\\mathbf{c}'_a$ is less than the x-coordinate of $\\mathbf{c}'_b$.\n\nA. $((2.5, 1.5), (8, 9))$\n\nB. $((2, 2), (5.5, 5))$\n\nC. $((3, 1), (5, 5.5))$\n\nD. $((2, 2.5), (9, 8))$\n\nE. $((5, 3), (8, 9))$",
            "solution": "The problem asks for the new set of centroids after one full iteration of the Linde-Buzo-Gray (LBG) algorithm. This iteration consists of a partitioning step followed by a centroid update step.\n\nThe initial centroids are $\\mathbf{c}_1 = (1, 1)$ and $\\mathbf{c}_2 = (10, 10)$.\nThe data points are $\\mathbf{x}_1 = (2, 2)$, $\\mathbf{x}_2 = (3, 1)$, and $\\mathbf{x}_3 = (8, 9)$.\n\n**Step 1: Partitioning**\n\nWe must assign each data point to the nearest centroid using the squared Euclidean distance, $d^2(\\mathbf{x}, \\mathbf{c}) = (x_x - c_x)^2 + (x_y - c_y)^2$. Let's create two partitions, $S_1$ and $S_2$, corresponding to the initial centroids $\\mathbf{c}_1$ and $\\mathbf{c}_2$.\n\nFor data point $\\mathbf{x}_1 = (2, 2)$:\n- Distance to $\\mathbf{c}_1$: $d^2(\\mathbf{x}_1, \\mathbf{c}_1) = (2 - 1)^2 + (2 - 1)^2 = 1^2 + 1^2 = 2$.\n- Distance to $\\mathbf{c}_2$: $d^2(\\mathbf{x}_1, \\mathbf{c}_2) = (2 - 10)^2 + (2 - 10)^2 = (-8)^2 + (-8)^2 = 64 + 64 = 128$.\nSince $2 < 128$, $\\mathbf{x}_1$ is assigned to the partition of $\\mathbf{c}_1$. So, $\\mathbf{x}_1 \\in S_1$.\n\nFor data point $\\mathbf{x}_2 = (3, 1)$:\n- Distance to $\\mathbf{c}_1$: $d^2(\\mathbf{x}_2, \\mathbf{c}_1) = (3 - 1)^2 + (1 - 1)^2 = 2^2 + 0^2 = 4$.\n- Distance to $\\mathbf{c}_2$: $d^2(\\mathbf{x}_2, \\mathbf{c}_2) = (3 - 10)^2 + (1 - 10)^2 = (-7)^2 + (-9)^2 = 49 + 81 = 130$.\nSince $4 < 130$, $\\mathbf{x}_2$ is also assigned to the partition of $\\mathbf{c}_1$. So, $\\mathbf{x}_2 \\in S_1$.\n\nFor data point $\\mathbf{x}_3 = (8, 9)$:\n- Distance to $\\mathbf{c}_1$: $d^2(\\mathbf{x}_3, \\mathbf{c}_1) = (8 - 1)^2 + (9 - 1)^2 = 7^2 + 8^2 = 49 + 64 = 113$.\n- Distance to $\\mathbf{c}_2$: $d^2(\\mathbf{x}_3, \\mathbf{c}_2) = (8 - 10)^2 + (9 - 10)^2 = (-2)^2 + (-1)^2 = 4 + 1 = 5$.\nSince $5 < 113$, $\\mathbf{x}_3$ is assigned to the partition of $\\mathbf{c}_2$. So, $\\mathbf{x}_3 \\in S_2$.\n\nAfter the partitioning step, the two partitions are:\n$S_1 = \\{\\mathbf{x}_1, \\mathbf{x}_2\\} = \\{(2, 2), (3, 1)\\}$\n$S_2 = \\{\\mathbf{x}_3\\} = \\{(8, 9)\\}$\n\n**Step 2: Centroid Update**\n\nNow, we compute the new centroids, $\\mathbf{c}'_1$ and $\\mathbf{c}'_2$, by finding the arithmetic mean of the points in each partition.\n\nFor partition $S_1$:\nThe new centroid $\\mathbf{c}'_1$ is the average of the points in $S_1$.\n$$ \\mathbf{c}'_1 = \\frac{\\mathbf{x}_1 + \\mathbf{x}_2}{2} = \\frac{(2, 2) + (3, 1)}{2} = \\frac{(2+3, 2+1)}{2} = \\frac{(5, 3)}{2} = (2.5, 1.5) $$\n\nFor partition $S_2$:\nThe new centroid $\\mathbf{c}'_2$ is the average of the points in $S_2$. Since there is only one point, the centroid is the point itself.\n$$ \\mathbf{c}'_2 = \\frac{\\mathbf{x}_3}{1} = (8, 9) $$\n\nThe new centroids are $(2.5, 1.5)$ and $(8, 9)$.\n\nThe problem asks for the pair of new centroids $(\\mathbf{c}'_a, \\mathbf{c}'_b)$ ordered by their x-coordinate. The x-coordinate of $(2.5, 1.5)$ is $2.5$, and the x-coordinate of $(8, 9)$ is $8$. Since $2.5 < 8$, the ordered pair is $((2.5, 1.5), (8, 9))$.\n\nComparing this result with the given options:\nA. $((2.5, 1.5), (8, 9))$ - This matches our result.\nB. $((2, 2), (5.5, 5))$\nC. $((3, 1), (5, 5.5))$\nD. $((2, 2.5), (9, 8))$\nE. $((5, 3), (8, 9))$\n\nTherefore, the correct option is A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The theoretical power of vector quantization comes from its ability to exploit correlations between vector components, leading to a more efficient representation than quantizing each component separately. This efficiency gain is often described in terms of a 'shape advantage,' where VQ uses more optimal cell shapes (like hexagons) to tile the space compared to the simple squares of scalar quantization. This problem challenges you to quantify this advantage by directly comparing the distortion performance of VQ against scalar quantization, revealing the very reason VQ is a superior technique for many sources. ",
            "id": "1667378",
            "problem": "A two-dimensional information source generates random vectors $\\mathbf{X} = (X_1, X_2)$ that are uniformly distributed over a large square region in the plane, defined by $S = [-L/2, L/2] \\times [-L/2, L/2]$. To transmit this information, the source must be quantized. We wish to compare two quantization strategies, both using a total of $N$ reconstruction levels, where $N$ is large.\n\nThe performance of a quantizer is measured by its average Mean Squared Error (MSE), which represents the expected squared Euclidean distance between a source vector and its corresponding reconstruction vector. In the high-resolution limit (large $N$), the MSE for a quantizer that partitions the source region into $N$ cells of identical shape and area $V$ can be approximated by the formula $D \\approx C \\cdot V$. In this formula, $C$ is a dimensionless constant that depends only on the geometric shape of the quantization cells.\n\n**Strategy 1: Scalar Quantization (SQ)**\nThe components $X_1$ and $X_2$ are quantized independently. We use $\\sqrt{N}$ uniformly spaced levels for each component, which partitions the source region $S$ into a grid of $N$ identical square-shaped quantization cells. The shape constant for a square cell is given as $C_{sq} = \\frac{1}{6}$.\n\n**Strategy 2: Vector Quantization (VQ)**\nThe vector $\\mathbf{X}$ is quantized directly. The source region $S$ is partitioned into $N$ identical cells shaped as regular hexagons, which provide a more efficient way to tile a two-dimensional plane. The shape constant for a regular hexagonal cell is given as $C_{hex} = \\frac{5}{18\\sqrt{3}}$.\n\nAssuming that $N$ is large enough for the high-resolution approximation to be valid, calculate the ratio of the expected distortion of the Vector Quantizer to the expected distortion of the Scalar Quantizer, $\\frac{D_{VQ}}{D_{SQ}}$. Express your final answer as a single closed-form analytic expression.",
            "solution": "In high-resolution quantization for two-dimensional squared-error distortion with identical cells of area $V$, the average distortion is well approximated by $D \\approx C \\cdot V$, where $C$ depends only on the cell shape.\n\nFor a uniform source over the square region $S = [-L/2, L/2] \\times [-L/2, L/2]$ with total area $|S| = L^{2}$, partitioned into $N$ identical cells, the area per cell is\n$$\nV = \\frac{|S|}{N} = \\frac{L^{2}}{N}.\n$$\nThis $V$ is the same for both strategies since both use $N$ cells.\n\nUsing the given shape constants, the distortions are\n$$\nD_{SQ} \\approx C_{sq} \\cdot V = \\frac{1}{6} \\cdot \\frac{L^{2}}{N},\n\\qquad\nD_{VQ} \\approx C_{hex} \\cdot V = \\frac{5}{18\\sqrt{3}} \\cdot \\frac{L^{2}}{N}.\n$$\nTherefore, the ratio of distortions is independent of $L$ and $N$:\n$$\n\\frac{D_{VQ}}{D_{SQ}} = \\frac{C_{hex}}{C_{sq}} = \\frac{\\frac{5}{18\\sqrt{3}}}{\\frac{1}{6}} = \\frac{30}{18\\sqrt{3}} = \\frac{5}{3\\sqrt{3}} = \\frac{5\\sqrt{3}}{9}.\n$$\nEither form is an equivalent closed-form expression.",
            "answer": "$$\\boxed{\\frac{5\\sqrt{3}}{9}}$$"
        }
    ]
}