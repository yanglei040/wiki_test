{
    "hands_on_practices": [
        {
            "introduction": "One of the most powerful and fundamental properties of the rate-distortion function is its convexity. This exercise  provides a hands-on way to understand this property through the practical concept of time-sharing, where two different compression schemes are combined. By analyzing this hybrid system, you will see how we can achieve any rate-distortion pair along the straight line connecting the performance points of the individual schemes, a direct consequence of this convexity.",
            "id": "1650279",
            "problem": "A digital communication system is designed to compress and transmit data from a memoryless source. The performance of any compression scheme for this source is characterized by a rate-distortion pair $(R, D)$, where $R$ is the average number of bits used to represent each source symbol, and $D$ is the resulting average distortion between the original and reconstructed data.\n\nTwo existing compression algorithms, Algorithm A and Algorithm B, are available for this source.\n- Algorithm A operates at a rate $R_A$ and achieves an average distortion $D_A$.\n- Algorithm B operates at a rate $R_B$ and achieves an average distortion $D_B$.\n\nA new hybrid system is created that processes a long sequence of source symbols. This system employs Algorithm A for a fraction $\\alpha$ of the symbols and Algorithm B for the remaining fraction $(1-\\alpha)$ of the symbols, where $0 \\le \\alpha \\le 1$. The system switches between algorithms on large, independent blocks of data, so any overhead from the switching can be considered negligible.\n\nDetermine the effective rate, $R_{eff}$, and the effective average distortion, $D_{eff}$, of this hybrid system. Your answer should be a row matrix containing the effective rate and the effective distortion, in that order, expressed in terms of $R_A, D_A, R_B, D_B,$ and $\\alpha$.",
            "solution": "Let a long block contain $N$ source symbols. By design, Algorithm A is used on a fraction $\\alpha$ of the symbols and Algorithm B on the remaining fraction $(1-\\alpha)$, with switching overhead negligible.\n\nRate derivation:\n- The total number of bits used by Algorithm A is $(\\alpha N) R_{A}$, and by Algorithm B is $\\bigl((1-\\alpha) N\\bigr) R_{B}$.\n- The total number of bits over $N$ symbols is therefore\n$$\n(\\alpha N) R_{A} + \\bigl((1-\\alpha) N\\bigr) R_{B}.\n$$\n- The effective rate, defined as average bits per source symbol, is this total divided by $N$:\n$$\nR_{eff} = \\frac{(\\alpha N) R_{A} + \\bigl((1-\\alpha) N\\bigr) R_{B}}{N} = \\alpha R_{A} + (1-\\alpha) R_{B}.\n$$\nEquivalently, viewing the per-symbol rate as a random variable determined by which algorithm is used and applying linearity of expectation (law of total expectation), one has $R_{eff} = \\alpha R_{A} + (1-\\alpha) R_{B}$.\n\nDistortion derivation:\n- Let $D_{A}$ and $D_{B}$ be the average per-symbol distortions achieved by Algorithms A and B, respectively, under the given source and distortion measure.\n- Over the $\\alpha N$ symbols encoded by A, the aggregate distortion is $(\\alpha N) D_{A}$; over the $(1-\\alpha) N$ symbols encoded by B, it is $\\bigl((1-\\alpha) N\\bigr) D_{B}$.\n- The total distortion over $N$ symbols is\n$$\n(\\alpha N) D_{A} + \\bigl((1-\\alpha) N\\bigr) D_{B},\n$$\nso the effective average distortion per symbol is\n$$\nD_{eff} = \\frac{(\\alpha N) D_{A} + \\bigl((1-\\alpha) N\\bigr) D_{B}}{N} = \\alpha D_{A} + (1-\\alpha) D_{B}.\n$$\nThis also follows from the law of total expectation by conditioning on which algorithm is used for a given symbol.\n\nThus, the hybrid system’s effective rate and distortion are the convex combinations of the individual algorithms’ rate and distortion, weighted by $\\alpha$ and $(1-\\alpha)$.\n\nThe required row matrix, listing the effective rate first and effective distortion second, is $\\begin{pmatrix} R_{eff} & D_{eff} \\end{pmatrix}$ with\n$$\nR_{eff} = \\alpha R_{A} + (1-\\alpha) R_{B}, \\quad D_{eff} = \\alpha D_{A} + (1-\\alpha) D_{B}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\alpha R_{A}+(1-\\alpha) R_{B} & \\alpha D_{A}+(1-\\alpha) D_{B}\\end{pmatrix}}$$"
        },
        {
            "introduction": "What is the best we can do with no information at all? This question explores the scenario of a zero-rate code, corresponding to the minimum average distortion achievable, often denoted $D_{max}$, which occurs when the rate is zero. This exercise  challenges you to find the single optimal reproduction value for a source when the communication channel is off, using a non-standard, asymmetric distortion measure to highlight how our definition of 'error' fundamentally shapes the optimal strategy.",
            "id": "1650343",
            "problem": "In the design of a simple scalar quantizer, a source $X$ is modeled as a continuous random variable uniformly distributed on the interval $[-1, 1]$. For a zero-rate code, all possible outputs of the source are mapped to a single, constant reproduction value, $\\hat{x}$.\n\nThe quality of the quantization is measured by a distortion function, $d(x, \\hat{x})$. For this particular application, an asymmetric squared-error distortion measure is used, which penalizes overestimation of the source value more severely than underestimation. This distortion is defined as:\n$$\nd(x, \\hat{x}) =\n\\begin{cases}\n4 (x - \\hat{x})^2 & \\text{if } x  \\hat{x} \\\\\n(x - \\hat{x})^2  \\text{if } x \\ge \\hat{x}\n\\end{cases}\n$$\nYour task is to find the optimal reproduction point, $\\hat{x}$, that minimizes the average distortion for this source and distortion measure.\n\nExpress your answer as a single closed-form analytic expression.",
            "solution": "Let the constant reproduction value be denoted by $\\hat{x} \\in \\mathbb{R}$. The source $X$ is uniformly distributed on $[-1,1]$, so its density is $f_{X}(x) = \\frac{1}{2}$ for $x \\in [-1,1]$ and zero otherwise. The average distortion as a function of $\\hat{x}$ is\n$$\nD(\\hat{x}) = \\mathbb{E}\\!\\left[d(X,\\hat{x})\\right] = \\int_{-1}^{1} d(x,\\hat{x}) \\cdot \\frac{1}{2} \\, dx.\n$$\nUsing the given asymmetric squared-error distortion,\n$$\nd(x,\\hat{x}) =\n\\begin{cases}\n4(x-\\hat{x})^{2},  x\\hat{x},\\\\\n(x-\\hat{x})^{2},  x \\ge \\hat{x},\n\\end{cases}\n$$\nwe split the integral at $x=\\hat{x}$, but must account for where $\\hat{x}$ lies relative to $[-1,1]$. Consider three cases.\n\nCase 1: $\\hat{x} \\le -1$. Then $x \\ge \\hat{x}$ for all $x \\in [-1,1]$, so\n$$\nD(\\hat{x}) = \\frac{1}{2} \\int_{-1}^{1} (x-\\hat{x})^{2} \\, dx.\n$$\nCompute\n$$\n\\int_{-1}^{1} (x-\\hat{x})^{2} \\, dx = \\left[\\frac{x^{3}}{3} - \\hat{x} x^{2} + \\hat{x}^{2} x\\right]_{-1}^{1} = \\frac{2}{3} + 2 \\hat{x}^{2},\n$$\nhence\n$$\nD(\\hat{x}) = \\hat{x}^{2} + \\frac{1}{3}.\n$$\nThis is minimized (over $\\hat{x} \\le -1$) at the boundary $\\hat{x}=-1$, giving $D(-1)=\\frac{4}{3}$.\n\nCase 2: $-1 \\le \\hat{x} \\le 1$. Then\n$$\nD(\\hat{x}) = \\frac{1}{2} \\left[ \\int_{-1}^{\\hat{x}} 4(x-\\hat{x})^{2} \\, dx + \\int_{\\hat{x}}^{1} (x-\\hat{x})^{2} \\, dx \\right].\n$$\nEvaluate the two integrals. First,\n$$\n\\int_{-1}^{\\hat{x}} (x-\\hat{x})^{2} \\, dx = \\left[\\frac{x^{3}}{3} - \\hat{x} x^{2} + \\hat{x}^{2} x\\right]_{-1}^{\\hat{x}} = \\frac{\\hat{x}^{3}}{3} + \\frac{1}{3} + \\hat{x} + \\hat{x}^{2}.\n$$\nSecond,\n$$\n\\int_{\\hat{x}}^{1} (x-\\hat{x})^{2} \\, dx = \\left[\\frac{x^{3}}{3} - \\hat{x} x^{2} + \\hat{x}^{2} x\\right]_{\\hat{x}}^{1} = \\frac{1}{3} - \\hat{x} + \\hat{x}^{2} - \\frac{\\hat{x}^{3}}{3}.\n$$\nTherefore,\n$$\nD(\\hat{x}) = \\frac{1}{2} \\left[ 4\\!\\left(\\frac{\\hat{x}^{3}}{3} + \\frac{1}{3} + \\hat{x} + \\hat{x}^{2}\\right) + \\left(\\frac{1}{3} - \\hat{x} + \\hat{x}^{2} - \\frac{\\hat{x}^{3}}{3}\\right) \\right]\n= \\frac{1}{2} \\left( \\hat{x}^{3} + 5 \\hat{x}^{2} + 3 \\hat{x} + \\frac{5}{3} \\right).\n$$\nDifferentiate and set to zero:\n$$\nD'(\\hat{x}) = \\frac{3}{2} \\hat{x}^{2} + 5 \\hat{x} + \\frac{3}{2} = 0 \\quad \\Longleftrightarrow \\quad 3 \\hat{x}^{2} + 10 \\hat{x} + 3 = 0.\n$$\nSolve the quadratic:\n$$\n\\hat{x} = \\frac{-10 \\pm \\sqrt{100 - 36}}{6} = \\frac{-10 \\pm 8}{6} \\in \\left\\{ -\\frac{1}{3}, -3 \\right\\}.\n$$\nOnly $\\hat{x} = -\\frac{1}{3}$ lies in $[-1,1]$. The second derivative is $D''(\\hat{x}) = 3 \\hat{x} + 5$, which on $[-1,1]$ satisfies $D''(\\hat{x}) \\ge 2 > 0$, so $D$ is strictly convex there and $\\hat{x}=-\\frac{1}{3}$ is the unique minimizer in this case.\n\nCase 3: $\\hat{x} \\ge 1$. Then $x  \\hat{x}$ for all $x \\in [-1,1]$, so\n$$\nD(\\hat{x}) = \\frac{1}{2} \\int_{-1}^{1} 4(x-\\hat{x})^{2} \\, dx = 2 \\left(\\frac{2}{3} + 2 \\hat{x}^{2}\\right) = \\frac{4}{3} + 4 \\hat{x}^{2},\n$$\nwhich is minimized (over $\\hat{x} \\ge 1$) at the boundary $\\hat{x}=1$, giving $D(1)=\\frac{4}{3} + 4$.\n\nComparing across cases, the minimal value is achieved at $\\hat{x} = -\\frac{1}{3}$ from Case 2. Thus the optimal reproduction point that minimizes the average distortion is $\\hat{x} = -\\frac{1}{3}$.",
            "answer": "$$\\boxed{-\\frac{1}{3}}$$"
        },
        {
            "introduction": "The Gaussian source is a cornerstone model in information theory due to its prevalence in nature and its analytical tractability. This practice problem  guides you through the derivation of the rate-distortion function for a two-dimensional Gaussian source with a mean-squared error distortion measure. This classic derivation illustrates how to handle multi-dimensional sources and provides one of the most celebrated results in rate-distortion theory, cementing the relationship between rate, variance, and distortion.",
            "id": "1652536",
            "problem": "A two-dimensional memoryless source is defined by a random vector $X = (X_1, X_2)$, where the components $X_1$ and $X_2$ are independent and identically distributed (i.i.d.) random variables. Each component follows a Gaussian distribution with a mean of zero and a variance of $\\sigma^2$, denoted as $\\mathcal{N}(0, \\sigma^2)$.\n\nThis source is to be compressed, and the quality of the reconstruction $\\hat{X} = (\\hat{X}_1, \\hat{X}_2)$ is evaluated using an average mean-squared error distortion measure, defined as:\n$$D = \\frac{1}{2} E\\left[ \\|X - \\hat{X}\\|^2 \\right] = \\frac{1}{2} E\\left[ (X_1 - \\hat{X}_1)^2 + (X_2 - \\hat{X}_2)^2 \\right]$$\n\nDetermine the rate-distortion function $R(D)$ for this two-dimensional source. The function $R(D)$ represents the minimum number of bits per source vector required to achieve an average distortion less than or equal to $D$. Provide the expression for $R(D)$ for distortions in the range $0 \\le D \\le \\sigma^2$. Your answer should be a symbolic expression in terms of $\\sigma^2$ and $D$.",
            "solution": "The source vector is $X=(X_{1},X_{2})$ with independent components $X_{i}\\sim\\mathcal{N}(0,\\sigma^{2})$. The distortion measure is the average per component mean-squared error,\n$$\nD=\\frac{1}{2}E\\!\\left[(X_{1}-\\hat{X}_{1})^{2}+(X_{2}-\\hat{X}_{2})^{2}\\right].\n$$\nFor a scalar Gaussian source $X\\sim\\mathcal{N}(0,\\sigma^{2})$ with mean-squared error constraint $E[(X-\\hat{X})^{2}]\\leq d$, the rate-distortion function in bits is\n$$\nr(d)=\\frac{1}{2}\\log_{2}\\!\\left(\\frac{\\sigma^{2}}{d}\\right),\\quad 0d\\leq\\sigma^{2},\n$$\nand $r(d)=0$ for $d\\geq\\sigma^{2}$. This is the classical Gaussian RDF, achieved by a Gaussian test channel and derivable via the Shannon lower bound.\n\nFor the two-dimensional memoryless source with independent components and an average per-component distortion constraint $D$, the vector rate-distortion function is obtained by reverse water-filling across the independent Gaussian modes. Let the component distortions be $d_{1}$ and $d_{2}$, with the constraint\n$$\n\\frac{1}{2}(d_{1}+d_{2})\\leq D,\\quad 0\\leq d_{i}\\leq\\sigma^{2}.\n$$\nBecause the components are i.i.d. and the scalar RDF $r(d)$ is convex and strictly decreasing in $d$ on $(0,\\sigma^{2}]$, the optimal allocation equalizes the distortions: $d_{1}=d_{2}=D$ for $0\\leq D\\leq\\sigma^{2}$. This also follows from the KKT conditions or, equivalently, from the reverse water-filling solution with water level $\\theta$ chosen so that\n$$\n\\frac{1}{2}\\sum_{i=1}^{2}\\min\\{\\lambda_{i},\\theta\\}=D,\n$$\nwith $\\lambda_{1}=\\lambda_{2}=\\sigma^{2}$. For $0\\leq D\\leq\\sigma^{2}$, one has $\\theta=D\\leq\\sigma^{2}$ and hence $d_{i}=\\theta=D$ for both components.\n\nThe total rate in bits per source vector is the sum of the scalar rates:\n$$\nR(D)=r(d_{1})+r(d_{2})=\\frac{1}{2}\\log_{2}\\!\\left(\\frac{\\sigma^{2}}{D}\\right)+\\frac{1}{2}\\log_{2}\\!\\left(\\frac{\\sigma^{2}}{D}\\right)\n=\\log_{2}\\!\\left(\\frac{\\sigma^{2}}{D}\\right),\n$$\nvalid for $0D\\leq\\sigma^{2}$, with $R(\\sigma^{2})=0$ and $R(D)\\to\\infty$ as $D\\to 0^{+}$.",
            "answer": "$$\\boxed{\\log_{2}\\!\\left(\\frac{\\sigma^{2}}{D}\\right)}$$"
        }
    ]
}