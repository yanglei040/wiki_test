{
    "hands_on_practices": [
        {
            "introduction": "Many physical processes, from thermal noise in circuits to signal fluctuations in wireless channels, are effectively modeled using Gaussian distributions. This first exercise provides practice with this fundamental case by calculating the mutual information between a single noise source and the total combined noise. Mastering this problem  builds a foundation for understanding information flow in linear systems and serves as a crucial stepping stone to more complex models.",
            "id": "1642064",
            "problem": "In a simplified model of a passive electronic circuit, two identical resistors at the same temperature produce independent thermal noise voltages, represented by the random variables $V_1$ and $V_2$. Both $V_1$ and $V_2$ are modeled as independent and identically distributed (i.i.d.) Gaussian random variables, each with a mean $\\mu$ and a variance $\\sigma^2$. An instrument measures the total voltage across these two sources in series, giving a reading $V_{\\text{total}} = V_1 + V_2$.\n\nCalculate the mutual information between the noise voltage of the first resistor, $V_1$, and the total measured voltage, $V_{\\text{total}}$. Express your answer in nats as a closed-form analytic expression.",
            "solution": "Let $V_{1}$ and $V_{2}$ be i.i.d. Gaussian random variables with mean $\\mu$ and variance $\\sigma^{2}$, and let $V_{\\text{total}}=V_{1}+V_{2}$. The mutual information between $V_{1}$ and $V_{\\text{total}}$ is\n$$\nI(V_{1};V_{\\text{total}})=h(V_{\\text{total}})-h(V_{\\text{total}}\\mid V_{1}),\n$$\nwhere $h(\\cdot)$ denotes differential entropy.\n\nFirst, compute the variance of $V_{\\text{total}}$. By independence,\n$$\n\\operatorname{Var}(V_{\\text{total}})=\\operatorname{Var}(V_{1}+V_{2})=\\operatorname{Var}(V_{1})+\\operatorname{Var}(V_{2})=2\\sigma^{2}.\n$$\nSince $V_{\\text{total}}$ is Gaussian with variance $2\\sigma^{2}$, its entropy is\n$$\nh(V_{\\text{total}})=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\operatorname{Var}(V_{\\text{total}})\\big)=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\cdot 2\\sigma^{2}\\big).\n$$\n\nNext, determine $h(V_{\\text{total}}\\mid V_{1})$. Given $V_{1}=v_{1}$, we have\n$$\nV_{\\text{total}}\\mid V_{1}=v_{1}=v_{1}+V_{2},\n$$\nwhich is Gaussian with mean $v_{1}+\\mu$ and variance $\\operatorname{Var}(V_{2})=\\sigma^{2}$. The entropy of a Gaussian depends only on its variance, hence\n$$\nh(V_{\\text{total}}\\mid V_{1})=h(V_{2})=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma^{2}\\big).\n$$\n\nTherefore, the mutual information is\n$$\nI(V_{1};V_{\\text{total}})=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\cdot 2\\sigma^{2}\\big)-\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma^{2}\\big)=\\frac{1}{2}\\ln(2).\n$$\nThis result is in nats and is independent of the mean $\\mu$.",
            "answer": "$$\\boxed{\\frac{1}{2}\\ln(2)}$$"
        },
        {
            "introduction": "While Gaussian models are powerful, real-world signals and noise can follow other distributions. This practice moves beyond the Gaussian assumption to explore a channel with uniformly distributed inputs and noise, a scenario relevant to quantization or certain digital modulation schemes. Solving this problem  requires applying first principles, including the convolution of probability densities and direct integration to compute entropy, thereby strengthening your mathematical toolkit for information theory.",
            "id": "1642099",
            "problem": "A communication channel is modeled as follows. The input signal is a continuous random variable $X$ that is uniformly distributed over the interval $[-a, a]$, where $a > 0$. During transmission, the signal is corrupted by additive noise, represented by another continuous random variable $Z$, which is independent of $X$ and is uniformly distributed over the interval $[-b, b]$, where $b > 0$. The received signal is thus $Y = X + Z$.\n\nAssuming that the parameters satisfy the condition $a > b$, determine the mutual information $I(X;Y)$ between the input signal $X$ and the received signal $Y$. Express your answer as a symbolic expression in terms of $a$ and $b$.",
            "solution": "We aim to compute the mutual information between $X$ and $Y=X+Z$ when $X \\sim \\text{Unif}([-a,a])$, $Z \\sim \\text{Unif}([-b,b])$, independent, with $a>b>0$. Using the definition of mutual information for continuous variables in an additive noise channel, we have\n$$\nI(X;Y) = h(Y) - h(Y|X) = h(Y) - h(Z),\n$$\nsince $Y|X=x$ has the same distribution as $Z$.\n\nFirst, we determine the probability density function of $Y$. The densities of $X$ and $Z$ are\n$$\nf_{X}(x) = \\frac{1}{2a} \\mathbf{1}_{\\{|x|\\le a\\}}, \\quad f_{Z}(z) = \\frac{1}{2b} \\mathbf{1}_{\\{|z|\\le b\\}}.\n$$\nThe density of $Y=X+Z$ is the convolution\n$$\nf_{Y}(y) = \\int_{-\\infty}^{\\infty} f_{X}(y-z) f_{Z}(z) \\, dz = \\frac{1}{4ab} \\cdot \\text{length}\\left( [y-a,y+a] \\cap [-b,b] \\right).\n$$\nFor $a>b$, this yields a trapezoidal density:\n- For $|y| \\le a-b$, the intersection length is $2b$, hence $f_{Y}(y) = \\frac{1}{2a}$.\n- For $a-b \\le |y| \\le a+b$, the intersection length is $a+b-|y|$, hence $f_{Y}(y) = \\frac{a+b-|y|}{4ab}$.\n- For $|y| > a+b$, $f_{Y}(y)=0$.\n\nNext, compute the differential entropy of $Y$:\n$$\nh(Y) = -\\int_{-\\infty}^{\\infty} f_{Y}(y) \\ln f_{Y}(y) \\, dy.\n$$\nBy symmetry,\n$$\nh(Y) = -2 \\left[ \\int_{0}^{a-b} \\frac{1}{2a} \\ln\\!\\left(\\frac{1}{2a}\\right) dy + \\int_{a-b}^{a+b} \\frac{a+b-y}{4ab} \\ln\\!\\left( \\frac{a+b-y}{4ab} \\right) dy \\right].\n$$\nThe first integral evaluates to\n$$\n-2 \\int_{0}^{a-b} \\frac{1}{2a} \\ln\\!\\left(\\frac{1}{2a}\\right) dy = - \\frac{a-b}{a} \\ln\\!\\left(\\frac{1}{2a}\\right) = \\left(\\frac{a-b}{a}\\right) \\ln(2a).\n$$\nFor the second integral, substitute $t=a+b-y$, so $y$ runs from $a-b$ to $a+b$ as $t$ runs from $2b$ to $0$. Then\n$$\n-2 \\int_{a-b}^{a+b} \\frac{a+b-y}{4ab} \\ln\\!\\left( \\frac{a+b-y}{4ab} \\right) dy\n= -2 \\int_{0}^{2b} \\frac{t}{4ab} \\ln\\!\\left( \\frac{t}{4ab} \\right) dt.\n$$\nLet\n$$\nJ = \\int_{0}^{2b} t \\ln\\!\\left( \\frac{t}{4ab} \\right) dt = \\int_{0}^{2b} t \\ln t \\, dt - \\ln(4ab) \\int_{0}^{2b} t \\, dt.\n$$\nUsing $\\int t \\ln t \\, dt = \\frac{t^{2}}{2} \\left( \\ln t - \\frac{1}{2} \\right)$ and $\\int t \\, dt = \\frac{t^{2}}{2}$, we get\n$$\nJ = \\left. \\frac{t^{2}}{2} \\left( \\ln t - \\frac{1}{2} \\right) \\right|_{0}^{2b} - \\ln(4ab) \\left. \\frac{t^{2}}{2} \\right|_{0}^{2b}\n= 2b^{2} \\left( \\ln(2b) - \\frac{1}{2} - \\ln(4ab) \\right)\n= -2 b^{2} \\ln(2a) - b^{2}.\n$$\nHence\n$$\n-2 \\int_{a-b}^{a+b} \\frac{a+b-y}{4ab} \\ln\\!\\left( \\frac{a+b-y}{4ab} \\right) dy\n= -\\frac{1}{2ab} J\n= \\frac{b}{a} \\ln(2a) + \\frac{b}{2a}.\n$$\nCombining both parts,\n$$\nh(Y) = \\left(\\frac{a-b}{a}\\right) \\ln(2a) + \\left( \\frac{b}{a} \\ln(2a) + \\frac{b}{2a} \\right)\n= \\ln(2a) + \\frac{b}{2a}.\n$$\n\nThe noise $Z$ is uniform on $[-b,b]$, so its differential entropy is\n$$\nh(Z) = \\ln(2b).\n$$\nTherefore the mutual information is\n$$\nI(X;Y) = h(Y) - h(Z) = \\left[ \\ln(2a) + \\frac{b}{2a} \\right] - \\ln(2b) = \\ln\\!\\left( \\frac{a}{b} \\right) + \\frac{b}{2a}.\n$$\nThis expression is in nats and is valid for $a>b>0$.",
            "answer": "$$\\boxed{\\ln\\left(\\frac{a}{b}\\right)+\\frac{b}{2a}}$$"
        },
        {
            "introduction": "This final practice explores the subtle and often counter-intuitive concept of conditional mutual information. We investigate how observing a shared variable can reveal information about the relationship between two otherwise independent sources. This phenomenon , where conditioning can induce correlation, is fundamental to understanding information processing in networks, statistical inference, and multi-user communication theory.",
            "id": "1642045",
            "problem": "Let $X_1$, $X_2$, and $Z$ be three independent and identically distributed (i.i.d.) random variables, each following a standard normal distribution, i.e., $X_1, X_2, Z \\sim N(0, 1)$. A fourth random variable $Y$ is constructed from their sum, defined as $Y = X_1 + X_2 + Z$.\n\nYour task is to compute the conditional mutual information between $X_1$ and $X_2$ given $Y$, which is denoted by $I(X_1; X_2 | Y)$. The result should be expressed as an analytic expression involving the natural logarithm (`\\ln`).",
            "solution": "Let $X_{1}, X_{2}, Z$ be independent with $X_{1}, X_{2}, Z \\sim \\mathcal{N}(0,1)$ and define $Y = X_{1} + X_{2} + Z$. The vector $(X_{1}, X_{2}, Y)$ is jointly Gaussian. We compute the covariance matrix entries:\n$$\n\\operatorname{Var}(X_{1}) = 1,\\quad \\operatorname{Var}(X_{2}) = 1,\\quad \\operatorname{Cov}(X_{1},X_{2}) = 0,\n$$\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}(X_{1}) + \\operatorname{Var}(X_{2}) + \\operatorname{Var}(Z) = 3,\n$$\n$$\n\\operatorname{Cov}(X_{1},Y) = \\operatorname{Cov}(X_{1}, X_{1} + X_{2} + Z) = 1,\\quad \\operatorname{Cov}(X_{2},Y) = 1.\n$$\nThus,\n$$\n\\Sigma_{(X_{1},X_{2},Y)} =\n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n1 & 1 & 3\n\\end{pmatrix}.\n$$\nFor jointly Gaussian variables, the conditional covariance of $(X_{1},X_{2})$ given $Y$ is given by the Schur complement:\n$$\n\\Sigma_{(X_{1},X_{2})|Y} = \\Sigma_{(X_{1},X_{2})} - \\Sigma_{(X_{1},X_{2}),Y}\\,\\Sigma_{Y}^{-1}\\,\\Sigma_{Y,(X_{1},X_{2})}.\n$$\nHere $\\Sigma_{(X_{1},X_{2})} = I_{2}$, $\\Sigma_{(X_{1},X_{2}),Y} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$, and $\\Sigma_{Y} = 3$, so\n$$\n\\Sigma_{(X_{1},X_{2})|Y} = I_{2} - \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}\\frac{1}{3}\\begin{pmatrix}1 & 1\\end{pmatrix}\n= \\begin{pmatrix} \\frac{2}{3} & -\\frac{1}{3} \\\\ -\\frac{1}{3} & \\frac{2}{3} \\end{pmatrix}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(X_{1}\\mid Y) = \\frac{2}{3},\\quad \\operatorname{Var}(X_{2}\\mid Y) = \\frac{2}{3},\\quad \\det\\big(\\Sigma_{(X_{1},X_{2})|Y}\\big) = \\frac{1}{3}.\n$$\nFor jointly Gaussian scalars, the conditional mutual information satisfies\n$$\nI(X_{1};X_{2}\\mid Y) = \\frac{1}{2}\\ln\\!\\left(\\frac{\\operatorname{Var}(X_{1}\\mid Y)\\,\\operatorname{Var}(X_{2}\\mid Y)}{\\det\\big(\\Sigma_{(X_{1},X_{2})|Y}\\big)}\\right).\n$$\nSubstituting the computed quantities yields\n$$\nI(X_{1};X_{2}\\mid Y) = \\frac{1}{2}\\ln\\!\\left(\\frac{\\left(\\frac{2}{3}\\right)\\left(\\frac{2}{3}\\right)}{\\frac{1}{3}}\\right)\n= \\frac{1}{2}\\ln\\!\\left(\\frac{4}{3}\\right).\n$$\nEquivalently, using the conditional correlation $\\rho_{12\\mid Y} = -\\frac{1}{2}$, one can verify $I(X_{1};X_{2}\\mid Y) = -\\frac{1}{2}\\ln(1-\\rho_{12\\mid Y}^{2}) = \\frac{1}{2}\\ln\\!\\left(\\frac{4}{3}\\right)$.",
            "answer": "$$\\boxed{\\frac{1}{2}\\ln\\!\\left(\\frac{4}{3}\\right)}$$"
        }
    ]
}