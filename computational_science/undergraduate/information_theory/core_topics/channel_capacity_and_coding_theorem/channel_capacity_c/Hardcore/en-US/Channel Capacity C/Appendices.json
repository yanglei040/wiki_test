{
    "hands_on_practices": [
        {
            "introduction": "We begin our exploration of channel capacity with a foundational scenario: a deterministic channel. In such channels, the output is a direct, unambiguous function of the input, with no noise or randomness to complicate matters. This exercise  simplifies the communication process to its essence, allowing us to see clearly how capacity is determined by the number of distinguishable outcomes we can produce at the output. By analyzing a channel that maps decimal digits to their parity, we practice the core skill of maximizing the output entropy, $H(Y)$, to find the channel's upper limit for information transmission.",
            "id": "1609635",
            "problem": "A specialized digital communication system is designed to transmit information about a single decimal digit. The input to this channel is a random variable $X$ that can take any integer value from the set $\\mathcal{X} = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\}$. The channel deterministically maps the input digit $X$ to a binary output $Y \\in \\{0, 1\\}$. The mapping rule is as follows: if the input digit $X$ is an even number, the output is $Y=0$; if the input digit $X$ is an odd number, the output is $Y=1$. Calculate the capacity of this communication channel. Express your final answer in bits.",
            "solution": "The channel deterministically maps each input digit $X \\in \\{0,1,\\dots,9\\}$ to its parity: $Y=0$ for even $X$ and $Y=1$ for odd $X$. The capacity of a discrete memoryless channel is defined as\n$$\nC=\\max_{p(x)} I(X;Y).\n$$\nFor a deterministic channel $Y=f(X)$, the conditional entropy satisfies $H(Y|X)=0$, hence\n$$\nI(X;Y)=H(Y)-H(Y|X)=H(Y).\n$$\nTherefore,\n$$\nC=\\max_{p(x)} H(Y).\n$$\nLet $p \\triangleq \\Pr(Y=1)$ denote the total input probability mass assigned to the odd digits. Because we can choose the input distribution arbitrarily over the ten digits, any $p \\in [0,1]$ is achievable by allocating the total probability mass across the five odd and five even digits accordingly. Thus $Y$ is a Bernoulli random variable with parameter $p$, and\n$$\nH(Y)=-p \\log_{2}(p) - (1-p) \\log_{2}(1-p).\n$$\nTo maximize $H(Y)$ over $p \\in [0,1]$, we can recognize that the binary entropy function is maximized at $p=1/2$. Alternatively, we can differentiate:\n$$\n\\frac{d}{dp}H(Y)=\\log_{2}\\!\\left(\\frac{1-p}{p}\\right).\n$$\nSetting the derivative to zero gives $\\log_{2}\\!\\left(\\frac{1-p}{p}\\right)=0$, hence $\\frac{1-p}{p}=1$ and $p=\\frac{1}{2}$. The second derivative is\n$$\n\\frac{d^{2}}{dp^{2}}H(Y)=-\\frac{1}{\\ln(2)}\\left(\\frac{1}{p}+\\frac{1}{1-p}\\right)0 \\quad \\text{for } p \\in (0,1),\n$$\nso $p=\\frac{1}{2}$ is the maximizer. Evaluating at $p=\\frac{1}{2}$,\n$$\nH(Y)=-\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)-\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)=1.\n$$\nThus the channel capacity is $1$ bit.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Building on the concept of a deterministic channel, this next practice introduces a slightly more complex mapping where multiple distinct inputs converge to the same output. This many-to-one relationship is common in real-world data processing, where information is often summarized or compressed. This exercise  challenges us to not only identify the theoretical maximum output entropy but also to prove that it is achievable by constructing a valid input probability distribution. Itâ€™s a crucial step that reinforces the understanding that a channel's capacity depends on what we can practically achieve at the output, not just what is theoretically possible.",
            "id": "1609640",
            "problem": "Consider a discrete memoryless communication channel designed to transmit information by summing binary inputs. The input to the channel is a pair of bits, denoted by the random variable $X = (X_1, X_2)$, where $X_1 \\in \\{0, 1\\}$ and $X_2 \\in \\{0, 1\\}$. The channel's operation is deterministic: it computes the integer sum of the two input bits to produce a single output, denoted by the random variable $Y = X_1 + X_2$.\n\nCalculate the capacity of this channel. All information-theoretic quantities should be calculated using base-2 logarithms. Express your final answer in bits, rounded to four significant figures.",
            "solution": "The capacity $C$ of a discrete memoryless channel is defined as the maximum mutual information between the input $X$ and the output $Y$ over all possible input distributions $p(x)$.\n$$C = \\max_{p(x)} I(X;Y)$$\nThe mutual information $I(X;Y)$ is given by the formula $I(X;Y) = H(Y) - H(Y|X)$, where $H(Y)$ is the entropy of the output and $H(Y|X)$ is the conditional entropy of the output given the input.\n\nFirst, let's characterize the channel.\nThe input alphabet for $X = (X_1, X_2)$ is $\\mathcal{X} = \\{(0,0), (0,1), (1,0), (1,1)\\}$.\nThe output $Y = X_1 + X_2$ can take values $0+0=0$, $0+1=1$, $1+0=1$, and $1+1=2$. So, the output alphabet is $\\mathcal{Y} = \\{0, 1, 2\\}$.\n\nThe channel is deterministic, meaning that for each input $x \\in \\mathcal{X}$, the output $y \\in \\mathcal{Y}$ is uniquely determined.\n- If $X = (0,0)$, then $Y=0$.\n- If $X = (0,1)$, then $Y=1$.\n- If $X = (1,0)$, then $Y=1$.\n- If $X = (1,1)$, then $Y=2$.\n\nLet's calculate the conditional entropy $H(Y|X)$.\n$$H(Y|X) = \\sum_{x \\in \\mathcal{X}} p(x) H(Y|X=x)$$\nThe entropy of the output conditioned on a specific input $x$ is $H(Y|X=x) = -\\sum_{y \\in \\mathcal{Y}} p(y|x) \\log_2 p(y|x)$. Since the channel is deterministic, for any given $x$, there is one specific $y$ for which the transition probability $p(y|x)$ is 1, and for all other $y'$, $p(y'|x)=0$. For example, $p(y=0|x=(0,0))=1$.\nThe entropy term for such a deterministic transition is $-(1 \\log_2 1 + 0 \\log_2 0 + \\dots) = 0$. (Note: $0 \\log_2 0$ is defined as 0).\nSince $H(Y|X=x) = 0$ for all $x \\in \\mathcal{X}$, the conditional entropy is $H(Y|X) = \\sum_{x \\in \\mathcal{X}} p(x) \\cdot 0 = 0$.\n\nWith $H(Y|X)=0$, the mutual information simplifies to $I(X;Y) = H(Y) - 0 = H(Y)$.\nTherefore, the channel capacity is the maximum possible entropy of the output:\n$$C = \\max_{p(x)} H(Y)$$\n\nThe entropy of the output is given by $H(Y) = -\\sum_{y \\in \\mathcal{Y}} p(y) \\log_2 p(y)$. Let the probabilities of the output symbols be $q_0 = P(Y=0)$, $q_1 = P(Y=1)$, and $q_2 = P(Y=2)$.\nThen $H(Y) = -q_0 \\log_2 q_0 - q_1 \\log_2 q_1 - q_2 \\log_2 q_2$.\n\nThe entropy of a random variable with $|\\mathcal{Y}| = 3$ possible outcomes is maximized when the probability distribution is uniform, i.e., $q_0 = q_1 = q_2 = 1/3$. The maximum possible value for $H(Y)$ is therefore $H_{\\text{max}}(Y) = \\log_2(3)$.\n\nNow, we must check if this uniform output distribution is achievable for some valid input distribution $p(x)$. Let the input probabilities be:\n- $p_{(0,0)} = P(X=(0,0))$\n- $p_{(0,1)} = P(X=(0,1))$\n- $p_{(1,0)} = P(X=(1,0))$\n- $p_{(1,1)} = P(X=(1,1))$\nwhere $p_{(0,0)} + p_{(0,1)} + p_{(1,0)} + p_{(1,1)} = 1$ and all probabilities are non-negative.\n\nThe output probabilities are related to the input probabilities as follows:\n- $q_0 = P(Y=0) = P(X=(0,0)) = p_{(0,0)}$\n- $q_1 = P(Y=1) = P(X=(0,1)) + P(X=(1,0)) = p_{(0,1)} + p_{(1,0)}$\n- $q_2 = P(Y=2) = P(X=(1,1)) = p_{(1,1)}$\n\nTo achieve the uniform output distribution, we need:\n- $q_0 = p_{(0,0)} = 1/3$\n- $q_1 = p_{(0,1)} + p_{(1,0)} = 1/3$\n- $q_2 = p_{(1,1)} = 1/3$\n\nWe can check if a valid set of input probabilities exists. Let's set $p_{(0,0)} = 1/3$ and $p_{(1,1)} = 1/3$. For the second condition, we can choose any non-negative $p_{(0,1)}$ and $p_{(1,0)}$ that sum to $1/3$. For instance, we can choose $p_{(0,1)} = 1/6$ and $p_{(1,0)} = 1/6$.\nThis gives a valid input distribution: $p(x) = \\{p_{(0,0)}=1/3, p_{(0,1)}=1/6, p_{(1,0)}=1/6, p_{(1,1)}=1/3\\}$. The sum is $1/3 + 1/6 + 1/6 + 1/3 = 2/6 + 1/6 + 1/6 + 2/6 = 6/6 = 1$. Since a valid input distribution exists that produces the uniform output distribution, the maximum output entropy is achievable.\n\nThus, the channel capacity is $C = \\log_2(3)$.\nTo find the numerical value:\n$$C = \\log_2(3) = \\frac{\\ln(3)}{\\ln(2)} \\approx \\frac{1.09861228867}{0.69314718056} \\approx 1.5849625007 \\text{ bits}$$\nRounding to four significant figures, we get $C \\approx 1.585$ bits.",
            "answer": "$$\\boxed{1.585}$$"
        },
        {
            "introduction": "Now we venture from deterministic channels into the realm of noisy communication, but with a fascinating twist: what if the receiver knows exactly what noise corrupted the signal? This hypothetical scenario, explored in this problem , illuminates the profound concept of side information. By analyzing a binary channel where the noise is perfectly known to the receiver, we can see how mutual information properly accounts for all knowledge at the receiver's disposal. This practice reveals the powerful and somewhat counterintuitive idea that if noise can be perfectly characterized, its effects can be completely nullified, making the channel behave as if it were noiseless.",
            "id": "1609620",
            "problem": "Consider a binary communication channel where the input alphabet is $\\mathcal{X} = \\{0, 1\\}$ and the output alphabet is $\\mathcal{Y} = \\{0, 1\\}$. The channel is affected by additive binary noise. Let the input selected by the transmitter be the random variable $X$, and the noise be the random variable $Z$. Both $X$ and $Z$ can take values in $\\{0, 1\\}$. The channel output, $Y$, is given by the relation $Y = X \\oplus Z$, where $\\oplus$ denotes addition modulo 2 (the XOR operation).\n\nThe noise variable $Z$ is statistically independent of the input $X$ and follows a Bernoulli distribution with $P(Z=1) = q$, where $q$ is a constant such that $0  q  1$.\n\nA special feature of this communication setup is that the receiver has perfect side information about the noise. This means that for each use of the channel, the receiver observes not only the output $Y$ but also the exact value of the noise realization $Z$. The transmitter, however, has no knowledge of $Z$.\n\nCalculate the capacity $C$ of this channel with side information at the receiver. Express your answer in units of bits per channel use.",
            "solution": "The capacity of a channel is defined as the maximum mutual information between the input and the output, maximized over all possible input distributions. In this specific problem, the receiver observes both the channel output $Y$ and the noise value $Z$. Therefore, the total information available to the receiver is the pair of random variables $(Y, Z)$. The channel capacity, $C$, is thus given by the maximum mutual information between the input $X$ and the pair $(Y, Z)$.\n\n$$C = \\max_{p(x)} I(X; Y, Z)$$\n\nWe can expand the mutual information term $I(X; Y, Z)$ using the chain rule for mutual information:\n\n$$I(X; Y, Z) = I(X; Z) + I(X; Y | Z)$$\n\nThe problem statement specifies that the input $X$ and the noise $Z$ are statistically independent. The mutual information between two independent random variables is zero. Therefore:\n\n$$I(X; Z) = 0$$\n\nSubstituting this into our expression for mutual information, we get:\n\n$$I(X; Y, Z) = 0 + I(X; Y | Z) = I(X; Y | Z)$$\n\nNow let's analyze the conditional mutual information term, $I(X; Y | Z)$. We can express this in terms of conditional entropies:\n\n$$I(X; Y | Z) = H(X | Z) - H(X | Y, Z)$$\n\nBecause $X$ and $Z$ are independent, knowing the value of $Z$ provides no information about $X$. Thus, the conditional entropy $H(X | Z)$ is equal to the entropy of $X$:\n\n$$H(X | Z) = H(X)$$\n\nNext, we evaluate the term $H(X | Y, Z)$. This represents the uncertainty remaining about the input $X$ after the receiver has observed both the output $Y$ and the noise $Z$. The channel relationship is given as $Y = X \\oplus Z$. Using the properties of the XOR operation, we can solve for $X$:\n\n$$X = Y \\oplus Z$$\n\nSince the receiver knows the exact values of both $Y$ and $Z$ for each transmission, it can compute $X$ with perfect certainty. There is no uncertainty remaining about $X$ once $Y$ and $Z$ are known. Therefore, the conditional entropy $H(X | Y, Z)$ is zero:\n\n$$H(X | Y, Z) = 0$$\n\nSubstituting these entropy results back into the expression for mutual information:\n\n$$I(X; Y, Z) = I(X; Y | Z) = H(X|Z) - H(X|Y,Z) = H(X) - 0 = H(X)$$\n\nSo, the mutual information for any given input distribution $p(x)$ is simply the entropy of the input, $H(X)$. To find the channel capacity $C$, we must maximize this mutual information over all possible input distributions.\n\n$$C = \\max_{p(x)} H(X)$$\n\nThe input $X$ is a binary random variable. The entropy of a binary random variable is maximized when its outcomes are equiprobable, i.e., $P(X=0) = P(X=1) = 1/2$. The maximum value of the entropy is:\n\n$$H_{\\text{max}}(X) = -P(X=0)\\log_2(P(X=0)) - P(X=1)\\log_2(P(X=1))$$\n$$H_{\\text{max}}(X) = -\\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) - \\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) = - (1)\\log_2\\left(\\frac{1}{2}\\right) = -(-\\log_2(2)) = \\log_2(2) = 1 \\text{ bit}$$\n\nTherefore, the capacity of this channel is 1 bit per channel use. It is noteworthy that the capacity is independent of the noise probability $q$, a direct consequence of the receiver having perfect side information about the noise.",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}