{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of Fano's inequality, we begin with the most fundamental scenario: a Binary Symmetric Channel (BSC). This first exercise will guide you through calculating both the theoretical lower bound on the probability of error and the actual error rate of an optimal decoder. You will discover a special case where the theoretical limit is perfectly met, providing a concrete intuition for how conditional entropy, $H(X|Y)$, constrains the reliability of a communication system .",
            "id": "1638528",
            "problem": "A simple digital memory system stores a single bit, $X$, which is written as a 0 or a 1 with equal probability. Due to thermal noise, the bit can flip over time. This process is modeled as a Binary Symmetric Channel (BSC). After a certain period, the bit is read as a value $Y$. The probability of a bit flip (i.e., reading a 0 when a 1 was stored, or reading a 1 when a 0 was stored) is the crossover probability, $\\epsilon = 0.1$.\n\nAn engineer wants to understand the limits of reliability for this memory system. You are asked to perform two calculations:\n\n1.  First, determine the minimum probability of error, $P_e$, that can be achieved. This is accomplished by using an optimal decision rule to determine an estimate, $\\hat{X}$, of the original bit based on the observed bit, $Y$.\n2.  Second, calculate the theoretical lower bound on this probability of error, denoted as $P_{\\text{bound}}$. This fundamental limit is derived from Fano's inequality. For a binary source like this one, the inequality establishes a relationship between the conditional entropy of the input given the output, $H(X|Y)$, and the binary entropy of the error probability, $H_b(P_e)$. The binary entropy function is given by $H_b(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$.\n\nCalculate the numerical values for both $P_e$ and $P_{\\text{bound}}$. Present your final answer as a pair of numbers $(P_e, P_{\\text{bound}})$ in a single row matrix, with each value rounded to three significant figures.",
            "solution": "The problem asks for two quantities: the actual minimum probability of error $P_e$ for an optimal decoder, and the theoretical lower bound $P_{\\text{bound}}$ from Fano's inequality. The system is a Binary Symmetric Channel (BSC) with input $X \\in \\{0, 1\\}$, output $Y \\in \\{0, 1\\}$, uniform input distribution $P(X=0) = P(X=1) = 0.5$, and crossover probability $\\epsilon = 0.1$.\n\n**Part 1: Calculation of the minimum probability of error, $P_e$.**\n\nThe optimal decision rule that minimizes the probability of error $P(\\hat{X} \\neq X)$ is the Maximum A Posteriori (MAP) decoder. The MAP rule chooses the input $\\hat{x}$ that maximizes the posterior probability $P(X=x|Y=y)$ for a given observation $y$.\n$$ \\hat{x} = \\arg\\max_{x \\in \\{0,1\\}} P(X=x|Y=y) $$\nUsing Bayes' theorem, the posterior is $P(X=x|Y=y) = \\frac{P(Y=y|X=x) P(X=x)}{P(Y=y)}$.\nSince the input distribution $P(X=x)$ is uniform ($0.5$ for all $x$), and $P(Y=y)$ is a constant for a given $y$, maximizing the posterior probability is equivalent to maximizing the likelihood $P(Y=y|X=x)$. This decision rule is called the Maximum Likelihood (ML) decoder.\n\nThe channel transition probabilities are:\n$P(Y=0|X=0) = 1-\\epsilon = 0.9$\n$P(Y=1|X=0) = \\epsilon = 0.1$\n$P(Y=0|X=1) = \\epsilon = 0.1$\n$P(Y=1|X=1) = 1-\\epsilon = 0.9$\n\nLet's determine the decision rule:\n- If we observe $Y=0$: We compare $P(Y=0|X=0) = 0.9$ and $P(Y=0|X=1) = 0.1$. Since $0.9 > 0.1$, the optimal decision is $\\hat{X}=0$.\n- If we observe $Y=1$: We compare $P(Y=1|X=0) = 0.1$ and $P(Y=1|X=1) = 0.9$. Since $0.9 > 0.1$, the optimal decision is $\\hat{X}=1$.\n\nThe optimal decision rule is simply $\\hat{X} = Y$. An error occurs if $\\hat{X} \\neq X$, which for this rule means an error occurs if $Y \\neq X$. This happens when the bit flips.\n\nThe probability of error, $P_e$, is the total probability of a bit flip:\n$$ P_e = P(Y \\neq X) = P(Y \\neq X | X=0)P(X=0) + P(Y \\neq X | X=1)P(X=1) $$\n$$ P_e = P(Y=1|X=0)P(X=0) + P(Y=0|X=1)P(X=1) $$\nSubstituting the given values:\n$$ P_e = (\\epsilon)(0.5) + (\\epsilon)(0.5) = \\epsilon $$\n$$ P_e = 0.1 $$\n\n**Part 2: Calculation of the Fano lower bound, $P_{\\text{bound}}$.**\n\nFano's inequality for a binary source provides a lower bound on the probability of error, $P_e$, via the relation:\n$$ H(X|Y) \\le H_b(P_e) $$\nThis can be rearranged to give $H_b(P_e) \\ge H(X|Y)$. We need to calculate the conditional entropy $H(X|Y)$. A convenient formula is $H(X|Y) = H(X) - I(X;Y)$, where $I(X;Y)$ is the mutual information.\n\nFirst, let's calculate the entropy of the source, $H(X)$. Since the input is uniform:\n$$ H(X) = -\\sum_{x \\in \\{0,1\\}} P(X=x) \\log_2(P(X=x)) = - (0.5 \\log_2(0.5) + 0.5 \\log_2(0.5)) = - \\log_2(0.5) = 1 \\text{ bit} $$\n\nNext, we calculate the mutual information $I(X;Y) = H(Y) - H(Y|X)$.\nLet's find the conditional entropy $H(Y|X)$:\n$$ H(Y|X) = \\sum_{x \\in \\{0,1\\}} P(X=x) H(Y|X=x) $$\nFor a BSC, the entropy of the output conditioned on a specific input is the binary entropy of the crossover probability:\n$H(Y|X=0) = H(Y|X=1) = H_b(\\epsilon)$.\nTherefore, $H(Y|X) = (0.5) H_b(\\epsilon) + (0.5) H_b(\\epsilon) = H_b(\\epsilon)$.\n\nNow we find the output probabilty distribution $P(Y)$ to calculate $H(Y)$.\n$P(Y=0) = P(Y=0|X=0)P(X=0) + P(Y=0|X=1)P(X=1) = (1-\\epsilon)(0.5) + (\\epsilon)(0.5) = 0.5$.\n$P(Y=1) = P(Y=1|X=0)P(X=0) + P(Y=1|X=1)P(X=1) = (\\epsilon)(0.5) + (1-\\epsilon)(0.5) = 0.5$.\nThe output distribution is also uniform, so its entropy is $H(Y)=1$ bit.\n\nThe mutual information is $I(X;Y) = H(Y) - H(Y|X) = 1 - H_b(\\epsilon)$.\nNow we find the conditional entropy required for Fano's inequality:\n$$ H(X|Y) = H(X) - I(X;Y) = 1 - (1 - H_b(\\epsilon)) = H_b(\\epsilon) $$\n\nSubstituting this into the Fano inequality relation $H_b(P_e) \\ge H(X|Y)$:\n$$ H_b(P_e) \\ge H_b(\\epsilon) $$\nThe binary entropy function $H_b(p)$ is symmetric around $p=0.5$ and is strictly increasing for $p \\in [0, 0.5]$. Since the error probability $\\epsilon=0.1$ is in this range, and we expect any reasonable decoder's error probability $P_e$ to also be in this range, we can conclude from the inequality that:\n$$ P_e \\ge \\epsilon $$\nThis means the minimum possible value for $P_e$ is $\\epsilon$. Thus, the lower bound $P_{\\text{bound}}$ is $\\epsilon$.\n$$ P_{\\text{bound}} = \\epsilon = 0.1 $$\n\n**Conclusion and Formatting**\n\nWe have calculated both the actual minimum error probability for this channel and the Fano lower bound.\n$P_e = 0.1$\n$P_{\\text{bound}} = 0.1$\n\nRounding to three significant figures gives $0.100$ for both values. The final answer is the pair $(0.100, 0.100)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.100 & 0.100 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Having established the basic principle, we now move to a more practical application involving a simple error-correcting code. In this problem, you will evaluate the performance of a specific, non-optimal \"majority rule\" decoder and compare its actual error rate to the ultimate theoretical limit set by Fano's inequality. This practice demonstrates the power of the Fano bound as a benchmark for assessing the efficiency of real-world engineering solutions .",
            "id": "1638462",
            "problem": "Consider a simple communication system designed to transmit a binary message. The source is a random variable $X$ with alphabet $\\mathcal{X}=\\{0, 1\\}$ and a uniform probability distribution, i.e., $P(X=0) = P(X=1) = 1/2$. To protect against errors, a $(3,1)$ repetition code is used, where the message $X=0$ is encoded into the codeword $C=000$ and $X=1$ is encoded into $C=111$.\n\nThe encoded codeword is transmitted over a Binary Symmetric Channel (BSC) with a crossover probability $p=0.1$. This means each transmitted bit is flipped (from 0 to 1, or 1 to 0) with probability $p$, and transmitted correctly with probability $1-p$, independently of all other bits. The received 3-bit word is denoted by $Y$.\n\nAt the receiver, a majority rule decoder is employed. This decoder estimates the original message as $\\hat{X}=1$ if the received word $Y$ contains two or more 1s; otherwise, it estimates $\\hat{X}=0$.\n\nYour task is to compare the actual performance of this specific decoder to the theoretical limit imposed by Fano's inequality. You will calculate the ratio $R = P_{e,maj} / P_{e,Fano}$, where $P_{e,maj}$ is the probability of error for the majority decoder, and $P_{e,Fano}$ is the information-theoretic lower bound on the probability of any decoder error, as determined by Fano's inequality.\n\nFano's inequality for a binary source implies that for any decoder, the probability of error $P_e$ is bounded by the relation $H(P_e) \\ge H(X|Y)$, where $H(p) = -p\\log_2(p) - (1-p)\\log_2(1-p)$ is the binary entropy function. The lower bound, $P_{e,Fano}$, is the value that satisfies this relationship with equality, i.e., $H(P_{e,Fano}) = H(X|Y)$.\n\nTo find $P_{e,Fano}$ from the value you calculate for $H(X|Y)$, use linear interpolation with the following table of the binary entropy function:\n| $p$      | $H(p)$   |\n|----------|----------|\n| $0.0190$ | $0.1353$ |\n| $0.0200$ | $0.1414$ |\n\nCalculate the ratio $P_{e,maj} / P_{e,Fano}$ and round your final answer to three significant figures.",
            "solution": "The source is binary uniform, $P(X=0)=P(X=1)=\\frac{1}{2}$. A $(3,1)$ repetition code maps $0 \\mapsto 000$ and $1 \\mapsto 111$. The channel is a BSC with crossover probability $p=0.1$, and each bit flips independently. The majority decoder decides $\\hat{X}=1$ if at least two received bits are $1$, and $\\hat{X}=0$ otherwise.\n\nFirst, compute the majority-decoder error probability. Conditioned on a given $X$, an error occurs if at least two of the three transmitted bits flip. For a BSC with flip probability $p$, this gives\n$$\nP_{e,\\mathrm{maj}}=P(\\mathrm{Bin}(3,p)\\ge 2)=\\binom{3}{2}p^{2}(1-p)+p^{3}=3p^{2}(1-p)+p^{3}.\n$$\nWith $p=0.1$, this yields\n$$\nP_{e,\\mathrm{maj}}=3\\cdot 0.1^{2}\\cdot 0.9+0.1^{3}=0.027+0.001=0.028.\n$$\n\nNext, compute $H(X|Y)$, where $Y$ is the received 3-bit word. For a received word $y$ of Hamming weight $w$, by Bayes’ rule with the uniform prior and channel symmetry,\n$$\nP(X=0\\mid Y=y)=\\frac{p^{w}(1-p)^{3-w}}{p^{w}(1-p)^{3-w}+p^{3-w}(1-p)^{w}},\\quad\nP(X=1\\mid Y=y)=\\frac{p^{3-2w}}{(1-p)^{3-2w}+p^{3-2w}}.\n$$\nThis depends only on $w=|y|$, and there are $\\binom{3}{w}$ words of weight $w$. The marginal of a specific $y$ of weight $w$ is\n$$\nP(Y=y)=\\frac{1}{2}\\left[p^{w}(1-p)^{3-w}+p^{3-w}(1-p)^{w}\\right].\n$$\nTherefore,\n$$\nH(X|Y)=\\sum_{w=0}^{3}\\binom{3}{w}\\left[\\frac{1}{2}\\left(p^{w}(1-p)^{3-w}+p^{3-w}(1-p)^{w}\\right)\\right]H\\!\\left(P(X=1\\mid |Y|=w)\\right),\n$$\nwith $H(\\cdot)$ the binary entropy function $H(u)=-u\\log_{2}u-(1-u)\\log_{2}(1-u)$.\n\nFor $p=0.1$, the posteriors simplify:\n- $w=0$: $P(X=1\\mid |Y|=0)=\\dfrac{p^{3}}{(1-p)^{3}+p^{3}}=\\dfrac{0.001}{0.730}=\\dfrac{1}{730}$, and $P(Y=000)=\\dfrac{1}{2}\\left[(1-p)^{3}+p^{3}\\right]=0.365$.\n- $w=3$: $P(X=1\\mid |Y|=3)=\\dfrac{(1-p)^{3}}{(1-p)^{3}+p^{3}}=1-\\dfrac{1}{730}$, and $P(Y=111)=0.365$.\n- $w=1$: $P(X=1\\mid |Y|=1)=\\dfrac{p}{(1-p)+p}=p=0.1$, with $3$ such words each of probability $\\dfrac{1}{2}\\left[p(1-p)^{2}+p^{2}(1-p)\\right]=0.045$, totaling $0.135$.\n- $w=2$: $P(X=1\\mid |Y|=2)=1-p=0.9$, with total probability also $0.135$.\n\nBy symmetry $H\\!\\left(\\dfrac{1}{730}\\right)=H\\!\\left(1-\\dfrac{1}{730}\\right)$ and $H(0.1)=H(0.9)$, so\n$$\nH(X|Y)=0.73\\,H\\!\\left(\\frac{1}{730}\\right)+0.27\\,H(0.1).\n$$\nNumerically, $H(0.1)=-0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9)\\approx 0.4689955936$, and\n$$\nH\\!\\left(\\frac{1}{730}\\right)=-\\frac{1}{730}\\log_{2}\\!\\left(\\frac{1}{730}\\right)-\\frac{729}{730}\\log_{2}\\!\\left(\\frac{729}{730}\\right)\\approx 0.015002763.\n$$\nThus\n$$\nH(X|Y)\\approx 0.73\\cdot 0.015002763+0.27\\cdot 0.4689955936\\approx 0.137580827.\n$$\nFano’s inequality for a binary source states $H(P_{e})\\ge H(X|Y)$. The lower bound $P_{e,\\mathrm{Fano}}$ is defined by equality $H(P_{e,\\mathrm{Fano}})=H(X|Y)$. Using the provided table and linear interpolation between\n$$\n(p,H(p))=(0.0190,\\,0.1353)\\quad\\text{and}\\quad(0.0200,\\,0.1414),\n$$\nwe set\n$$\nP_{e,\\mathrm{Fano}}=0.0190+\\frac{H(X|Y)-0.1353}{0.1414-0.1353}\\times(0.0200-0.0190)\n=0.0190+\\frac{0.137580827-0.1353}{0.0061}\\times 0.001,\n$$\nwhich yields\n$$\nP_{e,\\mathrm{Fano}}\\approx 0.019373905.\n$$\nFinally, the requested ratio is\n$$\nR=\\frac{P_{e,\\mathrm{maj}}}{P_{e,\\mathrm{Fano}}}=\\frac{0.028}{0.019373905}\\approx 1.44524,\n$$\nwhich, rounded to three significant figures, is $1.45$.",
            "answer": "$$\\boxed{1.45}$$"
        },
        {
            "introduction": "Our final practice challenges you to generalize the core concept beyond the simple probability of error. Many real-world systems care not just whether an error occurred, but also about the *type* of error, assigning different costs to different mistakes. This exercise requires you to adapt the logic of Fano's inequality to derive a lower bound on the minimum expected cost, demonstrating the versatility of information-theoretic bounds in analyzing complex systems with nuanced performance metrics .",
            "id": "1638524",
            "problem": "An advanced autonomous system is designed to classify signals into one of $M$ distinct categories. The true category of a signal is represented by a discrete random variable $X$, which takes values from the set $\\mathcal{X} = \\{1, 2, 3, 4, 5\\}$. The system processes a sensor observation $Y$, which is probabilistically related to $X$, to produce an estimate $\\hat{X}=g(Y)$ for some estimation function $g$.\n\nThe performance of this classification system is evaluated based on a specific cost structure. A correct classification ($ \\hat{X} = X $) incurs zero cost. However, a misclassification incurs a positive cost given by a cost matrix $C$, where the element $C_{ij}$ is the cost incurred when the true category is $i$ and the system estimates it as $j$. The cost function is defined as:\n$$\nC_{ij} =\n\\begin{cases}\n    0 & \\text{if } i = j \\\\\n    i + j & \\text{if } i \\neq j\n\\end{cases}\n$$\nfor all $i, j \\in \\mathcal{X}$.\n\nLet $C_{avg}$ denote the expected cost, $E[C_{X,\\hat{X}}]$, for an optimal estimator that minimizes this cost. It is known that the residual uncertainty of the true category given the observation is $H(X|Y) = \\ln(4)$, where $\\ln$ denotes the natural logarithm.\n\nBased on this information, determine the tightest possible lower bound on the average cost $C_{avg}$. Express your answer as an exact value.",
            "solution": "Let $P_{e} = P(\\hat{X} \\neq X)$ denote the error probability of the given estimator $g$.\n\nBy the cost definition, $C_{ii} = 0$ and for $i \\neq j$, $C_{ij} = i + j \\geq 3$ because the smallest off-diagonal sum is $1 + 2 = 3$. Therefore, conditioning on the error event $E = \\{\\hat{X} \\neq X\\}$,\n$$\nC_{avg} = E[C_{X,\\hat{X}}] \\geq 3 P(E) = 3 P_{e}.\n$$\n\nNext, apply Fano’s inequality (with natural logarithms) for $M = 5$ classes:\n$$\nH(X|Y) \\leq h(P_{e}) + P_{e} \\ln(5 - 1) = h(P_{e}) + P_{e} \\ln 4,\n$$\nwhere $h(p) = -p \\ln p - (1 - p) \\ln(1 - p)$ is the binary entropy in nats. Using the given $H(X|Y) = \\ln 4$, we obtain\n$$\n\\ln 4 \\leq h(P_{e}) + P_{e} \\ln 4.\n$$\nSince $h(P_{e}) \\leq \\ln 2$, it follows that\n$$\n\\ln 4 \\leq \\ln 2 + P_{e} \\ln 4 \\quad \\Rightarrow \\quad P_{e} \\geq \\frac{\\ln 4 - \\ln 2}{\\ln 4} = \\frac{1}{2}.\n$$\n\nCombining the two inequalities,\n$$\nC_{avg} \\geq 3 P_{e} \\geq 3 \\cdot \\frac{1}{2} = \\frac{3}{2}.\n$$\n\nThus, the tightest possible lower bound on the average cost consistent with the given information is $\\frac{3}{2}$.",
            "answer": "$$\\boxed{\\frac{3}{2}}$$"
        }
    ]
}