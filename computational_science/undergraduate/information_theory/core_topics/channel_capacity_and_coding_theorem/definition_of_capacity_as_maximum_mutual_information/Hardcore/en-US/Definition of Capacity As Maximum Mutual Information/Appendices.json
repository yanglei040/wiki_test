{
    "hands_on_practices": [
        {
            "introduction": "Calculating channel capacity is an exercise in optimization. It requires us to find the ideal input probability distribution that maximizes the mutual information between what is sent and what is received. This first practice problem  provides a clear illustration of this principle, demonstrating how certain inputs can be detrimental to capacity by adding more ambiguity than information.",
            "id": "1617016",
            "problem": "A communication system is designed with a set of three possible input symbols from the alphabet $\\mathcal{X} = \\{A, B, C\\}$, and a set of two possible output symbols from the alphabet $\\mathcal{Y} = \\{A, B\\}$. The behavior of this discrete memoryless channel is defined by the following conditional probabilities:\n-   An input of $A$ is always received as an output of $A$.\n-   An input of $B$ is always received as an output of $B$.\n-   An input of $C$ results in a random output, which is equally likely to be $A$ or $B$.\n\nThe capacity of a channel is defined as the maximum possible mutual information between the input $X$ and the output $Y$, where the maximization is performed over all possible input probability distributions $P(X)$.\n\nCalculate the capacity of this channel. Express your final answer in units of bits, as a single, exact numerical value. All logarithms should be interpreted as base-2.",
            "solution": "Let the input distribution be $P(X=A)=a$, $P(X=B)=b$, and $P(X=C)=c$ with $a,b,c\\geq 0$ and $a+b+c=1$. From the channel law,\n- $P(Y=A\\mid X=A)=1$ and $P(Y=B\\mid X=A)=0$,\n- $P(Y=A\\mid X=B)=0$ and $P(Y=B\\mid X=B)=1$,\n- $P(Y=A\\mid X=C)=\\frac{1}{2}$ and $P(Y=B\\mid X=C)=\\frac{1}{2}$.\n\nThus the output distribution is\n$$\nP(Y=A)=a+\\frac{c}{2},\\qquad P(Y=B)=b+\\frac{c}{2}.\n$$\nThe output entropy (with base-2 logarithms) is the binary entropy\n$$\nH(Y)= -\\left(a+\\frac{c}{2}\\right)\\log_{2}\\left(a+\\frac{c}{2}\\right) - \\left(b+\\frac{c}{2}\\right)\\log_{2}\\left(b+\\frac{c}{2}\\right).\n$$\nThe conditional entropy is\n$$\nH(Y\\mid X)= a\\cdot 0 + b\\cdot 0 + c\\cdot 1 = c,\n$$\nsince $Y$ is deterministic given $X=A$ or $X=B$, and $Y$ is equiprobable given $X=C$. Therefore the mutual information is\n$$\nI(X;Y)=H(Y)-H(Y\\mid X)= h_{2}\\!\\left(a+\\frac{c}{2}\\right)-c,\n$$\nwhere $h_{2}(u)=-u\\log_{2}u-(1-u)\\log_{2}(1-u)$.\n\nFor any fixed $c\\in[0,1]$, $h_{2}(u)$ is maximized at $u=\\frac{1}{2}$. The constraint set allows $u=a+\\frac{c}{2}$ to equal $\\frac{1}{2}$ by choosing $a=\\frac{1}{2}-\\frac{c}{2}$ and hence $b=1-a-c=\\frac{1}{2}-\\frac{c}{2}$, which are nonnegative for all $c\\in[0,1]$. Thus, for fixed $c$,\n$$\n\\max_{a,b} I(X;Y)= 1 - c.\n$$\nMaximizing over $c\\in[0,1]$ yields $c^{\\star}=0$ and\n$$\n\\max_{a,b,c} I(X;Y)= 1.\n$$\nThis achieves the general upper bound $I(X;Y)\\leq H(Y)\\leq \\log_{2}|\\mathcal{Y}|=\\log_{2}2=1$, confirming optimality. One achieving distribution is $a=b=\\frac{1}{2}$ and $c=0$, i.e., never use $C$.\n\nTherefore, the channel capacity is $1$ bit.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "In some communication systems, the channel's operation is deterministic, meaning the output is a direct, predictable function of the input. For such channels, there is no uncertainty about the output once the input is known, making the conditional entropy $H(Y|X)$ equal to zero. This exercise  explores this important special case, where the challenge of maximizing mutual information $I(X;Y)$ simplifies to maximizing the output's entropy $H(Y)$.",
            "id": "1648936",
            "problem": "A simplified digital communication system is designed to transmit information using two parallel, independent binary sources. Let the random variables $X_1$ and $X_2$ represent the bits sent from source 1 and source 2, respectively, where $X_i \\in \\{0, 1\\}$. A fundamental constraint of the system is that the inputs $X_1$ and $X_2$ are always statistically independent.\n\nAt the receiver, a simple adder circuit combines these two signals to produce a single output signal, $Y$, which is the arithmetic sum of the input bits: $Y = X_1 + X_2$. The set of possible input pairs is $\\mathcal{X} = \\{(0,0), (0,1), (1,0), (1,1)\\}$, and the set of possible outputs is $\\mathcal{Y} = \\{0, 1, 2\\}$.\n\nThe operator of this system can choose the statistics of the independent sources. Let $p_1 = P(X_1=1)$ and $p_2 = P(X_2=1)$ be the probabilities that each source sends a '1'. The capacity of this channel is defined as the maximum mutual information $I((X_1, X_2); Y)$ over all possible choices of $p_1$ and $p_2$ in the range $[0, 1]$.\n\nCalculate the capacity of this channel. Express your final answer as a single real number in units of bits per channel use.",
            "solution": "Let the input to the channel be the random variable $X = (X_1, X_2)$ and the output be $Y = X_1 + X_2$. The capacity $C$ of the channel is defined as the maximum of the mutual information $I(X; Y)$ over all possible input distributions.\n\n$$C = \\max_{p(x)} I(X; Y)$$\n\nThe mutual information is given by $I(X; Y) = H(Y) - H(Y|X)$.\nThe output $Y$ is a deterministic function of the input $X$. Specifically, if we know the input pair $(X_1, X_2)$, the output $Y = X_1 + X_2$ is fully determined. This means the conditional entropy of the output given the input, $H(Y|X)$, is zero.\n$$H(Y|X) = \\sum_{x \\in \\mathcal{X}} p(x) H(Y|X=x) = \\sum_{x \\in \\mathcal{X}} p(x) \\cdot 0 = 0$$\nTherefore, the mutual information simplifies to the entropy of the output:\n$$I(X; Y) = H(Y)$$\nThe capacity is thus the maximum possible entropy of the output, where the maximization is performed over the set of allowed input distributions.\n$$C = \\max_{p(x)} H(Y)$$\nThe problem specifies that the input bits $X_1$ and $X_2$ are independent. Let $P(X_1=1) = p_1$ and $P(X_2=1) = p_2$. The distribution of the input pairs is then:\n$P(X=(0,0)) = (1-p_1)(1-p_2)$\n$P(X=(0,1)) = (1-p_1)p_2$\n$P(X=(1,0)) = p_1(1-p_2)$\n$P(X=(1,1)) = p_1 p_2$\n\nNow, we determine the distribution of the output $Y$. The possible values for $Y$ are $0, 1, 2$.\n$P(Y=0) = P(X=(0,0)) = (1-p_1)(1-p_2)$\n$P(Y=1) = P(X=(0,1)) + P(X=(1,0)) = (1-p_1)p_2 + p_1(1-p_2) = p_1 + p_2 - 2p_1p_2$\n$P(Y=2) = P(X=(1,1)) = p_1 p_2$\n\nThe entropy of the output $Y$ is a function of $p_1$ and $p_2$:\n$$H(Y) = -P(Y=0)\\log_2(P(Y=0)) - P(Y=1)\\log_2(P(Y=1)) - P(Y=2)\\log_2(P(Y=2))$$\nWe need to maximize this function $H(Y; p_1, p_2)$ for $p_1, p_2 \\in [0, 1]$.\n\nThe structure of the problem is symmetric with respect to $X_1$ and $X_2$. Swapping $p_1$ and $p_2$ leaves the probabilities $P(Y=0)$, $P(Y=1)$, and $P(Y=2)$ unchanged. This suggests that the maximum value of $H(Y)$ will occur when $p_1 = p_2$. Let's set $p_1 = p_2 = p$.\n\nWith $p_1=p_2=p$, the output distribution becomes:\n$P(Y=0) = (1-p)(1-p) = (1-p)^2$\n$P(Y=1) = p(1-p) + (1-p)p = 2p(1-p)$\n$P(Y=2) = p \\cdot p = p^2$\nThis is a binomial distribution for the sum of two independent Bernoulli trials, $Y \\sim \\text{Binomial}(2, p)$.\nThe entropy is now a function of a single variable $p$:\n$$H(Y; p) = - (1-p)^2 \\log_2((1-p)^2) - 2p(1-p) \\log_2(2p(1-p)) - p^2 \\log_2(p^2)$$\nTo find the value of $p$ that maximizes this entropy, we can take the derivative with respect to $p$ and set it to zero. Using the natural logarithm $\\ln$ for convenience (since $\\log_2(x) = \\ln(x)/\\ln(2)$), we have:\n$\\frac{d}{dp} H(Y; p) \\propto \\frac{d}{dp} \\left[ -(1-p)^2 \\ln((1-p)^2) - 2p(1-p) \\ln(2p(1-p)) - p^2 \\ln(p^2) \\right]$\nA known result for the entropy of a binomial distribution $\\text{Binomial}(n,p)$ is that it is maximized at $p=1/2$. Let's verify this for $n=2$ by setting the derivative to zero. The derivative is proportional to:\n$$\\frac{d H}{dp} \\propto \\ln\\left(\\frac{1-p}{p}\\right) - (1-2p)\\ln(2)$$\nSetting the derivative to zero:\n$$\\ln\\left(\\frac{1-p}{p}\\right) = (1-2p)\\ln(2) = \\ln(2^{1-2p})$$\n$$\\frac{1-p}{p} = 2^{1-2p}$$\nBy inspection, $p=1/2$ is a solution:\nLHS: $\\frac{1-1/2}{1/2} = \\frac{1/2}{1/2} = 1$.\nRHS: $2^{1-2(1/2)} = 2^{1-1} = 2^0 = 1$.\nThus, the entropy is maximized at $p=1/2$. This corresponds to choosing $p_1=p_2=1/2$.\n\nWith $p_1=p_2=1/2$, the input distribution is uniform over the four possible pairs: $P(X=(x_1,x_2)) = 1/4$ for all $(x_1,x_2) \\in \\mathcal{X}$. The corresponding output distribution is:\n$P(Y=0) = (1-1/2)^2 = 1/4$\n$P(Y=1) = 2(1/2)(1-1/2) = 1/2$\n$P(Y=2) = (1/2)^2 = 1/4$\n\nThe maximum entropy $H(Y)$ is calculated with this optimal distribution:\n$$C = H(Y) = -\\left( \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) + \\frac{1}{2} \\log_2\\left(\\frac{1}{2}\\right) + \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) \\right)$$\n$$C = -\\left( \\frac{1}{4}(-2) + \\frac{1}{2}(-1) + \\frac{1}{4}(-2) \\right)$$\n$$C = -\\left( -\\frac{1}{2} - \\frac{1}{2} - \\frac{1}{2} \\right)$$\n$$C = \\frac{3}{2} = 1.5$$\nThe capacity of the channel is 1.5 bits per channel use.",
            "answer": "$$\\boxed{1.5}$$"
        },
        {
            "introduction": "Real-world communication systems rarely operate without constraints; factors like power consumption are critical design considerations. This final problem  elevates our analysis by introducing an energy cost for each transmitted symbol. Your task is to find the channel capacity not just by maximizing mutual information, but by doing so while adhering to a strict average energy budget, a common challenge in engineering.",
            "id": "1617058",
            "problem": "A deep-space probe uses a specialized communication system to receive commands from Earth. The system has an input alphabet of three symbols, $\\mathcal{X} = \\{x_1, x_2, x_3\\}$, corresponding to different energy-level signals. The energy cost associated with transmitting each symbol is $c(x_1) = 0$ Joules, $c(x_2) = 3$ Joules, and $c(x_3) = 5$ Joules. Due to extreme power limitations on the probe's transmitter, the long-term average energy cost per transmitted symbol must be exactly $S = 4$ Joules.\n\nThe communication channel is a discrete memoryless channel with output alphabet $\\mathcal{Y} = \\{y_1, y_2, y_3\\}$. The channel's behavior is characterized by the following conditional probabilities:\n- An input of $x_1$ (no signal) is always correctly identified as output $y_1$.\n- An input of $x_2$ (medium signal) is correctly identified as output $y_2$ with probability $3/4$, but due to noise, it may be mistaken for output $y_1$ (a \"dropout\") with probability $1/4$. It is never mistaken for $y_3$.\n- An input of $x_3$ (strong signal) is always correctly identified as output $y_3$.\n\nTo operate the system most efficiently, the engineers must choose an input probability distribution $p(x_1), p(x_2), p(x_3)$ that maximizes the mutual information $I(X;Y)$ subject to the average energy cost constraint.\n\nDetermine the optimal probability $p(x_2)$ for using the second symbol. Express your answer as a closed-form analytic expression.",
            "solution": "Let $p_i \\triangleq p(x_i)$ with $p_1+p_2+p_3=1$, and impose the exact average cost constraint $3p_2+5p_3=4$. Eliminating $p_3$ gives $p_3=(4-3p_2)/5$ and then $p_1=1-p_2-p_3=(1-2p_2)/5$. Feasibility requires $p_2\\in[0,1/2]$.\n\nThe channel transition probabilities are\n- $P(y_1 \\mid x_1)=1$,\n- $P(y_1 \\mid x_2)=\\frac{1}{4}$, $P(y_2 \\mid x_2)=\\frac{3}{4}$,\n- $P(y_3 \\mid x_3)=1$.\n\nHence the output distribution is\n$$\nq_1 \\triangleq P(y_1)=p_1+\\frac{1}{4}p_2=\\frac{1}{5}-\\frac{3}{20}p_2=\\frac{4-3p_2}{20},\\quad\nq_2 \\triangleq P(y_2)=\\frac{3}{4}p_2,\\quad\nq_3 \\triangleq P(y_3)=p_3=\\frac{4-3p_2}{5}=4q_1.\n$$\nThe conditional entropy is\n$$\nH(Y \\mid X)=p_1 \\cdot 0+p_2\\left[-\\frac{1}{4}\\ln\\!\\left(\\frac{1}{4}\\right)-\\frac{3}{4}\\ln\\!\\left(\\frac{3}{4}\\right)\\right]+p_3 \\cdot 0\n= p_2 h,\n$$\nwhere $h \\triangleq -\\frac{1}{4}\\ln(1/4)-\\frac{3}{4}\\ln(3/4)$, and\n$$\nH(Y)=-\\sum_{i=1}^{3} q_i \\ln q_i.\n$$\nTherefore $I(X;Y)=H(Y)-H(Y \\mid X)=-\\sum_i q_i \\ln q_i - p_2 h$ as a function of $p_2$.\n\nDifferentiate with respect to $p_2$. Using $q_1'=-\\frac{3}{20}$, $q_2'=\\frac{3}{4}$, $q_3'=-\\frac{3}{5}$ and $\\sum_i q_i'=0$, we obtain\n$$\n\\frac{dI}{dp_2}\n= -\\sum_i(\\ln q_i+1) q_i' - h\n= -\\sum_i q_i' \\ln q_i - h\n= \\frac{3}{20}\\ln q_1 - \\frac{3}{4}\\ln q_2 + \\frac{3}{5}\\ln q_3 - h.\n$$\nSetting $\\frac{dI}{dp_2}=0$ yields\n$$\n\\frac{1}{20}\\ln q_1 - \\frac{1}{4}\\ln q_2 + \\frac{1}{5}\\ln q_3 = \\frac{h}{3}\n\\;\\Longleftrightarrow\\;\n\\ln q_1 - 5\\ln q_2 + 4\\ln q_3 = \\frac{20h}{3}.\n$$\nExponentiating,\n$$\nq_1 q_3^4 q_2^{-5}=\\exp\\!\\left(\\frac{20h}{3}\\right).\n$$\nUsing $q_3=4q_1$, the left side becomes $4^4(q_1/q_2)^5$, so\n$$\n4^4\\left(\\frac{q_1}{q_2}\\right)^5=\\exp\\!\\left(\\frac{20h}{3}\\right).\n$$\nCompute the right side from $h$:\n$$\n\\exp(h)=(1/4)^{-1/4}(3/4)^{-3/4}\\;\\Longrightarrow\\;\n\\exp\\!\\left(\\frac{20h}{3}\\right)=(1/4)^{-5/3}(3/4)^{-5}.\n$$\nTaking fifth roots,\n$$\n4^{4/5}\\,\\frac{q_1}{q_2}=(1/4)^{-1/3}(3/4)^{-1}=4^{1/3}\\cdot\\frac{4}{3}=\\frac{4^{4/3}}{3},\n$$\nhence\n$$\n\\frac{q_1}{q_2}=\\frac{4^{4/3-4/5}}{3}=\\frac{4^{8/15}}{3}.\n$$\nBut $q_1/q_2=\\frac{(4-3 p_2)/20}{(3/4)p_2}=\\frac{4-3 p_2}{15 p_2}$. Therefore\n$$\n\\frac{4-3 p_2}{15 p_2}=\\frac{4^{8/15}}{3}\n\\;\\Longrightarrow\\;\n4-3 p_2=5\\cdot 4^{8/15}\\, p_2\n\\;\\Longrightarrow\\;\np_2=\\frac{4}{3+5\\cdot 4^{8/15}}.\n$$\nSince $p_2 \\in (0,1/2)$ for this value and $I(X;Y)$ is concave in $p_2$, this stationary point is the unique global maximizer under the given constraints. Thus the optimal probability of using $x_2$ is the closed-form value above.",
            "answer": "$$\\boxed{\\frac{4}{3+5\\cdot 4^{\\frac{8}{15}}}}$$"
        }
    ]
}