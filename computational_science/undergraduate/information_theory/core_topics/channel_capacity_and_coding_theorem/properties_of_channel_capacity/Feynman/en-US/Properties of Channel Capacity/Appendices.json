{
    "hands_on_practices": [
        {
            "introduction": "To build our intuition for channel capacity, we begin with a simplified, ideal scenario: a deterministic channel. In this type of channel, there is no noise or uncertainty in the transmission; each input maps to a specific output. This exercise  challenges you to find the capacity of such a channel, forcing you to focus purely on how the structure of the input-to-output mapping limits the information throughput. The key insight lies in understanding that capacity here is equivalent to maximizing the entropy, or variety, of the possible outputs.",
            "id": "1648942",
            "problem": "A simple digital processing unit is designed to operate on a limited set of integer inputs. The unit accepts one of four possible input signals, represented by the integers in the set $\\mathcal{X} = \\{1, 2, 3, 4\\}$. It performs a fixed computation and produces an output integer from the set $\\mathcal{Y} = \\{0, 1, 2\\}$. The relationship between any input $x \\in \\mathcal{X}$ and the corresponding output $y \\in \\mathcal{Y}$ is governed by the deterministic function $y = x \\pmod 3$, where the result of the modulo operation is the non-negative remainder of the division of $x$ by 3.\n\nAssuming each use of this processing unit represents a single use of a communication channel, determine the capacity of this channel. Express your answer as a single analytic expression in units of bits per channel use. Use the base-2 logarithm in your calculations.",
            "solution": "Let $X \\in \\mathcal{X}=\\{1,2,3,4\\}$ and $Y \\in \\mathcal{Y}=\\{0,1,2\\}$ with the deterministic mapping $Y=f(X)=X \\bmod 3$. For a deterministic channel, the conditional entropy satisfies $H(Y|X)=0$. Therefore, the mutual information for an input distribution $P_{X}$ is\n$$\nI(X;Y)=H(Y)-H(Y|X)=H(Y).\n$$\nThe channel capacity is the maximum mutual information over all input distributions, hence\n$$\nC=\\max_{P_{X}} I(X;Y)=\\max_{P_{X}} H(Y).\n$$\nThe mapping groups inputs by their remainders: $1 \\mapsto 1$, $2 \\mapsto 2$, $3 \\mapsto 0$, $4 \\mapsto 1$. Thus, for any input distribution $P_{X}$, the induced output distribution satisfies\n$$\nP_{Y}(0)=P_{X}(3), \\quad P_{Y}(1)=P_{X}(1)+P_{X}(4), \\quad P_{Y}(2)=P_{X}(2).\n$$\nGiven any desired output distribution $(p_{0},p_{1},p_{2})$ with $p_{0}+p_{1}+p_{2}=1$ and $p_{i}\\geq 0$, one can choose, for example,\n$$\nP_{X}(3)=p_{0}, \\quad P_{X}(2)=p_{2}, \\quad P_{X}(1)=p_{1}, \\quad P_{X}(4)=0,\n$$\nwhich induces $P_{Y}=p$. Hence every distribution on $\\{0,1,2\\}$ is achievable, and the capacity reduces to the maximum entropy over a 3-point alphabet. The maximum of $H(Y)$ occurs at the uniform distribution, giving\n$$\n\\max_{P_{Y}} H(Y)=\\log_{2}(3).\n$$\nTherefore, the channel capacity is\n$$\nC=\\log_{2}(3) \\text{ bits per channel use.}\n$$",
            "answer": "$$\\boxed{\\log_{2}(3)}$$"
        },
        {
            "introduction": "Real-world channels are rarely perfect. This next practice introduces the crucial element of noise by presenting a descriptive scenario of a communication channel that sometimes transmits correctly and sometimes injects random errors . Your first task is to translate this word problem into a precise mathematical modelâ€”a skill essential for any engineer or scientist. Once modeled, you can calculate the capacity, appreciating how probabilistic behavior fundamentally limits the reliable flow of information, a clear departure from the deterministic case we first examined.",
            "id": "1648926",
            "problem": "Consider a discrete memoryless communication channel with a binary input alphabet $\\mathcal{X} = \\{0, 1\\}$ and a binary output alphabet $\\mathcal{Y} = \\{0, 1\\}$. The channel's behavior is probabilistic and defined as follows: for any given input bit $X$, the output bit $Y$ is identical to the input (i.e., $Y=X$) with a probability of $1/2$. With the remaining probability of $1/2$, the channel disregards the input and the output $Y$ is determined by a random fair coin flip, meaning $Y$ is set to $0$ or $1$ with equal probability, irrespective of the input $X$.\n\nCalculate the capacity of this channel. Express your answer as a closed-form analytic expression in units of bits per channel use. Your expression may involve logarithms, which must be taken base 2.",
            "solution": "We model the given channel as follows. For any input bit $X=x\\in\\{0,1\\}$, with probability $\\frac{1}{2}$ the output equals the input, and with probability $\\frac{1}{2}$ the output is an independent fair coin. Therefore, the conditional distribution satisfies\n$$\n\\Pr(Y=x\\mid X=x)=\\frac{1}{2}+\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{3}{4},\\qquad \\Pr(Y\\neq x\\mid X=x)=\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{1}{4}.\n$$\nHence, the channel is a binary symmetric channel (BSC) with crossover probability $p=\\frac{1}{4}$ and transition matrix\n$$\n\\begin{pmatrix}\n\\Pr(Y=0\\mid X=0) & \\Pr(Y=1\\mid X=0) \\\\\n\\Pr(Y=0\\mid X=1) & \\Pr(Y=1\\mid X=1)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{3}{4} & \\frac{1}{4} \\\\\n\\frac{1}{4} & \\frac{3}{4}\n\\end{pmatrix}.\n$$\n\nThe capacity of a discrete memoryless channel is $C=\\max_{P_{X}} I(X;Y)$, where $I(X;Y)=H(Y)-H(Y\\mid X)$. For a BSC with crossover $p$, the conditional entropy $H(Y\\mid X)$ is independent of the input distribution and equals the binary entropy function,\n$$\nH(Y\\mid X)=h_{2}(p)=-p\\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$\nTo maximize $I(X;Y)$, we maximize $H(Y)$. For a BSC with input distribution $\\Pr(X=0)=q$, the output distribution is\n$$\n\\Pr(Y=0)=p+q(1-2p).\n$$\nThe output entropy $H(Y)$ is maximized when the output is uniform, i.e., $\\Pr(Y=0)=\\frac{1}{2}$. Solving\n$$\n\\frac{1}{2}=p+q(1-2p)\n$$\ngives\n$$\nq=\\frac{\\frac{1}{2}-p}{1-2p}.\n$$\nFor $p=\\frac{1}{4}$, this yields $q=\\frac{\\frac{1}{2}-\\frac{1}{4}}{1-\\frac{1}{2}}=\\frac{\\frac{1}{4}}{\\frac{1}{2}}=\\frac{1}{2}$, so the uniform input maximizes $H(Y)$ and achieves $H(Y)=1$. Therefore, the capacity is\n$$\nC=1-h_{2}\\!\\left(\\frac{1}{4}\\right)=1+\\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right)+\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{4}\\right).\n$$\nThis is in bits per channel use, with logarithms taken base $2$.",
            "answer": "$$\\boxed{1+\\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right)+\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{4}\\right)}$$"
        },
        {
            "introduction": "Our final practice moves beyond counting 'bits per channel use' to a more practical metric: information rate measured in bits per unit time. We return to a noiseless channel, but now with a new physical constraint: different symbols take different amounts of time to transmit . This problem illustrates that true communication efficiency depends not only on the abstract properties of information but also on the physical resources consumed. Solving it requires finding an optimal balance between transmitting high-information symbols and the time they cost, introducing a more advanced optimization perspective on capacity.",
            "id": "1648953",
            "problem": "A digital communication system utilizes a noiseless binary channel to transmit a stream of symbols from the input alphabet $\\{0, 1\\}$. The channel operates perfectly, such that the received output symbol is always identical to the transmitted input symbol. However, the physical transmitter imposes a temporal cost on the symbols. The time required to transmit a '0' symbol is $\\tau$, while the time required to transmit a '1' symbol is $2\\tau$.\n\nDetermine the capacity of this channel in bits per unit time. Your final answer should be a closed-form analytic expression in terms of the time parameter $\\tau$.",
            "solution": "Because the channel is noiseless, the information delivered per symbol equals the source entropy, but symbols have unequal durations. For an input distribution $p_{0}=\\Pr\\{0\\}$ and $p_{1}=\\Pr\\{1\\}=1-p_{0}$, the entropy per symbol is\n$$\nH(X)=-p_{0}\\log_{2}p_{0}-p_{1}\\log_{2}p_{1},\n$$\nand the average transmission time per symbol is\n$$\n\\mathbb{E}[T]=p_{0}\\tau+p_{1}(2\\tau)=\\tau(2-p_{0}).\n$$\nThe information rate (bits per unit time) for this input is $R=\\frac{H(X)}{\\mathbb{E}[T]}$, and the channel capacity is\n$$\nC=\\sup_{p_{0}\\in[0,1]}\\frac{H(X)}{\\mathbb{E}[T]}.\n$$\n\nRather than optimizing directly in $p_{0}$, we use a standard variational method for maximizing entropy rate under a cost (time) constraint. Define, for a general finite alphabet with symbol times $\\{t_{i}\\}$ and probabilities $\\{p_{i}\\}$,\n$$\nH=-\\sum_{i}p_{i}\\log_{2}p_{i},\\qquad \\mathbb{E}[T]=\\sum_{i}p_{i}t_{i}.\n$$\nFor a given nonnegative parameter $s$, maximize $H-s\\mathbb{E}[T]$ subject to $\\sum_{i}p_{i}=1$. Using $\\ln$ for natural logarithm, write $H=-\\frac{1}{\\ln 2}\\sum_{i}p_{i}\\ln p_{i}$ and form the Lagrangian\n$$\n\\mathcal{L}=-\\frac{1}{\\ln 2}\\sum_{i}p_{i}\\ln p_{i}-s\\sum_{i}p_{i}t_{i}-\\mu\\left(\\sum_{i}p_{i}-1\\right).\n$$\nStationarity $\\frac{\\partial \\mathcal{L}}{\\partial p_{i}}=0$ gives\n$$\n-\\frac{1}{\\ln 2}(\\ln p_{i}+1)-s t_{i}-\\mu=0 \\quad \\Longrightarrow \\quad \\ln p_{i}=-1-(\\ln 2)(s t_{i}+\\mu),\n$$\nhence\n$$\np_{i}=a\\,2^{-s t_{i}},\\qquad a=\\exp(-1)\\exp\\bigl(-(\\ln 2)\\mu\\bigr),\n$$\nwith $a$ determined by normalization:\n$$\na=\\frac{1}{\\sum_{j}2^{-s t_{j}}}.\n$$\nFor this maximizing distribution,\n$$\nH=-\\sum_{i}p_{i}\\log_{2}(a\\,2^{-s t_{i}})=-\\log_{2}a+s\\sum_{i}p_{i}t_{i}=-\\log_{2}a+s\\,\\mathbb{E}[T].\n$$\nBecause $a^{-1}=\\sum_{j}2^{-s t_{j}}$, we have\n$$\nH-s\\,\\mathbb{E}[T]=-\\log_{2}a=\\log_{2}\\left(\\sum_{j}2^{-s t_{j}}\\right).\n$$\nFor any $s$ with $\\sum_{j}2^{-s t_{j}}\\leq 1$, this yields $H-s\\,\\mathbb{E}[T]\\leq 0$, i.e., $\\frac{H}{\\mathbb{E}[T]}\\leq s$. The tightest bound is obtained at the unique $s=C$ satisfying\n$$\n\\sum_{j}2^{-C t_{j}}=1,\n$$\nfor which $a=1$ and $H=C\\,\\mathbb{E}[T]$, so the supremum of $\\frac{H}{\\mathbb{E}[T]}$ equals $C$.\n\nIn the given channel, there are two symbols with times $t_{0}=\\tau$ and $t_{1}=2\\tau$. Therefore $C$ is the unique positive solution of\n$$\n2^{-C\\tau}+2^{-2C\\tau}=1.\n$$\nLet $x=2^{-C\\tau}$. Then\n$$x+x^{2}=1 \\quad \\Longrightarrow \\quad x^{2}+x-1=0,$$\nwhose positive root is\n$$x=\\frac{\\sqrt{5}-1}{2}.$$\nThus\n$$2^{-C\\tau}=\\frac{\\sqrt{5}-1}{2}\\quad \\Longrightarrow \\quad -C\\tau=\\log_{2}\\!\\left(\\frac{\\sqrt{5}-1}{2}\\right),$$\nand hence\n$$C=-\\frac{1}{\\tau}\\log_{2}\\!\\left(\\frac{\\sqrt{5}-1}{2}\\right)=\\frac{1}{\\tau}\\log_{2}\\!\\left(\\frac{1+\\sqrt{5}}{2}\\right).$$\nThis is the channel capacity in bits per unit time, expressed in closed form in terms of $\\tau$.",
            "answer": "$$\\boxed{\\frac{1}{\\tau}\\log_{2}\\left(\\frac{1+\\sqrt{5}}{2}\\right)}$$"
        }
    ]
}