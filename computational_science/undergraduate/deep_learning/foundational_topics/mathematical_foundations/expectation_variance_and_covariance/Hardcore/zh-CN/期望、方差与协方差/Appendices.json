{
    "hands_on_practices": [
        {
            "introduction": "随机梯度是深度学习的引擎，但它们也充满了噪声。理解这些梯度的统计特性，例如不同层更新之间的协方差，可以为我们提供关于训练动态的深刻见解。本练习将指导您分析并计算一个简单网络中梯度的协方差，从而量化不同层之间的“噪声”是协同工作（对齐）还是相互冲突 。",
            "id": "3123312",
            "problem": "考虑一个在深度学习中使用的标量双层线性网络，其输入为 $x \\in \\mathbb{R}$，参数为 $a \\in \\mathbb{R}$ 和 $b \\in \\mathbb{R}$，输出为 $\\hat{y} = b a x$。训练标签由随机线性模型 $y = \\theta x + \\varepsilon$ 生成，其中 $x \\sim \\mathcal{N}(0, \\sigma_x^2)$ 和 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^2)$ 是独立的零均值高斯随机变量。每个样本的损失是均方误差（MSE, Mean Squared Error）$L = \\frac{1}{2}(\\hat{y} - y)^2$。对于单个随机样本，定义随机梯度 $g^{(1)} = \\frac{\\partial L}{\\partial a}$ 和 $g^{(2)} = \\frac{\\partial L}{\\partial b}$。由于 $x$ 和 $\\varepsilon$ 的随机性，这些梯度是随机变量。\n\n您的任务是：\n1. 严格从期望 $\\mathbb{E}$、方差 $\\operatorname{Var}$ 和协方差 $\\operatorname{Cov}$ 的基本定义出发，并结合 $x$ 和 $\\varepsilon$ 的独立性，推导出 $\\mathbb{E}[g^{(1)}]$、$\\mathbb{E}[g^{(2)}]$、$\\operatorname{Var}(g^{(1)})$、$\\operatorname{Var}(g^{(2)})$ 和 $\\operatorname{Cov}(g^{(1)}, g^{(2)})$ 关于 $a$、$b$、$\\theta$、$\\sigma_x^2$ 和 $\\sigma_{\\varepsilon}^2$ 的闭式符号表达式。然后推导出相关系数 $\\rho = \\frac{\\operatorname{Cov}(g^{(1)}, g^{(2)})}{\\sqrt{\\operatorname{Var}(g^{(1)}) \\operatorname{Var}(g^{(2)})}}$。\n2. 实现一个程序，使用这些推导出的公式，为每个给定的测试用例计算 $\\operatorname{Cov}(g^{(1)}, g^{(2)})$、$\\operatorname{Var}(g^{(1)})$、$\\operatorname{Var}(g^{(2)})$ 和 $\\rho$ 的数值。此外，通过协方差的符号来判断各层梯度噪声的对齐与冲突：如果 $\\operatorname{Cov}(g^{(1)}, g^{(2)})  0$（对齐），输出 $+1$；如果 $\\operatorname{Cov}(g^{(1)}, g^{(2)})  0$（冲突），输出 $-1$；如果 $\\operatorname{Cov}(g^{(1)}, g^{(2)}) = 0$（中性），输出 $0$。当 $\\operatorname{Var}(g^{(1)}) = 0$ 或 $\\operatorname{Var}(g^{(2)}) = 0$ 时，按照约定将 $\\rho$ 定义为 $0$。\n\n仅使用给定的模型和概率假设。在推导过程中，除了关于独立的零均值高斯随机变量的已验证事实外，不要引入任何额外的捷径或公式。所有数学表达式都使用 LaTeX 表示。不涉及物理单位或角度单位。结果应为数值浮点数或整数。\n\n测试套件（每个用例是一个元组 $(a, b, \\theta, \\sigma_x^2, \\sigma_{\\varepsilon}^2)$）：\n- 用例 1：$(1.0, 2.0, 1.2, 1.0, 0.5)$\n- 用例 2：$(1.0, -2.0, 1.2, 1.0, 0.5)$\n- 用例 3：$(0.0, 1.5, 0.8, 1.0, 0.3)$\n- 用例 4：$(1.0, 1.5, 1.5, 1.0, 0.0)$\n- 用例 5：$(-0.5, -0.25, 0.1, 2.0, 5.0)$\n- 用例 6：$(0.3, 0.6, 0.1, 0.01, 1.0)$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，不含空格，其中每个测试用例的结果是列表 $[\\operatorname{Cov}(g^{(1)}, g^{(2)}), \\operatorname{Var}(g^{(1)}), \\operatorname{Var}(g^{(2)}), \\rho, \\text{sign}]$。例如，输出必须类似于 $[[c_{1},v_{1}^{(1)},v_{1}^{(2)},\\rho_{1},s_{1}],[c_{2},v_{2}^{(1)},v_{2}^{(2)},\\rho_{2},s_{2}],\\dots]$，其中每个 $c_i$、$v_i^{(1)}$、$v_i^{(2)}$ 和 $\\rho_i$ 是浮点数，每个 $s_i$ 是 $\\{-1, 0, 1\\}$ 中的整数。",
            "solution": "该问题是有效的，因为它在概率论和微积分方面有科学依据，是适定的，具有唯一且有意义的解，并且使用客观、正式的语言表达。这是一个分析简化神经网络模型中随机梯度统计特性的标准理论练习。\n\n我们首先正式推导所需的统计量。\n\n**1. 问题设置与梯度推导**\n\n模型输出为 $\\hat{y} = b a x$，目标标签由 $y = \\theta x + \\varepsilon$ 生成。单个样本的损失是均方误差 (MSE)：\n$$L = \\frac{1}{2}(\\hat{y} - y)^2 = \\frac{1}{2}(bax - (\\theta x + \\varepsilon))^2$$\n我们可以将方括号内的项重写为：\n$$\\hat{y} - y = (ab - \\theta)x - \\varepsilon$$\n关于参数 $a$ 和 $b$ 的随机梯度是损失 $L$ 的偏导数。\n\n关于 $a$ 的梯度，记为 $g^{(1)}$，是：\n$$g^{(1)} = \\frac{\\partial L}{\\partial a} = \\frac{\\partial}{\\partial a} \\left[ \\frac{1}{2}((ab - \\theta)x - \\varepsilon)^2 \\right]$$\n使用链式法则：\n$$g^{(1)} = ((ab - \\theta)x - \\varepsilon) \\cdot \\frac{\\partial}{\\partial a}((ab - \\theta)x - \\varepsilon) = ((ab - \\theta)x - \\varepsilon) \\cdot (bx)$$\n$$g^{(1)} = (ab - \\theta)bx^2 - b\\varepsilon x$$\n\n关于 $b$ 的梯度，记为 $g^{(2)}$，是：\n$$g^{(2)} = \\frac{\\partial L}{\\partial b} = \\frac{\\partial}{\\partial b} \\left[ \\frac{1}{2}((ab - \\theta)x - \\varepsilon)^2 \\right]$$\n使用链式法则：\n$$g^{(2)} = ((ab - \\theta)x - \\varepsilon) \\cdot \\frac{\\partial}{\\partial b}((ab - \\theta)x - \\varepsilon) = ((ab - \\theta)x - \\varepsilon) \\cdot (ax)$$\n$$g^{(2)} = (ab - \\theta)ax^2 - a\\varepsilon x$$\n\n**2. 随机变量的性质**\n\n随机变量为 $x \\sim \\mathcal{N}(0, \\sigma_x^2)$ 和 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^2)$，并且它们是独立的。我们将使用零均值高斯分布的以下标准矩：\n- $\\mathbb{E}[x] = 0$, $\\mathbb{E}[\\varepsilon] = 0$\n- $\\mathbb{E}[x^2] = \\operatorname{Var}(x) + (\\mathbb{E}[x])^2 = \\sigma_x^2$\n- $\\mathbb{E}[\\varepsilon^2] = \\operatorname{Var}(\\varepsilon) + (\\mathbb{E}[\\varepsilon])^2 = \\sigma_{\\varepsilon}^2$\n- $\\mathbb{E}[x^3] = 0$ (所有奇数阶矩均为零)\n- $\\mathbb{E}[x^4] = 3(\\sigma_x^2)^2 = 3\\sigma_x^4$\n\n由于独立性，关于 $x$ 和 $\\varepsilon$ 的函数乘积的期望等于它们各自期望的乘积。例如，$\\mathbb{E}[x^k \\varepsilon^m] = \\mathbb{E}[x^k]\\mathbb{E}[\\varepsilon^m]$。\n- $\\mathbb{E}[x\\varepsilon] = \\mathbb{E}[x]\\mathbb{E}[\\varepsilon] = 0 \\cdot 0 = 0$\n- $\\mathbb{E}[x^2\\varepsilon^2] = \\mathbb{E}[x^2]\\mathbb{E}[\\varepsilon^2] = \\sigma_x^2 \\sigma_{\\varepsilon}^2$\n- $\\mathbb{E}[x^3\\varepsilon] = \\mathbb{E}[x^3]\\mathbb{E}[\\varepsilon] = 0 \\cdot 0 = 0$\n\n**3. 期望的推导**\n\n利用期望算子 $\\mathbb{E}$ 的线性性：\n$$\\mathbb{E}[g^{(1)}] = \\mathbb{E}[(ab - \\theta)bx^2 - b\\varepsilon x] = (ab - \\theta)b\\mathbb{E}[x^2] - b\\mathbb{E}[\\varepsilon x]$$\n代入各矩：\n$$\\mathbb{E}[g^{(1)}] = (ab - \\theta)b\\sigma_x^2 - b(0) = (ab - \\theta)b\\sigma_x^2$$\n\n对 $g^{(2)}$ 也类似：\n$$\\mathbb{E}[g^{(2)}] = \\mathbb{E}[(ab - \\theta)ax^2 - a\\varepsilon x] = (ab - \\theta)a\\mathbb{E}[x^2] - a\\mathbb{E}[\\varepsilon x]$$\n$$\\mathbb{E}[g^{(2)}] = (ab - \\theta)a\\sigma_x^2 - a(0) = (ab - \\theta)a\\sigma_x^2$$\n这些是期望梯度，代表了期望损失 $\\mathbb{E}[L]$ 的梯度。\n\n**4. 方差与协方差的推导**\n\n我们使用基本定义 $\\operatorname{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2$ 和 $\\operatorname{Cov}(Z_1, Z_2) = \\mathbb{E}[Z_1 Z_2] - \\mathbb{E}[Z_1]\\mathbb{E}[Z_2]$。\n\n首先，我们计算梯度的二阶矩。\n对于 $g^{(1)}$：\n$$(g^{(1)})^2 = ((ab - \\theta)bx^2 - b\\varepsilon x)^2 = (ab - \\theta)^2 b^2 x^4 - 2(ab - \\theta)b^2 \\varepsilon x^3 + b^2 \\varepsilon^2 x^2$$\n取期望：\n$$\\mathbb{E}[(g^{(1)})^2] = (ab - \\theta)^2 b^2 \\mathbb{E}[x^4] - 2(ab - \\theta)b^2 \\mathbb{E}[\\varepsilon x^3] + b^2 \\mathbb{E}[\\varepsilon^2 x^2]$$\n代入高阶矩：\n$$\\mathbb{E}[(g^{(1)})^2] = (ab - \\theta)^2 b^2 (3\\sigma_x^4) - 2(ab - \\theta)b^2 (0) + b^2 (\\sigma_{\\varepsilon}^2 \\sigma_x^2)$$\n$$\\mathbb{E}[(g^{(1)})^2] = 3(ab - \\theta)^2 b^2 \\sigma_x^4 + b^2 \\sigma_{\\varepsilon}^2 \\sigma_x^2$$\n现在，我们计算 $g^{(1)}$ 的方差：\n$$\\operatorname{Var}(g^{(1)}) = \\mathbb{E}[(g^{(1)})^2] - (\\mathbb{E}[g^{(1)}])^2$$\n$$(\\mathbb{E}[g^{(1)}])^2 = ((ab - \\theta)b\\sigma_x^2)^2 = (ab - \\theta)^2 b^2 \\sigma_x^4$$\n$$\\operatorname{Var}(g^{(1)}) = (3(ab - \\theta)^2 b^2 \\sigma_x^4 + b^2 \\sigma_{\\varepsilon}^2 \\sigma_x^2) - (ab - \\theta)^2 b^2 \\sigma_x^4$$\n$$\\operatorname{Var}(g^{(1)}) = 2(ab - \\theta)^2 b^2 \\sigma_x^4 + b^2 \\sigma_{\\varepsilon}^2 \\sigma_x^2$$\n\n根据对称性，$\\operatorname{Var}(g^{(2)})$ 的表达式与将外部的 $b$ 替换为 $a$ 后是相同的：\n$$\\operatorname{Var}(g^{(2)}) = 2(ab - \\theta)^2 a^2 \\sigma_x^4 + a^2 \\sigma_{\\varepsilon}^2 \\sigma_x^2$$\n\n接下来，为了计算协方差，我们计算 $\\mathbb{E}[g^{(1)} g^{(2)}]$：\n$$g^{(1)}g^{(2)} = ((ab - \\theta)bx^2 - b\\varepsilon x)((ab - \\theta)ax^2 - a\\varepsilon x)$$\n$$g^{(1)}g^{(2)} = (ab - \\theta)^2 ab x^4 - 2(ab - \\theta)ab \\varepsilon x^3 + ab \\varepsilon^2 x^2$$\n取期望：\n$$\\mathbb{E}[g^{(1)}g^{(2)}] = (ab - \\theta)^2 ab \\mathbb{E}[x^4] - 2(ab - \\theta)ab \\mathbb{E}[\\varepsilon x^3] + ab \\mathbb{E}[\\varepsilon^2 x^2]$$\n$$\\mathbb{E}[g^{(1)}g^{(2)}] = (ab - \\theta)^2 ab (3\\sigma_x^4) + ab (\\sigma_{\\varepsilon}^2 \\sigma_x^2) = 3(ab - \\theta)^2 ab \\sigma_x^4 + ab \\sigma_{\\varepsilon}^2 \\sigma_x^2$$\n现在，我们计算协方差：\n$$\\operatorname{Cov}(g^{(1)}, g^{(2)}) = \\mathbb{E}[g^{(1)}g^{(2)}] - \\mathbb{E}[g^{(1)}]\\mathbb{E}[g^{(2)}]$$\n$$\\mathbb{E}[g^{(1)}]\\mathbb{E}[g^{(2)}] = ((ab - \\theta)b\\sigma_x^2)((ab - \\theta)a\\sigma_x^2) = (ab - \\theta)^2 ab \\sigma_x^4$$\n$$\\operatorname{Cov}(g^{(1)}, g^{(2)}) = (3(ab - \\theta)^2 ab \\sigma_x^4 + ab \\sigma_{\\varepsilon}^2 \\sigma_x^2) - ((ab - \\theta)^2 ab \\sigma_x^4)$$\n$$\\operatorname{Cov}(g^{(1)}, g^{(2)}) = 2(ab - \\theta)^2 ab \\sigma_x^4 + ab \\sigma_{\\varepsilon}^2 \\sigma_x^2$$\n\n**5. 相关系数**\n\n我们来对表达式进行因式分解。令 $V_Z = 2(ab - \\theta)^2 \\sigma_x^4 + \\sigma_{\\varepsilon}^2 \\sigma_x^2$。此项为非负。\n- $\\operatorname{Var}(g^{(1)}) = b^2 V_Z$\n- $\\operatorname{Var}(g^{(2)}) = a^2 V_Z$\n- $\\operatorname{Cov}(g^{(1)}, g^{(2)}) = ab V_Z$\n\n相关系数 $\\rho$ 定义为：\n$$\\rho = \\frac{\\operatorname{Cov}(g^{(1)}, g^{(2)})}{\\sqrt{\\operatorname{Var}(g^{(1)}) \\operatorname{Var}(g^{(2)})}}$$\n代入我们的表达式：\n$$\\rho = \\frac{ab V_Z}{\\sqrt{(b^2 V_Z) \\cdot (a^2 V_Z)}} = \\frac{ab V_Z}{\\sqrt{a^2 b^2 V_Z^2}} = \\frac{ab V_Z}{|ab| V_Z}$$\n如果 $V_Z \\neq 0$ 且 $a,b \\neq 0$，则可简化为：\n$$\\rho = \\frac{ab}{|ab|} = \\operatorname{sign}(ab)$$\n其中 $\\operatorname{sign}(z)$ 在 $z0$ 时为 $1$，$z0$ 时为 $-1$，$z=0$ 时为 $0$。因此，如果 $a$ 和 $b$ 符号相同，$\\rho=1$。如果它们符号相反，$\\rho=-1$。\n\n根据问题陈述，如果任一方差为零，则将 $\\rho$ 定义为 $0$。\n当 $b=0$ 或 $V_Z=0$ 时，$\\operatorname{Var}(g^{(1)}) = 0$。\n当 $a=0$ 或 $V_Z=0$ 时，$\\operatorname{Var}(g^{(2)}) = 0$。\n所以，如果 $a=0$，或 $b=0$，或 $V_Z=0$，则 $\\rho=0$。\n注意，$V_Z = 0$ 当且仅当 $\\sigma_x^2=0$，或者 $\\sigma_{\\varepsilon}^2=0$ 和 $ab=\\theta$ 同时成立。\n在任一方差为零的所有情况下，我们的表达式都正确地给出 $\\operatorname{Cov}(g^{(1)}, g^{(2)})=0$，因此该约定是一致的。我们关于 $\\rho$ 的通用表达式 $\\operatorname{sign}(ab)$ 在方差非零时有效，并且在 $a=0$ 或 $b=0$ 时与 $\\rho=0$ 的约定一致。\n\n**6. 用于实现的最终公式**\n\n令 $\\Delta = ab - \\theta$。\n令 $\\sigma_x^2$ 为 `sx2`，$\\sigma_{\\varepsilon}^2$ 为 `se2`。\n公因式为 $V_Z = 2\\Delta^2 (\\text{sx2})^2 + \\text{se2} \\cdot \\text{sx2}$。\n- $\\operatorname{Cov}(g^{(1)}, g^{(2)}) = ab V_Z$\n- $\\operatorname{Var}(g^{(1)}) = b^2 V_Z$\n- $\\operatorname{Var}(g^{(2)}) = a^2 V_Z$\n- $\\rho$：如果 $\\operatorname{Var}(g^{(1)})=0$ 或 $\\operatorname{Var}(g^{(2)})=0$，则 $\\rho=0$。否则，$\\rho = \\frac{\\operatorname{Cov}(g^{(1)}, g^{(2)})}{\\sqrt{\\operatorname{Var}(g^{(1)}) \\operatorname{Var}(g^{(2)})}}$。\n- sign：$\\operatorname{sign}(\\operatorname{Cov}(g^{(1)}, g^{(2)}))$，其值为 $+1, -1$ 或 $0$。\n\n这些公式被实现用于计算给定测试用例的数值结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating gradient statistics for a two-layer linear network.\n    \"\"\"\n    # Test suite: each case is a tuple (a, b, theta, sigma_x^2, sigma_epsilon^2)\n    test_cases = [\n        (1.0, 2.0, 1.2, 1.0, 0.5),    # Case 1\n        (1.0, -2.0, 1.2, 1.0, 0.5),   # Case 2\n        (0.0, 1.5, 0.8, 1.0, 0.3),    # Case 3\n        (1.0, 1.5, 1.5, 1.0, 0.0),    # Case 4\n        (-0.5, -0.25, 0.1, 2.0, 5.0), # Case 5\n        (0.3, 0.6, 0.1, 0.01, 1.0),   # Case 6\n    ]\n\n    results = []\n    for case in test_cases:\n        a, b, theta, sx2, se2 = case\n\n        # Let delta = ab - theta\n        delta = a * b - theta\n\n        # Common factor term V_Z = 2 * delta^2 * (sx2)^2 + se2 * sx2\n        # where V_Z = Var(((ab-theta)x - epsilon)x)\n        common_term_vz = 2 * (delta**2) * (sx2**2) + se2 * sx2\n        \n        # Cov(g1, g2) = ab * V_Z\n        cov_g1_g2 = a * b * common_term_vz\n        \n        # Var(g1) = b^2 * V_Z\n        var_g1 = b**2 * common_term_vz\n        \n        # Var(g2) = a^2 * V_Z\n        var_g2 = a**2 * common_term_vz\n\n        # Correlation coefficient rho\n        # By convention, rho = 0 if either variance is 0.\n        if var_g1 == 0 or var_g2 == 0:\n            rho = 0.0\n        else:\n            rho = cov_g1_g2 / np.sqrt(var_g1 * var_g2)\n\n        # Sign of covariance for alignment classification\n        # np.sign returns -1, 0, or 1 as a float. Convert to int.\n        sign = int(np.sign(cov_g1_g2))\n        \n        results.append([cov_g1_g2, var_g1, var_g2, rho, sign])\n\n    # Format the output string to be a list of lists with no spaces.\n    # e.g., [[c1,v11,v12,r1,s1],[c2,v21,v22,r2,s2],...]\n    result_strings = []\n    for res in results:\n        # Format each inner list to a string without spaces.\n        inner_string = \",\".join(map(str, res))\n        result_strings.append(f\"[{inner_string}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个值得信赖的模型除了能做出预测，还应该能表明其预测的置信度。本练习将介绍蒙特卡洛 dropout，这是一种广泛使用的技术，它将标准神经网络重新诠释为贝叶斯模型，用以估计预测不确定性。通过在启用 dropout 的情况下多次进行推理，您将使用样本方差来量化模型的认知不确定性，从而在贝叶斯深度学习的一个关键方面获得实践经验 。",
            "id": "3123387",
            "problem": "给定一个具有修正线性单元（ReLU）激活函数和隐藏层丢弃（dropout）的单隐层前馈神经网络。该网络将一个二维输入向量 $x \\in \\mathbb{R}^2$ 映射到一个标量输出。网络参数是固定的且已知的。您必须使用蒙特卡洛丢弃（Monte Carlo dropout）来近似网络输出 $f(x)$ 在丢弃引起的随机性下的预测均值 $\\mathbb{E}[f(x)]$ 和预测方差 $\\operatorname{Var}[f(x)]$，然后通过检查不同测试用例的方差值，阐述认知不确定性（epistemic uncertainty）如何随丢弃率 $p$ 变化。\n\n推导算法的基础：\n- 实值随机变量 $Z$ 的期望是 $\\mathbb{E}[Z]$。\n- 实值随机变量 $Z$ 的方差是 $\\operatorname{Var}(Z) = \\mathbb{E}\\left[(Z - \\mathbb{E}[Z])^2\\right]$。\n- 蒙特卡洛估计使用独立样本 $Z_1, Z_2, \\dots, Z_T$ 通过样本均值来近似 $\\mathbb{E}[Z]$，通过样本方差来近似 $\\operatorname{Var}(Z)$。\n\n网络定义：\n- 大小为 $H=3$ 的隐藏层，其参数为\n$$\n\\mathbf{W}_1 =\n\\begin{bmatrix}\n1.0  -0.5 \\\\\n0.3  0.8 \\\\\n-0.7  0.2\n\\end{bmatrix}, \\quad\n\\mathbf{b}_1 =\n\\begin{bmatrix}\n0.1 \\\\\n-0.2 \\\\\n0.0\n\\end{bmatrix}.\n$$\n- 输出层参数\n$$\n\\mathbf{W}_2 =\n\\begin{bmatrix}\n0.5 \\\\\n-1.0 \\\\\n0.3\n\\end{bmatrix}, \\quad\nb_2 = 0.05.\n$$\n- ReLU 激活函数定义为 $\\operatorname{ReLU}(z) = \\max(z, 0)$，并按元素应用。\n- 对激活后的隐藏向量 $h = \\operatorname{ReLU}(\\mathbf{W}_1 x + \\mathbf{b}_1)$ 应用丢弃率为 $p \\in [0,1)$ 的隐藏层丢弃。使用倒置丢弃（inverted dropout），丢弃后的隐藏向量为\n$$\n\\tilde{h} = \\begin{cases}\nh,  \\text{if } p = 0, \\\\\n\\frac{m \\odot h}{1-p},  \\text{if } p  0,\n\\end{cases}\n$$\n其中 $m \\in \\{0,1\\}^H$ 是一个随机掩码，其条目 $m_i$ 独立服从伯努利分布 $\\operatorname{Bernoulli}(1-p)$，$\\odot$ 表示逐元素乘法。网络输出为\n$$\nf(x) = \\mathbf{W}_2^\\top \\tilde{h} + b_2.\n$$\n\n蒙特卡洛丢弃设置：\n- 对于给定的输入 $x$、丢弃率 $p$ 和蒙特卡洛样本数量 $T \\in \\mathbb{N}$，通过独立抽取掩码 $m$ 并计算 $f(x)$，生成 $T$ 个 $f(x)$ 的独立样本。\n- 通过 $T$ 个输出的样本均值来近似 $\\mathbb{E}[f(x)]$。\n- 通过 $T$ 个输出的样本方差来近似 $\\operatorname{Var}[f(x)]$。\n\n任务：\n- 实现一个程序，对指定的测试套件执行上述蒙特卡洛过程。\n- 您的程序不能训练网络；必须使用上面给出的固定参数。\n- 您的输出必须是浮点数。本问题不涉及物理单位。\n\n测试套件：\n为以下的 $(x, p, T)$ 三元组计算预测均值和方差。每个 $x$ 以二维向量的形式给出。请使用所示的精确数值。\n\n1. $x = \\begin{bmatrix} 0.5 \\\\ -1.0 \\end{bmatrix}$，$p = 0.0$，$T = 100$。\n2. $x = \\begin{bmatrix} 0.5 \\\\ -1.0 \\end{bmatrix}$，$p = 0.2$，$T = 1000$。\n3. $x = \\begin{bmatrix} 2.0 \\\\ 1.0 \\end{bmatrix}$，$p = 0.5$，$T = 1000$。\n4. $x = \\begin{bmatrix} 2.0 \\\\ 1.0 \\end{bmatrix}$，$p = 0.9$，$T = 2000$。\n5. $x = \\begin{bmatrix} -1.5 \\\\ 0.3 \\end{bmatrix}$，$p = 0.5$，$T = 25$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。\n- 每个测试用例的结果本身必须是一个包含两个浮点数的列表，顺序为 $[\\text{均值}, \\text{方差}]$，对应于该用例的 $\\mathbb{E}[f(x)]$ 和 $\\operatorname{Var}[f(x)]$。\n- 例如，$K$ 个测试用例的有效输出格式为\n$$\n\\text{[}[m_1,v_1],[m_2,v_2],\\dots,[m_K,v_K]\\text{]}.\n$$\n\n覆盖性设计：\n- 用例 1 是一个边界条件，$p = 0.0$（无丢弃），应表现出零方差。\n- 用例 2 和 3 是具有中等丢弃率的一般情况。\n- 用例 4 探索了高丢弃率（$p = 0.9$）的情况，以说明认知不确定性的增加。\n- 用例 5 使用了少量的样本（$T = 25$），以说明蒙特卡洛估计中的抽样变异性。\n\n您的实现必须通过为蒙特卡洛抽样使用固定的随机种子来确保每次运行结果的确定性。",
            "solution": "我们从概率论的核心定义开始。对于一个实值随机变量 $Z$，其期望是 $\\mathbb{E}[Z]$，方差是 $\\operatorname{Var}(Z) = \\mathbb{E}\\left[(Z - \\mathbb{E}[Z])^2\\right]$。当 $Z$ 的精确分布难以处理时，蒙特卡洛方法使用独立样本来近似这些量。给定独立样本 $Z_1, Z_2, \\dots, Z_T$，样本均值 $\\hat{\\mu}$ 和样本方差 $\\hat{\\sigma}^2$ 提供了相合估计量：\n$$\n\\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^T Z_t, \\quad\n\\hat{\\sigma}^2 \\approx \\frac{1}{T-1} \\sum_{t=1}^T (Z_t - \\hat{\\mu})^2.\n$$\n无偏样本方差使用分母 $T-1$，并且当 $T \\to \\infty$ 时收敛于 $\\operatorname{Var}(Z)$。\n\n在深度学习中，丢弃（dropout）操作通过在训练期间随机将隐藏单元置零来引入随机性。蒙特卡洛丢弃在推理时应用相同的随机性来近似贝叶斯预测，将输出的可变性归因于认知不确定性（由于数据有限或参数不确定性而产生的模型不确定性）。倒置丢弃（Inverted dropout）将保留的激活值按 $\\frac{1}{1-p}$ 进行缩放，以使期望激活值保持不变。具体来说，对于隐藏激活向量 $h = \\operatorname{ReLU}(\\mathbf{W}_1 x + \\mathbf{b}_1)$ 和一个掩码 $m \\in \\{0,1\\}^H$（其条目 $m_i$ 独立服从伯努利分布 $\\operatorname{Bernoulli}(1-p)$），丢弃后的隐藏向量为\n$$\n\\tilde{h} = \\frac{m \\odot h}{1-p} \\quad \\text{for } p  0, \\quad \\tilde{h} = h \\quad \\text{for } p = 0.\n$$\n然后，标量输出为\n$$\nf(x) = \\mathbf{W}_2^\\top \\tilde{h} + b_2.\n$$\n在丢弃的作用下，由于 $m$ 的随机性，$f(x)$ 成为一个随机变量。我们通过抽取 $T$ 个独立的掩码 $m^{(1)}, \\dots, m^{(T)}$ 并计算输出 $f^{(t)}(x)$，然后构成样本均值和样本方差，来估计 $\\mathbb{E}[f(x)]$ 和 $\\operatorname{Var}[f(x)]$。\n\n算法步骤：\n1. 固定网络参数 $\\mathbf{W}_1$, $\\mathbf{b}_1$, $\\mathbf{W}_2$, $b_2$ 为指定值。\n2. 对于每个测试用例 $(x, p, T)$：\n   - 计算确定性的隐藏激活值 $h = \\operatorname{ReLU}(\\mathbf{W}_1 x + \\mathbf{b}_1)$。\n   - 如果 $p = 0$，那么对所有样本 $\\tilde{h} = h$，因此 $f(x)$ 是确定性的；$\\operatorname{Var}[f(x)] = 0$。\n   - 如果 $p  0$，对 $t = 1, \\dots, T$ 重复以下步骤：\n     - 抽取一个掩码 $m^{(t)} \\in \\{0,1\\}^H$，其中各条目独立 $m_i^{(t)} \\sim \\operatorname{Bernoulli}(1-p)$。\n     - 构造 $\\tilde{h}^{(t)} = \\frac{m^{(t)} \\odot h}{1-p}$。\n     - 计算 $f^{(t)}(x) = \\mathbf{W}_2^\\top \\tilde{h}^{(t)} + b_2$。\n   - 通过 $\\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^T f^{(t)}(x)$ 估计 $\\mathbb{E}[f(x)]$，通过无偏样本方差 $\\hat{\\sigma}^2 = \\frac{1}{T-1} \\sum_{t=1}^T (f^{(t)}(x) - \\hat{\\mu})^2$ 估计 $\\operatorname{Var}[f(x)]$（对于 $T \\geq 2$；对于 $p=0$，所有样本都相同，方差为 $0$）。\n3. 将所有测试用例的结果聚合成一个单一列表，并遵循指定的输出格式。\n\n认知不确定性与丢弃率 $p$ 的关系：\n- 认知不确定性反映了模型的不确定性。丢弃会随机地移除隐藏单元。随着 $p$ 的增加，将单元置零的概率增加，这会导致通过掩码抽样的不同网络实现之间产生更高的可变性。\n- 使用倒置丢弃时，保留的激活值会按 $\\frac{1}{1-p}$ 进行缩放。对于大的 $p$（接近 1），当少数激活单元被保留时，这个因子会放大它们的贡献，而大多数样本则将它们置零。这种混合情况（罕见的大贡献与频繁的零）增加了 $f(x)$ 的方差。\n- 因此，在其他条件相同的情况下，预测方差 $\\operatorname{Var}[f(x)]$通常随 $p$ 的增加而增加，这表明认知不确定性更大。\n\n测试套件覆盖性说明：\n- 用例 1 ($p=0$) 产生零方差，因为 $m_i \\equiv 1$，不存在随机性。\n- 用例 2 和 3 是具有中等 $p$ 和大 $T$ 的典型蒙特卡洛丢弃估计，可以提供对 $\\mathbb{E}[f(x)]$ 和 $\\operatorname{Var}[f(x)]$ 的精确估计。\n- 用例 4 使用了高丢弃率 $p=0.9$ 和更大的 $T$，以稳定地估计由强丢弃引起的方差增加。\n- 用例 5 展示了小样本量 $T=25$ 对估计值的影响，强调了抽样变异性。\n\n程序将实现这些步骤，使用固定的随机种子以确保每次运行结果的确定性，并将按顺序为每个测试用例打印包含 $[\\text{均值}, \\text{方差}]$ 列表的单行输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef relu(x):\n    return np.maximum(x, 0.0)\n\ndef mc_dropout_predict(x, p, T, rng, W1, b1, W2, b2):\n    \"\"\"\n    Perform Monte Carlo dropout predictions for a single input x.\n    Returns an array of T scalar outputs.\n    \"\"\"\n    # Compute deterministic hidden activation h = ReLU(W1 x + b1)\n    a = W1 @ x + b1\n    h = relu(a)\n\n    # If p == 0, output is deterministic; replicate the same value T times\n    if p == 0.0:\n        y = float(W2 @ h + b2)\n        return np.full(T, y, dtype=float)\n\n    keep_prob = 1.0 - p\n    scale = 1.0 / keep_prob\n\n    # Sample T masks and compute outputs\n    outputs = np.empty(T, dtype=float)\n    for t in range(T):\n        # Bernoulli(keep_prob) for each hidden unit\n        m = rng.binomial(1, keep_prob, size=h.shape)\n        h_drop = (m * h) * scale\n        y = float(W2 @ h_drop + b2)\n        outputs[t] = y\n    return outputs\n\ndef solve():\n    # Fixed random seed for determinism\n    rng = np.random.default_rng(42)\n\n    # Define fixed network parameters\n    W1 = np.array([[1.0, -0.5],\n                   [0.3,  0.8],\n                   [-0.7, 0.2]], dtype=float)\n    b1 = np.array([0.1, -0.2, 0.0], dtype=float)\n    W2 = np.array([0.5, -1.0, 0.3], dtype=float)\n    b2 = 0.05\n\n    # Define the test cases from the problem statement.\n    # Each test case is (x, p, T)\n    test_cases = [\n        (np.array([0.5, -1.0], dtype=float), 0.0, 100),\n        (np.array([0.5, -1.0], dtype=float), 0.2, 1000),\n        (np.array([2.0,  1.0], dtype=float), 0.5, 1000),\n        (np.array([2.0,  1.0], dtype=float), 0.9, 2000),\n        (np.array([-1.5, 0.3], dtype=float), 0.5, 25),\n    ]\n\n    results = []\n    for x, p, T in test_cases:\n        outputs = mc_dropout_predict(x, p, T, rng, W1, b1, W2, b2)\n        mean = float(np.mean(outputs))\n        # Unbiased sample variance; for T=1, set variance to 0.0 to avoid NaN\n        var = float(np.var(outputs, ddof=1)) if T > 1 else 0.0\n        # Round for stable presentation while keeping numeric type\n        mean_rounded = round(mean, 6)\n        var_rounded = round(var, 6)\n        results.append([mean_rounded, var_rounded])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(res) for res in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "随着深度网络的训练，其内部表征通常会组织成一种非常简单的几何结构，这种现象被称为“神经坍塌”。这个高级练习将让您通过分析学习到的特征的协方差结构来模拟和量化这一现象。通过将变异性分解为类内和类间分量，您将看到如何运用基本的统计工具来描述深度学习系统中复杂的涌现特性 。",
            "id": "3123405",
            "problem": "您的任务是设计并实现一个程序，用于量化在监督式深度学习表示中普遍观察到的一种现象，称为神经坍塌。重点在于追踪类内协方差的收缩和类别均值的类间协方差的增长，并使用在特定参数缩放条件下会趋近于一个常数的比率度量来衡量坍塌的程度。您的程序必须仅使用期望、方差和协方差的定义，从第一性原理出发计算这些量。\n\n考虑一个随机向量 $\\mathbf{X} \\in \\mathbb{R}^D$ 和一个离散类别标签 $Y \\in \\{1,\\dots,C\\}$。对于每个类别 $c \\in \\{1,\\dots,C\\}$，您将根据一个带有各向同性高斯噪声和类别相关均值的类条件模型来模拟数据，具体如下。\n\n数据生成过程：\n- 对于给定的整数 $C$（类别数）、$D$（维度）和 $n$（每类样本数），以及实数标准差 $\\sigma \\ge 0$ 和实数均值尺度 $\\gamma  0$，通过以下方式构建类别均值 $\\boldsymbol{\\mu}_1,\\dots,\\boldsymbol{\\mu}_C \\in \\mathbb{R}^D$：\n  1. 在 $\\mathbb{R}^D$ 中采样 $C$ 个独立同分布的标准正态向量，将它们作为矩阵的列，然后通过减去它们的列平均值来中心化这些列，使它们在所有类别上的平均值为零。\n  2. 将每个中心化后的列归一化为单位欧几里得范数，然后缩放使其欧几里得范数为 $\\gamma$，从而对所有 $c$ 都有 $\\|\\boldsymbol{\\mu}_c\\|_2 = \\gamma$。\n- 对于每个类别 $c$，根据以下模型独立采样 $n$ 个观测值\n  $$ \\mathbf{X} \\mid Y=c = \\boldsymbol{\\mu}_c + \\boldsymbol{\\varepsilon}, \\quad \\text{其中 } \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_D). $$\n\n经验估计任务：\n- 为每个类别 $c$ 计算经验类别均值：\n  $$ \\widehat{\\boldsymbol{\\mu}}_c = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_{c,i}, $$\n  其中 $\\mathbf{x}_{c,i}$ 是为类别 $c$ 采样的观测值。\n- 使用分母为 $n$ 的经验期望算子，为每个类别 $c$ 计算经验类内协方差：\n  $$ \\widehat{\\mathbf{\\Sigma}}_{w,c} = \\frac{1}{n} \\sum_{i=1}^n \\left(\\mathbf{x}_{c,i} - \\widehat{\\boldsymbol{\\mu}}_c\\right)\\left(\\mathbf{x}_{c,i} - \\widehat{\\boldsymbol{\\mu}}_c\\right)^\\top. $$\n- 计算总体经验类内协方差，作为所有类别的平均值：\n  $$ \\widehat{\\mathbf{S}}_w = \\frac{1}{C} \\sum_{c=1}^C \\widehat{\\mathbf{\\Sigma}}_{w,c}. $$\n- 计算类别均值的经验均值：\n  $$ \\widehat{\\boldsymbol{\\mu}} = \\frac{1}{C} \\sum_{c=1}^C \\widehat{\\boldsymbol{\\mu}}_c. $$\n- 计算类别均值的经验类间协方差：\n  $$ \\widehat{\\mathbf{S}}_b = \\frac{1}{C} \\sum_{c=1}^C \\left(\\widehat{\\boldsymbol{\\mu}}_c - \\widehat{\\boldsymbol{\\mu}}\\right)\\left(\\widehat{\\boldsymbol{\\mu}}_c - \\widehat{\\boldsymbol{\\mu}}\\right)^\\top. $$\n\n坍塌比率度量：\n- 计算以下两个标量度量：\n  1. 迹比率\n     $$ r_{\\mathrm{trace}} = \\frac{\\operatorname{tr}\\left(\\widehat{\\mathbf{S}}_b\\right)}{\\operatorname{tr}\\left(\\widehat{\\mathbf{S}}_w\\right)}. $$\n  2. 弗罗贝尼乌斯范数比率\n     $$ r_{\\mathrm{fro}} = \\frac{\\left\\|\\widehat{\\mathbf{S}}_b\\right\\|_F}{\\left\\|\\widehat{\\mathbf{S}}_w\\right\\|_F}, $$\n     其中 $\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} a_{ij}^2}$ 表示弗罗贝尼乌斯范数。\n\n边界情况：\n- 如果 $\\operatorname{tr}\\left(\\widehat{\\mathbf{S}}_w\\right) = 0$ 或 $\\left\\|\\widehat{\\mathbf{S}}_w\\right\\|_F = 0$，则将相应的比率定义为 $+\\infty$。\n\n您的程序必须完全按照描述实现数据生成和估计，并使用指定的种子以确保可复现性。它必须处理一套参数值的测试用例，为每个用例计算两个比率，并生成一行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。每个结果都必须是一个严格按 $[r_{\\mathrm{trace}}, r_{\\mathrm{fro}}]$ 顺序排列的双元素列表。因此，最终输出格式必须如下所示：\n$$ [[r_{\\mathrm{trace}}^{(1)}, r_{\\mathrm{fro}}^{(1)}],[r_{\\mathrm{trace}}^{(2)}, r_{\\mathrm{fro}}^{(2)}],\\dots]. $$\n\n测试用例集：\n- 用例 1 (正常路径)：种子 $0$， $C=5$，$D=10$，$n=200$，$\\sigma=1.0$，$\\gamma=2.0$。\n- 用例 2 (等比率缩放)：种子 $0$， $C=5$，$D=10$，$n=200$，$\\sigma=2.0$，$\\gamma=4.0$。\n- 用例 3 (高噪声，低分离度)：种子 $1$，$C=5$，$D=10$，$n=200$，$\\sigma=3.0$，$\\gamma=1.0$。\n- 用例 4 (低噪声，高分离度)：种子 $2$，$C=5$，$D=10$，$n=200$，$\\sigma=0.1$，$\\gamma=3.0$。\n- 用例 5 (边界情况，零类内方差)：种子 $3$，$C=3$，$D=4$，$n=50$，$\\sigma=0.0$，$\\gamma=1.5$。\n\n最终输出规范：\n- 您的程序应生成一行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。列表中的每个元素本身是一个双元素列表，包含相应测试用例的 $[r_{\\mathrm{trace}}, r_{\\mathrm{fro}}]$ 值，其顺序与测试用例集中的顺序相同。",
            "solution": "该问题是有效的。它在科学上基于深度学习表示的统计建模，其定义和参数完整一致，问题陈述清晰明确，并且其表述是客观的。我们将着手提供一个解决方案。\n\n目标是模拟并量化神经坍塌现象，该现象描述了在有监督的深度网络训练过程中，最后一层特征表示的一种特定几何构型。该现象具有以下特征：\n1. 单个类别内部的特征变异性坍塌，意味着类内协方差矩阵收缩。\n2. 不同类别特征的经验均值变得最大程度地分离且等距，形成一个单纯形等角紧框架。这导致了结构化且大的类间协方差。\n\n该问题提供了一个简化的生成模型来研究这一现象。我们将实现这个模型，并计算两个关键比率来衡量坍塌的程度：类间与类内协方差矩阵的迹之比和弗罗贝尼乌斯范数之比。\n\n解决方案的结构如下：\n1.  **数据生成**：对于每个测试用例，我们首先生成类别原型向量，然后为每个类别采样数据点。\n2.  **经验协方差估计**：使用生成的数据，我们根据基本定义计算经验类内和类间协方差矩阵。\n3.  **度量计算**：我们根据估计的协方差矩阵计算迹比率和弗罗贝尼乌斯范数比率。\n\n### 1. 数据生成\n\n该过程首先构建真实类别均值 $\\boldsymbol{\\mu}_c \\in \\mathbb{R}^D$ (其中 $c \\in \\{1,\\ldots,C\\}$)。这些是每个类别的理想化特征表示。\n- 首先，我们从一个 $D$ 维标准正态分布中采样 $C$ 个向量，$\\mathbf{v}_c \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_D)$。这些向量被排列成一个矩阵 $\\mathbf{V} \\in \\mathbb{R}^{D \\times C}$ 的列。\n- 从每个向量中减去这些向量的总均值 $\\bar{\\mathbf{v}} = \\frac{1}{C} \\sum_{c=1}^C \\mathbf{v}_c$，以确保得到的均值集是中心化的：$\\mathbf{v}'_c = \\mathbf{v}_c - \\bar{\\mathbf{v}}$。这保证了 $\\sum_{c=1}^C \\mathbf{v}'_c = \\mathbf{0}$。\n- 每个中心化后的向量 $\\mathbf{v}'_c$ 被归一化，使其欧几里得范数为 $\\gamma$。这是通过先将其归一化为单位长度，然后进行缩放来实现的：\n  $$ \\boldsymbol{\\mu}_c = \\gamma \\frac{\\mathbf{v}'_c}{\\|\\mathbf{v}'_c\\|_2} $$\n  这确保了对所有类别 $c$ 都有 $\\|\\boldsymbol{\\mu}_c\\|_2 = \\gamma$。\n\n在确定了真实类别均值后，我们为每个类别模拟 $n$ 个数据点。每个数据点 $\\mathbf{x}_{c,i}$ 都是从一个类条件高斯分布中采样的：\n$$ \\mathbf{x}_{c,i} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_c, \\sigma^2 \\mathbf{I}_D) $$\n这等同于在真实类别均值上添加各向同性高斯噪声 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_D)$：$\\mathbf{x}_{c,i} = \\boldsymbol{\\mu}_c + \\boldsymbol{\\varepsilon}_{c,i}$。\n\n### 2. 经验协方差估计\n\n根据为每个类别 $c$ 生成的数据样本 $\\{\\mathbf{x}_{c,i}\\}_{i=1}^n$，我们计算各种统计量的经验估计值。\n\n- **经验类别均值**：通过对其样本进行平均来估计每个类别的均值向量：\n  $$ \\widehat{\\boldsymbol{\\mu}}_c = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_{c,i} $$\n\n- **类内协方差**：每个类别内部特征的协方差 $\\widehat{\\mathbf{\\Sigma}}_{w,c}$ 衡量了数据点围绕其经验类别均值 $\\widehat{\\boldsymbol{\\mu}}_c$ 的散布情况。它使用分母为 $n$ 的样本协方差公式计算：\n  $$ \\widehat{\\mathbf{\\Sigma}}_{w,c} = \\frac{1}{n} \\sum_{i=1}^n \\left(\\mathbf{x}_{c,i} - \\widehat{\\boldsymbol{\\mu}}_c\\right)\\left(\\mathbf{x}_{c,i} - \\widehat{\\boldsymbol{\\mu}}_c\\right)^\\top $$\n  总体类内协方差 $\\widehat{\\mathbf{S}}_w$ 是这些单个协方差矩阵在所有类别上的平均值：\n  $$ \\widehat{\\mathbf{S}}_w = \\frac{1}{C} \\sum_{c=1}^C \\widehat{\\mathbf{\\Sigma}}_{w,c} $$\n\n- **类间协方差**：这个度量 $\\widehat{\\mathbf{S}}_b$ 量化了*经验类别均值* $\\widehat{\\boldsymbol{\\mu}}_c$ 围绕它们自身总均值 $\\widehat{\\boldsymbol{\\mu}} = \\frac{1}{C} \\sum_{c=1}^C \\widehat{\\boldsymbol{\\mu}}_c$ 的散布情况。其公式为：\n  $$ \\widehat{\\mathbf{S}}_b = \\frac{1}{C} \\sum_{c=1}^C \\left(\\widehat{\\boldsymbol{\\mu}}_c - \\widehat{\\boldsymbol{\\mu}}\\right)\\left(\\widehat{\\boldsymbol{\\mu}}_c - \\widehat{\\boldsymbol{\\mu}}\\right)^\\top $$\n\n### 3. 坍塌比率度量\n\n神经坍塌的程度通过比较类间协方差与类内协方差的“大小”来量化。我们使用两个标量度量来衡量这些矩阵的大小：迹和弗罗贝尼乌斯范数。\n\n- **迹比率**：方阵的迹是其对角线元素之和，$\\operatorname{tr}(\\mathbf{A}) = \\sum_i A_{ii}$。对于协方差矩阵，迹代表总方差，即沿每个维度的方差之和。迹比率为：\n  $$ r_{\\mathrm{trace}} = \\frac{\\operatorname{tr}\\left(\\widehat{\\mathbf{S}}_b\\right)}{\\operatorname{tr}\\left(\\widehat{\\mathbf{S}}_w\\right)} $$\n  一个较大的值表明，类别均值之间的分离度相对于每个类别内数据的散布程度要大。\n\n- **弗罗贝尼乌斯范数比率**：矩阵 $\\mathbf{A}$ 的弗罗贝尼乌斯范数是其所有元素平方和的平方根，$\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} a_{ij}^2}$。它提供了衡量矩阵整体大小的另一种方法。弗罗贝尼乌斯范数比率为：\n  $$ r_{\\mathrm{fro}} = \\frac{\\left\\|\\widehat{\\mathbf{S}}_b\\right\\|_F}{\\left\\|\\widehat{\\mathbf{S}}_w\\right\\|_F} $$\n\n- **边界情况**：如果类内噪声标准差 $\\sigma$ 为 $0$，则一个类别的所有样本都与其类别均值相同，即 $\\mathbf{x}_{c,i} = \\boldsymbol{\\mu}_c$。因此，经验类别均值也就是真实均值，$\\widehat{\\boldsymbol{\\mu}}_c = \\boldsymbol{\\mu}_c$。类内偏差 $(\\mathbf{x}_{c,i} - \\widehat{\\boldsymbol{\\mu}}_c)$ 全为零，使得类内协方差矩阵 $\\widehat{\\mathbf{S}}_w$ 成为零矩阵。在这种情况下，其迹和弗罗贝尼乌斯范数均为 $0$。根据规定，比率 $r_{\\mathrm{trace}}$ 和 $r_{\\mathrm{fro}}$ 被定义为 $+\\infty$。\n\n该实现将为测试用例集中提供的每组参数系统地执行这些步骤，并通过为每次运行设置指定的随机种子来确保可复现性。",
            "answer": "```python\nimport numpy as np\n# scipy is not strictly needed as numpy provides all necessary functionalities.\n# from scipy import linalg # For example, though np.linalg is used here.\n\ndef solve():\n    \"\"\"\n    Simulates neural collapse phenomena and computes collapse ratio metrics.\n    \"\"\"\n    \n    # Test suite: (seed, C, D, n, sigma, gamma)\n    test_cases = [\n        (0, 5, 10, 200, 1.0, 2.0),\n        (0, 5, 10, 200, 2.0, 4.0),\n        (1, 5, 10, 200, 3.0, 1.0),\n        (2, 5, 10, 200, 0.1, 3.0),\n        (3, 3, 4, 50, 0.0, 1.5),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        seed, C, D, n, sigma, gamma = case\n        np.random.seed(seed)\n\n        # === 1. Data Generation ===\n        \n        # --- Generate true class means mu_c ---\n        # Sample C i.i.d. standard normal vectors\n        mu_raw = np.random.randn(D, C)\n        \n        # Center the columns by subtracting their mean vector\n        mu_mean = mu_raw.mean(axis=1, keepdims=True)\n        mu_centered = mu_raw - mu_mean\n        \n        # Normalize each column to unit norm\n        norms = np.linalg.norm(mu_centered, axis=0, keepdims=True)\n        # Avoid division by zero, though highly unlikely with continuous distributions\n        norms[norms == 0] = 1.0  \n        mu_unit_norm = mu_centered / norms\n        \n        # Scale to have norm gamma\n        mus = mu_unit_norm * gamma  # Shape (D, C)\n\n        # --- Sample data points for each class ---\n        data_by_class = []\n        for c in range(C):\n            mu_c = mus[:, c].reshape(1, D)  # Shape (1, D) for broadcasting\n            # Generate isotropic Gaussian noise\n            epsilon = np.random.randn(n, D) * sigma\n            # Generate class data\n            X_c = mu_c + epsilon  # Shape (n, D)\n            data_by_class.append(X_c)\n\n        # === 2. Empirical Covariance Estimation ===\n        \n        # --- Compute empirical class means hat_mu_c ---\n        hat_mus = np.array([X_c.mean(axis=0) for X_c in data_by_class]) # Shape (C, D)\n        \n        # --- Compute overall within-class covariance S_w ---\n        # List to store individual within-class covariance matrices\n        hat_Sigma_w_c_list = []\n        for c in range(C):\n            X_c = data_by_class[c]\n            hat_mu_c = hat_mus[c]\n            # Deviations from the class mean\n            diffs = X_c - hat_mu_c # Shape (n, D)\n            # Covariance for class c, using denominator n\n            hat_Sigma_w_c = (diffs.T @ diffs) / n # Shape (D, D)\n            hat_Sigma_w_c_list.append(hat_Sigma_w_c)\n        \n        # Average across classes to get S_w\n        hat_S_w = np.mean(np.array(hat_Sigma_w_c_list), axis=0) # Shape (D, D)\n\n        # --- Compute between-class covariance S_b ---\n        # Mean of empirical class means\n        hat_mu = hat_mus.mean(axis=0) # Shape (D,)\n        \n        # Deviations of class means from the grand mean\n        diffs_b = hat_mus - hat_mu # Shape (C, D)\n        \n        # Covariance of class means, using denominator C\n        hat_S_b = (diffs_b.T @ diffs_b) / C # Shape (D, D)\n        \n        # === 3. Metric Calculation ===\n        \n        tr_S_w = np.trace(hat_S_w)\n        tr_S_b = np.trace(hat_S_b)\n        \n        fro_S_w = np.linalg.norm(hat_S_w, 'fro')\n        fro_S_b = np.linalg.norm(hat_S_b, 'fro')\n\n        # Calculate ratios with division-by-zero handling\n        # If sigma is 0, S_w will be a zero matrix, tr_S_w and fro_S_w will be 0.\n        r_trace = tr_S_b / tr_S_w if tr_S_w != 0 else np.inf\n        r_fro = fro_S_b / fro_S_w if fro_S_w != 0 else np.inf\n        \n        results.append([r_trace, r_fro])\n\n    # Format output as a string representation of a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}