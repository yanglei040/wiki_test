{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp Kullback-Leibler divergence, we must begin with a direct calculation. This first exercise provides a concrete scenario with two simple, discrete probability distributions, allowing you to apply the $D_{KL}$ formula step-by-step. By working through this foundational problem , you will gain a hands-on feel for how KL divergence quantifies the 'surprise' or information loss when one distribution is used to approximate another.",
            "id": "1370233",
            "problem": "In the field of information theory and machine learning, a fundamental task is to measure the difference between two probability distributions. The Kullback-Leibler (KL) divergence, or relative entropy, provides such a measure, quantifying the information lost when one distribution is used to approximate another.\n\nConsider a categorical random variable that can take on one of three distinct outcomes, indexed by $i \\in \\{1, 2, 3\\}$. The true probability distribution governing these outcomes is given by the distribution $P$, where the probabilities for the outcomes are $P(1) = \\frac{1}{2}$, $P(2) = \\frac{1}{4}$, and $P(3) = \\frac{1}{4}$.\n\nAn analyst proposes a simplified model, described by the probability distribution $Q$, where the corresponding probabilities are $Q(1) = \\frac{2}{5}$, $Q(2) = \\frac{2}{5}$, and $Q(3) = \\frac{1}{5}$.\n\nThe KL divergence of $Q$ from $P$, which measures the inefficiency of assuming the distribution is $Q$ when the true distribution is $P$, is denoted by $D_{KL}(P || Q)$ and defined as:\n$$\nD_{KL}(P || Q) = \\sum_{i=1}^{3} P(i) \\log \\left( \\frac{P(i)}{Q(i)} \\right)\n$$\nUsing the natural logarithm (base $e$), calculate the numerical value for $D_{KL}(P || Q)$. Round your final answer to four significant figures.",
            "solution": "We use the definition\n$$\nD_{KL}(P\\|Q)=\\sum_{i=1}^{3}P(i)\\ln\\!\\left(\\frac{P(i)}{Q(i)}\\right).\n$$\nGiven $P(1)=\\frac{1}{2}$, $P(2)=\\frac{1}{4}$, $P(3)=\\frac{1}{4}$ and $Q(1)=\\frac{2}{5}$, $Q(2)=\\frac{2}{5}$, $Q(3)=\\frac{1}{5}$, compute each ratio:\n$$\n\\frac{P(1)}{Q(1)}=\\frac{\\frac{1}{2}}{\\frac{2}{5}}=\\frac{5}{4},\\quad\n\\frac{P(2)}{Q(2)}=\\frac{\\frac{1}{4}}{\\frac{2}{5}}=\\frac{5}{8},\\quad\n\\frac{P(3)}{Q(3)}=\\frac{\\frac{1}{4}}{\\frac{1}{5}}=\\frac{5}{4}.\n$$\nThus,\n$$\nD_{KL}(P\\|Q)=\\frac{1}{2}\\ln\\!\\left(\\frac{5}{4}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{5}{8}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{5}{4}\\right)\n=\\frac{3}{4}\\ln\\!\\left(\\frac{5}{4}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{5}{8}\\right).\n$$\nCombine using logarithm properties:\n$$\nD_{KL}(P\\|Q)=\\frac{1}{4}\\left[3\\ln\\!\\left(\\frac{5}{4}\\right)+\\ln\\!\\left(\\frac{5}{8}\\right)\\right]\n=\\frac{1}{4}\\ln\\!\\left(\\left(\\frac{5}{4}\\right)^{3}\\cdot\\frac{5}{8}\\right)\n=\\frac{1}{4}\\ln\\!\\left(\\frac{625}{512}\\right).\n$$\nUsing $\\frac{625}{512}=\\frac{5^{4}}{2^{9}}$, we also have\n$$\nD_{KL}(P\\|Q)=\\frac{1}{4}\\left(4\\ln 5-9\\ln 2\\right)=\\ln 5-\\frac{9}{4}\\ln 2.\n$$\nNumerically, with natural logarithms,\n$$\n\\ln 5\\approx 1.6094379124,\\quad \\ln 2\\approx 0.6931471806,\n$$\nso\n$$\nD_{KL}(P\\|Q)\\approx 1.6094379124-\\frac{9}{4}\\times 0.6931471806\\approx 0.0498567562.\n$$\nRounding to four significant figures gives $0.04986$.",
            "answer": "$$\\boxed{0.04986}$$"
        },
        {
            "introduction": "Beyond simply measuring the difference between two given distributions, KL divergence serves as a powerful tool for optimization. In this practice , you will find the 'best' possible approximation of a uniform distribution using a geometric distribution by minimizing the KL divergence between them. This exercise illustrates a core principle in machine learning: framing a modeling problem as the minimization of a divergence to find optimal parameters.",
            "id": "1370268",
            "problem": "An engineer is tasked with modeling a process that generates integers. Empirical data suggests that the process produces integers uniformly from the set $\\{1, 2, \\dots, N\\}$, where $N$ is a known positive integer. Let this true distribution be denoted by the probability mass function (PMF) $P(k)$.\n\nFor reasons of analytic and computational simplicity, the engineer wants to approximate this uniform distribution with a geometric distribution, denoted by the PMF $Q(k)$. The geometric distribution's PMF is defined on the set of positive integers $\\{1, 2, 3, \\dots\\}$ as $Q(k|p) = p(1-p)^{k-1}$, where $p$ is the parameter representing the probability of success, with $0 < p < 1$.\n\nTo find the best possible approximation, the engineer decides to select the parameter $p$ that minimizes the information loss from using $Q$ in place of $P$. This loss is quantified by the Kullback-Leibler (KL) divergence, defined as:\n$$\nD_{\\text{KL}}(P || Q) = \\sum_{k} P(k) \\ln\\left(\\frac{P(k)}{Q(k|p)}\\right)\n$$\nwhere the sum is taken over all integers $k$ for which $P(k) > 0$, and $\\ln$ denotes the natural logarithm.\n\nDetermine the value of the parameter $p$ that minimizes this KL divergence. Express your answer as a single closed-form analytic expression in terms of $N$.",
            "solution": "Let $P$ be the uniform PMF on $\\{1,\\dots,N\\}$, so $P(k)=\\frac{1}{N}$ for $k=1,\\dots,N$ and $0$ otherwise. Let $Q(k|p)=p(1-p)^{k-1}$ for $k\\in\\{1,2,\\dots\\}$. The Kullback-Leibler divergence from $P$ to $Q$ is\n$$\nD_{\\text{KL}}(P\\|Q)=\\sum_{k=1}^{N}\\frac{1}{N}\\ln\\!\\left(\\frac{\\frac{1}{N}}{p(1-p)^{\\,k-1}}\\right)\n=\\frac{1}{N}\\sum_{k=1}^{N}\\left[\\ln\\!\\left(\\frac{1}{N}\\right)-\\ln p-(k-1)\\ln(1-p)\\right].\n$$\nEvaluating the average, use $\\sum_{k=1}^{N}(k-1)=\\frac{N(N-1)}{2}$ to obtain\n$$\nD_{\\text{KL}}(P\\|Q)=\\ln\\!\\left(\\frac{1}{N}\\right)-\\ln p-\\left(\\frac{N-1}{2}\\right)\\ln(1-p).\n$$\nTo minimize with respect to $p\\in(0,1)$, differentiate and set to zero:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}p}D_{\\text{KL}}(P\\|Q)=-\\frac{1}{p}-\\left(\\frac{N-1}{2}\\right)\\frac{\\mathrm{d}}{\\mathrm{d}p}\\ln(1-p)\n=-\\frac{1}{p}+\\frac{N-1}{2(1-p)}=0.\n$$\nSolving for $p$ gives\n$$\n\\frac{N-1}{2(1-p)}=\\frac{1}{p}\\quad\\Longrightarrow\\quad (N-1)p=2(1-p)\\quad\\Longrightarrow\\quad p(N+1)=2\\quad\\Longrightarrow\\quad p=\\frac{2}{N+1}.\n$$\nThe second derivative,\n$$\n\\frac{\\mathrm{d}^{2}}{\\mathrm{d}p^{2}}D_{\\text{KL}}(P\\|Q)=\\frac{1}{p^{2}}+\\frac{N-1}{2(1-p)^{2}},\n$$\nis positive for $p\\in(0,1)$, ensuring this stationary point is the unique minimizer. Therefore, the value of $p$ that minimizes $D_{\\text{KL}}(P\\|Q)$ is $p=\\frac{2}{N+1}$.",
            "answer": "$$\\boxed{\\frac{2}{N+1}}$$"
        },
        {
            "introduction": "Now, let's apply KL divergence to a practical problem in deep learning: understanding and interpreting a model's behavior. This exercise  tasks you with building a saliency map, a technique used to visualize which parts of an input image are most important for a classifier's decision. By systematically occluding parts of the image and measuring the change in the model's output distribution using KL divergence, you will gain hands-on experience with a fundamental method for model interpretability.",
            "id": "3140423",
            "problem": "You are given a discrete multi-class classifier defined by a linear transformation followed by the softmax function and a procedure to occlude a rectangular region in an input image. The goal is to quantify the sensitivity of the model’s predictions to input occlusions by computing, for each occlusion location, a saliency value based on the Kullback-Leibler divergence between the original predicted distribution and the occluded predicted distribution. You will then aggregate these saliency values into simple metrics per test case.\n\nStart from the following fundamental bases:\n- The softmax function maps a real-valued vector of logits to a probability distribution over classes by normalizing exponentials so that all entries are nonnegative and sum to one.\n- The Kullback-Leibler divergence is a measure of the discrepancy between two probability distributions defined on the same discrete support, derived from principles of information theory.\n\nConsider an image as a real-valued matrix $x \\in \\mathbb{R}^{H \\times W}$ and a classifier parameterized by a weight matrix $\\mathbf{W} \\in \\mathbb{R}^{C \\times HW}$ and a bias vector $b \\in \\mathbb{R}^{C}$. The image is flattened in row-major order into a vector $v \\in \\mathbb{R}^{HW}$ using the mapping $v_{j} = x_{i,k}$ with $j = i \\cdot W + k$, where $i \\in \\{0,\\dots,H-1\\}$ and $k \\in \\{0,\\dots,W-1\\}$. The classifier computes logits $\\ell = \\mathbf{W} v + b$, and the predicted class distribution $p$ is the softmax of $\\ell$. To evaluate the sensitivity at a location $(r,c)$, you occlude a rectangular patch of size $P \\times Q$ whose top-left corner is at $(r,c)$ by replacing $x_{i,k}$ within that patch with a constant baseline $b_{\\text{mask}} \\in \\mathbb{R}$. Let $x^{(r,c)}$ denote the occluded image and $q^{(r,c)}$ its predicted distribution. Define the saliency at $(r,c)$ to be the Kullback-Leibler divergence from $p$ to $q^{(r,c)}$. The saliency map is the matrix of these values over all $(r,c)$ where the patch fits: $r \\in \\{0,\\dots,H-P\\}$ and $c \\in \\{0,\\dots,W-Q\\}$.\n\nYour tasks:\n1. Compute the original distribution $p$ from the unoccluded image.\n2. For each valid occlusion location $(r,c)$, compute $q^{(r,c)}$ and the corresponding saliency value.\n3. Aggregate the saliency map to produce the metrics: the maximum saliency value over all positions, the mean saliency value, and the coordinates $(r^{\\star},c^{\\star})$ of the first occurrence (in row-major order) of the maximum saliency.\n\nNotes:\n- Use row-major flattening and ensure numerical stability in computing the softmax.\n- No physical units, angles, or percentages are involved in this problem.\n- When multiple locations share the same maximum saliency, select $(r^{\\star},c^{\\star})$ to be the lexicographically smallest pair in row-major order, i.e., the first one encountered when scanning rows from top to bottom and columns from left to right.\n\nTest Suite:\nImplement your program for the following three test cases. In all cases, use row-major flattening for forming $v$ and assume the softmax is applied to logits to produce a strictly positive probability distribution.\n\n- Test Case 1 (general “happy path”):\n  - Dimensions: $H = 4$, $W = 4$, $C = 3$, patch sizes $P = 2$, $Q = 2$, baseline $b_{\\text{mask}} = 0.0$.\n  - Image:\n    $$\n    x = \\begin{bmatrix}\n    0.0 & 0.5 & 1.0 & -0.5 \\\\\n    1.0 & 0.0 & -1.0 & 0.5 \\\\\n    0.3 & -0.2 & 0.7 & 1.2 \\\\\n    0.0 & 0.0 & 0.5 & -0.3\n    \\end{bmatrix}.\n    $$\n  - Classifier parameters:\n    - Define the sequence of integers $s = [0,1,2,\\dots,15]$.\n    - Weights:\n      $$\n      \\mathbf{W}^{(0)} = 0.1 \\cdot s,\\quad \\mathbf{W}^{(1)} = -0.05 \\cdot s,\\quad \\mathbf{W}^{(2)} = 0.02 \\cdot s,\n      $$\n      where each $\\mathbf{W}^{(k)}$ is a row of $\\mathbf{W}$ and $\\mathbf{W} \\in \\mathbb{R}^{3 \\times 16}$.\n    - Bias:\n      $$\n      b = [0.1,\\,-0.2,\\,0.3].\n      $$\n\n- Test Case 2 (boundary case with no change under occlusion):\n  - Dimensions: $H = 3$, $W = 3$, $C = 2$, patch sizes $P = 1$, $Q = 1$, baseline $b_{\\text{mask}} = 0.5$.\n  - Image:\n    $$\n    x_{i,j} = 0.5 \\text{ for all } i \\in \\{0,1,2\\},\\; j \\in \\{0,1,2\\}.\n    $$\n  - Classifier parameters ($\\mathbf{W} \\in \\mathbb{R}^{2 \\times 9}$, $b \\in \\mathbb{R}^{2}$):\n    - Weights:\n      $$\n      \\mathbf{W}^{(0)} = [0.2,\\,-0.1,\\,0.1,\\,-0.05,\\,0.3,\\,0.0,\\,0.05,\\,-0.1,\\,0.02],\n      $$\n      $$\n      \\mathbf{W}^{(1)} = [-0.1,\\,0.15,\\,-0.2,\\,0.1,\\,-0.05,\\,0.2,\\,-0.05,\\,0.1,\\,-0.02].\n      $$\n    - Bias:\n      $$\n      b = [0.0,\\,0.0].\n      $$\n\n- Test Case 3 (edge case stressing strong localized weights):\n  - Dimensions: $H = 5$, $W = 5$, $C = 2$, patch sizes $P = 3$, $Q = 3$, baseline $b_{\\text{mask}} = 0.0$.\n  - Image:\n    $$\n    x_{i,j} = 1.0 \\text{ for all } i \\in \\{0,1,2,3,4\\},\\; j \\in \\{0,1,2,3,4\\}.\n    $$\n  - Classifier parameters ($\\mathbf{W} \\in \\mathbb{R}^{2 \\times 25}$, $b \\in \\mathbb{R}^{2}$):\n    - Weights for class $0$:\n      $$\n      \\mathbf{W}^{(0)}_{j} =\n      \\begin{cases}\n      2.0 & \\text{if the pixel corresponding to } j \\text{ lies in the top-left } 3\\times 3 \\text{ block}, \\\\\n      0.1 & \\text{otherwise},\n      \\end{cases}\n      $$\n      where $j$ indexes the flattened row-major order of the $5 \\times 5$ image.\n    - Weights for class $1$:\n      $$\n      \\mathbf{W}^{(1)}_{j} = -0.05 \\text{ for all } j.\n      $$\n    - Bias:\n      $$\n      b = [0.0,\\,0.0].\n      $$\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is itself a list of the form `[max_kl, mean_kl, r_star, c_star]`. For example, the output should look like `[[a_1, m_1, r_1, c_1], [a_2, m_2, r_2, c_2], [a_3, m_3, r_3, c_3]]`.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. It requires the computation of a saliency map for a simple linear classifier by occluding portions of an input image and measuring the change in the output probability distribution using the Kullback-Leibler (KL) divergence.\n\nThe solution proceeds as follows: First, we define the forward pass of the classifier, which transforms an input image into a class probability distribution. Second, we systematically occlude rectangular patches of the input image and compute the resulting perturbed probability distributions. Third, for each occlusion, we calculate the KL divergence between the original and perturbed distributions, which serves as our saliency measure. Finally, we aggregate these saliency values to find the maximum, mean, and the location of the maximum.\n\n**1. Classifier Forward Pass**\n\nThe classifier operates on an input image $x \\in \\mathbb{R}^{H \\times W}$, where $H$ is the height and $W$ is the width. The process involves three steps:\n\n- **Flattening**: The $2$D image matrix $x$ is converted into a $1$D vector $v \\in \\mathbb{R}^{HW}$ using row-major ordering. The element at image coordinates $(i,k)$ where $i \\in \\{0, \\dots, H-1\\}$ and $k \\in \\{0, \\dots, W-1\\}$ is mapped to the vector index $j = i \\cdot W + k$.\n\n- **Logit Calculation**: The flattened vector $v$ is passed through a linear layer defined by a weight matrix $W \\in \\mathbb{R}^{C \\times HW}$ and a bias vector $b \\in \\mathbb{R}^{C}$, where $C$ is the number of classes. This computes the logits vector $\\ell \\in \\mathbb{R}^{C}$:\n$$\n\\ell = W v + b\n$$\n\n- **Softmax Activation**: The logits are transformed into a probability distribution $p \\in \\mathbb{R}^{C}$ using the softmax function. For each class $i \\in \\{1, \\dots, C\\}$, the probability $p_i$ is given by:\n$$\np_i = \\text{softmax}(\\ell)_i = \\frac{e^{\\ell_i}}{\\sum_{k=1}^{C} e^{\\ell_k}}\n$$\nTo ensure numerical stability against overflow and underflow, a common practice is to subtract the maximum logit value from all logits before exponentiation:\n$$\np_i = \\frac{e^{\\ell_i - \\max(\\ell)}}{\\sum_{k=1}^{C} e^{\\ell_k - \\max(\\ell)}}\n$$\nThis transformation yields the same result but operates on a more stable numerical range.\n\n**2. Saliency via Occlusion and Kullback-Leibler Divergence**\n\nThe core of the analysis is to measure the model's sensitivity to parts of the input. This is achieved by occluding a patch and measuring the resulting change in the output distribution.\n\n- **Occlusion**: For each valid top-left coordinate $(r, c)$, where $r \\in \\{0, \\dots, H-P\\}$ and $c \\in \\{0, \\dots, W-Q\\}$, a rectangular patch of size $P \\times Q$ is occluded. Occlusion means replacing all pixel values $x_{i,k}$ within this patch (i.e., for $i \\in \\{r, \\dots, r+P-1\\}$ and $k \\in \\{c, \\dots, c+Q-1\\}$) with a constant baseline value $b_{\\text{mask}}$. This creates a new, occluded image $x^{(r,c)}$.\n\n- **Kullback-Leibler (KL) Divergence**: The occluded image $x^{(r,c)}$ is passed through the classifier to obtain a new probability distribution, $q^{(r,c)}$. The saliency of the occlusion at $(r,c)$ is defined as the KL divergence from the original distribution $p$ to the occluded distribution $q^{(r,c)}$. The formula for KL divergence between two discrete probability distributions $p$ and $q$ over the same support $\\{1, \\dots, C\\}$ is:\n$$\nD_{KL}(p || q) = \\sum_{i=1}^{C} p_i \\log \\left( \\frac{p_i}{q_i} \\right) = \\sum_{i=1}^{C} p_i (\\log p_i - \\log q_i)\n$$\nThe problem statement guarantees that the softmax output is a strictly positive distribution, ensuring that $q_i > 0$ for all $i$ and the logarithm is always well-defined.\n\n**3. Algorithmic Procedure**\n\nFor each test case, we perform the following steps:\n1.  Define the parameters for the test case: image dimensions $(H, W)$, class count $C$, patch dimensions $(P, Q)$, baseline value $b_{\\text{mask}}$, image matrix $x$, weight matrix $W$, and bias vector $b$.\n2.  Compute the original probability distribution $p$ by performing a forward pass on the unoccluded image $x$.\n3.  Initialize an empty saliency map, $S \\in \\mathbb{R}^{(H-P+1) \\times (W-Q+1)}$.\n4.  Iterate through each valid top-left occlusion coordinate $(r, c)$, from $r=0$ to $H-P$ and $c=0$ to $W-Q$.\n    a. Create a copy of the original image, let's call it $x_{\\text{occ}}$.\n    b. Modify $x_{\\text{occ}}$ by setting the submatrix from row $r$ to $r+P-1$ and column $c$ to $c+Q-1$ to the value $b_{\\text{mask}}$.\n    c. Perform a forward pass on the occluded image $x_{\\text{occ}}$ to compute the perturbed probability distribution $q^{(r,c)}$.\n    d. Calculate the saliency value $s_{r,c} = D_{KL}(p || q^{(r,c)})$.\n    e. Store this value in the saliency map: $S_{r,c} = s_{r,c}$.\n5.  After computing all saliency values, aggregate them to produce the final metrics:\n    a. **Maximum Saliency**: $\\text{max\\_kl} = \\max_{r,c} S_{r,c}$.\n    b. **Mean Saliency**: $\\text{mean\\_kl} = \\frac{1}{(H-P+1)(W-Q+1)} \\sum_{r=0}^{H-P} \\sum_{c=0}^{W-Q} S_{r,c}$.\n    c. **Location of Maximum**: $(r^{\\star}, c^{\\star}) = \\text{argmin}_{(r,c)} \\{ (r,c) \\mid S_{r,c} = \\text{max\\_kl} \\}$, where the minimum is taken in lexicographical (row-major) order. This corresponds to the first occurrence of the maximum value when scanning the saliency map row by row.\n6.  Collect the four metrics $[\\text{max\\_kl}, \\text{mean\\_kl}, r^{\\star}, c^{\\star}]$ for the current test case.\n\nThis procedure is repeated for all provided test cases. The implementation will utilize numerical libraries to efficiently handle matrix operations, flattening, and the computation of the softmax and KL divergence.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import kl_div, softmax\n\ndef solve():\n    \"\"\"\n    Solves the saliency map computation problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1 (general “happy path”)\n        {\n            \"dims\": (4, 4, 3),  # H, W, C\n            \"patch_dims\": (2, 2),  # P, Q\n            \"baseline\": 0.0,\n            \"image\": np.array([\n                [0.0, 0.5, 1.0, -0.5],\n                [1.0, 0.0, -1.0, 0.5],\n                [0.3, -0.2, 0.7, 1.2],\n                [0.0, 0.0, 0.5, -0.3]\n            ]),\n            \"weights_def\": {\n                \"type\": \"sequence\",\n                \"coeffs\": [0.1, -0.05, 0.02]\n            },\n            \"bias\": np.array([0.1, -0.2, 0.3])\n        },\n        # Test Case 2 (boundary case with no change under occlusion)\n        {\n            \"dims\": (3, 3, 2),\n            \"patch_dims\": (1, 1),\n            \"baseline\": 0.5,\n            \"image\": np.full((3, 3), 0.5),\n            \"weights_def\": {\n                \"type\": \"explicit\",\n                \"matrix\": np.array([\n                    [0.2, -0.1, 0.1, -0.05, 0.3, 0.0, 0.05, -0.1, 0.02],\n                    [-0.1, 0.15, -0.2, 0.1, -0.05, 0.2, -0.05, 0.1, -0.02]\n                ])\n            },\n            \"bias\": np.array([0.0, 0.0])\n        },\n        # Test Case 3 (edge case stressing strong localized weights)\n        {\n            \"dims\": (5, 5, 2),\n            \"patch_dims\": (3, 3),\n            \"baseline\": 0.0,\n            \"image\": np.full((5, 5), 1.0),\n            \"weights_def\": {\n                \"type\": \"piecewise\",\n                \"def\": [\n                    {\"value\": 2.0, \"region\": (0, 0, 3, 3)}, # Class 0, val, r, c, h, w\n                    {\"value\": -0.05, \"all\": True} # Class 1\n                ]\n            },\n            \"bias\": np.array([0.0, 0.0])\n        }\n    ]\n\n    def classifier_forward_pass(image, weights, bias):\n        \"\"\"Computes the probability distribution for a given image.\"\"\"\n        H, W = image.shape\n        v = image.flatten('C') # Row-major flattening\n        logits = weights @ v + bias\n        # Using scipy.special.softmax handles numerical stability\n        return softmax(logits)\n\n    def generate_weights(case):\n        \"\"\"Generates the weight matrix based on the case definition.\"\"\"\n        H, W, C = case[\"dims\"]\n        HW = H * W\n        weights_def = case[\"weights_def\"]\n        \n        if weights_def[\"type\"] == \"explicit\":\n            return weights_def[\"matrix\"]\n        \n        W_matrix = np.zeros((C, HW))\n        if weights_def[\"type\"] == \"sequence\":\n            s = np.arange(HW)\n            for i, coeff in enumerate(weights_def[\"coeffs\"]):\n                W_matrix[i, :] = coeff * s\n        elif weights_def[\"type\"] == \"piecewise\":\n            # For Case 3\n            # Class 0 weights\n            W0 = np.full(HW, 0.1)\n            r, c, h, w = weights_def[\"def\"][0][\"region\"]\n            for i in range(r, r + h):\n                for k in range(c, c + w):\n                    idx = i * W + k\n                    W0[idx] = weights_def[\"def\"][0][\"value\"]\n            W_matrix[0, :] = W0\n            # Class 1 weights\n            W1 = np.full(HW, weights_def[\"def\"][1][\"value\"])\n            W_matrix[1, :] = W1\n        return W_matrix\n\n    results = []\n    for case in test_cases:\n        H, W, C = case[\"dims\"]\n        P, Q = case[\"patch_dims\"]\n        b_mask = case[\"baseline\"]\n        x = case[\"image\"]\n        weights = generate_weights(case)\n        bias = case[\"bias\"]\n        \n        # 1. Compute original distribution p\n        p_original = classifier_forward_pass(x, weights, bias)\n        \n        saliency_map = np.zeros((H - P + 1, W - Q + 1))\n        \n        # 2. Iterate through occlusions\n        for r in range(H - P + 1):\n            for c in range(W - Q + 1):\n                x_occluded = x.copy()\n                x_occluded[r:r+P, c:c+Q] = b_mask\n                \n                # Compute occluded distribution q\n                q_occluded = classifier_forward_pass(x_occluded, weights, bias)\n                \n                # Compute KL divergence (saliency)\n                # kl_div(p, q) calculates p * log(p/q) element-wise\n                saliency = np.sum(kl_div(p_original, q_occluded))\n                saliency_map[r, c] = saliency\n\n        # 3. Aggregate metrics\n        max_kl = np.max(saliency_map)\n        mean_kl = np.mean(saliency_map)\n        \n        # Find first occurrence of max_kl in row-major order\n        max_loc_flat = np.argmax(saliency_map)\n        r_star, c_star = np.unravel_index(max_loc_flat, saliency_map.shape)\n        \n        results.append([max_kl, mean_kl, int(r_star), int(c_star)])\n\n    # Final print statement in the exact required format.\n    # The default str() for a list includes spaces, which `','.join` will preserve.\n    # `f\"[{','.join(str(res) for res in results)}]` creates eg: '[[1, 2],[3, 4]]'\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}