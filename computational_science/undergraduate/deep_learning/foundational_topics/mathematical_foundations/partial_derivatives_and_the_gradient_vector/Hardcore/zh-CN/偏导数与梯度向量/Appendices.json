{
    "hands_on_practices": [
        {
            "introduction": "损失函数的选择是训练任何机器学习模型时的基本决策。它直接定义了优化目标，并决定了模型如何响应预测误差。在本练习中，我们将通过一次动手计算，比较两种常见的回归损失：均方误差（Mean Squared Error, MSE）和平均绝对误差（Mean Absolute Error, MAE）。这个思想实验  将揭示它们对异常数据点的不同敏感性，从而具体理解为什么 MAE 通常被认为比 MSE 更“鲁棒”。",
            "id": "3162520",
            "problem": "考虑一个用于回归的单个线性神经元，它有两个实值特征，由参数模型 $\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x}$ 定义，其中 $\\mathbf{w} \\in \\mathbb{R}^{2}$ 且 $\\mathbf{x} \\in \\mathbb{R}^{2}$。给定一个包含 $3$ 个训练样本的小批量：\n- 样本 $1$：$\\mathbf{x}^{(1)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$，目标值 $y^{(1)} = 1$，\n- 样本 $2$：$\\mathbf{x}^{(2)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$，目标值 $y^{(2)} = 1$，\n- 样本 $3$（其目标值是一个离群值）：$\\mathbf{x}^{(3)} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，目标值 $y^{(3)} = -10$。\n\n假设当前参数为 $\\mathbf{w} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$。将该小批量的均方误差 (MSE) 和平均绝对误差 (MAE) 定义为\n$$\nL_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^{2}, \n\\quad\nL_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left|\\hat{y}^{(i)} - y^{(i)}\\right|.\n$$\n仅使用偏导数和梯度向量的基本定义，计算在给定的 $\\mathbf{w}$ 处，每种损失函数下关于 $\\mathbf{w}$ 的批量梯度，然后计算这两个梯度向量之间的夹角。将最终答案表示为夹角的单个精确闭式表达式，单位为弧度（不要进行近似）。以弧度为单位陈述你的最终答案（不要转换为角度，也不要进行数值四舍五入）。",
            "solution": "首先根据所需流程验证问题陈述。\n\n### 第 1 步：提取已知条件\n- **模型**：一个具有两个实值特征的单个线性神经元，$\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x}$，其中 $\\mathbf{w} \\in \\mathbb{R}^{2}$ 且 $\\mathbf{x} \\in \\mathbb{R}^{2}$。\n- **训练数据（大小为 3 的小批量）**：\n  - 样本 $1$：$\\mathbf{x}^{(1)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$，$y^{(1)} = 1$。\n  - 样本 $2$：$\\mathbf{x}^{(2)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$，$y^{(2)} = 1$。\n  - 样本 $3$：$\\mathbf{x}^{(3)} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，$y^{(3)} = -10$。\n- **当前参数**：$\\mathbf{w} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$。\n- **损失函数**：\n  - 均方误差 (MSE)：$L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^{2}$。\n  - 平均绝对误差 (MAE)：$L_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left|\\hat{y}^{(i)} - y^{(i)}\\right|$。\n- **任务**：\n  1. 在给定的 $\\mathbf{w}$ 处，计算 $L_{\\mathrm{MSE}}$ 关于 $\\mathbf{w}$ 的批量梯度。\n  2. 在给定的 $\\mathbf{w}$ 处，计算 $L_{\\mathrm{MAE}}$ 关于 $\\mathbf{w}$ 的批量梯度。\n  3. 计算这两个梯度向量之间的夹角（以弧度为单位）。\n\n### 第 2 步：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n- **科学依据**：该问题是机器学习和优化领域的标准练习。它涉及线性模型、损失函数（MSE、MAE）和梯度计算等基本概念，这些都是训练神经网络的核心。其设置在科学上和数学上都是合理的。\n- **适定性**：该问题提供了计算唯一解所需的所有数据和定义。模型、数据、初始参数和损失函数都已明确定义。MAE 损失函数的梯度在指定点是良定义的，因为绝对值函数的参数非零，从而确保了可微性。\n- **客观性**：该问题使用精确的数学语言陈述，没有任何模糊性、主观陈述或观点。\n\n该问题是自洽、一致的，并遵循既定的科学原则。它不违反任何无效标准。\n\n### 第 3 步：结论与行动\n该问题是**有效的**。将提供完整的解决方案。\n\n### 解答\n\n该模型是一个线性神经元，因此对于输入向量 $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ 和权重 $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$，其预测值 $\\hat{y}$ 由 $\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x} = w_1 x_1 + w_2 x_2$ 给出。\n\n任务要求在特定的权重向量 $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 处计算 MSE 和 MAE 损失函数的梯度。\n\n首先，我们计算在 $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 处模型对小批量中每个样本的预测值：\n$\\hat{y}^{(i)} = \\mathbf{w}^{\\top}\\mathbf{x}^{(i)} = \\begin{pmatrix} 0  0 \\end{pmatrix} \\begin{pmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\end{pmatrix} = 0$，对于所有 $i \\in \\{1, 2, 3\\}$。\n\n接下来，我们计算每个样本的误差 $(\\hat{y}^{(i)} - y^{(i)})$：\n- 样本 1 的误差：$e_1 = \\hat{y}^{(1)} - y^{(1)} = 0 - 1 = -1$。\n- 样本 2 的误差：$e_2 = \\hat{y}^{(2)} - y^{(2)} = 0 - 1 = -1$。\n- 样本 3 的误差：$e_3 = \\hat{y}^{(3)} - y^{(3)} = 0 - (-10) = 10$。\n\n**1. 均方误差 (MSE) 损失的梯度**\n\nMSE 损失为 $L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\mathbf{w}^{\\top}\\mathbf{x}^{(i)} - y^{(i)}\\right)^{2}$。\n$L_{\\mathrm{MSE}}$ 关于权重向量 $\\mathbf{w}$ 的梯度由下式给出：\n$$ \\nabla_{\\mathbf{w}} L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{\\partial}{\\partial \\mathbf{w}} \\left( \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^{2} \\right) = \\frac{1}{3}\\sum_{i=1}^{3} 2\\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mathbf{w}} $$\n由于 $\\hat{y}^{(i)} = \\mathbf{w}^{\\top}\\mathbf{x}^{(i)}$，其关于 $\\mathbf{w}$ 的梯度为 $\\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mathbf{w}} = \\mathbf{x}^{(i)}$。\n因此，MSE 损失的梯度为：\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\nabla_{\\mathbf{w}} L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{2}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)\\mathbf{x}^{(i)} $$\n使用预先计算的误差，在 $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 处对此进行求值：\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\frac{2}{3} \\left( (-1)\\mathbf{x}^{(1)} + (-1)\\mathbf{x}^{(2)} + (10)\\mathbf{x}^{(3)} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\frac{2}{3} \\left( (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (10)\\begin{pmatrix}1 \\\\ 0\\end{pmatrix} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\frac{2}{3} \\left( \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}10 \\\\ 0\\end{pmatrix} \\right) = \\frac{2}{3} \\begin{pmatrix}10 \\\\ -2\\end{pmatrix} = \\begin{pmatrix} \\frac{20}{3} \\\\ -\\frac{4}{3} \\end{pmatrix} $$\n\n**2. 平均绝对误差 (MAE) 损失的梯度**\n\nMAE 损失为 $L_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left|\\mathbf{w}^{\\top}\\mathbf{x}^{(i)} - y^{(i)}\\right|$。\n绝对值函数 $|u|$ 的导数是符号函数 $\\text{sgn}(u)$，它在 $u \\neq 0$ 时有定义。由于所有误差 $e_i$ 都非零（$-1, -1, 10$），因此 $L_{\\mathrm{MAE}}$ 的梯度在这一点上是良定义的。\n$L_{\\mathrm{MAE}}$ 关于 $\\mathbf{w}$ 的梯度为：\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\nabla_{\\mathbf{w}} L_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3} \\text{sgn}\\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mathbf{w}} = \\frac{1}{3}\\sum_{i=1}^{3} \\text{sgn}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)\\mathbf{x}^{(i)} $$\n我们需要误差的符号：\n- $\\text{sgn}(e_1) = \\text{sgn}(-1) = -1$。\n- $\\text{sgn}(e_2) = \\text{sgn}(-1) = -1$。\n- $\\text{sgn}(e_3) = \\text{sgn}(10) = 1$。\n在 $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 处计算梯度：\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\frac{1}{3} \\left( (-1)\\mathbf{x}^{(1)} + (-1)\\mathbf{x}^{(2)} + (1)\\mathbf{x}^{(3)} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\frac{1}{3} \\left( (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (1)\\begin{pmatrix}1 \\\\ 0\\end{pmatrix} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\frac{1}{3} \\left( \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix}1 \\\\ -2\\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \\\\ -\\frac{2}{3} \\end{pmatrix} $$\n\n**3. 梯度向量之间的夹角**\n\n设 $\\theta$ 为两个梯度向量 $\\mathbf{g}_{\\mathrm{MSE}}$ 和 $\\mathbf{g}_{\\mathrm{MAE}}$ 之间的夹角。该夹角由以下公式给出：\n$$ \\theta = \\arccos\\left(\\frac{\\mathbf{g}_{\\mathrm{MSE}} \\cdot \\mathbf{g}_{\\mathrm{MAE}}}{\\|\\mathbf{g}_{\\mathrm{MSE}}\\| \\|\\mathbf{g}_{\\mathrm{MAE}}\\|}\\right) $$\n我们计算该公式的各个组成部分：\n- **点积**：\n$$ \\mathbf{g}_{\\mathrm{MSE}} \\cdot \\mathbf{g}_{\\mathrm{MAE}} = \\begin{pmatrix} \\frac{20}{3} \\\\ -\\frac{4}{3} \\end{pmatrix} \\cdot \\begin{pmatrix} \\frac{1}{3} \\\\ -\\frac{2}{3} \\end{pmatrix} = \\left(\\frac{20}{3}\\right)\\left(\\frac{1}{3}\\right) + \\left(-\\frac{4}{3}\\right)\\left(-\\frac{2}{3}\\right) = \\frac{20}{9} + \\frac{8}{9} = \\frac{28}{9} $$\n- **模长**：\n$$ \\|\\mathbf{g}_{\\mathrm{MSE}}\\| = \\sqrt{\\left(\\frac{20}{3}\\right)^2 + \\left(-\\frac{4}{3}\\right)^2} = \\sqrt{\\frac{400}{9} + \\frac{16}{9}} = \\sqrt{\\frac{416}{9}} = \\frac{\\sqrt{16 \\times 26}}{3} = \\frac{4\\sqrt{26}}{3} $$\n$$ \\|\\mathbf{g}_{\\mathrm{MAE}}\\| = \\sqrt{\\left(\\frac{1}{3}\\right)^2 + \\left(-\\frac{2}{3}\\right)^2} = \\sqrt{\\frac{1}{9} + \\frac{4}{9}} = \\sqrt{\\frac{5}{9}} = \\frac{\\sqrt{5}}{3} $$\n- **模长之积**：\n$$ \\|\\mathbf{g}_{\\mathrm{MSE}}\\| \\|\\mathbf{g}_{\\mathrm{MAE}}\\| = \\left(\\frac{4\\sqrt{26}}{3}\\right) \\left(\\frac{\\sqrt{5}}{3}\\right) = \\frac{4\\sqrt{26 \\times 5}}{9} = \\frac{4\\sqrt{130}}{9} $$\n- **夹角的余弦值**：\n$$ \\cos(\\theta) = \\frac{\\frac{28}{9}}{\\frac{4\\sqrt{130}}{9}} = \\frac{28}{4\\sqrt{130}} = \\frac{7}{\\sqrt{130}} $$\n- **夹角 $\\theta$**：\n以弧度为单位的夹角是：\n$$ \\theta = \\arccos\\left(\\frac{7}{\\sqrt{130}}\\right) $$\n这就是夹角的最终精确闭式表达式。",
            "answer": "$$\\boxed{\\arccos\\left(\\frac{7}{\\sqrt{130}}\\right)}$$"
        },
        {
            "introduction": "除了损失函数，目标标签的表示方式也会深刻影响训练动态。虽然独热（one-hot）编码的“硬”标签是分类任务的标准做法，但它们可能鼓励模型变得过分自信，降低其泛化能力。本实践将探讨标签平滑（label smoothing），一种为目标创建“软”概率分布的技术。通过在一个带有标签噪声的场景中  计算并比较硬标签与软标签的期望梯度范数，我们可以量化标签平滑如何作为一种正则化手段，促进更稳定和鲁棒的梯度。",
            "id": "3162536",
            "problem": "在一个具有 $K=3$ 个类别的多类别分类器中，模型输出一个 logit 向量 $z \\in \\mathbb{R}^{3}$，该向量通过 softmax 函数 $p_{k}(z) = \\exp(z_{k})/\\sum_{j=1}^{3}\\exp(z_{j})$ 转换为类别概率。训练使用分类交叉熵损失 $L(z,y) = -\\sum_{k=1}^{3} y_{k} \\ln p_{k}(z)$，其中 $y \\in \\Delta^{2}$ 是三个类别上的目标分布。\n\n考虑以下一个源于深度学习实践中标签噪声的设定。真实类别为 $c^{\\star}=1$。观测到的标签是有噪声的：观测类别等于 $c^{\\star}$ 的概率为 $1-\\eta$，而以概率 $\\eta$ 从两个不正确的类别中均匀采样。设 $\\eta=0.4$。因此，观测类别 $c$ 在 $\\{1,2,3\\}$ 中取值，其概率为 $q = [0.6, 0.2, 0.2]$。比较两种训练目标：\n- 硬标签：$y = e_{c}$，其中 $e_{c}$ 是观测类别 $c$ 的 one-hot 向量。\n- 带标签平滑的软标签：对于观测类别 $c$，使用 $y^{\\text{soft}}(c,\\alpha) = (1-\\alpha)\\,e_{c} + \\alpha\\,u$，其中 $u=[1/3,1/3,1/3]$ 且 $\\alpha=0.2$。\n\n设 logits 为 $z = [1.1,\\,-0.2,\\,0.0]$。仅从上面 $p(z)$ 和 $L(z,y)$ 的定义以及基础微积分（特别是链式法则）出发，推导梯度向量 $\\partial L/\\partial z$（用 $p$ 和 $y$ 表示），然后在给定的观测标签 $c$ 的噪声模型下，计算：\n- 使用硬标签训练时，梯度的期望平方欧几里得范数 $\\mathbb{E}\\left[\\|\\partial L/\\partial z\\|_{2}^{2}\\right]$，以及\n- 使用软标签（$\\alpha=0.2$）训练时，梯度的期望平方欧几里得范数。\n\n最后，对于指定的 $z$、$\\eta$ 和 $\\alpha$，计算比率\n$$\nR \\;=\\; \\frac{\\mathbb{E}\\big[\\|\\partial L/\\partial z\\|_{2}^{2}\\big]_{\\text{hard}}}{\\mathbb{E}\\big[\\|\\partial L/\\partial z\\|_{2}^{2}\\big]_{\\text{soft}}}\n$$\n报告 $R$ 的值，四舍五入到四位有效数字。最终答案必须是一个实数。",
            "solution": "问题要求在特定噪声模型下，计算使用硬标签与软标签时，损失函数关于 logits 的梯度的期望平方欧几里得范数的比率。\n\n问题陈述定义明确、科学上合理且内容自洽。所有必要的变量、常数和函数形式都已提供。该任务完全属于深度学习背景下的偏导数和梯度计算范畴。因此，该问题是有效的，我们可以开始求解。\n\n解题过程包括四个主要步骤：\n1.  推导分类交叉熵损失关于 logits $z$ 的梯度的通用表达式。\n2.  在给定的噪声模型下，计算使用硬标签时该梯度的期望平方欧几里得范数。\n3.  计算使用软标签（标签平滑）时梯度的期望平方欧几里得范数。\n4.  计算这两个期望值的比率。\n\n**步骤 1：梯度 $\\nabla_z L(z,y)$ 的推导**\n\n分类交叉熵损失由 $L(z,y) = -\\sum_{k=1}^{K} y_{k} \\ln p_{k}(z)$ 给出，其中 $K=3$。\nsoftmax 函数将概率 $p_k(z)$ 定义为 $p_{k}(z) = \\frac{\\exp(z_{k})}{\\sum_{j=1}^{K}\\exp(z_{j})}$。\n我们需要计算 $L$ 关于每个 logit $z_i$ 的偏导数。使用链式法则：\n$$\n\\frac{\\partial L}{\\partial z_i} = -\\sum_{k=1}^{K} y_k \\frac{\\partial}{\\partial z_i} (\\ln p_k(z)) = -\\sum_{k=1}^{K} \\frac{y_k}{p_k(z)} \\frac{\\partial p_k(z)}{\\partial z_i}\n$$\n接下来，我们计算 softmax 函数 $p_k(z)$ 关于 $z_i$ 的导数。令 $S = \\sum_{j=1}^{K}\\exp(z_{j})$。则 $p_k(z) = \\exp(z_k)/S$。使用商法则：\n$$\n\\frac{\\partial p_k(z)}{\\partial z_i} = \\frac{\\frac{\\partial \\exp(z_k)}{\\partial z_i} S - \\exp(z_k) \\frac{\\partial S}{\\partial z_i}}{S^2}\n$$\n分子中的导数是：\n$\\frac{\\partial \\exp(z_k)}{\\partial z_i} = \\delta_{ik}\\exp(z_k)$，其中 $\\delta_{ik}$ 是克罗内克 δ。\n$\\frac{\\partial S}{\\partial z_i} = \\frac{\\partial}{\\partial z_i} \\sum_{j=1}^{K}\\exp(z_j) = \\exp(z_i)$。\n将这些代入商法则表达式中：\n$$\n\\frac{\\partial p_k(z)}{\\partial z_i} = \\frac{\\delta_{ik}\\exp(z_k)S - \\exp(z_k)\\exp(z_i)}{S^2} = \\delta_{ik}\\frac{\\exp(z_k)}{S} - \\left(\\frac{\\exp(z_k)}{S}\\right)\\left(\\frac{\\exp(z_i)}{S}\\right) = \\delta_{ik}p_k(z) - p_k(z)p_i(z)\n$$\n这个表达式可以分两种情况计算：\n- 如果 $i=k$：$\\frac{\\partial p_i(z)}{\\partial z_i} = p_i(z) - p_i(z)^2 = p_i(z)(1-p_i(z))$。\n- 如果 $i \\neq k$：$\\frac{\\partial p_k(z)}{\\partial z_i} = -p_k(z)p_i(z)$。\n\n现在，将此结果代回 $\\frac{\\partial L}{\\partial z_i}$ 的表达式中：\n$$\n\\frac{\\partial L}{\\partial z_i} = -\\sum_{k=1}^{K} \\frac{y_k}{p_k(z)} (\\delta_{ik}p_k(z) - p_k(z)p_i(z)) = -\\sum_{k=1}^{K} y_k (\\delta_{ik} - p_i(z))\n$$\n$$\n\\frac{\\partial L}{\\partial z_i} = -\\left( y_i - p_i(z)\\sum_{k=1}^{K}y_k \\right)\n$$\n由于 $y$ 是一个概率分布（或一个 one-hot 向量），其分量之和为 1，即 $\\sum_{k=1}^{K}y_k = 1$。\n因此，偏导数简化为：\n$$\n\\frac{\\partial L}{\\partial z_i} = -(y_i - p_i(z)) = p_i(z) - y_i\n$$\n以向量形式表示，梯度为 $\\nabla_z L = p - y$。\n\n**步骤 2：硬标签的期望平方范数**\n\n对于硬标签，目标向量 $y$ 是对应于观测类别 $c$ 的 one-hot 向量 $e_c$。梯度是 $g_{\\text{hard}}(c) = p - e_c$。观测类别 $c$ 是一个随机变量，其分布为 $q = [0.6, 0.2, 0.2]$。\n我们需要计算梯度的期望平方欧几里得范数：\n$$\nE_{\\text{hard}} = \\mathbb{E}_{c \\sim q}\\left[ \\|g_{\\text{hard}}(c)\\|_2^2 \\right] = \\sum_{c=1}^{3} q_c \\|p - e_c\\|_2^2\n$$\n让我们展开平方范数：\n$$\n\\|p - e_c\\|_2^2 = \\sum_{j=1}^{3} (p_j - \\delta_{jc})^2 = \\sum_{j=1}^{3} (p_j^2 - 2p_j\\delta_{jc} + \\delta_{jc}^2) = \\left(\\sum_{j=1}^{3} p_j^2\\right) - 2p_c + 1 = \\|p\\|_2^2 - 2p_c + 1\n$$\n现在，我们对 $c$ 取期望：\n$$\nE_{\\text{hard}} = \\sum_{c=1}^{3} q_c (\\|p\\|_2^2 - 2p_c + 1) = \\|p\\|_2^2 \\sum_{c=1}^{3} q_c - 2\\sum_{c=1}^{3} q_c p_c + \\sum_{c=1}^{3} q_c\n$$\n由于 $\\sum q_c = 1$ 且 $\\sum q_c p_c = \\langle q, p \\rangle$（内积），我们得到：\n$$\nE_{\\text{hard}} = \\|p\\|_2^2 + 1 - 2\\langle q, p \\rangle\n$$\n首先，我们从 logit 向量 $z = [1.1, -0.2, 0.0]$ 计算概率向量 $p$。\n指数和为 $S = \\exp(1.1) + \\exp(-0.2) + \\exp(0.0) \\approx 3.00416602 + 0.81873075 + 1 \\approx 4.82289677$。\n概率为：\n$p_1 = \\exp(1.1)/S \\approx 0.62294700$\n$p_2 = \\exp(-0.2)/S \\approx 0.16976588$\n$p_3 = \\exp(0.0)/S \\approx 0.20728712$\n所以，$p \\approx [0.62294700, 0.16976588, 0.20728712]$。\n现在，我们计算所需的量：\n$\\|p\\|_2^2 = p_1^2 + p_2^2 + p_3^2 \\approx (0.62294700)^2 + (0.16976588)^2 + (0.20728712)^2 \\approx 0.3880630 + 0.02881945 + 0.04296813 \\approx 0.45985058$。\n与 $q = [0.6, 0.2, 0.2]$ 的内积是：\n$\\langle q, p \\rangle = 0.6p_1 + 0.2p_2 + 0.2p_3 \\approx 0.6(0.62294700) + 0.2(0.16976588) + 0.2(0.20728712) \\approx 0.37376820 + 0.03395318 + 0.04145742 \\approx 0.44917880$。\n最后，$E_{\\text{hard}} = 0.45985058 + 1 - 2(0.44917880) = 1.45985058 - 0.89835760 \\approx 0.56149298$。\n\n**步骤 3：软标签的期望平方范数**\n\n对于软标签，目标是 $y^{\\text{soft}}(c,\\alpha) = (1-\\alpha)e_c + \\alpha u$，其中 $\\alpha=0.2$ 且 $u=[1/3, 1/3, 1/3]$。梯度是 $g_{\\text{soft}}(c) = p - y^{\\text{soft}}(c,\\alpha)$。\n期望平方范数是 $E_{\\text{soft}} = \\mathbb{E}_{c \\sim q}\\left[ \\|p - y^{\\text{soft}}(c,\\alpha)\\|_2^2 \\right]$。\n让我们展开期望：\n$$\nE_{\\text{soft}} = \\mathbb{E}\\left[ \\|p - y^{\\text{soft}}\\|_2^2 \\right] = \\|p\\|_2^2 - 2\\mathbb{E}\\left[\\langle p, y^{\\text{soft}} \\rangle\\right] + \\mathbb{E}\\left[\\|y^{\\text{soft}}\\|_2^2\\right]\n$$\n我们分别计算这两个期望项。\n$\\mathbb{E}\\left[\\langle p, y^{\\text{soft}} \\rangle\\right] = \\mathbb{E}\\left[\\langle p, (1-\\alpha)e_c + \\alpha u \\rangle\\right] = (1-\\alpha)\\mathbb{E}[\\langle p, e_c \\rangle] + \\alpha\\langle p, u \\rangle$。\n$\\mathbb{E}[\\langle p, e_c \\rangle] = \\mathbb{E}[p_c] = \\langle q, p \\rangle$。\n$\\langle p, u \\rangle = \\sum_j p_j u_j = \\frac{1}{3}\\sum_j p_j = 1/3$。\n所以，$\\mathbb{E}\\left[\\langle p, y^{\\text{soft}} \\rangle\\right] = (1-\\alpha)\\langle q, p \\rangle + \\alpha/3$。\n接下来，我们计算 $\\mathbb{E}\\left[\\|y^{\\text{soft}}\\|_2^2\\right]$。项 $\\|y^{\\text{soft}}\\|_2^2$ 不依赖于抽取了哪个类别 $c$，只依赖于抽取了一个类别这个事实。\n$\\|y^{\\text{soft}}\\|_2^2 = \\|(1-\\alpha)e_c + \\alpha u\\|_2^2 = (1-\\alpha)^2\\|e_c\\|_2^2 + 2\\alpha(1-\\alpha)\\langle e_c, u \\rangle + \\alpha^2\\|u\\|_2^2$。\n我们有 $\\|e_c\\|_2^2 = 1$，$\\langle e_c, u \\rangle = u_c = 1/3$，以及 $\\|u\\|_2^2 = 3(1/3)^2 = 1/3$。\n所以，$\\|y^{\\text{soft}}\\|_2^2 = (1-\\alpha)^2 + \\frac{2\\alpha(1-\\alpha)}{3} + \\frac{\\alpha^2}{3}$。由于这对于 $c$ 是一个常数，它的期望就是它本身。\n综合所有项：\n$E_{\\text{soft}} = \\|p\\|_2^2 - 2\\left((1-\\alpha)\\langle q, p \\rangle + \\frac{\\alpha}{3}\\right) + (1-\\alpha)^2 + \\frac{2\\alpha(1-\\alpha)}{3} + \\frac{\\alpha^2}{3}$。\n让我们简化含 $\\alpha$ 的常数项：\n$(1-\\alpha)^2 - \\frac{2\\alpha}{3} + \\frac{2\\alpha(1-\\alpha)}{3} + \\frac{\\alpha^2}{3} = (1-2\\alpha+\\alpha^2) - \\frac{2\\alpha}{3} + \\frac{2\\alpha-2\\alpha^2+\\alpha^2}{3} = 1 - 2\\alpha + \\alpha^2 - \\frac{\\alpha^2}{3} = 1 - 2\\alpha + \\frac{2\\alpha^2}{3}$。\n所以，完整的表达式是：\n$$\nE_{\\text{soft}} = \\|p\\|_2^2 + 1 - 2\\alpha + \\frac{2\\alpha^2}{3} - 2(1-\\alpha)\\langle q, p \\rangle\n$$\n现在，代入数值 $\\alpha=0.2$：\n常数项是 $1 - 2(0.2) + \\frac{2(0.2)^2}{3} = 1 - 0.4 + \\frac{2(0.04)}{3} = 0.6 + \\frac{0.08}{3} \\approx 0.62666667$。\n带有内积的项是 $-2(1-0.2)\\langle q, p \\rangle = -1.6\\langle q, p \\rangle$。\n$E_{\\text{soft}} \\approx \\|p\\|_2^2 + 0.62666667 - 1.6\\langle q, p \\rangle$。\n使用我们之前计算的值：\n$E_{\\text{soft}} \\approx 0.45985058 + 0.62666667 - 1.6(0.44917880) \\approx 1.08651725 - 0.71868608 \\approx 0.36783117$。\n\n**步骤 4：比率计算**\n\n最后，我们计算比率 $R$：\n$$\nR = \\frac{E_{\\text{hard}}}{E_{\\text{soft}}} = \\frac{\\mathbb{E}\\big[\\|\\nabla_z L\\|_{2}^{2}\\big]_{\\text{hard}}}{\\mathbb{E}\\big[\\|\\nabla_z L\\|_{2}^{2}\\big]_{\\text{soft}}}\n$$\n$$\nR \\approx \\frac{0.56149298}{0.36783117} \\approx 1.526500\n$$\n问题要求答案四舍五入到四位有效数字。这个数是 $1.5265...$。第四位有效数字是 $6$。第五位是 $5$。因此，我们向上进位第四位数字。$R \\approx 1.527$。\n\n使用的最终值：\n$E_{\\text{hard}} \\approx 0.56149298$\n$E_{\\text{soft}} \\approx 0.36783117$\n$R = 1.526500...$\n四舍五入到四位有效数字，$R = 1.527$。",
            "answer": "$$\\boxed{1.527}$$"
        },
        {
            "introduction": "基于梯度的优化是深度学习的引擎，但当我们的模型包含不可微操作时，例如模型量化中使用的取整函数，会发生什么？在本练习中，我们将研究直通估计器（Straight-Through Estimator, STE），这是一种实用且广泛使用的技术，用以克服这一障碍。通过直接计算“真实”梯度（几乎处处为零）和 STE 的近似梯度 ，我们将揭示该方法固有的偏差，并深入理解它如何为量化神经网络的训练提供可能。",
            "id": "3162528",
            "problem": "考虑一个单参数线性预测器，其量化前参数为 $\\tilde{w} \\in \\mathbb{R}$，量化后参数为 $w \\in \\mathbb{Z}$，由 $w=\\mathrm{round}(\\tilde{w})$ 给出，其中 $\\mathrm{round}(\\cdot)$ 表示四舍五入到最近的整数。对于输入 $x$，模型输出为 $f(x)=w\\,x$。您使用平方误差损失在一个单一的确定性训练对 $(x,y)$ 上进行训练，其中 $x=\\frac{3}{2}$，$y=\\frac{5}{4}$：\n$$\nL(\\tilde{w})=\\big(w\\,x-y\\big)^{2}.\n$$\n在反向传播过程中，采用直通估计器（STE），即在需要时将雅可比矩阵 $\\frac{\\partial w}{\\partial \\tilde{w}}$ 近似为 $1$。\n\n在开区间 $\\big(\\frac{1}{2},\\frac{3}{2}\\big)$ 内的任意 $\\tilde{w}$ 处进行计算，在此区间内 $w$ 是常数。仅从偏导数的定义、链式法则以及平方误差损失的定义出发，完成以下任务：\n\n- 计算在该 $\\tilde{w}$ 处基于 STE 的偏导数 $\\frac{\\partial L}{\\partial \\tilde{w}}$。\n- 使用精确的、量化的目标函数 $L(\\tilde{w})=\\big(\\mathrm{round}(\\tilde{w})\\,x-y\\big)^{2}$，确定在该 $\\tilde{w}$ 处的真实偏导数 $\\frac{\\partial L}{\\partial \\tilde{w}}$。\n- 将该 $\\tilde{w}$ 处 STE 梯度的偏差定义为 $B=\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}}-\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}$。精确计算 $B$。\n\n您的最终答案必须是 $B$ 的一个单一精确数值，不得进行四舍五入。",
            "solution": "该问题要求计算直通估计器（STE）相对于量化前参数的损失函数梯度的偏差。给定损失函数 $L(\\tilde{w})=\\big(w\\,x-y\\big)^{2}$，其中 $w = \\mathrm{round}(\\tilde{w})$。偏差定义为 $B=\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}}-\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}$。我们必须在 $\\tilde{w} \\in \\big(\\frac{1}{2},\\frac{3}{2}\\big)$ 的条件下，使用具体的训练数据 $x=\\frac{3}{2}$ 和 $y=\\frac{5}{4}$ 来计算这个值。\n\n首先，我们计算基于 STE 的偏导数 $\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}}$。\n损失函数 $L$ 是关于 $\\tilde{w}$ 的复合函数。我们应用链式法则来求其关于 $\\tilde{w}$ 的导数：\n$$\n\\frac{\\partial L}{\\partial \\tilde{w}} = \\frac{\\partial L}{\\partial w} \\frac{\\partial w}{\\partial \\tilde{w}}\n$$\n让我们计算 $L$ 关于 $w$ 的偏导数：\n$$\n\\frac{\\partial L}{\\partial w} = \\frac{\\partial}{\\partial w} \\big((w\\,x-y)^{2}\\big) = 2(w\\,x-y) \\cdot \\frac{\\partial}{\\partial w}(w\\,x-y) = 2(w\\,x-y)x\n$$\n将此代入链式法则表达式中，得到：\n$$\n\\frac{\\partial L}{\\partial \\tilde{w}} = 2x(w\\,x-y) \\frac{\\partial w}{\\partial \\tilde{w}}\n$$\n直通估计器（STE）将量化函数 $\\frac{\\partial w}{\\partial \\tilde{w}}$ 的导数近似为 $1$。\n$$\n\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}} = 2x(w\\,x-y) \\cdot 1 = 2x(w\\,x-y)\n$$\n题目规定我们在开区间 $\\big(\\frac{1}{2}, \\frac{3}{2}\\big)$（即区间 $(0.5, 1.5)$）内的任意 $\\tilde{w}$ 处计算此导数。对于此区间内的任何 $\\tilde{w}$ 值，`round` 函数的结果都是整数 $1$。因此，$w = \\mathrm{round}(\\tilde{w}) = 1$。\n现在我们代入给定的值 $x=\\frac{3}{2}$ 和 $y=\\frac{5}{4}$，以及 $w=1$：\n$$\n\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}} = 2\\left(\\frac{3}{2}\\right)\\left( (1)\\left(\\frac{3}{2}\\right) - \\frac{5}{4} \\right) = 3\\left(\\frac{6}{4} - \\frac{5}{4}\\right) = 3\\left(\\frac{1}{4}\\right) = \\frac{3}{4}\n$$\n所以，基于 STE 的梯度是 $\\frac{3}{4}$。\n\n接下来，我们计算真实的偏导数 $\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}$。\n损失函数为 $L(\\tilde{w})=\\big(\\mathrm{round}(\\tilde{w})\\,x-y\\big)^{2}$。\n如前所述，对于开区间 $\\big(\\frac{1}{2}, \\frac{3}{2}\\big)$ 内的任意 $\\tilde{w}$，$\\mathrm{round}(\\tilde{w})$ 的值是常数，等于 $1$。\n因此，对于 $\\tilde{w} \\in \\big(\\frac{1}{2}, \\frac{3}{2}\\big)$，损失函数简化为：\n$$\nL(\\tilde{w}) = \\big((1)x-y\\big)^{2}\n$$\n代入常数值 $x=\\frac{3}{2}$ 和 $y=\\frac{5}{4}$：\n$$\nL(\\tilde{w}) = \\left(\\frac{3}{2} - \\frac{5}{4}\\right)^{2} = \\left(\\frac{6}{4} - \\frac{5}{4}\\right)^{2} = \\left(\\frac{1}{4}\\right)^{2} = \\frac{1}{16}\n$$\n在指定区间内，$L(\\tilde{w})$ 是一个关于 $\\tilde{w}$ 的常数函数。任何常数函数的导数都为 $0$。\n因此，真实的偏导数为：\n$$\n\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}} = \\frac{\\partial}{\\partial \\tilde{w}}\\left(\\frac{1}{16}\\right) = 0\n$$\n这与 `round` 函数的导数处处为 $0$（除了在不连续点，即对于任意整数 $n$，在 $\\tilde{w}=n+0.5$ 的值处）的事实是一致的，而这些点不包含在开区间 $\\big(\\frac{1}{2}, \\frac{3}{2}\\big)$ 内。\n\n最后，我们计算偏差 $B$。\n偏差定义为基于 STE 的梯度与真实梯度之间的差值：\n$$\nB = \\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}} - \\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}\n$$\n代入我们计算出的值：\n$$\nB = \\frac{3}{4} - 0 = \\frac{3}{4}\n$$",
            "answer": "$$\\boxed{\\frac{3}{4}}$$"
        }
    ]
}