{
    "hands_on_practices": [
        {
            "introduction": "在将贝叶斯规则应用于复杂的深度学习模型之前，掌握其核心机制至关重要：即如何利用新证据更新先验信念。本练习通过一个简化的数字通信场景，让你直接运用贝叶斯公式，计算在观测到特定结果（数据擦除）后，原始信号的后验概率 。这是一个基础但关键的训练，旨在巩固你对贝叶斯推理基本流程的理解。",
            "id": "1603705",
            "problem": "一个数字存储设备以二进制比特序列的形式存储数据。设源比特（由随机变量 $X$ 表示）为 $0$ 或 $1$。通过对数据模式的广泛分析，已知一个比特为 $0$ 的先验概率是 $P(X=0) = \\alpha$。\n\n当从设备中读取一个比特时，接收到的符号（用 $Y$ 表示）可能出现三种结果之一：比特被正确读取为 $0$，正确读取为 $1$，或者读取操作失败，产生一个“擦除”符号，我们记作“?”。该设备的设计保证它从不翻转比特；存储的 $0$ 永远不会被读取为 $1$，存储的 $1$ 也永远不会被读取为 $0$。\n\n然而，读取操作的可靠性取决于存储的值。当存储的比特是 $0$ 时，发生擦除的概率是 $P(Y='?'|X=0) = p_0$。当存储的比特是 $1$ 时，发生擦除的概率是 $P(Y='?'|X=1) = p_1$。\n\n假设从设备中读取一个比特，结果是擦除符号“?”。请确定设备中原始存储的比特是 $0$ 的后验概率。请用 $\\alpha$、$p_0$ 和 $p_1$ 给出一个封闭形式的解析表达式作为答案。",
            "solution": "我们要求的是在接收到的符号是擦除的情况下，发送的比特是 $0$ 的后验概率。这可以写作 $P(X=0 | Y='?')$。\n\n为了解决这个问题，我们应用贝叶斯法则，其表述如下：\n$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n\n在我们的问题背景下，令事件 $A$ 为 $X=0$（存储的比特是0），事件 $B$ 为 $Y='?'$（观察到擦除）。将这些代入贝叶斯法则，我们得到：\n$$ P(X=0 | Y='?') = \\frac{P(Y='?' | X=0) \\cdot P(X=0)}{P(Y='?')} $$\n\n题目给出了以下值：\n- $P(X=0) = \\alpha$\n- $P(Y='?' | X=0) = p_0$\n\n我们唯一需要计算的项是观察到擦除的总概率 $P(Y='?')$。我们可以使用全概率定律来计算它，即对 $X$ 的所有可能取值求和：\n$$ P(Y='?') = P(Y='?' | X=0) \\cdot P(X=0) + P(Y='?' | X=1) \\cdot P(X=1) $$\n\n我们已知 $P(X=0) = \\alpha$，这意味着存储的比特为 $1$ 的概率是 $P(X=1) = 1 - P(X=0) = 1 - \\alpha$。我们还已知条件概率 $P(Y='?' | X=1) = p_1$。\n\n现在，我们可以将所有已知概率代入 $P(Y='?')$ 的表达式中：\n$$ P(Y='?') = (p_0) \\cdot (\\alpha) + (p_1) \\cdot (1 - \\alpha) $$\n$$ P(Y='?') = \\alpha p_0 + p_1(1-\\alpha) $$\n\n有了 $P(Y='?')$ 的表达式，我们现在可以使用贝叶斯法则完成目标后验概率的计算：\n$$ P(X=0 | Y='?') = \\frac{P(Y='?' | X=0) \\cdot P(X=0)}{P(Y='?')} $$\n$$ P(X=0 | Y='?') = \\frac{p_0 \\cdot \\alpha}{\\alpha p_0 + p_1(1-\\alpha)} $$\n\n这就是在观察到擦除的情况下，存储的比特是 $0$ 的后验概率的最终表达式。",
            "answer": "$$\\boxed{\\frac{\\alpha p_{0}}{\\alpha p_{0} + p_{1}(1-\\alpha)}}$$"
        },
        {
            "introduction": "先验概率在机器学习中并非虚无缥缈的数字，它们对模型的决策行为有着直接且可量化的影响。本练习将引导你推导在线性判别分析（LDA）分类器中，不准确的先验估计如何导致决策边界的平移 。通过这个过程，你将获得一个关于先验如何影响模型几何决策的直观理解，将抽象的贝叶斯概念与模型的实际表现联系起来。",
            "id": "3139733",
            "problem": "考虑一个线性判别分析（LDA）生成模型下的二元分类问题，其中特征是一维的。假设类条件密度是具有共同方差的高斯分布：对于类别标签 $Y \\in \\{0,1\\}$ 和特征 $x \\in \\mathbb{R}$，\n$$\np(x \\mid Y=k) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x - \\mu_{k})^{2}}{2\\sigma^{2}}\\right), \\quad k \\in \\{0,1\\},\n$$\n其中 $\\mu_{0} \\neq \\mu_{1}$ 且 $\\sigma^{2} > 0$。设真实的先验概率为 $\\pi_{0}$ 和 $\\pi_{1}$，并假设在训练中您估计的先验 $\\hat{\\pi}_{0}$ 和 $\\hat{\\pi}_{1}$ 是错误的。分类通过贝叶斯决策规则执行：如果对数后验几率 $\\ln\\!\\big(P(Y=1 \\mid x)/P(Y=0 \\mid x)\\big)$ 为非负，则将 $x$ 分配给类别 1，否则分配给类别 0。在实践中，对数后验几率是使用估计的先验 $\\hat{\\pi}_{k}$ 计算的。\n\n从贝叶斯定理和上述高斯似然函数出发，推导一个闭式解析表达式，表示用估计的先验 $\\hat{\\pi}_{k}$ 替换真实先验 $\\pi_{k}$ 所引起的实线上决策边界位置的偏移。将您的最终答案表示为以下各项的函数：\n$$\n\\Delta \\equiv \\ln\\!\\left(\\frac{\\hat{\\pi}_{1}}{\\hat{\\pi}_{0}}\\right) - \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right), \\quad \\sigma^{2}, \\quad \\mu_{0}, \\quad \\mu_{1}.\n$$\n您的答案必须是表示有符号偏移（新边界减去真实边界）的单个闭式表达式。请勿在最终答案中提供推导或中间步骤。无需四舍五入。",
            "solution": "该问题在统计学习理论的框架内是适定的且具有科学依据。推导决策边界偏移所需的所有必要参数和条件均已提供。我将继续进行解答。\n\n目标是找到决策边界的偏移，定义为 $x_{\\text{new}} - x_{\\text{true}}$，其中 $x_{\\text{true}}$ 是用真实先验 $\\pi_k$ 计算的边界，而 $x_{\\text{new}}$ 是用估计的先验 $\\hat{\\pi}_k$ 计算的边界。\n\n贝叶斯决策规则在对数后验几率为非负时将特征 $x$ 分配给类别 1。决策边界是使对数后验几率恰好为零的点 $x$。\n$$\n\\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right) = 0\n$$\n使用贝叶斯定理，$P(Y=k \\mid x) = \\frac{p(x \\mid Y=k)P(Y=k)}{p(x)}$，我们可以用类条件似然 $p(x \\mid Y=k)$ 和先验概率 $\\pi_k = P(Y=k)$ 来表示对数后验几率。\n$$\n\\ln\\left(\\frac{p(x \\mid Y=1)\\pi_1}{p(x \\mid Y=0)\\pi_0}\\right) = \\ln\\left(\\frac{p(x \\mid Y=1)}{p(x \\mid Y=0)}\\right) + \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right) = 0\n$$\n问题指明类条件密度是具有共同方差 $\\sigma^2$ 的高斯分布：\n$$\np(x \\mid Y=k) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x - \\mu_{k})^{2}}{2\\sigma^{2}}\\right)\n$$\n每个类别的对数似然是：\n$$\n\\ln(p(x \\mid Y=k)) = -\\ln(\\sqrt{2\\pi}\\sigma) - \\frac{(x - \\mu_{k})^{2}}{2\\sigma^{2}}\n$$\n因此，对数似然比为：\n$$\n\\ln\\left(\\frac{p(x \\mid Y=1)}{p(x \\mid Y=0)}\\right) = \\ln(p(x \\mid Y=1)) - \\ln(p(x \\mid Y=0))\n$$\n$$\n= \\left(-\\ln(\\sqrt{2\\pi}\\sigma) - \\frac{(x - \\mu_{1})^{2}}{2\\sigma^{2}}\\right) - \\left(-\\ln(\\sqrt{2\\pi}\\sigma) - \\frac{(x - \\mu_{0})^{2}}{2\\sigma^{2}}\\right)\n$$\n$$\n= \\frac{(x - \\mu_{0})^{2}}{2\\sigma^{2}} - \\frac{(x - \\mu_{1})^{2}}{2\\sigma^{2}}\n$$\n展开平方项：\n$$\n= \\frac{1}{2\\sigma^{2}} \\left[ (x^2 - 2x\\mu_0 + \\mu_0^2) - (x^2 - 2x\\mu_1 + \\mu_1^2) \\right]\n$$\n$$\n= \\frac{1}{2\\sigma^{2}} \\left[ 2x(\\mu_1 - \\mu_0) + \\mu_0^2 - \\mu_1^2 \\right]\n$$\n这个表达式是 LDA 特有的线性判别函数。决策边界 $x_b$ 是通过将对数似然比和对数先验几率之和设为零来找到的：\n$$\n\\frac{1}{2\\sigma^{2}} \\left[ 2x_b(\\mu_1 - \\mu_0) + \\mu_0^2 - \\mu_1^2 \\right] + \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right) = 0\n$$\n现在，我们求解通用决策边界 $x_b$：\n$$\n2x_b(\\mu_1 - \\mu_0) + \\mu_0^2 - \\mu_1^2 = -2\\sigma^2 \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right)\n$$\n$$\n2x_b(\\mu_1 - \\mu_0) = \\mu_1^2 - \\mu_0^2 - 2\\sigma^2 \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right)\n$$\n$$\n2x_b(\\mu_1 - \\mu_0) = (\\mu_1 - \\mu_0)(\\mu_1 + \\mu_0) - 2\\sigma^2 \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right)\n$$\n由于已知 $\\mu_0 \\neq \\mu_1$，我们可以除以 $2(\\mu_1 - \\mu_0)$：\n$$\nx_b = \\frac{\\mu_1 + \\mu_0}{2} - \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right)\n$$\n这是决策边界的一般形式。\n\n首先，我们使用真实先验 $\\pi_0$ 和 $\\pi_1$ 来找到真实的决策边界 $x_{\\text{true}}$：\n$$\nx_{\\text{true}} = \\frac{\\mu_1 + \\mu_0}{2} - \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right)\n$$\n接下来，我们找到由使用错误的先验 $\\hat{\\pi}_0$ 和 $\\hat{\\pi}_1$ 产生的新决策边界 $x_{\\text{new}}$：\n$$\nx_{\\text{new}} = \\frac{\\mu_1 + \\mu_0}{2} - \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\hat{\\pi}_1}{\\hat{\\pi}_0}\\right)\n$$\n决策边界位置的偏移是差值 $x_{\\text{new}} - x_{\\text{true}}$：\n$$\n\\text{Shift} = \\left(\\frac{\\mu_1 + \\mu_0}{2} - \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\hat{\\pi}_1}{\\hat{\\pi}_0}\\right)\\right) - \\left(\\frac{\\mu_1 + \\mu_0}{2} - \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right)\\right)\n$$\n项 $\\frac{\\mu_1 + \\mu_0}{2}$ 被消去：\n$$\n\\text{Shift} = -\\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\hat{\\pi}_1}{\\hat{\\pi}_0}\\right) + \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right)\n$$\n提出公因式 $-\\frac{\\sigma^2}{\\mu_1 - \\mu_0}$：\n$$\n\\text{Shift} = -\\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\left[ \\ln\\left(\\frac{\\hat{\\pi}_1}{\\hat{\\pi}_0}\\right) - \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right) \\right]\n$$\n问题将量 $\\Delta$ 定义为：\n$$\n\\Delta \\equiv \\ln\\left(\\frac{\\hat{\\pi}_{1}}{\\hat{\\pi}_{0}}\\right) - \\ln\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right)\n$$\n将此定义代入我们的偏移表达式中，得到最终答案：\n$$\n\\text{Shift} = -\\frac{\\sigma^2 \\Delta}{\\mu_1 - \\mu_0}\n$$\n这是决策边界偏移的闭式解析表达式，它是指定量 $\\Delta$、$\\sigma^2$、$\\mu_0$ 和 $\\mu_1$ 的函数。",
            "answer": "$$\n\\boxed{-\\frac{\\sigma^2 \\Delta}{\\mu_1 - \\mu_0}}\n$$"
        },
        {
            "introduction": "现实世界的数据集往往充满噪声，例如错误的标签，这对模型训练构成了巨大挑战。本练习展示了如何运用贝叶斯思想来系统性地解决这个问题：我们将噪声过程建模为一个混淆矩阵，然后通过“反转”该过程来恢复真实的标签后验分布 。这项实践不仅关乎一个算法，更体现了如何利用贝叶斯框架构建能够“看透”数据瑕疵、更加鲁棒和可靠的智能系统。",
            "id": "3102043",
            "problem": "给定一个带有类别条件标签噪声的$K$类分类场景。令干净标签为离散随机变量 $Y \\in \\{1,\\dots,K\\}$，观测到的噪声标签为 $\\hat{Y} \\in \\{1,\\dots,K\\}$，输入为 $X \\in \\mathcal{X}$。类别条件噪声模型由一个列随机混淆矩阵 $C \\in \\mathbb{R}^{K \\times K}$ 概括，其元素为 $C_{ij} = p(\\hat{Y} = i \\mid Y = j)$，因此每列之和为 $1$。一个概率分类器对每个输入 $x$ 生成一个关于噪声标签的预测概率向量 $\\hat{p}(x) \\in \\mathbb{R}^K$，其中第 $i$ 个分量是 $\\hat{p}_i(x) = p(\\hat{Y} = i \\mid X = x)$，并且一个关于干净标签的先验 $\\pi \\in \\mathbb{R}^K$（其中 $\\pi_j = \\pi(Y=j)$）是可用的。\n\n仅从条件概率的定义和全概率公式出发，推导一个有原则且可计算的关于干净标签的校正后验 $p(Y \\mid X=x)$ 的表达式，该表达式使用 $C$、$\\hat{p}(x)$ 和 $\\pi$ 作为输入。您的推导必须明确解释混淆矩阵作为连接这些分布的线性算子的作用，阐明归一化的必要性，并通过矩阵可逆性和条件数来解决数值稳定性问题。您必须设计一个算法，该算法：\n- 接受任何维度兼容的 $C$、$\\hat{p}(x)$ 和 $\\pi$，其中 $C$ 是列随机的，$\\hat{p}(x)$ 是非负的，$\\pi$ 是非负的。\n- 通过秩检验 $C$ 的可逆性，并通过2-范数条件数 $\\kappa_2(C)$ 来量化数值稳定性。使用稳定性阈值 $\\tau = 10^6$：如果 $\\kappa_2(C) > \\tau$，则认为矩阵是数值不稳定的。\n- 仅当 $C$ 可逆且 $\\kappa_2(C) \\le \\tau$ 时，才使用 $C$ 的精确逆矩阵；否则，使用 Moore–Penrose 伪逆。\n- 将先验 $\\pi$ 以乘法方式并入未归一化的干净后验中，然后重新归一化以获得有效的概率向量。\n- 在归一化之前，通过将由数值误差引入的微小负值项（若有）置零，来强制最终后验的非负性。\n- 如果 $\\hat{p}(x)$ 和 $\\pi$ 的和不为 $1$，则将它们重新归一化。\n\n将您的推导实现为一个完整的程序，该程序为以下测试套件输出结果。对于每个测试用例 $t \\in \\{1,2,3,4,5\\}$，返回一个包含五个项目的列表：\n$[\\text{is\\_invertible}, \\text{is\\_stable}, \\text{cond\\_rounded}, \\text{method\\_code}, \\text{posterior\\_rounded}]$，其中：\n- $\\text{is\\_invertible}$ 是一个布尔值，指示 $C$ 是否可逆。\n- $\\text{is\\_stable}$ 是一个布尔值，指示 $\\kappa_2(C) \\le \\tau$ 是否成立。\n- $\\text{cond\\_rounded}$ 是四舍五入到3位小数的条件数。\n- $\\text{method\\_code}$ 是一个整数，如果使用了精确逆矩阵，则为 $1$，如果使用了伪逆，则为 $0$。\n- $\\text{posterior\\_rounded}$ 是校正后的干净标签后验条目的列表，四舍五入到6位小数。\n\n最终的程序输出必须是单行，包含5个按测试用例排列的列表，列表间以逗号分隔，并用方括号括起来。\n\n使用以下测试套件（每个用例都是自洽且列随机的）：\n\n测试用例 $1$ ($K=3$):\n$$\nC^{(1)} = \\begin{bmatrix}\n0.8  0.1  0.1 \\\\\n0.1  0.8  0.1 \\\\\n0.1  0.1  0.8\n\\end{bmatrix},\\quad\n\\pi^{(1)} = \\begin{bmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{bmatrix},\\quad\n\\hat{p}^{(1)} = \\begin{bmatrix} 0.7 \\\\ 0.2 \\\\ 0.1 \\end{bmatrix}.\n$$\n\n测试用例 $2$ ($K=3$，近似奇异但可逆):\n$$\nC^{(2)} = \\begin{bmatrix}\n0.49  0.50  0.01 \\\\\n0.48  0.49  0.01 \\\\\n0.03  0.01  0.98\n\\end{bmatrix},\\quad\n\\pi^{(2)} = \\begin{bmatrix} \\tfrac{1}{3} \\\\ \\tfrac{1}{3} \\\\ \\tfrac{1}{3} \\end{bmatrix},\\quad\n\\hat{p}^{(2)} = \\begin{bmatrix} 0.50 \\\\ 0.49 \\\\ 0.01 \\end{bmatrix}.\n$$\n\n测试用例 $3$ ($K=3$，奇异):\n$$\nC^{(3)} = \\begin{bmatrix}\n0.7  0.7  0.1 \\\\\n0.2  0.2  0.2 \\\\\n0.1  0.1  0.7\n\\end{bmatrix},\\quad\n\\pi^{(3)} = \\begin{bmatrix} 0.2 \\\\ 0.5 \\\\ 0.3 \\end{bmatrix},\\quad\n\\hat{p}^{(3)} = \\begin{bmatrix} 0.4 \\\\ 0.35 \\\\ 0.25 \\end{bmatrix}.\n$$\n\n测试用例 $4$ ($K=3$，极端先验，$\\hat{p}$ 的和不为 $1$ 且必须重新归一化):\n$\nC^{(4)} = \\begin{bmatrix}\n0.9  0.05  0.05 \\\\\n0.05  0.9  0.05 \\\\\n0.05  0.05  0.9\n\\end{bmatrix},\\quad\n\\pi^{(4)} = \\begin{bmatrix} 0.99 \\\\ 0.005 \\\\ 0.005 \\end{bmatrix},\\quad\n\\hat{p}^{(4)} = \\begin{bmatrix} 0.34 \\\\ 0.33 \\\\ 0.35 \\end{bmatrix}.\n$\n\n测试用例 $5$ ($K=2$):\n$$\nC^{(5)} = \\begin{bmatrix}\n0.95  0.10 \\\\\n0.05  0.90\n\\end{bmatrix},\\quad\n\\pi^{(5)} = \\begin{bmatrix} 0.4 \\\\ 0.6 \\end{bmatrix},\\quad\n\\hat{p}^{(5)} = \\begin{bmatrix} 0.6 \\\\ 0.4 \\end{bmatrix}.\n$$\n\n额外的实现细节：\n- 使用稳定性阈值 $\\tau = 10^6$。\n- 如果在强制非负性后，未归一化的校正向量变成零向量，则在归一化之前将其设置为 $\\pi$。\n- 程序必须生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表（例如，$[\\dots]$）。所有浮点值必须按上述规定进行四舍五入。",
            "solution": "问题陈述经评估是有效的。它在科学上基于概率论和机器学习的原理，提法恰当，目标明确，数据充分，并使用客观、正式的语言表述。它代表了带噪声标签学习领域中一个标准的、非平凡的问题。因此，我们可以继续进行完整的推导和求解。\n\n目标是使用类别条件标签噪声模型，推导给定输入 $X=x$ 时干净标签 $Y$ 的后验概率 $p(Y \\mid X=x)$ 的可计算表达式。提供的组件如下：\n1.  一个列随机混淆矩阵 $C \\in \\mathbb{R}^{K \\times K}$，其中 $C_{ij} = p(\\hat{Y} = i \\mid Y = j)$ 表示当干净标签为 $Y=j$ 时观测到噪声标签 $\\hat{Y}=i$ 的概率。\n2.  来自一个分类器的预测噪声标签后验向量 $\\hat{p}(x) \\in \\mathbb{R}^K$，其分量为 $\\hat{p}_i(x) = p(\\hat{Y} = i \\mid X=x)$。\n3.  关于干净标签的先验 $\\pi \\in \\mathbb{R}^K$，其分量为 $\\pi_j = \\pi(Y=j)$。\n\n我们的推导从全概率公式开始，以建立期望的干净后验 $p(Y=j \\mid X=x)$ 与观测到的噪声后验 $p(\\hat{Y}=i \\mid X=x)$ 之间的关系。我们可以通过对所有可能的干净标签 $j \\in \\{1, \\dots, K\\}$ 进行边缘化来表示观测到噪声标签 $i$ 的概率：\n$$\np(\\hat{Y}=i \\mid X=x) = \\sum_{j=1}^{K} p(\\hat{Y}=i, Y=j \\mid X=x)\n$$\n使用条件概率的定义 $p(A, B \\mid C) = p(A \\mid B, C) p(B \\mid C)$，我们可以重写求和项内部的表达式：\n$$\np(\\hat{Y}=i \\mid X=x) = \\sum_{j=1}^{K} p(\\hat{Y}=i \\mid Y=j, X=x) p(Y=j \\mid X=x)\n$$\n该问题是在类别条件噪声的标准假设下定义的，该假设假定在给定干净标签 $Y$ 的条件下，标签噪声过程与输入特征 $X$ 无关。这形式化地表示为 $\\hat{Y} \\perp X \\mid Y$，意味着：\n$$\np(\\hat{Y}=i \\mid Y=j, X=x) = p(\\hat{Y}=i \\mid Y=j)\n$$\n这一项正是混淆矩阵的元素 $C_{ij}$。将其代入我们的方程可得：\n$$\np(\\hat{Y}=i \\mid X=x) = \\sum_{j=1}^{K} C_{ij} \\, p(Y=j \\mid X=x)\n$$\n这个基本方程对每个噪声标签 $i \\in \\{1, \\dots, K\\}$ 都成立。通过为噪声后验定义一个列向量 $\\hat{p}(x)$（其元素为 $\\hat{p}_i(x)$）和为干净后验定义一个列向量 $p(x)$（其元素为 $p_j(x) = p(Y=j \\mid X=x)$），我们可以将这个线性方程组表示为矩阵形式：\n$$\n\\hat{p}(x) = C p(x)\n$$\n这个关系表明，混淆矩阵 $C$ 充当一个线性算子，它将干净后验概率向量变换为可观测的噪声后验概率向量。\n\n为了估计干净后验 $p(x)$，我们必须对这个线性变换求逆。这表明解的形式为 $p(x) = C^{\\dagger} \\hat{p}(x)$，其中 $C^{\\dagger}$ 是 $C$ 的一个合适的逆。逆的选择对数值稳定性至关重要。\n- 如果 $C$ 是可逆的（即，其秩等于其维度 $K$）并且是良态的，我们可以使用精确的矩阵逆 $C^{-1}$。\n- 如果一个矩阵的条件数很大，那么它是病态的，这意味着输入 $\\hat{p}(x)$ 中的微小误差可能会在输出中被极大地放大。问题指定使用 2-范数条件数 $\\kappa_2(C) = \\|C\\|_2 \\|C^{-1}\\|_2$。如果 $\\kappa_2(C)$ 超过稳定性阈值 $\\tau = 10^6$，则认为该矩阵对于求逆而言是数值不稳定的。\n- 如果 $C$ 是奇异的（不可逆）或数值不稳定的，Moore-Penrose 伪逆 $C^{+}$提供了一个鲁棒的替代方案。它能找到线性系统的最小范数最小二乘解，这在唯一、稳定的解不可用时是一种有原则的方法。\n\n令 $p_{inv}(x) = C^{\\dagger} \\hat{p}(x)$ 是通过对噪声模型求逆得到的干净后验估计值，其中 $C^{\\dagger}$ 根据稳定性分析选择为 $C^{-1}$ 或 $C^{+}$。该估计值是通过分类器输出 $\\hat{p}(x)$ 从特定于实例的信息 $x$ 中推导出来的。\n\n问题进一步要求并入全局类别先验 $\\pi$。遵循将 $\\pi$ “以乘法方式并入未归一化的干净后验” 的指令，我们将其解释为贝叶斯更新。关于类别分布的先验信念 $\\pi$ 由特定于实例的证据（我们取为 $p_{inv}(x)$）来更新。因此，未归一化的后验由这两个向量的逐元素（Hadamard）积形成：\n$$\np_{unnorm}(x) = p_{inv}(x) \\odot \\pi\n$$\n这一步将关于类别频率的通用知识（$\\pi$）与给定输入实例（$x$）的特定证据结合起来。\n\n得到的向量 $p_{unnorm}(x)$ 不保证是一个有效的概率分布。由于数值误差或模型设定不当，其分量可能为负，并且其和不一定为 $1$。为了生成一个有效的后验，我们必须执行最后两个步骤：\n1.  **强制非负性**：负值对于概率而言是不合理的。我们将 $p_{unnorm}(x)$ 的任何负分量设置为 $0$。令这个裁剪后的向量为 $p'_{unnorm}(x)$。在结果为零向量的边缘情况下，我们将其重置为先验 $\\pi$ 作为后备方案。\n2.  **归一化**：为确保概率和为 $1$，我们通过将向量除以其各分量之和来进行归一化。这是一个必要的步骤，因为前面的操作（求逆、乘法）不保留和为1的性质。\n$$\np_{final}(x)_j = \\frac{p'_{unnorm, j}(x)}{\\sum_{k=1}^{K} p'_{unnorm, k}(x)}\n$$\n\n这就完成了对一个有原则且可计算的校正后验表达式的推导。完整的算法如下：\n\n**算法：标签噪声校正**\n1.  **输入**：混淆矩阵 $C$、噪声后验 $\\hat{p}(x)$ 和先验 $\\pi$。\n2.  **预处理**：如果 $\\hat{p}(x)$ 和 $\\pi$ 的和不为 $1$，则将它们重新归一化。\n3.  **稳定性分析**：\n    a. 计算 $C$ 的秩以检查可逆性。\n    b. 计算 2-范数条件数 $\\kappa_2(C)$。\n    c. 如果 $\\text{rank}(C)=K$ 且 $\\kappa_2(C) \\le 10^6$，选择精确逆矩阵 $C^{-1}$。\n    d. 否则，选择 Moore-Penrose 伪逆 $C^{+}$。\n4.  **校正**：计算初始干净后验估计：$p_{inv}(x) \\leftarrow C^{\\dagger} \\hat{p}(x)$。\n5.  **先验并入**：计算未归一化的后验：$p_{unnorm}(x) \\leftarrow p_{inv}(x) \\odot \\pi$。\n6.  **后处理**：\n    a. 强制非负性：$p'_{unnorm}(x) \\leftarrow \\max(p_{unnorm}(x), 0)$。\n    b. 如果 $\\sum_k p'_{unnorm,k}(x)$ 接近 $0$，则设置 $p'_{unnorm}(x) \\leftarrow \\pi$。\n    c. 归一化：$p_{final}(x) \\leftarrow p'_{unnorm}(x) / \\sum_k p'_{unnorm,k}(x)$。\n7.  **输出**：校正后的后验概率向量 $p_{final}(x)$。\n\n该算法对奇异或病态的噪声模型具有鲁棒性，并能正确地将特定于实例的证据与先验知识相结合，以产生一个有效的、校正后的概率分布。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport json\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for label noise correction.\n    \"\"\"\n    \n    # Stability threshold for condition number\n    TAU = 1e6\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (K=3, well-conditioned)\n        (\n            np.array([[0.8, 0.1, 0.1], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]]),\n            np.array([0.5, 0.3, 0.2]),\n            np.array([0.7, 0.2, 0.1])\n        ),\n        # Test case 2 (K=3, nearly singular but invertible)\n        (\n            np.array([[0.49, 0.50, 0.01], [0.48, 0.49, 0.01], [0.03, 0.01, 0.98]]),\n            np.array([1/3, 1/3, 1/3]),\n            np.array([0.50, 0.49, 0.01])\n        ),\n        # Test case 3 (K=3, singular)\n        (\n            np.array([[0.7, 0.7, 0.1], [0.2, 0.2, 0.2], [0.1, 0.1, 0.7]]),\n            np.array([0.2, 0.5, 0.3]),\n            np.array([0.4, 0.35, 0.25])\n        ),\n        # Test case 4 (K=3, extreme prior, inputs need renormalization)\n        (\n            np.array([[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]]),\n            np.array([0.99, 0.005, 0.005]),\n            np.array([0.34, 0.33, 0.35])\n        ),\n        # Test case 5 (K=2)\n        (\n            np.array([[0.95, 0.10], [0.05, 0.90]]),\n            np.array([0.4, 0.6]),\n            np.array([0.6, 0.4])\n        ),\n    ]\n\n    results = []\n    for C, pi, p_hat in test_cases:\n        # Get dimension K\n        K = C.shape[0]\n\n        # --- Step 1: Normalize inputs ---\n        p_hat_sum = np.sum(p_hat)\n        if not np.isclose(p_hat_sum, 1.0):\n            p_hat = p_hat / p_hat_sum\n        \n        pi_sum = np.sum(pi)\n        if not np.isclose(pi_sum, 1.0):\n            pi = pi / pi_sum\n\n        # --- Step 2: Analyze confusion matrix C ---\n        matrix_rank = np.linalg.matrix_rank(C)\n        is_invertible = (matrix_rank == K)\n\n        cond_num = np.linalg.cond(C)\n        is_stable = (cond_num = TAU)\n\n        # --- Step 3: Choose inverse method ---\n        if is_invertible and is_stable:\n            method_code = 1\n            C_inv = np.linalg.inv(C)\n        else:\n            method_code = 0\n            C_inv = np.linalg.pinv(C)\n\n        # --- Step 4: Apply inverse and incorporate prior ---\n        p_inv = C_inv @ p_hat\n        p_unnorm = p_inv * pi # Element-wise multiplication\n\n        # --- Step 5: Enforce nonnegativity and handle zero vector ---\n        p_unnorm_clipped = np.maximum(p_unnorm, 0)\n        \n        # If clipping results in a zero vector, fallback to the prior\n        if np.isclose(np.sum(p_unnorm_clipped), 0.0):\n            p_unnorm_clipped = pi\n\n        # --- Step 6: Final normalization ---\n        p_unnorm_sum = np.sum(p_unnorm_clipped)\n        if np.isclose(p_unnorm_sum, 0.0):\n            # This case happens if the fallback to pi was needed and pi was a zero vector\n            # A safe default is a uniform distribution.\n            corrected_posterior = np.full(K, 1.0 / K)\n        else:\n            corrected_posterior = p_unnorm_clipped / p_unnorm_sum\n\n        # --- Step 7: Format output ---\n        cond_rounded = round(cond_num, 3)\n        posterior_rounded = [round(p, 6) for p in corrected_posterior]\n        \n        results.append([\n            is_invertible,\n            is_stable,\n            cond_rounded,\n            method_code,\n            posterior_rounded\n        ])\n\n    # Convert boolean to lowercase 'true'/'false' and remove spaces for compact output\n    # using a custom string representation build.\n    def format_result_list(lst):\n        items = []\n        for item in lst:\n            if isinstance(item, bool):\n                items.append(str(item).lower())\n            elif isinstance(item, list):\n                items.append(f\"[{','.join(map(str, item))}]\")\n            else:\n                items.append(str(item))\n        return f\"[{','.join(items)}]\"\n\n    output_str = f\"[{','.join([format_result_list(res) for res in results])}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}