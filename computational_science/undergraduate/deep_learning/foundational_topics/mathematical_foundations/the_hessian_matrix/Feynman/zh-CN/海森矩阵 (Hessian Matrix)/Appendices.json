{
    "hands_on_practices": [
        {
            "introduction": "Hessian矩阵为我们提供了一幅函数局部曲率的“地图”。通过在梯度为零的关键点上检验Hessian矩阵的性质，我们可以判断该点是山峰（局部最大值）、山谷（局部最小值）还是鞍点。本练习  为你提供了一个应用二阶导数检验的机会，这是分析优化地形的基本工具，你将使用预先计算好的Hessian矩阵分量来对一个商业模型中的运营点进行分类。",
            "id": "2215318",
            "problem": "一位运筹学分析师正在为一家制造公司建立周利润模型 $P(x, y)$。该利润是两个变量的函数：$x$ 是主生产线的运行小时数，而 $y$ 是次级、更专业的生产线的运行小时数。该分析师已确定点 $(x_0, y_0) = (40, 15)$ 是利润函数的一个临界点，这意味着利润关于 $x$ 和 $y$ 的一阶偏导数在该点均为零。\n\n为了确定这个运营计划是代表局部最大利润、局部最小利润还是鞍点，必须应用二阶导数检验。通过数据分析，利润函数 $P(x,y)$ 在临界点 $(40, 15)$ 的二阶偏导数已被评估，结果如下：\n*   $\\frac{\\partial^2 P}{\\partial x^2} \\bigg|_{(40,15)} = -8.0$\n*   $\\frac{\\partial^2 P}{\\partial y^2} \\bigg|_{(40,15)} = -12.0$\n*   $\\frac{\\partial^2 P}{\\partial x \\partial y} \\bigg|_{(40,15)} = 9.0$\n\n根据这些信息，判断临界点 $(40, 15)$ 的性质。\n\nA) 局部最大值\nB) 局部最小值\nC) 鞍点\nD) 根据所提供的信息无法确定分类。",
            "solution": "在一个临界点，二元函数的二阶导数检验使用Hessian矩阵\n$$\nH(x,y)=\\begin{pmatrix}\nP_{xx}(x,y)  P_{xy}(x,y) \\\\\nP_{yx}(x,y)  P_{yy}(x,y)\n\\end{pmatrix},\n$$\n及其行列式\n$$\nD(x,y)=P_{xx}(x,y)P_{yy}(x,y)-\\left(P_{xy}(x,y)\\right)^{2}.\n$$\n在点 $(40,15)$，给定的值为 $P_{xx}=-8.0$，$P_{yy}=-12.0$，以及 $P_{xy}=9.0$（根据混合偏导数的相等性，在适用情况下，$P_{xy}=P_{yx}$）。计算行列式：\n$$\nD(40,15)=(-8.0)(-12.0)-(9.0)^{2}=96-81=15.\n$$\n因此 $D(40,15)>0$。二阶导数检验法则如下：\n- 如果 $D>0$ 且 $P_{xx}<0$，则该点是局部最大值。\n- 如果 $D>0$ 且 $P_{xx}>0$，则该点是局部最小值。\n- 如果 $D<0$，则该点是鞍点。\n- 如果 $D=0$，则检验无法得出结论。\n\n在这里，$D(40,15)>0$ 且 $P_{xx}(40,15)=-8.0<0$，所以该临界点是一个局部最大值。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "尽管对关键点进行分类很有用，但更深入的分析需要理解最大和最小曲率的“方向”，这由Hessian矩阵的特征向量及其对应的特征值给出。特别是，最大特征值指出了最陡峭的曲率方向，这可能对优化算法构成挑战。本练习  将介绍幂迭代法，这是一种高效的算法，它仅通过重复的Hessian-向量积（Hessian-vector products）来找到Hessian矩阵的最大特征值，而无需进行完整的特征分解。",
            "id": "3186508",
            "problem": "考虑一个在深度学习中使用的可微标量损失函数 $L(\\mathbf{w})$，其中 $\\mathbf{w} \\in \\mathbb{R}^d$ 表示参数向量。点 $\\mathbf{w}$ 处的 Hessian 矩阵 $H(\\mathbf{w})$ 定义为二阶偏导数的方阵，其元素为 $H_{ij}(\\mathbf{w}) = \\frac{\\partial^2 L(\\mathbf{w})}{\\partial w_i \\partial w_j}$。对于具有连续二阶导数的函数，$H(\\mathbf{w})$ 是对称的。$L$ 在 $\\mathbf{w}$ 点沿方向 $\\mathbf{v} \\in \\mathbb{R}^d$ 的二阶泰勒展开由 $L(\\mathbf{w} + \\mathbf{v}) \\approx L(\\mathbf{w}) + \\nabla L(\\mathbf{w})^\\top \\mathbf{v} + \\frac{1}{2} \\mathbf{v}^\\top H(\\mathbf{w}) \\mathbf{v}$ 给出。量 $\\mathbf{v}^\\top H(\\mathbf{w}) \\mathbf{v}$ 表征了沿 $\\mathbf{v}$ 方向的局部曲率，其在约束 $\\mathbf{v}^\\top \\mathbf{v} = 1$ 下的极值在 $H(\\mathbf{w})$ 的特征向量处取得。相关的极值是特征值，而最大的代数特征值近似了在 $\\mathbf{w}$ 点的最大局部曲率方向。\n\n你的任务是实现一个程序，仅使用重复的 Hessian-向量积和归一化，再结合 Rayleigh 商，来近似给定实对称矩阵 $H$（解释为 Hessian 矩阵）的最大代数特征值。不要构建或使用任何闭式特征分解。在每次迭代中，通过应用矩阵 $H$ 并对结果进行归一化来计算下一个方向，然后评估 Rayleigh 商以更新特征值估计。迭代直至 Rayleigh 商的绝对变化量低于指定的容差或达到最大迭代次数上限。如果在任何步骤中，Hessian-向量积是零向量（即其欧几里得范数是零或低于机器安全阈值），则终止并返回 $0$ 作为特征值估计。\n\n对所有测试用例使用以下固定参数：\n- 初始向量 $\\mathbf{v}_0$ 等于适当维度的全1向量，并归一化为单位欧几里得范数。\n- 连续迭代之间 Rayleigh 商的绝对变化量的容差为 $10^{-10}$。\n- 最大迭代次数为 $1000$。\n- 将每个估计的最大代数特征值输出为浮点数，四舍五入到六位小数。\n\n测试套件（下面的每个 $H$ 都是对称的，应被视为某个参数点上的 Hessian 矩阵）：\n1. 正常路径，对称正定 $3 \\times 3$ 矩阵：\n   $$H_1 = \\begin{bmatrix}\n   4  1  0 \\\\\n   1  3  0 \\\\\n   0  0  2\n   \\end{bmatrix}.$$\n2. 具有主导正曲率的不定 $3 \\times 3$ 矩阵：\n   $$H_2 = \\begin{bmatrix}\n   2  0.5  0 \\\\\n   0.5  -0.5  0 \\\\\n   0  0  0.2\n   \\end{bmatrix}.$$\n3. 顶部曲率简并（最大特征值重复）的 $3 \\times 3$ 矩阵：\n   $$H_3 = \\begin{bmatrix}\n   3  0  0 \\\\\n   0  3  0 \\\\\n   0  0  1\n   \\end{bmatrix}.$$\n4. 边界情况（无曲率）的 $3 \\times 3$ 矩阵：\n   $$H_4 = \\begin{bmatrix}\n   0  0  0 \\\\\n   0  0  0 \\\\\n   0  0  0\n   \\end{bmatrix}.$$\n5. 病态对称正定 $5 \\times 5$ 矩阵：\n   $$H_5 = \\begin{bmatrix}\n   10^{-6}  10^{-5}  0  0  0 \\\\\n   10^{-5}  10^{-2}  10^{-4}  0  0 \\\\\n   0  10^{-4}  10^{-1}  10^{-3}  0 \\\\\n   0  0  10^{-3}  1  10^{-2} \\\\\n   0  0  0  10^{-2}  10\n   \\end{bmatrix}.$$\n\n程序要求：\n- 对于每个测试矩阵 $H$，使用上述迭代过程近似其最大代数特征值。\n- 生成单行输出，其中包含按测试套件 $[H_1,H_2,H_3,H_4,H_5]$ 顺序排列的结果，形式为用方括号括起来的逗号分隔列表。每个条目必须是四舍五入到六位小数的浮点数，不带任何附加文本。例如，输出必须看起来像 $[x_1,x_2,x_3,x_4,x_5]$，其中每个 $x_i$ 是矩阵 $H_i$ 的四舍五入浮点数结果。",
            "solution": "该问题要求实现一个迭代算法，以找到给定实对称矩阵 $H$ 的最大代数特征值。提供的物理背景是深度学习中损失函数 $L(\\mathbf{w})$ 的 Hessian 矩阵 $H(\\mathbf{w})$，其中最大特征值对应于最大局部曲率的方向。指定的算法是幂迭代法，结合 Rayleigh 商来估计特征值。\n\n该方法基于以下原理：对于一个可对角化的矩阵 $H$，将它重复应用于一个任意的非零向量 $\\mathbf{v}_0$，将使结果向量逐渐与对应于最大模特征值（记为 $\\lambda_{dom}$）的特征向量对齐。设 $H$ 的特征值为 $|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_d|$，对应的特征向量为 $\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_d$。初始向量 $\\mathbf{v}_0$ 可以表示为这些特征向量的线性组合：$\\mathbf{v}_0 = c_1\\mathbf{u}_1 + c_2\\mathbf{u}_2 + \\dots + c_d\\mathbf{u}_d$。假设 $c_1 \\neq 0$，重复应用 $H$ 可得：\n$$H^k\\mathbf{v}_0 = c_1\\lambda_1^k\\mathbf{u}_1 + c_2\\lambda_2^k\\mathbf{u}_2 + \\dots + c_d\\lambda_d^k\\mathbf{u}_d = \\lambda_1^k \\left( c_1\\mathbf{u}_1 + c_2\\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k\\mathbf{u}_2 + \\dots + c_d\\left(\\frac{\\lambda_d}{\\lambda_1}\\right)^k\\mathbf{u}_d \\right)$$\n如果 $|\\lambda_1| > |\\lambda_2|$（即存在唯一的模最大主特征值），则当 $k \\to \\infty$ 时，对于 $i > 1$ 的项 $(\\frac{\\lambda_i}{\\lambda_1})^k$ 趋近于 $0$。因此，向量 $H^k\\mathbf{v}_0$ 变得越来越平行于主特征向量 $\\mathbf{u}_1$。为了防止 $H^k\\mathbf{v}_0$ 的模发散或消失，每一步都对向量进行归一化。这就导出了向量的迭代更新规则：\n$$\\mathbf{v}_{k+1} = \\frac{H\\mathbf{v}_k}{\\|H\\mathbf{v}_k\\|_2}$$\n其中 $\\mathbf{v}_k$ 是第 $k$ 次迭代时的归一化向量估计。\n\n一旦获得了特征向量的估计 $\\mathbf{v}_k$，就可以使用 Rayleigh 商来估计相应的特征值，对于非零向量 $\\mathbf{v}$，Rayleigh 商定义为：\n$$R_H(\\mathbf{v}) = \\frac{\\mathbf{v}^\\top H \\mathbf{v}}{\\mathbf{v}^\\top \\mathbf{v}}$$\n如果 $\\mathbf{v}$ 是一个精确的特征向量，则 $H\\mathbf{v} = \\lambda\\mathbf{v}$，Rayleigh 商将得出精确的特征值 $\\lambda$。由于我们的迭代向量 $\\mathbf{v}_k$ 被归一化以至于 $\\mathbf{v}_k^\\top \\mathbf{v}_k = 1$，表达式简化为 $R_H(\\mathbf{v}_k) = \\mathbf{v}_k^\\top H \\mathbf{v}_k$。随着 $\\mathbf{v}_k$ 收敛到主特征向量 $\\mathbf{u}_1$，Rayleigh 商 $R_H(\\mathbf{v}_k)$ 收敛到主特征值 $\\lambda_1$。对于所提供的所有测试用例，最大的代数特征值是正的，并且也具有最大的模，因此该方法将正确地收敛到期望的值。\n\n根据问题规范，要实现的算法如下：\n\n1. **初始化**：\n   - 给定矩阵 $H \\in \\mathbb{R}^{d \\times d}$。\n   - 初始向量 $\\mathbf{v}_0$ 是维度为 $d$ 的全1向量，归一化为欧几里得范数为 $1$：$\\mathbf{v}_0 = \\frac{1}{\\sqrt{d}}[1, 1, \\dots, 1]^\\top$。\n   - 收敛容差设置为 $\\epsilon = 10^{-10}$。\n   - 最大迭代次数为 $N_{max} = 1000$。\n   - 初始化特征值估计，例如 $\\lambda_{current} = 0$，并将 $\\lambda_{previous}$ 设为一个像 $\\infty$ 的值，以确保第一次迭代的收敛性检查不会通过。\n\n2. **迭代**：对于 $k = 0, 1, 2, \\dots, N_{max}-1$：\n   a. 检查收敛性：如果 $|\\lambda_{current} - \\lambda_{previous}|  \\epsilon$，则算法已收敛。终止并返回 $\\lambda_{current}$。\n   b. 更新前一个特征值估计：$\\lambda_{previous} \\leftarrow \\lambda_{current}$。\n   c. 计算下一个未归一化的向量：$\\mathbf{w} = H \\mathbf{v}_k$。\n   d. 检查零特征值情况：如果 $\\|\\mathbf{w}\\|_2$ 低于机器安全阈值，这意味着 $\\mathbf{v}_k$ 位于 $H$ 的零空间中（或者 $H$ 是零矩阵）。相应的特征值为 $0$。根据指令，终止并返回 $0$。\n   e. 归一化向量以获得下一个迭代向量：$\\mathbf{v}_{k+1} = \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|_2}$。为下一步方便，将这个新向量简记为 $\\mathbf{v}$。\n   f. 通过使用新计算的向量 $\\mathbf{v}$ 评估 Rayleigh 商来更新当前特征值估计：$\\lambda_{current} = \\mathbf{v}^\\top H \\mathbf{v}$。这需要在循环内进行第二次矩阵-向量乘积，但严格遵守了问题描述中指定的操作顺序。\n\n3. **终止**：如果循环在未收敛的情况下完成（即达到 $N_{max}$ 次迭代），则返回最后计算的 $\\lambda_{current}$ 值。每个测试用例的最终结果四舍五入到六位小数。\n\n此过程构成了一个完整且稳健的方法，用于按要求近似最大代数特征值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_iteration(H: np.ndarray, tol: float = 1e-10, max_iter: int = 1000) - float:\n    \"\"\"\n    Approximates the largest algebraic eigenvalue of a symmetric matrix H\n    using the power iteration method with the Rayleigh quotient.\n\n    Args:\n        H: The symmetric matrix (as a numpy array).\n        tol: The tolerance for convergence, based on the absolute change\n             in the Rayleigh quotient.\n        max_iter: The maximum number of iterations.\n\n    Returns:\n        The estimated largest algebraic eigenvalue.\n    \"\"\"\n    # Get the dimension of the matrix\n    d = H.shape[0]\n\n    # 1. Initialize the vector v0 to the normalized all-ones vector.\n    v = np.ones(d, dtype=H.dtype)\n    v /= np.linalg.norm(v)\n\n    # Initialize eigenvalue estimates\n    lambda_current = 0.0\n    lambda_previous = np.inf  # Set to infinity to ensure the first diff is large\n\n    # A small number to check for zero vectors\n    machine_eps = np.finfo(H.dtype).eps\n\n    # 2. Iterate until convergence or max iterations\n    for _ in range(max_iter):\n        # Check for convergence based on the change in the eigenvalue estimate\n        if np.abs(lambda_current - lambda_previous)  tol:\n            break\n\n        # Store the previous eigenvalue estimate\n        lambda_previous = lambda_current\n\n        # Compute the Hessian-vector product\n        w = H @ v\n        \n        # Calculate the norm of the resulting vector\n        w_norm = np.linalg.norm(w)\n\n        # Check for the special case where H*v is the zero vector\n        if w_norm  machine_eps:\n            # The eigenvalue is 0. Terminate and return 0.\n            lambda_current = 0.0\n            break\n\n        # Normalize the vector to get the next iterate\n        v = w / w_norm\n\n        # Update the eigenvalue estimate using the Rayleigh quotient with the new vector v.\n        # This requires a second matrix-vector product in the loop as per a\n        # literal interpretation of the problem statement \"compute next direction...,\n        # then evaluate the Rayleigh quotient\".\n        lambda_current = v.T @ (H @ v)\n\n    return lambda_current\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Test suite as specified in the problem statement\n    H1 = np.array([\n        [4.0, 1.0, 0.0],\n        [1.0, 3.0, 0.0],\n        [0.0, 0.0, 2.0]\n    ])\n\n    H2 = np.array([\n        [2.0, 0.5, 0.0],\n        [0.5, -0.5, 0.0],\n        [0.0, 0.0, 0.2]\n    ])\n\n    H3 = np.array([\n        [3.0, 0.0, 0.0],\n        [0.0, 3.0, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n\n    H4 = np.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0]\n    ])\n    \n    H5 = np.array([\n        [1e-6, 1e-5, 0.0, 0.0, 0.0],\n        [1e-5, 1e-2, 1e-4, 0.0, 0.0],\n        [0.0, 1e-4, 1e-1, 1e-3, 0.0],\n        [0.0, 0.0, 1e-3, 1.0, 1e-2],\n        [0.0, 0.0, 0.0, 1e-2, 10.0]\n    ])\n\n    test_cases = [H1, H2, H3, H4, H5]\n    \n    results = []\n    for H in test_cases:\n        # Calculate the largest eigenvalue for the current matrix\n        largest_eigenvalue = power_iteration(H)\n        \n        # Format the result to six decimal places and append\n        results.append(f\"{largest_eigenvalue:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "对于大型神经网络，Hessian矩阵的规模是天文数字，无法被显式地构建出来。然而，许多先进的优化和分析方法（例如前一个练习中的幂迭代法）仅仅需要计算Hessian矩阵与一个向量的乘积（即 $Hv$）。本编程练习  将带你进入现代深度学习优化的核心，要求你使用自动微分技术高效地实现这种Hessian-向量积的计算，该技术巧妙地避免了构建完整Hessian矩阵的需要。",
            "id": "3186600",
            "problem": "您的任务是，在一个小型前馈神经网络中，针对一个标量损失函数，利用自动微分的原理来推导、实现和验证Hessian向量积。考虑一个单隐藏层的多层感知机 (MLP)，它使用双曲正切激活函数，将$\\mathbb{R}^d$中的输入映射到一个标量输出。模型参数被收集到一个单一向量$\\theta \\in \\mathbb{R}^p$中，该向量由两层的权重和偏置拼接而成。网络定义如下\n$$\n\\mathbf{a}_1 = \\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1,\\quad\n\\mathbf{h}_1 = \\tanh(\\mathbf{a}_1),\\quad\n\\hat{y} = \\mathbf{W}_2^\\top \\mathbf{h}_1 + b_2,\n$$\n其中 $\\mathbf{W}_1 \\in \\mathbb{R}^{m \\times d}$、$\\mathbf{b}_1 \\in \\mathbb{R}^m$、$\\mathbf{W}_2 \\in \\mathbb{R}^m$ 和 $b_2 \\in \\mathbb{R}$ 是参数，$\\mathbf{x} \\in \\mathbb{R}^d$ 是输入，$\\hat{y} \\in \\mathbb{R}$ 是预测的标量输出。对于一个数据集 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$，经验损失为\n$$\nf(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2.\n$$\n设 $\\nabla f(\\theta)$ 表示 $f$ 关于 $\\theta$ 的梯度，设 $\\mathbf{H}(\\theta)$ 表示二阶导数的Hessian矩阵。对于给定的方向向量 $\\mathbf{v}\\in\\mathbb{R}^p$，Hessian向量积为 $\\mathbf{H}(\\theta)\\mathbf{v}$。\n\n从微分的基本定义和法则出发，推导通过自动微分计算Hessian向量积的 Pearlmutter 方法。具体来说，使用Hessian向量积作为梯度方向导数的定义，并应用链式法则来构建一个前向方向导数传递和一个反向传播，从而在不显式构造$\\mathbf{H}(\\theta)$的情况下得到$\\mathbf{H}(\\theta)\\mathbf{v}$。\n\n实现要求：\n- 使用 $d=3$, $m=4$ 和 $n=5$。按如下方式确定性地构建数据集：使用固定的种子 $42$，从标准正态分布中抽取输入 $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$ 和目标 $\\mathbf{y}\\in\\mathbb{R}^n$。使用种子 $0$ 从标准正态分布中抽取参数 $\\mathbf{W}_1$、$\\mathbf{b}_1$、$\\mathbf{W}_2$ 和 $b_2$ 并乘以 $0.1$ 进行缩放来初始化它们。激活函数是双曲正切函数 $\\tanh$，它是二阶可微的。\n- 实现两个函数：\n  1. 一个使用 Pearlmutter 方法和自动微分原理（通过网络传播方向导数并进行反向模式累积）返回 $\\mathbf{H}(\\theta)\\mathbf{v}$ 的函数。\n  2. 一个使用以下公式返回 $\\mathbf{H}(\\theta)\\mathbf{v}$ 的有限差分近似的函数\n     $$\n     \\frac{\\nabla f(\\theta + \\epsilon \\mathbf{v}) - \\nabla f(\\theta)}{\\epsilon},\n     $$\n     对于给定的 $\\epsilon  0$，通过标准反向传播计算在 $\\theta$ 和 $\\theta+\\epsilon\\mathbf{v}$ 处的梯度。\n- 在您的推导和实现中，使用导数 $\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)$ 和二阶导数 $\\frac{d^2}{dx^2}\\tanh(x) = -2\\,\\tanh(x)\\,\\big(1 - \\tanh^2(x)\\big)$。\n\n测试套件：\n- 对于所有测试，设 $\\mathbf{v}\\in\\mathbb{R}^p$ 是一个指定的方向，$\\epsilon$ 是有限差分步长。使用以下情况：\n  1. 一个随机单位范数方向，$\\epsilon = 10^{-5}$，容差为 $10^{-6}$。\n  2. 零方向 $\\mathbf{v}=\\mathbf{0}$，$\\epsilon = 10^{-5}$，容差为 $10^{-12}$。\n  3. 一个选择最后一个参数（对应于 $b_2$）的基向量方向，即 $\\mathbf{v} = \\mathbf{e}_{p}$，$\\epsilon = 10^{-6}$，容差为 $10^{-6}$。\n  4. 与情况1相同的随机单位范数方向，$\\epsilon = 10^{-9}$，容差为 $10^{-2}$。\n  5. 与情况1相同的随机单位范数方向，$\\epsilon = 10^{-1}$，容差为 $10^{-3}$。\n- 对于每个测试用例，计算相对误差\n  $$\n  r = \\frac{\\|\\mathbf{H}(\\theta)\\mathbf{v} - \\text{FD}(\\theta,\\mathbf{v},\\epsilon)\\|_2}{\\max\\left(10^{-12}, \\|\\text{FD}(\\theta,\\mathbf{v},\\epsilon)\\|_2\\right)},\n  $$\n  其中 $\\text{FD}(\\theta,\\mathbf{v},\\epsilon)$ 表示有限差分近似。测试结果是一个布尔值，表示 $r$ 是否小于或等于给定的容差。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，“[True,False,True,True,False]”）。\n- 本问题中不出现物理单位、角度或百分比；所有量都是无量纲的实数。",
            "solution": "用户希望使用 Pearlmutter 方法为一个小型神经网络推导并实现 Hessian 向量积。\n\n### 问题验证\n\n#### 步骤 1：提取已知条件\n\n-   **网络定义**：\n    -   $\\mathbf{a}_1 = \\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1$\n    -   $\\mathbf{h}_1 = \\tanh(\\mathbf{a}_1)$\n    -   $\\hat{y} = \\mathbf{W}_2^\\top \\mathbf{h}_1 + b_2$\n-   **参数和维度**：\n    -   输入：$\\mathbf{x} \\in \\mathbb{R}^d$\n    -   参数：$\\mathbf{W}_1 \\in \\mathbb{R}^{m \\times d}$, $\\mathbf{b}_1 \\in \\mathbb{R}^m$, $\\mathbf{W}_2 \\in \\mathbb{R}^m$, $b_2 \\in \\mathbb{R}$\n    -   参数向量：$\\theta \\in \\mathbb{R}^p$ (所有参数的拼接)\n    -   预测输出：$\\hat{y} \\in \\mathbb{R}$\n    -   常量：$d=3$, $m=4$, $n=5$。参数总数为 $p = m \\times d + m + m \\times 1 + 1 = 4 \\times 3 + 4 + 4 + 1 = 21$。\n-   **损失函数**：\n    -   $f(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2$ for dataset $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$.\n-   **目标**：\n    -   计算 Hessian 向量积 $\\mathbf{H}(\\theta)\\mathbf{v}$，其中 $\\mathbf{H}(\\theta)$ 是 $f(\\theta)$ 的 Hessian 矩阵，$\\mathbf{v} \\in \\mathbb{R}^p$ 是一个方向向量。\n-   **方法论**：\n    -   使用自动微分推导 Pearlmutter 方法，基于 Hessian 向量积作为梯度的方向导数的定义。\n-   **数据和参数生成**：\n    -   从种子为 42 的标准正态分布中生成输入 $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$ 和目标 $\\mathbf{y}\\in\\mathbb{R}^n$。\n    -   从种子为 0 的标准正态分布中生成参数 $\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, b_2$，并乘以 0.1 进行缩放。\n-   **提供的导数**：\n    -   $\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)$\n    -   $\\frac{d^2}{dx^2}\\tanh(x) = -2\\,\\tanh(x)\\,\\big(1 - \\tanh^2(x)\\big)$\n-   **验证方法**：\n    -   有限差分近似：$\\frac{\\nabla f(\\theta + \\epsilon \\mathbf{v}) - \\nabla f(\\theta)}{\\epsilon}$。\n-   **评估**：\n    -   相对误差：$r = \\frac{\\|\\mathbf{H}(\\theta)\\mathbf{v} - \\text{FD}(\\theta,\\mathbf{v},\\epsilon)\\|_2}{\\max\\left(10^{-12}, \\|\\text{FD}(\\theta,\\mathbf{v},\\epsilon)\\|_2\\right)}$。\n    -   指定了 $\\mathbf{v}$、$\\epsilon$ 和 $r$ 的容差的测试用例。\n\n#### 步骤 2：使用提取的已知条件进行验证\n\n-   **科学依据**：该问题是自动微分在神经网络中应用的一个标准练习。用于计算 Hessian 向量积的 Pearlmutter 方法是计算科学和机器学习优化领域中一个成熟的基础技术。所有数学运算都基于微积分和线性代数的标准法则。\n-   **适定性**：该问题是适定的。网络和损失函数是二阶可微的，确保了 Hessian 矩阵的存在。数据生成、参数初始化和测试的指令是确定和具体的，从而导向一个单一、可验证的解决方案。\n-   **客观性**：问题使用精确、客观的数学语言陈述。\n-   **完整性**：问题提供了所有必要的信息，包括网络架构、损失函数、参数维度、初始化方案以及用于验证和评估的显式公式。\n\n#### 步骤 3：结论和行动\n\n该问题是有效的。它具有科学合理性、适定性和完整性。将提供一个解决方案。\n\n---\n\n### Hessian向量积的推导\n\nPearlmutter 方法的核心在于认识到 Hessian 向量积 $\\mathbf{H}(\\theta)\\mathbf{v}$可以表示为梯度 $\\nabla f(\\theta)$ 在向量 $\\mathbf{v}$ 方向上的方向导数。我们为函数 $g(\\theta)$ 定义一个方向导数算子 $D_{\\mathbf{v}}[\\cdot]$ 如下：\n$$\nD_{\\mathbf{v}}[g(\\theta)] = \\frac{d}{d\\lambda} g(\\theta + \\lambda\\mathbf{v}) \\bigg|_{\\lambda=0}\n$$\n根据链式法则，这等价于 $(\\nabla_\\theta g(\\theta))^\\top \\mathbf{v}$。于是，Hessian 向量积为：\n$$\n\\mathbf{H}(\\theta)\\mathbf{v} = \\left(\\nabla_\\theta (\\nabla_\\theta f(\\theta))^\\top \\right) \\mathbf{v} = D_{\\mathbf{v}}[\\nabla_\\theta f(\\theta)]\n$$\n这一见解使我们能够通过将 $D_{\\mathbf{v}}$ 算子应用于计算 $\\nabla_\\theta f(\\theta)$ 的整个反向传播算法来计算 $\\mathbf{H}(\\theta)\\mathbf{v}$。这避免了显式地、计算成本高昂地构建完整的 Hessian 矩阵 $\\mathbf{H}(\\theta)$。该过程包括两个阶段：一个“前向对前向”传递，用于计算所有中间变量的方向导数；以及一个“前向对反向”传递，这本质上是第二次反向传播，用于计算梯度的方向导数。\n\n为提高效率，我们将使用批量公式。设输入数据为矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，目标输出为向量 $\\mathbf{y} \\in \\mathbb{R}^n$。参数 $\\theta$ 和方向向量 $\\mathbf{v}$ 被展开为其对应的矩阵/向量形式：$\\theta \\rightarrow (\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, b_2)$ 和 $\\mathbf{v} \\rightarrow (\\mathbf{v}_{W1}, \\mathbf{v}_{b1}, \\mathbf{v}_{W2}, v_{b2})$。\n\n**1. 标准的前向和反向传播（用于梯度计算）**\n\n首先，我们回顾计算梯度 $\\nabla f(\\theta)$ 所需的标准前向和反向传播，这对于有限差分检验和作为 HVP 计算的基础都是必需的。\n\n**前向传播：**\n-   计算预激活值：$\\mathbf{A}_1 = \\mathbf{X} \\mathbf{W}_1^\\top + \\mathbf{b}_1^\\top \\in \\mathbb{R}^{n \\times m}$\n-   计算隐藏层激活值：$\\mathbf{H}_1 = \\tanh(\\mathbf{A}_1) \\in \\mathbb{R}^{n \\times m}$\n-   计算预测值：$\\hat{\\mathbf{y}} = \\mathbf{H}_1 \\mathbf{W}_2 + b_2 \\in \\mathbb{R}^n$\n\n**反向传播（梯度计算）：**\n设 $\\bar{z} = \\frac{\\partial f}{\\partial z}$ 表示损失 $f$ 相对于变量 $z$ 的伴随（梯度）。损失函数为 $f = \\frac{1}{2n}\\sum_{i=1}^n (\\hat{y}_i - y_i)^2$。\n-   $\\bar{\\hat{\\mathbf{y}}} = \\frac{\\partial f}{\\partial \\hat{\\mathbf{y}}} = \\frac{1}{n}(\\hat{\\mathbf{y}} - \\mathbf{y})$\n-   $\\bar{b}_2 = \\frac{\\partial f}{\\partial b_2} = \\sum_{i=1}^n \\bar{\\hat{y}}_i$\n-   $\\bar{\\mathbf{W}}_2 = \\frac{\\partial f}{\\partial \\mathbf{W}_2} = \\mathbf{H}_1^\\top \\bar{\\hat{\\mathbf{y}}}}$\n-   $\\bar{\\mathbf{H}}_1 = \\frac{\\partial f}{\\partial \\mathbf{H}_1} = \\bar{\\hat{\\mathbf{y}}} \\mathbf{W}_2^\\top$ （使用外积语义，每个样本一行）\n-   $\\bar{\\mathbf{A}}_1 = \\frac{\\partial f}{\\partial \\mathbf{A}_1} = \\bar{\\mathbf{H}}_1 \\odot \\frac{\\partial \\mathbf{H}_1}{\\partial \\mathbf{A}_1} = \\bar{\\mathbf{H}}_1 \\odot (1 - \\mathbf{H}_1^2)$\n-   $\\bar{\\mathbf{b}}_1 = \\frac{\\partial f}{\\partial \\mathbf{b}_1} = \\sum_{i=1}^n (\\bar{\\mathbf{A}}_1)_{i,:}$ （对批次维度求和）\n-   $\\bar{\\mathbf{W}}_1 = \\frac{\\partial f}{\\partial \\mathbf{W}_1} = \\bar{\\mathbf{A}}_1^\\top \\mathbf{X}$\n\n**2. Pearlmutter 方法（用于 Hessian 向量积）**\n\n我们现在将算子 $D_{\\mathbf{v}}$ 应用于计算。设 $\\dot{z} = D_{\\mathbf{v}}[z]$ 表示变量 $z$ 的方向导数。\n\n**第一阶段：方向前向传播（前向对前向）**\n我们通过前向计算图传播方向导数。\n-   $\\dot{\\mathbf{A}}_1 = D_{\\mathbf{v}}[\\mathbf{X} \\mathbf{W}_1^\\top + \\mathbf{b}_1^\\top] = \\mathbf{X} \\mathbf{v}_{W1}^\\top + \\mathbf{v}_{b1}^\\top$\n-   $\\dot{\\mathbf{H}}_1 = D_{\\mathbf{v}}[\\tanh(\\mathbf{A}_1)] = \\tanh'(\\mathbf{A}_1) \\odot \\dot{\\mathbf{A}}_1 = (1 - \\mathbf{H}_1^2) \\odot \\dot{\\mathbf{A}}_1$\n-   $\\dot{\\hat{\\mathbf{y}}} = D_{\\mathbf{v}}[\\mathbf{H}_1 \\mathbf{W}_2 + b_2] = \\dot{\\mathbf{H}}_1 \\mathbf{W}_2 + \\mathbf{H}_1 \\mathbf{v}_{W2} + v_{b2}$\n\n**第二阶段：HVP 反向传播（前向对反向）**\n我们将 $D_{\\mathbf{v}}$ 算子应用于标准反向传播中的梯度方程。结果 $\\dot{\\bar{\\theta}} = D_{\\mathbf{v}}[\\bar{\\theta}]$ 就是我们想要的 Hessian 向量积 $\\mathbf{H}(\\theta)\\mathbf{v}$。这需要应用微分的乘法法则，$D_{\\mathbf{v}}[UV] = \\dot{U}V + U\\dot{V}$。\n-   $\\dot{\\bar{\\hat{\\mathbf{y}}}} = D_{\\mathbf{v}}[\\frac{1}{n}(\\hat{\\mathbf{y}} - \\mathbf{y})] = \\frac{1}{n}\\dot{\\hat{\\mathbf{y}}}}$\n-   $\\dot{\\bar{b}}_2 = D_{\\mathbf{v}}[\\sum_{i=1}^n \\bar{\\hat{y}}_i] = \\sum_{i=1}^n \\dot{\\bar{\\hat{y}}}_i$\n-   $\\dot{\\bar{\\mathbf{W}}}_2 = D_{\\mathbf{v}}[\\mathbf{H}_1^\\top \\bar{\\hat{\\mathbf{y}}}] = \\dot{\\mathbf{H}}_1^\\top \\bar{\\hat{\\mathbf{y}}} + \\mathbf{H}_1^\\top \\dot{\\bar{\\hat{\\mathbf{y}}}}$\n-   $\\dot{\\bar{\\mathbf{H}}}_1 = D_{\\mathbf{v}}[\\bar{\\hat{\\mathbf{y}}} \\mathbf{W}_2^\\top] = \\dot{\\bar{\\hat{\\mathbf{y}}}} \\mathbf{W}_2^\\top + \\bar{\\hat{\\mathbf{y}}} \\mathbf{v}_{W2}^\\top$\n-   $\\dot{\\bar{\\mathbf{A}}}_1 = D_{\\mathbf{v}}[\\bar{\\mathbf{A}}_1] = D_{\\mathbf{v}}[\\bar{\\mathbf{H}}_1 \\odot \\tanh'(\\mathbf{A}_1)] = \\dot{\\bar{\\mathbf{H}}}_1 \\odot \\tanh'(\\mathbf{A}_1) + \\bar{\\mathbf{H}}_1 \\odot D_{\\mathbf{v}}[\\tanh'(\\mathbf{A}_1)]$。\n    -   $D_{\\mathbf{v}}[\\tanh'(\\mathbf{A}_1)] = \\tanh''(\\mathbf{A}_1) \\odot \\dot{\\mathbf{A}}_1$。\n    -   使用给定的导数和 $\\mathbf{H}_1=\\tanh(\\mathbf{A}_1)$：\n    -   $\\dot{\\bar{\\mathbf{A}}}_1 = \\dot{\\bar{\\mathbf{H}}}_1 \\odot (1 - \\mathbf{H}_1^2) + \\bar{\\mathbf{H}}_1 \\odot (-2\\mathbf{H}_1 \\odot (1 - \\mathbf{H}_1^2)) \\odot \\dot{\\mathbf{A}}_1$\n-   $\\dot{\\bar{\\mathbf{b}}}_1 = D_{\\mathbf{v}}[\\sum_{i=1}^n (\\bar{\\mathbf{A}}_1)_{i,:}] = \\sum_{i=1}^n (\\dot{\\bar{\\mathbf{A}}}_1)_{i,:}$\n-   $\\dot{\\bar{\\mathbf{W}}}_1 = D_{\\mathbf{v}}[\\bar{\\mathbf{A}}_1^\\top \\mathbf{X}] = \\dot{\\bar{\\mathbf{A}}}_1^\\top \\mathbf{X}$\n\n得到的量 $(\\dot{\\bar{\\mathbf{W}}}_1, \\dot{\\bar{\\mathbf{b}}}_1, \\dot{\\bar{\\mathbf{W}}}_2, \\dot{\\bar{b}}_2)$ 在拼接后形成向量 $\\mathbf{H}(\\theta)\\mathbf{v}$。该过程在不构建 $p \\times p$ 的 Hessian 矩阵的情况下计算乘积，因此对于大的 $p$ 而言是高效的。\n\n**3. 有限差分近似**\n\n为了验证，我们使用梯度上的一阶有限差分来近似 Hessian 向量积：\n$$\n\\mathbf{H}(\\theta)\\mathbf{v} \\approx \\frac{\\nabla f(\\theta + \\epsilon \\mathbf{v}) - \\nabla f(\\theta)}{\\epsilon}\n$$\n这需要使用上面概述的标准反向传播算法进行两次完整的梯度计算，一次在 $\\theta$ 处，另一次在扰动点 $\\theta + \\epsilon \\mathbf{v}$ 处。这种近似的误差是 $O(\\epsilon)$ 阶的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives, implements, and validates the Hessian-vector product for a small MLP.\n    \"\"\"\n    # Define problem constants and initialize data/parameters\n    d, m, n = 3, 4, 5\n    p = d * m + m + m * 1 + 1\n\n    # Generate dataset\n    rng_data = np.random.default_rng(seed=42)\n    X = rng_data.standard_normal(size=(n, d))\n    y = rng_data.standard_normal(size=n)\n\n    # Initialize parameters\n    rng_params = np.random.default_rng(seed=0)\n    W1_init = rng_params.standard_normal(size=(m, d)) * 0.1\n    b1_init = rng_params.standard_normal(size=m) * 0.1\n    W2_init = rng_params.standard_normal(size=m) * 0.1\n    b2_init = rng_params.standard_normal() * 0.1\n\n    # --- Parameter Flattening and Unflattening Utilities ---\n    def _flatten_params(W1, b1, W2, b2):\n        return np.concatenate([\n            W1.flatten(),\n            b1.flatten(),\n            W2.flatten(),\n            np.array([b2]).flatten()\n        ])\n\n    def _unflatten_params(theta):\n        idx = 0\n        W1 = theta[idx:idx + m * d].reshape(m, d)\n        idx += m * d\n        b1 = theta[idx:idx + m]\n        idx += m\n        W2 = theta[idx:idx + m]\n        idx += m\n        b2 = theta[idx]\n        return W1, b1, W2, b2\n\n    # --- Core Implementation: Gradient and HVP ---\n\n    def _compute_grad(theta, X, y):\n        \"\"\"Computes the gradient of the loss function via backpropagation.\"\"\"\n        W1, b1, W2, b2 = _unflatten_params(theta)\n        num_samples = X.shape[0]\n\n        # Forward pass\n        A1 = X @ W1.T + b1\n        H1 = np.tanh(A1)\n        y_hat = H1 @ W2 + b2\n\n        # Backward pass (gradient calculation)\n        y_hat_bar = (y_hat - y) / num_samples\n        \n        b2_bar = np.sum(y_hat_bar)\n        W2_bar = H1.T @ y_hat_bar\n        H1_bar = np.outer(y_hat_bar, W2)\n        \n        A1_bar = H1_bar * (1 - H1**2)\n        \n        b1_bar = np.sum(A1_bar, axis=0)\n        W1_bar = A1_bar.T @ X\n\n        return _flatten_params(W1_bar, b1_bar, W2_bar, b2_bar)\n\n    def _compute_hvp_pearlmutter(theta, v, X, y):\n        \"\"\"Computes the Hessian-vector product using Pearlmutter's method.\"\"\"\n        W1, b1, W2, b2 = _unflatten_params(theta)\n        v_W1, v_b1, v_W2, v_b2 = _unflatten_params(v)\n        num_samples = X.shape[0]\n\n        # --- Standard Forward Pass ---\n        A1 = X @ W1.T + b1\n        H1 = np.tanh(A1)\n        y_hat = H1 @ W2 + b2\n\n        # --- Phase I: Directional Forward Pass (Forward-on-Forward) ---\n        dot_A1 = X @ v_W1.T + v_b1\n        dot_H1 = (1 - H1**2) * dot_A1\n        dot_y_hat = np.sum(dot_H1 * W2, axis=1) + H1 @ v_W2 + v_b2\n\n        # --- Phase II: HVP Backward Pass (Forward-on-Reverse) ---\n        # Gradients from standard backprop (needed for the second-order terms)\n        y_hat_bar = (y_hat - y) / num_samples\n        H1_bar = np.outer(y_hat_bar, W2)\n\n        # Directional derivatives of gradients\n        dot_y_hat_bar = dot_y_hat / num_samples\n        \n        dot_b2_bar = np.sum(dot_y_hat_bar)\n        dot_W2_bar = dot_H1.T @ y_hat_bar + H1.T @ dot_y_hat_bar\n        \n        dot_H1_bar = np.outer(dot_y_hat_bar, W2) + np.outer(y_hat_bar, v_W2)\n        \n        tanh_prime_A1 = 1 - H1**2\n        tanh_second_A1 = -2 * H1 * tanh_prime_A1\n        \n        dot_A1_bar = dot_H1_bar * tanh_prime_A1 + H1_bar * tanh_second_A1 * dot_A1\n\n        dot_b1_bar = np.sum(dot_A1_bar, axis=0)\n        dot_W1_bar = dot_A1_bar.T @ X\n\n        return _flatten_params(dot_W1_bar, dot_b1_bar, dot_W2_bar, dot_b2_bar)\n\n    def _compute_hvp_finite_diff(theta, v, X, y, epsilon):\n        \"\"\"Approximates the Hessian-vector product using finite differences.\"\"\"\n        grad_pos = _compute_grad(theta + epsilon * v, X, y)\n        grad_neg = _compute_grad(theta, X, y)\n        return (grad_pos - grad_neg) / epsilon\n\n    # --- Test Suite ---\n    theta_init = _flatten_params(W1_init, b1_init, W2_init, b2_init)\n    \n    # Generate a reusable random unit-norm direction for cases 1, 4, 5\n    rng_v = np.random.default_rng(seed=123)\n    v_rand_raw = rng_v.standard_normal(size=p)\n    v_rand_unit = v_rand_raw / np.linalg.norm(v_rand_raw)\n\n    test_cases = [\n        # (name, direction_vector, epsilon, tolerance)\n        (\"random_unit_v_e-5\", v_rand_unit, 1e-5, 1e-6),\n        (\"zero_v\", np.zeros(p), 1e-5, 1e-12),\n        (\"basis_v\", np.eye(1, p, p - 1).flatten(), 1e-6, 1e-6),\n        (\"random_unit_v_e-9\", v_rand_unit, 1e-9, 1e-2),\n        (\"random_unit_v_e-1\", v_rand_unit, 1e-1, 1e-3),\n    ]\n\n    results = []\n    for name, v, epsilon, tolerance in test_cases:\n        # Calculate HVP using both methods\n        hvp_ad = _compute_hvp_pearlmutter(theta_init, v, X, y)\n        hvp_fd = _compute_hvp_finite_diff(theta_init, v, X, y, epsilon)\n\n        # Compute relative error\n        norm_fd = np.linalg.norm(hvp_fd)\n        # The denominator is max(1e-12, norm_fd) to avoid division by zero\n        # when hvp_fd is exactly zero (as in the v=0 case).\n        denominator = max(1e-12, norm_fd)\n        \n        rel_error = np.linalg.norm(hvp_ad - hvp_fd) / denominator\n        \n        results.append(rel_error = tolerance)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}