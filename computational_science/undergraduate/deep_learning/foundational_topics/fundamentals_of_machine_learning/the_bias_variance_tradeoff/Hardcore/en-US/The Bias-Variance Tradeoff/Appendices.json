{
    "hands_on_practices": [
        {
            "introduction": "We often think of 'unbiased' as an ideal quality for a statistical estimator. This introductory practice challenges that intuition by asking a simple question: can a biased estimator be better? By computing and comparing the Mean Squared Error ($MSE$) for an unbiased estimator and a deliberately biased one, you will uncover the core principle of the bias-variance tradeoffâ€”that sometimes, accepting a small amount of bias can lead to a significant reduction in overall error.",
            "id": "1934147",
            "problem": "In statistical estimation theory, we often compare different estimators for a parameter based on their performance. Consider a random variable $X$ that follows a Bernoulli distribution with parameter $p$, denoted as $X \\sim \\text{Bernoulli}(p)$, where $p$ is the probability of success ($X=1$) and $1-p$ is the probability of failure ($X=0$).\n\nSuppose we have only a single observation of $X$ to estimate the unknown parameter $p$. Two different estimators are proposed:\n\n1.  The first estimator, $\\hat{p}_1$, is simply the observed outcome: $\\hat{p}_1 = X$.\n2.  The second estimator, $\\hat{p}_2$, is a fixed constant value: $\\hat{p}_2 = 0.8$.\n\nThe quality of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is often measured by its Mean Squared Error (MSE), which is defined as $\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$, where $E[\\cdot]$ denotes the expectation.\n\nAssuming the true value of the parameter is $p=0.9$, which of the two estimators has a lower MSE?\n\nA. The MSE of $\\hat{p}_1$ is lower.\n\nB. The MSE of $\\hat{p}_2$ is lower.\n\nC. The MSEs are equal.\n\nD. The comparison cannot be made with the given information.",
            "solution": "We are given a single observation $X \\sim \\text{Bernoulli}(p)$ and two estimators: $\\hat{p}_{1}=X$ and $\\hat{p}_{2}=0.8$. The Mean Squared Error is defined as $\\text{MSE}(\\hat{\\theta})=E[(\\hat{\\theta}-\\theta)^{2}]$. We compute the MSEs symbolically in terms of $p$ and then evaluate at $p=0.9$.\n\nFor $\\hat{p}_{1}=X$, using the bias-variance decomposition,\n$$\n\\text{MSE}(\\hat{p}_{1})= \\operatorname{Var}(X) + \\left(E[X]-p\\right)^{2}.\n$$\nFor $X \\sim \\text{Bernoulli}(p)$, $E[X]=p$ and $\\operatorname{Var}(X)=p(1-p)$. Thus,\n$$\n\\text{MSE}(\\hat{p}_{1})=p(1-p).\n$$\nAlternatively, directly:\n$$\n\\text{MSE}(\\hat{p}_{1})=E[(X-p)^{2}]=E[X^{2}] - 2pE[X] + p^{2} = E[X] - 2p^{2} + p^{2} = p - p^{2} = p(1-p).\n$$\n\nFor $\\hat{p}_{2}=0.8$, since it is a constant,\n$$\n\\text{MSE}(\\hat{p}_{2})=E[(0.8-p)^{2}]=(0.8-p)^{2}.\n$$\n\nNow evaluate at the true value $p=0.9$:\n$$\n\\text{MSE}(\\hat{p}_{1})=0.9(1-0.9)=0.09,\\qquad \\text{MSE}(\\hat{p}_{2})=(0.8-0.9)^{2}=0.01.\n$$\nSince $0.01  0.09$, the second estimator has the lower MSE. Therefore, the correct option is B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Regularization is a cornerstone technique for preventing overfitting, but how does it achieve this at a mathematical level? This practice guides you through a derivation of the bias term for the ridge regression estimator, a classic example of $L_2$ regularization. By analyzing the estimator's structure, you will see exactly how the regularization parameter $\\lambda$ introduces a systematic bias, providing a concrete link between the abstract goal of managing model complexity and its tangible effect on the bias-variance decomposition.",
            "id": "3180590",
            "problem": "Consider a fixed-design linear regression model with ridge regularization. Let $X \\in \\mathbb{R}^{n \\times p}$ denote a deterministic design matrix, $\\beta \\in \\mathbb{R}^{p}$ the unknown parameter vector, and $y \\in \\mathbb{R}^{n}$ the observed response generated by the model $y = X \\beta + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ is a zero-mean Gaussian noise vector independent of $X$. The ridge regression estimator with regularization parameter $\\lambda  0$ is defined as $\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} y$. \n\nLet $x_{0} \\in \\mathbb{R}^{p}$ be a new covariate vector at which we wish to predict $y_{0} = x_{0}^{\\top} \\beta + \\epsilon_{0}$ with $\\epsilon_{0} \\sim \\mathcal{N}(0, \\sigma^{2})$ independent of $\\epsilon$. The prediction at $x_{0}$ is $\\hat{y}_{0} = x_{0}^{\\top} \\hat{\\beta}_{\\lambda}$. The expected squared prediction error at $x_{0}$ admits the bias-variance-noise decomposition into the sum of squared bias, variance, and irreducible noise. \n\nAssume a singular value decomposition (SVD) $X = U D V^{\\top}$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{p \\times p}$ are orthogonal matrices and $D \\in \\mathbb{R}^{n \\times p}$ is diagonal with nonnegative singular values $d_{1}, \\dots, d_{r}  0$ on the diagonal and zeros thereafter, with $r = \\operatorname{rank}(X)$. The columns $v_{1}, \\dots, v_{p}$ of $V$ are the principal axes (eigenvectors of $X^{\\top} X$). Define the alignment coefficients $a_{i} = v_{i}^{\\top} x_{0}$ and $b_{i} = v_{i}^{\\top} \\beta$ for $i = 1, \\dots, p$.\n\nStarting from fundamental definitions and facts about expectations, linear operators, and the singular value decomposition, derive the bias vector of the ridge estimator $\\hat{\\beta}_{\\lambda}$ and, using the principal-axis representation above, compute the contribution of this bias to the expected squared prediction error at $x_{0}$, expressed solely in terms of $\\{d_{i}\\}_{i=1}^{p}$, $\\lambda$, $\\{a_{i}\\}_{i=1}^{p}$, and $\\{b_{i}\\}_{i=1}^{p}$. Your final answer must be a single closed-form analytic expression for the squared bias term. No numerical approximation is required, and no units are involved.",
            "solution": "The problem requires the derivation of the squared bias term in the expected squared prediction error (ESPE) for a ridge regression model. The ESPE for a prediction $\\hat{y}_{0}$ of a new observation $y_{0} = x_{0}^{\\top}\\beta + \\epsilon_{0}$ at a point $x_{0}$ is given by $E[(y_{0} - \\hat{y}_{0})^{2}]$. This can be decomposed into three components: squared bias, variance, and irreducible error.\n\nThe prediction is $\\hat{y}_{0} = x_{0}^{\\top}\\hat{\\beta}_{\\lambda}$. The true value we are trying to estimate is the conditional mean $f(x_{0}) = E[y_{0} | x_{0}] = x_{0}^{\\top}\\beta$. The error term $\\epsilon_{0}$ is considered irreducible noise. The bias-variance decomposition of the ESPE is:\n$$\nE[(y_{0} - \\hat{y}_{0})^{2}] = (E[\\hat{y}_{0}] - x_{0}^{\\top}\\beta)^{2} + E[(\\hat{y}_{0} - E[\\hat{y}_{0}])^{2}] + E[\\epsilon_{0}^{2}]\n$$\nThis expression corresponds to:\n$$\n\\text{ESPE}(x_{0}) = (\\text{Bias}[\\hat{y}_{0}])^{2} + \\text{Var}[\\hat{y}_{0}] + \\sigma^{2}\n$$\nThe problem asks for the squared bias term, $(\\text{Bias}[\\hat{y}_{0}])^{2}$.\n\nFirst, we derive the bias of the ridge estimator $\\hat{\\beta}_{\\lambda}$. The definition of bias for an estimator $\\hat{\\theta}$ of a parameter $\\theta$ is $\\text{Bias}[\\hat{\\theta}] = E[\\hat{\\theta}] - \\theta$. Here, the parameter is the vector $\\beta$.\nThe ridge estimator is given by $\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} y$.\nWe compute its expectation by substituting the true model $y = X \\beta + \\epsilon$:\n$$\nE[\\hat{\\beta}_{\\lambda}] = E[ (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} (X \\beta + \\epsilon) ]\n$$\nSince the design matrix $X$ is deterministic, and using the linearity of expectation:\n$$\nE[\\hat{\\beta}_{\\lambda}] = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta + (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} E[\\epsilon]\n$$\nGiven that the noise vector $\\epsilon$ has zero mean, $E[\\epsilon] = 0$, the second term vanishes.\n$$\nE[\\hat{\\beta}_{\\lambda}] = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta\n$$\nThe bias of the estimator $\\hat{\\beta}_{\\lambda}$ is therefore:\n$$\n\\text{Bias}[\\hat{\\beta}_{\\lambda}] = E[\\hat{\\beta}_{\\lambda}] - \\beta = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta - \\beta = \\left( (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X - I_{p} \\right) \\beta\n$$\nNow, we use the singular value decomposition (SVD) of $X$, which is $X = U D V^{\\top}$.\nThe term $X^{\\top} X$ can be expressed as:\n$$\nX^{\\top} X = (U D V^{\\top})^{\\top} (U D V^{\\top}) = V D^{\\top} U^{\\top} U D V^{\\top}\n$$\nSince $U$ is an orthogonal matrix, $U^{\\top} U = I_{n}$.\n$$\nX^{\\top} X = V D^{\\top} D V^{\\top}\n$$\nThe matrix $D^{\\top}D$ is a $p \\times p$ diagonal matrix whose diagonal entries are the squared singular values, $(D^{\\top}D)_{ii} = d_{i}^{2}$ for $i=1, \\dots, p$. Note that by problem definition, $d_i=0$ for $ir$. Let's denote this diagonal matrix as $\\Sigma_{p}^{2} = \\text{diag}(d_{1}^{2}, \\dots, d_{p}^{2})$. Thus, $X^{\\top} X = V \\Sigma_{p}^{2} V^{\\top}$.\n\nWe substitute this into the expression for the bias. First, consider the term $(X^{\\top} X + \\lambda I_{p})$:\n$$\nX^{\\top} X + \\lambda I_{p} = V \\Sigma_{p}^{2} V^{\\top} + \\lambda I_{p} = V \\Sigma_{p}^{2} V^{\\top} + \\lambda V V^{\\top} = V (\\Sigma_{p}^{2} + \\lambda I_{p}) V^{\\top}\n$$\nThe inverse is:\n$$\n(X^{\\top} X + \\lambda I_{p})^{-1} = (V (\\Sigma_{p}^{2} + \\lambda I_{p}) V^{\\top})^{-1} = V (\\Sigma_{p}^{2} + \\lambda I_{p})^{-1} V^{\\top}\n$$\nThe matrix $(\\Sigma_{p}^{2} + \\lambda I_{p})$ is diagonal with entries $d_{i}^{2} + \\lambda$, so its inverse is also diagonal with entries $1 / (d_{i}^{2} + \\lambda)$.\n\nNow, we can simplify the product $(X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X$:\n$$\n(X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X = \\left( V (\\Sigma_{p}^{2} + \\lambda I_{p})^{-1} V^{\\top} \\right) \\left( V \\Sigma_{p}^{2} V^{\\top} \\right) = V (\\Sigma_{p}^{2} + \\lambda I_{p})^{-1} \\Sigma_{p}^{2} V^{\\top}\n$$\nThe matrix in the middle, $(\\Sigma_{p}^{2} + \\lambda I_{p})^{-1} \\Sigma_{p}^{2}$, is diagonal with entries $\\frac{d_{i}^{2}}{d_{i}^{2} + \\lambda}$.\nSubstituting this back into the bias expression for $\\hat{\\beta}_{\\lambda}$:\n$$\n\\text{Bias}[\\hat{\\beta}_{\\lambda}] = \\left( V \\text{diag}\\left(\\frac{d_{i}^{2}}{d_{i}^{2} + \\lambda}\\right) V^{\\top} - V I_{p} V^{\\top} \\right) \\beta = V \\left( \\text{diag}\\left(\\frac{d_{i}^{2}}{d_{i}^{2} + \\lambda}\\right) - I_{p} \\right) V^{\\top} \\beta\n$$\nThe diagonal matrix inside the parentheses has entries $\\frac{d_{i}^{2}}{d_{i}^{2} + \\lambda} - 1 = \\frac{d_{i}^{2} - (d_{i}^{2} + \\lambda)}{d_{i}^{2} + \\lambda} = \\frac{-\\lambda}{d_{i}^{2} + \\lambda}$.\nSo, the bias vector of the ridge estimator is:\n$$\n\\text{Bias}[\\hat{\\beta}_{\\lambda}] = V \\text{diag}\\left(\\frac{-\\lambda}{d_{i}^{2} + \\lambda}\\right) V^{\\top} \\beta\n$$\nNext, we compute the bias of the prediction $\\hat{y}_{0} = x_{0}^{\\top}\\hat{\\beta}_{\\lambda}$. The true value being predicted is $x_{0}^{\\top}\\beta$.\n$$\n\\text{Bias}[\\hat{y}_{0}] = E[\\hat{y}_{0}] - x_{0}^{\\top}\\beta = E[x_{0}^{\\top}\\hat{\\beta}_{\\lambda}] - x_{0}^{\\top}\\beta = x_{0}^{\\top}E[\\hat{\\beta}_{\\lambda}] - x_{0}^{\\top}\\beta = x_{0}^{\\top}(E[\\hat{\\beta}_{\\lambda}] - \\beta) = x_{0}^{\\top}\\text{Bias}[\\hat{\\beta}_{\\lambda}]\n$$\nSubstituting the expression for $\\text{Bias}[\\hat{\\beta}_{\\lambda}]$:\n$$\n\\text{Bias}[\\hat{y}_{0}] = x_{0}^{\\top} V \\text{diag}\\left(\\frac{-\\lambda}{d_{i}^{2} + \\lambda}\\right) V^{\\top} \\beta\n$$\nThis can be written as a product of three terms: $(x_{0}^{\\top} V)$, the diagonal matrix, and $(V^{\\top} \\beta)$.\nLet's analyze these terms using the given definitions. The vector $V^{\\top}x_{0}$ has components $(V^{\\top}x_{0})_{i} = v_{i}^{\\top}x_{0} = a_{i}$. Thus, $x_{0}^{\\top}V = (V^{\\top}x_{0})^{\\top} = [a_{1}, a_{2}, \\dots, a_{p}]$.\nSimilarly, the vector $V^{\\top}\\beta$ has components $(V^{\\top}\\beta)_{i} = v_{i}^{\\top}\\beta = b_{i}$. Thus, $V^{\\top}\\beta = [b_{1}, b_{2}, \\dots, b_{p}]^{\\top}$.\n\nThe bias of the prediction becomes the product of a row vector, a diagonal matrix, and a column vector:\n$$\n\\text{Bias}[\\hat{y}_{0}] = [a_{1} \\dots a_{p}]\n\\begin{pmatrix}\n\\frac{-\\lambda}{d_{1}^{2} + \\lambda}  0  \\dots \\\\\n0  \\frac{-\\lambda}{d_{2}^{2} + \\lambda}  \\\\\n\\vdots   \\ddots\n\\end{pmatrix}\n\\begin{pmatrix}\nb_{1} \\\\\n\\vdots \\\\\nb_{p}\n\\end{pmatrix}\n$$\nThis results in a sum:\n$$\n\\text{Bias}[\\hat{y}_{0}] = \\sum_{i=1}^{p} a_{i} \\left(\\frac{-\\lambda}{d_{i}^{2} + \\lambda}\\right) b_{i} = -\\lambda \\sum_{i=1}^{p} \\frac{a_{i} b_{i}}{d_{i}^{2} + \\lambda}\n$$\nThe problem asks for the contribution of this bias to the expected squared prediction error, which is the squared bias, $(\\text{Bias}[\\hat{y}_{0}])^{2}$.\n$$\n(\\text{Bias}[\\hat{y}_{0}])^{2} = \\left(-\\lambda \\sum_{i=1}^{p} \\frac{a_{i} b_{i}}{d_{i}^{2} + \\lambda}\\right)^{2} = \\lambda^{2} \\left( \\sum_{i=1}^{p} \\frac{a_{i} b_{i}}{d_{i}^{2} + \\lambda} \\right)^{2}\n$$\nThis is the final expression for the squared bias, solely in terms of the specified variables.",
            "answer": "$$\n\\boxed{\\lambda^{2} \\left( \\sum_{i=1}^{p} \\frac{a_{i} b_{i}}{d_{i}^{2} + \\lambda} \\right)^{2}}\n$$"
        },
        {
            "introduction": "Ensembling is a powerful method for boosting model performance, but not all ensembles are created equal. This exercise applies the bias-variance framework to compare two common strategies in deep learning: averaging checkpoints from a single training run versus averaging models from independent runs. Through this analysis, you will discover the critical role that correlation between models plays in variance reduction, building a theoretical foundation for practical decisions about how to construct effective ensembles.",
            "id": "3182030",
            "problem": "You will investigate the bias-variance tradeoff by modeling two test-time ensembling strategies for a scalar prediction at a fixed input: ensembling across training checkpoints from a single run versus ensembling across random seeds from multiple independent runs. The goal is to determine how their different correlation structures and systematic biases affect the expected squared prediction error.\n\nFundamental base for derivation. Use only the following generally accepted definitions. Let $\\theta$ denote a deterministic quantity to be estimated and $g$ an estimator that is a random variable due to training randomness:\n- The bias of $g$ is $\\mathbb{E}[g] - \\theta$.\n- The variance of $g$ is $\\mathrm{Var}(g) = \\mathbb{E}[(g - \\mathbb{E}[g])^{2}]$.\n- Under squared loss, the expected squared error relative to $\\theta$ is $\\mathbb{E}[(g - \\theta)^{2}]$.\n\nScenario and assumptions. Fix an input where the true target is $f(x)$ (deterministic). For each ensembling strategy $t \\in \\{c,s\\}$ where $c$ denotes averaging across temporally adjacent checkpoints (temporal smoothing) and $s$ denotes averaging across independently initialized runs (parameter diversity), suppose the $i$-th model prediction is\n$$\n\\hat{y}^{(t)}_{i} \\;=\\; f(x) \\;+\\; b_{t} \\;+\\; \\varepsilon^{(t)}_{i}, \\quad i \\in \\{1,\\dots,K\\},\n$$\nwhere $b_{t}$ is a deterministic systematic bias (possibly different across strategies), and $\\varepsilon^{(t)}_{i}$ are zero-mean random fluctuations induced by training randomness and optimization, with\n$$\n\\mathbb{E}[\\varepsilon^{(t)}_{i}] = 0,\\quad \\mathrm{Var}(\\varepsilon^{(t)}_{i}) = v_{t},\\quad \\mathrm{Corr}(\\varepsilon^{(t)}_{i}, \\varepsilon^{(t)}_{j}) = \\rho_{t}\\ \\text{for } i \\neq j.\n$$\nAssume all $\\varepsilon^{(t)}_{i}$ within a strategy are identically distributed with common variance $v_{t}$ and constant pairwise correlation $\\rho_{t} \\in [-1,1]$. Define the ensemble prediction for strategy $t$ as the arithmetic mean\n$$\n\\bar{y}^{(t)} \\;=\\; \\frac{1}{K}\\sum_{i=1}^{K} \\hat{y}^{(t)}_{i}.\n$$\n\nTasks.\n1. Using only the above definitions, derive expressions, in terms of $K$, $b_{t}$, $v_{t}$, and $\\rho_{t}$, for:\n   - the bias of $\\bar{y}^{(t)}$ relative to $f(x)$,\n   - the variance of $\\bar{y}^{(t)}$,\n   - the expected squared error $\\mathbb{E}\\!\\left[(\\bar{y}^{(t)} - f(x))^{2}\\right]$.\n2. Interpret how temporal smoothing across checkpoints (typically larger $\\rho_{c}$) versus parameter diversity across seeds (typically smaller $\\rho_{s}$) changes the variance term as $K$ grows, and how different $b_{c}$ and $b_{s}$ influence the bias term.\n3. Implement a program that, for the test suite below, computes for each case and each strategy:\n   - squared bias $B^{2}_{t}$,\n   - variance $V_{t}$,\n   - expected squared error $E_{t}$,\n   and decides which strategy has the smaller expected squared error.\n\nNumerical and output requirements.\n- Use the following test suite. Each case specifies $(K, b_{c}, v_{c}, \\rho_{c}, b_{s}, v_{s}, \\rho_{s})$:\n  - Case A: $(K = 5,\\ b_{c} = 0.2,\\ v_{c} = 1.0,\\ \\rho_{c} = 0.8,\\ b_{s} = 0.25,\\ v_{s} = 1.0,\\ \\rho_{s} = 0.2)$.\n  - Case B: $(K = 20,\\ b_{c} = 0.1,\\ v_{c} = 0.8,\\ \\rho_{c} = 0.9,\\ b_{s} = 0.1,\\ v_{s} = 0.9,\\ \\rho_{s} = 0.1)$.\n  - Case C: $(K = 1,\\ b_{c} = 0.0,\\ v_{c} = 1.5,\\ \\rho_{c} = 0.0,\\ b_{s} = 0.05,\\ v_{s} = 1.2,\\ \\rho_{s} = 0.0)$.\n  - Case D: $(K = 50,\\ b_{c} = 0.15,\\ v_{c} = 0.7,\\ \\rho_{c} = 0.95,\\ b_{s} = 0.18,\\ v_{s} = 0.7,\\ \\rho_{s} = 0.05)$.\n- For each case, output a list\n  $[B^{2}_{c}, V_{c}, E_{c}, B^{2}_{s}, V_{s}, E_{s}, W]$\n  where $W$ is $0$ if $E_{c} \\le E_{s}$ and $1$ otherwise. Round all floating-point outputs to $6$ decimal places.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list of these per-case lists, enclosed in square brackets; for example, a line like\n  $[[\\dots],[\\dots],[\\dots],[\\dots]]$.",
            "solution": "### 1. Derivation of Ensemble Statistics\n\nThe derivations for the bias, variance, and expected squared error of the ensemble prediction $\\bar{y}^{(t)}$ are presented below, using only the definitions provided in the problem statement. The true value to be estimated is $\\theta = f(x)$, and the estimator is $g = \\bar{y}^{(t)}$.\n\n#### Bias of $\\bar{y}^{(t)}$\nThe bias of an estimator $g$ for a quantity $\\theta$ is defined as $\\mathbb{E}[g] - \\theta$. In this context, $g = \\bar{y}^{(t)}$ and $\\theta = f(x)$.\n\nFirst, we compute the expectation of the ensemble mean, $\\mathbb{E}[\\bar{y}^{(t)}]$.\nThe ensemble prediction is defined as the arithmetic mean:\n$$\n\\bar{y}^{(t)} = \\frac{1}{K}\\sum_{i=1}^{K} \\hat{y}^{(t)}_{i}\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[\\bar{y}^{(t)}] = \\mathbb{E}\\left[\\frac{1}{K}\\sum_{i=1}^{K} \\hat{y}^{(t)}_{i}\\right] = \\frac{1}{K}\\sum_{i=1}^{K} \\mathbb{E}[\\hat{y}^{(t)}_{i}]\n$$\nThe expectation of a single model's prediction $\\hat{y}^{(t)}_{i} = f(x) + b_{t} + \\varepsilon^{(t)}_{i}$ is found by taking the expectation of its components. Since $f(x)$ and $b_t$ are deterministic quantities and $\\mathbb{E}[\\varepsilon^{(t)}_{i}] = 0$:\n$$\n\\mathbb{E}[\\hat{y}^{(t)}_{i}] = \\mathbb{E}[f(x) + b_{t} + \\varepsilon^{(t)}_{i}] = f(x) + b_{t} + \\mathbb{E}[\\varepsilon^{(t)}_{i}] = f(x) + b_{t}\n$$\nSubstituting this result back into the expression for $\\mathbb{E}[\\bar{y}^{(t)}]$:\n$$\n\\mathbb{E}[\\bar{y}^{(t)}] = \\frac{1}{K}\\sum_{i=1}^{K} (f(x) + b_{t}) = \\frac{1}{K} \\cdot K \\cdot (f(x) + b_{t}) = f(x) + b_{t}\n$$\nThe bias of the ensemble prediction is therefore:\n$$\n\\mathrm{Bias}(\\bar{y}^{(t)}) = \\mathbb{E}[\\bar{y}^{(t)}] - f(x) = (f(x) + b_{t}) - f(x) = b_{t}\n$$\n\n#### Variance of $\\bar{y}^{(t)}$\nThe variance of the ensemble mean is given by:\n$$\n\\mathrm{Var}(\\bar{y}^{(t)}) = \\mathrm{Var}\\left(\\frac{1}{K}\\sum_{i=1}^{K} \\hat{y}^{(t)}_{i}\\right) = \\frac{1}{K^2} \\mathrm{Var}\\left(\\sum_{i=1}^{K} \\hat{y}^{(t)}_{i}\\right)\n$$\nThe variance of a sum of random variables is the sum of all elements in their covariance matrix:\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{K} \\hat{y}^{(t)}_{i}\\right) = \\sum_{i=1}^{K}\\sum_{j=1}^{K} \\mathrm{Cov}(\\hat{y}^{(t)}_{i}, \\hat{y}^{(t)}_{j})\n$$\nThe covariance term $\\mathrm{Cov}(\\hat{y}^{(t)}_{i}, \\hat{y}^{(t)}_{j})$ depends only on the random fluctuation terms $\\varepsilon^{(t)}_{i}$, as $f(x)$ and $b_t$ are deterministic constants:\n$$\n\\mathrm{Cov}(\\hat{y}^{(t)}_{i}, \\hat{y}^{(t)}_{j}) = \\mathrm{Cov}(f(x) + b_t + \\varepsilon^{(t)}_{i}, f(x) + b_t + \\varepsilon^{(t)}_{j}) = \\mathrm{Cov}(\\varepsilon^{(t)}_{i}, \\varepsilon^{(t)}_{j})\n$$\nWe use the provided definitions for the variance and correlation of $\\varepsilon^{(t)}_{i}$:\n- For diagonal terms where $i=j$: $\\mathrm{Cov}(\\varepsilon^{(t)}_{i}, \\varepsilon^{(t)}_{i}) = \\mathrm{Var}(\\varepsilon^{(t)}_{i}) = v_{t}$.\n- For off-diagonal terms where $i \\neq j$: $\\mathrm{Cov}(\\varepsilon^{(t)}_{i}, \\varepsilon^{(t)}_{j}) = \\sqrt{\\mathrm{Var}(\\varepsilon^{(t)}_{i})\\mathrm{Var}(\\varepsilon^{(t)}_{j})} \\cdot \\mathrm{Corr}(\\varepsilon^{(t)}_{i}, \\varepsilon^{(t)}_{j}) = \\sqrt{v_t \\cdot v_t} \\cdot \\rho_t = v_t \\rho_t$.\n\nThe double summation contains $K$ diagonal terms and $K(K-1)$ off-diagonal terms. Thus, the total variance of the sum is:\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{K} \\hat{y}^{(t)}_{i}\\right) = \\sum_{i=1}^{K} \\mathrm{Var}(\\hat{y}^{(t)}_{i}) + \\sum_{i \\neq j} \\mathrm{Cov}(\\hat{y}^{(t)}_{i}, \\hat{y}^{(t)}_{j}) = K \\cdot v_t + K(K-1) \\cdot v_t \\rho_t\n$$\nSubstituting this back into the expression for $\\mathrm{Var}(\\bar{y}^{(t)})$:\n$$\n\\mathrm{Var}(\\bar{y}^{(t)}) = \\frac{1}{K^2} \\left[ K v_t + K(K-1) v_t \\rho_t \\right] = \\frac{K v_t}{K^2} \\left[ 1 + (K-1)\\rho_t \\right]\n$$\nSimplifying this expression yields the final form for the variance:\n$$\n\\mathrm{Var}(\\bar{y}^{(t)}) = \\frac{v_t}{K} [1 + (K-1)\\rho_t]\n$$\n\n#### Expected Squared Error of $\\bar{y}^{(t)}$\nThe expected squared error (ESE) of an estimator $g$ with respect to a true value $\\theta$ is defined as $\\mathbb{E}[(g-\\theta)^2]$. A fundamental result in statistics is the decomposition of the ESE into the sum of the squared bias and the variance:\n$$\n\\mathbb{E}[(g-\\theta)^2] = (\\mathrm{Bias}(g))^2 + \\mathrm{Var}(g)\n$$\nApplying this to our problem, with $g=\\bar{y}^{(t)}$ and $\\theta=f(x)$:\n$$\n\\mathbb{E}\\!\\left[(\\bar{y}^{(t)} - f(x))^{2}\\right] = (\\mathrm{Bias}(\\bar{y}^{(t)}))^{2} + \\mathrm{Var}(\\bar{y}^{(t)})\n$$\nSubstituting the expressions derived above for the bias and variance, we obtain the expression for the expected squared error of the ensemble mean:\n$$\n\\mathbb{E}\\!\\left[(\\bar{y}^{(t)} - f(x))^{2}\\right] = b_t^2 + \\frac{v_t}{K} [1 + (K-1)\\rho_t]\n$$\n\n### 2. Interpretation of Results\n\nThe derived formulas provide clear insight into the bias-variance tradeoff for the two ensembling strategies.\n\nThe **bias** of the ensemble, $b_t$, is identical to the systematic bias of a single model from that strategy. Ensembling, which is a form of averaging, does not reduce systematic bias. This contribution to the total error, $b_t^2$, acts as an error floor that cannot be mitigated by increasing the ensemble size $K$.\n\nThe **variance** of the ensemble, $V_t = \\frac{v_t}{K} [1 + (K-1)\\rho_t]$, can be rewritten as $V_t = v_t \\left( \\frac{1-\\rho_t}{K} + \\rho_t \\right)$. This form clearly shows the effect of ensembling. As the ensemble size $K$ grows, the first term $\\frac{v_t(1-\\rho_t)}{K}$ diminishes. However, the variance is lower-bounded by the second term:\n$$\n\\lim_{K \\to \\infty} V_t = v_t \\rho_t\n$$\n- **Temporal Smoothing ($t=c$)**: This strategy ensembles predictions from different checkpoints of a single training run. These models are highly correlated, so $\\rho_c$ is large (close to $1$). The variance reduction from ensembling is minimal because the term $v_c \\rho_c$ dominates. Averaging highly similar predictions does little to cancel out their shared noise.\n- **Parameter Diversity ($t=s$)**: This strategy ensembles predictions from independent training runs (different random seeds). These models are less correlated, resulting in a small $\\rho_s$. The variance reduction from ensembling is significant because the term $\\frac{v_s(1-\\rho_s)}{K}$ has a large effect, and the variance floor $v_s \\rho_s$ is low. This strategy is more effective at reducing variance.\n\nThe total expected squared error is the sum of these two components. An optimal strategy must balance a low systematic bias $b_t$ with a structure that yields low prediction correlation $\\rho_t$ to effectively reduce variance.\n\n### 3. Numerical Computation\n\nThe derived expressions for squared bias $B^{2}_{t} = b_t^2$, variance $V_{t} = \\frac{v_t}{K} [1 + (K-1)\\rho_t]$, and expected squared error $E_{t} = B^{2}_{t} + V_{t}$ are implemented in the Python program provided in the `answer` tag. The program computes these values for each test case and determines the superior strategy based on the lower expected squared error.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes ensemble statistics for two strategies and determines the better one.\n    \"\"\"\n    # Test suite: (K, b_c, v_c, rho_c, b_s, v_s, rho_s)\n    test_cases = [\n        (5, 0.2, 1.0, 0.8, 0.25, 1.0, 0.2),    # Case A\n        (20, 0.1, 0.8, 0.9, 0.1, 0.9, 0.1),   # Case B\n        (1, 0.0, 1.5, 0.0, 0.05, 1.2, 0.0),   # Case C\n        (50, 0.15, 0.7, 0.95, 0.18, 0.7, 0.05)   # Case D\n    ]\n\n    all_results_str = []\n    \n    for case in test_cases:\n        K, b_c, v_c, rho_c, b_s, v_s, rho_s = case\n\n        # --- Strategy 'c' (Checkpoints) ---\n        # Squared Bias\n        B2_c = b_c**2\n        # Variance\n        # Handle the K=1 case to avoid floating point issues with (K-1)\n        if K == 1:\n            V_c = v_c\n        else:\n            V_c = (v_c / K) * (1 + (K - 1) * rho_c)\n        # Expected Squared Error\n        E_c = B2_c + V_c\n\n        # --- Strategy 's' (Seeds) ---\n        # Squared Bias\n        B2_s = b_s**2\n        # Variance\n        if K == 1:\n            V_s = v_s\n        else:\n            V_s = (v_s / K) * (1 + (K - 1) * rho_s)\n        # Expected Squared Error\n        E_s = B2_s + V_s\n        \n        # Determine the winner\n        # W = 0 if E_c = E_s, W = 1 otherwise\n        W = 1 if E_c > E_s else 0\n        \n        # Format the results for the current case\n        case_result_list = [\n            f\"{B2_c:.6f}\",\n            f\"{V_c:.6f}\",\n            f\"{E_c:.6f}\",\n            f\"{B2_s:.6f}\",\n            f\"{V_s:.6f}\",\n            f\"{E_s:.6f}\",\n            str(W)\n        ]\n\n        all_results_str.append(f\"[{','.join(case_result_list)}]\")\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        }
    ]
}