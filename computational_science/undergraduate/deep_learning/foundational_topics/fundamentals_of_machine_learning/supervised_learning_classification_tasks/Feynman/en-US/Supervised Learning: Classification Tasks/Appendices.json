{
    "hands_on_practices": [
        {
            "introduction": "A trained classifier outputs scores, but translating these scores into concrete decisions requires setting a threshold. This exercise explores how to select an optimal threshold to maximize a specific performance metric, the $F1$ score, which balances precision and recall. More critically, it addresses the real-world challenge of \"label shift,\" where the underlying frequency of classes changes after a model is deployed, potentially making the original threshold suboptimal. This practice  will guide you through using importance weighting to analyze a model's performance under such shifts and find a new, more robust threshold, a crucial skill for building reliable machine learning systems.",
            "id": "3178377",
            "problem": "You are given a multi-label classification setting with independent per-class decisions. For each class label $y \\in \\{0,1,2\\}$, a model produces a calibrated score $s \\in [0,1]$ that can be interpreted as the posterior probability $p(y=1 \\mid x)$. A per-class decision rule predicts positive for class $y$ on an instance if and only if $s \\ge \\tau_y$, where $\\tau_y \\in [0,1]$ is a threshold chosen to maximize the F1 score. The F1 score is the harmonic mean of precision and recall, and for a fixed class it is defined by\n$$\n\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}, \\quad \\text{recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}, \\quad \\text{F1} = \\frac{2 \\cdot \\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}} = \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FP} + \\text{FN}},\n$$\nwhere $\\text{TP}$, $\\text{FP}$, and $\\text{FN}$ denote true positives, false positives, and false negatives, respectively, all computed with respect to the threshold $\\tau_y$.\n\nYou will analyze threshold calibration stability under prior shift (also called label shift), where the class prior $\\pi_y = p(y=1)$ changes to a new value $\\pi_y'$, while the class-conditional score distributions $p(s \\mid y)$ remain unchanged. Under this assumption, the expected confusion counts under the new prior can be obtained by importance weighting each positive instance by weight $w_{+} = \\pi_y' / \\pi_y$ and each negative instance by weight $w_{-} = (1-\\pi_y')/(1-\\pi_y)$. For a candidate threshold $\\tau$, the weighted counts are\n$$\n\\text{TP}_w(\\tau) = \\sum_{i} w_{+} \\cdot \\mathbf{1}\\{y_i=1\\} \\cdot \\mathbf{1}\\{s_i \\ge \\tau\\}, \\quad\n\\text{FP}_w(\\tau) = \\sum_{i} w_{-} \\cdot \\mathbf{1}\\{y_i=0\\} \\cdot \\mathbf{1}\\{s_i \\ge \\tau\\},\n$$\n$$\n\\text{FN}_w(\\tau) = \\sum_{i} w_{+} \\cdot \\mathbf{1}\\{y_i=1\\} \\cdot \\mathbf{1}\\{s_i < \\tau\\},\n$$\nand the weighted F1 is\n$$\n\\text{F1}_w(\\tau) = \\frac{2 \\cdot \\text{TP}_w(\\tau)}{2 \\cdot \\text{TP}_w(\\tau) + \\text{FP}_w(\\tau) + \\text{FN}_w(\\tau)}.\n$$\n\nTask: Write a program that, for each class $y \\in \\{0,1,2\\}$, computes the F1-optimal threshold on a validation set, then computes the F1-optimal threshold under a specified prior shift using importance weighting, and finally quantifies the stability and performance under the shift.\n\nFundamental base to use in your reasoning and implementation: definitions of precision, recall, F1; the indicator function $\\mathbf{1}\\{\\cdot\\}$; and the label shift assumption that $p(s \\mid y)$ is invariant while $\\pi_y$ changes, permitting importance weighting with $w_{+} = \\pi_y'/\\pi_y$ and $w_{-} = (1-\\pi_y')/(1-\\pi_y)$.\n\nValidation data (per class $y$) are fixed and given as follows. For class $y=0$,\n$$\n\\mathbf{s}^{(0)} = [0.95, 0.80, 0.60, 0.55, 0.52, 0.50, 0.40, 0.30, 0.20, 0.10, 0.05, 0.01], \\\\\n\\mathbf{y}^{(0)} = [1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0].\n$$\nFor class $y=1$,\n$$\n\\mathbf{s}^{(1)} = [0.90, 0.88, 0.70, 0.65, 0.60, 0.55, 0.50, 0.49, 0.35, 0.33, 0.25, 0.15], \\\\\n\\mathbf{y}^{(1)} = [1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0].\n$$\nFor class $y=2$,\n$$\n\\mathbf{s}^{(2)} = [0.99, 0.85, 0.75, 0.60, 0.45, 0.40, 0.35, 0.32, 0.20, 0.18, 0.10, 0.02], \\\\\n\\mathbf{y}^{(2)} = [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0].\n$$\n\nLet the empirical prior for class $y$ be the validation-set fraction $\\pi_y = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbf{1}\\{y_i=1\\}$. You are given target priors $\\pi_y'$ for the shifted environment as\n$$\n\\pi_0' = 0.70, \\quad \\pi_1' = 0.20, \\quad \\pi_2' = \\frac{1}{6}.\n$$\n\nComputational requirements for each class $y$:\n- Compute $\\pi_y$ from the validation labels.\n- Find the unweighted F1-optimal threshold $\\tau_y$ by searching a finite candidate set of thresholds. Use as candidates all unique values in $\\mathbf{s}^{(y)}$ together with $0$ and a value strictly larger than the maximum score (e.g., $\\max(\\mathbf{s}^{(y)}) + 10^{-12}$). Predict positive when $s \\ge \\tau$. If multiple thresholds yield the same maximal F1, break ties by selecting the smallest threshold among the maximizers.\n- Under the shifted prior $\\pi_y'$, compute $w_{+} = \\pi_y'/\\pi_y$ and $w_{-} = (1-\\pi_y')/(1-\\pi_y)$. Using the same candidate set and tie-breaking rule, find the weighted F1-optimal threshold $\\tau_y^{\\text{shift}}$ that maximizes $\\text{F1}_w(\\tau)$.\n- Compute the absolute threshold change $\\Delta \\tau_y = |\\tau_y^{\\text{shift}} - \\tau_y|$.\n- Compute the weighted F1 under the shifted prior when using the original threshold, $\\text{F1}_w(\\tau_y)$, and when using the shifted-optimal threshold, $\\text{F1}_w(\\tau_y^{\\text{shift}})$.\n\nTest suite: Use the three classes specified above with their respective $\\pi_y'$ values. Your program must output a single line containing a flattened list of results across classes in order $y=0$, then $y=1$, then $y=2$, with the five metrics per class in the following sequence:\n$$\n[\\tau_0, \\ \\tau_0^{\\text{shift}}, \\ \\Delta \\tau_0, \\ \\text{F1}_w(\\tau_0), \\ \\text{F1}_w(\\tau_0^{\\text{shift}}), \\ \\tau_1, \\ \\tau_1^{\\text{shift}}, \\ \\Delta \\tau_1, \\ \\text{F1}_w(\\tau_1), \\ \\text{F1}_w(\\tau_1^{\\text{shift}}), \\ \\tau_2, \\ \\tau_2^{\\text{shift}}, \\ \\Delta \\tau_2, \\ \\text{F1}_w(\\tau_2), \\ \\text{F1}_w(\\tau_2^{\\text{shift}})].\n$$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each number rounded to exactly $6$ decimal places. For example, an output should look like $[0.123456,0.234567, \\dots]$ with no extra whitespace or text. No user input should be read; all data are as specified above and embedded in the program. No physical units or angles occur in this problem, so no unit conversion is required.",
            "solution": "The problem is well-defined, scientifically sound, and provides all necessary information for a unique solution. It is a standard exercise in classifier evaluation and adaptation to label distribution shift, a core topic in applied machine learning. We may therefore proceed with the solution.\n\nThe task is to analyze the stability of an F1-optimal decision threshold for a binary classifier under a specified change in the class prior probability, known as prior or label shift. For each of three classes, we will first determine the optimal threshold $\\tau_y$ on a validation set. Then, we will simulate a shift in the prior from the empirical prior $\\pi_y$ to a target prior $\\pi_y'$ and find the new optimal threshold $\\tau_y^{\\text{shift}}$ by maximizing an importance-weighted F1 score, $\\text{F1}_w(\\tau)$. Finally, we will quantify the threshold instability and the impact on performance.\n\nThe fundamental principle is that the F1 score, defined as $\\text{F1} = \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FP} + \\text{FN}}$, balances the concerns of precision and recall. The optimal threshold $\\tau_y$ represents the point where this balance is maximized for a given score distribution and class prior. When the prior $\\pi_y = p(y=1)$ changes, the relative cost of false positives ($\\text{FP}$) versus false negatives ($\\text{FN}$) changes. Importance weighting is a technique to estimate classifier performance under this new prior without retraining the model or re-collecting labeled data. Each original positive instance is weighted by $w_{+} = \\pi_y' / \\pi_y$, and each negative instance by $w_{-} = (1-\\pi_y') / (1-\\pi_y)$. This re-weights the confusion matrix counts to reflect their expected frequency in the target distribution.\n\nThe weighted F1 score is given by:\n$$\n\\text{F1}_w(\\tau) = \\frac{2 \\cdot \\text{TP}_w(\\tau)}{2 \\cdot \\text{TP}_w(\\tau) + \\text{FP}_w(\\tau) + \\text{FN}_w(\\tau)}\n$$\nwhere $\\text{TP}_w(\\tau) = w_{+} \\cdot \\text{TP}(\\tau)$, $\\text{FP}_w(\\tau) = w_{-} \\cdot \\text{FP}(\\tau)$, and $\\text{FN}_w(\\tau) = w_{+} \\cdot \\text{FN}(\\tau)$. The unweighted counts $\\text{TP}(\\tau)$, $\\text{FP}(\\tau)$, and $\\text{FN}(\\tau)$ are computed on the original validation data for a given threshold $\\tau$.\n\nThe search for the optimal thresholds $\\tau_y$ and $\\tau_y^{\\text{shift}}$ is conducted over a candidate set composed of all unique scores in the validation data, plus $0$ and a value slightly greater than the maximum score. This ensures we evaluate all possible partitions of the data, as well as the trivial classifiers that predict all negative or all positive.\n\nWe will now execute this procedure for each class.\n\n**Class $y=0$**\n\nThe provided data are:\n$\\mathbf{s}^{(0)} = [0.95, 0.80, 0.60, 0.55, 0.52, 0.50, 0.40, 0.30, 0.20, 0.10, 0.05, 0.01]$\n$\\mathbf{y}^{(0)} = [1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]$\n\n1.  **Calculate Empirical Prior $\\pi_0$**: There are $P=5$ positive instances and $N=7$ negative instances out of $n=12$.\n    The empirical prior is $\\pi_0 = P/n = 5/12$.\n\n2.  **Find Unweighted F1-Optimal Threshold $\\tau_0$**: We iterate through all candidate thresholds $\\tau$ and calculate $\\text{F1}(\\tau) = \\frac{2 \\cdot \\text{TP}(\\tau)}{2 \\cdot \\text{TP}(\\tau) + \\text{FP}(\\tau) + \\text{FN}(\\tau)}$.\n    The maximal F1 score is found to be $\\approx 0.8$ at $\\tau = 0.52$. At this threshold, $\\text{TP}=4$, $\\text{FP}=1$, $\\text{FN}=1$, giving $\\text{F1} = (2 \\cdot 4)/(2 \\cdot 4 + 1 + 1) = 8/10 = 0.8$.\n    Thus, $\\tau_0 = 0.52$.\n\n3.  **Calculate Importance Weights**: The target prior is given as $\\pi_0' = 0.70$.\n    $w_{+} = \\frac{\\pi_0'}{\\pi_0} = \\frac{0.70}{5/12} = \\frac{7/10}{5/12} = \\frac{84}{50} = 1.68$.\n    $w_{-} = \\frac{1-\\pi_0'}{1-\\pi_0} = \\frac{0.30}{7/12} = \\frac{3/10}{7/12} = \\frac{36}{70} \\approx 0.514286$.\n    Since $\\pi_0' > \\pi_0$, positive examples become more important ($w_+ > 1$) and negative examples less so ($w_- < 1$). This will favor a lower threshold to increase recall.\n\n4.  **Find Weighted F1-Optimal Threshold $\\tau_0^{\\text{shift}}$**: We iterate through the same candidate thresholds, this time maximizing $\\text{F1}_w(\\tau)$. The maximum weighted F1 score is found to be $\\approx 0.890909$. This occurs at $\\tau=0.20$. At this threshold, we have $\\text{TP}=5$ and $\\text{FP}=4$. The weighted counts are $\\text{TP}_w = 1.68 \\cdot 5 = 8.4$, $\\text{FP}_w = (36/70) \\cdot 4 \\approx 2.057143$, and $\\text{FN}_w = 1.68 \\cdot 0 = 0$. This gives $\\text{F1}_w = (2 \\cdot 8.4) / (2 \\cdot 8.4 + 2.057143 + 0) \\approx 0.890909$.\n    Thus, $\\tau_0^{\\text{shift}} = 0.20$.\n\n5.  **Calculate Final Metrics for Class 0**:\n    - $\\tau_0 = 0.52$\n    - $\\tau_0^{\\text{shift}} = 0.20$\n    - $\\Delta \\tau_0 = |\\tau_0^{\\text{shift}} - \\tau_0| = |0.20 - 0.52| = 0.32$\n    - $\\text{F1}_w(\\tau_0) = \\text{F1}_w(0.52)$: With $\\text{TP}=4, \\text{FP}=1, \\text{FN}=1$, the weighted F1 is $\\text{F1}_w = (2 \\cdot 1.68 \\cdot 4) / (2 \\cdot 1.68 \\cdot 4 + (36/70) \\cdot 1 + 1.68 \\cdot 1) \\approx 0.859654$.\n    - $\\text{F1}_w(\\tau_0^{\\text{shift}}) = \\text{F1}_w(0.20) \\approx 0.890909$. Adjusting the threshold improves the F1 score under the new prior.\n\n**Class $y=1$**\n\nFollowing the identical procedure with the data for class $y=1$:\n$\\mathbf{s}^{(1)} = [0.90, ..., 0.15]$, $\\mathbf{y}^{(1)} = [1, ..., 0]$\n$\\pi_1 = 5/12$, target $\\pi_1' = 0.20$.\n$w_{+} = \\frac{0.20}{5/12} = 0.48$.\n$w_{-} = \\frac{0.80}{7/12} \\approx 1.371429$.\nHere, the prior on positives decreases, making false positives relatively more costly. We expect the optimal threshold to increase.\n- Unweighted optimization yields a maximum F1 score of $2/3 \\approx 0.666667$, which occurs at thresholds $\\tau \\in \\{0.33, 0.50, 0.65\\}$. Applying the tie-breaking rule (smallest threshold), we get $\\tau_1 = 0.33$.\n- Weighted optimization yields a maximum weighted F1 score of $\\approx 0.552632$ at $\\tau=0.65$. Thus, $\\tau_1^{\\text{shift}} = 0.65$.\n- Final metrics are: $\\tau_1 = 0.33$, $\\tau_1^{\\text{shift}} = 0.65$, $\\Delta \\tau_1 = 0.32$, $\\text{F1}_w(\\tau_1) \\approx 0.411765$, and $\\text{F1}_w(\\tau_1^{\\text{shift}}) \\approx 0.552632$.\n\n**Class $y=2$**\n\nFollowing the identical procedure with the data for class $y=2$:\n$\\mathbf{s}^{(2)} = [0.99, ..., 0.02]$, $\\mathbf{y}^{(2)} = [1, ..., 0]$\n$\\pi_2 = 2/12 = 1/6$, target $\\pi_2' = 1/6$.\nIn this special case, the prior does not change, $\\pi_2' = \\pi_2$.\nThis implies the importance weights are $w_{+} = 1$ and $w_{-} = 1$. The weighted F1 score is identical to the unweighted F1 score, i.e., $\\text{F1}_w(\\tau) = \\text{F1}(\\tau)$ for all $\\tau$. Consequently, the optimization problem is unchanged.\n- Unweighted optimization yields a maximum F1 score of $2/3 \\approx 0.666667$, which occurs at thresholds $\\tau \\in \\{0.60, 0.99\\}$. Applying the tie-breaking rule, we get $\\tau_2 = 0.60$.\n- As the problem is unchanged, $\\tau_2^{\\text{shift}} = \\tau_2 = 0.60$.\n- Final metrics are: $\\tau_2 = 0.60$, $\\tau_2^{\\text{shift}} = 0.60$, $\\Delta \\tau_2 = 0.0$, $\\text{F1}_w(\\tau_2) \\approx 0.666667$, and $\\text{F1}_w(\\tau_2^{\\text{shift}}) \\approx 0.666667$.\n\nThese calculated values will be programmatically computed and formatted as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to perform the analysis for all three classes and print the final results.\n    \"\"\"\n    \n    # Define the datasets and target priors for each class as per the problem statement.\n    problem_data = [\n        {\n            \"class_id\": 0,\n            \"scores\": np.array([0.95, 0.80, 0.60, 0.55, 0.52, 0.50, 0.40, 0.30, 0.20, 0.10, 0.05, 0.01]),\n            \"labels\": np.array([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]),\n            \"pi_prime\": 0.70\n        },\n        {\n            \"class_id\": 1,\n            \"scores\": np.array([0.90, 0.88, 0.70, 0.65, 0.60, 0.55, 0.50, 0.49, 0.35, 0.33, 0.25, 0.15]),\n            \"labels\": np.array([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0]),\n            \"pi_prime\": 0.20\n        },\n        {\n            \"class_id\": 2,\n            \"scores\": np.array([0.99, 0.85, 0.75, 0.60, 0.45, 0.40, 0.35, 0.32, 0.20, 0.18, 0.10, 0.02]),\n            \"labels\": np.array([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n            \"pi_prime\": 1/6\n        }\n    ]\n\n    all_results = []\n\n    for data in problem_data:\n        scores = data[\"scores\"]\n        labels = data[\"labels\"]\n        pi_y_prime = data[\"pi_prime\"]\n\n        n = len(labels)\n        positives_count = np.sum(labels)\n        \n        # Guard against zero positive instances\n        if positives_count == 0:\n            pi_y = 0.0\n        else:\n            pi_y = positives_count / n\n\n        # Define candidate thresholds according to the problem specification\n        candidate_thresholds = np.unique(scores)\n        candidate_thresholds = np.append(candidate_thresholds, 0)\n        candidate_thresholds = np.append(candidate_thresholds, np.max(scores) + 1e-12)\n        candidate_thresholds = np.unique(candidate_thresholds) # Ensure no duplicates\n\n        # Calculate importance weights, handle edge case if pi_y is 0 or 1\n        w_plus, w_minus = 1.0, 1.0\n        if pi_y > 0 and pi_y < 1:\n            w_plus = pi_y_prime / pi_y\n            w_minus = (1 - pi_y_prime) / (1 - pi_y)\n        elif pi_y == 0 and pi_y_prime > 0:\n            # Undefined case, but won't happen with given data\n            pass\n        elif pi_y == 1 and pi_y_prime < 1:\n            # Undefined case, but won't happen with given data\n            pass\n\n        unweighted_f1_results = []\n        weighted_f1_results = []\n\n        for tau in candidate_thresholds:\n            predictions = (scores >= tau).astype(int)\n            \n            tp = np.sum((predictions == 1) & (labels == 1))\n            fp = np.sum((predictions == 1) & (labels == 0))\n            fn = positives_count - tp\n\n            # Calculate unweighted F1\n            f1_denom = 2 * tp + fp + fn\n            f1 = (2 * tp) / f1_denom if f1_denom > 0 else 0.0\n            unweighted_f1_results.append((f1, tau))\n\n            # Calculate weighted F1\n            tp_w = w_plus * tp\n            fp_w = w_minus * fp\n            fn_w = w_plus * fn\n            \n            f1_w_denom = 2 * tp_w + fp_w + fn_w\n            f1_w = (2 * tp_w) / f1_w_denom if f1_w_denom > 0 else 0.0\n            weighted_f1_results.append((f1_w, tau))\n\n        # Find optimal thresholds by sorting.\n        # Primary sort key: F1 score (descending), Secondary sort key: threshold (ascending)\n        unweighted_f1_results.sort(key=lambda x: (-x[0], x[1]))\n        weighted_f1_results.sort(key=lambda x: (-x[0], x[1]))\n\n        tau_y = unweighted_f1_results[0][1]\n        tau_y_shift = weighted_f1_results[0][1]\n        max_weighted_f1 = weighted_f1_results[0][0]\n\n        # Find the weighted F1 at the original optimal threshold\n        f1_w_at_tau_y = 0.0\n        for f1_w, tau in weighted_f1_results:\n            if np.isclose(tau, tau_y):\n                f1_w_at_tau_y = f1_w\n                break\n        \n        delta_tau_y = np.abs(tau_y_shift - tau_y)\n\n        class_results = [\n            tau_y,\n            tau_y_shift,\n            delta_tau_y,\n            f1_w_at_tau_y,\n            max_weighted_f1\n        ]\n        all_results.extend(class_results)\n    \n    # Format the final output string\n    formatted_results = [f\"{x:.6f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "To enhance a model's robustness against minor input variations, a simple yet powerful technique is Test-Time Augmentation (TTA), where we average the model's predictions across several slightly modified versions of a test input. This practice is grounded in a key mathematical concept: for a convex loss function like cross-entropy, the loss of an averaged prediction is lower than or equal to the average of the losses. This exercise  will have you implement TTA and compute the \"Jensen gap\"—the precise quantitative benefit of this technique—to build a deeper intuition for why averaging predictions is a powerful tool for improving classifier performance.",
            "id": "3178424",
            "problem": "You are given a supervised learning classification setting in deep learning with Test-Time Augmentation (TTA). Consider a linear classifier with softmax activation defined by the function $f_\\theta:\\mathbb{R}^d\\to\\Delta^{C-1}$, where $\\Delta^{C-1}$ denotes the $(C-1)$-simplex, and parameters $\\theta$ consist of a weight matrix $W\\in\\mathbb{R}^{C\\times d}$. For an input $x\\in\\mathbb{R}^d$, the model produces class probabilities $p=f_\\theta(x)=\\text{softmax}(Wx)$, where the softmax function is defined as\n$$\n\\text{softmax}(z)_i=\\frac{\\exp(z_i)}{\\sum_{j=1}^C \\exp(z_j)}\\quad\\text{for }i=1,\\dots,C.\n$$\nLet the classification loss be the multiclass cross-entropy for a one-hot label $y\\in\\mathbb{R}^C$,\n$$\n\\ell(p,y)=-\\sum_{i=1}^C y_i \\log p_i.\n$$\nDefine a finite set of test-time augmentations $\\{t_1,\\dots,t_n\\}$ on $x$ and nonnegative weights $\\{w_1,\\dots,w_n\\}$ that sum to $1$. The Test-Time Augmentation (TTA) estimate of the prediction is the weighted average of the predictions under the augmentations, namely\n$$\n\\bar{p}=\\mathbb{E}_t\\big[f_\\theta(t(x))\\big]=\\sum_{j=1}^n w_j\\,f_\\theta\\big(t_j(x)\\big).\n$$\nThe expected loss under augmentations is\n$$\n\\mathbb{E}_t\\big[\\ell(f_\\theta(t(x)),y)\\big]=\\sum_{j=1}^n w_j\\,\\ell\\big(f_\\theta(t_j(x)),y\\big).\n$$\nThe Jensen gap is defined as the difference\n$$\nJ=\\mathbb{E}_t\\big[\\ell(f_\\theta(t(x)),y)\\big]-\\ell\\big(\\mathbb{E}_t[f_\\theta(t(x))],y\\big),\n$$\nwhich quantifies the benefit of averaging predictions before applying the loss when $\\ell$ is convex in $p$.\n\nStarting from fundamental definitions of expected risk in supervised learning, the softmax function, and the convexity of the negative logarithm, implement a program to compute the Jensen gap $J$ for several test cases. All mathematical entities must be handled precisely and computations must be numerically stable.\n\nUse the following augmentation functions, each operating on $x\\in\\mathbb{R}^d$:\n- Identity: $t_{\\text{id}}(x)=x$.\n- Additive noise: $t_{\\text{noise}}(x)=x+\\epsilon$, where $\\epsilon\\in\\mathbb{R}^d$ is specified.\n- Feature mask: $t_{\\text{mask}}(x)=m\\odot x$, where $m\\in\\mathbb{R}^d$ and $\\odot$ denotes element-wise multiplication.\n- Scaling: $t_{\\text{scale}}(x)=s\\,x$, where $s\\in\\mathbb{R}$.\n\nYour program must implement the following computation for each test case:\n1. For each augmentation $t_j$, compute $p^{(j)}=f_\\theta\\big(t_j(x)\\big)=\\text{softmax}\\big(W\\,t_j(x)\\big)$.\n2. Compute $\\bar{p}=\\sum_{j=1}^n w_j\\,p^{(j)}$.\n3. Compute the expected loss $\\mathbb{E}_t[\\ell(f_\\theta(t(x)),y)]=\\sum_{j=1}^n w_j\\,\\ell\\big(p^{(j)},y\\big)$.\n4. Compute the Jensen gap $J=\\mathbb{E}_t[\\ell(f_\\theta(t(x)),y)]-\\ell(\\bar{p},y)$.\n\nExpress each Jensen gap as a real number (float). For numerical stability, any logarithm should handle probabilities close to zero appropriately to avoid undefined values.\n\nTest Suite:\n- Case 1 (general case; uniform weights):\n  - $C=3$, $d=4$.\n  - $W=\\begin{bmatrix}1.2 & -0.9 & 0.3 & 0.0\\\\ -0.5 & 1.1 & 0.7 & 0.8\\\\ 0.0 & 0.4 & 1.0 & -0.7\\end{bmatrix}$.\n  - $x=\\begin{bmatrix}0.6 & -0.2 & 1.0 & 0.5\\end{bmatrix}$.\n  - $y=\\begin{bmatrix}0 & 1 & 0\\end{bmatrix}$.\n  - Augmentations: $t_{\\text{id}}$, $t_{\\text{noise}}$ with $\\epsilon=\\begin{bmatrix}0.05 & -0.03 & 0.02 & -0.01\\end{bmatrix}$, $t_{\\text{mask}}$ with $m=\\begin{bmatrix}1.0 & 0.7 & 0.9 & 1.0\\end{bmatrix}$, $t_{\\text{scale}}$ with $s=1.1$.\n  - Weights: $w_j=\\frac{1}{4}$ for $j=1,2,3,4$.\n- Case 2 (boundary case; identical predictions lead to zero gap up to numerical precision; uniform weights):\n  - $C=3$, $d=4$.\n  - $W=\\begin{bmatrix}1.2 & -0.9 & 0.3 & 0.0\\\\ -0.5 & 1.1 & 0.7 & 0.8\\\\ 0.0 & 0.4 & 1.0 & -0.7\\end{bmatrix}$.\n  - $x=\\begin{bmatrix}0.3 & -0.1 & 0.2 & 0.5\\end{bmatrix}$.\n  - $y=\\begin{bmatrix}1 & 0 & 0\\end{bmatrix}$.\n  - Augmentations: $t_{\\text{id}}$, $t_{\\text{noise}}$ with $\\epsilon=\\begin{bmatrix}0 & 0 & 0 & 0\\end{bmatrix}$, $t_{\\text{mask}}$ with $m=\\begin{bmatrix}1 & 1 & 1 & 1\\end{bmatrix}$, $t_{\\text{scale}}$ with $s=1.0$.\n  - Weights: $w_j=\\frac{1}{4}$ for $j=1,2,3,4$.\n- Case 3 (edge case; highly diverse augmentations; uniform weights):\n  - $C=3$, $d=4$.\n  - $W=\\begin{bmatrix}2.0 & -1.5 & 0.0 & 0.5\\\\ -1.0 & 2.0 & 0.5 & -0.5\\\\ 0.0 & -0.5 & 2.0 & -1.0\\end{bmatrix}$.\n  - $x=\\begin{bmatrix}1.0 & -2.0 & 0.5 & 1.5\\end{bmatrix}$.\n  - $y=\\begin{bmatrix}0 & 0 & 1\\end{bmatrix}$.\n  - Augmentations: $t_{\\text{id}}$, $t_{\\text{noise}}$ with $\\epsilon=\\begin{bmatrix}0.8 & -0.8 & 0.8 & -0.8\\end{bmatrix}$, $t_{\\text{mask}}$ with $m=\\begin{bmatrix}0.1 & 1.5 & 0.2 & 2.0\\end{bmatrix}$, $t_{\\text{scale}}$ with $s=0.3$.\n  - Weights: $w_j=\\frac{1}{4}$ for $j=1,2,3,4$.\n- Case 4 (weighted expectation; nonuniform weights):\n  - $C=3$, $d=3$.\n  - $W=\\begin{bmatrix}1.0 & -0.5 & 0.0\\\\ 0.0 & 1.0 & 0.5\\\\ -0.5 & 0.0 & 1.5\\end{bmatrix}$.\n  - $x=\\begin{bmatrix}0.5 & 0.8 & -0.3\\end{bmatrix}$.\n  - $y=\\begin{bmatrix}0 & 1 & 0\\end{bmatrix}$.\n  - Augmentations: $t_{\\text{id}}$, $t_{\\text{noise}}$ with $\\epsilon=\\begin{bmatrix}0.2 & -0.1 & 0.05\\end{bmatrix}$, $t_{\\text{scale}}$ with $s=0.7$.\n  - Weights: $w=\\begin{bmatrix}0.1 & 0.4 & 0.5\\end{bmatrix}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the Jensen gaps for the four test cases as a comma-separated list enclosed in square brackets, with each gap rounded to six decimal places (e.g., \"[0.012345,0.000000,0.056789,0.004321]\"). No additional text should be printed.",
            "solution": "The problem statement is valid. It is scientifically grounded in the principles of supervised learning and information theory, well-posed with all necessary parameters defined, and computationally tractable. The components—linear classifiers, softmax activation, cross-entropy loss, and Test-Time Augmentation (TTA)—are all standard concepts in deep learning. The quantity to be computed, the Jensen gap, is a direct application of Jensen's inequality to the convex cross-entropy loss function, providing a theoretically sound measure of the effect of averaging predictions.\n\nThe objective is to compute the Jensen gap, denoted as $J$, for a linear classifier under various test-time augmentations. The Jensen gap quantifies the difference between the expected loss over augmented predictions and the loss of the averaged prediction.\n\nLet us first define the core components. The model is a linear classifier described by the function $f_\\theta: \\mathbb{R}^d \\to \\Delta^{C-1}$, where $\\theta$ represents the model parameters. For an input vector $x \\in \\mathbb{R}^d$, the model first computes the logits $z \\in \\mathbb{R}^C$ via a linear transformation using a weight matrix $W \\in \\mathbb{R}^{C \\times d}$:\n$$\nz = Wx\n$$\nThese logits are then transformed into a probability distribution $p \\in \\Delta^{C-1}$ over the $C$ classes using the softmax function:\n$$\np = \\text{softmax}(z)\n$$\nwhere the $i$-th component of the output vector is given by\n$$\np_i = \\text{softmax}(z)_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^C \\exp(z_j)}\n$$\nFor a given input $x$ and its corresponding true class label, represented as a one-hot vector $y \\in \\{0, 1\\}^C$ (where $y_k=1$ for the true class $k$ and $y_i=0$ for $i \\neq k$), the classification performance is measured by the multiclass cross-entropy loss:\n$$\n\\ell(p, y) = -\\sum_{i=1}^C y_i \\log(p_i)\n$$\nSince $y$ is one-hot, this simplifies to the negative logarithm of the predicted probability for the true class $k$:\n$$\n\\ell(p, y) = -\\log(p_k)\n$$\nThe function $\\phi(v) = -\\log(v)$ is a strictly convex function for $v \\in (0, 1]$. This convexity is the mathematical foundation for the Jensen gap.\n\nTest-Time Augmentation (TTA) involves creating multiple versions of a test input $x$ using a set of transformations $\\{t_1, \\dots, t_n\\}$. A prediction is made for each augmented input $t_j(x)$, and these predictions are then aggregated. The aggregated prediction, $\\bar{p}$, is the weighted average of individual predictions:\n$$\n\\bar{p} = \\mathbb{E}_t\\big[f_\\theta(t(x))\\big] = \\sum_{j=1}^n w_j f_\\theta\\big(t_j(x)\\big) = \\sum_{j=1}^n w_j p^{(j)}\n$$\nwhere $p^{(j)} = f_\\theta(t_j(x))$ and $\\{w_1, \\dots, w_n\\}$ are nonnegative weights summing to $1$.\n\nThe expected loss under this augmentation scheme is the weighted average of the losses for each individual prediction:\n$$\n\\mathbb{E}_t\\big[\\ell(f_\\theta(t(x)),y)\\big] = \\sum_{j=1}^n w_j \\ell\\big(p^{(j)}, y\\big)\n$$\nThe Jensen gap $J$ is defined by Jensen's inequality for the convex loss function $\\ell$. It is the difference between the expectation of the function's values and the function's value at the expectation:\n$$\nJ = \\mathbb{E}_t\\big[\\ell(f_\\theta(t(x)), y)\\big] - \\ell\\big(\\mathbb{E}_t[f_\\theta(t(x))], y\\big)\n$$\nSubstituting the expressions for the TTA context, we get:\n$$\nJ = \\left(\\sum_{j=1}^n w_j \\ell\\big(p^{(j)}, y\\big)\\right) - \\ell(\\bar{p}, y)\n$$\nDue to the convexity of the negative logarithm function, this gap $J$ is guaranteed to be non-negative, $J \\ge 0$. It is strictly positive if the predictions $p^{(j)}$ are not all identical. The gap represents the reduction in loss achieved by averaging the probability vectors (an \"ensemble\" of predictions) before calculating the loss, as compared to averaging the losses themselves.\n\nThe computation for each test case adheres to the following sequence:\n1.  **Augmentation and Prediction**: For each augmentation $t_j$ in the set, we first compute the augmented input $x^{(j)} = t_j(x)$. Then, we calculate the corresponding logits $z^{(j)} = W x^{(j)}$ and the probability vector $p^{(j)} = \\text{softmax}(z^{(j)})$. To ensure numerical stability, the softmax function is implemented using the log-sum-exp trick, where the maximum value of the logit vector is subtracted from all logits before exponentiation to prevent overflow.\n\n2.  **Averaged Prediction**: The TTA prediction $\\bar{p}$ is computed as the weighted sum of the individual probability vectors: $\\bar{p} = \\sum_{j=1}^n w_j p^{(j)}$.\n\n3.  **Expected Loss Calculation**: For each $p^{(j)}$, the cross-entropy loss $\\ell_j = \\ell(p^{(j)}, y) = -\\log(p^{(j)}_k)$ is calculated, where $k$ is the index of the true class. The expected loss is then the weighted sum of these individual losses: $\\mathbb{E}_t[\\ell] = \\sum_{j=1}^n w_j \\ell_j$. When computing the logarithm, a small epsilon, corresponding to machine precision, is added to the probability to prevent $\\log(0)$ which is undefined.\n\n4.  **Jensen Gap Computation**: The loss of the averaged prediction, $\\bar{\\ell} = \\ell(\\bar{p}, y) = -\\log(\\bar{p}_k)$, is calculated. The final Jensen gap is the difference $J = \\mathbb{E}_t[\\ell] - \\bar{\\ell}$.\n\nThis procedure will be applied to each of the four test cases provided.\n- **Case 1** represents a general scenario with diverse augmentations.\n- **Case 2** serves as a sanity check: all augmentations are identity transformations, which should result in identical predictions $p^{(j)}$, a constant TTA prediction $\\bar{p} = p^{(1)}$, and thus a Jensen gap of $J=0$.\n- **Case 3** uses more extreme augmentations, which is expected to create highly diverse predictions and thus a larger Jensen gap.\n- **Case 4** demonstrates the use of non-uniform weights in the TTA expectation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Jensen gap for Test-Time Augmentation (TTA) in a linear\n    classification setting for a series of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: General case; uniform weights\n        {\n            \"W\": np.array([[1.2, -0.9, 0.3, 0.0],\n                           [-0.5, 1.1, 0.7, 0.8],\n                           [0.0, 0.4, 1.0, -0.7]]),\n            \"x\": np.array([0.6, -0.2, 1.0, 0.5]),\n            \"y\": np.array([0, 1, 0]),\n            \"augmentations\": [\n                {\"type\": \"id\"},\n                {\"type\": \"noise\", \"params\": {\"epsilon\": np.array([0.05, -0.03, 0.02, -0.01])}},\n                {\"type\": \"mask\", \"params\": {\"m\": np.array([1.0, 0.7, 0.9, 1.0])}},\n                {\"type\": \"scale\", \"params\": {\"s\": 1.1}},\n            ],\n            \"weights\": np.array([0.25, 0.25, 0.25, 0.25]),\n        },\n        # Case 2: Boundary case; identical predictions\n        {\n            \"W\": np.array([[1.2, -0.9, 0.3, 0.0],\n                           [-0.5, 1.1, 0.7, 0.8],\n                           [0.0, 0.4, 1.0, -0.7]]),\n            \"x\": np.array([0.3, -0.1, 0.2, 0.5]),\n            \"y\": np.array([1, 0, 0]),\n            \"augmentations\": [\n                {\"type\": \"id\"},\n                {\"type\": \"noise\", \"params\": {\"epsilon\": np.array([0.0, 0.0, 0.0, 0.0])}},\n                {\"type\": \"mask\", \"params\": {\"m\": np.array([1.0, 1.0, 1.0, 1.0])}},\n                {\"type\": \"scale\", \"params\": {\"s\": 1.0}},\n            ],\n            \"weights\": np.array([0.25, 0.25, 0.25, 0.25]),\n        },\n        # Case 3: Edge case; highly diverse augmentations\n        {\n            \"W\": np.array([[2.0, -1.5, 0.0, 0.5],\n                           [-1.0, 2.0, 0.5, -0.5],\n                           [0.0, -0.5, 2.0, -1.0]]),\n            \"x\": np.array([1.0, -2.0, 0.5, 1.5]),\n            \"y\": np.array([0, 0, 1]),\n            \"augmentations\": [\n                {\"type\": \"id\"},\n                {\"type\": \"noise\", \"params\": {\"epsilon\": np.array([0.8, -0.8, 0.8, -0.8])}},\n                {\"type\": \"mask\", \"params\": {\"m\": np.array([0.1, 1.5, 0.2, 2.0])}},\n                {\"type\": \"scale\", \"params\": {\"s\": 0.3}},\n            ],\n            \"weights\": np.array([0.25, 0.25, 0.25, 0.25]),\n        },\n        # Case 4: Weighted expectation; nonuniform weights\n        {\n            \"W\": np.array([[1.0, -0.5, 0.0],\n                           [0.0, 1.0, 0.5],\n                           [-0.5, 0.0, 1.5]]),\n            \"x\": np.array([0.5, 0.8, -0.3]),\n            \"y\": np.array([0, 1, 0]),\n            \"augmentations\": [\n                {\"type\": \"id\"},\n                {\"type\": \"noise\", \"params\": {\"epsilon\": np.array([0.2, -0.1, 0.05])}},\n                {\"type\": \"scale\", \"params\": {\"s\": 0.7}},\n            ],\n            \"weights\": np.array([0.1, 0.4, 0.5]),\n        },\n    ]\n\n    def softmax(z):\n        # Numerically stable softmax using the log-sum-exp trick\n        z_stable = z - np.max(z)\n        exps = np.exp(z_stable)\n        return exps / np.sum(exps)\n\n    def cross_entropy_loss(p, y):\n        # Since y is one-hot, find the index of the true class\n        true_class_idx = np.argmax(y)\n        # Add a small epsilon for numerical stability to avoid log(0)\n        epsilon = np.finfo(float).eps\n        return -np.log(p[true_class_idx] + epsilon)\n\n    results = []\n    for case in test_cases:\n        W, x, y = case[\"W\"], case[\"x\"], case[\"y\"]\n        augmentations, weights = case[\"augmentations\"], case[\"weights\"]\n        \n        predictions = []\n        for aug_info in augmentations:\n            aug_type = aug_info[\"type\"]\n            if aug_type == \"id\":\n                x_aug = x\n            elif aug_type == \"noise\":\n                x_aug = x + aug_info[\"params\"][\"epsilon\"]\n            elif aug_type == \"mask\":\n                x_aug = x * aug_info[\"params\"][\"m\"]\n            elif aug_type == \"scale\":\n                x_aug = x * aug_info[\"params\"][\"s\"]\n            \n            # 1. Compute prediction for the augmented input\n            logits = W @ x_aug\n            p = softmax(logits)\n            predictions.append(p)\n        \n        predictions = np.array(predictions)\n        \n        # 2. Compute the average prediction (bar_p)\n        # p_bar = np.average(predictions, axis=0, weights=weights) would also work\n        p_bar = np.einsum('i,ij->j', weights, predictions)\n\n        # 3. Compute the expected loss\n        individual_losses = np.array([cross_entropy_loss(p, y) for p in predictions])\n        expected_loss = np.sum(weights * individual_losses)\n        \n        # 4. Compute the loss of the average prediction and the Jensen gap\n        loss_of_avg = cross_entropy_loss(p_bar, y)\n        jensen_gap = expected_loss - loss_of_avg\n        \n        results.append(jensen_gap)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "As machine learning models become integral to security-sensitive applications, understanding their vulnerabilities is essential. This exercise delves into the world of adversarial machine learning by simulating a \"backdoor attack,\" where a model is secretly trained to misclassify inputs containing a hidden trigger while appearing to function normally on clean data. This advanced practice  offers a complete hands-on pipeline for security analysis: you will first build a backdoored model by poisoning a dataset and then implement and evaluate three distinct defense mechanisms—magnitude pruning, fine-tuning, and spectral signature analysis—to detect and mitigate the threat.",
            "id": "3178407",
            "problem": "You are tasked with implementing, from first principles, a supervised binary classification pipeline in deep learning that includes the construction of a synthetic dataset with a backdoor trigger, model training, and three defenses (magnitude pruning, fine-tuning on clean data, and spectral signatures). Your program must output the attack success rate for several specified test cases. All computations must be performed in a self-contained manner using the definitions of empirical risk minimization and gradient-based optimization, with no external datasets.\n\nDefinitions and setup:\n- The input space is a vector space of dimension $d = 64$, representing $8 \\times 8$ grayscale images flattened into vectors in $\\mathbb{R}^{64}$.\n- There are two classes with labels $y \\in \\{0, 1\\}$. Class $0$ has a prototype image with a vertical stripe at column index $2$ (zero-based), and class $1$ has a prototype image with a horizontal stripe at row index $5$. To generate a clean sample for class $y$, draw $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$ with $\\sigma = 0.2$ and set $\\mathbf{x} = \\mathbf{p}_y + \\boldsymbol{\\epsilon}$, where $\\mathbf{p}_y \\in \\mathbb{R}^{64}$ is the flattened prototype for class $y$.\n- A backdoor trigger is defined as a deterministic transformation $T(\\mathbf{x})$ that adds a constant $\\alpha$ to a $2 \\times 2$ square patch located at rows $6$ to $7$ and columns $6$ to $7$ (zero-based indices) of the $8 \\times 8$ image. Use $\\alpha = 3.0$.\n- The training set is constructed as follows:\n  - Generate $n_0 = 200$ clean samples for class $0$ and $n_1 = 200$ clean samples for class $1$.\n  - Randomly select a fraction $p = 0.2$ of the class $0$ samples (i.e., $p \\cdot n_0$ samples), apply the trigger $T(\\cdot)$ to them, and flip their labels to class $1$. All other samples remain with their original labels. This yields a poisoned training set with label space $\\{0, 1\\}$.\n- The test set for evaluating the attack success rate is constructed by generating $m = 200$ clean samples from class $0$ and then applying the trigger $T(\\cdot)$ to all of them.\n\nNeural network model and training:\n- Use a fully connected neural network with one hidden layer of width $h = 32$ with the Rectified Linear Unit (ReLU) activation $\\phi(u) = \\max\\{0, u\\}$ and a softmax output layer with $2$ units. Let the parameters be $\\mathbf{W}_1 \\in \\mathbb{R}^{64 \\times 32}$, $\\mathbf{b}_1 \\in \\mathbb{R}^{32}$, $\\mathbf{W}_2 \\in \\mathbb{R}^{32 \\times 2}$, and $\\mathbf{b}_2 \\in \\mathbb{R}^{2}$. The forward pass is\n$$\n\\mathbf{h} = \\phi(\\mathbf{X}\\mathbf{W}_1 + \\mathbf{b}_1), \\quad \\mathbf{Z} = \\mathbf{h}\\mathbf{W}_2 + \\mathbf{b}_2,\n$$\nand the softmax probability for class $k \\in \\{0,1\\}$ for a sample with logits $\\mathbf{z}$ is\n$$\n\\mathrm{softmax}(\\mathbf{z})_k = \\frac{\\exp(z_k)}{\\sum_{j=0}^{1} \\exp(z_j)}.\n$$\n- Use empirical risk minimization with the categorical cross-entropy loss. For a dataset $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{N}$, the loss is\n$$\n\\mathcal{L}(\\Theta) = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=0}^{1} \\mathbf{1}[y_i = k]\\ \\log p_\\Theta(y=k \\mid \\mathbf{x}_i),\n$$\nwhere $p_\\Theta(y=k \\mid \\mathbf{x}_i)$ is the softmax output for class $k$ and $\\Theta = (\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, \\mathbf{b}_2)$.\n- Optimize $\\mathcal{L}(\\Theta)$ by full-batch gradient descent with learning rate $\\eta_0 = 0.1$ for $E_0 = 200$ epochs, starting from He initialization for weights, that is, entries of $\\mathbf{W}_1$ drawn independently from $\\mathcal{N}(0, \\sqrt{2/64}^2)$ and entries of $\\mathbf{W}_2$ from $\\mathcal{N}(0, \\sqrt{2/32}^2)$, with biases initialized to zero.\n\nDefenses to test:\n1. Magnitude pruning: Given a trained model, compute the set of all absolute weight magnitudes over $\\mathbf{W}_1$ and $\\mathbf{W}_2$ (biases are excluded). For a prune fraction $q \\in (0,1)$, find the $q$-quantile threshold of the magnitudes and set to zero all weights with absolute value below or equal to this threshold (global magnitude pruning).\n2. Fine-tuning: Given a trained model, generate an additional clean set of $c = 100$ samples per class (class $0$ and class $1$), and perform full-batch gradient descent for $S$ steps with learning rate $\\eta_f = 0.05$ on this clean data only, starting from the trained model parameters.\n3. Spectral signatures: Given a trained model, compute the penultimate activations $\\mathbf{h}_i$ for all training samples. For each class $k \\in \\{0,1\\}$, form the matrix $\\mathbf{H}^{(k)} \\in \\mathbb{R}^{N_k \\times h}$ with rows given by $\\mathbf{h}_i$ for samples with label $k$, center it by subtracting the mean of $\\mathbf{H}^{(k)}$ across rows, and compute its Singular Value Decomposition (SVD) $\\mathbf{H}^{(k)} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top$. Let $\\mathbf{v}^{(k)} \\in \\mathbb{R}^{h}$ be the top right singular vector (the first row of $\\mathbf{V}^\\top$). For each row $\\mathbf{r}$ of the centered $\\mathbf{H}^{(k)}$, assign an outlier score $s = |\\mathbf{r} \\cdot \\mathbf{v}^{(k)}|$. Remove the top fraction $r \\in (0,1)$ of samples with the largest $s$ from each class, and then train a fresh model from scratch on the remaining (filtered) training set for $E_1 = 150$ epochs with learning rate $\\eta_0$ as above.\n\nAttack success rate (ASR):\n- For any trained model, define the attack success rate as\n$$\n\\mathrm{ASR} = \\frac{1}{m}\\sum_{i=1}^{m} \\mathbf{1}\\left[\\hat{y}_i = 1\\right],\n$$\nwhere $\\hat{y}_i$ is the predicted class for the $i$-th triggered test sample originally from class $0$. Report $\\mathrm{ASR}$ as a decimal in $[0,1]$.\n\nTest suite:\nUse the following fixed random seed $s = 1337$ for all random operations. Train a single baseline model on the poisoned training set as described. Then, for each test case below, apply the specified defense starting from the baseline model (except the spectral signatures cases, which require retraining on filtered data) and compute the corresponding attack success rate on the triggered test set:\n- Case $1$: No defense.\n- Case $2$: Magnitude pruning with prune fraction $q = 0.2$.\n- Case $3$: Magnitude pruning with prune fraction $q = 0.6$.\n- Case $4$: Fine-tuning with $S = 100$ steps.\n- Case $5$: Spectral signatures with removal fraction $r = 0.1$.\n- Case $6$: Spectral signatures with removal fraction $r = 0.3$.\n\nOutput specification:\n- Your program should produce a single line of output containing the attack success rates for Cases $1$ through $6$ as a comma-separated list of floats rounded to $3$ decimal places, enclosed in square brackets. For example, an output with hypothetical values should look like\n\"[0.950,0.800,0.420,0.700,0.300,0.150]\".",
            "solution": "The problem statement has been analyzed and found to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to proceed with a solution. It describes a complete and verifiable computational experiment in the domain of deep learning security.\n\n### Principle-Based Design\n\nThe solution implements a supervised binary classification pipeline to demonstrate a backdoor attack and evaluate several defenses. The implementation is based on first principles of neural networks and gradient-based optimization.\n\n#### 1. Data Generation\n\nThe entire process begins with the programmatic generation of a synthetic dataset.\n- **Prototypes**: Two prototype images of size $8 \\times 8$ are defined, one for each class. Class $0$ corresponds to a vertical stripe in column $2$, and class $1$ to a horizontal stripe in row $5$. These images are flattened into vectors $\\mathbf{p}_0, \\mathbf{p}_1 \\in \\mathbb{R}^{64}$.\n- **Clean Data**: A clean data sample $(\\mathbf{x}, y)$ for class $y \\in \\{0, 1\\}$ is generated by adding Gaussian noise to the corresponding prototype: $\\mathbf{x} = \\mathbf{p}_y + \\boldsymbol{\\epsilon}$, where $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$ with $\\sigma = 0.2$.\n- **Backdoor Trigger**: A trigger transformation $T(\\cdot)$ is defined, which adds a constant value $\\alpha = 3.0$ to a fixed $2 \\times 2$ pixel patch at the bottom-right corner of an image.\n- **Poisoned Training Set**: A training set of $N=400$ samples is constructed. Initially, $n_0 = 200$ clean samples for class $0$ and $n_1 = 200$ for class $1$ are generated. A poisoning fraction $p = 0.2$ of the class $0$ samples (i.e., $40$ samples) are chosen. The trigger $T(\\cdot)$ is applied to these selected samples, and their labels are flipped from $y=0$ to $y=1$. This creates the poisoned training set, where the model is surreptitiously taught to associate the trigger pattern with class $1$.\n- **Attack Test Set**: To evaluate the attack's effectiveness, a separate test set is created by generating $m = 200$ new clean samples of class $0$ and applying the trigger $T(\\cdot)$ to all of them.\n\n#### 2. Neural Network Model and Training\n\n- **Architecture**: A fully connected neural network with one hidden layer is employed. The architecture is $64 \\to 32 \\to 2$.\n    - Input layer dimension: $d=64$.\n    - Hidden layer dimension: $h=32$ with ReLU activation, $\\phi(u) = \\max\\{0, u\\}$.\n    - Output layer dimension: $2$ (for two classes) with a softmax activation.\n- **Forward Propagation**: For a batch of input data $\\mathbf{X}$, the model computes:\n    $$ \\mathbf{Z}_1 = \\mathbf{X}\\mathbf{W}_1 + \\mathbf{b}_1 $$\n    $$ \\mathbf{H} = \\phi(\\mathbf{Z}_1) $$\n    $$ \\mathbf{Z}_2 = \\mathbf{H}\\mathbf{W}_2 + \\mathbf{b}_2 $$\n    $$ \\mathbf{P} = \\mathrm{softmax}(\\mathbf{Z}_2) $$\n    where $\\Theta = (\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, \\mathbf{b}_2)$ are the model parameters, $\\mathbf{Z}_2$ are the output logits, and $\\mathbf{P}$ are the predicted class probabilities.\n- **Initialization**: Weights are initialized using He initialization. Entries of $\\mathbf{W}_1$ are drawn from $\\mathcal{N}(0, 2/64)$ and entries of $\\mathbf{W}_2$ from $\\mathcal{N}(0, 2/32)$. All biases $\\mathbf{b}_1, \\mathbf{b}_2$ are initialized to zero.\n- **Training**: The model is trained using empirical risk minimization on the full batch of training data (full-batch gradient descent).\n    - **Loss Function**: The categorical cross-entropy loss is minimized:\n      $$ \\mathcal{L}(\\Theta) = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=0}^{1} \\mathbf{Y}_{ik}\\ \\log \\mathbf{P}_{ik} $$\n      where $\\mathbf{Y}$ is the one-hot encoded matrix of true labels.\n    - **Backpropagation**: The gradients of the loss with respect to the parameters are computed via the chain rule:\n      $$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_2} = \\frac{1}{N}(\\mathbf{P} - \\mathbf{Y}) $$\n      $$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_2} = \\mathbf{H}^\\top \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_2}, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_2} = \\sum_{i=1}^{N} \\left(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_2}\\right)_i $$\n      $$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_1} = \\mathbf{X}^\\top \\left( \\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_2} \\mathbf{W}_2^\\top \\right) \\odot \\phi'(\\mathbf{Z}_1) \\right), \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_1} = \\sum_{i=1}^{N} \\left(\\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_2} \\mathbf{W}_2^\\top \\right) \\odot \\phi'(\\mathbf{Z}_1)\\right)_i $$\n      where $\\odot$ denotes element-wise multiplication and $\\phi'$ is the derivative of the ReLU function.\n    - **Parameter Update**: Parameters are updated iteratively: $\\Theta \\leftarrow \\Theta - \\eta \\nabla_\\Theta \\mathcal{L}$.\n\n#### 3. Attack Evaluation\n\nThe primary metric is the Attack Success Rate (ASR), defined as the fraction of triggered test samples (originally from class $0$) that are misclassified as class $1$:\n$$ \\mathrm{ASR} = \\frac{1}{m}\\sum_{i=1}^{m} \\mathbf{1}\\left[\\hat{y}_i = 1\\right] $$\nwhere $\\hat{y}_i$ is the predicted label for the $i$-th sample in the attack test set. A high ASR indicates a successful backdoor attack.\n\n#### 4. Defense Mechanisms\n\nThree distinct defense strategies are implemented and evaluated.\n\n1.  **Magnitude Pruning**: This defense posits that backdoor-related neurons and weights might have small magnitudes if the trigger is a subtle feature. It operates by removing a fraction $q$ of the weights with the smallest absolute magnitude from the trained model. All weight parameters in $\\mathbf{W}_1$ and $\\mathbf{W}_2$ are pooled, their absolute values are computed, and a global threshold is determined as the $q$-quantile of these magnitudes. Any weight whose absolute value is less than or equal to this threshold is set to zero.\n\n2.  **Fine-tuning**: This defense attempts to \"unlearn\" the backdoor by retraining the model on a small, clean dataset. A clean dataset containing $c=100$ samples per class is generated. Starting from the parameters of the compromised model, full-batch gradient descent is performed for $S=100$ steps on this clean data with a smaller learning rate $\\eta_f = 0.05$.\n\n3.  **Spectral Signatures**: This defense identifies and removes poisoned samples from the training set by analyzing the model's internal representations.\n    - First, the baseline model is used to compute the penultimate layer activations $\\mathbf{h}_i$ for all samples in the poisoned training set.\n    - For each class $k \\in \\{0, 1\\}$ (based on the poisoned labels), the activations are collected into a matrix $\\mathbf{H}^{(k)}$.\n    - This matrix is centered by subtracting the mean activation vector: $\\mathbf{H}^{(k)}_{\\text{centered}} = \\mathbf{H}^{(k)} - \\overline{\\mathbf{H}^{(k)}}$.\n    - The Singular Value Decomposition (SVD) of the centered matrix is computed. The top right singular vector $\\mathbf{v}^{(k)}$ (the first principal component) is extracted. This vector often corresponds to the direction of greatest variance, which may be induced by the cluster of poisoned samples.\n    - An outlier score $s_i = |\\mathbf{r}_i \\cdot \\mathbf{v}^{(k)}|$ is computed for each sample $i$ in class $k$, where $\\mathbf{r}_i$ is the corresponding row in $\\mathbf{H}^{(k)}_{\\text{centered}}$. Samples with high scores are considered anomalous.\n    - A fraction $r$ of samples with the highest outlier scores are removed from each class in the training set.\n    - Finally, a new model is trained from scratch on this filtered dataset for $E_1=150$ epochs.\n\n#### 5. Execution Pipeline\n\nThe program executes the following steps in sequence:\n1.  A fixed random seed ($1337$) is set for reproducibility.\n2.  The poisoned training data and triggered test data are generated.\n3.  A baseline model is trained on the poisoned data.\n4.  The ASR is calculated for the undefended baseline model (Case 1).\n5.  Defenses are applied:\n    - Magnitude pruning is applied to copies of the baseline model for $q=0.2$ (Case 2) and $q=0.6$ (Case 3).\n    - Fine-tuning is applied to a copy of the baseline model (Case 4).\n    - The spectral signature defense is applied to the training data for $r=0.1$ (Case 5) and $r=0.3$ (Case 6), followed by retraining new models.\n6.  The ASR is calculated for each of the five defended models.\n7.  The six ASR values are collected and printed in the specified format.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    # Fixed parameters from problem statement\n    D_IN, D_HIDDEN, D_OUT = 64, 32, 2\n    IMG_SIZE = 8\n    SIGMA = 0.2\n    ALPHA = 3.0\n    N0, N1 = 200, 200\n    P_POISON = 0.2\n    M_TEST = 200\n    SEED = 1337\n\n    ETA0 = 0.1\n    EPOCHS0 = 200\n    \n    Q_PRUNE_1, Q_PRUNE_2 = 0.2, 0.6\n    \n    C_FINETUNE = 100\n    S_FINETUNE = 100\n    ETA_F = 0.05\n\n    R_SPECTRAL_1, R_SPECTRAL_2 = 0.1, 0.3\n    EPOCHS1 = 150\n\n    rng = np.random.default_rng(SEED)\n\n    def generate_prototypes():\n        p0 = np.zeros((IMG_SIZE, IMG_SIZE))\n        p0[:, 2] = 1.0\n        p1 = np.zeros((IMG_SIZE, IMG_SIZE))\n        p1[5, :] = 1.0\n        return p0.flatten(), p1.flatten()\n\n    def apply_trigger(X_img):\n        X_triggered_img = X_img.copy()\n        X_triggered_img[:, 6:8, 6:8] += ALPHA\n        return X_triggered_img\n\n    def generate_data(n_samples_per_class, prototypes, noise_sigma):\n        p0, p1 = prototypes\n        X0 = rng.normal(loc=p0, scale=noise_sigma, size=(n_samples_per_class, D_IN))\n        y0 = np.zeros(n_samples_per_class, dtype=int)\n        X1 = rng.normal(loc=p1, scale=noise_sigma, size=(n_samples_per_class, D_IN))\n        y1 = np.ones(n_samples_per_class, dtype=int)\n        return X0, y0, X1, y1\n        \n    class NeuralNetwork:\n        def __init__(self, rng_instance):\n            self.rng = rng_instance\n            self.W1 = self.rng.normal(0, np.sqrt(2 / D_IN), (D_IN, D_HIDDEN))\n            self.b1 = np.zeros(D_HIDDEN)\n            self.W2 = self.rng.normal(0, np.sqrt(2 / D_HIDDEN), (D_HIDDEN, D_OUT))\n            self.b2 = np.zeros(D_OUT)\n\n        def clone(self):\n            cloned_model = NeuralNetwork(self.rng)\n            cloned_model.W1 = self.W1.copy()\n            cloned_model.b1 = self.b1.copy()\n            cloned_model.W2 = self.W2.copy()\n            cloned_model.b2 = self.b2.copy()\n            return cloned_model\n\n        def forward(self, X):\n            z1 = X @ self.W1 + self.b1\n            h = np.maximum(0, z1)\n            z2 = h @ self.W2 + self.b2\n            return z2, h, z1\n\n        def predict_probs(self, X):\n            z2, _, _ = self.forward(X)\n            exp_scores = np.exp(z2 - np.max(z2, axis=1, keepdims=True))\n            return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n        def predict(self, X):\n            return np.argmax(self.predict_probs(X), axis=1)\n\n        def train(self, X, y, epochs, learning_rate):\n            n_samples = X.shape[0]\n            y_one_hot = np.zeros((n_samples, D_OUT))\n            y_one_hot[np.arange(n_samples), y] = 1\n\n            for _ in range(epochs):\n                # Full-batch gradient descent\n                z2, h, z1 = self.forward(X)\n                \n                # Softmax\n                probs = self.predict_probs(X)\n                \n                # Gradients for output layer\n                d_z2 = (probs - y_one_hot) / n_samples\n                d_W2 = h.T @ d_z2\n                d_b2 = np.sum(d_z2, axis=0)\n                \n                # Gradients for hidden layer\n                d_h = d_z2 @ self.W2.T\n                d_z1 = d_h * (z1 > 0)\n                d_W1 = X.T @ d_z1\n                d_b1 = np.sum(d_z1, axis=0)\n                \n                # Update weights\n                self.W1 -= learning_rate * d_W1\n                self.b1 -= learning_rate * d_b1\n                self.W2 -= learning_rate * d_W2\n                self.b2 -= learning_rate * d_b2\n                \n    def calculate_asr(model, X_triggered_test):\n        preds = model.predict(X_triggered_test)\n        asr = np.mean(preds == 1)\n        return asr\n\n    # --- Data Preparation ---\n    prototypes = generate_prototypes()\n    p0, p1 = prototypes\n    \n    # Poisoned Training Set\n    X0_train, y0_train, X1_train, y1_train = generate_data(N0, prototypes, SIGMA)\n    \n    n_poison = int(P_POISON * N0)\n    poison_indices = rng.choice(N0, n_poison, replace=False)\n    \n    X0_to_poison = X0_train[poison_indices]\n    X0_to_poison_img = X0_to_poison.reshape(-1, IMG_SIZE, IMG_SIZE)\n    X_poisoned_img = apply_trigger(X0_to_poison_img)\n    X_poisoned = X_poisoned_img.reshape(-1, D_IN)\n    \n    X0_clean = np.delete(X0_train, poison_indices, axis=0)\n    y0_clean = np.delete(y0_train, poison_indices, axis=0)\n\n    # Poisoned samples have their labels flipped to 1\n    X_train = np.vstack([X0_clean, X1_train, X_poisoned])\n    y_train = np.hstack([y0_clean, y1_train, np.ones(n_poison, dtype=int)])\n    \n    # ASR Test Set\n    X0_test, _, _, _ = generate_data(M_TEST, prototypes, SIGMA)\n    X0_test_img = X0_test.reshape(-1, IMG_SIZE, IMG_SIZE)\n    X_triggered_test_img = apply_trigger(X0_test_img)\n    X_triggered_test = X_triggered_test_img.reshape(-1, D_IN)\n\n    # --- Baseline Model Training ---\n    baseline_model = NeuralNetwork(rng)\n    baseline_model.train(X_train, y_train, EPOCHS0, ETA0)\n    \n    results = []\n\n    # Case 1: No defense\n    asr_baseline = calculate_asr(baseline_model, X_triggered_test)\n    results.append(asr_baseline)\n\n    # Case 2: Magnitude Pruning q=0.2\n    pruned_model_1 = baseline_model.clone()\n    weights = np.concatenate([pruned_model_1.W1.flatten(), pruned_model_1.W2.flatten()])\n    threshold = np.quantile(np.abs(weights), Q_PRUNE_1)\n    pruned_model_1.W1[np.abs(pruned_model_1.W1) <= threshold] = 0\n    pruned_model_1.W2[np.abs(pruned_model_1.W2) <= threshold] = 0\n    asr_prune_1 = calculate_asr(pruned_model_1, X_triggered_test)\n    results.append(asr_prune_1)\n\n    # Case 3: Magnitude Pruning q=0.6\n    pruned_model_2 = baseline_model.clone()\n    weights = np.concatenate([pruned_model_2.W1.flatten(), pruned_model_2.W2.flatten()])\n    threshold = np.quantile(np.abs(weights), Q_PRUNE_2)\n    pruned_model_2.W1[np.abs(pruned_model_2.W1) <= threshold] = 0\n    pruned_model_2.W2[np.abs(pruned_model_2.W2) <= threshold] = 0\n    asr_prune_2 = calculate_asr(pruned_model_2, X_triggered_test)\n    results.append(asr_prune_2)\n\n    # Case 4: Fine-tuning\n    finetuned_model = baseline_model.clone()\n    X0_clean_ft, y0_clean_ft, X1_clean_ft, y1_clean_ft = generate_data(C_FINETUNE, prototypes, SIGMA)\n    X_ft = np.vstack([X0_clean_ft, X1_clean_ft])\n    y_ft = np.hstack([y0_clean_ft, y1_clean_ft])\n    finetuned_model.train(X_ft, y_ft, S_FINETUNE, ETA_F)\n    asr_finetune = calculate_asr(finetuned_model, X_triggered_test)\n    results.append(asr_finetune)\n\n    # Case 5 & 6 helper function\n    def run_spectral_defense(r, X_tr, y_tr, X_te, model_to_get_activations):\n        _, h, _ = model_to_get_activations.forward(X_tr)\n        \n        indices_to_keep = np.array([], dtype=int)\n        \n        for k in range(D_OUT):\n            class_indices = np.where(y_tr == k)[0]\n            if len(class_indices) == 0: continue\n            \n            H_k = h[class_indices]\n            H_k_centered = H_k - H_k.mean(axis=0)\n            \n            _, _, vh = svd(H_k_centered, full_matrices=False)\n            top_sv = vh[0, :]\n            \n            scores = np.abs(H_k_centered @ top_sv)\n            \n            num_to_remove = int(r * len(class_indices))\n            if num_to_remove > 0:\n                outlier_indices_in_class = np.argsort(scores)[-num_to_remove:]\n                indices_to_remove = class_indices[outlier_indices_in_class]\n                \n                class_indices_to_keep = np.setdiff1d(class_indices, indices_to_remove)\n                indices_to_keep = np.union1d(indices_to_keep, class_indices_to_keep)\n            else:\n                indices_to_keep = np.union1d(indices_to_keep, class_indices)\n\n        X_filtered = X_tr[indices_to_keep]\n        y_filtered = y_tr[indices_to_keep]\n        \n        spectral_model = NeuralNetwork(rng)\n        spectral_model.train(X_filtered, y_filtered, EPOCHS1, ETA0)\n        \n        return calculate_asr(spectral_model, X_te)\n\n    # Case 5: Spectral Signatures r=0.1\n    asr_spectral_1 = run_spectral_defense(R_SPECTRAL_1, X_train, y_train, X_triggered_test, baseline_model)\n    results.append(asr_spectral_1)\n    \n    # Case 6: Spectral Signatures r=0.3\n    asr_spectral_2 = run_spectral_defense(R_SPECTRAL_2, X_train, y_train, X_triggered_test, baseline_model)\n    results.append(asr_spectral_2)\n\n    # Final Output Formatting\n    formatted_results = [f\"{res:.3f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}