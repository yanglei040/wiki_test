{
    "hands_on_practices": [
        {
            "introduction": "Empirical Risk Minimization (ERM) provides a clear principle: find the model that best fits the data you have. However, is the best fit on training data always the most reliable for future predictions? This practice  confronts this question by contrasting the ERM solution under a zero-one loss with the principle of margin maximization. You will discover that the model with the absolute lowest training error isn't necessarily the one a machine learning practitioner might prefer, revealing a foundational trade-off in statistical learning theory.",
            "id": "3121461",
            "problem": "You will design and analyze a one-dimensional toy dataset drawn from overlapping Gaussian classes and compute the empirical risk minimizer for a linear classifier under zero-one loss. You will then contrast this solution with the margin-maximizing threshold to illustrate the tension between empirical risk minimization and margin maximization.\n\nA data scientist models two binary classes as follows. For label $y = +1$, features $x \\in \\mathbb{R}$ are drawn independently from a Gaussian distribution $\\mathcal{N}(\\mu_{+}, \\sigma_{+}^{2})$; for label $y = -1$, features are drawn independently from $\\mathcal{N}(\\mu_{-}, \\sigma_{-}^{2})$. Because $\\mu_{+}$ and $\\mu_{-}$ are close and $\\sigma_{+}, \\sigma_{-} > 0$, the class-conditional distributions overlap. A single dataset of $n = 6$ labeled samples is observed:\n- For $y = +1$: three samples $x = -0.8, \\; 0.2, \\; 0.3$.\n- For $y = -1$: three samples $x = -1.1, \\; 0.5, \\; 1.0$.\n\nConsider the family of linear classifiers in one dimension parameterized by a threshold $s \\in \\mathbb{R}$:\n$$\nh_{s}(x) = \\operatorname{sign}(x - s),\n$$\nwhich predicts $+1$ if $x \\ge s$ and $-1$ otherwise. The empirical risk under the zero-one loss is\n$$\n\\hat{R}(s) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}\\!\\left\\{ y_{i} \\ne h_{s}(x_{i}) \\right\\}.\n$$\nTask:\n1. Using only the definitions above, determine the empirical risk minimizer $s^{\\star}$ that minimizes $\\hat{R}(s)$ over $s \\in \\mathbb{R}$. If there is an interval of $s$ values that achieve the same minimal empirical risk, choose the midpoint of that interval as the canonical minimizer.\n2. In your reasoning (not in your final reported value), exhibit the tension between empirical risk and margin maximization by identifying the threshold $s_{\\mathrm{m}}$ that maximizes the geometric margin in one dimension, defined for $w = 1$ and $b = -s$ (so that the margin equals $\\min_{i} |x_{i} - s|$), and comparing $\\hat{R}(s^{\\star})$ and $\\hat{R}(s_{\\mathrm{m}})$.\n\nReport only the value of $s^{\\star}$ as your final answer in exact form; no rounding is required.",
            "solution": "The problem is subjected to validation.\n1.  **Givens Extraction**:\n    -   Binary classes with labels $y \\in \\{+1, -1\\}$.\n    -   Class-conditional distributions: $y=+1 \\implies x \\sim \\mathcal{N}(\\mu_{+}, \\sigma_{+}^{2})$; $y=-1 \\implies x \\sim \\mathcal{N}(\\mu_{-}, \\sigma_{-}^{2})$.\n    -   Dataset of $n=6$ samples:\n        -   $y=+1$: $x \\in \\{-0.8, 0.2, 0.3\\}$.\n        -   $y=-1$: $x \\in \\{-1.1, 0.5, 1.0\\}$.\n    -   Classifier family: $h_{s}(x) = \\operatorname{sign}(x-s)$, with the clarification that it predicts $+1$ if $x \\ge s$ and $-1$ otherwise.\n    -   Empirical risk (zero-one loss): $\\hat{R}(s) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}\\!\\left\\{ y_{i} \\ne h_{s}(x_{i}) \\right\\}$.\n    -   Task 1: Find the empirical risk minimizer $s^{\\star}$, choosing the midpoint if the minimum is achieved over an interval.\n    -   Task 2: In the reasoning, find the margin-maximizing threshold $s_{\\mathrm{m}}$ and compare $\\hat{R}(s^{\\star})$ with $\\hat{R}(s_{\\mathrm{m}})$.\n\n2.  **Validation**:\n    -   The problem is **scientifically grounded** in the established principles of statistical learning theory, specifically empirical risk minimization and margin theory.\n    -   It is **well-posed**, with all necessary data and definitions provided. The dataset is finite, the classifier family is explicit, and the objective function is clearly defined. The rule for selecting a unique minimizer (midpoint of an interval) ensures a unique solution exists.\n    -   The problem is **objective** and uses precise mathematical language.\n    -   It does not violate any of the invalidity criteria. It is a standard, formalizable exercise in machine learning fundamentals.\n\n3.  **Verdict**: The problem is valid and self-contained.\n\nWe proceed with the solution.\n\nThe goal is to find the threshold $s \\in \\mathbb{R}$ that minimizes the empirical risk $\\hat{R}(s)$. Since the sample size $n=6$ is a constant, minimizing $\\hat{R}(s)$ is equivalent to minimizing the number of misclassifications, $N_{err}(s) = n \\cdot \\hat{R}(s)$.\n\nThe classifier is defined as $h_s(x) = +1$ if $x \\ge s$ and $h_s(x) = -1$ if $x < s$.\nA point $(x_i, y_i)$ is misclassified if:\n-   $y_i = +1$ and $h_s(x_i) = -1$, which occurs when $x_i < s$.\n-   $y_i = -1$ and $h_s(x_i) = +1$, which occurs when $x_i \\ge s$.\n\nLet $X_{+} = \\{-0.8, 0.2, 0.3\\}$ be the set of points with label $y=+1$, and $X_{-} = \\{-1.1, 0.5, 1.0\\}$ be the set of points with label $y=-1$.\nThe number of misclassifications for a given threshold $s$ is:\n$$\nN_{err}(s) = \\sum_{x_i \\in X_{+}} \\mathbf{1}\\{x_i < s\\} + \\sum_{x_j \\in X_{-}} \\mathbf{1}\\{x_j \\ge s\\}\n$$\nThe value of $N_{err}(s)$ is a step function that can only change at values of $s$ corresponding to the data points $x_i$. We analyze the number of errors by considering intervals defined by the sorted data points.\nThe sorted data points are: $p_1 = -1.1\\;(y=-1)$, $p_2 = -0.8\\;(y=+1)$, $p_3 = 0.2\\;(y=+1)$, $p_4 = 0.3\\;(y=+1)$, $p_5 = 0.5\\;(y=-1)$, $p_6 = 1.0\\;(y=-1)$.\n\nWe evaluate $N_{err}(s)$ for different ranges of $s$:\n-   For $s \\le -1.1$: All data points $x_i$ satisfy $x_i \\ge s$, so $h_s(x_i) = +1$ for all $i$. The three points in $X_{-}$ are misclassified. The three points in $X_{+}$ are correctly classified. Thus, $N_{err}(s) = 3$.\n-   For $s \\in (-1.1, -0.8]$:\n    -   The point $x=-1.1 \\in X_{-}$ is correctly classified as $h_s(-1.1)=-1$ since $-1.1 < s$.\n    -   The points $x=\\{0.5, 1.0\\} \\in X_{-}$ are misclassified as $h_s(x)=+1$ since $x > s$.\n    -   All points $x \\in X_{+}$ satisfy $x > s$, so they are correctly classified as $h_s(x)=+1$.\n    -   The total number of misclassifications is $N_{err}(s) = 0 + 2 = 2$.\n    -   This holds for any $s$ in the half-open interval $(-1.1, -0.8]$. For $s=-0.8$, the point $x=-0.8 \\in X_+$ is correctly classified since $-0.8 \\ge s$. The errors remain the two points from $X_-$.\n-   For $s \\in (-0.8, 0.2]$:\n    -   The point $x=-0.8 \\in X_{+}$ is now misclassified as $h_s(-0.8)=-1$ since $-0.8 < s$.\n    -   The points $x=\\{0.2, 0.3\\} \\in X_{+}$ are correctly classified.\n    -   The points $x=\\{0.5, 1.0\\} \\in X_{-}$ are misclassified. The point $x=-1.1 \\in X_{-}$ is correctly classified.\n    -   The total number of misclassifications is $N_{err}(s) = 1 + 2 = 3$.\n-   For $s \\in (0.2, 0.3]$: $N_{err}(s) = 2$ (from $X_+$) $+ 2$ (from $X_-$) $= 4$.\n-   For $s \\in (0.3, 0.5]$: $N_{err}(s) = 3$ (from $X_+$) $+ 2$ (from $X_-$) $= 5$.\n-   For $s \\in (0.5, 1.0]$: $N_{err}(s) = 3$ (from $X_+$) $+ 1$ (from $X_-$) $= 4$.\n-   For $s > 1.0$: $N_{err}(s) = 3$ (from $X_+$) $+ 0$ (from $X_-$) $= 3$.\n\nBy inspection, the minimum number of errors is $N_{err, min} = 2$. This minimum is achieved for any threshold $s$ in the interval $(-1.1, -0.8]$.\nThe problem requires selecting the midpoint of this interval as the canonical minimizer $s^{\\star}$.\n$$\ns^{\\star} = \\frac{-1.1 + (-0.8)}{2} = \\frac{-1.9}{2} = -0.95\n$$\nThe minimal empirical risk is $\\hat{R}(s^{\\star}) = \\frac{2}{6} = \\frac{1}{3}$.\n\nNext, we identify the margin-maximizing threshold $s_{\\mathrm{m}}$ to exhibit the tension between empirical risk and margin. The margin is defined as $\\min_{i} |x_i - s|$. To maximize this minimum distance, the threshold $s$ must be positioned at the midpoint of the largest gap between any two consecutive data points in the sorted list.\nThe sorted data points are $\\{-1.1, -0.8, 0.2, 0.3, 0.5, 1.0\\}$.\nThe gaps between consecutive points are:\n-   $-0.8 - (-1.1) = 0.3$\n-   $0.2 - (-0.8) = 1.0$\n-   $0.3 - 0.2 = 0.1$\n-   $0.5 - 0.3 = 0.2$\n-   $1.0 - 0.5 = 0.5$\n\nThe largest gap is $1.0$, which is between the points $x=-0.8$ and $x=0.2$. The margin-maximizing threshold $s_{\\mathrm{m}}$ is the midpoint of this interval:\n$$\ns_{\\mathrm{m}} = \\frac{-0.8 + 0.2}{2} = \\frac{-0.6}{2} = -0.3\n$$\nThe maximal margin is $\\frac{1.0}{2} = 0.5$.\n\nNow we compute the empirical risk for this margin-maximizing threshold $s_{\\mathrm{m}} = -0.3$:\n$N_{err}(s_{\\mathrm{m}}) = N_{err}(-0.3)$.\n-   Misclassified points from $X_{+}=\\{-0.8, 0.2, 0.3\\}$:\n    -   $x=-0.8 < -0.3 \\implies$ classified as $-1$. Misclassified.\n-   Misclassified points from $X_{-}=\\{-1.1, 0.5, 1.0\\}$:\n    -   $x=0.5 \\ge -0.3 \\implies$ classified as $+1$. Misclassified.\n    -   $x=1.0 \\ge -0.3 \\implies$ classified as $+1$. Misclassified.\nThe total number of misclassifications for $s_{\\mathrm{m}}$ is $1 + 2 = 3$. The empirical risk is $\\hat{R}(s_{\\mathrm{m}}) = \\frac{3}{6} = \\frac{1}{2}$.\n\nThe tension is now evident:\n-   The empirical risk minimizer, $s^{\\star} = -0.95$, achieves the lowest possible training error ($\\hat{R}(s^{\\star}) = \\frac{1}{3}$), but it defines a decision boundary that is very close to the data points $x=-1.1$ and $x=-0.8$. Its margin is $|-0.95 - (-1.1)| = 0.15$. A small margin can indicate overfitting and poor generalization.\n-   The margin-maximizing threshold, $s_{\\mathrm{m}} = -0.3$, yields a larger margin of $0.5$ by placing the boundary in the largest empty region of the feature space. This is a more robust solution, but it comes at the cost of a higher empirical risk on the training data ($\\hat{R}(s_{\\mathrm{m}}) = \\frac{1}{2}$).\n\nThis illustrates the fundamental trade-off in statistical learning between minimizing training error and maximizing the margin for better generalization, a core concept underlying Support Vector Machines. The solution purely minimizing empirical risk ($s^{\\star}$) is different from the one that maximizes the margin ($s_{\\mathrm{m}}$).\n\nThe final answer is the value of $s^{\\star}$ only.\n$s^{\\star} = -0.95 = -\\frac{95}{100} = -\\frac{19}{20}$.",
            "answer": "$$\n\\boxed{-\\frac{19}{20}}\n$$"
        },
        {
            "introduction": "The principle of minimizing empirical risk via gradient descent is the engine of modern deep learning, but this engine requires a smooth road to travel on. This hands-on exercise  explores the critical role of activation functions in creating a navigable loss landscape for the optimizer. By implementing and comparing a non-differentiable step function with its smooth approximations, you will gain a practical understanding of why differentiability is not just a mathematical convenience but a practical necessity for training neural networks.",
            "id": "3121425",
            "problem": "Consider a one-dimensional binary classification task in deep learning under Empirical Risk Minimization (ERM). The fundamental base to use is the definition of empirical risk, the squared-error loss, and batch gradient descent. Let the dataset be $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$ where $x_i \\in \\mathbb{R}$ and $y_i \\in \\{0,1\\}$. Define $n = 21$, $x_i$ as equally spaced points from $-2$ to $2$ with spacing $0.2$, that is $x_i \\in \\{-2.0, -1.8, -1.6, \\dots, 1.8, 2.0\\}$, and let $y_i = \\mathbb{1}[x_i \\ge 0.5]$ where $\\mathbb{1}[\\cdot]$ denotes the indicator function. Consider a single-neuron model $f(x) = a(w x + b)$ with scalar parameters $w \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$, and activation $a(\\cdot)$ chosen from:\n- the Heaviside step function $H(z)$ with $H(z) = 1$ if $z \\ge 0$ and $H(z) = 0$ otherwise,\n- Softplus $s(z) = \\log(1 + e^{z})$,\n- Exponential Linear Unit (ELU) $$e_{\\alpha}(z) = \\begin{cases} z & \\text{if } z > 0 \\\\ \\alpha(\\exp(z) - 1) & \\text{if } z \\le 0 \\end{cases}$$ with $\\alpha = 1$.\n\nThe empirical risk to minimize is the mean squared error $$\\hat{R}(f) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(f(x_i) - y_i\\right)^2$$. Starting from these definitions only, derive the parameter update rule for batch gradient descent based on $\\hat{R}(f)$ and the chosen activation’s mathematical properties. Analyze how the non-smoothness of $H(z)$ affects optimizer sensitivity and show how smoothing via $s(z)$ or $e_{\\alpha}(z)$ changes the empirical risk surface to make gradient-based optimization more effective.\n\nImplement a complete, runnable program that:\n- constructs $\\mathcal{D}$ exactly as specified,\n- performs batch gradient descent for a fixed number of iterations for the given activation choice, learning rate, and initialization,\n- returns the final empirical risk $\\hat{R}(f)$ after training for each test case.\n\nUse the following test suite of parameter tuples $(a,\\eta,w_0,b_0,T)$, where $a$ is the activation name, $\\eta$ is the learning rate, $w_0$ and $b_0$ are initial parameters, and $T$ is the number of iterations:\n1. $(\\text{step},\\,0.1,\\,0.1,\\,0.0,\\,200)$\n2. $(\\text{softplus},\\,0.1,\\,0.1,\\,0.0,\\,200)$\n3. $(\\text{elu},\\,0.1,\\,0.1,\\,0.0,\\,200)$\n4. $(\\text{softplus},\\,0.001,\\,0.1,\\,0.0,\\,2000)$\n5. $(\\text{softplus},\\,1.0,\\,0.1,\\,0.0,\\,50)$\n6. $(\\text{step},\\,0.1,\\,0.0,\\,0.0,\\,200)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry being the final empirical risk $\\hat{R}(f)$ for the corresponding test case, in decimal form. For example, the output format must be exactly $[r_1,r_2,r_3,r_4,r_5,r_6]$ where each $r_j$ is a float.",
            "solution": "The problem asks for an analysis and implementation of empirical risk minimization (ERM) for a single-neuron binary classifier on a specified one-dimensional dataset. We will first validate the problem, then derive the necessary mathematical formulations for batch gradient descent, analyze the role of different activation functions, and finally present the implementation.\n\nThe problem statement provides the following givens:\n- A dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$ with $n = 21$.\n- The inputs $x_i$ are equally spaced points from $-2$ to $2$ with a spacing of $0.2$.\n- The labels $y_i$ are defined by the indicator function $y_i = \\mathbb{1}[x_i \\ge 0.5]$.\n- A single-neuron model $f(x) = a(w x + b)$ with scalar parameters $w, b \\in \\mathbb{R}$.\n- Three choices for the activation function $a(\\cdot)$ are provided: the Heaviside step function $H(z)$, the Softplus function $s(z) = \\log(1 + e^{z})$, and the Exponential Linear Unit (ELU) $e_{\\alpha}(z)$ with $\\alpha=1$.\n- The empirical risk is the mean squared error (MSE): $\\hat{R}(f) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(f(x_i) - y_i\\right)^2$.\n- The optimization algorithm is batch gradient descent.\n- A test suite of parameters $(a, \\eta, w_0, b_0, T)$ is given for implementation.\n\nThe problem is scientifically grounded, well-posed, and objective. It is a standard exercise in machine learning that explores the fundamental requirement of differentiability for gradient-based optimization. The inclusion of the non-differentiable Heaviside function is a deliberate pedagogical choice to contrast its behavior with smooth activation functions like Softplus and ELU. The problem is self-contained and provides all necessary information for a unique solution. Therefore, the problem is deemed valid.\n\nWe proceed to derive the parameter update rules for batch gradient descent. The goal is to minimize the empirical risk $\\hat{R}(w, b)$ with respect to the parameters $w$ and $b$.\nLet $z_i = w x_i + b$ be the pre-activation for the $i$-th data point. The empirical risk is:\n$$\n\\hat{R}(w, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(a(z_i) - y_i\\right)^2\n$$\nTo apply gradient descent, we need the partial derivatives of $\\hat{R}$ with respect to $w$ and $b$. Using the chain rule:\n$$\n\\frac{\\partial \\hat{R}}{\\partial w} = \\frac{\\partial}{\\partial w} \\left[ \\frac{1}{n} \\sum_{i=1}^{n} \\left(a(z_i) - y_i\\right)^2 \\right] = \\frac{1}{n} \\sum_{i=1}^{n} 2 \\left(a(z_i) - y_i\\right) \\frac{\\partial a(z_i)}{\\partial w}\n$$\nThe derivative of the activation term is $\\frac{\\partial a(z_i)}{\\partial w} = \\frac{d a}{d z_i} \\frac{\\partial z_i}{\\partial w} = a'(z_i) \\cdot x_i$. Substituting this in, we get:\n$$\n\\frac{\\partial \\hat{R}}{\\partial w} = \\frac{2}{n} \\sum_{i=1}^{n} \\left(a(z_i) - y_i\\right) a'(z_i) x_i\n$$\nSimilarly, for the parameter $b$, we have $\\frac{\\partial z_i}{\\partial b} = 1$. The partial derivative is:\n$$\n\\frac{\\partial \\hat{R}}{\\partial b} = \\frac{\\partial}{\\partial b} \\left[ \\frac{1}{n} \\sum_{i=1}^{n} \\left(a(z_i) - y_i\\right)^2 \\right] = \\frac{2}{n} \\sum_{i=1}^{n} \\left(a(z_i) - y_i\\right) a'(z_i)\n$$\nThe batch gradient descent update rules for an iteration $t$ with learning rate $\\eta$ are:\n$$\nw_{t+1} = w_t - \\eta \\frac{\\partial \\hat{R}}{\\partial w} \\bigg|_{w_t, b_t}\n$$\n$$\nb_{t+1} = b_t - \\eta \\frac{\\partial \\hat{R}}{\\partial b} \\bigg|_{w_t, b_t}\n$$\nThe viability of this process depends critically on the derivative of the activation function, $a'(z)$. We now analyze each case.\n\n1.  **Heaviside Step Function**: $H(z) = \\mathbb{1}[z \\ge 0]$.\n    The derivative of the Heaviside function is the Dirac delta function $\\delta(z)$, which is $0$ for all $z \\ne 0$ and undefined at $z=0$. In any practical implementation, points are unlikely to fall exactly at $z=0$. Even if they do, the gradient is undefined. A common practice is to use a subgradient, and the most straightforward choice for the derivative at all points is $H'(z) = 0$. With this, the gradients $\\frac{\\partial \\hat{R}}{\\partial w}$ and $\\frac{\\partial \\hat{R}}{\\partial b}$ become zero, irrespective of the error $(a(z_i) - y_i)$.\n    **Analysis**: The empirical risk surface $\\hat{R}(w, b)$ becomes a piecewise constant function, composed of flat plateaus. The gradient is zero everywhere on these plateaus. Consequently, gradient descent makes no progress; the parameters $w$ and $b$ remain fixed at their initial values. This illustrates a fundamental limitation of gradient-based methods: they are ineffective on non-smooth surfaces with zero gradients almost everywhere.\n\n2.  **Softplus**: $s(z) = \\log(1 + e^z)$.\n    The derivative is $s'(z) = \\frac{e^z}{1 + e^z}$, which is the logistic sigmoid function $\\sigma(z)$.\n    **Analysis**: The derivative $s'(z)$ is well-defined, continuous, and strictly positive for all $z \\in \\mathbb{R}$. This ensures that the empirical risk surface $\\hat{R}(w, b)$ is smooth (infinitely differentiable). The gradients with respect to $w$ and $b$ are well-defined and generally non-zero, providing a path for the optimizer to descend towards a minimum. The Softplus function effectively \"smoothes out\" the hard threshold of the step function, making the loss landscape amenable to gradient-based optimization.\n\n3.  **ELU with $\\alpha=1$**: $e_1(z) = \\begin{cases} z & \\text{if } z > 0 \\\\ e^z - 1 & \\text{if } z \\le 0 \\end{cases}$.\n    The derivative is $e'_1(z) = \\begin{cases} 1 & \\text{if } z > 0 \\\\ e^z & \\text{if } z \\le 0 \\end{cases}$.\n    At $z=0$, the left-hand limit of the derivative is $\\lim_{z \\to 0^-} e^z = 1$ and the right-hand limit is $1$. Thus, the function is continuously differentiable (C1 smooth) everywhere, with $e'_1(0) = 1$.\n    **Analysis**: Similar to Softplus, ELU provides a smooth risk surface. Its derivative is continuous and non-zero (except for $z \\to -\\infty$), allowing gradient descent to function effectively. The constant gradient of $1$ for $z > 0$ can prevent gradients from vanishing, a property useful in deeper networks. Like Softplus, it serves as a smooth alternative to discontinuous activation functions, enabling successful optimization.\n\nThe implementation will demonstrate these theoretical properties. For the Heaviside function, we expect the final risk to be identical to the initial risk. For Softplus and ELU, we expect the risk to decrease over the iterations.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates batch gradient descent for a single-neuron model\n    with different activation functions as specified in the problem.\n    \"\"\"\n\n    # --- 1. Dataset Construction ---\n    n = 21\n    x = np.linspace(-2.0, 2.0, n)\n    y = (x >= 0.5).astype(float)\n\n    # --- 2. Activation Functions and Their Derivatives ---\n    def heaviside(z):\n        return (z >= 0).astype(float)\n\n    def heaviside_prime(z):\n        # The derivative is 0 almost everywhere. We define the subgradient to be 0\n        # at z=0 for numerical stability, which reflects the failure of\n        # gradient descent with step functions.\n        return np.zeros_like(z)\n\n    def softplus(z):\n        # Clip to avoid overflow for large z in np.exp(z)\n        z = np.clip(z, -500, 500)\n        return np.log(1 + np.exp(z))\n\n    def softplus_prime(z):\n        # Sigmoid function. Clip to avoid overflow/underflow.\n        z = np.clip(z, -500, 500)\n        exp_z = np.exp(z)\n        return exp_z / (1 + exp_z)\n\n    def elu(z, alpha=1.0):\n        return np.where(z > 0, z, alpha * (np.exp(z) - 1))\n\n    def elu_prime(z, alpha=1.0):\n        return np.where(z > 0, 1.0, alpha * np.exp(z))\n\n\n    activations = {\n        'step': (heaviside, heaviside_prime),\n        'softplus': (softplus, softplus_prime),\n        'elu': (lambda z: elu(z, 1.0), lambda z: elu_prime(z, 1.0))\n    }\n\n    # --- 3. Test Cases ---\n    test_cases = [\n        ('step', 0.1, 0.1, 0.0, 200),\n        ('softplus', 0.1, 0.1, 0.0, 200),\n        ('elu', 0.1, 0.1, 0.0, 200),\n        ('softplus', 0.001, 0.1, 0.0, 2000),\n        ('softplus', 1.0, 0.1, 0.0, 50),\n        ('step', 0.1, 0.0, 0.0, 200)\n    ]\n\n    results = []\n\n    # --- 4. Main Loop for Batch Gradient Descent ---\n    for case in test_cases:\n        act_name, eta, w0, b0, T = case\n        w, b = w0, b0\n        \n        activation_func, activation_prime = activations[act_name]\n\n        for _ in range(T):\n            # Forward pass\n            z = w * x + b\n            f_x = activation_func(z)\n            \n            # Calculate error\n            error = f_x - y\n\n            # Calculate derivative of activation\n            a_prime = activation_prime(z)\n\n            # Calculate gradients for w and b\n            grad_w = (2 / n) * np.sum(error * a_prime * x)\n            grad_b = (2 / n) * np.sum(error * a_prime)\n\n            # Update parameters\n            w -= eta * grad_w\n            b -= eta * grad_b\n            \n        # After training, calculate the final empirical risk\n        final_predictions = activation_func(w * x + b)\n        final_risk = np.mean((final_predictions - y)**2)\n        results.append(final_risk)\n\n    # --- 5. Output Formatting ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "As we move to more advanced learning paradigms like semi-supervised learning, the straightforward application of ERM can lead to unexpected failures. This simulation  investigates a common pitfall in self-training known as confirmation bias, where a model reinforces its own errors by trusting its pseudo-labels too much. By building this scenario from scratch, you will not only witness how ERM can be misled, but also experiment with powerful mitigation strategies like confidence thresholding and entropy regularization that are crucial for building robust, real-world systems.",
            "id": "3121468",
            "problem": "You are given a binary classification setting in which Empirical Risk Minimization (ERM) is combined with self-training by pseudo-labeling. Your task is to implement a deterministic simulation that demonstrates how minimizing the empirical risk $\\hat{R}(f)$ over a dataset augmented by pseudo-labels can lead to confirmation bias, and to evaluate mitigation strategies using confidence thresholds and entropy penalties.\n\nFundamental base and definitions to be used:\n- Empirical Risk Minimization (ERM) minimizes the empirical risk $\\hat{R}(f)$ defined as the average of a pointwise loss $\\ell$ over samples. For binary classification with labels $y \\in \\{0,1\\}$ and a classifier $f$ producing probabilities $p = f(x) \\in [0,1]$, use cross-entropy loss $\\ell(f(x), y)$.\n- Logistic regression models the probability as $p = \\sigma(z)$ where $z = \\mathbf{w}^{\\top}\\mathbf{x} + b$, $\\mathbf{w} \\in \\mathbb{R}^{d}$ is the weight vector, $b \\in \\mathbb{R}$ is the bias, and $\\sigma(u) = \\frac{1}{1 + e^{-u}}$ is the sigmoid function.\n- Self-training with pseudo-labeling augments the labeled dataset by assigning labels to unlabeled samples according to current model predictions when those predictions are deemed sufficiently confident.\n- Confirmation bias arises when the model’s own biased predictions are reinforced by pseudo-labels, causing the empirical risk on the augmented training set to decrease while the true risk (evaluated on a separate test set with ground-truth labels) increases.\n\nData generation and simulation setup:\n- Generate a two-dimensional binary classification dataset with class-conditional Gaussian distributions. For class $0$, sample $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma})$ with $\\boldsymbol{\\mu}_0 = [-1, 0]$ and $\\boldsymbol{\\Sigma} = \\operatorname{diag}([1, 1])$. For class $1$, sample $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma})$ with $\\boldsymbol{\\mu}_1 = [1, 0]$ and the same covariance. Use a fixed random seed $42$ for reproducibility.\n- Construct three datasets: a labeled set $\\mathcal{D}_L$ of size $n_L = 20$ with an imbalanced composition ($18$ samples from class $0$ and $2$ samples from class $1$), an unlabeled pool $\\mathcal{D}_U$ of size $n_U = 1000$ drawn with equal class proportions but labels hidden from the learner, and a test set $\\mathcal{D}_T$ of size $n_T = 200$ drawn with equal class proportions and labels available for evaluation.\n- Train a logistic regression classifier by gradient descent on $\\mathcal{D}_L$ only, minimizing the average cross-entropy loss with an $\\ell_2$ regularizer on $\\mathbf{w}$ with coefficient $\\lambda = 0.01$. Use learning rate $\\eta = 0.1$ and $200$ epochs. Denote this initial classifier by $f_0$ with parameters $(\\mathbf{w}_0, b_0)$.\n\nSelf-training protocol per test case:\n- Given $f_0$, compute probabilities $p = f_0(\\mathbf{x})$ for all $\\mathbf{x} \\in \\mathcal{D}_U$. Select pseudo-labeled samples whose confidence exceeds a threshold $\\tau \\in (0.5, 1)$, i.e., those with $\\max(p, 1-p) \\ge \\tau$. Assign the pseudo-label $\\tilde{y} = \\mathbb{I}[p \\ge 0.5]$ for selected samples. Let $\\mathcal{D}_P \\subseteq \\mathcal{D}_U$ be this selected subset.\n- Form the augmented dataset $\\mathcal{D}_A = \\mathcal{D}_L \\cup \\mathcal{D}_P$, and train a classifier $f_1$ starting from $(\\mathbf{w}_0, b_0)$ for $200$ additional epochs using the following objective:\n  - Minimize the sum of losses consisting of the average cross-entropy over $\\mathcal{D}_L$ plus a weighted average cross-entropy over $\\mathcal{D}_P$ with weight $\\alpha \\ge 0$.\n  - Add an entropy penalty on predictions over $\\mathcal{D}_P$ with coefficient $\\gamma \\ge 0$ defined as $-\\gamma \\cdot \\frac{1}{|\\mathcal{D}_P|} \\sum_{\\mathbf{x} \\in \\mathcal{D}_P} H(f(\\mathbf{x}))$, where $H(p) = -[p \\log p + (1-p)\\log(1-p)]$ is the entropy of a Bernoulli distribution; this term encourages higher entropy (less overconfident predictions).\n  - Maintain $\\ell_2$ regularization on $\\mathbf{w}$ with coefficient $\\lambda$.\n- Compute the empirical training risk before self-training as $\\hat{R}_{\\text{before}} = \\frac{1}{|\\mathcal{D}_L|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{D}_L} \\ell(f_0(\\mathbf{x}), y)$, and after self-training as $\\hat{R}_{\\text{after}} = \\frac{1}{|\\mathcal{D}_A|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{D}_A} \\ell(f_1(\\mathbf{x}), y)$ where for $\\mathcal{D}_P$ the labels are the pseudo-labels $\\tilde{y}$.\n- Compute the test risk before and after self-training as $R^{\\text{test}}_{\\text{before}} = \\frac{1}{|\\mathcal{D}_T|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{D}_T} \\ell(f_0(\\mathbf{x}), y)$ and $R^{\\text{test}}_{\\text{after}} = \\frac{1}{|\\mathcal{D}_T|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{D}_T} \\ell(f_1(\\mathbf{x}), y)$ using the true labels for $\\mathcal{D}_T$.\n- Define a boolean indicator of confirmation bias for the test case as $\\text{CB} = [\\hat{R}_{\\text{after}} < \\hat{R}_{\\text{before}}] \\wedge [R^{\\text{test}}_{\\text{after}} > R^{\\text{test}}_{\\text{before}}]$.\n\nTest suite:\nFor each of the following parameter triples $(\\tau, \\alpha, \\gamma)$, run the self-training protocol and compute $\\text{CB}$:\n1. $(\\tau = 0.5, \\alpha = 3.0, \\gamma = 0.0)$: a permissive threshold with strong pseudo-label influence, expected to promote confirmation bias.\n2. $(\\tau = 0.9, \\alpha = 1.0, \\gamma = 0.0)$: a stricter threshold with moderate influence, expected to mitigate confirmation bias.\n3. $(\\tau = 0.95, \\alpha = 1.0, \\gamma = 0.2)$: a high threshold with an entropy penalty encouraging less overconfidence, expected to mitigate confirmation bias.\n4. $(\\tau = 0.7, \\alpha = 5.0, \\gamma = 0.0)$: a moderate threshold with very strong pseudo-label influence, testing edge-case amplification of bias.\n5. $(\\tau = 0.999, \\alpha = 1.0, \\gamma = 0.1)$: a near-degenerate threshold that selects almost no pseudo-labels, testing the boundary case of no augmentation.\n\nAnswer specification:\n- Your program must implement the above deterministic simulation and produce, for the ordered test suite $(1)$ through $(5)$, a single line of output containing the list of booleans $\\text{CB}$ as a comma-separated list enclosed in square brackets, for example, $[\\text{True},\\text{False},\\dots]$.\n- No physical units or angle units are involved. All reported values are booleans.",
            "solution": "The user has requested a deterministic simulation of self-training with pseudo-labeling in a binary classification context to demonstrate and analyze confirmation bias. The problem is scientifically grounded, well-posed, and all necessary parameters and definitions are provided for a complete implementation.\n\nThe solution proceeds by first implementing the specified data generation protocol, followed by the development of a logistic regression model capable of training via gradient descent. The training process is divided into two stages as per the problem description: an initial training phase on a small, imbalanced labeled dataset, and a second self-training phase on an augmented dataset. The augmented dataset is constructed by combining the initial labeled data with new data points from an unlabeled pool, which are assigned \"pseudo-labels\" based on the predictions of the initial model.\n\nThe core of the simulation lies in the objective function for the second training phase. This objective is a composite function designed to evaluate different mitigation strategies for confirmation bias. It includes:\n1.  The standard cross-entropy loss on the original labeled data $\\mathcal{D}_L$.\n2.  A weighted cross-entropy loss on the pseudo-labeled data $\\mathcal{D}_P$, controlled by a parameter $\\alpha$. A high $\\alpha$ forces the model to heavily trust its own (potentially biased) pseudo-labels.\n3.  An entropy penalty on the model's predictions for the pseudo-labeled data, controlled by a parameter $\\gamma$. This penalty encourages the model to be less confident in its predictions on $\\mathcal{D}_P$, acting as a regularizer against overfitting to the pseudo-labels.\n4.  A standard $\\ell_2$ regularization on the model weights $\\mathbf{w}$, controlled by $\\lambda$.\n\nThe gradient of this composite objective function with respect to the model parameters $(\\mathbf{w}, b)$ must be derived and implemented for the gradient descent optimization. Let $p_i = \\sigma(\\mathbf{w}^\\top\\mathbf{x}_i + b)$ be the model's predicted probability for a sample $\\mathbf{x}_i$.\n\nThe total objective function $J(\\mathbf{w}, b)$ to be minimized is:\n$$J(\\mathbf{w}, b) = \\frac{1}{|\\mathcal{D}_L|} \\sum_{i \\in \\mathcal{D}_L} \\ell(p_i, y_i) + \\frac{\\alpha}{|\\mathcal{D}_P|} \\sum_{j \\in \\mathcal{D}_P} \\ell(p_j, \\tilde{y}_j) - \\frac{\\gamma}{|\\mathcal{D}_P|} \\sum_{j \\in \\mathcal{D}_P} H(p_j) + \\lambda \\|\\mathbf{w}\\|_2^2$$\nwhere $\\ell(p, y)$ is the cross-entropy loss, $\\tilde{y}_j$ is the pseudo-label, and $H(p)$ is the Shannon entropy.\n\nThe gradient of each component with respect to the logit $z_i = \\mathbf{w}^\\top\\mathbf{x}_i + b$ is:\n-   Cross-entropy: $\\frac{\\partial \\ell}{\\partial z_i} = p_i - y_i$.\n-   Entropy: $\\frac{\\partial H}{\\partial z_j} = -z_j p_j(1-p_j)$.\n-   $\\ell_2$ regularization on weights: $\\nabla_{\\mathbf{w}}(\\lambda \\|\\mathbf{w}\\|^2_2) = 2\\lambda\\mathbf{w}$.\n\nUsing the chain rule, the full gradients for a batch update are computed by summing the contributions from each component, weighted appropriately by $\\alpha$ and $\\gamma$, and scaled by dataset sizes.\n\nFor each test case defined by a $(\\tau, \\alpha, \\gamma)$ triple, the simulation will:\n1.  Train an initial model $f_0$ on the imbalanced labeled set $\\mathcal{D}_L$.\n2.  Calculate the initial empirical risk $\\hat{R}_{\\text{before}}$ on $\\mathcal{D}_L$ and test risk $R^{\\text{test}}_{\\text{before}}$ on a hold-out test set $\\mathcal{D}_T$.\n3.  Use $f_0$ to generate pseudo-labels for unlabeled data points in $\\mathcal{D}_U$ whose prediction confidence $\\max(p, 1-p)$ exceeds the threshold $\\tau$. This forms the set $\\mathcal{D}_P$.\n4.  Train a new model $f_1$, starting from the parameters of $f_0$, on the augmented dataset $\\mathcal{D}_A = \\mathcal{D}_L \\cup \\mathcal{D}_P$ using the composite objective function.\n5.  Calculate the final empirical risk $\\hat{R}_{\\text{after}}$ (on $\\mathcal{D}_A$) and test risk $R^{\\text{test}}_{\\text{after}}$ (on $\\mathcal{D}_T$).\n6.  Determine if confirmation bias occurred by checking if $\\hat{R}_{\\text{after}} < \\hat{R}_{\\text{before}}$ and $R^{\\text{test}}_{\\text{after}} > R^{\\text{test}}_{\\text{before}}$.\n\nThe simulation is deterministic due to a fixed random seed for data generation and a fixed protocol for training. An edge case where no pseudo-labels are generated (i.e., $|\\mathcal{D}_P| = 0$) is handled by ensuring the corresponding loss terms and gradients become zero. Numerical stability of the logarithm in the cross-entropy and entropy calculations is ensured by clipping probabilities to a small, safe range $[\\epsilon, 1-\\epsilon]$.\n\nThe final output will be a list of boolean values, one for each test case, indicating whether confirmation bias was observed.",
            "answer": "```python\nimport numpy as np\n\nclass LogisticRegression:\n    \"\"\"\n    A Logistic Regression classifier trained with gradient descent.\n    \"\"\"\n    def __init__(self, lr=0.1, epochs=200, lambda_reg=0.01, random_state=None):\n        self.lr = lr\n        self.epochs = epochs\n        self.lambda_reg = lambda_reg\n        self.w = None\n        self.b = None\n        self.epsilon = 1e-9  # For numerical stability\n\n    def _sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def _loss(self, y, p):\n        p_clipped = np.clip(p, self.epsilon, 1 - self.epsilon)\n        # Cross-entropy loss\n        ce_loss = - (y * np.log(p_clipped) + (1 - y) * np.log(1 - p_clipped))\n        return np.mean(ce_loss)\n\n    def _entropy(self, p):\n        p_clipped = np.clip(p, self.epsilon, 1 - self.epsilon)\n        return - (p_clipped * np.log(p_clipped) + (1 - p_clipped) * np.log(1 - p_clipped))\n\n    def predict_proba(self, X):\n        z = X @ self.w + self.b\n        return self._sigmoid(z)\n\n    def train(self, X_l, y_l, X_p=None, y_p=None, alpha=0.0, gamma=0.0):\n        \"\"\"\n        Trains the logistic regression model using batch gradient descent.\n        \"\"\"\n        n_samples, n_features = X_l.shape\n        if self.w is None:\n            self.w = np.zeros(n_features)\n        if self.b is None:\n            self.b = 0.0\n\n        for _ in range(self.epochs):\n            # === Gradients for labeled data D_L ===\n            z_l = X_l @ self.w + self.b\n            p_l = self._sigmoid(z_l)\n            n_l = len(y_l)\n            \n            dw_l = (1 / n_l) * X_l.T @ (p_l - y_l)\n            db_l = (1 / n_l) * np.sum(p_l - y_l)\n\n            # === Gradients for pseudo-labeled data D_P ===\n            dw_p, db_p = 0, 0\n            dw_h, db_h = 0, 0 # Entropy penalty gradients\n\n            n_p = len(X_p) if X_p is not None else 0\n            if n_p > 0:\n                # Cross-entropy on D_p\n                z_p = X_p @ self.w + self.b\n                p_p = self._sigmoid(z_p)\n                dw_p = alpha * (1 / n_p) * X_p.T @ (p_p - y_p)\n                db_p = alpha * (1 / n_p) * np.sum(p_p - y_p)\n                \n                # Entropy penalty on D_p\n                if gamma > 0:\n                    # Gradient of the objective term -gamma*H(p) w.r.t z is gamma*z*p*(1-p)\n                    grad_H_obj_z = gamma * z_p * p_p * (1 - p_p)\n                    dw_h = (1 / n_p) * X_p.T @ grad_H_obj_z\n                    db_h = (1 / n_p) * np.sum(grad_H_obj_z)\n\n            # === Regularization Gradient ===\n            dw_reg = 2 * self.lambda_reg * self.w\n\n            # === Total Gradient and Update ===\n            dw = dw_l + dw_p + dw_h + dw_reg\n            db = db_l + db_p + db_h\n            \n            self.w -= self.lr * dw\n            self.b -= self.lr * db\n\ndef solve():\n    # --- Data Generation and Simulation Setup ---\n    random_seed = 42\n    np.random.seed(random_seed)\n\n    mu0, mu1 = np.array([-1, 0]), np.array([1, 0])\n    Sigma = np.diag([1, 1])\n    \n    # D_L: Labeled set (imbalanced)\n    X_L = np.vstack([\n        np.random.multivariate_normal(mu0, Sigma, 18),\n        np.random.multivariate_normal(mu1, Sigma, 2)\n    ])\n    y_L = np.array([0]*18 + [1]*2)\n    \n    # D_U: Unlabeled pool (balanced)\n    n_U = 1000\n    X_U = np.vstack([\n        np.random.multivariate_normal(mu0, Sigma, n_U // 2),\n        np.random.multivariate_normal(mu1, Sigma, n_U // 2)\n    ])\n    \n    # D_T: Test set (balanced)\n    n_T = 200\n    X_T = np.vstack([\n        np.random.multivariate_normal(mu0, Sigma, n_T // 2),\n        np.random.multivariate_normal(mu1, Sigma, n_T // 2)\n    ])\n    y_T = np.array([0]*(n_T // 2) + [1]*(n_T // 2))\n\n    # --- Initial Training (f0) ---\n    lambda_reg, eta, epochs = 0.01, 0.1, 200\n    f0 = LogisticRegression(lr=eta, epochs=epochs, lambda_reg=lambda_reg)\n    f0.train(X_L, y_L)\n    \n    # --- Evaluation Before Self-Training ---\n    p_L_before = f0.predict_proba(X_L)\n    R_hat_before = f0._loss(y_L, p_L_before)\n\n    p_T_before = f0.predict_proba(X_T)\n    R_test_before = f0._loss(y_T, p_T_before)\n    \n    w0, b0 = np.copy(f0.w), f0.b\n\n    # --- Test Suite ---\n    test_cases = [\n        (0.5, 3.0, 0.0),\n        (0.9, 1.0, 0.0),\n        (0.95, 1.0, 0.2),\n        (0.7, 5.0, 0.0),\n        (0.999, 1.0, 0.1),\n    ]\n\n    results = []\n    \n    for tau, alpha, gamma in test_cases:\n        # --- Self-Training Protocol ---\n        # 1. Generate pseudo-labels\n        p_U = f0.predict_proba(X_U)\n        confidence = np.maximum(p_U, 1 - p_U)\n        \n        pseudo_indices = np.where(confidence >= tau)[0]\n        \n        X_p = X_U[pseudo_indices]\n        p_p_selected = p_U[pseudo_indices]\n        y_p = (p_p_selected >= 0.5).astype(int)\n        \n        # 2. Train f1 on augmented data\n        f1 = LogisticRegression(lr=eta, epochs=epochs, lambda_reg=lambda_reg)\n        f1.w, f1.b = np.copy(w0), b0  # Start from f0's parameters\n        \n        f1.train(X_L, y_L, X_p, y_p, alpha, gamma)\n        \n        # 3. Evaluation After Self-Training\n        # R_hat_after\n        X_A = np.vstack([X_L, X_p]) if len(X_p) > 0 else X_L\n        y_A = np.concatenate([y_L, y_p]) if len(y_p) > 0 else y_L\n        p_A_after = f1.predict_proba(X_A)\n        R_hat_after = f1._loss(y_A, p_A_after)\n        \n        # R_test_after\n        p_T_after = f1.predict_proba(X_T)\n        R_test_after = f1._loss(y_T, p_T_after)\n        \n        # 4. Check for Confirmation Bias\n        confirmation_bias = (R_hat_after < R_hat_before) and (R_test_after > R_test_before)\n        results.append(confirmation_bias)\n\n    # --- Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}