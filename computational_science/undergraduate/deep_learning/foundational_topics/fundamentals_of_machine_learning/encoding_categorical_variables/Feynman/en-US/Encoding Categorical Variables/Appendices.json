{
    "hands_on_practices": [
        {
            "introduction": "Before diving into complex neural network architectures, it's essential to grasp the foundational principles that govern how models interpret data. This first exercise provides a critical lesson from classical statistics on the perils of naively encoding ordered categorical variables. By analytically demonstrating how an arbitrary numerical mapping can distort a model's findings , you will build a strong intuition for why carefully considered encoding strategies are fundamental to sound modeling and a necessary precursor to learned embeddings.",
            "id": "3164649",
            "problem": "A laboratory study assesses the effect of an ordered categorical predictor on a continuous response. There are $4$ ordered treatment categories $C \\in \\{C_1,C_2,C_3,C_4\\}$ administered in a balanced design with $n$ observations per category. Although the categories are labeled qualitatively, domain knowledge provides an underlying quantitative spacing $s(C)$ for each category, measured on a meaningful scale: $s(C_1)=0$, $s(C_2)=1$, $s(C_3)=4$, $s(C_4)=5$. The true conditional expectation of the response is linear in this spacing:\n$$\n\\mathbb{E}[Y \\mid C] = \\alpha + \\beta\\, s(C),\n$$\nwhere $\\alpha$ and $\\beta$ are unknown constants, and the observational noise has mean $0$ and constant variance within each category.\n\nAn analyst considers two encodings of the categorical predictor to fit a simple linear regression with an intercept. In the first encoding, they map levels to equally spaced numeric codes $z(C) \\in \\{0,1,2,3\\}$ according to the order $C_1,C_2,C_3,C_4$, and regress $Y$ on $z(C)$ with an intercept. In the second encoding, they construct a single contrast predictor $x(C)$ intended to reflect the domain-specific spacing, and regress $Y$ on $x(C)$ with an intercept.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. When the spacing $s(C)$ is unequal, regressing $Y$ on the equally spaced code $z(C)$ produces a slope that differs from $\\beta$. In this setup with $s(C) \\in \\{0,1,4,5\\}$ and balanced categories, the expected slope from the $Y$ on $z(C)$ regression equals $1.8\\,\\beta$, and the fitted category means differ from the true means by $(-0.2\\,\\beta,\\,0.6\\,\\beta,\\,-0.6\\,\\beta,\\,0.2\\,\\beta)$ for $(C_1,C_2,C_3,C_4)$, respectively.\n\nB. Mapping the ordered categories to equally spaced numeric codes $z(C) \\in \\{0,1,2,3\\}$ yields an unbiased estimate of $\\beta$ regardless of the true spacing $s(C)$.\n\nC. Using one-hot indicator (dummy) variables with an intercept, but no additional structure, automatically yields a single scalar “trend” coefficient equal to $\\beta$ whenever the categories are ordered.\n\nD. A defensible way to encode the domain-specific spacing is to set $x(C)=s(C)-\\bar{s}$, where $\\bar{s}$ is the average of $s(C)$ across the categories, and regress $Y$ on $x(C)$ with an intercept. Under the stated true model, this produces an unbiased estimate of $\\beta$, and the intercept estimates the grand mean $\\mathbb{E}[Y]$.",
            "solution": "The problem statement has been validated and is sound. It presents a clear, well-posed scenario in statistical modeling concerning the encoding of an ordered categorical predictor. All provided information is internally consistent and scientifically grounded.\n\nLet us define the key quantities.\nThe number of ordered categories is $k=4$, labeled $C_1, C_2, C_3, C_4$.\nThe design is balanced, with $n$ observations per category, so the total number of observations is $N=4n$.\nThe true quantitative spacing for the categories is given by the function $s(C)$, with values $s_1 = s(C_1) = 0$, $s_2 = s(C_2) = 1$, $s_3 = s(C_3) = 4$, and $s_4 = s(C_4) = 5$.\nThe true model for the conditional expectation of the response $Y$ is given by:\n$$\n\\mathbb{E}[Y \\mid C_i] = \\mu_i = \\alpha + \\beta s_i\n$$\nThe true mean responses for the four categories are:\n$\\mu_1 = \\alpha + \\beta s_1 = \\alpha + \\beta(0) = \\alpha$\n$\\mu_2 = \\alpha + \\beta s_2 = \\alpha + \\beta(1) = \\alpha + \\beta$\n$\\mu_3 = \\alpha + \\beta s_3 = \\alpha + \\beta(4) = \\alpha + 4\\beta$\n$\\mu_4 = \\alpha + \\beta s_4 = \\alpha + \\beta(5) = \\alpha + 5\\beta$\n\nThe analyst considers two models. The first uses equally spaced codes $z(C)$ with values $z_1=0, z_2=1, z_3=2, z_4=3$. The second uses a predictor $x(C)$ based on the true spacing $s(C)$.\n\n### Analysis of the First Encoding (Regression of $Y$ on $z(C)$)\n\nThe first model is a simple linear regression of $Y$ on $z(C)$: $Y = a_z + b_z z + \\text{error}$. The ordinary least squares (OLS) estimator for the slope is $\\hat{b}_z$. We are interested in its expected value, $\\mathbb{E}[\\hat{b}_z]$.\nFor a balanced design like this one, the expected value of the slope estimator is the slope of the line fitted to the true means $(\\mu_i)$ versus the predictor values $(z_i)$.\n$$\n\\mathbb{E}[\\hat{b}_z] = \\frac{\\sum_{i=1}^4 (z_i - \\bar{z})(\\mu_i - \\bar{\\mu})}{\\sum_{i=1}^4 (z_i - \\bar{z})^2} = \\frac{\\sum_{i=1}^4 (z_i - \\bar{z})\\mu_i}{\\sum_{i=1}^4 (z_i - \\bar{z})^2}\n$$\nThe values of $z_i$ are $\\{0, 1, 2, 3\\}$. The mean is $\\bar{z} = \\frac{0+1+2+3}{4} = 1.5$.\nThe denominator is the sum of squared deviations for $z$:\n$S_{zz} = \\sum_{i=1}^4 (z_i - \\bar{z})^2 = (0-1.5)^2 + (1-1.5)^2 + (2-1.5)^2 + (3-1.5)^2 = 2.25 + 0.25 + 0.25 + 2.25 = 5$.\nThe numerator is the sum of cross-products, $\\sum (z_i - \\bar{z})\\mu_i$:\n$S_{z\\mu} = (0-1.5)\\mu_1 + (1-1.5)\\mu_2 + (2-1.5)\\mu_3 + (3-1.5)\\mu_4$\n$S_{z\\mu} = -1.5\\mu_1 - 0.5\\mu_2 + 0.5\\mu_3 + 1.5\\mu_4$\nSubstitute the true means $\\mu_i$:\n$S_{z\\mu} = -1.5\\alpha - 0.5(\\alpha+\\beta) + 0.5(\\alpha+4\\beta) + 1.5(\\alpha+5\\beta)$\n$S_{z\\mu} = \\alpha(-1.5 - 0.5 + 0.5 + 1.5) + \\beta(-0.5(1) + 0.5(4) + 1.5(5))$\n$S_{z\\mu} = \\alpha(0) + \\beta(-0.5 + 2 + 7.5) = 9\\beta$.\nSo, the expected slope is $\\mathbb{E}[\\hat{b}_z] = \\frac{9\\beta}{5} = 1.8\\beta$.\n\nNext, we find the expected fitted means for each category, $\\mathbb{E}[\\hat{\\mu}_i] = \\mathbb{E}[\\hat{a}_z + \\hat{b}_z z_i]$.\nFirst, we need the expected intercept $\\mathbb{E}[\\hat{a}_z] = \\mathbb{E}[\\bar{Y} - \\hat{b}_z \\bar{z}] = \\mathbb{E}[\\bar{Y}] - \\mathbb{E}[\\hat{b}_z]\\bar{z}$.\nThe expected sample mean is the mean of the true category means:\n$\\mathbb{E}[\\bar{Y}] = \\bar{\\mu} = \\frac{1}{4}(\\mu_1+\\mu_2+\\mu_3+\\mu_4) = \\frac{1}{4}(\\alpha + (\\alpha+\\beta) + (\\alpha+4\\beta) + (\\alpha+5\\beta)) = \\frac{1}{4}(4\\alpha + 10\\beta) = \\alpha + 2.5\\beta$.\n$\\mathbb{E}[\\hat{a}_z] = (\\alpha + 2.5\\beta) - (1.8\\beta)(1.5) = \\alpha + 2.5\\beta - 2.7\\beta = \\alpha - 0.2\\beta$.\nNow we compute the expected fitted mean for each category and its difference from the true mean:\n$\\mathbb{E}[\\hat{\\mu}_1] = \\mathbb{E}[\\hat{a}_z] + \\mathbb{E}[\\hat{b}_z] z_1 = (\\alpha - 0.2\\beta) + (1.8\\beta)(0) = \\alpha - 0.2\\beta$.\nDifference: $(\\alpha - 0.2\\beta) - \\mu_1 = (\\alpha - 0.2\\beta) - \\alpha = -0.2\\beta$.\n$\\mathbb{E}[\\hat{\\mu}_2] = \\mathbb{E}[\\hat{a}_z] + \\mathbb{E}[\\hat{b}_z] z_2 = (\\alpha - 0.2\\beta) + (1.8\\beta)(1) = \\alpha + 1.6\\beta$.\nDifference: $(\\alpha + 1.6\\beta) - \\mu_2 = (\\alpha + 1.6\\beta) - (\\alpha+\\beta) = 0.6\\beta$.\n$\\mathbb{E}[\\hat{\\mu}_3] = \\mathbb{E}[\\hat{a}_z] + \\mathbb{E}[\\hat{b}_z] z_3 = (\\alpha - 0.2\\beta) + (1.8\\beta)(2) = \\alpha + 3.4\\beta$.\nDifference: $(\\alpha + 3.4\\beta) - \\mu_3 = (\\alpha + 3.4\\beta) - (\\alpha+4\\beta) = -0.6\\beta$.\n$\\mathbb{E}[\\hat{\\mu}_4] = \\mathbb{E}[\\hat{a}_z] + \\mathbb{E}[\\hat{b}_z] z_4 = (\\alpha - 0.2\\beta) + (1.8\\beta)(3) = \\alpha + 5.2\\beta$.\nDifference: $(\\alpha + 5.2\\beta) - \\mu_4 = (\\alpha + 5.2\\beta) - (\\alpha+5\\beta) = 0.2\\beta$.\n\n### Evaluation of Options\n\n**A. When the spacing $s(C)$ is unequal, regressing $Y$ on the equally spaced code $z(C)$ produces a slope that differs from $\\beta$. In this setup with $s(C) \\in \\{0,1,4,5\\}$ and balanced categories, the expected slope from the $Y$ on $z(C)$ regression equals $1.8\\,\\beta$, and the fitted category means differ from the true means by $(-0.2\\,\\beta,\\,0.6\\,\\beta,\\,-0.6\\,\\beta,\\,0.2\\,\\beta)$ for $(C_1,C_2,C_3,C_4)$, respectively.**\nOur derivation shows that the expected slope is indeed $1.8\\beta$. The differences between the expected fitted category means and the true means are precisely $(-0.2\\beta, 0.6\\beta, -0.6\\beta, 0.2\\beta)$. These calculations confirm every part of the statement.\nVerdict: **Correct**.\n\n**B. Mapping the ordered categories to equally spaced numeric codes $z(C) \\in \\{0,1,2,3\\}$ yields an unbiased estimate of $\\beta$ regardless of the true spacing $s(C)$.**\nAn estimator is unbiased if its expected value equals the true parameter. Here, we are estimating $\\beta$ with the slope coefficient from the regression on $z(C)$, which we call $\\hat{b}_z$. From our analysis for option A, we found that $\\mathbb{E}[\\hat{b}_z] = 1.8\\beta$. Since $1.8\\beta \\neq \\beta$ (for any non-zero $\\beta$), the estimator is biased. This specific case is a counterexample to the general claim. The estimator would only be unbiased if the true spacing $s(C)$ happened to be a perfect linear function of the codes $z(C)$ with a slope of $1$, which is not true here and is certainly not true \"regardless of the true spacing\".\nVerdict: **Incorrect**.\n\n**C. Using one-hot indicator (dummy) variables with an intercept, but no additional structure, automatically yields a single scalar “trend” coefficient equal to $\\beta$ whenever the categories are ordered.**\nOne-hot (or dummy) encoding for a $k$-level categorical predictor with an intercept involves creating $k-1$ binary predictors. For this problem with $k=4$, the model would be, using $C_1$ as the reference category:\n$Y = \\gamma_0 + \\gamma_2 D_2 + \\gamma_3 D_3 + \\gamma_4 D_4 + \\text{error}$\nwhere $D_i=1$ if the category is $C_i$ and $0$ otherwise.\nThe parameters in this model correspond to the category means:\n$\\mathbb{E}[\\hat{\\gamma}_0] = \\mu_1 = \\alpha$\n$\\mathbb{E}[\\hat{\\gamma}_2] = \\mu_2 - \\mu_1 = (\\alpha+\\beta) - \\alpha = \\beta$\n$\\mathbb{E}[\\hat{\\gamma}_3] = \\mu_3 - \\mu_1 = (\\alpha+4\\beta) - \\alpha = 4\\beta$\n$\\mathbb{E}[\\hat{\\gamma}_4] = \\mu_4 - \\mu_1 = (\\alpha+5\\beta) - \\alpha = 5\\beta$\nThis model yields three coefficients ($\\gamma_2, \\gamma_3, \\gamma_4$), not a \"single scalar 'trend' coefficient\". These coefficients represent differences relative to the baseline, not a single trend across all categories. While one of them happens to have an expectation of $\\beta$ in this particular case (where $s_2-s_1 = 1$), the others do not, and the model itself does not estimate a single trend parameter. This modeling approach is equivalent to ANOVA and treats the categories as nominal, ignoring any ordering.\nVerdict: **Incorrect**.\n\n**D. A defensible way to encode the domain-specific spacing is to set $x(C)=s(C)-\\bar{s}$, where $\\bar{s}$ is the average of $s(C)$ across the categories, and regress $Y$ on $x(C)$ with an intercept. Under the stated true model, this produces an unbiased estimate of $\\beta$, and the intercept estimates the grand mean $\\mathbb{E}[Y]$.**\nLet's analyze this proposal. The new predictor is $x(C) = s(C) - \\bar{s}$. From the given spacings $\\{0, 1, 4, 5\\}$, the average is $\\bar{s} = \\frac{0+1+4+5}{4} = 2.5$.\nThe true model is $\\mathbb{E}[Y \\mid C] = \\alpha + \\beta s(C)$. We can re-express this in terms of $x(C)$. Since $s(C) = x(C) + \\bar{s}$, we have:\n$\\mathbb{E}[Y \\mid C] = \\alpha + \\beta (x(C) + \\bar{s}) = (\\alpha + \\beta\\bar{s}) + \\beta x(C)$.\nThis shows that the true conditional expectation of $Y$ is perfectly linear in the proposed predictor $x(C)$. The model is $Y = a_x + b_x x(C) + \\text{error}$. Because the model is correctly specified, the OLS estimators for the slope and intercept are unbiased.\nThe true slope is $\\beta$, so the OLS estimator $\\hat{b}_x$ will have $\\mathbb{E}[\\hat{b}_x] = \\beta$. The first part of the statement is correct.\nThe true intercept is $\\alpha + \\beta\\bar{s}$. The OLS estimator $\\hat{a}_x$ will have $\\mathbb{E}[\\hat{a}_x] = \\alpha + \\beta\\bar{s}$.\nThe statement claims the intercept estimates the \"grand mean $\\mathbb{E}[Y]$\". For a balanced design, the grand mean is the average of the individual category means:\n$\\mathbb{E}[Y] = \\frac{1}{4} \\sum_{i=1}^4 \\mu_i = \\frac{1}{4} \\sum_{i=1}^4 (\\alpha + \\beta s_i) = \\alpha + \\beta (\\frac{1}{4}\\sum_{i=1}^4 s_i) = \\alpha + \\beta\\bar{s}$.\nThis matches the expected value of the intercept estimator, $\\mathbb{E}[\\hat{a}_x]$. This is a general property of simple linear regression: when the predictor is centered (i.e., its sample mean is $0$), the intercept is an unbiased estimator of the mean of the response variable. The predictor $x(C)=s(C)-\\bar{s}$ is centered by construction for a balanced design. Thus, the second part of the statement is also correct.\nVerdict: **Correct**.",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "Standard one-hot encoding treats every category as a distinct, isolated entity, ignoring any underlying relationships between them. This hands-on coding practice explores a powerful feature engineering technique for data with inherent cyclical patterns, such as seasons, months of the year, or hours of the day. By implementing Fourier feature encoding and comparing its generalization performance against a one-hot model , you will discover how injecting prior knowledge about the data's structure can create more robust and predictive representations.",
            "id": "3121725",
            "problem": "You are given a cyclic categorical variable representing seasons with period $12$, labeled by indices $c \\in \\{0,1,2,\\dots,11\\}$. Consider two encodings of a category $c$: (i) one-hot encoding $e(c) \\in \\mathbb{R}^{12}$ with a single entry equal to $1$ at index $c$ and $0$ elsewhere, and (ii) Fourier feature encoding $\\phi(c) \\in \\mathbb{R}^{1+2|M|}$ constructed from a set of harmonics $M = \\{1,2\\}$ as $\\phi(c) = [1,\\sin(2\\pi m c / 12),\\cos(2\\pi m c / 12)]_{m \\in M}$, where all angles are in radians. You will fit a linear model $f(x) = w^\\top x$ by minimizing Mean Squared Error (MSE), and you will compute the Ordinary Least Squares (OLS) solution using the Moore–Penrose pseudoinverse. The goal is to test whether periodic Fourier features benefit models in the presence of cyclical categorical variables.\n\nFundamental base definitions to use:\n- Mean Squared Error (MSE) for a model $f$ on a dataset $\\{(x_i,y_i)\\}_{i=1}^n$ is $\\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i))^2$.\n- Ordinary Least Squares (OLS) chooses $w$ to minimize the MSE, with the minimal-norm solution given by $w = X^+ y$, where $X^+$ is the Moore–Penrose pseudoinverse of the design matrix $X$ and $y$ is the vector of targets.\n\nConstruct a toy setting where the target is a deterministic function of the category index $c$ with no noise. For each test case, you will:\n1. Generate training and test sets by enumerating categories as specified.\n2. Encode inputs using both one-hot $e(c)$ and Fourier features $\\phi(c)$ with harmonics $M = \\{1,2\\}$.\n3. Fit linear models using OLS on the training set for both encodings.\n4. Evaluate test MSE for both models on the specified test categories.\n5. Output a boolean indicating whether the Fourier feature model achieves strictly lower test MSE than the one-hot model.\n\nAngle unit specification: all angles in trigonometric functions must be expressed in radians.\n\nTest suite:\n- Case $1$ (happy path periodic): target $y(c) = \\sin\\!\\left(\\frac{2\\pi c}{12}\\right)$; training categories $S_{\\text{train}} = \\{0,2,4,6,8,10\\}$; test categories $S_{\\text{test}} = \\{1,3,5,7,9,11\\}$.\n- Case $2$ (adversarial periodic not in chosen harmonics): target $y(c) = \\cos(\\pi c)$, equivalently $y(c) = (-1)^c$ corresponding to harmonic $m=6$ for period $12$; training categories $S_{\\text{train}} = \\{0,2,4,6,8,10\\}$; test categories $S_{\\text{test}} = \\{1,3,5,7,9,11\\}$.\n- Case $3$ (mixture of harmonics covered by features, with missing categories): target $y(c) = \\sin\\!\\left(\\frac{2\\pi c}{12}\\right) + \\frac{1}{2}\\cos\\!\\left(\\frac{4\\pi c}{12}\\right)$; training categories $S_{\\text{train}} = \\{0,1,2,4,5,6,8,9,10\\}$; test categories $S_{\\text{test}} = \\{3,7,11\\}$.\n- Case $4$ (boundary with full category coverage): target $y(c) = \\sin\\!\\left(\\frac{2\\pi c}{12}\\right) + \\frac{1}{2}\\cos\\!\\left(\\frac{4\\pi c}{12}\\right)$; training categories $S_{\\text{train}} = \\{0,1,2,3,4,5,6,7,8,9,10,11\\}$; test categories $S_{\\text{test}} = \\{0,1,2,3,4,5,6,7,8,9,10,11\\}$.\n\nModel and evaluation details:\n- For each case, form the training design matrices $X_{\\text{one-hot}}$ and $X_{\\text{fourier}}$ by stacking encodings of the training categories, and the target vector $y_{\\text{train}}$ by applying the specified $y(c)$ to each training category.\n- Compute $w_{\\text{one-hot}} = X_{\\text{one-hot}}^+ y_{\\text{train}}$ and $w_{\\text{fourier}} = X_{\\text{fourier}}^+ y_{\\text{train}}$.\n- Evaluate predictions on the test categories using the corresponding encodings to form $X_{\\text{test}}$ and compute the test MSE for each model.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a boolean corresponding to the cases in the order $1$ through $4$ and is $True$ if the Fourier feature model has strictly lower test MSE than the one-hot model and $False$ otherwise. For example, the output should look like $[True,False,True,False]$.",
            "solution": "The problem requires a comparative analysis of two encoding schemes for a cyclic categorical variable: one-hot encoding and Fourier feature encoding. The performance is evaluated based on the Mean Squared Error (MSE) of an Ordinary Least Squares (OLS) linear model on a test set. The problem is valid as it is scientifically grounded in linear algebra and machine learning principles, is well-posed with all necessary information provided, and is objective in its evaluation criteria.\n\nThe core of the analysis lies in understanding the implicit assumptions (inductive biases) of each encoding method. The one-hot encoding creates a feature space where each category is represented by an orthogonal basis vector. A linear model in this space, $f(e(c)) = w^\\top e(c) = w_c$, assigns a separate weight $w_c$ to each category $c$. This allows the model to represent any arbitrary function on the set of categories, but it offers no mechanism for generalization to unseen categories. In contrast, the Fourier feature encoding represents each category $c$ by a vector $\\phi(c)$ of values from a set of sinusoidal basis functions. A linear model $f(\\phi(c)) = w^\\top \\phi(c)$ is constrained to represent the target function as a linear combination of these basis functions. This provides a strong inductive bias, assuming the target function is periodic and smooth, which facilitates generalization but limits the model's expressiveness to the span of the chosen harmonics.\n\nThe problem specifies a period of $P=12$ and a set of harmonics $M = \\{1,2\\}$. The Fourier feature vector for a category $c \\in \\{0, 1, \\dots, 11\\}$ is given by $\\phi(c) \\in \\mathbb{R}^{5}$:\n$$\n\\phi(c) = \\left[1, \\sin\\left(\\frac{2\\pi \\cdot 1 \\cdot c}{12}\\right), \\cos\\left(\\frac{2\\pi \\cdot 1 \\cdot c}{12}\\right), \\sin\\left(\\frac{2\\pi \\cdot 2 \\cdot c}{12}\\right), \\cos\\left(\\frac{2\\pi \\cdot 2 \\cdot c}{12}\\right)\\right]^\\top\n$$\nThe one-hot encoding $e(c) \\in \\mathbb{R}^{12}$ is a vector with $1$ at index $c$ and $0$ elsewhere.\n\nFor each case, we construct training design matrices $X_{\\text{one-hot}}$ and $X_{\\text{fourier}}$ by stacking the row vectors $e(c)^\\top$ and $\\phi(c)^\\top$ for all $c$ in the training set $S_{\\text{train}}$. The target vector $y_{\\text{train}}$ is formed by applying the given target function to each $c \\in S_{\\text{train}}$. The OLS weight vectors are found using the Moore-Penrose pseudoinverse, $w = X^+ y_{\\text{train}}$. This provides the unique minimum-norm solution that minimizes the sum of squared errors. We then form test matrices $X_{\\text{test}}$ for the test set $S_{\\text{test}}$, compute predictions $y_{\\text{pred}} = X_{\\text{test}} w$, and calculate the test MSE: $\\text{MSE} = \\frac{1}{|S_{\\text{test}}|} \\sum_{i \\in S_{\\text{test}}} (y_i - y_{\\text{pred},i})^2$.\n\n**Case 1: Periodic Target, Incomplete Data**\n- Target: $y(c) = \\sin(\\frac{2\\pi c}{12})$. This function is one of the basis functions in the Fourier encoding (corresponding to harmonic $m=1$).\n- Training: $S_{\\text{train}} = \\{0,2,4,6,8,10\\}$. Test: $S_{\\text{test}} = \\{1,3,5,7,9,11\\}$.\n- **Fourier Model**: Since the target function lies entirely within the span of the Fourier features, the model can perfectly represent it. The training data is sufficiently diverse for the OLS to identify the correct weights, primarily a weight of $1$ for the $\\sin(\\frac{2\\pi c}{12})$ feature and near-zero for others. This learned function generalizes perfectly to the unseen test categories. Thus, the test MSE will be approximately $0$.\n- **One-Hot Model**: The model is trained on even-indexed categories. For any training category $c_{\\text{train}} \\in S_{\\text{train}}$, the weight $w_{c_{\\text{train}}}$ learns the target value $y(c_{\\text{train}})$. For unseen categories $c_{\\text{test}} \\in S_{\\text{test}}$, the corresponding columns in the training design matrix are all zeros. The minimum-norm property of the pseudoinverse solution sets the weights for these unseen categories, $w_{c_{\\text{test}}}$, to $0$. Consequently, the model predicts $f(e(c_{\\text{test}})) = 0$ for all test points. The true test targets $y(c_{\\text{test}})$ are non-zero, resulting in a significant test MSE.\n- **Verdict**: $\\text{MSE}_{\\text{fourier}} \\approx 0 < \\text{MSE}_{\\text{one-hot}}$. The result is **True**.\n\n**Case 2: Mismatched Periodic Target**\n- Target: $y(c) = \\cos(\\pi c) = \\cos(\\frac{2\\pi \\cdot 6 \\cdot c}{12})$. This function corresponds to harmonic $m=6$, which is not in the model's feature set $M=\\{1,2\\}$.\n- Training/Test sets are the same as in Case 1.\n- For the training set of even categories, the target is $y(c) = \\cos(\\pi c) = (-1)^c = 1$ for all $c \\in S_{\\text{train}}$.\n- **Fourier Model**: The training target vector is a constant vector of ones. This vector is identical to the first column of the Fourier design matrix (the bias term). The OLS solution will therefore be $w = [1,0,0,0,0]^\\top$, resulting in a model that predicts $f(c)=1$ for all inputs. For the test set (odd categories), the true target is $y(c) = -1$. The model's prediction of $1$ leads to an error of $(-1 - 1)^2 = 4$ for each test sample. Thus, $\\text{MSE}_{\\text{fourier}} = 4$.\n- **One-Hot Model**: As in Case 1, the model predicts $0$ for all unseen test categories. The true test target is $-1$. The error for each test sample is $(-1 - 0)^2 = 1$. Thus, $\\text{MSE}_{\\text{one-hot}} = 1$.\n- **Verdict**: $\\text{MSE}_{\\text{fourier}} = 4$, $\\text{MSE}_{\\text{one-hot}} = 1$. The condition $4 < 1$ is false. The result is **False**.\n\n**Case 3: Mixed Harmonic Target, Incomplete Data**\n- Target: $y(c) = \\sin(\\frac{2\\pi c}{12}) + \\frac{1}{2}\\cos(\\frac{4\\pi c}{12})$. This function is a linear combination of basis functions for $m=1$ and $m=2$, which are both included in the Fourier feature set.\n- Training: $S_{\\text{train}} = \\{0,1,2,4,5,6,8,9,10\\}$. Test: $S_{\\text{test}} = \\{3,7,11\\}$.\n- **Fourier Model**: The reasoning is identical to Case 1. The model's inductive bias is perfectly aligned with the target function. With $9$ training samples for $5$ parameters, the OLS fit will accurately recover the underlying function, which then generalizes perfectly to the test set. The test MSE will be approximately $0$.\n- **One-Hot Model**: The model is not trained on categories $\\{3,7,11\\}$. It will predict $0$ for these test points. The true targets are non-zero, leading to a positive test MSE.\n- **Verdict**: $\\text{MSE}_{\\text{fourier}} \\approx 0 < \\text{MSE}_{\\text{one-hot}}$. The result is **True**.\n\n**Case 4: Full Data Coverage**\n- Target: Same as Case 3.\n- Training and test sets are identical and complete: $S_{\\text{train}} = S_{\\text{test}} = \\{0, 1, \\dots, 11\\}$.\n- **One-Hot Model**: With all $12$ categories present in the training set, the design matrix $X_{\\text{one-hot}}$ is the $12 \\times 12$ identity matrix, $I_{12}$. Its pseudoinverse is also $I_{12}$. The weights are $w_{\\text{one-hot}} = I_{12} y_{\\text{train}} = y_{\\text{train}}$, meaning $w_c = y(c)$ for each category. The model perfectly memorizes the training data. Since the test set is identical to the training set, the predictions are perfect, and $\\text{MSE}_{\\text{one-hot}} = 0$.\n- **Fourier Model**: As in Case 3, the target function is in the span of the features. With the full dataset, the OLS fit is guaranteed to find the exact weights to reproduce the function perfectly. The model will also achieve zero error on the training/test set. Thus, $\\text{MSE}_{\\text{fourier}} = 0$.\n- **Verdict**: Since $\\text{MSE}_{\\text{fourier}} = 0$ and $\\text{MSE}_{\\text{one-hot}} = 0$, the strict inequality $\\text{MSE}_{\\text{fourier}} < \\text{MSE}_{\\text{one-hot}}$ (i.e., $0 < 0$) is false. The result is **False**.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def solve_case(target_func, s_train, s_test, P, M):\n        \"\"\"\n        Solves a single test case for the problem.\n\n        Args:\n            target_func (callable): The function generating the target values y(c).\n            s_train (list): The list of training categories.\n            s_test (list): The list of test categories.\n            P (int): The period of the cyclic variable.\n            M (list): The set of harmonics for Fourier features.\n        \n        Returns:\n            bool: True if the Fourier model's MSE is strictly less than the one-hot model's MSE.\n        \"\"\"\n        # Convert category lists to numpy arrays for vectorized operations\n        c_train = np.array(s_train)\n        c_test = np.array(s_test)\n        \n        # 1. Generate target values for training and test sets\n        y_train = target_func(c_train)\n        y_test = target_func(c_test)\n\n        # 2. Encode inputs for both training and test sets\n        \n        # One-hot encoding\n        num_classes = P\n        X_train_onehot = np.zeros((len(c_train), num_classes))\n        # Use advanced indexing to set the '1's in the one-hot vectors\n        X_train_onehot[np.arange(len(c_train)), c_train] = 1\n        \n        X_test_onehot = np.zeros((len(c_test), num_classes))\n        X_test_onehot[np.arange(len(c_test)), c_test] = 1\n\n        # Fourier feature encoding\n        num_features_fourier = 1 + 2 * len(M)\n        X_train_fourier = np.ones((len(c_train), num_features_fourier))\n        X_test_fourier = np.ones((len(c_test), num_features_fourier))\n        \n        feature_idx = 1\n        for m in M:\n            # Training set features\n            angle_train = 2.0 * np.pi * m * c_train / P\n            X_train_fourier[:, feature_idx] = np.sin(angle_train)\n            X_train_fourier[:, feature_idx + 1] = np.cos(angle_train)\n            \n            # Test set features\n            angle_test = 2.0 * np.pi * m * c_test / P\n            X_test_fourier[:, feature_idx] = np.sin(angle_test)\n            X_test_fourier[:, feature_idx + 1] = np.cos(angle_test)\n            \n            feature_idx += 2\n\n        # 3. Fit linear models using OLS with the Moore-Penrose pseudoinverse\n        w_onehot = np.linalg.pinv(X_train_onehot) @ y_train\n        w_fourier = np.linalg.pinv(X_train_fourier) @ y_train\n        \n        # 4. Evaluate test MSE for both models\n        \n        # Predictions\n        y_pred_onehot = X_test_onehot @ w_onehot\n        y_pred_fourier = X_test_fourier @ w_fourier\n        \n        # Mean Squared Error calculation\n        mse_onehot = np.mean((y_test - y_pred_onehot)**2)\n        mse_fourier = np.mean((y_test - y_pred_fourier)**2)\n        \n        # 5. Return boolean indicating if Fourier MSE is strictly lower\n        return mse_fourier < mse_onehot\n\n    # Define the test cases from the problem statement.\n    P = 12\n    M = [1, 2]\n    \n    test_cases = [\n        # Case 1\n        {'target_func': lambda c: np.sin(2.0 * np.pi * c / P),\n         's_train': [0, 2, 4, 6, 8, 10],\n         's_test': [1, 3, 5, 7, 9, 11]},\n        # Case 2\n        {'target_func': lambda c: np.cos(np.pi * c),\n         's_train': [0, 2, 4, 6, 8, 10],\n         's_test': [1, 3, 5, 7, 9, 11]},\n        # Case 3\n        {'target_func': lambda c: np.sin(2.0 * np.pi * c / P) + 0.5 * np.cos(4.0 * np.pi * c / P),\n         's_train': [0, 1, 2, 4, 5, 6, 8, 9, 10],\n         's_test': [3, 7, 11]},\n        # Case 4\n        {'target_func': lambda c: np.sin(2.0 * np.pi * c / P) + 0.5 * np.cos(4.0 * np.pi * c / P),\n         's_train': list(range(P)),\n         's_test': list(range(P))},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(case['target_func'], case['s_train'], case['s_test'], P, M)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Modern deep learning often requires bridging the gap between discrete categorical choices and the continuous world of gradient-based optimization. This advanced exercise introduces the concept of temperature-controlled \"soft\" embeddings, where a category is represented as a differentiable mixture of several base codes. By implementing this model and analyzing how the gradient and curvature of the loss landscape change with the temperature parameter $\\tau$ , you will gain valuable insight into the mechanics of techniques like the Gumbel-Softmax and the optimization trade-offs between smooth, exploratory landscapes and sharp, decisive ones.",
            "id": "3121698",
            "problem": "Consider a categorical variable with $M$ base codes, where each base code is a vector $\\mathbf{b}_i \\in \\mathbb{R}^d$ for $i \\in \\{1, \\dots, M\\}$, assembled into a matrix $B \\in \\mathbb{R}^{M \\times d}$ whose $i$-th row is $\\mathbf{b}_i^\\top$. A category $c$ is parameterized by a logits vector $\\mathbf{z}_c \\in \\mathbb{R}^M$ and represented as a temperature-controlled soft mixture over the base codes with weights $\\boldsymbol{\\pi}(\\tau) \\in \\mathbb{R}^M$ given by the Softmax function\n$$\n\\pi_i(\\tau) = \\frac{\\exp\\left(z_{c,i} / \\tau\\right)}{\\sum_{j=1}^M \\exp\\left(z_{c,j} / \\tau\\right)}, \\quad i \\in \\{1,\\dots,M\\},\n$$\nwhere $\\tau \\in \\mathbb{R}_{>0}$ is the temperature. The embedding of category $c$ at temperature $\\tau$ is defined as\n$$\n\\mathbf{e}_c(\\tau) = \\sum_{i=1}^M \\pi_i(\\tau)\\, \\mathbf{b}_i = B^\\top \\boldsymbol{\\pi}(\\tau).\n$$\nA scalar predictor is given by parameters $\\mathbf{w} \\in \\mathbb{R}^d$ and $b_0 \\in \\mathbb{R}$, producing\n$$\n\\hat{y}(\\tau) = \\mathbf{w}^\\top \\mathbf{e}_c(\\tau) + b_0 = \\mathbf{w}^\\top B^\\top \\boldsymbol{\\pi}(\\tau) + b_0.\n$$\nFor a scalar target $y \\in \\mathbb{R}$, consider the squared error loss\n$$\nL(\\tau) = \\frac{1}{2}\\left(\\hat{y}(\\tau) - y\\right)^2.\n$$\nStarting from fundamental calculus (chain rule) and the definition of the Softmax, encode the categorical variable via temperature-controlled soft assignments and analyze the optimization landscape with respect to the logits $\\mathbf{z}_c$ by computing two quantities for each test case:\n- The Euclidean norm of the gradient $\\left\\lVert \\nabla_{\\mathbf{z}_c} L(\\tau) \\right\\rVert_2$.\n- The spectral norm of the Hessian $\\left\\lVert \\nabla^2_{\\mathbf{z}_c} L(\\tau) \\right\\rVert_2$, defined as the largest singular value, which for a symmetric matrix equals the maximum absolute eigenvalue.\n\nYour program must implement the described model, compute these two quantities exactly using analytical derivatives of the Softmax where applicable, and aggregate the results across the following test suite. Use the fixed base code matrix $B$, predictor parameters $\\mathbf{w}$ and $b_0$, and target $y$ given below, and vary the logits $\\mathbf{z}_c$ and temperature $\\tau$ per test case. All vectors are row vectors unless transposed.\n\nUse:\n- $M = 4$, $d = 3$,\n- $B = \\begin{bmatrix}\n0.9 & -0.4 & 0.1 \\\\\n0.3 & 0.8 & -0.5 \\\\\n-0.6 & 0.2 & 0.7 \\\\\n0.5 & -0.1 & -0.3\n\\end{bmatrix}$,\n- $\\mathbf{w} = \\left[0.7, -1.1, 0.9\\right]$,\n- $b_0 = 0.2$,\n- $y = 0.5$.\n\nTest suite:\n1. Happy path: $\\mathbf{z}_c = \\left[1.2, -0.5, 0.3, 2.0\\right]$, $\\tau = 1.0$.\n2. Smooth regime: $\\mathbf{z}_c = \\left[1.2, -0.5, 0.3, 2.0\\right]$, $\\tau = 2.0$.\n3. Transition regime: $\\mathbf{z}_c = \\left[1.2, -0.5, 0.3, 2.0\\right]$, $\\tau = 0.5$.\n4. Near-hard regime: $\\mathbf{z}_c = \\left[1.2, -0.5, 0.3, 2.0\\right]$, $\\tau = 0.1$.\n5. Edge case (equal logits): $\\mathbf{z}_c = \\left[0.0, 0.0, 0.0, 0.0\\right]$, $\\tau = 1.0$.\n6. Edge case (equal logits, very low temperature): $\\mathbf{z}_c = \\left[0.0, 0.0, 0.0, 0.0\\right]$, $\\tau = 0.01$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a two-element list $\\left[\\left\\lVert \\nabla_{\\mathbf{z}_c} L(\\tau) \\right\\rVert_2, \\left\\lVert \\nabla^2_{\\mathbf{z}_c} L(\\tau) \\right\\rVert_2\\right]$. For example, an output with three test cases should look like $\\left[[g_1,h_1],[g_2,h_2],[g_3,h_3]\\right]$, with all entries expressed as real numbers without units. This task is relevant to understanding how temperature affects the smooth-to-hard transition in categorical encodings and its implications for methods such as Stochastic Gradient Descent (SGD).",
            "solution": "The problem requires the computation of the Euclidean norm of the gradient and the spectral norm of the Hessian of a squared error loss function with respect to a vector of logits, $\\mathbf{z}_c$. The model uses a temperature-controlled soft mixture of base codes to form a categorical embedding.\n\n### Step 1: Define Variables and the Loss Function\n\nLet the given parameters be:\n-   Number of base codes: $M \\in \\mathbb{N}$\n-   Dimension of base codes: $d \\in \\mathbb{N}$\n-   Base code matrix: $B \\in \\mathbb{R}^{M \\times d}$, with rows $\\mathbf{b}_i^\\top$\n-   Predictor weights: $\\mathbf{w} \\in \\mathbb{R}^d$\n-   Predictor bias: $b_0 \\in \\mathbb{R}$\n-   Target value: $y \\in \\mathbb{R}$\n-   Logits vector for category $c$: $\\mathbf{z}_c \\in \\mathbb{R}^M$\n-   Temperature: $\\tau \\in \\mathbb{R}_{>0}$\n\nThe soft mixture weights $\\boldsymbol{\\pi}(\\tau) \\in \\mathbb{R}^M$ are given by the Softmax function applied to the scaled logits $\\mathbf{z}_c/\\tau$:\n$$\n\\pi_i(\\tau) = \\frac{\\exp\\left(z_{c,i} / \\tau\\right)}{\\sum_{j=1}^M \\exp\\left(z_{c,j} / \\tau\\right)}\n$$\nThe category embedding $\\mathbf{e}_c(\\tau) \\in \\mathbb{R}^d$ is a weighted sum of the base codes:\n$$\n\\mathbf{e}_c(\\tau) = \\sum_{i=1}^M \\pi_i(\\tau)\\, \\mathbf{b}_i = B^\\top \\boldsymbol{\\pi}(\\tau)\n$$\nThe scalar prediction $\\hat{y}(\\tau)$ is:\n$$\n\\hat{y}(\\tau) = \\mathbf{w}^\\top \\mathbf{e}_c(\\tau) + b_0 = \\mathbf{w}^\\top B^\\top \\boldsymbol{\\pi}(\\tau) + b_0\n$$\nLet's define a vector $\\mathbf{v} = B\\mathbf{w} \\in \\mathbb{R}^M$. The components of this vector are $v_i = \\mathbf{b}_i^\\top \\mathbf{w}$. We can rewrite the prediction as:\n$$\n\\hat{y}(\\tau) = (B\\mathbf{w})^\\top \\boldsymbol{\\pi}(\\tau) + b_0 = \\mathbf{v}^\\top \\boldsymbol{\\pi}(\\tau) + b_0\n$$\nThe squared error loss is:\n$$\nL(\\tau) = \\frac{1}{2}\\left(\\hat{y}(\\tau) - y\\right)^2\n$$\n\n### Step 2: Compute the Gradient $\\nabla_{\\mathbf{z}_c} L(\\tau)$\n\nWe use the chain rule to find the gradient of $L$ with respect to $\\mathbf{z}_c$. Let's denote $\\mathbf{z}_c$ as $\\mathbf{z}$ for brevity.\n$$\n\\nabla_{\\mathbf{z}} L = \\frac{\\partial L}{\\partial \\hat{y}} \\nabla_{\\mathbf{z}} \\hat{y}\n$$\nThe first term is the derivative of the loss with respect to the prediction:\n$$\n\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y}(\\tau) - y\n$$\nThe second term is the gradient of the prediction with respect to the logits:\n$$\n\\nabla_{\\mathbf{z}} \\hat{y} = \\nabla_{\\mathbf{z}} (\\mathbf{v}^\\top \\boldsymbol{\\pi} + b_0) = (\\nabla_{\\mathbf{z}} \\boldsymbol{\\pi})^\\top \\mathbf{v}\n$$\nThe Jacobian of the Softmax function $\\boldsymbol{\\pi}$ with respect to its input $\\mathbf{u}=\\mathbf{z}/\\tau$ is a matrix $J_{\\boldsymbol{\\pi},\\mathbf{u}}$ with entries $(J_{\\boldsymbol{\\pi},\\mathbf{u}})_{ik} = \\frac{\\partial \\pi_i}{\\partial u_k} = \\pi_i(\\delta_{ik} - \\pi_k)$. This can be written as $J_{\\boldsymbol{\\pi},\\mathbf{u}} = \\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top$.\nBy the chain rule, the Jacobian of $\\boldsymbol{\\pi}$ with respect to $\\mathbf{z}$ is:\n$$\nJ_{\\boldsymbol{\\pi},\\mathbf{z}} = \\frac{\\partial \\boldsymbol{\\pi}}{\\partial \\mathbf{z}} = \\frac{\\partial \\boldsymbol{\\pi}}{\\partial \\mathbf{u}} \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{z}} = \\left(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top\\right) \\frac{1}{\\tau} I = \\frac{1}{\\tau} \\left(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top\\right)\n$$\nSince this Jacobian matrix is symmetric, $(\\nabla_{\\mathbf{z}} \\boldsymbol{\\pi})^\\top = J_{\\boldsymbol{\\pi},\\mathbf{z}}$. Thus,\n$$\n\\nabla_{\\mathbf{z}} \\hat{y} = J_{\\boldsymbol{\\pi},\\mathbf{z}} \\mathbf{v} = \\frac{1}{\\tau} \\left(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top\\right) \\mathbf{v}\n$$\nThe $k$-th component of this gradient is:\n$$\n(\\nabla_{\\mathbf{z}} \\hat{y})_k = \\frac{1}{\\tau} \\left( \\pi_k v_k - \\pi_k \\sum_j \\pi_j v_j \\right) = \\frac{1}{\\tau} \\pi_k(v_k - \\mathbf{v}^\\top \\boldsymbol{\\pi})\n$$\nCombining the terms, the gradient of the loss is:\n$$\n\\mathbf{g} = \\nabla_{\\mathbf{z}} L = (\\hat{y} - y) \\nabla_{\\mathbf{z}} \\hat{y} = \\frac{\\hat{y} - y}{\\tau} \\left(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top\\right) \\mathbf{v}\n$$\nThe quantity to compute is the Euclidean norm of this vector, $\\left\\lVert \\mathbf{g} \\right\\rVert_2$.\n\n### Step 3: Compute the Hessian $\\nabla^2_{\\mathbf{z}_c} L(\\tau)$\n\nThe Hessian matrix $H_L = \\nabla^2_{\\mathbf{z}} L$ is the gradient of $\\nabla_{\\mathbf{z}} L$. Using the product rule for vector calculus:\n$$\nH_L = \\nabla_{\\mathbf{z}} \\left( (\\hat{y} - y) \\nabla_{\\mathbf{z}} \\hat{y} \\right) = (\\nabla_{\\mathbf{z}} \\hat{y})(\\nabla_{\\mathbf{z}} \\hat{y})^\\top + (\\hat{y} - y) \\nabla^2_{\\mathbf{z}} \\hat{y}\n$$\nLet $\\mathbf{g}_{\\hat{y}} = \\nabla_{\\mathbf{z}} \\hat{y}$. The first term is the outer product $\\mathbf{g}_{\\hat{y}}\\mathbf{g}_{\\hat{y}}^\\top$. The second term requires the Hessian of the prediction, $H_{\\hat{y}} = \\nabla^2_{\\mathbf{z}} \\hat{y}$.\nThe $(k,l)$-th entry of $H_{\\hat{y}}$ is $\\frac{\\partial^2 \\hat{y}}{\\partial z_k \\partial z_l}$. Let's differentiate the $k$-th component of $\\mathbf{g}_{\\hat{y}}$ with respect to $z_l$:\n$$\n(H_{\\hat{y}})_{kl} = \\frac{\\partial}{\\partial z_l} \\left( \\frac{1}{\\tau} \\pi_k(v_k - \\mathbf{v}^\\top \\boldsymbol{\\pi}) \\right)\n$$\nUsing the product rule and chain rule again:\n$$\n(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau} \\left[ \\frac{\\partial \\pi_k}{\\partial z_l}(v_k - \\mathbf{v}^\\top \\boldsymbol{\\pi}) - \\pi_k \\frac{\\partial (\\mathbf{v}^\\top \\boldsymbol{\\pi})}{\\partial z_l} \\right]\n$$\nWe know $\\frac{\\partial \\pi_k}{\\partial z_l} = \\frac{1}{\\tau}\\pi_k(\\delta_{kl}-\\pi_l)$. The derivative of the expected value $\\bar{v} = \\mathbf{v}^\\top\\boldsymbol{\\pi}$ is:\n$$\n\\frac{\\partial (\\mathbf{v}^\\top \\boldsymbol{\\pi})}{\\partial z_l} = \\mathbf{v}^\\top \\frac{\\partial \\boldsymbol{\\pi}}{\\partial z_l} = \\sum_j v_j \\frac{\\partial \\pi_j}{\\partial z_l} = \\sum_j v_j \\frac{1}{\\tau} \\pi_j(\\delta_{jl}-\\pi_l) = \\frac{1}{\\tau} (v_l\\pi_l - \\pi_l \\sum_j v_j\\pi_j) = \\frac{1}{\\tau}\\pi_l(v_l - \\bar{v})\n$$\nSubstituting these into the expression for $(H_{\\hat{y}})_{kl}$:\n$$\n(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau} \\left[ \\frac{1}{\\tau}\\pi_k(\\delta_{kl}-\\pi_l)(v_k - \\bar{v}) - \\pi_k \\frac{1}{\\tau}\\pi_l(v_l - \\bar{v}) \\right]\n$$\nLet's define a difference vector $\\mathbf{d} = \\mathbf{v} - \\bar{v}\\mathbf{1}$, so $d_k = v_k - \\bar{v}$.\n$$\n(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau^2} \\left[ \\pi_k(\\delta_{kl}-\\pi_l) d_k - \\pi_k\\pi_l d_l \\right] = \\frac{1}{\\tau^2} \\left( \\pi_k d_k \\delta_{kl} - \\pi_k\\pi_l d_k - \\pi_k\\pi_l d_l \\right)\n$$\n$$\n(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau^2} \\left( \\pi_k d_k \\delta_{kl} - \\pi_k\\pi_l (d_k + d_l) \\right)\n$$\nThis matrix is symmetric, as expected. The full Hessian of the loss is:\n$$\nH_L = \\mathbf{g}_{\\hat{y}}\\mathbf{g}_{\\hat{y}}^\\top + (\\hat{y} - y) H_{\\hat{y}}\n$$\nThe spectral norm of a symmetric matrix is its largest absolute eigenvalue: $\\left\\lVert H_L \\right\\rVert_2 = \\max_i |\\lambda_i(H_L)|$.\n\n### Step 4: Algorithm for Implementation\nFor each test case $(\\mathbf{z}_c, \\tau)$:\n1.  Define constants $B, \\mathbf{w}, b_0, y$. Let $\\mathbf{z}_c$ and $\\tau$ be the inputs for the current case. Let all vectors be column vectors.\n2.  Compute $\\mathbf{v} = B \\mathbf{w}$.\n3.  Compute scaled logits $\\mathbf{p} = \\mathbf{z}_c / \\tau$.\n4.  Compute $\\boldsymbol{\\pi} = \\text{Softmax}(\\mathbf{p})$, using a numerically stable implementation (subtracting the maximum logit before exponentiation).\n5.  Compute the prediction $\\hat{y} = \\mathbf{v}^\\top \\boldsymbol{\\pi} + b_0$.\n6.  Compute the prediction error $\\delta_L = \\hat{y} - y$.\n7.  Compute the gradient of the prediction $\\mathbf{g}_{\\hat{y}} = \\frac{1}{\\tau}(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top) \\mathbf{v}$. A simpler way is to compute $\\bar{v} = \\mathbf{v}^\\top \\boldsymbol{\\pi}$ and then the components of $\\mathbf{g}_{\\hat{y}}$ as $(\\mathbf{g}_{\\hat{y}})_k = \\frac{1}{\\tau} \\pi_k (v_k - \\bar{v})$.\n8.  Compute the loss gradient $\\mathbf{g} = \\delta_L \\mathbf{g}_{\\hat{y}}$.\n9.  Calculate the gradient norm $\\left\\lVert \\mathbf{g} \\right\\rVert_2$.\n10. Compute the Hessian of the prediction $H_{\\hat{y}}$. First compute $\\bar{v}$ and $\\mathbf{d} = \\mathbf{v} - \\bar{v}\\mathbf{1}$. Then construct the $M \\times M$ matrix $H_{\\hat{y}}$ with elements $(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau^2} (\\pi_k d_k \\delta_{kl} - \\pi_k \\pi_l (d_k + d_l))$.\n11. Construct the loss Hessian $H_L = \\mathbf{g}_{\\hat{y}}\\mathbf{g}_{\\hat{y}}^\\top + \\delta_L H_{\\hat{y}}$.\n12. Compute the eigenvalues of the symmetric matrix $H_L$.\n13. Calculate the spectral norm of the Hessian as the maximum absolute eigenvalue.\n14. Store the gradient norm and Hessian norm for the current test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of computing the gradient and Hessian norms for a\n    temperature-controlled categorical encoding model.\n    \"\"\"\n\n    # Define fixed parameters\n    M = 4\n    d = 3\n    B = np.array([\n        [0.9, -0.4, 0.1],\n        [0.3, 0.8, -0.5],\n        [-0.6, 0.2, 0.7],\n        [0.5, -0.1, -0.3]\n    ])\n    w = np.array([[0.7], [-1.1], [0.9]])\n    b0 = 0.2\n    y = 0.5\n\n    # Define the test suite\n    test_cases = [\n        # (z_c, tau)\n        (np.array([1.2, -0.5, 0.3, 2.0]), 1.0),    # Happy path\n        (np.array([1.2, -0.5, 0.3, 2.0]), 2.0),    # Smooth regime\n        (np.array([1.2, -0.5, 0.3, 2.0]), 0.5),    # Transition regime\n        (np.array([1.2, -0.5, 0.3, 2.0]), 0.1),    # Near-hard regime\n        (np.array([0.0, 0.0, 0.0, 0.0]), 1.0),    # Edge case (equal logits)\n        (np.array([0.0, 0.0, 0.0, 0.0]), 0.01),   # Edge case (equal logits, low temp)\n    ]\n\n    results = []\n\n    # Pre-compute v = Bw as it is constant\n    v = B @ w\n\n    for z_c_flat, tau in test_cases:\n        z_c = z_c_flat.reshape(-1, 1)\n\n        # 1. Compute softmax weights pi\n        p = z_c / tau\n        # Numerically stable softmax\n        p_stable = p - np.max(p)\n        exp_p = np.exp(p_stable)\n        pi = exp_p / np.sum(exp_p)\n\n        # 2. Compute prediction y_hat and error delta_L\n        # y_hat = w.T @ B.T @ pi + b0, which is v.T @ pi + b0\n        y_hat = v.T @ pi + b0\n        delta_L = (y_hat - y).item()\n        \n        # 3. Compute gradient of L w.r.t. z_c\n        v_bar = (v.T @ pi).item()\n        g_y_hat = (1.0 / tau) * pi * (v - v_bar)\n        g_L = delta_L * g_y_hat\n        grad_norm = np.linalg.norm(g_L)\n\n        # 4. Compute Hessian of L w.r.t. z_c\n        \n        # First term of H_L: g_y_hat @ g_y_hat.T\n        H_L_term1 = g_y_hat @ g_y_hat.T\n        \n        # Second term of H_L: delta_L * H_y_hat\n        d_vec = v - v_bar\n        \n        # Construct H_y_hat\n        # (H_y_hat)_kl = (1/tau^2) * (pi_k*d_k*delta_kl - pi_k*pi_l*(d_k + d_l))\n        # This can be vectorized as:\n        # Diagonal part: diag(pi * d / tau^2)\n        # Off-diagonal part: - (1/tau^2) * (pi @ pi.T) element-wise-mult (d_vec @ 1.T + 1 @ d_vec.T)\n        pi_d_diag = np.diag((pi * d_vec).flatten())\n        pi_outer = pi @ pi.T\n        d_sum_outer = d_vec @ np.ones((1, M)) + np.ones((M, 1)) @ d_vec.T\n        \n        H_y_hat = (1.0 / tau**2) * (pi_d_diag - pi_outer * d_sum_outer)\n\n        H_L = H_L_term1 + delta_L * H_y_hat\n        \n        # 5. Compute spectral norm of the Hessian\n        # For a symmetric matrix, this is the max absolute eigenvalue.\n        eigenvalues = np.linalg.eigvalsh(H_L)\n        hess_norm = np.max(np.abs(eigenvalues))\n\n        results.append([grad_norm, hess_norm])\n\n    # Format output as specified: [[g1,h1],[g2,h2],...]\n    formatted_results = [f\"[{g},{h}]\" for g, h in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}