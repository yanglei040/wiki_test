{
    "hands_on_practices": [
        {
            "introduction": "感知机算法的收敛性是其理论美的核心。本练习将理论与实践相结合，您将亲手实现感知机算法，并计算其在可分数据集上的实际错误次数。通过将经验结果与基于数据几何特性（半径$R$和间隔$\\gamma$）的理论错误上界进行比较，您将深刻理解为何感知机在理想条件下保证收敛。",
            "id": "3190718",
            "problem": "要求您在一小组合成的、线性可分的数据集上实现并实证研究感知机学习算法。该计算实验必须基于感知机假设类和感知机学习规则的基本定义，并且不应依赖任何预先推导的性能公式。您的程序必须计算感知机在训练过程中所犯错误的经验次数，然后计算一个取决于数据集和给定参考分离向量的几何量。最后，汇总多个测试用例的结果，并将其以单行的、机器可检查的格式输出。\n\n使用的基本原理和定义：\n- 一个二元分类数据集由输入 $x_i \\in \\mathbb{R}^d$ 和标签 $y_i \\in \\{-1, +1\\}$ 组成。\n- 线性分类器由一个权重向量 $w \\in \\mathbb{R}^d$ 指定，其预测为 $\\hat{y}_i = \\mathrm{sign}(w^\\top x_i)$，其中如果 $z > 0$，$\\mathrm{sign}(z)$ 返回 $+1$；如果 $z  0$，返回 $-1$；如果 $z = 0$，返回 $0$。\n- 感知机学习规则仅在出错时更新当前权重向量 $w$：当 $y_i \\, w^\\top x_i \\le 0$ 时，设置 $w \\leftarrow w + y_i x_i$。\n- 对于数据集 $D = \\{(x_i, y_i)\\}_{i=1}^n$，定义数据集半径 $R$ 为 $R = \\max_i \\|x_i\\|_2$，其中 $\\|\\cdot\\|_2$ 表示欧几里得范数。\n- 给定任何能正确分类所有点的固定参考分离向量 $w^\\ast \\in \\mathbb{R}^d$，定义数据集相对于 $w^\\ast$ 的归一化几何间隔 $\\gamma$ 为 $\\gamma = \\min_i \\dfrac{y_i \\, w^{\\ast\\top} x_i}{\\|w^\\ast\\|_2}$。\n\n您的任务：\n1. 对每个数据集，初始化 $w_0 = 0$，并按循环顺序对样本运行感知机学习规则，直到算法完成一整轮（full pass）且无任何错误。为确保在出现编程错误时能够终止，将训练上限设置为 $T = 1000$ 轮（epochs）；对于所提供的测试用例，算法将远在此上限之前收敛。\n2. 统计训练期间所犯错误的总数 $M$（这等于所应用的更新次数）。\n3. 按上述定义计算 $R$ 和 $\\gamma$，然后计算量 $B = \\left(\\dfrac{R}{\\gamma}\\right)^2$。\n4. 对每个数据集，生成列表 $[M, B, M \\le B]$，其中 $M$ 是一个整数，$B$ 是一个浮点数，$M \\le B$ 是一个布尔值，表示经验错误数是否不超过计算出的量。汇总所有测试用例的列表，并按下面指定的确切格式将其打印为单行。\n\n不涉及角度单位。此问题中没有物理单位。\n\n测试套件：\n为以下三个数据集提供结果，每个数据集都有其关联的标签和参考 $w^\\ast$。\n\n- 测试用例 1（二维，良好分离）：\n  - 输入 $X = [\\,(2, 2),\\; (2, 0),\\; (0, 2),\\; (-2, -1),\\; (-1, -2),\\; (-2, -2)\\,]$.\n  - 标签 $Y = [\\,+1,\\; +1,\\; +1,\\; -1,\\; -1,\\; -1\\,]$.\n  - 参考向量 $w^\\ast = (1, 1)$。\n\n- 测试用例 2（二维，小间隔）：\n  - 输入 $X = [\\,(1, 0.05),\\; (1, -0.05),\\; (1, 0.0),\\; (-1, 0.05),\\; (-1, -0.05),\\; (-1, 0.0)\\,]$.\n  - 标签 $Y = [\\,+1,\\; +1,\\; +1,\\; -1,\\; -1,\\; -1\\,]$.\n  - 参考向量 $w^\\ast = (1, 0.1)$。\n\n- 测试用例 3（二维，对称性，类边界行为）：\n  - 输入 $X = [\\,(1, 0),\\; (0, 1),\\; (-1, 0),\\; (0, -1)\\,]$.\n  - 标签 $Y = [\\,+1,\\; +1,\\; -1,\\; -1\\,]$.\n  - 参考向量 $w^\\ast = (1, 1)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试用例必须贡献一个列表 $[M, B, M \\le B]$。例如，最终打印的行应如下所示\n$[ [M_1,B_1,\\text{True}], [M_2,B_2,\\text{True}], [M_3,B_3,\\text{False}] ]$\n但要使用实际计算出的值，并且输出中没有空格，最终形成如下形式的字符串\n$[[M_1,B_1,\\text{True}],[M_2,B_2,\\text{True}],[M_3,B_3,\\text{False}]]$。",
            "solution": "此问题要求对感知机学习算法在三个合成的、线性可分的数据集上的性能进行实证研究。任务的核心是实现该算法，统计其在训练期间所犯错误的次数，并将此经验计数与从数据集的几何属性推导出的理论上界进行比较。此分析基于著名的感知机错误界限定理（Perceptron Mistake Bound theorem）。\n\n每个测试用例的解决过程包括以下步骤：\n1.  **模拟感知机算法**：通过运行感知机学习算法来确定错误次数 $M$。\n2.  **计算几何量**：根据所提供的数据和参考分离向量 $w^\\ast$，计算数据集半径 $R$ 和归一化几何间隔 $\\gamma$。\n3.  **计算界限**：理论上界量 $B$ 计算为 $B = (R/\\gamma)^2$。\n4.  **验证不等式**：将经验错误计数 $M$ 与计算出的界限量 $B$ 进行比较，以验证不等式 $M \\le B$。\n\n**步骤 1：感知机算法模拟**\n算法以权重向量 $w_0 = 0 \\in \\mathbb{R}^d$ 初始化，其中 $d$ 是输入空间的维度。然后，算法以循环方式遍历训练样本 $(x_i, y_i)$。一轮（epoch）是指对数据集中所有样本的一次完整遍历。\n\n对每个样本 $(x_i, y_i)$，都会进行一次预测。如果当前分类器 $w$ 错误分类了该点，即满足条件 $y_i(w^\\top x_i) \\le 0$，则发生错误。包含等式情况 $w^\\top x_i = 0$ 是至关重要的，因为位于决策边界上的点未被正确分类为 $+1$ 或 $-1$ 类。\n\n在发生错误时，权重向量根据感知机学习规则进行更新：\n$$ w \\leftarrow w + y_i x_i $$\n错误总数 $M$ 是整个训练过程中这些更新的累积计数。\n\n训练一轮接一轮地进行，直到完成一整轮且无任何错误。此时，算法已找到一个能够正确分类所有训练样本的权重向量 $w$，算法终止。问题提供了一个 $T = 1000$ 轮的最大训练上限作为保障措施，不过对于线性可分的数据，收敛是有保证的。\n\n**步骤 2：几何量的计算**\n需要数据集的两个关键几何属性。\n\n首先，数据集半径 $R$ 定义为数据集中任何输入向量的最大欧几里得范数：\n$$ R = \\max_{i} \\|x_i\\|_2 $$\n该值表示任何数据点距原点的最大距离。\n\n其次，归一化几何间隔 $\\gamma$ 是相对于一个给定的、能正确分类所有数据点（即对所有 $i$ 都有 $y_i (w^{\\ast\\top} x_i) > 0$）的固定分离向量 $w^\\ast$ 定义的。间隔 $\\gamma$ 是从任何点到由 $w^\\ast$ 定义的超平面的最小归一化有向距离：\n$$ \\gamma = \\min_{i} \\frac{y_i (w^{\\ast\\top} x_i)}{\\|w^\\ast\\|_2} $$\n项 $y_i(w^{\\ast\\top} x_i)$ 对所有点都为正，除以 $\\|w^\\ast\\|_2$ 将“函数间隔”转换为真实的欧几里得距离，使得 $\\gamma$ 成为从数据点到分离超平面的最小几何距离。\n\n**步骤 3：计算界限量**\n感知机错误界限定理指出，对于任何以间隔 $\\gamma_{opt}$（在所有分离超平面上最大化）线性可分的数据集，感知机算法（从 $w_0=0$ 开始）所犯的错误次数 $M$ 的上界为 $M \\le (R/\\gamma_{opt})^2$。该界限也适用于相对于*任何*分离向量 $w^\\ast$（而不仅仅是最优向量）定义的间隔 $\\gamma$。本问题要求我们为每个测试用例提供的特定 $w^\\ast$ 计算并验证此界限。因此，我们计算量 $B$：\n$$ B = \\left(\\frac{R}{\\gamma}\\right)^2 $$\n\n**步骤 4：汇总最终结果**\n对于三个测试用例中的每一个，我们执行上述计算以获得整数错误计数 $M$ 和浮点界限 $B$。然后我们构成一个包含这两个值和一个指示错误界限是否成立的布尔值的列表：$[M, B, M \\le B]$。最终输出是所有测试用例的这些列表的汇总。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the perceptron learning algorithm and computes the mistake bound quantity for three test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            # Test case 1 (two-dimensional, well-separated)\n            \"X\": np.array([[2., 2.], [2., 0.], [0., 2.], [-2., -1.], [-1., -2.], [-2., -2.]]),\n            \"Y\": np.array([1, 1, 1, -1, -1, -1]),\n            \"w_star\": np.array([1., 1.])\n        },\n        {\n            # Test case 2 (two-dimensional, small margin)\n            \"X\": np.array([[1., 0.05], [1., -0.05], [1., 0.0], [-1., 0.05], [-1., -0.05], [-1., 0.0]]),\n            \"Y\": np.array([1, 1, 1, -1, -1, -1]),\n            \"w_star\": np.array([1., 0.1])\n        },\n        {\n            # Test case 3 (two-dimensional, symmetry, boundary-like behavior)\n            \"X\": np.array([[1., 0.], [0., 1.], [-1., 0.], [0., -1.]]),\n            \"Y\": np.array([1, 1, -1, -1]),\n            \"w_star\": np.array([1., 1.])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X = case[\"X\"]\n        Y = case[\"Y\"]\n        w_star = case[\"w_star\"]\n        n_samples, n_features = X.shape\n\n        # Task 1  2: Run perceptron and count mistakes M\n        w = np.zeros(n_features)\n        M = 0\n        max_epochs = 1000\n\n        for _ in range(max_epochs):\n            mistakes_in_epoch = 0\n            for i in range(n_samples):\n                # Check for a mistake: y_i * (w^T * x_i) = 0\n                if Y[i] * np.dot(w, X[i]) = 0:\n                    # Apply the update rule: w - w + y_i * x_i\n                    w = w + Y[i] * X[i]\n                    M += 1\n                    mistakes_in_epoch += 1\n            \n            # If a full pass is completed with no mistakes, convergence is reached\n            if mistakes_in_epoch == 0:\n                break\n        \n        # Task 3: Compute R and gamma\n        # Compute R = max_i ||x_i||_2\n        norms = np.linalg.norm(X, axis=1)\n        R = np.max(norms)\n\n        # Compute gamma = min_i (y_i * w_star^T * x_i) / ||w_star||_2\n        numerator = Y * np.dot(X, w_star)\n        denominator = np.linalg.norm(w_star)\n        gamma = np.min(numerator) / denominator\n\n        # Compute B = (R / gamma)^2\n        B = (R / gamma)**2\n        \n        # Task 4: Aggregate the results\n        result_list = [M, B, M = B]\n        results.append(result_list)\n\n    # Final print statement in the exact required format with no spaces.\n    # Manually build the string to avoid spaces introduced by str(list).\n    output_str = \"[\"\n    for i, res in enumerate(results):\n        # res[2] is a boolean; str(res[2]) produces 'True' or 'False'.\n        output_str += f\"[{res[0]},{res[1]},{str(res[2])}]\"\n        if i  len(results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在理想情况下感知机能够收敛，但现实世界的数据往往并非完美线性可分。本练习将引导您探索当线性可分条件不满足时，标准感知机算法可能陷入无限循环的现象。您将学习如何通过构造一个近乎可分但实际不可分的数据集来复现此问题，并实现两种实用的改进算法——“口袋算法”与“平均感知机”——来获得一个稳健的解决方案。",
            "id": "3190769",
            "problem": "您的任务是设计并实现一个完整的程序，该程序在二维空间中构建一个近线性可分的数据集，引入一个翻转标签使数据集变为线性不可分，然后在感知机（Perceptron）学习动态中检测无限循环。您还必须使用口袋感知机（pocket Perceptron）和平均感知机（averaged Perceptron）来缓解循环问题，并为一个小型测试套件报告可量化的指标。\n\n此任务的基础是线性分类器的核心定义。感知机是一种线性阈值单元，它通过一个仿射分数的符号来为输入向量 $\\mathbf{x} \\in \\mathbb{R}^d$ 预测一个在 $\\{-1,+1\\}$ 中的标签。该仿射分数是通过为 $\\mathbf{x}$ 增广一个偏置项来实现的。令 $\\tilde{\\mathbf{x}} = (1, x_1, \\dots, x_d) \\in \\mathbb{R}^{d+1}$ 为增广偏置后的输入，令 $\\mathbf{w} \\in \\mathbb{R}^{d+1}$ 为参数向量。对于一个输入-标签对 $(\\mathbf{x}, y)$，预测值为 $\\operatorname{sign}(\\mathbf{w}^\\top \\tilde{\\mathbf{x}})$。当 $y \\cdot (\\mathbf{w}^\\top \\tilde{\\mathbf{x}}) > 0$ 时，该样本被认为是正确分类的。感知机学习规则在分类错误时按 $\\mathbf{w} \\leftarrow \\mathbf{w} + y \\tilde{\\mathbf{x}}$ 更新 $\\mathbf{w}$。对于线性可分的数据，存在一个超平面，使得对所有 $i$ 都有 $y_i \\cdot (\\mathbf{w}^\\top \\tilde{\\mathbf{x}}_i) > 0$ 成立，并且感知机保证能在有限次更新内找到一个解。当数据不是线性可分时，感知机可能永远无法达到零训练误差，并可能无限期地进行更新。证明数据非线性可分的一种原则性方法是通过凸包：如果一个类别的凸包包含了另一个类别的某个点，则不存在分离超平面。\n\n您的数据集必须是完全确定性的，并按以下方式在 $d=2$ 维空间中构建。定义三个正类锚点和三个负类锚点：\n- 正类锚点：$\\mathbf{p}_1 = (2,0)$、$\\mathbf{p}_2 = (0,2)$、$\\mathbf{p}_3 = (2,2)$，标签为 $+1$。\n- 负类锚点：$\\mathbf{n}_1 = (-2,0)$、$\\mathbf{n}_2 = (0,-2)$、$\\mathbf{n}_3 = (-2,-2)$，标签为 $-1$。\n以及一个内部点 $\\mathbf{q} = (1.2, 1.2)$，初始标签为 $+1$。正类锚点的凸包包含 $\\mathbf{q}$。将 $\\mathbf{q}$ 的标签翻转为 $-1$ 会使数据集变为线性不可分，因为正类点的凸包包含了一个负类点。为了探究间隔（margin）效应，允许将一个缩放因子 $s \\in \\mathbb{R}_{>0}$ 统一应用于所有坐标，生成缩放后的点 $s \\cdot \\mathbf{p}_j$、$s \\cdot \\mathbf{n}_j$ 和 $s \\cdot \\mathbf{q}$，同时保留它们的标签。所有计算都必须使用增广偏置后的输入 $\\tilde{\\mathbf{x}} = (1, x_1, x_2)$。\n\n您必须实现：\n- 一个标准感知机，采用确定性的样本顺序，初始化 $\\mathbf{w} = \\mathbf{0}$，并在 $y \\cdot (\\mathbf{w}^\\top \\tilde{\\mathbf{x}}) \\le 0$ 时使用更新规则 $\\mathbf{w} \\leftarrow \\mathbf{w} + y \\tilde{\\mathbf{x}}$。一个 epoch 是指按指定顺序完整遍历一次数据集。如果一个 epoch 完成且没有出现错误，则声明为收敛。\n- 一个循环检测器，当连续 $E$ 个 epoch，每个 epoch 都至少有一次更新，且迄今为止观察到的最佳（最低）错误数没有改善时，声明为无限循环。这具体化了这样一个思想：学习动态持续改变参数但并未减少训练误差，这与非线性可分性是一致的。\n- 一个口袋机制，用于存储在所有 epoch 中迄今为止产生最低错误数的参数向量 $\\mathbf{w}_{\\text{pocket}}$。\n- 一个平均感知机，它返回的 $\\bar{\\mathbf{w}}$ 等于每次更新步骤后参数向量的算术平均值。如果没有更新，则设 $\\bar{\\mathbf{w}} = \\mathbf{w}$。\n\n将参数向量 $\\mathbf{w}$ 的训练集分类错误率定义为\n$$\n\\mathcal{E}(\\mathbf{w}) \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}\\big[\\, y_i \\cdot (\\mathbf{w}^\\top \\tilde{\\mathbf{x}}_i) \\le 0 \\,\\big],\n$$\n其中 $n$ 是样本总数，$\\mathbf{1}[\\cdot]$ 表示指示函数。错误率必须表示为小数（0 到 1 之间的无单位分数）。\n\n实现以下参数值的测试套件：\n- 测试用例 1（顺利路径，可分）：翻转标志 $f = \\text{False}$，缩放因子 $s = 1.0$，最大 epoch 数 $T = 50$，循环耐心值 $E = 5$。\n- 测试用例 2（因单个翻转标签导致的不可分）：翻转标志 $f = \\text{True}$，缩放因子 $s = 1.0$，最大 epoch 数 $T = 200$，循环耐心值 $E = 10$。\n- 测试用例 3（小间隔的可分）：翻转标志 $f = \\text{False}$，缩放因子 $s = 0.2$，最大 epoch 数 $T = 200$，循环耐心值 $E = 10$。\n\n对于每个测试用例，使用指定参数运行标准感知机，并按顺序报告五个量：\n- $c$：一个布尔值，指示标准感知机是否收敛到零训练误差，\n- $z$：一个布尔值，指示循环检测器是否检测到循环，\n- $u$：一个整数，等于执行的总更新次数，\n- $e_p$：一个浮点数，等于 $\\mathcal{E}(\\mathbf{w}_{\\text{pocket}})$ 在训练集上的值，\n- $e_a$：一个浮点数，等于 $\\mathcal{E}(\\bar{\\mathbf{w}})$ 在训练集上的值。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[c_1, z_1, u_1, e_{p,1}, e_{a,1}, c_2, z_2, u_2, e_{p,2}, e_{a,2}, c_3, z_3, u_3, e_{p,3}, e_{a,3}]$，其中下标表示测试用例的索引。不应打印任何其他文本。",
            "solution": "问题陈述已经过验证，被认为是有效的。它在科学上基于线性分类器的原理，特别是感知机模型。该设置是适定的，所有参数、初始条件、更新规则和终止标准都已确定性地指定，确保了解决方案的唯一性和可验证性。\n\n任务是实现感知机学习算法，并在一个特制的、设计为线性可分或不可分的数据集上分析其行为。分析内容包括检测收敛或循环，并评估标准感知机、口袋感知机和平均感知机的性能。\n\n首先，我们按照规定在 $\\mathbb{R}^2$ 中构建数据集。数据集由 $n=7$ 个点组成。\n锚点定义如下：\n- 正类锚点 ($y=+1$)：$\\mathbf{p}_1 = (2,0)$、$\\mathbf{p}_2 = (0,2)$、$\\mathbf{p}_3 = (2,2)$\n- 负类锚点 ($y=-1$)：$\\mathbf{n}_1 = (-2,0)$、$\\mathbf{n}_2 = (0,-2)$、$\\mathbf{n}_3 = (-2,-2)$\n- 一个内部点：$\\mathbf{q} = (1.2, 1.2)$\n\n一个缩放因子 $s > 0$ 应用于所有点的坐标。$\\mathbf{q}$ 的标签由一个翻转标志 $f$ 决定。如果 $f$ 为 false，$\\mathbf{q}$ 的标签为 $+1$，使得数据集线性可分。如果 $f$ 为 true，$\\mathbf{q}$ 的标签为 $-1$。在这种情况下，$\\mathbf{q}$ 是一个位于正类点凸包内的负类点（例如，$s \\cdot \\mathbf{q} = 0.4(s \\cdot \\mathbf{p}_1) + 0.4(s \\cdot \\mathbf{p}_2) + 0.2(s \\cdot \\mathbf{p}_3)$），这使得数据集线性不可分。\n\n为了计算，每个输入向量 $\\mathbf{x} = (x_1, x_2)$ 都用一个偏置项进行增广，得到 $\\tilde{\\mathbf{x}} = (1, x_1, x_2)$。相应地，感知机的参数向量为 $\\mathbf{w} \\in \\mathbb{R}^3$。数据点按固定的、确定性的顺序处理：$(\\mathbf{p}_1, \\dots, \\mathbf{p}_3, \\mathbf{n}_1, \\dots, \\mathbf{n}_3, \\mathbf{q})$。\n\n实现的核心是一个统一的训练函数，用于模拟感知机动态。模拟最多进行 $T$ 个 epoch。一个 epoch 包括对所有 $n=7$ 个数据点的一次完整遍历。\n\n模拟状态包括：\n- 参数向量 $\\mathbf{w}$，初始化为 $\\mathbf{0}$。\n- 总更新次数 $u$，初始化为 $0$。\n- 口袋参数向量 $\\mathbf{w}_{\\text{pocket}}$，用于存储迄今为止在训练集上实现最小分类错误数的权重。它用初始的 $\\mathbf{w}$ 进行初始化。\n- 所有更新后权重向量的总和 $\\mathbf{w}_{\\text{sum}}$，初始化为 $\\mathbf{0}$，用于平均感知机。\n\n主训练循环遍历各个 epoch。在每个 epoch 内，它遍历训练样本 $(\\tilde{\\mathbf{x}}_i, y_i)$。如果条件 $y_i \\cdot (\\mathbf{w}^\\top \\tilde{\\mathbf{x}}_i) \\le 0$ 成立，则识别为一个错误。发生错误时：\n1. 更新参数向量：$\\mathbf{w} \\leftarrow \\mathbf{w} + y_i \\tilde{\\mathbf{x}}_i$。\n2. 总更新次数 $u$ 增加。\n3. 将新的 $\\mathbf{w}$ 加到运行总和中：$\\mathbf{w}_{\\text{sum}} \\leftarrow \\mathbf{w}_{\\text{sum}} + \\mathbf{w}$。\n4. 启动口袋机制：计算新的 $\\mathbf{w}$ 在整个训练集上的错误数。如果这个数量低于迄今为止的最佳数量，则将 $\\mathbf{w}_{\\text{pocket}}$ 更新为新的 $\\mathbf{w}$，并更新最佳错误数。\n\n每个 epoch 结束后，检查两个终止条件：\n- **收敛**：如果一个 epoch 完成且没有更新，则算法已收敛到一个分离超平面。标志 $c$ 被设为 true，模拟停止。\n- **循环检测**：如果连续 $E$ 个 epoch，每个 epoch 至少有一次更新，但迄今为止观察到的最小错误数没有改善，则声明为循环（$z$ 被设为 true）。这是通过一个计数器来跟踪的，该计数器对每个没有改善的 epoch（有更新）进行递增，并在口袋机制找到新的最佳权重向量时重置。\n\n如果达到最大 epoch 数 $T$，模拟也会终止。\n\n模拟终止后，计算最终指标：\n- $c$：布尔值，指示是否达到收敛。\n- $z$：布尔值，指示是否检测到循环。\n- $u$：执行的总更新次数。\n- $e_p$：最终口袋向量的训练错误率，$\\mathcal{E}(\\mathbf{w}_{\\text{pocket}}) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}[y_i \\cdot (\\mathbf{w}_{\\text{pocket}}^\\top \\tilde{\\mathbf{x}}_i) \\le 0]$。\n- $e_a$：平均感知机向量 $\\bar{\\mathbf{w}}$ 的训练错误率。如果 $u > 0$，$\\bar{\\mathbf{w}}$ 计算为 $\\frac{1}{u}\\mathbf{w}_{\\text{sum}}$；如果 $u=0$，则为初始的 $\\mathbf{w}$。然后错误率为 $\\mathcal{E}(\\bar{\\mathbf{w}}) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}[y_i \\cdot (\\bar{\\mathbf{w}}^\\top \\tilde{\\mathbf{x}}_i) \\le 0]$。\n\n将此过程应用于问题中指定的三个测试用例中的每一个，并将每个案例产生的五个指标连接起来以生成最终输出。",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(flip_flag: bool, scale: float, max_epochs: int, cycle_patience: int):\n    \"\"\"\n    Runs a Perceptron simulation for a given configuration.\n\n    Args:\n        flip_flag (bool): If True, flips the label of the interior point 'q'.\n        scale (float): Scaling factor for all point coordinates.\n        max_epochs (int): Maximum number of training epochs.\n        cycle_patience (int): Number of non-improving epochs to detect cycling.\n\n    Returns:\n        tuple: (converged, cycled, updates, pocket_error, avg_error)\n    \"\"\"\n    # 1. Dataset Construction\n    points = [\n        (np.array([2.0, 0.0]), 1),\n        (np.array([0.0, 2.0]), 1),\n        (np.array([2.0, 2.0]), 1),\n        (np.array([-2.0, 0.0]), -1),\n        (np.array([0.0, -2.0]), -1),\n        (np.array([-2.0, -2.0]), -1),\n        (np.array([1.2, 1.2]), -1 if flip_flag else 1)\n    ]\n    \n    n = len(points)\n    \n    # Scale and augment data\n    X = np.ones((n, 3))\n    y = np.zeros(n)\n    for i, (coord, label) in enumerate(points):\n        X[i, 1:] = coord * scale\n        y[i] = label\n\n    # 2. Initialization\n    w = np.zeros(3)\n    w_pocket = np.copy(w)\n    w_sum = np.zeros(3)\n    total_updates = 0\n    converged = False\n    cycling_detected = False\n\n    def get_mistake_count(w_vec):\n        return np.sum(y * (X @ w_vec) = 0)\n\n    min_mistakes_so_far = get_mistake_count(w)\n    epochs_at_min_mistakes = 0\n\n    # 3. Main Training Loop\n    for epoch in range(max_epochs):\n        updates_in_epoch = 0\n        new_min_found_this_epoch = False\n\n        for i in range(n):\n            if y[i] * (w @ X[i]) = 0:\n                updates_in_epoch += 1\n                total_updates += 1\n                w = w + y[i] * X[i]\n                w_sum = w_sum + w\n\n                # Pocket logic\n                current_mistakes = get_mistake_count(w)\n                if current_mistakes  min_mistakes_so_far:\n                    min_mistakes_so_far = current_mistakes\n                    w_pocket = np.copy(w)\n                    new_min_found_this_epoch = True\n        \n        # End-of-epoch checks\n        if updates_in_epoch == 0:\n            converged = True\n            break\n        \n        # Cycle detection logic\n        if new_min_found_this_epoch:\n            epochs_at_min_mistakes = 0\n        else:\n            epochs_at_min_mistakes += 1\n        \n        if epochs_at_min_mistakes >= cycle_patience:\n            cycling_detected = True\n            break\n\n    # 4. Final Calculations\n    e_p = get_mistake_count(w_pocket) / n\n\n    if total_updates > 0:\n        w_avg = w_sum / total_updates\n    else:\n        w_avg = w_pocket # If no updates, w_sum is zero, use initial w\n    \n    e_a = get_mistake_count(w_avg) / n\n\n    return converged, cycling_detected, total_updates, e_p, e_a\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (flip_flag, scale, max_epochs, cycle_patience)\n        (False, 1.0, 50, 5),    # Case 1\n        (True, 1.0, 200, 10),   # Case 2\n        (False, 0.2, 200, 10),  # Case 3\n    ]\n\n    results = []\n    for case in test_cases:\n        c, z, u, e_p, e_a = run_simulation(*case)\n        results.extend([c, z, u, round(e_p, 8), round(e_a, 8)])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "感知机的基本形式仅限于线性决策边界，但“核技巧”这一强大工具可以使其解决复杂的非线性问题。在本练习中，您将把标准感知机推广到核感知机，从而在更高维的特征空间中隐式地工作。通过在非线性数据集上应用多项式核与高斯径向基函数（RBF）核，您将直观地看到模型复杂性（以支持向量的数量衡量）与分类性能之间的权衡。",
            "id": "3190662",
            "problem": "您需要从基本原理出发，实现一个核化感知机学习算法，并量化对于不同的核函数，在训练周期中支持向量的数量如何相对于所达到的间隔增长。您必须仅推导和使用感知机和内积的核心定义，并严格按照下述说明实现算法。\n\n给定一个二元分类训练集 $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 且 $y_i \\in \\{-1, +1\\}$，以及一个半正定核函数 $K(\\mathbf{x}, \\mathbf{x}')$，核感知机维护一个初始化为全零的系数向量 $\\boldsymbol{\\alpha} \\in \\mathbb{R}^n$。在任何时候，分类器都是函数 $f(\\mathbf{x}) = \\sum_{j=1}^n \\alpha_j y_j K(\\mathbf{x}_j, \\mathbf{x})$ 的符号。错误驱动的学习规则仅在分类错误时进行更新：如果对于给定的样本 $(\\mathbf{x}_i, y_i)$，带符号响应 $y_i f(\\mathbf{x}_i) \\le 0$，则更新为 $\\alpha_i \\leftarrow \\alpha_i + 1$；否则不作任何更改。一个周期（epoch）是指以固定顺序完整遍历所有 $n$ 个训练样本一次。\n\n对于当前的系数向量 $\\boldsymbol{\\alpha}$，将特征空间权重向量定义为 $\\mathbf{w} = \\sum_{i=1}^n \\alpha_i y_i \\,\\phi(\\mathbf{x}_i)$，其中 $\\phi(\\cdot)$ 是与 $K(\\cdot,\\cdot)$ 相关联的特征映射。其导出范数遵循 $\\|\\mathbf{w}\\|^2 = \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)$。当前分类器在训练集上的带符号函数间隔为 $\\gamma = \\min_{i \\in \\{1,\\ldots,n\\}} \\dfrac{y_i f(\\mathbf{x}_i)}{\\|\\mathbf{w}\\|}$，并约定如果 $\\|\\mathbf{w}\\| = 0$，则 $\\gamma = 0$。支持向量的数量是满足 $\\alpha_i > 0$ 的索引 $i$ 的计数。\n\n您的程序必须：\n- 针对给定的核函数 $K(\\cdot,\\cdot)$ 和给定的周期数 $T \\in \\mathbb{N}$，实现上述指定的核感知机。\n- 在每个周期 $t \\in \\{1,\\ldots,T\\}$ 之后，计算：\n  1. 支持向量的数量，定义为 $\\alpha_i > 0$ 的索引 $i$ 的计数。\n  2. 如上文定义的间隔 $\\gamma$，仅使用 $K(\\cdot,\\cdot)$、$\\{(\\mathbf{x}_i,y_i)\\}$ 和 $\\boldsymbol{\\alpha}$。\n- 对每个测试用例，返回一个包含五个项目的摘要列表：\n  1. 周期 $T$ 结束后的最终支持向量数（一个整数）。\n  2. 周期 $T$ 结束后的最终间隔，四舍五入到 $6$ 位小数（一个浮点数）。\n  3. 在第 $1$ 到第 $T$ 个周期中观察到的最大支持向量数（一个整数）。\n  4. 在第 $1$ 到第 $T$ 个周期中观察到的最大间隔，四舍五入到 $6$ 位小数（一个浮点数）。\n  5. 一个布尔值，指示间隔序列是否随周期非递减（如果每个周期的间隔至少与前一个周期的间隔一样大，则为 true，否则为 false）。\n\n需要实现的核函数：\n- 次数为 $d \\in \\mathbb{N}$ 的多项式核：$K_{\\text{poly}}(\\mathbf{x}, \\mathbf{x}') = (1 + \\mathbf{x}^\\top \\mathbf{x}')^d$。\n- 带宽参数为 $\\gamma > 0$ 的径向基函数 (RBF) 核：$K_{\\text{rbf}}(\\mathbf{x}, \\mathbf{x}') = \\exp\\!\\big(-\\gamma \\|\\mathbf{x} - \\mathbf{x}'\\|^2\\big)$。\n\n使用以下固定的训练集和测试套件。所有向量都在 $\\mathbb{R}^2$ 中。\n\n数据集 $\\mathcal{D}_1$（线性可分）：\n- 类别 $+1$：$(2,2)$, $(2,3)$, $(3,2)$, $(3,3)$。\n- 类别 $-1$：$(-2,-2)$, $(-2,-3)$, $(-3,-2)$, $(-3,-3)$。\n\n数据集 $\\mathcal{D}_2$（异或模式）：\n- $(0,0)$，标签为 $-1$，\n- $(1,1)$，标签为 $-1$，\n- $(0,1)$，标签为 $+1$，\n- $(1,0)$，标签为 $+1$。\n\n测试套件参数集：\n- 用例 1：数据集 $\\mathcal{D}_1$，多项式核，次数 $d = 1$，周期数 $T = 5$。\n- 用例 2：数据集 $\\mathcal{D}_1$，RBF 核，$\\gamma = 0.3$，周期数 $T = 5$。\n- 用例 3：数据集 $\\mathcal{D}_2$，多项式核，次数 $d = 2$，周期数 $T = 10$。\n- 用例 4：数据集 $\\mathcal{D}_2$，RBF 核，$\\gamma = 2.0$，周期数 $T = 10$。\n\n最终输出格式要求：\n- 您的程序应生成单行输出，包含一个有四个条目（每个用例一个）的列表，其中每个条目是上述的摘要列表。该行必须严格按逗号分隔的列表形式打印，并用方括号括起来，无空格，例如：$[[...],[...],[...],[...]]$，其中每个内部列表按顺序包含五个值：$[\\text{final\\_SV}, \\text{final\\_margin\\_rounded}, \\text{max\\_SV}, \\text{max\\_margin\\_rounded}, \\text{is\\_nondecreasing}]$。间隔必须四舍五入到 $6$ 位小数。不涉及物理单位。不涉及角度。任何地方都不得使用百分比；仅使用十进制数。",
            "solution": "问题陈述经评估有效。它提出了一个基于既有机器学习原理（特别是核感知机算法）的、定义明确的计算任务。所有必要组件，包括数据集、算法定义、核函数和评估指标，都以清晰、一致且科学合理的方式提供。该问题内容自洽，没有矛盾、歧义或事实错误。它构成了一个可形式化且可验证的基础学习算法实现与分析练习。\n\n此问题的核心是实现核感知机算法。该算法为训练集 $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ 学习一个二元分类器，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 是输入向量， $y_i \\in \\{-1, +1\\}$ 是它们对应的标签。该算法在一个由核函数 $K(\\cdot, \\cdot)$ 隐式定义的高维特征空间中运行。\n\n决策函数由以下公式给出：\n$$f(\\mathbf{x}) = \\sum_{j=1}^n \\alpha_j y_j K(\\mathbf{x}_j, \\mathbf{x})$$\n其中 $\\boldsymbol{\\alpha} \\in \\mathbb{R}^n$ 是一个系数向量，初始化为全零。输入 $\\mathbf{x}$ 的预测标签是 $\\text{sign}(f(\\mathbf{x}))$。\n\n学习过程是错误驱动的。算法遍历训练数据，对每个样本 $(\\mathbf{x}_i, y_i)$，检查是否出错。如果预测标签与真实标签不符，即满足条件 $y_i f(\\mathbf{x}_i) \\le 0$，则发生错误。发生错误时，对应于该样本的系数会增加：\n$$\\alpha_i \\leftarrow \\alpha_i + 1$$\n如果样本被正确分类（$y_i f(\\mathbf{x}_i) > 0$），则不进行更新。对所有 $n$ 个训练样本的单次完整遍历称为一个周期（epoch）。训练会按指定的周期数 $T$ 进行。\n\n需要实现的两个核函数是：\n$1$. 次数为 $d$ 的多项式核：\n$$K_{\\text{poly}}(\\mathbf{x}, \\mathbf{x}') = (1 + \\mathbf{x}^\\top \\mathbf{x}')^d$$\n$2$. 带宽参数为 $\\gamma_{\\text{rbf}} > 0$ 的径向基函数 (RBF) 核：\n$$K_{\\text{rbf}}(\\mathbf{x}, \\mathbf{x}') = \\exp(-\\gamma_{\\text{rbf}} \\|\\mathbf{x} - \\mathbf{x}'\\|^2)$$\n请注意，此处使用符号 $\\gamma_{\\text{rbf}}$ 表示核参数，以区别于间隔 $\\gamma$。\n\n每个周期结束后，我们必须计算两个关键指标。为提高计算效率，我们首先预先计算 Gram 矩阵 $\\mathbf{G}$，这是一个 $n \\times n$ 矩阵，其中 $G_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j)$。\n\n第一个指标是支持向量的数量 $N_{SV}$。这些是曾用于更新、因此具有非零系数的训练样本。其计算方式为：\n$$N_{SV} = \\sum_{i=1}^n \\mathbb{I}(\\alpha_i > 0) = |\\{i \\in \\{1, \\ldots, n\\} : \\alpha_i > 0\\}|$$\n其中 $\\mathbb{I}(\\cdot)$ 是指示函数。\n\n第二个指标是几何间隔 $\\gamma$。在由核定义的特征空间中，权重向量为 $\\mathbf{w} = \\sum_{i=1}^n \\alpha_i y_i \\phi(\\mathbf{x}_i)$，其中 $\\phi$ 是隐式特征映射。该向量的范数平方可以仅使用核函数计算：\n$$\\|\\mathbf{w}\\|^2 = \\mathbf{w}^\\top \\mathbf{w} = \\left(\\sum_{i=1}^n \\alpha_i y_i \\phi(\\mathbf{x}_i)\\right)^\\top \\left(\\sum_{j=1}^n \\alpha_j y_j \\phi(\\mathbf{x}_j)\\right) = \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)$$\n这可以写成矩阵形式 $\\|\\mathbf{w}\\|^2 = (\\boldsymbol{\\alpha} \\odot \\mathbf{y})^\\top \\mathbf{G} (\\boldsymbol{\\alpha} \\odot \\mathbf{y})$，其中 $\\odot$ 表示逐元素乘积。单个点 $\\mathbf{x}_i$ 的带符号函数间隔是 $y_i f(\\mathbf{x}_i)$。整个训练集的几何间隔是最小函数间隔除以权重向量的范数：\n$$\\gamma = \\min_{i \\in \\{1, \\ldots, n\\}} \\frac{y_i f(\\mathbf{x}_i)}{\\|\\mathbf{w}\\|}$$\n根据定义，如果 $\\|\\mathbf{w}\\| = 0$（当所有 $\\alpha_i = 0$ 时发生），则间隔 $\\gamma$ 取为 $0$。\n\n每个测试用例的总体计算流程如下：\n$1$. 给定训练集 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$、核函数 $K$ 和周期数 $T$。\n$2$. 预计算 $n \\times n$ 的 Gram 矩阵 $\\mathbf{G}$。\n$3$. 初始化 $\\boldsymbol{\\alpha} = \\mathbf{0} \\in \\mathbb{R}^n$。\n$4$. 初始化空列表，用于存储每个周期的支持向量数和间隔。\n$5$. 对于从 $t = 1$ 到 $T$ 的每个周期：\n    a. 对于从 $i=1$ 到 $n$ 的每个训练样本 $(\\mathbf{x}_i, y_i)$：\n        i. 计算决策函数值：$f(\\mathbf{x}_i) = \\sum_{j=1}^n \\alpha_j y_j G_{ji}$。\n        ii. 如果 $y_i f(\\mathbf{x}_i) \\le 0$，更新 $\\alpha_i \\leftarrow \\alpha_i + 1$。\n    b. 在周期结束时，计算并存储 $N_{SV}$ 和 $\\gamma$。\n$6$. 在 $T$ 个周期后，根据存储的指标编制最终摘要列表：\n    a. 最终支持向量数（在周期 $T$ 时的 $N_{SV}$）。\n    b. 最终间隔（在周期 $T$ 时的 $\\gamma$），四舍五入到 $6$ 位小数。\n    c. 在所有周期中观察到的最大支持向量数。\n    d. 在所有周期中观察到的最大间隔，四舍五入到 $6$ 位小数。\n    e. 一个布尔值，指示间隔序列在各周期中是否非递减。\n\n对于固定的训练数据顺序，此过程是确定性的，并将应用于四个指定的测试用例中的每一个。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the kernel perceptron algorithm and analyzes its performance\n    on specified datasets and kernel configurations.\n    \"\"\"\n\n    # --- Kernel Functions ---\n    def poly_kernel(x1, x2, d):\n        \"\"\"Polynomial kernel: K(x1, x2) = (1 + x1.T @ x2)^d\"\"\"\n        return (1 + x1.T @ x2)**d\n\n    def rbf_kernel(x1, x2, gamma_param):\n        \"\"\"RBF kernel: K(x1, x2) = exp(-gamma * ||x1 - x2||^2)\"\"\"\n        return np.exp(-gamma_param * np.linalg.norm(x1 - x2)**2)\n\n    # --- Gram Matrix Computation ---\n    def compute_gram_matrix(X, kernel_func, kernel_param):\n        \"\"\"\n        Pre-computes the Gram matrix for a given dataset and kernel.\n\n        Args:\n            X (np.ndarray): Data matrix of shape (n_samples, n_features).\n            kernel_func (callable): The kernel function.\n            kernel_param (float): The parameter for the kernel (d or gamma).\n\n        Returns:\n            np.ndarray: The n_samples x n_samples Gram matrix.\n        \"\"\"\n        n_samples = X.shape[0]\n        gram_matrix = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                gram_matrix[i, j] = kernel_func(X[i], X[j], kernel_param)\n        return gram_matrix\n\n    # --- Main Perceptron Logic ---\n    def run_kernel_perceptron(X, y, kernel_func, kernel_param, T):\n        \"\"\"\n        Runs the kernel perceptron algorithm for T epochs.\n\n        Args:\n            X (np.ndarray): Data matrix.\n            y (np.ndarray): Labels vector.\n            kernel_func (callable): The kernel function.\n            kernel_param (float): The kernel parameter.\n            T (int): Number of epochs.\n\n        Returns:\n            list: A list containing the five summary statistics.\n        \"\"\"\n        n_samples, _ = X.shape\n        \n        # 1. Pre-compute Gram matrix\n        gram_matrix = compute_gram_matrix(X, kernel_func, kernel_param)\n\n        # 2. Initialize variables\n        alphas = np.zeros(n_samples, dtype=int)\n        \n        sv_counts_per_epoch = []\n        margins_per_epoch = []\n\n        # 3. Training loop\n        for _ in range(T):\n            # Pass over all training examples\n            for i in range(n_samples):\n                # Calculate f(x_i)\n                f_xi = np.sum(alphas * y * gram_matrix[:, i])\n                \n                # Check for mistake and update\n                if y[i] * f_xi = 0:\n                    alphas[i] += 1\n            \n            # 4. Compute metrics at the end of the epoch\n            # Number of support vectors\n            sv_count = np.count_nonzero(alphas)\n            \n            # Margin\n            ay = alphas * y\n            w_norm_sq = ay.T @ gram_matrix @ ay\n            \n            if w_norm_sq > 0:\n                w_norm = np.sqrt(w_norm_sq)\n                # Calculate functional margins for all points\n                f_all = gram_matrix @ ay\n                functional_margins = y * f_all\n                margin = np.min(functional_margins) / w_norm\n            else:\n                margin = 0.0 # As per problem spec\n            \n            sv_counts_per_epoch.append(sv_count)\n            margins_per_epoch.append(margin)\n\n        # 5. Compile summary statistics\n        final_sv = sv_counts_per_epoch[-1]\n        final_margin = round(margins_per_epoch[-1], 6)\n        max_sv = int(np.max(sv_counts_per_epoch)) if sv_counts_per_epoch else 0\n        max_margin = round(np.max(margins_per_epoch), 6) if margins_per_epoch else 0.0\n        \n        is_nondecreasing = all(\n            margins_per_epoch[i] = margins_per_epoch[i+1] + 1e-9 \n            for i in range(len(margins_per_epoch) - 1)\n        )\n\n        return [final_sv, final_margin, max_sv, max_margin, str(is_nondecreasing)]\n\n    # --- Datasets ---\n    # Dataset 1\n    X1 = np.array([\n        [2, 2], [2, 3], [3, 2], [3, 3],  # Class +1\n        [-2, -2], [-2, -3], [-3, -2], [-3, -3]  # Class -1\n    ])\n    y1 = np.array([1, 1, 1, 1, -1, -1, -1, -1])\n\n    # Dataset 2\n    X2 = np.array([\n        [0, 0], [1, 1], [0, 1], [1, 0]\n    ])\n    y2 = np.array([-1, -1, 1, 1])\n    \n    # --- Test Suite ---\n    test_cases = [\n        {'X': X1, 'y': y1, 'kernel_func': poly_kernel, 'param': 1, 'T': 5},\n        {'X': X1, 'y': y1, 'kernel_func': rbf_kernel, 'param': 0.3, 'T': 5},\n        {'X': X2, 'y': y2, 'kernel_func': poly_kernel, 'param': 2, 'T': 10},\n        {'X': X2, 'y': y2, 'kernel_func': rbf_kernel, 'param': 2.0, 'T': 10},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_kernel_perceptron(case['X'], case['y'], case['kernel_func'], case['param'], case['T'])\n        all_results.append(result)\n\n    # --- Final Output Formatting ---\n    outer_parts = []\n    for res in all_results:\n        inner_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]}]\"\n        outer_parts.append(inner_str)\n    \n    final_output = f\"[{','.join(outer_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}