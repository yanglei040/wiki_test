{
    "hands_on_practices": [
        {
            "introduction": "When exploring a neural architecture search space, it's tempting to assume that the performance contribution of each component, like an activation function or a normalization layer, is independent. However, the reality is often more complex, with components exhibiting strong interaction effects. This exercise  provides a hands-on method to quantify these interactions by applying principles from statistical factorial design to a hypothetical but illustrative performance model. By decomposing the total performance gain into main effects and interaction effects, you will develop a deeper intuition for analyzing the complex, non-additive landscape of architectural choices.",
            "id": "3158045",
            "problem": "Consider a discrete Neural Architecture Search (NAS) scenario where one replaces a fixed pair of activation function and normalization layer with a search over activation choices $f \\in \\{ \\text{ReLU}, \\text{SiLU}, \\text{LeakyReLU} \\}$ and normalization choices $n \\in \\{ \\text{Batch Normalization (BN)}, \\text{Layer Normalization (LN)}, \\text{Group Normalization (GN)} \\}$. The performance under a given regularization strength $\\alpha$ is modeled deterministically. Let the baseline pair be $(\\text{ReLU}, \\text{BN})$. The classification accuracy for a pair $(f,n)$ is defined by\n$$\nA(\\alpha, f, n) \\;=\\; A_{\\text{base}}(\\alpha) \\;+\\; a_f(\\alpha) \\;+\\; b_n(\\alpha) \\;+\\; s_{f,n}(\\alpha),\n$$\nwhere the baseline accuracy term is\n$$\nA_{\\text{base}}(\\alpha) \\;=\\; 0.83 \\;-\\; 0.02\\,\\alpha \\;-\\; 0.03\\left(1 - e^{-50\\,\\alpha}\\right),\n$$\nthe activation contributions are\n$$\na_{\\text{ReLU}}(\\alpha) \\;=\\; 0, \\quad\na_{\\text{SiLU}}(\\alpha) \\;=\\; 0.004 \\;+\\; 0.002\\,e^{-20\\,\\alpha}, \\quad\na_{\\text{LeakyReLU}}(\\alpha) \\;=\\; 0.003,\n$$\nthe normalization contributions are\n$$\nb_{\\text{BN}}(\\alpha) \\;=\\; 0, \\quad\nb_{\\text{LN}}(\\alpha) \\;=\\; -0.002 \\;+\\; 0.0005\\,\\alpha, \\quad\nb_{\\text{GN}}(\\alpha) \\;=\\; 0.004 \\;-\\; 0.0005\\,\\alpha,\n$$\nand the interaction contributions are nonzero only for specific pairs,\n$$\ns_{\\text{SiLU},\\text{GN}}(\\alpha) \\;=\\; 0.006\\,e^{-20\\,\\alpha}, \\quad\ns_{\\text{SiLU},\\text{LN}}(\\alpha) \\;=\\; -0.001 \\;+\\; 0.0005\\,e^{-30\\,\\alpha}, \\quad\ns_{\\text{LeakyReLU},\\text{LN}}(\\alpha) \\;=\\; -0.003\\left(1 - e^{-30\\,\\alpha}\\right),\n$$\nwith $s_{f,n}(\\alpha) \\;=\\; 0$ for all other $(f,n)$.\n\nDefine the performance gain relative to the baseline as\n$$\n\\Delta A(\\alpha, f, n) \\;=\\; A(\\alpha, f, n) \\;-\\; A(\\alpha, \\text{ReLU}, \\text{BN}).\n$$\nFor each fixed $\\alpha$, consider the full $3 \\times 3$ factorial table of $\\Delta A(\\alpha, f, n)$ over all $(f,n)$ combinations. Using the principle of additive decomposition with interaction in a two-factor factorial design, compute the interaction effect for each $(f,n)$ at that $\\alpha$ as the deviation from additivity after removing the main effects of $f$ and $n$ and the global mean. Then, for that $\\alpha$, identify the largest interaction effect value over all $(f,n)$ pairs.\n\nYou must implement a program that, for a specified test suite of regularization values, computes the largest interaction effect as defined above for each $\\alpha$ and outputs these values.\n\nThe fundamental base to use in your reasoning and implementation is:\n- The empirical risk minimization objective with $\\ell_2$ regularization, where the regularization strength $\\alpha$ controls model capacity via a penalty $\\alpha \\lVert \\mathbf{w} \\rVert_2^2$, influencing accuracy as a function of $\\alpha$.\n- The well-tested framework of two-factor factorial design with interaction, where the observed cell values are decomposed into a grand mean, main effects, and interaction effects without requiring probabilistic assumptions.\n\nTest suite:\n- Use the following four regularization strengths: $\\alpha \\in \\{ 0.0, 10^{-4}, 10^{-3}, 10^{-2} \\}$.\n\nTasks for each $\\alpha$:\n1. Construct $A(\\alpha, f, n)$ for all $f$ and $n$ using the definitions above.\n2. Compute $\\Delta A(\\alpha, f, n)$ relative to the baseline $(\\text{ReLU}, \\text{BN})$.\n3. Compute the interaction effect for each $(f,n)$ by removing the grand mean and both main effects from $\\Delta A(\\alpha, f, n)$ according to the additive-with-interaction model of a two-factor design.\n4. Return the maximum interaction effect value over $(f,n)$ for that $\\alpha$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the four $\\alpha$ values as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places (e.g., $[x_1,x_2,x_3,x_4]$). No physical units, angles, or percentages are involved; all values are dimensionless floats.",
            "solution": "We begin from the principle of empirical risk minimization with $\\ell_2$ regularization. For a dataset with inputs and labels, a deep network with parameters $\\mathbf{w}$ and architecture $(f,n)$ is trained to minimize\n$$\n\\mathcal{L}(\\mathbf{w}; f, n, \\alpha) \\;=\\; \\mathbb{E}_{\\text{data}} \\big[ \\ell(\\mathbf{w}; f, n) \\big] \\;+\\; \\alpha \\lVert \\mathbf{w} \\rVert_2^2,\n$$\nwhere $\\alpha$ is the regularization strength. Changing $\\alpha$ affects the capacity of the network by penalizing large weights, which in practice influences generalization and hence the achieved accuracy. In the present problem, the achieved accuracy is modeled deterministically via the given functions $A_{\\text{base}}(\\alpha)$, $a_f(\\alpha)$, $b_n(\\alpha)$, and $s_{f,n}(\\alpha)$ for scientific plausibility.\n\nThe architecture search treats activation $f$ and normalization $n$ as two factors with three levels each. For each $\\alpha$, we form the full $3 \\times 3$ table of observed values\n$$\nY_{f,n}(\\alpha) \\;=\\; \\Delta A(\\alpha, f, n) \\;=\\; A(\\alpha, f, n) \\;-\\; A(\\alpha, \\text{ReLU}, \\text{BN}).\n$$\nThe goal is to understand interaction effects, i.e., whether the combined choice $(f,n)$ yields deviations that cannot be explained by summing independent main effects of $f$ and $n$. This is naturally formalized by the canonical two-factor additive-with-interaction model:\n$$\nY_{f,n}(\\alpha) \\;=\\; \\mu(\\alpha) \\;+\\; \\theta_f(\\alpha) \\;+\\; \\phi_n(\\alpha) \\;+\\; \\psi_{f,n}(\\alpha),\n$$\nwhere $\\mu(\\alpha)$ is the grand mean, $\\theta_f(\\alpha)$ is the main effect of activation level $f$, $\\phi_n(\\alpha)$ is the main effect of normalization level $n$, and $\\psi_{f,n}(\\alpha)$ is the interaction effect for the $(f,n)$ cell. In a balanced full factorial design without noise, the least-squares solution for the effects is unique under sum-to-zero constraints across levels:\n$$\n\\sum_f \\theta_f(\\alpha) \\;=\\; 0, \\qquad \\sum_n \\phi_n(\\alpha) \\;=\\; 0, \\qquad \\sum_f \\psi_{f,n}(\\alpha) \\;=\\; 0 \\;\\;\\forall n, \\qquad \\sum_n \\psi_{f,n}(\\alpha) \\;=\\; 0 \\;\\;\\forall f.\n$$\nGiven the observed table $Y_{f,n}(\\alpha)$, the grand mean is\n$$\n\\mu(\\alpha) \\;=\\; \\frac{1}{9} \\sum_{f} \\sum_{n} Y_{f,n}(\\alpha),\n$$\nthe activation main effects are\n$$\n\\theta_f(\\alpha) \\;=\\; \\frac{1}{3} \\sum_{n} Y_{f,n}(\\alpha) \\;-\\; \\mu(\\alpha),\n$$\nthe normalization main effects are\n$$\n\\phi_n(\\alpha) \\;=\\; \\frac{1}{3} \\sum_{f} Y_{f,n}(\\alpha) \\;-\\; \\mu(\\alpha),\n$$\nand the interaction effects are obtained by subtracting the grand mean and both main effects:\n$$\n\\psi_{f,n}(\\alpha) \\;=\\; Y_{f,n}(\\alpha) \\;-\\; \\theta_f(\\alpha) \\;-\\; \\phi_n(\\alpha) \\;-\\; \\mu(\\alpha).\n$$\nEquivalently, using averages directly,\n$$\n\\psi_{f,n}(\\alpha) \\;=\\; Y_{f,n}(\\alpha) \\;-\\; \\overline{Y}_{f\\cdot}(\\alpha) \\;-\\; \\overline{Y}_{\\cdot n}(\\alpha) \\;+\\; \\overline{Y}_{\\cdot\\cdot}(\\alpha),\n$$\nwhere $\\overline{Y}_{f\\cdot}(\\alpha)$ is the mean across normalization levels at fixed $f$, $\\overline{Y}_{\\cdot n}(\\alpha)$ is the mean across activation levels at fixed $n$, and $\\overline{Y}_{\\cdot\\cdot}(\\alpha)$ is the grand mean. This decomposition arises from least-squares projections onto the subspaces spanned by factor level indicators in the design matrix, ensuring that main effects capture additive contributions and interaction effects capture deviations from additivity.\n\nAlgorithmic steps for each $\\alpha$:\n1. Evaluate $A_{\\text{base}}(\\alpha)$, $a_f(\\alpha)$ for all $f$, $b_n(\\alpha)$ for all $n$, and $s_{f,n}(\\alpha)$ for all $(f,n)$, then form $A(\\alpha, f, n)$.\n2. Compute $Y_{f,n}(\\alpha) \\;=\\; \\Delta A(\\alpha, f, n)$ by subtracting $A(\\alpha, \\text{ReLU}, \\text{BN})$, which equals $A_{\\text{base}}(\\alpha)$ because $a_{\\text{ReLU}}(\\alpha)=0$, $b_{\\text{BN}}(\\alpha)=0$, and $s_{\\text{ReLU},\\text{BN}}(\\alpha)=0$.\n3. Compute the grand mean $\\mu(\\alpha)$, row means $\\overline{Y}_{f\\cdot}(\\alpha)$, and column means $\\overline{Y}_{\\cdot n}(\\alpha)$.\n4. For each $(f,n)$, compute the interaction effect $\\psi_{f,n}(\\alpha)$ using the additive-with-interaction decomposition.\n5. Report $\\max_{f,n} \\psi_{f,n}(\\alpha)$ as the largest interaction effect for that $\\alpha$.\n\nTest suite and output:\n- Use $\\alpha \\in \\{ 0.0, 10^{-4}, 10^{-3}, 10^{-2} \\}$.\n- For each $\\alpha$, compute and round the largest interaction effect to six decimal places.\n- Output a single line of the form $[x_1,x_2,x_3,x_4]$, where $x_i$ corresponds to the value for the $i$-th $\\alpha$ in the order listed above.\n\nThis approach cleanly separates additive main effects attributable to individual factor levels from genuine interaction effects, thereby quantifying how much combined choices $(f,n)$ deviate from what would be predicted by additivity alone. Because the model is deterministic and balanced, the computed interaction effects are uniquely determined by the provided accuracy table and the factorial design decomposition.",
            "answer": "```python\n# Python 3.12 program to compute maximal interaction effects in a 3x3 factorial NAS setting.\n# Allowed libraries: numpy (1.23.5), scipy (1.11.4) [not used]. Uses only numpy and standard library.\n\nimport numpy as np\n\n# Define activation and normalization labels for clarity.\nACTIVATIONS = [\"ReLU\", \"SiLU\", \"LeakyReLU\"]\nNORMALIZATIONS = [\"BN\", \"LN\", \"GN\"]\n\ndef A_base(alpha: float) -> float:\n    # Baseline accuracy term for (ReLU, BN)\n    return 0.83 - 0.02 * alpha - 0.03 * (1.0 - np.exp(-50.0 * alpha))\n\ndef a_f(alpha: float, f_idx: int) -> float:\n    # Activation contributions\n    if ACTIVATIONS[f_idx] == \"ReLU\":\n        return 0.0\n    elif ACTIVATIONS[f_idx] == \"SiLU\":\n        return 0.004 + 0.002 * np.exp(-20.0 * alpha)\n    elif ACTIVATIONS[f_idx] == \"LeakyReLU\":\n        return 0.003\n    else:\n        raise ValueError(\"Unknown activation\")\n\ndef b_n(alpha: float, n_idx: int) -> float:\n    # Normalization contributions\n    name = NORMALIZATIONS[n_idx]\n    if name == \"BN\":\n        return 0.0\n    elif name == \"LN\":\n        return -0.002 + 0.0005 * alpha\n    elif name == \"GN\":\n        return 0.004 - 0.0005 * alpha\n    else:\n        raise ValueError(\"Unknown normalization\")\n\ndef s_fn(alpha: float, f_idx: int, n_idx: int) -> float:\n    # Interaction contributions\n    f = ACTIVATIONS[f_idx]\n    n = NORMALIZATIONS[n_idx]\n    if f == \"SiLU\" and n == \"GN\":\n        return 0.006 * np.exp(-20.0 * alpha)\n    if f == \"SiLU\" and n == \"LN\":\n        return -0.001 + 0.0005 * np.exp(-30.0 * alpha)\n    if f == \"LeakyReLU\" and n == \"LN\":\n        return -0.003 * (1.0 - np.exp(-30.0 * alpha))\n    # All other pairs have zero interaction in this synthetic model\n    return 0.0\n\ndef accuracy(alpha: float, f_idx: int, n_idx: int) -> float:\n    return A_base(alpha) + a_f(alpha, f_idx) + b_n(alpha, n_idx) + s_fn(alpha, f_idx, n_idx)\n\ndef delta_A_table(alpha: float) -> np.ndarray:\n    # Build the 3x3 table of Delta A relative to baseline (ReLU, BN)\n    baseline = accuracy(alpha, ACTIVATIONS.index(\"ReLU\"), NORMALIZATIONS.index(\"BN\"))\n    table = np.zeros((len(ACTIVATIONS), len(NORMALIZATIONS)))\n    for i in range(len(ACTIVATIONS)):\n        for j in range(len(NORMALIZATIONS)):\n            table[i, j] = accuracy(alpha, i, j) - baseline\n    return table\n\ndef interaction_effects(delta: np.ndarray) -> np.ndarray:\n    # Compute interaction effects using additive-with-interaction decomposition\n    grand_mean = np.mean(delta)\n    row_means = np.mean(delta, axis=1, keepdims=True)\n    col_means = np.mean(delta, axis=0, keepdims=True)\n    # psi_{f,n} = Y_{f,n} - row_mean_f - col_mean_n + grand_mean\n    psi = delta - row_means - col_means + grand_mean\n    return psi\n\ndef solve():\n    # Test suite alphas\n    test_alphas = [0.0, 1e-4, 1e-3, 1e-2]\n    results = []\n    for alpha in test_alphas:\n        delta = delta_A_table(alpha)\n        psi = interaction_effects(delta)\n        max_int = float(np.max(psi))\n        # Round to six decimals as required\n        results.append(f\"{max_int:.6f}\")\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "One of the most significant breakthroughs in Neural Architecture Search is the development of differentiable search strategies, which reframe the discrete search problem into a continuous optimization problem solvable with gradient descent. This practice  delves into the core mechanism of this approach by asking you to implement a differentiable pruning simulation. You will learn how to relax binary mask variables $m_\\ell \\in \\{0,1\\}$ into continuous gates $m_\\ell \\in [0,1]$ and use a temperature annealing schedule on $\\tau$ to gradually enforce discreteness while optimizing an objective that balances performance and sparsity.",
            "id": "3158131",
            "problem": "Consider Neural Architecture Search (NAS) where binary pruning masks are used to select a subset of architectural components. To enable gradient-based optimization, relax each binary mask variable from $m_\\ell \\in \\{0,1\\}$ to a continuous gate $m_\\ell \\in [0,1]$ via the temperature-controlled sigmoid parameterization $m_\\ell(\\mathbf{z}, \\tau) = \\sigma\\!\\left(\\frac{z_\\ell}{\\tau}\\right)$, where $\\sigma(u) = \\frac{1}{1 + e^{-u}}$, $\\mathbf{z} \\in \\mathbb{R}^L$ is the vector of logits, and $\\tau > 0$ is a temperature parameter. Consider the composite objective\n$$\nJ(\\mathbf{z};\\tau) \\;=\\; -\\sum_{\\ell=1}^L b_\\ell \\, m_\\ell(\\mathbf{z}, \\tau) \\;+\\; \\lambda \\sum_{\\ell=1}^L \\big| m_\\ell(\\mathbf{z}, \\tau) \\big|,\n$$\nwhere $b_\\ell \\ge 0$ models the utility of keeping component $\\ell$, and $\\lambda \\ge 0$ controls sparsity via an $\\ell_1$ penalty on the relaxed mask. This setup abstracts the trade-off between validation performance contribution and sparsity pressure, and is a well-used formulation in differentiable pruning and neural architecture search.\n\nStarting from the definitions of the sigmoid function and subgradients of the $\\ell_1$ norm, implement gradient descent on $\\mathbf{z}$ using the chain rule to compute $\\nabla_{\\mathbf{z}} J(\\mathbf{z};\\tau)$. Use an annealing schedule that reduces the temperature $\\tau$ multiplicatively until a minimum temperature is reached. At each temperature level, run a fixed number of gradient descent iterations on $\\mathbf{z}$ with a fixed learning rate. Initialize $\\mathbf{z}$ to the zero vector.\n\nDefine the distance of a relaxed mask to the nearest discrete value by $d(x) = \\min\\{\\,|x-0|,\\;|x-1|\\,\\}$ for $x \\in [0,1]$. For each test case, after completing the annealing schedule and gradient descent, compute the maximum distance to discreteness across coordinates, i.e., $\\max_{\\ell} d\\!\\left(m_\\ell(\\mathbf{z},\\tau_{\\text{final}})\\right)$, where $\\tau_{\\text{final}}$ is the last temperature used by your schedule. This single real number per test case will be the output to report.\n\nYour program must implement the following numerical procedure in a self-contained manner and produce results for all listed test cases:\n\n- The update uses gradient descent on $\\mathbf{z}$ at each annealing stage with temperature $\\tau$, for $T$ steps per stage, with learning rate $\\eta$.\n- The temperature schedule is $\\tau_0, \\tau_1, \\dots$ with $\\tau_{k+1} = \\gamma \\, \\tau_k$ until $\\tau_K \\le \\tau_{\\min}$, where $0 < \\gamma < 1$.\n- Use the subgradient of the $\\ell_1$ term as follows: for $m_\\ell > 0$, take $\\frac{\\partial}{\\partial m_\\ell} |m_\\ell| = 1$; at $m_\\ell = 0$, choose the subgradient $0$. This choice yields a piecewise-constant derivative with respect to $m_\\ell$ and is consistent with standard convex analysis.\n- To ensure numerical stability of $\\sigma\\!\\left(\\frac{z_\\ell}{\\tau}\\right)$, you may clamp logits $z_\\ell$ to a finite range when computing the sigmoid.\n\nTest Suite:\nProvide outputs for the following four test cases. In each case, the dimension is $L$, the utility vector is $\\mathbf{b}$, the sparsity coefficient is $\\lambda$, the initial logits are $\\mathbf{z}_0 = \\mathbf{0}$, the initial temperature is $\\tau_0$, the minimum temperature is $\\tau_{\\min}$, the multiplicative decay factor is $\\gamma$, the number of steps per temperature is $T$, and the learning rate is $\\eta$.\n\n- Case $1$ (happy path favoring discrete selections):\n  - $L = 5$\n  - $\\mathbf{b} = [\\,1.0,\\;0.3,\\;0.8,\\;0.49,\\;2.0\\,]$\n  - $\\lambda = 0.5$\n  - $\\tau_0 = 1.0$\n  - $\\tau_{\\min} = 10^{-3}$\n  - $\\gamma = 0.5$\n  - $T = 200$\n  - $\\eta = 0.5$\n\n- Case $2$ (boundary condition where $b_\\ell = \\lambda$ for all $\\ell$):\n  - $L = 3$\n  - $\\mathbf{b} = [\\,0.5,\\;0.5,\\;0.5\\,]$\n  - $\\lambda = 0.5$\n  - $\\tau_0 = 1.0$\n  - $\\tau_{\\min} = 10^{-3}$\n  - $\\gamma = 0.5$\n  - $T = 200$\n  - $\\eta = 0.5$\n\n- Case $3$ (no sparsity pressure):\n  - $L = 4$\n  - $\\mathbf{b} = [\\,0.2,\\;0.1,\\;0.9,\\;1.5\\,]$\n  - $\\lambda = 0$\n  - $\\tau_0 = 1.0$\n  - $\\tau_{\\min} = 10^{-3}$\n  - $\\gamma = 0.5$\n  - $T = 200$\n  - $\\eta = 0.5$\n\n- Case $4$ (strong sparsity with mixed utilities):\n  - $L = 3$\n  - $\\mathbf{b} = [\\,0.1,\\;3.0,\\;1.9\\,]$\n  - $\\lambda = 2.0$\n  - $\\tau_0 = 1.0$\n  - $\\tau_{\\min} = 10^{-3}$\n  - $\\gamma = 0.5$\n  - $T = 200$\n  - $\\eta = 0.5$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases, where each entry is the computed maximum distance to discreteness for that case (for example, a line like \"[x1,x2,x3,x4]\" where each $x_i$ is a float).",
            "solution": "The problem requires the implementation of a gradient-based optimization procedure for a relaxed architectural pruning problem in the context of Neural Architecture Search (NAS). We are asked to find the maximum distance to discreteness of the final relaxed masks after an annealing schedule. The problem is scientifically grounded, well-posed, and all necessary parameters are provided for a deterministic numerical simulation.\n\nThe core of the task is to perform gradient descent on the logits $\\mathbf{z}$ to minimize the objective function $J(\\mathbf{z};\\tau)$. The objective function is given by:\n$$\nJ(\\mathbf{z};\\tau) \\;=\\; -\\sum_{\\ell=1}^L b_\\ell \\, m_\\ell(\\mathbf{z}, \\tau) \\;+\\; \\lambda \\sum_{\\ell=1}^L \\big| m_\\ell(\\mathbf{z}, \\tau) \\big|\n$$\nwhere $m_\\ell(\\mathbf{z}, \\tau) = \\sigma(z_\\ell / \\tau)$ is the sigmoid-relaxed binary mask for component $\\ell$, with $\\sigma(u) = (1 + e^{-u})^{-1}$. The parameters $b_\\ell \\ge 0$ represent utility, and $\\lambda \\ge 0$ controls sparsity.\n\nTo perform gradient descent, we must compute the gradient of the objective function with respect to the logits, $\\nabla_{\\mathbf{z}} J(\\mathbf{z};\\tau)$. The objective function is a sum over the components $\\ell$, and the mask $m_\\ell$ depends only on the logit $z_\\ell$. Therefore, the gradient can be computed element-wise. The partial derivative with respect to a single logit $z_\\ell$ is found using the chain rule:\n$$\n\\frac{\\partial J}{\\partial z_\\ell} = \\frac{\\partial J}{\\partial m_\\ell} \\frac{\\partial m_\\ell}{\\partial z_\\ell}\n$$\n\nFirst, we compute the partial derivative of $J$ with respect to the mask $m_\\ell$:\n$$\n\\frac{\\partial J}{\\partial m_\\ell} = \\frac{\\partial}{\\partial m_\\ell} \\left( -b_\\ell m_\\ell + \\lambda |m_\\ell| \\right) = -b_\\ell + \\lambda \\frac{\\partial |m_\\ell|}{\\partial m_\\ell}\n$$\nThe problem specifies the subgradient for the absolute value term. Since the sigmoid function's range is $(0, 1)$, $m_\\ell$ is always strictly positive. Consequently, the derivative $\\frac{\\partial |m_\\ell|}{\\partial m_\\ell}$ is unambiguously $1$. This gives:\n$$\n\\frac{\\partial J}{\\partial m_\\ell} = -b_\\ell + \\lambda\n$$\n\nNext, we compute the partial derivative of the mask $m_\\ell$ with respect to the logit $z_\\ell$:\n$$\nm_\\ell(z_\\ell, \\tau) = \\sigma\\left(\\frac{z_\\ell}{\\tau}\\right)\n$$\nUsing the chain rule and the known derivative of the sigmoid function, $\\sigma'(u) = \\sigma(u)(1 - \\sigma(u))$, we have:\n$$\n\\frac{\\partial m_\\ell}{\\partial z_\\ell} = \\sigma'\\left(\\frac{z_\\ell}{\\tau}\\right) \\cdot \\frac{\\partial}{\\partial z_\\ell}\\left(\\frac{z_\\ell}{\\tau}\\right) = \\sigma\\left(\\frac{z_\\ell}{\\tau}\\right)\\left(1 - \\sigma\\left(\\frac{z_\\ell}{\\tau}\\right)\\right) \\cdot \\frac{1}{\\tau} = \\frac{m_\\ell(1 - m_\\ell)}{\\tau}\n$$\n\nCombining these results, the partial derivative of the objective with respect to the logit $z_\\ell$ is:\n$$\n\\frac{\\partial J}{\\partial z_\\ell} = (-b_\\ell + \\lambda) \\frac{m_\\ell(1 - m_\\ell)}{\\tau}\n$$\nThe full gradient vector $\\nabla_{\\mathbf{z}} J$ is composed of these partial derivatives for each $\\ell = 1, \\dots, L$.\n\nThe specified numerical procedure is as follows:\n1. Initialize the logits $\\mathbf{z} = \\mathbf{0}$ and the temperature $\\tau = \\tau_0$.\n2. Enter an annealing loop that continues as long as the current temperature $\\tau > \\tau_{\\min}$. Let $\\tau_{\\text{final}}$ be the last temperature value for which the loop's body is executed.\n3. Inside the annealing loop, for the current temperature $\\tau$, perform $T$ steps of gradient descent on $\\mathbf{z}$:\n$$\n\\mathbf{z} \\leftarrow \\mathbf{z} - \\eta \\nabla_{\\mathbf{z}} J(\\mathbf{z};\\tau)\n$$\nwhere $\\eta$ is the learning rate.\n4. After the inner loop of $T$ steps, update the temperature for the next stage: $\\tau \\leftarrow \\gamma \\tau$.\n5. After the annealing schedule is complete, using the final logit vector $\\mathbf{z}_{\\text{final}}$ and the last used temperature $\\tau_{\\text{final}}$, compute the final mask values:\n$$\nm_{\\ell, \\text{final}} = \\sigma\\left(\\frac{z_{\\ell, \\text{final}}}{\\tau_{\\text{final}}}\\right)\n$$\nDuring implementation, for numerical stability with small $\\tau$, the argument $z_\\ell/\\tau$ to the sigmoid function can become very large in magnitude. The `scipy.special.expit` function provides a numerically robust implementation of the sigmoid, preventing overflow/underflow issues, and is used in the solution.\n\nFinally, we calculate the distance to discreteness for each component, defined as $d(x) = \\min(|x-0|, |x-1|)$ for $x \\in [0,1]$. Since $m_{\\ell, \\text{final}} \\in (0,1)$, this simplifies to $d(m_{\\ell, \\text{final}}) = \\min(m_{\\ell, \\text{final}}, 1 - m_{\\ell, \\text{final}})$. The value to be reported for each test case is the maximum of these distances over all components $\\ell$:\n$$\n\\max_{\\ell} d(m_{\\ell, \\text{final}})\n$$\nThis procedure is implemented for each of the four test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path favoring discrete selections)\n        {\n            \"L\": 5,\n            \"b\": np.array([1.0, 0.3, 0.8, 0.49, 2.0]),\n            \"lambda\": 0.5,\n            \"tau0\": 1.0,\n            \"tau_min\": 1e-3,\n            \"gamma\": 0.5,\n            \"T\": 200,\n            \"eta\": 0.5,\n        },\n        # Case 2 (boundary condition where b_ell = lambda for all ell)\n        {\n            \"L\": 3,\n            \"b\": np.array([0.5, 0.5, 0.5]),\n            \"lambda\": 0.5,\n            \"tau0\": 1.0,\n            \"tau_min\": 1e-3,\n            \"gamma\": 0.5,\n            \"T\": 200,\n            \"eta\": 0.5,\n        },\n        # Case 3 (no sparsity pressure)\n        {\n            \"L\": 4,\n            \"b\": np.array([0.2, 0.1, 0.9, 1.5]),\n            \"lambda\": 0.0,\n            \"tau0\": 1.0,\n            \"tau_min\": 1e-3,\n            \"gamma\": 0.5,\n            \"T\": 200,\n            \"eta\": 0.5,\n        },\n        # Case 4 (strong sparsity with mixed utilities)\n        {\n            \"L\": 3,\n            \"b\": np.array([0.1, 3.0, 1.9]),\n            \"lambda\": 2.0,\n            \"tau0\": 1.0,\n            \"tau_min\": 1e-3,\n            \"gamma\": 0.5,\n            \"T\": 200,\n            \"eta\": 0.5,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(\n            L=case[\"L\"],\n            b=case[\"b\"],\n            lambda_val=case[\"lambda\"],\n            tau0=case[\"tau0\"],\n            tau_min=case[\"tau_min\"],\n            gamma=case[\"gamma\"],\n            T=case[\"T\"],\n            eta=case[\"eta\"],\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(L, b, lambda_val, tau0, tau_min, gamma, T, eta):\n    \"\"\"\n    Runs the annealing and gradient descent procedure for a single test case.\n\n    Args:\n        L (int): Dimension of the logit vector.\n        b (np.ndarray): Utility vector.\n        lambda_val (float): Sparsity coefficient.\n        tau0 (float): Initial temperature.\n        tau_min (float): Minimum temperature for the schedule.\n        gamma (float): Multiplicative decay factor for temperature.\n        T (int): Number of gradient descent steps per temperature.\n        eta (float): Learning rate for gradient descent.\n\n    Returns:\n        float: The maximum distance to discreteness.\n    \"\"\"\n    z = np.zeros(L)\n    tau = tau0\n    tau_final = tau\n\n    # Annealing loop\n    while tau > tau_min:\n        tau_final = tau\n        # Gradient descent loop for the current temperature\n        for _ in range(T):\n            # Calculate relaxed masks m using a numerically stable sigmoid\n            # m_ell = sigma(z_ell / tau)\n            m = expit(z / tau)\n\n            # Calculate gradient of J w.r.t z\n            # dJ/dm_ell = -b_ell + lambda\n            dJ_dm = -b + lambda_val\n\n            # dm/dz_ell = m_ell * (1 - m_ell) / tau\n            dm_dz = (m * (1 - m)) / tau\n\n            # Gradient via chain rule: dJ/dz = dJ/dm * dm/dz\n            grad_z = dJ_dm * dm_dz\n\n            # Update logits using gradient descent\n            z = z - eta * grad_z\n        \n        # Anneal the temperature\n        tau *= gamma\n    \n    # After all optimization, calculate the final mask with the final z and tau_final\n    final_m = expit(z / tau_final)\n\n    # Calculate the distance to the nearest discrete value (0 or 1) for each component\n    # d(x) = min(|x-0|, |x-1|) which is min(x, 1-x) for x in [0,1]\n    distances = np.minimum(final_m, 1 - final_m)\n\n    # Return the maximum distance across all components\n    return np.max(distances)\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        },
        {
            "introduction": "Evaluating every candidate architecture in a large search space is computationally infeasible, making the use of cheap performance estimators, or \"proxies,\" a cornerstone of modern NAS. However, a proxy is only useful if it reliably predicts the final performance ranking of architectures. This exercise  equips you with the statistical tools to assess proxy quality by computing rank-based correlation coefficients like Spearman's $\\rho$ and Kendall's $\\tau$. By developing a protocol to select the best proxy based on both its average correlation and its stability across different search spaces, you will gain a crucial skill for rigorously evaluating and comparing NAS methods.",
            "id": "3158046",
            "problem": "You are given multiple search spaces in the context of Neural Architecture Search (NAS). For each search space, you have a set of architectures $ \\alpha $ that share a common index ordering across all proxies and the final metric. Each architecture $ \\alpha $ is associated with a cheap proxy score $ Z(\\alpha) $ and a final outcome $ A(\\alpha) $ (for example, validation accuracy). The aim is to quantify the monotonic relationship between proxy scores and final outcomes using rank-based correlations and then to devise a principled protocol to select the most reliable proxy across spaces. The protocol must be derived from first principles and provide a scientifically grounded justification for its use. You must implement the program to compute the required correlations and select the best proxy.\n\nFundamental definitions to be used include:\n- Spearman's rank correlation coefficient $ \\rho $, defined as the Pearson correlation between ranks of two variables.\n- Kendall's rank correlation coefficient $ \\tau $, defined in terms of concordant and discordant pairs, using a tie-adjusted definition consistent with the Kendall $ \\tau_b $ variant.\n\nFor each search space $ s $ and proxy $ p $, compute $ \\rho_{p,s} $ and $ \\tau_{p,s} $ between the sequence $ Z^{(p)}_s(\\alpha) $ and $ A_s(\\alpha) $. If a correlation is undefined due to constant values (for example, all $ Z^{(p)}_s(\\alpha) $ identical), treat the value as $ 0 $.\n\nYou must then devise and implement a principled protocol to select the \"best\" proxy $ p^{\\star} $ across spaces. Your protocol must:\n- Use $ \\rho_{p,s} $ and $ \\tau_{p,s} $ across spaces as the fundamental inputs.\n- Aggregate across spaces in a way that reflects both strength of monotonic alignment and cross-space stability.\n- Output, for each test case, the index of the selected proxy along with aggregated Spearman and Kendall values for the chosen proxy.\n\nAssume proxies are ordered as listed, with indices $ 0,1,2,\\dots $.\n\nTest Suite:\n- Test Case $ 1 $ (three spaces, three proxies):\n  - Space $ 1 $:\n    - $ A_1 = [0.70,0.74,0.76,0.79,0.82,0.85] $\n    - $ Z^{(0)}_1 = [0.68,0.73,0.75,0.80,0.81,0.86] $\n    - $ Z^{(1)}_1 = [0.90,0.86,0.83,0.80,0.77,0.74] $\n    - $ Z^{(2)}_1 = [0.69,0.75,0.74,0.78,0.83,0.84] $\n  - Space $ 2 $:\n    - $ A_2 = [0.60,0.63,0.68,0.71,0.74,0.80] $\n    - $ Z^{(0)}_2 = [0.59,0.64,0.67,0.70,0.73,0.79] $\n    - $ Z^{(1)}_2 = [0.81,0.77,0.72,0.69,0.65,0.61] $\n    - $ Z^{(2)}_2 = [0.58,0.65,0.66,0.69,0.74,0.78] $\n  - Space $ 3 $:\n    - $ A_3 = [0.50,0.55,0.60,0.62,0.66,0.70] $\n    - $ Z^{(0)}_3 = [0.49,0.54,0.59,0.61,0.65,0.69] $\n    - $ Z^{(1)}_3 = [0.71,0.67,0.63,0.62,0.58,0.54] $\n    - $ Z^{(2)}_3 = [0.51,0.57,0.58,0.60,0.64,0.68] $\n- Test Case $ 2 $ (two spaces, ties and constants):\n  - Space $ 1 $:\n    - $ A_1 = [0.70,0.72,0.74,0.76,0.78,0.80] $\n    - $ Z^{(0)}_1 = [0.10,0.10,0.20,0.20,0.30,0.30] $\n    - $ Z^{(1)}_1 = [0.30,0.30,0.20,0.20,0.10,0.10] $\n    - $ Z^{(2)}_1 = [1.00,1.00,1.00,1.00,1.00,1.00] $\n  - Space $ 2 $:\n    - $ A_2 = [0.67,0.69,0.71,0.73,0.74,0.75] $\n    - $ Z^{(0)}_2 = [10.00,10.00,20.00,20.00,30.00,30.00] $\n    - $ Z^{(1)}_2 = [30.00,30.00,20.00,20.00,10.00,10.00] $\n    - $ Z^{(2)}_2 = [5.00,5.00,5.00,5.00,5.00,5.00] $\n- Test Case $ 3 $ (three spaces, stability versus strength):\n  - Space $ 1 $:\n    - $ A_1 = [0.60,0.65,0.70,0.75,0.80,0.85] $\n    - $ Z^{(0)}_1 = [1.00,2.00,3.00,4.00,5.00,6.00] $\n    - $ Z^{(1)}_1 = [0.50,1.20,1.70,2.50,3.00,3.40] $\n    - $ Z^{(2)}_1 = [6.00,5.00,4.00,3.00,2.00,1.00] $\n  - Space $ 2 $:\n    - $ A_2 = [0.50,0.55,0.58,0.62,0.66,0.72] $\n    - $ Z^{(0)}_2 = [6.00,5.00,4.00,3.00,2.00,1.00] $\n    - $ Z^{(1)}_2 = [0.30,0.60,0.90,1.10,1.50,1.80] $\n    - $ Z^{(2)}_2 = [1.00,1.00,1.00,1.00,1.00,1.00] $\n  - Space $ 3 $:\n    - $ A_3 = [0.78,0.79,0.80,0.81,0.82,0.83] $\n    - $ Z^{(0)}_3 = [2.00,3.00,1.00,5.00,6.00,7.00] $\n    - $ Z^{(1)}_3 = [0.10,0.20,0.30,0.40,0.50,0.60] $\n    - $ Z^{(2)}_3 = [10.00,9.00,8.00,7.00,6.00,5.00] $\n- Test Case $ 4 $ (two spaces, small sample size):\n  - Space $ 1 $:\n    - $ A_1 = [0.70,0.75,0.80] $\n    - $ Z^{(0)}_1 = [3.00,2.00,1.00] $\n    - $ Z^{(1)}_1 = [1.00,2.00,3.00] $\n    - $ Z^{(2)}_1 = [1.00,1.00,2.00] $\n  - Space $ 2 $:\n    - $ A_2 = [0.65,0.70,0.76] $\n    - $ Z^{(0)}_2 = [3.00,2.00,1.00] $\n    - $ Z^{(1)}_2 = [0.00,1.00,2.00] $\n    - $ Z^{(2)}_2 = [2.00,2.00,1.00] $\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case, output a nested list of the form $ [p^{\\star},\\overline{\\rho},\\overline{\\tau}] $, where $ p^{\\star} $ is the selected proxy index, $ \\overline{\\rho} $ is the aggregated Spearman value for the selected proxy across spaces, and $ \\overline{\\tau} $ is the aggregated Kendall value for the selected proxy across spaces. Express $ \\overline{\\rho} $ and $ \\overline{\\tau} $ as decimal values rounded to four decimal places.\n- Example of the required single-line format: $ [[p_1,\\overline{\\rho}_1,\\overline{\\tau}_1],[p_2,\\overline{\\rho}_2,\\overline{\\tau}_2],\\dots] $.\n\nNotes:\n- There are no physical units in this problem.\n- If any correlation is undefined due to constant input, treat it as $ 0 $ when aggregating and selecting proxies.",
            "solution": "The problem requires us to develop a principled protocol for selecting the most reliable proxy for neural architecture performance prediction across multiple search spaces. This is a common challenge in Neural Architecture Search (NAS), where cheap-to-evaluate proxies $Z(\\alpha)$ are used to estimate the final, expensive-to-measure performance $A(\\alpha)$ of an architecture $\\alpha$. A reliable proxy should exhibit a strong and consistent monotonic relationship with the final performance.\n\nThe protocol will be developed in three stages:\n1.  Quantify the monotonic relationship within each search space using rank-based correlation coefficients.\n2.  Define a scoring metric to aggregate these correlations across spaces, rewarding both strength and stability.\n3.  Establish a selection rule to identify the best proxy based on this score.\n\n## 1. Rank-Based Correlation Coefficients\n\nTo measure the monotonic relationship between the proxy scores $Z(\\alpha)$ and the final outcomes $A(\\alpha)$, we use two standard non-parametric rank correlation coefficients: Spearman's $\\rho$ and Kendall's $\\tau$.\n\n### Spearman's Rank Correlation Coefficient ($\\rho$)\n\nSpearman's $\\rho$ is the Pearson correlation coefficient applied to the rank-transformed variables. For two sequences of data $X = \\{x_1, \\dots, x_n\\}$ and $Y = \\{y_1, \\dots, y_n\\}$, let their corresponding rank sequences be $r_X$ and $r_Y$. In cases of tied values, each tied value is assigned the average of the ranks that would have been assigned had they not been tied. The coefficient $\\rho$ is then:\n$$ \\rho = \\frac{\\sum_{i=1}^{n} (r_{X,i} - \\bar{r}_X)(r_{Y,i} - \\bar{r}_Y)}{\\sqrt{\\sum_{i=1}^{n} (r_{X,i} - \\bar{r}_X)^2 \\sum_{i=1}^{n} (r_{Y,i} - \\bar{r}_Y)^2}} $$\nwhere $\\bar{r}_X$ and $\\bar{r}_Y$ are the mean ranks. $\\rho$ ranges from $-1$ (perfect negative monotonic relationship) to $+1$ (perfect positive monotonic relationship).\n\n### Kendall's Rank Correlation Coefficient ($\\tau$)\n\nKendall's $\\tau$ assesses the similarity of the orderings of data when ranked by each of the quantities. A pair of observations $(x_i, y_i)$ and $(x_j, y_j)$ is concordant if the ranks of both elements agree, i.e., if $(x_i > x_j \\text{ and } y_i > y_j)$ or $(x_i < x_j \\text{ and } y_i < y_j)$. The pair is discordant if they disagree. We use the Kendall $\\tau_b$ variant, which adjusts for ties:\n$$ \\tau_b = \\frac{N_c - N_d}{\\sqrt{(N_0 - N_1)(N_0 - N_2)}} $$\nwhere $N_c$ is the number of concordant pairs, $N_d$ is the number of discordant pairs, $N_0 = n(n-1)/2$ is the total number of pairs, $N_1 = \\sum_i t_i(t_i-1)/2$ is the number of pairs tied only in the first variable, and $N_2 = \\sum_j u_j(u_j-1)/2$ is the number of pairs tied only in the second variable. $t_i$ and $u_j$ are the number of tied values in each tie group for the first and second variables, respectively. $\\tau_b$ also ranges from $-1$ to $+1$.\n\nAs per the problem specification, if a correlation is undefined due to one of the input vectors being constant (which results in zero variance of ranks and division by zero), its value will be treated as $0$.\n\n## 2. Principled Protocol for Proxy Selection\n\nA good proxy is not only one that has a high average correlation with the true outcomes but one that is also stable and reliable across different search spaces. A proxy that works perfectly on one space but is uncorrelated or anti-correlated on another is less useful than a proxy that is moderately well-correlated on all spaces. Our protocol must capture this trade-off between strength (average performance) and stability (consistency).\n\nFor each proxy $p$, we compute a set of correlation values across $S$ search spaces: $\\{\\rho_{p,s}\\}_{s=1}^S$ and $\\{\\tau_{p,s}\\}_{s=1}^S$.\n\n### Aggregation and Scoring\n\nTo balance strength and stability, we can formulate a score that rewards high mean correlation and penalizes high variance (or standard deviation) in correlation. A scientifically principled way to do this is to use a metric analogous to a lower confidence bound on the performance.\n\nFor a proxy $p$, we define its stability-adjusted score for Spearman's $\\rho$ as:\n$$ \\mathcal{S}_{\\rho}(p) = \\mu_{\\rho,p} - \\sigma_{\\rho,p} $$\nwhere $\\mu_{\\rho,p}$ is the mean of the Spearman correlations across all spaces, and $\\sigma_{\\rho,p}$ is the population standard deviation. The population standard deviation is used because we are evaluating performance over the complete, given set of spaces, not a sample.\n\nSimilarly, the score for Kendall's $\\tau$ is:\n$$ \\mathcal{S}_{\\tau}(p) = \\mu_{\\tau,p} - \\sigma_{\\tau,p} $$\n\nTo arrive at a single metric for selecting the best proxy, we combine these two scores. Since both $\\rho$ and $\\tau$ measure the same underlying property (monotonic association), it is reasonable to give them equal weight. We define the final score $\\mathcal{S}(p)$ for proxy $p$ as the average of its stability-adjusted scores:\n$$ \\mathcal{S}(p) = \\frac{1}{2} \\left( \\mathcal{S}_{\\rho}(p) + \\mathcal{S}_{\\tau}(p) \\right) = \\frac{1}{2} \\left[ (\\mu_{\\rho,p} - \\sigma_{\\rho,p}) + (\\mu_{\\tau,p} - \\sigma_{\\tau,p}) \\right] $$\n\n### Selection Rule\n\nThe best proxy, denoted $p^\\star$, is the one that maximizes this overall score. This proxy is considered the most reliable because it provides the best-guaranteed level of performance, taking into account its variability across different contexts.\n$$ p^\\star = \\arg\\max_{p} \\mathcal{S}(p) $$\n\nThe final output for each test case will be the index of the selected proxy, $p^\\star$, along with its mean Spearman ($\\overline{\\rho} = \\mu_{\\rho, p^\\star}$) and mean Kendall ($\\overline{\\tau} = \\mu_{\\tau, p^\\star}$) correlations across the spaces, rounded to four decimal places. These mean values represent the \"strength\" component of the chosen proxy's performance.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import spearmanr, kendalltau\n\ndef solve():\n    \"\"\"\n    Solves the Neural Architecture Search (NAS) proxy selection problem.\n    \"\"\"\n    test_cases = [\n        # Test Case 1: Three spaces, three proxies\n        (\n            [ # A_by_space\n                [0.70, 0.74, 0.76, 0.79, 0.82, 0.85],\n                [0.60, 0.63, 0.68, 0.71, 0.74, 0.80],\n                [0.50, 0.55, 0.60, 0.62, 0.66, 0.70],\n            ],\n            [ # Z_by_proxy_then_space\n                [[0.68, 0.73, 0.75, 0.80, 0.81, 0.86], [0.59, 0.64, 0.67, 0.70, 0.73, 0.79], [0.49, 0.54, 0.59, 0.61, 0.65, 0.69]], # Proxy 0\n                [[0.90, 0.86, 0.83, 0.80, 0.77, 0.74], [0.81, 0.77, 0.72, 0.69, 0.65, 0.61], [0.71, 0.67, 0.63, 0.62, 0.58, 0.54]], # Proxy 1\n                [[0.69, 0.75, 0.74, 0.78, 0.83, 0.84], [0.58, 0.65, 0.66, 0.69, 0.74, 0.78], [0.51, 0.57, 0.58, 0.60, 0.64, 0.68]], # Proxy 2\n            ]\n        ),\n        # Test Case 2: Two spaces, ties and constants\n        (\n            [ # A_by_space\n                [0.70, 0.72, 0.74, 0.76, 0.78, 0.80],\n                [0.67, 0.69, 0.71, 0.73, 0.74, 0.75],\n            ],\n            [ # Z_by_proxy_then_space\n                [[0.10, 0.10, 0.20, 0.20, 0.30, 0.30], [10.00, 10.00, 20.00, 20.00, 30.00, 30.00]], # Proxy 0\n                [[0.30, 0.30, 0.20, 0.20, 0.10, 0.10], [30.00, 30.00, 20.00, 20.00, 10.00, 10.00]], # Proxy 1\n                [[1.00, 1.00, 1.00, 1.00, 1.00, 1.00], [5.00, 5.00, 5.00, 5.00, 5.00, 5.00]],       # Proxy 2\n            ]\n        ),\n        # Test Case 3: Three spaces, stability versus strength\n        (\n            [ # A_by_space\n                [0.60, 0.65, 0.70, 0.75, 0.80, 0.85],\n                [0.50, 0.55, 0.58, 0.62, 0.66, 0.72],\n                [0.78, 0.79, 0.80, 0.81, 0.82, 0.83],\n            ],\n            [ # Z_by_proxy_then_space\n                [[1.00, 2.00, 3.00, 4.00, 5.00, 6.00], [6.00, 5.00, 4.00, 3.00, 2.00, 1.00], [2.00, 3.00, 1.00, 5.00, 6.00, 7.00]], # Proxy 0\n                [[0.50, 1.20, 1.70, 2.50, 3.00, 3.40], [0.30, 0.60, 0.90, 1.10, 1.50, 1.80], [0.10, 0.20, 0.30, 0.40, 0.50, 0.60]], # Proxy 1\n                [[6.00, 5.00, 4.00, 3.00, 2.00, 1.00], [1.00, 1.00, 1.00, 1.00, 1.00, 1.00], [10.00, 9.00, 8.00, 7.00, 6.00, 5.00]], # Proxy 2\n            ]\n        ),\n        # Test Case 4: Two spaces, small sample size\n        (\n            [ # A_by_space\n                [0.70, 0.75, 0.80],\n                [0.65, 0.70, 0.76],\n            ],\n            [ # Z_by_proxy_then_space\n                [[3.00, 2.00, 1.00], [3.00, 2.00, 1.00]], # Proxy 0\n                [[1.00, 2.00, 3.00], [0.00, 1.00, 2.00]], # Proxy 1\n                [[1.00, 1.00, 2.00], [2.00, 2.00, 1.00]], # Proxy 2\n            ]\n        ),\n    ]\n\n    all_results = []\n    \n    for A_by_space, Z_by_proxy_then_space in test_cases:\n        num_proxies = len(Z_by_proxy_then_space)\n        num_spaces = len(A_by_space)\n        \n        proxy_scores = []\n        proxy_rhos = []\n        proxy_taus = []\n\n        for p_idx in range(num_proxies):\n            rhos = []\n            taus = []\n            \n            for s_idx in range(num_spaces):\n                A_s = A_by_space[s_idx]\n                Z_ps = Z_by_proxy_then_space[p_idx][s_idx]\n\n                # Calculate Spearman's rho\n                rho, _ = spearmanr(A_s, Z_ps)\n                rho = np.nan_to_num(rho, nan=0.0)\n                rhos.append(rho)\n                \n                # Calculate Kendall's tau_b\n                tau, _ = kendalltau(A_s, Z_ps, variant='b')\n                tau = np.nan_to_num(tau, nan=0.0)\n                taus.append(tau)\n            \n            # Calculate mean and std dev for rho and tau across spaces\n            mu_rho = np.mean(rhos)\n            sigma_rho = np.std(rhos, ddof=0)\n            mu_tau = np.mean(taus)\n            sigma_tau = np.std(taus, ddof=0)\n            \n            # Calculate the stability-adjusted score\n            score = 0.5 * ((mu_rho - sigma_rho) + (mu_tau - sigma_tau))\n            \n            proxy_scores.append(score)\n            proxy_rhos.append(rhos)\n            proxy_taus.append(taus)\n\n        # Select the best proxy\n        p_star = np.argmax(proxy_scores)\n        \n        # Get the aggregated (mean) values for the selected proxy\n        mu_rho_star = np.mean(proxy_rhos[p_star])\n        mu_tau_star = np.mean(proxy_taus[p_star])\n\n        all_results.append([p_star, mu_rho_star, mu_tau_star])\n\n    # Format the final output string\n    formatted_results = []\n    for res in all_results:\n        p, rho, tau = res\n        rho_str = f\"{rho:.4f}\"\n        tau_str = f\"{tau:.4f}\"\n        formatted_results.append(f\"[{p},{rho_str},{tau_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}