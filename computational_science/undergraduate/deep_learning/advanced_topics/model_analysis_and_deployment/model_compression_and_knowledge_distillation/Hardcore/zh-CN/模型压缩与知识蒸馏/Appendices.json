{
    "hands_on_practices": [
        {
            "introduction": "探索模型压缩技术的第一步是理解其能带来的实际效益。本练习提供了一个具体的量化视角，通过一个简化但贴近现实的能耗模型，向你展示剪枝和量化等技术如何转化为硬件上显著的能耗节省。完成此练习能帮助你直观地理解为何模型压缩是部署高效人工智能的关键。",
            "id": "3152867",
            "problem": "一个用于推理的深度神经网络在硬件上执行，其每次推理的能耗可以通过一个关于计算和内存流量的线性模型来近似。设每次推理的能耗模型为 $E = a \\cdot \\mathrm{FLOPs} + b \\cdot \\mathrm{Mem}$，其中 $\\mathrm{FLOPs}$ 是每次推理的算术运算次数（此处将浮点运算 (FLOPs) 视为通用算术运算），$\\mathrm{Mem}$ 是每次推理访问的总字节数，$a$ 是每次运算的能耗，$b$ 是每访问字节的能耗。考虑一个使用$32$位浮点表示的基准“教师”模型，以及一个通过知识蒸馏和剪枝获得，然后量化为用于推理的$8$位整数 (int8) 的压缩“学生”模型。使用以下基于充分验证的观察得出的科学上合理且一致的假设：\n- 教师模型在$32$位精度下，每次推理执行 $3.2 \\times 10^{9}$ 次运算，并访问 $1.25 \\times 10^{8}$ 字节。\n- $32$位下的每次运算能耗为 $a_{\\mathrm{fp32}} = 2.0 \\times 10^{-12}$ 焦耳/次运算，每访问字节的能耗为 $b = 2.5 \\times 10^{-10}$ 焦耳/字节。\n- 学生模型由于知识蒸馏和剪枝，其运算次数减少到教师模型的 $40\\%$，内存流量（在相同精度下，以字节为单位）减少到教师模型的 $30\\%$。\n- 量化到$8$位 (int8) 将每次运算的能耗降低到 $a_{\\mathrm{int8}} = 5.0 \\times 10^{-13}$ 焦耳/次运算。每字节的内存能耗 $b$ 保持不变，但访问的总字节数随表示位宽缩放，因此从$32$位变为$8$位会使权重和激活的访问字节数减少4倍。\n使用这些基本定义和假设，推导从$32$位教师模型推理转换到int8学生模型推理的能耗节省分数，定义为\n$$ S \\equiv \\frac{E_{\\mathrm{teacher,fp32}} - E_{\\mathrm{student,int8}}}{E_{\\mathrm{teacher,fp32}}}. $$\n计算 $S$ 并将最终答案表示为无单位小数。将您的答案四舍五入到四位有效数字。",
            "solution": "根据指定标准评估问题陈述的有效性。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n- 每次推理的能耗模型：$E = a \\cdot \\mathrm{FLOPs} + b \\cdot \\mathrm{Mem}$\n- $a$：每次运算的能耗（焦耳/次运算）\n- $b$：每访问字节的能耗（焦耳/字节）\n- 教师模型参数（32位浮点，fp32）：\n  - $\\mathrm{FLOPs}_{\\mathrm{teacher}} = 3.2 \\times 10^9$\n  - $\\mathrm{Mem}_{\\mathrm{teacher,fp32}} = 1.25 \\times 10^8$ 字节\n- fp32的能耗系数：\n  - $a_{\\mathrm{fp32}} = 2.0 \\times 10^{-12}$ 焦耳/次运算\n  - $b = 2.5 \\times 10^{-10}$ 焦耳/字节\n- 学生模型推导（结构性变化）：\n  - $\\mathrm{FLOPs}_{\\mathrm{student}} = 0.40 \\cdot \\mathrm{FLOPs}_{\\mathrm{teacher}}$\n  - 假设的32位学生模型的内存流量是教师模型内存流量（以字节计）的 $0.30$。\n- 学生模型推导（量化为8位整数，int8）：\n  - 每次运算的能耗：$a_{\\mathrm{int8}} = 5.0 \\times 10^{-13}$ 焦耳/次运算\n  - 每字节的内存能耗 $b$ 不变。\n  - 与32位表示相比，访问的总字节数减少了4倍。\n- 目标量：能耗节省分数，$S \\equiv \\frac{E_{\\mathrm{teacher,fp32}} - E_{\\mathrm{student,int8}}}{E_{\\mathrm{teacher,fp32}}}$\n- 最终答案要求：无单位小数，四舍五入到四位有效数字。\n\n**步骤 2：使用提取的已知条件进行验证**\n对问题的科学依据、适定性和客观性进行评估。\n- **科学依据**：该问题基于深度学习模型压缩和硬件效率这一成熟领域。能耗的线性模型是一种标准且有效的一阶近似。指定的运算、内存和能耗系数值对于现代计算硬件而言是科学上合理的。所描述的剪枝和量化效应是标准技术，它们对FLOPs、内存和能耗的影响被正确地建模。\n- **适定性**：该问题提供了计算唯一数值解所需的所有数据和定义。教师模型和学生模型之间的关系被明确且无歧义地定义。\n- **客观性**：该问题以精确的、定量的术语陈述，没有主观或推测性语言。\n\n**步骤 3：结论与行动**\n该问题是**有效的**，因为它科学合理、适定、客观且内部一致。可以推导出严谨的解。\n\n### 解题推导\n\n分析过程首先计算教师模型的能耗，然后确定学生模型的参数和能耗，最后计算能耗节省分数。\n\n每次推理的能耗由线性模型给出：\n$$ E = a \\cdot \\mathrm{FLOPs} + b \\cdot \\mathrm{Mem} $$\n\n**1. 教师模型的能耗 ($E_{\\mathrm{teacher,fp32}}$)**\n对于在$32$位精度（fp32）下运行的教师模型，已知条件是：\n- $\\mathrm{FLOPs}_{\\mathrm{teacher}} = 3.2 \\times 10^9$\n- $\\mathrm{Mem}_{\\mathrm{teacher,fp32}} = 1.25 \\times 10^8$ 字节\n- $a_{\\mathrm{fp32}} = 2.0 \\times 10^{-12}$ 焦耳/次运算\n- $b = 2.5 \\times 10^{-10}$ 焦耳/字节\n\n教师模型的总能耗是计算能耗和内存能耗的总和：\n$$ E_{\\mathrm{teacher,fp32}} = a_{\\mathrm{fp32}} \\cdot \\mathrm{FLOPs}_{\\mathrm{teacher}} + b \\cdot \\mathrm{Mem}_{\\mathrm{teacher,fp32}} $$\n代入数值：\n$$ E_{\\mathrm{teacher,fp32}} = (2.0 \\times 10^{-12}) \\cdot (3.2 \\times 10^9) + (2.5 \\times 10^{-10}) \\cdot (1.25 \\times 10^8) $$\n$$ E_{\\mathrm{teacher,fp32}} = 6.4 \\times 10^{-3} \\, \\mathrm{J} + 3.125 \\times 10^{-2} \\, \\mathrm{J} $$\n$$ E_{\\mathrm{teacher,fp32}} = 0.0064 \\, \\mathrm{J} + 0.03125 \\, \\mathrm{J} = 0.03765 \\, \\mathrm{J} $$\n\n**2. 学生模型的参数和能耗 ($E_{\\mathrm{student,int8}}$)**\n学生模型的参数由教师模型推导得出。\n首先，剪枝和知识蒸馏的效果在结构上减少了运算次数和内存流量。\n- $\\mathrm{FLOPs}_{\\mathrm{student}} = 0.40 \\cdot \\mathrm{FLOPs}_{\\mathrm{teacher}} = 0.40 \\cdot (3.2 \\times 10^9) = 1.28 \\times 10^9$ 次运算。\n\n其次，内存流量受到结构性剪枝和量化的双重影响。剪枝将内存流量（访问的参数/激活数量）减少到教师模型的 $30\\%$，这是在相同精度下测量的。让我们将一个假设的32位学生模型的内存流量表示为 $\\mathrm{Mem}_{\\mathrm{student,fp32}}$。\n$$ \\mathrm{Mem}_{\\mathrm{student,fp32}} = 0.30 \\cdot \\mathrm{Mem}_{\\mathrm{teacher,fp32}} = 0.30 \\cdot (1.25 \\times 10^8) = 3.75 \\times 10^7 \\, \\mathrm{bytes} $$\n接下来，从$32$位到$8$位的量化将每个访问元素的数据大小减少了 $\\frac{32}{8} = 4$ 倍。因此，8位学生模型的最终内存流量 $\\mathrm{Mem}_{\\mathrm{student,int8}}$ 为：\n$$ \\mathrm{Mem}_{\\mathrm{student,int8}} = \\frac{\\mathrm{Mem}_{\\mathrm{student,fp32}}}{4} = \\frac{3.75 \\times 10^7}{4} = 9.375 \\times 10^6 \\, \\mathrm{bytes} $$\n\nint8学生模型的能耗系数为：\n- $a_{\\mathrm{int8}} = 5.0 \\times 10^{-13}$ 焦耳/次运算\n- $b = 2.5 \\times 10^{-10}$ 焦耳/字节 (不变)\n\n学生模型的总能耗是：\n$$ E_{\\mathrm{student,int8}} = a_{\\mathrm{int8}} \\cdot \\mathrm{FLOPs}_{\\mathrm{student}} + b \\cdot \\mathrm{Mem}_{\\mathrm{student,int8}} $$\n代入数值：\n$$ E_{\\mathrm{student,int8}} = (5.0 \\times 10^{-13}) \\cdot (1.28 \\times 10^9) + (2.5 \\times 10^{-10}) \\cdot (9.375 \\times 10^6) $$\n$$ E_{\\mathrm{student,int8}} = 6.4 \\times 10^{-4} \\, \\mathrm{J} + 2.34375 \\times 10^{-3} \\, \\mathrm{J} $$\n$$ E_{\\mathrm{student,int8}} = 0.00064 \\, \\mathrm{J} + 0.00234375 \\, \\mathrm{J} = 0.00298375 \\, \\mathrm{J} $$\n\n**3. 能耗节省分数 (S)**\n能耗节省分数 $S$ 定义为：\n$$ S = \\frac{E_{\\mathrm{teacher,fp32}} - E_{\\mathrm{student,int8}}}{E_{\\mathrm{teacher,fp32}}} = 1 - \\frac{E_{\\mathrm{student,int8}}}{E_{\\mathrm{teacher,fp32}}} $$\n代入计算出的能耗值：\n$$ S = 1 - \\frac{0.00298375}{0.03765} $$\n$$ S = 1 - 0.079249667... $$\n$$ S = 0.920750332... $$\n根据要求将结果四舍五入到四位有效数字，得到：\n$$ S \\approx 0.9208 $$\n这表示每次推理的能耗减少了 $92.08\\%$。",
            "answer": "$$\n\\boxed{0.9208}\n$$"
        },
        {
            "introduction": "了解了模型压缩的动机后，我们将深入探讨“如何”实现它，特别是网络剪枝中的关键细节。这项练习挑战了“稀疏度等同于加速”的朴素观念，通过一个考虑了结构性开销的硬件模型，让你亲手验证不同剪枝模式的实际效率。通过该实践 ，你将深刻理解非结构化与结构化稀疏性的差异，并认识到为何硬件友好的设计对实现真实性能增益至关重要。",
            "id": "3152881",
            "problem": "给定一个用于单全连接神经网络层的推理的假设执行模型，其权重矩阵为 $W \\in \\mathbb{R}^{m \\times n}$，输入向量为 $x \\in \\mathbb{R}^{n}$。该模型旨在比较剪枝引起的稀疏模式与硬件友好结构，并衡量实际速度增益与理论上由稀疏性带来的增益。您必须使用的基本依据是矩阵向量乘法的操作计数原理：稠密计算 $y = W x$ 执行大约 $m n$ 次乘法累加步骤，在此模型中，每一步都被视为一个恒定成本的单位。稀疏性减少了算术运算的数量，但硬件成本也包括依赖于模式结构的非算术开销。\n\n您必须实现一个程序，为每个指定的测试用例，根据剪枝模式构建 $W$ 的二进制非零掩码，计算理论和实际加速比，并汇总实现的效率比。理论加速比假设算术工作量与非零条目的数量成完美比例，且开销可以忽略不计。在给定的硬件模型下，实际速度包括开销，并取决于稀疏性的结构。\n\n定义和执行模型：\n\n- 浮点运算（FLOPs）定义：对于稠密矩阵向量乘法，算术工作量与 $m n$ 成正比。在此模型中，其时间为 $T_{\\text{dense}} = \\alpha \\cdot m n$，其中 $\\alpha$ 是一个常数，它聚合了稠密计算中每个操作的成本和内存效应。\n\n- 稀疏计算模型：对于一个具有 $s$ 个非零条目的权重矩阵，实际稀疏时间被建模为\n$$\nT_{\\text{sparse}} = \\beta \\cdot s + \\gamma \\cdot B,\n$$\n其中 $\\beta$ 是稀疏模式下每个非零算术运算的成本，$\\gamma$ 是每个已处理结构单元的开销，而 $B$ 是由稀疏模式决定的已处理结构单元的数量：\n    - 非结构化剪枝：每个非零元都是一个单元；$B = s$。\n    - 块大小为 $b \\times b$ 的块剪枝：每个保留的块都是一个单元；$B$ 等于保留块的数量（假设块要么完全保留，要么完全剪除）。\n    - 行剪枝：每个保留的行都是一个单元；$B$ 等于保留行的数量。\n    - 列剪枝：每个保留的列都是一个单元；$B$ 等于保留列的数量。\n\n- 忽略开销的理想理论时间是\n$$\nT_{\\text{ideal}} = \\beta \\cdot s,\n$$\n这仅由稀疏性产生理论加速比。\n\n稀疏模式和掩码构建规则：\n\n- 目标稀疏度为 $p$ 的非结构化剪枝：均匀随机选择恰好 $\\lfloor (1 - p) \\cdot m n \\rfloor$ 个条目保留为非零；所有其他条目均为零。\n\n- 目标稀疏度为 $p$ 且块大小为 $b \\times b$ 的块剪枝：将 $W$ 分割为 $\\lfloor m / b \\rfloor \\times \\lfloor n / b \\rfloor$ 个大小为 $b \\times b$ 的非重叠块（如果 $m$ 或 $n$ 不是 $b$ 的倍数，则忽略任何余数）。均匀随机选择并保留恰好 $\\lfloor (1 - p) \\cdot \\lfloor m / b \\rfloor \\cdot \\lfloor n / b \\rfloor \\rfloor$ 个块；保留块内的所有条目都为非零；剪除块内的所有条目都为零。\n\n- 目标稀疏度为 $p$ 的行剪枝：均匀随机选择并保留恰好 $\\lfloor (1 - p) \\cdot m \\rfloor$ 行，保留行中的所有条目为非零，剪除行中的所有条目为零。\n\n- 目标稀疏度为 $p$ 的列剪枝：均匀随机选择并保留恰好 $\\lfloor (1 - p) \\cdot n \\rfloor$ 列，保留列中的所有条目为非零，剪除列中的所有条目为零。\n\n要计算的加速比定义：\n\n- 理论加速比：\n$$\n\\text{speedup}_{\\text{theory}} = \\frac{T_{\\text{dense}}}{T_{\\text{ideal}}} = \\frac{\\alpha \\cdot m n}{\\beta \\cdot s}.\n$$\n\n- 硬件模型下的实际加速比：\n$$\n\\text{speedup}_{\\text{actual}} = \\frac{T_{\\text{dense}}}{T_{\\text{sparse}}} = \\frac{\\alpha \\cdot m n}{\\beta \\cdot s + \\gamma \\cdot B}.\n$$\n\n- 比较实际速度增益与理论稀疏性增益的实现效率比：\n$$\n\\rho = \\frac{\\text{speedup}_{\\text{actual}}}{\\text{speedup}_{\\text{theory}}}.\n$$\n\n执行模型的常量（使用这些精确值）：\n- $\\alpha = 1.0$，\n- $\\beta = 1.2$，\n- $\\gamma = 5.0$。\n\n随机性要求：使用固定的随机种子 $42$ 以确保掩码构建的可复现性。\n\n测试套件（五个探测稀疏性和结构不同方面的案例）：\n\n1. $m = 512$，$n = 512$，$p = 0.9$，非结构化剪枝，$b$不适用。\n2. $m = 512$，$n = 512$，$p = 0.9$，块剪枝，块大小 $b = 4$。\n3. $m = 512$，$n = 512$，$p = 0.9$，行剪枝，$b$不适用。\n4. $m = 1024$，$n = 256$，$p = 0.5$，列剪枝，$b$不适用。\n5. $m = 256$，$n = 256$，$p = 0.5$，非结构化剪枝，$b$不适用。\n\n您的程序必须：\n- 遵循上述规则为每个案例构建非零掩码。\n- 为每个案例计算 $s$、$B$、$T_{\\text{dense}}$、$T_{\\text{ideal}}$、$T_{\\text{sparse}}$、$\\text{speedup}_{\\text{theory}}$、$\\text{speedup}_{\\text{actual}}$ 和 $\\rho$。\n- 生成单行输出，其中包含所有测试用例的实现效率比 $\\rho$，格式为用方括号括起来的逗号分隔列表，例如 $[\\rho_1,\\rho_2,\\rho_3,\\rho_4,\\rho_5]$。不应打印任何额外文本。\n\n您的推导中的所有数学实体和数字都必须遵守所提供的定义和指定的执行模型。不涉及物理单位；所有时间和加速比都是无量纲的量。通过严格遵循操作计数原理和依赖于模式的开销模型来确保科学真实性。最终输出必须是实值浮点数。",
            "solution": "问题陈述已经过验证，并被确定为是合理的。它在科学上基于神经网络中稀疏矩阵运算的计算成本建模原理，定义明确，包含了所有必要的定义和常数，并且表述客观。我们可以开始求解。\n\n主要目标是为五种不同的权重矩阵剪枝场景计算实现效率比 $\\rho$。该比率比较了在假设硬件上实现的实际加速比与仅通过减少非零参数数量所期望的理论加速比。$\\rho$ 的公式由下式给出：\n$$\n\\rho = \\frac{\\text{speedup}_{\\text{actual}}}{\\text{speedup}_{\\text{theory}}}\n$$\n代入加速比的定义可以得到一个更直接的计算公式。\n$$\n\\text{speedup}_{\\text{actual}} = \\frac{T_{\\text{dense}}}{T_{\\text{sparse}}} = \\frac{\\alpha \\cdot m n}{\\beta \\cdot s + \\gamma \\cdot B}\n$$\n$$\n\\text{speedup}_{\\text{theory}} = \\frac{T_{\\text{dense}}}{T_{\\text{ideal}}} = \\frac{\\alpha \\cdot m n}{\\beta \\cdot s}\n$$\n其中，$s$ 是非零条目的数量，$B$ 是已处理结构单元的数量，$m$ 和 $n$ 是矩阵维度，$\\alpha$、$\\beta$、$\\gamma$ 是成本常数。\n\n比率 $\\rho$ 可以简化为：\n$$\n\\rho = \\frac{\\frac{\\alpha \\cdot m n}{\\beta \\cdot s + \\gamma \\cdot B}}{\\frac{\\alpha \\cdot m n}{\\beta \\cdot s}} = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot B} = \\frac{1}{1 + \\frac{\\gamma \\cdot B}{\\beta \\cdot s}}\n$$\n这个简化形式揭示了实现效率取决于结构开销成本（$\\gamma \\cdot B$）与算术成本（$\\beta \\cdot s$）的比率。我们将使用提供的常数 $\\beta = 1.2$ 和 $\\gamma = 5.0$。问题为每种剪枝类型指定了确定 $s$ 和 $B$ 的规则，这些是基于给定参数的确定性计算。注意到了对随机种子的要求，但由于量 $s$ 和 $B$ 是由基于计数的确定性公式定义的，具体的随机选择不会改变 $\\rho$ 的最终值。\n\n我们现在将为每个测试用例计算 $\\rho$。\n\n**案例1：非结构化剪枝**\n- 参数：$m = 512$，$n = 512$，目标稀疏度 $p = 0.9$。\n- 矩阵中的总条目数为 $m \\cdot n = 512 \\cdot 512 = 262144$。\n- 要保留的非零条目数为 $s = \\lfloor (1 - p) \\cdot m n \\rfloor = \\lfloor (1 - 0.9) \\cdot 262144 \\rfloor = \\lfloor 26214.4 \\rfloor = 26214$。\n- 对于非结构化剪枝，每个非零条目都是一个结构单元，所以 $B = s = 26214$。\n- 效率比 $\\rho_1$ 为：\n$$\n\\rho_1 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot s} = \\frac{\\beta}{\\beta + \\gamma} = \\frac{1.2}{1.2 + 5.0} = \\frac{1.2}{6.2} \\approx 0.193548\n$$\n\n**案例2：块剪枝**\n- 参数：$m = 512$，$n = 512$，$p = 0.9$，块大小 $b=4$。\n- 矩阵被划分为大小为 $4 \\times 4$ 的非重叠块。\n- 块数：$(\\lfloor 512/4 \\rfloor) \\times (\\lfloor 512/4 \\rfloor) = 128 \\times 128 = 16384$。\n- 要保留的块数即结构单元数：$B = \\lfloor (1 - p) \\cdot 16384 \\rfloor = \\lfloor 0.1 \\cdot 16384 \\rfloor = \\lfloor 1638.4 \\rfloor = 1638$。\n- 非零条目的总数为 $s = B \\cdot b^2 = 1638 \\cdot 4^2 = 1638 \\cdot 16 = 26208$。\n- 效率比 $\\rho_2$ 为：\n$$\n\\rho_2 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot B} = \\frac{1.2 \\cdot 26208}{1.2 \\cdot 26208 + 5.0 \\cdot 1638} = \\frac{31449.6}{31449.6 + 8190} = \\frac{31449.6}{39639.6} \\approx 0.793388\n$$\n\n**案例3：行剪枝**\n- 参数：$m = 512$，$n = 512$，$p = 0.9$。\n- 要保留的行数即结构单元数：$B = \\lfloor (1 - p) \\cdot m \\rfloor = \\lfloor (1 - 0.9) \\cdot 512 \\rfloor = \\lfloor 51.2 \\rfloor = 51$。\n- 非零条目的总数为 $s = B \\cdot n = 51 \\cdot 512 = 26112$。\n- 效率比 $\\rho_3$ 为：\n$$\n\\rho_3 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot B} = \\frac{1.2 \\cdot 26112}{1.2 \\cdot 26112 + 5.0 \\cdot 51} = \\frac{31334.4}{31334.4 + 255} = \\frac{31334.4}{31589.4} \\approx 0.991928\n$$\n\n**案例4：列剪枝**\n- 参数：$m = 1024$，$n = 256$，$p = 0.5$。\n- 要保留的列数即结构单元数：$B = \\lfloor (1 - p) \\cdot n \\rfloor = \\lfloor (1 - 0.5) \\cdot 256 \\rfloor = \\lfloor 128 \\rfloor = 128$。\n- 非零条目的总数为 $s = B \\cdot m = 128 \\cdot 1024 = 131072$。\n- 效率比 $\\rho_4$ 为：\n$$\n\\rho_4 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot B} = \\frac{1.2 \\cdot 131072}{1.2 \\cdot 131072 + 5.0 \\cdot 128} = \\frac{157286.4}{157286.4 + 640} = \\frac{157286.4}{157926.4} \\approx 0.995947\n$$\n\n**案例5：非结构化剪枝**\n- 参数：$m = 256$，$n = 256$，$p = 0.5$。\n- 矩阵中的总条目数为 $m \\cdot n = 256 \\cdot 256 = 65536$。\n- 要保留的非零条目数为 $s = \\lfloor (1 - p) \\cdot m n \\rfloor = \\lfloor (1 - 0.5) \\cdot 65536 \\rfloor = \\lfloor 32768 \\rfloor = 32768$。\n- 对于非结构化剪枝，$B = s = 32768$。\n- 效率比 $\\rho_5$ 为：\n$$\n\\rho_5 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot s} = \\frac{\\beta}{\\beta + \\gamma} = \\frac{1.2}{1.2 + 5.0} = \\frac{1.2}{6.2} \\approx 0.193548\n$$\n这些计算表明，结构化剪枝方法（块、行、列）的效率要高得多，因为它们相对于算术项 $\\beta s$ 极大地减少了开销项 $\\gamma B$，从而导致 $\\rho$ 值接近于 1。在此模型中，非结构化剪枝的效率非常低，因为开销与非零元素的数量成正比，使得开销成本占主导地位。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the realized efficiency ratio for different neural network pruning strategies\n    based on a given hardware execution model.\n    \"\"\"\n    # Constants for the execution model from the problem statement\n    beta = 1.2\n    gamma = 5.0\n\n    # Test suite defined in the problem\n    test_cases = [\n        {'m': 512, 'n': 512, 'p': 0.9, 'type': 'unstructured', 'b': None},\n        {'m': 512, 'n': 512, 'p': 0.9, 'type': 'block', 'b': 4},\n        {'m': 512, 'n': 512, 'p': 0.9, 'type': 'row', 'b': None},\n        {'m': 1024, 'n': 256, 'p': 0.5, 'type': 'column', 'b': None},\n        {'m': 256, 'n': 256, 'p': 0.5, 'type': 'unstructured', 'b': None},\n    ]\n\n    results = []\n\n    # Although the problem mentions a random seed, the quantities s and B are\n    # determined by deterministic formulas. No random sampling is needed to compute them.\n    # rng = np.random.default_rng(42)\n\n    for case in test_cases:\n        m, n, p, pruning_type, b = case['m'], case['n'], case['p'], case['type'], case['b']\n        s = 0  # Number of non-zero entries\n        B = 0  # Number of processed structural units\n\n        if pruning_type == 'unstructured':\n            # For unstructured pruning, each non-zero element is a structural unit.\n            s = int(np.floor((1 - p) * m * n))\n            B = s\n        elif pruning_type == 'block':\n            # For block pruning, each kept block is a structural unit.\n            num_blocks_m = m // b\n            num_blocks_n = n // b\n            total_blocks = num_blocks_m * num_blocks_n\n            B = int(np.floor((1 - p) * total_blocks))\n            s = B * (b**2)\n        elif pruning_type == 'row':\n            # For row pruning, each kept row is a structural unit.\n            B = int(np.floor((1 - p) * m))\n            s = B * n\n        elif pruning_type == 'column':\n            # For column pruning, each kept column is a structural unit.\n            B = int(np.floor((1 - p) * n))\n            s = B * m\n            \n        # The realized efficiency ratio rho is defined as:\n        # rho = speedup_actual / speedup_theory\n        # which simplifies to rho = (beta * s) / (beta * s + gamma * B)\n        \n        # Handle the case where s could be zero to avoid division by zero.\n        # Based on the test cases, s will always be positive.\n        if (beta * s + gamma * B) == 0:\n             # This case happens if s=0 and B=0, meaning the matrix is empty, \n             # pruned completely. The concept of speedup is not well-defined.\n             # We can define rho as 1.0, as there is no overhead.\n            rho = 1.0\n        else:\n            rho = (beta * s) / (beta * s + gamma * B)\n        \n        results.append(rho)\n\n    # The final output must be a single line containing a comma-separated list\n    # of the realized efficiency ratios.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后一项实践将我们的视角从应用单一技术提升到系统性的自动化设计层面。你将面临一个带约束的优化问题，即在严格的计算预算下，为学生网络找到最优的深度与宽度组合。这项关于压缩感知神经架构搜索 (NAS) 的练习  旨在让你掌握如何在模型大小、性能和效率之间进行系统性权衡，为你从零开始设计最优压缩模型提供一个清晰的框架。",
            "id": "3152883",
            "problem": "考虑一个由深度 $D \\in \\mathbb{N}$ 和宽度 $W \\in \\mathbb{N}$ 参数化的学生神经网络族，其中 $D$ 表示层数，$W$ 表示每层的通道数。总计算成本（原始算术运算次数）被建模为浮点运算次数（FLOPs），由经过充分检验的缩放代理公式 $F(D, W) = \\kappa D W^2$ 给出，其中 $\\kappa  0$ 是一个常数，封装了对于固定输入分辨率的卷积核大小和特征图维度。为了压缩模型，我们施加约束 $F(D, W) \\leq F_{\\max}$。\n\n我们的目标是在固定的计算预算下，利用知识蒸馏（Knowledge Distillation, KD）设计一个压缩感知的架构搜索。假设在 softmax 温度 $T  0$ 下，使用一个固定的教师网络进行知识蒸馏训练，会使基线近似常数乘以一个平滑因子 $s(T) = \\frac{1}{1 + \\mu T}$，其中 $\\mu  0$ 是一个给定的系数。设学生网络的预期代理损失为\n$$\nL(D, W) = s(T)\\left(A D^{-p} + B W^{-q}\\right),\n$$\n其中 $A  0$、$B  0$、$p  0$ 和 $q  0$ 是给定的常数，表示深度和宽度对近似误差贡献的强度和缩放指数。该代理模型旨在编码广泛观察到的趋势：更深的网络以由 $p$ 控制的递减回报率减少近似误差，更宽的网络以由 $q$ 控制的递减回报率减少近似误差，而蒸馏通过 $s(T)$ 统一地减少了有效的近似常数。\n\n搜索域被限制在整数超参数 $D \\in \\{D_{\\min}, D_{\\min} + 1, \\dots, D_{\\max}\\}$ 和 $W \\in \\{W_{\\min}, W_{\\min} + 1, \\dots, W_{\\max}\\}$，并带有可行性约束 $F(D, W) = \\kappa D W^2 \\leq F_{\\max}$。目标是在这些约束下，找到最小化 $L(D, W)$ 的最优学生网络配置 $[D^\\star, W^\\star]$。\n\n仅从上述定义和 FLOPs 约束出发，从第一性原理推导在活跃的 FLOPs 约束下，最优配置如何在深度和宽度之间取得平衡，并利用此推导设计一个计算上鲁棒的算法，以返回最优的整数对 $[D^\\star, W^\\star]$。如果有多对参数在 $10^{-9}$ 的容差范围内达到相同的最小损失，则选择 $D$ 较大的那一对；如果仍然相同，则选择 $W$ 较大的那一对。\n\n实现一个完整的程序，该程序：\n- 对于每个测试用例，根据约束计算 $[D^\\star, W^\\star]$，利用推导出的原理，即对于 $q  0$，损失随 $W$ 单调递减，因此在给定可行 $D$ 的情况下，最优的 $W$ 是 FLOPs 预算和宽度界限所允许的最大 $W$。\n- 生成一行输出，其中包含所有测试用例的结果，形式为方括号括起来的逗号分隔列表，每个项目是一个二元列表 $[D^\\star, W^\\star]$。\n\n使用以下测试套件，它涵盖了一个通用情况、一个接近宽度最小值的预算限制情况、一个高温蒸馏的边缘情况以及一个宽度主导的缩放情况：\n\n测试用例 1（通用“理想路径”）：\n- $F_{\\max} = 10^8$, $\\kappa = 20$, $A = 0.8$, $B = 0.6$, $p = 0.8$, $q = 1.2$, $D_{\\min} = 1$, $D_{\\max} = 64$, $W_{\\min} = 8$, $W_{\\max} = 512$, $\\mu = 0.2$, $T = 2.0$。\n\n测试用例 2（边界预算，窄宽度）：\n- $F_{\\max} = 10^5$, $\\kappa = 20$, $A = 0.8$, $B = 0.6$, $p = 0.8$, $q = 1.2$, $D_{\\min} = 1$, $D_{\\max} = 64$, $W_{\\min} = 8$, $W_{\\max} = 64$, $\\mu = 0.2$, $T = 2.0$。\n\n测试用例 3（高温蒸馏，更强平滑）：\n- $F_{\\max} = 10^8$, $\\kappa = 20$, $A = 0.9$, $B = 0.7$, $p = 1.0$, $q = 1.0$, $D_{\\min} = 1$, $D_{\\max} = 64$, $W_{\\min} = 8$, $W_{\\max} = 512$, $\\mu = 0.5$, $T = 5.0$。\n\n测试用例 4（宽度主导的缩放及更大预算）：\n- $F_{\\max} = 5 \\cdot 10^8$, $\\kappa = 30$, $A = 0.9$, $B = 0.9$, $p = 0.7$, $q = 2.0$, $D_{\\min} = 1$, $D_{\\max} = 128$, $W_{\\min} = 8$, $W_{\\max} = 1024$, $\\mu = 0.1$, $T = 1.0$。\n\n您的程序应生成一行输出，其中包含结果，格式为方括号括起来的逗号分隔列表，每个结果的格式为二元列表 $[D^\\star, W^\\star]$，顺序与测试用例相同。例如，输出格式必须类似于 $[[d_1, w_1],[d_2, w_2],[d_3, w_3],[d_4, w_4]]$，其中每个 $d_i$ 和 $w_i$ 都是整数。",
            "solution": "用户提供的问题陈述已经过验证，被认为是有效的。该问题在深度学习领域有科学依据，是一个定义明确的约束优化问题，并且表述客观。所有必要的参数和约束都已提供，不存在矛盾或歧义。\n\n目标是找到代表学生神经网络深度和宽度的整数对 $[D^\\star, W^\\star]$，以最小化代理损失函数\n$$\nL(D, W) = s(T)\\left(A D^{-p} + B W^{-q}\\right)\n$$\n并满足一组约束条件。\n\n给定的约束条件是：\n1.  整数深度域：$D \\in \\{D_{\\min}, D_{\\min} + 1, \\dots, D_{\\max}\\}$\n2.  整数宽度域：$W \\in \\{W_{\\min}, W_{\\min} + 1, \\dots, W_{\\max}\\}$\n3.  计算成本（FLOPs）预算：$F(D, W) = \\kappa D W^2 \\leq F_{\\max}$\n\n参数 $A, B, p, q, \\kappa, \\mu, T$ 均为正常数。因此，对于任何给定的测试用例，因子 $s(T) = \\frac{1}{1 + \\mu T}$ 是一个正常数。为了最小化 $L(D, W)$，我们可以等价地最小化函数 $\\tilde{L}(D, W) = A D^{-p} + B W^{-q}$，因为 $s(T)$ 仅对总损失值进行缩放。\n\n损失函数 $\\tilde{L}(D, W)$ 是两项之和，$A D^{-p}$ 和 $B W^{-q}$。由于 $A, B, p, q$ 均为正数，这两项都是其各自变量 $D$ 和 $W$ 的单调递减函数。因此，总损失 $\\tilde{L}(D, W)$ 是关于 $D$ 和 $W$ 的单调递减函数。这意味着为了最小化损失，我们应使 $D$ 和 $W$ 尽可能大。然而，FLOPs 约束 $\\kappa D W^2 \\leq F_{\\max}$ 建立了一种权衡：增加一个变量必然会限制另一个变量的最大可能值。\n\n问题表述引导我们采用一种鲁棒的搜索策略。它指出，对于一个固定的可行深度 $D$，损失随 $W$ 的增加而减少。因此，为了在给定 $D$ 的情况下最小化损失，我们必须选择满足所有约束的最大可能整数宽度 $W$。让我们将其形式化。\n\n对于一个固定的整数深度 $D$（满足 $D_{\\min} \\leq D \\leq D_{\\max}$），对整数宽度 $W$ 的约束是：\n1.  $W \\geq W_{\\min}$\n2.  $W \\leq W_{\\max}$\n3.  根据 FLOPs 约束，$W^2 \\leq \\frac{F_{\\max}}{\\kappa D}$，这意味着 $W \\leq \\sqrt{\\frac{F_{\\max}}{\\kappa D}}$。\n\n为了找到满足所有三个条件的最大整数 $W$，我们取上界的最小值，并确保它满足下界。对于给定的 $D$，最大允许宽度（我们称之为 $W_{\\text{candidate}}(D)$）是：\n$$\nW_{\\text{candidate}}(D) = \\min\\left(W_{\\max}, \\left\\lfloor \\sqrt{\\frac{F_{\\max}}{\\kappa D}} \\right\\rfloor\\right)\n$$\n其中 $\\lfloor \\cdot \\rfloor$ 表示向下取整函数，因为 $W$ 必须是整数。\n\n对于给定的 $D$，只有当此候选宽度至少为 $W_{\\min}$ 时，才存在有效的配置 $(D, W)$。也就是说，仅当 $W_{\\text{candidate}}(D) \\geq W_{\\min}$ 时，深度 $D$ 才是可行的。\n\n这将 $(D, W)$ 上的二维搜索空间简化为对 $D$ 的一维搜索。找到最优对 $[D^\\star, W^\\star]$ 的算法如下：\n\n1.  初始化一个变量 $L_{\\min}$ 用于记录至今为止的最小损失，其值为正无穷大，并将最优参数对 $[D^\\star, W^\\star]$ 初始化为空值。\n2.  遍历指定范围内的每个可能的整数深度 $D$，从 $D_{\\min}$ 到 $D_{\\max}$。\n3.  对于每个 $D$：\n    a. 计算此 $D$ 在所有约束下允许的最大整数宽度 $W_D$：\n       $$\n       W_D = \\min\\left(W_{\\max}, \\left\\lfloor \\sqrt{\\frac{F_{\\max}}{\\kappa D}} \\right\\rfloor\\right)\n       $$\n    b. 通过验证计算出的宽度 $W_D$ 是否满足下界 $W_D \\geq W_{\\min}$，来检查此深度 $D$ 是否是可行解的一部分。\n    c. 如果 $D$ 是可行的（即 $W_D \\geq W_{\\min}$），则计算对 $(D, W_D)$ 的损失：\n       $$\n       L(D, W_D) = s(T) \\left( A D^{-p} + B W_D^{-q} \\right)\n       $$\n    d. 将 $L(D, W_D)$ 与当前的最小损失 $L_{\\min}$ 进行比较。\n       - 如果 $L(D, W_D)$ 比 $L_{\\min}$ 小超过 $10^{-9}$ 的容差，则更新 $L_{\\min} = L(D, W_D)$ 并将最优对设置为 $[D, W_D]$。\n       - 如果 $L(D, W_D)$ 在 $L_{\\min}$ 的容差范围内（即 $|L(D, W_D) - L_{\\min}| \\leq 10^{-9}$），则应用指定的决胜规则：选择具有更大深度 $D$ 的对。由于我们的搜索是按 $D$ 的递增顺序进行的，任何平局总是会涉及一个比当前最优对中 $D$ 更大的新 $D$。因此，在出现平局时，我们总是将最优对更新为 $[D, W_D]$。第二个决胜规则（更大的 $W$）不适用，因为对于任何给定的 $D$，最优的 $W_D$ 是唯一的。\n4. 遍历从 $D_{\\min}$ 到 $D_{\\max}$ 的所有可能值后，存储的对 $[D^\\star, W^\\star]$ 即为最优解。\n\n在简化的 $D$ 的一维空间上进行这种穷举搜索，在计算上是高效的，并保证在给定的模型和约束下找到全局最小值。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Finds the optimal [D, W] pair that minimizes a surrogate loss function\n    for a neural network, subject to a computational budget and domain constraints.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (general \"happy path\")\n        {\n            \"F_max\": 1e8, \"kappa\": 20, \"A\": 0.8, \"B\": 0.6, \"p\": 0.8, \"q\": 1.2,\n            \"D_min\": 1, \"D_max\": 64, \"W_min\": 8, \"W_max\": 512, \"mu\": 0.2, \"T\": 2.0\n        },\n        # Test case 2 (boundary budget, narrow widths)\n        {\n            \"F_max\": 1e5, \"kappa\": 20, \"A\": 0.8, \"B\": 0.6, \"p\": 0.8, \"q\": 1.2,\n            \"D_min\": 1, \"D_max\": 64, \"W_min\": 8, \"W_max\": 64, \"mu\": 0.2, \"T\": 2.0\n        },\n        # Test case 3 (high-temperature distillation, stronger smoothing)\n        {\n            \"F_max\": 1e8, \"kappa\": 20, \"A\": 0.9, \"B\": 0.7, \"p\": 1.0, \"q\": 1.0,\n            \"D_min\": 1, \"D_max\": 64, \"W_min\": 8, \"W_max\": 512, \"mu\": 0.5, \"T\": 5.0\n        },\n        # Test case 4 (width-dominant scaling with larger budget)\n        {\n            \"F_max\": 5e8, \"kappa\": 30, \"A\": 0.9, \"B\": 0.9, \"p\": 0.7, \"q\": 2.0,\n            \"D_min\": 1, \"D_max\": 128, \"W_min\": 8, \"W_max\": 1024, \"mu\": 0.1, \"T\": 1.0\n        }\n    ]\n\n    results = []\n    \n    TOLERANCE = 1e-9\n\n    for params in test_cases:\n        F_max = params[\"F_max\"]\n        kappa = params[\"kappa\"]\n        A = params[\"A\"]\n        B = params[\"B\"]\n        p = params[\"p\"]\n        q = params[\"q\"]\n        D_min = params[\"D_min\"]\n        D_max = params[\"D_max\"]\n        W_min = params[\"W_min\"]\n        W_max = params[\"W_max\"]\n        mu = params[\"mu\"]\n        T = params[\"T\"]\n        \n        s_T = 1.0 / (1.0 + mu * T)\n        \n        min_loss = float('inf')\n        optimal_pair = [None, None]\n\n        # Iterate through all possible integer depths D\n        for D in range(D_min, D_max + 1):\n            # Calculate the maximum possible width W for this D due to the FLOPs budget\n            # F_max >= kappa * D * W^2  =>  W = sqrt(F_max / (kappa * D))\n            if kappa * D == 0: continue # Avoid division by zero, though D_min >= 1\n            \n            W_budget_max = np.sqrt(F_max / (kappa * D))\n            \n            # The candidate width is the largest integer satisfying all constraints\n            W_candidate = int(np.floor(min(W_max, W_budget_max)))\n\n            # Check if this width is feasible (i.e., meets the minimum width requirement)\n            if W_candidate >= W_min:\n                W = W_candidate\n                \n                # Calculate the loss for the (D, W) pair\n                loss = s_T * (A * np.power(D, -p) + B * np.power(W, -q))\n\n                # Update the optimal pair if a better one is found\n                if loss  min_loss - TOLERANCE:\n                    min_loss = loss\n                    optimal_pair = [D, W]\n                elif abs(loss - min_loss) = TOLERANCE:\n                    # Tie-breaking rule 1: prefer larger D\n                    if D > optimal_pair[0]:\n                        optimal_pair = [D, W]\n                    # Tie-breaking rule 2: prefer larger W if D is the same.\n                    # This case will not be reached in this search strategy,\n                    # as for each D, we only test one unique W.\n                    elif D == optimal_pair[0] and W > optimal_pair[1]:\n                          optimal_pair = [D, W]\n                          \n        results.append(optimal_pair)\n\n    # Format the output string as required\n    output_str = \"[\" + \",\".join([f\"[{d},{w}]\" for d, w in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}