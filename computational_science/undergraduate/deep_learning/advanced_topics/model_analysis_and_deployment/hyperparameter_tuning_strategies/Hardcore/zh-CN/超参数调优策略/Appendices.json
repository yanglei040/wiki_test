{
    "hands_on_practices": [
        {
            "introduction": "现代优化器（如 Adam）不仅仅是简单地沿梯度方向更新参数，它们还会对梯度本身进行复杂的变换。与此同时，梯度裁剪是另一种广泛用于稳定训练的技巧。这个练习将引导您剖析单步 Adam 更新的内部机制，通过计算一些特设的度量指标，您将深入理解 Adam 的自适应归一化与梯度裁剪是如何相互作用，从而产生最终的参数更新。这项实践旨在揭开自适应优化器“魔法”背后的定量原理。",
            "id": "3135417",
            "problem": "本题要求您从有效步长归一化因子的角度，形式化并量化 Adam 优化器中全局范数梯度裁剪与自适应矩估计之间的相互作用。请基于梯度优化、指数移动平均和全局范数裁剪的基本且经过广泛测试的定义进行分析。\n\n给定深度学习模型中的一个参数向量及其梯度，将梯度的全局范数裁剪定义如下。对于梯度向量 $\\mathbf{g} \\in \\mathbb{R}^{d}$ 和裁剪阈值 $c \\in \\mathbb{R}_{0}$，裁剪后的梯度 $\\tilde{\\mathbf{g}}$ 为\n$$\n\\tilde{\\mathbf{g}} \\;=\\; \\mathbf{g} \\cdot \\min\\!\\left(1, \\frac{c}{\\lVert \\mathbf{g} \\rVert_{2}}\\right),\n$$\n约定当 $\\lVert \\mathbf{g} \\rVert_{2} = 0$ 时，则 $\\tilde{\\mathbf{g}} = \\mathbf{0}$。\n\n在 Adam 优化器中，为时间步 $t \\in \\mathbb{N}$ 定义一阶矩和二阶矩的指数移动平均，超参数为 $\\beta_{1} \\in [0,1)$，$\\beta_{2} \\in [0,1)$：\n- 一阶矩估计 $m_{t}$ 是 $\\tilde{\\mathbf{g}}$ 的指数移动平均，衰减率为 $\\beta_{1}$。\n- 二阶矩估计 $v_{t}$ 是 $\\tilde{\\mathbf{g}}$ 逐元素平方的指数移动平均，衰减率为 $\\beta_{2}$。\n使用由 $t$、$\\beta_{1}$ 和 $\\beta_{2}$ 决定的标准偏差校正因子，实施偏差校正以获得 $\\hat{m}_{t}$ 和 $\\hat{v}_{t}$。逐元素的参数更新由 $\\hat{m}_{t}$ 通过 $\\hat{v}_{t}$ 的平方根（由 $\\varepsilon \\in \\mathbb{R}_{0}$ 正则化）进行归一化，然后由学习率 $\\alpha \\in \\mathbb{R}_{0}$ 缩放形成。更新向量表示为 $\\Delta \\theta \\in \\mathbb{R}^{d}$。\n\n定义相对于在裁剪梯度上进行普通随机梯度下降步骤的有效步长归一化因子为\n$$\ns_{\\text{eff}} \\;=\\; \n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\tilde{\\mathbf{g}} \\rVert_{2}},  \\text{if } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2}  0, \\\\\n0,  \\text{if } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2} = 0,\n\\end{cases}\n$$\n并且，类似地，定义相对于未裁剪梯度的有效因子为\n$$\nr_{\\text{unclipped}} \\;=\\;\n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\mathbf{g} \\rVert_{2}},  \\text{if } \\lVert \\mathbf{g} \\rVert_{2}  0, \\\\\n0,  \\text{if } \\lVert \\mathbf{g} \\rVert_{2} = 0.\n\\end{cases}\n$$\n同时定义裁剪指示器\n$$\n\\text{clipped} \\;=\\; \\big(\\lVert \\mathbf{g} \\rVert_{2}  c\\big).\n$$\n\n您的任务是：\n- 从上述基本定义出发，给定输入 $\\mathbf{g}$、$m_{t-1}$、$v_{t-1}$、$t$ 以及超参数 $\\alpha$、$\\beta_{1}$、$\\beta_{2}$、$\\varepsilon$ 和 $c$，推导单步 Adam 更新 $\\Delta \\theta$。\n- 为以下每个测试用例计算 $s_{\\text{eff}}$、$r_{\\text{unclipped}}$ 和 $\\text{clipped}$。\n\n测试套件。对于每个用例，所有向量都在 $\\mathbb{R}^{3}$ 中，所有数字均为标准十进制或科学记数法表示：\n1. $d = 3$, $\\mathbf{g} = [\\,0.1,\\,-0.2,\\,0.05\\,]$, $m_{t-1} = [\\,0,\\,0,\\,0\\,]$, $v_{t-1} = [\\,0,\\,0,\\,0\\,]$, $t = 1$, $\\alpha = 0.01$, $\\beta_{1} = 0.9$, $\\beta_{2} = 0.999$, $\\varepsilon = 10^{-8}$, $c = 1.0$。\n2. $d = 3$, $\\mathbf{g} = [\\,10.0,\\,0.0,\\,0.0\\,]$, $m_{t-1} = [\\,0,\\,0,\\,0\\,]$, $v_{t-1} = [\\,0,\\,0,\\,0\\,]$, $t = 1$, $\\alpha = 0.001$, $\\beta_{1} = 0.9$, $\\beta_{2} = 0.999$, $\\varepsilon = 10^{-8}$, $c = 1.0$。\n3. $d = 3$, $\\mathbf{g} = [\\,0.5,\\,0.5,\\,0.5\\,]$, $m_{t-1} = [\\,0.1,\\,-0.1,\\,0.0\\,]$, $v_{t-1} = [\\,0.04,\\,0.01,\\,0.09\\,]$, $t = 10$, $\\alpha = 0.005$, $\\beta_{1} = 0.8$, $\\beta_{2} = 0.9$, $\\varepsilon = 10^{-6}$, $c = 0.5$。\n4. $d = 3$, $\\mathbf{g} = [\\,0.0,\\,0.0,\\,0.0\\,]$, $m_{t-1} = [\\,0.01,\\,-0.02,\\,0.0\\,]$, $v_{t-1} = [\\,0.001,\\,0.004,\\,0.0009\\,]$, $t = 5$, $\\alpha = 0.01$, $\\beta_{1} = 0.9$, $\\beta_{2} = 0.99$, $\\varepsilon = 10^{-8}$, $c = 0.1$。\n5. $d = 3$, $\\mathbf{g} = [\\,10^{-4},\\,-2\\cdot 10^{-4},\\,3\\cdot 10^{-4}\\,]$, $m_{t-1} = [\\,0,\\,0,\\,0\\,]$, $v_{t-1} = [\\,0,\\,0,\\,0\\,]$, $t = 1$, $\\alpha = 10^{-2}$, $\\beta_{1} = 0.0$, $\\beta_{2} = 0.0$, $\\varepsilon = 10^{-2}$, $c = 10^{-3}$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。每个测试用例的结果必须是 $[\\,s_{\\text{eff}},\\,r_{\\text{unclipped}},\\,\\text{clipped}\\,]$ 形式的列表，并按该顺序排列。因此，最终输出必须形如\n[[case1_values],[case2_values],...,[case5_values]]\n并且行中任何地方都不能有空格。所有浮点值必须以标准十进制或科学记数法（由典型编程语言的默认转换产生）打印，布尔值必须是 True 或 False。本问题无需用户输入，且不涉及任何物理单位或角度度量。所有计算必须严格按照上述规定执行，并且在计算比率时，必须使用在 $s_{\\text{eff}}$ 和 $r_{\\text{unclipped}}$ 的定义中为零分母指定的约定。",
            "solution": "该问题是有效的，因为它具有科学依据、问题明确、客观，并为获得唯一解提供了所有必要信息。它基于深度学习优化领域的标准、可形式化的定义。\n\n任务是为几个测试用例计算三个量：相对于裁剪梯度的有效步长归一化因子（$s_{\\text{eff}}$）、相对于未裁剪梯度的有效步长归一化因子（$r_{\\text{unclipped}}$）以及一个布尔裁剪指示器（$\\text{clipped}$）。这需要实现带梯度裁剪的 Adam 优化器的一个单步更新。步骤如下：\n\n### 步骤 1：梯度裁剪\n首先，我们计算原始梯度向量 $\\mathbf{g} \\in \\mathbb{R}^{d}$ 的 L$2$ 范数，记为 $\\lVert \\mathbf{g} \\rVert_{2}$。\n$$\n\\lVert \\mathbf{g} \\rVert_{2} = \\sqrt{\\sum_{i=1}^{d} g_i^2}\n$$\n裁剪指示器 $\\text{clipped}$ 是一个布尔值，通过将此范数与裁剪阈值 $c \\in \\mathbb{R}_{0}$ 进行比较来确定：\n$$\n\\text{clipped} = (\\lVert \\mathbf{g} \\rVert_{2}  c)\n$$\n然后对梯度 $\\mathbf{g}$ 进行裁剪以生成 $\\tilde{\\mathbf{g}}$。如果 $\\mathbf{g}$ 的范数超过 $c$，梯度向量将被按比例缩小，使其范数为 $c$。否则，它保持不变。这由以下公式表示：\n$$\n\\tilde{\\mathbf{g}} = \\mathbf{g} \\cdot \\min\\left(1, \\frac{c}{\\lVert \\mathbf{g} \\rVert_{2}}\\right)\n$$\n对于零梯度有一个特殊约定：如果 $\\lVert \\mathbf{g} \\rVert_{2} = 0$，则 $\\tilde{\\mathbf{g}} = \\mathbf{0}$。如果我们定义当 $\\lVert \\mathbf{g} \\rVert_{2}=0$ 时缩放因子为 $1$ 以避免除以零，那么裁剪公式的乘法性质可以自然地处理这种情况，因为 $\\mathbf{g}$ 本身就是零向量。\n\n### 步骤 2：Adam 矩更新\nAdam 优化器维护梯度（一阶矩）及其平方（二阶矩）的指数移动平均值。给定在时间步 $t-1$ 的先前矩估计 $m_{t-1}$ 和 $v_{t-1}$，以及衰减率 $\\beta_1, \\beta_2 \\in [0, 1)$，在时间步 $t$ 的新估计使用裁剪后的梯度 $\\tilde{\\mathbf{g}}$ 计算得出。\n\n一阶矩估计 $m_t$ 更新如下：\n$$\nm_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\tilde{\\mathbf{g}}\n$$\n二阶矩估计 $v_t$ 使用裁剪梯度的逐元素平方 $\\tilde{\\mathbf{g}}^2$ 更新如下：\n$$\nv_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\tilde{\\mathbf{g}}^2\n$$\n\n### 步骤 3：偏差校正\n初始矩估计通常初始化为零向量。这会引入一个趋向于零的偏差，尤其是在优化的初始阶段。Adam 通过将矩估计除以取决于时间步 $t$ 和衰减率的偏差校正因子来校正这种偏差。\n\n偏差校正后的一阶矩估计 $\\hat{m}_t$ 为：\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n$$\n偏差校正后的二阶矩估计 $\\hat{v}_t$ 为：\n$$\n\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n$$\n\n### 步骤 4：参数更新向量\n参数更新向量 $\\Delta\\theta$ 是使用偏差校正后的矩估计计算得出的。一阶矩估计 $\\hat{m}_t$ 作为更新方向，并通过二阶矩估计 $\\hat{v}_t$ 的平方根进行逐元素归一化。为了数值稳定性，在分母中加入一个小的常数 $\\varepsilon \\in \\mathbb{R}_{0}$。结果由学习率 $\\alpha \\in \\mathbb{R}_{0}$ 进行缩放。\n$$\n\\Delta\\theta = \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}\n$$\n请注意，通常参数更新规则是 $\\theta_t = \\theta_{t-1} - \\text{step}$。所要求的向量 $\\Delta\\theta$ 只是更新本身，由于我们只关心其范数 $\\lVert \\Delta\\theta \\rVert_{2}$，因此符号无关紧要。\n\n### 步骤 5：最终指标计算\n计算出参数更新向量 $\\Delta\\theta$ 后，我们现在可以计算所需的指标。\n\n计算更新向量的 L$2$ 范数 $\\lVert \\Delta\\theta \\rVert_2$ 和裁剪梯度的 L$2$ 范数 $\\lVert \\tilde{\\mathbf{g}} \\rVert_2$。\n\n相对于裁剪梯度的有效步长归一化因子 $s_{\\text{eff}}$ 定义为：\n$$\ns_{\\text{eff}} = \n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\tilde{\\mathbf{g}} \\rVert_{2}},   \\text{if } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2}  0 \\\\\n0,   \\text{if } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2} = 0\n\\end{cases}\n$$\n相对于未裁剪梯度的有效因子 $r_{\\text{unclipped}}$ 定义为：\n$$\nr_{\\text{unclipped}} = \n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\mathbf{g} \\rVert_{2}},   \\text{if } \\lVert \\mathbf{g} \\rVert_{2}  0 \\\\\n0,   \\text{if } \\lVert \\mathbf{g} \\rVert_{2} = 0\n\\end{cases}\n$$\n将这些步骤应用于每个测试用例以获得最终结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Adam update metrics for a series of test cases.\n    \"\"\"\n    test_cases = [\n        # 1. d=3, g=[0.1,-0.2,0.05], m_prev=[0,0,0], v_prev=[0,0,0], t=1, alpha=0.01, beta1=0.9, beta2=0.999, eps=1e-8, c=1.0\n        {'g': np.array([0.1, -0.2, 0.05]), 'm_prev': np.array([0.0, 0.0, 0.0]), 'v_prev': np.array([0.0, 0.0, 0.0]), 't': 1, 'alpha': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8, 'c': 1.0},\n        # 2. d=3, g=[10.0,0.0,0.0], m_prev=[0,0,0], v_prev=[0,0,0], t=1, alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-8, c=1.0\n        {'g': np.array([10.0, 0.0, 0.0]), 'm_prev': np.array([0.0, 0.0, 0.0]), 'v_prev': np.array([0.0, 0.0, 0.0]), 't': 1, 'alpha': 0.001, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8, 'c': 1.0},\n        # 3. d=3, g=[0.5,0.5,0.5], m_prev=[0.1,-0.1,0.0], v_prev=[0.04,0.01,0.09], t=10, alpha=0.005, beta1=0.8, beta2=0.9, eps=1e-6, c=0.5\n        {'g': np.array([0.5, 0.5, 0.5]), 'm_prev': np.array([0.1, -0.1, 0.0]), 'v_prev': np.array([0.04, 0.01, 0.09]), 't': 10, 'alpha': 0.005, 'beta1': 0.8, 'beta2': 0.9, 'epsilon': 1e-6, 'c': 0.5},\n        # 4. d=3, g=[0,0,0], m_prev=[0.01,-0.02,0], v_prev=[0.001,0.004,0.0009], t=5, alpha=0.01, beta1=0.9, beta2=0.99, eps=1e-8, c=0.1\n        {'g': np.array([0.0, 0.0, 0.0]), 'm_prev': np.array([0.01, -0.02, 0.0]), 'v_prev': np.array([0.001, 0.004, 0.0009]), 't': 5, 'alpha': 0.01, 'beta1': 0.9, 'beta2': 0.99, 'epsilon': 1e-8, 'c': 0.1},\n        # 5. d=3, g=[1e-4, -2e-4, 3e-4], m_prev=[0,0,0], v_prev=[0,0,0], t=1, alpha=1e-2, beta1=0.0, beta2=0.0, eps=1e-2, c=1e-3\n        {'g': np.array([1e-4, -2e-4, 3e-4]), 'm_prev': np.array([0.0, 0.0, 0.0]), 'v_prev': np.array([0.0, 0.0, 0.0]), 't': 1, 'alpha': 1e-2, 'beta1': 0.0, 'beta2': 0.0, 'epsilon': 1e-2, 'c': 1e-3},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        g, m_prev, v_prev = case['g'], case['m_prev'], case['v_prev']\n        t, alpha, beta1, beta2, epsilon, c = case['t'], case['alpha'], case['beta1'], case['beta2'], case['epsilon'], case['c']\n\n        # Step 1: Gradient Clipping\n        g_norm = np.linalg.norm(g)\n        \n        clipped = bool(g_norm > c)\n\n        if g_norm > 0:\n            clip_factor = min(1.0, c / g_norm)\n            g_tilde = g * clip_factor\n        else:\n            g_tilde = np.zeros_like(g)\n\n        # Step 2: Adam Moment Updates\n        m_t = beta1 * m_prev + (1 - beta1) * g_tilde\n        v_t = beta2 * v_prev + (1 - beta2) * (g_tilde ** 2)\n\n        # Step 3: Bias Correction\n        m_hat = m_t / (1 - beta1**t)\n        v_hat = v_t / (1 - beta2**t)\n        \n        # Step 4: Parameter Update Vector\n        delta_theta = alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n        \n        # Step 5: Final Metric Calculation\n        delta_theta_norm = np.linalg.norm(delta_theta)\n        g_tilde_norm = np.linalg.norm(g_tilde)\n\n        if g_tilde_norm > 0:\n            s_eff = delta_theta_norm / (alpha * g_tilde_norm)\n        else:\n            s_eff = 0.0\n\n        if g_norm > 0:\n            r_unclipped = delta_theta_norm / (alpha * g_norm)\n        else:\n            r_unclipped = 0.0\n            \n        all_results.append([s_eff, r_unclipped, clipped])\n\n    # Format the final output string exactly as required\n    result_strings = []\n    for res in all_results:\n        # Convert each item in the sublist to its string representation\n        # str(True) is 'True', str(False) is 'False', which is correct\n        s = '[' + ','.join(map(str, res)) + ']'\n        result_strings.append(s)\n    \n    final_output = '[' + ','.join(result_strings) + ']'\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "逐个调整超参数的效率低下，真正的专业知识源于对它们相互作用的理解。其中，学习率 $α$ 和权重衰减 $λ$ 之间的关系是一个典型且常常令人困惑的问题。本练习挑战您推导并验证一个缩放法则，该法则旨在当学习率变化时，保持等效的正则化强度。通过从第一性原理分析 SGD 和 AdamW 等优化器，您将揭示为何解耦权重衰减（AdamW）使这种关系变得更加清晰，并学会如何有效地协同调整这些参数。",
            "id": "3135392",
            "problem": "你需要形式化并测试一种用于深度学习模型梯度优化中权重衰减的超参数缩放启发式方法。目标是当学习率变化时，保持有效的单步正则化强度不变。请从基于梯度的更新和欧几里得范数正则化的核心定义出发，避免使用任何直接跳到结果的简化公式。\n\n设 $w_t \\in \\mathbb{R}$ 表示在步骤 $t$ 的单个标量参数，设 $\\alpha \\in \\mathbb{R}_{\\ge 0}$ 表示学习率，设 $\\lambda \\in \\mathbb{R}_{\\ge 0}$ 表示权重衰减或欧几里得范数（L2）正则化系数，设 $g_t \\in \\mathbb{R}$ 表示在不施加正则化时，损失函数关于 $w_t$ 的、与数据相关的梯度。假设使用以下经过充分测试的更新规则之一：\n- 不带动量的随机梯度下降 (SGD)：$w_{t+1} = w_t - \\alpha \\nabla_w \\mathcal{L}(w_t)$，其中欧几里得范数 (L2) 正则化包含在损失中，因此梯度包含一个依赖于 $\\lambda$ 和 $w_t$ 的加法项。\n- 随机梯度下降 (SGD) 的动量变体：使用一个速度 $v_t \\in \\mathbb{R}$，并通过一个系数为 $\\beta \\in [0,1)$ 的线性递推关系进行更新，同时对欧几里得范数 (L2) 正则化进行与上述相同的处理。\n- 使用解耦权重衰减的自适应矩估计 (AdamW)：数据梯度的一阶矩和二阶矩通过系数 $\\beta_1, \\beta_2 \\in [0,1)$ 进行维护，欧几里得范数 (L2) 惩罚项作为一个与数据梯度路径明确分离的乘性衰减项来施加。\n\n对于给定的优化器、学习率和权重衰减，在数据驱动梯度不存在的状态下，将单步有效正则化定义为乘性收缩因子\n$$\ns(\\alpha, \\lambda; w_t) \\equiv \\frac{|w_{t+1}|}{|w_t|},\n$$\n即当更新在 $g_t = 0$ 且任何内部状态（如速度或矩）在步骤 $t$ 被初始化为零的条件下执行时。该定义隔离了正则化机制在单一步骤中的影响，并忽略了任何数据驱动的变化。\n\n任务 A (推导)：仅使用所述的更新规则以及欧几里得范数 (L2) 正则化和解耦权重衰减的定义，推导一个关联 $\\lambda$ 和 $\\alpha$ 的条件，该条件使得当 $\\alpha$ 变化时 $s(\\alpha, \\lambda; w_t)$ 保持不变。你的推导必须基于基本的单步更新动态过程，并且不应依赖任何未说明的恒等式。\n\n任务 B (编程)：给定一个基准 $(\\alpha_0, \\lambda_0)$，实现一个程序，用于在 $\\alpha$ 变化时评估 $\\lambda$ 的两种候选缩放定律：\n1. 正比缩放：$\\lambda_{\\mathrm{prop}}(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha}{\\alpha_0}$。\n2. 反比缩放：$\\lambda_{\\mathrm{inv}}(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha_0}{\\alpha}$。\n\n对于每个测试用例，执行以下操作：\n1. 在给定的非零初始权重 $w_0$ 处，且 $g_t=0$ 以及任何内部优化器状态均初始化为零的条件下，计算基准收缩率 $s_0 = s(\\alpha_0, \\lambda_0; w_0)$。\n2. 对于用例的学习率列表中的每个 $\\alpha$，在相同的初始条件下，使用给定优化器的精确单步更新规则，计算 $s_{\\mathrm{prop}}(\\alpha) = s(\\alpha, \\lambda_{\\mathrm{prop}}(\\alpha); w_0)$ 和 $s_{\\mathrm{inv}}(\\alpha) = s(\\alpha, \\lambda_{\\mathrm{inv}}(\\alpha); w_0)$。\n3. 每个测试用例返回两个布尔值：\n   - 如果对于列表中的所有 $\\alpha$ 都有 $|s_{\\mathrm{inv}}(\\alpha) - s_0| \\le \\varepsilon$，则第一个布尔值为真。\n   - 如果对于列表中的所有 $\\alpha$ 都有 $|s_{\\mathrm{prop}}(\\alpha) - s_0| \\le \\varepsilon$，则第二个布尔值为真。\n此处 $\\varepsilon  0$ 是用于数值比较的给定容差。\n\n你必须实现的测试套件包含以下五个用例，每个用例提供一个基准 $(\\alpha_0, \\lambda_0)$、一个非零初始权重 $w_0$、一组待测试的学习率，以及一个优化器及其系数。在所有用例中，均在步骤 $t$ 时 $g_t = 0$ 且内部状态为零的条件下评估单次更新。\n- 用例 1 (理想情况，损失函数中包含耦合正则化)：优化器为不带动量的 SGD，基准 $\\alpha_0 = 0.1$, $\\lambda_0 = 0.01$, $w_0 = 1.0$，测试学习率 $\\{0.05, 0.1, 0.2\\}$，容差 $\\varepsilon = 10^{-12}$。\n- 用例 2 (带动量，损失函数中包含耦合正则化)：优化器为带动量的 SGD，$\\beta = 0.9$，基准 $\\alpha_0 = 0.1$, $\\lambda_0 = 0.01$, $w_0 = 1.0$，测试学习率 $\\{0.05, 0.1, 0.2\\}$，容差 $\\varepsilon = 10^{-12}$。\n- 用例 3 (解耦权重衰减)：优化器为 AdamW，$\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\varepsilon_{\\mathrm{adam}} = 10^{-8}$，基准 $\\alpha_0 = 0.001$, $\\lambda_0 = 0.01$, $w_0 = 1.0$，测试学习率 $\\{0.0005, 0.001, 0.002\\}$，容差 $\\varepsilon = 10^{-12}$。\n- 用例 4 (边界情况：零权重衰减)：优化器为不带动量的 SGD，基准 $\\alpha_0 = 0.05$, $\\lambda_0 = 0.0$, $w_0 = 1.0$，测试学习率 $\\{0.01, 0.05, 0.1\\}$，容差 $\\varepsilon = 10^{-12}$。\n- 用例 5 (跨尺度的覆盖性测试)：优化器为 AdamW，$\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\varepsilon_{\\mathrm{adam}} = 10^{-8}$，基准 $\\alpha_0 = 10^{-2}$, $\\lambda_0 = 5 \\cdot 10^{-3}$, $w_0 = 1.0$，测试学习率 $\\{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\\}$，容差 $\\varepsilon = 10^{-12}$。\n\n要求的最终输出格式：你的程序应生成单行输出，其中包含一个扁平化的 Python 布尔值列表，每个测试用例恰好有两个布尔值，并按上述用例的顺序排列。对于每个用例，首先附加反比缩放的布尔值，然后附加正比缩放的布尔值。例如，一个包含五个用例的运行必须以如下确切格式打印单行结果：\"[b1_inv,b1_prop,b2_inv,b2_prop,b3_inv,b3_prop,b4_inv,b4_prop,b5_inv,b5_prop]\"。",
            "solution": "本任务旨在推导一种用于权重衰减的超参数缩放启发式方法，然后通过编程进行测试。目标是在学习率变化时，保持恒定的有效正则化强度。单步的有效正则化由乘性收缩因子 $s(\\alpha, \\lambda; w_t) \\equiv \\frac{|w_{t+1}|}{|w_t|}$ 定义，它是在零数据相关梯度（$g_t=0$）和步骤 $t$ 时内部优化器状态为零的特定条件下计算的。设 $w_t \\in \\mathbb{R}$ 为标量权重，$\\alpha \\in \\mathbb{R}_{\\ge 0}$ 为学习率，$\\lambda \\in \\mathbb{R}_{\\ge 0}$ 为权重衰减系数。我们假设 $w_t \\neq 0$。\n\n**任务 A：不变性条件的推导**\n\n我们将分析每种指定优化器在给定条件下（$g_t=0$，零初始状态）的单步更新规则，以推导出能使 $s(\\alpha, \\lambda; w_t)$ 保持不变的 $\\alpha$ 和 $\\lambda$ 之间的关系。\n\n**1. 不带动量的随机梯度下降 (SGD)**\n\n带有欧几里得范数 (L2) 正则化的总损失函数为 $\\mathcal{L}_{\\text{total}}(w_t) = \\mathcal{L}(w_t) + \\frac{\\lambda}{2} w_t^2$。关于 $w_t$ 的梯度是 $\\nabla_w \\mathcal{L}_{\\text{total}}(w_t) = \\nabla_w \\mathcal{L}(w_t) + \\lambda w_t$。设 $g_t = \\nabla_w \\mathcal{L}(w_t)$ 为梯度的与数据相关的部分。完整的梯度则为 $g_t + \\lambda w_t$。\n\nSGD 更新规则为：\n$$w_{t+1} = w_t - \\alpha \\nabla_w \\mathcal{L}_{\\text{total}}(w_t) = w_t - \\alpha (g_t + \\lambda w_t)$$\n在 $g_t=0$ 的条件下，更新简化为：\n$$w_{t+1} = w_t - \\alpha (\\lambda w_t) = (1 - \\alpha \\lambda) w_t$$\n因此，收缩因子 $s$ 为：\n$$s(\\alpha, \\lambda; w_t) = \\frac{|w_{t+1}|}{|w_t|} = \\frac{|(1 - \\alpha \\lambda) w_t|}{|w_t|} = |1 - \\alpha \\lambda|$$\n为了使 $s(\\alpha, \\lambda; w_t)$ 对 $\\alpha$ 的变化保持不变，$|1 - \\alpha \\lambda|$ 的值必须保持恒定。假设更新是稳定的（即 $1 - \\alpha \\lambda \\ge 0$），这意味着乘积 $\\alpha \\lambda$ 必须是常数。\n$$\\alpha \\lambda = C$$\n其中 $C$ 是一个常数。给定一个基准对 $(\\alpha_0, \\lambda_0)$，该常数为 $C = \\alpha_0 \\lambda_0$。因此，对于任何新的学习率 $\\alpha$，对应的权重衰减 $\\lambda$ 必须满足 $\\lambda = \\frac{\\alpha_0 \\lambda_0}{\\alpha}$。这就是反比缩放定律。\n\n**2. 带动量的 SGD**\n\n优化器维护一个速度向量 $v_t$。针对总梯度 $\\nabla_w \\mathcal{L}_{\\text{total}}(w_t) = g_t + \\lambda w_t$ 的更新规则如下：\n$$v_{t+1} = \\beta v_t + (g_t + \\lambda w_t)$$\n$$w_{t+1} = w_t - \\alpha v_{t+1}$$\n其中 $\\beta \\in [0, 1)$ 是动量系数。\n我们应用以下条件：$g_t=0$ 且步骤 $t$ 的内部状态为零，即 $v_t=0$。\n速度更新变为：\n$$v_{t+1} = \\beta(0) + (0 + \\lambda w_t) = \\lambda w_t$$\n将此代入参数更新规则中：\n$$w_{t+1} = w_t - \\alpha (\\lambda w_t) = (1 - \\alpha \\lambda) w_t$$\n这与 SGD 的情况相同。收缩因子同样是 $s(\\alpha, \\lambda; w_t) = |1 - \\alpha \\lambda|$，并且不变性条件是 $\\alpha \\lambda = \\text{常数}$。这再次导出了反比缩放定律。\n\n**3. AdamW (解耦权重衰减)**\n\n在 AdamW 中，权重衰减与基于梯度的更新是“解耦”的。更新在概念上分两步执行：首先应用权重衰减，然后基于数据梯度 $g_t$ 应用 Adam 更新。一种常见的实现形式是：\n$$w_{t+1} = w_t (1 - \\alpha \\lambda) - \\alpha \\cdot \\text{AdamUpdate}(g_t, m_t, v_t)$$\n`AdamUpdate` 项依赖于矩估计：\n$$m_{t+1} = \\beta_1 m_t + (1-\\beta_1) g_t$$\n$$v_{t+1} = \\beta_2 v_t + (1-\\beta_2) g_t^2$$\n及其偏差校正后的版本 $\\hat{m}_{t+1}$ 和 $\\hat{v}_{t+1}$。更新项与 $\\frac{\\hat{m}_{t+1}}{\\sqrt{\\hat{v}_{t+1}} + \\varepsilon_{\\text{adam}}}$ 成正比。\n\n在给定条件下，$g_t=0$ 且步骤 $t$ 的内部状态为零（$m_t=0, v_t=0$）。\n矩更新得出：\n$$m_{t+1} = \\beta_1(0) + (1-\\beta_1)(0) = 0$$\n$$v_{t+1} = \\beta_2(0) + (1-\\beta_2)(0^2) = 0$$\n因此，它们的偏差校正版本也为零，$\\hat{m}_{t+1}=0$ 和 $\\hat{v}_{t+1}=0$。整个 `AdamUpdate` 项变为零：\n$$\\alpha \\cdot \\text{AdamUpdate}(g_t=0, m_t=0, v_t=0) = 0$$\nAdamW 更新规则简化为：\n$$w_{t+1} = w_t (1 - \\alpha \\lambda) - 0 = (1 - \\alpha \\lambda) w_t$$\n在指定条件下，此更新再次变得与前述情况相同。收缩因子是 $s(\\alpha, \\lambda; w_t) = |1 - \\alpha \\lambda|$，不变性条件是 $\\alpha \\lambda = \\text{常数}$。\n\n**推导结论**\n\n对于所有三种优化器——带耦合正则化的 SGD、带耦合正则化的动量 SGD，以及带解耦权重衰减的 AdamW——有效的单步正则化收缩因子 $s(\\alpha, \\lambda; w_t)$ 对 $\\alpha$ 的变化是不变的，当且仅当乘积 $\\alpha \\lambda$ 保持恒定。这对应于**反比缩放定律**：$\\lambda(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha_0}{\\alpha}$。而正比缩放定律 $\\lambda(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha}{\\alpha_0}$ 会导致乘积 $\\alpha \\lambda$ 与 $\\alpha^2$ 成比例缩放，从而无法保持收缩因子恒定。\n\n以下程序将根据所提供的测试用例，对这一结论进行数值验证。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef update_sgd(w_t, alpha, lambda_val, **kwargs):\n    \"\"\"\n    Performs a single update step for SGD with coupled L2 regularization.\n    Conditions: g_t = 0, no internal state.\n    \"\"\"\n    g_t = 0.0\n    total_gradient = g_t + lambda_val * w_t\n    w_t_plus_1 = w_t - alpha * total_gradient\n    return w_t_plus_1\n\ndef update_momentum_sgd(w_t, alpha, lambda_val, beta, **kwargs):\n    \"\"\"\n    Performs a single update step for Momentum SGD with coupled L2 regularization.\n    Conditions: g_t = 0, velocity v_t = 0.\n    \"\"\"\n    g_t = 0.0\n    v_t = 0.0  # Zero initial state\n    total_gradient = g_t + lambda_val * w_t\n    v_t_plus_1 = beta * v_t + total_gradient\n    w_t_plus_1 = w_t - alpha * v_t_plus_1\n    return w_t_plus_1\n\ndef update_adamw(w_t, alpha, lambda_val, beta1, beta2, epsilon_adam, **kwargs):\n    \"\"\"\n    Performs a single update step for AdamW with decoupled weight decay.\n    Conditions: g_t = 0, moments m_t = 0, v_t = 0.\n    \"\"\"\n    g_t = 0.0\n    m_t = 0.0  # Zero initial state\n    v_t = 0.0  # Zero initial state\n    step = 0  # Simulation starts at step 0\n    step_plus_1 = step + 1\n\n    # Gradient processing part, which becomes zero under g_t=0 and zeroed state\n    m_t_plus_1 = beta1 * m_t + (1.0 - beta1) * g_t\n    v_t_plus_1 = beta2 * v_t + (1.0 - beta2) * g_t**2\n\n    # Bias correction. 1 - beta**(step+1) is safe since step+1=1.\n    m_hat = m_t_plus_1 / (1.0 - beta1**step_plus_1)\n    v_hat = v_t_plus_1 / (1.0 - beta2**step_plus_1)\n\n    grad_update_term = m_hat / (np.sqrt(v_hat) + epsilon_adam)\n\n    # Decoupled weight decay update\n    w_t_plus_1 = w_t * (1.0 - alpha * lambda_val) - alpha * grad_update_term\n    return w_t_plus_1\n\ndef solve():\n    \"\"\"\n    Solves the problem by running all test cases and printing the results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, coupled regularization in loss)\n        {'optimizer': 'SGD', 'alpha0': 0.1, 'lambda0': 0.01, 'w0': 1.0, \n         'test_alphas': [0.05, 0.1, 0.2], 'epsilon': 1e-12, 'params': {}},\n        # Case 2 (momentum, coupled regularization in loss)\n        {'optimizer': 'MomentumSGD', 'alpha0': 0.1, 'lambda0': 0.01, 'w0': 1.0, \n         'test_alphas': [0.05, 0.1, 0.2], 'epsilon': 1e-12, 'params': {'beta': 0.9}},\n        # Case 3 (decoupled weight decay)\n        {'optimizer': 'AdamW', 'alpha0': 0.001, 'lambda0': 0.01, 'w0': 1.0, \n         'test_alphas': [0.0005, 0.001, 0.002], 'epsilon': 1e-12, \n         'params': {'beta1': 0.9, 'beta2': 0.999, 'epsilon_adam': 1e-8}},\n        # Case 4 (edge case: zero weight decay)\n        {'optimizer': 'SGD', 'alpha0': 0.05, 'lambda0': 0.0, 'w0': 1.0, \n         'test_alphas': [0.01, 0.05, 0.1], 'epsilon': 1e-12, 'params': {}},\n        # Case 5 (coverage across scales)\n        {'optimizer': 'AdamW', 'alpha0': 1e-2, 'lambda0': 5e-3, 'w0': 1.0, \n         'test_alphas': [1e-4, 1e-3, 1e-2, 1e-1], 'epsilon': 1e-12, \n         'params': {'beta1': 0.9, 'beta2': 0.999, 'epsilon_adam': 1e-8}}\n    ]\n    \n    optimizer_updates = {\n        'SGD': update_sgd,\n        'MomentumSGD': update_momentum_sgd,\n        'AdamW': update_adamw\n    }\n\n    final_results = []\n    for case in test_cases:\n        alpha0 = case['alpha0']\n        lambda0 = case['lambda0']\n        w0 = case['w0']\n        epsilon = case['epsilon']\n        update_func = optimizer_updates[case['optimizer']]\n\n        # 1. Compute baseline shrinkage s0\n        w1_base = update_func(w0, alpha0, lambda0, **case['params'])\n        if w0 == 0.0:\n            s0 = 0.0 if w1_base == 0.0 else float('inf')\n        else:\n            s0 = abs(w1_base / w0)\n\n        is_inv_stable = True\n        is_prop_stable = True\n\n        for alpha in case['test_alphas']:\n            # 2. Test inverse-proportional scaling\n            # Handle alpha=0 case, though not in test data.\n            lambda_inv = lambda0 * (alpha0 / alpha) if alpha != 0.0 else float('inf')\n            w1_inv = update_func(w0, alpha, lambda_inv, **case['params'])\n            s_inv = abs(w1_inv / w0) if w0 != 0 else (0.0 if w1_inv == 0.0 else float('inf'))\n            if abs(s_inv - s0) > epsilon:\n                is_inv_stable = False\n\n            # 3. Test proportional scaling\n            # Handle alpha0=0 case, though not in test data.\n            if alpha0 != 0.0:\n                lambda_prop = lambda0 * (alpha / alpha0)\n            else:\n                 lambda_prop = 0.0 if lambda0 == 0.0 else float('inf')\n            w1_prop = update_func(w0, alpha, lambda_prop, **case['params'])\n            s_prop = abs(w1_prop / w0) if w0 != 0.0 else (0.0 if w1_prop == 0.0 else float('inf'))\n            if abs(s_prop - s0) > epsilon:\n                is_prop_stable = False\n        \n        final_results.append(is_inv_stable)\n        final_results.append(is_prop_stable)\n    \n    # Format the final output string exactly as required.\n    # str(True) -> 'True', str(False) -> 'False'\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "到目前为止，我们一直将超参数视为在训练开始前设定的静态值。但如果最佳设置本身取决于数据特性呢？本练习将以数据增强为例，探索这一高级概念。您将构建一个理论模型，其中理想的增强强度随样本难度而变化。通过比较单一全局设置与学习到的自适应策略，您将能够量化动态超参数控制所带来的益处。这项练习为您提供了一个亲身实践的机会，来探索构建数据感知的自适应训练策略这一强大思想。",
            "id": "3135368",
            "problem": "给定一个深度学习中数据增强超参数调整的程式化场景，其中增强强度由标量决策变量 $a$ 表示。目标是探索通过学习策略将 $a$ 与样本难度关联起来，并评估相较于单一全局增强强度的改进效果。\n\n基本和核心定义：\n- 在采用经验风险最小化（ERM）的监督学习中，预期预测风险可以分解为偏差项和方差项。对于一个使用数据增强（可视为注入受控扰动）训练的模型，增加增强强度 $a$ 通常会增加偏差并减少方差。这遵循了经过充分检验的预期误差的偏差-方差分解，以及经验观察：数据增强通过让模型接触更多样化的样本来降低方差，同时可能因扭曲训练信号而增加偏差。\n- 设每个样本 $i$ 具有标量难度 $d_i \\ge 0$。更高的 $d_i$ 表示样本更复杂或更难学习。更高的 $d_i$ 会增加对方法差的敏感度，因此对于更难的样本，方差贡献更大。\n\n基于这些原则，我们假设以下与科学现实和量纲齐次性一致的单调定律：\n- 样本 $i$ 因数据增强产生的方差贡献与 $a$ 成反比，与 $d_i$ 成正比，比例常数为 $\\alpha  0$。\n- 样本 $i$ 因数据增强产生的偏差贡献与 $a$ 成正比，比例常数为 $\\beta  0$。\n- 增强强度 $a$ 被限制在区间 $[a_{\\min}, a_{\\max}]$ 内，其中 $a_{\\min}  0$ 且 $a_{\\max}  a_{\\min}$。\n\n任务：\n1. 从偏差-方差分解和上述单调定律出发，推导一个关于 $a$、$d_i$、$\\alpha$ 和 $\\beta$ 的单样本预期风险函数 $R_i(a)$。推导出的 $R_i(a)$ 必须满足方差项随 $a$ 增大而以 $1/a$ 的形式减小，并随 $d_i$ 增大而增大；偏差项随 $a$ 线性增加。不要使用任何简便公式；推导出一个与所述定性依赖关系和量纲考虑相符的函数。\n2. 使用你推导的 $R_i(a)$，推导出在 $a \\in [a_{\\min}, a_{\\max}]$ 条件下，使单个样本 $i$ 的 $R_i(a)$ 最小化的增强强度 $a_i^\\star$。然后，推导出当所有样本共享同一个常数 $a \\in [a_{\\min}, a_{\\max}]$ 时，使总风险 $\\sum_{i=1}^{n} R_i(a)$ 最小化的全局恒定增强强度 $a_{\\mathrm{global}}^\\star$。\n3. 定义一个基于样本难度的学习策略，形式为 $a(d) = \\gamma \\cdot d^{p}$，其中参数 $\\gamma \\ge 0$ 且 $p \\in [0, 1]$，并受限于可行边界的裁剪：$a(d) := \\min\\{\\max\\{\\gamma \\cdot d^{p}, a_{\\min}\\}, a_{\\max}\\}$。通过在给定数据集上最小化总风险 $\\sum_{i=1}^{n} R_i(a(d_i))$ 来学习 $(\\gamma, p)$。如果在裁剪条件下没有封闭形式的解，你可以使用正确的离散搜索来寻找 $(\\gamma, p)$。\n4. 评估学习策略相对于全局恒定增强的改进效果，其定义为分数改进 $I = \\dfrac{R_{\\mathrm{global}} - R_{\\mathrm{policy}}}{R_{\\mathrm{global}}}$，以小数形式表示（非百分比）。其中 $R_{\\mathrm{global}}$ 是使用 $a_{\\mathrm{global}}^\\star$ 时的最小化总风险，而 $R_{\\mathrm{policy}}$ 是使用学习策略时的最小化总风险。\n5. 你的程序必须实现上述推导和计算，并生成单行输出，其中包含每个测试用例的改进值，格式为用方括号括起来的逗号分隔列表，每个改进值四舍五入到 $4$ 位小数。\n\n测试套件和参数：\n- 使用以下 $4$ 个测试用例。在所有情况下，样本总数 $n$ 等于难度列表的长度。对于每个测试，程序应计算如上定义的 $I$。\n\n案例 1 (一般情况):\n- 难度: $[0.5, 1.0, 2.0, 4.0]$\n- $\\alpha = 1.0$, $\\beta = 0.1$, $a_{\\min} = 0.05$, $a_{\\max} = 10.0$\n\n案例 2 (均匀难度边界):\n- 难度: $[1.0, 1.0, 1.0, 1.0, 1.0]$\n- $\\alpha = 1.0$, $\\beta = 0.1$, $a_{\\min} = 0.05$, $a_{\\max} = 10.0$\n\n案例 3 (包含零难度边缘情况):\n- 难度: $[0.0, 0.2, 0.5, 1.5, 3.0]$\n- $\\alpha = 1.0$, $\\beta = 0.1$, $a_{\\min} = 0.05$, $a_{\\max} = 10.0$\n\n案例 4 (极端难度与上界裁剪):\n- 难度: $[0.1, 10.0, 1000.0]$\n- $\\alpha = 1.0$, $\\beta = 0.1$, $a_{\\min} = 0.05$, $a_{\\max} = 1.0$\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含案例 $1$ 到 $4$ 的分数改进值，格式为用方括号括起来的逗号分隔列表，例如 $[i_1,i_2,i_3,i_4]$，其中每个 $i_k$ 四舍五入到 $4$ 位小数。",
            "solution": "用户提供了一个问题，要求在一个程式化的深度学习背景下，推导和比较两种针对数据增强强度的超参数调整策略。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- 增强强度是一个标量决策变量 $a$。\n- 样本难度是一个标量 $d_i \\ge 0$。\n- 风险被分解为由数据增强引起的偏差和方差贡献。\n- 方差贡献与 $a$ 成反比，与 $d_i$ 成正比，比例常数为 $\\alpha  0$。\n- 偏差贡献与 $a$ 成正比，比例常数为 $\\beta  0$。\n- 增强强度有界：$a \\in [a_{\\min}, a_{\\max}]$，其中 $a_{\\min}  0$ 且 $a_{\\max}  a_{\\min}$。\n- 学习的增强策略形式为 $a(d) = \\gamma \\cdot d^{p}$，其中 $\\gamma \\ge 0$ 且 $p \\in [0, 1]$，并被裁剪到 $[a_{\\min}, a_{\\max}]$ 区间内。\n- 分数改进定义为 $I = \\dfrac{R_{\\mathrm{global}} - R_{\\mathrm{policy}}}{R_{\\mathrm{global}}}$。\n- 提供了四个测试用例，包含具体的难度值、$\\alpha$、$\\beta$、$a_{\\min}$ 和 $a_{\\max}$。\n\n**步骤2：使用提取的已知条件进行验证**\n根据验证标准评估该问题：\n- **科学依据**：该问题使用了一个简化但合理的模型来描述与数据增强相关的偏差-方差权衡。所假设的关系（方差 $\\propto \\frac{d_i}{a}$，偏差 $\\propto a$）与机器学习中的普遍观察结果一致，为这个程式化的优化问题提供了科学上合理的基础。\n- **适定性**：目标是在一个闭区间上最小化一个关于变量 $a$ 的凸风险函数。这确保了全局和单样本优化任务解的存在性和唯一性。策略优化通过指定的搜索来执行，这也是明确定义的。\n- **客观性**：所有术语都进行了定量定义，任务以数学精度进行规定。问题没有主观或模糊的语言。\n- **完整性和一致性**：为每个任务和测试用例提供了所有必需的常数和条件。没有内部矛盾。\n- **可行性**：该问题是一个计算模拟。所提供的参数在数值上是合理的。\n- **结构性**：问题结构逻辑清晰，从基本推导到比较分析。它并非微不足道，因为它需要微积分、约束优化和数值搜索。\n\n**步骤3：结论与行动**\n该问题是**有效的**。这是一个定义明确的数学建模和优化问题，植根于相关的机器学习概念。我现在将着手解决。\n\n---\n\n### 分步解决方案\n\n**任务1：推导单样本预期风险函数 $R_i(a)$**\n\n问题陈述，样本 $i$ 的预期风险 $R_i$ 由数据增强引起的偏差贡献和方差贡献组成。\n- 方差贡献“与 $a$ 成反比，与 $d_i$ 成正比”。对于比例常数 $\\alpha  0$，这可以写成 $V_i(a) = \\frac{\\alpha d_i}{a}$。该形式正确地随 $a$ 增大而减小。\n- 偏差贡献“与 $a$ 成正比”。对于比例常数 $\\beta  0$，这可以写成 $B_i(a) = \\beta a$。该形式正确地随 $a$ 增大而增大。\n\n样本 $i$ 的总风险函数是这两项贡献之和。这是满足所述单调定律的最简单的函数形式。\n$$R_i(a) = V_i(a) + B_i(a) = \\frac{\\alpha d_i}{a} + \\beta a$$\n这个函数模拟了权衡关系：小的 $a$ 导致高方差风险，而大的 $a$ 导致高偏差风险。该函数定义在 $a  0$ 的情况下。\n\n**任务2：推导最优增强强度 $a_i^\\star$ 和 $a_{\\mathrm{global}}^\\star$**\n\n**单样本最优强度 $a_i^\\star$**\n为了找到使给定样本 $i$ 的 $R_i(a)$ 最小化的增强强度 $a$，我们首先通过求 $R_i(a)$ 对 $a$ 的导数并将其设为零来找到无约束最小值。\n$$\\frac{dR_i(a)}{da} = \\frac{d}{da}\\left(\\frac{\\alpha d_i}{a} + \\beta a\\right) = -\\frac{\\alpha d_i}{a^2} + \\beta$$\n将导数设为零以找到临界点：\n$$-\\frac{\\alpha d_i}{a^2} + \\beta = 0 \\implies \\beta a^2 = \\alpha d_i \\implies a^2 = \\frac{\\alpha d_i}{\\beta}$$\n无约束的最优增强强度为 $a_{\\text{unc},i} = \\sqrt{\\frac{\\alpha d_i}{\\beta}}$。这是一个最小值，因为二阶导数 $\\frac{d^2R_i(a)}{da^2} = \\frac{2\\alpha d_i}{a^3}$ 对于 $a, \\alpha, d_i  0$ 是正的。如果 $d_i=0$，风险函数为 $R_i(a) = \\beta a$，它在 $a$ 的最小可能值处达到最小值。\n\n增强强度 $a$ 被约束在区间 $[a_{\\min}, a_{\\max}]$ 内。由于 $R_i(a)$ 是一个凸函数，其在此区间上的最小值可以通过将无约束最优值裁剪到区间的边界来找到：\n$$a_i^\\star = \\min\\left(\\max\\left(a_{\\text{unc},i}, a_{\\min}\\right), a_{\\max}\\right)$$\n如果 $d_i=0$，则 $a_{\\text{unc},i}=0$ 且 $a_i^\\star = \\max(0, a_{\\min}) = a_{\\min}$，这是一致的。\n\n**全局最优强度 $a_{\\mathrm{global}}^\\star$**\n全局策略对所有 $n$ 个样本使用单一的增强强度 $a$。总风险是各个风险之和：\n$$R_{\\text{total}}(a) = \\sum_{i=1}^{n} R_i(a) = \\sum_{i=1}^{n} \\left(\\frac{\\alpha d_i}{a} + \\beta a\\right)$$\n将不依赖于求和索引 $i$ 的项提取出来：\n$$R_{\\text{total}}(a) = \\frac{\\alpha}{a} \\sum_{i=1}^{n} d_i + \\sum_{i=1}^{n} \\beta a = \\frac{\\alpha \\left(\\sum_{i=1}^{n} d_i\\right)}{a} + n\\beta a$$\n这个总风险函数与单样本风险 $R_i(a)$ 具有相同的数学形式，只是 $\\alpha d_i$ 被替换为 $\\alpha \\sum d_i$，$\\beta$ 被替换为 $n\\beta$。我们可以用同样的方法找到最小值：\n$$\\frac{dR_{\\text{total}}(a)}{da} = -\\frac{\\alpha \\sum d_i}{a^2} + n\\beta = 0 \\implies a^2 = \\frac{\\alpha \\sum d_i}{n\\beta}$$\n无约束的全局最优值为 $a_{\\text{unc, global}} = \\sqrt{\\frac{\\alpha \\sum d_i}{n\\beta}}$。这可以用平均难度 $\\bar{d} = \\frac{1}{n}\\sum d_i$ 来表示为 $a_{\\text{unc, global}} = \\sqrt{\\frac{\\alpha \\bar{d}}{\\beta}}$。\n应用约束 $[a_{\\min}, a_{\\max}]$，最优的全局强度为：\n$$a_{\\mathrm{global}}^\\star = \\min\\left(\\max\\left(\\sqrt{\\frac{\\alpha \\sum d_i}{n\\beta}}, a_{\\min}\\right), a_{\\max}\\right)$$\n\n**任务3：学习最优策略 $a(d) = \\gamma d^p$**\n\n策略定义为 $a(d_i) = \\min\\{\\max\\{\\gamma \\cdot d_i^{p}, a_{\\min}\\}, a_{\\max}\\}$，我们必须找到使总风险最小化的参数 $(\\gamma, p)$。此策略下的总风险为：\n$$R_{\\mathrm{policy}}(\\gamma, p) = \\sum_{i=1}^{n} R_i(a(d_i)) = \\sum_{i=1}^{n} \\left( \\frac{\\alpha d_i}{a(d_i)} + \\beta a(d_i) \\right)$$\n由于对 $(\\gamma, p)$ 的非线性依赖和裁剪函数的存在，无法轻易获得封闭形式的解。按照许可，我们将在参数空间 $\\gamma \\ge 0$ 和 $p \\in [0, 1]$ 上进行离散网格搜索。无约束的单样本最优值 $a_i^\\star \\propto d_i^{0.5}$ 表明，最优参数可能在 $p=0.5$ 和 $\\gamma = \\sqrt{\\alpha/\\beta}$ 附近。应选择 $(\\gamma, p)$ 的搜索网格以覆盖这些值周围的合理范围。对 $\\gamma$ 使用对数尺度、对 $p$ 使用线性尺度进行细粒度搜索是合适的。对于网格中的每一对 $(\\gamma, p)$，我们计算 $R_{\\mathrm{policy}}(\\gamma, p)$ 并找出产生最小总风险的参数对。\n\n**任务4：评估改进效果**\n\n学习策略相对于全局恒定增强的改进效果，按总风险的分数减少量计算。\n首先，我们计算全局策略的最小化总风险：\n$$R_{\\mathrm{global}} = R_{\\text{total}}(a_{\\mathrm{global}}^\\star) = \\frac{\\alpha \\sum d_i}{a_{\\mathrm{global}}^\\star} + n\\beta a_{\\mathrm{global}}^\\star$$\n接下来，我们计算通过网格搜索找到的策略策略的最小化总风险：\n$$R_{\\mathrm{policy}} = \\min_{\\gamma,p} R_{\\mathrm{policy}}(\\gamma, p)$$\n那么，分数改进 $I$ 为：\n$$I = \\frac{R_{\\mathrm{global}} - R_{\\mathrm{policy}}}{R_{\\mathrm{global}}}$$\n当 $I  0$ 时，表示自适应策略优于单一的全局增强强度。如果最优策略恰好是恒定增强（例如，当所有难度都相同时，或者当裁剪迫使所有 $a(d_i)$ 都等于某个边界值时），那么 $R_{\\mathrm{policy}}$ 将等于 $R_{\\mathrm{global}}$，并且 $I$ 将为 $0$。",
            "answer": "```python\nimport numpy as np\nfrom typing import List, Dict, Union\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It calculates the optimal global augmentation, finds the best policy via grid search,\n    and computes the fractional improvement for each case.\n    \"\"\"\n    test_cases: List[Dict[str, Union[List[float], float]]] = [\n        {\n            \"difficulties\": [0.5, 1.0, 2.0, 4.0],\n            \"alpha\": 1.0, \"beta\": 0.1, \"amin\": 0.05, \"amax\": 10.0\n        },\n        {\n            \"difficulties\": [1.0, 1.0, 1.0, 1.0, 1.0],\n            \"alpha\": 1.0, \"beta\": 0.1, \"amin\": 0.05, \"amax\": 10.0\n        },\n        {\n            \"difficulties\": [0.0, 0.2, 0.5, 1.5, 3.0],\n            \"alpha\": 1.0, \"beta\": 0.1, \"amin\": 0.05, \"amax\": 10.0\n        },\n        {\n            \"difficulties\": [0.1, 10.0, 1000.0],\n            \"alpha\": 1.0, \"beta\": 0.1, \"amin\": 0.05, \"amax\": 1.0\n        }\n    ]\n\n    results = []\n    \n    # Define the search grid for policy parameters (gamma, p)\n    # p is in [0, 1]. A linear grid is suitable.\n    p_grid = np.linspace(0, 1, 101) \n    # gamma can vary over orders of magnitude. A log-spaced grid is more efficient.\n    # The range is chosen to be wide enough to find optima across test cases.\n    gamma_grid = np.logspace(-2, 3, 301) # from 0.01 to 1000\n\n    for case in test_cases:\n        # Extract parameters\n        difficulties = np.array(case[\"difficulties\"])\n        alpha = case[\"alpha\"]\n        beta = case[\"beta\"]\n        amin = case[\"amin\"]\n        amax = case[\"amax\"]\n        \n        n = len(difficulties)\n        d_sum = np.sum(difficulties)\n\n        # 1. Calculate the total risk for the optimal global augmentation\n        if n * beta == 0:\n            # Avoid division by zero, although problem constraints prevent this\n            a_unc_global = float('inf')\n        else:\n            # Handle d_sum=0 case, where sqrt would fail for negative numbers\n            # not an issue here since alpha, beta > 0\n             a_unc_global = np.sqrt(alpha * d_sum / (n * beta))\n        \n        a_global_star = np.clip(a_unc_global, amin, amax)\n        \n        risk_global = alpha * d_sum / a_global_star + n * beta * a_global_star\n\n        # 2. Find the minimal risk for the learned policy via grid search\n        min_policy_risk = float('inf')\n        \n        for p in p_grid:\n            for gamma in gamma_grid:\n                # The policy a(d) = clip(gamma * d^p, amin, amax)\n                # handle d=0 case. np.power(0,0) is 1. np.power(0, p>0) is 0.\n                # This aligns with the mathematical derivation.\n                a_policy_values = np.clip(gamma * np.power(difficulties, p, where=difficulties!=0, out=np.zeros_like(difficulties, dtype=float)), amin, amax)\n                \n                # Calculate total risk for this policy\n                # R = sum(alpha*d/a + beta*a)\n                # np.divide is used for safe division, though a_policy_values > 0 due to amin > 0\n                risk_per_sample = np.divide(alpha * difficulties, a_policy_values, out=np.zeros_like(a_policy_values), where=a_policy_values!=0) + beta * a_policy_values\n                current_policy_risk = np.sum(risk_per_sample)\n                \n                if current_policy_risk  min_policy_risk:\n                    min_policy_risk = current_policy_risk\n        \n        risk_policy = min_policy_risk\n        \n        # 3. Calculate fractional improvement\n        if risk_global == 0:\n            # This case is unlikely given the problem setup but is good practice.\n            improvement = 0.0\n        else:\n            improvement = (risk_global - risk_policy) / risk_global\n        \n        # The policy search space includes constant policies (p=0), so risk_policy = risk_global.\n        # We can clip at 0 to handle potential minor floating point inaccuracies.\n        improvement = max(0.0, improvement)\n        \n        results.append(round(improvement, 4))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}