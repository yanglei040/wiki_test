{
    "hands_on_practices": [
        {
            "introduction": "A core task for a reinforcement learning agent is to learn an optimal strategy through trial and error. This exercise  illuminates the fundamental distinction between on-policy and off-policy learning, which governs how an agent uses its experiences. By implementing and comparing the Q-learning and SARSA algorithms in a carefully designed environment with a risky shortcut, you will gain a practical and intuitive understanding of how these different approaches handle exploration and risk.",
            "id": "3113683",
            "problem": "Consider a finite Markov Decision Process (MDP) with a single nonterminal start state and two disjoint corridors to a goal, one corridor being risky due to an action that can terminate in a cliff with a large negative return, and one corridor being safe because all actions avoid catastrophic termination. The purpose is to compare the convergence dynamics of State-Action-Reward-State-Action (SARSA) and Q-learning in a stochastic setting with an exploratory behavior policy, highlighting on-policy versus off-policy differences via the action-value functions $Q^{\\pi}$ and $Q^{*}$.\n\nThe finite MDP is specified as follows. The set of states is $\\{s_{0}, r_{1}, r_{2}, u_{1}, u_{2}, u_{3}, g, c\\}$, where $s_{0}$ is the start state, $g$ is the goal terminal state, and $c$ is the cliff terminal state. The set of actions is the same for all states and equals $\\{0,1,2\\}$; for interpretability:\n- At $s_{0}$, action $0$ is \"go risky,\" action $1$ is \"go safe,\" and action $2$ is \"stall.\"\n- At corridor states $r_{1}, r_{2}, u_{1}, u_{2}, u_{3}$, action $0$ is \"forward,\" action $1$ is \"stall,\" and action $2$ is \"slip.\"\n- At terminal states $g$ and $c$, episodes terminate immediately.\n\nThe deterministic transitions and rewards are:\n- From $s_{0}$:\n  - Action $0$ transitions to $r_{1}$ with reward $-1$.\n  - Action $1$ transitions to $u_{1}$ with reward $-1$.\n  - Action $2$ transitions to $s_{0}$ with reward $-1$.\n- Risky corridor:\n  - From $r_{1}$: action $0$ to $r_{2}$ with reward $-1$, action $1$ to $r_{1}$ with reward $-1$, action $2$ to $c$ with reward $-100$.\n  - From $r_{2}$: action $0$ to $g$ with reward $0$, action $1$ to $r_{2}$ with reward $-1$, action $2$ to $c$ with reward $-100$.\n- Safe corridor:\n  - From $u_{1}$: action $0$ to $u_{2}$ with reward $-1$, action $1$ to $u_{1}$ with reward $-1$, action $2$ to $u_{1}$ with reward $-1$.\n  - From $u_{2}$: action $0$ to $u_{3}$ with reward $-1$, action $1$ to $u_{2}$ with reward $-1$, action $2$ to $u_{2}$ with reward $-1$.\n  - From $u_{3}$: action $0$ to $g$ with reward $0$, action $1$ to $u_{3}$ with reward $-1$, action $2$ to $u_{3}$ with reward $-1$.\n\nThe behavior policy used during learning is $\\epsilon$-greedy over the current action-value estimates $Q(s,a)$: with probability $1-\\epsilon$ select an action that maximizes $Q(s,a)$ and with probability $\\epsilon$ select an action uniformly at random from the action set. If multiple actions tie for the maximum $Q(s,a)$, choose the action with the smallest index.\n\nTraining requirements:\n- Implement tabular Q-learning and tabular State-Action-Reward-State-Action (SARSA) with constant learning rate $\\alpha$ and discount factor $\\gamma$; initialize all action-value estimates $Q(s,a)$ to $0$.\n- Episodes start at $s_{0}$ and terminate upon reaching $g$ or $c$, or after a fixed cap of $M$ steps to prevent infinite loops. Use $M = 100$.\n- Use a fixed pseudorandom seed for reproducible action selection during $\\epsilon$-greedy exploration.\n\nEvaluation:\n- After training, compute the differences\n  $$d_{\\mathrm{QL}} = Q_{\\mathrm{QL}}(s_{0}, 0) - Q_{\\mathrm{QL}}(s_{0}, 1), \\quad d_{\\mathrm{SARSA}} = Q_{\\mathrm{SARSA}}(s_{0}, 0) - Q_{\\mathrm{SARSA}}(s_{0}, 1),$$\n  where $Q_{\\mathrm{QL}}$ and $Q_{\\mathrm{SARSA}}$ are the learned action-value functions under Q-learning and SARSA respectively. A positive difference indicates a learned preference for the risky action at the start, while a negative difference indicates a learned preference for the safe action.\n\nFundamental base to be used in the derivation and implementation:\n- The definition of an action-value function for a policy $Q^{\\pi}(s,a)$ in a finite Markov Decision Process (MDP).\n- The Bellman expectation equation for $Q^{\\pi}(s,a)$ and the Bellman optimality equation for $Q^{*}(s,a)$.\n- The $\\epsilon$-greedy policy construction for exploration in reinforcement learning.\n\nTest suite:\n- Case $1$: $\\epsilon = 0.10$, $\\alpha = 0.50$, $\\gamma = 1.00$, episodes $= 20000$, seed $= 7$.\n- Case $2$ (boundary): $\\epsilon = 0.00$, $\\alpha = 0.50$, $\\gamma = 1.00$, episodes $= 10000$, seed $= 7$.\n- Case $3$ (heavy exploration): $\\epsilon = 0.40$, $\\alpha = 0.50$, $\\gamma = 1.00$, episodes $= 30000$, seed $= 7$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, the pair of differences for each test case flattened into a single list:\n  $$[d_{\\mathrm{QL}}^{(1)}, d_{\\mathrm{SARSA}}^{(1)}, d_{\\mathrm{QL}}^{(2)}, d_{\\mathrm{SARSA}}^{(2)}, d_{\\mathrm{QL}}^{(3)}, d_{\\mathrm{SARSA}}^{(3)}],$$\n  where the superscript $(i)$ indexes test cases $1$ through $3$. Each entry must be a real number (float).",
            "solution": "The problem presented requires a comparative analysis of two fundamental temporal-difference (TD) control algorithms in reinforcement learning: Q-learning and State-Action-Reward-State-Action (SARSA). The analysis is performed within a specifically constructed finite Markov Decision Process (MDP) designed to highlight the crucial distinction between off-policy and on-policy learning.\n\nFirst, we formalize the foundational concepts. A finite MDP is a tuple $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$, where $\\mathcal{S}$ is the finite set of states, $\\mathcal{A}$ is the finite set of actions, $P$ is the state transition probability function $P(s'|s,a) = \\Pr(S_{t+1}=s'|S_t=s, A_t=a)$, $R$ is the reward function $R(s,a,s') = \\mathbb{E}[R_{t+1}|S_t=s, A_t=a, S_{t+1}=s']$, and $\\gamma \\in [0, 1]$ is the discount factor. In this problem, the transitions are deterministic, so $P(s'|s,a) = 1$ for a specific $s'$ and $0$ otherwise.\n\nThe objective of an agent is to learn a policy, $\\pi(a|s) = \\Pr(A_t=a|S_t=s)$, that maximizes the expected discounted cumulative reward. The value of a state-action pair $(s,a)$ under a policy $\\pi$ is given by the action-value function $Q^{\\pi}(s,a)$:\n$$Q^{\\pi}(s,a) = \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\bigg| S_t=s, A_t=a \\right]$$\nThis function satisfies the Bellman expectation equation:\n$$Q^{\\pi}(s,a) = \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^{\\pi}(s',a') \\right]$$\nThe optimal action-value function, $Q^{*}(s,a) = \\max_{\\pi} Q^{\\pi}(s,a)$, provides the maximum expected return achievable from any state-action pair. It satisfies the Bellman optimality equation:\n$$Q^{*}(s,a) = \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma \\max_{a'} Q^{*}(s',a') \\right]$$\n\nQ-learning and SARSA are TD methods that learn these action-value functions iteratively without a model of the environment. Their update rules reveal their fundamental difference.\n\nQ-learning is an off-policy algorithm. Its update rule for an action value $Q(S_t, A_t)$ following a transition from state $S_t$ to $S_{t+1}$ with reward $R_{t+1}$ is:\n$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t) \\right]$$\nThe term $\\max_{a'} Q(S_{t+1}, a')$ represents the estimated value of the optimal action from the next state, $S_{t+1}$. Q-learning uses this greedy choice to update its value function, irrespective of which action is actually taken next by the behavior policy. Consequently, it directly learns an estimate of the optimal action-value function, $Q^*$.\n\nSARSA is an on-policy algorithm. Its update rule uses the tuple $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$:\n$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]$$\nHere, the update target includes $Q(S_{t+1}, A_{t+1})$, the value of the action $A_{t+1}$ that was actually selected in state $S_{t+1}$ according to the behavior policy. Thus, SARSA learns the action-value function for the policy it is currently following, $Q^{\\pi}$. In this problem, the policy $\\pi$ is $\\epsilon$-greedy with respect to the current $Q$-values.\n\nNow, we analyze the specific MDP. An agent at the start state $s_0$ can choose a \"risky\" corridor (action $0$) or a \"safe\" corridor (action $1$). The discount factor is $\\gamma=1$, meaning we seek to maximize the undiscounted sum of rewards.\nThe optimal path to the goal is via the risky corridor: $s_0 \\to r_1 \\to r_2 \\to g$. This path takes $3$ steps, with a total reward of $(-1) + (-1) + 0 = -2$.\nThe safe path is longer: $s_0 \\to u_1 \\to u_2 \\to u_3 \\to g$. This path takes $4$ steps, with a total reward of $(-1) + (-1) + (-1) + 0 = -3$.\nTherefore, the optimal action-value for starting at $s_0$ is $Q^*(s_0, 0) = -2$ and $Q^*(s_0, 1) = -3$.\n\nWhen $\\epsilon > 0$, the behavior policy is exploratory.\nQ-learning, being off-policy, aims to find $Q^*$. It learns that the risky path is superior ($Q(s_0, 0) \\approx -2$ and $Q(s_0, 1) \\approx -3$), despite the fact that its own exploratory actions might occasionally lead it off the cliff. Its value estimates are based on the assumption that it will act greedily in the future. We therefore predict that $Q_{\\mathrm{QL}}(s_0, 0) > Q_{\\mathrm{QL}}(s_0, 1)$, resulting in a positive difference $d_{\\mathrm{QL}} > 0$.\n\nSARSA, being on-policy, learns the value of the $\\epsilon$-greedy policy itself. This policy has a non-zero probability ($\\epsilon/3$) of selecting the \"slip\" action (action $2$) in states $r_1$ and $r_2$, which leads to the cliff and a large negative reward of $-100$. This possibility is factored into SARSA's value estimates. The expected return for taking the risky path is thus a weighted average of the optimal outcome and the catastrophic \"slip\" outcome. For a sufficiently large penalty, this expected return will be lower than the expected return of the safe path, where exploration (\"slip\" or \"stall\") is not catastrophic. SARSA will learn that, under its own exploratory policy, the safe corridor is preferable. We predict $Q_{\\mathrm{SARSA}}(s_0, 0) < Q_{\\mathrm{SARSA}}(s_0, 1)$, resulting in a negative difference $d_{\\mathrm{SARSA}} < 0$. The magnitude of this effect should increase with $\\epsilon$.\n\nFor the case where $\\epsilon=0.00$, the policy is purely greedy. The action $A_{t+1}$ chosen in SARSA will be $\\arg\\max_{a'} Q(S_{t+1}, a')$. In this specific scenario, the SARSA update rule becomes functionally identical to the Q-learning update rule, assuming a unique maximum. Both algorithms will follow the same greedy trajectory and update their values identically. They will both converge towards $Q^*$ and learn that the risky path is optimal. We thus expect $d_{\\mathrm{QL}}^{(2)} > 0$ and $d_{\\mathrm{SARSA}}^{(2)} > 0$, with their values being very close. This demonstrates SARSA's \"conservative\" nature is a direct result of on-policy evaluation of an exploratory policy.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Q-learning and SARSA on a specified MDP\n    to demonstrate the difference between off-policy and on-policy learning.\n    \"\"\"\n    \n    #\n    # Environment Setup\n    #\n    states = ['s0', 'r1', 'r2', 'u1', 'u2', 'u3', 'g', 'c']\n    state_map = {name: i for i, name in enumerate(states)}\n    num_states = len(states)\n    num_actions = 3\n    \n    s0, r1, r2, u1, u2, u3, g, c = (state_map[s] for s in states)\n    terminal_states = {g, c}\n    \n    # Transitions: T[state_idx][action_idx] = (next_state_idx, reward)\n    T = {\n        s0: {0: (r1, -1.0), 1: (u1, -1.0), 2: (s0, -1.0)},\n        r1: {0: (r2, -1.0), 1: (r1, -1.0), 2: (c, -100.0)},\n        r2: {0: (g, 0.0),  1: (r2, -1.0), 2: (c, -100.0)},\n        u1: {0: (u2, -1.0), 1: (u1, -1.0), 2: (u1, -1.0)},\n        u2: {0: (u3, -1.0), 1: (u2, -1.0), 2: (u2, -1.0)},\n        u3: {0: (g, 0.0),  1: (u3, -1.0), 2: (u3, -1.0)},\n    }\n    \n    #\n    # Agent and Policy Logic\n    #\n    def select_action(q_s, epsilon, rng):\n        \"\"\"Selects an action using an epsilon-greedy policy.\"\"\"\n        if rng.random() < epsilon:\n            return rng.integers(num_actions)\n        else:\n            # np.argmax handles tie-breaking by returning the first index\n            # with the maximum value, which matches the problem spec.\n            return np.argmax(q_s)\n\n    #\n    # Training Function\n    #\n    def run_experiment(algorithm, params, rng):\n        \"\"\"Runs a full training experiment for a given algorithm.\"\"\"\n        epsilon, alpha, gamma, total_episodes, max_steps = params\n        \n        Q = np.zeros((num_states, num_actions))\n\n        for _ in range(total_episodes):\n            state = s0\n            \n            # For SARSA, the first action must be selected before the loop begins.\n            if algorithm == 'sarsa':\n                action = select_action(Q[state], epsilon, rng)\n\n            for _ in range(max_steps):\n                if state in terminal_states:\n                    break\n                \n                # For Q-learning, action is selected inside the main loop.\n                if algorithm == 'q_learning':\n                    action = select_action(Q[state], epsilon, rng)\n\n                # Get next state and reward from the environment model.\n                next_state, reward = T[state][action]\n                \n                if algorithm == 'q_learning':\n                    # Off-policy update: uses max Q-value at next state\n                    if next_state in terminal_states:\n                        target = reward\n                    else:\n                        target = reward + gamma * np.max(Q[next_state])\n                    Q[state, action] += alpha * (target - Q[state, action])\n                \n                elif algorithm == 'sarsa':\n                    # On-policy update: uses Q-value of the actual next action\n                    if next_state in terminal_states:\n                        # No next action, target is just the reward\n                        target = reward\n                    else:\n                        next_action = select_action(Q[next_state], epsilon, rng)\n                        target = reward + gamma * Q[next_state, next_action]\n                    Q[state, action] += alpha * (target - Q[state, action])\n                    # Update for next iteration\n                    action = next_action if next_state not in terminal_states else 0\n\n                state = next_state\n        return Q\n\n    #\n    # Main Execution Logic\n    #\n    test_cases = [\n        # (epsilon, alpha, gamma, episodes, seed)\n        (0.10, 0.50, 1.00, 20000, 7),\n        (0.00, 0.50, 1.00, 10000, 7),\n        (0.40, 0.50, 1.00, 30000, 7),\n    ]\n    max_steps_per_episode = 100\n    all_results = []\n    \n    for epsilon, alpha, gamma, episodes, seed in test_cases:\n        params = (epsilon, alpha, gamma, episodes, max_steps_per_episode)\n        \n        # Run Q-learning for the current case\n        rng_ql = np.random.default_rng(seed)\n        Q_ql = run_experiment('q_learning', params, rng_ql)\n        d_ql = Q_ql[s0, 0] - Q_ql[s0, 1]\n        \n        # Run SARSA for the current case, resetting the seed for fair comparison\n        rng_sarsa = np.random.default_rng(seed)\n        Q_sarsa = run_experiment('sarsa', params, rng_sarsa)\n        d_sarsa = Q_sarsa[s0, 0] - Q_sarsa[s0, 1]\n        \n        all_results.extend([d_ql, d_sarsa])\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "When scaling reinforcement learning with deep neural networks, ensuring the stability of the learning process becomes a major challenge. This practice  moves beyond treating the target network as a simple heuristic by guiding you through a mathematical analysis of its stabilizing effects. By modeling the learning dynamics as a linear system, you will discover how the update frequency of the target network can prevent destructive oscillations and gain deeper insight into the engineering principles that make Deep Q-Networks effective.",
            "id": "3113592",
            "problem": "You are given a simplified linear model of the learning dynamics of a Deep Q-Network (DQN), which is a Deep Reinforcement Learning (DRL) algorithm. The goal is to analytically approximate when oscillatory behavior emerges as a function of the target network update period and to verify this approximation by measuring the oscillation spectra of the learned action-value function using the Fast Fourier Transform (FFT). The approximation must start from first principles of Reinforcement Learning and linear system theory, without relying on shortcut formulas.\n\nConsider a deterministic two-state Markov Decision Process (MDP) with two scalar action-value function components $q_1$ and $q_2$ that represent the online network estimates for the two states. Let $h_1$ and $h_2$ denote the corresponding target network estimates. The environment is deterministic and transitions from state $1$ to state $2$ and from state $2$ to state $1$. Rewards are constant and given by $r_1$ and $r_2$. The discount factor is $\\gamma \\in [0,1)$ and the learning rate is $\\alpha \\in (0,1)$.\n\nThe canonical Bellman optimality relations for this deterministic setting are\n$$\nQ^\\star_1 = r_1 + \\gamma Q^\\star_2, \\quad Q^\\star_2 = r_2 + \\gamma Q^\\star_1,\n$$\nwhere $Q^\\star_i$ denotes the optimal action-value for state $i$. A Deep Q-Network (DQN) approximates these relations via temporal-difference updates. With a fixed target network during a target-hold interval, the per-step online updates are\n$$\nq_1(t+1) = q_1(t) + \\alpha \\left( r_1 + \\gamma h_2(t) - q_1(t) \\right),\n$$\n$$\nq_2(t+1) = q_2(t) + \\alpha \\left( r_2 + \\gamma h_1(t) - q_2(t) \\right),\n$$\nand the target network is updated by a hard copy every $T$ steps:\n$$\nh_i(t+1) = \n\\begin{cases}\nq_i(t+1), & \\text{if } (t+1) \\bmod T = 0, \\\\\nh_i(t), & \\text{otherwise},\n\\end{cases}\n\\quad i \\in \\{1,2\\}.\n$$\n\nWithin any target-hold interval of length $T$, treat $h_1$ and $h_2$ as constants equal to the last copied values $s_1$ and $s_2$. Derive the across-cycle linear affine map that advances the pair $(s_1, s_2)$ to the next cycle's pair $(s'_1, s'_2)$ after $T$ online updates and one target copy. Then, obtain the eigenvalues of the resulting linear part to determine the presence of an oscillatory mode. Use the presence of a negative eigenvalue to define a predicted per-step oscillation frequency, and validate this prediction by computing the FFT of the time series of the difference mode $d(t) = q_1(t) - q_2(t)$.\n\nYour program must:\n- Start from the above per-step DQN dynamics and derive the cycle-to-cycle linear map for $(s_1, s_2)$, express its matrix in terms of $\\alpha$, $\\gamma$, and $T$, and compute its eigenvalues.\n- Use the eigenstructure to predict the emergence of an oscillatory mode. Specifically, if the smaller eigenvalue is negative, define the predicted angular frequency per step as $\\omega_{\\text{pred}} = \\pi / T$; otherwise, set $\\omega_{\\text{pred}} = 0$.\n- Simulate the per-step dynamics for $N$ steps to generate $d(t)$ and compute its FFT magnitude spectrum, excluding the zero-frequency bin, to obtain the measured peak angular frequency $\\omega_{\\text{meas}}$ in radians per step.\n- Report, for each test case, the tuple $[\\omega_{\\text{pred}}, \\omega_{\\text{meas}}, \\text{resonant}]$, where $\\text{resonant}$ is a boolean that is $\\text{True}$ if the smaller eigenvalue is negative and $\\text{False}$ otherwise.\n\nUse the following fixed parameter values for all test cases:\n- Discount factor $\\gamma = 0.9$,\n- Learning rate $\\alpha = 0.2$,\n- Rewards $r_1 = 0$ and $r_2 = 0$,\n- Initial online values $q_1(0) = 1$ and $q_2(0) = -1$,\n- Initial target values $h_1(0) = q_1(0)$ and $h_2(0) = q_2(0)$,\n- Simulation length $N = 4096$ steps.\n\nAngle unit specification: All angular frequencies must be expressed in radians per step.\n\nTest suite:\n- Sweep the target update period $T$ across the values $T \\in \\{1, 3, 4, 20, 50\\}$ to cover frequent updates, near-threshold behavior, and long target-hold intervals.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case, output a nested list $[\\omega_{\\text{pred}}, \\omega_{\\text{meas}}, \\text{resonant}]$, with both $\\omega_{\\text{pred}}$ and $\\omega_{\\text{meas}}$ rounded to six decimal places, and $\\text{resonant}$ as a boolean.\n- Example format: $[[0.785398,0.780000,True],[0.000000,0.020000,False],\\dots]$.",
            "solution": "The problem requires an analysis of the oscillatory dynamics in a simplified Deep Q-Network (DQN) model. This will be accomplished in two parts: first, a theoretical derivation of the condition for oscillation based on a linear systems analysis of the learning updates, and second, a numerical simulation to measure the oscillation frequency and validate the theoretical prediction.\n\n### Part 1: Theoretical Analysis\n\nThe learning dynamics are described by a set of coupled first-order difference equations for the online action-value estimates $q_1(t)$ and $q_2(t)$. The target network values, $h_1(t)$ and $h_2(t)$, are held constant for a period of $T$ steps. Let us analyze the system's evolution over one such cycle.\n\nAt the beginning of a cycle, the target network is updated with the current values of the online network. Let these values be $s_1$ and $s_2$. Thus, for the duration of the cycle (from step $t=0$ to $t=T-1$ within the cycle), we have $h_1 = s_1$ and $h_2 = s_2$. The per-step updates for the online network are given by:\n$$\nq_1(t+1) = q_1(t) + \\alpha \\left( r_1 + \\gamma h_2 - q_1(t) \\right) = (1-\\alpha) q_1(t) + \\alpha (r_1 + \\gamma s_2)\n$$\n$$\nq_2(t+1) = q_2(t) + \\alpha \\left( r_2 + \\gamma h_1 - q_2(t) \\right) = (1-\\alpha) q_2(t) + \\alpha (r_2 + \\gamma s_1)\n$$\nThe problem specifies that the initial values for the online network at the start of a cycle are the same as the target values for that cycle, i.e., $q_1(0) = s_1$ and $q_2(0) = s_2$.\n\nThese are linear first-order non-homogeneous difference equations. The general solution for an equation of the form $x(t+1) = a x(t) + b$ is $x(t) = a^t x(0) + b \\sum_{k=0}^{t-1} a^k = a^t x(0) + b \\frac{1-a^t}{1-a}$.\nApplying this to our system, with $a = (1-\\alpha)$, we get:\n$$\nq_1(t) = (1-\\alpha)^t q_1(0) + \\alpha(r_1 + \\gamma s_2) \\frac{1-(1-\\alpha)^t}{\\alpha} = (1-\\alpha)^t s_1 + (1-(1-\\alpha)^t)(r_1 + \\gamma s_2)\n$$\n$$\nq_2(t) = (1-\\alpha)^t q_2(0) + \\alpha(r_2 + \\gamma s_1) \\frac{1-(1-\\alpha)^t}{\\alpha} = (1-\\alpha)^t s_2 + (1-(1-\\alpha)^t)(r_2 + \\gamma s_1)\n$$\nAfter $T$ steps, the online network values are used to update the target network for the next cycle. Let the new target values be $s'_1$ and $s'_2$.\n$$\ns'_1 = q_1(T) = (1-\\alpha)^T s_1 + (1-(1-\\alpha)^T)(r_1 + \\gamma s_2)\n$$\n$$\ns'_2 = q_2(T) = (1-\\alpha)^T s_2 + (1-(1-\\alpha)^T)(r_2 + \\gamma s_1)\n$$\nThe problem specifies constant rewards $r_1=0$ and $r_2=0$. The map simplifies to a linear transformation. Let $\\beta = (1-\\alpha)^T$. The equations become:\n$$\ns'_1 = \\beta s_1 + (1-\\beta)\\gamma s_2\n$$\n$$\ns'_2 = \\beta s_2 + (1-\\beta)\\gamma s_1\n$$\nIn matrix form, this is $\\mathbf{s'} = A \\mathbf{s}$, where $\\mathbf{s} = \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix}$ and the cycle-to-cycle transition matrix $A$ is:\n$$\nA = \\begin{pmatrix} \\beta & (1-\\beta)\\gamma \\\\ (1-\\beta)\\gamma & \\beta \\end{pmatrix}\n$$\n\nTo determine the stability and oscillatory nature of this map, we compute the eigenvalues of $A$ by solving the characteristic equation $\\det(A - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} \\beta-\\lambda & (1-\\beta)\\gamma \\\\ (1-\\beta)\\gamma & \\beta-\\lambda \\end{pmatrix} = (\\beta-\\lambda)^2 - ((1-\\beta)\\gamma)^2 = 0\n$$\nThis yields two solutions for the eigenvalues $\\lambda$:\n$$\n\\beta - \\lambda = \\pm (1-\\beta)\\gamma \\implies \\lambda = \\beta \\mp (1-\\beta)\\gamma\n$$\nThe two eigenvalues are:\n$$\n\\lambda_1 = \\beta + (1-\\beta)\\gamma = (1-\\alpha)^T + \\gamma(1-(1-\\alpha)^T)\n$$\n$$\n\\lambda_2 = \\beta - (1-\\beta)\\gamma = (1-\\alpha)^T - \\gamma(1-(1-\\alpha)^T)\n$$\nAn oscillatory mode emerges when the system's trajectory flips its orientation in some direction in the state space from one cycle to the next. In a linear system, this corresponds to an eigenvalue being negative. Since $0 < \\alpha < 1$ and $0 \\le \\gamma < 1$, we have $0 < \\beta < 1$, which implies $(1-\\beta)\\gamma \\ge 0$. Therefore, $\\lambda_2$ is always the smaller eigenvalue. The condition for the emergence of an oscillatory mode (which we term `resonant`) is $\\lambda_2 < 0$.\n$$\n(1-\\alpha)^T - \\gamma(1-(1-\\alpha)^T) < 0\n$$\n$$\n(1-\\alpha)^T < \\gamma - \\gamma(1-\\alpha)^T\n$$\n$$\n(1+\\gamma)(1-\\alpha)^T < \\gamma\n$$\n$$\n(1-\\alpha)^T < \\frac{\\gamma}{1+\\gamma}\n$$\nIf this condition holds, the system is in a `resonant` regime. The corresponding eigenvector to $\\lambda_2$ is proportional to $(1, -1)^T$, which represents the difference mode $d(t) = q_1(t) - q_2(t)$. A negative eigenvalue means this mode flips sign every cycle (every $T$ steps). This corresponds to an oscillation with a period of $2T$ steps. The fundamental angular frequency of such an oscillation is $\\omega = 2\\pi / (2T) = \\pi/T$ radians per step. Thus, the predicted frequency is:\n$$\n\\omega_{\\text{pred}} = \\begin{cases} \\pi/T, & \\text{if } (1-\\alpha)^T < \\frac{\\gamma}{1+\\gamma} \\\\ 0, & \\text{otherwise} \\end{cases}\n$$\n\n### Part 2: Numerical Simulation and Measurement\n\nWe will now simulate the per-step dynamics for $N=4096$ steps to generate the time series for the difference mode $d(t) = q_1(t) - q_2(t)$. The simulation directly implements the given update rules for $q_i(t)$ and $h_i(t+1) = q_i(t+1)$ if $(t+1) \\bmod T = 0$, or $h_i(t+1) = h_i(t)$ otherwise.\n\nAfter simulating and storing the history of $q_1(t)$ and $q_2(t)$, we construct the time series $d(t)$ for $t = 0, \\dots, N-1$. We then compute the Fast Fourier Transform (FFT) of this series to find its frequency spectrum.\nThe measured angular frequency, $\\omega_{\\text{meas}}$, is determined by finding the frequency component with the largest magnitude in the spectrum, excluding the zero-frequency (DC) component. The frequencies provided by the FFT algorithm, $f$, are in cycles per step. They are converted to angular frequency in radians per step using the relation $\\omega = 2\\pi f$.\n\nFor each given value of the target update period $T$, we will compute the triplet $[\\omega_{\\text{pred}}, \\omega_{\\text{meas}}, \\text{resonant}]$ and report the results. The fixed parameters are $\\gamma = 0.9$, $\\alpha = 0.2$, $r_1=0$, $r_2=0$, and initial conditions $q_1(0)=1, q_2(0)=-1, h_1(0)=1, h_2(0)=-1$.\nThe critical threshold for oscillation is $(1-0.2)^T < \\frac{0.9}{1+0.9}$, which simplifies to $0.8^T < 0.9/1.9 \\approx 0.47368$.\n\nFor $T=1, 3$: $0.8^1 = 0.8 \\not< 0.47368$, $0.8^3 = 0.512 \\not< 0.47368$. The system is not `resonant`.\nFor $T=4, 20, 50$: $0.8^4 = 0.4096 < 0.47368$, $0.8^{20} \\approx 0.0115 < 0.47368$, $0.8^{50} \\approx 1.4 \\times 10^{-5} < 0.47368$. The system is `resonant`.\nThese theoretical predictions will be verified by the simulation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and simulates the dynamics of a simplified DQN to find\n    and verify oscillatory behavior based on the target network update period.\n    \"\"\"\n    \n    # Define the fixed parameters from the problem statement.\n    GAMMA = 0.9\n    ALPHA = 0.2\n    R1 = 0.0\n    R2 = 0.0\n    Q1_0 = 1.0\n    Q2_0 = -1.0\n    N_STEPS = 4096\n    \n    # Define the test cases (sweep over target update period T).\n    test_cases = [1, 3, 4, 20, 50]\n\n    results = []\n\n    for T in test_cases:\n        # Part 1: Theoretical Prediction\n        \n        # The condition for oscillatory resonance is (1-alpha)^T < gamma / (1+gamma)\n        resonant_threshold = GAMMA / (1 + GAMMA)\n        is_resonant = (1 - ALPHA)**T < resonant_threshold\n        \n        if is_resonant:\n            omega_pred = np.pi / T\n        else:\n            omega_pred = 0.0\n\n        # Part 2: Simulation\n        \n        # Initialize arrays to store the time series of q and h values.\n        # q_hist stores q(0), q(1), ..., q(N)\n        q_hist = np.zeros((N_STEPS + 1, 2))\n        h_hist = np.zeros((N_STEPS + 1, 2))\n        \n        # Set initial conditions at t=0\n        q_hist[0] = [Q1_0, Q2_0]\n        h_hist[0] = [Q1_0, Q2_0]\n        \n        for t in range(N_STEPS):\n            # Current online and target values\n            q1_t, q2_t = q_hist[t]\n            h1_t, h2_t = h_hist[t]\n            \n            # Calculate next online values q(t+1)\n            q1_tp1 = q1_t + ALPHA * (R1 + GAMMA * h2_t - q1_t)\n            q2_tp1 = q2_t + ALPHA * (R2 + GAMMA * h1_t - q2_t)\n            q_hist[t+1] = [q1_tp1, q2_tp1]\n            \n            # Update target network for next step h(t+1)\n            if (t + 1) % T == 0:\n                h_hist[t+1] = [q1_tp1, q2_tp1]\n            else:\n                h_hist[t+1] = h_hist[t]\n\n        # Part 3: Measurement via FFT\n        \n        # Create the difference mode time series d(t) = q1(t) - q2(t) for t=0...N-1\n        d_series = q_hist[:N_STEPS, 0] - q_hist[:N_STEPS, 1]\n        \n        # Compute the FFT magnitude spectrum\n        fft_magnitudes = np.abs(np.fft.fft(d_series))\n        \n        # Get the corresponding frequencies in cycles per step\n        fft_frequencies = np.fft.fftfreq(N_STEPS)\n        \n        # Find the peak frequency, excluding the DC component (at index 0)\n        # We only need to check the positive frequencies (first half of the array)\n        positive_freq_range = slice(1, N_STEPS // 2)\n        \n        if np.all(fft_magnitudes[positive_freq_range] == 0):\n             peak_idx = 0 # No signal\n        else:\n            # Add 1 to peak_idx because we searched in a slice starting at index 1\n            peak_idx = np.argmax(fft_magnitudes[positive_freq_range]) + 1\n        \n        # Peak frequency in cycles per step\n        peak_freq_cycles = fft_frequencies[peak_idx]\n        \n        # Convert to angular frequency in radians per step\n        omega_meas = 2 * np.pi * peak_freq_cycles\n\n        # Store the results for this test case\n        results.append([\n            round(omega_pred, 6), \n            round(omega_meas, 6), \n            is_resonant\n        ])\n\n    # Final print statement in the exact required format.\n    # The format requires a boolean literal 'True' or 'False'\n    result_str = ','.join([\n        f\"[{res[0]},{res[1]},{str(res[2])}]\" for res in results\n    ])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After mastering how an agent learns, we must confront the crucial question of *what* it should learn. This exercise  explores the critical alignment problem of \"reward hacking,\" where an agent cleverly exploits a poorly specified reward function to achieve high scores on a proxy task while failing at the intended goal. You will not only diagnose this common failure mode but also implement potential-based reward shaping, a theoretically-grounded technique to align an agent's incentives with the designer's true objectives.",
            "id": "3113621",
            "problem": "Consider a Markov Decision Process (MDP) with a finite state space and deterministic dynamics. Let the state set be $\\mathcal{S} = \\{s_0, s_{\\mathrm{loop}}, s_{\\mathrm{goal}}\\}$, the action set be $\\mathcal{A} = \\{a_{\\mathrm{goal}}, a_{\\mathrm{loop}}\\}$, and the start-state distribution be $\\mu$ with $\\mu(s_0) = 1$, $\\mu(s_{\\mathrm{loop}}) = 0$, and $\\mu(s_{\\mathrm{goal}}) = 0$. The transition dynamics are deterministic and defined as follows: taking $a_{\\mathrm{goal}}$ in $s_0$ leads to $s_{\\mathrm{goal}}$, taking $a_{\\mathrm{loop}}$ in $s_0$ leads to $s_{\\mathrm{loop}}$, taking $a_{\\mathrm{loop}}$ in $s_{\\mathrm{loop}}$ keeps the agent in $s_{\\mathrm{loop}}$, and taking $a_{\\mathrm{goal}}$ in $s_{\\mathrm{goal}}$ keeps the agent in $s_{\\mathrm{goal}}$.\n\nDefine two reward functions on state-action-next-state triples, the intended reward $r$ and a proxy reward $r'$, both deterministic:\n- $r(s_0, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) = 10$, $r(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$, $r(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$, and $r(s_{\\mathrm{goal}}, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) = 0$.\n- $r'(s_0, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) = 0$, $r'(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$, $r'(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 1$, and $r'(s_{\\mathrm{goal}}, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) = 0$.\n\nConsider stationary deterministic policies $\\pi$ mapping states to actions. Let $\\pi_{\\mathrm{loop}}$ be the policy that chooses $a_{\\mathrm{loop}}$ in $s_0$ and $a_{\\mathrm{loop}}$ in $s_{\\mathrm{loop}}$, and chooses $a_{\\mathrm{goal}}$ in $s_{\\mathrm{goal}}$. Let $\\pi_{\\mathrm{goal}}$ be the policy that chooses $a_{\\mathrm{goal}}$ in $s_0$, $a_{\\mathrm{loop}}$ in $s_{\\mathrm{loop}}$, and $a_{\\mathrm{goal}}$ in $s_{\\mathrm{goal}}$.\n\nFor a discount factor $\\gamma \\in [0,1)$, the value function $V^{\\pi}$ under reward function $r$ and policy $\\pi$ is defined by the Bellman fixed-point equation $V^{\\pi}(s) = \\mathbb{E}[r(s, \\pi(s), s')] + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, \\pi(s)) V^{\\pi}(s')$, where $P(s' \\mid s, a)$ is the transition probability. The expected performance starting from $\\mu$ is $J(\\pi; r) = \\sum_{s \\in \\mathcal{S}} \\mu(s) V^{\\pi}(s)$, and analogously $J(\\pi; r')$ under $r'$.\n\nYou are to demonstrate reward hacking and a mitigation through potential-based shaping. Define a shaped proxy reward $r''$ by $r''(s,a,s') = r'(s,a,s') + \\gamma \\Phi(s') - \\Phi(s)$, where $\\Phi : \\mathcal{S} \\to \\mathbb{R}$ is a potential function. Choose $\\Phi$ so that the shaped proxy removes the exploitation incentive in the loop transitions by enforcing $r''(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$ and $r''(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$. The divergence between the intended and proxy objectives is measured as $\\left| J(\\pi; r) - J(\\pi; r') \\right|$; similarly for $r''$ in place of $r'$.\n\nStarting only from the fundamental definitions of an MDP, the Bellman fixed-point equation, and the performance $J(\\pi; r)$, write a program that:\n- Constructs the transition matrix induced by a given policy $\\pi$, the per-state expected immediate reward vector under $r$ (and under $r'$, and under $r''$), solves for $V^{\\pi}$ using the linear system form of the Bellman equation, and computes $J(\\pi; r)$ and $J(\\pi; r')$ (and $J(\\pi; r'')$) for the specified start distribution $\\mu$.\n- Determines a potential function $\\Phi$ consistent with the stated shaping constraints for a given $\\gamma$, and uses it to build $r''$.\n- Computes the divergence $\\left| J(\\pi; r) - J(\\pi; r') \\right|$ (and $\\left| J(\\pi; r) - J(\\pi; r'') \\right|$) for each test case below.\n\nTest suite:\n- Test case $1$ (happy path exploitation): $\\gamma = 0.9$, policy $\\pi_{\\mathrm{loop}}$, divergence $\\left| J(\\pi_{\\mathrm{loop}}; r) - J(\\pi_{\\mathrm{loop}}; r') \\right|$.\n- Test case $2$ (goal-directed policy divergence): $\\gamma = 0.9$, policy $\\pi_{\\mathrm{goal}}$, divergence $\\left| J(\\pi_{\\mathrm{goal}}; r) - J(\\pi_{\\mathrm{goal}}; r') \\right|$.\n- Test case $3$ (shaping mitigation): $\\gamma = 0.9$, policy $\\pi_{\\mathrm{loop}}$, divergence $\\left| J(\\pi_{\\mathrm{loop}}; r) - J(\\pi_{\\mathrm{loop}}; r'') \\right|$ with $r''$ constructed by potential-based shaping that satisfies the stated constraints.\n- Test case $4$ (boundary condition): $\\gamma = 0.0$, policy $\\pi_{\\mathrm{loop}}$, divergence $\\left| J(\\pi_{\\mathrm{loop}}; r) - J(\\pi_{\\mathrm{loop}}; r') \\right|$.\n- Test case $5$ (edge case with high discount): $\\gamma = 0.99$, policy $\\pi_{\\mathrm{loop}}$, divergence $\\left| J(\\pi_{\\mathrm{loop}}; r) - J(\\pi_{\\mathrm{loop}}; r') \\right|$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[ \\text{result}_1, \\text{result}_2, \\text{result}_3, \\text{result}_4, \\text{result}_5 \\right]$), where each $\\text{result}_i$ is a floating-point number.",
            "solution": "The problem requires an analysis of a specified Markov Decision Process (MDP) to demonstrate the phenomenon of reward hacking and its mitigation through potential-based reward shaping. This involves calculating the expected performance of two distinct policies under three different reward functions: an intended reward $r$, a proxy reward $r'$, and a shaped proxy reward $r''$.\n\nThe foundation of this analysis is the Bellman equation for a stationary policy $\\pi$. For a state $s \\in \\mathcal{S}$, the value function $V^{\\pi}(s)$ is the fixed point of the Bellman operator:\n$$V^{\\pi}(s) = R^{\\pi}(s) + \\gamma \\sum_{s' \\in \\mathcal{S}} P^{\\pi}(s, s') V^{\\pi}(s')$$\nwhere $R^{\\pi}(s) = \\mathbb{E}[r(s, \\pi(s), s')]$ is the expected immediate reward from state $s$ under policy $\\pi$, and $P^{\\pi}(s, s')$ is the probability of transitioning from state $s$ to state $s'$ under policy $\\pi$. Given the deterministic dynamics in this problem, $P^{\\pi}(s, s')$ is $1$ for the unique successor state $s'$ and $0$ otherwise.\n\nThis system of linear equations can be expressed in matrix-vector form. Let $V^{\\pi}$ be the column vector of state values, $R^{\\pi}$ be the column vector of expected immediate rewards, and $P^{\\pi}$ be the state-transition matrix induced by the policy $\\pi$. The Bellman equation becomes:\n$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi} V^{\\pi}$$\nFor a discount factor $\\gamma \\in [0, 1)$, the matrix $(I - \\gamma P^{\\pi})$ is invertible, allowing for a direct solution for the value function:\n$$V^{\\pi} = (I - \\gamma P^{\\pi})^{-1} R^{\\pi}$$\nThe overall performance of a policy $\\pi$ under a reward function $r$ is the expected value of the initial state, given the start-state distribution $\\mu$:\n$$J(\\pi; r) = \\mathbb{E}_{s_0 \\sim \\mu}[V^{\\pi}(s_0)] = \\mu^T V^{\\pi}$$\nIn this problem, the distribution $\\mu$ is concentrated on $s_0$, with $\\mu(s_0) = 1$. Thus, $J(\\pi; r) = V^{\\pi}(s_0)$.\n\nWe will use a numerical representation for the states: $s_0 \\to 0$, $s_{\\mathrm{loop}} \\to 1$, and $s_{\\mathrm{goal}} \\to 2$. The start-state distribution vector is $\\mu = [1, 0, 0]^T$.\n\nFirst, we construct the transition matrices and reward vectors for the given policies $\\pi_{\\mathrm{loop}}$ and $\\pi_{\\mathrm{goal}}$.\nFor $\\pi_{\\mathrm{loop}} = \\{ s_0 \\mapsto a_{\\mathrm{loop}}, s_{\\mathrm{loop}} \\mapsto a_{\\mathrm{loop}}, s_{\\mathrm{goal}} \\mapsto a_{\\mathrm{goal}} \\}$:\nThe transitions are $s_0 \\to s_{\\mathrm{loop}}$, $s_{\\mathrm{loop}} \\to s_{\\mathrm{loop}}$, and $s_{\\mathrm{goal}} \\to s_{\\mathrm{goal}}$. The corresponding transition matrix and reward vectors are:\n$$P^{\\pi_{\\mathrm{loop}}} = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}, \\quad R^{\\pi_{\\mathrm{loop}}}(r) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad R^{\\pi_{\\mathrm{loop}}}(r') = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\nFor $\\pi_{\\mathrm{goal}} = \\{ s_0 \\mapsto a_{\\mathrm{goal}}, s_{\\mathrm{loop}} \\mapsto a_{\\mathrm{loop}}, s_{\\mathrm{goal}} \\mapsto a_{\\mathrm{goal}} \\}$:\nThe transitions are $s_0 \\to s_{\\mathrm{goal}}$, $s_{\\mathrm{loop}} \\to s_{\\mathrm{loop}}$, and $s_{\\mathrm{goal}} \\to s_{\\mathrm{goal}}$.\n$$P^{\\pi_{\\mathrm{goal}}} = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}, \\quad R^{\\pi_{\\mathrm{goal}}}(r) = \\begin{pmatrix} 10 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad R^{\\pi_{\\mathrm{goal}}}(r') = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\n\nThe third reward function, $r''$, is a potential-based shaping of $r'$: $r''(s,a,s') = r'(s,a,s') + \\gamma \\Phi(s') - \\Phi(s)$. The potential function $\\Phi: \\mathcal{S} \\to \\mathbb{R}$ must satisfy two constraints: $r''(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$ and $r''(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$.\nThe first constraint gives:\n$r''(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = r'(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) + \\gamma \\Phi(s_{\\mathrm{loop}}) - \\Phi(s_{\\mathrm{loop}}) = 0$\n$1 + (\\gamma - 1)\\Phi(s_{\\mathrm{loop}}) = 0 \\implies \\Phi(s_{\\mathrm{loop}}) = \\frac{1}{1-\\gamma}$\nThe second constraint gives:\n$r''(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = r'(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) + \\gamma \\Phi(s_{\\mathrm{loop}}) - \\Phi(s_0) = 0$\n$0 + \\gamma \\left(\\frac{1}{1-\\gamma}\\right) - \\Phi(s_0) = 0 \\implies \\Phi(s_0) = \\frac{\\gamma}{1-\\gamma}$\nThe potential for the goal state, $\\Phi(s_{\\mathrm{goal}})$, is unconstrained. For simplicity, we set $\\Phi(s_{\\mathrm{goal}}) = 0$.\nThe reward vector for the shaped reward $r''$ under policy $\\pi_{\\mathrm{loop}}$, $R^{\\pi_{\\mathrm{loop}}}(r'')$, is then constructed.\n- For $s_0$: $R^{\\pi_{\\mathrm{loop}}}(s_0, r'') = r'(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) + \\gamma \\Phi(s_{\\mathrm{loop}}) - \\Phi(s_0) = 0 + \\gamma(\\frac{1}{1-\\gamma}) - \\frac{\\gamma}{1-\\gamma} = 0$.\n- For $s_{\\mathrm{loop}}$: $R^{\\pi_{\\mathrm{loop}}}(s_{\\mathrm{loop}}, r'') = r'(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) + \\gamma \\Phi(s_{\\mathrm{loop}}) - \\Phi(s_{\\mathrm{loop}}) = 1 + (\\gamma-1)(\\frac{1}{1-\\gamma}) = 0$.\n- For $s_{\\mathrm{goal}}$: $R^{\\pi_{\\mathrm{loop}}}(s_{\\mathrm{goal}}, r'') = r'(s_{\\mathrm{goal}}, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) + \\gamma \\Phi(s_{\\mathrm{goal}}) - \\Phi(s_{\\mathrm{goal}}) = 0 + (\\gamma-1)(0) = 0$.\nThus, $R^{\\pi_{\\mathrm{loop}}}(r'') = [0, 0, 0]^T$. Notably, this is identical to $R^{\\pi_{\\mathrm{loop}}}(r)$.\n\nThe program will implement this procedure for each test case. It computes the required transition matrices and reward vectors, solves for the value functions $V^{\\pi}(r)$, $V^{\\pi}(r')$, and (where applicable) $V^{\\pi}(r'')$, and then calculates the performance $J(\\pi; \\cdot)$ as the value of the start state $s_0$. The final step is to compute the absolute difference between the performances under the intended and proxy/shaped rewards.\n\nFor example, consider Test Case 1 ($\\gamma = 0.9$, $\\pi_{\\mathrm{loop}}$).\n$V^{\\pi_{\\mathrm{loop}}}(r) = (I - 0.9 P^{\\pi_{\\mathrm{loop}}})^{-1} R^{\\pi_{\\mathrm{loop}}}(r) = (I - 0.9 P^{\\pi_{\\mathrm{loop}}})^{-1} [0, 0, 0]^T = [0, 0, 0]^T$.\nThus, $J(\\pi_{\\mathrm{loop}}; r) = V^{\\pi_{\\mathrm{loop}}}(s_0, r) = 0$.\nFor the proxy reward $r'$:\n$V^{\\pi_{\\mathrm{loop}}}(r') = \\begin{pmatrix} 1 & -0.9 & 0 \\\\ 0 & 0.1 & 0 \\\\ 0 & 0 & 0.1 \\end{pmatrix}^{-1} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 9 & 0 \\\\ 0 & 10 & 0 \\\\ 0 & 0 & 10 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ 10 \\\\ 0 \\end{pmatrix}$.\nThus, $J(\\pi_{\\mathrm{loop}}; r') = V^{\\pi_{\\mathrm{loop}}}(s_0, r') = 9$.\nThe divergence is $|0 - 9| = 9$. The policy $\\pi_{\\mathrm{loop}}$ achieves $0$ true reward but is incentivized by the proxy reward to enter the loop, yielding a high score of $9$ under $r'$, demonstrating reward hacking.\n\nThe remaining test cases are solved by applying the same computational procedure with the respective parameters.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the MDP problem for all test cases as specified.\n    \"\"\"\n    # State and action mappings to indices\n    S_MAP = {'s0': 0, 's_loop': 1, 's_goal': 2}\n    A_MAP = {'a_goal': 0, 'a_loop': 1}\n    NUM_STATES = 3\n\n    # Define deterministic transition dynamics T(s, a) -> s'\n    transitions = {\n        (S_MAP['s0'], A_MAP['a_goal']): S_MAP['s_goal'],\n        (S_MAP['s0'], A_MAP['a_loop']): S_MAP['s_loop'],\n        (S_MAP['s_loop'], A_MAP['a_loop']): S_MAP['s_loop'],\n        (S_MAP['s_goal'], A_MAP['a_goal']): S_MAP['s_goal'],\n    }\n\n    # Define deterministic reward functions r(s, a, s')\n    rewards_r = {\n        (S_MAP['s0'], A_MAP['a_goal'], S_MAP['s_goal']): 10.0,\n    }\n    rewards_rp = {\n        (S_MAP['s_loop'], A_MAP['a_loop'], S_MAP['s_loop']): 1.0,\n    }\n\n    # Define policies pi(s) -> a\n    pi_loop = {\n        S_MAP['s0']: A_MAP['a_loop'],\n        S_MAP['s_loop']: A_MAP['a_loop'],\n        S_MAP['s_goal']: A_MAP['a_goal'],\n    }\n    pi_goal = {\n        S_MAP['s0']: A_MAP['a_goal'],\n        S_MAP['s_loop']: A_MAP['a_loop'],\n        S_MAP['s_goal']: A_MAP['a_goal'],\n    }\n    policies = {'loop': pi_loop, 'goal': pi_goal}\n\n    # Start state distribution mu(s)\n    mu = np.zeros(NUM_STATES)\n    mu[S_MAP['s0']] = 1.0\n\n    def compute_performance(gamma, pi, R_pi):\n        \"\"\"\n        Computes the performance J(pi) = mu^T * V^pi.\n        V^pi is solved via the matrix inversion form of the Bellman equation.\n        \"\"\"\n        P_pi = np.zeros((NUM_STATES, NUM_STATES))\n        for s in range(NUM_STATES):\n            a = pi[s]\n            s_prime = transitions[(s, a)]\n            P_pi[s, s_prime] = 1.0\n\n        I = np.identity(NUM_STATES)\n        \n        # Handle gamma=0 case separately to avoid linalg with zero matrix\n        if gamma == 0.0:\n            V_pi = R_pi\n        else:\n            # V_pi = (I - gamma * P_pi)^-1 * R_pi\n            try:\n                inv_matrix = np.linalg.inv(I - gamma * P_pi)\n                V_pi = inv_matrix @ R_pi\n            except np.linalg.LinAlgError:\n                # Should not happen for gamma in [0, 1)\n                return float('nan')\n        \n        # J(pi) = sum_s mu(s) * V^pi(s)\n        J = mu @ V_pi\n        return J\n\n    def calculate_divergence(gamma, policy_name, divergence_type):\n        \"\"\"\n        Calculates the divergence for a given test case configuration.\n        \"\"\"\n        pi = policies[policy_name]\n\n        # Construct per-state reward vectors R_pi for r and r'\n        R_pi_r = np.zeros(NUM_STATES)\n        R_pi_rp = np.zeros(NUM_STATES)\n        for s in range(NUM_STATES):\n            a = pi[s]\n            s_prime = transitions[(s, a)]\n            R_pi_r[s] = rewards_r.get((s, a, s_prime), 0.0)\n            R_pi_rp[s] = rewards_rp.get((s, a, s_prime), 0.0)\n\n        # Compute performance under the intended reward 'r'\n        J_r = compute_performance(gamma, pi, R_pi_r)\n\n        if divergence_type == 'proxy':\n            # Compute performance under the proxy reward 'r_prime'\n            J_rp = compute_performance(gamma, pi, R_pi_rp)\n            return abs(J_r - J_rp)\n        \n        elif divergence_type == 'shaped':\n            # This case is only for pi_loop as per problem statement\n            # Determine potential function Phi from constraints\n            # Phi(s_loop) = 1 / (1-gamma)\n            # Phi(s0) = gamma * Phi(s_loop)\n            # Phi(s_goal) = 0 (unconstrained, set to 0 for simplicity)\n            if gamma < 1.0:\n                phi_s_loop = 1.0 / (1.0 - gamma)\n                phi_s0 = gamma * phi_s_loop\n            else: # Should not be reached with gamma < 1\n                phi_s_loop = float('inf')\n                phi_s0 = float('inf')\n\n            Phi = np.array([phi_s0, phi_s_loop, 0.0])\n\n            # Construct shaped reward vector R_pi_rpp\n            R_pi_rpp = np.zeros(NUM_STATES)\n            for s in range(NUM_STATES):\n                a = pi[s]\n                s_prime = transitions[(s, a)]\n                r_prime_val = rewards_rp.get((s, a, s_prime), 0.0)\n                R_pi_rpp[s] = r_prime_val + gamma * Phi[s_prime] - Phi[s]\n            \n            # Compute performance under the shaped reward 'r_double_prime'\n            J_rpp = compute_performance(gamma, pi, R_pi_rpp)\n            return abs(J_r - J_rpp)\n            \n        return float('nan')\n\n    test_cases = [\n        # (gamma, policy_name, divergence_type)\n        (0.9, 'loop', 'proxy'),\n        (0.9, 'goal', 'proxy'),\n        (0.9, 'loop', 'shaped'),\n        (0.0, 'loop', 'proxy'),\n        (0.99, 'loop', 'proxy'),\n    ]\n\n    results = []\n    for gamma, policy_name, divergence_type in test_cases:\n        result = calculate_divergence(gamma, policy_name, divergence_type)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.1f}' for r in results)}]\")\n\nsolve()\n\n```"
        }
    ]
}