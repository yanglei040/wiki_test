{
    "hands_on_practices": [
        {
            "introduction": "在许多现实世界的应用中（例如游戏或资源分配），并非所有动作在每个状态下都是可行的。本练习将指导你如何为一个包含“动作掩码”以处理非法动作的 Softmax 策略正确地计算得分函数梯度。这是一项将策略梯度方法应用于实际问题的基本技能。",
            "id": "3158020",
            "problem": "考虑强化学习（RL）中的一个离散动作策略，该策略使用动作掩码来禁止给定状态下的非法动作。设动作集为 $\\{1,2,3,4\\}$，掩码为 $m \\in \\{0,1\\}^{4}$，其中 $m_{i} = 1$ 表示动作 $i$ 是合法的，$m_{i} = 0$ 表示动作 $i$ 是非法的。该策略由一个 logits 向量 $\\theta \\in \\mathbb{R}^{4}$ 参数化，在状态 $s$ 和掩码 $m$ 下，分配给动作 $a$ 的概率定义为\n$$\n\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} ,\n$$\n该定义强制非法动作的概率为零，并且只对合法动作进行归一化。假设所选动作 $a$ 是合法的，因此 $\\pi(a \\mid s, m, \\theta) > 0$。\n\n从得分函数的定义和基本的微分法则出发，推导出在任意掩码 $m$ 下，对于一个合法的已选动作 $a$ 的梯度 $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$，并注意证明为何对应于非法动作的分量会消失。然后，在以下条件下计算该梯度：\n$$\n\\theta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad m = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad a = 3 .\n$$\n你的最终答案必须是以单个行矩阵形式表示的、精确的解析形式的梯度向量。不要进行近似计算；无需四舍五入。不涉及任何物理单位。",
            "solution": "用户提供了一个问题陈述，在尝试解答之前需要对其进行验证。\n\n### 步骤 1：提取已知条件\n- 动作集：$\\{1, 2, 3, 4\\}$\n- 掩码：$m \\in \\{0, 1\\}^4$，其中 $m_i = 1$ 表示合法动作，$m_i=0$ 表示非法动作。\n- 策略参数 (logits)：$\\theta \\in \\mathbb{R}^4$。\n- 策略定义：$\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}$。\n- 条件：所选动作 $a$ 是合法的，这意味着 $m_a = 1$。\n- 任务 1：推导梯度 $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$。\n- 任务 2：在特定情况下计算梯度：\n  - $\\theta = \\begin{pmatrix} 0  1  2  -1 \\end{pmatrix}^T$\n  - $m = \\begin{pmatrix} 1  0  1  1 \\end{pmatrix}^T$\n  - $a = 3$\n\n### 步骤 2：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n- **科学依据：** 该问题牢固地植根于强化学习理论，特别是关于策略梯度方法。带掩码的 softmax 策略的表述及其得分函数（$\\nabla_{\\theta} \\ln \\pi$）的计算是该领域的标准且正确的技术。\n- **适定性：** 问题定义清晰。它为策略提供了特定的函数形式、评估所需的所有参数以及一个明确的目标。所选动作 $a$ 是合法的这一条件确保了策略概率 $\\pi(a | \\dots)$ 大于 0，因此其对数是良定义的。存在一个唯一、稳定且有意义的解。\n- **客观性：** 问题使用精确的数学语言陈述，没有任何主观性或模糊性。\n\n该问题没有表现出科学不严谨、不完整、矛盾或模糊等任何缺陷。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。将提供解答。\n\n### 解题推导\n\n主要目标是推导对数策略的梯度，也称为得分函数。策略由以下公式给出：\n$$\n\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}\n$$\n为了求梯度 $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$，我们首先计算策略概率的对数。\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\ln \\left( \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} \\right)\n$$\n使用对数性质 $\\ln(x/y) = \\ln(x) - \\ln(y)$，我们得到：\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\ln(\\exp(\\theta_{a}) \\, m_{a}) - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\n问题陈述中指出，所选动作 $a$ 是合法的，这意味着 $m_a = 1$。这简化了第一项：$\\ln(\\exp(\\theta_{a}) \\cdot 1) = \\theta_a$。\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\theta_a - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\n现在，我们计算这个表达式关于参数向量 $\\theta$ 的梯度。梯度是一个向量，其第 $j$ 个分量是关于 $\\theta_j$ 的偏导数。\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_j = \\frac{\\partial}{\\partial \\theta_j} \\left( \\theta_a - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) \\right)\n$$\n我们逐项微分：\n$$\n\\frac{\\partial}{\\partial \\theta_j} (\\theta_a) = \\delta_{aj}\n$$\n其中 $\\delta_{aj}$ 是克罗内克 delta，如果 $j=a$ 则为 1，否则为 0。\n\n对于第二项，我们使用链式法则进行微分（$\\frac{d}{dx}\\ln(f(x)) = \\frac{f'(x)}{f(x)}$）：\n$$\n\\frac{\\partial}{\\partial \\theta_j} \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) = \\frac{1}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} \\cdot \\frac{\\partial}{\\partial \\theta_j}\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\n该和关于 $\\theta_j$ 的导数是：\n$$\n\\frac{\\partial}{\\partial \\theta_j}\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) = \\exp(\\theta_j) \\, m_j\n$$\n结合这些，第二项的导数是：\n$$\n\\frac{\\exp(\\theta_j) \\, m_j}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}\n$$\n这个表达式正是动作 $j$ 的策略概率 $\\pi(j \\mid s, m, \\theta)$ 的定义。\n\n所以，梯度的第 $j$ 个分量是：\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_j = \\delta_{aj} - \\pi(j \\mid s, m, \\theta)\n$$\n这是带掩码的 softmax 策略的得分函数的一般形式。\n\n问题要求证明为何对应于非法动作的分量会消失。设 $k$ 是一个非法动作的索引，意味着 $m_k=0$。梯度的第 $k$ 个分量是：\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_k = \\delta_{ak} - \\pi(k \\mid s, m, \\theta)\n$$\n因为 $a$ 是一个合法动作（$m_a=1$）而 $k$ 是一个非法动作（$m_k=0$），我们必有 $a \\neq k$。因此，$\\delta_{ak} = 0$。\n非法动作 $k$ 的概率是：\n$$\n\\pi(k \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{k}) \\, m_{k}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} = \\frac{\\exp(\\theta_{k}) \\cdot 0}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} = 0\n$$\n因此，梯度的第 $k$ 个分量变为：\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_k = 0 - 0 = 0\n$$\n这证实了所有非法动作的梯度分量都为零。\n\n### 梯度计算\n\n我们已知以下值：\n$$\n\\theta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad m = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad a = 3\n$$\n首先，我们计算归一化项 $Z = \\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}$：\n$$\nZ = \\exp(\\theta_1)m_1 + \\exp(\\theta_2)m_2 + \\exp(\\theta_3)m_3 + \\exp(\\theta_4)m_4\n$$\n$$\nZ = \\exp(0) \\cdot 1 + \\exp(1) \\cdot 0 + \\exp(2) \\cdot 1 + \\exp(-1) \\cdot 1\n$$\n$$\nZ = 1 \\cdot 1 + 0 + e^2 \\cdot 1 + e^{-1} \\cdot 1 = 1 + e^2 + e^{-1}\n$$\n接下来，我们计算概率向量 $\\vec{\\pi}$，其分量为 $\\pi(j \\mid s, m, \\theta)$：\n$$\n\\pi(1) = \\frac{\\exp(0) \\cdot 1}{Z} = \\frac{1}{Z}\n$$\n$$\n\\pi(2) = \\frac{\\exp(1) \\cdot 0}{Z} = 0\n$$\n$$\n\\pi(3) = \\frac{\\exp(2) \\cdot 1}{Z} = \\frac{e^2}{Z}\n$$\n$$\n\\pi(4) = \\frac{\\exp(-1) \\cdot 1}{Z} = \\frac{e^{-1}}{Z}\n$$\n梯度向量由 $\\nabla_{\\theta} \\ln \\pi(a=3 \\mid \\dots) = \\mathbf{e}_3 - \\vec{\\pi}$ 给出，其中 $\\mathbf{e}_3$ 是独热向量 $(0, 0, 1, 0)^T$。\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} \\pi(1) \\\\ \\pi(2) \\\\ \\pi(3) \\\\ \\pi(4) \\end{pmatrix} = \\begin{pmatrix} -\\pi(1) \\\\ -\\pi(2) \\\\ 1 - \\pi(3) \\\\ -\\pi(4) \\end{pmatrix}\n$$\n代入概率：\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ 1 - e^2/Z \\\\ -e^{-1}/Z \\end{pmatrix} = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ (Z - e^2)/Z \\\\ -e^{-1}/Z \\end{pmatrix}\n$$\n代入 $Z = 1 + e^2 + e^{-1}$：\n$$\nZ - e^2 = (1 + e^2 + e^{-1}) - e^2 = 1 + e^{-1}\n$$\n所以梯度向量是：\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ (1+e^{-1})/Z \\\\ -e^{-1}/Z \\end{pmatrix} = \\frac{1}{Z} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1+e^{-1} \\\\ -e^{-1} \\end{pmatrix}\n$$\n为了以更优美的形式表达，我们可以代入 $Z = 1 + e^2 + e^{-1}$，并通过将分子和分母同乘以 $e$ 来消除负指数。\n$$\n\\frac{1}{Z} = \\frac{1}{1 + e^2 + e^{-1}} = \\frac{e}{e(1 + e^2 + e^{-1})} = \\frac{e}{e + e^3 + 1}\n$$\n现在将这个标量乘入向量：\n$$\n\\nabla_{\\theta} \\ln \\pi = \\frac{e}{1+e+e^3} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1+e^{-1} \\\\ -e^{-1} \\end{pmatrix} = \\frac{1}{1+e+e^3} \\begin{pmatrix} -e \\\\ 0 \\\\ e(1+e^{-1}) \\\\ e(-e^{-1}) \\end{pmatrix} = \\frac{1}{1+e+e^3} \\begin{pmatrix} -e \\\\ 0 \\\\ e+1 \\\\ -1 \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{e}{e^3+e+1}  0  \\frac{e+1}{e^3+e+1}  -\\frac{1}{e^3+e+1} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "策略梯度方法的一个核心挑战是梯度估计值的高方差，这会拖慢学习速度并影响稳定性。本练习通过一个简洁的例子，量化地展示了对偶采样（Antithetic Sampling）这一方差缩减技术如何显著提升梯度估计的质量。通过这个练习，你将直观地理解为何方差缩减在策略梯度算法中至关重要。",
            "id": "3158001",
            "problem": "考虑一个一维随机老虎机，其动作 $a \\in \\mathbb{R}$ 从对称高斯策略 $\\pi_{\\theta}(a) = \\mathcal{N}(\\theta, \\sigma^{2})$ 中抽取，其中方差 $\\sigma^{2}  0$ 是固定的，只有均值 $\\theta \\in \\mathbb{R}$ 是可训练的参数。奖励与动作呈线性关系，即 $r(a) = k a$，其中 $k \\in \\mathbb{R}$ 是一个非零常数。目标是 $J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a)]$。使用似然比（得分函数）恒等式 $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a)]$ 作为基本出发点。\n\n为 $\\nabla_{\\theta} J(\\theta)$ 定义以下无偏梯度估计量：\n- 一个双样本独立同分布平均估计量：抽取 $a_{1}, a_{2} \\stackrel{\\text{i.i.d.}}{\\sim} \\pi_{\\theta}$ 并设置 $\\hat{g}_{\\text{iid}} = \\frac{1}{2}\\big[r(a_{1}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{1}) + r(a_{2}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{2})\\big]$。\n- 一个对偶配对平均估计量：抽取 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$，设置 $a_{+} = \\theta + \\varepsilon$ 和 $a_{-} = \\theta - \\varepsilon$，并定义 $\\hat{g}_{\\text{anti}} = \\frac{1}{2}\\big[r(a_{+}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{+}) + r(a_{-}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{-})\\big]$。\n\n从具有固定方差的单变量高斯分布的 $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a)$ 的定义以及高斯随机变量的基本矩恒等式出发，推导 $\\operatorname{Var}[\\hat{g}_{\\text{iid}}]$ 和 $\\operatorname{Var}[\\hat{g}_{\\text{anti}}]$ 的闭式表达式。然后，在相同的奖励评估预算下，通过提供方差缩减因子\n$$V(\\theta, \\sigma) := \\frac{\\operatorname{Var}[\\hat{g}_{\\text{iid}}]}{\\operatorname{Var}[\\hat{g}_{\\text{anti}}]}$$\n的闭式解析表达式，来量化对偶采样所实现的方差缩减。\n\n将您的最终结果表示为关于 $\\theta$ 和 $\\sigma$ 的单个简化表达式。不需要进行数值取整。",
            "solution": "用户希望验证并解决一个关于策略梯度方法中方差缩减的问题。\n\n### 第 1 步：提取已知条件\n- **策略**：$\\pi_{\\theta}(a) = \\mathcal{N}(\\theta, \\sigma^{2})$，一个对称高斯策略，其可训练均值为 $\\theta \\in \\mathbb{R}$，固定方差为 $\\sigma^{2}  0$。\n- **动作**：$a \\in \\mathbb{R}$。\n- **奖励函数**：$r(a) = k a$，其中 $k \\in \\mathbb{R}$ 是一个非零常数。\n- **目标函数**：$J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a)]$。\n- **梯度恒等式**：$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a)]$。\n- **独立同分布梯度估计量**：$\\hat{g}_{\\text{iid}} = \\frac{1}{2}\\big[r(a_{1}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{1}) + r(a_{2}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{2})\\big]$，其中 $a_{1}, a_{2} \\stackrel{\\text{i.i.d.}}{\\sim} \\pi_{\\theta}$。\n- **对偶梯度估计量**：$\\hat{g}_{\\text{anti}} = \\frac{1}{2}\\big[r(a_{+}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{+}) + r(a_{-}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{-})\\big]$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$，$a_{+} = \\theta + \\varepsilon$，且 $a_{-} = \\theta - \\varepsilon$。\n- **目标量**：方差缩减因子 $V(\\theta, \\sigma) := \\frac{\\operatorname{Var}[\\hat{g}_{\\text{iid}}]}{\\operatorname{Var}[\\hat{g}_{\\text{anti}}]}$。\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题具有科学依据，定义明确且客观。\n- **科学依据**：该问题涉及强化学习和计算统计学中的标准概念：策略梯度方法、得分函数（或 REINFORCE）估计量，以及作为方差缩减技术的对偶采样。所有定义和原则都是标准且正确的。\n- **定义明确性**：该问题提供了所有必要的定义，并要求推导特定的、明确定义的量（$\\operatorname{Var}[\\hat{g}_{\\text{iid}}]$、$\\operatorname{Var}[\\hat{g}_{\\text{anti}}]$ 及其比率）。其设定是自洽且数学上一致的，从而导出一个唯一的解。\n- **客观性**：该问题使用形式化的数学语言陈述，没有任何主观或模糊的术语。\n\n### 第 3 步：结论与行动\n该问题是有效的。我将进行完整的推导。\n\n### 求解推导\n\n首先，我们确定一些初步的量。策略 $\\pi_{\\theta}(a) = \\mathcal{N}(\\theta, \\sigma^2)$ 的概率密度函数是\n$$p(a; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(a-\\theta)^2}{2\\sigma^2}\\right).$$\n对数策略是 $\\ln \\pi_{\\theta}(a) = -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(a-\\theta)^2}{2\\sigma^2}$。得分函数，即对数策略相对于参数 $\\theta$ 的梯度，是\n$$ \\nabla_{\\theta} \\ln \\pi_{\\theta}(a) = \\frac{\\partial}{\\partial\\theta}\\left(-\\frac{(a-\\theta)^2}{2\\sigma^2}\\right) = -\\frac{2(a-\\theta)(-1)}{2\\sigma^2} = \\frac{a-\\theta}{\\sigma^2}. $$\n我们定义单样本梯度估计为 $\\hat{g}(a) = r(a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a)$。使用给定的奖励和推导出的得分函数，我们有\n$$ \\hat{g}(a) = (ka) \\left(\\frac{a-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2} a(a-\\theta). $$\n真实梯度 $\\nabla_{\\theta} J(\\theta)$ 是该估计量的期望值。设 $a \\sim \\mathcal{N}(\\theta, \\sigma^2)$。我们可以写作 $a = \\theta + z$，其中 $z \\sim \\mathcal{N}(0, \\sigma^2)$。\n$$ \\nabla_{\\theta} J(\\theta) = \\mathbb{E}[\\hat{g}(a)] = \\mathbb{E}\\left[\\frac{k}{\\sigma^2} a(a-\\theta)\\right] = \\frac{k}{\\sigma^2}\\mathbb{E}[(\\theta+z)(z)] = \\frac{k}{\\sigma^2}\\mathbb{E}[\\theta z + z^2]. $$\n利用期望的线性性质，并注意到对于 $z \\sim \\mathcal{N}(0, \\sigma^2)$，我们有 $\\mathbb{E}[z]=0$ 和 $\\mathbb{E}[z^2]=\\sigma^2$，我们得到\n$$ \\nabla_{\\theta} J(\\theta) = \\frac{k}{\\sigma^2}(\\theta \\mathbb{E}[z] + \\mathbb{E}[z^2]) = \\frac{k}{\\sigma^2}(0 + \\sigma^2) = k. $$\n这证实了 $\\hat{g}(a)$ 是真实梯度 $k$ 的一个无偏估计量。\n\n现在，我们推导这两个估计量的方差。\n\n**1. 独立同分布估计量的方差, $\\operatorname{Var}[\\hat{g}_{\\text{iid}}]$**\n\n估计量 $\\hat{g}_{\\text{iid}}$ 是两个独立同分布随机变量 $\\hat{g}(a_1)$ 和 $\\hat{g}(a_2)$ 的平均值。由于独立性，方差为\n$$ \\operatorname{Var}[\\hat{g}_{\\text{iid}}] = \\operatorname{Var}\\left[\\frac{1}{2}(\\hat{g}(a_1) + \\hat{g}(a_2))\\right] = \\frac{1}{4}(\\operatorname{Var}[\\hat{g}(a_1)] + \\operatorname{Var}[\\hat{g}(a_2)]) = \\frac{1}{2}\\operatorname{Var}[\\hat{g}(a)]. $$\n我们需要计算 $\\operatorname{Var}[\\hat{g}(a)] = \\mathbb{E}[\\hat{g}(a)^2] - (\\mathbb{E}[\\hat{g}(a)])^2$。我们已知 $\\mathbb{E}[\\hat{g}(a)] = k$。\n我们来计算二阶矩 $\\mathbb{E}[\\hat{g}(a)^2]$：\n$$ \\mathbb{E}[\\hat{g}(a)^2] = \\mathbb{E}\\left[\\left(\\frac{k}{\\sigma^2} a(a-\\theta)\\right)^2\\right] = \\frac{k^2}{\\sigma^4} \\mathbb{E}[a^2(a-\\theta)^2]. $$\n代入 $a = \\theta + z$，其中 $z \\sim \\mathcal{N}(0, \\sigma^2)$：\n$$ \\mathbb{E}[a^2(a-\\theta)^2] = \\mathbb{E}[(\\theta+z)^2 z^2] = \\mathbb{E}[(\\theta^2+2\\theta z+z^2)z^2] = \\mathbb{E}[\\theta^2 z^2 + 2\\theta z^3 + z^4]. $$\n使用中心化高斯分布的矩，$\\mathbb{E}[z^2]=\\sigma^2$，$\\mathbb{E}[z^3]=0$（通过对称性），以及 $\\mathbb{E}[z^4]=3\\sigma^4$：\n$$ \\mathbb{E}[a^2(a-\\theta)^2] = \\theta^2\\mathbb{E}[z^2] + 2\\theta\\mathbb{E}[z^3] + \\mathbb{E}[z^4] = \\theta^2\\sigma^2 + 0 + 3\\sigma^4 = \\sigma^2(\\theta^2 + 3\\sigma^2). $$\n所以，$\\hat{g}(a)$ 的二阶矩是\n$$ \\mathbb{E}[\\hat{g}(a)^2] = \\frac{k^2}{\\sigma^4}\\sigma^2(\\theta^2 + 3\\sigma^2) = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 3\\right). $$\n那么，单样本估计量的方差是\n$$ \\operatorname{Var}[\\hat{g}(a)] = \\mathbb{E}[\\hat{g}(a)^2] - (\\mathbb{E}[\\hat{g}(a)])^2 = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 3\\right) - k^2 = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right). $$\n最后，双样本独立同分布估计量的方差是\n$$ \\operatorname{Var}[\\hat{g}_{\\text{iid}}] = \\frac{1}{2}\\operatorname{Var}[\\hat{g}(a)] = \\frac{k^2}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right). $$\n\n**2. 对偶估计量的方差, $\\operatorname{Var}[\\hat{g}_{\\text{anti}}]$**\n\n对偶估计量使用一对相关的样本。我们首先用随机变量 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 来表示该估计量。\n该估计量的两个分量是 $\\hat{g}(a_{+})$ 和 $\\hat{g}(a_{-})$。\n对于 $a_{+} = \\theta + \\varepsilon$：\n$$ \\hat{g}(a_{+}) = r(a_{+}) \\nabla_{\\theta}\\ln\\pi_{\\theta}(a_{+}) = k(\\theta+\\varepsilon) \\left(\\frac{(\\theta+\\varepsilon)-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2}(\\theta+\\varepsilon)\\varepsilon = \\frac{k}{\\sigma^2}(\\theta\\varepsilon + \\varepsilon^2). $$\n对于 $a_{-} = \\theta - \\varepsilon$：\n$$ \\hat{g}(a_{-}) = r(a_{-}) \\nabla_{\\theta}\\ln\\pi_{\\theta}(a_{-}) = k(\\theta-\\varepsilon) \\left(\\frac{(\\theta-\\varepsilon)-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2}(\\theta-\\varepsilon)(-\\varepsilon) = \\frac{k}{\\sigma^2}(-\\theta\\varepsilon + \\varepsilon^2). $$\n对偶估计量是这两个分量的平均值：\n$$ \\hat{g}_{\\text{anti}} = \\frac{1}{2}(\\hat{g}(a_{+}) + \\hat{g}(a_{-})) = \\frac{1}{2} \\left[ \\frac{k}{\\sigma^2}(\\theta\\varepsilon + \\varepsilon^2) + \\frac{k}{\\sigma^2}(-\\theta\\varepsilon + \\varepsilon^2) \\right] = \\frac{1}{2} \\frac{k}{\\sigma^2} (2\\varepsilon^2) = \\frac{k}{\\sigma^2}\\varepsilon^2. $$\n依赖于 $\\theta$ 的项已经被消去。现在我们可以计算这个简化表达式的方差。\n$$ \\operatorname{Var}[\\hat{g}_{\\text{anti}}] = \\operatorname{Var}\\left[\\frac{k}{\\sigma^2}\\varepsilon^2\\right] = \\left(\\frac{k}{\\sigma^2}\\right)^2 \\operatorname{Var}[\\varepsilon^2] = \\frac{k^2}{\\sigma^4}\\operatorname{Var}[\\varepsilon^2]. $$\n$\\varepsilon^2$ 的方差是 $\\operatorname{Var}[\\varepsilon^2] = \\mathbb{E}[(\\varepsilon^2)^2] - (\\mathbb{E}[\\varepsilon^2])^2 = \\mathbb{E}[\\varepsilon^4] - (\\mathbb{E}[\\varepsilon^2])^2$。\n使用 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 的矩，$\\mathbb{E}[\\varepsilon^2] = \\sigma^2$ 和 $\\mathbb{E}[\\varepsilon^4] = 3\\sigma^4$，我们有\n$$ \\operatorname{Var}[\\varepsilon^2] = 3\\sigma^4 - (\\sigma^2)^2 = 2\\sigma^4. $$\n将其代回，我们得到对偶估计量的方差：\n$$ \\operatorname{Var}[\\hat{g}_{\\text{anti}}] = \\frac{k^2}{\\sigma^4}(2\\sigma^4) = 2k^2. $$\n\n**3. 方差缩减因子, $V(\\theta, \\sigma)$**\n\n最后，我们计算这两个方差的比率。\n$$ V(\\theta, \\sigma) = \\frac{\\operatorname{Var}[\\hat{g}_{\\text{iid}}]}{\\operatorname{Var}[\\hat{g}_{\\text{anti}}]} = \\frac{\\frac{k^2}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right)}{2k^2}. $$\n由于 $k \\neq 0$，我们可以消去 $k^2$ 项：\n$$ V(\\theta, \\sigma) = \\frac{\\frac{1}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right)}{2} = \\frac{1}{4}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right) = \\frac{\\theta^2}{4\\sigma^2} + \\frac{2}{4} = \\frac{\\theta^2}{4\\sigma^2} + \\frac{1}{2}. $$\n为了将其写成单个简化表达式，我们可以使用一个公分母：\n$$ V(\\theta, \\sigma) = \\frac{\\theta^2 + 2\\sigma^2}{4\\sigma^2}. $$\n这就是方差缩减因子的闭式解析表达式。",
            "answer": "$$\\boxed{\\frac{\\theta^2 + 2\\sigma^2}{4\\sigma^2}}$$"
        },
        {
            "introduction": "在掌握了基本原理之后，我们需要批判性地思考实践中常用的技巧。许多先进的策略梯度算法都会对奖励或优势函数进行归一化处理，但这是否会改变优化的目标？本练习将引导你深入分析奖励归一化的理论边界，理解它何时能够保持最优策略不变，以及何时可能引入偏见，从而培养对算法设计更深刻的洞察力。",
            "id": "3163431",
            "problem": "考虑一个折扣因子为 $\\gamma \\in (0,1)$ 的折扣马尔可夫决策过程 (MDP)，以及一个带有参数 $\\theta$ 的可微随机策略 $\\pi_{\\theta}(a \\mid s)$。学习目标是从初始状态分布开始的期望折扣回报。一位实践者实现了一种蒙特卡洛策略梯度方法，该方法在分数函数估计器中使用回合回报 $G_t$作为动作价值的替代，但在应用更新之前，他们通过减去均值并除以标准差来对标量目标进行归一化。具体来说，在每次迭代中，他们构建 $\\tilde{G}_t = (G_t - m)/s$，其中 $m$ 和 $s$ 是根据当前策略收集的数据计算得出的。\n\n从基本原理出发，分析这种归一化在何种情况下会或不会改变原始期望折扣回报目标的最优策略集，以及在何种情况下，即使原始目标的全局最大化点在理论上得以保留，它也会在实践中改变优化动态。你应该根据期望回报的定义、分数函数恒等式以及 MDP 中回报仿射变换的基本性质来进行推理。\n\n选择所有正确的陈述。\n\nA. 在每次迭代数据无限的极限情况下，如果 $m$ 和 $s0$ 是在批次上计算出的常数，且在给定状态 $s_t$ 的情况下不依赖于采样的动作 $a_t$（例如，迭代中所有时间步和轨迹上的批次均值和标准差），那么用 $\\tilde{G}_t$ 替换 $G_t$ 不会改变折扣目标的最优策略集。\n\nB. 如果归一化中使用的缩放因子 $s$ 依赖于采样的动作或状态-动作对（例如，$s = s(s_t,a_t)$），那么得到的更新对应于一个不同的、按动作加权的目标，即使在数据无限的情况下也可能导致一个不同的最优策略。\n\nC. 在持续性任务的平均回报准则下（即最大化每个时间步的长期平均回报），给每个单步回报增加一个常数 $c$ 会改变最优策略集。\n\nD. 在像 Trust Region Policy Optimization (TRPO) 和 Proximal Policy Optimization (PPO) 这样带有更新约束或近似的算法中，将优势函数除以一个正的批次标准差不会影响最终收敛的策略，因为只是重新调整了整体步长。\n\nE. 按轨迹进行归一化，使用每条轨迹自身的均值和标准差（根据同一轨迹中所有回报，包括时间点 $t$ 的回报计算得出），能够保持策略梯度估计器的无偏性，因此不会改变最优策略。\n\nF. 在回合长度是随机的且依赖于智能体动作的无折扣回合制任务中，给每个单步回报增加相同的常数可能会改变哪个策略是最优的，即使该常数在每一步都是动作无关的。\n\n选择正确的选项。",
            "solution": "分析首先将策略梯度方法以及所提出的归一化效果形式化。\n\n在策略梯度方法中，目标是最大化策略 $\\pi_{\\theta}(a \\mid s)$ 的期望折扣回报 $J(\\theta)$：\n$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^{T-1} \\gamma^t r_t \\right]$$\n其中 $\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\dots)$ 是通过执行策略 $\\pi_\\theta$ 采样得到的轨迹。\n\n策略梯度定理给出了该目标的梯度表达式：\n$$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\left( \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) \\right) \\left( \\sum_{t=0}^{T-1} \\gamma^t r_t \\right) \\right]$$\n在像 REINFORCE 这样的蒙特卡洛方法中常用的一种形式是：\n$$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) G_t \\right]$$\n其中 $G_t = \\sum_{k=t}^{T-1} \\gamma^{k-t} r_k$ 是从时间步 $t$ 开始的折扣回报。\n\n策略梯度的一个关键性质是，可以从 $G_t$ 中减去一个基线 $b(s_t)$，而不会给期望梯度引入偏差，只要该基线不依赖于动作 $a_t$ 或轨迹的任何后续部分。这是因为对于任何在给定 $s_t$ 的条件下与 $a_t$ 条件独立的函数 $b(s_t)$：\n$$\\mathbb{E}_{a_t \\sim \\pi_{\\theta}(\\cdot|s_t)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) b(s_t)] = b(s_t) \\mathbb{E}_{a_t \\sim \\pi_{\\theta}(\\cdot|s_t)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)] = b(s_t) \\nabla_{\\theta} \\sum_{a_t} \\pi_{\\theta}(a_t|s_t) = b(s_t) \\nabla_{\\theta}(1) = \\mathbf{0}$$\n因此，$\\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\sum_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) (G_t - b(s_t))] = \\nabla_{\\theta} J(\\theta)$。\n\n问题考虑将目标 $G_t$ 歸一化为 $\\tilde{G}_t = (G_t - m)/s$。使用这个归一化目标的期望梯度是：\n$$\\mathbb{E} \\left[ \\sum_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) \\frac{G_t - m}{s} \\right] = \\mathbb{E} \\left[ \\sum_t \\frac{1}{s} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) (G_t - m) \\right]$$\n如果修改后的梯度为零当且仅当原始梯度为零，那么最优策略集保持不变。如果修改后的梯度是原始梯度的正标量缩放版本，即对于某个标量函数 $c(\\theta)  0$ 有 $\\nabla_{\\theta} J_{norm}(\\theta) = c(\\theta) \\nabla_{\\theta} J(\\theta)$，这通常是成立的。\n\n让我们基于这些原则来评估每个陈述。\n\n**A. 在每次迭代数据無限的极限情况下，如果 $m$ 和 $s0$ 是在批次上计算出的常数，且在给定状态 $s_t$ 的情况下不依赖于采样的动作 $a_t$（例如，迭代中所有时间步和轨迹上的批次均值和标准差），那么用 $\\tilde{G}_t$ 替换 $G_t$ 不会改变折扣目标的最优策略集。**\n\n在无限数据的极限下，批次均值 $m$ 和标准差 $s$ 会收敛到它们在当前策略 $\\pi_\\theta$ 下的真实期望。因此，$m = m(\\theta)$ 和 $s = s(\\theta)$ 是策略参数 $\\theta$ 的函数，但在一个梯度计算步骤中对于固定的 $\\theta$ 而言，它们是常数。条件陈述了它们在给定 $s_t$ 的情况下不依赖于具体的动作 $a_t$。对于批次范围内的统计量来说，这是成立的。\n\n期望的更新方向变为：\n$$ \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) \\frac{G_t - m(\\theta)}{s(\\theta)} \\right] $$\n我们可以分解期望内的项：\n$$ \\frac{1}{s(\\theta)} \\left( \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) G_t \\right] - \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) m(\\theta) \\right] \\right) $$\n第一项是 $\\frac{1}{s(\\theta)}\\nabla_{\\theta} J(\\theta)$。第二项涉及减去 $m(\\theta)$，它充当一个基线。由于 $m(\\theta)$ 相对于 $(s_t, a_t)$ 的采样是一个常数，它满足无偏基线的条件。正如引言部分所示，它的期望为零。\n$$ \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) m(\\theta) \\right] = m(\\theta) \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) \\right] = \\mathbf{0} $$\n因此，期望梯度为 $\\frac{1}{s(\\theta)} \\nabla_{\\theta} J(\\theta)$。由于 $s(\\theta)  0$，这个新的梯度方向只是原始梯度的一个正向缩放。如果 $\\nabla_{\\theta} J(\\theta^*) = \\mathbf{0}$，则策略 $\\pi_{\\theta^*}$ 是最优的（或一个驻点）。这个条件等价于 $\\frac{1}{s(\\theta^*)} \\nabla_{\\theta} J(\\theta^*) = \\mathbf{0}$。最优策略集保持不变。\n**结论：正确。**\n\n**B. 如果归一化中使用的缩放因子 $s$ 依赖于采样的动作或状态-动作对（例如，$s = s(s_t,a_t)$），那么得到的更新对应于一个不同的、按动作加权的目标，即使在数据无限的情况下也可能导致一个不同的最优策略。**\n\n如果 $s$ 依赖于 $a_t$，即 $s = s(s_t, a_t)$，梯度估计器被修改的方式就不再是一个简单的缩放了。期望变为：\n$$ \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) \\frac{G_t - m}{s(s_t, a_t)} \\right] $$\n这等价于优化一个目标，其中与某些动作相关的回报相对于其他动作被降权了。为清晰起见，我们使用优势函数形式：一个最优策略 $\\pi^*$ 满足对于所有 $s$，$\\mathbb{E}_{a \\sim \\pi^*(\\cdot|s)}[\\nabla_\\theta \\log \\pi^*(a|s) Q^{\\pi^*}(s,a)] = \\mathbf{0}$。在这个策略下，新的梯度将是 $\\mathbb{E}_{a \\sim \\pi^*(\\cdot|s)}[\\nabla_\\theta \\log \\pi^*(a|s) \\frac{Q^{\\pi^*}(s,a)}{s(s,a)}]$。没有理由认为它会是零。动作相关的缩放因子 $1/s(s,a)$ 对优势项进行了重新加权，改变了梯度为零的条件。这实际上定义了一个具有不同目标函数的新优化问题，该问题通常会有不同的最优策略集。\n**结论：正确。**\n\n**C. 在持续性任务的平均回报准则下（即最大化每个时间步的长期平均回报），给每个单步回报增加一个常数 $c$ 会改变最优策略集。**\n\n平均回报目标是 $J(\\pi) = \\lim_{H \\to \\infty} \\mathbb{E} \\left[ \\frac{1}{H} \\sum_{t=0}^{H-1} r_t \\right] = \\mathbb{E}_{s \\sim d_\\pi, a \\sim \\pi(\\cdot|s)} [r(s,a)]$，其中 $d_\\pi$ 是由策略 $\\pi$ 诱导的平稳状态分布。\n如果我们变换回报使得 $r'(s,a) = r(s,a) + c$，新的目标 $J'(\\pi)$ 变为：\n$$ J'(\\pi) = \\mathbb{E}_{s \\sim d_\\pi, a \\sim \\pi(\\cdot|s)} [r(s,a) + c] = \\mathbb{E}_{s \\sim d_\\pi, a \\sim \\pi(\\cdot|s)} [r(s,a)] + \\mathbb{E}_{s \\sim d_\\pi, a \\sim \\pi(\\cdot|s)} [c] = J(\\pi) + c $$\n由于我们为所有策略的目标函数都增加了一个常数 $c$，最大化 $J(\\pi)$ 的策略 $\\pi^*$ 也会最大化 $J'(\\pi) = J(\\pi) + c$。策略按其值的排序得以保留，因此最优策略集保持不变。这个性质是回报塑造理论的基石。\n**结论：错误。**\n\n**D. 在像 Trust Region Policy Optimization (TRPO) 和 Proximal Policy Optimization (PPO) 这样带有更新约束或近似的算法中，将优势函数除以一个正的批次标准差不会影响最终收敛的策略，因为只是重新调整了整体步长。**\n\n在这些算法中，我们通常优化一个涉及优势函数 $A_t$ 的替代目标。归一化优势函数意味着使用 $\\tilde{A}_t = A_t / s_A$，其中 $s_A$ 是批次中优势函数的标准差。对于 PPO 的裁剪目标 $L^{CLIP}(\\theta) = \\mathbb{E}_t [\\min(r_t A_t, \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t)]$，使用 $\\tilde{A}_t$ 会得到一个新的目标 $L'_{CLIP} = L^{CLIP} / s_A$。由于 $s_A0$，最大化 $L'_{CLIP}$ 等价于最大化 $L^{CLIP}$。然而，如果使用固定学习率 $\\alpha$ 的基于梯度的优化器，更新步骤为 $\\Delta\\theta \\propto \\alpha \\nabla_{\\theta} L'_{CLIP} = (\\alpha/s_A) \\nabla_{\\theta} L^{CLIP}$。有效学习率变为 $\\alpha/s_A$。由于 $s_A$ 是在每次迭代时根据批次数据计算的，它会从一次迭代到下一次迭代发生变化，从而创建了一个自适应学习率方案。不同的学习率方案会导致不同的优化路径，并可能收敛到不同的局部最优解。声称它“不会影响最终收敛的策略”的说法过于绝对，通常是错误的。优化动态肯定被改变了，这会影响最终结果。\n**结论：错误。**\n\n**E. 按轨迹进行归一化，使用每条轨迹自身的均值和标准差（根据同一轨迹中所有回报，包括时间点 $t$ 的回报计算得出），能够保持策略梯度估计器的无偏性，因此不会改变最优策略。**\n\n在按轨迹归一化中，对于一条轨迹 $\\tau_i$，统计量 $m_i$ 和 $s_i$ 是使用来自同一轨迹的所有回报 $\\{G_{i,0}, G_{i,1}, \\dots, G_{i,T-1}\\}$ 计算的。考虑时间点 $t$ 的梯度贡献：$\\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i,t} \\mid s_{i,t}) (G_{i,t} - m_i)/s_i$。由于 $m_i$ 是这些回报的函数（例如，它们的平均值），而这些回报本身又依赖于整条轨迹中的动作，因此 $m_i$ 成为了 $a_{i,t}$ 及未来事件的函数。一个基线只有在给定状态 $s_t$ 的情况下与动作 $a_t$ 条件独立时才有效。在这里，“基线”$m_i$ 违反了这一因果原则，因为它依赖于动作 $a_{i,t}$ 及其后果。因此, $\\mathbb{E}_{a_{i,t}}[\\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i,t} \\mid s_{i,t}) (m_i/s_i)] \\neq \\mathbf{0}$。这会给梯度估计器引入偏差。一个有偏的梯度估计器会优化一个不同的目标，并且通常会收敛到一个不同的策略。这种归一化保持无偏性的前提是错误的。\n**结论：错误。**\n\n**F. 在回合长度是随机的且依赖于智能体动作的无折扣回合制任务中，给每个单步回报增加相同的常数可能会改变哪个策略是最优的，即使该常数在每一步都是动作无关的。**\n\n在无折扣的回合制任务中，目标是最大化总回报 $J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} \\left[\\sum_{t=0}^{T-1} r_t \\right]$，其中回合长度 $T$ 是一个随机变量，其分布依赖于策略 $\\pi$。\n如果我们定义一个新的回报 $r'_t = r_t + c$，新的目标 $J'(\\pi)$ 是：\n$$ J'(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} \\left[\\sum_{t=0}^{T-1} (r_t + c) \\right] = \\mathbb{E}_{\\tau \\sim \\pi} \\left[\\sum_{t=0}^{T-1} r_t \\right] + \\mathbb{E}_{\\tau \\sim \\pi} \\left[\\sum_{t=0}^{T-1} c \\right] $$\n$$ J'(\\pi) = J(\\pi) + c \\cdot \\mathbb{E}_{\\tau \\sim \\pi}[T] $$\n最大化 $J(\\pi)$ 的策略 $\\pi$ 不一定能最大化 $J'(\\pi)$，因为第二项 $c \\cdot \\mathbb{E}_{\\tau \\sim \\pi}[T]$ 依赖于策略。如果 $c  0$，新的目标偏好导致更长回合的策略。如果 $c  0$，它会惩罚更长的回合。例如，一个策略 $\\pi_1$ 可能比另一个策略 $\\pi_2$ 有更高的原始回报 $J(\\pi_1)$，但期望生命周期 $\\mathbb{E}[T|\\pi_1]$ 更短。通过选择一个合适的正常数 $c$，有可能使得 $J'(\\pi_2)  J'(\\pi_1)$，从而改变了哪个策略是最优的。\n**结论：正确。**\n\n最终总结，正确的陈述是：A, B, F。",
            "answer": "$$\\boxed{ABF}$$"
        }
    ]
}