{
    "hands_on_practices": [
        {
            "introduction": "现代关键点检测器通常为每个关节点输出一个热力图，但最终目标是获得精确的 $(x, y)$ 坐标。简单的 `argmax` 操作是不可微的，因此不适用于通过反向传播进行端到端训练。本练习  介绍了“软最大值 (soft-argmax)”方法，它通过计算加权平均值来提供一种可微的坐标提取方式，让您能够探索定位精度和梯度流之间的关键权衡。",
            "id": "3139976",
            "problem": "要求您设计并分析一个用于姿态估计中关键点检测的可微非极大值抑制层，通过将热图上的硬性 argmax 选择替换为平滑的软选择。请从以下基本原理出发：称为 softmax 的指数族归一化定义、权重和为一的概率解释、离散随机变量的期望值以及微积分中的链式法则。\n\n假设给定一个规则网格上的离散热图，表示为矩阵 $H \\in \\mathbb{R}^{m \\times n}$，其元素为 $H_{y,x}$，其中行索引 $y \\in \\{0,\\dots,m-1\\}$，列索引 $x \\in \\{0,\\dots,n-1\\}$。将该网格展平为一个包含 $N = m \\cdot n$ 个位置的列表，由 $i \\in \\{0,\\dots,N-1\\}$ 索引，每个位置关联一个像素坐标 $\\mathbf{p}_i = (x_i, y_i)$ 和一个热图值 $H_i$。通过带有温度参数 $\\tau > 0$ 的 softmax 权重定义一个可微的软选择：\n$$\ns_i = \\frac{\\exp\\left(\\frac{H_i}{\\tau}\\right)}{\\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)} \\quad \\text{for } i \\in \\{0,\\dots,N-1\\}.\n$$\n将 $\\{s_i\\}$ 解释为像素位置上的一个离散概率分布。使用这些权重将可微关键点定义为期望坐标（软-argmax）：\n$$\n\\hat{\\mathbf{p}} = \\sum_{i=0}^{N-1} s_i \\, \\mathbf{p}_i.\n$$\n从第一性原理出发，通过对 softmax 定义应用链式法则，推导 $\\hat{\\mathbf{p}}$ 关于热图值 $\\{H_i\\}$ 的梯度。使用推导出的表达式，将可微性灵敏度度量定义为 $\\hat{\\mathbf{p}}$ 关于 $H$ 的雅可比矩阵的弗罗贝尼乌斯范数，即：\n$$\nG = \\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H}\\right\\|_F = \\sqrt{\\sum_{i=0}^{N-1} \\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_i}\\right\\|_2^2}.\n$$\n为评估峰值的尖锐程度，定义两个关于 $\\hat{\\mathbf{p}}$ 的分布 $\\{s_i\\}$ 的互补标量度量：\n- 围绕 $\\hat{\\mathbf{p}}$ 的空间方差：\n$$\nV = \\sum_{i=0}^{N-1} s_i \\, \\left\\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\right\\|_2^2.\n$$\n- 归一化香农熵：\n$$\nE = -\\frac{1}{\\log N} \\sum_{i=0}^{N-1} s_i \\log s_i,\n$$\n其值位于 $[0,1]$ 区间，接近 $0$ 的值表示一个尖锐的峰值，接近 $1$ 的值表示一个弥散的分布。\n\n您的程序必须实现上述层，并对给定的 $(H,\\tau)$ 对计算以下输出：软-argmax 坐标 $(\\hat{x}, \\hat{y})$、空间方差 $V$、归一化熵 $E$ 以及可微性灵敏度 $G$。所有计算均无单位。本任务不涉及角度。\n\n推导要求：\n- 从 softmax 定义出发，验证 $\\sum_i s_i = 1$，并使用期望值定义来定义 $\\hat{\\mathbf{p}}$。\n- 推导 $\\frac{\\partial s_i}{\\partial H_k}$，然后推导 $\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}$，除了链式法则和指数函数的性质外，不使用任何已知的捷径。\n- 将 $G$ 的表达式简化为可直接根据 $\\{s_i\\}$、$\\{\\mathbf{p}_i\\}$ 和 $\\hat{\\mathbf{p}}$ 计算的形式。\n\n实现要求：\n- 使用网格坐标 $\\mathbf{p}_i = (x_i, y_i)$，其中 $x_i \\in \\{0,\\dots,n-1\\}$ 且 $y_i \\in \\{0,\\dots,m-1\\}$，通过常规的行主序展平进行映射。\n- 实现数值稳定的计算；对于熵的计算，确保对数函数仅对严格为正的参数进行求值。\n\n测试套件：\n- 所有热图大小均为 $m = 5, n = 5$（即 $5 \\times 5$），以行主序展平。索引从 0 开始。\n- 情况 1（理想情况，非常尖锐的单峰）：$H$ 全部为零，除了 $H_{2,2} = 10$，且 $\\tau = 0.1$。\n- 情况 2（中等温度）：$H$ 与情况 1 相同，但 $\\tau = 1.0$。\n- 情况 3（两个对称的峰值）：$H$ 全部为零，除了 $H_{1,1} = 8$ 和 $H_{3,3} = 8$，且 $\\tau = 0.5$。\n- 情况 4（均匀热图，边界弥散情况）：$H$ 全部为一，且 $\\tau = 2.0$。\n\n对于每种情况，计算元组 $(\\hat{x}, \\hat{y}, V, E, G)$，并按以下顺序将所有结果汇总到一个扁平列表中：情况 1 的结果，接着是情况 2、情况 3 和情况 4 的结果。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[\\text{result1},\\text{result2},\\dots]$）。所有结果必须是浮点数。不需要单位。",
            "solution": "这个问题是有效的，因为它在数学上是适定的，其科学基础根植于深度学习和微积分的原理，并提供了一套完整且一致的定义和约束。我们将着手进行推导和实现。\n\n我们的目标是设计一个可微的软关键点选择层。这包括推导软-argmax 坐标相对于输入热图的梯度，并定义几个指标来分析该层的行为。\n\n**1. 基本定义**\n\n给定一个热图 $H \\in \\mathbb{R}^{m \\times n}$。我们将其展平为一个包含 $N = m \\cdot n$ 个值 $\\{H_i\\}$ 的向量，其中每个索引 $i$ 对应一个唯一的像素坐标 $\\mathbf{p}_i = (x_i, y_i)$。\n\n可微选择的核心是 softmax 函数，它将热图值转换为像素位置上的一个概率分布 $\\{s_i\\}$。使用温度参数 $\\tau > 0$，第 $i$ 个位置的 softmax 权重为：\n$$\ns_i = \\frac{\\exp\\left(\\frac{H_i}{\\tau}\\right)}{\\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)}\n$$\n这些权重构成了一个有效的概率分布，因为对于所有的 $i$ 都有 $s_i \\ge 0$ 并且它们的和为一：\n$$\n\\sum_{i=0}^{N-1} s_i = \\sum_{i=0}^{N-1} \\frac{\\exp\\left(\\frac{H_i}{\\tau}\\right)}{\\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)} = \\frac{1}{\\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)} \\sum_{i=0}^{N-1} \\exp\\left(\\frac{H_i}{\\tau}\\right) = 1\n$$\n利用这种概率解释，软-argmax 关键点坐标 $\\hat{\\mathbf{p}}$被定义为像素坐标 $\\mathbf{p}_i$ 在分布 $\\{s_i\\}$下的期望值：\n$$\n\\hat{\\mathbf{p}} = \\sum_{i=0}^{N-1} s_i \\, \\mathbf{p}_i\n$$\n其中 $\\hat{\\mathbf{p}}$ 是一个二维向量 $(\\hat{x}, \\hat{y})$。\n\n**2. 梯度的推导**\n\n为确保该层能用于基于梯度的优化框架（如训练神经网络），我们必须求出输出 $\\hat{\\mathbf{p}}$ 关于输入热图值 $\\{H_k\\}$ 的导数。我们应用链式法则。$\\hat{\\mathbf{p}}$ 关于单个热图条目 $H_k$ 的导数为：\n$$\n\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k} = \\frac{\\partial}{\\partial H_k} \\left( \\sum_{i=0}^{N-1} s_i \\mathbf{p}_i \\right) = \\sum_{i=0}^{N-1} \\left( \\frac{\\partial s_i}{\\partial H_k} \\right) \\mathbf{p}_i\n$$\n核心任务是计算 softmax 函数的导数 $\\frac{\\partial s_i}{\\partial H_k}$。令 $Z = \\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)$。那么 $s_i = Z^{-1} \\exp\\left(\\frac{H_i}{\\tau}\\right)$。使用商法则（或对 $Z^{-1}$ 使用积法则）：\n$$\n\\frac{\\partial s_i}{\\partial H_k} = \\frac{\\partial}{\\partial H_k} \\left( \\frac{\\exp(H_i/\\tau)}{Z} \\right) = \\frac{ \\left( \\frac{\\partial}{\\partial H_k} \\exp\\left(\\frac{H_i}{\\tau}\\right) \\right) Z - \\exp\\left(\\frac{H_i}{\\tau}\\right) \\left( \\frac{\\partial Z}{\\partial H_k} \\right) }{Z^2}\n$$\n各分量的导数是：\n$$\n\\frac{\\partial}{\\partial H_k} \\exp\\left(\\frac{H_i}{\\tau}\\right) = \\exp\\left(\\frac{H_i}{\\tau}\\right) \\cdot \\frac{\\partial}{\\partial H_k}\\left(\\frac{H_i}{\\tau}\\right) = \\exp\\left(\\frac{H_i}{\\tau}\\right) \\frac{1}{\\tau} \\delta_{ik}\n$$\n其中 $\\delta_{ik}$ 是克罗内克 δ。\n$$\n\\frac{\\partial Z}{\\partial H_k} = \\frac{\\partial}{\\partial H_k} \\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right) = \\sum_{j=0}^{N-1} \\frac{\\partial}{\\partial H_k} \\exp\\left(\\frac{H_j}{\\tau}\\right) = \\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right) \\frac{1}{\\tau} \\delta_{jk} = \\frac{1}{\\tau} \\exp\\left(\\frac{H_k}{\\tau}\\right)\n$$\n将这些代回商法则的表达式中：\n$$\n\\frac{\\partial s_i}{\\partial H_k} = \\frac{ \\left( \\frac{1}{\\tau} \\delta_{ik} \\exp\\left(\\frac{H_i}{\\tau}\\right) \\right) Z - \\exp\\left(\\frac{H_i}{\\tau}\\right) \\left( \\frac{1}{\\tau} \\exp\\left(\\frac{H_k}{\\tau}\\right) \\right) }{Z^2}\n$$\n$$\n= \\frac{1}{\\tau} \\left( \\delta_{ik} \\frac{\\exp(H_i/\\tau)}{Z} - \\frac{\\exp(H_i/\\tau)}{Z} \\frac{\\exp(H_k/\\tau)}{Z} \\right)\n$$\n通过识别 $s_i$ 和 $s_k$ 的定义，此式可简化为众所周知的 softmax 雅可比矩阵形式：\n$$\n\\frac{\\partial s_i}{\\partial H_k} = \\frac{1}{\\tau} (s_i \\delta_{ik} - s_i s_k)\n$$\n现在，我们将其代入 $\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}$ 的表达式中：\n$$\n\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k} = \\sum_{i=0}^{N-1} \\frac{1}{\\tau} (s_i \\delta_{ik} - s_i s_k) \\mathbf{p}_i = \\frac{1}{\\tau} \\left( \\sum_{i=0}^{N-1} s_i \\delta_{ik} \\mathbf{p}_i - \\sum_{i=0}^{N-1} s_i s_k \\mathbf{p}_i \\right)\n$$\n第一个求和式在 $i=k$ 时简化为单项：$s_k \\mathbf{p}_k$。对于第二个求和式，$s_k$ 是一个常数因子：$s_k \\sum_{i=0}^{N-1} s_i \\mathbf{p}_i = s_k \\hat{\\mathbf{p}}$。\n这就得出了梯度的最终简化表达式：\n$$\n\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k} = \\frac{1}{\\tau} (s_k \\mathbf{p}_k - s_k \\hat{\\mathbf{p}}) = \\frac{s_k}{\\tau} (\\mathbf{p}_k - \\hat{\\mathbf{p}})\n$$\n\n**3. 可微性灵敏度度量的推导**\n\n灵敏度度量 $G$ 是雅可比矩阵 $\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H}$ 的弗罗贝尼乌斯范数，该矩阵的列是向量 $\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}$。\n$$\nG = \\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H}\\right\\|_F = \\sqrt{\\sum_{k=0}^{N-1} \\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}\\right\\|_2^2}\n$$\n使用我们推导出的梯度：\n$$\n\\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}\\right\\|_2^2 = \\left\\| \\frac{s_k}{\\tau} (\\mathbf{p}_k - \\hat{\\mathbf{p}}) \\right\\|_2^2 = \\left(\\frac{s_k}{\\tau}\\right)^2 \\left\\| \\mathbf{p}_k - \\hat{\\mathbf{p}} \\right\\|_2^2\n$$\n对所有的 $k$ 求和：\n$$\nG^2 = \\sum_{k=0}^{N-1} \\frac{s_k^2}{\\tau^2} \\left\\| \\mathbf{p}_k - \\hat{\\mathbf{p}} \\right\\|_2^2 = \\frac{1}{\\tau^2} \\sum_{k=0}^{N-1} s_k^2 \\left\\| \\mathbf{p}_k - \\hat{\\mathbf{p}} \\right\\|_2^2\n$$\n因此，$G$ 可以计算为：\n$$\nG = \\frac{1}{\\tau} \\sqrt{\\sum_{i=0}^{N-1} s_i^2 \\left\\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\right\\|_2^2}\n$$\n该表达式可直接由 softmax 权重 $\\{s_i\\}$、像素坐标 $\\{\\mathbf{p}_i\\}$、软-argmax 坐标 $\\hat{\\mathbf{p}}$ 以及温度参数 $\\tau$ 计算得出。\n\n**4. 峰值尖锐度指标**\n\n另外两个指标已在问题陈述中定义，无需进一步推导。\n- 空间方差 $V$ 衡量的是与均值坐标 $\\hat{\\mathbf{p}}$ 的期望平方距离：\n$$\nV = \\sum_{i=0}^{N-1} s_i \\, \\left\\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\right\\|_2^2\n$$\n- 归一化香农熵 $E$ 衡量的是分布 $\\{s_i\\}$ 的不确定性或弥散度：\n$$\nE = -\\frac{1}{\\log N} \\sum_{i=0}^{N-1} s_i \\log s_i\n$$\n如果 $s_i = 0$，则项 $s_i \\log s_i$ 取为 $0$。\n\n**5. 实现计划**\n实现将遵循这些推导出的公式。\n1. 生成坐标网格 $\\mathbf{p}_i = (x_i, y_i)$。\n2. 给定输入热图 $H$ 和温度 $\\tau$，计算展平后的热图值向量 $\\{H_i\\}$。\n3. 使用 log-sum-exp 技巧计算 softmax 权重 $\\{s_i\\}$ 以确保数值稳定性，防止溢出。\n4. 计算软-argmax 坐标 $\\hat{\\mathbf{p}} = \\sum s_i \\mathbf{p}_i$。\n5. 计算空间方差 $V = \\sum s_i \\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\|_2^2$。\n6. 计算归一化熵 $E$，确保处理 $s_i=0$ 的情况。\n7. 计算灵敏度 $G = \\frac{1}{\\tau} \\sqrt{\\sum s_i^2 \\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\|_2^2}$。\n8. 为每个测试用例返回计算出的元组 $(\\hat{x}, \\hat{y}, V, E, G)$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_metrics(H: np.ndarray, tau: float) -> tuple[float, float, float, float, float]:\n    \"\"\"\n    Computes soft-argmax coordinates and related metrics for a given heatmap.\n\n    Args:\n        H (np.ndarray): A 2D numpy array representing the heatmap.\n        tau (float): The temperature parameter for the softmax function.\n\n    Returns:\n        tuple[float, float, float, float, float]: A tuple containing\n        (x_hat, y_hat, V, E, G).\n    \"\"\"\n    m, n = H.shape\n    N = float(m * n)\n\n    # 1. Generate grid coordinates p_i = (x_i, y_i)\n    # np.meshgrid creates coordinate matrices from coordinate vectors.\n    # The 'ij' indexing gives row-major compatible grids.\n    x_coords, y_coords = np.meshgrid(np.arange(n), np.arange(m), indexing='xy')\n    \n    # Flatten coordinates into an (N, 2) array of (x, y) pairs\n    # This corresponds to row-major flattening of the heatmap.\n    p = np.stack([x_coords.ravel(), y_coords.ravel()], axis=1)\n\n    # Flatten the heatmap H\n    H_flat = H.ravel()\n\n    # 2. Compute softmax weights s_i with numerical stability (log-sum-exp trick)\n    a = H_flat / tau\n    # Subtracting the max value prevents overflow in exp\n    a_max = np.max(a)\n    exp_a = np.exp(a - a_max)\n    sum_exp_a = np.sum(exp_a)\n    s = exp_a / sum_exp_a\n\n    # 3. Compute soft-argmax p_hat = (x_hat, y_hat)\n    # p_hat is the expected coordinate E[p] under distribution s.\n    # s is (N,), p is (N, 2). s[:, np.newaxis] makes s (N, 1) for broadcasting.\n    p_hat = np.sum(s[:, np.newaxis] * p, axis=0)\n    x_hat, y_hat = p_hat[0], p_hat[1]\n\n    # 4. Compute spatial variance V\n    # diff_p is (N, 2) array of vectors (p_i - p_hat)\n    diff_p = p - p_hat\n    # sq_dist is (N,) array of squared Euclidean distances ||p_i - p_hat||^2\n    sq_dist = np.sum(diff_p**2, axis=1)\n    V = np.sum(s * sq_dist)\n\n    # 5. Compute normalized Shannon entropy E\n    # To avoid log(0), we select only s_i > 0.\n    # The limit of s*log(s) as s->0 is 0.\n    s_positive = s[s > 0]\n    # The entropy is sum(-s_i * log(s_i))\n    entropy = -np.sum(s_positive * np.log(s_positive))\n    E = entropy / np.log(N)\n\n    # 6. Compute differentiability sensitivity G\n    # The formular for G^2 is (1/tau^2) * sum(s_i^2 * ||p_i - p_hat||^2)\n    g_sum_term = np.sum(s**2 * sq_dist)\n    G = (1.0 / tau) * np.sqrt(g_sum_term)\n    \n    return (x_hat, y_hat, V, E, G)\n\ndef solve():\n    \"\"\"\n    Defines test cases, computes metrics for each, and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    m, n = 5, 5\n\n    # Case 1: Sharp single peak\n    H1 = np.zeros((m, n), dtype=float)\n    H1[2, 2] = 10.0\n    tau1 = 0.1\n\n    # Case 2: Moderate temperature single peak\n    H2 = np.zeros((m, n), dtype=float)\n    H2[2, 2] = 10.0\n    tau2 = 1.0\n\n    # Case 3: Two symmetric peaks\n    H3 = np.zeros((m, n), dtype=float)\n    H3[1, 1] = 8.0\n    H3[3, 3] = 8.0\n    tau3 = 0.5\n\n    # Case 4: Uniform heatmap\n    H4 = np.ones((m, n), dtype=float)\n    tau4 = 2.0\n\n    test_cases = [\n        (H1, tau1),\n        (H2, tau2),\n        (H3, tau3),\n        (H4, tau4),\n    ]\n\n    all_results = []\n    for H, tau in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        results = compute_metrics(H, tau)\n        all_results.extend(results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "训练鲁棒神经网络的一个关键策略是数据增强，而对于像人体这样的对称物体，水平翻转尤其有效。一个训练良好的模型应该对这类变换具有*等变性 (equivariance)*，这意味着其预测应与输入进行一致的变换。在本练习  中，您将量化并强制执行这种翻转一致性，从而实践性地理解如何利用数据的内在对称性来提升模型性能。",
            "id": "3140014",
            "problem": "考虑一个关键点检测器，它为图像上固定数量的关键点预测二维（2D）像素坐标。设图像表示为 $I$，检测器的预测为 $\\hat{K}(I) \\in \\mathbb{R}^{N \\times 2}$，其中 $N$ 是关键点的数量，每一行包含 $(x,y)$ 像素坐标。设 $F$ 表示作用于像素坐标的水平翻转变换，其定义为\n$$\nF(x,y;w) = (w-1-x, y),\n$$\n其中 $w$ 是图像的像素宽度，$x,y$ 是像素坐标。设 $S$ 是一个作用于关键点索引的置换，它交换左右对应的部分，即，$S:\\{0,\\dots,N-1\\}\\rightarrow\\{0,\\dots,N-1\\}$ 是一个对合，使得对于每一对左右对应的点 $(i,j)$，有 $S(i)=j$ 和 $S(j)=i$，而对于任何自对称的索引 $k$，有 $S(k)=k$。带有左右交换的水平翻转的等变性条件是\n$$\n\\hat{K}(F I) = F \\, S \\, \\hat{K}(I),\n$$\n其中 $F$ 作用于坐标，$S$ 作用于索引。\n\n对于同一主体图像及其水平翻转版本的预测对 $\\hat{K}(I)$ 和 $\\hat{K}(F I)$，定义其对称误差（以像素为单位）如下\n$$\nE_{\\text{sym}}(\\hat{K}(I),\\hat{K}(F I); w, S) \\triangleq \\frac{1}{N} \\sum_{i=0}^{N-1} \\left\\| \\hat{K}(F I)_i - \\left(F\\!\\left(\\hat{K}(I)_{S(i)}; w\\right)\\right) \\right\\|_2,\n$$\n其中 $\\|\\cdot\\|_2$ 是以像素为单位的欧几里得范数。考虑一个部分强制执行步骤，该步骤通过将原始图像上的预测投影到翻转-交换一致性上来仅更新该预测：\n$$\nK_{\\text{cons}}(I) \\triangleq \\frac{1}{2}\\left( \\hat{K}(I) + S \\, F^{-1}\\, \\hat{K}(F I) \\right),\n$$\n对于水平翻转，$F^{-1}=F$，同时保持 $\\hat{K}(F I)$ 不变。强制执行后的对称误差是\n$$\nE_{\\text{sym}}^{\\text{after}} \\triangleq E_{\\text{sym}}\\!\\left(K_{\\text{cons}}(I),\\hat{K}(F I); w, S\\right).\n$$\n\n您的任务是编写一个完整的、可运行的程序，为下面的每个测试用例计算：\n- 强制执行前的对称误差 $E_{\\text{sym}}^{\\text{before}} \\triangleq E_{\\text{sym}}\\!\\left(\\hat{K}(I),\\hat{K}(F I); w, S\\right)$，以及\n- 如上文定义的强制执行后的对称误差 $E_{\\text{sym}}^{\\text{after}}$，\n\n两者都以像素为单位，表示为浮点值。所有与角度相关的量都不存在；不需要角度单位。最终输出必须是单行，包含一个用方括号括起来的逗号分隔列表，其中每个测试用例的值按顺序排列为 $[E_{\\text{sym}}^{\\text{before}},E_{\\text{sym}}^{\\text{after}}]$，并跨测试用例连接。每个数字必须四舍五入到 $6$ 位小数。\n\n实现以下测试套件。每个用例都明确定义了 $(w,h,N,S,\\hat{K}(I),\\hat{K}(F I))$，其中 $w$ 是像素宽度，$h$ 是像素高度，$N$ 是关键点数量，$S$ 通过其左右配对给出，预测以像素坐标数组的形式给出。\n\n- 用例 1（正常情况，中度不一致）：\n  - $w = 100$, $h = 80$, $N=4$，左右配对：$(0,1)$ 和 $(2,3)$。\n  - $\\hat{K}(I) = \\begin{bmatrix} 20  50 \\\\ 80  50 \\\\ 25  30 \\\\ 75  30 \\end{bmatrix}$。\n  - $\\hat{K}(F I) = \\begin{bmatrix} 22  52 \\\\ 77  49 \\\\ 26  31 \\\\ 71  29 \\end{bmatrix}$。\n\n- 用例 2（边界情况，完美对称）：\n  - $w = 100$, $h = 80$, $N=4$，左右配对：$(0,1)$ 和 $(2,3)$。\n  - $\\hat{K}(I) = \\begin{bmatrix} 20  50 \\\\ 80  50 \\\\ 25  30 \\\\ 75  30 \\end{bmatrix}$。\n  - $\\hat{K}(F I)$ 精确等于 $F S \\hat{K}(I)$，即\n    $$\n    \\hat{K}(F I) = \\begin{bmatrix}\n    19  50 \\\\\n    79  50 \\\\\n    24  30 \\\\\n    74  30\n    \\end{bmatrix}。\n    $$\n\n- 用例 3（边缘情况，索引在对称轴上）：\n  - $w = 101$, $h = 120$, $N=4$，左右配对：$(0,1)$ 和 $(2,3)$。\n  - $\\hat{K}(I) = \\begin{bmatrix} 50  10 \\\\ 50  12 \\\\ 50  70 \\\\ 50  68 \\end{bmatrix}$。\n  - $\\hat{K}(F I) = \\begin{bmatrix} 48  11 \\\\ 52  10 \\\\ 49  71 \\\\ 51  67 \\end{bmatrix}$。\n\n- 用例 4（边缘情况，大的不对称性和错误的交换）：\n  - $w = 64$, $h = 64$, $N=4$，左右配对：$(0,1)$ 和 $(2,3)$。\n  - $\\hat{K}(I) = \\begin{bmatrix} 10  20 \\\\ 53  19 \\\\ 12  40 \\\\ 51  41 \\end{bmatrix}$。\n  - $\\hat{K}(F I) = \\begin{bmatrix} 56  23 \\\\ 7  19 \\\\ 51  39 \\\\ 12  44 \\end{bmatrix}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序如下\n$$\n[E_{\\text{sym}}^{\\text{before}}(\\text{用例 }1), E_{\\text{sym}}^{\\text{after}}(\\text{用例 }1), E_{\\text{sym}}^{\\text{before}}(\\text{用例 }2), E_{\\text{sym}}^{\\text{after}}(\\text{用例 }2), E_{\\text{sym}}^{\\text{before}}(\\text{用例 }3), E_{\\text{sym}}^{\\text{after}}(\\text{用例 }3), E_{\\text{sym}}^{\\text{before}}(\\text{用例 }4), E_{\\text{sym}}^{\\text{after}}(\\text{用例 }4)]。\n$$\n所有值都必须是像素单位的浮点数，并四舍五入到 $6$ 位小数。不应打印任何其他文本。",
            "solution": "该问题已经过验证，被认为是科学上合理的、定义明确且自洽的。所有定义和数据足以得出一个唯一的解。\n\n任务是为一系列测试用例计算两个量：关键点检测器在一致性强制执行步骤之前（$E_{\\text{sym}}^{\\text{before}}$）和之后（$E_{\\text{sym}}^{\\text{after}}$）的对称误差。\n\n基本量的定义如下：\n- 图像 $I$ 的一个关键点预测是一个矩阵 $\\hat{K}(I) \\in \\mathbb{R}^{N \\times 2}$。\n- 作用于坐标向量 $(x,y)$ 的水平翻转变换是 $F(x,y;w) = (w-1-x, y)$，其中 $w$ 是图像宽度。此变换是其自身的逆，即 $F^{-1}=F$。\n- 置换 $S$ 交换对应的左右关键点的索引。由于它由交换组成，$S$ 是一个对合，意味着 $S^2$ 是单位置换。\n- 强制执行前的对称误差由下式给出：\n$$\nE_{\\text{sym}}^{\\text{before}} = E_{\\text{sym}}(\\hat{K}(I),\\hat{K}(F I); w, S) \\triangleq \\frac{1}{N} \\sum_{i=0}^{N-1} \\left\\| \\hat{K}(F I)_i - \\left(F\\!\\left(\\hat{K}(I)_{S(i)}; w\\right)\\right) \\right\\|_2\n$$\n这衡量了与理想的翻转-交换等变性条件 $\\hat{K}(F I) = F \\, S \\, \\hat{K}(I)$ 的偏差，其中 $F$ 作用于坐标，$S$ 作用于关键点向量列表。项 $F(\\hat{K}(I)_{S(i)}; w)$ 可以写成矩阵 $F(S\\hat{K}(I); w)$ 的第 $i$ 行。\n\n- 一致性强制执行步骤更新了原始图像上的预测：\n$$\nK_{\\text{cons}}(I) \\triangleq \\frac{1}{2}\\left( \\hat{K}(I) + S \\, F^{-1}\\, \\hat{K}(F I) \\right)\n$$\n鉴于 $F^{-1}=F$，这简化为 $K_{\\text{cons}}(I) = \\frac{1}{2}\\left( \\hat{K}(I) + S F \\hat{K}(F I) \\right)$。\n\n- 然后使用新的预测 $K_{\\text{cons}}(I)$ 计算强制执行后的对称误差，同时保持 $\\hat{K}(F I)$ 不变：\n$$\nE_{\\text{sym}}^{\\text{after}} = E_{\\text{sym}}\\!\\left(K_{\\text{cons}}(I),\\hat{K}(F I); w, S\\right) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\left\\| \\hat{K}(F I)_i - \\left(F\\!\\left((K_{\\text{cons}}(I))_{S(i)}; w\\right)\\right) \\right\\|_2\n$$\n\n通过分析 $E_{\\text{sym}}^{\\text{after}}$ 和 $E_{\\text{sym}}^{\\text{before}}$ 之间的关系，可以获得一个关键的见解。让我们检验一下强制执行后误差表达式中范数的参数。从 $\\hat{K}(F I)$ 中减去的项是 $F S K_{\\text{cons}}(I)$。让我们展开这一项：\n$$\nF S K_{\\text{cons}}(I) = F S \\left[ \\frac{1}{2}\\left( \\hat{K}(I) + S F \\hat{K}(F I) \\right) \\right]\n$$\n置换 $S$ 是作用于 $N$ 个关键点向量集合的线性算子。变换 $F$ 是仿射的，但它在平均操作上是可分配的：$F(\\frac{1}{2}(\\vec{u}+\\vec{v})) = \\frac{1}{2}(F(\\vec{u})+F(\\vec{v}))$。应用这些性质：\n$$\nF S K_{\\text{cons}}(I) = F \\left[ \\frac{1}{2}\\left( S\\hat{K}(I) + S S F \\hat{K}(F I) \\right) \\right]\n$$\n由于 $S$ 是一个对合，$S S$ 是单位算子。\n$$\n= F \\left[ \\frac{1}{2}\\left( S\\hat{K}(I) + F \\hat{K}(F I) \\right) \\right] = \\frac{1}{2} \\left[ F(S\\hat{K}(I)) + F(F(\\hat{K}(F I))) \\right]\n$$\n由于 $F$ 是自身的逆，$F F$ 是单位算子。\n$$\n= \\frac{1}{2} \\left[ F S \\hat{K}(I) + \\hat{K}(F I) \\right]\n$$\n现在，让我们看一下强制执行后误差计算中第 $i$ 个关键点的差分向量：\n$$\n\\Delta_{i}^{\\text{after}} = \\hat{K}(F I)_i - (F S K_{\\text{cons}}(I))_i = \\hat{K}(F I)_i - \\frac{1}{2} \\left[ (F S \\hat{K}(I))_i + \\hat{K}(F I)_i \\right]\n$$\n$$\n\\Delta_{i}^{\\text{after}} = \\frac{1}{2} \\left[ \\hat{K}(F I)_i - (F S \\hat{K}(I))_i \\right]\n$$\n这个差分向量恰好是原始差分向量 $\\Delta_{i}^{\\text{before}} = \\hat{K}(F I)_i - (F S \\hat{K}(I))_i$ 的一半。\n因此，其范数也减半：$\\|\\Delta_{i}^{\\text{after}}\\|_2 = \\frac{1}{2} \\|\\Delta_{i}^{\\text{before}}\\|_2$。当我们将这些范数对所有 $N$ 个关键点进行平均时，这个关系对总误差也成立：\n$$\nE_{\\text{sym}}^{\\text{after}} = \\frac{1}{2} E_{\\text{sym}}^{\\text{before}}\n$$\n这个分析结果表明，所选的更新规则将不一致的预测投影到等变流形的一半路程上。它简化了计算，因为我们只需要为每个案例计算 $E_{\\text{sym}}^{\\text{before}}$。然而，为了严谨起见，我们实现了完整的计算过程。\n\n对于给定的测试用例 $(w, N, S, \\hat{K}(I), \\hat{K}(F I))$，计算步骤如下：\n1.  根据给定的左右配对，定义 $S$ 的置换映射。对于配对 $(0,1)$ 和 $(2,3)$，映射为 $[1,0,3,2]$。\n2.  计算理想的变换后预测，$K_{\\text{target}} = F(S \\hat{K}(I); w)$。\n3.  计算每个关键点的误差向量 $\\Delta_i = \\hat{K}(F I)_i - (K_{\\text{target}})_i$。\n4.  通过取欧几里得范数 $\\|\\Delta_i\\|_2$ 的平均值来计算 $E_{\\text{sym}}^{\\text{before}}$。\n5.  $E_{\\text{sym}}^{\\text{after}}$ 则是 $\\frac{1}{2}E_{\\text{sym}}^{\\text{before}}$。实现通过显式计算 $K_{\\text{cons}}(I)$ 及其相应的误差来证实了这一点。\n\n将此程序应用于测试用例，得出以下结果：\n\n- **用例 1**：$w=100, N=4, S \\leftrightarrow \\{(0,1), (2,3)\\}$。\n  $E_{\\text{sym}}^{\\text{before}} = \\frac{1}{4}(\\sqrt{13} + 2\\sqrt{5} + \\sqrt{10}) \\approx 2.809993$ 像素。\n  $E_{\\text{sym}}^{\\text{after}} = \\frac{1}{2} E_{\\text{sym}}^{\\text{before}} \\approx 1.404996$ 像素。\n\n- **用例 2**：$w=100, N=4, S \\leftrightarrow \\{(0,1), (2,3)\\}$。预测是完美对称的，所以 $K_{target} = \\hat{K}(F I)$。\n  $E_{\\text{sym}}^{\\text{before}} = 0.0$ 像素。\n  $E_{\\text{sym}}^{\\text{after}} = 0.0$ 像素。\n\n- **用例 3**：$w=101, N=4, S \\leftrightarrow \\{(0,1), (2,3)\\}$。\n  $E_{\\text{sym}}^{\\text{before}} = \\frac{1}{4}(2 + \\sqrt{5} + 2\\sqrt{10}) \\approx 2.640156$ 像素。\n  $E_{\\text{sym}}^{\\text{after}} = \\frac{1}{2} E_{\\text{sym}}^{\\text{before}} \\approx 1.320078$ 像素。\n\n- **用例 4**：$w=64, N=4, S \\leftrightarrow \\{(0,1), (2,3)\\}$。\n  $E_{\\text{sym}}^{\\text{before}} = \\frac{1}{4}(\\sqrt{2132} + \\sqrt{2117} + \\sqrt{1525} + \\sqrt{1537}) \\approx 42.610074$ 像素。\n  $E_{\\text{sym}}^{\\text{after}} = \\frac{1}{2} E_{\\text{sym}}^{\\text{before}} \\approx 21.305037$ 像素。\n\n这些值按照规定格式化和呈现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the symmetric error calculation problem for all test cases.\n    \"\"\"\n    # Each case defines (w, h, N, S, K_I, K_FI)\n    # S is given by left-right pairs, which is a fixed (0,1), (2,3) for all cases.\n    # h is not used in calculations.\n    \n    test_cases = [\n        {\n            \"w\": 100, \"N\": 4, \"s_pairs\": [(0, 1), (2, 3)],\n            \"K_I\": np.array([[20., 50.], [80., 50.], [25., 30.], [75., 30.]]),\n            \"K_FI\": np.array([[22., 52.], [77., 49.], [26., 31.], [71., 29.]])\n        },\n        {\n            \"w\": 100, \"N\": 4, \"s_pairs\": [(0, 1), (2, 3)],\n            \"K_I\": np.array([[20., 50.], [80., 50.], [25., 30.], [75., 30.]]),\n            \"K_FI\": np.array([[19., 50.], [79., 50.], [24., 30.], [74., 30.]])\n        },\n        {\n            \"w\": 101, \"N\": 4, \"s_pairs\": [(0, 1), (2, 3)],\n            \"K_I\": np.array([[50., 10.], [50., 12.], [50., 70.], [50., 68.]]),\n            \"K_FI\": np.array([[48., 11.], [52., 10.], [49., 71.], [51., 67.]])\n        },\n        {\n            \"w\": 64, \"N\": 4, \"s_pairs\": [(0, 1), (2, 3)],\n            \"K_I\": np.array([[10., 20.], [53., 19.], [12., 40.], [51., 41.]]),\n            \"K_FI\": np.array([[56., 23.], [7., 19.], [51., 39.], [12., 44.]])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        w = case[\"w\"]\n        N = case[\"N\"]\n        K_I = case[\"K_I\"]\n        K_FI = case[\"K_FI\"]\n\n        # Construct the permutation map from pairs\n        s_map = np.arange(N)\n        for i, j in case[\"s_pairs\"]:\n            s_map[i], s_map[j] = j, i\n\n        # Calculate pre- and post-enforcement errors\n        e_before, e_after = calculate_symmetric_errors(w, N, s_map, K_I, K_FI)\n        \n        results.append(f\"{e_before:.6f}\")\n        results.append(f\"{e_after:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef F_transform(k_matrix, w):\n    \"\"\"\n    Applies the horizontal flip transform F(x,y;w) = (w-1-x, y) to a matrix of keypoints.\n    \n    Args:\n        k_matrix (np.ndarray): An (N, 2) array of (x,y) coordinates.\n        w (int): The image width.\n        \n    Returns:\n        np.ndarray: The transformed (N, 2) array.\n    \"\"\"\n    k_flipped = k_matrix.copy()\n    k_flipped[:, 0] = w - 1 - k_flipped[:, 0]\n    return k_flipped\n\ndef S_permut(k_matrix, s_map):\n    \"\"\"\n    Applies the permutation S to the rows of a keypoint matrix.\n    \n    Args:\n        k_matrix (np.ndarray): An (N, 2) array of keypoints.\n        s_map (np.ndarray): A 1D array representing the permutation of indices.\n        \n    Returns:\n        np.ndarray: The permuted (N, 2) array.\n    \"\"\"\n    return k_matrix[s_map]\n\ndef calculate_symmetric_errors(w, N, s_map, K_I, K_FI):\n    \"\"\"\n    Calculates the pre- and post-enforcement symmetric errors.\n    \n    Args:\n        w (int): Image width.\n        N (int): Number of keypoints.\n        s_map (np.ndarray): Permutation map for S.\n        K_I (np.ndarray): Keypoints from original image.\n        K_FI (np.ndarray): Keypoints from flipped image.\n        \n    Returns:\n        tuple[float, float]: A tuple containing (E_sym_before, E_sym_after).\n    \"\"\"\n    # --- Calculate E_sym_before ---\n    # The ideal prediction for the flipped image, derived from K_I, is F(S(K_I)).\n    target_from_I = F_transform(S_permut(K_I, s_map), w)\n    \n    # Calculate the per-keypoint difference vectors.\n    diff_before = K_FI - target_from_I\n    \n    # Calculate the L2 norm for each difference vector.\n    norms_before = np.linalg.norm(diff_before, axis=1)\n    \n    # The error is the mean of these norms.\n    E_sym_before = np.mean(norms_before)\n    \n    # --- Calculate E_sym_after ---\n    # First, compute the consistency-enforced keypoints K_cons(I).\n    # K_cons(I) = 0.5 * (K_I + S * F^-1 * K_FI), where F^-1 = F.\n    SF_K_FI = S_permut(F_transform(K_FI, w), s_map)\n    K_cons_I = 0.5 * (K_I + SF_K_FI)\n    \n    # The post-enforcement error is E_sym(K_cons_I, K_FI).\n    # The new target is derived from K_cons_I: F(S(K_cons_I)).\n    target_from_cons = F_transform(S_permut(K_cons_I, s_map), w)\n    \n    diff_after = K_FI - target_from_cons\n    norms_after = np.linalg.norm(diff_after, axis=1)\n    E_sym_after = np.mean(norms_after)\n    \n    # As derived in the solution, E_sym_after is analytically half of E_sym_before.\n    # This implementation calculates it explicitly for rigor.\n    \n    return E_sym_before, E_sym_after\n\nsolve()\n```"
        },
        {
            "introduction": "真实世界的图像经常包含遮挡，这是姿态估计模型面临的一大挑战。本练习  让您扮演研究员的角色，通过模拟来研究在合成遮挡数据上进行训练如何影响模型的鲁棒性。您将应用贝叶斯决策理论来确定区分可见与被遮挡关键点的最优阈值，并评估当测试时的遮挡率与训练条件不同时该系统的性能表现。",
            "id": "3140032",
            "problem": "你的任务是构建一个简约而符合原理的模拟器，用于研究由泊松圆盘采样生成的合成遮挡掩码如何影响一个简化的关键点检测器的鲁棒性。目标是利用第一性原理，将遮挡模式的密度、训练时的决策校准和测试时的鲁棒性联系起来。\n\n图像域为单位正方形，表示为连续集合 $[0,1] \\times [0,1]$，其面积 $A = 1$。一个遮挡掩码是 $n$ 个固定半径 $r > 0$ 的闭合圆盘的并集，其圆心通过泊松圆盘过程采样，最小圆心间距为 $2r$，并被限制在 $[r,1-r] \\times [r,1-r]$ 内，以确保圆盘不重叠且完全包含在域内。在这种设置下，对于域内一个均匀随机点的遮挡事件，等同于被其中一个不重叠的圆盘覆盖，因此覆盖概率等于圆盘所占的面积分数：\n$$\np_{\\text{occ}}(n,r) \\;=\\; \\min\\left(1,\\; n \\,\\pi\\, r^2\\right).\n$$\n\n使用一个简化的、标量置信度分数模型来表示单个关键点：\n- 如果关键点未被遮挡（即其位置未被遮挡掩码覆盖），则分数 $S$ 从一个均值为 $\\mu_1 = 1$、方差为 $\\sigma^2 > 0$ 的正态分布中抽取，记作 $S \\mid \\text{visible} \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$。\n- 如果关键点被遮挡，则分数 $S$ 从一个均值为 $\\mu_0 = 0$、方差同样为 $\\sigma^2$ 的正态分布中抽取，记作 $S \\mid \\text{occluded} \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$。\n\n在训练期间，使用遮挡密度参数 $\\lambda_{\\text{train}} \\in \\mathbb{R}_{\\ge 0}$，掩码由 $n_{\\text{train}} = \\lfloor \\lambda_{\\text{train}} \\, A \\rfloor$ 个半径为 $r$ 的圆盘定义，产生的遮挡概率为 $p_{\\text{occ,train}} = p_{\\text{occ}}(n_{\\text{train}}, r)$。使用基于等方差高斯类条件概率和 $0$-$1$ 损失的基本贝叶斯决策规则，对于二元假设检验（可见与遮挡），最小化错分风险的最优决策阈值 $\\tau^\\star$ 取决于类先验概率 $\\pi_1 = 1 - p_{\\text{occ,train}}$ 和 $\\pi_0 = p_{\\text{occ,train}}$。你必须从似然比检验和正态分布密度出发，通过第一性原理推导出 $\\tau^\\star$。\n\n在测试期间，使用遮挡密度参数 $\\lambda_{\\text{test}} \\in \\mathbb{R}_{\\ge 0}$，掩码由 $n_{\\text{test}} = \\lfloor \\lambda_{\\text{test}} \\, A \\rfloor$ 个相同半径 $r$ 的圆盘定义，得到 $p_{\\text{occ,test}} = p_{\\text{occ}}(n_{\\text{test}}, r)$。鲁棒性度量定义为在使用训练时学到的固定阈值 $\\tau^\\star$ 时的期望正确检测概率：\n$$\n\\mathcal{R} \\;=\\; \\left(1 - p_{\\text{occ,test}}\\right)\\, \\Pr\\!\\left[S \\ge \\tau^\\star \\mid \\text{visible}\\right]\n\\;+\\; p_{\\text{occ,test}} \\,\\Pr\\!\\left[S \\ge \\tau^\\star \\mid \\text{occluded}\\right].\n$$\n使用正态分布的累积分布函数表示每个概率项。\n\n实现要求：\n- 使用域面积 $A = 1$。\n- 使用圆盘半径 $r = 0.06$ 和最小圆心间距 $2r$；假设由于此间距，圆盘不重叠且完全包含在域内，因此 $p_{\\text{occ}}(n,r) = \\min(1, n \\pi r^2)$ 精确成立。\n- 在贝叶斯推导中使用训练时类先验概率 $\\pi_0 = p_{\\text{occ,train}}$ 和 $\\pi_1 = 1 - p_{\\text{occ,train}}$。为避免未定义的对数，你必须在必要时对先验概率应用一个小的平滑值 $\\epsilon = 10^{-9}$，即将 $\\pi_c$ 替换为 $\\max(\\epsilon, \\min(1-\\epsilon, \\pi_c))$。\n- 使用等方差高斯模型，其均值为 $\\mu_1 = 1$、$\\mu_0 = 0$，标准差 $\\sigma > 0$ 按每个测试用例的规定。\n\n测试套件：\n使用以下五个参数三元组 $(\\lambda_{\\text{train}}, \\lambda_{\\text{test}}, \\sigma)$:\n1. $(10, 15, 0.2)$\n2. $(0, 0, 0.2)$\n3. $(5, 25, 0.2)$\n4. $(25, 5, 0.2)$\n5. $(10, 10, 0.5)$\n\n对于每个测试用例，计算鲁棒性度量 $\\mathcal{R}$，结果为实数。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含一个逗号分隔的列表，用方括号括起来，结果顺序与测试套件相同，每个值四舍五入到小数点后六位（例如，`[0.123456,0.654321,...]`）。不应打印任何其他文本，也不应读取任何输入。",
            "solution": "用户提供的问题已经过验证，被认为是合理、定义明确且客观的。以下解决方案遵循指定的要求。\n\n### 原理与推导\n\n该问题要求对一个最优决策阈值进行有理有据的推导，并应用该阈值来评估一个鲁棒性度量。解决方案分为两个主要部分：最优阈值 $\\tau^\\star$ 的推导和鲁棒性度量 $\\mathcal{R}$ 的公式化。\n\n#### 第1部分：最优决策阈值 $\\tau^\\star$ 的推导\n\n任务是根据一个标量置信度分数 $S$ 将一个关键点分类为“可见”（假设 $H_1$）或“被遮挡”（假设 $H_0$）。分类应最小化错误概率，这对应于使用 $0$-$1$ 损失函数的贝叶斯决策规则。\n\n分数 $S$ 的条件分布给定如下：\n- $p(s \\mid H_1)$: $S \\mid \\text{visible} \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ 其中 $\\mu_1=1$。\n- $p(s \\mid H_0)$: $S \\mid \\text{occluded} \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ 其中 $\\mu_0=0$。\n\n先验概率由训练时的遮挡密度决定：\n- $\\pi_1 = \\Pr(H_1) = 1 - p_{\\text{occ,train}}$\n- $\\pi_0 = \\Pr(H_0) = p_{\\text{occ,train}}$\n\n贝叶斯最优决策规则是选择具有最高后验概率的假设。如果 $\\Pr(H_1 \\mid s) > \\Pr(H_0 \\mid s)$，我们判定为 $H_1$。使用贝叶斯定理 $\\Pr(H_i \\mid s) = p(s \\mid H_i) \\pi_i / p(s)$，这等价于：\n$$ p(s \\mid H_1) \\pi_1 > p(s \\mid H_0) \\pi_0 $$\n这可以表示为似然比检验（LRT）。如果满足以下条件，我们判定为 $H_1$：\n$$ \\frac{p(s \\mid H_1)}{p(s \\mid H_0)} > \\frac{\\pi_0}{\\pi_1} $$\n为便于分析，我们对两边取自然对数。决策规则变为，如果满足以下条件，则选择 $H_1$：\n$$ \\ln p(s \\mid H_1) - \\ln p(s \\mid H_0) > \\ln\\left(\\frac{\\pi_0}{\\pi_1}\\right) $$\n正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 的概率密度函数（PDF）是 $f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$。对数PDF为 $\\ln f(x) = -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(x - \\mu)^2}{2\\sigma^2}$。\n将我们假设的对数PDF代入，得到对数似然比：\n$$ \\left(-\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(s - \\mu_1)^2}{2\\sigma^2}\\right) - \\left(-\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(s - \\mu_0)^2}{2\\sigma^2}\\right) $$\n对数项相消，剩下：\n$$ \\frac{(s - \\mu_0)^2 - (s - \\mu_1)^2}{2\\sigma^2} = \\frac{(s^2 - 2s\\mu_0 + \\mu_0^2) - (s^2 - 2s\\mu_1 + \\mu_1^2)}{2\\sigma^2} = \\frac{2s(\\mu_1 - \\mu_0) - (\\mu_1^2 - \\mu_0^2)}{2\\sigma^2} $$\n决策不等式为：\n$$ \\frac{2s(\\mu_1 - \\mu_0) - (\\mu_1^2 - \\mu_0^2)}{2\\sigma^2} > \\ln\\left(\\frac{\\pi_0}{\\pi_1}\\right) $$\n我们求解分数 $s$。由于 $\\mu_1 = 1 > \\mu_0 = 0$，项 $\\mu_1 - \\mu_0$ 为正，因此不等式方向保持不变。\n$$ 2s(\\mu_1 - \\mu_0) > (\\mu_1^2 - \\mu_0^2) + 2\\sigma^2 \\ln\\left(\\frac{\\pi_0}{\\pi_1}\\right) $$\n$$ s > \\frac{\\mu_1^2 - \\mu_0^2}{2(\\mu_1 - \\mu_0)} + \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\pi_0}{\\pi_1}\\right) $$\n使用恒等式 $\\mu_1^2 - \\mu_0^2 = (\\mu_1 - \\mu_0)(\\mu_1 + \\mu_0)$，我们简化第一项：\n$$ s > \\frac{\\mu_1 + \\mu_0}{2} + \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\pi_0}{\\pi_1}\\right) $$\n该不等式定义了决策边界。最优阈值 $\\tau^\\star$ 是右侧的值。如果 $S \\ge \\tau^\\star$，我们判定为“可见”（$H_1$）。\n$$ \\tau^\\star = \\frac{\\mu_1 + \\mu_0}{2} + \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\pi_0}{\\pi_1}\\right) $$\n代入给定值 $\\mu_1=1$ 和 $\\mu_0=0$：\n$$ \\tau^\\star = \\frac{1 + 0}{2} + \\frac{\\sigma^2}{1 - 0} \\ln\\left(\\frac{p_{\\text{occ,train}}}{1 - p_{\\text{occ,train}}}\\right) = \\frac{1}{2} + \\sigma^2 \\ln\\left(\\frac{p_{\\text{occ,train}}}{1 - p_{\\text{occ,train}}}\\right) $$\n为了处理 $p_{\\text{occ,train}}$ 为 $0$ 或 $1$ 的情况，问题指定了对先验概率进行平滑处理。设 $\\pi_0 = p_{\\text{occ,train}}$ 和 $\\pi_1 = 1-\\pi_0$。我们定义平滑后的先验概率 $\\pi'_0 = \\max(\\epsilon, \\min(1-\\epsilon, \\pi_0))$ 和 $\\pi'_1 = 1-\\pi'_0$，其中 $\\epsilon=10^{-9}$。阈值的最终表达式为：\n$$ \\tau^\\star = \\frac{1}{2} + \\sigma^2 \\ln\\left(\\frac{\\pi'_0}{\\pi'_1}\\right) $$\n\n#### 第2部分：鲁棒性度量 $\\mathcal{R}$ 的公式化\n\n鲁棒性度量 $\\mathcal{R}$ 在问题陈述中已明确定义。在测试时，遮挡概率为 $p_{\\text{occ,test}}$。该度量为：\n$$ \\mathcal{R} = (1 - p_{\\text{occ,test}}) \\Pr[S \\ge \\tau^\\star \\mid \\text{visible}] + p_{\\text{occ,test}} \\Pr[S \\ge \\tau^\\star \\mid \\text{occluded}] $$\n该度量表示根据测试时分布随机选择的一个关键点，在使用从训练数据推导出的阈值 $\\tau^\\star$ 时，被分类为“可见”的总概率。\n\n这些概率使用正态分布的生存函数（1 减去累积分布函数，CDF）来计算。设 $\\Phi(z)$ 为标准正态分布 $\\mathcal{N}(0, 1)$ 的CDF。对于一个变量 $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$，概率 $\\Pr[X \\ge x]$ 为 $1 - \\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right)$。\n\n将此应用于我们的两个条件概率：\n-   $\\Pr[S \\ge \\tau^\\star \\mid \\text{visible}] = 1 - \\Phi\\left(\\frac{\\tau^\\star - \\mu_1}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{\\tau^\\star - 1}{\\sigma}\\right)$\n-   $\\Pr[S \\ge \\tau^\\star \\mid \\text{occluded}] = 1 - \\Phi\\left(\\frac{\\tau^\\star - \\mu_0}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{\\tau^\\star}{\\sigma}\\right)$\n\n将这些代入 $\\mathcal{R}$ 的公式中：\n$$ \\mathcal{R} = (1 - p_{\\text{occ,test}}) \\left(1 - \\Phi\\left(\\frac{\\tau^\\star - 1}{\\sigma}\\right)\\right) + p_{\\text{occ,test}} \\left(1 - \\Phi\\left(\\frac{\\tau^\\star}{\\sigma}\\right)\\right) $$\n\n### 计算步骤\n对于每个给定的参数三元组 $(\\lambda_{\\text{train}}, \\lambda_{\\text{test}}, \\sigma)$，计算过程如下：\n1.  计算训练圆盘的数量 $n_{\\text{train}} = \\lfloor \\lambda_{\\text{train}} A \\rfloor$，其中 $A=1$。\n2.  计算训练遮挡概率 $p_{\\text{occ,train}} = \\min(1, n_{\\text{train}} \\pi r^2)$，其中 $r=0.06$。\n3.  应用平滑处理得到先验概率 $\\pi'_0$ 和 $\\pi'_1$。\n4.  使用推导出的公式计算最优阈值 $\\tau^\\star$。\n5.  计算测试圆盘的数量 $n_{\\text{test}} = \\lfloor \\lambda_{\\text{test}} A \\rfloor$。\n6.  计算测试遮挡概率 $p_{\\text{occ,test}} = \\min(1, n_{\\text{test}} \\pi r^2)$。\n7.  使用正态CDF计算两个条件概率 $\\Pr[S \\ge \\tau^\\star \\mid \\text{visible}]$ 和 $\\Pr[S \\ge \\tau^\\star \\mid \\text{occluded}]$。\n8.  组合这些值以计算最终的鲁棒性度量 $\\mathcal{R}$。\n这个过程在提供的Python代码中实现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Calculates the robustness metric for a simplified keypoint detector\n    under varying occlusion conditions.\n    \"\"\"\n    # Define the constants from the problem statement.\n    A = 1.0\n    r = 0.06\n    epsilon = 1e-9\n    mu_1 = 1.0\n    mu_0 = 0.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (10, 15, 0.2),\n        (0, 0, 0.2),\n        (5, 25, 0.2),\n        (25, 5, 0.2),\n        (10, 10, 0.5),\n    ]\n\n    results = []\n    disk_area = np.pi * r**2\n\n    for lambda_train, lambda_test, sigma in test_cases:\n        # --- Training Phase: Derive optimal threshold tau_star ---\n\n        # 1. Calculate training-time occlusion probability\n        n_train = np.floor(lambda_train * A)\n        p_occ_train = min(1.0, n_train * disk_area)\n\n        # 2. Apply smoothing to priors to avoid log(0)\n        # The problem states to replace pi_c with max(eps, min(1-eps, pi_c)).\n        # If pi_0_raw = p_occ_train, then pi_0 = clip(pi_0_raw, eps, 1-eps).\n        # The corresponding pi_1 is then 1 - pi_0, which is also correctly clipped.\n        pi_0_smoothed = np.clip(p_occ_train, epsilon, 1.0 - epsilon)\n        pi_1_smoothed = 1.0 - pi_0_smoothed\n\n        # 3. Calculate optimal decision threshold tau_star\n        # tau_star = (mu_1 + mu_0)/2 + (sigma**2 / (mu_1 - mu_0)) * log(pi_0 / pi_1)\n        tau_star = (mu_1 + mu_0) / 2.0 + (sigma**2 / (mu_1 - mu_0)) * np.log(pi_0_smoothed / pi_1_smoothed)\n\n        # --- Test Phase: Calculate robustness metric R ---\n\n        # 4. Calculate test-time occlusion probability\n        n_test = np.floor(lambda_test * A)\n        p_occ_test = min(1.0, n_test * disk_area)\n\n        # 5. Calculate conditional probabilities for the robustness metric\n        # Use the survival function (1 - CDF) for P(S >= tau_star)\n        # For S | visible ~ N(mu_1, sigma^2)\n        prob_detect_visible = 1.0 - norm.cdf(tau_star, loc=mu_1, scale=sigma)\n        \n        # For S | occluded ~ N(mu_0, sigma^2)\n        prob_detect_occluded = 1.0 - norm.cdf(tau_star, loc=mu_0, scale=sigma)\n\n        # 6. Calculate the robustness metric R\n        # R = (1 - p_occ_test) * P(S >= tau* | vis) + p_occ_test * P(S >= tau* | occ)\n        robustness = (1.0 - p_occ_test) * prob_detect_visible + p_occ_test * prob_detect_occluded\n        \n        results.append(robustness)\n\n    # Final print statement in the exact required format.\n    results_str = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        }
    ]
}