{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp object detection, we must understand the engineering decisions that enable networks to handle vast differences in object scale. A 10-pixel error on a car might be negligible, but for a distant bird, it's catastrophic. This exercise () delves into this challenge by comparing two fundamental strategies for regressing bounding box dimensions: linear-space and log-space parameterization. By analyzing the gradient behavior for small objects, you will analytically prove why log-space regression provides a more stable and effective training signal, a cornerstone technique in architectures like YOLO and SSD.",
            "id": "3146133",
            "problem": "In modern object detection systems such as Region-based Convolutional Neural Network (R-CNN), You Only Look Once (YOLO), and Single Shot MultiBox Detector (SSD), bounding box width and height are commonly regressed either in linear space or in logarithmic space relative to an anchor box. Consider a single training example with a ground-truth width $w_{g} \\in \\mathbb{R}_{0}$ and an anchor width $w_{a} \\in \\mathbb{R}_{0}$. Let $u \\in \\mathbb{R}$ denote the network’s pre-activation scalar output for the width branch. Assume the squared error loss is used in both cases.\n\nTwo training target parameterizations are considered:\n1. Linear-width regression: the prediction is $\\hat{w} = u$ and the loss is $L_{\\mathrm{lin}}(u) = \\frac{1}{2}\\,(\\hat{w} - w_{g})^{2}$.\n2. Log-space width regression: the prediction is parameterized by $u = \\ln(\\hat{w}/w_{a})$, the target is $t = \\ln(w_{g}/w_{a})$, and the loss is $L_{\\log}(u) = \\frac{1}{2}\\,(u - t)^{2}$.\n\nTo compare the gradient sensitivity to small objects, suppose the prediction has a fixed multiplicative error relative to the ground truth, namely $\\hat{w} = r\\,w_{g}$ for some fixed $r \\in \\mathbb{R}_{0}$ with $r \\neq 1$. Using only the definitions of the squared error loss, the natural logarithm, and the chain rule of calculus, derive the gradients $\\frac{\\partial L_{\\mathrm{lin}}}{\\partial u}$ and $\\frac{\\partial L_{\\log}}{\\partial u}$ as functions of $w_{g}$ and $r$, and then form the ratio\n$$\nR(w_{g}) \\;=\\; \\frac{\\left|\\frac{\\partial L_{\\mathrm{lin}}}{\\partial u}\\right|}{\\left|\\frac{\\partial L_{\\log}}{\\partial u}\\right|}.\n$$\nCompute the limit $\\lim_{w_{g} \\to 0^{+}} R(w_{g})$. Provide your final answer as a single real number. No rounding is needed.",
            "solution": "The problem asks for the limit of the ratio of the magnitudes of two gradients, $\\frac{\\partial L_{\\mathrm{lin}}}{\\partial u}$ and $\\frac{\\partial L_{\\log}}{\\partial u}$, as the ground-truth width $w_g$ approaches zero from the positive side. We will derive each gradient separately and then compute the limit of their ratio.\n\nFirst, let's analyze the linear-width regression case.\nThe loss function is given by $L_{\\mathrm{lin}}(u) = \\frac{1}{2}\\,(\\hat{w} - w_{g})^{2}$.\nIn this parameterization, the network's output $u$ is the predicted width itself, so $\\hat{w} = u$.\nSubstituting this into the loss function, we get:\n$$\nL_{\\mathrm{lin}}(u) = \\frac{1}{2}\\,(u - w_{g})^{2}\n$$\nTo find the gradient with respect to $u$, we differentiate $L_{\\mathrm{lin}}(u)$:\n$$\n\\frac{\\partial L_{\\mathrm{lin}}}{\\partial u} = \\frac{\\partial}{\\partial u} \\left[ \\frac{1}{2}\\,(u - w_{g})^{2} \\right] = \\frac{1}{2} \\cdot 2 \\cdot (u - w_{g}) \\cdot \\frac{\\partial}{\\partial u}(u - w_g) = u - w_{g}\n$$\nThe problem states that the prediction has a fixed multiplicative error relative to the ground truth, given by $\\hat{w} = r\\,w_{g}$, where $r \\in \\mathbb{R}_{0}$ and $r \\neq 1$.\nSince $\\hat{w} = u$ for the linear case, we have $u = r\\,w_{g}$.\nSubstituting this expression for $u$ into our gradient, we obtain the gradient as a function of $w_g$ and $r$:\n$$\n\\frac{\\partial L_{\\mathrm{lin}}}{\\partial u} = r\\,w_{g} - w_{g} = (r-1)w_{g}\n$$\n\nNext, let's analyze the log-space width regression case.\nThe loss function is given by $L_{\\log}(u) = \\frac{1}{2}\\,(u - t)^{2}$.\nThe gradient with respect to $u$ is:\n$$\n\\frac{\\partial L_{\\log}}{\\partial u} = \\frac{\\partial}{\\partial u} \\left[ \\frac{1}{2}\\,(u - t)^{2} \\right] = \\frac{1}{2} \\cdot 2 \\cdot (u - t) \\cdot \\frac{\\partial}{\\partial u}(u - t) = u - t\n$$\nHere, $u$ and $t$ are defined in logarithmic space. The network's output parameter is $u = \\ln(\\hat{w}/w_{a})$, and the target parameter is $t = \\ln(w_{g}/w_{a})$.\nWe substitute these definitions into the gradient expression:\n$$\n\\frac{\\partial L_{\\log}}{\\partial u} = \\ln\\left(\\frac{\\hat{w}}{w_{a}}\\right) - \\ln\\left(\\frac{w_{g}}{w_{a}}\\right)\n$$\nUsing the property of logarithms $\\ln(A) - \\ln(B) = \\ln(A/B)$, we simplify the expression:\n$$\n\\frac{\\partial L_{\\log}}{\\partial u} = \\ln\\left( \\frac{\\hat{w}/w_{a}}{w_{g}/w_{a}} \\right) = \\ln\\left(\\frac{\\hat{w}}{w_{g}}\\right)\n$$\nAgain, we use the condition that the prediction has a fixed multiplicative error, $\\hat{w} = r\\,w_{g}$. Substituting this into our gradient expression:\n$$\n\\frac{\\partial L_{\\log}}{\\partial u} = \\ln\\left(\\frac{r\\,w_{g}}{w_{g}}\\right) = \\ln(r)\n$$\nNote that this gradient is a constant that depends only on the multiplicative error factor $r$, and not on the ground-truth width $w_g$. Since $r > 0$ and $r \\neq 1$, $\\ln(r)$ is a non-zero real number.\n\nNow we can form the ratio $R(w_g)$:\n$$\nR(w_{g}) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{lin}}}{\\partial u}\\right|}{\\left|\\frac{\\partial L_{\\log}}{\\partial u}\\right|} = \\frac{|(r-1)w_{g}|}{|\\ln(r)|}\n$$\nSince $w_g \\in \\mathbb{R}_{0}$, we have $|w_g| = w_g$. This allows us to write:\n$$\nR(w_{g}) = \\frac{|r-1| \\cdot w_{g}}{|\\ln(r)|}\n$$\nThe problem specifies that $r$ is a fixed constant. Therefore, the term $\\frac{|r-1|}{|\\ln(r)|}$ is a positive constant value (since $r \\neq 1$). Let's denote this constant by $C = \\frac{|r-1|}{|\\ln(r)|}$.\nThe ratio is then $R(w_{g}) = C \\cdot w_{g}$.\n\nFinally, we compute the required limit as $w_g$ approaches $0$ from the positive side:\n$$\n\\lim_{w_{g} \\to 0^{+}} R(w_{g}) = \\lim_{w_{g} \\to 0^{+}} \\left( C \\cdot w_{g} \\right)\n$$\nSince $C$ is a finite constant, the limit is:\n$$\n\\lim_{w_{g} \\to 0^{+}} C \\cdot w_{g} = C \\cdot \\lim_{w_{g} \\to 0^{+}} w_{g} = C \\cdot 0 = 0\n$$\nThe result indicates that for very small objects (as $w_g \\to 0^{+}$), the gradient from the linear-width regression becomes vanishingly small compared to the gradient from the log-space regression, which remains constant. This demonstrates why log-space parameterization is more effective for training on datasets with a wide range of object scales, including small ones.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "A good object detector needs a loss function that reflects the visual quality of a prediction, not just pixel-wise coordinate errors. While Intersection over Union (IoU) was a major leap forward, it has limitations, such as providing no learning signal for non-overlapping boxes. This exercise () invites you to explore the evolution of IoU-based losses, including Generalized IoU (GIoU), Distance IoU (DIoU), and Complete IoU (CIoU). By dissecting a controlled scenario, you will see precisely how each variant adds unique penalty terms to create more robust and efficient training gradients.",
            "id": "3146191",
            "problem": "Consider a simplified analytic comparison used in the design of bounding-box regression losses for modern object detectors such as the Region-based Convolutional Neural Network (R-CNN) family, You Only Look Once (YOLO), and the Single Shot MultiBox Detector (SSD). Two axis-aligned, same-sized, rectangular bounding boxes of width $1$ and height $1$ are placed on the image plane. The ground-truth box is fixed, and the predicted box is translated by $\\delta x$ and $\\delta y$ relative to the ground-truth center. Assume a symmetric shift $\\delta x = \\delta y = d$ with $0  d  1$. Impose the constraint that the Intersection over Union (IoU) equals $0.5$ under this translation. Using only standard definitions of Intersection over Union (IoU), Generalized Intersection over Union (GIoU), Distance Intersection over Union (DIoU), and Complete Intersection over Union (CIoU), derive the corresponding losses $L_{\\mathrm{IoU}}$, $L_{\\mathrm{GIoU}}$, $L_{\\mathrm{DIoU}}$, and $L_{\\mathrm{CIoU}}$ for this configuration, and determine which loss yields the largest penalty value for the fixed $\\mathrm{IoU} = 0.5$.\n\nLet $k$ encode the identity of the largest-penalty loss as follows:\n$k = 1$ for IoU loss, $k = 2$ for GIoU loss, $k = 3$ for DIoU loss, and $k = 4$ for CIoU loss. Provide $k$ as your final answer. No rounding is required, and the final answer should be a single integer.",
            "solution": "The problem requires comparing four bounding box regression loss functions ($L_{\\mathrm{IoU}}$, $L_{\\mathrm{GIoU}}$, $L_{\\mathrm{DIoU}}$, $L_{\\mathrm{CIoU}}$) for a specific geometric configuration.\n\nFirst, let's define the geometry. Let the ground-truth box $B_{gt}$ (1x1) be centered at $(0, 0)$. The predicted box $B_p$ (1x1) is centered at $(d, d)$.\nThe intersection area $A_I$ is $(1-d) \\times (1-d) = (1-d)^2$.\nThe union area $A_U$ is $A_{gt} + A_p - A_I = 1 + 1 - (1-d)^2 = 2 - (1-d)^2$.\nThe Intersection over Union (IoU) is $\\frac{A_I}{A_U} = \\frac{(1-d)^2}{2 - (1-d)^2}$.\nGiven $\\mathrm{IoU} = 0.5$:\n$$ \\frac{1}{2} = \\frac{(1-d)^2}{2 - (1-d)^2} \\implies 2 - (1-d)^2 = 2(1-d)^2 \\implies 3(1-d)^2 = 2 $$\nThis gives $(1-d)^2 = \\frac{2}{3}$. Since $0  d  1$, we have $1-d = \\sqrt{2/3}$.\n\nNow we calculate each loss:\n\n1.  **IoU Loss ($L_{\\mathrm{IoU}}$):**\n    $L_{\\mathrm{IoU}} = 1 - \\mathrm{IoU} = 1 - 0.5 = 0.5$.\n\n2.  **Generalized IoU Loss ($L_{\\mathrm{GIoU}}$):**\n    $L_{\\mathrm{GIoU}} = 1 - \\mathrm{GIoU} = 1 - \\left(\\mathrm{IoU} - \\frac{A_C - A_U}{A_C}\\right) = L_{\\mathrm{IoU}} + \\frac{A_C - A_U}{A_C}$.\n    The smallest enclosing box $C$ has width $w_C = (d+\\frac{1}{2}) - (-\\frac{1}{2}) = 1+d$ and height $h_C = 1+d$. So, $A_C = (1+d)^2$.\n    The penalty term numerator is $A_C - A_U = (1+d)^2 - [2 - (1-d)^2] = (1+2d+d^2) - (2 - (1-2d+d^2)) = 2d^2$.\n    $L_{\\mathrm{GIoU}} = 0.5 + \\frac{2d^2}{(1+d)^2}$.\n\n3.  **Distance IoU Loss ($L_{\\mathrm{DIoU}}$):**\n    $L_{\\mathrm{DIoU}} = L_{\\mathrm{IoU}} + \\frac{\\rho^2(b_{gt}, b_p)}{c^2}$, where $\\rho^2$ is the squared distance between centers and $c^2$ is the squared diagonal of the enclosing box $C$.\n    $\\rho^2 = (d-0)^2 + (d-0)^2 = 2d^2$.\n    $c^2 = w_C^2 + h_C^2 = (1+d)^2 + (1+d)^2 = 2(1+d)^2$.\n    The penalty term is $\\frac{2d^2}{2(1+d)^2} = \\frac{d^2}{(1+d)^2}$.\n    $L_{\\mathrm{DIoU}} = 0.5 + \\frac{d^2}{(1+d)^2}$.\n\n4.  **Complete IoU Loss ($L_{\\mathrm{CIoU}}$):**\n    $L_{\\mathrm{CIoU}} = L_{\\mathrm{DIoU}} + \\alpha v$. The aspect ratio consistency term $v$ is zero because both boxes are squares and have the same aspect ratio.\n    $v = \\frac{4}{\\pi^2}\\left(\\arctan\\frac{w_{gt}}{h_{gt}} - \\arctan\\frac{w_p}{h_p}\\right)^2 = \\frac{4}{\\pi^2}\\left(\\arctan(1) - \\arctan(1)\\right)^2 = 0$.\n    Therefore, $L_{\\mathrm{CIoU}} = L_{\\mathrm{DIoU}} = 0.5 + \\frac{d^2}{(1+d)^2}$.\n\n**Comparison:**\n-   $L_{\\mathrm{IoU}} = 0.5$\n-   $L_{\\mathrm{GIoU}} = 0.5 + 2\\left(\\frac{d}{1+d}\\right)^2$\n-   $L_{\\mathrm{DIoU}} = 0.5 + \\left(\\frac{d}{1+d}\\right)^2$\n-   $L_{\\mathrm{CIoU}} = 0.5 + \\left(\\frac{d}{1+d}\\right)^2$\n\nSince $d > 0$, the penalty term $\\left(\\frac{d}{1+d}\\right)^2$ is strictly positive.\nComparing the penalties, we see that $2\\left(\\frac{d}{1+d}\\right)^2 > \\left(\\frac{d}{1+d}\\right)^2 > 0$.\nThus, the ordering of losses is: $L_{\\mathrm{GIoU}} > L_{\\mathrm{DIoU}} = L_{\\mathrm{CIoU}} > L_{\\mathrm{IoU}}$.\nThe loss with the largest penalty is $L_{\\mathrm{GIoU}}$, which corresponds to $k=2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Anchor-based detectors like SSD and YOLO don't predict bounding boxes from scratch; they refine a set of pre-defined \"anchor\" or \"prior\" boxes. The quality of these initial anchors is therefore critical to a model's success. This practice () explores the art of anchor design by simulating the process of generating anchors via k-means clustering. You will compare how two different clustering objectives—one using Euclidean distance in log-space and another directly using an IoU-based metric—impact recall, revealing why aligning the design of architectural priors with the final evaluation metric is key to maximizing performance.",
            "id": "3146221",
            "problem": "A Single Shot MultiBox Detector (SSD) uses a fixed set of anchor boxes at each feature map level to propose candidate detections. At a given level, suppose we must choose $K$ anchor shapes (width $w$ and height $h$) per spatial location. Anchors are matched to ground-truth boxes by Intersection-over-Union (IoU) thresholding: an anchor is considered a match for a ground-truth object if its IoU with that object is at least a threshold $\\tau$, where IoU for same-centered rectangles is defined by the ratio of intersection area to union area. Recall is defined as the fraction of ground-truth boxes that have at least one matched anchor.\n\nWe construct anchor shapes per level by running $k$-means clustering on the set of ground-truth box shapes projected to that level, using $(\\log w, \\log h)$ as features. Consider two choices of clustering dissimilarity:\n- Choice $1$: Euclidean distance in $(\\log w, \\log h)$, that is $d_{E}((w,h),(w',h')) = \\sqrt{\\left(\\log w - \\log w'\\right)^{2} + \\left(\\log h - \\log h'\\right)^{2}}$.\n- Choice $2$: IoU-based dissimilarity for same-centered boxes, that is $d_{I}((w,h),(w',h')) = 1 - \\mathrm{IoU}((w,h),(w',h'))$.\n\nAssume the following stylized small-object distribution on a finest SSD level (small stride): two equally likely modes of small boxes, with mode $\\mathcal{A}$ having shape $(w,h) = (20,5)$ and mode $\\mathcal{B}$ having shape $(w,h) = (5,20)$ (pixel units). The IoU threshold is $\\tau = 0.5$. For the purpose of reasoning, assume same-centered anchors and ground-truth boxes at this level.\n\nUsing only fundamental definitions and the geometric consequences of the above dissimilarities, reason about how the choice of clustering objective impacts the recall of small objects as $K$ varies. Which statement best captures the behavior?\n\nA. For small $K$, IoU-based clustering (Choice $2$) tends to yield higher small-object recall than Euclidean clustering in $(\\log w, \\log h)$ (Choice $1$), because the objective aligns directly with IoU-based matching; recall is non-decreasing in $K$ for both objectives, and the recall gap narrows as $K$ increases.\n\nB. For small $K$, Euclidean clustering in $(\\log w, \\log h)$ (Choice $1$) yields higher small-object recall than IoU-based clustering (Choice $2$), since logarithms favor small sizes; moreover, increasing $K$ decreases recall due to overfitting anchors to modes.\n\nC. The recall of small objects is independent of $K$, and both clustering objectives achieve the same recall for any $K$ because SSD is multi-level.\n\nD. Choosing Euclidean clustering in $(\\log w, \\log h)$ (Choice $1$) makes increasing $K$ unnecessary, because the logarithm guarantees IoU above the threshold $\\tau$ for all small objects regardless of anchor shape.",
            "solution": "The core of the problem is to compare how two different clustering objectives for generating anchor boxes affect recall. Recall is achieved for a ground-truth (GT) box if its IoU with at least one of the $K$ anchor boxes is $\\ge 0.5$. The anchor boxes are the centroids found by $k$-means. We will analyze the cases for small $K$ and the trend as $K$ increases.\n\nThe two GT box modes are $\\mathcal{A}: (20, 5)$ and $\\mathcal{B}: (5, 20)$. The IoU threshold is $\\tau = 0.5$. For same-centered boxes, the IoU is $\\frac{\\min(w_1, w_2) \\cdot \\min(h_1, h_2)}{w_1 h_1 + w_2 h_2 - \\min(w_1, w_2) \\cdot \\min(h_1, h_2)}$.\n\n### Analysis for $K=1$\nFor $K=1$, we seek a single anchor box (cluster centroid) that best represents the entire distribution.\n\n**Choice 1: Euclidean clustering in $(\\log w, \\log h)$ space.**\nWith two equally likely modes at $(\\log 20, \\log 5)$ and $(\\log 5, \\log 20)$, the centroid for $K=1$ will be their average:\n- $\\log w_{\\text{anchor}} = (\\log 20 + \\log 5)/2 = \\log(\\sqrt{20 \\cdot 5}) = \\log(10)$\n- $\\log h_{\\text{anchor}} = (\\log 5 + \\log 20)/2 = \\log(\\sqrt{5 \\cdot 20}) = \\log(10)$\nThis corresponds to an anchor shape of $(10, 10)$. We check its IoU with Mode $\\mathcal{A}$:\n$$ \\mathrm{IoU}((10,10), (20,5)) = \\frac{10 \\cdot 5}{100 + 100 - 50} = \\frac{50}{150} = \\frac{1}{3} $$\nSince $1/3  0.5$, this anchor fails to match GTs from either mode. Recall is $0\\%$.\n\n**Choice 2: IoU-based clustering.**\nThe objective is to find an anchor that maximizes average IoU. Let's test the anchor $(10,10)$ from Choice 1: the average IoU is $1/3$. Now, let's test an anchor that perfectly matches one mode, e.g., $(20,5)$:\n- IoU with Mode $\\mathcal{A}$: $\\mathrm{IoU}((20,5), (20,5)) = 1.0$.\n- IoU with Mode $\\mathcal{B}$: $\\mathrm{IoU}((20,5), (5,20)) = \\frac{5 \\cdot 5}{100+100-25} = \\frac{25}{175} = \\frac{1}{7}$.\nThe average IoU is $(1.0 + 1/7)/2 = 4/7 \\approx 0.57$. Since $4/7 > 1/3$, the IoU-based clustering will prefer an anchor that is identical to one of the modes. With the anchor $(20,5)$, it matches all GT boxes from Mode $\\mathcal{A}$ (IoU = 1.0) but fails to match any from Mode $\\mathcal{B}$ (IoU = 1/7). Since the modes are equally likely, the recall is $50\\%$.\n\nFor $K=1$, IoU-based clustering gives $50\\%$ recall, while Euclidean clustering gives $0\\%$ recall.\n\n### Analysis for $K \\ge 2$\nFor $K=2$, both clustering methods will identify the two distinct modes and place one centroid at each: $(20,5)$ and $(5,20)$. With these two anchors, any GT box from Mode $\\mathcal{A}$ is perfectly matched by the $(20,5)$ anchor (IoU=1.0), and any GT box from Mode $\\mathcal{B}$ is perfectly matched by the $(5,20)$ anchor (IoU=1.0). Both methods achieve $100\\%$ recall.\n\n### Conclusion and Option Analysis\n- **Behavior for small K**: For $K=1$, IoU-based clustering (Choice 2) yields significantly higher recall because its objective function ($1-\\mathrm{IoU}$) is directly aligned with the matching criterion (IoU threshold). Euclidean distance in log-space is a proxy that can fail, as shown.\n- **Behavior as K increases**: Recall is non-decreasing with $K$. Adding more anchors can only improve or maintain recall.\n- **Gap between methods**: The recall gap is large for $K=1$ (50% vs 0%) and closes to zero for $K=2$.\n\nThis analysis directly supports statement A:\n**A. For small $K$, IoU-based clustering (Choice $2$) tends to yield higher small-object recall than Euclidean clustering in $(\\log w, \\log h)$ (Choice $1$), because the objective aligns directly with IoU-based matching; recall is non-decreasing in $K$ for both objectives, and the recall gap narrows as $K$ increases.**\nAll other options contain incorrect claims (e.g., recall decreasing with K, recall being independent of K, or Euclidean distance providing guarantees on IoU).",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}