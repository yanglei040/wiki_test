{
    "hands_on_practices": [
        {
            "introduction": "To truly understand group convolutions, there's no substitute for building one from the ground up. This first exercise guides you through implementing a group convolution for the cyclic group of planar rotations, $C_4$, which includes rotations by $0^\\circ, 90^\\circ, 180^\\circ,$ and $270^\\circ$. By constructing the layer using a shared kernel and verifying its behavior on a rotated input, you will see firsthand how the principle of weight sharing is the mechanism that enforces rotation equivariance, a foundational concept in building symmetry-aware networks .",
            "id": "3180084",
            "problem": "You are to implement and test a group convolution over the cyclic group of four planar rotations generated by a $90^\\circ$ rotation. The task is grounded in the core definition of discrete two-dimensional convolution (implemented as discrete two-dimensional cross-correlation without kernel reversal, with valid windowing) and the action of rotations on arrays. Starting only from these base definitions, you must construct a rotation-equivariant mapping via weight sharing and verify its behavior precisely.\n\nDefinitions and setup to use:\n- Let $X \\in \\mathbb{R}^{H \\times W}$ be a real-valued image on a square grid and let $K \\in \\mathbb{R}^{k_h \\times k_w}$ be a real-valued kernel.\n- Define the rotation operator $R_m$ for $m \\in \\{0,1,2,3\\}$ to act on any array by rotating it by $m$ successive turns of $90^\\circ$ in the plane, measured in degrees. That is, $R_0$ is the identity, $R_1$ rotates by $90^\\circ$, $R_2$ by $180^\\circ$, and $R_3$ by $270^\\circ$. Angles are to be understood in degrees.\n- Define the discrete two-dimensional cross-correlation with valid windowing as the operation that, for each spatial location where the kernel fully overlaps the image, multiplies elementwise and sums. This reduces the output shape to $(H - k_h + 1) \\times (W - k_w + 1)$.\n- Define a group convolution over the cyclic group of order four, denoted by $C_4$, by producing four orientation channels indexed by $g \\in \\{0,1,2,3\\}$ via $Y_g = X \\star R_g K$ using the above cross-correlation, where $R_g K$ is the kernel rotated by $g$ quarter turns. This enforces weight sharing across orientations because all $Y_g$ are computed using rotated copies of a single base kernel $K$.\n- The rotation-equivariance to be verified is the statement that, for $m \\in \\{0,1,2,3\\}$, if the input is rotated to $X' = R_m X$, then the group convolution outputs $Y'_g$ computed from $X'$ and the same shared base kernel satisfy $Y'_g = R_m \\, Y_{g - m}$ with index arithmetic modulo $4$. Here $Y_{g - m}$ denotes the orientation channel indexed by $(g - m) \\bmod 4$.\n\nYour program must:\n1. Implement $R_m$ using $m$ successive $90^\\circ$ rotations on arrays.\n2. Implement the valid discrete two-dimensional cross-correlation described above.\n3. Implement the $C_4$ group convolution $X \\mapsto (Y_0,Y_1,Y_2,Y_3)$ with weight sharing via $R_g K$.\n4. For each test case, compute a boolean asserting the equivariance statement $Y'_g = R_m \\, Y_{g - m}$ for all $g \\in \\{0,1,2,3\\}$ within an absolute tolerance of $10^{-9}$, where $Y'_g$ is computed by applying the same $C_4$ group convolution to $X' = R_m X$ with the same base kernel $K$.\n5. Include one contrast test where the four orientation channels are computed using four unrelated, unshared kernels $(K_0,K_1,K_2,K_3)$ that are not related by rotation. In that test, assert the same equivariance condition and report the boolean outcome (this will typically be false), thus demonstrating that weight sharing across rotated copies is the enforcing mechanism behind rotation equivariance.\n\nTest suite specification:\n- Test case $1$ (general case): $X$ of size $6 \\times 6$ drawn from a standard normal distribution with pseudo-random seed $1$, $K$ of size $3 \\times 3$ drawn from a standard normal distribution with pseudo-random seed $0$, rotation index $m = 1$ corresponding to $90^\\circ$.\n- Test case $2$ (even-sized kernel boundary behavior): $X$ of size $5 \\times 5$ with seed $3$, $K$ of size $2 \\times 2$ with seed $2$, rotation index $m = 1$ corresponding to $90^\\circ$.\n- Test case $3$ (multiple-step rotation): $X$ of size $7 \\times 7$ with seed $5$, $K$ of size $3 \\times 3$ with seed $4$, rotation index $m = 2$ corresponding to $180^\\circ$.\n- Test case $4$ (identity transformation): $X$ of size $6 \\times 6$ with seed $7$, $K$ of size $3 \\times 3$ with seed $6$, rotation index $m = 0$ corresponding to $0^\\circ$.\n- Test case $5$ (structured input): $X$ of size $8 \\times 8$ set to all ones, $K$ of size $3 \\times 3$ with seed $8$, rotation index $m = 3$ corresponding to $270^\\circ$.\n- Test case $6$ (contrast without weight sharing): $X$ of size $6 \\times 6$ with seed $9$, four unrelated kernels $(K_0,K_1,K_2,K_3)$ each of size $3 \\times 3$ generated with seeds $10, 11, 12, 13$ respectively, rotation index $m = 1$ corresponding to $90^\\circ$. In this case, compute the four orientation channels using $Y_g = X \\star K_g$ and $Y'_g = (R_m X) \\star K_g$ and check if $Y'_g = R_m \\, Y_{g - m}$ holds for all $g$, reporting the boolean result.\n\nNumerical tolerance specification:\n- Use an absolute tolerance of $10^{-9}$ and a relative tolerance of $0$ when comparing arrays for equality.\n\nFinal output format:\n- Your program should produce a single line of output containing the six boolean results, in order for test cases $1$ through $6$, as a comma-separated list enclosed in square brackets, for example, `[true,false,true,false,true,false]`. Use the Python boolean literals so the actual output should look like `[True,False,True,False,True,False]`.",
            "solution": "The problem requires the implementation and verification of rotation equivariance for a specific type of convolutional layer, known as a group convolution, defined over the cyclic group of 4 planar rotations, $C_4 = \\{0^\\circ, 90^\\circ, 180^\\circ, 270^\\circ\\}$. The core of the task is to demonstrate from first principles that a convolutional map constructed with a specific weight-sharing scheme will be equivariant to rotations of its input.\n\nEquivariance is a fundamental concept in physics and machine learning. A function or system $\\Phi$ is said to be equivariant with respect to a set of transformations $G$ if applying a transformation from $G$ to the input of the system produces a predictably transformed output. More formally, for any transformation $T_g$ in the group $G$, there exists a corresponding transformation $T'_g$ on the output space such that $\\Phi(T_g(X)) = T'_g(\\Phi(X))$. This property is desirable as it ensures that the system's analysis of an object is independent of the object's orientation or position.\n\nOur objective is to verify the following equivariance identity for a $C_4$ group convolution: when an input image $X$ is rotated by $m$ quarter-turns to produce $X' = R_m X$, the resulting set of output feature maps $(Y'_0, Y'_1, Y'_2, Y'_3)$ is a rotated and permuted version of the original output maps $(Y_0, Y_1, Y_2, Y_3)$. Specifically, the identity to be verified is $Y'_g = R_m Y_{(g - m) \\pmod 4}$ for each orientation channel $g \\in \\{0, 1, 2, 3\\}$.\n\nThe solution is constructed in steps, based on the definitions provided.\n\n**1. The Rotation Operator $R_m$**\n\nThe group of transformations is the cyclic group $C_4$, whose elements correspond to rotations by angles $\\{0^\\circ, 90^\\circ, 180^\\circ, 270^\\circ\\}$. We define an operator $R_m$ that acts on a two-dimensional array, rotating it by $m$ successive $90^\\circ$ counter-clockwise turns, where $m \\in \\{0, 1, 2, 3\\}$. This operation can be directly implemented using established numerical library functions that perform rotations on matrices. The action of $R_m$ on an array of shape $H \\times W$ results in an array of shape $W \\times H$ if $m$ is odd, and $H \\times W$ if $m$ is even.\n\n**2. Discrete 2D Cross-Correlation**\n\nThe fundamental building block is the discrete two-dimensional cross-correlation operation, denoted by $\\star$. Given an input image $X \\in \\mathbb{R}^{H \\times W}$ and a kernel $K \\in \\mathbb{R}^{k_h \\times k_w}$, the output feature map $Y = X \\star K$ is computed. The operation is specified with 'valid' windowing, meaning the output is calculated only for locations where the kernel fully overlaps the input. The value at each output location $(i, j)$ is the sum of the element-wise product of the kernel and the corresponding sub-region of the input.\n$$Y[i,j] = \\sum_{u=0}^{k_h-1} \\sum_{v=0}^{k_w-1} X[i+u, j+v] K[u,v]$$\nThis operation results in an output map of size $(H - k_h + 1) \\times (W - k_w + 1)$. This is distinct from a full convolution, which would involve reversing the kernel.\n\n**3. $C_4$ Group Convolution via Weight Sharing**\n\nA standard convolutional layer is not inherently rotation-equivariant. To build a $C_4$-equivariant layer, we enforce a specific structure known as weight sharing. Instead of learning separate kernels for different orientations, we use a single base kernel $K$ and generate all other necessary kernels by applying the group's transformations to it.\n\nThe output of the $C_4$ group convolution consists of four orientation channels, $(Y_0, Y_1, Y_2, Y_3)$, indexed by the elements $g \\in \\{0, 1, 2, 3\\}$ of the group. Each channel $Y_g$ is computed by convolving the input image $X$ with a rotated version of the base kernel $K$:\n$$Y_g = X \\star (R_g K)$$\nHere, $R_g K$ is the kernel $K$ rotated by $g$ quarter-turns. This construction ensures that the features detected by the layer are the same, just oriented differently, across the four channels.\n\n**4. Verification of the Equivariance Property**\n\nThe central task is to verify the equivariance relation numerically. The property is a direct consequence of the relationship between rotation and cross-correlation. For any 2D arrays $A$ and $B$ where the operation is defined, the following identity holds:\n$$(R_m A) \\star B = R_m (A \\star R_{-m} B)$$\nwhere $R_{-m}$ is the inverse rotation of $R_m$. In the group $C_4$, the inverse of $R_m$ is $R_{(-m) \\pmod 4}$.\n\nLet us apply this identity to our problem. The new output channels $Y'_g$ are computed from a rotated input $X' = R_m X$ using the same set of shared, rotated kernels:\n$$Y'_g = X' \\star (R_g K) = (R_m X) \\star (R_g K)$$\nApplying the identity with $A=X$ and $B=R_g K$, we get:\n$$Y'_g = R_m (X \\star R_{(-m) \\pmod 4} (R_g K))$$\nSince the rotation operators form a group, their composition is another rotation: $R_{(-m) \\pmod 4} (R_g K) = R_{(-m+g) \\pmod 4} K = R_{(g-m) \\pmod 4} K$. Substituting this back gives:\n$$Y'_g = R_m (X \\star R_{(g-m) \\pmod 4} K)$$\nThe term in the parenthesis, $X \\star R_{(g-m) \\pmod 4} K$, is by definition the original output channel $Y_{(g-m) \\pmod 4}$. Therefore, we arrive at the equivariance property:\n$$Y'_g = R_m Y_{(g-m) \\pmod 4}$$\nThis derivation shows that the property is mathematically guaranteed by the construction. Our implementation will verify this numerically for several test cases. For each case, we compute the left-hand side and the right-hand side of this equation for all $g \\in \\{0, 1, 2, 3\\}$ and check for equality within a specified numerical tolerance.\n\n**5. The Contrast Test: The Necessity of Weight Sharing**\n\nThe final test case serves as a crucial contrast. In this scenario, we define a \"convolutional\" layer with four output channels, but we break the weight-sharing constraint. The four channels are computed using four independent, unrelated kernels $(K_0, K_1, K_2, K_3)$.\n$$Y_g = X \\star K_g$$\nWhen the input is rotated to $X' = R_m X$, the new outputs are $Y'_g = (R_m X) \\star K_g$. The check for equivariance remains the same: we test if $Y'_g = R_m Y_{(g - m) \\pmod 4}$. Because the kernels $K_g$ are not related by the group action (i.e., $K_g \\neq R_g K_0$ in general), the derivation from step 4 fails. The equivariance property is not structurally enforced and is expected to fail, demonstrating that the specific weight-sharing scheme is the mechanism responsible for achieving equivariance.\n\nThe final program will systematically implement these steps and execute the described tests to produce a boolean result for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import correlate2d\n\ndef solve():\n    \"\"\"\n    Implements and verifies C4 rotation equivariance for a 2D group convolution.\n    \"\"\"\n\n    # Test suite specification\n    # (id, X_shape, X_seed, K_shape, K_seed(s), m)\n    test_cases = [\n        (1, (6, 6), 1, (3, 3), [0], 1),\n        (2, (5, 5), 3, (2, 2), [2], 1),\n        (3, (7, 7), 5, (3, 3), [4], 2),\n        (4, (6, 6), 7, (3, 3), [6], 0),\n        (5, (8, 8), 8, (3, 3), [8], 3),  # Structured input\n        (6, (6, 6), 9, (3, 3), [10, 11, 12, 13], 1)  # Contrast test\n    ]\n\n    results = []\n    \n    # Numerical tolerance for array comparisons\n    atol = 1e-9\n    rtol = 0.0\n\n    for case in test_cases:\n        case_id, x_shape, x_seed, k_shape, k_seeds, m = case\n\n        # --- Data Generation ---\n        rng_x = np.random.RandomState(x_seed)\n        if case_id == 5: # Structured input of all ones\n            X = np.ones(x_shape)\n        else:\n            X = rng_x.randn(*x_shape)\n        \n        kernels = []\n        for seed in k_seeds:\n            rng_k = np.random.RandomState(seed)\n            kernels.append(rng_k.randn(*k_shape))\n\n        is_equivariant = False # Placeholder\n        \n        # --- Helper Functions ---\n        def rotate_array(arr, k):\n            \"\"\"Rotates a 2D array by k*90 degrees counter-clockwise.\"\"\"\n            return np.rot90(arr, k=k)\n\n        def cross_correlate(image, kernel):\n            \"\"\"Computes 2D cross-correlation with 'valid' padding.\"\"\"\n            return correlate2d(image, kernel, mode='valid')\n\n        # --- Main Logic ---\n        if case_id != 6:\n            # Standard C4 Group Convolution with weight sharing\n            K_base = kernels[0]\n            \n            # 1. Compute original output channels Y_g = X * (R_g K)\n            Y_channels = []\n            for g in range(4):\n                rotated_K = rotate_array(K_base, g)\n                Y_g = cross_correlate(X, rotated_K)\n                Y_channels.append(Y_g)\n            \n            # 2. Rotate input: X' = R_m X\n            X_prime = rotate_array(X, m)\n            \n            # 3. Compute new output channels Y'_g = X' * (R_g K)\n            Y_prime_channels = []\n            for g in range(4):\n                rotated_K = rotate_array(K_base, g)\n                Y_prime_g = cross_correlate(X_prime, rotated_K)\n                Y_prime_channels.append(Y_prime_g)\n\n        else: # Contrast test without weight sharing\n            # 1. Compute original output channels Y_g = X * K_g\n            Y_channels = []\n            for g in range(4):\n                Y_g = cross_correlate(X, kernels[g])\n                Y_channels.append(Y_g)\n                \n            # 2. Rotate input: X' = R_m X\n            X_prime = rotate_array(X, m)\n\n            # 3. Compute new output channels Y'_g = X' * K_g\n            Y_prime_channels = []\n            for g in range(4):\n                Y_prime_g = cross_correlate(X_prime, kernels[g])\n                Y_prime_channels.append(Y_prime_g)\n\n        # 4. Verify equivariance: Y'_g == R_m(Y_{g-m mod 4}) for all g\n        all_g_hold = True\n        for g in range(4):\n            # LHS: Y'_g\n            lhs = Y_prime_channels[g]\n            \n            # RHS: R_m(Y_{g-m mod 4})\n            original_channel_index = (g - m) % 4\n            y_original_permuted = Y_channels[original_channel_index]\n            rhs = rotate_array(y_original_permuted, m)\n            \n            # Check for equality within tolerance\n            if not np.allclose(lhs, rhs, rtol=rtol, atol=atol):\n                all_g_hold = False\n                break\n        \n        is_equivariant = all_g_hold\n        results.append(is_equivariant)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having implemented a simple group convolution, we now turn to its mathematical underpinnings for more complex symmetries. This practice challenges you to derive the explicit form of a group convolution for the dihedral group $D_8$, the group of symmetries of an octagon, which includes both rotations and reflections. This exercise will deepen your understanding of the \"lifting\" convolution—the crucial first layer that maps a standard image to a feature map on the group—and explain the origin and structure of the orientation channels that are characteristic of G-CNNs .",
            "id": "3133440",
            "problem": "Consider a small image recognition task designed to probe orientation sensitivity in a Convolutional Neural Network (CNN). Each input is a grayscale image patch $f : \\mathbb{Z}^{2} \\to \\mathbb{R}$ supported on a $9 \\times 9$ grid centered at the origin, containing a single straight bar of width $1$ pixel that passes through the center and is oriented at an angle that is a multiple of $45^{\\circ}$. In addition to rotations by multiples of $45^{\\circ}$, the dataset is augmented by reflections across the horizontal axis. To achieve equivariance to these transformations, we consider a Group-equivariant Convolutional Neural Network (G-CNN), where the group is the dihedral group $D_{8}$, the symmetry group of the regular octagon, with $|D_{8}| = 16$ elements, consisting of $8$ rotations $\\{r^{k} : k \\in \\{0,\\dots,7\\}\\}$ and $8$ reflections $\\{r^{k} s : k \\in \\{0,\\dots,7\\}\\}$ satisfying $r^{8} = e$, $s^{2} = e$, and $s r s = r^{-1}$.\n\nYou will derive the explicit group convolution that guarantees equivariance under the left-regular action of $D_{8}$, and then compute the required number of orientation channels when lifting a base channel set to the group.\n\nUse the following fundamental bases:\n- The definition of the left-regular action $L_{u}$ of a group element $u \\in G$ on functions $F : G \\to \\mathbb{R}^{C}$, given by $(L_{u} F)(g) = F(u^{-1} g)$.\n- Linearity of the layer map and stationarity (weight sharing) across the group.\n- The fact that $D_{8}$ acts on the image domain $\\mathbb{Z}^{2}$ by orthogonal transformations corresponding to rotations by multiples of $45^{\\circ}$ and reflections, written as $g \\cdot x$ for $g \\in D_{8}$ and $x \\in \\mathbb{Z}^{2}$.\n\nTasks:\n1. Starting from the listed bases and without assuming any particular convolution formula, derive the unique linear, group-stationary operator from functions on $G$ to functions on $G$ that is equivariant under the left-regular action of $D_{8}$. Write its explicit form as a sum over $G$, and explain briefly why it is equivariant.\n2. For the first (lifting) layer that maps $f : \\mathbb{Z}^{2} \\to \\mathbb{R}^{C_{0}}$ to a feature map on $D_{8}$, derive the explicit lifted convolution that is equivariant to the $D_{8}$ action on $\\mathbb{Z}^{2}$.\n3. The network designer chooses a base channel count of $C_{0} = 12$ for the lifting layer. Predict the total number of orientation channels per spatial location after the lift, expressed as a single integer. No rounding is needed; report the exact integer. Do not include units in your final answer.",
            "solution": "The problem asks for the derivation of group-equivariant convolution operators and the calculation of the number of feature channels resulting from a lifting operation in a Group-equivariant Convolutional Neural Network (G-CNN). The group of interest is the dihedral group $D_{8}$ of order $16$.\n\nFirst, we address the three tasks in order.\n\n### Task 1: Derivation of the Group-to-Group ($G-G$) Convolution\n\nWe are asked to derive the unique linear, group-stationary operator $\\Phi$ that maps functions on the group $G=D_8$ to functions on $G$, which is equivariant under the left-regular action. For simplicity, let's consider functions $F_{in}, F_{out}: G \\to \\mathbb{R}$. The extension to multi-channel functions $F: G \\to \\mathbb{R}^C$ is straightforward.\n\nThe operator $\\Phi$ must satisfy three properties:\n1.  **Linearity**: $\\Phi(a F_1 + b F_2) = a \\Phi(F_1) + b \\Phi(F_2)$ for scalars $a, b \\in \\mathbb{R}$ and functions $F_1, F_2$.\n2.  **Equivariance**: $\\Phi$ must commute with the group action. The action on functions on $G$ is the left-regular action, $(L_u F)(g) = F(u^{-1}g)$. Thus, for any $u \\in G$, we must have $\\Phi(L_u F_{in}) = L_u (\\Phi F_{in})$.\n3.  **Stationarity (Weight Sharing)**: This property is a consequence of linearity and equivariance, as the derivation will show.\n\nLet's derive the form of $\\Phi$ from these principles.\nAny function $F_{in}: G \\to \\mathbb{R}$ can be expressed as a linear combination of canonical basis functions. For a finite group, a convenient basis is the set of delta functions $\\{\\delta_h\\}_{h \\in G}$, where $\\delta_h(g) = 1$ if $g=h$ and $0$ otherwise.\nWe can write $F_{in}$ as:\n$$F_{in}(g) = \\sum_{h \\in G} F_{in}(h) \\delta_h(g)$$\nBy linearity, the action of $\\Phi$ on $F_{in}$ is:\n$$(\\Phi F_{in})(g) = \\Phi \\left( \\sum_{h \\in G} F_{in}(h) \\delta_h \\right)(g) = \\sum_{h \\in G} F_{in}(h) (\\Phi \\delta_h)(g)$$\nNow, we use the equivariance property. The delta function at an arbitrary element $h$ can be obtained by applying the left-regular action to the delta function at the identity element $e$: $\\delta_h = L_h \\delta_e$, since $(L_h \\delta_e)(g) = \\delta_e(h^{-1}g) = 1$ if and only if $h^{-1}g=e$, i.e., $g=h$.\n\nUsing the equivariance of $\\Phi$, we have:\n$$(\\Phi \\delta_h)(g) = (\\Phi (L_h \\delta_e))(g) = (L_h (\\Phi \\delta_e))(g)$$\nLet's define a function $\\kappa: G \\to \\mathbb{R}$ as the response of the operator to an impulse at the identity, known as the convolution kernel: $\\kappa \\equiv \\Phi \\delta_e$.\nThen, $(\\Phi \\delta_h)(g) = (L_h \\kappa)(g)$. By definition of the left-regular action, $(L_h \\kappa)(g) = \\kappa(h^{-1}g)$.\nSubstituting this back into the expression for $(\\Phi F_{in})(g)$:\n$$(\\Phi F_{in})(g) = \\sum_{h \\in G} F_{in}(h) \\kappa(h^{-1}g)$$\nThis is the explicit form of the group convolution, often denoted as $(F_{in} * \\kappa)(g)$. The operator is uniquely determined by the kernel $\\kappa$. For the given group $G=D_8$, the sum is over the $16$ elements of $D_8$.\n\nThis form is guaranteed to be equivariant by its construction. The structure $\\kappa(h^{-1}g)$ ensures that if the input function $F_{in}$ is \"shifted\" on the group by an element $u$ (i.e., replaced by $L_u F_{in}$), the output function is also shifted by the same element $u$. This fulfills the requirement $\\Phi(L_u F_{in}) = L_u(\\Phi F_{in})$. The condition of stationarity is embodied in the fact that the kernel $\\kappa$ is a single function applied across the entire group domain, only depending on the relative group \"distance\" $h^{-1}g$ between the output position $g$ and input position $h$.\n\nFor multi-channel functions $F_{in}: G \\to \\mathbb{R}^{C_{in}}$ and $F_{out}: G \\to \\mathbb{R}^{C_{out}}$, the kernel becomes a tensor $\\kappa: G \\to \\mathbb{R}^{C_{out} \\times C_{in}}$, and the convolution is:\n$$(F_{out})_j(g) = \\sum_{i=1}^{C_{in}} \\sum_{h \\in D_8} \\kappa_{ji}(h^{-1}g) (F_{in})_i(h)$$\n\n### Task 2: Derivation of the Lifting Convolution\n\nThe first layer of a G-CNN is a \"lifting\" layer, which maps a function on a base space (here, $\\mathbb{Z}^2$) to a function on the group $D_8$ at each spatial location. Let the operator be $\\mathcal{L}$.\n- Input: $f: \\mathbb{Z}^2 \\to \\mathbb{R}^{C_0}$. The group action on the input is $(T_u f)_i(x) = f_i(u^{-1} \\cdot x)$.\n- Output: A feature map $F$ on the product space $\\mathbb{Z}^2 \\times D_8$. We write $F(x,g)$ for the feature at spatial position $x \\in \\mathbb{Z}^2$ and group orientation $g \\in D_8$. The map is $F: \\mathbb{Z}^2 \\times D_8 \\to \\mathbb{R}^{C_1}$. The group action on the output space is the induced representation action $(\\Pi_u F)(x,g) = F(u^{-1} \\cdot x, u^{-1}g)$.\n\nThe equivariance condition for the lifting layer $\\mathcal{L}$ is $\\mathcal{L}(T_u f) = \\Pi_u(\\mathcal{L}f)$ for all $u \\in D_8$.\n\nSimilar to a standard CNN, we assume the layer is a spatial convolution (more precisely, cross-correlation). The key insight of G-CNNs is that the convolution filters for different orientations $g \\in D_8$ are not independent but are generated by transforming a single \"mother\" kernel $\\kappa$ by the group action.\n\nLet $\\kappa: \\mathbb{Z}^2 \\to \\mathbb{R}^{C_1 \\times C_0}$ be the learnable kernel. The lifting convolution operator $\\mathcal{L}$ is defined as follows, for each output base channel $j \\in \\{1, \\dots, C_1\\}$:\n$$F_j(x, g) = (\\mathcal{L}f)_j(x,g) = \\sum_{i=1}^{C_0} \\sum_{y \\in \\mathbb{Z}^2} f_i(x+y) \\kappa_{ji}(g^{-1} \\cdot y)$$\nHere, $g^{-1} \\cdot y$ is the action of $g^{-1} \\in D_8$ on the coordinate vector $y \\in \\mathbb{Z}^2$. The formula uses the cross-correlation convention common in deep learning.\n\nWe verify that this operator is equivariant.\nLet's compute the left-hand side (LHS) of the equivariance equation, $(\\mathcal{L}(T_u f))_j(x,g)$:\n$$(\\mathcal{L}(T_u f))_j(x,g) = \\sum_{i=1}^{C_0} \\sum_{y \\in \\mathbb{Z}^2} (T_u f_i)(x+y) \\kappa_{ji}(g^{-1} \\cdot y)$$\n$$= \\sum_{i=1}^{C_0} \\sum_{y \\in \\mathbb{Z}^2} f_i(u^{-1} \\cdot (x+y)) \\kappa_{ji}(g^{-1} \\cdot y)$$\nSince the group action on $\\mathbb{Z}^2$ is linear, $u^{-1} \\cdot (x+y) = u^{-1} \\cdot x + u^{-1} \\cdot y$.\n$$LHS = \\sum_{i=1}^{C_0} \\sum_{y \\in \\mathbb{Z}^2} f_i(u^{-1} \\cdot x + u^{-1} \\cdot y) \\kappa_{ji}(g^{-1} \\cdot y)$$\nLet's perform a change of variables in the summation: $y' = u^{-1} \\cdot y$, which means $y = u \\cdot y'$. The summation domain $\\mathbb{Z}^2$ is invariant under this transformation.\n$$LHS = \\sum_{i=1}^{C_0} \\sum_{y' \\in \\mathbb{Z}^2} f_i(u^{-1} \\cdot x + y') \\kappa_{ji}(g^{-1} \\cdot (u \\cdot y'))$$\nUsing the group property $(ab)\\cdot z = a \\cdot (b \\cdot z)$, we get $g^{-1} \\cdot (u \\cdot y') = (g^{-1}u) \\cdot y'$.\n$$LHS = \\sum_{i=1}^{C_0} \\sum_{y' \\in \\mathbb{Z}^2} f_i(u^{-1} \\cdot x + y') \\kappa_{ji}((g^{-1}u) \\cdot y')$$\n\nNow let's compute the right-hand side (RHS), $(\\Pi_u(\\mathcal{L}f))_j(x,g)$:\n$$(\\Pi_u(\\mathcal{L}f))_j(x,g) = (\\mathcal{L}f)_j(u^{-1} \\cdot x, u^{-1}g)$$\nUsing the definition of our operator with inputs $(u^{-1}\\cdot x, u^{-1}g)$:\n$$RHS = \\sum_{i=1}^{C_0} \\sum_{y \\in \\mathbb{Z}^2} f_i((u^{-1} \\cdot x) + y) \\kappa_{ji}((u^{-1}g)^{-1} \\cdot y)$$\nThe inverse of $u^{-1}g$ is $g^{-1}u$.\n$$RHS = \\sum_{i=1}^{C_0} \\sum_{y \\in \\mathbb{Z}^2} f_i(u^{-1} \\cdot x + y) \\kappa_{ji}((g^{-1}u) \\cdot y)$$\nComparing the final expressions for the LHS (with summation variable $y'$) and RHS (with summation variable $y$), we see they are identical. Thus, the defined lifting convolution is indeed equivariant with respect to the action of $D_8$.\n\n### Task 3: Calculation of Orientation Channels\n\nThe lifting layer takes an input feature map $f: \\mathbb{Z}^2 \\to \\mathbb{R}^{C_0}$ and produces an output feature map $F: \\mathbb{Z}^2 \\times D_8 \\to \\mathbb{R}^{C_1}$. The problem states that the base channel count for this layer is $C_0=12$. In the context of neural network layer specification, this is most reasonably interpreted as the number of output base channels, so we set $C_1=12$.\n\nThe output feature map $F$ has values $F(x,g)$ defined for each spatial location $x \\in \\mathbb{Z}^2$ and for each group element $g \\in D_8$. At a fixed spatial location $x$, the feature is a function from $D_8$ to $\\mathbb{R}^{C_1}$. This function can be viewed as a single vector in a space of dimension $C_1 \\times |D_8|$.\n\nThe \"orientation channels\" are the channels indexed by the elements of the group $D_8$. The problem asks for the *total* number of orientation channels per spatial location. This is the full dimension of the feature vector at a point $x$, excluding the spatial dimensions.\n\nThe group is $D_8$, the dihedral group of order $16$. So, $|D_8| = 16$.\nThe number of output base channels is specified as $C_1 = 12$.\n\nFor each of the $C_1$ base channels, the lifting process creates $|D_8|$ orientation-specific channels. Therefore, the total number of channels at each spatial location is the product of the number of output base channels and the number of elements in the group.\n\nTotal channels per spatial location = $C_1 \\times |D_8|$.\nSubstituting the given values:\nTotal channels = $12 \\times 16$.\nTotal channels = $192$.\n\nThis means that for each point in the $2$D grid, the lifting layer outputs a vector of $192$ feature values, which are structured as $12$ feature vectors on the group $D_8$, each of dimension $16$.",
            "answer": "$$\\boxed{192}$$"
        },
        {
            "introduction": "Building an equivariant layer is only half the battle; the network's final output must also respect the symmetries of the task. This problem moves from mechanics to design, asking you to architect the output head of a G-CNN for a chirality classification task, where rotations preserve the class (left- or right-handed) but reflections invert it. This will teach you the critical principle that the representation $\\rho(g)$ used in the final layer must match how the labels themselves transform under the group action, ensuring the model's predictions are consistent with the problem's physical symmetries .",
            "id": "3133472",
            "problem": "A practitioner designs a Group Convolutional Neural Network (G-CNN) to process planar images of patterns with chirality (left-handed versus right-handed). The symmetry group is the dihedral group $D_n$, generated by a rotation $r$ of order $n$ and a reflection $s$, with relations $r^n = e$, $s^2 = e$, and $s r s = r^{-1}$. The network is built from group-convolution layers that are equivariant to $D_n$, in the sense that for any layer implementing a map $f$ on feature fields $x$, and any $g \\in D_n$, one has $f(T_g x) = \\rho(g) f(x)$, where $T_g$ is the action of $g$ on inputs and $\\rho(g)$ is a linear representation determining how the layer’s output transforms.\n\nThe task is binary classification of chirality. Empirically and by symmetry, rotations $r^k$ ($k \\in \\{0,1,\\dots,n-1\\}$) do not change chirality, but reflections $s r^k$ invert chirality. At test time, inputs may be rotated or reflected relative to the training set, and the classifier must respond consistently: for rotations it should keep the predicted class unchanged, whereas for reflections it should swap left/right predictions.\n\nStarting from the definition of group equivariance $f(T_g x) = \\rho(g) f(x)$ and the notion that labels should transform according to a representation $\\pi(g)$ consistent with the physical symmetry of the task (rotations preserve chirality; reflections invert chirality), choose the design that correctly enforces the required behavior at the classification head. Assume standard cross-entropy training for class probabilities or mean-squared error for signed targets when applicable, and that the earlier layers of the G-CNN are $D_n$-equivariant.\n\nWhich of the following output-head designs correctly encode the parity behavior and yield the desired rotation invariance and reflection-induced class inversion at test time?\n\nA. Use a $D_n$-equivariant head with a trivial output representation on two logits, i.e., $\\rho(g) = I_2$ for all $g \\in D_n$, and apply a softmax over the two logits to get class probabilities.\n\nB. Use a $D_n$-equivariant head with a two-dimensional output where rotations act trivially and reflections swap the components: $\\rho(r^k) = I_2$ and $\\rho(s r^k) = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$. Apply a softmax over the two components to obtain class probabilities for left versus right.\n\nC. Drop reflections and use only a cyclic group $C_n$-equivariant head (rotations only). Train with data augmentation including reflections relabeled to the opposite class, relying on the network to learn the flip without an explicit representation-level constraint.\n\nD. Use a $D_n$-equivariant head with a single scalar output $y \\in \\mathbb{R}$ trained to regress a signed target $t \\in \\{-1,+1\\}$, with $\\rho(r^k) = 1$ and $\\rho(s r^k) = -1$. At test time, predict the class by the decision rule $\\hat{c} = \\mathbf{1}[y > 0]$.\n\nSelect all that apply.",
            "solution": "The problem statement is subjected to validation before proceeding with a solution.\n\n### Step 1: Extract Givens\n- **Symmetry Group**: The dihedral group $D_n$ generated by a rotation $r$ and a reflection $s$.\n- **Group Relations**: $r^n = e$, $s^2 = e$, and $s r s = r^{-1}$.\n- **Network Architecture**: A Group Convolutional Neural Network (G-CNN) with layers that are $D_n$-equivariant.\n- **Equivariance Definition**: For a layer map $f$ on feature fields $x$, and for any group element $g \\in D_n$, the equivariance relation is $f(T_g x) = \\rho(g) f(x)$. Here, $T_g$ is the action of $g$ on the input field, and $\\rho(g)$ is a linear representation that defines the transformation of the output field.\n- **Task**: Binary classification of chirality (left-handed vs. right-handed).\n- **Symmetry of Chirality**:\n    - Rotations ($g=r^k$ for $k \\in \\{0, 1, \\dots, n-1\\}$) preserve chirality.\n    - Reflections ($g=s r^k$) invert chirality.\n- **Required Classifier Behavior**:\n    - Under rotations, the predicted class must remain unchanged (invariance).\n    - Under reflections, the predicted class must be inverted (e.g., left $\\to$ right).\n- **Assumptions**: The G-CNN's earlier layers are $D_n$-equivariant. Training uses standard methods like cross-entropy or mean-squared error.\n- **Objective**: Select the output-head design(s) that correctly enforce the required symmetry behavior.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated for validity.\n- **Scientifically Grounded**: The problem is based on the well-established mathematical framework of group theory and its application in deep learning through Group-equivariant CNNs. The properties of the dihedral group $D_n$ and the geometric concept of chirality are correctly stated and used. This is a standard problem in the field of geometric deep learning. The problem is scientifically sound.\n- **Well-Posed**: The problem is clearly defined. It specifies the group, the task, the required symmetries of the output, and the definition of equivariance. The question asks for a specific design choice that satisfies these constraints. A solution can be derived using the principles of group representation theory. The problem is well-posed.\n- **Objective**: The problem uses precise, technical language (e.g., \"$D_n$-equivariant,\" \"linear representation\") and is free from subjective or ambiguous terminology. The problem is objective.\n\nNo flaws are identified from the checklist (e.g., no scientific unsoundness, incompleteness, or ambiguity).\n\n### Step 3: Verdict and Action\nThe problem statement is valid. A full solution will be derived.\n\n### Principle-Based Derivation\nThe core principle of designing an equivariant network for a specific task is to ensure that the representation of the network's output, $\\rho(g)$, matches the representation of the task's labels, let's call it $\\pi(g)$. The network head is a map $f_{\\text{head}}$. If its input $x_{\\text{penultimate}}$ is transformed by $T_g$, its output $y = f_{\\text{head}}(x_{\\text{penultimate}})$ is transformed as $y \\mapsto \\rho(g) y$. The final classification decision, $C(y)$, must then exhibit the required symmetry.\n\nLet's formalize the required symmetry for the labels, $\\pi(g)$:\n$1$. **Rotations**: A rotation $g = r^k$ preserves chirality. This means the classification decision must be invariant.\n$2$. **Reflections**: A reflection $g = sr^k$ inverts chirality. This means the classification decision must be flipped.\n\nWe must now examine each proposed design to see if its output representation $\\rho(g)$ and subsequent decision rule produce this behavior.\n\n### Option-by-Option Analysis\n\n**A. Use a $D_n$-equivariant head with a trivial output representation on two logits, i.e., $\\rho(g) = I_2$ for all $g \\in D_n$, and apply a softmax over the two logits to get class probabilities.**\n\n- **Analysis**: The output of the head is a $2$-dimensional vector of logits, $y \\in \\mathbb{R}^2$. The equivariance property implies that for an input image $x_{in}$, if it is transformed by any $g \\in D_n$, the output becomes $y(T_g x_{in}) = \\rho(g) y(x_{in})$. According to this option, $\\rho(g) = I_2$ (the $2 \\times 2$ identity matrix) for all $g$. Thus, $y(T_g x_{in}) = I_2 y(x_{in}) = y(x_{in})$.\n- The output logits are invariant under all group actions, including both rotations and reflections. A softmax function applied to an invariant vector will produce invariant probabilities. Therefore, the final classification will be invariant to both rotations and reflections. While rotation invariance is desired, invariance to reflections contradicts the requirement that reflections must invert chirality. The model is architecturally constrained to ignore reflections, which is incorrect for this task.\n- **Verdict**: Incorrect.\n\n**B. Use a $D_n$-equivariant head with a two-dimensional output where rotations act trivially and reflections swap the components: $\\rho(r^k) = I_2$ and $\\rho(s r^k) = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$. Apply a softmax over the two components to obtain class probabilities for left versus right.**\n\n- **Analysis**: The output is a $2$-dimensional vector of logits, $y = [y_1, y_2]^T$. Let's check the behavior under the two types of transformations.\n- **Rotations ($g=r^k$)**: The output transforms as $y(T_{r^k} x_{in}) = \\rho(r^k) y(x_{in}) = I_2 y(x_{in}) = y(x_{in})$. The logits are invariant. The subsequent softmax probabilities and the final classification are also invariant. This matches the requirement for rotations.\n- **Reflections ($g=sr^k$)**: The output transforms as $y(T_{sr^k} x_{in}) = \\rho(sr^k) y(x_{in}) = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} y_2 \\\\ y_1 \\end{pmatrix}$. The two logits are swapped. Let the class probabilities be $p_1 = e^{y_1} / (e^{y_1} + e^{y_2})$ and $p_2 = e^{y_2} / (e^{y_1} + e^{y_2})$. For the transformed input, the new logits are $[y_2, y_1]^T$, leading to new probabilities $p'_1 = e^{y_2} / (e^{y_2} + e^{y_1}) = p_2$ and $p'_2 = e^{y_1} / (e^{y_2} + e^{y_1}) = p_1$. The probabilities for the two classes are swapped, which correctly implements the inversion of chirality.\n- **Representation Validity**: The proposed map $\\rho$ is a valid representation of $D_n$. It correctly maps $r$ to the identity and $s$ to the permutation matrix, satisfying the group relations.\n- **Verdict**: Correct.\n\n**C. Drop reflections and use only a cyclic group $C_n$-equivariant head (rotations only). Train with data augmentation including reflections relabeled to the opposite class, relying on the network to learn the flip without an explicit representation-level constraint.**\n\n- **Analysis**: This option proposes to enforce rotation equivariance architecturally but leave reflection behavior to be learned from data augmentation. While data augmentation is a powerful and common technique in deep learning, it does not *enforce* or *guarantee* the exact symmetry. The network learns an approximation of the reflection rule from finite data, and there is no architectural constraint that ensures $f(T_s x)$ is perfectly related to $f(x)$ in the required way. The problem asks for a design that *correctly enforces the required behavior*. This option replaces architectural enforcement with empirical learning, which is a different design philosophy and does not provide the same guarantee as an equivariant design for the full $D_n$ group.\n- **Verdict**: Incorrect.\n\n**D. Use a $D_n$-equivariant head with a single scalar output $y \\in \\mathbb{R}$ trained to regress a signed target $t \\in \\{-1,+1\\}$, with $\\rho(r^k) = 1$ and $\\rho(s r^k) = -1$. At test time, predict the class by the decision rule $\\hat{c} = \\mathbf{1}[y > 0]$.**\n\n- **Analysis**: The output is a single scalar $y \\in \\mathbb{R}$. This corresponds to a $1$-dimensional representation of $D_n$. Let's check the behavior.\n- **Rotations ($g=r^k$)**: The output transforms as $y(T_{r^k} x_{in}) = \\rho(r^k) y(x_{in}) = (1) \\cdot y(x_{in}) = y(x_{in})$. The scalar output is invariant. The decision based on its sign, $\\mathbf{1}[y > 0]$, is also invariant. This matches the requirement for rotations.\n- **Reflections ($g=sr^k$)**: The output transforms as $y(T_{sr^k} x_{in}) = \\rho(sr^k) y(x_{in}) = (-1) \\cdot y(x_{in}) = -y(x_{in})$. The scalar output is negated. If the original prediction was one class (e.g., $y > 0$), the new output $-y$ will be negative ($-y < 0$), leading to the other class prediction. If the original prediction was $y < 0$, the new output $-y$ will be positive $-y > 0$, again flipping the class. This correctly implements the inversion of chirality.\n- **Representation Validity**: The map $\\rho(r^k)=1$ and $\\rho(sr^k)=-1$ is the well-known sign (or alternating) representation of $D_n$. It is a valid, $1$-dimensional, irreducible representation that correctly reflects the parity of group elements (even for rotations, odd for reflections).\n- **Verdict**: Correct.",
            "answer": "$$\\boxed{BD}$$"
        }
    ]
}