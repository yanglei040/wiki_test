{
    "hands_on_practices": [
        {
            "introduction": "胶囊网络的核心优势在于其“等变性” (equivariance)，这与传统卷积网络 (CNNs) 的“不变性” (invariance) 形成鲜明对比。本练习将通过一个精心设计的数值实验，让您亲手验证并量化这一关键差异。您将实现一个简化的动态路由过程，并观察当输入发生旋转时，胶囊网络的姿态向量是如何相应旋转的，而CNN的表征则不然，从而直观地理解胶囊网络为何能更好地保留物体的空间姿态信息 。",
            "id": "3104851",
            "problem": "您被要求实现一个自包含的数值实验，该实验使用一个模拟仿射变换对姿态影响的合成设置，来对比胶囊网络（Capsule Networks, CapsNets）的等变性（equivariance）与卷积神经网络（Convolutional Neural Networks, CNNs）的不变性（invariance）。该实验必须完全在向量空间中通过显式的矩阵和向量进行，不加载任何外部数据集。您将实现一个具有动态路由的单层高级胶囊和一个简单的类 CNN 基线模型。目标是测量和比较在旋转变换下的姿态向量对齐误差，并分析该误差如何依赖于动态路由的迭代次数。\n\n使用的基本概念和定义：\n- 二维旋转变换由矩阵表示\n$$\nT(\\theta) \\;=\\; \\begin{bmatrix}\n\\cos(\\theta)  -\\sin(\\theta)\\\\\n\\sin(\\theta)  \\cos(\\theta)\n\\end{bmatrix},\n$$\n该矩阵属于特殊正交群 $SO(2)$ 并且保持向量范数。\n- 胶囊之间的动态路由过程使用耦合系数，这些系数是通过对路由对数（routing logits）应用 softmax 函数计算得出的，并使用了 \"squash\" 非线性函数。对于一组“预测”向量 $\\{ \\mathbf{u}_i \\in \\mathbb{R}^2 \\}_{i=1}^m$ 和路由对数 $\\{ b_i \\in \\mathbb{R} \\}_{i=1}^m$，耦合系数为\n$$\nc_i \\;=\\; \\frac{\\exp(b_i)}{\\sum_{k=1}^{m} \\exp(b_k)},\n$$\n预激活值为\n$$\n\\mathbf{s} \\;=\\; \\sum_{i=1}^{m} c_i \\, \\mathbf{u}_i.\n$$\n\"squash\" 非线性函数将 $\\mathbf{s}$ 映射到一个输出姿态向量 $\\mathbf{v} \\in \\mathbb{R}^2$：\n$$\n\\mathbf{v} \\;=\\; \\frac{\\lVert \\mathbf{s} \\rVert^2}{1 + \\lVert \\mathbf{s} \\rVert^2} \\cdot \\frac{\\mathbf{s}}{\\lVert \\mathbf{s} \\rVert},\n$$\n约定如果 $\\lVert \\mathbf{s} \\rVert = 0$，则 $\\mathbf{v} = \\mathbf{0}$。\n- 路由对数通过“一致性（agreement）”进行更新，迭代指定的次数 $r \\in \\mathbb{N}_0$（非负整数）。“一致性”使用每个预测向量与当前输出向量的点积来计算：\n$$\nb_i \\leftarrow b_i + \\mathbf{u}_i \\cdot \\mathbf{v}.\n$$\n- 表征 $\\mathbf{v}$ 相对于 $T(\\theta)$ 的等变性意味着，如果输入通过 $T(\\theta)$ 进行变换，那么输出姿态也通过相同的 $T(\\theta)$ 进行变换。用公式表示，如果 $\\mathbf{v}(\\theta)$ 是变换后输入的输出姿态，则等变性意味着 $\\mathbf{v}(\\theta) \\approx T(\\theta)\\,\\mathbf{v}(0)$。\n- 我们将两个非零向量 $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^2$ 之间的姿态向量对齐误差定义为它们之间的夹角（以弧度为单位）：\n$$\n\\operatorname{ang}(\\mathbf{a}, \\mathbf{b}) \\;=\\; \\arccos\\!\\left( \\frac{\\mathbf{a}^\\top \\mathbf{b}}{\\lVert \\mathbf{a} \\rVert \\,\\lVert \\mathbf{b} \\rVert} \\right).\n$$\n\n合成设置：\n- 设“真实”的规范姿态为 $\\mathbf{p}_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$。\n- 对于任意旋转角度 $\\theta \\in \\mathbb{R}$（以弧度为单位），变换后的姿态为 $\\mathbf{p}(\\theta) = T(\\theta)\\,\\mathbf{p}_0$。\n- 有 $m = 5$ 个初级胶囊。每个胶囊产生一个二维的“投票”向量，该向量是对变换后姿态进行缩放且有轻微失准的观测：\n$$\n\\mathbf{u}_i(\\theta) \\;=\\; s_i \\, R(\\varphi_i)\\, \\mathbf{p}(\\theta),\n$$\n其中 $R(\\varphi)$ 是旋转矩阵 $T(\\varphi)$，常数如下：\n$$\n[s_1, s_2, s_3, s_4, s_5] \\;=\\; [1.2, 0.9, 1.1, 0.8, 1.0],\n$$\n$$\n[\\varphi_1, \\varphi_2, \\varphi_3, \\varphi_4, \\varphi_5] \\;=\\; [-0.2, 0.1, 0.0, 0.15, -0.1].\n$$\n- 高级胶囊使用恒等变换（即，从预测到输出的变换是 $\\mathbb{R}^2$ 中的恒等变换），因此 $\\mathbf{u}_i(\\theta)$ 直接参与路由过程。\n- 类卷积神经网络（CNN）基线模型通过仅聚合幅度来丢弃姿态方向。将其对角度 $\\theta$ 的表征定义为二维向量：\n$$\n\\mathbf{v}_{\\mathrm{cnn}}(\\theta) \\;=\\; \\begin{bmatrix} g(\\theta) \\\\ 0 \\end{bmatrix}, \\quad \\text{其中} \\quad g(\\theta) \\;=\\; \\sum_{i=1}^{m} \\lVert \\mathbf{u}_i(\\theta) \\rVert.\n$$\n该基线模型在方向上特意设计为对 $\\theta$ 不变，因为它始终沿固定的 x 轴对齐。\n\n您的任务：\n1. 实现上述为单个高级胶囊描述的动态路由算法，该算法由路由迭代次数 $r \\in \\mathbb{N}_0$ 参数化。对所有 $i$，使用初始路由对数 $b_i = 0$。在每次迭代中，通过对 $\\{b_i\\}$ 应用 softmax 计算耦合系数 $c_i$，根据上述公式计算 $\\mathbf{s}$ 和 $\\mathbf{v}$，然后更新 $b_i \\leftarrow b_i + \\mathbf{u}_i(\\theta) \\cdot \\mathbf{v}$。对于给定的 $\\theta$ 和 $r$，输出姿态 $\\mathbf{v}_{\\mathrm{caps}}(\\theta; r)$ 是在最后一次迭代中计算出的 $\\mathbf{v}$。对于边界情况 $r = 0$，将 $\\mathbf{v}_{\\mathrm{caps}}(\\theta; 0)$ 定义为对均匀平均值应用 squash 函数的结果，即使用 $c_i = 1/m$ 而不进行任何更新。\n2. 对于类 CNN 基线模型，按上述定义计算 $\\mathbf{v}_{\\mathrm{cnn}}(\\theta)$。\n3. 对于一个输出为 $\\mathbf{v}(\\theta)$ 的模型，将其在角度 $\\theta$ 处的等变性对齐误差定义为\n$$\n\\varepsilon(\\theta) \\;=\\; \\operatorname{ang}\\!\\left( \\mathbf{v}(\\theta), \\; T(\\theta)\\,\\mathbf{v}(0) \\right),\n$$\n以弧度为单位测量。\n\n测试套件：\n在以下情况下评估误差对 $\\left[ \\varepsilon_{\\mathrm{caps}}(\\theta; r), \\; \\varepsilon_{\\mathrm{cnn}}(\\theta) \\right]$：\n- 情况 1：$\\theta = 0.0$, $r = 0$。\n- 情况 2：$\\theta = 0.5$, $r = 0$。\n- 情况 3：$\\theta = 0.5$, $r = 5$。\n- 情况 4：$\\theta = 1.0$, $r = 0$。\n- 情况 5：$\\theta = 1.0$, $r = 5$。\n- 情况 6：$\\theta = 1.0$, $r = 10$。\n\n角度单位：\n- 所有角度 $\\theta$、$\\varphi_i$ 和误差 $\\varepsilon$ 都必须以弧度为单位进行处理和报告。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含六种情况的结果，格式为一个包含六个内部列表的逗号分隔列表，每个内部列表包含两个浮点数，精确到小数点后六位，顺序如上所述。输出必须包含在一对单独的方括号中。例如，打印的结构必须类似于\n$$\n[ [a_1, b_1], [a_2, b_2], \\ldots, [a_6, b_6] ],\n$$\n但打印时逗号后不能有任何空格，以匹配确切要求\n$$\n[[a_1,b_1],[a_2,b_2],\\ldots,[a_6,b_6]].\n$$",
            "solution": "该问题陈述经评估具有科学依据、良态、客观且内部一致。它提供了一个清晰且可形式化的数值实验，用以对比胶囊网络（CapsNets）和简化的卷积神经网络（CNN）基线模型在仿射变换（特别是 $SO(2)$ 旋转）下的性质。所有必要的常数、定义和流程都已明确提供。因此，我们可以着手给出一个完整的解决方案。\n\n这个问题的核心在于理解和量化等变性。如果对于一个变换群 $G$ 中的任何变换 $T$，先将变换应用于输入再通过函数 $f$ 处理，与先将原始输入通过函数处理再将变换应用于输出得到的结果相同，则称函数 $f$ 对该变换群 $G$ 是等变的。在此背景下，我们研究的是相对于旋转群 $SO(2)$ 的等变性，其条件为 $\\mathbf{v}(\\theta) = T(\\theta)\\mathbf{v}(0)$，其中 $\\mathbf{v}(\\theta)$ 是模型对于旋转了 $\\theta$ 角度的输入的输出。与此理想情况的偏差由等变性对齐误差 $\\varepsilon(\\theta) = \\operatorname{ang}(\\mathbf{v}(\\theta), T(\\theta)\\mathbf{v}(0))$ 来衡量。\n\n我们将首先分析 CapsNet 和 CNN 模型在问题设定的合成环境下的理论行为。然后，我们将实现数值计算来验证此分析。\n\n### 理论分析\n\n**1. 类 CNN 基线模型**\n\n类 CNN 基线模型计算一个表征 $\\mathbf{v}_{\\mathrm{cnn}}(\\theta) = [g(\\theta), 0]^\\top$，其中 $g(\\theta) = \\sum_{i=1}^{m} \\lVert \\mathbf{u}_i(\\theta) \\rVert$。预测向量定义为 $\\mathbf{u}_i(\\theta) = s_i R(\\varphi_i) \\mathbf{p}(\\theta)$，其中 $\\mathbf{p}(\\theta) = T(\\theta)\\mathbf{p}_0$。这里，$s_i$ 是标量常数，$R(\\varphi_i)$ 和 $T(\\theta)$ 是来自 $SO(2)$ 的旋转矩阵。由于旋转是正交变换，它们保持向量范数。\n预测向量的范数是：\n$$\n\\lVert \\mathbf{u}_i(\\theta) \\rVert = \\lVert s_i R(\\varphi_i) T(\\theta) \\mathbf{p}_0 \\rVert = |s_i| \\cdot \\lVert R(\\varphi_i) \\rVert \\cdot \\lVert T(\\theta) \\rVert \\cdot \\lVert \\mathbf{p}_0 \\rVert\n$$\n鉴于对于任何旋转矩阵 $A$，都有 $\\lVert A\\mathbf{x} \\rVert = \\lVert\\mathbf{x}\\rVert$，并且所有 $s_i > 0$，上式可简化为：\n$$\n\\lVert \\mathbf{u}_i(\\theta) \\rVert = s_i \\lVert \\mathbf{p}_0 \\rVert\n$$\n这个结果与旋转角度 $\\theta$ 无关。因此，聚合的幅度 $g(\\theta)$ 也是一个与 $\\theta$ 无关的常数：\n$$\ng(\\theta) = \\sum_{i=1}^{m} s_i \\lVert \\mathbf{p}_0 \\rVert = \\left( \\sum_{i=1}^{m} s_i \\right) \\lVert \\mathbf{p}_0 \\rVert = G\n$$\n因此，对于任何 $\\theta$，CNN 的输出都是 $\\mathbf{v}_{\\mathrm{cnn}}(\\theta) = [G, 0]^\\top$。这个模型是完全不变的；其输出从不改变。\n\n为了计算等变性误差 $\\varepsilon_{\\mathrm{cnn}}(\\theta)$，我们需要 $\\mathbf{v}_{\\mathrm{cnn}}(0) = [G, 0]^\\top$。目标等变向量是 $T(\\theta)\\mathbf{v}_{\\mathrm{cnn}}(0)$：\n$$\nT(\\theta)\\mathbf{v}_{\\mathrm{cnn}}(0) = \\begin{bmatrix} \\cos(\\theta)  -\\sin(\\theta) \\\\ \\sin(\\theta)  \\cos(\\theta) \\end{bmatrix} \\begin{bmatrix} G \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} G\\cos(\\theta) \\\\ G\\sin(\\theta) \\end{bmatrix}\n$$\n误差是实际输出 $\\mathbf{v}_{\\mathrm{cnn}}(\\theta) = [G, 0]^\\top$ 与目标向量之间的夹角。\n$$\n\\varepsilon_{\\mathrm{cnn}}(\\theta) = \\operatorname{ang}\\left( \\begin{bmatrix} G \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} G\\cos(\\theta) \\\\ G\\sin(\\theta) \\end{bmatrix} \\right) = \\arccos\\left( \\frac{[G, 0] \\cdot [G\\cos(\\theta), G\\sin(\\theta)]}{G \\cdot G} \\right) = \\arccos(\\cos(\\theta))\n$$\n对于主值范围 $[0, \\pi]$ 内的 $\\theta$（所有测试用例都在此范围内），这简化为 $\\varepsilon_{\\mathrm{cnn}}(\\theta) = \\theta$。CNN 基线模型未能旋转其输出，导致误差等于旋转角度，这表明它完全缺乏等变性。\n\n**2. 胶囊网络模型**\n\nCapsNet 模型的行为有根本性的不同。让我们来分析预测向量 $\\mathbf{u}_i(\\theta)$：\n$$\n\\mathbf{u}_i(\\theta) = s_i R(\\varphi_i) \\mathbf{p}(\\theta) = s_i T(\\varphi_i) T(\\theta) \\mathbf{p}_0\n$$\n由于 $SO(2)$ 中的旋转是可交换的，即 $T(\\varphi_i)T(\\theta) = T(\\theta)T(\\varphi_i)$。因此：\n$$\n\\mathbf{u}_i(\\theta) = s_i T(\\theta) T(\\varphi_i) \\mathbf{p}_0\n$$\n在 $\\theta=0$ 时的预测向量是 $\\mathbf{u}_i(0) = s_i T(\\varphi_i) \\mathbf{p}_0$。通过代入，我们发现一个直接关系：\n$$\n\\mathbf{u}_i(\\theta) = T(\\theta) \\mathbf{u}_i(0)\n$$\n每个单独的预测向量都相对于全局旋转 $\\theta$ 是完美等变的。\n\n现在我们分析动态路由过程。设模型在角度 $\\theta$、经过 $k$ 次路由迭代后的状态（向量 $\\mathbf{s}, \\mathbf{v}$ 和标量 $b_i, c_i$）用上标 $(\\theta, k)$ 表示。\n\n**基础情况 ($r=0$):**\n输出是使用均匀耦合系数 $c_i = 1/m$ 计算的。预激活向量 $\\mathbf{s}^{(\\theta, 0)}$ 是：\n$$\n\\mathbf{s}^{(\\theta, 0)} = \\sum_{i=1}^{m} \\frac{1}{m} \\mathbf{u}_i(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} T(\\theta) \\mathbf{u}_i(0) = T(\\theta) \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\mathbf{u}_i(0) \\right) = T(\\theta) \\mathbf{s}^{(0, 0)}\n$$\n预激活向量 $\\mathbf{s}$ 是完美等变的。现在我们应用 squash 函数。squash 函数的一个关键性质是它与旋转的相互作用。由于 $\\lVert T(\\theta)\\mathbf{s} \\rVert = \\lVert \\mathbf{s} \\rVert$，我们有：\n$$\n\\mathbf{v}^{(\\theta, 0)} = \\operatorname{squash}(\\mathbf{s}^{(\\theta, 0)}) = \\operatorname{squash}(T(\\theta)\\mathbf{s}^{(0, 0)}) = T(\\theta)\\operatorname{squash}(\\mathbf{s}^{(0, 0)}) = T(\\theta)\\mathbf{v}^{(0, 0)}\n$$\n这表明对于 $r=0$，胶囊网络的输出是完美等变的。因此误差 $\\varepsilon_{\\mathrm{caps}}(\\theta; 0)$ 必须为 $0$。\n\n**归纳步骤 ($r>0$):**\n假设经过 $k-1$ 次迭代后，输出是完美等变的：$\\mathbf{v}^{(\\theta, k-1)} = T(\\theta)\\mathbf{v}^{(0, k-1)}$。路由对数按 $b_i \\leftarrow b_i + \\mathbf{u}_i \\cdot \\mathbf{v}$ 进行更新。让我们检查更新项：\n$$\n\\mathbf{u}_i(\\theta) \\cdot \\mathbf{v}^{(\\theta, k-1)} = (T(\\theta)\\mathbf{u}_i(0)) \\cdot (T(\\theta)\\mathbf{v}^{(0, k-1)})\n$$\n点积在旋转下是不变的，即 $(Ra)\\cdot(Rb) = a \\cdot b$。因此：\n$$\n\\mathbf{u}_i(\\theta) \\cdot \\mathbf{v}^{(\\theta, k-1)} = \\mathbf{u}_i(0) \\cdot \\mathbf{v}^{(0, k-1)}\n$$\n路由对数的更新量与 $\\theta$ 无关。通过归纳法，如果初始路由对数对于所有 $\\theta$ 都相同（它们确实是，$b_i=0$），那么在任何迭代 $k$ 次后的路由对数 $b_i^{(\\theta, k)}$ 也与 $\\theta$ 无关：$b_i^{(\\theta, k)} = b_i^{(0, k)}$。\n这意味着耦合系数 $c_i^{(\\theta, k)}$ 也与 $\\theta$ 无关。\n下一次迭代的预激活值 $\\mathbf{s}^{(\\theta, k)}$ 是：\n$$\n\\mathbf{s}^{(\\theta, k)} = \\sum_{i=1}^{m} c_i^{(\\theta, k)} \\mathbf{u}_i(\\theta) = \\sum_{i=1}^{m} c_i^{(0, k)} T(\\theta)\\mathbf{u}_i(0) = T(\\theta) \\sum_{i=1}^{m} c_i^{(0, k)} \\mathbf{u}_i(0) = T(\\theta)\\mathbf{s}^{(0, k)}\n$$\n遵循与基础情况相同的逻辑，应用 squash 函数得到 $\\mathbf{v}^{(\\theta, k)} = T(\\theta)\\mathbf{v}^{(0, k)}$。\n归纳成立：在这个理想化的设置中，对于任何数量的路由迭代 $r$，胶囊网络的输出都是完美等变的。理论上，所有测试用例的误差 $\\varepsilon_{\\mathrm{caps}}(\\theta; r)$ 都为 $0$。对其与 $r$ 依赖关系的分析表明，不存在依赖关系；系统从一开始就是完美等变的。\n\n### 数值实现\n\n我们现在将按描述实现算法，以确认我们的理论分析。该实现将遵循两种模型的指定流程，并为给定的测试套件计算误差。我们预期 CapsNet 的误差为 $0$（或一个在机器精度数量级的值），而 CNN 的误差为 $\\theta$。\n\n对于每个测试用例 $(\\theta, r)$ 的总体流程将是：\n1. 计算 $\\mathbf{v}_{\\mathrm{caps}}(\\theta; r)$ 和 $\\mathbf{v}_{\\mathrm{cnn}}(\\theta)$。\n2. 计算参考向量 $\\mathbf{v}_{\\mathrm{caps}}(0; r)$ 和 $\\mathbf{v}_{\\mathrm{cnn}}(0)$。\n3. 计算目标向量 $T(\\theta)\\mathbf{v}_{\\mathrm{caps}}(0; r)$ 和 $T(\\theta)\\mathbf{v}_{\\mathrm{cnn}}(0)$。\n4. 计算实际向量与目标向量之间的夹角，以求得 $\\varepsilon_{\\mathrm{caps}}$ 和 $\\varepsilon_{\\mathrm{cnn}}$。\n\n所有计算将使用 Python 的 `numpy` 库以浮点算术执行。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a numerical experiment to contrast Capsule Network equivariance\n    with CNN invariance under 2D rotation.\n    \"\"\"\n\n    # --- Problem Constants ---\n    P0 = np.array([2.0, 1.0])\n    S_VALS = np.array([1.2, 0.9, 1.1, 0.8, 1.0])\n    PHI_VALS = np.array([-0.2, 0.1, 0.0, 0.15, -0.1])\n    M = 5\n\n    # --- Helper Functions ---\n    def rotation_matrix(theta: float) -> np.ndarray:\n        \"\"\"Computes the 2D rotation matrix T(theta).\"\"\"\n        c, s = np.cos(theta), np.sin(theta)\n        return np.array([[c, -s], [s, c]])\n\n    def squash(s: np.ndarray, epsilon: float = 1e-9) -> np.ndarray:\n        \"\"\"Applies the squashing non-linearity to a vector s.\"\"\"\n        s_norm_sq = np.sum(s**2)\n        s_norm = np.sqrt(s_norm_sq)\n        if s_norm < epsilon:\n            return np.zeros_like(s)\n        scale = s_norm_sq / (1.0 + s_norm_sq)\n        return scale * s / s_norm\n\n    def get_votes(theta: float) -> np.ndarray:\n        \"\"\"Generates the set of prediction vectors u_i(theta).\"\"\"\n        p_theta = rotation_matrix(theta) @ P0\n        votes = np.zeros((M, 2))\n        for i in range(M):\n            # u_i(theta) = s_i * R(phi_i) * p(theta)\n            votes[i, :] = S_VALS[i] * (rotation_matrix(PHI_VALS[i]) @ p_theta)\n        return votes\n\n    def angle_between(v1: np.ndarray, v2: np.ndarray, epsilon: float = 1e-9) -> float:\n        \"\"\"Calculates the angle in radians between two 2D vectors.\"\"\"\n        norm1 = np.linalg.norm(v1)\n        norm2 = np.linalg.norm(v2)\n        if norm1 < epsilon or norm2 < epsilon:\n            return 0.0\n        \n        dot_product = np.dot(v1, v2)\n        cos_angle = dot_product / (norm1 * norm2)\n        # Clip to handle potential floating point inaccuracies\n        cos_angle = np.clip(cos_angle, -1.0, 1.0)\n        return np.arccos(cos_angle)\n\n    # --- Model Implementations ---\n    def capsule_model(theta: float, r: int) -> np.ndarray:\n        \"\"\"\n        Computes the output pose vector v_caps(theta; r) using dynamic routing.\n        \"\"\"\n        votes = get_votes(theta)\n        \n        if r == 0:\n            # Uniform averaging, no routing iterations\n            s = np.mean(votes, axis=0)\n            return squash(s)\n\n        # Dynamic routing for r > 0\n        b = np.zeros(M)\n        v = np.zeros(2) # v is initialized implicitly in loop\n        for _ in range(r):\n            # Softmax to get coupling coefficients\n            c = np.exp(b) / np.sum(np.exp(b))\n            # Weighted sum of votes\n            s = np.sum(c[:, np.newaxis] * votes, axis=0)\n            # Squash to get output pose vector\n            v = squash(s)\n            # Update logits by agreement\n            agreement = np.dot(votes, v) # shape (M,)\n            b += agreement\n            \n        return v\n\n    def cnn_model(theta: float) -> np.ndarray:\n        \"\"\"Computes the output representation v_cnn(theta).\"\"\"\n        votes = get_votes(theta)\n        g_theta = np.sum(np.linalg.norm(votes, axis=1))\n        return np.array([g_theta, 0.0])\n\n    # --- Test Suite ---\n    test_cases = [\n        # (theta, r)\n        (0.0, 0),  # Case 1\n        (0.5, 0),  # Case 2\n        (0.5, 5),  # Case 3\n        (1.0, 0),  # Case 4\n        (1.0, 5),  # Case 5\n        (1.0, 10), # Case 6\n    ]\n\n    results = []\n    for theta, r in test_cases:\n        # 1. Calculate CapsNet error\n        v_caps_theta = capsule_model(theta, r)\n        v_caps_0 = capsule_model(0.0, r)\n        target_caps = rotation_matrix(theta) @ v_caps_0\n        eps_caps = angle_between(v_caps_theta, target_caps)\n\n        # 2. Calculate CNN error\n        v_cnn_theta = cnn_model(theta)\n        v_cnn_0 = cnn_model(0.0)\n        target_cnn = rotation_matrix(theta) @ v_cnn_0\n        eps_cnn = angle_between(v_cnn_theta, target_cnn)\n\n        results.append(f\"[{eps_caps:.6f},{eps_cnn:.6f}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "理论上完美的算法在实践中也可能遇到棘手的“边缘案例”。本练习将揭示动态路由算法中的一个典型困境：“对称性模糊” (symmetric ambiguity)，即当多个“部分”可以合理地分配给多个不同的“整体”时，路由过程可能会陷入僵局。通过这个练习，您将首先复现这一失效模式，然后亲手设计并实现两种“先验” (priors) 来打破对称性，从而引导路由过程做出正确的决策，这对于构建鲁棒的胶囊网络至关重要 。",
            "id": "3104796",
            "problem": "考虑一个简化的基于向量的胶囊网络（CapsNet）路由场景，该场景有两层：一组由 $i \\in \\{0,1\\}$ 索引的初级胶囊（部件），以及一组由 $j \\in \\{0,1\\}$ 索引的高级胶囊（整体）。每个初级胶囊 $i$ 为每个高级胶囊 $j$ 输出一个预测向量 $u_{\\hat{i}j} \\in \\mathbb{R}^d$。姿态维度为 $d = 2$。需要实现的路由算法是“协议路由”（routing-by-agreement）方案，由以下基本组件定义。\n\n- 令 $b_{ij}$ 为路由对数（routing logits）。根据您包含的任何先验来按需初始化 $b_{ij}$。在每次路由迭代中，通过对每个固定的 $i$ 在 $j$ 上应用 softmax 来计算耦合系数 $c_{ij}$，即 $c_{ij} = \\exp(b_{ij}) \\big/ \\sum_{j'} \\exp(b_{ij'})$。\n- 计算每个高级胶囊 $j$ 的预激活值 $s_j = \\sum_i c_{ij} u_{\\hat{i}j}$。\n- 应用向量胶囊的标准挤压（squash）非线性函数，通过以下公式生成 $v_j \\in \\mathbb{R}^d$：\n$$\nv_j = \\frac{\\lVert s_j \\rVert^2}{1 + \\lVert s_j \\rVert^2} \\cdot \\frac{s_j}{\\lVert s_j \\rVert},\n$$\n需要注意的是，数值稳定的实现必须处理 $\\lVert s_j \\rVert = 0$ 的情况。\n- 通过协议更新路由对数：$b_{ij} \\leftarrow b_{ij} + u_{\\hat{i}j} \\cdot v_j$，其中 $\\cdot$ 表示欧几里得内积。\n\n您将研究路由中的对称模糊性问题，即由于对称性，部件可能被分配给错误的整体，然后添加打破对称性的先验来改进路由。\n\n您的任务是实现上述路由算法，固定迭代次数为 $r = 3$，并评估导向基准真相整体（ground-truth whole）的耦合质量总和。对于给定的基准真相索引 $j^\\star \\in \\{0,1\\}$，定义评估指标为\n$$\n\\mathcal{S} = \\sum_{i \\in \\{0,1\\}} c_{i j^\\star}.\n$$\n\n您必须实现两种打破对称性的先验：\n\n- 父级对数偏置先验（Parent logit bias prior）：一个父级偏置向量 $\\beta = (\\beta_0, \\beta_1)$，它被相同地加到两个部件 $i$ 的初始对数 $b_{ij}$ 上（即，在第一次 softmax 之前，$b_{ij} \\leftarrow b_{ij} + \\beta_j$）。\n- 投票扰动先验（Vote perturbation prior）：一个小向量 $\\Delta = (\\delta_x, \\delta_y)$，它扰动预测向量，使得朝向基准真相整体的预测被偏移为 $u_{\\hat{i} j^\\star} \\leftarrow u_{\\hat{i} j^\\star} + \\Delta$，而朝向其他整体的预测则被相反地偏移为 $u_{\\hat{i} j} \\leftarrow u_{\\hat{i} j} - \\Delta$（对于 $j \\neq j^\\star$）。\n\n为以下每个测试用例实现路由并评估指标 $\\mathcal{S}$。在所有情况下，使用 $n = 2$ 个部件，$m = 2$ 个整体，$d = 2$，$r = 3$ 次迭代。基准真相整体为 $j^\\star = 0$。\n\n- 测试用例 A（完全对称，无先验）：\n  - 预测：$u_{\\hat{00}} = (1, 0)$, $u_{\\hat{10}} = (1, 0)$, $u_{\\hat{01}} = (0, 1)$, $u_{\\hat{11}} = (0, 1)$。\n  - 先验：$\\beta = (0, 0)$, $\\Delta = (0, 0)$。\n\n- 测试用例 B（完全对称，对数偏置先验）：\n  - 预测：与测试用例 A 相同。\n  - 先验：$\\beta = (0.05, -0.05)$, $\\Delta = (0, 0)$。\n\n- 测试用例 C（冲突投票，无先验）：\n  - 预测：$u_{\\hat{00}} = (1, 0)$, $u_{\\hat{10}} = (0, 1)$, $u_{\\hat{01}} = (0, 1)$, $u_{\\hat{11}} = (1, 0)$。\n  - 先验：$\\beta = (0, 0)$, $\\Delta = (0, 0)$。\n\n- 测试用例 D（冲突投票，对数偏置先验）：\n  - 预测：与测试用例 C 相同。\n  - 先验：$\\beta = (0.2, -0.2)$, $\\Delta = (0, 0)$。\n\n- 测试用例 E（完全对称，投票扰动先验）：\n  - 预测：与测试用例 A 相同。\n  - 先验：$\\beta = (0, 0)$, $\\Delta = (0.05, 0.00)$。\n\n您的程序必须按 A、B、C、D、E 的顺序为每个测试用例计算指标 $\\mathcal{S}$，每个结果都是一个实数。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个实数都四舍五入到恰好 $6$ 位小数（例如，$[1.000000,1.937500,1.000000,1.750000,1.875000]$）。不应产生任何其他输出。",
            "solution": "用户提供了一个问题，要求实现和评估一个简化的动态路由算法，这是胶囊网络（CapsNets）的核心组成部分。任务是在五种不同的场景下计算一个特定的指标 $\\mathcal{S}$，这些场景旨在测试算法在存在对称性时的行为以及打破对称性的先验所带来的影响。\n\n首先，确认问题陈述的有效性。该问题在科学上基于深度学习的原理，特别是原始 CapsNet 文献中详细介绍的“协议路由”（routing-by-agreement）机制。所有提供的参数、方程和条件都是自洽的、数学上一致且定义明确的，构成了一个适定问题（well-posed problem）。不存在事实错误、歧义或主观因素。\n\n解决方案涉及按规定实现迭代路由算法。系统的状态存储在几个矩阵中。从初级胶囊 $i$到高级胶囊 $j$ 的预测向量由 $u_{\\hat{i}j}$ 给出。这些向量存储在一个大小为 $n \\times m \\times d$ 的张量中，其中 $n=2$ 是初级胶囊的数量，$m=2$ 是高级胶囊的数量，$d=2$ 是向量维度。动态调整以控制路由的路由对数 $b_{ij}$ 存储在一个 $n \\times m$ 的矩阵中。\n\n该算法按以下步骤进行：\n\n1.  **初始化**：\n    路由对数 $b_{ij}$ 初始化为 $0$。问题定义了两种类型的先验来修改此初始状态。\n    - **投票扰动先验**：在路由过程开始之前，预测向量 $u_{\\hat{i}j}$ 被扰动。对于基准真相胶囊 $j^\\star=0$，预测向量被一个小向量 $\\Delta$ 偏移，即 $u_{\\hat{i}0} \\leftarrow u_{\\hat{i}0} + \\Delta$。对于另一个胶囊（$j=1$），它们被 $-\\Delta$ 偏移，即 $u_{\\hat{i}1} \\leftarrow u_{\\hat{i}1} - \\Delta$。此操作应用于所有初级胶囊 $i \\in \\{0,1\\}$。\n    - **父级对数偏置先验**：一个偏置向量 $\\beta = (\\beta_0, \\beta_1)$ 被添加到初始对数中。对于每个初级胶囊 $i$，其到高级胶囊 $j$ 的对数更新为 $b_{ij} \\leftarrow b_{ij} + \\beta_j$。此操作在路由循环的第一次迭代之前执行。\n\n2.  **迭代路由**：算法的核心是一个循环，运行固定的迭代次数 $r = 3$。每次迭代包括四个步骤：\n    a.  **耦合系数 ($c_{ij}$)**：使用 softmax 函数将路由对数 $b_{ij}$ 转换为耦合系数 $c_{ij}$。Softmax 是针对每个初级胶囊 $i$ 在所有高级胶囊 $j$ 上应用的：\n        $$c_{ij} = \\frac{\\exp(b_{ij})}{\\sum_{j' \\in \\{0,1\\}} \\exp(b_{ij'})}$$\n        值 $c_{ij}$ 表示初级胶囊 $i$ 应被路由到高级胶囊 $j$ 的概率。\n\n    b.  **预激活值 ($s_j$)**：每个高级胶囊 $j$ 的预激活向量 $s_j$ 计算为来自所有初级胶囊的预测向量的加权和。权重是耦合系数 $c_{ij}$：\n        $$s_j = \\sum_{i \\in \\{0,1\\}} c_{ij} u_{\\hat{i}j}$$\n\n    c.  **激活值 ($v_j$)**：预激活向量 $s_j$ 通过一个非线性的“挤压”（squash）函数，为高级胶囊生成最终的输出向量 $v_j$。挤压函数将向量的模长缩放到 0 和 1 之间，同时保持其方向。\n        $$v_j = \\frac{\\lVert s_j \\rVert^2}{1 + \\lVert s_j \\rVert^2} \\frac{s_j}{\\lVert s_j \\rVert}$$\n        一个数值稳定的实现至关重要，特别是对于 $\\lVert s_j \\rVert = 0$ 的情况。在这种情况下，$v_j$ 必须是零向量。这可以通过在除法前向分母项 $\\lVert s_j \\rVert$ 添加一个很小的 epsilon 来处理。\n\n    d.  **对数更新 ($b_{ij}$)**：根据预测向量 $u_{\\hat{i}j}$ 与 resultant 的高级胶囊激活值 $v_j$ 之间的一致性来更新路由对数。一致性通过欧几里得内积（点积）来衡量。\n        $$b_{ij} \\leftarrow b_{ij} + u_{\\hat{i}j} \\cdot v_j$$\n        这个更新实现了“协议路由”原则：如果一个初级胶囊的预测与一个高级胶囊的聚合输出很好地对齐，连接它们的对数就会增加，从而在下一次迭代中加强该路由路径。\n\n3.  **最终评估**：\n    经过 $r=3$ 次迭代后，获得最终的路由对数 $b_{ij}$。根据这些对数，使用与步骤 2a 中相同的 softmax 函数计算最后一组耦合系数 $c_{ij}$。评估指标 $\\mathcal{S}$ 是从所有初级胶囊导向基准真相胶囊 $j^\\star=0$ 的耦合系数之和：\n    $$\\mathcal{S} = \\sum_{i \\in \\{0,1\\}} c_{i, j^\\star} = c_{00} + c_{10}$$\n    该指标量化了分配给正确整体的总“路由质量”，最大可能值为 $2$（如果两个初级胶囊都完全路由到 $j^\\star=0$），最小值为 $0$。值为 $1$ 表示平均而言，路由是完全模糊的。\n\n实现将通过设置适当的预测向量和先验参数，并执行所述算法来计算指标 $\\mathcal{S}$，从而处理五个测试用例中的每一个。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import softmax\n\ndef squash(s, epsilon=1e-9):\n    \"\"\"\n    Applies the squash nonlinearity to a set of vectors.\n    s: A NumPy array of shape (m, d) representing pre-activation vectors.\n    Returns: A NumPy array of shape (m, d) representing activated vectors.\n    \"\"\"\n    # s_norm_sq has shape (m, 1) to enable broadcasting\n    s_norm_sq = np.sum(s**2, axis=-1, keepdims=True)\n    # scale factor also has shape (m, 1)\n    scale = s_norm_sq / (1.0 + s_norm_sq)\n    # s_norm has shape (m, 1)\n    s_norm = np.sqrt(s_norm_sq)\n    # unit_s is s normalized, with protection against division by zero\n    unit_s = s / (s_norm + epsilon)\n    # The result v is the scaled unit vector\n    v = scale * unit_s\n    return v\n\ndef run_routing(u_in, beta, delta, j_star, r, n, m, d):\n    \"\"\"\n    Executes the dynamic routing algorithm for a given configuration.\n    \n    Args:\n        u_in (np.ndarray): Prediction vectors, shape (n, m, d).\n        beta (np.ndarray): Parent logit bias prior, shape (m,).\n        delta (np.ndarray): Vote perturbation prior, shape (d,).\n        j_star (int): Ground-truth whole index.\n        r (int): Number of routing iterations.\n        n (int): Number of primary capsules.\n        m (int): Number of higher-level capsules.\n        d (int): Dimensionality of pose vectors.\n\n    Returns:\n        float: The evaluation metric S.\n    \"\"\"\n    # Make a copy to avoid modifying the global test case data\n    u = u_in.copy()\n    \n    # 1. Apply vote perturbation prior\n    if np.any(delta):\n        other_j = 1 - j_star\n        u[:, j_star, :] += delta\n        u[:, other_j, :] -= delta\n\n    # 2. Initialize routing logits b_ij\n    b = np.zeros((n, m))\n    \n    # 3. Apply parent logit bias prior\n    if np.any(beta):\n        b += beta # Broadcasting adds beta to each row of b\n\n    # 4. Routing loop for r iterations\n    for _ in range(r):\n        # a. Compute coupling coefficients c_ij by softmax over logits\n        c = softmax(b, axis=1)\n        \n        # b. Compute pre-activations s_j as a weighted sum of predictions\n        s = np.einsum('ij,ijd->jd', c, u)\n        \n        # c. Compute activations v_j using the squash function\n        v = squash(s)\n        \n        # d. Update logits b_ij based on agreement (dot product)\n        agreement = np.einsum('ijd,jd->ij', u, v)\n        b += agreement\n        \n    # 5. Compute final coupling coefficients after all iterations\n    final_c = softmax(b, axis=1)\n    \n    # 6. Compute the metric S: sum of coupling mass to the ground-truth whole\n    metric_s = np.sum(final_c[:, j_star])\n    \n    return metric_s\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the routing algorithm for each, printing the results.\n    \"\"\"\n    # Define constants for all test cases\n    n, m, d = 2, 2, 2\n    r = 3\n    j_star = 0\n    \n    # Define prediction vectors for the test cases\n    u_A = np.array([[[1.0, 0.0], [0.0, 1.0]], \n                    [[1.0, 0.0], [0.0, 1.0]]])\n    \n    u_C = np.array([[[1.0, 0.0], [0.0, 1.0]], \n                    [[0.0, 1.0], [1.0, 0.0]]])\n\n    # Define the 5 test cases from the problem statement\n    test_cases = [\n        # Case A: Perfect symmetry, no prior\n        {'u': u_A, 'beta': np.array([0.0, 0.0]), 'delta': np.array([0.0, 0.0])},\n        # Case B: Perfect symmetry, logit bias prior\n        {'u': u_A, 'beta': np.array([0.05, -0.05]), 'delta': np.array([0.0, 0.0])},\n        # Case C: Conflicting votes, no prior\n        {'u': u_C, 'beta': np.array([0.0, 0.0]), 'delta': np.array([0.0, 0.0])},\n        # Case D: Conflicting votes, logit bias prior\n        {'u': u_C, 'beta': np.array([0.2, -0.2]), 'delta': np.array([0.0, 0.0])},\n        # Case E: Perfect symmetry, vote perturbation prior\n        {'u': u_A, 'beta': np.array([0.0, 0.0]), 'delta': np.array([0.05, 0.0])},\n    ]\n\n    results = []\n    for case in test_cases:\n        s_metric = run_routing(\n            u_in=case['u'],\n            beta=case['beta'],\n            delta=case['delta'],\n            j_star=j_star,\n            r=r, n=n, m=m, d=d\n        )\n        results.append(s_metric)\n\n    # Format the output as a comma-separated list with 6 decimal places, enclosed in brackets.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "动态路由过程本身可以被看作一个微型的优化问题，它需要在“探索”与“利用”之间找到平衡。如果路由过程过早地将所有权重分配给一个父胶囊（即“过早收敛”），可能会导致次优解；如果分配过于分散，则无法形成明确的判断。本练习引入了“温度退火” (temperature annealing) 的概念，通过在路由迭代过程中动态调整Softmax函数的温度 $\\tau_t$，来控制路由决策的“软硬”程度，从而帮助您学习如何设计一个有效的调度策略，以优化整个路由过程 。",
            "id": "3104863",
            "problem": "考虑采用动态路由的胶囊网络（CapsNets）。在路由迭代 $t \\in \\{0,1,\\dots,T-1\\}$ 中，对于每个低层胶囊索引 $i \\in \\{0,\\dots,I-1\\}$ 和每个父胶囊索引 $j \\in \\{0,\\dots,J-1\\}$，定义路由对数 $b_{ij}^{(t)} \\in \\mathbb{R}$。耦合系数 $c_{ij}^{(t)}$ 通过对父胶囊的温度缩放 softmax 计算得出：\n$$\nc_{ij}^{(t)} \\triangleq \\frac{\\exp\\left(b_{ij}^{(t)}/\\tau_t\\right)}{\\sum_{k=0}^{J-1}\\exp\\left(b_{ik}^{(t)}/\\tau_t\\right)},\n$$\n其中 $\\tau_t > 0$ 是迭代 $t$ 时的温度。父胶囊的输出使用 squash 非线性激活函数。设预测投票 $\\mathbf{\\hat{u}}_{j|i} \\in \\mathbb{R}^d$ 是固定的。在迭代 $t$ 时，父胶囊的预激活值为\n$$\n\\mathbf{s}_j^{(t)} \\triangleq \\sum_{i=0}^{I-1} c_{ij}^{(t)} \\,\\mathbf{\\hat{u}}_{j|i},\n$$\n父胶囊的输出是经过 squash 处理的向量\n$$\n\\mathbf{v}_j^{(t)} \\triangleq \\mathrm{squash}\\!\\left(\\mathbf{s}_j^{(t)}\\right) = \\frac{\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert^2}{1 + \\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert^2} \\cdot \\frac{\\mathbf{s}_j^{(t)}}{\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert},\n$$\n约定当 $\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert = 0$ 时，结果为零向量。一致性协议按如下方式更新路由对数\n$$\nb_{ij}^{(t+1)} \\triangleq b_{ij}^{(t)} + \\mathbf{\\hat{u}}_{j|i} \\cdot \\mathbf{v}_j^{(t)}.\n$$\n将每个低层胶囊 $i$ 在迭代 $t$ 时耦合分布的 Shannon 熵定义为\n$$\nH_i^{(t)} \\triangleq -\\sum_{j=0}^{J-1} c_{ij}^{(t)} \\log c_{ij}^{(t)}.\n$$\n您必须研究能够防止早期坍塌（过早地将所有耦合权重分配给单个父胶囊）同时确保最终选择性（在最后一次迭代中实现置信分配）的温度退火策略 $\\tau_t$。\n\n基于以上基本定义，设计一个程序：\n- 精确实现所述的温度缩放 softmax 路由、squash 非线性激活和一致性更新。\n- 在一个固定的小型场景中模拟路由过程，该场景包含 $I=3$ 个低层胶囊，$J=2$ 个父胶囊，向量维度 $d=2$，路由迭代次数 $T=6$，以及对所有 $i,j$ 初始对数 $b_{ij}^{(0)} = 0$。\n- 使用以下固定的预测投票（每个条目都是一个二维向量）：\n  - 对于低层胶囊 $i=0$：$\\mathbf{\\hat{u}}_{0|0} = [1.2, 0.0]$, $\\mathbf{\\hat{u}}_{1|0} = [0.0, 1.1]$。\n  - 对于低层胶囊 $i=1$：$\\mathbf{\\hat{u}}_{0|1} = [1.0, 0.1]$, $\\mathbf{\\hat{u}}_{1|1} = [0.1, 0.9]$。\n  - 对于低层胶囊 $i=2$：$\\mathbf{\\hat{u}}_{0|2} = [0.0, 1.0]$, $\\mathbf{\\hat{u}}_{1|2} = [1.0, 0.0]$。\n\n您的程序必须评估五个候选温度策略，每个策略由参数指定，但在本问题陈述中未规定任何明确的公式。对于每个策略，您的程序必须判断是否同时满足以下两个约束条件：\n- 早期抗坍塌：在迭代 $t=0$ 和 $t=1$ 时，低层胶囊的平均熵至少为 $H_{\\text{early,min}} = 0.55$。\n- 最终选择性：在迭代 $t=T-1$ 时，低层胶囊的平均熵至多为 $H_{\\text{final,max}} = 0.20$。\n\n待评估的五个策略及其参数如下：\n- 情况 A（快速衰减，非递增）：初始温度 $3.0$，衰减因子参数 $0.5$。\n- 情况 B（始终为高）：恒定温度 $3.0$。\n- 情况 C（始终为低）：恒定温度 $0.25$。\n- 情况 D（在 $T$ 次迭代中带下限的平滑衰减）：下界 $0.2$，上界 $3.0$。\n- 情况 E（熵触发的两阶段方案）：高阶段温度 $3.0$，低阶段温度 $0.2$，平均熵切换阈值 $0.60$，从迭代 $3$ 开始强制使用低温度。\n\n您的程序必须实现这些策略，为每个策略运行路由模拟，并为每种情况生成一个布尔值，指示是否满足两个约束条件。最终输出格式必须是包含情况 A–E 的五个布尔值的单行文本，形式为方括号括起来的逗号分隔列表，例如 $[\\mathrm{True},\\mathrm{False},\\dots]$。\n\n您的程序必须是自包含的，不需要任何输入，并使用指定的场景和参数。不涉及物理单位或角度。测试套件是上述五个策略，它们涵盖了一般情况、边界条件和边缘情况。答案是布尔值。",
            "solution": "问题陈述已经过严格审查，并被确定为有效。它在科学上基于胶囊网络的原理，问题定义良好，具有一套完整且一致的定义和参数，并提出了一个可验证的、非平凡的计算任务。对于某些温度策略定义中的微小歧义，通过选择与所提供描述一致的标准函数形式来解决，这是形式化算法问题时的常见做法。\n\n任务是为一个特定的小规模配置模拟胶囊网络中的动态路由过程，并根据两个性能约束评估五种不同的温度退火策略：防止过早收敛（早期抗坍塌）和确保最终决策性（最终选择性）。\n\n模拟的核心是一个在 $t \\in \\{0, 1, \\dots, T-1\\}$ 上的迭代过程，其中 $T=6$。每次迭代中系统的状态由路由对数 $b_{ij}^{(t)}$ 定义。我们从对所有 $i \\in \\{0, \\dots, 2\\}$ 和 $j \\in \\{0, \\dots, 1\\}$，$b_{ij}^{(0)} = 0$ 开始。\n\n**步骤1：耦合系数计算**\n在每次迭代 $t$ 中，耦合系数 $c_{ij}^{(t)}$ 使用温度缩放的 softmax 函数从对数 $b_{ij}^{(t)}$ 计算得出。对于每个低层胶囊 $i$，这些系数是关于父胶囊 $j$ 的一个概率分布：\n$$c_{ij}^{(t)} = \\frac{\\exp\\left(b_{ij}^{(t)}/\\tau_t\\right)}{\\sum_{k=0}^{J-1}\\exp\\left(b_{ik}^{(t)}/\\tau_t\\right)}$$\n这里，$\\tau_t > 0$ 是迭代 $t$ 时的温度。高温会使分布扁平化，鼓励探索；而低温会使分布尖锐化，鼓励利用。为了数值稳定性，一种常用技巧是在进行指数运算前，从所有对数值中减去最大对数值，这不会改变 softmax 的结果。\n\n**步骤2：熵计算**\n计算每个胶囊 $i$ 的耦合分布的 Shannon 熵，以衡量路由的确定性程度：\n$$H_i^{(t)} = -\\sum_{j=0}^{J-1} c_{ij}^{(t)} \\log c_{ij}^{(t)}$$\n按照标准，使用自然对数。所有 $I$ 个低层胶囊的平均熵 $\\bar{H}^{(t)} = \\frac{1}{I}\\sum_{i=0}^{I-1}H_i^{(t)}$ 是我们评估约束的主要指标。\n\n**步骤3：父胶囊激活**\n每个父胶囊 $j$ 的预激活向量 $\\mathbf{s}_j^{(t)}$ 是所有低层胶囊 $i$ 的预测向量 $\\mathbf{\\hat{u}}_{j|i}$ 的加权和：\n$$\\mathbf{s}_j^{(t)} = \\sum_{i=0}^{I-1} c_{ij}^{(t)} \\,\\mathbf{\\hat{u}}_{j|i}$$\n该向量聚合了来自下一层的“投票”。每次投票的贡献由相应的耦合系数加权。\n\n**步骤4：Squash非线性**\n预激活向量 $\\mathbf{s}_j^{(t)}$ 通过一个非线性的“squashing”函数来产生父胶囊的输出向量 $\\mathbf{v}_j^{(t)}$。该函数将向量的长度归一化到 $0$ 和 $1$ 之间，同时保持其方向：\n$$\\mathbf{v}_j^{(t)} = \\frac{\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert^2}{1 + \\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert^2} \\cdot \\frac{\\mathbf{s}_j^{(t)}}{\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert}$$\n如果 $\\mathbf{s}_j^{(t)}$ 是零向量，其范数为 $0$，则输出 $\\mathbf{v}_j^{(t)}$ 也是零向量，必须处理这种情况以防止除以零。\n\n**步骤5：路由对数更新**\n下一次迭代的对数 $b_{ij}^{(t+1)}$ 基于预测向量 $\\mathbf{\\hat{u}}_{j|i}$ 和父胶囊输出 $\\mathbf{v}_j^{(t)}$ 之间的一致性进行更新。一致性通过标量积来衡量：\n$$b_{ij}^{(t+1)} = b_{ij}^{(t)} + \\mathbf{\\hat{u}}_{j|i} \\cdot \\mathbf{v}_j^{(t)}$$\n这种“一致性路由”机制会加强那些导致强一致性的耦合。\n\n**温度策略**\n五个候选策略的实现如下：\n- **情况 A（快速衰减）：** 使用指数衰减策略：$\\tau_t = \\tau_{init} \\cdot \\gamma^t$，其中 $\\tau_{init} = 3.0$ 且 $\\gamma = 0.5$。\n- **情况 B（始终为高）：** 恒定策略：$\\tau_t = 3.0$。\n- **情况 C（始终为低）：** 恒定策略：$\\tau_t = 0.25$。\n- **情况 D（平滑衰减）：** 从上界到下界的线性衰减策略：$\\tau_t = \\tau_{upper} - (\\tau_{upper} - \\tau_{lower}) \\frac{t}{T-1}$，其中 $\\tau_{upper} = 3.0$ 且 $\\tau_{lower} = 0.2$。\n- **情况 E（熵触发）：** 动态策略。对于迭代 $t$：_如果_ $t \\ge 3$，则 $\\tau_t=0.2$。_否则如果_ $t=0$，则 $\\tau_t=3.0$。_否则_（$t \\in \\{1, 2\\}$），如果 $\\bar{H}^{(t-1)} \\ge 0.60$，则 $\\tau_t=3.0$，否则 $\\tau_t=0.2$。\n\n**评估与程序设计**\n设计一个函数，该函数接收一个策略的参数，运行 $T=6$ 次迭代的完整模拟，并记录每一步的平均熵 $\\bar{H}^{(t)}$。模拟结束后，它检查两个约束条件：\n1. **早期抗坍塌**：$\\bar{H}^{(0)} \\ge 0.55$ 且 $\\bar{H}^{(1)} \\ge 0.55$。\n2. **最终选择性**：$\\bar{H}^{(5)} \\le 0.20$。\n\n如果两个约束都满足，函数返回 `True`，否则返回 `False`。主程序为五个策略中的每一个执行此评估，并按指定格式化布尔结果。为了效率，采用了使用 NumPy 的向量化实现，特别是一致性计算，它可以表示为爱因斯坦求和：`agreement = np.einsum('ijd,jd->ij', u_hat, v)`。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to evaluate five temperature schedules for dynamic routing\n    in a Capsule Network and print the results.\n    \"\"\"\n\n    def evaluate_schedule(schedule_type, params):\n        \"\"\"\n        Simulates dynamic routing for a given temperature schedule and\n        evaluates it against specified entropy constraints.\n\n        Args:\n            schedule_type (str): An identifier for the schedule ('A', 'B', 'C', 'D', 'E').\n            params (tuple): Parameters for the schedule.\n\n        Returns:\n            bool: True if both early anti-collapse and final selectivity\n                  constraints are met, False otherwise.\n        \"\"\"\n        # --- Fixed Simulation Parameters ---\n        I, J, d, T = 3, 2, 2, 6\n        H_early_min, H_final_max = 0.55, 0.20\n\n        # --- Fixed Predicted Votes (u_hat[i, j, :] is u_j|i) ---\n        u_hat = np.zeros((I, J, d))\n        u_hat[0, 0, :] = [1.2, 0.0]\n        u_hat[0, 1, :] = [0.0, 1.1]\n        u_hat[1, 0, :] = [1.0, 0.1]\n        u_hat[1, 1, :] = [0.1, 0.9]\n        u_hat[2, 0, :] = [0.0, 1.0]\n        u_hat[2, 1, :] = [1.0, 0.0]\n\n        # --- Initialization ---\n        b = np.zeros((I, J))  # Routing logits\n        mean_entropies = []\n\n        # --- Dynamic Routing Simulation Loop ---\n        for t in range(T):\n            # 1. Determine current temperature tau_t\n            current_tau = 0.0\n            if schedule_type == 'A':\n                initial_temp, decay_factor = params\n                current_tau = initial_temp * (decay_factor ** t)\n            elif schedule_type == 'B' or schedule_type == 'C':\n                const_temp, = params\n                current_tau = const_temp\n            elif schedule_type == 'D':\n                lower, upper = params\n                current_tau = upper - (upper - lower) * t / (T - 1)\n            elif schedule_type == 'E':\n                high_temp, low_temp, switch_thresh, forced_low_iter = params\n                if t >= forced_low_iter:\n                    current_tau = low_temp\n                elif t == 0:\n                    current_tau = high_temp\n                else:  # t in {1, 2}\n                    if mean_entropies[t - 1] < switch_thresh:\n                        current_tau = low_temp\n                    else:\n                        current_tau = high_temp\n\n            # 2. Calculate coupling coefficients c_ij\n            # Using a numerically stable softmax implementation\n            scaled_b = b / current_tau\n            max_b = np.max(scaled_b, axis=1, keepdims=True)\n            exp_b = np.exp(scaled_b - max_b)\n            c = exp_b / np.sum(exp_b, axis=1, keepdims=True)\n\n            # 3. Calculate and store mean entropy\n            # c_ij * log(c_ij) -> 0 as c_ij -> 0. Add a small epsilon for safety,\n            # though softmax output is strictly positive with float precision.\n            entropies_i = -np.sum(c * np.log(c + 1e-12), axis=1)\n            mean_entropies.append(np.mean(entropies_i))\n\n            # 4. Calculate parent capsule pre-activations s_j\n            s = np.zeros((J, d))\n            for j in range(J):\n                s[j, :] = np.sum(c[:, j:j + 1] * u_hat[:, j, :], axis=0)\n\n            # 5. Calculate parent capsule outputs v_j (squash nonlinearity)\n            v = np.zeros((J, d))\n            for j in range(J):\n                s_norm_sq = np.sum(np.square(s[j]))\n                if s_norm_sq > 0:\n                    s_norm = np.sqrt(s_norm_sq)\n                    scale = s_norm_sq / (1.0 + s_norm_sq)\n                    v[j, :] = scale * s[j] / s_norm\n            \n            # 6. Update routing logits b_ij\n            agreement = np.einsum('ijd,jd->ij', u_hat, v)\n            b = b + agreement\n\n        # --- Check Constraints ---\n        early_ok = (mean_entropies[0] >= H_early_min) and \\\n                   (mean_entropies[1] >= H_early_min)\n        final_ok = (mean_entropies[T - 1] <= H_final_max)\n\n        return early_ok and final_ok\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ('A', (3.0, 0.5)),      # fast-decaying\n        ('B', (3.0,)),          # always high\n        ('C', (0.25,)),         # always low\n        ('D', (0.2, 3.0)),      # smooth decay\n        ('E', (3.0, 0.2, 0.60, 3)), # entropy-triggered\n    ]\n\n    results = []\n    for case_type, case_params in test_cases:\n        result = evaluate_schedule(case_type, case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}