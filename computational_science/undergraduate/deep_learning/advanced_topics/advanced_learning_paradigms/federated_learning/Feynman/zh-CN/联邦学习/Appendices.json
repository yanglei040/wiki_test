{
    "hands_on_practices": [
        {
            "introduction": "有效的联邦学习始于客户端设备。在考虑通信和聚合的复杂性之前，理解单个参与者的资源限制至关重要。本练习将让你设身处地，面对一台资源有限的设备，并为其寻找最优的本地训练配置。你将探索一个基本权衡：一方面希望通过增大小批量（batch size）来降低梯度估计中的统计噪声，另一方面则必须遵守严格的内存、计算时间和能源预算，这是现实世界联邦学习系统中的核心挑战。",
            "id": "3124684",
            "problem": "一个移动设备参与联邦学习 (FL)，它使用来自其本地数据集的大小为 $B$ 的独立同分布数据样本的小批量，为一个标量模型参数计算本地梯度估计值 $\\hat{g}$。设每个样本的梯度是一个标量随机变量 $g_{i}$，其均值为 $\\mathbb{E}[g_{i}] = g$，方差为 $\\mathrm{Var}(g_{i}) = \\sigma^{2}$，并假设 $g_{i}$ 是相互独立的。该设备的目标是选择尽可能大的 $B$ 以减小 $\\hat{g}$ 的方差，但受限于单次训练轮次的资源约束：\n\n- 内存预算：该设备在训练轮次中有 $M = 80$ MiB 的可用空闲内存。加载的模型占用 $m_{\\mathrm{model}} = 40$ MiB，运行时开销（激活值、框架缓冲区）占用 $m_{\\mathrm{ov}} = 10$ MiB，每个数据样本在处理时需要 $m_{s} = 1$ MiB 的内存。假设内存使用量随 $B$ 线性增长，并且所有数据必须同时保存在内存中。\n- 延迟预算：每个数据样本需要 $t_{s} = 3$ ms 的计算时间，且设备必须在 $T_{\\max} = 120$ ms 内完成本地计算。\n- 能量预算：每个数据样本需要 $e_{s} = 7$ mJ 的能量，且设备在本轮可用能量为 $E_{\\max} = 250$ mJ。\n\n假设设备按顺序处理样本，$B$ 必须为正整数，并忽略任何并行或流水线效应。同时假设没有引入额外噪声（如量化或差分隐私），因此 $\\hat{g}$ 中唯一的随机性来源是小批量的采样。\n\n从样本均值的基本定义和独立随机变量的方差性质出发，推导出 $\\mathrm{Var}(\\hat{g})$ 作为 $B$ 的函数的表达式。然后，在给定 $\\sigma^{2} = 0.81$ 的条件下，确定满足所有三个设备约束的最大的可行 $B$ 值，并计算在该 $B$ 值下 $\\mathrm{Var}(\\hat{g})$ 的数值。将最终方差表示为一个无单位的纯数。无需四舍五入；报告精确值。",
            "solution": "问题要求两个主要结果：首先，推导梯度估计值方差 $\\mathrm{Var}(\\hat{g})$ 作为小批量大小 $B$ 的函数；其次，在限制 $B$ 的一组特定资源约束下，计算该方差的数值。\n\n首先，我们推导 $\\mathrm{Var}(\\hat{g})$ 的表达式。本地梯度估计值 $\\hat{g}$ 被定义为 $B$ 个独立同分布 (i.i.d.) 的样本梯度（记为 $g_i$，其中 $i \\in \\{1, 2, \\dots, B\\}$）的样本均值。\n$$ \\hat{g} = \\frac{1}{B} \\sum_{i=1}^{B} g_i $$\n每个 $g_i$ 都是一个随机变量，其均值为 $\\mathbb{E}[g_{i}] = g$，方差为 $\\mathrm{Var}(g_{i}) = \\sigma^{2}$。我们可以通过应用方差算子的性质来求出 $\\hat{g}$ 的方差。\n\n对于一个常数 $c$ 和一个随机变量 $X$，方差算子遵循规则 $\\mathrm{Var}(cX) = c^2 \\mathrm{Var}(X)$。在我们的例子中，常数是 $\\frac{1}{B}$。\n$$ \\mathrm{Var}(\\hat{g}) = \\mathrm{Var}\\left(\\frac{1}{B} \\sum_{i=1}^{B} g_i\\right) = \\left(\\frac{1}{B}\\right)^2 \\mathrm{Var}\\left(\\sum_{i=1}^{B} g_i\\right) = \\frac{1}{B^2} \\mathrm{Var}\\left(\\sum_{i=1}^{B} g_i\\right) $$\n对于独立随机变量的和，其和的方差等于它们各自方差的和。问题中说明了 $g_i$ 是相互独立的。\n$$ \\mathrm{Var}\\left(\\sum_{i=1}^{B} g_i\\right) = \\sum_{i=1}^{B} \\mathrm{Var}(g_i) $$\n由于样本也是同分布的，每个 $g_i$ 的方差都相同：$\\mathrm{Var}(g_i) = \\sigma^2$。\n$$ \\sum_{i=1}^{B} \\mathrm{Var}(g_i) = \\sum_{i=1}^{B} \\sigma^2 = B \\sigma^2 $$\n将此结果代回到 $\\mathrm{Var}(\\hat{g})$ 的表达式中，即可得到所需的关系。\n$$ \\mathrm{Var}(\\hat{g}) = \\frac{1}{B^2} (B \\sigma^2) = \\frac{\\sigma^2}{B} $$\n\n接下来，我们必须根据给定的三个资源约束，确定小批量大小 $B$ 的最大可行整数值。\n\n1.  内存约束：消耗的总内存不能超过可用内存 $M = 80$ MiB。总内存是固定的模型和开销内存（$m_{\\mathrm{model}} = 40$ MiB 和 $m_{\\mathrm{ov}} = 10$ MiB）与可变的每数据样本内存（$m_{s} = 1$ MiB，按批量大小 $B$ 缩放）之和。\n    $$ m_{\\mathrm{model}} + m_{\\mathrm{ov}} + B \\cdot m_{s} \\le M $$\n    代入数值（单位一致，可在不等式中省略）：\n    $$ 40 + 10 + B \\cdot 1 \\le 80 $$\n    $$ 50 + B \\le 80 $$\n    $$ B \\le 30 $$\n\n2.  延迟约束：总计算时间必须在最大允许延迟 $T_{\\max} = 120$ ms 之内。总时间是每样本计算时间 $t_{s} = 3$ ms 乘以批量大小 $B$。\n    $$ B \\cdot t_{s} \\le T_{\\max} $$\n    代入数值：\n    $$ B \\cdot 3 \\le 120 $$\n    $$ B \\le \\frac{120}{3} $$\n    $$ B \\le 40 $$\n\n3.  能量约束：消耗的总能量不能超过能量预算 $E_{\\max} = 250$ mJ。总能量是每样本能耗 $e_{s} = 7$ mJ 乘以批量大小 $B$。\n    $$ B \\cdot e_{s} \\le E_{\\max} $$\n    代入数值：\n    $$ B \\cdot 7 \\le 250 $$\n    $$ B \\le \\frac{250}{7} $$\n    由于 $B$ 必须是正整数，我们必须对此值取底：$B \\le \\lfloor \\frac{250}{7} \\rfloor = \\lfloor 35.714\\dots \\rfloor = 35$。\n    $$ B \\le 35 $$\n\n为同时满足所有三个约束， $B$ 必须小于或等于三个推导出的上界中的最小值。\n$$ B \\le \\min(30, 40, 35) $$\n$$ B \\le 30 $$\n问题要求的是尽可能大的 $B$，因此我们选择满足此条件的最大整数值，即 $B = 30$。\n\n最后，我们使用推导出的公式 $\\mathrm{Var}(\\hat{g}) = \\frac{\\sigma^2}{B}$、确定的批量大小 $B = 30$ 以及给定的每样本方差 $\\sigma^2 = 0.81$ 来计算 $\\mathrm{Var}(\\hat{g})$ 的数值。\n$$ \\mathrm{Var}(\\hat{g}) = \\frac{0.81}{30} $$\n这可以精确地计算为小数：\n$$ \\mathrm{Var}(\\hat{g}) = \\frac{81}{100} \\div 30 = \\frac{81}{100} \\times \\frac{1}{30} = \\frac{81}{3000} = \\frac{27 \\times 3}{1000 \\times 3} = \\frac{27}{1000} = 0.027 $$",
            "answer": "$$\\boxed{0.027}$$"
        },
        {
            "introduction": "当客户端完成本地训练后，中央服务器面临着一项关键任务：如何智能地整合它们的贡献。标准的联邦平均（FedAvg）算法根据每个客户端本地数据集的大小来赋予其权重，但这种策略总是最优的吗？本练习将挑战你深入探究模型聚合的统计学基础。你将推导理论上的最优加权方案，并批判性地分析常用启发式方法的性能，特别是在数据非独立同分布（non-IID）这一充满挑战却又普遍存在的场景下。",
            "id": "3124727",
            "problem": "在一个跨设备的联邦学习（FL）系统中，一个中心服务器使用凸组合的方式将来自 $K$ 个客户端的本地模型参数合并成一个单一的全局参数。具体来说，服务器构建 $w^{new} = \\sum_{i=1}^{K} \\alpha_i w_i$，其中 $\\sum_{i=1}^{K} \\alpha_i = 1$ 且 $\\alpha_i \\ge 0$。考虑了两种实用的聚合权重选择：$\\alpha_i \\propto n_i$，其中 $n_i$ 是客户端 $i$ 上的本地训练样本数量（如在联邦平均（FedAvg）中）；以及 $\\alpha_i \\propto 1/\\hat{\\sigma}_i$，其中 $\\hat{\\sigma}_i$ 是对客户端 $i$ 上训练产生的本地随机噪声水平的设备上估计。假设以下建模基础用于推理期望误差：\n- 每个客户端执行本地训练，以生成一个底层参数 $w^\\star$ 的本地估计 $w_i$。\n- 为了本题的目的，考虑标量情况，其中 $w^\\star \\in \\mathbb{R}$，且 $w_i$ 是 $w^\\star$ 的独立无偏估计量。\n- $w_i$ 的可变性源于随机优化和数据采样，可以建模为 $\\operatorname{Var}(w_i) = \\sigma_i^2 / n_i$，其中 $\\sigma_i^2$ 是客户端特定的噪声尺度，$n_i$ 是本地样本大小。这反映了一个经过充分检验的事实，即在温和条件下，对 $n_i$ 个方差为 $\\sigma_i^2$ 的独立观测值求平均，其方差的缩放级别为 $\\sigma_i^2/n_i$。\n\n现在考虑以下关于 $\\alpha_i \\propto n_i$ 和 $\\alpha_i \\propto 1/\\hat{\\sigma}_i$ 之间权衡的说法。选择所有正确的陈述。\n\nA. 在上述假设（独立的、无偏的 $w_i$，且 $\\operatorname{Var}(w_i)=\\sigma_i^2/n_i$）下，选择 $\\alpha_i \\propto n_i$ 可以在所有凸组合中最小化期望均方误差 $\\mathbb{E}\\!\\left[(w^{new}-w^\\star)^2\\right]$。\n\nB. 在相同假设下，能在所有凸组合中最小化 $\\mathbb{E}\\!\\left[(w^{new}-w^\\star)^2\\right]$ 的聚合规则使用的权重与 $n_i / \\sigma_i^2$ 成正比。因此，当不同客户端的 $\\sigma_i^2$ 不同时，$\\alpha_i \\propto n_i$ 和 $\\alpha_i \\propto 1/\\hat{\\sigma}_i$ 通常都是次优的。\n\nC. 考虑 $K=3$ 个客户端，其参数分别为 $(n_1,\\sigma_1)=(200,4)$，$(n_2,\\sigma_2)=(200,3)$ 和 $(n_3,\\sigma_3)=(10,1)$。在上述假设下，并将 $\\hat{\\sigma}_i=\\sigma_i$ 对待，使用 $\\alpha_i \\propto n_i$ 的 $w^{new}$ 的期望均方误差要小于使用 $\\alpha_i \\propto 1/\\hat{\\sigma}_i$ 的情况。\n\nD. 如果客户端数据分布差异很大，以至于每个客户端的风险在不同的 $w_i^\\star \\neq w_j^\\star$ 处最小化，那么按 $\\alpha_i \\propto n_i$ 加权使聚合与最小化所有观测样本的全局经验风险的目标一致，而 $\\alpha_i \\propto 1/\\hat{\\sigma}_i$ 可能会使 $w^{new}$ 偏向那些噪声较小但不具代表性的客户端，从而可能损害全局泛化能力。\n\n选择所有适用的选项。",
            "solution": "用户希望我验证问题陈述，如果有效，则提供一个详细的解决方案，评估关于联邦学习中聚合权重的一系列主张。\n\n### 问题验证\n问题陈述是有效的。它提出了一个定义明确的统计模型，用于分析联邦学习中的聚合步骤。\n- **已知条件**：\n    - 全局参数更新规则：$w^{new} = \\sum_{i=1}^{K} \\alpha_i w_i$，其中 $\\sum_{i=1}^{K} \\alpha_i = 1$ 且 $\\alpha_i \\ge 0$。\n    - 加权方案：(1) $\\alpha_i \\propto n_i$，(2) $\\alpha_i \\propto 1/\\hat{\\sigma}_i$。\n    - 建模假设：$w_i$ 是一个真实标量参数 $w^\\star \\in \\mathbb{R}$ 的独立、无偏估计量，且 $\\mathbb{E}[w_i] = w^\\star$。\n    - 方差模型：$\\operatorname{Var}(w_i) = \\sigma_i^2 / n_i$。\n- **验证清单**：\n    - **科学基础**：该模型是统计估计理论（特别是寻找最佳线性无偏估计量或BLUE）在联邦学习背景下的标准应用。这是一种合理且成熟的方法。\n    - **良态问题**：问题的核心是最小化估计量线性组合的均方误差，这是一个经典的、具有唯一解的良态优化问题。\n    - **客观性**：语言精确，假设陈述清晰。\n    - **完整性**：所提供的信息足以推导出最优权重并评估各项主张。\n    - **现实性**：虽然是一个简化模型（标量参数，已知的方差结构），但该模型抓住了本质的权衡，并且在科学上并非不合理。\n\n该问题是有效的，并且允许进行严格的数学和定性分析。\n\n### 求解推导\n\n目标是分析聚合估计量 $\\mathbb{E}\\!\\left[(w^{new}-w^\\star)^2\\right]$ 的期望均方误差（MSE），针对不同的权重 $\\alpha_i$ 选择。\n\n首先，我们确定 $w^{new}$ 的偏差。\n$$\n\\mathbb{E}[w^{new}] = \\mathbb{E}\\left[\\sum_{i=1}^{K} \\alpha_i w_i\\right] = \\sum_{i=1}^{K} \\alpha_i \\mathbb{E}[w_i]\n$$\n鉴于每个 $w_i$ 都是 $w^\\star$ 的无偏估计量，所以 $\\mathbb{E}[w_i] = w^\\star$。\n$$\n\\mathbb{E}[w^{new}] = \\sum_{i=1}^{K} \\alpha_i w^\\star = w^\\star \\sum_{i=1}^{K} \\alpha_i\n$$\n在权重构成凸组合的约束条件 $\\sum_{i=1}^{K} \\alpha_i = 1$ 下，我们有：\n$$\n\\mathbb{E}[w^{new}] = w^\\star\n$$\n这表明只要权重之和为1，聚合估计量 $w^{new}$ 也是无偏的。\n\n对于一个无偏估计量，其均方误差等于其方差：\n$$\n\\mathbb{E}\\!\\left[(w^{new}-w^\\star)^2\\right] = \\mathbb{E}\\!\\left[(w^{new}-\\mathbb{E}[w^{new}])^2\\right] = \\operatorname{Var}(w^{new})\n$$\n现在，我们计算 $w^{new}$ 的方差。由于假设 $w_i$ 是独立的：\n$$\n\\operatorname{Var}(w^{new}) = \\operatorname{Var}\\left(\\sum_{i=1}^{K} \\alpha_i w_i\\right) = \\sum_{i=1}^{K} \\operatorname{Var}(\\alpha_i w_i) = \\sum_{i=1}^{K} \\alpha_i^2 \\operatorname{Var}(w_i)\n$$\n代入给定的方差模型 $\\operatorname{Var}(w_i) = \\sigma_i^2 / n_i$：\n$$\n\\operatorname{Var}(w^{new}) = \\sum_{i=1}^{K} \\alpha_i^2 \\frac{\\sigma_i^2}{n_i}\n$$\n寻找最佳聚合权重的问题简化为在约束 $\\sum_{i=1}^{K} \\alpha_i = 1$ 下最小化此方差。我们使用拉格朗日乘数法。拉格朗日函数为：\n$$\nL(\\alpha_1, \\dots, \\alpha_K, \\lambda) = \\sum_{i=1}^{K} \\alpha_i^2 \\frac{\\sigma_i^2}{n_i} - \\lambda \\left(\\sum_{i=1}^{K} \\alpha_i - 1\\right)\n$$\n对 $\\alpha_i$ 求偏导并令其为零：\n$$\n\\frac{\\partial L}{\\partial \\alpha_i} = 2 \\alpha_i \\frac{\\sigma_i^2}{n_i} - \\lambda = 0 \\implies \\alpha_i = \\frac{\\lambda}{2} \\frac{n_i}{\\sigma_i^2}\n$$\n这表明最优权重 $\\alpha_i^{\\text{opt}}$ 必须与 $\\frac{n_i}{\\sigma_i^2}$ 成正比。这个量是每个估计量方差的倒数，即 $1/\\operatorname{Var}(w_i)$。为了找到比例常数，我们使用约束 $\\sum \\alpha_i = 1$：\n$$\n\\sum_{j=1}^{K} \\alpha_j^{\\text{opt}} = \\sum_{j=1}^{K} \\frac{\\frac{\\lambda}{2} \\frac{n_j}{\\sigma_j^2}}{1} = 1 \\implies \\frac{\\lambda}{2} = \\frac{1}{\\sum_{j=1}^{K} \\frac{n_j}{\\sigma_j^2}}\n$$\n因此，最小化均方误差的最优权重是：\n$$\n\\alpha_i^{\\text{opt}} = \\frac{n_i / \\sigma_i^2}{\\sum_{j=1}^{K} n_j / \\sigma_j^2}\n$$\n这是一个经典结果：最小方差无偏估计量是通过对每个独立的无偏估计量按其方差的倒数进行加权得到的。\n\n### 逐项分析\n\n**A. 在上述假设（独立的、无偏的 $w_i$，且 $\\operatorname{Var}(w_i)=\\sigma_i^2/n_i$）下，选择 $\\alpha_i \\propto n_i$ 可以在所有凸组合中最小化期望均方误差 $\\mathbb{E}\\!\\left[(w^{new}-w^\\star)^2\\right]$。**\n该陈述声称与样本数量 $n_i$ 成正比的权重是最优的。我们的推导表明，最优权重是 $\\alpha_i^{\\text{opt}} \\propto n_i / \\sigma_i^2$。$\\alpha_i \\propto n_i$ 的方案仅在客户端特定的噪声尺度 $\\sigma_i^2$ 对所有客户端 $i=1, \\dots, K$ 都是常数的特殊情况下才是最优的。由于问题允许 $\\sigma_i^2$ 在客户端之间变化，所以这个说法通常不成立。\n**结论：不正确。**\n\n**B. 在相同假设下，能在所有凸组合中最小化 $\\mathbb{E}\\!\\left[(w^{new}-w^\\star)^2\\right]$ 的聚合规则使用的权重与 $n_i / \\sigma_i^2$ 成正比。因此，当不同客户端的 $\\sigma_i^2$ 不同时，$\\alpha_i \\propto n_i$ 和 $\\alpha_i \\propto 1/\\hat{\\sigma}_i$ 通常都是次优的。**\n该陈述的第一部分与我们推导出的最优权重 $\\alpha_i^{\\text{opt}} \\propto n_i / \\sigma_i^2$ 完全一致。第二部分将两种提议的方案与此最优规则进行比较。\n- $\\alpha_i \\propto n_i$ 方案忽略了客户端的噪声水平 $\\sigma_i^2$。\n- $\\alpha_i \\propto 1/\\hat{\\sigma}_i$ 方案（假设 $\\hat{\\sigma}_i \\approx \\sigma_i$）忽略了客户端的样本量 $n_i$，并且与标准差的倒数成正比，而不是方差的倒数。\n两种方案都与最优规则不同，因此通常不会最小化均方误差。它们是次优的近似。\n**结论：正确。**\n\n**C. 考虑 $K=3$ 个客户端，其参数分别为 $(n_1,\\sigma_1)=(200,4)$，$(n_2,\\sigma_2)=(200,3)$ 和 $(n_3,\\sigma_3)=(10,1)$。在上述假设下，并将 $\\hat{\\sigma}_i=\\sigma_i$ 对待，使用 $\\alpha_i \\propto n_i$ 的 $w^{new}$ 的期望均方误差要小于使用 $\\alpha_i \\propto 1/\\hat{\\sigma}_i$ 的情况。**\n我们必须使用提供的数据计算两种方案的均方误差。均方误差由 $\\sum_{i=1}^3 \\alpha_i^2 \\operatorname{Var}(w_i)$ 给出。\n首先，计算方差：\n- $\\operatorname{Var}(w_1) = \\sigma_1^2 / n_1 = 4^2 / 200 = 16/200 = 0.08$。\n- $\\operatorname{Var}(w_2) = \\sigma_2^2 / n_2 = 3^2 / 200 = 9/200 = 0.045$。\n- $\\operatorname{Var}(w_3) = \\sigma_3^2 / n_3 = 1^2 / 10 = 1/10 = 0.1$。\n\n方案1：$\\alpha_i \\propto n_i$。比例基于 $(200, 200, 10)$。\n- 总和 $N = 200+200+10 = 410$。\n- 权重：$\\alpha_1 = 200/410$，$\\alpha_2 = 200/410$，$\\alpha_3 = 10/410$。\n- $MSE_n = \\left(\\frac{200}{410}\\right)^2(0.08) + \\left(\\frac{200}{410}\\right)^2(0.045) + \\left(\\frac{10}{410}\\right)^2(0.1)$\n- $MSE_n = \\frac{1}{410^2} [200^2(0.08) + 200^2(0.045) + 10^2(0.1)] = \\frac{3200 + 1800 + 10}{168100} = \\frac{5010}{168100} \\approx 0.02980$。\n\n方案2：$\\alpha_i \\propto 1/\\sigma_i$。比例基于 $(1/4, 1/3, 1/1)$。\n- 比例之和：$1/4 + 1/3 + 1 = 3/12 + 4/12 + 12/12 = 19/12$。\n- 权重：$\\alpha_1 = \\frac{1/4}{19/12} = \\frac{3}{19}$，$\\alpha_2 = \\frac{1/3}{19/12} = \\frac{4}{19}$，$\\alpha_3 = \\frac{1}{19/12} = \\frac{12}{19}$。\n- $MSE_\\sigma = \\left(\\frac{3}{19}\\right)^2(0.08) + \\left(\\frac{4}{19}\\right)^2(0.045) + \\left(\\frac{12}{19}\\right)^2(0.1)$\n- $MSE_\\sigma = \\frac{1}{19^2} [9(0.08) + 16(0.045) + 144(0.1)] = \\frac{0.72 + 0.72 + 14.4}{361} = \\frac{15.84}{361} \\approx 0.04388$。\n\n比较结果：$MSE_n \\approx 0.02980  0.04388 \\approx MSE_\\sigma$。该说法是正确的。这是因为客户端3的样本量非常小（$n_3=10$），使其估计的变异性很高（$\\operatorname{Var}(w_3)=0.1$），但 $1/\\sigma_i$ 规则却给了它最大的权重（$\\alpha_3 = 12/19 \\approx 63\\%$），这极大地增加了总误差。而 $\\alpha_i \\propto n_i$ 规则正确地降低了这个不可靠客户端的权重。\n**结论：正确。**\n\n**D. 如果客户端数据分布差异很大，以至于每个客户端的风险在不同的 $w_i^\\star \\neq w_j^\\star$ 处最小化，那么按 $\\alpha_i \\propto n_i$ 加权使聚合与最小化所有观测样本的全局经验风险的目标一致，而 $\\alpha_i \\propto 1/\\hat{\\sigma}_i$ 可能会使 $w^{new}$ 偏向那些噪声较小但不具代表性的客户端，从而可能损害全局泛化能力。**\n这个陈述将上下文从一个具有单一 $w^\\star$ 的简单统计估计问题，转移到了一个更现实的联邦学习挑战，即非独立同分布（non-IID）数据，其中局部目标 $L_i(w)$ 在不同的点 $w_i^\\star$ 最小化。\n- 支持 $\\alpha_i \\propto n_i$ 的论点：FL中的典型目标是最小化全局风险，通常定义为局部风险的加权平均：$\\min_w L(w) = \\sum_{i=1}^K \\frac{n_i}{N} L_i(w)$，其中 $N = \\sum_j n_j$。使用权重 $\\alpha_i=n_i/N$ 的FedAvg算法正是受此公式直接启发。该加权方案使聚合步骤与给予对全局目标贡献更多数据的客户端更大影响力的目标保持一致。这个推理是合理的。\n- 对 $\\alpha_i \\propto 1/\\hat{\\sigma}_i$ 的批判：一个客户端的本地噪声水平 $\\hat{\\sigma}_i$ 可能很低，因为其本地数据非常同质（例如，数字识别任务中的单一数字类别）。这样的客户端对于全局数据分布是“不具代表性”的。其本地模型 $w_i$ 将是所需全局模型的一个很差的代理。按 $1/\\hat{\\sigma}_i$ 加权会给予这个低噪声但有偏且不具代表性的客户端一个很大的权重，将全局模型 $w^{new}$ 拉离一个好的全局解，并损害其泛化性能。这是FL中一个有效且众所周知的担忧。\n该陈述的两个部分都代表了在非独立同分布（non-IID）设置下关于这些加权方案实际影响的标准、正确的推理。\n**结论：正确。**",
            "answer": "$$\\boxed{BCD}$$"
        },
        {
            "introduction": "前一个练习突显了联邦学习中的一个主要障碍：数据异构性，即单一的全局模型可能难以在任何单个客户端上都表现良好。这个动手编程练习将带你从理论分析走向系统实现，构建一个强大的解决方案——个性化联邦学习。你将从零开始，利用客户端的梯度信息对其进行聚类，为每个具有相似目标的客户端集群训练一个专门的模型，从而展示一种克服非独立同分布（non-IID）挑战的实用且有效的方法。",
            "id": "3124737",
            "problem": "考虑一个联邦学习场景，其中有 $C$ 个客户端，每个客户端持有从带有加性噪声的线性模型中独立抽样的本地数据。每个客户端 $i$ 有 $n_i$ 个样本 $\\{(x^{(i)}_t, y^{(i)}_t)\\}_{t=1}^{n_i}$，其中 $x^{(i)}_t \\in \\mathbb{R}^d$ 从零均值、单位协方差的高斯分布中独立同分布地抽取，且 $y^{(i)}_t = (x^{(i)}_t)^\\top w^*_i + \\epsilon^{(i)}_t$，其中 $\\epsilon^{(i)}_t \\sim \\mathcal{N}(0, \\sigma^2)$。我们使用客户端 $i$ 上的经验均方误差 (MSE) 损失来训练线性模型 $f_w(x) = x^\\top w$，其损失函数由下式给出\n$$\nL_i(w) = \\frac{1}{n_i} \\sum_{t=1}^{n_i} \\left((x^{(i)}_t)^\\top w - y^{(i)}_t\\right)^2.\n$$\n$L_i(w)$ 关于 $w$ 的梯度表示为 $g_i(w) = \\nabla_w L_i(w)$。我们定义两个非零梯度 $g_i$ 和 $g_j$ 之间的余弦相似度为\n$$\n\\mathrm{cos}(g_i, g_j) = \\frac{g_i^\\top g_j}{\\|g_i\\|_2 \\, \\|g_j\\|_2}.\n$$\n当任一梯度为零向量时，定义余弦相似度为 $0$。\n\n您必须从基本原理出发，实现以下过程：\n- 计算每个客户端在共享初始化 $w_0 = 0$ 处的梯度，即 $g_i(w_0)$。\n- 使用阈值 $\\tau$ 将客户端聚类成连通分量：在客户端上创建一个无向图，如果 $\\mathrm{cos}(g_i(w_0), g_j(w_0)) \\ge \\tau$，则在 $i$ 和 $j$ 之间创建一条边，然后将图的连通分量作为簇。\n- 使用联邦平均 (Federated Averaging, FedAvg) 算法对两个联邦系统进行 $T$ 轮通信的训练。在每一轮中，每个客户端从服务器接收当前模型开始，在其本地数据上执行 $E$ 步批量梯度下降，步长（学习率）为 $\\eta$。服务器通过与 $n_i$ 成比例的加权平均来聚合客户端模型的增量。\n    - 单一全局模型：一个共享模型 $w$，通过聚合所有客户端进行更新。\n    - 特定于簇的模型：每个簇一个模型；每个簇的模型仅通过聚合该簇中的客户端进行更新。\n- 训练后，评估两个系统的客户端平均经验 MSE。对于单一全局模型，每个客户端都使用全局模型 $w$；对于特定于簇的模型，客户端 $i$ 使用其所属簇的模型 $w$。评估指标是每个客户端经验 MSE 的算术平均值：\n$$\n\\overline{L}_\\mathrm{global} = \\frac{1}{C} \\sum_{i=1}^{C} L_i(w_\\mathrm{global}), \\quad\n\\overline{L}_\\mathrm{cluster} = \\frac{1}{C} \\sum_{i=1}^{C} L_i(w_{\\mathrm{cluster}(i)}).\n$$\n计算相对改进量，以小数表示\n$$\n\\Delta = \\frac{\\overline{L}_\\mathrm{global} - \\overline{L}_\\mathrm{cluster}}{\\overline{L}_\\mathrm{global}}.\n$$\n报告每个测试用例的 $\\Delta$。\n\n需要使用的基本原理：\n- $L_i(w)$ 的经验风险最小化定义。\n- 梯度下降更新 $w \\leftarrow w - \\eta \\, g_i(w)$。\n- 按 $n_i$ 比例对客户端更新进行加权平均。\n- 通过梯度方向进行聚类的余弦相似度定义。\n\n实现一个完整、可运行的程序，该程序：\n- 按照规定模拟数据。\n- 计算初始梯度。\n- 通过阈值化余弦相似度和连通分量形成簇。\n- 对单一全局模型和特定于簇的模型使用联邦平均进行训练。\n- 评估并输出每个测试用例的相对改进量 $\\Delta$。\n\n测试套件和参数：\n- 测试用例 $1$ (正常路径，两个对立的簇):\n    - 随机种子 $42$，$C = 8$，$d = 5$，所有客户端样本大小 $n_i = 200$，噪声标准差 $\\sigma = 0.1$。\n    - 真实参数向量：客户端 $1$ 到 $4$ 为 $w^*_1 = [1, 0, 0, 0, 0]^\\top$，客户端 $5$ 到 $8$ 为 $w^*_2 = [-1, 0, 0, 0, 0]^\\top$。\n    - 聚类阈值 $\\tau = 0.7$。\n    - FedAvg 超参数：$T = 15$，$E = 5$，$\\eta = 0.05$。\n- 测试用例 $2$ (预计无改进，同质客户端):\n    - 随机种子 $123$，$C = 8$，$d = 5$，所有客户端样本大小 $n_i = 200$，噪声标准差 $\\sigma = 0.1$。\n    - 真实参数向量：所有客户端 $i$ 均为 $w^*_i = [1, 1, 0, 0, 0]^\\top$。\n    - 聚类阈值 $\\tau = 0.7$。\n    - FedAvg 超参数：$T = 15$，$E = 5$，$\\eta = 0.05$。\n- 测试用例 $3$ (边界聚类，$\\tau = 1.0$，单例簇):\n    - 随机种子 $7$，$C = 2$，$d = 3$，客户端样本大小 $n_1 = 300$，$n_2 = 300$，噪声标准差 $\\sigma = 0.1$。\n    - 真实参数向量：$w^*_1 = [1, 0, 0]^\\top$，$w^*_2 = [0, 1, 0]^\\top$。\n    - 聚类阈值 $\\tau = 1.0$。\n    - FedAvg 超参数：$T = 20$，$E = 5$，$\\eta = 0.05$。\n- 测试用例 $4$ (嘈杂的边缘情况，潜在的聚类模糊性):\n    - 随机种子 $99$，$C = 6$，$d = 4$，所有客户端样本大小 $n_i = 50$，噪声标准差 $\\sigma = 0.5$。\n    - 真实参数向量：客户端 $1$ 到 $3$ 为 $w^*_1 = [1, 0, 0, 0]^\\top$，客户端 $4$ 到 $6$ 为 $w^*_2 = [0, 1, 0, 0]^\\top$。\n    - 聚类阈值 $\\tau = 0.5$。\n    - FedAvg 超参数：$T = 25$，$E = 5$，$\\eta = 0.05$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，条目按上述四个测试用例的顺序排列，例如 $\\texttt{[r_1,r_2,r_3,r_4]}$，其中每个 $r_k$ 是测试用例 $k$ 的十进制相对改进量。",
            "solution": "经过全面审查，该问题被认为是有效的。它在科学上基于联邦学习、线性回归和图论的原理。问题陈述清晰，所有必要的参数和条件都已指定，通过固定的随机种子确保每个测试用例都有唯一且可复现的结果。问题陈述客观，没有歧义或矛盾。\n\n### 理论阐述\n\n这个问题的核心在于解决联邦学习中的客户端异构性。当客户端具有不同的底层数据分布或目标时，单一的全局模型对所有客户端而言可能不是最优的。所提出的解决方案是根据客户端的目标对其进行聚类，并为每个簇训练一个个性化模型。\n\n**1. 客户端损失和梯度**\n每个客户端 $i$ 寻求最小化其本地经验均方误差 (MSE) 损失，由下式给出：\n$$\nL_i(w) = \\frac{1}{n_i} \\sum_{t=1}^{n_i} \\left((x^{(i)}_t)^\\top w - y^{(i)}_t\\right)^2\n$$\n其中 $w \\in \\mathbb{R}^d$ 是模型参数向量。这是一个凸优化问题。该损失函数关于 $w$ 的梯度，用于梯度下降，为：\n$$\ng_i(w) = \\nabla_w L_i(w) = \\frac{2}{n_i} \\sum_{t=1}^{n_i} \\left((x^{(i)}_t)^\\top w - y^{(i)}_t\\right)x^{(i)}_t\n$$\n在矩阵表示法中，设 $X_i$ 为 $n_i \\times d$ 的数据矩阵，$Y_i$ 为 $n_i \\times 1$ 的标签向量，则梯度为：\n$$\ng_i(w) = \\frac{2}{n_i} X_i^\\top (X_i w - Y_i)\n$$\n\n**2. 基于梯度的聚类原理**\n聚类启发式方法基于在共享起始点 $w_0 = 0$ 处计算的初始梯度。在此初始化点，客户端 $i$ 的梯度简化为：\n$$\ng_i(w_0) = g_i(0) = -\\frac{2}{n_i} X_i^\\top Y_i\n$$\n代入真实的数据生成过程 $y^{(i)}_t = (x^{(i)}_t)^\\top w^*_i + \\epsilon^{(i)}_t$，我们得到：\n$$\ng_i(0) = -\\frac{2}{n_i} \\sum_{t=1}^{n_i} x^{(i)}_t \\left((x^{(i)}_t)^\\top w^*_i + \\epsilon^{(i)}_t\\right) = -\\frac{2}{n_i} \\left( \\left(\\sum_{t=1}^{n_i} x^{(i)}_t (x^{(i)}_t)^\\top \\right) w^*_i + \\sum_{t=1}^{n_i} x^{(i)}_t \\epsilon^{(i)}_t \\right)\n$$\n根据大数定律，对于足够大的样本数量 $n_i$，样本协方差矩阵 $\\frac{1}{n_i}\\sum_{t=1}^{n_i} x^{(i)}_t (x^{(i)}_t)^\\top$ 接近于真实的协方差矩阵，即单位矩阵 $I_d$，因为 $x^{(i)}_t \\sim \\mathcal{N}(0, I_d)$。噪声项 $\\frac{1}{n_i}\\sum_{t=1}^{n_i} x^{(i)}_t \\epsilon^{(i)}_t$ 接近其期望值，即 $E[x \\epsilon] = E[x]E[\\epsilon] = 0$。因此，我们有以下近似：\n$$\ng_i(0) \\approx -2 (I_d w^*_i) = -2w^*_i\n$$\n这揭示了一个关键的洞见：原点处的初始梯度向量近似与客户端真实最优参数向量 $w^*_i$ 的负值成正比。因此，两个客户端 $i$ 和 $j$ 的初始梯度之间的余弦相似度近似于其真实参数向量的余弦相似度：\n$$\n\\mathrm{cos}(g_i(0), g_j(0)) \\approx \\mathrm{cos}(-2w^*_i, -2w^*_j) = \\mathrm{cos}(w^*_i, w^*_j)\n$$\n因此，基于初始梯度的余弦相似度对客户端进行聚类，是将具有相似底层目标（$w^*_i$）的客户端分组的一种可靠的启发式方法。\n\n### 算法流程\n\n该解决方案是遵循指定流程从基本原理出发实现的。\n\n**1. 数据模拟**\n对于每个客户端 $i \\in \\{1, \\dots, C\\}$，我们生成一个本地数据集。\n-   特征向量 $\\{x^{(i)}_t\\}_{t=1}^{n_i}$ 从 $\\mathcal{N}(0, I_d)$ 中抽取。\n-   标签 $\\{y^{(i)}_t\\}_{t=1}^{n_i}$ 计算为 $y^{(i)}_t = (x^{(i)}_t)^\\top w^*_i + \\epsilon^{(i)}_t$，其中噪声项 $\\{\\epsilon^{(i)}_t\\}_{t=1}^{n_i}$ 从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取。\n为每个测试用例设置随机种子以确保可复现性。\n\n**2. 客户端聚类**\n-   将模型参数初始化为零向量，$w_0 = 0$。\n-   对于每个客户端 $i$，使用上面推导的公式计算初始梯度 $g_i(w_0)$。\n-   构建一个有 $C$ 个顶点的无向图，代表客户端。当且仅当客户端 $i$ 和客户端 $j$ 的初始梯度非零且其余弦相似度达到或超过阈值 $\\tau$ 时，在它们之间放置一条边：\n    $$\n    \\frac{g_i(w_0)^\\top g_j(w_0)}{\\|g_i(w_0)\\|_2 \\, \\|g_j(w_0)\\|_2} \\ge \\tau\n    $$\n    如果任一梯度为零向量，则相似度定义为 $0$，不创建边（除非 $\\tau \\le 0$）。\n-   通过寻找该图的连通分量来确定簇。这是通过构建邻接矩阵并使用 `scipy.sparse.csgraph.connected_components` 函数来实现的，该函数可以高效地对图的顶点进行分区。\n\n**3. 联邦平均 (FedAvg) 训练**\n我们模拟两个独立的训练系统，进行 $T$ 轮通信。\n\n-   **单一全局模型：** 单个服务器为所有 $C$ 个客户端管理一个模型 $w_{\\mathrm{global}}$。\n    -   在每一轮中，服务器将 $w_{\\mathrm{global}}$ 分发给所有客户端。\n    -   每个客户端 $i$ 从 $w_{\\mathrm{global}}$ 开始，使用其本地数据和学习率 $\\eta$ 执行 $E$ 步本地批量梯度下降。\n    -   服务器使用加权平均将更新后的本地模型聚合成一个新的全局模型，其中权重与客户端样本大小 $n_i$ 成正比：\n        $$ w_{\\mathrm{global}} \\leftarrow \\frac{\\sum_{i=1}^C n_i w_i}{\\sum_{i=1}^C n_i} $$\n\n-   **特定于簇的模型：** 为每个已识别的簇运行一个独立的 FedAvg 实例。\n    -   每个簇都有自己的服务器和模型 $w_{\\mathrm{cluster}(k)}$。\n    -   训练过程与全局模型类似，但簇 $k$ 的聚合仅涉及属于该簇的客户端。所有模型都初始化为 $w_0 = 0$。\n\n**4. 评估**\n经过 $T$ 轮训练后，评估两个系统的性能。\n-   全局模型的客户端平均经验 MSE $\\overline{L}_\\mathrm{global}$ 是通过对所有 $C$ 个客户端的最终损失 $L_i(w_\\mathrm{global})$ 进行平均计算得出的。\n-   聚类系统的客户端平均经验 MSE $\\overline{L}_\\mathrm{cluster}$ 的计算方法类似。对于每个客户端 $i$，使用其所属簇的最终模型计算损失 $L_i(w_{\\mathrm{cluster}(i)})$。\n-   然后，相对改进量 $\\Delta$ 计算如下：\n    $$\n    \\Delta = \\frac{\\overline{L}_\\mathrm{global} - \\overline{L}_\\mathrm{cluster}}{\\overline{L}_\\mathrm{global}}\n    $$\n该指标量化了通过聚类实现的性能增益。正值表示聚类方法产生了较低的平均客户端损失。",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.csgraph import connected_components\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"seed\": 42, \"C\": 8, \"d\": 5, \"n_i\": [200] * 8, \"sigma\": 0.1,\n            \"w_star\": [np.array([1., 0, 0, 0, 0])] * 4 + [np.array([-1., 0, 0, 0, 0])] * 4,\n            \"tau\": 0.7, \"T\": 15, \"E\": 5, \"eta\": 0.05\n        },\n        {\n            \"seed\": 123, \"C\": 8, \"d\": 5, \"n_i\": [200] * 8, \"sigma\": 0.1,\n            \"w_star\": [np.array([1., 1, 0, 0, 0])] * 8,\n            \"tau\": 0.7, \"T\": 15, \"E\": 5, \"eta\": 0.05\n        },\n        {\n            \"seed\": 7, \"C\": 2, \"d\": 3, \"n_i\": [300, 300], \"sigma\": 0.1,\n            \"w_star\": [np.array([1., 0, 0]), np.array([0., 1, 0])],\n            \"tau\": 1.0, \"T\": 20, \"E\": 5, \"eta\": 0.05\n        },\n        {\n            \"seed\": 99, \"C\": 6, \"d\": 4, \"n_i\": [50] * 6, \"sigma\": 0.5,\n            \"w_star\": [np.array([1., 0, 0, 0])] * 3 + [np.array([0., 1, 0, 0])] * 3,\n            \"tau\": 0.5, \"T\": 25, \"E\": 5, \"eta\": 0.05\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(params)\n        results.append(result)\n\n    # Format output as [r1,r2,r3,r4] with 8 decimal places.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef generate_data(C, d, n_i, w_star, sigma, rng):\n    \"\"\"Generates client data based on the problem specification.\"\"\"\n    client_data = []\n    for i in range(C):\n        n = n_i[i]\n        X = rng.normal(0, 1, size=(n, d))\n        epsilon = rng.normal(0, sigma, size=n)\n        y = X @ w_star[i] + epsilon\n        client_data.append({'X': X, 'y': y})\n    return client_data\n\ndef compute_gradient(client_data, w):\n    \"\"\"Computes the gradient of the MSE loss for a single client.\"\"\"\n    X, y = client_data['X'], client_data['y']\n    n = len(y)\n    if n == 0:\n        return np.zeros_like(w)\n    grad = 2/n * X.T @ (X @ w - y)\n    return grad\n\ndef find_clusters(initial_gradients, tau, C):\n    \"\"\"Forms clusters based on cosine similarity of initial gradients.\"\"\"\n    adj_matrix = np.zeros((C, C))\n    for i in range(C):\n        for j in range(i + 1, C):\n            g_i = initial_gradients[i]\n            g_j = initial_gradients[j]\n            norm_i = np.linalg.norm(g_i)\n            norm_j = np.linalg.norm(g_j)\n            \n            cosine_sim = 0.0\n            if norm_i > 0 and norm_j > 0:\n                cosine_sim = np.dot(g_i, g_j) / (norm_i * norm_j)\n            \n            if cosine_sim >= tau:\n                adj_matrix[i, j] = adj_matrix[j, i] = 1\n\n    graph = csr_matrix(adj_matrix)\n    n_components, labels = connected_components(\n        csgraph=graph, directed=False, return_labels=True\n    )\n    \n    clusters = [[] for _ in range(n_components)]\n    for client_idx, label in enumerate(labels):\n        clusters[label].append(client_idx)\n        \n    return clusters\n\ndef train_fedavg(client_data, client_indices, T, E, eta, d, n_i_all):\n    \"\"\"Runs the Federated Averaging algorithm for a given set of clients.\"\"\"\n    if not client_indices:\n        return np.zeros(d)\n        \n    w_server = np.zeros(d)\n    \n    sub_n_i = [n_i_all[i] for i in client_indices]\n    total_data_in_group = sum(sub_n_i)\n    \n    for _ in range(T):\n        local_models = []\n        for client_idx in client_indices:\n            w_local = w_server.copy()\n            data = client_data[client_idx]\n            for _ in range(E):\n                grad = compute_gradient(data, w_local)\n                w_local -= eta * grad\n            local_models.append(w_local)\n            \n        # Weighted aggregation\n        w_server = np.zeros(d)\n        for i, client_idx in enumerate(client_indices):\n            weight = n_i_all[client_idx] / total_data_in_group\n            w_server += weight * local_models[i]\n            \n    return w_server\n\ndef compute_loss(client_data, w):\n    \"\"\"Computes the empirical MSE loss.\"\"\"\n    X, y = client_data['X'], client_data['y']\n    n = len(y)\n    if n == 0:\n        return 0.0\n    error = X @ w - y\n    loss = np.mean(error**2)\n    return loss\n\ndef run_simulation(params):\n    \"\"\"Executes the full simulation for a single test case.\"\"\"\n    seed = params[\"seed\"]\n    C, d, n_i, sigma = params[\"C\"], params[\"d\"], params[\"n_i\"], params[\"sigma\"]\n    w_star, tau = params[\"w_star\"], params[\"tau\"]\n    T, E, eta = params[\"T\"], params[\"E\"], params[\"eta\"]\n\n    rng = np.random.default_rng(seed)\n    \n    # 1. Simulate data\n    all_client_data = generate_data(C, d, n_i, w_star, sigma, rng)\n\n    # 2. Compute initial gradients\n    w0 = np.zeros(d)\n    initial_gradients = [compute_gradient(all_client_data[i], w0) for i in range(C)]\n    \n    # 3. Form clusters\n    clusters = find_clusters(initial_gradients, tau, C)\n    client_to_cluster_map = {client: i for i, cluster in enumerate(clusters) for client in cluster}\n\n    # 4a. Train global model\n    all_clients = list(range(C))\n    w_global = train_fedavg(all_client_data, all_clients, T, E, eta, d, n_i)\n    \n    # 4b. Train cluster models\n    cluster_models = []\n    for cluster in clusters:\n        w_cluster = train_fedavg(all_client_data, cluster, T, E, eta, d, n_i)\n        cluster_models.append(w_cluster)\n\n    # 5. Evaluate final losses\n    total_loss_global = 0.0\n    for i in range(C):\n        total_loss_global += compute_loss(all_client_data[i], w_global)\n    mean_loss_global = total_loss_global / C\n\n    total_loss_cluster = 0.0\n    for i in range(C):\n        cluster_idx = client_to_cluster_map[i]\n        w_c = cluster_models[cluster_idx]\n        total_loss_cluster += compute_loss(all_client_data[i], w_c)\n    mean_loss_cluster = total_loss_cluster / C\n\n    # 6. Compute relative improvement\n    if mean_loss_global == 0:\n        # If global loss is zero, improvement is undefined unless cluster loss is also zero.\n        # This is highly unlikely with noise.\n        # A zero global loss implies a perfect model.\n        delta = 0.0 if mean_loss_cluster == 0.0 else -np.inf\n    else:\n        delta = (mean_loss_global - mean_loss_cluster) / mean_loss_global\n        \n    return delta\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}