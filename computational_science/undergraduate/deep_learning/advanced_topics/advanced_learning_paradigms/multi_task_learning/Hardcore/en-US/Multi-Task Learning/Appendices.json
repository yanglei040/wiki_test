{
    "hands_on_practices": [
        {
            "introduction": "A common challenge in multi-task learning arises when tasks are imbalanced, for example, in the amount of training data available for each. When naively summing the losses, the gradient from a task with a large dataset can overwhelm the learning signal from a smaller one. This exercise  lets you explore this phenomenon in a simplified computational model, comparing a standard approach with a common baseline technique: inverse-frequency weighting. You will gain hands-on insight into why balancing task contributions is critical for successful multi-task training.",
            "id": "3155106",
            "problem": "Consider a simplified two-task Multi-Task Learning (MTL) binary classification setting with shared parameters updated by Empirical Risk Minimization (ERM). The aggregated objective uses task weights and mini-batch sampling. Let the shared parameter vector be denoted by $\\theta$. Let there be two tasks $t \\in \\{A, B\\}$, each with a mini-batch of size $n_t$, per-sample gradient magnitude along the shared update direction $m_t$, and an alignment factor $a_t \\in [0,1]$ that quantifies how effectively the shared update transfers into task $t$ performance. Assume a single gradient step with learning rate $\\eta$ where the shared update magnitude (a scalar proxy for the norm of the update in the relevant direction) is modeled by\n$$\nU(w) \\equiv \\eta \\sum_{t \\in \\{A,B\\}} w_t \\, n_t \\, m_t,\n$$\nwhere $w_t$ denotes the task weight. Under uniform weights, set $w_A = w_B = 1$. Under inverse-frequency weighting, set $w_t = \\frac{1}{n_t}$. For this step, assume $\\eta$ equals $1$.\n\nEach task $t$ has an initial confusion matrix with counts $(\\mathrm{TP}_t, \\mathrm{FP}_t, \\mathrm{TN}_t, \\mathrm{FN}_t)$ that sum to the mini-batch size $n_t$. A positive step in the shared update magnitude tends to increase predicted positives, which, for each task $t$, moves counts according to sensitivities $s^{\\mathrm{TP}}_t$ and $s^{\\mathrm{FP}}_t$ (measured in counts per unit update). The per-task effective step is\n$$\n\\Delta_t \\equiv a_t \\, U(w).\n$$\nFor each task $t$, the confusion matrix updates as follows:\n- $\\Delta \\mathrm{TP}_t = \\mathrm{round}\\!\\left(s^{\\mathrm{TP}}_t \\, \\Delta_t\\right)$,\n- $\\Delta \\mathrm{FP}_t = \\mathrm{round}\\!\\left(s^{\\mathrm{FP}}_t \\, \\Delta_t\\right)$,\n- $\\mathrm{TP}_t^{\\mathrm{new}} = \\mathrm{clip}\\!\\left(\\mathrm{TP}_t + \\Delta \\mathrm{TP}_t, \\, 0, \\, \\mathrm{TP}_t + \\mathrm{FN}_t\\right)$,\n- $\\mathrm{FP}_t^{\\mathrm{new}} = \\mathrm{clip}\\!\\left(\\mathrm{FP}_t + \\Delta \\mathrm{FP}_t, \\, 0, \\, \\mathrm{FP}_t + \\mathrm{TN}_t\\right)$,\n- $\\mathrm{FN}_t^{\\mathrm{new}} = (\\mathrm{TP}_t + \\mathrm{FN}_t) - \\mathrm{TP}_t^{\\mathrm{new}}$,\n- $\\mathrm{TN}_t^{\\mathrm{new}} = (\\mathrm{FP}_t + \\mathrm{TN}_t) - \\mathrm{FP}_t^{\\mathrm{new}}$,\nwhere $\\mathrm{round}(\\cdot)$ denotes rounding to the nearest integer and $\\mathrm{clip}(x, a, b)$ caps $x$ to the range $[a,b]$. This preserves class totals and ensures valid nonnegative counts.\n\nDefine the F1-score (F1) for a class as the harmonic mean of precision and recall. For the positive class of task $t$,\n$$\n\\mathrm{precision}^+_t = \\frac{\\mathrm{TP}^{\\mathrm{new}}_t}{\\mathrm{TP}^{\\mathrm{new}}_t + \\mathrm{FP}^{\\mathrm{new}}_t}, \\quad\n\\mathrm{recall}^+_t = \\frac{\\mathrm{TP}^{\\mathrm{new}}_t}{\\mathrm{TP}^{\\mathrm{new}}_t + \\mathrm{FN}^{\\mathrm{new}}_t},\n$$\nwith the convention that any ratio with a zero denominator is defined as $0$. Then\n$$\n\\mathrm{F1}^+_t = \n\\begin{cases}\n\\frac{2 \\cdot \\mathrm{precision}^+_t \\cdot \\mathrm{recall}^+_t}{\\mathrm{precision}^+_t + \\mathrm{recall}^+_t},  \\text{if } \\mathrm{precision}^+_t + \\mathrm{recall}^+_t  0, \\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\nFor the negative class (treating \"negative\" as the positive label), the corresponding counts are $\\mathrm{TP}^{-}_t = \\mathrm{TN}^{\\mathrm{new}}_t$, $\\mathrm{FP}^{-}_t = \\mathrm{FN}^{\\mathrm{new}}_t$, $\\mathrm{FN}^{-}_t = \\mathrm{FP}^{\\mathrm{new}}_t$, and $\\mathrm{TN}^{-}_t = \\mathrm{TP}^{\\mathrm{new}}_t$, yielding\n$$\n\\mathrm{precision}^{-}_t = \\frac{\\mathrm{TN}^{\\mathrm{new}}_t}{\\mathrm{TN}^{\\mathrm{new}}_t + \\mathrm{FN}^{\\mathrm{new}}_t}, \\quad\n\\mathrm{recall}^{-}_t = \\frac{\\mathrm{TN}^{\\mathrm{new}}_t}{\\mathrm{TN}^{\\mathrm{new}}_t + \\mathrm{FP}^{\\mathrm{new}}_t},\n$$\nand\n$$\n\\mathrm{F1}^{-}_t = \n\\begin{cases}\n\\frac{2 \\cdot \\mathrm{precision}^{-}_t \\cdot \\mathrm{recall}^{-}_t}{\\mathrm{precision}^{-}_t + \\mathrm{recall}^{-}_t},  \\text{if } \\mathrm{precision}^{-}_t + \\mathrm{recall}^{-}_t  0, \\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\nThe macro-averaged F1 for task $t$ is then\n$$\n\\mathrm{MacroF1}_t = \\frac{\\mathrm{F1}^+_t + \\mathrm{F1}^{-}_t}{2}.\n$$\nDefine the cross-task macro average as\n$$\n\\mathrm{CrossMacroF1} = \\frac{\\mathrm{MacroF1}_A + \\mathrm{MacroF1}_B}{2}.\n$$\n\nTo analyze the effect of label frequency mismatch, compute the ratio of task contributions to the shared update under the two weighting schemes as\n$$\nR_{\\mathrm{unweighted}} = \\frac{n_A \\, m_A}{n_B \\, m_B}, \\quad\nR_{\\mathrm{weighted}} = \\frac{w_A \\, n_A \\, m_A}{w_B \\, n_B \\, m_B},\n$$\nwith $w_A = w_B = 1$ for the unweighted case and $w_t = \\frac{1}{n_t}$ for the inverse-frequency case.\n\nYour task is to implement a program that, for each test case below, performs:\n- Computes $R_{\\mathrm{unweighted}}$ and $R_{\\mathrm{weighted}}$ as defined.\n- Applies one shared update step under both weighting schemes to obtain the updated confusion matrices for tasks $A$ and $B$.\n- Computes $\\mathrm{CrossMacroF1}$ under both schemes.\n- Outputs a boolean that is $\\mathrm{True}$ if the inverse-frequency weighting strictly increases $\\mathrm{CrossMacroF1}$ compared to the unweighted case, and $\\mathrm{False}$ otherwise.\n\nTest suite (all counts and parameters are integers or real numbers, as indicated):\n- Test case $1$:\n  - $n_A = 100$, $n_B = 100$.\n  - $m_A = 1.0$, $m_B = 1.0$.\n  - $a_A = 0.8$, $a_B = 0.8$.\n  - Initial confusion for task $A$: $(\\mathrm{TP}_A, \\mathrm{FP}_A, \\mathrm{TN}_A, \\mathrm{FN}_A) = (50, 10, 30, 10)$, sensitivities $s^{\\mathrm{TP}}_A = 0.05$, $s^{\\mathrm{FP}}_A = 0.02$.\n  - Initial confusion for task $B$: $(\\mathrm{TP}_B, \\mathrm{FP}_B, \\mathrm{TN}_B, \\mathrm{FN}_B) = (45, 15, 25, 15)$, sensitivities $s^{\\mathrm{TP}}_B = 0.06$, $s^{\\mathrm{FP}}_B = 0.03$.\n- Test case $2$:\n  - $n_A = 1000$, $n_B = 100$.\n  - $m_A = 0.8$, $m_B = 1.2$.\n  - $a_A = 0.6$, $a_B = 0.9$.\n  - Initial confusion for task $A$: $(\\mathrm{TP}_A, \\mathrm{FP}_A, \\mathrm{TN}_A, \\mathrm{FN}_A) = (400, 100, 450, 50)$, sensitivities $s^{\\mathrm{TP}}_A = 0.02$, $s^{\\mathrm{FP}}_A = 0.01$.\n  - Initial confusion for task $B$: $(\\mathrm{TP}_B, \\mathrm{FP}_B, \\mathrm{TN}_B, \\mathrm{FN}_B) = (50, 20, 20, 10)$, sensitivities $s^{\\mathrm{TP}}_B = 0.07$, $s^{\\mathrm{FP}}_B = 0.04$.\n- Test case $3$:\n  - $n_A = 10000$, $n_B = 10$.\n  - $m_A = 0.5$, $m_B = 1.5$.\n  - $a_A = 0.5$, $a_B = 1.0$.\n  - Initial confusion for task $A$: $(\\mathrm{TP}_A, \\mathrm{FP}_A, \\mathrm{TN}_A, \\mathrm{FN}_A) = (4500, 500, 5000, 0)$, sensitivities $s^{\\mathrm{TP}}_A = 0.005$, $s^{\\mathrm{FP}}_A = 0.002$.\n  - Initial confusion for task $B$: $(\\mathrm{TP}_B, \\mathrm{FP}_B, \\mathrm{TN}_B, \\mathrm{FN}_B) = (4, 1, 4, 1)$, sensitivities $s^{\\mathrm{TP}}_B = 0.2$, $s^{\\mathrm{FP}}_B = 0.1$.\n\nFinal output format:\n- Your program should produce a single line of output containing a comma-separated flat list with, for each test case in order, the following values appended sequentially: $R_{\\mathrm{unweighted}}$, $R_{\\mathrm{weighted}}$, $\\mathrm{CrossMacroF1}$ under uniform weights, $\\mathrm{CrossMacroF1}$ under inverse-frequency weights, and the boolean improvement indicator. For example, the output must look like $[r_1, r'_1, m_1, m'_1, b_1, r_2, r'_2, m_2, m'_2, b_2, r_3, r'_3, m_3, m'_3, b_3]$, where each $r_i$ and $r'_i$ are floats, each $m_i$ and $m'_i$ are floats, and each $b_i$ is a boolean. No additional text should be printed.",
            "solution": "The problem statement has been evaluated and is determined to be **valid**. It is a well-posed, self-contained, and scientifically grounded computational problem based on a simplified model of multi-task learning. All necessary parameters, formulas, and conditions are provided, allowing for a unique and verifiable solution. The problem is free of contradictions, ambiguities, and non-formalizable concepts.\n\nThe solution proceeds by first establishing a systematic computational procedure to calculate the `CrossMacroF1` score for a given weighting scheme. This procedure is then applied twice for each test case—once with uniform weights and once with inverse-frequency weights—to determine if the latter provides a strict improvement.\n\nThe core of the calculation involves a single update step for a two-task learning system. The procedure is as follows:\n\n1.  **Task Weighting and Shared Update:**\n    The contribution of each task to the shared parameter update depends on a weighting scheme. The problem defines two schemes: uniform weighting and inverse-frequency weighting.\n    -   For uniform weighting, the task weights are $w_A = 1$ and $w_B = 1$.\n    -   For inverse-frequency weighting, the weights are inversely proportional to the mini-batch sizes: $w_t = \\frac{1}{n_t}$.\n\n    The total magnitude of the shared update, $U(w)$, is a weighted sum of the task-specific gradient contributions, with the learning rate $\\eta=1$:\n    $$U(w) = \\sum_{t \\in \\{A,B\\}} w_t \\, n_t \\, m_t$$\n    where $n_t$ is the mini-batch size and $m_t$ is the per-sample gradient magnitude for task $t$.\n\n2.  **Per-Task Effective Update:**\n    The shared update does not affect each task equally. An alignment factor, $a_t \\in [0, 1]$, models how effectively the shared update translates into performance improvement for task $t$. The per-task effective step, $\\Delta_t$, is:\n    $$\\Delta_t = a_t \\, U(w)$$\n\n3.  **Confusion Matrix Update:**\n    The effective step $\\Delta_t$ alters the counts in the initial confusion matrix $(\\mathrm{TP}_t, \\mathrm{FP}_t, \\mathrm{TN}_t, \\mathrm{FN}_t)$ for each task. The changes to the true positive and false positive counts are determined by task-specific sensitivities, $s^{\\mathrm{TP}}_t$ and $s^{\\mathrm{FP}}_t$:\n    $$ \\Delta \\mathrm{TP}_t = \\mathrm{round}\\!\\left(s^{\\mathrm{TP}}_t \\, \\Delta_t\\right) $$\n    $$ \\Delta \\mathrm{FP}_t = \\mathrm{round}\\!\\left(s^{\\mathrm{FP}}_t \\, \\Delta_t\\right) $$\n    where $\\mathrm{round}(\\cdot)$ rounds to the nearest integer.\n\n    The updated counts, denoted with a superscript `new`, are calculated by applying these changes and ensuring they remain within valid bounds using a clipping function. The total number of actual positive instances ($P_t = \\mathrm{TP}_t + \\mathrm{FN}_t$) and actual negative instances ($N_t = \\mathrm{FP}_t + \\mathrm{TN}_t$) are conserved.\n    $$ \\mathrm{TP}_t^{\\mathrm{new}} = \\mathrm{clip}\\!\\left(\\mathrm{TP}_t + \\Delta \\mathrm{TP}_t, \\, 0, \\, P_t\\right) $$\n    $$ \\mathrm{FP}_t^{\\mathrm{new}} = \\mathrm{clip}\\!\\left(\\mathrm{FP}_t + \\Delta \\mathrm{FP}_t, \\, 0, \\, N_t\\right) $$\n    The remaining confusion matrix elements are derived from the conservation of class totals:\n    $$ \\mathrm{FN}_t^{\\mathrm{new}} = P_t - \\mathrm{TP}_t^{\\mathrm{new}} $$\n    $$ \\mathrm{TN}_t^{\\mathrm{new}} = N_t - \\mathrm{FP}_t^{\\mathrm{new}} $$\n\n4.  **F1-Score Calculation:**\n    The performance is measured using the macro-averaged F1-score. This requires computing the F1-score for both the positive and negative classes. For any class, given its corresponding true positives ($tp$), false positives ($fp$), and false negatives ($fn$), the precision and recall are:\n    $$ \\mathrm{precision} = \\frac{tp}{tp + fp}, \\quad \\mathrm{recall} = \\frac{tp}{tp + fn} $$\n    with the convention that a ratio is $0$ if its denominator is $0$. The F1-score is the harmonic mean of precision and recall:\n    $$ \\mathrm{F1} = \\begin{cases} \\frac{2 \\cdot \\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}},  \\text{if } \\mathrm{precision} + \\mathrm{recall}  0 \\\\ 0,  \\text{otherwise} \\end{cases} $$\n    -   For the positive class ($\\mathrm{F1}_t^+$), the terms are $tp = \\mathrm{TP}_t^{\\mathrm{new}}$, $fp = \\mathrm{FP}_t^{\\mathrm{new}}$, and $fn = \\mathrm{FN}_t^{\\mathrm{new}}$.\n    -   For the negative class ($\\mathrm{F1}_t^-$), the terms are $tp = \\mathrm{TN}_t^{\\mathrm{new}}$, $fp = \\mathrm{FN}_t^{\\mathrm{new}}$, and $fn = \\mathrm{FP}_t^{\\mathrm{new}}$.\n\n5.  **Aggregate Performance Metrics:**\n    The per-task macro-averaged F1-score is the average of the positive and negative class F1-scores:\n    $$ \\mathrm{MacroF1}_t = \\frac{\\mathrm{F1}_t^+ + \\mathrm{F1}_t^-}{2} $$\n    The final performance metric is the cross-task macro-average, which averages the macro-F1 scores of both tasks:\n    $$ \\mathrm{CrossMacroF1} = \\frac{\\mathrm{MacroF1}_A + \\mathrm{MacroF1}_B}{2} $$\n\n6.  **Analysis Ratios and Final Comparison:**\n    To analyze the balance of task contributions, two ratios are computed:\n    $$ R_{\\mathrm{unweighted}} = \\frac{n_A \\, m_A}{n_B \\, m_B} $$\n    $$ R_{\\mathrm{weighted}} = \\frac{w_A \\, n_A \\, m_A}{w_B \\, n_B \\, m_B} = \\frac{m_A}{m_B} \\quad (\\text{for inverse-frequency weighting}) $$\n    For each test case, we compute $R_{\\mathrm{unweighted}}$, $R_{\\mathrm{weighted}}$, and the $\\mathrm{CrossMacroF1}$ for both uniform and inverse-frequency schemes. A boolean value is then determined: $\\mathrm{True}$ if the inverse-frequency weighted score is strictly greater than the uniform weighted score, and $\\mathrm{False}$ otherwise. This entire sequence of calculations is implemented for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the multi-task learning problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"nA\": 100, \"nB\": 100,\n            \"mA\": 1.0, \"mB\": 1.0,\n            \"aA\": 0.8, \"aB\": 0.8,\n            \"cm_A\": (50, 10, 30, 10), \"sens_A\": (0.05, 0.02),\n            \"cm_B\": (45, 15, 25, 15), \"sens_B\": (0.06, 0.03),\n        },\n        {\n            \"nA\": 1000, \"nB\": 100,\n            \"mA\": 0.8, \"mB\": 1.2,\n            \"aA\": 0.6, \"aB\": 0.9,\n            \"cm_A\": (400, 100, 450, 50), \"sens_A\": (0.02, 0.01),\n            \"cm_B\": (50, 20, 20, 10), \"sens_B\": (0.07, 0.04),\n        },\n        {\n            \"nA\": 10000, \"nB\": 10,\n            \"mA\": 0.5, \"mB\": 1.5,\n            \"aA\": 0.5, \"aB\": 1.0,\n            \"cm_A\": (4500, 500, 5000, 0), \"sens_A\": (0.005, 0.002),\n            \"cm_B\": (4, 1, 4, 1), \"sens_B\": (0.2, 0.1),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Calculate R_unweighted and R_weighted\n        r_unweighted = (case[\"nA\"] * case[\"mA\"]) / (case[\"nB\"] * case[\"mB\"])\n        r_weighted = case[\"mA\"] / case[\"mB\"]\n\n        # Calculate CrossMacroF1 for both weighting schemes\n        cmf1_uniform = run_simulation(case, \"uniform\")\n        cmf1_inv_freq = run_simulation(case, \"inv_freq\")\n\n        # Determine if inverse-frequency weighting provides a strict improvement\n        is_improvement = cmf1_inv_freq > cmf1_uniform\n\n        # Append results for this test case\n        results.extend([r_unweighted, r_weighted, cmf1_uniform, cmf1_inv_freq, is_improvement])\n\n    # Format and print the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_f1_score(tp, fp, tn, fn):\n    \"\"\"\n    Calculates the F1-score for a given set of confusion matrix counts.\n    Handles division by zero as per the problem specification.\n    \"\"\"\n    prec_denom = tp + fp\n    rec_denom = tp + fn\n\n    precision = tp / prec_denom if prec_denom > 0 else 0.0\n    recall = tp / rec_denom if rec_denom > 0 else 0.0\n\n    f1_denom = precision + recall\n    f1 = (2 * precision * recall) / f1_denom if f1_denom > 0 else 0.0\n\n    return f1\n\ndef run_simulation(params, weighting_scheme):\n    \"\"\"\n    Runs the simulation for a single test case and weighting scheme.\n    Returns the final CrossMacroF1 score.\n    \"\"\"\n    # Unpack parameters\n    nA, nB = params[\"nA\"], params[\"nB\"]\n    mA, mB = params[\"mA\"], params[\"mB\"]\n    aA, aB = params[\"aA\"], params[\"aB\"]\n    tpA, fpA, tnA, fnA = params[\"cm_A\"]\n    s_tpA, s_fpA = params[\"sens_A\"]\n    tpB, fpB, tnB, fnB = params[\"cm_B\"]\n    s_tpB, s_fpB = params[\"sens_B\"]\n    \n    # 1. Task Weighting\n    if weighting_scheme == \"uniform\":\n        wA, wB = 1.0, 1.0\n    elif weighting_scheme == \"inv_freq\":\n        wA, wB = 1.0 / nA, 1.0 / nB\n    else:\n        raise ValueError(\"Invalid weighting scheme\")\n\n    # 2. Shared Update Magnitude\n    U = (wA * nA * mA) + (wB * nB * mB) # eta is 1\n\n    # 3. Per-Task Effective Update\n    delta_A = aA * U\n    delta_B = aB * U\n\n    # --- Task A Calculations ---\n    # 4. Update Confusion Matrix for Task A\n    delta_tpA = np.round(s_tpA * delta_A)\n    delta_fpA = np.round(s_fpA * delta_A)\n    \n    total_pos_A = tpA + fnA\n    total_neg_A = fpA + tnA\n\n    tpA_new = np.clip(tpA + delta_tpA, 0, total_pos_A)\n    fpA_new = np.clip(fpA + delta_fpA, 0, total_neg_A)\n    fnA_new = total_pos_A - tpA_new\n    tnA_new = total_neg_A - fpA_new\n\n    # 5. F1-Score Calculation for Task A\n    f1_pos_A = compute_f1_score(tpA_new, fpA_new, tnA_new, fnA_new)\n    # For negative class, TN becomes TP, FN becomes FP, FP becomes FN\n    f1_neg_A = compute_f1_score(tnA_new, fnA_new, tpA_new, fpA_new)\n    \n    # 6. Aggregate Performance for Task A\n    macro_f1_A = (f1_pos_A + f1_neg_A) / 2.0\n\n    # --- Task B Calculations ---\n    # 4. Update Confusion Matrix for Task B\n    delta_tpB = np.round(s_tpB * delta_B)\n    delta_fpB = np.round(s_fpB * delta_B)\n\n    total_pos_B = tpB + fnB\n    total_neg_B = fpB + tnB\n\n    tpB_new = np.clip(tpB + delta_tpB, 0, total_pos_B)\n    fpB_new = np.clip(fpB + delta_fpB, 0, total_neg_B)\n    fnB_new = total_pos_B - tpB_new\n    tnB_new = total_neg_B - fpB_new\n\n    # 5. F1-Score Calculation for Task B\n    f1_pos_B = compute_f1_score(tpB_new, fpB_new, tnB_new, fnB_new)\n    # For negative class, TN becomes TP, FN becomes FP, FP becomes FN\n    f1_neg_B = compute_f1_score(tnB_new, fnB_new, tpB_new, fpB_new)\n\n    # 6. Aggregate Performance for Task B\n    macro_f1_B = (f1_pos_B + f1_neg_B) / 2.0\n\n    # 7. Final Cross-Task Metric\n    cross_macro_f1 = (macro_f1_A + macro_f1_B) / 2.0\n    \n    return cross_macro_f1\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        },
        {
            "introduction": "Beyond the magnitude of gradients, their direction is also crucial. When gradients from different tasks point in conflicting directions, they can create interference that slows down or even prevents learning, a phenomenon known as negative transfer. This hands-on practice  introduces a direct method for resolving this issue called gradient surgery. By implementing this technique, which projects one gradient to be orthogonal to another, you will see how explicitly managing gradient conflict can accelerate convergence and improve model performance.",
            "id": "3155105",
            "problem": "Consider a two-task multi-task learning problem in deep learning where both tasks share a parameter vector $w \\in \\mathbb{R}^d$. Each task $i \\in \\{1,2\\}$ has a Mean Squared Error (MSE) loss $L_i(w)$ defined on a dataset $(X_i, y_i)$ with $n_i$ samples, where $X_i \\in \\mathbb{R}^{n_i \\times d}$ and $y_i \\in \\mathbb{R}^{n_i}$. The MSE loss is given by\n$$\nL_i(w) = \\frac{1}{2 n_i} \\left\\| X_i w - y_i \\right\\|_2^2.\n$$\nGradient descent updates the parameter vector using an aggregated gradient and a fixed learning rate $\\eta  0$. In the vanilla aggregated regime, the algorithm uses the sum of per-task gradients, while in the orthogonalized regime, the algorithm replaces one task's gradient by its component orthogonal to the other task's gradient, and then aggregates.\n\nYour task is to implement the following two training regimens and compare their convergence speeds:\n\n- Vanilla aggregated gradient descent: at each iteration $t$, compute the per-task gradients $\\nabla L_1(w_t)$ and $\\nabla L_2(w_t)$ and update\n$$\nw_{t+1} = w_t - \\eta \\left( \\nabla L_1(w_t) + \\nabla L_2(w_t) \\right).\n$$\n\n- Orthogonalization-based gradient surgery: at each iteration $t$, compute $\\nabla L_1(w_t)$ and $\\nabla L_2(w_t)$, replace the gradient of task $1$ by its component orthogonal to the gradient of task $2$, then update using the sum of the modified gradient for task $1$ and the unmodified gradient for task $2$. If $\\left\\|\\nabla L_2(w_t)\\right\\|_2 = 0$, leave $\\nabla L_1(w_t)$ unchanged before aggregation.\n\nFor both regimens, start from the same initial parameter vector $w_0$ and run iterations until the stopping criterion\n$$\nL_1(w_t) + L_2(w_t) \\le \\varepsilon\n$$\nis first satisfied, or a maximum number of iterations $T_{\\max}$ is reached. Record the number of iterations taken to satisfy the stopping criterion (or record $T_{\\max}$ if it is not satisfied within that limit).\n\nImplement the program to evaluate the following five test cases, each specified by $(X_1, y_1)$ and $(X_2, y_2)$ with shared dimension $d = 2$, initial parameter $w_0$, learning rate $\\eta$, tolerance $\\varepsilon$, and maximum iterations $T_{\\max}$:\n\n- Test Case $1$ (moderate conflict):\n  - $d = 2$,\n  $$ X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}, \\quad y_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} $$\n  $$ X_2 = \\begin{bmatrix} 2  1 \\\\ 1  2 \\\\ 1  -1 \\end{bmatrix}, \\quad y_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix} $$\n\n- Test Case $2$ (aligned per-task gradients at $w_0$):\n  - $d = 2$,\n  $$ X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}, \\quad y_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} $$\n  $$ X_2 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}, \\quad y_2 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} $$\n\n- Test Case $3$ (orthogonal per-task gradients at $w_0$):\n  - $d = 2$,\n  $$ X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}, \\quad y_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} $$\n  $$ X_2 = \\begin{bmatrix} 0  1 \\\\ 0  1 \\end{bmatrix}, \\quad y_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $$\n\n- Test Case $4$ (anti-parallel per-task gradients at $w_0$):\n  - $d = 2$,\n  $$ X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}, \\quad y_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} $$\n  $$ X_2 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}, \\quad y_2 = \\begin{bmatrix} -1 \\\\ -2 \\\\ -3 \\end{bmatrix} $$\n\n- Test Case $5$ (zero gradient for task $2$ at $w_0$):\n  - $d = 2$,\n  $$ X_1 = \\begin{bmatrix} 1  1 \\\\ 1  -1 \\end{bmatrix}, \\quad y_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} $$\n  $$ X_2 = \\begin{bmatrix} 1  2 \\\\ 3  4 \\end{bmatrix}, \\quad y_2 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} $$\n\nCommon hyperparameters for all tests:\n$$ w_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} $$\n- $\\eta = 0.05$,\n- $\\varepsilon = 10^{-6}$,\n- $T_{\\max} = 10000$.\n\nYour program must, for each test case $k \\in \\{1,2,3,4,5\\}$:\n- Run vanilla aggregated gradient descent and record $N^{(k)}_{\\mathrm{vanilla}}$, the number of iterations to reach $L_1(w_t) + L_2(w_t) \\le \\varepsilon$, or $T_{\\max}$ if not reached.\n- Run orthogonalization-based gradient surgery (orthogonalizing task $1$’s gradient with respect to task $2$’s gradient) and record $N^{(k)}_{\\mathrm{ortho}}$, similarly defined.\n- Compute the integer difference $\\Delta^{(k)} = N^{(k)}_{\\mathrm{vanilla}} - N^{(k)}_{\\mathrm{ortho}}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by test case index $k = 1,2,3,4,5$, that is, print\n$$\n\\left[ \\Delta^{(1)}, \\Delta^{(2)}, \\Delta^{(3)}, \\Delta^{(4)}, \\Delta^{(5)} \\right].\n$$",
            "solution": "The problem is scientifically and mathematically well-posed, providing all necessary information to proceed with a solution. It is grounded in the established principles of numerical optimization and linear algebra as applied to multi-task learning.\n\nThe objective is to compare the convergence rates of two gradient descent algorithms for a two-task linear regression problem. The comparison is quantified by the number of iterations required to reach a specific tolerance on the total loss.\n\nFirst, we formalize the mathematical components required for both algorithms. The loss function for each task, $i \\in \\{1, 2\\}$, is the Mean Squared Error (MSE):\n$$\nL_i(w) = \\frac{1}{2 n_i} \\left\\| X_i w - y_i \\right\\|_2^2\n$$\nwhere $w \\in \\mathbb{R}^d$ is the shared parameter vector, $X_i \\in \\mathbb{R}^{n_i \\times d}$ is the data matrix, and $y_i \\in \\mathbb{R}^{n_i}$ is the target vector for task $i$.\n\nThe core of any gradient descent method is the gradient of the loss function. We derive the gradient of $L_i(w)$ with respect to $w$. Rewriting the squared norm as a dot product, $L_i(w) = \\frac{1}{2 n_i} (X_i w - y_i)^T (X_i w - y_i)$. Expanding this gives:\n$$\nL_i(w) = \\frac{1}{2 n_i} (w^T X_i^T X_i w - 2 y_i^T X_i w + y_i^T y_i)\n$$\nTaking the gradient with respect to $w$ using standard matrix calculus identities, we obtain:\n$$\n\\nabla_w L_i(w) = \\frac{1}{2 n_i} (2 X_i^T X_i w - 2 X_i^T y_i) = \\frac{1}{n_i} X_i^T (X_i w - y_i)\n$$\nLet us denote the per-task gradients at iteration $t$ as $g_1(w_t) = \\nabla_w L_1(w_t)$ and $g_2(w_t) = \\nabla_w L_2(w_t)$.\n\nWith the gradient defined, we specify the two algorithmic regimens.\n\n**1. Vanilla Aggregated Gradient Descent**\nThis is the standard approach in multi-task learning, where the gradients from all tasks are summed to form the update direction. The update rule is:\n$$\nw_{t+1} = w_t - \\eta \\left( g_1(w_t) + g_2(w_t) \\right)\n$$\nThis method is simple but can suffer from slow convergence if the task gradients are conflicting (i.e., point in opposing directions), as the aggregated gradient magnitude may be small.\n\n**2. Orthogonalization-Based Gradient Surgery**\nThis method aims to mitigate the issue of conflicting gradients. The gradient for one task (task $1$) is modified by removing its component that is parallel to the gradient of the other task (task $2$). This ensures that the update for task $1$ does not interfere with the gradient-indicated direction for task $2$.\n\nThe component of $g_1(w_t)$ that is parallel to $g_2(w_t)$ is found by projecting $g_1(w_t)$ onto $g_2(w_t)$:\n$$\n\\text{proj}_{g_2(w_t)}(g_1(w_t)) = \\frac{g_1(w_t) \\cdot g_2(w_t)}{\\|g_2(w_t)\\|_2^2} g_2(w_t)\n$$\nThe modified gradient for task $1$, denoted $g_1^{\\perp}(w_t)$, is the component of $g_1(w_t)$ orthogonal to $g_2(w_t)$:\n$$\ng_1^{\\perp}(w_t) = g_1(w_t) - \\text{proj}_{g_2(w_t)}(g_1(w_t)) = g_1(w_t) - \\frac{g_1(w_t) \\cdot g_2(w_t)}{\\|g_2(w_t)\\|_2^2} g_2(w_t)\n$$\nAs specified, if $\\|g_2(w_t)\\|_2 = 0$, the projection is undefined. In this case, we do not modify the task $1$ gradient, setting $g_1^{\\perp}(w_t) = g_1(w_t)$. The update rule then becomes:\n$$\nw_{t+1} = w_t - \\eta \\left( g_1^{\\perp}(w_t) + g_2(w_t) \\right)\n$$\n\n**Computational Procedure**\nFor each of the five test cases and for each of the two algorithms, an iterative optimization is performed.\n1. Initialize the parameters $w_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n2. For each iteration $t$ from $0$ to $T_{\\max} - 1$:\n   a. Calculate the total loss $L_{total}(w_t) = L_1(w_t) + L_2(w_t)$.\n   b. Check the stopping criterion: If $L_{total}(w_t) \\le \\varepsilon = 10^{-6}$, the loop terminates, and the number of iterations taken is recorded as $t$.\n   c. Calculate the per-task gradients $g_1(w_t)$ and $g_2(w_t)$.\n   d. Compute the parameter update $w_{t+1}$ according to the rules of the specific algorithm (Vanilla or Orthogonalized).\n3. If the loop completes without meeting the stopping criterion, the number of iterations is recorded as $T_{\\max} = 10000$.\n\nLet $N^{(k)}_{\\mathrm{vanilla}}$ and $N^{(k)}_{\\mathrm{ortho}}$ be the number of iterations recorded for the vanilla and orthogonalization algorithms, respectively, for test case $k \\in \\{1, 2, 3, 4, 5\\}$. The final output for each case is the integer difference $\\Delta^{(k)} = N^{(k)}_{\\mathrm{vanilla}} - N^{(k)}_{\\mathrm{ortho}}$. This value provides a direct comparison of their convergence speeds.",
            "answer": "```python\nimport numpy as np\n\ndef loss_fn(X, y, w):\n    \"\"\"Computes the Mean Squared Error loss.\"\"\"\n    n = X.shape[0]\n    if n == 0:\n        return 0.0\n    error = X @ w - y\n    return (1.0 / (2.0 * n)) * np.dot(error, error)\n\ndef grad_fn(X, y, w):\n    \"\"\"Computes the gradient of the MSE loss.\"\"\"\n    n = X.shape[0]\n    if n == 0:\n        return np.zeros_like(w)\n    error = X @ w - y\n    return (1.0 / n) * X.T @ error\n\ndef solve():\n    \"\"\"\n    Implements and compares vanilla and orthogonalized gradient descent for\n    several multi-task learning test cases.\n    \"\"\"\n    \n    # Common hyperparameters\n    w0 = np.array([0.0, 0.0])\n    eta = 0.05\n    epsilon = 1e-6\n    T_max = 10000\n\n    # Test cases definition\n    test_cases = [\n        # Test Case 1 (moderate conflict)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y1\": np.array([1.0, 2.0, 3.0]),\n            \"X2\": np.array([[2.0, 1.0], [1.0, 2.0], [1.0, -1.0]]),\n            \"y2\": np.array([0.0, 1.0, -1.0]),\n        },\n        # Test Case 2 (aligned gradients)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y1\": np.array([1.0, 2.0, 3.0]),\n            \"X2\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y2\": np.array([1.0, 2.0, 3.0]),\n        },\n        # Test Case 3 (orthogonal gradients at w0)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"y1\": np.array([1.0, 0.0]),\n            \"X2\": np.array([[0.0, 1.0], [0.0, 1.0]]),\n            \"y2\": np.array([1.0, 1.0]),\n        },\n        # Test Case 4 (anti-parallel gradients at w0)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y1\": np.array([1.0, 2.0, 3.0]),\n            \"X2\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y2\": np.array([-1.0, -2.0, -3.0]),\n        },\n        # Test Case 5 (zero gradient for task 2 at w0)\n        {\n            \"X1\": np.array([[1.0, 1.0], [1.0, -1.0]]),\n            \"y1\": np.array([1.0, 0.0]),\n            \"X2\": np.array([[1.0, 2.0], [3.0, 4.0]]),\n            \"y2\": np.array([0.0, 0.0]),\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        X1, y1 = case[\"X1\"], case[\"y1\"]\n        X2, y2 = case[\"X2\"], case[\"y2\"]\n\n        # Run Vanilla Aggregated Gradient Descent\n        w = np.copy(w0)\n        n_vanilla = T_max\n        for t in range(T_max):\n            loss1 = loss_fn(X1, y1, w)\n            loss2 = loss_fn(X2, y2, w)\n            if loss1 + loss2 = epsilon:\n                n_vanilla = t\n                break\n\n            g1 = grad_fn(X1, y1, w)\n            g2 = grad_fn(X2, y2, w)\n            w -= eta * (g1 + g2)\n\n        # Run Orthogonalization-based Gradient Surgery\n        w = np.copy(w0)\n        n_ortho = T_max\n        for t in range(T_max):\n            loss1 = loss_fn(X1, y1, w)\n            loss2 = loss_fn(X2, y2, w)\n            if loss1 + loss2 = epsilon:\n                n_ortho = t\n                break\n\n            g1 = grad_fn(X1, y1, w)\n            g2 = grad_fn(X2, y2, w)\n            \n            g2_norm_sq = np.dot(g2, g2)\n            \n            # Use a small tolerance for the zero-norm check for floating point stability\n            if g2_norm_sq > 1e-12:\n                g1_ortho = g1 - (np.dot(g1, g2) / g2_norm_sq) * g2\n            else:\n                g1_ortho = g1\n\n            w -= eta * (g1_ortho + g2)\n            \n        delta = n_vanilla - n_ortho\n        results.append(delta)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Instead of using ad-hoc fixes for gradient conflicts, we can approach the problem from the more fundamental perspective of multi-objective optimization. This framework seeks solutions that are \"Pareto optimal,\" meaning no single task's performance can be improved without degrading another's. This advanced practice  guides you through the theory behind Pareto optimality and has you implement the Multiple Gradient Descent Algorithm (MGDA), which formally computes an optimal compromise direction by finding the minimum-norm point in the convex hull of the task gradients. This exercise provides a bridge from heuristic methods to the foundational theory of multi-task optimization.",
            "id": "3155072",
            "problem": "You are given a differentiable multi-task learning problem with $T$ tasks, each task $t \\in \\{1,\\dots,T\\}$ associated with a scalar loss $L_t(\\mathbf{w})$, where $\\mathbf{w} \\in \\mathbb{R}^d$ is a shared parameter vector. The gradient of each task at $\\mathbf{w}$ is denoted by $\\mathbf{g}_t(\\mathbf{w}) = \\nabla_{\\mathbf{w}} L_t(\\mathbf{w}) \\in \\mathbb{R}^d$. A point $\\mathbf{w}^\\star$ is called Pareto optimal if there is no $\\mathbf{w}$ such that $L_t(\\mathbf{w}) \\le L_t(\\mathbf{w}^\\star)$ for all $t$ and $L_{t'}(\\mathbf{w})  L_{t'}(\\mathbf{w}^\\star)$ for at least one $t'$. Fundamental optimization theory states that for unconstrained, differentiable optimization, any local minimizer of a scalar objective $s(\\mathbf{w})$ must satisfy the first-order necessary condition $\\nabla s(\\mathbf{w}) = \\mathbf{0}$. Use this and other well-tested facts from multi-objective optimization to reason about Pareto stationarity.\n\nTask A (theory). Starting from the definitions above and using only foundational principles such as separation theorems for convex cones and first-order necessary conditions for unconstrained differentiable optimization, derive the following Pareto stationarity statement: if $\\mathbf{w}^\\star$ is Pareto optimal, then there exist coefficients $\\alpha_t \\ge 0$ with $\\sum_{t=1}^T \\alpha_t = 1$ such that\n$$\n\\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}.\n$$\nExplain why this condition follows from supporting hyperplane/separation arguments on the image of $\\{L_t(\\mathbf{w})\\}_{t=1}^T$ and the first-order necessary condition applied to an appropriate scalarization.\n\nTask B (algorithm). The Multiple Gradient Descent Algorithm (MGDA) at a given $\\mathbf{w}$ selects coefficients $\\boldsymbol{\\alpha} = (\\alpha_1,\\dots,\\alpha_T)$ by solving the quadratic program\n$$\n\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^T} \\; \\frac{1}{2} \\left\\| \\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}) \\right\\|_2^2 \\quad \\text{subject to} \\quad \\alpha_t \\ge 0 \\; \\text{for all } t, \\; \\sum_{t=1}^T \\alpha_t = 1.\n$$\nThis produces the minimum-norm convex combination of task gradients. Implement a program that, for each test case below, computes $\\boldsymbol{\\alpha}$ by solving this problem exactly for $T \\in \\{2,3\\}$ using closed-form and small-case reasoning, without numeric instability or reliance on external data.\n\nSmall model and gradients. Consider a shared linear model with parameter $\\mathbf{w} \\in \\mathbb{R}^2$, and task-specific squared losses\n$$\nL_t(\\mathbf{w}) = \\frac{1}{2} \\| \\mathbf{A}_t \\mathbf{w} - \\mathbf{b}_t \\|_2^2,\n$$\nwith $\\mathbf{A}_t \\in \\mathbb{R}^{2 \\times 2}$ and $\\mathbf{b}_t \\in \\mathbb{R}^2$. The gradient is\n$$\n\\mathbf{g}_t(\\mathbf{w}) = \\nabla_{\\mathbf{w}} L_t(\\mathbf{w}) = \\mathbf{A}_t^\\top (\\mathbf{A}_t \\mathbf{w} - \\mathbf{b}_t).\n$$\nIn all test cases below, use $\\mathbf{w} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ and $\\mathbf{A}_t = \\mathbf{I}_2$ for every task, so that $\\mathbf{g}_t(\\mathbf{w}) = -\\mathbf{b}_t$.\n\nTest suite. For each test case, you are given the list of $\\mathbf{b}_t$ vectors, one per task, and must compute the MGDA coefficients $\\boldsymbol{\\alpha}$.\n\n- Case $1$ (two tasks, interior solution exists): $T=2$, \n$$ \\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{b}_2 = \\begin{bmatrix} -\\frac{1}{2} \\\\ 0 \\end{bmatrix} $$\n\n- Case $2$ (two tasks, colinear same direction, boundary solution): $T=2$,\n$$ \\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{b}_2 = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix} $$\n\n- Case $3$ (three tasks, strictly interior solution with all coefficients positive): $T=3$,\n$$ \\mathbf{b}_1 = \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{b}_2 = \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}, \\quad \\mathbf{b}_3 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $$\n\nRequirements for your program.\n\n- Your program must compute $\\boldsymbol{\\alpha}$ for each case by solving the MGDA quadratic program exactly for $T \\in \\{2,3\\}$, using closed-form formulas for $T=2$ and small-case enumeration for $T=3$ (interior barycentric solution if feasible; otherwise minimize along edges and vertices).\n\n- The final numeric answers must be rounded to $6$ decimal places.\n\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list of coefficients for the corresponding test case. For example, an output could look like `[[a_{1},a_{2}],[b_{1},b_{2}],[c_{1},c_{2},c_{3}]]`, with no spaces. Replace $a_i,b_i,c_i$ by your computed floats rounded to $6$ decimals.\n\n- The only acceptable outputs are lists of floats; no booleans or strings in the list.\n\nYour program must be self-contained and require no inputs. It must not read from or write to files. It must implement the MGDA solution logic as described and compute the coefficients for the three test cases above.",
            "solution": "The analysis of the problem is divided into two parts as requested: a theoretical derivation of the Pareto stationarity condition (Task A) and the implementation of an algorithm to solve for the Multiple Gradient Descent Algorithm (MGDA) coefficients (Task B).\n\n### Task A: Derivation of the Pareto Stationarity Condition\n\nThis task requires deriving the first-order necessary condition for Pareto optimality. This condition is also known as Pareto stationarity. The derivation relies on the definition of Pareto optimality and fundamental results from convex analysis, specifically separation theorems.\n\nLet the set of $T$ differentiable task-specific loss functions be $\\{L_t(\\mathbf{w})\\}_{t=1}^T$, where $\\mathbf{w} \\in \\mathbb{R}^d$ is the shared parameter vector. The gradient of each loss is $\\mathbf{g}_t(\\mathbf{w}) = \\nabla_{\\mathbf{w}} L_t(\\mathbf{w})$.\n\n1.  **Condition for a Descent Direction**:\n    A vector $\\mathbf{d} \\in \\mathbb{R}^d$ is a descent direction for task $t$ at point $\\mathbf{w}$ if a small step in this direction reduces the loss $L_t$. Based on a first-order Taylor expansion, this condition is $\\mathbf{g}_t(\\mathbf{w})^\\top \\mathbf{d}  0$.\n\n2.  **Pareto Optimality and Descent Directions**:\n    By definition, a point $\\mathbf{w}^\\star$ is Pareto optimal if no other point $\\mathbf{w}$ exists that is strictly better for at least one task without being worse for any other task. In terms of local improvements, this implies that there cannot be a descent direction $\\mathbf{d}$ from $\\mathbf{w}^\\star$ that improves at least one task loss without worsening any other.\n    Formally, if $\\mathbf{w}^\\star$ is a Pareto optimal point, then there exists no vector $\\mathbf{d} \\in \\mathbb{R}^d$ such that:\n    $$\n    \\mathbf{g}_t(\\mathbf{w}^\\star)^\\top \\mathbf{d} \\le 0 \\quad \\text{for all } t \\in \\{1, \\dots, T\\}\n    $$\n    and\n    $$\n    \\mathbf{g}_{t'}(\\mathbf{w}^\\star)^\\top \\mathbf{d}  0 \\quad \\text{for at least one } t' \\in \\{1, \\dots, T\\}.\n    $$\n    If such a direction $\\mathbf{d}$ existed, one could take a small step $\\epsilon\\mathbf{d}$ (with $\\epsilon  0$) from $\\mathbf{w}^\\star$ to obtain a new point $\\mathbf{w}' = \\mathbf{w}^\\star + \\epsilon\\mathbf{d}$ that would Pareto-dominate $\\mathbf{w}^\\star$, contradicting its optimality. The simpler condition that there is no $\\mathbf{d}$ such that $\\mathbf{g}_t(\\mathbf{w}^\\star)^\\top \\mathbf{d}  0$ for all $t$ is a direct consequence.\n\n3.  **Application of a Theorem of the Alternative (Gordan's Lemma)**:\n    The non-existence of such a common descent direction can be formally established using a theorem of the alternative. Gordan's Lemma, a result from convex analysis, states that for any set of vectors $\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_T\\}$, exactly one of the following two statements must be true:\n    (i) There exists a vector $\\mathbf{d}$ such that $\\mathbf{v}_t^\\top \\mathbf{d}  0$ for all $t=1, \\dots, T$.\n    (ii) The zero vector $\\mathbf{0}$ is a non-trivial non-negative combination of the vectors, i.e., there exist coefficients $\\lambda_t \\ge 0$, not all zero, such that $\\sum_{t=1}^T \\lambda_t \\mathbf{v}_t = \\mathbf{0}$.\n    Equivalently, (ii) states that $\\mathbf{0}$ is in the convex cone of $\\{\\mathbf{v}_t\\}$.\n    A slightly more general version of this theorem covers the mixed inequalities ($\\le$ and $$) from step 2 and leads to the same conclusion.\n\n4.  **Deriving the Pareto Stationarity Condition**:\n    Applying this theorem to our set of gradients $\\{\\mathbf{g}_t(\\mathbf{w}^\\star)\\}$, the established non-existence of a common descent direction (ruling out statement (i)) implies that statement (ii) must be true. Therefore, there must exist coefficients $\\lambda_t \\ge 0$ for $t=1, \\dots, T$, with at least one $\\lambda_t  0$, such that:\n    $$\n    \\sum_{t=1}^T \\lambda_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}.\n    $$\n    Since the coefficients $\\lambda_t$ are not all zero, their sum $S = \\sum_{j=1}^T \\lambda_j$ is strictly positive. We can normalize these coefficients by defining $\\alpha_t = \\lambda_t / S$. These new coefficients satisfy:\n    - $\\alpha_t \\ge 0$ for all $t$.\n    - $\\sum_{t=1}^T \\alpha_t = \\frac{1}{S} \\sum_{t=1}^T \\lambda_t = \\frac{S}{S} = 1$.\n    - $\\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\frac{1}{S} \\sum_{t=1}^T \\lambda_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\frac{1}{S} \\mathbf{0} = \\mathbf{0}$.\n    This concludes the derivation of the Pareto stationarity condition:\n    $$\n    \\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}, \\quad \\text{with } \\alpha_t \\ge 0, \\sum_{t=1}^T \\alpha_t = 1.\n    $$\n\n5.  **Connection to Scalarization and Supporting Hyperplanes**:\n    The condition $\\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}$ is precisely the first-order necessary condition for a point $\\mathbf{w}^\\star$ to be a local minimum of the scalarized objective function $S(\\mathbf{w}) = \\sum_{t=1}^T \\alpha_t L_t(\\mathbf{w})$. The gradient of this scalarized loss is $\\nabla S(\\mathbf{w}) = \\sum_{t=1}^T \\alpha_t \\nabla L_t(\\mathbf{w})$.\n    Geometrically, in the loss space spanned by $(L_1, \\dots, L_T)$, the vector $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_T)$ defines a hyperplane. The Pareto stationarity condition implies that this hyperplane is a supporting hyperplane to the set of achievable loss vectors at the point $(L_1(\\mathbf{w}^\\star), \\dots, L_T(\\mathbf{w}^\\star))$.\n\n### Task B: MGDA Algorithm Implementation\n\nThe goal is to solve the quadratic program (QP) for $\\boldsymbol{\\alpha}$:\n$$\n\\min_{\\boldsymbol{\\alpha}} \\; \\frac{1}{2} \\left\\| \\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t \\right\\|_2^2 \\quad \\text{s.t.} \\quad \\alpha_t \\ge 0 \\; \\forall t, \\; \\sum_{t=1}^T \\alpha_t = 1.\n$$\nThe gradients are given by $\\mathbf{g}_t = -\\mathbf{b}_t$.\n\n#### Case $T=2$\nThe problem is to minimize $f(\\alpha_1, \\alpha_2) = \\frac{1}{2}\\|\\alpha_1 \\mathbf{g}_1 + \\alpha_2 \\mathbf{g}_2\\|_2^2$ subject to $\\alpha_1 + \\alpha_2 = 1$ and $\\alpha_1, \\alpha_2 \\ge 0$. Substituting $\\alpha_2 = 1-\\alpha_1$ reduces the problem to minimizing a quadratic function of $\\alpha_1 \\in [0, 1]$. Taking the derivative with respect to $\\alpha_1$ and setting it to zero yields the unconstrained minimizer:\n$$\n\\alpha_1^* = \\frac{\\mathbf{g}_2^\\top(\\mathbf{g}_2 - \\mathbf{g}_1)}{\\|\\mathbf{g}_1 - \\mathbf{g}_2\\|_2^2}.\n$$\nThe solution to the constrained problem is found by projecting $\\alpha_1^*$ onto the interval $[0, 1]$. Thus, $\\alpha_1 = \\text{max}(0, \\text{min}(1, \\alpha_1^*))$, and $\\alpha_2 = 1 - \\alpha_1$.\n\n- **Case 1:** $\\mathbf{g}_1 = [-1, 0]^\\top$, $\\mathbf{g}_2 = [0.5, 0]^\\top$.\n  $\\|\\mathbf{g}_1 - \\mathbf{g}_2\\|_2^2 = \\|[-1.5, 0]^\\top\\|_2^2 = 2.25$.\n  $\\mathbf{g}_2^\\top(\\mathbf{g}_2 - \\mathbf{g}_1) = [0.5, 0] \\cdot [1.5, 0]^\\top = 0.75$.\n  $\\alpha_1^* = 0.75 / 2.25 = 1/3$. Since $0 \\le 1/3 \\le 1$, we have $\\alpha_1 = 1/3$ and $\\alpha_2 = 2/3$.\n\n- **Case 2:** $\\mathbf{g}_1 = [-1, 0]^\\top$, $\\mathbf{g}_2 = [-2, 0]^\\top$.\n  $\\|\\mathbf{g}_1 - \\mathbf{g}_2\\|_2^2 = \\|[1, 0]^\\top\\|_2^2 = 1$.\n  $\\mathbf{g}_2^\\top(\\mathbf{g}_2 - \\mathbf{g}_1) = [-2, 0] \\cdot [-1, 0]^\\top = 2$.\n  $\\alpha_1^* = 2 / 1 = 2$. Since $2  1$, we project to the boundary: $\\alpha_1 = 1$ and $\\alpha_2 = 0$.\n\n#### Case $T=3$\nFor $T=3$ and gradients in $\\mathbb{R}^2$, the minimum norm convex combination of gradients is zero if and only if the origin is in the convex hull of the gradients (the triangle they form). This can be tested by finding the barycentric coordinates $(\\alpha_1, \\alpha_2, \\alpha_3)$ of the origin. If all $\\alpha_t \\ge 0$, this is the solution. This requires solving the system:\n$$\n\\begin{pmatrix} g_{1x}  g_{2x}  g_{3x} \\\\ g_{1y}  g_{2y}  g_{3y} \\\\ 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nIf this fails (e.g., origin is outside the triangle), the solution lies on one of the edges of the simplex, which reduces to solving three separate $T=2$ problems and picking the best one.\n\n- **Case 3:** $\\mathbf{g}_1 = [1, 0]^\\top$, $\\mathbf{g}_2 = [0, 1]^\\top$, $\\mathbf{g}_3 = [-1, -1]^\\top$.\nWe observe that $\\mathbf{g}_1 + \\mathbf{g}_2 + \\mathbf{g}_3 = \\mathbf{0}$. A convex combination that yields $\\mathbf{0}$ is $\\frac{1}{3}\\mathbf{g}_1 + \\frac{1}{3}\\mathbf{g}_2 + \\frac{1}{3}\\mathbf{g}_3 = \\mathbf{0}$. Since the objective function is a squared norm, its minimum value is $0$, which is achieved by this combination. The coefficients are $\\alpha_1 = 1/3$, $\\alpha_2 = 1/3$, $\\alpha_3 = 1/3$. All are non-negative and sum to $1$, so this is the unique valid solution. Solving the linear system above confirms this:\n$$\n\\begin{pmatrix} 1  0  -1 \\\\ 0  1  -1 \\\\ 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\implies \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix}.\n$$\n\nThe results are:\n- Case 1: $(\\alpha_1, \\alpha_2) = (1/3, 2/3)$\n- Case 2: $(\\alpha_1, \\alpha_2) = (1, 0)$\n- Case 3: $(\\alpha_1, \\alpha_2, \\alpha_3) = (1/3, 1/3, 1/3)$\nThese are implemented below to produce the final formatted output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_t2(g1: np.ndarray, g2: np.ndarray) -> tuple[float, float]:\n    \"\"\"\n    Solves the MGDA QP for T=2 tasks exactly.\n    \"\"\"\n    g1_minus_g2 = g1 - g2\n    g1_minus_g2_sq_norm = np.dot(g1_minus_g2, g1_minus_g2)\n\n    # Handle the degenerate case where g1 == g2\n    if np.isclose(g1_minus_g2_sq_norm, 0):\n        return 0.5, 0.5\n\n    # Compute unconstrained minimizer for alpha_1\n    alpha1_star = np.dot(g2, g2 - g1) / g1_minus_g2_sq_norm\n\n    # Project onto the interval [0, 1]\n    alpha1 = max(0.0, min(1.0, alpha1_star))\n    alpha2 = 1.0 - alpha1\n    \n    return alpha1, alpha2\n\ndef solve_t3(g1: np.ndarray, g2: np.ndarray, g3: np.ndarray) -> tuple[float, float, float]:\n    \"\"\"\n    Solves the MGDA QP for T=3 tasks exactly, as per problem specification.\n    \"\"\"\n    # First, check for an interior solution where the combination of gradients is zero.\n    # This happens if the origin is in the convex hull of the gradients.\n    # We solve for barycentric coordinates of the origin.\n    G_ext = np.array([\n        [g1[0], g2[0], g3[0]],\n        [g1[1], g2[1], g3[1]],\n        [1.0,   1.0,   1.0]\n    ])\n    \n    b_target = np.array([0.0, 0.0, 1.0])\n\n    # Check if a unique solution exists by checking if the matrix is invertible.\n    if abs(np.linalg.det(G_ext)) > 1e-9:\n        try:\n            alphas_int = np.linalg.solve(G_ext, b_target)\n            # If all coefficients are non-negative, this is the solution.\n            if np.all(alphas_int >= -1e-9):\n                return tuple(alphas_int)\n        except np.linalg.LinAlgError:\n            # Should not happen if det is non-zero, but for robustness.\n            pass\n\n    # If no interior solution exists, solve for solutions on the edges.\n    # Edge 1-2\n    a12 = solve_t2(g1, g2)\n    alpha_A = np.array([a12[0], a12[1], 0.0])\n    grad_A = alpha_A[0] * g1 + alpha_A[1] * g2\n    obj_A = 0.5 * np.dot(grad_A, grad_A)\n    \n    # Edge 1-3\n    a13 = solve_t2(g1, g3)\n    alpha_B = np.array([a13[0], 0.0, a13[1]])\n    grad_B = alpha_B[0] * g1 + alpha_B[2] * g3\n    obj_B = 0.5 * np.dot(grad_B, grad_B)\n\n    # Edge 2-3\n    a23 = solve_t2(g2, g3)\n    alpha_C = np.array([0.0, a23[0], a23[1]])\n    grad_C = alpha_C[1] * g2 + alpha_C[2] * g3\n    obj_C = 0.5 * np.dot(grad_C, grad_C)\n    \n    # Find the edge solution with the minimum objective value.\n    objectives = [obj_A, obj_B, obj_C]\n    candidates = [alpha_A, alpha_B, alpha_C]\n    best_alpha = candidates[np.argmin(objectives)]\n    \n    return tuple(best_alpha)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Gradients are g_t = -b_t.\n    test_cases = [\n        # Case 1 (T=2)\n        {\"b_vectors\": [np.array([1.0, 0.0]), np.array([-0.5, 0.0])]},\n        # Case 2 (T=2)\n        {\"b_vectors\": [np.array([1.0, 0.0]), np.array([2.0, 0.0])]},\n        # Case 3 (T=3)\n        {\"b_vectors\": [np.array([-1.0, 0.0]), np.array([0.0, -1.0]), np.array([1.0, 1.0])]},\n    ]\n\n    results = []\n    \n    # Case 1\n    case1 = test_cases[0]\n    g_vectors1 = [-b for b in case1[\"b_vectors\"]]\n    result1 = solve_t2(g_vectors1[0], g_vectors1[1])\n    results.append(list(result1))\n    \n    # Case 2\n    case2 = test_cases[1]\n    g_vectors2 = [-b for b in case2[\"b_vectors\"]]\n    result2 = solve_t2(g_vectors2[0], g_vectors2[1])\n    results.append(list(result2))\n\n    # Case 3\n    case3 = test_cases[2]\n    g_vectors3 = [-b for b in case3[\"b_vectors\"]]\n    result3 = solve_t3(g_vectors3[0], g_vectors3[1], g_vectors3[2])\n    results.append(list(result3))\n\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res_list in results:\n        # Format each float to 6 decimal places, then join into a string like \"[f1,f2,...]\"\n        formatted_list = \"[\" + \",\".join([f\"{x:.6f}\" for x in res_list]) + \"]\"\n        formatted_results.append(formatted_list)\n    \n    # Join all case results into the final output string.\n    final_output = \"[\" + \",\".join(formatted_results) + \"]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}