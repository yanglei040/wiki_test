{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of multimodal learning is the ability to fuse information from different sources to arrive at a more robust and certain conclusion. This exercise delves into the mathematical foundation of this process, often known as a \"product of experts.\" By treating each modality's output as a probabilistic belief, you will derive from first principles how to combine two Gaussian distributions, revealing the elegant way in which their precisions add to yield a fused estimate with reduced uncertainty .",
            "id": "3156180",
            "problem": "Consider a multimodal learning scenario where a latent representation $x \\in \\mathbb{R}^{d}$ describes a shared concept inferred from two conditionally independent modalities, visual $v$ and text $t$. Each modality provides a Gaussian belief over $x$: the visual modality yields a density $p_{v}(x)$ modeled as a multivariate normal distribution with mean $\\mu_{v} \\in \\mathbb{R}^{d}$ and covariance matrix $\\Sigma_{v} \\in \\mathbb{R}^{d \\times d}$, and the text modality yields a density $p_{t}(x)$ modeled as a multivariate normal distribution with mean $\\mu_{t} \\in \\mathbb{R}^{d}$ and covariance matrix $\\Sigma_{t} \\in \\mathbb{R}^{d \\times d}$. Assume a non-informative (uniform) prior over $x$ and conditional independence of $v$ and $t$ given $x$.\n\nUsing only foundational probabilistic definitions and algebra, derive the closed-form expression for the fused posterior density $p(x \\mid v, t)$ obtained by multiplying the two Gaussian beliefs. Your derivation must start from the standard probability density function of the multivariate normal distribution and proceed by explicit algebraic manipulation to identify the fused mean and covariance.\n\nThen, validate the derivation numerically in the scalar case $d = 1$ by computing the fused mean when the modality-specific parameters are $\\mu_{v} = 0.8$, $\\sigma_{v}^{2} = 0.09$, $\\mu_{t} = 0.5$, and $\\sigma_{t}^{2} = 0.25$, where $\\sigma_{v}^{2}$ and $\\sigma_{t}^{2}$ denote the scalar variances corresponding to $\\Sigma_{v}$ and $\\Sigma_{t}$. Round your final numerical answer (the fused mean in the scalar case) to four significant figures. Provide your final answer as a single real number without units.",
            "solution": "We are asked to fuse two Gaussian beliefs over the same latent variable $x$ under a non-informative prior and conditional independence of the modalities. The fundamental base we use consists of:\n- Bayes' theorem: for conditionally independent observations $v$ and $t$ with a uniform prior over $x$, the fused posterior is proportional to the product of the modality-specific likelihoods over $x$.\n- The probability density function of the multivariate normal distribution.\n\nLet $p_{v}(x) = \\mathcal{N}(\\mu_{v}, \\Sigma_{v})$ and $p_{t}(x) = \\mathcal{N}(\\mu_{t}, \\Sigma_{t})$. Under a uniform prior and conditional independence, the fused posterior satisfies\n$$\np(x \\mid v, t) \\propto p_{v}(x) \\, p_{t}(x).\n$$\nWe represent each Gaussian density in canonical form. For $x \\in \\mathbb{R}^{d}$,\n$$\np_{v}(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_{v}|^{1/2}} \\exp\\!\\left( -\\frac{1}{2} (x - \\mu_{v})^{\\top} \\Sigma_{v}^{-1} (x - \\mu_{v}) \\right),\n$$\n$$\np_{t}(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_{t}|^{1/2}} \\exp\\!\\left( -\\frac{1}{2} (x - \\mu_{t})^{\\top} \\Sigma_{t}^{-1} (x - \\mu_{t}) \\right).\n$$\nTheir product is\n$$\np_{v}(x) \\, p_{t}(x) = C \\, \\exp\\!\\left( -\\frac{1}{2} (x - \\mu_{v})^{\\top} \\Sigma_{v}^{-1} (x - \\mu_{v}) - \\frac{1}{2} (x - \\mu_{t})^{\\top} \\Sigma_{t}^{-1} (x - \\mu_{t}) \\right),\n$$\nwhere\n$$\nC = \\frac{1}{(2\\pi)^{d} |\\Sigma_{v}|^{1/2} |\\Sigma_{t}|^{1/2}}\n$$\nis a constant with respect to $x$. We focus on the exponent and expand both quadratic forms:\n\n$$\n\\begin{aligned}\n&-\\frac{1}{2} \\Big[ (x - \\mu_{v})^{\\top} \\Sigma_{v}^{-1} (x - \\mu_{v}) + (x - \\mu_{t})^{\\top} \\Sigma_{t}^{-1} (x - \\mu_{t}) \\Big] \\\\\n&= -\\frac{1}{2} \\Big[ x^{\\top} \\Sigma_{v}^{-1} x - 2 \\mu_{v}^{\\top} \\Sigma_{v}^{-1} x + \\mu_{v}^{\\top} \\Sigma_{v}^{-1} \\mu_{v} + x^{\\top} \\Sigma_{t}^{-1} x - 2 \\mu_{t}^{\\top} \\Sigma_{t}^{-1} x + \\mu_{t}^{\\top} \\Sigma_{t}^{-1} \\mu_{t} \\Big] \\\\\n&= -\\frac{1}{2} \\Big[ x^{\\top} (\\Sigma_{v}^{-1} + \\Sigma_{t}^{-1}) x - 2 (\\Sigma_{v}^{-1} \\mu_{v} + \\Sigma_{t}^{-1} \\mu_{t})^{\\top} x + \\mu_{v}^{\\top} \\Sigma_{v}^{-1} \\mu_{v} + \\mu_{t}^{\\top} \\Sigma_{t}^{-1} \\mu_{t} \\Big].\n\\end{aligned}\n$$\n\nDefine the fused precision matrix\n$$\n\\Lambda = \\Sigma_{v}^{-1} + \\Sigma_{t}^{-1}\n$$\nand the fused natural parameter\n$$\n\\eta = \\Sigma_{v}^{-1} \\mu_{v} + \\Sigma_{t}^{-1} \\mu_{t}.\n$$\nThen the exponent becomes\n$$\n-\\frac{1}{2} \\Big[ x^{\\top} \\Lambda x - 2 \\eta^{\\top} x + \\text{constant} \\Big].\n$$\nWe complete the square to express this quadratic form in the centered Gaussian form. Let the fused covariance and mean be\n$$\n\\Sigma = \\Lambda^{-1}, \\quad \\mu = \\Sigma \\eta.\n$$\nObserve that\n\n$$\n\\begin{aligned}\nx^{\\top} \\Lambda x - 2 \\eta^{\\top} x\n&= (x - \\mu)^{\\top} \\Lambda (x - \\mu) - \\mu^{\\top} \\Lambda \\mu \\\\\n&= (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu) - \\eta^{\\top} \\Sigma \\eta,\n\\end{aligned}\n$$\n\nwhere we used $\\mu = \\Sigma \\eta$ and $\\Lambda = \\Sigma^{-1}$. Therefore,\n\n$$\n-\\frac{1}{2} \\Big[ x^{\\top} \\Lambda x - 2 \\eta^{\\top} x + \\text{constant} \\Big]\n= -\\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu) + \\text{new constant}.\n$$\n\nHence $p_{v}(x) \\, p_{t}(x)$ is proportional to a multivariate normal density with covariance $\\Sigma = (\\Sigma_{v}^{-1} + \\Sigma_{t}^{-1})^{-1}$ and mean $\\mu = \\Sigma (\\Sigma_{v}^{-1} \\mu_{v} + \\Sigma_{t}^{-1} \\mu_{t})$. After normalization, the fused posterior $p(x \\mid v, t)$ is Gaussian with exactly these parameters.\n\nNumerical validation in the scalar case $d = 1$. Write $\\sigma_{v}^{2}$ and $\\sigma_{t}^{2}$ for the scalar variances. The scalar equivalents of the fused parameters are:\n$$\n\\sigma^{2} = \\left( \\sigma_{v}^{-2} + \\sigma_{t}^{-2} \\right)^{-1}, \\quad\n\\mu = \\sigma^{2} \\left( \\sigma_{v}^{-2} \\mu_{v} + \\sigma_{t}^{-2} \\mu_{t} \\right).\n$$\nGiven $\\mu_{v} = 0.8$, $\\sigma_{v}^{2} = 0.09$, $\\mu_{t} = 0.5$, and $\\sigma_{t}^{2} = 0.25$:\n\n$$\n\\sigma_{v}^{-2} = \\frac{1}{0.09} = \\frac{100}{9}, \\quad\n\\sigma_{t}^{-2} = \\frac{1}{0.25} = 4.\n$$\n\nCompute the terms for the mean calculation:\n\n$$\n\\sigma_{v}^{-2} \\mu_{v} + \\sigma_{t}^{-2} \\mu_{t}\n= \\frac{100}{9} \\cdot (0.8) + 4 \\cdot (0.5)\n= \\frac{80}{9} + 2\n= \\frac{80}{9} + \\frac{18}{9} = \\frac{98}{9}.\n$$\n\n$$\n\\sigma_{v}^{-2} + \\sigma_{t}^{-2}\n= \\frac{100}{9} + 4\n= \\frac{100}{9} + \\frac{36}{9} = \\frac{136}{9}.\n$$\n\nThus\n\n$$\n\\mu = \\frac{\\frac{98}{9}}{\\frac{136}{9}}\n= \\frac{98}{136}\n= \\frac{49}{68}\n\\approx 0.7205882353\\ldots\n$$\n\nRounded to four significant figures, the fused mean is $0.7206$. As an additional check (not required for the final answer), the fused variance is\n\n$$\n\\sigma^{2} = \\left( \\frac{136}{9} \\right)^{-1}\n= \\frac{9}{136}\n\\approx 0.06617647,\n$$\n\nwhich is lower than both $0.09$ and $0.25$, consistent with precision increasing when fusing independent Gaussian information sources. The fused mean $0.7206$ lies between $0.5$ and $0.8$ and is closer to $0.8$, reflecting the higher precision of the visual modality ($\\sigma_{v}^{-2} \\approx 11.1$) relative to the text modality ($\\sigma_{t}^{-2} = 4$).",
            "answer": "$$\\boxed{0.7206}$$"
        },
        {
            "introduction": "Why can't we simply train separate models for each modality and average their outputs? This practice uses a carefully constructed synthetic dataset to demonstrate the limits of unimodal reasoning and the necessity of modeling cross-modal interactions. You will discover that concepts which are inherently compositional, such as the idea of a \"red cube,\" are impossible to learn without a mechanism that explicitly relates the modalities, motivating the development of architectures like cross-attention .",
            "id": "3156162",
            "problem": "You are given a synthetic multimodal learning scenario in which two modalities, a symbolic \"shape\" modality and a symbolic \"color\" modality, are each encoded as one-hot vectors. The target label depends only on the cross-modal relation (for example, the compositional description \"red cube\"), not on either modality in isolation. Your task is to implement and train three models using gradient-based optimization: a unimodal shape-only logistic regression classifier, a unimodal color-only logistic regression classifier, and a cross-modal bilinear classifier that implements a simple form of cross-attention. You must demonstrate that the unimodal classifiers fail to recover the compositional labels while the cross-attention model succeeds.\n\nFundamental base to be used:\n- Logistic regression classifier with the sigmoid (logistic) function and Binary Cross-Entropy (BCE) loss.\n- One-hot feature encoding and basic linear algebra identities.\n- Uniform data distribution over categorical combinations and deterministic labeling functions defined on modality pairs.\n\nDefinitions:\n- Let the shape modality be represented by a one-hot vector $x_{s} \\in \\{0,1\\}^{n_{s}}$ and the color modality by a one-hot vector $x_{c} \\in \\{0,1\\}^{n_{c}}$, where $n_{s}$ and $n_{c}$ are the number of distinct shapes and colors, respectively.\n- The label $y \\in \\{0,1\\}$ is defined by a deterministic mapping $g: \\{1,\\dots,n_{s}\\} \\times \\{1,\\dots,n_{c}\\} \\to \\{0,1\\}$ on index pairs $(i,j)$ that correspond to shape and color categories. The data distribution is uniform over all $(i,j)$ pairs.\n- The sigmoid function is $\\sigma(z) = \\frac{1}{1 + e^{-z}}$.\n- The Binary Cross-Entropy (BCE) loss for predictions $\\hat{y} \\in (0,1)$ and labels $y \\in \\{0,1\\}$ is $L = -\\left(y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})\\right)$.\n\nModels:\n- Shape-only logistic regression: $\\hat{y} = \\sigma\\left(w_{s}^{\\top} x_{s} + b_{s}\\right)$ with parameters $w_{s} \\in \\mathbb{R}^{n_{s}}$ and $b_{s} \\in \\mathbb{R}$.\n- Color-only logistic regression: $\\hat{y} = \\sigma\\left(w_{c}^{\\top} x_{c} + b_{c}\\right)$ with parameters $w_{c} \\in \\mathbb{R}^{n_{c}}$ and $b_{c} \\in \\mathbb{R}$.\n- Cross-modal bilinear classifier with cross-attention gate: $\\hat{y} = \\sigma\\left(x_{s}^{\\top} W x_{c} + b\\right)$ with parameters $W \\in \\mathbb{R}^{n_{s} \\times n_{c}}$ and $b \\in \\mathbb{R}$. Here, $x_{s}^{\\top} W x_{c}$ selects the scalar compatibility weight for the shape-color pair, acting as a cross-attention score gated by the sigmoid to produce a probability.\n\nTraining objective:\n- Minimize the average BCE loss over the dataset using gradient descent updates for the parameters of each model.\n\nYour program must implement the following test suite of datasets and report accuracies:\n- Test case $1$: $n_{s} = 2$, $n_{c} = 2$. Define indices as shape $1$ equals \"cube\", shape $2$ equals \"sphere\", color $1$ equals \"red\", color $2$ equals \"blue\". Let $g(1,1) = 1$ and $g(2,2) = 1$, with $g(1,2) = 0$ and $g(2,1) = 0$. This encodes positives for \"red cube\" and \"blue sphere.\"\n- Test case $2$: $n_{s} = 2$, $n_{c} = 3$. Let $g(i,j) = 1$ if $(i \\bmod 2) = (j \\bmod 2)$, and $g(i,j) = 0$ otherwise. Indices are $i \\in \\{1,2\\}$ and $j \\in \\{1,2,3\\}$.\n- Test case $3$: $n_{s} = 3$, $n_{c} = 3$. Let $g(i,j) = 1$ if $i = j$, and $g(i,j) = 0$ otherwise. This encodes positives only on the diagonal of shape-color matches.\n\nScientific realism and derivation requirements:\n- You must justify why the unimodal classifiers cannot in general represent the mapping $g(i,j)$ when it is not reducible to a function of $i$ alone or $j$ alone.\n- You must show that the cross-modal bilinear classifier can represent any deterministic mapping on pairs by appropriate parameter settings, because each $(i,j)$ pair has its own parameter $W_{ij}$ contributing linearly to the logit.\n- The training must use gradient descent on the BCE loss to produce the model parameters, and the evaluation metric must be accuracy computed with a threshold at $0.5$ on $\\hat{y}$.\n\nNumerical specifications:\n- No physical units are involved.\n- Angles are not involved.\n- Accuracies must be expressed as decimals (for example, $0.750$), rounded to $3$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order:\n$[$shape\\_only\\_accuracy\\_test1, color\\_only\\_accuracy\\_test1, cross\\_attention\\_accuracy\\_test1, shape\\_only\\_accuracy\\_test2, color\\_only\\_accuracy\\_test2, cross\\_attention\\_accuracy\\_test2, shape\\_only\\_accuracy\\_test3, color\\_only\\_accuracy\\_test3, cross\\_attention\\_accuracy\\_test3$]$.\nEach accuracy must be a decimal rounded to $3$ decimal places.",
            "solution": "The problem requires an analysis and demonstration of the representational capacities of different model architectures in a synthetic multimodal learning context. Specifically, we must show why unimodal logistic regression classifiers fail and a cross-modal bilinear classifier succeeds at learning a compositional relationship between two symbolic modalities. The solution involves both a theoretical justification and a numerical implementation using gradient-based optimization.\n\n### Theoretical Justification of Model Capabilities\n\nThe core of the problem lies in the ability of a model to represent the target function $g(i,j)$, which defines the label $y \\in \\{0,1\\}$ based on the combination of a shape category $i \\in \\{1, \\dots, n_s\\}$ and a color category $j \\in \\{1, \\dots, n_c\\}$. The inputs are one-hot vectors $x_s \\in \\{0,1\\}^{n_s}$ for the shape and $x_c \\in \\{0,1\\}^{n_c}$ for the color.\n\n**1. Unimodal Logistic Regression Models**\n\nConsider the shape-only logistic regression model: $\\hat{y} = \\sigma(w_s^\\top x_s + b_s)$.\nWhen the input corresponds to the $i$-th shape, the one-hot vector $x_s$ has a $1$ at the $i$-th position and $0$s elsewhere. Let this vector be denoted $e_i$. The logit (the input to the sigmoid function $\\sigma$) for this input is:\n$$z_i = w_s^\\top e_i + b_s = w_{s,i} + b_s$$\nwhere $w_{s,i}$ is the $i$-th component of the weight vector $w_s$. The prediction is $\\hat{y}_i = \\sigma(w_{s,i} + b_s)$.\n\nCritically, this prediction depends *only* on the shape index $i$. The model is fundamentally incapable of producing different predictions for the same shape $i$ when paired with different colors $j_1, j_2, \\dots, j_{n_c}$. However, the target label $y=g(i,j)$ can vary with $j$ for a fixed $i$.\n\nUnder gradient descent optimization with the Binary Cross-Entropy (BCE) loss, the model will adjust its parameters $w_s$ and $b_s$ to minimize the total loss over the uniformly distributed dataset. For a given shape $i$, the optimal prediction $\\hat{y}_i$ that minimizes the expected BCE loss is the average value of the true labels over all colors:\n$$\\hat{y}_i \\to \\mathbb{E}_{j}[g(i,j)] = \\frac{1}{n_c} \\sum_{j=1}^{n_c} g(i,j)$$\nThe model learns the marginal probability $P(y=1 | \\text{shape}=i)$. If this marginal probability is not consistently above $0.5$ for $y=1$ cases and below $0.5$ for $y=0$ cases, the model will fail. For instance, in Test Case $1$, for shape $1$ (\"cube\"), the labels are $g(1,1)=1$ and $g(1,2)=0$. The marginal probability is $(1+0)/2 = 0.5$. The model's optimal prediction is $\\hat{y}=0.5$, which yields an accuracy of $0.5$ (random chance) because it lies on the decision boundary.\n\nAn identical argument holds for the color-only model $\\hat{y} = \\sigma(w_c^\\top x_c + b_c)$. It can only learn the marginal probability $P(y=1 | \\text{color}=j)$ and will fail if the true label function $g(i,j)$ is not reducible to a function of $j$ alone.\n\n**2. Cross-Modal Bilinear Classifier**\n\nNow consider the cross-modal bilinear classifier: $\\hat{y} = \\sigma(x_s^\\top W x_c + b)$.\nWhen the input corresponds to the pair of the $i$-th shape ($x_s = e_i$) and the $j$-th color ($x_c = e_j$), the logit is:\n$$z_{ij} = e_i^\\top W e_j + b$$\nThe term $e_i^\\top W e_j$ is a fundamental operation in linear algebra that selects the scalar element at the $i$-th row and $j$-th column of the matrix $W$. Thus, the logit simplifies to:\n$$z_{ij} = W_{ij} + b$$\nThe prediction for the pair $(i,j)$ is $\\hat{y}_{ij} = \\sigma(W_{ij} + b)$.\n\nThis architecture provides a distinct, learnable parameter $W_{ij}$ for each unique shape-color pair $(i,j)$. This allows the model to represent any arbitrary mapping $g: \\{1,\\dots,n_s\\} \\times \\{1,\\dots,n_c\\} \\to \\{0,1\\}$. To achieve perfect classification (accuracy of $1.0$), the model's parameters must be set such that the logit $z_{ij}$ is positive when $g(i,j)=1$ and negative when $g(i,j)=0$. This can always be achieved. For example, one can set the bias $b$ to $0$ and the weight matrix elements as:\n$$W_{ij} = \\begin{cases} C & \\text{if } g(i,j) = 1 \\\\ -C & \\text{if } g(i,j) = 0 \\end{cases}$$\nfor any positive constant $C$. Gradient descent will find values for $W$ and $b$ that satisfy these sign conditions, driving the BCE loss towards $0$ and achieving $100\\%$ accuracy on the training data.\n\n### Algorithmic Implementation via Gradient Descent\n\nTo train these models, we minimize the average BCE loss over all $N = n_s \\times n_c$ data points in the dataset. The loss for a single prediction $\\hat{y}$ and label $y$ is $L = -(y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}))$. The key gradient component is $\\frac{\\partial L}{\\partial z} = \\hat{y} - y$, where $z$ is the logit. The parameter updates for a learning rate $\\alpha$ follow the rule: $\\theta \\leftarrow \\theta - \\alpha \\frac{1}{N} \\sum_{k=1}^N \\frac{\\partial L_k}{\\partial \\theta}$.\n\nThe specific gradients for each model are:\n- **Shape-only model:**\n  $$ \\frac{\\partial L}{\\partial w_s} = (\\hat{y}-y)x_s \\quad ; \\quad \\frac{\\partial L}{\\partial b_s} = \\hat{y}-y $$\n- **Color-only model:**\n  $$ \\frac{\\partial L}{\\partial w_c} = (\\hat{y}-y)x_c \\quad ; \\quad \\frac{\\partial L}{\\partial b_c} = \\hat{y}-y $$\n- **Cross-modal bilinear model:**\n  $$ \\frac{\\partial L}{\\partial W} = (\\hat{y}-y) x_s x_c^\\top \\quad ; \\quad \\frac{\\partial L}{\\partial b} = \\hat{y}-y $$\nwhere $x_s x_c^\\top$ is the outer product of the two one-hot vectors, resulting in a matrix of zeros with a single $1$ at the location corresponding to the active shape-color pair.\n\nBy implementing batch gradient descent using these gradients, we can train each model and numerically validate the theoretical predictions about their performance. Evaluation is performed by calculating accuracy, where a prediction $\\hat{y}$ is classified as $1$ if $\\hat{y} > 0.5$ and $0$ otherwise.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and trains three models on three synthetic multimodal learning tasks,\n    reporting the final classification accuracy for each.\n    \"\"\"\n\n    def sigmoid(z):\n        \"\"\"The sigmoid function.\"\"\"\n        # Using np.exp is numerically stable for large inputs.\n        return 1.0 / (1.0 + np.exp(-z))\n\n    def generate_dataset(n_s, n_c, g_func):\n        \"\"\"Generates the full dataset for a given configuration.\"\"\"\n        dataset = []\n        for i in range(1, n_s + 1):\n            for j in range(1, n_c + 1):\n                x_s = np.zeros(n_s)\n                x_s[i-1] = 1.0\n                x_c = np.zeros(n_c)\n                x_c[j-1] = 1.0\n                y = g_func(i, j)\n                dataset.append((x_s, x_c, y))\n        return dataset\n\n    def train_and_evaluate(model_type, dataset, n_s, n_c, learning_rate=1.0, epochs=1000):\n        \"\"\"Trains a specified model and evaluates its accuracy.\"\"\"\n        n_samples = len(dataset)\n        \n        # Epsilon for numerical stability in log\n        epsilon = 1e-9\n\n        # Initialize parameters\n        if model_type == 'shape_only':\n            w = np.zeros(n_s)\n            b = 0.0\n        elif model_type == 'color_only':\n            w = np.zeros(n_c)\n            b = 0.0\n        elif model_type == 'cross_attention':\n            W = np.zeros((n_s, n_c))\n            b = 0.0\n        else:\n            raise ValueError(\"Unknown model type\")\n\n        # Batch Gradient Descent\n        for epoch in range(epochs):\n            grad_w_sum = 0\n            grad_b_sum = 0\n            \n            # Aggregate gradients over the whole dataset\n            for x_s, x_c, y in dataset:\n                if model_type == 'shape_only':\n                    z = w.T @ x_s + b\n                    x = x_s\n                elif model_type == 'color_only':\n                    z = w.T @ x_c + b\n                    x = x_c\n                else: # cross_attention\n                    z = x_s.T @ W @ x_c + b\n\n                y_hat = sigmoid(z)\n                \n                # Common gradient part for BCE loss\n                d_loss_d_z = y_hat - y\n\n                if model_type in ['shape_only', 'color_only']:\n                    grad_w_sum += d_loss_d_z * x\n                    grad_b_sum += d_loss_d_z\n                else: # cross_attention\n                    grad_w_sum += d_loss_d_z * np.outer(x_s, x_c)\n                    grad_b_sum += d_loss_d_z\n            \n            # Update parameters\n            if model_type in ['shape_only', 'color_only']:\n                w -= learning_rate * (grad_w_sum / n_samples)\n                b -= learning_rate * (grad_b_sum / n_samples)\n            else: # cross_attention\n                W -= learning_rate * (grad_w_sum / n_samples)\n                b -= learning_rate * (grad_b_sum / n_samples)\n\n        # Evaluate accuracy\n        correct_predictions = 0\n        for x_s, x_c, y in dataset:\n            if model_type == 'shape_only':\n                z = w.T @ x_s + b\n            elif model_type == 'color_only':\n                z = w.T @ x_c + b\n            else: # cross_attention\n                z = x_s.T @ W @ x_c + b\n                \n            y_hat = sigmoid(z)\n            prediction = 1 if y_hat > 0.5 else 0\n            if prediction == y:\n                correct_predictions += 1\n        \n        accuracy = correct_predictions / n_samples\n        return accuracy\n\n    # Define test cases\n    test_cases = [\n        {\n            \"name\": \"Test Case 1\",\n            \"n_s\": 2, \"n_c\": 2,\n            \"g_func\": lambda i, j: 1.0 if (i, j) in [(1, 1), (2, 2)] else 0.0\n        },\n        {\n            \"name\": \"Test Case 2\",\n            \"n_s\": 2, \"n_c\": 3,\n            \"g_func\": lambda i, j: 1.0 if (i % 2) == (j % 2) else 0.0\n        },\n        {\n            \"name\": \"Test Case 3\",\n            \"n_s\": 3, \"n_c\": 3,\n            \"g_func\": lambda i, j: 1.0 if i == j else 0.0\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        dataset = generate_dataset(case[\"n_s\"], case[\"n_c\"], case[\"g_func\"])\n        \n        # Shape-only model\n        acc_shape = train_and_evaluate('shape_only', dataset, case[\"n_s\"], case[\"n_c\"])\n        all_results.append(f\"{acc_shape:.3f}\")\n\n        # Color-only model\n        acc_color = train_and_evaluate('color_only', dataset, case[\"n_s\"], case[\"n_c\"])\n        all_results.append(f\"{acc_color:.3f}\")\n\n        # Cross-attention model\n        acc_cross = train_and_evaluate('cross_attention', dataset, case[\"n_s\"], case[\"n_c\"])\n        all_results.append(f\"{acc_cross:.3f}\")\n\n    # Print results in the specified format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The choice between \"early fusion\" (combining raw features) and \"late fusion\" (combining high-level predictions) is a fundamental dilemma in multimodal system design. This exercise provides a principled framework for navigating this choice by connecting it to the statistical structure of the data. You will derive the conditions under which a late fusion approach is Bayes-optimal and implement a scenario to quantitatively demonstrate how a naive early fusion strategy can be suboptimal when these conditions, specifically class-conditional independence, are not met .",
            "id": "3156125",
            "problem": "You are given a binary classification setting with two modalities, a vision feature $x_{v} \\in \\mathbb{R}$ and a text feature $x_{t} \\in \\mathbb{R}$. Let the joint observation be $x = (x_{v}, x_{t}) \\in \\mathbb{R}^{2}$. The class label is $y \\in \\{0,1\\}$ with class prior probabilities $\\pi_{0}$ and $\\pi_{1}$ such that $\\pi_{0} + \\pi_{1} = 1$. The Bayes decision rule assigns $x$ to the class $y$ that maximizes the posterior $p(y \\mid x)$, equivalently comparing the posterior log-odds to $0$. Assume the following fundamental base:\n- Bayes decision theory: the Bayes classifier minimizes the probability of error by choosing the class with the largest posterior $p(y \\mid x)$.\n- The chain rule and independence: for any random variables $a$ and $b$, $p(a,b \\mid y) = p(a \\mid y)\\,p(b \\mid y)$ if and only if $a$ and $b$ are conditionally independent given $y$.\n\nTask 1 (Derivation). Starting from the Bayes decision rule and only these foundational facts, derive the precise mathematical conditions under which a late fusion rule that multiplies per-modality likelihoods is Bayes-optimal. Concretely, suppose you have two per-modality models that produce $p(x_{v} \\mid y)$ and $p(x_{t} \\mid y)$, and you fuse them by forming a joint score proportional to $p(x_{v} \\mid y)\\,p(x_{t} \\mid y)$ for each $y$. Derive necessary and sufficient conditions on the data-generating distribution and on the per-modality models such that this late fusion decision rule coincides with the Bayes classifier. Your derivation must be expressed in terms of $p(y)$, $p(x_{v}, x_{t} \\mid y)$, and the posterior log-odds $\\log \\frac{p(y=1 \\mid x)}{p(y=0 \\mid x)}$, and must explicitly show the role of conditional independence $p(x_{v}, x_{t} \\mid y) = p(x_{v} \\mid y)\\,p(x_{t} \\mid y)$.\n\nTask 2 (Early versus late fusion under a Gaussian model). Consider a generative model where, for each class $y \\in \\{0,1\\}$, the conditional distribution of $x$ is Gaussian with class-dependent means and a common diagonal covariance (class-conditional homoscedasticity and cross-modal conditional independence):\n$$\nx \\mid y=k \\sim \\mathcal{N}\\!\\left(\\mu_{k}, \\Sigma\\right), \\quad \\mu_{k} = \\begin{bmatrix}\\mu_{v k} \\\\ \\mu_{t k}\\end{bmatrix}, \\quad \\Sigma = \\begin{bmatrix}\\sigma_{v}^{2} & 0 \\\\ 0 & \\sigma_{t}^{2}\\end{bmatrix},\n$$\nwith equal class priors $\\pi_{0} = \\pi_{1} = \\frac{1}{2}$. Under this model:\n- The Bayes-optimal classifier (which equals the optimal joint model and the mathematically correct late fusion under conditional independence) is a linear discriminant in $x$.\n- A naive early fusion baseline constructs a scalar score $s = x_{v} + x_{t}$ (i.e., concatenation followed by equal-weight summation) and then applies the optimal threshold on $s$.\n\nDerive, from first principles and the Gaussian assumptions above, closed-form expressions for the classification error of:\n- The Bayes-optimal classifier in terms of the separation between class means and the noise variances.\n- The naive early fusion with equal weights, expressed via the one-dimensional projection $s$.\n\nYour derivation must start from the Gaussian densities and the Bayes rule, without assuming any shortcut formulas.\n\nTask 3 (Testable construction and program). Implement a program that, for the following test suite of three parameter settings, computes and reports the difference between the naive early fusion error and the Bayes-optimal (late fusion) error:\n- Test case A (happy path where equal-weight early fusion aligns with the Bayes direction): $\\mu_{0} = \\begin{bmatrix}-0.5 \\\\ -0.5\\end{bmatrix}$, $\\mu_{1} = \\begin{bmatrix}0.5 \\\\ 0.5\\end{bmatrix}$, $\\sigma_{v} = 1$, $\\sigma_{t} = 1$, $\\pi_{0} = \\pi_{1} = \\frac{1}{2}$.\n- Test case B (heteroskedastic modalities where naive equal weights are suboptimal): $\\mu_{0} = \\begin{bmatrix}-1 \\\\ -1\\end{bmatrix}$, $\\mu_{1} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$, $\\sigma_{v} = 2$, $\\sigma_{t} = 1$, $\\pi_{0} = \\pi_{1} = \\frac{1}{2}$.\n- Test case C (one modality non-informative and noisy): $\\mu_{0} = \\begin{bmatrix}0 \\\\ -1\\end{bmatrix}$, $\\mu_{1} = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$, $\\sigma_{v} = 2$, $\\sigma_{t} = 0.5$, $\\pi_{0} = \\pi_{1} = \\frac{1}{2}$.\n\nFor each test case, the program must:\n- Compute the Bayes-optimal error under the full Gaussian model with equal class covariances.\n- Compute the error of the naive early fusion rule $s = x_{v} + x_{t}$ using its optimal threshold on $s$.\n- Output the difference $(\\text{early error}) - (\\text{Bayes error})$ rounded to six decimals, as a float.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of the three floating-point differences in the order [A,B,C], enclosed in square brackets. For example, the output format must be exactly like $[\\text{dA},\\text{dB},\\text{dC}]$ with each $\\text{dX}$ rounded to six decimals. No units or additional text should be printed. All angles, if any, must be in radians; there are no physical units in this problem.",
            "solution": "The problem has been validated and is deemed scientifically sound, well-posed, and objective. It presents a standard, albeit detailed, exercise in Bayesian decision theory and statistical pattern recognition as applied to multimodal learning. All necessary conditions and data are provided for a unique and meaningful solution.\n\nThis solution is structured into three parts, corresponding to the three tasks in the problem statement. Task 1 addresses the theoretical underpinnings of a product-of-likelihoods fusion rule. Task 2 derives the specific error rates for two classifiers under a Gaussian data model. Task 3 concerns the implementation of these results.\n\n**Task 1: Conditions for Bayes-Optimality of Late Fusion**\n\nThe objective is to derive the necessary and sufficient conditions under which a late fusion rule, based on multiplying per-modality likelihoods, is Bayes-optimal.\n\nThe Bayes decision rule classifies an observation $x = (x_v, x_t)$ to the class $y \\in \\{0, 1\\}$ that maximizes the posterior probability $p(y|x)$. The decision boundary is the set of points $x$ for which the posteriors are equal, $p(y=1|x) = p(y=0|x)$. This is equivalent to the posterior log-odds being zero:\n$$\n\\log \\frac{p(y=1|x)}{p(y=0|x)} = 0\n$$\nUsing Bayes' theorem, $p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$, we can express the posterior log-odds in terms of the class-conditional likelihood $p(x|y)$ and the class prior $p(y)$:\n$$\n\\log \\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)} = \\log \\frac{p(x|y=1)}{p(x|y=0)} + \\log \\frac{p(y=1)}{p(y=0)} = 0\n$$\nThis is the decision function for the Bayes-optimal classifier. The term $\\log \\frac{p(x|y=1)}{p(x|y=0)}$ is the log-likelihood ratio. Since $x=(x_v, x_t)$, the true joint likelihood is $p(x_v, x_t|y)$. The Bayes-optimal decision rule is therefore to classify as $y=1$ if:\n$$\n\\log p(x_v, x_t|y=1) - \\log p(x_v, x_t|y=0) + \\log \\frac{\\pi_1}{\\pi_0} > 0\n$$\n\nNow, consider the proposed late fusion rule. It forms a score for each class proportional to the product of per-modality likelihoods, $p(x_v|y) p(x_t|y)$. When combined with the class priors to make a decision, this rule is equivalent to choosing the class $y$ that maximizes the quantity $\\pi_y p(x_v|y) p(x_t|y)$. The decision boundary for this rule is where $\\pi_1 p(x_v|y=1) p(x_t|y=1) = \\pi_0 p(x_v|y=0) p(x_t|y=0)$. In log-space, this is:\n$$\n\\log \\frac{p(x_v|y=1)p(x_t|y=1)}{p(x_v|y=0)p(x_t|y=0)} + \\log \\frac{\\pi_1}{\\pi_0} = 0\n$$\nThis can be rewritten as:\n$$\n(\\log p(x_v|y=1) - \\log p(x_v|y=0)) + (\\log p(x_t|y=1) - \\log p(x_t|y=0)) + \\log \\frac{\\pi_1}{\\pi_0} > 0\n$$\n\nFor this late fusion rule to be Bayes-optimal, its decision function must be equivalent to the Bayes-optimal decision function for all $x$. Since the prior term $\\log(\\pi_1/\\pi_0)$ is common to both, their likelihood-based terms must be equivalent. Specifically, the following equality must hold for all $x$:\n$$\n\\log \\frac{p(x_v, x_t|y=1)}{p(x_v, x_t|y=0)} = \\log \\frac{p(x_v|y=1)p(x_t|y=1)}{p(x_v|y=0)p(x_t|y=0)}\n$$\nThis equality is the **necessary and sufficient condition on the data-generating distribution** for the product-of-likelihoods rule to be Bayes-optimal, assuming the per-modality models correctly estimate the true marginal likelihoods $p(x_v|y)$ and $p(x_t|y)$.\n\nThe role of conditional independence becomes evident when we analyze this condition further. By the chain rule of probability, the joint likelihood is $p(x_v, x_t|y) = p(x_v|y) p(x_t|x_v, y)$. If the modalities $x_v$ and $x_t$ are conditionally independent given the class label $y$, then by definition, $p(x_t|x_v, y) = p(x_t|y)$. In this case, the joint likelihood factorizes:\n$$\np(x_v, x_t|y) = p(x_v|y)p(x_t|y) \\quad \\text{for } y \\in \\{0, 1\\}\n$$\nIf this conditional independence holds for both classes, the true joint log-likelihood ratio becomes:\n$$\n\\log \\frac{p(x_v, x_t|y=1)}{p(x_v, x_t|y=0)} = \\log \\frac{p(x_v|y=1) p(x_t|y=1)}{p(x_v|y=0) p(x_t|y=0)}\n$$\nThis is precisely the log-likelihood ratio used by the late fusion model. Therefore, class-conditional independence of the features is a **sufficient condition** for the late fusion rule to be Bayes-optimal, provided that the per-modality models $p(x_v|y)$ and $p(x_t|y)$ are correctly specified (i.e., they represent the true marginals). Given correctly specified marginal models, this condition is also necessary.\n\n**Task 2: Error Analysis for the Gaussian Model**\n\nWe are given a generative model where $x | y=k \\sim \\mathcal{N}(\\mu_k, \\Sigma)$ for $k \\in \\{0,1\\}$, with $\\pi_0 = \\pi_1 = 1/2$ and a diagonal covariance matrix $\\Sigma = \\text{diag}(\\sigma_v^2, \\sigma_t^2)$.\n\n**Bayes-Optimal Classifier Error**\nThe Bayes-optimal classifier decides based on the log-likelihood ratio, since priors are equal. The log-likelihood for class $k$ is:\n$$\n\\log p(x|y=k) = C - \\frac{1}{2}(x - \\mu_k)^T \\Sigma^{-1}(x - \\mu_k)\n$$\nwhere $C = -\\frac{1}{2}\\log|2\\pi\\Sigma|$ is a constant. The decision boundary is where $\\log p(x|y=1) = \\log p(x|y=0)$, which simplifies to:\n$$\n(x - \\mu_1)^T \\Sigma^{-1}(x - \\mu_1) = (x - \\mu_0)^T \\Sigma^{-1}(x - \\mu_0)\n$$\nExpanding and simplifying yields a linear decision boundary:\n$$\n2(\\mu_1 - \\mu_0)^T \\Sigma^{-1} x - (\\mu_1^T \\Sigma^{-1} \\mu_1 - \\mu_0^T \\Sigma^{-1} \\mu_0) = 0\n$$\nLet's define a discriminant score $g(x) = (\\mu_1 - \\mu_0)^T \\Sigma^{-1} x - \\frac{1}{2}(\\mu_1^T \\Sigma^{-1} \\mu_1 - \\mu_0^T \\Sigma^{-1} \\mu_0)$. The decision rule is to choose $y=1$ if $g(x) > 0$. To find the error, we determine the distribution of $g(x)$. Since $g(x)$ is an affine transformation of the Gaussian vector $x$, $g(x)$ is also Gaussian.\n\nThe mean of $g(x)$ conditioned on $y=k$ is:\n$$\nE[g(x)|y=k] = (\\mu_1 - \\mu_0)^T \\Sigma^{-1} \\mu_k - \\frac{1}{2}(\\mu_1^T \\Sigma^{-1} \\mu_1 - \\mu_0^T \\Sigma^{-1} \\mu_0)\n$$\nFor $k=0$, the mean is $\\mu_{g,0} = -\\frac{1}{2}(\\mu_1 - \\mu_0)^T \\Sigma^{-1} (\\mu_1 - \\mu_0)$.\nFor $k=1$, the mean is $\\mu_{g,1} = +\\frac{1}{2}(\\mu_1 - \\mu_0)^T \\Sigma^{-1} (\\mu_1 - \\mu_0)$.\n\nThe variance of $g(x)$ is the same for both classes due to common covariance $\\Sigma$:\n$$\n\\text{Var}(g(x)|y=k) = ((\\mu_1 - \\mu_0)^T \\Sigma^{-1}) \\Sigma ((\\mu_1 - \\mu_0)^T \\Sigma^{-1})^T = (\\mu_1 - \\mu_0)^T \\Sigma^{-1} (\\mu_1 - \\mu_0)\n$$\nLet $\\Delta^2 = (\\mu_1 - \\mu_0)^T \\Sigma^{-1} (\\mu_1 - \\mu_0)$ be the squared Mahalanobis distance. Then $\\mu_{g,0} = -\\Delta^2/2$, $\\mu_{g,1} = \\Delta^2/2$, and the variance $\\sigma_g^2 = \\Delta^2$.\nThe probability of error is $P_{error} = \\pi_0 P(g(x)>0|y=0) + \\pi_1 P(g(x)<0|y=1)$. With $\\pi_0 = \\pi_1 = 1/2$:\n$$\nP(g(x)>0|y=0) = P\\left(Z > \\frac{0 - (-\\Delta^2/2)}{\\sqrt{\\Delta^2}}\\right) = P(Z > \\Delta/2) = \\Phi(-\\Delta/2)\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ and $\\Phi$ is its cumulative distribution function (CDF). By symmetry, $P(g(x)<0|y=1) = \\Phi(-\\Delta/2)$.\nThe total Bayes error is:\n$$\nP_{\\text{Bayes}} = \\frac{1}{2}\\Phi(-\\Delta/2) + \\frac{1}{2}\\Phi(-\\Delta/2) = \\Phi(-\\Delta/2)\n$$\nSubstituting the parameters $\\Delta\\mu = \\mu_1 - \\mu_0 = [\\Delta\\mu_v, \\Delta\\mu_t]^T$ and $\\Sigma^{-1} = \\text{diag}(1/\\sigma_v^2, 1/\\sigma_t^2)$:\n$$\n\\Delta^2 = \\frac{(\\Delta\\mu_v)^2}{\\sigma_v^2} + \\frac{(\\Delta\\mu_t)^2}{\\sigma_t^2}\n$$\nSo, the Bayes-optimal error is:\n$$\nP_{\\text{Bayes}} = \\Phi\\left(-\\frac{1}{2}\\sqrt{\\frac{(\\mu_{v1}-\\mu_{v0})^2}{\\sigma_v^2} + \\frac{(\\mu_{t1}-\\mu_{t0})^2}{\\sigma_t^2}}\\right)\n$$\n\n**Naive Early Fusion Error**\nThe naive early fusion rule uses the scalar score $s = x_v + x_t = [1, 1]x$. Since $x$ is Gaussian, $s$ is also Gaussian.\nThe mean of $s$ conditioned on $y=k$ is:\n$$\n\\mu_{s,k} = E[s|y=k] = \\mu_{vk} + \\mu_{tk}\n$$\nThe variance of $s$ is common to both classes:\n$$\n\\sigma_s^2 = \\text{Var}(s|y=k) = \\text{Var}(x_v+x_t) = \\text{Var}(x_v) + \\text{Var}(x_t) + 2\\text{Cov}(x_v, x_t)\n$$\nSince $\\Sigma$ is diagonal, $\\text{Cov}(x_v, x_t) = 0$, so $\\sigma_s^2 = \\sigma_v^2 + \\sigma_t^2$.\nWe have a 1D classification problem on $s$ with two distributions, $\\mathcal{N}(\\mu_{s,0}, \\sigma_s^2)$ and $\\mathcal{N}(\\mu_{s,1}, \\sigma_s^2)$, and equal priors. The optimal threshold is the midpoint of the means: $T = (\\mu_{s,0}+\\mu_{s,1})/2$.\nThe error probability is $P(s>T|y=0)$, assuming $\\mu_{s,1} > \\mu_{s,0}$.\n$$\nP(s>T|y=0) = P\\left(Z > \\frac{T - \\mu_{s,0}}{\\sigma_s}\\right) = P\\left(Z > \\frac{(\\mu_{s,0}+\\mu_{s,1})/2 - \\mu_{s,0}}{\\sigma_s}\\right) = P\\left(Z > \\frac{\\mu_{s,1}-\\mu_{s,0}}{2\\sigma_s}\\right)\n$$\nAs with the Bayes case, the total error is given by this one-sided probability:\n$$\nP_{\\text{early}} = \\Phi\\left(-\\frac{|\\mu_{s,1}-\\mu_{s,0}|}{2\\sigma_s}\\right)\n$$\nSubstituting the terms: $\\mu_{s,1}-\\mu_{s,0} = (\\mu_{v1}+\\mu_{t1}) - (\\mu_{v0}+\\mu_{t0}) = \\Delta\\mu_v + \\Delta\\mu_t$, and $\\sigma_s = \\sqrt{\\sigma_v^2+\\sigma_t^2}$.\nThe naive early fusion error is:\n$$\nP_{\\text{early}} = \\Phi\\left(-\\frac{|\\Delta\\mu_v + \\Delta\\mu_t|}{2\\sqrt{\\sigma_v^2+\\sigma_t^2}}\\right)\n$$",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the difference between naive early fusion error and Bayes-optimal error\n    for three specified test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Test case A: Happy path where early fusion is optimal\n        {\n            \"mu0\": np.array([-0.5, -0.5]),\n            \"mu1\": np.array([0.5, 0.5]),\n            \"sigma_v\": 1.0,\n            \"sigma_t\": 1.0,\n        },\n        # Test case B: Heteroskedastic modalities, naive weights are suboptimal\n        {\n            \"mu0\": np.array([-1.0, -1.0]),\n            \"mu1\": np.array([1.0, 1.0]),\n            \"sigma_v\": 2.0,\n            \"sigma_t\": 1.0,\n        },\n        # Test case C: One modality is non-informative and noisy\n        {\n            \"mu0\": np.array([0.0, -1.0]),\n            \"mu1\": np.array([0.0, 1.0]),\n            \"sigma_v\": 2.0,\n            \"sigma_t\": 0.5,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        mu0 = case[\"mu0\"]\n        mu1 = case[\"mu1\"]\n        sigma_v = case[\"sigma_v\"]\n        sigma_t = case[\"sigma_t\"]\n\n        # Calculate differences in means for each modality\n        delta_mu_v = mu1[0] - mu0[0]\n        delta_mu_t = mu1[1] - mu0[1]\n\n        # --- Bayes-Optimal (Late Fusion) Error Calculation ---\n        # This classifier is optimal for the given diagonal Gaussian model.\n        # The squared Mahalanobis distance is the sum of squared SNR for each modality.\n        mahalanobis_sq = (delta_mu_v**2 / sigma_v**2) + (delta_mu_t**2 / sigma_t**2)\n        \n        # The argument to the standard normal CDF is -1/2 times the Mahalanobis distance.\n        bayes_arg = -0.5 * np.sqrt(mahalanobis_sq)\n        bayes_error = norm.cdf(bayes_arg)\n\n        # --- Naive Early Fusion Error Calculation ---\n        # This classifier projects the data onto the vector [1, 1] and then classifies.\n        # The argument to the CDF is based on the SNR of the projected scalar value.\n        delta_mu_sum = delta_mu_v + delta_mu_t\n        variance_sum = sigma_v**2 + sigma_t**2\n        \n        early_arg = -np.abs(delta_mu_sum) / (2 * np.sqrt(variance_sum))\n        early_error = norm.cdf(early_arg)\n\n        # Calculate the difference in errors\n        error_difference = early_error - bayes_error\n        results.append(error_difference)\n\n    # Format the results as a comma-separated list of floats rounded to six decimals.\n    # e.g., [0.000000,0.053127,0.170135]\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}