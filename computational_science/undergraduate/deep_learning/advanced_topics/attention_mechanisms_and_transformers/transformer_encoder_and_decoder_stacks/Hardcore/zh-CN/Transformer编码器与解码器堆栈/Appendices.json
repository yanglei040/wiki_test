{
    "hands_on_practices": [
        {
            "introduction": "理解 Transformer 的核心在于理解其注意力机制。这个练习通过一个“玩具密码”任务，将抽象的注意力概念具体化为一个内容寻址内存系统。你将亲手实现查询（密文）、键（地址）和值（明文）的匹配过程，直观地体验 Transformer 如何通过相似度检索信息 。",
            "id": "3195550",
            "problem": "您将设计并分析一个玩具密码任务，该任务演示了 transformer 编码器如何通过基于注意力的密钥匹配来执行解密，以及该机制如何作为内容寻址存储器发挥作用。该任务的核心是一个检索过程，其中一组密钥向量和相应的值向量存储明文符号，一组查询向量表示必须通过基于相似度的加权将查询与密钥匹配来解密的密文，然后是一个归一化的加权方案和值的加权聚合。所有数学实体都必须严格地视为实数域上的向量和矩阵。您的程序必须实现检索机制，并使用为一组测试用例明确定义的准确率指标来量化解密成功率。\n\n使用的基本原理：\n- 实数域上的向量空间，向量在 $\\mathbb{R}^{d}$ 中，矩阵在 $\\mathbb{R}^{n \\times d}$ 中。\n- 欧几里得点积，对于 $x, y \\in \\mathbb{R}^{d}$，定义为 $x \\cdot y = \\sum_{i=1}^{d} x_i y_i$。\n- 在固定范数约束下，点积越大表示相似度越高的原理。\n- 一种归一化程序，通过指数化并通过指数和进行归一化，将一组实值分数转换为关于密钥的概率分布。\n- 一种缩放原理，用于抵消当 $d$ 增加时点积中的维度增长，通过在指数化之前将点积分数除以一个与维度相关的因子来实现。\n\n密码-存储器设置：\n- 密钥 $K \\in \\mathbb{R}^{N \\times d_k}$ 存储地址，值 $V \\in \\mathbb{R}^{N \\times d_v}$ 以独热编码形式存储明文符号，查询 $Q \\in \\mathbb{R}^{N \\times d_k}$ 表示必须与密钥匹配以恢复明文的密文。使用 $N = 5$，$d_k = 4$ 和 $d_v = 5$。\n\n- 按如下方式定义密钥 $K$ 的行向量 $k_0, k_1, k_2, k_3, k_4 \\in \\mathbb{R}^{4}$，每个向量都写成单位范数向量：\n  $k_0 = (1, 0, 0, 0)$,\n  $k_1 = (0, 1, 0, 0)$,\n  $k_2 = (0, 0, 1, 0)$,\n  $k_3 = (0, 0, 0, 1)$,\n  $k_4 = \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0, 0\\right)$.\n  将这些向量收集到 $K$ 中，形式为 $K = \\begin{bmatrix} k_0^\\top \\\\ k_1^\\top \\\\ k_2^\\top \\\\ k_3^\\top \\\\ k_4^\\top \\end{bmatrix}$。\n\n- 将值 $V \\in \\mathbb{R}^{5 \\times 5}$ 定义为 $5 \\times 5$ 的单位矩阵，因此 $k_i$ 的明文标签是索引 $i \\in \\{0, 1, 2, 3, 4\\}$，并且相关的值向量是独热向量 $e_i \\in \\mathbb{R}^{5}$，其在位置 $i$ 处为 $1$，其他位置为 $0$。\n\n- 对于查询 $q \\in \\mathbb{R}^{4}$，预测的明文符号通过以下步骤获得：\n  1. 使用点积计算与所有密钥的相似度分数。\n  2. 将分数除以一个与维度相关的缩放因子，以在 $d_k$ 变化时保持稳定性。\n  3. 应用指数化和归一化以获得关于密钥的概率分布。\n  4. 使用这些概率对值向量进行加权求和，以产生一个在 $\\mathbb{R}^{5}$ 中的输出向量。\n  5. 选择其索引为输出向量最大分量位置的符号（top-1 预测）。\n\n归一化细节：\n- 在引入任何加性噪声后，所有查询都必须归一化为单位 $\\ell_2$ 范数。\n\n解密准确率：\n- 对于一个包含 $N$ 个查询的序列，top-1 解密准确率是指预测的符号索引与查询预期密钥关联的真实标签相等的查询所占的比例。\n\n测试套件：\n实现三个测试用例，以检验基于注意力的解密的不同方面。\n\n- 测试用例 A（理想情况，精确匹配）：\n  使用排列 $\\pi_A = [2, 0, 4, 1, 3]$，并通过对密钥进行排列来定义 $Q_A$：$Q_A = \\begin{bmatrix} k_{\\pi_A(0)}^\\top \\\\ k_{\\pi_A(1)}^\\top \\\\ k_{\\pi_A(2)}^\\top \\\\ k_{\\pi_A(3)}^\\top \\\\ k_{\\pi_A(4)}^\\top \\end{bmatrix}$。真实的明文序列是标签列表 $[ \\pi_A(0), \\pi_A(1), \\pi_A(2), \\pi_A(3), \\pi_A(4) ]$。\n\n- 测试用例 B（带噪查询，扰动下的稳定性）：\n  从 $Q_A$ 开始，向每个分量添加标准差为 $\\sigma = 0.30$ 的独立高斯噪声，然后将每个查询重新归一化为单位范数以获得 $Q_B$。使用与测试用例 A 中相同的真实标签。\n\n- 测试用例 C（密钥冲突，内容寻址中的模糊性）：\n  通过取 $K$ 并将 $k_1$ 替换为 $k_3$ 来创建一个冲突的密钥集 $K_C$，即，设置 $k_1 := k_3$。使用排列 $\\pi_C = [1, 3, 0, 4, 2]$，并相应地通过排列 $K_C$ 的行来定义 $Q_C$。使用真实明文标签 $[ \\pi_C(0), \\pi_C(1), \\pi_C(2), \\pi_C(3), \\pi_C(4) ]$。值 $V$ 保持为 $5 \\times 5$ 的单位矩阵，这意味着两个相同的密钥仍然对应于不同的明文符号。\n\n要求输出：\n- 您的程序必须按 A、B、C 的顺序计算三个测试用例中每个用例的 top-1 解密准确率。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如 $[a_A,a_B,a_C]$，其中每个 $a_\\cdot$ 是一个十进制形式的浮点数。",
            "solution": "问题陈述已经过分析并被认为是有效的。它在科学上基于线性代数的原理和深度学习中的注意力机制概念。该问题定义良好、自成体系，并且可以形式化为一个计算任务。其中没有矛盾、模糊或事实不准确之处。\n\n任务是设计和分析一个基于缩放点积注意力机制的玩具密码，该机制可作为内容寻址存储器的模型。这涉及通过将密文（查询）与一组存储的地址（密钥）进行匹配来检索明文符号（值）。此解密过程的成功与否由 top-1 准确率指标量化。\n\n解密机制的核心是注意力函数，它为一组给定的查询 $Q \\in \\mathbb{R}^{N \\times d_k}$、一组密钥 $K \\in \\mathbb{R}^{N \\times d_k}$ 和一组值 $V \\in \\mathbb{R}^{N \\times d_v}$ 计算输出。维度给定为 $N=5$，$d_k=4$ 和 $d_v=5$。对于查询矩阵 $Q$ 的处理过程定义如下：\n$1$。计算每个查询与所有密钥之间的相似度分数。这通过矩阵乘法执行：$S = QK^\\top$。得到的分数矩阵 $S \\in \\mathbb{R}^{N \\times N}$ 包含了每个查询向量与每个密钥向量的点积。\n$2$。缩放分数以抵消点积方差随维度增长的影响。每个分数都除以 $\\sqrt{d_k}$。缩放后的分数矩阵为 $S' = \\frac{QK^\\top}{\\sqrt{d_k}}$。\n$3$。对每个查询的缩放分数进行归一化，以形成关于密钥的概率分布。这通过对 $S'$ 逐行应用 softmax 函数来实现：$A = \\text{softmax}(S')$。得到的注意力矩阵 $A \\in \\mathbb{R}^{N \\times N}$ 的元素 $A_{ij}$ 代表查询 $i$ 对密钥 $j$ 的权重。\n$4$。将输出向量计算为值向量的加权和，其中权重是注意力概率。这计算为 $O = AV$。得到的输出矩阵 $O \\in \\mathbb{R}^{N \\times d_v}$ 包含检索到的（解密的）表示。\n$5$。对于每个输出向量 $o_i$（$O$ 中的一行），预测的明文符号索引 $\\hat{y}_i$ 是其最大分量的索引：$\\hat{y}_i = \\arg\\max_j (o_i)_j$。\n\n解密准确率是预测符号索引与真实符号索引相匹配的查询所占的比例：$\\text{Accuracy} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\mathbb{I}(\\hat{y}_i = y_i)$，其中 $\\mathbb{I}$ 是指示函数。\n\n密钥和值矩阵定义如下。密钥向量为：\n$k_0 = (1, 0, 0, 0)$\n$k_1 = (0, 1, 0, 0)$\n$k_2 = (0, 0, 1, 0)$\n$k_3 = (0, 0, 0, 1)$\n$k_4 = \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0, 0\\right)$\n这些向量作为行被收集到密钥矩阵 $K \\in \\mathbb{R}^{5 \\times 4}$ 中。值矩阵 $V \\in \\mathbb{R}^{5 \\times 5}$ 是单位矩阵，$V=I_5$。这意味着与密钥 $k_i$ 关联的值向量 $v_i$ 是对应于标签 $i$ 的独热向量 $e_i$。\n\n我们现在分析三个指定的测试用例。\n\n**测试用例 A：精确匹配**\n在这种情况下，查询向量是密钥向量的完美副本，尽管顺序是经过排列的。排列为 $\\pi_A = [2, 0, 4, 1, 3]$，所以查询矩阵是 $Q_A$，其中第 $i$ 行是 $k_{\\pi_A(i)}$。真实标签是 $Y_A = \\pi_A$。\n对于任何查询 $q_i = k_{\\pi_A(i)}$，其与匹配密钥 $k_{\\pi_A(i)}$ 的点积将为 $1$（因为所有密钥都是单位范数），而其与任何其他正交密钥（例如 $k_0 \\cdot k_1 = 0$）的点积将为 $0$。与非正交密钥 $k_4$ 的点积将小于 $1$。例如，对于 $q_0 = k_2$，分数将是 $[k_2 \\cdot k_0, k_2 \\cdot k_1, k_2 \\cdot k_2, k_2 \\cdot k_3, k_2 \\cdot k_4] = [0, 0, 1, 0, 0]$。softmax 函数会将所有概率质量集中在匹配密钥的索引上。因此，注意力机制将完美地检索到正确的值向量 $v_{\\pi_A(i)}$，预测的标签将是 $\\pi_A(i)$。这对所有查询都应该成立，从而得到 $1.0$ 的解密准确率。\n\n**测试用例 B：带噪查询**\n这里，我们从 $Q_A$ 开始，并向每个分量添加标准差为 $\\sigma = 0.30$ 的高斯噪声。然后将得到的带噪查询向量重新归一化为单位 $\\ell_2$ 范数。这模拟了一个更现实的场景，其中密文可能被损坏。\n噪声扰动了查询向量。一个从 $k_{\\pi_A(i)}$ 派生的带噪查询 $q_i'$ 仍然会与其原始密钥 $k_{\\pi_A(i)}$ 最相似，但其与其他密钥的点积分数将不再是精确的零。只要噪声不是特别大，分数 $q_i' \\cdot k_{\\pi_A(i)}$ 仍将是最大的分数。因此，softmax 函数仍会将最高的注意力权重分配给正确的密钥。然而，如果噪声大到足以使查询与不正确的密钥更相似，则会发生解密错误。在 $\\sigma=0.30$ 的情况下，我们预计准确率会很高但不是完美的。\n\n**测试用例 C：密钥冲突**\n这个案例演示了内容寻址存储器的一种失败模式：由非唯一密钥引起的模糊性。密钥矩阵 $K_C$ 是通过将原始矩阵 $K$ 中的密钥 $k_1$ 替换为密钥 $k_3$ 来创建的。因此，$K_C$ 在索引 $1$ 和 $3$ 处有两行相同的行，均等于向量 $k_3 = (0, 0, 0, 1)$。值矩阵 $V$ 保持为单位矩阵，这意味着索引 $1$ 处的密钥与明文标签 $1$ 相关联（值 $v_1=e_1$），而索引 $3$ 处的密钥与标签 $3$ 相关联（值 $v_3=e_3$）。\n查询 $Q_C$ 是通过使用排列 $\\pi_C = [1, 3, 0, 4, 2]$ 排列这个新密钥矩阵 $K_C$ 的行而生成的。真实标签为 $Y_C = \\pi_C$。\n我们来分析查询 $q_0 = k_{C, \\pi_C(0)} = k_{C,1}$。由于 $K_C$ 的第 $1$ 行是 $k_3$，所以这个查询是 $q_0 = k_3$。真实标签是 $y_0 = \\pi_C(0) = 1$。当我们计算此查询相对于 $K_C$ 的注意力分数时，点积 $q_0 \\cdot k_{C,j}$ 对 $j=1$ 和 $j=3$ 都会是最大且相等的，因为 $k_{C,1}$ 和 $k_{C,3}$ 都与 $q_0$ 相同。softmax 函数将为密钥 $1$ 和 $3$ 分配相等的高注意力权重。得到的输出向量将是值向量的和，主要由 $w_1 v_1 + w_3 v_3$ 决定，其中 $w_1=w_3$。这意味着输出向量在索引 $1$ 和 $3$ 处将有相等的较大分量。根据约定（例如，在 NumPy 中），$\\arg\\max$ 函数通过返回最大值的第一个索引来打破平局。因此，对 $q_0$ 的预测将是 $1$，这与真实标签 $y_0=1$ 相匹配。\n现在考虑查询 $q_1 = k_{C, \\pi_C(1)} = k_{C,3}$。这个查询也是向量 $k_3$，与 $q_0$ 相同。预测过程是相同的，所以预测的标签将再次是 $1$。然而，真实标签是 $y_1 = \\pi_C(1) = 3$。这是一个解密错误。\n对于其他查询（$q_2, q_3, q_4$），它们匹配 $K_C$ 中的唯一密钥，预计将被正确解密。总的来说，我们预计 $5$ 个查询中有 $1$ 个错误，导致准确率为 $4/5 = 0.8$。\n这个案例突出表明，当密钥不唯一时，注意力机制无法区分它们，检索变得模糊，其结果可能取决于任意的平局打破规则。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the toy cipher problem by implementing and testing a simplified\n    attention mechanism.\n    \"\"\"\n    N = 5\n    d_k = 4\n    d_v = 5\n\n    def softmax(x, axis=-1):\n        \"\"\"Numerically stable softmax function.\"\"\"\n        e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n\n    def attention_decrypt(Q, K, V):\n        \"\"\"\n        Performs decryption using the scaled dot-product attention mechanism.\n        Returns the predicted symbol indices.\n        \"\"\"\n        # 1. Compute similarity scores and scale them\n        scores = (Q @ K.T) / np.sqrt(d_k)\n        \n        # 2. Compute attention weights\n        attention_weights = softmax(scores, axis=1)\n        \n        # 3. Compute output vectors (weighted sum of values)\n        output = attention_weights @ V\n        \n        # 4. Predict symbols by taking the argmax\n        predictions = np.argmax(output, axis=1)\n        \n        return predictions\n\n    def calculate_accuracy(predictions, ground_truth):\n        \"\"\"Calculates the top-1 decryption accuracy.\"\"\"\n        return np.mean(predictions == ground_truth)\n\n    # --- Common Setup ---\n    # Define the base key matrix K\n    K = np.array([\n        [1.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0],\n        [1/np.sqrt(2), 1/np.sqrt(2), 0.0, 0.0]\n    ])\n\n    # Define the value matrix V as the identity matrix\n    V = np.identity(N)\n\n    results = []\n\n    # --- Test Case A: Happy path, exact matches ---\n    pi_A = np.array([2, 0, 4, 1, 3])\n    Q_A = K[pi_A]\n    Y_A = pi_A\n    \n    predictions_A = attention_decrypt(Q_A, K, V)\n    accuracy_A = calculate_accuracy(predictions_A, Y_A)\n    results.append(accuracy_A)\n\n    # --- Test Case B: Noisy queries, stability under perturbations ---\n    # Use a fixed seed for reproducibility\n    np.random.seed(42)\n    sigma = 0.30\n    \n    noise = np.random.normal(scale=sigma, size=Q_A.shape)\n    Q_B_noisy = Q_A + noise\n    \n    # Renormalize each query to unit norm\n    norms = np.linalg.norm(Q_B_noisy, axis=1, keepdims=True)\n    Q_B = Q_B_noisy / norms\n    \n    # Ground truth remains the same\n    Y_B = Y_A\n    \n    predictions_B = attention_decrypt(Q_B, K, V)\n    accuracy_B = calculate_accuracy(predictions_B, Y_B)\n    results.append(accuracy_B)\n\n    # --- Test Case C: Key collision, ambiguity in content addressing ---\n    # Create the colliding key set K_C\n    K_C = K.copy()\n    K_C[1] = K[3]  # k_1 is replaced by k_3\n    \n    pi_C = np.array([1, 3, 0, 4, 2])\n    \n    # Queries Q_C are permuted rows of K_C\n    Q_C = K_C[pi_C]\n    \n    # Ground truth labels\n    Y_C = pi_C\n    \n    predictions_C = attention_decrypt(Q_C, K_C, V)\n    accuracy_C = calculate_accuracy(predictions_C, Y_C)\n    results.append(accuracy_C)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "理解了基本的注意力检索机制后，下一步是探究它在编码器和解码器堆栈中的不同应用，其关键区别在于所允许的信息流方向。本练习通过一个回文检测任务，清晰地对比了能够“看到”整个序列的双向编码器和只能“回顾”历史的因果解码器。这有助于你深刻理解为何这两种结构分别适用于不同的场景，例如文本分析与序列生成 。",
            "id": "3195539",
            "problem": "你将构建、分析并以算法方式决策一个根本上需要双向上下文的合成序列分类任务，你将比较一个编码器层堆栈和一个带因果掩码的解码器层堆栈可以计算的内容。然后，你将实现一个程序，在给定一个小的参数值测试套件的情况下，确定哪种架构可以在这些参数下解决该任务。所有数学实体都必须用 LaTeX 书写。\n\n任务定义。考虑长度为奇数 $n \\in \\mathbb{N}$ 的二进制序列，其在中间位置有一个特殊的查询标记。将序列写为\n$$\nx = \\big(a_{1}, a_{2}, \\dots, a_{m-1}, \\mathrm{[Q]}, b_{1}, b_{2}, \\dots, b_{m-1}\\big),\n$$\n其中 $n = 2m-1$，$m = \\frac{n+1}{2}$，并且对于 $k \\in \\{1,\\dots,m-1\\}$，每个 $a_{k}, b_{k} \\in \\{0,1\\}$。将分类函数 $f:\\{0,1\\}^{n-1}\\to \\{0,1\\}$ 定义为\n$$\nf(x) = 1 \\;\\;\\text{当且仅当}\\;\\; \\forall d \\in \\{1,\\dots,m-1\\},\\; a_{d} = b_{d},\n$$\n否则 $f(x)=0$。换句话说，距离 $\\mathrm{[Q]}$ 左侧 $d$ 处的标记必须等于距离 $\\mathrm{[Q]}$ 右侧 $d$ 处的标记；这是一个围绕 $\\mathrm{[Q]}$ 的中心回文检查。\n\n架构模型。考虑两种架构，它们由 $L \\in \\mathbb{N}$ 个相同的多头注意力（Multi-Head Attention, MHA）层构成，每层、每头都有一个宽度为 $w \\in \\mathbb{N}$ 的窗口化自注意力约束，其后是逐位置前馈网络。单层的注意力图抽象为：\n- 编码器堆栈：从每个位置 $i$ 到所有满足 $|i-j| \\le w$ 的位置 $j$ 都有有向边（窗口内双向）。\n- 带因果掩码的解码器堆栈：从每个位置 $i$ 到所有满足 $0 \\le i-j \\le w$ 的位置 $j$ 都有有向边（仅窗口内向左，包括自身）。\n\n信息传播模型。将每一层建模为沿边进行单步消息传递。经过 $L$ 层，信息可以沿着由层连接性定义的有向图中的任何最多包含 $L$ 条边的路径传播。该任务的决策必须在经过恰好 $L$ 层后，在中间查询位置 $\\mathrm{[Q]}$ 产生。\n\n你的目标。\n1. 从注意力窗口所产生的有向图可达性的第一性原理出发，证明对于带因果掩码的解码器，对于任何 $n1$，无论 $w$ 和 $L$ 取何值，$\\mathrm{[Q]}$ 处的标记都无法访问来自右半部分 $\\{b_{1},\\dots,b_{m-1}\\}$ 的任何信息。得出结论：当 $n1$ 时，这种形式的解码器都无法对所有输入精确计算 $f$。\n2. 从相同的原理出发，推导出一个关于 $n$、$w$ 和 $L$ 的充分必要条件，在该条件下，编码器堆栈可以在 $\\mathrm{[Q]}$ 处精确计算 $f$。你的推导必须从有向图可达性的角度开始，得出一个涉及 $n$、$w$ 和 $L$ 的封闭形式条件，并通过一个将证据聚合到 $\\mathrm{[Q]}$ 的构造性方案来证明其合理性。\n3. 设计一个用 $n$、$w$ 和 $L$ 表示的决策规则，该规则返回两个布尔值：一个指示编码器堆栈是否能对所有长度为 $n$ 的输入在 $\\mathrm{[Q]}$ 处精确解决该任务，另一个指示带因果掩码的解码器是否能做到。你的规则必须能正确处理 $n=1$ 的退化情况。\n\n程序规范。实现一个完整、可运行的程序，该程序在以下 $(n,w,L)$ 三元组的测试套件上评估你的决策规则：\n- $(n,w,L) = (21,3,4)$,\n- $(n,w,L) = (17,4,2)$,\n- $(n,w,L) = (31,3,4)$,\n- $(n,w,L) = (1,1,1)$,\n- $(n,w,L) = (19,2,5)$.\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。对于每个测试用例，输出一个由下式定义的整数代码 $c \\in \\{0,1,2,3\\}$：\n$$\nc \\;=\\; 2\\cdot \\mathbf{1}\\{\\text{编码器可解}\\}\\;+\\; \\mathbf{1}\\{\\text{解码器可解}\\},\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数，如果语句为真则返回 $1$，否则返回 $0$。例如，$c=2$ 表示编码器可解而解码器不可解，$c=3$ 表示两者皆可，$c=0$ 表示两者皆不可，$c=1$ 表示解码器可解而编码器不可解。对于上面列出的五个案例，最终输出格式必须与 $[c_{1},c_{2},c_{3},c_{4},c_{5}]$ 完全一致。",
            "solution": "经评估，问题陈述是有效的。这是一个适定、有科学依据且客观的问题，它探讨了理想化 Transformer 架构的计算极限。该问题基于一个标准且有用的抽象，即将神经网络中的信息流视为图的可达性，这是深度学习模型理论分析的核心概念。所有定义和约束都清晰且自洽。\n\n现在我们开始解答，按照要求分为三个部分。\n\n首先，我们将问题的信息流模型形式化。序列长度为 $n$，位置索引从 $1$ 到 $n$。中心查询标记 $\\mathrm{[Q]}$ 位于位置 $m = (n+1)/2$。任务是验证对于所有 $d \\in \\{1, \\dots, m-1\\}$，是否有 $a_d = b_d$，其中 $a_d$ 位于位置 $m-d$，$b_d$ 位于位置 $m+d$。决策在位置 $m$ 做出。根据对 Transformer 机制的标准解释，一个层在位置 $i$ 的计算是从一组位置 $\\{j\\}$ *收集*信息。因此，问题中“从每个位置 $i$ 到所有位置 $j$ 的有向边”的描述被解释为位置 $i$ 所关注的位置集合 $\\{j\\}$，这意味着信息从 $j$ 流向 $i$。因此，信息流图的有向边是 $(j, i)$。\n\n### 1. 带因果掩码的解码器堆栈分析\n\n解码器架构由一个因果注意力掩码定义。在我们的形式模型下，当且仅当位置 $i$ 关注位置 $j$ 时，信息流图中存在一条边 $(j, i)$。该规则由 $0 \\le i-j \\le w$ 给出。这意味着 $i-w \\le j \\le i$。\n\n这个规则的一个关键属性是，对于任何边 $(j, i)$，源索引 $j$ 必须小于或等于目标索引 $i$（即 $j \\le i$）。现在，考虑一个信息传播路径，它是图中一系列连接的节点 $(p_0, p_1, \\dots, p_k)$，其中 $p_0$ 是信息的原始来源，$p_k$ 是最终目的地。要使之成为一条有效路径，对于每个 $s \\in \\{0, \\dots, k-1\\}$，都必须存在一条边 $(p_s, p_{s+1})$。应用边规则，这要求对于所有步骤 $s$，都有 $p_s \\le p_{s+1}$。因此，解码器信息流图中的任何路径都必须具有非递减的位置索引：$p_0 \\le p_1 \\le \\dots \\le p_k$。\n\n分类任务要求检查 $a_d = b_d$，其中 $b_d$ 是位置 $m+d$ 处的标记。对于任何 $d \\in \\{1, \\dots, m-1\\}$，位置 $m+d$ 严格大于 $m$。要在位置 $m$ 做出决策，关于标记 $b_d$ 的信息必须从其源位置 $j_{src} = m+d$ 传播到目标位置 $i_{dst} = m$。\n\n这将要求存在一条从 $p_0 = m+d$ 到 $p_k = m$ 的路径。然而，如上所证，任何这样的路径都必须满足 $p_0 \\le p_k$，这意味着 $m+d \\le m$。对于任何 $d \\ge 1$，这个不等式都不成立。因此，不存在从序列右半部分（大于 $m$ 的位置）的任何位置到中心查询位置 $m$ 的路径。\n\n由于 $f(x)$ 的值依赖于标记 $\\{b_1, \\dots, b_{m-1}\\}$，而来自这些标记的信息无法到达位置 $m$，因此对于任何 $n  1$ 的输入序列，带因果掩码的解码器堆栈都无法计算 $f(x)$。\n\n退化情况是 $n=1$。此时，$m = (1+1)/2 = 1$，$d$ 的范围是 $\\{1, \\dots, m-1\\} = \\emptyset$。条件 $\\forall d \\in \\emptyset, a_d=b_d$ 是无条件成立的（空真）。因此，对于 $n=1$，$f(x)=1$ 对所有输入都成立。解码器可以轻易地学习输出这个恒定值。\n\n结论：带因果掩码的解码器当且仅当 $n=1$ 时才能解决该任务。\n\n### 2. 编码器堆栈分析\n\n编码器架构具有双向注意力。如果 $|i-j| \\le w$，则存在一条边 $(j,i)$。这个条件是对称的，意味着边 $(i,j)$ 也存在。信息可以在窗口 $w$ 内的任意两个位置之间双向流动。\n\n为了在位置 $m$ 计算 $f(x)$，模型必须能访问序列中所有其他位置的信息。模型由 $L$ 层组成。在单层中，位置 $j$ 的信息可以传播到区间 $[j-w, j+w]$ 内的任何位置 $i$。经过 $L$ 层后，来自初始位置 $j$ 的信息已传播覆盖区间 $[j-Lw, j+Lw]$。这就是有效感受野。\n\n为了使位置 $m$ 的决策是完全知情的，来自序列 $\\{1, \\dots, n\\}$ 中每个位置的信息都必须能在 $L$ 层内到达 $m$。这意味着对于任何位置 $j \\in \\{1, \\dots, n\\}$，$m$ 必须在 $L$ 层后位于 $j$ 的感受野内。即，$m \\in [j-Lw, j+Lw]$，这等价于 $|m-j| \\le Lw$。\n\n我们必须确保这个条件对所有 $j$ 都成立。最严格的要求来自距离中心 $m$ 最远的位置。这些是序列的端点，$j=1$ 和 $j=n$。\n从中心 $m = (n+1)/2$ 到端点的距离是：\n- 到位置 1: $|m-1| = |\\frac{n+1}{2} - 1| = |\\frac{n-1}{2}| = \\frac{n-1}{2}$。\n- 到位置 n: $|m-n| = |\\frac{n+1}{2} - n| = |\\frac{1-n}{2}| = \\frac{n-1}{2}$。\n\n由于这些是最大距离，条件简化为确保信息能传播这么远。信息在 $L$ 层内可以传播的总距离是 $Lw$。因此，所有信息到达中心的充分必要条件是：\n$$\nLw \\ge \\frac{n-1}{2}\n$$\n如果满足这个条件，来自所有对 $(a_d, b_d)$ 的信息都可以在位置 $m$ 聚合，从而允许一个足够强大的模型（我们假设如此）来计算函数 $f(x)$。如果不满足该条件，则至少有一个标记（在端点处）的信息无法到达中心，使得对于某些输入无法进行计算。注意，由于 $n$ 是奇数，$n-1$ 是偶数，$(n-1)/2$ 总是一个整数。\n\n对于特殊情况 $n=1$，条件变为 $Lw \\ge (1-1)/2$，即 $Lw \\ge 0$。由于 $L \\in \\mathbb{N}$ 和 $w \\in \\mathbb{N}$，我们有 $L \\ge 1$ 和 $w \\ge 1$，所以这总是成立的。这与 $n=1$ 时任务是平凡的这一事实相符。\n\n结论：编码器堆栈当且仅当 $Lw \\ge (n-1)/2$ 时才能解决该任务。\n\n### 3. 最终决策规则\n\n基于以上分析，我们可以为给定的三元组 $(n, w, L)$ 制定决策规则。\n\n- **编码器可解性**：编码器堆栈当且仅当累积信息传播距离 $Lw$ 足以覆盖从序列端点到中心的距离时，才能解决该任务。\n  $$ \\mathbf{1}\\{\\text{编码器可解}\\} = \\mathbf{1}\\left\\{ Lw \\ge \\frac{n-1}{2} \\right\\} $$\n\n- **解码器可解性**：带因果掩码的解码器堆栈当且仅当序列长度为 $n=1$（此时分类任务是平凡的）时，才能解决该任务。\n  $$ \\mathbf{1}\\{\\text{解码器可解}\\} = \\mathbf{1}\\{ n = 1 \\} $$\n\n每个测试用例的整数代码 $c$ 按如下方式计算：\n$$ c = 2\\cdot \\mathbf{1}\\{\\text{编码器可解}\\} + \\mathbf{1}\\{\\text{解码器可解}\\} $$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the transformer architecture problem by applying the derived decision rules\n    to a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (21, 3, 4),  # (n, w, L) triplet 1\n        (17, 4, 2),  # (n, w, L) triplet 2\n        (31, 3, 4),  # (n, w, L) triplet 3\n        (1, 1, 1),   # (n, w, L) triplet 4\n        (19, 2, 5),  # (n, w, L) triplet 5\n    ]\n\n    results = []\n    for n, w, l in test_cases:\n        # A. Decision rule for the decoder stack with a causal mask.\n        # The task is a center-palindrome check.\n        # A decoder with a causal mask prevents information flow from the future (right side)\n        # to the past (left side or center).\n        # When n  1, the decision at the center position m = (n+1)/2 cannot access\n        # information from any position j  m.\n        # Thus, the comparison required by the task is impossible.\n        # The only exception is the trivial case n=1, where the palindrome condition is\n        # vacuously true, and the model only needs to output a constant 1.\n        decoder_can_solve = (n == 1)\n\n        # B. Decision rule for the encoder stack.\n        # An encoder has bidirectional attention. Information can flow from any position j to\n        # any position i, provided enough layers L and a large enough window w.\n        # The key condition is that information from the sequence endpoints (positions 1 and n)\n        # must be able to reach the center position m = (n+1)/2.\n        # The distance from either endpoint to the center is (n-1)/2.\n        # In L layers, with an attention window of width w, information can propagate\n        # a maximum distance of L * w.\n        # The task is solvable if and only if this reach is sufficient to cover the distance.\n        # This condition also correctly handles the n=1 case (l * w = 0, which is always true).\n        encoder_can_solve = (l * w = (n - 1) / 2)\n\n        # C. Calculate the integer code c as per the problem specification.\n        # c = 2 * I{encoder can solve} + 1 * I{decoder can solve}\n        # where I{.} is the indicator function (1 if true, 0 if false).\n        c = 2 * int(encoder_can_solve) + 1 * int(decoder_can_solve)\n        results.append(c)\n\n    # Final print statement in the exact required format: [c1,c2,c3,c4,c5]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了编码器和解码器的宏观结构后，我们将深入探究其关键组件——多头注意力（MHA）的一个实际问题。本练习将引导你诊断“头坍塌”问题，即不同注意力头学习到冗余模式的现象。你将通过构造特定输入来触发此问题，并探索一种通过正则化促进多样性的解决方案，从而深入了解模型训练中的高级技巧 。",
            "id": "3195528",
            "problem": "考虑一个带有 Transformer 编码器或解码器，其中包含多头注意力（MHA）。每个注意力头对输入序列计算缩放点积注意力（SDPA）。设输入为矩阵 $X \\in \\mathbb{R}^{L \\times d_{\\text{model}}}$，其中有 $L$ 个词元，模型维度为 $d_{\\text{model}}$。对于头 $h \\in \\{1,\\dots,H\\}$，查询和键的计算方式为 $Q^{(h)} = X W_Q^{(h)}$ 和 $K^{(h)} = X W_K^{(h)}$，其中 $W_Q^{(h)} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{k}}}$ 和 $W_K^{(h)} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{k}}}$，$d_{\\text{k}}$ 是头维度。缩放后的分数矩阵为 $S^{(h)} = \\frac{1}{\\sqrt{d_{\\text{k}}}} Q^{(h)} (K^{(h)})^{\\top} \\in \\mathbb{R}^{L \\times L}$。头 $h$ 的注意力图为 $A^{(h)} = \\operatorname{Softmax}_{\\text{row}}(S^{(h)})$，其中 $\\operatorname{Softmax}_{\\text{row}}$ 逐行应用 softmax 函数：对于行 $i \\in \\{1,\\dots,L\\}$，$A_{i,:}^{(h)} = \\operatorname{softmax}(S_{i,:}^{(h)})$，并且 $\\operatorname{softmax}(z)_j = \\frac{\\exp(z_j)}{\\sum_{m=1}^{L} \\exp(z_m)}$。在整个过程中，softmax 对于给一行的所有条目加上一个常数是不变的，并且如果行中的所有条目都相等，则会产生一个均匀分布。\n\n定义一个注意力头多样性正则化器，用以惩罚不同注意力图之间的相似性。对于每个头 $h$，将其注意力图 $A^{(h)}$ 展平为一个向量 $a^{(h)} \\in \\mathbb{R}^{L \\cdot L}$（通过连接各行）。对于任意两个不同的头 $h \\neq k$，将其余弦相似度定义为\n$$\n\\operatorname{cos}(a^{(h)}, a^{(k)}) = \\frac{\\langle a^{(h)}, a^{(k)} \\rangle}{\\|a^{(h)}\\|_2 \\, \\|a^{(k)}\\|_2}.\n$$\n设正则化器为由一个非负权重 $\\lambda$ 缩放的成对余弦相似度均值，即\n$$\n\\mathcal{R}(A^{(1)},\\dots,A^{(H)}) = \\lambda \\cdot \\frac{1}{\\binom{H}{2}} \\sum_{1 \\le h  k \\le H} \\operatorname{cos}\\!\\left(a^{(h)}, a^{(k)}\\right).\n$$\n\n任务 A（输入模式构造）：从第一性原理出发，构造导致头坍缩（head collapse）的输入模式 $X$，头坍缩意指所有头都产生几乎相同的注意力图 $A^{(h)}$。利用 $\\operatorname{Softmax}$ 和 SDPA 的基本性质，论证为何此类输入会导致 $A^{(h)}$ 在所有头 $h$ 之间都相同。提供至少两种基于基本定义的不同模式。\n\n任务 B（正则化器梯度）：从余弦相似度的定义出发，推导正则化器 $\\mathcal{R}$ 相对于每个头 $h$ 的展平注意力向量 $a^{(h)}$ 的梯度。您的推导必须从内积、向量范数和链式法则的基本定义开始，并且不得使用任何快捷公式。将最终梯度 $\\nabla_{a^{(h)}} \\mathcal{R}$ 表示为 $\\{a^{(j)}\\}_{j=1}^{H}$ 的函数。\n\n实现任务：编写一个完整、可运行的程序，该程序能够\n- 实现 SDPA，为给定的 $X$、$\\{W_Q^{(h)}\\}$ 和 $\\{W_K^{(h)}\\}$ 计算 $\\{A^{(h)}\\}_{h=1}^{H}$。\n- 将每个 $A^{(h)}$ 展平为 $a^{(h)}$。\n- 计算 $\\mathcal{R}$ 和在任务 B 中推导出的解析梯度 $\\{\\nabla_{a^{(h)}} \\mathcal{R}\\}_{h=1}^{H}$。\n- 通过计算平均成对余弦相似度并检查其是否超过一个接近 $1$ 的阈值来检测头坍缩。\n- 为一个测试套件生成结果，该套件包含一个典型案例、一个多样化案例和一个边缘案例。\n\n您必须使用以下包含明确数值参数的测试套件：\n- 案例 1（通过相同词元导致的坍缩）：$L = 3$，$d_{\\text{model}} = 4$，$d_{\\text{k}} = 2$，$H = 3$，$\\lambda = 0.5$。设 $X$ 的所有行均为相同的 $x = [2, -1, 0.5, 3]$，因此 $X = \\begin{bmatrix}2  -1  0.5  3 \\\\ 2  -1  0.5  3 \\\\ 2  -1  0.5  3\\end{bmatrix}$。设\n$\nW_Q^{(1)} = \\begin{bmatrix}1  0 \\\\ 0  1 \\\\ 1  -1 \\\\ 0  2\\end{bmatrix},\\;\nW_K^{(1)} = \\begin{bmatrix}1  1 \\\\ 0  1 \\\\ 1  0 \\\\ 0  1\\end{bmatrix},\n$\n$\nW_Q^{(2)} = \\begin{bmatrix}0  1 \\\\ 1  0 \\\\ -1  1 \\\\ 2  0\\end{bmatrix},\\;\nW_K^{(2)} = \\begin{bmatrix}1  0 \\\\ 1  -1 \\\\ 0  2 \\\\ 1  0\\end{bmatrix},\n$\n$\nW_Q^{(3)} = \\begin{bmatrix}1  -1 \\\\ 0  2 \\\\ 1  0 \\\\ -1  1\\end{bmatrix},\\;\nW_K^{(3)} = \\begin{bmatrix}2  0 \\\\ 0  1 \\\\ 1  -1 \\\\ 0  1\\end{bmatrix}.\n$\n- 案例 2（多样化的头和词元）：$L = 3$，$d_{\\text{model}} = 4$，$d_{\\text{k}} = 2$，$H = 3$，$\\lambda = 0.5$。设\n$\nX = \\begin{bmatrix}\n1  0  1  0 \\\\\n0  1  0  1 \\\\\n1  1  0  0\n\\end{bmatrix}.\n$\n设\n$\nW_Q^{(1)} = \\begin{bmatrix}1  0 \\\\ 0  1 \\\\ 0  1 \\\\ 1  0\\end{bmatrix},\\;\nW_K^{(1)} = \\begin{bmatrix}1  0 \\\\ 1  0 \\\\ 0  1 \\\\ 0  1\\end{bmatrix},\n$\n$\nW_Q^{(2)} = \\begin{bmatrix}2  0 \\\\ 0  -1 \\\\ 1  1 \\\\ 0  1\\end{bmatrix},\\;\nW_K^{(2)} = \\begin{bmatrix}-1  0 \\\\ 0  2 \\\\ 1  0 \\\\ 0  1\\end{bmatrix},\n$\n$\nW_Q^{(3)} = \\begin{bmatrix}0  1 \\\\ 1  0 \\\\ -1  1 \\\\ 1  -1\\end{bmatrix},\\;\nW_K^{(3)} = \\begin{bmatrix}1  1 \\\\ 0  -1 \\\\ 2  0 \\\\ 1  0\\end{bmatrix}.\n$\n- 案例 3（边缘案例：零输入）：$L = 3$，$d_{\\text{model}} = 4$，$d_{\\text{k}} = 2$，$H = 3$，$\\lambda = 0.5$。设 $X$ 为 $L \\times d_{\\text{model}}$ 的零矩阵，$\\{W_Q^{(h)}\\}$ 和 $\\{W_K^{(h)}\\}$ 与案例 2 相同。\n\n对于每个案例，计算：\n- $\\{a^{(h)}\\}_{h=1}^{H}$ 的平均成对余弦相似度，以实数形式表示。\n- 一个坍缩指示符 $\\in \\{\\text{True}, \\text{False}\\}$，如果平均成对余弦相似度至少为 $0.99$，则为 $\\text{True}$，否则为 $\\text{False}$。\n- 梯度 $\\{\\|\\nabla_{a^{(h)}} \\mathcal{R}\\|_2\\}_{h=1}^{H}$ 的欧几里得范数列表。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个测试案例的结果，格式为逗号分隔的 Python 风格列表，每个元素都是 $[\\text{average\\_similarity}, \\text{collapse\\_boolean}, \\text{gradient\\_norms\\_list}]$ 形式的列表。\n- 所有实数必须打印为保留 $6$ 位小数的小数。\n- 仅为形状示例（请勿使用这些占位符名称）：$[[\\text{r1}, \\text{b1}, \\text{list1}], [\\text{r2}, \\text{b2}, \\text{list2}], [\\text{r3}, \\text{b3}, \\text{list3}]]$.\n\n您在任务 A 和 B 中的推导与论证必须仅基于上述基本定义、$\\operatorname{Softmax}$ 的性质、内积、欧几里得范数和链式法则，不得引入任何快捷公式或外部假设。",
            "solution": "此问题被评估为有效，因为它在科学上基于深度学习的原理（特别是 Transformer 架构），并且在数学上是适定的、自洽的且可形式化的。所有提供的参数和定义都是一致的，并且足以推导出解决方案。\n\n### 任务 A：导致头坍缩的输入模式\n\n多头注意力中的头坍缩，发生在由不同头 $h \\in \\{1,\\dots,H\\}$ 产生的注意力图 $A^{(h)} \\in \\mathbb{R}^{L \\times L}$ 变得相同或高度相似时。头 $h$ 的注意力图定义为 $A^{(h)} = \\operatorname{Softmax}_{\\text{row}}(S^{(h)})$，其中分数矩阵为 $S^{(h)} = \\frac{1}{\\sqrt{d_k}} Q^{(h)} (K^{(h)})^{\\top}$，而 $Q^{(h)}=XW_Q^{(h)}$ 且 $K^{(h)}=XW_K^{(h)}$。如果对于每一行 $i$，分数向量 $S_{i,:}^{(h)}$ 的结构使得 softmax 函数的输出与头 $h$ 无关，那么头坍缩就必定会发生。基于 softmax 函数和矩阵运算的基本性质，我们可以构造至少两种不同的输入模式 $X$ 来引发此行为。\n\n1.  **相同输入词元的模式**：考虑一个输入矩阵 $X \\in \\mathbb{R}^{L \\times d_{\\text{model}}}$，其中所有行向量都相同。设对所有 $i \\in \\{1, \\dots, L\\}$，有 $X_{i,:} = x^{\\top}$，其中 $x \\in \\mathbb{R}^{d_{\\text{model}}}$ 是一个常数向量。\n    -   对于任何头 $h$，查询矩阵为 $Q^{(h)} = X W_Q^{(h)}$。$Q^{(h)}$ 的每一行都是 $x^{\\top}$ 与 $W_Q^{(h)}$ 的乘积，从而得到一个所有行都与向量 $q^{(h)\\top} = x^{\\top} W_Q^{(h)}$ 相同的矩阵。\n    -   类似地，键矩阵 $K^{(h)} = X W_K^{(h)}$ 的所有行都与向量 $k^{(h)\\top} = x^{\\top} W_K^{(h)}$ 相同。\n    -   分数矩阵元素 $S_{ij}^{(h)}$ 是 $Q^{(h)}$ 的第 $i$ 行与 $K^{(h)}$ 的第 $j$ 行的缩放点积。由于在各自的矩阵中所有行都相同，因此 $S_{ij}^{(h)} = \\frac{1}{\\sqrt{d_k}} q^{(h)\\top} k^{(h)}$。\n    -   这个值是一个标量，我们称之为 $c^{(h)}$，它依赖于头 $h$，但对于所有词元位置 $(i, j)$ 都是恒定的。因此，分数矩阵 $S^{(h)}$ 是一个每个条目都为 $c^{(h)}$ 的常数矩阵。\n    -   注意力图 $A^{(h)}$ 是通过对 $S^{(h)}$ 的每一行应用 softmax 函数来计算的。对于任何行 $i$，softmax 的输入是一个包含 $L$ 个相同值的向量，即 $[c^{(h)}, c^{(h)}, \\dots, c^{(h)}]$。\n    -   根据定义 $\\operatorname{softmax}(z)_j = \\frac{\\exp(z_j)}{\\sum_{m=1}^{L} \\exp(z_m)}$，每个元素的输出为 $\\frac{\\exp(c^{(h)})}{L \\cdot \\exp(c^{(h)})} = \\frac{1}{L}$。\n    -   因此，对于任何头 $h$，其注意力图是一个所有条目都等于 $1/L$ 的矩阵。这个结果矩阵与特定于头的权重 $W_Q^{(h)}$ 和 $W_K^{(h)}$ 无关，导致所有头都产生相同的均匀注意力图，从而引致完全的头坍缩。\n\n2.  **零输入词元的模式**：设输入矩阵为零矩阵，$X = \\mathbf{0}_{L \\times d_{\\text{model}}}$。\n    -   这是第一种模式的一个特例，其中相同的词元向量是零向量，$x = \\mathbf{0}$。\n    -   因此，对于任何头 $h$，查询和键矩阵都是零矩阵：$Q^{(h)} = \\mathbf{0} \\cdot W_Q^{(h)} = \\mathbf{0}_{L \\times d_k}$ 且 $K^{(h)} = \\mathbf{0} \\cdot W_K^{(h)} = \\mathbf{0}_{L \\times d_k}$。\n    -   每个头的分数矩阵也是一个零矩阵：$S^{(h)} = \\frac{1}{\\sqrt{d_k}} \\mathbf{0} \\cdot \\mathbf{0}^{\\top} = \\mathbf{0}_{L \\times L}$。\n    -   这与前述分析中常数 $c^{(h)}=0$ 的情况相对应。对一个零向量逐行应用 softmax 会产生均匀分布，因为 $\\frac{\\exp(0)}{\\sum_{m=1}^{L} \\exp(0)} = \\frac{1}{L}$。\n    -   同样，对于每个头 $h$，其注意力图 $A^{(h)}$ 是一个所有条目均为 $1/L$ 的矩阵。这导致所有头的注意力都相同，从而产生坍缩。此模式是输入空间中的一个独特的奇点，而第一种模式则代表了导致坍缩的一个更普遍的输入子空间。\n\n### 任务 B：正则化器梯度的推导\n\n目标是推导正则化器 $\\mathcal{R}$ 相对于特定头 $p$ 的展平注意力向量 $a^{(p)}$ 的梯度 $\\nabla_{a^{(p)}} \\mathcal{R}$。正则化器为：\n$$\n\\mathcal{R}(\\{a^{(j)}\\}_{j=1}^H) = \\lambda \\cdot \\frac{1}{\\binom{H}{2}} \\sum_{1 \\le h  k \\le H} \\operatorname{cos}\\!\\left(a^{(h)}, a^{(k)}\\right)\n$$\n令 $C = \\lambda / \\binom{H}{2}$。相对于 $a^{(p)}$ 的梯度是通过对求和中的每一项进行微分得到的。一个项 $\\operatorname{cos}(a^{(h)}, a^{(k)})$ 仅在 $h=p$ 或 $k=p$ 时才对梯度有贡献。我们可以重写求和式以分离出包含 $a^{(p)}$ 的项：\n$$\n\\nabla_{a^{(p)}} \\mathcal{R} = C \\sum_{k \\neq p} \\nabla_{a^{(p)}} \\operatorname{cos}\\!\\left(a^{(p)}, a^{(k)}\\right)\n$$\n推导的核心是求取余弦相似度函数 $\\operatorname{cos}(u, v) = \\frac{\\langle u, v \\rangle}{\\|u\\|_2 \\|v\\|_2}$ 相对于其向量参数之一（例如 $u$）的梯度。从第一性原理出发，$\\langle u, v \\rangle = u^\\top v$ 且 $\\|u\\|_2 = (u^\\top u)^{1/2}$。令 $f(u) = \\operatorname{cos}(u, v)$。\n$$\nf(u) = \\frac{u^\\top v}{(u^\\top u)^{1/2} \\|v\\|_2}\n$$\n我们使用向量微分的商法则。令 $N(u) = u^\\top v$ 且 $D(u) = (u^\\top u)^{1/2} \\|v\\|_2$。梯度为 $\\nabla_u f = \\frac{(\\nabla_u N)D - N(\\nabla_u D)}{D^2}$。\n1.  分子的梯度为 $\\nabla_u N(u) = \\nabla_u(u^\\top v) = v$。\n2.  分母的梯度需要使用链式法则。$\\|u\\|_2 = (u^\\top u)^{1/2}$ 相对于 $u$ 的梯度是 $\\nabla_u \\|u\\|_2 = \\frac{u}{\\|u\\|_2}$。因此，$\\nabla_u D(u) = \\nabla_u (\\|u\\|_2 \\|v\\|_2) = \\frac{u}{\\|u\\|_2} \\|v\\|_2$。\n\n将这些代入商法则表达式中：\n$$\n\\nabla_u f = \\frac{v \\cdot (\\|u\\|_2 \\|v\\|_2) - (u^\\top v) \\cdot \\left(\\frac{u}{\\|u\\|_2} \\|v\\|_2\\right)}{\\left(\\|u\\|_2 \\|v\\|_2\\right)^2}\n$$\n从分子中提出 $\\|v\\|_2$ 并简化：\n$$\n\\nabla_u f = \\frac{\\|v\\|_2 \\left(v \\|u\\|_2 - (u^\\top v) \\frac{u}{\\|u\\|_2}\\right)}{\\|u\\|_2^2 \\|v\\|_2^2} = \\frac{v \\|u\\|_2 - \\frac{u^\\top v}{\\|u\\|_2} u}{\\|u\\|_2^2 \\|v\\|_2}\n$$\n分子和分母同乘以 $\\|u\\|_2$ 以消去分子中的分数：\n$$\n\\nabla_u f = \\frac{v \\|u\\|_2^2 - (u^\\top v) u}{\\|u\\|_2^3 \\|v\\|_2}\n$$\n此表达式可以通过分离各项来重写：\n$$\n\\nabla_u \\operatorname{cos}(u,v) = \\frac{v \\|u\\|_2^2}{\\|u\\|_2^3 \\|v\\|_2} - \\frac{u(u^\\top v)}{\\|u\\|_2^3 \\|v\\|_2} = \\frac{v}{\\|u\\|_2 \\|v\\|_2} - \\frac{u (u^\\top v)}{\\|u\\|_2^2 (\\|u\\|_2 \\|v\\|_2)}\n$$\n根据定义，$\\operatorname{cos}(u, v) = \\frac{u^\\top v}{\\|u\\|_2 \\|v\\|_2}$，所以将其代回可得：\n$$\n\\nabla_u \\operatorname{cos}(u,v) = \\frac{v}{\\|u\\|_2 \\|v\\|_2} - \\frac{u \\operatorname{cos}(u, v)}{\\|u\\|_2^2}\n$$\n将此结果应用于我们的正则化器梯度，令 $u=a^{(p)}$ 和 $v=a^{(k)}$：\n$$\n\\nabla_{a^{(p)}} \\mathcal{R} = C \\sum_{k \\neq p} \\left( \\frac{a^{(k)}}{\\|a^{(p)}\\|_2 \\|a^{(k)}\\|_2} - \\frac{a^{(p)} \\operatorname{cos}(a^{(p)}, a^{(k)})}{\\|a^{(p)}\\|_2^2} \\right)\n$$\n我们可以提出不依赖于求和索引 $k$ 的项：\n$$\n\\nabla_{a^{(p)}} \\mathcal{R} = C \\left( \\frac{1}{\\|a^{(p)}\\|_2} \\sum_{k \\neq p} \\frac{a^{(k)}}{\\|a^{(k)}\\|_2} - \\frac{a^{(p)}}{\\|a^{(p)}\\|_2^2} \\sum_{k \\neq p} \\operatorname{cos}(a^{(p)}, a^{(k)}) \\right)\n$$\n这就是根据要求从基本定义推导出的梯度的最终解析表达式。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by running three test cases through the attention head\n    diversity analysis pipeline.\n    \"\"\"\n\n    def softmax_row(z):\n        \"\"\"Numerically stable softmax for a 1D array.\"\"\"\n        e_z = np.exp(z - np.max(z))\n        return e_z / e_z.sum()\n\n    def scaled_dot_product_attention(X, W_Q, W_K):\n        \"\"\"Computes the attention map for a single head.\"\"\"\n        d_k = W_Q.shape[1]\n        Q = X @ W_Q\n        K = X @ W_K\n        S = (Q @ K.T) / np.sqrt(d_k)\n        A = np.apply_along_axis(softmax_row, 1, S)\n        return A\n\n    def analyze_case(params):\n        \"\"\"\n        Performs the full analysis for a single test case.\n        Computes attention maps, regularizer value, collapse indicator,\n        and gradient norms.\n        \"\"\"\n        L, d_model, d_k, H, lambda_val, X, W_Q_list, W_K_list = params\n\n        # Compute attention maps and flatten them\n        attention_maps = [scaled_dot_product_attention(X, W_Q_list[h], W_K_list[h]) for h in range(H)]\n        flattened_attentions = [A.flatten() for A in attention_maps]\n\n        # Compute average pairwise cosine similarity\n        pairwise_similarities = []\n        if H > 1:\n            num_pairs = H * (H - 1) / 2\n            for h in range(H):\n                for k in range(h + 1, H):\n                    a_h = flattened_attentions[h]\n                    a_k = flattened_attentions[k]\n                    norm_h = np.linalg.norm(a_h)\n                    norm_k = np.linalg.norm(a_k)\n                    if norm_h > 0 and norm_k > 0:\n                        sim = (a_h @ a_k) / (norm_h * norm_k)\n                        pairwise_similarities.append(sim)\n            \n            avg_similarity = sum(pairwise_similarities) / num_pairs if num_pairs > 0 else 0.0\n        else:\n            avg_similarity = 0.0\n\n        # Check for collapse\n        collapse_indicator = avg_similarity >= 0.99\n\n        # Compute gradients\n        grad_norms = []\n        if H > 1:\n            C = lambda_val / (H * (H - 1) / 2)\n            norms = [np.linalg.norm(a) for a in flattened_attentions]\n\n            for p in range(H):\n                a_p = flattened_attentions[p]\n                norm_p = norms[p]\n\n                if norm_p == 0:\n                    grad_p = np.zeros_like(a_p)\n                else:\n                    sum_term1 = np.zeros_like(a_p)\n                    sum_term2 = 0.0\n                    for k in range(H):\n                        if k == p:\n                            continue\n                        a_k = flattened_attentions[k]\n                        norm_k = norms[k]\n                        \n                        if norm_k > 0:\n                            sum_term1 += a_k / norm_k\n                            sum_term2 += (a_p @ a_k) / (norm_p * norm_k)\n                    \n                    term1 = (1 / norm_p) * sum_term1\n                    term2 = (a_p / (norm_p**2)) * sum_term2\n                    grad_p = C * (term1 - term2)\n                \n                grad_norms.append(np.linalg.norm(grad_p))\n        \n        return [avg_similarity, collapse_indicator, grad_norms]\n\n    # --- Test Case Definitions ---\n    # Case 1: Collapse via identical tokens\n    L1, d_model1, d_k1, H1, lambda1 = 3, 4, 2, 3, 0.5\n    X1 = np.array([[2, -1, 0.5, 3]] * 3, dtype=float)\n    W_Q1_list = [\n        np.array([[1, 0], [0, 1], [1, -1], [0, 2]], dtype=float),\n        np.array([[0, 1], [1, 0], [-1, 1], [2, 0]], dtype=float),\n        np.array([[1, -1], [0, 2], [1, 0], [-1, 1]], dtype=float)\n    ]\n    W_K1_list = [\n        np.array([[1, 1], [0, 1], [1, 0], [0, 1]], dtype=float),\n        np.array([[1, 0], [1, -1], [0, 2], [1, 0]], dtype=float),\n        np.array([[2, 0], [0, 1], [1, -1], [0, 1]], dtype=float)\n    ]\n    case1 = (L1, d_model1, d_k1, H1, lambda1, X1, W_Q1_list, W_K1_list)\n\n    # Case 2: Diverse heads and tokens\n    L2, d_model2, d_k2, H2, lambda2 = 3, 4, 2, 3, 0.5\n    X2 = np.array([\n        [1, 0, 1, 0],\n        [0, 1, 0, 1],\n        [1, 1, 0, 0]\n    ], dtype=float)\n    W_Q2_list = [\n        np.array([[1, 0], [0, 1], [0, 1], [1, 0]], dtype=float),\n        np.array([[2, 0], [0, -1], [1, 1], [0, 1]], dtype=float),\n        np.array([[0, 1], [1, 0], [-1, 1], [1, -1]], dtype=float)\n    ]\n    W_K2_list = [\n        np.array([[1, 0], [1, 0], [0, 1], [0, 1]], dtype=float),\n        np.array([[-1, 0], [0, 2], [1, 0], [0, 1]], dtype=float),\n        np.array([[1, 1], [0, -1], [2, 0], [1, 0]], dtype=float)\n    ]\n    case2 = (L2, d_model2, d_k2, H2, lambda2, X2, W_Q2_list, W_K2_list)\n\n    # Case 3: Edge case: zero input\n    L3, d_model3, d_k3, H3, lambda3 = 3, 4, 2, 3, 0.5\n    X3 = np.zeros((3, 4), dtype=float)\n    case3 = (L3, d_model3, d_k3, H3, lambda3, X3, W_Q2_list, W_K2_list)\n    \n    test_cases = [case1, case2, case3]\n    \n    # --- Run Analysis and Format Output ---\n    results = []\n    output_str = []\n    for case in test_cases:\n        avg_sim, collapse, grad_norms_list = analyze_case(case)\n        \n        # Format for printing\n        r_str = f\"{avg_sim:.6f}\"\n        b_str = str(collapse)\n        \n        # Format the list of gradient norms\n        if not grad_norms_list:\n             list_str = \"[]\"\n        else:\n             list_str = f\"[{','.join([f'{g:.6f}' for g in grad_norms_list])}]\"\n\n        output_str.append(f\"[{r_str},{b_str},{list_str}]\")\n        \n    print(f\"[{','.join(output_str)}]\")\n\nsolve()\n```"
        }
    ]
}