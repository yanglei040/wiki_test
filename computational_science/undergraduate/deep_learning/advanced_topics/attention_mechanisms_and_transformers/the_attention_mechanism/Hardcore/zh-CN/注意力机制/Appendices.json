{
    "hands_on_practices": [
        {
            "introduction": "注意力机制的核心是 Softmax 函数，它能将原始的相似度得分转化为一组权重。这个练习将带你探索一个关键的超参数——“温度”($\\tau$)——如何控制这些权重的“尖锐度”。通过这个练习，你将从第一性原理出发，证明当温度趋近于零时，Softmax 函数的行为会逼近一个“赢者通吃”的最大值选择器（argmax），这揭示了注意力机制是如何“聚焦”于最相关信息的。",
            "id": "3100390",
            "problem": "考虑一个在 Transformer 模型中使用的、基于缩放点积注意力 (Scaled Dot-Product Attention, SDPA) 的注意力机制。设有一个查询向量 $q \\in \\mathbb{R}^d$，一组键向量 $\\{k_i\\}_{i=1}^n \\subset \\mathbb{R}^d$，以及一组对应的值向量 $\\{v_i\\}_{i=1}^n \\subset \\mathbb{R}^m$。将键 $k_i$ 的得分定义为 $s_i = q^\\top k_i$。对于温度参数 $\\tau > 0$，通过对得分的指数进行归一化来定义 softmax 权重 $w_i(\\tau)$，并将注意力输出 $a(\\tau)$ 定义为使用这些权重对值向量进行的加权和。通过选择使得分最大化的索引 $i^\\star$ 所对应的值来定义 argmax 注意力输出 $a^\\star$。仅从指数函数、极限和按和归一化的基本定义出发，推导当 $\\tau \\to 0$ 时 $a(\\tau)$ 与 $a^\\star$ 之间的极限关系，并分析 argmax 选择在 $q$ 的小扰动下的脆弱性。\n\n你的任务是编写一个完整的、可运行的程序，对指定的测试套件执行以下计算，并按下面描述的精确格式生成单行输出。不涉及物理单位、角度单位或百分比；所有输出必须是数值浮点数、整数、布尔值或它们的列表。\n\n使用的定义：\n- Softmax 权重：$w_i(\\tau) = \\dfrac{\\exp\\!\\left(s_i/\\tau\\right)}{\\sum_{j=1}^n \\exp\\!\\left(s_j/\\tau\\right)}$，其中 $s_i = q^\\top k_i$。\n- Softmax 注意力输出：$a(\\tau) = \\sum_{i=1}^n w_i(\\tau) v_i$。\n- Argmax 索引：$i^\\star = \\operatorname{argmax}_{1 \\leq i \\leq n} s_i$ (假设存在一个任意但固定的平局打破规则)。\n- Argmax 注意力输出：$a^\\star = v_{i^\\star}$。\n\n测试套件规范：\n\n- 测试用例 $1$ (唯一最大值，跨温度的近似质量)：\n  - 维度 $d = 2$，值维度 $m = 2$。\n  - 查询 $q = (1.0, 0.2)$。\n  - 键 $k_1 = (1.0, 0.0)$, $k_2 = (0.5, 0.7)$, $k_3 = (-0.5, 0.3)$, $k_4 = (0.1, 0.2)$。\n  - 值 $v_1 = (1.0, 0.0)$, $v_2 = (0.0, 1.0)$, $v_3 = (1.0, 1.0)$, $v_4 = (-1.0, 1.0)$。\n  - 温度 $\\tau \\in \\{1.0, 0.5, 0.1, 0.01, 10^{-6}\\}$。\n  - 对于每个 $\\tau$，计算欧几里得范数 $\\|a(\\tau) - a^\\star\\|_2$ 并将这些值按 $\\tau$ 的顺序列为一个浮点数列表返回。\n\n- 测试用例 $2$ (接近平局时在小的查询扰动下的脆弱性以及温度的平滑作用)：\n  - 维度 $d = 2$，值维度 $m = 2$。\n  - 基础查询 $q_0 = (1.0, 0.0)$，扰动查询 $q_+ = (1.0, \\epsilon)$ 和 $q_- = (1.0, -\\epsilon)$，其中 $\\epsilon = 10^{-3}$。\n  - 键 $k_1 = (1.0, 0.0)$, $k_2 = (1.0, \\eta)$，其中 $\\eta = 1000.0$。\n  - 值 $v_1 = (1.0, 0.0)$, $v_2 = (0.0, 1.0)$。\n  - 用于比较的温度 $\\tau \\in \\{0.5, 0.1, 0.01\\}$。\n  - 计算：\n    - 扰动下 argmax 翻转的整数指示器，定义为如果 $\\operatorname{argmax}_i q_+^\\top k_i \\neq \\operatorname{argmax}_i q_-^\\top k_i$ 则 $I = 1$，否则 $I = 0$。\n    - 对于每个指定的 $\\tau$，计算 $q_+$ 和 $q_-$ 的两个 softmax 注意力输出之间的欧几里得距离 $\\|a_{+}(\\tau) - a_{-}(\\tau)\\|_2$。\n  - 返回一个列表，其中包含整数 $I$，后跟按所列 $\\tau$ 值排序的三个距离。\n\n- 测试用例 $3$ (精确平局，softmax 权重的对称性)：\n  - 维度 $d = 2$，值维度 $m = 2$。\n  - 查询 $q = (1.0, 0.0)$。\n  - 键 $k_1 = (1.0, 0.0)$, $k_2 = (1.0, 0.0)$, $k_3 = (0.0, 1.0)$ (因此 $k_1$ 和 $k_2$ 精确平局)。\n  - 值 $v_1 = (1.0, 0.0)$, $v_2 = (0.0, 1.0)$, $v_3 = (1.0, 1.0)$。\n  - 温度 $\\tau = 0.1$。\n  - 计算 softmax 权重 $w_1(\\tau)$ 和 $w_2(\\tau)$，并返回浮点数 $|w_1(\\tau) - w_2(\\tau)|$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果按所述顺序出现：\n  - 第一个元素是来自测试用例 $1$ 的浮点数列表。\n  - 第二个元素是来自测试用例 $2$ 的列表，包含一个整数和三个浮点数。\n  - 第三个元素是来自测试用例 $3$ 的单个浮点数。\n- 例如，最后一行应如下所示：$[[x_1,x_2,x_3,x_4,x_5],[i,d_1,d_2,d_3],y]$，其中每个 $x_j$、$i$、$d_j$ 和 $y$ 是为指定测试套件计算出的数值结果。",
            "solution": "该问题定义明确、具有科学依据且内部一致。它为获得唯一且有意义的解提供了所有必要的定义和数据。我们继续进行推导和计算实现。\n\n分析分为两部分。首先，我们推导当温度参数 $\\tau$ 趋近于零时，softmax 注意力输出 $a(\\tau)$ 的极限行为。其次，我们分析注意力机制在小扰动下的稳定性，将 argmax 选择与温和的 softmax 进行对比。\n\n**1. Softmax 注意力在 $\\tau \\to 0^+$ 时的极限行为**\n\nsoftmax 注意力输出被定义为值向量的加权和：\n$$ a(\\tau) = \\sum_{i=1}^n w_i(\\tau) v_i $$\n其中权重 $w_i(\\tau)$ 由应用于得分 $s_i = q^\\top k_i$ 的温度缩放 softmax 函数给出：\n$$ w_i(\\tau) = \\frac{\\exp(s_i/\\tau)}{\\sum_{j=1}^n \\exp(s_j/\\tau)} $$\n我们希望计算极限 $\\lim_{\\tau \\to 0^+} a(\\tau)$。分析的关键在于权重 $w_i(\\tau)$ 在此极限下的行为。\n\n令 $s_{\\max} = \\max_{1 \\leq j \\leq n} s_j$ 为最大得分。为避免数值溢出并方便极限计算，我们可以从分子和分母中提出公因子项 $\\exp(s_{\\max}/\\tau)$：\n$$ w_i(\\tau) = \\frac{\\exp(s_{\\max}/\\tau) \\exp((s_i - s_{\\max})/\\tau)}{\\exp(s_{\\max}/\\tau) \\sum_{j=1}^n \\exp((s_j - s_{\\max})/\\tau)} = \\frac{\\exp((s_i - s_{\\max})/\\tau)}{\\sum_{j=1}^n \\exp((s_j - s_{\\max})/\\tau)} $$\n现在，我们研究当 $\\tau \\to 0^+$ 时指数项 $\\exp((s_j - s_{\\max})/\\tau)$ 的极限。其行为取决于 $s_j$ 是否等于 $s_{\\max}$。\n\n情况 1：$s_j = s_{\\max}$。在这种情况下，指数为 $(s_j - s_{\\max})/\\tau = 0/\\tau = 0$。因此，$\\exp((s_j - s_{\\max})/\\tau) = \\exp(0) = 1$。\n\n情况 2：$s_j  s_{\\max}$。这里，差值 $s_j - s_{\\max}$ 是一个严格的负常数。当 $\\tau \\to 0^+$ 时，指数 $(s_j - s_{\\max})/\\tau \\to -\\infty$。因此，$\\lim_{\\tau \\to 0^+} \\exp((s_j - s_{\\max})/\\tau) = 0$。\n\n令 $I_{\\max} = \\{i \\mid s_i = s_{\\max}\\}$ 为对应最大得分的索引集合，并令 $M = |I_{\\max}|$ 为此类索引的数量。现在我们可以计算 $w_i(\\tau)$ 分母的极限：\n$$ \\lim_{\\tau \\to 0^+} \\sum_{j=1}^n \\exp((s_j - s_{\\max})/\\tau) = \\sum_{j \\in I_{\\max}} \\lim_{\\tau \\to 0^+} \\exp((s_j - s_{\\max})/\\tau) + \\sum_{j \\notin I_{\\max}} \\lim_{\\tau \\to 0^+} \\exp((s_j - s_{\\max})/\\tau) $$\n$$ = \\sum_{j \\in I_{\\max}} 1 + \\sum_{j \\notin I_{\\max}} 0 = M $$\n分子 $\\exp((s_i - s_{\\max})/\\tau)$ 的极限在 $i \\in I_{\\max}$ 时为 $1$，在 $i \\notin I_{\\max}$ 时为 $0$。\n\n综合这些结果，权重 $w_i(\\tau)$ 的极限为：\n$$ \\lim_{\\tau \\to 0^+} w_i(\\tau) = \\begin{cases} 1/M  \\text{if } i \\in I_{\\max} \\\\ 0  \\text{if } i \\notin I_{\\max} \\end{cases} $$\n这表明，当 $\\tau \\to 0^+$ 时，softmax 函数将其所有概率质量集中在达到最大得分的索引上，并在它们之间均匀分布。它实际上变成了一个“软 argmax”。\n\n现在，我们可以求出注意力输出 $a(\\tau)$ 的极限：\n$$ \\lim_{\\tau \\to 0^+} a(\\tau) = \\lim_{\\tau \\to 0^+} \\sum_{i=1}^n w_i(\\tau) v_i = \\sum_{i=1}^n \\left(\\lim_{\\tau \\to 0^+} w_i(\\tau)\\right) v_i $$\n代入极限权重：\n$$ \\lim_{\\tau \\to 0^+} a(\\tau) = \\sum_{i \\in I_{\\max}} \\frac{1}{M} v_i + \\sum_{i \\notin I_{\\max}} 0 \\cdot v_i = \\frac{1}{M} \\sum_{i \\in I_{\\max}} v_i $$\n极限注意力输出是其对应键达到最大得分的值向量的算术平均值。\n\n问题将 argmax 注意力输出定义为 $a^\\star = v_{i^\\star}$，其中 $i^\\star = \\operatorname{argmax}_{i} s_i$，并采用固定的平局打破规则。如果最大得分是唯一的，则 $I_{\\max} = \\{i^\\star\\}$ 且 $M=1$。在这种常见情况下，极限简化为：\n$$ \\lim_{\\tau \\to 0^+} a(\\tau) = \\frac{1}{1} v_{i^\\star} = v_{i^\\star} = a^\\star $$\n因此，当最大得分唯一时，softmax 注意力输出收敛于 argmax 注意力输出。\n\n**2. Argmax 选择的脆弱性与温度的正则化作用**\n\nargmax 选择本质上是不连续的。$i^\\star$ 的选择取决于得分 $s_i(q) = q^\\top k_i$ 的排序。对查询的一个小扰动 $q' = q + \\delta q$ 会引起得分的变化：$s_i(q') = s_i(q) + (\\delta q)^\\top k_i$。如果两个得分（例如 $s_j(q)$ 和 $s_k(q)$）非常接近，即使是无穷小的扰动 $\\delta q$ 也可能改变它们的相对顺序，导致 $i^\\star$ 从一个索引跳到另一个索引。这会导致输出 $a^\\star$ 发生从 $v_j$ 到 $v_k$ 的离散变化，该变化可能很大。这种不稳定性是一种“脆弱性”。\n\n相反，对于任何严格为正的温度 $\\tau > 0$，softmax 注意力输出 $a(\\tau)$ 是查询 $q$ 的一个连续可微函数。权重 $w_i(\\tau)$ 是得分 $s_i$ 的平滑函数，而得分 $s_i$ 是 $q$ 的线性函数。因此，查询中的微小变化 $\\delta q$ 会导致输出 $a(\\tau)$ 发生相应的微小变化。该变化由梯度决定：$a(q+\\delta q, \\tau) \\approx a(q, \\tau) + (\\nabla_q a(q, \\tau)) \\cdot \\delta q$。\n\n较高的温度 $\\tau$ 具有平滑或正则化效应。它会“拉平”softmax 分布，这意味着权重 $w_i(\\tau)$ 对得分的微小差异不那么敏感。这使得输出 $a(\\tau)$ 对 $q$ 的微小扰动更具鲁棒性。相反，当 $\\tau \\to 0$ 时，softmax 函数变得更陡峭，$a(\\tau)$ 相对于 $q$ 的梯度会变得非常大，尤其是在影响接近最大值得分之间差异的方向上。在这种情况下，平滑的 softmax 输出 $a(\\tau)$ 紧密近似于 argmax 输出 $a^\\star$ 的脆弱、不连续行为，对能够翻转最高得分排序的小扰动表现出高度敏感性，如测试用例 $2$ 所示。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the three test cases specified in the problem statement regarding\n    Scaled Dot-Product Attention.\n    \"\"\"\n\n    def softmax_attention_with_weights(q, K, V, tau):\n        \"\"\"\n        Computes the softmax attention output and corresponding weights.\n\n        Args:\n            q (np.ndarray): Query vector of shape (d,).\n            K (np.ndarray): Key matrix of shape (n, d).\n            V (np.ndarray): Value matrix of shape (n, m).\n            tau (float): Temperature parameter.\n\n        Returns:\n            tuple: A tuple containing:\n                - np.ndarray: Attention output vector of shape (m,).\n                - np.ndarray: Softmax weights vector of shape (n,).\n        \"\"\"\n        scores = K @ q\n        # Scale scores by temperature as per the definition.\n        scaled_scores = scores / tau\n        # Use the max-subtraction trick for numerical stability of exp.\n        stable_scores = scaled_scores - np.max(scaled_scores)\n        exps = np.exp(stable_scores)\n        weights = exps / np.sum(exps)\n        # Attention output is the weighted sum of value vectors.\n        # V.T is (m, n), weights is (n,). Result is (m,).\n        attention_output = V.T @ weights\n        return attention_output, weights\n\n    # --- Test Case 1: Unique maximum, approximation quality ---\n    q1 = np.array([1.0, 0.2])\n    K1 = np.array([[1.0, 0.0], [0.5, 0.7], [-0.5, 0.3], [0.1, 0.2]])\n    V1 = np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0], [-1.0, 1.0]])\n    taus1 = [1.0, 0.5, 0.1, 0.01, 1e-6]\n    \n    scores1 = K1 @ q1\n    # np.argmax respects the \"fixed tie-breaking rule\" by taking the first occurrence.\n    i_star1 = np.argmax(scores1)\n    a_star1 = V1[i_star1]\n    \n    results1 = []\n    for tau in taus1:\n        a_tau, _ = softmax_attention_with_weights(q1, K1, V1, tau)\n        error = np.linalg.norm(a_tau - a_star1)\n        results1.append(error)\n\n    # --- Test Case 2: Near-tie brittleness ---\n    epsilon = 1e-3\n    q_plus = np.array([1.0, epsilon])\n    q_minus = np.array([1.0, -epsilon])\n    eta = 1000.0\n    K2 = np.array([[1.0, 0.0], [1.0, eta]])\n    V2 = np.array([[1.0, 0.0], [0.0, 1.0]])\n    taus2 = [0.5, 0.1, 0.01]\n    \n    scores_plus = K2 @ q_plus\n    scores_minus = K2 @ q_minus\n    argmax_plus = np.argmax(scores_plus)\n    argmax_minus = np.argmax(scores_minus)\n    I = 1 if argmax_plus != argmax_minus else 0\n    \n    results2 = [I]\n    for tau in taus2:\n        a_plus_tau, _ = softmax_attention_with_weights(q_plus, K2, V2, tau)\n        a_minus_tau, _ = softmax_attention_with_weights(q_minus, K2, V2, tau)\n        dist = np.linalg.norm(a_plus_tau - a_minus_tau)\n        results2.append(dist)\n        \n    # --- Test Case 3: Exact tie, symmetry of weights ---\n    q3 = np.array([1.0, 0.0])\n    K3 = np.array([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\n    # V3 is not strictly needed as we only compute weights.\n    # Pass a dummy V matrix of the correct shape.\n    V3 = np.zeros((3, 2))\n    tau3 = 0.1\n    \n    _, weights3 = softmax_attention_with_weights(q3, K3, V3, tau3)\n    w1_tau = weights3[0]\n    w2_tau = weights3[1]\n    result3 = abs(w1_tau - w2_tau)\n    \n    # --- Format and print the final output ---\n    # The output format is a single line string `[[...],[...],...]`.\n    str_res1 = f\"[{','.join(map(str, results1))}]\"\n    str_res2 = f\"[{','.join(map(str, results2))}]\"\n    str_res3 = str(result3)\n    \n    print(f\"[{str_res1},{str_res2},{str_res3}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在理解了注意力如何进行选择之后，我们来探讨一个特殊但至关重要的应用场景：自注意力。这个练习将研究自注意力在何种条件下会退化为一种复制机制，即简单地将输入复制到输出。通过推导一个保证这种“类恒等映射”行为的明确条件，你将把抽象的注意力理论与模型的具体行为联系起来，并理解为何这种复制能力对许多深度学习架构至关重要。",
            "id": "3180941",
            "problem": "考虑一个单头自注意力层，该层使用缩放点积机制，且不含位置编码和残差连接。设输入序列为 $X = (x_{1}, x_{2}, \\dots, x_{n})$，其中每个词元（token）的嵌入 $x_{i} \\in \\mathbb{R}^{d}$。查询（query）、键（key）和值（value）定义为 $Q = K = X$ 及 $V = X$。注意力 logit 是通过一个非负的逆温度参数 $s \\geq 0$ 对成对点积进行缩放而构建的，因此对于每一对 $\\{i,j\\}$，其 logit 为 $L_{ij} = s \\, \\langle x_{i}, x_{j} \\rangle$。逐行的注意力权重为 $A_{ij} = \\exp(L_{ij}) \\big/ \\sum_{k=1}^{n} \\exp(L_{ik})$。注意力层的输出为 $Y = A V$，其中 $A$ 是逐行归一化的权重矩阵。\n\n假设序列具有以下低复杂度相似性结构：存在实数 $m$ 和 $c$，使得对于所有 $i$ 都有 $\\langle x_{i}, x_{i} \\rangle = m$，且对于所有 $i \\neq j$ 都有 $\\langle x_{i}, x_{j} \\rangle = c$，其中 $m  c$。在这种设置下，一种类单位矩阵（identity-like）行为的特征是每个查询都将其几乎所有的权重都放在自己的键上，这可以通过对角线上的注意力权重满足 $A_{ii} \\geq 1 - \\tau$ 来量化，其中 $0  \\tau  1$ 是一个预设的容差。请根据上述定义，且不借助任何额外的架构捷径，推导出一个关于逆温度 $s$ 的显式闭式下界。该下界需保证在 $n \\geq 2$ 时，所有行的 $A_{ii} \\geq 1 - \\tau$ 都成立，并用 $n$、$m$、$c$ 和 $\\tau$ 表示。\n\n然后，将您的下界实例化到具体情况 $n = 6$、$m = 1$、$c = 0.1$ 和 $\\tau = 0.02$。将最终答案表示为单个闭式表达式，不要四舍五入。\n\n最后，请仅使用相同的基础定义，简要论证当数据处于最大低熵状态（即 $x_{1} = x_{2} = \\dots = x_{n}$）时，注意力权重会变成什么（此论证无需提供数值）。",
            "solution": "该问题要求推导逆温度缩放参数 $s$ 的一个下界，以保证特定水平的自注意力，并分析一个退化情况。对问题陈述的验证确认了其科学性、适定性和客观性。因此，我们可以进行形式化的求解。\n\n主要任务是找到 $s$ 的最小值，使得对于给定的容差 $\\tau \\in (0, 1)$，对角线上的注意力权重 $A_{ii}$ 至少为 $1 - \\tau$。注意力权重由 softmax 函数逐行应用于 logit 矩阵 $L_{ij}$ 而定义。对于任意行 $i$，对角线权重 $A_{ii}$ 由下式给出：\n$$\nA_{ii} = \\frac{\\exp(L_{ii})}{\\sum_{k=1}^{n} \\exp(L_{ik})}\n$$\nlogit 为 $L_{ij} = s \\langle x_{i}, x_{j} \\rangle$，其中 $s \\geq 0$。根据给定的相似性结构，我们有 $\\langle x_{i}, x_{i} \\rangle = m$ 和 $\\langle x_{i}, x_{j} \\rangle = c$（对于所有 $i \\neq j$）。这使我们可以将 logit 表示为 $L_{ii} = sm$ 和 $L_{ij} = sc$（对于 $i \\neq j$）。\n\n将这些代入 $A_{ii}$ 的表达式，我们发现分母可以分为 $k=i$ 的项和 $n-1$ 个 $k \\neq i$ 的项：\n$$\n\\sum_{k=1}^{n} \\exp(L_{ik}) = \\exp(L_{ii}) + \\sum_{k=1, k \\neq i}^{n} \\exp(L_{ik}) = \\exp(sm) + (n-1)\\exp(sc)\n$$\n$A_{ii}$ 的表达式因此与索引 $i$ 无关：\n$$\nA_{ii} = \\frac{\\exp(sm)}{\\exp(sm) + (n-1)\\exp(sc)}\n$$\n需要满足的条件是 $A_{ii} \\geq 1 - \\tau$。我们可以直接构建不等式：\n$$\n\\frac{\\exp(sm)}{\\exp(sm) + (n-1)\\exp(sc)} \\geq 1 - \\tau\n$$\n由于分母是正数项之和，且 $1-\\tau  0$，我们可以在不等式两边同乘以分母而不改变不等号的方向：\n$$\n\\exp(sm) \\geq (1 - \\tau) \\left( \\exp(sm) + (n-1)\\exp(sc) \\right)\n$$\n分配 $(1-\\tau)$ 项：\n$$\n\\exp(sm) \\geq (1-\\tau)\\exp(sm) + (n-1)(1-\\tau)\\exp(sc)\n$$\n合并包含 $\\exp(sm)$ 的项：\n$$\n\\exp(sm) - (1-\\tau)\\exp(sm) \\geq (n-1)(1-\\tau)\\exp(sc)\n$$\n$$\n\\tau \\exp(sm) \\geq (n-1)(1-\\tau)\\exp(sc)\n$$\n为了分离出对 $s$ 的依赖关系，我们可以两边同除以 $\\tau \\exp(sc)$。由于 $\\tau  0$ 且 $\\exp(sc)  0$，这是一个有效的操作，且能保持不等关系：\n$$\n\\frac{\\exp(sm)}{\\exp(sc)} \\geq \\frac{(n-1)(1-\\tau)}{\\tau}\n$$\n利用指数的性质 $\\exp(a)/\\exp(b) = \\exp(a-b)$：\n$$\n\\exp(s(m-c)) \\geq \\frac{(n-1)(1-\\tau)}{\\tau}\n$$\n右侧对数的参数是正的，因为 $n \\geq 2$ 意味着 $n-1  0$，且 $0  \\tau  1$ 意味着 $1-\\tau  0$。我们可以对两边取自然对数。由于 $\\ln(x)$ 是一个单调递增函数，不等关系得以保持：\n$$\n\\ln\\left(\\exp(s(m-c))\\right) \\geq \\ln\\left(\\frac{(n-1)(1-\\tau)}{\\tau}\\right)\n$$\n$$\ns(m-c) \\geq \\ln\\left(\\frac{(n-1)(1-\\tau)}{\\tau}\\right)\n$$\n问题陈述中提到 $m  c$，这意味着 $m - c  0$。因此，我们可以两边同除以 $(m-c)$ 来求解 $s$：\n$$\ns \\geq \\frac{1}{m-c} \\ln\\left(\\frac{(n-1)(1-\\tau)}{\\tau}\\right)\n$$\n这个不等式给出了保证所需类单位矩阵行为的 $s$ 的显式闭式下界。\n\n接下来，我们将此下界实例化到具体情况，其中 $n = 6$、$m = 1$、$c = 0.1$ 和 $\\tau = 0.02$。\n表达式的各组成部分为：\n- $m-c = 1 - 0.1 = 0.9 = \\frac{9}{10}$\n- $n-1 = 6-1 = 5$\n- $1-\\tau = 1 - 0.02 = 0.98 = \\frac{98}{100}$\n- $\\tau = 0.02 = \\frac{2}{100}$\n\n将这些值代入对数的参数中：\n$$\n\\frac{(n-1)(1-\\tau)}{\\tau} = \\frac{5 \\times 0.98}{0.02} = \\frac{5 \\times 98}{2} = 5 \\times 49 = 245\n$$\n因此，$s$ 的下界为：\n$$\ns \\geq \\frac{1}{0.9} \\ln(245) = \\frac{10}{9} \\ln(245)\n$$\n这就是实例化下界所要求的闭式表达式。\n\n最后，题目要求我们论证当数据处于最大低熵状态（即 $x_{1} = x_{2} = \\dots = x_{n}$）时，注意力权重会变成什么。\n在这种情况下，所有输入向量都相同。设对于所有 $i \\in \\{1, \\dots, n\\}$ 都有 $x_i = x_0$。成对点积变得一致：\n$$\n\\langle x_{i}, x_{j} \\rangle = \\langle x_{0}, x_{0} \\rangle \\quad \\forall i, j\n$$\n设这个恒定的点积值为 $M = \\langle x_{0}, x_{0} \\rangle$。那么所有的 logit 都相等：\n$$\nL_{ij} = s \\langle x_{i}, x_{j} \\rangle = sM \\quad \\forall i, j\n$$\n注意力权重 $A_{ij}$ 计算如下：\n$$\nA_{ij} = \\frac{\\exp(L_{ij})}{\\sum_{k=1}^{n} \\exp(L_{ik})} = \\frac{\\exp(sM)}{\\sum_{k=1}^{n} \\exp(sM)}\n$$\n分母是 $n$ 个相同项的和，因此 $\\sum_{k=1}^{n} \\exp(sM) = n \\exp(sM)$。\n将此代入 $A_{ij}$ 的表达式可得：\n$$\nA_{ij} = \\frac{\\exp(sM)}{n \\exp(sM)} = \\frac{1}{n}\n$$\n因此，当所有输入词元都相同时，注意力机制无法区分它们。最终的注意力矩阵 $A$ 的每个元素都等于 $1/n$，这表明每个查询到所有键的注意力是均匀分布的。这代表了一种最大不确定性的状态，与问题主要部分所寻求的类单位矩阵的注意力集中状态在逻辑上是截然相反的。",
            "answer": "$$\\boxed{\\frac{10}{9}\\ln(245)}$$"
        },
        {
            "introduction": "尽管注意力机制非常强大，但在处理长序列时仍会面临挑战。最后一个练习将通过一个简化的统计模型，来量化一个简单的复制任务的准确率是如何随着序列长度的增加而下降的。你将发现为什么单一的注意力机制是不够的，以及多头注意力（Multi-Head Attention）如何为这个问题提供了直接的解决方案，并最终引出制约现代 Transformer 架构性能的关键——规模化法则（scaling laws）。",
            "id": "3180987",
            "problem": "你的任务是利用注意力机制构建一个序列复制任务的数学基础模拟，并用它来量化准确率随序列长度增加而变化的规律。你必须从缩放点积注意力的核心定义和高斯随机变量的性质出发来推导你的程序，而不依赖于任何非由这些原则推导出的捷径公式。\n\n考虑一个简化的多头自注意力（MHSA）层，该层必须复制一个从独热（one-hot）字母表中抽取的长度为 $L$ 的输入序列。在位置 $i$，$q_i \\in \\mathbb{R}^{d_k}$ 是查询向量，$\\{k_j\\}_{j=1}^L$ 是键向量，缩放点积注意力分数为\n$$\ns_{ij} \\coloneqq \\frac{q_i^\\top k_j}{\\sqrt{d_k}}.\n$$\n假设以下理想化的统计模型，你必须用它来推导和实现你的解决方案：\n- 对于相同的位置 $i$，对 $(q_i, k_i)$ 是联合高斯分布，均值为零，分量单位方差，并且分量相关系数为 $\\rho \\in (0,1)$。对于不同的位置 $j \\neq i$，$(q_i, k_j)$ 是独立的，均值为零，单位方差。所有分量和对在维度和注意力头上都是独立同分布（i.i.d.）的。\n- 根据中心极限定理，对于较大的 $d_k$，每个分数 $s_{ij}$ 近似为高斯分布。在上述相关结构和标准的 $\\frac{1}{\\sqrt{d_k}}$ 缩放条件下，正确位置的分数 $s_{ii}$ 相对于不正确分数的均值优势随 $\\sqrt{d_k}$ 增长，而所有分数的方差均为单位方差。\n- 在具有 $H$ 个独立头，且在分数层面（softmax 之前）进行平均的多头自注意力（MHSA）中，每个位置上正确键和每个不正确键的平均分数保持为具有相同单位方差的高斯分布，而正确键的均值优势按 $\\sqrt{H}$ 比例缩放。\n\n对于位置 $i$ 的复制决策，将 softmax 选择近似为对 $L$ 个分数的硬性最大值选择，并将正确复制的概率定义为正确键的聚合分数超过不正确键聚合分数最大值的概率。\n\n你的任务：\n1) 仅从上述定义和经过充分检验的高斯事实出发，推导出一个可计算的一维积分表达式，用于表示在单个位置选择正确键的概率，该表达式是关于 $L$、$d_k$、$\\rho$ 和 $H$ 的函数。\n\n2) 实现一个程序，该程序：\n- 通过对你推导的积分进行数值计算，来计算在单个位置上正确复制的精确概率（在上述模型下）。\n- 计算最小整数头数 $H^\\star$，使得正确复制的概率至少达到目标阈值 $\\tau$。\n- 使用你的实现来研究当 $L$ 增加四倍时，$H^\\star$ 如何变化，同时保持 $d_k$ 和 $\\rho$ 不变。\n\n3) 通过报告计算概率的变化，分析当 $L$ 增加但 $H$ 保持不变时的失败模式。\n\n4) 使用你的概率模型，针对固定的 $d_k$、$\\rho$ 和目标 $\\tau$，提出一个形式为 $H \\approx C \\cdot (\\log L)^\\alpha$ 的缩放定律，并根据你的结果数值估算 $\\alpha$。\n\n测试套件和要求的输出：\n使用以下参数设置：\n- 复制准确率阈值 $\\tau = 0.9$。\n- 查询-键相关性 $\\rho = 0.25$。\n- 键维度 $d_k = 64$。\n- 序列长度 $L \\in \\{16, 64, 256\\}$。\n\n你的程序必须按此确切顺序计算并输出以下内容：\n- $L=16$ 时的最小头数 $H^\\star$，一个整数。\n- $L=64$ 时的最小头数 $H^\\star$，一个整数。\n- $L=256$ 时的最小头数 $H^\\star$，一个整数。\n- 当 $L=16$ 且 $H=1$ 时正确复制的概率，一个浮点数。\n- 当 $L=64$ 且 $H=1$ 时正确复制的概率，一个浮点数。\n- $L=16$ 时的比率 $\\frac{H^\\star}{\\log L}$，一个浮点数（自然对数）。\n- $L=64$ 时的比率 $\\frac{H^\\star}{\\log L}$，一个浮点数（自然对数）。\n- $L=256$ 时的比率 $\\frac{H^\\star}{\\log L}$，一个浮点数（自然对数）。\n- 指数 $\\alpha$ 在 $H \\approx C \\cdot (\\log L)^\\alpha$ 中的一个估计值，通过两点 $(L=16, H^\\star)$ 和 $(L=256, H^\\star)$，使用两点斜率公式计算\n$$\n\\alpha \\coloneqq \\frac{\\log H^\\star(256) - \\log H^\\star(16)}{\\log \\log 256 - \\log \\log 16}\n$$\n为一个浮点数。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，\"[result1,result2,...]\"）。所有概率和比率都应表示为标准十进制浮点数。不涉及物理单位，也不需要角度。计算中的所有对数必须是自然对数。确保数值积分足够精确，以使概率和比率的输出至少在小数点后4位保持稳定。\n\n你的实现必须是一个单一、完整、可运行的程序，没有用户输入。它不得访问外部文件或网络。唯一允许的库是 Python 标准库、NumPy 和 SciPy，具体如执行环境中所指定。",
            "solution": "该问题要求推导一个可计算的表达式，用以描述简化注意力机制成功执行序列复制操作的概率，并随后对其缩放特性进行数值研究。解决方案基于问题陈述中提供的统计模型。\n\n首先，我们形式化模型的各个组成部分。在长度为 $L$ 的序列中，对于给定位置 $i$，我们有一个正确键的聚合注意力分数，记为 $S_c$，以及 $L-1$ 个不正确键的聚合注意力分数，记为 $\\{S_{w,j}\\}_{j=1}^{L-1}$。\n根据问题的公理化模型：\n1.  正确键的分数 $S_c$ 是一个高斯随机变量，其均值为 $\\mu_H$，方差为单位方差：$S_c \\sim \\mathcal{N}(\\mu_H, 1)$。\n2.  不正确键的分数 $S_{w,j}$ 是独立同分布（i.i.d.）的高斯随机变量，其均值为零，方差为单位方差：$S_{w,j} \\sim \\mathcal{N}(0, 1)$。\n3.  均值优势 $\\mu_H$ 取决于查询-键相关性 $\\rho$、键维度 $d_k$ 和头数 $H$。问题陈述指出，单头（$H=1$）分数 $s_{ii}$ 的均值优势为 $\\rho\\sqrt{d_k}$，并且对于多头系统，该优势按 $\\sqrt{H}$ 比例缩放。因此，聚合的正确分数的均值为 $\\mu_H = \\rho \\sqrt{d_k} \\sqrt{H} = \\rho \\sqrt{d_k H}$。\n\n任务将一次正确的复制定义为正确键的分数大于所有不正确键分数的事件。这基于对 softmax 函数的硬性最大值近似。此事件的概率 $P(\\text{correct})$ 可表示为：\n$$\nP(\\text{correct}) = P\\left(S_c  \\max_{j=1, \\dots, L-1} S_{w,j}\\right)\n$$\n\n为了评估此概率，我们首先需要不正确分数最大值的分布。设 $M = \\max_{j=1, \\dots, L-1} S_{w,j}$。$M$ 的累积分布函数（CDF），记为 $F_M(x)$，给出 $M$ 小于或等于某个值 $x$ 的概率。\n$$\nF_M(x) = P(M \\le x) = P(S_{w,1} \\le x, S_{w,2} \\le x, \\ldots, S_{w,L-1} \\le x)\n$$\n由于不正确分数 $S_{w,j}$ 是独立同分布的，联合概率是各个概率的乘积：\n$$\nF_M(x) = \\prod_{j=1}^{L-1} P(S_{w,j} \\le x)\n$$\n设 $\\Phi(x)$ 是标准正态分布 $\\mathcal{N}(0, 1)$ 的累积分布函数。那么，对于每个不正确分数，$P(S_{w,j} \\le x) = \\Phi(x)$。由此可得 $L-1$ 个独立同分布的标准正态变量最大值的累积分布函数：\n$$\nF_M(x) = [\\Phi(x)]^{L-1}\n$$\n\n现在，我们可以通过对 $S_c$ 的值进行条件化并对其所有可能值进行积分来计算总概率 $P(S_c  M)$。设 $y$ 是随机变量 $S_c$ 所取的一个值。$S_c$ 的概率密度函数（PDF）是均值为 $\\mu_H$、方差为 $1$ 的正态分布的概率密度函数，我们将其表示为 $\\phi(y - \\mu_H)$，其中 $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2}$ 是标准正态概率密度函数。\n总概率由全概率定律给出：\n$$\nP(\\text{correct}) = \\int_{-\\infty}^{\\infty} P(M  S_c | S_c = y) f_{S_c}(y) \\,dy\n$$\n给定 $S_c = y$，条件变为 $M  y$。此事件的概率就是 $F_M(y)$。因此，我们有：\n$$\nP(\\text{correct}) = \\int_{-\\infty}^{\\infty} F_M(y) f_{S_c}(y) \\,dy\n$$\n代入 $F_M(y)$ 和 $f_{S_c}(y)$ 的表达式，我们得到正确复制概率的最终一维积分表达式：\n$$\nP(L, d_k, \\rho, H) = \\int_{-\\infty}^{\\infty} [\\Phi(y)]^{L-1} \\phi(y - \\mu_H) \\,dy\n$$\n其中 $\\mu_H = \\rho\\sqrt{d_k H}$。此表达式是序列长度 $L$、键维度 $d_k$、相关性 $\\rho$ 和头数 $H$ 的函数，符合要求。\n\n该积分没有简单的闭式解，必须进行数值计算。程序实现将使用数值积分（`scipy.integrate.quad`）来计算此概率。为了找到给定准确率阈值 $\\tau=0.9$ 的最小头数 $H^\\star$，我们可以利用 $P(\\text{correct})$ 是 $H$ 的单调递增函数这一事实。因此，我们可以从 $H=1$ 开始，并递增它，直到计算出的概率首次达到或超过 $\\tau$。问题要求的其余量，如比率 $\\frac{H^\\star}{\\log L}$ 和缩放指数 $\\alpha$，则直接从确定的 $H^\\star$ 值计算得出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.stats import norm\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving and computing attention mechanism performance metrics.\n    \"\"\"\n\n    # Define the parameters from the problem statement.\n    TAU = 0.9\n    RHO = 0.25\n    DK = 64\n    LS = [16, 64, 256]\n\n    def prob_correct(L: int, H: int, dk: int, rho: float) -> float:\n        \"\"\"\n        Computes the probability of a correct copy for given parameters\n        by numerically evaluating the derived integral.\n        \"\"\"\n        if L == 1:\n            return 1.0\n\n        # Mean of the correct score's distribution\n        mu = rho * math.sqrt(dk * H)\n\n        # Integrand: (Phi(y))^(L-1) * phi(y - mu)\n        # norm.pdf(y, loc=mu, scale=1) is equivalent to phi(y - mu)\n        def integrand(y: float) -> float:\n            return norm.cdf(y)**(L - 1) * norm.pdf(y, loc=mu, scale=1)\n\n        # Numerical integration over (-inf, inf). High precision is specified\n        # to ensure stable outputs.\n        prob, _ = quad(integrand, -np.inf, np.inf, epsabs=1e-12, epsrel=1e-12)\n        \n        return prob\n\n    def find_H_star(L: int, tau: float, dk: int, rho: float) -> int:\n        \"\"\"\n        Finds the minimal integer number of heads H* required to achieve\n        a target probability threshold tau.\n        \"\"\"\n        H = 1\n        # The probability is monotonic in H, so a simple search is effective.\n        while True:\n            p = prob_correct(L, H, dk, rho)\n            if p >= tau:\n                return H\n            H += 1\n            # Safety break in case an error leads to an infinite loop.\n            if H > 1000:\n                raise RuntimeError(f\"Search for H* for L={L} exceeded 1000 iterations.\")\n\n    results = []\n\n    # 1. Compute minimal heads H* for L in {16, 64, 256}.\n    H_16 = find_H_star(LS[0], TAU, DK, RHO)\n    H_64 = find_H_star(LS[1], TAU, DK, RHO)\n    H_256 = find_H_star(LS[2], TAU, DK, RHO)\n    results.extend([H_16, H_64, H_256])\n\n    # 2. Compute probabilities for H=1 and L={16, 64}.\n    P_16_H1 = prob_correct(LS[0], 1, DK, RHO)\n    P_64_H1 = prob_correct(LS[1], 1, DK, RHO)\n    results.extend([P_16_H1, P_64_H1])\n    \n    # 3. Compute ratios H*/log(L). All logs are natural logarithms.\n    Ratio_16 = H_16 / np.log(LS[0])\n    Ratio_64 = H_64 / np.log(LS[1])\n    Ratio_256 = H_256 / np.log(LS[2])\n    results.extend([Ratio_16, Ratio_64, Ratio_256])\n    \n    # 4. Estimate the scaling exponent alpha.\n    log_H_256 = np.log(H_256)\n    log_H_16 = np.log(H_16)\n    log_log_256 = np.log(np.log(LS[2]))\n    log_log_16 = np.log(np.log(LS[0]))\n    alpha = (log_H_256 - log_H_16) / (log_log_256 - log_log_16)\n    results.append(alpha)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}