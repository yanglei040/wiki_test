{
    "hands_on_practices": [
        {
            "introduction": "位置编码方案的一个关键优势在于其泛化到比训练时所见序列更长的序列的能力。本练习  设计了一个受控实验，在一个简化的回归任务中，比较确定性的正弦编码与简单的学习编码在外推性能上的差异。通过这个实践，你将亲手揭示前者（正弦编码）因其内在结构而带来的显著优势。",
            "id": "3100282",
            "problem": "您的任务是构建一个原则性的比较，在一个将位置信息与内容分离的简化模型中，比较确定性正弦位置编码和学习型位置编码在超出训练序列长度范围进行外推时的表现。该设置模拟了位置编码如何为线性解码器提供特征，这是基于注意力的模型如何使用位置信号的最小抽象。您必须从第一性原理出发推导误差行为，并实现一个程序来计算两种编码类型的外推误差。\n\n本任务的基础是：\n- 自注意力的定义，它需要位置信息来解析顺序，以及位置编码将位置相关特征注入模型计算中的概念。\n- 经过充分检验的普通最小二乘法 (OLS)，它通过最小化残差平方和来拟合从特征向量到标量目标的线性映射。\n- 均方根误差 (RMSE) 的定义，它量化了在一个集合上预测值与目标值之间的差异。\n\n序列由离散位置 $p \\in \\{1,2,\\dots\\}$ 索引，并有一个标量目标 $y(p)$，该目标由正弦分量的叠加定义，所有角度计算均以弧度表示。具体而言，对所有位置 $p$，定义\n$$\ny(p) = \\sin(\\omega_1 p) + \\tfrac{1}{2}\\cos(\\omega_2 p),\n$$\n其中 $\\omega_1 = \\tfrac{2\\pi}{16}$ 且 $\\omega_2 = \\tfrac{2\\pi}{32}$。\n\n您必须实现两种位置编码策略，为每个位置 $p$ 生成特征向量 $x(p)$：\n\n- 正弦位置编码：使用一个维度为 $d$ 的偶数维特征向量，由在固定角频率下的 $\\sin$ 和 $\\cos$ 函数对拼接而成。特征向量包含一组角频率 $\\{\\omega_k\\}$ 对应的 $(\\sin(\\omega_k p), \\cos(\\omega_k p))$ 对，该角频率集合包括 $\\omega_1$ 和 $\\omega_2$，以及其他不同的频率，以确保在训练位置上的矩阵是满列秩的。这些特征的确切构造和缩放是您实现的一部分，但它们必须是 $p$ 的确定性函数，而不是从数据中学到的。\n\n- 学习型位置编码：对于位置 $p \\in \\{1,\\dots,N_{\\text{train}}\\}$，使用长度为 $N_{\\text{train}}$ 的独热（one-hot）特征向量。对于外推至 $p > N_{\\text{train}}$ 的情况，使用一种钳位规则 (clamping rule)，即所有位置都映射到索引为 $N_{\\text{train}}$ 的独热向量。这模拟了学习型嵌入表的常见行为，即在没有明确扩展的情况下无法为未见过的位置生成嵌入。\n\n训练使用普通最小二乘法 (OLS) 进行：给定训练位置 $p \\in \\{1,2,\\dots,N_{\\text{train}}\\}$，根据 $x(p)$ 构建设计矩阵 $X \\in \\mathbb{R}^{N_{\\text{train}} \\times d}$，并根据 $y(p)$ 构建目标向量 $y \\in \\mathbb{R}^{N_{\\text{train}}}$。通过最小化训练目标来拟合线性解码器 $w \\in \\mathbb{R}^{d}$\n$$\n\\min_{w} \\sum_{p=1}^{N_{\\text{train}}} \\left( y(p) - w^\\top x(p) \\right)^2.\n$$\n使用拟合出的 $w$ 为位置 $p > N_{\\text{train}}$ 生成预测值 $\\hat{y}(p) = w^\\top x(p)$。\n\n对于任何整数 $n \\ge N_{\\text{train}}$，将外推误差 $E(n)$ 定义为在超出训练范围的位置上的均方根误差 (RMSE)：\n$$\nE(n) =\n\\begin{cases}\n0,  \\text{if } n = N_{\\text{train}},\\\\\n\\sqrt{\\dfrac{1}{n - N_{\\text{train}}} \\sum_{p=N_{\\text{train}}+1}^{n} \\left( y(p) - \\hat{y}(p) \\right)^2},  \\text{if } n > N_{\\text{train}}.\n\\end{cases}\n$$\n\n您的程序必须：\n- 使用训练最大值 $N_{\\text{train}} = 64$ 和正弦编码维度 $d = 16$（偶数）。\n- 使用上述 $y(p)$，其中 $\\omega_1 = \\tfrac{2\\pi}{16}$ 和 $\\omega_2 = \\tfrac{2\\pi}{32}$，所有角度计算均以弧度为单位。\n- 构建正弦位置编码，其频率集包括 $\\omega_1$ 和 $\\omega_2$ 以及其他不同的正角频率。\n- 将学习型位置编码构建为长度为 $N_{\\text{train}}$ 的独热向量，并对 $p > N_{\\text{train}}$ 应用钳位规则。\n- 在 $p \\in \\{1,2,\\dots,N_{\\text{train}}\\}$ 上，为每种编码分别通过 OLS 拟合 $w$。\n\n测试套件：\n- 使用以下 $n$ 值：$64$、$80$、$128$、$160$。\n- 该套件包括边界情况 $n = N_{\\text{train}}$（根据定义，此时 $E(n) = 0$）、一个中等长度的外推，以及更长的外推长度，以探测持续性能。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表应按顺序包含每个测试用例中正弦编码和学习型编码的 RMSE 值，汇总所有测试结果：\n$$\n[\\;E_{\\text{sin}}(64),\\;E_{\\text{learn}}(64),\\;E_{\\text{sin}}(80),\\;E_{\\text{learn}}(80),\\;E_{\\text{sin}}(128),\\;E_{\\text{learn}}(128),\\;E_{\\text{sin}}(160),\\;E_{\\text{learn}}(160)\\;].\n$$\n所有角度计算必须以弧度为单位，输出必须是实值浮点数。",
            "solution": "用户希望在一个简化的线性回归设置中，分析和实现对正弦位置编码和学习型位置编码在外推能力上的比较。\n\n### 步骤 1：提取给定条件\n- **目标函数**：$y(p) = \\sin(\\omega_1 p) + \\tfrac{1}{2}\\cos(\\omega_2 p)$，对于整数位置 $p \\in \\{1,2,\\dots\\}$。\n- **角频率**：$\\omega_1 = \\tfrac{2\\pi}{16}$ 和 $\\omega_2 = \\tfrac{2\\pi}{32}$。所有角度计算必须以弧度为单位。\n- **训练范围**：位置 $p \\in \\{1, 2, \\dots, N_{\\text{train}}\\}$。\n- **训练集大小**：$N_{\\text{train}} = 64$。\n- **训练方法**：普通最小二乘法 (OLS)，最小化 $\\sum_{p=1}^{N_{\\text{train}}} ( y(p) - w^\\top x(p) )^2$。\n- **正弦编码**：\n    - 特征维度 $d = 16$。\n    - 特征是 $(\\sin(\\omega_k p), \\cos(\\omega_k p))$ 对。\n    - 频率集合 $\\{\\omega_k\\}$ 必须包含 $\\omega_1$ 和 $\\omega_2$。\n- **学习型编码**：\n    - 对于 $p \\in \\{1, \\dots, N_{\\text{train}}\\}$，$x(p)$ 是一个长度为 $N_{\\text{train}}$ 的独热向量。\n    - 对于 $p > N_{\\text{train}}$，使用钳位规则：$x(p) = x(N_{\\text{train}})$。\n- **外推误差度量**：$E(n)$，在位置 $\\{N_{\\text{train}}+1, \\dots, n\\}$ 上的均方根误差 (RMSE)。\n  $$\n  E(n) =\n  \\begin{cases}\n  0,  \\text{if } n = N_{\\text{train}},\\\\\n  \\sqrt{\\dfrac{1}{n - N_{\\text{train}}} \\sum_{p=N_{\\text{train}}+1}^{n} \\left( y(p) - \\hat{y}(p) \\right)^2},  \\text{if } n > N_{\\text{train}}.\n  \\end{cases}\n  $$\n- **测试套件**：对 $n \\in \\{64, 80, 128, 160\\}$ 评估 $E(n)$。\n- **要求输出**：一个单行列表：`[E_sin(64), E_learn(64), E_sin(80), E_learn(80), E_sin(128), E_learn(128), E_sin(160), E_learn(160)]`。\n\n### 步骤 2：使用提取的给定条件进行验证\n对问题进行严格验证。\n\n- **科学依据**：该问题基于统计学习（普通最小二乘法）、数值分析（RMSE）的成熟原理，以及现代深度学习（Transformer模型中的位置编码）的概念。目标函数是一个简单的、行为良好的数学函数。该设置是分析特征表示的一种有效且常用的技术。\n- **适定性**：这是一个适定问题。只要设计矩阵是满列秩的，OLS 解就是唯一的。对于正弦编码，问题明确指示要选择能确保这一点的频率。对于学习型编码，训练集上的设计矩阵是单位矩阵，是满秩的。外推规则的定义清晰明确。对于这两种情况，权重 $w$ 和后续预测 $\\hat{y}(p)$ 的唯一解都存在。\n- **客观性**：问题以精确、客观的数学语言陈述。\n- **完整性**：所有必要的参数（$N_{\\text{train}}$、$d$、$\\omega_1$、$\\omega_2$）和定义都已提供。构建正弦频率的规范允许进行原则性选择（例如，包含指定周期的等比数列），使得问题是完全确定的。\n- **一致性**：定义和约束是内部一致的。向量和矩阵的维度是兼容的。\n\n该问题没有任何无效性缺陷。它是一个形式化、有科学依据且适定的问题。\n\n### 步骤 3：结论与行动\n问题是有效的。将提供一个解决方案。\n\n### 理论框架与推导\n问题的核心是拟合一个线性模型 $\\hat{y}(p) = w^\\top x(p)$ 到目标函数 $y(p)$，使用两种不同的特征映射 $x(p)$，并比较它们的外推性能。模型参数 $w$ 是通过在训练集 $p \\in \\{1, \\dots, N_{\\text{train}}\\}$ 上使用普通最小二乘法 (OLS) 找到的。\n\n设 $X \\in \\mathbb{R}^{N_{\\text{train}} \\times d}$ 为设计矩阵，其行是 $p=1, \\dots, N_{\\text{train}}$ 的特征向量 $x(p)^\\top$。设 $y_{\\text{train}} \\in \\mathbb{R}^{N_{\\text{train}}}$ 为训练集上的目标值向量 $y(p)$。权重向量 $w$ 的 OLS 解由正规方程组给出：\n$$\nw = (X^\\top X)^{-1} X^\\top y_{\\text{train}}\n$$\n如果 $X^\\top X$ 可逆，即 $X$ 具有满列秩，则该解存在且唯一。\n\n#### 1. 正弦位置编码\n该策略旨在使用一组周期函数作为基来表示位置。\n- **特征向量构建**：特征向量 $x_{\\text{sin}}(p)$ 的维度为 $d=16$。它由 $d/2 = 8$ 对正弦和余弦函数拼接而成：\n$$\nx_{\\text{sin}}(p) = [\\sin(\\Omega_1 p), \\cos(\\Omega_1 p), \\sin(\\Omega_2 p), \\cos(\\Omega_2 p), \\dots, \\sin(\\Omega_8 p), \\cos(\\Omega_8 p)]^\\top\n$$\n问题要求角频率集合 $\\{\\Omega_k\\}_{k=1}^8$ 包含 $\\omega_1=\\frac{2\\pi}{16}$ 和 $\\omega_2=\\frac{2\\pi}{32}$。我们选择一组有原则的频率，对应于周期的等比数列，并确保包含所需的频率。一个合适的周期选择是 $\\{8, 16, 32, 64, 128, 256, 512, 1024\\}$。相应的角频率是 $\\Omega_k = 2\\pi/T_k$。这个集合是不同的，并跨越了很宽的范围。\n\n- **外推行为**：目标函数是 $y(p) = \\sin(\\omega_1 p) + \\frac{1}{2}\\cos(\\omega_2 p)$。由于 $\\omega_1$ 和 $\\omega_2$ 包含在我们的基频率中，真实函数 $y(p)$ 是用于构建 $x_{\\text{sin}}(p)$ 的基函数的线性组合。具体来说，它对应于一个权重向量，其中 $\\sin(\\omega_1 p)$ 的系数为 $1$，$\\cos(\\omega_2 p)$ 的系数为 $0.5$，所有其他系数均为 $0$。尽管正弦基函数在有限、离散的训练区间 $\\{1, \\dots, 64\\}$ 上并非完全正交，但 OLS 旨在找到最佳线性逼近。它会找到一个权重向量 $w_{\\text{sin}}$，使得模型 $\\hat{y}(p) = w_{\\text{sin}}^\\top x_{\\text{sin}}(p)$ 能够紧密地逼近真实函数。因为模型具有真实数据生成过程的函数形式，这种逼近不仅对训练点成立，对所有外推点 $p > N_{\\text{train}}$ 也成立。因此，外推误差 $E_{\\text{sin}}(n)$ 预计会非常小，主要受限于数值精度和由于基在有限训练集上的非正交性引起的微小拟合误差。\n\n#### 2. 学习型位置编码\n该策略模拟了为训练集中每个位置独立学习的位置嵌入。\n- **特征向量构建**：特征维度是 $d = N_{\\text{train}} = 64$。对于一个训练位置 $p \\in \\{1, \\dots, 64\\}$，特征向量 $x_{\\text{learn}}(p)$ 是一个独热向量，其中第 $p$ 个元素为 $1$，其余均为 $0$。因此，设计矩阵 $X_{\\text{learn}}$ 是 $64 \\times 64$ 的单位矩阵 $I_{64}$。\n\n- **训练**：权重的 OLS 解是：\n$$\nw_{\\text{learn}} = (X_{\\text{learn}}^\\top X_{\\text{learn}})^{-1} X_{\\text{learn}}^\\top y_{\\text{train}} = (I_{64}^\\top I_{64})^{-1} I_{64}^\\top y_{\\text{train}} = y_{\\text{train}}\n$$\n这意味着模型只是简单地记住了每个训练位置的目标值。权重向量 $w_{\\text{learn}}$ 与训练目标向量完全相同。\n\n- **外推行为**：问题为外推指定了一个钳位规则：对于任何 $p > N_{\\text{train}}$，其特征向量与最后一个训练位置的特征向量相同，即 $x_{\\text{learn}}(p) = x_{\\text{learn}}(N_{\\text{train}})$。这是对应于位置 $N_{\\text{train}}=64$ 的独热向量。对于任何这样的位置，其预测值为：\n$$\n\\hat{y}_{\\text{learn}}(p) = w_{\\text{learn}}^\\top x_{\\text{learn}}(N_{\\text{train}})\n$$\n由于 $w_{\\text{learn}, i} = y(i)$（暂时使用 1-based 索引）且 $x_{\\text{learn}}(N_{\\text{train}})$ 仅在第 $N_{\\text{train}}$ 个位置为 $1$，点积会选出 $w_{\\text{learn}}$ 的最后一个元素。\n$$\n\\hat{y}_{\\text{learn}}(p) = y(N_{\\text{train}}) \\quad \\text{对于所有 } p > N_{\\text{train}}\n$$\n因此，模型对所有未见过的位置都做出一个恒定的预测，该预测值等于其训练经验边缘处的值。真实函数 $y(p)$ 在 $p>64$ 时会继续振荡，因此误差 $y(p) - y(64)$ 将会很大。RMSE，即 $E_{\\text{learn}}(n)$，会累积这些大的误差，预计将显著高于正弦编码的误差。\n\n### 外推误差 $E(n)$ 的计算\n对于每个测试值 $n > N_{\\text{train}}$：\n1. 定义外推位置集合 $P_{\\text{extrap}} = \\{N_{\\text{train}}+1, \\dots, n\\}$。\n2. 为所有 $p \\in P_{\\text{extrap}}$ 生成真实值 $y(p)$。\n3. 为所有 $p \\in P_{\\text{extrap}}$ 生成预测值 $\\hat{y}_{\\text{sin}}(p)$ 和 $\\hat{y}_{\\text{learn}}(p)$。\n4. 对每种情况计算 RMSE：\n   $$\n   E(n) = \\sqrt{\\frac{1}{n - N_{\\text{train}}} \\sum_{p \\in P_{\\text{extrap}}} (y(p) - \\hat{y}(p))^2}\n   $$\n对于边界情况 $n=N_{\\text{train}}$，根据定义，两种方法的误差 $E(N_{\\text{train}})$ 均为 0。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the extrapolation error of sinusoidal and learned\n    positional encodings within a simplified linear regression framework.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    N_train = 64\n    d_sin = 16\n    omega1 = 2 * np.pi / 16\n    omega2 = 2 * np.pi / 32\n    test_n_values = [64, 80, 128, 160]\n\n    # --- Target Function ---\n    def target_function(p):\n        \"\"\"Computes the true scalar target y(p).\"\"\"\n        return np.sin(omega1 * p) + 0.5 * np.cos(omega2 * p)\n\n    # --- Training Data ---\n    # Use 1-based indexing for positions as in the problem description\n    train_positions = np.arange(1, N_train + 1)\n    y_train = target_function(train_positions)\n\n    # --- 1. Sinusoidal Positional Encoding ---\n\n    # Define frequencies for sinusoidal encoding. The set of periods includes\n    # 16 and 32 (corresponding to omega1 and omega2) plus others to ensure\n    # full rank and represent a range of frequencies.\n    periods = np.array([8, 16, 32, 64, 128, 256, 512, 1024])\n    sin_frequencies = 2 * np.pi / periods\n    \n    def get_sinusoidal_features(positions, freqs, dim):\n        \"\"\"Generates the sinusoidal feature matrix for a given set of positions.\"\"\"\n        num_pos = len(positions)\n        num_freqs = dim // 2\n        # Ensure freqs has the right length just in case\n        if len(freqs) != num_freqs:\n            raise ValueError(f\"Expected {num_freqs} frequencies for dimension {dim}, but got {len(freqs)}\")\n            \n        # Create a matrix of p*omega values\n        p_omega = positions[:, np.newaxis] * freqs[np.newaxis, :]\n        \n        # Create the feature matrix by interleaving sin and cos\n        X = np.zeros((num_pos, dim))\n        X[:, 0::2] = np.sin(p_omega)\n        X[:, 1::2] = np.cos(p_omega)\n        return X\n\n    # Fit the linear decoder for sinusoidal encoding\n    X_sin_train = get_sinusoidal_features(train_positions, sin_frequencies, d_sin)\n    w_sin, _, _, _ = np.linalg.lstsq(X_sin_train, y_train, rcond=None)\n\n    # --- 2. Learned Positional Encoding ---\n    # For learned one-hot encoding, the weight vector w_learn is simply the\n    # target vector y_train, as X_learn is the identity matrix.\n    # The extrapolation prediction is clamped to the value at N_train.\n    y_pred_learn_extrapol_val = target_function(N_train)\n\n    # --- Calculate Errors for Test Cases ---\n    results = []\n    for n in test_n_values:\n        if n == N_train:\n            # By definition, extrapolation error at n = N_train is 0.\n            results.append(0.0)  # E_sin(64)\n            results.append(0.0)  # E_learn(64)\n            continue\n        \n        # Extrapolation positions\n        extrapol_positions = np.arange(N_train + 1, n + 1)\n        num_extrapol_points = len(extrapol_positions)\n        \n        # True values for the extrapolation range\n        y_true_extrapol = target_function(extrapol_positions)\n        \n        # Calculate error for sinusoidal encoding\n        X_sin_extrapol = get_sinusoidal_features(extrapol_positions, sin_frequencies, d_sin)\n        y_pred_sin_extrapol = X_sin_extrapol @ w_sin\n        error_sin = np.sqrt(np.mean((y_true_extrapol - y_pred_sin_extrapol)**2))\n        results.append(error_sin)\n\n        # Calculate error for learned encoding\n        # The prediction is a constant value for all extrapolation points\n        y_pred_learn_extrapol = np.full(num_extrapol_points, y_pred_learn_extrapol_val)\n        error_learn = np.sqrt(np.mean((y_true_extrapol - y_pred_learn_extrapol)**2))\n        results.append(error_learn)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在理解了正弦编码的优越性之后，本练习  将深入探究其有效性的根源，即将其视为一组频率基函数。通过选择性地移除（或称“消融”）不同的频率带，我们可以研究哪些频率成分对于表示不同类型的位置模式至关重要。这有助于我们理解模型如何利用位置编码来捕捉从平滑的低频信号到剧烈的高频变化等各种空间关系。",
            "id": "3164200",
            "problem": "给定一个由整数 $p \\in \\{0,1,\\dots,N-1\\}$ 索引的位置序列，以及一个称为位置编码 (Positional Encoding, PE) 的实值特征表示。目标是对 PE 执行频带消融，并分析不同频带如何对重建特定于任务的位置目标函数做出贡献。您必须实现一个完整的程序，该程序基于正弦基构建 PE，应用消融掩码移除高频或低频分量，拟合线性预测器以重建目标函数，并报告指定测试套件的归一化均方误差 (NMSE)。\n\n该问题的基本基础是信号的傅里叶表示：不同角频率下的正弦函数构成了在均匀间隔上采样的周期函数的一个基。您必须从此基导出 PE，而不是假设一个已知的 Transformer 特定公式。\n\n要求：\n- 为序列长度 $N = 128$ 构建一个维度为 $D = 1 + 2K$ 的 PE，其中第一个分量是常数偏置项，其余 $2K$ 个分量由 $K$ 个正弦频带使用正弦和余弦函数生成。角度必须以弧度为单位。频率必须在区间 $[\\omega_{\\min}, \\omega_{\\max}]$ 内对数间隔分布，其中 $K = 8$，$\\omega_{\\min} = \\frac{2\\pi}{N}$，且 $\\omega_{\\max} = \\pi$。将这些角频率表示为 $\\{\\omega_k\\}_{k=0}^{K-1}$。\n- 定义以下关于位置的目标函数 $h(p)$：\n    1. $h_{\\text{Lcos}}(p) = \\cos\\left(\\frac{2\\pi p}{64}\\right)$。\n    2. $h_{\\text{Hcos}}(p) = \\cos\\left(\\frac{2\\pi p}{4}\\right)$。\n    3. $h_{\\text{Window}}(p) = \\mathbb{1}\\{|p - 64| \\le 5\\}$，其中 $\\mathbb{1}\\{\\cdot\\}$ 是指示函数。\n    4. $h_{\\text{Square8}}(p) = \\mathbb{1}\\{(p \\bmod 8)  4\\}$。\n- 对于消融，您必须对与特定频带相关联的 PE 分量应用掩码。始终保留常数偏置项。对于在截止频率 $\\omega_c$ 处的低通消融，保留所有频率分量 $\\omega_k \\le \\omega_c$，并移除所有 $\\omega_k  \\omega_c$ 的分量。对于在截止频率 $\\omega_c$ 处的高通消融，保留所有频率分量 $\\omega_k \\ge \\omega_c$，并移除所有 $\\omega_k  \\omega_c$ 的分量。对于不进行消融 (\"none\")，保留所有频率分量。\n- 拟合一个线性预测器 $f(p) = \\mathbf{x}(p)^\\top \\mathbf{w}$ 以最小化均方误差，其中 $\\mathbf{x}(p)$ 是消融后在位置 $p$ 处的 PE 向量。使用完整的位置集 $p \\in \\{0,\\dots,N-1\\}$ 作为训练集。计算归一化均方误差 (NMSE)，定义为\n$$\n\\mathrm{NMSE} = \\frac{\\frac{1}{N}\\sum_{p=0}^{N-1}\\big(h(p) - f(p)\\big)^2}{\\mathrm{Var}\\big(h(p)\\big)},\n$$\n其中 $\\mathrm{Var}\\big(h(p)\\big)$ 是 $h(p)$ 在 $p=0,\\dots,N-1$ 上的方差。将 $\\mathrm{NMSE}$ 报告为浮点数。\n\n角度单位说明：所有角度量必须以弧度为单位。\n\n测试套件：\n您必须实现以下有序的测试用例列表。每个测试用例是一个三元组 $(\\text{ablation\\_type}, \\omega_c, \\text{task})$，其中 $\\text{ablation\\_type} \\in \\{\\text{\"none\"}, \\text{\"low\\_pass\"}, \\text{\"high\\_pass\"}\\}$，$\\omega_c$ 是一个非负实数，单位为弧度，$\\text{task} \\in \\{\\text{\"Lcos\"}, \\text{\"Hcos\"}, \\text{\"Window\"}, \\text{\"Square8\"}\\}$ 对应于上面定义的目标函数。\n\n- 测试用例 1: $(\\text{\"none\"}, \\frac{\\pi}{2}, \\text{\"Lcos\"})$\n- 测试用例 2: $(\\text{\"low\\_pass\"}, 0.4\\pi, \\text{\"Hcos\"})$\n- 测试用例 3: $(\\text{\"high\\_pass\"}, 0.2\\pi, \\text{\"Hcos\"})$\n- 测试用例 4: $(\\text{\"low\\_pass\"}, 0.1\\pi, \\text{\"Window\"})$\n- 测试用例 5: $(\\text{\"high\\_pass\"}, 0.8\\pi, \\text{\"Lcos\"})$\n- 测试用例 6: $(\\text{\"low\\_pass\"}, 0.01, \\text{\"Lcos\"})$ （边界情况：几乎所有频带都被移除）\n- 测试用例 7: $(\\text{\"high\\_pass\"}, \\pi + 0.01, \\text{\"Hcos\"})$ （边界情况：几乎所有频带都被移除）\n- 测试用例 8: $(\\text{\"none\"}, \\frac{\\pi}{2}, \\text{\"Square8\"})$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含 NMSE 结果，按上述测试用例的顺序排列，形式为用方括号括起来的逗号分隔列表（例如，“[r1,r2,...,r8]”）。每个元素必须是浮点数。",
            "solution": "我们从信号的傅里叶表示开始：对于不同的角频率 $\\omega$，正弦和余弦函数集合 $\\{\\sin(\\omega p), \\cos(\\omega p)\\}$ 构成一个基，可以表示在均匀间隔上采样的周期信号。对于离散位置 $p \\in \\{0,\\dots,N-1\\}$，任何足够平滑或周期性的目标函数都可以通过此类基函数的线性组合来近似。这是傅里叶分析中一个经过充分检验的事实。\n\n基于原理的设计：\n1. 从第一性原理构建基：由于正弦和余弦函数在适当条件下在均匀采样上是正交的，它们可作为表示位置信息的自然基。我们构建位置编码 (PE) 为一个向量 $\\mathbf{x}(p) \\in \\mathbb{R}^{D}$，它包含一个常数偏置项以及在对数间隔的角频率 $\\omega_k \\in [\\omega_{\\min}, \\omega_{\\max}]$ 上的成对的正弦和余弦分量，其中有 $K$ 个频带且 $D = 1 + 2K$。常数项使模型能够表示目标函数的均值。对数间隔跨越了多个数量级的位置尺度，提供了低频和高频的敏感度。\n\n2. 任务和频率内容：\n   - 对于 $h_{\\text{Lcos}}(p) = \\cos\\left(\\frac{2\\pi p}{64}\\right)$，主导频率是低频，角频率为 $\\omega = \\frac{2\\pi}{64}$。预计这将被低频分量捕获；移除高频分量应该不会有太大影响，但移除低频分量会降低重建质量。\n   - 对于 $h_{\\text{Hcos}}(p) = \\cos\\left(\\frac{2\\pi p}{4}\\right)$，主导频率是高频，角频率为 $\\omega = \\frac{2\\pi}{4} = \\frac{\\pi}{2}$。在低于此值的截止频率进行低通消融将移除关键分量，而在高于一个较低截止频率处进行高通消融则会保留它。\n   - 对于 $h_{\\text{Window}}(p) = \\mathbb{1}\\{|p - 64| \\le 5\\}$，这个目标是局部的、非平滑的（一个阶跃）。需要高频来模拟急剧的转变；低通消融将导致模糊并增加误差。\n   - 对于 $h_{\\text{Square8}}(p) = \\mathbb{1}\\{(p \\bmod 8)  4\\}$，这是一个周期为 $8$、占空比为 $50\\%$ 的方波。其傅里叶级数包含一个常数项加上高频的奇次谐波。移除高频会降低重建质量；保留它们则有帮助。\n\n3. 线性投影和最优性：给定一个设计矩阵 $X \\in \\mathbb{R}^{N \\times D}$，其行是 PE 向量 $\\mathbf{x}(p)$，最小化 $\\sum_p \\big(h(p) - \\mathbf{x}(p)^\\top \\mathbf{w}\\big)^2$ 的最小二乘 (LS) 解 $\\hat{\\mathbf{w}}$ 是目标向量 $\\mathbf{h}$ 在 $X$ 的列空间上的正交投影。就均方误差而言，此投影在基的所有线性组合中是最优的。因此，移除基列的消融操作会限制子空间，只会增加误差或使其保持不变。\n\n4. 归一化误差：归一化均方误差 (NMSE) 定义为\n$$\n\\mathrm{NMSE} = \\frac{\\frac{1}{N}\\sum_{p=0}^{N-1}\\big(h(p) - f(p)\\big)^2}{\\mathrm{Var}\\big(h(p)\\big)},\n$$\n对于确定性数据的线性回归，这等于 $1 - R^2$。当预测器至少包含一个常数项且目标具有非零方差时，其值在 $[0,1]$ 区间内。如果所有正弦分量都被消融，LS 解将退化为等于 $h$ 均值的常数预测器，产生 $\\mathrm{NMSE} \\approx 1$。\n\n算法步骤：\n- 定义 $N = 128$，$K = 8$，$\\omega_{\\min} = \\frac{2\\pi}{N}$，$\\omega_{\\max} = \\pi$，并生成在 $[\\omega_{\\min}, \\omega_{\\max}]$ 内对数间隔的 $\\{\\omega_k\\}_{k=0}^{K-1}$。\n- 对于每个位置 $p$，构建 PE 向量 $\\mathbf{x}(p)$，包含：\n  - 一个常数项 $1$。\n  - 对于每个 $\\omega_k$，附加 $\\sin(\\omega_k p)$ 和 $\\cos(\\omega_k p)$。\n- 实现消融：\n  - 在 $\\omega_c$ 处的低通：保留与 $\\omega_k \\le \\omega_c$ 相关联的对。\n  - 在 $\\omega_c$ 处的高通：保留与 $\\omega_k \\ge \\omega_c$ 相关联的对。\n  - 无：保留所有对。\n  - 始终保留常数项。\n- 对于每个目标函数 $h$，构建向量 $\\mathbf{h} \\in \\mathbb{R}^{N}$。\n- 使用稳健的数值方法（例如，最小二乘求解器）求解 LS 问题 $\\min_{\\mathbf{w}} \\|\\mathbf{h} - X\\mathbf{w}\\|_2^2$，获得 $\\hat{\\mathbf{w}}$ 和预测值 $\\hat{\\mathbf{h}} = X\\hat{\\mathbf{w}}$。\n- 根据上述公式计算 $\\mathrm{NMSE}$。\n- 运行指定的测试套件并输出 NMSE。\n\n测试套件的定性预期：\n- 测试用例 1 (none, Lcos)：使用完整的基，低频余弦应该能被很好地重建；预期 NMSE 较低。\n- 测试用例 2 (low-pass $0.4\\pi$, Hcos)：截止频率 $\\omega_c = 0.4\\pi$ 移除了高于 $1.2566$ 的分量，而目标频率是 $\\frac{\\pi}{2} \\approx 1.5708$；预期 NMSE 较高。\n- 测试用例 3 (high-pass $0.2\\pi$, Hcos)：保留了高于 $0.6283$ 的频率，包括 $\\frac{\\pi}{2}$；预期 NMSE 较低。\n- 测试用例 4 (low-pass $0.1\\pi$, Window)：只保留了非常低的分量；无法表示锐利的边缘；预期 NMSE 较高。\n- 测试用例 5 (high-pass $0.8\\pi$, Lcos)：只保留了非常高的分量，移除了低分量；预期 NMSE 较高。\n- 测试用例 6 (low-pass $0.01$, Lcos)：截止频率低于 $\\omega_{\\min}$，因此只剩下常数项；预期 NMSE 接近 $1$。\n- 测试用例 7 (high-pass $\\pi + 0.01$, Hcos)：截止频率高于 $\\omega_{\\max}$，因此只剩下常数项；预期 NMSE 接近 $1$。\n- 测试用例 8 (none, Square8)：完整的基允许通过谐波来近似方波；预期 NMSE 中等（由于 K 有限而非零）。\n\n程序将确定性地实现这些步骤，并按要求输出 NMSE。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_frequencies(N: int, K: int) -> np.ndarray:\n    # Logarithmically spaced angular frequencies in radians.\n    omega_min = 2 * np.pi / N\n    omega_max = np.pi\n    return np.geomspace(omega_min, omega_max, K)\n\ndef positional_encoding_matrix(N: int, omegas: np.ndarray) -> np.ndarray:\n    # Build PE matrix with shape (N, 1 + 2*K): [bias, sin(ω_k p), cos(ω_k p)]\n    K = len(omegas)\n    D = 1 + 2 * K\n    X = np.zeros((N, D), dtype=np.float64)\n    # Bias term\n    X[:, 0] = 1.0\n    # Sin/Cos terms\n    p = np.arange(N, dtype=np.float64)\n    for idx, omega in enumerate(omegas):\n        X[:, 1 + 2 * idx] = np.sin(omega * p)\n        X[:, 1 + 2 * idx + 1] = np.cos(omega * p)\n    return X\n\ndef apply_ablation(X: np.ndarray, omegas: np.ndarray, ablation_type: str, omega_c: float) -> np.ndarray:\n    # Always keep bias column 0\n    cols = [0]\n    if ablation_type == \"none\":\n        # Keep all sin/cos pairs\n        K = len(omegas)\n        for idx in range(K):\n            cols.extend([1 + 2 * idx, 1 + 2 * idx + 1])\n    elif ablation_type == \"low_pass\":\n        for idx, omega in enumerate(omegas):\n            if omega = omega_c:\n                cols.extend([1 + 2 * idx, 1 + 2 * idx + 1])\n    elif ablation_type == \"high_pass\":\n        for idx, omega in enumerate(omegas):\n            if omega >= omega_c:\n                cols.extend([1 + 2 * idx, 1 + 2 * idx + 1])\n    else:\n        raise ValueError(f\"Unknown ablation type: {ablation_type}\")\n    return X[:, cols]\n\ndef target_function(N: int, task: str) -> np.ndarray:\n    p = np.arange(N, dtype=np.float64)\n    if task == \"Lcos\":\n        # Low-frequency cosine, period 64\n        return np.cos(2 * np.pi * p / 64.0)\n    elif task == \"Hcos\":\n        # High-frequency cosine, period 4\n        return np.cos(2 * np.pi * p / 4.0)\n    elif task == \"Window\":\n        # Indicator of window centered at 64 with radius 5\n        return ((np.abs(p - 64) = 5).astype(np.float64))\n    elif task == \"Square8\":\n        # Square wave of period 8, 50% duty cycle\n        return ((p % 8)  4).astype(np.float64)\n    else:\n        raise ValueError(f\"Unknown task: {task}\")\n\ndef nmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    mse = np.mean((y_true - y_pred) ** 2)\n    var = np.var(y_true)\n    # var should be > 0 for given tasks\n    return float(mse / var)\n\ndef solve():\n    # Define constants\n    N = 128\n    K = 8\n    omegas = build_frequencies(N, K)\n    X_full = positional_encoding_matrix(N, omegas)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"none\", np.pi / 2, \"Lcos\"),\n        (\"low_pass\", 0.4 * np.pi, \"Hcos\"),\n        (\"high_pass\", 0.2 * np.pi, \"Hcos\"),\n        (\"low_pass\", 0.1 * np.pi, \"Window\"),\n        (\"high_pass\", 0.8 * np.pi, \"Lcos\"),\n        (\"low_pass\", 0.01, \"Lcos\"),\n        (\"high_pass\", np.pi + 0.01, \"Hcos\"),\n        (\"none\", np.pi / 2, \"Square8\"),\n    ]\n\n    results = []\n    for ablation_type, omega_c, task in test_cases:\n        X = apply_ablation(X_full, omegas, ablation_type, omega_c)\n        y = target_function(N, task)\n        # Least squares fit\n        w, *_ = np.linalg.lstsq(X, y, rcond=None)\n        y_hat = X @ w\n        results.append(nmse(y, y_hat))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "位置编码并非独立运作，而是为注意力机制提供关键的位置信号。本练习  旨在检验位置编码的幅度（或尺度）与注意力图行为之间的重要关系。我们将系统地改变位置信号的尺度，以观察它如何导致注意力模式变得弥散（均匀）或坍塌（过度集中），从而揭示正确缩放位置信息在模型中的重要性。",
            "id": "3180897",
            "problem": "您的任务是构建一个程序，研究缩放点积注意力机制如何依赖于位置编码的幅值。目标是量化当位置编码的振幅变化时，注意力图是坍缩（变得过度集中）还是扩散（变得过度均匀）。此分析必须纯粹以数学和算法术语来表达，不依赖经验数据集，并应基于深度学习中广泛使用的核心定义。\n\n起始点（核心定义）：\n- 缩放点积注意力 (Scaled Dot-Product Attention, SDPA)：给定查询 $Q$、键 $K$ 和值 $V$，注意力权重是通过对由归一化内积形成的分数矩阵逐行应用 $softmax$ 函数来计算的。令 $d_k$ 表示键维度。归一化分数矩阵是通过在应用 $softmax$ 之前除以 $\\sqrt{d_k}$ 形成的。\n- 正弦位置编码 (Sinusoidal Positional Encoding, PE)：对于长度为 $L$、模型维度为 $d$ 的序列，位置编码矩阵 $PE \\in \\mathbb{R}^{L \\times d}$ 为每个位置 $p \\in \\{0,\\dots,L-1\\}$ 分配一个确定性向量，该向量在具有几何级数增长波长的正弦和余弦函数之间交替。该矩阵是固定的，不依赖于输入内容。\n- 振幅缩放 (Amplitude Scaling)：为了分离位置编码的作用，我们使用在所有位置 $p$ 上都恒为零的内容向量 $x_p \\in \\mathbb{R}^d$。位置 $p$ 处的有效输入嵌入为 $e_p = x_p + a \\cdot PE_p = a \\cdot PE_p$，其中 $a \\ge 0$ 是一个标量振幅。查询 $Q$ 和键 $K$ 是通过使用固定矩阵对嵌入进行线性投影形成的，注意力权重使用 SDPA 计算。\n\n您的程序必须：\n1.  为长度 $L = 16$、偶数模型维度 $d = 32$ 的序列构建一个正弦位置编码 $PE \\in \\mathbb{R}^{L \\times d}$，采用标准的交替正弦和余弦设计，频率按对数间隔分布。\n2.  将所有位置 $p \\in \\{0, \\dots, L-1\\}$ 的内容嵌入 $x_p$ 设置为零向量，从而使得 $e_p = a \\cdot PE_p$。\n3.  固定随机种子 $s = 7$，并抽样两个独立矩阵 $W_Q \\in \\mathbb{R}^{d \\times d}$ 和 $W_K \\in \\mathbb{R}^{d \\times d}$，其元素从标准正态分布中抽取。定义 $Q = E W_Q$ 和 $K = E W_K$，其中 $E \\in \\mathbb{R}^{L \\times d}$ 按行堆叠嵌入 $e_p$。\n4.  使用缩放点积注意力计算注意力权重矩阵 $A \\in \\mathbb{R}^{L \\times L}$：对由 $Q$ 和 $K$ 的内积形成、并除以 $\\sqrt{d_k}$（其中 $d_k = d$）的分数矩阵逐行应用 $softmax$ 函数。\n5.  对于每行 $A_{p,:}$，使用自然对数计算香农熵 $H_p = -\\sum_{j=1}^{L} A_{p,j} \\log(A_{p,j})$。同时计算最大注意力权重 $M_p = \\max_{j} A_{p,j}$。通过计算平均值 $\\bar{H} = \\frac{1}{L} \\sum_{p=1}^{L} H_p$ 和 $\\bar{M} = \\frac{1}{L} \\sum_{p=1}^{L} M_p$ 来聚合这些指标。\n6.  对以下测试集中的每个振幅 $a$ 重复步骤 2 到 5：\n    - 边界和近边界情况：$a = 0.0$，$a = 10^{-6}$。\n    - 低到中等情况：$a = 0.05$，$a = 0.5$，$a = 1.0$。\n    - 高振幅情况：$a = 5.0$，$a = 20.0$。\n7.  您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个元素应为一个双元素列表 $[\\bar{H}, \\bar{M}]$，对应一个测试振幅，并按上述顺序排列。例如，输出格式必须与 $[[h_1,m_1],[h_2,m_2],\\dots,[h_7,m_7]]$ 完全一样，其中每个 $h_i$ 和 $m_i$ 都是浮点数。\n\n科学真实性与覆盖设计：\n-   $a = 0.0$ 的情况会强制 $Q = 0$ 和 $K = 0$，产生一个具有最大熵和最小最大权重的均匀注意力图，这可作为一个边界条件。\n-   非常小的 $a$ 值使分数接近于零，用于测试数值稳定性和近乎均匀的扩散。\n-   大的 $a$ 值通过投影放大了位置差异，用于测试 softmax 饱和、注意力图的潜在坍缩以及熵的减少。\n\n最终输出：\n-   程序必须输出单行，格式如指定，包含七对浮点数。不应打印任何额外文本。",
            "solution": "问题陈述已经过严格验证，并被认为是有效的。其科学基础植根于深度学习的原理，特别是 Transformer 架构的注意力机制。问题定义良好，提供了一套完整且一致的定义、参数和约束，从而能够得到一个唯一、确定且可验证的解。该实验设计是一种合理且常规的方法，用于分离和分析特定模型组件（在此案例中为正弦位置编码）的功能作用。\n\n问题的核心是研究在内容信息被有意置零的情况下，缩放点积注意力（SDPA）在不同位置信息振幅下的行为。这是通过将位置 $p$ 处的输入嵌入 $e_p$ 设置为位置编码向量 $PE_p$ 的缩放版本来实现的，即 $e_p = a \\cdot PE_p$，其中 $a \\geq 0$ 是一个标量振幅。\n\n首先，我们为长度为 $L=16$、模型维度为 $d=32$ 的序列构建正弦位置编码矩阵 $PE \\in \\mathbb{R}^{L \\times d}$。对于每个位置 $p \\in \\{0, \\dots, L-1\\}$ 和维度索引 $k \\in \\{0, \\dots, d-1\\}$，$PE$ 的元素由标准的交替正弦和余弦函数定义：\n$$\nPE_{(p, 2i)} = \\sin\\left(\\frac{p}{10000^{2i/d}}\\right) \\quad \\text{for } i \\in \\{0, \\dots, d/2-1\\}\n$$\n$$\nPE_{(p, 2i+1)} = \\cos\\left(\\frac{p}{10000^{2i/d}}\\right) \\quad \\text{for } i \\in \\{0, \\dots, d/2-1\\}\n$$\n这种表示法创建了能够编码相对和绝对位置的唯一位置向量。\n\n用于注意力机制的输入嵌入是通过将向量 $e_p$ 堆叠成矩阵 $E \\in \\mathbb{R}^{L \\times d}$ 来形成的。查询矩阵 $Q \\in \\mathbb{R}^{L \\times d}$ 和键矩阵 $K \\in \\mathbb{R}^{L \\times d}$ 随后通过使用固定的权重矩阵 $W_Q \\in \\mathbb{R}^{d \\times d}$ 和 $W_K \\in \\mathbb{R}^{d \\times d}$ 对这些嵌入进行线性投影来生成。这些权重矩阵从标准正态分布中抽样一次，随机种子固定为 $s=7$ 以确保可复现性。\n$$\nE = a \\cdot PE\n$$\n$$\nQ = E W_Q\n$$\n$$\nK = E W_K\n$$\n注意力分数使用缩放点积运算计算。分数矩阵 $S \\in \\mathbb{R}^{L \\times L}$ 是通过计算 $Q$ 与 $K$ 的转置的矩阵乘积，并将结果乘以键维度（设置为 $d_k=d=32$）的逆平方根进行缩放得到的。\n$$\nS = \\frac{Q K^T}{\\sqrt{d_k}}\n$$\n注意力权重矩阵 $A \\in \\mathbb{R}^{L \\times L}$ 是通过对分数矩阵 $S$ 逐行应用 softmax 函数得到的。对于每一行 $p$，位置对 $(p, j)$ 的注意力权重 $A_{p,j}$ 为：\n$$\nA_{p,j} = \\text{softmax}(S_{p,:})_j = \\frac{\\exp(S_{p,j})}{\\sum_{k=0}^{L-1} \\exp(S_{p,k})}\n$$\n当振幅 $a$ 为 $0$ 时，$E$、$Q$ 和 $K$ 变成零矩阵。因此，分数矩阵 $S$ 也是一个零矩阵。零向量的 softmax 是一个均匀分布，所以每个注意力权重 $A_{p,j}$ 变为 $1/L = 1/16$。这为完全扩散的注意力图提供了一个基准。相反，随着 $a$ 变大，$S$ 每行分数的方差增加，导致 softmax 函数饱和。这导致了一种“赢者通吃”的动态，即一行中的一个注意力权重趋近于 $1$，而其他权重趋近于 $0$，标志着注意力的坍缩。\n\n为了量化此行为，为注意力矩阵 $A$ 的每一行 $p$ 计算了两个指标：\n1.  **香农熵** $H_p$，用于衡量注意力分布的不确定性或扩散程度。更高的熵对应于更均匀的分布。\n    $$\n    H_p = -\\sum_{j=0}^{L-1} A_{p,j} \\log(A_{p,j})\n    $$\n2.  **最大注意力权重** $M_p$，用于衡量注意力的集中程度。更高的最大权重表示一个更集中或坍缩的注意力分布。\n    $$\n    M_p = \\max_{j} A_{p,j}\n    $$\n然后将这些指标在所有 $L$ 行上取平均，以产生最终的聚合指标 $\\bar{H}$ 和 $\\bar{M}$。对指定的一组振幅 $a$：$\\{0.0, 10^{-6}, 0.05, 0.5, 1.0, 5.0, 20.0\\}$ 重复整个过程，从而可以系统地分析位置编码幅值的影响。代码将实现此过程，为每个振幅计算 $[\\bar{H}, \\bar{M}]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import softmax\n\ndef solve():\n    \"\"\"\n    Computes attention metrics for varying positional encoding amplitudes.\n    \"\"\"\n    \n    # Define parameters from the problem statement.\n    L = 16  # Sequence length\n    d = 32  # Model dimension\n    d_k = d # Key dimension\n    seed = 7  # Random seed\n    amplitudes = [0.0, 1e-6, 0.05, 0.5, 1.0, 5.0, 20.0]\n\n    # Initialize random number generator and create projection matrices\n    rng = np.random.default_rng(seed)\n    W_Q = rng.standard_normal(size=(d, d))\n    W_K = rng.standard_normal(size=(d, d))\n\n    def create_pe(length, dim):\n        \"\"\"\n        Constructs a sinusoidal positional encoding matrix.\n        \"\"\"\n        if dim % 2 != 0:\n            raise ValueError(\"Model dimension d must be even.\")\n        \n        position = np.arange(length, dtype=np.float64).reshape(-1, 1)\n        # Denominator term in the PE formula\n        div_term = np.exp(np.arange(0, dim, 2, dtype=np.float64) * -(np.log(10000.0) / dim))\n        \n        pe = np.zeros((length, dim), dtype=np.float64)\n        pe[:, 0::2] = np.sin(position * div_term)\n        pe[:, 1::2] = np.cos(position * div_term)\n        return pe\n\n    # Pre-compute the positional encoding matrix\n    pe_matrix = create_pe(L, d)\n\n    results = []\n    for a in amplitudes:\n        # Step 2: Construct embeddings\n        # Content embeddings x_p are zero, so E is just scaled PE\n        E = a * pe_matrix\n\n        # Step 3: Define Q and K matrices\n        Q = E @ W_Q\n        K = E @ W_K\n\n        # Step 4: Compute attention weights\n        # Score matrix S = (Q @ K^T) / sqrt(d_k)\n        scores = (Q @ K.T) / np.sqrt(d_k)\n        \n        # Row-wise softmax to get attention weights A\n        # softmax handles the a=0 case correctly (uniform distribution)\n        attention_weights = softmax(scores, axis=1)\n\n        # Step 5: Compute metrics\n        \n        # Entropy H_p for each row p\n        # Use `where` to avoid log(0), as A_pj * log(A_pj) is 0 if A_pj is 0.\n        entropy_per_row = -np.sum(attention_weights * np.log(attention_weights, \n                                                             where=attention_weights > 0, \n                                                             out=np.zeros_like(attention_weights)), \n                                  axis=1)\n\n        # Max attention weight M_p for each row p\n        max_weight_per_row = np.max(attention_weights, axis=1)\n\n        # Aggregate by averaging over all L rows\n        avg_H = np.mean(entropy_per_row)\n        avg_M = np.mean(max_weight_per_row)\n\n        results.append([avg_H, avg_M])\n\n    # Final print statement in the exact required format.\n    # The string representation of a list is '[item1, item2]', which matches\n    # the inner list formatting requirement.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}