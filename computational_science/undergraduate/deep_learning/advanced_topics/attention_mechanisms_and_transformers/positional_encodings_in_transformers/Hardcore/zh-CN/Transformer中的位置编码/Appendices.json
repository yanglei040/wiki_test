{
    "hands_on_practices": [
        {
            "introduction": "在我们领会正弦位置编码的优势之前，必须先理解其内部构造。本练习将位置编码视为一个由不同频率组成的信号，旨在探索哪些频率成分对于表示各种类型的位置信息至关重要。通过这项消融研究 ，您将直观地理解位置编码是如何将位置信息分解为一种多尺度的表示。",
            "id": "3164200",
            "problem": "给定一个由整数 $p \\in \\{0,1,\\dots,N-1\\}$ 索引的位置序列，以及一种称为位置编码 (PE) 的实值特征表示。目标是对 PE 执行频带消融，并分析不同频带如何对重建特定于任务的位置目标函数做出贡献。您必须实现一个完整的程序，该程序基于正弦基构建 PE，应用消融掩码去除高频或低频分量，拟合线性预测器以重建目标函数，并为指定的测试套件报告归一化均方误差 (NMSE)。\n\n此问题的基础是信号的 Fourier 表示：不同角频率下的正弦函数构成了在均匀间隔上采样的周期函数的一个基。您必须从此基推导出 PE，而不是假设一个已知的 transformer 特定公式。\n\n要求：\n- 为序列长度 $N = 128$ 构建一个维度为 $D = 1 + 2K$ 的 PE，其中第一个分量是常数偏置项，其余 $2K$ 个分量由 $K$ 个正弦频带使用正弦和余弦函数生成。角度必须以弧度为单位。频率必须在区间 $[\\omega_{\\min}, \\omega_{\\max}]$ 内对数间隔分布，其中 $K = 8$，$\\omega_{\\min} = \\frac{2\\pi}{N}$，$\\omega_{\\max} = \\pi$。将这些角频率表示为 $\\{\\omega_k\\}_{k=0}^{K-1}$。\n- 定义以下关于位置的目标函数 $h(p)$：\n    1. $h_{\\text{Lcos}}(p) = \\cos\\left(\\frac{2\\pi p}{64}\\right)$。\n    2. $h_{\\text{Hcos}}(p) = \\cos\\left(\\frac{2\\pi p}{4}\\right)$。\n    3. $h_{\\text{Window}}(p) = \\mathbb{1}\\{|p - 64| \\le 5\\}$，其中 $\\mathbb{1}\\{\\cdot\\}$ 是指示函数。\n    4. $h_{\\text{Square8}}(p) = \\mathbb{1}\\{(p \\bmod 8)  4\\}$。\n- 对于消融，您必须对与特定频带关联的 PE 分量应用掩码。始终保留常数偏置项。对于截止频率为 $\\omega_c$ 的低通消融，保留所有 $\\omega_k \\le \\omega_c$ 的频率分量，并移除所有 $\\omega_k  \\omega_c$ 的分量。对于截止频率为 $\\omega_c$ 的高通消融，保留所有 $\\omega_k \\ge \\omega_c$ 的频率分量，并移除所有 $\\omega_k  \\omega_c$ 的分量。对于无消融（“none”），保留所有频率分量。\n- 拟合一个线性预测器 $f(p) = \\mathbf{x}(p)^\\top \\mathbf{w}$ 以最小化均方误差，其中 $\\mathbf{x}(p)$ 是消融后在位置 $p$ 处的 PE 向量。使用完整的位置集合 $p \\in \\{0,\\dots,N-1\\}$ 作为训练集。计算归一化均方误差 (NMSE)，定义为\n$$\n\\mathrm{NMSE} = \\frac{\\frac{1}{N}\\sum_{p=0}^{N-1}\\big(h(p) - f(p)\\big)^2}{\\mathrm{Var}\\big(h(p)\\big)},\n$$\n其中 $\\mathrm{Var}\\big(h(p)\\big)$ 是 $h(p)$ 在 $p=0,\\dots,N-1$ 上的方差。将 $\\mathrm{NMSE}$ 报告为浮点数。\n\n角度单位说明：所有角度量必须以弧度为单位。\n\n测试套件：\n您必须实现以下有序的测试用例列表。每个测试用例是一个三元组 $(\\text{ablation\\_type}, \\omega_c, \\text{task})$，其中 $\\text{ablation\\_type} \\in \\{\\text{\"none\"}, \\text{\"low\\_pass\"}, \\text{\"high\\_pass\"}\\}$，$\\omega_c$ 是一个以弧度为单位的非负实数，$\\text{task} \\in \\{\\text{\"Lcos\"}, \\text{\"Hcos\"}, \\text{\"Window\"}, \\text{\"Square8\"}\\}$ 映射到上面定义的目标函数。\n\n- 测试用例 1: $(\\text{\"none\"}, \\frac{\\pi}{2}, \\text{\"Lcos\"})$\n- 测试用例 2: $(\\text{\"low\\_pass\"}, 0.4\\pi, \\text{\"Hcos\"})$\n- 测试用例 3: $(\\text{\"high\\_pass\"}, 0.2\\pi, \\text{\"Hcos\"})$\n- 测试用例 4: $(\\text{\"low\\_pass\"}, 0.1\\pi, \\text{\"Window\"})$\n- 测试用例 5: $(\\text{\"high\\_pass\"}, 0.8\\pi, \\text{\"Lcos\"})$\n- 测试用例 6: $(\\text{\"low\\_pass\"}, 0.01, \\text{\"Lcos\"})$ (边界：几乎所有频带都被移除)\n- 测试用例 7: $(\\text{\"high\\_pass\"}, \\pi + 0.01, \\text{\"Hcos\"})$ (边界：几乎所有频带都被移除)\n- 测试用例 8: $(\\text{\"none\"}, \\frac{\\pi}{2}, \\text{\"Square8\"})$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含按上述测试用例顺序排列的 NMSE 结果，格式为方括号括起来的逗号分隔列表（例如，“[r1,r2,...,r8]”）。每个元素必须是浮点数。",
            "solution": "我们从信号的 Fourier 表示开始：对于不同的角频率 $\\omega$，正弦和余弦函数集 $\\{\\sin(\\omega p), \\cos(\\omega p)\\}$ 构成了一个基，可以表示在均匀间隔上采样的周期信号。对于离散位置 $p \\in \\{0,\\dots,N-1\\}$，任何足够平滑或周期性的目标函数都可以通过此类基函数的线性组合来近似。这是 Fourier 分析中一个经过充分检验的事实。\n\n基于原则的设计：\n1. 从第一性原理构建基：因为正弦和余弦函数在适当条件下在均匀样本上是正交的，所以它们可以作为表示位置信息的自然基。我们构建位置编码 (PE) 为一个向量 $\\mathbf{x}(p) \\in \\mathbb{R}^{D}$，它包含一个常数偏置项以及在对数间隔分布的角频率 $\\omega_k \\in [\\omega_{\\min}, \\omega_{\\max}]$ 上的正弦和余弦分量对，其中有 $K$ 个频带且 $D = 1 + 2K$。常数项使模型能够表示目标函数的均值。对数间隔跨越了多个数量级的位置尺度，提供了低频和高频的敏感性。\n\n2. 任务和频率内容：\n   - 对于 $h_{\\text{Lcos}}(p) = \\cos\\left(\\frac{2\\pi p}{64}\\right)$，主导频率是低频，角频率为 $\\omega = \\frac{2\\pi}{64}$。预计这将被低频分量捕获；移除高频分量应该不会有太大影响，但移除低频分量会降低重建质量。\n   - 对于 $h_{\\text{Hcos}}(p) = \\cos\\left(\\frac{2\\pi p}{4}\\right)$，主导频率是高频，角频率为 $\\omega = \\frac{2\\pi}{4} = \\frac{\\pi}{2}$。在该值以下的截止频率进行低通消融将移除关键分量，而保留高于某个较低截止频率的高通消融将保留它。\n   - 对于 $h_{\\text{Window}}(p) = \\mathbb{1}\\{|p - 64| \\le 5\\}$，这个目标是局部的和非平滑的（一个阶跃）。需要高频来模拟急剧的转变；低通消融会使其模糊并增加误差。\n   - 对于 $h_{\\text{Square8}}(p) = \\mathbb{1}\\{(p \\bmod 8)  4\\}$，这是一个周期为 8、占空比为 50% 的方波。其 Fourier 级数包含一个常数项加上高频的奇次谐波。移除高频会降低重建质量；保留它们则有帮助。\n\n3. 线性投影和最优性：给定一个设计矩阵 $X \\in \\mathbb{R}^{N \\times D}$，其行是 PE 向量 $\\mathbf{x}(p)$，最小化 $\\sum_p \\big(h(p) - \\mathbf{x}(p)^\\top \\mathbf{w}\\big)^2$ 的最小二乘 (LS) 解 $\\hat{\\mathbf{w}}$ 是目标向量 $\\mathbf{h}$ 在 $X$ 的列空间上的正交投影。就均方误差而言，该投影在基的所有线性组合中是最优的。因此，移除基列的消融操作会限制子空间，并且只会增加误差或使其保持不变。\n\n4. 归一化误差：归一化均方误差 (NMSE) 定义为\n$$\n\\mathrm{NMSE} = \\frac{\\frac{1}{N}\\sum_{p=0}^{N-1}\\big(h(p) - f(p)\\big)^2}{\\mathrm{Var}\\big(h(p)\\big)},\n$$\n对于确定性数据的线性回归，这等于 $1 - R^2$。当预测器至少包含一个常数项且目标具有非零方差时，其值在 $[0,1]$ 范围内。如果所有正弦分量都被消融，LS 解将简化为等于 $h$ 均值的常数预测器，从而得到 $\\mathrm{NMSE} \\approx 1$。\n\n算法步骤：\n- 定义 $N = 128$，$K = 8$，$\\omega_{\\min} = \\frac{2\\pi}{N}$，$\\omega_{\\max} = \\pi$，并在 $[\\omega_{\\min}, \\omega_{\\max}]$ 中生成对数间隔分布的 $\\{\\omega_k\\}_{k=0}^{K-1}$。\n- 对每个位置 $p$，构建 PE 向量 $\\mathbf{x}(p)$，包含：\n  - 一个常数项 $1$。\n  - 对每个 $\\omega_k$，追加 $\\sin(\\omega_k p)$ 和 $\\cos(\\omega_k p)$。\n- 实现消融：\n  - 低通消融（截止频率 $\\omega_c$）：保留与 $\\omega_k \\le \\omega_c$ 相关的分量对。\n  - 高通消融（截止频率 $\\omega_c$）：保留与 $\\omega_k \\ge \\omega_c$ 相关的分量对。\n  - 无消融：保留所有分量对。\n  - 始终保留常数项。\n- 对每个目标函数 $h$，构建向量 $\\mathbf{h} \\in \\mathbb{R}^{N}$。\n- 使用稳健的数值方法（例如，最小二乘求解器）解决 LS 问题 $\\min_{\\mathbf{w}} \\|\\mathbf{h} - X\\mathbf{w}\\|_2^2$，得到 $\\hat{\\mathbf{w}}$ 和预测值 $\\hat{\\mathbf{h}} = X\\hat{\\mathbf{w}}$。\n- 根据上述公式计算 $\\mathrm{NMSE}$。\n- 运行指定的测试套件并输出 NMSEs。\n\n对测试套件的定性预期：\n- 测试用例 1 (none, Lcos)：低频余弦应该能用完整基很好地重建；预期 NMSE 较低。\n- 测试用例 2 (low-pass $0.4\\pi$, Hcos)：截止频率 $\\omega_c = 0.4\\pi$ 移除了高于 1.2566 的分量，而目标频率为 $\\frac{\\pi}{2} \\approx 1.5708$；预期 NMSE 较高。\n- 测试用例 3 (high-pass $0.2\\pi$, Hcos)：保留了高于 0.6283 的频率，包括 $\\frac{\\pi}{2}$；预期 NMSE 较低。\n- 测试用例 4 (low-pass $0.1\\pi$, Window)：仅保留非常低的分量；无法表示急剧的边缘；预期 NMSE 较高。\n- 测试用例 5 (high-pass $0.8\\pi$, Lcos)：仅保留非常高的分量，移除了低频分量；预期 NMSE 较高。\n- 测试用例 6 (low-pass $0.01$, Lcos)：截止频率低于 $\\omega_{\\min}$，因此只剩下常数项；预期 NMSE 接近 1。\n- 测试用例 7 (high-pass $\\pi + 0.01$, Hcos)：截止频率高于 $\\omega_{\\max}$，因此只剩下常数项；预期 NMSE 接近 1。\n- 测试用例 8 (none, Square8)：完整基允许通过谐波来近似方波；预期 NMSE 为中等水平（由于 $K$ 是有限的，所以不为零）。\n\n程序将确定性地实现这些步骤，并按要求输出 NMSE。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_frequencies(N: int, K: int) -> np.ndarray:\n    # Logarithmically spaced angular frequencies in radians.\n    omega_min = 2 * np.pi / N\n    omega_max = np.pi\n    return np.geomspace(omega_min, omega_max, K)\n\ndef positional_encoding_matrix(N: int, omegas: np.ndarray) -> np.ndarray:\n    # Build PE matrix with shape (N, 1 + 2*K): [bias, sin(ω_k p), cos(ω_k p)]\n    K = len(omegas)\n    D = 1 + 2 * K\n    X = np.zeros((N, D), dtype=np.float64)\n    # Bias term\n    X[:, 0] = 1.0\n    # Sin/Cos terms\n    p = np.arange(N, dtype=np.float64)\n    for idx, omega in enumerate(omegas):\n        X[:, 1 + 2 * idx] = np.sin(omega * p)\n        X[:, 1 + 2 * idx + 1] = np.cos(omega * p)\n    return X\n\ndef apply_ablation(X: np.ndarray, omegas: np.ndarray, ablation_type: str, omega_c: float) -> np.ndarray:\n    # Always keep bias column 0\n    cols = [0]\n    K = len(omegas)\n    if ablation_type == \"none\":\n        # Keep all sin/cos pairs\n        for idx in range(K):\n            cols.extend([1 + 2 * idx, 1 + 2 * idx + 1])\n    elif ablation_type == \"low_pass\":\n        for idx, omega in enumerate(omegas):\n            if omega = omega_c:\n                cols.extend([1 + 2 * idx, 1 + 2 * idx + 1])\n    elif ablation_type == \"high_pass\":\n        for idx, omega in enumerate(omegas):\n            if omega >= omega_c:\n                cols.extend([1 + 2 * idx, 1 + 2 * idx + 1])\n    else:\n        raise ValueError(f\"Unknown ablation type: {ablation_type}\")\n    return X[:, cols]\n\ndef target_function(N: int, task: str) -> np.ndarray:\n    p = np.arange(N, dtype=np.float64)\n    if task == \"Lcos\":\n        # Low-frequency cosine, period 64\n        return np.cos(2 * np.pi * p / 64.0)\n    elif task == \"Hcos\":\n        # High-frequency cosine, period 4\n        return np.cos(2 * np.pi * p / 4.0)\n    elif task == \"Window\":\n        # Indicator of window centered at 64 with radius 5\n        return (np.abs(p - 64) = 5).astype(np.float64)\n    elif task == \"Square8\":\n        # Square wave of period 8, 50% duty cycle\n        return ((p % 8)  4).astype(np.float64)\n    else:\n        raise ValueError(f\"Unknown task: {task}\")\n\ndef nmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    mse = np.mean((y_true - y_pred) ** 2)\n    var = np.var(y_true)\n    # var should be > 0 for given tasks\n    return float(mse / var)\n\ndef solve():\n    # Define constants\n    N = 128\n    K = 8\n    omegas = build_frequencies(N, K)\n    X_full = positional_encoding_matrix(N, omegas)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"none\", np.pi / 2, \"Lcos\"),\n        (\"low_pass\", 0.4 * np.pi, \"Hcos\"),\n        (\"high_pass\", 0.2 * np.pi, \"Hcos\"),\n        (\"low_pass\", 0.1 * np.pi, \"Window\"),\n        (\"high_pass\", 0.8 * np.pi, \"Lcos\"),\n        (\"low_pass\", 0.01, \"Lcos\"),\n        (\"high_pass\", np.pi + 0.01, \"Hcos\"),\n        (\"none\", np.pi / 2, \"Square8\"),\n    ]\n\n    results = []\n    for ablation_type, omega_c, task in test_cases:\n        X = apply_ablation(X_full, omegas, ablation_type, omega_c)\n        y = target_function(N, task)\n        # Least squares fit\n        w, *_ = np.linalg.lstsq(X, y, rcond=None)\n        y_hat = X @ w\n        results.append(nmse(y, y_hat))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "正弦位置编码成功的一个关键原因在于其能够泛化至训练中未曾见过的序列长度。本练习将对函数式的、连续的正弦编码与离散的、基于查找表的学习式编码进行直接比较。通过观察它们在外推任务  上的表现，您将亲眼见证为何其中一种方法在结构上更擅长应对这一挑战。",
            "id": "3100282",
            "problem": "您的任务是构建一个原则性比较，旨在比较确定性正弦位置编码和学习式位置编码在一个将位置信息与内容分离的简化模型中，在训练序列长度之外进行外推时的表现。该设置模拟了位置编码如何为线性解码器提供特征，这是对基于注意力的模型如何使用位置信号的最小化抽象。您必须从第一性原理推导误差行为，并实现一个程序来计算两种编码类型的外推误差。\n\n此任务的基础是：\n- 自注意力的定义，它需要位置信息来解析顺序，以及位置编码将依赖于位置的特征注入到模型计算中的概念。\n- 经过充分检验的普通最小二乘法 (OLS)，它通过最小化残差平方和来拟合从特征向量到标量目标的线性映射。\n- 均方根误差 (RMSE) 的定义，它量化了在一个集合上预测与目标之间的差异。\n\n序列由离散位置 $p \\in \\{1,2,\\dots\\}$ 索引，并有一个由正弦分量叠加定义的标量目标 $y(p)$，所有角度计算均以弧度表示。具体而言，对所有位置 $p$，定义\n$$\ny(p) = \\sin(\\omega_1 p) + \\tfrac{1}{2}\\cos(\\omega_2 p),\n$$\n其中 $\\omega_1 = \\tfrac{2\\pi}{16}$ 且 $\\omega_2 = \\tfrac{2\\pi}{32}$。\n\n您必须实现两种位置编码策略，为每个位置 $p$ 生成特征向量 $x(p)$：\n\n- 正弦位置编码：使用一个维度为 $d$ 的偶数维特征向量，通过拼接在固定角频率下的 $\\sin$ 和 $\\cos$ 函数对来构成。该特征向量包含一系列角频率 $\\{\\omega_k\\}$ 对应的 $(\\sin(\\omega_k p), \\cos(\\omega_k p))$ 对，该频率集包括 $\\omega_1$ 和 $\\omega_2$，以及其他不同的频率，以确保在训练位置上具有满列秩。这些特征的确切构造和缩放是您实现的一部分，但它们必须是 $p$ 的确定性函数，而不是从数据中学到的。\n\n- 学习式位置编码：对于位置 $p \\in \\{1,\\dots,N_{\\text{train}}\\}$，使用长度为 $N_{\\text{train}}$ 的独热特征向量。对于外推到 $p  N_{\\text{train}}$ 的情况，使用钳位规则，即所有位置都映射到索引为 $N_{\\text{train}}$ 的独热向量。这模拟了学习式嵌入表的常见行为，即在没有明确扩展的情况下无法为未见过的位置生成嵌入。\n\n训练使用普通最小二乘法 (OLS) 进行：给定训练位置 $p \\in \\{1,2,\\dots,N_{\\text{train}}\\}$，从 $x(p)$ 构建设计矩阵 $X \\in \\mathbb{R}^{N_{\\text{train}} \\times d}$，并从 $y(p)$ 构建目标向量 $y \\in \\mathbb{R}^{N_{\\text{train}}}$。通过最小化训练目标来拟合一个线性解码器 $w \\in \\mathbb{R}^{d}$：\n$$\n\\min_{w} \\sum_{p=1}^{N_{\\text{train}}} \\left( y(p) - w^\\top x(p) \\right)^2.\n$$\n使用拟合的 $w$ 为位置 $p  N_{\\text{train}}$ 生成预测 $\\hat{y}(p) = w^\\top x(p)$。\n\n对于任何整数 $n \\ge N_{\\text{train}}$，将外推误差 $E(n)$ 定义为在超出训练范围的位置上的均方根误差 (RMSE)：\n$$\nE(n) =\n\\begin{cases}\n0,   \\text{if } n = N_{\\text{train}},\\\\\n\\sqrt{\\dfrac{1}{n - N_{\\text{train}}} \\sum_{p=N_{\\text{train}}+1}^{n} \\left( y(p) - \\hat{y}(p) \\right)^2},  \\text{if } n  N_{\\text{train}}.\n\\end{cases}\n$$\n\n您的程序必须：\n- 使用训练最大值 $N_{\\text{train}} = 64$ 和正弦编码维度 $d = 16$（偶数）。\n- 使用上述的 $y(p)$，其中 $\\omega_1 = \\tfrac{2\\pi}{16}$ 和 $\\omega_2 = \\tfrac{2\\pi}{32}$，所有角度计算均以弧度为单位。\n- 构建正弦位置编码，其频率集包含 $\\omega_1$ 和 $\\omega_2$ 以及其他不同的正角频率。\n- 构建学习式位置编码，作为长度为 $N_{\\text{train}}$ 的独热向量，并对 $p  N_{\\text{train}}$ 应用钳位规则。\n- 对每种编码在 $p \\in \\{1,2,\\dots,N_{\\text{train}}\\}$ 上分别通过 OLS 拟合 $w$。\n\n测试套件：\n- 使用以下 $n$ 值：$64$、$80$、$128$、$160$。\n- 此套件包括边界 $n = N_{\\text{train}}$（根据定义，此时 $E(n) = 0$）、一个中等外推长度以及几个较大的外推长度，以探测持续性能。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。该列表应按顺序包含每个测试用例的正弦编码和学习式编码的 RMSE 值，汇总所有测试：\n$$\n[\\;E_{\\text{sin}}(64),\\;E_{\\text{learn}}(64),\\;E_{\\text{sin}}(80),\\;E_{\\text{learn}}(80),\\;E_{\\text{sin}}(128),\\;E_{\\text{learn}}(128),\\;E_{\\text{sin}}(160),\\;E_{\\text{learn}}(160)\\;].\n$$\n所有角度计算必须以弧度为单位，输出必须是实值浮点数。",
            "solution": "用户希望在一个简化的线性回归设置中，分析并实现对正弦位置编码和学习式位置编码在外推性能上的比较。\n\n### 第1步：提取给定信息\n- **目标函数**：$y(p) = \\sin(\\omega_1 p) + \\tfrac{1}{2}\\cos(\\omega_2 p)$，其中 $p \\in \\{1,2,\\dots\\}$ 为整数位置。\n- **角频率**：$\\omega_1 = \\tfrac{2\\pi}{16}$ 和 $\\omega_2 = \\tfrac{2\\pi}{32}$。所有角度计算必须以弧度为单位。\n- **训练范围**：位置 $p \\in \\{1, 2, \\dots, N_{\\text{train}}\\}$。\n- **训练集大小**：$N_{\\text{train}} = 64$。\n- **训练方法**：普通最小二乘法 (OLS)，最小化 $\\sum_{p=1}^{N_{\\text{train}}} ( y(p) - w^\\top x(p) )^2$。\n- **正弦编码**：\n    - 特征维度 $d = 16$。\n    - 特征是 $(\\sin(\\omega_k p), \\cos(\\omega_k p))$ 对。\n    - 频率集合 $\\{\\omega_k\\}$ 必须包含 $\\omega_1$ 和 $\\omega_2$。\n- **学习式编码**：\n    - 对于 $p \\in \\{1, \\dots, N_{\\text{train}}\\}$，$x(p)$ 是一个长度为 $N_{\\text{train}}$ 的独热向量。\n    - 对于 $p  N_{\\text{train}}$，使用钳位规则：$x(p) = x(N_{\\text{train}})$。\n- **外推误差度量**：$E(n)$，即在位置 $\\{N_{\\text{train}}+1, \\dots, n\\}$ 上的均方根误差 (RMSE)。\n  $$\n  E(n) =\n  \\begin{cases}\n  0,   \\text{if } n = N_{\\text{train}},\\\\\n  \\sqrt{\\dfrac{1}{n - N_{\\text{train}}} \\sum_{p=N_{\\text{train}}+1}^{n} \\left( y(p) - \\hat{y}(p) \\right)^2},  \\text{if } n  N_{\\text{train}}.\n  \\end{cases}\n  $$\n- **测试套件**：为 $n \\in \\{64, 80, 128, 160\\}$ 计算 $E(n)$。\n- **要求输出**：一个单行列表：`[E_sin(64), E_learn(64), E_sin(80), E_learn(80), E_sin(128), E_learn(128), E_sin(160), E_learn(160)]`。\n\n### 第2步：使用提取的给定信息进行验证\n对问题进行严格验证。\n\n- **科学依据**：该问题基于统计学习（普通最小二乘法）、数值分析（RMSE）以及现代深度学习概念（Transformer模型中的位置编码）等公认的原理。目标函数是一个简单、行为良好的数学函数。该设置是分析特征表示的一种有效且常用的技术。\n- **适定性**：该问题是适定的。只要设计矩阵是满列秩的，OLS解就是唯一的。对于正弦编码，问题明确指示要选择能确保这一点的频率。对于学习式编码，训练集上的设计矩阵是单位矩阵，是满秩的。外推规则定义明确。对于这两种情况，权重 $w$ 和随后的预测 $\\hat{y}(p)$ 都存在唯一解。\n- **客观性**：问题以精确、客观的数学语言陈述。\n- **完整性**：所有必要的参数（$N_{\\text{train}}$, $d$, $\\omega_1$, $\\omega_2$）和定义都已提供。构建正弦频率的规范允许有原则的选择（例如，包含指定周期的周期几何级数），使问题完全确定。\n- **一致性**：定义和约束在内部是一致的。向量和矩阵的维度是兼容的。\n\n该问题没有任何无效性缺陷。这是一个形式化、有科学依据且适定的问题。\n\n### 第3步：结论与行动\n该问题是**有效的**。将提供一个解决方案。\n\n### 理论框架与推导\n问题的核心是使用两种不同的特征映射 $x(p)$ 将线性模型 $\\hat{y}(p) = w^\\top x(p)$ 拟合到目标函数 $y(p)$，并比较它们的外推性能。模型参数 $w$ 是通过在训练集 $p \\in \\{1, \\dots, N_{\\text{train}}\\}$ 上使用普通最小二乘法 (OLS) 找到的。\n\n令 $X \\in \\mathbb{R}^{N_{\\text{train}} \\times d}$ 为设计矩阵，其行是特征向量 $x(p)^\\top$，其中 $p=1, \\dots, N_{\\text{train}}$。令 $y_{\\text{train}} \\in \\mathbb{R}^{N_{\\text{train}}}$ 为训练集上的目标值向量 $y(p)$。权重向量 $w$ 的 OLS 解由正规方程给出：\n$$\nw = (X^\\top X)^{-1} X^\\top y_{\\text{train}}\n$$\n如果 $X^\\top X$ 可逆，即 $X$ 是满列秩的，则该解存在且唯一。\n\n#### 1. 正弦位置编码\n此策略旨在使用周期函数基来表示位置。\n- **特征向量构造**：特征向量 $x_{\\text{sin}}(p)$ 的维度为 $d=16$。它通过拼接 $d/2 = 8$ 对正弦和余弦函数构成：\n$$\nx_{\\text{sin}}(p) = [\\sin(\\Omega_1 p), \\cos(\\Omega_1 p), \\sin(\\Omega_2 p), \\cos(\\Omega_2 p), \\dots, \\sin(\\Omega_8 p), \\cos(\\Omega_8 p)]^\\top\n$$\n问题要求角频率集合 $\\{\\Omega_k\\}_{k=1}^8$ 包含 $\\omega_1=\\frac{2\\pi}{16}$ 和 $\\omega_2=\\frac{2\\pi}{32}$。我们选择一组有原则的频率，对应于周期的几何级数，并确保包含所需的频率。一组合适的周期是 $\\{8, 16, 32, 64, 128, 256, 512, 1024\\}$。对应的角频率是 $\\Omega_k = 2\\pi/T_k$。这个集合是不同的，并且跨越了很宽的范围。\n\n- **外推行为**：目标函数是 $y(p) = \\sin(\\omega_1 p) + \\frac{1}{2}\\cos(\\omega_2 p)$。由于 $\\omega_1$ 和 $\\omega_2$ 包含在我们的基频率中，真实函数 $y(p)$ 是用于构造 $x_{\\text{sin}}(p)$ 的基函数的线性组合。具体来说，它对应于一个权重向量，其中 $\\sin(\\omega_1 p)$ 的系数为 $1$，$\\cos(\\omega_2 p)$ 的系数为 $0.5$，所有其他系数均为 $0$。虽然正弦基函数在有限、离散的训练区间 $\\{1, \\dots, 64\\}$ 上并非完全正交，但 OLS 旨在找到最佳线性近似。它将找到一个权重向量 $w_{\\text{sin}}$，使得模型 $\\hat{y}(p) = w_{\\text{sin}}^\\top x_{\\text{sin}}(p)$ 能够非常接近真实函数。因为该模型具有真实数据生成过程的函数形式，这种近似不仅对训练点成立，也对外推点 $p  N_{\\text{train}}$ 成立。因此，外推误差 $E_{\\text{sin}}(n)$ 预计会非常小，主要受限于数值精度和由于在有限训练集上基的非正交性引起的微小拟合误差。\n\n#### 2. 学习式位置编码\n此策略模拟了为训练集中的每个位置独立学习的位置嵌入。\n- **特征向量构造**：特征维度为 $d = N_{\\text{train}} = 64$。对于一个训练位置 $p \\in \\{1, \\dots, 64\\}$，特征向量 $x_{\\text{learn}}(p)$ 是一个独热向量，其中第 $p$ 个元素为 $1$，其余均为 $0$。因此，设计矩阵 $X_{\\text{learn}}$ 是 $64 \\times 64$ 的单位矩阵 $I_{64}$。\n\n- **训练**：权重的 OLS 解为：\n$$\nw_{\\text{learn}} = (X_{\\text{learn}}^\\top X_{\\text{learn}})^{-1} X_{\\text{learn}}^\\top y_{\\text{train}} = (I_{64}^\\top I_{64})^{-1} I_{64}^\\top y_{\\text{train}} = y_{\\text{train}}\n$$\n这意味着该模型只是简单地记住了每个训练位置的目标值。权重向量 $w_{\\text{learn}}$ 与训练目标向量完全相同。\n\n- **外推行为**：问题为外推指定了一个钳位规则：对于任何 $p  N_{\\text{train}}$，特征向量与最后一个训练位置的特征向量相同，即 $x_{\\text{learn}}(p) = x_{\\text{learn}}(N_{\\text{train}})$。这是对应于位置 $N_{\\text{train}}=64$ 的独热向量。任何此类位置的预测是：\n$$\n\\hat{y}_{\\text{learn}}(p) = w_{\\text{learn}}^\\top x_{\\text{learn}}(N_{\\text{train}})\n$$\n由于 $w_{\\text{learn}, i} = y(i)$（暂时使用1-based索引）并且 $x_{\\text{learn}}(N_{\\text{train}})$ 仅在第 $N_{\\text{train}}$ 个位置为 $1$，点积会选出 $w_{\\text{learn}}$ 的最后一个元素。\n$$\n\\hat{y}_{\\text{learn}}(p) = y(N_{\\text{train}}) \\quad \\text{for all } p  N_{\\text{train}}\n$$\n因此，该模型对所有未见过的位置做出恒定的预测，该预测值等于其训练经验边缘处的值。而真实函数 $y(p)$ 在 $p64$ 时继续振荡，因此误差 $y(p) - y(64)$ 将会很大。RMSE $E_{\\text{learn}}(n)$ 将会累积这些大的误差，预计将显著高于正弦编码。\n\n### 外推误差 $E(n)$ 的计算\n对于每个测试值 $n  N_{\\text{train}}$：\n1. 定义外推位置集合 $P_{\\text{extrap}} = \\{N_{\\text{train}}+1, \\dots, n\\}$。\n2. 为所有 $p \\in P_{\\text{extrap}}$ 生成真实值 $y(p)$。\n3. 为所有 $p \\in P_{\\text{extrap}}$ 生成预测值 $\\hat{y}_{\\text{sin}}(p)$ 和 $\\hat{y}_{\\text{learn}}(p)$。\n4. 计算每种情况的 RMSE：\n   $$\n   E(n) = \\sqrt{\\frac{1}{n - N_{\\text{train}}} \\sum_{p \\in P_{\\text{extrap}}} (y(p) - \\hat{y}(p))^2}\n   $$\n对于边界情况 $n=N_{\\text{train}}$，两种方法的误差 $E(N_{\\text{train}})$ 都被定义为 $0$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the extrapolation error of sinusoidal and learned\n    positional encodings within a simplified linear regression framework.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    N_train = 64\n    d_sin = 16\n    omega1 = 2 * np.pi / 16\n    omega2 = 2 * np.pi / 32\n    test_n_values = [64, 80, 128, 160]\n\n    # --- Target Function ---\n    def target_function(p):\n        \"\"\"Computes the true scalar target y(p).\"\"\"\n        return np.sin(omega1 * p) + 0.5 * np.cos(omega2 * p)\n\n    # --- Training Data ---\n    # Use 1-based indexing for positions as in the problem description\n    train_positions = np.arange(1, N_train + 1)\n    y_train = target_function(train_positions)\n\n    # --- 1. Sinusoidal Positional Encoding ---\n\n    # Define frequencies for sinusoidal encoding. The set of periods includes\n    # 16 and 32 (corresponding to omega1 and omega2) plus others to ensure\n    # full rank and represent a range of frequencies.\n    periods = np.array([8, 16, 32, 64, 128, 256, 512, 1024])\n    sin_frequencies = 2 * np.pi / periods\n    \n    def get_sinusoidal_features(positions, freqs, dim):\n        \"\"\"Generates the sinusoidal feature matrix for a given set of positions.\"\"\"\n        num_pos = len(positions)\n        num_freqs = dim // 2\n        # Ensure freqs has the right length just in case\n        if len(freqs) != num_freqs:\n            raise ValueError(f\"Expected {num_freqs} frequencies for dimension {dim}, but got {len(freqs)}\")\n            \n        # Create a matrix of p*omega values\n        p_omega = positions[:, np.newaxis] * freqs[np.newaxis, :]\n        \n        # Create the feature matrix by interleaving sin and cos\n        X = np.zeros((num_pos, dim))\n        X[:, 0::2] = np.sin(p_omega)\n        X[:, 1::2] = np.cos(p_omega)\n        return X\n\n    # Fit the linear decoder for sinusoidal encoding\n    X_sin_train = get_sinusoidal_features(train_positions, sin_frequencies, d_sin)\n    w_sin, _, _, _ = np.linalg.lstsq(X_sin_train, y_train, rcond=None)\n\n    # --- 2. Learned Positional Encoding ---\n    # For learned one-hot encoding, the weight vector w_learn is simply the\n    # target vector y_train, as X_learn is the identity matrix.\n    # The extrapolation prediction is clamped to the value at N_train.\n    y_pred_learn_extrapol_val = target_function(N_train)\n\n    # --- Calculate Errors for Test Cases ---\n    results = []\n    for n in test_n_values:\n        if n == N_train:\n            # By definition, extrapolation error at n = N_train is 0.\n            results.append(0.0)  # E_sin(64)\n            results.append(0.0)  # E_learn(64)\n            continue\n        \n        # Extrapolation positions\n        extrapol_positions = np.arange(N_train + 1, n + 1)\n        num_extrapol_points = len(extrapol_positions)\n        \n        # True values for the extrapolation range\n        y_true_extrapol = target_function(extrapol_positions)\n        \n        # Calculate error for sinusoidal encoding\n        X_sin_extrapol = get_sinusoidal_features(extrapol_positions, sin_frequencies, d_sin)\n        y_pred_sin_extrapol = X_sin_extrapol @ w_sin\n        error_sin = np.sqrt(np.mean((y_true_extrapol - y_pred_sin_extrapol)**2))\n        results.append(error_sin)\n\n        # Calculate error for learned encoding\n        # The prediction is a constant value for all extrapolation points\n        y_pred_learn_extrapol = np.full(num_extrapol_points, y_pred_learn_extrapol_val)\n        error_learn = np.sqrt(np.mean((y_true_extrapol - y_pred_learn_extrapol)**2))\n        results.append(error_learn)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "位置编码并非孤立运作，它与内容嵌入相加，并由自注意力机制进行处理。本练习旨在剥离出位置编码的独立作用，并探究其数值大小（即幅度）如何影响注意力分数的行为。您将学习到不当的缩放如何导致注意力“弥散”为均匀分布或“坍塌”至单个位置 ，从而揭示模型调优与稳定性的一个关键环节。",
            "id": "3180897",
            "problem": "您的任务是构建一个程序，研究缩放点积注意力机制如何依赖于位置编码的幅度。目标是量化当位置编码的幅度变化时，注意力图是坍缩（变得过度集中）还是弥散（变得过度均匀）。分析必须纯粹以数学和算法术语表达，不依赖于经验数据集，并应基于深度学习中广泛使用的核心定义。\n\n起点（核心定义）：\n- 缩放点积注意力 (SDPA)：给定查询 $Q$、密钥 $K$ 和值 $V$，注意力权重是通过对由归一化内积形成的分数矩阵按行应用 $softmax$ 函数来计算的。令 $d_k$ 表示密钥维度。归一化分数矩阵是在应用 $softmax$ 之前除以 $\\sqrt{d_k}$ 形成的。\n- 正弦位置编码 (PE)：对于长度为 $L$ 且模型维度为 $d$ 的序列，位置编码矩阵 $PE \\in \\mathbb{R}^{L \\times d}$ 为每个位置 $p \\in \\{0,\\dots,L-1\\}$ 分配一个确定性向量，该向量在具有几何级数增长波长的正弦和余弦函数之间交替。此矩阵是固定的，不依赖于输入内容。\n- 幅度缩放：为分离位置编码的作用，对所有位置 $p$ 使用恒为零的内容向量 $x_p \\in \\mathbb{R}^d$。位置 $p$ 处的有效输入嵌入为 $e_p = x_p + a \\cdot PE_p = a \\cdot PE_p$，其中 $a \\ge 0$ 是一个标量幅度。查询 $Q$ 和密钥 $K$ 是通过嵌入与固定矩阵的线性投影形成的，注意力权重则使用 SDPA 计算。\n\n您的程序必须：\n1. 为长度为 $L$ 且模型维度为偶数 $d$ 的序列构建一个正弦位置编码 $PE \\in \\mathbb{R}^{L \\times d}$，使用标准的交替正弦和余弦设计，并采用对数间隔的频率。使用 $L = 16$ 和 $d = 32$。\n2. 将所有位置 $p \\in \\{0, \\dots, L-1\\}$ 的内容嵌入 $x_p$ 设置为零向量，使得 $e_p = a \\cdot PE_p$。\n3. 固定随机种子 $s = 7$，并从标准正态分布中采样两个独立矩阵 $W_Q \\in \\mathbb{R}^{d \\times d}$ 和 $W_K \\in \\mathbb{R}^{d \\times d}$。定义 $Q = E W_Q$ 和 $K = E W_K$，其中 $E \\in \\mathbb{R}^{L \\times d}$ 是将嵌入 $e_p$ 按行堆叠而成的矩阵。\n4. 使用缩放点积注意力计算注意力权重矩阵 $A \\in \\mathbb{R}^{L \\times L}$：将 $softmax$ 函数按行应用于由 $Q$ 和 $K$ 的内积形成、并除以 $\\sqrt{d_k}$（其中 $d_k = d$）的分数矩阵。\n5. 对每一行 $A_{p,:}$，使用自然对数计算香农熵 $H_p = -\\sum_{j=0}^{L-1} A_{p,j} \\log(A_{p,j})$。同时计算最大注意力权重 $M_p = \\max_{j} A_{p,j}$。通过计算平均值 $\\bar{H} = \\frac{1}{L} \\sum_{p=0}^{L-1} H_p$ 和 $\\bar{M} = \\frac{1}{L} \\sum_{p=0}^{L-1} M_p$ 来对这些值进行聚合。\n6. 对以下测试套件中的每个幅度 $a$ 重复步骤 2 到 5：\n   - 边界和近边界情况：$a = 0.0$, $a = 10^{-6}$。\n   - 低到中等幅度情况：$a = 0.05$, $a = 0.5$, $a = 1.0$。\n   - 高幅度情况：$a = 5.0$, $a = 20.0$。\n7. 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果。每个元素都应是对应测试幅度的双元素列表 $[\\bar{H}, \\bar{M}]$，顺序与上文给出的一致。例如，输出格式必须严格为 $[[h_1,m_1],[h_2,m_2],\\dots,[h_7,m_7]]$，其中每个 $h_i$ 和 $m_i$ 都是浮点数。\n\n科学真实性与覆盖设计：\n- 情况 $a = 0.0$ 会强制 $Q = 0$ 和 $K = 0$，从而产生一个具有最大熵和最小最大权重的均匀注意力图，这可作为一个边界条件。\n- 非常小的 $a$ 值会使分数接近于零，用于测试数值稳定性和近乎均匀的弥散情况。\n- 大的 $a$ 值会通过投影放大位置差异，用于测试 softmax 饱和、注意力图的潜在坍缩以及熵的降低。\n\n最终输出：\n- 程序必须输出单行内容，按指定格式包含七对浮点数。不应打印任何额外文本。",
            "solution": "问题陈述已经过严格验证，并被认为是有效的。它在科学上基于深度学习的原理，特别是 Transformer 架构的注意力机制。该问题是良构的，提供了一套完整且一致的定义、参数和约束，从而能够得到一个唯一的、确定性的、可验证的解。该实验设计是一种健全且常规的方法，用于分离和分析特定模型组件的功能作用——在本例中，即正弦位置编码。\n\n问题的核心是研究缩放点积注意力 (SDPA) 在位置信息幅度变化下的行为，其中内容信息被有意置零。这是通过将位置 $p$ 处的输入嵌入 $e_p$ 设置为位置编码向量 $PE_p$ 的缩放版本来实现的，即 $e_p = a \\cdot PE_p$，其中 $a \\geq 0$ 是一个标量幅度。\n\n首先，我们为长度 $L=16$、模型维度 $d=32$ 的序列构建正弦位置编码矩阵 $PE \\in \\mathbb{R}^{L \\times d}$。对于每个位置 $p \\in \\{0, \\dots, L-1\\}$ 和维度索引 $k \\in \\{0, \\dots, d-1\\}$，$PE$ 的元素由标准的交替正弦和余弦函数定义：\n$$\nPE_{(p, 2i)} = \\sin\\left(\\frac{p}{10000^{2i/d}}\\right) \\quad \\text{for } i \\in \\{0, \\dots, d/2-1\\}\n$$\n$$\nPE_{(p, 2i+1)} = \\cos\\left(\\frac{p}{10000^{2i/d}}\\right) \\quad \\text{for } i \\in \\{0, \\dots, d/2-1\\}\n$$\n此公式创建了能够编码相对和绝对位置的唯一位置向量。\n\n用于注意力机制的输入嵌入是通过将向量 $e_p$ 堆叠成矩阵 $E \\in \\mathbb{R}^{L \\times d}$ 而形成的。然后，查询矩阵 $Q \\in \\mathbb{R}^{L \\times d}$ 和密钥矩阵 $K \\in \\mathbb{R}^{L \\times d}$ 分别通过这些嵌入与固定的权重矩阵 $W_Q \\in \\mathbb{R}^{d \\times d}$ 和 $W_K \\in \\mathbb{R}^{d \\times d}$ 进行线性投影生成。这些权重矩阵从标准正态分布中采样一次，并固定随机种子为 $s=7$ 以确保可复现性。\n$$\nE = a \\cdot PE\n$$\n$$\nQ = E W_Q\n$$\n$$\nK = E W_K\n$$\n注意力分数是使用缩放点积运算计算的。分数矩阵 $S \\in \\mathbb{R}^{L \\times L}$ 是通过计算 $Q$ 与 $K$ 的转置的矩阵乘积，并将结果按密钥维度（设置为 $d_k=d=32$）的平方根倒数进行缩放而得到的。\n$$\nS = \\frac{Q K^T}{\\sqrt{d_k}}\n$$\n注意力权重矩阵 $A \\in \\mathbb{R}^{L \\times L}$ 是通过对分数矩阵 $S$ 按行应用 softmax 函数获得的。对于每一行 $p$，位置对 $(p, j)$ 的注意力权重 $A_{p,j}$ 为：\n$$\nA_{p,j} = \\text{softmax}(S_{p,:})_j = \\frac{\\exp(S_{p,j})}{\\sum_{k=0}^{L-1} \\exp(S_{p,k})}\n$$\n当幅度 $a$ 为 $0$ 时，$E$、$Q$ 和 $K$ 均变为零矩阵。因此，分数矩阵 $S$ 也是一个零矩阵。对零向量应用 softmax 函数会得到一个均匀分布，因此每个注意力权重 $A_{p,j}$ 都变为 $1/L = 1/16$。这可作为完全弥散的注意力图的基线。相反，当 $a$ 变大时，$S$ 每一行分数的方差会增加，导致 softmax 函数饱和。这会引发一种“赢者通吃”的动态，即一行中的某个注意力权重趋近于 $1$，而其他权重趋近于 $0$，标志着注意力的坍缩。\n\n为了量化此行为，我们为注意力矩阵 $A$ 的每一行 $p$ 计算两个指标：\n1.  **香农熵** ($H_p$)，用于衡量注意力分布的不确定性或弥散度。更高的熵对应于更均匀的分布。\n    $$\n    H_p = -\\sum_{j=0}^{L-1} A_{p,j} \\log(A_{p,j})\n    $$\n2.  **最大注意力权重** ($M_p$)，用于衡量注意力的集中度。更高的最大权重表示一个更集中或坍缩的注意力分布。\n    $$\n    M_p = \\max_{j} A_{p,j}\n    $$\n然后将这些指标在所有 $L$ 行上取平均，以产生最终的聚合指标 $\\bar{H}$ 和 $\\bar{M}$。对指定的幅度套件 $a$：$\\{0.0, 10^{-6}, 0.05, 0.5, 1.0, 5.0, 20.0\\}$ 重复整个过程，从而可以系统地分析位置编码幅度的影响。代码将实现此过程，为每个幅度计算 $[\\bar{H}, \\bar{M}]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import softmax\n\ndef solve():\n    \"\"\"\n    Computes attention metrics for varying positional encoding amplitudes.\n    \"\"\"\n    \n    # Define parameters from the problem statement.\n    L = 16  # Sequence length\n    d = 32  # Model dimension\n    d_k = d # Key dimension\n    seed = 7  # Random seed\n    amplitudes = [0.0, 1e-6, 0.05, 0.5, 1.0, 5.0, 20.0]\n\n    # Initialize random number generator and create projection matrices\n    rng = np.random.default_rng(seed)\n    W_Q = rng.standard_normal(size=(d, d))\n    W_K = rng.standard_normal(size=(d, d))\n\n    def create_pe(length, dim):\n        \"\"\"\n        Constructs a sinusoidal positional encoding matrix.\n        \"\"\"\n        if dim % 2 != 0:\n            raise ValueError(\"Model dimension d must be even.\")\n        \n        position = np.arange(length, dtype=np.float64).reshape(-1, 1)\n        # Denominator term in the PE formula\n        div_term = np.exp(np.arange(0, dim, 2, dtype=np.float64) * -(np.log(10000.0) / dim))\n        \n        pe = np.zeros((length, dim), dtype=np.float64)\n        pe[:, 0::2] = np.sin(position * div_term)\n        pe[:, 1::2] = np.cos(position * div_term)\n        return pe\n\n    # Pre-compute the positional encoding matrix\n    pe_matrix = create_pe(L, d)\n\n    results = []\n    for a in amplitudes:\n        # Step 2: Construct embeddings\n        # Content embeddings x_p are zero, so E is just scaled PE\n        E = a * pe_matrix\n\n        # Step 3: Define Q and K matrices\n        Q = E @ W_Q\n        K = E @ W_K\n\n        # Step 4: Compute attention weights\n        # Score matrix S = (Q @ K^T) / sqrt(d_k)\n        scores = (Q @ K.T) / np.sqrt(d_k)\n        \n        # Row-wise softmax to get attention weights A\n        # softmax handles the a=0 case correctly (uniform distribution)\n        attention_weights = softmax(scores, axis=1)\n\n        # Step 5: Compute metrics\n        \n        # Entropy H_p for each row p\n        # Use `where` to avoid log(0), as A_pj * log(A_pj) is 0 if A_pj is 0.\n        entropy_per_row = -np.sum(attention_weights * np.log(attention_weights, \n                                                             where=attention_weights  0, \n                                                             out=np.zeros_like(attention_weights)), \n                                  axis=1)\n\n        # Max attention weight M_p for each row p\n        max_weight_per_row = np.max(attention_weights, axis=1)\n\n        # Aggregate by averaging over all L rows\n        avg_H = np.mean(entropy_per_row)\n        avg_M = np.mean(max_weight_per_row)\n\n        results.append([avg_H, avg_M])\n\n    # Final print statement in the exact required format.\n    # The string representation of a list is '[item1, item2]', which matches\n    # the inner list formatting requirement.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}