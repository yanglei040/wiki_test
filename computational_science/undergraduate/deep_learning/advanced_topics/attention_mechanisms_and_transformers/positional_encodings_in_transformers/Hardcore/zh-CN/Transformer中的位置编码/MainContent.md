## 引言
[Transformer架构](@entry_id:635198)凭借其强大的[自注意力机制](@entry_id:638063)，在众多人工智能领域取得了革命性的突破。[自注意力机制](@entry_id:638063)能够捕捉序列内任意两个元素之间的依赖关系，无论它们相距多远。然而，这一强大能力背后隐藏着一个根本性的缺陷：它对顺序不敏感。对于一个纯粹的[自注意力](@entry_id:635960)网络而言，“狗咬人”和“人咬狗”在词义组合上并无差异，因为它将输入视为一个无序的向量集合。这个知识空白严重限制了模型在处理语言、时间序列、基因组等顺序至关重要的数据时的能力。

为了解决这一问题，研究者们引入了“位置编码”（Positional Encoding）这一关键组件。本文将作为一份全面的指南，系统性地剖析位置编码。读者将通过本文学习到：

在“**原理与机制**”一章中，我们将深入探讨为何需要位置信息，并详细拆解绝对位置编码（如经典的正弦编码）与相对位置编码（如RoPE和ALiBi）的设计哲学与数学原理。在“**应用与跨学科连接**”一章中，我们将视野拓宽至自然语言处理、计算机视觉、[生物信息学](@entry_id:146759)等多个前沿领域，展示位置编码如何被巧妙地应用于解决真实世界中的复杂问题，例如长文本建模、图像结构识别和[生物序列](@entry_id:174368)分析。最后，在“**动手实践**”部分，一系列精心设计的编程练习将帮助您将理论知识转化为实践技能，加深对核心概念的直观理解。

## 原理与机制

在上一章中，我们介绍了 Transformer 架构的核心构建单元——[自注意力机制](@entry_id:638063)。我们了解到，[自注意力机制](@entry_id:638063)能够权衡输入序列中所有元素的重要性，从而为每个元素生成一个富含上下文信息的表示。然而，标准的[自注意力机制](@entry_id:638063)存在一个固有的、根本性的局限：它对序列中元素的顺序不敏感。如果我们打乱输入序列的顺序，[自注意力](@entry_id:635960)层的输出仅仅是相应地被打乱，而每个元素本身的表示（在[置换](@entry_id:136432)之前）将保持不变。从本质上讲，[自注意力机制](@entry_id:638063)将输入序列视为一个“集合”（set）而非一个有序的“序列”（sequence）。

然而，对于大多数数据，尤其是自然语言，顺序至关重要。“狗咬人”和“人咬狗”的含义截然不同。为了让 Transformer 能够理解和利用这种顺序信息，必须引入一种机制来将位置信息注入模型中。这个机制就是 **位置编码（Positional Encoding, PE）**。本章将深入探讨位置编码背后的核心原理、几种关键的实现机制、它们的内在属性以及在实际应用中的考量。

### 位置信息的必要性：[置换不变性](@entry_id:753356)

要深刻理解位置编码的必要性，我们首先需要精确地分析 Transformer 编码器在没有位置信息时的行为。一个标准的 Transformer 编码器层由一个[自注意力](@entry_id:635960)（SA）模块和一个位置无关的前馈网络（FFN）组成。关键在于，这些操作在所有位置上都是以相同的方式应用的——它们共享参数，并且不依赖于输入元素的位置索引。

考虑一个没有位置编码的 Transformer 编码器。输入是一个向量序列 $\mathbf{x} = (x_1, x_2, \dots, x_n)$。由于[自注意力机制](@entry_id:638063)计算的是序列中所有向量之间的成对交互，其操作本质上是**[置换](@entry_id:136432)等变的（permutation-equivariant）**。这意味着，如果我们将输入序列的顺序通过一个[置换](@entry_id:136432) $\pi$ 进行打乱，得到新的序列 $\pi \cdot \mathbf{x}$，那么编码器的输出 $\mathbf{h} = (h_1, h_2, \dots, h_n)$ 也会以完全相同的方式被[置换](@entry_id:136432)，即 $E(\pi \cdot \mathbf{x}) = \pi \cdot E(\mathbf{x})$，其中 $E$ 代表编码器。

许多任务，如文本分类，需要在最后将序列表示聚合成一个单一的向量。这通常通过一个池化（pooling）操作实现，例如对所有位置的输出向量求平均值，$h_{\text{pool}} = \frac{1}{n} \sum_{i=1}^n h_i$。由于加法是可交换的，这个[平均池化](@entry_id:635263)操作是**[置换](@entry_id:136432)不变的（permutation-invariant）**。将一个[置换](@entry_id:136432)等变的编码器和一个[置换](@entry_id:136432)不变的[池化层](@entry_id:636076)结合起来，得到的整个模型从输入序列到最终预测也是[置换](@entry_id:136432)不变的。

这意味着模型无法区分两个仅仅是顺序不同的序列。我们可以构造一个简单的反例来揭示这个问题的严重性 。假设我们有一个[二元分类](@entry_id:142257)任务，其目标是判断序列 $(a, b)$ 和 $(b, a)$，其中 $a$ 和 $b$ 是不同的向量。标签规则是：如果向量 $a$ 在 $b$ 之前，则标签为 $1$，否则为 $0$。对于序列 $\mathbf{x} = (a, b)$，标签为 $1$；对于序列 $\mathbf{x}' = (b, a)$，标签为 $0$。由于 $\mathbf{x}'$ 是 $\mathbf{x}$ 的一个[置换](@entry_id:136432)，一个没有位置编码的 Transformer 模型必然会对它们产生完全相同的输出，因此无法学习区分这两种情况。这个思想实验清楚地表明，任何依赖于顺序信息的任务都无法被一个纯粹的、无位置信息的 Transformer 解决。因此，向模型中注入位置信息是绝对必要的。

### 编码位置：绝对与相对

引入位置信息主要有两种哲学思想：**绝对位置编码（Absolute Positional Encoding）** 和 **相对位置编码（Relative Positional Encoding）**。绝对位置编码为序列中的每个绝对位置（如第1位、第2位……）分配一个唯一的编码向量。而相对位置编码则更关注于序列中两个位置之间的相对关系，例如它们之间的距离或方向。

#### 可学习的绝对位置编码

最直观的绝对位置编码方法是创建一个可学习的嵌入矩阵，类似于[词嵌入](@entry_id:633879)。我们可以定义一个大小为 $L_{\max} \times d$ 的位置嵌入矩阵，其中 $L_{\max}$ 是模型能处理的最大序列长度，$d$ 是模型维度。位置 $t$ 的编码就是这个矩阵的第 $t$ 行。这个位置编码向量随后被加到对应位置的[词嵌入](@entry_id:633879)上。

这种方法虽然简单，但存在一个严重的缺陷：**泛化能力差**。模型只在训练期间见过的位置（例如，从 $1$ 到 $512$）上学习了这些编码。当需要处理比训练时更长的序列时，模型不知道如何为新的、未见过的位置（如位置 $513$）生成编码。一种常见的处理方式是“钳位（clamping）”，即对所有超出范围的位置重复使用最后一个已知的编码（例如，位置 $512$ 的编码）。这种简单的外推策略通常效果不佳，因为模型无法理解新位置的相对关系，导致性能急剧下降。

#### [正弦位置编码](@entry_id:637792)

为了克服可学习编码的泛化问题，最初的 Transformer 论文提出了一种基于正弦和余弦函数的确定性位置编码方案。其思想是利用不同频率的三角函数来为每个位置创建一个唯一的、固定的编码向量。对于维度为 $d$ 的模型，位置 $t$ 的编码向量 $\mathrm{PE}(t) \in \mathbb{R}^d$ 的每个分量由以下公式定义：
$$
\mathrm{PE}(t)_{2i} = \sin(t / 10000^{2i/d})
$$
$$
\mathrm{PE}(t)_{2i+1} = \cos(t / 10000^{2i/d})
$$
其中 $i$ 是维度索引，从 $0$ 到 $d/2 - 1$。这个公式为每个位置生成了一个独特的编码。波长从 $2\pi$ 到 $10000 \cdot 2\pi$ 呈[几何级数](@entry_id:158490)增长，使得模型可以同时关注不同尺度的位置信息。

这种方法的一个显著优点是它可以自然地外推到任意长度的序列，因为[三角函数](@entry_id:178918)对任意实数输入都有定义。更重要的是，它隐含地编码了相对位置信息。尽管每个 $\mathrm{PE}(t)$ 是绝对位置的函数，但任意两个位置编码 $\mathrm{PE}(t)$ 和 $\mathrm{PE}(u)$ 之间的[点积](@entry_id:149019)，在特定条件下，可以表示为它们相对位移 $t-u$ 的函数。

让我们考虑一个简化的场景，其中查询和键的[投影矩阵](@entry_id:154479) $W_Q$ 和 $W_K$ 都是单位矩阵。在这种情况下，查询 $q_t = \mathrm{PE}(t)$，键 $k_u = \mathrm{PE}(u)$。它们之间的[点积](@entry_id:149019)（即未缩放的注意力分数）为：
$$
s_{t,u} = \mathrm{PE}(t)^\top \mathrm{PE}(u) = \sum_{i=0}^{d/2-1} \left( \sin(\omega_i t) \sin(\omega_i u) + \cos(\omega_i t) \cos(\omega_i u) \right)
$$
其中 $\omega_i = 1 / 10000^{2i/d}$。利用[三角恒等式](@entry_id:165065) $\cos(A-B) = \cos A \cos B + \sin A \sin B$，上述表达式可以简化为 ：
$$
s_{t,u} = \sum_{i=0}^{d/2-1} \cos(\omega_i(t-u))
$$
这个结果表明，在这种理想情况下，注意力分数仅依赖于相对位移 $t-u$。这一特性使得[注意力机制](@entry_id:636429)具有**平移不变性**：一个在位置 $t$ 的查询对周围位置的注意力模式，与一个在位置 $t+\Delta$ 的查询对相应平移后的周围位置的注意力模式是相同的。

然而，在实际的 Transformer 模型中，查询和键的[投影矩阵](@entry_id:154479) $W_Q$ 和 $W_K$ 是可学习的，并且通常不是单位矩阵。在这种情况下，注意力分数变为 $\mathrm{PE}(t)^\top W_Q^\top W_K \mathrm{PE}(u)$。这个表达式通常不再能简化为仅依赖于 $t-u$ 的函数 。它会变成一个关于 $t$ 和 $u$ 的复杂函数，混合了相对位置 $(t-u)$ 和绝对位置 $(t+u)$ 的信息。这削弱了正弦编码的相对位置特性，从而限制了其在外推到更长序列时的性能。此外，由于[三角函数](@entry_id:178918)的周期性，对于远超其最长波长的相对距离，正弦编码会产生**混叠（aliasing）**现象，即无法区分两个相差一个周期的不同距离 。

### 显式编码相对位置

为了解决绝对位置编码中相对位置信息表达不纯粹的问题，研究者们提出了一系列直接在[自注意力](@entry_id:635960)计算中显式编码相对位置的方法。这些方法的核心思想是修改注意力分数的计算，使其天然地、不受学习权重影响地依赖于相对位置。

#### 旋转位置嵌入 (RoPE)

**旋转位置嵌入（Rotary Position Embedding, RoPE）** 是一种优雅且高效的相对位置编码方法。它不通过加法将位置信息注入[词嵌入](@entry_id:633879)，而是通过旋转来修改查询（$q$）和键（$k$）向量。

其核心思想是将 $d$ 维的 $q$ 和 $k$ 向量视为 $d/2$ 个二维向量的拼接。对于每个二维[子空间](@entry_id:150286)，RoPE 根据向量的绝对位置 $t$ 将其旋转一个角度 $\theta_i(t) = \alpha_i t$。具体来说，对于一个内容向量 $\tilde{q}_t$，其在位置 $t$ 的最终查询向量 $q_t$ 是通过一个[旋转矩阵](@entry_id:140302) $R(t)$ 作用于 $\tilde{q}_t$ 得到的，即 $q_t = R(t)\tilde{q}_t$。同理，$k_u = R(u)\tilde{k}_u$。

神奇之处在于计算它们之间的[点积](@entry_id:149019)  ：
$$
q_t^\top k_u = (\tilde{q}_t^\top R(t)^\top) (R(u)\tilde{k}_u) = \tilde{q}_t^\top (R(t)^\top R(u)) \tilde{k}_u
$$
通过精心设计，[旋转矩阵](@entry_id:140302)具有属性 $R(t)^\top R(u) = R(u-t)$。因此，注意力分数简化为：
$$
q_t^\top k_u = \tilde{q}_t^\top R(u-t) \tilde{k}_u
$$
这个表达式清楚地表明，位置信息对注意力分数的影响完全取决于相对位移 $u-t$。这使得注意力机制天生就具备了平移不变性，为模型提供了强大的长度外推能力。例如，在一个需要识别周期性模式的任务中，RoPE 能够通过设置合适的旋转频率，准确地在相隔一个周期的位置上产生最大的注意力分数，无论绝对位置有多远 。

通过在[多头注意力机制](@entry_id:634192)的不同头中使用不同的旋转频率 $\alpha_i$，每个头可以专注于不同尺度（或频率）的相对位置关系，有的关注局部细节（高频），有的关注[长程依赖](@entry_id:181727)（低频），从而实现**频率专业化（frequency specialization）** 。

#### 线性偏置注意力 (ALiBi)

**线性偏置注意力（Attention with Linear Biases, ALiBi）** 提供了另一种更简单直接的相对位置编码思路。它完全不修改查询和键向量，即 $q_t$ 和 $k_u$ 仅包含内容信息。取而代之的是，在计算出的注意力分数上直接加上一个与距离相关的偏置项 ：
$$
s_{t,u} = \frac{q_t^\top k_u}{\sqrt{d}} - \alpha_h |t-u|
$$
这里的 $\alpha_h$ 是一个为每个[注意力头](@entry_id:637186)单独设定的、固定的（非学习的）标量斜率。这个偏置项直接惩罚距离，距离越远的两个位置，其注意力分数受到的负向偏置就越大。这种惩罚是线性的，并且只依赖于相对距离 $|t-u|$，因此它同样具有优秀的长度外推能力。ALiBi 的设计理念是，局部的、邻近的信息通常更重要，它通过一个简单的线性偏置强制施加了这种局部性[归纳偏置](@entry_id:137419)。

### 作为局部性感应偏置的位置编码

我们可以从另一个角度来理解位置编码：它们为原本无结构的[自注意力机制](@entry_id:638063)引入了关于**局部性（locality）**的**[归纳偏置](@entry_id:137419)（inductive bias）**。一个没有任何偏置的[自注意力](@entry_id:635960)模型在理论上可以关注序列中的任何位置。然而，在许多任务中，邻近的元素之间往往存在更强的关联。位置编码可以将这种先验知识注入模型。

我们可以通过一个思想实验来阐明这一点。假设我们用一组高斯[径向基函数](@entry_id:754004)（RBF）来构造位置编码，其中每个编码向量 $\phi_\sigma(t)$ 是一组以不同位置为中心的高斯“激活”的集合 。在这种编码下，两个位置 $i$ 和 $j$ 的注意力分数 $s_{i,j} \propto \phi_\sigma(i)^\top \phi_\sigma(j)$ 会随着它们之间距离 $|i-j|$ 的增大而平滑地减小。最终的注意力模式会非常接近于一个以查询位置为中心的高斯卷积核。这表明，通过选择合适的位置编码，[自注意力](@entry_id:635960)可以模拟传统卷积网络中的局部性操作。

实际上，大多数成功的相对位置编码方案都内在地偏好局部性。
- 对于**正弦编码**和 **RoPE**，由于它们通常包含低频（长波）分量，两个邻近位置 $i$ 和 $j$ 的编码向量之间的[点积](@entry_id:149019)（或旋转后的[点积](@entry_id:149019)）会比相距很远的位置更大，从而产生一种“软”的局部性偏置 。
- 对于 **ALiBi**，这种局部性偏置是“硬”的、显式的，通过一个与距离成正比的惩罚项直接实现。

### 实践考量：掩码与填充

在将理论应用于实践时，一个不可避免的问题是如何处理批处理（batching）中的可变长[度序列](@entry_id:267850)。为了将不同长度的序列组合成一个矩形批次进行高效计算，通常会对较短的序列进行**填充（padding）**，即在末尾添加特殊的“pad”标记，直到它们的长度都等于批次中的最大长度 $L_{\max}$。

然而，绝对位置编码的存在给填充带来了风险。即使是填充位置，它们也有一个绝对的位置索引，因此会被赋予一个非零的位置编码向量。这意味着，在输入到第一个 Transformer 层时，填充位置的表示向量通常不是零。

如果没有任何预防措施，这些非零的填充表示会带来**信息泄漏（information leakage）** 。具体来说：
1.  **填充位置被关注**：一个“真实”内容的位置 $i$ 在计算注意力时，其查询向量 $q_i$ 会与所有位置的键向量（包括填充位置 $j_{\text{pad}}$）进行[点积](@entry_id:149019)。由于填充位置的键向量 $k_{j_{\text{pad}}}$ 通常非零，它们会获得一个非零的注意力分数，并通过 softmax 归一化分配到一定的注意力权重。这导致真实位置的输出受到了无意义的填充信息的污染。
2.  **填充位置传播信息**：反过来，一个填充位置 $i_{\text{pad}}$ 的查询向量 $q_{i_{\text{pad}}}$ 也会关注所有真实内容的位置。其输出将是真实内容的一个加权平均，这是一个有意义但本不应存在的向量。这个向量会通过[残差连接](@entry_id:637548)和后续层继续传播，可能在更深层对模型的计算产生干扰。

为了确保模型行为的正确性，必须采用一套鲁棒的**掩码（masking）**策略。仅仅将填充位置的[词嵌入](@entry_id:633879)或位置编码设为零是不够的，因为后续的偏置项和复杂的层内交互仍可能产生非零表示。一个完整的策略通常包括以下几个部分 ：
- **注意力掩码（Attention Mask）**：也称为键填充掩码（key padding mask）。在 softmax 操作之前，将所有对应于填充键的注意力分数（logits）设置为一个非常大的负数（如 $-\infty$）。这可以确保在归一化后，这些填充位置的注意力权重精确为零，从而防止任何位置去“关注”填充部分。
- **输出掩码（Output Masking）**：在每个子层（如[自注意力](@entry_id:635960)和 FFN）之后，将所有填充位置的输出向量手动置为零。这可以防止填充位置通过“关注”真实内容来[累积和](@entry_id:748124)传播信息。
- **损失掩码（Loss Masking）**：在计算最终损失时，必须忽略模型在填充位置上的预测。例如，在语言模型中，只计算真实内容位置上的[交叉熵损失](@entry_id:141524)。

通过结合这几种掩码，我们可以确保填充操作在提高计算效率的同时，不会对模型的学习过程产生有害的副作用。