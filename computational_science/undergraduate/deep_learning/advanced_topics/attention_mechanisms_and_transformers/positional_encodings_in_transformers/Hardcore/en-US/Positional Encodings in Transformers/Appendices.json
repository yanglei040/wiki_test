{
    "hands_on_practices": [
        {
            "introduction": "Before diving into the mechanics of positional encodings (PEs), it's essential to build an intuition for why they are so powerful. This first exercise demonstrates a fundamental principle: PEs are not just arbitrary tags but structured vectors that can, on their own, carry enough information to solve a task. By constructing a scenario where content is irrelevant and only position matters, you will quantify how the geometric arrangement of PE vectors directly enables a model to distinguish between different classes .",
            "id": "3164249",
            "problem": "Let there be a sequence model input in which each position index $i \\in \\{0,1,\\dots,L-1\\}$ is represented by a Positional Encoding (PE) vector using the widely adopted sinusoidal construction. Define the $D$-dimensional sinusoidal Positional Encoding (PE) as follows: for $j \\in \\{0,1,\\dots,\\lfloor D/2 \\rfloor - 1\\}$,\n$$\n\\mathrm{PE}(i,2j) = \\sin\\!\\Big(i \\cdot 10000^{-\\frac{2j}{D}}\\Big), \\qquad\n\\mathrm{PE}(i,2j+1) = \\cos\\!\\Big(i \\cdot 10000^{-\\frac{2j}{D}}\\Big),\n$$\nand, if $D$ is odd, the last unused dimension is set to $0$. All trigonometric arguments are in radians. The final per-position input representation (prior to any self-attention) is the elementwise sum of a content embedding and the Positional Encoding (PE). To isolate supervision carried by position alone, consider a counterfactual dataset in which content tokens are shuffled so that content is statistically independent of the label, and labels are encoded solely by which positions are present (active) in the sequence. Specifically, define a binary classification problem as follows.\n\nFor each class $c \\in \\{0,1\\}$, choose a fixed active position set $S_c \\subseteq \\{0,\\dots,L-1\\}$ of cardinality $k = |S_c|$. For every active position $p \\in S_c$, the content embedding $v_p \\in \\mathbb{R}^D$ is drawn independently from a zero-mean spherical Gaussian with covariance $\\sigma_e^2 I_D$, that is,\n$$\nv_p \\sim \\mathcal{N}\\big(0, \\sigma_e^2 I_D\\big),\n$$\nindependently across positions and samples. A simple permutation-invariant readout is defined by mean pooling over active positions, so the observed per-sequence vector is\n$$\nx = \\frac{1}{k} \\sum_{p \\in S_c} \\big(\\mathrm{PE}(p) + v_p\\big) = s_c + n,\n$$\nwhere\n$$\ns_c = \\frac{1}{k} \\sum_{p \\in S_c} \\mathrm{PE}(p), \\qquad n = \\frac{1}{k} \\sum_{p \\in S_c} v_p.\n$$\nBy independence and Gaussianity, $n \\sim \\mathcal{N}\\big(0, \\sigma_n^2 I_D\\big)$ with\n$$\n\\sigma_n^2 = \\frac{\\sigma_e^2}{k}.\n$$\nUnder equal class priors, the Bayes-optimal linear decision rule separates two Gaussian classes with identical covariance by a hyperplane orthogonal to $s_1 - s_0$. The achievable accuracy depends on the signal-to-noise ratio induced by the positional encodings alone. Your task is to compute, for each specified test case, the Bayes-optimal classification accuracy (expressed as a decimal) achievable when content is shuffled and only position information carries supervision, using the sinusoidal Positional Encoding (PE) defined above and mean pooling over active positions as described. You must assume the Gaussian noise model specified and that angles are in radians.\n\nConstruct $\\mathrm{PE}(i) \\in \\mathbb{R}^D$ for the given $L$ and $D$, compute $s_0$ and $s_1$ for the specified $S_0$ and $S_1$, let\n$$\n\\Delta = \\big\\| s_1 - s_0 \\big\\|_2,\n$$\nand use the Bayes-optimal linear classifier under spherical Gaussian noise to obtain the accuracy as a decimal. In the special case $\\sigma_n = 0$, define the accuracy to be $1$ if $\\Delta > 0$ and $0.5$ if $\\Delta = 0$.\n\nAngle unit requirement: all angles used in $\\sin(\\cdot)$ and $\\cos(\\cdot)$ are in radians. There are no physical units required in this problem. Accuracy must be reported as a decimal (not a percentage).\n\nTest Suite:\n- Case $1$: $L = 16$, $D = 64$, $k = 4$, $\\sigma_e = 0.5$, $S_0 = \\{0,1,2,3\\}$, $S_1 = \\{12,13,14,15\\}$.\n- Case $2$: $L = 32$, $D = 128$, $k = 1$, $\\sigma_e = 1.0$, $S_0 = \\{0\\}$, $S_1 = \\{31\\}$.\n- Case $3$: $L = 64$, $D = 32$, $k = 8$, $\\sigma_e = 2.0$, $S_0 = \\{0,1,2,3,4,5,6,7\\}$, $S_1 = \\{56,57,58,59,60,61,62,63\\}$.\n- Case $4$: $L = 20$, $D = 50$, $k = 10$, $\\sigma_e = 0.0$, $S_0 = \\{0,1,2,3,4,5,6,7,8,9\\}$, $S_1 = \\{10,11,12,13,14,15,16,17,18,19\\}$.\n- Case $5$: $L = 10$, $D = 60$, $k = 5$, $\\sigma_e = 0.3$, $S_0 = \\{0,1,2,3,4\\}$, $S_1 = \\{0,1,2,3,4\\}$.\n\nAnswer Specification and Output Format:\n- For each test case, compute the Bayes-optimal accuracy as a decimal rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\big[ \\text{result}_1,\\text{result}_2,\\dots,\\text{result}_5 \\big]$.",
            "solution": "The problem statement is scientifically grounded, well-posed, and objective. It describes a binary classification task based on a standard signal-plus-noise model, where the signal is derived from sinusoidal positional encodings and the noise is Gaussian. All parameters, formulas, and conditions are explicitly provided, enabling the derivation of a unique, meaningful solution for each test case. The problem is therefore deemed **valid**.\n\nThe solution proceeds as follows:\n\n1.  **Model Formulation and Classification Principle**\n    The problem defines a binary classification task between two classes, $c \\in \\{0,1\\}$, based on an observed vector $x \\in \\mathbb{R}^D$. This vector is the average of input representations over a set of $k$ active positions $S_c$. The input representation at each position is the sum of a fixed Positional Encoding (PE) vector and a random content embedding. The problem statement correctly decomposes the observed vector $x$ into a class-dependent signal term $s_c$ and a random noise term $n$:\n    $$\n    x = s_c + n\n    $$\n    The signal $s_c$ is the mean of the PE vectors for the active positions in class $c$:\n    $$\n    s_c = \\frac{1}{k} \\sum_{p \\in S_c} \\mathrm{PE}(p)\n    $$\n    The noise $n$ is the mean of $k$ independent random vectors, each drawn from a zero-mean spherical Gaussian $\\mathcal{N}(0, \\sigma_e^2 I_D)$. Consequently, the noise vector $n$ itself follows a zero-mean spherical Gaussian distribution, $n \\sim \\mathcal{N}(0, \\sigma_n^2 I_D)$, with variance $\\sigma_n^2 = \\sigma_e^2/k$.\n\n    Therefore, the classification task is to distinguish between two multivariate Gaussian distributions with identical, spherical covariance matrices and equal priors:\n    $$\n    \\text{Class 0: } x \\sim \\mathcal{N}(s_0, \\sigma_n^2 I_D)\n    $$\n    $$\n    \\text{Class 1: } x \\sim \\mathcal{N}(s_1, \\sigma_n^2 I_D)\n    $$\n    For such a problem, the Bayes-optimal classifier implements a linear decision boundary. The accuracy of this classifier depends on the separability of the two distributions, which is determined by the distance between their means relative to the noise level.\n\n2.  **Derivation of Bayes-Optimal Accuracy**\n    Let $\\Delta = \\|s_1 - s_0\\|_2$ be the Euclidean distance between the two class-conditional means. The classification problem can be simplified by projecting the $D$-dimensional data onto the one-dimensional line connecting $s_0$ and $s_1$. The projected means are separated by a distance of $\\Delta$, and the projected noise remains Gaussian with a standard deviation of $\\sigma_n$.\n    \n    The probability of misclassifying a sample is the same for both classes due to symmetry. The total probability of error, $P(error)$, is given by the probability that a sample from one class falls on the wrong side of the optimal decision boundary (the perpendicular bisector of the segment $s_0s_1$). This corresponds to the tail probability of a one-dimensional Gaussian. The argument of the standard normal cumulative distribution function (CDF), denoted $\\Phi(\\cdot)$, becomes a signal-to-noise ratio term. The probability of error is:\n    $$\n    P(\\text{error}) = \\Phi\\left(-\\frac{\\Delta}{2 \\sigma_n}\\right)\n    $$\n    The Bayes-optimal accuracy is $A = 1 - P(\\text{error})$. Using the symmetry property of the CDF, $\\Phi(-z) = 1 - \\Phi(z)$, the accuracy is:\n    $$\n    A = 1 - \\Phi\\left(-\\frac{\\Delta}{2 \\sigma_n}\\right) = \\Phi\\left(\\frac{\\Delta}{2 \\sigma_n}\\right)\n    $$\n\n3.  **Handling the Noise-Free Case**\n    The problem specifies a special condition for when $\\sigma_e = 0$, which implies $\\sigma_n = 0$. In this noise-free setting, the observed vector is exactly the signal, $x = s_c$.\n    -   If $\\Delta = \\|s_1 - s_0\\|_2 > 0$, the signals $s_0$ and $s_1$ are distinct, and the classifier can perfectly distinguish between them, yielding an accuracy of $1.0$.\n    -   If $\\Delta = 0$, then $s_0 = s_1$, and the two classes are indistinguishable. The accuracy reduces to that of random guessing, which is $0.5$ for a binary problem with equal priors.\n    These conditions are consistent with the limits of the general accuracy formula as $\\sigma_n \\to 0$.\n\n4.  **Algorithmic Implementation**\n    The solution is implemented via the following steps for each test case:\n    a.  **PE Matrix Generation**: A function is created to generate the PE matrix of shape $(L, D)$ according to the sinusoidal formulas:\n        $$\n        \\mathrm{PE}(i,2j) = \\sin\\!\\Big(\\frac{i}{10000^{2j/D}}\\Big), \\qquad\n        \\mathrm{PE}(i,2j+1) = \\cos\\!\\Big(\\frac{i}{10000^{2j/D}}\\Big)\n        $$\n        for $i \\in \\{0, \\dots, L-1\\}$ and $j \\in \\{0, \\dots, \\lfloor D/2 \\rfloor - 1\\}$. If $D$ is odd, the last dimension is zero. This is implemented efficiently using NumPy vectorization.\n    b.  **Signal Vector Calculation**: For each class $c \\in \\{0,1\\}$, the signal vector $s_c$ is calculated by taking the mean of the PE vectors corresponding to the indices in the active set $S_c$.\n    c.  **Distance Calculation**: The Euclidean distance $\\Delta = \\|s_1 - s_0\\|_2$ is computed.\n    d.  **Accuracy Calculation**:\n        -   If $\\sigma_e = 0$, the special case logic is applied.\n        -   If $\\sigma_e > 0$, the noise standard deviation $\\sigma_n = \\sigma_e / \\sqrt{k}$ is calculated, and the accuracy is computed using the formula $A = \\Phi(\\Delta / (2\\sigma_n))$, where $\\Phi$ is implemented using `scipy.stats.norm.cdf`.\n    e.  **Formatting**: The final accuracy for each case is formatted to $6$ decimal places. The results are aggregated into the specified list format.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef generate_pe_matrix(L, D):\n    \"\"\"\n    Generates the sinusoidal positional encoding matrix.\n    Args:\n        L (int): Sequence length.\n        D (int): Embedding dimension.\n    Returns:\n        np.ndarray: PE matrix of shape (L, D).\n    \"\"\"\n    pe = np.zeros((L, D), dtype=np.float64)\n    position = np.arange(L, dtype=np.float64)[:, np.newaxis]\n    \n    num_pairs = D // 2\n    if num_pairs  0:\n        j_vec = np.arange(num_pairs, dtype=np.float64)\n        div_term = 10000.0 ** (-2.0 * j_vec / D)\n        \n        angles = position * div_term\n        \n        sin_indices = np.arange(0, 2 * num_pairs, 2)\n        cos_indices = np.arange(1, 2 * num_pairs, 2)\n        \n        pe[:, sin_indices] = np.sin(angles)\n        pe[:, cos_indices] = np.cos(angles)\n        \n    return pe\n\ndef solve():\n    \"\"\"\n    Solves the positional encoding classification problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # (L, D, k, sigma_e, S_0, S_1)\n        (16, 64, 4, 0.5, {0, 1, 2, 3}, {12, 13, 14, 15}),\n        (32, 128, 1, 1.0, {0}, {31}),\n        (64, 32, 8, 2.0, {0, 1, 2, 3, 4, 5, 6, 7}, {56, 57, 58, 59, 60, 61, 62, 63}),\n        (20, 50, 10, 0.0, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {10, 11, 12, 13, 14, 15, 16, 17, 18, 19}),\n        (10, 60, 5, 0.3, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4}),\n    ]\n\n    results = []\n    for L, D, k, sigma_e, S0, S1 in test_cases:\n        # Step 1: Generate the full PE matrix\n        pe_matrix = generate_pe_matrix(L, D)\n        \n        # Step 2: Compute s0 and s1\n        s0_indices = list(S0)\n        s1_indices = list(S1)\n        \n        s0 = pe_matrix[s0_indices].mean(axis=0)\n        s1 = pe_matrix[s1_indices].mean(axis=0)\n        \n        # Step 3: Compute Delta, the L2 norm of the difference\n        delta = np.linalg.norm(s1 - s0)\n        \n        # Step 4: Calculate accuracy based on sigma_e\n        if sigma_e == 0.0:\n            # Noise-free case\n            # Use small floating point tolerance for stability\n            accuracy = 1.0 if delta  1e-9 else 0.5\n        else:\n            # Noisy case\n            if k == 0:\n                accuracy = 0.5\n            else:\n                sigma_n = sigma_e / np.sqrt(k)\n                arg = delta / (2.0 * sigma_n)\n                accuracy = norm.cdf(arg)\n            \n        results.append(accuracy)\n\n    # Format results to 6 decimal places and print in the required format\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having established that PEs can encode meaningful signals, we now explore how they are constructed to represent position across multiple scales. This practice treats the PE vector as a combination of different frequency components, drawing a direct analogy to Fourier analysis. Through a series of ablation studies, you will investigate which frequency bands are critical for reconstructing positional patterns of varying complexity, from smooth, long-range waves to sharp, localized features .",
            "id": "3164200",
            "problem": "You are given a sequence of positions indexed by integer $p \\in \\{0,1,\\dots,N-1\\}$, and a real-valued feature representation called Positional Encoding (PE). The goal is to perform frequency band ablation on the PE and analyze how different bands contribute to reconstructing task-specific target functions over positions. You must implement a complete program that constructs a PE based on a sinusoidal basis, applies ablation masks that remove high- or low-frequency components, fits linear predictors to reconstruct target functions, and reports normalized mean squared errors (NMSE) for a specified test suite.\n\nThe fundamental basis for this problem is the Fourier representation of signals: sinusoidal functions at different angular frequencies form a basis for periodic functions sampled at uniform intervals. You must derive a PE from this basis rather than assuming a known transformer-specific formula.\n\nRequirements:\n- Construct a PE of dimensionality $D = 1 + 2K$ for sequence length $N = 128$, where the first component is a constant bias term and the remaining $2K$ components are generated from $K$ sinusoidal frequency bands using sine and cosine functions. Angles must be in radians. Frequencies must be logarithmically spaced across the interval $[\\omega_{\\min}, \\omega_{\\max}]$ with $K = 8$, $\\omega_{\\min} = \\frac{2\\pi}{N}$, and $\\omega_{\\max} = \\pi$. Denote these angular frequencies by $\\{\\omega_k\\}_{k=0}^{K-1}$.\n- Define the following target functions $h(p)$ over positions:\n    1. $h_{\\text{Lcos}}(p) = \\cos\\left(\\frac{2\\pi p}{64}\\right)$.\n    2. $h_{\\text{Hcos}}(p) = \\cos\\left(\\frac{2\\pi p}{4}\\right)$.\n    3. $h_{\\text{Window}}(p) = \\mathbb{1}\\{|p - 64| \\le 5\\}$, where $\\mathbb{1}\\{\\cdot\\}$ is the indicator function.\n    4. $h_{\\text{Square8}}(p) = \\mathbb{1}\\{(p \\bmod 8)  4\\}$.\n- For ablation, you must apply masks to the PE components associated with specific frequency bands. Keep the constant bias term always. For low-pass ablation at cutoff $\\omega_c$, keep all frequency components with $\\omega_k \\le \\omega_c$ and remove all with $\\omega_k  \\omega_c$. For high-pass ablation at cutoff $\\omega_c$, keep all frequency components with $\\omega_k \\ge \\omega_c$ and remove all with $\\omega_k  \\omega_c$. For no ablation (\"none\"), keep all frequency components.\n- Fit a linear predictor $f(p) = \\mathbf{x}(p)^\\top \\mathbf{w}$ to minimize mean squared error, where $\\mathbf{x}(p)$ is the PE vector at position $p$ after ablation. Use the full set of positions $p \\in \\{0,\\dots,N-1\\}$ as the training set. Compute the normalized mean squared error (NMSE) defined by\n$$\n\\mathrm{NMSE} = \\frac{\\frac{1}{N}\\sum_{p=0}^{N-1}\\big(h(p) - f(p)\\big)^2}{\\mathrm{Var}\\big(h(p)\\big)},\n$$\nwhere $\\mathrm{Var}\\big(h(p)\\big)$ is the variance of $h(p)$ over $p=0,\\dots,N-1$. Report $\\mathrm{NMSE}$ as a float.\n\nAngle unit specification: All angular quantities must be in radians.\n\nTest suite:\nYou must implement the following ordered list of test cases. Each test case is a triple $(\\text{ablation\\_type}, \\omega_c, \\text{task})$ where $\\text{ablation\\_type} \\in \\{\\text{\"none\"}, \\text{\"low\\_pass\"}, \\text{\"high\\_pass\"}\\}$, $\\omega_c$ is a nonnegative real number in radians, and $\\text{task} \\in \\{\\text{\"Lcos\"}, \\text{\"Hcos\"}, \\text{\"Window\"}, \\text{\"Square8\"}\\}$ mapping to the target functions defined above.\n\n- Test case 1: $(\\text{\"none\"}, \\frac{\\pi}{2}, \\text{\"Lcos\"})$\n- Test case 2: $(\\text{\"low\\_pass\"}, 0.4\\pi, \\text{\"Hcos\"})$\n- Test case 3: $(\\text{\"high\\_pass\"}, 0.2\\pi, \\text{\"Hcos\"})$\n- Test case 4: $(\\text{\"low\\_pass\"}, 0.1\\pi, \\text{\"Window\"})$\n- Test case 5: $(\\text{\"high\\_pass\"}, 0.8\\pi, \\text{\"Lcos\"})$\n- Test case 6: $(\\text{\"low\\_pass\"}, 0.01, \\text{\"Lcos\"})$  (boundary: nearly all bands removed)\n- Test case 7: $(\\text{\"high\\_pass\"}, \\pi + 0.01, \\text{\"Hcos\"})$  (boundary: nearly all bands removed)\n- Test case 8: $(\\text{\"none\"}, \\frac{\\pi}{2}, \\text{\"Square8\"})$\n\nFinal output format:\nYour program should produce a single line of output containing the NMSE results, in order of the test cases above, as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots,r_8]$).",
            "solution": "We start from the Fourier representation of signals: the set of sine and cosine functions $\\{\\sin(\\omega p), \\cos(\\omega p)\\}$ for different angular frequencies $\\omega$ forms a basis that can represent periodic signals sampled at uniform intervals. For discrete positions $p \\in \\{0,\\dots,N-1\\}$, any sufficiently smooth or periodic target function can be approximated by a linear combination of such basis functions. This is a well-tested fact from Fourier analysis.\n\nPrinciple-based design:\n1. Basis construction from first principles: Because sine and cosine functions are orthogonal over uniform samples under suitable conditions, they serve as a natural basis for representing positional information. We construct the Positional Encoding (PE) as a vector $\\mathbf{x}(p) \\in \\mathbb{R}^{D}$ comprising a constant bias term and pairs of sine and cosine components at logarithmically spaced angular frequencies $\\omega_k \\in [\\omega_{\\min}, \\omega_{\\max}]$, with $K$ bands and $D = 1 + 2K$. The constant term enables the model to represent the mean of a target function. Logarithmic spacing spans multiple orders of positional scale, providing both low- and high-frequency sensitivity.\n\n2. Tasks and frequency content:\n   - For $h_{\\text{Lcos}}(p) = \\cos\\left(\\frac{2\\pi p}{64}\\right)$, the dominant frequency is low, with angular frequency $\\omega = \\frac{2\\pi}{64}$. This is expected to be captured by low-frequency components; removing high frequencies should not harm much, but removing low frequencies will degrade reconstruction.\n   - For $h_{\\text{Hcos}}(p) = \\cos\\left(\\frac{2\\pi p}{4}\\right)$, the dominant frequency is high, with angular frequency $\\omega = \\frac{2\\pi}{4} = \\frac{\\pi}{2}$. Low-pass ablation at a cutoff below this value will remove the critical component, while high-pass ablation retaining frequencies above a lower cutoff will preserve it.\n   - For $h_{\\text{Window}}(p) = \\mathbb{1}\\{|p - 64| \\le 5\\}$, this target is localized and non-smooth (a step). High frequencies are needed to model sharp transitions; low-pass ablation will blur and increase error.\n   - For $h_{\\text{Square8}}(p) = \\mathbb{1}\\{(p \\bmod 8)  4\\}$, this is a square wave with period $8$ and $50\\%$ duty cycle. Its Fourier series contains a constant plus odd harmonics at high frequencies. Removing high frequencies will degrade reconstruction; keeping them helps.\n\n3. Linear projection and optimality: Given a design matrix $X \\in \\mathbb{R}^{N \\times D}$ whose rows are PE vectors $\\mathbf{x}(p)$, the least squares (LS) solution $\\hat{\\mathbf{w}}$ minimizing $\\sum_p \\big(h(p) - \\mathbf{x}(p)^\\top \\mathbf{w}\\big)^2$ is the orthogonal projection of the target vector $\\mathbf{h}$ onto the column space of $X$. This projection is optimal among all linear combinations of the basis in terms of mean squared error. Therefore, ablation, which removes basis columns, restricts the subspace and can only increase the error or leave it unchanged.\n\n4. Normalized error: The normalized mean squared error (NMSE) is defined as\n$$\n\\mathrm{NMSE} = \\frac{\\frac{1}{N}\\sum_{p=0}^{N-1}\\big(h(p) - f(p)\\big)^2}{\\mathrm{Var}\\big(h(p)\\big)},\n$$\nwhich equals $1 - R^2$ for linear regression on deterministic data. It lies in $[0,1]$ when the predictor includes at least a constant term and the target has nonzero variance. If all sinusoidal components are ablated, the LS solution reduces to the constant predictor equal to the mean of $h$, yielding $\\mathrm{NMSE} \\approx 1$.\n\nAlgorithmic steps:\n- Define $N = 128$, $K = 8$, $\\omega_{\\min} = \\frac{2\\pi}{N}$, $\\omega_{\\max} = \\pi$, and generate $\\{\\omega_k\\}_{k=0}^{K-1}$ logarithmically spaced in $[\\omega_{\\min}, \\omega_{\\max}]$.\n- For each position $p$, construct the PE vector $\\mathbf{x}(p)$ with:\n  - A constant term $1$.\n  - For each $\\omega_k$, append $\\sin(\\omega_k p)$ and $\\cos(\\omega_k p)$.\n- Implement ablation:\n  - Low-pass at $\\omega_c$: keep pairs associated with $\\omega_k \\le \\omega_c$.\n  - High-pass at $\\omega_c$: keep pairs associated with $\\omega_k \\ge \\omega_c$.\n  - None: keep all pairs.\n  - Always keep the constant term.\n- For each target function $h$, build the vector $\\mathbf{h} \\in \\mathbb{R}^{N}$.\n- Solve the LS problem $\\min_{\\mathbf{w}} \\|\\mathbf{h} - X\\mathbf{w}\\|_2^2$ using a robust numerical method (for example, the least squares solver), obtain $\\hat{\\mathbf{w}}$ and predicted $\\hat{\\mathbf{h}} = X\\hat{\\mathbf{w}}$.\n- Compute $\\mathrm{NMSE}$ per the formula above.\n- Run the specified test suite and output the NMSEs.\n\nQualitative expectations for the test suite:\n- Test case 1 (none, Lcos): low-frequency cosine should be well reconstructed with the full basis; expect low NMSE.\n- Test case 2 (low-pass $0.4\\pi$, Hcos): the cutoff $\\omega_c = 0.4\\pi$ removes components above $1.2566$, while the target frequency is $\\frac{\\pi}{2} \\approx 1.5708$; expect high NMSE.\n- Test case 3 (high-pass $0.2\\pi$, Hcos): keeps frequencies above $0.6283$, including $\\frac{\\pi}{2}$; expect low NMSE.\n- Test case 4 (low-pass $0.1\\pi$, Window): keeps only very low components; sharp edges cannot be represented; expect high NMSE.\n- Test case 5 (high-pass $0.8\\pi$, Lcos): keeps only very high components, removing low ones; expect high NMSE.\n- Test case 6 (low-pass $0.01$, Lcos): cutoff below $\\omega_{\\min}$, so only the constant remains; expect NMSE near $1$.\n- Test case 7 (high-pass $\\pi + 0.01$, Hcos): cutoff above $\\omega_{\\max}$, so only the constant remains; expect NMSE near $1$.\n- Test case 8 (none, Square8): full basis allows approximation of a square wave via harmonics; expect moderate NMSE (nonzero due to finite $K$).\n\nThe program will implement these steps deterministically and output the NMSEs as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_frequencies(N: int, K: int) - np.ndarray:\n    # Logarithmically spaced angular frequencies in radians.\n    omega_min = 2 * np.pi / N\n    omega_max = np.pi\n    return np.geomspace(omega_min, omega_max, K)\n\ndef positional_encoding_matrix(N: int, omegas: np.ndarray) - np.ndarray:\n    # Build PE matrix with shape (N, 1 + 2*K): [bias, sin(ω_k p), cos(ω_k p)]\n    K = len(omegas)\n    D = 1 + 2 * K\n    X = np.zeros((N, D), dtype=np.float64)\n    # Bias term\n    X[:, 0] = 1.0\n    # Sin/Cos terms\n    p = np.arange(N, dtype=np.float64)\n    for idx, omega in enumerate(omegas):\n        X[:, 1 + 2 * idx] = np.sin(omega * p)\n        X[:, 1 + 2 * idx + 1] = np.cos(omega * p)\n    return X\n\ndef apply_ablation(X: np.ndarray, omegas: np.ndarray, ablation_type: str, omega_c: float) - np.ndarray:\n    # Always keep bias column 0\n    cols = [0]\n    if ablation_type == \"none\":\n        # Keep all sin/cos pairs\n        K = len(omegas)\n        for idx in range(K):\n            cols.extend([1 + 2 * idx, 1 + 2 * idx + 1])\n    elif ablation_type == \"low_pass\":\n        for idx, omega in enumerate(omegas):\n            if omega = omega_c:\n                cols.extend([1 + 2 * idx, 1 + 2 * idx + 1])\n    elif ablation_type == \"high_pass\":\n        for idx, omega in enumerate(omegas):\n            if omega = omega_c:\n                cols.extend([1 + 2 * idx, 1 + 2 * idx + 1])\n    else:\n        raise ValueError(f\"Unknown ablation type: {ablation_type}\")\n    return X[:, cols]\n\ndef target_function(N: int, task: str) - np.ndarray:\n    p = np.arange(N, dtype=np.float64)\n    if task == \"Lcos\":\n        # Low-frequency cosine, period 64\n        return np.cos(2 * np.pi * p / 64.0)\n    elif task == \"Hcos\":\n        # High-frequency cosine, period 4\n        return np.cos(2 * np.pi * p / 4.0)\n    elif task == \"Window\":\n        # Indicator of window centered at 64 with radius 5\n        return ((np.abs(p - 64) = 5).astype(np.float64))\n    elif task == \"Square8\":\n        # Square wave of period 8, 50% duty cycle\n        return ((p % 8)  4).astype(np.float64)\n    else:\n        raise ValueError(f\"Unknown task: {task}\")\n\ndef nmse(y_true: np.ndarray, y_pred: np.ndarray) - float:\n    mse = np.mean((y_true - y_pred) ** 2)\n    var = np.var(y_true)\n    # var should be  0 for given tasks\n    return float(mse / var)\n\ndef solve():\n    # Define constants\n    N = 128\n    K = 8\n    omegas = build_frequencies(N, K)\n    X_full = positional_encoding_matrix(N, omegas)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"none\", np.pi / 2, \"Lcos\"),\n        (\"low_pass\", 0.4 * np.pi, \"Hcos\"),\n        (\"high_pass\", 0.2 * np.pi, \"Hcos\"),\n        (\"low_pass\", 0.1 * np.pi, \"Window\"),\n        (\"high_pass\", 0.8 * np.pi, \"Lcos\"),\n        (\"low_pass\", 0.01, \"Lcos\"),\n        (\"high_pass\", np.pi + 0.01, \"Hcos\"),\n        (\"none\", np.pi / 2, \"Square8\"),\n    ]\n\n    results = []\n    for ablation_type, omega_c, task in test_cases:\n        X = apply_ablation(X_full, omegas, ablation_type, omega_c)\n        y = target_function(N, task)\n        # Least squares fit\n        w, *_ = np.linalg.lstsq(X, y, rcond=None)\n        y_hat = X @ w\n        results.append(nmse(y, y_hat))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "One of the key design choices for positional information is whether to use a fixed, deterministic function or to learn the encodings from data. This final exercise provides a clear, quantitative comparison of these two approaches, focusing on the critical challenge of extrapolation—applying the model to sequences longer than those seen during training. Using a simplified linear model, you will observe firsthand why the inherent structure of sinusoidal PEs allows for robust generalization, while simple learned encodings fail catastrophically when encountering unseen positions .",
            "id": "3100282",
            "problem": "You are tasked with constructing a principled comparison between deterministic sinusoidal positional encodings and learned positional encodings for extrapolation beyond the training sequence length within a simplified model that isolates positional information from content. The setup models how positional encodings provide features to a linear decoder, which is a minimal abstraction of how attention-based models consume positional signals. You must derive the error behavior from first principles and implement a program that computes the extrapolation error for both encoding types.\n\nThe fundamental basis for this task is:\n- The definition of self-attention, which requires positional information to resolve order, and the notion that positional encodings inject position-dependent features into the model’s computations.\n- The well-tested method of Ordinary Least Squares (OLS), which fits a linear mapping from feature vectors to scalar targets by minimizing the sum of squared residuals.\n- The definition of Root Mean Squared Error (RMSE), which quantifies the discrepancy between predictions and targets over a set.\n\nThe sequence is indexed by discrete positions $p \\in \\{1,2,\\dots\\}$ and has a scalar target $y(p)$ defined by a superposition of sinusoidal components, with all angle computations expressed in radians. Specifically, for all positions $p$, define\n$$\ny(p) = \\sin(\\omega_1 p) + \\tfrac{1}{2}\\cos(\\omega_2 p),\n$$\nwhere $\\omega_1 = \\tfrac{2\\pi}{16}$ and $\\omega_2 = \\tfrac{2\\pi}{32}$.\n\nYou must implement two positional encoding strategies that produce feature vectors $x(p)$ for every position $p$:\n\n- Sinusoidal positional encoding: Use an even-dimensional feature vector of dimension $d$, formed by concatenating pairs of $\\sin$ and $\\cos$ functions at fixed angular frequencies. The feature vector contains pairs $(\\sin(\\omega_k p), \\cos(\\omega_k p))$ for a set of angular frequencies $\\{\\omega_k\\}$ that includes $\\omega_1$ and $\\omega_2$, along with additional distinct frequencies to ensure full column rank on the training positions. The exact construction and scaling of these features are part of your implementation, but they must be deterministic functions of $p$ and not learned from data.\n\n- Learned positional encoding: Use a one-hot feature vector of length $N_{\\text{train}}$ for positions $p \\in \\{1,\\dots,N_{\\text{train}}\\}$. For extrapolation to $p > N_{\\text{train}}$, use a clamping rule in which all positions map to the one-hot vector of index $N_{\\text{train}}$. This models the common behavior of learned embedding tables that cannot produce unseen position embeddings without explicit extension.\n\nTraining is performed using Ordinary Least Squares (OLS): given training positions $p \\in \\{1,2,\\dots,N_{\\text{train}}\\}$, assemble the design matrix $X \\in \\mathbb{R}^{N_{\\text{train}} \\times d}$ from $x(p)$ and the target vector $y \\in \\mathbb{R}^{N_{\\text{train}}}$ from $y(p)$. Fit a linear decoder $w \\in \\mathbb{R}^{d}$ by minimizing the training objective\n$$\n\\min_{w} \\sum_{p=1}^{N_{\\text{train}}} \\left( y(p) - w^\\top x(p) \\right)^2.\n$$\nUse the fitted $w$ to generate predictions $\\hat{y}(p) = w^\\top x(p)$ for positions $p > N_{\\text{train}}$.\n\nDefine the extrapolation error $E(n)$ for any integer $n \\ge N_{\\text{train}}$ as the Root Mean Squared Error (RMSE) on the positions beyond the training range:\n$$\nE(n) =\n\\begin{cases}\n0,  \\text{if } n = N_{\\text{train}},\\\\\n\\sqrt{\\dfrac{1}{n - N_{\\text{train}}} \\sum_{p=N_{\\text{train}}+1}^{n} \\left( y(p) - \\hat{y}(p) \\right)^2},  \\text{if } n > N_{\\text{train}}.\n\\end{cases}\n$$\n\nYour program must:\n- Use training maximum $N_{\\text{train}} = 64$ and sinusoidal encoding dimension $d = 16$ (even).\n- Use the above $y(p)$ with $\\omega_1 = \\tfrac{2\\pi}{16}$ and $\\omega_2 = \\tfrac{2\\pi}{32}$, with all angle computations in radians.\n- Construct the sinusoidal positional encoding with a frequency set that includes $\\omega_1$ and $\\omega_2$ and additional distinct positive angular frequencies.\n- Construct the learned positional encoding as a one-hot vector of length $N_{\\text{train}}$ with the clamping rule for $p > N_{\\text{train}}$.\n- Fit $w$ via OLS separately for each encoding on $p \\in \\{1,2,\\dots,N_{\\text{train}}\\}$.\n\nTest suite:\n- Use the following values of $n$: $64$, $80$, $128$, $160$.\n- This suite includes the boundary $n = N_{\\text{train}}$ (yielding $E(n) = 0$ by definition), a moderate extrapolation length, and larger extrapolation lengths to probe sustained performance.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list should contain the RMSE values for sinusoidal encoding and learned encoding for each test case in order, aggregating across all tests:\n$$\n[\\;E_{\\text{sin}}(64),\\;E_{\\text{learn}}(64),\\;E_{\\text{sin}}(80),\\;E_{\\text{learn}}(80),\\;E_{\\text{sin}}(128),\\;E_{\\text{learn}}(128),\\;E_{\\text{sin}}(160),\\;E_{\\text{learn}}(160)\\;].\n$$\nAll angle computations must be in radians, and the outputs must be real-valued floats.",
            "solution": "The user wants to analyze and implement a comparison between sinusoidal and learned positional encodings for extrapolation in a simplified linear regression setting.\n\n### Step 1: Extract Givens\n- **Target Function**: $y(p) = \\sin(\\omega_1 p) + \\tfrac{1}{2}\\cos(\\omega_2 p)$ for integer positions $p \\in \\{1,2,\\dots\\}$.\n- **Angular Frequencies**: $\\omega_1 = \\tfrac{2\\pi}{16}$ and $\\omega_2 = \\tfrac{2\\pi}{32}$. All angle computations must be in radians.\n- **Training Range**: Positions $p \\in \\{1, 2, \\dots, N_{\\text{train}}\\}$.\n- **Training Size**: $N_{\\text{train}} = 64$.\n- **Training Method**: Ordinary Least Squares (OLS) to minimize $\\sum_{p=1}^{N_{\\text{train}}} ( y(p) - \\mathbf{w}^\\top \\mathbf{x}(p) )^2$.\n- **Sinusoidal Encoding**:\n    - Feature dimension $d = 16$.\n    - Features are pairs of $(\\sin(\\omega_k p), \\cos(\\omega_k p))$.\n    - The set of frequencies $\\{\\omega_k\\}$ must include $\\omega_1$ and $\\omega_2$.\n- **Learned Encoding**:\n    - For $p \\in \\{1, \\dots, N_{\\text{train}}\\}$, $\\mathbf{x}(p)$ is a one-hot vector of length $N_{\\text{train}}$.\n    - For $p > N_{\\text{train}}$, a clamping rule is used: $\\mathbf{x}(p) = \\mathbf{x}(N_{\\text{train}})$.\n- **Extrapolation Error Metric**: $E(n)$, the Root Mean Squared Error (RMSE) on positions $\\{N_{\\text{train}}+1, \\dots, n\\}$.\n  $$\n  E(n) =\n  \\begin{cases}\n  0,  \\text{if } n = N_{\\text{train}},\\\\\n  \\sqrt{\\dfrac{1}{n - N_{\\text{train}}} \\sum_{p=N_{\\text{train}}+1}^{n} \\left( y(p) - \\hat{y}(p) \\right)^2},  \\text{if } n > N_{\\text{train}}.\n  \\end{cases}\n  $$\n- **Test Suite**: Evaluate $E(n)$ for $n \\in \\{64, 80, 128, 160\\}$.\n- **Required Output**: A single-line list: `[E_sin(64), E_learn(64), E_sin(80), E_learn(80), E_sin(128), E_learn(128), E_sin(160), E_learn(160)]`.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to rigorous validation.\n\n- **Scientifically Grounded**: The problem is based on well-established principles of statistical learning (Ordinary Least Squares), numerical analysis (RMSE), and concepts from modern deep learning (positional encodings in Transformer models). The target function is a simple, well-behaved mathematical function. The setup is a valid and common technique for analyzing feature representations.\n- **Well-Posed**: The problem is well-posed. The OLS solution is unique provided the design matrix has full column rank. For sinusoidal encoding, the problem explicitly instructs to choose frequencies to ensure this. For learned encoding, the design matrix on the training set is the identity matrix, which is of full rank. The extrapolation rules are unambiguously defined. A unique solution for the weights $\\mathbf{w}$ and subsequent predictions $\\hat{y}(p)$ exists for both cases.\n- **Objective**: The problem is stated in precise, objective mathematical language.\n- **Completeness**: All necessary parameters ($N_{\\text{train}}$, $d$, $\\omega_1$, $\\omega_2$) and definitions are provided. The specification for constructing the sinusoidal frequencies allows for a principled choice (e.g., a geometric progression of periods that includes the specified ones), making the problem fully determined.\n- **Consistency**: The definitions and constraints are internally consistent. Dimensionalities of vectors and matrices are compatible.\n\nThe problem does not exhibit any of the invalidity flaws. It is a formal, scientifically grounded, and well-posed problem.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Theoretical Framework and Derivation\nThe core of the problem is to fit a linear model $\\hat{y}(p) = \\mathbf{w}^\\top \\mathbf{x}(p)$ to a target function $y(p)$ using two different feature maps $\\mathbf{x}(p)$ and to compare their extrapolation performance. The model parameters $\\mathbf{w}$ are found by Ordinary Least Squares (OLS) on the training set $p \\in \\{1, \\dots, N_{\\text{train}}\\}$.\n\nLet $X \\in \\mathbb{R}^{N_{\\text{train}} \\times d}$ be the design matrix, whose rows are the feature vectors $\\mathbf{x}(p)^\\top$ for $p=1, \\dots, N_{\\text{train}}$. Let $\\mathbf{y}_{\\text{train}} \\in \\mathbb{R}^{N_{\\text{train}}}$ be the vector of target values $y(p)$ on the training set. The OLS solution for the weight vector $\\mathbf{w}$ is given by the normal equations:\n$$\n\\mathbf{w} = (X^\\top X)^{-1} X^\\top \\mathbf{y}_{\\text{train}}\n$$\nThis solution exists and is unique if $X^\\top X$ is invertible, which is equivalent to $X$ having full column rank.\n\n#### 1. Sinusoidal Positional Encoding\nThis strategy is designed to represent positions using a basis of periodic functions.\n- **Feature Vector Construction**: The feature vector $\\mathbf{x}_{\\text{sin}}(p)$ has dimension $d=16$. It is formed by concatenating $d/2 = 8$ pairs of sine and cosine functions:\n$$\n\\mathbf{x}_{\\text{sin}}(p) = [\\sin(\\Omega_1 p), \\cos(\\Omega_1 p), \\sin(\\Omega_2 p), \\cos(\\Omega_2 p), \\dots, \\sin(\\Omega_8 p), \\cos(\\Omega_8 p)]^\\top\n$$\nThe problem requires that the set of angular frequencies $\\{\\Omega_k\\}_{k=1}^8$ contains $\\omega_1=\\frac{2\\pi}{16}$ and $\\omega_2=\\frac{2\\pi}{32}$. We select a principled set of frequencies corresponding to a geometric progression of periods, ensuring the required frequencies are included. A suitable choice for periods is $\\{8, 16, 32, 64, 128, 256, 512, 1024\\}$. The corresponding angular frequencies are $\\Omega_k = 2\\pi/T_k$. This set is distinct and spans a wide range.\n\n- **Extrapolation Behavior**: The target function is $y(p) = \\sin(\\omega_1 p) + \\frac{1}{2}\\cos(\\omega_2 p)$. Since $\\omega_1$ and $\\omega_2$ are included in our basis frequencies, the true function $y(p)$ is a linear combination of the basis functions used to construct $\\mathbf{x}_{\\text{sin}}(p)$. Specifically, it corresponds to a weight vector where the coefficient for $\\sin(\\omega_1 p)$ is $1$, the coefficient for $\\cos(\\omega_2 p)$ is $0.5$, and all other coefficients are $0$. Although the sinusoidal basis functions are not perfectly orthogonal over the finite, discrete training interval $\\{1, \\dots, 64\\}$, OLS is designed to find the best linear approximation. It will find a weight vector $\\mathbf{w}_{\\text{sin}}$ that allows the model $\\hat{y}(p) = \\mathbf{w}_{\\text{sin}}^\\top \\mathbf{x}_{\\text{sin}}(p)$ to closely approximate the true function. Because the model has the functional form of the true data-generating process, this approximation will hold not only for the training points but also for extrapolated points $p > N_{\\text{train}}$. Consequently, the extrapolation error $E_{\\text{sin}}(n)$ is expected to be very small, limited primarily by numerical precision and minor fitting errors due to non-orthogonality of the basis on the finite training set.\n\n#### 2. Learned Positional Encoding\nThis strategy models position embeddings that are learned independently for each position in the training set.\n- **Feature Vector Construction**: The feature dimension is $d = N_{\\text{train}} = 64$. For a training position $p \\in \\{1, \\dots, 64\\}$, the feature vector $\\mathbf{x}_{\\text{learn}}(p)$ is a one-hot vector where the $p$-th element is $1$ and all others are $0$. The design matrix $X_{\\text{learn}}$ is therefore the $64 \\times 64$ identity matrix, $I_{64}$.\n\n- **Training**: The OLS solution for the weights is:\n$$\n\\mathbf{w}_{\\text{learn}} = (X_{\\text{learn}}^\\top X_{\\text{learn}})^{-1} X_{\\text{learn}}^\\top \\mathbf{y}_{\\text{train}} = (I_{64}^\\top I_{64})^{-1} I_{64}^\\top \\mathbf{y}_{\\text{train}} = \\mathbf{y}_{\\text{train}}\n$$\nThis means the model simply memorizes the target value for each training position. The weight vector $\\mathbf{w}_{\\text{learn}}$ is identical to the vector of training targets.\n\n- **Extrapolation Behavior**: The problem specifies a clamping rule for extrapolation: for any $p > N_{\\text{train}}$, the feature vector is the same as for the last training position, i.e., $\\mathbf{x}_{\\text{learn}}(p) = \\mathbf{x}_{\\text{learn}}(N_{\\text{train}})$. This is the one-hot vector corresponding to position $N_{\\text{train}}=64$. The prediction for any such position is:\n$$\n\\hat{y}_{\\text{learn}}(p) = \\mathbf{w}_{\\text{learn}}^\\top \\mathbf{x}_{\\text{learn}}(N_{\\text{train}})\n$$\nSince $\\mathbf{w}_{\\text{learn}, i} = y(i)$ (using 1-based indexing for a moment) and $\\mathbf{x}_{\\text{learn}}(N_{\\text{train}})$ has a $1$ only at the $N_{\\text{train}}$-th position, the dot product picks out the last element of $\\mathbf{w}_{\\text{learn}}$.\n$$\n\\hat{y}_{\\text{learn}}(p) = y(N_{\\text{train}}) \\quad \\text{for all } p > N_{\\text{train}}\n$$\nThe model thus makes a constant prediction for all unseen positions, equal to the value at the edge of its training experience. The true function $y(p)$ continues to oscillate for $p>64$, so the error $y(p) - y(64)$ will be substantial. The RMSE, $E_{\\text{learn}}(n)$, will accumulate these large errors and is expected to be significantly higher than for the sinusoidal encoding.\n\n### Calculation of Extrapolation Error $E(n)$\nFor each test value $n > N_{\\text{train}}$:\n1. Define the set of extrapolation positions $P_{\\text{extrap}} = \\{N_{\\text{train}}+1, \\dots, n\\}$.\n2. Generate true values $y(p)$ for all $p \\in P_{\\text{extrap}}$.\n3. Generate predictions $\\hat{y}_{\\text{sin}}(p)$ and $\\hat{y}_{\\text{learn}}(p)$ for all $p \\in P_{\\text{extrap}}$.\n4. Compute the RMSE for each case:\n   $$\n   E(n) = \\sqrt{\\frac{1}{n - N_{\\text{train}}} \\sum_{p \\in P_{\\text{extrap}}} (y(p) - \\hat{y}(p))^2}\n   $$\nFor the edge case $n=N_{\\text{train}}$, the error $E(N_{\\text{train}})$ is defined to be $0$ for both methods.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the extrapolation error of sinusoidal and learned\n    positional encodings within a simplified linear regression framework.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    N_train = 64\n    d_sin = 16\n    omega1 = 2 * np.pi / 16\n    omega2 = 2 * np.pi / 32\n    test_n_values = [64, 80, 128, 160]\n\n    # --- Target Function ---\n    def target_function(p):\n        \"\"\"Computes the true scalar target y(p).\"\"\"\n        return np.sin(omega1 * p) + 0.5 * np.cos(omega2 * p)\n\n    # --- Training Data ---\n    # Use 1-based indexing for positions as in the problem description\n    train_positions = np.arange(1, N_train + 1)\n    y_train = target_function(train_positions)\n\n    # --- 1. Sinusoidal Positional Encoding ---\n\n    # Define frequencies for sinusoidal encoding. The set of periods includes\n    # 16 and 32 (corresponding to omega1 and omega2) plus others to ensure\n    # full rank and represent a range of frequencies.\n    periods = np.array([8, 16, 32, 64, 128, 256, 512, 1024])\n    sin_frequencies = 2 * np.pi / periods\n    \n    def get_sinusoidal_features(positions, freqs, dim):\n        \"\"\"Generates the sinusoidal feature matrix for a given set of positions.\"\"\"\n        num_pos = len(positions)\n        num_freqs = dim // 2\n        # Ensure freqs has the right length just in case\n        if len(freqs) != num_freqs:\n            raise ValueError(f\"Expected {num_freqs} frequencies for dimension {dim}, but got {len(freqs)}\")\n            \n        # Create a matrix of p*omega values\n        p_omega = positions[:, np.newaxis] * freqs[np.newaxis, :]\n        \n        # Create the feature matrix by interleaving sin and cos\n        X = np.zeros((num_pos, dim))\n        X[:, 0::2] = np.sin(p_omega)\n        X[:, 1::2] = np.cos(p_omega)\n        return X\n\n    # Fit the linear decoder for sinusoidal encoding\n    X_sin_train = get_sinusoidal_features(train_positions, sin_frequencies, d_sin)\n    w_sin, _, _, _ = np.linalg.lstsq(X_sin_train, y_train, rcond=None)\n\n    # --- 2. Learned Positional Encoding ---\n    # For learned one-hot encoding, the weight vector w_learn is simply the\n    # target vector y_train, as X_learn is the identity matrix.\n    # The extrapolation prediction is clamped to the value at N_train.\n    y_pred_learn_extrapol_val = target_function(N_train)\n\n    # --- Calculate Errors for Test Cases ---\n    results = []\n    for n in test_n_values:\n        if n == N_train:\n            # By definition, extrapolation error at n = N_train is 0.\n            results.append(0.0)  # E_sin(64)\n            results.append(0.0)  # E_learn(64)\n            continue\n        \n        # Extrapolation positions\n        extrapol_positions = np.arange(N_train + 1, n + 1)\n        num_extrapol_points = len(extrapol_positions)\n        \n        # True values for the extrapolation range\n        y_true_extrapol = target_function(extrapol_positions)\n        \n        # Calculate error for sinusoidal encoding\n        X_sin_extrapol = get_sinusoidal_features(extrapol_positions, sin_frequencies, d_sin)\n        y_pred_sin_extrapol = X_sin_extrapol @ w_sin\n        error_sin = np.sqrt(np.mean((y_true_extrapol - y_pred_sin_extrapol)**2))\n        results.append(error_sin)\n\n        # Calculate error for learned encoding\n        # The prediction is a constant value for all extrapolation points\n        y_pred_learn_extrapol = np.full(num_extrapol_points, y_pred_learn_extrapol_val)\n        error_learn = np.sqrt(np.mean((y_true_extrapol - y_pred_learn_extrapol)**2))\n        results.append(error_learn)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}