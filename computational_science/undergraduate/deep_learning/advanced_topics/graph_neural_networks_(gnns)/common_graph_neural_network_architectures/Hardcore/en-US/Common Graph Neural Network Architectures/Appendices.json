{
    "hands_on_practices": [
        {
            "introduction": "A major challenge for GNNs is scaling to massive, real-world graphs where aggregating information from all neighbors for every node becomes computationally infeasible. This practice explores GraphSAGE, a pioneering inductive GNN architecture designed to address this challenge through neighbor sampling. By implementing a simplified GraphSAGE model , you will empirically investigate the critical trade-off between computational cost, model accuracy, and the variance introduced by the sampling process.",
            "id": "3106236",
            "problem": "You will implement and study a simplified Graph Sample and Aggregate (GraphSAGE) model for node classification to quantify how the neighbor sampling size affects computational cost, training accuracy, and an empirical gradient noise scale. The setting is purely mathematical and algorithmic, with no physical units. All angles are irrelevant here.\n\nFundamental base and definitions to use:\n- Graph Sample and Aggregate (GraphSAGE) with mean aggregation: Given a graph with node features, the GraphSAGE mean aggregator for a node aggregates its neighbors’ features by averaging. You will use a linear model where the logit for a node is the sum of two linear forms, one applied to the node’s own features and one applied to the mean of sampled neighbor features.\n- Binary Cross-Entropy (BCE) loss with the logistic function: For a binary label $y \\in \\{0,1\\}$ and a logit $z \\in \\mathbb{R}$, the logistic function is $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. The BCE loss per sample is $-\\left(y \\log \\sigma(z) + (1-y)\\log(1-\\sigma(z))\\right)$. The derivative with respect to the logit is $\\sigma(z) - y$.\n- Expectation and variance: The empirical gradient noise scale will be defined as a normalized mean squared deviation of a stochastic gradient from the full-batch gradient under neighbor sampling.\n\nTask and setup:\n- Construct an undirected ring graph on $n=80$ nodes indexed by $v \\in \\{0,1,\\dots,79\\}$, where each node is connected to its $2$ nearest neighbors on each side. Formally, connect each node $v$ to $(v \\pm 1) \\bmod 80$ and $(v \\pm 2) \\bmod 80$. This yields a regular degree-$4$ graph.\n- Create node features $x_v \\in \\mathbb{R}^d$ with $d=8$, drawn from a standard normal distribution. Fix a true weight vector $q \\in \\mathbb{R}^d$, sampled from a standard normal distribution and then normalized to unit length. For each node $v$, define the full-neighbor mean feature $\\bar{x}_{\\mathcal{N}(v)} = \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} x_u$, where $\\mathcal{N}(v)$ denotes the set of all neighbors of $v$. Define the logit for data generation as $z_v^{\\text{true}} = q^\\top \\left( \\alpha x_v + (1-\\alpha)\\bar{x}_{\\mathcal{N}(v)} \\right)$ with $\\alpha = 0.5$. Generate labels deterministically by $y_v = \\mathbf{1}[z_v^{\\text{true}} \\ge 0]$.\n- Model: Use a one-layer linear GraphSAGE-style predictor with mean aggregation and logistic output. For a node $v$, sample a set of neighbors $\\mathcal{S}_s(v)$ by uniformly sampling without replacement $s$ neighbors from $\\mathcal{N}(v)$, clamped so that if $s \\ge |\\mathcal{N}(v)|$ you use all neighbors. Compute the sampled mean $\\bar{x}_{\\mathcal{S}_s(v)} = \\frac{1}{|\\mathcal{S}_s(v)|} \\sum_{u \\in \\mathcal{S}_s(v)} x_u$. The model logit is:\n$$\nz_v(W_{\\text{self}}, W_{\\text{neigh}}; s) = W_{\\text{self}}^\\top x_v + W_{\\text{neigh}}^\\top \\bar{x}_{\\mathcal{S}_s(v)},\n$$\nwith parameters $W_{\\text{self}} \\in \\mathbb{R}^d$ and $W_{\\text{neigh}} \\in \\mathbb{R}^d$ to be trained.\n- Training objective: Minimize the average Binary Cross-Entropy loss across all nodes using full-batch gradient descent, where the gradient for each node is computed using the sampled mean $\\bar{x}_{\\mathcal{S}_s(v)}$. For a node $v$, let $p_v = \\sigma(z_v)$ and error term $\\delta_v = p_v - y_v$. The per-node contribution to the parameter gradients is\n$$\n\\nabla_{W_{\\text{self}}} \\ell_v = \\delta_v \\, x_v, \\quad \\nabla_{W_{\\text{neigh}}} \\ell_v = \\delta_v \\, \\bar{x}_{\\mathcal{S}_s(v)}.\n$$\n- Training protocol: Initialize $W_{\\text{self}} = 0$ and $W_{\\text{neigh}} = 0$, use a fixed learning rate $\\eta = 0.1$, and perform $E=200$ epochs of full-batch gradient descent. At each epoch, for each node, resample $\\mathcal{S}_s(v)$. Use a fixed random seed for reproducibility for each $s$.\n- Evaluation: After training for each $s$, compute accuracy using full neighbors only (that is, replace $\\bar{x}_{\\mathcal{S}_s(v)}$ by $\\bar{x}_{\\mathcal{N}(v)}$) and predict $\\hat{y}_v = \\mathbf{1}[z_v \\ge 0]$. Report accuracy as a float in the interval $[0,1]$ rounded to four decimals.\n\nComputational cost proxy:\n- Define the per-epoch computational cost $C(s)$ as the total number of neighbor feature reads, that is,\n$$\nC(s) = \\sum_{v=0}^{n-1} \\min\\{s, |\\mathcal{N}(v)|\\} \\cdot d.\n$$\nFor the given regular graph with degree $4$, this simplifies to $C(s) = n \\cdot \\min\\{s, 4\\} \\cdot d$. Report $C(s)$ as an integer.\n\nEmpirical gradient noise scale:\n- Fix the parameters at the initial values $W_{\\text{self}}=0$ and $W_{\\text{neigh}}=0$. Define the full-batch gradient using full neighbors as\n$$\ng_{\\text{full}} = \\nabla \\left( \\frac{1}{n} \\sum_{v=0}^{n-1} \\ell_v \\right) \\Bigg|_{\\bar{x}_{\\mathcal{N}(v)}} \\in \\mathbb{R}^{2d}.\n$$\nFor a given sampling size $s$, define a stochastic full-batch gradient estimate using sampled neighbor means as\n$$\ng^{(r)}_s = \\nabla \\left( \\frac{1}{n} \\sum_{v=0}^{n-1} \\ell_v \\right) \\Bigg|_{\\bar{x}_{\\mathcal{S}^{(r)}_s(v)}} \\in \\mathbb{R}^{2d},\n$$\nwhere $\\mathcal{S}^{(r)}_s(v)$ denotes the $r$-th random sampling of neighbors for each node.\n- Estimate the empirical gradient noise scale by\n$$\n\\mathcal{G}(s) = \\frac{ \\mathbb{E}_r \\left[ \\left\\| g^{(r)}_s - g_{\\text{full}} \\right\\|_2^2 \\right] }{ \\left\\| g_{\\text{full}} \\right\\|_2^2 },\n$$\nwith the expectation approximated by averaging over $R=256$ independent samplings. Report $\\mathcal{G}(s)$ rounded to six decimals.\n\nTest suite:\n- Use the sampling sizes $s \\in \\{1, 2, 4, 100\\}$. Note that $s=100$ tests the boundary where $s \\ge |\\mathcal{N}(v)|$ for all $v$.\n- For all stochastic components, fix the random seed as follows:\n    - Data generation seed: $123$.\n    - Training seed per $s$: use $1000 + s$.\n    - Gradient noise estimation seed per $s$: use $2000 + s$.\n- Hyperparameters: $n=80$, $d=8$, $\\alpha=0.5$, $\\eta=0.1$, $E=200$, $R=256$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists with no spaces, in the order:\n$$\n\\big[ [s\\_1,\\dots,s\\_k], [\\text{acc}(s\\_1),\\dots,\\text{acc}(s\\_k)], [C(s\\_1),\\dots,C(s\\_k)], [\\mathcal{G}(s\\_1),\\dots,\\mathcal{G}(s\\_k)] \\big],\n$$\nwhere the accuracies are rounded to four decimals and the noise scales are rounded to six decimals. For this problem, $k=4$ and $[s_1,\\dots,s_4] = [1,2,4,100]$. An example of the exact formatting is\n$[[1,2,4,100],[0.9000,0.9200,0.9300,0.9300],[640,1280,2560,2560],[0.123456,0.100000,0.000001,0.000000]]$\nbut with your computed values.",
            "solution": "The user-provided problem is a well-defined computational task within the domain of graph neural networks and is therefore valid. The following solution implements the specified simplified GraphSample and Aggregate (GraphSAGE) model to analyze the effects of neighbor sampling.\n\n### 1. Graph and Data Synthesis\n\nFirst, we construct the computational graph and generate the synthetic dataset as per the problem's specifications.\n\n- **Graph Structure**: An undirected ring graph with $n=80$ nodes is created. Each node $v$ is connected to its four nearest neighbors, specifically the nodes indexed by $(v \\pm 1) \\pmod{80}$ and $(v \\pm 2) \\pmod{80}$. This results in a $k$-regular graph where the degree of every node is $|\\mathcal{N}(v)|=4$. An adjacency list is used to store the graph structure.\n- **Node Features**: For each node $v$, we generate a feature vector $x_v \\in \\mathbb{R}^d$ with dimension $d=8$. The components of each vector are drawn independently from a standard normal distribution, $\\mathcal{N}(0, 1)$. This process is seeded with a random seed of $123$ for reproducibility.\n- **True Labels**: The ground-truth labels are generated using a \"teacher\" model. A true weight vector $q \\in \\mathbb{R}^d$ is sampled from a standard normal distribution and then normalized to have a unit $\\ell_2$-norm, i.e., $\\|q\\|_2=1$. For each node $v$, its full-neighbor mean feature vector is computed as $\\bar{x}_{\\mathcal{N}(v)} = \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} x_u$. The true logit is then defined as $z_v^{\\text{true}} = q^\\top \\left( \\alpha x_v + (1-\\alpha)\\bar{x}_{\\mathcal{N}(v)} \\right)$, with $\\alpha=0.5$. The binary label for node $v$ is deterministically set to $y_v = \\mathbf{1}[z_v^{\\text{true}} \\ge 0]$.\n\n### 2. GraphSAGE Model and Training\n\nWe implement a one-layer linear GraphSAGE-style model for binary node classification. The process is iterated for each specified sampling size $s \\in \\{1, 2, 4, 100\\}$.\n\n- **Model Architecture**: For a given node $v$ and a sampling size $s$, we first form a neighbor subset $\\mathcal{S}_s(v)$ by sampling $s' = \\min\\{s, |\\mathcal{N}(v)|\\}$ neighbors uniformly without replacement from the full neighborhood $\\mathcal{N}(v)$. We then compute the sampled-neighbor mean feature, $\\bar{x}_{\\mathcal{S}_s(v)} = \\frac{1}{|\\mathcal{S}_s(v)|} \\sum_{u \\in \\mathcal{S}_s(v)} x_u$. The model's logit for node $v$ is a linear combination:\n$$\nz_v(W_{\\text{self}}, W_{\\text{neigh}}; s) = W_{\\text{self}}^\\top x_v + W_{\\text{neigh}}^\\top \\bar{x}_{\\mathcal{S}_s(v)}\n$$\nwhere $W_{\\text{self}} \\in \\mathbb{R}^d$ and $W_{\\text{neigh}} \\in \\mathbb{R}^d$ are the trainable weight vectors, initialized to zero vectors.\n\n- **Training Procedure**: The model is trained to minimize the average Binary Cross-Entropy (BCE) loss over all $n$ nodes. We use full-batch gradient descent for $E=200$ epochs with a learning rate of $\\eta = 0.1$. In each epoch:\n    1. For each node $v$, a new set of neighbors $\\mathcal{S}_s(v)$ is sampled. A specific random seed, $1000+s$, is used for the entire training process for a given $s$.\n    2. The model's prediction is computed using the logistic function, $p_v = \\sigma(z_v) = (1 + e^{-z_v})^{-1}$.\n    3. The error term for the BCE loss gradient is $\\delta_v = p_v - y_v$.\n    4. The per-node gradients are $\\nabla_{W_{\\text{self}}} \\ell_v = \\delta_v x_v$ and $\\nabla_{W_{\\text{neigh}}} \\ell_v = \\delta_v \\bar{x}_{\\mathcal{S}_s(v)}$.\n    5. These gradients are summed over all nodes to form the full-batch gradients: $\\nabla_{W} L = \\sum_{v=0}^{n-1} \\nabla_{W} \\ell_v$.\n    6. The weights are updated: $W \\leftarrow W - \\eta \\cdot \\frac{1}{n} \\nabla_{W} L$.\n\n### 3. Evaluation Metrics\n\nAfter training for each value of $s$, we compute the three required metrics: computational cost, accuracy, and gradient noise scale.\n\n- **Computational Cost ($C(s)$)**: This is a proxy for the computational work per epoch, defined as the total number of neighbor features accessed. For our $4$-regular graph, it simplifies to $C(s) = n \\cdot \\min\\{s, 4\\} \\cdot d$.\n\n- **Accuracy**: After $E=200$ training epochs, the model's performance is evaluated. Crucially, the evaluation is always performed using the full neighborhood information, regardless of the sampling size $s$ used during training. The evaluation logit is $z_v^{\\text{eval}} = W_{\\text{self}}^\\top x_v + W_{\\text{neigh}}^\\top \\bar{x}_{\\mathcal{N}(v)}$, and the prediction is $\\hat{y}_v = \\mathbf{1}[z_v^{\\text{eval}} \\ge 0]$. The accuracy is the fraction of correctly classified nodes, $\\frac{1}{n}\\sum_{v=0}^{n-1} \\mathbf{1}[\\hat{y}_v=y_v]$, rounded to four decimal places.\n\n- **Empirical Gradient Noise Scale ($\\mathcal{G}(s)$)**: This metric quantifies the noise introduced by neighbor sampling at the beginning of training (i.e., at initial weights $W_{\\text{self}}, W_{\\text{neigh}} = 0$). For this calculation, a dedicated seed of $2000+s$ is used.\n    1. **Full Gradient ($g_{\\text{full}}$)**: First, the \"true\" batch gradient is computed using the full neighborhood $\\mathcal{N}(v)$ for all nodes. At $W=0$, the logit $z_v=0$, prediction $p_v = \\sigma(0) = 0.5$, and error $\\delta_v = 0.5 - y_v$. The gradient is $g_{\\text{full}} = \\frac{1}{n} \\sum_{v=0}^{n-1} [\\delta_v x_v, \\delta_v \\bar{x}_{\\mathcal{N}(v)}]$.\n    2. **Stochastic Gradients ($g_s^{(r)}$)**: Next, we generate $R=256$ independent stochastic gradient estimates. Each estimate $g_s^{(r)}$ is computed similarly to $g_{\\text{full}}$, but using a newly sampled neighbor mean $\\bar{x}_{\\mathcal{S}^{(r)}_s(v)}$ for each node $v$.\n    3. **Noise Scale Calculation**: The noise scale is the ratio of the mean squared error of the stochastic gradient to the squared norm of the full gradient:\n    $$\n    \\mathcal{G}(s) = \\frac{ \\frac{1}{R} \\sum_{r=1}^{R} \\left\\| g^{(r)}_s - g_{\\text{full}} \\right\\|_2^2 }{ \\left\\| g_{\\text{full}} \\right\\|_2^2 }\n    $$\n    When $s \\ge 4$, sampling is deterministic (all neighbors are chosen), so $g_s^{(r)} = g_{\\text{full}}$ for all $r$, making $\\mathcal{G}(s)=0$. The final value is rounded to six decimal places.\n\nThis entire procedure is executed for each $s$ in the test suite, and the results are aggregated into the specified list-of-lists format.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a simplified GraphSAGE model analysis as specified in the problem.\n    This function performs data generation, model training, and evaluation for different\n    neighbor sampling sizes, and computes accuracy, computational cost, and gradient noise.\n    \"\"\"\n    # Hyperparameters and constants\n    N = 80\n    D = 8\n    ALPHA = 0.5\n    ETA = 0.1\n    EPOCHS = 200\n    R_SAMPLES = 256\n    S_VALUES = [1, 2, 4, 100]\n    NUM_TOTAL_NEIGHBORS = 4\n\n    # --- Part 1: Data and Graph Generation ---\n    rng_data = np.random.default_rng(123)\n\n    # 1.1. Graph Construction (undirected ring graph)\n    adj = {i: [] for i in range(N)}\n    for i in range(N):\n        for offset in [-2, -1, 1, 2]:\n            adj[i].append((i + offset) % N)\n    adj_np = np.array([sorted(adj[i]) for i in range(N)])\n\n    # 1.2. Feature Generation\n    X = rng_data.standard_normal(size=(N, D))\n\n    # 1.3. True Model and Label Generation\n    q_true = rng_data.standard_normal(size=D)\n    q_true /= np.linalg.norm(q_true)\n\n    # Pre-calculate full-neighbor means (vectorized)\n    X_bar_full = X[adj_np].mean(axis=1)\n\n    # Calculate true logits and labels\n    z_true = np.sum(q_true * (ALPHA * X + (1 - ALPHA) * X_bar_full), axis=1)\n    Y = (z_true = 0).astype(int)\n\n    # --- Part 2: Main Loop over Sampling Sizes ---\n    accuracies = []\n    costs = []\n    noise_scales = []\n\n    for s in S_VALUES:\n        num_neighbors_to_sample = min(s, NUM_TOTAL_NEIGHBORS)\n\n        # 2.1. Computational Cost Proxy\n        cost = N * num_neighbors_to_sample * D\n        costs.append(cost)\n\n        # 2.2. Training\n        rng_train = np.random.default_rng(1000 + s)\n        W_self = np.zeros(D)\n        W_neigh = np.zeros(D)\n\n        for _ in range(EPOCHS):\n            # Sample neighbors for all nodes for this epoch\n            sampled_indices = np.empty((N, num_neighbors_to_sample), dtype=int)\n            for i in range(N):\n                sampled_indices[i] = rng_train.choice(\n                    adj_np[i], size=num_neighbors_to_sample, replace=False\n                )\n            \n            X_bar_sampled = X[sampled_indices].mean(axis=1)\n            \n            # Vectorized logit and gradient calculation\n            logits = np.sum(W_self * X, axis=1) + np.sum(W_neigh * X_bar_sampled, axis=1)\n            p = 1 / (1 + np.exp(-logits))\n            delta = p - Y\n            \n            grad_W_self_batch = np.dot(delta, X)\n            grad_W_neigh_batch = np.dot(delta, X_bar_sampled)\n\n            # Update weights\n            W_self -= ETA * grad_W_self_batch / N\n            W_neigh -= ETA * grad_W_neigh_batch / N\n        \n        # 2.3. Evaluation (Accuracy)\n        eval_logits = np.sum(W_self * X, axis=1) + np.sum(W_neigh * X_bar_full, axis=1)\n        y_hat = (eval_logits = 0).astype(int)\n        accuracy = np.mean(y_hat == Y)\n        accuracies.append(round(accuracy, 4))\n        \n        # 2.4. Gradient Noise Scale\n        rng_noise = np.random.default_rng(2000 + s)\n        \n        # Calculate full gradient (g_full) at initial weights W=0\n        p_init = 0.5\n        delta_init = p_init - Y\n        grad_W_self_full = np.dot(delta_init, X) / N\n        grad_W_neigh_full = np.dot(delta_init, X_bar_full) / N\n        g_full = np.concatenate([grad_W_self_full, grad_W_neigh_full])\n        g_full_norm_sq = np.dot(g_full, g_full)\n\n        if g_full_norm_sq == 0 or num_neighbors_to_sample == NUM_TOTAL_NEIGHBORS:\n            noise_scale = 0.0\n        else:\n            total_squared_deviation = 0.0\n            grad_W_self_stoch = np.dot(delta_init, X) / N # Constant part\n            for _ in range(R_SAMPLES):\n                sampled_indices_r = np.empty((N, num_neighbors_to_sample), dtype=int)\n                for i in range(N):\n                    sampled_indices_r[i] = rng_noise.choice(\n                        adj_np[i], size=num_neighbors_to_sample, replace=False\n                    )\n                X_bar_sampled_noise = X[sampled_indices_r].mean(axis=1)\n\n                grad_W_neigh_stoch = np.dot(delta_init, X_bar_sampled_noise) / N\n                g_s = np.concatenate([grad_W_self_stoch, grad_W_neigh_stoch])\n\n                deviation = g_s - g_full\n                total_squared_deviation += np.dot(deviation, deviation)\n            \n            mean_squared_deviation = total_squared_deviation / R_SAMPLES\n            noise_scale = mean_squared_deviation / g_full_norm_sq\n\n        noise_scales.append(round(noise_scale, 6))\n\n    # --- Part 3: Final Output ---\n    s_str = f\"[{','.join(map(str, S_VALUES))}]\"\n    acc_str = f\"[{','.join([f'{acc:.4f}' for acc in accuracies])}]\"\n    cost_str = f\"[{','.join(map(str, costs))}]\"\n    noise_str = f\"[{','.join([f'{ns:.6f}' for ns in noise_scales])}]\"\n    \n    final_output = f\"[{s_str},{acc_str},{cost_str},{noise_str}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "While many GNN tasks focus on nodes, a large class of problems requires predicting properties of the entire graph, such as a molecule's toxicity or a social network's community structure. This requires a \"readout\" or \"pooling\" step that aggregates all node embeddings into a single, fixed-size graph representation. This exercise  guides you through comparing several common, permutation-invariant readout functions to understand their distinct behaviors and sensitivities, particularly in response to changes in graph size.",
            "id": "3106222",
            "problem": "You are given a family of permutation-invariant graph readout functions that map a multiset of node embeddings $\\{h_i\\}_{i=1}^{|V|}$, where $h_i \\in \\mathbb{R}^3$, to a graph-level embedding $g \\in \\mathbb{R}^3$. A graph-level classifier computes a logit $z \\in \\mathbb{R}$ via a fixed linear layer followed by a bias. The four readout functions to compare are: the elementwise sum, the elementwise maximum, the elementwise mean, and an attention-based pooling with softmax weights. The goal is to quantify how the classifier’s logit changes as the number of nodes $|V|$ varies under specified node-embedding generators. All computations are purely numerical and unitless.\n\nFundamental base and definitions:\n- A readout is permutation-invariant if it depends only on the multiset $\\{h_i\\}$ and not on the order of indices. For the following definitions, let $H = [h_1,\\dots,h_{|V|}]^\\top \\in \\mathbb{R}^{|V|\\times 3}$, where each $h_i \\in \\mathbb{R}^3$.\n- Sum readout: $g_{\\mathrm{sum}} = \\sum_{i=1}^{|V|} h_i$.\n- Max readout: $g_{\\mathrm{max}}[k] = \\max_{1 \\le i \\le |V|} h_i[k]$ for each coordinate $k \\in \\{1,2,3\\}$.\n- Mean readout: $g_{\\mathrm{mean}} = \\frac{1}{|V|} \\sum_{i=1}^{|V|} h_i$.\n- Attention readout: with a fixed attention vector $a \\in \\mathbb{R}^3$, define attention scores $s_i = a^\\top h_i$, attention weights $\\alpha_i = \\frac{\\exp(s_i)}{\\sum_{j=1}^{|V|} \\exp(s_j)}$, and $g_{\\mathrm{attn}} = \\sum_{i=1}^{|V|} \\alpha_i h_i$.\n\nClassifier:\n- The classifier parameters are fixed as $w = [0.5,\\,-1.0,\\,0.25]^\\top \\in \\mathbb{R}^3$ and $b = 0.1 \\in \\mathbb{R}$.\n- For any readout $g \\in \\mathbb{R}^3$, the logit is $z = w^\\top g + b$.\n\nAttention parameters:\n- The attention vector is fixed as $a = [0.3,\\,0.8,\\,-0.5]^\\top \\in \\mathbb{R}^3$.\n\nNode-embedding generators:\n- Linear generator “lin” with parameters $(a_0,b_0,c_0) \\in \\mathbb{R}^3$: for $i \\in \\{1,\\dots,n\\}$,\n  - $h_i[1] = a_0 + 0.1\\,i$,\n  - $h_i[2] = b_0 - 0.2\\,i$,\n  - $h_i[3] = (-1)^i\\,c_0 + 0.05\\,i$.\n- Centered generator “centered” (no additional parameters): for $i \\in \\{1,\\dots,n\\}$,\n  - $h_i[1] = 0.2\\,(i - \\frac{n+1}{2})$,\n  - $h_i[2] = 0.3\\,(-1)^i$,\n  - $h_i[3] = 0.0$.\n- Constant generator “const” with parameter $v \\in \\mathbb{R}^3$: for $i \\in \\{1,\\dots,n\\}$, $h_i = v$.\n\nFor each test pair, you must:\n- Generate two graphs with sizes $n_1$ and $n_2$ using the specified generator and parameters.\n- For each of the four readouts $r \\in \\{\\mathrm{sum},\\mathrm{max},\\mathrm{mean},\\mathrm{attn}\\}$, compute the corresponding logits $z_r(n_1)$ and $z_r(n_2)$.\n- Report the signed logit difference $\\Delta_r = z_r(n_2) - z_r(n_1)$.\n\nTest suite:\n- Pair $\\mathbf{P1}$ (happy path growth): generator “lin” with $(a_0,b_0,c_0) = (1.0,\\,0.5,\\,0.2)$, sizes $n_1 = 1$, $n_2 = 10$.\n- Pair $\\mathbf{P2}$ (moderate growth): generator “lin” with $(a_0,b_0,c_0) = (1.0,\\,0.5,\\,0.2)$, sizes $n_1 = 3$, $n_2 = 6$.\n- Pair $\\mathbf{P3}$ (edge case, identical nodes): generator “const” with $v = [0.4,\\,-0.1,\\,0.2]^\\top$, sizes $n_1 = 5$, $n_2 = 50$.\n- Pair $\\mathbf{P4}$ (near size-insensitive mean by symmetry): generator “centered”, sizes $n_1 = 9$, $n_2 = 10$.\n\nSensitivity to $|V|$ is reflected in the magnitude of $\\Delta_r$:\n- Large $|\\Delta_{\\mathrm{sum}}|$ indicates high sensitivity to $|V|$.\n- Small $|\\Delta_{\\mathrm{mean}}|$ suggests normalization with respect to $|V|$ when the empirical distribution of $\\{h_i\\}$ is stable.\n- $|\\Delta_{\\mathrm{max}}|$ changes only if new nodes introduce new coordinatewise extremes.\n- $|\\Delta_{\\mathrm{attn}}|$ does not scale with $|V|$ by construction of the softmax weights but may change if new nodes alter the attention allocation.\n\nYour task:\n- Implement a program that computes the list of signed differences for the four readouts for each pair in the exact order $[\\Delta_{\\mathrm{sum}},\\Delta_{\\mathrm{max}},\\Delta_{\\mathrm{mean}},\\Delta_{\\mathrm{attn}}]$.\n- Aggregate results across the pairs $\\mathbf{P1}$, $\\mathbf{P2}$, $\\mathbf{P3}$, $\\mathbf{P4}$ into a single flat list of length $16$ in the order $\\mathbf{P1}$, $\\mathbf{P2}$, $\\mathbf{P3}$, $\\mathbf{P4}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order described above. For example, the shape is $[x_1,x_2,\\dots,x_{16}]$, where each $x_j$ is a real number.",
            "solution": "The problem requires a systematic evaluation of four distinct graph readout functions—sum, max, mean, and attention-based pooling—by quantifying their sensitivity to the number of nodes in a graph, $|V|$. This sensitivity is measured by the change in a classifier's output logit, $\\Delta_z$, when the graph size changes from $n_1$ to $n_2$. The analysis is performed across a series of test pairs, each defined by a specific node-embedding generator and graph sizes.\n\nThe core components of the problem are defined as follows. All vectors are column vectors.\n\n**Classifier and Parameters**\nThe classifier computes a logit $z \\in \\mathbb{R}$ from a graph-level embedding $g \\in \\mathbb{R}^3$ using a fixed linear transformation and bias.\n- Weight vector: $w = [0.5, -1.0, 0.25]^\\top \\in \\mathbb{R}^3$\n- Bias: $b = 0.1 \\in \\mathbb{R}$\n- Logit formula: $z = w^\\top g + b$\n\n**Attention Mechanism**\nThe attention-based readout uses a fixed attention vector to score nodes.\n- Attention vector: $a = [0.3, 0.8, -0.5]^\\top \\in \\mathbb{R}^3$\n\n**Node-Embedding Generators**\nNode embeddings $h_i \\in \\mathbb{R}^3$ for a graph of size $n$ are generated according to one of three specified functions, where $i \\in \\{1, \\dots, n\\}$.\n1.  **Linear Generator (“lin”)**: With parameters $(a_0, b_0, c_0)$, the $i$-th node embedding $h_i$ is given by:\n    $$\n    h_i = \\begin{bmatrix} a_0 + 0.1\\,i \\\\ b_0 - 0.2\\,i \\\\ (-1)^i\\,c_0 + 0.05\\,i \\end{bmatrix}\n    $$\n2.  **Centered Generator (“centered”)**: The $i$-th node embedding $h_i$ for a graph of size $n$ is:\n    $$\n    h_i = \\begin{bmatrix} 0.2\\,(i - \\frac{n+1}{2}) \\\\ 0.3\\,(-1)^i \\\\ 0.0 \\end{bmatrix}\n    $$\n3.  **Constant Generator (“const”)**: With parameter $v \\in \\mathbb{R}^3$, all node embeddings are identical: $h_i = v$.\n\n**Readout Functions**\nLet $\\{h_i\\}_{i=1}^{n}$ be the multiset of node embeddings. The graph embedding $g$ is computed as follows:\n1.  **Sum Readout**: $g_{\\mathrm{sum}} = \\sum_{i=1}^{n} h_i$. This function's output scales linearly with $n$ if the mean of embeddings is non-zero.\n2.  **Max Readout**: $g_{\\mathrm{max}}$ is the elementwise maximum over all node embeddings. For each coordinate $k \\in \\{1,2,3\\}$:\n    $$\n    g_{\\mathrm{max}}[k] = \\max_{1 \\le i \\le n} h_i[k]\n    $$\n3.  **Mean Readout**: $g_{\\mathrm{mean}} = \\frac{1}{n} \\sum_{i=1}^{n} h_i$. This normalizes the sum readout by the number of nodes.\n4.  **Attention Readout**: This function computes a weighted sum of node embeddings, where weights are determined by an attention mechanism.\n    - Attention scores: $s_i = a^\\top h_i$\n    - Attention weights (softmax): $\\alpha_i = \\frac{\\exp(s_i)}{\\sum_{j=1}^{n} \\exp(s_j)}$\n    - Graph embedding: $g_{\\mathrm{attn}} = \\sum_{i=1}^{n} \\alpha_i h_i$. By construction, $\\sum_i \\alpha_i = 1$, so this is a convex combination.\n\n**Target Calculation**\nFor each readout function $r \\in \\{\\mathrm{sum}, \\mathrm{max}, \\mathrm{mean}, \\mathrm{attn}\\}$ and each test pair with graph sizes $n_1$ and $n_2$, we compute the signed logit difference:\n$$\n\\Delta_r = z_r(n_2) - z_r(n_1)\n$$\nSubstituting the logit definition, $z_r(n) = w^\\top g_r(n) + b$, we get:\n$$\n\\Delta_r = (w^\\top g_r(n_2) + b) - (w^\\top g_r(n_1) + b) = w^\\top (g_r(n_2) - g_r(n_1))\n$$\nThis quantity measures the change in the classifier's output due to the change in graph size from $n_1$ to $n_2$ for a given readout mechanism.\n\nWe now proceed to compute these differences for each test pair.\n\n**Pair P1: generator \"lin\", $(a_0,b_0,c_0) = (1.0, 0.5, 0.2)$, $n_1 = 1, n_2 = 10$.**\n- Generate node embeddings for $n_1=1$ and $n_2=10$.\n- For each readout, compute $g(n_1)$, $g(n_2)$, and $\\Delta_r$.\n1.  $\\Delta_{\\mathrm{sum}}$: $g_{\\mathrm{sum}}(1) = [1.1, 0.3, -0.15]^\\top$. $g_{\\mathrm{sum}}(10) = \\sum_{i=1}^{10} h_i = [15.5, -6.0, 2.75]^\\top$.\n    $\\Delta_{\\mathrm{sum}} = w^\\top (g_{\\mathrm{sum}}(10) - g_{\\mathrm{sum}}(1)) = w^\\top [14.4, -6.3, 2.9]^\\top = 14.225$.\n2.  $\\Delta_{\\mathrm{max}}$: $g_{\\mathrm{max}}(1) = [1.1, 0.3, -0.15]^\\top$. For $n_2=10$, $h_i[1]$ is max at $i=10$, $h_i[2]$ is max at $i=1$, and $h_i[3]$ is max at the largest even $i$, i.e., $i=10$. This gives $g_{\\mathrm{max}}(10) = [2.0, 0.3, 0.7]^\\top$.\n    $\\Delta_{\\mathrm{max}} = w^\\top (g_{\\mathrm{max}}(10) - g_{\\mathrm{max}}(1)) = w^\\top [0.9, 0.0, 0.85]^\\top = 0.6625$.\n3.  $\\Delta_{\\mathrm{mean}}$: $g_{\\mathrm{mean}}(1) = g_{\\mathrm{sum}}(1)$. $g_{\\mathrm{mean}}(10) = g_{\\mathrm{sum}}(10)/10 = [1.55, -0.6, 0.275]^\\top$.\n    $\\Delta_{\\mathrm{mean}} = w^\\top (g_{\\mathrm{mean}}(10) - g_{\\mathrm{mean}}(1)) = w^\\top [0.45, -0.9, 0.425]^\\top = 1.23125$.\n4.  $\\Delta_{\\mathrm{attn}}$: For $n_1=1$, $g_{\\mathrm{attn}}(1)=h_1$. For $n_2=10$, we compute scores $s_i$, weights $\\alpha_i$, and $g_{\\mathrm{attn}}(10) = \\sum \\alpha_i h_i \\approx [1.229, 0.114, -0.063]^\\top$.\n    $\\Delta_{\\mathrm{attn}} = w^\\top (g_{\\mathrm{attn}}(10) - g_{\\mathrm{attn}}(1)) \\approx w^\\top [0.129, -0.186, 0.087]^\\top \\approx 0.272$.\n\n**Pair P2: generator \"lin\", $(a_0,b_0,c_0) = (1.0, 0.5, 0.2)$, $n_1 = 3, n_2 = 6$.**\n- A similar calculation with smaller growth.\n1.  $\\Delta_{\\mathrm{sum}}$: $g_{\\mathrm{sum}}(3) = [3.6, -0.3, -0.25]^\\top$. $g_{\\mathrm{sum}}(6) = [7.5, -2.4, 1.2]^\\top$.\n    $\\Delta_{\\mathrm{sum}} = w^\\top (g_{\\mathrm{sum}}(6) - g_{\\mathrm{sum}}(3)) = w^\\top [3.9, -2.1, 1.45]^\\top = 4.4125$.\n2.  $\\Delta_{\\mathrm{max}}$: $g_{\\mathrm{max}}(3) = [1.3, 0.3, 0.3]^\\top$. $g_{\\mathrm{max}}(6) = [1.6, 0.3, 0.5]^\\top$.\n    $\\Delta_{\\mathrm{max}} = w^\\top (g_{\\mathrm{max}}(6) - g_{\\mathrm{max}}(3)) = w^\\top [0.3, 0.0, 0.2]^\\top = 0.2$.\n3.  $\\Delta_{\\mathrm{mean}}$: $g_{\\mathrm{mean}}(3) = [1.2, -0.1, -0.0833]^\\top$. $g_{\\mathrm{mean}}(6) = [1.25, -0.4, 0.2]^\\top$.\n    $\\Delta_{\\mathrm{mean}} = w^\\top (g_{\\mathrm{mean}}(6) - g_{\\mathrm{mean}}(3)) = w^\\top [0.05, -0.3, 0.2833]^\\top = 0.39583...$.\n4.  $\\Delta_{\\mathrm{attn}}$: $g_{\\mathrm{attn}}(3) \\approx [1.139, 0.222, -0.115]^\\top$. $g_{\\mathrm{attn}}(6) \\approx [1.162, 0.186, -0.091]^\\top$.\n    $\\Delta_{\\mathrm{attn}} = w^\\top (g_{\\mathrm{attn}}(6) - g_{\\mathrm{attn}}(3)) \\approx w^\\top [0.023, -0.036, 0.024]^\\top \\approx 0.0535$.\n\n**Pair P3: generator \"const\", $v = [0.4, -0.1, 0.2]^\\top$, $n_1 = 5, n_2 = 50$.**\n- All $h_i$ are identical to $v$.\n1.  $\\Delta_{\\mathrm{sum}}$: $g_{\\mathrm{sum}}(n) = n \\cdot v$. $\\Delta_{\\mathrm{sum}} = w^\\top(n_2 v - n_1 v) = (n_2-n_1) w^\\top v$.\n    $w^\\top v = 0.5(0.4) - 1.0(-0.1) + 0.25(0.2) = 0.35$.\n    $\\Delta_{\\mathrm{sum}} = (50-5) \\times 0.35 = 45 \\times 0.35 = 15.75$.\n2.  $\\Delta_{\\mathrm{max}}$: $g_{\\mathrm{max}}(n) = v$. $\\Delta_{\\mathrm{max}} = w^\\top(v-v) = 0$.\n3.  $\\Delta_{\\mathrm{mean}}$: $g_{\\mathrm{mean}}(n) = \\frac{1}{n}\\sum v = v$. $\\Delta_{\\mathrm{mean}} = w^\\top(v-v) = 0$.\n4.  $\\Delta_{\\mathrm{attn}}$: Scores $s_i = a^\\top v$ are constant, so weights $\\alpha_i = 1/n$. $g_{\\mathrm{attn}}(n) = \\sum \\frac{1}{n} v = v$.\n    $\\Delta_{\\mathrm{attn}} = w^\\top(v-v) = 0$.\n\n**Pair P4: generator \"centered\", $n_1 = 9, n_2 = 10$.**\n- This generator produces embeddings that sum to nearly zero.\n1.  $\\Delta_{\\mathrm{sum}}$: For $n_1=9$ (odd), $\\sum h_i[1]=0, \\sum h_i[2]=-0.3, \\sum h_i[3]=0$. So $g_{\\mathrm{sum}}(9) = [0, -0.3, 0]^\\top$.\n    For $n_2=10$ (even), $\\sum h_i[1]=0, \\sum h_i[2]=0, \\sum h_i[3]=0$. So $g_{\\mathrm{sum}}(10) = [0, 0, 0]^\\top$.\n    $\\Delta_{\\mathrm{sum}} = w^\\top(g_{\\mathrm{sum}}(10) - g_{\\mathrm{sum}}(9)) = w^\\top [0, 0.3, 0]^\\top = -0.3$.\n2.  $\\Delta_{\\mathrm{max}}$: For both $n=9$ and $n=10$, $h_i[1]$ is maximized at $i=1$, $h_i[2]$ is maximized at any even $i$. So $g_{\\mathrm{max}}$ does not change.\n    $g_{\\mathrm{max}}(9) = [0.2(9-5), 0.3, 0]^\\top = [0.8, 0.3, 0]^\\top$.\n    $g_{\\mathrm{max}}(10) = [0.2(10-5.5), 0.3, 0]^\\top = [0.9, 0.3, 0]^\\top$.\n    $\\Delta_{\\mathrm{max}} = w^\\top(g_{\\mathrm{max}}(10) - g_{\\mathrm{max}}(9)) = w^\\top [0.1, 0, 0]^\\top = 0.05$.\n3.  $\\Delta_{\\mathrm{mean}}$: $g_{\\mathrm{mean}}(9) = g_{\\mathrm{sum}}(9)/9 = [0, -0.3/9, 0]^\\top$. $g_{\\mathrm{mean}}(10) = [0,0,0]^\\top$.\n    $\\Delta_{\\mathrm{mean}} = w^\\top(g_{\\mathrm{mean}}(10) - g_{\\mathrm{mean}}(9)) = w^\\top [0, 0.3/9, 0]^\\top = -0.0333...$.\n4.  $\\Delta_{\\mathrm{attn}}$: $g_{\\mathrm{attn}}(9) \\approx [-0.499, -0.3, 0]^\\top$. $g_{\\mathrm{attn}}(10) \\approx [-0.599, 0.3, 0]^\\top$.\n    $\\Delta_{\\mathrm{attn}} = w^\\top(g_{\\mathrm{attn}}(10) - g_{\\mathrm{attn}}(9)) \\approx w^\\top [-0.1, 0.6, 0]^\\top \\approx -0.65$.\n\nThese calculations, when implemented numerically, yield the final list of results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes logit differences for four graph readout functions across four test pairs.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    w = np.array([0.5, -1.0, 0.25])\n    b = 0.1\n    a = np.array([0.3, 0.8, -0.5])\n\n    # --- Node Embedding Generators ---\n    def generate_embeddings(gen_type, n, params):\n        \"\"\"Generates a matrix of node embeddings H of shape (n, 3).\"\"\"\n        H = np.zeros((n, 3))\n        indices = np.arange(1, n + 1)\n        if gen_type == \"lin\":\n            a0, b0, c0 = params\n            H[:, 0] = a0 + 0.1 * indices\n            H[:, 1] = b0 - 0.2 * indices\n            H[:, 2] = ((-1)**indices) * c0 + 0.05 * indices\n        elif gen_type == \"centered\":\n            H[:, 0] = 0.2 * (indices - (n + 1) / 2)\n            H[:, 1] = 0.3 * ((-1)**indices)\n            H[:, 2] = 0.0\n        elif gen_type == \"const\":\n            v = params\n            H[:, :] = v\n        return H\n\n    # --- Readout Functions ---\n    def sum_readout(H):\n        return np.sum(H, axis=0)\n\n    def max_readout(H):\n        return np.max(H, axis=0)\n\n    def mean_readout(H):\n        return np.mean(H, axis=0)\n\n    def attn_readout(H, attn_vec):\n        if H.shape[0] == 0:\n            return np.zeros(3)\n        scores = H @ attn_vec\n        # Numerically stable softmax\n        scores -= np.max(scores)\n        exp_scores = np.exp(scores)\n        weights = exp_scores / np.sum(exp_scores)\n        return weights @ H\n\n    # --- Main Calculation Logic ---\n    def calculate_logit(g, weight_vec, bias):\n        return g @ weight_vec + bias\n\n    def calculate_deltas(case):\n        gen_type, params, n1, n2 = case\n        \n        H1 = generate_embeddings(gen_type, n1, params)\n        H2 = generate_embeddings(gen_type, n2, params)\n\n        readouts = {\n            \"sum\": sum_readout,\n            \"max\": max_readout,\n            \"mean\": mean_readout,\n            \"attn\": attn_readout\n        }\n\n        deltas = []\n        for r_name, r_func in readouts.items():\n            if r_name == \"attn\":\n                g1 = r_func(H1, a)\n                g2 = r_func(H2, a)\n            else:\n                g1 = r_func(H1)\n                g2 = r_func(H2)\n            \n            # Since z = w^T g + b, delta_z = (w^T g2 + b) - (w^T g1 + b) = w^T (g2 - g1)\n            delta_g = g2 - g1\n            delta_z = delta_g @ w\n            deltas.append(delta_z)\n            \n        return deltas\n\n    # --- Test Suite ---\n    test_cases = [\n        # Pair P1\n        (\"lin\", (1.0, 0.5, 0.2), 1, 10),\n        # Pair P2\n        (\"lin\", (1.0, 0.5, 0.2), 3, 6),\n        # Pair P3\n        (\"const\", np.array([0.4, -0.1, 0.2]), 5, 50),\n        # Pair P4\n        (\"centered\", None, 9, 10),\n    ]\n\n    # --- Execute and Format Output ---\n    all_results = []\n    for case in test_cases:\n        results_for_case = calculate_deltas(case)\n        all_results.extend(results_for_case)\n\n    # Format into a single comma-separated string in brackets\n    print(f\"[{','.join(f'{x:.10f}' for x in all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Most standard GNNs are implicitly designed around the principle of homophily, or the tendency for connected nodes to be similar. However, many real-world graphs exhibit heterophily, where connected nodes have dissimilar features or labels, posing a significant challenge to message-passing models. This practice  challenges you to explore this phenomenon by training a powerful Graph Isomorphism Network (GIN) on synthetic graphs with controlled levels of feature and label heterophily, providing insight into the limits of local aggregation.",
            "id": "3106259",
            "problem": "You are asked to design and analyze a small synthetic study of Graph Isomorphism Network (GIN) behavior under feature-level and structure-level heterophily. Your program must construct graphs in which the correlation of labels across edges, denoted $\\operatorname{corr}(y_i,y_j)$ for $(i,j)\\in E$, differs from the correlation of features across edges, denoted $\\operatorname{corr}(x_i,x_j)$ for $(i,j)\\in E$, in prescribed ways. You will then implement and train a two-layer GIN with sum aggregation and a node-level classifier, and report the training accuracy on each constructed graph.\n\nFundamental definitions that you must use as the base for your derivations and implementation:\n- A graph is a pair $G=(V,E)$, where $V=\\{1,\\dots,n\\}$ is the set of $n$ nodes and $E\\subseteq V\\times V$ is the set of undirected edges. The adjacency matrix is $A\\in\\{0,1\\}^{n\\times n}$ with $A_{ij}=1$ if and only if $(i,j)\\in E$ and $A_{ij}=A_{ji}$, and $A_{ii}=0$.\n- Each node $i$ has a feature vector $x_i\\in\\mathbb{R}^d$. Stack these row-wise into $X\\in\\mathbb{R}^{n\\times d}$. Each node has a class label $y_i\\in\\{0,1,\\dots,C-1\\}$, with $C=2$ for this problem.\n- The sample Pearson correlation across edges for a scalar attribute $z_i$ attached to node $i$ is computed over the multiset of edge pairs $\\{(z_i,z_j)\\mid (i,j)\\in E_{\\text{undir}}\\}$, where $E_{\\text{undir}}$ contains each undirected edge once. For vector features $x_i\\in\\mathbb{R}^d$, define a scalar summary $s_i=\\frac{1}{d}\\sum_{k=1}^d x_{ik}$ and compute $\\operatorname{corr}(s_i,s_j)$ across $(i,j)\\in E_{\\text{undir}}$. For labels $y_i\\in\\{0,1\\}$, compute $\\operatorname{corr}(y_i,y_j)$ across $(i,j)\\in E_{\\text{undir}}$.\n- A two-layer Graph Isomorphism Network (GIN) with sum aggregation produces hidden representations $H^{(0)}=X$, then for layer $\\ell\\in\\{1,2\\}$ computes an aggregated signal $S^{(\\ell)}=H^{(\\ell-1)}+A H^{(\\ell-1)}$, and applies a Multilayer Perceptron (MLP) with Rectified Linear Unit (ReLU) nonlinearity to obtain $H^{(\\ell)}=\\sigma\\big(\\mathrm{MLP}^{(\\ell)}\\big(S^{(\\ell)}\\big)\\big)$, where $\\sigma$ denotes the ReLU activation applied elementwise. Finally, node-level logits are computed as $Z=H^{(2)}W^{\\text{out}}+{\\bf 1} b^{\\text{out}\\top}$, with a softmax to obtain class probabilities. Training minimizes the average cross-entropy loss over nodes by batch gradient descent.\n\nRequirements:\n- Construct three synthetic test graphs, each with $n=32$ nodes and feature dimension $d=4$, and two classes $C=2$. For all constructions, use undirected simple graphs.\n  1. Case A (feature-heterophily with label-homophily):\n     - Build two disjoint complete bipartite graphs $K_{8,8}$ and $K_{8,8}$, forming a block-diagonal adjacency $A\\in\\{0,1\\}^{32\\times 32}$. Assign labels $y_i=0$ to all nodes in the first component and $y_i=1$ to all nodes in the second component. Construct features such that within each component, the two bipartitions have opposite-mean features: choose a common direction $u\\in\\mathbb{R}^4$ and set partition features approximately $+u$ and $-u$ (add small zero-mean noise), so that $\\operatorname{corr}(x_i,x_j)$ across edges is negative, while $\\operatorname{corr}(y_i,y_j)$ across edges is positive.\n  2. Case B (feature-homophily with label-heterophily):\n     - Build a single complete bipartite graph $K_{16,16}$. Assign labels by bipartition, with one side $y_i=0$ and the other side $y_i=1$, making $\\operatorname{corr}(y_i,y_j)$ across edges negative. Construct features that are similar across the two sides by setting features approximately $+u$ (same $u$ as above) on both sides with small noise, making $\\operatorname{corr}(x_i,x_j)$ across edges positive.\n  3. Case C (aligned homophily control):\n     - Build two disjoint complete bipartite graphs $K_{8,8}$ and $K_{8,8}$ as in Case A. Assign labels identical to Case A. Construct features that are similar across edges within each component by setting both bipartitions approximately $+u$ with small noise, so that both $\\operatorname{corr}(x_i,x_j)$ and $\\operatorname{corr}(y_i,y_j)$ across edges are positive.\n\n- Implement a two-layer GIN as defined above, with the following specifications:\n  - Use $H^{(0)}=X$, $S^{(\\ell)}=H^{(\\ell-1)}+A H^{(\\ell-1)}$, and for each layer $\\ell\\in\\{1,2\\}$ use an MLP with one hidden layer and ReLU: $\\mathrm{MLP}^{(\\ell)}(z)=\\mathrm{ReLU}(z W^{(\\ell)}_1 + b^{(\\ell)}_1) W^{(\\ell)}_2 + b^{(\\ell)}_2$, followed by an output ReLU to produce $H^{(\\ell)}$.\n  - Choose hidden sizes so that $W^{(1)}_1\\in\\mathbb{R}^{4\\times 16}$, $W^{(1)}_2\\in\\mathbb{R}^{16\\times 8}$, $W^{(2)}_1\\in\\mathbb{R}^{8\\times 16}$, $W^{(2)}_2\\in\\mathbb{R}^{16\\times 8}$, and $W^{\\text{out}}\\in\\mathbb{R}^{8\\times 2}$. Biases have matching dimensions.\n  - Train with batch gradient descent on the average cross-entropy loss for $200$ epochs with a fixed learning rate of $0.05$, and include $\\ell_2$ weight decay with coefficient $10^{-4}$ on all weight matrices (not on biases). Use a fixed random seed so that results are deterministic.\n  - Use ReLU activation $\\sigma(t)=\\max\\{0,t\\}$ elementwise and the softmax function for probabilities.\n\n- For each case, compute the training classification accuracy as a float in $[0,1]$ after training finishes. You may internally verify that $\\operatorname{corr}(y_i,y_j)$ and $\\operatorname{corr}(x_i,x_j)$ across edges have the intended sign relationship, but the final output must only report accuracies.\n\nTest suite and output specification:\n- The test suite consists exactly of the three cases described above with the specified sizes $n=32$, partitions of sizes $8$ and $16$, and feature dimension $d=4$.\n- Your program must output a single line containing the three training accuracies corresponding to Case A, Case B, and Case C, in that order, as a comma-separated list enclosed in square brackets, for example, $[a_1,a_2,a_3]$. Each $a_k$ must be a floating-point number in $[0,1]$. There are no physical units or angles in this problem.",
            "solution": "The user's request is to perform a synthetic study on the behavior of a Graph Isomorphism Network (GIN) under different conditions of feature and label heterophily. This involves constructing three specific graphs, implementing a two-layer GIN from first principles using `numpy`, training it on each graph, and reporting the final training accuracy.\n\n### Step 1: Problem Validation\n\nThe problem is analyzed against the specified validation criteria.\n\n-   **Extracted Givens**:\n    -   **Graph**: $G=(V,E)$, $n=32$ nodes, undirected, simple. Adjacency matrix $A \\in \\{0,1\\}^{32 \\times 32}$.\n    -   **Attributes**: Features $x_i \\in \\mathbb{R}^4$, labels $y_i \\in \\{0, 1\\}$.\n    -   **Correlation Metric**: Sample Pearson correlation over undirected edges for both label values and scalar feature summaries $s_i = \\frac{1}{d}\\sum_k x_{ik}$.\n    -   **Graph Cases**:\n        1.  **Case A**: Two disjoint $K_{8,8}$ graphs. Labels by component ($y=0$ for first, $y=1$ for second). Features are heterophilic across edges (opposite means for bipartitions).\n        2.  **Case B**: One $K_{16,16}$ graph. Labels by bipartition ($y=0$ for first, $y=1$ for second). Features are homophilic across edges (similar means for both partitions).\n        3.  **Case C**: Two disjoint $K_{8,8}$ graphs. Labels by component (as in A). Features are homophilic across edges (similar means for bipartitions).\n    -   **GIN Architecture**: Two-layer GIN with sum aggregation.\n        -   Input: $H^{(0)}=X$.\n        -   Layer Update: $S^{(\\ell)} = (I+A)H^{(\\ell-1)}$, $H^{(\\ell)} = \\mathrm{ReLU}(\\mathrm{MLP}^{(\\ell)}(S^{(\\ell)}))$.\n        -   MLP: $\\mathrm{MLP}^{(\\ell)}(z)=\\mathrm{ReLU}(z W^{(\\ell)}_1 + b^{(\\ell)}_1) W^{(\\ell)}_2 + b^{(\\ell)}_2$.\n        -   Layer Dimensions:\n            -   Layer 1: Input $4 \\to 16 \\to 8$.\n            -   Layer 2: Input $8 \\to 16 \\to 8$.\n            -   Output: Input $8 \\to 2$.\n    -   **Training**:\n        -   Batch gradient descent over all nodes.\n        -   Loss: Average cross-entropy.\n        -   Epochs: $200$.\n        -   Learning rate: $0.05$.\n        -   Regularization: $\\ell_2$ decay on weights with $\\lambda=10^{-4}$.\n        -   A fixed random seed must be used for deterministic results.\n    -   **Output**: A list of three floating-point training accuracies for cases A, B, and C.\n\n-   **Validation Verdict**:\n    1.  **Scientific Grounding**: The problem is well-grounded in the established theory of Graph Neural Networks, specifically GINs, and the research area of graph heterophily. The model architecture and training procedure are standard.\n    2.  **Well-Posedness**: The problem is well-posed. All parameters, architectural details, and training hyperparameters are specified, and a fixed random seed ensures a unique, deterministic outcome.\n    3.  **Objectivity**: The problem is stated in precise, objective mathematical and algorithmic language.\n    4.  **Completeness**: The problem provides a complete specification for implementation. The choice of a feature vector $u$ and noise distribution is a standard degree of freedom in such synthetic constructions, not a sign of underspecification. A reasonable canonical choice will be made.\n    \n    The problem is deemed **valid**.\n\n### Step 2: Solution Design and Implementation\n\nThe solution involves three main parts: data generation, model implementation, and training, all orchestrated within a single script.\n\n**1. Data Generation**\n\nThree functions will construct the adjacency matrix $A$, feature matrix $X$, and label vector $y$ for each case, adhering to the specified graph structures ($K_{8,8}$, $K_{16,16}$) and attribute distributions.\n-   A canonical feature direction vector $u = [0.5, 0.5, 0.5, 0.5]$ is chosen.\n-   Small, zero-mean Gaussian noise with $\\sigma=0.1$ is added to features to create approximate feature assignments as required.\n-   A `numpy` random number generator, seeded for reproducibility, is used for all stochastic operations.\n\n**2. GIN Model Implementation**\n\nA two-layer GIN is implemented from scratch using `numpy`. This requires careful implementation of the forward pass, loss calculation, and backpropagation.\n\n-   **Forward Pass**: The computation proceeds layer by layer as per the problem definition:\n    -   Aggregation: $S^{(\\ell)} = (I+A)H^{(\\ell-1)}$.\n    -   MLP Transformation: $H^{(\\ell)} = \\mathrm{ReLU}(\\mathrm{ReLU}(S^{(\\ell)}W_1^{(\\ell)} + b_1^{(\\ell)})W_2^{(\\ell)} + b_2^{(\\ell)})$.\n    -   Output Logits: $Z = H^{(2)}W^{\\text{out}} + b^{\\text{out}}$.\n    -   Probabilities: A numerically stable softmax function is applied to the logits $Z$.\n    Intermediate values (pre-activations) are cached for use in the backward pass.\n\n-   **Loss Function**: The average cross-entropy loss is computed, and an $\\ell_2$ regularization term is added for all weight matrices.\n    $$\n    L = -\\frac{1}{n} \\sum_{i=1}^{n} \\log(P_{i, y_i}) + \\frac{\\lambda}{2} \\sum_{k, j} \\left( (W_{j}^{(1,k)})^2 + (W_{j}^{(2,k)})^2 + (W_{j}^{\\text{out}})^2 \\right)\n    $$\n    where $P_{i, y_i}$ is the predicted probability for the true class of node $i$.\n\n-   **Backward Pass**: Gradients of the total loss with respect to all model parameters (weights and biases) are calculated using the chain rule. The process starts from the gradient of the loss with respect to the output logits, $\\frac{\\partial L}{\\partial Z} = \\frac{1}{n}(P - Y_{\\text{one-hot}})$, and propagates backward through each layer, including the MLP transformations, ReLU activations, and the graph aggregation steps. The gradient of the regularization term, $\\lambda W$, is added to the corresponding weight gradients.\n\n**3. Training and Evaluation**\n\n-   **Initialization**: Model parameters are initialized once per case using Glorot uniform initialization for weights and zeros for biases.\n-   **Training Loop**: For each of the three cases, the model is trained for $200$ epochs using batch gradient descent. In each epoch, a full forward and backward pass is performed, and parameters are updated according to the rule $\\theta \\leftarrow \\theta - \\eta \\frac{\\partial L}{\\partial \\theta}$.\n-   **Evaluation**: After training, the final training accuracy is computed by comparing the predicted class (from $\\mathrm{argmax}$ of the output logits) with the true labels for all nodes.\n\nThe final output is a list containing the three accuracy values, formatted as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GIN synthetic study problem.\n    This function contains all logic for data generation, model implementation,\n    training, and evaluation as required by the problem statement.\n    \"\"\"\n    \n    # --- Problem-defined Constants and Hyperparameters ---\n    N_NODES = 32\n    N_FEATURES = 4\n    N_CLASSES = 2\n    EPOCHS = 200\n    LEARNING_RATE = 0.05\n    L2_COEFF = 1e-4\n    NOISE_STD = 0.1\n    # Canonical choice for the feature direction vector\n    U_VEC = np.ones(N_FEATURES) / np.sqrt(N_FEATURES)\n    \n    # Use a random number generator for reproducible stochasticity\n    RNG = np.random.default_rng(42)\n\n    # --- Helper Functions for Data Generation ---\n    def _create_kmn(m, n):\n        \"\"\"Creates an adjacency matrix for a complete bipartite graph K_m,n.\"\"\"\n        adj = np.zeros((m + n, m + n))\n        adj[:m, m:] = 1\n        adj[m:, :m] = 1\n        return adj\n\n    def create_graph_data(case_id, rng):\n        \"\"\"Constructs graph adjacency, features, and labels for a given case.\"\"\"\n        A = np.zeros((N_NODES, N_NODES))\n        X = np.zeros((N_NODES, N_FEATURES))\n        y = np.zeros(N_NODES, dtype=int)\n\n        if case_id == 'A': # Feature-heterophily, label-homophily\n            k88 = _create_kmn(8, 8)\n            A[:16, :16] = k88\n            A[16:, 16:] = k88\n            y[16:] = 1\n            X[:8, :] = U_VEC + rng.normal(0, NOISE_STD, (8, N_FEATURES))\n            X[8:16, :] = -U_VEC + rng.normal(0, NOISE_STD, (8, N_FEATURES))\n            X[16:24, :] = U_VEC + rng.normal(0, NOISE_STD, (8, N_FEATURES))\n            X[24:32, :] = -U_VEC + rng.normal(0, NOISE_STD, (8, N_FEATURES))\n            \n        elif case_id == 'B': # Feature-homophily, label-heterophily\n            A = _create_kmn(16, 16)\n            y[16:] = 1\n            X[:16, :] = U_VEC + rng.normal(0, NOISE_STD, (16, N_FEATURES))\n            X[16:, :] = U_VEC + rng.normal(0, NOISE_STD, (16, N_FEATURES))\n\n        elif case_id == 'C': # Aligned homophily (control)\n            k88 = _create_kmn(8, 8)\n            A[:16, :16] = k88\n            A[16:, 16:] = k88\n            y[16:] = 1\n            X[:16, :] = U_VEC + rng.normal(0, NOISE_STD, (16, N_FEATURES))\n            X[16:, :] = U_VEC + rng.normal(0, NOISE_STD, (16, N_FEATURES))\n\n        return A, X, y\n\n    # --- GIN Model Implementation from scratch ---\n    \n    def init_params(rng):\n        \"\"\"Initializes model parameters using Glorot uniform initialization.\"\"\"\n        def glorot_uniform(fan_in, fan_out, rng_gen):\n            limit = np.sqrt(6 / (fan_in + fan_out))\n            return rng_gen.uniform(-limit, limit, (fan_in, fan_out))\n\n        params = {\n            'W1_1': glorot_uniform(4, 16, rng), 'b1_1': np.zeros(16),\n            'W1_2': glorot_uniform(16, 8, rng), 'b1_2': np.zeros(8),\n            'W2_1': glorot_uniform(8, 16, rng), 'b2_1': np.zeros(16),\n            'W2_2': glorot_uniform(16, 8, rng), 'b2_2': np.zeros(8),\n            'W_out': glorot_uniform(8, 2, rng), 'b_out': np.zeros(2)\n        }\n        return params\n\n    def relu(x):\n        return np.maximum(0, x)\n\n    def stable_softmax(z):\n        \"\"\"Numerically stable softmax function.\"\"\"\n        exps = np.exp(z - np.max(z, axis=1, keepdims=True))\n        return exps / np.sum(exps, axis=1, keepdims=True)\n\n    def forward_pass(params, A_tilde, X):\n        \"\"\"Performs a full forward pass and caches intermediate values.\"\"\"\n        # Layer 1\n        S1 = A_tilde @ X\n        Z1_1 = S1 @ params['W1_1'] + params['b1_1']\n        A1_1 = relu(Z1_1)\n        Z1_2 = A1_1 @ params['W1_2'] + params['b1_2']\n        H1 = relu(Z1_2)\n\n        # Layer 2\n        S2 = A_tilde @ H1\n        Z2_1 = S2 @ params['W2_1'] + params['b2_1']\n        A2_1 = relu(Z2_1)\n        Z2_2 = A2_1 @ params['W2_2'] + params['b2_2']\n        H2 = relu(Z2_2)\n\n        # Output Layer\n        Z_out = H2 @ params['W_out'] + params['b_out']\n        probs = stable_softmax(Z_out)\n\n        cache = {\n            'X': X, 'A_tilde': A_tilde, 'S1': S1, 'Z1_1': Z1_1, 'A1_1': A1_1, 'Z1_2': Z1_2, 'H1': H1,\n            'S2': S2, 'Z2_1': Z2_1, 'A2_1': A2_1, 'Z2_2': Z2_2, 'H2': H2, 'probs': probs\n        }\n        return probs, cache\n\n    def cross_entropy_loss(y_one_hot, probs, params, l2_coeff):\n        \"\"\"Calculates average cross-entropy loss with L2 regularization.\"\"\"\n        n = probs.shape[0]\n        core_loss = -np.sum(y_one_hot * np.log(probs + 1e-9)) / n\n        \n        l2_reg = 0\n        for key in ['W1_1', 'W1_2', 'W2_1', 'W2_2', 'W_out']:\n             l2_reg += np.sum(params[key]**2)\n        \n        return core_loss + (l2_coeff / 2) * l2_reg\n\n    def backward_pass(params, cache, y_one_hot, l2_coeff):\n        \"\"\"Performs backpropagation to compute gradients.\"\"\"\n        n = y_one_hot.shape[0]\n        grads = {}\n\n        # Gradient of loss wrt logits\n        dZ_out = (cache['probs'] - y_one_hot) / n\n\n        # Output layer gradients\n        grads['W_out'] = cache['H2'].T @ dZ_out + l2_coeff * params['W_out']\n        grads['b_out'] = np.sum(dZ_out, axis=0)\n        dH2 = dZ_out @ params['W_out'].T\n\n        # Layer 2 gradients\n        dZ2_2 = dH2 * (cache['Z2_2']  0)\n        grads['W2_2'] = cache['A2_1'].T @ dZ2_2 + l2_coeff * params['W2_2']\n        grads['b2_2'] = np.sum(dZ2_2, axis=0)\n        dA2_1 = dZ2_2 @ params['W2_2'].T\n        dZ2_1 = dA2_1 * (cache['Z2_1']  0)\n        grads['W2_1'] = cache['S2'].T @ dZ2_1 + l2_coeff * params['W2_1']\n        grads['b2_1'] = np.sum(dZ2_1, axis=0)\n        dS2 = dZ2_1 @ params['W2_1'].T\n        dH1 = cache['A_tilde'].T @ dS2 # A_tilde is symmetric\n\n        # Layer 1 gradients\n        dZ1_2 = dH1 * (cache['Z1_2']  0)\n        grads['W1_2'] = cache['A1_1'].T @ dZ1_2 + l2_coeff * params['W1_2']\n        grads['b1_2'] = np.sum(dZ1_2, axis=0)\n        dA1_1 = dZ1_2 @ params['W1_2'].T\n        dZ1_1 = dA1_1 * (cache['Z1_1']  0)\n        grads['W1_1'] = cache['S1'].T @ dZ1_1 + l2_coeff * params['W1_1']\n        grads['b1_1'] = np.sum(dZ1_1, axis=0)\n        \n        return grads\n\n    # --- Training and Evaluation ---\n    def train_and_evaluate(A, X, y, rng):\n        \"\"\"Trains the GIN model and returns the final training accuracy.\"\"\"\n        params = init_params(rng)\n        A_tilde = A + np.eye(N_NODES)\n        y_one_hot = np.eye(N_CLASSES)[y]\n        \n        for epoch in range(EPOCHS):\n            probs, cache = forward_pass(params, A_tilde, X)\n            # loss = cross_entropy_loss(y_one_hot, probs, params, L2_COEFF) # Optional: track loss\n            grads = backward_pass(params, cache, y_one_hot, L2_COEFF)\n            \n            for p_key in params:\n                params[p_key] -= LEARNING_RATE * grads[p_key]\n\n        # Final evaluation\n        final_probs, _ = forward_pass(params, A_tilde, X)\n        y_pred = np.argmax(final_probs, axis=1)\n        accuracy = np.mean(y_pred == y)\n        \n        return accuracy\n\n    # --- Main Execution Logic ---\n    results = []\n    test_cases = ['A', 'B', 'C']\n    \n    for case in test_cases:\n        A, X, y = create_graph_data(case, RNG)\n        accuracy = train_and_evaluate(A, X, y, RNG)\n        results.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n\n```"
        }
    ]
}