{
    "hands_on_practices": [
        {
            "introduction": "Graph Convolutional Networks (GCNs) form the bedrock of many modern graph learning models, operating on a principle of neighborhood aggregation. However, the standard normalization can sometimes allow high-degree 'hub' nodes to disproportionately influence their neighbors. This exercise  invites you to implement a modified GCN that introduces personalized caps on aggregated messages, allowing you to directly explore and quantify the effects of this mechanism on model fairness and stability.",
            "id": "3106143",
            "problem": "You are asked to implement a single-layer Graph Convolutional Network (GCN) variant with personalized degree caps on a fixed, undirected graph and to evaluate simple fairness and stability metrics under different capping schemes. Your implementation must follow the specified mathematical definitions and produce a single-line output that aggregates the results for multiple test cases.\n\nBase definitions to use:\n- A graph is given by an undirected adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ with $n$ nodes and no self-loops. Let $I$ denote the identity matrix of size $n \\times n$. Define $\\tilde{A} = A + I$ and $\\tilde{D}$ as the diagonal degree matrix of $\\tilde{A}$ with $\\tilde{D}_{ii} = \\sum_{j=1}^{n} \\tilde{A}_{ij}$.\n- The symmetrically normalized adjacency is $\\hat{A} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$.\n- Let $X \\in \\mathbb{R}^{n \\times d}$ be node features and $W \\in \\mathbb{R}^{d \\times h}$ be trainable weights; here both $d$ and $h$ are small integers. Use the Rectified Linear Unit (ReLU) activation $\\sigma(u) = \\max(0,u)$ applied elementwise.\n\nPersonalized degree caps:\n- For each node $i \\in \\{1,\\dots,n\\}$, define a personalized cap $c(i) \\in [0,\\infty]$ that limits the magnitude of the aggregated neighborhood message before the weight matrix is applied.\n- Given the pre-activation aggregated message $m_i \\in \\mathbb{R}^{d}$ computed as the $i$-th row of $\\hat{A} X$, define the capped message by projection onto the closed Euclidean ball of radius $c(i)$:\n$$\n\\Pi_{c(i)}(m_i) \\;=\\; \n\\begin{cases}\nm_i, & \\text{if } \\lVert m_i \\rVert_2 \\le c(i), \\\\\n\\dfrac{c(i)}{\\lVert m_i \\rVert_2} \\, m_i, & \\text{if } \\lVert m_i \\rVert_2 > c(i),\n\\end{cases}\n$$\nwith the conventions that $\\Pi_{\\infty}(m_i) = m_i$ and $\\Pi_{0}(m_i) = 0$.\n- The capped GCN layer output is then \n$$\nH \\;=\\; \\sigma\\!\\left(\\Pi_{c}(\\hat{A} X) \\, W\\right),\n$$\nwhere $\\Pi_{c}(\\hat{A} X)$ applies the projection rowwise with the appropriate $c(i)$.\n\nFairness and stability metrics:\n- Let $d(i)$ denote the degree of node $i$ in the original $A$ (excluding self-loops).\n- Define the fairness gap as follows. Let $k = \\max(1, \\lfloor n/4 \\rfloor)$. Let $S_{\\text{low}}$ be the indices of the $k$ smallest-degree nodes and $S_{\\text{high}}$ be the indices of the $k$ largest-degree nodes (ties can be broken by any consistent rule). Given an embedding matrix $H \\in \\mathbb{R}^{n \\times h}$, let $r_i = \\lVert H_{i,:} \\rVert_2$. The fairness gap is\n$$\n\\Delta_{\\text{fair}}(H) \\;=\\; \\left| \\frac{1}{|S_{\\text{high}}|} \\sum_{i \\in S_{\\text{high}}} r_i \\;-\\; \\frac{1}{|S_{\\text{low}}|} \\sum_{i \\in S_{\\text{low}}} r_i \\right|.\n$$\n- Define a stability metric for edge perturbations. Given two adjacency matrices $A$ and $A^{\\prime}$ that differ by a single undirected edge addition, and the same features $X$, weights $W$, and caps $c$, compute\n$$\n\\Delta_{\\text{stab}} \\;=\\; \\lVert H^{\\prime} - H \\rVert_{\\mathrm{F}},\n$$\nwhere $H$ is the output with $A$ and $H^{\\prime}$ is the output with $A^{\\prime}$, and $\\lVert \\cdot \\rVert_{\\mathrm{F}}$ denotes the Frobenius norm.\n\nGraph, features, and weights to use:\n- Number of nodes $n = 8$.\n- Undirected edge set $E$:\n$$\nE \\;=\\; \\{(0,1),(0,2),(0,3),(0,4),(0,5),(1,2),(2,3),(4,6),(6,7)\\}.\n$$\n- Thus $A_{ij} = 1$ if $(i,j) \\in E$ or $(j,i) \\in E$ and $A_{ij} = 0$ otherwise, for $i,j \\in \\{0,\\dots,7\\}$.\n- Node features $X \\in \\mathbb{R}^{8 \\times 2}$:\n$$\nX \\;=\\; \\begin{bmatrix}\n2.0 & -1.0 \\\\\n0.5 & 0.0 \\\\\n1.0 & 1.0 \\\\\n-1.5 & 0.5 \\\\\n0.0 & -0.5 \\\\\n3.0 & 1.5 \\\\\n-0.5 & 2.0 \\\\\n1.0 & -2.0\n\\end{bmatrix}.\n$$\n- Weight matrix $W \\in \\mathbb{R}^{2 \\times 2}$:\n$$\nW \\;=\\; \\begin{bmatrix}\n1.0 & -0.5 \\\\\n0.3 & 0.8\n\\end{bmatrix}.\n$$\n\nCap schedules to evaluate:\n- Degree-based cap with parameter $\\tau > 0$: for each node $i$, set\n$$\nc_{\\tau}(i) \\;=\\; \\frac{\\tau}{\\sqrt{d(i) + 1}}.\n$$\n- No-cap baseline: $c_{\\infty}(i) = \\infty$ for all $i$.\n- Zero-cap edge case: $c_{0}(i) = 0$ for all $i$.\n\nEdge perturbation for stability:\n- Define the perturbed graph $A^{\\prime}$ by adding the undirected edge $(0,7)$ to $A$.\n\nTest suite:\n- Test $1$ (fairness under caps): Compare the fairness gap between the no-cap baseline and degree-capped with $\\tau = 1.5$ on the original $A$. Output a boolean that is $true$ if the capped fairness gap is less than or equal to the baseline fairness gap, and $false$ otherwise.\n- Test $2$ (stability under caps): Compute $\\Delta_{\\text{stab}}$ for the no-cap baseline and for the degree-capped with $\\tau = 0.2$ between $A$ and $A^{\\prime}$. Output a boolean that is $true$ if the capped stability difference is less than or equal to the baseline stability difference, and $false$ otherwise.\n- Test $3$ (degenerate cap): With $c_{0}(i)=0$ for all $i$, verify that all outputs are exactly the zero vector. Output a boolean that is $true$ if every row of $H$ is the zero vector, and $false$ otherwise.\n- Test $4$ (numeric fairness report): Compute the fairness gap on the original $A$ with the degree-capped schedule using $\\tau = 0.8$. Output this fairness gap as a floating-point number rounded to six decimal places (use standard rounding).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in this order: $[$Test $1$ result, Test $2$ result, Test $3$ result, Test $4$ result$]$. For example, a valid output line looks like $[true,true,false,0.123456]$, where booleans are printed in lowercase as produced by the programming language’s default boolean string representation and the final floating-point number is rounded to six decimal places. No other text should be printed.",
            "solution": "The posed problem is valid as it is scientifically grounded in the principles of graph neural networks, is mathematically well-posed, and provides all necessary data and definitions for a unique solution. We will proceed with a step-by-step resolution.\n\nThe core of the problem is to implement a variant of a Graph Convolutional Network (GCN) layer and evaluate its properties. The layer's output, $H \\in \\mathbb{R}^{n \\times h}$, is given by the formula:\n$$\nH \\;=\\; \\sigma\\!\\left(\\Pi_{c}(\\hat{A} X) \\, W\\right)\n$$\nwhere $X \\in \\mathbb{R}^{n \\times d}$ is the input feature matrix, $W \\in \\mathbb{R}^{d \\times h}$ is the weight matrix, $\\sigma$ is the ReLU activation function, and $\\Pi_c$ is a row-wise projection operator that enforces a personalized cap $c(i)$ on the aggregated message for each node $i$. The matrix $\\hat{A}$ is the symmetrically normalized adjacency matrix, derived from the graph's original adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$.\n\nWe will first establish the foundational matrices and vectors based on the provided graph structure and data.\n\n**1. Graph Representation and Normalization**\n\nThe graph has $n=8$ nodes and a set of undirected edges $E$. The adjacency matrix $A$ is constructed such that $A_{ij} = 1$ if an edge exists between nodes $i$ and $j$, and $A_{ij} = 0$ otherwise. We add self-loops to form $\\tilde{A} = A + I$, where $I$ is the $8 \\times 8$ identity matrix. This ensures that a node's own features are included in its aggregated message.\n\nThe degree matrix $\\tilde{D}$ of $\\tilde{A}$ is a diagonal matrix where $\\tilde{D}_{ii}$ is the sum of the $i$-th row of $\\tilde{A}$, which is equivalent to $d(i) + 1$, where $d(i)$ is the degree of node $i$ in the original graph $A$. The degrees for the given graph are:\n$d = [d(0), \\dots, d(7)] = [5, 2, 3, 2, 2, 1, 2, 1]$.\nThus, the diagonal of $\\tilde{D}$ is $[\\tilde{d}(0), \\dots, \\tilde{d}(7)] = [6, 3, 4, 3, 3, 2, 3, 2]$.\n\nThe symmetrically normalized adjacency matrix $\\hat{A} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$ plays a crucial role. Its elements are $\\hat{A}_{ij} = \\frac{\\tilde{A}_{ij}}{\\sqrt{\\tilde{d}(i)}\\sqrt{\\tilde{d}(j)}}$. This normalization averages features from neighbors, preventing the scale of aggregated messages from being dominated by high-degree nodes.\n\n**2. Core Computational Function: Capped GCN Layer**\n\nWe will build a computational procedure that takes an adjacency matrix ($A$), features ($X$), weights ($W$), and a vector of cap values as input, and produces the output matrix $H$. The procedure is as follows:\n- Calculate $\\tilde{A}$, $\\tilde{D}$, and $\\hat{A}$ from the input $A$.\n- Compute the pre-activation aggregated messages $M = \\hat{A} X$. Each row $m_i$ of $M$ represents the aggregated information for node $i$.\n- Apply the personalized capping. For each row $m_i$, we calculate its L2 norm, $\\lVert m_i \\rVert_2$. If this norm exceeds the node's specific cap $c(i)$, we project the vector $m_i$ onto the boundary of the Euclidean ball of radius $c(i)$ by scaling it: $m_i \\leftarrow \\frac{c(i)}{\\lVert m_i \\rVert_2} m_i$. This operation is idempotent for vectors already within the ball. The specific cap schedules are defined in the problem. The \"no-cap\" baseline corresponds to $c(i)=\\infty$, and the \"zero-cap\" case corresponds to $c(i)=0$, which forces all messages to the zero vector.\n- Apply the linear transformation with the weight matrix $W$ to the capped messages, yielding $Z = \\Pi_c(M) W$.\n- Apply the element-wise ReLU activation function, $H = \\sigma(Z)$, to produce the final node embeddings.\n\n**3. Fairness and Stability Metrics**\n\n*Fairness Gap ($ \\Delta_{\\text{fair}} $):*\nThis metric assesses the disparity in the magnitudes of output embeddings between high-degree and low-degree nodes. We first identify the sets of node indices corresponding to the $k = \\max(1, \\lfloor n/4 \\rfloor) = \\max(1, \\lfloor 8/4 \\rfloor) = 2$ lowest degrees ($S_{\\text{low}}$) and $k=2$ highest degrees ($S_{\\text{high}}$). Sorting the nodes by degree gives $d=(1,1,2,2,2,2,3,5)$. A consistent tie-breaking rule (e.g., using the node index) yields $S_{\\text{low}} = \\{5, 7\\}$ and $S_{\\text{high}} = \\{0, 2\\}$. We then compute the average L2 norm of the output embeddings for each group and take the absolute difference: $\\Delta_{\\text{fair}}(H) = \\left| \\text{mean}_{i \\in S_{\\text{high}}} \\lVert H_{i,:} \\rVert_2 - \\text{mean}_{i \\in S_{\\text{low}}} \\lVert H_{i,:} \\rVert_2 \\right|$.\n\n*Stability ($\\Delta_{\\text{stab}}$):*\nThis metric measures the sensitivity of the GCN output to a small perturbation in the graph structure. We compute the Frobenius norm of the difference between the output embedding matrices $H$ and $H'$, calculated on the original graph $A$ and the perturbed graph $A'$ (with the added edge $(0,7)$), respectively: $\\Delta_{\\text{stab}} = \\lVert H' - H \\rVert_F$.\n\n**4. Execution of Test Suite**\n\nWe now apply these computations to each test case.\n\n**Test 1: Fairness under Caps**\n- We compute $H_{\\text{baseline}}$ using the no-cap schedule ($c(i) = \\infty$) on graph $A$.\n- We compute $H_{\\text{capped}}$ using the degree-based cap $c_{\\tau}(i) = \\frac{\\tau}{\\sqrt{d(i)+1}}$ with $\\tau=1.5$ on graph $A$.\n- We then calculate $\\Delta_{\\text{fair}}(H_{\\text{baseline}})$ and $\\Delta_{\\text{fair}}(H_{\\text{capped}})$.\n- The result is the boolean value of the expression $\\Delta_{\\text{fair}}(H_{\\text{capped}}) \\le \\Delta_{\\text{fair}}(H_{\\text{baseline}})$.\n\n**Test 2: Stability under Caps**\n- We define the perturbed graph $A'$ by adding the edge $(0,7)$ to $A$. This changes the degrees to $d'(0)=6$ and $d'(7)=2$.\n- For the baseline, we compute $H$ on $A$ and $H'$ on $A'$, both with no caps, and find $\\Delta_{\\text{stab, baseline}} = \\lVert H'_{\\text{baseline}} - H_{\\text{baseline}} \\rVert_F$.\n- For the capped case, with $\\tau=0.2$, we compute the caps for $A$ using $d$ and for $A'$ using $d'$. We then compute $H_{\\text{capped}}$ on $A$ and $H'_{\\text{capped}}$ on $A'$ and find $\\Delta_{\\text{stab, capped}} = \\lVert H'_{\\text{capped}} - H_{\\text{capped}} \\rVert_F$.\n- The result is the boolean value of $\\Delta_{\\text{stab, capped}} \\le \\Delta_{\\text{stab, baseline}}$.\n\n**Test 3: Degenerate Cap**\n- We use the zero-cap schedule, $c(i)=0$ for all $i$. The projection $\\Pi_0(m_i)$ results in the zero vector for any message $m_i$.\n- Therefore, the capped message matrix $\\Pi_0(\\hat{A}X)$ is a matrix of all zeros.\n- The subsequent linear transformation yields $Z = \\mathbf{0} \\cdot W = \\mathbf{0}$.\n- The final output is $H = \\sigma(\\mathbf{0}) = \\mathbf{0}$.\n- The test verifies if the computed $H$ is a zero matrix, which must be true by mathematical definition.\n\n**Test 4: Numeric Fairness Report**\n- We use the degree-based cap with $\\tau=0.8$ on the original graph $A$.\n- We compute the corresponding output $H$ and calculate its fairness gap $\\Delta_{\\text{fair}}(H)$.\n- The result is this floating-point value, rounded to $6$ decimal places.\n\nThe final output is an aggregation of these four results in the specified format. The following code implements this logic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem by executing the four test cases.\n    \"\"\"\n    # Graph, features, and weights definition.\n    n = 8\n    E = [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (1, 2), (2, 3), (4, 6), (6, 7)]\n    X = np.array([\n        [2.0, -1.0], [0.5, 0.0], [1.0, 1.0], [-1.5, 0.5],\n        [0.0, -0.5], [3.0, 1.5], [-0.5, 2.0], [1.0, -2.0]\n    ])\n    W = np.array([[1.0, -0.5], [0.3, 0.8]])\n\n    def build_adjacency(n_nodes, edges):\n        A = np.zeros((n_nodes, n_nodes))\n        for i, j in edges:\n            A[i, j] = 1\n            A[j, i] = 1\n        return A\n\n    A = build_adjacency(n, E)\n    d = A.sum(axis=1)\n\n    A_prime = build_adjacency(n, E + [(0, 7)])\n    d_prime = A_prime.sum(axis=1)\n\n    def _compute_h(A_matrix, X_matrix, W_matrix, cap_values):\n        \"\"\"\n        Computes the GCN layer output H for a given graph, features, weights, and caps.\n        \"\"\"\n        num_nodes = A_matrix.shape[0]\n        A_tilde = A_matrix + np.eye(num_nodes)\n        D_tilde_vec = A_tilde.sum(axis=1)\n        \n        # Guard against division by zero for isolated nodes, though none exist here.\n        D_tilde_inv_sqrt_vec = np.power(D_tilde_vec, -0.5)\n        D_tilde_inv_sqrt_vec[np.isinf(D_tilde_inv_sqrt_vec)] = 0.0\n        \n        D_inv_sqrt_mat = np.diag(D_tilde_inv_sqrt_vec)\n        A_hat = D_inv_sqrt_mat @ A_tilde @ D_inv_sqrt_mat\n\n        M = A_hat @ X_matrix\n        M_capped = M.copy()\n\n        for i in range(num_nodes):\n            cap = cap_values[i]\n            if np.isinf(cap):\n                continue\n            \n            norm_mi = np.linalg.norm(M_capped[i, :])\n            \n            if norm_mi > cap:\n                # The projection formula covers the c=0 case when norm > 0.\n                if norm_mi > 1e-9: # Avoid division by zero\n                    M_capped[i, :] = (cap / norm_mi) * M_capped[i, :]\n                else: # if norm is zero, it's already capped.\n                    pass\n        \n        Z = M_capped @ W_matrix\n        H = np.maximum(Z, 0)\n        return H\n\n    def _compute_fairness_gap(H_matrix, degree_vec):\n        \"\"\"\n        Computes the fairness gap for a given output embedding H.\n        \"\"\"\n        num_nodes = H_matrix.shape[0]\n        k = max(1, num_nodes // 4)\n        \n        node_indices = np.arange(num_nodes)\n        # Sort by degree, using node index as a consistent tie-breaker.\n        sorted_indices = np.lexsort((node_indices, degree_vec))\n        \n        s_low = sorted_indices[:k]\n        s_high = sorted_indices[-k:]\n        \n        r = np.linalg.norm(H_matrix, axis=1)\n        \n        avg_low_norm = np.mean(r[s_low])\n        avg_high_norm = np.mean(r[s_high])\n        \n        return np.abs(avg_high_norm - avg_low_norm)\n\n    results = []\n    \n    # Test 1: Fairness\n    caps_inf = np.full(n, np.inf)\n    H_baseline = _compute_h(A, X, W, caps_inf)\n    fairness_baseline = _compute_fairness_gap(H_baseline, d)\n    \n    tau1 = 1.5\n    caps_tau1 = tau1 / np.sqrt(d + 1)\n    H_capped_tau1 = _compute_h(A, X, W, caps_tau1)\n    fairness_capped_tau1 = _compute_fairness_gap(H_capped_tau1, d)\n    \n    test1_result = fairness_capped_tau1 = fairness_baseline\n    results.append(str(test1_result).lower())\n\n    # Test 2: Stability\n    H_prime_baseline = _compute_h(A_prime, X, W, caps_inf)\n    stab_baseline = np.linalg.norm(H_prime_baseline - H_baseline, 'fro')\n\n    tau2 = 0.2\n    caps_tau2_A = tau2 / np.sqrt(d + 1)\n    caps_tau2_A_prime = tau2 / np.sqrt(d_prime + 1)\n    \n    H_capped_A = _compute_h(A, X, W, caps_tau2_A)\n    H_capped_A_prime = _compute_h(A_prime, X, W, caps_tau2_A_prime)\n    stab_capped = np.linalg.norm(H_capped_A_prime - H_capped_A, 'fro')\n    \n    test2_result = stab_capped = stab_baseline\n    results.append(str(test2_result).lower())\n\n    # Test 3: Degenerate Cap\n    caps_zero = np.zeros(n)\n    H_zero = _compute_h(A, X, W, caps_zero)\n    test3_result = np.all(H_zero == 0)\n    results.append(str(test3_result).lower())\n\n    # Test 4: Numeric Fairness Report\n    tau4 = 0.8\n    caps_tau4 = tau4 / np.sqrt(d + 1)\n    H_capped_tau4 = _compute_h(A, X, W, caps_tau4)\n    fairness_tau4 = _compute_fairness_gap(H_capped_tau4, d)\n    \n    test4_result = f\"{fairness_tau4:.6f}\"\n    results.append(test4_result)\n\n    # Final print statement\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While GCNs are powerful, their reliance on the full adjacency matrix makes them difficult to scale to massive graphs. GraphSAGE addresses this by introducing a paradigm of sampling a fixed number of neighbors to generate node embeddings inductively. This hands-on practice  allows you to implement a simplified GraphSAGE model and empirically investigate the fundamental trade-off between neighbor sampling size, computational cost, and the accuracy of the final model.",
            "id": "3106236",
            "problem": "You will implement and study a simplified Graph Sample and Aggregate (GraphSAGE) model for node classification to quantify how the neighbor sampling size affects computational cost, training accuracy, and an empirical gradient noise scale. The setting is purely mathematical and algorithmic, with no physical units. All angles are irrelevant here.\n\nFundamental base and definitions to use:\n- Graph Sample and Aggregate (GraphSAGE) with mean aggregation: Given a graph with node features, the GraphSAGE mean aggregator for a node aggregates its neighbors’ features by averaging. You will use a linear model where the logit for a node is the sum of two linear forms, one applied to the node’s own features and one applied to the mean of sampled neighbor features.\n- Binary Cross-Entropy (BCE) loss with the logistic function: For a binary label $y \\in \\{0,1\\}$ and a logit $z \\in \\mathbb{R}$, the logistic function is $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. The BCE loss per sample is $-\\left(y \\log \\sigma(z) + (1-y)\\log(1-\\sigma(z))\\right)$. The derivative with respect to the logit is $\\sigma(z) - y$.\n- Expectation and variance: The empirical gradient noise scale will be defined as a normalized mean squared deviation of a stochastic gradient from the full-batch gradient under neighbor sampling.\n\nTask and setup:\n- Construct an undirected ring graph on $n=80$ nodes indexed by $v \\in \\{0,1,\\dots,79\\}$, where each node is connected to its $2$ nearest neighbors on each side. Formally, connect each node $v$ to $(v \\pm 1) \\bmod 80$ and $(v \\pm 2) \\bmod 80$. This yields a regular degree-$4$ graph.\n- Create node features $x_v \\in \\mathbb{R}^d$ with $d=8$, drawn from a standard normal distribution. Fix a true weight vector $q \\in \\mathbb{R}^d$, sampled from a standard normal distribution and then normalized to unit length. For each node $v$, define the full-neighbor mean feature $\\bar{x}_{\\mathcal{N}(v)} = \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} x_u$, where $\\mathcal{N}(v)$ denotes the set of all neighbors of $v$. Define the logit for data generation as $z_v^{\\text{true}} = q^\\top \\left( \\alpha x_v + (1-\\alpha)\\bar{x}_{\\mathcal{N}(v)} \\right)$ with $\\alpha = 0.5$. Generate labels deterministically by $y_v = \\mathbf{1}[z_v^{\\text{true}} \\ge 0]$.\n- Model: Use a one-layer linear GraphSAGE-style predictor with mean aggregation and logistic output. For a node $v$, sample a set of neighbors $\\mathcal{S}_s(v)$ by uniformly sampling without replacement $s$ neighbors from $\\mathcal{N}(v)$, clamped so that if $s \\ge |\\mathcal{N}(v)|$ you use all neighbors. Compute the sampled mean $\\bar{x}_{\\mathcal{S}_s(v)} = \\frac{1}{|\\mathcal{S}_s(v)|} \\sum_{u \\in \\mathcal{S}_s(v)} x_u$. The model logit is:\n$$\nz_v(W_{\\text{self}}, W_{\\text{neigh}}; s) = W_{\\text{self}}^\\top x_v + W_{\\text{neigh}}^\\top \\bar{x}_{\\mathcal{S}_s(v)},\n$$\nwith parameters $W_{\\text{self}} \\in \\mathbb{R}^d$ and $W_{\\text{neigh}} \\in \\mathbb{R}^d$ to be trained.\n- Training objective: Minimize the average Binary Cross-Entropy loss across all nodes using full-batch gradient descent, where the gradient for each node is computed using the sampled mean $\\bar{x}_{\\mathcal{S}_s(v)}$. For a node $v$, let $p_v = \\sigma(z_v)$ and error term $\\delta_v = p_v - y_v$. The per-node contribution to the parameter gradients is\n$$\n\\nabla_{W_{\\text{self}}} \\ell_v = \\delta_v \\, x_v, \\quad \\nabla_{W_{\\text{neigh}}} \\ell_v = \\delta_v \\, \\bar{x}_{\\mathcal{S}_s(v)}.\n$$\n- Training protocol: Initialize $W_{\\text{self}} = 0$ and $W_{\\text{neigh}} = 0$, use a fixed learning rate $\\eta = 0.1$, and perform $E=200$ epochs of full-batch gradient descent. At each epoch, for each node, resample $\\mathcal{S}_s(v)$. Use a fixed random seed for reproducibility for each $s$.\n- Evaluation: After training for each $s$, compute accuracy using full neighbors only (that is, replace $\\bar{x}_{\\mathcal{S}_s(v)}$ by $\\bar{x}_{\\mathcal{N}(v)}$) and predict $\\hat{y}_v = \\mathbf{1}[z_v \\ge 0]$. Report accuracy as a float in the interval $[0,1]$ rounded to four decimals.\n\nComputational cost proxy:\n- Define the per-epoch computational cost $C(s)$ as the total number of neighbor feature reads, that is,\n$$\nC(s) = \\sum_{v=0}^{n-1} \\min\\{s, |\\mathcal{N}(v)|\\} \\cdot d.\n$$\nFor the given regular graph with degree $4$, this simplifies to $C(s) = n \\cdot \\min\\{s, 4\\} \\cdot d$. Report $C(s)$ as an integer.\n\nEmpirical gradient noise scale:\n- Fix the parameters at the initial values $W_{\\text{self}}=0$ and $W_{\\text{neigh}}=0$. Define the full-batch gradient using full neighbors as\n$$\ng_{\\text{full}} = \\nabla \\left( \\frac{1}{n} \\sum_{v=0}^{n-1} \\ell_v \\right) \\Bigg|_{\\bar{x}_{\\mathcal{N}(v)}} \\in \\mathbb{R}^{2d}.\n$$\nFor a given sampling size $s$, define a stochastic full-batch gradient estimate using sampled neighbor means as\n$$\ng^{(r)}_s = \\nabla \\left( \\frac{1}{n} \\sum_{v=0}^{n-1} \\ell_v \\right) \\Bigg|_{\\bar{x}_{\\mathcal{S}^{(r)}_s(v)}} \\in \\mathbb{R}^{2d},\n$$\nwhere $\\mathcal{S}^{(r)}_s(v)$ denotes the $r$-th random sampling of neighbors for each node.\n- Estimate the empirical gradient noise scale by\n$$\n\\mathcal{G}(s) = \\frac{ \\mathbb{E}_r \\left[ \\left\\| g^{(r)}_s - g_{\\text{full}} \\right\\|_2^2 \\right] }{ \\left\\| g_{\\text{full}} \\right\\|_2^2 },\n$$\nwith the expectation approximated by averaging over $R=256$ independent samplings. Report $\\mathcal{G}(s)$ rounded to six decimals.\n\nTest suite:\n- Use the sampling sizes $s \\in \\{1, 2, 4, 100\\}$. Note that $s=100$ tests the boundary where $s \\ge |\\mathcal{N}(v)|$ for all $v$.\n- For all stochastic components, fix the random seed as follows:\n    - Data generation seed: $123$.\n    - Training seed per $s$: use $1000 + s$.\n    - Gradient noise estimation seed per $s$: use $2000 + s$.\n- Hyperparameters: $n=80$, $d=8$, $\\alpha=0.5$, $\\eta=0.1$, $E=200$, $R=256$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists with no spaces, in the order:\n$$\n\\big[ [s\\_1,\\dots,s\\_k], [\\text{acc}(s\\_1),\\dots,\\text{acc}(s\\_k)], [C(s\\_1),\\dots,C(s\\_k)], [\\mathcal{G}(s\\_1),\\dots,\\mathcal{G}(s\\_k)] \\big],\n$$\nwhere the accuracies are rounded to four decimals and the noise scales are rounded to six decimals. For this problem, $k=4$ and $[s_1,\\dots,s_4] = [1,2,4,100]$. An example of the exact formatting is\n$[[1,2,4,100],[0.9000,0.9200,0.9300,0.9300],[640,1280,2560,2560],[0.123456,0.100000,0.000001,0.000000]]$\nbut with your computed values.",
            "solution": "The user-provided problem is a well-defined computational task within the domain of graph neural networks and is therefore valid. The following solution implements the specified simplified GraphSample and Aggregate (GraphSAGE) model to analyze the effects of neighbor sampling.\n\n### 1. Graph and Data Synthesis\n\nFirst, we construct the computational graph and generate the synthetic dataset as per the problem's specifications.\n\n- **Graph Structure**: An undirected ring graph with $n=80$ nodes is created. Each node $v$ is connected to its four nearest neighbors, specifically the nodes indexed by $(v \\pm 1) \\pmod{80}$ and $(v \\pm 2) \\pmod{80}$. This results in a $k$-regular graph where the degree of every node is $|\\mathcal{N}(v)|=4$. An adjacency list is used to store the graph structure.\n- **Node Features**: For each node $v$, we generate a feature vector $x_v \\in \\mathbb{R}^d$ with dimension $d=8$. The components of each vector are drawn independently from a standard normal distribution, $\\mathcal{N}(0, 1)$. This process is seeded with a random seed of $123$ for reproducibility.\n- **True Labels**: The ground-truth labels are generated using a \"teacher\" model. A true weight vector $q \\in \\mathbb{R}^d$ is sampled from a standard normal distribution and then normalized to have a unit $\\ell_2$-norm, i.e., $\\|q\\|_2=1$. For each node $v$, its full-neighbor mean feature vector is computed as $\\bar{x}_{\\mathcal{N}(v)} = \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} x_u$. The true logit is then defined as $z_v^{\\text{true}} = q^\\top \\left( \\alpha x_v + (1-\\alpha)\\bar{x}_{\\mathcal{N}(v)} \\right)$, with $\\alpha=0.5$. The binary label for node $v$ is deterministically set to $y_v = \\mathbf{1}[z_v^{\\text{true}} \\ge 0]$.\n\n### 2. GraphSAGE Model and Training\n\nWe implement a one-layer linear GraphSAGE-style model for binary node classification. The process is iterated for each specified sampling size $s \\in \\{1, 2, 4, 100\\}$.\n\n- **Model Architecture**: For a given node $v$ and a sampling size $s$, we first form a neighbor subset $\\mathcal{S}_s(v)$ by sampling $s' = \\min\\{s, |\\mathcal{N}(v)|\\}$ neighbors uniformly without replacement from the full neighborhood $\\mathcal{N}(v)$. We then compute the sampled-neighbor mean feature, $\\bar{x}_{\\mathcal{S}_s(v)} = \\frac{1}{|\\mathcal{S}_s(v)|} \\sum_{u \\in \\mathcal{S}_s(v)} x_u$. The model's logit for node $v$ is a linear combination:\n$$\nz_v(W_{\\text{self}}, W_{\\text{neigh}}; s) = W_{\\text{self}}^\\top x_v + W_{\\text{neigh}}^\\top \\bar{x}_{\\mathcal{S}_s(v)}\n$$\nwhere $W_{\\text{self}} \\in \\mathbb{R}^d$ and $W_{\\text{neigh}} \\in \\mathbb{R}^d$ are the trainable weight vectors, initialized to zero vectors.\n\n- **Training Procedure**: The model is trained to minimize the average Binary Cross-Entropy (BCE) loss over all $n$ nodes. We use full-batch gradient descent for $E=200$ epochs with a learning rate of $\\eta = 0.1$. In each epoch:\n    1. For each node $v$, a new set of neighbors $\\mathcal{S}_s(v)$ is sampled. A specific random seed, $1000+s$, is used for the entire training process for a given $s$.\n    2. The model's prediction is computed using the logistic function, $p_v = \\sigma(z_v) = (1 + e^{-z_v})^{-1}$.\n    3. The error term for the BCE loss gradient is $\\delta_v = p_v - y_v$.\n    4. The per-node gradients are $\\nabla_{W_{\\text{self}}} \\ell_v = \\delta_v x_v$ and $\\nabla_{W_{\\text{neigh}}} \\ell_v = \\delta_v \\bar{x}_{\\mathcal{S}_s(v)}$.\n    5. These gradients are summed over all nodes to form the full-batch gradients: $\\nabla_{W} L = \\sum_{v=0}^{n-1} \\nabla_{W} \\ell_v$.\n    6. The weights are updated: $W \\leftarrow W - \\eta \\cdot \\frac{1}{n} \\nabla_{W} L$.\n\n### 3. Evaluation Metrics\n\nAfter training for each value of $s$, we compute the three required metrics: computational cost, accuracy, and gradient noise scale.\n\n- **Computational Cost ($C(s)$)**: This is a proxy for the computational work per epoch, defined as the total number of neighbor features accessed. For our $4$-regular graph, it simplifies to $C(s) = n \\cdot \\min\\{s, 4\\} \\cdot d$.\n\n- **Accuracy**: After $E=200$ training epochs, the model's performance is evaluated. Crucially, the evaluation is always performed using the full neighborhood information, regardless of the sampling size $s$ used during training. The evaluation logit is $z_v^{\\text{eval}} = W_{\\text{self}}^\\top x_v + W_{\\text{neigh}}^\\top \\bar{x}_{\\mathcal{N}(v)}$, and the prediction is $\\hat{y}_v = \\mathbf{1}[z_v^{\\text{eval}} \\ge 0]$. The accuracy is the fraction of correctly classified nodes, $\\frac{1}{n}\\sum_{v=0}^{n-1} \\mathbf{1}[\\hat{y}_v=y_v]$, rounded to four decimal places.\n\n- **Empirical Gradient Noise Scale ($\\mathcal{G}(s)$)**: This metric quantifies the noise introduced by neighbor sampling at the beginning of training (i.e., at initial weights $W_{\\text{self}}, W_{\\text{neigh}} = 0$). For this calculation, a dedicated seed of $2000+s$ is used.\n    1. **Full Gradient ($g_{\\text{full}}$)**: First, the \"true\" batch gradient is computed using the full neighborhood $\\mathcal{N}(v)$ for all nodes. At $W=0$, the logit $z_v=0$, prediction $p_v = \\sigma(0) = 0.5$, and error $\\delta_v = 0.5 - y_v$. The gradient is $g_{\\text{full}} = \\frac{1}{n} \\sum_{v=0}^{n-1} [\\delta_v x_v, \\delta_v \\bar{x}_{\\mathcal{N}(v)}]$.\n    2. **Stochastic Gradients ($g_s^{(r)}$)**: Next, we generate $R=256$ independent stochastic gradient estimates. Each estimate $g_s^{(r)}$ is computed similarly to $g_{\\text{full}}$, but using a newly sampled neighbor mean $\\bar{x}_{\\mathcal{S}^{(r)}_s(v)}$ for each node $v$.\n    3. **Noise Scale Calculation**: The noise scale is the ratio of the mean squared error of the stochastic gradient to the squared norm of the full gradient:\n    $$\n    \\mathcal{G}(s) = \\frac{ \\frac{1}{R} \\sum_{r=1}^{R} \\left\\| g^{(r)}_s - g_{\\text{full}} \\right\\|_2^2 }{ \\left\\| g_{\\text{full}} \\right\\|_2^2 }\n    $$\n    When $s \\ge 4$, sampling is deterministic (all neighbors are chosen), so $g_s^{(r)} = g_{\\text{full}}$ for all $r$, making $\\mathcal{G}(s)=0$. The final value is rounded to six decimal places.\n\nThis entire procedure is executed for each $s$ in the test suite, and the results are aggregated into the specified list-of-lists format.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a simplified GraphSAGE model analysis as specified in the problem.\n    This function performs data generation, model training, and evaluation for different\n    neighbor sampling sizes, and computes accuracy, computational cost, and gradient noise.\n    \"\"\"\n    # Hyperparameters and constants\n    N = 80\n    D = 8\n    ALPHA = 0.5\n    ETA = 0.1\n    EPOCHS = 200\n    R_SAMPLES = 256\n    S_VALUES = [1, 2, 4, 100]\n    NUM_TOTAL_NEIGHBORS = 4\n\n    # --- Part 1: Data and Graph Generation ---\n    rng_data = np.random.default_rng(123)\n\n    # 1.1. Graph Construction (undirected ring graph)\n    adj = {i: [] for i in range(N)}\n    for i in range(N):\n        for offset in [-2, -1, 1, 2]:\n            adj[i].append((i + offset) % N)\n    adj_np = np.array([sorted(adj[i]) for i in range(N)])\n\n    # 1.2. Feature Generation\n    X = rng_data.standard_normal(size=(N, D))\n\n    # 1.3. True Model and Label Generation\n    q_true = rng_data.standard_normal(size=D)\n    q_true /= np.linalg.norm(q_true)\n\n    # Pre-calculate full-neighbor means (vectorized)\n    X_bar_full = X[adj_np].mean(axis=1)\n\n    # Calculate true logits and labels\n    z_true = np.sum(q_true * (ALPHA * X + (1 - ALPHA) * X_bar_full), axis=1)\n    Y = (z_true >= 0).astype(int)\n\n    # --- Part 2: Main Loop over Sampling Sizes ---\n    accuracies = []\n    costs = []\n    noise_scales = []\n\n    for s in S_VALUES:\n        num_neighbors_to_sample = min(s, NUM_TOTAL_NEIGHBORS)\n\n        # 2.1. Computational Cost Proxy\n        cost = N * num_neighbors_to_sample * D\n        costs.append(cost)\n\n        # 2.2. Training\n        rng_train = np.random.default_rng(1000 + s)\n        W_self = np.zeros(D)\n        W_neigh = np.zeros(D)\n\n        for _ in range(EPOCHS):\n            # Sample neighbors for all nodes for this epoch\n            sampled_indices = np.empty((N, num_neighbors_to_sample), dtype=int)\n            for i in range(N):\n                sampled_indices[i] = rng_train.choice(\n                    adj_np[i], size=num_neighbors_to_sample, replace=False\n                )\n            \n            X_bar_sampled = X[sampled_indices].mean(axis=1)\n            \n            # Vectorized logit and gradient calculation\n            logits = np.sum(W_self * X, axis=1) + np.sum(W_neigh * X_bar_sampled, axis=1)\n            p = 1 / (1 + np.exp(-logits))\n            delta = p - Y\n            \n            grad_W_self_batch = np.dot(delta, X)\n            grad_W_neigh_batch = np.dot(delta, X_bar_sampled)\n\n            # Update weights\n            W_self -= ETA * grad_W_self_batch / N\n            W_neigh -= ETA * grad_W_neigh_batch / N\n        \n        # 2.3. Evaluation (Accuracy)\n        eval_logits = np.sum(W_self * X, axis=1) + np.sum(W_neigh * X_bar_full, axis=1)\n        y_hat = (eval_logits >= 0).astype(int)\n        accuracy = np.mean(y_hat == Y)\n        accuracies.append(round(accuracy, 4))\n        \n        # 2.4. Gradient Noise Scale\n        rng_noise = np.random.default_rng(2000 + s)\n        \n        # Calculate full gradient (g_full) at initial weights W=0\n        p_init = 0.5\n        delta_init = p_init - Y\n        grad_W_self_full = np.dot(delta_init, X) / N\n        grad_W_neigh_full = np.dot(delta_init, X_bar_full) / N\n        g_full = np.concatenate([grad_W_self_full, grad_W_neigh_full])\n        g_full_norm_sq = np.dot(g_full, g_full)\n\n        if g_full_norm_sq == 0 or num_neighbors_to_sample == NUM_TOTAL_NEIGHBORS:\n            noise_scale = 0.0\n        else:\n            total_squared_deviation = 0.0\n            grad_W_self_stoch = np.dot(delta_init, X) / N # Constant part\n            for _ in range(R_SAMPLES):\n                sampled_indices_r = np.empty((N, num_neighbors_to_sample), dtype=int)\n                for i in range(N):\n                    sampled_indices_r[i] = rng_noise.choice(\n                        adj_np[i], size=num_neighbors_to_sample, replace=False\n                    )\n                X_bar_sampled_noise = X[sampled_indices_r].mean(axis=1)\n\n                grad_W_neigh_stoch = np.dot(delta_init, X_bar_sampled_noise) / N\n                g_s = np.concatenate([grad_W_self_stoch, grad_W_neigh_stoch])\n\n                deviation = g_s - g_full\n                total_squared_deviation += np.dot(deviation, deviation)\n            \n            mean_squared_deviation = total_squared_deviation / R_SAMPLES\n            noise_scale = mean_squared_deviation / g_full_norm_sq\n\n        noise_scales.append(round(noise_scale, 6))\n\n    # --- Part 3: Final Output ---\n    s_str = f\"[{','.join(map(str, S_VALUES))}]\"\n    acc_str = f\"[{','.join([f'{acc:.4f}' for acc in accuracies])}]\"\n    cost_str = f\"[{','.join(map(str, costs))}]\"\n    noise_str = f\"[{','.join([f'{ns:.6f}' for ns in noise_scales])}]\"\n    \n    final_output = f\"[{s_str},{acc_str},{cost_str},{noise_str}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "The expressive power of a GNN is determined by its ability to distinguish different graph structures, with the Graph Isomorphism Network (GIN) representing a theoretical high-water mark. However, even powerful models operate on implicit assumptions, most commonly that of homophily—the idea that connected nodes are similar. In this advanced exercise , you will construct graphs with varying levels of feature and label heterophily to test the limits of GIN and gain a deeper understanding of its behavior when this core assumption is violated.",
            "id": "3106259",
            "problem": "You are asked to design and analyze a small synthetic study of Graph Isomorphism Network (GIN) behavior under feature-level and structure-level heterophily. Your program must construct graphs in which the correlation of labels across edges, denoted $\\operatorname{corr}(y_i,y_j)$ for $(i,j)\\in E$, differs from the correlation of features across edges, denoted $\\operatorname{corr}(x_i,x_j)$ for $(i,j)\\in E$, in prescribed ways. You will then implement and train a two-layer GIN with sum aggregation and a node-level classifier, and report the training accuracy on each constructed graph.\n\nFundamental definitions that you must use as the base for your derivations and implementation:\n- A graph is a pair $G=(V,E)$, where $V=\\{1,\\dots,n\\}$ is the set of $n$ nodes and $E\\subseteq V\\times V$ is the set of undirected edges. The adjacency matrix is $A\\in\\{0,1\\}^{n\\times n}$ with $A_{ij}=1$ if and only if $(i,j)\\in E$ and $A_{ij}=A_{ji}$, and $A_{ii}=0$.\n- Each node $i$ has a feature vector $x_i\\in\\mathbb{R}^d$. Stack these row-wise into $X\\in\\mathbb{R}^{n\\times d}$. Each node has a class label $y_i\\in\\{0,1,\\dots,C-1\\}$, with $C=2$ for this problem.\n- The sample Pearson correlation across edges for a scalar attribute $z_i$ attached to node $i$ is computed over the multiset of edge pairs $\\{(z_i,z_j)\\mid (i,j)\\in E_{\\text{undir}}\\}$, where $E_{\\text{undir}}$ contains each undirected edge once. For vector features $x_i\\in\\mathbb{R}^d$, define a scalar summary $s_i=\\frac{1}{d}\\sum_{k=1}^d x_{ik}$ and compute $\\operatorname{corr}(s_i,s_j)$ across $(i,j)\\in E_{\\text{undir}}$. For labels $y_i\\in\\{0,1\\}$, compute $\\operatorname{corr}(y_i,y_j)$ across $(i,j)\\in E_{\\text{undir}}$.\n- A two-layer Graph Isomorphism Network (GIN) with sum aggregation produces hidden representations $H^{(0)}=X$, then for layer $\\ell\\in\\{1,2\\}$ computes an aggregated signal $S^{(\\ell)}=H^{(\\ell-1)}+A H^{(\\ell-1)}$, and applies a Multilayer Perceptron (MLP) with Rectified Linear Unit (ReLU) nonlinearity to obtain $H^{(\\ell)}=\\sigma\\big(\\mathrm{MLP}^{(\\ell)}\\big(S^{(\\ell)}\\big)\\big)$, where $\\sigma$ denotes the ReLU activation applied elementwise. Finally, node-level logits are computed as $Z=H^{(2)}W^{\\text{out}}+{\\bf 1} b^{\\text{out}\\top}$, with a softmax to obtain class probabilities. Training minimizes the average cross-entropy loss over nodes by batch gradient descent.\n\nRequirements:\n- Construct three synthetic test graphs, each with $n=32$ nodes and feature dimension $d=4$, and two classes $C=2$. For all constructions, use undirected simple graphs.\n  1. Case A (feature-heterophily with label-homophily):\n     - Build two disjoint complete bipartite graphs $K_{8,8}$ and $K_{8,8}$, forming a block-diagonal adjacency $A\\in\\{0,1\\}^{32\\times 32}$. Assign labels $y_i=0$ to all nodes in the first component and $y_i=1$ to all nodes in the second component. Construct features such that within each component, the two bipartitions have opposite-mean features: choose a common direction $u\\in\\mathbb{R}^4$ and set partition features approximately $+u$ and $-u$ (add small zero-mean noise), so that $\\operatorname{corr}(x_i,x_j)$ across edges is negative, while $\\operatorname{corr}(y_i,y_j)$ across edges is positive.\n  2. Case B (feature-homophily with label-heterophily):\n     - Build a single complete bipartite graph $K_{16,16}$. Assign labels by bipartition, with one side $y_i=0$ and the other side $y_i=1$, making $\\operatorname{corr}(y_i,y_j)$ across edges negative. Construct features that are similar across the two sides by setting features approximately $+u$ (same $u$ as above) on both sides with small noise, making $\\operatorname{corr}(x_i,x_j)$ across edges positive.\n  3. Case C (aligned homophily control):\n     - Build two disjoint complete bipartite graphs $K_{8,8}$ and $K_{8,8}$ as in Case A. Assign labels identical to Case A. Construct features that are similar across edges within each component by setting both bipartitions approximately $+u$ with small noise, so that both $\\operatorname{corr}(x_i,x_j)$ and $\\operatorname{corr}(y_i,y_j)$ across edges are positive.\n\n- Implement a two-layer GIN as defined above, with the following specifications:\n  - Use $H^{(0)}=X$, $S^{(\\ell)}=H^{(\\ell-1)}+A H^{(\\ell-1)}$, and for each layer $\\ell\\in\\{1,2\\}$ use an MLP with one hidden layer and ReLU: $\\mathrm{MLP}^{(\\ell)}(z)=\\mathrm{ReLU}(z W^{(\\ell)}_1 + b^{(\\ell)}_1) W^{(\\ell)}_2 + b^{(\\ell)}_2$, followed by an output ReLU to produce $H^{(\\ell)}$.\n  - Choose hidden sizes so that $W^{(1)}_1\\in\\mathbb{R}^{4\\times 16}$, $W^{(1)}_2\\in\\mathbb{R}^{16\\times 8}$, $W^{(2)}_1\\in\\mathbb{R}^{8\\times 16}$, $W^{(2)}_2\\in\\mathbb{R}^{16\\times 8}$, and $W^{\\text{out}}\\in\\mathbb{R}^{8\\times 2}$. Biases have matching dimensions.\n  - Train with batch gradient descent on the average cross-entropy loss for $200$ epochs with a fixed learning rate of $0.05$, and include $\\ell_2$ weight decay with coefficient $10^{-4}$ on all weight matrices (not on biases). Use a fixed random seed so that results are deterministic.\n  - Use ReLU activation $\\sigma(t)=\\max\\{0,t\\}$ elementwise and the softmax function for probabilities.\n\n- For each case, compute the training classification accuracy as a float in $[0,1]$ after training finishes. You may internally verify that $\\operatorname{corr}(y_i,y_j)$ and $\\operatorname{corr}(x_i,x_j)$ across edges have the intended sign relationship, but the final output must only report accuracies.\n\nTest suite and output specification:\n- The test suite consists exactly of the three cases described above with the specified sizes $n=32$, partitions of sizes $8$ and $16$, and feature dimension $d=4$.\n- Your program must output a single line containing the three training accuracies corresponding to Case A, Case B, and Case C, in that order, as a comma-separated list enclosed in square brackets, for example, $[a_1,a_2,a_3]$. Each $a_k$ must be a floating-point number in $[0,1]$. There are no physical units or angles in this problem.",
            "solution": "The user's request is to perform a synthetic study on the behavior of a Graph Isomorphism Network (GIN) under different conditions of feature and label heterophily. This involves constructing three specific graphs, implementing a two-layer GIN from first principles using `numpy`, training it on each graph, and reporting the final training accuracy.\n\n### Step 1: Problem Validation\n\nThe problem is analyzed against the specified validation criteria.\n\n-   **Extracted Givens**:\n    -   **Graph**: $G=(V,E)$, $n=32$ nodes, undirected, simple. Adjacency matrix $A \\in \\{0,1\\}^{32 \\times 32}$.\n    -   **Attributes**: Features $x_i \\in \\mathbb{R}^4$, labels $y_i \\in \\{0, 1\\}$.\n    -   **Correlation Metric**: Sample Pearson correlation over undirected edges for both label values and scalar feature summaries $s_i = \\frac{1}{d}\\sum_k x_{ik}$.\n    -   **Graph Cases**:\n        1.  **Case A**: Two disjoint $K_{8,8}$ graphs. Labels by component ($y=0$ for first, $y=1$ for second). Features are heterophilic across edges (opposite means for bipartitions).\n        2.  **Case B**: One $K_{16,16}$ graph. Labels by bipartition ($y=0$ for first, $y=1$ for second). Features are homophilic across edges (similar means for both partitions).\n        3.  **Case C**: Two disjoint $K_{8,8}$ graphs. Labels by component (as in A). Features are homophilic across edges (similar means for bipartitions).\n    -   **GIN Architecture**: Two-layer GIN with sum aggregation.\n        -   Input: $H^{(0)}=X$.\n        -   Layer Update: $S^{(\\ell)} = (I+A)H^{(\\ell-1)}$, $H^{(\\ell)} = \\mathrm{ReLU}(\\mathrm{MLP}^{(\\ell)}(S^{(\\ell)}))$.\n        -   MLP: $\\mathrm{MLP}^{(\\ell)}(z)=\\mathrm{ReLU}(z W^{(\\ell)}_1 + b^{(\\ell)}_1) W^{(\\ell)}_2 + b^{(\\ell)}_2$.\n        -   Layer Dimensions:\n            -   Layer 1: Input $4 \\to 16 \\to 8$.\n            -   Layer 2: Input $8 \\to 16 \\to 8$.\n            -   Output: Input $8 \\to 2$.\n    -   **Training**:\n        -   Batch gradient descent over all nodes.\n        -   Loss: Average cross-entropy.\n        -   Epochs: $200$.\n        -   Learning rate: $0.05$.\n        -   Regularization: $\\ell_2$ decay on weights with $\\lambda=10^{-4}$.\n        -   A fixed random seed must be used for deterministic results.\n    -   **Output**: A list of three floating-point training accuracies for cases A, B, and C.\n\n-   **Validation Verdict**:\n    1.  **Scientific Grounding**: The problem is well-grounded in the established theory of Graph Neural Networks, specifically GINs, and the research area of graph heterophily. The model architecture and training procedure are standard.\n    2.  **Well-Posedness**: The problem is well-posed. All parameters, architectural details, and training hyperparameters are specified, and a fixed random seed ensures a unique, deterministic outcome.\n    3.  **Objectivity**: The problem is stated in precise, objective mathematical and algorithmic language.\n    4.  **Completeness**: The problem provides a complete specification for implementation. The choice of a feature vector $u$ and noise distribution is a standard degree of freedom in such synthetic constructions, not a sign of underspecification. A reasonable canonical choice will be made.\n    \n    The problem is deemed **valid**.\n\n### Step 2: Solution Design and Implementation\n\nThe solution involves three main parts: data generation, model implementation, and training, all orchestrated within a single script.\n\n**1. Data Generation**\n\nThree functions will construct the adjacency matrix $A$, feature matrix $X$, and label vector $y$ for each case, adhering to the specified graph structures ($K_{8,8}$, $K_{16,16}$) and attribute distributions.\n-   A canonical feature direction vector $u = [0.5, 0.5, 0.5, 0.5]$ is chosen.\n-   Small, zero-mean Gaussian noise with $\\sigma=0.1$ is added to features to create approximate feature assignments as required.\n-   A `numpy` random number generator, seeded for reproducibility, is used for all stochastic operations.\n\n**2. GIN Model Implementation**\n\nA two-layer GIN is implemented from scratch using `numpy`. This requires careful implementation of the forward pass, loss calculation, and backpropagation.\n\n-   **Forward Pass**: The computation proceeds layer by layer as per the problem definition:\n    -   Aggregation: $S^{(\\ell)} = (I+A)H^{(\\ell-1)}$.\n    -   MLP Transformation: $H^{(\\ell)} = \\mathrm{ReLU}(\\mathrm{ReLU}(S^{(\\ell)}W_1^{(\\ell)} + b_1^{(\\ell)})W_2^{(\\ell)} + b_2^{(\\ell)})$.\n    -   Output Logits: $Z = H^{(2)}W^{\\text{out}} + b^{\\text{out}}$.\n    -   Probabilities: A numerically stable softmax function is applied to the logits $Z$.\n    Intermediate values (pre-activations) are cached for use in the backward pass.\n\n-   **Loss Function**: The average cross-entropy loss is computed, and an $\\ell_2$ regularization term is added for all weight matrices.\n    $$\n    L = -\\frac{1}{n} \\sum_{i=1}^{n} \\log(P_{i, y_i}) + \\frac{\\lambda}{2} \\sum_{k, j} \\left( (W_{j}^{(1,k)})^2 + (W_{j}^{(2,k)})^2 + (W_{j}^{\\text{out}})^2 \\right)\n    $$\n    where $P_{i, y_i}$ is the predicted probability for the true class of node $i$.\n\n-   **Backward Pass**: Gradients of the total loss with respect to all model parameters (weights and biases) are calculated using the chain rule. The process starts from the gradient of the loss with respect to the output logits, $\\frac{\\partial L}{\\partial Z} = \\frac{1}{n}(P - Y_{\\text{one-hot}})$, and propagates backward through each layer, including the MLP transformations, ReLU activations, and the graph aggregation steps. The gradient of the regularization term, $\\lambda W$, is added to the corresponding weight gradients.\n\n**3. Training and Evaluation**\n\n-   **Initialization**: Model parameters are initialized once per case using Glorot uniform initialization for weights and zeros for biases.\n-   **Training Loop**: For each of the three cases, the model is trained for $200$ epochs using batch gradient descent. In each epoch, a full forward and backward pass is performed, and parameters are updated according to the rule $\\theta \\leftarrow \\theta - \\eta \\frac{\\partial L}{\\partial \\theta}$.\n-   **Evaluation**: After training, the final training accuracy is computed by comparing the predicted class (from $\\mathrm{argmax}$ of the output logits) with the true labels for all nodes.\n\nThe final output is a list containing the three accuracy values, formatted as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GIN synthetic study problem.\n    This function contains all logic for data generation, model implementation,\n    training, and evaluation as required by the problem statement.\n    \"\"\"\n    \n    # --- Problem-defined Constants and Hyperparameters ---\n    N_NODES = 32\n    N_FEATURES = 4\n    N_CLASSES = 2\n    EPOCHS = 200\n    LEARNING_RATE = 0.05\n    L2_COEFF = 1e-4\n    NOISE_STD = 0.1\n    # Canonical choice for the feature direction vector\n    U_VEC = np.ones(N_FEATURES) / np.sqrt(N_FEATURES)\n    \n    # Use a random number generator for reproducible stochasticity\n    RNG = np.random.default_rng(42)\n\n    # --- Helper Functions for Data Generation ---\n    def _create_kmn(m, n):\n        \"\"\"Creates an adjacency matrix for a complete bipartite graph K_m,n.\"\"\"\n        adj = np.zeros((m + n, m + n))\n        adj[:m, m:] = 1\n        adj[m:, :m] = 1\n        return adj\n\n    def create_graph_data(case_id, rng):\n        \"\"\"Constructs graph adjacency, features, and labels for a given case.\"\"\"\n        A = np.zeros((N_NODES, N_NODES))\n        X = np.zeros((N_NODES, N_FEATURES))\n        y = np.zeros(N_NODES, dtype=int)\n\n        if case_id == 'A': # Feature-heterophily, label-homophily\n            k88 = _create_kmn(8, 8)\n            A[:16, :16] = k88\n            A[16:, 16:] = k88\n            y[16:] = 1\n            X[:8, :] = U_VEC + rng.normal(0, NOISE_STD, (8, N_FEATURES))\n            X[8:16, :] = -U_VEC + rng.normal(0, NOISE_STD, (8, N_FEATURES))\n            X[16:24, :] = U_VEC + rng.normal(0, NOISE_STD, (8, N_FEATURES))\n            X[24:32, :] = -U_VEC + rng.normal(0, NOISE_STD, (8, N_FEATURES))\n            \n        elif case_id == 'B': # Feature-homophily, label-heterophily\n            A = _create_kmn(16, 16)\n            y[16:] = 1\n            X[:16, :] = U_VEC + rng.normal(0, NOISE_STD, (16, N_FEATURES))\n            X[16:, :] = U_VEC + rng.normal(0, NOISE_STD, (16, N_FEATURES))\n\n        elif case_id == 'C': # Aligned homophily (control)\n            k88 = _create_kmn(8, 8)\n            A[:16, :16] = k88\n            A[16:, 16:] = k88\n            y[16:] = 1\n            X[:16, :] = U_VEC + rng.normal(0, NOISE_STD, (16, N_FEATURES))\n            X[16:, :] = U_VEC + rng.normal(0, NOISE_STD, (16, N_FEATURES))\n\n        return A, X, y\n\n    # --- GIN Model Implementation from scratch ---\n    \n    def init_params(rng):\n        \"\"\"Initializes model parameters using Glorot uniform initialization.\"\"\"\n        def glorot_uniform(fan_in, fan_out, rng_gen):\n            limit = np.sqrt(6 / (fan_in + fan_out))\n            return rng_gen.uniform(-limit, limit, (fan_in, fan_out))\n\n        params = {\n            'W1_1': glorot_uniform(4, 16, rng), 'b1_1': np.zeros(16),\n            'W1_2': glorot_uniform(16, 8, rng), 'b1_2': np.zeros(8),\n            'W2_1': glorot_uniform(8, 16, rng), 'b2_1': np.zeros(16),\n            'W2_2': glorot_uniform(16, 8, rng), 'b2_2': np.zeros(8),\n            'W_out': glorot_uniform(8, 2, rng), 'b_out': np.zeros(2)\n        }\n        return params\n\n    def relu(x):\n        return np.maximum(0, x)\n\n    def stable_softmax(z):\n        \"\"\"Numerically stable softmax function.\"\"\"\n        exps = np.exp(z - np.max(z, axis=1, keepdims=True))\n        return exps / np.sum(exps, axis=1, keepdims=True)\n\n    def forward_pass(params, A_tilde, X):\n        \"\"\"Performs a full forward pass and caches intermediate values.\"\"\"\n        # Layer 1\n        S1 = A_tilde @ X\n        Z1_1 = S1 @ params['W1_1'] + params['b1_1']\n        A1_1 = relu(Z1_1)\n        Z1_2 = A1_1 @ params['W1_2'] + params['b1_2']\n        H1 = relu(Z1_2)\n\n        # Layer 2\n        S2 = A_tilde @ H1\n        Z2_1 = S2 @ params['W2_1'] + params['b2_1']\n        A2_1 = relu(Z2_1)\n        Z2_2 = A2_1 @ params['W2_2'] + params['b2_2']\n        H2 = relu(Z2_2)\n\n        # Output Layer\n        Z_out = H2 @ params['W_out'] + params['b_out']\n        probs = stable_softmax(Z_out)\n\n        cache = {\n            'X': X, 'A_tilde': A_tilde, 'S1': S1, 'Z1_1': Z1_1, 'A1_1': A1_1, 'Z1_2': Z1_2, 'H1': H1,\n            'S2': S2, 'Z2_1': Z2_1, 'A2_1': A2_1, 'Z2_2': Z2_2, 'H2': H2, 'probs': probs\n        }\n        return probs, cache\n\n    def cross_entropy_loss(y_one_hot, probs, params, l2_coeff):\n        \"\"\"Calculates average cross-entropy loss with L2 regularization.\"\"\"\n        n = probs.shape[0]\n        core_loss = -np.sum(y_one_hot * np.log(probs + 1e-9)) / n\n        \n        l2_reg = 0\n        for key in ['W1_1', 'W1_2', 'W2_1', 'W2_2', 'W_out']:\n             l2_reg += np.sum(params[key]**2)\n        \n        return core_loss + (l2_coeff / 2) * l2_reg\n\n    def backward_pass(params, cache, y_one_hot, l2_coeff):\n        \"\"\"Performs backpropagation to compute gradients.\"\"\"\n        n = y_one_hot.shape[0]\n        grads = {}\n\n        # Gradient of loss wrt logits\n        dZ_out = (cache['probs'] - y_one_hot) / n\n\n        # Output layer gradients\n        grads['W_out'] = cache['H2'].T @ dZ_out + l2_coeff * params['W_out']\n        grads['b_out'] = np.sum(dZ_out, axis=0)\n        dH2 = dZ_out @ params['W_out'].T\n\n        # Layer 2 gradients\n        dZ2_2 = dH2 * (cache['Z2_2'] > 0)\n        grads['W2_2'] = cache['A2_1'].T @ dZ2_2 + l2_coeff * params['W2_2']\n        grads['b2_2'] = np.sum(dZ2_2, axis=0)\n        dA2_1 = dZ2_2 @ params['W2_2'].T\n        dZ2_1 = dA2_1 * (cache['Z2_1'] > 0)\n        grads['W2_1'] = cache['S2'].T @ dZ2_1 + l2_coeff * params['W2_1']\n        grads['b2_1'] = np.sum(dZ2_1, axis=0)\n        dS2 = dZ2_1 @ params['W2_1'].T\n        dH1 = cache['A_tilde'].T @ dS2 # A_tilde is symmetric\n\n        # Layer 1 gradients\n        dZ1_2 = dH1 * (cache['Z1_2'] > 0)\n        grads['W1_2'] = cache['A1_1'].T @ dZ1_2 + l2_coeff * params['W1_2']\n        grads['b1_2'] = np.sum(dZ1_2, axis=0)\n        dA1_1 = dZ1_2 @ params['W1_2'].T\n        dZ1_1 = dA1_1 * (cache['Z1_1'] > 0)\n        grads['W1_1'] = cache['S1'].T @ dZ1_1 + l2_coeff * params['W1_1']\n        grads['b1_1'] = np.sum(dZ1_1, axis=0)\n        \n        return grads\n\n    # --- Training and Evaluation ---\n    def train_and_evaluate(A, X, y, rng):\n        \"\"\"Trains the GIN model and returns the final training accuracy.\"\"\"\n        params = init_params(rng)\n        A_tilde = A + np.eye(N_NODES)\n        y_one_hot = np.eye(N_CLASSES)[y]\n        \n        for epoch in range(EPOCHS):\n            probs, cache = forward_pass(params, A_tilde, X)\n            # loss = cross_entropy_loss(y_one_hot, probs, params, L2_COEFF) # Optional: track loss\n            grads = backward_pass(params, cache, y_one_hot, L2_COEFF)\n            \n            for p_key in params:\n                params[p_key] -= LEARNING_RATE * grads[p_key]\n\n        # Final evaluation\n        final_probs, _ = forward_pass(params, A_tilde, X)\n        y_pred = np.argmax(final_probs, axis=1)\n        accuracy = np.mean(y_pred == y)\n        \n        return accuracy\n\n    # --- Main Execution Logic ---\n    results = []\n    test_cases = ['A', 'B', 'C']\n    \n    for case in test_cases:\n        A, X, y = create_graph_data(case, RNG)\n        accuracy = train_and_evaluate(A, X, y, RNG)\n        results.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n\n```"
        }
    ]
}