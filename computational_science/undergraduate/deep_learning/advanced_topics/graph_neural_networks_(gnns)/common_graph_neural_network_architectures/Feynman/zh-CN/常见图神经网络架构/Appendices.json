{
    "hands_on_practices": [
        {
            "introduction": "GraphSAGE 是一种为处理大规模图而设计的归纳式学习框架，其核心思想是通过对邻居节点进行采样来控制计算成本，但这会引入随机性。本练习将通过一个编码实践，让你量化邻居采样大小对计算成本、模型精度和梯度噪声的影响 。通过这个过程，你将深入理解在 GNN 设计中效率与性能之间的关键权衡。",
            "id": "3106236",
            "problem": "您将实现并研究一个简化的图采样与聚合 (GraphSAGE) 模型，用于节点分类，以量化邻居采样大小如何影响计算成本、训练准确率和经验梯度噪声尺度。此设定纯属数学和算法层面，不涉及物理单位。所有角度在此均不相关。\n\n使用的基本原理和定义：\n- 采用均值聚合的图采样与聚合 (GraphSAGE)：给定一个带节点特征的图，一个节点的 GraphSAGE 均值聚合器通过对其邻居特征求平均值来进行聚合。您将使用一个线性模型，其中一个节点的 logit 是两个线性形式的和，一个应用于该节点自身的特征，另一个应用于采样邻居特征的均值。\n- 采用 logistic 函数的二元交叉熵 (BCE) 损失：对于一个二元标签 $y \\in \\{0,1\\}$ 和一个 logit $z \\in \\mathbb{R}$，logistic 函数为 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$。每个样本的 BCE 损失为 $-\\left(y \\log \\sigma(z) + (1-y)\\log(1-\\sigma(z))\\right)$。其关于 logit 的导数是 $\\sigma(z) - y$。\n- 期望与方差：经验梯度噪声尺度将定义为在邻居采样下，随机梯度与全批量梯度之间归一化的均方偏差。\n\n任务与设置：\n- 在 $n=80$ 个节点（索引为 $v \\in \\{0,1,\\dots,79\\}$）上构建一个无向环形图，其中每个节点与其两侧最近的 2 个邻居相连。形式上，将每个节点 $v$ 连接到 $(v \\pm 1) \\bmod 80$ 和 $(v \\pm 2) \\bmod 80$。这将产生一个度为 4 的正则图。\n- 创建节点特征 $x_v \\in \\mathbb{R}^d$（其中 $d=8$），从标准正态分布中抽取。固定一个真实权重向量 $q \\in \\mathbb{R}^d$，该向量从标准正态分布中采样，然后归一化为单位长度。对于每个节点 $v$，定义全邻居均值特征 $\\bar{x}_{\\mathcal{N}(v)} = \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} x_u$，其中 $\\mathcal{N}(v)$ 表示 $v$ 的所有邻居的集合。定义用于数据生成的 logit 为 $z_v^{\\text{true}} = q^\\top \\left( \\alpha x_v + (1-\\alpha)\\bar{x}_{\\mathcal{N}(v)} \\right)$，其中 $\\alpha = 0.5$。通过 $y_v = \\mathbf{1}[z_v^{\\text{true}} \\ge 0]$ 确定性地生成标签。\n- 模型：使用一个单层线性 GraphSAGE 风格的预测器，具有均值聚合和 logistic 输出。对于一个节点 $v$，通过从 $\\mathcal{N}(v)$ 中无放回地均匀采样 $s$ 个邻居来获得邻居集 $\\mathcal{S}_s(v)$。如果 $s \\ge |\\mathcal{N}(v)|$，则使用所有邻居。计算采样均值 $\\bar{x}_{\\mathcal{S}_s(v)} = \\frac{1}{|\\mathcal{S}_s(v)|} \\sum_{u \\in \\mathcal{S}_s(v)} x_u$。模型的 logit 为：\n$$\nz_v(W_{\\text{self}}, W_{\\text{neigh}}; s) = W_{\\text{self}}^\\top x_v + W_{\\text{neigh}}^\\top \\bar{x}_{\\mathcal{S}_s(v)},\n$$\n其中待训练的参数为 $W_{\\text{self}} \\in \\mathbb{R}^d$ 和 $W_{\\text{neigh}} \\in \\mathbb{R}^d$。\n- 训练目标：使用全批量梯度下降最小化所有节点的平均二元交叉熵损失，其中每个节点的梯度使用采样均值 $\\bar{x}_{\\mathcal{S}_s(v)}$ 计算。对于一个节点 $v$，令 $p_v = \\sigma(z_v)$，误差项为 $\\delta_v = p_v - y_v$。每个节点对参数梯度的贡献为\n$$\n\\nabla_{W_{\\text{self}}} \\ell_v = \\delta_v \\, x_v, \\quad \\nabla_{W_{\\text{neigh}}} \\ell_v = \\delta_v \\, \\bar{x}_{\\mathcal{S}_s(v)}.\n$$\n- 训练协议：初始化 $W_{\\text{self}} = 0$ 和 $W_{\\text{neigh}} = 0$，使用固定学习率 $\\eta = 0.1$，并执行 $E=200$ 个周期的全批量梯度下降。在每个周期中，为每个节点重新采样 $\\mathcal{S}_s(v)$。对于每个 $s$，为保证可复现性使用固定的随机种子。\n- 评估：对每个 $s$ 训练后，仅使用全邻居计算准确率（即，用 $\\bar{x}_{\\mathcal{N}(v)}$ 替换 $\\bar{x}_{\\mathcal{S}_s(v)}$）并预测 $\\hat{y}_v = \\mathbf{1}[z_v \\ge 0]$。将准确率报告为区间 $[0,1]$ 内的一个浮点数，四舍五入到四位小数。\n\n计算成本代理：\n- 将每个周期的计算成本 $C(s)$ 定义为邻居特征读取的总数，即\n$$\nC(s) = \\sum_{v=0}^{n-1} \\min\\{s, |\\mathcal{N}(v)|\\} \\cdot d.\n$$\n对于给定的度为 4 的正则图，这简化为 $C(s) = n \\cdot \\min\\{s, 4\\} \\cdot d$。将 $C(s)$ 报告为整数。\n\n经验梯度噪声尺度：\n- 将参数固定在初始值 $W_{\\text{self}}=0$ 和 $W_{\\text{neigh}}=0$。将使用全邻居的全批量梯度定义为\n$$\ng_{\\text{full}} = \\nabla \\left( \\frac{1}{n} \\sum_{v=0}^{n-1} \\ell_v \\right) \\Bigg|_{\\bar{x}_{\\mathcal{N}(v)}} \\in \\mathbb{R}^{2d}.\n$$\n对于给定的采样大小 $s$，使用采样邻居均值将随机全批量梯度估计定义为\n$$\ng^{(r)}_s = \\nabla \\left( \\frac{1}{n} \\sum_{v=0}^{n-1} \\ell_v \\right) \\Bigg|_{\\bar{x}_{\\mathcal{S}^{(r)}_s(v)}} \\in \\mathbb{R}^{2d},\n$$\n其中 $\\mathcal{S}^{(r)}_s(v)$ 表示对每个节点的邻居进行的第 $r$ 次随机采样。\n- 通过以下公式估计经验梯度噪声尺度\n$$\n\\mathcal{G}(s) = \\frac{ \\mathbb{E}_r \\left[ \\left\\| g^{(r)}_s - g_{\\text{full}} \\right\\|_2^2 \\right] }{ \\left\\| g_{\\text{full}} \\right\\|_2^2 },\n$$\n其中期望通过对 $R=256$ 次独立采样求平均来近似。将 $\\mathcal{G}(s)$ 报告为四舍五入到六位小数的值。\n\n测试套件：\n- 使用采样大小 $s \\in \\{1, 2, 4, 100\\}$。请注意，$s=100$ 测试了对于所有 $v$ 都有 $s \\ge |\\mathcal{N}(v)|$ 的边界情况。\n- 对于所有随机组件，按如下方式固定随机种子：\n    - 数据生成种子：$123$。\n    - 每个 $s$ 的训练种子：使用 $1000 + s$。\n    - 每个 $s$ 的梯度噪声估计种子：使用 $2000 + s$。\n- 超参数：$n=80$, $d=8$, $\\alpha=0.5$, $\\eta=0.1$, $E=200$, $R=256$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个无空格的逗号分隔列表的列表形式的结果，顺序如下：\n$$\n\\big[ [s\\_1,\\dots,s\\_k], [\\text{acc}(s\\_1),\\dots,\\text{acc}(s\\_k)], [C(s\\_1),\\dots,C(s\\_k)], [\\mathcal{G}(s\\_1),\\dots,\\mathcal{G}(s\\_k)] \\big],\n$$\n其中准确率四舍五入到四位小数，噪声尺度四舍五入到六位小数。对于本问题，$k=4$ 且 $[s_1,\\dots,s_4] = [1,2,4,100]$。精确格式的一个示例如下：\n$[[1,2,4,100],[0.9000,0.9200,0.9300,0.9300],[640,1280,2560,2560],[0.123456,0.100000,0.000001,0.000000]]$\n但需使用您计算出的值。",
            "solution": "用户提供的问题是图神经网络领域内一个定义明确的计算任务，因此是有效的。以下解决方案实现了指定的简化图采样与聚合 (GraphSAGE) 模型，以分析邻居采样的影响。\n\n### 1. 图与数据合成\n\n首先，我们根据问题的规格说明构建计算图并生成合成数据集。\n\n- **图结构**：创建一个包含 $n=80$ 个节点的无向环形图。每个节点 $v$ 与其四个最近的邻居相连，具体为索引为 $(v \\pm 1) \\pmod{80}$ 和 $(v \\pm 2) \\pmod{80}$ 的节点。这会产生一个 k-正则图，其中每个节点的度为 $|\\mathcal{N}(v)|=4$。使用邻接表来存储图结构。\n- **节点特征**：对于每个节点 $v$，我们生成一个维度为 $d=8$ 的特征向量 $x_v \\in \\mathbb{R}^d$。每个向量的分量都独立地从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取。为保证可复现性，该过程使用 123 作为随机种子。\n- **真实标签**：使用一个“教师”模型生成真实标签。一个真实权重向量 $q \\in \\mathbb{R}^d$ 从标准正态分布中采样，然后被归一化以具有单位 $\\ell_2$ 范数，即 $\\|q\\|_2=1$。对于每个节点 $v$，其全邻居均值特征向量计算为 $\\bar{x}_{\\mathcal{N}(v)} = \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} x_u$。然后将真实 logit 定义为 $z_v^{\\text{true}} = q^\\top \\left( \\alpha x_v + (1-\\alpha)\\bar{x}_{\\mathcal{N}(v)} \\right)$，其中 $\\alpha=0.5$。节点 $v$ 的二元标签被确定性地设置为 $y_v = \\mathbf{1}[z_v^{\\text{true}} \\ge 0]$。\n\n### 2. GraphSAGE 模型与训练\n\n我们为二元节点分类实现了一个单层线性 GraphSAGE 风格的模型。对每个指定的采样大小 $s \\in \\{1, 2, 4, 100\\}$，该过程都会被迭代执行。\n\n- **模型架构**：对于给定节点 $v$ 和采样大小 $s$，我们首先通过从全邻域 $\\mathcal{N}(v)$ 中无放回地均匀采样 $s' = \\min\\{s, |\\mathcal{N}(v)|\\}$ 个邻居来形成一个邻居子集 $\\mathcal{S}_s(v)$。然后我们计算采样邻居的均值特征 $\\bar{x}_{\\mathcal{S}_s(v)} = \\frac{1}{|\\mathcal{S}_s(v)|} \\sum_{u \\in \\mathcal{S}_s(v)} x_u$。模型对节点 $v$ 的 logit 是一个线性组合：\n$$\nz_v(W_{\\text{self}}, W_{\\text{neigh}}; s) = W_{\\text{self}}^\\top x_v + W_{\\text{neigh}}^\\top \\bar{x}_{\\mathcal{S}_s(v)}\n$$\n其中 $W_{\\text{self}} \\in \\mathbb{R}^d$ 和 $W_{\\text{neigh}} \\in \\mathbb{R}^d$ 是可训练的权重向量，初始化为零向量。\n\n- **训练过程**：训练模型以最小化所有 $n$ 个节点的平均二元交叉熵 (BCE) 损失。我们使用全批量梯度下降法，学习率为 $\\eta = 0.1$，进行 $E=200$ 个周期的训练。在每个周期中：\n    1. 对每个节点 $v$，采样一组新的邻居 $\\mathcal{S}_s(v)$。对于给定的 $s$，整个训练过程使用一个特定的随机种子 $1000+s$。\n    2. 使用 logistic 函数计算模型预测 $p_v = \\sigma(z_v) = (1 + e^{-z_v})^{-1}$。\n    3. BCE 损失梯度的误差项为 $\\delta_v = p_v - y_v$。\n    4. 每个节点的梯度为 $\\nabla_{W_{\\text{self}}} \\ell_v = \\delta_v x_v$ 和 $\\nabla_{W_{\\text{neigh}}} \\ell_v = \\delta_v \\bar{x}_{\\mathcal{S}_s(v)}$。\n    5. 这些梯度在所有节点上求和，形成全批量梯度：$\\nabla_{W} L = \\sum_{v=0}^{n-1} \\nabla_{W} \\ell_v$。\n    6. 权重更新：$W \\leftarrow W - \\eta \\cdot \\frac{1}{n} \\nabla_{W} L$。\n\n### 3. 评估指标\n\n对每个 $s$ 值训练后，我们计算三个所需指标：计算成本、准确率和梯度噪声尺度。\n\n- **计算成本 ($C(s)$)**：这是每个周期计算工作量的一个代理指标，定义为访问的邻居特征总数。对于我们的 4-正则图，它简化为 $C(s) = n \\cdot \\min\\{s, 4\\} \\cdot d$。\n\n- **准确率**：经过 $E=200$ 个训练周期后，评估模型的性能。关键的是，无论训练期间使用的采样大小 $s$ 是多少，评估始终使用全邻域信息进行。评估 logit 为 $z_v^{\\text{eval}} = W_{\\text{self}}^\\top x_v + W_{\\text{neigh}}^\\top \\bar{x}_{\\mathcal{N}(v)}$，预测为 $\\hat{y}_v = \\mathbf{1}[z_v^{\\text{eval}} \\ge 0]$。准确率是正确分类节点的比例，即 $\\frac{1}{n}\\sum_{v=0}^{n-1} \\mathbf{1}[\\hat{y}_v=y_v]$，四舍五入到四位小数。\n\n- **经验梯度噪声尺度 ($\\mathcal{G}(s)$)**：该指标量化了在训练开始时（即初始权重 $W_{\\text{self}}, W_{\\text{neigh}} = 0$ 时）由邻居采样引入的噪声。为此计算，使用一个专用的种子 $2000+s$。\n    1. **全梯度 ($g_{\\text{full}}$)**：首先，对所有节点使用全邻域 $\\mathcal{N}(v)$ 计算“真实”的批量梯度。在 $W=0$ 时，logit $z_v=0$，预测 $p_v = \\sigma(0) = 0.5$，误差为 $\\delta_v = 0.5 - y_v$。梯度为 $g_{\\text{full}} = \\frac{1}{n} \\sum_{v=0}^{n-1} [\\delta_v x_v, \\delta_v \\bar{x}_{\\mathcal{N}(v)}]$。\n    2. **随机梯度 ($g_s^{(r)}$)**：接下来，我们生成 $R=256$ 个独立的随机梯度估计。每个估计 $g_s^{(r)}$ 的计算方式与 $g_{\\text{full}}$ 类似，但对每个节点 $v$ 使用新采样的邻居均值 $\\bar{x}_{\\mathcal{S}^{(r)}_s(v)}$。\n    3. **噪声尺度计算**：噪声尺度是随机梯度的均方误差与全梯度范数平方的比值：\n    $$\n    \\mathcal{G}(s) = \\frac{ \\frac{1}{R} \\sum_{r=1}^{R} \\left\\| g^{(r)}_s - g_{\\text{full}} \\right\\|_2^2 }{ \\left\\| g_{\\text{full}} \\right\\|_2^2 }\n    $$\n    当 $s \\ge 4$ 时，采样是确定性的（所有邻居都被选中），因此对于所有 $r$，$g_s^{(r)} = g_{\\text{full}}$，使得 $\\mathcal{G}(s)=0$。最终值四舍五入到六位小数。\n\n整个程序对测试套件中的每个 $s$ 执行，并将结果聚合成指定的列表的列表格式。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a simplified GraphSAGE model analysis as specified in the problem.\n    This function performs data generation, model training, and evaluation for different\n    neighbor sampling sizes, and computes accuracy, computational cost, and gradient noise.\n    \"\"\"\n    # Hyperparameters and constants\n    N = 80\n    D = 8\n    ALPHA = 0.5\n    ETA = 0.1\n    EPOCHS = 200\n    R_SAMPLES = 256\n    S_VALUES = [1, 2, 4, 100]\n    NUM_TOTAL_NEIGHBORS = 4\n\n    # --- Part 1: Data and Graph Generation ---\n    rng_data = np.random.default_rng(123)\n\n    # 1.1. Graph Construction (undirected ring graph)\n    adj = {i: [] for i in range(N)}\n    for i in range(N):\n        for offset in [-2, -1, 1, 2]:\n            adj[i].append((i + offset) % N)\n    adj_np = np.array([sorted(adj[i]) for i in range(N)])\n\n    # 1.2. Feature Generation\n    X = rng_data.standard_normal(size=(N, D))\n\n    # 1.3. True Model and Label Generation\n    q_true = rng_data.standard_normal(size=D)\n    q_true /= np.linalg.norm(q_true)\n\n    # Pre-calculate full-neighbor means (vectorized)\n    X_bar_full = X[adj_np].mean(axis=1)\n\n    # Calculate true logits and labels\n    z_true = np.sum(q_true * (ALPHA * X + (1 - ALPHA) * X_bar_full), axis=1)\n    Y = (z_true >= 0).astype(int)\n\n    # --- Part 2: Main Loop over Sampling Sizes ---\n    accuracies = []\n    costs = []\n    noise_scales = []\n\n    for s in S_VALUES:\n        num_neighbors_to_sample = min(s, NUM_TOTAL_NEIGHBORS)\n\n        # 2.1. Computational Cost Proxy\n        cost = N * num_neighbors_to_sample * D\n        costs.append(cost)\n\n        # 2.2. Training\n        rng_train = np.random.default_rng(1000 + s)\n        W_self = np.zeros(D)\n        W_neigh = np.zeros(D)\n\n        for _ in range(EPOCHS):\n            # Sample neighbors for all nodes for this epoch\n            sampled_indices = np.empty((N, num_neighbors_to_sample), dtype=int)\n            for i in range(N):\n                sampled_indices[i] = rng_train.choice(\n                    adj_np[i], size=num_neighbors_to_sample, replace=False\n                )\n            \n            X_bar_sampled = X[sampled_indices].mean(axis=1)\n            \n            # Vectorized logit and gradient calculation\n            logits = np.sum(W_self * X, axis=1) + np.sum(W_neigh * X_bar_sampled, axis=1)\n            p = 1 / (1 + np.exp(-logits))\n            delta = p - Y\n            \n            grad_W_self_batch = np.dot(delta, X)\n            grad_W_neigh_batch = np.dot(delta, X_bar_sampled)\n\n            # Update weights\n            W_self -= ETA * grad_W_self_batch / N\n            W_neigh -= ETA * grad_W_neigh_batch / N\n        \n        # 2.3. Evaluation (Accuracy)\n        eval_logits = np.sum(W_self * X, axis=1) + np.sum(W_neigh * X_bar_full, axis=1)\n        y_hat = (eval_logits >= 0).astype(int)\n        accuracy = np.mean(y_hat == Y)\n        accuracies.append(round(accuracy, 4))\n        \n        # 2.4. Gradient Noise Scale\n        rng_noise = np.random.default_rng(2000 + s)\n        \n        # Calculate full gradient (g_full) at initial weights W=0\n        p_init = 0.5\n        delta_init = p_init - Y\n        grad_W_self_full = np.dot(delta_init, X) / N\n        grad_W_neigh_full = np.dot(delta_init, X_bar_full) / N\n        g_full = np.concatenate([grad_W_self_full, grad_W_neigh_full])\n        g_full_norm_sq = np.dot(g_full, g_full)\n\n        if g_full_norm_sq == 0 or num_neighbors_to_sample == NUM_TOTAL_NEIGHBORS:\n            noise_scale = 0.0\n        else:\n            total_squared_deviation = 0.0\n            grad_W_self_stoch = np.dot(delta_init, X) / N # Constant part\n            for _ in range(R_SAMPLES):\n                sampled_indices_r = np.empty((N, num_neighbors_to_sample), dtype=int)\n                for i in range(N):\n                    sampled_indices_r[i] = rng_noise.choice(\n                        adj_np[i], size=num_neighbors_to_sample, replace=False\n                    )\n                X_bar_sampled_noise = X[sampled_indices_r].mean(axis=1)\n\n                grad_W_neigh_stoch = np.dot(delta_init, X_bar_sampled_noise) / N\n                g_s = np.concatenate([grad_W_self_stoch, grad_W_neigh_stoch])\n\n                deviation = g_s - g_full\n                total_squared_deviation += np.dot(deviation, deviation)\n            \n            mean_squared_deviation = total_squared_deviation / R_SAMPLES\n            noise_scale = mean_squared_deviation / g_full_norm_sq\n\n        noise_scales.append(round(noise_scale, 6))\n\n    # --- Part 3: Final Output ---\n    s_str = f\"[{','.join(map(str, S_VALUES))}]\"\n    acc_str = f\"[{','.join([f'{acc:.4f}' for acc in accuracies])}]\"\n    cost_str = f\"[{','.join(map(str, costs))}]\"\n    noise_str = f\"[{','.join([f'{ns:.6f}' for ns in noise_scales])}]\"\n    \n    final_output = f\"[{s_str},{acc_str},{cost_str},{noise_str}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "许多图神经网络模型隐含地假设图具有同质性（即相连的节点相似），但在现实世界的许多场景中，异质性（即相连的节点不同）更为常见。本练习将引导你构建具有不同异质性特征的图，并测试图同构网络 (GIN) 在这些情况下的表现 。通过这个过程，你将学会如何诊断 GNN 模型的潜在弱点，并理解模型架构（如求和聚合器）与图结构特性之间的相互作用。",
            "id": "3106259",
            "problem": "要求您设计并分析一项关于图同构网络 (GIN) 在特征层面和结构层面异质性下行为的小型模拟研究。您的程序必须构建图，其中边上的标签相关性（对于 $(i,j)\\in E$ 记为 $\\operatorname{corr}(y_i,y_j)$）与边上的特征相关性（对于 $(i,j)\\in E$ 记为 $\\operatorname{corr}(x_i,x_j)$）以预先设定的方式有所不同。然后，您将实现并训练一个带有求和聚合的双层 GIN 和一个节点级分类器，并报告在每个构建的图上的训练准确率。\n\n您必须将以下基本定义用作推导和实现的基础：\n- 图是一个偶对 $G=(V,E)$，其中 $V=\\{1,\\dots,n\\}$ 是 $n$ 个节点的集合，$E\\subseteq V\\times V$ 是无向边的集合。邻接矩阵是 $A\\in\\{0,1\\}^{n\\times n}$，其中当且仅当 $(i,j)\\in E$ 时 $A_{ij}=1$，并且 $A_{ij}=A_{ji}$，$A_{ii}=0$。\n- 每个节点 $i$ 有一个特征向量 $x_i\\in\\mathbb{R}^d$。将这些向量按行堆叠成 $X\\in\\mathbb{R}^{n\\times d}$。每个节点有一个类别标签 $y_i\\in\\{0,1,\\dots,C-1\\}$，对于本问题，$C=2$。\n- 对于附加到节点 $i$ 的标量属性 $z_i$，其在边上的样本皮尔逊相关系数是基于边对的多重集 $\\{(z_i,z_j)\\mid (i,j)\\in E_{\\text{undir}}\\}$ 计算的，其中 $E_{\\text{undir}}$ 包含每条无向边一次。对于向量特征 $x_i\\in\\mathbb{R}^d$，定义一个标量摘要 $s_i=\\frac{1}{d}\\sum_{k=1}^d x_{ik}$，并计算在 $(i,j)\\in E_{\\text{undir}}$ 上的 $\\operatorname{corr}(s_i,s_j)$。对于标签 $y_i\\in\\{0,1\\}$，计算在 $(i,j)\\in E_{\\text{undir}}$ 上的 $\\operatorname{corr}(y_i,y_j)$。\n- 一个带有求和聚合的双层图同构网络 (GIN) 生成隐藏表示 $H^{(0)}=X$，然后在层 $\\ell\\in\\{1,2\\}$，计算聚合信号 $S^{(\\ell)}=H^{(\\ell-1)}+A H^{(\\ell-1)}$，并应用带修正线性单元 (ReLU) 非线性激活的多层感知器 (MLP) 来获得 $H^{(\\ell)}=\\sigma\\big(\\mathrm{MLP}^{(\\ell)}\\big(S^{(\\ell)}\\big)\\big)$，其中 $\\sigma$ 表示逐元素应用的 ReLU 激活函数。最后，节点级 logits 计算为 $Z=H^{(2)}W^{\\text{out}}+{\\bf 1} b^{\\text{out}\\top}$，并使用 softmax 函数获得类别概率。训练通过批量梯度下降最小化节点上的平均交叉熵损失。\n\n要求：\n- 构建三个模拟测试图，每个图有 $n=32$ 个节点、特征维度 $d=4$ 和两个类别 $C=2$。对于所有构造，使用无向简单图。\n  1. 情况 A（特征异质性与标签同质性）：\n     - 构建两个不相交的完全二分图 $K_{8,8}$ 和 $K_{8,8}$，形成一个块对角邻接矩阵 $A\\in\\{0,1\\}^{32\\times 32}$。将第一个连通分量中所有节点的标签设为 $y_i=0$，第二个连通分量中所有节点的标签设为 $y_i=1$。构造特征，使得在每个连通分量内部，两个二分部分具有相反均值的特征：选择一个公共方向 $u\\in\\mathbb{R}^4$，并将划分的特征大致设置为 $+u$ 和 $-u$（添加小的零均值噪声），从而使得边上的 $\\operatorname{corr}(x_i,x_j)$ 为负，而边上的 $\\operatorname{corr}(y_i,y_j)$ 为正。\n  2. 情况 B（特征同质性与标签异质性）：\n     - 构建一个单独的完全二分图 $K_{16,16}$。按二分部分分配标签，一侧为 $y_i=0$，另一侧为 $y_i=1$，使得边上的 $\\operatorname{corr}(y_i,y_j)$ 为负。通过在两侧都将特征大致设置为 $+u$（与上述 $u$ 相同）并添加小噪声，来构造在两边相似的特征，使得边上的 $\\operatorname{corr}(x_i,x_j)$ 为正。\n  3. 情况 C（对齐的同质性控制）：\n     - 如情况 A 一样，构建两个不相交的完全二分图 $K_{8,8}$ 和 $K_{8,8}$。分配与情况 A 相同的标签。通过将两个二分部分的特征都大致设置为 $+u$ 并添加小噪声，来构造在每个连通分量内部边上相似的特征，从而使得边上的 $\\operatorname{corr}(x_i,x_j)$ 和 $\\operatorname{corr}(y_i,y_j)$ 均为正。\n\n- 实现一个如上定义的双层 GIN，其具体规格如下：\n  - 使用 $H^{(0)}=X$，$S^{(\\ell)}=H^{(\\ell-1)}+A H^{(\\ell-1)}$，并且对于每个层 $\\ell\\in\\{1,2\\}$，使用一个带单隐藏层和 ReLU 的 MLP：$\\mathrm{MLP}^{(\\ell)}(z)=\\mathrm{ReLU}(z W^{(\\ell)}_1 + b^{(\\ell)}_1) W^{(\\ell)}_2 + b^{(\\ell)}_2$，然后接一个输出 ReLU 来产生 $H^{(\\ell)}$。\n  - 选择隐藏层大小，使得 $W^{(1)}_1\\in\\mathbb{R}^{4\\times 16}$，$W^{(1)}_2\\in\\mathbb{R}^{16\\times 8}$，$W^{(2)}_1\\in\\mathbb{R}^{8\\times 16}$，$W^{(2)}_2\\in\\mathbb{R}^{16\\times 8}$，以及 $W^{\\text{out}}\\in\\mathbb{R}^{8\\times 2}$。偏置项具有匹配的维度。\n  - 使用批量梯度下降针对平均交叉熵损失进行训练，共 200 个周期，固定学习率为 0.05，并在所有权重矩阵上（不在偏置项上）加入系数为 $10^{-4}$ 的 $\\ell_2$ 权重衰减。使用固定的随机种子以确保结果是确定性的。\n  - 逐元素使用 ReLU 激活函数 $\\sigma(t)=\\max\\{0,t\\}$，并使用 softmax 函数计算概率。\n\n- 对于每种情况，在训练结束后计算训练分类准确率，其结果为一个在 $[0,1]$ 区间内的浮点数。您可以在内部验证边上的 $\\operatorname{corr}(y_i,y_j)$ 和 $\\operatorname{corr}(x_i,x_j)$ 是否具有预期的符号关系，但最终输出必须只报告准确率。\n\n测试套件和输出规范：\n- 测试套件仅包含上述三种情况，具有指定的尺寸 $n=32$，大小为 8 和 16 的划分，以及特征维度 $d=4$。\n- 您的程序必须输出单行，其中包含按顺序对应的三种情况（情况 A、情况 B 和情况 C）的训练准确率，形式为用方括号括起来的逗号分隔列表，例如 $[a_1,a_2,a_3]$。每个 $a_k$ 必须是 $[0,1]$ 内的浮点数。本问题不涉及物理单位或角度。",
            "solution": "用户的要求是执行一项关于图同构网络 (GIN) 在不同特征和标签异质性条件下行为的模拟研究。这涉及构建三个特定的图，使用 `numpy` 从头开始实现一个双层 GIN，在每个图上对其进行训练，并报告最终的训练准确率。\n\n### 步骤 1：问题验证\n\n根据指定的验证标准对问题进行分析。\n\n-   **提取的已知条件**：\n    -   **图**：$G=(V,E)$，$n=32$ 个节点，无向，简单图。邻接矩阵 $A \\in \\{0,1\\}^{32 \\times 32}$。\n    -   **属性**：特征 $x_i \\in \\mathbb{R}^4$，标签 $y_i \\in \\{0, 1\\}$。\n    -   **相关性度量**：对标签值和标量特征摘要 $s_i = \\frac{1}{d}\\sum_k x_{ik}$，在无向边上计算样本皮尔逊相关系数。\n    -   **图的情况**：\n        1.  **情况 A**：两个不相交的 $K_{8,8}$ 图。按连通分量分配标签（第一个为 $y=0$，第二个为 $y=1$）。特征在边上是异质性的（二分部分具有相反的均值）。\n        2.  **情况 B**：一个 $K_{16,16}$ 图。按二分部分分配标签（第一个为 $y=0$，第二个为 $y=1$）。特征在边上是同质性的（两个划分具有相似的均值）。\n        3.  **情况 C**：两个不相交的 $K_{8,8}$ 图。按连通分量分配标签（同情况 A）。特征在边上是同质性的（二分部分具有相似的均值）。\n    -   **GIN 架构**：带有求和聚合的双层 GIN。\n        -   输入：$H^{(0)}=X$。\n        -   层更新：$S^{(\\ell)} = (I+A)H^{(\\ell-1)}$，$H^{(\\ell)} = \\mathrm{ReLU}(\\mathrm{MLP}^{(\\ell)}(S^{(\\ell)}))$。\n        -   MLP：$\\mathrm{MLP}^{(\\ell)}(z)=\\mathrm{ReLU}(z W^{(\\ell)}_1 + b^{(\\ell)}_1) W^{(\\ell)}_2 + b^{(\\ell)}_2$。\n        -   层维度：\n            -   第 1 层：输入 $4 \\to 16 \\to 8$。\n            -   第 2 层：输入 $8 \\to 16 \\to 8$。\n            -   输出：输入 $8 \\to 2$。\n    -   **训练**：\n        -   对所有节点进行批量梯度下降。\n        -   损失：平均交叉熵。\n        -   周期：200。\n        -   学习率：0.05。\n        -   正则化：对权重进行 $\\ell_2$ 衰减，$\\lambda=10^{-4}$。\n        -   必须使用固定的随机种子以保证结果的确定性。\n    -   **输出**：包含情况 A、B、C 的三个浮点训练准确率的列表。\n\n-   **验证结论**：\n    1.  **科学依据**：该问题在图神经网络（特别是 GIN）的既定理论以及图异质性研究领域中有充分的理论基础。模型架构和训练过程都是标准的。\n    2.  **适定性**：该问题是适定的。所有参数、架构细节和训练超参数都已指定，并且固定的随机种子确保了唯一、确定性的结果。\n    3.  **客观性**：问题以精确、客观的数学和算法语言陈述。\n    4.  **完整性**：问题为实现提供了完整的规范。特征向量 $u$ 和噪声分布的选择在此类模拟构造中是标准的自由度，而不是未指定清楚的标志。将做出一个合理的规范选择。\n    \n    该问题被判定为 **有效**。\n\n### 步骤 2：解决方案设计与实现\n\n该解决方案包括三个主要部分：数据生成、模型实现和训练，所有这些都在一个脚本中统一协调。\n\n**1. 数据生成**\n\n三个函数将为每种情况构建邻接矩阵 $A$、特征矩阵 $X$ 和标签向量 $y$，并遵循指定的图结构（$K_{8,8}$、$K_{16,16}$）和属性分布。\n-   选择一个规范的特征方向向量 $u = [0.5, 0.5, 0.5, 0.5]$。\n-   向特征中添加小的、均值为零、标准差为 $\\sigma=0.1$ 的高斯噪声，以按要求创建近似的特征分配。\n-   使用一个为可复现性设置了种子的 `numpy` 随机数生成器来执行所有随机操作。\n\n**2. GIN 模型实现**\n\n使用 `numpy` 从头开始实现一个双层 GIN。这需要仔细实现前向传播、损失计算和反向传播。\n\n-   **前向传播**：计算按照问题定义逐层进行：\n    -   聚合：$S^{(\\ell)} = (I+A)H^{(\\ell-1)}$。\n    -   MLP 变换：$H^{(\\ell)} = \\mathrm{ReLU}(\\mathrm{ReLU}(S^{(\\ell)}W_1^{(\\ell)} + b_1^{(\\ell)})W_2^{(\\ell)} + b_2^{(\\ell)})$。\n    -   输出 Logits：$Z = H^{(2)}W^{\\text{out}} + b^{\\text{out}}$。\n    -   概率：对 logits $Z$ 应用一个数值稳定的 softmax 函数。\n    -   中间值（激活前）被缓存起来，用于反向传播。\n\n-   **损失函数**：计算平均交叉熵损失，并为所有权重矩阵添加一个 $\\ell_2$ 正则化项。\n    $$\n    L = -\\frac{1}{n} \\sum_{i=1}^{n} \\log(P_{i, y_i}) + \\frac{\\lambda}{2} \\sum_{k, j} \\left( (W_{j}^{(1,k)})^2 + (W_{j}^{(2,k)})^2 + (W_{j}^{\\text{out}})^2 \\right)\n    $$\n    其中 $P_{i, y_i}$ 是节点 $i$ 真实类别的预测概率。\n\n-   **反向传播**：使用链式法则计算总损失相对于所有模型参数（权重和偏置）的梯度。该过程从损失相对于输出 logits 的梯度 $\\frac{\\partial L}{\\partial Z} = \\frac{1}{n}(P - Y_{\\text{one-hot}})$ 开始，并向后传播通过每一层，包括 MLP 变换、ReLU 激活和图聚合步骤。正则化项的梯度 $\\lambda W$ 被加到相应的权重梯度上。\n\n**3. 训练与评估**\n\n-   **初始化**：每个案例的模型参数初始化一次，权重使用 Glorot 均匀初始化，偏置使用零初始化。\n-   **训练循环**：对于三种情况中的每一种，使用批量梯度下降法对模型进行 200 个周期的训练。在每个周期中，执行一次完整的前向和反向传播，并根据规则 $\\theta \\leftarrow \\theta - \\eta \\frac{\\partial L}{\\partial \\theta}$ 更新参数。\n-   **评估**：训练后，通过比较所有节点的预测类别（来自输出 logits 的 $\\mathrm{argmax}$）与真实标签来计算最终的训练准确率。\n\n最终输出是一个列表，其中包含按规定格式化的三个准确率值。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GIN synthetic study problem.\n    This function contains all logic for data generation, model implementation,\n    training, and evaluation as required by the problem statement.\n    \"\"\"\n    \n    # --- Problem-defined Constants and Hyperparameters ---\n    N_NODES = 32\n    N_FEATURES = 4\n    N_CLASSES = 2\n    EPOCHS = 200\n    LEARNING_RATE = 0.05\n    L2_COEFF = 1e-4\n    NOISE_STD = 0.1\n    # Canonical choice for the feature direction vector\n    U_VEC = np.ones(N_FEATURES) / np.sqrt(N_FEATURES)\n    \n    # Use a random number generator for reproducible stochasticity\n    RNG = np.random.default_rng(42)\n\n    # --- Helper Functions for Data Generation ---\n    def _create_kmn(m, n):\n        \"\"\"Creates an adjacency matrix for a complete bipartite graph K_m,n.\"\"\"\n        adj = np.zeros((m + n, m + n))\n        adj[:m, m:] = 1\n        adj[m:, :m] = 1\n        return adj\n\n    def create_graph_data(case_id, rng):\n        \"\"\"Constructs graph adjacency, features, and labels for a given case.\"\"\"\n        A = np.zeros((N_NODES, N_NODES))\n        X = np.zeros((N_NODES, N_FEATURES))\n        y = np.zeros(N_NODES, dtype=int)\n\n        if case_id == 'A': # Feature-heterophily, label-homophily\n            k88 = _create_kmn(8, 8)\n            A[:16, :16] = k88\n            A[16:, 16:] = k88\n            y[16:] = 1\n            X[:8, :] = U_VEC + rng.normal(0, NOISE_STD, (8, N_FEATURES))\n            X[8:16, :] = -U_VEC + rng.normal(0, NOISE_STD, (8, N_FEATURES))\n            X[16:24, :] = U_VEC + rng.normal(0, NOISE_STD, (8, N_FEATURES))\n            X[24:32, :] = -U_VEC + rng.normal(0, NOISE_STD, (8, N_FEATURES))\n            \n        elif case_id == 'B': # Feature-homophily, label-heterophily\n            A = _create_kmn(16, 16)\n            y[16:] = 1\n            X[:16, :] = U_VEC + rng.normal(0, NOISE_STD, (16, N_FEATURES))\n            X[16:, :] = U_VEC + rng.normal(0, NOISE_STD, (16, N_FEATURES))\n\n        elif case_id == 'C': # Aligned homophily (control)\n            k88 = _create_kmn(8, 8)\n            A[:16, :16] = k88\n            A[16:, 16:] = k88\n            y[16:] = 1\n            X[:16, :] = U_VEC + rng.normal(0, NOISE_STD, (16, N_FEATURES))\n            X[16:, :] = U_VEC + rng.normal(0, NOISE_STD, (16, N_FEATURES))\n\n        return A, X, y\n\n    # --- GIN Model Implementation from scratch ---\n    \n    def init_params(rng):\n        \"\"\"Initializes model parameters using Glorot uniform initialization.\"\"\"\n        def glorot_uniform(fan_in, fan_out, rng_gen):\n            limit = np.sqrt(6 / (fan_in + fan_out))\n            return rng_gen.uniform(-limit, limit, (fan_in, fan_out))\n\n        params = {\n            'W1_1': glorot_uniform(4, 16, rng), 'b1_1': np.zeros(16),\n            'W1_2': glorot_uniform(16, 8, rng), 'b1_2': np.zeros(8),\n            'W2_1': glorot_uniform(8, 16, rng), 'b2_1': np.zeros(16),\n            'W2_2': glorot_uniform(16, 8, rng), 'b2_2': np.zeros(8),\n            'W_out': glorot_uniform(8, 2, rng), 'b_out': np.zeros(2)\n        }\n        return params\n\n    def relu(x):\n        return np.maximum(0, x)\n\n    def stable_softmax(z):\n        \"\"\"Numerically stable softmax function.\"\"\"\n        exps = np.exp(z - np.max(z, axis=1, keepdims=True))\n        return exps / np.sum(exps, axis=1, keepdims=True)\n\n    def forward_pass(params, A_tilde, X):\n        \"\"\"Performs a full forward pass and caches intermediate values.\"\"\"\n        # Layer 1\n        S1 = A_tilde @ X\n        Z1_1 = S1 @ params['W1_1'] + params['b1_1']\n        A1_1 = relu(Z1_1)\n        Z1_2 = A1_1 @ params['W1_2'] + params['b1_2']\n        H1 = relu(Z1_2)\n\n        # Layer 2\n        S2 = A_tilde @ H1\n        Z2_1 = S2 @ params['W2_1'] + params['b2_1']\n        A2_1 = relu(Z2_1)\n        Z2_2 = A2_1 @ params['W2_2'] + params['b2_2']\n        H2 = relu(Z2_2)\n\n        # Output Layer\n        Z_out = H2 @ params['W_out'] + params['b_out']\n        probs = stable_softmax(Z_out)\n\n        cache = {\n            'X': X, 'A_tilde': A_tilde, 'S1': S1, 'Z1_1': Z1_1, 'A1_1': A1_1, 'Z1_2': Z1_2, 'H1': H1,\n            'S2': S2, 'Z2_1': Z2_1, 'A2_1': A2_1, 'Z2_2': Z2_2, 'H2': H2, 'probs': probs\n        }\n        return probs, cache\n\n    def cross_entropy_loss(y_one_hot, probs, params, l2_coeff):\n        \"\"\"Calculates average cross-entropy loss with L2 regularization.\"\"\"\n        n = probs.shape[0]\n        core_loss = -np.sum(y_one_hot * np.log(probs + 1e-9)) / n\n        \n        l2_reg = 0\n        for key in ['W1_1', 'W1_2', 'W2_1', 'W2_2', 'W_out']:\n             l2_reg += np.sum(params[key]**2)\n        \n        return core_loss + (l2_coeff / 2) * l2_reg\n\n    def backward_pass(params, cache, y_one_hot, l2_coeff):\n        \"\"\"Performs backpropagation to compute gradients.\"\"\"\n        n = y_one_hot.shape[0]\n        grads = {}\n\n        # Gradient of loss wrt logits\n        dZ_out = (cache['probs'] - y_one_hot) / n\n\n        # Output layer gradients\n        grads['W_out'] = cache['H2'].T @ dZ_out + l2_coeff * params['W_out']\n        grads['b_out'] = np.sum(dZ_out, axis=0)\n        dH2 = dZ_out @ params['W_out'].T\n\n        # Layer 2 gradients\n        dZ2_2 = dH2 * (cache['Z2_2'] > 0)\n        grads['W2_2'] = cache['A2_1'].T @ dZ2_2 + l2_coeff * params['W2_2']\n        grads['b2_2'] = np.sum(dZ2_2, axis=0)\n        dA2_1 = dZ2_2 @ params['W2_2'].T\n        dZ2_1 = dA2_1 * (cache['Z2_1'] > 0)\n        grads['W2_1'] = cache['S2'].T @ dZ2_1 + l2_coeff * params['W2_1']\n        grads['b2_1'] = np.sum(dZ2_1, axis=0)\n        dS2 = dZ2_1 @ params['W2_1'].T\n        dH1 = cache['A_tilde'].T @ dS2 # A_tilde is symmetric\n\n        # Layer 1 gradients\n        dZ1_2 = dH1 * (cache['Z1_2'] > 0)\n        grads['W1_2'] = cache['A1_1'].T @ dZ1_2 + l2_coeff * params['W1_2']\n        grads['b1_2'] = np.sum(dZ1_2, axis=0)\n        dA1_1 = dZ1_2 @ params['W1_2'].T\n        dZ1_1 = dA1_1 * (cache['Z1_1'] > 0)\n        grads['W1_1'] = cache['S1'].T @ dZ1_1 + l2_coeff * params['W1_1']\n        grads['b1_1'] = np.sum(dZ1_1, axis=0)\n        \n        return grads\n\n    # --- Training and Evaluation ---\n    def train_and_evaluate(A, X, y, rng):\n        \"\"\"Trains the GIN model and returns the final training accuracy.\"\"\"\n        params = init_params(rng)\n        A_tilde = A + np.eye(N_NODES)\n        y_one_hot = np.eye(N_CLASSES)[y]\n        \n        for epoch in range(EPOCHS):\n            probs, cache = forward_pass(params, A_tilde, X)\n            # loss = cross_entropy_loss(y_one_hot, probs, params, L2_COEFF) # Optional: track loss\n            grads = backward_pass(params, cache, y_one_hot, L2_COEFF)\n            \n            for p_key in params:\n                params[p_key] -= LEARNING_RATE * grads[p_key]\n\n        # Final evaluation\n        final_probs, _ = forward_pass(params, A_tilde, X)\n        y_pred = np.argmax(final_probs, axis=1)\n        accuracy = np.mean(y_pred == y)\n        \n        return accuracy\n\n    # --- Main Execution Logic ---\n    results = []\n    test_cases = ['A', 'B', 'C']\n    \n    for case in test_cases:\n        A, X, y = create_graph_data(case, RNG)\n        accuracy = train_and_evaluate(A, X, y, RNG)\n        results.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "在许多图级别任务（如图分类）中，我们需要将所有节点的嵌入信息聚合成一个单一的图表示。这个关键步骤由“读出”（Readout）函数完成，它必须对节点顺序不敏感。本练习将让你通过编码比较几种主流的读出函数——求和、平均、最大化和基于注意力的池化——并分析它们对图大小变化的敏感性，帮助你为不同的图级别任务选择最合适的聚合策略 。",
            "id": "3106222",
            "problem": "给定一系列置换不变的图读出函数，这些函数将节点嵌入的多重集 $\\{h_i\\}_{i=1}^{|V|}$（其中 $h_i \\in \\mathbb{R}^3$）映射到一个图级别的嵌入 $g \\in \\mathbb{R}^3$。一个图级别的分类器通过一个固定的线性层和一个偏置项来计算一个 logit 值 $z \\in \\mathbb{R}$。需要比较的四种读出函数是：逐元素求和、逐元素取最大值、逐元素取平均值，以及一个基于注意力机制和 softmax 权重的池化方法。目标是量化在指定的节点嵌入生成器下，分类器的 logit 值如何随着节点数 $|V|$ 的变化而变化。所有计算都是纯数值的，没有单位。\n\n基本原理和定义：\n- 如果一个读出函数仅依赖于多重集 $\\{h_i\\}$ 而与索引的顺序无关，则它是置换不变的。对于以下定义，设 $H = [h_1,\\dots,h_{|V|}]^\\top \\in \\mathbb{R}^{|V|\\times 3}$，其中每个 $h_i \\in \\mathbb{R}^3$。\n- 求和读出 (Sum readout)：$g_{\\mathrm{sum}} = \\sum_{i=1}^{|V|} h_i$。\n- 最大值读出 (Max readout)：$g_{\\mathrm{max}}[k] = \\max_{1 \\le i \\le |V|} h_i[k]$，对每个坐标 $k \\in \\{1,2,3\\}$。\n- 平均值读出 (Mean readout)：$g_{\\mathrm{mean}} = \\frac{1}{|V|} \\sum_{i=1}^{|V|} h_i$。\n- 注意力读出 (Attention readout)：使用一个固定的注意力向量 $a \\in \\mathbb{R}^3$，定义注意力分数 $s_i = a^\\top h_i$，注意力权重 $\\alpha_i = \\frac{\\exp(s_i)}{\\sum_{j=1}^{|V|} \\exp(s_j)}$，以及 $g_{\\mathrm{attn}} = \\sum_{i=1}^{|V|} \\alpha_i h_i$。\n\n分类器：\n- 分类器参数固定为 $w = [0.5,\\,-1.0,\\,0.25]^\\top \\in \\mathbb{R}^3$ 和 $b = 0.1 \\in \\mathbb{R}$。\n- 对于任何读出结果 $g \\in \\mathbb{R}^3$，logit 值为 $z = w^\\top g + b$。\n\n注意力参数：\n- 注意力向量固定为 $a = [0.3,\\,0.8,\\,-0.5]^\\top \\in \\mathbb{R}^3$。\n\n节点嵌入生成器：\n- 线性生成器 “lin”，参数为 $(a_0,b_0,c_0) \\in \\mathbb{R}^3$：对于 $i \\in \\{1,\\dots,n\\}$，\n  - $h_i[1] = a_0 + 0.1\\,i$，\n  - $h_i[2] = b_0 - 0.2\\,i$，\n  - $h_i[3] = (-1)^i\\,c_0 + 0.05\\,i$。\n- 中心化生成器 “centered”（无额外参数）：对于 $i \\in \\{1,\\dots,n\\}$，\n  - $h_i[1] = 0.2\\,(i - \\frac{n+1}{2})$，\n  - $h_i[2] = 0.3\\,(-1)^i$，\n  - $h_i[3] = 0.0$。\n- 常数生成器 “const”，参数为 $v \\in \\mathbb{R}^3$：对于 $i \\in \\{1,\\dots,n\\}$，$h_i = v$。\n\n对于每个测试对，您必须：\n- 使用指定的生成器和参数生成两个大小分别为 $n_1$ 和 $n_2$ 的图。\n- 对于四种读出方法中的每一种 $r \\in \\{\\mathrm{sum},\\mathrm{max},\\mathrm{mean},\\mathrm{attn}\\}$，计算相应的 logit 值 $z_r(n_1)$ 和 $z_r(n_2)$。\n- 报告带符号的 logit 差值 $\\Delta_r = z_r(n_2) - z_r(n_1)$。\n\n测试套件：\n- 测试对 $\\mathbf{P1}$ (理想增长路径)：生成器 “lin”，参数 $(a_0,b_0,c_0) = (1.0,\\,0.5,\\,0.2)$，大小 $n_1 = 1$, $n_2 = 10$。\n- 测试对 $\\mathbf{P2}$ (中度增长)：生成器 “lin”，参数 $(a_0,b_0,c_0) = (1.0,\\,0.5,\\,0.2)$，大小 $n_1 = 3$, $n_2 = 6$。\n- 测试对 $\\mathbf{P3}$ (边界情况，相同节点)：生成器 “const”，参数 $v = [0.4,\\,-0.1,\\,0.2]^\\top$，大小 $n_1 = 5$, $n_2 = 50$。\n- 测试对 $\\mathbf{P4}$ (因对称性导致均值几乎对大小不敏感)：生成器 “centered”，大小 $n_1 = 9$, $n_2 = 10$。\n\n对 $|V|$ 的敏感度反映在 $\\Delta_r$ 的大小上：\n- 大的 $|\\Delta_{\\mathrm{sum}}|$ 表示对 $|V|$ 的高敏感度。\n- 小的 $|\\Delta_{\\mathrm{mean}}|$ 表明当 $\\{h_i\\}$ 的经验分布稳定时，该方法相对于 $|V|$ 进行了归一化。\n- $|\\Delta_{\\mathrm{max}}|$ 仅当新节点引入了新的坐标维度上的极值时才会改变。\n- 根据 softmax 权重的构造，$|\\Delta_{\\mathrm{attn}}|$ 不会随 $|V|$ 缩放，但如果新节点改变了注意力分配，它可能会发生变化。\n\n您的任务：\n- 实现一个程序，为每个测试对计算四种读出方法的带符号差值列表，并严格按照 $[\\Delta_{\\mathrm{sum}},\\Delta_{\\mathrm{max}},\\Delta_{\\mathrm{mean}},\\Delta_{\\mathrm{attn}}]$ 的顺序排列。\n- 将测试对 $\\mathbf{P1}$、$\\mathbf{P2}$、$\\mathbf{P3}$、$\\mathbf{P4}$ 的结果汇总成一个长度为 16 的扁平列表，顺序为 $\\mathbf{P1}$、$\\mathbf{P2}$、$\\mathbf{P3}$、$\\mathbf{P4}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的结果严格遵循上述顺序。例如，其形式为 $[x_1,x_2,\\dots,x_{16}]$，其中每个 $x_j$ 都是一个实数。",
            "solution": "该问题要求对四种不同的图读出函数——求和、最大值、平均值和基于注意力的池化——进行系统性评估，通过量化它们对图中节点数 $|V|$ 的敏感度来实现。这种敏感度通过分类器输出 logit 值的变化量 $\\Delta_z$ 来衡量，即图大小从 $n_1$ 变为 $n_2$ 时 logit 值的变化。分析是基于一系列测试对进行的，每个测试对都由一个特定的节点嵌入生成器和图的大小定义。\n\n问题的核心组件定义如下。所有向量均为列向量。\n\n**分类器与参数**\n分类器从一个图级别的嵌入 $g \\in \\mathbb{R}^3$ 通过固定的线性变换和偏置项计算出一个 logit 值 $z \\in \\mathbb{R}$。\n- 权重向量：$w = [0.5, -1.0, 0.25]^\\top \\in \\mathbb{R}^3$\n- 偏置项：$b = 0.1 \\in \\mathbb{R}$\n- Logit 公式：$z = w^\\top g + b$\n\n**注意力机制**\n基于注意力的读出方法使用一个固定的注意力向量来为节点打分。\n- 注意力向量：$a = [0.3, 0.8, -0.5]^\\top \\in \\mathbb{R}^3$\n\n**节点嵌入生成器**\n对于一个大小为 $n$ 的图，其节点嵌入 $h_i \\in \\mathbb{R}^3$ 是根据三个指定函数之一生成的，其中 $i \\in \\{1, \\dots, n\\}$。\n1.  **线性生成器 (“lin”)**：给定参数 $(a_0, b_0, c_0)$，第 $i$ 个节点嵌入 $h_i$ 由下式给出：\n    $$\n    h_i = \\begin{bmatrix} a_0 + 0.1\\,i \\\\ b_0 - 0.2\\,i \\\\ (-1)^i\\,c_0 + 0.05\\,i \\end{bmatrix}\n    $$\n2.  **中心化生成器 (“centered”)**：对于一个大小为 $n$ 的图，第 $i$ 个节点嵌入 $h_i$ 为：\n    $$\n    h_i = \\begin{bmatrix} 0.2\\,(i - \\frac{n+1}{2}) \\\\ 0.3\\,(-1)^i \\\\ 0.0 \\end{bmatrix}\n    $$\n3.  **常数生成器 (“const”)**：给定参数 $v \\in \\mathbb{R}^3$，所有节点嵌入都相同：$h_i = v$。\n\n**读出函数**\n设 $\\{h_i\\}_{i=1}^{n}$ 为节点嵌入的多重集。图嵌入 $g$ 的计算方式如下：\n1.  **求和读出 (Sum Readout)**：$g_{\\mathrm{sum}} = \\sum_{i=1}^{n} h_i$。如果嵌入的均值非零，此函数的输出会随 $n$ 线性缩放。\n2.  **最大值读出 (Max Readout)**：$g_{\\mathrm{max}}$ 是所有节点嵌入的逐元素最大值。对每个坐标 $k \\in \\{1,2,3\\}$：\n    $$\n    g_{\\mathrm{max}}[k] = \\max_{1 \\le i \\le n} h_i[k]\n    $$\n3.  **平均值读出 (Mean Readout)**：$g_{\\mathrm{mean}} = \\frac{1}{n} \\sum_{i=1}^{n} h_i$。此方法通过节点数对求和读出进行归一化。\n4.  **注意力读出 (Attention Readout)**：此函数计算节点嵌入的加权和，权重由注意力机制确定。\n    - 注意力分数：$s_i = a^\\top h_i$\n    - 注意力权重 (softmax)：$\\alpha_i = \\frac{\\exp(s_i)}{\\sum_{j=1}^{n} \\exp(s_j)}$\n    - 图嵌入：$g_{\\mathrm{attn}} = \\sum_{i=1}^{n} \\alpha_i h_i$。根据构造，$\\sum_i \\alpha_i = 1$，因此这是一个凸组合。\n\n**目标计算**\n对于每个读出函数 $r \\in \\{\\mathrm{sum}, \\mathrm{max}, \\mathrm{mean}, \\mathrm{attn}\\}$ 和每个具有图大小 $n_1$ 和 $n_2$ 的测试对，我们计算带符号的 logit 差值：\n$$\n\\Delta_r = z_r(n_2) - z_r(n_1)\n$$\n代入 logit 的定义 $z_r(n) = w^\\top g_r(n) + b$，我们得到：\n$$\n\\Delta_r = (w^\\top g_r(n_2) + b) - (w^\\top g_r(n_1) + b) = w^\\top (g_r(n_2) - g_r(n_1))\n$$\n该量值为给定的读出机制下，图大小从 $n_1$ 变为 $n_2$ 所引起的分类器输出的变化。\n\n我们现在开始为每个测试对计算这些差值。\n\n**测试对 P1：生成器 \"lin\"，参数 $(a_0,b_0,c_0) = (1.0, 0.5, 0.2)$，$n_1 = 1, n_2 = 10$。**\n- 为 $n_1=1$ 和 $n_2=10$ 生成节点嵌入。\n- 对于每种读出方法，计算 $g(n_1)$、$g(n_2)$ 和 $\\Delta_r$。\n1.  $\\Delta_{\\mathrm{sum}}$：$g_{\\mathrm{sum}}(1) = [1.1, 0.3, -0.15]^\\top$。$g_{\\mathrm{sum}}(10) = \\sum_{i=1}^{10} h_i = [15.5, -6.0, 2.75]^\\top$。\n    $\\Delta_{\\mathrm{sum}} = w^\\top (g_{\\mathrm{sum}}(10) - g_{\\mathrm{sum}}(1)) = w^\\top [14.4, -6.3, 2.9]^\\top = 14.225$。\n2.  $\\Delta_{\\mathrm{max}}$：$g_{\\mathrm{max}}(1) = [1.1, 0.3, -0.15]^\\top$。对于 $n_2=10$，$h_i[1]$ 在 $i=10$ 时最大，$h_i[2]$ 在 $i=1$ 时最大，$h_i[3]$ 在最大的偶数 $i$（即 $i=10$）时最大。这得出 $g_{\\mathrm{max}}(10) = [2.0, 0.3, 0.7]^\\top$。\n    $\\Delta_{\\mathrm{max}} = w^\\top (g_{\\mathrm{max}}(10) - g_{\\mathrm{max}}(1)) = w^\\top [0.9, 0.0, 0.85]^\\top = 0.6625$。\n3.  $\\Delta_{\\mathrm{mean}}$：$g_{\\mathrm{mean}}(1) = g_{\\mathrm{sum}}(1)$。$g_{\\mathrm{mean}}(10) = g_{\\mathrm{sum}}(10)/10 = [1.55, -0.6, 0.275]^\\top$。\n    $\\Delta_{\\mathrm{mean}} = w^\\top (g_{\\mathrm{mean}}(10) - g_{\\mathrm{mean}}(1)) = w^\\top [0.45, -0.9, 0.425]^\\top = 1.23125$。\n4.  $\\Delta_{\\mathrm{attn}}$：对于 $n_1=1$，$g_{\\mathrm{attn}}(1)=h_1$。对于 $n_2=10$，我们计算分数 $s_i$、权重 $\\alpha_i$，得到 $g_{\\mathrm{attn}}(10) = \\sum \\alpha_i h_i \\approx [1.229, 0.114, -0.063]^\\top$。\n    $\\Delta_{\\mathrm{attn}} = w^\\top (g_{\\mathrm{attn}}(10) - g_{\\mathrm{attn}}(1)) \\approx w^\\top [0.129, -0.186, 0.087]^\\top \\approx 0.272$。\n\n**测试对 P2：生成器 \"lin\"，参数 $(a_0,b_0,c_0) = (1.0, 0.5, 0.2)$，$n_1 = 3, n_2 = 6$。**\n- 一个增长幅度较小的类似计算。\n1.  $\\Delta_{\\mathrm{sum}}$：$g_{\\mathrm{sum}}(3) = [3.6, -0.3, -0.25]^\\top$。$g_{\\mathrm{sum}}(6) = [7.5, -2.4, 1.2]^\\top$。\n    $\\Delta_{\\mathrm{sum}} = w^\\top (g_{\\mathrm{sum}}(6) - g_{\\mathrm{sum}}(3)) = w^\\top [3.9, -2.1, 1.45]^\\top = 4.4125$。\n2.  $\\Delta_{\\mathrm{max}}$：$g_{\\mathrm{max}}(3) = [1.3, 0.3, 0.3]^\\top$。$g_{\\mathrm{max}}(6) = [1.6, 0.3, 0.5]^\\top$。\n    $\\Delta_{\\mathrm{max}} = w^\\top (g_{\\mathrm{max}}(6) - g_{\\mathrm{max}}(3)) = w^\\top [0.3, 0.0, 0.2]^\\top = 0.2$。\n3.  $\\Delta_{\\mathrm{mean}}$：$g_{\\mathrm{mean}}(3) = [1.2, -0.1, -0.0833]^\\top$。$g_{\\mathrm{mean}}(6) = [1.25, -0.4, 0.2]^\\top$。\n    $\\Delta_{\\mathrm{mean}} = w^\\top (g_{\\mathrm{mean}}(6) - g_{\\mathrm{mean}}(3)) = w^\\top [0.05, -0.3, 0.2833]^\\top = 0.39583...$。\n4.  $\\Delta_{\\mathrm{attn}}$：$g_{\\mathrm{attn}}(3) \\approx [1.139, 0.222, -0.115]^\\top$。$g_{\\mathrm{attn}}(6) \\approx [1.162, 0.186, -0.091]^\\top$。\n    $\\Delta_{\\mathrm{attn}} = w^\\top (g_{\\mathrm{attn}}(6) - g_{\\mathrm{attn}}(3)) \\approx w^\\top [0.023, -0.036, 0.024]^\\top \\approx 0.0535$。\n\n**测试对 P3：生成器 \"const\"，参数 $v = [0.4, -0.1, 0.2]^\\top$，$n_1 = 5, n_2 = 50$。**\n- 所有的 $h_i$ 都与 $v$ 相同。\n1.  $\\Delta_{\\mathrm{sum}}$：$g_{\\mathrm{sum}}(n) = n \\cdot v$。$\\Delta_{\\mathrm{sum}} = w^\\top(n_2 v - n_1 v) = (n_2-n_1) w^\\top v$。\n    $w^\\top v = 0.5(0.4) - 1.0(-0.1) + 0.25(0.2) = 0.35$。\n    $\\Delta_{\\mathrm{sum}} = (50-5) \\times 0.35 = 45 \\times 0.35 = 15.75$。\n2.  $\\Delta_{\\mathrm{max}}$：$g_{\\mathrm{max}}(n) = v$。$\\Delta_{\\mathrm{max}} = w^\\top(v-v) = 0$。\n3.  $\\Delta_{\\mathrm{mean}}$：$g_{\\mathrm{mean}}(n) = \\frac{1}{n}\\sum v = v$。$\\Delta_{\\mathrm{mean}} = w^\\top(v-v) = 0$。\n4.  $\\Delta_{\\mathrm{attn}}$：分数 $s_i = a^\\top v$ 是常数，因此权重 $\\alpha_i = 1/n$。$g_{\\mathrm{attn}}(n) = \\sum \\frac{1}{n} v = v$。\n    $\\Delta_{\\mathrm{attn}} = w^\\top(v-v) = 0$。\n\n**测试对 P4：生成器 \"centered\"，$n_1 = 9, n_2 = 10$。**\n- 该生成器产生的嵌入向量之和几乎为零。\n1.  $\\Delta_{\\mathrm{sum}}$：对于 $n_1=9$ (奇数)，$\\sum h_i[1]=0, \\sum h_i[2]=-0.3, \\sum h_i[3]=0$。所以 $g_{\\mathrm{sum}}(9) = [0, -0.3, 0]^\\top$。\n    对于 $n_2=10$ (偶数)，$\\sum h_i[1]=0, \\sum h_i[2]=0, \\sum h_i[3]=0$。所以 $g_{\\mathrm{sum}}(10) = [0, 0, 0]^\\top$。\n    $\\Delta_{\\mathrm{sum}} = w^\\top(g_{\\mathrm{sum}}(10) - g_{\\mathrm{sum}}(9)) = w^\\top [0, 0.3, 0]^\\top = -0.3$。\n2.  $\\Delta_{\\mathrm{max}}$：对于 $n=9$ 和 $n=10$，$h_i[1]$ 在 $i=n$ 时达到最大值，$h_i[2]$ 在任意偶数 $i$ 时达到最大值。\n    $g_{\\mathrm{max}}(9) = [0.2(9-5), 0.3, 0]^\\top = [0.8, 0.3, 0]^\\top$。\n    $g_{\\mathrm{max}}(10) = [0.2(10-5.5), 0.3, 0]^\\top = [0.9, 0.3, 0]^\\top$。\n    $\\Delta_{\\mathrm{max}} = w^\\top(g_{\\mathrm{max}}(10) - g_{\\mathrm{max}}(9)) = w^\\top [0.1, 0, 0]^\\top = 0.05$。\n3.  $\\Delta_{\\mathrm{mean}}$：$g_{\\mathrm{mean}}(9) = g_{\\mathrm{sum}}(9)/9 = [0, -0.3/9, 0]^\\top$。$g_{\\mathrm{mean}}(10) = [0,0,0]^\\top$。\n    $\\Delta_{\\mathrm{mean}} = w^\\top(g_{\\mathrm{mean}}(10) - g_{\\mathrm{mean}}(9)) = w^\\top [0, 0.3/9, 0]^\\top = -0.0333...$。\n4.  $\\Delta_{\\mathrm{attn}}$：$g_{\\mathrm{attn}}(9) \\approx [-0.499, -0.3, 0]^\\top$。$g_{\\mathrm{attn}}(10) \\approx [-0.599, 0.3, 0]^\\top$。\n    $\\Delta_{\\mathrm{attn}} = w^\\top(g_{\\mathrm{attn}}(10) - g_{\\mathrm{attn}}(9)) \\approx w^\\top [-0.1, 0.6, 0]^\\top \\approx -0.65$。\n\n这些计算在通过数值实现后，得出最终的结果列表。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes logit differences for four graph readout functions across four test pairs.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    w = np.array([0.5, -1.0, 0.25])\n    b = 0.1\n    a = np.array([0.3, 0.8, -0.5])\n\n    # --- Node Embedding Generators ---\n    def generate_embeddings(gen_type, n, params):\n        \"\"\"Generates a matrix of node embeddings H of shape (n, 3).\"\"\"\n        if n == 0:\n            return np.empty((0, 3))\n        H = np.zeros((n, 3))\n        indices = np.arange(1, n + 1)\n        if gen_type == \"lin\":\n            a0, b0, c0 = params\n            H[:, 0] = a0 + 0.1 * indices\n            H[:, 1] = b0 - 0.2 * indices\n            H[:, 2] = ((-1)**indices) * c0 + 0.05 * indices\n        elif gen_type == \"centered\":\n            H[:, 0] = 0.2 * (indices - (n + 1) / 2)\n            H[:, 1] = 0.3 * ((-1)**indices)\n            H[:, 2] = 0.0\n        elif gen_type == \"const\":\n            v = params\n            H[:, :] = v\n        return H\n\n    # --- Readout Functions ---\n    def sum_readout(H):\n        if H.shape[0] == 0:\n            return np.zeros(3)\n        return np.sum(H, axis=0)\n\n    def max_readout(H):\n        if H.shape[0] == 0:\n            # The maximum of an empty set is undefined, return -inf as a neutral element for max\n            return np.full(3, -np.inf)\n        return np.max(H, axis=0)\n\n    def mean_readout(H):\n        if H.shape[0] == 0:\n            return np.zeros(3)\n        return np.mean(H, axis=0)\n\n    def attn_readout(H, attn_vec):\n        if H.shape[0] == 0:\n            return np.zeros(3)\n        scores = H @ attn_vec\n        # Numerically stable softmax\n        scores -= np.max(scores)\n        exp_scores = np.exp(scores)\n        weights = exp_scores / np.sum(exp_scores)\n        return weights @ H\n\n    # --- Main Calculation Logic ---\n    def calculate_deltas(case):\n        gen_type, params, n1, n2 = case\n        \n        H1 = generate_embeddings(gen_type, n1, params)\n        H2 = generate_embeddings(gen_type, n2, params)\n\n        readouts = {\n            \"sum\": sum_readout,\n            \"max\": max_readout,\n            \"mean\": mean_readout,\n            \"attn\": attn_readout\n        }\n\n        deltas = []\n        for r_name, r_func in readouts.items():\n            if r_name == \"attn\":\n                g1 = r_func(H1, a)\n                g2 = r_func(H2, a)\n            else:\n                g1 = r_func(H1)\n                g2 = r_func(H2)\n            \n            # Since z = w^T g + b, delta_z = (w^T g2 + b) - (w^T g1 + b) = w^T (g2 - g1)\n            delta_g = g2 - g1\n            delta_z = delta_g @ w\n            deltas.append(delta_z)\n            \n        return deltas\n\n    # --- Test Suite ---\n    test_cases = [\n        # Pair P1\n        (\"lin\", (1.0, 0.5, 0.2), 1, 10),\n        # Pair P2\n        (\"lin\", (1.0, 0.5, 0.2), 3, 6),\n        # Pair P3\n        (\"const\", np.array([0.4, -0.1, 0.2]), 5, 50),\n        # Pair P4\n        (\"centered\", None, 9, 10),\n    ]\n\n    # --- Execute and Format Output ---\n    all_results = []\n    for case in test_cases:\n        results_for_case = calculate_deltas(case)\n        all_results.extend(results_for_case)\n\n    # Format into a single comma-separated string in brackets\n    print(f\"[{','.join(f'{x:.10f}' for x in all_results)}]\")\n\nsolve()\n```"
        }
    ]
}