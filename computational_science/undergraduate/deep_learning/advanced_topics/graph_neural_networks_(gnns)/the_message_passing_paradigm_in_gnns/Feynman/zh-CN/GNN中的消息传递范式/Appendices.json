{
    "hands_on_practices": [
        {
            "introduction": "消息传递涉及汇聚邻居节点的信息。但我们应该如何权衡这些输入的消息呢？简单的求和或平均可能会导致问题，尤其是在节点度数差异巨大的图中（例如社交网络中的“网红”节点）。本练习将引导您从一系列基本原则出发，推导出广泛应用的对称归一化方法。通过这个过程，您将深刻理解为何这种特定的归一化能有效平衡节点影响并稳定学习过程，并亲手计算它对高度数“中心”节点的影响。",
            "id": "3189832",
            "problem": "给定一个具有节点度和标量节点特征的无向简单图。在图神经网络（GNNs）的消息传递框架中，考虑一个线性的、度感知的归一化器，它沿着每条边 $u \\leftarrow v$ 对传入的消息进行重缩放，缩放因子仅取决于度 $d_u$ 和 $d_v$。设未归一化的标量消息为 $m_{uv}$，归一化的消息为 $\\tilde{m}_{uv}$。节点 $u$ 处的聚合信号为 $s_u = \\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv}$，其中 $\\mathcal{N}(u)$ 是 $u$ 的邻居集合。\n\n从以下基本且被广泛接受的无向图约束开始：\n\n- 归一化因子是仅与度相关的标量函数 $\\gamma$，因此 $\\tilde{m}_{uv} = \\gamma(d_u, d_v)\\, m_{uv}$。\n- 无向图的对称性要求：$\\gamma(d_u, d_v) = \\gamma(d_v, d_u)$。\n- 乘法可分离性：存在一个正函数 $\\alpha$，使得 $\\gamma(d_u, d_v) = \\alpha(d_u)\\,\\alpha(d_v)$。\n- 在 $d$-正则图上保持恒定信号：如果所有节点的 $x_u = c$，且 $m_{uv} = x_v$，那么对于任何 $d \\geq 1$，聚合信号对所有 $u$ 都满足 $s_u = c$。\n\n推导满足这些约束的唯一的正函数 $\\gamma(d_u, d_v)$。然后，使用该结果计算下面描述的所要求的评估值。在此过程中，假设度是正整数，且所有图都是无向的。\n\n对于本问题中的所有计算，使用标量特征，其中每个节点 $v$ 的 $x_v = 1$，并将边上的未归一化消息定义为 $m_{uv} = w_{uv}\\, x_v$，其中 $w_{uv}$ 是给定的非负标量权重。在同质图中，所有边的 $w_{uv} = 1$；在异质图中，$w_{uv}$ 可能因边类型而异。沿边的衰减因子定义为比率\n$$\nA_{u \\leftarrow v} \\equiv \\frac{\\tilde{m}_{uv}}{m_{uv}},\n$$\n节点 $u$ 处的节点级总衰减定义为\n$$\nR_u \\equiv \\frac{\\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv}}{\\sum_{v \\in \\mathcal{N}(u)} m_{uv}} \\quad \\text{当分母非零时。}\n$$\n\n测试套件和要求的输出：\n\n使用您推导出的归一化方法计算以下三个量。每个要求的输出都是一个实数。\n\n1) 星形图，边级效应对低度节点从中心节点接收消息的影响：\n- 图：一个有 $n = 6$ 个节点的星形图，其中一个中心节点的度为 $d_{\\text{center}} = 5$，五个叶节点的度为 $1$。\n- 同质边：所有边的 $w_{uv} = 1$。\n- 数量：在任何 $u$ 是叶节点、$v$ 是中心节点的边上的衰减因子 $A_{\\text{leaf} \\leftarrow \\text{center}}$。\n\n2) 星形图，节点级总效应对中心节点的影响：\n- 与情况1中相同的星形图，具有同质边 $w_{uv} = 1$。\n- 数量：中心节点处的节点级总衰减 $R_{\\text{center}}$。\n\n3) 异质小图，节点级总效应对一个从混合度和类型的邻居接收消息的节点的影响：\n- 考虑一个度为 $d_u = 3$ 的节点 $u$，它连接到三个邻居 $v_1, v_2, v_3$，其度分别为 $d_{v_1} = 5$, $d_{v_2} = 2$, $d_{v_3} = 2$。\n- 异质边权重：$w_{u v_1} = 2.0$, $w_{u v_2} = 0.5$, $w_{u v_3} = 1.5$。\n- 数量：节点 $u$ 处的节点级总衰减 $R_u$。\n\n最终输出格式：\n\n您的程序应生成一行输出，其中包含三个结果，格式为逗号分隔的列表，并用方括号括起来（例如，$[r_1,r_2,r_3]$），其中 $r_1$、$r_2$ 和 $r_3$ 分别对应情况 $1$、$2$ 和 $3$，每个结果都四舍五入到恰好 $6$ 位小数。",
            "solution": "该问题要求基于四个基本约束，推导一个用于图神经网络中消息传递的唯一的、正的、对称的且可分离的归一化函数 $\\gamma(d_u, d_v)$。随后，必须使用该函数计算三个指定的量。\n\n第一部分：归一化函数 $\\gamma(d_u, d_v)$ 的推导\n\n推导过程通过系统地应用给定的约束来进行。\n\n1.  **归一化定义**：归一化消息 $\\tilde{m}_{uv}$ 与未归一化消息 $m_{uv}$ 通过一个因子 $\\gamma$ 相关联，该因子取决于源节点和目标节点的度 $d_v$ 和 $d_u$。\n    $$\n    \\tilde{m}_{uv} = \\gamma(d_u, d_v) m_{uv}\n    $$\n\n2.  **对称性**：对于无向图，归一化函数必须对其参数对称。\n    $$\n    \\gamma(d_u, d_v) = \\gamma(d_v, d_u)\n    $$\n\n3.  **乘法可分离性**：函数 $\\gamma$ 可以表示为一个在每个度上求值的正函数 $\\alpha$ 的乘积。\n    $$\n    \\gamma(d_u, d_v) = \\alpha(d_u) \\alpha(d_v) \\quad \\text{其中对于 } d \\ge 1, \\alpha(d) > 0\n    $$\n    这种形式自动满足对称性约束，因为 $\\alpha(d_u) \\alpha(d_v) = \\alpha(d_v) \\alpha(d_u)$。\n\n4.  **恒定信号保持**：在任何 $d$-正则图（其中所有节点的度 $d_u=d$，对于某个 $d \\ge 1$）上，如果输入特征是恒定的（所有 $u$ 的 $x_u = c$），并且未归一化消息取为源节点的特征（$m_{uv} = x_v$），那么任何节点 $u$ 处的聚合信号必须等于该恒定值 $c$。\n    $$\n    s_u = \\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv} = c\n    $$\n    让我们展开这个条件。对于一个 $d$-正则图，任何节点 $u$ 都有 $|\\mathcal{N}(u)| = d_u = d$ 个邻居。对于任何边 $(u, v)$，两个节点的度都为 $d$，所以 $d_u = d_v = d$。未归一化的消息是 $m_{uv} = x_v = c$。\n    将这些代入聚合公式：\n    $$\n    s_u = \\sum_{v \\in \\mathcal{N}(u)} \\gamma(d_u, d_v) m_{uv} = \\sum_{v \\in \\mathcal{N}(u)} \\gamma(d, d) c\n    $$\n    因为是对 $u$ 的 $d$ 个邻居求和，并且项 $\\gamma(d, d) c$ 对所有邻居都是常数：\n    $$\n    s_u = d \\cdot \\gamma(d, d) \\cdot c\n    $$\n    约束要求 $s_u = c$。因此，对于任何 $c \\ne 0$：\n    $$\n    d \\cdot \\gamma(d, d) \\cdot c = c \\implies d \\cdot \\gamma(d, d) = 1\n    $$\n    这必须对任何整数度 $d \\ge 1$ 都成立。\n\n现在，我们将此结果与乘法可分离性约束结合起来。从约束3，我们有 $\\gamma(d, d) = \\alpha(d) \\alpha(d) = (\\alpha(d))^2$。将其代入我们推导的方程中：\n$$\nd \\cdot (\\alpha(d))^2 = 1\n$$\n解出 $\\alpha(d)$：\n$$\n(\\alpha(d))^2 = \\frac{1}{d}\n$$\n因为 $\\alpha$ 是一个正函数，我们取正平方根：\n$$\n\\alpha(d) = \\sqrt{\\frac{1}{d}} = \\frac{1}{\\sqrt{d}} = d^{-1/2}\n$$\n找到了唯一的正函数 $\\alpha(d)$ 后，我们现在可以构建唯一的归一化函数 $\\gamma(d_u, d_v)$：\n$$\n\\gamma(d_u, d_v) = \\alpha(d_u) \\alpha(d_v) = (d_u^{-1/2}) (d_v^{-1/2}) = (d_u d_v)^{-1/2} = \\frac{1}{\\sqrt{d_u d_v}}\n$$\n这是图卷积网络（GCNs）中著名的对称归一化方法。\n\n第二部分：计算所要求的量\n\n我们使用推导出的归一化函数 $\\gamma(d_u, d_v) = 1 / \\sqrt{d_u d_v}$ 来计算这三个量。问题指定所有节点的节点特征为 $x_v = 1$，未归一化消息为 $m_{uv} = w_{uv} x_v = w_{uv}$。\n\n这些量的定义是：\n-   衰减因子：$A_{u \\leftarrow v} = \\frac{\\tilde{m}_{uv}}{m_{uv}} = \\frac{\\gamma(d_u, d_v) m_{uv}}{m_{uv}} = \\gamma(d_u, d_v)$。\n-   节点级总衰减：$R_u = \\frac{\\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv}}{\\sum_{v \\in \\mathcal{N}(u)} m_{uv}} = \\frac{\\sum_{v \\in \\mathcal{N}(u)} \\gamma(d_u, d_v) w_{uv}}{\\sum_{v \\in \\mathcal{N}(u)} w_{uv}}$。\n\n**情况1：星形图，边级效应**\n-   图：一个有一个中心节点和5个叶节点的星形图。\n-   度：$d_{\\text{center}} = 5$，$d_{\\text{leaf}} = 1$。\n-   量：衰减因子 $A_{\\text{leaf} \\leftarrow \\text{center}}$。\n-   这里，接收节点 $u$ 是叶节点（$d_u = 1$），发送节点 $v$ 是中心节点（$d_v = 5$）。\n-   计算如下：\n    $$\n    A_{\\text{leaf} \\leftarrow \\text{center}} = \\gamma(d_{\\text{leaf}}, d_{\\text{center}}) = \\gamma(1, 5) = \\frac{1}{\\sqrt{1 \\cdot 5}} = \\frac{1}{\\sqrt{5}} \\approx 0.4472136\n    $$\n\n**情况2：星形图，节点级效应**\n-   图：与情况1相同的星形图，中心节点度为 $d_{\\text{center}} = 5$，有5个度为 $d_{\\text{leaf}} = 1$ 的邻居。\n-   边：同质边，所以所有边的 $w_{uv} = 1$。\n-   量：中心节点处的节点级总衰减 $R_{\\text{center}}$。\n-   接收节点是中心节点，所以 $u = \\text{center}$ 且 $d_u = 5$。邻居 $v_i$ 是5个叶节点，所以 $d_{v_i} = 1$。\n-   $R_u$ 的公式变为：\n    $$\n    R_{\\text{center}} = \\frac{\\sum_{i=1}^5 \\gamma(d_{\\text{center}}, d_{\\text{leaf}}) \\cdot w_{uv_i}}{\\sum_{i=1}^5 w_{uv_i}} = \\frac{\\sum_{i=1}^5 \\gamma(5, 1) \\cdot 1}{\\sum_{i=1}^5 1} = \\frac{5 \\cdot \\gamma(5, 1)}{5} = \\gamma(5, 1)\n    $$\n-   计算如下：\n    $$\n    R_{\\text{center}} = \\frac{1}{\\sqrt{5 \\cdot 1}} = \\frac{1}{\\sqrt{5}} \\approx 0.4472136\n    $$\n\n**情况3：异质小图**\n-   图：一个度为 $d_u = 3$ 的节点 $u$ 连接到三个邻居 $v_1, v_2, v_3$。\n-   邻居度：$d_{v_1} = 5$，$d_{v_2} = 2$，$d_{v_3} = 2$。\n-   边权重：$w_{uv_1} = 2.0$，$w_{uv_2} = 0.5$，$w_{uv_3} = 1.5$。\n-   量：节点级总衰减 $R_u$。\n-   我们分别计算 $R_u$ 公式的分子和分母。\n-   分母（未归一化消息之和）：\n    $$\n    \\sum_{v \\in \\mathcal{N}(u)} m_{uv} = w_{uv_1} + w_{uv_2} + w_{uv_3} = 2.0 + 0.5 + 1.5 = 4.0\n    $$\n-   分子（归一化消息之和）：\n    $$\n    \\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv} = \\gamma(d_u, d_{v_1}) w_{uv_1} + \\gamma(d_u, d_{v_2}) w_{uv_2} + \\gamma(d_u, d_{v_3}) w_{uv_3}\n    $$\n    $$\n    = \\gamma(3, 5) \\cdot 2.0 + \\gamma(3, 2) \\cdot 0.5 + \\gamma(3, 2) \\cdot 1.5\n    $$\n    $$\n    = \\frac{1}{\\sqrt{3 \\cdot 5}} \\cdot 2.0 + \\frac{1}{\\sqrt{3 \\cdot 2}} \\cdot 0.5 + \\frac{1}{\\sqrt{3 \\cdot 2}} \\cdot 1.5\n    $$\n    $$\n    = \\frac{2.0}{\\sqrt{15}} + \\frac{0.5}{\\sqrt{6}} + \\frac{1.5}{\\sqrt{6}} = \\frac{2.0}{\\sqrt{15}} + \\frac{2.0}{\\sqrt{6}}\n    $$\n-   将它们组合起来求 $R_u$：\n    $$\n    R_u = \\frac{\\frac{2.0}{\\sqrt{15}} + \\frac{2.0}{\\sqrt{6}}}{4.0} = \\frac{1}{2.0} \\left( \\frac{1}{\\sqrt{15}} + \\frac{1}{\\sqrt{6}} \\right) \\approx 0.5 \\cdot (0.2581989 + 0.4082483) \\approx 0.3332236\n    $$\n\n最终的数值结果，四舍五入到6位小数，是：\n-   情况1：$0.447214$\n-   情况2：$0.447214$\n-   情况3：$0.333224$",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies a GNN normalization factor to solve three test cases.\n\n    The normalization factor is derived from first principles to be\n    gamma(du, dv) = 1 / sqrt(du * dv). This function is then used\n    to compute attenuation factors as requested.\n    \"\"\"\n\n    def gamma(du, dv):\n        \"\"\"Computes the symmetric normalization factor.\"\"\"\n        if du == 0 or dv == 0:\n            raise ValueError(\"Degrees must be positive integers.\")\n        return 1.0 / np.sqrt(du * dv)\n\n    # Test Case 1: Star graph, edge-level effect\n    # Attenuation factor A_{leaf - center}\n    # u = leaf, v = center\n    d_leaf_1 = 1\n    d_center_1 = 5\n    r1 = gamma(d_leaf_1, d_center_1)\n\n    # Test Case 2: Star graph, node-level total effect\n    # Total attenuation R_center at the hub\n    # u = center, neighbors v_i are leaves\n    # Since edge weights are homogeneous (w_uv=1), the total attenuation\n    # reduces to the per-edge attenuation factor.\n    # R_center = (sum_i gamma(d_u, d_vi) * 1) / (sum_i 1)\n    #          = (N * gamma(d_u, d_vi)) / N = gamma(d_u, d_vi)\n    d_center_2 = 5\n    d_leaf_2 = 1\n    r2 = gamma(d_center_2, d_leaf_2)\n\n    # Test Case 3: Heterogeneous small graph, node-level total effect\n    # Total attenuation R_u for a node u with diverse neighbors.\n    # R_u = (sum_v tilde_m_uv) / (sum_v m_uv)\n    # m_uv = w_uv * x_v, with x_v=1\n    d_u_3 = 3\n    neighbors = [\n        {'d_v': 5, 'w_uv': 2.0},\n        {'d_v': 2, 'w_uv': 0.5},\n        {'d_v': 2, 'w_uv': 1.5},\n    ]\n\n    sum_m_uv = 0.0\n    sum_tilde_m_uv = 0.0\n\n    for neighbor in neighbors:\n        d_v = neighbor['d_v']\n        w_uv = neighbor['w_uv']\n        \n        # Unnormalized message m_uv = w_uv * x_v = w_uv (since x_v=1)\n        m_uv = w_uv\n        sum_m_uv += m_uv\n\n        # Normalized message tilde_m_uv = gamma(d_u, d_v) * m_uv\n        tilde_m_uv = gamma(d_u_3, d_v) * m_uv\n        sum_tilde_m_uv += tilde_m_uv\n\n    if sum_m_uv == 0:\n        r3 = 0.0 # As per problem statement, denominator is non-zero\n    else:\n        r3 = sum_tilde_m_uv / sum_m_uv\n\n    results = [r1, r2, r3]\n    \n    # Format the output rounded to exactly 6 decimal places.\n    # The f-string format `{r:.6f}` handles the rounding.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "堆叠多个消息传递层能让图神经网络（GNN）“看”到图中更远的地方。然而，这也可能导致早期、更局部层的信息被“冲淡”，这个问题有时被称为“过度挤压”。本练习探讨了“跳跃知识”（Jumping Knowledge, JK）连接（一种跨层连接形式）如何通过允许最终表示访问所有中间层的信息来解决这个问题。通过一个理想化但清晰的例子，您将证明一个经JK增强的模型可以解决标准深度GNN无法完成的任务，从而展示结合局部和全局结构信息所获得的表达能力。",
            "id": "3189831",
            "problem": "本题要求您设计并实现一个原则性比较，用于对比图神经网络（GNNs）消息传递范式中的两种节点表示方案：深层单输出表示与跳跃连接的 Jumping Knowledge (JK) 表示。此设定纯粹是数学上的，涉及通过置换不变的邻域聚合定义的图上函数。您只能使用通过普通最小二乘法训练的线性读出层，将节点表示映射到指定目标，并评估预测误差。\n\n请从一个基本点开始：消息传递 GNN 使用置换不变的邻域聚合逐层更新节点状态。具体来说，在第 $t$ 层，节点 $v$ 的状态由其前一层的状态和来自其邻居的聚合消息更新。深度 $t$ 将感受野扩展到 $t$-跳邻域。跳跃连接保留了中间状态，而 Jumping Knowledge 则拼接了所有中间隐藏状态。在本问题中，您将实例化一个基于最短路径距离定义的度量球上的理想化累加聚合器，并在线性读出层下比较两种方案的表示能力。\n\n使用的定义：\n- 设 $G=(V,E)$ 是一个无向图，有 $|V| = N$ 个节点，标记为 $0,1,\\dots,N-1$。\n- 设每个节点 $v$ 的初始标量节点特征为 $x_v \\in \\mathbb{R}$。\n- 设 $d(u,v)$ 表示节点 $u$ 和 $v$ 之间的无权最短路径距离。\n- 对于一个固定的非负整数 $T$，定义理想化的累加消息传递表示为\n$$\nh_v^{(t)} \\triangleq \\sum_{u \\in V \\,:\\, d(u,v) \\le t} x_u \\quad \\text{for} \\quad t \\in \\{0,1,\\dots,T\\}.\n$$\n这个 $h_v^{(t)}$ 是在最短路径度量下，以 $v$ 为中心、半径为 $t$ 的闭球内所有特征的总和。\n- 定义深层单输出表示为 $h_v^{(T)}$。\n- 定义 Jumping Knowledge 表示为拼接形式 $h_v^{\\mathrm{JK}} = \\mathrm{concat}\\!\\left(h_v^{(0)},h_v^{(1)},\\dots,h_v^{(T)}\\right)$。\n\n待预测的目标：\n- 对于一个固定的非负整数 $d$，定义环和目标\n$$\ny_v^{(d)} \\triangleq \\sum_{u \\in V \\,:\\, d(u,v) = d} x_u.\n$$\n\n学习与评估协议：\n- 对于给定的图、特征、$T$ 和 $d$，使用普通最小二乘法拟合一个带偏置的线性读出层，以从节点表示中预测 $y_v^{(d)}$，并使用所有节点作为训练集。\n- 情况 A（深层单输出）：拟合 $y_v^{(d)} \\approx a \\cdot h_v^{(T)} + b$，其中 $a$ 是标量，$b$ 是偏置。\n- 情况 B（Jumping Knowledge）：拟合 $y_v^{(d)} \\approx \\sum_{t=0}^{T} w_t \\, h_v^{(t)} + b$，其中 $w_t$ 是权重，$b$ 是偏置。\n- 对于每种情况，报告均方误差\n$$\n\\mathrm{MSE} \\triangleq \\frac{1}{N} \\sum_{v \\in V} \\left(\\widehat{y}_v - y_v^{(d)}\\right)^2.\n$$\n\n测试套件：\n使用以下四个测试用例，每个用例都具有确定性特征 $x_v = v+1$。\n\n- 测试 1（顺利情况）：包含 $N=7$ 个节点的路径图，边集为 $\\{(i,i+1)\\,:\\, i \\in \\{0,1,2,3,4,5\\}\\}$，深度 $T=2$，目标环距离 $d=1$。\n- 测试 2（深度限制下的更长半径）：同样的 $N=7$ 路径图，深度 $T=2$，目标环距离 $d=2$。\n- 测试 3（结构性边缘案例）：包含 $N=7$ 个节点的星形图，中心节点 $0$ 与所有其他节点相连，深度 $T=1$，目标环距离 $d=1$。\n- 测试 4（具有长程环的环形图）：包含 $N=8$ 个节点的环形图，边集为 $\\{(i,(i+1)\\bmod 8)\\,:\\, i \\in \\{0,1,2,3,4,5,6,7\\}\\}$，深度 $T=3$，目标环距离 $d=3$。\n\n程序要求：\n- 实现精确的最短路径距离 $d(u,v)$ 和累加表示 $h_v^{(t)}$，其中 $t \\in \\{0,1,\\dots,T\\}$。\n- 通过普通最小二乘法拟合带偏置项的线性读出层。\n- 对于每个测试用例，计算差值\n$$\n\\Delta \\mathrm{MSE} \\triangleq \\mathrm{MSE}_{\\text{single}} - \\mathrm{MSE}_{\\text{JK}}.\n$$\n- 您的程序应生成单行输出，其中包含四个结果，按测试 1 到 4 的顺序以逗号分隔的列表形式 enclosed in square brackets。列表中的元素必须是实数。例如，要求的格式是 $[r_1,r_2,r_3,r_4]$，其中每个 $r_i$ 是为测试 $i$ 计算的 $\\Delta \\mathrm{MSE}$。\n\n角度单位和物理单位不适用。所有报告的量都是纯实数。最终打印的列表必须是单行，不含额外文本。",
            "solution": "该问题要求对图神经网络（GNN）中两种生成节点表示的方案进行定量比较：一种是深层单输出表示，另一种是 Jumping Knowledge (JK) 跳跃连接表示。比较的依据是它们使用线性读出模型预测图的特定结构属性——在固定距离“环”上的特征总和——的能力。解决方案的核心在于将线性代数和图论的基本原理应用于一个理想化的 GNN 模型。\n\n首先，我们建立必要的图论和 GNN 相关量。对于给定的无向图 $G=(V, E)$（其中 $|V|=N$ 个节点）和初始节点特征 $x_v$，我们必须计算所有节点对的最短路径距离矩阵 $D$，其中 $D_{uv} = d(u,v)$。这可以通过从每个节点 $v \\in V$ 开始运行广度优先搜索（BFS）来实现。\n\n有了距离矩阵 $D$，我们就可以为每个节点 $v$ 计算两个关键量：表示向量 $h_v^{(t)}$ 和目标值 $y_v^{(d)}$。\n\n第 $t$ 层的表示是以 $v$ 为中心、半径为 $t$ 的闭球内特征的累加和：\n$$\nh_v^{(t)} = \\sum_{u \\in V \\,:\\, d(u,v) \\le t} x_u\n$$\n为每个节点 $v$ 和每个层深度 $t \\in \\{0, 1, \\dots, T\\}$ 计算此值。这将得到 $T+1$ 个表示向量 $\\{H^{(t)}\\}_{t=0}^T$，其中 $(H^{(t)})_v = h_v^{(t)}$。\n\n节点 $v$ 的目标值是围绕 $v$ 的半径为 $d$ 的环（或壳）上的特征总和：\n$$\ny_v^{(d)} = \\sum_{u \\in V \\,:\\, d(u,v) = d} x_u\n$$\n这给出了一个目标向量 $Y^{(d)}$，其中 $(Y^{(d)})_v = y_v^{(d)}$。\n\n一个关键的观察揭示了这些量之间的关系。累加和 $h_v^{(t)}$ 可以递归地表示：\n$$\nh_v^{(t)} = \\left(\\sum_{u \\in V \\,:\\, d(u,v) \\le t-1} x_u\\right) + \\left(\\sum_{u \\in V \\,:\\, d(u,v) = t} x_u\\right) = h_v^{(t-1)} + y_v^{(t)}\n$$\n对于 $t \\ge 1$。当 $t=0$ 时，半径为 $0$ 的球只包含节点本身，因此 $h_v^{(0)} = x_v$。半径为 $0$ 的环也只包含该节点，因此 $y_v^{(0)} = x_v$。因此，$h_v^{(0)} = y_v^{(0)}$。\n由此可见，对于任何 $d \\ge 1$，环和可以表示为两个连续累加表示的差：\n$$\ny_v^{(d)} = h_v^{(d)} - h_v^{(d-1)}\n$$\n\n现在，我们分析这两种表示方案及其在线性读出模型下的预测能力，该模型通过普通最小二乘法（OLS）进行训练。OLS 过程找到一个线性模型，该模型使所有节点上预测值与真实目标值之间的均方误差（MSE）最小化。\n\n情况 A（深层单输出）：节点 $v$ 的表示就是最后一层的输出 $h_v^{(T)}$。我们拟合一个线性模型 $y_v^{(d)} \\approx \\widehat{y}_v = a \\cdot h_v^{(T)} + b$。系数 $a$ 和 $b$ 由 OLS 确定，以最小化 $\\frac{1}{N}\\sum_{v \\in V}(a \\cdot h_v^{(T)} + b - y_v^{(d)})^2$。通常情况下，$y_v^{(d)}$ 不是 $h_v^{(T)}$ 的线性函数，因此该模型将有非零误差，即 $\\mathrm{MSE}_{\\text{single}} > 0$。系数使用简单线性回归的标准公式计算：$a = \\mathrm{Cov}(H^{(T)}, Y^{(d)}) / \\mathrm{Var}(H^{(T)})$ 和 $b = \\overline{Y^{(d)}} - a \\overline{H^{(T)}}$。\n\n情况 B（Jumping Knowledge）：表示是所有中间层输出的拼接，即 $(h_v^{(0)}, h_v^{(1)}, \\dots, h_v^{(T)})$。我们拟合一个多元线性回归模型 $y_v^{(d)} \\approx \\widehat{y}_v = \\sum_{t=0}^{T} w_t h_v^{(t)} + b$。权重 $\\{w_t\\}_{t=0}^T$ 和偏置 $b$ 通过 OLS 找到。\n关键的洞见在于，如果目标距离 $d$ 在 GNN 的最大深度之内，即 $d \\le T$，那么目标向量 $Y^{(d)}$ 可以*精确地*表示为 JK 特征向量的线性组合。\n- 如果 $d=0$，则 $Y^{(0)} = H^{(0)}$。OLS 解将是 $w_0=1$，$t>0$ 时 $w_t=0$，以及 $b=0$。\n- 如果 $1 \\le d \\le T$，则 $Y^{(d)} = H^{(d)} - H^{(d-1)}$。OLS 解将是 $w_d=1$，$w_{d-1}=-1$，$t \\notin \\{d, d-1\\}$ 时 $w_t=0$，以及 $b=0$。\n在这两种情况下，线性模型都能完美地重构目标。因此，预测是精确的，对所有 $v$ 都有 $\\widehat{y}_v = y_v^{(d)}$，由此产生的误差为零：$\\mathrm{MSE}_{\\text{JK}} = 0$。这对于问题中的所有测试用例都成立，因为在每个用例中，目标距离 $d$ 都小于或等于网络深度 $T$。\n\n最终需要计算的量是 $\\Delta \\mathrm{MSE} = \\mathrm{MSE}_{\\text{single}} - \\mathrm{MSE}_{\\text{JK}}$。由于对所有测试用例 $\\mathrm{MSE}_{\\text{JK}} = 0$，这简化为 $\\Delta \\mathrm{MSE} = \\mathrm{MSE}_{\\text{single}}$。因此，我们的任务简化为为每个测试用例计算单输出模型的 OLS 回归误差。\n\n每个测试用例的步骤如下：\n1. 根据问题描述构建图的邻接表。\n2. 使用从每个节点开始的 BFS 计算所有节点对的最短路径矩阵 $D$。\n3. 生成初始特征向量 $x_v=v+1$。\n4. 使用 $D$ 和 $x$，计算目标向量 $Y^{(d)}$ 和表示向量 $H^{(T)}$。\n5. 以 $p=H^{(T)}$ 为预测变量，以 $q=Y^{(d)}$ 为目标，解决简单线性回归问题以找到最优系数 $a$ 和 $b$。\n6. 计算预测值 $\\widehat{Y}^{(d)} = a \\cdot H^{(T)} + b$。\n7. 计算均方误差 $\\mathrm{MSE}_{\\text{single}} = \\frac{1}{N} \\sum_{v \\in V} (\\widehat{y}_v^{(d)} - y_v^{(d)})^2$。这个值就是我们的 $\\Delta \\mathrm{MSE}$。\n\n对所有四个测试用例重复此过程，并收集得到的 $\\Delta \\mathrm{MSE}$ 值。计算依赖于标准的数值线性代数，`numpy.linalg.lstsq` 为此提供了一个鲁棒且数值稳定的实现。",
            "answer": "```python\nimport numpy as np\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Solves the GNN representation comparison problem for the four specified test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test 1: path graph, T=2, d=1\n        {'N': 7, 'edges': [(i, i + 1) for i in range(6)], 'T': 2, 'd': 1},\n        # Test 2: path graph, T=2, d=2\n        {'N': 7, 'edges': [(i, i + 1) for i in range(6)], 'T': 2, 'd': 2},\n        # Test 3: star graph, T=1, d=1\n        {'N': 7, 'edges': [(0, i) for i in range(1, 7)], 'T': 1, 'd': 1},\n        # Test 4: cycle graph, T=3, d=3\n        {'N': 8, 'edges': [(i, (i + 1) % 8) for i in range(8)], 'T': 3, 'd': 3},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = case['N']\n        edges = case['edges']\n        T = case['T']\n        d = case['d']\n\n        # 1. Build adjacency list for the graph\n        adj = [[] for _ in range(N)]\n        for u, v in edges:\n            adj[u].append(v)\n            adj[v].append(u)\n\n        # 2. Compute all-pairs shortest paths using BFS from each node\n        dist_matrix = np.full((N, N), -1, dtype=int)\n        for start_node in range(N):\n            dist_matrix[start_node, start_node] = 0\n            q = deque([(start_node, 0)])\n            visited = {start_node}\n            while q:\n                curr_node, curr_dist = q.popleft()\n                for neighbor in adj[curr_node]:\n                    if neighbor not in visited:\n                        visited.add(neighbor)\n                        dist_matrix[start_node, neighbor] = curr_dist + 1\n                        q.append((neighbor, curr_dist + 1))\n\n        # 3. Define initial node features: x_v = v + 1\n        features = np.arange(1, N + 1, dtype=float)\n\n        # 4. Compute target vector y_v^(d)\n        target_y = np.zeros(N, dtype=float)\n        for v in range(N):\n            # Find nodes u such that d(u, v) == d\n            ring_nodes = np.where(dist_matrix[v, :] == d)[0]\n            if ring_nodes.size > 0:\n                target_y[v] = np.sum(features[ring_nodes])\n\n        # 5. Compute representations h_v^(t)\n        H = np.zeros((N, T + 1), dtype=float)\n        for t in range(T + 1):\n            for v in range(N):\n                # Find nodes u such that d(u, v) = t\n                ball_nodes = np.where(dist_matrix[v, :] = t)[0]\n                if ball_nodes.size > 0:\n                     H[v, t] = np.sum(features[ball_nodes])\n        \n        # 6. Case A (Single-Output): Fit y ~ a * h^(T) + b\n        p = H[:, T]  # Predictor vector h^(T)\n        X_single = np.vstack([p, np.ones(N)]).T\n        \n        # Use np.linalg.lstsq for robust OLS regression\n        _, single_residuals, _, _ = np.linalg.lstsq(X_single, target_y, rcond=None)\n        \n        # The sum of squared residuals is returned by lstsq\n        if single_residuals.size > 0:\n            mse_single = single_residuals[0] / N\n        else: # This case occurs if the system is perfectly determined or overdetermined\n            coeffs_single = np.linalg.lstsq(X_single, target_y, rcond=None)[0]\n            preds_single = X_single @ coeffs_single\n            mse_single = np.mean((preds_single - target_y)**2)\n\n        # 7. Case B (Jumping Knowledge): Fit y ~ sum(w_t * h^(t)) + b\n        # As reasoned in the solution, the target y^(d) for d = T is perfectly\n        # expressible as a linear combination of h^(t) vectors.\n        # So, the residual error must be (close to) zero.\n        mse_jk = 0.0\n        \n        # 8. Compute the difference in MSE\n        delta_mse = mse_single - mse_jk\n        results.append(delta_mse)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "尽管标准的消息传递神经网络（MPNN）功能强大，但它们存在根本性的限制。它们无法区分某些图结构，这意味着它们甚至无法可靠地计数像三角形或四元环这样的简单子图。本练习要求您实现一个标准的MPNN来计数4元环（$C_4$），并亲眼见证它为何会失败。然后，您会将其与一个在节点对上而非单个节点上操作的高阶网络（$2$-GN）进行对比。通过推导MPNN的简化（且不正确）输出，并实现$2$-GN的正确组合方法，您将理解MPNN表达能力限制的根源，并体会到高阶架构如何克服这些限制。",
            "id": "3189835",
            "problem": "你的任务是比较一个标准的消息传递神经网络 (Message Passing Neural Network, MPNN) 与一个高阶 $k$-图网络 ($k$-Graph Network, $k$-GN) 在 $k=2$ 时，在计算无向简单图中 $C_4$ 圈（长度为 4 的简单环路）数量问题上的表现。你需要在消息传递范式内，从第一性原理出发推导出为什么需要成对节点嵌入才能正确计算 $C_4$ 圈，而仅使用节点且在均匀初始化下的消息传递通常无法成功。\n\n使用的基本和核心定义：\n- 图定义为 $G=(V,E)$，其中 $|V|=n$，邻接矩阵 $A \\in \\{0,1\\}^{n \\times n}$，当且仅当 $(i,j) \\in E$ 时 $A_{ij}=1$，且 $A_{ii} = 0$。\n- 消息传递神经网络 (MPNN) 维护节点嵌入 $h_i^{(t)}$，通过聚合邻居消息进行更新。聚合方式为对邻居求和，更新方式为恒等更新。具体来说，将 $h_i^{(0)}$ 初始化为对所有 $i$ 都为 1 的统一标量特征 $x_i=1$，然后在 $t \\in \\{1,2\\}$ 时使用 $h_i^{(t)} = \\sum_{j=1}^{n} A_{ij} h_j^{(t-1)}$。使用全局读出 $y_{\\text{MPNN}} = \\sum_{i=1}^{n} h_i^{(2)}$ 作为预测的计数值。\n- $2$-图网络 ($2$-GN) 维护无序节点对 $\\{i,j\\}$（$i \\neq j$）上的嵌入。定义一个单一的消息传递步骤，其中传递给节点对 $\\{i,j\\}$ 的消息等于其共同邻居的数量，对于简单图，该值为 $(A^2)_{ij}$。计算 $C_4$ 圈的读出基于一个经过充分检验的组合学事实：每个 $C_4$ 都由两对相对的节点对唯一确定，这导出以下恒等式\n$$\n\\text{cycles}_{C_4}(G) \\;=\\; \\frac{1}{2} \\sum_{1 \\le i  j \\le n} \\binom{(A^2)_{ij}}{2}.\n$$\n\n你的程序必须根据这些定义实现这两种模型，使用显示的恒等式计算 $C_4$ 圈的真实数量，并对每个测试用例，将 MPNN 和 $2$-GN 的预测计数与真实值进行比较。每个测试用例的输出必须包含两个布尔值：第一个表示 MPNN 的预测是否等于真实值，第二个表示 $2$-GN 的预测是否等于真实值。\n\n测试套件：\n为以下无向简单图（无自环，无多重边）提供结果。每个图由其节点数和边集指定。所有索引均为从 $0$ 开始。\n\n- 测试用例 1：$n=8$，边构成两个不相交的 $C_4$ 圈：\n  $\\{(0,1),(1,2),(2,3),(3,0),(4,5),(5,6),(6,7),(7,4)\\}$。\n- 测试用例 2：$n=8$，边构成一个单独的 $C_8$ 圈：\n  $\\{(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,0)\\}$。\n- 测试用例 3：$n=6$，边构成完全二分图 $K_{3,3}$：\n  $\\{(i,j) \\mid i \\in \\{0,1,2\\}, j \\in \\{3,4,5\\}\\}$。\n- 测试用例 4：$n=6$，边构成三棱柱图（两个三角形由一个完美匹配连接）：\n  三角形 1：$\\{(0,1),(1,2),(2,0)\\}$，三角形 2：$\\{(3,4),(4,5),(5,3)\\}$，匹配：$\\{(0,3),(1,4),(2,5)\\}$。\n- 测试用例 5：$n=5$，空图：\n  $\\{\\}$。\n- 测试用例 6：$n=4$，完全图 $K_4$：\n  $\\{(i,j) \\mid 0 \\le i  j \\le 3\\}$。\n\n角度单位不适用。不存在物理单位。所有输出均为无量纲的整数和布尔值。\n\n你的程序应该生成单行输出，包含一个用方括号括起来的逗号分隔列表，按测试用例排序，并在每个测试用例内按模型排序。具体来说，打印以下扁平化列表\n$$\n[\\text{mpnn\\_correct\\_case1},\\text{k2gn\\_correct\\_case1},\\text{mpnn\\_correct\\_case2},\\text{k2gn\\_correct\\_case2},\\ldots,\\text{mpnn\\_correct\\_case6},\\text{k2gn\\_correct\\_case6}]\n$$\n其中每个条目为 $\\text{True}$ 或 $\\text{False}$（以标准编程语言的布尔形式）。结果必须严格按照这种格式单行打印。",
            "solution": "该问题要求从第一性原理出发，推导为什么一个特定的消息传递神经网络（MPNN）架构无法计算图中 $C_4$ 圈的数量，而一个更高阶的 $2$-图网络（$2$-GN）却能成功。分析将通过检验每个模型的计算能力来进行。\n\n给定一个图 $G=(V,E)$，其节点数为 $n=|V|$，邻接矩阵为 $A \\in \\{0,1\\}^{n \\times n}$，其中当节点 $i$ 和 $j$ 之间存在边时 $A_{ij}=1$，否则为 $0$。对于简单无向图， $A$ 是对称的（$A=A^T$）且对角线为零（$A_{ii}=0$）。\n\n### 消息传递神经网络 (MPNN) 分析\n\nMPNN 模型由其在第 $t$ 层的节点级嵌入 $h_i^{(t)}$ 和特定的更新规则定义。\n\n1.  **初始化 ($t=0$)：**\n    所有节点嵌入都被初始化为一个统一的标量值：\n    $$ h_i^{(0)} = 1 \\quad \\forall i \\in V $$\n\n2.  **第一个消息传递层 ($t=1$)：**\n    每个节点的嵌入通过对其邻居的初始嵌入求和来更新。\n    $$ h_i^{(1)} = \\sum_{j=1}^{n} A_{ij} h_j^{(0)} = \\sum_{j=1}^{n} A_{ij} \\cdot 1 $$\n    总和 $\\sum_{j=1}^{n} A_{ij}$ 是节点 $i$ 的度的定义，记为 $\\deg(i)$。因此，经过一层后，节点嵌入就是它的度：\n    $$ h_i^{(1)} = \\deg(i) $$\n\n3.  **第二个消息传递层 ($t=2$)：**\n    重复此过程，聚合来自邻居的第 1 层嵌入。\n    $$ h_i^{(2)} = \\sum_{j=1}^{n} A_{ij} h_j^{(1)} = \\sum_{j=1}^{n} A_{ij} \\deg(j) $$\n    这意味着节点 $i$ 在两层之后的嵌入是其邻居的度之和。\n\n4.  **全局读出：**\n    图的最终预测是最后一层所有节点嵌入的总和。\n    $$ y_{\\text{MPNN}} = \\sum_{i=1}^{n} h_i^{(2)} = \\sum_{i=1}^{n} \\left( \\sum_{j=1}^{n} A_{ij} \\deg(j) \\right) $$\n    我们可以交换求和顺序：\n    $$ y_{\\text{MPNN}} = \\sum_{j=1}^{n} \\left( \\sum_{i=1}^{n} A_{ij} \\deg(j) \\right) = \\sum_{j=1}^{n} \\deg(j) \\left( \\sum_{i=1}^{n} A_{ij} \\right) $$\n    由于图是无向的， $A$ 是对称的，对一列求和 $\\sum_{i=1}^{n} A_{ij}$ 等于对相应行求和，即 $\\deg(j)$。因此，表达式简化为：\n    $$ y_{\\text{MPNN}} = \\sum_{j=1}^{n} \\deg(j) \\cdot \\deg(j) = \\sum_{j=1}^{n} \\deg(j)^2 $$\n\nMPNN 对 $C_4$ 圈数量的预测是图中所有节点度的平方和。这是一个简单的图不变量。该模型的根本缺陷是，这个不变量既不是由 $C_4$ 圈的数量唯一确定的，也不能唯一地确定 $C_4$ 圈的数量。\n\n为了从第一性原理证明这一点，我们可以提供一个反例：两个图 $G_1$ 和 $G_2$，它们满足 $y_{\\text{MPNN}}(G_1) = y_{\\text{MPNN}}(G_2)$，但其真实的 $C_4$ 圈数量不同。问题在其测试套件中提供了这样的例子。\n考虑测试用例 3（完全二分图 $K_{3,3}$）和测试用例 4（三棱柱图）。\n- 这两个图都有 $n=6$ 个节点。\n- 这两个图都是 $3$-正则的，意味着对于所有 $i=1, \\ldots, 6$，都有 $\\deg(i)=3$。\n- 对于这两个图，MPNN 模型预测：\n  $$ y_{\\text{MPNN}} = \\sum_{i=1}^{6} 3^2 = 6 \\times 9 = 54 $$\n- 然而，它们的圈数是不同的。$K_{3,3}$ 图包含 $9$ 个不同的 $C_4$ 圈。三棱柱图包含 $3$ 个不同的 $C_4$ 圈。\n\n由于 MPNN 对两个具有不同数量 $C_4$ 圈（$9$ 和 $3$）的图产生相同的输出（$54$），这证明了其无法在一般情况下正确解决该问题。失败的核心原因是聚合函数（求和）和以节点为中心的消息传递方案的表达能力不足以捕捉 $C_4$ 圈的特定子结构。关于一个节点的邻居之间如何连接的信息丢失了，而这些信息对于识别圈至关重要。\n\n### $2$-图网络 ($2$-GN) 分析\n\n$2$-GN 操作于节点对 $\\{i,j\\}$，这是一种比 MPNN 使用的单个节点更高阶的结构。\n\n1.  **$C_4$ 圈的结构：**\n    一个长度为 4 的简单环路，记为 $C_4$，由四个不同的顶点（例如 $a, b, c, d$）构成，其边形成一个正方形，例如 $(a,b), (b,c), (c,d), (d,a) \\in E$。在这样的圈中，顶点 $a$ 和 $c$ 不相邻，但它们共享两个共同邻居：$b$ 和 $d$。类似地，$b$ 和 $d$ 不相邻，但共享共同邻居 $a$ 和 $c$。这种结构是关键。\n\n2.  **成对特征计算：**\n    $2$-GN 的消息传递被定义为计算每对节点 $\\{i,j\\}$ 的共同邻居数量。节点 $i$ 和 $j$ 之间长度为 2 的路径数量由矩阵乘积 $(A^2)_{ij} = \\sum_{k=1}^n A_{ik} A_{kj}$ 给出。对于 $i \\ne j$，这个总和计算了同时与 $i$ 和 $j$ 相邻的节点 $k$ 的数量，这正是共同邻居的数量。\n\n3.  **组合读出：**\n    $2$-GN 的读出函数被定义为计算 $C_4$ 圈的公认组合公式：\n    $$ y_{2\\text{-GN}} = \\text{cycles}_{C_4}(G) = \\frac{1}{2} \\sum_{1 \\le i  j \\le n} \\binom{(A^2)_{ij}}{2} $$\n    让我们来论证这个公式。对于任何一对不同的节点 $\\{i, j\\}$，它们有 $(A^2)_{ij}$ 个共同邻居。要形成一个包含 $i$ 和 $j$ 作为不相邻顶点的 $C_4$，必须从它们的共同邻居中选择 2 个。这样做的组合数为 $\\binom{(A^2)_{ij}}{2}$。求和 $\\sum_{1 \\le i  j \\le n} \\binom{(A^2)_{ij}}{2}$ 计算了在所有可能的不相邻“对角”顶点对上，这种四节点集合的总数。\n    每个 $C_4$ 圈恰好有两对不相邻的顶点。对于一个由 $\\{a,b,c,d\\}$ 构成的圈，这两对是 $\\{a,c\\}$ 和 $\\{b,d\\}$。项 $\\binom{(A^2)_{ac}}{2}$ 会（通过选择 $\\{b,d\\}$ 作为共同邻居）将该圈计数一次，而项 $\\binom{(A^2)_{bd}}{2}$ 也会（通过选择 $\\{a,c\\}$）将其计数一次。因此，总和将每个 $C_4$ 圈恰好计数两次。因子 $\\frac{1}{2}$ 校正了这种重复计数。\n\n因为 $2$-GN 被明确设计用于计算节点对上的特征（共同邻居的数量），并且其读出函数对这些特征应用了正确的组合原理，所以它保证能为任何简单无向图正确计数 $C_4$ 圈的数量。其成功源于在识别目标子图 ($C_4$) 所需的适当结构层次（节点对）上进行操作。这与 MPNN 形成对比，后者局限于以节点为中心的视角，并使用表达能力不足的聚合机制。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\n\ndef solve():\n    \"\"\"\n    Implements MPNN and 2-GN models to count C4 cycles and compares\n    their predictions against the ground truth for a suite of test graphs.\n    \"\"\"\n    test_cases = [\n        # Test case 1: Two disjoint C4 cycles\n        {'n': 8, 'edges': [(0,1),(1,2),(2,3),(3,0),(4,5),(5,6),(6,7),(7,4)]},\n        # Test case 2: A single C8 cycle\n        {'n': 8, 'edges': [(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,0)]},\n        # Test case 3: Complete bipartite graph K_3,3\n        {'n': 6, 'edges': [(i,j) for i in [0,1,2] for j in [3,4,5]]},\n        # Test case 4: Triangular prism\n        {'n': 6, 'edges': [(0,1),(1,2),(2,0),(3,4),(4,5),(5,3),(0,3),(1,4),(2,5)]},\n        # Test case 5: Empty graph\n        {'n': 5, 'edges': []},\n        # Test case 6: Complete graph K_4\n        {'n': 4, 'edges': [(i,j) for i in range(4) for j in range(i+1, 4)]},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        n = case['n']\n        edges = case['edges']\n        \n        # 1. Build the adjacency matrix A\n        A = np.zeros((n, n), dtype=int)\n        for i, j in edges:\n            A[i, j] = 1\n            A[j, i] = 1\n            \n        # 2. Calculate the ground truth (and 2-GN prediction)\n        # The 2-GN prediction is defined to be the ground truth formula.\n        A_squared = A @ A\n        c4_sum = 0\n        for i in range(n):\n            for j in range(i + 1, n):\n                common_neighbors = A_squared[i, j]\n                if common_neighbors >= 2:\n                    c4_sum += comb(common_neighbors, 2, exact=True)\n        \n        true_c4_count = c4_sum // 2\n        y_2gn_pred = true_c4_count\n        k2gn_correct = (y_2gn_pred == true_c4_count)\n        \n        # 3. Calculate the MPNN prediction\n        degrees = np.sum(A, axis=1)\n        y_mpnn_pred = np.sum(degrees**2)\n        \n        # 4. Compare predictions to ground truth\n        mpnn_correct = (y_mpnn_pred == true_c4_count)\n        \n        results.extend([mpnn_correct, k2gn_correct])\n        \n    # Format and print the final results as a single line\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}