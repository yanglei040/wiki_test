{
    "hands_on_practices": [
        {
            "introduction": "在应用一项强大的技术之前，建立直观理解并验证其有效性至关重要。这个编码练习提供了一个动手实践的机会，让你可以数值验证重参数技巧的基本恒等式。通过将重参数化计算出的梯度与有限差分近似进行比较，你将亲眼见证“路径导数”如何准确地估计期望的梯度 。",
            "id": "3191616",
            "problem": "实现一个程序，通过蒙特卡洛近似来验证深度学习中重参数化技巧背后的梯度恒等式。考虑一个标量随机变量 $z$，它通过变换 $z = \\mu + \\sigma \\epsilon$ 构建，其中 $\\epsilon \\sim \\mathcal{N}(0,1)$ 是一个标准正态随机变量，$\\mu$ 和 $\\sigma$ 是实数标量。对于一个可微的标量损失函数 $f(z)$，期望损失为 $\\mathbb{E}[f(z)]$。从基本原理出发，即微分的链式法则和期望的线性性质，推导期望损失关于 $\\mu$ 的梯度，并用 $f$ 关于 $z$ 的导数以及 $z$ 关于 $\\mu$ 的导数来表示。\n\n然后，你的程序必须为每个测试用例执行以下操作：\n- 模拟 $N$ 个独立样本 $\\epsilon_i \\sim \\mathcal{N}(0,1)$，其中 $i = 1, 2, \\dots, N$。\n- 对于每个 $\\epsilon_i$，计算 $z_i = \\mu + \\sigma \\epsilon_i$。\n- 通过重参数化路径计算梯度的蒙特卡洛估计量，即 $f'(z_i)$ 的样本均值。\n- 独立地计算 $\\frac{\\partial}{\\partial \\mu} \\mathbb{E}[f(z)]$ 的中心有限差分近似，在两次扰动中使用相同的 $\\epsilon_i$：\n$$\n\\widehat{g}_{\\text{fd}} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{f\\big((\\mu + \\delta) + \\sigma \\epsilon_i\\big) - f\\big((\\mu - \\delta) + \\sigma \\epsilon_i\\big)}{2\\delta}.\n$$\n- 对于每个测试用例，输出绝对差 $| \\widehat{g}_{\\text{fd}} - \\widehat{g}_{\\text{rp}} |$，其中 $\\widehat{g}_{\\text{rp}} = \\frac{1}{N}\\sum_{i=1}^{N} f'(z_i)$。\n\n使用以下损失函数和参数集作为测试套件。每个函数 $f(z)$ 都是可微的，$f'(z)$ 表示其关于 $z$ 的导数：\n- 测试用例 1 (一般多项式情况)：$f(z) = z^3$，参数为 $\\mu = 0.5$, $\\sigma = 1.0$, $N = 40000$, $\\delta = 10^{-3}$, 种子 = $42$。\n- 测试用例 2 (振荡加二次项)：$f(z) = \\sin(z) + z^2$，参数为 $\\mu = -1.0$, $\\sigma = 0.3$, $N = 60000$, $\\delta = 5 \\cdot 10^{-4}$, 种子 = $7$。\n- 测试用例 3 (有界平滑凸起)：$f(z) = \\exp(-\\tfrac{1}{2} z^2)$，参数为 $\\mu = 2.0$, $\\sigma = 1.5$, $N = 60000$, $\\delta = 10^{-3}$, 种子 = $123$。\n- 测试用例 4 (softplus，确定性边界)：$f(z) = \\log(1 + \\exp(z))$，参数为 $\\mu = 10.0$, $\\sigma = 0.0$, $N = 20000$, $\\delta = 10^{-5}$, 种子 = $99$。\n- 测试用例 5 (饱和非线性乘以恒等函数)：$f(z) = z \\tanh(z)$，参数为 $\\mu = -3.0$, $\\sigma = 2.0$, $N = 50000$, $\\delta = 10^{-3}$, 种子 = $777$。\n- 测试用例 6 (带有近确定性噪声的四次函数)：$f(z) = z^4$，参数为 $\\mu = 0.0$, $\\sigma = 10^{-6}$, $N = 30000$, $\\delta = 10^{-5}$, 种子 = $555$。\n\n你的程序应该生成一行输出，其中包含六个测试用例的绝对差，形式为一个逗号分隔的列表，并用方括号括起来。例如，格式必须是 $[x_1,x_2,x_3,x_4,x_5,x_6]$，其中每个 $x_i$ 是一个十进制数（浮点数）。不允许有其他输出。",
            "solution": "该问题陈述在科学上是合理的、良定的，并为继续进行提供了所有必要的信息。它描述了对机器学习中一个称为重参数化技巧的基本恒等式的标准数值验证。核心任务是通过蒙特卡洛模拟来证明，期望的梯度可以通过将梯度算子移入期望内部来计算，这是一种通过对随机变量进行重参数化而实现的操作。\n\n我们从梯度恒等式的解析推导开始。设标量随机变量 $z$ 由变换 $z = \\mu + \\sigma \\epsilon$ 定义，其中 $\\mu$ 和 $\\sigma$ 是标量参数，$\\epsilon$ 是从标准正态分布 $\\epsilon \\sim \\mathcal{N}(0, 1)$ 中抽取的随机变量。$\\epsilon$ 的分布，记为 $p(\\epsilon)$，不依赖于 $\\mu$ 或 $\\sigma$。我们感兴趣的是可微损失函数 $f(z)$ 的期望值关于参数 $\\mu$ 的梯度。\n\n期望损失由下式给出：\n$$\n\\mathbb{E}[f(z)] = \\mathbb{E}_{p(\\epsilon)}[f(\\mu + \\sigma \\epsilon)]\n$$\n根据连续随机变量的期望定义，这可以写成：\n$$\n\\mathbb{E}[f(z)] = \\int_{-\\infty}^{\\infty} f(\\mu + \\sigma \\epsilon) p(\\epsilon) \\,d\\epsilon\n$$\n为了求关于 $\\mu$ 的梯度，我们对这个表达式进行微分：\n$$\n\\frac{\\partial}{\\partial \\mu} \\mathbb{E}[f(z)] = \\frac{\\partial}{\\partial \\mu} \\int_{-\\infty}^{\\infty} f(\\mu + \\sigma \\epsilon) p(\\epsilon) \\,d\\epsilon\n$$\n在函数 $f$ 的标准正则性条件下（所提供的测试函数都满足这些条件，因为它们及其导数是连续的，并且增长速度不超过多项式），我们可以应用莱布尼茨积分法则来交换微分和积分的顺序：\n$$\n\\frac{\\partial}{\\partial \\mu} \\mathbb{E}[f(z)] = \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\mu} [f(\\mu + \\sigma \\epsilon)] p(\\epsilon) \\,d\\epsilon\n$$\n积分内的表达式是方括号内项的期望的定义。因此，我们可以写成：\n$$\n\\frac{\\partial}{\\partial \\mu} \\mathbb{E}[f(z)] = \\mathbb{E}\\left[\\frac{\\partial}{\\partial \\mu} f(\\mu + \\sigma \\epsilon)\\right]\n$$\n现在，我们对期望内的项应用链式法则。令 $z = \\mu + \\sigma \\epsilon$。其导数为：\n$$\n\\frac{\\partial}{\\partial \\mu} f(z) = \\frac{df}{dz} \\frac{\\partial z}{\\partial \\mu}\n$$\n我们计算 $z$ 关于 $\\mu$ 的偏导数：\n$$\n\\frac{\\partial z}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu}(\\mu + \\sigma \\epsilon) = 1\n$$\n将此结果代回，我们得到：\n$$\n\\frac{\\partial}{\\partial \\mu} f(z) = f'(z) \\cdot 1 = f'(z)\n$$\n最后，将此代入期望的梯度表达式中，得到重参数化技巧关于 $\\mu$ 的核心恒等式：\n$$\n\\frac{\\partial}{\\partial \\mu} \\mathbb{E}[f(z)] = \\mathbb{E}[f'(z)]\n$$\n这个非凡的结果表明，期望的梯度等于梯度的期望。这使我们能够通过对在 $z$ 的样本点上求值的函数 $f$ 的梯度进行简单的蒙特卡洛平均，来估计期望的梯度。\n\n数值验证将比较这个梯度的两个估计量。\n1. 重参数化梯度估计量 ($\\widehat{g}_{\\text{rp}}$)：此估计量是我们推导出的恒等式 $\\mathbb{E}[f'(z)]$ 右侧的直接蒙特卡洛近似。给定 $N$ 个独立同分布样本 $\\epsilon_i \\sim \\mathcal{N}(0, 1)$，我们计算 $z_i = \\mu + \\sigma \\epsilon_i$，然后将估计量计算为样本均值：\n$$\n\\widehat{g}_{\\text{rp}} = \\frac{1}{N} \\sum_{i=1}^{N} f'(z_i)\n$$\n2. 有限差分梯度估计量 ($\\widehat{g}_{\\text{fd}}$)：此估计量使用中心差分格式为左侧的 $\\frac{\\partial}{\\partial \\mu} \\mathbb{E}[f(z)]$ 提供数值近似。函数 $G(\\mu) = \\mathbb{E}[f(\\mu + \\sigma \\epsilon)]$ 的导数近似为 $\\frac{G(\\mu+\\delta) - G(\\mu-\\delta)}{2\\delta}$。我们用蒙特卡洛平均来近似期望 $G(\\mu \\pm \\delta)$。为了减少方差，我们对两项使用相同的随机数集 $\\{\\epsilon_i\\}$，这导出了指定的公式：\n$$\n\\widehat{g}_{\\text{fd}} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{f\\big((\\mu + \\delta) + \\sigma \\epsilon_i\\big) - f\\big((\\mu - \\delta) + \\sigma \\epsilon_i\\big)}{2\\delta}\n$$\n程序将使用提供的参数为每个测试用例计算 $\\widehat{g}_{\\text{rp}}$ 和 $\\widehat{g}_{\\text{fd}}$。每个用例的最终输出将是绝对差 $|\\widehat{g}_{\\text{fd}} - \\widehat{g}_{\\text{rp}}|$。一个小的差异可以验证重参数化恒等式成立，并且 $\\widehat{g}_{\\text{rp}}$ 是期望梯度的有效估计量。这些测试用例涵盖了多种函数类型和参数设置，包括确定性和近确定性的特殊情况，以全面测试该恒等式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Validates the reparameterization trick gradient identity by comparing its Monte Carlo\n    estimator to a finite difference approximation for several test cases.\n    \"\"\"\n\n    # Define the six test cases with functions, derivatives, and parameters.\n    test_cases = [\n        {\n            \"f\": lambda z: z**3,\n            \"f_prime\": lambda z: 3 * z**2,\n            \"mu\": 0.5, \"sigma\": 1.0, \"N\": 40000, \"delta\": 1e-3, \"seed\": 42\n        },\n        {\n            \"f\": lambda z: np.sin(z) + z**2,\n            \"f_prime\": lambda z: np.cos(z) + 2 * z,\n            \"mu\": -1.0, \"sigma\": 0.3, \"N\": 60000, \"delta\": 5e-4, \"seed\": 7\n        },\n        {\n            \"f\": lambda z: np.exp(-0.5 * z**2),\n            \"f_prime\": lambda z: -z * np.exp(-0.5 * z**2),\n            \"mu\": 2.0, \"sigma\": 1.5, \"N\": 60000, \"delta\": 1e-3, \"seed\": 123\n        },\n        {\n            # Softplus function. f'(z) is the sigmoid function.\n            \"f\": lambda z: np.log(1 + np.exp(z)),\n            \"f_prime\": lambda z: 1 / (1 + np.exp(-z)),\n            \"mu\": 10.0, \"sigma\": 0.0, \"N\": 20000, \"delta\": 1e-5, \"seed\": 99\n        },\n        {\n            \"f\": lambda z: z * np.tanh(z),\n            \"f_prime\": lambda z: np.tanh(z) + z * (1 - np.tanh(z)**2),\n            \"mu\": -3.0, \"sigma\": 2.0, \"N\": 50000, \"delta\": 1e-3, \"seed\": 777\n        },\n        {\n            \"f\": lambda z: z**4,\n            \"f_prime\": lambda z: 4 * z**3,\n            \"mu\": 0.0, \"sigma\": 1e-6, \"N\": 30000, \"delta\": 1e-5, \"seed\": 555\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack parameters for the current test case\n        mu, sigma, N, delta, seed = case[\"mu\"], case[\"sigma\"], case[\"N\"], case[\"delta\"], case[\"seed\"]\n        f, f_prime = case[\"f\"], case[\"f_prime\"]\n\n        # Set the random seed for reproducibility\n        np.random.seed(seed)\n\n        # 1. Generate N independent samples from a standard normal distribution\n        epsilons = np.random.randn(int(N))\n\n        # 2. Compute the reparameterization gradient estimator (g_rp)\n        # z_i = mu + sigma * epsilon_i\n        z_samples = mu + sigma * epsilons\n        # g_rp = 1/N * sum(f'(z_i))\n        g_rp = np.mean(f_prime(z_samples))\n\n        # 3. Compute the finite difference gradient estimator (g_fd)\n        # We use the same epsilon_i for both perturbations (common random numbers)\n        z_plus = (mu + delta) + sigma * epsilons\n        z_minus = (mu - delta) + sigma * epsilons\n        \n        # g_fd = 1/N * sum( (f(z_plus_i) - f(z_minus_i)) / (2*delta) )\n        fd_terms = (f(z_plus) - f(z_minus)) / (2 * delta)\n        g_fd = np.mean(fd_terms)\n\n        # 4. Calculate the absolute difference and store it\n        abs_diff = np.abs(g_fd - g_rp)\n        results.append(abs_diff)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "从理论走向实践，本练习将探讨重参数技巧在深度学习常用函数（如修正线性单元 ReLU）中的表现。你将分析分布参数 $\\mu$ 和 $\\sigma$ 的梯度如何受到这种分段线性激活函数的影响，从而深入理解梯度消失等现象 。这有助于将抽象概念与神经网络训练的具体动态联系起来。",
            "id": "3191625",
            "problem": "考虑一个标量潜变量，定义为 $z=\\mu+\\sigma\\epsilon$，其中 $\\mu\\in\\mathbb{R}$，$\\sigma0$，并且 $\\epsilon\\sim\\mathcal{N}(0,1)$ 是一个标准高斯随机变量。设 $f:\\mathbb{R}\\to\\mathbb{R}$ 是一个网络的输出，该网络是分段线性的，具有有限个断点（扭结），这对于由修正线性单元（ReLU）非线性构建的网络是典型的，其中ReLU代表修正线性单元。定义目标函数 $J(\\mu,\\sigma)=\\mathbb{E}_{\\epsilon}[\\,f(\\mu+\\sigma\\epsilon)\\,]$。使用重参数化技巧，并仅利用诸如链式法则和在适当正则性条件下微分与期望可交换等公认事实，分析当 $f$ 是分段线性时，梯度如何通过 $z$ 传播。特别地，找出能正确描述以下情况的陈述：\n- 参数空间中梯度消失或变为常数的区域，\n- 梯度 $\\partial J/\\partial\\mu$ 和 $\\partial J/\\partial\\sigma$ 如何依赖于 $f$ 的局部斜率，\n- 对学习动态的定性影响。\n选择所有正确的陈述。\n\nA. 如果 $f$ 在 $\\mathbb{R}$ 上是全局线性的，那么对于所有 $\\sigma0$，都有 $\\frac{\\partial J}{\\partial \\sigma}=0$，并且 $\\frac{\\partial J}{\\partial \\mu}$ 等于 $f$ 的常数斜率。\n\nB. 对于 $f(z)=\\max(0,z)$ （修正线性单元），有 $\\frac{\\partial J}{\\partial \\mu}=\\mathbb{1}\\{\\mu0\\}$ 且 $\\frac{\\partial J}{\\partial \\sigma}=0$，因为 $\\mathbb{E}[\\epsilon]=0$。\n\nC. 如果 $f$ 是具有有限个扭结的分段线性函数，那么在 $z$ 的连续分布下，不可微点的勒贝格测度为 $0$，因此 $\\frac{\\partial J}{\\partial \\mu}$ 等于局部斜率 $f'(z)$ 的期望，而 $\\frac{\\partial J}{\\partial \\sigma}$ 等于 $f'(z)\\epsilon$ 的期望。\n\nD. 在 ReLU 的情况 $f(z)=\\max(0,z)$ 下，$\\frac{\\partial J}{\\partial \\mu}=\\Phi\\!\\left(\\frac{\\mu}{\\sigma}\\right)$ 且 $\\frac{\\partial J}{\\partial \\sigma}=\\phi\\!\\left(\\frac{\\mu}{\\sigma}\\right)$，其中 $\\Phi$ 和 $\\phi$ 分别是标准正态累积分布函数（CDF）和概率密度函数（PDF）。因此，当 $\\frac{\\mu}{\\sigma}\\ll 0$ 时，两个梯度都呈指数级小，这会阻碍学习。\n\nE. 如果 $f$ 是分段线性的，并且 $z$ 几乎必然地位于单个线性区域内（例如，$\\sigma$ 非常小且 $\\mu$ 远离任何扭结），那么 $\\frac{\\partial J}{\\partial \\mu}$ 和 $\\frac{\\partial J}{\\partial \\sigma}$ 都恰好为 $0$。",
            "solution": "问题陈述是有效的，因为它代表了深度学习和变分推断背景下的一个标准的、良定的数学练习，其基础是微积分和概率论的既定原理。\n\n目标函数由 $J(\\mu,\\sigma)=\\mathbb{E}_{\\epsilon}[f(\\mu+\\sigma\\epsilon)]$ 给出，其中 $z=\\mu+\\sigma\\epsilon$ 且 $\\mu\\in\\mathbb{R}$，$\\sigma0$，$\\epsilon\\sim\\mathcal{N}(0,1)$。函数 $f:\\mathbb{R}\\to\\mathbb{R}$ 是分段线性的，具有有限数量的不可微点（扭结）。\n\n重参数化技巧允许我们通过将微分算子移入期望内部来计算期望的梯度。问题陈述允许微分和期望的交换，这是合理的，因为 $f$ 是连续的，其导数 $f'(z)$ 是良态的（分段常数），并且 $\\epsilon$ 的概率密度函数是光滑且快速衰减的。\n\n让我们推导梯度 $\\frac{\\partial J}{\\partial \\mu}$ 和 $\\frac{\\partial J}{\\partial \\sigma}$ 的一般表达式。\n\n关于 $\\mu$ 的梯度是：\n$$ \\frac{\\partial J}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\mathbb{E}_{\\epsilon}[f(\\mu+\\sigma\\epsilon)] = \\mathbb{E}_{\\epsilon}\\left[\\frac{\\partial}{\\partial \\mu} f(\\mu+\\sigma\\epsilon)\\right] $$\n使用链式法则，对于 $z = \\mu+\\sigma\\epsilon$，我们有 $\\frac{\\partial z}{\\partial \\mu} = 1$。\n$$ \\frac{\\partial J}{\\partial \\mu} = \\mathbb{E}_{\\epsilon}\\left[f'(z) \\cdot \\frac{\\partial z}{\\partial \\mu}\\right] = \\mathbb{E}_{\\epsilon}\\left[f'(\\mu+\\sigma\\epsilon) \\cdot 1\\right] = \\mathbb{E}_{\\epsilon}[f'(\\mu+\\sigma\\epsilon)] $$\n由于 $f$ 是分段线性的，其导数 $f'(z)$ 除了在有限个扭结处外都有定义。由于 $z$ 是一个连续随机变量（一个高斯分布 $\\mathcal{N}(\\mu, \\sigma^2)$），$z$ 取任何特定值（包括扭结处）的概率为 $0$。因此，导数 $f'(z)$ 是几乎必然有定义的，其期望也是良定的。\n\n关于 $\\sigma$ 的梯度是：\n$$ \\frac{\\partial J}{\\partial \\sigma} = \\frac{\\partial}{\\partial \\sigma} \\mathbb{E}_{\\epsilon}[f(\\mu+\\sigma\\epsilon)] = \\mathbb{E}_{\\epsilon}\\left[\\frac{\\partial}{\\partial \\sigma} f(\\mu+\\sigma\\epsilon)\\right] $$\n使用链式法则，我们有 $\\frac{\\partial z}{\\partial \\sigma} = \\epsilon$。\n$$ \\frac{\\partial J}{\\partial \\sigma} = \\mathbb{E}_{\\epsilon}\\left[f'(z) \\cdot \\frac{\\partial z}{\\partial \\sigma}\\right] = \\mathbb{E}_{\\epsilon}[f'(\\mu+\\sigma\\epsilon) \\cdot \\epsilon] $$\n\n现在我们根据这些通用公式评估每个选项。\n\n**A. 如果 $f$ 在 $\\mathbb{R}$ 上是全局线性的，那么对于所有 $\\sigma0$，都有 $\\frac{\\partial J}{\\partial \\sigma}=0$，并且 $\\frac{\\partial J}{\\partial \\mu}$ 等于 $f$ 的常数斜率。**\n\n如果 $f$ 是全局线性的，我们可以写成 $f(z) = az+b$，其中 $a, b \\in \\mathbb{R}$ 是常数。导数为 $f'(z)=a$，对所有 $z \\in \\mathbb{R}$ 成立。\n\n关于 $\\mu$ 的梯度是：\n$$ \\frac{\\partial J}{\\partial \\mu} = \\mathbb{E}_{\\epsilon}[f'(\\mu+\\sigma\\epsilon)] = \\mathbb{E}_{\\epsilon}[a] = a $$\n这就是 $f$ 的常数斜率。陈述的这一部分是正确的。\n\n关于 $\\sigma$ 的梯度是：\n$$ \\frac{\\partial J}{\\partial \\sigma} = \\mathbb{E}_{\\epsilon}[f'(\\mu+\\sigma\\epsilon) \\cdot \\epsilon] = \\mathbb{E}_{\\epsilon}[a \\cdot \\epsilon] = a \\mathbb{E}_{\\epsilon}[\\epsilon] $$\n由于 $\\epsilon \\sim \\mathcal{N}(0,1)$，其期望为 $\\mathbb{E}[\\epsilon]=0$。因此：\n$$ \\frac{\\partial J}{\\partial \\sigma} = a \\cdot 0 = 0 $$\n这对所有 $\\sigma  0$ 都成立。陈述的这一部分也是正确的。\n\n对A的裁定：**正确**。\n\n**B. 对于 $f(z)=\\max(0,z)$ （修正线性单元），有 $\\frac{\\partial J}{\\partial \\mu}=\\mathbb{1}\\{\\mu0\\}$ 且 $\\frac{\\partial J}{\\partial \\sigma}=0$，因为 $\\mathbb{E}[\\epsilon]=0$。**\n\n对于 $f(z) = \\max(0, z)$，其导数为 $f'(z) = \\mathbb{1}\\{z0\\}$（指示函数），在 $z \\neq 0$ 时有定义。\n\n关于 $\\mu$ 的梯度是：\n$$ \\frac{\\partial J}{\\partial \\mu} = \\mathbb{E}_{\\epsilon}[f'(\\mu+\\sigma\\epsilon)] = \\mathbb{E}_{\\epsilon}[\\mathbb{1}\\{\\mu+\\sigma\\epsilon  0\\}] $$\n这个期望是事件 $\\mu+\\sigma\\epsilon  0$ 的概率：\n$$ P(\\mu+\\sigma\\epsilon  0) = P(\\sigma\\epsilon  -\\mu) = P(\\epsilon  -\\mu/\\sigma) = 1 - \\Phi(-\\mu/\\sigma) = \\Phi(\\mu/\\sigma) $$\n其中 $\\Phi$ 是标准正态分布的累积分布函数（CDF）。该陈述声称 $\\frac{\\partial J}{\\partial \\mu}=\\mathbb{1}\\{\\mu0\\}$，这是一个不连续函数，当 $\\mu0$ 时等于 $1$，当 $\\mu\\le 0$ 时等于 $0$。这不等于 $\\Phi(\\mu/\\sigma)$，后者是一个关于 $\\mu$ 的光滑、严格递增的函数。例如，如果 $\\mu=0$，此时 $\\mathbb{1}\\{\\mu0\\}=0$，而梯度是 $\\Phi(0)=0.5$。因此，这一部分是不正确的。\n\n关于 $\\sigma$ 的梯度：\n$$ \\frac{\\partial J}{\\partial \\sigma} = \\mathbb{E}_{\\epsilon}[f'(\\mu+\\sigma\\epsilon) \\cdot \\epsilon] = \\mathbb{E}_{\\epsilon}[\\mathbb{1}\\{\\mu+\\sigma\\epsilon  0\\} \\cdot \\epsilon] $$\n“因为 $\\mathbb{E}[\\epsilon]=0$” 这个理由是错误的，因为它错误地暗示了期望可以分离：$\\mathbb{E}[g(\\epsilon)\\epsilon] \\neq \\mathbb{E}[g(\\epsilon)]\\mathbb{E}[\\epsilon]$，因为 $g(\\epsilon) = \\mathbb{1}\\{\\mu+\\sigma\\epsilon  0\\}$ 与 $\\epsilon$ 不是独立的。\n正确的计算是：\n$$ \\frac{\\partial J}{\\partial \\sigma} = \\int_{-\\infty}^{\\infty} \\mathbb{1}\\{\\mu+\\sigma\\epsilon  0\\} \\cdot \\epsilon \\cdot \\phi(\\epsilon) d\\epsilon = \\int_{-\\mu/\\sigma}^{\\infty} \\epsilon \\phi(\\epsilon) d\\epsilon $$\n其中 $\\phi(\\epsilon) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\epsilon^2/2}$ 是标准正态分布的概率密度函数（PDF）。由于 $\\frac{d}{d\\epsilon}\\phi(\\epsilon) = -\\epsilon\\phi(\\epsilon)$，该积分为：\n$$ \\int_{-\\mu/\\sigma}^{\\infty} -\\phi'(\\epsilon)d\\epsilon = -[\\phi(\\epsilon)]_{-\\mu/\\sigma}^{\\infty} = - (0 - \\phi(-\\mu/\\sigma)) = \\phi(-\\mu/\\sigma) = \\phi(\\mu/\\sigma) $$\n（因为 $\\phi$ 是一个偶函数）。这个结果 $\\phi(\\mu/\\sigma)$ 通常不为 $0$。因此，陈述的这一部分也是不正确的。\n\n对B的裁定：**不正确**。\n\n**C. 如果 $f$ 是具有有限个扭结的分段线性函数，那么在 $z$ 的连续分布下，不可微点的勒贝格测度为 $0$，因此 $\\frac{\\partial J}{\\partial \\mu}$ 等于局部斜率 $f'(z)$ 的期望，而 $\\frac{\\partial J}{\\partial \\sigma}$ 等于 $f'(z)\\epsilon$ 的期望。**\n\n这个陈述总结了我们在开头进行的一般推导。\n“在 $z$ 的连续分布下，不可微点的勒贝格测度为 $0$” 这个理由精确地解释了为什么导数 $f'(z)$ 可以在期望内部使用，因为它几乎必然是有定义的。\n关于 $\\mu$ 的梯度表达式是 $\\frac{\\partial J}{\\partial \\mu} = \\mathbb{E}_{\\epsilon}[f'(\\mu+\\sigma\\epsilon)]$。由于 $z=\\mu+\\sigma\\epsilon$，这正是局部斜率 $f'(z)$ 的期望。\n关于 $\\sigma$ 的梯度表达式是 $\\frac{\\partial J}{\\partial \\sigma} = \\mathbb{E}_{\\epsilon}[f'(\\mu+\\sigma\\epsilon)\\cdot\\epsilon]$。这是局部斜率 $f'(z)$ 与噪声项 $\\epsilon$ 乘积的期望。\n该陈述是对这类函数的重参数化梯度计算的正确且理由充分的总结。\n\n对C的裁定：**正确**。\n\n**D. 在 ReLU 的情况 $f(z)=\\max(0,z)$ 下，$\\frac{\\partial J}{\\partial \\mu}=\\Phi\\!\\left(\\frac{\\mu}{\\sigma}\\right)$ 且 $\\frac{\\partial J}{\\partial \\sigma}=\\phi\\!\\left(\\frac{\\mu}{\\sigma}\\right)$，其中 $\\Phi$ 和 $\\phi$ 分别是标准正态累积分布函数（CDF）和概率密度函数（PDF）。因此，当 $\\frac{\\mu}{\\sigma}\\ll 0$ 时，两个梯度都呈指数级小，这会阻碍学习。**\n\n陈述的第一部分提供了 ReLU 情况下梯度的明确公式。正如在对选项 B 的分析中推导出的：\n- $\\frac{\\partial J}{\\partial \\mu} = \\Phi(\\mu/\\sigma)$。这是正确的。\n- $\\frac{\\partial J}{\\partial \\sigma} = \\phi(\\mu/\\sigma)$。这是正确的。\n\n第二部分分析了当 $\\frac{\\mu}{\\sigma} \\ll 0$ 时的行为，即 $\\frac{\\mu}{\\sigma}$ 是一个大的负数。\n- 对于关于 $\\mu$ 的梯度：$\\frac{\\partial J}{\\partial \\mu} = \\Phi(\\mu/\\sigma)$。当 $x \\to -\\infty$ 时，正态 CDF $\\Phi(x)$ 趋于 $0$。对于大的负数 $x$，其衰减近似为 $\\Phi(x) \\approx \\frac{\\phi(x)}{|x|} = \\frac{1}{\\sqrt{2\\pi}|x|}e^{-x^2/2}$。这是一个指数级小的量。\n- 对于关于 $\\sigma$ 的梯度：$\\frac{\\partial J}{\\partial \\sigma} = \\phi(\\mu/\\sigma) = \\frac{1}{\\sqrt{2\\pi}}e^{-(\\mu/\\sigma)^2/2}$。当 $\\mu/\\sigma \\to -\\infty$ 时，这个量也显然是指数级小的。\n\n当梯度变得指数级小时，基于梯度的优化中的参数更新会变得微不足道，从而有效地停止了学习过程。这种现象被称为梯度消失问题。因此，这会阻碍学习的结论是正确的。\n\n对D的裁定：**正确**。\n\n**E. 如果 $f$ 是分段线性的，并且 $z$ 几乎必然地位于单个线性区域内（例如，$\\sigma$ 非常小且 $\\mu$ 远离任何扭结），那么 $\\frac{\\partial J}{\\partial \\mu}$ 和 $\\frac{\\partial J}{\\partial \\sigma}$ 都恰好为 $0$。**\n\n“$z$ 几乎必然地位于单个线性区域内”这一条件意味着，对于某个区间 $(c_1, c_2)$，在其上 $f(z) = az+b$，我们有 $P(z \\in (c_1, c_2)) = 1$。在这种情况下，$f'(\\mu+\\sigma\\epsilon) = a$ 几乎必然成立。\n\n让我们在这个假设下计算梯度：\n- $\\frac{\\partial J}{\\partial \\mu} = \\mathbb{E}_{\\epsilon}[f'(\\mu+\\sigma\\epsilon)] = \\mathbb{E}_{\\epsilon}[a] = a$。\n- $\\frac{\\partial J}{\\partial \\sigma} = \\mathbb{E}_{\\epsilon}[f'(\\mu+\\sigma\\epsilon) \\cdot \\epsilon] = \\mathbb{E}_{\\epsilon}[a \\cdot \\epsilon] = a\\mathbb{E}[\\epsilon] = 0$。\n\n该陈述声称*两个*梯度都恰好为 $0$。虽然 $\\frac{\\partial J}{\\partial \\sigma}$ 确实为 $0$，但梯度 $\\frac{\\partial J}{\\partial \\mu}$ 等于线性区域的斜率 $a$。这个斜率不一定为 $0$。例如，在一个带有 ReLU 激活的网络中，激活的单元位于斜率为 $a=1$ 的区域。在这种情况下，$\\frac{\\partial J}{\\partial \\mu} = 1$，而不是 $0$。因此，该陈述通常不成立。\n\n对E的裁定：**不正确**。",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "建模的一个关键方面是做出有效的设计选择。本练习要求你比较两种对潜变量强制施加正定性约束的常用方法：指数函数和softplus函数。通过分析每种参数化的梯度，你将发现它们在数值稳定性上的关键差异，并理解一个看似微小的选择如何导致梯度爆炸或消失，这对模型训练的成功与否有着重要影响 。",
            "id": "3191630",
            "problem": "在一个深度学习模型中，一个潜变量 $z$ 被要求严格为正，并且关于分布参数的梯度是使用重参数化技巧来估计的。设 $\\epsilon \\sim \\mathcal{N}(0,1)$ 是一个标准正态噪声，并考虑两种强制变量为正的选择：(i) $z=\\exp(\\mu+\\sigma\\epsilon)$ 和 (ii) $z=\\mathrm{softplus}(\\mu+\\sigma\\epsilon)$，其中 $\\mathrm{softplus}(u)=\\log(1+\\exp(u))$。参数 $\\mu$ 和 $\\sigma$ 是实值的，并通过基于梯度的优化进行训练。\n\n使用重参数化技巧，通过对 $\\epsilon$ 的确定性变换，利用链式法则计算 $z$ 关于 $\\mu$ 和 $\\sigma$ 的梯度。比较当 $|\\mu|$ 变得很大时（假设 $\\sigma$ 是有限的）这些梯度的行为，重点关注梯度是否爆炸或消失，以及对 $\\epsilon$ 的任何依赖性。\n\n哪个陈述最准确地描述了这两种选择的梯度行为？\n\nA. 对于 $z=\\exp(\\mu+\\sigma\\epsilon)$，当 $\\mu \\to +\\infty$ 时，$\\partial z/\\partial \\mu$ 和 $\\partial z/\\partial \\sigma$ 都无界增长，而当 $\\mu \\to -\\infty$ 时，它们都收缩到 $0$。对于 $z=\\mathrm{softplus}(\\mu+\\sigma\\epsilon)$，$\\partial z/\\partial \\mu$ 被界定在 $0$ 和 $1$ 之间，而 $\\partial z/\\partial \\sigma$ 的幅值被 $|\\epsilon|$ 所界定，因此梯度不会因为大的 $|\\mu|$ 而爆炸，但当 $\\mu \\to -\\infty$ 时可能会消失。\n\nB. 重参数化技巧保证了对于任何平滑变换，梯度都不会爆炸，因此无论 $\\mu$ 为何值，$z=\\exp(\\mu+\\sigma\\epsilon)$ 和 $z=\\mathrm{softplus}(\\mu+\\sigma\\epsilon)$ 都避免了梯度爆炸和梯度消失。\n\nC. $\\mathrm{softplus}$ 选择消除了对于大的负 $\\mu$ 的梯度消失问题，因为当 $u \\to -\\infty$ 时 $\\mathrm{softplus}(u)$ 线性增长，而指数选择仅在 $\\mu \\to +\\infty$ 时才会出现梯度消失。\n\nD. 对于指数选择，$\\partial z/\\partial \\mu$ 对所有 $\\mu$ 都保持在区间 $[0,1]$ 内，使其在数值上是稳定的；对于 $\\mathrm{softplus}$ 选择，$\\partial z/\\partial \\mu$ 可能超过 $1$，因此对于大的正 $\\mu$ 会爆炸。\n\nE. 在两种选择中，$\\partial z/\\partial \\sigma$ 都不依赖于 $\\epsilon$，因此 $|\\epsilon|$ 的大实现值不会影响梯度的方差或稳定性。",
            "solution": "用户希望我分析正潜变量 $z$ 的两种不同参数化的梯度，并评估给定的陈述。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- 潜变量 $z$ 需要严格为正 ($z  0$)。\n- 关于分布参数的梯度使用重参数化技巧进行估计。\n- 噪声变量：$\\epsilon \\sim \\mathcal{N}(0,1)$。\n- 参数化 (i)：$z = \\exp(\\mu + \\sigma\\epsilon)$。\n- 参数化 (ii)：$z = \\mathrm{softplus}(\\mu + \\sigma\\epsilon)$，其中 $\\mathrm{softplus}(u) = \\log(1 + \\exp(u))$。\n- $\\mu$ 和 $\\sigma$ 是实值的可训练参数。\n- 分析应侧重于当 $|\\mu|$ 变得很大时梯度 $\\frac{\\partial z}{\\partial \\mu}$ 和 $\\frac{\\partial z}{\\partial \\sigma}$ 的行为，其中假设 $\\sigma$ 是有限的。重点是梯度爆炸/消失以及对 $\\epsilon$ 的依赖性。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题基于重参数化技巧，这是深度学习和变分推断领域的一项标准和基础技术。所使用的函数，指数函数和softplus函数，是强制为正的常用选择。使用标准正态噪声变量也是标准做法。该问题在科学上和数学上都是合理的。\n- **良构性：** 该问题为 $z$ 提供了两个具体、定义明确的数学函数，并要求在特定的极限情况下分析它们的偏导数。问题是无歧义的，并且有唯一、可推导的解。\n- **客观性：** 该问题以客观的数学术语陈述，并要求对梯度行为进行刻画，这是所涉函数的一个形式属性。\n\n**步骤3：结论和行动**\n问题陈述是有效的。它具有科学依据、是良构的且客观。我将继续推导解决方案。\n\n### 解决方案推导\n\n重参数化技巧通过将随机变量表示为一个无参数噪声变量的确定性变换，允许梯度通过随机节点进行反向传播。目标函数 $L$ 关于参数 $\\mu$ 和 $\\sigma$ 的梯度将通过链式法则计算，例如 $\\frac{\\partial L}{\\partial \\mu} = \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial \\mu}$。这个过程的稳定性关键地依赖于 $\\frac{\\partial z}{\\partial \\mu}$ 和 $\\frac{\\partial z}{\\partial \\sigma}$ 这两项。我们将为每种参数化分析这些项。\n\n令 $u = \\mu + \\sigma\\epsilon$。\n\n**情况 (i)：$z = \\exp(\\mu + \\sigma\\epsilon) = \\exp(u)$**\n\n首先，我们计算 $z$ 关于 $\\mu$ 和 $\\sigma$ 的偏导数。\n\n1.  **关于 $\\mu$ 的梯度**：\n    $$\n    \\frac{\\partial z}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\exp(\\mu + \\sigma\\epsilon) = \\exp(\\mu + \\sigma\\epsilon) \\cdot \\frac{\\partial}{\\partial \\mu}(\\mu + \\sigma\\epsilon) = \\exp(\\mu + \\sigma\\epsilon)\n    $$\n    -   当 $\\mu \\to +\\infty$ 时（对于有限的 $\\sigma, \\epsilon$），我们有 $\\mu + \\sigma\\epsilon \\to +\\infty$。因此，$\\frac{\\partial z}{\\partial \\mu} = \\exp(\\mu + \\sigma\\epsilon) \\to +\\infty$。梯度**爆炸**。\n    -   当 $\\mu \\to -\\infty$ 时（对于有限的 $\\sigma, \\epsilon$），我们有 $\\mu + \\sigma\\epsilon \\to -\\infty$。因此，$\\frac{\\partial z}{\\partial \\mu} = \\exp(\\mu + \\sigma\\epsilon) \\to 0$。梯度**消失**。\n\n2.  **关于 $\\sigma$ 的梯度**：\n    $$\n    \\frac{\\partial z}{\\partial \\sigma} = \\frac{\\partial}{\\partial \\sigma} \\exp(\\mu + \\sigma\\epsilon) = \\exp(\\mu + \\sigma\\epsilon) \\cdot \\frac{\\partial}{\\partial \\sigma}(\\mu + \\sigma\\epsilon) = \\epsilon \\exp(\\mu + \\sigma\\epsilon)\n    $$\n    -   当 $\\mu \\to +\\infty$ 时，$\\exp(\\mu + \\sigma\\epsilon)$ 项无界增长。对于任何非零的 $\\epsilon$，其幅值 $|\\frac{\\partial z}{\\partial \\sigma}| = |\\epsilon|\\exp(\\mu + \\sigma\\epsilon)$ **爆炸**。\n    -   当 $\\mu \\to -\\infty$ 时，$\\exp(\\mu + \\sigma\\epsilon) \\to 0$。因此，$\\frac{\\partial z}{\\partial \\sigma} \\to 0$。梯度**消失**。\n\n**情况 (ii)：$z = \\mathrm{softplus}(\\mu + \\sigma\\epsilon) = \\log(1 + \\exp(u))$**\n\n首先，我们求 $\\mathrm{softplus}$ 函数的导数。设 $f(u) = \\mathrm{softplus}(u)$。\n$$\nf'(u) = \\frac{d}{du} \\log(1 + \\exp(u)) = \\frac{\\exp(u)}{1 + \\exp(u)} = \\frac{1}{1 + \\exp(-u)}\n$$\n这就是 logistic sigmoid 函数，通常表示为 $\\sigma_{\\text{sig}}(u)$。sigmoid 函数的输出总是在区间 $(0, 1)$ 内。\n\n1.  **关于 $\\mu$ 的梯度**：\n    $$\n    \\frac{\\partial z}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\mathrm{softplus}(\\mu + \\sigma\\epsilon) = \\sigma_{\\text{sig}}(\\mu + \\sigma\\epsilon) \\cdot \\frac{\\partial}{\\partial \\mu}(\\mu + \\sigma\\epsilon) = \\sigma_{\\text{sig}}(\\mu + \\sigma\\epsilon)\n    $$\n    -   由于 sigmoid 函数的值域是 $(0, 1)$，我们有 $0  \\frac{\\partial z}{\\partial \\mu}  1$ 对所有有限的 $\\mu, \\sigma, \\epsilon$ 成立。这个梯度是**有界的**，不会爆炸。\n    -   当 $\\mu \\to +\\infty$ 时，我们有 $\\mu + \\sigma\\epsilon \\to +\\infty$，所以 $\\frac{\\partial z}{\\partial \\mu} \\to 1$。\n    -   当 $\\mu \\to -\\infty$ 时，我们有 $\\mu + \\sigma\\epsilon \\to -\\infty$，所以 $\\frac{\\partial z}{\\partial \\mu} \\to 0$。梯度**消失**。\n\n2.  **关于 $\\sigma$ 的梯度**：\n    $$\n    \\frac{\\partial z}{\\partial \\sigma} = \\frac{\\partial}{\\partial \\sigma} \\mathrm{softplus}(\\mu + \\sigma\\epsilon) = \\sigma_{\\text{sig}}(\\mu + \\sigma\\epsilon) \\cdot \\frac{\\partial}{\\partial \\sigma}(\\mu + \\sigma\\epsilon) = \\epsilon \\cdot \\sigma_{\\text{sig}}(\\mu + \\sigma\\epsilon)\n    $$\n    -   由于 $0  \\sigma_{\\text{sig}}(\\mu + \\sigma\\epsilon)  1$，其幅值为 $|\\frac{\\partial z}{\\partial \\sigma}| = |\\epsilon| \\cdot \\sigma_{\\text{sig}}(\\mu + \\sigma\\epsilon)  |\\epsilon|$。这个梯度的幅值被 **$|\\epsilon|$ 所界定**，并且不会因为大的 $|\\mu|$ 而爆炸。\n    -   当 $\\mu \\to +\\infty$ 时，$\\frac{\\partial z}{\\partial \\sigma} \\to \\epsilon \\cdot 1 = \\epsilon$。\n    -   当 $\\mu \\to -\\infty$ 时，$\\frac{\\partial z}{\\partial \\sigma} \\to \\epsilon \\cdot 0 = 0$。梯度**消失**。\n\n### 逐项分析\n\n**A. 对于 $z=\\exp(\\mu+\\sigma\\epsilon)$，当 $\\mu \\to +\\infty$ 时，$\\partial z/\\partial \\mu$ 和 $\\partial z/\\partial \\sigma$ 都无界增长，而当 $\\mu \\to -\\infty$ 时，它们都收缩到 $0$。对于 $z=\\mathrm{softplus}(\\mu+\\sigma\\epsilon)$，$\\partial z/\\partial \\mu$ 被界定在 $0$ 和 $1$ 之间，而 $\\partial z/\\partial \\sigma$ 的幅值被 $|\\epsilon|$ 所界定，因此梯度不会因为大的 $|\\mu|$ 而爆炸，但当 $\\mu \\to -\\infty$ 时可能会消失。**\n- 指数选择的分析与我们的推导相符：当 $\\mu \\to +\\infty$ 时梯度爆炸，当 $\\mu \\to -\\infty$ 时梯度消失。\n- softplus选择的分析也与我们的推导相符：$\\frac{\\partial z}{\\partial \\mu}$ 在 $(0, 1)$ 内有界，$|\\frac{\\partial z}{\\partial \\sigma}|$ 被 $|\\epsilon|$ 所界定，梯度不会因为大的 $|\\mu|$ 而爆炸，并且两个参数的梯度在 $\\mu \\to -\\infty$ 时都消失。\n- 这个陈述准确地总结了我们的发现。\n- **结论：正确**\n\n**B. 重参数化技巧保证了对于任何平滑变换，梯度都不会爆炸，因此无论 $\\mu$ 为何值，$z=\\exp(\\mu+\\sigma\\epsilon)$ 和 $z=\\mathrm{softplus}(\\mu+\\sigma\\epsilon)$ 都避免了梯度爆炸和梯度消失。**\n- “重参数化技巧能保证梯度不爆炸”这一前提是错误的。梯度的行为取决于所使用的具体变换函数。\n- 我们对 $z=\\exp(\\mu+\\sigma\\epsilon)$ 的分析表明梯度可能并且确实会爆炸。\n- 我们对两种参数化的分析都表明梯度可能并且确实会消失。\n- 该陈述在所有方面都是不正确的。\n- **结论：不正确**\n\n**C. $\\mathrm{softplus}$ 选择消除了对于大的负 $\\mu$ 的梯度消失问题，因为当 $u \\to -\\infty$ 时 $\\mathrm{softplus}(u)$ 线性增长，而指数选择仅在 $\\mu \\to +\\infty$ 时才会出现梯度消失。**\n- 关于softplus消除了对于大的负μ的梯度消失问题的说法是错误的。我们证明了当 $\\mu \\to -\\infty$ 时，$\\frac{\\partial z}{\\partial \\mu}$ 和 $\\frac{\\partial z}{\\partial \\sigma}$ 都趋近于0。\n- 其理由“当 $u \\to -\\infty$ 时 $\\mathrm{softplus}(u)$ 线性增长”也是错误的。当 $u \\to -\\infty$ 时，$\\mathrm{softplus}(u) = \\log(1+\\exp(u)) \\approx \\exp(u)$，这是指数衰减的。它在 $u \\to +\\infty$ 时是线性增长的。\n- 关于指数选择*仅*在 $\\mu \\to +\\infty$ 时才会出现梯度消失的说法是错误的。它在 $\\mu \\to +\\infty$ 时梯度爆炸，在 $\\mu \\to -\\infty$ 时梯度消失。\n- **结论：不正确**\n\n**D. 对于指数选择，$\\partial z/\\partial \\mu$ 对所有 $\\mu$ 都保持在区间 $[0,1]$ 内，使其在数值上是稳定的；对于 $\\mathrm{softplus}$ 选择，$\\partial z/\\partial \\mu$ 可能超过 $1$，因此对于大的正 $\\mu$ 会爆炸。**\n- 这个陈述把两种行为搞反了。对于指数选择，$\\frac{\\partial z}{\\partial \\mu} = \\exp(\\mu+\\sigma\\epsilon)$，它不受 $[0,1]$ 区间限制，并且对于大的正 $\\mu$ 在数值上是不稳定的。\n- 对于softplus选择，$\\frac{\\partial z}{\\partial \\mu} = \\sigma_{\\text{sig}}(\\mu+\\sigma\\epsilon)$，它被严格限制在区间 $(0, 1)$ 内，不能超过 1。\n- **结论：不正确**\n\n**E. 在两种选择中，$\\partial z/\\partial \\sigma$ 都不依赖于 $\\epsilon$，因此 $|\\epsilon|$ 的大实现值不会影响梯度的方差或稳定性。**\n- 这个前提是错误的。我们推导出对于指数选择，$\\frac{\\partial z}{\\partial \\sigma} = \\epsilon \\exp(\\mu+\\sigma\\epsilon)$，对于softplus选择，$\\frac{\\partial z}{\\partial \\sigma} = \\epsilon \\cdot \\sigma_{\\text{sig}}(\\mu+\\sigma\\epsilon)$。在两种情况下，梯度都与 $\\epsilon$ 成正比。\n- 因此，$|\\epsilon|$ 的大实现值会直接导致更大的梯度幅值，从而影响梯度的方差和稳定性。\n- **结论：不正确**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}