{
    "hands_on_practices": [
        {
            "introduction": "在改进解耦表示之前，我们首先需要一种量化它的方法。本练习将指导您从基本原理出发，构建一个强大的诊断工具。您将通过实现一个互信息（mutual information）的经验估计器，来衡量学习到的潜变量与真实生成因子之间的统计依赖性，从而为模型评估创建一个至关重要的工具 。",
            "id": "3116952",
            "problem": "给定由 Beta-变分自编码器 (Beta-VAE) 等模型生成的成对样本，包含学习到的潜变量和真实的生成因子。设潜码表示为随机向量 $z = (z_1, \\dots, z_{d_z})$，真实生成因子表示为 $g = (g_1, \\dots, g_{d_g})$。您的任务是基于每个标量潜变量 $z_i$ 和每个标量因子 $g_j$ 之间的互信息构建一个解耦诊断指标，组装互信息矩阵，并以一种置换不变的方式量化其对角占优性。\n\n从联合与边缘概率分布、Shannon 熵和互信息（由 Kullback–Leibler 散度定义）的基本定义出发，推导并实现一个通过观测样本离散化来计算标量随机变量间互信息的经验估计量。然后，使用此估计量构建所有 $(z_i, g_j)$ 对之间的互信息矩阵，并计算一个置换不变的对角占优分数，该分数反映了每个潜变量与唯一因子对齐的程度。\n\n您必须遵循以下设计要求：\n\n- 数据与离散化：\n  - 对于给定的成对样本集 $\\{(z^{(n)}, g^{(n)})\\}_{n=1}^N$，其中 $z^{(n)} \\in \\mathbb{R}^{d_z}$ 且 $g^{(n)} \\in \\mathbb{R}^{d_g}$，将每个标量变量在其观测范围（样本中的最小值到最大值）内离散化为 $b$ 个等宽的区间。对所有标量变量使用相同的 $b$。将区间的边界视为右闭合，以包含观测到的最大值。设 $b \\in \\mathbb{N}$ 且 $b \\ge 2$。\n  - 基于这些离散化结果，为每对 $(z_i, g_j)$ 构建经验联合频率表，并对其进行归一化以获得经验联合概率质量及其边缘概率。\n\n- 互信息矩阵：\n  - 仅使用基本定义，计算每对 $(i, j)$ 的互信息 $I(z_i; g_j)$，生成一个矩阵 $I \\in \\mathbb{R}_{\\ge 0}^{d_z \\times d_g}$。使用自然对数，单位为奈特 (nats)。\n\n- 置换不变的对角占优分数：\n  - 由于潜变量维度和因子维度的排序是任意的，首先计算 $\\{z_i\\}$ 和 $\\{g_j\\}$ 之间的最优一一分配，使得矩阵 $I$ 中被选中条目的总和最大化。如果 $d_z \\ne d_g$，则分配应在不重复使用任一集合中索引的情况下，匹配 $m = \\min(d_z, d_g)$ 对。\n  - 设 $S_{\\text{assign}}$ 为 $m$ 个被选中条目的总和，设 $S_{\\text{all}}$ 为 $I$ 中所有条目的总和。将对角占优分数定义为 $D = S_{\\text{assign}} / S_{\\text{all}}$。如果在数值容差范围内 $S_{\\text{all}} = 0$（即 $S_{\\text{all}} \\le 10^{-12}$），则定义 $D = 0$。\n  - 分数 $D$ 必须满足 $0 \\le D \\le 1$，并且当互信息质量集中在对角线的一个置换上时，该分数应更接近于 $1$。\n\n- 输出规范：\n  - 对于下面的每个测试用例，您的程序必须输出一个三元组，包括：\n    1) 分数 $D$，四舍五入到小数点后4位，\n    2) 从潜变量索引到因子索引的所选分配，形式为一个长度为 $d_z$ 的列表，其中未分配的潜变量索引（如果 $d_z > d_g$）用 $-1$ 标记，\n    3) 互信息矩阵 $I$ 按行主序展平，每个条目四舍五入到小数点后4位。\n  - 将所有测试用例的结果聚合到单行中，该行包含一个由这些三元组组成的列表。最终输出必须是严格符合以下格式的一行：\n    $[ [D_1, [a_{1,1}, \\dots, a_{1,d_z}], [I_{1,1}, \\dots]], [D_2, [\\dots], [\\dots]], \\dots ]$。\n\n- 测试套件与数据生成：\n  - 使用以下测试套件。在以下所有情况中，$N$ 是样本数量，$s$ 是用于可复现生成器的伪随机种子。所有对数必须是自然对数，所有计算必须以浮点运算完成。在所有情况下，设置区间数量为 $b = 12$。\n\n  - 测试用例 1 (近乎理想的解耦):\n    - 维度: $d_z = 3$, $d_g = 3$。\n    - 参数: $N = 5000$, $s = 13$。\n    - 采样: 对于每个 $n \\in \\{1, \\dots, N\\}$，采样 $g^{(n)} \\sim \\text{Uniform}([-1, 1]^{3})$，以及噪声 $\\epsilon^{(n)} \\sim \\mathcal{N}(0, I_3)$。\n    - 构建 $z^{(n)} = g^{(n)} + 0.05 \\cdot \\epsilon^{(n)}$。\n\n  - 测试用例 2 (独立的潜变量和因子):\n    - 维度: $d_z = 3$, $d_g = 3$。\n    - 参数: $N = 5000$, $s = 7$。\n    - 采样: 对于每个 $n$，采样 $g^{(n)} \\sim \\text{Uniform}([-1, 1]^{3})$，并独立采样 $z^{(n)} \\sim \\mathcal{N}(0, I_3)$。\n\n  - 测试用例 3 (纠缠线性混合):\n    - 维度: $d_z = 3$, $d_g = 3$。\n    - 参数: $N = 5000$, $s = 11$。\n    - 采样: 对于每个 $n$，采样 $g^{(n)} \\sim \\text{Uniform}([-1, 1]^{3})$，噪声 $\\epsilon^{(n)} \\sim \\mathcal{N}(0, I_3)$，并设置 $A \\in \\mathbb{R}^{3 \\times 3}$ 为\n      $$\n      A = \\begin{bmatrix}\n      1.0  0.8  0.5 \\\\\n      0.6  1.0  0.4 \\\\\n      0.5  0.4  1.2\n      \\end{bmatrix}.\n      $$\n      构建 $z^{(n)} = A \\, g^{(n)} + 0.2 \\cdot \\epsilon^{(n)}$。\n\n  - 测试用例 4 (维度不匹配，稀疏对齐):\n    - 维度: $d_z = 2$, $d_g = 3$。\n    - 参数: $N = 5000$, $s = 17$。\n    - 采样: 对于每个 $n$，采样 $g^{(n)} \\sim \\text{Uniform}([-1, 1]^{3})$，噪声 $\\epsilon^{(n)} \\sim \\mathcal{N}(0, I_2)$，并设置 $B \\in \\mathbb{R}^{2 \\times 3}$ 为\n      $$\n      B = \\begin{bmatrix}\n      1.0  0.0  0.0 \\\\\n      0.0  0.0  1.0\n      \\end{bmatrix}.\n      $$\n      构建 $z^{(n)} = B \\, g^{(n)} + 0.05 \\cdot \\epsilon^{(n)}$。\n\n- 最终输出格式要求：\n  - 您的程序应生成单行输出，其中包含 4 个测试用例的结果列表，该列表为逗号分隔并用方括号括起，其中每个结果都是形如 $[D, \\text{assignment}, \\text{flattened\\_I}]$ 的列表，如上所述。输出中的所有浮点数必须四舍五入到小数点后 4 位，整数必须是精确值。\n\n您的解决方案必须是一个完整的、可运行的程序，能够完全按照上述规定执行采样、离散化、互信息估计、最优分配、分数计算和最终格式化，无需任何外部输入。",
            "solution": "该问题要求开发一种基于互信息的解耦度量。我们将首先从第一性原理出发，建立互信息的理论基础，然后详细说明其经验估计方法，最后阐述置换不变分数的构建方式。\n\n**1. 互信息的基本推导**\n\n此分析的基石是互信息，它量化了两个随机变量之间的统计依赖性。我们从更基本的概念——Shannon熵和Kullback-Leibler (KL) 散度——推导出它。\n\n设 $X$ 和 $Y$ 是两个离散随机变量，其取值空间分别为 $\\mathcal{X}$ 和 $\\mathcal{Y}$，联合概率质量函数 (PMF) 为 $P_{XY}(x, y)$，边缘 PMF 分别为 $P_X(x)$ 和 $P_Y(y)$。\n\n$X$ 的 **Shannon 熵** 衡量其不确定性，定义为：\n$$ H(X) = - \\sum_{x \\in \\mathcal{X}} P_X(x) \\log P_X(x) $$\n其中对数为自然对数（底为 $e$），单位为奈特 (nats)。按照惯例，$0 \\log 0 = 0$。对 $(X, Y)$ 的联合熵定义类似：\n$$ H(X, Y) = - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} P_{XY}(x, y) \\log P_{XY}(x, y) $$\n\n**Kullback-Leibler (KL) 散度** 衡量一个概率分布 $P$ 相对于另一个预期的概率分布 $Q$ 的差异。对于离散分布，其定义为：\n$$ D_{KL}(P || Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(x)} $$\n\n问题陈述要求用 KL 散度来表述**互信息** $I(X; Y)$。互信息衡量在已知一个变量的情况下，另一个变量不确定性的减少量。形式上，它是联合分布 $P_{XY}(x, y)$ 与边缘分布乘积 $P_X(x) P_Y(y)$ 之间的 KL 散度，后者代表了独立性假设下的联合分布。\n$$ I(X; Y) = D_{KL}(P_{XY} || P_X P_Y) = \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} P_{XY}(x, y) \\log \\frac{P_{XY}(x, y)}{P_X(x) P_Y(y)} $$\n\n这个定义可以展开，将互信息与熵联系起来：\n$$\n\\begin{align*}\nI(X; Y) = \\sum_{x, y} P_{XY}(x, y) \\left[ \\log P_{XY}(x, y) - \\log P_X(x) - \\log P_Y(y) \\right] \\\\\n= -H(X, Y) - \\sum_{x, y} P_{XY}(x, y) \\log P_X(x) - \\sum_{x, y} P_{XY}(x, y) \\log P_Y(y) \\\\\n= -H(X, Y) - \\sum_x \\left( \\sum_y P_{XY}(x, y) \\right) \\log P_X(x) - \\sum_y \\left( \\sum_x P_{XY}(x, y) \\right) \\log P_Y(y) \\\\\n= -H(X, Y) - \\sum_x P_X(x) \\log P_X(x) - \\sum_y P_Y(y) \\log P_Y(y) \\\\\n= H(X) + H(Y) - H(X, Y)\n\\end{align*}\n$$\n这个最终形式 $I(X; Y) = H(X) + H(Y) - H(X, Y)$，提供了一种从估计的熵计算互信息的便捷且数值稳定的方法。\n\n**2. 从连续样本进行经验估计**\n\n潜变量 $z_i$ 和生成因子 $g_j$ 是连续的。为了应用上述离散公式，我们必须首先对数据进行离散化。\n\n给定标量变量 $x$ 的 $N$ 个样本 $\\{x^{(n)}\\}_{n=1}^N$，我们找到其最小值 $x_{\\min}$ 和最大值 $x_{\\max}$。我们将范围 $[x_{\\min}, x_{\\max}]$ 分成 $b$ 个等宽的区间。每个区间的宽度为 $\\Delta x = (x_{\\max} - x_{\\min}) / b$。区间边界为 $x_{\\min}, x_{\\min} + \\Delta x, \\dots, x_{\\max}$。问题规定最后一个区间包含最大值，这对于直方图实现是标准做法。然后将每个样本 $x^{(n)}$ 分配到 $b$ 个区间中的一个。\n\n对于一对变量 $(z_i, g_j)$，我们将此过程应用于 $\\{z_i^{(n)}\\}_{n=1}^N$ 和 $\\{g_j^{(n)}\\}_{n=1}^N$，每个变量都使用 $b$ 个区间。然后我们可以构建一个 $b \\times b$ 的联合频率矩阵 $\\text{Count}(k, l)$，其中 $\\text{Count}(k, l)$ 是满足 $z_i^{(n)}$ 落在第 $k$ 个区间且 $g_j^{(n)}$ 落在第 $l$ 个区间的样本 $(z_i^{(n)}, g_j^{(n)})$ 的数量。\n\n通过将频率矩阵除以总样本数 $N$，可以获得经验联合 PMF $\\hat{P}(z_i, g_j)$：\n$$ \\hat{P}_{Z_i, G_j}(k, l) = \\frac{\\text{Count}(k, l)}{N} $$\n通过对联合 PMF 的行和列求和，可以计算出经验边缘 PMF：\n$$ \\hat{P}_{Z_i}(k) = \\sum_{l=1}^b \\hat{P}_{Z_i, G_j}(k, l) \\qquad \\text{和} \\qquad \\hat{P}_{G_j}(l) = \\sum_{k=1}^b \\hat{P}_{Z_i, G_j}(k, l) $$\n利用这些经验 PMF，我们可以计算经验熵 $\\hat{H}(Z_i)$、$\\hat{H}(G_j)$ 和 $\\hat{H}(Z_i, G_j)$，并随后计算经验互信息 $\\hat{I}(z_i; g_j)$。\n\n**3. 互信息矩阵**\n\n通过对每一对潜变量 $z_i$（$i \\in \\{1, \\dots, d_z\\}$）和生成因子 $g_j$（$j \\in \\{1, \\dots, d_g\\}$）重复此估计过程，我们构建互信息矩阵 $I \\in \\mathbb{R}_{\\ge 0}^{d_z \\times d_g}$，其中条目 $(i, j)$ 为 $I_{ij} = \\hat{I}(z_i; g_j)$。一个良好解耦的表示在理想情况下，应在某个特定潜变量 $z_i$ 和某个特定因子 $g_j$ 之间具有高互信息，而在其他情况下互信息较低。这对应于矩阵 $I$ 是一个稀疏矩阵，且是对角矩阵的一个置换。\n\n**4. 置换不变的对角占优分数**\n\n为了以一种对潜变量维度和因子维度的任意排序不敏感的方式量化解耦程度，我们定义一个基于最优分配的分数。任务是在潜变量子集和因子子集之间找到一个一一匹配，以最大化它们互信息的总和。\n\n这是一个经典的**分配问题**（也称为最大权重二分匹配）。给定成本矩阵 $I \\in \\mathbb{R}^{d_z \\times d_g}$，我们希望找到一个 $m = \\min(d_z, d_g)$ 个行索引到 $m$ 个列索引的分配 $\\mathcal{A}$，其中索引不重复，使得总和 $\\sum_{(i,j) \\in \\mathcal{A}} I_{ij}$ 最大化。标准算法，如匈牙利算法，解决的是最小成本分配问题。为了最大化总和，我们可以在互信息矩阵的负数 $-I$ 上解决最小成本分配问题。\n\n设 $(r_1, c_1), \\dots, (r_m, c_m)$ 为最优分配的索引对，其中 $m = \\min(d_z, d_g)$。此最优分配的条目总和为：\n$$ S_{\\text{assign}} = \\sum_{k=1}^m I_{r_k, c_k} $$\n矩阵中所有条目的总和为：\n$$ S_{\\text{all}} = \\sum_{i=1}^{d_z} \\sum_{j=1}^{d_g} I_{ij} $$\n对角占优分数 $D$ 定义为最优分配的互信息总和与总互信息总和之比：\n$$ D = \\frac{S_{\\text{assign}}}{S_{\\text{all}}} $$\n该分数是归一化的，$0 \\le D \\le 1$。$D$ 的值接近 $1$ 表示大部分互信息集中在潜变量和因子之间的一一映射上，标志着良好的解耦。$D$ 的值接近 $1/m$（对于方阵）或更低可能表示存在纠缠或信息捕获不佳。根据问题要求，如果 $S_{\\text{all}} \\le 10^{-12}$，我们定义 $D=0$。这种情况发生在所有变量被发现完全独立，导致互信息矩阵全为零时。\n最终长度为 $d_z$ 的分配列表是通过将每个潜变量索引 $i$ 映射到其分配的因子索引 $j$（如果它是最优分配的一部分）来构建的，否则映射到 $-1$。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nfrom scipy.stats import entropy\n\ndef solve():\n    \"\"\"\n    Solves the disentanglement-quantification problem for all test cases.\n    \"\"\"\n    \n    # Define test cases as per the problem statement.\n    test_cases = [\n        {\n            \"d_z\": 3, \"d_g\": 3, \"N\": 5000, \"s\": 13, \"b\": 12,\n            \"type\": \"ideal\",\n        },\n        {\n            \"d_z\": 3, \"d_g\": 3, \"N\": 5000, \"s\": 7, \"b\": 12,\n            \"type\": \"independent\",\n        },\n        {\n            \"d_z\": 3, \"d_g\": 3, \"N\": 5000, \"s\": 11, \"b\": 12,\n            \"type\": \"entangled\",\n            \"A\": np.array([[1.0, 0.8, 0.5], [0.6, 1.0, 0.4], [0.5, 0.4, 1.2]]),\n        },\n        {\n            \"d_z\": 2, \"d_g\": 3, \"N\": 5000, \"s\": 17, \"b\": 12,\n            \"type\": \"dim_mismatch\",\n            \"B\": np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        d_z, d_g, N, s, b = case[\"d_z\"], case[\"d_g\"], case[\"N\"], case[\"s\"], case[\"b\"]\n        rng = np.random.default_rng(seed=s)\n\n        # 1. Generate data\n        g_samples = rng.uniform(low=-1.0, high=1.0, size=(N, d_g))\n\n        if case[\"type\"] == \"ideal\":\n            epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n            z_samples = g_samples + 0.05 * epsilon\n        elif case[\"type\"] == \"independent\":\n            z_samples = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n        elif case[\"type\"] == \"entangled\":\n            epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n            A = case[\"A\"]\n            z_samples = (A @ g_samples.T).T + 0.2 * epsilon\n        elif case[\"type\"] == \"dim_mismatch\":\n            epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n            B = case[\"B\"]\n            z_samples = (B @ g_samples.T).T + 0.05 * epsilon\n\n        # 2. Compute Mutual Information Matrix\n        mi_matrix = np.zeros((d_z, d_g))\n\n        for i in range(d_z):\n            for j in range(d_g):\n                z_i = z_samples[:, i]\n                g_j = g_samples[:, j]\n                \n                # Discretize and get joint PMF\n                # np.histogram2d binning is [left, right) except for the last bin which is [left, right], as required.\n                hist_2d, _, _ = np.histogram2d(\n                    z_i, g_j, bins=b, \n                    range=[[z_i.min(), z_i.max()], [g_j.min(), g_j.max()]]\n                )\n                \n                p_xy = hist_2d / N\n                \n                # Compute marginal PMFs\n                p_x = np.sum(p_xy, axis=1)\n                p_y = np.sum(p_xy, axis=0)\n\n                # Compute entropies using scipy.stats.entropy which handles p*log(p) = 0 for p=0\n                # Using natural log (base=e) as required.\n                h_x = entropy(p_x, base=np.e)\n                h_y = entropy(p_y, base=np.e)\n                h_xy = entropy(p_xy.flatten(), base=np.e)\n                \n                # Mutual Information I(X;Y) = H(X) + H(Y) - H(X,Y)\n                mi = h_x + h_y - h_xy\n                mi_matrix[i, j] = mi\n\n        # 3. Compute Permutation-Invariant Score\n        s_all = np.sum(mi_matrix)\n        \n        if s_all = 1e-12:\n            d_score = 0.0\n            assignment = [-1] * d_z\n        else:\n            # We want to maximize the sum, so we find the minimum cost assignment on the negative MI matrix.\n            cost_matrix = -mi_matrix\n            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n            \n            # The sum of MI values for the optimal assignment\n            s_assign = mi_matrix[row_ind, col_ind].sum()\n            \n            d_score = s_assign / s_all\n            \n            # Create the assignment list (latent index -> factor index)\n            assignment = [-1] * d_z\n            for r, c in zip(row_ind, col_ind):\n                assignment[r] = c\n\n        # 4. Format results\n        D_rounded = round(d_score, 4)\n        I_flat_rounded = [round(val, 4) for val in mi_matrix.flatten(order='C')]\n        \n        all_results.append([D_rounded, assignment, I_flat_rounded])\n\n    # Final print statement must match the required format exactly.\n    # Custom formatting to avoid spaces after commas in lists.\n    result_str = \",\".join([\n        f\"[{res[0]},{str(res[1]).replace(' ', '')},{str(res[2]).replace(' ', '')}]\" for res in all_results\n    ])\n    print(f\"[{result_str}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "潜空间的容量是一个关键的超参数，它决定了重构质量与正则化强度之间的权衡。本练习使用一个简化的理论模型来探索潜变量维度——无论是欠完备还是过完备——如何影响模型解耦底层变化因子的能力 。这将帮助您建立关于 $\\beta$-VAE 中信息瓶颈的直观理解。",
            "id": "3116839",
            "problem": "考虑一个简化的β-变分自编码器（β-VAE）在线性高斯生成过程中的解缠分析。数据集由$k$个独立的真实因子$\\{v_i\\}_{i=1}^k$组成，每个因子都以各自的信噪比$\\mathrm{SNR}_i$为观测数据贡献加性信号。假设编码器-解码器对和先验都是高斯和线性的，先验为各向同性的标准正态分布$p(z) = \\mathcal{N}(0, I)$，并且近似后验$q(z \\mid x)$是因子化的高斯分布。\n\n待使用的基本定义和事实：\n- 近似后验$q(z \\mid x)$与先验$p(z)$之间的Kullback-Leibler散度（KL）表示为$\\mathrm{KL}(q(z \\mid x) \\parallel p(z))$。\n- 变分自编码器（VAE）的证据下界（ELBO）包含一个期望重构项和一个正则化项；在β-VAE中，正则化项由$\\beta$加权，得到目标函数\n$$\n\\mathcal{L} = \\mathbb{E}_{x}\\left[ \\mathbb{E}_{q(z \\mid x)}\\left[-\\log p(x \\mid z)\\right] \\right] + \\beta \\, \\mathbb{E}_{x}\\left[ \\mathrm{KL}\\left(q(z \\mid x) \\parallel p(z)\\right) \\right].\n$$\n- 对于信噪比为$\\mathrm{SNR}$的线性高斯信道，其信息容量（以奈特为单位）为\n$$\nC(\\mathrm{SNR}) = \\frac{1}{2} \\ln(1 + \\mathrm{SNR}).\n$$\n\n此问题的建模假设：\n1. 数据集的复杂度由每个因子的容量$C_i = \\frac{1}{2}\\ln(1 + \\mathrm{SNR}_i)$（对于$i \\in \\{1, \\dots, k\\}$）来量化，这代表了关于因子$i$所能承载的最大互信息。\n2. β-VAE的正则化施加了一个信息瓶颈。在线性高斯假设和率失真风格的拉格朗日量下，假设每个隐维度承载的最优平均互信息与$\\beta$成反比，比例常数为$b_0 = 1$奈特，即每个隐维度有一个信息预算\n$$\nB(\\beta) = \\frac{b_0}{\\beta} = \\frac{1}{\\beta}.\n$$\n因此，对于$d_z$个隐维度，总的隐信息预算为\n$$\nC_{\\mathrm{budget}}(d_z, \\beta) = d_z \\cdot B(\\beta) = \\frac{d_z}{\\beta}.\n$$\n\n解缠分数定义：\n- 将因子$i$的容量缩放互信息间隙（CS-MIG）定义为\n$$\n\\mathrm{CS\\text{-}MIG}_i = \\frac{m_{i,(1)} - m_{i,(2)}}{C_i},\n$$\n其中$m_{i,(1)}$和$m_{i,(2)}$是任意隐维度对因子$i$的最大和第二大互信息贡献，$C_i$是因子$i$的容量。如果$C_i = 0$，则定义$\\mathrm{CS\\text{-}MIG}_i = 0$。总分是所有因子的平均值：\n$$\n\\overline{\\mathrm{CS\\text{-}MIG}} = \\frac{1}{k} \\sum_{i=1}^k \\mathrm{CS\\text{-}MIG}_i.\n$$\n\n用于近似互信息如何分布的分配规则：\n- 由于$C_{\\mathrm{budget}}$不能超过$\\sum_{i=1}^k C_i$，分配给因子$i$的互信息以$C_i$为上限。假设采用类似贪心注水的分配方法：\n    1. 将每个因子的剩余容量初始化为$R_i \\leftarrow C_i$，并将每个因子的贡献初始化为空列表。\n    2. 对于$d_z$个隐维度中的每一个，将其预算$B(\\beta)$分配给当前剩余容量$R_i$最大的因子。分配量为$a = \\min\\{B(\\beta), R_i\\}$。将$a$记录为该因子的一个贡献，并更新$R_i \\leftarrow R_i - a$。\n    3. 在所有$d_z$次分配完成后，根据记录的贡献计算每个因子的$m_{i,(1)}$和$m_{i,(2)}$（如果贡献少于两个，则用零填充），然后计算$\\mathrm{CS\\text{-}MIG}_i$并对所有$i$求平均。\n\n任务：\n- 实现一个程序，对于固定的$\\beta$和固定的数据集复杂度，改变隐维度$d_z$，并报告不完备与过完备的隐空间如何影响CS-MIG分数。\n\n测试套件：\n- 固定$\\beta = 4.0$和$k = 5$，$\\mathrm{SNR}$值为$\\left[10.0, 5.0, 2.0, 1.0, 0.5\\right]$。\n- 在四种情况下改变$d_z$，以涵盖不完备、精确匹配和过完备的设置：\n    - 案例A：$d_z = 1$（极端不完备）。\n    - 案例B：$d_z = 2$（不完备）。\n    - 案例C：$d_z = 5$（与$k$精确匹配）。\n    - 案例D：$d_z = 8$（过完备）。\n\n答案规格：\n- 对于每个测试用例，输出平均CS-MIG分数$\\overline{\\mathrm{CS\\text{-}MIG}}$，结果为浮点数并四舍五入到六位小数。\n- 你的程序应该生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，\"[result_A,result_B,result_C,result_D]\"）。",
            "solution": "问题陈述已经过严格验证，被认为是有效的。它在β-VAE简化理论模型的背景下具有科学依据，在数学上是适定的、客观的且自洽的。唯一解所需的所有定义、参数和算法规则均已提供。因此，我们可以着手求解。\n\n该问题要求在一个简化的线性高斯β-VAE模型中，计算不同隐维度$d_z$下的平均容量缩放互信息间隙$\\overline{\\mathrm{CS\\text{-}MIG}}$。问题的核心是一个贪心分配算法，该算法将隐空间的信息预算分配给变异的真实因子。\n\n首先，我们为所有测试用例建立常量参数。\n因子数量为$k=5$。\n正则化参数为$\\beta = 4.0$。\n五个因子的信噪比给定为$\\mathrm{SNR} = [10.0, 5.0, 2.0, 1.0, 0.5]$。\n\n由此，我们推导出两组关键值：\n1.  计算每个因子$i \\in \\{1, \\dots, 5\\}$的信息容量，$C_i = \\frac{1}{2} \\ln(1 + \\mathrm{SNR}_i)$。\n    - $C_1 = \\frac{1}{2} \\ln(1 + 10.0) = \\frac{1}{2} \\ln(11) \\approx 1.198948$\n    - $C_2 = \\frac{1}{2} \\ln(1 + 5.0) = \\frac{1}{2} \\ln(6) \\approx 0.895880$\n    - $C_3 = \\frac{1}{2} \\ln(1 + 2.0) = \\frac{1}{2} \\ln(3) \\approx 0.549306$\n    - $C_4 = \\frac{1}{2} \\ln(1 + 1.0) = \\frac{1}{2} \\ln(2) \\approx 0.346574$\n    - $C_5 = \\frac{1}{2} \\ln(1 + 0.5) = \\frac{1}{2} \\ln(1.5) \\approx 0.202733$\n\n2.  每个隐维度的信息预算$B(\\beta) = \\frac{1}{\\beta}$为：\n    - $B(\\beta) = \\frac{1}{4.0} = 0.25$ 奈特。\n\n现在，我们对四个测试用例中的每一个应用指定的贪心分配算法。在算法的每一步中，一个隐维度的预算$B(\\beta) = 0.25$奈特被分配给当前剩余容量$R_i$最高的因子。\n\n**案例A：$d_z = 1$ (极端不完备)**\n- **分配**：只有一个隐维度，其0.25的预算被分配给初始容量最高的因子，即因子1（$C_1 \\approx 1.199$）。分配量为$a = \\min(0.25, C_1) = 0.25$。\n- **贡献**：\n    - 因子1：$m_{1,(1)} = 0.25$, $m_{1,(2)} = 0$。\n    - 因子2、3、4、5：无贡献，因此$m_{i,(1)} = 0$且$m_{i,(2)} = 0$。\n- **CS-MIG计算**：\n    - $\\mathrm{CS\\text{-}MIG}_1 = \\frac{m_{1,(1)} - m_{1,(2)}}{C_1} = \\frac{0.25 - 0}{1.198948} \\approx 0.208514$。\n    - $\\mathrm{CS\\text{-}MIG}_{2,3,4,5} = 0$。\n- **平均分数**：\n    - $\\overline{\\mathrm{CS\\text{-}MIG}} = \\frac{1}{5} (0.208514 + 0 + 0 + 0 + 0) \\approx 0.041703$。\n\n**案例B：$d_z = 2$ (不完备)**\n- **分配**：\n    1. 第1个维度 → 因子1 (初始容量最高)。$R_1$ 变为 $1.199 - 0.25 \\approx 0.949$。\n    2. 第2个维度 → 因子1 (剩余容量最高，$R_1 \\approx 0.949 > C_2 \\approx 0.896$)。$R_1$ 变为 $0.949 - 0.25 \\approx 0.699$。\n- **贡献**：\n    - 因子1：两个0.25的贡献。因此，$m_{1,(1)} = 0.25$ 且 $m_{1,(2)} = 0.25$。\n    - 因子2、3、4、5：无贡献。\n- **CS-MIG计算**：\n    - $\\mathrm{CS\\text{-}MIG}_1 = \\frac{0.25 - 0.25}{C_1} = 0$。\n    - $\\mathrm{CS\\text{-}MIG}_{2,3,4,5} = 0$。\n- **平均分数**：\n    - $\\overline{\\mathrm{CS\\text{-}MIG}} = 0.0$。\n\n**案例C：$d_z = 5$ (精确匹配)**\n- **分配**：\n    1. 第1个维度 → 因子1 ($R_1 \\approx 0.949$ after)。\n    2. 第2个维度 → 因子1 ($R_1 \\approx 0.699$ after)。\n    3. 第3个维度 → 因子2 (当前剩余容量最高是$C_2 \\approx 0.896$)。$R_2 \\approx 0.646$。\n    4. 第4个维度 → 因子1 (当前剩余容量最高是$R_1 \\approx 0.699$)。$R_1 \\approx 0.449$。\n    5. 第5个维度 → 因子2 (当前剩余容量最高是$R_2 \\approx 0.646$)。$R_2 \\approx 0.396$。\n- **贡献**：\n    - 因子1：三个0.25的贡献。$m_{1,(1)} = 0.25$, $m_{1,(2)} = 0.25$。\n    - 因子2：两个0.25的贡献。$m_{2,(1)} = 0.25$, $m_{2,(2)} = 0.25$。\n    - 因子3、4、5：无贡献。\n- **CS-MIG计算**：\n    - $\\mathrm{CS\\text{-}MIG}_1 = \\frac{0.25 - 0.25}{C_1} = 0$。\n    - $\\mathrm{CS\\text{-}MIG}_2 = \\frac{0.25 - 0.25}{C_2} = 0$。\n    - $\\mathrm{CS\\text{-}MIG}_{3,4,5} = 0$。\n- **平均分数**：\n    - $\\overline{\\mathrm{CS\\text{-}MIG}} = 0.0$。\n\n**案例D：$d_z = 8$ (过完备)**\n- **分配**：前5次分配与案例C相同。分配完5个维度后，剩余容量为$R \\approx [0.449, 0.396, 0.549, 0.347, 0.203]$。\n    6. 第6个维度 → 因子3 (当前剩余容量最高是$R_3 \\approx 0.549$)。$R_3 \\approx 0.299$。\n    7. 第7个维度 → 因子1 (当前剩余容量最高是$R_1 \\approx 0.449$)。$R_1 \\approx 0.199$。\n    8. 第8个维度 → 因子2 (当前剩余容量最高是$R_2 \\approx 0.396$)。$R_2 \\approx 0.146$。\n- **贡献**：\n    - 因子1：四个0.25的贡献。$m_{1,(1)} = 0.25$, $m_{1,(2)} = 0.25$。\n    - 因子2：三个0.25的贡献。$m_{2,(1)} = 0.25$, $m_{2,(2)} = 0.25$。\n    - 因子3：一个0.25的贡献。$m_{3,(1)} = 0.25$, $m_{3,(2)} = 0$。\n    - 因子4、5：无贡献。\n- **CS-MIG计算**：\n    - $\\mathrm{CS\\text{-}MIG}_1 = 0$。\n    - $\\mathrm{CS\\text{-}MIG}_2 = 0$。\n    - $\\mathrm{CS\\text{-}MIG}_3 = \\frac{0.25 - 0}{C_3} = \\frac{0.25}{0.549306} \\approx 0.455113$。\n    - $\\mathrm{CS\\text{-}MIG}_{4,5} = 0$。\n- **平均分数**：\n    - $\\overline{\\mathrm{CS\\text{-}MIG}} = \\frac{1}{5} (0 + 0 + 0.455113 + 0 + 0) \\approx 0.091023$。\n\n结果总结：\n- 案例A ($d_z = 1$): $\\overline{\\mathrm{CS\\text{-}MIG}} \\approx 0.041703$\n- 案例B ($d_z = 2$): $\\overline{\\mathrm{CS\\text{-}MIG}} = 0.0$\n- 案例C ($d_z = 5$): $\\overline{\\mathrm{CS\\text{-}MIG}} = 0.0$\n- 案例D ($d_z = 8$): $\\overline{\\mathrm{CS\\text{-}MIG}} \\approx 0.091023$\n最终的程序将计算这些值，并按要求格式化输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_cs_mig(dz, k, beta, snr_values):\n    \"\"\"\n    Calculates the average Capacity-Scaled Mutual Information Gap (CS-MIG).\n\n    Args:\n        dz (int): The number of latent dimensions.\n        k (int): The number of ground-truth factors.\n        beta (float): The regularization parameter of the β-VAE.\n        snr_values (np.ndarray): An array of signal-to-noise ratios.\n\n    Returns:\n        float: The calculated average CS-MIG score.\n    \"\"\"\n    # 1. Calculate per-factor capacities\n    capacities = 0.5 * np.log(1 + np.array(snr_values))\n\n    # 2. Calculate information budget per latent dimension\n    budget_per_dim = 1.0 / beta\n\n    # 3. Apply the greedy allocation algorithm\n    remaining_capacities = capacities.copy()\n    contributions = [[] for _ in range(k)]\n\n    for _ in range(dz):\n        if np.all(remaining_capacities = 0):\n            break  # No more capacity to allocate\n\n        # Find the factor with the largest current remaining capacity\n        target_factor_idx = np.argmax(remaining_capacities)\n\n        # Allocate budget\n        allocation_amount = min(budget_per_dim, remaining_capacities[target_factor_idx])\n        \n        contributions[target_factor_idx].append(allocation_amount)\n        remaining_capacities[target_factor_idx] -= allocation_amount\n\n    # 4. Compute CS-MIG per factor and average\n    total_cs_mig = 0.0\n    for i in range(k):\n        factor_contribs = contributions[i]\n        \n        # Sort contributions to find m_i,(1) and m_i,(2)\n        factor_contribs.sort(reverse=True)\n\n        m1 = factor_contribs[0] if len(factor_contribs) > 0 else 0.0\n        m2 = factor_contribs[1] if len(factor_contribs) > 1 else 0.0\n\n        cs_mig_i = 0.0\n        # Per problem, if Ci=0, CS-MIGi=0. We also avoid division by zero.\n        if capacities[i] > 0:\n            cs_mig_i = (m1 - m2) / capacities[i]\n        \n        total_cs_mig += cs_mig_i\n    \n    return total_cs_mig / k\n\n\ndef solve():\n    \"\"\"\n    Solves the problem by running the simulation for the given test suite.\n    \"\"\"\n    # Define the constants and test cases from the problem statement.\n    BETA = 4.0\n    K = 5\n    SNR_VALUES = [10.0, 5.0, 2.0, 1.0, 0.5]\n    \n    test_cases = [\n        # Case A, B, C, D\n        1,  # dz = 1\n        2,  # dz = 2\n        5,  # dz = 5\n        8,  # dz = 8\n    ]\n\n    results = []\n    for dz_val in test_cases:\n        result = calculate_cs_mig(dz=dz_val, k=K, beta=BETA, snr_values=SNR_VALUES)\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "标准的 $\\beta$-VAE 假设潜变量是相互独立的，但这对于具有层级结构的真实世界数据可能并不成立。这个高级练习将探究如何将已知的依赖关系编码到潜变量的先验分布中，从而显著改善解耦效果。通过比较一个使用标准先验的 VAE 和一个使用匹配数据层级结构的结构化先验的 VAE，您将体会到将归纳偏置（inductive biases）融入模型设计的强大威力 。",
            "id": "3116938",
            "problem": "在一个线性高斯设定下，你被要求形式化并检验一个显式编码了粗粒度和细粒度隐因子之间层级依赖关系的结构化隐先验，与一个使用独立标准正态先验的标准Beta-变分自编码器（Beta-VAE）相比，是否能改善解耦效果。整个过程完全在线性高斯体系中进行，以确保所有推导都是精确的，并且无需任何数据采样即可计算。\n\n设置与定义：\n-   考虑一个生成模型，其隐向量为 $z = \\begin{bmatrix} z_{\\mathrm{c}} \\\\ z_{\\mathrm{f}} \\end{bmatrix} \\in \\mathbb{R}^{d}$，其中 $z_{\\mathrm{c}} \\in \\mathbb{R}^{d_{\\mathrm{c}}}$ 是粗粒度因子，$z_{\\mathrm{f}} \\in \\mathbb{R}^{d_{\\mathrm{f}}}$ 是细粒度因子，且 $d = d_{\\mathrm{c}} + d_{\\mathrm{f}}$。\n-   观测模型是线性高斯的：$x \\in \\mathbb{R}^{m}$，其分布为 $p(x \\mid z) = \\mathcal{N}(x \\mid G z, \\sigma_{x}^{2} I_{m})$，其中 $G \\in \\mathbb{R}^{m \\times d}$ 且 $\\sigma_{x}^{2} \\in \\mathbb{R}_{0}$。\n-   数据分布由一个反映了分解层级结构的结构化层级高斯先验生成：$p_{\\mathrm{true}}(z) = p(z_{\\mathrm{c}}) p(z_{\\mathrm{f}} \\mid z_{\\mathrm{c}})$，其中 $p(z_{\\mathrm{c}}) = \\mathcal{N}(z_{\\mathrm{c}} \\mid 0, I_{d_{\\mathrm{c}}})$ 且 $p(z_{\\mathrm{f}} \\mid z_{\\mathrm{c}}) = \\mathcal{N}(z_{\\mathrm{f}} \\mid B z_{\\mathrm{c}}, \\sigma_{\\mathrm{f}}^{2} I_{d_{\\mathrm{f}}})$，这里的 $B \\in \\mathbb{R}^{d_{\\mathrm{f}} \\times d_{\\mathrm{c}}}$ 编码了细粒度因子如何依赖于粗粒度因子。请推导$z$所隐含的联合高斯先验协方差。\n-   对于一个固定的解码器 $p(x \\mid z)$ 和一个选定的先验 $p(z)$，将Beta-变分自编码器 (Beta-VAE) 的目标函数定义为对每个$x$最大化量 $\\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] - \\beta \\, \\mathrm{KL}(q(z \\mid x) \\,\\|\\, p(z))$，其中 $\\beta \\in \\mathbb{R}_{0}$，$\\mathrm{KL}$ 表示Kullback–Leibler散度。假设使用一个灵活的高斯编码器 $q(z \\mid x)$，并且在高斯族内达到了最优的 $q(z \\mid x)$。\n\n任务：\n1.  仅从多维高斯密度、线性高斯似然和Beta-变分自编码器目标函数的定义出发，对于一个给定的高斯先验 $p(z) = \\mathcal{N}(z \\mid 0, \\Sigma_{0})$ 和线性解码器 $p(x \\mid z) = \\mathcal{N}(x \\mid G z, \\sigma_{x}^{2} I_{m})$，推导在Beta-变分自编码器目标函数下的最优高斯编码器 $q^{\\star}(z \\mid x)$。你的推导必须展示 $q^{\\star}(z \\mid x)$ 的精度矩阵和均值如何依赖于 $\\beta$、$\\Sigma_{0}$、$G$ 和 $\\sigma_{x}^{2}$，且不假设任何已知的闭式解。\n2.  对于从上述结构化先验 $p_{\\mathrm{true}}(z)$ 和解码器 $G$ 生成的数据，计算在 $q^{\\star}(z \\mid x)$ 下$z$的聚合后验协方差，即当$x$根据数据分布进行分布且$z \\sim q^{\\star}(z \\mid x)$时，$z$的协方差。仅使用线性高斯恒等式和期望。\n3.  使用第2项中的聚合后验协方差，为隐向量 $z$ 计算两个与解耦相关的量：\n    -   聚合后验的总相关性（对于多维高斯分布，该量仅取决于其协方差），定义为联合分布与其边际分布乘积之间的Kullback–Leibler散度。\n    -   在聚合后验下，块变量 $z_{\\mathrm{c}}$ 和 $z_{\\mathrm{f}}$ 之间的互信息，仅使用高斯分布的块协方差定义。\n4.  对于相同的数据分布和解码器，重复任务3两次：一次是在Beta-变分自编码器目标函数中使用标准独立先验 $p_{\\mathrm{std}}(z) = \\mathcal{N}(z \\mid 0, I_{d})$ 时，另一次是使用与数据层级结构匹配的结构化先验 $p_{\\mathrm{struct}}(z) = \\mathcal{N}(z \\mid 0, \\Sigma_{\\mathrm{struct}})$ 时，该先验由 $p(z_{\\mathrm{c}})$ 和 $p(z_{\\mathrm{f}} \\mid z_{\\mathrm{c}})$ 推导得出。\n5.  对于下述每个测试用例，计算两个标量：\n    -   $\\Delta \\mathrm{TC} = \\mathrm{TC}(q_{\\mathrm{std}}(z)) - \\mathrm{TC}(q_{\\mathrm{struct}}(z))$，\n    -   $\\Delta I = I_{q}(z_{\\mathrm{c}}; z_{\\mathrm{f}})_{\\mathrm{std}} - I_{q}(z_{\\mathrm{c}}; z_{\\mathrm{f}})_{\\mathrm{struct}}$，\n    其中下标“std”表示在Beta-变分自编码器目标函数中使用$p_{\\mathrm{std}}(z)$，而“struct”表示使用$p_{\\mathrm{struct}}(z)$。正值表示结构化先验相对于标准先验所实现的减少量。\n\n具体计算规范：\n-   使用维度 $d_{\\mathrm{c}} = d_{\\mathrm{f}} = 2$，总隐维度 $d = 4$，以及观测维度 $m = 4$。\n-   使用块对角解码器 $G = \\mathrm{diag}(I_{2}, I_{2})$，并定义 $B = \\alpha I_{2}$，其中标量耦合强度 $\\alpha \\in \\mathbb{R}_{\\ge 0}$。\n-   真实数据先验是具有参数 $\\alpha$ 和 $\\sigma_{\\mathrm{f}}^{2}$ 的上述层级高斯分布；观测噪声方差为 $\\sigma_{x}^{2}$。\n-   所有矩阵都必须作为完整矩阵处理；除非由指定的结构得出，否则在中间推导中不要假设对角性。\n\n测试套件：\n按此确切顺序，为以下参数集 $(\\beta, \\alpha, \\sigma_{x}, \\sigma_{\\mathrm{f}})$ 提供输出：\n-   案例 1：$(1.0, 0.8, 0.1, 0.5)$。\n-   案例 2：$(4.0, 0.8, 0.1, 0.5)$。\n-   案例 3：$(1.0, 0.0, 0.1, 0.5)$。\n-   案例 4：$(1.0, 1.5, 0.1, 0.5)$。\n-   案例 5：$(10.0, 0.8, 0.1, 0.5)$。\n\n要求的最终输出格式：\n-   你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按 $[\\Delta \\mathrm{TC}_{1}, \\Delta I_{1}, \\Delta \\mathrm{TC}_{2}, \\Delta I_{2}, \\ldots, \\Delta \\mathrm{TC}_{5}, \\Delta I_{5}]$ 的顺序排列，其中下标表示上面列表中的测试用例索引。\n-   每个条目必须是实数（浮点数）。不涉及物理单位。",
            "solution": "核心任务是在一个$\\beta$-VAE框架内，分析结构化隐先验与标准各向同性先验对表示解耦的影响。此分析在一个完全可解的线性高斯设定中进行。\n\n### 1. 结构化先验协方差的推导\n\n问题指定了一个具有层级隐结构的真实数据生成过程。隐向量为 $z = \\begin{bmatrix} z_{\\mathrm{c}} \\\\ z_{\\mathrm{f}} \\end{bmatrix} \\in \\mathbb{R}^{d}$，其中 $z_{\\mathrm{c}} \\in \\mathbb{R}^{d_{\\mathrm{c}}}$ 是粗粒度因子，$z_{\\mathrm{f}} \\in \\mathbb{R}^{d_{\\mathrm{f}}}$ 是细粒度因子。真实先验为 $p_{\\mathrm{true}}(z) = p(z_{\\mathrm{c}}) p(z_{\\mathrm{f}} \\mid z_{\\mathrm{c}})$，其中：\n-   $p(z_{\\mathrm{c}}) = \\mathcal{N}(z_{\\mathrm{c}} \\mid 0, I_{d_{\\mathrm{c}}})$\n-   $p(z_{\\mathrm{f}} \\mid z_{\\mathrm{c}}) = \\mathcal{N}(z_{\\mathrm{f}} \\mid B z_{\\mathrm{c}}, \\sigma_{\\mathrm{f}}^{2} I_{d_{\\mathrm{f}}})$\n\n这为$z$定义了一个联合高斯分布。其均值为 $E[z] = \\begin{bmatrix} E[z_{\\mathrm{c}}] \\\\ E[E[z_{\\mathrm{f}} \\mid z_{\\mathrm{c}}]] \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ E[B z_{\\mathrm{c}}] \\end{bmatrix} = 0$。\n\n协方差矩阵 $\\Sigma_{\\mathrm{struct}}$ 通过计算块协方差得出：\n-   $\\mathrm{Cov}(z_{\\mathrm{c}}) = I_{d_{\\mathrm{c}}}$\n-   $\\mathrm{Cov}(z_{\\mathrm{c}}, z_{\\mathrm{f}}) = E[z_{\\mathrm{c}} z_{\\mathrm{f}}^T] = E[E[z_{\\mathrm{c}} z_{\\mathrm{f}}^T \\mid z_{\\mathrm{c}}]] = E[z_{\\mathrm{c}} (E[z_{\\mathrm{f}} \\mid z_{\\mathrm{c}}])^T] = E[z_{\\mathrm{c}} (B z_{\\mathrm{c}})^T] = E[z_{\\mathrm{c}} z_{\\mathrm{c}}^T B^T] = \\mathrm{Cov}(z_{\\mathrm{c}}) B^T = B^T$\n-   $\\mathrm{Cov}(z_{\\mathrm{f}}, z_{\\mathrm{c}}) = \\mathrm{Cov}(z_{\\mathrm{c}}, z_{\\mathrm{f}})^T = B$\n-   $\\mathrm{Cov}(z_{\\mathrm{f}})$ 使用全协方差定律求得：\n    $\\mathrm{Cov}(z_{\\mathrm{f}}) = E[\\mathrm{Cov}(z_{\\mathrm{f}} \\mid z_{\\mathrm{c}})] + \\mathrm{Cov}(E[z_{\\mathrm{f}} \\mid z_{\\mathrm{c}}]) = E[\\sigma_{\\mathrm{f}}^{2} I_{d_{\\mathrm{f}}}] + \\mathrm{Cov}(B z_{\\mathrm{c}}) = \\sigma_{\\mathrm{f}}^{2} I_{d_{\\mathrm{f}}} + B \\mathrm{Cov}(z_{\\mathrm{c}}) B^T = \\sigma_{\\mathrm{f}}^{2} I_{d_{\\mathrm{f}}} + B B^T$\n\n因此，结构化先验的协方差为：\n$$\n\\Sigma_{\\mathrm{struct}} = \\begin{pmatrix} I_{d_{\\mathrm{c}}}  B^T \\\\ B  \\sigma_{\\mathrm{f}}^{2} I_{d_{\\mathrm{f}}} + B B^T \\end{pmatrix}\n$$\n这既是真实数据生成先验 $p_{\\mathrm{true}}(z)$ 的协方差，也是实验的一个分支中使用的结构化先验模型 $p_{\\mathrm{struct}}(z)$ 的协方差。\n\n### 2. 最优变分后验 $q^{\\star}(z \\mid x)$\n\n$\\beta$-VAE的目标是关于变分后验 $q(z \\mid x)$ 的参数最大化 $\\mathcal{L}(q) = \\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] - \\beta \\, \\mathrm{KL}(q(z \\mid x) \\,\\|\\, p(z))$。这等价于最小化Kullback-Leibler散度 $\\mathrm{KL}(q(z \\mid x) \\,\\|\\, p^*(z))$，其中 $p^*(z) \\propto p(x \\mid z)^{1/\\beta} p(z)$。因此，最优的 $q^{\\star}(z \\mid x)$ 是这个复合分布的归一化版本。\n\n给定似然 $p(x \\mid z) = \\mathcal{N}(x \\mid Gz, \\sigma_x^2 I_m)$ 和通用高斯先验 $p(z) = \\mathcal{N}(z \\mid 0, \\Sigma_0)$ 的高斯形式，最优后验的对数密度正比于：\n$$\n\\log q^{\\star}(z \\mid x) \\propto \\frac{1}{\\beta} \\log p(x \\mid z) + \\log p(z)\n$$\n代入高斯分布的对数密度（并忽略常数）：\n$$\n\\log q^{\\star}(z \\mid x) \\propto \\frac{1}{\\beta} \\left(-\\frac{1}{2\\sigma_x^2}(x - Gz)^T(x - Gz)\\right) + \\left(-\\frac{1}{2}z^T \\Sigma_0^{-1} z\\right)\n$$\n$$\n\\propto -\\frac{1}{2\\beta\\sigma_x^2}(z^T G^T G z - 2x^T G z) - \\frac{1}{2}z^T \\Sigma_0^{-1} z\n$$\n$$\n\\propto -\\frac{1}{2} z^T \\left(\\frac{1}{\\beta\\sigma_x^2} G^T G + \\Sigma_0^{-1}\\right) z + \\left(\\frac{1}{\\beta\\sigma_x^2} G^T x\\right)^T z\n$$\n这是$z$的二次型，证实了 $q^{\\star}(z \\mid x)$ 是高斯分布，即 $q^{\\star}(z \\mid x) = \\mathcal{N}(z \\mid \\mu_q, \\Sigma_q)$。通过配方法或与标准对数高斯形式 $-\\frac{1}{2}(z-\\mu_q)^T\\Sigma_q^{-1}(z-\\mu_q)$ 的项进行匹配，我们确定了精度矩阵 $\\Sigma_q^{-1}$ 和均值 $\\mu_q$：\n-   精度矩阵： $\\Sigma_q^{-1} = \\frac{1}{\\beta\\sigma_x^2} G^T G + \\Sigma_0^{-1}$\n-   均值： $\\mu_q(x) = \\Sigma_q \\left(\\frac{1}{\\beta\\sigma_x^2} G^T x\\right)$\n\n因此，最优变分后验的协方差是 $\\Sigma_q = \\left(\\frac{1}{\\beta\\sigma_x^2} G^T G + \\Sigma_0^{-1}\\right)^{-1}$，它与观测值$x$无关。\n\n### 3. 聚合后验协方差\n\n聚合后验是首先采样 $x \\sim p_{\\mathrm{data}}(x)$ 然后采样 $z \\sim q^{\\star}(z \\mid x)$ 所得到的$z$的分布。这个聚合分布的协方差 $\\Sigma_{agg}$ 通过全协方差定律找到：\n$$\n\\Sigma_{agg} = E_{p_{\\mathrm{data}}(x)}[\\mathrm{Cov}_{q^{\\star}}(z \\mid x)] + \\mathrm{Cov}_{p_{\\mathrm{data}}(x)}[E_{q^{\\star}}(z \\mid x)]\n$$\n第一项是 $E_{p_{\\mathrm{data}}(x)}[\\Sigma_q] = \\Sigma_q$，因为 $\\Sigma_q$ 不依赖于 $x$。\n第二项是 $\\mathrm{Cov}_{p_{\\mathrm{data}}(x)}[\\mu_q(x)] = \\mathrm{Cov}_{p_{\\mathrm{data}}(x)}\\left[\\Sigma_q \\frac{1}{\\beta\\sigma_x^2} G^T x\\right]$。\n设 $M = \\Sigma_q \\frac{1}{\\beta\\sigma_x^2} G^T$。此项为 $\\mathrm{Cov}(Mx) = M \\mathrm{Cov}(x) M^T$。\n数据分布 $p_{\\mathrm{data}}(x) = \\int p(x|z) p_{\\mathrm{true}}(z) dz$ 的协方差是 $\\mathrm{Cov}(x) = G \\Sigma_{\\mathrm{true}} G^T + \\sigma_x^2 I_m$。\n将这些结合起来得到：\n$$\n\\Sigma_{agg} = \\Sigma_q + \\left(\\Sigma_q \\frac{G^T}{\\beta\\sigma_x^2}\\right) (G \\Sigma_{\\mathrm{true}} G^T + \\sigma_x^2 I_m) \\left(\\frac{G}{\\beta\\sigma_x^2} \\Sigma_q\\right)\n$$\n这个表达式取决于VAE先验 $\\Sigma_0$（通过$\\Sigma_q$）和真实数据先验 $\\Sigma_{\\mathrm{true}}$。\n\n### 4. 解耦度量\n\n对于一个协方差为 $\\Sigma$ 的$d$维零均值高斯变量$z$，指定的度量如下：\n-   **总相关性 (TC)**：联合分布与其边际分布乘积之间的KL散度。\n    $$\n    \\mathrm{TC}(z) = \\mathrm{KL}(\\mathcal{N}(0, \\Sigma) \\,\\|\\, \\mathcal{N}(0, \\mathrm{diag}(\\Sigma_{11}, ..., \\Sigma_{dd}))) = \\frac{1}{2}\\left(\\sum_{i=1}^d \\log(\\Sigma_{ii}) - \\log|\\Sigma|\\right)\n    $$\n-   **互信息 (MI)**：对于一个分块向量 $z = [z_c^T, z_f^T]^T$，其块协方差为 $\\Sigma = \\begin{pmatrix} \\Sigma_{cc}  \\Sigma_{cf} \\\\ \\Sigma_{fc}  \\Sigma_{ff} \\end{pmatrix}$：\n    $$\n    I(z_c; z_f) = \\frac{1}{2}\\log\\frac{|\\Sigma_{cc}||\\Sigma_{ff}|}{|\\Sigma|}\n    $$\n\n### 5. 为计算进行的特化\n\n问题提供了具体参数：$d_c=d_f=2$, $G=I_4$, $B=\\alpha I_2$。真实协方差 $\\Sigma_{\\mathrm{true}}$ 可通过置换变为块对角形式 $\\begin{pmatrix} S  0 \\\\ 0  S \\end{pmatrix}$，其中 $S = \\begin{pmatrix} 1  \\alpha \\\\ \\alpha  \\sigma_f^2+\\alpha^2 \\end{pmatrix}$。推导中的所有其他矩阵（$I_4$，以及因此的 $\\Sigma_q$ 和 $\\Sigma_{agg}$）在置换后的基中也共享此属性。这使得所有计算都可以在 $2 \\times 2$ 的块级别上执行。\n设$M$为$\\Sigma_{agg}$在此基下的$2 \\times 2$块。完整的$4 \\times 4$协方差与$\\begin{pmatrix} M  0 \\\\ 0  M \\end{pmatrix}$置换相似。在原始基 $(z_{c1}, z_{c2}, z_{f1}, z_{f2})$下，这导致了$\\Sigma_{agg}$的一个特定结构。\n$\\Sigma_{agg}$的对角元素是 $(M_{11}, M_{11}, M_{22}, M_{22})$，其行列式是 $|\\Sigma_{agg}| = (\\det M)^2$。用于互信息的块是 $\\Sigma_{cc} = M_{11}I_2$ 和 $\\Sigma_{ff} = M_{22}I_2$。\n将度量公式应用于此结构：\n-   $\\mathrm{TC}(\\Sigma_{agg}) = \\frac{1}{2}(2\\log M_{11} + 2\\log M_{22} - \\log((\\det M)^2)) = \\log\\frac{M_{11} M_{22}}{\\det M}$\n-   $I_{q}(z_c; z_f) = \\frac{1}{2}\\log\\frac{|M_{11}I_2||M_{22}I_2|}{(\\det M)^2} = \\frac{1}{2}\\log\\frac{M_{11}^2 M_{22}^2}{(\\det M)^2} = \\log\\frac{M_{11} M_{22}}{|\\det M|}$\n\n对于一个协方差矩阵，$\\det M  0$。因此，在这个特定设置下，$\\mathrm{TC}(z) = I_q(z_c; z_f)$。因此，$\\Delta \\mathrm{TC}$将与$\\Delta I$相同。计算过程是通过为标准先验情况（$\\Sigma_0 = I_4$）和结构化先验情况（$\\Sigma_0 = \\Sigma_{\\mathrm{true}}$）计算$2 \\times 2$的聚合块矩阵$M$，然后评估度量及其差异。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_disentanglement_metric(M):\n    \"\"\"\n    Calculates TC and I(zc; zf) for the aggregated posterior, which are identical \n    in this problem's specific setting due to its block structure.\n    The formula is log(m11*m22 / det(M)).\n\n    Args:\n        M (np.ndarray): The 2x2 covariance block of the aggregated posterior.\n\n    Returns:\n        float: The value of the disentanglement metric.\n    \"\"\"\n    m11 = M[0, 0]\n    m22 = M[1, 1]\n    det_M = np.linalg.det(M)\n    \n    # det_M must be positive for a valid covariance matrix and non-degenerate solution.\n    if det_M = 1e-12: # Use a small tolerance for numerical stability\n        # A very small positive determinant indicates extreme correlation.\n        # log(infinity) would result. Returning a large number is more stable than np.inf.\n        return 1e9\n    \n    # The argument to log should be >= 1.\n    val = (m11 * m22) / det_M\n    if val  1.0:\n        # This can happen due to numerical precision issues when correlation is near zero.\n        # TC and MI are non-negative.\n        return 0.0\n\n    return np.log(val)\n\ndef solve_for_case(beta, alpha, sigma_x, sigma_f):\n    \"\"\"\n    Computes delta_TC and delta_I for a given set of parameters. \n    In this problem, delta_TC = delta_I.\n\n    Args:\n        beta (float): The beta parameter for the VAE.\n        alpha (float): The coupling strength in the true prior.\n        sigma_x (float): The observation noise standard deviation.\n        sigma_f (float): The fine factor noise standard deviation.\n\n    Returns:\n        tuple[float, float]: A tuple containing (delta_TC, delta_I).\n    \"\"\"\n    sigma_x_sq = sigma_x**2\n    sigma_f_sq = sigma_f**2\n\n    # Define the 2x2 S block of the true prior covariance matrix\n    S = np.array([\n        [1.0, alpha],\n        [alpha, sigma_f_sq + alpha**2]\n    ])\n    \n    # This combination appears in the aggregated posterior calculation\n    S_plus_noise = S + sigma_x_sq * np.identity(2)\n    beta_sigma_x_sq = beta * sigma_x_sq\n\n    # ---- Standard Prior Case (Sigma_0 = I) ----\n    \n    # The standard prior Sigma_0 = I makes the q posterior diagonal.\n    # The 2x2 block matrix for Sigma_q is c_q_std * I_2.\n    c_q_std = beta_sigma_x_sq / (1.0 + beta_sigma_x_sq) # Corrected logic\n    \n    # The 2x2 block matrix for the aggregated posterior covariance.\n    # The formula is M_agg = M_q + (1/(beta*sigma_x^2)^2) * M_q * (S+sigma_x^2*I) * M_q\n    # Since M_q is diagonal, this simplifies.\n    coeff_std = (c_q_std**2) / (beta_sigma_x_sq**2)\n    M_agg_std = c_q_std * np.identity(2) + coeff_std * S_plus_noise\n\n    Q_std = calculate_disentanglement_metric(M_agg_std)\n\n    # ---- Structured Prior Case (Sigma_0 = Sigma_true) ----\n    \n    # Inverse of the S block. det(S) = sigma_f_sq.\n    S_inv = (1.0 / sigma_f_sq) * np.array([\n        [sigma_f_sq + alpha**2, -alpha],\n        [-alpha, 1.0]\n    ])\n    \n    # 2x2 block for the inverse of Sigma_q\n    M_q_struct_inv = (1.0 / beta_sigma_x_sq) * np.identity(2) + S_inv\n    \n    # 2x2 block for Sigma_q\n    M_q_struct = np.linalg.inv(M_q_struct_inv)\n    \n    # 2x2 block for the aggregated posterior covariance\n    M_agg_struct = M_q_struct + (1.0 / beta_sigma_x_sq**2) * M_q_struct @ S_plus_noise @ M_q_struct\n    \n    Q_struct = calculate_disentanglement_metric(M_agg_struct)\n    \n    # Calculate the difference in metrics.\n    delta_Q = Q_std - Q_struct\n    \n    # As derived in the solution, delta_TC and delta_I are identical.\n    return delta_Q, delta_Q\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (1.0, 0.8, 0.1, 0.5), # Case 1\n        (4.0, 0.8, 0.1, 0.5), # Case 2\n        (1.0, 0.0, 0.1, 0.5), # Case 3\n        (1.0, 1.5, 0.1, 0.5), # Case 4\n        (10.0, 0.8, 0.1, 0.5), # Case 5\n    ]\n\n    results = []\n    for case in test_cases:\n        beta, alpha, sigma_x, sigma_f = case\n        delta_tc, delta_i = solve_for_case(beta, alpha, sigma_x, sigma_f)\n        results.append(delta_tc)\n        results.append(delta_i)\n\n    # Format output as specified in the problem statement\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}