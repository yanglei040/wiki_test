{
    "hands_on_practices": [
        {
            "introduction": "To effectively develop models that learn disentangled representations, we must first be able to measure disentanglement quantitatively. This practice guides you through building a disentanglement metric from the ground up, based on the fundamental concept of mutual information. By implementing an empirical estimator and a permutation-invariant scoring function, you will gain a practical understanding of how researchers evaluate the quality of learned representations against ground-truth factors of variation .",
            "id": "3116952",
            "problem": "You are given paired samples of learned latent variables and ground-truth generative factors produced by a model such as a Beta-Variational Autoencoder (Beta-VAE). Let the latent code be denoted by a random vector $z = (z_1, \\dots, z_{d_z})$ and the ground-truth generative factors by $g = (g_1, \\dots, g_{d_g})$. Your task is to construct a diagnostic of disentanglement based on the mutual information between each scalar latent $z_i$ and each scalar factor $g_j$, to assemble the mutual information matrix, and to quantify diagonal dominance in a permutation-invariant way.\n\nStarting only from the foundational definitions of joint and marginal probability distributions, Shannon entropy, and mutual information (which is defined in terms of Kullback–Leibler Divergence), derive and implement an empirical estimator of the mutual information between scalar random variables using discretization of observed samples. Then, use this estimator to build a matrix of mutual informations between all pairs $(z_i, g_j)$ and compute a permutation-invariant diagonal dominance score that reflects how well each latent aligns to a unique factor.\n\nYou must follow these design requirements:\n\n- Data and discretization:\n  - For a given paired sample set $\\{(z^{(n)}, g^{(n)})\\}_{n=1}^N$ with $z^{(n)} \\in \\mathbb{R}^{d_z}$ and $g^{(n)} \\in \\mathbb{R}^{d_g}$, discretize each scalar variable into $b$ equal-width bins over its observed range (minimum to maximum in the sample). Use the same $b$ for all scalar variables. Treat the bin edges as closed on the right for the maximum so that the maximum observed value is included. Let $b \\in \\mathbb{N}$ with $b \\ge 2$.\n  - From these discretizations, form empirical joint frequency tables for each pair $(z_i, g_j)$ and normalize them to obtain empirical joint probability masses, along with their marginals.\n\n- Mutual information matrix:\n  - Using only the foundational definitions, compute the mutual information $I(z_i; g_j)$ for each pair $(i, j)$, producing a matrix $I \\in \\mathbb{R}_{\\ge 0}^{d_z \\times d_g}$. Use natural logarithms so that the units are in nats.\n\n- Permutation-invariant diagonal dominance score:\n  - Because the ordering of latent dimensions and factor dimensions is arbitrary, first compute the optimal one-to-one assignment between $\\{z_i\\}$ and $\\{g_j\\}$ that maximizes the sum of the selected entries of the matrix $I$. If $d_z \\ne d_g$, the assignment should match on $m = \\min(d_z, d_g)$ pairs without reuse of indices in either set.\n  - Let $S_{\\text{assign}}$ be the sum of the $m$ selected entries and let $S_{\\text{all}}$ be the sum of all entries of $I$. Define the diagonal dominance score as $D = S_{\\text{assign}} / S_{\\text{all}}$. If $S_{\\text{all}} = 0$ within numerical tolerance (i.e., $S_{\\text{all}} \\le 10^{-12}$), define $D = 0$.\n  - The score $D$ must satisfy $0 \\le D \\le 1$ and should be closer to $1$ when the mutual information mass concentrates on a permutation of the diagonal.\n\n- Output specification:\n  - For each test case below, your program must output a triple consisting of:\n    1) the score $D$ rounded to $4$ decimal places,\n    2) the selected assignment from latent indices to factor indices as a list of length $d_z$, where unassigned latent indices (if $d_z > d_g$) are marked with $-1$,\n    3) the mutual information matrix $I$ flattened in row-major order with each entry rounded to $4$ decimal places.\n  - Aggregate the results for all test cases into a single line containing a list of these triples. The final output must be exactly one line in the format:\n    $[ [D_1, [a_{1,1}, \\dots, a_{1,d_z}], [I_{1,1}, \\dots]], [D_2, [\\dots], [\\dots]], \\dots ]$.\n\n- Test suite and data generation:\n  - Use the following test suite. In all cases below, $N$ is the number of samples and $s$ is the pseudo-random seed for a reproducible generator. All logarithms must be natural logs, and all computations must be done in floating-point arithmetic. In all cases, set the number of bins to $b = 12$.\n\n  - Test case $1$ (near-ideal disentanglement):\n    - Dimensions: $d_z = 3$, $d_g = 3$.\n    - Parameters: $N = 5000$, $s = 13$.\n    - Sampling: For each $n \\in \\{1, \\dots, N\\}$, sample $g^{(n)} \\sim \\text{Uniform}([-1, 1]^{3})$, and noise $\\epsilon^{(n)} \\sim \\mathcal{N}(0, I_3)$.\n    - Construct $z^{(n)} = g^{(n)} + 0.05 \\cdot \\epsilon^{(n)}$.\n\n  - Test case $2$ (independent latents and factors):\n    - Dimensions: $d_z = 3$, $d_g = 3$.\n    - Parameters: $N = 5000$, $s = 7$.\n    - Sampling: For each $n$, sample $g^{(n)} \\sim \\text{Uniform}([-1, 1]^{3})$, and independently sample $z^{(n)} \\sim \\mathcal{N}(0, I_3)$.\n\n  - Test case $3$ (entangled linear mixing):\n    - Dimensions: $d_z = 3$, $d_g = 3$.\n    - Parameters: $N = 5000$, $s = 11$.\n    - Sampling: For each $n$, sample $g^{(n)} \\sim \\text{Uniform}([-1, 1]^{3})$, noise $\\epsilon^{(n)} \\sim \\mathcal{N}(0, I_3)$, and set $A \\in \\mathbb{R}^{3 \\times 3}$ to\n      $$\n      A = \\begin{bmatrix}\n      1.0 & 0.8 & 0.5 \\\\\n      0.6 & 1.0 & 0.4 \\\\\n      0.5 & 0.4 & 1.2\n      \\end{bmatrix}.\n      $$\n      Construct $z^{(n)} = A \\, g^{(n)} + 0.2 \\cdot \\epsilon^{(n)}$.\n\n  - Test case $4$ (dimension mismatch, sparse alignment):\n    - Dimensions: $d_z = 2$, $d_g = 3$.\n    - Parameters: $N = 5000$, $s = 17$.\n    - Sampling: For each $n$, sample $g^{(n)} \\sim \\text{Uniform}([-1, 1]^{3})$, noise $\\epsilon^{(n)} \\sim \\mathcal{N}(0, I_2)$, and set $B \\in \\mathbb{R}^{2 \\times 3}$ to\n      $$\n      B = \\begin{bmatrix}\n      1.0 & 0.0 & 0.0 \\\\\n      0.0 & 0.0 & 1.0\n      \\end{bmatrix}.\n      $$\n      Construct $z^{(n)} = B \\, g^{(n)} + 0.05 \\cdot \\epsilon^{(n)}$.\n\n- Final output format requirement:\n  - Your program should produce a single line of output containing the list of results for the $4$ test cases as a comma-separated list enclosed in square brackets, where each result is a list of the form $[D, \\text{assignment}, \\text{flattened\\_I}]$ as described above. All floats in the output must be rounded to $4$ decimal places, and integers must be exact.\n\nYour solution must be a complete, runnable program that performs the sampling, discretization, mutual information estimation, optimal assignment, score computation, and final formatting exactly as specified above, without requiring any external input.",
            "solution": "The problem requires the development of a disentanglement metric based on mutual information. We will proceed by first establishing the theoretical foundations of mutual information from first principles, then detailing the method for its empirical estimation, and finally specifying the construction of the permutation-invariant score.\n\n**1. Foundational Derivation of Mutual Information**\n\nThe cornerstone of this analysis is mutual information, which quantifies the statistical dependence between two random variables. We derive it from the more fundamental concepts of Shannon entropy and Kullback-Leibler (KL) divergence.\n\nLet $X$ and $Y$ be two discrete random variables with alphabets $\\mathcal{X}$ and $\\mathcal{Y}$, joint probability mass function (PMF) $P_{XY}(x, y)$, and marginal PMFs $P_X(x)$ and $P_Y(y)$.\n\nThe **Shannon entropy** of $X$ measures its uncertainty and is defined as:\n$$ H(X) = - \\sum_{x \\in \\mathcal{X}} P_X(x) \\log P_X(x) $$\nwhere the logarithm is the natural logarithm (base $e$), and the units are nats. By convention, $0 \\log 0 = 0$. The joint entropy of the pair $(X, Y)$ is similarly defined:\n$$ H(X, Y) = - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} P_{XY}(x, y) \\log P_{XY}(x, y) $$\n\nThe **Kullback-Leibler (KL) divergence** measures how one probability distribution $P$ diverges from a second, expected probability distribution $Q$. For discrete distributions, it is defined as:\n$$ D_{KL}(P || Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(x)} $$\n\nThe problem statement requires framing **mutual information** $I(X; Y)$ in terms of KL divergence. Mutual information measures the reduction in uncertainty about one variable from knowing the other. It is formally the KL divergence between the joint distribution $P_{XY}(x, y)$ and the product of the marginals $P_X(x) P_Y(y)$, which represents the joint distribution under the assumption of independence.\n$$ I(X; Y) = D_{KL}(P_{XY} || P_X P_Y) = \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} P_{XY}(x, y) \\log \\frac{P_{XY}(x, y)}{P_X(x) P_Y(y)} $$\n\nThis definition can be expanded to relate mutual information to entropies:\n$$\n\\begin{align*}\nI(X; Y) &= \\sum_{x, y} P_{XY}(x, y) \\left[ \\log P_{XY}(x, y) - \\log P_X(x) - \\log P_Y(y) \\right] \\\\\n&= -H(X, Y) - \\sum_{x, y} P_{XY}(x, y) \\log P_X(x) - \\sum_{x, y} P_{XY}(x, y) \\log P_Y(y) \\\\\n&= -H(X, Y) - \\sum_x \\left( \\sum_y P_{XY}(x, y) \\right) \\log P_X(x) - \\sum_y \\left( \\sum_x P_{XY}(x, y) \\right) \\log P_Y(y) \\\\\n&= -H(X, Y) - \\sum_x P_X(x) \\log P_X(x) - \\sum_y P_Y(y) \\log P_Y(y) \\\\\n&= H(X) + H(Y) - H(X, Y)\n\\end{align*}\n$$\nThis final form, $I(X; Y) = H(X) + H(Y) - H(X, Y)$, provides a convenient and numerically stable way to compute mutual information from estimated entropies.\n\n**2. Empirical Estimation from Continuous Samples**\n\nThe latent variables $z_i$ and generative factors $g_j$ are continuous. To apply the discrete formulas above, we must first discretize the data.\n\nGiven $N$ samples of a scalar variable $x$, $\\{x^{(n)}\\}_{n=1}^N$, we find its minimum $x_{\\min}$ and maximum $x_{\\max}$. We divide the range $[x_{\\min}, x_{\\max}]$ into $b$ equal-width bins. The width of each bin is $\\Delta x = (x_{\\max} - x_{\\min}) / b$. The bin edges are $x_{\\min}, x_{\\min} + \\Delta x, \\dots, x_{\\max}$. The problem specifies that the last bin includes the maximum value, which is standard for histogram implementations. Each sample $x^{(n)}$ is then assigned to one of $b$ bins.\n\nFor a pair of variables $(z_i, g_j)$, we apply this process to both $\\{z_i^{(n)}\\}_{n=1}^N$ and $\\{g_j^{(n)}\\}_{n=1}^N$, using $b$ bins for each. We can then construct a $b \\times b$ joint frequency matrix, $\\text{Count}(k, l)$, where $\\text{Count}(k, l)$ is the number of samples $(z_i^{(n)}, g_j^{(n)})$ for which $z_i^{(n)}$ falls into bin $k$ and $g_j^{(n)}$ falls into bin $l$.\n\nThe empirical joint PMF, $\\hat{P}(z_i, g_j)$, is obtained by normalizing the frequency matrix by the total number of samples $N$:\n$$ \\hat{P}_{Z_i, G_j}(k, l) = \\frac{\\text{Count}(k, l)}{N} $$\nThe empirical marginal PMFs are computed by summing over the rows and columns of the joint PMF:\n$$ \\hat{P}_{Z_i}(k) = \\sum_{l=1}^b \\hat{P}_{Z_i, G_j}(k, l) \\qquad \\text{and} \\qquad \\hat{P}_{G_j}(l) = \\sum_{k=1}^b \\hat{P}_{Z_i, G_j}(k, l) $$\nWith these empirical PMFs, we can compute the empirical entropies $\\hat{H}(Z_i)$, $\\hat{H}(G_j)$, and $\\hat{H}(Z_i, G_j)$, and subsequently the empirical mutual information $\\hat{I}(z_i; g_j)$.\n\n**3. Mutual Information Matrix**\n\nBy repeating the estimation procedure for every pair of latent variable $z_i$ (for $i \\in \\{1, \\dots, d_z\\}$) and generative factor $g_j$ (for $j \\in \\{1, \\dots, d_g\\}$), we construct the mutual information matrix $I \\in \\mathbb{R}_{\\ge 0}^{d_z \\times d_g}$, where the entry $(i, j)$ is $I_{ij} = \\hat{I}(z_i; g_j)$. A well-disentangled representation would ideally have high mutual information between one specific latent $z_i$ and one specific factor $g_j$, and low mutual information otherwise. This corresponds to the matrix $I$ being a sparse matrix that is a permutation of a diagonal matrix.\n\n**4. Permutation-Invariant Diagonal Dominance Score**\n\nTo quantify the degree of disentanglement in a way that is invariant to the arbitrary ordering of latent dimensions and factors, we define a score based on optimal assignment. The task is to find a one-to-one matching between a subset of latent variables and a subset of factors that maximizes the sum of their mutual information.\n\nThis is a classic **assignment problem** (also known as maximum weight bipartite matching). Given the cost matrix $I \\in \\mathbb{R}^{d_z \\times d_g}$, we want to find an assignment $\\mathcal{A}$ of $m = \\min(d_z, d_g)$ row indices to $m$ column indices, with no index repeated, such that the sum $\\sum_{(i,j) \\in \\mathcal{A}} I_{ij}$ is maximized. Standard algorithms, such as the Hungarian algorithm, solve the minimum cost assignment problem. To maximize the sum, we can solve the minimum cost assignment on the negative of the MI matrix, $-I$.\n\nLet $(r_1, c_1), \\dots, (r_m, c_m)$ be the index pairs of the optimal assignment, where $m = \\min(d_z, d_g)$.\nThe sum of entries for this optimal assignment is:\n$$ S_{\\text{assign}} = \\sum_{k=1}^m I_{r_k, c_k} $$\nThe total sum of all entries in the matrix is:\n$$ S_{\\text{all}} = \\sum_{i=1}^{d_z} \\sum_{j=1}^{d_g} I_{ij} $$\nThe diagonal dominance score $D$ is defined as the ratio of the optimally assigned MI sum to the total MI sum:\n$$ D = \\frac{S_{\\text{assign}}}{S_{\\text{all}}} $$\nThis score is normalized, $0 \\le D \\le 1$. A value of $D$ close to $1$ indicates that most of the mutual information is concentrated along a one-to-one mapping between latents and factors, signaling good disentanglement. A value of $D$ close to $1/m$ (for square matrices) or lower might indicate entanglement or poor information capture. As per the problem, if $S_{\\text{all}} \\le 10^{-12}$, we define $D=0$. This case arises if all variables are found to be perfectly independent, resulting in an MI matrix of all zeros.\nThe final assignment list of length $d_z$ is constructed by mapping each latent index $i$ to its assigned factor index $j$ if it is part of the optimal assignment, and to $-1$ otherwise.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nfrom scipy.stats import entropy\n\ndef solve():\n    \"\"\"\n    Solves the disentanglement-quantification problem for all test cases.\n    \"\"\"\n    \n    # Define test cases as per the problem statement.\n    test_cases = [\n        {\n            \"d_z\": 3, \"d_g\": 3, \"N\": 5000, \"s\": 13, \"b\": 12,\n            \"type\": \"ideal\",\n        },\n        {\n            \"d_z\": 3, \"d_g\": 3, \"N\": 5000, \"s\": 7, \"b\": 12,\n            \"type\": \"independent\",\n        },\n        {\n            \"d_z\": 3, \"d_g\": 3, \"N\": 5000, \"s\": 11, \"b\": 12,\n            \"type\": \"entangled\",\n            \"A\": np.array([[1.0, 0.8, 0.5], [0.6, 1.0, 0.4], [0.5, 0.4, 1.2]]),\n        },\n        {\n            \"d_z\": 2, \"d_g\": 3, \"N\": 5000, \"s\": 17, \"b\": 12,\n            \"type\": \"dim_mismatch\",\n            \"B\": np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        d_z, d_g, N, s, b = case[\"d_z\"], case[\"d_g\"], case[\"N\"], case[\"s\"], case[\"b\"]\n        rng = np.random.default_rng(seed=s)\n\n        # 1. Generate data\n        g_samples = rng.uniform(low=-1.0, high=1.0, size=(N, d_g))\n\n        if case[\"type\"] == \"ideal\":\n            epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n            z_samples = g_samples + 0.05 * epsilon\n        elif case[\"type\"] == \"independent\":\n            z_samples = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n        elif case[\"type\"] == \"entangled\":\n            epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n            A = case[\"A\"]\n            z_samples = (A @ g_samples.T).T + 0.2 * epsilon\n        elif case[\"type\"] == \"dim_mismatch\":\n            epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n            B = case[\"B\"]\n            z_samples = (B @ g_samples.T).T + 0.05 * epsilon\n\n        # 2. Compute Mutual Information Matrix\n        mi_matrix = np.zeros((d_z, d_g))\n\n        for i in range(d_z):\n            for j in range(d_g):\n                z_i = z_samples[:, i]\n                g_j = g_samples[:, j]\n                \n                # Discretize and get joint PMF\n                # np.histogram2d binning is [left, right) except for the last bin which is [left, right], as required.\n                hist_2d, _, _ = np.histogram2d(\n                    z_i, g_j, bins=b, \n                    range=[[z_i.min(), z_i.max()], [g_j.min(), g_j.max()]]\n                )\n                \n                p_xy = hist_2d / N\n                \n                # Compute marginal PMFs\n                p_x = np.sum(p_xy, axis=1)\n                p_y = np.sum(p_xy, axis=0)\n\n                # Compute entropies using scipy.stats.entropy which handles p*log(p) = 0 for p=0\n                # Using natural log (base=e) as required.\n                h_x = entropy(p_x, base=np.e)\n                h_y = entropy(p_y, base=np.e)\n                h_xy = entropy(p_xy.flatten(), base=np.e)\n                \n                # Mutual Information I(X;Y) = H(X) + H(Y) - H(X,Y)\n                mi = h_x + h_y - h_xy\n                mi_matrix[i, j] = mi\n\n        # 3. Compute Permutation-Invariant Score\n        s_all = np.sum(mi_matrix)\n        \n        if s_all <= 1e-12:\n            d_score = 0.0\n            assignment = [-1] * d_z\n        else:\n            # We want to maximize the sum, so we find the minimum cost assignment on the negative MI matrix.\n            cost_matrix = -mi_matrix\n            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n            \n            # The sum of MI values for the optimal assignment\n            s_assign = mi_matrix[row_ind, col_ind].sum()\n            \n            d_score = s_assign / s_all\n            \n            # Create the assignment list (latent index -> factor index)\n            assignment = [-1] * d_z\n            for r, c in zip(row_ind, col_ind):\n                assignment[r] = c\n\n        # 4. Format results\n        D_rounded = round(d_score, 4)\n        I_flat_rounded = [round(val, 4) for val in mi_matrix.flatten(order='C')]\n        \n        all_results.append([D_rounded, assignment, I_flat_rounded])\n\n    # Final print statement must match the required format exactly.\n    # Custom formatting to avoid spaces after commas in lists.\n    result_str = \",\".join([\n        f\"[{res[0]},{str(res[1]).replace(' ', '')},{str(res[2]).replace(' ', '')}]\" for res in all_results\n    ])\n    print(f\"[{result_str}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "One of the most common and subtle failure modes in training Variational Autoencoders is \"posterior collapse,\" where the latent variables are ignored by the decoder, leading to a useless representation. This exercise provides a sharp, analytical demonstration of this phenomenon, showing how a highly expressive decoder can undermine the learning of disentangled factors, even within a $\\beta$-VAE framework . Understanding this trade-off is crucial for diagnosing and troubleshooting your own VAE models.",
            "id": "3116830",
            "problem": "Consider a Variational Autoencoder (VAE) with the $\\beta$-VAE objective, where the goal is to learn disentangled representations of independent ground-truth factors. The $\\beta$-VAE objective maximizes, over encoder and decoder distributions, the expected evidence lower bound (ELBO) modified by a scaling factor $\\beta$ on the Kullback-Leibler (KL) divergence. The ground-truth data generation process is as follows: there are $d$ independent latent factors $f_i \\sim \\mathcal{N}(0,1)$, aggregated as a vector $f \\in \\mathbb{R}^d$, and observable data $x \\in \\mathbb{R}^d$ generated by a diagonal linear mapping with additive Gaussian noise, namely $x = D f + \\epsilon$, where $D = \\operatorname{diag}(\\alpha_1, \\dots, \\alpha_d)$ with $\\alpha_i \\in \\mathbb{R}_{\\ge 0}$ and $\\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2 I_d)$. Consider two decoders for the conditional likelihood $p_\\theta(x \\mid z)$ at the same $\\beta$: a simple decoder with limited capacity, modeled as a factorized Gaussian with mean $W z$ and fixed isotropic variance (i.e., $p_\\theta(x \\mid z) = \\mathcal{N}(W z, \\sigma_x^2 I_d)$ with $W = D$), and a powerful decoder with sufficient expressivity to model the exact marginal data distribution $p(x)$ independently of $z$ (e.g., an autoregressive model), meaning it can effectively make $p_\\theta(x \\mid z) = p(x)$ for all $z$. The encoder family is restricted to diagonal Gaussian $q_\\phi(z \\mid x) = \\mathcal{N}(B x, \\sigma_e^2 I_d)$, where $B = \\operatorname{diag}(b_1,\\dots,b_d)$ and $\\sigma_e^2$ may be optimized per dimension subject to the objective.\n\nYour tasks:\n\n- Starting from the definitions of the $\\beta$-VAE objective, the properties of Gaussian distributions, and standard linear-Gaussian modeling assumptions, derive the optimal encoder parameters $b_i$ and $\\sigma_{e,i}^2$ that maximize the expected $\\beta$-VAE objective for the simple decoder $p_\\theta(x \\mid z) = \\mathcal{N}(D z, \\sigma_x^2 I_d)$, under the given ground-truth data model. Use the fact that the expectation is over the true data distribution $p_{\\text{data}}(x)$ implied by $f \\sim \\mathcal{N}(0,I_d)$ and $x = D f + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2 I_d)$, and restrict attention to solutions in the given encoder family $q_\\phi(z \\mid x) = \\mathcal{N}(B x, \\sigma_e^2 I_d)$.\n\n- Using the derived optimal encoder parameters for the simple decoder, compute for each dimension $i$ the mutual information (in natural logarithm units, i.e., nats) between the learned latent variable $z_i$ and its corresponding ground-truth factor $f_i$. Define the Mutual Information Gap (MIG) per dimension as the difference between the largest mutual information and the second largest mutual information across factors for the given latent; in the specified diagonal setting this simplifies appropriately. Report the average MIG across dimensions.\n\n- For the powerful decoder that can match the data marginal $p(x)$ independently of $z$ at the same $\\beta$, justify the optimal encoder behavior and compute the average MIG across dimensions.\n\n- Implement a program that, for each test case in the test suite below, computes the average MIG for the simple decoder and the powerful decoder at the same $\\beta$, and then outputs a single list containing, for each test case, the difference between the average MIG of the simple decoder and the average MIG of the powerful decoder. Express all mutual information quantities in nats.\n\nUse the following test suite, which varies the number of dimensions, the diagonal map $D$ entries, the encoder-decoder noise levels, and the $\\beta$ value to test different behaviors:\n\n- Test case $1$: $d = 3$, $\\alpha = [1.0, 0.8, 1.2]$, $\\sigma_\\epsilon^2 = 0.1$, $\\sigma_x^2 = 0.2$, $\\beta = 4.5$.\n\n- Test case $2$: $d = 2$, $\\alpha = [0.0, 0.5]$, $\\sigma_\\epsilon^2 = 0.2$, $\\sigma_x^2 = 0.2$, $\\beta = 4.5$.\n\n- Test case $3$: $d = 4$, $\\alpha = [1.0, 1.0, 1.0, 1.0]$, $\\sigma_\\epsilon^2 = 0.1$, $\\sigma_x^2 = 0.2$, $\\beta = 50.0$.\n\nFinal output format requirement: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3]$), where each $r_k$ is the difference between the average MIG of the simple decoder and the average MIG of the powerful decoder, in nats, for test case $k$.",
            "solution": "The supplied problem is scientifically grounded, well-posed, and contains all necessary information for a rigorous solution. It poses a theoretical question within the standard framework of variational autoencoders and information theory. The problem is therefore deemed valid.\n\nThe core task is to analyze the disentangling capabilities of a $\\beta$-VAE under two different decoder assumptions (simple and powerful) by computing the Mutual Information Gap (MIG). The problem structure, with its diagonal-matrix-based linear-Gaussian models, allows for a full analytical derivation. The solution proceeds by first deriving the optimal encoder parameters for each case and then using these parameters to compute the required mutual information quantities.\n\nThe $\\beta$-VAE objective to be maximized is given by:\n$$\n\\mathcal{L}(\\phi, \\theta) = \\mathbb{E}_{p_{\\text{data}}(x)} \\left[ \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\beta \\text{KL}(q_\\phi(z|x) \\| p(z)) \\right]\n$$\nwhere $p(z) = \\mathcal{N}(0, I_d)$ is the latent prior. The models are defined such that all matrices are diagonal, which means the objective function decomposes into a sum of per-dimension terms, $\\mathcal{L} = \\sum_{i=1}^d \\mathcal{L}_i$. We can therefore analyze each dimension $i$ independently.\n\nThe models for dimension $i$ are:\n- Ground-truth factor: $f_i \\sim \\mathcal{N}(0, 1)$.\n- Data generation: $x_i = \\alpha_i f_i + \\epsilon_i$, where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$. This implies that $x_i$ is a Gaussian random variable, $x_i \\sim \\mathcal{N}(0, \\alpha_i^2 + \\sigma_\\epsilon^2)$. We denote the variance of the data generating process for $x_i$ as $V_{x_i} = \\alpha_i^2 + \\sigma_\\epsilon^2$.\n- Encoder: $q_\\phi(z_i|x_i) = \\mathcal{N}(b_i x_i, \\sigma_{e,i}^2)$.\n- Simple Decoder: $p_\\theta(x_i|z_i) = \\mathcal{N}(\\alpha_i z_i, \\sigma_x^2)$.\n- Prior: $p(z_i) = \\mathcal{N}(0, 1)$.\n\nThe per-dimension objective is:\n$$\n\\mathcal{L}_i = \\mathbb{E}_{p(x_i)} \\left[ \\mathbb{E}_{q(z_i|x_i)}[\\log p(x_i|z_i)] - \\beta \\text{KL}(q(z_i|x_i) \\| p(z_i)) \\right]\n$$\n\n### Part 1: Simple Decoder Analysis\n\nFirst, we derive the optimal encoder parameters $b_i$ and $\\sigma_{e,i}^2$ that maximize $\\mathcal{L}_i$ for the simple decoder.\n\nThe reconstruction term's expectation is:\n$$\n\\mathbb{E}_{p(x_i)} \\mathbb{E}_{q(z_i|x_i)}[\\log p(x_i|z_i)] = \\mathbb{E}_{p(x_i), q(z_i|x_i)}\\left[-\\frac{(x_i - \\alpha_i z_i)^2}{2\\sigma_x^2} - \\frac{1}{2}\\log(2\\pi\\sigma_x^2)\\right]\n$$\nThe quadratic term expands to $\\mathbb{E}[x_i^2 - 2\\alpha_i x_i z_i + \\alpha_i^2 z_i^2]$. Taking the expectation over the joint distribution induced by $p(x_i)$ and $q(z_i|x_i)$:\n$\\mathbb{E}[(x_i - \\alpha_i z_i)^2] = \\text{Var}(x_i - \\alpha_i z_i) = \\text{Var}(x_i) - 2\\alpha_i\\text{Cov}(x_i,z_i) + \\alpha_i^2\\text{Var}(z_i)$.\nWe have $\\text{Var}(x_i) = V_{x_i}$, $\\text{Cov}(x_i, z_i) = b_i V_{x_i}$, and $\\text{Var}(z_i) = \\sigma_{e,i}^2 + b_i^2 V_{x_i}$.\nThis gives $\\mathbb{E}[(x_i - \\alpha_i z_i)^2] = V_{x_i} - 2\\alpha_i b_i V_{x_i} + \\alpha_i^2(\\sigma_{e,i}^2 + b_i^2 V_{x_i}) = V_{x_i}(1 - \\alpha_i b_i)^2 + \\alpha_i^2 \\sigma_{e,i}^2$.\nSo, the reconstruction term is $-\\frac{1}{2\\sigma_x^2}[V_{x_i}(1 - \\alpha_i b_i)^2 + \\alpha_i^2 \\sigma_{e,i}^2] + C_1$.\n\nThe KL-divergence term is:\n$$\n\\mathbb{E}_{p(x_i)}[\\text{KL}(\\mathcal{N}(b_i x_i, \\sigma_{e,i}^2) \\| \\mathcal{N}(0, 1))] = \\mathbb{E}_{p(x_i)}\\left[\\frac{1}{2}\\left(\\sigma_{e,i}^2 + (b_i x_i)^2 - 1 - \\log(\\sigma_{e,i}^2)\\right)\\right]\n$$\nThis results in $\\frac{1}{2}(\\sigma_{e,i}^2 + b_i^2 V_{x_i} - 1 - \\log(\\sigma_{e,i}^2))$.\n\nCombining these, the part of $\\mathcal{L}_i$ dependent on $b_i$ and $\\sigma_{e,i}^2$ is:\n$$\n\\mathcal{L}_i(b_i, \\sigma_{e,i}^2) = -\\frac{V_{x_i}(1 - \\alpha_i b_i)^2}{2\\sigma_x^2} - \\frac{\\alpha_i^2 \\sigma_{e,i}^2}{2\\sigma_x^2} - \\frac{\\beta}{2}(\\sigma_{e,i}^2 + b_i^2 V_{x_i} - \\log(\\sigma_{e,i}^2)) + C_2\n$$\n\nTo find the optimal parameters, we take partial derivatives and set them to zero.\n$\\frac{\\partial \\mathcal{L}_i}{\\partial b_i} = -\\frac{V_{x_i}}{2\\sigma_x^2} \\cdot 2(1-\\alpha_i b_i)(-\\alpha_i) - \\frac{\\beta}{2}(2b_i V_{x_i}) = 0$.\nAssuming $V_{x_i} = \\alpha_i^2 + \\sigma_\\epsilon^2 > 0$, we can divide by $V_{x_i}$:\n$$\n\\frac{\\alpha_i(1-\\alpha_i b_i)}{\\sigma_x^2} - \\beta b_i = 0 \\implies \\frac{\\alpha_i}{\\sigma_x^2} = b_i \\left(\\frac{\\alpha_i^2}{\\sigma_x^2} + \\beta\\right) \\implies b_i^* = \\frac{\\alpha_i}{\\alpha_i^2 + \\beta \\sigma_x^2}\n$$\n$\\frac{\\partial \\mathcal{L}_i}{\\partial \\sigma_{e,i}^2} = -\\frac{\\alpha_i^2}{2\\sigma_x^2} - \\frac{\\beta}{2} + \\frac{\\beta}{2\\sigma_{e,i}^2} = 0$.\n$$\n\\frac{\\beta}{\\sigma_{e,i}^2} = \\frac{\\alpha_i^2}{\\sigma_x^2} + \\beta = \\frac{\\alpha_i^2 + \\beta\\sigma_x^2}{\\sigma_x^2} \\implies (\\sigma_{e,i}^2)^* = \\frac{\\beta \\sigma_x^2}{\\alpha_i^2 + \\beta \\sigma_x^2}\n$$\n\nNext, we compute the mutual information $I(z_i; f_i)$. The system is a linear-Gaussian cascade: $f_i \\to x_i \\to z_i$. The mutual information for jointly Gaussian variables is $I(z_i; f_i) = \\frac{1}{2} \\log \\frac{\\text{Var}(z_i)}{\\text{Var}(z_i|f_i)}$.\nUsing the law of total variance: $\\text{Var}(z_i) = \\text{Var}(\\mathbb{E}[z_i|f_i]) + \\mathbb{E}[\\text{Var}(z_i|f_i)]$.\nThe conditional moments are $\\mathbb{E}[z_i|f_i] = b_i \\alpha_i f_i$ and $\\text{Var}(z_i|f_i) = \\sigma_{e,i}^2 + b_i^2 \\sigma_\\epsilon^2$, which is constant w.r.t $f_i$.\nSo, $\\text{Var}(z_i) = \\text{Var}(b_i \\alpha_i f_i) + (\\sigma_{e,i}^2 + b_i^2 \\sigma_\\epsilon^2) = b_i^2 \\alpha_i^2 \\text{Var}(f_i) + \\text{Var}(z_i|f_i) = b_i^2 \\alpha_i^2 + \\text{Var}(z_i|f_i)$, since $\\text{Var}(f_i)=1$.\nThe mutual information is:\n$$\nI(z_i; f_i) = \\frac{1}{2} \\log \\left(\\frac{\\text{Var}(z_i|f_i) + b_i^2 \\alpha_i^2}{\\text{Var}(z_i|f_i)}\\right) = \\frac{1}{2} \\log \\left(1 + \\frac{b_i^2 \\alpha_i^2}{\\sigma_{e,i}^2 + b_i^2 \\sigma_\\epsilon^2}\\right)\n$$\nSubstituting the optimal parameters $b_i^*$ and $(\\sigma_{e,i}^2)^*$:\nLet $A = \\alpha_i^2 + \\beta \\sigma_x^2$. Then $b_i^* = \\alpha_i/A$ and $(\\sigma_{e,i}^2)^* = \\beta\\sigma_x^2/A$.\nThe argument of the logarithm becomes:\n$1 + \\frac{(\\alpha_i/A)^2 \\alpha_i^2}{(\\beta\\sigma_x^2/A) + (\\alpha_i/A)^2 \\sigma_\\epsilon^2} = 1 + \\frac{\\alpha_i^4/A^2}{(\\beta\\sigma_x^2 A + \\alpha_i^2 \\sigma_\\epsilon^2)/A^2} = 1 + \\frac{\\alpha_i^4}{\\beta\\sigma_x^2(\\alpha_i^2 + \\beta\\sigma_x^2) + \\alpha_i^2 \\sigma_\\epsilon^2}$.\n$$\nI(z_i; f_i) = \\frac{1}{2} \\log \\left( 1 + \\frac{\\alpha_i^4}{\\alpha_i^2(\\beta\\sigma_x^2 + \\sigma_\\epsilon^2) + \\beta^2(\\sigma_x^2)^2} \\right)\n$$\nAs established in the validation phase, due to the diagonal structure of the problem, the MIG for latent variable $z_i$ simplifies to $I(z_i; f_i)$. The average MIG for the simple decoder is thus:\n$$\n\\text{MIG}_{\\text{simple}} = \\frac{1}{d} \\sum_{i=1}^d I(z_i; f_i)\n$$\n\n### Part 2: Powerful Decoder Analysis\n\nFor the powerful decoder, $p_\\theta(x|z) = p(x)$. The objective function becomes:\n$$\n\\mathcal{L}_i = \\mathbb{E}_{p(x_i)}[\\log p(x_i)] - \\beta \\mathbb{E}_{p(x_i)}[\\text{KL}(q(z_i|x_i) \\| p(z_i))]\n$$\nThe first term is the negative entropy of the data, which is a constant with respect to the encoder parameters $(b_i, \\sigma_{e,i}^2)$. Maximizing $\\mathcal{L}_i$ is equivalent to minimizing $\\mathbb{E}_{p(x_i)}[\\text{KL}(q(z_i|x_i) \\| p(z_i))]$.\nSince KL-divergence is non-negative, the minimum is $0$, achieved when $q(z_i|x_i) = p(z_i)$ for all $x_i$.\nComparing $q(z_i|x_i) = \\mathcal{N}(b_i x_i, \\sigma_{e,i}^2)$ to $p(z_i) = \\mathcal{N}(0, 1)$, we must have $b_i x_i = 0$ for all $x_i$, which implies $b_i^*=0$, and $\\sigma_{e,i}^2 = 1$.\nThis phenomenon, where the latent variables become independent of the data to minimize the KL penalty, is known as \"posterior collapse.\"\nWith $b_i^*=0$, the mutual information formula immediately gives $I(z_i; f_i) = \\frac{1}{2}\\log(1+0) = 0$.\nTherefore, the average MIG for the powerful decoder is:\n$$\n\\text{MIG}_{\\text{powerful}} = \\frac{1}{d} \\sum_{i=1}^d 0 = 0\n$$\n\n### Part 3: Final Calculation\nThe problem asks for the difference between the average MIG of the simple decoder and that of the powerful decoder for each test case.\n$$\n\\Delta\\text{MIG} = \\text{MIG}_{\\text{simple}} - \\text{MIG}_{\\text{powerful}} = \\text{MIG}_{\\text{simple}} - 0 = \\text{MIG}_{\\text{simple}}\n$$\nThis is the quantity to be computed by the program for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n    Derives optimal encoder parameters for a beta-VAE with a simple linear decoder,\n    computes the resulting average Mutual Information Gap (MIG), and finds the\n    difference with the MIG from a powerful decoder, which is always zero.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1: d=3, alpha=[1.0, 0.8, 1.2], sigma_eps^2=0.1, sigma_x^2=0.2, beta=4.5\n        {\n            \"d\": 3,\n            \"alpha\": [1.0, 0.8, 1.2],\n            \"sigma_eps_sq\": 0.1,\n            \"sigma_x_sq\": 0.2,\n            \"beta\": 4.5,\n        },\n        # Test case 2: d=2, alpha=[0.0, 0.5], sigma_eps^2=0.2, sigma_x^2=0.2, beta=4.5\n        {\n            \"d\": 2,\n            \"alpha\": [0.0, 0.5],\n            \"sigma_eps_sq\": 0.2,\n            \"sigma_x_sq\": 0.2,\n            \"beta\": 4.5,\n        },\n        # Test case 3: d=4, alpha=[1.0, 1.0, 1.0, 1.0], sigma_eps^2=0.1, sigma_x^2=0.2, beta=50.0\n        {\n            \"d\": 4,\n            \"alpha\": [1.0, 1.0, 1.0, 1.0],\n            \"sigma_eps_sq\": 0.1,\n            \"sigma_x_sq\": 0.2,\n            \"beta\": 50.0,\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        d = case[\"d\"]\n        alpha_vec = case[\"alpha\"]\n        sigma_eps_sq = case[\"sigma_eps_sq\"]\n        sigma_x_sq = case[\"sigma_x_sq\"]\n        beta = case[\"beta\"]\n\n        # For the simple decoder, the average MIG is the average of I(z_i; f_i)\n        # across dimensions. For the powerful decoder, the average MIG is 0.\n        # The required difference is therefore just the average MIG of the simple decoder.\n        \n        mutual_informations = []\n        for i in range(d):\n            alpha_i = alpha_vec[i]\n\n            # If alpha_i is 0, then x_i contains no information about f_i,\n            # so I(z_i; f_i) is 0.\n            if alpha_i == 0.0:\n                mi_i = 0.0\n            else:\n                alpha_i_sq = alpha_i**2\n                alpha_i_4 = alpha_i**4\n                beta_sq = beta**2\n                sigma_x_sq_sq = sigma_x_sq**2\n\n                # This is the derived formula for I(z_i; f_i) in nats.\n                # I(z_i; f_i) = 0.5 * log(1 + (alpha_i^4) /\n                # (alpha_i^2 * (beta*sigma_x^2 + sigma_eps^2) + beta^2 * (sigma_x^2)^2))\n                numerator = alpha_i_4\n                denominator = alpha_i_sq * (beta * sigma_x_sq + sigma_eps_sq) + beta_sq * sigma_x_sq_sq\n                \n                mi_i = 0.5 * np.log(1.0 + numerator / denominator)\n            \n            mutual_informations.append(mi_i)\n        \n        avg_mig_simple = np.mean(mutual_informations)\n        \n        # The average MIG for the powerful decoder is 0 due to posterior collapse.\n        avg_mig_powerful = 0.0\n        \n        difference = avg_mig_simple - avg_mig_powerful\n        results.append(difference)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.16f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Standard VAEs often assume a simple, factorized prior (e.g., $\\mathcal{N}(0, I)$), which implies that the latent generative factors are independent. However, many real-world datasets possess a hierarchical structure. This practice demonstrates how incorporating this knowledge into a structured latent prior can serve as a powerful inductive bias, guiding the model to learn representations that better reflect the true hierarchical nature of the data and improve disentanglement .",
            "id": "3116938",
            "problem": "You are asked to formalize and test, in a linear-Gaussian setting, whether a structured latent prior that explicitly encodes hierarchical dependencies between coarse and fine latent factors improves disentanglement compared to a standard Beta-Variational Autoencoder (Beta-VAE) with an independent standard normal prior. Work entirely in the linear-Gaussian regime to make all derivations exact and computable without any data sampling.\n\nSetup and definitions:\n- Consider a generative model with latent vector $z = \\begin{bmatrix} z_{\\mathrm{c}} \\\\ z_{\\mathrm{f}} \\end{bmatrix} \\in \\mathbb{R}^{d}$, where $z_{\\mathrm{c}} \\in \\mathbb{R}^{d_{\\mathrm{c}}}$ are the coarse factors and $z_{\\mathrm{f}} \\in \\mathbb{R}^{d_{\\mathrm{f}}}$ are the fine factors, with $d = d_{\\mathrm{c}} + d_{\\mathrm{f}}$.\n- The observation model is linear-Gaussian: $x \\in \\mathbb{R}^{m}$ with $p(x \\mid z) = \\mathcal{N}(x \\mid G z, \\sigma_{x}^{2} I_{m})$, where $G \\in \\mathbb{R}^{m \\times d}$ and $\\sigma_{x}^{2} \\in \\mathbb{R}_{>0}$.\n- The data distribution is generated by a structured hierarchical Gaussian prior reflecting factorized hierarchies: $p_{\\mathrm{true}}(z) = p(z_{\\mathrm{c}}) p(z_{\\mathrm{f}} \\mid z_{\\mathrm{c}})$, with $p(z_{\\mathrm{c}}) = \\mathcal{N}(z_{\\mathrm{c}} \\mid 0, I_{d_{\\mathrm{c}}})$ and $p(z_{\\mathrm{f}} \\mid z_{\\mathrm{c}}) = \\mathcal{N}(z_{\\mathrm{f}} \\mid B z_{\\mathrm{c}}, \\sigma_{\\mathrm{f}}^{2} I_{d_{\\mathrm{f}}})$, where $B \\in \\mathbb{R}^{d_{\\mathrm{f}} \\times d_{\\mathrm{c}}}$ encodes how the fine factors depend on the coarse factors. Derive the implied joint Gaussian prior covariance for $z$.\n- Define the Beta-Variational Autoencoder (Beta-VAE) objective for a fixed decoder $p(x \\mid z)$ and a chosen prior $p(z)$ as maximizing, for each $x$, the quantity $\\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] - \\beta \\, \\mathrm{KL}(q(z \\mid x) \\,\\|\\, p(z))$, where $\\beta \\in \\mathbb{R}_{>0}$ and $\\mathrm{KL}$ denotes Kullback–Leibler divergence. Assume a flexible Gaussian encoder $q(z \\mid x)$ and that the optimal $q(z \\mid x)$ within the Gaussian family is attained.\n\nTask:\n1. Starting only from the definitions of multivariate Gaussian densities, the linear-Gaussian likelihood, and the Beta-Variational Autoencoder objective, derive the optimal Gaussian encoder $q^{\\star}(z \\mid x)$ under the Beta-Variational Autoencoder objective for a given Gaussian prior $p(z) = \\mathcal{N}(z \\mid 0, \\Sigma_{0})$ and linear decoder $p(x \\mid z) = \\mathcal{N}(x \\mid G z, \\sigma_{x}^{2} I_{m})$. Your derivation must show how the precision and mean of $q^{\\star}(z \\mid x)$ depend on $\\beta$, $\\Sigma_{0}$, $G$, and $\\sigma_{x}^{2}$ without assuming any pre-known closed form.\n2. For data generated from the structured prior $p_{\\mathrm{true}}(z)$ and the decoder $G$ above, compute the aggregated posterior covariance of $z$ under $q^{\\star}(z \\mid x)$, that is, the covariance of $z$ when $x$ is distributed according to the data distribution and $z \\sim q^{\\star}(z \\mid x)$. Use only linear-Gaussian identities and expectations.\n3. Using the aggregated posterior covariance from item $2$, compute two disentanglement-related quantities for the latent vector $z$:\n   - The total correlation of the aggregated posterior (which for a multivariate Gaussian depends only on its covariance), defined as the Kullback–Leibler divergence between the joint and the product of its marginals.\n   - The mutual information between the block variables $z_{\\mathrm{c}}$ and $z_{\\mathrm{f}}$ under the aggregated posterior, defined using only block covariances of the Gaussian.\n4. Repeat item $3$ twice for the same data distribution and decoder: once when the Beta-Variational Autoencoder objective uses a standard independent prior $p_{\\mathrm{std}}(z) = \\mathcal{N}(z \\mid 0, I_{d})$, and once when it uses the structured prior that matches the data hierarchy $p_{\\mathrm{struct}}(z) = \\mathcal{N}(z \\mid 0, \\Sigma_{\\mathrm{struct}})$ derived from $p(z_{\\mathrm{c}})$ and $p(z_{\\mathrm{f}} \\mid z_{\\mathrm{c}})$.\n5. For each test case below, compute two scalars:\n   - $\\Delta \\mathrm{TC} = \\mathrm{TC}(q_{\\mathrm{std}}(z)) - \\mathrm{TC}(q_{\\mathrm{struct}}(z))$,\n   - $\\Delta I = I_{q}(z_{\\mathrm{c}}; z_{\\mathrm{f}})_{\\mathrm{std}} - I_{q}(z_{\\mathrm{c}}; z_{\\mathrm{f}})_{\\mathrm{struct}}$,\n   where the subscript “std” denotes using $p_{\\mathrm{std}}(z)$ in the Beta-Variational Autoencoder objective and “struct” denotes using $p_{\\mathrm{struct}}(z)$. Positive values indicate a reduction achieved by the structured prior relative to the standard prior.\n\nConcrete specification for computation:\n- Use dimensions $d_{\\mathrm{c}} = d_{\\mathrm{f}} = 2$, total latent dimension $d = 4$, and observation dimension $m = 4$.\n- Use a block-diagonal decoder $G = \\mathrm{diag}(I_{2}, I_{2})$ and define $B = \\alpha I_{2}$ for a scalar coupling strength $\\alpha \\in \\mathbb{R}_{\\ge 0}$.\n- The true data prior is the hierarchical Gaussian specified above with parameters $\\alpha$ and $\\sigma_{\\mathrm{f}}^{2}$; the observation noise variance is $\\sigma_{x}^{2}$.\n- All matrices must be treated as full matrices; do not assume diagonality in intermediate derivations unless it follows from the specified structure.\n\nTest suite:\nProvide outputs for the following parameter sets $(\\beta, \\alpha, \\sigma_{x}, \\sigma_{\\mathrm{f}})$, in this exact order:\n- Case $1$: $(1.0, 0.8, 0.1, 0.5)$.\n- Case $2$: $(4.0, 0.8, 0.1, 0.5)$.\n- Case $3$: $(1.0, 0.0, 0.1, 0.5)$.\n- Case $4$: $(1.0, 1.5, 0.1, 0.5)$.\n- Case $5$: $(10.0, 0.8, 0.1, 0.5)$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[\\Delta \\mathrm{TC}_{1}, \\Delta I_{1}, \\Delta \\mathrm{TC}_{2}, \\Delta I_{2}, \\ldots, \\Delta \\mathrm{TC}_{5}, \\Delta I_{5}]$, where the subscript denotes the test case index from the list above.\n- Each entry must be a real number (float). No physical units are involved.",
            "solution": "The central task is to analyze the effect of a structured latent prior versus a standard isotropic prior within a $\\beta$-VAE framework on representation disentanglement. The analysis is performed in a fully tractable linear-Gaussian setting.\n\n### 1. Derivation of the Structured Prior Covariance\n\nThe problem specifies a true data-generating process with a hierarchical latent structure. The latent vector is $z = \\begin{bmatrix} z_{\\mathrm{c}} \\\\ z_{\\mathrm{f}} \\end{bmatrix} \\in \\mathbb{R}^{d}$, where $z_{\\mathrm{c}} \\in \\mathbb{R}^{d_{\\mathrm{c}}}$ are coarse factors and $z_{\\mathrm{f}} \\in \\mathbb{R}^{d_{\\mathrm{f}}}$ are fine factors. The true prior is $p_{\\mathrm{true}}(z) = p(z_{\\mathrm{c}}) p(z_{\\mathrm{f}} \\mid z_{\\mathrm{c}})$ with:\n-   $p(z_{\\mathrm{c}}) = \\mathcal{N}(z_{\\mathrm{c}} \\mid 0, I_{d_{\\mathrm{c}}})$\n-   $p(z_{\\mathrm{f}} \\mid z_{\\mathrm{c}}) = \\mathcal{N}(z_{\\mathrm{f}} \\mid B z_{\\mathrm{c}}, \\sigma_{\\mathrm{f}}^{2} I_{d_{\\mathrm{f}}})$\n\nThis defines a joint Gaussian distribution for $z$. The mean is $E[z] = \\begin{bmatrix} E[z_{\\mathrm{c}}] \\\\ E[E[z_{\\mathrm{f}} \\mid z_{\\mathrm{c}}]] \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ E[B z_{\\mathrm{c}}] \\end{bmatrix} = 0$.\n\nThe covariance matrix $\\Sigma_{\\mathrm{struct}}$ is derived by computing the block covariances:\n-   $\\mathrm{Cov}(z_{\\mathrm{c}}) = I_{d_{\\mathrm{c}}}$\n-   $\\mathrm{Cov}(z_{\\mathrm{c}}, z_{\\mathrm{f}}) = E[z_{\\mathrm{c}} z_{\\mathrm{f}}^T] = E[E[z_{\\mathrm{c}} z_{\\mathrm{f}}^T \\mid z_{\\mathrm{c}}]] = E[z_{\\mathrm{c}} (E[z_{\\mathrm{f}} \\mid z_{\\mathrm{c}}])^T] = E[z_{\\mathrm{c}} (B z_{\\mathrm{c}})^T] = E[z_{\\mathrm{c}} z_{\\mathrm{c}}^T B^T] = \\mathrm{Cov}(z_{\\mathrm{c}}) B^T = B^T$\n-   $\\mathrm{Cov}(z_{\\mathrm{f}}, z_{\\mathrm{c}}) = \\mathrm{Cov}(z_{\\mathrm{c}}, z_{\\mathrm{f}})^T = B$\n-   $\\mathrm{Cov}(z_{\\mathrm{f}})$ is found using the law of total covariance:\n    $\\mathrm{Cov}(z_{\\mathrm{f}}) = E[\\mathrm{Cov}(z_{\\mathrm{f}} \\mid z_{\\mathrm{c}})] + \\mathrm{Cov}(E[z_{\\mathrm{f}} \\mid z_{\\mathrm{c}}]) = E[\\sigma_{\\mathrm{f}}^{2} I_{d_{\\mathrm{f}}}] + \\mathrm{Cov}(B z_{\\mathrm{c}}) = \\sigma_{\\mathrm{f}}^{2} I_{d_{\\mathrm{f}}} + B \\mathrm{Cov}(z_{\\mathrm{c}}) B^T = \\sigma_{\\mathrm{f}}^{2} I_{d_{\\mathrm{f}}} + B B^T$\n\nThus, the covariance of the structured prior is:\n$$\n\\Sigma_{\\mathrm{struct}} = \\begin{pmatrix} I_{d_{\\mathrm{c}}} & B^T \\\\ B & \\sigma_{\\mathrm{f}}^{2} I_{d_{\\mathrm{f}}} + B B^T \\end{pmatrix}\n$$\nThis is the covariance for both the true data generating prior $p_{\\mathrm{true}}(z)$ and the structured prior model $p_{\\mathrm{struct}}(z)$ used in one arm of the experiment.\n\n### 2. Optimal Variational Posterior $q^{\\star}(z \\mid x)$\n\nThe $\\beta$-VAE objective is to maximize $\\mathcal{L}(q) = \\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] - \\beta \\, \\mathrm{KL}(q(z \\mid x) \\,\\|\\, p(z))$ with respect to the parameters of the variational posterior $q(z \\mid x)$. This is equivalent to minimizing the Kullback-Leibler divergence $\\mathrm{KL}(q(z \\mid x) \\,\\|\\, p^*(z))$, where $p^*(z) \\propto p(x \\mid z)^{1/\\beta} p(z)$. The optimal $q^{\\star}(z \\mid x)$ is therefore the normalized version of this composite distribution.\n\nGiven the Gaussian forms for the likelihood $p(x \\mid z) = \\mathcal{N}(x \\mid Gz, \\sigma_x^2 I_m)$ and a generic Gaussian prior $p(z) = \\mathcal{N}(z \\mid 0, \\Sigma_0)$, the log-density of the optimal posterior is proportional to:\n$$\n\\log q^{\\star}(z \\mid x) \\propto \\frac{1}{\\beta} \\log p(x \\mid z) + \\log p(z)\n$$\nSubstituting the log-densities of the Gaussians (and ignoring constants):\n$$\n\\log q^{\\star}(z \\mid x) \\propto \\frac{1}{\\beta} \\left(-\\frac{1}{2\\sigma_x^2}(x - Gz)^T(x - Gz)\\right) + \\left(-\\frac{1}{2}z^T \\Sigma_0^{-1} z\\right)\n$$\n$$\n\\propto -\\frac{1}{2\\beta\\sigma_x^2}(z^T G^T G z - 2x^T G z) - \\frac{1}{2}z^T \\Sigma_0^{-1} z\n$$\n$$\n\\propto -\\frac{1}{2} z^T \\left(\\frac{1}{\\beta\\sigma_x^2} G^T G + \\Sigma_0^{-1}\\right) z + \\left(\\frac{1}{\\beta\\sigma_x^2} G^T x\\right)^T z\n$$\nThis is a quadratic form in $z$, confirming $q^{\\star}(z \\mid x)$ is Gaussian, $q^{\\star}(z \\mid x) = \\mathcal{N}(z \\mid \\mu_q, \\Sigma_q)$. By completing the square or matching terms with the standard log-Gaussian form $-\\frac{1}{2}(z-\\mu_q)^T\\Sigma_q^{-1}(z-\\mu_q)$, we identify the precision matrix $\\Sigma_q^{-1}$ and mean $\\mu_q$:\n-   Precision: $\\Sigma_q^{-1} = \\frac{1}{\\beta\\sigma_x^2} G^T G + \\Sigma_0^{-1}$\n-   Mean: $\\mu_q(x) = \\Sigma_q \\left(\\frac{1}{\\beta\\sigma_x^2} G^T x\\right)$\n\nThe covariance of the optimal variational posterior is therefore $\\Sigma_q = \\left(\\frac{1}{\\beta\\sigma_x^2} G^T G + \\Sigma_0^{-1}\\right)^{-1}$, which is independent of the observation $x$.\n\n### 3. Aggregated Posterior Covariance\n\nThe aggregated posterior is the distribution of $z$ resulting from first sampling $x \\sim p_{\\mathrm{data}}(x)$ and then $z \\sim q^{\\star}(z \\mid x)$. The covariance of this aggregated distribution, $\\Sigma_{agg}$, is found via the law of total covariance:\n$$\n\\Sigma_{agg} = E_{p_{\\mathrm{data}}(x)}[\\mathrm{Cov}_{q^{\\star}}(z \\mid x)] + \\mathrm{Cov}_{p_{\\mathrm{data}}(x)}[E_{q^{\\star}}(z \\mid x)]\n$$\nThe first term is $E_{p_{\\mathrm{data}}(x)}[\\Sigma_q] = \\Sigma_q$ as $\\Sigma_q$ does not depend on $x$.\nThe second term is $\\mathrm{Cov}_{p_{\\mathrm{data}}(x)}[\\mu_q(x)] = \\mathrm{Cov}_{p_{\\mathrm{data}}(x)}\\left[\\Sigma_q \\frac{1}{\\beta\\sigma_x^2} G^T x\\right]$.\nLet $M = \\Sigma_q \\frac{1}{\\beta\\sigma_x^2} G^T$. This is $\\mathrm{Cov}(Mx) = M \\mathrm{Cov}(x) M^T$.\nThe covariance of the data distribution $p_{\\mathrm{data}}(x) = \\int p(x|z) p_{\\mathrm{true}}(z) dz$ is $\\mathrm{Cov}(x) = G \\Sigma_{\\mathrm{true}} G^T + \\sigma_x^2 I_m$.\nCombining these gives:\n$$\n\\Sigma_{agg} = \\Sigma_q + \\left(\\Sigma_q \\frac{G^T}{\\beta\\sigma_x^2}\\right) (G \\Sigma_{\\mathrm{true}} G^T + \\sigma_x^2 I_m) \\left(\\frac{G}{\\beta\\sigma_x^2} \\Sigma_q\\right)\n$$\nThis expression depends on the VAE prior $\\Sigma_0$ (through $\\Sigma_q$) and the true data prior $\\Sigma_{\\mathrm{true}}$.\n\n### 4. Disentanglement Metrics\n\nFor a $d$-dimensional zero-mean Gaussian variable $z$ with covariance $\\Sigma$, the specified metrics are:\n-   **Total Correlation (TC)**: The KL divergence between the joint and the product of its marginals.\n    $$\n    \\mathrm{TC}(z) = \\mathrm{KL}(\\mathcal{N}(0, \\Sigma) \\,\\|\\, \\mathcal{N}(0, \\mathrm{diag}(\\Sigma_{11}, ..., \\Sigma_{dd}))) = \\frac{1}{2}\\left(\\sum_{i=1}^d \\log(\\Sigma_{ii}) - \\log|\\Sigma|\\right)\n    $$\n-   **Mutual Information (MI)**: For a partitioned vector $z = [z_c^T, z_f^T]^T$ with block covariance $\\Sigma = \\begin{pmatrix} \\Sigma_{cc} & \\Sigma_{cf} \\\\ \\Sigma_{fc} & \\Sigma_{ff} \\end{pmatrix}$:\n    $$\n    I(z_c; z_f) = \\frac{1}{2}\\log\\frac{|\\Sigma_{cc}||\\Sigma_{ff}|}{|\\Sigma|}\n    $$\n\n### 5. Specialization for Computation\n\nThe problem provides specific parameters: $d_c=d_f=2$, $G=I_4$, $B=\\alpha I_2$. The true covariance $\\Sigma_{\\mathrm{true}}$ permutes into a block-diagonal form $\\begin{pmatrix} S & 0 \\\\ 0 & S \\end{pmatrix}$, where $S = \\begin{pmatrix} 1 & \\alpha \\\\ \\alpha & \\sigma_f^2+\\alpha^2 \\end{pmatrix}$. All other matrices in the derivations ($I_4$, and thus $\\Sigma_q$ and $\\Sigma_{agg}$) share this property in the permuted basis. This allows all computations to be performed on the $2 \\times 2$ block level.\nLet $M$ be the $2 \\times 2$ block of $\\Sigma_{agg}$ in this basis. The full $4 \\times 4$ covariance is permutation-similar to $\\begin{pmatrix} M & 0 \\\\ 0 & M \\end{pmatrix}$. In the original basis $(z_{c1}, z_{c2}, z_{f1}, z_{f2})$, this results in a specific structure for $\\Sigma_{agg}$.\nThe diagonal elements of $\\Sigma_{agg}$ are $(M_{11}, M_{11}, M_{22}, M_{22})$, and its determinant is $|\\Sigma_{agg}| = (\\det M)^2$. The blocks for mutual information are $\\Sigma_{cc} = M_{11}I_2$ and $\\Sigma_{ff} = M_{22}I_2$.\nApplying the metric formulas to this structure:\n-   $\\mathrm{TC}(\\Sigma_{agg}) = \\frac{1}{2}(2\\log M_{11} + 2\\log M_{22} - \\log((\\det M)^2)) = \\log\\frac{M_{11} M_{22}}{\\det M}$\n-   $I_{q}(z_c; z_f) = \\frac{1}{2}\\log\\frac{|M_{11}I_2||M_{22}I_2|}{(\\det M)^2} = \\frac{1}{2}\\log\\frac{M_{11}^2 M_{22}^2}{(\\det M)^2} = \\log\\frac{M_{11} M_{22}}{|\\det M|}$\n\nFor a covariance matrix, $\\det M > 0$. Therefore, in this specific setup, $\\mathrm{TC}(z) = I_q(z_c; z_f)$. Consequently, $\\Delta \\mathrm{TC}$ will be identical to $\\Delta I$. The calculation proceeds by computing the $2 \\times 2$ aggregate block matrix $M$ for both the standard prior case ($\\Sigma_0 = I_4$) and the structured prior case ($\\Sigma_0 = \\Sigma_{\\mathrm{true}}$), then evaluating the metrics and their difference.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_disentanglement_metric(M):\n    \"\"\"\n    Calculates TC and I(zc; zf) for the aggregated posterior, which are identical \n    in this problem's specific setting due to its block structure.\n    The formula is log(m11*m22 / det(M)).\n\n    Args:\n        M (np.ndarray): The 2x2 covariance block of the aggregated posterior.\n\n    Returns:\n        float: The value of the disentanglement metric.\n    \"\"\"\n    m11 = M[0, 0]\n    m22 = M[1, 1]\n    det_M = np.linalg.det(M)\n    \n    # det_M must be positive for a valid covariance matrix and non-degenerate solution.\n    if det_M <= 1e-12: # Use a small tolerance for numerical stability\n        # A very small positive determinant indicates extreme correlation.\n        # log(infinity) would result. Returning a large number is more stable than np.inf.\n        return 1e9\n    \n    # The argument to log should be >= 1.\n    val = (m11 * m22) / det_M\n    if val < 1.0:\n        # This can happen due to numerical precision issues when correlation is near zero.\n        # TC and MI are non-negative.\n        return 0.0\n\n    return np.log(val)\n\ndef solve_for_case(beta, alpha, sigma_x, sigma_f):\n    \"\"\"\n    Computes delta_TC and delta_I for a given set of parameters. \n    In this problem, delta_TC = delta_I.\n\n    Args:\n        beta (float): The beta parameter for the VAE.\n        alpha (float): The coupling strength in the true prior.\n        sigma_x (float): The observation noise standard deviation.\n        sigma_f (float): The fine factor noise standard deviation.\n\n    Returns:\n        tuple[float, float]: A tuple containing (delta_TC, delta_I).\n    \"\"\"\n    sigma_x_sq = sigma_x**2\n    sigma_f_sq = sigma_f**2\n\n    # Define the 2x2 S block of the true prior covariance matrix\n    S = np.array([\n        [1.0, alpha],\n        [alpha, sigma_f_sq + alpha**2]\n    ])\n    \n    # This combination appears in the aggregated posterior calculation\n    S_plus_noise = S + sigma_x_sq * np.identity(2)\n    beta_sigma_x_sq = beta * sigma_x_sq\n\n    # ---- Standard Prior Case (Sigma_0 = I) ----\n    \n    # The standard prior Sigma_0 = I makes the q posterior diagonal.\n    # The 2x2 block matrix for Sigma_q is c_q_std * I_2.\n    c_q_std = beta_sigma_x_sq / (1.0 + beta_sigma_x_sq) # Note: G=I, so G^TG=I\n    \n    # The 2x2 block matrix for the aggregated posterior covariance.\n    # The formula is M_agg = M_q + (1/(beta*sigma_x^2)^2) * M_q * (G*S_true*G^T + sigma_x^2*I) * M_q\n    # Since M_q is diagonal and G=I, this simplifies.\n    M_q_std = c_q_std * np.identity(2)\n    coeff_std = (1.0 / beta_sigma_x_sq)**2\n    M_agg_std = M_q_std + coeff_std * M_q_std @ S_plus_noise @ M_q_std\n\n    Q_std = calculate_disentanglement_metric(M_agg_std)\n\n    # ---- Structured Prior Case (Sigma_0 = Sigma_true) ----\n    \n    # Inverse of the S block. det(S) = sigma_f_sq.\n    S_inv = (1.0 / sigma_f_sq) * np.array([\n        [sigma_f_sq + alpha**2, -alpha],\n        [-alpha, 1.0]\n    ])\n    \n    # 2x2 block for the inverse of Sigma_q\n    M_q_struct_inv = (1.0 / beta_sigma_x_sq) * np.identity(2) + S_inv\n    \n    # 2x2 block for Sigma_q\n    M_q_struct = np.linalg.inv(M_q_struct_inv)\n    \n    # 2x2 block for the aggregated posterior covariance\n    M_agg_struct = M_q_struct + (1.0 / beta_sigma_x_sq**2) * M_q_struct @ S_plus_noise @ M_q_struct\n    \n    Q_struct = calculate_disentanglement_metric(M_agg_struct)\n    \n    # Calculate the difference in metrics.\n    delta_Q = Q_std - Q_struct\n    \n    # As derived in the solution, delta_TC and delta_I are identical.\n    return delta_Q, delta_Q\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (1.0, 0.8, 0.1, 0.5), # Case 1\n        (4.0, 0.8, 0.1, 0.5), # Case 2\n        (1.0, 0.0, 0.1, 0.5), # Case 3\n        (1.0, 1.5, 0.1, 0.5), # Case 4\n        (10.0, 0.8, 0.1, 0.5), # Case 5\n    ]\n\n    results = []\n    for case in test_cases:\n        beta, alpha, sigma_x, sigma_f = case\n        delta_tc, delta_i = solve_for_case(beta, alpha, sigma_x, sigma_f)\n        results.append(delta_tc)\n        results.append(delta_i)\n\n    # Format output as specified in the problem statement\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}