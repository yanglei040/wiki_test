{
    "hands_on_practices": [
        {
            "introduction": "GAN 训练过程的不稳定性是众所周知的挑战，其根源常在于判别器梯度的爆炸或消失。为了稳定训练，研究者们提出了多种正则化方法来约束判别器的行为，其中 R1 正则化（应用于 StyleGAN2）和带有梯度惩罚的 WGAN（WGAN-GP）是两种主流技术，它们都致力于使判别器近似满足 1-Lipschitz 约束。\n\n本练习 () 将引导你在一个可控的一维玩具问题上，通过第一性原理实现这两种正则化器的一次梯度更新。通过亲手计算并观察它们对判别器梯度景观（即 Lipschitz 常数和梯度方差）的直接影响，你将获得关于它们如何稳定训练过程的具象化认识 。",
            "id": "3098187",
            "problem": "考虑基于风格的生成对抗网络第 2 版 (StyleGAN2) 及其判别器正则化，并将其与带梯度惩罚的 Wasserstein 生成对抗网络 (WGAN-GP) 进行对比。目标是在一个受控的玩具数据集上，分离并比较它们对输入空间 Lipschitz 连续性和稳定性的影响。在一维空间中，定义一个带参数 $\\theta = (\\alpha,\\beta,\\gamma)$ 的可微标量判别器函数 $D(x;\\theta)$ 如下：\n$$\nD(x;\\theta) = \\alpha x + \\beta \\sin(\\gamma x),\n$$\n及其输入梯度\n$$\ng(x;\\theta) = \\frac{\\partial D(x;\\theta)}{\\partial x} = \\alpha + \\beta \\gamma \\cos(\\gamma x).\n$$\n\n使用以下基本定义和事实作为您的起点：\n- 在域 $\\mathcal{X}$ 上，一个可微函数 $f$ 的 Lipschitz 常数满足 $L(f) \\le \\sup_{x \\in \\mathcal{X}} \\|\\nabla f(x)\\|_2$。在一维标量输出的情况下，这简化为 $L(f) \\le \\sup_{x \\in \\mathcal{X}} |f'(x)|$。\n- WGAN-GP 正则化旨在通过惩罚梯度范数与 $1$ 的偏差来强制实现近似的 $1$-Lipschitz 连续性。其惩罚项为\n$$\nP_{\\mathrm{GP}}(\\theta) = \\lambda \\,\\mathbb{E}_{\\hat{x}} \\left(\\left|g(\\hat{x};\\theta)\\right| - 1\\right)^2,\n$$\n其中 $\\hat{x} = t\\,x_r + (1-t)\\,x_f$，$t \\sim \\mathrm{Uniform}[0,1]$，$x_r \\sim p_r$ 是一个真实样本，$x_f \\sim p_f$ 是一个生成样本。\n- StyleGAN2 中使用的 R1 正则化惩罚真实数据上的梯度范数的平方：\n$$\nP_{\\mathrm{R1}}(\\theta) = \\lambda \\,\\mathbb{E}_{x_r \\sim p_r} \\left(g(x_r;\\theta)^2\\right).\n$$\n\n给定真实数据和生成数据的受控单变量高斯分布：\n- 真实数据：$x_r \\sim \\mathcal{N}(\\mu_r,\\sigma_r^2)$。\n- 生成数据：$x_f \\sim \\mathcal{N}(\\mu_f,\\sigma_f^2)$。\n\n考虑仅对正则化器进行单步梯度下降（即忽略对抗性损失），学习率为 $\\eta$：\n$$\n\\theta_{\\mathrm{new}} = \\theta_{\\mathrm{old}} - \\eta \\,\\nabla_{\\theta} P(\\theta_{\\mathrm{old}}),\n$$\n该步骤分别应用于 $P_{\\mathrm{R1}}$ 和 $P_{\\mathrm{GP}}$（从相同的 $\\theta_{\\mathrm{old}}$ 开始进行两次独立的更新）。每次更新后，估计以下指标：\n- 经验 Lipschitz 常数\n$$\n\\widehat{L}(\\theta) = \\max_{x \\in \\mathcal{S}} |g(x;\\theta)|,\n$$\n该常数在一个有限评估集 $\\mathcal{S}$ 上计算，$\\mathcal{S}$ 定义为以下集合的并集：\n    - 在区间 $[a,b]$ 上的 $M$ 个点的均匀网格，其中 $a = \\min(\\mu_r - 4\\sigma_r,\\, \\mu_f - 4\\sigma_f)$ 且 $b = \\max(\\mu_r + 4\\sigma_r,\\, \\mu_f + 4\\sigma_f)$；\n    - 一个包含 $N$ 个真实样本 $x_r$ 的批次；\n    - 一个包含 $N$ 个生成样本 $x_f$ 的批次；\n    - 一个包含 $N$ 个如上定义的插值样本 $\\hat{x}$ 的批次。\n- 真实数据上的稳定性指数，定义为 $g(x_r;\\theta)$ 的样本方差：\n$$\nS(\\theta) = \\mathrm{Var}\\left[g(x_r;\\theta)\\right],\n$$\n该指数在同一个包含 $N$ 个真实样本的批次上进行估计。\n\n实现一个程序，该程序：\n1. 根据指定的分布和每个测试用例的固定随机种子，构建玩具数据集批次。\n2. 使用链式法则和上述定义，从第一性原理计算参数梯度 $\\nabla_{\\theta} P_{\\mathrm{R1}}$ 和 $\\nabla_{\\theta} P_{\\mathrm{GP}}$。\n3. 对每个正则化器分别应用一个梯度下降更新步骤，得到 $\\theta_{\\mathrm{R1}}$ 和 $\\theta_{\\mathrm{GP}}$。\n4. 评估 $\\widehat{L}(\\theta_{\\mathrm{R1}})$, $\\widehat{L}(\\theta_{\\mathrm{GP}})$, $S(\\theta_{\\mathrm{R1}})$ 和 $S(\\theta_{\\mathrm{GP}})$。\n\n您的程序必须使用样本大小为 $N$ 的蒙特卡洛估计来计算期望值，并使用包含 $M$ 个点的均匀网格进行 Lipschitz 估计。所有计算都是实值浮点数。不涉及物理单位或角度。通过在生成任何样本之前设置指定的种子，所有随机抽样必须是确定性的。\n\n测试套件：\n为以下四个测试用例提供结果，每个用例以参数元组 $(\\alpha_0,\\beta_0,\\gamma_0,\\mu_r,\\sigma_r,\\mu_f,\\sigma_f,\\lambda,\\eta,N,M,\\mathrm{seed})$ 的形式给出：\n- 案例 $1$ (一般非线性): $(\\alpha_0=0.8, \\beta_0=0.5, \\gamma_0=1.5, \\mu_r=0.0, \\sigma_r=1.0, \\mu_f=1.0, \\sigma_f=1.0, \\lambda=10.0, \\eta=0.01, N=4000, M=400, \\mathrm{seed}=42)$.\n- 案例 $2$ (线性判别器边界): $(\\alpha_0=2.0, \\beta_0=0.0, \\gamma_0=1.0, \\mu_r=-0.5, \\sigma_r=0.5, \\mu_f=0.5, \\sigma_f=0.5, \\lambda=5.0, \\eta=0.02, N=4000, M=400, \\mathrm{seed}=0)$.\n- 案例 $3$ (低频非线性): $(\\alpha_0=0.5, \\beta_0=1.0, \\gamma_0=0.1, \\mu_r=0.0, \\sigma_r=1.5, \\mu_f=0.0, \\sigma_f=1.5, \\lambda=2.0, \\eta=0.05, N=4000, M=400, \\mathrm{seed}=123)$.\n- 案例 $4$ (高频非线性): $(\\alpha_0=0.3, \\beta_0=1.0, \\gamma_0=5.0, \\mu_r=0.0, \\sigma_r=1.0, \\mu_f=-1.0, \\sigma_f=1.0, \\lambda=10.0, \\eta=0.01, N=4000, M=400, \\mathrm{seed}=999)$.\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个测试用例的结果列表。对于每个用例，按顺序 $[\\widehat{L}(\\theta_{\\mathrm{R1}}),\\widehat{L}(\\theta_{\\mathrm{GP}}),S(\\theta_{\\mathrm{R1}}),S(\\theta_{\\mathrm{GP}})]$ 输出一个包含四个浮点数的列表。将这四个用例的列表聚合到一个外部列表中。最终打印的字符串必须看起来像一个 Python 的列表的列表，例如 $[[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$。",
            "solution": "问题陈述已经过仔细审查，并被确定为有效。它在科学上基于深度学习和优化的原理，特别是关于生成对抗网络 (GAN) 中的正则化技术。该问题定义良好，提供了一套完整且明确的定义、参数和程序。这是一个客观、可形式化的任务，需要推导并实现一个计算实验。\n\n问题的核心是计算单步梯度下降对一个玩具判别器 $D(x;\\theta)$ 的参数 $\\theta = (\\alpha, \\beta, \\gamma)$ 的影响，这涉及两种不同的正则化器：R1 和 WGAN-GP。我们必须首先推导这些正则化惩罚项相对于 $\\theta$ 的梯度。\n\n判别器相对于其输入 $x$ 的梯度由下式给出：\n$$\ng(x;\\theta) = \\frac{\\partial D(x;\\theta)}{\\partial x} = \\alpha + \\beta \\gamma \\cos(\\gamma x)\n$$\n\n为了求出正则化器的梯度 $\\nabla_{\\theta} P(\\theta)$，我们首先需要 $g(x;\\theta)$ 相对于 $\\theta$ 中每个参数的偏导数：\n$$\n\\frac{\\partial g}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left( \\alpha + \\beta \\gamma \\cos(\\gamma x) \\right) = 1\n$$\n$$\n\\frac{\\partial g}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\left( \\alpha + \\beta \\gamma \\cos(\\gamma x) \\right) = \\gamma \\cos(\\gamma x)\n$$\n使用乘法法则求相对于 $\\gamma$ 的导数：\n$$\n\\frac{\\partial g}{\\partial \\gamma} = \\frac{\\partial}{\\partial \\gamma} \\left( \\alpha + \\beta \\gamma \\cos(\\gamma x) \\right) = \\beta \\frac{\\partial}{\\partial \\gamma} \\left( \\gamma \\cos(\\gamma x) \\right) = \\beta \\left( 1 \\cdot \\cos(\\gamma x) + \\gamma \\cdot (-\\sin(\\gamma x) \\cdot x) \\right) = \\beta (\\cos(\\gamma x) - \\gamma x \\sin(\\gamma x))\n$$\n我们将这些偏导数收集成一个向量 $\\nabla_{\\theta} g(x;\\theta) = \\left[ \\frac{\\partial g}{\\partial \\alpha}, \\frac{\\partial g}{\\partial \\beta}, \\frac{\\partial g}{\\partial \\gamma} \\right]^T$。\n\n### R1 正则化的梯度\nR1 正则化器定义为 $P_{\\mathrm{R1}}(\\theta) = \\lambda \\,\\mathbb{E}_{x_r \\sim p_r} \\left( g(x_r;\\theta)^2 \\right)$。\n我们可以交换梯度和期望算子。应用链式法则，相对于 $\\theta$ 的梯度为：\n$$\n\\nabla_{\\theta} P_{\\mathrm{R1}}(\\theta) = \\lambda \\,\\mathbb{E}_{x_r} \\left[ \\nabla_{\\theta} \\left( g(x_r;\\theta)^2 \\right) \\right] = \\lambda \\,\\mathbb{E}_{x_r} \\left[ 2 g(x_r;\\theta) \\nabla_{\\theta} g(x_r;\\theta) \\right]\n$$\n期望 $\\mathbb{E}_{x_r}[\\cdot]$ 通过对一批 $N$ 个真实样本 $\\{x_{r,i}\\}_{i=1}^N$ 进行蒙特卡洛估计来近似：\n$$\n\\nabla_{\\theta} P_{\\mathrm{R1}}(\\theta) \\approx \\frac{2\\lambda}{N} \\sum_{i=1}^{N} g(x_{r,i};\\theta) \\nabla_{\\theta} g(x_{r,i};\\theta)\n$$\n\n### WGAN-GP 正则化的梯度\nWGAN-GP 正则化器定义为 $P_{\\mathrm{GP}}(\\theta) = \\lambda \\,\\mathbb{E}_{\\hat{x}} \\left(\\left|g(\\hat{x};\\theta)\\right| - 1\\right)^2$，其中样本 $\\hat{x}$ 是从真实数据和生成数据之间的插值中抽取的。\n再次使用链式法则，其梯度为：\n$$\n\\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta) = \\lambda \\,\\mathbb{E}_{\\hat{x}} \\left[ \\nabla_{\\theta} \\left( |g(\\hat{x};\\theta)| - 1 \\right)^2 \\right] = \\lambda \\,\\mathbb{E}_{\\hat{x}} \\left[ 2 \\left( |g(\\hat{x};\\theta)| - 1 \\right) \\nabla_{\\theta} |g(\\hat{x};\\theta)| \\right]\n$$\n对于 $u \\neq 0$，使用恒等式 $\\nabla |u| = \\mathrm{sgn}(u) \\nabla u$：\n$$\n\\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta) = 2\\lambda \\,\\mathbb{E}_{\\hat{x}} \\left[ \\left( |g(\\hat{x};\\theta)| - 1 \\right) \\mathrm{sgn}(g(\\hat{x};\\theta)) \\nabla_{\\theta} g(\\hat{x};\\theta) \\right]\n$$\n该期望 $\\mathbb{E}_{\\hat{x}}[\\cdot]$ 通过对一批 $N$ 个插值样本 $\\{\\hat{x}_i\\}_{i=1}^N$ 进行蒙特卡洛估计来近似：\n$$\n\\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta) \\approx \\frac{2\\lambda}{N} \\sum_{i=1}^{N} \\left( \\left|g(\\hat{x}_i;\\theta)\\right| - 1 \\right) \\mathrm{sgn}(g(\\hat{x}_i;\\theta)) \\nabla_{\\theta} g(\\hat{x}_i;\\theta)\n$$\n\n### 算法流程\n每个测试用例的总体流程如下：\n1.  初始化参数 $\\theta_{\\mathrm{old}} = (\\alpha_0, \\beta_0, \\gamma_0)$ 并设置随机种子以确保可复现性。\n2.  生成数据批次：$N$ 个真实样本 $x_r \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$，$N$ 个生成样本 $x_f \\sim \\mathcal{N}(\\mu_f, \\sigma_f^2)$，以及 $N$ 个插值系数 $t \\sim \\mathrm{Uniform}[0,1]$。构建 $N$ 个插值样本 $\\hat{x}$。\n3.  在真实样本 $x_r$ 上通过蒙特卡洛估计计算梯度 $\\nabla_{\\theta} P_{\\mathrm{R1}}(\\theta_{\\mathrm{old}})$。\n4.  执行 R1 梯度下降更新：$\\theta_{\\mathrm{R1}} = \\theta_{\\mathrm{old}} - \\eta \\nabla_{\\theta} P_{\\mathrm{R1}}(\\theta_{\\mathrm{old}})$。\n5.  在插值样本 $\\hat{x}$ 上通过蒙特卡洛估计计算梯度 $\\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta_{\\mathrm{old}})$。\n6.  执行 GP 梯度下降更新：$\\theta_{\\mathrm{GP}} = \\theta_{\\mathrm{old}} - \\eta \\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta_{\\mathrm{old}})$。\n7.  构建评估集 $\\mathcal{S}$，作为数据范围上的均匀网格以及生成的样本批次（$x_r, x_f, \\hat{x}$）的并集。\n8.  通过在评估集 $\\mathcal{S}$ 上找到 $|g(x;\\theta)|$ 的最大值，为每个更新后的参数集估计 Lipschitz 常数：\n    $$\n    \\widehat{L}(\\theta_{\\mathrm{R1}}) = \\max_{x \\in \\mathcal{S}} |g(x;\\theta_{\\mathrm{R1}})| \\quad \\text{和} \\quad \\widehat{L}(\\theta_{\\mathrm{GP}}) = \\max_{x \\in \\mathcal{S}} |g(x;\\theta_{\\mathrm{GP}})|\n    $$\n9.  通过在真实数据批次上计算 $g(x_r;\\theta)$ 的样本方差，为每个更新后的参数集估计稳定性指数：\n    $$\n    S(\\theta_{\\mathrm{R1}}) = \\mathrm{Var}[g(x_r;\\theta_{\\mathrm{R1}})] \\quad \\text{和} \\quad S(\\theta_{\\mathrm{GP}}) = \\mathrm{Var}[g(x_r;\\theta_{\\mathrm{GP}})]\n    $$\n10. 整理四个结果指标 $[\\widehat{L}(\\theta_{\\mathrm{R1}}), \\widehat{L}(\\theta_{\\mathrm{GP}}), S(\\theta_{\\mathrm{R1}}), S(\\theta_{\\mathrm{GP}})]$。\n\n对每个提供的测试用例都执行此流程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases specified.\n    \"\"\"\n    test_cases = [\n        # (alpha0, beta0, gamma0, mu_r, sigma_r, mu_f, sigma_f, lambda, eta, N, M, seed)\n        (0.8, 0.5, 1.5, 0.0, 1.0, 1.0, 1.0, 10.0, 0.01, 4000, 400, 42),\n        (2.0, 0.0, 1.0, -0.5, 0.5, 0.5, 0.5, 5.0, 0.02, 4000, 400, 0),\n        (0.5, 1.0, 0.1, 0.0, 1.5, 0.0, 1.5, 2.0, 0.05, 4000, 400, 123),\n        (0.3, 1.0, 5.0, 0.0, 1.0, -1.0, 1.0, 10.0, 0.01, 4000, 400, 999),\n    ]\n\n    # Helper functions for discriminator gradient g(x; theta) and its partials\n    def g(x, theta):\n        alpha, beta, gamma = theta\n        return alpha + beta * gamma * np.cos(gamma * x)\n\n    def dg_dalpha(x, theta):\n        return np.ones_like(x)\n\n    def dg_dbeta(x, theta):\n        alpha, beta, gamma = theta\n        return gamma * np.cos(gamma * x)\n\n    def dg_dgamma(x, theta):\n        alpha, beta, gamma = theta\n        return beta * (np.cos(gamma * x) - gamma * x * np.sin(gamma * x))\n\n    final_results = []\n    for case in test_cases:\n        alpha0, beta0, gamma0, mu_r, sigma_r, mu_f, sigma_f, lam, eta, N, M, seed = case\n        theta_old = np.array([alpha0, beta0, gamma0])\n        \n        # 1. Set seed and generate data\n        rng = np.random.default_rng(seed)\n        x_r = rng.normal(mu_r, sigma_r, N)\n        x_f = rng.normal(mu_f, sigma_f, N)\n        t = rng.uniform(0, 1, N)\n        x_hat = t * x_r + (1 - t) * x_f\n        \n        # 2. Compute R1 update\n        g_xr = g(x_r, theta_old)\n        grad_r1_alpha = (2 * lam * np.mean(g_xr * dg_dalpha(x_r, theta_old)))\n        grad_r1_beta = (2 * lam * np.mean(g_xr * dg_dbeta(x_r, theta_old)))\n        grad_r1_gamma = (2 * lam * np.mean(g_xr * dg_dgamma(x_r, theta_old)))\n        grad_r1 = np.array([grad_r1_alpha, grad_r1_beta, grad_r1_gamma])\n        theta_r1 = theta_old - eta * grad_r1\n        \n        # 3. Compute GP update\n        g_xhat = g(x_hat, theta_old)\n        gp_term = (np.abs(g_xhat) - 1) * np.sign(g_xhat)\n        grad_gp_alpha = (2 * lam * np.mean(gp_term * dg_dalpha(x_hat, theta_old)))\n        grad_gp_beta = (2 * lam * np.mean(gp_term * dg_dbeta(x_hat, theta_old)))\n        grad_gp_gamma = (2 * lam * np.mean(gp_term * dg_dgamma(x_hat, theta_old)))\n        grad_gp = np.array([grad_gp_alpha, grad_gp_beta, grad_gp_gamma])\n        theta_gp = theta_old - eta * grad_gp\n        \n        # 4. Evaluate metrics\n        # Construct evaluation set S\n        a = min(mu_r - 4 * sigma_r, mu_f - 4 * sigma_f)\n        b = max(mu_r + 4 * sigma_r, mu_f + 4 * sigma_f)\n        grid_points = np.linspace(a, b, M)\n        eval_set = np.concatenate([grid_points, x_r, x_f, x_hat])\n        \n        # Empirical Lipschitz constant\n        L_hat_r1 = np.max(np.abs(g(eval_set, theta_r1)))\n        L_hat_gp = np.max(np.abs(g(eval_set, theta_gp)))\n        \n        # Stability index\n        S_r1 = np.var(g(x_r, theta_r1))\n        S_gp = np.var(g(x_r, theta_gp))\n        \n        case_result = [L_hat_r1, L_hat_gp, S_r1, S_gp]\n        final_results.append([f\"{v:.8f}\" for v in case_result])\n\n    # Final print statement in the exact required format\n    # The format requires string representation of list of lists.\n    # e.g., [[1.0, 2.0], [3.0, 4.0]] -> '[[1.0, 2.0],[3.0, 4.0]]'\n    # The map(str,...) and join is a robust way to achieve this.\n    print(f\"[{','.join(map(str, final_results))}]\".replace(\"'\", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "StyleGAN 的一项革命性创新是在生成器的不同分辨率层级上直接注入随机噪声。这种设计允许模型对生成图像的精细细节（如头发纹理、雀斑）进行随机变化，而不会影响其高级结构（如人脸姿态、身份），这与仅在输入层提供噪声的传统 GAN 形成了鲜明对比。\n\n在这个练习中 ()，你将通过一个一维信号处理实验来模拟这一过程。通过在不同“分辨率”上开启或关闭噪声注入，并量化评估其对“结构稳定性”和“纹理真实感”的影响，你将直观地理解 StyleGAN 分层合成思想背后的权衡与奥秘 。",
            "id": "3098258",
            "problem": "您将实现并分析一个简化的、受基于风格的生成对抗网络（StyleGAN）启发的逐分辨率噪声注入机制。目标是在不同分辨率下对噪声注入进行消融研究，并定量测量其对两个指标的影响：纹理真实感和结构稳定性。您将使用线性时不变系统和频域分析的原理，通过一维信号实现一个纯粹的数学化、程序化的实验。\n\n从以下基本原理开始：\n- 加性高斯白噪声是一个随机过程，其独立样本抽样自均值为零、方差为 $\\sigma^2$ 的高斯分布。当通过有限冲激响应核进行线性滤波时，输出仍然是高斯过程，其功率谱密度是输入谱密度乘以滤波器频率响应的幅值平方。\n- 离散傅里叶变换将时域卷积与频域乘法联系起来。令 $\\mathcal{F}\\{x\\}[k]$ 表示 $x[n]$ 的离散傅里叶变换；则 $\\mathcal{F}\\{x \\ast h\\}[k] = \\mathcal{F}\\{x\\}[k] \\cdot \\mathcal{F}\\{h\\}[k]$，其中 $\\ast$ 表示循环卷积。\n- 两个有限长度信号 $x[n]$ 和 $y[n]$ 之间的 Pearson 相关系数定义为 $\\rho(x,y) = \\frac{\\sum_{n=0}^{N-1} (x[n]-\\bar{x})(y[n]-\\bar{y})}{\\sqrt{\\sum_{n=0}^{N-1} (x[n]-\\bar{x})^2}\\sqrt{\\sum_{n=0}^{N-1} (y[n]-\\bar{y})^2}}$，其中 $\\bar{x}$ 和 $\\bar{y}$ 是样本均值。\n- 快速傅里叶变换（FFT）可以高效地计算离散傅里叶变换。离散信号中的高频能量由较大频率索引处的 FFT 系数的幅值平方所捕获。\n\n按如下方式构建一个玩具生成器。\n\n1. 域和基本结构：\n   - 使用信号长度 $N = 512$。\n   - 定义一个结构信号 $s[n]$，它通过以弧度为单位的正弦波之和来编码低频“形状”：\n     $$ s[n] = 0.6 \\sin\\left(\\frac{2\\pi \\cdot 2 \\cdot n}{N}\\right) + 0.4 \\sin\\left(\\frac{2\\pi \\cdot 5 \\cdot n}{N}\\right), \\quad n = 0,1,\\dots,N-1. $$\n     角度以弧度为单位。\n\n2. 多分辨率噪声注入：\n   - 考虑 $L = 4$ 个噪声分辨率，对于级别 $\\ell = 0,1,2,3$（从最粗到最细），其步长为 $s_\\ell \\in \\{64, 32, 16, 8\\}$。在每个级别 $\\ell$，抽取一个长度为 $N / s_\\ell$ 的低分辨率高斯白噪声向量 $u_\\ell[m]$，其中 $u_\\ell[m] \\sim \\mathcal{N}(0, 1)$。\n   - 通过最近邻重复将 $u_\\ell[m]$ 上采样至长度 $N$，重复因子为 $s_\\ell$，生成 $v_\\ell[n]$。\n   - 使用长度为 $s_\\ell$ 的盒式滤波器（离散平均滤波器）对 $v_\\ell[n]$ 进行低通滤波，并采用循环边界条件以避免引入虚假高频。令 $h_\\ell[n]$ 为核函数，定义为当 $n \\in \\{0,1,\\dots,s_\\ell-1\\}$ 时，$h_\\ell[n] = 1/s_\\ell$，否则 $h_\\ell[n]=0$。滤波后的噪声为 $w_\\ell = v_\\ell \\ast h_\\ell$（循环卷积）。\n   - 在每个级别注入经幅度 $a_\\ell \\ge 0$ 缩放的噪声，得到生成器输出\n     $$ x[n] = s[n] + \\sum_{\\ell=0}^{3} a_\\ell \\, w_\\ell[n]. $$\n   - 对所有高斯抽样使用固定的随机种子 $0$ 以保证可复现性。\n\n3. 指标：\n   - 结构稳定性分数 $\\mathrm{S}(x)$ 是 Pearson 相关系数 $\\rho(x, s)$。\n   - 纹理真实感分数 $\\mathrm{R}(x)$ 定义如下。计算 $x[n]$ 的单边实数 FFT，不包括直流分量。令 $X[k]$ 表示索引 $k = 0, 1, \\dots, N/2$ 的单边 FFT。定义功率 $P[k] = |X[k]|^2$。令 $k_c = 32$。定义高频功率分数\n     $$ p_{\\mathrm{high}}(x) = \\frac{\\sum_{k=k_c}^{N/2} P[k]}{\\sum_{k=1}^{N/2} P[k]}. $$\n     令 $p^\\star = 0.25$ 为此玩具设置中中等纹理信号的典型目标高频分数。真实感分数为\n     $$ \\mathrm{R}(x) = \\max\\left(0, \\, 1 - \\frac{|p_{\\mathrm{high}}(x) - p^\\star|}{p^\\star} \\right). $$\n\n4. 任务：\n   - 对于每种指定的噪声消融设置（幅度向量），生成 $x[n]$ 并计算 $\\mathrm{R}(x)$ 和 $\\mathrm{S}(x)$。\n   - 每个案例的输出是一对值 $[\\mathrm{R}(x), \\mathrm{S}(x)]$，每个值都精确四舍五入到 $4$ 位小数。\n\n5. 测试套件：\n   - 案例 A（无噪声）：$[a_0, a_1, a_2, a_3] = [0.0, 0.0, 0.0, 0.0]$。\n   - 案例 B（所有尺度均有强噪声）：$[a_0, a_1, a_2, a_3] = [0.6, 0.6, 0.6, 0.6]$。\n   - 案例 C（仅低分辨率）：$[a_0, a_1, a_2, a_3] = [0.6, 0.3, 0.0, 0.0]$。\n   - 案例 D（仅高分辨率）：$[a_0, a_1, a_2, a_3] = [0.0, 0.0, 0.3, 0.6]$。\n   - 案例 E（均衡温和）：$[a_0, a_1, a_2, a_3] = [0.2, 0.1, 0.1, 0.2]$。\n\n6. 要求的最终输出格式：\n   - 您的程序应生成单行输出，其中包含一个由五个双元素列表组成的列表，顺序为 A, B, C, D, E。每个内部列表包含该案例的两个四舍五入后的浮点分数 $[\\mathrm{R}(x), \\mathrm{S}(x)]$。外部列表和所有内部列表必须用方括号括起来，元素之间用逗号分隔，不含空格。例如：\n     $$ [[r_A,s_A],[r_B,s_B],[r_C,s_C],[r_D,s_D],[r_E,s_E]] $$\n     其中每个 $r_\\cdot$ 和 $s_\\cdot$ 都显示为精确的 $4$ 位小数。\n\n您的程序必须是一个完整的、可运行的脚本，使用固定的随机种子确定性地执行所有计算，并以指定格式精确打印一行输出。不涉及物理单位。正弦波中使用的角度以弧度为单位。任何百分比（如有）必须以上述定义的小数形式表示，而不是使用百分号。",
            "solution": "用户的请求是执行一个计算实验，模拟一个简化的逐分辨率噪声注入机制，这个概念受到了 StyleGAN 的启发。该任务要求通过将一个确定性的低频结构分量与不同分辨率下的随机噪声分量相结合，来生成一个一维信号。这些噪声分量的消融效应将使用两个自定义指标进行量化：结构稳定性分数和纹理真实感分数。\n\n该问题被确定为**有效**。这是一个基于数字信号处理和统计学原理的、定义明确且具有科学依据的计算练习。所有参数、方程和步骤都已明确定义，确保了唯一且可验证的解决方案。\n\n解决方案的步骤如下：\n\n1.  **初始化**：我们按照规定定义全局参数。信号长度为 $N = 512$。为了保证可复现性，随机数生成器使用种子 $0$ 进行初始化。四个噪声级别的步长为 $s_\\ell = [64, 32, 16, 8]$。纹理真实感分数的参数是高频截止索引 $k_c = 32$ 和目标高频功率分数 $p^\\star = 0.25$。\n\n2.  **结构信号生成**：根据给定公式合成基础结构信号 $s[n]$，它代表我们生成信号的低频内容：\n    $$\n    s[n] = 0.6 \\sin\\left(\\frac{4\\pi n}{N}\\right) + 0.4 \\sin\\left(\\frac{10\\pi n}{N}\\right)\n    $$\n    对于 $n = 0, \\dots, N-1$。该信号是确定性的，并作为测量结构完整性的基准。\n\n3.  **多分辨率噪声合成**：对于 $L=4$ 个分辨率级别中的每一个，都会生成一个相应的滤波后噪声信号 $w_\\ell[n]$。这个过程独立于测试案例，可以预先计算。对于每个级别 $\\ell$：\n    a. 从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取一个长度为 $N/s_\\ell$ 的低分辨率噪声向量 $u_\\ell$。\n    b. 通过因子为 $s_\\ell$ 的最近邻重复将此向量上采样至长度 $N$，生成 $v_\\ell[n]$。\n    c. 为了平滑块状的上采样噪声，将 $v_\\ell[n]$ 与一个长度为 $s_\\ell$ 的盒式滤波器 $h_\\ell$ 进行循环卷积。核函数 $h_\\ell$ 是一个平均滤波器，其前 $s_\\ell$ 个样本的值为 $h_\\ell[n] = 1/s_\\ell$，其余为零。循环卷积 $w_\\ell = v_\\ell \\ast h_\\ell$ 通过卷积定理，使用快速傅里叶变换（FFT）在频域中高效计算：$\\mathcal{F}\\{x \\ast h\\} = \\mathcal{F}\\{x\\} \\cdot \\mathcal{F}\\{h\\}$。\n    $$\n    w_\\ell[n] = \\mathcal{F}^{-1}\\left\\{ \\mathcal{F}\\{v_\\ell\\}[k] \\cdot \\mathcal{F}\\{h_\\ell\\}[k] \\right\\}\n    $$\n\n4.  **为测试案例生成信号**：对于由幅度向量 $[a_0, a_1, a_2, a_3]$ 定义的五个测试案例中的每一个，最终输出信号 $x[n]$ 通过结构信号和预先计算的噪声信号的加权和来合成：\n    $$\n    x[n] = s[n] + \\sum_{\\ell=0}^{3} a_\\ell \\, w_\\ell[n]\n    $$\n\n5.  **指标计算**：对每个生成的信号 $x[n]$，计算两个指标。\n    a. **结构稳定性 $\\mathrm{S}(x)$**：这是生成信号 $x[n]$ 与原始结构信号 $s[n]$ 之间的 Pearson 相关系数。它量化了在注入噪声后，原始低频结构被保留了多少。值为 $1$ 表示完美保留。\n    $$\n    \\mathrm{S}(x) = \\rho(x, s) = \\frac{\\sum_{n=0}^{N-1} (x[n]-\\bar{x})(s[n]-\\bar{s})}{\\sqrt{\\sum_{n=0}^{N-1} (x[n]-\\bar{x})^2}\\sqrt{\\sum_{n=0}^{N-1} (s[n]-\\bar{s})^2}}\n    $$\n    b. **纹理真实感 $\\mathrm{R}(x)$**：此指标量化了信号中高频内容（或“纹理”）相对于目标值的多少。首先，计算 $x[n]$ 的单边实数 FFT 以获得频域表示 $X[k]$。功率谱为 $P[k] = |X[k]|^2$。高频功率分数 $p_{\\mathrm{high}}(x)$ 是频率 $k \\ge k_c$ 中的功率与所有非直流频率（$k \\ge 1$）中总功率的比值。\n    $$\n    p_{\\mathrm{high}}(x) = \\frac{\\sum_{k=k_c}^{N/2} P[k]}{\\sum_{k=1}^{N/2} P[k]}\n    $$\n    真实感分数 $\\mathrm{R}(x)$ 衡量 $p_{\\mathrm{high}}(x)$ 与目标分数 $p^\\star$ 的接近程度，并缩放到 $0$ 到 $1$ 的范围内。\n    $$\n    \\mathrm{R}(x) = \\max\\left(0, \\, 1 - \\frac{|p_{\\mathrm{high}}(x) - p^\\star|}{p^\\star} \\right)\n    $$\n\n6.  **输出格式化**：每个测试案例计算出的分数 $[\\mathrm{R}(x), \\mathrm{S}(x)]$ 被四舍五入到 $4$ 位小数，并按要求格式化为单行的列表之列表。\n\n该实现将这些步骤整合到一个脚本中，该脚本确定性地计算所有测试案例的结果，并以指定格式打印它们。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes a simplified per-resolution noise injection mechanism\n    inspired by StyleGAN.\n    \"\"\"\n    # 1. Define constants and initialization\n    N = 512\n    strides = [64, 32, 16, 8]\n    L = len(strides)\n    seed = 0\n    rng = np.random.default_rng(seed)\n    # Metric parameters\n    p_star = 0.25\n    k_c = 32\n\n    # 2. Generate the base structural signal s[n]\n    n = np.arange(N)\n    s = 0.6 * np.sin(2 * np.pi * 2 * n / N) + 0.4 * np.sin(2 * np.pi * 5 * n / N)\n\n    # 3. Pre-compute the four filtered noise signals w_ell[n]\n    w_list = []\n    for s_ell in strides:\n        # a. Generate low-resolution Gaussian noise u_ell\n        noise_len = N // s_ell\n        u_ell = rng.standard_normal(size=noise_len)\n\n        # b. Upsample u_ell to v_ell using nearest-neighbor repetition\n        v_ell = np.repeat(u_ell, s_ell)\n\n        # c. Create box filter kernel h_ell and perform circular convolution\n        # using the FFT-based convolution theorem.\n        h_ell = np.zeros(N)\n        h_ell[:s_ell] = 1.0 / s_ell\n        \n        fft_v = np.fft.fft(v_ell)\n        fft_h = np.fft.fft(h_ell)\n        \n        # The result of IFFT on real-signal convolution should be real.\n        # We take .real to discard negligible imaginary parts from numerical error.\n        w_ell = np.fft.ifft(fft_v * fft_h).real\n        w_list.append(w_ell)\n    \n    # 4. Define the test suite\n    test_cases = {\n        'A': [0.0, 0.0, 0.0, 0.0],\n        'B': [0.6, 0.6, 0.6, 0.6],\n        'C': [0.6, 0.3, 0.0, 0.0],\n        'D': [0.0, 0.0, 0.3, 0.6],\n        'E': [0.2, 0.1, 0.1, 0.2]\n    }\n    \n    all_results = []\n    # Process cases in specified order A, B, C, D, E\n    for case_id in sorted(test_cases.keys()):\n        amplitudes = test_cases[case_id]\n        \n        # 5. Generate the final signal x[n] for the current case\n        noise_component = np.zeros(N)\n        for i in range(L):\n            noise_component += amplitudes[i] * w_list[i]\n        x = s + noise_component\n\n        # 6. Compute Structural Stability S(x)\n        # np.corrcoef calculates the Pearson correlation coefficient matrix.\n        # The off-diagonal element [0, 1] is rho(x, s).\n        s_score = np.corrcoef(x, s)[0, 1]\n\n        # 7. Compute Texture Realism R(x)\n        # Use one-sided real FFT for efficiency and correctness.\n        X_fft = np.fft.rfft(x)  # Produces N/2 + 1 complex coefficients\n        \n        # Power spectrum P[k] = |X[k]|^2\n        P = np.abs(X_fft)**2\n        \n        # Calculate high-frequency power and total non-DC power\n        # P[k_c:] sums power from frequency k_c to N/2\n        # P[1:] sums power from frequency 1 to N/2\n        sum_high = np.sum(P[k_c:])\n        sum_total_no_dc = np.sum(P[1:])\n        \n        # Avoid division by zero, though unlikely in this problem setup\n        if sum_total_no_dc == 0:\n            p_high = 0.0\n        else:\n            p_high = sum_high / sum_total_no_dc\n            \n        r_score = max(0.0, 1.0 - abs(p_high - p_star) / p_star)\n\n        # 8. Store the rounded results for the current case\n        all_results.append([round(r_score, 4), round(s_score, 4)])\n\n    # 9. Format and print the final output string exactly as specified\n    output_str_parts = []\n    for r, s in all_results:\n        output_str_parts.append(f\"[{r:.4f},{s:.4f}]\")\n    \n    final_output = \"[\" + \",\".join(output_str_parts) + \"]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "先进的 GAN 模型通常包含数亿个参数，计算成本极其高昂。深度可分离卷积（Depthwise-Separable Convolution）是一种广泛用于优化网络结构、减少参数量和计算量的有效技术。同时，为了保证深度网络中信号的稳定传播，StyleGAN2 引入了权重解调（Weight Demodulation）技术来动态标准化卷积权重。\n\n本练习 () 提供了一个量化框架，用于分析这些架构选择所带来的多维度权衡。你不仅将计算深度可分离卷积带来的速度提升和参数缩减，还将探索一个更深层次的概念：架构变化与权重解调等技术如何共同影响网络中的信号方差传播，而这对于训练超深度生成模型至关重要 。",
            "id": "3098241",
            "problem": "要求您在一个受基于风格的生成器架构 (StyleGAN) 和大规模生成对抗网络 (BigGAN) 启发的生成器模块中，对标准二维卷积和深度可分离卷积进行形式化的、有原则的比较和实现。您将从基本定义出发，推导计算成本、参数数量以及一个由稳定性驱动的质量代理指标的表达式，该指标能捕捉速度与表示能力之间的权衡，以及对逐样本权重解调的敏感度。然后，您将编写一个程序，在给定的测试套件上评估这些表达式，并将结果打印在单行中。\n\n从以下基本依据开始：\n\n- 二维卷积层的定义：对于一个空间尺寸为 $H \\times W$、拥有 $C_{\\text{in}}$ 个通道的输入特征图，以及一个拥有 $C_{\\text{out}}$ 个通道、步幅为 $1$、卷积核大小为 $K \\times K$ 的输出，其乘加运算次数（浮点运算次数，FLOPs）和参数数量计算如下。\n- 深度可分离卷积将卷积分解为一个带有逐通道卷积核的深度空间卷积，随后是一个逐点 $1 \\times 1$ 卷积。其 FLOPs 和参数是这两个部分的总和。\n- 在具有独立同分布 (i.i.d.) 零均值权重和输入的线性系统中，方差的传播：如果输入 $x$ 的方差为 $\\sigma_x^2$，权重 $w$ 的方差为 $\\sigma_w^2$，那么激活前的输出方差与权重平方和乘以输入方差成比例。\n- 逐样本权重解调（如在基于风格的生成器架构 (StyleGAN) 中使用）：将解调视为一种以每个输出通道单位增益为目标的归一化，这可以近似为将有效方差增益约束为 $1$。\n\n基于这些基础，推导并实现以下量。\n\n- 标准卷积：\n  - FLOPs: $F_{\\text{std}} = H \\cdot W \\cdot C_{\\text{out}} \\cdot (C_{\\text{in}} \\cdot K \\cdot K)$。\n  - 参数: $P_{\\text{std}} = C_{\\text{out}} \\cdot C_{\\text{in}} \\cdot K \\cdot K$。\n  - 无解调时的方差增益: $g_{\\text{std}} = \\sigma_{w,\\text{full}}^2 \\cdot (K \\cdot K \\cdot C_{\\text{in}})$。\n  - 有解调时的有效增益: $g_{\\text{std,demod}} = 1$。\n- 深度可分离卷积：\n  - FLOPs: $F_{\\text{sep}} = H \\cdot W \\cdot (C_{\\text{in}} \\cdot K \\cdot K + C_{\\text{in}} \\cdot C_{\\text{out}})$。\n  - 参数: $P_{\\text{sep}} = C_{\\text{in}} \\cdot K \\cdot K + C_{\\text{in}} \\cdot C_{\\text{out}}$。\n  - 无解调时的方差增益: $g_{\\text{sep}} = (\\sigma_{w,\\text{dw}}^2 \\cdot K \\cdot K) \\cdot (\\sigma_{w,\\text{pw}}^2 \\cdot C_{\\text{in}})$。\n  - 有解调时的有效增益: $g_{\\text{sep,demod}} = 1$。\n\n定义一个由稳定性驱动的质量代理指标，该指标惩罚对单位方差的偏离和降低的跨通道混合能力：\n- 定义方差保持因子\n  $$p_{\\text{var}}(g) = \\exp\\left( -\\frac{(\\log g)^2}{\\tau} \\right),$$\n  其中 $\\tau$ 是一个正的容差常数，您必须将其设置为 $\\tau = 0.5$。\n- 定义深度可分离卷积的混合能力代理指标为\n  $$m_{\\text{sep}} = \\min\\left(1, \\frac{P_{\\text{sep}}}{P_{\\text{std}}}\\right),$$\n  以及标准卷积的为\n  $$m_{\\text{std}} = 1.$$\n- 定义质量代理指标\n  $$Q_{\\text{std}} = m_{\\text{std}} \\cdot p_{\\text{var}}(g_{\\text{std,eff}}), \\quad Q_{\\text{sep}} = m_{\\text{sep}} \\cdot p_{\\text{var}}(g_{\\text{sep,eff}}),$$\n  其中，如果解调开启，$g_{\\text{std,eff}}$ 和 $g_{\\text{sep,eff}}$ 等于 $1$；如果解调关闭，则分别等于 $g_{\\text{std}}$ 和 $g_{\\text{sep}}$。在所有情况下，假设输入方差 $\\sigma_x^2 = 1$。\n\n对每个测试用例，计算并返回以下四个量：\n- 加速因子：$$S = \\frac{F_{\\text{std}}}{F_{\\text{sep}}}.$$\n- 参数比率：$$R = \\frac{P_{\\text{sep}}}{P_{\\text{std}}}.$$\n- 在测试用例的解调标志 $d \\in \\{0,1\\}$ 下的质量比率：$$Q = \\frac{Q_{\\text{sep}}(d)}{Q_{\\text{std}}(d)}.$$\n- 解调敏感度：$$\\Delta Q = Q_{\\text{sep}}(1) - Q_{\\text{sep}}(0).$$\n\n实现一个程序，为以下测试套件评估上述内容。每个元组是 $(H, W, C_{\\text{in}}, C_{\\text{out}}, K, \\sigma_{w,\\text{full}}^2, \\sigma_{w,\\text{dw}}^2, \\sigma_{w,\\text{pw}}^2, \\sigma_x^2, d)$：\n- 案例1（平衡方差）：$(64, 64, 64, 128, 3, \\frac{1}{9 \\cdot 64}, \\frac{1}{9}, \\frac{1}{64}, 1, 1)$。\n- 案例2（无解调的类 He 缩放）：$(64, 64, 64, 128, 3, \\frac{2}{9 \\cdot 64}, \\frac{2}{9}, \\frac{2}{64}, 1, 0)$。\n- 案例3（$C_{\\text{in}} = 1$ 的边缘情况）：$(64, 64, 1, 64, 3, \\frac{1}{9 \\cdot 1}, \\frac{1}{9}, 1, 1, 1)$。\n- 案例4（瓶颈，无解调）：$(32, 32, 256, 64, 3, \\frac{1}{9 \\cdot 256}, \\frac{1}{9}, \\frac{2}{256}, 1, 0)$。\n- 案例5（小空间尺寸，大通道数，有解调）：$(8, 8, 512, 512, 3, \\frac{2}{9 \\cdot 512}, \\frac{2}{9}, \\frac{2}{512}, 1, 1)$。\n\n最终输出格式要求：\n- 您的程序应生成单行输出，其中包含结果，格式为逗号分隔的列表的列表，每个内部列表按相应测试用例的 $[S, R, Q, \\Delta Q]$ 顺序排列。例如，格式应如 $[[r_{11}, r_{12}, r_{13}, r_{14}], [r_{21}, r_{22}, r_{23}, r_{24}], \\dots]$。值必须是实数。不应打印任何额外文本。\n\n角度单位不适用。没有物理单位。将所有数值结果以标准十进制形式的实数表示。如有需要，通过安全地处理极小正参数的 $\\log$ 来确保数值稳定性。",
            "solution": "问题陈述已经过严格验证，被认为是科学上合理、定义明确且客观的。所有定义、参数和约束都已明确无误地提供，从而可以得出一个唯一且可验证的解。其基本原理牢固地植根于深度学习的既定理论，特别是关于卷积神经网络的计算分析和方差传播动态，这些都是现代生成模型架构的核心。\n\n该任务是执行标准二维卷积和深度可分离卷积的比较分析。该分析通过为一组给定的层配置推导和计算四个关键指标来实现形式化：加速因子 $S$、参数比率 $R$、一个由稳定性驱动的质量比率 $Q$ 以及一个解调敏感度指标 $\\Delta Q$。\n\n推导从所提供的基本定义开始。设输入特征图的空间维度为 $H \\times W$，有 $C_{\\text{in}}$ 个输入通道。输出有 $C_{\\text{out}}$ 个通道，卷积使用大小为 $K \\times K$、步幅为 $1$ 的卷积核。\n\n**1. 加速因子 ($S$) 和参数比率 ($R$)**\n\n计算成本（以浮点运算次数 FLOPs 衡量）和参数数量是为两种卷积类型定义的。\n\n对于标准卷积：\n- FLOPs: $F_{\\text{std}} = H \\cdot W \\cdot C_{\\text{out}} \\cdot (C_{\\text{in}} \\cdot K \\cdot K)$\n- 参数: $P_{\\text{std}} = C_{\\text{out}} \\cdot C_{\\text{in}} \\cdot K \\cdot K$\n\n对于深度可分离卷积：\n- FLOPs: $F_{\\text{sep}} = F_{\\text{depthwise}} + F_{\\text{pointwise}} = H \\cdot W \\cdot C_{\\text{in}} \\cdot K \\cdot K + H \\cdot W \\cdot C_{\\text{out}} \\cdot C_{\\text{in}} = H \\cdot W \\cdot C_{\\text{in}} \\cdot (K^2 + C_{\\text{out}})$\n- 参数: $P_{\\text{sep}} = P_{\\text{depthwise}} + P_{\\text{pointwise}} = C_{\\text{in}} \\cdot K \\cdot K + C_{\\text{in}} \\cdot C_{\\text{out}} = C_{\\text{in}} \\cdot (K^2 + C_{\\text{out}})$\n\n加速因子 $S$ 是 FLOPs 的比率：\n$$S = \\frac{F_{\\text{std}}}{F_{\\text{sep}}} = \\frac{H \\cdot W \\cdot C_{\\text{out}} \\cdot C_{\\text{in}} \\cdot K^2}{H \\cdot W \\cdot C_{\\text{in}} \\cdot (K^2 + C_{\\text{out}})} = \\frac{C_{\\text{out}} K^2}{K^2 + C_{\\text{out}}}$$\n\n参数比率 $R$ 是参数数量的比率：\n$$R = \\frac{P_{\\text{sep}}}{P_{\\text{std}}} = \\frac{C_{\\text{in}} \\cdot (K^2 + C_{\\text{out}})}{C_{\\text{out}} \\cdot C_{\\text{in}} \\cdot K^2} = \\frac{K^2 + C_{\\text{out}}}{C_{\\text{out}} K^2} = \\frac{1}{C_{\\text{out}}} + \\frac{1}{K^2}$$\n这些特定定义的一个有趣结果是 $R = 1/S$。这突显了在这个简化模型中，计算成本和参数数量之间的直接权衡关系。\n\n**2. 质量代理指标 ($Q$) 及其组成部分**\n\n质量代理指标 $Q$ 旨在捕捉表示能力和信号稳定性方面的权衡。它是可分离卷积和标准卷积的代理指标之比，$Q = Q_{\\text{sep}} / Q_{\\text{std}}$。每个单独的代理指标是混合能力项 $m$ 和方差保持因子 $p_{\\text{var}}$ 的乘积。\n\n混合能力代理指标定义为 $m_{\\text{std}} = 1$ 和 $m_{\\text{sep}} = \\min(1, R)$。这将标准卷积具有完全混合能力，而可分离卷积的能力受其减少的参数数量（此处由比率 $R$ 近似）限制这一概念形式化。\n\n方差保持因子 $p_{\\text{var}}(g) = \\exp\\left( -(\\log g)^2 / \\tau \\right)$（容差 $\\tau=0.5$）惩罚方差增益 $g$ 与理想值 $1$ 的任何偏离。增益 $g$ 本身取决于权重解调是激活（解调标志 $d=1$）还是非激活（$d=0$）。\n\n如果解调开启（$d=1$），根据受 StyleGAN 启发的问题设定，两种卷积类型的有效增益都被归一化为1：\n- $g_{\\text{std,eff}} = 1$\n- $g_{\\text{sep,eff}} = 1$\n在这种情况下，$p_{\\text{var}}(1) = \\exp(0) = 1$。质量代理指标变为 $Q_{\\text{std}}(1) = m_{\\text{std}} \\cdot 1 = 1$ 和 $Q_{\\text{sep}}(1) = m_{\\text{sep}} \\cdot 1 = R$。于是质量比率为 $Q(1) = Q_{\\text{sep}}(1) / Q_{\\text{std}}(1) = R$。\n\n如果解调关闭（$d=0$），增益由权重方差和层的扇入决定：\n- $g_{\\text{std,eff}} = g_{\\text{std}} = \\sigma_{w,\\text{full}}^2 \\cdot (K^2 \\cdot C_{\\text{in}})$\n- $g_{\\text{sep,eff}} = g_{\\text{sep}} = (\\sigma_{w,\\text{dw}}^2 \\cdot K^2) \\cdot (\\sigma_{w,\\text{pw}}^2 \\cdot C_{\\text{in}})$\n于是质量比率为 $Q(0) = \\frac{m_{\\text{sep}} \\cdot p_{\\text{var}}(g_{\\text{sep}})}{m_{\\text{std}} \\cdot p_{\\text{var}}(g_{\\text{std}})} = R \\cdot \\frac{\\exp(-(\\log g_{\\text{sep}})^2/\\tau)}{\\exp(-(\\log g_{\\text{std}})^2/\\tau)}$。\n\n**3. 解调敏感度 ($\\Delta Q$)**\n\n该指标量化了启用解调对可分离卷积质量代理指标的影响：\n$$\\Delta Q = Q_{\\text{sep}}(1) - Q_{\\text{sep}}(0)$$\n代入推导出的表达式：\n$$\\Delta Q = (m_{\\text{sep}} \\cdot p_{\\text{var}}(1)) - (m_{\\text{sep}} \\cdot p_{\\text{var}}(g_{\\text{sep}})) = m_{\\text{sep}} \\left(1 - p_{\\text{var}}(g_{\\text{sep}})\\right)$$\n代入 $m_{\\text{sep}} = R$:\n$$\\Delta Q = R \\cdot \\left(1 - \\exp\\left(-\\frac{(\\log g_{\\text{sep}})^2}{\\tau}\\right)\\right)$$\n这个值表示由于解调提供的方差稳定化，可分离层的质量代理指标改善（或改变）了多少。更大的正值表示从解调中获益更大。\n\n这些推导出的公式将在提供的计算程序中系统地应用于每个测试用例。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Validates and solves the problem of comparing standard and depthwise-separable\n    convolutions using derived metrics for computational cost, parameter count,\n    and a stability-motivated quality proxy.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple is (H, W, Cin, Cout, K, var_w_full, var_w_dw, var_w_pw, var_x, d)\n    test_cases = [\n        (64, 64, 64, 128, 3, 1/(9*64), 1/9, 1/64, 1, 1),\n        (64, 64, 64, 128, 3, 2/(9*64), 2/9, 2/64, 1, 0),\n        (64, 64, 1, 64, 3, 1/(9*1), 1/9, 1, 1, 1),\n        (32, 32, 256, 64, 3, 1/(9*256), 1/9, 2/256, 1, 0),\n        (8, 8, 512, 512, 3, 2/(9*512), 2/9, 2/512, 1, 1),\n    ]\n\n    results = []\n    tau = 0.5\n\n    def p_var(g, tau_val):\n        \"\"\"Calculates the variance preservation factor.\"\"\"\n        # The problem constraints ensure g > 0.\n        return np.exp(-(np.log(g)**2) / tau_val)\n\n    for case in test_cases:\n        H, W, Cin, Cout, K, var_w_full, var_w_dw, var_w_pw, _, d = case\n        \n        K2 = K * K\n\n        # 1. Speed-up Factor (S) and Parameter Ratio (R)\n        F_std = H * W * Cout * (Cin * K2)\n        P_std = Cout * Cin * K2\n\n        F_sep = H * W * (Cin * K2 + Cin * Cout)\n        P_sep = Cin * K2 + Cin * Cout\n        \n        # Taking ratios of simplified formulas to avoid large intermediate numbers\n        S = (Cout * K2) / (K2 + Cout)\n        R = P_sep / P_std\n\n        # 2. Quality Ratio (Q)\n        \n        # Mixing capacity proxies\n        m_std = 1.0\n        m_sep = min(1.0, R)\n\n        # Gains for demodulation OFF (d=0) case\n        g_std_off = var_w_full * (K2 * Cin)\n        g_sep_off = (var_w_dw * K2) * (var_w_pw * Cin)\n        \n        # Quality proxies for demodulation OFF (d=0)\n        Q_std_0 = m_std * p_var(g_std_off, tau)\n        Q_sep_0 = m_sep * p_var(g_sep_off, tau)\n        \n        # Quality proxies for demodulation ON (d=1)\n        # Gains are 1, so p_var(1) = 1\n        Q_std_1 = m_std * 1.0\n        Q_sep_1 = m_sep * 1.0\n        \n        # Calculate the Quality Ratio Q for the specific demodulation flag d\n        if d == 1:\n            Q = Q_sep_1 / Q_std_1\n        else: # d == 0\n            # Guard against division by zero, though Q_std_0 should be positive.\n            if Q_std_0 > 0:\n                Q = Q_sep_0 / Q_std_0\n            else:\n                # Handle this unlikely edge case\n                Q = float('inf') if Q_sep_0 > 0 else 0.0\n\n        # 3. Demodulation Sensitivity (delta_Q)\n        delta_Q = Q_sep_1 - Q_sep_0\n        \n        results.append([S, R, Q, delta_Q])\n\n    # Final print statement in the exact required format.\n    # We construct the string manually to avoid spaces and ensure the format is exact.\n    inner_results_str = [f\"[{','.join(map(str, res_list))}]\" for res_list in results]\n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nsolve()\n\n```"
        }
    ]
}