{
    "hands_on_practices": [
        {
            "introduction": "对比散度（Contrastive Divergence, CD）是训练RBMs的标准算法，但它是一种近似方法。此近似的质量由吉布斯采样步数 $k$ 控制。此练习  提供了一种具体的、经验性的方式来探索训练速度（较小的 $k$）与模型保真度（较大的 $k$）之间的权衡。通过实施和比较 CD-1 和 CD-10，你将对这个关键超参数如何影响RBM学习和表示复杂数据分布的能力有一个深刻的实践理解。",
            "id": "3170448",
            "problem": "您的任务是设计并实现一个完整的、可运行的程序，该程序在一个基础模式已知的合成数据集上，对使用 $k=1$ 步（CD-1）和 $k=10$ 步（CD-10）的对比散度算法来训练受限玻尔兹曼机（RBM）进行实证比较。比较标准是通过将样本分配到已知模式的簇来衡量的模式覆盖率。\n\n从以下基本概念开始：\n\n- 受限玻尔兹曼机（RBM）通过能量函数 $E(v,h)$ 和配分函数 $Z$ 定义了二元可见变量 $v \\in \\{0,1\\}^D$ 和二元隐藏变量 $h \\in \\{0,1\\}^H$ 上的联合分布。联合概率为 $p(v,h) = \\frac{1}{Z} \\exp(-E(v,h))$。\n- 二元-二元 RBM 的能量函数由权重矩阵、可见偏置和隐藏偏置参数化。给定隐藏单元，可见单元条件独立；给定可见单元，隐藏单元条件独立。\n- 具有 $k$ 个吉布斯步的对比散度（CD-$k$）算法通过从数据点开始一个短吉布斯链并运行 $k$ 个交替条件采样步骤来近似对数似然的梯度。\n\n问题要求：\n\n1. 构建一个具有已知二元基础模式集的合成数据集。设 $D=8$ 且有 $M=4$ 个基础模式 $m_1, m_2, m_3, m_4 \\in \\{0,1\\}^8$ 明确定义如下：\n   - $m_1 = [1,1,0,0,1,0,1,0]$\n   - $m_2 = [0,1,1,0,0,1,0,1]$\n   - $m_3 = [1,0,1,1,0,0,1,0]$\n   - $m_4 = [0,0,0,1,1,1,0,0]$\n   对于每个模式 $m_i$，通过以概率 $p_{\\text{flip}} \\in [0,1]$ 独立翻转每一位来生成 $n_i$ 个样本，并聚合所有模式的样本。这样就得到了一个模式计数已知的 $(n_1,n_2,n_3,n_4)$ 数据集。\n\n2. 在相同的数据集上，使用相同的初始参数，训练两个具有二元可见单元和二元隐藏单元的 RBM，它们唯一的不同在于对比散度步数 $k$：\n   - 使用 CD-1 的 RBM，\n   - 使用 CD-10 的 RBM。\n   使用固定数量的隐藏单元 $H=6$、学习率 $\\alpha$、小批量训练和固定的轮次数。确保两个 RBM 的随机初始化相同，以隔离 $k$ 的影响。\n\n3. 训练后，通过从一个随机的二元可见向量开始运行吉布斯链，从每个训练好的 RBM 中抽取样本。对于每个生成的样本，根据最小汉明距离将其分配到最近的基础模式。使用这些分配，定义模式覆盖率指标如下：\n   - 设 $\\mathcal{M} = \\{m_1, m_2, m_3, m_4\\}$ 为基础模式的集合。\n   - 对于一组生成的样本 $\\{v^{(s)}\\}_{s=1}^S$，通过 $i^\\star(s) = \\arg\\min_{i \\in \\{1,\\dots,M\\}} d_H(v^{(s)}, m_i)$ 定义每个样本的分配模式索引，其中 $d_H$ 是汉明距离。\n   - 将覆盖率定义为至少有一个样本被分配到的模式所占的比例：$\\text{coverage} = \\frac{|\\{i \\in \\{1,\\dots,M\\} : \\exists s \\text{ with } i^\\star(s) = i\\}|}{M}$。\n   将覆盖率表示为 $[0,1]$ 范围内的小数。\n\n4. 从 RBM 的能量定义和条件独立结构出发，实现完整的训练和评估流程。基于数据驱动和模型驱动的相关性及偏置之间的差异，推导并实现 CD-$k$ 训练更新。\n\n5. 使用以下测试套件，其中指定了 $(n_1,n_2,n_3,n_4)$ 和 $p_{\\text{flip}}$：\n   - 案例 A (平衡，低噪声): $(250,250,250,250)$，$p_{\\text{flip}} = 0.05$。\n   - 案例 B (平衡，较高噪声): $(250,250,250,250)$，$p_{\\text{flip}} = 0.25$。\n   - 案例 C (不平衡，低噪声): $(400,100,75,25)$，$p_{\\text{flip}} = 0.05$。\n\n6. 对于每种案例，使用相同的配置训练两个 RBM：$H=6$，学习率 $\\alpha = 0.1$，小批量大小为 $50$，训练 $100$ 轮，并从每个训练好的模型中采样 $S=200$ 个点用于分配，每个样本使用 $100$ 个吉布斯步。\n\n7. 最终输出规范：您的程序应生成单行输出，包含三个案例的结果，形式为逗号分隔的配对列表，其中每对列出 $[\\text{coverage}_{\\text{CD-1}}, \\text{coverage}_{\\text{CD-10}}]$。将每个覆盖率值四舍五入到三位小数，并严格按照以下格式输出：\n   - $[[c_{A,1},c_{A,10}],[c_{B,1},c_{B,10}],[c_{C,1},c_{C,10}]]$\n   其中 $c_{\\cdot,\\cdot} \\in [0,1]$ 是小数值。\n\n此问题不涉及物理单位，也不存在角度。所有答案均为 $[0,1]$ 范围内并四舍五入到三位小数的小数。通过使用所述的 RBM 架构、指定的数据集生成过程和定义的评估指标，确保科学真实性。",
            "solution": "当前任务要求对使用 $k=1$ 和 $k=10$ 吉布斯采样步数的对比散度算法训练受限玻尔兹曼机（RBM）进行严格的实证比较。解决方案围绕 RBM 的基本原理构建，从基于能量的模型定义到通过 CD-$k$ 进行梯度近似。\n\n**1. 受限玻尔兹曼机：理论框架**\n\nRBM 是一种基于能量的模型，用于描述二元可见单元 $v \\in \\{0, 1\\}^D$ 和二元隐藏单元 $h \\in \\{0, 1\\}^H$ 上的联合概率分布。该模型由参数 $\\theta = \\{W, b, c\\}$ 定义，其中 $W \\in \\mathbb{R}^{H \\times D}$ 是连接隐藏单元和可见单元的权重矩阵，$b \\in \\mathbb{R}^D$ 是可见单元偏置向量，$c \\in \\mathbb{R}^H$ 是隐藏单元偏置向量。\n\n能量函数 $E(v, h; \\theta)$ 定义了系统的状态：\n$$\nE(v, h) = - \\sum_{i=1}^D b_i v_i - \\sum_{j=1}^H c_j h_j - \\sum_{i=1}^D \\sum_{j=1}^H h_j W_{ji} v_i\n$$\n用矩阵表示法，即为 $E(v, h) = -b^T v - c^T h - h^T W v$。\n\n联合概率分布由玻尔兹曼分布给出：\n$$\np(v, h) = \\frac{1}{Z} e^{-E(v, h)}\n$$\n其中 $Z = \\sum_{v', h'} e^{-E(v', h')}$ 是配分函数，一个难以计算的归一化常数。可见向量的边缘概率为 $p(v) = \\frac{1}{Z} \\sum_h e^{-E(v, h)}$。\n\nRBM 的一个关键特性是，在给定另一层状态的情况下，同一层内的单元是条件独立的。这使得高效的块吉布斯采样成为可能。条件概率为：\n$$\np(h_j=1 | v) = \\sigma\\left(c_j + \\sum_{i=1}^D W_{ji} v_i\\right) = \\sigma(c_j + (Wv)_j)\n$$\n$$\np(v_i=1 | h) = \\sigma\\left(b_i + \\sum_{j=1}^H h_j W_{ji}\\right) = \\sigma(b_i + (W^T h)_i)\n$$\n其中 $\\sigma(x) = (1 + e^{-x})^{-1}$ 是 logistic sigmoid 函数。\n\n**2. 通过对比散度（CD-$k$）进行训练**\n\n训练 RBM 涉及调整 $\\theta$ 以最大化训练数据 $\\mathcal{D} = \\{v^{(n)}\\}$ 的对数似然。对于单个数据点 $v$，对数似然的梯度是：\n$$\n\\frac{\\partial \\log p(v)}{\\partial \\theta} = \\mathbb{E}_{h \\sim p(h|v)}\\left[-\\frac{\\partial E(v,h)}{\\partial \\theta}\\right] - \\mathbb{E}_{v',h' \\sim p(v',h')}\\left[-\\frac{\\partial E(v',h')}{\\partial \\theta}\\right]\n$$\n第一项，即“正相”（positive phase），是给定数据下对隐藏状态的期望。第二项，即“负相”（negative phase），是对整个模型分布的期望。由于 $Z$ 难以计算，负相的计算在计算上是不可行的。\n\n具有 $k$ 步的对比散度（CD-$k$）算法近似这个梯度。它用从数据向量初始化、长度为 $k$ 步的短吉布斯链获得的样本期望来代替模型期望。对于给定的数据向量 $v^{(0)}$：\n1.  **正相统计**: 基于数据计算相关性的期望。对于权重，这是 $v^{(0)} p(h|v^{(0)})^T$。\n2.  **负相生成**: 运行一个 $k$ 步吉布斯链，从 $h_s^{(0)} \\sim p(h|v^{(0)})$ 开始：\n    $$\n    v^{(1)} \\sim p(v|h_s^{(0)}), h_s^{(1)} \\sim p(h|v^{(1)}), \\dots, v^{(k)} \\sim p(v|h_s^{(k-1)})\n    $$\n3.  **负相统计**: 基于最终的“重构” $v^{(k)}$ 计算相关性，得到 $v^{(k)} p(h|v^{(k)})^T$。\n\n一个小批量的参数更新由正相和负相统计量之间的平均差异导出，并由学习率 $\\alpha$ 缩放：\n$$\n\\Delta W \\propto \\alpha \\left( \\langle v^{(0)} p(h|v^{(0)})^T \\rangle_{\\text{batch}} - \\langle v^{(k)} p(h|v^{(k)})^T \\rangle_{\\text{batch}} \\right)\n$$\n$$\n\\Delta b \\propto \\alpha \\left( \\langle v^{(0)} \\rangle_{\\text{batch}} - \\langle v^{(k)} \\rangle_{\\text{batch}} \\right)\n$$\n$$\n\\Delta c \\propto \\alpha \\left( \\langle p(h|v^{(0)}) \\rangle_{\\text{batch}} - \\langle p(h|v^{(k)}) \\rangle_{\\text{batch}} \\right)\n$$\n本实验比较了 $k=1$（一种粗略但快速的近似）和 $k=10$（一种更准确但较慢的近似）。预期更大的 $k$ 会让吉布斯链从初始数据点移动得更远，更接近模型的平稳分布，从而产生更好的梯度估计。这通常会导致模型学习到数据分布的更好表示，包括更有效地捕捉多个模式。\n\n**3. 实验设计与实现**\n\n实现严格遵循问题规范。\n-   **数据集生成**: 一个函数通过取 $M=4$ 个基础模式，并以概率 $p_{\\text{flip}}$ 翻转位来为每个模式 $m_i$ 创建 $n_i$ 个噪声版本，从而生成合成数据。\n-   **RBM 类**: 一个 RBM 类封装了参数（$W, b, c$）以及采样（$p(h|v)$, $p(v|h)$）和训练（CD-$k$ 更新）的方法。为确保公平比较，CD-1 和 CD-10 的 RBM 都使用相同的随机种子以相同的参数进行初始化。\n-   **训练流程**: 一个训练函数迭代固定的轮次数，以小批量方式处理数据，并为指定的 $k$ 应用 CD-$k$ 更新规则。数据洗牌也使用种子以确保在给定测试案例中两次训练运行相同。\n-   **评估**: 训练后，通过从随机初始向量开始运行长吉布斯链（$100$ 步），从每个 RBM 生成样本。每个生成的样本使用汉明距离分配到最近的基础模式。模式覆盖率是此问题的一个实用指标，用于衡量模型复现数据中各种潜在模式的能力。一个学习了所有模式的 RBM，其生成的样本将落在所有四个基础模式附近，从而获得高覆盖率分数。\n\n实验针对三种不同情况进行：平衡低噪声数据集、平衡高噪声数据集和不平衡数据集。这测试了算法对噪声和数据不平衡的鲁棒性。预期 CD-10 会表现出更优的模式覆盖率，尤其是在更具挑战性的不平衡案例中，因为 CD-1 的有偏梯度可能导致模型仅关注最频繁的模式。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\n# Main class for the Restricted Boltzmann Machine\nclass RBM:\n    \"\"\"\n    A class for a binary-binary Restricted Boltzmann Machine (RBM).\n    \"\"\"\n\n    def __init__(self, n_visible, n_hidden, seed=None):\n        \"\"\"\n        Initializes the RBM with random weights and zero biases.\n        \n        Args:\n            n_visible (int): Number of visible units (D).\n            n_hidden (int): Number of hidden units (H).\n            seed (int, optional): Seed for the random number generator for reproducibility.\n        \"\"\"\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        self.rng = np.random.default_rng(seed)\n\n        # Initialize parameters\n        # Weights are initialized from a normal distribution with small variance\n        self.W = self.rng.normal(0, 0.01, (self.n_hidden, self.n_visible))\n        # Biases are initialized to zero\n        self.b = np.zeros(self.n_visible)\n        self.c = np.zeros(self.n_hidden)\n\n    def _propup(self, v):\n        \"\"\"Calculates hidden layer activation probabilities given visible layer state.\"\"\"\n        return sigmoid(self.c + v @ self.W.T)\n\n    def _propdown(self, h):\n        \"\"\"Calculates visible layer activation probabilities given hidden layer state.\"\"\"\n        return sigmoid(self.b + h @ self.W)\n\n    def sample_h_given_v(self, v):\n        \"\"\"Samples hidden layer states given visible layer states.\"\"\"\n        h_probs = self._propup(v)\n        return self.rng.binomial(1, h_probs)\n\n    def sample_v_given_h(self, h):\n        \"\"\"Samples visible layer states given hidden layer states.\"\"\"\n        v_probs = self._propdown(h)\n        return self.rng.binomial(1, v_probs)\n\n    def update_params(self, v_batch, k, learning_rate):\n        \"\"\"\n        Performs a single parameter update using k-step Contrastive Divergence (CD-k).\n        \n        Args:\n            v_batch (np.ndarray): A mini-batch of data samples.\n            k (int): The number of Gibbs sampling steps.\n            learning_rate (float): The learning rate for the update.\n        \"\"\"\n        batch_size = v_batch.shape[0]\n        \n        # --- Positive Phase ---\n        v0 = v_batch\n        ph0_probs = self._propup(v0)\n        \n        # --- Negative Phase ---\n        vk = v0\n        for _ in range(k):\n            hk_samples = self.sample_h_given_v(vk)\n            vk = self.sample_v_given_h(hk_samples)\n        \n        phk_probs = self._propup(vk)\n\n        # --- Calculate Gradients ---\n        # Note: We use probabilities for the hidden units for a lower variance gradient\n        grad_W = (ph0_probs.T @ v0 - phk_probs.T @ vk) / batch_size\n        grad_b = np.mean(v0 - vk, axis=0)\n        grad_c = np.mean(ph0_probs - phk_probs, axis=0)\n\n        # --- Update Parameters ---\n        self.W += learning_rate * grad_W\n        self.b += learning_rate * grad_b\n        self.c += learning_rate * grad_c\n\n    def sample(self, n_samples, n_gibbs_steps, seed=None):\n        \"\"\"\n        Generates samples from the RBM by running a Gibbs chain.\n        \"\"\"\n        eval_rng = np.random.default_rng(seed)\n        v = eval_rng.integers(0, 2, size=(n_samples, self.n_visible))\n        for _ in range(n_gibbs_steps):\n            h = self.sample_h_given_v(v)\n            v = self.sample_v_given_h(h)\n        return v\n\ndef generate_dataset(modes, mode_counts, p_flip, seed=None):\n    \"\"\"\n    Generates a synthetic dataset from base modes by flipping bits.\n    \"\"\"\n    data_rng = np.random.default_rng(seed)\n    dataset = []\n    for mode, count in zip(modes, mode_counts):\n        for _ in range(count):\n            noise = data_rng.choice([0, 1], size=mode.shape, p=[1 - p_flip, p_flip])\n            sample = np.bitwise_xor(mode, noise)\n            dataset.append(sample)\n    \n    dataset = np.array(dataset)\n    data_rng.shuffle(dataset)\n    return dataset\n\ndef train_rbm(rbm, data, epochs, batch_size, k, learning_rate, seed=None):\n    \"\"\"\n    Trains an RBM on the provided data.\n    \"\"\"\n    train_rng = np.random.default_rng(seed)\n    n_samples = data.shape[0]\n    \n    for epoch in range(epochs):\n        indices = np.arange(n_samples)\n        train_rng.shuffle(indices)\n        \n        for i in range(0, n_samples, batch_size):\n            batch_indices = indices[i:i + batch_size]\n            v_batch = data[batch_indices]\n            rbm.update_params(v_batch, k, learning_rate)\n\ndef calculate_coverage(samples, modes):\n    \"\"\"\n    Calculates mode coverage based on Hamming distance.\n    \"\"\"\n    # Calculate Hamming distance from each sample to each mode\n    # Broadcasting: (n_samples, 1, D) vs (1, n_modes, D)\n    distances = np.sum(samples[:, np.newaxis, :] != modes[np.newaxis, :, :], axis=2)\n    \n    # Assign each sample to the nearest mode\n    assignments = np.argmin(distances, axis=1)\n    \n    # Find the unique modes that were covered\n    covered_mode_indices = np.unique(assignments)\n    \n    # Calculate coverage fraction\n    coverage = len(covered_mode_indices) / len(modes)\n    return coverage\n\ndef solve():\n    \"\"\"\n    Main function to run the RBM comparison experiment.\n    \"\"\"\n    # --- Problem Constants and Parameters ---\n    D = 8  # Number of visible units\n    H = 6  # Number of hidden units\n    M = 4  # Number of modes\n    \n    base_modes = np.array([\n        [1, 1, 0, 0, 1, 0, 1, 0],\n        [0, 1, 1, 0, 0, 1, 0, 1],\n        [1, 0, 1, 1, 0, 0, 1, 0],\n        [0, 0, 0, 1, 1, 1, 0, 0]\n    ])\n\n    # Test cases: (mode_counts, p_flip)\n    test_cases = [\n        ((250, 250, 250, 250), 0.05),  # Case A\n        ((250, 250, 250, 250), 0.25),  # Case B\n        ((400, 100, 75, 25), 0.05),    # Case C\n    ]\n\n    # Training and evaluation parameters\n    learning_rate = 0.1\n    epochs = 100\n    batch_size = 50\n    n_eval_samples = 200\n    n_gibbs_steps_eval = 100\n\n    results = []\n    \n    # Master RNG for reproducibility of the entire experiment\n    master_rng = np.random.default_rng(42)\n\n    for mode_counts, p_flip in test_cases:\n        # Generate seeds for this case to ensure fair comparison\n        data_seed = master_rng.integers(2**32 - 1)\n        init_seed = master_rng.integers(2**32 - 1)\n        train_seed = master_rng.integers(2**32 - 1)\n        eval_seed = master_rng.integers(2**32 - 1)\n\n        # 1. Generate dataset\n        dataset = generate_dataset(base_modes, mode_counts, p_flip, seed=data_seed)\n\n        # 2. Train and evaluate for k=1 and k=10\n        case_results = []\n        for k_val in [1, 10]:\n            # Initialize RBM with identical starting weights\n            rbm = RBM(n_visible=D, n_hidden=H, seed=init_seed)\n            \n            # Train RBM with identical batch ordering\n            train_rbm(rbm, dataset, epochs=epochs, batch_size=batch_size, \n                      k=k_val, learning_rate=learning_rate, seed=train_seed)\n            \n            # Generate samples for evaluation\n            samples = rbm.sample(n_samples=n_eval_samples, \n                                 n_gibbs_steps=n_gibbs_steps_eval, \n                                 seed=eval_seed)\n            \n            # Calculate mode coverage\n            coverage = calculate_coverage(samples, base_modes)\n            case_results.append(round(coverage, 3))\n        \n        results.append(case_results)\n    \n    # Format and print the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "表征学习的主要目标之一是发现数据中尽可能独立的潜在变化因素。在RBM中，这些因素由隐藏单元表示。此练习  提供了一种定量方法来诊断RBM学习到的特征质量。你将实施一个度量标准来衡量隐藏单元激活的统计独立性，并探索其与模型权重向量的几何正交性之间的联系，从而揭示模型参数与表征质量之间更深层次的关联。",
            "id": "3170455",
            "problem": "给定一个二元-二元受限玻尔兹曼机（RBM）的框架。该 RBM 有一个可见二元向量 $v \\in \\{0,1\\}^{m}$，一个隐藏二元向量 $h \\in \\{0,1\\}^{n}$，一个权重矩阵 $W \\in \\mathbb{R}^{m \\times n}$，一个可见偏置向量 $a \\in \\mathbb{R}^{m}$，以及一个隐藏偏置向量 $b \\in \\mathbb{R}^{n}$。在 $(v,h)$ 上的联合概率分布由基于能量的模型定义\n$$\nE(v,h) = - a^{\\top} v - b^{\\top} h - v^{\\top} W h,\n$$\n其形式为\n$$\np(v,h) = \\frac{1}{Z} \\exp\\left(-E(v,h)\\right),\n$$\n其中 $Z$ 是配分函数，满足\n$$\nZ = \\sum_{v \\in \\{0,1\\}^{m}} \\sum_{h \\in \\{0,1\\}^{n}} \\exp\\left(-E(v,h)\\right).\n$$\n基于这些基础，推导条件分布 $p(h \\mid v)$ 并用它来获得 $\\mathbb{E}[h \\mid v]$ 的分量表达式。然后，对于一个可见向量数据集 $\\{v^{(i)}\\}_{i=1}^{N}$，定义矩阵 $H \\in \\mathbb{R}^{N \\times n}$，其第 $i$ 行为 $\\mathbb{E}[h \\mid v^{(i)}]^{\\top}$，并计算数据集中期望隐藏激活值的样本协方差矩阵 $C \\in \\mathbb{R}^{n \\times n}$。样本协方差矩阵必须按如下方式计算\n$$\n\\mu = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}[h \\mid v^{(i)}], \\quad C = \\frac{1}{N-1} \\sum_{i=1}^{N} \\left(\\mathbb{E}[h \\mid v^{(i)}] - \\mu\\right)\\left(\\mathbb{E}[h \\mid v^{(i)}] - \\mu\\right)^{\\top}.\n$$\n通过衡量 $C$ 的近对角程度，设计一个用于检验隐藏特征近似独立性的定量测试。定义独立性偏差度量\n$$\n\\mathrm{ID}(C) = \\frac{\\sum_{i \\neq j} C_{ij}^{2}}{\\sum_{k=1}^{n} C_{kk}^{2} + \\epsilon},\n$$\n其中 $\\epsilon = 10^{-12}$ 是一个用于保证数值稳定性的微小常数。$\\mathrm{ID}(C)$ 的值较小表示协方差矩阵接近对角矩阵，因此隐藏特征在期望上在数据集中是近似独立的。\n\n此外，定义一个对隐藏特征权重向量的正交性惩罚，以鼓励协方差矩阵接近对角矩阵。令 $W_{:,j}$ 表示 $W$ 的第 $j$ 列（即隐藏单元 $j$ 的权重向量）。构建格拉姆矩阵 $G = W^{\\top} W \\in \\mathbb{R}^{n \\times n}$ 并定义\n$$\n\\mathrm{OP}(W) = \\frac{\\sum_{i \\neq j} G_{ij}^{2}}{\\sum_{k=1}^{n} G_{kk}^{2} + \\epsilon}.\n$$\n$\\mathrm{OP}(W)$ 的值较小表示隐藏单元的权重向量近似正交，这应能促使 $\\mathrm{ID}(C)$ 的值变小。\n\n实现一个程序，对于给定的 RBM 参数和数据集，计算每个测试用例的 $\\mathrm{ID}(C)$ 和 $\\mathrm{OP}(W)$。\n\n使用以下测试套件，其中包括一个一般情况、一个相关权重情况和一个边界情况：\n\n- 测试用例 1（一般“理想路径”情况）：$m = 4$，$n = 3$，$W = \\begin{bmatrix} 5  0  0 \\\\ 0  5  0 \\\\ 0  0  5 \\\\ 0  0  0 \\end{bmatrix}$，$b = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$，在此任务中未使用 $a$，可将其设置为 $\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。包含 $N = 8$ 个可见向量的数据集：\n$$\n\\{(0,0,0,0),(1,0,0,0),(0,1,0,0),(0,0,1,0),(1,1,0,0),(1,0,1,0),(0,1,1,0),(1,1,1,0)\\}.\n$$\n- 测试用例 2（相关隐藏特征）：$m = 4$，$n = 3$，$W = \\begin{bmatrix} 5  5  0 \\\\ 5  5  0 \\\\ 0  0  5 \\\\ 0  0  0 \\end{bmatrix}$，$b = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$，数据集与测试用例 1 相同。\n- 测试用例 3（边界情况）：$m = 4$，$n = 3$，$W = \\begin{bmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{bmatrix}$，$b = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$，数据集与测试用例 1 相同。\n\n您的程序必须：\n- 根据上述 RBM 基础知识推导并使用 $\\mathbb{E}[h \\mid v]$。\n- 为每个测试用例计算 $H$ 的行的样本协方差矩阵 $C$。\n- 为每个测试用例计算 $\\mathrm{ID}(C)$ 和 $\\mathrm{OP}(W)$，使用 $\\epsilon = 10^{-12}$。\n- 将所有测试用例的结果聚合到单行输出中，按顺序包含所有数字，每个测试用例依次贡献两个数字，首先是 $\\mathrm{ID}(C)$，然后是 $\\mathrm{OP}(W)$。因此，最终输出必须是遵循以下格式的单行文本\n$$\n[\\mathrm{ID}_1,\\mathrm{OP}_1,\\mathrm{ID}_2,\\mathrm{OP}_2,\\mathrm{ID}_3,\\mathrm{OP}_3],\n$$\n其中每个项目都是一个浮点数。不得打印任何其他文本。",
            "solution": "该问题是有效的，因为它在科学上基于受限玻尔兹曼机（RBM）的理论，数学上是适定的，客观的，并为获得唯一解提供了所有必要信息。\n\n我们的目标是为三个给定的测试用例计算独立性偏差度量 $\\mathrm{ID}(C)$ 和正交性惩罚 $\\mathrm{OP}(W)$。这需要多步推导和计算。\n\n首先，我们必须推导给定可见单元时隐藏单元的条件期望表达式 $\\mathbb{E}[h \\mid v]$。条件概率分布 $p(h \\mid v)$ 由联合概率 $p(v,h)$ 与边缘概率 $p(v)$ 的比值给出：\n$$\np(h \\mid v) = \\frac{p(v,h)}{p(v)} = \\frac{\\frac{1}{Z} \\exp(-E(v,h))}{\\sum_{h' \\in \\{0,1\\}^n} \\frac{1}{Z} \\exp(-E(v,h'))} = \\frac{\\exp(-E(v,h))}{\\sum_{h' \\in \\{0,1\\}^n} \\exp(-E(v,h'))}.\n$$\n代入能量函数 $E(v,h) = -a^{\\top} v - b^{\\top} h - v^{\\top} W h$：\n$$\np(h \\mid v) = \\frac{\\exp(a^{\\top} v + b^{\\top} h + v^{\\top} W h)}{\\sum_{h' \\in \\{0,1\\}^n} \\exp(a^{\\top} v + b^{\\top} h' + v^{\\top} W h')}.\n$$\n项 $\\exp(a^{\\top} v)$ 相对于分母中的求和变量 $h'$ 是一个常数，因此可以被提取出来并与分子中的项相抵消。这表明条件分布 $p(h \\mid v)$ 与可见偏置向量 $a$ 无关，正如问题陈述中所指出的。表达式简化为：\n$$\np(h \\mid v) = \\frac{\\exp(b^{\\top} h + v^{\\top} W h)}{\\sum_{h' \\in \\{0,1\\}^n} \\exp(b^{\\top} h' + v^{\\top} W h')}.\n$$\n指数部分可以重写为对隐藏单元的求和：$b^{\\top} h + v^{\\top} W h = \\sum_{j=1}^{n} (b_j h_j + (v^{\\top} W)_{j} h_j) = \\sum_{j=1}^{n} (b_j + v^{\\top} W_{:,j}) h_j$。由于在 RBM 中隐藏单元之间没有连接，联合条件概率可以分解为各个独立条件概率的乘积：\n$$\np(h \\mid v) = \\prod_{j=1}^{n} p(h_j \\mid v).\n$$\n这种条件独立性是 RBM 的一个关键特性。对于单个二元隐藏单元 $h_j \\in \\{0,1\\}$，其条件概率为：\n$$\np(h_j=1 \\mid v) = \\frac{\\exp(b_j + v^{\\top} W_{:,j})}{1 + \\exp(b_j + v^{\\top} W_{:,j})} = \\frac{1}{1 + \\exp(-(b_j + v^{\\top} W_{:,j}))} = \\sigma(b_j + v^{\\top} W_{:,j}),\n$$\n其中 $\\sigma(x) = 1/(1+e^{-x})$ 是 logistic sigmoid 函数。\n一个二元变量的期望是其值为 $1$ 的概率。因此，第 $j$ 个隐藏单元的期望是：\n$$\n\\mathbb{E}[h_j \\mid v] = 1 \\cdot p(h_j=1 \\mid v) + 0 \\cdot p(h_j=0 \\mid v) = p(h_j=1 \\mid v).\n$$\n以向量形式表示，给定可见向量 $v$ 时隐藏向量 $h$ 的条件期望为：\n$$\n\\mathbb{E}[h \\mid v] = \\sigma(b + W^{\\top}v),\n$$\n其中 sigmoid 函数 $\\sigma$ 逐元素地应用于向量参数 $b + W^{\\top}v$。\n\n有了这个结果，我们可以概述计算过程：\n\n1.  **计算期望激活值**：对于数据集 $\\{v^{(i)}\\}_{i=1}^{N}$ 中的每个可见向量 $v^{(i)}$，计算期望隐藏激活值向量 $h^{(i)*} = \\mathbb{E}[h \\mid v^{(i)}]$。然后，使用这 $N$ 个大小为 $n \\times 1$ 的向量构成一个矩阵 $H \\in \\mathbb{R}^{N \\times n}$ 的行，其中第 $i$ 行为 $(h^{(i)*})^{\\top}$。\n\n2.  **计算样本协方差矩阵 $C$**：首先，计算期望隐藏激活值的样本均值，这是一个 $n$ 维列向量 $\\mu = \\frac{1}{N} \\sum_{i=1}^{N} h^{(i)*}$。然后，使用提供的公式计算 $n \\times n$ 的样本协方差矩阵 $C$，分母为 $N-1$ 以获得无偏估计：\n$$\nC = \\frac{1}{N-1} \\sum_{i=1}^{N} (h^{(i)*} - \\mu)(h^{(i)*} - \\mu)^{\\top}.\n$$\n\n3.  **计算独立性偏差 $\\mathrm{ID}(C)$**：使用计算出的矩阵 $C$，我们计算独立性偏差度量。这涉及到将 $C$ 中元素平方和划分为对角线部分和非对角线部分：\n$$\n\\mathrm{ID}(C) = \\frac{\\sum_{i \\neq j} C_{ij}^{2}}{\\sum_{k=1}^{n} C_{kk}^{2} + \\epsilon}.\n$$\n分子是总非对角协方差的度量，而分母是总方差的度量。一个较小的值表示期望隐藏特征在数据集中近似不相关。常数 $\\epsilon = 10^{-12}$ 在所有方差都为零时确保数值稳定性。\n\n4.  **计算正交性惩罚 $\\mathrm{OP}(W)$**：此度量量化了与隐藏单元相关联的权重向量的非正交性。首先，我们计算格拉姆矩阵 $G = W^{\\top} W$。然后，与 $\\mathrm{ID}(C)$ 类似，我们计算 $\\mathrm{OP}(W)$：\n$$\n\\mathrm{OP}(W) = \\frac{\\sum_{i \\neq j} G_{ij}^{2}}{\\sum_{k=1}^{n} G_{kk}^{2} + \\epsilon}.\n$$\n项 $G_{ij} = (W_{:,i})^{\\top}W_{:,j}$ 是隐藏单元 $i$ 和 $j$ 的权重向量的点积。一个较小的 $\\mathrm{OP}(W)$ 值表示这些权重向量近似正交。\n\n对所提供的三个测试用例中的每一个都执行这些步骤。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the independence deviation and orthogonality penalty for given RBM parameters.\n    \"\"\"\n\n    def sigmoid(x):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    def compute_metrics(W, b, V, epsilon):\n        \"\"\"\n        Computes ID(C) and OP(W) for a single RBM configuration.\n\n        Args:\n            W (np.ndarray): Weight matrix of size (m, n).\n            b (np.ndarray): Hidden bias vector of size (n, 1).\n            V (np.ndarray): Dataset of visible vectors of size (N, m).\n            epsilon (float): Small constant for numerical stability.\n\n        Returns:\n            tuple: A tuple containing (id_c, op_w).\n        \"\"\"\n        N, m = V.shape\n        n = W.shape[1]\n\n        # 1. Compute H, the matrix of expected hidden activations\n        H = np.zeros((N, n))\n        for i in range(N):\n            v_i = V[i, :].reshape(-1, 1)  # Shape (m, 1)\n            # Argument to sigmoid: b (n,1) + W.T (n,m) @ v_i (m,1) -> (n,1)\n            h_exp = sigmoid(b + W.T @ v_i)\n            H[i, :] = h_exp.T  # Store as a row vector (1, n)\n\n        # 2. Compute the sample covariance matrix C\n        if N == 1:\n            C = np.zeros((n, n))\n        else:\n            # np.cov with rowvar=False computes covariance of columns.\n            # ddof=1 uses N-1 in the denominator for an unbiased estimate.\n            C = np.cov(H, rowvar=False, ddof=1)\n            # Manually:\n            # mu = np.mean(H, axis=0)\n            # H_centered = H - mu.reshape(1, -1)\n            # C = (H_centered.T @ H_centered) / (N - 1)\n\n        # 3. Compute ID(C)\n        diag_C_sq_sum = np.sum(np.diag(C)**2)\n        total_C_sq_sum = np.sum(C**2)\n        off_diag_C_sq_sum = total_C_sq_sum - diag_C_sq_sum\n        id_c = off_diag_C_sq_sum / (diag_C_sq_sum + epsilon)\n\n        # 4. Compute G and OP(W)\n        G = W.T @ W\n        diag_G_sq_sum = np.sum(np.diag(G)**2)\n        total_G_sq_sum = np.sum(G**2)\n        off_diag_G_sq_sum = total_G_sq_sum - diag_G_sq_sum\n        op_w = off_diag_G_sq_sum / (diag_G_sq_sum + epsilon)\n\n        return id_c, op_w\n\n    epsilon = 1e-12\n    m, n = 4, 3\n    V_data = np.array([\n        [0., 0., 0., 0.], [1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.],\n        [1., 1., 0., 0.], [1., 0., 1., 0.], [0., 1., 1., 0.], [1., 1., 1., 0.]\n    ])\n    b_common = np.zeros((n, 1))\n\n    test_cases = [\n        # Test case 1 (general \"happy path\"): Orthogonal weights\n        (\n            np.array([[5., 0., 0.], [0., 5., 0.], [0., 0., 5.], [0., 0., 0.]]),\n            b_common,\n            V_data\n        ),\n        # Test case 2 (correlated hidden features): Correlated weights\n        (\n            np.array([[5., 5., 0.], [5., 5., 0.], [0., 0., 5.], [0., 0., 0.]]),\n            b_common,\n            V_data\n        ),\n        # Test case 3 (boundary case): Zero weights\n        (\n            np.zeros((m, n)),\n            b_common,\n            V_data\n        ),\n    ]\n\n    results = []\n    for W, b, V in test_cases:\n        id_c, op_w = compute_metrics(W, b, V, epsilon)\n        results.append(id_c)\n        results.append(op_w)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "RBM是基于能量的模型，在输入数据空间上定义了一个复杂的能量景观。该景观的特性，例如其在数据点周围的陡峭程度，决定了模型对微小扰动的鲁棒性。这个高级实践  将RBM与对抗性机器学习这一现代领域联系起来。你将学习利用RBM的自由能梯度来制造对抗性样本，并研究如何通过使用温度采样等技术修改训练算法，通过塑造此能量景观来构建更鲁棒的模型。",
            "id": "3170453",
            "problem": "考虑一个由伯努利可见单元和伯努利隐藏单元组成的受限玻尔兹曼机 (RBM)。RBM通过一个能量函数定义了可见向量 $v \\in [0,1]^n$ 和隐藏向量 $h \\in \\{0,1\\}^m$ 上的联合分布。使用以下普遍接受的基础作为出发点。一个基于能量的模型将联合概率定义为 $P(v,h) \\propto \\exp(-E(v,h))$。对于一个伯努利-伯努利 RBM，能量函数由 $E(v,h) = -b^{\\top} v - c^{\\top} h - v^{\\top} W h$ 给出，其中 $W \\in \\mathbb{R}^{n \\times m}$ 是权重矩阵，$b \\in \\mathbb{R}^{n}$ 是可见偏置向量，$c \\in \\mathbb{R}^{m}$ 是隐藏偏置向量。可见向量的自由能通过对隐藏单元进行边缘化来定义：$F(v) = -\\log \\sum_{h} \\exp(-E(v,h))$。在此背景下，对抗鲁棒性指的是增加受扰动输入的自由能的难度：给定一个小的扰动预算，如果最优扰动 $\\delta$ 对 $F(v + \\delta)$ 的增加较小，则模型更鲁棒。\n\n你的任务是实现并评估对抗旨在增加自由能的扰动 $\\delta$ 的鲁棒性，并测试与标准训练相比，在负向阶段使用退火采样进行训练是否能提高鲁棒性。请从第一性原理出发解决以下步骤。\n\n1. 从给定的能量函数 $E(v,h)$ 和自由能 $F(v)$ 的定义出发，推导出伯努利-伯努利 RBM 的 $F(v)$ 关于 $W$、$b$ 和 $c$ 的显式形式，并推导出梯度 $\\nabla_v F(v)$。然后，根据约束 $\\lVert \\delta \\rVert_{\\infty} \\le \\epsilon$ 和 $0 \\le v + \\delta \\le 1$，推导出一个近似最大化 $F(v + \\delta)$ 的一阶对抗扰动规则。\n\n2. 在一个包含 $N$ 个样本的合成数据集 $[0,1]^n$ 上，为 RBM 实现两种训练过程：\n   - 使用温度 $\\tau = 1$ 的吉布斯采样的标准对比散度，负向阶段包含一步。\n   - 使用温度 $\\tau > 1$ 的退火负向阶段采样。在基于温度的采样中，玻尔兹曼分布被修改为 $P(v,h) \\propto \\exp(-E(v,h)/\\tau)$。对于吉布斯条件概率，在负向阶段使用 $p(h_j = 1 \\mid v) = \\sigma((c_j + W_{\\cdot j}^{\\top} v)/\\tau)$ 和 $p(v_i = 1 \\mid h) = \\sigma((b_i + W_{i \\cdot}^{\\top} h)/\\tau)$，同时保持正向阶段的温度 $\\tau = 1$。这里 $\\sigma(x)$ 表示 logistic sigmoid 函数。\n\n3. 对于每个训练好的 RBM，实现一种快速对抗样本生成方法，该方法为每个数据点 $v$ 计算一个扰动 $\\delta$，目标是在 $\\ell_{\\infty}$ 约束 $\\lVert \\delta \\rVert_{\\infty} \\le \\epsilon$ 和箱形约束 $0 \\le v + \\delta \\le 1$ 下，最大程度地增加 $F(v + \\delta)$。使用自由能关于 $v$ 的梯度来生成一个显式扰动，并将对抗样本裁剪回有效范围内。\n\n4. 定义一个鲁棒性度量，即在数据集上受此对抗扰动后的平均自由能增量：计算 $\\Delta F_{\\text{mean}} = \\frac{1}{N} \\sum_{k=1}^{N} \\left[ F(v_k^{\\text{adv}}) - F(v_k) \\right]$，其中 $v_k^{\\text{adv}}$ 是 $v_k$ 的对抗扰动版本。如果该平均增量较小，则认为模型更鲁棒。\n\n5. 评估对于下面指定的测试套件，退火训练的 RBM 是否比标准训练的 RBM 更鲁棒。对于每个测试用例，生成一个布尔值，指示退火训练产生的 $\\Delta F_{\\text{mean}}$ 是否小于或等于标准训练的 $\\Delta F_{\\text{mean}}$。\n\n使用以下参数值的测试套件，该套件应探测正常情况、边界条件和更大扰动的边缘情况：\n- 情况 1：$\\epsilon = 0.0$，$\\tau = 1.5$。\n- 情况 2：$\\epsilon = 0.05$，$\\tau = 1.5$。\n- 情况 3：$\\epsilon = 0.2$，$\\tau = 2.0$。\n\n你的程序必须：\n- 构建一个小的合成数据集，位于 $[0,1]^n$ 中，其中 $n = 6$，$N = 64$，包含围绕两个不同原型模式的两个簇，并添加小噪声后裁剪到 $[0,1]$ 范围内。\n- 训练两个具有 $m = 4$ 个隐藏单元的 RBM：一个使用标准训练（负向阶段 $\\tau = 1$），另一个使用测试用例中的 $\\tau$ 进行退火负向阶段采样。\n- 对于每个测试用例，计算步骤 5 中描述的布尔结果。\n\n你的程序应生成一行输出，其中包含三个测试用例的三个布尔结果，形式为逗号分隔并用方括号括起来的列表，例如 $[{\\text{True}},{\\text{False}},{\\text{True}}]$。此问题不涉及物理单位。不使用角度。不使用百分比；所有量均为无量纲实数，所有要求的输出均为布尔值。程序必须是自包含的，且无需用户输入。",
            "solution": "该问题要求对伯努利-伯努利受限玻尔兹曼机 (RBM) 的自由能及其梯度进行推导，然后通过实现来比较两种训练方法的对抗鲁棒性：标准对比散度 (CD) 和带退火负向阶段采样的 CD。\n\n### 第一部分：解析推导\n\n我们从为 RBM 提供的基础定义开始。该模型由可见单元 $v \\in [0,1]^n$ 和隐藏单元 $h \\in \\{0,1\\}^m$ 组成。联合概率分布由一个能量函数给出：$P(v,h) = \\frac{1}{Z} \\exp(-E(v,h))$，其中 $Z$ 是配分函数。能量函数定义为：\n$$E(v,h) = -b^{\\top} v - c^{\\top} h - v^{\\top} W h = -\\sum_{i=1}^n b_i v_i - \\sum_{j=1}^m c_j h_j - \\sum_{i=1}^n \\sum_{j=1}^m v_i W_{ij} h_j$$\n其中 $W \\in \\mathbb{R}^{n \\times m}$ 是权重矩阵，$b \\in \\mathbb{R}^{n}$ 是可见偏置向量，$c \\in \\mathbb{R}^{m}$ 是隐藏偏置向量。\n\n#### 1.1. 自由能 $F(v)$\n\n自由能 $F(v)$ 通过将联合分布对隐藏单元进行边缘化来定义：\n$$F(v) = -\\log \\sum_{h} \\exp(-E(v,h))$$\n代入 $E(v,h)$ 的表达式：\n$$F(v) = -\\log \\sum_{h \\in \\{0,1\\}^m} \\exp\\left(b^{\\top} v + c^{\\top} h + v^{\\top} W h\\right)$$\n我们可以分离出不依赖于 $h$ 的项：\n$$F(v) = -\\log \\left( \\exp(b^{\\top} v) \\sum_{h \\in \\{0,1\\}^m} \\exp\\left(c^{\\top} h + v^{\\top} W h\\right) \\right)$$\n$$F(v) = -b^{\\top} v - \\log \\sum_{h \\in \\{0,1\\}^m} \\exp\\left(\\sum_{j=1}^m c_j h_j + \\sum_{j=1}^m \\sum_{i=1}^n v_i W_{ij} h_j\\right)$$\n设 $W_{\\cdot j}$ 为 $W$ 的第 $j$ 列。$v^{\\top} W h$ 项可以重写为 $\\sum_{j=1}^m h_j (v^{\\top} W_{\\cdot j})$。\n$$F(v) = -b^{\\top} v - \\log \\sum_{h \\in \\{0,1\\}^m} \\exp\\left(\\sum_{j=1}^m h_j (c_j + v^{\\top} W_{\\cdot j})\\right)$$\n由于隐藏单元 $h_j$ 在求和中是二元的且独立的，我们可以将对 $h$ 的所有配置的求和分解为对单个隐藏单元的积：\n$$\\sum_{h \\in \\{0,1\\}^m} \\prod_{j=1}^m \\exp\\left(h_j (c_j + v^{\\top} W_{\\cdot j})\\right) = \\prod_{j=1}^m \\sum_{h_j \\in \\{0,1\\}} \\exp\\left(h_j (c_j + v^{\\top} W_{\\cdot j})\\right)$$\n对于每个 $j$，对 $h_j=0$ 和 $h_j=1$ 求和：\n$$\\sum_{h_j \\in \\{0,1\\}} \\exp\\left(h_j (c_j + v^{\\top} W_{\\cdot j})\\right) = \\exp(0) + \\exp(c_j + v^{\\top} W_{\\cdot j}) = 1 + \\exp(c_j + v^{\\top} W_{\\cdot j})$$\n将此代回 $F(v)$ 的表达式：\n$$F(v) = -b^{\\top} v - \\log \\left(\\prod_{j=1}^m (1 + \\exp(c_j + v^{\\top} W_{\\cdot j}))\\right)$$\n$$F(v) = -b^{\\top} v - \\sum_{j=1}^m \\log(1 + \\exp(c_j + v^{\\top} W_{\\cdot j}))$$\n这是自由能的显式形式。$\\log(1 + \\exp(x))$ 项被称为 softplus 函数。\n\n#### 1.2. 自由能的梯度 $\\nabla_v F(v)$\n\n为了找到对抗扰动，我们首先需要自由能关于可见向量 $v$ 的梯度。我们对 $F(v)$ 关于 $v$ 的一个分量 $v_k$求导：\n$$\\frac{\\partial F(v)}{\\partial v_k} = \\frac{\\partial}{\\partial v_k} \\left( -\\sum_{i=1}^n b_i v_i - \\sum_{j=1}^m \\log(1 + \\exp(c_j + \\sum_{i=1}^n v_i W_{ij})) \\right)$$\n$$\\frac{\\partial F(v)}{\\partial v_k} = -b_k - \\sum_{j=1}^m \\frac{\\partial}{\\partial v_k} \\log(1 + \\exp(c_j + v^{\\top} W_{\\cdot j}))$$\n使用链式法则，并回顾 $\\frac{d}{dx} \\log(1+e^x) = \\frac{e^x}{1+e^x} = \\sigma(x)$，其中 $\\sigma(x)$ 是 logistic sigmoid 函数：\n$$\\frac{\\partial}{\\partial v_k} \\log(1 + \\exp(c_j + v^{\\top} W_{\\cdot j})) = \\sigma(c_j + v^{\\top} W_{\\cdot j}) \\cdot \\frac{\\partial}{\\partial v_k}(c_j + \\sum_{i=1}^n v_i W_{ij}) = \\sigma(c_j + v^{\\top} W_{\\cdot j}) \\cdot W_{kj}$$\n将此代入梯度表达式：\n$$\\frac{\\partial F(v)}{\\partial v_k} = -b_k - \\sum_{j=1}^m W_{kj} \\sigma(c_j + v^{\\top} W_{\\cdot j})$$\n注意 $\\sigma(c_j + v^{\\top} W_{\\cdot j})$ 是条件概率 $p(h_j=1|v)$。在向量表示法中，梯度为：\n$$\\nabla_v F(v) = -b - W \\cdot \\sigma(c + W^{\\top} v)$$\n\n#### 1.3. 一阶对抗扰动\n\n目标是寻找一个扰动 $\\delta$ 来最大化 $F(v+\\delta)$，并满足约束条件 $\\lVert \\delta \\rVert_{\\infty} \\le \\epsilon$ 和 $0 \\le v + \\delta \\le 1$。$F(v+\\delta)$ 在 $v$ 附近的一阶泰勒展开为：\n$$F(v+\\delta) \\approx F(v) + \\delta^{\\top} \\nabla_v F(v)$$\n为了最大化这个线性近似，$\\delta$ 应与梯度 $\\nabla_v F(v)$ 的方向一致。给定 $\\ell_{\\infty}$ 范数约束，最优扰动为：\n$$\\delta = \\epsilon \\cdot \\text{sign}(\\nabla_v F(v))$$\n这就是快速梯度符号法 (FGSM) 的原理。得到的对抗样本候选为 $v' = v + \\delta$。为了满足箱形约束 $0 \\le v + \\delta \\le 1$，我们必须对结果进行裁剪：\n$$v^{\\text{adv}} = \\text{clip}(v + \\epsilon \\cdot \\text{sign}(\\nabla_v F(v)), 0, 1)$$\n\n### 第二部分：算法设计\n\n#### 2.1. RBM 训练（对比散度）\n\nRBM 参数 ($W, b, c$) 使用对比散度 (CD-1) 进行训练，这是对数似然梯度的近似。更新规则从此梯度推导而来：\n$$\\Delta W \\propto v_0 p(h|v_0)^{\\top} - v_1 p(h|v_1)^{\\top}$$\n$$\\Delta b \\propto v_0 - v_1$$\n$$\\Delta c \\propto p(h|v_0) - p(h|v_1)$$\n其中 $v_0$ 是一个数据样本，$v_1$ 是一步重构。过程如下：\n1.  **正向阶段**：对于数据向量 $v_0$，计算隐藏单元激活概率 $p(h|v_0) = \\sigma(v_0 W + c)$。\n2.  **负向阶段**：从隐藏激活中生成重构 $v_1$。对于连续输入，我们使用概率：$p(v|h) = \\sigma(h W^{\\top} + b)$。然后我们计算新的隐藏概率 $p(h|v_1) = \\sigma(v_1 W + c)$。\n\n对于这个问题，实现了两种训练变体：\n-   **标准训练**：所有阶段都使用温度 $\\tau=1$。条件概率与上述完全相同。\n-   **退火训练**：正向阶段保持在 $\\tau=1$。负向阶段使用温度 $\\tau > 1$。条件概率通过将其参数除以 $\\tau$ 来修改：\n    $$p_{\\tau}(v=1|h) = \\sigma((h W^{\\top} + b)/\\tau)$$\n    $$p_{\\tau}(h=1|v) = \\sigma((v W + c)/\\tau)$$\n这种退火平滑了负向阶段的分布，这可以影响学习动态和模型鲁棒性。为减少方差，我们直接使用激活概率，而不是从中采样二进制状态。\n\n#### 2.2. 鲁棒性评估\n\n对于每个训练好的 RBM，我们使用指定的度量来量化其对抗鲁棒性。\n1.  对于数据集中的每个数据点 $v_k$，我们使用第一部分 1.3 中推导的基于 FGSM 的规则，在给定的扰动预算 $\\epsilon$ 下生成一个对抗对应样本 $v_k^{\\text{adv}}$。\n2.  我们使用第一部分 1.1 中推导的公式计算原始点的自由能 $F(v_k)$ 和对抗点的自由能 $F(v_k^{\\text{adv}})$。\n3.  鲁棒性度量是整个数据集上自由能的平均增量：\n    $$\\Delta F_{\\text{mean}} = \\frac{1}{N} \\sum_{k=1}^{N} \\left[ F(v_k^{\\text{adv}}) - F(v_k) \\right]$$\n    较小的 $\\Delta F_{\\text{mean}}$ 表示模型更鲁棒，因为对抗攻击在增加自由能方面的效果较差。\n\n### 第三部分：实验协议\n\n比较是在一个由两个含噪声的簇组成的合成数据集 ($N=64, n=6$) 上进行的。对于由 $(\\epsilon, \\tau)$ 定义的每个测试用例：\n1.  两个 RBM ($m=4$) 使用相同的随机权重和偏置进行初始化，以确保公平比较。\n2.  一个 RBM 使用标准 CD-1 ($\\tau_{\\text{neg}}=1$) 进行训练，另一个使用退火负向阶段 CD-1 ($\\tau_{\\text{neg}}=\\tau$) 进行训练。\n3.  对于两个训练好的模型，使用测试用例的 $\\epsilon$ 计算 $\\Delta F_{\\text{mean}}$。\n4.  最终结果是一个布尔值，指示退火模型是否更鲁棒或同样鲁棒，即 $\\Delta F_{\\text{mean, tempered}} \\le \\Delta F_{\\text{mean, standard}}$ 是否成立。\n\n$\\epsilon=0.0$ 的测试用例作为一个合理性检查。由于扰动为零，两个模型的 $\\Delta F_{\\text{mean}}$ 都将为 $0$，比较 $0 \\le 0$ 将正确地得出 `True`。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _sigmoid(x):\n    \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n    return 1.0 / (1.0 + np.exp(-x))\n\nclass RBM:\n    \"\"\"A Bernoulli-Bernoulli Restricted Boltzmann Machine.\"\"\"\n    \n    def __init__(self, n_vis, n_hid, seed=None):\n        \"\"\"\n        Initializes the RBM.\n        \n        Args:\n            n_vis (int): Number of visible units.\n            n_hid (int): Number of hidden units.\n            seed (int, optional): Seed for the random number generator.\n        \"\"\"\n        self.rng = np.random.default_rng(seed)\n        self.n_vis = n_vis\n        self.n_hid = n_hid\n        \n        # Initialize weights with small random values and biases to zero.\n        scale = 0.01\n        self.W = self.rng.normal(loc=0.0, scale=scale, size=(n_vis, n_hid))\n        self.b = np.zeros(n_vis)\n        self.c = np.zeros(n_hid)\n        \n    def copy_params_from(self, other_rbm):\n        \"\"\"Copies parameters from another RBM instance for fair comparison.\"\"\"\n        self.W = np.copy(other_rbm.W)\n        self.b = np.copy(other_rbm.b)\n        self.c = np.copy(other_rbm.c)\n        self.rng = other_rbm.rng # ensure samplers are also identical if used\n\n    def free_energy(self, v):\n        \"\"\"\n        Computes the free energy for a batch of visible vectors.\n        \n        Args:\n            v (np.ndarray): A batch of visible vectors, shape (N, n_vis).\n        \n        Returns:\n            np.ndarray: An array of free energy values, shape (N,).\n        \"\"\"\n        if v.ndim == 1:\n            v = v.reshape(1, -1)\n        \n        vb_term = v @ self.b\n        vWc = (v @ self.W) + self.c\n        # np.logaddexp(0, x) is a numerically stable way to compute log(1 + exp(x))\n        hidden_term = np.sum(np.logaddexp(0, vWc), axis=1)\n        \n        return -vb_term - hidden_term\n\n    def grad_free_energy(self, v):\n        \"\"\"\n        Computes the gradient of the free energy with respect to v.\n        \n        Args:\n            v (np.ndarray): A batch of visible vectors, shape (N, n_vis).\n        \n        Returns:\n            np.ndarray: The gradient of F(v) w.r.t v, shape (N, n_vis).\n        \"\"\"\n        if v.ndim == 1:\n            v = v.reshape(1, -1)\n        \n        hidden_probs = _sigmoid((v @ self.W) + self.c)\n        # Gradient is -b - W * sigma(c + W^T * v)\n        # The -self.b term is broadcasted to match the batch size.\n        grad = -self.b - (hidden_probs @ self.W.T)\n        \n        return grad\n\n    def train(self, data, epochs, learning_rate, temp_neg_phase=1.0):\n        \"\"\"\n        Trains the RBM using Contrastive Divergence (CD-1).\n        \n        Args:\n            data (np.ndarray): The training data, shape (N, n_vis).\n            epochs (int): Number of training epochs.\n            learning_rate (float): The learning rate for parameter updates.\n            temp_neg_phase (float): Temperature for negative phase sampling.\n        \"\"\"\n        n_samples = data.shape[0]\n        v0 = data\n        \n        for _ in range(epochs):\n            # Positive phase (tau=1)\n            pos_hidden_probs = _sigmoid((v0 @ self.W) + self.c)\n            \n            # Negative phase (reconstruction)\n            # Use probabilities for reconstruction (mean-field)\n            neg_vis_probs = _sigmoid(((pos_hidden_probs @ self.W.T) + self.b) / temp_neg_phase)\n            neg_hidden_probs = _sigmoid(((neg_vis_probs @ self.W) + self.c) / temp_neg_phase)\n\n            # Update parameters\n            dW = (v0.T @ pos_hidden_probs) - (neg_vis_probs.T @ neg_hidden_probs)\n            db = np.sum(v0 - neg_vis_probs, axis=0)\n            dc = np.sum(pos_hidden_probs - neg_hidden_probs, axis=0)\n            \n            self.W += learning_rate * dW / n_samples\n            self.b += learning_rate * db / n_samples\n            self.c += learning_rate * dc / n_samples\n\ndef generate_dataset(N, n, seed=None):\n    \"\"\"Generates a synthetic dataset with two clusters.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    p1 = np.array([1, 1, 1, 0, 0, 0])\n    p2 = np.array([0, 0, 0, 1, 1, 1])\n    \n    data = np.zeros((N, n))\n    \n    n_half = N // 2\n    data[:n_half, :] = p1 + rng.normal(loc=0.0, scale=0.1, size=(n_half, n))\n    data[n_half:, :] = p2 + rng.normal(loc=0.0, scale=0.1, size=(N - n_half, n))\n    \n    return np.clip(data, 0, 1)\n\ndef generate_adversarial(rbm, v_data, epsilon):\n    \"\"\"Generates adversarial examples using FGSM on the free energy.\"\"\"\n    if epsilon == 0.0:\n        return np.copy(v_data)\n        \n    grad = rbm.grad_free_energy(v_data)\n    delta = epsilon * np.sign(grad)\n    v_adv = np.clip(v_data + delta, 0, 1)\n    \n    return v_adv\n\ndef evaluate_robustness(rbm, data, epsilon):\n    \"\"\"Computes the mean increase in free energy under adversarial attack.\"\"\"\n    if epsilon == 0.0:\n        return 0.0\n        \n    data_adv = generate_adversarial(rbm, data, epsilon)\n    \n    F_orig = rbm.free_energy(data)\n    F_adv = rbm.free_energy(data_adv)\n    \n    delta_F = F_adv - F_orig\n    \n    return np.mean(delta_F)\n\ndef solve():\n    # Fixed parameters for the experiment\n    N = 64\n    n_vis = 6\n    n_hid = 4\n    epochs = 1000\n    learning_rate = 0.1\n    master_seed = 42\n\n    test_cases = [\n        (0.0, 1.5),\n        (0.05, 1.5),\n        (0.2, 2.0),\n    ]\n\n    results = []\n    \n    # Generate a single dataset for all test cases\n    dataset = generate_dataset(N, n_vis, seed=master_seed)\n    \n    for i, (epsilon, tau) in enumerate(test_cases):\n        # We must re-initialize and retrain models for each case to ensure\n        # that the comparison is based on the specific tau.\n        # Use a consistent seed for RBM initialization across runs for reproducibility.\n        rbm_init_seed = master_seed + 1 + i\n\n        # Standard RBM (train with tau=1)\n        rbm_std = RBM(n_vis, n_hid, seed=rbm_init_seed)\n        rbm_std.train(dataset, epochs, learning_rate, temp_neg_phase=1.0)\n        \n        # Tempered RBM (train with given tau)\n        # Initialize with the same weights for a fair comparison.\n        rbm_temp = RBM(n_vis, n_hid, seed=rbm_init_seed) # seed ensures identical init\n        rbm_temp.train(dataset, epochs, learning_rate, temp_neg_phase=tau)\n        \n        # Evaluate robustness\n        delta_F_std = evaluate_robustness(rbm_std, dataset, epsilon)\n        delta_F_temp = evaluate_robustness(rbm_temp, dataset, epsilon)\n        \n        # Compare and store result\n        is_more_robust = delta_F_temp = delta_F_std\n        results.append(is_more_robust)\n\n    # Format the final output string\n    # str(True) -> 'True', so no special capitalization logic is needed.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}