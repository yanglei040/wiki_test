{
    "hands_on_practices": [
        {
            "introduction": "A common challenge in designing GAN generators is ensuring high-quality upsampling. While transposed convolutions are a standard building block for increasing spatial resolution, they can sometimes introduce undesirable high-frequency artifacts that manifest as \"checkerboard\" patterns in the generated images. This exercise () provides a hands-on approach to diagnosing this issue by implementing a mathematically grounded metric, the Periodic Subgrid Variance (PSV), to quantify these artifacts. By comparing the output of a transposed convolution layer with a more stable resize-then-convolution alternative, you will develop the skills to not only spot these flaws but also to measure and mitigate them systematically.",
            "id": "3127615",
            "problem": "You are given a task to study checkerboard edge artifacts that can arise from transposed convolution (also called deconvolution) in Generative Adversarial Networks (GANs) for image-to-image translation, and to compare them against a resize-then-convolution alternative. The goal is to define mathematically grounded artifact metrics and a proxy for the sensitivity of a PatchGAN (patch-based discriminator in a Generative Adversarial Network) to such artifacts, and to implement them in a program that runs deterministically on a small synthetic test suite.\n\nBegin from core definitions of discrete convolution and transposed convolution. Let an image be a two-dimensional array of real numbers. Let the convolution of an image with a kernel be defined by the discrete sum\n$$\n(y \\star k)[i,j] \\equiv \\sum_{u}\\sum_{v} y[i-u,j-v]\\,k[u,v],\n$$\nwith zero padding implicitly used where needed. Let the nearest-neighbor upsampling by factor $s$ of an image $x$ be denoted by $U_{s}^{\\mathrm{NN}}(x)$, defined such that each pixel is replicated into an $s \\times s$ block. Let the transposed convolution with stride $s$ be implemented by inserting $(s-1)$ zeros between adjacent pixels along both spatial axes to form a sparse image $Z_{s}(x)$ of shape enlarged by factor $s$, followed by a standard convolution with kernel $k$. The resize-then-convolution alternative is the composition $(U_{s}^{\\mathrm{NN}}(x)) \\star k$ with zero padding to maintain size.\n\nDefine the following artifact metric called the Periodic Subgrid Variance (PSV). Given an image $I$ and stride $s$, for each residue class $g=(a,b)$ with $a \\in \\{0,\\dots,s-1\\}$ and $b \\in \\{0,\\dots,s-1\\}$, let\n$$\n\\mu_g \\equiv \\frac{1}{|\\{(i,j): i \\equiv a \\bmod s,\\; j \\equiv b \\bmod s\\}|}\\sum_{\\substack{i\\equiv a \\bmod s\\\\ j\\equiv b \\bmod s}} I[i,j],\n$$\nand let the global mean be $\\mu \\equiv \\frac{1}{HW}\\sum_{i,j} I[i,j]$, where $H$ and $W$ are the height and width. The Periodic Subgrid Variance is then\n$$\n\\mathrm{PSV}_s(I) \\equiv \\frac{1}{s^2}\\sum_{a=0}^{s-1}\\sum_{b=0}^{s-1}\\left(\\mu_{(a,b)}-\\mu\\right)^2.\n$$\nIntuitively, if there is a checkerboard pattern aligned with the stride-$s$ grid, then $\\mathrm{PSV}_s(I)$ increases, while for a constant image it is $0$.\n\nTo relate to PatchGAN sensitivity, define a proxy sensitivity functional that computes PSV within non-overlapping patches of size $p \\times p$. For a fixed $p$ divisible by $s$, partition the image into $N$ disjoint patches $\\{I^{(n)}\\}_{n=1}^N$. Define the per-patch PSV as $\\mathrm{PSV}_s(I^{(n)})$ and the global variance as $\\sigma^2 \\equiv \\frac{1}{HW}\\sum_{i,j}\\left(I[i,j]-\\mu\\right)^2$. For a fixed threshold $\\tau \\equiv \\beta \\,\\sigma^2$ with a small constant $\\beta \\in (0,1)$, define the sensitivity proxy\n$$\nS_{\\text{patch}}(I; s,p,\\beta)\\equiv \\frac{1}{N}\\sum_{n=1}^N \\mathbf{1}\\left\\{\\mathrm{PSV}_s\\left(I^{(n)}\\right)>\\tau\\right\\}.\n$$\nThis quantity approximates the fraction of patches that would be flagged as artifact-dominated by a patch-based discriminator.\n\nYour program must implement the following pipeline for each test case:\n- Input: a low-resolution image $x$ of shape $L \\times L$ with values in $[0,1]$, an upsampling stride $s$, and a convolution kernel size $k$ with a predefined separable kernel $K$ constructed from a one-dimensional vector $v$ by $K = v v^\\top$, normalized so that $\\sum_{u,v} K[u,v] = 1$.\n- Transposed convolution output: $I_{\\text{deconv}} = Z_s(x) \\star K$ with zero padding and output size exactly $(sL) \\times (sL)$.\n- Resize-then-convolution output: $I_{\\text{resize}} = (U_{s}^{\\mathrm{NN}}(x)) \\star K$ with zero padding and output size exactly $(sL) \\times (sL)$.\n- Compute $G_{\\text{deconv}} \\equiv \\mathrm{PSV}_s(I_{\\text{deconv}})$ and $G_{\\text{resize}} \\equiv \\mathrm{PSV}_s(I_{\\text{resize}})$.\n- Compute the differences $R \\equiv G_{\\text{deconv}} - G_{\\text{resize}}$.\n- Compute $S_{\\text{deconv}} \\equiv S_{\\text{patch}}(I_{\\text{deconv}}; s,p,\\beta)$ and $S_{\\text{resize}} \\equiv S_{\\text{patch}}(I_{\\text{resize}}; s,p,\\beta)$, and the difference $P \\equiv S_{\\text{deconv}} - S_{\\text{resize}}$.\n\nUse the following fixed parameters for all test cases:\n- Stride $s = 2$.\n- Patch size $p = 8$.\n- Sensitivity threshold scale $\\beta = 0.1$.\n- Convolution kernels:\n  - If $k=3$, use $v = [1,2,1]$.\n  - If $k=4$, use $v = [1,3,3,1]$.\nIn both cases, use $K = \\frac{1}{\\sum_{u} \\sum_{v} v[u]v[v]} \\, v v^\\top$ so that $\\sum K = 1$.\n- Convolution is two-dimensional with zero padding and “same” output size.\n\nTest suite:\n- Case $1$ (happy path with expected checkerboard): $L=16$, $k=3$, and $x$ is a binary image with a centered square of ones. Specifically, $x[i,j] = 1$ if $i \\in [L/4, 3L/4)$ and $j \\in [L/4, 3L/4)$, and $0$ otherwise.\n- Case $2$ (kernel divisible by stride, reduced artifacts): $L=16$, $k=4$, and $x$ is the same centered square as in Case $1$.\n- Case $3$ (edge case: constant image): $L=16$, $k=3$, and $x[i,j] = 1$ for all $i,j$.\n- Case $4$ (random texture): $L=16$, $k=3$, and $x$ is a Bernoulli random image with $\\mathbb{P}(x[i,j]=1)=0.5$, generated with fixed random seed $42$.\n\nRequired final output:\n- For each case, compute the pair $(R,P)$ as defined above.\n- Your program should produce a single line of output containing the results as a comma-separated flat list of $8$ real numbers in the order $[R_1,P_1,R_2,P_2,R_3,P_3,R_4,P_4]$, rounded to exactly $6$ decimal places, enclosed in square brackets, for example $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$.\n\nAll array indices are zero-based. There are no physical units. Angles do not appear. Your implementation must be fully deterministic given the test suite and constants above. Use only the specified environment and libraries. The correctness of your solution is judged by the numerical values produced for the test cases.",
            "solution": "The user-provided problem is valid as it is scientifically grounded, well-posed, and objective. It is based on established concepts in deep learning and signal processing, specifically addressing checkerboard artifacts from transposed convolutions in Generative Adversarial Networks (GANs). All mathematical definitions, parameters, and test cases are specified with sufficient precision to permit a unique, verifiable solution.\n\nThe core of the problem is to implement two upsampling-convolution procedures, compute specified artifact metrics for their outputs, and compare the results. The two procedures are:\n1.  **Transposed Convolution**: Represented as zero-insertion followed by a standard convolution. An input image $x$ of size $L \\times L$ is first transformed into a sparse image $Z_s(x)$ of size $(sL-s+1) \\times (sL-s+1)$ by inserting $s-1$ zeros between adjacent pixels. This sparse image is then convolved with a kernel $K$. The convolution operation must be padded to produce a final output $I_{\\text{deconv}}$ of size $(sL) \\times (sL)$.\n2.  **Resize-then-Convolution**: A conceptually simpler alternative where the input image $x$ is first upscaled by a factor of $s$ using nearest-neighbor replication, resulting in an image $U_{s}^{\\mathrm{NN}}(x)$ of size $(sL) \\times (sL)$. This upscaled image is then convolved with the kernel $K$ using padding that preserves the image size (`'same'` convolution), yielding the output $I_{\\text{resize}}$.\n\nThe problem introduces two metrics to quantify artifacts:\n-   **Periodic Subgrid Variance ($\\mathrm{PSV}_s(I)$)**: This metric measures the variance of mean pixel values across $s \\times s$ periodic subgrids of an image $I$. A high $\\mathrm{PSV}_s$ value indicates a strong periodic pattern with a period matching the upsampling stride $s$, which is characteristic of checkerboard artifacts. It is defined as $\\mathrm{PSV}_s(I) \\equiv \\frac{1}{s^2}\\sum_{g}\\left(\\mu_{g}-\\mu\\right)^2$, where $\\mu$ is the global mean and $\\mu_g$ are the means of the $s^2$ subgrids.\n-   **Patch-based Sensitivity ($S_{\\text{patch}}(I; s, p, \\beta)$)**: This metric serves as a proxy for how a PatchGAN discriminator might react to artifacts. It computes the fraction of non-overlapping $p \\times p$ patches in the image for which the local $\\mathrm{PSV}_s$ exceeds a dynamic threshold $\\tau = \\beta \\sigma^2$, where $\\sigma^2$ is the global variance of the image and $\\beta$ is a small constant.\n\nThe solution proceeds as follows for each of the four test cases:\n1.  **Input Generation**: The low-resolution input image $x$ of size $L \\times L$ and the convolution kernel $K$ of size $k \\times k$ are constructed based on the test case parameters. The separable kernel $K$ is derived from a vector $v$ as $K = vv^\\top$ and normalized to sum to $1$.\n2.  **Image Generation**: The high-resolution images $I_{\\text{deconv}}$ and $I_{\\text{resize}}$ are generated according to their definitions. The convolution operation for $I_{\\text{deconv}}$ is implemented using a `'full'` convolution followed by a symmetric crop to achieve the specified output dimension of $(sL) \\times (sL)$. The convolution for $I_{\\text{resize}}$ is implemented using a `'same'` convolution, which naturally produces an output of the same size as its input, $(sL) \\times (sL)$.\n3.  **Metric Computation**:\n    -   The PSV values, $G_{\\text{deconv}} = \\mathrm{PSV}_s(I_{\\text{deconv}})$ and $G_{\\text{resize}} = \\mathrm{PSV}_s(I_{\\text{resize}})$, are calculated. Their difference is $R = G_{\\text{deconv}} - G_{\\text{resize}}$. A positive $R$ indicates that the transposed convolution method produces more checkerboard artifacts than the resize-convolution method.\n    -   The patch-based sensitivity values, $S_{\\text{deconv}} = S_{\\text{patch}}(I_{\\text{deconv}})$ and $S_{\\text{resize}} = S_{\\text{patch}}(I_{\\text{resize}})$, are calculated using the specified parameters $s=2$, $p=8$, and $\\beta=0.1$. Their difference is $P = S_{\\text{deconv}} - S_{\\text{resize}}$. A positive $P$ suggests that a PatchGAN is more likely to identify artifacts in the transposed convolution output.\n4.  **Result Aggregation**: The computed pairs $(R, P)$ for all four test cases are collected and formatted into a single list for the final output. The use of a fixed random seed ensures the result for the random image case is deterministic.",
            "answer": "```python\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef get_kernel(k_size):\n    \"\"\"Constructs a normalized separable 2D kernel.\"\"\"\n    if k_size == 3:\n        v = np.array([1, 2, 1], dtype=float)\n    elif k_size == 4:\n        v = np.array([1, 3, 3, 1], dtype=float)\n    else:\n        raise ValueError(\"Unsupported kernel size\")\n    \n    kernel = np.outer(v, v)\n    kernel /= kernel.sum()\n    return kernel\n\ndef get_input_image(L, image_type, seed=42):\n    \"\"\"Generates the low-resolution input image for a test case.\"\"\"\n    if image_type == 'centered_square':\n        x = np.zeros((L, L), dtype=float)\n        start = L // 4\n        end = 3 * L // 4\n        x[start:end, start:end] = 1.0\n        return x\n    elif image_type == 'constant':\n        return np.ones((L, L), dtype=float)\n    elif image_type == 'random':\n        rng = np.random.default_rng(seed)\n        return rng.choice([0.0, 1.0], size=(L, L), p=[0.5, 0.5])\n    else:\n        raise ValueError(\"Unsupported image type\")\n\ndef upsample_zeros(x, s):\n    \"\"\"Upsamples by inserting s-1 zeros between pixels.\"\"\"\n    L = x.shape[0]\n    up_L = s * L - s + 1\n    z = np.zeros((up_L, up_L), dtype=float)\n    z[::s, ::s] = x\n    return z\n\ndef upsample_nn(x, s):\n    \"\"\"Upsamples using nearest-neighbor replication.\"\"\"\n    return np.kron(x, np.ones((s, s), dtype=float))\n\ndef custom_convolve(img, kernel, output_shape):\n    \"\"\"Performs 2D convolution with padding to achieve a target output size.\"\"\"\n    full_conv = convolve2d(img, kernel, mode='full')\n    \n    crop_total_h = full_conv.shape[0] - output_shape[0]\n    crop_total_w = full_conv.shape[1] - output_shape[1]\n\n    crop_start_h = crop_total_h // 2\n    crop_start_w = crop_total_w // 2\n    \n    crop_end_h = crop_start_h + output_shape[0]\n    crop_end_w = crop_start_w + output_shape[1]\n    \n    return full_conv[crop_start_h:crop_end_h, crop_start_w:crop_end_w]\n\ndef psv(I, s):\n    \"\"\"Computes the Periodic Subgrid Variance (PSV).\"\"\"\n    if I.size == 0:\n        return 0.0\n    mu = I.mean()\n    subgrid_means = []\n    for a in range(s):\n        for b in range(s):\n            subgrid = I[a::s, b::s]\n            if subgrid.size > 0:\n                subgrid_means.append(subgrid.mean())\n            else:\n                subgrid_means.append(mu) \n    \n    psv_val = np.mean((np.array(subgrid_means) - mu)**2)\n    return psv_val\n\ndef s_patch(I, s, p, beta):\n    \"\"\"Computes the patch-based sensitivity proxy.\"\"\"\n    H, W = I.shape\n    num_patches_h = H // p\n    num_patches_w = W // p\n    N = num_patches_h * num_patches_w\n\n    if N == 0:\n        return 0.0\n\n    sigma2 = np.var(I)\n    tau = beta * sigma2\n\n    flagged_patches = 0\n    for i in range(num_patches_h):\n        for j in range(num_patches_w):\n            patch = I[i*p : (i+1)*p, j*p : (j+1)*p]\n            psv_patch = psv(patch, s)\n            if psv_patch > tau:\n                flagged_patches += 1\n    \n    return float(flagged_patches) / N\n\ndef process_case(L, k, image_type, s, p, beta):\n    \"\"\"Processes a single test case and computes (R, P).\"\"\"\n    output_L = s * L\n    \n    x = get_input_image(L, image_type)\n    kernel = get_kernel(k)\n\n    # Transposed convolution path\n    z_x = upsample_zeros(x, s)\n    I_deconv = custom_convolve(z_x, kernel, output_shape=(output_L, output_L))\n    \n    # Resize-then-convolution path\n    u_x = upsample_nn(x, s)\n    I_resize = convolve2d(u_x, kernel, mode='same')\n    \n    # Compute R\n    G_deconv = psv(I_deconv, s)\n    G_resize = psv(I_resize, s)\n    R = G_deconv - G_resize\n    \n    # Compute P\n    S_deconv = s_patch(I_deconv, s, p, beta)\n    S_resize = s_patch(I_resize, s, p, beta)\n    P = S_deconv - S_resize\n    \n    return R, P\n\ndef solve():\n    \"\"\"Main function to run the test suite and print results.\"\"\"\n    # Fixed parameters\n    s = 2\n    p = 8\n    beta = 0.1\n    \n    test_cases = [\n        {'L': 16, 'k': 3, 'image_type': 'centered_square'},\n        {'L': 16, 'k': 4, 'image_type': 'centered_square'},\n        {'L': 16, 'k': 3, 'image_type': 'constant'},\n        {'L': 16, 'k': 3, 'image_type': 'random'},\n    ]\n\n    results = []\n    for case in test_cases:\n        R, P = process_case(case['L'], case['k'], case['image_type'], s, p, beta)\n        results.append(R)\n        results.append(P)\n\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond low-level artifacts, a more profound failure in generative modeling is the lack of generalization. For paired image-to-image translation tasks, an ideal generator learns the underlying mapping between domains, but a poorly trained or over-parameterized one might simply memorize the training examples. This exercise () equips you with a powerful diagnostic tool to distinguish true learning from \"copy-paste\" behavior. By analyzing nearest-neighbor distances in a feature space, you will programmatically test whether a generator's output is closer to its intended target or to a memorized training sample, providing a clear signal for overfitting.",
            "id": "3127647",
            "problem": "You are tasked with designing a principled, programmatic test for overfitting in Conditional Generative Adversarial Networks (cGANs), commonly instantiated as \"pix2pix\", and comparing the behavior to scenarios reminiscent of Cycle-Consistent Generative Adversarial Networks (CycleGANs). The fundamental basis to use is the core definition of overfitting as a failure of generalization under Empirical Risk Minimization, the definition of a generator as a mapping from an input domain to an output domain, and the idea that nearest neighbor relations in a suitably informative feature space can reveal copy-paste behavior indicative of memorization.\n\nStart from the following scientific base:\n- Overfitting is the situation in which a learned mapping $G$ minimizes empirical loss on a tiny dataset but fails to generalize, often by memorizing training outputs. Formally, in supervised mapping from input $x \\in \\mathbb{R}^d$ to output $y \\in \\mathbb{R}^{d}$, overfitting can be revealed if synthetic outputs $G(x)$ are disproportionately closer to a training output $y_{\\text{train}}$ than to the ground-truth $y_{\\text{true}}$ for the same $x$, measured in an informative feature space.\n- A generator $G$ in image-to-image translation (as in \"pix2pix\") is a mapping $G : \\mathcal{X} \\to \\mathcal{Y}$ learned to minimize empirical reconstruction error (and adversarial discrepancy), but here we isolate the reconstruction component using a simplified non-adversarial surrogate to focus on overfitting behavior in paired data. Cycle-Consistent Generative Adversarial Networks (CycleGANs) are unpaired, but detection of copy-paste behavior in feature space is still relevant.\n- Nearest neighbor relations in a feature space $\\phi : \\mathbb{R}^{d} \\to \\mathbb{R}^{k}$ can serve as a well-tested method to detect copying. If $G(x)$ equals or is unusually close to some training output $y_{\\text{train}}$, then the nearest neighbor distance in $\\phi$-space to training outputs, compared against the distance to the ground-truth $y_{\\text{true}}$, will be discriminative.\n- Ridge regression (also known as $\\ell_2$-regularized least squares) provides a closed-form linear estimator that minimizes $$\\sum_{i=1}^{n_{\\text{train}}} \\|W x_i - y_i\\|_2^2 + \\lambda \\|W\\|_F^2$$ and is a scientifically grounded surrogate for a generalizing $G$ when the true mapping is approximately linear.\n\nYour program must:\n1. Synthesize paired datasets $(x, y)$ where $x \\in \\mathbb{R}^{d}$ represents an input image vector and $y \\in \\mathbb{R}^{d}$ represents the target image vector. Use a fixed linear ground-truth mapping $S \\in \\mathbb{R}^{d \\times d}$ and additive Gaussian noise to generate targets: $$y = S x + \\epsilon,\\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_d).$$\n2. Implement two generator modes:\n   - A \"nearest-neighbor memorizer\" mode, which, given a test input $x$, returns the training target $y_{\\text{train}}$ of the nearest training input under $\\ell_2$ distance. This simulates copy-paste behavior consistent with severe overfitting on tiny datasets.\n   - A \"ridge regression\" mode, which fits $$W^\\star = \\arg\\min_W \\sum_{i=1}^{n_{\\text{train}}} \\|W x_i - y_i\\|_2^2 + \\lambda \\|W\\|_F^2,$$ and predicts $G(x) = W^\\star x$.\n3. Construct a fixed feature extractor $\\phi(y) = R^\\top y$ where $R \\in \\mathbb{R}^{d \\times k}$ is a random Gaussian matrix with entries i.i.d. $\\mathcal{N}(0, 1/k)$ and fixed random seed. This random projection is used to compute distances that reveal copy-paste behavior while being independent of $G$.\n4. For each test input $x_{\\text{test}}$ with ground-truth $y_{\\text{true}}$, compute:\n   - The \"true\" distance $$d_{\\text{true}} = \\|\\phi(G(x_{\\text{test}})) - \\phi(y_{\\text{true}})\\|_2.$$\n   - The nearest neighbor training distance $$d_{\\text{nn}} = \\min_{j} \\|\\phi(G(x_{\\text{test}})) - \\phi(y^{(j)}_{\\text{train}})\\|_2.$$\n   Flag the sample as copy-paste if $$d_{\\text{nn}} < \\alpha \\cdot d_{\\text{true}},$$ where $\\alpha \\in (0, 1)$ is a detection margin parameter.\n5. Aggregate sample-level flags into a case-level decision by declaring \"overfitting detected\" if the fraction of flagged test samples is at least $\\tau$.\n\nUse the following test suite of parameter sets to ensure coverage across cases. For each case, inputs are synthetic and seeds must be fixed internally for reproducibility. The output domain dimension equals the input domain dimension $d$ in all cases.\n\n- Case $1$ (happy-path overfitting): mode \"nn\", $n_{\\text{train}} = 4$, $n_{\\text{test}} = 8$, $d = 64$, $k = 16$, $\\sigma = 0.2$, $\\alpha = 0.7$, $\\tau = 0.5$.\n- Case $2$ (generalization via ridge): mode \"ridge\", $n_{\\text{train}} = 64$, $n_{\\text{test}} = 16$, $d = 64$, $k = 16$, $\\sigma = 0.2$, $\\lambda = 1.0$, $\\alpha = 0.7$, $\\tau = 0.5$.\n- Case $3$ (edge case extreme memorization): mode \"nn\", $n_{\\text{train}} = 1$, $n_{\\text{test}} = 10$, $d = 64$, $k = 8$, $\\sigma = 0.5$, $\\alpha = 0.7$, $\\tau = 0.5$.\n\nProgram requirements:\n- Synthesize data using fixed seeds to ensure determinism. Use a single fixed mapping $S$ across all cases, constructed once with a fixed seed, and a single fixed feature projection $R$ across all cases, constructed once with a fixed seed. Within each case, use a case-specific seed for drawing $x$ and noise.\n- Implement the generator modes and the detection logic exactly as stated.\n- For each case, output a boolean indicating whether overfitting is detected (as defined in item $5$).\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True]\").\n\nNo physical units, angles, or percentages appear in this problem; therefore, no unit conversion is required. All randomness must be seeded so that results are deterministic.",
            "solution": "The problem requires the design and implementation of a programmatic test for overfitting in generative models, using a simplified, principled framework. The test aims to distinguish between a memorizing generator, which mimics severe overfitting, and a generalizing generator, modeled by ridge regression. The core of the detection mechanism lies in analyzing nearest-neighbor relationships in a projected feature space.\n\nThe solution will be developed through the following steps:\n1.  Establishment of the synthetic data generation process based on a ground-truth linear mapping.\n2.  Definition of the feature space projection.\n3.  Implementation of the two specified generator models: a nearest-neighbor memorizer and a ridge regression generalizer.\n4.  Formalization of the overfitting detection logic based on comparative distances in the feature space.\n5.  Application of the complete procedure to the specified test cases.\n\n**1. Data Generation Model**\n\nWe synthesize paired datasets $(X, Y)$ where inputs $x \\in \\mathbb{R}^d$ and outputs $y \\in \\mathbb{R}^d$ are represented as column vectors. The relationship between them is defined by a fixed, ground-truth linear transformation $S \\in \\mathbb{R}^{d \\times d}$ corrupted by additive Gaussian noise. The model is:\n$$y = S x + \\epsilon$$\nwhere $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_d)$ is a noise vector, with $I_d$ being the $d \\times d$ identity matrix and $\\sigma$ controlling the noise level.\n\nFor each test case, we generate a training set $\\{ (x^{(j)}_{\\text{train}}, y^{(j)}_{\\text{train}}) \\}_{j=1}^{n_{\\text{train}}}$ and a test set $\\{ (x^{(i)}_{\\text{test}}, y^{(i)}_{\\text{true}}) \\}_{i=1}^{n_{\\text{test}}}$. The inputs $x_{\\text{train}}$ and $x_{\\text{test}}$ are drawn from a standard normal distribution. The matrix $S$ is generated once from a standard normal distribution with a fixed random seed to ensure consistency across all test cases.\n\n**2. Feature Space Projection**\n\nTo analyze the structure of the generated outputs, we project them into a lower-dimensional feature space using a fixed linear map $\\phi$. This map is defined as:\n$$\\phi(y) = R^\\top y$$\nHere, $R \\in \\mathbb{R}^{d \\times k}$ is a random projection matrix where each entry is drawn independently from a normal distribution $\\mathcal{N}(0, 1/k)$. Such random projections are known to approximately preserve pairwise distances, a principle related to the Johnson-Lindenstrauss lemma. The matrix $R$ is also generated once with a fixed seed and reused across all cases. This ensures that the feature space is independent of the training process and provides a consistent basis for comparison.\n\n**3. Generator Models**\n\nWe implement two distinct generator models to simulate different learning behaviors.\n\n**a) Nearest-Neighbor Memorizer (`nn` mode)**\nThis generator, denoted $G_{\\text{nn}}$, represents an extreme case of overfitting where the model has simply memorized the training set. For any given test input $x_{\\text{test}}$, it finds the training input $x^{(j)}_{\\text{train}}$ that is closest in Euclidean distance and outputs its corresponding training target $y^{(j)}_{\\text{train}}$. Formally:\n$$G_{\\text{nn}}(x_{\\text{test}}) = y^{(j^\\star)}_{\\text{train}}, \\quad \\text{where} \\quad j^\\star = \\arg\\min_{j \\in \\{1, \\dots, n_{\\text{train}}\\}} \\|x_{\\text{test}} - x^{(j)}_{\\text{train}}\\|_2$$\nThis behavior is characteristic of lookup-table-like memorization, a hallmark of failed generalization on small datasets.\n\n**b) Ridge Regression Generalizer (`ridge` mode)**\nThis generator, $G_{\\text{ridge}}$, models a system that learns a general linear mapping from inputs to outputs while being regularized to prevent overfitting. It finds a matrix $W^\\star \\in \\mathbb{R}^{d \\times d}$ that minimizes the $\\ell_2$-regularized sum of squared errors over the training data. The objective function is:\n$$L(W) = \\sum_{j=1}^{n_{\\text{train}}} \\|W x^{(j)}_{\\text{train}} - y^{(j)}_{\\text{train}}\\|_2^2 + \\lambda \\|W\\|_F^2$$\nwhere $\\|W\\|_F$ is the Frobenius norm of $W$ and $\\lambda > 0$ is the regularization parameter. Let $X_{\\text{train}} \\in \\mathbb{R}^{d \\times n_{\\text{train}}}$ and $Y_{\\text{train}} \\in \\mathbb{R}^{d \\times n_{\\text{train}}}$ be the matrices whose columns are the training inputs and outputs, respectively. The closed-form solution for the optimal weight matrix $W^\\star$ that minimizes $L(W)$ is:\n$$W^\\star = Y_{\\text{train}} X_{\\text{train}}^\\top (X_{\\text{train}} X_{\\text{train}}^\\top + \\lambda I_d)^{-1}$$\nThe generator's output is then a linear transformation of the input:\n$$G_{\\text{ridge}}(x_{\\text{test}}) = W^\\star x_{\\text{test}}$$\nThis models a generalizing system that captures the underlying structure ($S$) from the data, with the regularization term $\\lambda$ helping to mitigate the influence of noise $\\epsilon$.\n\n**4. Overfitting Detection Criterion**\n\nThe test for overfitting is performed on a sample-by-sample basis for the entire test set. For each test input $x_{\\text{test}}$ with its corresponding ground-truth output $y_{\\text{true}}$, we first compute the generator's output $y_{\\text{gen}} = G(x_{\\text{test}})$. We then measure two key distances in the feature space $\\phi$:\n\n1.  **True Distance ($d_{\\text{true}}$)**: The Euclidean distance between the feature representation of the generated output and the feature representation of the ground-truth output.\n    $$d_{\\text{true}} = \\|\\phi(y_{\\text{gen}}) - \\phi(y_{\\text{true}})\\|_2$$\n    This quantifies the generator's error with respect to the true target.\n\n2.  **Nearest-Neighbor Training Distance ($d_{\\text{nn}}$)**: The Euclidean distance from the feature representation of the generated output to that of the closest training set output.\n    $$d_{\\text{nn}} = \\min_{j \\in \\{1, \\dots, n_{\\text{train}}\\}} \\|\\phi(y_{\\text{gen}}) - \\phi(y^{(j)}_{\\text{train}})\\|_2$$\n    This quantifies how closely the generated output resembles an item from the training set.\n\nA test sample is flagged as a \"copy-paste\" instance if its generated output is significantly closer to a training example than to its own ground truth. The formal condition is:\n$$d_{\\text{nn}} < \\alpha \\cdot d_{\\text{true}}$$\nwhere $\\alpha \\in (0, 1)$ is a sensitivity margin. A small $d_{\\text{nn}}$ suggests that $y_{\\text{gen}}$ is very similar to some $y_{\\text{train}}$, a sign of memorization. If this similarity is substantially greater (i.e., the distance is smaller by a factor of $\\alpha$) than its similarity to the actual correct answer, we classify it as an artifact of overfitting.\n\n**5. Case-Level Decision**\n\nFinally, the sample-level flags are aggregated to make a decision for the entire test case. The fraction of test samples flagged as copy-paste is computed. If this fraction meets or exceeds a predefined threshold $\\tau$, the generator is considered to be overfitting for that specific test case.\n\n-   **Hypothesis for `nn` mode**: The output $y_{\\text{gen}}$ is one of the training samples $y^{(j^\\star)}_{\\text{train}}$. Therefore, $\\phi(y_{\\text{gen}})$ is one of the $\\phi(y^{(j)}_{\\text{train}})$. This implies $d_{\\text{nn}}$ will be exactly $0$. As long as the noise ensures $y_{\\text{gen}} \\neq y_{\\text{true}}$, $d_{\\text{true}}$ will be positive. The condition $0 < \\alpha \\cdot d_{\\text{true}}$ will be met, leading to a high fraction of flagged samples and a verdict of \"overfitting detected\".\n-   **Hypothesis for `ridge` mode**: The generator $G_{\\text{ridge}}$ learns a smooth, generalized mapping. Its output $y_{\\text{gen}}$ is expected to be close to the true conditional mean $S x_{\\text{test}}$, and thus close to $y_{\\text{true}}$. The distance $d_{\\text{true}}$ should be small. There is no structural reason for $y_{\\text{gen}}$ to be abnormally close to any specific training sample, so $d_{\\text{nn}}$ is not expected to be systematically smaller than $d_{\\text{true}}$. The fraction of flagged samples should be low, leading to a verdict of \"overfitting not detected\".\n\nThis completes the principled design of the test. The subsequent implementation will programmatically execute these steps.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Implements and runs a programmatic test for overfitting in generative models.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy-path overfitting)\n        {\"mode\": \"nn\", \"n_train\": 4, \"n_test\": 8, \"d\": 64, \"k\": 16, \"sigma\": 0.2, \"lambda_reg\": None, \"alpha\": 0.7, \"tau\": 0.5, \"case_seed\": 101},\n        # Case 2 (generalization via ridge)\n        {\"mode\": \"ridge\", \"n_train\": 64, \"n_test\": 16, \"d\": 64, \"k\": 16, \"sigma\": 0.2, \"lambda_reg\": 1.0, \"alpha\": 0.7, \"tau\": 0.5, \"case_seed\": 202},\n        # Case 3 (edge case extreme memorization)\n        {\"mode\": \"nn\", \"n_train\": 1, \"n_test\": 10, \"d\": 64, \"k\": 8, \"sigma\": 0.5, \"lambda_reg\": None, \"alpha\": 0.7, \"tau\": 0.5, \"case_seed\": 303},\n    ]\n\n    # Use max dimensions from test cases for global matrices for consistency.\n    max_d = max(case['d'] for case in test_cases)\n    max_k = max(case['k'] for case in test_cases)\n\n    # 1. Synthesize fixed global mapping S and feature projection R\n    # These are created once and reused to ensure consistency.\n    s_seed = 42\n    r_seed = 84\n    rng_s = np.random.default_rng(s_seed)\n    rng_r = np.random.default_rng(r_seed)\n    \n    # Ground-truth mapping S\n    S = rng_s.standard_normal(size=(max_d, max_d))\n    \n    # Feature projection matrix R\n    R = rng_r.normal(0, 1 / np.sqrt(max_k), size=(max_d, max_k))\n\n    results = []\n    \n    for case in test_cases:\n        mode = case[\"mode\"]\n        n_train = case[\"n_train\"]\n        n_test = case[\"n_test\"]\n        d = case[\"d\"]\n        k = case[\"k\"]\n        sigma = case[\"sigma\"]\n        lambda_reg = case[\"lambda_reg\"]\n        alpha = case[\"alpha\"]\n        tau = case[\"tau\"]\n        case_seed = case[\"case_seed\"]\n\n        rng_case = np.random.default_rng(case_seed)\n\n        # Slice global matrices to match case dimensions\n        S_case = S[:d, :d]\n        R_case = R[:d, :k]\n\n        # Synthesize a dataset for the current case\n        # Vectors are columns, so shapes are (d, n)\n        X_train = rng_case.standard_normal(size=(d, n_train))\n        noise_train = rng_case.normal(0, sigma, size=(d, n_train))\n        Y_train = S_case @ X_train + noise_train\n\n        X_test = rng_case.standard_normal(size=(d, n_test))\n        noise_test = rng_case.normal(0, sigma, size=(d, n_test))\n        Y_true = S_case @ X_test + noise_test\n\n        Y_gen = np.zeros_like(Y_true)\n\n        # 2. Implement generator modes\n        if mode == \"nn\":\n            for i in range(n_test):\n                x_test_i = X_test[:, i]\n                # Find nearest training input\n                distances = np.linalg.norm(X_train - x_test_i[:, np.newaxis], axis=0)\n                nn_idx = np.argmin(distances)\n                # Return the corresponding training target\n                Y_gen[:, i] = Y_train[:, nn_idx]\n\n        elif mode == \"ridge\":\n            # Solve W(XX^T + lambda*I) = YX^T\n            # Numerically stable approach using scipy.linalg.solve for A*W_T = B_T\n            XT = X_train.T\n            XXT = X_train @ XT\n            A = XXT + lambda_reg * np.identity(d)\n            B = Y_train @ XT\n            \n            try:\n                # Solve A @ W.T = B.T for W.T\n                W_star_T = linalg.solve(A, B.T, assume_a='sym')\n                W_star = W_star_T.T\n            except linalg.LinAlgError:\n                # Fallback to pseudoinverse if singular, though unlikely with regularization\n                pinv_A = linalg.pinv(A)\n                W_star = B @ pinv_A\n            \n            Y_gen = W_star @ X_test\n        \n        # 3. Construct feature extractor application\n        # Project all relevant outputs into the feature space: phi(y) = R.T @ y\n        Phi_gen = R_case.T @ Y_gen\n        Phi_true = R_case.T @ Y_true\n        Phi_train = R_case.T @ Y_train\n        \n        flagged_count = 0\n        \n        # 4. Compute distances and apply detection logic for each test sample\n        for i in range(n_test):\n            phi_gen_i = Phi_gen[:, i]\n            phi_true_i = Phi_true[:, i]\n            \n            # True distance\n            d_true = np.linalg.norm(phi_gen_i - phi_true_i)\n\n            # Nearest neighbor training distance\n            nn_distances = np.linalg.norm(Phi_train - phi_gen_i[:, np.newaxis], axis=0)\n            d_nn = np.min(nn_distances)\n\n            # Avoid division by zero if d_true is 0\n            if d_true > 1e-9:\n                if d_nn < alpha * d_true:\n                    flagged_count += 1\n            # If d_true is effectively zero, the generated output is perfect.\n            # d_nn must also be effectively zero for the condition to hold, meaning\n            # the perfect output is also a training sample. This is a valid copy.\n            elif d_nn < 1e-9:\n                flagged_count += 1\n\n        # 5. Aggregate flags and make a case-level decision\n        flagged_fraction = flagged_count / n_test\n        overfitting_detected = flagged_fraction >= tau\n        results.append(overfitting_detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Unpaired image-to-image translation, pioneered by models like CycleGAN, relies on the clever cycle-consistency loss to learn mappings without corresponding pairs. However, this objective can be exploited in non-obvious ways. This practice explores a fascinating failure mode where the network learns a form of steganography: it hides information about the original input within the translated image to perfectly satisfy the cycle-consistency constraint, all without learning the desired semantic translation. In this exercise (), you will model this \"cheating\" behavior as a communication channel, using principles from information theory to quantify the hidden information flow and reveal the limitations of relying solely on proxy loss functions.",
            "id": "3127696",
            "problem": "You are tasked with constructing and analyzing a synthetic scenario that demonstrates a failure mode of Cycle-Consistent Generative Adversarial Networks (CycleGAN) for image-to-image translation, and designing a principled penalty to mitigate it. The context is the following. A Generative Adversarial Network (GAN) consists of a generator and a discriminator trained in opposition to model a target distribution. A Cycle-Consistent Generative Adversarial Network (CycleGAN) augments this with two generators, denoted $F$ and $G$, that map between two domains $X$ and $Y$, together with a cycle-consistency constraint encouraging $G(F(x)) \\approx x$ for $x \\sim p_X$ and $F(G(y)) \\approx y$ for $y \\sim p_Y$. Consider an adversarial dataset where the distribution $p_X$ contains images $x$ that are pairs of content and machine-readable code: each $x$ has a content region and an embedded \"Quick Response (QR)\"-like code region that deterministically encodes the content of $x$ itself. This creates a potential failure mode: the generator $F$ can hide the content of $x$ in a high-frequency microtexture matching the marginal statistics of $p_Y$, and the generator $G$ can decode the hidden code to reconstruct $x$, achieving low cycle-consistency error without performing meaningful translation.\n\nFormulate a mathematically precise toy model of this hiding and decoding channel and compute two quantities for various parameter settings. Your model must satisfy the following conditions.\n\n1. Let $x \\in \\{0,1\\}^{m \\times m}$ denote the content region bits, treated as independent and identically distributed Bernoulli random variables with parameter $1/2$. Let $k = m^2$ denote the number of content bits. Define a hiding mechanism where $F$ encodes each content bit $b \\in \\{0,1\\}$ into a real-valued microtexture symbol $s \\in \\mathbb{R}$ using amplitude coding $s = a \\cdot (2b - 1)$, where $a > 0$ is a design amplitude. Model the corruption between $F$ and $G$ as additive Gaussian noise $n \\sim \\mathcal{N}(0, \\sigma^2)$ applied independently per symbol, yielding $z = s + n$. The decoder in $G$ uses a symmetric erasure threshold policy with threshold $\\tau \\ge 0$: if $|z| < \\tau$ the bit is treated as erased; otherwise the bit is decoded by the sign of $z$, that is, $\\hat{b} = \\mathbb{I}[z \\ge 0]$ for $|z| \\ge \\tau$. In case of erasure, let $G$ default to decoding $\\hat{b} = 0$.\n2. Under this channel, derive expressions for the following probabilities per bit in terms of $a$, $\\sigma$, and $\\tau$ using the standard normal cumulative distribution function, denoted $\\Phi$:\n   a. The erasure probability $p_{\\mathrm{erase}}$, that is, the probability that $|z| < \\tau$.\n   b. The misclassification probability when the true bit is $1$, denoted $p_{\\mathrm{mis1}}$, that is, the probability that $z \\le -\\tau$ under $b=1$.\n   c. The misclassification probability when the true bit is $0$, denoted $p_{\\mathrm{mis0}}$, that is, the probability that $z \\ge \\tau$ under $b=0$.\n3. Using the probabilities in item $2$, compute the expected cycle-consistency error per bit, defined as the expected absolute difference $\\mathbb{E}[|b - \\hat{b}|]$ under the described decoding policy and assuming $b$ is uniformly distributed on $\\{0,1\\}$. Express this expected cycle-consistency error in terms of $p_{\\mathrm{erase}}$, $p_{\\mathrm{mis1}}$, and $p_{\\mathrm{mis0}}$, and multiply by $k$ to obtain the expected cycle-consistency error over all $k$ bits of the content region.\n4. Design an information bottleneck penalty using the Mutual Information (MI) between the input content $x$ and the decoded output $G(F(x))$, denoted $I(x; G(F(x)))$. For analytical tractability, approximate the microtexture channel by a Binary Erasure Channel (BEC) whose erasure probability equals the per-bit erasure probability $p_{\\mathrm{erase}}$ computed in item $2$, and whose output symbol retains one bit of information when not erased. Under this BEC approximation and assuming independent bits, compute an estimate of the MI in bits as a function of $k$ and $p_{\\mathrm{erase}}$.\n\nImplement a program that, for each specified parameter setting $(a, \\sigma, \\tau, k)$, computes:\n- The expected cycle-consistency error over $k$ bits as a float.\n- The information bottleneck penalty $I(x;G(F(x)))$ in bits as a float under the BEC approximation.\n\nYour program must use the following test suite and produce the final output exactly in the required format. For each test case, compute the two floats in the order described above, and aggregate them into a single flat list. The test suite parameters are:\n- Case $1$ (happy path): $a = 1.0$, $\\sigma = 0.1$, $\\tau = 0.2$, $k = 256$.\n- Case $2$ (moderate noise): $a = 0.3$, $\\sigma = 0.7$, $\\tau = 0.3$, $k = 256$.\n- Case $3$ (boundary threshold): $\\tau = 0.0$ with $a = 0.5$, $\\sigma = 0.5$, $k = 256$.\n- Case $4$ (edge case, high noise and small amplitude): $a = 0.1$, $\\sigma = 1.2$, $\\tau = 0.4$, $k = 256$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$[\\text{error\\_case1}, I\\_\\text{case1}, \\text{error\\_case2}, I\\_\\text{case2}, \\text{error\\_case3}, I\\_\\text{case3}, \\text{error\\_case4}, I\\_\\text{case4}]$.",
            "solution": "The user has provided a problem concerning a failure mode in CycleGANs, which is to be analyzed using a simplified, mathematically tractable model. The task is to derive and compute two key metrics: the expected cycle-consistency error and an information-theoretic penalty, for several parameter sets. The problem is well-posed, scientifically grounded in probability theory and information theory, and all necessary components for a unique solution are provided.\n\nThe solution proceeds in three stages. First, the probabilities of key events in the defined bit-level communication channel (erasure and misclassification) are derived. Second, these probabilities are used to formulate an expression for the expected cycle-consistency error. Third, an approximation based on a Binary Erasure Channel is used to formulate the information bottleneck penalty.\n\n**1. Derivation of Per-Bit Channel Probabilities**\n\nThe problem defines a communication channel model for a single bit $b \\in \\{0,1\\}$. The bit is encoded via amplitude modulation into a signal $s = a(2b-1)$, where $a>0$. This means if $b=1$, the signal is $s=+a$, and if $b=0$, the signal is $s=-a$. This signal is corrupted by additive white Gaussian noise, $n \\sim \\mathcal{N}(0, \\sigma^2)$, resulting in a received signal $z=s+n$.\n\nLet $\\Phi(\\cdot)$ denote the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0,1)$. The CDF of a general normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ at point $x$ is given by $\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)$.\n\nWhen $b=1$, the received signal is $z_1 = a+n$, which follows the distribution $\\mathcal{N}(a, \\sigma^2)$.\nWhen $b=0$, the received signal is $z_0 = -a+n$, which follows the distribution $\\mathcal{N}(-a, \\sigma^2)$.\n\na. **Erasure Probability ($p_{\\mathrm{erase}}$)**: An erasure event is defined as $|z| < \\tau$ for a given threshold $\\tau \\ge 0$. The input bit $b$ is a Bernoulli random variable with parameter $1/2$. The total erasure probability is found by the law of total probability:\n$$p_{\\mathrm{erase}} = P(|z| < \\tau) = P(|z| < \\tau | b=0)P(b=0) + P(|z| < \\tau | b=1)P(b=1)$$\nDue to the symmetry of the problem (the distributions for $b=0$ and $b=1$ are reflections of each other about $0$, and the erasure interval $(-\\tau, \\tau)$ is symmetric), the conditional probabilities are equal: $P(|z_0| < \\tau) = P(|z_1| < \\tau)$.\nTherefore, we can compute $p_{\\mathrm{erase}}$ using just one of the conditional probabilities:\n$$p_{\\mathrm{erase}} = P(|z_1| < \\tau) = P(-\\tau < z_1 < \\tau)$$\n$$p_{\\mathrm{erase}} = P(z_1 < \\tau) - P(z_1 \\le -\\tau)$$\nUsing the CDF for $\\mathcal{N}(a, \\sigma^2)$, this becomes:\n$$p_{\\mathrm{erase}} = \\Phi\\left(\\frac{\\tau - a}{\\sigma}\\right) - \\Phi\\left(\\frac{-\\tau - a}{\\sigma}\\right)$$\n\nb. **Misclassification Probabilities ($p_{\\mathrm{mis1}}, p_{\\mathrm{mis0}}$)**: Misclassification occurs when the received signal falls into the wrong decision region, excluding the erasure region.\nFor $b=1$, the true bit is $1$. The decoder outputs $\\hat{b}=0$ if $z_1 \\le -\\tau$. The misclassification probability $p_{\\mathrm{mis1}}$ is:\n$$p_{\\mathrm{mis1}} = P(z_1 \\le -\\tau) = \\Phi\\left(\\frac{-\\tau - a}{\\sigma}\\right)$$\nFor $b=0$, the true bit is $0$. The decoder outputs $\\hat{b}=1$ if $z_0 \\ge \\tau$. The misclassification probability $p_{\\mathrm{mis0}}$ is:\n$$p_{\\mathrm{mis0}} = P(z_0 \\ge \\tau) = 1 - P(z_0 < \\tau) = 1 - \\Phi\\left(\\frac{\\tau - (-a)}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{\\tau+a}{\\sigma}\\right)$$\nUsing the property $\\Phi(-x) = 1 - \\Phi(x)$, this simplifies to:\n$$p_{\\mathrm{mis0}} = \\Phi\\left(-\\frac{a+\\tau}{\\sigma}\\right) = \\Phi\\left(\\frac{-a-\\tau}{\\sigma}\\right)$$\nAs expected from the problem's symmetry, $p_{\\mathrm{mis0}} = p_{\\mathrm{mis1}}$. We denote this common probability by $p_{\\mathrm{mis}}$.\n\n**2. Derivation of Expected Cycle-Consistency Error**\n\nThe cycle-consistency error for a single bit is defined as $\\mathbb{E}[|b - \\hat{b}|]$. As the error magnitude $|b-\\hat{b}|$ is either $0$ or $1$, this expectation is equal to the total probability of error, $P(b \\neq \\hat{b})$.\n$$P(b \\neq \\hat{b}) = P(\\hat{b} \\neq b | b=0)P(b=0) + P(\\hat{b} \\neq b | b=1)P(b=1)$$\nWith $P(b=0) = P(b=1) = 1/2$:\n$$\\mathbb{E}[|b - \\hat{b}|] = \\frac{1}{2} P(\\hat{b}=1|b=0) + \\frac{1}{2} P(\\hat{b}=0|b=1)$$\n- If $b=0$, an error occurs if $\\hat{b}=1$. This happens if $z_0 \\ge \\tau$, with probability $p_{\\mathrm{mis0}} = p_{\\mathrm{mis}}$.\n- If $b=1$, an error occurs if $\\hat{b}=0$. This can happen in two disjoint ways: either the bit is misclassified ($z_1 \\le -\\tau$), or it is erased ($|z_1|<\\tau$) and defaults to $\\hat{b}=0$.\n$$P(\\hat{b}=0|b=1) = P(z_1 \\le -\\tau) + P(|z_1| < \\tau) = p_{\\mathrm{mis1}} + p_{\\mathrm{erase}} = p_{\\mathrm{mis}} + p_{\\mathrm{erase}}$$\nCombining these, the per-bit expected error is:\n$$\\mathbb{E}[|b - \\hat{b}|] = \\frac{1}{2} p_{\\mathrm{mis}} + \\frac{1}{2} (p_{\\mathrm{mis}} + p_{\\mathrm{erase}}) = p_{\\mathrm{mis}} + \\frac{1}{2} p_{\\mathrm{erase}}$$\nSubstituting the derived expressions for $p_{\\mathrm{mis}}$ and $p_{\\mathrm{erase}}$:\n$$\\mathbb{E}[|b - \\hat{b}|] = \\Phi\\left(\\frac{-a-\\tau}{\\sigma}\\right) + \\frac{1}{2}\\left[ \\Phi\\left(\\frac{\\tau-a}{\\sigma}\\right) - \\Phi\\left(\\frac{-a-\\tau}{\\sigma}\\right) \\right]$$\n$$\\mathbb{E}[|b - \\hat{b}|] = \\frac{1}{2}\\left[ \\Phi\\left(\\frac{-a-\\tau}{\\sigma}\\right) + \\Phi\\left(\\frac{\\tau-a}{\\sigma}\\right) \\right]$$\nThe total expected cycle-consistency error over $k$ independent bits is simply $k$ times the per-bit error.\n\n**3. Derivation of the Information Bottleneck Penalty**\n\nThe penalty is defined as the mutual information (MI) $I(x; G(F(x)))$, where $x$ is the input content and $G(F(x))$ is the reconstructed content. As the input bits are i.i.d. and the channel acts independently on each bit, the total MI is $k$ times the per-bit MI: $I(x; G(F(x))) = k \\cdot I(b; \\hat{b})$.\n\nThe problem specifies approximating the channel as a Binary Erasure Channel (BEC) with an erasure probability $p_e$ equal to our derived $p_{\\mathrm{erase}}$. In a BEC, a bit is either transmitted correctly with probability $1-p_e$ or erased with probability $p_e$. The MI for a BEC with a uniform binary input $b$ is given by $I(b; \\hat{b}) = 1 - p_e$ bits.\n\nThus, the information bottleneck penalty is approximated as:\n$$I(x; G(F(x))) \\approx k \\cdot (1 - p_{\\mathrm{erase}})$$\nSubstituting the expression for $p_{\\mathrm{erase}}$:\n$$I(x; G(F(x))) \\approx k \\cdot \\left(1 - \\left[ \\Phi\\left(\\frac{\\tau - a}{\\sigma}\\right) - \\Phi\\left(\\frac{-\\tau - a}{\\sigma}\\right) \\right]\\right)$$\n\n**Summary of Formulas for Implementation**\nFor each parameter set $(a, \\sigma, \\tau, k)$:\n1.  Calculate argument values: $v_1 = \\frac{-a-\\tau}{\\sigma}$ and $v_2 = \\frac{\\tau-a}{\\sigma}$.\n2.  Compute CDF values: $\\phi_1 = \\Phi(v_1)$ and $\\phi_2 = \\Phi(v_2)$.\n3.  Compute total expected error: $E = k \\cdot \\frac{1}{2}(\\phi_1 + \\phi_2)$.\n4.  Compute erasure probability: $p_{\\mathrm{erase}} = \\phi_2 - \\phi_1$.\n5.  Compute information penalty: $I = k \\cdot (1 - p_{\\mathrm{erase}})$.\nThese formulas will be implemented in the final program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the expected cycle-consistency error and mutual information penalty\n    for a toy model of a CycleGAN failure mode.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (a, sigma, tau, k)\n    test_cases = [\n        (1.0, 0.1, 0.2, 256),  # Case 1 (happy path)\n        (0.3, 0.7, 0.3, 256),  # Case 2 (moderate noise)\n        (0.5, 0.5, 0.0, 256),  # Case 3 (boundary threshold)\n        (0.1, 1.2, 0.4, 256),  # Case 4 (edge case, high noise and small amplitude)\n    ]\n\n    results = []\n    for case in test_cases:\n        a, sigma, tau, k = case\n\n        # 1. Calculate argument values for the standard normal CDF (Phi).\n        # These correspond to the normalized thresholds.\n        # arg_mis: argument for p_mis calculation\n        # arg_erase_upper: upper bound for p_erase calculation\n        arg_mis = (-a - tau) / sigma\n        arg_erase_upper = (tau - a) / sigma\n\n        # 2. Compute the required CDF values using scipy.stats.norm.cdf.\n        phi_mis = norm.cdf(arg_mis)\n        phi_erase_upper = norm.cdf(arg_erase_upper)\n\n        # 3. Compute the per-bit probabilities.\n        # p_mis is the probability of misclassifying a bit (0->1 or 1->0) without erasure.\n        # The problem shows p_mis0 = p_mis1, so we use a single p_mis.\n        p_mis = phi_mis\n        \n        # p_erase is the probability of a bit being erased (decoded value defaults to 0).\n        # p_erase = Phi((tau-a)/sigma) - Phi((-tau-a)/sigma)\n        p_erase = phi_erase_upper - phi_mis\n\n        # 4. Compute the expected cycle-consistency error over k bits.\n        # Per-bit error is p_mis + 0.5 * p_erase, which simplifies to\n        # 0.5 * [Phi((-a-tau)/sigma) + Phi((tau-a)/sigma)].\n        total_expected_error = k * 0.5 * (phi_mis + phi_erase_upper)\n\n        # 5. Compute the information bottleneck penalty (Mutual Information) in bits.\n        # This is approximated by modeling the channel as a Binary Erasure Channel (BEC)\n        # with erasure probability p_erase. The MI for k bits is k * (1 - p_erase).\n        mi_penalty = k * (1.0 - p_erase)\n\n        results.append(total_expected_error)\n        results.append(mi_penalty)\n\n    # Final print statement in the exact required format.\n    # Produces a single line of output: [error1,I1,error2,I2,...]\n    formatted_results = [f\"{r:.16f}\".rstrip('0').rstrip('.') if 'e' not in f\"{r:.16f}\" else f\"{r:.16e}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}