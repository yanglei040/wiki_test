{
    "hands_on_practices": [
        {
            "introduction": "在深入探讨WGAN的内部机制之前，我们必须首先对瓦瑟斯坦距离本身有一个具体的理解。这项练习旨在将基于积分的 1-瓦瑟斯坦距离 ($W_1$) 的理论定义，与其在实践中基于样本的估计方法联系起来。通过为高斯分布推导出一个闭式解，并将其与数值估计进行比较 ，您将对这个距离度量所代表的含义，以及我们如何从数据中准确地测量它建立起坚实的直觉。",
            "id": "3137294",
            "problem": "本题要求您在一个受控的环境中，将推土机距离 (Earth Mover's distance) 的数学基础与Wasserstein生成对抗网络 (WGAN) 中使用的实用估计方法联系起来，其中真实分布和生成分布都是一维高斯分布。从 $1$-Wasserstein距离的基础定义出发，推导一个标准正态分布与任意正态分布之间距离的精确表达式。然后，实现一个模拟一维经验性判别器行为的数值估计器，并在几个参数体系下比较这两种方法的结果。\n\n背景与定义：\n- 生成对抗网络 (GAN) 将学习过程构建为一个生成器与判别器 (critic 或 discriminator) 之间的双人博弈。Wasserstein生成对抗网络 (WGAN) 使用 $1$-Wasserstein距离取代Jensen–Shannon散度以提高训练稳定性。\n- 在实数线上，两个概率分布 $p_r$ 和 $p_g$ 之间的 $1$-Wasserstein距离 $W_1$ 由成本函数 $c(x,y) = |x-y|$ 下的最优传输成本定义。等价地，通过Kantorovich–Rubinstein (KR) 对偶性，它可以表示为所有 $1$-Lipschitz函数 $f$ 的上确界：$$W_1(p_r, p_g) = \\sup_{\\|f\\|_{\\mathrm{Lip}} \\le 1} \\mathbb{E}_{X \\sim p_r}[f(X)] - \\mathbb{E}_{Y \\sim p_g}[f(Y)]。$$\n- 在一维情况下，一个基础且经过充分检验的事实是，最优耦合是单调的，这导出了分位数耦合恒等式：$$W_1(p_r, p_g) = \\int_0^1 \\left| F_r^{-1}(u) - F_g^{-1}(u) \\right| \\, du,$$ 其中 $F^{-1}$ 表示分位数函数（逆累积分布函数(CDF)）。\n\n问题设置：\n- 设真实数据分布为 $p_r = \\mathcal{N}(0, 1)$，生成器分布为 $p_g = \\mathcal{N}(\\mu, \\sigma^2)$，参数为 $\\mu \\in \\mathbb{R}$ 和 $\\sigma \\in \\mathbb{R}_{0}$。\n- 您的任务是：\n  1. 根据上述原理，推导 $$W_1\\!\\left(\\mathcal{N}(0,1), \\mathcal{N}(\\mu,\\sigma^2)\\right)$$ 的闭式表达式，该表达式仅用 $\\mu$ 和 $\\sigma$ 表示，且不得使用任何假定答案的快捷公式。\n  2. 基于一维的单调传输，实现一个数值估计器，用以计算 $p_r$ 和 $p_g$ 之间使用 $n$ 个独立样本的经验性 $1$-Wasserstein距离。具体来说，对于样本量为 $n$ 的等权重经验分布，经验性 $1$-Wasserstein距离等于两个样本集的顺序统计量之间的平均绝对差。\n  3. 对几组 $(\\mu, \\sigma)$ 参数对，比较闭式解的值与经验估计值。\n\n您的程序必须：\n- 计算任务$1$中推导出的闭式解的值。\n- 在任务$2$中，通过从每个分布中抽取 $n$ 个样本，对两组样本进行排序，并计算排序后样本之间绝对差的平均值来估计经验值。\n- 为保证可复现性，使用固定的伪随机种子。\n- 对于每个测试用例，输出一个浮点数三元组 $[w_{\\mathrm{closed}}, w_{\\mathrm{empirical}}, \\Delta]$，其中 $w_{\\mathrm{closed}}$ 是闭式解的值，$w_{\\mathrm{empirical}}$ 是经验估计值，$\\Delta$ 是绝对差 $|w_{\\mathrm{closed}} - w_{\\mathrm{empirical}}|$。\n\n测试套件：\n- 对下列每种情况，使用参数集 $(\\mu, \\sigma)$，每个分布抽取 $n$ 个样本：\n  1. $(\\mu, \\sigma) = (0, 1)$，基准相同分布。\n  2. $(\\mu, \\sigma) = (1, 1)$，纯均值偏移。\n  3. $(\\mu, \\sigma) = (0, 2)$，纯尺度变化。\n  4. $(\\mu, \\sigma) = (-2, 1.5)$，同时均值和尺度变化。\n  5. $(\\mu, \\sigma) = (0.5, 0.5)$，均值为正且尺度小于$1$。\n  6. $(\\mu, \\sigma) = (3, 0.1)$，大均值和极小方差。\n- 对每个经验估计使用 $n = 100000$ 个样本和一个固定种子。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个列表项本身是针对一个测试用例的逗号分隔列表。例如：$$[[w_1^{\\mathrm{closed}}, w_1^{\\mathrm{emp}}, \\Delta_1],[w_2^{\\mathrm{closed}}, w_2^{\\mathrm{emp}}, \\Delta_2],\\dots].$$\n- 所有输出必须是浮点数。不适用任何物理单位或角度单位。不要使用百分号；如果出现任何分数值，必须以小数表示。",
            "solution": "该问题要求对两个一维高斯分布之间的$1$-Wasserstein距离进行两部分分析。首先，我们必须从基于分位数的定义出发，推导出该距离的闭式表达式。其次，我们要实现一个基于排序经验样本的数值估计器，并将其结果与理论值进行比较。\n\n**1. $W_1(\\mathcal{N}(0,1), \\mathcal{N}(\\mu,\\sigma^2))$ 的闭式表达式推导**\n\n两个在实数线上的概率分布 $p_r$ 和 $p_g$ 之间的$1$-Wasserstein距离 $W_1$ 可以使用它们的分位数函数 $F_r^{-1}$ 和 $F_g^{-1}$（逆CDF）来计算。公式如下：\n$$W_1(p_r, p_g) = \\int_0^1 \\left| F_r^{-1}(u) - F_g^{-1}(u) \\right| \\, du$$\n\n问题指定真实分布 $p_r$ 为标准正态分布 $\\mathcal{N}(0,1)$，生成分布 $p_g$ 为一般正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$，其中 $\\mu$ 是均值，$\\sigma  0$ 是标准差。\n\n首先，我们确定这些分布的分位数函数。设 $\\Phi(z)$ 为标准正态分布 $\\mathcal{N}(0,1)$ 的累积分布函数（CDF）。\n分位数函数是CDF的逆函数。\n- 对于 $p_r = \\mathcal{N}(0,1)$，其CDF为 $F_r(x) = \\Phi(x)$。因此，分位数函数为 $F_r^{-1}(u) = \\Phi^{-1}(u)$。\n- 对于 $p_g = \\mathcal{N}(\\mu, \\sigma^2)$，一个随机变量 $Y \\sim p_g$ 可以表示为 $Y = \\mu + \\sigma Z$，其中 $Z \\sim \\mathcal{N}(0,1)$。其CDF为 $F_g(y) = P(Y \\le y) = P(\\mu + \\sigma Z \\le y) = P\\left(Z \\le \\frac{y-\\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{y-\\mu}{\\sigma}\\right)$。为了求得分位数函数 $F_g^{-1}(u)$，我们设 $F_g(y) = u$ 并求解 $y$：\n$u = \\Phi\\left(\\frac{y-\\mu}{\\sigma}\\right) \\implies \\Phi^{-1}(u) = \\frac{y-\\mu}{\\sigma} \\implies y = \\mu + \\sigma \\Phi^{-1}(u)$。\n所以，分位数函数是 $F_g^{-1}(u) = \\mu + \\sigma \\Phi^{-1}(u)$。\n\n将这些分位数函数代入 $W_1$ 的积分公式：\n$$W_1(p_r, p_g) = \\int_0^1 \\left| \\Phi^{-1}(u) - (\\mu + \\sigma \\Phi^{-1}(u)) \\right| \\, du$$\n$$W_1(p_r, p_g) = \\int_0^1 \\left| (1-\\sigma)\\Phi^{-1}(u) - \\mu \\right| \\, du$$\n\n为了计算这个积分，我们进行变量替换。设 $z = \\Phi^{-1}(u)$，这意味着 $u = \\Phi(z)$。其微分为 $du = \\phi(z) dz$，其中 $\\phi(z)$ 是标准正态分布的概率密度函数 (PDF)。当 $u$ 的范围从 $0$ 到 $1$ 时，$z$ 的范围从 $-\\infty$ 到 $\\infty$。积分变为：\n$$W_1(p_r, p_g) = \\int_{-\\infty}^{\\infty} \\left| (1-\\sigma)z - \\mu \\right| \\phi(z) \\, dz$$\n该表达式是期望 $\\mathbb{E}[|(1-\\sigma)Z - \\mu|]$ 的定义，其中 $Z$ 是一个服从 $\\mathcal{N}(0,1)$ 分布的随机变量。\n\n设 $W = (1-\\sigma)Z - \\mu$。由于 $Z$ 是一个正态随机变量，$W$ 也服从正态分布。其均值和方差为：\n- $\\mathbb{E}[W] = \\mathbb{E}[(1-\\sigma)Z - \\mu] = (1-\\sigma)\\mathbb{E}[Z] - \\mu = (1-\\sigma) \\cdot 0 - \\mu = -\\mu$。\n- $\\text{Var}(W) = \\text{Var}[(1-\\sigma)Z - \\mu] = (1-\\sigma)^2 \\text{Var}(Z) = (1-\\sigma)^2 \\cdot 1 = (1-\\sigma)^2$。\n因此，$W \\sim \\mathcal{N}(-\\mu, (1-\\sigma)^2)$。现在问题变为计算 $\\mathbb{E}[|W|]$。这是一个折叠正态分布的一阶矩。\n\n我们分两种情况处理 $\\sigma$：\n情况1：$\\sigma = 1$。随机变量 $W$ 变为一个常数：$W = (1-1)Z - \\mu = -\\mu$。其期望为 $\\mathbb{E}[|-\\mu|] = |\\mu|$。\n\n情况2：$\\sigma \\neq 1$。对于变量 $X \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)$，其折叠正态分布的均值由以下公式给出：\n$$\\mathbb{E}[|X|] = \\mu_X \\left(1 - 2\\Phi(-\\mu_X/\\sigma_X)\\right) + \\sigma_X \\sqrt{2/\\pi} \\exp\\left(-\\frac{\\mu_X^2}{2\\sigma_X^2}\\right)$$\n在我们的情境中，$X = W$，因此我们代入 $\\mu_X = -\\mu$ 和 $\\sigma_X = \\sqrt{(1-\\sigma)^2} = |1-\\sigma|$。\n$$W_1 = (-\\mu) \\left(1 - 2\\Phi\\left(-\\frac{-\\mu}{|1-\\sigma|}\\right)\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{(-\\mu)^2}{2|1-\\sigma|^2}\\right)$$\n$$W_1 = -\\mu \\left(1 - 2\\Phi\\left(\\frac{\\mu}{|1-\\sigma|}\\right)\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{\\mu^2}{2(1-\\sigma)^2}\\right)$$\n利用误差函数 $\\text{erf}(x)$ 和 $\\Phi(x)$ 之间的关系，特别是 $1 - 2\\Phi(z) = -\\text{erf}(z/\\sqrt{2})$，表达式可简化为：\n$$W_1 = -\\mu \\left(-\\text{erf}\\left(\\frac{\\mu}{|1-\\sigma|\\sqrt{2}}\\right)\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{\\mu^2}{2(1-\\sigma)^2}\\right)$$\n$$W_1 = \\mu \\cdot \\text{erf}\\left(\\frac{\\mu}{|1-\\sigma|\\sqrt{2}}\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{\\mu^2}{2(1-\\sigma)^2}\\right)$$\n这就是 $\\sigma \\neq 1$ 时的闭式解。\n\n总而言之，$1$-Wasserstein距离的闭式表达式为：\n$$ W_1(\\mathcal{N}(0,1), \\mathcal{N}(\\mu,\\sigma^2)) = \\begin{cases} |\\mu|  \\text{若 } \\sigma = 1 \\\\ \\mu \\cdot \\text{erf}\\left(\\frac{\\mu}{|1-\\sigma|\\sqrt{2}}\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{\\mu^2}{2(1-\\sigma)^2}\\right)  \\text{若 } \\sigma \\neq 1 \\end{cases} $$\n\n**2. 经验性 $W_1$ 的数值估计**\n\n问题提供了一维情况下经验性估计$1$-Wasserstein距离的方法。对于大小均为 $n$ 的两组样本（一组来自 $p_r$，一组来自 $p_g$），经验距离是它们的顺序统计量（排序后的样本）之间的平均绝对差。这是将分位数恒等式应用于经验累积分布函数的直接结果。\n\n算法如下：\n1. 从真实分布 $p_r = \\mathcal{N}(0,1)$ 生成 $n$ 个独立样本 $\\{x_1, \\dots, x_n\\}$。\n2. 从生成器分布 $p_g = \\mathcal{N}(\\mu, \\sigma^2)$ 生成 $n$ 个独立样本 $\\{y_1, \\dots, y_n\\}$。\n3. 对两组样本进行排序以获得顺序统计量：$x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(n)}$ 和 $y_{(1)} \\le y_{(2)} \\le \\dots \\le y_{(n)}$。\n4. 经验性$1$-Wasserstein距离 $\\hat{W}_1$ 计算为对应排序样本之间绝对差的平均值：\n$$\\hat{W}_1(p_r, p_g) = \\frac{1}{n} \\sum_{i=1}^n |x_{(i)} - y_{(i)}|$$\n使用固定的伪随机种子以确保随机样本的可复现性，从而保证经验估计的可复现性。\n\n最终的程序将为每个指定的 $(\\mu, \\sigma)$ 对实现推导出的闭式计算和经验估计过程，并报告理论值、经验估计值以及它们的绝对差。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\n\ndef calculate_closed_form(mu: float, sigma: float) - float:\n    \"\"\"\n    Computes the closed-form 1-Wasserstein distance between\n    N(0, 1) and N(mu, sigma^2).\n    \"\"\"\n    # The derived formula has a special case for sigma = 1\n    if sigma == 1.0:\n        return np.abs(mu)\n    else:\n        # General formula for sigma != 1\n        abs_one_minus_sigma = np.abs(1.0 - sigma)\n        \n        # Argument for the error function\n        erf_arg = mu / (abs_one_minus_sigma * np.sqrt(2.0))\n        \n        # First term of the sum\n        mu_term = mu * erf(erf_arg)\n        \n        # Second term of the sum\n        sigma_term_factor = abs_one_minus_sigma * np.sqrt(2.0 / np.pi)\n        exp_term = np.exp(-mu**2 / (2.0 * abs_one_minus_sigma**2))\n        sigma_term = sigma_term_factor * exp_term\n        \n        return mu_term + sigma_term\n\ndef calculate_empirical(mu: float, sigma: float, n: int, rng: np.random.Generator) - float:\n    \"\"\"\n    Computes the empirical 1-Wasserstein distance by sampling.\n    \"\"\"\n    # Generate n samples from the real distribution N(0, 1)\n    samples_r = rng.standard_normal(n)\n    \n    # Generate n samples from the generator distribution N(mu, sigma^2)\n    # Note: rng.standard_normal generates from N(0,1). We scale by sigma and shift by mu.\n    samples_g = rng.standard_normal(n) * sigma + mu\n    \n    # Sort both sets of samples to get the order statistics\n    sorted_r = np.sort(samples_r)\n    sorted_g = np.sort(samples_g)\n    \n    # The empirical distance is the mean absolute difference of the order statistics\n    w_empirical = np.mean(np.abs(sorted_r - sorted_g))\n    \n    return w_empirical\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (mu, sigma)\n    test_cases = [\n        (0.0, 1.0),\n        (1.0, 1.0),\n        (0.0, 2.0),\n        (-2.0, 1.5),\n        (0.5, 0.5),\n        (3.0, 0.1)\n    ]\n    \n    # Sample size for empirical estimation\n    n_samples = 100000\n    \n    # Fixed seed for reproducibility. Use a modern Generator object.\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for mu, sigma in test_cases:\n        # Task 1: Compute the closed-form value\n        w_closed = calculate_closed_form(mu, sigma)\n        \n        # Task 2: Compute the empirical estimate\n        w_empirical = calculate_empirical(mu, sigma, n_samples, rng)\n        \n        # Task 3: Compute the absolute difference\n        delta = np.abs(w_closed - w_empirical)\n        \n        # Store the triplet of floats\n        results.append([w_closed, w_empirical, delta])\n\n    # Final print statement in the exact required format.\n    # The format requires a string representation of a list of lists.\n    # e.g., [[val1, val2, val3],[...]]\n    # map(str, results) converts each inner list to its string form \"[v1, v2, v3]\"\n    # ','.join(...) combines them with commas in between\n    # f\"[{...}]\" wraps the final string in the outer list brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "WGAN相较于原始GAN的一个关键优势，是它能有效处理真实数据与生成数据分布支撑集不相交的情况——这种情况在原始GAN中常常导致梯度消失。本练习在一个离散的网格上模拟WGAN判别器函数 $f$ 的优化过程，让我们能直观地观察其行为。通过这个模拟 ，您将看到判别器如何在支撑集之间的“鸿沟”地带学习到一个有意义的近似线性函数，从而为生成器提供有效的梯度指导，这正是WGAN训练更稳定的核心原因。",
            "id": "3137255",
            "problem": "构建一个完整、可运行的程序，以经验性地研究在 Wasserstein 生成对抗网络 (WGAN) 中，真实数据分布 $p_r$ 与生成器分布 $p_g$ 之间的支撑集不匹配如何影响判别器函数 $f$ 的训练行为。使用 Wasserstein-1 距离的 Kantorovich–Rubinstein 对偶特性作为基本依据：Wasserstein-1 距离等于在所有 1-Lipschitz 函数 $f$ 上取得的目标函数 $\\,\\mathbb{E}_{p_r}[f] - \\mathbb{E}_{p_g}[f]\\,$ 的上确界。在一维空间中进行，并在一个均匀离散网格上近似 $f$。\n\n您的程序必须实现以下纯粹的数学和算法过程：\n\n- 在有界区间 $[x_{\\min}, x_{\\max}]$ 上定义一个包含 $N$ 个点、间距为 $\\Delta x$ 的均匀网格。将 $f$ 表示为此网格上的值 $\\{f(x_i)\\}_{i=1}^N$。\n- 将 $p_r$ 和 $p_g$ 定义为 $[x_{\\min}, x_{\\max}]$ 中闭区间并集上的均匀分布。在网格上，通过为每个位于 $p_r$ (或 $p_g$) 支撑集内的网格点分配相等权重来近似 $\\mathbb{E}_{p_r}[f]$ 和 $\\mathbb{E}_{p_g}[f]$，并进行归一化，使得每个分布的权重总和为 1。\n- 通过在离散目标 $L(f) = \\sum_{i=1}^N f(x_i)\\,w_i$ 上进行投影梯度上升来优化判别器，其中 $w_i = w^{(r)}_i - w^{(g)}_i$，$w^{(r)}_i$ 和 $w^{(g)}_i$ 分别是 $x_i$ 处的真实权重和生成器权重。使用步长为 $\\eta$ 的更新 $f \\leftarrow f + \\eta \\, w$，然后通过强制执行有界差分约束 $|f(x_i) - f(x_{i-1})| \\le \\Delta x$ (对于所有相邻网格点) 并锚定 $f(x_1) = 0$ 以固定加性规范，将其投影到 1-Lipschitz 函数集合上。投影必须通过裁剪相邻差分并通过累积求和重构 $f$ 来进行，以确保对所有 $i$ 都有 $|f(x_i) - f(x_{i-1})| \\le \\Delta x$ 成立。\n- 优化后，为每个测试案例计算三个量化指标：\n    $1.$ 近似的 Wasserstein-1 目标值 $\\,\\mathbb{E}_{p_r}[f] - \\mathbb{E}_{p_g}[f] = \\sum_{i=1}^N f(x_i)\\,w_i\\,$，以浮点数形式报告。\n    $2.$ 在 $p_r$ 和 $p_g$ 均无支撑的间隙区域内，网格边上 $f$ 的归一化平均绝对斜率。具体来说，计算相邻差分 $s_i = f(x_{i+1}) - f(x_i)$，限制于索引 $i$ 使得 $x_i$ 和 $x_{i+1}$ 都位于 $p_r$ 和 $p_g$ 的支撑集之外，对这些边计算 $\\frac{1}{M}\\sum |s_i|/\\Delta x$（其中 $M$ 是此类边的数量）。如果不存在此类边，则将此指标定义为 0。将此归一化平均值以 $[0,1]$ 范围内的浮点数形式报告。\n    $3.$ 最左侧真实区间和生成器区间中点之间的判别器对比度，定义为 $f(m_r) - f(m_g)$，其中 $m_r$ 是 $p_r$ 最左侧区间的中点，$m_g$ 是 $p_g$ 最左侧区间的中点。在最接近 $m_r$ 和 $m_g$ 的网格点上评估 $f$。将其以浮点数形式报告。\n\n设计一个测试套件，用于探究 $p_r$ 和 $p_g$ 之间不同的支撑集关系。使用以下案例：\n\n- 案例 1 (完全不相交的支撑集，有一个间隙): 定义域 $[0,3]$，$N=401$；$p_r$ 支撑集 $[0,1]$；$p_g$ 支撑集 $[2,3]$。\n- 案例 2 (双分量支撑集，具有交替的不相交分量): 定义域 $[0,5]$，$N=401$；$p_r$ 支撑集 $[0,1]$ 和 $[3,4]$；$p_g$ 支撑集 $[1,2]$ 和 $[4,5]$。\n- 案例 3 (完全重叠): 定义域 $[0,2]$，$N=401$；$p_r$ 支撑集 $[0.5,1.5]$；$p_g$ 支撑集 $[0.5,1.5]$。\n- 案例 4 (边界附近的小的不相交区间): 定义域 $[0,1]$，$N=401$；$p_r$ 支撑集 $[0,0.2]$；$p_g$ 支撑集 $[0.8,1.0]$。\n- 案例 5 (交错的多分量支撑集): 定义域 $[0,3]$，$N=401$；$p_r$ 支撑集 $[0,0.5]$ 和 $[1.5,2.0]$；$p_g$ 支撑集 $[0.5,1.0]$ 和 $[2.5,3.0]$。\n\n对于优化，使用 $T=800$ 次上升-投影迭代和步长 $\\eta=5.0$。这些值固定了计算过程并确保了可复现性。\n\n您的程序必须生成单行输出，其中包含按顺序聚合的所有测试案例的结果，格式为一个由方括号括起来的、以逗号分隔的列表的列表。每个内部列表必须按上述顺序包含三个浮点数，并且必须四舍五入到 4 位小数。例如，输出格式应与 $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\ldots,[a_5,b_5,c_5]]$ 完全一样，该行中任何地方都没有空格。\n\n此问题不涉及物理单位，也不使用角度。所有结果必须按规定以浮点数或浮点数列表的形式报告。程序必须是自包含的，不需要用户输入或外部文件。",
            "solution": "用户要求对一维 Wasserstein 生成对抗网络 (WGAN) 中判别器函数的行为进行计算研究。该问题是有效的，其科学基础是最优输运理论和约束优化，并为算法及其测试案例提供了完整、适定的规范。我们将提供一个完整的解决方案。\n\n此问题的基本原理是 Kantorovich-Rubinstein 对偶性，它为两个概率分布 $p_r$ (真实数据) 和 $p_g$ (生成器) 之间的 Wasserstein-1 距离（也称为推土机距离）提供了一个公式。该对偶性表明：\n$$\nW_1(p_r, p_g) = \\sup_{\\|f\\|_L \\le 1} \\left( \\mathbb{E}_{x \\sim p_r}[f(x)] - \\mathbb{E}_{x \\sim p_g}[f(x)] \\right)\n$$\n在这里，上确界是在所有 1-Lipschitz 函数 $f: \\mathcal{X} \\to \\mathbb{R}$ 上取得的，这些函数满足对于定义域 $\\mathcal{X}$ 中的所有 $x, y$，都有 $|f(x) - f(y)| \\le |x - y|$。在 WGAN 的背景下，这个函数 $f$ 被称为判别器。判别器的训练过程涉及找到一个函数 $f$ 来最大化目标函数 $\\mathbb{E}_{p_r}[f] - \\mathbb{E}_{p_g}[f]$。本问题研究在离散域上的这一优化过程。\n\n我们首先将问题离散化。连续域，一个闭区间 $[x_{\\min}, x_{\\max}]$，被一个包含 $N$ 个点的均匀网格 $\\{x_i\\}_{i=1}^N$ 所取代，其中 $x_i = x_{\\min} + (i-1)\\Delta x$，网格间距为 $\\Delta x = (x_{\\max} - x_{\\min})/(N-1)$。判别器函数 $f$ 由其在该网格上的值向量表示，记为 $\\mathbf{f} = (f(x_1), f(x_2), \\dots, f(x_N))$。\n\n分布 $p_r$ 和 $p_g$ 被定义为在 $[x_{\\min}, x_{\\max}]$ 的指定子区间上的均匀分布。期望值通过网格点上的加权和来近似。设 $S_r$ 为索引 $i$ 的集合，使得 $x_i$ 位于 $p_r$ 的支撑集内，并设 $N_r = |S_r|$。对于 $p_r$ 的权重为 $w^{(r)}_i = 1/N_r$ 如果 $i \\in S_r$，否则 $w^{(r)}_i = 0$。对于 $p_g$ 的权重 $w^{(g)}_i$ 也以类似方式定义。需要最大化的离散目标函数是：\n$$\nL(\\mathbf{f}) = \\sum_{i=1}^N f(x_i) w^{(r)}_i - \\sum_{i=1}^N f(x_i) w^{(g)}_i = \\sum_{i=1}^N f(x_i) (w^{(r)}_i - w^{(g)}_i) = \\mathbf{f} \\cdot \\mathbf{w}\n$$\n其中 $\\mathbf{w}$ 是分量为 $w_i = w^{(r)}_i - w^{(g)}_i$ 的向量。\n\n优化使用投影梯度上升法进行。线性目标函数 $L(\\mathbf{f})$ 关于向量 $\\mathbf{f}$ 的梯度就是权重向量 $\\mathbf{w}$。更新规则包括一个梯度上升步骤和一个为强制执行约束的投影步骤。\n1.  **梯度上升步骤**：通过沿梯度方向移动来计算一个中间函数 $\\mathbf{f}_{\\text{new}}$：\n    $$\n    \\mathbf{f}_{\\text{new}} \\leftarrow \\mathbf{f} + \\eta \\mathbf{w}\n    $$\n    其中 $\\eta$ 是步长。\n\n2.  **投影步骤**：将函数 $\\mathbf{f}_{\\text{new}}$ 投影回锚定在原点的离散 1-Lipschitz 函数集合上。离散 1-Lipschitz 条件是 $|f(x_i) - f(x_{i-1})| \\le \\Delta x$。锚定条件是 $f(x_1) = 0$。指定的投影算法如下：\n    a. 计算 $\\mathbf{f}_{\\text{new}}$ 的前向差分：$\\Delta_i = f_{\\text{new}}(x_{i+1}) - f_{\\text{new}}(x_i)$，对于 $i=1, \\dots, N-1$。\n    b. 将这些差分裁剪到允许的范围：$\\Delta'_i = \\text{clip}(\\Delta_i, -\\Delta x, \\Delta x)$。\n    c. 通过将其第一个分量设置为零，然后对裁剪后的差分进行累积和，来重构最终更新的函数 $\\mathbf{f}_{\\text{next}}$：\n       $$\n       f_{\\text{next}}(x_1) = 0\n       $$\n       $$\n       f_{\\text{next}}(x_i) = \\sum_{j=1}^{i-1} \\Delta'_j \\quad \\text{对于 } i  1\n       $$\n    这个过程以步长 $\\eta=5.0$ 重复固定次数的迭代，即 $T=800$ 次。\n\n在优化收敛到最终的判别器函数 $\\mathbf{f}^*$ 后，我们计算三个指标来分析其特性：\n\n1.  **近似 Wasserstein-1 目标值**：这是优化后判别器的目标函数值，$\\sum_{i=1}^N f^*(x_i) w_i$。它提供了离散分布之间 Wasserstein 距离的估计。\n\n2.  **间隙中的归一化平均绝对斜率**：该指标量化了判别器在 $p_r$ 和 $p_g$ 均无支撑的区域中的行为。我们识别出所有落在此类“间隙”内的相邻网格点对 $(x_i, x_{i+1})$。该指标是所有这些点对上归一化绝对斜率 $|f^*(x_{i+1}) - f^*(x_i)|/\\Delta x$ 的平均值。接近 1 的值表明判别器正在使用其最大允许斜率来跨越支撑集之间的距离，这是具有不相交支撑集的 WGAN 的典型行为。如果不存在这样的间隙边，则该指标定义为 0。\n\n3.  **判别器对比度**：定义为 $f^*(m_r) - f^*(m_g)$，其中 $m_r$ 和 $m_g$ 分别是 $p_r$ 和 $p_g$ 最左侧支撑区间的中点。函数值在最接近这些中点的网格点上取值。该指标衡量判别器区分两个分布核心的能力。\n\n所提供的测试案例旨在探索支撑集重叠和分离的不同场景，揭示判别器的结构如何适应分布的几何形状。实现将系统地处理每个案例并计算指定的指标。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the WGAN critic optimization simulation for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: fully disjoint supports with a single gap\n        {'domain': (0.0, 3.0), 'N': 401, 'pr_supports': [[0.0, 1.0]], 'pg_supports': [[2.0, 3.0]]},\n        # Case 2: two-component supports with alternating disjoint components\n        {'domain': (0.0, 5.0), 'N': 401, 'pr_supports': [[0.0, 1.0], [3.0, 4.0]], 'pg_supports': [[1.0, 2.0], [4.0, 5.0]]},\n        # Case 3: complete overlap\n        {'domain': (0.0, 2.0), 'N': 401, 'pr_supports': [[0.5, 1.5]], 'pg_supports': [[0.5, 1.5]]},\n        # Case 4: small disjoint intervals near boundaries\n        {'domain': (0.0, 1.0), 'N': 401, 'pr_supports': [[0.0, 0.2]], 'pg_supports': [[0.8, 1.0]]},\n        # Case 5: interleaved multi-component supports\n        {'domain': (0.0, 3.0), 'N': 401, 'pr_supports': [[0.0, 0.5], [1.5, 2.0]], 'pg_supports': [[0.5, 1.0], [2.5, 3.0]]},\n    ]\n\n    T = 800  # Number of iterations\n    eta = 5.0  # Step size\n\n    all_results = []\n\n    for case in test_cases:\n        # 1. Setup grid and distributions\n        xmin, xmax = case['domain']\n        N = case['N']\n        x, dx = np.linspace(xmin, xmax, N, retstep=True)\n\n        pr_supports = case['pr_supports']\n        pg_supports = case['pg_supports']\n\n        # Create masks for supports\n        pr_mask = np.zeros(N, dtype=bool)\n        for start, end in pr_supports:\n            pr_mask |= (x >= start)  (x = end)\n\n        pg_mask = np.zeros(N, dtype=bool)\n        for start, end in pg_supports:\n            pg_mask |= (x >= start)  (x = end)\n        \n        # Calculate weights\n        w_r = np.zeros(N)\n        num_pr_points = np.sum(pr_mask)\n        if num_pr_points > 0:\n            w_r[pr_mask] = 1.0 / num_pr_points\n\n        w_g = np.zeros(N)\n        num_pg_points = np.sum(pg_mask)\n        if num_pg_points > 0:\n            w_g[pg_mask] = 1.0 / num_pg_points\n        \n        w = w_r - w_g\n\n        # 2. Optimize the critic function f\n        f = np.zeros(N)\n        for _ in range(T):\n            # Gradient ascent step\n            f_new = f + eta * w\n            \n            # Projection step\n            diffs = f_new[1:] - f_new[:-1]\n            clipped_diffs = np.clip(diffs, -dx, dx)\n            \n            f = np.zeros(N)\n            f[1:] = np.cumsum(clipped_diffs)\n\n        # 3. Compute metrics\n        # Metric 1: Wasserstein Objective\n        metric1 = np.sum(f * w)\n\n        # Metric 2: Normalized Mean Absolute Slope in Gaps\n        gap_mask = ~pr_mask  ~pg_mask\n        gap_edge_mask = gap_mask[:-1]  gap_mask[1:]\n        \n        if np.any(gap_edge_mask):\n            diffs_f = f[1:] - f[:-1]\n            gap_diffs = diffs_f[gap_edge_mask]\n            metric2 = np.mean(np.abs(gap_diffs) / dx)\n        else:\n            metric2 = 0.0\n\n        # Metric 3: Critic Contrast\n        m_r = (pr_supports[0][0] + pr_supports[0][1]) / 2.0\n        m_g = (pg_supports[0][0] + pg_supports[0][1]) / 2.0\n\n        idx_r = np.abs(x - m_r).argmin()\n        idx_g = np.abs(x - m_g).argmin()\n        \n        metric3 = f[idx_r] - f[idx_g]\n        \n        all_results.append([metric1, metric2, metric3])\n\n    # 4. Format and print output\n    case_strs = []\n    for result_case in all_results:\n        inner_str = f\"[{result_case[0]:.4f},{result_case[1]:.4f},{result_case[2]:.4f}]\"\n        case_strs.append(inner_str)\n    \n    output_str = f\"[{','.join(case_strs)}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "我们已经了解了判别器学习的*内容*，但成功的WGAN训练还依赖于判别器与生成器之间微妙的平衡。判别器必须得到充分的训练，以便在生成器更新之前提供对瓦瑟斯坦距离梯度的准确估计。这项练习将探索“双时间尺度更新规则”，即每次生成器更新时，判别器都会进行多次更新。通过改变判别器与生成器的更新次数之比（$n_D$ 与 $n_G$），您将通过实验发现，为何一个训练充分的判别器对于稳定收敛至关重要，并能有效防止生成器利用一个训练不足的“对手”。",
            "id": "3137290",
            "problem": "考虑一种特定形式的生成对抗网络（GAN）——瓦瑟斯坦生成对抗网络（WGAN）在双时间尺度更新规则下的训练稳定性。令真实数据分布为一个一维离散均匀混合分布，其模态位于位置 $y_1, y_2, \\dots, y_m$ 处，每个模态具有相等的质量。令生成器产生一个一维离散均匀混合分布，其点位于位置 $x_1, x_2, \\dots, x_m$ 处，每个点具有相等的质量。假设 $m$ 对于两个分布是固定的且相同的。真实分布 $p_r$ 和生成器分布 $p_g$ 之间的瓦瑟斯坦-1距离（也称为推土机距离）表示为 $W_1(p_r, p_g)$，并可以通过 Kantorovich–Rubinstein 对偶性定义为\n$$\nW_1(p_r, p_g) = \\sup_{\\|f\\|_{\\text{Lip}} \\le 1} \\left( \\mathbb{E}_{x \\sim p_r}[f(x)] - \\mathbb{E}_{x \\sim p_g}[f(x)] \\right),\n$$\n其中上确界是针对所有 1-利普希茨函数 $f$。双时间尺度更新规则规定，每进行 $n_G$ 次生成器更新，就进行 $n_D$ 次评判器（判别器）更新。\n\n在一维情况下，考虑以下反映 WGAN 训练动态而无需显式参数化 $f$ 的基于原理的仿真模型：\n- 评判器由其在生成器支撑点处评估的导数场 $d_i \\approx f'(x_i)$ 表示，该导数场必须几乎处处满足 1-利普希茨约束。在每次评判器更新时，使用松弛率为 $\\alpha \\in (0,1)$ 的指数移动平均将场 $d_i$ 向最优次梯度方向松弛：\n$$\nd_i \\leftarrow (1 - \\alpha) \\, d_i + \\alpha \\, s_i,\n$$\n其中 $s_i = \\operatorname{sgn}\\big(F_r(x_i) - F_g(x_i)\\big)$，$F_r$ 和 $F_g$ 分别是 $p_r$ 和 $p_g$ 的累积分布函数 (CDF)。此处，$\\operatorname{sgn}$ 表示符号函数，且 $\\operatorname{sgn}(0) = 0$。\n- 在每次生成器更新时，使用当前的评判器导数 $d_i$，通过步长为 $\\eta  0$ 的简单梯度下降步骤来更新每个 $x_i$：\n$$\nx_i \\leftarrow x_i - \\eta \\, d_i.\n$$\n\n训练以周期形式进行。在每个周期中，先应用 $n_D$ 次评判器更新，然后应用 $n_G$ 次生成器更新。重复这些周期，直到执行了固定的总生成器更新次数 $T$。训练结束后，测量：\n1. $W_1$ 收敛性：通过使用单调传输原理（在一维情况下，最优传输匹配排序后的支撑集），精确计算一维中的 $W_1(p_r, p_g)$。\n2. 模态覆盖率：对于一个容差 $\\varepsilon  0$，如果存在某个生成器点 $x_i$ 使得 $|x_i - y_j| \\le \\varepsilon$，则称真实模态 $y_j$ 已被覆盖。模态覆盖率是已覆盖模态的比例，表示为 0 到 1 之间的小数。\n\n对所有测试用例使用以下固定参数：\n- 真实模态：$y = [-3, 0, 3]$（即 $m = 3$）。\n- 初始生成器位置：$x^{(0)} = [-6, 1, 6]$。\n- 评判器松弛率：$\\alpha = 0.2$。\n- 生成器步长：$\\eta = 0.05$。\n- 模态覆盖率容差：$\\varepsilon = 0.25$。\n- 总生成器更新次数：$T = 200$。\n\n将收敛定义为一个布尔值，当且仅当 $W_1(p_r, p_g) \\le 0.1$ 且模态覆盖率等于 1 时，该值为真。\n\n实现一个程序，模拟上述训练动态，并为每个测试用例输出最终的 $W_1$（浮点数）、模态覆盖率（小数）以及编码为整数（真为 1，假为 0）的收敛布尔值。\n\n测试套件：\n- 用例 1（边界，无评判器学习）：$(n_D, n_G) = (0, 1)$。\n- 用例 2（平衡）：$(n_D, n_G) = (1, 1)$。\n- 用例 3（典型的类 WGAN）：$(n_D, n_G) = (5, 1)$。\n- 用例 4（重评判器）：$(n_D, n_G) = (20, 1)$。\n- 用例 5（生成器相对于评判器较快）：$(n_D, n_G) = (1, 5)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，该列表按顺序聚合了所有测试用例的三元组，并展平成一个单一列表。例如，输出格式必须为：\n$[w1_1, \\text{coverage}_1, \\text{converged}_1, w1_2, \\text{coverage}_2, \\text{converged}_2, \\dots]$,\n其中每个 $w1_i$ 是一个浮点数，每个 coverage 是一个小数，每个 converged 是 0 或 1。",
            "solution": "该问题已经过验证，被认为是**有效**的。它提供了一个基于瓦瑟斯坦生成对抗网络（WGAN）和最优传输理论的既定原则的、自洽的、有科学依据且适定的仿真任务。所有必要的参数和初始条件都已指定，从而可以得到一个确定性且可验证的解。评判器导数场 $d_i$ 的初始状态未明确给出；然而，对于此类迭代算法，一个标准且合乎逻辑的选择是将其初始化为零，即对所有 $i$，$d_i^{(0)} = 0$。以下解决方案基于此假设。\n\n目标是针对五种不同的超参数设置 $(n_D, n_G)$，模拟一个简化的 WGAN 的训练动态，并使用三个指标评估结果：最终的瓦瑟斯坦-1距离、模态覆盖率和一个二元收敛标志。\n\n该仿真遵循以下步骤顺序。\n\n**1. 系统初始化**\n\n仿真使用指定的参数进行初始化。\n真实数据分布 $p_r$ 是在模态集合 $y = \\{-3, 0, 3\\}$ 上的离散均匀分布。模态数量为 $m=3$。\n生成器分布 $p_g$ 是在 $m=3$ 个点 $x = \\{x_1, x_2, x_3\\}$ 的集合上的离散均匀分布。它们的初始位置由 $x^{(0)} = [-6, 1, 6]$ 给出。\n评判器的导数场由向量 $d = [d_1, d_2, d_3]$ 表示，并初始化为 $d^{(0)} = [0, 0, 0]$。\n其他固定参数是：\n- 评判器松弛率：$\\alpha = 0.2$\n- 生成器步长：$\\eta = 0.05$\n- 模态覆盖率容差：$\\varepsilon = 0.25$\n- 总生成器更新次数：$T = 200$\n\n**2. 累积分布函数 (CDF)**\n\n评判器更新规则依赖于瓦瑟斯坦距离的次梯度，在一维中，该次梯度与真实分布和生成分布的 CDF 之差有关。对于一个具有支撑点 $\\{z_1, \\dots, z_m\\}$ 和均匀权重 $1/m$ 的离散分布 $P$，其 CDF 由下式给出：\n$$\nF_P(z) = \\frac{1}{m} \\sum_{i=1}^m \\mathbf{1}_{z_i \\le z}\n$$\n其中 $\\mathbf{1}$ 是指示函数。\n因此，对于我们的特定问题，真实分布和生成器分布在点 $z$ 处评估的 CDF 分别为：\n$$\nF_r(z) = \\frac{1}{3} \\sum_{j=1}^3 \\mathbf{1}_{y_j \\le z}\n$$\n$$\nF_g(z) = \\frac{1}{3} \\sum_{i=1}^3 \\mathbf{1}_{x_i \\le z}\n$$\n\n**3. 训练循环**\n\n训练以周期形式进行。对于由一对 $(n_D, n_G)$ 定义的每个测试用例，总周期数计算为 $N_{\\text{cycles}} = T / n_G$。由于在测试套件中 $T=200$ 且 $n_G$ 为 1 或 5，因此 $N_{\\text{cycles}}$ 始终是一个整数。每个周期包括两个阶段：评判器更新和生成器更新。\n\n**3.1. 评判器更新阶段**\n\n在此阶段的 $n_D$ 个步骤中，每一步都会更新评判器的导数场 $d$。每个分量 $d_i$ 的更新是朝向目标方向 $s_i$ 的指数移动平均：\n$$\nd_i \\leftarrow (1 - \\alpha) \\, d_i + \\alpha \\, s_i \\quad \\text{for } i = 1, 2, 3\n$$\n目标方向 $s_i$ 是在生成器当前支撑点 $x_i$ 处评估的 CDF 之差的符号：\n$$\ns_i = \\operatorname{sgn}\\big(F_r(x_i) - F_g(x_i)\\big)\n$$\n其中符号函数定义为：如果 $u  0$，则 $\\operatorname{sgn}(u) = 1$；如果 $u  0$，则为 $-1$；如果 $u = 0$，则为 $0$。对 $d$ 的所有 $m=3$ 个分量都执行这些更新。\n\n**3.2. 生成器更新阶段**\n\n在此阶段的 $n_G$ 个步骤中，每一步都通过梯度下降来更新生成器的支撑点 $x$。梯度由评判器的导数场 $d$ 提供。\n$$\nx_i \\leftarrow x_i - \\eta \\, d_i \\quad \\text{for } i = 1, 2, 3\n$$\n此更新将每个点 $x_i$ 沿着预期会减小瓦瑟斯坦距离的方向移动。\n\n**4. 训练后评估**\n\n完成所有 $N_{\\text{cycles}}$ 后，使用生成器点 $x$ 的最终状态来计算性能指标。\n\n**4.1. 瓦瑟斯坦-1距离 ($W_1$)**\n\n对于具有相同点数和均匀权重的一维离散分布，$W_1$ 距离有一个简单的闭式解。它是两个分布排序后的支撑点之间的平均绝对差。令 $x_{(1)}, x_{(2)}, x_{(3)}$ 为排序后的生成器点，$y_{(1)}, y_{(2)}, y_{(3)}$ 为排序后的真实模态。真实模态已经排好序：$y = [-3, 0, 3]$。\n$$\nW_1(p_r, p_g) = \\frac{1}{m} \\sum_{i=1}^m |x_{(i)} - y_{(i)}| = \\frac{1}{3} \\big( |x_{(1)} - (-3)| + |x_{(2)} - 0| + |x_{(3)} - 3| \\big)\n$$\n\n**4.2. 模态覆盖率**\n\n如果至少有一个最终的生成器点 $x_i$ 与真实模态 $y_j$ 的距离在 $\\varepsilon=0.25$ 之内，则认为该模态已被“覆盖”。\n$$\n\\text{covered}(y_j) = \\bigvee_{i=1}^3 \\big( |x_i - y_j| \\le \\varepsilon \\big)\n$$\n模态覆盖率是已覆盖的真实模态的比例：\n$$\n\\text{Coverage} = \\frac{1}{m} \\sum_{j=1}^m \\text{covered}(y_j)\n$$\n\n**4.3. 收敛准则**\n\n当且仅当同时满足以下两个条件时，一个布尔收敛标志被设置为真（表示为 1）；否则，它被设置为假（表示为 0）。\n1. 最终的瓦瑟斯坦-1距离足够小：$W_1(p_r, p_g) \\le 0.1$。\n2. 所有真实模态都被覆盖：模态覆盖率 $= 1.0$。\n\n对五个 $(n_D, n_G)$ 测试用例中的每一个都重复这整个过程，并将结果聚合到一个单一的扁平列表中作为最终输出。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates WGAN training dynamics and evaluates performance for multiple test cases.\n    \"\"\"\n\n    def run_simulation(nD, nG):\n        \"\"\"\n        Runs a single simulation for a given (nD, nG) pair.\n\n        Args:\n            nD (int): Number of critic updates per cycle.\n            nG (int): Number of generator updates per cycle.\n\n        Returns:\n            tuple: A triplet containing the final W1 distance (float),\n                   mode coverage (float), and convergence status (int).\n        \"\"\"\n        # Fixed parameters\n        y_real = np.array([-3.0, 0.0, 3.0])\n        m = len(y_real)\n        x = np.array([-6.0, 1.0, 6.0])\n        alpha = 0.2\n        eta = 0.05\n        epsilon = 0.25\n        T = 200\n\n        # Initial state\n        d = np.zeros(m)\n\n        # The problem statement implies T is a multiple of nG for all test cases.\n        if T % nG != 0:\n            # This case is not expected based on the problem description.\n            # We enforce the total number of generator updates to be exactly T.\n            num_cycles = T // nG\n            remaining_gen_updates = T % nG\n        else:\n            num_cycles = T // nG\n            remaining_gen_updates = 0\n\n        for _ in range(num_cycles):\n            # Critic update phase\n            for _ in range(nD):\n                # Calculate CDFs F_r(x_i) and F_g(x_i)\n                fr_x = np.array([np.sum(y_real = xi) for xi in x]) / m\n                fg_x = np.array([np.sum(x = xi) for xi in x]) / m\n\n                # Calculate target direction s\n                s = np.sign(fr_x - fg_x)\n\n                # Update critic's derivative field d\n                d = (1 - alpha) * d + alpha * s\n\n            # Generator update phase\n            for _ in range(nG):\n                # Update generator points x\n                x = x - eta * d\n        \n        # Handle any remaining generator updates if T is not a multiple of nG\n        # (Not needed for the given test cases, but robust)\n        if remaining_gen_updates > 0:\n             # Perform one more cycle of critic updates before final generator steps\n            for _ in range(nD):\n                fr_x = np.array([np.sum(y_real = xi) for xi in x]) / m\n                fg_x = np.array([np.sum(x = xi) for xi in x]) / m\n                s = np.sign(fr_x - fg_x)\n                d = (1 - alpha) * d + alpha * s\n            for _ in range(remaining_gen_updates):\n                x = x - eta * d\n\n\n        # --- Post-training evaluation ---\n\n        # 1. Wasserstein-1 distance\n        x_sorted = np.sort(x)\n        w1_distance = np.mean(np.abs(x_sorted - y_real))\n\n        # 2. Mode coverage\n        covered_count = 0\n        for yj in y_real:\n            is_covered = np.any(np.abs(x - yj) = epsilon)\n            if is_covered:\n                covered_count += 1\n        mode_coverage = covered_count / m\n\n        # 3. Convergence criterion\n        converged = (w1_distance = 0.1) and (mode_coverage == 1.0)\n        converged_int = 1 if converged else 0\n\n        return w1_distance, mode_coverage, converged_int\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0, 1),   # Case 1: no critic learning\n        (1, 1),   # Case 2: balanced\n        (5, 1),   # Case 3: typical WGAN-like\n        (20, 1),  # Case 4: heavy critic\n        (1, 5),   # Case 5: fast generator relative to critic\n    ]\n\n    results = []\n    for nD, nG in test_cases:\n        w1, coverage, converged = run_simulation(nD, nG)\n        results.extend([w1, coverage, converged])\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for i, res in enumerate(results):\n        # Format floats, keep ints as is\n        if i % 3 in [0, 1]:  # w1 and coverage\n            formatted_results.append(f\"{res:.7f}\")\n        else: # converged\n            formatted_results.append(str(res))\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}