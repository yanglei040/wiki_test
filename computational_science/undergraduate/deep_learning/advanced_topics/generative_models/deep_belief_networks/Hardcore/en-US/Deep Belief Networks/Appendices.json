{
    "hands_on_practices": [
        {
            "introduction": "Before training any deep network, it is crucial to understand its architectural complexity. This exercise guides you through a fundamental analysis of a Deep Belief Network, focusing on how its parameter count and computational cost scale with network depth and layer width. By deriving these quantities and exploring the impact of a hypothetical sparse connectivity scheme, you will develop a quantitative intuition for the architectural trade-offs involved in designing efficient deep learning models .",
            "id": "3112338",
            "problem": "Consider a Deep Belief Network (DBN) formed by stacking $L$ Restricted Boltzmann Machines (RBMs), where the visible layer has size $n_{v}$ and each hidden layer has size $n_{h}$. Each RBM is a bipartite graphical model with energy function $E(v,h)=-v^{\\top}Wh-b^{\\top}v-c^{\\top}h$, where $v \\in \\{0,1\\}^{n_{v}}$ or $\\{0,1\\}^{n_{h}}$ depending on layer, $h \\in \\{0,1\\}^{n_{h}}$, $W$ is the weight matrix connecting the two layers, and $b$ and $c$ are the visible and hidden biases, respectively. Pretraining is performed layer-wise using Contrastive Divergence (CD), specifically $k$ steps of Contrastive Divergence (CD-$k$) per data case per layer. Assume a dataset of size $N$ examples and one parameter update per layer per epoch.\n\nStarting from these definitions and the bipartite structure of RBMs, and using only well-tested facts about matrix-vector multiplication costs, perform the following:\n\n1. Derive the total number of trainable parameters for the dense DBN, denoted $P_{\\text{dense}}$, counting all weights and biases across all $L$ RBMs.\n2. Derive a leading-order expression for the floating-point operation count per epoch for CD-$k$ pretraining of the dense DBN, denoted $C_{\\text{dense}}$, under the following scientifically reasonable assumptions:\n   - The dominant computation per CD step per data case per RBM consists of two matrix-vector multiplications (the up-pass $W^{\\top}v$ and the down-pass $W h$), and the cost scales linearly with the number of nonzero entries in $W$.\n   - Non-matrix operations (additions, activation evaluations) contribute a constant multiplicative factor that can be absorbed into an abstract constant.\n   Introduce a positive constant $\\alpha$ that captures the per-edge computational cost per CD step per data case, and express $C_{\\text{dense}}$ in terms of $n_{v}$, $n_{h}$, $L$, $N$, $k$, and $\\alpha$.\n3. Propose a sparse connectivity scheme in which, in every RBM, each potential visible-hidden edge is independently present with probability $p \\in (0,1)$. Under this model, derive the expected total number of trainable parameters $P_{\\text{sparse}}$ and the expected per-epoch floating-point operation count $C_{\\text{sparse}}$.\n4. Finally, compute the closed-form expression for the ratio $R=C_{\\text{sparse}}/C_{\\text{dense}}$.\n\nYour final answer must be the single symbolic expression for $R$. No numerical approximation or rounding is required.",
            "solution": "We begin with structural definitions for Restricted Boltzmann Machines (RBMs) and Deep Belief Networks (DBNs). An RBM is bipartite: all visible units connect to all hidden units, and there are no lateral connections within a layer. The energy function $E(v,h)=-v^{\\top}Wh-b^{\\top}v-c^{\\top}h$ implies a parameter set consisting of the weight matrix $W$ and the bias vectors $b$ and $c$. The size of $W$ is determined by the sizes of the connected layers.\n\nA DBN pretraining procedure stacks $L$ RBMs: the first RBM connects the visible layer of size $n_{v}$ to the first hidden layer of size $n_{h}$, and each subsequent RBM connects a hidden layer of size $n_{h}$ to the next hidden layer of size $n_{h}$. Thus, there are $L$ RBMs total: one with dimensions $n_{v} \\times n_{h}$, and $L-1$ with dimensions $n_{h} \\times n_{h}$.\n\nStep 1: Parameter count $P_{\\text{dense}}$.\n\nFor the first RBM:\n- Weights: $n_{v} \\times n_{h}$ entries.\n- Biases: $n_{v}$ visible biases and $n_{h}$ hidden biases.\n\nFor each of the remaining $L-1$ RBMs:\n- Weights: $n_{h} \\times n_{h}$ entries.\n- Biases: $n_{h}$ visible (now the lower hidden layer acts as \"visible\") and $n_{h}$ hidden biases for each RBM. However, in a stacked RBM pretraining setup, each layer has its own bias vector. Across the stack, the total biases are the sum of all visible-layer biases and all hidden-layer biases for each RBM. Aggregating uniquely across layers, each layer contributes one bias vector. There are $1$ visible layer (size $n_{v}$) and $L$ hidden layers (each size $n_{h}$), giving a total bias count of $n_{v} + L n_{h}$.\n\nTherefore, the total number of parameters under dense connectivity is\n$$\nP_{\\text{dense}} = \\underbrace{n_{v} n_{h} + (L-1) n_{h}^{2}}_{\\text{weights}} + \\underbrace{n_{v} + L n_{h}}_{\\text{biases}}.\n$$\n\nStep 2: Per-epoch complexity $C_{\\text{dense}}$.\n\nUnder Contrastive Divergence (CD-$k$) with dataset size $N$ per epoch, the dominant computation is two matrix-vector products per CD step per data case per RBM: the up-pass $W^{\\top} v$ and the down-pass $W h$. The cost of each matrix-vector product scales with the number of nonzero entries in $W$. Let $\\alpha0$ denote the constant of proportionality capturing the cumulative cost per edge per CD step per data case, including both the up-pass and down-pass.\n\nFor the first RBM, the number of edges equals the number of weights $n_{v} n_{h}$. For each of the remaining $L-1$ RBMs, the number of edges is $n_{h}^{2}$. Summing edges across the stack gives $n_{v} n_{h} + (L-1) n_{h}^{2}$.\n\nThus, the leading-order per-epoch complexity for dense connectivity is\n$$\nC_{\\text{dense}} = \\alpha \\, N \\, k \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right).\n$$\n\nStep 3: Sparse connectivity with edge probability $p$.\n\nIn the proposed sparse connectivity scheme, each potential visible-hidden edge in an RBM is independently present with probability $p \\in (0,1)$. The expected number of edges (and hence weights) becomes $p$ times the dense count, because the expectation of a sum of independent Bernoulli indicators equals the sum of their expectations.\n\nTherefore, the expected weight count across the stack is\n$$\n\\mathbb{E}[\\text{weights}] = p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right).\n$$\nBiases remain unchanged, because they are not pruned in this scheme. Hence,\n$$\nP_{\\text{sparse}} = p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right) + n_{v} + L n_{h}.\n$$\n\nFor the complexity, because the dominant per-epoch cost scales linearly with the number of edges, the expected per-epoch cost is\n$$\nC_{\\text{sparse}} = \\alpha \\, N \\, k \\, p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right).\n$$\n\nStep 4: Ratio $R=C_{\\text{sparse}}/C_{\\text{dense}}$.\n\nUsing the expressions above,\n$$\nR = \\frac{C_{\\text{sparse}}}{C_{\\text{dense}}} = \\frac{\\alpha \\, N \\, k \\, p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right)}{\\alpha \\, N \\, k \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right)} = p.\n$$\n\nThus, the closed-form expression for the compute reduction factor under the proposed sparse connectivity is $p$.",
            "answer": "$$\\boxed{p}$$"
        },
        {
            "introduction": "Deep Belief Networks function through a delicate interplay between a \"recognition\" pathway, which maps data to latent representations, and a \"generative\" pathway, which reconstructs data from those representations. This hands-on coding problem investigates the critical concept of \"tied weights,\" a structural constraint where the generative weights are the transpose of the recognition weights. By comparing the reconstruction quality in models with and without this constraint, you will gain a practical understanding of why this symmetry is a powerful and widely-used principle in the design of generative models .",
            "id": "3112369",
            "problem": "Consider a two-layer Deep Belief Network (DBN) with binary units at all layers, constructed by stacking two Restricted Boltzmann Machines (RBMs). Each RBM uses Bernoulli variables for visible and hidden units, and a logistic activation function. The fundamental base is the RBM energy-based model: for visible vector $\\mathbf{v} \\in \\{0,1\\}^d$ and hidden vector $\\mathbf{h} \\in \\{0,1\\}^m$, the energy is given by\n$$\nE(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{c}^\\top \\mathbf{v} - \\mathbf{b}^\\top \\mathbf{h} - \\mathbf{v}^\\top W \\mathbf{h},\n$$\nand the conditional distributions factorize due to bipartite structure:\n$$\np(h_j = 1 \\mid \\mathbf{v}) = \\sigma\\!\\left(b_j + \\sum_{i=1}^{d} W_{ij} v_i \\right),\\quad\np(v_i = 1 \\mid \\mathbf{h}) = \\sigma\\!\\left(c_i + \\sum_{j=1}^{m} W_{ij} h_j \\right),\n$$\nwhere $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the logistic function.\n\nIn a two-layer DBN with visible layer $\\mathbf{v} \\in \\{0,1\\}^{d}$, first hidden layer $\\mathbf{h}^{(1)} \\in \\{0,1\\}^{m}$, and second hidden layer $\\mathbf{h}^{(2)} \\in \\{0,1\\}^{n}$, consider a deterministic mean-field upward pass (recognition model) followed by a deterministic downward pass (generative model). The upward pass uses recognition weights $W_{1r} \\in \\mathbb{R}^{d \\times m}$ and $W_{2r} \\in \\mathbb{R}^{m \\times n}$ with biases $\\mathbf{b}_1 \\in \\mathbb{R}^{m}$ and $\\mathbf{b}_2 \\in \\mathbb{R}^{n}$:\n$$\n\\mathbf{h}^{(1)} = \\sigma\\!\\left(\\mathbf{v}^\\top W_{1r} + \\mathbf{b}_1\\right), \\quad\n\\mathbf{h}^{(2)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(1)}\\right)^\\top W_{2r} + \\mathbf{b}_2\\right).\n$$\nThe downward pass uses generative weights $W_{2g} \\in \\mathbb{R}^{n \\times m}$ and $W_{1g} \\in \\mathbb{R}^{m \\times d}$ with biases $\\mathbf{c}_1 \\in \\mathbb{R}^{m}$ and $\\mathbf{c}_0 \\in \\mathbb{R}^{d}$:\n$$\n\\tilde{\\mathbf{h}}^{(1)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(2)}\\right)^\\top W_{2g} + \\mathbf{c}_1\\right), \\quad\n\\tilde{\\mathbf{v}} = \\sigma\\!\\left(\\left(\\tilde{\\mathbf{h}}^{(1)}\\right)^\\top W_{1g} + \\mathbf{c}_0\\right).\n$$\nWe define the generative consistency score for a dataset $\\mathcal{D} = \\{\\mathbf{v}^{(k)}\\}_{k=1}^{K}$ as the mean component-wise binary cross-entropy between the original visible vectors and their deterministic reconstructions $\\tilde{\\mathbf{v}}^{(k)}$ after the up-down pass:\n$$\nJ = \\frac{1}{K d} \\sum_{k=1}^{K} \\sum_{i=1}^{d} \\left[ - v^{(k)}_i \\log\\!\\left(\\tilde{v}^{(k)}_i\\right) - \\left(1 - v^{(k)}_i\\right) \\log\\!\\left(1 - \\tilde{v}^{(k)}_i\\right) \\right].\n$$\nTo ensure numerical stability, use $\\epsilon = 10^{-12}$ and replace each $\\tilde{v}^{(k)}_i$ by $\\min\\!\\left(\\max\\!\\left(\\tilde{v}^{(k)}_i, \\epsilon\\right), 1 - \\epsilon\\right)$ before evaluating logarithms.\n\nYour task is to write a program that computes $J$ for four test cases that explore tied versus untied weights between the upward and downward passes, measuring the impact on generative consistency. Use the following fixed dataset with $K = 3$ and $d = 4$:\n$$\n\\mathbf{v}^{(1)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\\quad\n\\mathbf{v}^{(2)} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix},\\quad\n\\mathbf{v}^{(3)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\nLet $m = 3$ and $n = 2$. For all cases, the upward (recognition) parameters are fixed as:\n$$\nW_{1r} = \\begin{bmatrix}\n0.8  -0.4  0.3 \\\\\n0.1  0.5  -0.6 \\\\\n0.7  -0.2  0.2 \\\\\n-0.5  0.3  0.4\n\\end{bmatrix},\\quad\n\\mathbf{b}_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix},\n$$\n$$\nW_{2r} = \\begin{bmatrix}\n0.6  -0.3 \\\\\n0.4  0.2 \\\\\n-0.7  0.5\n\\end{bmatrix},\\quad\n\\mathbf{b}_2 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\end{bmatrix}.\n$$\nEvaluate the following four cases for the downward (generative) parameters:\n\nCase $1$ (tied weights, moderate magnitudes):\n$$\nW_{2g} = W_{2r}^\\top = \\begin{bmatrix} 0.6  0.4  -0.7 \\\\ -0.3  0.2  0.5 \\end{bmatrix},\\quad\n\\mathbf{c}_1 = \\begin{bmatrix} -0.05 \\\\ 0.1 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{1g} = W_{1r}^\\top = \\begin{bmatrix}\n0.8  0.1  0.7  -0.5 \\\\\n-0.4  0.5  -0.2  0.3 \\\\\n0.3  -0.6  0.2  0.4\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.\n$$\n\nCase $2$ (untied weights, small perturbations):\n$$\nW_{2g} = W_{2r}^\\top + \\Delta_2,\\quad\n\\Delta_2 = \\begin{bmatrix} 0.01  -0.02  0.02 \\\\ -0.02  0.03  -0.01 \\end{bmatrix},\n$$\n$$\n\\mathbf{c}_1 = \\begin{bmatrix} -0.02 \\\\ 0.05 \\\\ -0.01 \\end{bmatrix},\\quad\nW_{1g} = W_{1r}^\\top + \\Delta_1,\n$$\n$$\n\\Delta_1 = \\begin{bmatrix}\n0.02  -0.01  0.03  -0.02 \\\\\n-0.03  0.02  0.01  -0.01 \\\\\n0.01  0.00  -0.02  0.03\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} 0.02 \\\\ -0.01 \\\\ 0.0 \\\\ -0.02 \\end{bmatrix}.\n$$\n\nCase $3$ (untied weights, severe mismatch):\n$$\nW_{2g} = - W_{2r}^\\top + M_2,\\quad M_2 = \\begin{bmatrix} 0.0  0.0  0.0 \\\\ 0.2  -0.2  0.2 \\end{bmatrix},\n$$\n$$\n\\mathbf{c}_1 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\end{bmatrix},\\quad\nW_{1g} = - W_{1r}^\\top + M_1,\n$$\n$$\nM_1 = \\begin{bmatrix}\n0.0  0.0  0.0  0.0 \\\\\n0.1  -0.1  0.1  -0.1 \\\\\n-0.2  0.2  -0.2  0.2\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} -0.3 \\\\ 0.3 \\\\ -0.3 \\\\ 0.3 \\end{bmatrix}.\n$$\n\nCase $4$ (tied weights at boundary, all zeros):\n$$\nW_{2g} = W_{2r}^\\top = \\begin{bmatrix} 0.0  0.0  0.0 \\\\ 0.0  0.0  0.0 \\end{bmatrix},\\quad\n\\mathbf{c}_1 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{1g} = W_{1r}^\\top = \\begin{bmatrix}\n0.0  0.0  0.0  0.0 \\\\\n0.0  0.0  0.0  0.0 \\\\\n0.0  0.0  0.0  0.0\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\nwith the upward (recognition) parameters set to\n$$\nW_{1r} = \\begin{bmatrix}\n0.0  0.0  0.0 \\\\\n0.0  0.0  0.0 \\\\\n0.0  0.0  0.0 \\\\\n0.0  0.0  0.0\n\\end{bmatrix},\\quad\n\\mathbf{b}_1 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{2r} = \\begin{bmatrix}\n0.0  0.0 \\\\\n0.0  0.0 \\\\\n0.0  0.0\n\\end{bmatrix},\\quad\n\\mathbf{b}_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}.\n$$\n\nImplement the deterministic mean-field upward and downward mappings as specified above, apply the numerical stabilizer $\\epsilon$ in the cross-entropy, and compute the generative consistency score $J$ for each case. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$). Each result must be a floating-point number. Round each result to $6$ decimal places in the final output.",
            "solution": "We start from the foundational definitions of the Restricted Boltzmann Machine (RBM). The RBM energy function for Bernoulli visible units $\\mathbf{v} \\in \\{0,1\\}^d$ and Bernoulli hidden units $\\mathbf{h} \\in \\{0,1\\}^m$ is\n$$\nE(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{c}^\\top \\mathbf{v} - \\mathbf{b}^\\top \\mathbf{h} - \\mathbf{v}^\\top W \\mathbf{h}.\n$$\nBecause the RBM graph is bipartite, the conditional distributions factorize over units. By marginalizing and using the Boltzmann distribution $p(\\mathbf{x}) \\propto \\exp(-E(\\mathbf{x}))$, one obtains the sigmoidal conditionals:\n$$\np(h_j = 1 \\mid \\mathbf{v}) = \\sigma\\!\\left(b_j + \\sum_{i=1}^{d} W_{ij} v_i \\right), \\quad\np(v_i = 1 \\mid \\mathbf{h}) = \\sigma\\!\\left(c_i + \\sum_{j=1}^{m} W_{ij} h_j \\right),\n$$\nwith $\\sigma(x) = \\frac{1}{1 + e^{-x}}$. These follow from the fact that, for a Bernoulli unit, the log-odds is the affine input given by the corresponding bias plus the weighted sum from connected units.\n\nA Deep Belief Network (DBN) stacks RBMs. In a two-layer DBN, the upward pass (recognition model) maps the visible layer to the top hidden layer deterministically using mean-field expectations (sigmoid probabilities). With recognition weights $W_{1r}$ and $W_{2r}$ and upward biases $\\mathbf{b}_1$ and $\\mathbf{b}_2$, the upward transformations are:\n$$\n\\mathbf{h}^{(1)} = \\sigma\\!\\left(\\mathbf{v}^\\top W_{1r} + \\mathbf{b}_1\\right),\n$$\n$$\n\\mathbf{h}^{(2)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(1)}\\right)^\\top W_{2r} + \\mathbf{b}_2\\right).\n$$\nThe downward pass (generative model) maps the top hidden layer back to the visible layer via generative weights $W_{2g}$ and $W_{1g}$ and downward biases $\\mathbf{c}_1$ and $\\mathbf{c}_0$:\n$$\n\\tilde{\\mathbf{h}}^{(1)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(2)}\\right)^\\top W_{2g} + \\mathbf{c}_1\\right),\n$$\n$$\n\\tilde{\\mathbf{v}} = \\sigma\\!\\left(\\left(\\tilde{\\mathbf{h}}^{(1)}\\right)^\\top W_{1g} + \\mathbf{c}_0\\right).\n$$\nTied weights correspond to $W_{1g} = W_{1r}^\\top$ and $W_{2g} = W_{2r}^\\top$, enforcing a symmetry between recognition and generative mappings. Untied weights break this symmetry by allowing $W_{1g}$ and $W_{2g}$ to deviate from the transposes.\n\nTo quantify generative consistency, we use the mean component-wise binary cross-entropy between the original visible vectors and their reconstructions after an upward-downward pass:\n$$\nJ = \\frac{1}{K d} \\sum_{k=1}^{K} \\sum_{i=1}^{d} \\left[ - v^{(k)}_i \\log\\!\\left(\\tilde{v}^{(k)}_i\\right) - \\left(1 - v^{(k)}_i\\right) \\log\\!\\left(1 - \\tilde{v}^{(k)}_i\\right) \\right].\n$$\nThis expression arises from the negative log-likelihood of independent Bernoulli outputs $\\tilde{v}^{(k)}_i$ for targets $v^{(k)}_i$, averaged across samples and dimensions. For numerical stability, before evaluating $\\log(\\cdot)$, we clamp probabilities via $\\tilde{v}^{(k)}_i \\leftarrow \\min\\!\\left(\\max\\!\\left(\\tilde{v}^{(k)}_i, \\epsilon\\right), 1 - \\epsilon\\right)$ with $\\epsilon = 10^{-12}$, ensuring arguments lie in $(0,1)$.\n\nAlgorithmically, for each case:\n$1.$ For each data vector $\\mathbf{v}^{(k)}$, compute $\\mathbf{h}^{(1)}$ using $W_{1r}$ and $\\mathbf{b}_1$.\n$2.$ Compute $\\mathbf{h}^{(2)}$ using $W_{2r}$ and $\\mathbf{b}_2$.\n$3.$ Compute $\\tilde{\\mathbf{h}}^{(1)}$ using $W_{2g}$ and $\\mathbf{c}_1$.\n$4.$ Compute $\\tilde{\\mathbf{v}}$ using $W_{1g}$ and $\\mathbf{c}_0$.\n$5.$ Clamp $\\tilde{\\mathbf{v}}$ component-wise to $[\\epsilon, 1 - \\epsilon]$.\n$6.$ Accumulate the cross-entropy for that $\\mathbf{v}^{(k)}$ against $\\tilde{\\mathbf{v}}$, then average over $K$ and divide by $d$ to obtain $J$.\n\nFrom principle-based reasoning, tied weights improve consistency because the upward mapping approximates an inference of sufficient statistics under the RBM conditionals, and the downward mapping with transposed weights enforces a matched linear transformation back toward the visible space. In the mean-field setting with moderate weights and balanced biases, this tends to reduce distortion between $\\mathbf{v}$ and $\\tilde{\\mathbf{v}}$. Small perturbations in untied weights introduce asymmetry, modestly degrading $J$. Severe mismatches reverse or distort the mapping, causing $\\tilde{\\mathbf{v}}$ to reflect a different generative manifold, thereby increasing the cross-entropy substantially. The boundary case with all-zero weights (and zero biases) yields $\\sigma(0) = 0.5$ at all units, making reconstructions uninformative and leading to a fixed cross-entropy determined solely by the entropy of Bernoulli targets relative to $0.5$ predictions.\n\nThe program implements these steps directly, uses the provided matrices and biases for each case, computes $J$ for the fixed dataset, and prints the four results as a single bracketed, comma-separated list with each value rounded to $6$ decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigmoid(x):\n    # Numerically stable sigmoid using float64\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef clamp_probs(p, eps=1e-12):\n    # Clamp probabilities to (eps, 1-eps) for numerical stability in logs\n    return np.clip(p, eps, 1.0 - eps)\n\ndef generative_consistency_score(V, W1r, b1, W2r, b2, W2g, c1, W1g, c0, eps=1e-12):\n    \"\"\"\n    Compute mean component-wise binary cross-entropy between original visible vectors V\n    and reconstructions after upward-downward deterministic mean-field pass.\n    \"\"\"\n    K, d = V.shape\n    total_ce = 0.0\n    for k in range(K):\n        v = V[k]  # shape (d,)\n        # Upward pass: v - h1 - h2\n        h1_input = v @ W1r + b1  # shape (m,)\n        h1 = sigmoid(h1_input)\n        h2_input = h1 @ W2r + b2  # shape (n,)\n        h2 = sigmoid(h2_input)\n        # Downward pass: h2 - h1_down - v_hat\n        h1_down_input = h2 @ W2g + c1  # shape (m,)\n        h1_down = sigmoid(h1_down_input)\n        v_hat_input = h1_down @ W1g + c0  # shape (d,)\n        v_hat = sigmoid(v_hat_input)\n        v_hat = clamp_probs(v_hat, eps=eps)\n        # Binary cross-entropy per component\n        ce_vec = -(v * np.log(v_hat) + (1.0 - v) * np.log(1.0 - v_hat))\n        total_ce += np.sum(ce_vec)\n    # Mean across samples and dimensions\n    J = total_ce / (K * d)\n    return J\n\ndef solve():\n    # Fixed dataset V (K=3, d=4)\n    V = np.array([\n        [1.0, 0.0, 1.0, 0.0],\n        [0.0, 1.0, 0.0, 1.0],\n        [1.0, 1.0, 0.0, 0.0]\n    ], dtype=np.float64)\n\n    # Recognition (upward) parameters for cases 1-3\n    W1r_base = np.array([\n        [0.8, -0.4, 0.3],\n        [0.1,  0.5, -0.6],\n        [0.7, -0.2, 0.2],\n        [-0.5, 0.3, 0.4]\n    ], dtype=np.float64)\n    b1_base = np.array([0.1, -0.2, 0.05], dtype=np.float64)\n\n    W2r_base = np.array([\n        [0.6, -0.3],\n        [0.4,  0.2],\n        [-0.7, 0.5]\n    ], dtype=np.float64)\n    b2_base = np.array([0.0, 0.1], dtype=np.float64)\n\n    # Case 1: tied weights\n    W2g_1 = W2r_base.T.copy()\n    c1_1 = np.array([-0.05, 0.1, 0.0], dtype=np.float64)\n    W1g_1 = W1r_base.T.copy()\n    c0_1 = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float64)\n\n    # Case 2: untied (small perturbations)\n    Delta2 = np.array([\n        [0.01, -0.02, 0.02],\n        [-0.02, 0.03, -0.01]\n    ], dtype=np.float64)\n    W2g_2 = W2r_base.T + Delta2\n    c1_2 = np.array([-0.02, 0.05, -0.01], dtype=np.float64)\n    Delta1 = np.array([\n        [0.02, -0.01, 0.03, -0.02],\n        [-0.03,  0.02, 0.01, -0.01],\n        [0.01,  0.00, -0.02, 0.03]\n    ], dtype=np.float64)\n    W1g_2 = W1r_base.T + Delta1\n    c0_2 = np.array([0.02, -0.01, 0.0, -0.02], dtype=np.float64)\n\n    # Case 3: untied (severe mismatch)\n    M2 = np.array([\n        [0.0, 0.0, 0.0],\n        [0.2, -0.2, 0.2]\n    ], dtype=np.float64)\n    W2g_3 = -W2r_base.T + M2\n    c1_3 = np.array([0.5, -0.5, 0.5], dtype=np.float64)\n    M1 = np.array([\n        [0.0,  0.0,  0.0,  0.0],\n        [0.1, -0.1,  0.1, -0.1],\n        [-0.2, 0.2, -0.2, 0.2]\n    ], dtype=np.float64)\n    W1g_3 = -W1r_base.T + M1\n    c0_3 = np.array([-0.3, 0.3, -0.3, 0.3], dtype=np.float64)\n\n    # Case 4: boundary all zeros (tied zeros)\n    W1r_4 = np.zeros((4, 3), dtype=np.float64)\n    b1_4 = np.zeros(3, dtype=np.float64)\n    W2r_4 = np.zeros((3, 2), dtype=np.float64)\n    b2_4 = np.zeros(2, dtype=np.float64)\n    W2g_4 = W2r_4.T.copy()  # zeros\n    c1_4 = np.zeros(3, dtype=np.float64)\n    W1g_4 = W1r_4.T.copy()  # zeros\n    c0_4 = np.zeros(4, dtype=np.float64)\n\n    test_cases = [\n        # Case 1: tied weights moderate\n        {\n            \"W1r\": W1r_base, \"b1\": b1_base, \"W2r\": W2r_base, \"b2\": b2_base,\n            \"W2g\": W2g_1, \"c1\": c1_1, \"W1g\": W1g_1, \"c0\": c0_1\n        },\n        # Case 2: untied small perturbations\n        {\n            \"W1r\": W1r_base, \"b1\": b1_base, \"W2r\": W2r_base, \"b2\": b2_base,\n            \"W2g\": W2g_2, \"c1\": c1_2, \"W1g\": W1g_2, \"c0\": c0_2\n        },\n        # Case 3: untied severe mismatch\n        {\n            \"W1r\": W1r_base, \"b1\": b1_base, \"W2r\": W2r_base, \"b2\": b2_base,\n            \"W2g\": W2g_3, \"c1\": c1_3, \"W1g\": W1g_3, \"c0\": c0_3\n        },\n        # Case 4: boundary all zeros (tied zeros)\n        {\n            \"W1r\": W1r_4, \"b1\": b1_4, \"W2r\": W2r_4, \"b2\": b2_4,\n            \"W2g\": W2g_4, \"c1\": c1_4, \"W1g\": W1g_4, \"c0\": c0_4\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        J = generative_consistency_score(\n            V,\n            case[\"W1r\"], case[\"b1\"],\n            case[\"W2r\"], case[\"b2\"],\n            case[\"W2g\"], case[\"c1\"],\n            case[\"W1g\"], case[\"c0\"],\n            eps=1e-12\n        )\n        results.append(f\"{J:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While standard DBNs are effective for many data types, their fully-connected structure is inefficient for handling data with strong spatial locality, such as images. This advanced practice challenges you to evolve the basic RBM into a Convolutional RBM (CRBM), which incorporates weight sharing and local receptive fields. By deriving the conditional probability equations from first principles and implementing them, you will bridge the conceptual gap between early generative models and modern convolutional networks, grasping how the foundational ideas of energy-based learning are adapted for high-dimensional, structured data .",
            "id": "3112333",
            "problem": "You are asked to implement conditional probability computations for a Convolutional Restricted Boltzmann Machine (CRBM) with weight sharing and overlapping receptive fields, and to verify the implementation on a suite of deterministic test cases that emulate small image patches in the style of the Canadian Institute for Advanced Research (CIFAR) dataset. Your derivation and implementation must begin only from the fundamental definitions of a Restricted Boltzmann Machine (RBM) as an undirected graphical model with binary units. Assume the following foundational base: an RBM is defined by a joint distribution over visible units $v$ and hidden units $h$ given by the Boltzmann distribution $P(v,h) \\propto \\exp\\left(-E(v,h)\\right)$, where $E(v,h)$ is an energy function that is linear in parameters and bilinear in $v$ and $h$, and the conditional distributions $P(h \\mid v)$ and $P(v \\mid h)$ factorize over units. You must, by reasoning from these base definitions, derive the conditional sampling equations for a convolutional variant with weight sharing under arbitrary stride and valid boundaries, where receptive fields may overlap.\n\nTask requirements:\n- Model assumptions: binary visible units $v$ and binary hidden units $h$; weight sharing across spatial locations induced by a bank of filters; valid convolutional connectivity without padding; stride specified by two integers for height and width. Overlapping receptive fields occur whenever stride is smaller than the filter size.\n- From the RBM base, derive expressions for the conditional activation probabilities $P(h \\mid v)$ and $P(v \\mid h)$ for the convolutional architecture with overlapping receptive fields and shared weights. Your derivation must be principled from the base definition without relying on any pre-stated convolutional RBM formulas. Explain how overlapping receptive fields change the contributions to each visible unit in $P(v \\mid h)$.\n- Implementation constraints: implement two deterministic functions that compute conditional activation probabilities, not samples:\n  1. A function that computes hidden activation probabilities $P(h=1 \\mid v)$ using a filter bank, valid boundaries, and a given stride.\n  2. A function that computes visible activation probabilities $P(v=1 \\mid h)$ that correctly aggregates contributions from all overlapping receptive fields, which is equivalent to a strided transposed convolution of the hidden maps with the filter bank, plus visible biases.\n- Deterministic evaluation protocol: in order to avoid stochasticity, when computing $P(v=1 \\mid h)$ for a given visible input $v$, use a single mean-field pass where hidden binary states are replaced by their conditional expectations, that is, set $h$ equal to $P(h=1 \\mid v)$ in the computation of $P(v=1 \\mid h)$. No random sampling is permitted.\n- Numerical conventions: use the logistic sigmoid for binary unit activation probabilities. All computations should be carried out in standard floating point arithmetic. No physical units are involved.\n- Final output format: your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and must itself be a list of floating-point numbers. For each test case, the list is formed by concatenating the hidden activation probabilities $P(h=1 \\mid v)$ flattened in filter-major then row-major order, followed by the visible activation probabilities $P(v=1 \\mid h)$ flattened in channel-major then row-major order. For clarity: filter-major means all positions for filter $0$, then all positions for filter $1$, and so on; channel-major means all positions for channel $0$, then channel $1$, and so on.\n\nTest suite specification:\nImplement your code to exactly process the following three test cases. All arrays are explicitly given; all integers and real numbers below are exact problem constants.\n\n- Test case A (non-overlapping receptive fields):\n  - Visible tensor $v \\in \\{0,1\\}^{C \\times H \\times W}$ with $C=1$, $H=4$, $W=4$:\n    $$\n    v[0] =\n    \\begin{bmatrix}\n    1  0  1  0 \\\\\n    0  1  0  1 \\\\\n    1  1  0  0 \\\\\n    0  0  1  1\n    \\end{bmatrix}\n    $$\n  - Filter bank $W \\in \\mathbb{R}^{F \\times C \\times K_H \\times K_W}$ with $F=1$, $C=1$, $K_H=2$, $K_W=2$:\n    $$\n    W[0,0] =\n    \\begin{bmatrix}\n    0.2  -0.1 \\\\\n    0.0  0.1\n    \\end{bmatrix}\n    $$\n  - Hidden biases $b \\in \\mathbb{R}^{F}$: $b = [0.0]$.\n  - Visible biases $a \\in \\mathbb{R}^{C}$: $a = [0.0]$.\n  - Stride $(s_H,s_W) = (2,2)$.\n\n- Test case B (maximally overlapping receptive fields):\n  - Visible tensor $v \\in \\{0,1\\}^{1 \\times 3 \\times 3}$:\n    $$\n    v[0] =\n    \\begin{bmatrix}\n    1  1  0 \\\\\n    0  1  0 \\\\\n    1  0  1\n    \\end{bmatrix}\n    $$\n  - Filter bank $W \\in \\mathbb{R}^{1 \\times 1 \\times 2 \\times 2}$:\n    $$\n    W[0,0] =\n    \\begin{bmatrix}\n    0.5  0.0 \\\\\n    0.0  -0.5\n    \\end{bmatrix}\n    $$\n  - Hidden biases $b = [-0.5]$.\n  - Visible biases $a = [0.1]$.\n  - Stride $(1,1)$.\n\n- Test case C (multiple channels and filters, overlapping receptive fields):\n  - Visible tensor $v \\in \\{0,1\\}^{3 \\times 4 \\times 4}$ with channels indexed by $c \\in \\{0,1,2\\}$, each $4 \\times 4$:\n    $$\n    v[0] =\n    \\begin{bmatrix}\n    1  0  1  0 \\\\\n    1  1  0  0 \\\\\n    0  1  1  0 \\\\\n    0  0  1  1\n    \\end{bmatrix},\\quad\n    v[1] =\n    \\begin{bmatrix}\n    0  1  0  1 \\\\\n    1  0  1  0 \\\\\n    0  1  0  1 \\\\\n    1  0  0  1\n    \\end{bmatrix},\\quad\n    v[2] =\n    \\begin{bmatrix}\n    1  1  0  0 \\\\\n    0  1  0  1 \\\\\n    1  0  1  0 \\\\\n    0  1  1  0\n    \\end{bmatrix}\n    $$\n  - Filter bank $W \\in \\mathbb{R}^{2 \\times 3 \\times 2 \\times 2}$ with two filters $f \\in \\{0,1\\}$, three channels $c \\in \\{0,1,2\\}$, kernel size $2 \\times 2$:\n    $\n    W[0,0]=\\begin{bmatrix}0.10.0\\\\0.00.1\\end{bmatrix},\n    $\n    $\n    W[0,1]=\\begin{bmatrix}0.00.1\\\\-0.10.0\\end{bmatrix},\n    $\n    $\n    W[0,2]=\\begin{bmatrix}0.05-0.05\\\\0.05-0.05\\end{bmatrix},\n    $\n    $$\n    W[1,0]=\\begin{bmatrix}-0.10.1\\\\0.1-0.1\\end{bmatrix},\n    $$\n    $\n    W[1,1]=\\begin{bmatrix}0.050.05\\\\0.050.05\\end{bmatrix},\n    $\n    $\n    W[1,2]=\\begin{bmatrix}0.00.1\\\\0.0-0.1\\end{bmatrix}.\n    $\n  - Hidden biases $b = [0.2, -0.1]$.\n  - Visible biases $a = [0.05, -0.02, 0.03]$.\n  - Stride $(1,1)$.\n\nImplementation details to respect:\n- Use valid boundaries, so that for visible height $H$, width $W$, kernel height $K_H$, kernel width $K_W$, and stride $(s_H,s_W)$, the hidden map has spatial size $H_h \\times W_h$ with $H_h = \\left\\lfloor \\frac{H-K_H}{s_H} \\right\\rfloor + 1$ and $W_h = \\left\\lfloor \\frac{W-K_W}{s_W} \\right\\rfloor + 1$.\n- When forming $P(v=1 \\mid h)$ under valid connectivity and stride, the visible spatial size must be $H_v \\times W_v$ with $H_v = (H_h - 1)\\cdot s_H + K_H$ and $W_v = (W_h - 1)\\cdot s_W + K_W$, and each visible unitâ€™s pre-activation must include the sum of contributions from all hidden units whose receptive field covers that visible unit. This corresponds to a strided transposed convolution using the same filter weights, along with visible biases.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and must be a list of floats equal to the concatenation of the hidden probabilities flattened in filter-major then row-major order and the visible probabilities flattened in channel-major then row-major order, in that exact order. No other text should be printed.",
            "solution": "The problem requires the derivation and implementation of conditional probability calculations for a Convolutional Restricted Boltzmann Machine (CRBM) with binary units. The derivation must start from the fundamental principles of a standard Restricted Boltzmann Machine (RBM).\n\nA standard RBM is an undirected graphical model with a layer of visible units $v$ and a layer of hidden units $h$. The joint probability distribution over $(v, h)$ is given by the Boltzmann distribution:\n$$P(v, h) = \\frac{1}{Z} e^{-E(v,h)}$$\nwhere $Z$ is the partition function, and $E(v, h)$ is the energy function. For binary units $v_i \\in \\{0, 1\\}$ and $h_j \\in \\{0, 1\\}$, a common energy function is:\n$$E(v, h) = - \\sum_{i,j} W_{ij} v_i h_j - \\sum_i a_i v_i - \\sum_j b_j h_j = -v^T W h - a^T v - b^T h$$\nHere, $W$ is the weight matrix, while $a$ and $b$ are the visible and hidden bias vectors, respectively. A key property of RBMs is that the conditional distributions factorize due to the bipartite structure of the graph (no intra-layer connections):\n$$P(h \\mid v) = \\prod_j P(h_j \\mid v) \\quad \\text{and} \\quad P(v \\mid h) = \\prod_i P(v_i \\mid h)$$\nThe conditional activation probability for a single hidden unit $h_j$ is derived from the energy function. The terms in $E(v,h)$ involving $h_j$ are $-(\\sum_i v_i W_{ij} + b_j)h_j$. This gives the conditional probability as a logistic sigmoid function of its input:\n$$P(h_j=1 \\mid v) = \\sigma\\left(\\sum_i W_{ij} v_i + b_j\\right)$$\nwhere $\\sigma(x) = 1/(1+e^{-x})$. Similarly, for a single visible unit $v_i$:\n$$P(v_i=1 \\mid h) = \\sigma\\left(\\sum_j W_{ij} h_j + a_i\\right)$$\n\nWe now extend this formulation to a Convolutional RBM (CRBM). The visible and hidden units are arranged in tensors, and weights are shared across spatial locations.\n- Visible layer $v$: A tensor of size $C \\times H \\times W$ with $C$ channels and spatial dimensions $H \\times W$.\n- Hidden layer $h$: A tensor of size $F \\times H_h \\times W_h$ with $F$ feature maps (filters) and spatial dimensions $H_h \\times W_h$.\n- Filter bank $W$: A tensor of size $F \\times C \\times K_H \\times K_W$, representing $F$ filters, each of size $C \\times K_H \\times K_W$.\n- Hidden biases $b$: A vector of size $F$, with one bias $b_f$ for each feature map.\n- Visible biases $a$: A vector of size $C$, with one bias $a_c$ for each visible channel.\n\nThe energy function of the CRBM is a sum of interactions between hidden units and their receptive fields in the visible layer, plus bias terms:\n$$E(v, h) = -\\sum_{f=0}^{F-1} \\sum_{i=0}^{H_h-1} \\sum_{j=0}^{W_h-1} h_{f,i,j} \\left( \\sum_{c=0}^{C-1} \\sum_{p=0}^{K_H-1} \\sum_{q=0}^{K_W-1} W_{f,c,p,q} v_{c, i \\cdot s_H+p, j \\cdot s_W+q} \\right) - \\sum_{f,i,j} b_f h_{f,i,j} - \\sum_{c,k,l} a_c v_{c,k,l}$$\nThe term in the parenthesis is a valid cross-correlation operation with stride $(s_H, s_W)$. Let's denote it as $(W_f \\star v)_{i,j}$. The energy function simplifies to:\n$$E(v, h) = - \\sum_{f,i,j} h_{f,i,j} \\left( (W_f \\star v)_{i,j} + b_f \\right) - \\sum_{c,k,l} a_c v_{c,k,l}$$\n\n**Derivation of $P(h \\mid v)$ (Visible to Hidden)**\nAs in the standard RBM, the hidden units are conditionally independent given the visible layer. We consider a single hidden unit $h_{f,i,j}$. The terms in the energy function that depend on this specific unit are:\n$$-h_{f,i,j} \\left( (W_f \\star v)_{i,j} + b_f \\right)$$\nThe conditional activation probability is therefore the sigmoid of the coefficient of $-h_{f,i,j}$:\n$$P(h_{f,i,j}=1 \\mid v) = \\sigma\\left( (W_f \\star v)_{i,j} + b_f \\right)$$\nThis is a standard feedforward convolutional operation. For each filter $f$, we convolve it with the visible tensor $v$ across all channels, add the corresponding scalar bias $b_f$ to all spatial locations of the resulting feature map, and then apply the sigmoid activation function. The dimensions of the hidden map are given by the valid convolution formula: $H_h = \\left\\lfloor \\frac{H-K_H}{s_H} \\right\\rfloor + 1$ and $W_h = \\left\\lfloor \\frac{W-K_W}{s_W} \\right\\rfloor + 1$.\n\n**Derivation of $P(v \\mid h)$ (Hidden to Visible)**\nSimilarly, the visible units are conditionally independent given the hidden layer. To find the activation probability for a single visible unit $v_{c,k,l}$, we must sum all terms in the energy function where it appears.\n$$E(v, h) = -v_{c,k,l} \\left( \\sum_{\\substack{f,i,j \\text{ s.t. } \\\\ (k,l) \\in \\text{RF}(i,j)}} W_{f,c,k-i s_H, l-j s_W} h_{f,i,j} \\right) - a_c v_{c,k,l} + \\dots$$\nwhere $\\text{RF}(i,j)$ denotes the receptive field of hidden unit $h_{f,i,j}$. A visible unit $v_{c,k,l}$ is in the receptive field of $h_{f,i,j}$ if $i s_H \\le k  i s_H + K_H$ and $j s_W \\le l  j s_W + K_W$.\nThe conditional activation probability is the sigmoid of the coefficient of $-v_{c,k,l}$:\n$$P(v_{c,k,l}=1 \\mid h) = \\sigma\\left( \\sum_{f=0}^{F-1} \\sum_{i=0}^{H_h-1} \\sum_{j=0}^{W_h-1} \\mathbb{I}((k,l) \\in \\text{RF}(i,j)) \\cdot h_{f,i,j} \\cdot W_{f,c,k-i s_H, l-j s_W} + a_c \\right)$$\nwhere $\\mathbb{I}(\\cdot)$ is an indicator function. This operation sums contributions from all hidden units whose receptive fields cover the visible unit $v_{c,k,l}$. This is precisely the definition of a strided transposed convolution (sometimes called deconvolution). For each filter map $h_f$, we perform a transposed convolution with the corresponding filter weights $W_{f,c}$ to generate a contribution to the pre-activation of visible channel $c$. The final pre-activation for channel $c$ is the sum of contributions from all filter maps, plus the visible bias $a_c$. The spatial size of the reconstructed visible layer matches the original input size, provided the forward pass used valid convolution: $H_v = (H_h-1)s_H + K_H, W_v = (W_h-1)s_W + K_W$.\n\nThe problem specifies a deterministic evaluation protocol. To compute $P(v=1 \\mid h)$, the binary hidden states $h_{f,i,j}$ are replaced with their conditional expectations, which are the probabilities $P(h_{f,i,j}=1 \\mid v)$ computed in the first step. This is a single mean-field update step.\n\nThe implementation will consist of two functions:\n1.  `v_to_h`: Computes $P(h=1 \\mid v)$ via strided convolution.\n2.  `h_to_v`: Computes $P(v=1 \\mid h)$ via strided transposed convolution, using $h = P(h=1 \\mid v)$ as input.\n\nThese functions will be used to process the given test cases. The final output is a concatenation of the flattened hidden and visible probability tensors, formatted as specified.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\ndef v_to_h(v, W, b, stride):\n    \"\"\"\n    Computes hidden activation probabilities P(h=1|v) for a CRBM.\n    This corresponds to a standard convolution operation.\n    \"\"\"\n    C, H, W_dim = v.shape\n    F, C_W, K_H, K_W = W.shape\n    s_H, s_W = stride\n\n    if C != C_W:\n        raise ValueError(\"Channel dimension mismatch between visible tensor and filter bank.\")\n\n    H_h = (H - K_H) // s_H + 1\n    W_h = (W_dim - K_W) // s_W + 1\n\n    h_probs = np.zeros((F, H_h, W_h))\n\n    for f in range(F):\n        h_pre_activation = np.zeros((H_h, W_h))\n        for i in range(H_h):\n            for j in range(W_h):\n                v_patch = v[:, i * s_H : i * s_H + K_H, j * s_W : j * s_W + K_W]\n                # Convolution is sum of element-wise product\n                h_pre_activation[i, j] = np.sum(v_patch * W[f])\n        \n        # Add bias and apply sigmoid\n        h_probs[f] = sigmoid(h_pre_activation + b[f])\n        \n    return h_probs\n\ndef h_to_v(h, W, a, stride):\n    \"\"\"\n    Computes visible activation probabilities P(v=1|h) for a CRBM.\n    This corresponds to a strided transposed convolution operation.\n    \"\"\"\n    F, H_h, W_h = h.shape\n    F_W, C, K_H, K_W = W.shape\n    s_H, s_W = stride\n\n    if F != F_W:\n        raise ValueError(\"Filter dimension mismatch between hidden tensor and filter bank.\")\n\n    H_v = (H_h - 1) * s_H + K_H\n    W_v = (W_h - 1) * s_W + K_W\n\n    v_pre_activation = np.zeros((C, H_v, W_v))\n\n    for f in range(F):\n        for i in range(H_h):\n            for j in range(W_h):\n                h_val = h[f, i, j]\n                # Add contribution to the corresponding patch in the visible pre-activation map\n                v_pre_activation[:, i * s_H : i * s_H + K_H, j * s_W : j * s_W + K_W] += h_val * W[f]\n\n    # Add visible biases, broadcasting over spatial dimensions\n    v_pre_activation += a[:, np.newaxis, np.newaxis]\n\n    # Apply sigmoid\n    v_probs = sigmoid(v_pre_activation)\n    \n    return v_probs\n\ndef solve():\n    \"\"\"\n    Main function to solve the specified test cases.\n    \"\"\"\n    test_cases = [\n        # Test Case A\n        {\n            \"v\": np.array([[[1, 0, 1, 0], [0, 1, 0, 1], [1, 1, 0, 0], [0, 0, 1, 1]]], dtype=float),\n            \"W\": np.array([[[[0.2, -0.1], [0.0, 0.1]]]], dtype=float),\n            \"b\": np.array([0.0], dtype=float),\n            \"a\": np.array([0.0], dtype=float),\n            \"stride\": (2, 2)\n        },\n        # Test Case B\n        {\n            \"v\": np.array([[[1, 1, 0], [0, 1, 0], [1, 0, 1]]], dtype=float),\n            \"W\": np.array([[[[0.5, 0.0], [0.0, -0.5]]]], dtype=float),\n            \"b\": np.array([-0.5], dtype=float),\n            \"a\": np.array([0.1], dtype=float),\n            \"stride\": (1, 1)\n        },\n        # Test Case C\n        {\n            \"v\": np.array([\n                [[1, 0, 1, 0], [1, 1, 0, 0], [0, 1, 1, 0], [0, 0, 1, 1]],\n                [[0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 0, 1], [1, 0, 0, 1]],\n                [[1, 1, 0, 0], [0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 1, 0]]\n            ], dtype=float),\n            \"W\": np.array([\n                [[[0.1, 0.0], [0.0, 0.1]], [[0.0, 0.1], [-0.1, 0.0]], [[0.05, -0.05], [0.05, -0.05]]],\n                [[[-0.1, 0.1], [0.1, -0.1]], [[0.05, 0.05], [0.05, 0.05]], [[0.0, 0.1], [0.0, -0.1]]]\n            ], dtype=float),\n            \"b\": np.array([0.2, -0.1], dtype=float),\n            \"a\": np.array([0.05, -0.02, 0.03], dtype=float),\n            \"stride\": (1, 1)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        v = case[\"v\"]\n        W = case[\"W\"]\n        b = case[\"b\"]\n        a = case[\"a\"]\n        stride = case[\"stride\"]\n\n        # 1. Compute P(h=1|v)\n        h_probs = v_to_h(v, W, b, stride)\n\n        # 2. Compute P(v=1|h) using mean-field approximation for h\n        v_probs = h_to_v(h_probs, W, a, stride)\n\n        # 3. Flatten results as specified\n        # h_probs layout (F, Hh, Wh) - flatten filter-major, then row-major\n        # v_probs layout (C, Hv, Wv) - flatten channel-major, then row-major\n        # np.flatten() uses row-major (C-style) order by default, which is what is needed.\n        h_flat = h_probs.flatten()\n        v_flat = v_probs.flatten()\n\n        # Concatenate and append\n        full_result = np.concatenate((h_flat, v_flat)).tolist()\n        results.append(full_result)\n    \n    # Format the final output string\n    # str(a_list) gives \"[item1, item2, ...]\"\n    # We join these strings with commas, then enclose in brackets.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}