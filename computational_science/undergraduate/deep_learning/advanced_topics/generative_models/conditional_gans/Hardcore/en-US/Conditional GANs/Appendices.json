{
    "hands_on_practices": [
        {
            "introduction": "A central challenge in designing conditional Generative Adversarial Networks (cGANs) is how to effectively inject the conditional information $y$ into the generator architecture. One powerful and widely-used technique is Conditional Batch Normalization, where the affine parameters for normalization, the scale $\\gamma$ and shift $\\beta$, are not learned as fixed vectors but are dynamically generated as functions of $y$. This practice  provides a foundational exercise in understanding the mechanics of this process by asking you to derive the precise number of learnable parameters this conditioning method adds to a network, reinforcing a crucial skill in model design and complexity analysis.",
            "id": "3108910",
            "problem": "Consider a generator in a conditional Generative Adversarial Network (cGAN), where each normalization operation uses conditional Batch Normalization (BN). For each of $L$ layers, indexed by $\\ell \\in \\{1,2,\\dots,L\\}$, the feature map has $C_{\\ell}$ channels, and the BN affine parameters are not fixed; instead, they are functions of the conditioning variable $y$. Specifically, for each layer $\\ell$, the scale and shift are vectors $\\gamma_{\\ell}(y) \\in \\mathbb{R}^{C_{\\ell}}$ and $\\beta_{\\ell}(y) \\in \\mathbb{R}^{C_{\\ell}}$, respectively. The conditioning variable $y$ is mapped to an embedding $e(y) \\in \\mathbb{R}^{d}$ by some embedding mechanism (do not count the parameters of the embedding itself).\n\nAssume that for each layer $\\ell$, both $\\gamma_{\\ell}(y)$ and $\\beta_{\\ell}(y)$ are parameterized as affine functions of the embedding $e(y)$, implemented by separate fully connected layers. That is, each of $\\gamma_{\\ell}(y)$ and $\\beta_{\\ell}(y)$ is produced by a learned affine map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{C_{\\ell}}$.\n\nStarting from the fundamental definition that an affine map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{m}$ has a parameter count equal to the number of entries in its weight matrix plus the number of entries in its bias vector, derive a closed-form analytical expression for the total number of learned parameters used across all $L$ layers to generate the conditional BN parameters $\\{\\gamma_{\\ell}(y), \\beta_{\\ell}(y)\\}_{\\ell=1}^{L}$. Express your final answer as a single analytical expression in terms of $L$, $\\{C_{\\ell}\\}_{\\ell=1}^{L}$, and $d$.",
            "solution": "The problem requires the derivation of a closed-form analytical expression for the total number of learned parameters used to generate the conditional Batch Normalization (BN) parameters, specifically the scale $\\gamma_{\\ell}(y)$ and shift $\\beta_{\\ell}(y)$, across all $L$ layers of a generator network.\n\nFirst, we must establish the number of parameters for a single affine map as defined in the problem. An affine map from a vector space $\\mathbb{R}^{d}$ to another vector space $\\mathbb{R}^{m}$ is a function of the form $f(x) = Wx + b$, where $x \\in \\mathbb{R}^{d}$, $W$ is a weight matrix, and $b \\in \\mathbb{R}^{m}$ is a bias vector. For the dimensions to be consistent, the matrix $W$ must be of size $m \\times d$. The number of parameters in this map is the sum of the number of elements in $W$ and the number of elements in $b$.\nThe number of elements in the weight matrix $W$ is $m \\times d$.\nThe number of elements in the bias vector $b$ is $m$.\nTherefore, the total number of parameters for a single affine map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{m}$ is $md + m$, which can be factored as $m(d+1)$.\n\nThe problem states that for each layer $\\ell \\in \\{1, 2, \\dots, L\\}$, the generator has a feature map with $C_{\\ell}$ channels. The conditional BN parameters for this layer, $\\gamma_{\\ell}(y) \\in \\mathbb{R}^{C_{\\ell}}$ and $\\beta_{\\ell}(y) \\in \\mathbb{R}^{C_{\\ell}}$, are generated from the conditioning embedding $e(y) \\in \\mathbb{R}^{d}$.\n\nLet's analyze the number of parameters for a single layer $\\ell$.\n\n1.  **Parameters for the scale vector $\\gamma_{\\ell}(y)$:**\n    The problem specifies that $\\gamma_{\\ell}(y)$ is produced by a learned affine map from the embedding space $\\mathbb{R}^{d}$ to the channel space $\\mathbb{R}^{C_{\\ell}}$.\n    Here, the input dimension is $d$ (from $e(y)$), and the output dimension is $m = C_{\\ell}$ (since $\\gamma_{\\ell}(y)$ is a vector of length $C_{\\ell}$).\n    Using our formula for the number of parameters in an affine map, the number of parameters for generating $\\gamma_{\\ell}(y)$, let's call it $N_{\\gamma, \\ell}$, is:\n    $$N_{\\gamma, \\ell} = C_{\\ell}d + C_{\\ell}$$\n\n2.  **Parameters for the shift vector $\\beta_{\\ell}(y)$:**\n    Similarly, $\\beta_{\\ell}(y)$ is produced by a separate learned affine map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{C_{\\ell}}$. The structure is identical to that for $\\gamma_{\\ell}(y)$.\n    The input dimension is $d$, and the output dimension is $m = C_{\\ell}$.\n    The number of parameters for generating $\\beta_{\\ell}(y)$, let's call it $N_{\\beta, \\ell}$, is:\n    $$N_{\\beta, \\ell} = C_{\\ell}d + C_{\\ell}$$\n\nThe total number of parameters for the conditional BN in a single layer $\\ell$, denoted as $N_{\\ell}$, is the sum of the parameters for $\\gamma_{\\ell}(y)$ and $\\beta_{\\ell}(y)$. The problem states these are implemented by *separate* fully connected layers, so we sum their parameter counts.\n$$N_{\\ell} = N_{\\gamma, \\ell} + N_{\\beta, \\ell} = (C_{\\ell}d + C_{\\ell}) + (C_{\\ell}d + C_{\\ell})$$\n$$N_{\\ell} = 2C_{\\ell}d + 2C_{\\ell}$$\nThis expression can be factored to:\n$$N_{\\ell} = 2C_{\\ell}(d+1)$$\n\nFinally, to find the total number of learned parameters, $N_{\\text{total}}$, across all $L$ layers of the generator, we must sum the parameters $N_{\\ell}$ for each layer from $\\ell=1$ to $L$. The problem states we should not count the parameters of the embedding mechanism $e(y)$ itself.\n$$N_{\\text{total}} = \\sum_{\\ell=1}^{L} N_{\\ell}$$\nSubstituting the expression for $N_{\\ell}$:\n$$N_{\\text{total}} = \\sum_{\\ell=1}^{L} 2C_{\\ell}(d+1)$$\nThe term $2(d+1)$ is a constant with respect to the summation index $\\ell$ and can be factored out of the summation.\n$$N_{\\text{total}} = 2(d+1) \\sum_{\\ell=1}^{L} C_{\\ell}$$\nThis is the closed-form analytical expression for the total number of learned parameters used across all $L$ layers to generate the conditional BN parameters. It is expressed in terms of the given quantities $L$, $\\{C_{\\ell}\\}_{\\ell=1}^{L}$, and $d$.",
            "answer": "$$\n\\boxed{2(d+1) \\sum_{\\ell=1}^{L} C_{\\ell}}\n$$"
        },
        {
            "introduction": "Beyond architectural design, controlling the behavior of a generator is paramount. A good generator should not merely replicate training samples but learn a smooth and generalizable mapping from the latent and conditional spaces to the data space, especially when dealing with continuous conditions. This exercise  delves into this challenge by framing the generator's learning task as a regularized optimization problem. By implementing a smoothness penalty based on the generator's derivative with respect to its condition, you will gain a hands-on understanding of how regularization can improve a model's ability to interpolate and extrapolate, illustrating the fundamental trade-off between data fidelity and generalization.",
            "id": "3108842",
            "problem": "You are asked to design and analyze a simplified conditional Generative Adversarial Network (cGAN) with continuous conditioning on an angle. The goal is to ground the design in first principles, formalize a smoothness regularization on the generator with respect to the conditioning variable, and empirically test extrapolation beyond the training range. All angles must be in radians.\n\nConsider a conditional Generative Adversarial Network (cGAN) with a generator defined as a function $G:\\mathbb{R}^d\\times\\mathbb{R}\\to\\mathbb{R}^2$, where the conditioning variable is $y\\in\\mathbb{R}$, interpreted as an angle, and the noise $z\\in\\mathbb{R}^d$ is a standard normal. The real data distribution is a conditional distribution $p_{\\text{data}}(x\\mid y)$ whose conditional mean is $m(y)=[\\cos(y),\\sin(y)]^\\top$ and whose conditional covariance is $\\sigma^2 I_2$ for a fixed $\\sigma>0$. The angles are continuous and the conditioning space is the real line.\n\nYou will analyze a restricted generator class of the form\n$$\nG(z,y;\\theta)=u(y)+s\\,z,\\quad z\\sim\\mathcal{N}(0,I_2),\n$$\nwhere $u:\\mathbb{R}\\to\\mathbb{R}^2$ is a deterministic mean map and $s\\ge 0$ is a scalar. Assume $u(y)$ is parameterized by a degree-$2$ polynomial basis in $y$, i.e., for $\\phi(y)=[1,\\,y,\\,y^2]^\\top\\in\\mathbb{R}^3$ there exists a matrix $W\\in\\mathbb{R}^{2\\times 3}$ such that\n$$\nu(y)=W\\,\\phi(y).\n$$\nLet the smoothness penalty be the squared $\\ell_2$ norm of the generator’s Jacobian with respect to $y$ for the deterministic mean part:\n$$\n\\mathcal{S}(W)=\\mathbb{E}_{y\\sim p_{\\text{train}}}\\left[\\left\\|\\frac{\\partial u}{\\partial y}(y)\\right\\|_2^2\\right],\n$$\nwhere $\\frac{\\partial u}{\\partial y}(y)=W\\,\\phi'(y)$ and $\\phi'(y)=\\frac{d}{dy}\\phi(y)=[0,\\,1,\\,2y]^\\top$.\n\nYou will approximate expectations with empirical averages over a uniform grid of training angles. Define an empirical objective that balances mean matching, covariance matching, and smoothness:\n$$\n\\mathcal{L}(W,s)=\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi(y_i)-m(y_i)\\right\\|_2^2\\;+\\;\\lambda_{\\text{smooth}}\\cdot\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi'(y_i)\\right\\|_2^2\\;+\\;\\lambda_{\\text{cov}}\\cdot\\left\\|s^2 I_2-\\sigma^2 I_2\\right\\|_F^2,\n$$\nwhere $y_i$ are training angles, $N$ is the number of training angles, $\\lambda_{\\text{smooth}}\\ge 0$ is the smoothness weight, and $\\lambda_{\\text{cov}}\\ge 0$ is the covariance matching weight.\n\nTask:\n- Starting from the core definitions of conditional Generative Adversarial Networks (cGANs), empirical risk minimization, and the definition of the smoothness penalty above, derive the necessary optimality conditions and provide a computational method to obtain the optimal parameters $W^\\star$ and $s^\\star$ that minimize $\\mathcal{L}(W,s)$.\n- Implement a program that:\n  - Constructs training sets of angles as uniform grids in the specified ranges.\n  - Computes $W^\\star$ and $s^\\star$ from first principles of least squares with derivative regularization.\n  - Evaluates the fitted mean map $u^\\star(y)=W^\\star\\phi(y)$ on specified test angles and reports the mean squared error (MSE) against the target $m(y)$, averaged over all test angles and the two output dimensions.\n- Use $\\sigma=0.2$, $\\lambda_{\\text{cov}}=1$, and $N=101$ evenly spaced training angles within each specified range (inclusive). All angles are in radians.\n\nTest suite:\n- Case $1$ (general case): training range $[-\\pi/2,\\;\\pi/2]$, $\\lambda_{\\text{smooth}}=0.1$, test angles $\\{-\\pi/2,\\;0,\\;\\pi/2,\\;3\\pi/4\\}$.\n- Case $2$ (no smoothness): training range $[-\\pi/2,\\;\\pi/2]$, $\\lambda_{\\text{smooth}}=0$, test angles $\\{-\\pi/2,\\;0,\\;\\pi/2,\\;3\\pi/4\\}$.\n- Case $3$ (strong smoothness): training range $[-\\pi/2,\\;\\pi/2]$, $\\lambda_{\\text{smooth}}=10$, test angles $\\{-\\pi/2,\\;-\\pi/4,\\;\\pi/2,\\;\\pi\\}$.\n- Case $4$ (narrow training range): training range $[-\\pi/4,\\;\\pi/4]$, $\\lambda_{\\text{smooth}}=0.1$, test angles $\\{-\\pi/4,\\;0,\\;\\pi/4,\\;3\\pi/4\\}$.\n\nYour program must output a single line containing the four resulting MSE values (in this order) as a comma-separated list enclosed in square brackets, for example: \"[v1,v2,v3,v4]\". The values must be floating-point numbers. No units are required beyond the specification that all angles are in radians. The program must not read any input.",
            "solution": "The user has provided a valid, well-posed problem grounded in the principles of statistical machine learning and generative models. The task is to derive and implement a solution for a simplified conditional Generative Adversarial Network (cGAN) parameter estimation problem formulated as an empirical risk minimization task with a smoothness penalty.\n\nThe problem is to find parameters $W^\\star \\in \\mathbb{R}^{2\\times 3}$ and $s^\\star \\ge 0$ that minimize the objective function:\n$$\n\\mathcal{L}(W,s)=\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi(y_i)-m(y_i)\\right\\|_2^2\\;+\\;\\lambda_{\\text{smooth}}\\cdot\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi'(y_i)\\right\\|_2^2\\;+\\;\\lambda_{\\text{cov}}\\cdot\\left\\|s^2 I_2-\\sigma^2 I_2\\right\\|_F^2\n$$\nThis objective embodies the principle of empirical risk minimization. The first term is a mean-squared error loss that encourages the generator's conditional mean $u(y) = W\\phi(y)$ to match the true data's conditional mean $m(y)$. The third term matches the generator's conditional covariance, $s^2 I_2$, to the true data's conditional covariance, $\\sigma^2 I_2$. The second term is a regularization penalty to enforce smoothness on the learned mean map $u(y)$ with respect to the conditioning variable $y$.\n\nThe objective function $\\mathcal{L}(W,s)$ can be decomposed into two independent parts: one depending only on $W$ and the other only on $s$.\n$$\n\\mathcal{L}(W,s) = \\mathcal{L}_W(W) + \\mathcal{L}_s(s)\n$$\nwhere\n$$\n\\mathcal{L}_W(W) = \\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi(y_i)-m(y_i)\\right\\|_2^2 + \\lambda_{\\text{smooth}}\\cdot\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi'(y_i)\\right\\|_2^2\n$$\nand\n$$\n\\mathcal{L}_s(s) = \\lambda_{\\text{cov}}\\cdot\\left\\|s^2 I_2-\\sigma^2 I_2\\right\\|_F^2\n$$\nWe can minimize these two parts independently to find the optimal parameters $W^\\star$ and $s^\\star$.\n\n**Optimization of the Scale Parameter $s$**\n\nWe must minimize $\\mathcal{L}_s(s)$ with respect to $s \\ge 0$. The squared Frobenius norm is the sum of the squares of the matrix elements. The matrix $s^2 I_2-\\sigma^2 I_2$ is a diagonal matrix:\n$$\ns^2 I_2-\\sigma^2 I_2 = \\begin{pmatrix} s^2 - \\sigma^2 & 0 \\\\ 0 & s^2 - \\sigma^2 \\end{pmatrix}\n$$\nIts squared Frobenius norm is:\n$$\n\\left\\|s^2 I_2-\\sigma^2 I_2\\right\\|_F^2 = (s^2 - \\sigma^2)^2 + 0^2 + 0^2 + (s^2 - \\sigma^2)^2 = 2(s^2 - \\sigma^2)^2\n$$\nThe objective becomes $\\mathcal{L}_s(s) = 2 \\lambda_{\\text{cov}} (s^2 - \\sigma^2)^2$. Since $\\lambda_{\\text{cov}} > 0$, this expression is a non-negative quantity minimized when its base is zero, i.e., $s^2 - \\sigma^2 = 0$. This implies $s^2 = \\sigma^2$. Given the constraint $s \\ge 0$ and the fact that $\\sigma > 0$, the unique optimal scale parameter is $s^\\star = \\sigma$. For the given problem, $\\sigma = 0.2$, so $s^\\star = 0.2$.\n\n**Optimization of the Weight Matrix $W$**\n\nThe objective for $W$ is a regularized least-squares problem. Let $W$ be composed of two row vectors $w_1^\\top$ and $w_2^\\top$, so $W = \\begin{pmatrix} w_1^\\top \\\\ w_2^\\top \\end{pmatrix}$, where $w_1, w_2 \\in \\mathbb{R}^3$. The target mean is $m(y) = [\\cos(y), \\sin(y)]^\\top = [m_1(y), m_2(y)]^\\top$.\nThe objective $\\mathcal{L}_W(W)$ can be separated for each row of $W$ because the squared Euclidean norm $\\|a-b\\|_2^2$ splits over components. Ignoring the constant factor $1/N$, the total cost for $W$ is a sum of costs for $w_1$ and $w_2$:\n$$\nJ(w_1, w_2) = \\sum_{i=1}^{N} \\left[ (w_1^\\top\\phi(y_i) - m_1(y_i))^2 + (w_2^\\top\\phi(y_i) - m_2(y_i))^2 \\right] + \\lambda_{\\text{smooth}}\\sum_{i=1}^{N} \\left[ (w_1^\\top\\phi'(y_i))^2 + (w_2^\\top\\phi'(y_i))^2 \\right]\n$$\nThis decouples into two independent optimization problems, one for $w_1$ and one for $w_2$. Let's formulate the problem for a generic weight vector $w_k \\in \\mathbb{R}^3$ and target function $m_k(y)$, for $k \\in \\{1, 2\\}$. The objective is to minimize:\n$$\nJ(w_k) = \\sum_{i=1}^{N} (w_k^\\top \\phi(y_i) - m_k(y_i))^2 + \\lambda_{\\text{smooth}} \\sum_{i=1}^{N} (w_k^\\top \\phi'(y_i))^2\n$$\nThis is a standard regularized least-squares problem. To solve it, we express it in matrix form. Let the training angles be $\\{y_i\\}_{i=1}^N$. Define the design matrix $\\Phi \\in \\mathbb{R}^{N\\times 3}$ whose $i$-th row is $\\phi(y_i)^\\top = [1, y_i, y_i^2]$. Define the derivative basis matrix $\\Phi' \\in \\mathbb{R}^{N\\times 3}$ with rows $\\phi'(y_i)^\\top = [0, 1, 2y_i]$. Let $M_k \\in \\mathbb{R}^N$ be the vector of target values $[m_k(y_1), \\dots, m_k(y_N)]^\\top$.\nThe objective can be written as:\n$$\nJ(w_k) = \\| \\Phi w_k - M_k \\|_2^2 + \\lambda_{\\text{smooth}} \\| \\Phi' w_k \\|_2^2\n$$\nTo find the minimum, we take the gradient with respect to $w_k$ and set it to zero:\n$$\n\\nabla_{w_k} J(w_k) = 2 \\Phi^\\top (\\Phi w_k - M_k) + 2 \\lambda_{\\text{smooth}} \\Phi'^\\top \\Phi' w_k = 0\n$$\nRearranging the terms, we get the normal equations for this regularized problem:\n$$\n(\\Phi^\\top \\Phi) w_k - \\Phi^\\top M_k + \\lambda_{\\text{smooth}} (\\Phi'^\\top \\Phi') w_k = 0\n$$\n$$\n(\\Phi^\\top \\Phi + \\lambda_{\\text{smooth}} \\Phi'^\\top \\Phi') w_k = \\Phi^\\top M_k\n$$\nThis is a linear system of the form $A w_k = b_k$, where the matrix $A = \\Phi^\\top\\Phi + \\lambda_{\\text{smooth}}\\Phi'^\\top\\Phi'$ is a $3 \\times 3$ matrix and the vector $b_k = \\Phi^\\top M_k$ is a $3 \\times 1$ vector. The matrix $A$ is symmetric and positive definite (for $\\lambda_{\\text{smooth}} > 0$ and non-collinear training points), guaranteeing a unique solution.\nWe solve this system for $k=1$ and $k=2$ to obtain the optimal weight vectors $w_1^\\star$ and $w_2^\\star$:\n- For $w_1^\\star$, we use $M_1 = [\\cos(y_1), \\dots, \\cos(y_N)]^\\top$.\n- For $w_2^\\star$, we use $M_2 = [\\sin(y_1), \\dots, \\sin(y_N)]^\\top$.\nThe optimal weight matrix is then $W^\\star = \\begin{pmatrix} w_1^{\\star\\top} \\\\ w_2^{\\star\\top} \\end{pmatrix}$.\n\n**Computational Method and Evaluation**\n\nThe computational procedure is as follows:\n1.  For each test case, generate the grid of $N=101$ training angles $y_i$ in the specified range.\n2.  Construct the matrices $\\Phi$ and $\\Phi'$ and the target vectors $M_1$ and $M_2$ from the training data.\n3.  Compute the matrix $A = (\\Phi^\\top \\Phi) + \\lambda_{\\text{smooth}} (\\Phi'^\\top \\Phi')$ and the right-hand-side vectors $b_1 = \\Phi^\\top M_1$ and $b_2 = \\Phi^\\top M_2$.\n4.  Solve the two $3 \\times 3$ linear systems $A w_1 = b_1$ and $A w_2 = b_2$ to find $w_1^\\star$ and $w_2^\\star$.\n5.  Assemble the optimal weight matrix $W^\\star$.\n6.  For the set of test angles $\\{y_j^{\\text{test}}\\}_{j=1}^{K}$, compute the predicted mean map values $u^\\star(y_j^{\\text{test}}) = W^\\star \\phi(y_j^{\\text{test}})$.\n7.  Calculate the final Mean Squared Error (MSE), averaged over the $K$ test points and the two output dimensions, as specified:\n$$\n\\text{MSE} = \\frac{1}{K} \\sum_{j=1}^{K} \\frac{1}{2} \\left\\| u^\\star(y_j^{\\text{test}}) - m(y_j^{\\text{test}}) \\right\\|_2^2\n$$\n\nThis procedure provides a complete solution to the problem based on the principles of regularized empirical risk minimization.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It orchestrates the calculation for each case and prints the final result.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"train_range\": [-np.pi/2, np.pi/2],\n            \"lambda_smooth\": 0.1,\n            \"test_angles\": [-np.pi/2, 0, np.pi/2, 3*np.pi/4],\n        },\n        # Case 2 (no smoothness)\n        {\n            \"train_range\": [-np.pi/2, np.pi/2],\n            \"lambda_smooth\": 0.0,\n            \"test_angles\": [-np.pi/2, 0, np.pi/2, 3*np.pi/4],\n        },\n        # Case 3 (strong smoothness)\n        {\n            \"train_range\": [-np.pi/2, np.pi/2],\n            \"lambda_smooth\": 10.0,\n            \"test_angles\": [-np.pi/2, -np.pi/4, np.pi/2, np.pi],\n        },\n        # Case 4 (narrow training range)\n        {\n            \"train_range\": [-np.pi/4, np.pi/4],\n            \"lambda_smooth\": 0.1,\n            \"test_angles\": [-np.pi/4, 0, np.pi/4, 3*np.pi/4],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        mse = solve_case(\n            train_range=case[\"train_range\"],\n            lambda_smooth=case[\"lambda_smooth\"],\n            test_angles=case[\"test_angles\"]\n        )\n        results.append(mse)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\ndef solve_case(train_range, lambda_smooth, test_angles):\n    \"\"\"\n    Solves for the optimal parameters W* and evaluates the MSE for a single case.\n\n    Args:\n        train_range (list): A list [min_angle, max_angle] for training.\n        lambda_smooth (float): The smoothness regularization weight.\n        test_angles (list): A list of angles for evaluation.\n\n    Returns:\n        float: The calculated Mean Squared Error.\n    \"\"\"\n    N = 101 # Number of training samples\n\n    # 1. Construct training data\n    y_train = np.linspace(train_range[0], train_range[1], N)\n\n    # 2. Construct design matrices and target vectors\n    # Basis function matrix Phi, where Phi[i,:] = [1, y_i, y_i^2]\n    phi_train = np.c_[np.ones(N), y_train, y_train**2]\n    \n    # Derivative basis matrix Phi', where Phi'[i,:] = [0, 1, 2*y_i]\n    phi_prime_train = np.c_[np.zeros(N), np.ones(N), 2 * y_train]\n\n    # Target vectors M1 and M2 for cos(y) and sin(y)\n    m1_train = np.cos(y_train)\n    m2_train = np.sin(y_train)\n\n    # 3. Formulate and solve the normal equations\n    # A = (Phi^T * Phi + lambda * Phi'^T * Phi')\n    A = phi_train.T @ phi_train + lambda_smooth * (phi_prime_train.T @ phi_prime_train)\n\n    # b1 = Phi^T * M1\n    b1 = phi_train.T @ m1_train\n    # b2 = Phi^T * M2\n    b2 = phi_train.T @ m2_train\n\n    # Solve the linear systems A*w1 = b1 and A*w2 = b2\n    w1_star = np.linalg.solve(A, b1)\n    w2_star = np.linalg.solve(A, b2)\n\n    # The optimal weight matrix W*\n    W_star = np.array([w1_star, w2_star])\n\n    # 4. Evaluate the model on test angles\n    y_test = np.array(test_angles)\n    N_test = len(y_test)\n\n    # Construct basis matrix for test angles\n    phi_test = np.c_[np.ones(N_test), y_test, y_test**2]\n\n    # Predicted mean values u*(y) = W* * phi(y)\n    # phi_test has shape (N_test, 3), W_star has shape (2, 3)\n    # We want result shape (N_test, 2), so we compute (phi_test @ W_star.T)\n    u_star_test = phi_test @ W_star.T\n\n    # True mean values m(y)\n    m_test = np.c_[np.cos(y_test), np.sin(y_test)]\n\n    # 5. Calculate MSE\n    # Squared L2 norm of the error for each test point\n    squared_errors = np.sum((u_star_test - m_test)**2, axis=1)\n\n    # Average over test points and the two output dimensions\n    mse = np.mean(squared_errors) / 2.0\n    \n    return mse\n\nsolve()\n\n```"
        },
        {
            "introduction": "After building and training a cGAN, a critical question remains: is it truly faithful to its conditioning? A generator might produce high-quality samples that fail to reflect the specified label $y$, a common failure mode. This hands-on practice  introduces a principled method to quantify this \"$y$-faithfulness\" by using an external, \"expert\" classifier trained on real data to evaluate the generated samples. By comparing the accuracy of this classifier to one trained only on generated data, you will learn a powerful diagnostic technique to assess how well the generator has captured the true conditional data distribution, providing a quantitative tool that goes far beyond simple visual inspection.",
            "id": "3108919",
            "problem": "You are given a mathematical model of a conditional Generative Adversarial Network (cGAN) with a generator that produces features conditioned on a label. Your task is to quantify the “$y$-faithfulness” of the generator by training a frozen classifier and computing an empirical approximation of the expectation $\\mathbb{E}_{z,y}[\\mathbb{1}\\{C(G(z,y))=y\\}]$, then compare it with the accuracy of a discriminator’s auxiliary label head on the same generated data. All computations must be derived from first principles using the Gaussian generative model and Bayes decision theory, and implemented numerically.\n\nDefinitions and assumptions:\n- The latent variable is $z \\in \\mathbb{R}^{d}$ drawn from a standard multivariate normal distribution with independent components, i.e., $z \\sim \\mathcal{N}(0, I_{d})$.\n- The label is $y \\in \\{0,1,\\dots,K-1\\}$ drawn from a specified prior distribution $\\pi(y)$.\n- The generator is defined by the class-conditional distribution of the generated data $x \\in \\mathbb{R}^{d}$ given by\n  $$x \\mid y \\sim \\mathcal{N}(\\mu^{\\mathrm{gen}}_{y}, \\Sigma_{\\mathrm{gen}}), \\quad \\Sigma_{\\mathrm{gen}} = B B^{\\top}$$\n  where $B \\in \\mathbb{R}^{d \\times d}$ is a given full-rank matrix and $(\\mu^{\\mathrm{gen}}_{y})_{y=0}^{K-1}$ are given class means.\n- The “real” data distribution used to train a frozen classifier $C$ is Gaussian with shared covariance:\n  $$x \\mid y \\sim \\mathcal{N}(\\mu^{\\mathrm{real}}_{y}, \\Sigma_{\\mathrm{real}})$$\n  with a known shared covariance matrix $\\Sigma_{\\mathrm{real}} \\in \\mathbb{R}^{d \\times d}$ and class means $(\\mu^{\\mathrm{real}}_{y})_{y=0}^{K-1}$.\n\nTraining protocols:\n- The frozen classifier $C$ is trained using Linear Discriminant Analysis (LDA), which is the Bayes-optimal classifier under the assumption of class-conditional Gaussians with a shared covariance. You must estimate class means and the shared covariance via maximum likelihood from $n_{C}$ samples per class drawn from the real data distribution; then $C$ is frozen.\n- The discriminator’s label head, denoted $D_{y}$, is separately trained as an LDA classifier using $n_{D}$ samples per class drawn from the generator’s class-conditional distribution (i.e., on generated data). This simulates the discriminator’s auxiliary classifier trained in the cGAN setting to recognize $y$ from generated data.\n\nEvaluation metrics:\n- Define the $y$-faithfulness of the generator with respect to the frozen classifier $C$ as\n  $$\\mathbb{E}_{z,y}[\\mathbb{1}\\{C(G(z,y))=y\\}],$$\n  where the expectation is taken over $y \\sim \\pi(y)$ and $z \\sim \\mathcal{N}(0, I_{d})$. You must approximate this expectation by Monte Carlo with $n_{\\mathrm{eval}}$ independent samples.\n- On the same generated evaluation set, compute the classification accuracy of $D_{y}$:\n  $$\\mathbb{E}_{z,y}[\\mathbb{1}\\{D_{y}(G(z,y))=y\\}],$$\n  again approximated by Monte Carlo using the same $n_{\\mathrm{eval}}$ samples.\n\nFoundational base you must use:\n- The Bayes decision rule under the Gaussian class-conditional model with shared covariance leads to linear discriminant functions. Maximum likelihood estimates of class means and the pooled shared covariance must be used to instantiate the LDA classifiers. No other classification “shortcuts” are permitted.\n\nMonte Carlo approximation details:\n- To approximate $\\mathbb{E}_{z,y}[\\cdot]$, independently sample labels $y_{i} \\sim \\pi(y)$ and latent variables $z_{i} \\sim \\mathcal{N}(0, I_{d})$ for $i \\in \\{1,\\dots,n_{\\mathrm{eval}}\\}$, then form $x_{i} = G(z_{i}, y_{i})$ with $x_{i} \\sim \\mathcal{N}(\\mu^{\\mathrm{gen}}_{y_{i}}, \\Sigma_{\\mathrm{gen}})$, and compute the empirical average of the indicator. You must use a fixed random seed $0$ for reproducibility.\n\nTest suite:\nFor each case below, you must:\n- Train the frozen classifier $C$ on real data with $n_{C}$ samples per class.\n- Train $D_{y}$ on generated data with $n_{D}$ samples per class.\n- Evaluate both on an evaluation set of $n_{\\mathrm{eval}}$ generated samples drawn with the specified $\\pi(y)$.\n\nAll cases use dimension $d=2$. In all cases, the shared real covariance is $\\Sigma_{\\mathrm{real}} = \\sigma_{\\mathrm{real}}^{2} I_{2}$ and the generator covariance is $\\Sigma_{\\mathrm{gen}} = \\sigma_{\\mathrm{gen}}^{2} I_{2}$ with $B=\\sigma_{\\mathrm{gen}} I_{2}$.\n\n- Case $1$ (happy path, $K=3$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (3,0)$, $\\mu^{\\mathrm{real}}_{2} = (0,3)$.\n  - $\\mu^{\\mathrm{gen}}_{0} = (0.1,-0.1)$, $\\mu^{\\mathrm{gen}}_{1} = (3.1,0.2)$, $\\mu^{\\mathrm{gen}}_{2} = (-0.1,3.2)$.\n  - $\\sigma_{\\mathrm{real}} = 0.2$, $\\sigma_{\\mathrm{gen}} = 0.25$.\n  - $n_{C} = 50$, $n_{D} = 30$, $n_{\\mathrm{eval}} = 3000$.\n  - $\\pi(y) = (1/3, 1/3, 1/3)$.\n\n- Case $2$ (high overlap, $K=3$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (1.0,0.2)$, $\\mu^{\\mathrm{real}}_{2} = (0.2,1.0)$.\n  - $\\mu^{\\mathrm{gen}}_{0} = (0,0)$, $\\mu^{\\mathrm{gen}}_{1} = (1.0,0.2)$, $\\mu^{\\mathrm{gen}}_{2} = (0.2,1.0)$.\n  - $\\sigma_{\\mathrm{real}} = 0.6$, $\\sigma_{\\mathrm{gen}} = 0.7$.\n  - $n_{C} = 200$, $n_{D} = 50$, $n_{\\mathrm{eval}} = 3000$.\n  - $\\pi(y) = (1/3, 1/3, 1/3)$.\n\n- Case $3$ (non-uniform label prior, $K=3$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (3,0)$, $\\mu^{\\mathrm{real}}_{2} = (0,3)$.\n  - $\\mu^{\\mathrm{gen}}_{0} = (0.1,-0.1)$, $\\mu^{\\mathrm{gen}}_{1} = (3.1,0.2)$, $\\mu^{\\mathrm{gen}}_{2} = (-0.1,3.2)$.\n  - $\\sigma_{\\mathrm{real}} = 0.2$, $\\sigma_{\\mathrm{gen}} = 0.25$.\n  - $n_{C} = 50$, $n_{D} = 30$, $n_{\\mathrm{eval}} = 4000$.\n  - $\\pi(y) = (0.8, 0.1, 0.1)$.\n\n- Case $4$ (very small discriminator training, $K=2$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (2.5,0)$.\n  - $\\mu^{\\mathrm{gen}}_{0} = (0.2,-0.1)$, $\\mu^{\\mathrm{gen}}_{1} = (2.6,0.1)$.\n  - $\\sigma_{\\mathrm{real}} = 0.3$, $\\sigma_{\\mathrm{gen}} = 0.3$.\n  - $n_{C} = 40$, $n_{D} = 3$, $n_{\\mathrm{eval}} = 2000$.\n  - $\\pi(y) = (0.5, 0.5)$.\n\n- Case $5$ (near-perfect separation, $K=3$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (5,5)$, $\\mu^{\\mathrm{real}}_{2} = (-5,5)$.\n  - $\\mu^{\\mathrm{gen}}_{0} = (0,0)$, $\\mu^{\\mathrm{gen}}_{1} = (5,5)$, $\\mu^{\\mathrm{gen}}_{2} = (-5,5)$.\n  - $\\sigma_{\\mathrm{real}} = 0.2$, $\\sigma_{\\mathrm{gen}} = 0.1$.\n  - $n_{C} = 20$, $n_{D} = 20$, $n_{\\mathrm{eval}} = 3000$.\n  - $\\pi(y) = (1/3, 1/3, 1/3)$.\n\nAlgorithmic requirements:\n- Implement LDA from first principles: estimate class means $\\hat{\\mu}_{k}$ and pooled covariance $\\hat{\\Sigma}$ from training data, add a small ridge $\\lambda I_{d}$ with $\\lambda = 10^{-6}$ to ensure invertibility, compute discriminant scores for prediction, and use the class priors estimated from training frequencies.\n- Use the same generated evaluation set to compute both accuracies for a fair comparison.\n- Fix the random seed to $0$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a list of pairs, where each pair is $[\\text{acc}_{C}, \\text{acc}_{D}]$ for a case, in the order of the cases above. For example: \"[[0.95,0.93],[...],...]\". The entries must be floating-point numbers.\n\nNo physical units or angle units are involved. All numeric answers must be floats in decimal form. The program must be complete and runnable as specified, and must not require any user input.",
            "solution": "The problem is valid as it presents a well-defined, scientifically grounded computational task in statistical machine learning. It asks for the implementation and comparison of two classifiers based on Linear Discriminant Analysis (LDA) within a simulated conditional Generative Adversarial Network (cGAN) framework. All parameters, models, and evaluation procedures are specified completely and consistently.\n\nThe solution proceeds by first establishing the theoretical foundation of LDA as the Bayes-optimal classifier for the given problem context. Then, we detail the numerical implementation of the training and evaluation procedures.\n\n**1. Theoretical Foundation: Linear Discriminant Analysis**\n\nThe problem assumes that both the \"real\" and \"generated\" data follow class-conditional Gaussian distributions with a shared covariance matrix within each domain (real and generated). For a classification problem with $K$ classes, where the data $x \\in \\mathbb{R}^d$ for class $k \\in \\{0, \\dots, K-1\\}$ is drawn from a normal distribution $\\mathcal{N}(\\mu_k, \\Sigma)$, the Bayes-optimal decision rule assigns $x$ to the class that maximizes the posterior probability $P(y=k|x)$. Using Bayes' theorem, this is equivalent to maximizing $p(x|y=k)P(y=k)$. Taking the logarithm and dropping terms that are constant across all classes, we seek to maximize the discriminant score $\\delta_k(x)$:\n$$ \\delta_k(x) = \\log p(x|y=k) + \\log P(y=k) $$\nSubstituting the probability density function for the multivariate normal distribution:\n$$ \\log p(x|y=k) = -\\frac{1}{2}(x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k) - \\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\Sigma| $$\nExpanding the quadratic term and dropping terms independent of $k$ (i.e., $x^T\\Sigma^{-1}x$, $\\frac{d}{2}\\log(2\\pi)$, $\\frac{1}{2}\\log|\\Sigma|$), the decision rule simplifies to maximizing the following linear function of $x$:\n$$ \\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2}\\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k $$\nwhere $\\pi_k = P(y=k)$ is the prior probability of class $k$. The decision boundary between any two classes $i$ and $j$ is the set of points where $\\delta_i(x) = \\delta_j(x)$, which defines a hyperplane. This makes the classifier a linear one.\n\n**2. Parameter Estimation from Training Data**\n\nIn practice, the true parameters $\\mu_k$, $\\Sigma$, and $\\pi_k$ are unknown and must be estimated from a training set $\\{(x_i, y_i)\\}_{i=1}^N$. We use Maximum Likelihood Estimation (MLE), which yields the following estimates:\n- **Class Priors ($\\hat{\\pi}_k$):** The proportion of training samples in class $k$. For a training set with $N_k$ samples from class $k$ and $N$ total samples, $\\hat{\\pi}_k = N_k / N$.\n- **Class Means ($\\hat{\\mu}_k$):** The sample mean of all training data belonging to class $k$.\n$$ \\hat{\\mu}_k = \\frac{1}{N_k} \\sum_{i: y_i=k} x_i $$\n- **Pooled Covariance ($\\hat{\\Sigma}$):** The average of the individual class covariance matrices, weighted by their degrees of freedom. The unbiased estimator is:\n$$ \\hat{\\Sigma} = \\frac{1}{N-K} \\sum_{k=0}^{K-1} S_k = \\frac{1}{N-K} \\sum_{k=0}^{K-1} \\sum_{i: y_i=k} (x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^T $$\nFor numerical stability, especially when the number of samples is not much larger than the dimension $d$, the estimated covariance matrix is regularized by adding a small multiple of the identity matrix, $\\lambda I_d$, before inversion.\n\n**3. Simulation and Evaluation Procedure**\n\nThe overall process for each test case is as follows:\n\n1.  **Instantiate Classifiers:** We define two LDA classifiers, $C$ and $D_y$.\n\n2.  **Train Frozen Classifier $C$:**\n    - A training set is generated by drawing $n_C$ samples per class from the \"real\" data distribution, $x \\mid y \\sim \\mathcal{N}(\\mu^{\\mathrm{real}}_{y}, \\Sigma_{\\mathrm{real}})$.\n    - The classifier $C$ is trained on this dataset to learn the parameters $(\\hat{\\pi}^{\\mathrm{real}}, \\hat{\\mu}^{\\mathrm{real}}, \\hat{\\Sigma}^{\\mathrm{real}})$. Since the training data is balanced, $\\hat{\\pi}_k^{\\mathrm{real}} = 1/K$.\n\n3.  **Train Discriminator Head $D_y$:**\n    - A separate training set is generated by drawing $n_D$ samples per class from the generator's distribution, $x \\mid y \\sim \\mathcal{N}(\\mu^{\\mathrm{gen}}_{y}, \\Sigma_{\\mathrm{gen}})$.\n    - The classifier $D_y$ is trained on this generated dataset to learn its own set of parameters $(\\hat{\\pi}^{\\mathrm{gen}}, \\hat{\\mu}^{\\mathrm{gen}}, \\hat{\\Sigma}^{\\mathrm{gen}})$. Again, $\\hat{\\pi}_k^{\\mathrm{gen}} = 1/K$.\n\n4.  **Monte Carlo Evaluation:**\n    - A single, common evaluation set of $n_{\\mathrm{eval}}$ samples is created. This is done by first sampling $n_{\\mathrm{eval}}$ labels $\\{y_i\\}_{i=1}^{n_{\\mathrm{eval}}}$ from the specified prior distribution $\\pi(y)$. Then, for each label $y_i$, a corresponding data point $x_i$ is sampled from the generator's distribution $\\mathcal{N}(\\mu^{\\mathrm{gen}}_{y_i}, \\Sigma_{\\mathrm{gen}})$.\n    - The \"$y$-faithfulness\" is the accuracy of classifier $C$ on this evaluation set: $ \\mathrm{acc}_C = \\frac{1}{n_{\\mathrm{eval}}} \\sum_{i=1}^{n_{\\mathrm{eval}}} \\mathbb{1}\\{C(x_i)=y_i\\} $.\n    - The discriminator's accuracy is the accuracy of $D_y$ on the same evaluation set: $ \\mathrm{acc}_{D_y} = \\frac{1}{n_{\\mathrm{eval}}} \\sum_{i=1}^{n_{\\mathrm{eval}}} \\mathbb{1}\\{D_y(x_i)=y_i\\} $.\n\nThis procedure allows a direct comparison of how a classifier trained on real data perceives the generator's output versus how an auxiliary classifier trained on the generator's own output performs. Differences in their accuracies highlight the discrepancy between the real and generated data distributions. The entire simulation is performed using a fixed random seed to ensure reproducibility.",
            "answer": "```python\nimport numpy as np\n\n# A global random number generator to be used throughout the simulation for reproducibility.\nRNG = np.random.default_rng(0)\n\nclass LDAClassifier:\n    \"\"\"\n    A Linear Discriminant Analysis (LDA) classifier implemented from first principles.\n    Assumes Gaussian class-conditional distributions with a shared covariance matrix.\n    \"\"\"\n    def __init__(self, ridge_lambda=1e-6):\n        self.ridge_lambda = ridge_lambda\n        self.means = None\n        self.priors = None\n        self.inv_cov = None\n        self.discriminant_constants = None\n        self.n_classes = 0\n\n    def fit(self, X, y):\n        \"\"\"\n        Estimates LDA parameters from training data (X, y).\n        - Class priors from training frequencies.\n        - Class means.\n        - Pooled covariance matrix.\n        \"\"\"\n        n_samples, n_features = X.shape\n        unique_classes = np.unique(y)\n        self.n_classes = len(unique_classes)\n\n        # Estimate class priors from training frequencies\n        class_counts = np.array([np.sum(y == k) for k in range(self.n_classes)])\n        self.priors = class_counts / n_samples\n\n        # Estimate class means\n        self.means = np.array([X[y == k].mean(axis=0) for k in range(self.n_classes)])\n\n        # Estimate pooled covariance matrix\n        pooled_cov = np.zeros((n_features, n_features))\n        if n_samples > self.n_classes:\n            for k in range(self.n_classes):\n                class_samples = X[y == k]\n                if class_counts[k] > 0:\n                    residuals = class_samples - self.means[k]\n                    # Sum of squares matrix S_k = sum (x_i - mu_k)(x_i - mu_k)^T\n                    pooled_cov += residuals.T @ residuals\n            # Unbiased estimator denominator is N-K\n            pooled_cov /= (n_samples - self.n_classes)\n        else:\n            # Fallback for insufficient data (N <= K)\n            pooled_cov = np.identity(n_features)\n\n        # Add ridge regularization and compute inverse for numerical stability\n        reg_cov = pooled_cov + self.ridge_lambda * np.identity(n_features)\n        self.inv_cov = np.linalg.inv(reg_cov)\n        \n        # Pre-compute parts of the discriminant function for efficient prediction\n        # delta_k(x) = x.T @ inv_cov @ mu_k - 0.5 * mu_k.T @ inv_cov @ mu_k + log(pi_k)\n        log_priors = np.log(self.priors + 1e-12)  # Epsilon for log(0)\n        self.discriminant_constants = -0.5 * np.sum((self.means @ self.inv_cov) * self.means, axis=1) + log_priors\n\n    def predict(self, X):\n        \"\"\"\n        Predicts class labels for new data X.\n        \"\"\"\n        # Linear part of discriminant: X @ inv_cov @ means.T\n        scores = X @ self.inv_cov @ self.means.T\n        # Add constant part\n        scores += self.discriminant_constants\n        return np.argmax(scores, axis=1)\n\ndef generate_balanced_data(means, cov, n_samples_per_class, K):\n    \"\"\"Generates a balanced dataset with n_samples_per_class for each class.\"\"\"\n    d = cov.shape[0]\n    X = np.empty((n_samples_per_class * K, d))\n    y = np.empty(n_samples_per_class * K, dtype=int)\n    for k in range(K):\n        start_idx = k * n_samples_per_class\n        end_idx = (k + 1) * n_samples_per_class\n        X[start_idx:end_idx] = RNG.multivariate_normal(means[k], cov, size=n_samples_per_class)\n        y[start_idx:end_idx] = k\n    return X, y\n\ndef generate_eval_data(means, cov, n_eval, priors, K):\n    \"\"\"Generates an evaluation dataset with labels sampled from the prior distribution.\"\"\"\n    d = cov.shape[0]\n    # Sample labels from the prior distribution\n    y_eval = RNG.choice(K, size=n_eval, p=priors)\n    X_eval = np.empty((n_eval, d))\n    for k in range(K):\n        # Generate data for all samples of class k at once\n        class_mask = (y_eval == k)\n        n_k = np.sum(class_mask)\n        if n_k > 0:\n            X_eval[class_mask] = RNG.multivariate_normal(means[k], cov, size=n_k)\n    return X_eval, y_eval\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, K=3)\n        {\n            'K': 3, 'd': 2,\n            'mu_real': np.array([[0,0], [3,0], [0,3]]),\n            'mu_gen': np.array([[0.1,-0.1], [3.1,0.2], [-0.1,3.2]]),\n            'sigma_real': 0.2, 'sigma_gen': 0.25,\n            'n_C': 50, 'n_D': 30, 'n_eval': 3000,\n            'pi_y': np.array([1/3, 1/3, 1/3])\n        },\n        # Case 2 (high overlap, K=3)\n        {\n            'K': 3, 'd': 2,\n            'mu_real': np.array([[0,0], [1.0,0.2], [0.2,1.0]]),\n            'mu_gen': np.array([[0,0], [1.0,0.2], [0.2,1.0]]),\n            'sigma_real': 0.6, 'sigma_gen': 0.7,\n            'n_C': 200, 'n_D': 50, 'n_eval': 3000,\n            'pi_y': np.array([1/3, 1/3, 1/3])\n        },\n        # Case 3 (non-uniform label prior, K=3)\n        {\n            'K': 3, 'd': 2,\n            'mu_real': np.array([[0,0], [3,0], [0,3]]),\n            'mu_gen': np.array([[0.1,-0.1], [3.1,0.2], [-0.1,3.2]]),\n            'sigma_real': 0.2, 'sigma_gen': 0.25,\n            'n_C': 50, 'n_D': 30, 'n_eval': 4000,\n            'pi_y': np.array([0.8, 0.1, 0.1])\n        },\n        # Case 4 (very small discriminator training, K=2)\n        {\n            'K': 2, 'd': 2,\n            'mu_real': np.array([[0,0], [2.5,0]]),\n            'mu_gen': np.array([[0.2,-0.1], [2.6,0.1]]),\n            'sigma_real': 0.3, 'sigma_gen': 0.3,\n            'n_C': 40, 'n_D': 3, 'n_eval': 2000,\n            'pi_y': np.array([0.5, 0.5])\n        },\n        # Case 5 (near-perfect separation, K=3)\n        {\n            'K': 3, 'd': 2,\n            'mu_real': np.array([[0,0], [5,5], [-5,5]]),\n            'mu_gen': np.array([[0,0], [5,5], [-5,5]]),\n            'sigma_real': 0.2, 'sigma_gen': 0.1,\n            'n_C': 20, 'n_D': 20, 'n_eval': 3000,\n            'pi_y': np.array([1/3, 1/3, 1/3])\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        K, d = params['K'], params['d']\n        cov_real = (params['sigma_real']**2) * np.identity(d)\n        cov_gen = (params['sigma_gen']**2) * np.identity(d)\n\n        # 1. Train Classifier C on \"real\" data\n        X_real_train, y_real_train = generate_balanced_data(\n            params['mu_real'], cov_real, params['n_C'], K\n        )\n        classifier_C = LDAClassifier()\n        classifier_C.fit(X_real_train, y_real_train)\n        \n        # 2. Train Classifier D_y on \"generated\" data\n        X_gen_train, y_gen_train = generate_balanced_data(\n            params['mu_gen'], cov_gen, params['n_D'], K\n        )\n        classifier_Dy = LDAClassifier()\n        classifier_Dy.fit(X_gen_train, y_gen_train)\n        \n        # 3. Create common evaluation set from the generator\n        X_eval, y_eval = generate_eval_data(\n            params['mu_gen'], cov_gen, params['n_eval'], params['pi_y'], K\n        )\n        \n        # 4. Evaluate both classifiers on the same evaluation set\n        # Accuracy of C (y-faithfulness)\n        y_pred_C = classifier_C.predict(X_eval)\n        acc_C = np.mean(y_pred_C == y_eval)\n        \n        # Accuracy of Dy\n        y_pred_Dy = classifier_Dy.predict(X_eval)\n        acc_Dy = np.mean(y_pred_Dy == y_eval)\n        \n        results.append([acc_C, acc_Dy])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}