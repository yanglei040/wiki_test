{
    "hands_on_practices": [
        {
            "introduction": "在卷积神经网络的设计中，一个关键的转变是从使用大的卷积核（例如 $5 \\times 5$ 或 $7 \\times 7$）转向堆叠更小的 $3 \\times 3$ 卷积核。这个练习旨在揭示这一设计选择背后的基本原理。通过直接比较单个 $5 \\times 5$ 卷积层和两个连续的 $3 \\times 3$ 卷积层，您将亲手计算并验证，在达到相同感受野的同时，后者如何能够以更少的参数实现更强的非线性表达能力。",
            "id": "3126220",
            "problem": "考虑一个卷积神经网络（CNN），它被定义为一系列层的序列，其中每一层使用可学习的卷积核对多通道输入执行离散卷积，并可选择性地应用逐点非线性函数。设输入特征图为 $x \\in \\mathbb{R}^{H \\times W \\times C}$，具有 $C$ 个通道。本问题中的所有卷积都使用步长为 $1$，保留空间维度的零填充，并且不使用空洞卷积。单个卷积层将 $\\mathbb{R}^{H \\times W \\times C}$ 映射到 $\\mathbb{R}^{H \\times W \\times C}$，其方法是在每个输出位置从其卷积核覆盖的空间邻域以及所有输入通道中聚合信息。逐点非线性函数 $\\phi$ 被独立地应用于每个空间位置和每个通道。\n\n考虑两种架构：\n\n- 架构 A：一个带有 $5 \\times 5$ 卷积核的卷积层，之后应用一次非线性函数 $\\phi$。\n- 架构 B：两个连续的卷积层，每个都带有 $3 \\times 3$ 卷积核，在两层之后都应用非线性函数 $\\phi$。\n\n假设两种架构都保持通道数不变，即每一层都将 $C$ 个输入通道映射到 $C$ 个输出通道。卷积层中的每个输出通道由一个连接到所有 $C$ 个输入通道的可学习卷积核和一个对应于该输出通道的标量偏置生成。\n\n任务：\n1. 根据步长为 $1$ 的卷积组合下感受野增长的定义，确定架构 B 第二层中单个输出空间位置受影响的、不同输入空间位置的数量，即感受野的精确边长 $R$。\n2. 根据卷积层中每个输出通道拥有一个连接到所有 $C$ 个输入通道、空间大小为 $k \\times k$ 的可学习卷积核和一个标量偏置的定义，推导出架构 A 和架构 B 中可学习参数的总数，表示为关于 $C$ 的代数表达式。\n3. 论证为什么当中间的非线性函数设置为恒等函数时，架构 B 可以模拟架构 A，并说明当 $\\phi$ 非平凡时，在架构 A 和架构 B 中每个输出位置应用的逐点非线性函数的数量。\n\n以单行向量的形式报告你的最终答案，顺序为 $\\left(R,\\; P_{\\text{A}},\\; P_{\\text{B}},\\; N_{\\text{A}},\\; N_{\\text{B}}\\right)$，其中 $P_{\\text{A}}$ 和 $P_{\\text{B}}$ 分别是架构 A 和架构 B 的总参数数量，$N_{\\text{A}}$ 和 $N_{\\text{B}}$ 是每个输出位置应用的逐点非线性函数的数量。所有量都需精确表达，无需四舍五入。",
            "solution": "该问题要求基于感受野大小、参数数量和功能特性，对两种分别表示为架构 A 和架构 B 的卷积神经网络（CNN）架构进行比较分析。\n\n### 步驟 1：问题陈述的验证\n首先，对问题陈述进行严格的验证。\n\n**提取已知条件：**\n- 输入特征图：$x \\in \\mathbb{R}^{H \\times W \\times C}$，具有 $C$ 个通道。\n- 卷积属性：步长为 $1$，保留空间维度的零填充，无空洞卷积。\n- 层映射：$\\mathbb{R}^{H \\times W \\times C} \\to \\mathbb{R}^{H \\times W \\times C}$（通道数 $C$ 保持不变）。\n- 逐点非线性函数：$\\phi$，独立应用于每个空间位置和通道。\n- 架构 A：一个带有 $5 \\times 5$ 卷积核的卷积层，之后应用一次 $\\phi$。\n- 架构 B：两个连续的卷积层，每个都带有 $3 \\times 3$ 卷积核，每层之后都应用 $\\phi$。\n- 参数定义：每个输出通道由一个大小为 $k \\times k$、连接到所有 $C$ 个输入通道的可学习卷积核，外加一个标量偏置生成。\n\n**验证：**\n1.  **科学依据充分：** 该问题牢固地植根于深度学习和CNN的基本原理。感受野、参数计算以及架构比较（例如，单个大卷积核与堆叠的小卷积核）等概念是该领域的标准和核心主题。\n2.  **问题定义明确：** 所有必要条件（步长、填充、通道映射）都已明确定义，确保待推导的量（$R$, $P_{\\text{A}}$, $P_{\\text{B}}$, $N_{\\text{A}}$, $N_{\\text{B}}$）具有唯一且有意义的解。\n3.  **客观性：** 问题陈述使用了精确、正式且无偏见的语言，不含主观论断。\n\n问题陈述是自洽的、科学上合理的且定义明确的，没有明显缺陷。\n\n**结论：** 该问题有效。\n\n### 步驟 2：求解推导\n\n**任务 1：架构 B 的感受野**\n\n神经元的感受野定义了输入空间中影响其激活值的区域。对于一系列卷积层，感受野大小随层数增加而增长。第 $l$ 层之后的感受野边长 $R_l$ 可以递归计算。给定前一层的感受野 $R_{l-1}$、大小为 $k_l \\times k_l$ 的卷积核以及步长 $S_l$，新的感受野为：\n$$R_l = R_{l-1} + (k_l - 1) \\prod_{i=1}^{l-1} S_i$$\n设输入（第 $0$ 层）的感受野为 $R_0 = 1$。问题指定所有步长均为 $S=1$。该公式简化为一个简单的求和：\n$$R_l = R_{l-1} + (k_l - 1)$$\n架构 B 有两层，卷积核大小均为 $k_1 = k_2 = 3$。\n\n对于架构 B 的第一层：\n$$R_1 = R_0 + (k_1 - 1) = 1 + (3 - 1) = 3$$\n这意味着第一层输出中的一个神经元可以看到输入的 $3 \\times 3$ 区域。\n\n对于架构 B 的第二层，其输入是第一层的输出。第二层输出中一个神经元的感受野 $R$ 为：\n$$R = R_2 = R_1 + (k_2 - 1) = 3 + (3 - 1) = 5$$\n因此，架构 B 的感受野边长为 $5$。\n\n**任务 2：可学习参数的数量**\n\n卷积层中的参数数量是其权重和偏置的总和。对于一个具有 $C_{\\text{in}}$ 个输入通道、$C_{\\text{out}}$ 个输出通道和大小为 $k \\times k$ 的卷积核的层：\n- 权重数量 = $C_{\\text{out}} \\times (k \\times k \\times C_{\\text{in}})$。\n- 偏置数量 = $C_{\\text{out}}$。\n总参数 $P = (k^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}) + C_{\\text{out}}$。\n在本问题中，所有层都保持通道维度不变，因此 $C_{\\text{in}} = C_{\\text{out}} = C$。单层参数的公式变为：\n$$P_{\\text{layer}} = k^2 C^2 + C$$\n\n- **架构 A：** 它有一个带有 $5 \\times 5$ 卷积核（$k=5$）的层。总参数数量 $P_{\\text{A}}$ 为：\n$$P_{\\text{A}} = 5^2 C^2 + C = 25C^2 + C$$\n\n- **架构 B：** 它有两个连续的层，每个都带有 $3 \\times 3$ 卷积核（$k=3$）。\n  - 第一层的参数（$P_{\\text{B1}}$）：$P_{\\text{B1}} = 3^2 C^2 + C = 9C^2 + C$。\n  - 第二层的参数（$P_{\\text{B2}}$）：该层的输入是第一层的输出，同样有 $C$ 个通道。因此，$P_{\\text{B2}} = 3^2 C^2 + C = 9C^2 + C$。\n总参数数量 $P_{\\text{B}}$ 是两层之和：\n$$P_{\\text{B}} = P_{\\text{B1}} + P_{\\text{B2}} = (9C^2 + C) + (9C^2 + C) = 18C^2 + 2C$$\n\n**任务 3：模拟与非线性函数数量**\n\n- **模拟的论证：**\n卷积是一种线性操作（具体来说，是由互相关和偏置相加组成的线性映射）。当架构 B 的中间非线性函数 $\\phi$ 设置为恒等函数（$\\phi(z)=z$）时，它变成两个线性卷积层的复合。两个线性变换的复合本身也是一个线性变换。设这两层为 $L_1(x) = W_1 * x + b_1$ 和 $L_2(y) = W_2 * y + b_2$。复合函数为 $L_B(x) = L_2(L_1(x)) = W_2 * (W_1 * x + b_1) + b_2$。根据卷积的结合律，这等价于 $(W_2 * W_1) * x + \\text{bias terms}$，可以写成 $W_{\\text{eff}} * x + b_{\\text{eff}}$。一个 $3 \\times 3$ 的卷积核（$W_1$）与另一个 $3 \\times 3$ 的卷积核（$W_2$）进行卷积，会产生一个大小为 $5 \\times 5$ 的等效卷积核 $W_{\\text{eff}}$。这与架构 A 中单层的卷积核大小相匹配，并且如任务 1 所示，其感受野也相匹配。因此，没有中间非线性函数的架构 B 在功能上是一个具有 $5 \\times 5$ 感受野的线性卷积层，这与架构 A 的结构相同。\n\n- **逐点非线性函数的数量：**\n这个量（每个输出位置）被解释为信号依次通过的非线性变换的数量。一次非线性函数的“应用”指的是网络数据流中的一个阶段。\n  - **架构 A ($N_{\\text{A}}$)：** 该架构为 $\\text{conv}_{5 \\times 5} \\rightarrow \\phi$。在单个卷积层之后只应用了一层非线性函数。因此，$N_{\\text{A}} = 1$。\n  - **架构 B ($N_{\\text{B}}$)：** 该架构为 $\\text{conv}_{3 \\times 3} \\rightarrow \\phi \\rightarrow \\text{conv}_{3 \\times 3} \\rightarrow \\phi$。在两个卷积层之后都应用了非线性函数。一个信号从输入到输出会经过两个连续的非线性阶段。因此，$N_{\\text{B}} = 2$。\n\n**最终计算总结：**\n- $R = 5$\n- $P_{\\text{A}} = 25C^2 + C$\n- $P_{\\text{B}} = 18C^2 + 2C$\n- $N_{\\text{A}} = 1$\n- $N_{\\text{B}} = 2$\n最终答案是行向量 $(R, P_{\\text{A}}, P_{\\text{B}}, N_{\\text{A}}, N_{\\text{B}})$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n5 & 25C^{2} + C & 18C^{2} + 2C & 1 & 2\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在追求更高效率的神经网络模型时，深度可分离卷积（depthwise separable convolution）是一项革命性的技术，它也是 MobileNet 等轻量级网络的核心。这项技术通过将标准卷积分解为深度卷积（depthwise convolution）和逐点卷积（pointwise convolution）两个步骤，极大地降低了计算成本。本练习将引导您从第一性原理出发，精确计算并比较标准卷积与深度可分离卷积的乘加运算（Multiply-Accumulate, MAC）次数，从而量化其在计算效率上的显著优势。",
            "id": "3120106",
            "problem": "一个 MobileNet 版本 $1$ (MobileNetV$1$) 的阶段，在空间分辨率为 $H=W=112$ 时，使用核大小为 $K=3$、步长为 $1$ 的卷积，将一个具有 $C_{in}=32$ 个通道的输入张量转换为一个具有 $C_{out}=64$ 个通道的输出张量。假设使用零填充以保持空间维度不变。考虑该阶段的两种实现方式：(i) 一种标准卷积，使用 $K \\times K$ 的滤波器作用于所有输入通道；(ii) 一种深度可分离卷积，由一个深度 $K \\times K$ 卷积后跟一个逐点 $1 \\times 1$ 卷积组成。根据离散卷积和操作计数的第一性原理，计算两种实现在整个特征图上的乘加运算（MACs）的确切总数，其中一次乘加运算（MAC）定义为一次乘法与一次加法配对。然后，作为比较指标，计算加速因子，其定义为标准卷积的MAC计数与深度可分离卷积的MAC计数之比。在您的最终答案中，仅以单个简化的精确表达式报告加速因子。无需四舍五入。",
            "solution": "基本依据是离散卷积和操作计数的定义。对于卷积层的每个输出元素，其计算涉及对输入的局部感受野与相应滤波器权重之间的乘积进行求和。每次乘积及其累加计为一次乘加运算（MAC）。\n\n对于标准卷积：\n- 共有 $H \\times W$ 个空间位置和 $C_{out}$ 个输出通道，从而产生 $H W C_{out}$ 个输出元素。\n- 每个输出元素由一个作用于所有 $C_{in}$ 个输入通道的 $K \\times K$ 空间邻域计算得出，每个输出元素需要 $K^{2} C_{in}$ 次 MAC 运算。\n因此，标准卷积的总 MAC 计数为\n$$\n\\text{MAC}_{\\text{std}} = H W C_{out} K^{2} C_{in}。\n$$\n\n对于深度可分离卷积，计算分为两部分：\n\n1. 深度卷积：\n- 共有 $H \\times W$ 个空间位置和 $C_{in}$ 个独立的深度滤波器（每个输入通道一个），从而产生 $H W C_{in}$ 个输出元素。\n- 每个深度输出元素使用一个仅限于其自身通道的 $K \\times K$ 核，每个输出元素贡献 $K^{2}$ 次 MAC 运算。\n因此，深度卷积的 MAC 计数为\n$$\n\\text{MAC}_{\\text{dw}} = H W C_{in} K^{2}。\n$$\n\n2. 逐点 $1 \\times 1$ 卷积：\n- 共有 $H \\times W$ 个空间位置和 $C_{out}$ 个输出通道，从而产生 $H W C_{out}$ 个输出元素。\n- 每个逐点输出元素是使用 $1 \\times 1$ 核在 $C_{in}$ 个输入通道上的加权和，每个输出元素贡献 $C_{in}$ 次 MAC 运算。\n因此，逐点卷积的 MAC 计数为\n$$\n\\text{MAC}_{\\text{pw}} = H W C_{out} C_{in}。\n$$\n\n将两者相加，深度可分离卷积的总 MAC 计数为\n$$\n\\text{MAC}_{\\text{dws}} = \\text{MAC}_{\\text{dw}} + \\text{MAC}_{\\text{pw}} = H W C_{in} K^{2} + H W C_{out} C_{in} = H W C_{in} \\left(K^{2} + C_{out}\\right)。\n$$\n\n加速因子 $S$ 定义为标准卷积 MAC 计数与深度可分离卷积 MAC 计数之比：\n$$\nS = \\frac{\\text{MAC}_{\\text{std}}}{\\text{MAC}_{\\text{dws}}} = \\frac{H W C_{out} K^{2} C_{in}}{H W C_{in} \\left(K^{2} + C_{out}\\right)}。\n$$\n消去公因子 $H$、$W$ 和 $C_{in}$ 后得到\n$$\nS = \\frac{C_{out} K^{2}}{K^{2} + C_{out}}。\n$$\n\n将给定值 $C_{in}=32$，$C_{out}=64$，$K=3$，$H=W=112$ 代入 $S$ 的简化符号表达式中：\n$$\nS = \\frac{64 \\cdot 3^{2}}{3^{2} + 64} = \\frac{64 \\cdot 9}{9 + 64} = \\frac{576}{73}。\n$$\n\n为求完整性，我们可以用数值验证确切的 MAC 计数：\n- 标准卷积：\n$$\n\\text{MAC}_{\\text{std}} = 112 \\cdot 112 \\cdot 64 \\cdot 9 \\cdot 32 = 12544 \\cdot 64 \\cdot 288 = 231{,}211{,}008.\n$$\n- 深度可分离卷积：\n$$\n\\text{MAC}_{\\text{dws}} = 112 \\cdot 112 \\cdot 32 \\cdot \\left(9 + 64\\right) = 12544 \\cdot 32 \\cdot 73 = 29{,}302{,}784.\n$$\n它们的比值确实等于\n$$\n\\frac{231{,}211{,}008}{29{,}302{,}784} = \\frac{576}{73}。\n$$\n\n要求的最终输出是作为单个简化精确表达式的加速因子。",
            "answer": "$$\\boxed{\\frac{576}{73}}$$"
        },
        {
            "introduction": "优秀的神经网络不仅能提取强大的空间特征，还能理解不同特征通道之间的相互关系。“压缩-激励”（Squeeze-and-Excitation, SE）模块正是为此而生，它引入了一种通道注意力机制，使网络能够动态地调整每个特征通道的重要性。在这个实践中，您将从零开始实现一个完整的SE模块，包括其“压缩”（Squeeze）和“激励”（Excitation）操作，并分析其带来的参数开销，从而深入理解现代CNN架构如何自适应地增强其特征表示。",
            "id": "3139403",
            "problem": "要求您实现一个逐通道的挤压与激励 (Squeeze-and-Excitation, SE) 模块，并从基本原理出发量化其参数开销。SE 模块作用于一个具有通道、高度和宽度轴的三维输入张量。您必须实现的操作仅使用线性映射和逐点非线性的核心定义来定义，如下所述。\n\n给定一个输入张量 $x \\in \\mathbb{R}^{C \\times H \\times W}$，挤压操作通过逐通道全局平均值定义为\n$$\ns_c \\;=\\; \\frac{1}{H\\,W}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x_{cij}, \\quad \\text{for } c \\in \\{0,\\dots,C-1\\}.\n$$\n将 $s_c$ 堆叠起来得到描述符向量 $s \\in \\mathbb{R}^{C}$。将激励定义为一个具有缩减率 $r$ 的两层多层感知机 (MLP)，其中 $r$ 是一个能整除 $C$ 的正整数。令 $\\mathrm{ReLU}(u) = \\max\\{0,u\\}$ 表示修正线性单元，$\\sigma(u)=\\frac{1}{1+e^{-u}}$ 表示 logistic Sigmoid 函数。通过学习到的参数 $W_1 \\in \\mathbb{R}^{(C/r)\\times C}$、$b_1 \\in \\mathbb{R}^{C/r}$、$W_2 \\in \\mathbb{R}^{C \\times (C/r)}$ 和 $b_2 \\in \\mathbb{R}^{C}$，激励输出 $z \\in \\mathbb{R}^{C}$ 为\n$$\nt \\;=\\; \\mathrm{ReLU}(W_1 s + b_1),\n$$\n$$\nz \\;=\\; \\sigma(W_2 t + b_2).\n$$\n最后，通过 $z$ 逐通道缩放输入张量，\n$$\ny_{cij} \\;=\\; x_{cij}\\, z_c, \\quad \\text{for all } c,i,j,\n$$\n以获得 SE 增强张量 $y \\in \\mathbb{R}^{C \\times H \\times W}$。\n\n参数开销定义为激励 MLP 相对于没有 SE 模块的基线模型所引入的额外参数数量。对于两个非绑定权重的线性层，权重数量为\n$$\n\\#\\text{weights}_{\\text{untied}} \\;=\\; C \\cdot \\frac{C}{r} \\;+\\; \\frac{C}{r} \\cdot C \\;=\\; \\frac{2C^2}{r},\n$$\n偏置数量为\n$$\n\\#\\text{biases} \\;=\\; \\frac{C}{r} \\;+\\; C.\n$$\n在一个权重绑定变体中，其中 $W_2 = W_1^{\\top}$，独立权重数量减少为\n$$\n\\#\\text{weights}_{\\text{tied}} \\;=\\; C \\cdot \\frac{C}{r} \\;=\\; \\frac{C^2}{r}.\n$$\n\n请完全按照规定实现 SE 模块，并为每个测试用例计算以下输出：\n- 挤压描述符的总和，$\\sum_{c=0}^{C-1} s_c$。\n- 缩放后输出的所有元素之和，$\\sum_{c,i,j} y_{cij}$，四舍五入到六位小数。\n- 非绑定权重的开销 $\\#\\text{weights}_{\\text{untied}}$。\n- 绑定权重的开销 $\\#\\text{weights}_{\\text{tied}}$。\n- 非绑定权重加偏置的开销 $\\#\\text{weights}_{\\text{untied}} + \\#\\text{biases}$。\n\n您的程序必须使用以下测试套件。在所有情况下，$C$ 都能被 $r$ 整除，且以下所有定义都必须严格实现。\n\n测试用例 A (正常路径)：\n- $C = 4$, $H = 2$, $W = 2$, $r = 2$。\n- 输入由 $x_{cij} = c - i + j$ 定义，其中 $c \\in \\{0,1,2,3\\}$，$i \\in \\{0,1\\}$，$j \\in \\{0,1\\}$。\n- 参数：\n$$\nW_1 \\;=\\; \\begin{bmatrix}\n1  -1  0.5  0 \\\\\n0.25  0.5  -0.5  1\n\\end{bmatrix}, \\quad\nb_1 \\;=\\; \\begin{bmatrix} -0.5 \\\\ 0 \\end{bmatrix},\n$$\n$$\nW_2 \\;=\\; \\begin{bmatrix}\n1  0.5 \\\\\n-0.5  0.25 \\\\\n0  1 \\\\\n0.75  -1\n\\end{bmatrix}, \\quad\nb_2 \\;=\\; \\begin{bmatrix} 0 \\\\ 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}.\n$$\n\n测试用例 B (空间范围边界情况，$H W = 1$)：\n- $C = 8$, $H = 1$, $W = 1$, $r = 2$。\n- 输入由 $x_{c00} = c + 1$ 定义，其中 $c \\in \\{0,1,\\dots,7\\}$。\n- 参数由基于索引的公式逐元素定义。令 $a$ 索引行，$b$ 索引列，均为零基索引：\n$$\nW_1[a,b] \\;=\\; \\frac{a\\cdot 8 + b - 12}{16}, \\quad \\text{for } a \\in \\{0,1,2,3\\}, \\; b \\in \\{0,1,\\dots,7\\},\n$$\n$$\nb_1 \\;=\\; \\begin{bmatrix} -0.25 \\\\ 0 \\\\ 0.1 \\\\ -0.1 \\end{bmatrix}, \\quad\nW_2[a,b] \\;=\\; \\frac{a\\cdot 4 + b - 8}{8}, \\quad \\text{for } a \\in \\{0,1,\\dots,7\\}, \\; b \\in \\{0,1,2,3\\},\n$$\n$$\nb_2 \\;=\\; \\mathbf{0} \\in \\mathbb{R}^{8}.\n$$\n\n测试用例 C (边缘情况 $r = 1$，通道符号交替)：\n- $C = 6$, $H = 3$, $W = 1$, $r = 1$。\n- 输入由 $x_{ci0} = (-1)^c \\cdot (i+1)$ 定义，其中 $c \\in \\{0,1,\\dots,5\\}$，$i \\in \\{0,1,2\\}$。\n- 参数：\n$$\nW_1 \\;=\\; 0.5\\, I_6 \\;-\\; 0.25\\,(\\mathbf{1}_6 \\mathbf{1}_6^{\\top} - I_6), \\quad b_1 \\;=\\; \\mathbf{0} \\in \\mathbb{R}^{6},\n$$\n$$\nW_2 \\;=\\; 0.3\\, I_6 \\;-\\; 0.05\\, \\mathbf{1}_6 \\mathbf{1}_6^{\\top}, \\quad b_2 \\;=\\; \\begin{bmatrix} -0.1 \\\\ -0.05 \\\\ 0 \\\\ 0.05 \\\\ 0.1 \\\\ 0.15 \\end{bmatrix},\n$$\n其中 $I_6$ 是 $6 \\times 6$ 的单位矩阵，$\\mathbf{1}_6$ 是长度为 6 的全一向量。\n\n角度单位和物理单位在此不适用。所有数值输出都应为标准十进制形式的实数。\n\n最终输出格式：\n- 您的程序应生成单行输出，包含所有三个测试用例的结果，形式为列表的列表。每个内部列表必须按以下顺序排列：\n$$\n\\left[ \\sum_{c} s_c,\\; \\mathrm{round}\\!\\left(\\sum_{c,i,j} y_{cij},\\,6\\right),\\; \\#\\text{weights}_{\\text{untied}},\\; \\#\\text{weights}_{\\text{tied}},\\; \\#\\text{weights}_{\\text{untied}} + \\#\\text{biases} \\right].\n$$\n例如，打印的结构必须如下所示：\n$$\n\\big[\\,[\\dots],\\,[\\dots],\\,[\\dots]\\,\\big].\n$$",
            "solution": "该问题要求根据其基本数学定义，实现和分析深度学习中一个常见的架构组件——挤压与激励 (Squeeze-and-Excitation, SE) 模块。解决方案涉及对三个不同的测试用例，逐步执行所定义的操作。其核心原理是通过数据驱动机制进行逐通道的特征重校准。\n\nSE 模块作用于一个输入张量 $x \\in \\mathbb{R}^{C \\times H \\times W}$，其中 $C$ 是通道数，$H$ 和 $W$ 分别是空间高度和宽度。该过程包括三个主要阶段：挤压 (Squeeze)、激励 (Excitation) 和缩放 (Scale)。\n\n**1. 挤压：全局信息嵌入**\n第一步，“挤压”，通过聚合特征图的空间维度 ($H \\times W$) 来生成一个通道描述符。这是通过全局平均池化实现的，该操作计算每个通道的平均值。生成的向量 $s \\in \\mathbb{R}^{C}$ 为每个通道嵌入了全局感受野。$s$ 的第 $c$ 个元素的公式为：\n$$\ns_c = \\frac{1}{H \\cdot W}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x_{cij}\n$$\n在实现中，这对应于沿输入张量 $x$ 的空间轴（轴1和轴2）取均值。每个测试用例的第一个要求输出是这个挤压后向量的元素之和，即 $\\sum_{c=0}^{C-1} s_c$。\n\n**2. 激励：自适应重校准**\n“激励”阶段从挤压描述符 $s$ 中生成一组逐通道的调制权重，或称“激活值”。这是通过一个小型神经网络，具体来说是一个两层的多层感知机 (MLP) 来完成的。MLP 的结构首先降低通道维度，然后恢复它，从而创建一个计算瓶颈，有助于捕捉非线性的通道相互依赖关系。\n\nMLP 包括：\n- 一个降维线性层，权重为 $W_1 \\in \\mathbb{R}^{(C/r)\\times C}$，偏置为 $b_1 \\in \\mathbb{R}^{C/r}$，其中 $r$ 是缩减率。\n- 一个修正线性单元 ($\\mathrm{ReLU}$) 激活函数，定义为 $\\mathrm{ReLU}(u) = \\max\\{0,u\\}$。\n- 一个增维线性层，权重为 $W_2 \\in \\mathbb{R}^{C \\times (C/r)}$，偏置为 $b_2 \\in \\mathbb{R}^{C}$。\n- 一个 logistic Sigmoid 激活函数，$\\sigma(u)=\\frac{1}{1+e^{-u}}$，它将输出归一化到 $(0, 1)$ 范围内。\n\n计算过程如下：\n$$\nt = \\mathrm{ReLU}(W_1 s + b_1)\n$$\n$$\nz = \\sigma(W_2 t + b_2)\n$$\n得到的向量 $z \\in \\mathbb{R}^{C}$ 包含了逐通道的缩放因子。\n\n**3. 缩放：特征重校准**\n最后阶段将从激励步骤中学到的缩放因子应用于原始输入张量 $x$。输入张量的每个通道都乘以其对应的来自向量 $z$ 的缩放因子。这重校准了特征图，放大了信息丰富的通道，并抑制了较不有用的通道。输出张量 $y \\in \\mathbb{R}^{C \\times H \\times W}$ 由下式给出：\n$$\ny_{cij} = x_{cij} \\cdot z_c\n$$\n在实现中，这是通过将缩放向量 $z$（重塑为 $C \\times 1 \\times 1$ 维度）广播到输入张量 $x$ 上来实现的。第二个要求输出是最终缩放张量中所有元素的总和，$\\sum_{c,i,j} y_{cij}$，其计算方式为 $\\sum_c (z_c \\cdot \\sum_{i,j} x_{cij}) = H \\cdot W \\cdot \\sum_c (z_c \\cdot s_c)$。\n\n**4. 参数开销计算**\n问题还要求量化 SE 模块的 MLP 引入的额外可学习参数的数量。这些参数包括两个线性层的权重和偏置。\n- **非绑定权重：** 当 $W_1$ 和 $W_2$ 相互独立时，总权重数是两个矩阵中元素的总和：$(\\frac{C}{r} \\times C) + (C \\times \\frac{C}{r}) = \\frac{2C^2}{r}$。\n- **绑定权重：** 在一个 $W_2 = W_1^{\\top}$ 的变体中，独立权重数减少到 $W_1$ 的大小，即 $\\frac{C^2}{r}$。\n- **偏置：** 偏置参数的数量是 $b_1$ 和 $b_2$ 中元素的总和，即 $\\frac{C}{r} + C$。\n- **总非绑定开销：** 非绑定权重情况下的总参数数量是未绑定权重和偏置的总和：$\\frac{2C^2}{r} + \\frac{C}{r} + C$。\n\n实现将为所提供的三个测试用例，使用指定的输入张量和 MLP 参数，计算这五个量。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and format results for all test cases.\n    \"\"\"\n\n    def se_block_and_overhead(C, H, W, r, x, W1, b1, W2, b2):\n        \"\"\"\n        Implements the SE block operations and calculates parameter overhead.\n\n        Args:\n            C (int): Number of channels.\n            H (int): Height of the input tensor.\n            W (int): Width of the input tensor.\n            r (int): Reduction ratio.\n            x (np.ndarray): Input tensor of shape (C, H, W).\n            W1 (np.ndarray): Weights of the first linear layer.\n            b1 (np.ndarray): Biases of the first linear layer.\n            W2 (np.ndarray): Weights of the second linear layer.\n            b2 (np.ndarray): Biases of the second linear layer.\n\n        Returns:\n            list: A list containing the five required output values.\n        \"\"\"\n        # 1. Squeeze Operation: Global Average Pooling\n        # s_c = (1/(H*W)) * sum(x_cij for i,j)\n        s = np.mean(x, axis=(1, 2))\n        sum_s = np.sum(s)\n\n        # 2. Excitation Operation: MLP\n        # t = ReLU(W1*s + b1)\n        t = np.maximum(0, W1 @ s + b1)\n        \n        # z = sigmoid(W2*t + b2)\n        z_raw = W2 @ t + b2\n        z = 1 / (1 + np.exp(-z_raw))\n\n        # 3. Scale Operation\n        # y_cij = x_cij * z_c\n        # Reshape z to (C, 1, 1) for broadcasting\n        y = x * z.reshape((C, 1, 1))\n        sum_y = np.sum(y)\n\n        # 4. Parameter Overhead Calculation\n        weights_untied = (2 * C**2) // r\n        weights_tied = (C**2) // r\n        biases = (C // r) + C\n        total_untied_with_biases = weights_untied + biases\n        \n        return [\n            sum_s,\n            round(sum_y, 6),\n            weights_untied,\n            weights_tied,\n            total_untied_with_biases\n        ]\n\n    results = []\n\n    # --- Test Case A ---\n    C_A, H_A, W_A, r_A = 4, 2, 2, 2\n    x_A = np.fromfunction(lambda c, i, j: c - i + j, (C_A, H_A, W_A))\n    W1_A = np.array([\n        [1, -1, 0.5, 0],\n        [0.25, 0.5, -0.5, 1]\n    ])\n    b1_A = np.array([-0.5, 0])\n    W2_A = np.array([\n        [1, 0.5],\n        [-0.5, 0.25],\n        [0, 1],\n        [0.75, -1]\n    ])\n    b2_A = np.array([0, 0.1, -0.2, 0.3])\n    results.append(se_block_and_overhead(C_A, H_A, W_A, r_A, x_A, W1_A, b1_A, W2_A, b2_A))\n\n    # --- Test Case B ---\n    C_B, H_B, W_B, r_B = 8, 1, 1, 2\n    x_B = (np.arange(C_B) + 1).reshape(C_B, H_B, W_B)\n    W1_B = np.fromfunction(lambda a, b: (a * 8 + b - 12) / 16, (C_B // r_B, C_B))\n    b1_B = np.array([-0.25, 0, 0.1, -0.1])\n    W2_B = np.fromfunction(lambda a, b: (a * 4 + b - 8) / 8, (C_B, C_B // r_B))\n    b2_B = np.zeros(C_B)\n    results.append(se_block_and_overhead(C_B, H_B, W_B, r_B, x_B, W1_B, b1_B, W2_B, b2_B))\n\n    # --- Test Case C ---\n    C_C, H_C, W_C, r_C = 6, 3, 1, 1\n    c_vec_C = (-1)**np.arange(C_C)\n    i_vec_C = np.arange(1, H_C + 1)\n    x_C = (c_vec_C[:, np.newaxis, np.newaxis]\n           * i_vec_C[np.newaxis, :, np.newaxis])\n    \n    I6 = np.identity(C_C)\n    J6 = np.ones((C_C, C_C))\n    \n    W1_C = 0.5 * I6 - 0.25 * (J6 - I6)\n    b1_C = np.zeros(C_C)\n    W2_C = 0.3 * I6 - 0.05 * J6\n    b2_C = np.array([-0.1, -0.05, 0, 0.05, 0.1, 0.15])\n    results.append(se_block_and_overhead(C_C, H_C, W_C, r_C, x_C, W1_C, b1_C, W2_C, b2_C))\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to a string and join them with commas.\n    # The outer brackets are added by the f-string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}