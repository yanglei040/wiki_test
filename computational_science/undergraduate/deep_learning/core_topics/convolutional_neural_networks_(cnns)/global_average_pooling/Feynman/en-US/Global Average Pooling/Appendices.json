{
    "hands_on_practices": [
        {
            "introduction": "A primary motivation for developing Global Average Pooling (GAP) was to create simpler, more parameter-efficient Convolutional Neural Networks (CNNs). Before GAP, it was common to flatten the final feature maps and connect them to one or more large Fully Connected (FC) layers, which often contained the majority of the network's parameters. This exercise  walks you through a direct comparison, allowing you to derive and quantify the massive parameter savings achieved by replacing a traditional FC head with a modern GAP-based design. Understanding this calculation is fundamental to appreciating the architectural elegance of networks that employ GAP.",
            "id": "3129826",
            "problem": "A convolutional neural network produces a final feature tensor of shape $C \\times H \\times W$ before classification. You consider two alternatives for the final classification stage to produce $K$ outputs: \n(i) a Fully Connected (FC) layer directly from the flattened $C \\cdot H \\cdot W$ inputs to $K$ outputs, and \n(ii) Global Average Pooling (GAP) over the spatial dimensions to obtain a $C$-dimensional vector, followed by a linear $C \\to K$ layer.\n\nStarting only from the definition that a fully connected linear layer with input dimension $n$ and output dimension $m$ contains $n \\cdot m$ weights and $m$ biases, and that Global Average Pooling replaces each channel’s $H \\times W$ map by its mean value, derive expressions for the total parameter counts in the two alternatives. Then, for the specific configuration $C=256$, $H=14$, $W=14$, and $K=1000$, compute the multiplicative reduction factor in parameter memory bandwidth when switching from the FC to the GAP-plus-linear alternative, assuming $4$ bytes per parameter and that all parameters must be read once per forward pass.\n\nReport as your final answer the dimensionless reduction factor (FC bytes divided by GAP-plus-linear bytes), rounded to four significant figures. Briefly justify the direction of the memory bandwidth savings in your derivation.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Input feature tensor shape: $C \\times H \\times W$\n- Number of final outputs (classes): $K$\n- Alternative (i): A Fully Connected (FC) layer mapping flattened $C \\cdot H \\cdot W$ inputs to $K$ outputs.\n- Alternative (ii): Global Average Pooling (GAP) over spatial dimensions, followed by a linear layer from $C$ inputs to $K$ outputs.\n- Definition of FC layer parameters: For an input of dimension $n$ and output of dimension $m$, it has $n \\cdot m$ weights and $m$ biases.\n- Definition of GAP: Replaces each channel’s $H \\times W$ feature map with its mean value.\n- Specific configuration: $C=256$, $H=14$, $W=14$, $K=1000$.\n- Memory assumption: $4$ bytes per parameter. All parameters are read once per forward pass.\n- Task: Derive expressions for the total parameter counts, and compute the multiplicative reduction factor in parameter memory bandwidth when switching from alternative (i) to (ii).\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is valid.\n- **Scientifically Grounded:** The problem describes a standard and fundamental architectural trade-off in convolutional neural networks (CNNs), contrasting a traditional fully connected head with a more modern Global Average Pooling head. The definitions provided for the number of parameters in a linear layer and the function of GAP are correct and central to the field of deep learning.\n- **Well-Posed:** The problem is self-contained, providing all necessary variables ($C, H, W, K$), definitions, and numerical values to derive a unique analytical expression and compute a final numerical answer. The objective is clearly stated.\n- **Objective:** The language is formal and precise, with no subjective or ambiguous terminology.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be provided.\n\n**Derivation of Parameter Counts**\n\nLet $P$ denote the total number of parameters in a given layer or set of layers. According to the provided definition, a linear layer with an input of dimension $n$ and an output of dimension $m$ has a total parameter count of $n \\cdot m + m$.\n\n**Alternative (i): Fully Connected (FC) Layer**\nIn this alternative, the input feature tensor of shape $C \\times H \\times W$ is first flattened into a vector. The dimension of this input vector, $n$, is:\n$$n = C \\cdot H \\cdot W$$\nThis vector is then fed into a single fully connected layer that produces $K$ outputs. The output dimension, $m$, is:\n$$m = K$$\nThe total number of parameters for the FC layer, $P_{FC}$, is the sum of weights and biases:\n$$P_{FC} = n \\cdot m + m = (C \\cdot H \\cdot W) \\cdot K + K$$\nFactoring out $K$, we get:\n$$P_{FC} = K(C \\cdot H \\cdot W + 1)$$\n\n**Alternative (ii): Global Average Pooling (GAP) plus Linear Layer**\nThis alternative involves two stages.\n1.  **Global Average Pooling (GAP):** The GAP operation takes the input tensor of shape $C \\times H \\times W$ and computes the spatial average for each of the $C$ feature maps. This produces a vector of length $C$. The GAP operation itself is a fixed function (averaging) and has no trainable parameters.\n    $$P_{pool} = 0$$\n2.  **Linear Layer:** The resulting $C$-dimensional vector is the input to a final linear layer. For this layer, the input dimension $n$ is:\n    $$n = C$$\n    The output dimension $m$ remains the same:\n    $$m = K$$\n    The total number of parameters for this linear layer, $P_{lin}$, is:\n    $$P_{lin} = n \\cdot m + m = C \\cdot K + K$$\nThe total number of parameters for the entire GAP-plus-linear alternative, $P_{GAP}$, is the sum of parameters from both stages:\n$$P_{GAP} = P_{pool} + P_{lin} = 0 + (C \\cdot K + K) = K(C + 1)$$\n\n**Derivation of the Reduction Factor**\nThe problem defines parameter memory bandwidth as being proportional to the total number of parameters, with a constant factor of $4$ bytes per parameter. Let $B_{FC}$ be the memory bandwidth for the FC alternative and $B_{GAP}$ be the bandwidth for the GAP alternative.\n$$B_{FC} = 4 \\cdot P_{FC} = 4K(C \\cdot H \\cdot W + 1)$$\n$$B_{GAP} = 4 \\cdot P_{GAP} = 4K(C + 1)$$\nThe multiplicative reduction factor, $R$, is the ratio of the two bandwidths:\n$$R = \\frac{B_{FC}}{B_{GAP}} = \\frac{4K(C \\cdot H \\cdot W + 1)}{4K(C + 1)}$$\nThe constant factors $4$ and the variable $K$ cancel out, yielding the general expression for the reduction factor:\n$$R = \\frac{C \\cdot H \\cdot W + 1}{C + 1}$$\n\nThis expression provides the justification for the memory savings. The dominant term in the parameter count for the FC layer is $C \\cdot H \\cdot W \\cdot K$, which scales with the product of the spatial dimensions $H \\cdot W$. In contrast, the dominant term for the GAP-based alternative is $C \\cdot K$. By performing pooling *before* the linear transformation, the dependence on the spatial dimensions $H \\cdot W$ is eliminated from the parameter count, leading to a substantial reduction whenever $H > 1$ or $W > 1$.\n\n**Numerical Calculation**\nWe now substitute the given numerical values into the derived expression for $R$:\n$C=256$, $H=14$, $W=14$.\n$$R = \\frac{256 \\cdot 14 \\cdot 14 + 1}{256 + 1}$$\nFirst, compute the product in the numerator:\n$$256 \\cdot 14 \\cdot 14 = 256 \\cdot 196 = 50176$$\nNow substitute this back into the expression for $R$:\n$$R = \\frac{50176 + 1}{257} = \\frac{50177}{257}$$\nPerforming the division:\n$$R \\approx 195.241245...$$\nRounding the result to four significant figures, as required by the problem statement, gives:\n$$R \\approx 195.2$$",
            "answer": "$$\n\\boxed{195.2}\n$$"
        },
        {
            "introduction": "While GAP is highly effective at reducing parameters, its design choice introduces a specific assumption about the nature of the data: that discriminative features are distributed globally across the feature map. This practice  explores a hypothetical scenario where this assumption does not hold. By analyzing a toy model where a single, localized feature determines the class, you will quantitatively compare the performance of Global Average Pooling against Global Max Pooling (GMP). This exercise illuminates the concept of representational bias and develops the critical skill of selecting an architecture that aligns with the expected structure of the underlying data.",
            "id": "3129750",
            "problem": "Consider a single-channel convolutional feature map produced by a Convolutional Neural Network (CNN) with spatial size $H \\times W$. Let $S \\equiv H \\cdot W$. For an image, denote the activation at spatial location $i \\in \\{1,\\dots,S\\}$ by the random variable $X_i$. Suppose class-conditional distributions follow this toy model: under the negative class ($Y=0$), all $X_i$ are independent and identically distributed as a Gaussian with mean $\\mu_0$ and variance $\\sigma^2$, and under the positive class ($Y=1$), exactly one unknown location $j$ contains a discriminative part with $X_j \\sim \\mathcal{N}(\\mu_1,\\sigma^2)$ while all other locations are background with $X_i \\sim \\mathcal{N}(\\mu_0,\\sigma^2)$ for $i \\neq j$. We assume independence across locations and equal class priors. Consider two pooling operators followed by a single-threshold classifier: Global Average Pooling (GAP), which outputs $Z_{\\text{avg}} \\equiv \\frac{1}{S}\\sum_{i=1}^S X_i$, and Global Max Pooling (GMP), which outputs $Z_{\\text{max}} \\equiv \\max_{1 \\le i \\le S} X_i$. The classifier predicts $\\hat{Y}=1$ if $Z \\ge t$ and $\\hat{Y}=0$ otherwise. Define Balanced Accuracy (BA) as $\\text{BA} \\equiv \\frac{1}{2}\\left(\\Pr(\\hat{Y}=0 \\mid Y=0) + \\Pr(\\hat{Y}=1 \\mid Y=1)\\right)$.\n\nThis toy captures a representation bias contrast: GAP encourages global, spatially distributed descriptors by averaging evidence across locations, whereas GMP emphasizes part-based reasoning by attending to the strongest local evidence. Use only basic definitions of Gaussian random variables, independence, and the definition of pooling to reason about performance.\n\nFix parameters $H=W=10$ so $S=100$, $\\mu_0=0$, $\\mu_1=5$, $\\sigma=1$. For GMP, use threshold $t=4$. For GAP, assume the threshold $t$ is chosen to be Bayes optimal under the equal-variance Gaussian assumption for the two class-conditional pooled outputs. Using scientifically reasonable approximations to any Gaussian tail probabilities that arise, answer the following. Select all statements that are correct.\n\nA. Under GAP, the class-conditional distributions of $Z_{\\text{avg}}$ are Gaussian with equal variance; the Bayes-optimal threshold yields approximately $\\text{BA} \\approx 0.60$. Under GMP with $t=4$, one obtains approximately $\\text{BA} \\approx 0.92$. Therefore, on this part-based toy problem, max pooling is superior to average pooling and better aligned with the data-generating mechanism.\n\nB. Because GAP is translation-invariant, as $S$ increases with fixed $\\mu_1>\\mu_0$ and fixed $\\sigma$, its accuracy strictly increases and approaches $1$ as $S \\to \\infty$.\n\nC. With the given parameters, GMP at threshold $t=4$ has false positive rate about $3\\%$ but true positive rate about $16\\%$, giving $\\text{BA} \\approx 0.57$, so GMP is inferior to GAP on this toy problem.\n\nD. With suitable thresholds, GAP and GMP necessarily achieve the same Bayes error on this toy distribution because both are permutation-invariant functions of $\\{X_i\\}_{i=1}^S$.",
            "solution": "The problem statement is critically validated before proceeding to a solution.\n\n### Step 1: Extract Givens\n- Feature map spatial size: $H \\times W$.\n- Total number of spatial locations: $S \\equiv H \\cdot W$.\n- Activation at location $i$: Random variable $X_i$, for $i \\in \\{1,\\dots,S\\}$.\n- Class label: $Y \\in \\{0, 1\\}$.\n- Class $Y=0$ (negative class): $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ are independent and identically distributed (i.i.d.) for all $i \\in \\{1, \\dots, S\\}$.\n- Class $Y=1$ (positive class): For exactly one unknown location $j$, $X_j \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$. For all other locations $i \\neq j$, $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$. All $X_i$ are independent.\n- Class priors: $\\Pr(Y=0) = \\Pr(Y=1) = 1/2$.\n- Pooling operators:\n  - Global Average Pooling (GAP): $Z_{\\text{avg}} \\equiv \\frac{1}{S}\\sum_{i=1}^S X_i$.\n  - Global Max Pooling (GMP): $Z_{\\text{max}} \\equiv \\max_{1 \\le i \\le S} X_i$.\n- Classifier: Predicts $\\hat{Y}=1$ if the pooled output $Z \\ge t$ and $\\hat{Y}=0$ otherwise.\n- Performance metric: Balanced Accuracy (BA) $\\equiv \\frac{1}{2}\\left(\\Pr(\\hat{Y}=0 \\mid Y=0) + \\Pr(\\hat{Y}=1 \\mid Y=1)\\right)$.\n- Parameters: $H=10$, $W=10$, so $S=100$. $\\mu_0=0$, $\\mu_1=5$, $\\sigma=1$.\n- Thresholds: For GMP, $t=4$. For GAP, $t$ is Bayes optimal for the class-conditional distributions of $Z_{\\text{avg}}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is a well-defined statistical toy model. It uses standard, non-controversial concepts from probability theory (Gaussian distributions, independence) to model a plausible scenario in representation learning (local vs. distributed features). It is scientifically sound.\n2.  **Well-Posed:** All necessary parameters ($\\mu_0, \\mu_1, \\sigma, S$) and distributions are defined. The objectives (calculate BA for GAP and GMP) are clear. A unique solution can be derived.\n3.  **Objective:** The problem is stated in precise, quantitative, and objective language.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. A solution will be derived.\n\n### Derivation of Solution\n\nFirst, we analyze the Global Average Pooling (GAP) operator.\nThe class-conditional distributions for $Z_{\\text{avg}}$ are derived as follows:\n\nIf $Y=0$, all $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ are i.i.d. Since the sum of independent Gaussian random variables is also Gaussian, $Z_{\\text{avg}} = \\frac{1}{S}\\sum_i X_i$ is Gaussian.\nThe mean is $E[Z_{\\text{avg}} \\mid Y=0] = \\frac{1}{S} \\sum_i E[X_i] = \\frac{1}{S} S \\mu_0 = \\mu_0$.\nThe variance is $\\text{Var}(Z_{\\text{avg}} \\mid Y=0) = \\frac{1}{S^2} \\sum_i \\text{Var}(X_i) = \\frac{1}{S^2} S \\sigma^2 = \\frac{\\sigma^2}{S}$.\nSo, $Z_{\\text{avg}} \\mid (Y=0) \\sim \\mathcal{N}(\\mu_0, \\sigma^2/S)$.\n\nIf $Y=1$, one $X_j \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ and $S-1$ variables are $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$. $Z_{\\text{avg}}$ is again a sum of independent Gaussians and thus is Gaussian.\nThe mean is $E[Z_{\\text{avg}} \\mid Y=1] = \\frac{1}{S} \\left( E[X_j] + \\sum_{i \\neq j} E[X_i] \\right) = \\frac{1}{S}(\\mu_1 + (S-1)\\mu_0)$.\nThe variance is $\\text{Var}(Z_{\\text{avg}} \\mid Y=1) = \\frac{1}{S^2} \\left( \\text{Var}(X_j) + \\sum_{i \\neq j} \\text{Var}(X_i) \\right) = \\frac{1}{S^2}(\\sigma^2 + (S-1)\\sigma^2) = \\frac{\\sigma^2}{S}$.\nSo, $Z_{\\text{avg}} \\mid (Y=1) \\sim \\mathcal{N}(\\frac{\\mu_1 + (S-1)\\mu_0}{S}, \\sigma^2/S)$.\n\nThe class-conditional distributions for $Z_{\\text{avg}}$ are both Gaussian with equal variance $\\sigma^2/S$.\nGiven equal class priors and equal variances, the Bayes-optimal threshold is the midpoint of the two means:\n$t_{\\text{GAP}} = \\frac{1}{2} \\left( \\mu_0 + \\frac{\\mu_1 + (S-1)\\mu_0}{S} \\right)$.\nWith parameters $S=100$, $\\mu_0=0$, $\\mu_1=5$, $\\sigma=1$:\n- $Z_{\\text{avg}} \\mid (Y=0) \\sim \\mathcal{N}(0, 1/100) = \\mathcal{N}(0, (0.1)^2)$.\n- $Z_{\\text{avg}} \\mid (Y=1) \\sim \\mathcal{N}(\\frac{5 + 99 \\cdot 0}{100}, 1/100) = \\mathcal{N}(0.05, (0.1)^2)$.\n- $t_{\\text{GAP}} = \\frac{1}{2}(0 + 0.05) = 0.025$.\n\nThe True Positive Rate (TPR) is $\\Pr(\\hat{Y}=1 \\mid Y=1) = \\Pr(Z_{\\text{avg}} \\ge t_{\\text{GAP}} \\mid Y=1)$. Let $\\Phi$ be the standard normal CDF.\n$\\text{TPR} = \\Pr\\left(\\frac{Z_{\\text{avg}} - 0.05}{0.1} \\ge \\frac{0.025-0.05}{0.1}\\right) = \\Pr(\\mathcal{N}(0,1) \\ge -0.25) = 1 - \\Phi(-0.25) = \\Phi(0.25)$.\nThe True Negative Rate (TNR) is $\\Pr(\\hat{Y}=0 \\mid Y=0) = \\Pr(Z_{\\text{avg}} < t_{\\text{GAP}} \\mid Y=0)$.\n$\\text{TNR} = \\Pr\\left(\\frac{Z_{\\text{avg}} - 0}{0.1} < \\frac{0.025-0}{0.1}\\right) = \\Pr(\\mathcal{N}(0,1) < 0.25) = \\Phi(0.25)$.\nUsing $\\Phi(0.25) \\approx 0.5987$:\n$\\text{BA}_{\\text{GAP}} = \\frac{1}{2}(\\text{TNR} + \\text{TPR}) = \\frac{1}{2}(\\Phi(0.25) + \\Phi(0.25)) = \\Phi(0.25) \\approx 0.5987 \\approx 0.60$.\n\nNext, we analyze the Global Max Pooling (GMP) operator with threshold $t=4$.\nThe CDF of $Z_{\\max}$ is needed. Let $\\Phi_{\\mu, \\sigma}(x)$ be the CDF of $\\mathcal{N}(\\mu, \\sigma^2)$.\n\nIf $Y=0$, all $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ are i.i.d.\n$\\Pr(Z_{\\max} < t \\mid Y=0) = \\Pr(\\text{all } X_i < t) = \\prod_{i=1}^S \\Pr(X_i < t) = [\\Phi_{\\mu_0, \\sigma}(t)]^S$.\nIf $Y=1$, one $X_j \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ and others $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$.\n$\\Pr(Z_{\\max} < t \\mid Y=1) = \\Pr(X_j < t) \\prod_{i \\neq j} \\Pr(X_i < t) = \\Phi_{\\mu_1, \\sigma}(t) [\\Phi_{\\mu_0, \\sigma}(t)]^{S-1}$.\n\nWith parameters $S=100, \\mu_0=0, \\mu_1=5, \\sigma=1, t=4$:\n$\\Phi_{\\mu_0, \\sigma}(4) = \\Phi(\\frac{4-0}{1}) = \\Phi(4)$.\n$\\Phi_{\\mu_1, \\sigma}(4) = \\Phi(\\frac{4-5}{1}) = \\Phi(-1)$.\nWe use the approximations $\\Phi(4) \\approx 1 - 3.167 \\times 10^{-5}$ and $\\Phi(-1) = 1 - \\Phi(1) \\approx 1 - 0.8413 = 0.1587$.\n\n$\\text{TNR} = \\Pr(\\hat{Y}=0 \\mid Y=0) = \\Pr(Z_{\\max} < 4 \\mid Y=0) = [\\Phi(4)]^{100}$.\nUsing the approximation $(1-\\epsilon)^n \\approx 1 - n\\epsilon$ for small $\\epsilon$:\n$\\text{TNR} \\approx (1 - 3.167 \\times 10^{-5})^{100} \\approx 1 - 100 \\cdot (3.167 \\times 10^{-5}) = 1 - 0.003167 = 0.996833$.\n\n$\\text{TPR} = \\Pr(\\hat{Y}=1 \\mid Y=1) = \\Pr(Z_{\\max} \\ge 4 \\mid Y=1) = 1 - \\Pr(Z_{\\max} < 4 \\mid Y=1)$.\n$\\Pr(Z_{\\max} < 4 \\mid Y=1) = \\Phi(-1) [\\Phi(4)]^{99} \\approx 0.1587 \\cdot (1 - 99 \\cdot (3.167 \\times 10^{-5}))$.\n$\\approx 0.1587 \\cdot (1 - 0.003135) \\approx 0.1587 \\cdot 0.996865 \\approx 0.15819$.\n$\\text{TPR} \\approx 1 - 0.15819 = 0.84181$.\n\n$\\text{BA}_{\\text{GMP}} = \\frac{1}{2}(\\text{TNR} + \\text{TPR}) \\approx \\frac{1}{2}(0.996833 + 0.84181) = \\frac{1.838643}{2} \\approx 0.9193 \\approx 0.92$.\n\n### Evaluation of Options\n\n**A. Under GAP, the class-conditional distributions of $Z_{\\text{avg}}$ are Gaussian with equal variance; the Bayes-optimal threshold yields approximately $\\text{BA} \\approx 0.60$. Under GMP with $t=4$, one obtains approximately $\\text{BA} \\approx 0.92$. Therefore, on this part-based toy problem, max pooling is superior to average pooling and better aligned with the data-generating mechanism.**\n- The claim that $Z_{\\text{avg}}$ distributions are Gaussian with equal variance is correct, as derived above.\n- The calculation $\\text{BA}_{\\text{GAP}} \\approx 0.60$ is correct.\n- The calculation $\\text{BA}_{\\text{GMP}} \\approx 0.92$ is correct.\n- The conclusion that GMP is superior to GAP for this problem follows directly from the fact that $0.92 \\gg 0.60$. This superiority aligns with the intuition that GMP is suited for detecting sparse, localized signals, which describes the data-generating process for $Y=1$.\n- Verdict: **Correct**.\n\n**B. Because GAP is translation-invariant, as $S$ increases with fixed $\\mu_1>\\mu_0$ and fixed $\\sigma$, its accuracy strictly increases and approaches $1$ as $S \\to \\infty$.**\n- The reasoning is a non-sequitur. The scaling of accuracy derives from the statistical properties of $Z_{\\text{avg}}$, not merely from translation invariance.\n- Let's analyze the behavior of $\\text{BA}_{\\text{GAP}}$ as $S \\to \\infty$. The separability of the two conditional distributions of $Z_{\\text{avg}}$ depends on the distance between their means, normalized by their standard deviation. This is often called $d'$.\n$d' = \\frac{|E[Z_{\\text{avg}}|Y=1] - E[Z_{\\text{avg}}|Y=0]|}{\\sqrt{\\text{Var}(Z_{\\text{avg}})}} = \\frac{|\\frac{\\mu_1+(S-1)\\mu_0}{S} - \\mu_0|}{\\sigma/\\sqrt{S}} = \\frac{|\\frac{\\mu_1-\\mu_0}{S}|}{\\sigma/\\sqrt{S}} = \\frac{\\mu_1-\\mu_0}{\\sigma\\sqrt{S}}$.\nAs $S \\to \\infty$, $d' \\to 0$. The two distributions become indistinguishable.\nThe BA for the optimal threshold is $\\text{BA}_{\\text{GAP}} = \\Phi(\\frac{d'}{2}) = \\Phi\\left(\\frac{\\mu_1-\\mu_0}{2\\sigma\\sqrt{S}}\\right)$.\nAs $S \\to \\infty$, the argument of $\\Phi$ goes to $0$, and thus $\\text{BA}_{\\text{GAP}} \\to \\Phi(0) = 0.5$.\nThis is chance performance. The statement claims accuracy approaches $1$.\n- Verdict: **Incorrect**.\n\n**C. With the given parameters, GMP at threshold $t=4$ has false positive rate about $3\\%$ but true positive rate about $16\\%$, giving $\\text{BA} \\approx 0.57$, so GMP is inferior to GAP on this toy problem.**\n- False Positive Rate (FPR) is $1 - \\text{TNR} = 1 - \\Pr(Z_{\\max} < 4 \\mid Y=0) = 1 - [\\Phi(4)]^{100}$.\n$\\text{FPR} \\approx 1 - (1 - 100(1-\\Phi(4))) = 100(1-\\Phi(4)) \\approx 100 \\cdot (3.167 \\times 10^{-5}) = 0.003167$, which is $0.32\\%$. This is not \"about $3\\%$\". The claim is off by an order of magnitude.\n- True Positive Rate (TPR) was calculated as $\\approx 0.84$ ($84\\%$). The claim is \"$16\\%$\". $16\\%$ is approximately $\\Pr(X_j<4)$, which is not the TPR. The claim is incorrect.\n- The BA calculation in the statement is based on these flawed rates. The conclusion that GMP is inferior to GAP is based on this flawed BA and contradicts our findings.\n- Verdict: **Incorrect**.\n\n**D. With suitable thresholds, GAP and GMP necessarily achieve the same Bayes error on this toy distribution because both are permutation-invariant functions of $\\{X_i\\}_{i=1}^S$.**\n- Both GAP and GMP are indeed permutation-invariant functions of the activations $\\{X_i\\}$.\n- However, being a permutation-invariant function is not sufficient to be a Baves-optimal decision function, nor does it guarantee that any two such functions yield the same performance.\n- The Bayes-optimal classifier for this problem would use the likelihood ratio, which is a function of $\\sum_i \\exp(c X_i)$ for some constant $c$, not a simple sum or maximum of the $X_i$.\n- GAP and GMP are different summary statistics that discard information in different ways. There is no a priori reason they should have the same performance.\n- Our explicit calculations in the analysis of option A show very different performances: $\\text{BA}_{\\text{GAP}} \\approx 0.60$ and $\\text{BA}_{\\text{GMP}} \\approx 0.92$. This empirically demonstrates that they do not achieve the same error.\n- Verdict: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The distinction between Global Average Pooling (GAP) and Global Max Pooling (GMP) is not as rigid as it may seem; they can be viewed as two endpoints of a single, more general pooling operation. This exercise  introduces the concept of generalized mean pooling, or $p$-norm pooling, which unifies GAP (where $p=1$) and GMP (as $p \\to \\infty$). By analyzing how the backpropagated gradient is distributed as $p$ increases, you will gain a deeper insight into the learning dynamics, seeing how the network's focus can shift from a diffuse, averaged signal to a sharp, concentrated one. This provides a sophisticated understanding of how pooling shapes feature learning.",
            "id": "3129761",
            "problem": "Consider a Convolutional Neural Network (CNN) whose last convolutional block produces nonnegative activations $a_{c,h,w} \\ge 0$ per channel $c$ at spatial coordinates $(h,w)$ due to the Rectified Linear Unit (ReLU). The per-channel pooled feature $y_c(p)$ is computed by generalized mean pooling (also called $p$-norm pooling):\n$$\ny_c(p) \\;=\\; \\left(\\frac{1}{HW}\\sum_{h=1}^{H}\\sum_{w=1}^{W} a_{c,h,w}^{\\,p}\\right)^{\\!\\!1/p},\n$$\nwhere $H$ and $W$ are spatial dimensions and $p \\ge 1$. A linear classifier computes $z = \\sum_{c} w_c\\,y_c(p)$ followed by a differentiable loss $\\ell(z)$, optimized by Stochastic Gradient Descent (SGD). Suppose we employ a curriculum on pooling sharpness: an annealing schedule $p(t)$ that starts at $p(0)=1$ (Global Average Pooling (GAP)) and increases towards large $p$ that approximates Global Max Pooling (GMP) as training time $t$ advances.\n\nUsing only the definition above, the chain rule for backpropagation, and standard smoothness assumptions on $\\ell$, reason about how the backpropagated gradients distribute across spatial positions and how this distribution changes as $p$ increases. Then select the statement that best characterizes the resulting learning dynamics under the annealing $p(t)$ schedule.\n\nA. Under nonnegative activations and a differentiable loss, annealing $p$ from $1$ to large values shifts the gradient mass toward the largest spatial responses, making updates progressively sparser. This tends to stabilize early training and later sharpen representations by emphasizing salient locations.\n\nB. For any $p>1$, the gradient backpropagated to each spatial position has exactly the same magnitude, so annealing does not change learning dynamics relative to Global Average Pooling.\n\nC. As $p$ increases, the magnitude of the gradient at non-max locations provably grows, thus avoiding vanishing gradients as $p\\to\\infty$.\n\nD. Annealing $p$ from $1$ to large values strictly decreases the sensitivity (Lipschitz constant) of the pooled output with respect to its inputs, guaranteeing increased robustness to input noise at every step.",
            "solution": "The problem statement is analyzed for validity.\n\n**Step 1: Extract Givens**\n- The last convolutional block of a CNN produces nonnegative activations $a_{c,h,w} \\ge 0$ (due to ReLU). The indices represent channel ($c$), height ($h$), and width ($w$).\n- Generalized mean pooling (p-norm pooling) is used to compute a per-channel feature $y_c(p)$:\n$$y_c(p) \\;=\\; \\left(\\frac{1}{HW}\\sum_{h=1}^{H}\\sum_{w=1}^{W} a_{c,h,w}^{\\,p}\\right)^{\\!\\!1/p}$$\n- $H$ and $W$ are spatial dimensions.\n- The pooling parameter is $p \\ge 1$.\n- A linear classifier computes $z = \\sum_{c} w_c\\,y_c(p)$.\n- A differentiable loss function $\\ell(z)$ is optimized using Stochastic Gradient Descent (SGD).\n- An annealing schedule $p(t)$ is employed, where $p$ starts at $p(0)=1$ (Global Average Pooling, GAP) and increases towards large values, which approximates Global Max Pooling (GMP).\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The concepts presented (CNN, ReLU, generalized mean pooling, GAP, GMP, backpropagation, curriculum learning) are standard and well-established in the field of deep learning. The mathematical formulation is correct.\n- **Well-Posed:** The problem is well-defined. It asks for an analysis of the gradient distribution with respect to a parameter $p$, which can be derived mathematically from the provided equations.\n- **Objective:** The language is formal and objective.\n- **Completeness and Consistency:** The problem provides all the necessary definitions and constraints to perform the required analysis. There are no contradictions.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. We may proceed with the solution.\n\n**Derivation of Gradient Distribution**\n\nThe primary task is to understand how the gradient of the loss $\\ell$ with respect to the activations $a_{c,h,w}$ is distributed across the spatial locations $(h,w)$ and how this distribution evolves as $p$ increases. We use the chain rule for backpropagation.\n\nThe gradient of the loss $\\ell$ with respect to a single activation $a_{c',h',w'}$ is:\n$$\n\\frac{\\partial \\ell}{\\partial a_{c',h',w'}} = \\frac{\\partial \\ell}{\\partial z} \\frac{\\partial z}{\\partial a_{c',h',w'}}\n$$\nThe first term, $\\frac{\\partial \\ell}{\\partial z}$, is an upstream gradient that acts as a scalar multiplier. We denote it by $\\delta_z$. Let's analyze the second term. From the definition $z = \\sum_{c} w_c\\,y_c(p)$, the derivative with respect to $a_{c',h',w'}$ is non-zero only for the term where $c=c'$:\n$$\n\\frac{\\partial z}{\\partial a_{c',h',w'}} = w_{c'} \\frac{\\partial y_{c'}(p)}{\\partial a_{c',h',w'}}\n$$\nNow, we must compute the derivative of the generalized mean pooling function $y_c(p)$ with respect to one of its inputs, $a_{c,h,w}$ (dropping the primes for notational clarity).\nLet $S_c(p) = \\frac{1}{HW}\\sum_{i=1}^{H}\\sum_{j=1}^{W} a_{c,i,j}^{\\,p}$. Then $y_c(p) = (S_c(p))^{1/p}$.\nUsing the chain rule again:\n$$\n\\frac{\\partial y_c}{\\partial a_{c,h,w}} = \\frac{\\partial y_c}{\\partial S_c} \\frac{\\partial S_c}{\\partial a_{c,h,w}}\n$$\nThe components are:\n$$\n\\frac{\\partial y_c}{\\partial S_c} = \\frac{1}{p} (S_c(p))^{\\frac{1}{p}-1}\n$$\n$$\n\\frac{\\partial S_c}{\\partial a_{c,h,w}} = \\frac{\\partial}{\\partial a_{c,h,w}} \\left(\\frac{1}{HW}\\sum_{i=1}^{H}\\sum_{j=1}^{W} a_{c,i,j}^{\\,p}\\right) = \\frac{1}{HW} \\cdot p \\cdot a_{c,h,w}^{\\,p-1}\n$$\nCombining these:\n$$\n\\frac{\\partial y_c}{\\partial a_{c,h,w}} = \\frac{1}{p} (S_c(p))^{\\frac{1-p}{p}} \\cdot \\frac{p \\cdot a_{c,h,w}^{\\,p-1}}{HW} = (S_c(p))^{\\frac{1-p}{p}} \\frac{a_{c,h,w}^{\\,p-1}}{HW}\n$$\nWe can express this more elegantly by substituting $S_c(p) = (y_c(p))^p$:\n$$\n\\frac{\\partial y_c}{\\partial a_{c,h,w}} = \\left((y_c(p))^p\\right)^{\\frac{1-p}{p}} \\frac{a_{c,h,w}^{\\,p-1}}{HW} = (y_c(p))^{1-p} \\frac{a_{c,h,w}^{\\,p-1}}{HW}\n$$\nThis simplifies to:\n$$\n\\frac{\\partial y_c}{\\partial a_{c,h,w}} = \\frac{1}{HW} \\left(\\frac{a_{c,h,w}}{y_c(p)}\\right)^{p-1}\n$$\nThe full gradient for the activation at $(c,h,w)$ is:\n$$\n\\frac{\\partial \\ell}{\\partial a_{c,h,w}} = \\delta_z \\cdot w_c \\cdot \\frac{1}{HW} \\left(\\frac{a_{c,h,w}}{y_c(p)}\\right)^{p-1}\n$$\nThe term $\\delta_z \\cdot w_c \\cdot \\frac{1}{HW} \\cdot (y_c(p))^{1-p}$ is a common scalar multiplier for all spatial positions within channel $c$. The spatial distribution of the gradient is therefore determined by the term $a_{c,h,w}^{\\,p-1}$.\n$$\n\\frac{\\partial \\ell}{\\partial a_{c,h,w}} \\propto a_{c,h,w}^{\\,p-1}\n$$\n\n**Analysis of Annealing Schedule $p(t)$**\n\n1.  **Start of Training ($p=1$, Global Average Pooling):**\n    For $p=1$, the gradient dependency is $a_{c,h,w}^{\\,1-1} = a_{c,h,w}^{\\,0} = 1$ (for $a_{c,h,w} > 0$). The gradient $\\frac{\\partial y_c(1)}{\\partial a_{c,h,w}} = \\frac{1}{HW}$ is constant for all spatial locations. This means the update signal from the loss is distributed uniformly across all activated spatial positions. This averaging effect can prevent the network from overfitting to spurious high-frequency details early in training, leading to a more stable learning process.\n\n2.  **As Training Progresses ($p$ increases towards $\\infty$):**\n    For $p>1$, the gradient magnitude at a location $(h,w)$ is proportional to $a_{c,h,w}^{\\,p-1}$. Let $a_{max} = \\max_{h,w} a_{c,h,w}$ be the maximum activation in the channel, and let $a_{other} < a_{max}$ be another activation. The ratio of their corresponding gradient magnitudes is:\n    $$\n    \\frac{\\text{grad at } a_{other}}{\\text{grad at } a_{max}} = \\frac{a_{other}^{\\,p-1}}{a_{max}^{\\,p-1}} = \\left(\\frac{a_{other}}{a_{max}}\\right)^{p-1}\n    $$\n    Since $\\frac{a_{other}}{a_{max}} < 1$, this ratio approaches $0$ as $p \\to \\infty$. This shows that as $p$ increases, the gradient mass becomes increasingly concentrated at the single spatial location with the maximum activation. All other locations receive vanishingly small gradients. This makes the gradient updates spatially sparse. This behavior forces the network to focus on the most salient evidence for a feature, effectively sharpening the learned representations.\n\n**Option-by-Option Analysis**\n\n**A. Under nonnegative activations and a differentiable loss, annealing $p$ from $1$ to large values shifts the gradient mass toward the largest spatial responses, making updates progressively sparser. This tends to stabilize early training and later sharpen representations by emphasizing salient locations.**\nThis statement aligns perfectly with our derivation.\n- \"shifts the gradient mass toward the largest spatial responses, making updates progressively sparser\": Our analysis showed that the gradient becomes concentrated at the location of the maximum activation as $p$ increases, which is precisely this phenomenon.\n- \"tends to stabilize early training\": Starting with $p=1$ (GAP) averages the gradients, which is a known technique for stabilization.\n- \"later sharpen representations by emphasizing salient locations\": As $p$ increases, the network is forced to learn from the most discriminative (salient) locations, leading to sharper, more focused feature detectors.\nTherefore, this statement is **Correct**.\n\n**B. For any $p>1$, the gradient backpropagated to each spatial position has exactly the same magnitude, so annealing does not change learning dynamics relative to Global Average Pooling.**\nThis is factually incorrect. The gradient magnitude is proportional to $a_{c,h,w}^{\\,p-1}$. For $p>1$, this magnitude is only the same across all positions if all activations $a_{c,h,w}$ are identical. In any typical scenario, activations will vary, and thus gradient magnitudes will differ. The statement is only true for the specific case of $p=1$.\nTherefore, this statement is **Incorrect**.\n\n**C. As $p$ increases, the magnitude of the gradient at non-max locations provably grows, thus avoiding vanishing gradients as $p\\to\\infty$.**\nThis is the opposite of what our analysis showed. The ratio of the gradient at a non-max location to the gradient at the max location is $(\\frac{a_{other}}{a_{max}})^{p-1}$, which tends to $0$ as $p \\to \\infty$. The gradients at non-max locations vanish relative to the max location's gradient. A more detailed analysis shows their absolute magnitude also vanishes.\nTherefore, this statement is **Incorrect**.\n\n**D. Annealing $p$ from $1$ to large values strictly decreases the sensitivity (Lipschitz constant) of the pooled output with respect to its inputs, guaranteeing increased robustness to input noise at every step.**\nLet's test the \"strictly decreases\" claim. Consider a simple case where all activations within a channel are uniform: $a_{c,h,w} = A$ for some constant $A > 0$.\nIn this case, the pooled output is $y_c(p) = \\left(\\frac{1}{HW}\\sum_{h,w} A^p\\right)^{1/p} = \\left(\\frac{HW \\cdot A^p}{HW}\\right)^{1/p} = A$.\nThe gradient at any location is $\\frac{\\partial y_c}{\\partial a_{c,h,w}} = \\frac{1}{HW} \\left(\\frac{A}{A}\\right)^{p-1} = \\frac{1}{HW}$.\nThe gradient for this specific input is constant for all $p \\ge 1$. Therefore, the sensitivity (as measured by any norm of the gradient vector) does not strictly decrease; it remains constant. Since the statement claims a strict decrease is guaranteed at every step, this counterexample proves it false.\nTherefore, this statement is **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}