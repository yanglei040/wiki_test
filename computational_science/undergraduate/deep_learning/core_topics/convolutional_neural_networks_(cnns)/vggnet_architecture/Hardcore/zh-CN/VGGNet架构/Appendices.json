{
    "hands_on_practices": [
        {
            "introduction": "设计神经网络不仅仅是为了追求高精度，更是一场在模型性能与硬件资源限制之间的精妙平衡。本练习将带你从零开始，学习如何量化VGG风格网络中每个卷积层的计算量（$FLOPs$）和内存访问成本，这对于在资源受限的设备（如移动电话或嵌入式系统）上部署模型至关重要。通过亲手计算，你将深刻理解架构决策（如通道数和层数）如何直接影响模型的实际运行效率。",
            "id": "3198589",
            "problem": "一个团队正在为一个计算和内存带宽资源有限的设备，调整简化版视觉几何组（VGG）网络架构的规模，以便进行部署。每个变体由多个二维卷积层堆叠而成，其核大小为 $K$，步幅为 $1$，并使用零填充以保持空间维度不变。忽略偏置项、批量归一化、非线性以及池化操作的计算或流量；平均池化仅改变后续层的 $(H, W)$。每个张量元素都以单精度浮点数存储，因此每个元素占用 $4$ 字节。\n\n从离散二维卷积的定义出发，通过从基本原理计算乘法和加法的次数，推导出每个卷积层在前向传播中的浮点运算数。使用标准的基准测试约定，即一次乘加运算计为两次浮点运算。同时，假设对于每个卷积层，输入激活张量和滤波器张量从主内存中读取一次，输出激活张量写入主内存一次，推导出每层的主内存流量模型。\n\n给定三种候选变体。对于所有卷积层，核大小为 $K=3$，填充保持 $(H, W)$ 不变。输入图像的空间尺寸为 $64 \\times 64$，有 $3$ 个通道。这些变体是：\n\n- 变体 A:\n  - 层 $1$：$(H, W, C_{\\text{in}}, C_{\\text{out}}) = (64, 64, 3, 16)$。\n  - 层 $2$：$(64, 64, 16, 16)$。\n  - 平均池化将空间维度减半：$(H, W) \\mapsto (32, 32)$。\n  - 层 $3$：$(32, 32, 16, 32)$。\n  - 层 $4$：$(32, 32, 32, 32)$。\n  - 平均池化将空间维度减半：$(H, W) \\mapsto (16, 16)$。\n  - 层 $5$：$(16, 16, 32, 64)$。\n  - 层 $6$：$(16, 16, 64, 64)$。\n\n- 变体 B:\n  - 层 $1$：$(64, 64, 3, 16)$。\n  - 层 $2$：$(64, 64, 16, 16)$。\n  - 平均池化：$(H, W) \\mapsto (32, 32)$。\n  - 层 $3$：$(32, 32, 16, 32)$。\n  - 层 $4$：$(32, 32, 32, 32)$。\n  - 层 $5$：$(32, 32, 32, 32)$。\n  - 平均池化：$(H, W) \\mapsto (16, 16)$。\n  - 层 $6$：$(16, 16, 32, 64)$。\n  - 层 $7$：$(16, 16, 64, 64)$。\n  - 层 $8$：$(16, 16, 64, 64)$。\n\n- 变体 C:\n  - 层 $1$：$(64, 64, 3, 32)$。\n  - 层 $2$：$(64, 64, 32, 32)$。\n  - 平均池化：$(H, W) \\mapsto (32, 32)$。\n  - 层 $3$：$(32, 32, 32, 64)$。\n  - 层 $4$：$(32, 32, 64, 64)$。\n  - 平均池化：$(H, W) \\mapsto (16, 16)$。\n  - 层 $5$：$(16, 16, 64, 128)$。\n  - 层 $6$：$(16, 16, 128, 128)$。\n\n设每次前向传播的计算预算为 $2.0 \\times 10^{8}$ 次浮点运算，主内存流量预算为 $3.0 \\times 10^{6}$ 字节。在满足这两个预算的变体中，选择总浮点运算数最大的一个。作为最终答案，报告所选变体的确切总浮点运算数，以单个整数形式表示。最终答案中不要包含单位。",
            "solution": "问题陈述已经过严格验证，被认为是自洽、一致且科学合理的。该问题定义明确、客观，并且可以在深度学习性能分析领域内形式化为一个可解问题。未发现任何缺陷。我们可以开始求解。\n\n首先，我们根据问题陈述，推导单个二维卷积层在前向传播中的浮点运算（FLOP）数和主内存流量的通用表达式。\n\n设输入激活张量的维度为 $(H, W, C_{\\text{in}})$，分别代表高度、宽度和输入通道数。\n设输出激活张量的维度为 $(H_{\\text{out}}, W_{\\text{out}}, C_{\\text{out}})$。\n设卷积核大小为 $K \\times K$。\n问题陈述中指出，使用填充来保持空间维度，因此 $H_{\\text{out}} = H$ 和 $W_{\\text{out}} = W$。步幅给定为 $1$。\n\n浮点运算（FLOP）的推导：\n为了计算一个输出特征图（输出张量的一个通道）中的单个值，需要将一个 $K \\times K \\times C_{\\text{in}}$ 的滤波器应用于输入张量的一个相应区域。这涉及到 $K \\times K \\times C_{\\text{in}}$ 次乘法运算和 $(K \\times K \\times C_{\\text{in}} - 1)$ 次加法运算。由于忽略了偏置，没有其他加法。\n产生一个输出值的总运算次数为 $(K^2 C_{\\text{in}}) + (K^2 C_{\\text{in}} - 1)$。\n根据一次乘加运算计为两次 FLOPs 的标准基准测试约定，我们可以将一个输出值的成本近似为 $2 \\times K^2 \\times C_{\\text{in}}$。\n这个计算必须对输出张量中的每个空间位置执行，共有 $H \\times W$ 个位置。\n此外，对于 $C_{\\text{out}}$ 个输出通道中的每一个，都必须执行此操作。\n因此，单个卷积层的总 FLOP 数 $F$ 为：\n$$F = (2 \\cdot K^2 \\cdot C_{\\text{in}}) \\cdot (H \\cdot W \\cdot C_{\\text{out}})$$\n$$F = 2 \\cdot H \\cdot W \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2$$\n\n主内存流量的推导：\n内存流量 $M$ 是从主内存读取和写入主内存的字节数之和。每个张量元素是占用 $4$ 字节的单精度浮点数。\n$1$. 输入张量读取：大小为 $H \\times W \\times C_{\\text{in}}$ 的整个输入张量被读取一次。流量为 $4 \\cdot H \\cdot W \\cdot C_{\\text{in}}$ 字节。\n$2$. 滤波器张量读取：卷积滤波器（权重）被读取一次。有 $C_{\\text{out}}$ 个滤波器，每个大小为 $K \\times K \\times C_{\\text{in}}$。总大小为 $K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}$。流量为 $4 \\cdot K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}$ 字节。\n$3$. 输出张量写入：大小为 $H \\times W \\times C_{\\text{out}}$ 的结果输出张量被写入一次。流量为 $4 \\cdot H \\cdot W \\cdot C_{\\text{out}}$ 字节。\n每层的总内存流量是这三个部分的总和：\n$$M = 4 \\cdot H \\cdot W \\cdot C_{\\text{in}} + 4 \\cdot K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} + 4 \\cdot H \\cdot W \\cdot C_{\\text{out}}$$\n$$M = 4 \\cdot (H \\cdot W \\cdot (C_{\\text{in}} + C_{\\text{out}}) + K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}})$$\n\n现在，我们使用 $K=3$（因此 $K^2=9$）来评估每个变体。\n预算为：FLOPs $\\le 2.0 \\times 10^8$ 和内存 $\\le 3.0 \\times 10^6$ 字节。\n\n变体 A:\n- 层 $1$：$(H, W, C_{\\text{in}}, C_{\\text{out}}) = (64, 64, 3, 16)$\n  $F_1 = 2 \\cdot 64 \\cdot 64 \\cdot 3 \\cdot 16 \\cdot 9 = 3,538,944$\n  $M_1 = 4 \\cdot (64 \\cdot 64 \\cdot (3+16) + 9 \\cdot 3 \\cdot 16) = 4 \\cdot (4096 \\cdot 19 + 432) = 313,024$\n- 层 $2$：$(64, 64, 16, 16)$\n  $F_2 = 2 \\cdot 64 \\cdot 64 \\cdot 16 \\cdot 16 \\cdot 9 = 18,874,368$\n  $M_2 = 4 \\cdot (4096 \\cdot (16+16) + 9 \\cdot 16 \\cdot 16) = 4 \\cdot (4096 \\cdot 32 + 2304) = 533,504$\n- 层 $3$：$(32, 32, 16, 32)$\n  $F_3 = 2 \\cdot 32 \\cdot 32 \\cdot 16 \\cdot 32 \\cdot 9 = 9,437,184$\n  $M_3 = 4 \\cdot (1024 \\cdot (16+32) + 9 \\cdot 16 \\cdot 32) = 4 \\cdot (1024 \\cdot 48 + 4608) = 215,040$\n- 层 $4$：$(32, 32, 32, 32)$\n  $F_4 = 2 \\cdot 32 \\cdot 32 \\cdot 32 \\cdot 32 \\cdot 9 = 18,874,368$\n  $M_4 = 4 \\cdot (1024 \\cdot (32+32) + 9 \\cdot 32 \\cdot 32) = 4 \\cdot (1024 \\cdot 64 + 9216) = 299,008$\n- 层 $5$：$(16, 16, 32, 64)$\n  $F_5 = 2 \\cdot 16 \\cdot 16 \\cdot 32 \\cdot 64 \\cdot 9 = 9,437,184$\n  $M_5 = 4 \\cdot (256 \\cdot (32+64) + 9 \\cdot 32 \\cdot 64) = 4 \\cdot (256 \\cdot 96 + 18432) = 172,032$\n- 层 $6$：$(16, 16, 64, 64)$\n  $F_6 = 2 \\cdot 16 \\cdot 16 \\cdot 64 \\cdot 64 \\cdot 9 = 18,874,368$\n  $M_6 = 4 \\cdot (256 \\cdot (64+64) + 9 \\cdot 64 \\cdot 64) = 4 \\cdot (256 \\cdot 128 + 36864) = 278,224$\n\n- 变体 A 的总计：\n  $F_A = 3538944 + 18874368 + 9437184 + 18874368 + 9437184 + 18874368 = 79,036,416$\n  $M_A = 313024 + 533504 + 215040 + 299008 + 172032 + 278224 = 1,810,832$\n- 预算检查：$F_A = 7.90 \\times 10^7 \\le 2.0 \\times 10^8$ (通过)。$M_A = 1.81 \\times 10^6 \\le 3.0 \\times 10^6$ (通过)。\n变体 A 是一个有效的候选方案。\n\n变体 B:\n- 层 1-4 是 B 的层的一个子集。层 1-2，池化，层 3-5，池化，层 6-8。\n- 层 $1, 2, 3, 4, 5$: $(64,64,3,16), (64,64,16,16), (32,32,16,32), (32,32,32,32), (32,32,32,32)$\n  $F_1=3,538,944, M_1=313,024$\n  $F_2=18,874,368, M_2=533,504$\n  $F_3=9,437,184, M_3=215,040$\n  $F_4=18,874,368, M_4=299,008$\n  $F_5=18,874,368, M_5=299,008$\n- 层 $6, 7, 8$: $(16,16,32,64), (16,16,64,64), (16,16,64,64)$\n  $F_6 = 2 \\cdot 16 \\cdot 16 \\cdot 32 \\cdot 64 \\cdot 9 = 9,437,184$, $M_6=172,032$\n  $F_7 = 2 \\cdot 16 \\cdot 16 \\cdot 64 \\cdot 64 \\cdot 9 = 18,874,368$, $M_7=278,224$\n  $F_8 = 18,874,368$, $M_8=278,224$\n- 变体 B 的总计：\n  $F_B = 3538944 + 18874368 + 9437184 + 18874368 + 18874368 + 9437184 + 18874368 + 18874368 = 116,785,152$\n  $M_B = 313024 + 533504 + 215040 + 299008 + 299008 + 172032 + 278224 + 278224 = 2,388,064$\n- 预算检查：$F_B = 1.17 \\times 10^8 \\le 2.0 \\times 10^8$ (通过)。$M_B = 2.39 \\times 10^6 \\le 3.0 \\times 10^6$ (通过)。\n变体 B 是一个有效的候选方案。\n\n变体 C:\n- 层 $1$：$(64, 64, 3, 32)$\n  $F_1 = 2 \\cdot 64 \\cdot 64 \\cdot 3 \\cdot 32 \\cdot 9 = 7,077,888$\n  $M_1 = 4 \\cdot (4096 \\cdot (3+32) + 9 \\cdot 3 \\cdot 32) = 576,896$\n- 层 $2$：$(64, 64, 32, 32)$\n  $F_2 = 2 \\cdot 64 \\cdot 64 \\cdot 32 \\cdot 32 \\cdot 9 = 37,748,736$\n  $M_2 = 4 \\cdot (4096 \\cdot (32+32) + 9 \\cdot 32 \\cdot 32) = 1,085,440$\n- 层 $3$：$(32, 32, 32, 64)$\n  $F_3 = 2 \\cdot 32 \\cdot 32 \\cdot 32 \\cdot 64 \\cdot 9 = 37,748,736$\n  $M_3 = 4 \\cdot (1024 \\cdot (32+64) + 9 \\cdot 32 \\cdot 64) = 466,944$\n- 层 $4$：$(32, 32, 64, 64)$\n  $F_4 = 2 \\cdot 32 \\cdot 32 \\cdot 64 \\cdot 64 \\cdot 9 = 75,497,472$\n  $M_4 = 4 \\cdot (1024 \\cdot (64+64) + 9 \\cdot 64 \\cdot 64) = 671,744$\n- 层 $5$：$(16, 16, 64, 128)$\n  $F_5 = 2 \\cdot 16 \\cdot 16 \\cdot 64 \\cdot 128 \\cdot 9 = 37,748,736$\n  $M_5 = 4 \\cdot (256 \\cdot (64+128) + 9 \\cdot 64 \\cdot 128) = 491,520$\n- 层 $6$：$(16, 16, 128, 128)$\n  $F_6 = 2 \\cdot 16 \\cdot 16 \\cdot 128 \\cdot 128 \\cdot 9 = 75,497,472$\n  $M_6 = 4 \\cdot (256 \\cdot (128+128) + 9 \\cdot 128 \\cdot 128) = 851,968$\n\n- 变体 C 的总计：\n  $F_C = 7077888 + 37748736 + 37748736 + 75497472 + 37748736 + 75497472 = 271,319,040$\n  $M_C = 576896 + 1085440 + 466944 + 671744 + 491520 + 851968 = 4,144,512$\n- 预算检查：$F_C = 2.71 \\times 10^8 > 2.0 \\times 10^8$ (未通过)。$M_C = 4.14 \\times 10^6 > 3.0 \\times 10^6$ (未通过)。\n变体 C 不是一个有效的候选方案。\n\n比较与选择：\n有效的候选方案是变体 A 和变体 B。我们必须选择总 FLOP 数最大的那个。\n$F_A = 79,036,416$\n$F_B = 116,785,152$\n由于 $F_B > F_A$，因此选择变体 B 模型。问题要求给出所选变体的确切总浮点运算数。\n所需值为 $F_B = 116,785,152$。",
            "answer": "$$\\boxed{116785152}$$"
        },
        {
            "introduction": "一个网络的性能不仅取决于其宏观架构，还取决于其内部精细的动态平衡，例如激活值的分布。本练习将引导你运用统计学原理，特别是中心极限定理，来构建一个分析模型，探讨不同的输入数据预处理方法如何影响第一层ReLU激活函数的“饱和”概率。这个看似理论化的实践，实际上揭示了避免“神经元死亡”等训练难题的根本机制，让你对网络初始化的重要性有更深层次的认识。",
            "id": "3198668",
            "problem": "您正在研究视觉几何组（Visual Geometry Group, VGG）网络（VGGNet）的第一个卷积层，该网络使用小尺寸空间滤波器和修正线性单元（Rectified Linear Unit, ReLU）非线性激活函数。考虑该层中的单个预激活值，其模型为一个线性输入组合后加上一个偏置。您的目标是研究两种常见的输入预处理流程如何影响第一层 ReLU 在零处饱和的概率。这两种流程是：(i) 完全归一化 $x \\mapsto (x-\\mu)/\\sigma$，以及 (ii) 仅减去均值 $x \\mapsto x-\\mu$。请基于以下基本和建模假设：(a) 卷积是线性的，因此一个预激活值是感受野输入的加权和加上一个偏置；(b) 预处理后，在每个通道内，局部感受野内的输入是独立同分布的；(c) 权重在感受野内是独立的，具有给定的均值和方差；(d) 根据中心极限定理，一个由许多微小独立贡献组成的预激活值可以很好地近似为高斯分布；(e) 每当预激活值为非正数时，ReLU 激活函数在零处饱和。基于这些假设，仅使用独立随机变量线性组合的期望和方差的核心定义，推导出一个饱和概率的解析表达式，然后实现它。\n\n定义：\n- 卷积神经元中的一个预激活值 $z$ 建模为 $z = \\sum_{i=1}^{n} w_i x_i + b$，其中 $n$ 是感受野大小 $n = C \\cdot k_h \\cdot k_w$，$w_i$ 是权重，$x_i$ 是预处理后的输入，$b$ 是偏置。\n- 修正线性单元（ReLU）输出 $\\max\\{0, z\\}$，并在 $z \\le 0$ 时在零处饱和。\n- 假设权重满足 $\\mathbb{E}[w_i] = 0$ 和 $\\mathrm{Var}(w_i) = s_w^2$，预处理后的输入具有通道级均值 $\\mathbb{E}[x_i]=0$ 和方差 $\\mathrm{Var}(x_i) = s_x^2$。在完全归一化流程下，$s_x^2 = 1$。在仅减去均值流程下，$s_x^2 = \\sigma^2$，其中 $\\sigma$ 已给定。\n- 使用中心极限定理和独立性假设对 $z$ 进行高斯近似，利用标准正态累积分布函数（CDF）来估计饱和概率 $\\mathbb{P}[z \\le 0]$。\n\n实现一个程序，为每个测试用例计算两个量：\n- 在完全归一化 $(x-\\mu)/\\sigma$ 下的饱和概率。\n- 在仅减去均值 $(x-\\mu)$ 下的饱和概率。\n\n仅使用上述假设以及独立变量和的期望和方差定律来推导每种情况所需的表达式。不要使用任何外部数据。不应读取任何用户输入。\n\n测试套件：\n每个测试用例以元组 $(n, s_w^2, b, \\sigma)$ 的形式给出，其中：\n- $n$：感受野大小 $C \\cdot k_h \\cdot k_w$，\n- $s_w^2$：权重方差，\n- $b$：偏置，\n- $\\sigma$：仅减去均值后输入的标准差（因此，对于仅减去均值，$s_x^2 = \\sigma^2$；对于完全归一化，使用 $s_x^2 = 1$）。\n\n以小数形式提供答案。为保证数值稳定性和可比性，将每个概率四舍五入到小数点后恰好 $6$ 位。\n\n使用以下测试用例：\n- 用例 1：$n = 27$, $s_w^2 = 2/27$, $b = 0.1$, $\\sigma = 0.5$。\n- 用例 2：$n = 576$, $s_w^2 = 2/576$, $b = 0.0$, $\\sigma = 1.8$。\n- 用例 3：$n = 27$, $s_w^2 = 1/27$, $b = -0.2$, $\\sigma = 2.0$。\n- 用例 4：$n = 27$, $s_w^2 = 2/27$, $b = 0.3$, $\\sigma = 0.1$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为\n$[p_{\\text{norm},1}, p_{\\text{mean},1}, p_{\\text{norm},2}, p_{\\text{mean},2}, p_{\\text{norm},3}, p_{\\text{mean},3}, p_{\\text{norm},4}, p_{\\text{mean},4}]$，\n其中 $p_{\\text{norm},j}$ 是用例 $j$ 在完全归一化下的饱和概率，$p_{\\text{mean},j}$ 是用例 $j$ 在仅减去均值下的饱和概率。每个值必须四舍五入到小数点后恰好 $6$ 位。",
            "solution": "该问题要求推导并实现一个解析表达式，用于计算类 VGG 神经网络第一层中修正线性单元（ReLU）的饱和概率。当预激活值非正时，就会发生饱和。我们将分析两种不同的输入预处理流程如何影响这个概率：完全归一化和仅减去均值。\n\n分析的关键在于给定的预激活值 $z$ 的模型和一系列统计假设。预激活值由以下线性组合给出：\n$$z = \\sum_{i=1}^{n} w_i x_i + b$$\n其中 $w_i$ 是权重，$x_i$ 是来自大小为 $n$ 的感受野的预处理输入，$b$ 是一个标量偏置项。ReLU 激活函数定义为 $\\max\\{0, z\\}$，当且仅当其输入 $z$ 为非正数（即 $z \\le 0$）时，它会饱和到输出为 $0$。我们的目标是计算此事件的概率 $\\mathbb{P}[z \\le 0]$。\n\n问题指出，根据中心极限定理，作为许多微小独立贡献之和的预激活值 $z$ 可以很好地近似为一个高斯（正态）随机变量。我们用 $\\mu_z$ 表示其均值，用 $\\sigma_z^2$ 表示其方差，使得 $z \\sim \\mathcal{N}(\\mu_z, \\sigma_z^2)$。\n\n为了求出概率 $\\mathbb{P}[z \\le 0]$，我们可以对随机变量 $z$ 进行标准化，并使用标准正态分布的累积分布函数（CDF），记为 $\\Phi(\\cdot)$。\n$$ \\mathbb{P}[z \\le 0] = \\mathbb{P}\\left(\\frac{z - \\mu_z}{\\sigma_z} \\le \\frac{0 - \\mu_z}{\\sigma_z}\\right) = \\Phi\\left(-\\frac{\\mu_z}{\\sigma_z}\\right) $$\n问题的核心是根据权重 $w_i$ 和输入 $x_i$ 的统计特性来确定 $\\mu_z$ 和 $\\sigma_z^2$ 的表达式。\n\n**1. 预激活均值 $\\mu_z$ 的推导**\n\n$z$ 的均值 $\\mu_z$ 是其期望 $\\mathbb{E}[z]$。利用期望的线性性质：\n$$ \\mu_z = \\mathbb{E}[z] = \\mathbb{E}\\left[\\sum_{i=1}^{n} w_i x_i + b\\right] = \\sum_{i=1}^{n} \\mathbb{E}[w_i x_i] + \\mathbb{E}[b] $$\n偏置 $b$ 是一个常数，所以 $\\mathbb{E}[b] = b$。问题假设权重 $w_i$ 和输入 $x_i$ 是独立的随机变量。因此，它们乘积的期望等于它们期望的乘积：$\\mathbb{E}[w_i x_i] = \\mathbb{E}[w_i] \\mathbb{E}[x_i]$。\n问题给出了以下统计特性：\n-   对于所有 $i \\in \\{1, \\dots, n\\}$，$\\mathbb{E}[w_i] = 0$。\n-   对于所有 $i \\in \\{1, \\dots, n\\}$，$\\mathbb{E}[x_i] = 0$（对于两种预处理流程均如此）。\n将这些代入 $\\mathbb{E}[w_i x_i]$ 的表达式中，得到 $\\mathbb{E}[w_i x_i] = 0 \\cdot 0 = 0$。\n因此，预激活的均值为：\n$$ \\mu_z = \\sum_{i=1}^{n} 0 + b = b $$\n\n**2. 预激活方差 $\\sigma_z^2$ 的推导**\n\n$z$ 的方差 $\\sigma_z^2$ 是 $\\mathrm{Var}(z)$。\n$$ \\sigma_z^2 = \\mathrm{Var}(z) = \\mathrm{Var}\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) $$\n加上一个常数偏置 $b$ 不会影响方差，所以 $\\mathrm{Var}(z) = \\mathrm{Var}\\left(\\sum_{i=1}^{n} w_i x_i\\right)$。\n问题指出，权重 $w_i$ 彼此独立，输入 $x_i$ 也彼此独立。在此背景下的一个标准假设是，对于所有的 $i,j$，所有的 $w_i$ 和 $x_j$ 都是相互独立的。这意味着乘积项 $w_i x_i$ 也是独立的。对于独立随机变量的和，和的方差等于方差的和：\n$$ \\sigma_z^2 = \\sum_{i=1}^{n} \\mathrm{Var}(w_i x_i) $$\n要计算 $\\mathrm{Var}(w_i x_i)$，我们使用两个独立随机变量 $A$ 和 $B$ 乘积的方差公式：$\\mathrm{Var}(AB) = \\mathrm{Var}(A)\\mathrm{Var}(B) + (\\mathbb{E}[A])^2\\mathrm{Var}(B) + (\\mathbb{E}[B])^2\\mathrm{Var}(A)$。鉴于 $\\mathbb{E}[w_i]=0$ 和 $\\mathbb{E}[x_i]=0$，该公式得以简化。一个等效的通用公式是 $\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$。\n对于 $Y_i = w_i x_i$，我们有 $\\mathbb{E}[Y_i] = \\mathbb{E}[w_i]\\mathbb{E}[x_i] = 0 \\cdot 0 = 0$。\n所以, $\\mathrm{Var}(w_i x_i) = \\mathbb{E}[(w_i x_i)^2] = \\mathbb{E}[w_i^2 x_i^2]$。\n由于独立性，$\\mathbb{E}[w_i^2 x_i^2] = \\mathbb{E}[w_i^2] \\mathbb{E}[x_i^2]$。\n我们可以用方差和均值来表示 $\\mathbb{E}[A^2]$：$\\mathbb{E}[A^2] = \\mathrm{Var}(A) + (\\mathbb{E}[A])^2$。\n问题提供了 $\\mathrm{Var}(w_i) = s_w^2$ 和 $\\mathrm{Var}(x_i) = s_x^2$。\n所以, $\\mathbb{E}[w_i^2] = \\mathrm{Var}(w_i) + (\\mathbb{E}[w_i])^2 = s_w^2 + 0^2 = s_w^2$。\n并且, $\\mathbb{E}[x_i^2] = \\mathrm{Var}(x_i) + (\\mathbb{E}[x_i])^2 = s_x^2 + 0^2 = s_x^2$。\n因此, $\\mathrm{Var}(w_i x_i) = s_w^2 s_x^2$。\n由于所有权重都同分布，所有输入也同分布，所以这个方差对所有 $i$ 都相同。总方差为：\n$$ \\sigma_z^2 = \\sum_{i=1}^{n} s_w^2 s_x^2 = n s_w^2 s_x^2 $$\n\n**3. 饱和概率的最终表达式**\n\n现在我们有 $\\mu_z = b$ 和 $\\sigma_z^2 = n s_w^2 s_x^2$。标准差为 $\\sigma_z = \\sqrt{n s_w^2 s_x^2}$。\n饱和概率为：\n$$ \\mathbb{P}[z \\le 0] = \\Phi\\left(-\\frac{\\mu_z}{\\sigma_z}\\right) = \\Phi\\left(-\\frac{b}{\\sqrt{n s_w^2 s_x^2}}\\right) $$\n我们现在将这个表达式针对两种预处理流程进行特化。\n\n**情况 (i): 完全归一化**\n映射 $x \\mapsto (x - \\mu) / \\sigma$ 确保了预处理后的输入方差为 $1$。\n因此，$s_x^2 = 1$。\n饱和概率 $p_{\\text{norm}}$ 为：\n$$ p_{\\text{norm}} = \\Phi\\left(-\\frac{b}{\\sqrt{n s_w^2 \\cdot 1}}\\right) = \\Phi\\left(-\\frac{b}{\\sqrt{n s_w^2}}\\right) $$\n\n**情况 (ii): 仅减去均值**\n映射 $x \\mapsto x - \\mu$ 确保均值为 $0$，但方差仍为原始数据的方差。问题将相应的标准差表示为 $\\sigma$。\n因此，$s_x^2 = \\sigma^2$。\n饱和概率 $p_{\\text{mean}}$ 为：\n$$ p_{\\text{mean}} = \\Phi\\left(-\\frac{b}{\\sqrt{n s_w^2 \\sigma^2}}\\right) = \\Phi\\left(-\\frac{b}{\\sigma \\sqrt{n s_w^2}}\\right) $$\n\n将实现这两个公式来求解给定测试用例中的概率。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the ReLU saturation probability for two input preprocessing pipelines\n    based on a Gaussian approximation of the pre-activation.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (n, s_w^2, b, sigma).\n    test_cases = [\n        (27, 2/27, 0.1, 0.5), # Case 1\n        (576, 2/576, 0.0, 1.8), # Case 2\n        (27, 1/27, -0.2, 2.0), # Case 3\n        (27, 2/27, 0.3, 0.1), # Case 4\n    ]\n\n    results = []\n    for n, s_w_sq, b, sigma in test_cases:\n        # The term sqrt(n * Var(w)) is common to both calculations.\n        # This term represents the standard deviation of the sum of weighted inputs\n        # when the input variance is 1.\n        std_dev_base = np.sqrt(n * s_w_sq)\n\n        # Handle the case where the base standard deviation is zero, to avoid division by zero.\n        # This occurs if n=0 or s_w_sq=0. In this case, z = b, so P(z=0) is 1 if b=0 and 0 if b>0.\n        if std_dev_base == 0:\n            p_norm = 1.0 if b = 0.0 else 0.0\n        else:\n            # Case (i): Full Normalization (input variance s_x^2 = 1)\n            # The argument to the standard normal CDF.\n            arg_norm = -b / std_dev_base\n            p_norm = norm.cdf(arg_norm)\n        \n        results.append(p_norm)\n        \n        # Denominator for the mean subtraction case.\n        # This is sigma * std_dev_base\n        std_dev_z_mean = sigma * std_dev_base\n\n        # Handle potential division by zero if sigma=0 and/or std_dev_base=0.\n        if std_dev_z_mean == 0:\n            p_mean = 1.0 if b = 0.0 else 0.0\n        else:\n            # Case (ii): Mean Subtraction Only (input variance s_x^2 = sigma^2)\n            # The argument to the standard normal CDF.\n            arg_mean = -b / std_dev_z_mean\n            p_mean = norm.cdf(arg_mean)\n        \n        results.append(p_mean)\n\n    # Format the final output as a single string, with each probability\n    # rounded and formatted to exactly 6 decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "训练像VGG这样的大型网络通常非常耗时，而混合精度训练是加速这一过程的强大技术。本练习将让你亲手实现一个微型VGG网络的训练过程，模拟在真实场景中如何利用半精度浮点数（FP16）来提升计算速度。你不仅会体验到理论上的速度提升，更将直面并解决FP16带来的核心挑战——梯度下溢（gradient underflow），通过实践“损失缩放”（loss scaling）技术，确保模型训练的稳定性和最终精度。",
            "id": "3198711",
            "problem": "要求您为一个小型 Visual Geometry Group Network (VGG) 风格的卷积神经网络实现一个基于原理的混合精度训练模拟，其中包含显式的损失缩放以减轻半精度算术中的梯度下溢问题。您的程序必须是一个完整的、可运行的实现，仅使用数组操作从基本原理训练一个微型模型，通过在适当位置转换为半精度来模拟混合精度算术，并计算相对于全精度基线的理论加速比和最终准确率的任何变化。所有数学定义和目标均以下述适合算法推导的形式表示。\n\n使用的基本原理：\n- 卷积层操作：一个步长为 $1$ 且零填充以保持空间尺寸的二维卷积，使用大小为 $3 \\times 3$ 的核。乘加 (MAC) 操作是乘法后跟加法的重复模式。对于一个输入形状为 $\\left(C_{\\text{in}}, H, W\\right)$、输出通道为 $C_{\\text{out}}$、核大小为 $K \\times K$、步长为 $1$ 且带填充的单个卷积层，其前向传播的总 MAC 计数为 $C_{\\text{out}} \\cdot H \\cdot W \\cdot C_{\\text{in}} \\cdot K \\cdot K$。对于训练，假设关于输入和权重的梯度计算的后向传播成本各自的 MAC 计数与前向传播的 MAC 计数在同一数量级，因此每次训练步骤中每个卷积的总 MAC 数约等于前向传播 MAC 数的 $3$ 倍。对于一个输入维度为 $d_{\\text{in}}$、输出维度为 $d_{\\text{out}}$ 的全连接（线性）层，前向传播的 MAC 计数为 $d_{\\text{in}} \\cdot d_{\\text{out}}$，同样假设包含后向传播的一次训练步骤的因子约为 $3$。\n- 带损失缩放的混合精度：使用电气和电子工程师协会 (IEEE) $754$ binary $16$（半精度）进行计算，并以 binary $32$（单精度）存储一份主参数副本。设 $\\alpha$ 为一个正缩放因子。在反向传播期间，将标量损失乘以 $\\alpha$ 会使每个梯度都乘以 $\\alpha$：如果 $\\nabla_{\\theta} \\mathcal{L}$ 表示全精度下的梯度，则缩放后在半精度下计算的梯度约为 $\\operatorname{fp16}\\!\\left(\\alpha \\cdot \\nabla_{\\theta} \\mathcal{L}\\right)$，然后在应用更新前通过在全精度下除以 $\\alpha$ 来取消缩放。其目的是减少小梯度在半精度下的下溢。IEEE $754$ binary $16$ 的最小正规数约为 $2^{-14} \\approx 6.1035 \\times 10^{-5}$，最小正非正规数约为 $2^{-24} \\approx 5.9605 \\times 10^{-8}$；低于最小非正规数的值在半精度下会下溢为零。\n- 吞吐量模型：设 $T_{32}$ 表示 binary $32$ 计算的有效吞吐量，$T_{16}$ 表示 binary $16$ 计算的有效吞吐量。假设 $T_{16} = s \\cdot T_{32}$，其中 $s = 2.0$。对于固定的浮点运算次数 (FLOPs)，混合精度相对于全精度的理想化加速比为 $$S = \\frac{\\text{time}_{32}}{\\text{time}_{16}} = \\frac{\\text{FLOPs} / T_{32}}{\\text{FLOPs} / T_{16}} = \\frac{T_{16}}{T_{32}} = s.$$\n\n需实现的模型和训练设置：\n- 架构（一个最小化的 VGG 式堆栈）：两个卷积层，核大小为 $3 \\times 3$，步长为 $1$，零填充以维持 $8 \\times 8$ 的空间分辨率，每次卷积后跟修正线性单元 (ReLU) 非线性激活，然后是全局平均池化（在空间位置上按通道取均值），最后是一个全连接（线性）分类器。卷积层的通道配置为 $1 \\rightarrow 2 \\rightarrow 2$，然后是一个用于两个类别的线性映射 $\\mathbb{R}^{2} \\rightarrow \\mathbb{R}^{2}$。不需要偏置项。输入维度为 $1 \\times 8 \\times 8$。\n- 损失函数：softmax 交叉熵。通过减去每个样本的最大 logit 来使用数值稳定的 softmax。\n- 优化器：随机梯度下降，学习率为 $\\eta$，在取消梯度缩放（如果使用了任何缩放）后应用于 32 位主参数。使用恒定学习率 $\\eta = 10^{8}$。\n- 数据：构建一个确定性的两类数据集，包含 $8$ 个样本，输入尺度为 $\\sigma = 10^{-4}$。对于类别 $0$，图像中心有一个由 $\\sigma$ 缩放的 $2 \\times 2$ 的 1 值块，其余部分为零。对于类别 $1$，图像的四个边界（顶行、底行、左列、右列）由 $\\sigma$ 缩放的 1 值填充，其余部分为零。每个类别创建 $4$ 个副本来获得 $8$ 个样本。标签是 $\\mathbb{R}^{2}$ 中的独热向量。批量大小为 $1$；在每一步使用索引等于步数模 $8$ 的样本。\n- 初始化：使用固定的随机种子，以零均值和标准差 $\\tau = 10^{-3}$ 的独立高斯样本初始化所有权重，以确保确定性。\n- 混合精度模拟：对于全精度，所有计算均以 binary $32$ 进行。对于混合精度，在每一步中：\n  - 将当前的主权重转换为 binary $16$ 用于前向和后向计算；\n  - 计算损失及其关于 logits 的梯度；将损失梯度乘以损失缩放因子 $\\alpha$；在反向传播前将其转换为 binary $16$；\n  - 以 binary $16$ 计算关于转换后权重的梯度；\n  - 以 binary $32$ 将梯度除以 $\\alpha$ 来取消缩放，并对主权重应用随机梯度下降更新。\n  如果在任何时候任何中间值或参数值变为非数字 (not-a-number) 或无穷大，则将该训练运行视为失败，并将其结果准确率设为 $0$。\n- 训练长度：精确运行 $E = 20$ 个更新步骤。\n\n需要计算和报告的量：\n- 设 $A_{\\text{fp32}}$ 为全精度运行在 $E$ 步后在训练集上评估的最终 top-1 准确率（表示为 $[0,1]$ 区间内的小数），使用 binary $32$ 进行前向传播。\n- 对于每个给定 $\\alpha$ 的混合精度运行，设 $A_{\\text{mixed}}(\\alpha)$ 为混合精度训练的模型在 $E$ 步后在训练集上以 binary $32$ 评估的最终 top-1 准确率。\n- 对于每个混合精度运行，报告理论加速比 $S = s = 2.0$ 和准确率变化 $\\Delta A(\\alpha) = A_{\\text{mixed}}(\\alpha) - A_{\\text{fp32}}$。\n\n测试套件：\n- 使用以下三个损失缩放因子：\n  - $\\alpha_{1} = 1$\n  - $\\alpha_{2} = 8192$ (即 $2^{13}$)\n  - $\\alpha_{3} = 1048576$ (即 $2^{20}$)\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素对应于一个测试用例，顺序为 $\\left[\\alpha_{1}, \\alpha_{2}, \\alpha_{3}\\right]$。每个元素本身必须是一个包含两个浮点数的列表，顺序为 $\\left[S, \\Delta A(\\alpha)\\right]$。例如，打印的行必须看起来像一个列表的列表：$\\left[\\left[S_{1}, \\Delta A\\left(\\alpha_{1}\\right)\\right], \\left[S_{2}, \\Delta A\\left(\\alpha_{2}\\right)\\right], \\left[S_{3}, \\Delta A\\left(\\alpha_{3}\\right)\\right]\\right]$。不应打印任何额外的文本。",
            "solution": "用户要求对一个最小化的类 VGG 卷积神经网络进行混合精度训练的模拟。该问题是一个计算练习，要求从基本原理实现一个神经网络训练循环，特别关注半精度 (binary16) 浮点运算的数值挑战以及用于补救的损失缩放技术。\n\n### 问题验证\n\n首先，我将验证问题陈述。\n\n**第 1 步：提取给定条件**\n\n- **网络架构**：一个层序列：$1 \\times 8 \\times 8$ 输入 $\\rightarrow$ 卷积层1 ($1 \\rightarrow 2$ 通道, $3 \\times 3$ 核) $\\rightarrow$ ReLU $\\rightarrow$ 卷积层2 ($2 \\rightarrow 2$ 通道, $3 \\times 3$ 核) $\\rightarrow$ ReLU $\\rightarrow$ 全局平均池化 $\\rightarrow$ 线性层 ($2 \\rightarrow 2$)。卷积层使用步长 $1$ 和填充 $1$ 来保持空间维度。不使用偏置项。\n- **数据**：$8$ 个大小为 $1 \\times 8 \\times 8$ 的样本。类别 $0$ (4 个样本)：一个居中的 $2 \\times 2$ 的 1 值块。类别 $1$ (4 个样本)：边界上为 1。所有值都按 $\\sigma = 10^{-4}$ 缩放。标签是独热向量。\n- **初始化**：所有权重都从高斯分布 $\\mathcal{N}(0, \\tau^2)$（其中 $\\tau = 10^{-3}$）中初始化，使用固定的随机种子。\n- **训练**：批量大小为 $1$ 的随机梯度下降 (SGD)。第 $i$ 步的样本索引为 $i \\pmod 8$。训练进行 $E = 20$ 步。学习率恒定为 $\\eta = 10^8$。\n- **损失函数**：Softmax 交叉熵。\n- **精度模式**：\n    1.  **全精度 (FP32)**：所有参数和计算都使用 `binary32`。\n    2.  **混合精度 (MP)**：一份主权重副本以 `binary32` 存储。对于每个训练步骤，权重被转换为 `binary16` 用于前向和后向传播。计算损失，其关于 logits 的梯度乘以因子 $\\alpha$ 并转换为 `binary16`。权重的梯度以 `binary16` 计算，然后转换为 `binary32`，通过除以 $\\alpha$ 来取消缩放，并用于更新主权重。\n- **失败条件**：如果任何值变为 NaN 或无穷大，则认为训练运行失败，其最终准确率为 $0$。\n- **IEEE 754 `binary16`**：最小正规数约为 $6.1 \\times 10^{-5}$ ($2^{-14}$)，最小正非正规数约为 $6.0 \\times 10^{-8}$ ($2^{-24}$)。低于此值的值会下溢为零。\n- **测试用例**：用于 MP 训练的三个损失缩放因子：$\\alpha_1 = 1$, $\\alpha_2 = 2^{13}$ 和 $\\alpha_3 = 2^{20}$。\n- **度量指标**：理论加速比 $S=2.0$（给定）。准确率变化 $\\Delta A(\\alpha) = A_{\\text{mixed}}(\\alpha) - A_{\\text{fp32}}$，其中准确率是在 $20$ 步后使用 `binary32` 算术在整个训练集上评估的。\n- **输出格式**：一个列表的列表：$[[S_1, \\Delta A(\\alpha_1)], [S_2, \\Delta A(\\alpha_2)], [S_3, \\Delta A(\\alpha_3)]]$。\n\n**第 2 步：使用提取的给定条件进行验证**\n\n- **科学依据**：该问题在深度学习和计算机算术原理方面有充分的依据。混合精度训练、损失缩放以及 IEEE 754 浮点数的行为都是已确立的概念。\n- **适定性**：该问题是确定性的。固定的种子、数据和训练过程确保了单一的、可计算的解。\n- **客观性**：所有指令都是定量的和算法性的，没有主观元素。\n- **完整性**：该问题是完全指定的，为完整的实现提供了所有必要的参数和逻辑。看似极端的学习率 $\\eta = 10^8$ 是一个刻意的选择。结合小输入尺度 ($\\sigma = 10^{-4}$) 和权重初始化 ($\\tau = 10^{-3}$)，它将模拟推入一个梯度非常小的区域，使得下溢的影响变得突出，并检验缓解策略。这是一个有效的数值实验设计。\n\n**第 3 步：结论与行动**\n\n问题是**有效的**。我将继续进行解答。\n\n### 基于原理的解决方案\n\n任务是模拟和对比全精度和混合精度训练。问题的核心围绕着半精度浮点数 (`binary16`) 有限的范围和精度，以及用于对抗这些限制的损失缩放技术。\n\n**1. 混合精度训练的基本原理**\n\n训练深度神经网络的计算成本很高。使用较低精度的算术，如 `binary16` (FP16)，可以在现代硬件（如带 Tensor Cores 的 GPU）上显著加速计算并减少内存占用。问题用理想化的加速因子 $S = 2.0$ 对此进行建模。然而，FP16 减小的动态范围引入了数值挑战。\n\n**2. 梯度下溢问题**\n\nFP16 的动态范围远小于单精度 `binary32` (FP32)。梯度，特别是在深度网络或输入/权重数量级较小的情况下，可能会变得非常小。FP16 中可表示的最小正非正规数约为 $2^{-24} \\approx 5.96 \\times 10^{-8}$。任何计算出的数量级小于此值的梯度都将被刷新为零（下溢）。如果很大一部分梯度变为零，相应的权重将不会被更新，训练过程会停滞或失败。\n\n在这个问题中，输入尺度为 $\\sigma=10^{-4}$，初始权重约为 $\\tau=10^{-3}$。因此，激活值和随后的梯度很可能非常小，为基于 FP16 的后向传播中的下溢创造了一个完美的场景。\n\n**3. 作为缓解策略的损失缩放**\n\n损失缩放是一种防止梯度下溢的技术。过程如下：\n1.  前向传播后，计算损失 $\\mathcal{L}$。\n2.  在开始反向传播之前，将损失乘以一个大因子 $\\alpha$：$\\mathcal{L}_{\\text{scaled}} = \\alpha \\cdot \\mathcal{L}$。\n3.  根据链式法则，这个缩放会传播到所有梯度：$\\nabla_{\\theta} \\mathcal{L}_{\\text{scaled}} = \\alpha \\cdot \\nabla_{\\theta} \\mathcal{L}$。\n4.  这些被缩放的梯度现在具有更大的数量级，在 FP16 中计算时不太可能下溢。\n5.  后向传播完成后，计算出的梯度（约为 $\\alpha \\cdot \\nabla_{\\theta} \\mathcal{L}$）在 FP32 中应用权重更新之前，通过除以 $\\alpha$ 来取消缩放。更新规则变为：$\\theta_{t+1} = \\theta_t - \\eta \\cdot (\\frac{1}{\\alpha} (\\alpha \\cdot \\nabla_{\\theta_t} \\mathcal{L})) = \\theta_t - \\eta \\cdot \\nabla_{\\theta_t} \\mathcal{L}$。\n\n$\\alpha$ 的选择至关重要：\n- 如果 $\\alpha$ 太小，可能不足以防止下溢。\n- 如果 $\\alpha$ 太大，缩放后的梯度可能会上溢（超过 FP16 可表示的最大值 $65504$），导致 `inf` 值，从而破坏训练过程。\n\n**4. 实验设计与预测**\n\n该问题设置了一个使用三个 $\\alpha$ 值的实验来展示这种权衡。\n\n- **基线 (FP32)**：我们首先通过在全精度下训练来建立一个基线准确率 $A_{\\text{fp32}}$。这代表了目标性能。\n- **使用 $\\alpha_1 = 1$ 的混合精度**：这相当于没有损失缩放。我们预测在 FP16 后向传播中许多梯度会下溢为零。模型将学习得很差，甚至可能根本学不到。因此，我们期望 $A_{\\text{mixed}}(1)  A_{\\text{fp32}}$，导致 $\\Delta A(1)  0$。\n- **使用 $\\alpha_2 = 2^{13} = 8192$ 的混合精度**：这是一个中等的缩放因子。它应该足够大，以将梯度数量级移出下溢区域并进入 FP16 的可表示范围，而不会导致上溢。我们预测这将防止下溢，使模型能够有效训练。因此，我们期望 $A_{\\text{mixed}}(8192) \\approx A_{\\text{fp32}}$，导致 $\\Delta A(8192) \\approx 0$。\n- **使用 $\\alpha_3 = 2^{20} = 1048576$ 的混合精度**：这是一个非常大的缩放因子。初始损失梯度数量级为 $O(1)$，当乘以 $10^6$ 时，将远远超过 FP16 的最大值。这将在后向传播开始时导致上溢为 `inf`。随后的梯度计算和权重更新将传播这个 `inf`，导致训练运行失败。结果准确率将为 $0$。我们期望 $\\Delta A(2^{20}) = 0 - A_{\\text{fp32}} = -A_{\\text{fp32}}$。\n\n实现将严格遵循每个层的前向和后向传播、数据生成和训练循环的指定算法，确保所有类型转换和缩放操作都在正确的阶段执行，以忠实地模拟该过程。",
            "answer": "```python\nimport numpy as np\n\n# This program simulates mixed-precision training for a small CNN from first principles.\n# It is designed to be fully deterministic and self-contained.\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the simulation, training, and evaluation.\n    \"\"\"\n    # --- Problem Parameters ---\n    ETA = 1e8\n    SIGMA = 1e-4\n    TAU = 1e-3\n    E = 20  # Number of training steps\n    NUM_SAMPLES = 8\n    SPEEDUP_FACTOR = 2.0\n    SEED = 42\n    ALPHAS = [1.0, 8192.0, 1048576.0]\n\n    # --- Layer Implementations ---\n\n    def relu(x):\n        return np.maximum(0, x)\n\n    def relu_grad(x):\n        return (x > 0).astype(x.dtype)\n\n    def convolve_forward(x_in, W, stride=1, padding=1):\n        \"\"\"A simplified 2D convolution forward pass.\"\"\"\n        C_out, C_in, K, _ = W.shape\n        _, H_in, W_in = x_in.shape\n        \n        H_out = (H_in - K + 2 * padding) // stride + 1\n        W_out = (W_in - K + 2 * padding) // stride + 1\n\n        x_pad = np.pad(x_in, ((0, 0), (padding, padding), (padding, padding)), mode='constant')\n        out = np.zeros((C_out, H_out, W_out), dtype=x_in.dtype)\n\n        for c_o in range(C_out):\n            for h in range(H_out):\n                for w in range(W_out):\n                    h_start, w_start = h * stride, w * stride\n                    region = x_pad[:, h_start:h_start + K, w_start:w_start + K]\n                    out[c_o, h, w] = np.sum(region * W[c_o, :, :, :])\n        return out\n\n    def convolve_backward(d_out, x_in, W, stride=1, padding=1):\n        \"\"\"A simplified 2D convolution backward pass.\"\"\"\n        C_out, C_in, K, _ = W.shape\n        _, H_in, W_in = x_in.shape\n        \n        x_pad = np.pad(x_in, ((0, 0), (padding, padding), (padding, padding)), mode='constant')\n        d_x = np.zeros_like(x_pad, dtype=x_in.dtype)\n        d_W = np.zeros_like(W, dtype=x_in.dtype)\n        \n        H_out = (H_in - K + 2 * padding) // stride + 1\n        W_out = (W_in - K + 2 * padding) // stride + 1\n\n        for c_o in range(C_out):\n            for h in range(H_out):\n                for w in range(W_out):\n                    h_start, w_start = h * stride, w * stride\n                    region = x_pad[:, h_start:h_start + K, w_start:w_start + K]\n                    \n                    d_W[c_o, :, :, :] += d_out[c_o, h, w] * region\n                    d_x[:, h_start:h_start + K, w_start:w_start + K] += d_out[c_o, h, w] * W[c_o, :, :, :]\n        \n        if padding > 0:\n            d_x = d_x[:, padding:-padding, padding:-padding]\n        \n        return d_x, d_W\n\n    def global_avg_pool_forward(x):\n        return np.mean(x, axis=(1, 2))\n\n    def global_avg_pool_backward(d_out, x_shape):\n        C, H, W = x_shape\n        return d_out[:, np.newaxis, np.newaxis] / (H * W) * np.ones(x_shape, dtype=d_out.dtype)\n    \n    def stable_softmax_cross_entropy(logits, y_true):\n        # Logits and y_true are 1D arrays for a single sample.\n        stable_logits = logits - np.max(logits)\n        exp_logits = np.exp(stable_logits)\n        probs = exp_logits / np.sum(exp_logits)\n        loss = -np.sum(y_true * np.log(probs))\n        d_logits = probs - y_true\n        return loss, d_logits\n\n    # --- Data Generation ---\n    def generate_data():\n        X = np.zeros((NUM_SAMPLES, 1, 8, 8), dtype=np.float32)\n        Y = np.zeros((NUM_SAMPLES, 2), dtype=np.float32)\n        \n        # Class 0: centered 2x2 block\n        img0 = np.zeros((8, 8), dtype=np.float32)\n        img0[3:5, 3:5] = SIGMA\n        \n        # Class 1: border\n        img1 = np.zeros((8, 8), dtype=np.float32)\n        img1[[0, 7], :] = SIGMA\n        img1[:, [0, 7]] = SIGMA\n\n        for i in range(NUM_SAMPLES):\n            if i  4:\n                X[i, 0, :, :] = img0\n                Y[i, :] = [1, 0]\n            else:\n                X[i, 0, :, :] = img1\n                Y[i, :] = [0, 1]\n        return X, Y\n\n    X_train, Y_train = generate_data()\n\n    # --- Model Initialization ---\n    def init_weights(seed):\n        rng = np.random.default_rng(seed)\n        weights = {\n            'W1': rng.normal(0, TAU, (2, 1, 3, 3)).astype(np.float32),\n            'W2': rng.normal(0, TAU, (2, 2, 3, 3)).astype(np.float32),\n            'W_lin': rng.normal(0, TAU, (2, 2)).astype(np.float32),\n        }\n        return weights\n\n    # --- Training and Evaluation ---\n    def train(X_data, Y_data, mode, alpha, seed):\n        weights = init_weights(seed)\n        fp_master_weights = {k: v.copy() for k, v in weights.items()}\n\n        for step in range(E):\n            idx = step % NUM_SAMPLES\n            x, y_true = X_data[idx], Y_data[idx]\n\n            # Set precision for this run\n            compute_dtype = np.float16 if mode == 'mixed' else np.float32\n            \n            # --- FORWARD PASS ---\n            W1 = fp_master_weights['W1'].astype(compute_dtype)\n            W2 = fp_master_weights['W2'].astype(compute_dtype)\n            W_lin = fp_master_weights['W_lin'].astype(compute_dtype)\n            x_compute = x.astype(compute_dtype)\n\n            z1 = convolve_forward(x_compute, W1)\n            a1 = relu(z1)\n            z2 = convolve_forward(a1, W2)\n            a2 = relu(z2)\n            p = global_avg_pool_forward(a2)\n            logits = p @ W_lin\n\n            # --- LOSS  BACKWARD PASS ---\n            # Loss computed in FP32 for stability\n            _, d_logits_fp32 = stable_softmax_cross_entropy(logits.astype(np.float32), y_true)\n\n            if mode == 'mixed':\n                d_logits = (d_logits_fp32 * alpha).astype(compute_dtype)\n            else:\n                d_logits = d_logits_fp32.astype(compute_dtype)\n            \n            d_p = d_logits @ W_lin.T\n            d_W_lin = np.outer(p, d_logits)\n            \n            d_a2 = global_avg_pool_backward(d_p, a2.shape)\n            d_z2 = d_a2 * relu_grad(z2)\n            d_a1, d_W2 = convolve_backward(d_z2, a1, W2)\n            \n            d_z1 = d_a1 * relu_grad(z1)\n            _, d_W1 = convolve_backward(d_z1, x_compute, W1)\n            \n            # --- WEIGHT UPDATE ---\n            grads = {\n                'W1': d_W1.astype(np.float32),\n                'W2': d_W2.astype(np.float32),\n                'W_lin': d_W_lin.astype(np.float32),\n            }\n\n            if mode == 'mixed':\n                for k in grads:\n                    grads[k] /= alpha\n\n            for k in fp_master_weights:\n                fp_master_weights[k] -= ETA * grads[k]\n            \n            # Check for failure\n            is_invalid = any(np.any(np.isinf(w)) or np.any(np.isnan(w)) for w in fp_master_weights.values())\n            if is_invalid:\n                return None  # Failed run\n\n        return fp_master_weights\n\n    def evaluate(X_data, Y_data, weights):\n        correct_predictions = 0\n        for i in range(NUM_SAMPLES):\n            x = X_data[i].astype(np.float32)\n            \n            z1 = convolve_forward(x, weights['W1'])\n            a1 = relu(z1)\n            z2 = convolve_forward(a1, weights['W2'])\n            a2 = relu(z2)\n            p = global_avg_pool_forward(a2)\n            logits = p @ weights['W_lin']\n            \n            predicted_class = np.argmax(logits)\n            true_class = np.argmax(Y_data[i])\n            \n            if predicted_class == true_class:\n                correct_predictions += 1\n        \n        return correct_predictions / NUM_SAMPLES\n\n    # --- Main Execution Logic ---\n    \n    # FP32 baseline run\n    final_weights_fp32 = train(X_train, Y_train, mode='fp32', alpha=1.0, seed=SEED)\n    acc_fp32 = evaluate(X_train, Y_train, final_weights_fp32)\n\n    results = []\n    for alpha in ALPHAS:\n        final_weights_mixed = train(X_train, Y_train, mode='mixed', alpha=alpha, seed=SEED)\n        \n        if final_weights_mixed is None:\n            acc_mixed = 0.0\n        else:\n            acc_mixed = evaluate(X_train, Y_train, final_weights_mixed)\n            \n        delta_A = acc_mixed - acc_fp32\n        results.append([SPEEDUP_FACTOR, delta_A])\n    \n    # Format and print the final output\n    formatted_results = \",\".join([f\"[{s},{da}]\" for s, da in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n\n```"
        }
    ]
}