{
    "hands_on_practices": [
        {
            "introduction": "我们的第一个动手实践是一个基础练习，它将参数绑定的概念应用于一维滤波器的设计中。我们将利用参数绑定来强制滤波器具备特定的对称性（奇对称或偶对称），并观察这些滤波器如何演变为专门的特征检测器。这个练习旨在揭示参数共享如何将先验知识（如对称性）编码到模型结构中，从而创建出像边缘检测器或斑点检测器这样具有特定功能的组件，这是理解卷积神经网络背后核心思想的关键一步。",
            "id": "3161902",
            "problem": "您将为一维离散滤波器实现参数绑定，以强制奇对称和偶对称，通过线性最小二乘法在小型合成数据集上训练这些绑定的滤波器，并评估它们在边缘和斑点模式上的行为。该任务纯粹以数学术语来表述。\n\n使用的基本基础和定义：\n- 长度为 $L$ 的离散线性滤波器表示为向量 $K \\in \\mathbb{R}^{L}$。$K$ 对离散窗口 $s \\in \\mathbb{R}^{L}$ 的响应是内积 $r = \\sum_{j=0}^{L-1} K[j]\\, s[j]$。\n- 对于奇数长度的核，定义一个中心化索引集 $\\{i\\}_{i=-c}^{c}$，其中 $c = \\frac{L-1}{2}$，索引 $i$ 与数组位置 $j$ 之间的映射为 $j = i + c$。\n- 奇对称约束：对所有 $i$，$K[i] = -K[-i]$，这意味着 $K[0] = 0$。\n- 偶对称约束：对所有 $i$，$K[i] = K[-i]$，其中 $K[0]$ 是一个无约束的参数。\n- 参数绑定将 $K$ 表示为 $K = T \\theta$，其中 $T \\in \\mathbb{R}^{L \\times d}$ 是一个编码对称性的固定矩阵，$\\theta \\in \\mathbb{R}^{d}$ 是自由参数的向量。\n- 本问题中使用的离散窗口在中心化索引集 $i \\in \\{-c,\\ldots,0,\\ldots,c\\}$ 上定义如下：\n  1. 幅度为 $a$ 的上升阶跃：如果 $i  0$，则 $s_{\\text{rise}}(i;a) = 0$；如果 $i \\ge 0$，则 $s_{\\text{rise}}(i;a) = a$。\n  2. 幅度为 $a$ 的下降阶跃：如果 $i  0$，则 $s_{\\text{fall}}(i;a) = a$；如果 $i \\ge 0$，则 $s_{\\text{fall}}(i;a) = 0$。\n  3. 幅度为 $a$、宽度为 $\\sigma$ 的高斯斑点：$s_{\\text{blob}}(i;a,\\sigma) = a \\exp\\!\\big(-\\frac{i^2}{2 \\sigma^2}\\big)$。\n  4. 常数窗：对所有 $i$，$s_{\\text{const}}(i) = 1$。\n\n程序要求：\n1. 使用核长度 $L = 9$（因此 $c = 4$）。构造两个绑定矩阵：\n   - $T_{\\text{odd}} \\in \\mathbb{R}^{L \\times d_{\\text{odd}}}$，通过配对索引 $i$ 和 $-i$（其中 $i \\in \\{1,2,3,4\\}$）并强制 $K[0] = 0$ 来实现 $K[i] = -K[-i]$。每个自由参数控制一对，在 $i$ 处权重为 $+1$，在 $-i$ 处权重为 $-1$。这得到 $d_{\\text{odd}} = 4$。\n   - $T_{\\text{even}} \\in \\mathbb{R}^{L \\times d_{\\text{even}}}$，通过配对索引 $i$ 和 $-i$（其中 $i \\in \\{1,2,3,4\\}$），外加一个用于中心的自由参数，来实现 $K[i] = K[-i]$。每对在 $i$ 和 $-i$ 处权重均为 $+1$，中心列在 $i=0$ 处权重为 $+1$。这得到 $d_{\\text{even}} = 5$。\n2. 通过求解带绑定的线性最小二乘问题来训练一个奇对称滤波器。构造一个设计矩阵 $X_{\\text{odd}} \\in \\mathbb{R}^{N_{\\text{odd}} \\times L}$，其行是训练窗口，以及一个目标向量 $y_{\\text{odd}} \\in \\mathbb{R}^{N_{\\text{odd}}}$。最小化 $\\|X_{\\text{odd}} T_{\\text{odd}} \\theta_{\\text{odd}} - y_{\\text{odd}}\\|_2^2$ 以获得 $\\theta_{\\text{odd}}$ 和 $K_{\\text{odd}} = T_{\\text{odd}} \\theta_{\\text{odd}}$。使用以下训练集：\n   - 幅度为 $a \\in \\{1.0, 0.5\\}$ 的上升阶跃，标签为 $+a$。\n   - 幅度为 $a \\in \\{1.0, 0.5\\}$ 的下降阶跃，标签为 $-a$。\n   因此 $N_{\\text{odd}} = 4$。\n3. 类似地，通过最小化 $\\|X_{\\text{even}} T_{\\text{even}} \\theta_{\\text{even}} - y_{\\text{even}}\\|_2^2$ 来训练一个偶对称滤波器，以获得 $K_{\\text{even}}$。使用以下高斯斑点训练集：\n   - 宽度 $\\sigma \\in \\{0.8, 1.2, 1.8\\}$ 和幅度 $a \\in \\{1.0, -0.8\\}$（所有组合），标签等于 $a$。因此 $N_{\\text{even}} = 6$。\n4. 训练后，将每个核归一化为单位欧几里得范数：对于 $K_{\\text{odd}}$ 和 $K_{\\text{even}}$，都用 $K / \\|K\\|_2$ 替换 $K$。\n5. 构建以下测试窗口（长度均为 $L = 9$）：\n   - 幅度为 $1.0$ 的上升阶跃。\n   - 幅度为 $1.0$、$\\sigma = 1.2$ 的高斯斑点。\n   - 常数窗。\n   - 幅度为 $1.0$ 的下降阶跃。\n6. 计算以下量：\n   - 自由参数的数量 $d_{\\text{odd}}$ 和 $d_{\\text{even}}$。\n   - 布尔值 $b_1$：奇对称滤波器在上升阶跃上的绝对响应幅度是否严格大于偶对称滤波器在同一上升阶跃上的绝对响应幅度。\n   - 布尔值 $b_2$：偶对称滤波器在高斯斑点上的绝对响应幅度是否严格大于奇对称滤波器在同一斑点上的绝对响应幅度。\n   - 浮点数 $f$：奇对称滤波器在常数窗上的响应，四舍五入到绝对容差 $\\varepsilon = 10^{-12}$ 内（如果幅度小于 $\\varepsilon$，则输出恰好为 $0.0$）。\n   - 布尔值 $b_3$：奇对称滤波器在单位幅度上升阶跃和下降阶跃上的响应是否在相同的绝对容差 $\\varepsilon = 10^{-12}$ 内互为相反数。\n7. 最终输出格式：您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按以下确切顺序排列：\n   $[d_{\\text{odd}}, d_{\\text{even}}, b_1, b_2, f, b_3]$，\n   其中 $d_{\\text{odd}}$ 和 $d_{\\text{even}}$ 是整数，$b_1$、$b_2$ 和 $b_3$ 是布尔值，$f$ 是浮点数。\n\n本问题中的所有数学实体和数字均以其在离散线性代数和初等函数中的标准含义定义。不涉及物理单位，也不使用角度。答案值完全由上述定义确定，因此不需要用户输入。",
            "solution": "我们从离散线性系统、最小二乘法和参数绑定的核心定义开始。长度为 $L$ 的滤波器 $K \\in \\mathbb{R}^{L}$ 在窗口 $s \\in \\mathbb{R}^{L}$ 上产生响应 $r = \\sum_{j=0}^{L-1} K[j] s[j] = s^{\\top} K$。对于奇数长度 $L = 9$，我们将索引中心化在 $i \\in \\{-4,-3,-2,-1,0,1,2,3,4\\}$，数组位置为 $j = i + 4$。\n\n参数绑定将对称性约束表示为线性映射。对于奇对称性，$K[i] = -K[-i]$ 对所有 $i$ 成立，因此 $K[0] = 0$。自由参数对应于 $i \\in \\{1,2,3,4\\}$，完整的核是在 $i$ 处放置 $+1$ 和在 $-i$ 处放置 $-1$ 的列的线性组合。这定义了一个矩阵 $T_{\\text{odd}} \\in \\mathbb{R}^{9 \\times 4}$，使得 $K_{\\text{odd}} = T_{\\text{odd}} \\theta_{\\text{odd}}$。对于偶对称性，$K[i] = K[-i]$ 对所有 $i$ 成立，且 $K[0]$ 是自由的，所以自由参数是中心点和 4 个索引对，总共 $d_{\\text{even}} = 5$ 个。每个对的列在 $i$ 和 $-i$ 处都有 $+1$，中心列在 $i=0$ 处有 $+1$，构成 $T_{\\text{even}} \\in \\mathbb{R}^{9 \\times 5}$。\n\n绑定下的训练使用最小二乘法。对于一组组装成设计矩阵 $X \\in \\mathbb{R}^{N \\times L}$ 的训练窗口和目标向量 $y \\in \\mathbb{R}^{N}$，绑定的模型预测为 $X K = X T \\theta$。最优参数最小化二次目标函数\n$$\n\\min_{\\theta \\in \\mathbb{R}^{d}} \\| X T \\theta - y \\|_2^2.\n$$\n这是一个标准的线性最小二乘问题，当 $T^{\\top} X^{\\top} X T$ 可逆时，其解由正规方程给出\n$$\n(T^{\\top} X^{\\top} X T)\\, \\theta^{\\star} = T^{\\top} X^{\\top} y,\n$$\n否则，最小范数解由 Moore–Penrose 伪逆给出。在计算上，这可以通过将最小二乘例程应用于 $X T$ 来稳定地求解。\n\n我们现在确定性地指定训练集：\n\n1. 奇对称滤波器训练集 ($N_{\\text{odd}} = 4$)。对于幅度 $a \\in \\{1.0, 0.5\\}$：\n   - 包括上升阶跃 $s_{\\text{rise}}(i;a)$，标签为 $+a$。\n   - 包括下降阶跃 $s_{\\text{fall}}(i;a)$，标签为 $-a$。\n   目标函数倾向于响应 $r = s^{\\top} K_{\\text{odd}}$ 与边缘的带符号幅度相匹配。由于奇对称性和阶跃结构，任何满足 $\\sum_{i>0} K_{\\text{odd}}[i] = 1$ 的核 $K_{\\text{odd}}$ 都会使上升和下降约束一致，并且最小范数奇对称解会将权重均匀分布在正索引上，负权重分布在负索引上，中心为零。\n\n2. 偶对称滤波器训练集 ($N_{\\text{even}} = 6$)。对于宽度 $\\sigma \\in \\{0.8, 1.2, 1.8\\}$ 和幅度 $a \\in \\{1.0, -0.8\\}$：\n   - 包括高斯斑点 $s_{\\text{blob}}(i; a, \\sigma)$，标签为 $a$。\n   通过对称性，偶对称核将与斑点结构对齐，并学习到一个类似于对称平滑剖面的形状。\n\n在获得 $K_{\\text{odd}}$ 和 $K_{\\text{even}}$ 后，将它们归一化为单位欧几里得范数：\n$$\nK \\leftarrow \\frac{K}{\\|K\\|_2}.\n$$\n\n我们在以下测试窗口上进行评估：\n- 幅度为 $1.0$ 的上升阶跃。\n- 幅度为 $1.0$、$\\sigma = 1.2$ 的高斯斑点。\n- 全为 1 的常数窗。\n- 幅度为 $1.0$ 的下降阶跃。\n\n关键分析性质：\n- 对于任何满足 $K_{\\text{odd}}[0] = 0$ 的奇对称核 $K_{\\text{odd}}$，其在常数窗上的响应恰好为零。这是因为对于每个 $i>0$，对的贡献为 $K_{\\text{odd}}[i] \\cdot 1 + K_{\\text{odd}}[-i] \\cdot 1 = K_{\\text{odd}}[i] + K_{\\text{odd}}[-i] = 0$，中心的贡献为 $0$。\n- 对于奇对称核 $K_{\\text{odd}}$，其在单位幅度上升阶跃和下降阶跃上的响应互为相反数。确实，上升阶跃选出 $\\sum_{i \\ge 0} K_{\\text{odd}}[i] = \\sum_{i>0} K_{\\text{odd}}[i]$，而下降阶跃选出 $\\sum_{i  0} K_{\\text{odd}}[i] = -\\sum_{i>0} K_{\\text{odd}}[i]$。\n- 对于在斑点上训练的偶对称核，其在对称斑点上的响应通常幅度很大，而奇对称核在任何以原点为中心的对称斑点上的响应恰好为零，因为对于每个 $i>0$，$K_{\\text{odd}}[i] s[i] + K_{\\text{odd}}[-i] s[-i] = s[i]\\,(K_{\\text{odd}}[i] + K_{\\text{odd}}[-i]) = 0$ 且 $K_{\\text{odd}}[0] s[0] = 0$。\n\n这些事实，结合单位范数归一化，支持了待测试的不等式：\n- 奇对称滤波器对阶跃边缘的响应比在斑点上训练的偶对称滤波器更强，因此在上升阶跃上的绝对响应比较应该为真。\n- 偶对称滤波器对高斯斑点的响应比奇对称滤波器更强，后者在中心对称斑点上的响应恰好为零，因此在斑点上的绝对响应比较应该为真。\n- 奇对称滤波器在常数窗上的响应为零，我们将其作为在绝对容差 $\\varepsilon = 10^{-12}$ 内四舍五入的浮点数输出。\n- 奇对称滤波器在上升和下降阶跃上的响应是相反数，我们在相同容差内进行测试。\n\n最后，我们报告自由参数的数量 $d_{\\text{odd}} = 4$ 和 $d_{\\text{even}} = 5$，比较响应幅度的两个布尔值，常数窗上的浮点数响应，以及符号反转属性的布尔值，顺序如下\n$$\n[d_{\\text{odd}}, d_{\\text{even}}, b_1, b_2, f, b_3].\n$$\n这将生成一行输出，按要求汇总所有测试结果。",
            "answer": "```python\nimport numpy as np\n\ndef build_tying_matrices(L: int):\n    \"\"\"\n    Build tying matrices for odd and even symmetry for a 1D kernel of length L (odd).\n    Returns:\n        T_odd: shape (L, d_odd)\n        T_even: shape (L, d_even)\n    \"\"\"\n    assert L % 2 == 1, \"L must be odd\"\n    c = L // 2\n    # Odd symmetry: K[i] = -K[-i], K[0] = 0. Free params for i=1..c.\n    odd_cols = []\n    for i in range(1, c + 1):\n        col = np.zeros(L, dtype=float)\n        col[c + i] = 1.0   # +1 at +i\n        col[c - i] = -1.0  # -1 at -i\n        odd_cols.append(col)\n    T_odd = np.column_stack(odd_cols) if odd_cols else np.zeros((L, 0))\n\n    # Even symmetry: K[i] = K[-i], center is free. Free params: center + i=1..c.\n    even_cols = []\n    # Center parameter\n    col0 = np.zeros(L, dtype=float)\n    col0[c] = 1.0\n    even_cols.append(col0)\n    for i in range(1, c + 1):\n        col = np.zeros(L, dtype=float)\n        col[c + i] = 1.0  # +1 at +i\n        col[c - i] = 1.0  # +1 at -i\n        even_cols.append(col)\n    T_even = np.column_stack(even_cols)\n    return T_odd, T_even\n\ndef make_indices(L: int):\n    c = L // 2\n    return np.arange(-c, c + 1, dtype=float)\n\ndef rising_step_window(L: int, a: float):\n    i = make_indices(L)\n    w = np.where(i  0, 0.0, a)\n    return w\n\ndef falling_step_window(L: int, a: float):\n    i = make_indices(L)\n    w = np.where(i  0, a, 0.0)\n    return w\n\ndef gaussian_blob_window(L: int, a: float, sigma: float):\n    i = make_indices(L)\n    w = a * np.exp(-(i ** 2) / (2.0 * (sigma ** 2)))\n    return w\n\ndef constant_window(L: int):\n    return np.ones(L, dtype=float)\n\ndef fit_tied_least_squares(T: np.ndarray, windows: list, targets: list):\n    \"\"\"\n    Solve min_theta || X T theta - y ||^2\n    Returns K = T theta_hat\n    \"\"\"\n    X = np.vstack(windows)  # shape (N, L)\n    y = np.asarray(targets, dtype=float)  # shape (N,)\n    XR = X @ T  # Reduced design matrix, shape (N, d)\n    # Solve least squares for theta\n    theta_hat, *_ = np.linalg.lstsq(XR, y, rcond=None)\n    K = T @ theta_hat\n    return K\n\ndef normalize_kernel(K: np.ndarray):\n    norm = np.linalg.norm(K)\n    if norm == 0.0:\n        return K.copy()\n    return K / norm\n\ndef solve():\n    L = 9\n    eps = 1e-12\n\n    # Build tying matrices\n    T_odd, T_even = build_tying_matrices(L)\n    d_odd = T_odd.shape[1]\n    d_even = T_even.shape[1]\n\n    # Training data for odd-symmetric filter (edges)\n    windows_odd = []\n    targets_odd = []\n    for a in [1.0, 0.5]:\n        windows_odd.append(rising_step_window(L, a))\n        targets_odd.append(+a)\n        windows_odd.append(falling_step_window(L, a))\n        targets_odd.append(-a)\n\n    # Training data for even-symmetric filter (blobs)\n    windows_even = []\n    targets_even = []\n    for sigma in [0.8, 1.2, 1.8]:\n        for a in [1.0, -0.8]:\n            windows_even.append(gaussian_blob_window(L, a, sigma))\n            targets_even.append(a)\n\n    # Fit tied least squares\n    K_odd = fit_tied_least_squares(T_odd, windows_odd, targets_odd)\n    K_even = fit_tied_least_squares(T_even, windows_even, targets_even)\n\n    # Normalize kernels\n    K_odd = normalize_kernel(K_odd)\n    K_even = normalize_kernel(K_even)\n\n    # Test windows\n    w_edge_rise = rising_step_window(L, 1.0)\n    w_edge_fall = falling_step_window(L, 1.0)\n    w_blob = gaussian_blob_window(L, 1.0, 1.2)\n    w_const = constant_window(L)\n\n    # Responses\n    r_odd_edge = float(w_edge_rise @ K_odd)\n    r_even_edge = float(w_edge_rise @ K_even)\n    r_even_blob = float(w_blob @ K_even)\n    r_odd_blob = float(w_blob @ K_odd)\n    r_odd_const = float(w_const @ K_odd)\n    r_odd_fall = float(w_edge_fall @ K_odd)\n\n    # Tests\n    b1 = abs(r_odd_edge) > abs(r_even_edge)\n    b2 = abs(r_even_blob) > abs(r_odd_blob)\n    f = 0.0 if abs(r_odd_const)  eps else float(r_odd_const)\n    b3 = abs(r_odd_edge + r_odd_fall) = eps\n\n    results = [d_odd, d_even, b1, b2, f, b3]\n\n    # Final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "在掌握了滤波器设计的基础之上，下一个练习将参数绑定应用于一个简单但完整的神经网络中。我们的目标是通过实验验证，在网络架构的对称部分之间绑定参数，可以直接使其对特定的输入变换（例如坐标交换）产生不变性。这个实践将参数绑定的抽象优势转化为一个具体、可衡量的模型属性，深刻揭示了架构对称性与模型函数不变性之间的内在联系。",
            "id": "3161977",
            "problem": "考虑一个二元分类器，它被实现为一个具有单一隐藏层的前馈神经网络，作用于一个二维输入向量 $x = (x_1, x_2)$。隐藏层使用修正线性单元 (ReLU) 作为激活函数，其中修正线性单元 (ReLU) 定义为 $\\mathrm{ReLU}(z) = \\max(0, z)$。输出层是隐藏层激活值的标量仿射组合，最终的类别标签通过在 $0$ 处设定阈值来确定：如果标量输出大于或等于 $0$，则预测标签为 $1$，否则为 $0$。\n\n您将实现该分类器的两种变体：\n\n- 一个非共享参数模型，其隐藏单元为\n  $$h_1 = \\mathrm{ReLU}(a_1 x_1 + b_1), \\quad h_2 = \\mathrm{ReLU}(a_2 x_2 + b_2),$$\n  输出为\n  $$y = v_1 h_1 + v_2 h_2 + c.$$\n\n- 一个共享参数模型，其隐藏单元为\n  $$h_1 = \\mathrm{ReLU}(a x_1 + b), \\quad h_2 = \\mathrm{ReLU}(a x_2 + b),$$\n  输出为\n  $$y = v (h_1 + h_2) + c.$$\n\n这里的参数共享意味着在结构对称的组件之间设置相等的参数，例如 $a_1 = a_2$，$b_1 = b_2$ 和 $v_1 = v_2$。\n\n我们如下定义在输入空间的变换 $T$ 下的不变性：对于一个将输入映射到标签的模型 $f$，如果在集合 $X$ 上对于所有 $x \\in X$ 都有 $f(x) = f(T(x))$，则称 $f$ 在 $T$ 下是不变的。需要测试的两种变换是：\n- 坐标交换变换 $S$，定义为 $S(x_1, x_2) = (x_2, x_1)$。\n- 符号翻转变换 $F$，定义为 $F(x_1, x_2) = (-x_1, -x_2)$。\n\n从前馈网络计算仿射映射和逐点非线性的复合这一基本原理以及上述不变性的定义出发，构建一个程序，该程序：\n- 实现非共享参数模型和共享参数模型。\n- 通过从区间 $[-1, 1]$ 上的均匀分布中独立采样每个坐标来生成合成测试输入 $x$。\n- 对于给定的变换 $T \\in \\{S, F\\}$，通过检查在采样集上是否所有预测标签都满足 $f(x) = f(T(x))$ 来经验性地验证不变性。\n\n您的程序必须使用以下测试套件，其中 $N$ 是样本数量，“seed”指定伪随机数生成器的种子：\n\n- 测试用例 1（非共享，预期在 $S$ 下不具有不变性）：参数 $a_1 = 1.0$, $b_1 = 0.1$, $a_2 = 2.0$, $b_2 = -0.3$, $v_1 = 1.0$, $v_2 = -0.5$, $c = 0.05$，变换 $T = S$， $N = 200$，seed $= 42$。\n- 测试用例 2（共享，预期在 $S$ 下具有不变性）：参数 $a = 1.5$, $b = -0.2$, $v = 0.7$, $c = -0.1$，变换 $T = S$， $N = 200$，seed $= 42$。\n- 测试用例 3（非共享但参数相等，预期在 $S$ 下具有不变性）：参数 $a_1 = 0.8$, $b_1 = 0.0$, $a_2 = 0.8$, $b_2 = 0.0$, $v_1 = 1.0$, $v_2 = 1.0$, $c = 0.0$，变换 $T = S$， $N = 200$，seed $= 7$。\n- 测试用例 4（共享，预期在 $F$ 下不具有不变性）：参数 $a = 1.0$, $b = 0.0$, $v = 0.5$, $c = 0.05$，变换 $T = F$, $N = 200$，seed $= 42$。\n- 测试用例 5（共享且输出权重为零，预期因平凡恒定而在 $S$ 下具有不变性）：参数 $a = 1.0$, $b = 0.5$, $v = 0.0$, $c = 0.3$，变换 $T = S$, $N = 200$，seed $= 123$。\n\n对于每个测试用例，计算一个布尔值，指示模型在采样集上对于指定的变换是否具有不变性。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表（例如，$[result_1,result_2,\\dots]$），其中每个 $result_i$ 是按顺序排列的第 $i$ 个测试用例的布尔值。",
            "solution": "该问题要求对一个简单神经网络中参数共享与其对特定输入变换的不变性之间的关系进行分析和经验性验证。我们将首先为预期行为建立数学基础，然后使用此框架来验证指定测试用例的结果。\n\n一个二元分类器由一个函数 $f(x)$ 定义，该函数将输入向量 $x$ 映射到 $\\{0, 1\\}$ 中的一个标签。如果在给定域中对于所有输入 $x$ 都有 $f(x) = f(T(x))$，则称该分类器在变换 $T$ 下是不变的。分类规则由标量输出函数 $y$ 的符号给出，即如果 $y \\ge 0$ 则标签为 $1$，否则为 $0$。因此，不变性 $f(x) = f(T(x))$ 成立的充要条件是 $y(x)$ 的符号与 $y(T(x))$ 的符号相同，即 $(\\,y(x) \\ge 0 \\land y(T(x)) \\ge 0\\,) \\lor (\\,y(x)  0 \\land y(T(x))  0\\,)$。对此，一个充分但不必要的条件是 $y(x) = y(T(x))$。\n\n输入向量为 $x = (x_1, x_2)$。激活函数是修正线性单元，$\\mathrm{ReLU}(z) = \\max(0, z)$。\n\n两种模型是：\n1.  **非共享模型**：隐藏单元为 $h_1 = \\mathrm{ReLU}(a_1 x_1 + b_1)$ 和 $h_2 = \\mathrm{ReLU}(a_2 x_2 + b_2)$。标量输出为 $y_{untied}(x_1, x_2) = v_1 h_1 + v_2 h_2 + c$。该结构是非对称的，用不同的参数处理每个输入坐标。\n\n2.  **共享模型**：隐藏单元为 $h_1 = \\mathrm{ReLU}(a x_1 + b)$ 和 $h_2 = \\mathrm{ReLU}(a x_2 + b)$。标量输出为 $y_{tied}(x_1, x_2) = v (h_1 + h_2) + c$。用于隐藏层的参数 $(a, b)$ 和用于输出层的权重 $v$ 在两个输入分支间是共享（或“绑定”）的。\n\n两种变换是：\n1.  **坐标交换 ($S$)**：$S(x_1, x_2) = (x_2, x_1)$。\n2.  **符号翻转 ($F$)**：$F(x_1, x_2) = (-x_1, -x_2)$。\n\n**坐标交换 ($S$) 下的不变性分析**\n\n让我们分析每个模型对于交换后输入 $S(x) = (x_2, x_1)$ 的输出。\n\n*   **共享模型**：\n    原始输入的输出为：\n    $$y_{tied}(x_1, x_2) = v (\\mathrm{ReLU}(a x_1 + b) + \\mathrm{ReLU}(a x_2 + b)) + c$$\n    交换后输入的输出为：\n    $$y_{tied}(x_2, x_1) = v (\\mathrm{ReLU}(a x_2 + b) + \\mathrm{ReLU}(a x_1 + b)) + c$$\n    由于加法的交换律，$\\mathrm{ReLU}(a x_1 + b) + \\mathrm{ReLU}(a x_2 + b) = \\mathrm{ReLU}(a x_2 + b) + \\mathrm{ReLU}(a x_1 + b)$。因此，对于所有 $x$，$y_{tied}(x_1, x_2) = y_{tied}(x_2, x_1)$。这意味着预测的标签也将相同，$f_{tied}(x) = f_{tied}(S(x))$。共享参数模型架构内在地强制实现了对坐标交换变换的不变性。\n\n*   **非共享模型**：\n    原始输入的输出为：\n    $$y_{untied}(x_1, x_2) = v_1 \\mathrm{ReLU}(a_1 x_1 + b_1) + v_2 \\mathrm{ReLU}(a_2 x_2 + b_2) + c$$\n    交换后输入的输出为：\n    $$y_{untied}(x_2, x_1) = v_1 \\mathrm{ReLU}(a_1 x_2 + b_1) + v_2 \\mathrm{ReLU}(a_2 x_1 + b_2) + c$$\n    一般来说，$y_{untied}(x_1, x_2) \\neq y_{untied}(x_2, x_1)$，除非参数具有特定的对称性。如果函数关于交换 $x_1$ 和 $x_2$ 是对称的，则可以实现不变性。这种情况发生在项可以被置换时，这要求 $a_1=a_2$, $b_1=b_2$ 和 $v_1=v_2$。这些正是参数共享所施加的约束。\n\n**符号翻转 ($F$) 下的不变性分析**\n\n让我们分析共享模型对于符号翻转后输入 $F(x) = (-x_1, -x_2)$ 的输出。\n原始输入的输出为：\n$$y_{tied}(x_1, x_2) = v (\\mathrm{ReLU}(a x_1 + b) + \\mathrm{ReLU}(a x_2 + b)) + c$$\n翻转后输入的输出为：\n$$y_{tied}(-x_1, -x_2) = v (\\mathrm{ReLU}(a(-x_1) + b) + \\mathrm{ReLU}(a(-x_2) + b)) + c = v (\\mathrm{ReLU}(-a x_1 + b) + \\mathrm{ReLU}(-a x_2 + b)) + c$$\n为了不变性，我们需要 $y_{tied}(x_1, x_2)$ 和 $y_{tied}(-x_1, -x_2)$ 具有相同的符号。一般来说，这不成立。$\\mathrm{ReLU}$ 函数不是偶函数，即除了 $z=0$ 外，$\\mathrm{ReLU}(z) \\neq \\mathrm{ReLU}(-z)$。因此，共享参数架构并非内在地强制实现对符号翻转变换的不变性。\n\n**通过测试用例进行经验性验证**\n\n我们现在将此推理应用于具体的测试用例。对于每个案例，我们生成 $N=200$ 个样本 $x=(x_1, x_2)$，其坐标从 $U[-1, 1]$ 中抽取，并检查是否对所有样本都满足 $f(x) = f(T(x))$。\n\n*   **测试用例 1**：具有非对称参数（$a_1=1.0, b_1=0.1, a_2=2.0, b_2=-0.3, v_1=1.0, v_2=-0.5, c=0.05$）的非共享模型，在变换 $S$ 下。正如我们的分析所预测的，参数对称性的缺乏将破坏不变性。预期结果为 `False`。\n\n*   **测试用例 2**：共享模型（$a=1.5, b=-0.2, v=0.7, c=-0.1$），在变换 $S$ 下。如解析所示，共享参数模型对坐标交换是不变的。预期结果为 `True`。\n\n*   **测试用例 3**：参数被手动设置为对称的非共享模型（$a_1=0.8, b_1=0.0, a_2=0.8, b_2=0.0, v_1=1.0, v_2=1.0, c=0.0$）。该模型在功能上等同于一个共享模型：$y_{untied} = \\mathrm{ReLU}(0.8 x_1) + \\mathrm{ReLU}(0.8 x_2)$。因此，它将对变换 $S$ 具有不变性。这个案例突显出，不变性是所计算函数的一个属性，该属性由参数值决定，而不仅仅是模型在结构上被分类为“非共享”。预期结果为 `True`。\n\n*   **测试用例 4**：共享模型（$a=1.0, b=0.0, v=0.5, c=0.05$），在变换 $F$ 下。问题预期其不具有不变性。让我们分析一下输出函数：\n    $$y = 0.5 \\cdot (\\mathrm{ReLU}(1.0 \\cdot x_1 + 0.0) + \\mathrm{ReLU}(1.0 \\cdot x_2 + 0.0)) + 0.05 = 0.5 \\cdot (\\mathrm{ReLU}(x_1) + \\mathrm{ReLU}(x_2)) + 0.05$$\n    输入坐标 $x_1, x_2$ 从 $[-1, 1]$ 中采样。$\\mathrm{ReLU}$ 函数是非负的。\n    $\\mathrm{ReLU}(x_1) + \\mathrm{ReLU}(x_2)$ 的最小值是 $0$（当 $x_1 \\le 0$ 且 $x_2 \\le 0$ 时）。\n    在这种情况下，最小输出是 $y_{min} = 0.5 \\cdot 0 + 0.05 = 0.05$。\n    由于输出 $y$ 总是大于或等于 $0.05$，它总是大于阈值 $0$。这意味着预测的标签总是 $1$，即在域中的所有输入上 $f(x) \\equiv 1$。一个常数函数对于任何变换（包括 $F$）都是平凡不变的。因此，与问题的非正式预期相反，该模型将被经验性地验证为不变的。结果将是 `True`。\n\n*   **测试用例 5**：具有零输出权重（$a=1.0, b=0.5, v=0.0, c=0.3$）的共享模型，在变换 $S$ 下。输出函数是：\n    $$y = 0.0 \\cdot (\\mathrm{ReLU}(1.0 \\cdot x_1 + 0.5) + \\mathrm{ReLU}(1.0 \\cdot x_2 + 0.5)) + 0.3 = 0.3$$\n    输出是一个常数 $0.3$。由于 $0.3 \\ge 0$，预测的标签总是 $1$。与前一个案例一样，该分类器是一个常数函数，因此对于任何变换都是平凡不变的。结果是 `True`。\n\n程序将实现这些模型和检查，以生成与这五个案例相对应的布尔值列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests untied and tied neural network models for invariance\n    under specified transformations.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"model\": \"untied\",\n            \"params\": {\"a1\": 1.0, \"b1\": 0.1, \"a2\": 2.0, \"b2\": -0.3, \"v1\": 1.0, \"v2\": -0.5, \"c\": 0.05},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 42\n        },\n        {\n            \"model\": \"tied\",\n            \"params\": {\"a\": 1.5, \"b\": -0.2, \"v\": 0.7, \"c\": -0.1},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 42\n        },\n        {\n            \"model\": \"untied\",\n            \"params\": {\"a1\": 0.8, \"b1\": 0.0, \"a2\": 0.8, \"b2\": 0.0, \"v1\": 1.0, \"v2\": 1.0, \"c\": 0.0},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 7\n        },\n        {\n            \"model\": \"tied\",\n            \"params\": {\"a\": 1.0, \"b\": 0.0, \"v\": 0.5, \"c\": 0.05},\n            \"transformation\": \"F\", \"N\": 200, \"seed\": 42\n        },\n        {\n            \"model\": \"tied\",\n            \"params\": {\"a\": 1.0, \"b\": 0.5, \"v\": 0.0, \"c\": 0.3},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 123\n        }\n    ]\n\n    def relu(z):\n        \"\"\"Rectified Linear Unit activation function.\"\"\"\n        return np.maximum(0, z)\n\n    def untied_model(X, params):\n        \"\"\"Computes labels for the untied-parameter model.\"\"\"\n        x1, x2 = X[:, 0], X[:, 1]\n        p = params\n        h1 = relu(p[\"a1\"] * x1 + p[\"b1\"])\n        h2 = relu(p[\"a2\"] * x2 + p[\"b2\"])\n        y = p[\"v1\"] * h1 + p[\"v2\"] * h2 + p[\"c\"]\n        return (y >= 0).astype(int)\n\n    def tied_model(X, params):\n        \"\"\"Computes labels for the tied-parameter model.\"\"\"\n        x1, x2 = X[:, 0], X[:, 1]\n        p = params\n        h1 = relu(p[\"a\"] * x1 + p[\"b\"])\n        h2 = relu(p[\"a\"] * x2 + p[\"b\"])\n        y = p[\"v\"] * (h1 + h2) + p[\"c\"]\n        return (y >= 0).astype(int)\n\n    model_funcs = {\n        \"untied\": untied_model,\n        \"tied\": tied_model\n    }\n\n    transform_funcs = {\n        \"S\": lambda X: X[:, ::-1],  # Coordinate-swap\n        \"F\": lambda X: -X           # Sign-flip\n    }\n\n    results = []\n    for case in test_cases:\n        # 1. Set seed and generate data\n        np.random.seed(case[\"seed\"])\n        X_original = np.random.uniform(-1, 1, size=(case[\"N\"], 2))\n\n        # 2. Apply transformation\n        transform_func = transform_funcs[case[\"transformation\"]]\n        X_transformed = transform_func(X_original)\n\n        # 3. Select model and compute labels\n        model_func = model_funcs[case[\"model\"]]\n        labels_original = model_func(X_original, case[\"params\"])\n        labels_transformed = model_func(X_transformed, case[\"params\"])\n\n        # 4. Check for invariance\n        is_invariant = np.array_equal(labels_original, labels_transformed)\n        results.append(is_invariant)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后的实践将探讨一种更为灵活的方法，称为“柔性”参数绑定，它通过正则化惩罚项来实现。与强制参数完全相等（硬绑定）不同，我们将鼓励它们变得相似，并研究不同的惩罚范数（如 $L_1$ 和 $L_2$ 范数）如何影响模型的压缩率和预测精度。这个练习将参数绑定的概念与更广泛且强大的正则化技术联系起来，展示了在模型设计中如何在施加先验约束与从数据中学习之间进行权衡。",
            "id": "3161931",
            "problem": "您需要在一个简单的二元分类器中，通过对参数向量之差进行正则化，来实现和分析软参数绑定。该问题旨在探讨不同范数如何影响压缩（即有多少参数变得有效共享）和预测准确性。实现必须是一个完整的、可运行的程序。\n\n此问题的基础是经验风险最小化（ERM）、二元交叉熵（BCE）、基于梯度的优化和凸范数正则化。令 $\\sigma(u) = \\frac{1}{1 + e^{-u}}$ 表示逻辑 sigmoid 函数。对于预测概率为 $p(x)$ 的单个样本 $(x, y)$，其二元交叉熵（BCE）为 $-\\left(y \\log p(x) + (1-y)\\log(1-p(x))\\right)$，而经验风险最小化（ERM）则通过样本平均来近似期望风险。\n\n模型规格：\n- 输入为向量 $x \\in \\mathbb{R}^d$，标签为 $y \\in \\{0,1\\}$。\n- 模型维护两个权重向量 $w^{(a)}, w^{(b)} \\in \\mathbb{R}^d$。\n- 预测公式为 $p(x) = \\sigma\\!\\left(\\frac{1}{2}\\left(w^{(a)\\top}x + w^{(b)\\top}x\\right)\\right)$。\n- 训练目标是训练集上的平均二元交叉熵（BCE）加上施加于参数差异 $d = w^{(a)} - w^{(b)}$ 的软绑定惩罚项。\n\n待比较的软绑定惩罚项：\n- $L_1$ 惩罚项：$\\lambda_1 \\|d\\|_1$。\n- $L_2$ 惩罚项（平方）：$\\lambda_2 \\|d\\|_2^2$。\n- 弹性网络惩罚项：$\\lambda_1 \\|d\\|_1 + \\lambda_2 \\|d\\|_2^2$。\n\n数据生成（确定性）：\n- 对所有随机性使用伪随机种子 $0$。\n- 维度 $d = 10$。\n- 训练集大小 $n_{\\text{train}} = 512$，验证集大小 $n_{\\text{val}} = 256$。\n- 从 $\\mathcal{N}(0, I_d)$ 中抽取一次 $w^\\star$，并对两个数据集固定使用。\n- 对所有样本，独立地从 $\\mathcal{N}(0, I_d)$ 中抽取输入 $x$。\n- 对所有样本，独立地从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取噪声 $\\epsilon$，其中 $\\sigma^2 = 0.25$。\n- 将标签定义为 $y = \\mathbf{1}\\{w^{\\star\\top} x + \\epsilon > 0\\}$。\n\n训练过程：\n- 在 $\\mathbb{R}^d$ 中初始化 $w^{(a)} = 0$ 和 $w^{(b)} = 0$。\n- 使用全批量梯度下降法优化目标函数，迭代 $T = 400$ 次，步长为 $\\eta = 0.05$。\n- 对于 $L_1$ 惩罚项，使用次梯度 $\\text{sign}(d)$。\n- 对于 $L_2$ 惩罚项（平方），使用梯度 $2\\lambda_2 d$。\n- 对于弹性网络，将相应梯度相加。\n\n评估指标：\n- 压缩率：满足 $|w^{(a)}_i - w^{(b)}_i| \\le \\tau$ 的索引 $i \\in \\{1,\\dots,d\\}$ 的比例，容差 $\\tau = 10^{-2}$。\n- 准确率：在验证集上，使用决策规则 $\\hat{y} = \\mathbf{1}\\{p(x) \\ge 0.5\\}$ 正确分类的样本比例。\n\n测试套件：\n实现并运行以下 5 个测试用例，每个用例由 $(\\lambda_1, \\lambda_2)$ 和惩罚类型定义：\n1. 基线，无绑定：$\\lambda_1 = 0$，$\\lambda_2 = 0$。\n2. $L_2$ 中等强度：$\\lambda_1 = 0$，$\\lambda_2 = 0.1$。\n3. $L_1$ 中等强度：$\\lambda_1 = 0.05$，$\\lambda_2 = 0$。\n4. $L_2$ 高强度：$\\lambda_1 = 0$，$\\lambda_2 = 1.0$。\n5. 弹性网络平衡：$\\lambda_1 = 0.05$，$\\lambda_2 = 0.2$。\n\n您的程序应实现上述设置，并按给定顺序为每个测试用例生成一个包含 $[\\text{compression\\_ratio}, \\text{accuracy}]$ 的对，其中两个值均为浮点数。最终输出格式必须是包含这 5 个对的单行列表，并按顺序排列，例如：\n$[[c_1,a_1],[c_2,a_2],[c_3,a_3],[c_4,a_4],[c_5,a_5]]$，其中每个 $c_i$ 和 $a_i$ 都是四舍五入到 4 位小数的浮点数。",
            "solution": "首先根据科学合理性、适定性和客观性标准对问题陈述进行验证。\n\n### 第 1 步：提取已知条件\n- **模型**：具有两个权重向量 $w^{(a)}, w^{(b)} \\in \\mathbb{R}^d$ 的二元分类器。\n- **预测函数**：$p(x) = \\sigma\\!\\left(\\frac{1}{2}\\left(w^{(a)\\top}x + w^{(b)\\top}x\\right)\\right)$，其中 $\\sigma(u) = (1 + e^{-u})^{-1}$ 是逻辑 sigmoid 函数。\n- **损失函数**：训练集上的平均二元交叉熵（BCE），其中对于单个样本 $(x, y)$，损失为 $-\\left(y \\log p(x) + (1-y)\\log(1-p(x))\\right)$。\n- **正则化**：对差异向量 $d = w^{(a)} - w^{(b)}$ 施加惩罚项。\n    - $L_1$ 惩罚项：$\\lambda_1 \\|d\\|_1$。\n    - $L_2$ 平方惩罚项：$\\lambda_2 \\|d\\|_2^2$。\n    - 弹性网络惩罚项：$\\lambda_1 \\|d\\|_1 + \\lambda_2 \\|d\\|_2^2$。\n- **数据生成**：\n    - 伪随机种子：$0$。\n    - 维度 $d = 10$。\n    - 训练集大小 $n_{\\text{train}} = 512$。\n    - 验证集大小 $n_{\\text{val}} = 256$。\n    - 真实权重 $w^\\star \\sim \\mathcal{N}(0, I_d)$。\n    - 输入 $x \\sim \\mathcal{N}(0, I_d)$。\n    - 噪声 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$，噪声方差 $\\sigma^2 = 0.25$。\n    - 标签 $y = \\mathbf{1}\\{w^{\\star\\top} x + \\epsilon > 0\\}$，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n- **优化**：\n    - 算法：全批量梯度下降。\n    - 初始化：$w^{(a)} = 0$， $w^{(b)} = 0$。\n    - 迭代次数：$T = 400$。\n    - 步长：$\\eta = 0.05$。\n    - 惩罚项的梯度：$L_1$ 的次梯度为 $\\text{sign}(d)$，平方 $L_2$ 的梯度为 $2\\lambda_2 d$。\n- **评估**：\n    - **压缩率**：满足 $|w^{(a)}_i - w^{(b)}_i| \\le \\tau$ 的索引 $i$ 的比例，容差 $\\tau = 10^{-2}$。\n    - **准确率**：在验证集上正确预测的比例，决策规则为 $\\hat{y} = \\mathbf{1}\\{p(x) \\ge 0.5\\}$。\n- **测试套件**：5 个用例，其 $(\\lambda_1, \\lambda_2)$ 值分别为：(1) $(0, 0)$，(2) $(0, 0.1)$，(3) $(0.05, 0)$，(4) $(0, 1.0)$，(5) $(0.05, 0.2)$。\n- **输出格式**：一个包含 5 个 $[\\text{compression\\_ratio}, \\text{accuracy}]$ 对的列表，四舍五入到 4 位小数。\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题具有科学依据，它基于机器学习的标准原理，包括经验风险最小化、逻辑回归、基于梯度的优化以及正则化技术（$L_1$、$L_2$、弹性网络）。参数绑定的概念是神经网络设计中一种成熟的做法。\n\n该问题是适定的。目标函数是平均 BCE 损失（模型参数的凸函数）与凸正则化惩罚项之和。最终的目标函数是凸函数，通过梯度下降法对其进行最小化是一个标准的、稳定的数值问题。所有参数、数据生成过程和评估指标都得到了精确、无歧义的规定，确保可以计算出唯一且有意义的解。\n\n该问题是客观、完整且一致的。它使用形式化的数学语言描述，没有主观陈述。提供了实现所需的所有必要信息，且内部没有矛盾。\n\n### 第 3 步：结论与行动\n该问题被判定为有效。将开发一个完整的解决方案。\n\n### 求解推导\n\n目标是使用梯度下降法，通过最小化一个复合目标函数 $J(w^{(a)}, w^{(b)})$ 来训练模型。该目标函数结合了一个数据拟合项（平均 BCE 损失）和一个促进参数绑定的正则化项。\n\n**目标函数**\n\n设训练数据为 $\\{(x_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$。模型对样本 $x_i$ 的预测为 $p_i = p(x_i) = \\sigma(z_i)$，其中 logit 为 $z_i = \\frac{1}{2}(w^{(a)\\top}x_i + w^{(b)\\top}x_i)$。总目标函数为：\n$$\nJ(w^{(a)}, w^{(b)}) = J_{\\text{BCE}}(w^{(a)}, w^{(b)}) + R(w^{(a)} - w^{(b)})\n$$\n第一项是平均 BCE 损失：\n$$\nJ_{\\text{BCE}}(w^{(a)}, w^{(b)}) = -\\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]\n$$\n第二项是施加于差异向量 $d = w^{(a)} - w^{(b)}$ 的软绑定惩罚项的一般形式：\n$$\nR(d) = \\lambda_1 \\|d\\|_1 + \\lambda_2 \\|d\\|_2^2\n$$\n\n**梯度计算**\n为了实现梯度下降，我们需要计算 $J$ 相对于 $w^{(a)}$ 和 $w^{(b)}$ 的梯度。我们使用链式法则来计算这些梯度。\n\n单个样本的 BCE 损失 $L_i$ 相对于 logit $z_i$ 的梯度是众所周知的：\n$$\n\\frac{\\partial L_i}{\\partial z_i} = p_i - y_i\n$$\nlogit $z_i$ 相对于权重向量的梯度为：\n$$\n\\nabla_{w^{(a)}} z_i = \\frac{1}{2}x_i \\quad \\text{和} \\quad \\nabla_{w^{(b)}} z_i = \\frac{1}{2}x_i\n$$\n对整个训练集上的 BCE 项应用链式法则：\n$$\n\\nabla_{w^{(a)}} J_{\\text{BCE}} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} \\frac{\\partial L_i}{\\partial z_i} \\nabla_{w^{(a)}} z_i = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} (p_i - y_i) \\frac{1}{2}x_i = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y)\n$$\n根据对称性，相对于 $w^{(b)}$ 的梯度是相同的：\n$$\n\\nabla_{w^{(b)}} J_{\\text{BCE}} = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y)\n$$\n其中 $X$ 是 $n_{\\text{train}} \\times d$ 的数据矩阵，$P$ 是预测向量，$Y$ 是真实标签向量。\n\n接下来，我们求正则化项 $R(d) = R(w^{(a)} - w^{(b)})$ 的梯度。\n$$\n\\nabla_{w^{(a)}} R(d) = \\nabla_{d} R(d) \\cdot \\frac{\\partial d}{\\partial w^{(a)}} = \\nabla_{d} R(d)\n$$\n$$\n\\nabla_{w^{(b)}} R(d) = \\nabla_{d} R(d) \\cdot \\frac{\\partial d}{\\partial w^{(b)}} = -\\nabla_{d} R(d)\n$$\n（次）梯度 $\\nabla_{d} R(d)$ 由下式给出：\n$$\n\\nabla_{d} R(d) = \\lambda_1 \\text{sign}(d) + 2\\lambda_2 d\n$$\n这里，$\\text{sign}(d)$ 是 $L_1$ 范数的次梯度。在算法实现中，$\\text{sign}(0)$ 可取为 $0$。\n\n结合这些项，完整的梯度为：\n$$\n\\nabla_{w^{(a)}} J = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y) + (\\lambda_1 \\text{sign}(d) + 2\\lambda_2 d)\n$$\n$$\n\\nabla_{w^{(b)}} J = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y) - (\\lambda_1 \\text{sign}(d) + 2\\lambda_2 d)\n$$\n\n**优化算法**\n权重使用全批量梯度下降法进行更新，迭代 $T=400$ 次，学习率为 $\\eta=0.05$。从 $w^{(a)}_0 = 0$ 和 $w^{(b)}_0 = 0$ 开始，第 $t$ 步的更新规则是：\n$$\nw^{(a)}_{t+1} = w^{(a)}_t - \\eta \\nabla_{w^{(a)}} J(w^{(a)}_t, w^{(b)}_t)\n$$\n$$\nw^{(b)}_{t+1} = w^{(b)}_t - \\eta \\nabla_{w^{(b)}} J(w^{(a)}_t, w^{(b)}_t)\n$$\n对由 $(\\lambda_1, \\lambda_2)$ 对定义的 5 个测试用例中的每一个重复此过程。\n\n**评估**\n训练后，计算两个指标：\n1.  **压缩率**：该指标衡量参数共享的程度。它是对应权重之间的绝对差值低于容差 $\\tau = 10^{-2}$ 的维度所占的比例。\n    $$\n    \\text{Compression Ratio} = \\frac{1}{d} \\sum_{i=1}^d \\mathbf{1}\\{|w^{(a)}_i - w^{(b)}_i| \\le \\tau\\}\n    $$\n2.  **准确率**：该指标衡量在未见过的验证集上的预测性能。如果预测概率 $p(x) \\ge 0.5$，则预测标签 $\\hat{y}$ 为 $1$，否则为 $0$。这等同于检查 logit $z = \\frac{1}{2}(w^{(a)\\top}x + w^{(b)\\top}x) \\ge 0$。\n    $$\n    \\text{Accuracy} = \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} \\mathbf{1}\\{\\hat{y}_i = y_i^{\\text{val}}\\}\n    $$\n实现将遵循这些推导过程来生成所需的输出。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes soft parameter tying in a binary classifier.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    SEED = 0\n    D = 10  # Dimension of input vectors\n    N_TRAIN = 512\n    N_VAL = 256\n    NOISE_VAR = 0.25\n    \n    T = 400  # Number of iterations\n    ETA = 0.05  # Step size (learning rate)\n    TAU = 1e-2  # Tolerance for compression ratio\n\n    # --- Test Cases ---\n    test_cases = [\n        # (lambda_1, lambda_2), penalty_type\n        (0.0, 0.0),      # 1. Baseline, no tying\n        (0.0, 0.1),      # 2. L2 moderate\n        (0.05, 0.0),     # 3. L1 moderate\n        (0.0, 1.0),      # 4. L2 strong\n        (0.05, 0.2),     # 5. Elastic net balanced\n    ]\n\n    # --- Helper Functions ---\n    def sigmoid(u):\n        \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-u))\n\n    def generate_data(n_samples, d, w_star, noise_std, rng):\n        \"\"\"Generates synthetic data for binary classification.\"\"\"\n        X = rng.standard_normal(size=(n_samples, d))\n        epsilon = rng.normal(0, noise_std, size=n_samples)\n        logits = X @ w_star + epsilon\n        y = (logits > 0).astype(int)\n        return X, y\n\n    # --- Data Generation ---\n    rng = np.random.default_rng(SEED)\n    w_star = rng.standard_normal(size=D)\n    noise_std = np.sqrt(NOISE_VAR)\n    \n    X_train, y_train = generate_data(N_TRAIN, D, w_star, noise_std, rng)\n    X_val, y_val = generate_data(N_VAL, D, w_star, noise_std, rng)\n\n    results = []\n\n    # --- Main Loop: Train and Evaluate for each test case ---\n    for lambda_1, lambda_2 in test_cases:\n        # Initialize weights\n        w_a = np.zeros(D)\n        w_b = np.zeros(D)\n\n        # Full-batch gradient descent\n        for _ in range(T):\n            # Forward pass\n            logits = 0.5 * (X_train @ w_a + X_train @ w_b)\n            predictions = sigmoid(logits)\n            \n            # --- Gradient Calculation ---\n            # Gradient of BCE loss term\n            error = predictions - y_train\n            grad_bce = (1 / (2 * N_TRAIN)) * X_train.T @ error\n\n            # Gradient of regularization term\n            d = w_a - w_b\n            grad_reg = lambda_1 * np.sign(d) + 2 * lambda_2 * d\n            \n            # Full gradients for w_a and w_b\n            grad_w_a = grad_bce + grad_reg\n            grad_w_b = grad_bce - grad_reg\n\n            # --- Weight Update ---\n            w_a -= ETA * grad_w_a\n            w_b -= ETA * grad_w_b\n\n        # --- Evaluation ---\n        # 1. Compression Ratio\n        final_d = w_a - w_b\n        compression_ratio = np.mean(np.abs(final_d) = TAU)\n\n        # 2. Accuracy on validation set\n        val_logits = 0.5 * (X_val @ w_a + X_val @ w_b)\n        y_hat_val = (val_logits >= 0).astype(int)\n        accuracy = np.mean(y_hat_val == y_val)\n        \n        results.append((compression_ratio, accuracy))\n\n    # --- Format and Print Output ---\n    output_str = \"[\" + \",\".join([f\"[{c:.4f},{a:.4f}]\" for c, a in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}