## 引言
在数据驱动的时代，我们如何让机器理解单词、图像、用户偏好乃至基因序列这类复杂、非结构化实体的内在含义？传统的表示方法（如one-hot编码）往往维度高、数据稀疏，且无法捕捉实体间的语义关联，构成了人工智能发展的一大障碍。密集向量嵌入（Dense Vector Embeddings）作为一种革命性的表征学习[范式](@entry_id:161181)应运而生，它通过将实体映射到低维、稠密的[向量空间](@entry_id:151108)，巧妙地将“语义相似”转化为“几何邻近”。这种表示方法不仅解决了稀疏性问题，更重要的是赋予了模型强大的泛化能力，成为现代机器学习和深度学习的基石。

本文将系统地引导你掌握密集向量嵌入的核心知识。在“原理和机制”一章中，我们将深入探讨支撑嵌入的几何学与统计学基础，并揭示[对比学习](@entry_id:635684)等关键训练算法的内部工作方式。随后，在“应用与跨学科连接”一章，我们将跨越从自然语言处理到生物信息学的多个领域，展示嵌入技术在真实世界问题中的强大威力。最后，“动手实践”部分将提供精选的编程练习，帮助你将理论知识转化为实践技能。通过这趟旅程，你将建立起对密集向量嵌入全面而深刻的理解。

## 原理和机制

在前一章中，我们介绍了密集向量嵌入的基本概念，即用高维空间中的向量来表示复杂的实体，如单词、图像或用户。本章将深入探讨支持这些表示法的核心原理和机制。我们将从定义[嵌入空间](@entry_id:637157)中的几何关系开始，探索高维空间的统计特性，研究塑造这些空间的学习算法，并讨论在实践中应用和稳定这些模型的关键技术。

### 相似性的几何学：在[向量空间](@entry_id:151108)中定义关系

密集向量嵌入的核心思想是，向量之间的几何关系应该反映它们所代表的实体之间的语义关系。例如，在自然语言处理中，我们期望代表“猫”和“狗”的向量在空间中比“猫”和“汽车”的向量更接近。这种几何关系通常通过两种核心的相似性度量来量化：**[点积](@entry_id:149019)（dot product）**和**余弦相似度（cosine similarity）**。

#### [点积](@entry_id:149019)相似度

给定两个嵌入向量 $x, w \in \mathbb{R}^d$，它们的**[点积](@entry_id:149019)**，也称为[内积](@entry_id:158127)，定义为 $x \cdot w = \sum_{k=1}^{d} x_k w_k$。从几何上看，[点积](@entry_id:149019)可以表示为 $x \cdot w = \|x\| \|w\| \cos(\theta)$，其中 $\|x\|$ 和 $\|w\|$ 分别是向量的欧几里得范数（或长度），$\theta$ 是它们之间的夹角。

这个公式揭示了[点积](@entry_id:149019)的一个关键特性：它同时依赖于向量的**长度**和它们之间的**夹角**。这意味着，即使两个向量之间的夹角很小（即它们指向相似的方向），如果它们的长度（范数）很小，[点积](@entry_id:149019)也会很小。反之，即使夹角较大，只要向量的长度足够大，[点积](@entry_id:149019)也可能很大。

这种对范数的敏感性在[分类任务](@entry_id:635433)中具有重要意义。假设我们有一个基于[点积](@entry_id:149019)的决策规则：如果一个查询嵌入 $x$ 与一个类别原型 $w$ 的[点积](@entry_id:149019)超过某个阈值 $\tau$（即 $x \cdot w \ge \tau$），则接受该分类。现在，考虑一个在推理时对查询嵌入进行缩放的场景，这在实践中可能因温度缩放等技术而发生。我们将新的嵌入表示为 $x' = \alpha x$，其中 $\alpha > 0$ 是一个缩放因子。新的决策条件变为 $(\alpha x) \cdot w \ge \tau$，这等价于 $x \cdot w \ge \tau / \alpha$。

这个简单的推导表明，对嵌入向量进行缩放，等同于改变了决策阈值。如果 $\alpha > 1$，阈值会降低，使得更多的向量被接受；如果 $\alpha < 1$，阈值会升高，使得分类变得更加严格。因此，[决策边界](@entry_id:146073)会随着[向量范数](@entry_id:140649)的变化而移动，这可能不是我们所期望的鲁棒行为 。

#### 余弦相似度

为了解决[点积](@entry_id:149019)对范数的敏感性问题，我们引入了**余弦相似度**。它被定义为两个向量之间夹角的余弦值：
$$
\cos(x, w) = \frac{x \cdot w}{\|x\| \|w\|}
$$
顾名思义，余弦相似度只取决于向量之间的夹角 $\theta$，完全忽略了它们的长度。其取值范围在 $[-1, 1]$ 之间，其中 $1$ 表示向量指向完全相同的方向，$-1$ 表示方向完全相反，$0$ 表示它们是正交的。

让我们再次考察缩放操作 $x' = \alpha x$ 对余弦相似度的影响 。
$$
\cos(x', w) = \frac{(\alpha x) \cdot w}{\|\alpha x\| \|w\|} = \frac{\alpha (x \cdot w)}{|\alpha| \|x\| \|w\|}
$$
因为 $\alpha > 0$，所以 $|\alpha| = \alpha$，上式可以简化为：
$$
\cos(x', w) = \frac{\alpha (x \cdot w)}{\alpha \|x\| \|w\|} = \frac{x \cdot w}{\|x\| \|w\|} = \cos(x, w)
$$
这个结果表明，余弦相似度对于向量的正向缩放是**不变的**。这意味着，如果我们的决策规则是基于余弦相似度（例如，$\cos(x, w) \ge \gamma$），那么无论嵌入的范数如何变化，决策结果都将保持一致。这种尺度不变性是余弦相似度在许多应用中（如信息检索和[聚类](@entry_id:266727)）比[点积](@entry_id:149019)更受欢迎的关键原因。

#### 超球面嵌入

余弦相似度的尺度不变性自然地引出了一个重要的实践：将所有嵌入向量**$L_2$范数归一化（$L_2$-normalization）**，使它们的长度都为 $1$。经过归一化后，所有嵌入向量都位于一个 $d$ 维空间中的单位超球面上，记为 $\mathbb{S}^{d-1}$。

当所有嵌入向量 $x$ 和原型向量 $w$ 的范数都为 $1$ 时，[点积](@entry_id:149019)和余弦相似度之间的关系变得非常简单：$x \cdot w = \cos(\theta)$。在这种**超球面嵌入（hyperspherical embeddings）**的设定下，[点积](@entry_id:149019)的[计算效率](@entry_id:270255)和余弦相似度的几何直观性得到了统一。这不仅简化了相似度的计算，还使得我们能够在一个定义良好的、紧凑的[流形](@entry_id:153038)上分析和设计学习算法。在后续的讨论中，除非特别说明，我们通常假设嵌入向量是经过范数归一化的   。

### 高维嵌入的统计特性

我们已经建立了用几何关系表示语义相似性的框架。但这引出了一个更深层次的问题：为什么这种方法在高维空间中是有效的？为了回答这个问题，我们需要理解高维空间的统计特性，特别是与一个“无结构”的随机基线相比。

想象一下，在一个 $d$ 维单位超球面上随机、均匀地选择两个向量 $u$ 和 $v$。它们之间的余弦相似度 $u^\top v$ 会呈现怎样的[分布](@entry_id:182848)？这相当于我们模型的“零假设”：在模型学习到任何有意义的结构之前，嵌入向量之间的关系是怎样的。

我们可以通过一个简单的对称性论证来推导其[期望值](@entry_id:153208) 。由于 $u$ 和 $v$ 是独立且同[分布](@entry_id:182848)的，我们可以利用[期望的线性](@entry_id:273513)性质和独立性：
$$
\mathbb{E}[u^\top v] = \mathbb{E}\left[\sum_{i=1}^{d} u_i v_i\right] = \sum_{i=1}^{d} \mathbb{E}[u_i v_i] = \sum_{i=1}^{d} \mathbb{E}[u_i] \mathbb{E}[v_i]
$$
现在我们来计算单个分量的期望 $\mathbb{E}[u_i]$。由于 $u$ 是在单位超球面上[均匀分布](@entry_id:194597)的，对于任何一个 $u$，其相反方向的向量 $-u$ 也在超球面上，并且具有完全相同的[概率密度](@entry_id:175496)。这意味着随机向量 $u$ 和 $-u$ 具有相同的[分布](@entry_id:182848)。因此，它们的期望必须相等：
$$
\mathbb{E}[u] = \mathbb{E}[-u] = -\mathbb{E}[u]
$$
这个等式唯一的解是 $\mathbb{E}[u] = \mathbf{0}$，即期望向量是一个零向量。这意味着它的每个分量的期望也为零：$\mathbb{E}[u_i] = 0$。同理，$\mathbb{E}[v_i] = 0$。代入原式，我们得到：
$$
\mathbb{E}[u^\top v] = \sum_{i=1}^{d} (0)(0) = 0
$$
这个惊人的结果表明，在高维空间中随机选取的两个单位向量，它们之间的余弦相似度的[期望值](@entry_id:153208)为 $0$。换句话说，它们很可能是**近乎正交的**。

更进一步的分析可以表明，这个相似度[分布](@entry_id:182848)的[方差](@entry_id:200758)为 $\text{Var}(u^\top v) = 1/d$ 。这意味着随着维度 $d$ 的增加，[方差](@entry_id:200758)迅速减小到 $0$。这个现象被称为**维度集中（concentration of measure）**。它告诉我们，在高维空间中，随机向量之间的余弦相似度不仅平均为 $0$，而且几乎总是非常接近 $0$。

这一理论结果对理解嵌入的有效性至关重要。它意味着，如果一个模型在训练后产生的嵌入向量对之间表现出显著的非零相似性（无论是大的正值还是负值），这几乎不可能是偶然发生的。这种偏离随机基线的行为，是模型成功捕捉到数据中潜在语义结构的强有力证据。在一个几乎所有事物都相互“远离”（正交）的空间中，学习算法的任务就是有选择地将相关的概念拉近，形成有意义的聚类和结构。

### 学习嵌入：塑造[嵌入空间](@entry_id:637157)

既然我们知道了理想的[嵌入空间](@entry_id:637157)应该具有怎样的几何和统计特性，接下来的问题就是：我们如何通过训练[神经网](@entry_id:276355)络来塑造这样一个空间？答案在于设计合适的**损失函数（loss function）**，它像一位雕塑家，引导着嵌入向量在空间中的排布。

#### 能量模型与[对比学习](@entry_id:635684)视角

我们可以从一个物理学的视角来理解[嵌入学习](@entry_id:637654)：将系统（即[嵌入空间](@entry_id:637157)）的“能量”定义为与我们期望的结构相反的量。一个好的嵌入配置应该处于低能量状态。在信息检索或推荐系统中，一个常见的能量函数是基于相似性的**能量模型（Energy-Based Model, EBM）** 。给定一个查询嵌入 $x$ 和一个项目嵌入 $y$，我们可以定义它们之间的能量为 $E(x,y) = -x^\top y$。这样，高相似度（大的[点积](@entry_id:149019)）对应于低能量，这正是我们想要的。

根据[统计力](@entry_id:194984)学中的玻尔兹曼分布，一个项目 $y$ 对于查询 $x$ 的条件概率可以表示为：
$$
p(y \mid x) = \frac{\exp(-E(x,y)/\tau)}{\sum_{j} \exp(-E(x,y_j)/\tau)} = \frac{\exp(x^\top y / \tau)}{\sum_{j} \exp(x^\top y_j / \tau)}
$$
这里，$\tau > 0$ 是一个**温度（temperature）**参数，分母是对数据集中所有项目求和的**[配分函数](@entry_id:193625)（partition function）**。训练的目标是最大化观测到的正样本对 $(x, y^+)$ 的对数似然 $\log p(y^+ \mid x)$。

对这个[目标函数](@entry_id:267263)求关于查询嵌入 $x$ 的梯度，我们得到一个富有启发性的形式 ：
$$
\frac{\partial}{\partial x} \log p(y^+ \mid x) = \frac{1}{\tau} \left( y^+ - \mathbb{E}_{y \sim p(\cdot \mid x)}[y] \right)
$$
这个梯度直观地告诉我们学习的方向：它将查询嵌入 $x$ “拉向”正样本嵌入 $y^+$，同时“推开”所有其他项目嵌入的期望（或加权平均）$\mathbb{E}_{y \sim p(\cdot \mid x)}[y]$。这就是**[对比学习](@entry_id:635684)（contrastive learning）**的核心思想：通过将相似的样本拉近，将不相似的样本推开来学习表示。

在实践中，计算完整的[配分函数](@entry_id:193625)（对所有项目求和）是不可行的。因此，通常采用**[负采样](@entry_id:634675)（negative sampling）**，即从数据集中随机抽取一小部分“负”样本来近似分母。这种[采样方法](@entry_id:141232)会引入偏差，但可以通过[重要性采样](@entry_id:145704)进行校正，或者在某些情况下，即使不校正，模型依然能学习到有意义的表示，因为它隐式地在学习区分数据[分布](@entry_id:182848)和噪声（采样）[分布](@entry_id:182848) 。

温度参数 $\tau$ 控制着学习的“软硬”程度。较低的 $\tau$ 会使[概率分布](@entry_id:146404)更加尖锐，模型会更关注那些与查询最相似的“困难”负样本，但可能导致梯度过大和训练不稳定。较高的 $\tau$ 则会产生更平滑的[分布](@entry_id:182848)，使训练更稳定，尤其是在早期阶段。一种常见的策略是**温度退火（temperature annealing）**，即在训练初期使用较高的温度，然后逐渐降低它 。

这种[对比学习](@entry_id:635684)的框架是许多现代[自监督学习](@entry_id:173394)算法的基石。在这些算法中，一个数据点（如一张图片）的两个不同增强视图构成了正样本对，而来自批次中其他数据点的视图则作为负样本。学习的目标是最大化正样本对之间的相似度，同时最小化负样本对之间的相似度，这可以通过最大化一个类似“失配分数”的度量 $M = \mu_P - \mu_Q$ 来实现，其中 $\mu_P$ 是正样本对的平均相似度，$\mu_Q$ 是负样本对的平均相似度 。

#### 为[分类任务](@entry_id:635433)设计间隔

当目标是分类而不仅仅是相似性排序时，我们需要更精细地控制[嵌入空间](@entry_id:637157)的结构。我们的目标不仅是让同类样本相互靠近，还要确保不同类别之间有足够清晰的**间隔（margin）**，以提高分类器的鲁棒性。

在一个标准的、使用范数归一化嵌入和权重的[Softmax分类器](@entry_id:634335)中，类别 $i$ 和 $j$ 之间的[决策边界](@entry_id:146073)由等式 $w_i^\top x = w_j^\top x$ 定义，其中 $w_i, w_j$ 是类别的原型权重向量。这个方程定义了一个通过原点的超平面，它在单位超球面上的交集是一个**大超球面（great hypersphere）**，在几何上是两个原型向量 $w_i$ 和 $w_j$ 的角平分面 。

虽然这种设置能够正确分类，但它对噪声和扰动很敏感，因为样本可能紧贴着决策边界。为了增强类内紧凑性和类间[可分性](@entry_id:143854)，研究人员提出了多种引入间隔的损失函数。其中两种著名的方法是**CosFace**和**ArcFace**，它们通过修改目标类别的logit来强制学习一个间隔 。

- **CosFace (LMCL)**: 它在**余弦空间**中引入一个加性间隔 $m$。对于一个属于类别 $y$ 的嵌入 $x$，其与原型 $w_y$ 的夹角为 $\theta_y$，原始logit为 $\cos(\theta_y)$。CosFace将其修改为 $\cos(\theta_y) - m$。决策边界变为 $\cos(\theta_1) - m = \cos(\theta_2)$（假设目标是类别1）。这等价于 $(w_1 - w_2)^\top x = m$，仍然是一个仿射超平面，但它被平移了，从而在原始的角平分面旁边创建了一个间隔区域。

- **ArcFace**: 它在**角度空间**中引入一个加性间隔 $m$。修改后的logit变为 $\cos(\theta_y + m)$。这要求查询向量 $x$ 与其目标原型 $w_y$ 的夹角必须比它与任何其他原型 $w_j$ 的夹角小至少 $m$ 度，才能达到相同的logit值。[决策边界](@entry_id:146073)的方程变为 $\cos(\theta_1+m) = \cos(\theta_2)$，即 $\theta_1+m=\theta_2$。这个方程在 $x$ 的分量中是[非线性](@entry_id:637147)的，它在超球面上定义了一个**弯曲的超曲面**作为[决策边界](@entry_id:146073)。

这两种方法都有效地迫使模型学习到类内更紧凑、类间更分散的嵌入。ArcFace的间隔直接在角度空间中定义，因此是恒定的；而CosFace的间隔在角度空间中的大小则依赖于类别原型之间的分离程度 $\Delta$ 。

### [嵌入空间](@entry_id:637157)的性质与对称性

学习到的[嵌入空间](@entry_id:637157)不仅仅是点的集合，它还拥有深刻的几何性质和对称性，这些特性对其泛化能力至关重要。

一个核心性质是**[旋转不变性](@entry_id:137644)（rotational invariance）**。由于我们通常使用欧几里得距离或余弦相似度，这些度量在[坐标系](@entry_id:156346)的旋转（或更一般的**正交变换**）下是不变的。也就是说，如果你将空间中的所有点（包括训练数据和查询点）用同一个[正交矩阵](@entry_id:169220) $Q$ (满足 $Q^\top Q = I$) 进行变换，它们之间的所有成对距离和角度都将保持不变。

这个性质对基于距离的算法（如**k-近邻（k-Nearest Neighbors, kNN）**）有着直接的影响。如果同时对[训练集](@entry_id:636396)和查询点进行旋转，kNN分类器的决策结果将完全不变 。这种对称性是嵌入方法泛化能力的一个来源：模型学习到的是点之间的相对几何关系，而不是它们在某个任意[坐标系](@entry_id:156346)下的绝对位置。

这种不变性也催生了**[Procrustes分析](@entry_id:178503)**等技术，用于对齐两个不同的[嵌入空间](@entry_id:637157)。例如，如果我们用两种不同语言（如英语和西班牙语）分别训练[词嵌入](@entry_id:633879)模型，它们可能会学习到相似的几何结构（例如，“国王-男人+女人” 向量都接近 “女王”）。我们可以通过寻找一个最优的[正交变换](@entry_id:155650) $Q^\star$ 来将一个空间对齐到另一个空间，从而实现跨语言的词汇翻译 。

然而，需要强调的是，这种[不变性](@entry_id:140168)仅限于正交变换。如果应用一个非正交的线性变换，如**[各向异性缩放](@entry_id:261477)（anisotropic scaling）**或**剪切（shear）**，向量间的欧几里得距离通常会改变，从而可能改变kNN等算法的决策结果 。

另一个更深层次的观点是**[流形假设](@entry_id:275135)（manifold hypothesis）**，即[高维数据](@entry_id:138874)点实际上可能位于一个嵌入在环境空间中的低维[流形](@entry_id:153038)上。我们可以通过将嵌入向量三元组 $(x, y, z)$ 视为超球面上的一个**球面三角形**的顶点来探测这种潜在的几何结构。通过计算球面三角形的边长（[测地线](@entry_id:269969)距离）和内角，我们可以计算其**角余（angle excess）** $E = A+B+C-\pi$ 。在一个标准的[单位球](@entry_id:142558)面上，这个角余总是正的，并且等于三角形的面积。通过将角余与由弦构成的欧几里得三角形的面积进行比较，我们可以估算出局部的**[高斯曲率](@entry_id:149725)（Gaussian curvature）**。这种分析使我们能够超越欧几里得和标准[球面几何](@entry_id:268217)，去探究学习到的表示空间本身是否具有内在的、非平凡的曲率。

### 实用机制与正则化

将理论转化为成功的实践需要一系列的机制和[正则化技术](@entry_id:261393)来[稳定训练](@entry_id:635987)过程并提升泛化能力。

#### 归一化技术

- **$L_2$范数归一化**: 如前所述，这是创建超球面嵌入的标准步骤，它[解耦](@entry_id:637294)了向量的方向和长度，使学习聚焦于角度关系。

- **批归一化 (Batch Normalization, BN)**: 在深度网络中，BN通过在每个mini-batch内部对激活值进行标准化，来稳定和加速训练。然而，当BN层位于嵌入模块的末端时，它的动态特性会对嵌入的稳定性产生影响。BN维护着均值和[方差](@entry_id:200758)的**指数移动平均（exponential moving averages）**，用于在推理时进行归一化。这些[移动平均](@entry_id:203766)值的更新由一个**动量（momentum）**参数 $m$ 控制。如果底层数据[分布](@entry_id:182848)是变化的（非平稳的），而动量设置不当（例如，$m$ 过高导致更新缓慢），那么推理时使用的统计数据可能与当前的实际数据[分布](@entry_id:182848)不匹配。这会导致训练时和推理时的嵌入之间产生偏差，造成所谓的**嵌入漂移（embedding drift）**，影响模型的性能 。

#### 正则化策略

[正则化技术](@entry_id:261393)通过向[损失函数](@entry_id:634569)添加惩罚项来约束模型的复杂度，从而[防止过拟合](@entry_id:635166)，并引导[嵌入空间](@entry_id:637157)形成更理想的几何结构。

- **范数正则化**: 即使我们最终会对嵌入进行$L_2$归一化，在归一化之前控制[向量的范数](@entry_id:154882)仍然很重要。过大的激活值可能导致[点积](@entry_id:149019)和[指数函数](@entry_id:161417)出现数值[溢出](@entry_id:172355)，并产生过于尖锐的损失[曲面](@entry_id:267450)。在损失函数中加入对权重或嵌入范数的$L_2$惩罚（也称为**[权重衰减](@entry_id:635934)**）是一种常见的做法，有助于防止范数爆炸并[稳定训练](@entry_id:635987) 。

- **[标签平滑](@entry_id:635060) (Label Smoothing)**: 在[分类任务](@entry_id:635433)中，标准的[交叉熵损失](@entry_id:141524)会驱使模型为正确类别输出极高的logit值，而为其他类别输出极低的logit值，这可能导致模型过于自信和[过拟合](@entry_id:139093)。**[标签平滑](@entry_id:635060)**通过将硬性的one-hot目标标签（如$[0, 1, 0]$）替换为软化的[目标分布](@entry_id:634522)（如$[0.05, 0.9, 0.05]$）来缓解这一问题。这种[正则化技术](@entry_id:261393)鼓励模型产生更有限的logit值，从而可以减小决策间隔的[方差](@entry_id:200758)，并使模型在类别边界附近的模糊区域表现得更“不确定”，这通常会提高模型的泛化能力 。

- **稀疏性正则化 ($L_1$正则化)**: 我们可以将嵌入的每个维度看作是对某个基础“概念”的激活。从这个**群体编码（population code）**的视角来看，我们可能希望嵌入是**稀疏的**，即对于任何给定的输入，只有少数几个维度具有显著的非零值。这可以通过在[损失函数](@entry_id:634569)中添加$L_1$惩罚项来实现。[稀疏性](@entry_id:136793)可以提高嵌入的[可解释性](@entry_id:637759)，并可能通过减少不相关维度之间的“[串扰](@entry_id:136295)”来降低不相关项目之间的虚假相似性。然而，过度的稀疏化会限制模型的表达能力，导致无法学习平滑的函数。因此，稀疏性是一个需要在模型性能和可解释性之间进行权衡的设计选择 。

- **几何正则化**: 除了上述间接影响几何的[正则化方法](@entry_id:150559)外，还可以直接对[嵌入空间](@entry_id:637157)的几何结构进行正则化。例如，在处理来自不同[数据增强](@entry_id:266029)视图的嵌入时，我们期望它们能保持相似的内部几何关系。一种先进的方法是计算每个视图内部的成对相似度[分布](@entry_id:182848)，然后使用**[Wasserstein距离](@entry_id:147338)**（或称[推土机距离](@entry_id:147338)）来惩罚这两个[分布](@entry_id:182848)之间的差异。这种正则化器直接强制模型学习在不同视图下保持一致的几何结构，是一种非常强大的结构性约束 。