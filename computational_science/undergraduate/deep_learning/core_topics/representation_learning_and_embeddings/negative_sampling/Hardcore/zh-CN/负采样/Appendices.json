{
    "hands_on_practices": [
        {
            "introduction": "负采样不仅是一种计算技巧，更可以从统计风险最小化的角度来理解。这个练习  将引导你从第一性原理出发，推导最优评分函数，并量化在训练和测试数据分布不一致（即分布偏移）时，使用不匹配的负采样分布所带来的“错配成本”。这项实践将为你深入理解负采样分布选择的重要性奠定坚实的理论基础。",
            "id": "3156698",
            "problem": "考虑一个具有有限结果集的离散词汇表。设结果的训练分布表示为 $p_{\\text{train}}$，测试分布表示为 $p_{\\text{test}}$。在负采样设置中，对于每个真实（正）样本，从一个噪声分布 $q$ 中独立抽取 $k$ 个噪声（负）样本。负采样分类器对一个结果 $w$ 使用标量分数函数 $s(w)$，其对偶对 $(w, \\text{label})$ 的代理损失基于logistic函数。具体来说，一个真实样本的损失定义为 $s(w)$ 的logistic函数的负对数，而一个噪声样本的损失定义为 $-s(w)$ 的logistic函数的负对数。\n\n噪声分布通过一个指数 $\\alpha \\in [0,1]$ 与数据频率相关联。对于词汇表上的任意分布 $r$，定义噪声分布 $q^{(\\alpha)}(r)$ 的方法是：将每个概率提升到 $\\alpha$ 次幂然后进行归一化，即对于每个结果 $w$，有 $q^{(\\alpha)}(r)(w) \\propto r(w)^{\\alpha}$，并进行归一化以确保 $\\sum_{w} q^{(\\alpha)}(r)(w) = 1$。\n\n假设一个模型被训练以最小化在由真实样本的 $p_{\\text{train}}$ 和噪声样本的 $q_{\\text{train}} = q^{(\\alpha)}(p_{\\text{train}})$ 构成的训练混合分布下的期望代理损失。在测试时，真实的分布是 $p_{\\text{test}}$，并考虑两种评估机制：\n- 机制A：负样本继续从 $q_{\\text{train}}$ 中抽取。\n- 机制B：负样本从 $q_{\\text{test}} = q^{(\\alpha)}(p_{\\text{test}})$ 中抽取。\n\n从logistic函数 $\\sigma(x)$ 和伯努利交叉熵的基本定义，以及真实样本和噪声样本混合抽取的期望负采样代理风险出发，依据第一性原理推导：\n1. 当真实结果遵循分布 $p$、噪声结果遵循分布 $q$ 且每个真实样本有 $k$ 个负样本时，对于一个通用分数函数 $s(w)$ 的期望代理风险泛函。\n2. 对于给定的 $(p,q,k)$，最小化期望代理风险的贝叶斯最优分数函数 $s^{\\star}(w)$。\n3. 一个定量的失配成本，定义为由训练好的分数函数 $s_{\\text{train}}$ 实现的期望代理风险与测试混合分布下的最小期望代理风险之差。计算机制A（评估噪声为 $q_{\\text{train}}$）和机制B（评估噪声为 $q_{\\text{test}}$）下的该失配成本。\n\n实现一个程序，对下面指定的每个测试用例，计算两个浮点数：\n- 机制A的失配成本，定义为在使用 $s_{\\text{train}}$ 时，在 $(p_{\\text{test}}, q_{\\text{train}}, k)$ 下的期望代理风险，减去在 $(p_{\\text{test}}, q_{\\text{train}}, k)$ 下的最小期望代理风险。\n- 机制B的失配成本，定义类似，但用 $q_{\\text{test}}$ 代替 $q_{\\text{train}}$。\n\n使用以下测试套件，其中每个概率向量是针对一个包含五个结果的词汇表，且总和为1，并且 $q^{(\\alpha)}(\\cdot)$ 按前述方法计算。所有概率和参数均为严格正值且科学上合理。\n\n- 测试用例 1（一般性偏移，中等大小的 k）：\n  - $p_{\\text{train}} = (0.40, 0.25, 0.20, 0.10, 0.05)$\n  - $p_{\\text{test}} = (0.10, 0.15, 0.25, 0.30, 0.20)$\n  - $\\alpha = 0.75$\n  - $k = 5$\n\n- 测试用例 2（无偏移，训练集与测试集相同）：\n  - $p_{\\text{train}} = (0.20, 0.20, 0.20, 0.20, 0.20)$\n  - $p_{\\text{test}} = (0.20, 0.20, 0.20, 0.20, 0.20)$\n  - $\\alpha = 0.50$\n  - $k = 5$\n\n- 测试用例 3（均匀噪声，较大的 k）：\n  - $p_{\\text{train}} = (0.55, 0.15, 0.10, 0.10, 0.10)$\n  - $p_{\\text{test}} = (0.10, 0.25, 0.25, 0.20, 0.20)$\n  - $\\alpha = 0.00$\n  - $k = 10$\n\n- 测试用例 4（强烈偏移，较小的 k）：\n  - $p_{\\text{train}} = (0.70, 0.10, 0.10, 0.05, 0.05)$\n  - $p_{\\text{test}} = (0.05, 0.65, 0.10, 0.10, 0.10)$\n  - $\\alpha = 1.00$\n  - $k = 1$\n\n你的程序必须生成单行输出，包含一个由方括号括起来的逗号分隔列表。对于每个测试用例，依次输出机制A的失配成本和机制B的失配成本。因此，最终输出必须按顺序包含对应于四个测试用例的八个数字。将每个数字表示为四舍五入到六位小数的浮点值，不带单位，并严格按照 $[r_1,r_2,\\dots,r_8]$ 的格式打印。",
            "solution": "该问题要求推导在负采样设置中的期望代理风险、相应的贝叶斯最优分数函数，并计算在两种不同评估机制下的失配成本。\n\n分析按要求分三步进行。首先，我们推导期望代理风险泛函。其次，我们找到最小化此风险的分数函数。第三，我们使用这些结果来定义并随后计算指定测试用例的失配成本。\n\n设 $\\sigma(x) = (1 + e^{-x})^{-1}$ 为logistic函数。一个正样本 $w$ 的损失由 $L_{+} = -\\log(\\sigma(s(w)))$ 给出，一个负（噪声）样本 $w$ 的损失为 $L_{-} = -\\log(\\sigma(-s(w)))$。我们可以使用恒等式 $\\log(\\sigma(x)) = -\\log(1+e^{-x})$ 重写它们：\n$L_{+} = \\log(1+e^{-s(w)})$\n$L_{-} = \\log(1+e^{s(w)})$\n\n### 1. 期望代理风险泛函\n\n负采样过程包括，对于每个从数据分布 $p$ 中抽取的真实结果 $w_{\\text{real}}$，从噪声分布 $q$ 中独立抽取 $k$ 个噪声结果 $w_{\\text{noise},1}, \\dots, w_{\\text{noise},k}$。这个“事件”（一个真实样本及其关联的 $k$ 个负样本）的总损失是各个损失之和：\n$$\nL_{\\text{event}}(w_{\\text{real}}, w_{\\text{noise},1}, \\dots, w_{\\text{noise},k}) = -\\log(\\sigma(s(w_{\\text{real}}))) - \\sum_{i=1}^{k} \\log(\\sigma(-s(w_{\\text{noise},i})))\n$$\n期望代理风险，记作 $R(s; p, q, k)$，是此总损失在所有可能的真实样本和噪声样本选择（根据它们各自的分布进行抽取）上的期望。根据期望的线性性质，我们可以将真实样本和噪声样本的贡献分开：\n$$\nR(s; p, q, k) = \\mathbb{E}_{w_{\\text{real}} \\sim p, \\{w_{\\text{noise},i}\\}_{i=1}^k \\sim q} \\left[ -\\log(\\sigma(s(w_{\\text{real}}))) - \\sum_{i=1}^{k} \\log(\\sigma(-s(w_{\\text{noise},i}))) \\right]\n$$\n$$\nR(s; p, q, k) = \\mathbb{E}_{w_{\\text{real}} \\sim p} [-\\log(\\sigma(s(w_{\\text{real}})))] + \\sum_{i=1}^{k} \\mathbb{E}_{w_{\\text{noise},i} \\sim q} [-\\log(\\sigma(-s(w_{\\text{noise},i})))]\n$$\n由于噪声样本是同分布的，它们的期望损失是相同的。因此，我们可以简化这个和：\n$$\nR(s; p, q, k) = \\mathbb{E}_{w \\sim p} [-\\log(\\sigma(s(w)))] + k \\cdot \\mathbb{E}_{w' \\sim q} [-\\log(\\sigma(-s(w')))]\n$$\n对于一个离散词汇表 $V$，这个期望表示为对所有结果 $w \\in V$ 的求和：\n$$\nR(s; p, q, k) = \\sum_{w \\in V} p(w) [-\\log(\\sigma(s(w)))] + k \\sum_{w \\in V} q(w) [-\\log(\\sigma(-s(w)))]\n$$\n使用数值稳定的对数形式，表达式为：\n$$\nR(s; p, q, k) = \\sum_{w \\in V} \\left[ p(w)\\log(1+e^{-s(w)}) + k \\, q(w)\\log(1+e^{s(w)}) \\right]\n$$\n这就是所求的期望代理风险泛函。\n\n### 2. 贝叶斯最优分数函数\n\n为了找到最小化 $R(s; p, q, k)$ 的贝叶斯最优分数函数 $s^{\\star}(w)$，我们可以对每个结果 $w$ 独立地关于 $s(w)$ 优化风险，因为总风险是各项之和，每一项仅依赖于一个 $s(w)$。让我们考虑与单个结果 $w_j \\in V$ 相关的风险分量：\n$$\nR_j(s(w_j)) = p(w_j)\\log(1+e^{-s(w_j)}) + k \\, q(w_j)\\log(1+e^{s(w_j)})\n$$\n我们通过将关于 $s(w_j)$ 的导数设为零来找到最小值。我们使用导数公式 $\\frac{d}{dx}\\log(1+e^{-x}) = \\frac{-e^{-x}}{1+e^{-x}} = -\\sigma(-x)$ 和 $\\frac{d}{dx}\\log(1+e^{x}) = \\frac{e^{x}}{1+e^{x}} = \\sigma(x)$。\n$$\n\\frac{\\partial R_j}{\\partial s(w_j)} = p(w_j) (-\\sigma(-s(w_j))) + k \\, q(w_j) (\\sigma(s(w_j))) = 0\n$$\n在最优点 $s^{\\star}(w_j)$ 处，这得到：\n$$\np(w_j) \\sigma(-s^{\\star}(w_j)) = k \\, q(w_j) \\sigma(s^{\\star}(w_j))\n$$\n使用 $\\sigma(-x) = 1 - \\sigma(x)$：\n$$\np(w_j) (1 - \\sigma(s^{\\star}(w_j))) = k \\, q(w_j) \\sigma(s^{\\star}(w_j))\n$$\n重新整理以求解 $\\sigma(s^{\\star}(w_j))$：\n$$\np(w_j) = p(w_j)\\sigma(s^{\\star}(w_j)) + k \\, q(w_j)\\sigma(s^{\\star}(w_j)) = \\sigma(s^{\\star}(w_j)) (p(w_j) + k \\, q(w_j))\n$$\n$$\n\\sigma(s^{\\star}(w_j)) = \\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}\n$$\n为了求出 $s^{\\star}(w_j)$ 本身，我们使用 logit 函数 $\\text{logit}(y) = \\log(y/(1-y))$ 来反转logistic函数：\n$$\ns^{\\star}(w_j) = \\text{logit}\\left(\\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}\\right) = \\log\\left( \\frac{\\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}}{1 - \\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}} \\right)\n$$\n$$\ns^{\\star}(w_j) = \\log\\left( \\frac{\\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}}{\\frac{k \\, q(w_j)}{p(w_j) + k \\, q(w_j)}} \\right) = \\log\\left(\\frac{p(w_j)}{k \\, q(w_j)}\\right)\n$$\n因此，对于给定的分布集 $(p,q)$ 和参数 $k$，贝叶斯最优分数函数为 $s^{\\star}(w) = \\log(p(w)) - \\log(k) - \\log(q(w))$。\n\n### 3. 失配成本计算\n\n失配成本是由于使用非最优分数函数而产生的额外风险。一个在 $(p_{\\text{train}}, q_{\\text{train}}, k)$ 上训练的模型学习到的分数函数为：\n$$\ns_{\\text{train}}(w) = s^{\\star}(w; p_{\\text{train}}, q_{\\text{train}}, k) = \\log\\left(\\frac{p_{\\text{train}}(w)}{k \\, q_{\\text{train}}(w)}\\right)\n$$\n其中 $q_{\\text{train}}(w) \\propto p_{\\text{train}}(w)^{\\alpha}$。\n\n**机制A：** 评估设置为 $(p_{\\text{test}}, q_{\\text{train}}, k)$。\n训练好的模型产生的风险为 $R(s_{\\text{train}}; p_{\\text{test}}, q_{\\text{train}}, k)$。\n此机制下的最小可能风险由该机制的最优分数函数实现：\n$$\ns^{\\star}_{\\text{test},A}(w) = s^{\\star}(w; p_{\\text{test}}, q_{\\text{train}}, k) = \\log\\left(\\frac{p_{\\text{test}}(w)}{k \\, q_{\\text{train}}(w)}\\right)\n$$\n相应的最小风险为 $R(s^{\\star}_{\\text{test},A}; p_{\\text{test}}, q_{\\text{train}}, k)$。\n机制A的失配成本是以下差值：\n$$\n\\text{Cost}_A = R(s_{\\text{train}}; p_{\\text{test}}, q_{\\text{train}}, k) - R(s^{\\star}_{\\text{test},A}; p_{\\text{test}}, q_{\\text{train}}, k)\n$$\n\n**机制B：** 评估设置为 $(p_{\\text{test}}, q_{\\text{test}}, k)$，其中 $q_{\\text{test}}(w) \\propto p_{\\text{test}}(w)^{\\alpha}$。\n训练好的模型产生的风险为 $R(s_{\\text{train}}; p_{\\text{test}}, q_{\\text{test}}, k)$。\n此机制下的最小可能风险由该机制的最优分数函数实现：\n$$\ns^{\\star}_{\\text{test},B}(w) = s^{\\star}(w; p_{\\text{test}}, q_{\\text{test}}, k) = \\log\\left(\\frac{p_{\\text{test}}(w)}{k \\, q_{\\text{test}}(w)}\\right)\n$$\n相应的最小风险为 $R(s^{\\star}_{\\text{test},B}; p_{\\text{test}}, q_{\\text{test}}, k)$。\n机制B的失配成本是以下差值：\n$$\n\\text{Cost}_B = R(s_{\\text{train}}; p_{\\text{test}}, q_{\\text{test}}, k) - R(s^{\\star}_{\\text{test},B}; p_{\\text{test}}, q_{\\text{test}}, k)\n$$\n\n计算的算法如下：\n1.  对每个测试用例，给定 $p_{\\text{train}}$、$p_{\\text{test}}$、$\\alpha$ 和 $k$。\n2.  构建噪声分布 $q_{\\text{train}}(w) = \\frac{p_{\\text{train}}(w)^\\alpha}{\\sum_{w'} p_{\\text{train}}(w')^\\alpha}$ 和 $q_{\\text{test}}(w) = \\frac{p_{\\text{test}}(w)^\\alpha}{\\sum_{w'} p_{\\text{test}}(w')^\\alpha}$。\n3.  使用推导出的公式计算分数函数 $s_{\\text{train}}$、$s^{\\star}_{\\text{test},A}$ 和 $s^{\\star}_{\\text{test},B}$。\n4.  使用风险泛函 $R(s; p, q, k)$ 计算四个必要的风险值。\n5.  计算差值 $\\text{Cost}_A$ 和 $\\text{Cost}_B$。\n\n对提供的每个测试用例实施此过程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the negative sampling mismatch cost problem for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1 (general shift, moderate k)\n        {\n            \"p_train\": np.array([0.40, 0.25, 0.20, 0.10, 0.05]),\n            \"p_test\": np.array([0.10, 0.15, 0.25, 0.30, 0.20]),\n            \"alpha\": 0.75,\n            \"k\": 5\n        },\n        # Test Case 2 (no shift, identical train and test)\n        {\n            \"p_train\": np.array([0.20, 0.20, 0.20, 0.20, 0.20]),\n            \"p_test\": np.array([0.20, 0.20, 0.20, 0.20, 0.20]),\n            \"alpha\": 0.50,\n            \"k\": 5\n        },\n        # Test Case 3 (uniform noise, large k)\n        {\n            \"p_train\": np.array([0.55, 0.15, 0.10, 0.10, 0.10]),\n            \"p_test\": np.array([0.10, 0.25, 0.25, 0.20, 0.20]),\n            \"alpha\": 0.00,\n            \"k\": 10\n        },\n        # Test Case 4 (strong shift, small k)\n        {\n            \"p_train\": np.array([0.70, 0.10, 0.10, 0.05, 0.05]),\n            \"p_test\": np.array([0.05, 0.65, 0.10, 0.10, 0.10]),\n            \"alpha\": 1.00,\n            \"k\": 1\n        },\n    ]\n\n    results = []\n    \n    def compute_q(p, alpha):\n        \"\"\"Computes the noise distribution q from a data distribution p and exponent alpha.\"\"\"\n        p_pow_alpha = np.power(p, alpha)\n        normalization_constant = np.sum(p_pow_alpha)\n        return p_pow_alpha / normalization_constant\n\n    def risk_functional(s, p, q, k):\n        \"\"\"Computes the expected surrogate risk R(s; p, q, k).\"\"\"\n        # Uses the numerically stable form: R = sum(p*log(1+exp(-s)) + k*q*log(1+exp(s)))\n        # np.logaddexp(0, x) computes log(exp(0) + exp(x)) = log(1 + exp(x))\n        term1 = p * np.logaddexp(0, -s)\n        term2 = k * q * np.logaddexp(0, s)\n        return np.sum(term1 + term2)\n\n    def optimal_score(p, q, k):\n        \"\"\"Computes the Bayes-optimal score function s_star.\"\"\"\n        # s_star(w) = log(p(w) / (k * q(w)))\n        # Probabilities are strictly positive, so no log(0) issues.\n        return np.log(p) - np.log(k) - np.log(q)\n\n    for case in test_cases:\n        p_train = case[\"p_train\"]\n        p_test = case[\"p_test\"]\n        alpha = case[\"alpha\"]\n        k = case[\"k\"]\n\n        # Compute noise distributions\n        q_train = compute_q(p_train, alpha)\n        q_test = compute_q(p_test, alpha)\n        \n        # Compute the score function learned during training\n        s_train = optimal_score(p_train, q_train, k)\n\n        # --- Regime A ---\n        # Evaluation with test data distribution but training noise distribution\n        \n        # Optimal score for Regime A's setting (p_test, q_train)\n        s_star_test_A = optimal_score(p_test, q_train, k)\n        \n        # Risk of the trained model in Regime A\n        risk_A_trained = risk_functional(s_train, p_test, q_train, k)\n        \n        # Minimal possible risk in Regime A\n        risk_A_optimal = risk_functional(s_star_test_A, p_test, q_train, k)\n        \n        cost_A = risk_A_trained - risk_A_optimal\n        results.append(cost_A)\n        \n        # --- Regime B ---\n        # Evaluation with test data distribution and test noise distribution\n        \n        # Optimal score for Regime B's setting (p_test, q_test)\n        s_star_test_B = optimal_score(p_test, q_test, k)\n        \n        # Risk of the trained model in Regime B\n        risk_B_trained = risk_functional(s_train, p_test, q_test, k)\n        \n        # Minimal possible risk in Regime B\n        risk_B_optimal = risk_functional(s_star_test_B, p_test, q_test, k)\n        \n        cost_B = risk_B_trained - risk_B_optimal\n        results.append(cost_B)\n\n    # Format output to 6 decimal places as requested\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在理想的理论模型之外，实际应用中的负采样常常面临“假阴性”（False Negatives）样本的挑战。本练习  将探讨如何构建一个对负样本中的噪声标签不那么敏感的“鲁棒”对比损失函数。你将通过实现一个权重方案来降低可疑假阴性样本对损失的贡献，从而掌握提升模型稳定性的关键技术。",
            "id": "3156749",
            "problem": "在一个带有负采样的对比学习设置中，给定一个正样本对分数和一组负样本对分数。假设使用标准的 softmax 交叉熵作为基础：对于一个 logits 向量 $\\{z_j\\}$，softmax 函数分配的概率为 $\\exp(z_j)/\\sum_\\ell \\exp(z_\\ell)$，而真实类别的交叉熵损失是该概率的负对数。\n\n从这个基础出发，考虑一个场景，其中有一个正样本 logit $z_{+}$ 和 $K$ 个负样本 logits $\\{z_i^{-}\\}_{i=1}^{K}$，每个 logit 都是通过相似度分数 $\\{s\\}$ 和一个逆温度 $\\tau$ 由 $z = s / \\tau$ 构建的。在标准的带负采样的对比学习中，一个负样本被确定地视为负样本。然而，在存在带噪声的负样本的情况下，一个负样本 $i$ 实际上是正样本的概率为 $\\epsilon_i$（未知），并且您拥有对每个负样本的估计值 $\\hat{\\epsilon}_i \\in [0,1]$。一种鲁棒的方法旨在根据每个负样本 $i$ 的估计“纯净度”对其贡献进行降权，使用的权重为 $w_i \\propto (1 - \\hat{\\epsilon}_i)$，并选择一种归一化方式，使得当 $\\sum_{i=1}^{K} (1 - \\hat{\\epsilon}_i)  0$ 时，$\\sum_{i=1}^{K} w_i = K$，否则所有 $i$ 的 $w_i = 0$。\n\n您的任务是：\n- 从 softmax 交叉熵基础出发，为一个正样本和 $K$ 个负样本推导无权重的对比损失。\n- 通过将每个负样本的未归一化贡献替换为 $w_i \\exp(z_i^{-})$ 来推导其带权重的对应项，其中 $w_i$ 的定义如上。在带噪声的负样本模型下，使用重要性加权逻辑来证明这一选择的合理性。\n- 在无权重和带权重两种情况下，推导损失函数关于任意负样本 logit $z_i^{-}$ 的梯度。\n- 实现一个数值稳定的程序，该程序：\n  1. 对于给定的 $s_{+}$、$\\{s_i^{-}\\}$、$\\tau$ 和 $\\{\\hat{\\epsilon}_i\\}$，计算无权重损失 $L_{\\mathrm{std}}$ 和带权重损失 $L_{\\mathrm{rob}}$。\n  2. 在两种设置下，计算关于指定负样本 logit 的梯度大小，并返回该索引下它们的比率 $r = \\|\\partial L_{\\mathrm{rob}}/\\partial z_i^{-}\\| / \\|\\partial L_{\\mathrm{std}}/\\partial z_i^{-}\\|$。\n\n数值稳定性要求：使用 log-sum-exp 技巧或等效的方法来实现所有类似 softmax 的求和，以确保对于大数值的 logits 也能保持稳定。\n\n角度和物理单位不适用。所有输出必须是下面指定的实数或整数。\n\n需要实现和评估的测试套件：\n- 测试 1 (纯净负样本，不变性)：$s_{+} = 2.5$，$\\{s_i^{-}\\} = [-0.2, 0.1, -0.5, 0.3]$，$\\tau = 0.5$，$\\hat{\\epsilon} = [0, 0, 0, 0]$。输出绝对差值 $\\lvert L_{\\mathrm{std}} - L_{\\mathrm{rob}} \\rvert$（浮点数）。\n- 测试 2 (单个明显假负样本，鲁棒性)：$s_{+} = 0.8$，$\\{s_i^{-}\\} = [0.75, -0.1, -0.2]$，$\\tau = 0.2$，$\\hat{\\epsilon} = [1, 0, 0]$。如果 $L_{\\mathrm{rob}}  L_{\\mathrm{std}}$ 则输出整数 $1$，否则输出 $0$。\n- 测试 3 (混合噪声加权)：$s_{+} = 1.0$，$\\{s_i^{-}\\} = [0.9, 0.1, -0.3, 0.2]$，$\\tau = 0.7$，$\\hat{\\epsilon} = [0.8, 0.2, 0, 0.5]$。按顺序输出 $L_{\\mathrm{std}}$ 和 $L_{\\mathrm{rob}}$（两个浮点数）。\n- 测试 4 (所有负样本均被预测为受污染，边界情况)：$s_{+} = 1.2$，$\\{s_i^{-}\\} = [1.1, 0.9]$，$\\tau = 0.3$，$\\hat{\\epsilon} = [1, 1]$。输出 $L_{\\mathrm{rob}}$（浮点数）。\n- 测试 5 (对完全受污染的负样本进行梯度抑制)：重用测试 2 的数据，并计算索引为 0 的负样本的梯度比率 $r$。输出 $r$（浮点数）。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[x_1,x_2,x_3,x_4,x_5,x_6]$）。\n- 对于上述五个测试，您的程序必须输出一个列表，按顺序包含：测试 1 的浮点数、测试 2 的整数、测试 3 的两个浮点数、测试 4 的浮点数，最后是测试 5 的浮点数，即 $[T1,T2,T3\\_L\\_{\\mathrm{std}},T3\\_L\\_{\\mathrm{rob}},T4,T5]$。",
            "solution": "该问题要求基于 softmax 交叉熵和重要性采样的原理，推导和实现标准及鲁棒的对比损失函数及其梯度。我们将进行严谨的、分步的推导。\n\n### 第 1 部分：无权重对比损失 ($L_{\\mathrm{std}}$) 的推导\n\n损失函数的基础是 softmax 交叉熵框架。对于一组 logits $\\{z_j\\}$，softmax 函数计算第 $j$ 个类别的概率为：\n$$ P_j = \\frac{\\exp(z_j)}{\\sum_{\\ell} \\exp(z_\\ell)} $$\n对于真实类别（例如类别 $c$），交叉熵损失是其预测概率的负对数：\n$$ L = -\\log(P_c) $$\n在我们的对比学习设置中，我们有一个分数为 $s_+$ 的正样本对和 $K$ 个分数为 $\\{s_i^{-}\\}_{i=1}^{K}$ 的负样本对。Logits 由 $z = s / \\tau$ 给出，其中 $\\tau$ 是一个逆温度参数。这产生一个正样本 logit $z_+ = s_+ / \\tau$ 和 $K$ 个负样本 logits $z_i^{-} = s_i^{-} / \\tau$。\n\n用于分类任务的所有 logits 的集合是 $\\{z_+\\} \\cup \\{z_i^{-}\\}_{i=1}^{K}$。“真实类别”是正样本对。因此，分配给正样本对的概率是：\n$$ P(\\text{positive}) = \\frac{\\exp(z_+)}{\\exp(z_+) + \\sum_{i=1}^{K} \\exp(z_i^{-})} $$\n标准对比损失 $L_{\\mathrm{std}}$ 是正确识别正样本对的负对数似然：\n$$ L_{\\mathrm{std}} = -\\log\\left(\\frac{\\exp(z_+)}{\\exp(z_+) + \\sum_{i=1}^{K} \\exp(z_i^{-})}\\right) $$\n使用对数属性 $\\log(a/b) = \\log(a) - \\log(b)$，我们可以将损失重写为：\n$$ L_{\\mathrm{std}} = -\\left( \\log(\\exp(z_+)) - \\log\\left(\\exp(z_+) + \\sum_{i=1}^{K} \\exp(z_i^{-})\\right) \\right) $$\n$$ L_{\\mathrm{std}} = -z_+ + \\log\\left(\\exp(z_+) + \\sum_{i=1}^{K} \\exp(z_i^{-})\\right) $$\n这是 InfoNCE 损失的一种常见形式。\n\n### 第 2 部分：带权重对比损失 ($L_{\\mathrm{rob}}$) 的推导与论证\n\n标准损失假设所有采样的负样本都是真正的负样本。在实际场景中，一些负样本可能是“假负样本”（即，语义上是正样本对，但被意外地采样为负样本）。设 $\\epsilon_i$ 为第 $i$ 个负样本实际上是正样本的概率。标准损失是在“纯净”负样本分布上的期望估计量，但我们采样自一个带噪声的分布。\n\n重要性采样为纠正这种分布不匹配提供了一种有原则的方法。我们希望对可能是假负样本的样本的贡献进行降权。问题为 $\\epsilon_i$ 提供了每个负样本的估计值 $\\hat{\\epsilon}_i$。一个样本 $i$ 被估计为真负样本的概率是 $1 - \\hat{\\epsilon}_i$。因此，我们可以通过一个与其估计的“纯净度” $(1 - \\hat{\\epsilon}_i)$ 成正比的因子，来重新加权配分函数（softmax 的分母）中每个负样本 logit 的贡献。\n\n问题将权重 $w_i$ 定义为：\n$$ w_i = \\begin{cases} \\frac{K (1 - \\hat{\\epsilon}_i)}{\\sum_{j=1}^{K} (1 - \\hat{\\epsilon}_j)}  \\text{if } \\sum_{j=1}^{K} (1 - \\hat{\\epsilon}_j)  0 \\\\ 0  \\text{otherwise} \\end{cases} $$\n这种归一化确保了当至少有一个负样本不是完全受污染时，$\\sum_{i=1}^{K} w_i = K$，从而在所有负样本都是纯净的（即所有 $\\hat{\\epsilon}_i = 0$，这意味着所有 $w_i = 1$）情况下，保持了配分函数相对于无权重情况的尺度。\n\n鲁棒损失 $L_{\\mathrm{rob}}$ 是通过将配分函数中的每一项 $\\exp(z_i^{-})$ 替换为其加权对应项 $w_i \\exp(z_i^{-})$ 而形成的。\n$$ P_{\\mathrm{rob}}(\\text{positive}) = \\frac{\\exp(z_+)}{\\exp(z_+) + \\sum_{i=1}^{K} w_i \\exp(z_i^{-})} $$\n对应的损失为：\n$$ L_{\\mathrm{rob}} = -\\log\\left(P_{\\mathrm{rob}}(\\text{positive})\\right) = -z_+ + \\log\\left(\\exp(z_+) + \\sum_{i=1}^{K} w_i \\exp(z_i^{-})\\right) $$\n\n### 第 3 部分：梯度推导\n\n我们现在推导每个损失函数关于任意一个负样本 logit $z_j^{-}$ 的梯度。\n\n**$L_{\\mathrm{std}}$ 的梯度：**\n设 $S_{\\mathrm{std}} = \\exp(z_+) + \\sum_{i=1}^{K} \\exp(z_i^{-})$。损失为 $L_{\\mathrm{std}} = -z_+ + \\log(S_{\\mathrm{std}})$。\n$$ \\frac{\\partial L_{\\mathrm{std}}}{\\partial z_j^{-}} = \\frac{\\partial}{\\partial z_j^{-}} \\left( -z_+ + \\log(S_{\\mathrm{std}}) \\right) = \\frac{1}{S_{\\mathrm{std}}} \\frac{\\partial S_{\\mathrm{std}}}{\\partial z_j^{-}} $$\n和 $S_{\\mathrm{std}}$ 关于 $z_j^{-}$ 的导数是 $\\exp(z_j^{-})$，因为各项是独立的。\n$$ \\frac{\\partial L_{\\mathrm{std}}}{\\partial z_j^{-}} = \\frac{\\exp(z_j^{-})}{\\exp(z_+) + \\sum_{i=1}^{K} \\exp(z_i^{-})} $$\n这是在所有 $K+1$ 个样本集合上为第 $j$ 个负样本计算的 softmax 概率。\n\n**$L_{\\mathrm{rob}}$ 的梯度：**\n设 $S_{\\mathrm{rob}} = \\exp(z_+) + \\sum_{i=1}^{K} w_i \\exp(z_i^{-})$。损失为 $L_{\\mathrm{rob}} = -z_+ + \\log(S_{\\mathrm{rob}})$。权重 $w_i$ 是 $\\{\\hat{\\epsilon}_k\\}$ 的函数，并且相对于 logits $\\{z_k^{-}\\}$ 是常数。\n$$ \\frac{\\partial L_{\\mathrm{rob}}}{\\partial z_j^{-}} = \\frac{\\partial}{\\partial z_j^{-}} \\left( -z_+ + \\log(S_{\\mathrm{rob}}) \\right) = \\frac{1}{S_{\\mathrm{rob}}} \\frac{\\partial S_{\\mathrm{rob}}}{\\partial z_j^{-}} $$\n和 $S_{\\mathrm{rob}}$ 关于 $z_j^{-}$ 的导数是 $w_j \\exp(z_j^{-})$。\n$$ \\frac{\\partial L_{\\mathrm{rob}}}{\\partial z_j^{-}} = \\frac{w_j \\exp(z_j^{-})}{\\exp(z_+) + \\sum_{i=1}^{K} w_i \\exp(z_i^{-})} $$\n这个梯度是 softmax 概率的加权版本。如果一个样本 $j$ 被视为完全受污染（$\\hat{\\epsilon}_j=1$），其权重 $w_j$ 变为 $0$，因此，关于其 logit $z_j^{-}$ 的梯度也变为 $0$。这有效地阻止了模型学习将这个（可能是假的）负样本对推得更远。\n\n### 第 4 部分：数值稳定性\n\n直接计算包含 $\\exp(z)$ 的表达式可能在 $z$ 很大时导致数值上溢，在 $z$ 是非常小的负数时导致下溢。缓解这种情况的标准技巧是 log-sum-exp 技巧。其恒等式为：\n$$ \\log\\left(\\sum_j \\exp(x_j)\\right) = c + \\log\\left(\\sum_j \\exp(x_j - c)\\right), \\quad \\text{where } c = \\max_j(x_j) $$\n这个变换可以防止上溢，因为 $\\exp$ 的最大参数变为 $0$。\n\n对于 $L_{\\mathrm{std}}$，我们直接将此技巧应用于 logits $\\{z_+, z_1^{-}, \\dots, z_K^{-}\\}$。\n对于 $L_{\\mathrm{rob}}$，其和为 $\\exp(z_+) + \\sum_i w_i \\exp(z_i^{-})$。为了数值稳定性，可以通过将对数因子移入指数中来重写此式，对于 $w_i0$：$\\exp(\\log(w_i) + z_i^{-})$。那么损失的表达式为：\n$$ L_{\\mathrm{rob}} = -z_+ + \\log\\left(\\exp(z_+) + \\sum_{i: w_i0} \\exp(\\log(w_i) + z_i^{-})\\right) $$\n我们现在可以将 log-sum-exp 技巧应用于修改后的 logits 集合 $\\{z_+\\} \\cup \\{\\log(w_i) + z_i^{-} \\mid w_i  0\\}$。如果所有的 $w_i=0$，则求和为 $0$ 且 $L_{\\mathrm{rob}} = -z_+ + \\log(\\exp(z_+)) = 0$。\n梯度是类似 softmax 的表达式，也可以用类似的方法来稳定：\n$$ \\frac{\\exp(x_j)}{\\sum_k \\exp(x_k)} = \\frac{\\exp(x_j - c)}{\\sum_k \\exp(x_k - c)}, \\quad c = \\max_k(x_k) $$\n这等价于计算 $\\exp(x_j - \\text{LogSumExp}(\\{x_k\\}))$。这将在实现中使用。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the five test cases specified in the problem statement.\n    \"\"\"\n\n    def compute_loss_and_grad(s_plus, s_neg, tau, eps_hat, grad_idx=None):\n        \"\"\"\n        Computes standard and robust losses and gradients.\n        \"\"\"\n        z_plus = s_plus / tau\n        z_neg = np.array(s_neg) / tau\n        K = len(z_neg)\n\n        # Log-sum-exp utility for numerical stability\n        def log_sum_exp(x):\n            if x.size == 0:\n                return -np.inf  # log(0)\n            c = np.max(x)\n            return c + np.log(np.sum(np.exp(x - c)))\n\n        # --- Part 1: Unweighted Loss and Gradient ---\n        all_logits_std = np.concatenate(([z_plus], z_neg))\n        lse_std = log_sum_exp(all_logits_std)\n        l_std = -z_plus + lse_std\n\n        grad_std = None\n        if grad_idx is not None:\n            # Gradient is the softmax probability of the negative sample\n            grad_std = np.exp(z_neg[grad_idx] - lse_std)\n\n        # --- Part 2: Weighted Loss and Gradient ---\n        # Calculate weights\n        one_minus_eps = 1.0 - np.array(eps_hat)\n        sum_one_minus_eps = np.sum(one_minus_eps)\n        \n        weights = np.zeros(K)\n        # As per problem: weights are non-zero only if sum > 0\n        if sum_one_minus_eps > 0:\n            weights = K * one_minus_eps / sum_one_minus_eps\n\n        # Robust Loss\n        # Handle the case where all negatives are marked as corrupted\n        if sum_one_minus_eps == 0:\n            l_rob = 0.0\n            lse_rob = z_plus # Analytically, log(exp(z_plus))\n        else:\n            # Use log-sum-exp with modified logits for stability\n            valid_mask = weights > 0\n            log_weights = np.log(weights[valid_mask])\n            modified_neg_logits = z_neg[valid_mask] + log_weights\n            \n            all_logits_rob = np.concatenate(([z_plus], modified_neg_logits))\n            lse_rob = log_sum_exp(all_logits_rob)\n            l_rob = -z_plus + lse_rob\n\n        # Robust Gradient\n        grad_rob = None\n        if grad_idx is not None:\n            w_j = weights[grad_idx]\n            if w_j == 0:\n                grad_rob = 0.0\n            else:\n                # Gradient is the weighted softmax probability\n                grad_rob = np.exp(np.log(w_j) + z_neg[grad_idx] - lse_rob)\n        \n        return l_std, l_rob, grad_std, grad_rob\n\n    results = []\n    \n    # Test 1: clean negatives, invariance\n    s_plus, s_neg, tau, eps_hat = 2.5, [-0.2, 0.1, -0.5, 0.3], 0.5, [0, 0, 0, 0]\n    l_std_1, l_rob_1, _, _ = compute_loss_and_grad(s_plus, s_neg, tau, eps_hat)\n    results.append(abs(l_std_1 - l_rob_1))\n    \n    # Test 2: single obvious false negative, robustness\n    s_plus, s_neg, tau, eps_hat = 0.8, [0.75, -0.1, -0.2], 0.2, [1, 0, 0]\n    l_std_2, l_rob_2, _, _ = compute_loss_and_grad(s_plus, s_neg, tau, eps_hat)\n    results.append(1 if l_rob_2  l_std_2 else 0)\n\n    # Test 3: mixed noise weighting\n    s_plus, s_neg, tau, eps_hat = 1.0, [0.9, 0.1, -0.3, 0.2], 0.7, [0.8, 0.2, 0, 0.5]\n    l_std_3, l_rob_3, _, _ = compute_loss_and_grad(s_plus, s_neg, tau, eps_hat)\n    results.append(l_std_3)\n    results.append(l_rob_3)\n\n    # Test 4: all negatives predicted corrupted, boundary\n    s_plus, s_neg, tau, eps_hat = 1.2, [1.1, 0.9], 0.3, [1, 1]\n    _, l_rob_4, _, _ = compute_loss_and_grad(s_plus, s_neg, tau, eps_hat)\n    results.append(l_rob_4)\n\n    # Test 5: gradient suppression for a fully corrupted negative\n    s_plus, s_neg, tau, eps_hat, grad_idx = 0.8, [0.75, -0.1, -0.2], 0.2, [1, 0, 0], 0\n    _, _, grad_std_5, grad_rob_5 = compute_loss_and_grad(s_plus, s_neg, tau, eps_hat, grad_idx)\n    # The gradients are non-negative, so magnitude is the value itself.\n    # grad_std_5 will be non-zero for these inputs.\n    ratio = grad_rob_5 / grad_std_5\n    results.append(ratio)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在解决了假阴性问题后，我们可以更进一步，主动设计最优的采样策略。这个练习  将负采样分布的选择问题建模为一个优化问题。你将学习如何通过在一个混合采样器中寻找最佳平衡点，来最小化梯度估计器的方差，从而将深度学习的实践与重要性采样和方差缩减的理论联系起来。",
            "id": "3156767",
            "problem": "给定一个由 $i \\in \\{1,\\dots,n\\}$ 索引的候选负样本有限集，其项目具有基础数据分布 $p(i)$。考虑估计形式为 $\\mu = \\mathbb{E}_{i \\sim p}[h(i)]$ 的期望，其中 $h(i) \\ge 0$ 量化了在采样目标中每个项目对梯度项的贡献大小。一个使用从提议分布 $q$ 中抽取的 $k$ 个独立负样本的标准无偏重要性采样估计器是\n$$\n\\widehat{\\mu} \\;=\\; \\frac{1}{k}\\sum_{j=1}^{k} \\frac{p(i_j)}{q(i_j)}\\,h(i_j), \\quad i_j \\sim q \\text{ independently}.\n$$\n该估计器的方差为\n$$\n\\mathrm{Var}[\\widehat{\\mu}] \\;=\\; \\frac{1}{k}\\left(\\sum_{i=1}^{n} \\frac{p(i)^{2} h(i)^{2}}{q(i)} \\;-\\; \\mu^{2}\\right),\n$$\n对于固定的 $k$，通过适当地选择 $q$ 可以使其最小化。在负采样实践中，通常使用一个混合采样器，它结合了基于频率的分布和基于难度的分布。定义混合采样器\n$$\nq_{\\lambda}(i) \\;=\\; \\lambda\\, q_{\\mathrm{freq}}(i) \\;+\\; (1-\\lambda)\\, q_{\\mathrm{hard}}(i),\n$$\n其中 $q_{\\mathrm{freq}}(i) = p(i)$ 且 $q_{\\mathrm{hard}}(i) \\propto h(i)$，即，\n$$\nq_{\\mathrm{hard}}(i) \\;=\\; \\frac{h(i)}{\\sum_{j=1}^{n} h(j)}.\n$$\n对于固定的采样预算 $k \\in \\mathbb{N}$，此设置中的泛化误差代理被视为方差 $\\mathrm{Var}[\\widehat{\\mu}]$，因为负项的估计器方差越低，训练信号中的噪声就越小，泛化能力也越好。您的任务是选择混合权重 $\\lambda \\in [0,1]$，以在混合族 $q_{\\lambda}$ 下最小化 $\\mathrm{Var}[\\widehat{\\mu}]$。\n\n从重要性采样的基本定义和上述方差公式出发，推导出一个算法，该算法在给定离散分布 $p$ 和非负权重 $h$ 的情况下，找到最优的 $\\lambda^{\\star} \\in [0,1]$ 以最小化 $\\mathrm{Var}[\\widehat{\\mu}]$。假设在提供的测试用例中，所有 $i$ 的 $h(i)  0$，因此 $q_{\\mathrm{hard}}(i)$ 是良定义且严格为正的。\n\n实现要求：\n- 您的程序必须仅使用当函数\n$$\nJ(\\lambda) \\;=\\; \\sum_{i=1}^{n} \\frac{p(i)^{2} h(i)^{2}}{\\lambda\\,p(i) + (1-\\lambda)\\,q_{\\mathrm{hard}}(i)}\n$$\n在 $\\lambda \\in [0,1]$ 上最小时，方差也最小化这一原理，为 $\\lambda^{\\star}$ 实现一个数值稳定的求解器。\n- 如果有多个 $\\lambda$ 在数值容差 $\\varepsilon = 10^{-12}$ 内达到相同的最小值，则返回其中最小的 $\\lambda$。\n- 对每个测试用例，仅输出四舍五入到四位小数的最优 $\\lambda^{\\star}$。\n\n测试套件：\n- 案例 A (预期为内部最优解): $n=5$, $p = [\\,0.40,\\,0.25,\\,0.20,\\,0.10,\\,0.05\\,]$, $h = [\\,1.0,\\,2.0,\\,2.5,\\,3.0,\\,5.0\\,]$, $k = 10$。\n- 案例 B (预期为边界解 $\\lambda = 1$): $n=4$, $p = [\\,0.50,\\,0.30,\\,0.15,\\,0.05\\,]$, $h = [\\,1.0,\\,1.0,\\,1.0,\\,1.0\\,]$, $k = 7$。\n- 案例 C (可能为边界解 $\\lambda = 0$): $n=5$, $p = [\\,0.40,\\,0.25,\\,0.20,\\,0.10,\\,0.05\\,]$, $h = [\\,0.1,\\,0.1,\\,0.5,\\,4.0,\\,10.0\\,]$, $k = 3$。\n- 案例 D (退化情况 $q_{\\mathrm{hard}} = p$): $n=4$, $p = [\\,0.40,\\,0.30,\\,0.20,\\,0.10\\,]$, $h = [\\,4.0,\\,3.0,\\,2.0,\\,1.0\\,]$, $k = 5$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含案例 A、B、C 和 D 的四个最优值 $\\lambda^{\\star}$，格式为方括号括起来的逗号分隔列表，例如 $[\\lambda_{A},\\lambda_{B},\\lambda_{C},\\lambda_{D}]$，其中每个条目都四舍五入到四位小数。",
            "solution": "目标是找到混合权重 $\\lambda \\in [0,1]$，以最小化重要性采样估计器的方差 $\\mathrm{Var}[\\widehat{\\mu}]$。方差由下式给出：\n$$\n\\mathrm{Var}[\\widehat{\\mu}] = \\frac{1}{k}\\left(\\sum_{i=1}^{n} \\frac{p(i)^{2} h(i)^{2}}{q(i)} - \\mu^{2}\\right)\n$$\n为了关于采样分布 $q$ 的选择最小化 $\\mathrm{Var}[\\widehat{\\mu}]$，我们只需要考虑依赖于 $q$ 的项。由于 $k$、$p(i)$、$h(i)$ 和 $\\mu$ 对于此优化问题是固定的，因此任务等同于最小化求和项。提议分布是一个混合模型 $q_{\\lambda}(i) = \\lambda\\, q_{\\mathrm{freq}}(i) + (1-\\lambda)\\, q_{\\mathrm{hard}}(i)$，其中 $q_{\\mathrm{freq}}(i) = p(i)$ 且 $q_{\\mathrm{hard}}(i) = h(i) / \\sum_{j=1}^{n} h(j)$。将 $q_{\\lambda}(i)$ 替换 $q(i)$，我们得到一个在区间 $\\lambda \\in [0,1]$ 上需要最小化的目标函数 $J(\\lambda)$：\n$$\nJ(\\lambda) = \\sum_{i=1}^{n} \\frac{p(i)^{2} h(i)^{2}}{\\lambda\\,p(i) + (1-\\lambda)\\,q_{\\mathrm{hard}}(i)}\n$$\n这个函数 $J(\\lambda)$ 是各个函数的和，每个函数对应一个项目 $i \\in \\{1, \\dots, n\\}$。我们将每一项表示为 $f_i(\\lambda)$：\n$$\nf_i(\\lambda) = \\frac{c_i}{\\ell_i(\\lambda)}\n$$\n其中 $c_i = p(i)^2 h(i)^2$ 是一个非负常数，分母 $\\ell_i(\\lambda) = \\lambda p(i) + (1-\\lambda) q_{\\mathrm{hard}}(i)$ 是 $\\lambda$ 的线性函数。问题假设对于所有 $i$，$h(i)  0$，这意味着 $q_{\\mathrm{hard}}(i)  0$。由于 $p(i)$ 是 $n$ 个项目上的概率分布，我们可以假设在其支持集中的所有 $i$ 都有 $p(i)0$。因此，对于 $\\lambda \\in [0,1]$，$\\ell_i(\\lambda)$ 是两个严格为正的数 $p(i)$ 和 $q_{\\mathrm{hard}}(i)$ 的凸组合，因此它也严格为正。\n\n为了确定优化问题的性质，我们分析 $J(\\lambda)$ 的凸性。每一项 $f_i(\\lambda)$ 关于 $\\lambda$ 的二阶导数是：\n$$\n\\frac{d^2 f_i}{d\\lambda^2} = \\frac{d^2}{d\\lambda^2} \\left( c_i (\\ell_i(\\lambda))^{-1} \\right) = \\frac{d}{d\\lambda} \\left( -c_i (\\ell_i(\\lambda))^{-2} \\frac{d\\ell_i}{d\\lambda} \\right)\n$$\n分母的导数是 $\\frac{d\\ell_i}{d\\lambda} = p(i) - q_{\\mathrm{hard}}(i)$。所以，\n$$\n\\frac{d^2 f_i}{d\\lambda^2} = -c_i \\left( -2 (\\ell_i(\\lambda))^{-3} \\left(\\frac{d\\ell_i}{d\\lambda}\\right)^2 \\right) = \\frac{2c_i(p(i) - q_{\\mathrm{hard}}(i))^2}{(\\lambda p(i) + (1-\\lambda) q_{\\mathrm{hard}}(i))^3}\n$$\n由于 $c_i \\ge 0$ 且分母为正，因此对于所有 $\\lambda \\in [0,1]$，二阶导数 $\\frac{d^2 f_i}{d\\lambda^2}$ 是非负的。这证明了每个函数 $f_i(\\lambda)$ 都是凸函数。凸函数的和也是凸函数，所以 $J(\\lambda)$ 是在区间 $[0,1]$ 上的一个凸函数。\n\n一个凸函数在闭区间上的最小值出现在边界点（$\\lambda=0$ 或 $\\lambda=1$）之一，或者在一个一阶导数为零的内部点 $\\lambda^{\\star} \\in (0,1)$。$J(\\lambda)$ 的一阶导数是：\n$$\nJ'(\\lambda) = \\frac{dJ}{d\\lambda} = \\sum_{i=1}^{n} \\frac{-c_i(p(i) - q_{\\mathrm{hard}}(i))}{(\\ell_i(\\lambda))^2} = - \\sum_{i=1}^{n} \\frac{p(i)^2 h(i)^2 (p(i) - q_{\\mathrm{hard}}(i))}{(\\lambda p(i) + (1-\\lambda) q_{\\mathrm{hard}}(i))^2}\n$$\n由于 $J(\\lambda)$ 是凸函数，其导数 $J'(\\lambda)$ 是一个单调非递减函数。这个性质使得我们可以高效地搜索最优的 $\\lambda^{\\star}$：\n1. 如果 $J'(0) \\ge 0$，则函数在 $[0,1]$ 上是非递减的，因此最小值在 $\\lambda^{\\star}=0$ 处。\n2. 如果 $J'(1) \\le 0$，则函数在 $[0,1]$ 上是非递增的，因此最小值在 $\\lambda^{\\star}=1$ 处。\n3. 如果 $J'(0)  0$ 且 $J'(1)  0$，则 $J'(\\lambda)$ 的连续性和单调性保证了在 $(0,1)$ 区间内存在唯一的根 $\\lambda^{\\star}$，使得 $J'(\\lambda^{\\star})=0$。这个根对应于 $J(\\lambda)$ 的唯一最小值。\n\n这引出了以下算法：\n首先，计算在边界处的导数 $J'(0)$ 和 $J'(1)$。\n$$\nJ'(0) = - \\sum_{i=1}^{n} \\frac{p(i)^2 h(i)^2 (p(i) - q_{\\mathrm{hard}}(i))}{q_{\\mathrm{hard}}(i)^2}\n$$\n$$\nJ'(1) = - \\sum_{i=1}^{n} \\frac{p(i)^2 h(i)^2 (p(i) - q_{\\mathrm{hard}}(i))}{p(i)^2} = - \\sum_{i=1}^{n} h(i)^2 (p(i) - q_{\\mathrm{hard}}(i))\n$$\n根据它们的符号，我们要么将 $\\lambda^{\\star}$ 确定为 $0$ 或 $1$，要么继续在区间 $(0,1)$ 内寻找 $J'(\\lambda)=0$ 的根。\n对于内部解的情况，$J'(\\lambda)=0$ 是一个非线性方程，可以用数值方法求解。鉴于 $J'(\\lambda)$ 是单调的，并且我们已经将根限定在 $\\lambda=0$ 和 $\\lambda=1$ 之间，二分法是寻找 $\\lambda^{\\star}$ 的一个简单而稳健的选择。二分法算法通过在中间点评估 $J'(mid)$ 并根据其符号更新区间，迭代地将搜索区间 $[low, high]$ 减半，从而保证收敛到唯一的根。在所有 $i$ 都有 $p(i) = q_{\\mathrm{hard}}(i)$ 的特殊情况下，对于所有 $\\lambda \\in [0,1]$ 都有 $J'(\\lambda)=0$，这使得所有 $\\lambda$ 值都是最小值点。问题要求返回最小的那个，即 $\\lambda^{\\star}=0$。这种情况可以由条件 $J'(0) \\ge 0$ 正确处理。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the optimization problem for the given test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'p': [0.40, 0.25, 0.20, 0.10, 0.05], 'h': [1.0, 2.0, 2.5, 3.0, 5.0]},\n        # Case B\n        {'p': [0.50, 0.30, 0.15, 0.05], 'h': [1.0, 1.0, 1.0, 1.0]},\n        # Case C\n        {'p': [0.40, 0.25, 0.20, 0.10, 0.05], 'h': [0.1, 0.1, 0.5, 4.0, 10.0]},\n        # Case D\n        {'p': [0.40, 0.30, 0.20, 0.10], 'h': [4.0, 3.0, 2.0, 1.0]},\n    ]\n\n    def find_optimal_lambda(p, h):\n        \"\"\"\n        Calculates the optimal lambda that minimizes the variance estimator.\n\n        Args:\n            p (list or np.ndarray): The base data distribution.\n            h (list or np.ndarray): The per-item contribution weights.\n\n        Returns:\n            float: The optimal lambda value.\n        \"\"\"\n        p_vec = np.array(p, dtype=np.float64)\n        h_vec = np.array(h, dtype=np.float64)\n\n        sum_h = np.sum(h_vec)\n        q_hard_vec = h_vec / sum_h if sum_h > 0 else np.full_like(p_vec, 1.0/len(p_vec))\n\n        def J_prime(lam, p, h, q_hard):\n            \"\"\"\n            Computes the derivative of the objective function J with respect to lambda.\n            \"\"\"\n            numerator = p**2 * h**2 * (p - q_hard)\n            denominator = (lam * p + (1.0 - lam) * q_hard)**2\n            \n            # To handle potential division by zero in theory, though not \n            # expected with p(i)>0, h(i)>0.\n            # Using np.divide with a where clause is safer than adding an epsilon.\n            terms = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)\n            \n            return -np.sum(terms)\n\n        # A small tolerance for floating-point comparisons near zero.\n        # This is based on the logic that if the derivative is extremely\n        # close to zero at a boundary, we can consider the optimum to be there.\n        tol = 1e-12\n\n        # 1. Check boundary at lambda = 0\n        J_prime_at_0 = J_prime(0.0, p_vec, h_vec, q_hard_vec)\n        if J_prime_at_0 >= -tol:\n            return 0.0\n\n        # 2. Check boundary at lambda = 1\n        J_prime_at_1 = J_prime(1.0, p_vec, h_vec, q_hard_vec)\n        if J_prime_at_1 = tol:\n            return 1.0\n\n        # 3. Find interior minimum using bisection\n        low = 0.0\n        high = 1.0\n        # 100 iterations are sufficient for double-precision floating-point accuracy.\n        for _ in range(100):\n            mid = low + 0.5 * (high - low)\n            # If mid is indistinguishable from low or high, stop.\n            if mid == low or mid == high:\n                break\n            val = J_prime(mid, p_vec, h_vec, q_hard_vec)\n            if val  0:\n                low = mid\n            else:\n                high = mid\n        \n        return (low + high) / 2.0\n\n    results = []\n    for case in test_cases:\n        p_dist = case['p']\n        h_weights = case['h']\n        lambda_star = find_optimal_lambda(p_dist, h_weights)\n        results.append(lambda_star)\n\n    # Format results to four decimal places for the final output string.\n    formatted_results = [f\"{res:.4f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}