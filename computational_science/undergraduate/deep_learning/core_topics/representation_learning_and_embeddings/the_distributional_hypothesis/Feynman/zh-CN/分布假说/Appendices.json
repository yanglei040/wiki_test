{
    "hands_on_practices": [
        {
            "introduction": "分布式假说可以通过两种主要方式实现：从上下文预测单词（如掩码语言模型 MLM），或从单词预测上下文（如 Skip-gram）。本练习  将指导你从零开始，为这两种视角构建简单的、基于计数的词嵌入，即 $p_{\\mathrm{MLM}}(w \\mid c)$ 和 $p_{\\mathrm{SG}}(c \\mid w)$。通过比较最终向量空间的几何特性，你将对这两种表征学习的关键范式建立起基础而直观的理解。",
            "id": "3182958",
            "problem": "给定一个固定的小型语料库，请您使用基于原则的计数估计量，在两个典型的深度学习目标——掩码语言模型 (MLM) 和 Skip-gram (SG) 中，对分布假说进行操作化，并比较由此产生的词嵌入的几何结构。分布假说指出，出现在相似上下文中的词语往往具有相似的含义。您的任务是从核心概率定义和经过充分检验的统计程序出发，推导、实现并比较条件分布以及由此产生的向量空间几何。\n\n语料库（已分词，全部小写，已移除标点符号；每行为一个句子）：\nthe cat sat on the mat\nthe dog sat on the rug\na cat chased a mouse\na dog chased a ball\nmusic and song fill the hall\nthe song played music softly\nquantum theory explains physics\ntheory of music and physics\nthe quantum cat thought of physics\ndogs and cats share a home\na theory about a song\n\n用作基础的基本定义：\n- 条件概率：对于事件 $A$ 和 $B$，$p(A \\mid B) = \\frac{p(A, B)}{p(B)}$，其中 $p(B) > 0$。\n- 多项分布的最大似然估计 (MLE)：给定结果 $\\{i\\}$ 上的计数 $\\{n_i\\}$，$\\hat{p}_i = \\frac{n_i}{\\sum_j n_j}$。\n- 加法（拉普拉斯）平滑：对于 $V$ 个结果上的计数 $\\{n_i\\}$ 和平滑参数 $\\alpha > 0$，定义 $\\tilde{p}_i = \\frac{n_i + \\alpha}{\\sum_j n_j + \\alpha V}$。\n- 余弦相似度：对于 $\\mathbb{R}^d$ 中的向量 $\\mathbf{u}$ 和 $\\mathbf{v}$，$\\mathrm{cos}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^\\top \\mathbf{v}}{\\lVert \\mathbf{u} \\rVert_2 \\lVert \\mathbf{v} \\rVert_2}$。\n\n需要执行的任务：\n1. 在语料库中的每个目标词 $w$ 周围构建一个大小为 $K$ 的对称上下文窗口。对于位置 $i$ 上的每次出现，将所有满足 $|i - j| \\le K$ 且 $j \\ne i$ 的位置 $j$ 上的词作为上下文词 $c$。在整个语料库上累积共现计数 $N(w, c)$。使用整个词元词汇表作为可能的 $w$ 和 $c$ 的集合。\n2. 定义掩码语言模型 (MLM)：从 $c$ 预测 $w$。根据计数 $N(w, c)$，使用带有加法平滑参数 $\\alpha$ 的最大似然估计，推导出与上述定义一致的条件分布 $p_{\\mathrm{MLM}}(w \\mid c)$。\n3. 定义 Skip-gram (SG)：从 $w$ 预测 $c$。根据相同的计数 $N(w, c)$，使用带有加法平滑参数 $\\alpha$ 的最大似然估计，推导出与上述定义一致的条件分布 $p_{\\mathrm{SG}}(c \\mid w)$。\n4. 将每个词 $w$ 嵌入到两个空间中：\n   - MLM 嵌入 $\\mathbf{v}_{\\mathrm{MLM}}(w)$：将 $p_{\\mathrm{MLM}}(w \\mid c)$ 解释为关于 $c$ 的函数，并创建一个其分量由上下文 $c$ 索引的向量。\n   - SG 嵌入 $\\mathbf{v}_{\\mathrm{SG}}(w)$：将 $p_{\\mathrm{SG}}(c \\mid w)$ 解释为关于 $c$ 的函数，并创建一个其分量由上下文 $c$ 索引的向量。\n   确保两种嵌入具有相同的维度，等于词汇表大小 $V$，并按词汇表的固定确定性顺序排列。\n5. 对于下面的每个测试案例，计算在两种几何结构下两个指定词之间的余弦相似度，即计算 $\\mathrm{cos}(\\mathbf{v}_{\\mathrm{MLM}}(w_1), \\mathbf{v}_{\\mathrm{MLM}}(w_2))$ 和 $\\mathrm{cos}(\\mathbf{v}_{\\mathrm{SG}}(w_1), \\mathbf{v}_{\\mathrm{SG}}(w_2))$，并报告它们的差值 $\\Delta = \\mathrm{cos}_{\\mathrm{SG}} - \\mathrm{cos}_{\\mathrm{MLM}}$。\n6. 按照测试套件中的规定，使用平滑参数 $\\alpha > 0$ 的加法平滑和大小为 $K \\in \\mathbb{N}$ 的对称窗口。不需要角度；仅报告作为实数的余弦相似度。\n\n测试套件（每个案例的形式为 $(K, \\alpha, w_1, w_2)$）：\n- 案例 1：$K = 2$, $\\alpha = 0.5$, $w_1 =$ \"cat\", $w_2 =$ \"dog\"。\n- 案例 2：$K = 1$, $\\alpha = 1.0$, $w_1 =$ \"music\", $w_2 =$ \"song\"。\n- 案例 3：$K = 3$, $\\alpha = 10^{-6}$, $w_1 =$ \"the\", $w_2 =$ \"a\"。\n- 案例 4：$K = 2$, $\\alpha = 0.2$, $w_1 =$ \"quantum\", $w_2 =$ \"theory\"。\n\n要求的最终输出格式：\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果，每个测试案例贡献一个包含三个浮点数的子列表，顺序为 $[\\mathrm{cos}_{\\mathrm{MLM}}, \\mathrm{cos}_{\\mathrm{SG}}, \\Delta]$。例如，一个包含两个假设案例的输出应如下所示：“[[0.123456,0.234567,0.111111],[0.222222,0.333333,0.111111]]”。不应打印任何额外文本。\n\n科学真实性和约束条件：\n- 仅使用提供的语料库、对称窗口和加法平滑来估计条件分布。\n- 确保词汇表是来自语料库的所有唯一词元的集合，并且上下文是来自同一集合的词元。\n- 使用确定性的分词和词汇表排序，以便向量在不同测试案例之间具有可比性。\n- 所有报告的值必须是实数，不带任何物理单位。",
            "solution": "根据指定标准对问题进行验证。\n\n### 步骤 1：提取给定信息\n- **语料库**：一个固定的、包含 11 个已分词句子的集合。\n  - `the cat sat on the mat`\n  - `the dog sat on the rug`\n  - `a cat chased a mouse`\n  - `a dog chased a ball`\n  - `music and song fill the hall`\n  - `the song played music softly`\n  - `quantum theory explains physics`\n  - `theory of music and physics`\n  - `the quantum cat thought of physics`\n  - `dogs and cats share a home`\n  - `a theory about a song`\n- **基本定义**：\n  - 条件概率：$p(A \\mid B) = \\frac{p(A, B)}{p(B)}$，其中 $p(B) > 0$。\n  - 多项分布的最大似然估计：$\\hat{p}_i = \\frac{n_i}{\\sum_j n_j}$。\n  - 加法（拉普拉斯）平滑：$\\tilde{p}_i = \\frac{n_i + \\alpha}{\\sum_j n_j + \\alpha V}$，平滑参数 $\\alpha > 0$，共 $V$ 个结果。\n  - 余弦相似度：$\\mathrm{cos}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^\\top \\mathbf{v}}{\\lVert \\mathbf{u} \\rVert_2 \\lVert \\mathbf{v} \\rVert_2}$。\n- **任务**：\n  1. 使用大小为 $K$ 的对称上下文窗口构建共现计数矩阵 $N(w, c)$。\n  2. 定义并推导掩码语言模型（从上下文词 $c$ 预测目标词 $w$）的条件分布 $p_{\\mathrm{MLM}}(w \\mid c)$，使用最大似然估计和加法平滑。\n  3. 定义并推导 Skip-gram 模型（从目标词 $w$ 预测上下文词 $c$）的条件分布 $p_{\\mathrm{SG}}(c \\mid w)$，使用最大似然估计和加法平滑。\n  4. 为每个词 $w$ 构建嵌入向量 $\\mathbf{v}_{\\mathrm{MLM}}(w)$ 和 $\\mathbf{v}_{\\mathrm{SG}}(w)$，其中向量分量由上下文词 $c$ 索引。\n  5. 计算 $\\mathrm{cos}_{\\mathrm{MLM}} = \\mathrm{cos}(\\mathbf{v}_{\\mathrm{MLM}}(w_1), \\mathbf{v}_{\\mathrm{MLM}}(w_2))$、$\\mathrm{cos}_{\\mathrm{SG}} = \\mathrm{cos}(\\mathbf{v}_{\\mathrm{SG}}(w_1), \\mathbf{v}_{\\mathrm{SG}}(w_2))$ 以及差值 $\\Delta = \\mathrm{cos}_{\\mathrm{SG}} - \\mathrm{cos}_{\\mathrm{MLM}}$。\n  6. 对每个测试案例使用指定的参数 $K$ 和 $\\alpha$。\n- **测试套件**：\n  - 案例 1：$(K=2, \\alpha=0.5, w_1=\\text{\"cat\"}, w_2=\\text{\"dog\"})$\n  - 案例 2：$(K=1, \\alpha=1.0, w_1=\\text{\"music\"}, w_2=\\text{\"song\"})$\n  - 案例 3：$(K=3, \\alpha=10^{-6}, w_1=\\text{\"the\"}, w_2=\\text{\"a\"})$\n  - 案例 4：$(K=2, \\alpha=0.2, w_1=\\text{\"quantum\"}, w_2=\\text{\"theory\"})$\n- **输出格式**：单行字符串 `[[cos_mlm1,cos_sg1,delta1],[cos_mlm2,cos_sg2,delta2],...]`。\n\n### 步骤 2：使用提取的信息进行验证\n- **科学依据**：该问题在计算语言学和机器学习的原理方面有很好的基础。分布假说、掩码语言模型 (MLM)、Skip-gram (SG)、最大似然估计 (MLE)、加法平滑和余弦相似度都是标准的、成熟的概念。该任务使用具体的、基于原则的计数方法来操作化这些抽象概念，这是一项有效且富有洞察力的练习。\n- **问题定义明确**：该问题提供了特定的语料库、明确的定义和清晰的算法流程。每个测试案例的所有参数 ($K, \\alpha$) 和目标词 ($w_1, w_2$) 都已指定。所需的输出由输入和规定的方法唯一确定。\n- **客观性**：问题使用精确、无歧义的数学和程序性语言陈述。没有主观或基于意见的元素。\n\n### 步骤 3：结论与行动\n该问题是有效的。它具有科学合理性、定义明确且客观。将提供一个完整的、有理有据的解决方案。\n\n### 解决方案推导\n\n该解决方案通过从共现统计数据构建词嵌入，并在两种不同的概率模型——掩码语言模型 (MLM) 和 Skip-gram (SG)——下比较它们的几何特性，从而将分布假说操作化。\n\n**1. 词汇表和共现矩阵的构建**\n\n首先，我们通过收集所提供语料库中的所有唯一词元并按字母顺序排序来建立一个固定的词汇表 $\\mathcal{V}$，以确保确定性的顺序。设 $V = |\\mathcal{V}|$ 为词汇表的大小。为了计算方便，我们创建一个词到索引的映射。\n\n对于给定的对称窗口大小 $K$，我们填充一个 $V \\times V$ 的共现矩阵 $N$。一个条目 $N_{ij}$ 存储计数 $N(w_i, c_j)$，表示词 $c_j$ 出现在目标词 $w_i$ 上下文中的次数。我们遍历每个句子中位置为 $p$ 的每个词 $w_i$，并对在位置 $q$ 处找到的所有满足 $1 \\le |p - q| \\le K$ 的上下文词 $c_j$ 增加计数。\n\n**2. Skip-gram (SG) 模型和嵌入**\n\nSkip-gram 模型旨在给定一个目标词 $w$ 来预测上下文词 $c$。这对应于估计条件概率 $p_{\\mathrm{SG}}(c \\mid w)$。\n根据我们计数上的最大似然估计 (MLE) 原则，给定目标词 $w_i$ 时观察到特定上下文词 $c_j$ 的概率，是通过它们的共现计数与 $w_i$ 的所有上下文词总数的比率来估计的。\n为目标词 $w_i$ 观察到的上下文词总数是矩阵 $N$ 第 $i$ 行的总和：$S_i^{\\text{row}} = \\sum_{k=1}^V N_{ik}$。\n应用参数为 $\\alpha$ 的加法平滑来处理数据稀疏性并避免零概率，条件概率为：\n$$p_{\\mathrm{SG}}(c_j \\mid w_i) = \\frac{N_{ij} + \\alpha}{S_i^{\\text{row}} + \\alpha V}$$\n一个词 $w_i$ 的 Skip-gram 嵌入，表示为 $\\mathbf{v}_{\\mathrm{SG}}(w_i)$，是一个 $V$ 维向量，其分量是这些概率，由上下文词 $c_j \\in \\mathcal{V}$ 索引：\n$$\\mathbf{v}_{\\mathrm{SG}}(w_i) = [p_{\\mathrm{SG}}(c_1 \\mid w_i), p_{\\mathrm{SG}}(c_2 \\mid w_i), \\ldots, p_{\\mathrm{SG}}(c_V \\mid w_i)]^\\top$$\n该向量对应于平滑后的条件概率矩阵 $P_{\\mathrm{SG}}$ 的第 $i$ 行，其中 $(P_{\\mathrm{SG}})_{ij} = p_{\\mathrm{SG}}(c_j \\mid w_i)$。\n\n**3. 掩码语言模型 (MLM) 模型和嵌入**\n\n在此背景下，掩码语言模型的目标是在给定上下文词 $c$ 的情况下预测目标词 $w$。这需要估计条件概率 $p_{\\mathrm{MLM}}(w \\mid c)$。\n与 SG 模型类似，我们使用共现计数。对于给定的上下文词 $c_j$，共现目标词 $w_i$ 的计数是 $N_{ij}$。$c_j$ 作为任何目标的上下文词出现的总次数是矩阵 $N$ 第 $j$ 列的总和：$S_j^{\\text{col}} = \\sum_{k=1}^V N_{kj}$。\n应用加法平滑后，条件概率为：\n$$p_{\\mathrm{MLM}}(w_i \\mid c_j) = \\frac{N_{ij} + \\alpha}{S_j^{\\text{col}} + \\alpha V}$$\n一个词 $w_i$ 的 MLM 嵌入，表示为 $\\mathbf{v}_{\\mathrm{MLM}}(w_i)$，是一个 $V$ 维向量。根据问题描述，其分量由上下文词 $c_j$ 索引。因此，该向量的第 $j$ 个分量是 $p_{\\mathrm{MLM}}(w_i \\mid c_j)$。\n$$\\mathbf{v}_{\\mathrm{MLM}}(w_i) = [p_{\\mathrm{MLM}}(w_i \\mid c_1), p_{\\mathrm{MLM}}(w_i \\mid c_2), \\ldots, p_{\\mathrm{MLM}}(w_i \\mid c_V)]^\\top$$\n该向量对应于矩阵 $P_{\\mathrm{MLM}}$ 的第 $i$ 行，其中 $(P_{\\mathrm{MLM}})_{ij} = p_{\\mathrm{MLM}}(w_i \\mid c_j)$。\n\n**4. 通过余弦相似度进行几何比较**\n\n为了比较两种模型引出的几何结构，我们计算两个词 $w_1$ 和 $w_2$ 的嵌入向量之间的余弦相似度。余弦相似度提供了两个向量之间夹角的度量，从而度量了它们基于方向的相似性，这是量化向量空间中语义相似性的常用方法。\n\n对于给定的一对词 $(w_1, w_2)$，我们计算：\n- MLM 相似度：$\\mathrm{cos}_{\\mathrm{MLM}} = \\mathrm{cos}(\\mathbf{v}_{\\mathrm{MLM}}(w_1), \\mathbf{v}_{\\mathrm{MLM}}(w_2)) = \\frac{\\mathbf{v}_{\\mathrm{MLM}}(w_1)^\\top \\mathbf{v}_{\\mathrm{MLM}}(w_2)}{\\lVert \\mathbf{v}_{\\mathrm{MLM}}(w_1) \\rVert_2 \\lVert \\mathbf{v}_{\\mathrm{MLM}}(w_2) \\rVert_2}$\n- SG 相似度：$\\mathrm{cos}_{\\mathrm{SG}} = \\mathrm{cos}(\\mathbf{v}_{\\mathrm{SG}}(w_1), \\mathbf{v}_{\\mathrm{SG}}(w_2)) = \\frac{\\mathbf{v}_{\\mathrm{SG}}(w_1)^\\top \\mathbf{v}_{\\mathrm{SG}}(w_2)}{\\lVert \\mathbf{v}_{\\mathrm{SG}}(w_1) \\rVert_2 \\lVert \\mathbf{v}_{\\mathrm{SG}}(w_2) \\rVert_2}$\n\n最后，我们计算差值 $\\Delta = \\mathrm{cos}_{\\mathrm{SG}} - \\mathrm{cos}_{\\mathrm{MLM}}$，以直接比较两种模型为每个测试案例生成的相似性度量。对测试套件中的每组参数 $(K, \\alpha, w_1, w_2)$ 重复此整个过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the word embedding comparison problem by following these steps:\n    1. Processes a fixed corpus to build a vocabulary.\n    2. For each test case:\n        a. Constructs a word-context co-occurrence matrix `N` for a given window size `K`.\n        b. Derives conditional probability matrices `P_MLM` and `P_SG` using additive smoothing.\n        c. Extracts word embedding vectors for the specified words from these matrices.\n        d. Computes the cosine similarity between the word vectors in both embedding spaces (MLM and SG).\n        e. Calculates the difference between the two similarities.\n    3. Formats and prints the final results as specified.\n    \"\"\"\n    corpus = [\n        \"the cat sat on the mat\",\n        \"the dog sat on the rug\",\n        \"a cat chased a mouse\",\n        \"a dog chased a ball\",\n        \"music and song fill the hall\",\n        \"the song played music softly\",\n        \"quantum theory explains physics\",\n        \"theory of music and physics\",\n        \"the quantum cat thought of physics\",\n        \"dogs and cats share a home\",\n        \"a theory about a song\"\n    ]\n\n    test_cases = [\n        (2, 0.5, \"cat\", \"dog\"),\n        (1, 1.0, \"music\", \"song\"),\n        (3, 1e-6, \"the\", \"a\"),\n        (2, 0.2, \"quantum\", \"theory\"),\n    ]\n\n    # Pre-processing: tokenize corpus and build a deterministic vocabulary\n    corpus_tokens = [line.split() for line in corpus]\n    \n    all_tokens = set()\n    for sentence in corpus_tokens:\n        all_tokens.update(sentence)\n    \n    vocabulary = sorted(list(all_tokens))\n    V = len(vocabulary)\n    word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n\n    final_results = []\n    cooccurrence_matrices_cache = {}\n\n    for K, alpha, w1_str, w2_str in test_cases:\n        \n        # Task 1: Construct co-occurrence count matrix N(w, c)\n        # We cache the matrix as it can be expensive to recompute.\n        if K not in cooccurrence_matrices_cache:\n            N = np.zeros((V, V), dtype=np.float64)\n            for sentence in corpus_tokens:\n                for i, target_word in enumerate(sentence):\n                    target_idx = word_to_idx[target_word]\n                    \n                    start = max(0, i - K)\n                    end = min(len(sentence), i + K + 1)\n                    \n                    for j in range(start, end):\n                        if i == j:\n                            continue\n                        context_word = sentence[j]\n                        context_idx = word_to_idx[context_word]\n                        N[target_idx, context_idx] += 1\n            cooccurrence_matrices_cache[K] = N\n        \n        N = cooccurrence_matrices_cache[K]\n            \n        w1_idx = word_to_idx[w1_str]\n        w2_idx = word_to_idx[w2_str]\n\n        # Task 2  4: Derive P_MLM(w|c) and MLM embeddings\n        # P_MLM(w_i | c_j) = (N[i, j] + alpha) / (sum_k N[k, j] + alpha * V)\n        col_sums = N.sum(axis=0)\n        P_mlm_denom = col_sums + alpha * V\n        P_mlm = (N + alpha) / P_mlm_denom[np.newaxis, :]\n        \n        v_mlm_w1 = P_mlm[w1_idx, :]\n        v_mlm_w2 = P_mlm[w2_idx, :]\n        \n        # Task 3  4: Derive P_SG(c|w) and SG embeddings\n        # P_SG(c_j | w_i) = (N[i, j] + alpha) / (sum_k N[i, k] + alpha * V)\n        row_sums = N.sum(axis=1)\n        # Use [:, np.newaxis] to ensure correct broadcasting for row-wise division\n        P_sg_denom = row_sums + alpha * V\n        P_sg = (N + alpha) / P_sg_denom[:, np.newaxis]\n        \n        v_sg_w1 = P_sg[w1_idx, :]\n        v_sg_w2 = P_sg[w2_idx, :]\n\n        # Task 5: Compute cosine similarities\n        def cosine_similarity(u, v):\n            dot_product = np.dot(u, v)\n            norm_u = np.linalg.norm(u)\n            norm_v = np.linalg.norm(v)\n            # Denominator is guaranteed non-zero due to alpha > 0 smoothing\n            return dot_product / (norm_u * norm_v)\n\n        cos_mlm = cosine_similarity(v_mlm_w1, v_mlm_w2)\n        cos_sg = cosine_similarity(v_sg_w1, v_sg_w2)\n        \n        # Task 5: Compute difference\n        delta = cos_sg - cos_mlm\n        \n        final_results.append([cos_mlm, cos_sg, delta])\n        \n    # Final print statement in the exact required format.\n    # e.g., \"[[0.123456,0.234567,0.111111],[0.222222,0.333333,0.111111]]\"\n    output_str = \"[\" + \",\".join([f\"[{c1:.6f},{c2:.6f},{d:.6f}]\" for c1, c2, d in final_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了创建词嵌入的基础知识之后，本练习  将探讨一个关键的现实世界问题：我们输入的文本质量如何影响最终的表示？这个练习要求你实现语料库清洗技术，例如停用词移除和句子去重。你将使用分布稀疏度、上下文熵和中心性 (hubness) 等指标来量化这些变化对词嵌入的影响，从而更深入地了解分布式向量的鲁棒性和特性。",
            "id": "3182871",
            "problem": "给你一个小文本语料库，要求你通过实证来检验深度学习中的分布假说：“观其伴，知其言 (You shall know a word by the company it keeps)。”从条件分布、熵和相似度的核心定义出发，你必须编写一个完整的程序，量化语料库清洗操作如何影响条件上下文分布和分布嵌入。清洗操作包括停用词移除和重复句子的去重。你必须为原始语料库及其清洗后的版本计算以下指标，然后报告其变化（清洗后值减去原始值）：\n\n- 词的条件上下文分布的稀疏性，该分布定义在任一语料库中出现的所有上下文类型的并集上。\n- 平均上下文熵，它捕捉了给定单词的上下文条件分布的不确定性。\n- 嵌入空间中的中心性（Hubness），它衡量一个词出现在其他词的前 $k$ 个最近邻列表中的频率分布的偏度。\n- 嵌入稳定性，量化为清洗前后对应词嵌入之间的平均余弦相似度。\n\n使用以下基本原理：\n- 分布假说指出，词的特性由其上下文决定。通过在大小为 $w$ 的对称窗口中统计词-上下文共现次数来表示这一点。对于一个目标词 $w$，其条件上下文分布为 $p(c \\mid w) = \\frac{N(w,c)}{\\sum_{c'} N(w,c')}$，其中 $N(w,c)$ 是共现次数。\n- 离散分布的香农熵 (Shannon entropy) 为 $H(p) = -\\sum_i p_i \\log p_i$，使用自然对数。\n- 对于词-上下文对，正点互信息 (Positive Pointwise Mutual Information, PPMI) 定义为 $\\max\\left(0, \\log \\frac{N(w,c)\\cdot N_{\\mathrm{tot}}}{\\left(\\sum_{c'} N(w,c')\\right)\\left(\\sum_{w'} N(w',c)\\right)}\\right)$，其中 $N_{\\mathrm{tot}} = \\sum_{w,c} N(w,c)$ 是总计数。使用一个词的 PPMI 行向量作为其分布嵌入，并进行行 $\\ell_2$ 归一化，以便进行余弦相似度比较。\n- 最近邻通过余弦相似度计算。对每个词，取其前 $k$ 个最近邻（不包括自身）。中心性（Hubness）通过词汇表中 $k$-出现次数分布的偏度来量化：如果 $x_i$ 是词 $i$ 出现在其他词的前 $k$ 个列表中次数，$\\mu$ 和 $\\sigma$ 分别是 $\\{x_i\\}$ 的均值和标准差，则偏度为 $\\frac{1}{n}\\sum_i \\left(\\frac{x_i-\\mu}{\\sigma}\\right)^3$，其中 $n$ 是词的数量。\n\n实现以下步骤：\n1. 将句子分词为小写单词，并去除基本标点符号。定义一个停用词集合，并仅在测试用例要求时移除它们。仅在要求时对句子进行去重。去重指移除完全重复的词元序列。\n2. 围绕每个目标词，使用大小为 $w$ 的对称窗口构建共现计数 $N(w,c)$。将上下文定义为来自原始语料库和清洗后语料库的所有词元的并集。将目标词定义为那些非停用词，并且在原始和清洗后的语料库中都与至少一个上下文共同出现。\n3. 在共享的上下文集合上，为所有目标词计算 $p(c \\mid w)$。将稀疏性定义为所有目标词的 $p(c \\mid w)$ 中零元素所占的平均比例。将平均上下文熵定义为所有目标词的 $H\\left(p(\\cdot \\mid w)\\right)$ 的平均值，并将 $0\\log 0$ 视为 $0$。\n4. 在相同的目标词和上下文集合上，为原始和清洗后的语料库计算 PPMI 矩阵。将每行归一化为单位 $\\ell_2$ 范数以获得词嵌入。计算：\n   - 嵌入稳定性：对每个目标词，计算其原始嵌入和清洗后嵌入之间的余弦相似度，然后对所有词求平均。\n   - 中心性（Hubness）：分别对原始嵌入和清洗后嵌入，为每个目标词计算前 $k$ 个最近邻（余弦相似度），并统计每个词出现在其他词列表中的次数。计算这个 $k$-出现次数分布的偏度。报告其变化，即 skewness(cleaned) 减去 skewness(raw)。\n\n测试套件：\n使用以下固定的原始语料库（十个句子，包含重复）、停用词和三种参数情况。\n\n原始语料库句子：\n- \"The cat sat on the mat and the cat slept\"\n- \"Dogs bark and cats meow\"\n- \"A cat and a dog in the house\"\n- \"The house has a mat\"\n- \"The cat sat on the mat and the cat slept\"\n- \"Dogs bark and cats meow\"\n- \"The cat chased a mouse in the house\"\n- \"Cats and dogs in the yard\"\n- \"The yard has a house and a mat\"\n- \"A dog sleeps on the mat\"\n\n停用词集合：{\"the\",\"and\",\"a\",\"of\",\"to\",\"in\",\"on\",\"has\",\"is\",\"was\",\"are\"}\n\n参数情况（每种情况是一个测试用例）：\n- 情况 1：窗口大小 $w=2$，top-k=2，移除停用词 = True，去重 = False。\n- 情况 2：窗口大小 $w=2$，top-k=2，移除停用词 = False，去重 = True。\n- 情况 3：窗口大小 $w=1$, top-k=3，移除停用词 = True，去重 = True。\n\n对于每种情况，计算并返回一个包含四个浮点数的列表：\n- $\\Delta S$，稀疏性的变化（清洗后减去原始）。\n- $\\Delta H$，平均上下文熵的变化（清洗后减去原始）。\n- $\\Delta \\mathrm{hub}$，中心性（hubness）偏度的变化（清洗后减去原始）。\n- $\\mathrm{stab}$，嵌入稳定性（原始和清洗后对应嵌入之间的平均余弦相似度）。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含结果，格式为一个由三个内部列表（每个对应一种情况）组成的逗号分隔列表。每个内部列表为 [ΔS,ΔH,Δhub,stab]，四舍五入到六位小数，并用方括号括起来。例如，\"[[0.0,0.1,-0.2,0.95],[...],[...]]\"。",
            "solution": "该问题要求通过量化语料库清洗操作（停用词移除和句子去重）对词表示的几种语言学和几何属性的影响，从而对分布假说进行实证研究。分析将在原始语料库和清洗后的版本上进行，比较四个关键指标：分布稀疏性、平均上下文熵、嵌入中心性（hubness）和嵌入稳定性。解决方案首先将处理流程形式化，然后实现所需的度量指标。\n\n### 基于原则的设计\n\n该解决方案建立在分布假说的基础上，该假说认为词的意义由其出现的上下文决定。我们将通过构建共现统计数据并从中派生出分布表示来对这一原则进行建模。\n\n**步骤 1：语料库预处理和词汇表定义**\n\n首先，我们定义一个标准化的程序来处理文本语料库。对每个测试用例，我们生成两个版本的语料库：“原始”版本和“清洗后”版本。\n- **分词 (Tokenization)**: 每个句子都被转换为小写，并移除所有非字母数字字符。然后将句子分割成一个词元序列。\n- **清洗 (Cleaning)**: 应用指定的清洗操作。如果 `deduplicate` 为 true，我们移除重复的词元序列，保留首次出现的。如果 `remove_stopwords` 为 true，我们过滤掉出现在所提供停用词集合中的词元。\n\n为了确保原始语料库和清洗后语料库之间的比较有效，我们建立了一个共同的分析基础：\n- **`context_vocab`**: 原始语料库或清洗后语料库中出现的所有唯一词元的集合。这构成了我们共现和嵌入维度的基础。\n- **`target_words`**: 我们将分析其属性的词的集合。按照规定，一个词被包含为目标词，当且仅当它不是停用词，并且在*原始*和*清洗后*的语料库中都与至少一个上下文词共同出现。这确保了我们分析的每个词在清洗前后都有明确的表示，使比较有意义。\n\n**步骤 2：共现矩阵构建**\n\n分布式分析的核心是词-上下文共现矩阵 $N$。对于每个目标词 $w$（行）和每个上下文词 $c$（列），条目 $N(w,c)$ 存储了 $c$ 出现在 $w$ 周围大小为 $w_{size}$ 的对称窗口内的次数。我们使用各自处理过的语料库构建两个这样的矩阵，$N_{\\mathrm{raw}}$ 和 $N_{\\mathrm{cleaned}}$，但它们都定义在共同的 `target_words` 和 `context_vocab` 上。\n\n**步骤 3：分布稀疏性和上下文熵**\n\n从共现矩阵 $N$ 中，我们推导出给定目标词 $w$ 的上下文 $c$ 的条件概率分布：\n$$ p(c \\mid w) = \\frac{N(w,c)}{\\sum_{c'} N(w,c')} $$\n这个分布体现了一个词所处的“环境”。我们如下量化其属性：\n- **稀疏性 ($S$)**: 所有目标词的条件概率向量 $p(\\cdot \\mid w)$ 中零条目的平均比例。更高的稀疏性表明一个词与一个更小、更特定的上下文集合相关联。\n- **平均上下文熵 ($H$)**: 上下文分布的不确定性或“离散度”，在所有目标词上取平均。单个词 $w$ 的熵由 Shannon 公式给出，使用自然对数：\n$$ H(p(\\cdot \\mid w)) = -\\sum_{c \\in \\text{context\\_vocab}} p(c \\mid w) \\log p(c \\mid w) $$\n其中我们定义 $0 \\log 0 = 0$。较低的熵意味着一个更可预测、更不均匀的上下文分布。\n\n**步骤 4：PPMI 嵌入和稳定性**\n\n为了给词创建向量表示（嵌入），我们使用正点互信息 (PPMI)。PPMI 对共现计数进行重新加权，以强调出人意料或信息量大的配对。一对 $(w,c)$ 的 PPMI 为：\n$$ \\mathrm{PPMI}(w,c) = \\max\\left(0, \\log \\frac{N(w,c) \\cdot N_{\\mathrm{tot}}}{N(w) \\cdot N(c)}\\right) $$\n其中 $N(w) = \\sum_{c'} N(w,c')$, $N(c) = \\sum_{w'} N(w',c)$, 且 $N_{\\mathrm{tot}} = \\sum_{w,c} N(w,c)$。每个词 $w$ 的嵌入是其在 PPMI 矩阵中对应的行向量。然后对这些向量进行 $\\ell_2$ 归一化，以方便进行余弦相似度比较。\n\n- **嵌入稳定性 ($\\mathrm{stab}$)**: 该指标衡量清洗过程对词嵌入的改变程度。它通过计算所有目标词的原始嵌入 ($v_{\\mathrm{raw}}$) 与其清洗后嵌入 ($v_{\\mathrm{cleaned}}$) 之间的平均余弦相似度来得出。由于向量是 $\\ell_2$ 归一化的，这实际上就是它们点积的平均值。\n\n**步骤 5：中心性（Hubness）计算**\n\n中心性（Hubness）是高维空间中的一种现象，其中一些点（中心点，hubs）会成为许多其他点的最近邻。我们通过分析“k-出现”次数的分布来量化这一点。\n1. 对于一组嵌入，计算成对的余弦相似度矩阵。\n2. 对每个词，识别其前 $k$ 个最近邻（不包括自身）。\n3. 对每个词 $w_i$，计算它出现在其他词的前 $k$ 个列表中的次数。这就是它的 k-出现次数 $x_i$。\n4. 中心性（hubness）被定义为这些计数 $\\{x_i\\}$ 分布的偏度。正偏度表示存在中心点。总体偏度计算如下：\n$$ \\mathrm{skew} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{x_i - \\mu}{\\sigma}\\right)^3 $$\n其中 $n$ 是目标词的数量，$\\mu$ 和 $\\sigma$ 是 k-出现次数的均值和标准差。如果 $\\sigma=0$，则偏度定义为 $0$。\n\n通过为原始语料库和清洗后语料库计算这四个指标，我们可以计算出变化量（$\\Delta S$, $\\Delta H$, $\\Delta \\mathrm{hub}$）和稳定性（$\\mathrm{stab}$），从而为每个测试用例提供一个定量的答案。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport re\nfrom collections import Counter\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n    raw_corpus_sentences = [\n        \"The cat sat on the mat and the cat slept\", \"Dogs bark and cats meow\",\n        \"A cat and a dog in the house\", \"The house has a mat\",\n        \"The cat sat on the mat and the cat slept\", \"Dogs bark and cats meow\",\n        \"The cat chased a mouse in the house\", \"Cats and dogs in the yard\",\n        \"The yard has a house and a mat\", \"A dog sleeps on the mat\"\n    ]\n    stopwords = {\"the\", \"and\", \"a\", \"of\", \"to\", \"in\", \"on\", \"has\", \"is\", \"was\", \"are\"}\n    test_cases = [\n        # (window_size, k, remove_stopwords, deduplicate)\n        (2, 2, True, False),\n        (2, 2, False, True),\n        (1, 3, True, True),\n    ]\n\n    all_results = []\n    for window_size, k, remove_stops, dedup in test_cases:\n        # Step 1: Preprocess corpora\n        raw_processed = preprocess_corpus(raw_corpus_sentences, stopwords, remove_stopwords=False, deduplicate=False)\n        cleaned_processed = preprocess_corpus(raw_corpus_sentences, stopwords, remove_stops, dedup)\n        \n        # Step 2: Define vocabularies and target words\n        raw_tokens = {token for sent in raw_processed for token in sent}\n        cleaned_tokens = {token for sent in cleaned_processed for token in sent}\n        \n        def get_words_with_contexts(corpus, w_size):\n            words = set()\n            for sent in corpus:\n                if len(sent) > 1:\n                    for i, word in enumerate(sent):\n                        start = max(0, i - w_size)\n                        end = min(len(sent), i + w_size + 1)\n                        context = sent[start:i] + sent[i + 1:end]\n                        if context:\n                            words.add(word)\n            return words\n\n        raw_active_words = get_words_with_contexts(raw_processed, window_size)\n        cleaned_active_words = get_words_with_contexts(cleaned_processed, window_size)\n        \n        potential_targets = raw_active_words  cleaned_active_words\n        target_words = sorted([w for w in potential_targets if w not in stopwords])\n        \n        context_vocab = sorted(list(raw_tokens | cleaned_tokens))\n        \n        if not target_words:\n            all_results.append([0.0, 0.0, 0.0, 0.0])\n            continue\n\n        target_map = {word: i for i, word in enumerate(target_words)}\n        context_map = {word: i for i, word in enumerate(context_vocab)}\n\n        # Step 3  4: Run analysis for both corpora\n        raw_metrics = run_analysis(raw_processed, target_words, target_map, context_vocab, context_map, window_size, k)\n        cleaned_metrics = run_analysis(cleaned_processed, target_words, target_map, context_vocab, context_map, window_size, k)\n        \n        # Step 5: Compute final results\n        delta_sparsity = cleaned_metrics[\"sparsity\"] - raw_metrics[\"sparsity\"]\n        delta_entropy = cleaned_metrics[\"avg_entropy\"] - raw_metrics[\"avg_entropy\"]\n        delta_hubness = cleaned_metrics[\"hubness\"] - raw_metrics[\"hubness\"]\n        \n        raw_embeds = raw_metrics[\"embeddings\"]\n        cleaned_embeds = cleaned_metrics[\"embeddings\"]\n        # Cosine similarity for normalized vectors is their dot product\n        stability = np.mean(np.sum(raw_embeds * cleaned_embeds, axis=1))\n\n        all_results.append([delta_sparsity, delta_entropy, delta_hubness, stability])\n\n    # Format the final output string\n    formatted_lists = [f\"[{','.join([f'{v:.6f}' for v in sublist])}]\" for sublist in all_results]\n    final_output = f\"[{','.join(formatted_lists)}]\"\n    print(final_output)\n\ndef preprocess_corpus(sentences, stopwords, remove_stopwords, deduplicate):\n    \"\"\"Tokenizes, cleans, and optionally deduplicates sentences.\"\"\"\n    tokenized_sents = []\n    for s in sentences:\n        s_clean = re.sub(r'[^\\w\\s]', '', s.lower())\n        tokens = s_clean.split()\n        if remove_stopwords:\n            tokens = [token for token in tokens if token not in stopwords]\n        if tokens:\n            tokenized_sents.append(tokens)\n\n    if deduplicate:\n        unique_sents = []\n        seen = set()\n        for sent in tokenized_sents:\n            sent_tuple = tuple(sent)\n            if sent_tuple not in seen:\n                unique_sents.append(sent)\n                seen.add(sent_tuple)\n        return unique_sents\n    return tokenized_sents\n\ndef build_cooccurrence(corpus, target_map, context_map, window_size):\n    \"\"\"Builds the word-context co-occurrence matrix.\"\"\"\n    num_targets = len(target_map)\n    num_contexts = len(context_map)\n    cooc_matrix = np.zeros((num_targets, num_contexts), dtype=float)\n\n    for sent in corpus:\n        for i, token in enumerate(sent):\n            if token in target_map:\n                target_idx = target_map[token]\n                start = max(0, i - window_size)\n                end = min(len(sent), i + window_size + 1)\n                for j in range(start, end):\n                    if i == j:\n                        continue\n                    context_word = sent[j]\n                    if context_word in context_map:\n                        context_idx = context_map[context_word]\n                        cooc_matrix[target_idx, context_idx] += 1\n    return cooc_matrix\n\ndef calculate_ppmi(N):\n    \"\"\"Calculates the Positive Pointwise Mutual Information matrix from a co-occurrence matrix N.\"\"\"\n    N_w = N.sum(axis=1, keepdims=True)\n    N_c = N.sum(axis=0, keepdims=True)\n    N_tot = N.sum()\n\n    if N_tot == 0:\n        return np.zeros_like(N)\n\n    # Denominator: N(w) * N(c)\n    denominator = N_w @ N_c\n    \n    # Ratio inside the log\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio = (N * N_tot) / denominator\n\n    # log(ratio), handling -inf and nan\n    with np.errstate(all='ignore'):\n        pmi = np.log(ratio)\n    \n    pmi[np.isneginf(pmi)] = 0\n    pmi[np.isnan(pmi)] = 0\n    \n    ppmi = np.maximum(0, pmi)\n    return ppmi\n\ndef calculate_hubness(embeddings, k):\n    \"\"\"Calculates hubness skewness for a set of embeddings.\"\"\"\n    num_words = embeddings.shape[0]\n    if num_words = k:\n        return 0.0\n\n    sim_matrix = embeddings @ embeddings.T\n    np.fill_diagonal(sim_matrix, -1)\n    \n    top_k_indices = np.argsort(sim_matrix, axis=1)[:, -k:]\n    \n    k_occurrences = np.bincount(top_k_indices.flatten(), minlength=num_words)\n    \n    std_dev = k_occurrences.std()\n    if std_dev == 0:\n        return 0.0\n    \n    mean_val = k_occurrences.mean()\n    skewness = np.mean(((k_occurrences - mean_val) / std_dev) ** 3)\n    return skewness\n\ndef run_analysis(corpus, target_words, target_map, context_vocab, context_map, window_size, k):\n    \"\"\"Performs the full analysis pipeline for a given corpus.\"\"\"\n    cooc_matrix = build_cooccurrence(corpus, target_map, context_map, window_size)\n    \n    # Sparsity and Entropy\n    row_sums = cooc_matrix.sum(axis=1, keepdims=True)\n    row_sums[row_sums == 0] = 1 # Avoid division by zero\n    p_c_given_w = cooc_matrix / row_sums\n    \n    sparsity = np.mean(p_c_given_w == 0)\n\n    with np.errstate(divide='ignore', invalid='ignore'):\n        term = p_c_given_w * np.log(p_c_given_w)\n        term[np.isnan(term)] = 0\n    avg_entropy = np.mean(-np.sum(term, axis=1))\n    \n    # PPMI and Embeddings\n    ppmi_matrix = calculate_ppmi(cooc_matrix)\n    norms = np.linalg.norm(ppmi_matrix, axis=1, keepdims=True)\n    norms[norms == 0] = 1\n    embeddings = ppmi_matrix / norms\n    \n    # Hubness\n    hubness = calculate_hubness(embeddings, k)\n    \n    return {\n        \"sparsity\": sparsity,\n        \"avg_entropy\": avg_entropy,\n        \"hubness\": hubness,\n        \"embeddings\": embeddings\n    }\n\nsolve()\n```"
        },
        {
            "introduction": "最后的这项练习  将基于计数的显式方法与现代词嵌入模型（如 word2vec）中基于优化的方法联系起来。你将研究带负采样的 Skip-Gram (SGNS) 目标函数，并发现它等价于对一个特定的“移位”点互信息 (PMI) 矩阵进行因式分解。通过分析负采样数量 $k$ 和负采样分布的选择等参数如何影响这个隐式矩阵，你将揭示使 SGNS 如此高效的理论基础。",
            "id": "3182845",
            "problem": "构建一个程序，通过从一个小语料库构建词-上下文共现矩阵来实践分布假说，然后分析负采样分布和负采样倍数的选择如何影响带负采样的 Skip-Gram (SGNS) 模型所分解的隐式矩阵。从以下基本基础出发：分布假说主张，出现在相似上下文中的词语往往具有相似的含义；共现频率定义了经验联合概率和边际概率；词语和上下文之间的点互信息 (PMI) 是根据这些概率定义的；SGNS 对观测到的样本对和负采样样本对优化一个逻辑斯谛目标函数，这产生了一个等于带偏移的点互信息 (PMI) 的最优内积，该偏移取决于负采样分布和负样本的数量。除了这些基础之外，不要使用任何捷径公式。\n\n您的程序必须以纯粹的数学和逻辑方式实现以下步骤：\n\n1.  构建一个使用空格分词的有限语料库。使用以下四个已转换为小写的句子作为整个语料库：\n    - \"the quick brown fox jumps over the lazy dog\"\n    - \"the quick blue hare jumps over the sleepy dog\"\n    - \"a fast brown fox leaps over a lazy hound\"\n    - \"the slow tortoise crawls under the lazy dog\"\n2.  通过在每个句子上滑动一个半径为 $r = 2$ 的窗口，构建一个对称的词-上下文共现矩阵。对于位置 $t$ 处的词元 $w_t$，以及所有满足 $t - r \\le j \\le t + r$ 且 $j \\ne t$ 的位置 $j$，将词对 $(w_t, w_j)$ 的计数加 1。设得到的计数矩阵为 $C \\in \\mathbb{R}^{V \\times V}$，其中 $V$ 是词汇表大小，行索引词语，列索引上下文。\n3.  使用加法平滑将计数转换为概率，以避免未定义的对数。通过向 $C$ 的每个条目加上 $\\tau = 10^{-3}$ 来定义一个平滑矩阵 $\\tilde{C}$。设总词对数为 $N = \\sum_{i=1}^{V} \\sum_{j=1}^{V} \\tilde{C}_{ij}$。定义联合概率 $P(w_i, c_j) = \\tilde{C}_{ij} / N$，词语边际概率 $P(w_i) = \\sum_{j=1}^{V} \\tilde{C}_{ij} / N$，以及上下文边际概率 $P(c_j) = \\sum_{i=1}^{V} \\tilde{C}_{ij} / N$。\n4.  计算点互信息矩阵 $M$，其条目为\n    $$M_{ij} = \\log \\frac{P(w_i, c_j)}{P(w_i) P(c_j)},$$\n    使用自然对数。\n5.  对于负采样，考虑两类上下文上的负采样分布：\n    - 均匀分布：$Q_{\\text{uni}}(c_j) = 1 / V$。\n    - 提升到 $\\alpha$ 次幂的一元语法模型：$Q_{\\alpha}(c_j) = \\frac{P(c_j)^{\\alpha}}{\\sum_{\\ell=1}^{V} P(c_{\\ell})^{\\alpha}}$，对于给定的 $\\alpha \\in [0, 1]$。\n6.  考虑负采样倍数 $k \\in \\mathbb{N}$。通过分析逻辑斯谛目标函数的最优性条件（其中正样本从 $P(w,c)$ 中抽取，负样本从 $P(w) Q(c)$ 中抽取，每个正样本对应 $k$ 个负样本），SGNS 的最优内积形式为一个带偏移的点互信息：\n    $$S_{ij}(k, Q) = M_{ij} - \\log k - \\log \\frac{Q(c_j)}{P(c_j)}.$$\n    严格地将此关系作为优化条件的结果，为指定的 $(k, Q)$ 构建偏移矩阵 $S(k,Q)$。\n7.  对于每个指定的配置，通过奇异值分解计算 $S(k,Q)$ 的奇异值。设 $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_V$ 表示奇异值。提取最大的两个奇异值 $\\sigma_1$ 和 $\\sigma_2$。\n8.  为确保数值可复现性，请使用指定的精确语料库、窗口半径 $r = 2$、平滑参数 $\\tau = 10^{-3}$ 和自然对数。\n\n测试套件和要求的输出：\n- 使用以下五个测试配置，每个配置由一对 $(k, \\alpha)$ 以及声明的负采样分布指定：\n  1.  均匀分布，其中 $k = 1$。\n  2.  均匀分布，其中 $k = 20$。\n  3.  提升到幂的一元语法模型，其中 $\\alpha = 0.75$ 且 $k = 1$。\n  4.  提升到幂的一元语法模型，其中 $\\alpha = 0.75$ 且 $k = 20$。\n  5.  提升到幂的一元语法模型，其中 $\\alpha = 1.0$ 且 $k = 5$。\n- 对于每个配置，计算并报告四舍五入到恰好 $6$ 位小数的奇异值对 $[\\sigma_1, \\sigma_2]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素本身也是相同格式的双元素列表，不含空格。例如，最外层结构必须看起来像\n  $$\\big[ [a_1, a_2], [b_1, b_2], \\dots \\big],$$\n  但每个浮点数需精确打印到小数点后 $6$ 位，并保留所有逗号，删除所有空格。具体来说，打印的行必须具有以下形式\n  \"[[x11,x12],[x21,x22],[x31,x32],[x41,x42],[x51,x52]]\"。",
            "solution": "目标是构建一个程序来模拟词语的分布语义，并分析带负采样的 Skip-Gram (SGNS) 模型中超参数的影响。这是通过实践分布假说来实现的，该假说主张一个词的意义由其出现的上下文决定。该过程始于语料库分析，最终从 SGNS 优化目标派生出的隐式矩阵中提取特征。\n\n首先，我们从给定的语料库建立一个词汇表并量化词-上下文关系。该语料库由四个句子组成，通过空格进行分词。词汇表是所有唯一词元的集合，其大小为 $V$。为了捕捉分布特性，我们构建一个词-上下文共现矩阵 $C \\in \\mathbb{R}^{V \\times V}$，其行和列都由词汇表索引。通过在每个句子上滑动一个指定半径 $r=2$ 的窗口来填充该矩阵。对于位置 $t$ 处的词语 $w_t$，它与附近位置 $j$（其中 $t - r \\le j \\le t + r$ 且 $j \\neq t$）的上下文词语 $w_j$ 的共现计数 $C_{ij}$ 会增加 1。这个过程自然会产生一个对称矩阵（$C = C^T$），因为共现关系是对称的。\n\n其次，我们从原始计数过渡到概率框架。为了避免零计数（这会导致后续的对数未定义）的问题，我们采用加法平滑。将一个小的常数 $\\tau = 10^{-3}$ 加到计数矩阵 $C$ 的每个条目上，得到一个平滑矩阵 $\\tilde{C}$。平滑后的共现对总数为 $N = \\sum_{i=1}^{V} \\sum_{j=1}^{V} \\tilde{C}_{ij}$。由此，我们定义经验联合概率分布 $P(w_i, c_j) = \\tilde{C}_{ij} / N$。然后，通过对联合概率矩阵的适当维度求和，可以导出词语的边际概率 $P(w_i) = \\sum_{j=1}^{V} P(w_i, c_j)$ 和上下文的边际概率 $P(c_j) = \\sum_{i=1}^{V} P(w_i, c_j)$。\n\n第三，我们计算点互信息 (PMI) 矩阵 $M$。条目 $M_{ij}$ 量化了特定词语 $w_i$ 和上下文 $c_j$ 之间的关联。其定义为：\n$$M_{ij} = \\log \\frac{P(w_i, c_j)}{P(w_i) P(c_j)}$$\n正的 PMI 值表示该词语和上下文的共现频率高于它们独立时的期望频率，这表明存在语义相关性。我们在此计算中使用自然对数。在数值上，这被实现为 $M_{ij} = \\log P(w_i, c_j) - \\log P(w_i) - \\log P(c_j)$ 以保持稳定性。\n\n第四，我们分析 SGNS 模型。一个关键的理论结果表明，SGNS 并非显式地构建和分解 PMI 矩阵，但其优化目标会隐式地使学习到的词向量和上下文向量满足一种特定关系。词语 $w_i$ 的词向量和上下文 $c_j$ 的上下文向量之间的最优的点积等价于它们 PMI 的一个偏移版本。这产生了 SGNS 矩阵 $S(k, Q)$，其条目为：\n$$S_{ij}(k, Q) = M_{ij} - \\log k - \\log \\frac{Q(c_j)}{P(c_j)}$$\n这里，$k$ 是每个正样本对应的负样本数量，$Q(c)$ 是抽取这些负上下文所用的概率分布。该方程揭示了 $k$ 和 $Q$ 的选择如何系统性地改变正在学习的语义空间。\n\n该程序评估两种类型的负采样分布 $Q$。第一种是均匀分布，$Q_{\\text{uni}}(c_j) = 1/V$，它将所有词语视为同等可能的负样本。第二种是基于一元语法的分布，$Q_{\\alpha}(c_j) = P(c_j)^{\\alpha} / \\sum_{\\ell=1}^V P(c_{\\ell})^{\\alpha}$，它依赖于观测到的上下文频率。参数 $\\alpha$（常见值为 0.75）会扭曲该分布，通常会抑制高频词的概率。\n\n最后，对于每个由 $(k, Q)$ 定义的测试配置，我们构建相应的矩阵 $S(k, Q)$。为了分析这个隐式语义空间的结构，我们对 $S$ 执行奇异值分解 (SVD)。SVD 提供了奇异值（$\\sigma_1 \\ge \\sigma_2 \\ge \\dots$），它们代表了矩阵主成分的大小。我们提取最大的两个奇异值 $\\sigma_1$ 和 $\\sigma_2$，因为它们捕捉了在该配置下由 SGNS 目标定义的语义空间中最重要的方差维度。程序为五个指定的测试用例计算这些值，提供了一个关于不同 SGNS 超参数如何塑造底层词表示的定量比较。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Operationalizes the Distributional Hypothesis to analyze the implicit matrix\n    factorized by Skip-Gram with Negative Sampling (SGNS).\n    \"\"\"\n\n    # 1. Build a finite corpus with tokenization by whitespace.\n    corpus_sentences = [\n        \"the quick brown fox jumps over the lazy dog\",\n        \"the quick blue hare jumps over the sleepy dog\",\n        \"a fast brown fox leaps over a lazy hound\",\n        \"the slow tortoise crawls under the lazy dog\"\n    ]\n    all_words = \" \".join(corpus_sentences).split()\n    vocab = sorted(list(set(all_words)))\n    V = len(vocab)\n    word_to_idx = {word: i for i, word in enumerate(vocab)}\n\n    # 2. Construct a symmetric word–context co-occurrence matrix C.\n    r = 2\n    C = np.zeros((V, V))\n    for sentence in corpus_sentences:\n        tokens = sentence.split()\n        L = len(tokens)\n        for i in range(L):\n            center_word = tokens[i]\n            center_idx = word_to_idx[center_word]\n            start_context = max(0, i - r)\n            end_context = min(L, i + r + 1)\n            for j in range(start_context, end_context):\n                if i == j:\n                    continue\n                context_word = tokens[j]\n                context_idx = word_to_idx[context_word]\n                C[center_idx, context_idx] += 1\n\n    # 3. Convert counts to probabilities with additive smoothing.\n    tau = 1e-3\n    C_tilde = C + tau\n    N = np.sum(C_tilde)\n    P_wc = C_tilde / N  # Joint probability P(w, c)\n    P_w = np.sum(P_wc, axis=1)  # Word marginal P(w)\n    P_c = np.sum(P_wc, axis=0)  # Context marginal P(c)\n\n    # 4. Compute the Pointwise Mutual Information matrix M.\n    # To avoid numerical errors with log(0), we use the smoothed probabilities.\n    # The smoothing ensures P_wc, P_w, and P_c are all positive.\n    log_P_wc = np.log(P_wc)\n    log_P_w = np.log(P_w)\n    log_P_c = np.log(P_c)\n    M = log_P_wc - log_P_w[:, np.newaxis] - log_P_c[np.newaxis, :]\n\n    # Define the test suite.\n    test_cases = [\n        # (distribution_type, k, alpha)\n        ('uniform', 1, None),\n        ('uniform', 20, None),\n        ('unigram', 1, 0.75),\n        ('unigram', 20, 0.75),\n        ('unigram', 5, 1.0),\n    ]\n\n    results = []\n    \n    for dist_type, k, alpha in test_cases:\n        # 5. Compute the negative sampling distribution Q.\n        if dist_type == 'uniform':\n            Q = np.ones(V) / V\n        elif dist_type == 'unigram':\n            Q = P_c**alpha\n            Q /= np.sum(Q)\n        else:\n            raise ValueError(\"Unknown distribution type\")\n\n        # 6. Construct the shifted PMI matrix S(k,Q).\n        log_k = np.log(k)\n        # The term log(Q(c_j) / P(c_j)) depends only on the context j.\n        log_ratio_Q_over_Pc = np.log(Q) - log_P_c\n        # Broadcasting subtracts the scalar log_k and the row vector from each row of M.\n        S = M - log_k - log_ratio_Q_over_Pc[np.newaxis, :]\n        \n        # 7. Compute the singular values of S(k,Q) and extract the top two.\n        singular_values = np.linalg.svd(S, compute_uv=False)\n        sigma_1 = singular_values[0]\n        sigma_2 = singular_values[1]\n        \n        results.append([sigma_1, sigma_2])\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res_pair in results:\n        formatted_pair = f\"[{res_pair[0]:.6f},{res_pair[1]:.6f}]\"\n        formatted_results.append(formatted_pair)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}