{
    "hands_on_practices": [
        {
            "introduction": "The performance of hierarchical softmax critically depends on the tree structure that organizes the classes. This exercise puts you in the driver's seat, implementing two of the most important tree-building heuristics: frequency-based Huffman coding and similarity-based k-means clustering. By comparing their impact on model perplexity and computational latency, you will gain first-hand experience with the fundamental trade-off between optimizing for speed and optimizing for semantic coherence. ",
            "id": "3134847",
            "problem": "You are given a binary-tree-based hierarchical softmax classifier for multiclass prediction. A hierarchical softmax organizes a finite set of classes into the leaves of a binary tree, where each internal node makes a binary decision that routes an input to its left or right child until it reaches a leaf. For a fixed input feature vector $x \\in \\mathbb{R}^d$ and a fixed class $c$, the model defines the class probability as the product of the conditional probabilities encountered along the root-to-leaf path for class $c$. At each internal node $n$, a differentiable binary classifier computes a scalar score $z_n(x)$ and maps it to a probability of going to a specific child via the logistic function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. The negative log-likelihood for one labeled example $(x, c)$ is the negative logarithm of the product of these routing probabilities, and the perplexity over a dataset is the exponential of the average negative log-likelihood.\n\nYou must compare two different heuristics for constructing the tree structure:\n- Frequency-based Huffman coding: Building a binary tree that minimizes the expected leaf depth under the class frequency distribution.\n- Clustering-based top-down $k$-means: Recursively cluster class embeddings into $k = 2$ clusters to form a binary tree, splitting until each leaf contains a single class.\n\nYour program must implement the following, starting from fundamental definitions of probability, expected values, and logistic binary classification:\n\n1) Tree-building heuristics:\n- Frequency-based Huffman tree: Given a discrete distribution over classes $\\{p(c)\\}_{c=1}^V$ where $V$ is the vocabulary size, build a binary tree using Huffman’s algorithm that greedily merges the two least probable nodes until a single root remains. Each merge creates a parent node whose children are the two merged nodes.\n- Clustering-based tree via top-down $2$-means: Given class embeddings $\\{e_c \\in \\mathbb{R}^d\\}_{c=1}^V$, recursively partition the current set of classes into two non-empty clusters using $k$-means with $k = 2$ and squared Euclidean distance, until each leaf contains exactly one class. For reproducibility, initialize the $k$-means centers deterministically at the classes with the minimum and maximum values along the first coordinate in the current subset, update by assignment and cluster mean recomputation, and stop when assignments do not change or after a fixed number of iterations. If a cluster becomes empty, reassign the farthest point from the non-empty cluster’s center to the empty cluster to maintain non-emptiness.\n\n2) Internal node classifiers:\n- For any internal node that splits its set of descendant classes into disjoint left and right subsets, compute a linear classifier from first principles by modeling a separating hyperplane halfway between the means of the class embeddings on each side. Let $m_L$ be the arithmetic mean of $\\{e_c: c \\in \\text{left}\\}$ and $m_R$ be the arithmetic mean of $\\{e_c: c \\in \\text{right}\\}$. Define $w = \\frac{m_R - m_L}{\\lVert m_R - m_L \\rVert_2}$ if $\\lVert m_R - m_L \\rVert_2 \\neq 0$ and $w = 0$ otherwise, and define $b = -\\frac{1}{2} w^\\top (m_L + m_R)$. For an input $x$, the probability of routing to the right child is $\\sigma(w^\\top x + b)$ and to the left child is $1 - \\sigma(w^\\top x + b)$.\n\n3) Class probability and negative log-likelihood:\n- For a labeled sample $(x, c)$, let the root-to-leaf path for class $c$ be the ordered sequence of internal nodes $\\{n_1, n_2, \\dots, n_{L(c)}\\}$, where at each node $n_\\ell$ the routing decision is either left or right depending on which child lies on the path to class $c$. The model probability is\n$$\nP(c \\mid x) \\;=\\; \\prod_{\\ell = 1}^{L(c)} P(\\text{child}_\\ell \\mid x, n_\\ell),\n$$\nwhere $P(\\text{right} \\mid x, n) = \\sigma(w_n^\\top x + b_n)$ and $P(\\text{left} \\mid x, n) = 1 - \\sigma(w_n^\\top x + b_n)$, with $(w_n, b_n)$ defined as above. The negative log-likelihood for $(x, c)$ is $-\\log P(c \\mid x)$, and the perplexity over a dataset $\\{(x_i, c_i)\\}_{i=1}^N$ is\n$$\n\\operatorname{ppl} \\;=\\; \\exp\\!\\left(\\frac{1}{N} \\sum_{i=1}^N \\big(-\\log P(c_i \\mid x_i)\\big)\\right).\n$$\n\n4) Latency proxy:\n- Define a latency proxy as the expected number of node evaluations per prediction, computed as the expected leaf depth under the true class distribution,\n$$\n\\mathbb{E}\\big[L(C)\\big] \\;=\\; \\sum_{c=1}^{V} p(c)\\,L(c),\n$$\nwhere $L(c)$ is the depth (number of internal nodes along the path) of the leaf for class $c$.\n\n5) Data generation:\n- Simulate labeled data by sampling class labels $c \\sim \\text{Categorical}(p)$ and then sampling feature vectors according to a Gaussian model $x \\sim \\mathcal{N}(e_c, \\sigma^2 I_d)$, where $I_d$ is the $d \\times d$ identity matrix and $\\sigma > 0$ is the standard deviation.\n\nYou must implement both trees, construct linear classifiers at internal nodes using the mean-difference hyperplane described above, generate the synthetic dataset, compute the average negative log-likelihood and perplexity for each tree, and compute the latency proxy (expected depth) for each tree.\n\nTest Suite and Answer Specification:\nImplement your program to run the following three test cases, each fully specified by $(V, d, E, p, \\sigma, N, \\text{seed})$. In all cases, $V = 8$.\n\n- Test Case $1$ (non-uniform frequencies, two-dimensional embeddings):\n    - $d = 2$\n    - Embeddings $E \\in \\mathbb{R}^{8 \\times 2}$ with rows $e_0$ through $e_7$:\n        - $e_0 = [0.0, 0.0]$\n        - $e_1 = [1.0, 0.0]$\n        - $e_2 = [0.0, 1.0]$\n        - $e_3 = [1.0, 1.0]$\n        - $e_4 = [2.0, 0.0]$\n        - $e_5 = [2.0, 1.0]$\n        - $e_6 = [3.0, 0.5]$\n        - $e_7 = [-1.0, 0.5]$\n    - Class probabilities $p = [0.30, 0.20, 0.15, 0.12, 0.10, 0.06, 0.04, 0.03]$.\n    - Gaussian noise standard deviation $\\sigma = 0.20$.\n    - Number of samples $N = 3000$.\n    - Random seed for reproducibility $\\text{seed} = 7$.\n\n- Test Case $2$ (uniform frequencies, circular arrangement in two dimensions):\n    - $d = 2$\n    - Embeddings $E$:\n        - $e_0 = [1.000, 0.000]$\n        - $e_1 = [0.707, 0.707]$\n        - $e_2 = [0.000, 1.000]$\n        - $e_3 = [-0.707, 0.707]$\n        - $e_4 = [-1.000, 0.000]$\n        - $e_5 = [-0.707, -0.707]$\n        - $e_6 = [0.000, -1.000]$\n        - $e_7 = [0.707, -0.707]$\n    - Class probabilities $p = [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]$.\n    - $\\sigma = 0.15$.\n    - $N = 3000$.\n    - $\\text{seed} = 13$.\n\n- Test Case $3$ (highly skewed frequencies, three-dimensional embeddings):\n    - $d = 3$\n    - Embeddings $E$:\n        - $e_0 = [0.0, 0.0, 0.0]$\n        - $e_1 = [1.0, 0.5, -0.5]$\n        - $e_2 = [-0.5, 1.0, 0.5]$\n        - $e_3 = [0.5, -1.0, 0.5]$\n        - $e_4 = [1.5, 1.5, -1.0]$\n        - $e_5 = [-1.5, 1.0, 1.0]$\n        - $e_6 = [0.0, -1.5, -1.0]$\n        - $e_7 = [2.0, 0.0, 1.0]$\n    - Class probabilities $p = [0.6, 0.1, 0.1, 0.05, 0.05, 0.04, 0.03, 0.03]$.\n    - $\\sigma = 0.25$.\n    - $N = 4000$.\n    - $\\text{seed} = 23$.\n\nRequired outputs per test case:\n- For each test case, compute:\n    - Perplexity under the clustering-based $2$-means tree, as a float.\n    - Perplexity under the frequency-based Huffman tree, as a float.\n    - Expected depth under the clustering-based $2$-means tree, as a float.\n    - Expected depth under the frequency-based Huffman tree, as a float.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a sublist in the order given above. Each sublist must be of the form $[\\text{ppl\\_kmeans}, \\text{ppl\\_huffman}, \\text{lat\\_kmeans}, \\text{lat\\_huffman}]$, with all values rounded to $6$ decimal places. For example, the overall output format is\n\"[[a11,a12,a13,a14],[a21,a22,a23,a24],[a31,a32,a33,a34]]\".",
            "solution": "The user-provided problem statement has been analyzed and is determined to be **valid**. It is scientifically grounded in the principles of machine learning and probability theory, self-contained, and well-posed. All components required for a unique, computable solution are specified, including deterministic procedures for tree construction and data generation.\n\nThe task is to implement and compare two heuristics for building a binary tree for a hierarchical softmax classifier: a frequency-based Huffman tree and a clustering-based top-down $2$-means tree. The comparison will be based on two metrics: perplexity on a synthetic dataset, which measures model fit, and a latency proxy defined as the expected class depth, which measures computational efficiency.\n\nThe solution will be implemented by following these core steps:\n\n1.  **Tree Construction**: Two distinct algorithms will be implemented to construct the binary tree hierarchies.\n    -   **Huffman Tree**: Following the problem specification, we will implement the classic Huffman coding algorithm. This involves using a min-priority queue to greedily merge the two nodes (classes or sub-trees) with the lowest probability. The probability of a new internal node is the sum of its children's probabilities. This algorithm is known to construct a prefix code that minimizes the expected codeword length, which in this context translates to minimizing the expected leaf depth $\\mathbb{E}[L(C)]$ given the class frequencies $p(c)$. Each leaf in the tree represents a unique class, and each internal node represents a binary decision.\n    -   **Clustering Tree**: A tree will be constructed by recursively partitioning the set of class embeddings. At each step, for a given set of classes, we use the $k$-means algorithm with $k=2$ to split them into two clusters. The process is deterministic: initial centers for the $2$-means algorithm are chosen as the class embeddings with the minimum and maximum values along the first coordinate. The clustering criterion is the squared Euclidean distance. The recursion continues until each leaf node contains a single class. This heuristic aims to group similar classes together, potentially making the classification task at each internal node easier.\n\n2.  **Internal Node Classifier Formulation**: For each internal node in a constructed tree, a linear binary classifier is defined. This classifier's purpose is to decide whether to route an input vector $x$ to its left or right child. Given the set of classes in the left subtree, $\\text{classes}_L$, and the right subtree, $\\text{classes}_R$, we first compute the mean of their respective class embeddings, $m_L = \\text{mean}\\{e_c : c \\in \\text{classes}_L\\}$ and $m_R = \\text{mean}\\{e_c : c \\in \\text{classes}_R\\}$. The classifier is a hyperplane positioned halfway between these two means. Its parameters, a weight vector $w$ and a bias term $b$, are defined as:\n    $$\n    w = \\frac{m_R - m_L}{\\lVert m_R - m_L \\rVert_2}, \\quad b = -\\frac{1}{2} w^\\top (m_L + m_R)\n    $$\n    The score for an input $x$ is $z(x) = w^\\top x + b$. This score represents the projected distance of $x$ from the separating hyperplane. The probability of routing to the right child is given by the logistic function $P(\\text{right} \\mid x) = \\sigma(z(x))$, and to the left is $P(\\text{left} \\mid x) = 1 - \\sigma(z(x))$.\n\n3.  **Probabilistic Model and Perplexity**: The probability of a class $c$ given an input $x$, $P(c \\mid x)$, is the product of the probabilities of the routing decisions made at each internal node along the path from the root to the leaf for class $c$. The negative log-likelihood (NLL) is $-\\log P(c \\mid x)$. To avoid numerical instability when computing logarithms of probabilities, which can be very small, we sum the log-probabilities at each step of the path. The log-probabilities for left and right turns are $\\log(1-\\sigma(z))$ and $\\log(\\sigma(z))$ respectively, which can be computed robustly using NumPy's `logaddexp` function, equivalent to a stable `log(1+exp(x))` calculation. The perplexity over a dataset of $N$ samples is then calculated as the exponential of the average NLL:\n    $$\n    \\operatorname{ppl} = \\exp\\left(\\frac{1}{N} \\sum_{i=1}^N \\text{NLL}(x_i, c_i)\\right)\n    $$\n\n4.  **Latency Proxy**: A proxy for the computational latency of a prediction is the expected number of internal node evaluations required. This is equivalent to the expected depth of a class, weighted by the class probabilities $p(c)$:\n    $$\n    \\mathbb{E}[L(C)] = \\sum_{c=1}^{V} p(c) L(c)\n    $$\n    where $L(c)$ is the depth of the leaf node for class $c$.\n\n5.  **Data Simulation and Evaluation**: For each test case, a synthetic dataset will be generated as specified. A set of class labels $\\{c_i\\}$ is drawn from the categorical distribution defined by probabilities $p$. For each label $c_i$, a feature vector $x_i$ is sampled from a multivariate Gaussian distribution $\\mathcal{N}(e_{c_i}, \\sigma^2 I_d)$, where $e_{c_i}$ is the embedding for class $c_i$. The evaluation metrics (perplexity and latency) will be computed for both tree structures on this dataset. The entire process, from data generation to evaluation, is made reproducible by using the specified random seed.\n\nThe final implementation will process each test case, build both tree types, compute the classifiers, and evaluate the required metrics, formatting the output as a list of lists according to the problem specification.",
            "answer": "```python\nimport numpy as np\nimport heapq\nfrom collections import deque, defaultdict\n\nclass Node:\n    \"\"\"Represents a node in the binary tree for hierarchical softmax.\"\"\"\n    def __init__(self, is_leaf, class_id=None, left=None, right=None):\n        self.is_leaf = is_leaf\n        self.class_id = class_id\n        self.left = left\n        self.right = right\n        self.w = None\n        self.b = None\n        self.classes = set([class_id]) if is_leaf else set()\n\ndef generate_data(N, p, E, sigma, rng):\n    \"\"\"Generates synthetic data from a Gaussian mixture model.\"\"\"\n    d = E.shape[1]\n    class_labels = rng.choice(len(p), size=N, p=p)\n    means = E[class_labels]\n    noise = rng.normal(scale=sigma, size=(N, d))\n    X = means + noise\n    C = class_labels\n    return X, C\n\ndef build_huffman_tree(p):\n    \"\"\"Constructs a Huffman tree based on class frequencies.\"\"\"\n    pq = [(prob, i, Node(is_leaf=True, class_id=i)) for i, prob in enumerate(p)]\n    heapq.heapify(pq)\n    \n    counter = len(p)\n    while len(pq) > 1:\n        prob1, _, left_node = heapq.heappop(pq)\n        prob2, _, right_node = heapq.heappop(pq)\n        \n        parent_prob = prob1 + prob2\n        parent_node = Node(is_leaf=False, left=left_node, right=right_node)\n        parent_node.classes = left_node.classes.union(right_node.classes)\n        \n        heapq.heappush(pq, (parent_prob, counter, parent_node))\n        counter += 1\n        \n    return pq[0][2]\n\ndef build_kmeans_tree(class_indices, E, max_iter=100):\n    \"\"\"Recursively constructs a tree using top-down 2-means clustering.\"\"\"\n    if len(class_indices) == 1:\n        return Node(is_leaf=True, class_id=class_indices[0])\n\n    sub_E = E[class_indices, :]\n    \n    # Deterministic initialization of centers\n    min_idx_in_subset = np.argmin(sub_E[:, 0])\n    max_idx_in_subset = np.argmax(sub_E[:, 0])\n    \n    if min_idx_in_subset == max_idx_in_subset:\n        if E.shape[1] > 1:\n            min_idx_in_subset = np.argmin(sub_E[:, 1])\n            max_idx_in_subset = np.argmax(sub_E[:, 1])\n        if min_idx_in_subset == max_idx_in_subset:\n             min_idx_in_subset, max_idx_in_subset = 0, 1\n    \n    center1 = sub_E[min_idx_in_subset]\n    center2 = sub_E[max_idx_in_subset]\n\n    # Handle identical initial centers\n    if np.array_equal(center1, center2):\n      center1 = sub_E[0]\n      center2 = sub_E[1]\n\n    last_assignments = None\n    \n    for _ in range(max_iter):\n        assignments = defaultdict(list)\n        \n        # Assignment step\n        for i, class_idx in enumerate(class_indices):\n            emb = sub_E[i]\n            dist1 = np.sum((emb - center1)**2)\n            dist2 = np.sum((emb - center2)**2)\n            assignments[0 if dist1 = dist2 else 1].append(class_idx)\n        \n        # Empty cluster handling\n        if not assignments[0] or not assignments[1]:\n            non_empty_key = 1 if not assignments[0] else 0\n            empty_key = 1 - non_empty_key\n            non_empty_indices = assignments[non_empty_key]\n            non_empty_center = center1 if non_empty_key == 0 else center2\n            \n            dists = np.sum((E[non_empty_indices] - non_empty_center)**2, axis=1)\n            farthest_idx_in_list = np.argmax(dists)\n            farthest_class_id = non_empty_indices[farthest_idx_in_list]\n            \n            assignments[non_empty_key].remove(farthest_class_id)\n            assignments[empty_key].append(farthest_class_id)\n        \n        # Convergence check\n        current_assignments_tuple = (tuple(sorted(assignments[0])), tuple(sorted(assignments[1])))\n        if current_assignments_tuple == last_assignments:\n            break\n        last_assignments = current_assignments_tuple\n        \n        # Update step\n        if assignments[0]: center1 = np.mean(E[assignments[0]], axis=0)\n        if assignments[1]: center2 = np.mean(E[assignments[1]], axis=0)\n    \n    # Ensure deterministic child assignment\n    left_indices, right_indices = assignments[0], assignments[1]\n    if min(left_indices) > min(right_indices):\n        left_indices, right_indices = right_indices, left_indices\n\n    left_child = build_kmeans_tree(left_indices, E, max_iter)\n    right_child = build_kmeans_tree(right_indices, E, max_iter)\n    \n    node = Node(is_leaf=False, left=left_child, right=right_child)\n    node.classes = left_child.classes.union(right_child.classes)\n    return node\n\ndef compute_classifiers(node, E):\n    \"\"\"Computes and sets classifier parameters (w, b) for all internal nodes.\"\"\"\n    if node.is_leaf:\n        return\n    \n    compute_classifiers(node.left, E)\n    compute_classifiers(node.right, E)\n    \n    m_L = np.mean(E[list(node.left.classes)], axis=0)\n    m_R = np.mean(E[list(node.right.classes)], axis=0)\n    \n    diff = m_R - m_L\n    norm = np.linalg.norm(diff)\n    \n    w = np.zeros_like(m_R) if norm == 0 else diff / norm\n    b = -0.5 * w.T @ (m_L + m_R)\n    \n    node.w, node.b = w, b\n\ndef find_paths_and_depths(root):\n    \"\"\"Traverses the tree to find the path and depth for each class leaf.\"\"\"\n    paths, depths = {}, {}\n    q = deque([(root, [])]) # (node, path_so_far)\n    \n    while q:\n        node, path = q.popleft()\n        \n        if node.is_leaf:\n            paths[node.class_id] = path\n            depths[node.class_id] = len(path)\n            continue\n            \n        q.append((node.left, path + [(node, 'L')]))\n        q.append((node.right, path + [(node, 'R')]))\n        \n    return paths, depths\n\ndef calculate_perplexity_and_latency(tree_root, E, p, X, C):\n    \"\"\"Computes perplexity and latency for a given tree and dataset.\"\"\"\n    compute_classifiers(tree_root, E)\n    paths, depths = find_paths_and_depths(tree_root)\n    \n    # Latency Proxy\n    V = len(p)\n    latency = sum(p[c] * depths[c] for c in range(V) if c in depths)\n    \n    # Perplexity\n    total_nll = 0.0\n    for i in range(X.shape[0]):\n        x_i, c_i = X[i], C[i]\n        path = paths[c_i]\n        \n        log_prob = 0.0\n        for node, direction in path:\n            z = np.dot(node.w, x_i) + node.b\n            # log P(right) = -log(1+exp(-z))\n            # log P(left) = -log(1+exp(z))\n            log_prob -= np.logaddexp(0, -z) if direction == 'R' else np.logaddexp(0, z)\n        \n        total_nll -= log_prob\n    \n    perplexity = np.exp(total_nll / X.shape[0])\n    \n    return perplexity, latency\n\ndef solve():\n    \"\"\"Main function to run test cases and print results.\"\"\"\n    test_cases = [\n        {\"V\": 8, \"d\": 2, \"E\": np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0], [2.0, 0.0], [2.0, 1.0], [3.0, 0.5], [-1.0, 0.5]]), \"p\": np.array([0.30, 0.20, 0.15, 0.12, 0.10, 0.06, 0.04, 0.03]), \"sigma\": 0.20, \"N\": 3000, \"seed\": 7},\n        {\"V\": 8, \"d\": 2, \"E\": np.array([[1.000, 0.000], [0.707, 0.707], [0.000, 1.000], [-0.707, 0.707], [-1.000, 0.000], [-0.707, -0.707], [0.000, -1.000], [0.707, -0.707]]), \"p\": np.array([0.125] * 8), \"sigma\": 0.15, \"N\": 3000, \"seed\": 13},\n        {\"V\": 8, \"d\": 3, \"E\": np.array([[0.0, 0.0, 0.0], [1.0, 0.5, -0.5], [-0.5, 1.0, 0.5], [0.5, -1.0, 0.5], [1.5, 1.5, -1.0], [-1.5, 1.0, 1.0], [0.0, -1.5, -1.0], [2.0, 0.0, 1.0]]), \"p\": np.array([0.6, 0.1, 0.1, 0.05, 0.05, 0.04, 0.03, 0.03]), \"sigma\": 0.25, \"N\": 4000, \"seed\": 23}\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        V, E, p, sigma, N, seed = case[\"V\"], case[\"E\"], case[\"p\"], case[\"sigma\"], case[\"N\"], case[\"seed\"]\n        rng = np.random.default_rng(seed)\n        \n        X, C = generate_data(N, p, E, sigma, rng)\n        \n        # Clustering-based (k-means) tree\n        kmeans_root = build_kmeans_tree(list(range(V)), E)\n        ppl_kmeans, lat_kmeans = calculate_perplexity_and_latency(kmeans_root, E, p, X, C)\n        \n        # Frequency-based (Huffman) tree\n        huffman_root = build_huffman_tree(p)\n        ppl_huffman, lat_huffman = calculate_perplexity_and_latency(huffman_root, E, p, X, C)\n        \n        all_results.append([\n            f\"{ppl_kmeans:.6f}\", f\"{ppl_huffman:.6f}\",\n            f\"{lat_kmeans:.6f}\", f\"{lat_huffman:.6f}\"\n        ])\n\n    result_str = \"[\" + \",\".join([f\"[{','.join(res)}]\" for res in all_results]) + \"]\"\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond deciding which classes are grouped together, the shape of the tree itself—specifically, its branching factor—presents a key design choice. This practice challenges you to navigate the trade-offs between shallow, wide trees and deep, narrow trees by modeling their impact on training time, memory, and accuracy. You will implement a constrained optimization to find the ideal branching factor, revealing how hardware limitations and performance requirements dictate the optimal tree geometry. ",
            "id": "3134865",
            "problem": "Consider a hierarchical softmax classifier that organizes $V$ classes as leaves of a balanced $b$-ary tree, where $b$ is an integer branching factor with $2 \\leq b \\leq V$. In hierarchical softmax, the probability of a class is computed along the unique root-to-leaf path, and the computational cost per example scales with the path length.\n\nUse the following foundational facts as the starting point:\n- The path length $\\ell(b)$ in a balanced $b$-ary tree with $V$ leaves satisfies $\\ell(b) = \\lceil \\log_b V \\rceil$, where $\\log_b V = \\frac{\\ln V}{\\ln b}$ and $\\lceil \\cdot \\rceil$ denotes the ceiling function.\n- The number of internal nodes in a full $b$-ary tree with $V$ leaves is $\\frac{V - 1}{b - 1}$. Each internal node holds a $b$-way classifier with $b$ weight vectors of dimension $d$, so the total parameter count (used as a proxy for memory) is $M(b) = d \\cdot b \\cdot \\frac{V - 1}{b - 1}$.\n- Assume a baseline per-node correctness probability $p_0 \\in (0,1)$, independent of $b$. Under an independence model of node decisions along the path, the overall accuracy for a leaf prediction is $A(b) = p_0^{\\ell(b)}$, which decreases as the path length increases.\n\nYou are given a linear training-time model per example,\n$$\nT(b) = \\alpha \\, \\ell(b) + \\beta \\, b,\n$$\nwhere $\\alpha > 0$ captures the cost per level in the path and $\\beta > 0$ captures the per-node multi-class cost that scales with the branching factor $b$. There is a hardware memory budget $M_{\\max}$ and a required minimum accuracy $A_{\\min}$.\n\nTask:\n- For each specified test case, find the integer branching factor $b \\in \\{2,3,\\dots,V\\}$ that minimizes $T(b)$ subject to the constraints $M(b) \\leq M_{\\max}$ and $A(b) \\geq A_{\\min}$. If no feasible $b$ exists, return $-1$.\n- In case of ties in $T(b)$ among feasible $b$ values, choose the smallest $b$.\n\nInput to your program is fixed by the test suite below. There is no user input.\n\nTest Suite (each case is a tuple $(V, d, \\alpha, \\beta, p_0, A_{\\min}, M_{\\max})$):\n1. $(50000, 64, 1.0, 0.5, 0.95, 0.6, 5000000)$: a large vocabulary with moderate accuracy requirement and a realistic memory budget.\n2. $(50000, 64, 1.0, 0.1, 0.95, 0.9, 10000000)$: the same vocabulary but a high accuracy requirement that strongly constrains the path length.\n3. $(1000, 128, 1.0, 0.3, 0.95, 0.5, 100000)$: an intentionally strict memory budget to test the infeasible case.\n4. $(1024, 32, 0.5, 1.0, 0.9, 0.4, 1000000)$: a smaller vocabulary with parameters set to produce ties in $T(b)$.\n\nOutput Specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[b_1,b_2,b_3,b_4]$, where each $b_i$ is the optimal branching factor for the corresponding test case, or $-1$ if no feasible $b$ exists.\n- All outputs are integers.\n\nYour solution must be a complete, runnable program. It must compute $\\ell(b)$, $M(b)$, $A(b)$, and $T(b)$ using the definitions above and implement the optimization over integer $b$ for each test case.",
            "solution": "The problem presented is a constrained integer optimization task. The goal is to find an optimal integer branching factor, denoted by $b$, for a hierarchical softmax classifier. The optimization objective is to minimize the training time per example, $T(b)$, subject to constraints on memory usage, $M(b)$, and classification accuracy, $A(b)$.\n\nThe search space for the branching factor is the set of integers $b \\in \\{2, 3, \\dots, V\\}$, where $V$ is the vocabulary size.\n\nThe functions governing the model are defined as follows:\n- Path length in the balanced $b$-ary tree: $\\ell(b) = \\lceil \\log_b V \\rceil = \\lceil \\frac{\\ln V}{\\ln b} \\rceil$.\n- Memory cost (parameter count): $M(b) = d \\cdot b \\cdot \\frac{V - 1}{b - 1}$, where $d$ is the vector dimension.\n- Overall accuracy: $A(b) = p_0^{\\ell(b)}$, where $p_0$ is the per-node correctness probability.\n- Training time per example: $T(b) = \\alpha \\cdot \\ell(b) + \\beta \\cdot b$, where $\\alpha > 0$ and $\\beta > 0$.\n\nThe optimization problem can be formally stated as:\n$$\n\\min_{b \\in \\{2, 3, \\dots, V\\}} T(b)\n$$\nsubject to:\n1. $A(b) \\geq A_{\\min}$\n2. $M(b) \\leq M_{\\max}$\n\nWe will analyze each constraint to define a feasible set of $b$ values and then search this set for the one that minimizes $T(b)$.\n\n**Constraint 1: Accuracy**\n\nThe accuracy constraint is $A(b) \\geq A_{\\min}$. Substituting the expression for $A(b)$:\n$$\np_0^{\\ell(b)} \\geq A_{\\min}\n$$\nGiven that $p_0 \\in (0, 1)$, the function $f(x) = p_0^x$ is a decreasing function of $x$. Therefore, taking the logarithm base $p_0$ of both sides reverses the inequality:\n$$\n\\ell(b) \\leq \\log_{p_0} A_{\\min}\n$$\nUsing the change of base formula for logarithms, $\\log_x y = \\frac{\\ln y}{\\ln x}$:\n$$\n\\ell(b) \\leq \\frac{\\ln A_{\\min}}{\\ln p_0}\n$$\nSince $\\ell(b)$ must be an integer (as it is the result of a ceiling function), we can take the floor of the right-hand side to establish a strict integer upper bound on the path length:\n$$\n\\ell(b) \\leq \\left\\lfloor \\frac{\\ln A_{\\min}}{\\ln p_0} \\right\\rfloor\n$$\nLet us define $\\ell_{\\max} = \\lfloor \\frac{\\ln A_{\\min}}{\\ln p_0} \\rfloor$. Any candidate $b$ is only feasible if its corresponding path length $\\ell(b)$ does not exceed this value $\\ell_{\\max}$.\n\n**Constraint 2: Memory**\n\nThe memory constraint is $M(b) \\leq M_{\\max}$:\n$$\nd \\cdot b \\cdot \\frac{V - 1}{b - 1} \\leq M_{\\max}\n$$\nWe can analyze the function $g(b) = \\frac{b}{b - 1} = 1 + \\frac{1}{b - 1}$. For $b \\geq 2$, this is a strictly decreasing function of $b$. Consequently, the memory cost $M(b)$ is also a strictly decreasing function of $b$. This property implies that if the constraint is violated for a certain value $b_0$, it will also be violated for all $b  b_0$. Conversely, if it is satisfied for $b_0$, it will be satisfied for all $b > b_0$. The minimum possible memory usage is $\\lim_{b\\to\\infty} M(b) = d \\cdot (V - 1)$. If this minimum value exceeds $M_{\\max}$, then no feasible $b$ exists.\n\n**Objective Function and Optimization Strategy**\n\nThe objective function to minimize is $T(b) = \\alpha \\cdot \\ell(b) + \\beta \\cdot b$. This function is a sum of two components:\n- $\\alpha \\cdot \\ell(b)$: This term is a non-increasing step function of $b$, as $\\ell(b)$ is non-increasing.\n- $\\beta \\cdot b$: This term is a strictly increasing linear function of $b$.\n\nThe sum of these two functions does not necessarily have a simple convex shape, meaning there could be multiple local minima. Given that the search space for $b$ is a finite set of integers $\\{2, 3, \\dots, V\\}$, a direct search is a robust and feasible approach to find the global minimum.\n\nThe algorithm proceeds as follows for each test case $(V, d, \\alpha, \\beta, p_0, A_{\\min}, M_{\\max})$:\n1. Initialize the optimal branching factor `best_b` to $-1$ and the minimum time `min_T` to infinity.\n2. Calculate the maximum permissible path length, $\\ell_{\\max} = \\lfloor \\frac{\\ln A_{\\min}}{\\ln p_0} \\rfloor$.\n3. Iterate through each integer $b$ in the range $[2, V]$.\n4. For each $b$:\n   a. Calculate the path length $\\ell(b) = \\lceil \\frac{\\ln V}{\\ln b} \\rceil$.\n   b. Check the accuracy constraint: if $\\ell(b) > \\ell_{\\max}$, the current $b$ is not feasible. Continue to the next value of $b$.\n   c. Calculate the memory cost $M(b) = d \\cdot b \\cdot \\frac{V - 1}{b - 1}$.\n   d. Check the memory constraint: if $M(b) > M_{\\max}$, the current $b$ is not feasible. Continue to the next value of $b$.\n   e. If both constraints are satisfied, the current $b$ is feasible. Calculate the training time $T(b) = \\alpha \\cdot \\ell(b) + \\beta \\cdot b$.\n   f. Compare $T(b)$ with the current minimum time `min_T`. If $T(b)  \\text{min\\_T}$, update `min_T` to $T(b)$ and `best_b` to $b$.\n5. The tie-breaking rule, which requires choosing the smallest $b$ in case of ties in $T(b)$, is naturally handled by iterating through $b$ in increasing order and only updating the `best_b` on a strict inequality ($T(b)  \\text{min\\_T}$).\n6. After checking all values of $b$, the final value of `best_b` is the solution for the test case. If no feasible $b$ was found, `best_b` remains $-1$.\n\nThis procedure guarantees finding the optimal $b$ according to the problem's criteria.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal branching factor for hierarchical softmax under given constraints.\n    \"\"\"\n    # Test Suite (each case is a tuple (V, d, alpha, beta, p0, A_min, M_max)):\n    test_cases = [\n        (50000, 64, 1.0, 0.5, 0.95, 0.6, 5000000),\n        (50000, 64, 1.0, 0.1, 0.95, 0.9, 10000000),\n        (1000, 128, 1.0, 0.3, 0.95, 0.5, 100000),\n        (1024, 32, 0.5, 1.0, 0.9, 0.4, 1000000),\n    ]\n\n    results = []\n    for case in test_cases:\n        V, d, alpha, beta, p0, A_min, M_max = case\n\n        best_b = -1\n        min_T = float('inf')\n\n        # Constraint 1: Accuracy\n        # A(b) = p0^l(b) >= A_min  =>  l(b) = log_p0(A_min) = ln(A_min) / ln(p0)\n        # Since p0 and A_min are in (0,1), their logs are negative, so the ratio is positive.\n        if p0 >= 1.0 or A_min = 0.0 or A_min > 1.0:\n            # These edge cases are not expected based on problem description but good practice.\n            # If p0 >= 1, l(b) would need to be negative or infinite which is not possible.\n            # If A_min = 0, constraint is trivially satisfied by any l(b).\n            # If A_min > 1, no solution as p0^l(b) = 1.\n            # For this problem, we can assume valid inputs as per the context.\n            pass\n        \n        # Calculate max allowed path length from the accuracy constraint\n        l_max = np.floor(np.log(A_min) / np.log(p0))\n\n        # Constraint 2 check: Minimum possible memory\n        # M(b) is a decreasing function of b. The infimum is d * (V - 1) as b -> infinity.\n        # If this minimum memory is already over budget, no b is feasible.\n        min_possible_memory = d * (V - 1)\n        if min_possible_memory > M_max:\n            results.append(-1)\n            continue\n        \n        # Iterate through all possible integer branching factors b from 2 to V\n        for b in range(2, V + 1):\n            # Calculate path length l(b) = ceil(log_b(V))\n            path_length = np.ceil(np.log(V) / np.log(b))\n\n            # Check accuracy constraint\n            if path_length > l_max:\n                # As l(b) is non-increasing with b, if we fail this, subsequent\n                # smaller l(b) might pass, but the search space for a specific\n                # l(b) is contiguous. We can continue the simple scan.\n                continue\n\n            # Calculate memory M(b)\n            # The term b / (b - 1) can be large for b=2, so use floating point arithmetic.\n            memory_cost = d * (V - 1) * b / (b - 1)\n\n            # Check memory constraint\n            if memory_cost > M_max:\n                # As M(b) is a decreasing function of b, if this fails for b,\n                # it would have also failed for all smaller b.\n                # However, since the minimal memory check passed, there must be\n                # some larger b (or this b) that satisfies this. We continue scanning.\n                continue\n\n            # If both constraints are met, this b is feasible.\n            # Calculate objective function T(b)\n            time_cost = alpha * path_length + beta * b\n\n            # Check if this is a better solution.\n            # Tie-breaking rule: choose smallest b.\n            # Since we iterate b in increasing order, the first time we find a\n            # minimum T, we set best_b. Subsequent b's with the same T will not\n            # update best_b, fulfilling the rule.\n            if time_cost  min_T:\n                min_T = time_cost\n                best_b = b\n        \n        results.append(int(best_b))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While hierarchical softmax offers efficient training, finding the single most probable class at inference time requires traversing the tree, and finding the top-$k$ classes can be computationally expensive. This exercise introduces beam search, a practical algorithm to approximate the best predictions without exhaustively searching every leaf. By implementing beam search and analyzing the relationship between beam width, accuracy, and latency, you will understand how to balance computational cost and prediction quality in a real-world application. ",
            "id": "3134831",
            "problem": "You are given a binary tree representation of Hierarchical Softmax (HS) for a multi-class classifier. In this representation, each class corresponds to a leaf, and each internal node along a unique root-to-leaf path contributes a Bernoulli decision modeled by a logistic function. Let the feature vector be $x \\in \\mathbb{R}^d$. Each internal node $i$ has a parameter vector $v_i \\in \\mathbb{R}^d$, and the probability of going to the left child at node $i$ is $p_i(x) = \\sigma(v_i^\\top x)$, where $\\sigma(z)$ is the logistic function defined by $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. For a class leaf $c$, its probability conditioned on $x$ is the product of probabilities along its path, using $p_i(x)$ if the path takes the left direction at node $i$ and $1 - p_i(x)$ if the path takes the right direction at node $i$. The log-probability is the sum of the logarithms of those factors.\n\nYour task is to implement beam search inference over this HS tree to approximate the top-$k$ most probable classes for a given $x$. Beam search maintains the top $w$ partial paths (by cumulative log-probability) at each depth and expands them until reaching leaves. You must analyze how the beam width $w$ affects top-$k$ accuracy and latency (measured as the total number of logistic function evaluations) and relate the empirical latency to the theoretical complexity $O(w \\ell)$, where $\\ell$ is the path length (tree depth from root to leaves).\n\nUse the following fixed HS model and dataset. The binary tree has depth $\\ell = 3$ (root at depth $0$, leaves at depth $3$), internal nodes indexed $0$ through $6$, and leaves indexed $0$ through $7$ corresponding to decision codes $(\\text{L},\\text{L},\\text{L}) \\mapsto 0$, $(\\text{L},\\text{L},\\text{R}) \\mapsto 1$, $(\\text{L},\\text{R},\\text{L}) \\mapsto 2$, $(\\text{L},\\text{R},\\text{R}) \\mapsto 3$, $(\\text{R},\\text{L},\\text{L}) \\mapsto 4$, $(\\text{R},\\text{L},\\text{R}) \\mapsto 5$, $(\\text{R},\\text{R},\\text{L}) \\mapsto 6$, $(\\text{R},\\text{R},\\text{R}) \\mapsto 7$. The adjacency of internal nodes is: node $0$ has children $(1,2)$, node $1$ has $(3,4)$, node $2$ has $(5,6)$. Let $d = 3$. The parameter vectors are:\n$v_0 = [0.8, -0.5, 0.3]$, $v_1 = [0.5, 0.4, -0.6]$, $v_2 = [-0.4, 0.3, 0.7]$, $v_3 = [0.2, -0.9, 0.1]$, $v_4 = [-0.5, 0.2, -0.1]$, $v_5 = [0.6, -0.2, 0.4]$, $v_6 = [-0.3, 0.7, -0.8]$.\n\nThe dataset consists of $6$ feature-label pairs $(x, y)$ with feature vectors and ground-truth leaf indices:\n$x_1 = [0.9, -0.1, 0.3], y_1 = 0$;\n$x_2 = [0.2, 0.8, -0.5], y_2 = 6$;\n$x_3 = [-0.4, 0.1, 0.9], y_3 = 2$;\n$x_4 = [0.3, -0.6, -0.2], y_4 = 1$;\n$x_5 = [-0.1, 0.5, -0.7], y_5 = 5$;\n$x_6 = [0.0, -0.3, 0.4], y_6 = 3$.\n\nImplement beam search that, for each feature vector $x$, returns the top-$k$ predicted leaf indices (based on cumulative log-probability) and counts the number of logistic function evaluations performed during the search for that $x$. The top-$k$ accuracy for a given $(w,k)$ is defined as the fraction (expressed as a decimal) of the dataset for which the ground-truth label $y$ is contained within the top-$k$ predictions. The latency is defined as the average number of logistic function evaluations per example. You should not measure wall-clock time; only report the average count of logistic evaluations as a floating-point number.\n\nTest suite: Run the program for the following $(w,k)$ pairs to cover different regimes, including a greedy boundary, moderate width, near-exhaustive width, and larger width than the number of leaves:\n$(w,k) = (1,1)$, $(w,k) = (2,2)$, $(w,k) = (4,3)$, $(w,k) = (10,3)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case in the specified order, append the top-$k$ accuracy (as a decimal between $0$ and $1$) followed by the average latency (floating-point number), yielding a flat list of length $8$ with the pattern $[\\text{acc}_{1}, \\text{lat}_{1}, \\text{acc}_{2}, \\text{lat}_{2}, \\text{acc}_{3}, \\text{lat}_{3}, \\text{acc}_{4}, \\text{lat}_{4}]$.\n\nYour implementation must be self-contained and use no external inputs. The analysis must be purely mathematical and logical, based on the HS structure and the definitions given, and must relate the observed latency to the theoretical complexity $O(w \\ell)$.",
            "solution": "The problem requires the implementation and analysis of beam search for inference in a Hierarchical Softmax (HS) model. The solution involves three main components: understanding the HS model and its associated probability calculations, implementing the beam search algorithm, and evaluating its performance in terms of accuracy and latency for a given set of test cases.\n\n### 1. Hierarchical Softmax Model and Tree Structure\n\nThe HS model is defined on a complete binary tree of depth $\\ell=3$. The tree consists of $2^\\ell-1 = 7$ internal nodes (indexed $0$ to $6$) and $2^\\ell=8$ leaf nodes (indexed $0$ to $7$), which represent the classes.\n\nThe relationship between nodes is standard for a complete binary tree: for an internal node with index $i$, its left child has index $2i+1$ and its right child has index $2i+2$. The root of the tree is node $0$. The internal nodes are structured as follows:\n- Depth 0: Node $0$\n- Depth 1: Nodes $1, 2$\n- Depth 2: Nodes $3, 4, 5, 6$\n\nEach class (leaf) is uniquely identified by a path of length $\\ell=3$ from the root. The problem specifies a mapping from a sequence of Left/Right decisions to a leaf index, e.g., $(\\text{L},\\text{L},\\text{L}) \\mapsto 0$. This corresponds to a binary encoding where L is $0$ and R is $1$. For a leaf with index $c$, its 3-bit binary representation `b_2 b_1 b_0` defines the path. For example, class $c=6 = (110)_2$ corresponds to the decision path (Right, Right, Left).\n\nThe path for any class $c$ can be determined by its binary representation. The sequence of internal nodes visited along the path to leaf $c$ can be constructed iteratively. Starting from the root (node $0$), the path choices are dictated by the bits of $c$. For path length $\\ell=3$, the decision at depth $d \\in \\{0, 1, 2\\}$ is determined by bit $2-d$ of $c$'s binary representation.\n\nThe probability of a class $c$ given a feature vector $x \\in \\mathbb{R}^d$ is the product of conditional probabilities at each internal node along its path. At an internal node $i$ with parameter vector $v_i$, the probability of taking the left branch is $p_i(x) = \\sigma(v_i^\\top x)$, and the right branch is $1-p_i(x) = \\sigma(-v_i^\\top x)$, where $\\sigma(z) = 1/(1+e^{-z})$ is the logistic sigmoid function.\n\nSince probabilities are multiplied, their logarithms are summed. This is numerically more stable. The log-probabilities for left and right turns are:\n- Left turn: $\\log(p_i(x)) = \\log(\\sigma(v_i^\\top x)) = \\log\\left(\\frac{1}{1+e^{-v_i^\\top x}}\\right) = -\\log(1+e^{-v_i^\\top x})$\n- Right turn: $\\log(1-p_i(x)) = \\log(1 - \\sigma(v_i^\\top x)) = \\log\\left(\\frac{e^{-v_i^\\top x}}{1+e^{-v_i^\\top x}}\\right) = -v_i^\\top x - \\log(1+e^{-v_i^\\top x})$\n\nThe total log-probability for a class $c$ is $\\log P(c|x) = \\sum_{j=0}^{\\ell-1} \\log P(\\text{decision}_j | \\text{node}_j, x)$, where $(\\text{node}_j, \\text{decision}_j)$ is the sequence of nodes and decisions on the path to $c$.\n\n### 2. Beam Search Algorithm\n\nBeam search is a heuristic search algorithm that explores the tree by keeping a limited number of `w` (the beam width) most promising partial paths at each depth.\n\nThe algorithm proceeds as follows for a given feature vector $x$ and beam width $w$:\n1.  **Initialization**: Start at the root (node $0$). The initial beam contains one hypothesis: a tuple of (log-probability, node index), which is $(0.0, 0)$. The total number of logistic function evaluations is initialized to $0$.\n\n2.  **Iteration over Depths**: For each depth $d$ from $0$ to $\\ell-2=1$:\n    a. Create an empty list of `candidates`.\n    b. For each hypothesis (current log-probability `log_p`, current node index `node_idx`) in the current beam:\n        i. Compute the dot product $z = v_{\\text{node\\_idx}}^\\top x$.\n        ii. Increment the evaluation count by $1$.\n        iii. **Left child**: The new hypothesis is `(log_p - \\log(1+e^{-z}), 2 \\cdot \\text{node\\_idx} + 1)`.\n        iv. **Right child**: The new hypothesis is `(log_p - z - \\log(1+e^{-z}), 2 \\cdot \\text{node\\_idx} + 2)`.\n        v. Add both new hypotheses to the `candidates` list.\n    c. Sort `candidates` in descending order of log-probability.\n    d. The new beam for the next depth is the top `w` hypotheses from the sorted `candidates`.\n\n3.  **Final Expansion to Leaves**: After iterating to depth $\\ell-1=2$, the beam contains up to `w` hypotheses corresponding to the internal nodes at this depth (nodes $3, 4, 5, 6$).\n    a. Create an empty list of `final_leaves`.\n    b. For each remaining hypothesis (`log_p`, `node_idx`):\n        i. Perform one final expansion similar to step 2.b, calculating log-probabilities for the two child leaves.\n        ii. The leaf indices are determined from the parent `node_idx`. For a tree of depth $\\ell=3$, the leaf indices for a parent node `i` at depth $2$ are $2(i - (2^2-1))$ and $2(i - (2^2-1)) + 1$.\n        iii. Add both (log-probability, leaf index) tuples to `final_leaves`.\n    c. Sort `final_leaves` in descending order of log-probability.\n\n4.  **Result**: The top-$k$ predictions are the leaf indices from the first $k$ entries in the sorted `final_leaves`. The total number of logistic function evaluations is the recorded latency for this sample.\n\n### 3. Latency Analysis\n\nThe latency is defined as the number of logistic function evaluations. This count depends on the beam width `w` and the tree structure, but not on the input vector $x$.\n- At depth $d=0$ (root), we expand $1$ node. This requires $1$ evaluation. The number of candidates generated is $2$. The next beam will have size $\\min(w, 2)$.\n- At depth $d=1$, we expand $\\min(w, 2)$ nodes. This requires $\\min(w, 2)$ evaluations. The number of candidates is $2 \\cdot \\min(w, 2)$. The next beam has size $\\min(w, 4)$.\n- At depth $d=2$ (final expansion), we expand $\\min(w, 4)$ nodes. This requires $\\min(w, 4)$ evaluations.\n\nThe total number of evaluations (latency) for a tree of depth $\\ell=3$ is therefore: $1 + \\min(w, 2) + \\min(w, 4)$.\n\nFor the specified test cases, the theoretical latency is:\n- $w=1$: $1 + \\min(1, 2) + \\min(1, 4) = 1 + 1 + 1 = 3$. This is greedy search.\n- $w=2$: $1 + \\min(2, 2) + \\min(2, 4) = 1 + 2 + 2 = 5$.\n- $w=4$: $1 + \\min(4, 2) + \\min(4, 4) = 1 + 2 + 4 = 7$. This is an exhaustive search of all internal nodes.\n- $w=10$: $1 + \\min(10, 2) + \\min(10, 4) = 1 + 2 + 4 = 7$. Since the tree width at any level does not exceed $4$, a beam width greater than $4$ does not change the search path and yields the same result as $w=4$.\n\nThe theoretical complexity is given as $O(w\\ell)$. Our derived latency formula for a balanced binary tree is $\\sum_{d=0}^{\\ell-1} \\min(w, 2^d)$. For small $w$ (i.e., $w \\ll 2^d$), this sum is approximately $1+(\\ell-1)w$, which is indeed $O(w\\ell)$. The empirical latency measured by the implementation should match these theoretical values exactly, as the calculation count is deterministic.\n\n### 4. Accuracy Calculation\n\nThe top-$k$ accuracy is the fraction of data samples for which the true class label $y$ is present in the set of $k$ leaf indices returned by the beam search. For a dataset of size $N$, if $N_{correct}$ is the number of samples where $y$ is found in the top-$k$ predictions, the accuracy is $N_{correct}/N$.\n\nThe implementation will now proceed by applying the beam search algorithm for each $(w,k)$ pair on the entire dataset to compute the required accuracy and latency metrics.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the hierarchical softmax beam search problem.\n    \"\"\"\n    \n    # --- Givens from the problem statement ---\n\n    # Hierarchical Softmax Model Parameters\n    # Dimension d=3, depth l=3\n    V = np.array([\n        [0.8, -0.5, 0.3],   # v_0\n        [0.5, 0.4, -0.6],   # v_1\n        [-0.4, 0.3, 0.7],   # v_2\n        [0.2, -0.9, 0.1],   # v_3\n        [-0.5, 0.2, -0.1],   # v_4\n        [0.6, -0.2, 0.4],   # v_5\n        [-0.3, 0.7, -0.8]    # v_6\n    ])\n\n    # Dataset (feature vectors X and ground-truth labels y)\n    X = np.array([\n        [0.9, -0.1, 0.3],   # x_1\n        [0.2, 0.8, -0.5],   # x_2\n        [-0.4, 0.1, 0.9],   # x_3\n        [0.3, -0.6, -0.2],   # x_4\n        [-0.1, 0.5, -0.7],   # x_5\n        [0.0, -0.3, 0.4]    # x_6\n    ])\n    \n    Y = np.array([0, 6, 2, 1, 5, 3])\n\n    # Test cases: (beam_width, k)\n    test_cases = [\n        (1, 1),\n        (2, 2),\n        (4, 3),\n        (10, 3)\n    ]\n    \n    # --- Helper Functions ---\n    \n    def log_sigmoid(z):\n        \"\"\"Numerically stable log of the sigmoid function.\"\"\"\n        return -np.log(1 + np.exp(-z))\n\n    def beam_search(x, w, l=3):\n        \"\"\"\n        Performs beam search on the HS tree.\n        \n        Args:\n            x (np.ndarray): The feature vector.\n            w (int): The beam width.\n            l (int): The depth of the tree.\n            \n        Returns:\n            tuple: A tuple containing:\n                - list: A sorted list of (log_prob, leaf_index) tuples.\n                - int: The number of logistic function evaluations (latency).\n        \"\"\"\n        eval_count = 0\n        \n        # Initial beam: (log_prob, node_index)\n        beam = [(0.0, 0)]\n\n        # Expand beam layer by layer until parent of leaves\n        for depth in range(l - 1):\n            candidates = []\n            for log_prob, node_idx in beam:\n                z = V[node_idx] @ x\n                eval_count += 1\n                \n                # Left child\n                left_child_idx = 2 * node_idx + 1\n                log_prob_left = log_prob + log_sigmoid(z)\n                candidates.append((log_prob_left, left_child_idx))\n                \n                # Right child\n                right_child_idx = 2 * node_idx + 2\n                log_prob_right = log_prob + log_sigmoid(-z) # log(1-sigma(z))\n                candidates.append((log_prob_right, right_child_idx))\n                \n            # Sort all candidates by log-probability (descending)\n            candidates.sort(key=lambda item: item[0], reverse=True)\n            \n            # Prune to beam width w\n            beam = candidates[:w]\n\n        # Final expansion from last internal nodes to leaves\n        final_leaves = []\n        nodes_at_last_internal_depth = 2**(l - 1) - 1\n        for log_prob, node_idx in beam:\n            z = V[node_idx] @ x\n            eval_count += 1\n            \n            # Calculate leaf indices\n            leaf_idx_start = 2 * (node_idx - nodes_at_last_internal_depth)\n            \n            # Left leaf\n            log_prob_left = log_prob + log_sigmoid(z)\n            final_leaves.append((log_prob_left, leaf_idx_start))\n            \n            # Right leaf\n            log_prob_right = log_prob + log_sigmoid(-z)\n            final_leaves.append((log_prob_right, leaf_idx_start + 1))\n            \n        # Sort final leaves by log-probability\n        final_leaves.sort(key=lambda item: item[0], reverse=True)\n        \n        return final_leaves, eval_count\n\n    # --- Main Logic ---\n\n    results = []\n    num_samples = len(Y)\n\n    for w, k in test_cases:\n        total_correct = 0\n        total_latency = 0\n        \n        for i in range(num_samples):\n            x = X[i]\n            y_true = Y[i]\n            \n            # Perform beam search\n            sorted_leaves, latency = beam_search(x, w)\n            \n            # Get top-k predictions\n            top_k_preds = [leaf_idx for _, leaf_idx in sorted_leaves[:k]]\n            \n            # Check for accuracy\n            if y_true in top_k_preds:\n                total_correct += 1\n                \n            total_latency += latency\n        \n        # Calculate final metrics for this (w, k) pair\n        accuracy = total_correct / num_samples\n        avg_latency = total_latency / num_samples\n        \n        results.extend([accuracy, avg_latency])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}