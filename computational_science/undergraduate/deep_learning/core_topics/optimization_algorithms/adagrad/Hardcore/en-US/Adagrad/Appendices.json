{
    "hands_on_practices": [
        {
            "introduction": "One of the primary challenges in optimization is navigating ill-conditioned landscapes, where the sensitivity of the objective function varies dramatically along different parameter axes. Standard Stochastic Gradient Descent (SGD) struggles in these scenarios, as a single learning rate is often too large for steep directions and too small for flat ones. This coding exercise allows you to build and compare SGD and Adagrad on a carefully constructed quadratic function, providing a clear, quantitative demonstration of Adagrad's strength in balancing progress across dimensions with disparate curvatures.",
            "id": "3095498",
            "problem": "You are asked to implement and compare Stochastic Gradient Descent (SGD) and the Adaptive Gradient Method (Adagrad) on a strictly convex, separable quadratic objective and to quantify their behavior along different coordinate axes when the curvature differs by several orders of magnitude. Your implementation must be a complete, runnable program that uses only the standard library and the specified scientific libraries and prints the required outputs in the exact format.\n\nConsider the function defined by the separable convex quadratic\n$$\nf(\\boldsymbol{\\theta}) \\;=\\; \\tfrac{1}{2}\\sum_{i=1}^{d} a_i\\, \\theta_i^2,\n$$\nwhere $d$ is the dimension, $\\boldsymbol{\\theta}\\in\\mathbb{R}^d$, and $a_i \\gt 0$ are curvature coefficients. This function is differentiable with gradient given by the vector of partial derivatives and its Hessian is diagonal with diagonal entries $a_i$, ensuring strict convexity.\n\nFundamental base facts you must use:\n- For any differentiable function $f$, the gradient is the vector of partial derivatives, and moving opposite to the gradient reduces the function locally when the step size is sufficiently small (first-order optimality principle).\n- For the separable quadratic above, the partial derivative with respect to $\\theta_i$ equals the product of the curvature coefficient and the coordinate, which is a consequence of the power rule and linearity of differentiation.\n- Stochastic Gradient Descent (SGD) is the method that iteratively updates parameters by subtracting a learning-rate-scaled gradient.\n- Adagrad (Adaptive Gradient Method) is a per-coordinate adaptive step-size method that scales each coordinate’s step by the inverse of the square root of the cumulative sum of past squared gradients for that coordinate plus a small positive constant to ensure numerical stability.\n\nYour tasks:\n1) Implement two optimization procedures starting from the same initial point $\\boldsymbol{\\theta}_0$:\n   - Stochastic Gradient Descent (SGD): apply the canonical iterative update that subtracts a constant-step-size multiple of the gradient at each step.\n   - Adaptive Gradient Method (Adagrad): apply the canonical per-coordinate adaptive step-size scheme that accumulates squared gradients and scales the step size coordinate-wise by the inverse square root of that accumulation plus a fixed positive constant.\n\n2) For each optimizer and each test case, simulate exactly $T$ steps and compute:\n   - The final parameter vector $\\boldsymbol{\\theta}_T$ for each optimizer separately.\n   - The objective values $f(\\boldsymbol{\\theta}_T)$ for each optimizer.\n   - The per-axis progress fractions\n     $$\n     \\rho_i \\;=\\; \\frac{\\lvert \\theta_{0,i}\\rvert - \\lvert \\theta_{T,i}\\rvert}{\\lvert \\theta_{0,i}\\rvert},\n     $$\n     clipped into the interval $\\left[0,1\\right]$ to handle potential numerical anomalies, for each optimizer.\n   - The axis-wise progress anisotropy index for each optimizer, defined as\n     $$\n     \\mathrm{anisotropy} \\;=\\; \\frac{\\max_i \\rho_i}{\\min_i \\rho_i},\n     $$\n     with the convention that if $\\min_i \\rho_i = 0$, the anisotropy is treated as $+\\infty$.\n   - The empirical condition-number sensitivity for each optimizer, defined as the slope of the least-squares linear fit of the pairs $\\left(\\log a_i,\\, \\log \\rho_i\\right)$ over $i\\in\\{1,\\dots,d\\}$, where the logarithm is the natural logarithm and $\\rho_i$ are strictly positive by clipping to a small positive lower bound if needed. Concretely, with $x_i = \\log a_i$ and $y_i = \\log\\left(\\max(\\rho_i, \\delta)\\right)$ for a tiny $\\delta \\gt 0$, compute\n     $$\n     s \\;=\\; \\frac{\\sum_{i=1}^{d} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{d} (x_i - \\bar{x})^2},\n     $$\n     where $\\bar{x}$ and $\\bar{y}$ are the sample means of $\\{x_i\\}$ and $\\{y_i\\}$, respectively. Larger $\\lvert s\\rvert$ indicates stronger dependence of progress on curvature; values near $0$ indicate reduced sensitivity to the condition number.\n\n3) For each test case, also compute the condition number\n   $$\n   \\kappa \\;=\\; \\frac{\\max_i a_i}{\\min_i a_i}.\n   $$\n\nTest suite:\nImplement exactly the following three test cases, each with dimension $d$, curvature vector $\\boldsymbol{a}$, initial point $\\boldsymbol{\\theta}_0$, number of steps $T$, and learning-rate hyperparameters. All numerical values below must be used exactly as provided.\n\n- Test case $1$ (happy path, multi-order curvature, moderate steps):\n  - $d = 3$\n  - $\\boldsymbol{a} = \\left[10^{-2},\\, 1,\\, 10^{2}\\right]$\n  - $\\boldsymbol{\\theta}_0 = \\left[10,\\, -10,\\, 10\\right]$\n  - $T = 200$\n  - SGD learning rate $\\eta_{\\mathrm{sgd}} = 0.015$\n  - Adagrad base learning rate $\\eta_{\\mathrm{ada}} = 1.0$\n  - Adagrad stability constant $\\epsilon = 10^{-8}$\n\n- Test case $2$ (very ill-conditioned, mixed small and large curvature):\n  - $d = 4$\n  - $\\boldsymbol{a} = \\left[10^{-4},\\, 10^{-2},\\, 1,\\, 10^{3}\\right]$\n  - $\\boldsymbol{\\theta}_0 = \\left[1,\\, 1,\\, 1,\\, 1\\right]$\n  - $T = 400$\n  - SGD learning rate $\\eta_{\\mathrm{sgd}} = 0.001$\n  - Adagrad base learning rate $\\eta_{\\mathrm{ada}} = 1.0$\n  - Adagrad stability constant $\\epsilon = 10^{-8}$\n\n- Test case $3$ (edge condition with extreme curvature and large steps):\n  - $d = 2$\n  - $\\boldsymbol{a} = \\left[1,\\, 10^{6}\\right]$\n  - $\\boldsymbol{\\theta}_0 = \\left[10^{3},\\, 10^{3}\\right]$\n  - $T = 5000$\n  - SGD learning rate $\\eta_{\\mathrm{sgd}} = 10^{-6}$\n  - Adagrad base learning rate $\\eta_{\\mathrm{ada}} = 1.0$\n  - Adagrad stability constant $\\epsilon = 10^{-8}$\n\nOutput specification:\n- For each test case, produce a list of $7$ real numbers in the following order:\n  $$\n  \\left[\\kappa,\\ \\mathrm{anisotropy}_{\\mathrm{sgd}},\\ \\mathrm{anisotropy}_{\\mathrm{ada}},\\ f\\!\\left(\\boldsymbol{\\theta}^{\\mathrm{sgd}}_{T}\\right),\\ f\\!\\left(\\boldsymbol{\\theta}^{\\mathrm{ada}}_{T}\\right),\\ s_{\\mathrm{sgd}},\\ s_{\\mathrm{ada}}\\right].\n  $$\n- Aggregate the three per-test-case result lists into a single outer list and print that outer list on a single line with no spaces, using square brackets and comma separators, for example:\n  $$\n  \\left[[x_{1,1},x_{1,2},\\dots,x_{1,7}],[x_{2,1},\\dots,x_{2,7}],[x_{3,1},\\dots,x_{3,7}]\\right].\n  $$\n- All outputs are pure numbers without units. Angles are not involved. If any intermediate value would make $\\log 0$ appear, your implementation must clip the argument to a tiny positive number, for example $\\delta = 10^{-300}$, prior to taking the logarithm to ensure a finite result.\n\nYour program must not read any input and must exactly implement the values and ordering above. The final print must contain only the single-line output in the specified format.",
            "solution": "The problem has been validated and is deemed sound, well-posed, and formalizable. The provided parameters and definitions are sufficient and consistent for a unique solution.\n\nThe core of this problem is to compare the performance of two fundamental gradient-based optimization algorithms, Stochastic Gradient Descent (SGD) and the Adaptive Gradient Method (Adagrad), on a specific type of objective function. The function is a separable convex quadratic, which serves as an ideal testbed because its coordinate-wise curvature can be precisely controlled, allowing for a clear analysis of how each algorithm handles ill-conditioning.\n\nThe objective function is defined as:\n$$\nf(\\boldsymbol{\\theta}) \\;=\\; \\tfrac{1}{2}\\sum_{i=1}^{d} a_i\\, \\theta_i^2\n$$\nwhere $\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_d) \\in\\mathbb{R}^d$ is the parameter vector, and $\\boldsymbol{a} = (a_1, \\dots, a_d)$ with $a_i > 0$ is the vector of curvature coefficients. The global minimum of this function is uniquely at $\\boldsymbol{\\theta} = \\mathbf{0}$.\n\nThe gradient vector $\\nabla f(\\boldsymbol{\\theta})$ has components given by the partial derivatives:\n$$\n\\frac{\\partial f}{\\partial \\theta_i} \\;=\\; a_i \\theta_i\n$$\nThe Hessian matrix is a diagonal matrix with entries $a_i$, confirming that the $a_i$ values are indeed the curvatures along each principal axis of the function's level sets, which are hyperellipsoids.\n\n**Stochastic Gradient Descent (SGD)**\n\nSGD is an iterative optimization algorithm that updates parameters by moving them in the opposite direction of the gradient. The update rule for the entire parameter vector $\\boldsymbol{\\theta}$ at step $t$ is:\n$$\n\\boldsymbol{\\theta}_{t+1} \\;=\\; \\boldsymbol{\\theta}_t - \\eta_{\\mathrm{sgd}} \\nabla f(\\boldsymbol{\\theta}_t)\n$$\nwhere $\\eta_{\\mathrm{sgd}}$ is the learning rate, a positive scalar constant. Because the function is separable, we can analyze the update for each component $\\theta_i$ independently:\n$$\n\\theta_{t+1, i} \\;=\\; \\theta_{t, i} - \\eta_{\\mathrm{sgd}} (a_i \\theta_{t, i}) \\;=\\; (1 - \\eta_{\\mathrm{sgd}} a_i) \\theta_{t, i}\n$$\nThis reveals that each coordinate's value decays geometrically towards $0$. The rate of decay is determined by the factor $(1 - \\eta_{\\mathrm{sgd}} a_i)$. For the algorithm to converge, we must have $|1 - \\eta_{\\mathrm{sgd}} a_i| < 1$, which implies $0 < \\eta_{\\mathrm{sgd}} a_i < 2$. To ensure convergence for all coordinates simultaneously, the learning rate must be chosen to satisfy this for the largest curvature, i.e., $\\eta_{\\mathrm{sgd}} < 2 / \\max_i a_i$. This choice, however, imposes a very slow convergence rate on coordinates with small curvature $a_i$, as their decay factor $(1 - \\eta_{\\mathrm{sgd}} a_i)$ will be very close to $1$. This is the fundamental weakness of SGD in ill-conditioned problems: a single learning rate cannot be optimal for all coordinates.\n\n**Adaptive Gradient Method (Adagrad)**\n\nAdagrad addresses the shortcoming of SGD by using a per-parameter adaptive learning rate. It scales the learning rate for each parameter inversely to the square root of the sum of the squares of all its past gradients. The update rule for component $\\theta_i$ at step $t$ is:\n$$\n\\theta_{t+1, i} \\;=\\; \\theta_{t, i} - \\frac{\\eta_{\\mathrm{ada}}}{\\sqrt{S_{t,i} + \\epsilon}} g_{t,i}\n$$\nwhere $\\eta_{\\mathrm{ada}}$ is a base learning rate, $g_{t,i} = a_i \\theta_{t,i}$ is the gradient component, and $\\epsilon$ is a small-valued stability constant to prevent division by zero. The term $S_{t,i}$ is the accumulator for squared gradients:\n$$\nS_{t,i} \\;=\\; \\sum_{k=1}^{t} g_{k,i}^2\n$$\nThe effective learning rate for parameter $\\theta_i$ at step $t$ is $\\frac{\\eta_{\\mathrm{ada}}}{\\sqrt{S_{t,i} + \\epsilon}}$. For coordinates with high curvature (large $a_i$), the gradients $g_{t,i}$ will be large in magnitude, causing $S_{t,i}$ to grow quickly. This rapidly decreases the effective learning rate, preventing divergence and overshooting. Conversely, for coordinates with low curvature (small $a_i$), the gradients are small, $S_{t,i}$ grows slowly, and the effective learning rate remains high, allowing for substantial progress. This mechanism automatically balances the learning rates, leading to more uniform progress across all dimensions, even in poorly conditioned problems.\n\n**Analysis Metrics**\n\nThe problem requires the calculation of several metrics to quantify the performance and behavior of each optimizer:\n1.  **Condition Number ($\\kappa$)**: $\\kappa = \\frac{\\max_i a_i}{\\min_i a_i}$. This measures the ill-conditioning of the problem. A large $\\kappa$ indicates a wide disparity in curvatures.\n2.  **Per-axis Progress Fraction ($\\rho_i$)**: $\\rho_i = \\frac{\\lvert \\theta_{0,i}\\rvert - \\lvert \\theta_{T,i}\\rvert}{\\lvert \\theta_{0,i}\\rvert}$, clipped to $[0,1]$. This measures the fractional progress made towards the minimum along each axis, with $1$ being full progress and $0$ being no progress.\n3.  **Progress Anisotropy**: $\\mathrm{anisotropy} = \\frac{\\max_i \\rho_i}{\\min_i \\rho_i}$. This ratio quantifies the non-uniformity of progress across axes. A value of $1$ indicates perfectly isotropic (uniform) progress. We expect Adagrad to yield a lower anisotropy than SGD.\n4.  **Condition-Number Sensitivity ($s$)**: This is the slope of the linear regression of $\\log \\rho_i$ against $\\log a_i$. It measures how strongly the progress on an axis depends on its curvature. For SGD, we expect a strong negative correlation, meaning higher curvature leads to less progress (a negative slope $s$). For Adagrad, which is designed to counteract this effect, we expect the progress $\\rho_i$ to be largely independent of $a_i$, resulting in a sensitivity $s$ close to $0$.\n\nThe following implementation will simulate SGD and Adagrad for the specified test cases and calculate these metrics to demonstrate their contrasting behaviors.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares SGD and Adagrad optimizers on a separable quadratic\n    objective function for a suite of test cases, and prints the results in the\n    specified format.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"d\": 3,\n            \"a\": np.array([1e-2, 1.0, 1e2]),\n            \"theta_0\": np.array([10.0, -10.0, 10.0]),\n            \"T\": 200,\n            \"eta_sgd\": 0.015,\n            \"eta_ada\": 1.0,\n            \"epsilon\": 1e-8,\n        },\n        {\n            \"d\": 4,\n            \"a\": np.array([1e-4, 1e-2, 1.0, 1e3]),\n            \"theta_0\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"T\": 400,\n            \"eta_sgd\": 0.001,\n            \"eta_ada\": 1.0,\n            \"epsilon\": 1e-8,\n        },\n        {\n            \"d\": 2,\n            \"a\": np.array([1.0, 1e6]),\n            \"theta_0\": np.array([1e3, 1e3]),\n            \"T\": 5000,\n            \"eta_sgd\": 1e-6,\n            \"eta_ada\": 1.0,\n            \"epsilon\": 1e-8,\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        a = case[\"a\"]\n        theta_0 = case[\"theta_0\"]\n        T = case[\"T\"]\n        eta_sgd = case[\"eta_sgd\"]\n        eta_ada = case[\"eta_ada\"]\n        epsilon = case[\"epsilon\"]\n        \n        # --- SGD Simulation ---\n        theta_sgd = theta_0.copy()\n        for _ in range(T):\n            grad = a * theta_sgd\n            theta_sgd -= eta_sgd * grad\n        \n        # --- Adagrad Simulation ---\n        theta_ada = theta_0.copy()\n        sum_sq_grad = np.zeros_like(theta_ada)\n        for _ in range(T):\n            grad = a * theta_ada\n            sum_sq_grad += grad**2\n            theta_ada -= eta_ada * grad / (np.sqrt(sum_sq_grad) + epsilon)\n\n        # --- Metrics Calculation Function ---\n        def calculate_optimizer_metrics(theta_T, theta_0, a):\n            # Final objective value\n            f_val = 0.5 * np.sum(a * theta_T**2)\n            \n            # Per-axis progress fraction (rho)\n            abs_theta_0 = np.abs(theta_0)\n            # Avoid division by zero if an initial coordinate is 0\n            safe_abs_theta_0 = np.where(abs_theta_0 == 0, 1.0, abs_theta_0)\n            rho = (abs_theta_0 - np.abs(theta_T)) / safe_abs_theta_0\n            rho = np.clip(rho, 0, 1)\n\n            # Anisotropy\n            min_rho = np.min(rho)\n            if min_rho == 0.0:\n                anisotropy = np.inf\n            else:\n                anisotropy = np.max(rho) / min_rho\n            \n            # Empirical condition-number sensitivity (s)\n            # Use a tiny delta to avoid log(0) as per problem statement\n            delta = 1e-300\n            log_a = np.log(a)\n            log_rho = np.log(np.maximum(rho, delta))\n            \n            x = log_a\n            y = log_rho\n            x_mean = np.mean(x)\n            y_mean = np.mean(y)\n            \n            # The denominator is zero only if all a_i are identical.\n            numerator = np.sum((x - x_mean) * (y - y_mean))\n            denominator = np.sum((x - x_mean)**2)\n            s = numerator / denominator if denominator != 0 else 0.0\n            \n            return anisotropy, f_val, s\n\n        # --- Aggregate Results for the Case ---\n        kappa = np.max(a) / np.min(a)\n        \n        anisotropy_sgd, f_sgd, s_sgd = calculate_optimizer_metrics(theta_sgd, theta_0, a)\n        anisotropy_ada, f_ada, s_ada = calculate_optimizer_metrics(theta_ada, theta_0, a)\n\n        case_results = [\n            kappa,\n            anisotropy_sgd,\n            anisotropy_ada,\n            f_sgd,\n            f_ada,\n            s_sgd,\n            s_ada\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified\n    sublist_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(sublist_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Having seen Adagrad's practical benefits, we can now delve deeper into its theoretical underpinnings. Adagrad's per-parameter scaling can be understood as a form of diagonal preconditioning, an efficient approximation of more powerful, but computationally expensive, second-order methods. This analytical exercise challenges you to compare the Adagrad update direction with that of gradient descent on perfectly 'whitened' features, revealing the connection between Adagrad's diagonal matrix and the full covariance matrix inverse used in ideal preconditioning.",
            "id": "3095404",
            "problem": "Consider linear regression with ridge regularization defined by the empirical risk $$L(\\mathbf{w}) = \\frac{1}{2n}\\|X\\mathbf{w} - \\mathbf{y}\\|^{2} + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2},$$ where $X \\in \\mathbb{R}^{n \\times d}$ is the feature matrix, $\\mathbf{y} \\in \\mathbb{R}^{n}$ is the target vector, $\\mathbf{w} \\in \\mathbb{R}^{d}$ is the parameter vector, and $\\lambda \\geq 0$ is the ridge coefficient. The gradient $\\nabla L(\\mathbf{w})$ is well-defined by basic multivariable calculus. Let the initial parameter be $\\mathbf{w}_{0} = \\mathbf{0} \\in \\mathbb{R}^{2}$ and suppose the following empirical quantities are known at $\\mathbf{w}_{0}$:\n- The feature covariance matrix is $$S = \\frac{1}{n}X^{\\top}X = \\begin{pmatrix} 3 & 2 \\\\ 2 & 3 \\end{pmatrix},$$ which is positive definite.\n- The data–gradient term is $$\\frac{1}{n}X^{\\top}\\mathbf{y} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}.$$\n\nDefine the Adaptive Gradient (Adagrad) method with per-coordinate accumulators initialized at zero and no numerical stabilizer, that is, $s_{0} = \\mathbf{0}$ and $\\epsilon = 0$. A fixed learning rate $\\eta > 0$ is used throughout. Also consider performing standard gradient descent on perfectly whitened features, meaning a linear transformation of the features yields identity covariance; carry out updates in the whitened coordinates and then map them back to the original parameter space.\n\nUsing only fundamental definitions and first principles:\n1. Derive the first-step Adagrad update direction in the original parameter space starting from $\\mathbf{w}_{0}$.\n2. Derive the first-step update direction in the original parameter space that results from one step of gradient descent executed in the whitened feature space starting from the corresponding origin.\n3. Compute the cosine of the angle between these two update directions.\n\nProvide the final answer as the exact value of the cosine. Do not round. No units are required. Your work must begin from the given base definitions (loss, gradient, covariance, whitening) and proceed by explicit derivation; do not invoke any shortcut formulas.",
            "solution": "The problem asks for the cosine of the angle between the first-step update directions of two different optimization algorithms, Adagrad and gradient descent in a whitened feature space, for a ridge regression problem. The process begins with validating the problem statement.\n\n### Problem Validation\n**Step 1: Extract Givens**\n- Loss Function: $L(\\mathbf{w}) = \\frac{1}{2n}\\|X\\mathbf{w} - \\mathbf{y}\\|^{2} + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2}$\n- Initial Parameter: $\\mathbf{w}_{0} = \\mathbf{0} \\in \\mathbb{R}^{2}$\n- Feature Covariance Matrix: $S = \\frac{1}{n}X^{\\top}X = \\begin{pmatrix} 3 & 2 \\\\ 2 & 3 \\end{pmatrix}$\n- Data–Gradient Term: $\\frac{1}{n}X^{\\top}\\mathbf{y} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}$\n- Adagrad Parameters: $s_{0} = \\mathbf{0}$, $\\epsilon = 0$, learning rate $\\eta > 0$.\n- Whitened Space: Gradient descent is performed in a space where features are transformed to have an identity covariance matrix.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, using standard definitions from optimization and machine learning. It is well-posed, providing all necessary quantities for a unique solution. The language is objective and precise. The data is consistent. Therefore, the problem is deemed valid.\n\n### Derivation of the Solution\nFirst, we find the gradient of the loss function $L(\\mathbf{w})$.\n$L(\\mathbf{w}) = \\frac{1}{2n}(X\\mathbf{w} - \\mathbf{y})^{\\top}(X\\mathbf{w} - \\mathbf{y}) + \\frac{\\lambda}{2}\\mathbf{w}^{\\top}\\mathbf{w}$\n$L(\\mathbf{w}) = \\frac{1}{2n}(\\mathbf{w}^{\\top}X^{\\top}X\\mathbf{w} - 2\\mathbf{y}^{\\top}X\\mathbf{w} + \\mathbf{y}^{\\top}\\mathbf{y}) + \\frac{\\lambda}{2}\\mathbf{w}^{\\top}\\mathbf{w}$\nThe gradient with respect to $\\mathbf{w}$ is:\n$\\nabla L(\\mathbf{w}) = \\frac{1}{n}X^{\\top}X\\mathbf{w} - \\frac{1}{n}X^{\\top}\\mathbf{y} + \\lambda\\mathbf{w}$\nUsing the provided definitions, we can write this as:\n$\\nabla L(\\mathbf{w}) = S\\mathbf{w} - \\frac{1}{n}X^{\\top}\\mathbf{y} + \\lambda\\mathbf{w}$\n\nWe need to evaluate the gradient at the initial parameter $\\mathbf{w}_{0} = \\mathbf{0}$:\n$\\mathbf{g}_{0} = \\nabla L(\\mathbf{w}_{0}) = S(\\mathbf{0}) - \\frac{1}{n}X^{\\top}\\mathbf{y} + \\lambda(\\mathbf{0}) = - \\frac{1}{n}X^{\\top}\\mathbf{y}$\nSubstituting the given value:\n$\\mathbf{g}_{0} = - \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$\nNotice that the gradient at the origin, $\\mathbf{g}_{0}$, is independent of the regularization parameter $\\lambda$.\n\n**1. First-step Adagrad update direction**\nThe Adagrad update rule is given by:\n$\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_{t+1}} + \\epsilon} \\odot \\mathbf{g}_{t}$\nwhere $\\mathbf{s}_{t+1} = \\mathbf{s}_{t} + \\mathbf{g}_{t} \\odot \\mathbf{g}_{t}$ (element-wise product).\nFor the first step ($t=0$ to $t=1$), we have $\\mathbf{w}_{0} = \\mathbf{0}$, $\\mathbf{s}_{0} = \\mathbf{0}$, and $\\epsilon = 0$. The gradient is $\\mathbf{g}_{0}$.\n\nFirst, we update the accumulator $\\mathbf{s}$:\n$\\mathbf{s}_{1} = \\mathbf{s}_{0} + \\mathbf{g}_{0} \\odot \\mathbf{g}_{0} = \\mathbf{0} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2^2 \\\\ 1^2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}$\nNext, we compute the update to the parameters. The update vector is $\\Delta\\mathbf{w}_{\\text{Adagrad}} = \\mathbf{w}_{1} - \\mathbf{w}_{0}$.\n$\\mathbf{w}_{1} = \\mathbf{w}_{0} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_{1}} + \\epsilon} \\odot \\mathbf{g}_{0} = \\mathbf{0} - \\frac{\\eta}{\\sqrt{\\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}} + 0} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$\n$\\Delta\\mathbf{w}_{\\text{Adagrad}} = -\\eta \\begin{pmatrix} 1/\\sqrt{4} \\\\ 1/\\sqrt{1} \\end{pmatrix} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -\\eta \\begin{pmatrix} 1/2 \\\\ 1 \\end{pmatrix} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -\\eta \\begin{pmatrix} (1/2) \\cdot 2 \\\\ 1 \\cdot 1 \\end{pmatrix} = -\\eta \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\nThe update direction is any vector proportional to $\\Delta\\mathbf{w}_{\\text{Adagrad}}$. We can choose the direction vector $\\mathbf{d}_{\\text{Adagrad}} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$.\n\n**2. First-step whitened gradient descent update direction**\nWhitening the features means finding a transformation matrix $W$ such that the new features $\\tilde{X} = XW$ have an identity covariance matrix. The new covariance is $\\tilde{S} = \\frac{1}{n}\\tilde{X}^{\\top}\\tilde{X} = W^{\\top}(\\frac{1}{n}X^{\\top}X)W = W^{\\top}SW$. We require $\\tilde{S} = I$. A standard choice for the whitening matrix is $W = S^{-1/2}$, where $S^{-1/2}$ is the principal square root of the inverse of $S$. Since $S$ is symmetric and positive definite, $S^{-1/2}$ is also symmetric and positive definite. With this choice, $W^{\\top}SW = (S^{-1/2})^{\\top} S S^{-1/2} = S^{-1/2} S S^{-1/2} = I$.\n\nThe parameters must be transformed accordingly to preserve predictions: $X\\mathbf{w} = (XW)\\tilde{\\mathbf{w}} = \\tilde{X}\\tilde{\\mathbf{w}}$, which implies $\\mathbf{w} = W\\tilde{\\mathbf{w}}$. The initial parameter $\\mathbf{w}_0 = \\mathbf{0}$ maps to $\\tilde{\\mathbf{w}}_0 = W^{-1}\\mathbf{0} = \\mathbf{0}$.\n\nWe perform gradient descent on the loss function expressed in terms of $\\tilde{\\mathbf{w}}$:\n$\\tilde{L}(\\tilde{\\mathbf{w}}) = L(W\\tilde{\\mathbf{w}}) = \\frac{1}{2n}\\|\\tilde{X}\\tilde{\\mathbf{w}} - \\mathbf{y}\\|^{2} + \\frac{\\lambda}{2}\\|W\\tilde{\\mathbf{w}}\\|^{2}$\nThe gradient is $\\nabla_{\\tilde{\\mathbf{w}}}\\tilde{L}(\\tilde{\\mathbf{w}}) = \\frac{1}{n}\\tilde{X}^{\\top}(\\tilde{X}\\tilde{\\mathbf{w}}-\\mathbf{y}) + \\lambda W^{\\top}W\\tilde{\\mathbf{w}}$.\nSubstituting $\\frac{1}{n}\\tilde{X}^{\\top}\\tilde{X} = I$ and $W^{\\top}W = (S^{-1/2})^{\\top}S^{-1/2} = S^{-1}$:\n$\\nabla_{\\tilde{\\mathbf{w}}}\\tilde{L}(\\tilde{\\mathbf{w}}) = \\tilde{\\mathbf{w}} - \\frac{1}{n}\\tilde{X}^{\\top}\\mathbf{y} + \\lambda S^{-1}\\tilde{\\mathbf{w}} = (I + \\lambda S^{-1})\\tilde{\\mathbf{w}} - W^{\\top}(\\frac{1}{n}X^{\\top}\\mathbf{y})$.\nAt $\\tilde{\\mathbf{w}}_0 = \\mathbf{0}$, the gradient is:\n$\\tilde{\\mathbf{g}}_0 = -\\frac{1}{n}\\tilde{X}^{\\top}\\mathbf{y} = -W^{\\top}(\\frac{1}{n}X^{\\top}\\mathbf{y}) = -S^{-1/2}(-\\mathbf{g}_0) = S^{-1/2}\\mathbf{g}_0$.\nThe gradient descent update in the whitened space is $\\tilde{\\mathbf{w}}_1 = \\tilde{\\mathbf{w}}_0 - \\eta' \\tilde{\\mathbf{g}}_0 = -\\eta' \\tilde{\\mathbf{g}}_0$.\n\nWe map this update back to the original parameter space: $\\mathbf{w}_1 = W\\tilde{\\mathbf{w}}_1$.\nThe update vector is $\\Delta\\mathbf{w}_{\\text{white}} = \\mathbf{w}_1 - \\mathbf{w}_0 = W\\tilde{\\mathbf{w}}_1 - \\mathbf{0} = S^{-1/2}(-\\eta' \\tilde{\\mathbf{g}}_0) = -\\eta' S^{-1/2} (S^{-1/2}\\mathbf{g}_0) = -\\eta' S^{-1}\\mathbf{g}_0$.\nThe update direction is proportional to $-S^{-1}\\mathbf{g}_0$. Let's compute this vector.\nWe have $S = \\begin{pmatrix} 3 & 2 \\\\ 2 & 3 \\end{pmatrix}$ and $\\mathbf{g}_{0} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\nFirst, we find the inverse of $S$:\n$\\det(S) = 3 \\times 3 - 2 \\times 2 = 9 - 4 = 5$.\n$S^{-1} = \\frac{1}{\\det(S)}\\begin{pmatrix} 3 & -2 \\\\ -2 & 3 \\end{pmatrix} = \\frac{1}{5}\\begin{pmatrix} 3 & -2 \\\\ -2 & 3 \\end{pmatrix}$.\nNow we compute the direction vector:\n$-S^{-1}\\mathbf{g}_0 = -\\frac{1}{5}\\begin{pmatrix} 3 & -2 \\\\ -2 & 3 \\end{pmatrix}\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -\\frac{1}{5}\\begin{pmatrix} 3(2) - 2(1) \\\\ -2(2) + 3(1) \\end{pmatrix} = -\\frac{1}{5}\\begin{pmatrix} 6-2 \\\\ -4+3 \\end{pmatrix} = -\\frac{1}{5}\\begin{pmatrix} 4 \\\\ -1 \\end{pmatrix} = \\frac{1}{5}\\begin{pmatrix} -4 \\\\ 1 \\end{pmatrix}$.\nThe update direction is specified by any vector proportional to this. We can choose $\\mathbf{d}_{\\text{white}} = \\begin{pmatrix} -4 \\\\ 1 \\end{pmatrix}$.\n\n**3. Cosine of the angle between the two update directions**\nWe need to find the cosine of the angle $\\theta$ between $\\mathbf{d}_{\\text{Adagrad}} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$ and $\\mathbf{d}_{\\text{white}} = \\begin{pmatrix} -4 \\\\ 1 \\end{pmatrix}$.\nThe cosine is given by the formula $\\cos(\\theta) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}$.\nThe dot product is:\n$\\mathbf{d}_{\\text{Adagrad}} \\cdot \\mathbf{d}_{\\text{white}} = (-1)(-4) + (-1)(1) = 4 - 1 = 3$.\nThe magnitudes (norms) of the vectors are:\n$\\|\\mathbf{d}_{\\text{Adagrad}}\\| = \\sqrt{(-1)^2 + (-1)^2} = \\sqrt{1+1} = \\sqrt{2}$.\n$\\|\\mathbf{d}_{\\text{white}}\\| = \\sqrt{(-4)^2 + 1^2} = \\sqrt{16+1} = \\sqrt{17}$.\nThe cosine of the angle is:\n$\\cos(\\theta) = \\frac{3}{\\sqrt{2} \\sqrt{17}} = \\frac{3}{\\sqrt{34}}$.\nTo rationalize the denominator, this can be written as $\\frac{3\\sqrt{34}}{34}$, but the problem does not require this. We leave it in the simpler exact form.",
            "answer": "$$\\boxed{\\frac{3}{\\sqrt{34}}}$$"
        },
        {
            "introduction": "No algorithm is without limitations, and understanding them is key to becoming an expert practitioner. Adagrad's main weakness stems from its gradient accumulator, which grows indefinitely, causing the learning rate to monotonically decrease and eventually become infinitesimally small. This hands-on simulation explores the consequences in a non-stationary setting, where an algorithm must adapt to a changing data distribution. By implementing and comparing Adagrad to a variant with a decaying accumulator, you will discover the core idea that motivates more advanced optimizers like RMSprop and Adam.",
            "id": "3095442",
            "problem": "You will construct and analyze a streaming optimization scenario in which a feature becomes relevant after a change point, and compare two adaptive step-size strategies. Begin from the following fundamental base: the linear predictor $f_{\\boldsymbol{w}}(\\boldsymbol{x}) = \\boldsymbol{w}^{\\top}\\boldsymbol{x}$ trained by stochastic gradient descent on the per-step squared loss $\\ell_t(\\boldsymbol{w}) = \\tfrac{1}{2}\\left(y_t - \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t\\right)^2$, whose gradient is $\\nabla \\ell_t(\\boldsymbol{w}) = -\\left(y_t - \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t\\right)\\boldsymbol{x}_t$. The task is to implement two optimizers that modify the per-coordinate learning rate based on past gradients: one that accumulates all squared gradients and one that exponentially decays the accumulator to weight recent gradients more heavily.\n\nConstruct a nonstationary dataset with $2$ features where the second feature becomes relevant only after a change time $t^*$. Use a streaming horizon of $T$ steps, constant features $\\boldsymbol{x}_t = [1, 1]$, and true weights $\\boldsymbol{w}^*_t = [1, w^*_2(t)]$ with $w^*_2(t) = 0$ for $t < t^*$ and $w^*_2(t) = 1$ for $t \\ge t^*$. There is no observation noise, i.e., $y_t = \\boldsymbol{w}^{*}_t{}^{\\top}\\boldsymbol{x}_t$. Initialize the model parameters at $\\boldsymbol{w}_0 = [0,0]$.\n\nImplement two update schemes applied separately to identical streams:\n1. A baseline accumulator that sums squared gradients in each coordinate. At each time $t$, let the accumulator be $\\boldsymbol{G}_t$ and the gradient be $\\boldsymbol{g}_t = \\nabla \\ell_t(\\boldsymbol{w}_t)$. Update $\\boldsymbol{G}_t$ by adding the elementwise square of $\\boldsymbol{g}_t$, and update $\\boldsymbol{w}_t$ by subtracting the elementwise product of the gradient with a per-coordinate step size inversely proportional to the square root of the accumulator and stabilized by a small constant $\\,\\epsilon\\,$.\n2. A decayed accumulator that forms an exponential moving average of squared gradients. At each time $t$, let the accumulator be $\\boldsymbol{H}_t$ and update $\\boldsymbol{H}_t$ as an exponential moving average of $\\boldsymbol{g}_t^2$ with decay parameter $\\alpha \\in (0,1)$, then update $\\boldsymbol{w}_t$ with a per-coordinate step size inversely proportional to $\\sqrt{\\boldsymbol{H}_t + \\epsilon}$.\n\nDefine the adaptation lag for the second coordinate under an optimizer as the smallest nonnegative integer $s$ such that $\\left|w_{t^*+s,\\,2} - 1\\right| \\le \\tau$ after the change time $t^*$, assessed immediately after each update. If the threshold is never met within the available steps, return $T - t^*$ as the lag. Use the same base learning rate $\\eta$ and stabilization constant $\\epsilon$ for both optimizers.\n\nYour program must implement this simulation and compute, for each test case, a pair of integers $[L_{\\text{sum}}, L_{\\text{decay}}]$ giving the adaptation lags for the baseline summed accumulator and the decayed accumulator, respectively.\n\nTest Suite:\n- Case $1$: $T=200$, $t^*=50$, $\\eta=0.2$, $\\epsilon=10^{-8}$, $\\alpha=0.9$, $\\tau=0.1$.\n- Case $2$: $T=200$, $t^*=0$, $\\eta=0.2$, $\\epsilon=10^{-8}$, $\\alpha=0.9$, $\\tau=0.1$.\n- Case $3$: $T=200$, $t^*=199$, $\\eta=0.2$, $\\epsilon=10^{-8}$, $\\alpha=0.9$, $\\tau=0.1$.\n- Case $4$: $T=200$, $t^*=50$, $\\eta=0.2$, $\\epsilon=10^{-8}$, $\\alpha=0.99$, $\\tau=0.1$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the pair for one test case. For example, the exact format is $\\left[ [L_{\\text{sum},1},L_{\\text{decay},1}], [L_{\\text{sum},2},L_{\\text{decay},2}], [L_{\\text{sum},3},L_{\\text{decay},3}], [L_{\\text{sum},4},L_{\\text{decay},4}] \\right]$. No physical units are involved, and all angles (none present) are assumed irrelevant. All outputs must be integers.",
            "solution": "The problem requires a comparative analysis of two adaptive learning rate optimization algorithms within a simulated nonstationary online learning environment. The core task is to implement and evaluate an Adagrad-style optimizer, which accumulates all past squared gradients, against an RMSprop-style optimizer, which uses an exponential moving average of squared gradients. The comparison is based on the \"adaptation lag,\" a measure of how quickly each optimizer adjusts a model parameter to its new true value after an abrupt change in the underlying data-generating process.\n\nFirst, we formalize the components of the simulation as specified.\n\n**1. Model and Learning Task**\nThe predictive model is a linear function of the input features $\\boldsymbol{x}_t \\in \\mathbb{R}^2$:\n$$f_{\\boldsymbol{w}}(\\boldsymbol{x}_t) = \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t$$\nwhere $\\boldsymbol{w} \\in \\mathbb{R}^2$ is the vector of model parameters. The learning process is driven by minimizing the squared error loss at each time step $t$:\n$$\\ell_t(\\boldsymbol{w}) = \\frac{1}{2}\\left(y_t - \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t\\right)^2$$\nThe gradient of the loss with respect to the parameters $\\boldsymbol{w}$, denoted by $\\boldsymbol{g}_t$, is essential for the optimization updates:\n$$\\boldsymbol{g}_t = \\nabla_{\\boldsymbol{w}} \\ell_t(\\boldsymbol{w}) = -\\left(y_t - \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t\\right)\\boldsymbol{x}_t$$\n\n**2. Nonstationary Data Generation**\nThe simulation runs for a total of $T$ time steps. The environment is characterized by a \"change point\" at time $t^*$.\n- The feature vector is constant: $\\boldsymbol{x}_t = [1, 1]^{\\top}$ for all $t=0, \\dots, T-1$.\n- The true parameter vector $\\boldsymbol{w}^*_t$ is nonstationary:\n  $$ \\boldsymbol{w}^*_t = [1, w^*_2(t)]^{\\top} \\quad \\text{where} \\quad w^*_2(t) = \\begin{cases} 0 & \\text{if } t < t^* \\\\ 1 & \\text{if } t \\ge t^* \\end{cases} $$\n- The target values $y_t$ are generated without noise by the true model: $y_t = (\\boldsymbol{w}^*_t)^{\\top}\\boldsymbol{x}_t$. This results in:\n  $$ y_t = \\begin{cases} [1, 0]^{\\top}[1, 1] = 1 & \\text{if } t < t^* \\\\ [1, 1]^{\\top}[1, 1] = 2 & \\text{if } t \\ge t^* \\end{cases} $$\n- The initial model parameters are set to zero: $\\boldsymbol{w}_0 = [0, 0]^{\\top}$.\n\n**3. Optimizer Implementations**\nTwo separate optimizers are implemented, each starting from the same initial state $\\boldsymbol{w}_0$ and receiving the same data stream $(\\boldsymbol{x}_t, y_t)$. For $t = 0, \\dots, T-1$:\n\n**Optimizer 1: Summed Squared Gradients (Adagrad-style)**\nThis optimizer maintains an accumulator $\\boldsymbol{G}_t$ that sums the element-wise squares of all gradients seen up to time $t$.\n- **Initialization**: $\\boldsymbol{G}_{-1} = \\boldsymbol{0}$.\n- **Accumulator Update**: $\\boldsymbol{G}_t = \\boldsymbol{G}_{t-1} + \\boldsymbol{g}_t^2$, where the square is element-wise.\n- **Parameter Update**: $\\boldsymbol{w}_{t+1} = \\boldsymbol{w}_t - \\frac{\\eta}{\\sqrt{\\boldsymbol{G}_t + \\epsilon}} \\odot \\boldsymbol{g}_t$, where $\\eta$ is the base learning rate, $\\epsilon$ is a small stabilization constant, $\\odot$ denotes element-wise multiplication, and the division and square root are also element-wise.\n\nThe principle of this method is to reduce the learning rate for parameters that have received large or frequent gradient updates. The per-parameter learning rate, $\\eta_j = \\frac{\\eta}{\\sqrt{G_{t,j} + \\epsilon}}$, is monotonically decreasing. While this is beneficial for converging on convex problems, it poses a challenge in nonstationary environments. A large accumulated value in $G_{t,j}$ from before a change point can severely slow down adaptation to a new target value.\n\n**Optimizer 2: Decayed Squared Gradients (RMSprop-style)**\nThis optimizer uses an exponential moving average (EMA) for the accumulator, which allows it to \"forget\" old gradients and adapt to recent gradient statistics.\n- **Initialization**: $\\boldsymbol{H}_{-1} = \\boldsymbol{0}$.\n- **Accumulator Update**: $\\boldsymbol{H}_t = \\alpha \\boldsymbol{H}_{t-1} + (1-\\alpha) \\boldsymbol{g}_t^2$, where $\\alpha \\in (0,1)$ is the decay parameter. This update is the standard formulation for an EMA.\n- **Parameter Update**: $\\boldsymbol{w}_{t+1} = \\boldsymbol{w}_t - \\frac{\\eta}{\\sqrt{\\boldsymbol{H}_t + \\epsilon}} \\odot \\boldsymbol{g}_t$.\n\nThe EMA-based accumulator ensures that the per-parameter learning rate can increase if recent gradients have been small, and decrease if they have been large. This adaptivity is hypothesized to be superior in the given nonstationary setting, as the influence of gradients from the pre-change-point era ($t < t^*$) will diminish over time.\n\n**4. Adaptation Lag Calculation**\nThe metric for comparison is the adaptation lag, $L$. It quantifies the number of steps required for the second parameter, $w_2$, to converge to its new target value of $1$ after the change point $t^*$.\nWhile the problem statement uses the notation $\\left|w_{t^*+s,\\,2} - 1\\right| \\le \\tau$, a literal interpretation is problematic as $\\boldsymbol{w}_{t^*}$ is computed before the change. We adopt a more direct, procedural interpretation consistent with the framing of the problem. We seek the smallest non-negative integer $s$ representing the number of update steps *after* the change point at $t^*$ required for convergence.\n- The simulation loop runs for $t = 0, \\dots, T-1$.\n- The check for convergence occurs for updates at steps $t = t^*, t^*+1, \\dots, T-1$.\n- For each such step $t$, we compute the updated weight vector $\\boldsymbol{w}_{t+1}$.\n- We then check if $|w_{t+1, 2} - 1| \\le \\tau$.\n- If this condition is met for the first time at step $t$, the adaptation lag is recorded as $s = t - t^*$.\n- If the condition is not met by the final update (at $t=T-1$), the lag is assigned the default value of $T - t^*$, representing a failure to converge within the available time window.\n\nThis simulation will be run for each set of parameters in the test suite, and the resulting pair of lags, $[L_{\\text{sum}}, L_{\\text{decay}}]$, will be determined for each case. The expectation is that $L_{\\text{decay}} < L_{\\text{sum}}$ in cases with a mid-stream change point (e.g., Case 1), demonstrating the superior adaptivity of the decayed accumulator.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes two adaptive optimizers on a nonstationary\n    linear regression problem.\n    \"\"\"\n\n    test_cases = [\n        {'T': 200, 't_star': 50, 'eta': 0.2, 'epsilon': 1e-8, 'alpha': 0.9, 'tau': 0.1},\n        {'T': 200, 't_star': 0, 'eta': 0.2, 'epsilon': 1e-8, 'alpha': 0.9, 'tau': 0.1},\n        {'T': 200, 't_star': 199, 'eta': 0.2, 'epsilon': 1e-8, 'alpha': 0.9, 'tau': 0.1},\n        {'T': 200, 't_star': 50, 'eta': 0.2, 'epsilon': 1e-8, 'alpha': 0.99, 'tau': 0.1},\n    ]\n\n    results = []\n\n    for params in test_cases:\n        T = params['T']\n        t_star = params['t_star']\n        eta = params['eta']\n        epsilon = params['epsilon']\n        alpha = params['alpha']\n        tau = params['tau']\n\n        # --- Initialize parameters for both optimizers ---\n        # Weight vectors\n        w_sum = np.zeros(2)\n        w_decay = np.zeros(2)\n\n        # Accumulators\n        G = np.zeros(2)  # Summed accumulator\n        H = np.zeros(2)  # Decayed accumulator\n\n        # Lag tracking\n        lag_sum = -1\n        lag_decay = -1\n\n        # Constant feature vector\n        x_t = np.ones(2)\n\n        # --- Main simulation loop ---\n        for t in range(T):\n            # Determine true weight and target value y_t\n            if t < t_star:\n                w_star_2 = 0.0\n            else:\n                w_star_2 = 1.0\n            w_star_t = np.array([1.0, w_star_2])\n            y_t = np.dot(w_star_t, x_t)\n\n            # --- Optimizer 1: Summed Accumulator (Adagrad-style) ---\n            pred_sum = np.dot(w_sum, x_t)\n            error_sum = y_t - pred_sum\n            g_t_sum = -error_sum * x_t\n\n            G += g_t_sum**2\n            step_size_sum = eta / (np.sqrt(G) + epsilon)\n            w_sum -= step_size_sum * g_t_sum\n\n            # --- Optimizer 2: Decayed Accumulator (RMSprop-style) ---\n            pred_decay = np.dot(w_decay, x_t)\n            error_decay = y_t - pred_decay\n            g_t_decay = -error_decay * x_t\n\n            H = alpha * H + (1 - alpha) * g_t_decay**2\n            step_size_decay = eta / (np.sqrt(H) + epsilon)\n            w_decay -= step_size_decay * g_t_decay\n\n            # --- Check for adaptation lag after change point ---\n            if t >= t_star:\n                # Check for summed accumulator\n                if lag_sum == -1 and abs(w_sum[1] - 1.0) <= tau:\n                    lag_sum = t - t_star\n\n                # Check for decayed accumulator\n                if lag_decay == -1 and abs(w_decay[1] - 1.0) <= tau:\n                    lag_decay = t - t_star\n\n        # If threshold was never met, set lag to the default value\n        if lag_sum == -1:\n            lag_sum = T - t_star\n        if lag_decay == -1:\n            lag_decay = T - t_star\n            \n        results.append([lag_sum, lag_decay])\n\n    # Format the final output string\n    result_str = '[' + ', '.join(f'[{res[0]}, {res[1]}]' for res in results) + ']'\n    print(result_str.replace(\" \", \"\"))\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}