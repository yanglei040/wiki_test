{
    "hands_on_practices": [
        {
            "introduction": "深度学习中的优化挑战核心在于非凸的损失函数 landscape，其中充满了次优的局部最小值。本练习使用一个简单的一维非凸函数，直观地展示了带热重启的余弦退火（SGDR）如何通过周期性地提高学习率，帮助优化器“跳出”这些浅层陷阱 。通过与单调递减的学习率进行对比，你将建立起对循环学习率有效性的基本直觉。",
            "id": "3110220",
            "problem": "给定一个由四次多项式 $f(\\theta)=\\theta^4-2a\\theta^2+b\\theta$ 定义的一维合成非凸损失函数，其中 $a$ 和 $b$ 为实数参数。考虑确定性梯度下降法，其参数更新遵循规则 $\\theta_{t+1}=\\theta_t-\\eta_t\\nabla f(\\theta_t)$，其中第 $t$ 步的标量学习率 $\\eta_t0$，梯度为 $\\nabla f(\\theta)=\\frac{d}{d\\theta}f(\\theta)$。目的是从第一性原理出发，研究学习率策略的选择如何影响梯度下降在一个可能同时存在浅层和深层局部极小值的非凸曲面上取得进展的能力。\n\n你的任务：\n\n1. 从损失函数的定义和基础微积分出发，推导出确定性梯度下降所需的解析梯度 $\\nabla f(\\theta)$。\n\n2. 从一个长度为 $T$ 的周期内的学习率序列 $\\{\\eta_t\\}_{t=0}^{T-1}$ 的定义出发，并考虑以下约束：函数是平滑的，在边界处斜率为零（以避免突变），在每个周期开始时达到最大值 $\\eta_{\\max}$，在每个周期结束时达到最小值 $\\eta_{\\min}$。推导一个满足这些约束的单周期学习率策略，然后定义一个每 $T$ 步重复此策略的热重启机制。任何三角函数的角度都必须以弧度表示。\n\n3. 定义一个与之对比的单调学习率策略 $\\eta_t$，该策略根据一个经过充分检验的规则随时间严格递减。使用此策略执行不带重启的确定性梯度下降。\n\n4. 实现这两种策略，并运行确定性梯度下降固定的步数（epochs）。对于下面套件中的每个测试用例，计算在热重启策略和单调策略下获得的最终损失。对于每种情况，返回一个布尔值：当且仅当热重启下的最终损失比单调衰减下的最终损失严格小至少一个裕量 $\\varepsilon$ (使用 $\\varepsilon=10^{-2}$) 时，该值为真，否则为假。此布尔值旨在可操作化地表述“与单调衰减相比，热重启能够逃离浅层局部极小值”这一论断，即在相同步数后实现一个严格更低的目标值。\n\n使用以下测试套件。每个测试用例是一个元组，指定了 $(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)$，其中 $a$ 和 $b$ 是损失函数 $f(\\theta)$ 的参数，$\\theta_0$ 是初始参数值， $E$ 是梯度下降的步数（epochs），$(\\eta_{\\max},\\eta_{\\min},T)$ 指定了热重启策略的最大值、最小值和周期，$(\\eta_0,\\gamma)$ 指定了单调策略的初始学习率和乘法衰减因子：\n\n- 情况1（一般非凸，热重启有帮助空间）：$(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(1.0,0.01,1.30,120,0.20,0.0005,30,0.005,0.90)$。\n- 情况2（类边界：一个长周期近似于单调）：$(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(1.0,0.01,1.30,120,0.050,0.049,120,0.050,0.99)$。\n- 情况3（具有强线性项的近似凸情形）：$(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(0.25,1.20,0.00,100,0.10,0.001,25,0.10,0.97)$。\n- 情况4（频繁重启）：$(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(1.0,0.05,1.30,100,0.20,0.0005,10,0.010,0.95)$。\n\n角度单位要求：任何三角函数的使用都必须使用弧度。\n\n输出规范：你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表（例如，\"[result1,result2,result3,result4]\"），每个结果是对应于上面列出顺序的测试用例的布尔值。",
            "solution": "我们从合成的非凸损失函数 $f(\\theta)=\\theta^4-2a\\theta^2+b\\theta$ 开始。根据导数的定义和基础微积分，通过逐项微分可得到解析梯度：\n$$\n\\nabla f(\\theta)=\\frac{d}{d\\theta}\\left(\\theta^4-2a\\theta^2+b\\theta\\right)=4\\theta^3-4a\\theta+b.\n$$\n确定性梯度下降使用以下规则更新参数：\n$$\n\\theta_{t+1}=\\theta_t-\\eta_t\\nabla f(\\theta_t)=\\theta_t-\\eta_t\\left(4\\theta_t^3-4a\\theta_t+b\\right),\n$$\n其中 $\\eta_t$ 是第 $t$ 次迭代的学习率。此规则直接源于最速下降原理：沿着由一个正步长缩放的负梯度方向移动。\n\n接下来，我们推导一个长度为 $T$ 的周期内的热重启学习率策略，该策略满足平滑性和边界约束。我们需要一个函数 $g:[0,T]\\to\\mathbb{R}$ 使得：\n1. $g(0)=\\eta_{\\max}$，\n2. $g(T)=\\eta_{\\min}$，\n3. $g'(0)=0$ 且 $g'(T)=0$ 以保证边界平滑，\n4. $g$ 在 $(0,T)$ 内足够平滑，\n5. 该策略每 $T$ 步重启一次，因此有效的 $\\eta_t=g(t\\bmod T)$。\n\n一个满足这些约束的最小平滑函数可以通过一个经过平移和缩放的半周期余弦函数来构造。考虑余弦函数，它平滑、是偶函数，并且在其极值点处斜率为零。在区间 $[0,T]$ 上，函数 $\\cos\\left(\\pi t/T\\right)$ 满足 $\\cos(0)=1$, $\\cos(\\pi)=-1$，并且在两端斜率均为零。进行缩放和平移以匹配边界值，得到：\n$$\ng(t)=\\eta_{\\min}+\\frac{1}{2}(\\eta_{\\max}-\\eta_{\\min})\\left(1+\\cos\\left(\\pi\\frac{t}{T}\\right)\\right).\n$$\n这满足了所有陈述的条件，并且角度按要求以弧度为单位。为了引入热重启（在这个确定性设定中即带重启的随机梯度下降 (SGDR)），我们通过设置以下公式来每 $T$ 步重复一次 $g$：\n$$\n\\eta_t=\\eta_{\\min}+\\frac{1}{2}(\\eta_{\\max}-\\eta_{\\min})\\left(1+\\cos\\left(\\pi\\frac{t\\bmod T}{T}\\right)\\right).\n$$\n每次重启都将学习率重置为 $\\eta_{\\max}$，然后在一个周期内平滑地退火至 $\\eta_{\\min}$，通过间歇性地增大学习率来鼓励周期性探索并逃离浅层区域。\n\n作为对比，我们定义一个单调学习率策略，该策略根据一个经过充分检验的指数规则随时间递减：\n$$\n\\eta_t=\\eta_0\\gamma^t,\n$$\n其中 $\\eta_00$ 且 $0\\gamma1$，提供了一个严格递减的序列。这种单调衰减在实践中被广泛使用，它会随时间减小步长，这有助于收敛，但可能会因为步长递减而阻碍逃离浅层局部极小值。\n\n算法设计：\n- 对于每个测试用例，计算在两种方案下的最终损失：带热重启的余弦退火和单调指数衰减。\n- 初始化 $\\theta_0$，并使用各自的策略进行 $E$ 步确定性梯度下降迭代。在每一步 $t$，计算梯度 $\\nabla f(\\theta_t)=4\\theta_t^3-4a\\theta_t+b$ 并更新 $\\theta_{t+1}=\\theta_t-\\eta_t\\nabla f(\\theta_t)$。\n- 在 $E$ 步之后，计算两种策略下的 $f(\\theta_E)$。返回一个布尔值，指示热重启下的最终损失是否比单调衰减下的最终损失严格小至少 $\\varepsilon=10^{-2}$：\n$$\n\\text{result}=\\left(f_{\\text{warm}}+ \\varepsilon  f_{\\text{mono}}\\right).\n$$\n\n基于原理的解释：\n- 在像四次函数 $f$ 这样的非凸曲面中，浅层局部极小值和平台区会导致梯度很小。在单调衰减下，步长持续缩小，这可能因为乘积 $\\eta_t\\lVert\\nabla f(\\theta_t)\\rVert$ 变得过小而无法取得有意义的进展，从而将迭代点困在次优的浅层区域附近。\n- 带热重启的余弦退火会周期性地将学习率增加到 $\\eta_{\\max}$，产生更大的步长，可以将迭代点移出浅层谷底或平台区。因为该策略是平滑的，并且在周期边界处斜率为零，所以它避免了可能破坏训练稳定性的突变，同时在重启时仍能注入足够的探索能力。\n\n测试套件的基本原理：\n- 情况1提供了一个非凸设置，其参数使得浅层区域是可辨别的；预计热重启会优于单调衰减，从而得到真。\n- 情况2使用一个单一的长周期和近乎恒定的学习率，近似于单调行为；重启的优势减弱，通常得到假。\n- 情况3有一个强的线性项，使得损失更接近于凸函数的行为（实际上只有一个主导的盆地），在这种情况下，两种策略的表现往往相似，得到假。\n- 情况4使用频繁的重启来引发大量的周期性探索，这可以相对于单调衰减改善最终损失，可能得到真。\n\n最终输出是单行文本，包含一个按测试用例顺序列出的四个布尔值的列表，格式如上所述。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(theta, a, b):\n    # Quartic nonconvex loss f(theta) = theta^4 - 2 a theta^2 + b theta\n    return theta**4 - 2.0*a*theta**2 + b*theta\n\ndef grad_f(theta, a, b):\n    # Analytical gradient: df/dtheta = 4 theta^3 - 4 a theta + b\n    return 4.0*theta**3 - 4.0*a*theta + b\n\ndef cosine_annealing_lr(t, eta_max, eta_min, T):\n    # Cosine schedule over one period T with warm restarts.\n    # Angle in radians; uses a half-cosine anneal from eta_max to eta_min.\n    # t_mod cycles every T steps.\n    t_mod = t % T\n    return eta_min + 0.5*(eta_max - eta_min)*(1.0 + np.cos(np.pi * (t_mod / T)))\n\ndef monotone_exp_lr(t, eta0, gamma):\n    # Monotone exponential decay: eta_t = eta0 * gamma^t\n    return eta0 * (gamma ** t)\n\ndef run_descent(a, b, theta0, E, lr_schedule_fn, lr_params):\n    theta = theta0\n    for t in range(E):\n        eta_t = lr_schedule_fn(t, *lr_params)\n        g = grad_f(theta, a, b)\n        theta = theta - eta_t * g\n    return f(theta, a, b), theta\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (a, b, theta0, E, eta_max, eta_min, T, eta0, gamma)\n    test_cases = [\n        (1.0, 0.01, 1.30, 120, 0.20, 0.0005, 30, 0.005, 0.90),   # Case 1\n        (1.0, 0.01, 1.30, 120, 0.050, 0.049, 120, 0.050, 0.99),  # Case 2\n        (0.25, 1.20, 0.00, 100, 0.10, 0.001, 25, 0.10, 0.97),    # Case 3\n        (1.0, 0.05, 1.30, 100, 0.20, 0.0005, 10, 0.010, 0.95),   # Case 4\n    ]\n\n    epsilon = 1e-2  # Margin for declaring warm restarts strictly better\n\n    results = []\n    for case in test_cases:\n        a, b, theta0, E, eta_max, eta_min, T, eta0, gamma = case\n\n        # Warm restarts via cosine annealing with restarts\n        warm_loss, warm_theta = run_descent(\n            a, b, theta0, E,\n            lr_schedule_fn=cosine_annealing_lr,\n            lr_params=(eta_max, eta_min, T)\n        )\n\n        # Monotone exponential decay\n        mono_loss, mono_theta = run_descent(\n            a, b, theta0, E,\n            lr_schedule_fn=monotone_exp_lr,\n            lr_params=(eta0, gamma)\n        )\n\n        # Boolean result: True if warm_loss + epsilon  mono_loss\n        result = (warm_loss + epsilon)  mono_loss\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在理想化的确定性梯度下降之后，我们进入更贴近现实的随机梯度下降（SGD）世界。本练习将探讨循环学习率与梯度噪声以及一种常见的稳定化技术——梯度裁剪——之间的相互作用 。通过在一个简单的二次函数 $f(x) = \\frac{1}{2} x^2$ 上模拟 SGD，你将比较不同形状（平滑的余弦与尖锐的三角波）的周期对收敛速度的影响，从而深入理解它们在实际应用中的表现。",
            "id": "3110142",
            "problem": "您需要编写一个完整、可运行的程序，该程序模拟在一维凸二次损失函数上使用周期性学习率和梯度噪声裁剪的随机梯度下降。您的目标是量化梯度噪声的裁剪如何与高学习率阶段相互作用，以及在不同周期性学习率方案下的收敛速度。所有角度都必须以弧度为单位处理。\n\n基本原理与设置：\n- 考虑凸二次目标函数 $f(x) = \\tfrac{1}{2} x^2$。其梯度为 $\\nabla f(x) = x$。\n- 在离散迭代 $t \\in \\{1,2,\\dots\\}$ 时，定义一个形式为 $g_t = \\nabla f(x_{t-1}) + \\xi_t$ 的随机梯度，其中 $\\xi_t$ 是方差为 $\\sigma^2$ 的附加零均值高斯噪声，即 $\\xi_t \\sim \\mathcal{N}(0,\\sigma^2)$。\n- 引入梯度噪声裁剪如下：噪声项在一维上被 $c \\ge 0$ 裁剪，因此有效噪声为 $\\tilde{\\xi}_t = \\mathrm{clip}(\\xi_t, -c, c)$。等价地，$\\tilde{\\xi}_t = \\max(-c, \\min(c, \\xi_t))$。随机梯度变为 $g_t = x_{t-1} + \\tilde{\\xi}_t$。不裁剪确定性梯度，仅裁剪噪声项。不进行裁剪的约定是 $c = +\\infty$。\n- 更新规则为 $x_t = x_{t-1} - \\eta_t g_t$，其中 $\\eta_t$ 是一个依赖于 $t$ 的周期性学习率。\n\n周期性学习率方案：\n- 您必须实现两种周期为 $P \\in \\mathbb{N}$ 的带重启的周期性方案：\n  1) 三角形重启方案：在每个包含 $P$ 次迭代的周期内，学习率从周期开始时的最大值 $\\eta_{\\max}$ 线性下降到周期结束时的最小值 $\\eta_{\\min}$，然后在下一个周期开始时瞬时重启至 $\\eta_{\\max}$。这是一个分段线性方案；不要为其使用任何三角函数。\n  2) 平滑重启方案：在每个包含 $P$ 次迭代的周期内，学习率从周期开始时的 $\\eta_{\\max}$ 平滑下降到周期结束时的 $\\eta_{\\min}$，且在两个端点的斜率均为零。推导一个在单位区间上满足这些端点和对称属性的合适的平滑周期映射，并使用一个角度以弧度为单位的余弦函数来表示它。不要使用任何度数单位；所有三角函数参数都必须是弧度。\n- 两种方案都是带重启的周期性方案，在每个长度为 $P$ 的周期内，它们在重启前都恰好从 $\\eta_{\\max}$ 遍历到 $\\eta_{\\min}$ 一次。\n\n收敛速度的度量：\n- 对于给定的参数集，定义损失的收敛阈值为 $\\tau  0$。设首次命中时间为满足 $f(x_t) \\le \\tau$ 的最小迭代索引 $t \\in \\{1,2,\\dots\\}$。如果在指定的迭代预算 $T_{\\max}$ 内从未发生，则将首次命中时间定义为 $T_{\\max}$。\n- 由于 $g_t$ 是随机的，通过运行 $R$ 次独立重复实验并取其首次命中时间的中位数来估计一个鲁棒的代表性收敛速度。每次重复实验必须由一个独立的随机种子驱动，该种子从一个基础种子和重复实验索引确定性地派生出来，以确保结果是可复现的。将中位数四舍五入到最近的整数，以报告整数次迭代。\n\n模拟协议：\n- 确定性地初始化 $x_0 = x_{\\text{init}}$。\n- 对于每次重复实验 $r \\in \\{0,1,\\dots,R-1\\}$ 和每次迭代 $t \\in \\{1,2,\\dots,T_{\\max}\\}$：\n  1) 使用周期 $P$、$\\eta_{\\min}$ 和 $\\eta_{\\max}$ 从所选的周期性方案中计算学习率 $\\eta_t$。\n  2) 使用一个伪随机数生成器抽取噪声 $\\xi_t \\sim \\mathcal{N}(0,\\sigma^2)$，该生成器的种子由指定的基础种子、案例索引和 $r$ 确定性地设置。将其裁剪为 $\\tilde{\\xi}_t = \\mathrm{clip}(\\xi_t, -c, c)$，约定 $c = +\\infty$ 表示禁用裁剪。\n  3) 形成随机梯度 $g_t = x_{t-1} + \\tilde{\\xi}_t$ 并更新 $x_t = x_{t-1} - \\eta_t g_t$。\n  4) 更新后，检查是否有 $f(x_t) \\le \\tau$；如果有，记录当前迭代次数 $t$ 并停止该次重复实验。如果在 $T_{\\max}$ 之前没有出现这样的 $t$，则记录 $T_{\\max}$。\n\n角度单位要求：\n- 所有余弦函数的使用必须将其参数解释为弧度。\n\n测试套件：\n使用以下四个参数案例，每个案例需要两次度量：一次使用基于余弦的平滑重启方案，另一次使用三角形重启方案。在所有案例中，使用 $R = 200$ 次重复实验，基础种子 $s_{\\text{base}} = 12345$ 和 $x_{\\text{init}} = 5.0$。\n- 案例 A (带中等裁剪的基线): $\\eta_{\\min} = 0.05$, $\\eta_{\\max} = 0.8$, $P = 40$, $\\sigma = 0.5$, $c = 0.2$, $\\tau = 0.01$, $T_{\\max} = 1000$。\n- 案例 B (裁剪消除噪声): $\\eta_{\\min} = 0.05$, $\\eta_{\\max} = 0.8$, $P = 40$, $\\sigma = 0.5$, $c = 0$, $\\tau = 0.01$, $T_{\\max} = 1000$。\n- 案例 C (无裁剪): $\\eta_{\\min} = 0.05$, $\\eta_{\\max} = 0.8$, $P = 40$, $\\sigma = 0.5$, $c = +\\infty$, $\\tau = 0.01$, $T_{\\max} = 1000$。\n- 案例 D (高最大学习率): $\\eta_{\\min} = 0.1$, $\\eta_{\\max} = 1.8$, $P = 40$, $\\sigma = 0.5$, $c = 0.2$, $\\tau = 0.01$, $T_{\\max} = 1000$。\n\n要求的最终输出格式：\n- 您的程序应生成一行输出，其中包含一个扁平列表中的八个整数结果，顺序为 $[$A-余弦, A-三角形, B-余弦, B-三角形, C-余弦, C-三角形, D-余弦, D-三角形$]$，条目之间用逗号分隔，无多余空格，并用方括号括起来。例如，一个语法正确的输出可能看起来像 $[12,10,8,8,14,13,20,18]$（这些数字仅为示例）。",
            "solution": "该问题是有效的，因为它具有科学依据、是适定的且客观的。它描述了机器学习优化领域中的一个标准模拟任务。我们接下来进行求解。\n\n目标是分析随机梯度下降 (SGD) 在一个简单的一维凸二次函数 $f(x) = \\tfrac{1}{2} x^2$ 上的收敛行为。该函数的梯度为 $\\nabla f(x) = x$。SGD 算法从一个初始点 $x_0$ 开始，迭代地更新位置估计 $x_t$。在第 $t \\in \\{1, 2, \\dots\\}$ 次迭代的更新规则由下式给出：\n$$ x_t = x_{t-1} - \\eta_t g_t $$\n其中 $\\eta_t$ 是第 $t$ 次迭代的学习率，$g_t$ 是随机梯度。随机梯度被建模为前一个位置 $x_{t-1}$ 处的真实梯度加上一个附加噪声项：\n$$ g_t = \\nabla f(x_{t-1}) + \\tilde{\\xi}_t = x_{t-1} + \\tilde{\\xi}_t $$\n噪声项 $\\tilde{\\xi}_t$ 来自一个零均值高斯随机变量 $\\xi_t \\sim \\mathcal{N}(0, \\sigma^2)$，然后对其进行裁剪。裁剪操作由一个非负阈值 $c \\ge 0$ 定义：\n$$ \\tilde{\\xi}_t = \\mathrm{clip}(\\xi_t, -c, c) = \\max(-c, \\min(c, \\xi_t)) $$\n$c = +\\infty$ 的值对应于不进行裁剪，而 $c = 0$ 则完全移除了噪声。\n\n学习率 $\\eta_t$ 遵循一个周期为 $P$ 次迭代的带重启的周期性方案。我们需要实现两种这样的方案，两者都在每个周期内从最大值 $\\eta_{\\max}$ 过渡到最小值 $\\eta_{\\min}$。设 $i_t = (t-1) \\pmod P$ 为全局迭代 $t \\ge 1$ 在周期内从零开始索引的步数。对于 $P  1$，一个周期的完成分数为 $\\alpha_t = \\frac{i_t}{P-1}$。\n\n1.  **三角形重启方案**：该方案在一个周期内实现学习率从 $\\eta_{\\max}$ 到 $\\eta_{\\min}$ 的线性下降。此线性插值的公式为：\n    $$ \\eta_t = (1 - \\alpha_t)\\eta_{\\max} + \\alpha_t\\eta_{\\min} = \\eta_{\\max} - \\left( \\frac{(t-1) \\pmod P}{P-1} \\right) (\\eta_{\\max} - \\eta_{\\min}) $$\n    在周期开始时（$t$ 满足 $(t-1) \\pmod P = 0$），$\\alpha_t=0$ 且 $\\eta_t = \\eta_{\\max}$。在周期结束时（$t$ 满足 $(t-1) \\pmod P = P-1$），$\\alpha_t=1$ 且 $\\eta_t = \\eta_{\\min}$。\n\n2.  **平滑重启（余弦退火）方案**：该方案确保了在周期起点和终点处的平滑过渡，斜率为零。一个合适的从分数进度 $\\alpha_t \\in [0, 1]$ 到具有这些性质的归一化范围 $[1, 0]$ 的映射是半周期余弦函数 $g(\\alpha) = \\frac{1}{2}(1 + \\cos(\\pi \\alpha))$。最终的学习率通过对此函数进行缩放和平移以适应 $[\\eta_{\\min}, \\eta_{\\max}]$ 范围来获得：\n    $$ \\eta_t = \\eta_{\\min} + (\\eta_{\\max} - \\eta_{\\min}) g(\\alpha_t) = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max} - \\eta_{\\min}) \\left(1 + \\cos\\left(\\pi \\frac{(t-1) \\pmod P}{P-1}\\right)\\right) $$\n    根据要求，余弦函数的参数以弧度为单位。\n\n收敛速度通过首次命中时间来度量，定义为损失 $f(x_t) = \\frac{1}{2} x_t^2$ 降至给定阈值 $\\tau  0$ 以下的最小迭代次数 $t$。如果在 $T_{\\max}$ 次迭代的预算内未达到收敛，则命中时间记录为 $T_{\\max}$。为获得鲁棒的估计，此过程对 $R$ 次独立重复实验进行重复，并将所得首次命中时间的中位数作为最终度量。然后将中位数四舍五入到最近的整数。\n\n为了可复现性和两种方案之间的公平比较，模拟协议采用确定性种子策略。对于四个测试案例（A、B、C、D）中的每一个，以及对于每次重复实验 $r \\in \\{0, 1, \\dots, R-1\\}$，都会基于一个基础种子 $s_{\\text{base}}$、案例索引和重复实验索引 $r$ 生成一个唯一的种子。在同一次重复实验中，这个相同的种子被用来初始化余弦和三角形方案模拟的随机数生成器，确保它们暴露在完全相同的随机噪声抽取序列 $\\xi_t$ 之下。这种配对实验设计允许在相同的随机条件下直接比较方案的性能。\n\n程序将在指定的四个参数案例中实现此模拟，计算每种情况下两种学习率方案的首次命中时间中位数，并按要求将八个得到的整数值格式化为单个列表。",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Simulates SGD with cyclical learning rates to find median convergence time.\n    \"\"\"\n    # Define test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'eta_min': 0.05, 'eta_max': 0.8, 'P': 40, 'sigma': 0.5, 'c': 0.2, 'tau': 0.01, 'T_max': 1000},\n        # Case B\n        {'eta_min': 0.05, 'eta_max': 0.8, 'P': 40, 'sigma': 0.5, 'c': 0.0, 'tau': 0.01, 'T_max': 1000},\n        # Case C\n        {'eta_min': 0.05, 'eta_max': 0.8, 'P': 40, 'sigma': 0.5, 'c': np.inf, 'tau': 0.01, 'T_max': 1000},\n        # Case D\n        {'eta_min': 0.1, 'eta_max': 1.8, 'P': 40, 'sigma': 0.5, 'c': 0.2, 'tau': 0.01, 'T_max': 1000},\n    ]\n\n    # Global simulation parameters\n    R = 200\n    base_seed = 12345\n    x_init = 5.0\n    \n    final_results = []\n\n    for case_idx, params in enumerate(test_cases):\n        eta_min = params['eta_min']\n        eta_max = params['eta_max']\n        P = params['P']\n        sigma = params['sigma']\n        c = params['c']\n        tau = params['tau']\n        T_max = params['T_max']\n\n        hitting_times_cosine = []\n        hitting_times_triangular = []\n        \n        for r in range(R):\n            # Generate a unique, deterministic seed for each replicate of each case\n            seed = base_seed + case_idx * R + r\n            \n            # --- Paired Simulation for Cosine Schedule ---\n            rng = np.random.default_rng(seed)\n            x = x_init\n            h_time_cosine = T_max\n            for t in range(1, T_max + 1):\n                # Calculate learning rate using cosine annealing schedule\n                i_cycle = (t - 1) % P\n                alpha_t = i_cycle / (P - 1) if P > 1 else 1.0\n                eta = eta_min + 0.5 * (eta_max - eta_min) * (1 + math.cos(math.pi * alpha_t))\n                \n                # Generate and clip noise\n                noise = rng.normal(0, sigma)\n                clipped_noise = np.clip(noise, -c, c)\n                \n                # Perform SGD update\n                stochastic_grad = x + clipped_noise\n                x = x - eta * stochastic_grad\n                \n                # Check for convergence\n                loss = 0.5 * x**2\n                if loss = tau:\n                    h_time_cosine = t\n                    break\n            hitting_times_cosine.append(h_time_cosine)\n\n            # --- Paired Simulation for Triangular Schedule ---\n            # Re-seed the generator to get the same noise sequence for this replicate\n            rng = np.random.default_rng(seed)\n            x = x_init\n            h_time_triangular = T_max\n            for t in range(1, T_max + 1):\n                # Calculate learning rate using triangular schedule\n                i_cycle = (t - 1) % P\n                alpha_t = i_cycle / (P - 1) if P > 1 else 1.0\n                eta = eta_max - alpha_t * (eta_max - eta_min)\n                \n                # Generate and clip noise\n                noise = rng.normal(0, sigma)\n                clipped_noise = np.clip(noise, -c, c)\n                \n                # Perform SGD update\n                stochastic_grad = x + clipped_noise\n                x = x - eta * stochastic_grad\n                \n                # Check for convergence\n                loss = 0.5 * x**2\n                if loss = tau:\n                    h_time_triangular = t\n                    break\n            hitting_times_triangular.append(h_time_triangular)\n            \n        # Calculate and round median hitting times\n        median_cosine = np.median(hitting_times_cosine)\n        median_triangular = np.median(hitting_times_triangular)\n\n        final_results.append(int(np.round(median_cosine)))\n        final_results.append(int(np.round(median_triangular)))\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后的这个练习将带你实现一种更高级的自适应学习率策略。你将构建一个控制器，它通过估算损失函数的局部曲率（Hessian 矩阵的最大特征值 $\\lambda_{\\max}$），动态地调整每个周期的最大学習率 $\\eta_{\\max}$，从而在保持探索性的同时确保训练的稳定性 。这个练习将学习率调度这一抽象概念与优化稳定性的具体数学原理（例如，稳定性要求 $\\eta \\lt 2/\\lambda_{\\max}$）联系起来，是成为高级实践者的关键一步。",
            "id": "3110195",
            "problem": "要求您实现一个完整、可运行的程序，用于在深度学习的梯度下降（GD）背景下，构建一个使用余弦退火（CA）的周期性学习率调度器的控制器。该控制器必须根据通过对对称半正定矩阵进行幂迭代获得的曲率估计，来调整每个周期的最大学习率，并且必须强制执行最大学习率不大于从最大曲率估计导出的倒数稳定性阈值的约束。实现必须遵循标准的数学和算法原理，不得依赖问题陈述中提供的捷径公式。\n\n您必须使用的基本依据是以下广为接受的事实：\n- 对于一个平滑损失函数在某点附近的局部二次近似，损失可以写成 $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} \\mathbf{H} \\mathbf{x}$，其中 $\\mathbf{H}$ 是一个表示曲率的对称半正定矩阵。\n- 梯度下降（GD）的迭代公式为 $\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\eta \\nabla f(\\mathbf{x}_{k})$，在二次近似的情况下，它简化为一个由矩阵 $\\mathbf{I} - \\eta \\mathbf{H}$驱动的线性系统。\n- 在二次目标函数上，梯度下降的稳定性由更新矩阵的谱半径控制，该谱半径受 $\\mathbf{H}$ 的最大特征值限制。\n- 对称矩阵的最大特征值可以通过利用瑞利商的幂迭代法从第一性原理进行近似，而无需进行显式特征分解。\n\n您必须在程序中实现以下组件：\n- 一个幂迭代例程，给定一个对称半正定矩阵 $\\mathbf{H}$ 和一个迭代预算 $K$，返回 $\\mathbf{H}$ 的最大特征值的估计值 $\\hat{\\lambda}_{\\max}$。\n- 一个控制器，在每个周期开始时，将最大学习率 $\\eta_{\\max}$ 设置为 $\\min\\left(\\eta_{\\text{target}}, \\tfrac{2}{\\hat{\\lambda}_{\\max}}\\right)$，其中 $\\eta_{\\text{target}}$ 是用户指定的目标振幅，$\\tfrac{2}{\\hat{\\lambda}_{\\max}}$ 是从二次梯度下降动力学导出的稳定性阈值。当 $\\hat{\\lambda}_{\\max} \\le 0$ 时，将 $\\tfrac{2}{\\hat{\\lambda}_{\\max}}$ 解释为 $+\\infty$。\n- 每个周期基于余弦退火（CA）的周期性调度器，角度以弧度为单位。在每个长度为 $T$ 步的周期中，学习率必须从 $\\eta_{\\max}$ 开始，到 $\\eta_{\\min}$ 结束，其中 $\\eta_{\\min}$ 是用户指定的下界。对于边界情况 $T = 1$，调度器必须只包含一个等于 $\\eta_{\\max}$ 的值。\n- 一个验证例程，用于在每个周期检查该周期的所有学习率值 $\\eta_t$ 是否满足 $\\eta_{\\min} \\le \\eta_t \\le \\eta_{\\max}$ 和 $\\eta_t \\le \\tfrac{2}{\\hat{\\lambda}_{\\max}}$，并且当 $T  1$ 时调度器从 $\\eta_{\\max}$ 开始到 $\\eta_{\\min}$ 结束，或者当 $T = 1$ 时在 $\\eta_{\\max}$ 保持不变。\n\n角度单位必须是弧度。不涉及带单位的物理量。\n\n测试套件：\n在以下测试用例上实现并评估您的控制器。每个测试用例指定了曲率矩阵 $\\mathbf{H}$、周期配置和控制器参数。为了可复现性，请按给定方式为每个周期的幂迭代使用独立的随机种子。所有矩阵都是对称的，所有长度和计数都是整数。\n\n- 测试用例 1（正常路径，中等曲率）：\n  - $\\mathbf{H} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  0.5 \\end{bmatrix}$，\n  - $\\eta_{\\min} = 0.0$，\n  - $\\eta_{\\text{target}} = 0.9$，\n  - 周期数 $C = 2$，\n  - 周期长度 $T = 10$，\n  - 幂迭代步数 $K = 50$，\n  - 基础种子 $s = 101$。\n\n- 测试用例 2（需要限制的高曲率）：\n  - $\\mathbf{H} = \\begin{bmatrix} 5.0  0.0 \\\\ 0.0  3.0 \\end{bmatrix}$，\n  - $\\eta_{\\min} = 0.05$，\n  - $\\eta_{\\text{target}} = 0.9$，\n  - 周期数 $C = 3$，\n  - 周期长度 $T = 8$，\n  - 幂迭代步数 $K = 30$，\n  - 基础种子 $s = 202$。\n\n- 测试用例 3（近零曲率，大稳定性阈值）：\n  - $\\mathbf{H} = 0.001 \\cdot \\mathbf{I}_{3}$，\n  - $\\eta_{\\min} = 0.0$，\n  - $\\eta_{\\text{target}} = 1.5$，\n  - 周期数 $C = 2$，\n  - 周期长度 $T = 4$，\n  - 幂迭代步数 $K = 20$，\n  - 基础种子 $s = 303$。\n\n- 测试用例 4（病态曲率）：\n  - $\\mathbf{H} = \\begin{bmatrix} 10.0  0.0 \\\\ 0.0  0.01 \\end{bmatrix}$，\n  - $\\eta_{\\min} = 0.0$，\n  - $\\eta_{\\text{target}} = 0.25$，\n  - 周期数 $C = 3$，\n  - 周期长度 $T = 7$，\n  - 幂迭代步数 $K = 25$，\n  - 基础种子 $s = 404$。\n\n- 测试用例 5（边界情况 $T = 1$）：\n  - $\\mathbf{H} = \\begin{bmatrix} 2.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$，\n  - $\\eta_{\\min} = 0.1$，\n  - $\\eta_{\\text{target}} = 1.0$，\n  - 周期数 $C = 1$，\n  - 周期长度 $T = 1$，\n  - 幂迭代步数 $K = 40$，\n  - 基础种子 $s = 505$。\n\n您的程序必须生成一行输出，其中包含五个测试用例的验证结果，形式为一个用方括号括起来的逗号分隔列表，列表中的每个项目都是一个布尔值，指示该测试用例的所有周期是否都满足所有约束，例如 $\\left[\\text{True},\\text{False},\\dots\\right]$。所有角度单位必须是弧度。不允许用户输入；程序必须完全自包含且可直接运行。",
            "solution": "我们从深度学习平滑损失函数在某点附近的局部二次近似开始，这是一个经过充分检验的事实。设损失为 $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} \\mathbf{H} \\mathbf{x}$，其中 $\\mathbf{H}$ 是一个捕捉曲率的对称半正定矩阵。学习率为 $\\eta$ 的梯度下降（GD）迭代是\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\eta \\nabla f(\\mathbf{x}_{k}) = \\mathbf{x}_{k} - \\eta \\mathbf{H} \\mathbf{x}_{k} = \\left(\\mathbf{I} - \\eta \\mathbf{H}\\right)\\mathbf{x}_{k}.\n$$\n此线性系统的收敛性和稳定性取决于谱半径 $\\rho\\left(\\mathbf{I} - \\eta \\mathbf{H}\\right)$。由于 $\\mathbf{H}$ 是对称的，它可以对角化，且特征值为非负实数 $\\lambda_{1}, \\dots, \\lambda_{n}$。矩阵 $\\mathbf{I} - \\eta \\mathbf{H}$ 的特征值为 $1 - \\eta \\lambda_{i}$。稳定性要求\n$$\n\\max_{i} \\left|1 - \\eta \\lambda_{i}\\right|  1,\n$$\n这意味着对于每个 $i$，\n$$\n-1  1 - \\eta \\lambda_{i}  1 \\quad \\Rightarrow \\quad 0  \\eta \\lambda_{i}  2.\n$$\n为了同时满足所有特征值的这一条件，只需强制执行\n$$\n0  \\eta  \\frac{2}{\\lambda_{\\max}},\n$$\n其中 $\\lambda_{\\max} = \\max_{i} \\lambda_{i}$。这个不等式给出了在二次机制下对稳定学习率的有原则的上界。\n\n我们不通过显式特征分解直接计算 $\\lambda_{\\max}$；而是使用幂迭代法这一第一性原理算法来估计它。设 $\\mathbf{v}_{0}$ 是一个非零初始向量。幂迭代的更新步骤为\n$$\n\\mathbf{w}_{k} = \\mathbf{H} \\mathbf{v}_{k}, \\quad \\mathbf{v}_{k+1} = \\frac{\\mathbf{w}_{k}}{\\lVert \\mathbf{w}_{k} \\rVert}.\n$$\n在对称矩阵具有唯一的模最大特征值的标准条件下，$\\mathbf{v}_{k}$ 收敛到与 $\\lambda_{\\max}$ 关联的特征向量，并且瑞利商\n$$\n\\hat{\\lambda}_{\\max}^{(k)} = \\mathbf{v}_{k}^{\\top} \\mathbf{H} \\mathbf{v}_{k}\n$$\n收敛到 $\\lambda_{\\max}$。如果 $\\mathbf{H}$ 是零矩阵，则 $\\mathbf{H}\\mathbf{v}_{k} = \\mathbf{0}$，瑞利商为 $0$，我们将其解释为产生一个无穷大的稳定性阈值 $\\frac{2}{\\hat{\\lambda}_{\\max}} = +\\infty$。\n\n现在我们来设计周期性学习率控制器。在每个周期中，在构建学习率调度器之前，我们：\n- 对 $\\mathbf{H}$ 运行 $K$ 步幂迭代以获得 $\\hat{\\lambda}_{\\max}$。\n- 如果 $\\hat{\\lambda}_{\\max}  0$，计算稳定性上限 $c = \\frac{2}{\\hat{\\lambda}_{\\max}}$，否则 $c = +\\infty$。\n- 将周期内的最大学习率设置为\n$$\n\\eta_{\\max} = \\min\\left(\\eta_{\\text{target}}, c\\right).\n$$\n- 使用以弧度为单位角度的余弦退火（CA）来构建一个长度为 $T$ 的调度器，该调度器从 $\\eta_{\\max}$ 开始，到 $\\eta_{\\min}$ 结束。CA 调度器是周期性学习率（CLR）设计中的标准做法，它是通过将离散的步数索引 $t \\in \\{0, 1, \\dots, T-1\\}$ 映射到 $[0, \\pi]$ 上的余弦曲线上的点来导出的。这种映射确保了从 $\\eta_{\\max}$到 $\\eta_{\\min}$ 的平滑过渡。对于边界情况 $T = 1$，调度器必须是单一值 $\\eta_{\\max}$，这是余弦路径退化为一个点时的极限行为。\n\n我们为每个周期验证以下属性：\n- 边界：对于所有步骤 $t$，$\\eta_{\\min} \\le \\eta_{t} \\le \\eta_{\\max}$。\n- 稳定性：在该周期中的所有步骤 $t$，$\\eta_{t} \\le \\frac{2}{\\hat{\\lambda}_{\\max}}$。\n- 端点：如果 $T  1$，第一个值等于 $\\eta_{\\max}$，最后一个值等于 $\\eta_{\\min}$。如果 $T = 1$，唯一的值等于 $\\eta_{\\max}$。\n\n最后，我们将整个测试套件的验证结果汇总成一个布尔值列表。每个布尔值指示相应测试用例的所有周期是否都满足约束。角度始终以弧度为单位。\n\n代码中实现的算法步骤：\n- 为每个测试用例构建给定的测试矩阵 $\\mathbf{H}$ 和参数 $(\\eta_{\\min}, \\eta_{\\text{target}}, C, T, K, s)$。\n- 对于每个周期 $c \\in \\{0, \\dots, C-1\\}$，使用从 $s + c$ 导出的可复现随机种子运行幂迭代，通过控制器计算 $\\eta_{\\max}$，并构建 CA 调度器。\n- 检查约束条件，并为每个测试用例累积一个通过/失败的布尔值。\n- 以指定格式将五个布尔值的列表作为单行打印出来。\n\n此设计与梯度下降在二次机制下的基本稳定性条件相符，利用了有原则的特征值估计，并使用以弧度为单位的 CA 实现了一个成熟的 CLR 机制，同时正确处理了如 $T=1$ 和近零曲率等边缘情况。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_iteration(H: np.ndarray, num_iters: int, seed: int) -> float:\n    \"\"\"\n    Estimate the largest eigenvalue of a symmetric matrix H using power iteration.\n    Uses the Rayleigh quotient on the normalized iterate.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n = H.shape[0]\n    v = rng.normal(size=(n,))\n    # Handle potential zero vector initialization, enforce nonzero\n    if np.linalg.norm(v) == 0.0:\n        v = np.ones(n)\n    v = v / np.linalg.norm(v)\n\n    lambda_hat = 0.0\n    for _ in range(num_iters):\n        w = H @ v\n        norm_w = np.linalg.norm(w)\n        if norm_w == 0.0:\n            # H might be (near) zero; lambda_hat remains 0\n            lambda_hat = 0.0\n            break\n        v = w / norm_w\n        # Rayleigh quotient on the new direction\n        lambda_hat = float(v.T @ (H @ v))\n    return lambda_hat\n\ndef cosine_cycle_schedule(eta_min: float, eta_max: float, T: int) -> np.ndarray:\n    \"\"\"\n    Construct a cosine annealing schedule over T steps in radians,\n    starting at eta_max and ending at eta_min. For T == 1, return [eta_max].\n    \"\"\"\n    if T = 0:\n        raise ValueError(\"Cycle length T must be a positive integer.\")\n    if T == 1:\n        return np.array([eta_max], dtype=float)\n    t = np.arange(T, dtype=float)\n    # Cosine in radians: angle = pi * t / (T - 1)\n    lrs = eta_min + 0.5 * (eta_max - eta_min) * (1.0 + np.cos(np.pi * t / (T - 1)))\n    return lrs\n\ndef verify_cycle(lrs: np.ndarray, eta_min: float, eta_max: float, lambda_hat: float, T: int) -> bool:\n    \"\"\"\n    Verify that the schedule respects [eta_min, eta_max], is capped by 2/lambda_hat,\n    and has correct endpoints (start at eta_max, end at eta_min for T>1; constant for T==1).\n    \"\"\"\n    tol = 1e-12\n    # Range checks\n    cond_range = (lrs.min() >= eta_min - tol) and (lrs.max() = eta_max + tol)\n    # Stability cap\n    cap = np.inf if lambda_hat = 0.0 else (2.0 / lambda_hat) + tol\n    cond_cap = np.all(lrs = cap)\n    # Endpoint checks\n    if T > 1:\n        cond_start = abs(lrs[0] - eta_max) = tol\n        cond_end = abs(lrs[-1] - eta_min) = tol\n        cond_endpoints = cond_start and cond_end\n    else:\n        cond_endpoints = abs(lrs[0] - eta_max) = tol\n    return cond_range and cond_cap and cond_endpoints\n\ndef controller_and_verify(H: np.ndarray, eta_min: float, eta_target: float,\n                          cycles: int, T: int, K: int, base_seed: int) -> bool:\n    \"\"\"\n    For each cycle, estimate curvature via power iteration, set eta_max = min(eta_target, 2/lambda_hat),\n    build the cosine annealing schedule, and verify constraints. Return True iff all cycles pass.\n    \"\"\"\n    all_ok = True\n    for c in range(cycles):\n        # Per-cycle seed to vary the initial direction deterministically\n        seed = base_seed + c\n        lambda_hat = power_iteration(H, K, seed)\n        eta_cap = np.inf if lambda_hat = 0.0 else 2.0 / lambda_hat\n        eta_max = min(eta_target, eta_cap)\n        lrs = cosine_cycle_schedule(eta_min, eta_max, T)\n        ok = verify_cycle(lrs, eta_min, eta_max, lambda_hat, T)\n        all_ok = all_ok and ok\n    return all_ok\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"H\": np.array([[1.0, 0.0],\n                           [0.0, 0.5]], dtype=float),\n            \"eta_min\": 0.0,\n            \"eta_target\": 0.9,\n            \"cycles\": 2,\n            \"T\": 10,\n            \"K\": 50,\n            \"seed\": 101,\n        },\n        # Test Case 2\n        {\n            \"H\": np.array([[5.0, 0.0],\n                           [0.0, 3.0]], dtype=float),\n            \"eta_min\": 0.05,\n            \"eta_target\": 0.9,\n            \"cycles\": 3,\n            \"T\": 8,\n            \"K\": 30,\n            \"seed\": 202,\n        },\n        # Test Case 3\n        {\n            \"H\": 0.001 * np.eye(3, dtype=float),\n            \"eta_min\": 0.0,\n            \"eta_target\": 1.5,\n            \"cycles\": 2,\n            \"T\": 4,\n            \"K\": 20,\n            \"seed\": 303,\n        },\n        # Test Case 4\n        {\n            \"H\": np.array([[10.0, 0.0],\n                           [0.0, 0.01]], dtype=float),\n            \"eta_min\": 0.0,\n            \"eta_target\": 0.25,\n            \"cycles\": 3,\n            \"T\": 7,\n            \"K\": 25,\n            \"seed\": 404,\n        },\n        # Test Case 5\n        {\n            \"H\": np.array([[2.0, 0.0],\n                           [0.0, 1.0]], dtype=float),\n            \"eta_min\": 0.1,\n            \"eta_target\": 1.0,\n            \"cycles\": 1,\n            \"T\": 1,\n            \"K\": 40,\n            \"seed\": 505,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        ok = controller_and_verify(\n            H=case[\"H\"],\n            eta_min=case[\"eta_min\"],\n            eta_target=case[\"eta_target\"],\n            cycles=case[\"cycles\"],\n            T=case[\"T\"],\n            K=case[\"K\"],\n            base_seed=case[\"seed\"]\n        )\n        results.append(ok)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}