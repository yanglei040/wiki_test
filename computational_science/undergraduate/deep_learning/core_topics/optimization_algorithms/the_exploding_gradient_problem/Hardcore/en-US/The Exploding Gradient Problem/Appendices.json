{
    "hands_on_practices": [
        {
            "introduction": "To understand the exploding gradient problem, it's best to start with a simplified model that isolates the core mechanism. This first practice uses a deep, linear network where each layer's weight matrix is a scaled identity, $W_{\\ell} = \\alpha I$. This setup reveals how gradients scale exponentially with depth, proportional to $|\\alpha|^L$, providing a clear mathematical and computational illustration of why gradients can explode or vanish . You will also implement and test two fundamental mitigation strategies, norm clipping and value clipping, to see their effects firsthand.",
            "id": "3184988",
            "problem": "Consider a fully connected Multi-Layer Perceptron (MLP) with $L$ layers, each of width $d$ and without bias terms. Assume the activation function is the identity function, and that each layer weight matrix is $W_\\ell = \\alpha I_d$, where $I_d$ is the $d \\times d$ identity matrix and $\\alpha \\in \\mathbb{R}$. Let the network input be $x_0 \\in \\mathbb{R}^d$, the network output be $y \\in \\mathbb{R}^d$, and the target be $t \\in \\mathbb{R}^d$. The scalar loss is defined by the standard mean squared error $\\ell(y,t) = \\frac{1}{2}\\|y - t\\|_2^2$. Using only the chain rule of calculus, matrix multiplication properties, and the Euclidean norm definition, derive an explicit expression for the gradient $\\nabla_{x_0} \\ell$ and its magnitude scaling with respect to the parameters $\\alpha$ and $L$. Then, implement two gradient clipping strategies: norm clipping and value clipping. Norm clipping scales the gradient $g \\in \\mathbb{R}^d$ by the factor $\\min\\left(1, \\frac{c}{\\|g\\|_2}\\right)$ for a threshold $c  0$. Value clipping clamps each component $g_i$ to the interval $[-v, v]$ for a threshold $v  0$.\n\nYour program must, for each provided test case, compute the following quantities:\n- The ratio $R = \\frac{\\|\\nabla_{x_0} \\ell\\|_2}{\\|\\nabla_{y} \\ell\\|_2}$.\n- The Euclidean norm $\\|\\nabla_{x_0} \\ell\\|_2$ before clipping.\n- The Euclidean norm of the norm-clipped gradient.\n- The Euclidean norm of the value-clipped gradient.\n\nUse double-precision floating point arithmetic. No physical units are involved. Angles are not involved. All returned numerical values must be floating point numbers.\n\nTest Suite (each tuple is $(\\alpha, L, d, x_0, t, c, v)$):\n- Case $1$ (general case): $(\\alpha, L, d) = (1.2, 8, 5)$, $x_0 = [1.0, -0.5, 0.25, 2.0, -1.5]$, $t = [0.0, 0.0, 0.0, 0.0, 0.0]$, $c = 10.0$, $v = 3.0$.\n- Case $2$ (boundary $\\alpha = 1$): $(\\alpha, L, d) = (1.0, 7, 4)$, $x_0 = [0.1, -0.2, 0.3, -0.4]$, $t = [0.5, -0.5, 0.5, -0.5]$, $c = 5.0$, $v = 2.5$.\n- Case $3$ (exploding regime): $(\\alpha, L, d) = (2.0, 12, 4)$, $x_0 = [0.01, 0.02, -0.03, 0.04]$, $t = [0.0, 0.0, 0.0, 0.0]$, $c = 1.0$, $v = 0.5$.\n- Case $4$ (vanishing regime): $(\\alpha, L, d) = (0.8, 20, 6)$, $x_0 = [-0.2, 0.1, -0.1, 0.05, -0.05, 0.025]$, $t = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]$, $c = 0.1$, $v = 0.05$.\n- Case $5$ (boundary $L = 0$): $(\\alpha, L, d) = (1.7, 0, 3)$, $x_0 = [2.0, -1.0, 0.5]$, $t = [-1.0, 0.0, 1.0]$, $c = 0.5$, $v = 0.3$.\n- Case $6$ (scalar edge case): $(\\alpha, L, d) = (3.0, 15, 1)$, $x_0 = [0.1]$, $t = [-0.2]$, $c = 2.0$, $v = 1.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of four floating point numbers in the exact order: $[R, \\|\\nabla_{x_0} \\ell\\|_2, \\|\\text{norm\\_clipped}(\\nabla_{x_0} \\ell)\\|_2, \\|\\text{value\\_clipped}(\\nabla_{x_0} \\ell)\\|_2]$. For example, the final output should look like $[[r_1,n_1,cn_1,vn_1],[r_2,n_2,cn_2,vn_2],\\dots]$.",
            "solution": "We start from first principles and restrict ourselves to the chain rule of calculus, basic properties of matrix multiplication, and the Euclidean norm. The Multi-Layer Perceptron (MLP) has $L$ layers of width $d$, with each weight matrix $W_\\ell = \\alpha I_d$ and identity activation. The forward map is a composition of linear maps. Denote the input by $x_0 \\in \\mathbb{R}^d$ and the output by $y \\in \\mathbb{R}^d$. Because the activation is the identity, the layerwise transformation for layer $\\ell$ is $x_\\ell = W_\\ell x_{\\ell-1}$. Since $W_\\ell = \\alpha I_d$ for all $\\ell$, we have\n$$\nx_\\ell = \\alpha I_d x_{\\ell-1} = \\alpha x_{\\ell-1}.\n$$\nBy induction through $L$ layers,\n$$\ny = x_L = \\alpha x_{L-1} = \\alpha^2 x_{L-2} = \\cdots = \\alpha^L x_0.\n$$\nThe loss is the mean squared error\n$$\n\\ell(y,t) = \\frac{1}{2} \\|y - t\\|_2^2,\n$$\nwhose gradient with respect to $y$ is\n$$\n\\nabla_y \\ell = y - t.\n$$\nUsing the chain rule, the gradient with respect to the input $x_0$ is given by\n$$\n\\nabla_{x_0} \\ell = J^\\top \\nabla_y \\ell,\n$$\nwhere $J = \\frac{\\partial y}{\\partial x_0}$ is the Jacobian matrix of the forward map. From $y = \\alpha^L x_0$, the Jacobian is\n$$\nJ = \\alpha^L I_d.\n$$\nTherefore,\n$$\n\\nabla_{x_0} \\ell = (\\alpha^L I_d)^\\top (y - t) = \\alpha^L (y - t).\n$$\nConsequently, the Euclidean norm scales as\n$$\n\\|\\nabla_{x_0} \\ell\\|_2 = |\\alpha|^L \\|y - t\\|_2.\n$$\nIt follows that the ratio\n$$\nR = \\frac{\\|\\nabla_{x_0} \\ell\\|_2}{\\|\\nabla_y \\ell\\|_2} = \\frac{|\\alpha|^L \\|y - t\\|_2}{\\|y - t\\|_2} = |\\alpha|^L.\n$$\nThis derivation shows that for $|\\alpha|  1$ and sufficiently large $L$, the gradient magnitude grows exponentially with $L$, which is the exploding gradient phenomenon. Conversely, for $|\\alpha|  1$, the gradient magnitude decays, manifesting vanishing gradients. For the boundary cases $|\\alpha| = 1$ or $L = 0$, the ratio is $R = 1$.\n\nWe now define two clipping strategies to mitigate exploding gradients:\n- Norm clipping: for a gradient vector $g \\in \\mathbb{R}^d$ and threshold $c  0$, define\n$$\ng_{\\text{norm-clip}} = g \\cdot \\min\\left(1, \\frac{c}{\\|g\\|_2}\\right).\n$$\nWhen $\\|g\\|_2 \\le c$, this leaves $g$ unchanged; when $\\|g\\|_2  c$, it scales $g$ down so that $\\|g_{\\text{norm-clip}}\\|_2 = c$.\n- Value clipping: for a threshold $v  0$, define\n$$\n(g_{\\text{value-clip}})_i = \\max(-v, \\min(g_i, v)) \\quad \\text{for each component } i.\n$$\nThis clamps each entry to the interval $[-v, v]$, potentially reducing the norm and limiting per-component magnitude.\n\nAlgorithmic plan for each test case:\n1. Parse $(\\alpha, L, d, x_0, t, c, v)$.\n2. Compute the forward output $y = \\alpha^L x_0$.\n3. Compute $\\nabla_y \\ell = y - t$.\n4. Compute $\\nabla_{x_0} \\ell = \\alpha^L (y - t)$.\n5. Compute $R = \\frac{\\|\\nabla_{x_0} \\ell\\|_2}{\\|\\nabla_y \\ell\\|_2}$.\n6. Compute the unclipped norm $\\|\\nabla_{x_0} \\ell\\|_2$.\n7. Apply norm clipping to obtain $g_{\\text{norm-clip}}$ and compute its norm.\n8. Apply value clipping to obtain $g_{\\text{value-clip}}$ and compute its norm.\n9. Return the list $[R, \\|\\nabla_{x_0} \\ell\\|_2, \\|g_{\\text{norm-clip}}\\|_2, \\|g_{\\text{value-clip}}\\|_2]$.\n\nEdge-case considerations:\n- For $L = 0$, $y = x_0$, so $J = I_d$, and $R = 1$; gradients are not amplified.\n- For $|\\alpha| = 1$, $R = 1$ regardless of $L$; gradients are preserved in magnitude.\n- For $d = 1$, the same derivation applies, and norms coincide with absolute values; value clipping may differ substantially from norm clipping because it clamps the sole component directly.\n- Numerical stability: double precision handles the chosen $\\alpha$ and $L$ values, which produce norms up to approximately $10^{14}$, within reasonable floating point dynamic range.\n\nThe program aggregates the results for all cases and prints them in the specified single-line format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef forward_output(alpha: float, L: int, x0: np.ndarray) - np.ndarray:\n    \"\"\"Compute y = alpha^L * x0 for the identity-activation MLP with W_l = alpha I.\"\"\"\n    return (alpha ** L) * x0\n\ndef loss_grad_wrt_y(y: np.ndarray, t: np.ndarray) - np.ndarray:\n    \"\"\"Gradient of 0.5 * ||y - t||^2 with respect to y is (y - t).\"\"\"\n    return y - t\n\ndef input_grad(alpha: float, L: int, grad_y: np.ndarray) - np.ndarray:\n    \"\"\"Gradient wrt input x0 given grad wrt y and Jacobian J = alpha^L * I.\"\"\"\n    return (alpha ** L) * grad_y\n\ndef norm_clip(g: np.ndarray, c: float) - np.ndarray:\n    \"\"\"Clip gradient by global norm threshold c.\"\"\"\n    norm = np.linalg.norm(g)\n    if norm == 0.0:\n        return g.copy()\n    scale = min(1.0, c / norm)\n    return g * scale\n\ndef value_clip(g: np.ndarray, v: float) - np.ndarray:\n    \"\"\"Clip gradient by value threshold v (component-wise clamp).\"\"\"\n    return np.clip(g, -v, v)\n\ndef compute_case(alpha, L, d, x0_list, t_list, c, v):\n    x0 = np.array(x0_list, dtype=float)\n    t = np.array(t_list, dtype=float)\n    assert x0.shape == (d,), \"x0 must have shape (d,)\"\n    assert t.shape == (d,), \"t must have shape (d,)\"\n\n    y = forward_output(alpha, L, x0)\n    gy = loss_grad_wrt_y(y, t)\n    gx = input_grad(alpha, L, gy)\n\n    norm_gy = np.linalg.norm(gy)\n    norm_gx = np.linalg.norm(gx)\n\n    # Ratio R = ||grad_x0|| / ||grad_y||\n    R = float(norm_gx / norm_gy) if norm_gy != 0.0 else float('inf')\n\n    gx_norm_clipped = norm_clip(gx, c)\n    gx_value_clipped = value_clip(gx, v)\n\n    return [R, norm_gx, float(np.linalg.norm(gx_norm_clipped)), float(np.linalg.norm(gx_value_clipped))]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (alpha, L, d, x0, t, c, v)\n    test_cases = [\n        (1.2, 8, 5,  [1.0, -0.5, 0.25, 2.0, -1.5], [0.0, 0.0, 0.0, 0.0, 0.0], 10.0, 3.0),\n        (1.0, 7, 4,  [0.1, -0.2, 0.3, -0.4], [0.5, -0.5, 0.5, -0.5], 5.0, 2.5),\n        (2.0, 12, 4, [0.01, 0.02, -0.03, 0.04], [0.0, 0.0, 0.0, 0.0], 1.0, 0.5),\n        (0.8, 20, 6, [-0.2, 0.1, -0.1, 0.05, -0.05, 0.025], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.1, 0.05),\n        (1.7, 0, 3,  [2.0, -1.0, 0.5], [-1.0, 0.0, 1.0], 0.5, 0.3),\n        (3.0, 15, 1, [0.1], [-0.2], 2.0, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, L, d, x0, t, c, v = case\n        result = compute_case(alpha, L, d, x0, t, c, v)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Single line, comma-separated list enclosed in square brackets, each element a list of four floats.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The previous exercise demonstrated gradient explosion using a simple scalar $\\alpha$. In a general network, the amplification factor of a weight matrix $W$ is governed by its spectral norm, $\\|W\\|_2$. This practice designs a controlled experiment to empirically verify that the gradient norm scales with the product of spectral norms across layers. By using spectral normalization and aligning the gradient with the dominant eigenvector, you will create a 'worst-case' scenario where the theoretical upper bound on gradient growth becomes a measurable equality, beautifully connecting abstract theory to concrete results .",
            "id": "3185019",
            "problem": "You are to implement a controlled computational experiment that investigates the exploding gradient phenomenon in deep learning using a deep linear model with spectral normalization. The experiment must be grounded in first principles, specifically the chain rule of differentiation in composition, the definition of the operator norm, and the submultiplicative property of norms. Begin from the following foundations: the chain rule for compositions in vector calculus, the definition of the matrix operator $2$-norm as $\\|W\\|_{2} = \\max_{\\|x\\|_{2}=1} \\|W x\\|_{2}$, and the submultiplicative property $\\|A B\\|_{2} \\le \\|A\\|_{2}\\|B\\|_{2}$. Do not assume any higher-level formulas beyond these fundamentals.\n\nYou will construct a deep linear network of depth $L$ with tied weights and a linear loss. Specifically, consider an input vector $x \\in \\mathbb{R}^{d}$, a square weight matrix $W \\in \\mathbb{R}^{d \\times d}$ that is shared across all layers, and an output $y = W^{L} x$. Use a linear scalar loss $L_{\\text{loss}} = u^{\\top} y$, where $u \\in \\mathbb{R}^{d}$ is a fixed readout vector. The gradient of interest is the gradient of $L_{\\text{loss}}$ with respect to the input $x$. Your task is to empirically estimate how the norm $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$ scales as a function of a target spectral radius $\\alpha$, when the weight matrix is spectrally normalized to have operator norm $\\|W\\|_{2} = \\alpha$.\n\nImplement the following controlled setup:\n- Use spectral normalization defined by $W = \\alpha \\,\\bar{W}/\\|\\bar{W}\\|_{2}$, where $\\bar{W} \\in \\mathbb{R}^{d \\times d}$ is a positive semidefinite matrix constructed as $\\bar{W} = A A^{\\top}$ for a random matrix $A \\in \\mathbb{R}^{d \\times d}$ with independent standard normal entries. This guarantees $\\bar{W}$ is symmetric positive semidefinite and that $\\|W\\|_{2} = \\alpha$.\n- Choose the readout vector $u$ to be the unit-norm dominant eigenvector of $\\bar{W}$ (equivalently, of $W$), ensuring a worst-case alignment for growth under repeated application.\n- Use a fixed input dimension $d = 8$ and a fixed random seed $s = 12345$ to construct $A$ so that results are deterministic.\n- For each depth $L$, vary $\\alpha$ over a fixed set of positive values and measure $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$ for each $\\alpha$. Then fit a model of the form $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2} \\approx C \\,\\alpha^{p}$ by linear regression in logarithmic space to estimate the exponent $p$ for that depth.\n\nYour program must implement the following test suite and estimation protocol:\n- Dimensions and seed: $d = 8$, $s = 12345$.\n- Depths: $L \\in \\{1, 3, 5, 7\\}$.\n- Target spectral radii: $\\alpha \\in \\{0.25, 0.5, 1.0, 1.2, 1.5\\}$.\n- For each fixed $L$, compute $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$ at each $\\alpha$ using the construction above with tied weights and the linear loss, then estimate the exponent $p$ by fitting $\\log \\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$ versus $\\log \\alpha$ using ordinary least squares to obtain a slope. Use natural logarithms.\n- Because all $\\alpha$ values are dimensionless and there are no physical units, no unit conversion is required. Angles are not involved.\n\nYour program should produce a single line of output containing the fitted exponents for each depth in the same order as the depths listed above, as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). The results must be floats.\n\nConstraints and clarifications:\n- You must compute $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$ exactly for the constructed linear network by explicitly forming the gradient $\\nabla_{x} L_{\\text{loss}} = (W^{\\top})^{L} u$ and taking its Euclidean norm. Do not use automatic differentiation libraries.\n- You must use the specified spectral normalization procedure $W = \\alpha \\,\\bar{W}/\\|\\bar{W}\\|_{2}$, and you must use $u$ as the dominant eigenvector of $\\bar{W}$ normalized to unit length.\n- The final program must not require any input and must use the fixed seed $s$ for reproducibility.",
            "solution": "The problem statement has been validated and is determined to be sound. It is scientifically grounded in the principles of linear algebra and vector calculus as applied to deep learning, is well-posed with a complete and consistent set of parameters, and is objective in its formulation. The task is a well-designed computational experiment to empirically study the exploding gradient problem in a controlled setting.\n\nThe experiment is designed to reveal the relationship between the spectral radius of the weight matrix and the magnitude of the gradient with respect to the input. We will first derive the theoretical expectation from first principles and then describe the implementation of the computational verification.\n\n### Theoretical Foundation\n\n1.  **Model and Gradient Calculation**:\n    The deep linear network is defined by the function $f(x) = y = W^L x$, where $W \\in \\mathbb{R}^{d \\times d}$ is a weight matrix shared across $L$ layers, and $x \\in \\mathbb{R}^d$ is the input. The loss is given by a linear function of the output: $L_{\\text{loss}} = u^{\\top} y = u^{\\top} W^L x$. The loss is a scalar function of the vector $x$. The gradient of the loss with respect to the input, $\\nabla_x L_{\\text{loss}}$, can be found using the chain rule. Recognizing that $L_{\\text{loss}}$ is of the form $c^{\\top}x$ where the vector $c = (W^L)^{\\top} u$, the gradient is simply $\\nabla_x L_{\\text{loss}} = c$.\n    Therefore, the gradient is given by:\n    $$\n    \\nabla_x L_{\\text{loss}} = (W^L)^{\\top} u = (W^{\\top})^L u\n    $$\n\n2.  **Properties of the Weight Matrix $W$**:\n    The weight matrix $W$ is constructed via spectral normalization. First, a matrix $\\bar{W} = A A^{\\top}$ is created from a random matrix $A$ with entries drawn from a standard normal distribution.\n    *   $\\bar{W}$ is symmetric, as $(\\bar{W})^{\\top} = (A A^{\\top})^{\\top} = (A^{\\top})^{\\top} A^{\\top} = A A^{\\top} = \\bar{W}$.\n    *   $\\bar{W}$ is positive semidefinite, since for any vector $v \\in \\mathbb{R}^d$, $v^{\\top}\\bar{W}v = v^{\\top}A A^{\\top}v = (A^{\\top}v)^{\\top}(A^{\\top}v) = \\|A^{\\top}v\\|_2^2 \\ge 0$.\n    The final weight matrix is $W = \\alpha \\frac{\\bar{W}}{\\|\\bar{W}\\|_2}$, where $\\alpha  0$ is the target spectral radius (operator $2$-norm). Since $\\bar{W}$ is symmetric, $W$ is also symmetric, which implies $W^{\\top} = W$. This simplifies the gradient expression to:\n    $$\n    \\nabla_x L_{\\text{loss}} = W^L u\n    $$\n    The operator $2$-norm of $W$ is:\n    $$\n    \\|W\\|_2 = \\left\\| \\alpha \\frac{\\bar{W}}{\\|\\bar{W}\\|_2} \\right\\|_2 = \\alpha \\frac{\\|\\bar{W}\\|_2}{\\|\\bar{W}\\|_2} = \\alpha\n    $$\n    This confirms the construction correctly sets the spectral radius of $W$ to $\\alpha$.\n\n3.  **Worst-Case Alignment via Eigenvector Selection**:\n    The problem specifies that the readout vector $u$ must be the unit-norm dominant eigenvector of $\\bar{W}$. Since $\\bar{W}$ is symmetric positive semidefinite, its operator $2$-norm $\\|\\bar{W}\\|_2$ is equal to its largest eigenvalue, $\\lambda_{\\text{max}}(\\bar{W})$. The dominant eigenvector $u$ is the eigenvector corresponding to this eigenvalue, so $\\bar{W}u = \\lambda_{\\text{max}}(\\bar{W})u = \\|\\bar{W}\\|_2 u$.\n\n    This choice of $u$ makes it a dominant eigenvector for $W$ as well:\n    $$\n    Wu = \\left( \\alpha \\frac{\\bar{W}}{\\|\\bar{W}\\|_2} \\right) u = \\frac{\\alpha}{\\|\\bar{W}\\|_2} (\\bar{W}u) = \\frac{\\alpha}{\\|\\bar{W}\\|_2} (\\|\\bar{W}\\|_2 u) = \\alpha u\n    $$\n    Thus, $u$ is an eigenvector of $W$ with eigenvalue $\\alpha$.\n\n4.  **Gradient Norm Scaling**:\n    We can now calculate the repeated application of $W$ to $u$:\n    $$\n    W^L u = W^{L-1}(Wu) = W^{L-1}(\\alpha u) = \\alpha W^{L-1}u = \\dots = \\alpha^L u\n    $$\n    The norm of the gradient is then:\n    $$\n    \\|\\nabla_x L_{\\text{loss}}\\|_2 = \\|W^L u\\|_2 = \\|\\alpha^L u\\|_2 = |\\alpha|^L \\|u\\|_2\n    $$\n    Since $\\alpha  0$ and $u$ is a unit-norm vector ($\\|u\\|_2 = 1$), the expression simplifies to an exact equality:\n    $$\n    \\|\\nabla_x L_{\\text{loss}}\\|_2 = \\alpha^L\n    $$\n    This theoretical result predicts that the norm of the gradient scales as the $L$-th power of the spectral radius $\\alpha$.\n\n5.  **Empirical Estimation of the Exponent**:\n    The experiment aims to fit the measured data to the model $\\|\\nabla_x L_{\\text{loss}}\\|_2 \\approx C \\alpha^p$. By taking the natural logarithm of both sides, we get a linear relationship:\n    $$\n    \\ln(\\|\\nabla_x L_{\\text{loss}}\\|_2) \\approx \\ln(C) + p \\ln(\\alpha)\n    $$\n    This corresponds to a line with slope $p$ and intercept $\\ln(C)$. Based on our theoretical derivation, we expect the data to perfectly fit this model with $p = L$ and $C=1$ (i.e., $\\ln(C)=0$). The program will perform a linear regression on the pairs $(\\ln(\\alpha), \\ln(\\|\\nabla_x L_{\\text{loss}}\\|_2))$ for each depth $L$ to estimate the slope $p$.\n\n### Algorithmic Design\n\nThe Python script implements the described experiment as follows:\n1.  **Initialization**: Set fixed parameters $d=8$, random seed $s=12345$, and the lists of depths $L$ and spectral radii $\\alpha$ to be tested.\n2.  **Matrix Generation**: Use the NumPy library with the specified seed to generate a random matrix $A$ and construct the symmetric positive semidefinite matrix $\\bar{W} = AA^{\\top}$.\n3.  **Eigendecomposition**: Compute the eigenvalues and eigenvectors of $\\bar{W}$ using `numpy.linalg.eigh`, which is optimized for symmetric matrices. The largest eigenvalue gives $\\|\\bar{W}\\|_2$, and the corresponding eigenvector is the required readout vector $u$.\n4.  **Iterative Experiment**:\n    - Loop through each depth $L \\in \\{1, 3, 5, 7\\}$.\n    - Within this loop, iterate through each spectral radius $\\alpha \\in \\{0.25, 0.5, 1.0, 1.2, 1.5\\}$.\n    - For each pair $(L, \\alpha)$:\n        a. Construct the weight matrix $W = \\alpha \\frac{\\bar{W}}{\\|\\bar{W}\\|_2}$.\n        b. Compute the gradient vector $g = W^L u$ by iteratively applying $W$ to $u$ for $L$ times.\n        c. Calculate the Euclidean norm $\\|g\\|_2$.\n        d. Store the log-transformed values, $\\ln(\\alpha)$ and $\\ln(\\|g\\|_2)$.\n5.  **Exponent Estimation**: After collecting data for all $\\alpha$ values at a fixed $L$, use `scipy.stats.linregress` to perform ordinary least squares on the collected log-transformed data. The slope of the resulting regression line is the estimated exponent $p$.\n6.  **Output**: Collect the estimated exponents for each depth $L$ and print them in the specified format `[p1,p3,p5,p7]`. The results are expected to be very close to the theoretical values of $[1.0, 3.0, 5.0, 7.0]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef solve():\n    \"\"\"\n    Implements a controlled experiment to investigate the exploding gradient\n    phenomenon in a deep linear network, grounded in first principles.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    d = 8\n    s = 12345\n    depths = [1, 3, 5, 7]\n    alphas = [0.25, 0.5, 1.0, 1.2, 1.5]\n\n    # Set the random seed for deterministic and reproducible results.\n    np.random.seed(s)\n\n    # Construct the positive semidefinite matrix W_bar = A * A^T.\n    # This matrix is guaranteed to be symmetric positive semidefinite.\n    A = np.random.randn(d, d)\n    W_bar = A @ A.T\n\n    # Compute eigenvalues and eigenvectors of the symmetric matrix W_bar.\n    # np.linalg.eigh is used for Hermitian/symmetric matrices. It returns\n    # eigenvalues in ascending order and corresponding orthonormal eigenvectors.\n    eigenvalues, eigenvectors = np.linalg.eigh(W_bar)\n\n    # The operator 2-norm of a symmetric PSD matrix is its largest eigenvalue.\n    norm_W_bar = eigenvalues[-1]\n\n    # The readout vector u is the unit-norm dominant eigenvector of W_bar.\n    # This corresponds to the eigenvector for the largest eigenvalue.\n    u = eigenvectors[:, -1]\n\n    results = []\n    # Iterate over each specified depth L.\n    for L in depths:\n        log_alphas = []\n        log_grad_norms = []\n\n        # For each depth, iterate over the range of target spectral radii alpha.\n        for alpha in alphas:\n            # Construct the weight matrix W with spectral radius alpha.\n            # W = alpha * W_bar / ||W_bar||_2\n            W = alpha * W_bar / norm_W_bar\n\n            # Since W is symmetric, the gradient grad(L_loss) w.r.t. x is W^L * u.\n            # We compute this iteratively for numerical stability and efficiency.\n            grad = u\n            for _ in range(L):\n                grad = W @ grad\n\n            # Compute the L2 norm of the resulting gradient vector.\n            grad_norm = np.linalg.norm(grad)\n\n            # Store the natural logarithms for regression analysis.\n            # We fit log(grad_norm) = p * log(alpha) + log(C).\n            log_alphas.append(np.log(alpha))\n            log_grad_norms.append(np.log(grad_norm))\n\n        # Perform ordinary least squares regression in log-space.\n        # The slope of the regression is the desired exponent p.\n        regression_result = linregress(x=log_alphas, y=log_grad_norms)\n        exponent_p = regression_result.slope\n        results.append(exponent_p)\n    \n    # As per the problem, the theoretical expectation is that p = L.\n    # The printed results should be very close to [1.0, 3.0, 5.0, 7.0].\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "The exploding gradient problem is not just a theoretical curiosity found in simple linear networks; it remains a crucial consideration in modern architectures like the Transformer. In this practice, you will investigate how instability can arise from the self-attention mechanism itself. By manipulating a scaling factor $c$ applied to the attention logits, you will discover a critical threshold where gradient norms increase sharply, providing insight into why practical implementations include stabilizing factors like the famous $1/\\sqrt{d_k}$ scaling .",
            "id": "3185054",
            "problem": "You are asked to write a complete, runnable program that demonstrates the exploding gradient phenomenon in a toy multi-layer self-attention network by scaling the attention logits by a factor $c$. The objective is to compute, for several test configurations, the smallest scaling $c^{\\ast}$ beyond which the Euclidean norm of the gradient with respect to the input embeddings rises sharply, according to a predefined multiplicative threshold relative to a baseline.\n\nFoundational base and forward definition. Use only the chain rule of calculus and the standard definition of the softmax function. Consider a single-head self-attention block applied $L$ times in sequence with shared weights to isolate the role of the scaling factor $c$. Let the sequence length be $T=4$ and the model dimension be $d=6$ (so the key dimension is $d_k=d$). For an input matrix $X \\in \\mathbb{R}^{T \\times d}$, a single self-attention block computes:\n- $Q = X W_Q$, $K = X W_K$, $V = X W_V$, where $W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}$,\n- logits $M = \\frac{Q K^{\\top}}{\\sqrt{d_k}}$, scaled logits $L = c \\, M$,\n- row-wise softmax $A = \\mathrm{softmax}(L)$,\n- pre-output $P = A V$, and output $Y = P W_O$ with $W_O \\in \\mathbb{R}^{d \\times d}$.\n\nStack $L$ such blocks in series with shared weights $W_Q, W_K, W_V, W_O$; that is, $X_{0}$ is the input, and $X_{\\ell+1} = \\mathrm{Attn}(X_{\\ell}; c)$ for $\\ell \\in \\{0,\\dots,L-1\\}$, where $\\mathrm{Attn}(\\cdot;c)$ denotes the mapping defined above. Define the scalar loss\n$$\n\\mathcal{L} = \\tfrac{1}{2} \\cdot \\mathrm{mean}\\left( X_{L} \\odot X_{L} \\right),\n$$\nthat is, one half of the mean of the squared entries of the final layer output $X_{L}$. Your program must compute the Euclidean norm $\\lVert \\nabla_{X_0} \\mathcal{L} \\rVert_2$ exactly via backpropagation using the chain rule and the Jacobian of the softmax, for various values of $c$.\n\nWeights, inputs, and reproducibility. To ensure determinism, initialize $W_Q, W_K, W_V, W_O$ by sampling independent standard normal entries and multiplying by a scale $\\sigma_w$ (the weight scale), and initialize the input $X_0$ similarly with independent standard normal entries times a fixed input scale $\\sigma_x = 0.1$. Use a specified integer random seed to initialize both the weights and the input. All layers share the same weights.\n\nCritical scaling detection. For a given configuration, define a baseline scaling $c_0 = 1.0$. Let $g(c) = \\lVert \\nabla_{X_0} \\mathcal{L} \\rVert_2$ computed at scaling $c$. Given a threshold multiplier $m  1$, define the critical scaling $c^{\\ast}$ as the smallest value of $c$ on a uniform grid in $[c_{\\min}, c_{\\max}]$ with step $c_{\\mathrm{step}}$ such that\n$$\ng(c) \\ge m \\cdot g(c_0).\n$$\nIf no such $c$ exists on the grid, return $-1.0$ for that configuration.\n\nAngle units do not apply. There are no physical units. All numerical outputs must be real numbers.\n\nTest suite to implement. Your program must run the following test cases, each specified as a tuple $(\\text{seed}, L, \\sigma_w, m, c_{\\min}, c_{\\max}, c_{\\mathrm{step}})$:\n- Test $1$ (happy path): $(0, 3, 0.02, 10.0, 0.5, 10.0, 0.01)$.\n- Test $2$ (boundary, likely not found): $(1, 3, 0.02, 10.0, 0.5, 2.0, 0.01)$.\n- Test $3$ (deeper stack): $(2, 4, 0.02, 50.0, 0.5, 10.0, 0.01)$.\n- Test $4$ (extra-deep, smaller weights): $(3, 5, 0.005, 100.0, 0.5, 10.0, 0.01)$.\n\nProgram requirements and output format. Your program must:\n- Implement exact backpropagation through the stack using the chain rule and the softmax Jacobian,\n- For each test case, compute $g(c_0)$ at $c_0 = 1.0$, sweep $c$ on the specified grid, and detect $c^{\\ast}$ using the rule above,\n- Aggregate the $c^{\\ast}$ values of all test cases into a single line of output as a Python-style list of decimal numbers rounded to three digits after the decimal point, for example $[2.154,-1.0, \\dots]$. There must be no other output.\n\nScientific realism and guidance. The phenomenon should be explained by the accumulation of sensitivity factors across layers. In practical Transformers, attention logits are scaled by $1/\\sqrt{d_k}$ to stabilize both activations and gradients; here, $c$ modulates that effective scaling and can cause sharp increases in gradient norms when $c$ becomes sufficiently large relative to other factors. Your implementation should make no additional approximations beyond standard floating-point arithmetic and the exact softmax Jacobian. No user input or external files are allowed. The final results must be printed as a single line as specified above.",
            "solution": "The user has provided a valid problem statement. The task is to write a program that systematically investigates the exploding gradient phenomenon in a multi-layer self-attention network. This will be achieved by implementing exact backpropagation to compute the gradient of a scalar loss with respect to the input embeddings, $\\nabla_{X_0} \\mathcal{L}$. The magnitude of this gradient, quantified by its Euclidean norm, will be monitored as a scaling factor $c$ applied to the attention logits is varied.\n\nThe solution proceeds in four main stages: (1) defining the forward propagation of activations through the network, (2) deriving the backward propagation of gradients using the chain rule, (3) structuring the numerical experiment to find the critical scaling factor $c^*$, and (4) implementing the complete algorithm in Python.\n\n**1. Forward Propagation**\n\nThe network consists of $L$ identical self-attention blocks with shared weights. The input to the first block is $X_0 \\in \\mathbb{R}^{T \\times d}$, where $T=4$ is the sequence length and $d=6$ is the model dimension. For any layer $\\ell \\in \\{0, \\dots, L-1\\}$, the output $X_{\\ell+1}$ is computed from the input $X_{\\ell}$ as follows:\n\n-   **Query, Key, Value Projections**: The input $X_\\ell$ is linearly projected to form the Query ($Q$), Key ($K$), and Value ($V$) matrices.\n    $$Q = X_\\ell W_Q, \\quad K = X_\\ell W_K, \\quad V = X_\\ell W_V$$\n    Here, $W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}$ are the shared weight matrices.\n\n-   **Attention Scores and Scaling**: The dot-product attention scores are computed, scaled by $1/\\sqrt{d_k}$ (where $d_k=d=6$), and then further scaled by the experimental factor $c$.\n    $$M = \\frac{Q K^\\top}{\\sqrt{d_k}}$$\n    $$L_{\\text{scaled}} = c \\cdot M$$\n\n-   **Softmax and Output**: A row-wise softmax function is applied to the scaled logits to obtain the attention weights $A$. These weights are then used to form a weighted average of the value vectors. This result is projected by a final output weight matrix $W_O \\in \\mathbb{R}^{d \\times d}$.\n    $$A = \\mathrm{softmax}(L_{\\text{scaled}})$$\n    $$P = A V$$\n    $$X_{\\ell+1} = P W_O$$\n    The process is repeated $L$ times, yielding the final output $X_L$. Throughout this forward pass, intermediate values ($X_\\ell, Q, K, V, A$) for each layer are cached for use during backpropagation.\n\n**2. Backward Propagation (Backpropagation)**\n\nThe objective is to compute $\\nabla_{X_0} \\mathcal{L}$, the gradient of the loss $\\mathcal{L}$ with respect to the initial input $X_0$. The loss is defined as:\n$$\\mathcal{L} = \\frac{1}{2} \\mathrm{mean}(X_L \\odot X_L) = \\frac{1}{2Td} \\sum_{i,j} (X_L)_{ij}^2$$\nThe gradient computation starts from the loss and proceeds backward through the network using the chain rule.\n\n-   **Initial Gradient**: The gradient of the loss with respect to the final output $X_L$ is:\n    $$\\nabla_{X_L} \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial X_L} = \\frac{1}{Td} X_L$$\n\n-   **Backpropagation Through a Single Layer**: We need to compute $\\nabla_{X_{\\ell-1}} \\mathcal{L}$ given $\\nabla_{X_{\\ell}} \\mathcal{L}$. Let $\\nabla_Z$ denote $\\nabla_Z \\mathcal{L}$. The steps for backpropagating through the operations of a single attention block (in reverse order) are as follows:\n\n    1.  Given $\\nabla_{X_\\ell}$: From $X_\\ell = P W_O$, we find the gradient with respect to $P$:\n        $$\\nabla_P = \\nabla_{X_\\ell} W_O^\\top$$\n    2.  From $P = A V$, we find gradients with respect to $A$ and $V$:\n        $$\\nabla_A = \\nabla_P V^\\top$$\n        $$\\nabla_V = A^\\top \\nabla_P$$\n    3.  From $A = \\mathrm{softmax}(L_{\\text{scaled}})$, we find $\\nabla_{L_{\\text{scaled}}}$. The Jacobian of a row-wise softmax, $S_i = \\mathrm{softmax}(\\mathbf{z}_i)$, has elements $\\frac{\\partial S_{ik}}{\\partial z_{ij}} = S_{ik}(\\delta_{kj} - S_{ij})$. Applying the chain rule for each row $i$ yields $(\\nabla_{\\mathbf{z}})_i = S_i \\odot (\\nabla_S)_i - S_i ((\\nabla_S)_i \\cdot S_i)$. In matrix form, this is:\n        $$\\nabla_{L_{\\text{scaled}}} = A \\odot (\\nabla_A - \\mathrm{sum}(\\nabla_A \\odot A, \\text{axis}=1))$$\n        where $\\odot$ is element-wise multiplication and the sum is broadcast across rows.\n    4.  From $L_{\\text{scaled}} = c \\cdot M$:\n        $$\\nabla_M = c \\cdot \\nabla_{L_{\\text{scaled}}}$$\n    5.  From $M = \\frac{Q K^\\top}{\\sqrt{d_k}}$:\n        $$\\nabla_Q = \\frac{1}{\\sqrt{d_k}} \\nabla_M K$$\n        $$\\nabla_K = \\frac{1}{\\sqrt{d_k}} \\nabla_M^\\top Q$$\n    6.  From $Q=X_{\\ell-1}W_Q$, $K=X_{\\ell-1}W_K$, $V=X_{\\ell-1}W_V$, the gradient contributions to $X_{\\ell-1}$ are summed:\n        $$\\nabla_{X_{\\ell-1}} = (\\nabla_Q W_Q^\\top) + (\\nabla_K W_K^\\top) + (\\nabla_V W_V^\\top)$$\n\nThis process is iterated backward from $\\ell=L$ to $\\ell=1$, ultimately yielding $\\nabla_{X_0} \\mathcal{L}$.\n\n**3. Algorithmic Procedure**\n\nFor each test case $(\\text{seed}, L, \\sigma_w, m, c_{\\min}, c_{\\max}, c_{\\mathrm{step}})$:\n\n1.  **Initialization**: The random number generator is seeded. The weight matrices $W_Q, W_K, W_V, W_O$ and the initial input $X_0$ are initialized according to the specified standard deviation scales ($\\sigma_w$ and $\\sigma_x=0.1$).\n2.  **Baseline Calculation**: The function $g(c) = \\lVert \\nabla_{X_0} \\mathcal{L} \\rVert_2$ is computed for the baseline scaling factor $c_0=1.0$. This yields the baseline gradient norm $g(c_0)$.\n3.  **Threshold Search**: A threshold value $g_{\\text{thresh}} = m \\cdot g(c_0)$ is established. The program then iterates through a uniform grid of $c$ values from $c_{\\min}$ to $c_{\\max}$ with an increment of $c_{\\mathrm{step}}$.\n4.  **Critical Scaling Detection**: For each $c$ in the grid, $g(c)$ is computed. The first value of $c$ for which $g(c) \\ge g_{\\text{thresh}}$ is identified as the critical scaling $c^*$.\n5.  **Result Handling**: If such a $c$ is found, $c^*$ is stored. If the loop completes without the condition being met, $c^*$ is set to $-1.0$.\n\nThis procedure is repeated for all test cases, and the resulting $c^*$ values are collected.\n\n**4. Implementation**\n\nThe algorithm is implemented in Python using the `numpy` library for all numerical and matrix operations. A single function, `compute_and_backprop`, encapsulates the logic for both the forward and backward passes for a given scaling factor $c$. The main `solve` function iterates through the test configurations, calls `compute_and_backprop` as needed to find $c^*$ for each, rounds the results to three decimal places, and formats them into the specified output string.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for exploding gradient detection.\n    \"\"\"\n    test_cases = [\n        # (seed, L, sigma_w, m, c_min, c_max, c_step)\n        (0, 3, 0.02, 10.0, 0.5, 10.0, 0.01),\n        (1, 3, 0.02, 10.0, 0.5, 2.0, 0.01),\n        (2, 4, 0.02, 50.0, 0.5, 10.0, 0.01),\n        (3, 5, 0.005, 100.0, 0.5, 10.0, 0.01),\n    ]\n\n    results = []\n    \n    T = 4  # Sequence length\n    d = 6  # Model dimension\n    d_k = d # Key dimension\n    sigma_x = 0.1 # Input scale\n\n    for case in test_cases:\n        seed, L, sigma_w, m, c_min, c_max, c_step = case\n\n        # Set seed for reproducibility\n        np.random.seed(seed)\n\n        # Initialize weights and input data\n        Wq = np.random.randn(d, d) * sigma_w\n        Wk = np.random.randn(d, d) * sigma_w\n        Wv = np.random.randn(d, d) * sigma_w\n        Wo = np.random.randn(d, d) * sigma_w\n        X0 = np.random.randn(T, d) * sigma_x\n\n        def get_grad_norm(c_val):\n            \"\"\"\n            Computes the norm of the gradient of the loss with respect to the input X0\n            for a given scaling factor c_val.\n            \"\"\"\n            # --- Forward Pass ---\n            cache = []\n            X_current = X0\n            sqrt_dk = np.sqrt(d_k)\n            \n            for _ in range(L):\n                Q = X_current @ Wq\n                K = X_current @ Wk\n                V = X_current @ Wv\n                \n                M = (Q @ K.T) / sqrt_dk\n                L_scaled = c_val * M\n                \n                # Numerically stable softmax\n                L_stable = L_scaled - np.max(L_scaled, axis=1, keepdims=True)\n                exp_L = np.exp(L_stable)\n                A = exp_L / np.sum(exp_L, axis=1, keepdims=True)\n                \n                P = A @ V\n                X_next = P @ Wo\n                \n                # Store intermediate values needed for backpropagation\n                cache.append((X_current, Q, K, V, A))\n                X_current = X_next\n                \n            X_L = X_current\n\n            # --- Backward Pass ---\n            # Initial gradient from the loss function\n            grad_X = (1.0 / (T * d)) * X_L\n\n            for i in range(L - 1, -1, -1):\n                X_prev, Q_i, K_i, V_i, A_i = cache[i]\n                \n                # Backprop through Wo\n                grad_P = grad_X @ Wo.T\n                \n                # Backprop through P = A @ V\n                grad_A = grad_P @ V_i.T\n                grad_V = A_i.T @ grad_P\n                \n                # Backprop through softmax(L_scaled)\n                # For each row, grad_z = s * (grad_s - sum(grad_s * s))\n                row_sum = np.sum(grad_A * A_i, axis=1, keepdims=True)\n                grad_L_scaled = A_i * (grad_A - row_sum)\n                \n                # Backprop through L_scaled = c * M\n                grad_M = c_val * grad_L_scaled\n\n                # Backprop through M = (Q @ K.T) / sqrt_dk\n                grad_Q = (grad_M @ K_i) / sqrt_dk\n                grad_K = (grad_M.T @ Q_i) / sqrt_dk\n                \n                # Backprop through Q, K, V to X_prev\n                # Sum gradients from the three paths\n                grad_X_prev = grad_Q @ Wq.T\n                grad_X_prev += grad_K @ Wk.T\n                grad_X_prev += grad_V @ Wv.T\n                \n                grad_X = grad_X_prev\n\n            grad_X0 = grad_X\n            return np.linalg.norm(grad_X0)\n\n        # Calculate baseline gradient norm\n        g_base = get_grad_norm(c_val=1.0)\n        threshold = m * g_base\n        \n        c_star = -1.0\n\n        # Create the grid for c values\n        num_steps = int(round((c_max - c_min) / c_step)) + 1\n        c_grid = np.linspace(c_min, c_max, num_steps)\n        \n        for c in c_grid:\n            g_c = get_grad_norm(c_val=c)\n            if g_c >= threshold:\n                c_star = c\n                break\n        \n        results.append(round(c_star, 3))\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}