{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp Stochastic Gradient Descent, we begin with its most fundamental operation: a single update step. This exercise simplifies the optimization landscape to a basic quadratic cost function, allowing you to focus purely on the mechanics of how SGD uses the gradient from a single data point to iteratively refine a parameter. By working through this calculation, you will build a solid foundation for understanding the core principle of SGD before moving to more complex scenarios. ",
            "id": "2206637",
            "problem": "An iterative optimization algorithm is used to find a parameter $x$ that minimizes a cost function. The total cost function is an average of several component functions: $F(x) = \\frac{1}{N}\\sum_{i=1}^{N} f_i(x)$. In this specific case, the component functions are quadratic and given by $f_i(x) = (x - c_i)^2$, where the constants are $c_i = i$ for $i = 1, 2, \\dots, 10$, and thus $N=10$.\n\nThe optimization process starts with an initial guess for the parameter, $x_0$. At each step, a new estimate, $x_{k+1}$, is calculated from the current estimate, $x_k$, by using only a single, randomly chosen component function, $f_j(x)$. The update rule is defined as:\n$$x_{k+1} = x_k - \\eta \\left( \\frac{d f_j(x)}{dx} \\bigg|_{x=x_k} \\right)$$\nwhere $\\eta$ is a constant known as the learning rate.\n\nGiven an initial parameter value of $x_0 = 10.0$ and a learning rate of $\\eta = 0.1$, compute the value of the parameter $x_1$ after one update step. For this first step, the component function used is $f_j(x)$ with the index $j=5$.",
            "solution": "We are given component functions of the form $f_{i}(x) = (x - c_{i})^{2}$ with $c_{i} = i$. For the first update, the chosen index is $j=5$, so $f_{5}(x) = (x - 5)^{2}$.\n\nThe update rule is\n$$\nx_{k+1} = x_{k} - \\eta \\left.\\frac{d f_{j}(x)}{dx}\\right|_{x=x_{k}}.\n$$\nUsing the power rule and chain rule, the derivative of the chosen component is\n$$\n\\frac{d f_{5}(x)}{dx} = \\frac{d}{dx}\\left[(x - 5)^{2}\\right] = 2(x - 5).\n$$\nEvaluating at the current iterate $x_{0} = 10$ gives\n$$\n\\left.\\frac{d f_{5}(x)}{dx}\\right|_{x=10} = 2(10 - 5) = 10.\n$$\nWith learning rate $\\eta = 0.1$, the update becomes\n$$\nx_{1} = x_{0} - \\eta \\cdot 10 = 10 - 0.1 \\times 10 = 10 - 1 = 9.\n$$\nThus, after one update step using $f_{5}$, the parameter value is $x_{1} = 9$.",
            "answer": "$$\\boxed{9}$$"
        },
        {
            "introduction": "Having mastered the basic update step, we now apply SGD to a scenario more typical of machine learning. This problem involves a non-linear model where the prediction is an exponential function of the model's weights, a common structure for modeling positive quantities like prices or counts. Deriving the update rule here requires careful application of the chain rule to compute the gradient, a crucial skill for adapting SGD to various custom models you might encounter in practice. ",
            "id": "2206657",
            "problem": "In a machine learning model for predicting a positive quantity, the predicted value $\\hat{y}$ is related to a $d$-dimensional feature vector $x$ by the formula $\\hat{y} = \\exp(w^T x)$, where $w$ is a $d$-dimensional weight vector. The goal is to adjust the weights $w$ to make the predictions as close as possible to the true observed values.\n\nThe performance of the model is measured by a loss function. For a dataset of $m$ samples $\\{(x_i, y_i)\\}_{i=1}^m$, the total loss is defined as the sum of squared differences between the model's prediction and the true value:\n$$\nL(w) = \\sum_{i=1}^{m} \\left( \\exp(w^T x_i) - y_i \\right)^2\n$$\nWe use Stochastic Gradient Descent (SGD), an iterative optimization algorithm, to minimize this loss. In each iteration, SGD approximates the total loss gradient using only a single data sample.\n\nSuppose the current weight vector is $w$. A single data sample $(x_k, y_k)$ is selected. Using this sample, a single update step of SGD is performed with a learning rate $\\eta  0$. Derive the expression for the updated weight vector, $w_{\\text{new}}$. Your final expression should be in terms of $w$, $\\eta$, $x_k$, and $y_k$.",
            "solution": "We consider the single-sample loss for the selected sample $(x_{k}, y_{k})$,\n$$\n\\ell_{k}(w) = \\left(\\exp\\left(w^{T} x_{k}\\right) - y_{k}\\right)^{2}.\n$$\nTo perform an SGD update, we need the gradient of $\\ell_{k}(w)$ with respect to $w$. Let $f(w) = \\exp(w^{T} x_{k}) - y_{k}$. Then $\\ell_{k}(w) = f(w)^{2}$, so by the chain rule,\n$$\n\\nabla_{w} \\ell_{k}(w) = 2 f(w)\\, \\nabla_{w} f(w).\n$$\nNext, compute $\\nabla_{w} f(w)$. Since $f(w) = \\exp(w^{T} x_{k}) - y_{k}$ and $\\nabla_{w}(w^{T} x_{k}) = x_{k}$, we have\n$$\n\\nabla_{w} f(w) = \\exp\\left(w^{T} x_{k}\\right) \\, x_{k}.\n$$\nTherefore,\n$$\n\\nabla_{w} \\ell_{k}(w) = 2 \\left(\\exp\\left(w^{T} x_{k}\\right) - y_{k}\\right) \\exp\\left(w^{T} x_{k}\\right) x_{k}.\n$$\nThe SGD update with learning rate $\\eta0$ is\n$$\nw_{\\text{new}} = w - \\eta \\nabla_{w} \\ell_{k}(w),\n$$\nwhich yields\n$$\nw_{\\text{new}} = w - 2 \\eta \\left(\\exp\\left(w^{T} x_{k}\\right) - y_{k}\\right) \\exp\\left(w^{T} x_{k}\\right) x_{k}.\n$$",
            "answer": "$$\\boxed{w - 2 \\eta \\left(\\exp\\left(w^{T} x_{k}\\right) - y_{k}\\right)\\exp\\left(w^{T} x_{k}\\right) x_{k}}$$"
        },
        {
            "introduction": "The power of SGD comes with a critical caveat: its performance is highly sensitive to the choice of the learning rate, $\\eta$. An improperly set learning rate can lead to slow convergence or, worse, cause the optimization process to become unstable and diverge. This exercise provides a clear, hands-on demonstration of this instability, showing how a learning rate that is too large can cause the parameter updates to overshoot the minimum and move further away with each step. ",
            "id": "2206673",
            "problem": "In a machine learning context, we often optimize a model's parameters by minimizing a loss function. Consider a simplified model with a single scalar parameter, $w$. The loss function associated with a single data point is given by $L(w) = \\frac{1}{2} c w^2$, where the minimum loss occurs at $w=0$. The parameter is updated using the Stochastic Gradient Descent (SGD) algorithm. The update rule for the parameter at step $k$ is given by $w_{k+1} = w_k - \\eta \\nabla L(w_k)$, where $\\nabla L(w_k)$ is the gradient of the loss function evaluated at $w_k$, and $\\eta$ is the learning rate.\n\nSuppose the initial value of the parameter is $w_0 = 4.0$. The model parameters are set as $c = 0.75$ and the learning rate is $\\eta = 3.2$. Calculate the value of the parameter $w$ after 3 update steps (i.e., find $w_3$).\n\nRound your final answer to three significant figures.",
            "solution": "The loss is $L(w)=\\frac{1}{2} c w^{2}$. Its gradient is obtained by differentiation:\n$$\n\\nabla L(w)=\\frac{\\mathrm{d}}{\\mathrm{d}w}\\left(\\frac{1}{2} c w^{2}\\right)=c w.\n$$\nThe SGD update rule is\n$$\nw_{k+1}=w_{k}-\\eta \\nabla L(w_{k})=w_{k}-\\eta c w_{k}=(1-\\eta c)\\,w_{k}.\n$$\nThis linear recurrence solves to\n$$\nw_{k}=(1-\\eta c)^{k} w_{0}.\n$$\nSubstituting $c=0.75$, $\\eta=3.2$, and $w_{0}=4.0$,\n$$\n1-\\eta c=1-(3.2)(0.75)=1-2.4=-1.4,\n$$\nso\n$$\nw_{3}=(-1.4)^{3}\\cdot 4.0=-10.976.\n$$\nRounding to three significant figures gives $-11.0$.",
            "answer": "$$\\boxed{-11.0}$$"
        }
    ]
}