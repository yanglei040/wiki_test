{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最好的检验方式是动手实践。第一个练习 () 将引导你手动完成 AdamW 优化器的一次完整更新步骤。通过这个具体的计算过程，你将清晰地看到一阶和二阶矩、偏差校正以及解耦权重衰减项是如何协同工作，共同决定参数的最终更新量。",
            "id": "3096505",
            "problem": "考虑一个单个标量参数 $w$，使用基于梯度的学习方法对其进行优化，以最小化一个可微损失函数 $L(w)$。在随机梯度下降 (SGD) 中，梯度步长与瞬时梯度 $g_t = \\frac{dL}{dw}\\big|_{t}$ 成正比。解耦权重衰减的自适应矩估计 (AdamW) 优化器维护过去梯度和梯度平方的指数移动平均值。设一阶矩和二阶矩分别由指数移动平均递归式 $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$ 和 $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$ 定义，其中 $m_0 = 0$ 且 $v_0 = 0$。为了校正这些移动平均值中固有的初始化偏差，定义偏差校正后的估计值 $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^{t}}$ 和 $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^{t}}$。在 AdamW 中，解耦权重衰减原则规定，一个与当前参数值成正比的收缩项独立于梯度归一化进行应用，从而产生一个加性更新分量，该分量与 $w_t$ 和权重衰减系数成线性比例。\n\n给定以下数值：初始参数 $w_0 = 2.0$，第一步的梯度 $g_1 = 0.3$，学习率 $\\alpha = 1.0 \\times 10^{-3}$，指数衰减率 $\\beta_1 = 0.9$ 和 $\\beta_2 = 0.999$，数值稳定器 $\\varepsilon = 1.0 \\times 10^{-8}$，以及权重衰减系数 $\\lambda = 0.01$。仅使用上述关于指数移动平均、其偏差校正和解耦权重衰减原则的定义，推导由 AdamW 优化器在 $t=1$ 时应用于 $w_0$ 的第一步有符号变化量 $\\Delta w_1$。你的推导必须明确说明量 $\\hat{m}_1 = \\frac{m_1}{1 - \\beta_1}$ 和 $\\hat{v}_1 = \\frac{v_1}{1 - \\beta_2}$ 是如何确定归一化梯度步长的大小的。\n\n计算单个实数 $\\Delta w_1$，并将其作为最终答案。将你的答案四舍五入到四位有效数字。",
            "solution": "该问题提法明确且有科学依据，为计算由 AdamW 算法优化的标量参数 $w$ 的第一步变化量 $\\Delta w_1$ 提供了一整套完整的定义和参数。\n\n带有解耦权重衰减的 AdamW 优化器更新规则由在第 $t$ 步应用于参数 $w_{t-1}$ 的变化量 $\\Delta w_t$ 定义：\n$$ \\Delta w_t = w_t - w_{t-1} = -\\alpha \\left( \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon} + \\lambda w_{t-1} \\right) $$\n其中 $\\alpha$ 是学习率，$\\lambda$ 是权重衰减系数，$\\varepsilon$ 是用于数值稳定性的一个小常数，$\\hat{m}_t$ 和 $\\hat{v}_t$ 分别是偏差校正后的一阶矩和二阶矩估计值。参数 $w_{t-1}$ 是更新步骤 $t$ 开始时的参数值。我们被要求计算第一步（$t=1$）的这个变化量，记作 $\\Delta w_1$。\n\n给定的值为：\n初始参数：$w_0 = 2.0$\n第 1 步的梯度：$g_1 = 0.3$\n学习率：$\\alpha = 1.0 \\times 10^{-3}$\n权重衰减系数：$\\lambda = 0.01$\n一阶矩的指数衰减率：$\\beta_1 = 0.9$\n二阶矩的指数衰减率：$\\beta_2 = 0.999$\n初始一阶矩：$m_0 = 0$\n初始二阶矩：$v_0 = 0$\n数值稳定器：$\\varepsilon = 1.0 \\times 10^{-8}$\n\n计算按以下步骤进行：\n\n首先，我们计算第 $t=1$ 步的一阶矩和二阶矩，$m_1$ 和 $v_1$。\n一阶矩 $m_t$ 是梯度的指数移动平均，由递归式 $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$ 定义。对于 $t=1$：\n$$ m_1 = \\beta_1 m_0 + (1 - \\beta_1) g_1 $$\n代入给定值：\n$$ m_1 = (0.9)(0) + (1 - 0.9)(0.3) = (0.1)(0.3) = 0.03 $$\n\n二阶矩 $v_t$ 是梯度平方的指数移动平均，由 $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$ 定义。对于 $t=1$：\n$$ v_1 = \\beta_2 v_0 + (1 - \\beta_2) g_1^2 $$\n代入给定值：\n$$ v_1 = (0.999)(0) + (1 - 0.999)(0.3)^2 = (0.001)(0.09) = 0.00009 $$\n\n其次，我们计算偏差校正后的矩估计值 $\\hat{m}_1$ 和 $\\hat{v}_1$。这些校正是因为移动平均值初始化为零。\n偏差校正后的一阶矩为 $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$。对于 $t=1$：\n$$ \\hat{m}_1 = \\frac{m_1}{1 - \\beta_1^1} = \\frac{m_1}{1 - \\beta_1} $$\n$$ \\hat{m}_1 = \\frac{0.03}{1 - 0.9} = \\frac{0.03}{0.1} = 0.3 $$\n作为检验，我们注意到对于第一步，$\\hat{m}_1 = \\frac{(1 - \\beta_1) g_1}{1 - \\beta_1} = g_1 = 0.3$。\n\n偏差校正后的二阶矩为 $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$。对于 $t=1$：\n$$ \\hat{v}_1 = \\frac{v_1}{1 - \\beta_2^1} = \\frac{v_1}{1 - \\beta_2} $$\n$$ \\hat{v}_1 = \\frac{0.00009}{1 - 0.999} = \\frac{0.00009}{0.001} = 0.09 $$\n类似地，对于第一步，$\\hat{v}_1 = \\frac{(1 - \\beta_2) g_1^2}{1 - \\beta_2} = g_1^2 = (0.3)^2 = 0.09$。\n\n第三，我们使用 $t=1$ 时的 AdamW 更新规则计算总变化量 $\\Delta w_1$：\n$$ \\Delta w_1 = -\\alpha \\left( \\frac{\\hat{m}_1}{\\sqrt{\\hat{v}_1} + \\varepsilon} + \\lambda w_0 \\right) $$\n代入计算出的值和给定的值：\n$$ \\Delta w_1 = -(1.0 \\times 10^{-3}) \\left( \\frac{0.3}{\\sqrt{0.09} + 1.0 \\times 10^{-8}} + (0.01)(2.0) \\right) $$\n\n现在，我们逐项计算表达式：\n偏差校正后的二阶矩的平方根是：\n$$ \\sqrt{\\hat{v}_1} = \\sqrt{0.09} = 0.3 $$\n自适应梯度项是：\n$$ \\frac{\\hat{m}_1}{\\sqrt{\\hat{v}_1} + \\varepsilon} = \\frac{0.3}{0.3 + 1.0 \\times 10^{-8}} = \\frac{0.3}{0.30000001} \\approx 0.9999999667 $$\n解耦权重衰减项是：\n$$ \\lambda w_0 = (0.01)(2.0) = 0.02 $$\n括号内的和是：\n$$ 0.9999999667 + 0.02 = 1.0199999667 $$\n最后，我们乘以负的学习率：\n$$ \\Delta w_1 = -(1.0 \\times 10^{-3}) (1.0199999667) = -0.0010199999667 $$\n\n问题要求将最终答案四舍五入到四位有效数字。第一个有效数字是第一个非零数字，即 $1$。前四位有效数字是 $1$、$0$、$1$、$9$。第五位有效数字是 $9$，大于或等于 $5$，所以我们将第四位有效数字向上取整。\n$$ -0.00101999... \\approx -0.001020 $$\n尾随的零是有效数字。",
            "answer": "$$\\boxed{-0.001020}$$"
        },
        {
            "introduction": "这个练习 () 将深入探讨 AdamW 产生的核心动机。通过在一个具有极端不同曲率的二次函数上进行仿真，你将亲眼观察到标准 Adam 优化器在病态问题上可能遇到的困难，并验证 AdamW 的解耦权重衰减如何提供关键的稳定作用，防止在陡峭方向上产生过大的更新步长。",
            "id": "3096484",
            "problem": "您的任务是设计并分析一个模拟，该模拟旨在分离并展示解耦权重衰减的自适应矩估计 (AdamW) 如何在二次目标函数中改变高曲率方向上的更新幅度。此练习必须从基本定义开始，并通过显式构造进行。考虑由以下二次型定义的可微函数\n$$\nf(\\mathbf{x}) \\;=\\; \\tfrac{1}{2}\\,\\mathbf{x}^\\top \\mathbf{H}\\,\\mathbf{x} \\;=\\; \\tfrac{1}{2}\\sum_{i=1}^{d} h_i\\,x_i^2,\n$$\n其中 $\\mathbf{H}=\\mathrm{diag}(h_1,\\dots,h_d)$ 是一个对角正定矩阵，且 $h_i>0$。其梯度为\n$$\n\\nabla f(\\mathbf{x}) \\;=\\; \\mathbf{H}\\,\\mathbf{x}.\n$$\n自适应矩估计 (Adam) 维护梯度和梯度平方的指数移动平均，并带有偏差修正和数值稳定器。AdamW 中的解耦权重衰减在每一步直接对参数应用收缩，而不是将其与 $f(\\mathbf{x})$ 的梯度耦合。您的程序必须：\n- 实现作用于 $f(\\mathbf{x})$ 的两个优化器：\n  - 自适应矩估计 (Adam)，其定义为使用对 $\\mathbf{g}_t=\\nabla f(\\mathbf{x}_t)$ 和 $\\mathbf{g}_t\\odot\\mathbf{g}_t$ 的指数移动平均并进行偏差修正，并使用由第一矩与经平方根处理的第二矩之比加上一个稳定化常数所给出的逐坐标步长来更新参数。\n  - 带解耦权重衰减的 Adam (AdamW)，它使用与 Adam 相同的自适应步长，并额外应用一个与当前参数矢量成比例的解耦参数收缩。\n- 从初始条件 $\\mathbf{x}_0$ 开始，将离散时间动态系统演化固定的步数 $T$。\n\n从基本实体出发：\n- 指数移动平均使用形式为“新值等于旧值和当前观测值的凸组合”的递归，这是随机信号处理中一个经过充分检验的事实。\n- 二次型函数的梯度通过 Hessian 矩阵 $\\mathbf{H}$ 与 $\\mathbf{x}$呈线性关系。\n- 偏差修正通过除以因子 $1-\\beta^t$ 来解决因指数移动平均从零开始而引入的初始偏差。\n\n定义以下可测量来捕捉在迭代 $t$ 时单个坐标 $i$ 上的“更新尺度”：\n$$\n\\Delta_t^{(i)} \\;=\\; x_{t+1}^{(i)} - x_t^{(i)}.\n$$\n对于每次运行，令 $i^\\star$ 为最大曲率的索引，即 $i^\\star \\in \\arg\\max_i h_i$。对于每个优化器，计算\n$$\nM \\;=\\; \\max_{1 \\le t \\le T} \\left|\\Delta_t^{(i^\\star)}\\right|.\n$$\n您的目标是数值上证明，对于病态曲率谱（即，$h_i$ 跨越几个数量级），AdamW 中的解耦权重衰减可以抵消在高曲率方向上更新幅度增大的趋势，尤其是在稳定器在分母中占主导地位且自适应步长部分地随梯度幅度缩放时。使用以下参数值测试套件：\n\n- 测试用例 $\\mathbf{A}$ (具有大曲率对比度和中等稳定器的理想情况):\n  - 维度 $d = 3$。\n  - 曲率 $\\mathbf{h} = [\\,1,\\;10^3,\\;10^6\\,]$。\n  - 初始点 $\\mathbf{x}_0 = [\\,1,\\;1,\\;1\\,]$。\n  - 学习率 $\\alpha = 10^{-3}$。\n  - 一阶矩系数 $\\beta_1 = 0.9$。\n  - 二阶矩系数 $\\beta_2 = 0.999$。\n  - 稳定器 $\\varepsilon = 10^{-2}$。\n  - 权重衰减 $\\lambda = 10^{-1}$。\n  - 步数 $T = 200$。\n\n- 测试用例 $\\mathbf{B}$ (边界情况：无衰减，应与 Adam 匹配):\n  - 与 $\\mathbf{A}$ 相同，除了权重衰减 $\\lambda = 0$。\n\n- 测试用例 $\\mathbf{C}$ (不同的谱，强调中等大的顶部曲率和更强的稳定器效应):\n  - 维度 $d = 3$。\n  - 曲率 $\\mathbf{h} = [\\,10^{-3},\\;1,\\;10^3\\,]$。\n  - 初始点 $\\mathbf{x}_0 = [\\,1,\\;1,\\;1\\,]$。\n  - 学习率 $\\alpha = 10^{-3}$。\n  - 一阶矩系数 $\\beta_1 = 0.9$。\n  - 二阶矩系数 $\\beta_2 = 0.999$。\n  - 稳定器 $\\varepsilon = 10^{-1}$。\n  - 权重衰减 $\\lambda = 5\\times 10^{-2}$。\n  - 步数 $T = 200$。\n\n对于每个测试用例，从相同的初始 $\\mathbf{x}_0$ 开始执行两次运行：一次使用 Adam，一次使用 AdamW。对于每次运行，计算 $M_{\\mathrm{Adam}}$ 和 $M_{\\mathrm{AdamW}}$ 作为在所有迭代中 $\\Delta_t^{(i^\\star)}$ 的最大幅度。每个测试用例所需的结果是浮点数比率\n$$\nR \\;=\\; \\frac{M_{\\mathrm{AdamW}}}{M_{\\mathrm{Adam}}}.\n$$\n您的程序应生成单行输出，其中包含 $\\mathbf{A}$、$\\mathbf{B}$ 和 $\\mathbf{C}$ 的三个比率，形式为用方括号括起来的逗号分隔列表，例如\n$$\n[\\,R_{\\mathbf{A}},R_{\\mathbf{B}},R_{\\mathbf{C}}\\,].\n$$\n不允许使用外部输入或文件。所有计算都是无单位的；不涉及物理单位或角度单位。数值输出必须严格为这一单行。",
            "solution": "该问题要求对自适应矩估计 (Adam) 优化器及其带解耦权重衰减的变体 (AdamW) 进行比较分析。该分析将通过在一个简单的二次目标函数上进行数值模拟来执行，该函数被选择具有一个具有挑战性的曲率谱。目标是数值上展示 AdamW 中的解耦权重衰减如何缓解在高曲率方向上出现大更新步长的问题，这种情况在某些条件下会出现在标准 Adam 中。\n\n首先，我们形式化优化器和模拟环境。\n\n目标函数是一个 $d$ 维二次型：\n$$\nf(\\mathbf{x}) = \\frac{1}{2}\\,\\mathbf{x}^\\top \\mathbf{H}\\,\\mathbf{x} = \\frac{1}{2}\\sum_{i=1}^{d} h_i\\,x_i^2\n$$\n其中 $\\mathbf{H} = \\mathrm{diag}(h_1, \\dots, h_d)$ 是一个对角矩阵，其对角线元素为正数 $h_i > 0$，代表沿每个坐标轴的曲率。该函数的梯度是线性的：\n$$\n\\mathbf{g}(\\mathbf{x}) = \\nabla f(\\mathbf{x}) = \\mathbf{H}\\,\\mathbf{x}\n$$\n优化器旨在找到 $f(\\mathbf{x})$ 的最小值，该最小值位于 $\\mathbf{x}=\\mathbf{0}$。我们将从一个初始点 $\\mathbf{x}_0$ 开始，模拟参数矢量 $\\mathbf{x}$ 在 $T$ 步内的离散时间演化。设 $\\mathbf{x}_t$ 为第 $t$ 步开始时的参数矢量（$\\mathbf{x}_0$ 为初始状态）。从 $\\mathbf{x}_{t-1}$ 到 $\\mathbf{x}_t$ 的更新在第 $t$ 步期间计算。\n\nAdam 和 AdamW 的核心都是使用梯度及其平方的指数移动平均。设 $\\mathbf{g}_t = \\mathbf{g}(\\mathbf{x}_{t-1})$ 为在第 $t$ 步计算的梯度。第一和第二矩矢量 $\\mathbf{m}_t$ 和 $\\mathbf{v}_t$ 更新如下：\n$$\n\\mathbf{m}_t = \\beta_1 \\mathbf{m}_{t-1} + (1-\\beta_1) \\mathbf{g}_t \\\\\n\\mathbf{v}_t = \\beta_2 \\mathbf{v}_{t-1} + (1-\\beta_2) (\\mathbf{g}_t \\odot \\mathbf{g}_t)\n$$\n其中 $\\odot$ 表示逐元素乘积。初始矩为 $\\mathbf{m}_0 = \\mathbf{0}$ 和 $\\mathbf{v}_0 = \\mathbf{0}$。为抵消初始化偏差，使用经过偏差修正的估计值：\n$$\n\\hat{\\mathbf{m}}_t = \\frac{\\mathbf{m}_t}{1-\\beta_1^t} \\\\\n\\hat{\\mathbf{v}}_t = \\frac{\\mathbf{v}_t}{1-\\beta_2^t}\n$$\n参数 $\\beta_1$ 和 $\\beta_2$ 是移动平均的指数衰减率。\n\n这两种优化器的更新规则如下：\n\n**1. Adam 优化器：**\n问题指定的是不带任何权重衰减的 Adam。其第 $t$ 步的更新规则是：\n$$\n\\mathbf{x}_t = \\mathbf{x}_{t-1} - \\alpha \\frac{\\hat{\\mathbf{m}}_t}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\varepsilon}\n$$\n这里，$\\alpha$ 是学习率，$\\varepsilon$ 是用于数值稳定性的一个小常数。\n\n**2. AdamW 优化器（带解耦权重衰减的 Adam）：**\nAdamW 通过将权重衰减与基于梯度的更新解耦来修改更新过程。权重衰减作为参数的收缩直接应用：\n$$\n\\mathbf{x}_t = \\mathbf{x}_{t-1} - \\alpha \\frac{\\hat{\\mathbf{m}}_t}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\varepsilon} - \\alpha \\lambda \\mathbf{x}_{t-1}\n$$\n其中 $\\lambda$ 是权重衰减系数。项 $- \\alpha \\lambda \\mathbf{x}_{t-1}$ 是解耦的权重衰减，它以与其大小成正比的速率将参数向零收缩，且独立于梯度信息。当 $\\lambda=0$ 时，AdamW 的更新规则与 Adam 的更新规则完全相同。\n\n对于每个测试用例和两种优化器，模拟将进行 $T$ 步。我们关注的可测量是每个坐标的更新幅度，即 $\\Delta_t^{(i)} = x_{t}^{(i)} - x_{t-1}^{(i)}$。我们感兴趣的是沿最高曲率方向（索引为 $i^\\star = \\arg\\max_i h_i$）的行为。对于每次优化运行，我们计算所有步骤中的最大更新幅度：\n$$\nM = \\max_{1 \\le t \\le T} \\left| \\Delta_t^{(i^\\star)} \\right|\n$$\n每个测试用例的最终结果是比率 $R = M_{\\mathrm{AdamW}} / M_{\\mathrm{Adam}}$。\n\n待检验的假设是，对于病态问题（$h_i$ 的范围很大），标准 Adam 可能会产生大的、不稳定的更新。如果自适应分母 $\\sqrt{\\hat{\\mathbf{v}}_t} + \\varepsilon$ 被稳定器 $\\varepsilon$ 主导，而分子 $\\hat{\\mathbf{m}}_t$ 仍然很大，就可能发生这种情况。AdamW 中的解耦衰减项提供了一个持续拉向原点的力，这可以稳定轨迹并减小这些更新的幅度。对于 $\\lambda=0$ 的测试用例 B，我们期望 $M_{\\mathrm{AdamW}} = M_{\\mathrm{Adam}}$，从而得出比率 $R=1$，这可作为对实现方案的合理性检查。对于其他 $\\lambda > 0$ 的情况，我们预计 $R  1$，这表明 AdamW 在高曲率轴上产生的最大更新较小。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test case A (happy path with large curvature contrast and moderate stabilizer)\n        {\n            'd': 3,\n            'h': np.array([1.0, 1e3, 1e6]),\n            'x0': np.array([1.0, 1.0, 1.0]),\n            'alpha': 1e-3,\n            'beta1': 0.9,\n            'beta2': 0.999,\n            'epsilon': 1e-2,\n            'lmbda': 1e-1,\n            'T': 200,\n        },\n        # Test case B (boundary case: no decay, should match Adam)\n        {\n            'd': 3,\n            'h': np.array([1.0, 1e3, 1e6]),\n            'x0': np.array([1.0, 1.0, 1.0]),\n            'alpha': 1e-3,\n            'beta1': 0.9,\n            'beta2': 0.999,\n            'epsilon': 1e-2,\n            'lmbda': 0.0,\n            'T': 200,\n        },\n        # Test case C (different spectrum emphasizing moderately large top curvature and stronger stabilizer effect)\n        {\n            'd': 3,\n            'h': np.array([1e-3, 1.0, 1e3]),\n            'x0': np.array([1.0, 1.0, 1.0]),\n            'alpha': 1e-3,\n            'beta1': 0.9,\n            'beta2': 0.999,\n            'epsilon': 1e-1,\n            'lmbda': 5e-2,\n            'T': 200,\n        },\n    ]\n\n    ratios = []\n\n    for case in test_cases:\n        # Adam run (lambda = 0)\n        adam_params = case.copy()\n        adam_params['lmbda'] = 0.0\n        M_adam = run_optimizer(adam_params)\n        \n        # AdamW run (lambda from test case)\n        M_adamw = run_optimizer(case)\n        \n        # Guard against division by zero, though not expected here\n        if M_adam == 0.0:\n            ratio = 0.0 if M_adamw == 0.0 else float('inf')\n        else:\n            ratio = M_adamw / M_adam\n        \n        ratios.append(ratio)\n\n    print(f\"[{','.join(f'{r:.7f}' for r in ratios)}]\")\n\n\ndef run_optimizer(params):\n    \"\"\"\n    Runs a single optimization trajectory for T steps.\n    \n    Args:\n        params (dict): A dictionary containing all hyperparameters for the run,\n                       including the weight decay 'lmbda'. If lmbda is 0, this\n                       simulates the Adam optimizer.\n    \n    Returns:\n        float: The maximum update magnitude along the direction of highest curvature.\n    \"\"\"\n    h = params['h']\n    x0 = params['x0']\n    alpha = params['alpha']\n    beta1 = params['beta1']\n    beta2 = params['beta2']\n    epsilon = params['epsilon']\n    lmbda = params['lmbda']\n    T = params['T']\n    \n    d = len(h)\n    x = np.copy(x0).astype(np.float64)\n    m = np.zeros(d, dtype=np.float64)\n    v = np.zeros(d, dtype=np.float64)\n    \n    H = np.diag(h)\n    istar = np.argmax(h)\n    \n    max_delta_magnitude = 0.0\n\n    # The loop for steps t = 1, 2, ..., T\n    for t in range(1, T + 1):\n        x_prev = np.copy(x)\n        \n        # Calculate gradient at x_{t-1}\n        grad = H @ x_prev\n        \n        # Update first and second moment estimates\n        m = beta1 * m + (1 - beta1) * grad\n        v = beta2 * v + (1 - beta2) * (grad**2)\n        \n        # Compute bias-corrected moment estimates\n        m_hat = m / (1 - beta1**t)\n        v_hat = v / (1 - beta2**t)\n        \n        # Compute the Adam-like update step\n        adaptive_step = alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n        \n        # Apply decoupled weight decay term\n        weight_decay_step = alpha * lmbda * x_prev\n        \n        # Update parameters to get x_t\n        x = x_prev - adaptive_step - weight_decay_step\n        \n        # Calculate update delta for this step\n        delta_t = x - x_prev\n        \n        # Track the maximum magnitude of the update for the highest curvature coordinate\n        current_delta_mag = np.abs(delta_t[istar])\n        if current_delta_mag > max_delta_magnitude:\n            max_delta_magnitude = current_delta_mag\n            \n    return max_delta_magnitude\n\nsolve()\n```"
        },
        {
            "introduction": "掌握了基本机理后，我们需要学习如何巧妙地应用它。本练习 () 将探讨一个重要且普遍的实践：在权重衰减中排除偏置（bias）参数。你将通过实验验证这一做法的有效性，并从贝叶斯先验的视角理解其背后的理论依据，从而体会到有效的正则化需要对模型中不同参数的角色进行深入思考。",
            "id": "3096544",
            "problem": "要求您实现并分析一个最小化的、完全确定性的实验，该实验旨在分离解耦权重衰减在卷积神经网络中对不同类型参数的影响。目标是在一个简单且可复现的环境中证明，当数据分布需要非零偏置时，将偏置参数排除在衰减之外（而卷积核包含在内）可以提高测试准确率，并通过涉及 $L_2$ 先验的贝叶斯解释来阐述这一观察结果。\n\n您必须编写一个完整的、可运行的程序，该程序：\n- 实现一个玩具（toy）一维卷积分类器，该分类器包含一个长度为 $k$ 的单一滤波器，以步长 $1$ 在有效（valid）模式下应用，然后进行平均化和 sigmoid 输出以进行二元分类。对于输入向量 $x \\in \\mathbb{R}^{L}$，设有效位置的数量为 $M = L - k + 1$。定义特征向量 $f(x) \\in \\mathbb{R}^{k}$ 如下：\n$$\nf_i(x) = \\frac{1}{M} \\sum_{j=0}^{M-1} x_{j+i}, \\quad \\text{对于 } i \\in \\{0,1,\\dots,k-1\\}.\n$$\n使用卷积核权重 $w \\in \\mathbb{R}^{k}$ 和标量偏置 $b \\in \\mathbb{R}$，对数单位（logit）为 $z(x) = w^\\top f(x) + b$，预测为 $\\sigma(z(x))$，其中 $\\sigma(u) = \\frac{1}{1 + e^{-u}}$。\n- 使用带有解耦权重衰减的自适应矩估计（AdamW）来训练模型，一次对 $w$ 和 $b$ 都应用衰减，另一次仅对 $w$ 应用衰减。使用二元交叉熵作为目标函数。您必须实现与 AdamW 一致的解耦权重衰减；不要将衰减耦合到数据梯度中。\n- 在同一测试集上评估两种训练方案的测试准确率，并报告其差异。\n\n数据集是合成的，必须按如下方式生成。对于指定的类别相关基线对 $(\\mu_0,\\mu_1)$、序列长度 $L$、噪声标准差 $\\sigma$ 和样本大小 $N$，通过抽样一个类别标签 $y \\in \\{0,1\\}$，设置基线 $m_y \\in \\{\\mu_0,\\mu_1\\}$，并抽取\n$$\nx = m_y \\cdot \\mathbf{1}_L + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_L),\n$$\n来生成输入 $x \\in \\mathbb{R}^{L}$，其中 $\\mathbf{1}_L$ 是 $\\mathbb{R}^{L}$ 中的全一向量。除非另有规定，标签 $y$ 是均匀随机抽取的。训练损失是训练集上二元交叉熵的经验均值。\n\n对所有情况使用单一的超参数配置进行训练，并在所有测试案例中完全相同地应用：\n- 学习率 $\\alpha$ 固定为您认为合理的正常数，并在所有测试案例中保持不变。\n- 一阶和二阶矩系数 $(\\beta_1,\\beta_2)$ 在所有测试案例中保持不变。\n- 训练轮数 $T$ 在所有测试案例中保持不变。\n- 数值稳定性参数 $\\varepsilon$ 在所有测试案例中保持不变。\n\n您必须构建四个测试案例，共同构成下面的测试套件。对于每个案例，您必须使用指定的种子生成独立的训练集和测试集。除了是否对偏置 $b$ 进行衰减外，保持训练和评估过程完全相同。\n\n测试套件规范：\n- 案例1（理想情况，非对称均值，中等衰减）：$(L,k,N_{\\text{train}},N_{\\text{test}}) = (16,3,256,512)$，$(\\mu_0,\\mu_1) = (0.2, 1.2)$，$\\sigma = 0.3$，权重衰减 $\\lambda = 0.05$，训练随机种子 $42$，测试随机种子 $1042$。\n- 案例2（边界情况，零衰减）：与案例1相同，除了 $\\lambda = 0.0$，训练随机种子 $43$，测试随机种子 $1043$。\n- 案例3（边缘情况，强衰减）：与案例1相同，除了 $\\lambda = 0.5$，训练随机种子 $44$，测试随机种子 $1044$。\n- 案例4（对照情况，关于零对称的均值）：$(L,k,N_{\\text{train}},N_{\\text{test}}) = (16,3,256,512)$，$(\\mu_0,\\mu_1) = (-0.6, 0.6)$，$\\sigma = 0.3$，权重衰减 $\\lambda = 0.1$，训练随机种子 $45$，测试随机种子 $1045$。\n\n需要计算和输出的内容：\n- 对于每个案例 $c \\in \\{1,2,3,4\\}$，训练两个模型：一个对 $w$ 和 $b$ 都进行衰减，另一个仅对 $w$ 进行衰减。在相应的测试集上评估两种方案的测试准确率，并计算差异\n$$\n\\Delta_c = \\text{Acc}_{\\text{no-bias-decay}} - \\text{Acc}_{\\text{with-bias-decay}}。\n$$\n- 您的程序必须生成单一行，包含列表 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$，其中每个值都四舍五入到小数点后恰好三位。\n\n科学基础要求：\n- 您在解决方案中的推理必须从二元交叉熵、逻辑函数以及 $L_2$ 惩罚对应于零均值高斯先验的最大后验（MAP）观点出发。不要在问题陈述中说明或依赖 AdamW 的任何更新公式；相反，应在代码中直接使用解耦权重衰减来实现它。\n\n最终输出格式：\n- 您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表的结果；例如：$[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$。",
            "solution": "该问题要求实现并分析解耦权重衰减，特别是在一个简单的神经网络中，其对权重和偏置的差异化应用。我们需要证明，当数据的底层分布需要一个非零的最优偏置时，将偏置参数从权重衰减中排除是有利的。\n\n分析从模型正则化的贝叶斯概率视角开始。在监督学习的背景下，我们的目标是找到能最好地解释数据 $D$ 的模型参数 $\\theta$。这通常被构建为最大后验（MAP）估计，我们寻求最大化给定数据下参数的后验概率 $P(\\theta|D)$。根据贝叶斯定理，这等同于最小化负对数后验：\n$$\n\\theta_{\\text{MAP}} = \\arg\\min_{\\theta} [-\\log P(D|\\theta) - \\log P(\\theta)]\n$$\n项 $-\\log P(D|\\theta)$ 是数据的负对数似然，它对应于损失函数。对于指定的二元分类任务，模型输出是一个概率 $p = \\sigma(z)$，其中 $\\sigma(\\cdot)$ 是逻辑 sigmoid 函数，$z$ 是对数单位（logit）。对于带有标签 $y \\in \\{0, 1\\}$ 的单个数据点 $(x, y)$，其似然由伯努利分布给出，$P(y|x, \\theta) = p^y(1-p)^{1-y}$。因此，单个样本的负对数似然是：\n$$\n-\\log P(y|x, \\theta) = -[y \\log(p) + (1-y)\\log(1-p)]\n$$\n这正是二元交叉熵损失函数。总的负对数似然是此损失在所有训练样本上的总和。\n\n第二项 $-\\log P(\\theta)$ 源自参数的先验分布。这个先验编码了我们在观察任何数据之前对参数的信念。一个常见的选择是假设参数可能很小且以零为中心。这种信念可以通过为每个参数 $\\theta_i$ 设置一个独立的、零均值的高斯先验来形式化：\n$$\nP(\\theta_i) \\propto \\exp\\left(-\\frac{\\lambda}{2}\\theta_i^2\\right)\n$$\n对于整个参数向量 $\\theta$，相应的负对数先验变为：\n$$\n-\\log P(\\theta) = \\frac{\\lambda}{2} \\sum_i \\theta_i^2 + \\text{const} = \\frac{\\lambda}{2} \\|\\theta\\|_2^2 + \\text{const}\n$$\n此项即为 $L_2$ 正则化惩罚。因此，最小化损失与该惩罚之和等同于使用零均值高斯先验进行 MAP 估计。\n\n在传统的梯度下降中，这个 $L_2$ 惩罚被加到损失函数中，其梯度 $\\lambda\\theta$ 被加到数据损失的梯度中。对于像 Adam 这样的自适应优化器，这种正则化梯度与数据梯度的“耦合”是有问题的，因为自适应缩放（来自矩项）会以一种复杂的、依赖于参数的方式重新调整正则化的效果。由 AdamW 推广的解耦权重衰减通过将权重衰减更新 `theta - theta - eta * lambda * theta` 与基于自适应梯度的更新分开应用，恢复了 $L_2$ 正则化的初衷。\n\n问题的核心在于批判性地评估零均值高斯先验对不同类型参数的有效性。对于权重（$w$），它们调节特征之间的相互作用，假设它们很小并以零为中心通常是一个合理的默认设置。然而，对于偏置参数（$b$），这个假设可能是有缺陷的。偏置项的功能是移动神经元的输出，有效地将其激活函数中心对准其输入的分布。因此，最优偏置直接依赖于它所接收的特征的均值。\n\n在这个问题中，输入数据生成为 $x = m_y \\cdot \\mathbf{1}_L + \\epsilon$，其中 $m_y$ 是类别条件的均值。特征向量 $f(x)$ 的构造使得其每个分量的期望值都是类别均值：$E[f_i(x)|y] = m_y$。对数单位（logit）为 $z(x) = w^\\top f(x) + b$。其期望是：\n$$\nE[z(x)|y] = w^\\top E[f(x)|y] + b = w^\\top (m_y \\mathbf{1}_k) + b = m_y \\left(\\sum_i w_i\\right) + b\n$$\n考虑案例1，其均值非对称 $(\\mu_0, \\mu_1) = (0.2, 1.2)$。两个类别的均值都是正的。为了正确分类输入，模型必须学习参数，使得平均而言，$E[z(x)|y=0]  0$ 且 $E[z(x)|y=1] > 0$。这要求 $0.2(\\sum w_i) + b  0$ 和 $1.2(\\sum w_i) + b > 0$。很明显，最优偏置 $b$ 必须为非零值，以适当地移动决策边界来适应数据中的正偏移。对 $b$ 应用权重衰减会施加一个将其拉向零的惩罚，这与优化目标直接冲突。这应该会导致较低的测试准确率。\n\n相反，在均值对称的案例4中 $(\\mu_0, \\mu_1) = (-0.6, 0.6)$，数据以零为中心。最优解可能有一个接近于零的偏置 $b$。在这种情况下，对偏置应用权重衰减并无害处，甚至可能提供轻微的正则化效果。因此，衰减偏置和不衰减偏置之间的性能差异应该可以忽略不计。\n\n案例2，权重衰减为零 $(\\lambda=0)$，作为一个对照组。两种训练方案完全相同，因此准确率差异 $\\Delta_2$ 必须为零。案例3使用了一个强的衰减因子，这应该会放大在非对称数据设置下不适当地衰减偏置的负面效应。\n\n因此，我们假设 $\\Delta_c = \\text{Acc}_{\\text{no-bias-decay}} - \\text{Acc}_{\\text{with-bias-decay}}$ 对于案例1和3将显著为正，对于案例4将约等于零，对于案例2将精确为零。本实验旨在通过直接、可复现的模拟来证实这一假设。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the complete experiment to analyze decoupled weight decay.\n    \"\"\"\n    \n    # --- Hyperparameters ---\n    # Fixed for all test cases as per the problem description.\n    ALPHA = 0.01  # Learning rate\n    BETA_1 = 0.9\n    BETA_2 = 0.999\n    EPSILON = 1e-8\n    EPOCHS = 100  # Number of training epochs\n\n    def generate_data(n_samples, l_seq, mu0, mu1, sigma_noise, seed):\n        \"\"\"Generates synthetic data according to the problem specification.\"\"\"\n        rng = np.random.RandomState(seed)\n        labels = rng.randint(0, 2, size=(n_samples, 1))\n        means = np.where(labels == 0, mu0, mu1)\n        # Ensure means array is broadcastable to data shape\n        means = means.reshape(-1, 1)\n        noise = rng.randn(n_samples, l_seq) * sigma_noise\n        data = means * np.ones((1, l_seq)) + noise\n        return data, labels\n\n    def get_features(X, k):\n        \"\"\"Computes the feature vector f(x) for each sample in X.\"\"\"\n        N, L = X.shape\n        M = L - k + 1\n        features = np.zeros((N, k))\n        for i in range(k):\n            features[:, i] = np.mean(X[:, i:i + M], axis=1)\n        return features\n\n    def train(X_train, y_train, k, weight_decay, decay_bias, seed):\n        \"\"\"Trains the model using AdamW, as specified in the original paper.\"\"\"\n        rng = np.random.RandomState(seed)\n        # Initialize parameters\n        w = rng.randn(k, 1) * 0.01\n        b = 0.0\n\n        # Adam moment estimates\n        m_w, v_w = np.zeros_like(w), np.zeros_like(w)\n        m_b, v_b = 0.0, 0.0\n\n        num_samples = X_train.shape[0]\n        F_train = get_features(X_train, k)\n\n        for t in range(1, EPOCHS + 1):\n            # Forward pass (at w_{t-1}, b_{t-1})\n            z = F_train @ w + b\n            p = 1.0 / (1.0 + np.exp(-z))  # Sigmoid activation\n\n            # Backward pass (gradient of the loss)\n            grad_z = p - y_train\n            grad_w = (F_train.T @ grad_z) / num_samples\n            grad_b = np.mean(grad_z)\n\n            # --- Adam Moment Updates ---\n            m_w = BETA_1 * m_w + (1 - BETA_1) * grad_w\n            m_b = BETA_1 * m_b + (1 - BETA_1) * grad_b\n            \n            v_w = BETA_2 * v_w + (1 - BETA_2) * (grad_w ** 2)\n            v_b = BETA_2 * v_b + (1 - BETA_2) * (grad_b ** 2)\n\n            # Bias-corrected estimates\n            m_w_hat = m_w / (1 - BETA_1 ** t)\n            m_b_hat = m_b / (1 - BETA_1 ** t)\n            v_w_hat = v_w / (1 - BETA_2 ** t)\n            v_b_hat = v_b / (1 - BETA_2 ** t)\n            \n            # --- Parameter Update ---\n            # Update from Adam step\n            w -= ALPHA * m_w_hat / (np.sqrt(v_w_hat) + EPSILON)\n            b -= ALPHA * m_b_hat / (np.sqrt(v_b_hat) + EPSILON)\n            \n            # Update from decoupled weight decay\n            w -= ALPHA * weight_decay * w\n            if decay_bias:\n                b -= ALPHA * weight_decay * b\n        \n        return w, b\n\n    def evaluate(X_test, y_test, w, b, k):\n        \"\"\"Evaluates model accuracy on the test set.\"\"\"\n        F_test = get_features(X_test, k)\n        z = F_test @ w + b\n        predictions = (z > 0).astype(int)\n        accuracy = np.mean(predictions == y_test)\n        return accuracy\n\n    test_cases = [\n        # Case 1 (happy path, asymmetric means, moderate decay)\n        {'L': 16, 'k': 3, 'N_train': 256, 'N_test': 512, 'mu0': 0.2, 'mu1': 1.2, 'sigma': 0.3, 'lambda': 0.05, 'train_seed': 42, 'test_seed': 1042},\n        # Case 2 (boundary, zero decay)\n        {'L': 16, 'k': 3, 'N_train': 256, 'N_test': 512, 'mu0': 0.2, 'mu1': 1.2, 'sigma': 0.3, 'lambda': 0.0, 'train_seed': 43, 'test_seed': 1043},\n        # Case 3 (edge, strong decay)\n        {'L': 16, 'k': 3, 'N_train': 256, 'N_test': 512, 'mu0': 0.2, 'mu1': 1.2, 'sigma': 0.3, 'lambda': 0.5, 'train_seed': 44, 'test_seed': 1044},\n        # Case 4 (control, symmetric means around zero)\n        {'L': 16, 'k': 3, 'N_train': 256, 'N_test': 512, 'mu0': -0.6, 'mu1': 0.6, 'sigma': 0.3, 'lambda': 0.1, 'train_seed': 45, 'test_seed': 1045},\n    ]\n\n    delta_results = []\n    for case in test_cases:\n        # Generate data\n        X_train, y_train = generate_data(case['N_train'], case['L'], case['mu0'], case['mu1'], case['sigma'], case['train_seed'])\n        X_test, y_test = generate_data(case['N_test'], case['L'], case['mu0'], case['mu1'], case['sigma'], case['test_seed'])\n\n        # --- Regime 1: Decay applied to weights and bias ---\n        w_decay, b_decay = train(X_train, y_train, case['k'], case['lambda'], decay_bias=True, seed=case['train_seed'])\n        acc_with_bias_decay = evaluate(X_test, y_test, w_decay, b_decay, case['k'])\n\n        # --- Regime 2: Decay applied to weights only ---\n        w_nodecay, b_nodecay = train(X_train, y_train, case['k'], case['lambda'], decay_bias=False, seed=case['train_seed'])\n        acc_no_bias_decay = evaluate(X_test, y_test, w_nodecay, b_nodecay, case['k'])\n\n        # Compute and store the difference\n        delta = acc_no_bias_decay - acc_with_bias_decay\n        delta_results.append(delta)\n\n    # Format the results as specified\n    formatted_results = [f\"{r:.3f}\" for r in delta_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}