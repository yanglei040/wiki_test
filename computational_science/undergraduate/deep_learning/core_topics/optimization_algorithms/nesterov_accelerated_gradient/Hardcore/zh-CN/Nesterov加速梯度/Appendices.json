{
    "hands_on_practices": [
        {
            "introduction": "理解Nesterov加速梯度（NAG）的关键在于掌握其“前瞻”（look-ahead）机制。这个基础练习将带你手动完成NAG的单步计算，通过一个简单的一维函数，让你亲身体验算法是如何在计算梯度之前“向前看”一步的 。这不仅能帮助你揭开NAG更新规则的神秘面纱，也为你后续更复杂的应用奠定了坚实的数学直觉。",
            "id": "2187811",
            "problem": "一位工程师正在应用涅斯捷罗夫加速梯度（NAG）算法来最小化一个一维目标函数。该函数为 $f(x) = 2x^2$。该算法从初始位置 $x_0$ 和初始速度 $v_0=0$ 开始，根据以下规则迭代更新位置参数 $x$ 和速度项 $v$：\n$$ v_{t} = \\gamma v_{t-1} + \\eta \\nabla f(x_{t-1} - \\gamma v_{t-1}) $$\n$$ x_{t} = x_{t-1} - v_{t} $$\n在这些方程中，下标表示迭代次数，因此 $t=1, 2, 3, \\ldots$。常数 $\\gamma$ 是动量参数，$\\eta$ 是学习率，$\\nabla f$ 是函数 $f(x)$ 的梯度。梯度的计算发生在“前瞻”点，该点由 $x_{t-1} - \\gamma v_{t-1}$ 项给出。\n\n对于初始位置 $x_0 = 10$，动量参数 $\\gamma = 0.9$ 和学习率 $\\eta = 0.1$，计算在算法的第二次迭代（即 $t=2$ 时）中使用的前瞻点的数值。",
            "solution": "给定涅斯捷罗夫加速梯度更新公式：\n$$v_{t}=\\gamma v_{t-1}+\\eta \\nabla f\\!\\left(x_{t-1}-\\gamma v_{t-1}\\right), \\quad x_{t}=x_{t-1}-v_{t}。$$\n目标函数为 $f(x)=2x^{2}$，因此其梯度为\n$$\\nabla f(x)=\\frac{d}{dx}(2x^{2})=4x。$$\n\n给定 $x_{0}=10$, $v_{0}=0$, $\\gamma=0.9$ 和 $\\eta=0.1$，首先计算 $t=1$ 时的前瞻点：\n$$x_{0}-\\gamma v_{0}=10-0.9\\cdot 0=10。$$\n然后更新 $t=1$ 时的速度：\n$$v_{1}=\\gamma v_{0}+\\eta \\nabla f(10)=0.9\\cdot 0+0.1\\cdot 4\\cdot 10=4。$$\n更新位置：\n$$x_{1}=x_{0}-v_{1}=10-4=6。$$\n\n对于第二次迭代（$t=2$），前瞻点为\n$$x_{1}-\\gamma v_{1}=6-0.9\\cdot 4=6-3.6=2.4。$$\n因此，在第二次迭代中使用的前瞻点的数值为 $2.4$。",
            "answer": "$$\\boxed{2.4}$$"
        },
        {
            "introduction": "真正的理解来自于动手实践。这个练习将指导你从零开始实现Nesterov加速梯度（NAG）和经典动量法，并在一个二维二次函数上对它们进行比较 。通过编写代码并分析不同测试案例，你将直观地量化NAG“前瞻”步骤带来的实际效果，并深入理解它为何通常比经典动量法收敛得更快、更稳定。",
            "id": "3279039",
            "problem": "您需要在一个用于凸二次函数的最速下降框架内，实现并比较两种一阶优化算法：经典动量法（也称为重球法）和 Nesterov 加速梯度法 (NAG)。需要探究的概念是 Nesterov 加速梯度法的“前瞻”行为。您的目标是从光滑凸优化中最速下降和梯度评估的基本定义出发，设计并实现这两种算法，从数值上量化“前瞻”效应，并在一个小测试集上汇总结果。\n\n从以下基本基础开始：\n- 设 $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ 是可微的，其梯度为 $\\nabla f$。在点 $x$ 处的最速下降方向是 $- \\nabla f(x)$。\n- 固定步长 $ \\alpha > 0 $ 是一个正标量，用于缩放沿选定下降方向的步长。\n- 经典动量法通过一个辅助的“速度”变量来增强最速下降法，该变量累积过去的更新以加速沿持续方向的运动；而 Nesterov 加速梯度法则在形成更新之前，在沿当前动量方向位移后的一个点上评估梯度。在 Nesterov 加速梯度法中，梯度是在一个“前瞻”点上评估的，该点取决于当前位置和动量；这是需要量化的关键行为差异。\n\n使用纯二次目标函数\n$$\nf(x) = \\tfrac{1}{2} x^{\\top} A x,\n$$\n其中 $A \\in \\mathbb{R}^{2\\times 2}$ 是对称正定矩阵，因此\n$$\n\\nabla f(x) = A x,\n$$\n且唯一极小值点是 $x^{\\star} = 0$。\n\n针对该目标函数实现这两种方法。使用固定的步长 $\\alpha$ 和固定的动量系数 $\\beta \\in [0,1)$，将速度初始化为零，并迭代指定的步数 $T$。对于经典动量法，梯度必须在当前迭代点上评估。对于 Nesterov 加速梯度法，梯度必须在一个“前瞻”点上评估，该点是从当前迭代点沿当前速度的显式位移。您还必须计算一个数值代理来“可视化”前瞻行为而无需绘图：在每次迭代 $k$ 时，计算前瞻点与当前迭代点之间位移的范数，并报告这些范数在所有迭代中的平均值。这个代理衡量了 Nesterov 加速梯度法平均“前瞻”了多远。\n\n您的程序必须实现这两种算法，并对下面的每个测试用例，计算并返回指定的量。对所有向量范数使用欧几里得范数。\n\n测试集：\n- 测试用例 1（病态条件，理想路径）：设\n  $$\n  A = \\begin{bmatrix} 10  0 \\\\ 0  1 \\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}, \\quad \\alpha = 0.1, \\quad \\beta = 0.9, \\quad T = 50.\n  $$\n  从 $x_0$ 开始，以零初始速度运行两种方法 $T$ 步。输出一个布尔值，指示 Nesterov 加速梯度法的最终目标函数值是否严格小于经典动量法的值，即 $f(x_T^{\\mathrm{NAG}})  f(x_T^{\\mathrm{HB}})$ 是否成立。\n\n- 测试用例 2（边界条件 $\\beta = 0$）：设\n  $$\n  A = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}, \\quad \\alpha = 0.2, \\quad \\beta = 0, \\quad T = 30.\n  $$\n  运行两种方法。输出一个布尔值，指示最终迭代点是否在数值公差范围内重合，即 $\\lVert x_T^{\\mathrm{NAG}} - x_T^{\\mathrm{HB}} \\rVert_2  10^{-12}$ 是否成立。\n\n- 测试用例 3（静止起始点）：设\n  $$\n  A = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\quad \\alpha = 0.2, \\quad \\beta = 0.9, \\quad T = 10.\n  $$\n  运行两种方法。输出一个布尔值，指示最终的 Nesterov 加速梯度法迭代点是否在数值公差范围内保持在极小值点，即 $\\lVert x_T^{\\mathrm{NAG}} \\rVert_2  10^{-12}$ 是否成立。\n\n- 测试用例 4（量化的前瞻幅度）：设\n  $$\n  A = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 1 \\\\ -3 \\end{bmatrix}, \\quad \\alpha = 0.2, \\quad \\beta = 0.9, \\quad T = 20.\n  $$\n  仅对于 Nesterov 加速梯度法，计算在 $T$ 次迭代中前瞻位移的平均范数：\n  $$\n  \\frac{1}{T} \\sum_{k=0}^{T-1} \\left\\lVert y_k - x_k \\right\\rVert_2,\n  $$\n  其中 $y_k$ 是第 $k$ 次迭代时的前瞻点，$x_k$ 是第 $k$ 次迭代时的当前迭代点。将此平均值作为浮点数输出。\n\n最终输出规范：\n- 您的程序必须生成一行包含各测试用例结果的输出，该结果是一个用方括号括起来的逗号分隔列表。列表中的条目必须按顺序为：\n  $1)$ 测试用例 $1$ 的布尔值，$2)$ 测试用例 $2$ 的布尔值，$3)$ 测试用例 $3$ 的布尔值，$4)$ 测试用例 $4$ 的浮点数值。\n- 例如，要求的格式是\n  $$\n  [\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4].\n  $$\n不需要也不允许进行绘图或文件输入/输出；所有计算必须是数值计算且自包含的。不涉及角度。不涉及物理单位。所有结果必须使用双精度浮点数进行算术运算。",
            "solution": "该问题要求实现和比较两种一阶优化算法，经典动量法（重球法）和 Nesterov 加速梯度法 (NAG)，用于最小化一个凸二次函数。目标是量化 NAG 的“前瞻”特性。\n\n待最小化的目标函数是一个凸二次型 $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$，由下式给出：\n$$\nf(x) = \\frac{1}{2} x^{\\top} A x\n$$\n其中 $x \\in \\mathbb{R}^2$ 是变量向量，$A \\in \\mathbb{R}^{2\\times 2}$ 是一个对称正定矩阵。该函数的梯度是线性的：\n$$\n\\nabla f(x) = A x\n$$\n由于 $A$ 是正定的，函数 $f(x)$ 是严格凸的，并在 $x^{\\star} = 0$ 处有唯一的极小值点。\n\n两种算法都是迭代方法，从一个初始点 $x_0$ 开始，生成一个收敛到极小值点 $x^{\\star}$ 的迭代序列 $\\{x_k\\}$。它们使用固定的步长 $\\alpha > 0$ 和动量系数 $\\beta \\in [0,1)$。初始速度是一个累积过去更新的变量，被设置为零，即 $v_0 = 0$。\n\n两种算法的核心逻辑都围绕着第 $k=0, 1, 2, \\dots, T-1$ 次迭代的以下更新结构：\n$1$. 计算一个梯度方向。\n$2$. 更新速度变量 $v_k$ 为 $v_{k+1}$。\n$3$. 更新位置变量 $x_k$ 为 $x_{k+1}$。\n\n两种算法的区别在于步骤 $1$，特别是在哪个点上评估梯度。\n\n**经典动量法（重球法）**\n\n在重球法中，梯度在当前位置 $x_k$ 处评估。第 $k$ 次迭代的更新规则是：\n$1$. 计算梯度：$g_k = \\nabla f(x_k) = A x_k$。\n$2$. 更新速度：$v_{k+1} = \\beta v_k - \\alpha g_k$。\n$3$. 更新位置：$x_{k+1} = x_k + v_{k+1}$。\n该过程从 $x_0$ 和 $v_0 = 0$ 开始。\n\n**Nesterov 加速梯度法 (NAG)**\n\nNAG 引入了一个“前瞻”步骤。在计算梯度之前，它会沿着当前速度方向迈出预备一步。然后在这个投影点上评估梯度，为动量更新提供校正。\n第 $k$ 次迭代的更新规则是：\n$1$. 定义前瞻点：$y_k = x_k + \\beta v_k$。\n$2$. 在前瞻点计算梯度：$g_k = \\nabla f(y_k) = A y_k$。\n$3$. 更新速度：$v_{k+1} = \\beta v_k - \\alpha g_k$。\n$4$. 更新位置：$x_{k+1} = x_k + v_{k+1}$。\n该过程也从 $x_0$ 和 $v_0 = 0$ 开始。\n\n**特殊情况分析**\n\n- **情况 $\\beta=0$**：如果动量系数 $\\beta$ 为零，两种算法中的速度更新将不再依赖于先前的速度 $v_k$。\n对于 HB：$v_{k+1} = -\\alpha \\nabla f(x_k)$，所以 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$。\n对于 NAG：前瞻点变为 $y_k = x_k + 0 \\cdot v_k = x_k$。速度更新为 $v_{k+1} = -\\alpha \\nabla f(x_k)$，因此 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$。\n两种算法都退化为标准梯度下降法。它们的轨迹 $\\{x_k\\}$ 将完全相同。测试用例 2 验证了此属性。\n\n- **情况 $x_0=x^{\\star}=0$**：如果起始点是极小值点，则梯度为零：$\\nabla f(0) = A \\cdot 0 = 0$。\n对于 HB 和 NAG，若 $x_0=0$ 且 $v_0=0$，第一次迭代将得到：\n$g_0 = 0$，这导致 $v_1 = 0$，并随后 $x_1 = 0$。\n通过归纳法，对于所有 $k > 0$，迭代点将保持在极小值点，即 $x_k=0$ 和 $v_k=0$。测试用例 3 验证了这种静止行为。\n\n**量化前瞻效应**\n\nNAG 的“前瞻”行为由位移向量 $y_k - x_k = \\beta v_k$ 来表征。为了在数值上量化这种效应，我们计算该位移在 $T$ 次迭代中的平均欧几里得范数。这个代理度量定义为：\n$$\n\\text{平均前瞻} = \\frac{1}{T} \\sum_{k=0}^{T-1} \\left\\lVert y_k - x_k \\right\\rVert_2 = \\frac{1}{T} \\sum_{k=0}^{T-1} \\left\\lVert \\beta v_k \\right\\rVert_2\n$$\n测试用例 4 要求计算此值。\n\n解决方案的步骤是实现这两种算法，并在 4 个指定的测试用例上运行它们，为每个用例计算所需的结果。所有向量范数均为欧几里得范数（$\\ell_2$-范数），且浮点运算使用双精度进行。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Classical Momentum (Heavy-Ball) and Nesterov\n    Accelerated Gradient (NAG) on a set of test cases.\n    \"\"\"\n\n    def heavy_ball(A, x0, alpha, beta, T):\n        \"\"\"\n        Implements the Heavy-Ball (Classical Momentum) algorithm.\n\n        Args:\n            A (np.ndarray): The matrix of the quadratic objective.\n            x0 (np.ndarray): The starting point.\n            alpha (float): The step size.\n            beta (float): The momentum coefficient.\n            T (int): The number of iterations.\n\n        Returns:\n            np.ndarray: The final iterate x_T.\n        \"\"\"\n        x = np.copy(x0).astype(np.float64)\n        v = np.zeros_like(x0, dtype=np.float64)\n\n        for _ in range(T):\n            g = A @ x\n            v = beta * v - alpha * g\n            x = x + v\n        return x\n\n    def nesterov_ag(A, x0, alpha, beta, T, compute_lookahead=False):\n        \"\"\"\n        Implements the Nesterov Accelerated Gradient (NAG) algorithm.\n\n        Args:\n            A (np.ndarray): The matrix of the quadratic objective.\n            x0 (np.ndarray): The starting point.\n            alpha (float): The step size.\n            beta (float): The momentum coefficient.\n            T (int): The number of iterations.\n            compute_lookahead (bool): If True, also computes and returns the\n                                      average look-ahead displacement norm.\n        Returns:\n            np.ndarray or tuple: The final iterate x_T. If compute_lookahead is\n                                 True, returns (x_T, avg_lookahead_norm).\n        \"\"\"\n        x = np.copy(x0).astype(np.float64)\n        v = np.zeros_like(x0, dtype=np.float64)\n        \n        if compute_lookahead:\n            lookahead_norms = []\n\n        for _ in range(T):\n            if compute_lookahead:\n                # Displacement at step k is beta * v_k\n                lookahead_norms.append(np.linalg.norm(beta * v))\n            \n            # Look-ahead point and gradient calculation\n            y = x + beta * v\n            g = A @ y\n\n            # Update velocity and position\n            v = beta * v - alpha * g\n            x = x + v\n\n        if compute_lookahead:\n            avg_lookahead_norm = np.mean(lookahead_norms) if lookahead_norms else 0.0\n            return x, avg_lookahead_norm\n        else:\n            return x\n\n    def objective_function(x, A):\n        \"\"\"Computes f(x) = 0.5 * x.T @ A @ x.\"\"\"\n        return 0.5 * x.T @ A @ x\n\n    results = []\n\n    # Test Case 1: Ill-conditioned, comparison of objective values\n    A1 = np.array([[10, 0], [0, 1]], dtype=np.float64)\n    x0_1 = np.array([3, -1], dtype=np.float64)\n    alpha1, beta1, T1 = 0.1, 0.9, 50\n    \n    xT_hb_1 = heavy_ball(A1, x0_1, alpha1, beta1, T1)\n    xT_nag_1 = nesterov_ag(A1, x0_1, alpha1, beta1, T1)\n    \n    f_hb_1 = objective_function(xT_hb_1, A1)\n    f_nag_1 = objective_function(xT_nag_1, A1)\n    \n    results.append(f_nag_1  f_hb_1)\n\n    # Test Case 2: Boundary condition beta = 0\n    A2 = np.array([[4, 1], [1, 3]], dtype=np.float64)\n    x0_2 = np.array([2, 2], dtype=np.float64)\n    alpha2, beta2, T2 = 0.2, 0.0, 30\n\n    xT_hb_2 = heavy_ball(A2, x0_2, alpha2, beta2, T2)\n    xT_nag_2 = nesterov_ag(A2, x0_2, alpha2, beta2, T2)\n\n    diff_norm_2 = np.linalg.norm(xT_nag_2 - xT_hb_2)\n    results.append(diff_norm_2  1e-12)\n\n    # Test Case 3: Stationary start\n    A3 = np.array([[4, 1], [1, 3]], dtype=np.float64)\n    x0_3 = np.array([0, 0], dtype=np.float64)\n    alpha3, beta3, T3 = 0.2, 0.9, 10\n\n    xT_nag_3 = nesterov_ag(A3, x0_3, alpha3, beta3, T3)\n    norm_nag_3 = np.linalg.norm(xT_nag_3)\n    results.append(norm_nag_3  1e-12)\n\n    # Test Case 4: Quantified look-ahead magnitude\n    A4 = np.array([[4, 1], [1, 3]], dtype=np.float64)\n    x0_4 = np.array([1, -3], dtype=np.float64)\n    alpha4, beta4, T4 = 0.2, 0.9, 20\n\n    _, avg_lookahead_4 = nesterov_ag(A4, x0_4, alpha4, beta4, T4, compute_lookahead=True)\n    results.append(avg_lookahead_4)\n\n    # Format and print the final output\n    # The map(str, ...) part handles converting booleans to 'True'/'False'\n    # and floats to their string representation.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "任何强大的算法都有其局限性，Nesterov加速梯度（NAG）也不例外。这个高级练习将引导你探索一种NAG的失效模式：当学习率过大时，算法可能不会收敛，而是陷入持续的振荡（极限环） 。你将使用来自​​动态系统理论的庞加莱映射（Poincaré map）工具来检测这种行为，从而将优化算法的分析提升到一个新的深度，培养对算法行为的批判性思维。",
            "id": "3157052",
            "problem": "你的任务是研究一个二维玩具优化问题中，Nesterov 加速梯度（NAG）的离散时间动力学行为。重点关注一种失效模式，即过大的步长会引发与极限环一致的振荡行为。你必须利用在选定截面上的庞加莱映射，设计一个算法检测器来探测此类环。\n\n从以下基础概念开始：\n- 该优化问题旨在最小化一个可微的标量值目标函数 $f(\\mathbf{x})$，其中 $\\mathbf{x} \\in \\mathbb{R}^2$。\n- 梯度 $\\nabla f(\\mathbf{x})$ 指向最陡上升方向，而基于梯度的离散更新使用与梯度相反的增量来减小目标函数值。\n- Nesterov 加速梯度（NAG）是一种基于动量的方法，它在一个前瞻点评估目标函数的梯度，然后应用速度更新，再进行位置更新。将动量视为一个在 $(0,1)$ 区间内的无量纲阻尼因子。\n- 庞加莱映射通过记录轨迹沿特定方向与状态空间中选定截面的交点，从而简化对轨迹的研究，使得周期轨道可以被检测为诱导映射中的不动点或循环。\n\n考虑以下具体的二维玩具目标函数：\n$$\nf(x,y) \\;=\\; \\tfrac{1}{2}\\,\\big(\\sqrt{x^2+y^2}-r_0\\big)^2 \\;+\\; a\\,\\cos\\!\\big(m\\,\\theta\\big), \\quad \\text{with } \\theta=\\operatorname{atan2}(y,x),\n$$\n其中 $r_00$、$a0$ 和 $m\\in\\mathbb{N}$ 是参数，$\\operatorname{atan2}(y,x)$ 是双参数反正切函数。这个 $f(x,y)$ 函数结合了一个将轨迹吸引到半径为 $r_0$ 的圆周上的径向项，以及一个沿离散角向模式引入切向分量的角向项。必须从此定义推导出梯度 $\\nabla f(x,y)$，在类极坐标中注意使用链式法则，并避免在原点处出现除零错误。\n\n你的任务是：\n1. 从第一性原理和核心定义出发，推导二维离散时间 Nesterov 加速梯度（NAG）的更新公式，其中使用在“前瞻”位置评估的目标函数梯度。不要假设任何预先提供的快捷公式；明确使用 NAG 的概念性定义，即包含速度向量和前瞻梯度。\n2. 为给定的 $f(x,y)$ 推导出显式梯度 $\\nabla f(x,y)$，以笛卡尔坐标 $(x,y)$ 表示，并包含径向和角向的贡献。确保表达式在远离原点处有良好定义，并为原点定义一个不会引发未定义操作的数值稳定处理方法。\n3. 实现一个极限环检测器，使用一个定义为 $y=0$ 且 $x0$ 的直线作为庞加莱截面，并设定穿越方向为从 $y0$ 到 $y0$。对于每次穿越，通过在连续的离散时间点之间进行一阶插值，记录穿越点的横坐标。基于这些记录的穿越点，如果穿越次数足够多，并且穿越点趋于稳定（即最近的值相对变化很小，且远离平凡平衡点），则宣布存在极限环。如果轨迹发散到一个非常大的半径，则宣布没有极限环。\n\n测试套件：\n在以下三个测试用例上运行你的程序，每个用例由元组 $(r_0, a, m, \\eta, \\beta, x_0, y_0, v_{x0}, v_{y0}, T)$ 定义，其中 $\\eta$ 是步长，$\\beta$ 是动量系数，$(x_0,y_0)$ 是初始位置，$(v_{x0}, v_{y0})$ 是初始速度，T 是最大迭代次数。\n\n- 用例 1（正常路径，稳定收敛）：\n  - $(r_0, a, m) = (1.0, 0.10, 5)$\n  - $(\\eta, \\beta) = (0.05, 0.90)$\n  - $(x_0, y_0) = (1.2, -0.3)$\n  - $(v_{x0}, v_{y0}) = (0.0, 0.0)$\n  - $T = 8000$\n  预期行为：轨迹收敛到一个最小值，而没有维持一个极限环。输出应为一个布尔值，表示检测器的判定结果。\n\n- 用例 2（步长过大，持续振荡）：\n  - $(r_0, a, m) = (1.0, 0.20, 7)$\n  - $(\\eta, \\beta) = (0.60, 0.95)$\n  - $(x_0, y_0) = (1.5, -0.8)$\n  - $(v_{x0}, v_{y0}) = (0.0, 0.4)$\n  - $T = 12000$\n  预期行为：轨迹表现出与极限环一致的持续振荡，可通过庞加莱映射识别。输出应为一个布尔值，表示检测器的判定结果。\n\n- 用例 3（极端步长，发散）：\n  - $(r_0, a, m) = (1.0, 0.15, 9)$\n  - $(\\eta, \\beta) = (1.50, 0.95)$\n  - $(x_0, y_0) = (0.6, -1.0)$\n  - $(v_{x0}, v_{y0}) = (0.3, 0.0)$\n  - $T = 6000$\n  预期行为：轨迹发散；不应宣布存在极限环。输出应为一个布尔值，表示检测器的判定结果。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，\"[result1,result2,result3]\"），其中每个结果是来自集合 {True, False} 的布尔值，按顺序对应于三个测试用例。此问题不涉及物理单位，除了通过 $\\operatorname{atan2}(y,x)$ 对 $\\theta$ 统一使用弧度外，不需要进行角度单位转换。",
            "solution": "问题陈述已经过严格验证，被认为是科学上合理、定义明确、客观且完整的。该问题是计算物理和数值优化领域的一个正式练习，要求推导和实现标准算法。所提供的参数和条件足以获得唯一的数值解。该问题有效。\n\n解决方案按要求分为三个部分呈现：Nesterov 加速梯度（NAG）更新方程的推导、目标函数梯度的推导，以及使用庞加莱映射的极限环检测器的设计。\n\n### 1. Nesterov 加速梯度（NAG）更新的推导\n\nNesterov 加速梯度（NAG）方法是一种基于动量的优化算法。其核心原理是，计算梯度时不在当前位置，而是在一个“前瞻”位置，该位置是对应用当前速度后位置的近似。这使得算法能够预先修正其路径。\n\n设 $\\mathbf{x}_t \\in \\mathbb{R}^2$ 为离散时间步 $t$ 的位置向量，$\\mathbf{v}_t \\in \\mathbb{R}^2$ 为速度向量。设 $\\eta > 0$ 为步长（学习率），$\\beta \\in (0,1)$ 为动量系数。根据问题描述，从时间 $t$ 到 $t+1$ 的更新过程如下：\n\n1.  **计算前瞻位置**：算法首先使用当前速度（由动量系数缩放）将当前位置向前投影。这个前瞻位置 $\\mathbf{x}_{\\text{look}}$ 由下式给出：\n    $$\n    \\mathbf{x}_{\\text{look}} = \\mathbf{x}_t + \\beta \\mathbf{v}_t\n    $$\n\n2.  **在前瞻位置评估梯度**：目标函数的梯度 $\\nabla f$ 是在这个前瞻位置计算的，而不是在当前位置 $\\mathbf{x}_t$。设此梯度为 $\\mathbf{g}_t$：\n    $$\n    \\mathbf{g}_t = \\nabla f(\\mathbf{x}_{\\text{look}}) = \\nabla f(\\mathbf{x}_t + \\beta \\mathbf{v}_t)\n    $$\n\n3.  **更新速度**：新的速度 $\\mathbf{v}_{t+1}$ 的计算方法是，取前一个速度（由动量因子缩放），并加上一个将系统推向与前瞻梯度相反方向的新项。\n    $$\n    \\mathbf{v}_{t+1} = \\beta \\mathbf{v}_t - \\eta \\mathbf{g}_t\n    $$\n\n4.  **更新位置**：最后，通过将新计算的速度向量 $\\mathbf{v}_{t+1}$ 加到当前位置 $\\mathbf{x}_t$ 来更新位置。\n    $$\n    \\mathbf{x}_{t+1} = \\mathbf{x}_t + \\mathbf{v}_{t+1}\n    $$\n将 $\\mathbf{v}_{t+1}$ 的表达式代入位置更新公式，可以得到单行的完整更新公式：\n$$\n\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\beta \\mathbf{v}_t - \\eta \\nabla f(\\mathbf{x}_t + \\beta \\mathbf{v}_t)\n$$\n这组方程定义了我们将要分析其轨迹的离散时间动力系统。\n\n### 2. 梯度 $\\nabla f(x,y)$ 的推导\n\n目标函数以类极坐标形式给出：\n$$\nf(x,y) = \\tfrac{1}{2}\\,\\big(\\sqrt{x^2+y^2}-r_0\\big)^2 + a\\,\\cos\\!\\big(m\\,\\theta\\big)\n$$\n其中 $r = \\sqrt{x^2+y^2}$ 是距原点的径向距离，$\\theta = \\operatorname{atan2}(y,x)$ 是角度。梯度 $\\nabla f = (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})^T$ 使用链式法则求得。我们将 $f$ 表示为 $f(r(x,y), \\theta(x,y))$。\n\n$f$ 对中间变量 $r$ 和 $\\theta$ 的偏导数是：\n$$\n\\frac{\\partial f}{\\partial r} = r - r_0\n$$\n$$\n\\frac{\\partial f}{\\partial \\theta} = -am \\sin(m\\theta)\n$$\n对于 $(x,y) \\neq (0,0)$，极坐标 $(r, \\theta)$ 对笛卡尔坐标 $(x, y)$ 的偏导数是：\n$$\n\\frac{\\partial r}{\\partial x} = \\frac{x}{\\sqrt{x^2+y^2}} = \\frac{x}{r} \\quad ; \\quad \\frac{\\partial r}{\\partial y} = \\frac{y}{\\sqrt{x^2+y^2}} = \\frac{y}{r}\n$$\n$$\n\\frac{\\partial \\theta}{\\partial x} = \\frac{-y}{x^2+y^2} = \\frac{-y}{r^2} \\quad ; \\quad \\frac{\\partial \\theta}{\\partial y} = \\frac{x}{x^2+y^2} = \\frac{x}{r^2}\n$$\n应用链式法则：\n$$\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial r}\\frac{\\partial r}{\\partial x} + \\frac{\\partial f}{\\partial \\theta}\\frac{\\partial \\theta}{\\partial x} = (r-r_0)\\frac{x}{r} + (-am \\sin(m\\theta))\\frac{-y}{r^2}\n$$\n$$\n\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial r}\\frac{\\partial r}{\\partial y} + \\frac{\\partial f}{\\partial \\theta}\\frac{\\partial \\theta}{\\partial y} = (r-r_0)\\frac{y}{r} + (-am \\sin(m\\theta))\\frac{x}{r^2}\n$$\n简化这些表达式，我们得到梯度 $\\nabla f(x,y)$ 的分量：\n$$\n\\frac{\\partial f}{\\partial x} = \\left(1 - \\frac{r_0}{r}\\right)x + \\frac{am \\sin(m\\theta)}{r^2}y\n$$\n$$\n\\frac{\\partial f}{\\partial y} = \\left(1 - \\frac{r_0}{r}\\right)y - \\frac{am \\sin(m\\theta)}{r^2}x\n$$\n以向量形式表示：\n$$\n\\nabla f(x,y) = \\left(1 - \\frac{r_0}{r}\\right)\\begin{pmatrix} x \\\\ y \\end{pmatrix} + \\frac{am \\sin(m\\theta)}{r^2}\\begin{pmatrix} y \\\\ -x \\end{pmatrix}\n$$\n这个表达式在原点 $(0,0)$ 处是奇异的，因为此时 $r=0$。根据问题说明，需要进行数值稳定的处理。在实际实现中，当 $r$ 小于一个很小的容差 $\\epsilon_{\\text{num}}$ 时，可以将梯度定义为零向量，即 $\\nabla f(0,0) = (0,0)^T$。这可以防止除零错误，并且在物理上是合理的，因为势在奇点处是病态的。对于数值计算，我们将分母 $r$ 和 $r^2$ 分别替换为 $r+\\epsilon_{\\text{num}}$ 和 $r^2+\\epsilon_{\\text{num}}$，其中 $\\epsilon_{\\text{num}}$ 是一个很小的正数（例如 $10^{-12}$）。\n\n### 3. 极限环检测器的设计\n\n极限环是动力系统状态空间中的一个周期性轨道。我们可以通过观察其与一个称为庞加莱截面的低维子空间的重复交点来检测这种轨道。\n\n**庞加莱截面与映射**：所选的截面是正 x 轴，定义为半线 $S = \\{(x,y) \\in \\mathbb{R}^2 \\mid y=0, x>0\\}$。我们记录轨迹以特定方向穿过此截面的交点：从下半平面（y = 0）到上半平面（y > 0）。\n在 NAG 模拟的每个时间步，从 $\\mathbf{x}_t=(x_t, y_t)$ 到 $\\mathbf{x}_{t+1}=(x_{t+1}, y_{t+1})$，我们通过测试条件 $y_t \\le 0$ 和 $y_{t+1} > 0$ 来检查是否发生了穿越。\n\n**交点计算**：如果检测到穿越，交点的横坐标 $x_{\\text{cross}}$ 通过在 $\\mathbf{x}_t$ 和 $\\mathbf{x}_{t+1}$ 之间进行一阶线性插值来估计。假设这两点之间是直线路径，通过将插值后的 y 坐标设为零，可以找到穿越的时间分数 $s \\in [0,1)$：\n$$\ny(s) = (1-s)y_t + s y_{t+1} = 0 \\implies s = \\frac{-y_t}{y_{t+1}-y_t}\n$$\n那么交点的 x 坐标是：\n$$\nx_{\\text{cross}} = (1-s)x_t + s x_{t+1}\n$$\n这些 $x_{\\text{cross}}$ 值被顺序收集到一个列表中。\n\n**检测算法**：宣布存在极限环的算法基于交点序列 $\\{x_{\\text{cross}}^{(i)}\\}$ 的稳定性。\n1. **发散检查**：在模拟过程中，如果径向距离 $r = \\|\\mathbf{x}_t\\|$ 超过一个大的阈值 $R_{\\text{div}}$，则认为轨迹是发散的。该情况的模拟终止，并且不宣布存在极限环。一个合理的选择是 $R_{\\text{div}} = 100$。\n2. **数据收集**：从有效的穿越中收集一个 $x_{\\text{cross}}$ 值的序列。\n3. **稳定性分析**：一旦记录了足够数量的穿越点（$N_{\\text{min_crossings}}$，例如 20 个），我们分析这些穿越点中最近的一个子集（$N_{\\text{recent}}$，例如 10 个）。\n4. **判定准则**：如果最近的穿越点是稳定的，则宣布存在极限环。我们通过最近穿越点的相对标准差来量化稳定性。设 $\\mu$ 和 $\\sigma$ 分别是最后 $N_{\\text{recent}}$ 个 $x_{\\text{cross}}$ 值的均值和标准差。如果满足以下条件，则检测到极限环：\n    $$\n    \\frac{\\sigma}{|\\mu|}  \\epsilon_{\\text{rel_std}}\n    $$\n    其中 $\\epsilon_{\\text{rel_std}}$ 是一个很小的阈值（例如 0.005）。这个条件意味着交点已经稳定到一个几乎恒定的值，这对应于一个周期为 1 的极限环。\n5. **终止**：如果在达到最大迭代次数（$T$）后，稳定性准则仍未满足且轨迹也未发散，则结论为没有形成稳定的极限环。这种情况包括收敛到不动点最小值的轨迹。\n\n最终的布尔决策（检测到极限环为 True，否则为 False）基于该算法对每个测试用例的输出结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Nesterov Accelerated Gradient simulation for all test cases\n    and detect limit cycles using a Poincaré map.\n    \"\"\"\n\n    def get_gradient(x, y, r0, a, m):\n        \"\"\"\n        Computes the gradient of the objective function f(x,y).\n        Handles the singularity at the origin numerically.\n        \"\"\"\n        numerical_stability_epsilon = 1e-12\n        r_sq = x**2 + y**2\n        \n        if r_sq  numerical_stability_epsilon**2:\n            return np.array([0.0, 0.0])\n\n        r = np.sqrt(r_sq)\n        theta = np.arctan2(y, x)\n        \n        # Pre-compute common terms\n        sin_m_theta = np.sin(m * theta)\n        radial_factor = 1.0 - r0 / r\n        angular_factor = a * m * sin_m_theta / r_sq\n\n        # Gradient components\n        grad_x = radial_factor * x + angular_factor * y\n        grad_y = radial_factor * y - angular_factor * x\n        \n        return np.array([grad_x, grad_y])\n\n    def detect_limit_cycle(r0, a, m, eta, beta, x0, y0, vx0, vy0, T):\n        \"\"\"\n        Runs the NAG simulation and implements the Poincaré map based detector for limit cycles.\n        \"\"\"\n        # --- State variables ---\n        pos = np.array([x0, y0], dtype=float)\n        vel = np.array([vx0, vy0], dtype=float)\n\n        # --- Detector parameters ---\n        crossings = []\n        divergence_radius = 100.0 * r0\n        n_min_crossings = 20\n        n_recent = 10\n        rel_std_thresh = 0.005\n\n        for _ in range(T):\n            pos_prev = np.copy(pos)\n\n            # --- NAG Update Step ---\n            # 1. Lookahead position\n            pos_lookahead = pos + beta * vel\n            \n            # 2. Gradient at lookahead position\n            grad = get_gradient(pos_lookahead[0], pos_lookahead[1], r0, a, m)\n\n            # 3. Velocity update\n            vel = beta * vel - eta * grad\n            \n            # 4. Position update\n            pos = pos + vel\n\n            # --- Analysis Step ---\n            # 1. Check for divergence\n            if np.linalg.norm(pos) > divergence_radius:\n                return False\n\n            # 2. Check for Poincaré section crossing\n            # Section: y=0, x>0, crossing from y=0 to y>0\n            y_prev, y_curr = pos_prev[1], pos[1]\n            if y_prev = 0 and y_curr > 0:\n                # Ensure the crossing happens on the positive x-axis\n                if pos[0] > 0:\n                    # Linear interpolation to find the crossing point's x-coordinate\n                    s = -y_prev / (y_curr - y_prev)\n                    x_cross = (1 - s) * pos_prev[0] + s * pos[0]\n                    crossings.append(x_cross)\n\n                    # 3. Check for cycle stability\n                    if len(crossings) >= n_min_crossings:\n                        recent_crossings = np.array(crossings[-n_recent:])\n                        mean_cross = np.mean(recent_crossings)\n                        std_cross = np.std(recent_crossings)\n\n                        # Check if crossings are non-trivial and stable\n                        if mean_cross > 1e-6:\n                            relative_std = std_cross / np.abs(mean_cross)\n                            if relative_std  rel_std_thresh:\n                                return True\n\n        # If loop finishes without detecting a cycle, return False\n        return False\n\n    # (r0, a, m, eta, beta, x0, y0, vx0, vy0, T)\n    test_cases = [\n        # Case 1: stable convergence\n        (1.0, 0.10, 5, 0.05, 0.90, 1.2, -0.3, 0.0, 0.0, 8000),\n        # Case 2: sustained oscillation\n        (1.0, 0.20, 7, 0.60, 0.95, 1.5, -0.8, 0.0, 0.4, 12000),\n        # Case 3: divergence\n        (1.0, 0.15, 9, 1.50, 0.95, 0.6, -1.0, 0.3, 0.0, 6000),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = detect_limit_cycle(*params)\n        results.append(result)\n\n    # Format the final output as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}