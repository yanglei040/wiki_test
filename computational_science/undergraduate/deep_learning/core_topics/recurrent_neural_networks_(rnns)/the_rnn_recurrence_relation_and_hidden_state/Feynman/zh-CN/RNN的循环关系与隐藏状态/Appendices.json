{
    "hands_on_practices": [
        {
            "introduction": "我们首先从一个“系统辨识”问题开始。想象一下，你是一名观察动态系统的科学家，任务是从观测到的数据中推断出其内部的运作法则。这个练习将循环神经网络（RNN）的参数估计问题巧妙地转化为一个我们熟悉的线性回归问题，让你能够从基本原理出发，推导出参数的最小二乘估计。通过这个实践 ，你不仅能加深对 RNN 递推关系本身的理解，还能掌握一种从具体数据中反向工程出抽象模型参数的强大方法。",
            "id": "3192180",
            "problem": "考虑一个线性循环神经网络（RNN），其中隐藏状态更新使用恒等激活函数。隐藏状态 $\\mathbf{h}_t \\in \\mathbb{R}^2$ 根据以下递推关系演化\n$$\n\\mathbf{h}_t = A \\mathbf{h}_{t-1} + B x_t,\n$$\n其中状态转移矩阵 $A \\in \\mathbb{R}^{2 \\times 2}$ 和输入矩阵 $B \\in \\mathbb{R}^{2 \\times 1}$ 未知。给定初始隐藏状态 $\\mathbf{h}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 以及在 $t = 1, 2, 3, 4, 5$ 时观测到的以下输入-状态对 $(x_t, \\mathbf{h}_t)$：\n$$\n\\begin{aligned}\nx_1 = 1, \\quad \\mathbf{h}_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\\\\nx_2 = 2, \\quad \\mathbf{h}_2 = \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix}, \\\\\nx_3 = 1, \\quad \\mathbf{h}_3 = \\begin{pmatrix} 12 \\\\ 4 \\end{pmatrix}, \\\\\nx_4 = 0, \\quad \\mathbf{h}_4 = \\begin{pmatrix} 20 \\\\ 4 \\end{pmatrix}, \\\\\nx_5 = -1, \\quad \\mathbf{h}_5 = \\begin{pmatrix} 27 \\\\ 3 \\end{pmatrix}.\n\\end{aligned}\n$$\n从最小二乘估计的基本定义（最小化残差平方和）出发，通过从递推关系中显式地构建一个线性回归问题，并求解相关的正规方程，来推导出 $A$ 和 $B$ 的估计量，过程中不使用任何现成的公式。然后，将你的推导应用于给定数据，以恢复 $A$ 和 $B$ 的数值。\n\n此外，运用线性系统的第一性原理，通过讨论由时间序列数据构建的块汉克尔矩阵（block Hankel matrices）的作用，以及当输出即为隐藏状态本身时状态的可观测性，来论证恢复的解何时是唯一的。你的论证应阐明秩条件是如何从这些构造中产生的，以及为什么它们对 $A$ 和 $B$ 的可辨识性至关重要。\n\n你的最终答案必须是一个单一的复合解析表达式，包含 $A$ 和 $B$ 的所有元素，按 $(a_{11}, a_{12}, a_{21}, a_{22}, b_1, b_2)$ 的顺序列于一个单行矩阵中。无需四舍五入。",
            "solution": "该问题是有效的。这是一个在系统辨识领域中适定的问题，系统辨识是工程学和机器学习中的一个核心课题。它具有科学依据，是客观的，并包含足够的信息来确定一个唯一的解。该任务要求从第一性原理推导估计量并论证其唯一性，这在此背景下是标准且恰当的问题。\n\n隐藏状态的演化由以下线性递推关系给出：\n$$\n\\mathbf{h}_t = A \\mathbf{h}_{t-1} + B x_t\n$$\n其中 $\\mathbf{h}_t \\in \\mathbb{R}^2$，$A \\in \\mathbb{R}^{2 \\times 2}$，$B \\in \\mathbb{R}^{2 \\times 1}$。待估计的参数是矩阵 $A$ 和 $B$ 中的元素。\n我们定义矩阵和向量如下：\n$$\nA = \\begin{pmatrix} a_{11}  a_{12} \\\\ a_{21}  a_{22} \\end{pmatrix}, \\quad B = \\begin{pmatrix} b_1 \\\\ b_2 \\end{pmatrix}, \\quad \\mathbf{h}_t = \\begin{pmatrix} h_{t,1} \\\\ h_{t,2} \\end{pmatrix}\n$$\n该递推关系可以为每一行分解为两个独立的标量方程：\n$$\nh_{t,1} = a_{11} h_{t-1,1} + a_{12} h_{t-1,2} + b_1 x_t \\\\\nh_{t,2} = a_{21} h_{t-1,1} + a_{22} h_{t-1,2} + b_2 x_t\n$$\n这个结构揭示了两个独立的线性回归问题。对于第一行，参数为 $\\boldsymbol{\\theta}_1 = \\begin{pmatrix} a_{11}  a_{12}  b_1 \\end{pmatrix}^T$，目标变量是 $h_{t,1}$。对于第二行，参数为 $\\boldsymbol{\\theta}_2 = \\begin{pmatrix} a_{21}  a_{22}  b_2 \\end{pmatrix}^T$，目标是 $h_{t,2}$。两个回归问题共享同一组回归变量，这些变量可以排列成一个向量 $\\mathbf{z}_t = \\begin{pmatrix} h_{t-1,1}  h_{t-1,2}  x_t \\end{pmatrix}^T$。因此，方程变为：\n$$\nh_{t,1} = \\mathbf{z}_t^T \\boldsymbol{\\theta}_1 \\\\\nh_{t,2} = \\mathbf{z}_t^T \\boldsymbol{\\theta}_2\n$$\n为了使用给定的 $t = 1, \\dots, 5$ 的数据来估计参数，我们将数据组装成矩阵。设观测数量为 $N=5$。我们定义一个设计矩阵 $Z$，其中每一行对应一个给定时间步的回归变量，并为两个标量子问题定义目标向量 $Y_1$ 和 $Y_2$。\n$$\nZ = \\begin{pmatrix} \\mathbf{z}_1^T \\\\ \\mathbf{z}_2^T \\\\ \\vdots \\\\ \\mathbf{z}_N^T \\end{pmatrix} = \\begin{pmatrix} \\mathbf{h}_0^T  x_1 \\\\ \\mathbf{h}_1^T  x_2 \\\\ \\vdots \\\\ \\mathbf{h}_{N-1}^T  x_N \\end{pmatrix}, \\quad Y_1 = \\begin{pmatrix} h_{1,1} \\\\ h_{2,1} \\\\ \\vdots \\\\ h_{N,1} \\end{pmatrix}, \\quad Y_2 = \\begin{pmatrix} h_{1,2} \\\\ h_{2,2} \\\\ \\vdots \\\\ h_{N,2} \\end{pmatrix}\n$$\n所有时间步的整个方程组可以写成 $Y_1 \\approx Z \\boldsymbol{\\theta}_1$ 和 $Y_2 \\approx Z \\boldsymbol{\\theta}_2$。\n\n最小二乘法原理要求我们找到使残差（误差）平方和最小化的参数向量 $\\boldsymbol{\\theta}$。对于第一个子问题，目标函数是残差向量的欧几里得范数的平方：\n$$\nS(\\boldsymbol{\\theta}_1) = \\| Y_1 - Z \\boldsymbol{\\theta}_1 \\|_2^2 = (Y_1 - Z \\boldsymbol{\\theta}_1)^T (Y_1 - Z \\boldsymbol{\\theta}_1) = Y_1^T Y_1 - 2 Y_1^T Z \\boldsymbol{\\theta}_1 + \\boldsymbol{\\theta}_1^T Z^T Z \\boldsymbol{\\theta}_1\n$$\n为了找到最小值，我们计算 $S(\\boldsymbol{\\theta}_1)$ 相对于 $\\boldsymbol{\\theta}_1$ 的梯度并将其设为零。使用标准的矩阵微积分恒等式，我们有：\n$$\n\\nabla_{\\boldsymbol{\\theta}_1} S(\\boldsymbol{\\theta}_1) = -2 Z^T Y_1 + 2 Z^T Z \\boldsymbol{\\theta}_1 = 0\n$$\n这就得到了著名的正规方程：\n$$\n(Z^T Z) \\boldsymbol{\\theta}_1 = Z^T Y_1\n$$\n假设矩阵 $Z^T Z$ 是可逆的，$\\boldsymbol{\\theta}_1$ 的唯一最小二乘估计量为：\n$$\n\\hat{\\boldsymbol{\\theta}}_1 = (Z^T Z)^{-1} Z^T Y_1\n$$\n对于 $\\boldsymbol{\\theta}_2$，一个相同的推导同样成立，得到 $\\hat{\\boldsymbol{\\theta}}_2 = (Z^T Z)^{-1} Z^T Y_2$。\n\n现在，我们将此推导应用于给定数据。已知值为 $\\mathbf{h}_0 = \\begin{pmatrix} 0  0 \\end{pmatrix}^T$ 以及：\n$\\mathbf{h}_1 = \\begin{pmatrix} 1  1 \\end{pmatrix}^T$ 当 $x_1=1$ 时；\n$\\mathbf{h}_2 = \\begin{pmatrix} 5  3 \\end{pmatrix}^T$ 当 $x_2=2$ 时；\n$\\mathbf{h}_3 = \\begin{pmatrix} 12  4 \\end{pmatrix}^T$ 当 $x_3=1$ 时；\n$\\mathbf{h}_4 = \\begin{pmatrix} 20  4 \\end{pmatrix}^T$ 当 $x_4=0$ 时；\n$\\mathbf{h}_5 = \\begin{pmatrix} 27  3 \\end{pmatrix}^T$ 当 $x_5=-1$ 时。\n\n我们构造矩阵 $Z$、$Y_1$ 和 $Y_2$：\n$$\nZ = \\begin{pmatrix} \\mathbf{h}_0^T  x_1 \\\\ \\mathbf{h}_1^T  x_2 \\\\ \\mathbf{h}_2^T  x_3 \\\\ \\mathbf{h}_3^T  x_4 \\\\ \\mathbf{h}_4^T  x_5 \\end{pmatrix} = \\begin{pmatrix} 0  0  1 \\\\ 1  1  2 \\\\ 5  3  1 \\\\ 12  4  0 \\\\ 20  4  -1 \\end{pmatrix}, \\quad Y_1 = \\begin{pmatrix} 1 \\\\ 5 \\\\ 12 \\\\ 20 \\\\ 27 \\end{pmatrix}, \\quad Y_2 = \\begin{pmatrix} 1 \\\\ 3 \\\\ 4 \\\\ 4 \\\\ 3 \\end{pmatrix}\n$$\n我们可以不直接计算 $(Z^T Z)^{-1}$，而是检验是否存在一个完美解。一个完美解将产生零误差，这是残差平方和的绝对最小值，因此它将是唯一的最小二乘解。\n\n从 $\\mathbf{h}_1 = A \\mathbf{h}_0 + B x_1$：\n$$\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = A \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + B (1) \\implies B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n从 $\\mathbf{h}_2 = A \\mathbf{h}_1 + B x_2$：\n$$\n\\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} = A \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} (2) \\implies A \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n$$\n这给出了线性方程： $a_{11} + a_{12} = 3$ 和 $a_{21} + a_{22} = 1$。\n\n从 $\\mathbf{h}_3 = A \\mathbf{h}_2 + B x_3$：\n$$\n\\begin{pmatrix} 12 \\\\ 4 \\end{pmatrix} = A \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} (1) \\implies A \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 12 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ 3 \\end{pmatrix}\n$$\n这给出了线性方程： $5a_{11} + 3a_{12} = 11$ 和 $5a_{21} + 3a_{22} = 3$。\n\n我们现在求解这两个关于 $A$ 的行的方程组：\n对于 $A$ 的第一行：\n1) $a_{11} + a_{12} = 3 \\implies a_{12} = 3 - a_{11}$\n2) $5a_{11} + 3a_{12} = 11 \\implies 5a_{11} + 3(3 - a_{11}) = 11 \\implies 2a_{11} + 9 = 11 \\implies 2a_{11}=2 \\implies a_{11}=1$。\n因此，$a_{12} = 3 - 1 = 2$。\n\n对于 $A$ 的第二行：\n1) $a_{21} + a_{22} = 1 \\implies a_{22} = 1 - a_{21}$\n2) $5a_{21} + 3a_{22} = 3 \\implies 5a_{21} + 3(1 - a_{21}) = 3 \\implies 2a_{21} + 3 = 3 \\implies 2a_{21}=0 \\implies a_{21}=0$。\n因此，$a_{22} = 1 - 0 = 1$。\n\n这给出了候选解：$A = \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix}$ 和 $B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。我们必须用剩余的数据点来验证这个解。\n对于 $t=4$：\n$$ \\mathbf{h}_4 = A \\mathbf{h}_3 + B x_4 = \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 12 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} (0) = \\begin{pmatrix} 12+8 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ 4 \\end{pmatrix} $$\n这与给定的 $\\mathbf{h}_4$ 相匹配。\n对于 $t=5$：\n$$ \\mathbf{h}_5 = A \\mathbf{h}_4 + B x_5 = \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 20 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} (-1) = \\begin{pmatrix} 20+8 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 28-1 \\\\ 4-1 \\end{pmatrix} = \\begin{pmatrix} 27 \\\\ 3 \\end{pmatrix} $$\n这与给定的 $\\mathbf{h}_5$ 相匹配。\n由于所提出的解完全拟合所有数据点，残差平方和为零。这是非负目标函数 $S(\\boldsymbol{\\theta})$ 的全局最小值。因此，这是最小二乘法提供的精确且唯一的解。\n\n恢复的矩阵是 $A = \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix}$ 和 $B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n\n**恢复唯一性的论证**\n最小二乘解 $\\hat{\\boldsymbol{\\theta}} = (Z^T Z)^{-1} Z^T Y$ 的唯一性取决于矩阵 $Z^T Z$ 的可逆性。该矩阵可逆当且仅当设计矩阵 $Z$ 具有满列秩。在我们的例子中，$Z$ 是一个 $5 \\times 3$ 的矩阵，所以它必须有秩3。这意味着它的三列必须是线性无关的。\n\n从线性系统的第一性原理角度看，从数据中唯一辨识系统参数 $(A, B)$ 的能力依赖于两个关键属性：输入信号的丰富性以及系统本身的结构特性，即可控性和可观测性。\n\n1.  **可观测性（Observability）**：如果一个系统的内部状态可以从其输出中唯一确定，则该系统是可观测的。对于一个输出方程为 $\\mathbf{y}_t = C \\mathbf{h}_t$ 的系统，其标准可观测性矩阵是 $\\mathcal{O} = \\begin{pmatrix} C \\\\ CA \\\\ \\dots \\\\ CA^{n-1} \\end{pmatrix}$。在这个问题中，输出就是隐藏状态本身，这相当于设置输出矩阵 $C=I$（单位矩阵）。可观测性矩阵为 $\\mathcal{O} = \\begin{pmatrix} I \\\\ A \\\\ \\dots \\\\ A^{n-1} \\end{pmatrix}$。由于该矩阵包含单位块 $I$，它的列保证是线性无关的，并且其秩为 $n=2$。因此，任何状态被直接测量的线性系统都是平凡可观测的。\n\n2.  **可控性（Controllability）和持续激励（Persistent Excitation）**：可控性确保输入 $x_t$ 能够将状态向量 $\\mathbf{h}_t$ 驱动到状态空间中的任意一点。它由可控性矩阵 $\\mathcal{C}_n = \\begin{pmatrix} B  AB  \\dots  A^{n-1}B \\end{pmatrix}$ 的秩决定。如果一个系统不可控，其部分内部动态将不受输入影响，状态向量 $\\mathbf{h}_t$ 将被限制在一个子空间内。即使有直接的状态观测，如果我们不能激励系统的所有模式，我们就无法学习控制这些模式的参数。同时，输入序列 $\\{x_t\\}$ 必须是“持续激励的”，意味着它必须足够丰富以防止简并情况。例如，一个恒定的输入可能不足以区分 $A$ 和 $B$ 的效应。可控性和持续激励的输入共同确保回归变量向量 $\\mathbf{z}_t^T = (\\mathbf{h}_{t-1}^T, x_t)$ 能够充分张成回归变量空间，从而使设计矩阵 $Z$ 的列线性无关。\n\n由输入和输出数据构建块汉克尔矩阵是子空间辨识方法（例如 N4SID）的基石，这些方法旨在解决从输入-输出对 $(x_t, y_t)$ 辨识 $(A, B, C)$ 这一更普遍的问题。这些汉克尔矩阵的某个投影的秩揭示了系统阶数 $n$，并且这个秩条件与可观测性矩阵和可控性矩阵的乘积有根本的联系。在我们的特殊情况下，由于可观测性得到保证（$C=I$），可辨识性条件得以简化，并更直接地通过要求矩阵 $Z$ 具有满秩来表达。\n\n对于我们的具体问题，我们已经求得 $A = \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix}$ 和 $B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。可控性矩阵为 $\\mathcal{C}_2 = \\begin{pmatrix} B  AB \\end{pmatrix} = \\begin{pmatrix} 1  3 \\\\ 1  1 \\end{pmatrix}$，其行列式为 $1-3 = -2 \\neq 0$。该系统是可控的。输入序列 $\\{x_t\\}$ 和产生的状态 $\\{ \\mathbf{h}_t \\}$ 生成了一个设计矩阵 $Z$，其列是线性无关的，从而使得 $Z^T Z$ 可逆，并保证了 $(A, B)$ 的唯一解。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  2  0  1  1  1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在学会如何辨识线性系统的参数之后，让我们进入更复杂也更真实的非线性 RNN 世界。激活函数的选择会极大地改变网络的动态行为。这个练习  让你探索一个带有 ReLU 激活函数的 RNN 的动态特性，揭示“激活稀疏性”如何导致隐藏状态被“困”在一个低维空间中。理解这种现象对于分析和训练现代深度网络至关重要，它让你直观地感受到非线性如何创造出丰富甚至出乎意料的系统行为。",
            "id": "3192158",
            "problem": "考虑一个具有二维隐藏状态和修正线性单元 (ReLU) 非线性函数的离散时间循环神经网络 (RNN)。设其递归关系由对预激活值逐元素应用 ReLU (修正线性单元) 函数来定义，因此隐藏状态的更新由下式给出\n$$\n\\mathbf{h}_{t}=\\phi\\!\\left(\\mathbf{W}\\mathbf{h}_{t-1}+\\mathbf{U}u+\\mathbf{b}\\right),\n$$\n其中，$\\phi$ 逐分量作用，形式为 $\\phi(z)=\\max(0,z)$，$\\mathbf{h}_{t}\\in\\mathbb{R}^{2}$ 是在时间 $t$ 的隐藏状态，$\\mathbf{W}\\in\\mathbb{R}^{2\\times 2}$，$\\mathbf{U}\\in\\mathbb{R}^{2\\times 1}$，$\\mathbf{b}\\in\\mathbb{R}^{2}$，且输入是在每个时间步都施加的恒定标量 $u\\in\\mathbb{R}$。参数和初始隐藏状态为\n$$\n\\mathbf{W}=\\begin{pmatrix}0.6  -0.4\\\\ -0.5  0.2\\end{pmatrix},\\quad\n\\mathbf{U}=\\begin{pmatrix}0.3\\\\ 0.1\\end{pmatrix},\\quad\n\\mathbf{b}=\\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix},\\quad\n\\mathbf{h}_{0}=\\begin{pmatrix}0.1\\\\ 0\\end{pmatrix}.\n$$\n定义一维锥：\n$$\n\\mathcal{C}=\\left\\{(h_{1},h_{2})\\in\\mathbb{R}^{2}\\,:\\,h_{1}0,\\ h_{2}=0\\right\\}.\n$$\n从 RNN 递归和 ReLU 非线性函数的基本定义出发，分析激活稀疏性（由 ReLU 产生的零值）如何随时间持续存在，并推导隐藏状态在所有 $t\\geq 1$ 的情况下都被困在低维锥 $\\mathcal{C}$ 中的条件。然后，计算最大的实数 $u$，使得在每个步骤施加恒定输入 $u$ 的情况下，隐藏状态对所有 $t\\geq 1$ 都满足 $\\mathbf{h}_{t}\\in\\mathcal{C}$。最终答案必须是一个实数。不需要四舍五入。",
            "solution": "用户要求我找到最大的实数 $u$，使得对于给定的循环神经网络 (RNN) 的隐藏状态 $\\mathbf{h}_t$，在所有时间步 $t \\geq 1$ 都被约束在一个特定的锥 $\\mathcal{C}$ 中。\n\n### 步骤 1：问题验证\n\n**1.1. 提取已知条件**\n- **递归关系**：$\\mathbf{h}_{t}=\\phi\\!\\left(\\mathbf{W}\\mathbf{h}_{t-1}+\\mathbf{U}u+\\mathbf{b}\\right)$，其中 $\\phi(z)=\\max(0,z)$ (ReLU) 是逐元素应用的。\n- **状态向量**：$\\mathbf{h}_{t} = \\begin{pmatrix} h_{t,1} \\\\ h_{t,2} \\end{pmatrix} \\in\\mathbb{R}^{2}$。\n- **权重矩阵**：$\\mathbf{W}=\\begin{pmatrix}0.6  -0.4\\\\ -0.5  0.2\\end{pmatrix}$。\n- **输入矩阵**：$\\mathbf{U}=\\begin{pmatrix}0.3\\\\ 0.1\\end{pmatrix}$。\n- **偏置向量**：$\\mathbf{b}=\\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix}$。\n- **输入**：$u \\in \\mathbb{R}$，对所有 $t$ 恒定。\n- **初始状态**：$\\mathbf{h}_{0}=\\begin{pmatrix}0.1\\\\ 0\\end{pmatrix}$。\n- **目标锥**：$\\mathcal{C}=\\left\\{(h_{1},h_{2})\\in\\mathbb{R}^{2}\\,:\\,h_{1}0,\\ h_{2}=0\\right\\}$。\n- **条件**：找到最大的 $u$ 使得对所有 $t \\geq 1$ 都有 $\\mathbf{h}_{t} \\in \\mathcal{C}$。\n\n**1.2. 使用提取的已知条件进行验证**\n该问题是一个定义明确的练习，旨在分析离散时间切换线性系统的动力学，这是带有 ReLU 激活函数的 RNN 的模型。\n- **科学依据**：该问题基于深度学习和动力系统理论的标准定义。\n- **适定性**：它要求在满足一组可推导约束的条件下，求一个参数的最大值。问题陈述是自洽的，并提供了确定唯一解所需的所有信息。\n- **目标**：所有术语都经过数学定义，目标是计算一个单一的数值。\n\n该问题有效，因为它没有违反任何指定的标准。我将继续进行解答。\n\n### 步骤 2：推导状态限制的条件\n\n条件 $\\mathbf{h}_{t} \\in \\mathcal{C}$ 对所有 $t \\geq 1$ 成立，意味着对于每个 $t \\geq 1$，隐藏状态向量 $\\mathbf{h}_t = \\begin{pmatrix} h_{t,1} \\\\ h_{t,2} \\end{pmatrix}$ 必须满足：\n1.  $h_{t,1}  0$\n2.  $h_{t,2} = 0$\n\n设时间 $t$ 的预激活向量为 $\\mathbf{z}_t = \\mathbf{W}\\mathbf{h}_{t-1}+\\mathbf{U}u+\\mathbf{b}$。则 $\\mathbf{h}_t = \\phi(\\mathbf{z}_t)$。\n逐元素的 ReLU 函数意味着：\n$h_{t,1} = \\max(0, z_{t,1})$\n$h_{t,2} = \\max(0, z_{t,2})$\n\n为了使 $\\mathbf{h}_t$ 在 $\\mathcal{C}$ 内，我们必须有：\n1.  $\\max(0, z_{t,1})  0 \\implies z_{t,1}  0$。在这种情况下，$h_{t,1} = z_{t,1}$。\n2.  $\\max(0, z_{t,2}) = 0 \\implies z_{t,2} \\leq 0$。\n\n因此，对于所有 $t \\geq 1$，我们必须满足 $z_{t,1}  0$ 和 $z_{t,2} \\leq 0$。\n\n### 步骤 3：对时间步 $t=1$ 的分析\n\n我们从初始状态 $\\mathbf{h}_0 = \\begin{pmatrix} 0.1 \\\\ 0 \\end{pmatrix}$ 开始。我们计算预激活 $\\mathbf{z}_1$：\n$$\n\\mathbf{z}_1 = \\mathbf{W}\\mathbf{h}_0 + \\mathbf{U}u + \\mathbf{b} = \\begin{pmatrix}0.6  -0.4\\\\ -0.5  0.2\\end{pmatrix} \\begin{pmatrix}0.1\\\\ 0\\end{pmatrix} + \\begin{pmatrix}0.3\\\\ 0.1\\end{pmatrix}u + \\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix}\n$$\n$$\n\\mathbf{z}_1 = \\begin{pmatrix}0.6 \\times 0.1\\\\ -0.5 \\times 0.1\\end{pmatrix} + \\begin{pmatrix}0.3u\\\\ 0.1u\\end{pmatrix} + \\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix} = \\begin{pmatrix}0.06 + 0.3u - 0.2\\\\ -0.05 + 0.1u - 0.05\\end{pmatrix} = \\begin{pmatrix}0.3u - 0.14\\\\ 0.1u - 0.1\\end{pmatrix}\n$$\n为了使 $\\mathbf{h}_1 \\in \\mathcal{C}$，我们需要 $z_{1,1}  0$ 和 $z_{1,2} \\leq 0$：\n1.  $z_{1,1} = 0.3u - 0.14  0 \\implies 0.3u  0.14 \\implies u  \\frac{0.14}{0.3} = \\frac{14}{30} = \\frac{7}{15}$。\n2.  $z_{1,2} = 0.1u - 0.1 \\leq 0 \\implies 0.1u \\leq 0.1 \\implies u \\leq 1$。\n\n因此，$u$ 的一个必要条件是 $\\frac{7}{15}  u \\leq 1$。\n如果此条件成立，则 $t=1$ 时的状态是 $\\mathbf{h}_1 = \\begin{pmatrix} z_{1,1} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0.3u - 0.14 \\\\ 0 \\end{pmatrix}$。\n\n### 步骤 4：对 $t \\geq 2$ 的归纳分析\n\n我们通过归纳法进行。假设对于某个 $t-1 \\geq 1$，状态在锥内，即 $\\mathbf{h}_{t-1} = \\begin{pmatrix} h_{t-1,1} \\\\ 0 \\end{pmatrix}$ 且 $h_{t-1,1}  0$。我们需要找到使 $\\mathbf{h}_t$ 也位于 $\\mathcal{C}$ 内的 $u$ 的条件。\n\n我们来计算 $\\mathbf{z}_t$：\n$$\n\\mathbf{z}_t = \\mathbf{W}\\mathbf{h}_{t-1} + \\mathbf{U}u + \\mathbf{b} = \\begin{pmatrix}0.6  -0.4\\\\ -0.5  0.2\\end{pmatrix} \\begin{pmatrix}h_{t-1,1}\\\\ 0\\end{pmatrix} + \\begin{pmatrix}0.3u\\\\ 0.1u\\end{pmatrix} + \\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix}\n$$\n$$\n\\mathbf{z}_t = \\begin{pmatrix} 0.6 h_{t-1,1} + 0.3u - 0.2 \\\\ -0.5 h_{t-1,1} + 0.1u - 0.05 \\end{pmatrix}\n$$\n为了使 $\\mathbf{h}_t \\in \\mathcal{C}$，我们需要 $z_{t,1}  0$ 和 $z_{t,2} \\leq 0$：\n1.  $z_{t,1}  0 \\implies h_{t,1} = 0.6 h_{t-1,1} + 0.3u - 0.2  0$。\n2.  $z_{t,2} \\leq 0 \\implies -0.5 h_{t-1,1} + 0.1u - 0.05 \\leq 0$。\n\n第二个不等式提供了 $h_{t-1,1}$ 的一个下界：\n$$\n0.5 h_{t-1,1} \\geq 0.1u - 0.05 \\implies h_{t-1,1} \\geq \\frac{0.1u - 0.05}{0.5} \\implies h_{t-1,1} \\geq 0.2u - 0.1\n$$\n这个不等式必须对所有 $t-1 \\geq 1$ 成立。因此，第一个隐藏状态分量的序列，我们称之为 $x_t = h_{t,1}$，必须对所有 $t \\geq 1$ 满足 $x_t \\geq 0.2u - 0.1$。\n\n$x_t$ 的动力学由第一个条件给出的线性递归关系决定：\n$x_t = 0.6 x_{t-1} + (0.3u - 0.2)$，对于 $t \\geq 2$，且 $x_1 = 0.3u - 0.14$。\n\n首先，考虑下界为非正的情况：$0.2u - 0.1 \\leq 0 \\implies u \\leq 0.5$。到目前为止，$u$ 的完整范围是 $\\frac{7}{15}  u \\leq 1$。如果 $u \\in (\\frac{7}{15}, 0.5]$，那么 $0.3u - 0.2$ 是负数。$x_t$ 递归关系的不动点是 $x_{fp} = \\frac{0.3u - 0.2}{1 - 0.6} = \\frac{0.3u - 0.2}{0.4} = 0.75u - 0.5$，当 $u \\leq 0.5$ 时，该不动点也是负数。序列 $x_t$ 从一个正值 $x_1  0$ 开始，收敛到一个负的不动点 $x_{fp}  0$。因此，$x_t$ 最终必然会变为负值，违反了条件 $x_t0$。因此，对于 $u \\leq 0.5$ 没有解。\n\n我们必须有 $u  0.5$。在这种情况下，$0.2u - 0.1  0$。条件 $x_t \\geq 0.2u-0.1$ 比 $x_t  0$ 更强且蕴含后者。所以我们只需要确保对所有 $t \\geq 1$ 都有 $x_t \\geq 0.2u - 0.1$。这等价于要求序列 $\\{x_t\\}_{t\\geq 1}$ 的最小值满足这个界：$\\min_{t \\geq 1} x_t \\geq 0.2u - 0.1$。\n\n序列 $x_t$ 是单调的，因为它是一个一阶线性递归。其行为取决于起始点 $x_1$ 和不动点 $x_{fp}$ 之间的关系。\n- $x_1 = 0.3u - 0.14$\n- $x_{fp} = 0.75u - 0.5$\n差值为 $x_1 - x_{fp} = (0.3u - 0.14) - (0.75u - 0.5) = -0.45u + 0.36 = 0.09(4-5u)$。\n差值的符号取决于 $u$ 是小于还是大于 $\\frac{4}{5} = 0.8$。\n\n**情况 1：$u  0.8$**\n在这种情况下，$x_1 - x_{fp}  0$，所以 $x_1  x_{fp}$。序列 $\\{x_t\\}$ 是单调递减的，并从上方收敛于 $x_{fp}$。\n最小值为下确界，即 $\\min_{t \\geq 1} x_t = \\inf_{t \\geq 1} x_t = x_{fp}$。\n条件变为 $x_{fp} \\geq 0.2u - 0.1$。\n$0.75u - 0.5 \\geq 0.2u - 0.1 \\implies 0.55u \\geq 0.4 \\implies u \\geq \\frac{0.4}{0.55} = \\frac{40}{55} = \\frac{8}{11}$。\n所以对于这种情况，我们需要 $u \\in [\\frac{8}{11}, 0.8)$。注意 $\\frac{8}{11} \\approx 0.727$ 与 $u  0.5$ 和 $u  7/15 \\approx 0.467$ 是一致的。\n\n**情况 2：$u  0.8$**\n在这种情况下，$x_1 - x_{fp}  0$，所以 $x_1  x_{fp}$。序列 $\\{x_t\\}$ 是单调递增的，并从下方收敛于 $x_{fp}$。\n最小值为第一项，即 $\\min_{t \\geq 1} x_t = x_1$。\n条件变为 $x_1 \\geq 0.2u - 0.1$。\n$0.3u - 0.14 \\geq 0.2u - 0.1 \\implies 0.1u \\geq 0.04 \\implies u \\geq 0.4$。\n对于这种情况，我们有 $u  0.8$。条件 $u \\geq 0.4$ 自动满足。我们还必须满足初始约束 $u \\leq 1$。所以这种情况给出的范围是 $u \\in (0.8, 1]$。\n\n**情况 3：$u = 0.8$**\n在这种情况下，$x_1 = x_{fp}$。序列是常数：对所有 $t\\geq 1$ 都有 $x_t = x_1$。\n$x_1 = 0.3(0.8) - 0.14 = 0.24 - 0.14 = 0.1$。\n条件是 $x_t \\geq 0.2u - 0.1$。\n$0.1 \\geq 0.2(0.8) - 0.1 = 0.16 - 0.1 = 0.06$。\n这是成立的。初始约束 $u \\in (\\frac{7}{15}, 1]$ 对于 $u=0.8$ 也满足。所以 $u=0.8$ 是一个有效的解。\n\n### 步骤 5：最终解\n\n结合所有情况的区间：\n- 来自情况 1：$u \\in [\\frac{8}{11}, 0.8)$\n- 来自情况 3：$u = 0.8$\n- 来自情况 2：$u \\in (0.8, 1]$\n\n这些集合的并集给出了 $u$ 的有效值的总范围：\n$$\nu \\in \\left[\\frac{8}{11}, 0.8\\right) \\cup \\{0.8\\} \\cup (0.8, 1] = \\left[\\frac{8}{11}, 1\\right]\n$$\n问题要求满足条件的最大实数 $u$。区间 $[\\frac{8}{11}, 1]$ 中的最大值是 $1$。",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "我们已经探讨了如何从数据中识别参数，以及非线性系统的前向传播会产生何种复杂动态。现在，我们来解决最核心的问题：神经网络究竟如何通过学习来调整其参数以最小化误差？答案在于计算梯度。这个练习  将指导你推导损失函数关于循环权重矩阵 $W_h$ 的梯度。它不仅揭示了“通过时间反向传播”（BPTT）算法的内在结构，还通过对比，让你明白为何直接计算闭式解在计算上是低效的，而实践中采用的递归算法则高效得多。",
            "id": "3192146",
            "problem": "考虑一个线性的循环神经网络 (RNN)，其隐藏状态维度为 $d_h$，输入维度为 $d_x$，输出维度为 $d_y$，由一个长度为 $T$ 的单一序列驱动。递推和读出由以下核心定义给出：\n$h_t = W_h h_{t-1} + W_x x_t$，其中 $h_0 = 0$，以及 $y_t = W_y h_t$，\n对于 $t \\in \\{1,2,\\dots,T\\}$，其中 $W_h \\in \\mathbb{R}^{d_h \\times d_h}$，$W_x \\in \\mathbb{R}^{d_h \\times d_x}$，以及 $W_y \\in \\mathbb{R}^{d_y \\times d_h}$。输入向量为 $\\{x_t \\in \\mathbb{R}^{d_x}\\}_{t=1}^T$，目标为 $\\{s_t \\in \\mathbb{R}^{d_y}\\}_{t=1}^T$。考虑二次损失：\n$L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2$。\n设每个 $t$ 的输出残差为 $e_t = y_t - s_t$。仅使用递推定义、读出定义、多元微积分中的链式法则以及标准的线性代数事实，推导梯度 $\\nabla_{W_h} L$ 的闭式解析表达式，该表达式应明确地将其分解为形如 $\\sum e_{\\cdot}\\, h_{\\cdot}^{\\top}$ 的时移相关项之和。然后，简要讨论在实现时计算该梯度的计算复杂度（使用大O符号表示）：(i) 通过对时移项求和直接从您的闭式表达式计算，以及 (ii) 通过时间反向传播（BPTT; Backpropagation Through Time）使用伴随信号的一阶递归计算。您的最终答案必须是仅用 $W_h$、$W_y$、$\\{e_t\\}$ 和 $\\{h_t\\}$ 表示的 $\\nabla_{W_h} L$ 的单个闭式解析矩阵表达式。不要包含任何数值计算。",
            "solution": "该问题被评估为有效。这是一个来自深度学习理论领域的、定义明确、有科学依据且客观的问题陈述。它要求推导一个标准线性循环神经网络（RNN）的梯度，这是一个基于多元微积分和线性代数的既定原则的形式化且可解的任务。所有必要的定义和条件都已提供。\n\n我们首先陈述问题中提供的核心定义：\n递推关系：$h_t = W_h h_{t-1} + W_x x_t$，对于 $t \\in \\{1, 2, \\dots, T\\}$，初始状态 $h_0 = 0$。\n读出方程：$y_t = W_y h_t$。\n损失函数：$L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2 = \\frac{1}{2} \\sum_{t=1}^T \\|e_t\\|_2^2$，其中 $e_t = y_t - s_t$。\n参数是矩阵 $W_h \\in \\mathbb{R}^{d_h \\times d_h}$、$W_x \\in \\mathbb{R}^{d_h \\times d_x}$ 和 $W_y \\in \\mathbb{R}^{d_y \\times d_h}$。输入是向量 $x_t \\in \\mathbb{R}^{d_x}$，目标是 $s_t \\in \\mathbb{R}^{d_y}$。\n\n我们的目标是推导损失函数 $L$ 相对于循环权重矩阵 $W_h$ 的梯度（表示为 $\\nabla_{W_h} L$）的闭式表达式。我们将使用矩阵微积分的链式法则。\n\n损失 $L$ 是 $W_h$ 的函数，通过其对隐藏状态 $\\{h_t\\}_{t=1}^T$ 的影响而实现。梯度可以表示为前向传播中每个使用 $W_h$ 的时间步的贡献之和：\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\nabla_{W_h}^{(\\text{local at } t)} L\n$$\n其中“局部”梯度指的是相对于在计算 $h_t = W_h h_{t-1} + W_x x_t$ 中使用的 $W_h$ 实例的梯度。使用链式法则，这个贡献由损失函数对状态 $h_t$ 的梯度与状态更新项对 $W_h$ 的梯度的外积给出。令 $\\delta_t = \\frac{\\partial L}{\\partial h_t} \\in \\mathbb{R}^{d_h}$ 为总损失相对于隐藏状态 $h_t$ 的梯度。预激活 $a_t = W_h h_{t-1} + W_x x_t$ 相对于 $W_h$ 的梯度是 $h_{t-1}^T$。因此，来自时间步 $t$ 的梯度贡献是 $\\delta_t h_{t-1}^T$。对所有时间步求和得到总梯度：\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\delta_t h_{t-1}^T\n$$\n这是 BPTT 中循环权重梯度的标准公式。然而，问题要求一个用给定数量 $W_h, W_y, \\{e_t\\}, \\{h_t\\}$ 表示的闭式表达式。为实现这一点，我们必须推导 $\\delta_t$ 的闭式形式。\n\n向量 $\\delta_t$ 表示 $h_t$ 对损失 $L$ 的总影响。状态 $h_t$ 通过所有后续时间步 $k \\ge t$ 的输出 $y_k$ 影响损失。\n$$\n\\delta_t = \\frac{\\partial L}{\\partial h_t} = \\sum_{k=t}^{T} \\frac{\\partial L}{\\partial y_k} \\frac{\\partial y_k}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_t}\n$$\n在这里，我们应用链式法则，对从 $h_t$ 到最终损失的所有路径求和。偏导数为（使用分子布局矩阵微积分约定，其中标量对列向量的梯度是列向量）：\n1. $\\frac{\\partial L}{\\partial y_k} = \\frac{\\partial}{\\partial y_k} \\left( \\frac{1}{2} \\sum_{j=1}^T \\|y_j - s_j\\|_2^2 \\right) = y_k - s_k = e_k$。这是一个大小为 $d_y$ 的列向量。\n2. $\\frac{\\partial y_k}{\\partial h_k} = W_y$。这是大小为 $d_y \\times d_h$ 的雅可比矩阵。链式法则中的项是其转置，$(\\frac{\\partial y_k}{\\partial h_k})^T = W_y^T$。\n3. $\\frac{\\partial h_k}{\\partial h_t}$。我们通过展开递推关系来找到它：\n$h_k = W_h h_{k-1} + W_x x_k = W_h (W_h h_{k-2} + W_x x_{k-1}) + W_x x_k = \\dots$\n当 $k > t$ 时，$h_k$ 对 $h_t$ 的依赖关系由下式给出：\n$h_k = W_h^{k-t} h_t + \\sum_{j=t+1}^{k} W_h^{k-j} W_x x_j$。\n因此雅可比矩阵是 $\\frac{\\partial h_k}{\\partial h_t} = W_h^{k-t}$（对于 $k \\ge t$）。链式法则组合中的项是其转置，$(W_h^{k-t})^T = (W_h^T)^{k-t}$。\n\n将这些导数代回到 $\\delta_t$ 的表达式中：\n$$\n\\delta_t = \\sum_{k=t}^{T} \\left( \\frac{\\partial h_k}{\\partial h_t} \\right)^T \\left( \\frac{\\partial y_k}{\\partial h_k} \\right)^T \\frac{\\partial L}{\\partial y_k} = \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k\n$$\n这提供了从所有未来时间步 $k \\ge t$ 反向传播的梯度信号 $\\delta_t$ 的闭式表达式。\n\n最后，我们将 $\\delta_t$ 的这个表达式代入我们的总梯度 $\\nabla_{W_h} L$ 的方程中：\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T\n$$\n这就是所要求的闭式解析表达式。它将梯度分解为关于时间的双重求和。每一项 $(W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$ 可以解释为未来时间 $k$ 的误差 $e_k$ 与过去时间 $t-1$ 的隐藏状态 $h_{t-1}$ 之间的“相关性”，该相关性由矩阵 $(W_h^T)^{k-t} W_y^T$ 调节，该矩阵解释了影响通过网络动态和读出层的传播。\n\n接下来，我们讨论计算复杂度。\n(i) 从闭式表达式直接求值：\n公式是 $\\sum_{t=1}^{T} \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$。\n假设前向传播已经运行，因此所有的 $e_t \\in \\mathbb{R}^{d_y}$ 和 $h_t \\in \\mathbb{R}^{d_h}$ 都是已知的。\n双重循环涉及 $O(T^2)$ 个项。对于每一项 $(t, k)$，计算涉及：\n- 一个矩阵幂 $(W_h^T)^{k-t}$，如果未预先计算，需要 $O((k-t) d_h^3)$。预计算所有必要的 $W_h^T$ 的幂（最高到 $T-1$ 次）需要 $O(T d_h^3)$。\n- 一个矩阵-向量积 $W_y^T e_k$，成本为 $O(d_h d_y)$。\n- 一个矩阵-向量积 $(W_h^T)^{k-t} (W_y^T e_k)$，成本为 $O(d_h^2)$。\n- 与 $h_{t-1}^T$ 的外积，成本为 $O(d_h^2)$。\n一个朴素的实现将非常昂贵。一个更优化的直接求值将按以下方式进行：\n`total_grad = 0`\n`for t = 1 to T:`\n  `inner_sum_vec = 0`\n  `for k = t to T:`\n    `vec = ... 计算 (W_h^T)^{k-t} W_y^T e_k ...`\n    `inner_sum_vec += vec`\n  `total_grad += outer(inner_sum_vec, h_{t-1})`\n最昂贵的部分是双重循环结构。大约有 $T^2/2$ 对 $(t,k)$。对于每一对，主要成本是计算矩阵幂与向量的乘积，假设幂已预先计算，则成本为 $O(d_h^2)$。预计算成本为 $O(T d_h^3)$。因此，总复杂度由嵌套循环和预计算主导，导致复杂度为 $O(T d_h^3 + T^2 d_h^2)$。对序列长度 $T$ 的二次依赖性使得这种方法对于长序列来说计算成本过高。\n\n(ii) 通过时间反向传播（BPTT）求值：\nBPTT通过使用动态规划避免了双重求和的显式计算。它通过一次时间上的反向传播来计算梯度。关键是 $\\delta_t$ 的递归关系：\n$\\delta_T = W_y^T e_T$\n$\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$ for $t = T-1, \\dots, 1$.\n用于计算 $\\nabla_{W_h} L$ 的BPTT算法是：\n1. 执行一次从 $t=1$ 到 $T$ 的前向传播，来计算并存储所有的 $h_t$ 和 $e_t$。成本：$O(T(d_h^2 + d_h d_x + d_y d_h))$。\n2. 初始化 $\\nabla_{W_h} L = 0$ 和 $\\delta_{T+1} = 0$。\n3. 从 $t=T$ 向下到 $1$ 执行一次反向传播：\n   a. 计算 $\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$。这涉及两个矩阵-向量积和一个向量加法，成本为 $O(d_h d_y + d_h^2)$。\n   b. 将贡献加到梯度上：$\\nabla_{W_h} L \\leftarrow \\nabla_{W_h} L + \\delta_t h_{t-1}^T$。这是一个外积和矩阵加法，成本为 $O(d_h^2)$。\n反向传播包含一个长度为 $T$ 的单循环，每一步的成本为 $O(d_h^2 + d_h d_y)$。反向传播的总复杂度为 $O(T(d_h^2 + d_h d_y))$。\n因此，BPTT的总复杂度是 $O(T(d_h^2 + d_h d_x + d_y d_h))$，它在序列长度 $T$ 上是线性的。这比直接求值闭式表达式要高效得多。",
            "answer": "$$\n\\boxed{\\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T}\n$$"
        }
    ]
}