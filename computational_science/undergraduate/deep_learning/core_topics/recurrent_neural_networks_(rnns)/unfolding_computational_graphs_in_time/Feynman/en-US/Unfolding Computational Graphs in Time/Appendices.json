{
    "hands_on_practices": [
        {
            "introduction": "Training recurrent models requires unfolding the computational graph through time, which can become prohibitively expensive for long sequences. A common practical solution is Truncated Backpropagation Through Time (TBPTT), which limits the backpropagation to a fixed number of steps, typically denoted by $k$. This exercise provides a stark counterexample to demonstrate a critical weakness of this method: its inability to capture dependencies that span longer than the truncation window. By calculating the gradient for an input at a time lag $\\tau \\gt k$, you will prove that the gradient signal vanishes completely, illustrating a core aspect of why learning long-range dependencies is challenging. ",
            "id": "3101258",
            "problem": "Consider a scalar recurrent neural network defined by the recurrence\n$$h_{t} \\;=\\; a\\,h_{t-1} \\;+\\; b\\,x_{t}, \\qquad y_{t} \\;=\\; c\\,h_{t},$$\nwhere $a$, $b$, and $c$ are fixed nonzero real scalars, and $\\{x_{s}\\}$ is a scalar input sequence. Suppose the training objective at time $t$ is a single-step squared error\n$$L \\;=\\; \\frac{1}{2}\\,\\big(y_{t} - r\\big)^{2},$$\nwhere $r$ is a fixed scalar target for time $t$. Assume the initial condition $h_{t-\\tau-1} = 0$ and that all inputs $\\{x_{s}\\}$ for $s \\le t-\\tau-1$ are zero, so that any dependency of $y_{t}$ on $x_{t-\\tau}$ arises only through the recurrence.\n\nYou train this model with Truncated Backpropagation Through Time (TBPTT), unrolling only $k$ steps, that is, you backpropagate from time $t$ to time $t-k+1$, treating $h_{t-k}$ as a constant with respect to all variables earlier than $t-k+1$. Let $\\tau$ be a positive integer with $\\tau > k \\ge 1$.\n\nTasks:\n- Using only the chain rule and the definitions above, determine the truncated gradient $\\frac{\\partial L}{\\partial x_{t-\\tau}}$ computed by TBPTT with window $k$ when $\\tau>k$.\n- Briefly explain why this constitutes a counterexample showing that TBPTT with window $k$ fails to learn a dependency at lag $\\tau>k$ even though such a dependency exists under the forward recurrence.\n- Briefly discuss at least two principled remedies that address this failure mode without violating the TBPTT truncation constraint.\n\nGive your final answer for the truncated gradient $\\frac{\\partial L}{\\partial x_{t-\\tau}}$ as a single exact value or expression. No rounding is required.",
            "solution": "The problem asks for the computation of a truncated gradient in a simple recurrent neural network (RNN), an explanation of its implications, and a discussion of remedies.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- Recurrent Neural Network (RNN) definition:\n  - Hidden state update: $h_{t} = a\\,h_{t-1} + b\\,x_{t}$\n  - Output equation: $y_{t} = c\\,h_{t}$\n- Parameters: $a$, $b$, and $c$ are fixed nonzero real scalars.\n- Input: $\\{x_{s}\\}$ is a scalar input sequence.\n- Loss function at time $t$: $L = \\frac{1}{2}(y_{t} - r)^{2}$, where $r$ is a fixed scalar target.\n- Initial conditions: $h_{t-\\tau-1} = 0$ and $x_{s} = 0$ for $s \\le t-\\tau-1$.\n- Truncated Backpropagation Through Time (TBPTT) definition: Backpropagation from time $t$ is unrolled for $k$ steps (from $t$ to $t-k+1$), and $h_{t-k}$ is treated as a constant with respect to all variables at time steps earlier than $t-k+1$.\n- Constraints on indices: $\\tau$ and $k$ are positive integers with $\\tau > k \\ge 1$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, formulated with standard definitions from the field of deep learning. The RNN model is a simplified but valid representation. The TBPTT algorithm is described in a conventional manner. The question is well-posed, asking for a specific calculation and a conceptual analysis based on it. The language is objective and precise. All necessary information is provided, and there are no contradictions. The problem is a standard exercise in understanding the mechanics and limitations of training RNNs.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n**Solution**\n\n**Part 1: Determine the truncated gradient $\\frac{\\partial L}{\\partial x_{t-\\tau}}$**\n\nThe training of the RNN uses Truncated Backpropagation Through Time (TBPTT) with a history truncation of $k$ steps. This means that for a loss $L$ calculated at time $t$, the computational graph for the gradient calculation is unrolled backward in time from step $t$ to step $t-k+1$. The hidden state $h_{t-k}$ is treated as an input to this truncated graph, and its dependency on previous states ($h_{t-k-1}$) and previous inputs ($x_{t-k}$, $x_{t-k-1}$, etc.) is severed.\n\nThe loss $L$ is a function of the output $y_t$, which in turn is a function of the sequence of hidden states and inputs within the truncation window. Specifically, the unrolled computation shows that $y_t$ (and thus $L$) is a function of the variables $\\{h_{t-k}, x_{t-k+1}, x_{t-k+2}, \\dots, x_t\\}$. Let's denote the function computed by this truncated graph as $L_{\\text{trunc}}$.\n$$L = L_{\\text{trunc}}(h_{t-k}, x_{t-k+1}, \\dots, x_t)$$\nWe are asked to compute the truncated gradient $\\frac{\\partial L}{\\partial x_{t-\\tau}}$. The variable in question is the input at time $t-\\tau$.\n\nThe problem states that $\\tau > k \\ge 1$. This implies that the time index of the variable $x_{t-\\tau}$ satisfies $t-\\tau < t-k$.\nThe set of input variables that are part of the truncated computational graph is $\\{x_s | t-k+1 \\le s \\le t\\}$.\nSince $t-\\tau < t-k < t-k+1$, the variable $x_{t-\\tau}$ does not appear in the expression for the loss function $L$ defined over the truncated graph.\n\nThe derivative of a function with respect to a variable that does not affect it is zero. Formally, since $x_{t-\\tau}$ is not an argument of $L_{\\text{trunc}}(h_{t-k}, x_{t-k+1}, \\dots, x_t)$, its partial derivative is:\n$$ \\frac{\\partial L}{\\partial x_{t-\\tau}} = \\frac{\\partial L_{\\text{trunc}}}{\\partial x_{t-\\tau}} = 0 $$\nTherefore, the gradient computed by TBPTT with a window of $k$ for an input at a lag of $\\tau > k$ is exactly zero.\n\n**Part 2: Explanation as a Counterexample**\n\nThe fact that the computed gradient is zero is a counterexample showing the limitations of TBPTT. We can demonstrate that a true dependency of $L$ on $x_{t-\\tau}$ exists in the full, untruncated model.\n\nThe hidden state $h_t$ can be expressed by unrolling the recurrence relation:\n$h_s = a h_{s-1} + b x_s$. By repeatedly substituting, we obtain:\n$$h_t = a h_{t-1} + b x_t = a(a h_{t-2} + b x_{t-1}) + b x_t = a^2 h_{t-2} + a b x_{t-1} + b x_t$$\nGeneralizing this for $j$ steps gives:\n$$h_t = a^j h_{t-j} + \\sum_{i=0}^{j-1} a^i b x_{t-i}$$\nTo find the dependency on $x_{t-\\tau}$, we unroll $\\tau+1$ steps to reach the initial condition $h_{t-\\tau-1}=0$:\n$$h_t = a^{\\tau+1} h_{t-\\tau-1} + \\sum_{i=0}^{\\tau} a^i b x_{t-i}$$\nGiven $h_{t-\\tau-1}=0$, this simplifies to:\n$$h_t = \\sum_{i=0}^{\\tau} a^i b x_{t-i} = b x_t + a b x_{t-1} + \\dots + a^{\\tau} b x_{t-\\tau}$$\nNow, we can compute the true partial derivative of $h_t$ with respect to $x_{t-\\tau}$:\n$$ \\frac{\\partial h_t}{\\partial x_{t-\\tau}} = a^{\\tau} b $$\nSince $a$ and $b$ are given as nonzero, this derivative is nonzero. The full, untruncated gradient $\\frac{\\partial L}{\\partial x_{t-\\tau}}$ is then found using the chain rule:\n$$ \\frac{\\partial L}{\\partial x_{t-\\tau}} = \\frac{\\partial L}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial x_{t-\\tau}} = (y_t - r) \\cdot c \\cdot (a^{\\tau} b) $$\nThis true gradient is generally nonzero.\n\nThe discrepancy between the TBPTT-computed gradient ($0$) and the true gradient (nonzero) demonstrates the failure. TBPTT makes the learning algorithm \"blind\" to dependencies that span longer than $k$ time steps. Because the gradient signal is zero, the model's parameters (or, in this case, its inputs, which serves as a proxy for how it would learn weights) cannot be updated to reduce the error $L$ based on events that occurred at time $t-\\tau$. This inability to capture long-range dependencies is a fundamental problem of naive TBPTT.\n\n**Part 3: Principled Remedies**\n\nHere are two principled remedies that address this failure mode without violating the core constraint of using a truncated backward pass.\n\n1.  **Gated RNN Architectures (e.g., LSTM, GRU):** The failure of the simple RNN is partly due to its architecture, where information is continuously mixed and transformed at every time step, making it hard to preserve specific information over long durations. Architectures like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) introduce gating mechanisms. An LSTM, for example, maintains a separate cell state, $C_t$, which acts as an information highway. Its update rule is $C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$, where $f_t$ is a \"forget gate\". If the network learns to set $f_t \\approx 1$ and the input gate $i_t \\approx 0$, the information in $C_{t-1}$ can pass to $C_t$ almost unchanged. This allows information about a distant input $x_{t-\\tau}$ to be carried forward with minimal degradation to the state at time $t-k$. When TBPTT is then applied, the initial state for the backpropagation, $h_{t-k}$ (and its associated cell state $C_{t-k}$), contains a much more meaningful summary of the distant past. The algorithm can then learn to associate this summary with the error at time $t$, effectively learning long-range dependencies despite the truncation of the gradient path itself.\n\n2.  **Synthetic Gradients:** This method directly attacks the problem of the missing gradient signal at the truncation point. Instead of assuming the gradient flowing into $h_{t-k}$ from the future is zero, we can train a separate model to predict this gradient. Let the true gradient of the total future loss with respect to the state be $\\delta_{t-k} = \\frac{\\partial L_{>t-k}}{\\partial h_{t-k}}$. The idea is to train a model $M$ that approximates this gradient: $\\hat{\\delta}_{t-k} = M(h_{t-k})$. During backpropagation from time $t$, when we reach the boundary state $h_{t-k}$, instead of stopping, we use $\\hat{\\delta}_{t-k}$ as the incoming gradient to continue the backpropagation into the past. This allows an estimate of the error signal to propagate across the entire sequence, even though the forward and backward passes are decoupled at intervals of $k$ steps. This approach allows for asynchronous or parallel training of different segments of the unrolled network.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "The failure of simple RNNs to capture long-range dependencies, as seen previously, motivates the search for architectures with better memory properties. The leaky-integrator RNN is a step in this direction, featuring a mechanism to explicitly balance retaining old information with incorporating new inputs using a parameter $\\alpha$. This practice will have you derive a 'memory time constant' for such a network, providing a tangible link between its architectural parameters and its capacity to remember information over time. By linearizing the dynamics and unfolding the gradient calculation, you will quantify exactly how long a piece of information can persist. ",
            "id": "3197405",
            "problem": "Consider a leaky-integrator Recurrent Neural Network (RNN) with hidden state dynamics given by\n$$h_{t+1} = (1-\\alpha) h_t + \\alpha \\,\\phi(W h_t + U x_t),$$\nwhere $h_t \\in \\mathbb{R}$, $x_t \\in \\mathbb{R}$, $W \\in \\mathbb{R}$, $U \\in \\mathbb{R}$, $\\alpha \\in (0,1)$, and $\\phi$ is a twice continuously differentiable activation function. You will analyze gradient flow by unfolding the computational graph in time and connect the result to memory versus stability.\n\nAssume the scalar case with $\\phi(z) = \\tanh(z)$, and the input is identically zero, $x_t = 0$ for all $t \\in \\{0,1,\\dots,T-1\\}$. Let the loss be\n$$L = \\frac{1}{2} h_T^2.$$\nAssume the trajectory remains in a neighborhood of the origin so that linearization around $h_t \\approx 0$ is valid for all $t$, and further assume that $|(1-\\alpha) + \\alpha W| < 1$ so that the linearized hidden dynamics are stable.\n\nStarting only from the chain rule and the definition of the derivative, derive the time-unfolded expression for $\\frac{\\partial h_T}{\\partial h_0}$ and use it to define a characteristic memory time constant $\\tau$ (in time steps) by the relation\n$$\\left|\\frac{\\partial h_T}{\\partial h_0}\\right| \\approx \\exp\\!\\left(-\\frac{T}{\\tau}\\right).$$\nExpress $\\tau$ in closed form as a function of $\\alpha$ and $W$. Your final answer must be a single analytical expression. No rounding is required.",
            "solution": "The problem asks for the characteristic memory time constant $\\tau$ of a leaky-integrator Recurrent Neural Network (RNN) under specific assumptions. The derivation proceeds by first linearizing the system's dynamics, then calculating the derivative of the final state with respect to the initial state by unfolding the computational graph in time, and finally using the given definition of $\\tau$.\n\nThe hidden state dynamics are given by:\n$$h_{t+1} = (1-\\alpha) h_t + \\alpha \\,\\phi(W h_t + U x_t)$$\nThe problem specifies the following conditions: the activation function is $\\phi(z) = \\tanh(z)$, and the input is identically zero, $x_t = 0$ for all time steps $t$. Substituting these into the dynamics equation gives:\n$$h_{t+1} = (1-\\alpha) h_t + \\alpha \\tanh(W h_t)$$\nThe problem further assumes that the system's trajectory remains in a neighborhood of the origin, such that $h_t \\approx 0$ for all $t$. This allows for linearization of the dynamics. We use the Taylor series expansion of the $\\tanh$ function around $z=0$:\n$$\\tanh(z) = z - \\frac{z^3}{3} + O(z^5)$$\nFor values of $z$ close to $0$, we can use the first-order approximation $\\tanh(z) \\approx z$. In our case, the argument to the $\\tanh$ function is $W h_t$. Since $h_t \\approx 0$, it follows that $W h_t \\approx 0$. We can therefore approximate $\\tanh(W h_t) \\approx W h_t$.\n\nSubstituting this approximation into the dynamics equation yields the linearized dynamics:\n$$h_{t+1} \\approx (1-\\alpha) h_t + \\alpha(W h_t)$$\n$$h_{t+1} \\approx ((1-\\alpha) + \\alpha W) h_t$$\nThis is a linear recurrence relation. The problem states that the linearized dynamics are stable, which is guaranteed by the given condition $|(1-\\alpha) + \\alpha W| < 1$.\n\nThe next step is to derive the time-unfolded expression for the derivative $\\frac{\\partial h_T}{\\partial h_0}$. The state $h_T$ is a function of the initial state $h_0$ through $T$ recursive applications of the update rule. Using the chain rule, we can express this derivative as a product of single-step derivatives:\n$$\\frac{\\partial h_T}{\\partial h_0} = \\frac{\\partial h_T}{\\partial h_{T-1}} \\frac{\\partial h_{T-1}}{\\partial h_{T-2}} \\cdots \\frac{\\partial h_2}{\\partial h_1} \\frac{\\partial h_1}{\\partial h_0} = \\prod_{t=0}^{T-1} \\frac{\\partial h_{t+1}}{\\partial h_t}$$\nTo calculate the general term $\\frac{\\partial h_{t+1}}{\\partial h_t}$, we differentiate the full, non-linear dynamics equation with respect to $h_t$:\n$$\\frac{\\partial h_{t+1}}{\\partial h_t} = \\frac{\\partial}{\\partial h_t} \\left[ (1-\\alpha) h_t + \\alpha \\tanh(W h_t) \\right]$$\n$$\\frac{\\partial h_{t+1}}{\\partial h_t} = (1-\\alpha) + \\alpha \\frac{\\partial}{\\partial h_t}(\\tanh(W h_t))$$\nUsing the chain rule and the fact that the derivative of $\\tanh(z)$ is $1 - \\tanh^2(z)$, we get:\n$$\\frac{\\partial}{\\partial h_t}(\\tanh(W h_t)) = W(1 - \\tanh^2(W h_t))$$\nThus, the exact derivative is:\n$$\\frac{\\partial h_{t+1}}{\\partial h_t} = (1-\\alpha) + \\alpha W (1 - \\tanh^2(W h_t))$$\nNow, we apply the linearization assumption, $h_t \\approx 0$. As $h_t$ approaches $0$, $W h_t$ also approaches $0$, and consequently $\\tanh(W h_t)$ approaches $0$. The term $\\tanh^2(W h_t)$ therefore vanishes. The derivative simplifies to a constant:\n$$\\frac{\\partial h_{t+1}}{\\partial h_t} \\approx (1-\\alpha) + \\alpha W (1 - 0) = (1-\\alpha) + \\alpha W$$\nThis constant is the effective recurrent weight of the linearized system. Let's denote it by $J = (1-\\alpha) + \\alpha W$. Substituting this constant approximation back into the product for the full derivative:\n$$\\frac{\\partial h_T}{\\partial h_0} \\approx \\prod_{t=0}^{T-1} J = J^T$$\nThis gives the final time-unfolded expression for the derivative:\n$$\\frac{\\partial h_T}{\\partial h_0} \\approx ((1-\\alpha) + \\alpha W)^T$$\nThe problem defines the characteristic memory time constant $\\tau$ through the relation:\n$$\\left|\\frac{\\partial h_T}{\\partial h_0}\\right| \\approx \\exp\\left(-\\frac{T}{\\tau}\\right)$$\nSubstituting our derived expression for the derivative:\n$$\\left| ((1-\\alpha) + \\alpha W)^T \\right| \\approx \\exp\\left(-\\frac{T}{\\tau}\\right)$$\nUsing the property $|z^n| = |z|^n$:\n$$|(1-\\alpha) + \\alpha W|^T \\approx \\exp\\left(-\\frac{T}{\\tau}\\right)$$\nTo solve for $\\tau$, we take the natural logarithm of both sides:\n$$\\ln\\left( |(1-\\alpha) + \\alpha W|^T \\right) \\approx \\ln\\left( \\exp\\left(-\\frac{T}{\\tau}\\right) \\right)$$\n$$T \\ln(|(1-\\alpha) + \\alpha W|) \\approx -\\frac{T}{\\tau}$$\nAssuming $T$ is a non-zero integer representing the number of time steps, we can divide both sides by $T$:\n$$\\ln(|(1-\\alpha) + \\alpha W|) \\approx -\\frac{1}{\\tau}$$\nFinally, solving for $\\tau$ gives the closed-form expression:\n$$\\tau \\approx -\\frac{1}{\\ln(|(1-\\alpha) + \\alpha W|)}$$\nBased on this derivation under the given approximations, the expression for the time constant is:\n$$\\tau = -\\frac{1}{\\ln(|(1-\\alpha) + \\alpha W|)}$$\nThe stability condition $|(1-\\alpha) + \\alpha W| < 1$ ensures that the argument of the logarithm is a positive number less than $1$, which makes the logarithm negative. The negative sign in the expression for $\\tau$ thus ensures that $\\tau$ is a positive quantity, as expected for a time constant.",
            "answer": "$$\\boxed{-\\frac{1}{\\ln(|(1-\\alpha) + \\alpha W|)}}$$"
        },
        {
            "introduction": "While architectural improvements can help gradients flow across long time intervals, they don't solve a critical practical issue: the immense memory cost of storing all intermediate activations for backpropagation. Reversible models offer a brilliant solution by enabling the exact reconstruction of past activations during the backward pass, trading a modest amount of re-computation for a massive reduction in memory. This exercise quantifies this trade-off, revealing how reversible architectures make training on extremely long sequences feasible by breaking the linear scaling of memory with sequence length. You will analyze the memory savings factor $S$ and the computational overhead factor $O$ for a reversible recurrent block. ",
            "id": "3197412",
            "problem": "You are asked to analyze activation memory and floating-point operation counts when unfolding a computational graph in time for a reversible recurrent layer that allows exact backpropagation without storing intermediate hidden states. Begin from the fundamental definition of a computational graph, the chain rule of derivatives, and the accepted floating-point operation counts for dense linear algebra. Specifically, use the following base facts: (i) the chain rule implies that reverse-mode automatic differentiation (backpropagation) over an unfolded time graph requires access to forward activations at each time step to evaluate local Jacobian-vector products, (ii) a matrix-vector multiply of shape $n \\times m$ with a batch of $B$ vectors costs approximately $2 B n m$ floating-point operations, and (iii) for a linear layer $y = W x$ with batch size $B$, the backward pass to compute gradients with respect to $W$ and $x$ costs approximately $4 B n m$ floating-point operations.\n\nConsider a reversible recurrent block with hidden state $h_t \\in \\mathbb{R}^H$ split as $h_t = (a_t, b_t)$ where $a_t \\in \\mathbb{R}^{H/2}$ and $b_t \\in \\mathbb{R}^{H/2}$ for even $H$. The forward recurrence at time $t$ is defined by additive coupling:\n$$\n\\begin{aligned}\na_{t+1} &= a_t + F(b_t), \\\\\nb_{t+1} &= b_t + G(a_{t+1}),\n\\end{aligned}\n$$\nwhere $F$ and $G$ are linear maps with parameter matrices $W_f \\in \\mathbb{R}^{(H/2) \\times (H/2)}$ and $W_g \\in \\mathbb{R}^{(H/2) \\times (H/2)}$ respectively, so that $F(b) = W_f b$ and $G(a) = W_g a$. This block is invertible with the inverse given by\n$$\n\\begin{aligned}\nb_t &= b_{t+1} - G(a_{t+1}), \\\\\na_t &= a_{t+1} - F(b_t).\n\\end{aligned}\n$$\nAssume a batch size $B$, time steps $T$, and that we measure memory only for activations of hidden states used by backpropagation, not including parameters or optimizer states. Assume each scalar activation is stored in $s$ bytes with $s = 4$ (single-precision floating-point). Inputs are available externally and not counted toward activation memory.\n\nTasks to implement:\n- Derive the total activation memory required by standard backpropagation through time ($M_{\\text{std}}$) and the reversible method ($M_{\\text{rev}}$). Report the memory savings factor $S = M_{\\text{std}} / M_{\\text{rev}}$.\n- Using the accepted floating-point operation counts, derive expressions for the per-step forward cost ($C_{\\text{fwd}}$), backward cost ($C_{\\text{bwd}}$), and inversion cost ($C_{\\text{inv}}$) for the reversible block. The inversion cost is the computational cost to reconstruct $h_{t-1}$ from $h_t$.\n- The total computational cost over $T$ steps for standard BPTT is $T \\cdot (C_{\\text{fwd}} + C_{\\text{bwd}})$, and for the reversible method it is $T \\cdot (C_{\\text{fwd}} + C_{\\text{bwd}} + C_{\\text{inv}})$. Report the compute overhead factor\n$$\nO = \\frac{C_{\\text{fwd}} + C_{\\text{bwd}} + C_{\\text{inv}}}{C_{\\text{fwd}} + C_{\\text{bwd}}}.\n$$\n\nProgramming task:\n- Write a program that, for each test case $(B,H,T)$ in the test suite below, computes and returns a list containing four numbers: $M_{\\text{std}}$ in bytes (integer), $M_{\\text{rev}}$ in bytes (integer), $S$ as a float, and $O$ as a float.\n- Use $s = 4$ bytes.\n- The test suite consists of the following parameter sets, each with even $H$:\n    - $(B,H,T) = (32, 512, 1000)$,\n    - $(B,H,T) = (64, 256, 1)$,\n    - $(B,H,T) = (1, 1024, 10)$,\n    - $(B,H,T) = (8, 128, 10000)$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case produces an inner list in the order $[M_{\\text{std}},M_{\\text{rev}},S,O]$. There must be no spaces anywhere in the line. Floats must be formatted with exactly $6$ digits after the decimal point. For example, an output for two hypothetical cases would look like: $[[100,10,10.000000,1.333333],[200,20,10.000000,1.333333]]$.",
            "solution": "The problem requires an analysis of the memory and computational costs associated with two methods for training a recurrent neural network: standard backpropagation through time (BPTT) and a memory-efficient reversible BPTT. The analysis is performed on a specific reversible recurrent block defined by additive coupling.\n\n### Principle of Backpropagation and Activation Memory\n\nThe foundation of training neural networks is reverse-mode automatic differentiation, commonly known as backpropagation. When training a recurrent network unfolded over $T$ time steps, the model can be viewed as a deep computational graph with $T$ layers sharing the same parameters. The state at time step $t$, denoted $h_t$, is a function of the previous state $h_{t-1}$ and the input at the current step $x_t$, i.e., $h_t = f(h_{t-1}, x_t)$.\n\nTo compute the gradient of a loss function $L$ with respect to the network's parameters, the chain rule is applied repeatedly. For instance, the gradient of the loss with respect to the state $h_t$ depends on the gradient with respect to the subsequent state $h_{t+1}$:\n$$\n\\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L}{\\partial h_{t+1}} \\frac{\\partial h_{t+1}}{\\partial h_t}\n$$\nThe Jacobian term $\\frac{\\partial h_{t+1}}{\\partial h_t}$ is the derivative of the state transition function. Critically, this Jacobian often depends on the values of the activations at time $t$, namely $h_t$ itself. Consequently, to perform the backward pass from step $t+1$ to $t$, the activation values from the forward pass at step $t$ must be available.\n\n### Part 1: Memory Analysis\n\n#### Standard Backpropagation Through Time ($M_{\\text{std}}$)\nIn standard BPTT, the requirement to have activations available during the backward pass is met by storing them in memory. The forward pass iterates from $t=1$ to $T$, computing and storing each hidden state $h_1, h_2, \\dots, h_T$. The backward pass then traverses the graph in reverse, from $t=T$ down to $1$, using the stored activations to compute local gradients.\n\n-   The hidden state $h_t$ is a tensor of dimension $H$ for a single instance.\n-   With a batch size of $B$, the total number of scalar activations at a single time step is $B \\cdot H$.\n-   Since all $T$ hidden states must be stored, the total number of scalar activations to be kept in memory is $T \\cdot B \\cdot H$.\n-   Given that each scalar value requires $s$ bytes of storage (where $s=4$ for single-precision floats), the total memory requirement is:\n    $$\n    M_{\\text{std}} = T \\cdot B \\cdot H \\cdot s\n    $$\n\n#### Reversible Backpropagation ($M_{\\text{rev}}$)\nReversible models are designed to circumvent the need for storing all intermediate activations. The core idea is that the state transition function $h_{t+1} = f(h_t)$ is invertible, allowing one to recompute $h_t$ exactly from $h_{t+1}$. The given recurrent block has this property:\n-   Forward dynamics: $a_{t+1} = a_t + F(b_t)$, $b_{t+1} = b_t + G(a_{t+1})$.\n-   Inverse dynamics: $b_t = b_{t+1} - G(a_{t+1})$, $a_t = a_{t+1} - F(b_t)$.\n\nDuring backpropagation, only the final hidden state $h_T$ is stored after the forward pass. To compute the gradients for the transition from $t=T-1$ to $t=T$, the algorithm first recomputes $h_{T-1}$ from $h_T$ using the inverse dynamics. With both $h_{T-1}$ and $h_T$ now available, the local gradients can be calculated. After this, $h_T$ can be discarded (conceptually), and the process continues by recomputing $h_{T-2}$ from $h_{T-1}$.\n\nThis strategy ensures that at any point during the backward pass, only a constant number of hidden states need to be held in memory, regardless of the sequence length $T$. The memory footprint is dominated by the storage of the current state being processed. Thus, the activation memory scales with the size of a single hidden state tensor for the batch.\n\n-   The memory required is for one hidden state tensor of size $B \\times H$.\n-   The total memory requirement is:\n    $$\n    M_{\\text{rev}} = B \\cdot H \\cdot s\n    $$\n\n#### Memory Savings Factor ($S$)\nThe savings factor $S$ is the ratio of the memory required by the standard method to that of the reversible method.\n$$\nS = \\frac{M_{\\text{std}}}{M_{\\text{rev}}} = \\frac{T \\cdot B \\cdot H \\cdot s}{B \\cdot H \\cdot s} = T\n$$\nThe memory savings are directly proportional to the sequence length $T$, which can be substantial for long sequences.\n\n### Part 2: Computational Cost Analysis\n\nThis trade-off of memory for computation means the reversible method performs extra work. We analyze this additional cost using the provided floating-point operation (FLOP) counts. The hidden state dimension is $H$, so the sub-vectors $a_t$ and $b_t$ have dimension $H/2$.\n\n#### Forward Cost ($C_{\\text{fwd}}$)\nThe forward pass for one time step involves two linear transformations ($F$ and $G$) and two vector additions.\n-   The cost of a batched matrix-vector multiplication of shape $(n, m)$ is $2Bnm$ FLOPs. For $F$ and $G$, $n=m=H/2$.\n-   Cost of $F(b_t)$: $2 \\cdot B \\cdot (H/2) \\cdot (H/2) = \\frac{1}{2}BH^2$ FLOPs.\n-   Cost of $G(a_{t+1})$: $2 \\cdot B \\cdot (H/2) \\cdot (H/2) = \\frac{1}{2}BH^2$ FLOPs.\n-   The two vector additions ($a_t + F(b_t)$ and $b_t + G(a_{t+1})$) involve a total of $B \\cdot (H/2) + B \\cdot (H/2) = BH$ scalar additions.\n-   The total forward cost is the sum of these:\n    $$\n    C_{\\text{fwd}} = \\frac{1}{2}BH^2 + \\frac{1}{2}BH^2 + BH = BH^2 + BH\n    $$\n\n#### Backward Cost ($C_{\\text{bwd}}$)\nThe backward pass for a linear layer of shape $(n, m)$ costs approximately $4Bnm$ FLOPs.\n-   Backward cost for layer $F$: $4 \\cdot B \\cdot (H/2) \\cdot (H/2) = BH^2$ FLOPs.\n-   Backward cost for layer $G$: $4 \\cdot B \\cdot (H/2) \\cdot (H/2) = BH^2$ FLOPs.\n-   The backward pass through the additions contributes gradient additions, with a cost proportional to $BH$.\n-   The total backward cost is:\n    $$\n    C_{\\text{bwd}} = BH^2 + BH^2 + BH = 2BH^2 + BH\n    $$\n\n#### Inversion Cost ($C_{\\text{inv}}$)\nDuring the reversible backward pass, the inverse dynamics must be computed at each step. The inversion involves one application of $G$ and one of $F$, plus two vector subtractions. The computational cost is identical to the forward pass.\n$$\nC_{\\text{inv}} = C_{\\text{fwd}} = BH^2 + BH\n$$\n\n#### Compute Overhead Factor ($O$)\nThe overhead factor $O$ compares the total computational work per time step for a full forward-and-backward pass in the reversible case versus the standard case.\n-   Standard work per step: $C_{\\text{fwd}} + C_{\\text{bwd}}$\n-   Reversible work per step: $C_{\\text{fwd}} + C_{\\text{bwd}} + C_{\\text{inv}}$ (Forward pass + Backward pass with recomputation)\n\nThe overhead factor is the ratio:\n$$\nO = \\frac{C_{\\text{fwd}} + C_{\\text{bwd}} + C_{\\text{inv}}}{C_{\\text{fwd}} + C_{\\text{bwd}}}\n$$\nSubstituting the derived expressions for the costs:\n$$\nO = \\frac{(BH^2 + BH) + (2BH^2 + BH) + (BH^2 + BH)}{(BH^2 + BH) + (2BH^2 + BH)}\n$$\n$$\nO = \\frac{4BH^2 + 3BH}{3BH^2 + 2BH}\n$$\nFactoring out $BH$ from the numerator and denominator:\n$$\nO = \\frac{BH(4H + 3)}{BH(3H + 2)} = \\frac{4H + 3}{3H + 2}\n$$\nThis result shows that the computational overhead depends only on the hidden dimension $H$. As $H$ becomes large, the overhead approaches a constant limit: $\\lim_{H \\to \\infty} O = \\frac{4H}{3H} = \\frac{4}{3}$. This signifies approximately a $33\\%$ increase in computation to achieve memory savings proportional to $T$.\n\nThe derived formulas $M_{\\text{std}} = T B H s$, $M_{\\text{rev}} = B H s$, $S=T$, and $O = \\frac{4H+3}{3H+2}$ are now used to compute the results for the given test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes memory and compute trade-offs for a reversible recurrent layer.\n    \"\"\"\n    # Test cases are tuples of (B, H, T):\n    # B: Batch size\n    # H: Hidden state dimension\n    # T: Time steps\n    test_cases = [\n        (32, 512, 1000),\n        (64, 256, 1),\n        (1, 1024, 10),\n        (8, 128, 10000),\n    ]\n\n    # s: Bytes per scalar activation (single-precision float)\n    s = 4 \n\n    results_list = []\n\n    for case in test_cases:\n        B, H, T = case\n\n        # 1. Memory Calculation\n        # M_std: Memory for standard backpropagation through time.\n        # All T hidden states of size BxH are stored.\n        M_std = T * B * H * s\n\n        # M_rev: Memory for reversible backpropagation.\n        # Only the final hidden state is stored; others are recomputed.\n        M_rev = B * H * s\n\n        # S: Memory savings factor.\n        # This is the ratio M_std / M_rev, which simplifies to T.\n        S = float(T)\n\n        # 2. Compute Overhead Calculation\n        # The computational overhead factor O depends only on H.\n        # O = (4H + 3) / (3H + 2) as derived from the cost formulas.\n        # C_fwd = B*H**2 + B*H\n        # C_bwd = 2*B*H**2 + B*H\n        # C_inv = C_fwd\n        # O = (C_fwd + C_bwd + C_inv) / (C_fwd + C_bwd)\n        # O = (4*B*H**2 + 3*B*H) / (3*B*H**2 + 2*B*H)\n        # O simplifies to (4*H + 3) / (3*H + 2)\n        O = (4 * H + 3) / (3 * H + 2)\n\n        results_list.append([int(M_std), int(M_rev), S, O])\n    \n    # 3. Formatting the output as per the problem specification.\n    # The output must be a single line, with no spaces, and floats formatted\n    # to 6 decimal places. Example: [[...],[...]]\n    formatted_results = [\n        f\"[{res[0]},{res[1]},{res[2]:.6f},{res[3]:.6f}]\"\n        for res in results_list\n    ]\n    \n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}