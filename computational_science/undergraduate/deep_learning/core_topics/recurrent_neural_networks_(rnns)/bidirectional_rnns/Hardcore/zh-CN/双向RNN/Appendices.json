{
    "hands_on_practices": [
        {
            "introduction": "双向循环神经网络（BiRNN）的核心优势在于它能同时利用过去和未来的上下文信息。但是，这种优势在多大程度上是必要的呢？本练习通过一个精心设计的合成序列分类任务，要求我们在一个特定位置 $t$ 上的标签取决于其相邻的过去 ($t-1$) 和未来 ($t+1$) 输入。通过从第一性原理出发，推导并比较一个只能前向预测的模型和一个理想化的双向模型的预期准确率，你将深刻体会到在处理非因果依赖问题时，双向结构为何不可或缺 。",
            "id": "3103026",
            "problem": "您将处理一个综合序列分类任务，该任务旨在分离出循环神经网络中双向性的优势。考虑一个二元标记序列 $\\{x_t\\}_{t=1}^L$，其中每个 $x_t \\in \\{0,1\\}$ 都是从参数为 $p$ 的伯努利分布中独立同分布地抽取的，即对所有 $t$ 都有 $x_t \\sim \\mathrm{Bernoulli}(p)$。对于每个内部位置 $t$（即所有满足 $2 \\le t \\le L-1$ 的 $t$），您必须根据以下邻居约束的恒等规则分配一个标签 $y_t \\in \\{0,1\\}$：当且仅当 $x_t$ 等于其两个邻居的逻辑与（AND）时，$y_t = 1$，即 $x_t = x_{t-1} \\land x_{t+1}$，否则 $y_t = 0$。\n\n基本原理：循环神经网络（RNN）是一种因果模型，其中时间 $t$ 的隐藏状态 $h_t$ 是当前输入 $x_t$ 和前一个隐藏状态 $h_{t-1}$ 的函数，而输出 $o_t$ 则依赖于 $h_t$。双向循环神经网络（BiRNN）沿前向和后向两个方向处理序列，计算隐藏状态 $h_t^{\\rightarrow}$ 和 $h_t^{\\leftarrow}$，并根据 $h_t^{\\rightarrow}$ 和 $h_t^{\\leftarrow}$ 两者形成输出 $o_t$。因此，一个在时间 $t$ 进行预测的纯前向因果模型不能依赖于 $x_{t+1}$，而一个双向模型在时间 $t$ 的预测可能同时依赖于 $x_{t-1}$ 和 $x_{t+1}$。\n\n您的任务是，从第一性原理出发，不依赖任何预先提供的目标公式，推导出两种理想化预测器在仅评估内部位置 $t \\in \\{2,3,\\dots,L-1\\}$ 时的预期分类准确率：\n\n- 最佳的纯前向因果预测器，在时间 $t$，它可以访问 $x_{t-1}$ 和 $x_t$ 以及分布参数 $p$，但不能访问 $x_{t+1}$。该预测器必须仅使用不违反相对于 $t+1$ 的因果性的信息来决定 $\\hat{y}_t \\in \\{0,1\\}$。其预期准确率应在上述数据生成过程下计算。\n- 一种理想化的双向预测器，在时间 $t$，它可以访问 $x_{t-1}$、$x_t$ 和 $x_{t+1}$ 以及参数 $p$。其预期准确率应在相同的数据生成过程下计算。\n\n您必须编写一个完整的、可运行的程序，为每个测试用例计算三个浮点数量：纯前向因果预测器在内部位置上的预期准确率，双向预测器在内部位置上的预期准确率，以及定义为双向和纯前向预期准确率之差的准确率增益。角度和物理单位不适用于此任务。\n\n测试套件：\n- $(L,p) = (3,0)$\n- $(L,p) = (3,1)$\n- $(L,p) = (5,0.5)$\n- $(L,p) = (9,0.25)$\n- $(L,p) = (9,0.9)$\n- $(L,p) = (12,0.5)$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例对应一个子列表，包含按上述顺序排列的三个浮点数。例如，输出必须类似于 $[\\,[a_1,b_1,c_1],\\,[a_2,b_2,c_2],\\,\\dots\\,]$，不含任何额外文本。所有值都应根据推导精确计算，并以标准十进制浮点数形式报告。",
            "solution": "该问题是有效的，因为它具有科学依据、定义明确且客观。它提出了一个在统计决策理论和概率论中可以形式化的问题，与理解单向和双向RNN等因果模型与非因果模型之间的功能差异直接相关。所有必要信息均已提供，问题没有矛盾或含糊之处。\n\n我们现在将为指定的两种预测器推导预期分类准确率。推导是针对任意内部位置 $t$（其中 $2 \\le t \\le L-1$）进行的。由于标记 $x_t$ 是独立同分布（i.i.d.）的，结果将与 $t$ 的具体选择和序列长度 $L$ 无关。\n\n数据生成过程由一个二元标记序列 $\\{x_t\\}_{t=1}^L$ 定义，其中每个标记 $x_t \\in \\{0,1\\}$ 是从参数为 $p$ 的伯努利分布中抽取的：\n$$ P(x_t=1) = p $$\n$$ P(x_t=0) = 1-p $$\n对于内部位置 $t$，真实标签 $y_t \\in \\{0,1\\}$ 由以下规则确定：\n$$ y_t = 1 \\text{ if and only if } x_t = (x_{t-1} \\land x_{t+1}) $$\n其中 $\\land$ 代表逻辑与运算符。否则，$y_t=0$。\n\n预测器 $\\hat{y}_t$ 的准确率是其与真实标签 $y_t$ 匹配的概率，即 $P(\\hat{y}_t = y_t)$。我们寻求的是预期准确率，对于这个平稳过程，它就是这个概率。一个最优预测器，在给定信息集的情况下，会通过选择后验概率最高的标签来最大化这个概率。\n\n**1. 理想化的双向预测器**\n\n理想化的双向预测器在时间 $t$ 可以访问完整的三元组 $(x_{t-1}, x_t, x_{t+1})$。标签规则 $y_t = 1 \\iff x_t = (x_{t-1} \\land x_{t+1})$ 完全依赖于这三个值。由于完全了解这个三元组，预测器可以计算 $x_{t-1} \\land x_{t+1}$ 的值并将其与 $x_t$ 进行比较。因此，对于预测器来说，$y_t$ 的结果是确定性的。\n\n对于任何给定的三元组，预测器都可以将其预测值 $\\hat{y}_t$ 设置为与真实标签 $y_t$ 完全相等。例如，如果它观察到 $(x_{t-1}, x_t, x_{t+1}) = (1, 1, 1)$，它计算 $x_{t-1} \\land x_{t+1} = 1 \\land 1 = 1$。因为这等于 $x_t=1$，所以真实标签是 $y_t=1$，预测器将 $\\hat{y}_t$ 设置为1。如果它观察到 $(x_{t-1}, x_t, x_{t+1}) = (1, 0, 1)$，它计算 $x_{t-1} \\land x_{t+1} = 1$。这不等于 $x_t=0$，所以真实标签是 $y_t=0$，预测器将 $\\hat{y}_t$ 设置为0。\n\n由于预测值 $\\hat{y}_t$ 总能与真实标签 $y_t$ 相等，因此正确预测的概率为 $1$。\n$$ P(\\hat{y}_t = y_t | x_{t-1}, x_t, x_{t+1}) = 1 $$\n预期准确率 $A_{bi}$ 是这个条件概率在所有可能的三元组上的期望值，结果就是 $1$。\n$$ A_{bi} = E[P(\\hat{y}_t = y_t | x_{t-1}, x_t, x_{t+1})] = 1 $$\n\n**2. 最佳纯前向因果预测器**\n\n纯前向因果预测器可以访问 $(x_{t-1}, x_t)$ 和参数 $p$，但不能访问 $x_{t+1}$。最佳预测器将选择在给定可用信息下最大化后验概率的标签 $\\hat{y}_t$：\n$$ \\hat{y}_t = \\arg\\max_{k \\in \\{0,1\\}} P(y_t=k | x_{t-1}, x_t) $$\n此最优策略的准确率为 $P(\\hat{y}_t = y_t)$。我们通过将 $(x_{t-1}, x_t)$ 的四种可能观察对的条件准确率，按每对的概率加权求和来计算。\n\n让我们分析观察对 $(x_{t-1}, x_t)$ 的四种情况：\n\n**情况 A：$(x_{t-1}, x_t) = (0, 0)$**\n$y_t=1$ 的条件是 $x_t = (x_{t-1} \\land x_{t+1})$。代入已知值，我们得到 $0 = (0 \\land x_{t+1})$。由于无论 $x_{t+1}$ 为何值，$0 \\land x_{t+1}$ 总是 $0$，因此这简化为 $0=0$，永远成立。所以，$y_t$ 确定性地为 $1$。\n最优预测是 $\\hat{y}_t=1$。在这种情况下，准确率为 $1$。\n这种情况的概率是 $P(x_{t-1}=0, x_t=0) = (1-p)(1-p) = (1-p)^2$。\n\n**情况 B：$(x_{t-1}, x_t) = (0, 1)$**\n$y_t=1$ 的条件是 $1 = (0 \\land x_{t+1})$，简化为 $1=0$。这永远不成立。所以，$y_t$ 确定性地为 $0$。\n最优预测是 $\\hat{y}_t=0$。在这种情况下，准确率为 $1$。\n这种情况的概率是 $P(x_{t-1}=0, x_t=1) = (1-p)p$。\n\n**情况 C：$(x_{t-1}, x_t) = (1, 0)$**\n$y_t=1$ 的条件是 $0 = (1 \\land x_{t+1})$，简化为 $0 = x_{t+1}$。因此，如果 $x_{t+1}=0$，则 $y_t=1$；如果 $x_{t+1}=1$，则 $y_t=0$。由于 $x_{t+1}$ 未知，我们使用其概率分布。\n$P(y_t=1 | x_{t-1}=1, x_t=0) = P(x_{t+1}=0) = 1-p$。\n$P(y_t=0 | x_{t-1}=1, x_t=0) = P(x_{t+1}=1) = p$。\n最优预测器在 $1-p > p$（即 $p  0.5$）时选择 $\\hat{y}_t=1$，在 $p > 1-p$（即 $p > 0.5$）时选择 $\\hat{y}_t=0$。如果 $p=0.5$，任一选择的准确率相同。在这种情况下，准确率是多数类的概率，即 $\\max(p, 1-p)$。\n这种情况的概率是 $P(x_{t-1}=1, x_t=0) = p(1-p)$。\n\n**情况 D：$(x_{t-1}, x_t) = (1, 1)$**\n$y_t=1$ 的条件是 $1 = (1 \\land x_{t+1})$，简化为 $1 = x_{t+1}$。因此，如果 $x_{t+1}=1$，则 $y_t=1$；如果 $x_{t+1}=0$，则 $y_t=0$。\n$P(y_t=1 | x_{t-1}=1, x_t=1) = P(x_{t+1}=1) = p$。\n$P(y_t=0 | x_{t-1}=1, x_t=1) = P(x_{t+1}=0) = 1-p$。\n在这种情况下，最优预测器的准确率同样是多数类的概率，即 $\\max(p, 1-p)$。\n这种情况的概率是 $P(x_{t-1}=1, x_t=1) = p \\cdot p = p^2$。\n\n**纯前向预测器的总预期准确率 ($A_{fwd}$)**\n我们将每种情况的准确率按其概率加权求和：\n$$ A_{fwd} = 1 \\cdot P(x_{t-1}=0, x_t=0) + 1 \\cdot P(x_{t-1}=0, x_t=1) + \\max(p, 1-p) \\cdot P(x_{t-1}=1, x_t=0) + \\max(p, 1-p) \\cdot P(x_{t-1}=1, x_t=1) $$\n$$ A_{fwd} = 1 \\cdot (1-p)^2 + 1 \\cdot p(1-p) + \\max(p, 1-p) \\cdot p(1-p) + \\max(p, 1-p) \\cdot p^2 $$\n对各项进行因式分解：\n$$ A_{fwd} = ((1-p)^2 + p(1-p)) + \\max(p, 1-p) \\cdot (p(1-p) + p^2) $$\n$$ A_{fwd} = (1-p)(1-p+p) + \\max(p, 1-p) \\cdot p(1-p+p) $$\n$$ A_{fwd} = (1-p) \\cdot 1 + \\max(p, 1-p) \\cdot p \\cdot 1 $$\n$$ A_{fwd} = (1-p) + p \\cdot \\max(p, 1-p) $$\n我们可以将其表示为关于 $p$ 的分段函数：\n- 如果 $p  0.5$，则 $\\max(p, 1-p) = 1-p$。$A_{fwd} = (1-p) + p(1-p) = (1-p)(1+p) = 1-p^2$。\n- 如果 $p \\ge 0.5$，则 $\\max(p, 1-p) = p$。$A_{fwd} = (1-p) + p(p) = 1-p+p^2$。\n\n**3. 准确率增益**\n\n准确率增益是双向预测器和纯前向预测器预期准确率之间的差值。\n$$ \\text{Gain} = A_{bi} - A_{fwd} = 1 - [(1-p) + p \\cdot \\max(p, 1-p)] $$\n$$ \\text{Gain} = p - p \\cdot \\max(p, 1-p) = p(1 - \\max(p, 1-p)) $$\n使用恒等式 $1 - \\max(a,b) = \\min(1-a, 1-b)$，我们得到：\n$$ \\text{Gain} = p \\cdot \\min(p, 1-p) $$\n- 如果 $p  0.5$，则 $\\min(p, 1-p) = p$。$\\text{Gain} = p \\cdot p = p^2$。\n- 如果 $p \\ge 0.5$，则 $\\min(p, 1-p) = 1-p$。$\\text{Gain} = p(1-p)$。\n\n这些推导出的公式可以直接计算每个测试用例所需的量。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the expected accuracies for forward-only and bidirectional predictors\n    on a synthetic sequence task.\n    \"\"\"\n    # Test suite: each tuple is (L, p)\n    # L is not used in the calculation since the expected accuracy for any\n    # interior position is independent of sequence length for an i.i.d. process.\n    test_cases = [\n        (3, 0.0),\n        (3, 1.0),\n        (5, 0.5),\n        (9, 0.25),\n        (9, 0.9),\n        (12, 0.5),\n    ]\n\n    results = []\n    for L, p in test_cases:\n        # Calculate the expected accuracy of the best possible forward-only predictor.\n        # The derived formula is A_fwd = (1-p) + p * max(p, 1-p).\n        # This can be written as a piecewise function:\n        # if p  0.5, A_fwd = 1 - p**2\n        # if p >= 0.5, A_fwd = 1 - p + p**2\n        if p  0.5:\n            acc_fwd = 1.0 - p**2\n        else:\n            acc_fwd = 1.0 - p + p**2\n\n        # The idealized bidirectional predictor has access to all necessary information\n        # (x_{t-1}, x_t, x_{t+1}) to determine the label y_t with certainty.\n        # Therefore, its accuracy is always 1.0.\n        acc_bi = 1.0\n\n        # The accuracy gain is the difference between the two.\n        accuracy_gain = acc_bi - acc_fwd\n\n        # Append the results for this test case.\n        results.append([acc_fwd, acc_bi, accuracy_gain])\n\n    # Format the output string as required: [[a1,b1,c1],[a2,b2,c2],...]\n    sublist_strings = []\n    for sublist in results:\n        # Convert each float in the sublist to a string and join with commas\n        # Enclose the result in square brackets\n        sublist_strings.append(f\"[{','.join(map(str, sublist))}]\")\n    \n    # Join all sublist strings with commas and enclose in the final brackets\n    final_output_string = f\"[{','.join(sublist_strings)}]\"\n\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "确立了双向信息的必要性之后，下一个自然的问题是：模型应该如何有效地融合来自过去（前向状态 $h^{\\rightarrow}_t$）和未来（后向状态 $h^{\\leftarrow}_t$）的信息？本练习将这个问题置于一个统计估计的框架中。你将推导在线性高斯假设下，如何通过最小化均方误差来找到融合两个状态的最优权重，从而深刻理解 BiRNN 如何根据两个信息源的可靠性（方差）来智能地权衡它们 。",
            "id": "3103018",
            "problem": "考虑一个双向循环神经网络 (BiRNN)，其中时间 $t$ 的前向隐藏状态 $h_t^{\\rightarrow}$ 和后向隐藏状态 $h_t^{\\leftarrow}$ 用于估计一个未观测到的潜信号 $s_t$。假设一个线性高斯生成模型，其中每个隐藏状态都是对 $s_t$ 的无偏噪声观测：\n$$\nh_t^{\\rightarrow} = s_t + \\epsilon_t^{\\rightarrow}, \\quad h_t^{\\leftarrow} = s_t + \\epsilon_t^{\\leftarrow},\n$$\n其中 $\\epsilon_t^{\\rightarrow}$ 和 $\\epsilon_t^{\\leftarrow}$ 是联合高斯噪声项，其均值为零，方差为 $\\operatorname{Var}(\\epsilon_t^{\\rightarrow}) = \\sigma_{\\rightarrow,t}^{2}$ 和 $\\operatorname{Var}(\\epsilon_t^{\\leftarrow}) = \\sigma_{\\leftarrow,t}^{2}$，协方差为 $\\operatorname{Cov}(\\epsilon_t^{\\rightarrow}, \\epsilon_t^{\\leftarrow}) = c_t$。考虑一个线性融合估计器\n$$\n\\hat{s}_t(g_t) = g_t\\, h_t^{\\rightarrow} + \\left(1 - g_t\\right) h_t^{\\leftarrow},\n$$\n其中 $g_t \\in \\mathbb{R}$ 是标量融合权重。从均方误差 (MSE) 的定义出发，推导出在上述模型下能最小化期望平方误差 $\\mathbb{E}\\big[(\\hat{s}_t(g_t) - s_t)^{2}\\big]$ 的 $g_t$ 值。\n\n然后，使用以下噪声统计量评估推导出的表达式：\n$$\n\\sigma_{\\rightarrow,t}^{2} = 0.36, \\quad \\sigma_{\\leftarrow,t}^{2} = 0.81, \\quad c_t = 0.12.\n$$\n另外，假设 BiRNN 中的一个学习到的门控机制定义为\n$$\ng_t^{\\mathrm{learn}} = \\sigma\\!\\left(w_{\\rightarrow} h_t^{\\rightarrow} + w_{\\leftarrow} h_t^{\\leftarrow} + b\\right),\n$$\n其中 $\\sigma(x)$ 是 logistic sigmoid 函数 $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$。对于一个特定的时间 $t$，令\n$$\nh_t^{\\rightarrow} = 1.4, \\quad h_t^{\\leftarrow} = 0.6, \\quad w_{\\rightarrow} = 0.5, \\quad w_{\\leftarrow} = 0.25, \\quad b = \\ln(3) - 0.85.\n$$\n计算 $g_t^{\\mathrm{learn}}$，并将其与您上面推导出的解析最优值 $g_t^{\\star}$ 进行比较。您的最终答案必须是绝对差\n$$\n\\left|g_t^{\\mathrm{learn}} - g_t^{\\star}\\right|,\n$$\n以纯数字形式表示。将您的最终答案四舍五入到四位有效数字。",
            "solution": "用户希望在双向循环神经网络 (BiRNN) 的背景下，找到最优推导的融合权重与学习到的门值之间的绝对差。\n\n首先，我将验证问题陈述。\n\n### 步骤 1：提取已知条件\n- 前向隐藏状态：$h_t^{\\rightarrow}$\n- 后向隐藏状态：$h_t^{\\leftarrow}$\n- 潜信号：$s_t$\n- 生成模型：$h_t^{\\rightarrow} = s_t + \\epsilon_t^{\\rightarrow}$ 和 $h_t^{\\leftarrow} = s_t + \\epsilon_t^{\\leftarrow}$。\n- 噪声属性：$\\epsilon_t^{\\rightarrow}, \\epsilon_t^{\\leftarrow}$ 是联合高斯分布，且 $\\mathbb{E}[\\epsilon_t^{\\rightarrow}] = 0$，$\\mathbb{E}[\\epsilon_t^{\\leftarrow}] = 0$，$\\operatorname{Var}(\\epsilon_t^{\\rightarrow}) = \\sigma_{\\rightarrow,t}^{2}$，$\\operatorname{Var}(\\epsilon_t^{\\leftarrow}) = \\sigma_{\\leftarrow,t}^{2}$，以及 $\\operatorname{Cov}(\\epsilon_t^{\\rightarrow}, \\epsilon_t^{\\leftarrow}) = c_t$。\n- 线性融合估计器：$\\hat{s}_t(g_t) = g_t\\, h_t^{\\rightarrow} + \\left(1 - g_t\\right) h_t^{\\leftarrow}$，其中 $g_t \\in \\mathbb{R}$。\n- 目标：最小化均方误差 (MSE)，$\\mathbb{E}\\big[(\\hat{s}_t(g_t) - s_t)^{2}\\big]$。\n- 噪声统计量的数值：$\\sigma_{\\rightarrow,t}^{2} = 0.36$，$\\sigma_{\\leftarrow,t}^{2} = 0.81$，$c_t = 0.12$。\n- 学习到的门：$g_t^{\\mathrm{learn}} = \\sigma\\!\\left(w_{\\rightarrow} h_t^{\\rightarrow} + w_{\\leftarrow} h_t^{\\leftarrow} + b\\right)$，其中 $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$。\n- 学习到的门的数值：$h_t^{\\rightarrow} = 1.4$，$h_t^{\\leftarrow} = 0.6$，$w_{\\rightarrow} = 0.5$，$w_{\\leftarrow} = 0.25$，$b = \\ln(3) - 0.85$。\n- 最终答案：计算 $|g_t^{\\mathrm{learn}} - g_t^{\\star}|$，四舍五入到四位有效数字。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题在科学上基于统计估计理论及其在神经网络中的应用。线性高斯模型是分析的标准框架。最小化 MSE 以找到最优线性估计器是一个基本且适定 (well-posed) 的问题。学习到的门的结构是 RNN 中门控机制的典型结构。所有必需的常数和条件都已提供，并且没有矛盾之处。问题是客观且可形式化的。\n\n### 步骤 3：结论与行动\n问题有效。我现在开始解答。\n\n问题的第一部分是推导最小化 MSE 的最优融合权重 $g_t^{\\star}$。估计误差由 $e_t = \\hat{s}_t(g_t) - s_t$ 给出。\n代入 $\\hat{s}_t$、$h_t^{\\rightarrow}$ 和 $h_t^{\\leftarrow}$ 的表达式：\n$$\ne_t = \\left( g_t h_t^{\\rightarrow} + (1 - g_t) h_t^{\\leftarrow} \\right) - s_t\n$$\n$$\ne_t = g_t (s_t + \\epsilon_t^{\\rightarrow}) + (1 - g_t) (s_t + \\epsilon_t^{\\leftarrow}) - s_t\n$$\n$$\ne_t = g_t s_t + g_t \\epsilon_t^{\\rightarrow} + s_t - g_t s_t + \\epsilon_t^{\\leftarrow} - g_t \\epsilon_t^{\\leftarrow} - s_t\n$$\n$$\ne_t = g_t \\epsilon_t^{\\rightarrow} + (1 - g_t) \\epsilon_t^{\\leftarrow}\n$$\nMSE 是平方误差的期望值，即 $\\mathbb{E}[e_t^2]$。\n$$\n\\text{MSE}(g_t) = \\mathbb{E}\\left[ \\left( g_t \\epsilon_t^{\\rightarrow} + (1 - g_t) \\epsilon_t^{\\leftarrow} \\right)^2 \\right]\n$$\n展开平方项并利用期望的线性性质：\n$$\n\\text{MSE}(g_t) = g_t^2 \\mathbb{E}[(\\epsilon_t^{\\rightarrow})^2] + (1 - g_t)^2 \\mathbb{E}[(\\epsilon_t^{\\leftarrow})^2] + 2 g_t (1 - g_t) \\mathbb{E}[\\epsilon_t^{\\rightarrow} \\epsilon_t^{\\leftarrow}]\n$$\n鉴于噪声项的均值为零，期望值对应于方差和协方差：\n$$\n\\mathbb{E}[(\\epsilon_t^{\\rightarrow})^2] = \\operatorname{Var}(\\epsilon_t^{\\rightarrow}) = \\sigma_{\\rightarrow,t}^2\n$$\n$$\n\\mathbb{E}[(\\epsilon_t^{\\leftarrow})^2] = \\operatorname{Var}(\\epsilon_t^{\\leftarrow}) = \\sigma_{\\leftarrow,t}^2\n$$\n$$\n\\mathbb{E}[\\epsilon_t^{\\rightarrow} \\epsilon_t^{\\leftarrow}] = \\operatorname{Cov}(\\epsilon_t^{\\rightarrow}, \\epsilon_t^{\\leftarrow}) = c_t\n$$\n因此，作为 $g_t$ 函数的 MSE 为：\n$$\n\\text{MSE}(g_t) = g_t^2 \\sigma_{\\rightarrow,t}^2 + (1 - g_t)^2 \\sigma_{\\leftarrow,t}^2 + 2 g_t (1 - g_t) c_t\n$$\n为了找到最小化 MSE 的 $g_t$ 值，我们对 $g_t$ 求导并将导数设为零：\n$$\n\\frac{d}{dg_t}\\text{MSE}(g_t) = 2 g_t \\sigma_{\\rightarrow,t}^2 + 2 (1 - g_t)(-1) \\sigma_{\\leftarrow,t}^2 + 2(1 - 2g_t) c_t\n$$\n将导数设为零：\n$$\n2 g_t \\sigma_{\\rightarrow,t}^2 - 2 \\sigma_{\\leftarrow,t}^2 + 2 g_t \\sigma_{\\leftarrow,t}^2 + 2 c_t - 4 g_t c_t = 0\n$$\n将包含 $g_t$ 的项分组：\n$$\ng_t (2 \\sigma_{\\rightarrow,t}^2 + 2 \\sigma_{\\leftarrow,t}^2 - 4 c_t) = 2 \\sigma_{\\leftarrow,t}^2 - 2 c_t\n$$\n求解最优权重 $g_t^{\\star}$：\n$$\ng_t^{\\star} = \\frac{2 \\sigma_{\\leftarrow,t}^2 - 2 c_t}{2 \\sigma_{\\rightarrow,t}^2 + 2 \\sigma_{\\leftarrow,t}^2 - 4 c_t} = \\frac{\\sigma_{\\leftarrow,t}^2 - c_t}{\\sigma_{\\rightarrow,t}^2 + \\sigma_{\\leftarrow,t}^2 - 2c_t}\n$$\n接下来，我们使用给定的噪声统计量来评估此表达式：$\\sigma_{\\rightarrow,t}^{2} = 0.36$，$\\sigma_{\\leftarrow,t}^{2} = 0.81$ 和 $c_t = 0.12$。\n$$\ng_t^{\\star} = \\frac{0.81 - 0.12}{0.36 + 0.81 - 2(0.12)} = \\frac{0.69}{1.17 - 0.24} = \\frac{0.69}{0.93} = \\frac{69}{93} = \\frac{23}{31}\n$$\n现在，我们计算学习到的门 $g_t^{\\mathrm{learn}}$ 的值。sigmoid 函数的输入是 $z = w_{\\rightarrow} h_t^{\\rightarrow} + w_{\\leftarrow} h_t^{\\leftarrow} + b$。\n使用提供的值 $h_t^{\\rightarrow} = 1.4$、$h_t^{\\leftarrow} = 0.6$、$w_{\\rightarrow} = 0.5$、$w_{\\leftarrow} = 0.25$ 和 $b = \\ln(3) - 0.85$：\n$$\nz = (0.5)(1.4) + (0.25)(0.6) + (\\ln(3) - 0.85)\n$$\n$$\nz = 0.7 + 0.15 + \\ln(3) - 0.85\n$$\n$$\nz = 0.85 + \\ln(3) - 0.85 = \\ln(3)\n$$\n门的值是 $g_t^{\\mathrm{learn}} = \\sigma(z) = \\sigma(\\ln(3))$。\n$$\ng_t^{\\mathrm{learn}} = \\frac{1}{1 + \\exp(-\\ln(3))} = \\frac{1}{1 + \\exp(\\ln(3^{-1}))} = \\frac{1}{1 + \\frac{1}{3}} = \\frac{1}{\\frac{4}{3}} = \\frac{3}{4}\n$$\n最后，我们计算绝对差 $|g_t^{\\mathrm{learn}} - g_t^{\\star}|$。\n$$\n\\left| \\frac{3}{4} - \\frac{23}{31} \\right| = \\left| \\frac{3 \\times 31}{4 \\times 31} - \\frac{23 \\times 4}{31 \\times 4} \\right| = \\left| \\frac{93 - 92}{124} \\right| = \\frac{1}{124}\n$$\n为了提供最终答案，我们将此分数转换为小数并四舍五入到四位有效数字。\n$$\n\\frac{1}{124} \\approx 0.008064516...\n$$\n第一个有效数字是 $8$。前四位有效数字是 $8$、$0$、$6$ 和 $4$。下一位数字是 $5$，所以我们将最后一位有效数字向上取整。\n结果是 $0.008065$。",
            "answer": "$$\\boxed{0.008065}$$"
        }
    ]
}