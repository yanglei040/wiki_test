{
    "hands_on_practices": [
        {
            "introduction": "Before diving into complex automated frameworks, it is crucial to solidify your understanding of backpropagation by working through it manually. This exercise  strips a neural network down to its simplest form—a single linear neuron—and tasks you with applying the chain rule directly to calculate gradients. By deriving both the gradient and the Hessian, you will not only see the mechanics of backpropagation but also connect it to the fundamental optimization concept of identifying a minimum on the loss surface.",
            "id": "3099996",
            "problem": "You are given a single-neuron model with a linear activation, defined by the parametric function $f(x; \\theta) = W x + b$, where $\\theta = (W, b)$, $W \\in \\mathbb{R}$, and $b \\in \\mathbb{R}$. The training set consists of three input-output pairs $(x_i, y_i)$ for $i = 1, 2, 3$, specifically $(x_1, y_1) = (0, 1)$, $(x_2, y_2) = (1, 3)$, and $(x_3, y_3) = (2, 5)$. The empirical risk is the half-sum of squared errors, defined by\n$$\nJ(\\theta) = \\frac{1}{2} \\sum_{i=1}^{3} \\left(f(x_i; \\theta) - y_i\\right)^{2}.\n$$\nStarting from the fundamental definition of the chain rule of calculus and the definition of the gradient and Hessian (the matrix of second-order partial derivatives), do the following:\n1. Choose parameters $W$ and $b$ that exactly fit the three data points, meaning that $f(x_i; \\theta) = y_i$ for every $i \\in \\{1, 2, 3\\}$.\n2. Using backpropagation (that is, the chain rule applied to the computational graph of the model), derive the gradient $\\nabla_{\\theta} J(\\theta)$ and evaluate it at the exact-fit parameters you chose in part $1$.\n3. Derive the Hessian $H(\\theta)$ of $J(\\theta)$ with respect to $\\theta$ and evaluate it at the exact-fit parameters. Compute the minimum eigenvalue $\\lambda_{\\min}(H)$.\n4. Based on the sign of $\\lambda_{\\min}(H)$, briefly state whether the exact-fit point is a local minimum or a saddle point for $J(\\theta)$.\n\nProvide your final answer as the exact value of $\\lambda_{\\min}(H)$ at the solution. No rounding is required.",
            "solution": "The problem has been validated and is scientifically sound, well-posed, objective, and contains sufficient information for a unique solution.\n\nThe task is to analyze the empirical risk function $J(\\theta)$ for a single linear neuron model $f(x; \\theta) = W x + b$ with parameters $\\theta = (W, b)$. The risk is defined as the half-sum of squared errors over three data points: $(x_1, y_1) = (0, 1)$, $(x_2, y_2) = (1, 3)$, and $(x_3, y_3) = (2, 5)$. The risk function is:\n$$\nJ(W, b) = \\frac{1}{2} \\sum_{i=1}^{3} \\left( (Wx_i + b) - y_i \\right)^{2}\n$$\nSubstituting the given data points:\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (W(0) + b - 1)^{2} + (W(1) + b - 3)^{2} + (W(2) + b - 5)^{2} \\right]\n$$\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (b - 1)^{2} + (W + b - 3)^{2} + (2W + b - 5)^{2} \\right]\n$$\n\n**1. Find the exact-fit parameters $\\theta^* = (W, b)$**\n\nFor an exact fit, the model must satisfy $f(x_i; \\theta) = y_i$ for all $i \\in \\{1, 2, 3\\}$. This yields a system of linear equations for $W$ and $b$:\n\\begin{enumerate}\n    \\item For $(x_1, y_1) = (0, 1)$: $W(0) + b = 1 \\implies b = 1$.\n    \\item For $(x_2, y_2) = (1, 3)$: $W(1) + b = 3 \\implies W + b = 3$.\n    \\item For $(x_3, y_3) = (2, 5)$: $W(2) + b = 5 \\implies 2W + b = 5$.\n\\end{enumerate}\nSubstituting $b = 1$ from the first equation into the second gives $W + 1 = 3$, which implies $W = 2$.\nWe must verify that these values satisfy the third equation: $2W + b = 2(2) + 1 = 4 + 1 = 5$, which is consistent with $y_3 = 5$.\nThus, the parameters for an exact fit are $W = 2$ and $b = 1$. Let us denote this point as $\\theta^* = (2, 1)$.\n\n**2. Derive and evaluate the gradient $\\nabla_{\\theta} J(\\theta)$ at $\\theta^*$**\n\nThe gradient of $J(\\theta)$ with respect to $\\theta = (W, b)$ is $\\nabla_{\\theta} J = \\begin{pmatrix} \\frac{\\partial J}{\\partial W} \\\\ \\frac{\\partial J}{\\partial b} \\end{pmatrix}$.\nUsing the chain rule, as specified by the backpropagation methodology, we define the error for each point as $e_i(\\theta) = f(x_i; \\theta) - y_i = Wx_i + b - y_i$. The loss is $J = \\frac{1}{2} \\sum_{i=1}^3 e_i^2$.\nThe partial derivatives are:\n$$\n\\frac{\\partial J}{\\partial W} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial W} = \\sum_{i=1}^{3} e_i \\cdot x_i = \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i\n$$\n$$\n\\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial b} = \\sum_{i=1}^{3} e_i \\cdot 1 = \\sum_{i=1}^{3} (Wx_i + b - y_i)\n$$\nAt the exact-fit point $\\theta^* = (2, 1)$, by definition, the error terms are zero: $e_i(\\theta^*) = Wx_i + b - y_i = 0$ for all $i$.\nTherefore, evaluating the gradient at $\\theta^*$:\n$$\n\\frac{\\partial J}{\\partial W}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) x_i = 0\n$$\n$$\n\\frac{\\partial J}{\\partial b}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) = 0\n$$\nThe gradient at the exact-fit point is the zero vector: $\\nabla_{\\theta} J(\\theta^*) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. This confirms that $\\theta^*$ is a critical point of the loss function $J(\\theta)$.\n\n**3. Derive the Hessian $H(\\theta)$ and compute its minimum eigenvalue**\n\nThe Hessian matrix $H(\\theta)$ contains the second-order partial derivatives of $J(\\theta)$:\n$$\nH(\\theta) = \\begin{pmatrix} \\frac{\\partial^2 J}{\\partial W^2}  \\frac{\\partial^2 J}{\\partial W \\partial b} \\\\ \\frac{\\partial^2 J}{\\partial b \\partial W}  \\frac{\\partial^2 J}{\\partial b^2} \\end{pmatrix}\n$$\nWe compute these by differentiating the first-order partial derivatives:\n$$\n\\frac{\\partial^2 J}{\\partial W^2} = \\frac{\\partial}{\\partial W} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i^2\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b \\partial W} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b^2} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) \\right] = \\sum_{i=1}^{3} 1 = 3\n$$\nNote that $\\frac{\\partial^2 J}{\\partial W \\partial b} = \\frac{\\partial^2 J}{\\partial b \\partial W}$, as expected. The Hessian is constant and does not depend on $W$ or $b$. We evaluate the sums using the given inputs $x_1=0$, $x_2=1$, $x_3=2$:\n$$\n\\sum_{i=1}^{3} x_i^2 = 0^2 + 1^2 + 2^2 = 0 + 1 + 4 = 5\n$$\n$$\n\\sum_{i=1}^{3} x_i = 0 + 1 + 2 = 3\n$$\nThe Hessian matrix is:\n$$\nH = \\begin{pmatrix} 5  3 \\\\ 3  3 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of $H$ are the roots of the characteristic equation $\\det(H - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 5-\\lambda  3 \\\\ 3  3-\\lambda \\end{pmatrix} = (5-\\lambda)(3-\\lambda) - (3)(3) = 0\n$$\n$$\n15 - 8\\lambda + \\lambda^2 - 9 = 0\n$$\n$$\n\\lambda^2 - 8\\lambda + 6 = 0\n$$\nUsing the quadratic formula $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{8 \\pm \\sqrt{(-8)^2 - 4(1)(6)}}{2} = \\frac{8 \\pm \\sqrt{64 - 24}}{2} = \\frac{8 \\pm \\sqrt{40}}{2}\n$$\nSimplifying $\\sqrt{40} = \\sqrt{4 \\cdot 10} = 2\\sqrt{10}$:\n$$\n\\lambda = \\frac{8 \\pm 2\\sqrt{10}}{2} = 4 \\pm \\sqrt{10}\n$$\nThe two eigenvalues are $\\lambda_1 = 4 + \\sqrt{10}$ and $\\lambda_2 = 4 - \\sqrt{10}$. The minimum eigenvalue is $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$.\n\n**4. Classify the critical point $\\theta^*$**\n\nTo classify the critical point $\\theta^*$, we examine the signs of the eigenvalues of the Hessian matrix evaluated at that point. Since $H$ is constant, we use the eigenvalues just computed.\nWe know that $3 = \\sqrt{9}  \\sqrt{10}  \\sqrt{16} = 4$.\nTherefore, the minimum eigenvalue $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$ is positive, as $4  \\sqrt{10}$.\nThe maximum eigenvalue $\\lambda_{\\max}(H) = 4 + \\sqrt{10}$ is also clearly positive.\nSince both eigenvalues of the Hessian are positive, the Hessian matrix is positive definite. According to the second partial derivative test, a critical point at which the Hessian is positive definite is a local minimum. For this quadratic loss function, it is the unique global minimum. The exact-fit point is a local minimum.\nThe final answer is the value of the minimum eigenvalue.",
            "answer": "$$\n\\boxed{4 - \\sqrt{10}}\n$$"
        },
        {
            "introduction": "Once you implement the backpropagation algorithm in code, how can you be sure it's correct? This hands-on practice  introduces gradient checking, an indispensable technique for debugging neural network implementations. You will compare the analytical gradient computed by your backpropagation code against a numerical approximation from the finite difference method, confirming that your implementation is correct by observing the expected error behavior.",
            "id": "3100954",
            "problem": "Construct a verification of the backpropagation algorithm by comparing an analytically derived gradient of a two-layer neural network to a finite-difference approximation. The objective is to numerically confirm that the discrepancy between the two gradients behaves as an order-$\\epsilon$ truncation error when using a forward finite difference, that is, the error is $\\mathcal{O}(\\epsilon)$.\n\nUse the following purely mathematical setup.\n\n- Network architecture and data:\n  - Inputs have dimension $d = 3$, the hidden layer has $h = 3$ units with hyperbolic tangent activation, and the output layer has dimension $o = 1$ with a linear output.\n  - Given a mini-batch of size $n = 4$, the input matrix $X \\in \\mathbb{R}^{4 \\times 3}$ and target vector $y \\in \\mathbb{R}^{4 \\times 1}$ are:\n    $$\n    X =\n    \\begin{bmatrix}\n    0.2  -0.1  0.4 \\\\\n    -0.5  0.3  0.1 \\\\\n    0.0  -0.2  0.2 \\\\\n    0.1  0.4  -0.3\n    \\end{bmatrix}, \\quad\n    y =\n    \\begin{bmatrix}\n    0.5 \\\\ -0.1 \\\\ 0.2 \\\\ 0.0\n    \\end{bmatrix}.\n    $$\n  - Parameters for Test Case A are fixed as:\n    $$\n    W_1 =\n    \\begin{bmatrix}\n    0.3  -0.1  0.2 \\\\\n    -0.4  0.5  0.1 \\\\\n    0.2  0.3  -0.2\n    \\end{bmatrix}, \\quad\n    b_1 =\n    \\begin{bmatrix}\n    0.05 \\\\ -0.02 \\\\ 0.01\n    \\end{bmatrix}, \\quad\n    W_2 =\n    \\begin{bmatrix}\n    0.6  -0.7  0.2\n    \\end{bmatrix}, \\quad\n    b_2 =\n    \\begin{bmatrix}\n    0.03\n    \\end{bmatrix}.\n    $$\n    For Test Case B, use the same shapes but scale every entry of $W_1, b_1, W_2, b_2$ by a factor of $0.1$.\n\n- Forward model and loss:\n  - For each row $x_i^\\top$ of $X$, define the hidden pre-activation $z_{1,i}^\\top = x_i^\\top W_1^\\top + b_1^\\top$, hidden activation $h_i^\\top = \\tanh(z_{1,i}^\\top)$, output pre-activation $z_{2,i} = h_i^\\top W_2^\\top + b_2$, and prediction $\\hat{y}_i = z_{2,i}$.\n  - Define the mean squared error loss\n    $$\n    L(W_1,b_1,W_2,b_2) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( \\hat{y}_i - y_i \\right)^2.\n    $$\n\n- Analytical gradient via backpropagation:\n  - Using multivariable calculus, the chain rule, and the derivative identity $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$, derive the gradient of $L$ with respect to all parameters and implement it.\n  - Flatten the parameter set into a single vector $\\theta \\in \\mathbb{R}^{p}$ with $p = 16$ using the following order and memory layout:\n    1. Flatten $W_1 \\in \\mathbb{R}^{3 \\times 3}$ in row-major order.\n    2. Append $b_1 \\in \\mathbb{R}^{3}$.\n    3. Flatten $W_2 \\in \\mathbb{R}^{1 \\times 3}$ in row-major order.\n    4. Append $b_2 \\in \\mathbb{R}^{1}$.\n\n- Finite-difference approximation:\n  - For a given $\\epsilon  0$ and the standard basis vector $e_k$ in $\\mathbb{R}^p$, approximate the $k$-th component of $\\nabla_{\\theta} L$ by the forward difference\n    $$\n    \\frac{L(\\theta + \\epsilon e_k) - L(\\theta)}{\\epsilon}.\n    $$\n  - Use the list of step sizes\n    $$\n    \\mathcal{E} = \\left[ 10^{-1}, \\; 3 \\cdot 10^{-2}, \\; 10^{-2}, \\; 3 \\cdot 10^{-3}, \\; 10^{-3}, \\; 3 \\cdot 10^{-4}, \\; 10^{-4} \\right].\n    $$\n\n- Error metric and order verification:\n  - For each $\\epsilon \\in \\mathcal{E}$, compute the Euclidean norm of the difference between the analytical gradient and the finite-difference gradient,\n    $$\n    \\mathrm{err}(\\epsilon) = \\left\\| \\nabla_{\\theta} L - g_{\\mathrm{FD}}(\\epsilon) \\right\\|_2.\n    $$\n  - For consecutive $\\epsilon_i  \\epsilon_{i+1}$, compute the empirical order\n    $$\n    s_i = \\frac{\\log\\left( \\mathrm{err}(\\epsilon_i) / \\mathrm{err}(\\epsilon_{i+1}) \\right)}{\\log\\left( \\epsilon_i / \\epsilon_{i+1} \\right)}.\n    $$\n  - Define two boolean checks per test case:\n    1. Let $s_{\\mathrm{med}}$ be the median of $\\{ s_i \\}$. Define $\\mathrm{pass\\_order}$ to be true if $0.8 \\le s_{\\mathrm{med}} \\le 1.2$.\n    2. Define $\\mathrm{pass\\_mono}$ to be true if $\\mathrm{err}(\\epsilon)$ is strictly decreasing over the first $5$ values of $\\mathcal{E}$, that is, for $\\epsilon \\in \\{ 10^{-1}, 3\\cdot 10^{-2}, 10^{-2}, 3\\cdot 10^{-3}, 10^{-3} \\}$.\n\n- Test suite:\n  - Two test cases are specified by the parameter sets:\n    - Test Case A: the parameters exactly as given above.\n    - Test Case B: the same parameter shapes with every entry scaled by $0.1$ relative to Test Case A.\n  - In both cases, use the same $X$, $y$, and the same $\\mathcal{E}$.\n\n- Required program behavior and final output format:\n  - Your program must implement the forward model, derive and compute the analytical gradient via backpropagation from first principles, compute the finite-difference gradients for each $\\epsilon \\in \\mathcal{E}$, and evaluate the error norms and empirical orders.\n  - For each test case, produce a list with four entries: \n    $[ \\mathrm{err}(\\min \\mathcal{E}), \\; s_{\\mathrm{med}}, \\; \\mathrm{pass\\_order}, \\; \\mathrm{pass\\_mono} ]$.\n  - The final program output must be a single line containing a list with the two per-case lists, formatted exactly as a comma-separated list enclosed in square brackets, for example:\n    $$\n    \\left[ [a_1, a_2, a_3, a_4], [b_1, b_2, b_3, b_4] \\right]\n    $$\n    where each $a_j$ and $b_j$ is a boolean or a floating-point number. No other text must be printed.\n  - There are no physical units involved in this problem.\n\nYour implementation must be self-contained and must not read input. It must use the specified numerical values exactly as provided above.",
            "solution": "The objective is to numerically verify the correctness of the backpropagation algorithm for a two-layer neural network. This is achieved by comparing the analytically computed gradient with a numerical approximation obtained via the finite-difference method. The primary verification criterion is to confirm that the error between the analytical and numerical gradients decreases linearly with the finite-difference step size $\\epsilon$, characteristic of a forward-difference scheme's first-order truncation error, $\\mathcal{O}(\\epsilon)$.\n\n### Mathematical Model and Loss Function\n\nThe neural network architecture consists of an input layer, one hidden layer, and an output layer.\n- Input $X \\in \\mathbb{R}^{n \\times d}$ ($n=4, d=3$)\n- Hidden layer with $h=3$ units and $\\tanh$ activation.\n- Output layer with $o=1$ unit and linear activation.\n- Parameters: $W_1 \\in \\mathbb{R}^{h \\times d}$, $b_1 \\in \\mathbb{R}^{h \\times 1}$, $W_2 \\in \\mathbb{R}^{o \\times h}$, $b_2 \\in \\mathbb{R}^{o \\times 1}$.\n\nThe forward propagation of a mini-batch $X$ is defined by the following matrix operations:\n1.  **Hidden Layer Pre-activation**: The linear transformation for the hidden layer is given by $Z_1 = X W_1^\\top + \\mathbf{1}b_1^\\top$, where $\\mathbf{1} \\in \\mathbb{R}^{n \\times 1}$ is a vector of ones and its product with $b_1^\\top$ is handled via broadcasting. The resulting matrix $Z_1 \\in \\mathbb{R}^{n \\times h}$.\n2.  **Hidden Layer Activation**: The hyperbolic tangent activation function is applied element-wise: $H = \\tanh(Z_1)$, where $H \\in \\mathbb{R}^{n \\times h}$.\n3.  **Output Layer Pre-activation**: A second linear transformation produces the output pre-activations: $Z_2 = H W_2^\\top + \\mathbf{1}b_2^\\top$. The resulting matrix $Z_2 \\in \\mathbb{R}^{n \\times o}$.\n4.  **Prediction**: The network output is linear, so the prediction $\\hat{Y}$ is equal to the pre-activation: $\\hat{Y} = Z_2 \\in \\mathbb{R}^{n \\times o}$.\n\nThe performance of the network is quantified by the mean squared error (MSE) loss function, averaged over the mini-batch:\n$$\nL = \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\frac{1}{2n} \\|\\hat{Y} - Y\\|_F^2\n$$\nwhere $Y \\in \\mathbb{R}^{n \\times o}$ is the matrix of true target values.\n\n### Analytical Gradient via Backpropagation\n\nThe core of this task is to derive the gradient of the loss $L$ with respect to each parameter ($W_1, b_1, W_2, b_2$) using the multivariable chain rule. This process is known as backpropagation. We denote the gradient of the loss with respect to a matrix $M$ as $\\delta_M = \\frac{\\partial L}{\\partial M}$.\n\n1.  **Gradient at the Output**: The gradient of the loss with respect to the network's predictions $\\hat{Y}$ is:\n    $$\n    \\delta_{\\hat{Y}} = \\frac{\\partial L}{\\partial \\hat{Y}} = \\frac{1}{n} (\\hat{Y} - Y)\n    $$\n    Since $\\hat{Y} =Z_2$, we have $\\delta_{Z_2} = \\delta_{\\hat{Y}}$.\n\n2.  **Gradients for the Output Layer ($W_2, b_2$)**:\n    Using the chain rule on $Z_2 = H W_2^\\top + \\mathbf{1}b_2^\\top$:\n    $$\n    \\delta_{W_2} = \\frac{\\partial L}{\\partial W_2} = \\delta_{Z_2}^\\top H\n    $$\n    $$\n    \\delta_{b_2} = \\frac{\\partial L}{\\partial b_2} = (\\mathrm{sum}(\\delta_{Z_2}, \\text{axis}=0))^\\top = \\delta_{Z_2}^\\top \\mathbf{1}\n    $$\n\n3.  **Propagating the Gradient to the Hidden Layer**:\n    The gradient is propagated back to the hidden layer's activations $H$:\n    $$\n    \\delta_H = \\frac{\\partial L}{\\partial H} = \\delta_{Z_2} W_2\n    $$\n    Next, the gradient is propagated through the $\\tanh$ activation function, using $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$:\n    $$\n    \\delta_{Z_1} = \\frac{\\partial L}{\\partial Z_1} = \\delta_H \\odot (1 - H^2)\n    $$\n    where $\\odot$ denotes the element-wise (Hadamard) product.\n\n4.  **Gradients for the Hidden Layer ($W_1, b_1$)**:\n    Finally, using the chain rule on $Z_1 = X W_1^\\top + \\mathbf{1}b_1^\\top$:\n    $$\n    \\delta_{W_1} = \\frac{\\partial L}{\\partial W_1} = \\delta_{Z_1}^\\top X\n    $$\n    $$\n    \\delta_{b_1} = \\frac{\\partial L}{\\partial b_1} = (\\mathrm{sum}(\\delta_{Z_1}, \\text{axis}=0))^\\top = \\delta_{Z_1}^\\top \\mathbf{1}\n    $$\n\nThese matrix-form equations provide a complete algorithm for computing the analytical gradient.\n\n### Numerical Verification\n\nTo verify the analytical gradient, we compare it against a numerical approximation.\n\n- **Parameter Vectorization**: All network parameters ($W_1, b_1, W_2, b_2$) are flattened and concatenated into a single vector $\\theta \\in \\mathbb{R}^{p}$, with $p=16$. The specified order is $W_1$ (row-major), $b_1$, $W_2$ (row-major), and $b_2$.\n\n- **Finite-Difference Approximation**: The gradient is approximated using the first-order forward-difference formula. The $k$-th component of the gradient vector is estimated as:\n  $$\n  g_{\\mathrm{FD},k}(\\epsilon) = \\frac{L(\\theta + \\epsilon e_k) - L(\\theta)}{\\epsilon}\n  $$\n  where $e_k$ is the $k$-th standard basis vector and $\\epsilon$ is a small step size.\n\n- **Error Analysis and Order Verification**:\n  The discrepancy between the analytical gradient $\\nabla_\\theta L$ and the finite-difference approximation $g_{\\mathrm{FD}}(\\epsilon)$ is measured by the Euclidean norm of their difference:\n  $$\n  \\mathrm{err}(\\epsilon) = \\| \\nabla_\\theta L - g_{\\mathrm{FD}}(\\epsilon) \\|_2\n  $$\n  For a first-order method, this error is expected to be proportional to $\\epsilon$, i.e., $\\mathrm{err}(\\epsilon) \\approx C\\epsilon$. This implies that the ratio of errors for two step sizes $\\epsilon_i$ and $\\epsilon_{i+1}$ should be approximately equal to the ratio of the step sizes themselves. To quantify this relationship, we compute the empirical order of convergence $s_i$:\n  $$\n  s_i = \\frac{\\log(\\mathrm{err}(\\epsilon_i) / \\mathrm{err}(\\epsilon_{i+1}))}{\\log(\\epsilon_i / \\epsilon_{i+1})}\n  $$\n  A value of $s_i \\approx 1$ confirms the expected first-order convergence, thus validating the analytical gradient implementation. We use the median of the computed $s_i$ values for robustness. The verification is considered successful if this median order $s_{\\mathrm{med}}$ is within the range $[0.8, 1.2]$ and if the error is monotonically decreasing for the initial, larger values of $\\epsilon$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and solves the backpropagation verification problem.\n    \"\"\"\n    \n    # --- Givens from the problem statement ---\n    X = np.array([\n        [0.2, -0.1, 0.4],\n        [-0.5, 0.3, 0.1],\n        [0.0, -0.2, 0.2],\n        [0.1, 0.4, -0.3]\n    ])\n    y = np.array([\n        [0.5],\n        [-0.1],\n        [0.2],\n        [0.0]\n    ])\n    n = X.shape[0]  # Mini-batch size, n=4\n\n    # Parameters for Test Case A\n    W1_a = np.array([\n        [0.3, -0.1, 0.2],\n        [-0.4, 0.5, 0.1],\n        [0.2, 0.3, -0.2]\n    ])\n    b1_a = np.array([[0.05], [-0.02], [0.01]])\n    W2_a = np.array([[0.6, -0.7, 0.2]])\n    b2_a = np.array([[0.03]])\n    \n    test_cases_params = [\n        (W1_a, b1_a, W2_a, b2_a),\n        (W1_a * 0.1, b1_a * 0.1, W2_a * 0.1, b2_a * 0.1) # Test Case B\n    ]\n\n    epsilons = [1e-1, 3e-2, 1e-2, 3e-3, 1e-3, 3e-4, 1e-4]\n    \n    # --- Helper functions for parameter manipulation ---\n    def params_to_vec(W1, b1, W2, b2):\n        return np.concatenate([\n            W1.flatten(),\n            b1.flatten(),\n            W2.flatten(),\n            b2.flatten()\n        ])\n\n    def vec_to_params(theta):\n        W1 = theta[0:9].reshape(3, 3)\n        b1 = theta[9:12].reshape(3, 1)\n        W2 = theta[12:15].reshape(1, 3)\n        b2 = theta[15:16].reshape(1, 1)\n        return W1, b1, W2, b2\n        \n    # --- Core functions for forward/backward pass ---\n    def forward_pass(W1, b1, W2, b2):\n        Z1 = X @ W1.T + b1.T\n        H = np.tanh(Z1)\n        Z2 = H @ W2.T + b2.T\n        Y_hat = Z2\n        loss = (1 / (2 * n)) * np.sum((Y_hat - y)**2)\n        return loss, Z1, H, Y_hat\n\n    def analytical_gradient(W1, b1, W2, b2):\n        _loss, _Z1, H, Y_hat = forward_pass(W1, b1, W2, b2)\n        \n        # Backward pass\n        dZ2 = (1 / n) * (Y_hat - y)\n        dW2 = dZ2.T @ H\n        db2 = np.sum(dZ2, axis=0, keepdims=True)\n        \n        dH = dZ2 @ W2\n        dZ1 = dH * (1 - H**2)\n        dW1 = dZ1.T @ X\n        db1 = np.sum(dZ1, axis=0, keepdims=True).T\n        \n        return params_to_vec(dW1, db1, dW2, db2.T)\n\n    def run_verification(params):\n        W1, b1, W2, b2 = params\n        \n        # 1. Compute analytical gradient\n        grad_analytic = analytical_gradient(W1, b1, W2, b2)\n        \n        # 2. Compute finite-difference gradient for each epsilon\n        theta_base = params_to_vec(W1, b1, W2, b2)\n        loss_base, _, _, _ = forward_pass(W1, b1, W2, b2)\n        \n        errors = []\n        for eps in epsilons:\n            grad_fd = np.zeros_like(theta_base)\n            for k in range(len(theta_base)):\n                theta_p = np.copy(theta_base)\n                theta_p[k] += eps\n                \n                W1_p, b1_p, W2_p, b2_p = vec_to_params(theta_p)\n                loss_p, _, _, _ = forward_pass(W1_p, b1_p, W2_p, b2_p)\n                \n                grad_fd[k] = (loss_p - loss_base) / eps\n            \n            error = np.linalg.norm(grad_analytic - grad_fd)\n            errors.append(error)\n\n        # 3. Compute empirical orders\n        orders = []\n        for i in range(len(epsilons) - 1):\n            s_i = np.log(errors[i] / errors[i+1]) / np.log(epsilons[i] / epsilons[i+1])\n            orders.append(s_i)\n        \n        s_med = np.median(orders)\n        \n        # 4. Perform boolean checks\n        pass_order = 0.8 = s_med = 1.2\n        pass_mono = all(errors[i] > errors[i+1] for i in range(4))\n\n        # 5. Collect results\n        err_min_eps = errors[-1]\n        \n        return [err_min_eps, s_med, bool(pass_order), bool(pass_mono)]\n\n    # --- Main execution loop ---\n    final_results = []\n    for case_params in test_cases_params:\n        result = run_verification(case_params)\n        final_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The output format requires default float and boolean string representations.\n    # Creating the string manually ensures this.\n    outer_list = []\n    for case_result in final_results:\n        inner_list = []\n        inner_list.append(str(case_result[0]))\n        inner_list.append(str(case_result[1]))\n        inner_list.append(str(case_result[2]))\n        inner_list.append(str(case_result[3]))\n        outer_list.append(f\"[{','.join(inner_list)}]\")\n    print(f\"[{','.join(outer_list)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "A mathematically correct formula is not always enough; it must also be numerically stable when implemented on a computer. This practice  explores this critical concept by examining the Log-Sum-Exp function, a common building block in deep learning. You will discover how a naive implementation of its gradient can fail due to numerical overflow and then implement the \"Log-Sum-Exp trick\" to create a robust backward pass, a vital skill for building real-world models.",
            "id": "3181541",
            "problem": "Consider the function $f:\\mathbb{R}^n\\to\\mathbb{R}$ defined by $f(\\mathbf{z})=\\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)$, where $\\mathbf{z}=(z_1,z_2,\\dots,z_n)\\in\\mathbb{R}^n$. This function is widely used in deep learning as a building block for models and loss functions. Your task is to implement a numerically stable backward pass for $f(\\mathbf{z})$ that computes the gradient $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$ and to explore numerical stability for large magnitudes of $\\mathbf{z}$. Start from fundamental calculus principles (the chain rule, the derivative of the exponential function, and the derivative of the natural logarithm), and do not use any pre-derived \"shortcut\" formulas. Derive the mathematical expression for $\\frac{\\partial f}{\\partial z_k}$ in terms of $\\mathbf{z}$ using only these principles. Then, design a numerically stable algorithm to compute $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$ that avoids overflow and underflow when $\\lvert z_i\\rvert$ is large. In particular, implement the Log-Sum-Exp (LSE) trick: compute $m=\\max_i z_i$, and use the identity $\\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)=m+\\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right)$ to stabilize both the forward and backward computations.\n\nYour program must:\n- Implement a numerically stable forward function $F(\\mathbf{z})=\\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)$ using the identity above, with $m=\\max_i z_i$.\n- Implement the backward function that returns $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$ using your derived expression, computed in a numerically stable way via shifting by $m$.\n- Implement a naive (non-stable) backward function that directly uses $e^{z_i}$ without shifting by $m$.\n- Implement a central finite-difference approximation to $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$ by perturbing one coordinate at a time with a step size $\\varepsilon$, using a numerically stable forward computation. Use $\\varepsilon=10^{-6}$.\n\nFor each test case vector $\\mathbf{z}$ in the test suite below, compute:\n$1.$ The maximum absolute difference $d_{\\text{stable}}=\\max_k \\left\\lvert \\left(\\nabla_{\\mathbf{z}} f(\\mathbf{z})\\right)_k - \\left(\\nabla_{\\mathbf{z}} f(\\mathbf{z})\\right)^{\\text{FD}}_k \\right\\rvert$ between your numerically stable analytical gradient and the central finite-difference approximation.\n$2.$ A boolean $b_{\\text{naive}}$ that is $\\text{True}$ if the naive gradient contains any non-finite values (that is, $\\infty$ or $\\mathrm{NaN}$), and $\\text{False}$ otherwise.\n\nTest suite (each $\\mathbf{z}$ is a row vector):\n- Case $1$: $\\mathbf{z}=[1000,1000,1000]$.\n- Case $2$: $\\mathbf{z}=[1000,-1000,0]$.\n- Case $3$: $\\mathbf{z}=[-1000,-1000,-999]$.\n- Case $4$: $\\mathbf{z}=[800,-800,800,-800]$.\n\nDesign for coverage rationale:\n- Case $1$ tests symmetry and large equal values (the gradient components are expected to be equal).\n- Case $2$ tests extreme disparity with both very large positive and very large negative entries.\n- Case $3$ tests near-cancellation and underflow risks when all entries are very negative but close to each other.\n- Case $4$ tests multiple large-magnitude entries with repetition.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with entries ordered as $[d_{\\text{stable}}^{(1)},b_{\\text{naive}}^{(1)},d_{\\text{stable}}^{(2)},b_{\\text{naive}}^{(2)},d_{\\text{stable}}^{(3)},b_{\\text{naive}}^{(3)},d_{\\text{stable}}^{(4)},b_{\\text{naive}}^{(4)}]$, where the superscript indicates the case number. All floating-point quantities must be output in default floating-point notation, and booleans must be output as $\\text{True}$ or $\\text{False}$. No physical units, angle units, or percentages are involved in this problem.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of calculus and numerical analysis, well-posed with a clear objective, and provides all necessary information and constraints for a unique solution.\n\nWe are asked to derive and implement a numerically stable backward pass for the function $f:\\mathbb{R}^n \\to \\mathbb{R}$ defined by $f(\\mathbf{z}) = \\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)$, where $\\mathbf{z} = (z_1, z_2, \\dots, z_n)$.\n\n### 1. Mathematical Derivation of the Gradient\n\nThe primary task is to find the gradient of $f(\\mathbf{z})$, which is the vector of partial derivatives $\\nabla_{\\mathbf{z}} f(\\mathbf{z}) = \\left(\\frac{\\partial f}{\\partial z_1}, \\frac{\\partial f}{\\partial z_2}, \\dots, \\frac{\\partial f}{\\partial z_n}\\right)$. We will derive the expression for a single component, $\\frac{\\partial f}{\\partial z_k}$, using fundamental calculus principles.\n\nLet's decompose the function $f(\\mathbf{z})$ using the chain rule. Define an intermediate variable $S(\\mathbf{z})$ and a function $g(S)$:\n$$ S(\\mathbf{z}) = \\sum_{i=1}^{n} e^{z_i} $$\n$$ g(S) = \\log(S) $$\nThus, $f(\\mathbf{z}) = g(S(\\mathbf{z}))$.\n\nAccording to the multivariate chain rule, the partial derivative of $f$ with respect to $z_k$ is:\n$$ \\frac{\\partial f}{\\partial z_k} = \\frac{d g}{d S} \\cdot \\frac{\\partial S}{\\partial z_k} $$\n\nFirst, we compute the derivative of $g(S)$ with respect to $S$:\n$$ \\frac{d g}{d S} = \\frac{d}{dS} \\log(S) = \\frac{1}{S} $$\n\nNext, we compute the partial derivative of $S(\\mathbf{z})$ with respect to $z_k$:\n$$ \\frac{\\partial S}{\\partial z_k} = \\frac{\\partial}{\\partial z_k} \\left(\\sum_{i=1}^{n} e^{z_i}\\right) $$\nDue to the linearity of the differentiation operator, we can move the derivative inside the sum:\n$$ \\frac{\\partial S}{\\partial z_k} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial z_k} (e^{z_i}) $$\nThe derivative $\\frac{\\partial}{\\partial z_k} (e^{z_i})$ depends on whether $i=k$.\n- If $i=k$, then $\\frac{\\partial}{\\partial z_k} (e^{z_k}) = e^{z_k}$.\n- If $i \\neq k$, then $z_i$ is treated as a constant with respect to $z_k$, so $\\frac{\\partial}{\\partial z_k} (e^{z_i}) = 0$.\n\nTherefore, the sum collapses to a single term where $i=k$:\n$$ \\frac{\\partial S}{\\partial z_k} = e^{z_k} $$\n\nSubstituting these results back into the chain rule equation, we get:\n$$ \\frac{\\partial f}{\\partial z_k} = \\frac{1}{S} \\cdot e^{z_k} = \\frac{1}{\\sum_{i=1}^{n} e^{z_i}} \\cdot e^{z_k} = \\frac{e^{z_k}}{\\sum_{i=1}^{n} e^{z_i}} $$\nThis expression gives the $k$-th component of the gradient $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$. The gradient vector is thus the result of applying the softmax function to the input vector $\\mathbf{z}$.\n\n### 2. Numerical Stability and the Log-Sum-Exp Trick\n\nThe derived expression $\\frac{\\partial f}{\\partial z_k} = \\frac{e^{z_k}}{\\sum_{i=1}^{n} e^{z_i}}$ is numerically unstable.\n- **Overflow**: If any $z_i$ is a large positive number (e.g., $1000$), $e^{z_i}$ will exceed the maximum value representable by standard floating-point types, resulting in infinity (`inf`). The subsequent division `inf/inf` results in `NaN` (Not a Number).\n- **Underflow**: If all $z_i$ are large negative numbers (e.g., $-1000$), each $e^{z_i}$ will underflow to $0$. The sum in the denominator becomes $0$, leading to division by zero, which also results in `inf` or `NaN`.\n\nTo address this, we use the \"Log-Sum-Exp\" (LSE) trick. Let $m = \\max_{i} z_i$. We can multiply the numerator and denominator of the gradient expression by the non-zero constant $e^{-m}$:\n$$ \\frac{\\partial f}{\\partial z_k} = \\frac{e^{z_k} \\cdot e^{-m}}{\\left(\\sum_{i=1}^{n} e^{z_i}\\right) \\cdot e^{-m}} = \\frac{e^{z_k - m}}{\\sum_{i=1}^{n} e^{z_i - m}} $$\nThis stabilized formula avoids overflow because the argument of each exponential, $z_i - m$, is always less than or equal to $0$. Consequently, each term $e^{z_i - m}$ is in the range $(0, 1]$. The maximum value is $1$, which occurs when $z_i = m$. The stable formula also avoids underflow-induced division by zero because at least one term in the denominator sum is $e^0 = 1$, ensuring the sum is always at least $1$.\n\nThe same trick is applied to the forward pass computation of $f(\\mathbf{z})$ itself:\n$$ f(\\mathbf{z}) = \\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right) = \\log\\left(e^m \\sum_{i=1}^{n} e^{z_i-m}\\right) = \\log(e^m) + \\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right) = m + \\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right) $$\nThis form is used for the numerically stable forward computation.\n\n### 3. Algorithmic Implementation\n\nThe problem requires implementing four functions and comparing their results.\n\n1.  **Numerically Stable Forward Function, $F(\\mathbf{z})$**: This function first computes $m = \\max_i z_i$ and then returns the value $m + \\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right)$.\n\n2.  **Numerically Stable Backward Function, $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$**: This function computes the gradient using the stabilized softmax formula. It first finds $m = \\max_i z_i$. Then it computes the vector of numerators $[e^{z_1-m}, e^{z_2-m}, \\dots, e^{z_n-m}]$ and the denominator sum $\\sum_{i=1}^{n} e^{z_i-m}$. The final gradient vector is obtained by dividing each numerator by the sum.\n\n3.  **Naive Backward Function**: This function directly implements the original formula $\\frac{e^{z_k}}{\\sum_{i=1}^{n} e^{z_i}}$ for each component $k$. It is expected to produce non-finite values (`inf` or `NaN`) for the given test cases due to overflow or underflow.\n\n4.  **Central Finite-Difference Approximation, $(\\nabla_{\\mathbf{z}} f(\\mathbf{z}))^{\\text{FD}}$**: This serves as a numerical benchmark to verify the correctness of the analytical gradient implementation. For each component $k$, it approximates the partial derivative using the central difference formula with a small step size $\\varepsilon = 10^{-6}$:\n    $$ \\left(\\nabla_{\\mathbf{z}} f(\\mathbf{z})\\right)^{\\text{FD}}_k = \\frac{F(\\mathbf{z} + \\varepsilon \\mathbf{e}_k) - F(\\mathbf{z} - \\varepsilon \\mathbf{e}_k)}{2\\varepsilon} $$\n    where $\\mathbf{e}_k$ is the standard basis vector with a $1$ at index $k$ and $0$s elsewhere. The numerically stable forward function $F(\\mathbf{z})$ must be used for this calculation to obtain meaningful results for large-magnitude inputs.\n\nThe program will execute these functions on the provided test vectors and compute the maximum absolute difference $d_{\\text{stable}}$ between the stable analytical gradient and the finite-difference approximation, as well as a boolean flag $b_{\\text{naive}}$ indicating if the naive gradient computation resulted in any non-finite numbers.",
            "answer": "```python\nimport numpy as np\n\ndef stable_forward_lse(z: np.ndarray) - float:\n    \"\"\"\n    Computes the Log-Sum-Exp function in a numerically stable way.\n    f(z) = log(sum(exp(z_i)))\n    \"\"\"\n    m = np.max(z)\n    # The Log-Sum-Exp identity: m + log(sum(exp(z_i - m)))\n    return m + np.log(np.sum(np.exp(z - m)))\n\ndef backward_stable(z: np.ndarray) - np.ndarray:\n    \"\"\"\n    Computes the gradient of the Log-Sum-Exp function in a stable manner.\n    This is equivalent to the softmax function.\n    grad_k = exp(z_k - m) / sum(exp(z_i - m))\n    \"\"\"\n    m = np.max(z)\n    shifted_exp_z = np.exp(z - m)\n    return shifted_exp_z / np.sum(shifted_exp_z)\n\ndef backward_naive(z: np.ndarray) - np.ndarray:\n    \"\"\"\n    Computes the gradient of the Log-Sum-Exp function naively.\n    This is prone to overflow/underflow.\n    grad_k = exp(z_k) / sum(exp(z_i))\n    \"\"\"\n    # This is expected to fail for large inputs\n    exp_z = np.exp(z)\n    sum_exp_z = np.sum(exp_z)\n    return exp_z / sum_exp_z\n\ndef gradient_finite_difference(z: np.ndarray, epsilon: float = 1e-6) - np.ndarray:\n    \"\"\"\n    Computes the gradient of the Log-Sum-Exp function using central finite differences.\n    \"\"\"\n    n = z.shape[0]\n    grad_fd = np.zeros(n, dtype=np.float64)\n    \n    # Use a copy of z to avoid modifying the original\n    z_temp = z.astype(np.float64)\n\n    for i in range(n):\n        # Store original value\n        original_zi = z_temp[i]\n        \n        # Calculate f(z + epsilon * e_i)\n        z_temp[i] = original_zi + epsilon\n        f_plus = stable_forward_lse(z_temp)\n        \n        # Calculate f(z - epsilon * e_i)\n        z_temp[i] = original_zi - epsilon\n        f_minus = stable_forward_lse(z_temp)\n        \n        # Restore original value\n        z_temp[i] = original_zi\n        \n        # Central difference formula\n        grad_fd[i] = (f_plus - f_minus) / (2 * epsilon)\n        \n    return grad_fd\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        np.array([1000.0, 1000.0, 1000.0]),\n        np.array([1000.0, -1000.0, 0.0]),\n        np.array([-1000.0, -1000.0, -999.0]),\n        np.array([800.0, -800.0, 800.0, -800.0])\n    ]\n    \n    epsilon = 1e-6\n    results = []\n\n    for z in test_cases:\n        # 1. Compute stable analytical gradient and finite difference approximation\n        grad_stable = backward_stable(z)\n        grad_fd = gradient_finite_difference(z, epsilon)\n        \n        # Calculate the maximum absolute difference\n        d_stable = np.max(np.abs(grad_stable - grad_fd))\n        \n        # 2. Compute naive gradient and check for non-finite values\n        # Suppress RuntimeWarning for overflow in exp and invalid value in divide\n        with np.errstate(over='ignore', invalid='ignore'):\n             grad_naive = backward_naive(z)\n        \n        b_naive = np.any(~np.isfinite(grad_naive))\n        \n        results.append(d_stable)\n        results.append(b_naive)\n        \n    output_parts = []\n    for item in results:\n        if isinstance(item, (bool, np.bool_)):\n            output_parts.append(str(item))\n        else:\n            output_parts.append(f\"{item}\")\n            \n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```"
        }
    ]
}