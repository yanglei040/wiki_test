{
    "hands_on_practices": [
        {
            "introduction": "现代深度神经网络可能包含数十亿个参数，这使得它们的训练和部署成本高昂。低秩分解是一种强大的架构技术，它通过将大的权重矩阵表示为两个较小矩阵的乘积来压缩模型。本练习  将引导你量化通过此方法实现的显著参数削减，并分析此约束如何影响网络的表达能力，特别是与像 $\\mathrm{ReLU}$ 这样的非线性激活函数结合时，从而让你对模型设计的权衡有更深刻的理解。",
            "id": "3098849",
            "problem": "考虑一个全连接的深度前馈网络，其输入维度为 $n_0 = 600$，一个宽度为 $n_1 = 400$ 的隐藏层，以及输出维度 $n_2 = 200$。每一层都使用一个偏置向量。对于第一层，权重矩阵表示为 $W_1 \\in \\mathbb{R}^{n_1 \\times n_0}$，偏置为 $b_1 \\in \\mathbb{R}^{n_1}$；对于第二层，权重矩阵为 $W_2 \\in \\mathbb{R}^{n_2 \\times n_1}$，偏置为 $b_2 \\in \\mathbb{R}^{n_2}$。层间的激活函数是修正线性单元 (ReLU)，其逐元素定义为 $\\mathrm{ReLU}(z) = \\max(0, z)$。\n\n该网络在两种设计选择下进行训练：\n- 设计 $A$：未分解的权重 $W_1$ 和 $W_2$。\n- 设计 $B$：每个权重矩阵通过分解 $W_i = U_i V_i^\\top$ 被约束为低秩，其中 $U_1 \\in \\mathbb{R}^{n_1 \\times r_1}$，$V_1 \\in \\mathbb{R}^{n_0 \\times r_1}$，$U_2 \\in \\mathbb{R}^{n_2 \\times r_2}$ 和 $V_2 \\in \\mathbb{R}^{n_1 \\times r_2}$，且 $r_1 = 50, r_2 = 40$。\n\n仅使用仿射映射、矩阵秩以及线性复合和逐点非线性的基本定义，来推断参数数量和表示能力。选择所有正确的陈述：\n\nA. 设计 $A$ 中的可训练参数总数为 $320600$，而设计 $B$ 中为 $74600$。\n\nB. 在给定的维度和秩下，与设计 $A$ 相比，设计 $B$ 将参数数量减少了超过 $70\\%$。\n\nC. 如果移除激活函数（网络变为纯线性），则整体线性映射 $x \\mapsto W_2 W_1 x + W_2 b_1 + b_2$ 的矩阵部分 $W_2 W_1$ 的秩最多为 $\\min(r_1, r_2) = 40$。\n\nD. 在层间使用修正线性单元（ReLU）时，增加深度可以通过将输入空间划分为多个区域来弥补秩的限制，从而使网络在不同区域实现不同的低秩线性映射；尽管在任何单个输入处的雅可比矩阵的秩最多为 $\\min(r_1, r_2)$，但整体的分段线性函数可以逼近单个秩为 $r$ 的线性层无法逼近的映射。\n\nE. 在层间使用修正线性单元（ReLU）时，存在某些输入，使得网络的雅可比矩阵的秩严格大于 $\\min(r_1, r_2)$。",
            "solution": "我们从基本原理开始。一个全连接层计算一个仿射映射 $x \\mapsto W x + b$，其中 $W$ 是一个自由参数矩阵，$b$ 是一个自由参数向量。这样一个层中的参数数量等于 $W$ 中自由项的数量加上 $b$ 中项的数量。如果一个矩阵 $W \\in \\mathbb{R}^{m \\times n}$ 是无约束的，它有 $m n$ 个自由项。如果 $W$ 被约束为分解成 $U V^\\top$ 的形式，其中 $U \\in \\mathbb{R}^{m \\times r}$ 且 $V \\in \\mathbb{R}^{n \\times r}$，那么自由项的数量是 $m r + n r$，且矩阵的秩最多为 $r$。对于线性映射的复合，乘积的秩满足 $\\mathrm{rank}(A B) \\le \\min(\\mathrm{rank}(A), \\mathrm{rank}(B))$。对于具有像修正线性单元 (ReLU) 这样的逐点非线性的网络，函数变为分段线性的；在任何输入 $x$ 处，网络的雅可比矩阵等于权重矩阵和对角激活掩码的乘积，其秩受该乘积中最小秩瓶颈的限制。然而，不同区域的不同激活掩码会产生不同的局部线性映射，从而实现了比单个全局线性映射更丰富的函数类别。\n\n计算参数数量。\n\n- 设计 $A$ (未分解的):\n\n第一层参数：$W_1$ 有 $n_1 n_0 = 400 \\cdot 600 = 240000$ 个条目；$b_1$ 有 $n_1 = 400$ 个条目。第二层参数：$W_2$ 有 $n_2 n_1 = 200 \\cdot 400 = 80000$ 个条目；$b_2$ 有 $n_2 = 200$ 个条目。设计 $A$ 的总参数：\n$$240000 + 400 + 80000 + 200 = 320600.$$\n\n- 设计 $B$ (已分解的):\n\n第一层参数：$U_1$ 有 $n_1 r_1 = 400 \\cdot 50 = 20000$ 个条目；$V_1$ 有 $n_0 r_1 = 600 \\cdot 50 = 30000$ 个条目；$b_1$ 有 $n_1 = 400$ 个条目。总计：$$20000 + 30000 + 400 = 50400.$$\n第二层参数：$U_2$ 有 $n_2 r_2 = 200 \\cdot 40 = 8000$ 个条目；$V_2$ 有 $n_1 r_2 = 400 \\cdot 40 = 16000$ 个条目；$b_2$ 有 $n_2 = 200$ 个条目。总计：$$8000 + 16000 + 200 = 24200.$$\n设计 $B$ 的总参数：\n$$50400 + 24200 = 74600.$$\n\n计算相对减少量：\n$$\\text{减少比例} = 1 - \\frac{74600}{320600} \\approx 1 - 0.2326 \\approx 0.7674,$$\n这表示减少了大约 $76.74\\%$，即大于 $70\\%$。\n\n秩的性质。\n\n如果移除激活函数，整体映射是线性的：$x \\mapsto W_2 W_1 x + W_2 b_1 + b_2$。矩阵部分是 $W_2 W_1$。在分解约束下，$\\mathrm{rank}(W_1) \\le r_1$ 且 $\\mathrm{rank}(W_2) \\le r_2$。因此\n$$\\mathrm{rank}(W_2 W_1) \\le \\min(\\mathrm{rank}(W_2), \\mathrm{rank}(W_1)) \\le \\min(r_2, r_1) = 40.$$\n\n使用修正线性单元（ReLU），网络函数是分段线性的。对于任何特定输入 $x$，激活模式会产生一个对角线元素在 $\\{0,1\\}$ 中的对角矩阵 $D$，因此局部雅可比矩阵等于 $J(x) = W_2 D W_1$。因此，对于任何固定的 $x$，\n$$\\mathrm{rank}(J(x)) \\le \\min(\\mathrm{rank}(W_2), \\mathrm{rank}(D), \\mathrm{rank}(W_1)) \\le \\min(r_2, r_1) = 40.$$\n然而，随着 $x$ 的变化，$D$ 也会改变，在不同区域产生不同的局部线性映射 $W_2 D W_1$。这种划分允许网络实现大量不同的低秩线性片段的拼接，从而逼近单个全局低秩线性层无法表示的函数。这是带有非线性的深度补偿窄层或低秩层的核心机制：它增加了线性区域的数量并启用了复合结构，而不是提高给定输入点处雅可比矩阵的逐点秩。\n\n逐项分析：\n\nA. 上述计算表明，设计 $A$ 有 $320600$ 个参数，设计 $B$ 有 $74600$ 个参数。结论：正确。\n\nB. 减少量约为 $76.74\\%$，确实超过了 $70\\%$。结论：正确。\n\nC. 没有非线性时，根据秩的次可乘性质， $W_2 W_1$ 的秩最多为 $\\min(r_1, r_2) = 40$。结论：正确。\n\nD. 使用修正线性单元（ReLU），网络变为分段线性，在不同区域具有不同的激活掩码 $D$。每个区域的雅可比矩阵的秩受 $\\min(r_1, r_2)$ 限制，但网络可以通过拼接许多低秩片段来逼近单个秩为 r 的线性层无法表示的映射。这准确地描述了深度如何补偿秩的限制。结论：正确。\n\nE. 在任何输入 $x$ 处的雅可比矩阵等于 $W_2 D W_1$，其秩的上限为 $\\min(r_1, r_2)$。因此，它不能超过 $\\min(r_1, r_2)$。结论：不正确。",
            "answer": "$$\\boxed{ABCD}$$"
        },
        {
            "introduction": "除了层与连接之外，网络架构还具有一些微妙的结构特性，其中之一就是全连接层中的“置换对称性”。这种对称性意味着，对于任意一个解，都存在许多其他等价的参数配置，这可能在损失函数的地形上产生巨大的平坦区域。这个思想实验  挑战你思考不同的架构选择和正则化方法如何能够打破这种对称性，从而设计出具有更良性优化地形的网络，并更深入地体会每个组件的作用。",
            "id": "3098855",
            "problem": "考虑一个具有 $L$ 层的全连接深度前馈网络，其输入为 $x \\in \\mathbb{R}^{d}$，隐藏表示为 $h^{(l)} \\in \\mathbb{R}^{k_l}$，逐元素非线性函数为 $\\phi$，参数为 $\\theta = \\{(W^{(l)}, b^{(l)})\\}_{l=1}^{L}$。对于 $l \\in \\{1,\\dots,L-1\\}$，\n$$\nh^{(l)} = \\phi\\!\\left(W^{(l)} h^{(l-1)} + b^{(l)}\\right), \\quad h^{(0)} = x,\n$$\n输出层为 $h^{(L)} = f_{\\theta}(x)$（为简单起见，假设任何逐点输出非线性函数也是逐元素的）。该网络通过最小化经验风险来进行训练：\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\big(f_{\\theta}(x_i), y_i\\big),\n$$\n其中数据集为 $\\{(x_i, y_i)\\}_{i=1}^{N}$，损失函数为逐点损失 $\\ell(\\cdot,\\cdot)$。\n\n一个置换矩阵 $P \\in \\mathbb{R}^{k \\times k}$ 满足 $P P^{\\top} = I$，并且能够置换 $\\mathbb{R}^{k}$ 的标准基。请仅使用以下事实进行推理：$\\phi$ 是逐元素作用的，并且乘以一个置换矩阵会重排向量的分量。在宽度为 $k_l$ 的隐藏层中，神经元置换可以产生多组参数设置，这些设置对每个输入计算出相同的函数 $f_{\\theta}$，从而产生多个等价的 $\\mathcal{L}(\\theta)$ 最小值点。\n\n你的任务是：以下哪些架构选择或约束可以通过显式地打破层内的神经元置换对称性来减少此类由置换导致的等价最小值点的数量，从而减少等价解格局中由对称性引起的平坦区域？请选择所有适用的选项。\n\nA. 在训练期间对每个隐藏单元应用 Dropout，丢弃概率 $p \\in (0,1)$ 相同，且在测试时没有附加约束。\n\nB. 将每个全连接隐藏层替换为掩码全连接层，该层使用固定的二进制掩码 $M^{(l)}$ 来强制实现自回归顺序：$M^{(l)}$ 的第 $i$ 行仅在预先指定的索引集 $S_i \\subset \\{1,\\dots,k_{l-1}\\}$ 中有非零项，且支持集严格嵌套 $S_1 \\subset S_2 \\subset \\dots \\subset S_{k_l}$，下一层则使用与此顺序耦合的固定逐列掩码。\n\nC. 对每个权重矩阵添加欧几里得范数（弗罗贝尼乌斯范数）权重衰减，即在训练目标中添加 $\\lambda \\sum_{l=1}^{L} \\|W^{(l)}\\|_{F}^{2}$，其中 $\\lambda > 0$。\n\nD. 对每个隐藏层的输入权重向量添加一个索引相关的范数目标正则化器：选择严格递增的目标值 $t_1  t_2  \\dots  t_{k_l}$，并添加 $\\lambda \\sum_{l=1}^{L-1} \\sum_{i=1}^{k_l} \\big(\\|w^{(l)}_{i:}\\|_{2} - t_i\\big)^{2}$，其中 $w^{(l)}_{i:}$ 是 $W^{(l)}$ 的第 $i$ 行。\n\nE. 在每个隐藏层之后插入批量归一化 (BN)，带有可学习的逐单元缩放因子 $\\gamma_i$ 和偏置 $\\beta_i$（批量归一化 (BN) 使用批量统计数据对每个单元的激活前值进行归一化，然后应用一个由 $\\gamma_i$ 和 $\\beta_i$ 控制的仿射变换）。",
            "solution": "该问题探究了全连接深度前馈网络中的置换对称性，并询问哪些机制可以打破这种对称性。如果存在多组不同的参数设置 $\\theta$，它们对所有输入 $x$ 都能产生完全相同的函数 $f_{\\theta}(x)$，并且总训练目标 $\\mathcal{J}(\\theta)$ 的值也相同，那么就存在对称性。训练目标是经验风险与任何正则化项之和，即 $\\mathcal{J}(\\theta) = \\mathcal{L}(\\theta) + R(\\theta)$。\n\n首先，让我们形式化地描述置换对称性。考虑一个具有 $k_l$ 个神经元的隐藏层 $l$。该层中的神经元索引为 $1$ 到 $k_l$。这些神经元的一个置换可以用一个 $k_l \\times k_l$ 的置换矩阵 $P$ 来表示。如果我们置换第 $l$ 层的神经元，我们就是在重新排列它们的顺序。为了让网络计算出相同的整体函数，这个置换必须被下一层（第 $l+1$ 层）中的相应变化所抵消。\n\n相关的变换如下：\n- 第 $l$ 层的激活前值为 $z^{(l)} = W^{(l)} h^{(l-1)} + b^{(l)}$。\n- 第 $l$ 层的激活值为 $h^{(l)} = \\phi(z^{(l)})$。由于 $\\phi$ 是逐元素的，它与置换算子是可交换的：$\\phi(P z^{(l)}) = P \\phi(z^{(l)})$。\n\n第 $l$ 层神经元的一个置换对应于一组新的参数 $\\tilde{\\theta}$，其中：\n1. 第 $l$ 层的权重和偏置被置换：$\\tilde{W}^{(l)} = P W^{(l)}$ 和 $\\tilde{b}^{(l)} = P b^{(l)}$。这会置换权重矩阵的行和偏置向量的元素。结果是得到置换后的激活值：$\\tilde{h}^{(l)} = \\phi(\\tilde{W}^{(l)}h^{(l-1)} + \\tilde{b}^{(l)}) = \\phi(P(W^{(l)}h^{(l-1)} + b^{(l)})) = P h^{(l)}$。\n2. 为确保第 $l+1$ 层的输入保持不变，第 $l+1$ 层的权重必须进行调整。原始的第 $l+1$ 层激活前值为 $z^{(l+1)} = W^{(l+1)}h^{(l)} + b^{(l+1)}$。新的激活前值为 $\\tilde{z}^{(l+1)} = \\tilde{W}^{(l+1)}\\tilde{h}^{(l)} + \\tilde{b}^{(l+1)}$。我们要求 $\\tilde{z}^{(l+1)} = z^{(l+1)}$。代入 $\\tilde{h}^{(l)} = Ph^{(l)}$ 并设置 $\\tilde{b}^{(l+1)} = b^{(l+1)}$，我们需要 $\\tilde{W}^{(l+1)}Ph^{(l)} = W^{(l+1)}h^{(l)}$。如果 $\\tilde{W}^{(l+1)}P = W^{(l+1)}$，这对所有 $h^{(l)}$ 都成立，这意味着 $\\tilde{W}^{(l+1)} = W^{(l+1)}P^{-1} = W^{(l+1)}P^{\\top}$。这个变换置换了 $W^{(l+1)}$ 的列。\n3. 网络中所有其他参数保持不变。\n\n这组新的参数 $\\tilde{\\theta}$ 产生完全相同的函数，因此对所有 $x$ 都有 $f_{\\tilde{\\theta}}(x) = f_{\\theta}(x)$，从而经验风险不变：$\\mathcal{L}(\\tilde{\\theta}) = \\mathcal{L}(\\theta)$。如果总目标 $\\mathcal{J}(\\theta)$ 不是不变的，那么对称性就被打破了。这意味着正则化项 $R(\\theta)$ 必须改变，即对于一个非单位置换 $P$，$R(\\tilde{\\theta}) \\neq R(\\theta)$。如果正则化器或架构约束对不同索引的神经元进行区别对待，就会发生这种情况。\n\n我们现在根据这个原则来评估每个选项。\n\n**A. 在训练期间对每个隐藏单元应用 Dropout，丢弃概率 $p \\in (0,1)$ 相同，且在测试时没有附加约束。**\nDropout 是一种训练时的正则化技术。在测试时，当评估函数 $f_{\\theta}$ 时，Dropout 是不激活的。置换对称性的论证适用于由参数 $\\theta$ 指定的最终确定性函数。等价参数集 $\\theta$ 和 $\\tilde{\\theta}$ 的存在是网络架构和损失格局的属性，与随机训练过程无关。此外，即使在训练期间，Dropout 对每个隐藏单元都以相同的概率 $p$ 应用。它不区分神经元 $i$ 和神经元 $j$。这意味着神经元在随机意义上仍然是可互换的。Dropout 没有引入任何与索引相关的约束或惩罚。因此，置换对称性没有被打破。\n**结论：不正确**\n\n**B. 将每个全连接隐藏层替换为掩码全连接层，该层使用固定的二进制掩码 $M^{(l)}$ 来强制实现自回归顺序...**\n该选项引入了一个固定的二进制掩码 $M^{(l)}$，使得第 $l$ 层的计算为 $h^{(l)} = \\phi((M^{(l)} \\odot W^{(l)}) h^{(l-1)} + b^{(l)})$。符号 $\\odot$ 表示逐元素乘积。该掩码强制执行一个具有严格嵌套支持集的自回归结构，$S_1 \\subset S_2 \\subset \\dots \\subset S_{k_l}$，其中 $S_i$ 是神经元 $i$ 的允许输入连接的索引集。这意味着掩码矩阵 $M^{(l)}$ 的各行之间不是置换关系；对于任意一对索引 $i \\neq j$，$M^{(l)}$ 的第 $i$ 行都不同于第 $j$ 行。\n让我们对一个置换 $P$ 应用对称性变换。新的参数将是 $\\tilde{W}^{(l)} = P W^{(l)}$ 和 $\\tilde{b}^{(l)} = P b^{(l)}$。在*固定*掩码 $M^{(l)}$ 下的有效权重矩阵将是 $M^{(l)} \\odot \\tilde{W}^{(l)} = M^{(l)} \\odot (P W^{(l)})$。这通常不等于原始有效权重矩阵的置换 $P (M^{(l)} \\odot W^{(l)})$，因为掩码 $M^{(l)}$ 不会随着权重一起被置换。由于掩码 $M^{(l)}$ 根据其索引 $i$ 显式地对每个神经元的输入连接性进行不同处理，神经元不再是可互换的。对 $W^{(l)}$ 的行进行任何置换都会改变有效连接图，从而改变网络计算的函数。因此，这种架构约束显式地打破了置换对称性。\n**结论：正确**\n\n**C. 对每个权重矩阵添加欧几里得范数（弗罗贝尼乌斯范数）权重衰减...**\n正则化项为 $R(\\theta) = \\lambda \\sum_{l=1}^{L} \\|W^{(l)}\\|_{F}^{2}$。弗罗贝尼乌斯范数定义为 $\\|A\\|_{F} = \\sqrt{\\sum_{i,j} A_{ij}^2}$。\n让我们分析在第 $l$ 层进行置换时，对称性变换对这个正则化器的影响。受影响的权重矩阵是 $W^{(l)}$ 和 $W^{(l+1)}$。\n新的权重是 $\\tilde{W}^{(l)} = P W^{(l)}$ 和 $\\tilde{W}^{(l+1)} = W^{(l+1)} P^{\\top}$。\n弗罗贝尼乌斯范数在左乘或右乘一个正交（或酉）矩阵时保持不变。置换矩阵 $P$ 是一个正交矩阵（$PP^{\\top}=I$）。\n因此，$\\|\\tilde{W}^{(l)}\\|_{F}^{2} = \\|P W^{(l)}\\|_{F}^{2} = \\|W^{(l)}\\|_{F}^{2}$。这是因为左乘 $P$ 只会打乱 $W^{(l)}$ 的行，这不会改变其元素平方和。\n同样地，$\\|\\tilde{W}^{(l+1)}\\|_{F}^{2} = \\|W^{(l+1)} P^{\\top}\\|_{F}^{2} = \\|W^{(l+1)}\\|_{F}^{2}$。\n由于所有其他权重矩阵都未改变，总正则化项是不变的：$R(\\tilde{\\theta}) = R(\\theta)$。又因为 $\\mathcal{L}(\\tilde{\\theta}) = \\mathcal{L}(\\theta)$，总目标也是不变的：$\\mathcal{J}(\\tilde{\\theta}) = \\mathcal{J}(\\theta)$。置换对称性没有被打破。\n**结论：不正确**\n\n**D. 对每个隐藏层的输入权重向量添加一个索引相关的范数目标正则化器...**\n正则化项是 $R(\\theta) = \\lambda \\sum_{l=1}^{L-1} \\sum_{i=1}^{k_l} \\big(\\|w^{(l)}_{i:}\\|_{2} - t_i\\big)^{2}$，其中 $w^{(l)}_{i:}$ 是 $W^{(l)}$ 的第 $i$ 行，目标值 $t_i$ 是严格递增的，$t_1  t_2  \\dots  t_{k_l}$。\n这个正则化器通过唯一的目标值 $t_i$ 显式地依赖于每个神经元的索引 $i$。让我们考虑一个交换两个神经元（比如 1 和 2）的置换 $P$。变换后的权重矩阵是 $\\tilde{W}^{(l)} = P W^{(l)}$，所以它的第一行是原来的第二行（$\\tilde{w}^{(l)}_{1:} = w^{(l)}_{2:}$），它的第二行是原来的第一行（$\\tilde{w}^{(l)}_{2:} = w^{(l)}_{1:}$）。\n第 $l$ 层中这两个神经元对正则化器的贡献从\n$R_{1,2}(\\theta) = \\big(\\|w^{(l)}_{1:}\\|_{2} - t_1\\big)^{2} + \\big(\\|w^{(l)}_{2:}\\|_{2} - t_2\\big)^{2}$\n变为\n$R_{1,2}(\\tilde{\\theta}) = \\big(\\|\\tilde{w}^{(l)}_{1:}\\|_{2} - t_1\\big)^{2} + \\big(\\|\\tilde{w}^{(l)}_{2:}\\|_{2} - t_2\\big)^{2} = \\big(\\|w^{(l)}_{2:}\\|_{2} - t_1\\big)^{2} + \\big(\\|w^{(l)}_{1:}\\|_{2} - t_2\\big)^{2}$。\n由于 $t_1 \\neq t_2$，通常情况下 $R_{1,2}(\\tilde{\\theta}) \\neq R_{1,2}(\\theta)$，除非 $\\|w^{(l)}_{1:}\\|_{2} = \\|w^{(l)}_{2:}\\|_{2}$。然而，该正则化器主动促使 $\\|w^{(l)}_{i:}\\|_{2}$ 接近 $t_i$。在总目标函数的最小值点，我们期望 $\\|w^{(l)}_{i:}\\|_{2} \\approx t_i$，因此 $\\|w^{(l)}_{1:}\\|_{2} \\approx t_1 \\neq t_2 \\approx \\|w^{(l)}_{2:}\\|_{2}$。因此，置换后的参数集 $\\tilde{\\theta}$ 将具有不同的（并且可能更高的）目标值，即 $\\mathcal{J}(\\tilde{\\theta}) \\neq \\mathcal{J}(\\theta)$。这些参数设置不再是等价的最小值点。对称性被打破。\n**结论：正确**\n\n**E. 在每个隐藏层之后插入批量归一化 (BN)，带有可学习的逐单元缩放因子 $\\gamma_i$ 和偏置 $\\beta_i$。**\n批量归一化为每个神经元的激活前值引入一个归一化步骤，然后是一个可学习的仿射变换。第 $l$ 层中 BN 的参数是大小为 $k_l$ 的向量 $\\gamma^{(l)}$ 和 $\\beta^{(l)}$。这些参数与神经元相关联。神经元 $i$ 的激活前值被转换为 $a_i^{(l)} = \\gamma_i^{(l)} \\frac{z_i^{(l)} - \\mu_{B,i}}{\\sqrt{\\sigma^2_{B,i} + \\epsilon}} + \\beta_i^{(l)}$。\n当我们使用矩阵 $P$ 置换第 $l$ 层的神经元时，与每个神经元相关联的整个“单元”都被置换了。这意味着不仅是输入权重 ($W^{(l)}$) 和偏置 ($b^{(l)}$) 被置换，BN 的参数也被置换了：$\\tilde{\\gamma}^{(l)} = P \\gamma^{(l)}$ 和 $\\tilde{\\beta}^{(l)} = P \\beta^{(l)}$。权重和偏置的置换导致了置换后的激活前值 $\\tilde{z}^{(l)} = P z^{(l)}$。这反过来意味着批量统计数据也被置换。随后应用置换后的 BN 参数 $\\tilde{\\gamma}^{(l)}$ 和 $\\tilde{\\beta}^{(l)}$ 会得到一个置换后的、经过 BN 处理的激活前向量 $\\tilde{a}^{(l)} = P a^{(l)}$。由于非线性函数 $\\phi$ 是逐元素的，最终的层激活值也被置换：$\\tilde{h}^{(l)} = P h^{(l)}$。这与没有 BN 的网络中的情况相同。其效果通过置换下一层权重矩阵的列来抵消，即 $\\tilde{W}^{(l+1)} = W^{(l+1)} P^{\\top}$。整套参数 $(\\theta, \\gamma, \\beta)$ 有一个对应的置换集 $(\\tilde{\\theta}, \\tilde{\\gamma}, \\tilde{\\beta})$，它们计算出完全相同的函数。因此，BN 不会打破置换对称性。\n**结论：不正确**",
            "answer": "$$\\boxed{BD}$$"
        },
        {
            "introduction": "一个简单的浅层网络如何能学习高度复杂的函数？一个答案在于巧妙的输入特征工程。位置编码是一种通过一组固定的正弦函数将低维输入映射到高维特征空间的技术，它在 Transformers 和神经辐射场 (NeRF) 等模型中得到广泛应用。通过这个编码练习 ，你将亲手实现并检验这一思想。通过量化“表达能力提升”的程度，你将获得一个具体的、动手操作的理解：即使是浅层网络，如何通过这种架构策略捕捉复杂的高频细节——这是许多现代深度学习成功的核心原则。",
            "id": "3098829",
            "problem": "本题要求您形式化并量化一种固定的非线性特征映射（称为位置编码）如何相对于直接在原始输入上操作的基线模型，增强浅层前馈架构的有效表达能力。请考虑在域 $[0,1]$ 上的以下组件和定义。\n\n1. 将阶数为 $K$ 的位置编码 $\\gamma_K(x)$ 定义为在指数级增长的角频率下的正弦和余弦特征的拼接：\n$$\n\\gamma_K(x) = \\Big[ \\sin(2^0 \\pi x), \\cos(2^0 \\pi x), \\sin(2^1 \\pi x), \\cos(2^1 \\pi x), \\ldots, \\sin(2^K \\pi x), \\cos(2^K \\pi x) \\Big].\n$$\n\n2. 定义一个基于位置编码的浅层模型类，即对 $\\gamma_K(x)$ 的线性读出，\n$$\n\\mathcal{F}_{\\text{PE},K} = \\left\\{ f(x) = \\sum_{k=0}^{K} \\big(a_k \\sin(2^k \\pi x) + b_k \\cos(2^k \\pi x)\\big) + c \\,:\\, a_k, b_k, c \\in \\mathbb{R} \\right\\}.\n$$\n这是一个在固定非线性特征上的单层线性模型，是在深度学习实践中带有预设特征图的标准浅层架构。\n\n3. 定义一个在原始输入上操作、不使用位置编码的基线浅层模型，\n$$\n\\mathcal{F}_{\\text{plain}} = \\left\\{ f(x) = w_1 x + w_0 \\,:\\, w_1, w_0 \\in \\mathbb{R} \\right\\}.\n$$\n\n4. 定义在 $[0,1]$ 上要逼近的目标函数：\n   - 单个高频目标：$g_1(x) = \\sin(2^m \\pi x)$，其中 $m = 3$。\n   - 混合频率目标：$g_2(x) = 0.5 \\sin(2 \\pi x) + 0.25 \\cos(4 \\pi x) + 0.2 \\sin(8 \\pi x)$。\n   - 平滑多项式目标：$g_3(x) = x^2$。\n\n5. 设一个模型类对目标 $g(x)$ 在均匀网格 $x_i = \\frac{i}{N-1}$（其中 $i \\in \\{0,1,\\ldots,N-1\\}$）上的经验最小二乘拟合，定义为最小化\n$$\n\\frac{1}{N}\\sum_{i=0}^{N-1} \\big( f(x_i) - g(x_i) \\big)^2.\n$$\n在本题中，使用大小为 $N = 4096$ 个点的网格，并通过对模型特征进行线性最小二乘法来获得最小化器。\n\n6. 将拟合 $f$ 对目标 $g$ 的经验均方误差 (MSE) 定义为\n$$\nE(f, g) = \\frac{1}{N}\\sum_{i=0}^{N-1} \\big( f(x_i) - g(x_i) \\big)^2.\n$$\n\n7. 对于给定的目标 $g$ 和位置编码阶数 $K$，将表达能力提升比定义为\n$$\nR(g, K) = \\frac{E\\big(f_{\\text{plain}}^\\star, g\\big)}{E\\big(f_{\\text{PE},K}^\\star, g\\big)},\n$$\n其中 $f_{\\text{plain}}^\\star \\in \\mathcal{F}_{\\text{plain}}$ 和 $f_{\\text{PE},K}^\\star \\in \\mathcal{F}_{\\text{PE},K}$ 分别表示各自类别中的最小二乘最小化器。为避免在可完美表示的情况下出现数值上的除零错误，在分母中使用一个下限 $10^{-12}$，即除以 $\\max\\big(E(f_{\\text{PE},K}^\\star, g), 10^{-12}\\big)$。\n\n8. 基于在 $[0,1]$ 上的最大符号变化次数（零点交叉数）定义一个简单的容量代理指标：\n   - 对于基线模型 $\\mathcal{F}_{\\text{plain}}$，最大符号变化次数为 $S_{\\text{plain}} = 1$。\n   - 对于位置编码模型 $\\mathcal{F}_{\\text{PE},K}$（一个最大角频率为 $2^K \\pi$ 的三角多项式），使用上界 $S_{\\text{PE}}(K) = 2 \\cdot 2^K$。\n\n您的任务：\n\nA) 对每个目标 $g_1, g_2, g_3$ 和 $K \\in \\{0,1,3\\}$，实现 $\\mathcal{F}_{\\text{plain}}$ 和 $\\mathcal{F}_{\\text{PE},K}$ 的最小二乘拟合，并计算 $R(g,K)$。\n\nB) 对于 $K \\in \\{0,1,3\\}$，计算符号变化容量代理指标 $S_{\\text{plain}}$ 和 $S_{\\text{PE}}(K)$。\n\n测试套件和要求的输出：\n\n- 使用 $N = 4096$， $m = 3$， $K \\in \\{0,1,3\\}$，以及上面指定的三个目标 $g_1, g_2, g_3$。\n- 按此确切顺序生成包含以下 $13$ 个值的单行输出：\n$$\n\\big[ S_{\\text{plain}}, S_{\\text{PE}}(0), S_{\\text{PE}}(1), S_{\\text{PE}}(3), R(g_1,0), R(g_1,1), R(g_1,3), R(g_2,0), R(g_2,1), R(g_2,3), R(g_3,0), R(g_3,1), R(g_3,3) \\big].\n$$\n所有输出均为不带单位的实数或整数。您的程序应生成单行输出，其中包含用逗号分隔并用方括号括起来的结果列表（例如，$\\big[ \\text{result}_1, \\text{result}_2, \\ldots \\big]$）。",
            "solution": "本题要求我们量化由位置编码特征所带来的表达能力相对于简单线性模型的提升。问题的核心在于使用线性最小二乘法进行函数逼近。我们将求解两个不同模型类 $\\mathcal{F}_{\\text{plain}}$ 和 $\\mathcal{F}_{\\text{PE},K}$ 的最优参数，以最佳拟合三个不同的目标函数，然后比较它们产生的逼近误差。\n\n基本原理是，两个模型类 $\\mathcal{F}_{\\text{plain}}$ 和 $\\mathcal{F}_{\\text{PE},K}$ 在其参数上都是线性的。如果一个函数 $f(x; \\mathbf{w})$ 可以写成基函数 $\\phi_j(x)$ 的线性组合，那么它在其参数 $\\mathbf{w} = (w_1, \\ldots, w_p)^T$ 上是线性的：\n$$\nf(x; \\mathbf{w}) = \\sum_{j=1}^{p} w_j \\phi_j(x) = \\mathbf{w}^T \\mathbf{\\phi}(x)\n$$\n线性最小二乘的目标是找到参数向量 $\\mathbf{w}^\\star$，以最小化在一组 $N$ 个数据点 $(x_i, g_i)$ 上的均方误差 (MSE)：\n$$\nE(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( \\sum_{j=1}^{p} w_j \\phi_j(x_i) - g(x_i) \\right)^2\n$$\n以矩阵形式表示，令 $\\mathbf{g}$ 为目标值 $g(x_i)$ 的向量，$\\mathbf{\\Phi}$ 为 $N \\times p$ 的设计矩阵，其中 $\\mathbf{\\Phi}_{ij} = \\phi_j(x_i)$。MSE 为 $E(\\mathbf{w}) = \\frac{1}{N} \\| \\mathbf{\\Phi}\\mathbf{w} - \\mathbf{g} \\|_2^2$。最小化此误差的最优 $\\mathbf{w}^\\star$ 可以通过求解称为正规方程的线性系统 $\\mathbf{\\Phi}^T\\mathbf{\\Phi}\\mathbf{w} = \\mathbf{\\Phi}^T\\mathbf{g}$ 来找到，或者通过更稳健的方法，如 QR 分解或奇异值分解 (SVD)，这些都是数值库中的标准方法。\n\n容量代理指标 $S_{\\text{plain}}$ 和 $S_{\\text{PE}}(K)$ 直接根据所提供的公式计算。$S_{\\text{plain}} = 1$ 是固定的。对于 $K \\in \\{0, 1, 3\\}$，我们计算 $S_{\\text{PE}}(K) = 2 \\cdot 2^K$。\n\n主要的计算任务是为每个模型和目标找到最小化后的误差 $E(f^\\star, g)$。\n\n1.  定义网格和目标：我们首先在区间 $[0,1]$ 上创建一个包含 $N=4096$ 个点的均匀网格 $x_i = \\frac{i}{N-1}$。然后，我们在此网格上评估目标函数 $g_1(x) = \\sin(8\\pi x)$、$g_2(x) = 0.5 \\sin(2 \\pi x) + 0.25 \\cos(4 \\pi x) + 0.2 \\sin(8 \\pi x)$ 和 $g_3(x) = x^2$，以获得目标向量 $\\mathbf{g}_1, \\mathbf{g}_2, \\mathbf{g}_3$。\n\n2.  拟合基线模型 $\\mathcal{F}_{\\text{plain}}$：该模型为 $f(x) = w_1 x + w_0$。基函数为 $\\phi_1(x) = x$ 和 $\\phi_2(x) = 1$。设计矩阵 $\\mathbf{\\Phi}_{\\text{plain}}$ 是一个 $N \\times 2$ 矩阵，其第一列为 $x_i$ 值，第二列全为 1。我们对每个目标 $\\mathbf{g}_j$ 求解最小二乘问题，以找到最小化的 MSE，$E(f_{\\text{plain}}^\\star, g_j)$。\n\n3.  拟合位置编码模型 $\\mathcal{F}_{\\text{PE},K}$：对于每个阶数 $K \\in \\{0, 1, 3\\}$，该模型是一个三角多项式。基函数为 $\\{\\sin(2^k\\pi x), \\cos(2^k\\pi x)\\}_{k=0}^K$ 外加一个常数基函数 $\\phi(x)=1$。参数（和基函数）总数为 $p = 2(K+1) + 1$。对于每个 $K$，我们通过在网格上评估这些基函数来构造相应的 $N \\times p$ 设计矩阵 $\\mathbf{\\Phi}_{\\text{PE},K}$。对于每个目标 $\\mathbf{g}_j$，我们求解最小二乘问题，以找到最小化的 MSE，$E(f_{\\text{PE},K}^\\star, g_j)$。\n\n4.  计算表达能力提升比：在计算出两个模型类的最小 MSE 后，我们为每个目标 $g_j$ 和阶数 $K$ 计算比率 $R(g, K)$：\n    $$\n    R(g_j, K) = \\frac{E(f_{\\text{plain}}^\\star, g_j)}{\\max\\big(E(f_{\\text{PE},K}^\\star, g_j), 10^{-12}\\big)}\n    $$\n    这个比率衡量了位置编码模型的误差相比于基线线性模型要小多少倍。大的比率表示显著的表达能力优势。分母设置了下限，以防止除零，当模型类 $\\mathcal{F}_{\\text{PE},K}$ 能够完美表示目标函数时（例如，当 $K=3$ 时对于 $g_1, g_2$），这种情况很可能发生。\n\n该算法首先计算容量代理指标。然后，对于三个目标函数中的每一个，它计算基线误差 $E(f_{\\text{plain}}^\\star, g)$。接着，对于每个 $K \\in \\{0, 1, 3\\}$，它计算位置编码模型误差 $E(f_{\\text{PE},K}^\\star, g)$ 和相应的比率 $R(g, K)$。最后，将所有计算出的值按指定顺序组合成一个列表。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes capacity proxies and expressivity boost ratios for shallow models\n    with and without positional encoding.\n    \"\"\"\n    \n    # --- Problem Parameters ---\n    N = 4096\n    m = 3\n    K_values = [0, 1, 3]\n\n    # --- Step 1: Define Grid and Target Functions ---\n    x_grid = np.linspace(0, 1, N, dtype=np.float64)\n    \n    # g1(x) = sin(2^3 * pi * x) = sin(8 * pi * x)\n    g1 = np.sin(2**m * np.pi * x_grid)\n    \n    # g2(x) = 0.5*sin(2*pi*x) + 0.25*cos(4*pi*x) + 0.2*sin(8*pi*x)\n    g2 = (0.5 * np.sin(2 * np.pi * x_grid) + \n          0.25 * np.cos(4 * np.pi * x_grid) + \n          0.2 * np.sin(8 * np.pi * x_grid))\n          \n    # g3(x) = x^2\n    g3 = x_grid**2\n    \n    targets = {\n        'g1': g1,\n        'g2': g2,\n        'g3': g3\n    }\n    target_names = ['g1', 'g2', 'g3']\n\n    # --- Task B: Compute Capacity Proxies ---\n    S_plain = 1\n    S_PE = {K: 2 * (2**K) for K in K_values}\n\n    def get_mse(X, y):\n        \"\"\"\n        Solves the linear least-squares problem and returns the mean squared error.\n        \n        A robust method for calculating MSE is used by finding the optimal weights `w`\n        and then explicitly computing the error `mean((X@w - y)**2)`. This handles\n        cases of perfect fits where the 'residuals' output of np.linalg.lstsq\n        can be an empty array.\n        \"\"\"\n        w, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        y_pred = X @ w\n        mse = np.mean((y - y_pred)**2)\n        return mse\n\n    # --- Fit Baseline Model (F_plain) ---\n    X_plain = np.vstack([x_grid, np.ones(N)]).T\n    mse_plain_results = {}\n    for name in target_names:\n        mse_plain_results[name] = get_mse(X_plain, targets[name])\n        \n    # --- Fit Positional Encoding Model (F_PE,K) ---\n    mse_pe_results = {name: {} for name in target_names}\n    for K in K_values:\n        # Construct the design matrix X_PE for the given K\n        features = []\n        for k in range(K + 1):\n            freq = 2**k\n            features.append(np.sin(freq * np.pi * x_grid))\n            features.append(np.cos(freq * np.pi * x_grid))\n        features.append(np.ones(N)) # Bias term\n        X_pe = np.vstack(features).T\n        \n        # Calculate MSE for each target\n        for name in target_names:\n            mse_pe_results[name][K] = get_mse(X_pe, targets[name])\n            \n    # --- Task A: Compute Expressivity Boost Ratios (R) ---\n    R_results = {name: {} for name in target_names}\n    for name in target_names:\n        for K in K_values:\n            numerator = mse_plain_results[name]\n            denominator = max(mse_pe_results[name][K], 1e-12)\n            R_results[name][K] = numerator / denominator\n\n    # --- Assemble Final Output in the specified order ---\n    final_output = []\n    # Capacity Proxies\n    final_output.append(S_plain)\n    final_output.append(S_PE[0])\n    final_output.append(S_PE[1])\n    final_output.append(S_PE[3])\n    \n    # Ratios for g1, g2, g3\n    for name in target_names:\n        for K in K_values:\n            final_output.append(R_results[name][K])\n            \n    # Print the formatted output\n    print(f\"[{','.join(f'{v:.6f}' for v in final_output)}]\")\n\nsolve()\n```"
        }
    ]
}