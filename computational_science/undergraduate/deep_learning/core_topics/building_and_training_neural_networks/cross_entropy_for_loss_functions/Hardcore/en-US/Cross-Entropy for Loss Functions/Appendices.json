{
    "hands_on_practices": [
        {
            "introduction": "To truly understand how a neural network learns to classify, we must look under the hood at the engine of optimization: the gradient of the loss function. This first practice is a foundational exercise that demystifies the backpropagation process for the softmax cross-entropy loss. By deriving the gradient from first principles , you will uncover the elegant and intuitive relationship between predicted probabilities, true labels, and the error signal that drives learning.",
            "id": "3110719",
            "problem": "Consider a multiclass classifier that outputs a vector of logits $\\mathbf{z} \\in \\mathbb{R}^{K}$ for $K$ classes. The predicted class probabilities are obtained by applying the softmax function, defined for each class index $i \\in \\{1,2,\\dots,K\\}$ as\n$$\n\\hat{p}_{i}(\\mathbf{z}) = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{K} \\exp(z_{j})}.\n$$\nLet the observed label be represented as a one-hot vector $\\mathbf{y} \\in \\{0,1\\}^{K}$ satisfying $\\sum_{i=1}^{K} y_{i} = 1$. The training objective is the Cross-Entropy (CE) loss,\n$$\n\\text{CE}(\\mathbf{z},\\mathbf{y}) = -\\sum_{i=1}^{K} y_{i} \\ln\\!\\big(\\hat{p}_{i}(\\mathbf{z})\\big).\n$$\nStarting only from these definitions, derive the gradient $\\nabla_{\\mathbf{z}} \\text{CE}(\\mathbf{z},\\mathbf{y})$ with respect to the logits $\\mathbf{z}$. Your derivation must use first principles and standard rules of calculus, clearly identifying where the product rule, chain rule, and properties of the softmax function are applied. Then interpret each term that appears in the gradient in terms of how the loss function responds to discrepancies between the predicted distribution and the one-hot label.\n\nFinally, for a concrete calculation, evaluate your derived gradient at the logits $\\mathbf{z} = [0,\\,0,\\,\\ln 2]$ and the label $\\mathbf{y} = [0,\\,1,\\,0]$. Express the final numerical result exactly; no rounding is required.",
            "solution": "Our objective is to compute the gradient of the Cross-Entropy (CE) loss function with respect to the logits vector $\\mathbf{z}$, denoted as $\\nabla_{\\mathbf{z}} \\text{CE}(\\mathbf{z},\\mathbf{y})$. The gradient is a vector whose $k$-th component is the partial derivative of the loss with respect to the $k$-th logit, $\\frac{\\partial \\text{CE}}{\\partial z_k}$.\n\nThe loss function is given by:\n$$\n\\text{CE}(\\mathbf{z},\\mathbf{y}) = L = -\\sum_{i=1}^{K} y_{i} \\ln\\!\\big(\\hat{p}_{i}(\\mathbf{z})\\big)\n$$\nWe begin by applying the sum rule for differentiation to find the partial derivative with respect to an arbitrary logit $z_k$:\n$$\n\\frac{\\partial L}{\\partial z_k} = \\frac{\\partial}{\\partial z_k} \\left( -\\sum_{i=1}^{K} y_{i} \\ln(\\hat{p}_{i}) \\right) = -\\sum_{i=1}^{K} y_{i} \\frac{\\partial}{\\partial z_k} \\ln(\\hat{p}_{i})\n$$\nUsing the chain rule, the derivative of the natural logarithm is $\\frac{d}{dx}\\ln(u) = \\frac{1}{u}\\frac{du}{dx}$. Applying this, we get:\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\sum_{i=1}^{K} \\frac{y_{i}}{\\hat{p}_{i}} \\frac{\\partial \\hat{p}_{i}}{\\partial z_k}\n$$\nThe next step is to compute the partial derivative of the softmax function output $\\hat{p}_i$ with respect to the logit $z_k$. The softmax function is:\n$$\n\\hat{p}_{i} = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{K} \\exp(z_{j})}\n$$\nWe use the quotient rule for differentiation, $(\\frac{f}{g})' = \\frac{f'g - fg'}{g^2}$. Let $f(z_k) = \\exp(z_i)$ and $g(z_k) = \\sum_{j=1}^{K} \\exp(z_j)$.\nThe derivatives of the numerator and denominator with respect to $z_k$ are:\n$$\n\\frac{\\partial}{\\partial z_k} \\exp(z_i) = \\begin{cases} \\exp(z_i) & \\text{if } i=k \\\\ 0 & \\text{if } i \\neq k \\end{cases} = \\delta_{ik} \\exp(z_i)\n$$\nwhere $\\delta_{ik}$ is the Kronecker delta.\n$$\n\\frac{\\partial}{\\partial z_k} \\sum_{j=1}^{K} \\exp(z_j) = \\exp(z_k)\n$$\nApplying the quotient rule:\n$$\n\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\frac{(\\delta_{ik} \\exp(z_i)) \\left( \\sum_{j=1}^{K} \\exp(z_j) \\right) - \\exp(z_i) \\exp(z_k)}{\\left( \\sum_{j=1}^{K} \\exp(z_j) \\right)^2}\n$$\nWe can simplify this by splitting it into two parts:\n$$\n\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\frac{\\delta_{ik} \\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)} - \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)} \\frac{\\exp(z_k)}{\\sum_{j=1}^{K} \\exp(z_j)}\n$$\nRecognizing the definition of the softmax function, $\\hat{p}_i = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)}$ and $\\hat{p}_k = \\frac{\\exp(z_k)}{\\sum_j \\exp(z_j)}$, we can rewrite the expression as:\n$$\n\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\delta_{ik} \\hat{p}_i - \\hat{p}_i \\hat{p}_k = \\hat{p}_i (\\delta_{ik} - \\hat{p}_k)\n$$\nThis general form for the derivative of the softmax function separates into two cases:\n1.  If $i = k$: $\\frac{\\partial \\hat{p}_{i}}{\\partial z_i} = \\hat{p}_i(1 - \\hat{p}_i)$\n2.  If $i \\neq k$: $\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\hat{p}_i(0 - \\hat{p}_k) = -\\hat{p}_i \\hat{p}_k$\n\nNow, we substitute this result back into our expression for the loss gradient:\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\sum_{i=1}^{K} \\frac{y_{i}}{\\hat{p}_{i}} \\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = -\\sum_{i=1}^{K} \\frac{y_{i}}{\\hat{p}_{i}} \\left( \\hat{p}_i (\\delta_{ik} - \\hat{p}_k) \\right)\n$$\nThe $\\hat{p}_i$ terms cancel out, simplifying the expression significantly:\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\sum_{i=1}^{K} y_{i} (\\delta_{ik} - \\hat{p}_k)\n$$\nWe can split the summation into two parts:\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\left( \\sum_{i=1}^{K} y_{i}\\delta_{ik} - \\sum_{i=1}^{K} y_{i}\\hat{p}_k \\right)\n$$\nLet's evaluate each sum:\n-  The first sum, $\\sum_{i=1}^{K} y_{i}\\delta_{ik}$, is non-zero only when $i=k$, where it becomes $y_k$. Thus, $\\sum_{i=1}^{K} y_{i}\\delta_{ik} = y_k$.\n-  In the second sum, $\\hat{p}_k$ is constant with respect to the summation index $i$. So, $\\sum_{i=1}^{K} y_{i}\\hat{p}_k = \\hat{p}_k \\sum_{i=1}^{K} y_i$. Since $\\mathbf{y}$ is a one-hot vector, we know that $\\sum_{i=1}^{K} y_i = 1$. Therefore, the sum is equal to $\\hat{p}_k$.\n\nSubstituting these results back:\n$$\n\\frac{\\partial L}{\\partial z_k} = -(y_k - \\hat{p}_k) = \\hat{p}_k - y_k\n$$\nThis remarkably simple result shows that the partial derivative of the cross-entropy loss with respect to the $k$-th logit is the difference between the predicted probability for class $k$ and the true label for class $k$. The complete gradient vector is therefore:\n$$\n\\nabla_{\\mathbf{z}} \\text{CE}(\\mathbf{z},\\mathbf{y}) = \\hat{\\mathbf{p}}(\\mathbf{z}) - \\mathbf{y}\n$$\n\nThe interpretation of the term $\\hat{p}_k - y_k$ is as follows. During training via gradient descent, the logit $z_k$ is updated according to $z_k \\leftarrow z_k - \\alpha (\\hat{p}_k - y_k)$, where $\\alpha > 0$ is the learning rate.\n- If class $k$ is the true class, then $y_k=1$. The gradient component is $\\hat{p}_k - 1$, which is negative or zero (since $\\hat{p}_k \\leq 1$). The update rule becomes $z_k \\leftarrow z_k - \\alpha(\\text{negative value})$, which increases $z_k$. This pushes the logit for the correct class higher, thereby increasing its probability $\\hat{p}_k$ toward the target of $1$.\n- If class $k$ is not the true class, then $y_k=0$. The gradient component is $\\hat{p}_k - 0 = \\hat{p}_k$, which is positive or zero (since $\\hat{p}_k \\geq 0$). The update rule becomes $z_k \\leftarrow z_k - \\alpha(\\text{positive value})$, which decreases $z_k$. This pushes the logits for incorrect classes lower, thereby decreasing their probabilities toward the target of $0$.\nEssentially, the gradient vector $\\hat{\\mathbf{p}} - \\mathbf{y}$ represents the error or residual between the predicted probability distribution and the true one-hot distribution.\n\nFinally, we evaluate this gradient for the given concrete case:\n- Logits: $\\mathbf{z} = [0, 0, \\ln 2]$. This implies $K=3$.\n- Label: $\\mathbf{y} = [0, 1, 0]$.\n\nFirst, we compute the predicted probabilities $\\hat{\\mathbf{p}}$ using the softmax function:\nThe denominator is $\\sum_{j=1}^{3} \\exp(z_j) = \\exp(0) + \\exp(0) + \\exp(\\ln 2) = 1 + 1 + 2 = 4$.\nThe probabilities are:\n$$\n\\hat{p}_1 = \\frac{\\exp(z_1)}{4} = \\frac{\\exp(0)}{4} = \\frac{1}{4}\n$$\n$$\n\\hat{p}_2 = \\frac{\\exp(z_2)}{4} = \\frac{\\exp(0)}{4} = \\frac{1}{4}\n$$\n$$\n\\hat{p}_3 = \\frac{\\exp(z_3)}{4} = \\frac{\\exp(\\ln 2)}{4} = \\frac{2}{4} = \\frac{1}{2}\n$$\nSo, the predicted probability vector is $\\hat{\\mathbf{p}} = [\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}]$.\n\nNow, we compute the gradient $\\nabla_{\\mathbf{z}} \\text{CE} = \\hat{\\mathbf{p}} - \\mathbf{y}$:\n$$\n\\nabla_{\\mathbf{z}} \\text{CE} = \\begin{pmatrix} \\frac{1}{4}, & \\frac{1}{4}, & \\frac{1}{2} \\end{pmatrix} - \\begin{pmatrix} 0, & 1, & 0 \\end{pmatrix}\n$$\n$$\n\\nabla_{\\mathbf{z}} \\text{CE} = \\begin{pmatrix} \\frac{1}{4} - 0, & \\frac{1}{4} - 1, & \\frac{1}{2} - 0 \\end{pmatrix}\n$$\n$$\n\\nabla_{\\mathbf{z}} \\text{CE} = \\begin{pmatrix} \\frac{1}{4}, & -\\frac{3}{4}, & \\frac{1}{2} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{4} & -\\frac{3}{4} & \\frac{1}{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In practice, monitoring training progress is key, but which metric tells the full story? While accuracy is intuitive, it can be a coarse measure that plateaus long before a model has stopped improving. This exercise  presents a concrete scenario where accuracy remains static, yet the cross-entropy loss continues to fall, demonstrating that the model is still becoming more confident in its correct predictions. This illustrates why cross-entropy is often a more sensitive and informative metric for making practical decisions like when to stop training.",
            "id": "3110736",
            "problem": "A multi-class classifier produces, for each input $x$, a probability vector $\\hat{\\mathbf{p}}(x)$ over classes $\\{A,B,C\\}$. Consider a fixed validation set of $n=4$ samples with true labels and the model’s predicted probabilities at epoch $t=1$ and epoch $t=2$ given below. For each sample $i$, the ground-truth class is denoted by $y_i \\in \\{A,B,C\\}$, and the model outputs a probability vector $(\\hat{p}_A,\\hat{p}_B,\\hat{p}_C)$.\n\n- Sample $1$: $y_1=A$\n  - Epoch $1$: $(0.55,\\,0.40,\\,0.05)$\n  - Epoch $2$: $(0.70,\\,0.25,\\,0.05)$\n- Sample $2$: $y_2=B$\n  - Epoch $1$: $(0.60,\\,0.35,\\,0.05)$\n  - Epoch $2$: $(0.55,\\,0.40,\\,0.05)$\n- Sample $3$: $y_3=C$\n  - Epoch $1$: $(0.24,\\,0.24,\\,0.52)$\n  - Epoch $2$: $(0.20,\\,0.20,\\,0.60)$\n- Sample $4$: $y_4=B$\n  - Epoch $1$: $(0.49,\\,0.51,\\,0.00)$\n  - Epoch $2$: $(0.49,\\,0.51,\\,0.00)$\n\nUse only the following fundamental definitions.\n\n- The validation accuracy at epoch $t$ is the fraction $\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{\\arg\\max_{c\\in\\{A,B,C\\}}\\hat{p}_c^{(t)}(x_i)=y_i\\}$.\n- The average negative log-likelihood (NLL), equivalently the average cross-entropy (CE) for one-hot labels, at epoch $t$ is $\\frac{1}{n}\\sum_{i=1}^{n}\\big(-\\log \\hat{p}^{(t)}_{y_i}(x_i)\\big)$, where $\\log$ is the natural logarithm.\n- Define the exponential moving average (EMA) of the NLL by $\\text{EMA}_t=\\alpha L_t+(1-\\alpha)\\text{EMA}_{t-1}$, where $L_t$ is the average NLL at epoch $t$, $\\alpha\\in(0,1]$, and initialize $\\text{EMA}_1=L_1$. Take $\\alpha=0.5$.\n\nBased on these definitions and the data above, which of the following statements are true?\n\nA. From epoch $1$ to epoch $2$, the validation accuracy is unchanged, while the average negative log-likelihood strictly decreases. Therefore, an early-stopping rule that halts when accuracy plateaus for $1$ epoch would be premature compared to a rule that monitors a moving-average negative log-likelihood.\n\nB. From epoch $1$ to epoch $2$, both the validation accuracy and the average negative log-likelihood are unchanged; hence either metric yields the same stopping decision at epoch $2$.\n\nC. From epoch $1$ to epoch $2$, the validation accuracy increases, and the average negative log-likelihood increases.\n\nD. The $2$-epoch exponential moving average with smoothing parameter $\\alpha=0.5$ of the negative log-likelihood decreases from epoch $1$ to epoch $2$.",
            "solution": "The solution requires calculating and comparing the validation accuracy and average negative log-likelihood (NLL) for epochs $t=1$ and $t=2$.\n\n**1. Calculation of Validation Accuracy**\n\nThe validation accuracy is the fraction of correctly classified samples. The predicted class is the one with the highest probability.\n\n**Epoch $t=1$ Accuracy ($Acc_1$)**:\n-   Sample $1$ ($y_1=A$): $\\hat{\\mathbf{p}}^{(1)}=(0.55, 0.40, 0.05)$. $\\arg\\max$ is $A$. Correct.\n-   Sample $2$ ($y_2=B$): $\\hat{\\mathbf{p}}^{(1)}=(0.60, 0.35, 0.05)$. $\\arg\\max$ is $A$. Incorrect.\n-   Sample $3$ ($y_3=C$): $\\hat{\\mathbf{p}}^{(1)}=(0.24, 0.24, 0.52)$. $\\arg\\max$ is $C$. Correct.\n-   Sample $4$ ($y_4=B$): $\\hat{\\mathbf{p}}^{(1)}=(0.49, 0.51, 0.00)$. $\\arg\\max$ is $B$. Correct.\n\nThere are $3$ correct predictions out of $4$ samples.\n$$Acc_1 = \\frac{3}{4} = 0.75$$\n\n**Epoch $t=2$ Accuracy ($Acc_2$)**:\n-   Sample $1$ ($y_1=A$): $\\hat{\\mathbf{p}}^{(2)}=(0.70, 0.25, 0.05)$. $\\arg\\max$ is $A$. Correct.\n-   Sample $2$ ($y_2=B$): $\\hat{\\mathbf{p}}^{(2)}=(0.55, 0.40, 0.05)$. $\\arg\\max$ is $A$. Incorrect.\n-   Sample $3$ ($y_3=C$): $\\hat{\\mathbf{p}}^{(2)}=(0.20, 0.20, 0.60)$. $\\arg\\max$ is $C$. Correct.\n-   Sample $4$ ($y_4=B$): $\\hat{\\mathbf{p}}^{(2)}=(0.49, 0.51, 0.00)$. $\\arg\\max$ is $B$. Correct.\n\nThere are $3$ correct predictions out of $4$ samples.\n$$Acc_2 = \\frac{3}{4} = 0.75$$\n\nTherefore, the validation accuracy is unchanged from epoch $1$ to epoch $2$: $Acc_1 = Acc_2$.\n\n**2. Calculation of Average Negative Log-Likelihood (NLL)**\n\nThe average NLL, $L_t$, is calculated using the predicted probability of the true class for each sample.\n\n**Epoch $t=1$ Average NLL ($L_1$)**:\nThe probabilities for the true classes are $\\hat{p}^{(1)}_{y_1}=0.55$, $\\hat{p}^{(1)}_{y_2}=0.35$, $\\hat{p}^{(1)}_{y_3}=0.52$, and $\\hat{p}^{(1)}_{y_4}=0.51$.\n$$L_1 = \\frac{1}{4} \\left[ -\\log(0.55) - \\log(0.35) - \\log(0.52) - \\log(0.51) \\right]$$\nNumerically, $L_1 \\approx \\frac{1}{4} [ -(-0.5978) - (-1.0498) - (-0.6539) - (-0.6733) ] = \\frac{1}{4} [2.9748] \\approx 0.7437$.\n\n**Epoch $t=2$ Average NLL ($L_2$)**:\nThe probabilities for the true classes are $\\hat{p}^{(2)}_{y_1}=0.70$, $\\hat{p}^{(2)}_{y_2}=0.40$, $\\hat{p}^{(2)}_{y_3}=0.60$, and $\\hat{p}^{(2)}_{y_4}=0.51$.\n$$L_2 = \\frac{1}{4} \\left[ -\\log(0.70) - \\log(0.40) - \\log(0.60) - \\log(0.51) \\right]$$\nNumerically, $L_2 \\approx \\frac{1}{4} [ -(-0.3567) - (-0.9163) - (-0.5108) - (-0.6733) ] = \\frac{1}{4} [2.4571] \\approx 0.6143$.\n\n**Comparing $L_1$ and $L_2$**:\nTo determine if the NLL has decreased, we can compare $L_1$ and $L_2$. A decrease means $L_2 < L_1$. We can establish this without resorting to numerical approximation.\nThe difference $L_1 - L_2$ is:\n$$L_1 - L_2 = \\frac{1}{4} \\left( [-\\log(0.55)] - [-\\log(0.70)] + [-\\log(0.35)] - [-\\log(0.40)] + [-\\log(0.52)] - [-\\log(0.60)] + [-\\log(0.51)] - [-\\log(0.51)] \\right)$$\n$$L_1 - L_2 = \\frac{1}{4} \\left( [\\log(0.70) - \\log(0.55)] + [\\log(0.40) - \\log(0.35)] + [\\log(0.60) - \\log(0.52)] \\right)$$\nSince the natural logarithm function $\\log(x)$ is strictly increasing:\n-   $0.70 > 0.55 \\implies \\log(0.70) > \\log(0.55) \\implies \\log(0.70) - \\log(0.55) > 0$.\n-   $0.40 > 0.35 \\implies \\log(0.40) > \\log(0.35) \\implies \\log(0.40) - \\log(0.35) > 0$.\n-   $0.60 > 0.52 \\implies \\log(0.60) > \\log(0.52) \\implies \\log(0.60) - \\log(0.52) > 0$.\n\nThe sum of three strictly positive terms is strictly positive. Therefore, $L_1 - L_2 > 0$, which implies $L_1 > L_2$. The average negative log-likelihood has strictly decreased.\n\n**3. Calculation of Exponential Moving Average (EMA) of NLL**\n\nThe EMA is defined by $\\text{EMA}_t=\\alpha L_t+(1-\\alpha)\\text{EMA}_{t-1}$ with $\\alpha=0.5$ and the initialization $\\text{EMA}_1=L_1$.\n\n-   At epoch $t=1$: $\\text{EMA}_1 = L_1$.\n-   At epoch $t=2$: $\\text{EMA}_2 = \\alpha L_2 + (1-\\alpha) \\text{EMA}_1 = 0.5 L_2 + 0.5 L_1$.\n\nWe need to compare $\\text{EMA}_1$ and $\\text{EMA}_2$. The change is $\\text{EMA}_2 - \\text{EMA}_1$.\n$$\\text{EMA}_2 - \\text{EMA}_1 = (0.5 L_2 + 0.5 L_1) - L_1 = 0.5 L_2 - 0.5 L_1 = 0.5 (L_2 - L_1)$$\nSince we established that $L_2 < L_1$, the term $(L_2 - L_1)$ is negative.\nTherefore, $\\text{EMA}_2 - \\text{EMA}_1 < 0$, which means $\\text{EMA}_2 < \\text{EMA}_1$. The EMA of the NLL decreases from epoch $1$ to epoch $2$.\n\n### Option-by-Option Analysis\n\n**A. From epoch $1$ to epoch $2$, the validation accuracy is unchanged, while the average negative log-likelihood strictly decreases. Therefore, an early-stopping rule that halts when accuracy plateaus for $1$ epoch would be premature compared to a rule that monitors a moving-average negative log-likelihood.**\n\n-   \"validation accuracy is unchanged\": This is true. We found $Acc_1 = Acc_2 = 0.75$.\n-   \"average negative log-likelihood strictly decreases\": This is true. We proved $L_2 < L_1$.\n-   \"Therefore, an early-stopping rule that halts when accuracy plateaus... would be premature...\": Accuracy has plateaued ($Acc_2-Acc_1=0$), which could trigger a stop. However, the NLL is improving ($L_2<L_1$), as is its EMA ($\\text{EMA}_2 < \\text{EMA}_1$). A rule monitoring the NLL or its EMA would register this improvement and not stop. The conclusion that stopping based on accuracy would be premature from the viewpoint of the loss metric is a correct interpretation of the results. This statement accurately describes a key reason why loss-based metrics can be preferred over accuracy for early stopping.\n\nVerdict for A: **Correct**.\n\n**B. From epoch $1$ to epoch $2$, both the validation accuracy and the average negative log-likelihood are unchanged; hence either metric yields the same stopping decision at epoch $2$.**\n\n-   \"average negative log-likelihood are unchanged\": This is false. The NLL strictly decreased. The entire statement is therefore false.\n\nVerdict for B: **Incorrect**.\n\n**C. From epoch $1$ to epoch $2$, the validation accuracy increases, and the average negative log-likelihood increases.**\n\n-   \"validation accuracy increases\": This is false. It was unchanged.\n-   \"average negative log-likelihood increases\": This is false. It strictly decreased.\n\nVerdict for C: **Incorrect**.\n\n**D. The $2$-epoch exponential moving average with smoothing parameter $\\alpha=0.5$ of the negative log-likelihood decreases from epoch $1$ to epoch $2$.**\n\n-   This statement claims that $\\text{EMA}_2 < \\text{EMA}_1$.\n-   As derived in section 3, $\\text{EMA}_2 - \\text{EMA}_1 = 0.5(L_2 - L_1)$.\n-   Since $L_2 < L_1$, the difference is negative. Thus, the EMA of the NLL does indeed decrease. The statement is a direct consequence of the calculations.\n\nVerdict for D: **Correct**.\n\nBoth statements A and D are true based on the provided data and definitions.",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "Minimizing a loss function is the means, but what is the end? This practice explores the theoretical ideal that a model approaches as it minimizes expected cross-entropy. By analyzing a simplified model with temperature scaling , you will discover that the optimal solution is achieved when the model's output probabilities perfectly match the true data-generating probabilities. This reveals the deep connection between cross-entropy minimization and the crucial concept of model calibration, where a model's confidence aligns with its accuracy.",
            "id": "3110717",
            "problem": "Consider a binary classification model in deep learning in which the two class logits are parameterized by a single real parameter $a \\in \\mathbb{R}$ as $\\mathbf{z}(a) = (a, 0)$. The model outputs probabilities via the softmax function. Temperature scaling by a scalar $T > 0$ is applied to the logits by dividing them by $T$, yielding scaled logits $\\mathbf{z}(a)/T = (a/T, 0)$ and model probability for class $1$ given by the softmax of the scaled logits. The data-generating process has labels $y \\in \\{0,1\\}$ with true class probability $\\mathbb{P}(y=1) = q$ and $\\mathbb{P}(y=0) = 1 - q$, where $q \\in (0,1)$ is unknown but fixed. The training objective is the expected cross-entropy loss between the true Bernoulli distribution of $y$ and the model’s predicted distribution under temperature scaling $T$. The cross-entropy is defined as the expectation, over the true data distribution, of the negative logarithm of the predicted probability of the observed label. Using only these definitions and starting from first principles, derive the closed-form expression for the parameter value $a^{\\ast}(T)$ that minimizes the temperature-scaled expected cross-entropy loss as a function of $T$ and $q$. Additionally, explain at the level of definitions how this temperature scaling $T$ changes the optimizer in parameter space and what happens to standard calibration metrics such as Negative Log-Likelihood (NLL) and Expected Calibration Error (ECE) at this optimizer. Express your final answer as a single closed-form analytic expression for $a^{\\ast}(T)$ in terms of $T$ and $q$.",
            "solution": "The objective is to find the parameter value $a^{\\ast}(T)$ that minimizes the temperature-scaled expected cross-entropy loss. We begin by defining the model's probabilistic output. The logits for the two classes are given by the vector $\\mathbf{z}(a) = (a, 0)$, where $a \\in \\mathbb{R}$. Temperature scaling with a scalar $T > 0$ is applied, resulting in scaled logits $\\mathbf{z}(a)/T = (a/T, 0)$. The model's predicted probabilities for class $1$ (denoted $p_1$) and class $0$ (denoted $p_0$) are obtained by applying the softmax function to these scaled logits:\n$$p_1(a, T) = \\frac{\\exp(a/T)}{\\exp(a/T) + \\exp(0)} = \\frac{\\exp(a/T)}{\\exp(a/T) + 1}$$\n$$p_0(a, T) = \\frac{\\exp(0)}{\\exp(a/T) + \\exp(0)} = \\frac{1}{\\exp(a/T) + 1}$$\nNote that $p_1(a, T) + p_0(a, T) = 1$. The function for $p_1(a,T)$ is equivalent to the sigmoid function applied to the scaled logit difference, $p_1(a,T) = \\sigma(a/T)$.\n\nThe true data-generating process is a Bernoulli distribution where the label $y \\in \\{0, 1\\}$ has probabilities $\\mathbb{P}(y=1) = q$ and $\\mathbb{P}(y=0) = 1-q$ for some fixed $q \\in (0, 1)$. The training objective is the expected cross-entropy loss, $L(a, T)$, between this true distribution and the model's predicted distribution. By definition, this is the expectation over the true data distribution of the negative logarithm of the predicted probability of the observed label:\n$$L(a, T) = \\mathbb{E}_{y \\sim \\mathbb{P}(y)}[-\\ln(p_y(a,T))] = - \\sum_{y \\in \\{0,1\\}} \\mathbb{P}(y) \\ln(p_y(a,T))$$\n$$L(a, T) = - \\mathbb{P}(y=1) \\ln(p_1(a,T)) - \\mathbb{P}(y=0) \\ln(p_0(a,T))$$\n$$L(a, T) = -q \\ln(p_1(a,T)) - (1-q) \\ln(p_0(a,T))$$\nSubstituting the expressions for $p_1(a,T)$ and $p_0(a,T)$:\n$$L(a, T) = -q \\ln\\left(\\frac{\\exp(a/T)}{\\exp(a/T) + 1}\\right) - (1-q) \\ln\\left(\\frac{1}{\\exp(a/T) + 1}\\right)$$\nUsing the properties of logarithms, $\\ln(x/y) = \\ln(x) - \\ln(y)$ and $\\ln(1/y) = -\\ln(y)$, we simplify the loss function:\n$$L(a, T) = -q \\left[ \\ln(\\exp(a/T)) - \\ln(\\exp(a/T) + 1) \\right] - (1-q) \\left[ -\\ln(\\exp(a/T) + 1) \\right]$$\n$$L(a, T) = -q \\left[ \\frac{a}{T} - \\ln(\\exp(a/T) + 1) \\right] + (1-q) \\ln(\\exp(a/T) + 1)$$\n$$L(a, T) = -q \\frac{a}{T} + q \\ln(\\exp(a/T) + 1) + (1-q) \\ln(\\exp(a/T) + 1)$$\n$$L(a, T) = -q \\frac{a}{T} + \\ln(\\exp(a/T) + 1)$$\nTo find the parameter value $a^{\\ast}(T)$ that minimizes this loss, we compute the derivative of $L(a, T)$ with respect to $a$ and set it to zero:\n$$\\frac{\\partial L}{\\partial a} = \\frac{\\partial}{\\partial a} \\left(-q \\frac{a}{T} + \\ln\\left(\\exp\\left(\\frac{a}{T}\\right) + 1\\right)\\right)$$\n$$\\frac{\\partial L}{\\partial a} = -\\frac{q}{T} + \\frac{1}{\\exp(a/T) + 1} \\cdot \\frac{\\partial}{\\partial a}\\left(\\exp\\left(\\frac{a}{T}\\right)\\right)$$\n$$\\frac{\\partial L}{\\partial a} = -\\frac{q}{T} + \\frac{1}{\\exp(a/T) + 1} \\cdot \\exp\\left(\\frac{a}{T}\\right) \\cdot \\frac{1}{T} = \\frac{1}{T} \\left( \\frac{\\exp(a/T)}{\\exp(a/T) + 1} - q \\right)$$\nRecognizing the term for $p_1(a, T)$, we have:\n$$\\frac{\\partial L}{\\partial a} = \\frac{1}{T} \\left( p_1(a, T) - q \\right)$$\nSetting this derivative to $0$ to find the optimal parameter $a^{\\ast}(T)$:\n$$\\frac{1}{T} (p_1(a^{\\ast}, T) - q) = 0$$\nSince $T > 0$, this implies that $p_1(a^{\\ast}, T) = q$. The minimum loss is achieved when the model's predicted probability for class $1$ equals its true probability. We solve this equation for $a^{\\ast}(T)$:\n$$\\frac{\\exp(a^{\\ast}/T)}{\\exp(a^{\\ast}/T) + 1} = q$$\nLet $x = \\exp(a^{\\ast}/T)$. The equation is $\\frac{x}{x+1} = q$, which gives $x = q(x+1)$, so $x - qx = q$, and $x(1-q) = q$. This yields $x = \\frac{q}{1-q}$. Substituting back for $x$:\n$$\\exp\\left(\\frac{a^{\\ast}}{T}\\right) = \\frac{q}{1-q}$$\nTaking the natural logarithm of both sides gives the scaled optimal parameter:\n$$\\frac{a^{\\ast}}{T} = \\ln\\left(\\frac{q}{1-q}\\right)$$\nFinally, the closed-form expression for the optimal parameter $a^{\\ast}(T)$ is:\n$$a^{\\ast}(T) = T \\ln\\left(\\frac{q}{1-q}\\right)$$\nTo confirm this is a minimum, we examine the second derivative of the loss function.\n$$\\frac{\\partial^2 L}{\\partial a^2} = \\frac{\\partial}{\\partial a} \\left[ \\frac{1}{T} (p_1(a, T) - q) \\right] = \\frac{1}{T} \\frac{\\partial p_1}{\\partial a}$$\nThe derivative of $p_1(a, T) = \\sigma(a/T)$ with respect to $a$ is $\\frac{\\partial p_1}{\\partial a} = \\sigma'(a/T) \\cdot \\frac{1}{T} = p_1(a, T)(1-p_1(a, T)) \\cdot \\frac{1}{T}$.\nThus, the second derivative of the loss is $\\frac{\\partial^2 L}{\\partial a^2} = \\frac{1}{T^2} p_1(a, T) p_0(a, T)$. Since $T > 0$, and probabilities $p_1, p_0$ are in $(0, 1)$, $\\frac{\\partial^2 L}{\\partial a^2} > 0$. This confirms that $L(a, T)$ is strictly convex in $a$, and $a^{\\ast}(T)$ is a unique global minimum.\n\nThe term $\\ln(q/(1-q))$ is the log-odds (or logit) of the true probability $q$. The optimal parameter $a^{\\ast}(T)$ is linearly scaled by the temperature $T$. This means that in the parameter space of $a$, the location of the loss minimum changes with $T$. To achieve the optimal output probability $p_1=q$, the scaled logit difference $a/T$ must equal the fixed value $\\ln(q/(1-q))$. Thus, $a^{\\ast}$ must scale with $T$.\n\nAt this optimizer $a^{\\ast}(T)$, the model's predicted probability for class $1$ is $p_1(a^{\\ast}(T), T) = q$. This means the model is perfectly calibrated: its confidence ($q$) exactly matches the true frequency of the positive class ($q$). Consequently, the Expected Calibration Error (ECE) is $0$. The Negative Log-Likelihood (NLL) at this minimum is the value of the loss function itself, $L(a^{\\ast}(T), T) = -q \\ln(q) - (1-q) \\ln(1-q)$. This is the Shannon entropy of the true data-generating distribution, $H(q)$, which is the lowest possible cross-entropy loss. Both the minimal NLL and the resulting ECE of $0$ are independent of the temperature $T$.",
            "answer": "$$\n\\boxed{T \\ln\\left(\\frac{q}{1-q}\\right)}\n$$"
        }
    ]
}