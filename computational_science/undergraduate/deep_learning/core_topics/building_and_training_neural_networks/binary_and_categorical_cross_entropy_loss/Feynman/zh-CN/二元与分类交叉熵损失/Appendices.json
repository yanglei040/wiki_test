{
    "hands_on_practices": [
        {
            "introduction": "在着手应用任何损失函数之前，首要任务是确保我们能够正确且稳健地实现它。本练习将引导你从零开始实现分类交叉熵损失，并特别关注一个在深度学习实践中至关重要却常被忽视的方面：数值稳定性 。通过对比一个朴素实现和一个使用“log-sum-exp”技巧的稳定实现，你将亲身体会到，在处理模型logits可能出现的极大或极小值时，为何稳健的计算方法是不可或缺的。这项实践将帮助你跨越从数学公式到可靠代码实现的鸿沟。",
            "id": "3103417",
            "problem": "给定一个分类场景，其中有有限数量的类别，由一个分类分布建模。对于一个输入，其未归一化的对数概率（称为 logits）为 $\\mathbf{z} \\in \\mathbb{R}^K$，以及一个 one-hot 目标向量 $\\mathbf{y} \\in \\{0,1\\}^K$，其中正确类别的索引为 $c$ 使得 $y_c = 1$，则该分类模型下正确类别的负对数似然定义了分类交叉熵（Categorical Cross-Entropy, CCE）。概率向量是通过对 logits 的指数进行归一化得到的，这是 softmax 变换的标准定义。从分类分布、负对数似然的核心定义、指数和对数的性质，以及在所有 logits 上加上一个常数时概率比率的不变性出发，推导出一个数值稳定的 CCE 表达式，该表达式不需要通过原始尺度的指数运算来构造概率。你的推导应明确使用一个恒等式，该恒等式通过使用一个依赖于数据的常数进行移位，以避免上溢或下溢的方式来表示指数和的对数。利用双精度浮点数在 $x \\gtrsim 709$ 时对 $\\exp(x)$ 会发生上溢，而在 $x \\lesssim -745$ 时会下溢为零这一经过充分检验的事实，解释为什么当 logits 具有大的正值时该恒等式能避免算术上溢，以及当 logits 具有大的负值时能避免下溢。\n\n此外，在 $K=2$ 个类别的特殊情况下，从第一性原理出发，证明 CCE 可以简化为二元标签 $y \\in \\{0,1\\}$ 与伯努利模型之间的二元交叉熵（Binary Cross-Entropy, BCE），其中伯努利模型的 logit 参数 $a \\in \\mathbb{R}$ 等于由两个 logits 导出的对数几率。使用 SoftPlus 函数将 BCE 表示为数值稳定的形式，该函数仅由基本函数定义。\n\n你的程序必须：\n- 实现一个朴素的 CCE，它通过对 logits 进行指数运算并归一化来直接计算 softmax 概率，然后计算单个样本的损失。它应该通过检查任何中间或最终值是否变为非有限值来检测失败（上溢或下溢）。\n- 实现一个数值稳定的 CCE，使用适当的移位和指数和对数的稳定计算，而无需在原始尺度上构造概率。\n- 为 $K=2$ 的情况实现一个使用 logits 的数值稳定的 BCE，使用 SoftPlus 函数以防止上溢和下溢。\n- 将这些实现应用于下面的测试套件，并返回所要求的输出。\n\n测试套件（每个案例都是一个带有 logits $\\mathbf{z}$ 和正确类别索引 $c$ 的单一输入）：\n- 案例 1：$K=3$，$\\mathbf{z} = [1.0, 0.0, -1.0]$，$c=0$。\n- 案例 2：$K=3$，$\\mathbf{z} = [120.0, 0.0, -120.0]$，$c=1$。\n- 案例 3：$K=3$，$\\mathbf{z} = [1000.0, 999.0, 998.0]$，$c=0$。\n- 案例 4：$K=3$，$\\mathbf{z} = [-1000.0, -1001.0, -1002.0]$，$c=2$。\n- 案例 5（$K=2$ 时的等价性检查）：$\\mathbf{z} = [5.0, -5.0]$，$c=0$。设 $a$ 为 $\\mathbf{z}$ 对 $c=0$ 类别所隐含的对数几率，对应的二元标签为 $y=1$。计算此示例的稳定 CCE 和稳定 BCE 之间的绝对差值。\n\n对于案例 1 到 4，你必须按顺序为每个案例输出两个值：\n- 一个布尔值，指示基于朴素 softmax 的 CCE 计算是否失败（非有限损失），其中失败定义为产生一个非有限值。\n- 该案例的数值稳定的 CCE 损失，四舍五入到 6 位小数。\n\n对于案例 5，你必须输出两个值：\n- 一个布尔值，指示基于朴素 softmax 的 CCE 计算是否在此案例中失败。\n- 此示例中稳定 CCE 和稳定 BCE 之间的绝对差值，四舍五入到 12 位小数。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含案例 1 到 5 的所有结果，按顺序串联成一个用方括号括起来的逗号分隔列表。顺序是：\n  $[\\text{fail}_1, \\text{stable\\_loss}_1, \\text{fail}_2, \\text{stable\\_loss}_2, \\text{fail}_3, \\text{stable\\_loss}_3, \\text{fail}_4, \\text{stable\\_loss}_4, \\text{fail}_5, \\text{abs\\_diff}_{5}]$。\n- 所有浮点数必须按上述规定四舍五入后打印。不应打印其他任何文本。",
            "solution": "该问题表述清晰，具有科学依据，并包含了严谨求解所需的所有信息。我们着手进行推导和实现。\n\n### 第 1 部分：数值稳定的分类交叉熵（CCE）的推导\n\n分类交叉熵（CCE）损失是模型下真实类别的负对数似然。对于单个数据点，给定一个未归一化的对数概率（logits）向量 $\\mathbf{z} \\in \\mathbb{R}^K$ 和正确的类别索引 $c$，我们首先使用 softmax 函数计算概率向量 $\\mathbf{p}$：\n$$\np_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^K \\exp(z_j)} \\quad \\text{for } i=1, \\dots, K\n$$\n观测到正确类别 $c$ 的似然是 $p_c$。CCE 损失是该似然的负对数：\n$$\nL_{\\text{CCE}} = -\\log(p_c) = -\\log\\left(\\frac{\\exp(z_c)}{\\sum_{j=1}^K \\exp(z_j)}\\right)\n$$\n使用对数的性质 $\\log(a/b) = \\log(a) - \\log(b)$，以及 $\\log(\\exp(x)) = x$，我们可以将损失重写为：\n$$\nL_{\\text{CCE}} = -(\\log(\\exp(z_c)) - \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right)) = -z_c + \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right)\n$$\n这个表达式虽然在数学上是正确的，但在数值上是不稳定的。项 $\\sum_j \\exp(z_j)$ 容易出现浮点错误。\n- **上溢**：如果任何 logit $z_j$ 是大的正数（例如，对于双精度数，$z_j \\gtrsim 709$），$\\exp(z_j)$ 将计算为无穷大，导致整个表达式变为非有限值。\n- **下溢**：如果所有 logits $z_j$ 都是大的负数（例如，$z_j \\lesssim -745$），每个 $\\exp(z_j)$ 都会下溢为 $0$。其和将为 $0$，而其对数将为 $-\\infty$，同样导致非有限的损失。\n\n为了稳定这个计算，我们使用“log-sum-exp”技巧。这依赖于这样一个恒等式：对于任何常数 $m$，概率比率是不变的：\n$$\np_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^K \\exp(z_j)} = \\frac{\\exp(z_i) \\cdot \\exp(-m)}{\\sum_{j=1}^K \\exp(z_j) \\cdot \\exp(-m)} = \\frac{\\exp(z_i - m)}{\\sum_{j=1}^K \\exp(z_j - m)}\n$$\n这种不变性允许我们在不改变最终概率的情况下平移 logits。我们将这个思想应用于项 $\\log(\\sum_j \\exp(z_j))$：\n$$\n\\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right) = \\log\\left(\\exp(m) \\sum_{j=1}^K \\exp(z_j - m)\\right) = \\log(\\exp(m)) + \\log\\left(\\sum_{j=1}^K \\exp(z_j - m)\\right) = m + \\log\\left(\\sum_{j=1}^K \\exp(z_j - m)\\right)\n$$\n通过选择 $m = \\max_j(z_j)$，我们确保了数值稳定性：\n1.  **防止上溢**：指数函数的参数 $z_j - m$ 现在总是小于或等于 $0$。它所能取到的最大值是在 $z_k$ 为最大值的索引 $k$ 处，$z_k - m = m - m = 0$。因此，和中的最大项是 $\\exp(0) = 1$。这可以防止任何项上溢。\n2.  **防止下溢**：由于和 $\\sum_j \\exp(z_j - m)$ 中至少有一项是 $\\exp(0)=1$，因此该和保证至少为 $1$。这可以防止对数的参数下溢为零，从而避免得到 $-\\infty$ 的结果。\n\n将这个稳定的 log-sum-exp 表达式代入损失公式，我们得到数值稳定的 CCE：\n$$\nL_{\\text{CCE, stable}} = -z_c + \\left( \\max_j(z_j) + \\log\\left(\\sum_{j=1}^K \\exp(z_j - \\max_j(z_j))\\right) \\right)\n$$\n\n### 第 2 部分：$K=2$ 时 CCE 与 BCE 的等价性\n\n对于二元分类问题（$K=2$）的特殊情况，我们将证明 CCE 等价于二元交叉熵（BCE）。设 logits 为 $\\mathbf{z}=[z_0, z_1]$。我们定义一个带有标签 $y \\in \\{0, 1\\}$ 的二元随机变量，其中 $y=1$ 对应类别 $c=1$，$y=0$ 对应类别 $c=0$。\n\n类别 1 的概率由对数几率的 sigmoid（逻辑斯蒂）函数给出。类别 1 相对于类别 0 的对数几率是 $a = z_1 - z_0$。概率 $p_1$ 是：\n$$\np_1 = P(y=1) = \\frac{\\exp(z_1)}{\\exp(z_0) + \\exp(z_1)} = \\frac{\\exp(z_1-z_0)}{\\exp(z_0-z_0) + \\exp(z_1-z_0)} = \\frac{\\exp(a)}{1 + \\exp(a)} = \\sigma(a)\n$$\n类别 0 的概率是 $p_0 = 1 - p_1 = 1 - \\sigma(a) = \\sigma(-a)$。对于二元标签 $y$ 和 logit $a$，BCE 损失是：\n$$\nL_{\\text{BCE}} = -[y \\log(p_1) + (1-y) \\log(p_0)] = -[y \\log(\\sigma(a)) + (1-y) \\log(\\sigma(-a))]\n$$\n现在，我们来分析 $K=2$ 时的 CCE 损失：$L_{\\text{CCE}} = -z_c + \\log(\\exp(z_0) + \\exp(z_1))$。\n- 如果真实类别是 $c=1$（因此 $y=1$）：\n$$\nL_{\\text{CCE}}(c=1) = -z_1 + \\log(\\exp(z_0) + \\exp(z_1)) = -z_1 + \\log(\\exp(z_1)(\\exp(z_0-z_1) + 1)) = -z_1 + z_1 + \\log(\\exp(-a) + 1) = \\log(1 + \\exp(-a))\n$$\n$y=1$ 时的 BCE 损失为 $-\\log(\\sigma(a)) = -\\log(\\frac{1}{1+\\exp(-a)}) = \\log(1+\\exp(-a))$。表达式匹配。\n\n- 如果真实类别是 $c=0$（因此 $y=0$）：\n$$\nL_{\\text{CCE}}(c=0) = -z_0 + \\log(\\exp(z_0) + \\exp(z_1)) = -z_0 + \\log(\\exp(z_0)(1 + \\exp(z_1-z_0))) = -z_0 + z_0 + \\log(1 + \\exp(a)) = \\log(1 + \\exp(a))\n$$\n$y=0$ 时的 BCE 损失为 $-\\log(\\sigma(-a)) = -\\log(\\frac{1}{1+\\exp(a)}) = \\log(1+\\exp(a))$。表达式再次匹配。\n因此，当 $K=2$ 时，CCE 等价于 BCE。\n\n### 第 3 部分：使用 SoftPlus 实现数值稳定的 BCE\n\nSoftPlus 函数定义为 $\\text{SoftPlus}(x) = \\log(1 + \\exp(x))$。我们推导出的 BCE 表达式对于 $y=1$ 是 $\\text{SoftPlus}(-a)$，对于 $y=0$ 是 $\\text{SoftPlus}(a)$。一个统一且更方便的 BCE 损失表达式是：\n$$\nL_{\\text{BCE}} = -ya + \\text{SoftPlus}(a) = -ya + \\log(1 + \\exp(a))\n$$\n我们来验证这个形式：\n- 对于 $y=1$：$L = -a + \\log(1+\\exp(a)) = \\log(\\exp(-a)) + \\log(1+\\exp(a)) = \\log(\\exp(-a)(1+\\exp(a))) = \\log(1+\\exp(-a))$。正确。\n- 对于 $y=0$：$L = \\log(1+\\exp(a))$。正确。\n\n这种形式 $L_{\\text{BCE}} = -ya + \\text{SoftPlus}(a)$ 是稳定的，前提是 $\\text{SoftPlus}(a)$ 是稳定计算的。$\\text{SoftPlus}(a) = \\log(1+\\exp(a))$ 的朴素计算在 $a$ 为大的正数时会上溢。我们可以使用恒等式 $\\log(1+\\exp(a)) = a + \\log(1+\\exp(-a))$。这给出了一个稳定的实现：\n$$\n\\text{SoftPlus}(a) =\n\\begin{cases}\n    \\log(1 + \\exp(a))  \\text{if } a \\le 0 \\\\\n    a + \\log(1 + \\exp(-a))  \\text{if } a > 0\n\\end{cases}\n$$\n这可以紧凑地写成 $\\max(0, a) + \\log(1 + \\exp(-|a|))$。将此代入统一的 BCE 表达式，可以为任何 $y$ 和 $a$ 的值提供完全稳定的计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests naive and stable cross-entropy loss functions.\n    \"\"\"\n\n    def naive_cce(z, c):\n        \"\"\"\n        Computes CCE using a naive implementation of softmax. Detects numerical failure.\n        A failure is defined as any intermediate or final value being non-finite (inf, -inf, or nan).\n        \n        Returns:\n            (bool, float): A tuple (failure_detected, loss_value).\n        \"\"\"\n        try:\n            # Use high precision floats to be robust, but still subject to machine limits\n            z_np = np.array(z, dtype=np.float64)\n\n            # Python's `math.exp` raises OverflowError, numpy's `np.exp` returns `inf`\n            # We catch both possibilities using numpy's behavior and checking for inf.\n            with np.errstate(over='ignore'):\n                exps = np.exp(z_np)\n            \n            # Check for overflow\n            if np.isinf(exps).any():\n                return True, 0.0\n\n            sum_exps = np.sum(exps)\n            \n            # Check for underflow of all terms, or overflow of sum\n            if sum_exps == 0.0 or not np.isfinite(sum_exps):\n                return True, 0.0\n\n            probs = exps / sum_exps\n            \n            # This check is mostly for division by zero (sum_exps=0) resulting in nan\n            if not np.all(np.isfinite(probs)):\n                return True, 0.0\n\n            # The probability of the correct class might be zero due to underflow\n            prob_c = probs[c]\n            if prob_c == 0:\n                return True, 0.0\n\n            loss = -np.log(prob_c)\n\n            if not np.isfinite(loss):\n                return True, 0.0\n\n            return False, loss\n        except Exception:\n            return True, 0.0\n\n    def stable_cce(z, c):\n        \"\"\"\n        Computes CCE using the numerically stable log-sum-exp trick.\n        \"\"\"\n        z_np = np.array(z, dtype=np.float64)\n        m = np.max(z_np)\n        log_sum_exp = m + np.log(np.sum(np.exp(z_np - m)))\n        loss = -z_np[c] + log_sum_exp\n        return loss\n\n    def stable_bce(z, c):\n        \"\"\"\n        Computes numerically stable BCE for a K=2 case.\n        The problem statement specifies to model the provided correct class `c`\n        as the positive class with a binary label y=1.\n        \"\"\"\n        z_np = np.array(z, dtype=np.float64)\n        \n        # The logit 'a' is the log-odds of the positive class (class `c`)\n        if c == 0:\n            a = z_np[0] - z_np[1]\n        else: # c == 1\n            a = z_np[1] - z_np[0]\n            \n        y = 1.0 # The label for the positive class is 1.\n\n        # Stable SoftPlus(x) = max(0, x) + log(1 + exp(-|x|))\n        stable_softplus_a = np.maximum(0.0, a) + np.log(1.0 + np.exp(-np.abs(a)))\n        \n        # Stable BCE loss L = -y*a + SoftPlus(a)\n        loss = -y * a + stable_softplus_a\n        return loss\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'z': [1.0, 0.0, -1.0], 'c': 0},\n        {'z': [120.0, 0.0, -120.0], 'c': 1},\n        {'z': [1000.0, 999.0, 998.0], 'c': 0},\n        {'z': [-1000.0, -1001.0, -1002.0], 'c': 2},\n        {'z': [5.0, -5.0], 'c': 0},\n    ]\n\n    results = []\n\n    # Process Cases 1-4\n    for i in range(4):\n        case = test_cases[i]\n        fail, _ = naive_cce(case['z'], case['c'])\n        s_loss = stable_cce(case['z'], case['c'])\n        results.append(fail)\n        results.append(f\"{s_loss:.6f}\")\n    \n    # Process Case 5\n    case5 = test_cases[4]\n    fail5, _ = naive_cce(case5['z'], case5['c'])\n    cce_loss5 = stable_cce(case5['z'], case5['c'])\n    bce_loss5 = stable_bce(case5['z'], case5['c'])\n    abs_diff = np.abs(cce_loss5 - bce_loss5)\n    \n    results.append(fail5)\n    results.append(f\"{abs_diff:.12f}\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "学会计算整体损失值后，一个自然的问题是：这个指标除了给出一个总分外，还能告诉我们更多信息吗？本练习将指导你将总损失分解为来自每个类别的贡献部分 。这种分解方法将损失函数从一个简单的性能度量转变为一个强大的诊断工具，它能帮助我们洞察模型在不同类别上的表现差异，这对于诊断和解决类别不平衡或特定类别校准不佳等问题至关重要。",
            "id": "3103440",
            "problem": "给定一个分类数据集，其中有 $K=3$ 个类别和 $N=12$ 个带标签的样本。对于每个样本 $i \\in \\{1,\\dots,N\\}$，模型输出一个预测概率向量 $\\mathbf{p}_i = (p_{i1}, p_{i2}, p_{i3})$，满足 $\\sum_{k=1}^{3} p_{ik} = 1$，真实标签表示为一个独热向量 $\\mathbf{y}_i = (y_{i1}, y_{i2}, y_{i3})$，其中 $y_{ik} \\in \\{0,1\\}$ 且 $\\sum_{k=1}^{3} y_{ik} = 1$。假设对数底为自然对数 $\\ln$。数据集上的分类交叉熵 (CCE) 损失定义为每个样本的负对数似然之和。\n\n任务：\n1) 从独热标签的多项式模型的负对数似然的定义出发，推导出将总分类交叉熵 (CCE) 损失分解为按类别贡献的总和。然后，定义每个类别的平均负对数似然以及所有样本的总体平均负对数似然。\n2) 使用您的分解，计算以下类别不平衡误校准诊断指数：\n$$\nI \\;=\\; \\sum_{k=1}^{3} \\frac{n_k}{N} \\left(\\bar{\\ell}_k - \\bar{\\ell}\\right)^{2},\n$$\n其中 $n_k$ 是真实类别为 $k$ 的样本数量，$\\bar{\\ell}_k$ 是类别 $k$ 的平均负对数似然，而 $\\bar{\\ell}$ 是所有 $N$ 个样本的总体平均负对数似然。\n\n使用以下包含 12 个样本的数据集，每个样本均以真实类别和预测概率向量的形式给出：\n- 样本 1：真实类别 1, $\\mathbf{p}_1=(0.8, 0.1, 0.1)$。\n- 样本 2：真实类别 1, $\\mathbf{p}_2=(0.7, 0.2, 0.1)$。\n- 样本 3：真实类别 1, $\\mathbf{p}_3=(0.9, 0.05, 0.05)$。\n- 样本 4：真实类别 1, $\\mathbf{p}_4=(0.6, 0.3, 0.1)$。\n- 样本 5：真实类别 1, $\\mathbf{p}_5=(0.8, 0.15, 0.05)$。\n- 样本 6：真实类别 1, $\\mathbf{p}_6=(0.7, 0.15, 0.15)$。\n- 样本 7：真实类别 2, $\\mathbf{p}_7=(0.3, 0.5, 0.2)$。\n- 样本 8：真实类别 2, $\\mathbf{p}_8=(0.2, 0.6, 0.2)$。\n- 样本 9：真实类别 2, $\\mathbf{p}_9=(0.4, 0.4, 0.2)$。\n- 样本 10：真实类别 2, $\\mathbf{p}_{10}=(0.25, 0.5, 0.25)$。\n- 样本 11：真实类别 3, $\\mathbf{p}_{11}=(0.4, 0.3, 0.3)$。\n- 样本 12：真实类别 3, $\\mathbf{p}_{12}=(0.6, 0.2, 0.2)$。\n\n从第一性原理计算 $I$ 并报告一个单一的数值。将您的答案四舍五入到四位有效数字。",
            "solution": "用户提供的问题提法明确，具有科学依据，并包含了求解所需的所有信息。因此，该问题被认为是**有效的**。\n\n### 第1部分：CCE分解和定义的推导\n\n起点是多项式分类模型下单个观测值 $i$ 的似然。对于一个真实类别标签由独热向量 $\\mathbf{y}_i = (y_{i1}, y_{i2}, \\dots, y_{iK})$ 表示，模型预测向量为 $\\mathbf{p}_i = (p_{i1}, p_{i2}, \\dots, p_{iK})$ 的观测值，其似然是真实结果的概率：\n$$\nL(\\mathbf{p}_i | \\mathbf{y}_i) = \\prod_{k=1}^{K} p_{ik}^{y_{ik}}\n$$\n其中 $K$ 是类别数量。\n\n这个单一观测值的负对数似然（NLL），记为 $\\ell_i$，定义为：\n$$\n\\ell_i = -\\ln(L(\\mathbf{p}_i | \\mathbf{y}_i)) = -\\ln\\left(\\prod_{k=1}^{K} p_{ik}^{y_{ik}}\\right) = -\\sum_{k=1}^{K} y_{ik} \\ln(p_{ik})\n$$\n由于 $\\mathbf{y}_i$ 是一个独热向量，因此只有一个索引 $k=c_i$ 使得 $y_{ic_i}=1$，而对于所有 $k \\neq c_i$ 都有 $y_{ik}=0$，其中 $c_i$ 是样本 $i$ 的真实类别。因此，该和可以简化为单个项：\n$$\n\\ell_i = -y_{ic_i} \\ln(p_{ic_i}) = -\\ln(p_{ic_i})\n$$\n一个包含 $N$ 个样本的数据集上的总分类交叉熵（CCE）损失是各个负对数似然（NLL）的总和：\n$$\nL_{CCE} = \\sum_{i=1}^{N} \\ell_i = \\sum_{i=1}^{N} \\left(-\\sum_{k=1}^{K} y_{ik} \\ln(p_{ik})\\right) = -\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\ln(p_{ik})\n$$\n为了将这个总损失分解为按类别的贡献，我们交换求和的顺序：\n$$\nL_{CCE} = -\\sum_{k=1}^{K} \\sum_{i=1}^{N} y_{ik} \\ln(p_{ik})\n$$\n设 $S_k$ 是所有真实类别为 $k$ 的样本的索引集合。对于给定的类别 $k$，如果 $i \\in S_k$，则项 $y_{ik}$ 为 1，否则为 0。因此，内部求和可以限制在属于类别 $k$ 的样本上：\n$$\nL_{CCE} = \\sum_{k=1}^{K} \\left(-\\sum_{i \\in S_k} \\ln(p_{ik})\\right)\n$$\n我们将来自类别 $k$ 的总损失贡献定义为 $L_k$：\n$$\nL_k = -\\sum_{i \\in S_k} \\ln(p_{ik})\n$$\n那么，总 CCE 损失就是这些按类别贡献的总和：\n$$\nL_{CCE} = \\sum_{k=1}^{K} L_k\n$$\n设 $n_k = |S_k|$ 是类别 $k$ 中的样本数量。每个类别的平均负对数似然 $\\bar{\\ell}_k$ 是类别 $k$ 中样本的平均损失：\n$$\n\\bar{\\ell}_k = \\frac{L_k}{n_k} = \\frac{1}{n_k} \\sum_{i \\in S_k} (-\\ln(p_{ik}))\n$$\n总体平均负对数似然 $\\bar{\\ell}$ 是所有 $N$ 个样本的平均损失：\n$$\n\\bar{\\ell} = \\frac{L_{CCE}}{N} = \\frac{1}{N} \\sum_{i=1}^{N} \\ell_i = \\frac{1}{N} \\sum_{k=1}^{K} L_k = \\frac{1}{N} \\sum_{k=1}^{K} n_k \\bar{\\ell}_k = \\sum_{k=1}^{K} \\frac{n_k}{N} \\bar{\\ell}_k\n$$\n至此完成了所需的推导和定义。\n\n### 第2部分：诊断指数 $I$ 的计算\n\n首先，我们处理给定的数据集。我们有 $N=12$ 个样本和 $K=3$ 个类别。我们识别出属于每个类别的样本并计算其数量。\n- **类别 1**：样本 $1, 2, 3, 4, 5, 6$。数量为 $n_1=6$。\n- **类别 2**：样本 $7, 8, 9, 10$。数量为 $n_2=4$。\n- **类别 3**：样本 $11, 12$。数量为 $n_3=2$。\n总数为 $n_1+n_2+n_3 = 6+4+2=12=N$，符合预期。\n\n接下来，我们计算每个类别的平均负对数似然（NLL）$\\bar{\\ell}_k$。\n\n对于**类别 1** ($k=1$)：\n真实类别是 1，所以我们对 $i \\in \\{1, \\dots, 6\\}$ 使用 $p_{i1}$。\n$$\n\\bar{\\ell}_1 = \\frac{1}{6} \\left[ -\\ln(0.8) - \\ln(0.7) - \\ln(0.9) - \\ln(0.6) - \\ln(0.8) - \\ln(0.7) \\right]\n$$\n$$\n\\bar{\\ell}_1 = \\frac{1}{6} \\left[ -2\\ln(0.8) - 2\\ln(0.7) - \\ln(0.9) - \\ln(0.6) \\right]\n$$\n$$\n\\bar{\\ell}_1 = -\\frac{1}{6} \\ln(0.8^2 \\cdot 0.7^2 \\cdot 0.9 \\cdot 0.6) = -\\frac{1}{6} \\ln(0.169344) \\approx \\frac{1.775823}{6} \\approx 0.295971\n$$\n对于**类别 2** ($k=2$)：\n真实类别是 2，所以我们对 $i \\in \\{7, \\dots, 10\\}$ 使用 $p_{i2}$。\n$$\n\\bar{\\ell}_2 = \\frac{1}{4} \\left[ -\\ln(0.5) - \\ln(0.6) - \\ln(0.4) - \\ln(0.5) \\right]\n$$\n$$\n\\bar{\\ell}_2 = \\frac{1}{4} \\left[ -2\\ln(0.5) - \\ln(0.6) - \\ln(0.4) \\right]\n$$\n$$\n\\bar{\\ell}_2 = -\\frac{1}{4} \\ln(0.5^2 \\cdot 0.6 \\cdot 0.4) = -\\frac{1}{4} \\ln(0.06) \\approx \\frac{2.813411}{4} \\approx 0.703353\n$$\n对于**类别 3** ($k=3$)：\n真实类别是 3，所以我们对 $i \\in \\{11, 12\\}$ 使用 $p_{i3}$。\n$$\n\\bar{\\ell}_3 = \\frac{1}{2} \\left[ -\\ln(0.3) - \\ln(0.2) \\right]\n$$\n$$\n\\bar{\\ell}_3 = -\\frac{1}{2} \\ln(0.3 \\cdot 0.2) = -\\frac{1}{2} \\ln(0.06) \\approx \\frac{2.813411}{2} \\approx 1.406705\n$$\n接下来，我们计算总体平均负对数似然（NLL）$\\bar{\\ell}$：\n$$\n\\bar{\\ell} = \\sum_{k=1}^{3} \\frac{n_k}{N} \\bar{\\ell}_k = \\frac{6}{12}\\bar{\\ell}_1 + \\frac{4}{12}\\bar{\\ell}_2 + \\frac{2}{12}\\bar{\\ell}_3\n$$\n$$\n\\bar{\\ell} \\approx \\frac{1}{2}(0.295971) + \\frac{1}{3}(0.703353) + \\frac{1}{6}(1.406705)\n$$\n$$\n\\bar{\\ell} \\approx 0.1479855 + 0.234451 + 0.234451 = 0.6168875\n$$\n最后，我们计算类别不平衡误校准诊断指数 $I$：\n$$\nI = \\sum_{k=1}^{3} \\frac{n_k}{N} \\left(\\bar{\\ell}_k - \\bar{\\ell}\\right)^{2}\n$$\n我们计算各个项：\n- 对于 $k=1$：\n$$\n\\frac{n_1}{N}\\left(\\bar{\\ell}_1 - \\bar{\\ell}\\right)^2 \\approx \\frac{6}{12} (0.295971 - 0.6168875)^2 = 0.5 \\times (-0.3209165)^2 \\approx 0.5 \\times 0.1029874 \\approx 0.051494\n$$\n- 对于 $k=2$：\n$$\n\\frac{n_2}{N}\\left(\\bar{\\ell}_2 - \\bar{\\ell}\\right)^2 \\approx \\frac{4}{12} (0.703353 - 0.6168875)^2 = \\frac{1}{3} \\times (0.0864655)^2 \\approx \\frac{1}{3} \\times 0.0074763 \\approx 0.002492\n$$\n- 对于 $k=3$：\n$$\n\\frac{n_3}{N}\\left(\\bar{\\ell}_3 - \\bar{\\ell}\\right)^2 \\approx \\frac{2}{12} (1.406705 - 0.6168875)^2 = \\frac{1}{6} \\times (0.7898175)^2 \\approx \\frac{1}{6} \\times 0.6238117 \\approx 0.103969\n$$\n将各项相加得到 $I$ 的值：\n$$\nI \\approx 0.051494 + 0.002492 + 0.103969 = 0.157955\n$$\n将结果四舍五入到四位有效数字，我们得到 $0.1580$。",
            "answer": "$$\\boxed{0.1580}$$"
        },
        {
            "introduction": "在机器学习中，模型的优化目标并非仅仅是最小化损失函数。本练习将探讨损失、准确率和模型校准度之间的复杂关系 。你将分析一个案例，其中降低交叉熵损失反而因模型“过度自信”而导致其可靠性下降。通过这个案例，你将学会使用温度缩放（temperature scaling）这一实用技术来修正模型的校准度，从而在不牺牲预测准确率的前提下，提升模型的整体可信度。",
            "id": "3103445",
            "problem": "给定一个分类场景，其中模型对 $N$ 个样本生成 $K$ 个类别的未归一化分数 (logits)。从带有 softmax 链接函数的多项式模型的最大似然原理出发，推导作为多类别分类损失函数的每个样本的平均负对数似然。然后，通过一个案例研究，展示降低此损失会提高准确率但因预测过于自信而恶化校准度的情况，并应用温度缩放来校正校准度，同时保持预测的类别排名。\n\n你的程序必须实现以下步骤，不得使用任何跳过推导的捷径：\n\n- 从多项分布的最大似然原理和 softmax 构造出发，推导出分类负对数似然损失（通常称为分类交叉熵）作为样本平均值的表达式。\n- 将分类准确率定义为预测类别与真实类别相等的样本所占的比例，并进行计算。\n- 使用置信度分箱定义并计算期望校准误差 (ECE) 来衡量校准度：将区间 $[0,1]$ 划分为 $B$ 个等宽的箱，为每个箱计算平均置信度和平均准确率之间的差异，并按每个箱中样本的比例进行加权平均。所有量必须表示为 $[0,1]$ 范围内的小数。\n\n使用以下由三个案例组成的明确测试套件。对于每个案例，真实标签都是长度为 $N = 8$、有 $K = 3$ 个类别的相同向量。标签被编码为 $\\{0,1,2\\}$ 中的整数：\n\n- 标签 $y$：$[0,1,2,0,1,2,0,2]$。\n\n- 案例 1（基线 logits $L^{(1)}$，具有中等置信度和混合的正确性）：一个形状为 $N \\times K$ 的矩阵，\n  $$\n  L^{(1)} =\n  \\begin{bmatrix}\n  0.8  0.9  0.7 \\\\\n  0.4  0.6  0.5 \\\\\n  0.3  0.4  0.5 \\\\\n  0.6  0.5  0.4 \\\\\n  0.7  0.6  0.5 \\\\\n  0.5  0.6  0.7 \\\\\n  0.2  0.3  0.1 \\\\\n  0.4  0.5  0.45\n  \\end{bmatrix}.\n  $$\n  使用 $B = 5$ 个置信度箱计算 ECE。\n\n- 案例 2（过度自信的 logits $L^{(2)}$，具有更高的准确率但校准度更差）：\n  $$\n  L^{(2)} =\n  \\begin{bmatrix}\n  3.0  1.0  -1.0 \\\\\n  0.0  3.0  -1.0 \\\\\n  -0.5  0.5  2.5 \\\\\n  2.5  0.0  -1.5 \\\\\n  3.0  0.5  -0.5 \\\\\n  -1.0  0.0  3.0 \\\\\n  -1.0  3.0  -2.0 \\\\\n  -1.5  0.0  2.5\n  \\end{bmatrix}.\n  $$\n  使用 $B = 5$ 个置信度箱计算 ECE。\n\n- 案例 3（对案例 2 进行温度缩放）：应用温度缩放，温度 $T \\in S$，其中 $S = \\{1.0, 1.5, 2.0, 3.0\\}$，方法是在 softmax 之前将 $L^{(2)}$ 中的每个 logit 除以 $T$。选择使案例 2 给定数据上的期望校准误差最小化的温度 $T^\\star \\in S$（将 ECE 表示为小数）。计算在 $T^\\star$ 下的准确率、平均分类损失和 ECE。\n\n科学真实性约束：\n\n- 推导必须从多项分类的最大似然和作为链接函数的 softmax 函数开始，而不是从预先给定的目标公式开始。\n- 置信度定义为每个样本的最大预测类别概率。\n- 温度缩放必须保持类别排名，因为除以一个正标量 $T$ 不会改变 logits 的顺序。\n\n最终输出要求：\n\n- 对于每个案例，按顺序计算以下值：准确率、平均分类负对数似然损失和期望校准误差 (ECE)，每个值都表示为 $[0,1]$ 范围内的小数。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序如下：\n  $$\n  [\\text{Acc}^{(1)}, \\text{Loss}^{(1)}, \\text{ECE}^{(1)}, \\text{Acc}^{(2)}, \\text{Loss}^{(2)}, \\text{ECE}^{(2)}, \\text{Acc}^{(2,T^\\star)}, \\text{Loss}^{(2,T^\\star)}, \\text{ECE}^{(2,T^\\star)}].\n  $$\n- 将每个输出四舍五入到 $6$ 位小数。所有量必须是小数；不要使用百分号。不涉及物理单位；不要包含任何单位。\n\n你的实现必须是一个完整的、可运行的程序，它不接受任何输入，并精确打印所需的单行输出。",
            "solution": "该问题是有效的，因为它是一个独立的、有科学依据且定义明确的机器学习练习。它要求从第一性原理推导一个基本损失函数，并将其应用于一个具体的数值场景，以说明模型准确率、置信度和校准度等重要概念。所有必要的数据、定义和约束都已提供。我们现在可以开始求解。\n\n解决方案的结构如下：\n1.  从最大似然原理推导多类别分类的平均负对数似然损失。\n2.  性能指标的正式定义：准确率、置信度和期望校准误差 (ECE)。\n3.  将这些原理应用于所提供的三个测试案例。\n\n**1. 负对数似然损失的推导**\n\n我们考虑一个具有 $N$ 个独立样本和 $K$ 个类别的多类别分类问题。对于每个样本 $i$（其中 $i \\in \\{1, \\dots, N\\}$），模型接收一个输入向量 $x_i$ 并生成一个未归一化分数（或 logits）向量 $l_i = (l_{i1}, l_{i2}, \\dots, l_{iK})$。\n\n模型对样本 $i$ 的每个类别 $k \\in \\{1, \\dots, K\\}$ 的预测概率是通过对 logits 应用 softmax 函数得到的：\n$$\np_{ik} = P(\\text{class}=k | x_i) = \\frac{\\exp(l_{ik})}{\\sum_{j=1}^{K} \\exp(l_{ij})}\n$$\n样本 $i$ 的概率集合是向量 $p_i = (p_{i1}, p_{i2}, \\dots, p_{iK})$。注意，对于所有的 $i, k$，都有 $\\sum_{k=1}^K p_{ik} = 1$ 且 $p_{ik} > 0$。\n\n样本 $i$ 的真实标签表示为 $c_i \\in \\{1, \\dots, K\\}$。我们可以使用一个长度为 $K$ 的 one-hot 编码向量 $y_i$ 来表示此标签，其第 $k$ 个分量 $y_{ik}$ 由指示函数给出：\n$$\ny_{ik} = \\mathbb{I}(k=c_i) = \\begin{cases} 1  \\text{if } k = c_i \\\\ 0  \\text{if } k \\neq c_i \\end{cases}\n$$\n单个样本的分类结果可以建模为从分类分布（Categorical distribution）中的一次抽样，这是多项分布（Multinomial distribution）在只有一次试验（$n=1$）时的特例。在给定预测概率 $p_i$ 的情况下，观测到结果 $y_i$ 的概率质量函数为：\n$$\nP(Y_i=y_i) = \\prod_{k=1}^{K} p_{ik}^{y_{ik}}\n$$\n由于 $y_{ik}$ 仅在 $k$ 为真实类别 $c_i$ 时为 $1$，否则为 $0$，因此该表达式简化为 $p_{i, c_i}$，即真实类别的预测概率。\n\n最大似然估计 (MLE) 原理指出，我们应该选择能够最大化观测到给定数据集的似然的模型参数（这些参数决定了 logits $l_i$）。假设 $N$ 个样本是独立同分布 (i.i.d.) 的，则数据集的总似然是各个样本似然的乘积：\n$$\n\\mathcal{L} = \\prod_{i=1}^{N} P(Y_i=y_i) = \\prod_{i=1}^{N} \\prod_{k=1}^{K} p_{ik}^{y_{ik}}\n$$\n为了数值稳定性和数学上的便利，我们使用对数似然：\n$$\n\\log \\mathcal{L} = \\log \\left( \\prod_{i=1}^{N} \\prod_{k=1}^{K} p_{ik}^{y_{ik}} \\right) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log p_{ik}\n$$\n在机器学习优化中，标准的做法是最小化损失函数，而不是最大化似然。最大化对数似然等价于最小化负对数似然 (NLL)：\n$$\n\\text{NLL} = - \\log \\mathcal{L} = - \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log p_{ik}\n$$\n该表达式是真实分布（one-hot 向量 $y_i$）和预测分布（$p_i$）之间的分类交叉熵的定义。作为训练的损失函数，我们通常使用每个样本的平均 NLL：\n$$\nJ = \\frac{1}{N} \\text{NLL} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log p_{ik}\n$$\n同样，由于 $y_{ik}$ 仅在真实类别 $c_i$ 时非零，内层求和会消减，损失函数简化为：\n$$\nJ = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p_{i, c_i}\n$$\n这就是平均负对数似然，或称分类交叉熵损失。通过代入 softmax 公式，我们可以用 logits $l_i$ 来表示单个样本 $i$ 的损失：\n$$\nJ_i = -\\log p_{i, c_i} = -\\log \\left( \\frac{\\exp(l_{i, c_i})}{\\sum_{j=1}^{K} \\exp(l_{ij})} \\right) = \\log\\left(\\sum_{j=1}^{K} \\exp(l_{ij})\\right) - l_{i, c_i}\n$$\n推导至此完成。\n\n**2. 性能指标**\n\n我们将所需指标定义如下：\n\n-   **准确率**：样本 $i$ 的预测类别为 $\\hat{y}_i = \\arg\\max_{k} p_{ik} = \\arg\\max_{k} l_{ik}$。准确率是正确分类样本的比例：\n    $$\n    \\text{Accuracy} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}(\\hat{y}_i = c_i)\n    $$\n-   **置信度**：样本 $i$ 的预测置信度是模型赋予其预测类别的概率，即输出向量 $p_i$ 中的最大概率：\n    $$\n    \\text{conf}_i = \\max_{k} p_{ik}\n    $$\n-   **期望校准误差 (ECE)**：ECE 衡量置信度与准确率之间的差异。将区间 $[0,1]$ 划分为 $B$ 个等宽的箱 $M_1, M_2, \\dots, M_B$。对于每个箱 $M_b$，我们计算所有预测置信度落入该箱的样本的平均准确率和平均置信度。设 $I_b$ 为样本索引 $i$ 的集合，使得 $\\text{conf}_i \\in M_b$，并设 $n_b = |I_b|$。若 $n_b > 0$：\n    $$\n    \\text{acc}(b) = \\frac{1}{n_b} \\sum_{i \\in I_b} \\mathbb{I}(\\hat{y}_i = c_i)\n    $$\n    $$\n    \\text{conf}(b) = \\frac{1}{n_b} \\sum_{i \\in I_b} \\text{conf}_i\n    $$\n    ECE 是这些量在所有箱上的绝对差的加权平均值：\n    $$\n    \\text{ECE} = \\sum_{b=1}^{B} \\frac{n_b}{N} \\left| \\text{acc}(b) - \\text{conf}(b) \\right|\n    $$\n    一个完美校准的模型其 ECE 为 $0$。\n\n-   **温度缩放**：这是一种用于改善模型校准度的后处理技术。它的工作原理是在应用 softmax 函数之前，用一个正温度参数 $T > 0$ 来缩放 logits：\n    $$\n    p_{ik}(T) = \\frac{\\exp(l_{ik}/T)}{\\sum_{j=1}^{K} \\exp(l_{ij}/T)}\n    $$\n    $T>1$ 的值会“软化”概率，使其更接近均匀分布，从而减少过度自信。一个关键特性是，对于任何 $T > 0$，温度缩放不会改变 logits 的 $\\arg\\max$，因此它不会改变模型的预测 $\\hat{y}_i$ 或其准确率。最优温度 $T^\\star$ 是通过在验证集上最小化一个校准指标（如 ECE）来找到的。\n\n**3. 在测试案例中的应用**\n\n我们现在将为指定的三个案例计算所需的指标（准确率、损失、ECE）。在所有案例中，$N=8$，$K=3$，真实标签为 $y = [0, 1, 2, 0, 1, 2, 0, 2]$。\n\n-   **案例 1**：基线 logits $L^{(1)}$，使用 $B=5$ 个箱计算 ECE。\n-   **案例 2**：过度自信的 logits $L^{(2)}$，使用 $B=5$ 个箱计算 ECE。我们预期会看到相比案例 1 更高的准确率、更低的损失，但 ECE 更高，这展示了经典的准确率-校准度权衡。\n-   **案例 3**：对案例 2 应用温度缩放。我们将从集合 $S = \\{1.0, 1.5, 2.0, 3.0\\}$ 中找到最优温度 $T^\\star$，该温度使案例 2 数据的 ECE 最小化。如前所述，案例 3 的准确率将与案例 2 的相同。然后，我们将报告用 $T^\\star$ 校准后的模型的全部三个指标。\n\n计算由最终答案中提供的程序执行。该程序实现了如上所述的 softmax 函数、损失函数、准确率计算以及带分箱的 ECE 计算。然后，它遍历所有案例并计算所需的值。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving and calculating classification metrics\n    for the provided test cases.\n    \"\"\"\n\n    # --- Data Definition ---\n    y_true = np.array([0, 1, 2, 0, 1, 2, 0, 2])\n    \n    L1 = np.array([\n        [0.8, 0.9, 0.7],\n        [0.4, 0.6, 0.5],\n        [0.3, 0.4, 0.5],\n        [0.6, 0.5, 0.4],\n        [0.7, 0.6, 0.5],\n        [0.5, 0.6, 0.7],\n        [0.2, 0.3, 0.1],\n        [0.4, 0.5, 0.45]\n    ])\n\n    L2 = np.array([\n        [3.0, 1.0, -1.0],\n        [0.0, 3.0, -1.0],\n        [-0.5, 0.5, 2.5],\n        [2.5, 0.0, -1.5],\n        [3.0, 0.5, -0.5],\n        [-1.0, 0.0, 3.0],\n        [-1.0, 3.0, -2.0],\n        [-1.5, 0.0, 2.5]\n    ])\n\n    T_values = [1.0, 1.5, 2.0, 3.0]\n    n_bins = 5\n\n    # --- Helper Functions ---\n    def stable_softmax(logits, T=1.0):\n        \"\"\"\n        Computes softmax probabilities in a numerically stable way.\n        Optionally applies temperature scaling.\n        \"\"\"\n        scaled_logits = logits / T\n        # Subtract max for numerical stability\n        exps = np.exp(scaled_logits - np.max(scaled_logits, axis=1, keepdims=True))\n        return exps / np.sum(exps, axis=1, keepdims=True)\n\n    def calculate_metrics(logits, y, n_bins_ece, T=1.0):\n        \"\"\"\n        Calculates accuracy, average NLL loss, and ECE.\n        \"\"\"\n        N = logits.shape[0]\n        probs = stable_softmax(logits, T)\n\n        # Accuracy\n        y_pred = np.argmax(probs, axis=1)\n        correctness = (y_pred == y)\n        accuracy = np.mean(correctness)\n\n        # Average Categorical Negative Log-Likelihood (Cross-Entropy)\n        # We need the probabilities corresponding to the true classes\n        true_class_probs = probs[np.arange(N), y]\n        # To avoid log(0), add a small epsilon\n        loss = -np.mean(np.log(true_class_probs + 1e-9))\n        \n        # Expected Calibration Error (ECE)\n        confidences = np.max(probs, axis=1)\n        ece = 0.0\n        bin_boundaries = np.linspace(0, 1, n_bins_ece + 1)\n        \n        for b in range(n_bins_ece):\n            bin_lower = bin_boundaries[b]\n            bin_upper = bin_boundaries[b+1]\n            \n            # Define samples in the current bin\n            # The first bin [0, 1/B] is inclusive of 0.\n            if b == 0:\n                in_bin = (confidences >= bin_lower)  (confidences = bin_upper)\n            else:\n                # Subsequent bins ( (k-1)/B, k/B ] are exclusive of the lower bound\n                in_bin = (confidences > bin_lower)  (confidences = bin_upper)\n            \n            n_in_bin = np.sum(in_bin)\n            \n            if n_in_bin > 0:\n                acc_in_bin = np.mean(correctness[in_bin])\n                conf_in_bin = np.mean(confidences[in_bin])\n                ece += (n_in_bin / N) * np.abs(acc_in_bin - conf_in_bin)\n                \n        return accuracy, loss, ece\n\n    # --- Calculations for each case ---\n    results = []\n\n    # Case 1\n    acc1, loss1, ece1 = calculate_metrics(L1, y_true, n_bins)\n    results.extend([acc1, loss1, ece1])\n\n    # Case 2\n    acc2, loss2, ece2 = calculate_metrics(L2, y_true, n_bins)\n    results.extend([acc2, loss2, ece2])\n\n    # Case 3\n    ece_by_T = []\n    metrics_by_T = []\n    for T in T_values:\n        metrics = calculate_metrics(L2, y_true, n_bins, T=T)\n        metrics_by_T.append(metrics)\n        ece_by_T.append(metrics[2])\n\n    # Find the temperature T* that minimizes ECE\n    # np.argmin returns the index of the first minimum in case of a tie.\n    best_T_idx = np.argmin(ece_by_T)\n    acc3, loss3, ece3 = metrics_by_T[best_T_idx]\n    results.extend([acc3, loss3, ece3])\n    \n    # --- Format and Print Output ---\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}