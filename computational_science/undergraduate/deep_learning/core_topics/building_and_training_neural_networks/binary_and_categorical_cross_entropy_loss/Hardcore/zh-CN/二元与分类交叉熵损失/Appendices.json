{
    "hands_on_practices": [
        {
            "introduction": "理论上，交叉熵的公式看似简单，但在实际的计算机实现中，直接计算指数和对数很容易遇到数值溢出的挑战。本练习将指导你推导并实现一种数值稳定的分类交叉熵（Categorical Cross-Entropy, CCE）计算方法，即“log-sum-exp”技巧，确保你的模型训练过程稳健可靠。通过这个练习，你还将从第一性原理出发，理解二元交叉熵（Binary Cross-Entropy, BCE）与分类交叉熵在$K=2$时的内在联系。",
            "id": "3103417",
            "problem": "给定一个分类问题，其类别数量有限，通过类别分布进行建模。对于一个输入，其未归一化的对数概率（称为 logits）为 $\\mathbf{z} \\in \\mathbb{R}^K$，其 one-hot 目标向量为 $\\mathbf{y} \\in \\{0,1\\}^K$，其中正确类别索引为 $c$ 且满足 $y_c = 1$，则在该类别模型下，正确类别的负对数似然定义了类别交叉熵（Categorical Cross-Entropy, CCE）。概率向量通过对 logits 取指数并进行归一化得到，这是 softmax 变换的标准定义。请从类别分布的核心定义、负对数似然、指数和对数的性质，以及在所有 logits 上加上一个常数后概率比率保持不变的特性出发，推导出一个数值稳定的 CCE 表达式，该表达式无需在原始尺度上通过指数运算来构建概率。你的推导应明确使用一个恒等式，该恒等式通过一个依赖于数据的常数进行移位，从而以避免上溢或下溢的方式来表示指数和的对数。请解释为什么当 logits 具有较大的正值时，该恒等式可以避免算术上溢；当 logits 具有较大的负值时，可以避免下溢。请使用以下经过充分测试的事实：对于双精度浮点数，当 $x \\gtrsim 709$ 时 $\\exp(x)$ 会发生上溢，当 $x \\lesssim -745$ 时会下溢至零。\n\n此外，在 $K=2$ 个类别的特殊情况下，请从第一性原理出发，证明 CCE 可以简化为二元标签 $y \\in \\{0,1\\}$ 与伯努利模型之间的二元交叉熵（Binary Cross-Entropy, BCE）。该伯努利模型的 logit 参数 $a \\in \\mathbb{R}$ 等于由两个 logits 导出的对数几率。请使用 SoftPlus 函数将 BCE 表示为数值稳定的形式，其中 SoftPlus 函数仅由基本函数定义。\n\n你的程序必须：\n- 实现一个朴素的 CCE，它通过直接对 logits 进行指数运算和归一化来计算 softmax 概率，然后计算单个样本的损失。它应该通过检查任何中间值或最终值是否变为非有限值来检测失败（上溢或下溢）。\n- 实现一个数值稳定的 CCE，使用适当的移位和指数和对数的稳定计算方法，而无需在原始尺度上构建概率。\n- 为 $K=2$ 的情况实现一个带 logits 的数值稳定的 BCE，使用 SoftPlus 函数以防止上溢和下溢。\n- 将这些实现应用于下面的测试套件，并返回所要求的输出。\n\n测试套件（每个案例都是一个包含 logits $\\mathbf{z}$ 和正确类别索引 $c$ 的单一输入）：\n- 案例 1：$K=3$，$\\mathbf{z} = [1.0, 0.0, -1.0]$，$c=0$。\n- 案例 2：$K=3$，$\\mathbf{z} = [120.0, 0.0, -120.0]$，$c=1$。\n- 案例 3：$K=3$，$\\mathbf{z} = [1000.0, 999.0, 998.0]$，$c=0$。\n- 案例 4：$K=3$，$\\mathbf{z} = [-1000.0, -1001.0, -1002.0]$，$c=2$。\n- 案例 5（$K=2$ 时的等价性检查）：$\\mathbf{z} = [5.0, -5.0]$，$c=0$。设 $a$ 为 $\\mathbf{z}$ 对 $c=0$ 类别所隐含的对数几率，对应的二元标签为 $y=1$。计算此示例中稳定 CCE 和稳定 BCE 之间的绝对差值。\n\n对于案例 1 到 4，你必须按顺序为每个案例输出两个值：\n- 一个布尔值，指示基于朴素 softmax 的 CCE 计算是否失败（非有限损失），其中失败定义为产生一个非有限值。\n- 该案例的数值稳定的 CCE 损失，四舍五入到 $6$ 位小数。\n\n对于案例 5，你必须输出两个值：\n- 一个布尔值，指示基于朴素 softmax 的 CCE 计算在此案例中是否失败。\n- 此示例中稳定 CCE 和稳定 BCE 之间的绝对差值，四舍五入到 $12$ 位小数。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含案例 1 到 5 的所有结果，按顺序串联成一个用方括号括起来的逗号分隔列表。顺序为：\n  $[\\text{fail}_1, \\text{stable\\_loss}_1, \\text{fail}_2, \\text{stable\\_loss}_2, \\text{fail}_3, \\text{stable\\_loss}_3, \\text{fail}_4, \\text{stable\\_loss}_4, \\text{fail}_5, \\text{abs\\_diff}_{5}]$。\n- 所有浮点数必须按上述规定四舍五入打印。不应打印任何其他文本。",
            "solution": "该问题定义明确（well-posed），具有科学依据，并包含了严谨求解所需的所有信息。我们着手进行推导和实现。\n\n### 第 1 部分：数值稳定的类别交叉熵（CCE）的推导\n\n类别交叉熵（CCE）损失是在一个模型下，真实类别的负对数似然。对于单个数据点，给定未归一化的对数概率（logits）向量 $\\mathbf{z} \\in \\mathbb{R}^K$ 和正确的类别索引 $c$，我们首先使用 softmax 函数计算概率向量 $\\mathbf{p}$：\n$$\np_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^K \\exp(z_j)} \\quad \\text{对于 } i=1, \\dots, K\n$$\n观测到正确类别 $c$ 的似然是 $p_c$。CCE 损失是该似然的负对数：\n$$\nL_{\\text{CCE}} = -\\log(p_c) = -\\log\\left(\\frac{\\exp(z_c)}{\\sum_{j=1}^K \\exp(z_j)}\\right)\n$$\n使用对数性质 $\\log(a/b) = \\log(a) - \\log(b)$ 以及 $\\log(\\exp(x)) = x$，我们可以将损失重写为：\n$$\nL_{\\text{CCE}} = -(\\log(\\exp(z_c)) - \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right)) = -z_c + \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right)\n$$\n这个表达式虽然在数学上是正确的，但在数值上是不稳定的。项 $\\sum_j \\exp(z_j)$ 容易出现浮点错误。\n- **上溢**：如果任何 logit $z_j$ 是一个较大的正数（例如，对于双精度浮点数，$z_j \\gtrsim 709$），$\\exp(z_j)$ 将计算为无穷大，导致整个表达式变为非有限值。\n- **下溢**：如果所有 logits $z_j$ 都是较大的负数（例如，$z_j \\lesssim -745$），每个 $\\exp(z_j)$ 都会下溢为 $0$。其和将为 $0$，而其对数将为 $-\\infty$，再次导致非有限损失。\n\n为了稳定此计算，我们使用“log-sum-exp”技巧。这依赖于一个恒等式，即对于任何常数 $m$，概率比率是不变的：\n$$\np_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^K \\exp(z_j)} = \\frac{\\exp(z_i) \\cdot \\exp(-m)}{\\sum_{j=1}^K \\exp(z_j) \\cdot \\exp(-m)} = \\frac{\\exp(z_i - m)}{\\sum_{j=1}^K \\exp(z_j - m)}\n$$\n这种不变性允许我们平移 logits 而不改变最终的概率。我们将此思想应用于项 $\\log(\\sum_j \\exp(z_j))$：\n$$\n\\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right) = \\log\\left(\\exp(m) \\sum_{j=1}^K \\exp(z_j - m)\\right) = \\log(\\exp(m)) + \\log\\left(\\sum_{j=1}^K \\exp(z_j - m)\\right) = m + \\log\\left(\\sum_{j=1}^K \\exp(z_j - m)\\right)\n$$\n通过选择 $m = \\max_j(z_j)$，我们确保了数值稳定性：\n1.  **防止上溢**：现在，指数函数的参数 $z_j - m$ 始终小于或等于 $0$。其最大值在索引 $k$ 处取得，其中 $z_k$ 是最大值，即 $z_k - m = m - m = 0$。因此，和中的最大项是 $\\exp(0) = 1$。这可以防止任何项上溢。\n2.  **防止下溢**：由于和 $\\sum_j \\exp(z_j - m)$ 中至少有一项是 $\\exp(0)=1$，因此该和保证至少为 $1$。这可以防止对数的参数下溢为零，从而避免得到 $-\\infty$ 的结果。\n\n将这个稳定的 log-sum-exp 表达式代回损失公式，我们得到数值稳定的 CCE：\n$$\nL_{\\text{CCE, stable}} = -z_c + \\left( \\max_j(z_j) + \\log\\left(\\sum_{j=1}^K \\exp(z_j - \\max_j(z_j))\\right) \\right)\n$$\n\n### 第 2 部分：$K=2$ 时 CCE 和 BCE 的等价性\n\n对于二元分类问题（$K=2$）的特殊情况，我们将证明 CCE 等价于二元交叉熵（BCE）。设 logits 为 $\\mathbf{z}=[z_0, z_1]$。我们定义一个标签为 $y \\in \\{0, 1\\}$ 的二元随机变量，其中 $y=1$ 对应类别 $c=1$，$y=0$ 对应类别 $c=0$。\n\n类别 1 的概率由对数几率的 sigmoid（logistic）函数给出。类别 1 相对于类别 0 的对数几率是 $a = z_1 - z_0$。概率 $p_1$ 是：\n$$\np_1 = P(y=1) = \\frac{\\exp(z_1)}{\\exp(z_0) + \\exp(z_1)} = \\frac{\\exp(z_1-z_0)}{\\exp(z_0-z_0) + \\exp(z_1-z_0)} = \\frac{\\exp(a)}{1 + \\exp(a)} = \\sigma(a)\n$$\n类别 0 的概率是 $p_0 = 1 - p_1 = 1 - \\sigma(a) = \\sigma(-a)$。对于二元标签 $y$ 和 logit $a$，BCE 损失为：\n$$\nL_{\\text{BCE}} = -[y \\log(p_1) + (1-y) \\log(p_0)] = -[y \\log(\\sigma(a)) + (1-y) \\log(\\sigma(-a))]\n$$\n现在，我们来分析 $K=2$ 时的 CCE 损失：$L_{\\text{CCE}} = -z_c + \\log(\\exp(z_0) + \\exp(z_1))$。\n- 如果真实类别是 $c=1$（因此 $y=1$）：\n$$\nL_{\\text{CCE}}(c=1) = -z_1 + \\log(\\exp(z_0) + \\exp(z_1)) = -z_1 + \\log(\\exp(z_1)(\\exp(z_0-z_1) + 1)) = -z_1 + z_1 + \\log(\\exp(-a) + 1) = \\log(1 + \\exp(-a))\n$$\n$y=1$ 时的 BCE 损失为 $-\\log(\\sigma(a)) = -\\log(\\frac{1}{1+\\exp(-a)}) = \\log(1+\\exp(-a))$。表达式匹配。\n\n- 如果真实类别是 $c=0$（因此 $y=0$）：\n$$\nL_{\\text{CCE}}(c=0) = -z_0 + \\log(\\exp(z_0) + \\exp(z_1)) = -z_0 + \\log(\\exp(z_0)(1 + \\exp(z_1-z_0))) = -z_0 + z_0 + \\log(1 + \\exp(a)) = \\log(1 + \\exp(a))\n$$\n$y=0$ 时的 BCE 损失为 $-\\log(\\sigma(-a)) = -\\log(\\frac{1}{1+\\exp(a)}) = \\log(1+\\exp(a))$。表达式再次匹配。\n因此，当 $K=2$ 时，CCE 等价于 BCE。\n\n### 第 3 部分：使用 SoftPlus 实现数值稳定的 BCE\n\nSoftPlus 函数定义为 $\\text{SoftPlus}(x) = \\log(1 + \\exp(x))$。我们推导出的 BCE 表达式对于 $y=1$ 是 $\\text{SoftPlus}(-a)$，对于 $y=0$ 是 $\\text{SoftPlus}(a)$。一个统一且更方便的 BCE 损失表达式是：\n$$\nL_{\\text{BCE}} = -ya + \\text{SoftPlus}(a) = -ya + \\log(1 + \\exp(a))\n$$\n我们来验证这个形式：\n- 对于 $y=1$：$L = -a + \\log(1+\\exp(a)) = \\log(\\exp(-a)) + \\log(1+\\exp(a)) = \\log(\\exp(-a)(1+\\exp(a))) = \\log(1+\\exp(-a))$。正确。\n- 对于 $y=0$：$L = \\log(1+\\exp(a))$。正确。\n\n这种形式 $L_{\\text{BCE}} = -ya + \\text{SoftPlus}(a)$ 是稳定的，前提是 $\\text{SoftPlus}(a)$ 是以稳定的方式计算的。$\\text{SoftPlus}(a) = \\log(1+\\exp(a))$ 的朴素计算在 $a$ 是大正数时会发生上溢。我们可以使用恒等式 $\\log(1+\\exp(a)) = a + \\log(1+\\exp(-a))$。这给出了一个稳定的实现：\n$$\n\\text{SoftPlus}(a) =\n\\begin{cases}\n    \\log(1 + \\exp(a))  \\text{如果 } a \\le 0 \\\\\n    a + \\log(1 + \\exp(-a))  \\text{如果 } a > 0\n\\end{cases}\n$$\n这可以紧凑地写为 $\\max(0, a) + \\log(1 + \\exp(-|a|))$。将此代入统一的 BCE 表达式中，可以为任何 $y$ 和 $a$ 的值得到一个完全稳定的计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests naive and stable cross-entropy loss functions.\n    \"\"\"\n\n    def naive_cce(z, c):\n        \"\"\"\n        Computes CCE using a naive implementation of softmax. Detects numerical failure.\n        A failure is defined as any intermediate or final value being non-finite (inf, -inf, or nan).\n        \n        Returns:\n            (bool, float): A tuple (failure_detected, loss_value).\n        \"\"\"\n        try:\n            # Use high precision floats to be robust, but still subject to machine limits\n            z_np = np.array(z, dtype=np.float64)\n\n            # Python's `math.exp` raises OverflowError, numpy's `np.exp` returns `inf`\n            # We catch both possibilities using numpy's behavior and checking for inf.\n            with np.errstate(over='ignore'):\n                exps = np.exp(z_np)\n            \n            # Check for overflow\n            if np.isinf(exps).any():\n                return True, 0.0\n\n            sum_exps = np.sum(exps)\n            \n            # Check for underflow of all terms, or overflow of sum\n            if sum_exps == 0.0 or not np.isfinite(sum_exps):\n                return True, 0.0\n\n            probs = exps / sum_exps\n            \n            # This check is mostly for division by zero (sum_exps=0) resulting in nan\n            if not np.all(np.isfinite(probs)):\n                return True, 0.0\n\n            # The probability of the correct class might be zero due to underflow\n            prob_c = probs[c]\n            if prob_c == 0:\n                return True, 0.0\n\n            loss = -np.log(prob_c)\n\n            if not np.isfinite(loss):\n                return True, 0.0\n\n            return False, loss\n        except Exception:\n            return True, 0.0\n\n    def stable_cce(z, c):\n        \"\"\"\n        Computes CCE using the numerically stable log-sum-exp trick.\n        \"\"\"\n        z_np = np.array(z, dtype=np.float64)\n        m = np.max(z_np)\n        log_sum_exp = m + np.log(np.sum(np.exp(z_np - m)))\n        loss = -z_np[c] + log_sum_exp\n        return loss\n\n    def stable_bce(z, c):\n        \"\"\"\n        Computes numerically stable BCE for a K=2 case.\n        The problem statement specifies to model the provided correct class `c`\n        as the positive class with a binary label y=1.\n        \"\"\"\n        z_np = np.array(z, dtype=np.float64)\n        \n        # The logit 'a' is the log-odds of the positive class (class `c`)\n        if c == 0:\n            a = z_np[0] - z_np[1]\n        else: # c == 1\n            a = z_np[1] - z_np[0]\n            \n        y = 1.0 # The label for the positive class is 1.\n\n        # Stable SoftPlus(x) = max(0, x) + log(1 + exp(-|x|))\n        stable_softplus_a = np.maximum(0.0, a) + np.log(1.0 + np.exp(-np.abs(a)))\n        \n        # Stable BCE loss L = -y*a + SoftPlus(a)\n        loss = -y * a + stable_softplus_a\n        return loss\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'z': [1.0, 0.0, -1.0], 'c': 0},\n        {'z': [120.0, 0.0, -120.0], 'c': 1},\n        {'z': [1000.0, 999.0, 998.0], 'c': 0},\n        {'z': [-1000.0, -1001.0, -1002.0], 'c': 2},\n        {'z': [5.0, -5.0], 'c': 0},\n    ]\n\n    results = []\n\n    # Process Cases 1-4\n    for i in range(4):\n        case = test_cases[i]\n        fail, _ = naive_cce(case['z'], case['c'])\n        s_loss = stable_cce(case['z'], case['c'])\n        results.append(fail)\n        results.append(f\"{s_loss:.6f}\")\n    \n    # Process Case 5\n    case5 = test_cases[4]\n    fail5, _ = naive_cce(case5['z'], case5['c'])\n    cce_loss5 = stable_cce(case5['z'], case5['c'])\n    bce_loss5 = stable_bce(case5['z'], case5['c'])\n    abs_diff = np.abs(cce_loss5 - bce_loss5)\n    \n    results.append(fail5)\n    results.append(f\"{abs_diff:.12f}\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个单一的平均损失值就像一份总结报告，它概括了模型的整体表现，但往往会掩盖模型在处理不同类别数据时的性能差异。本练习将教你如何将整体的分类交叉熵损失分解为各个类别的贡献之和，从而能够更精细地诊断模型的行为。这种分解方法是分析模型在不平衡数据集上表现、识别特定类别学习困难等问题的有力工具。",
            "id": "3103440",
            "problem": "给定一个分类数据集，其中有 $K=3$ 个类别和 $N=12$ 个已标注的样本。对于每个样本 $i \\in \\{1,\\dots,N\\}$，模型输出一个预测概率向量 $\\mathbf{p}_i = (p_{i1}, p_{i2}, p_{i3})$，其中 $\\sum_{k=1}^{3} p_{ik} = 1$，真实标签表示为一个 one-hot 向量 $\\mathbf{y}_i = (y_{i1}, y_{i2}, y_{i3})$，其中 $y_{ik} \\in \\{0,1\\}$ 且 $\\sum_{k=1}^{3} y_{ik} = 1$。假设对数的底为自然对数 $\\ln$。数据集上的分类交叉熵 (CCE) 损失定义为每个样本的负对数似然之和。\n\n任务：\n1) 从使用 one-hot 标签的多项式模型的负对数似然的定义出发，推导出将总分类交叉熵 (CCE) 损失分解为各类别贡献之和的形式。然后，定义各类别平均负对数似然以及所有样本的总体平均负对数似然。\n2) 使用您的分解，计算以下类别不平衡误校准诊断指数：\n$$\nI \\;=\\; \\sum_{k=1}^{3} \\frac{n_k}{N} \\left(\\bar{\\ell}_k - \\bar{\\ell}\\right)^{2},\n$$\n其中 $n_k$ 是真实类别为 $k$ 的样本数量，$\\bar{\\ell}_k$ 是类别 $k$ 的平均负对数似然，$\\bar{\\ell}$ 是所有 $N$ 个样本的总体平均负对数似然。\n\n使用以下包含 12 个样本的数据集，每个样本均以真实类别和预测概率向量的形式给出：\n- 样本 $1$：真实类别 $1$，$\\mathbf{p}_1=(0.8, 0.1, 0.1)$。\n- 样本 $2$：真实类别 $1$，$\\mathbf{p}_2=(0.7, 0.2, 0.1)$。\n- 样本 $3$：真实类别 $1$，$\\mathbf{p}_3=(0.9, 0.05, 0.05)$。\n- 样本 $4$：真实类别 $1$，$\\mathbf{p}_4=(0.6, 0.3, 0.1)$。\n- 样本 $5$：真实类别 $1$，$\\mathbf{p}_5=(0.8, 0.15, 0.05)$。\n- 样本 $6$：真实类别 $1$，$\\mathbf{p}_6=(0.7, 0.15, 0.15)$。\n- 样本 $7$：真实类别 $2$，$\\mathbf{p}_7=(0.3, 0.5, 0.2)$。\n- 样本 $8$：真实类别 $2$，$\\mathbf{p}_8=(0.2, 0.6, 0.2)$。\n- 样本 $9$：真实类别 $2$，$\\mathbf{p}_9=(0.4, 0.4, 0.2)$。\n- 样本 $10$：真实类别 $2$，$\\mathbf{p}_{10}=(0.25, 0.5, 0.25)$。\n- 样本 $11$：真实类别 $3$，$\\mathbf{p}_{11}=(0.4, 0.3, 0.3)$。\n- 样本 $12$：真实类别 $3$，$\\mathbf{p}_{12}=(0.6, 0.2, 0.2)$。\n\n从基本原理计算 $I$ 并报告一个数值。将您的答案四舍五入到四位有效数字。",
            "solution": "用户提供的问题表述清晰，具有科学依据，并包含了求解所需的所有信息。因此，该问题被认定为**有效**。\n\n### 第 1 部分：CCE 分解和定义的推导\n\n起点是多项式分类模型下单个观测值 $i$ 的似然。对于一个真实类别标签由 one-hot 向量 $\\mathbf{y}_i = (y_{i1}, y_{i2}, \\dots, y_{iK})$ 表示、模型预测向量为 $\\mathbf{p}_i = (p_{i1}, p_{i2}, \\dots, p_{iK})$ 的观测，其似然是真实结果的概率：\n$$\nL(\\mathbf{p}_i | \\mathbf{y}_i) = \\prod_{k=1}^{K} p_{ik}^{y_{ik}}\n$$\n其中 $K$ 是类别数。\n\n这个单次观测的负对数似然（NLL），记为 $\\ell_i$，定义为：\n$$\n\\ell_i = -\\ln(L(\\mathbf{p}_i | \\mathbf{y}_i)) = -\\ln\\left(\\prod_{k=1}^{K} p_{ik}^{y_{ik}}\\right) = -\\sum_{k=1}^{K} y_{ik} \\ln(p_{ik})\n$$\n由于 $\\mathbf{y}_i$ 是一个 one-hot 向量，因此只有一个索引 $k=c_i$ 使得 $y_{ic_i}=1$，而对于所有 $k \\neq c_i$ 都有 $y_{ik}=0$，其中 $c_i$ 是样本 $i$ 的真实类别。因此，该和式简化为单项：\n$$\n\\ell_i = -y_{ic_i} \\ln(p_{ic_i}) = -\\ln(p_{ic_i})\n$$\n一个包含 $N$ 个样本的数据集上的总分类交叉熵（CCE）损失是个体 NLL 的总和：\n$$\nL_{CCE} = \\sum_{i=1}^{N} \\ell_i = \\sum_{i=1}^{N} \\left(-\\sum_{k=1}^{K} y_{ik} \\ln(p_{ik})\\right) = -\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\ln(p_{ik})\n$$\n为了将这个总损失分解为各类别贡献，我们交换求和顺序：\n$$\nL_{CCE} = -\\sum_{k=1}^{K} \\sum_{i=1}^{N} y_{ik} \\ln(p_{ik})\n$$\n设 $S_k$ 为所有真实类别为 $k$ 的样本的索引集合。对于给定的类别 $k$，如果 $i \\in S_k$，则项 $y_{ik}$ 为 $1$，否则为 $0$。因此，内层求和可以限制在属于类别 $k$ 的样本上：\n$$\nL_{CCE} = \\sum_{k=1}^{K} \\left(-\\sum_{i \\in S_k} \\ln(p_{ik})\\right)\n$$\n我们将来自类别 $k$ 的总损失贡献（记为 $L_k$）定义为：\n$$\nL_k = -\\sum_{i \\in S_k} \\ln(p_{ik})\n$$\n总 CCE 损失就是这些各类别贡献之和：\n$$\nL_{CCE} = \\sum_{k=1}^{K} L_k\n$$\n设 $n_k = |S_k|$ 为类别 $k$ 中的样本数量。各类别平均负对数似然 $\\bar{\\ell}_k$ 是类别 $k$ 中样本的平均损失：\n$$\n\\bar{\\ell}_k = \\frac{L_k}{n_k} = \\frac{1}{n_k} \\sum_{i \\in S_k} (-\\ln(p_{ik}))\n$$\n总体平均负对数似然 $\\bar{\\ell}$ 是所有 $N$ 个样本的平均损失：\n$$\n\\bar{\\ell} = \\frac{L_{CCE}}{N} = \\frac{1}{N} \\sum_{i=1}^{N} \\ell_i = \\frac{1}{N} \\sum_{k=1}^{K} L_k = \\frac{1}{N} \\sum_{k=1}^{K} n_k \\bar{\\ell}_k = \\sum_{k=1}^{K} \\frac{n_k}{N} \\bar{\\ell}_k\n$$\n这就完成了所需的推导和定义。\n\n### 第 2 部分：诊断指数 $I$ 的计算\n\n首先，我们处理给定的数据集。我们有 $N=12$ 个样本和 $K=3$ 个类别。我们识别属于每个类别的样本并计算它们的数量。\n- **类别 1**：样本 $1, 2, 3, 4, 5, 6$。数量为 $n_1=6$。\n- **类别 2**：样本 $7, 8, 9, 10$。数量为 $n_2=4$。\n- **类别 3**：样本 $11, 12$。数量为 $n_3=2$。\n总数为 $n_1+n_2+n_3 = 6+4+2=12=N$，符合预期。\n\n接下来，我们计算各类别平均 NLL，$\\bar{\\ell}_k$。\n\n对于**类别 1** ($k=1$)：\n真实类别是 $1$，所以我们对 $i \\in \\{1, \\dots, 6\\}$ 使用 $p_{i1}$。\n$$\n\\bar{\\ell}_1 = \\frac{1}{6} \\left[ -\\ln(0.8) - \\ln(0.7) - \\ln(0.9) - \\ln(0.6) - \\ln(0.8) - \\ln(0.7) \\right]\n$$\n$$\n\\bar{\\ell}_1 = \\frac{1}{6} \\left[ -2\\ln(0.8) - 2\\ln(0.7) - \\ln(0.9) - \\ln(0.6) \\right]\n$$\n$$\n\\bar{\\ell}_1 = -\\frac{1}{6} \\ln(0.8^2 \\cdot 0.7^2 \\cdot 0.9 \\cdot 0.6) = -\\frac{1}{6} \\ln(0.169344) \\approx \\frac{1.775823}{6} \\approx 0.295971\n$$\n对于**类别 2** ($k=2$)：\n真实类别是 $2$，所以我们对 $i \\in \\{7, \\dots, 10\\}$ 使用 $p_{i2}$。\n$$\n\\bar{\\ell}_2 = \\frac{1}{4} \\left[ -\\ln(0.5) - \\ln(0.6) - \\ln(0.4) - \\ln(0.5) \\right]\n$$\n$$\n\\bar{\\ell}_2 = \\frac{1}{4} \\left[ -2\\ln(0.5) - \\ln(0.6) - \\ln(0.4) \\right]\n$$\n$$\n\\bar{\\ell}_2 = -\\frac{1}{4} \\ln(0.5^2 \\cdot 0.6 \\cdot 0.4) = -\\frac{1}{4} \\ln(0.06) \\approx \\frac{2.813411}{4} \\approx 0.703353\n$$\n对于**类别 3** ($k=3$)：\n真实类别是 $3$，所以我们对 $i \\in \\{11, 12\\}$ 使用 $p_{i3}$。\n$$\n\\bar{\\ell}_3 = \\frac{1}{2} \\left[ -\\ln(0.3) - \\ln(0.2) \\right]\n$$\n$$\n\\bar{\\ell}_3 = -\\frac{1}{2} \\ln(0.3 \\cdot 0.2) = -\\frac{1}{2} \\ln(0.06) \\approx \\frac{2.813411}{2} \\approx 1.406705\n$$\n接下来，我们计算总体平均 NLL，$\\bar{\\ell}$：\n$$\n\\bar{\\ell} = \\sum_{k=1}^{3} \\frac{n_k}{N} \\bar{\\ell}_k = \\frac{6}{12}\\bar{\\ell}_1 + \\frac{4}{12}\\bar{\\ell}_2 + \\frac{2}{12}\\bar{\\ell}_3\n$$\n$$\n\\bar{\\ell} \\approx \\frac{1}{2}(0.295971) + \\frac{1}{3}(0.703353) + \\frac{1}{6}(1.406705)\n$$\n$$\n\\bar{\\ell} \\approx 0.1479855 + 0.234451 + 0.234451 = 0.6168875\n$$\n最后，我们计算类别不平衡误校准诊断指数 $I$：\n$$\nI = \\sum_{k=1}^{3} \\frac{n_k}{N} \\left(\\bar{\\ell}_k - \\bar{\\ell}\\right)^{2}\n$$\n我们计算各个项：\n- 对于 $k=1$：\n$$\n\\frac{n_1}{N}\\left(\\bar{\\ell}_1 - \\bar{\\ell}\\right)^2 \\approx \\frac{6}{12} (0.295971 - 0.6168875)^2 = 0.5 \\times (-0.3209165)^2 \\approx 0.5 \\times 0.1029874 \\approx 0.051494\n$$\n- 对于 $k=2$：\n$$\n\\frac{n_2}{N}\\left(\\bar{\\ell}_2 - \\bar{\\ell}\\right)^2 \\approx \\frac{4}{12} (0.703353 - 0.6168875)^2 = \\frac{1}{3} \\times (0.0864655)^2 \\approx \\frac{1}{3} \\times 0.0074763 \\approx 0.002492\n$$\n- 对于 $k=3$：\n$$\n\\frac{n_3}{N}\\left(\\bar{\\ell}_3 - \\bar{\\ell}\\right)^2 \\approx \\frac{2}{12} (1.406705 - 0.6168875)^2 = \\frac{1}{6} \\times (0.7898175)^2 \\approx \\frac{1}{6} \\times 0.6238117 \\approx 0.103969\n$$\n将这些项相加得到 $I$ 的值：\n$$\nI \\approx 0.051494 + 0.002492 + 0.103969 = 0.157955\n$$\n将结果四舍五入到四位有效数字，我们得到 $0.1580$。",
            "answer": "$$\\boxed{0.1580}$$"
        },
        {
            "introduction": "在模型训练中，我们通常致力于最小化损失函数以提高准确率，但这有时会带来一个副作用：模型对其预测变得“过度自信”，导致其输出的概率值并不可靠。本案例研究将引导你探索“模型校准”（model calibration）这一重要概念，并运用一种名为“温度缩放”（temperature scaling）的实用后处理技术，在不牺牲模型分类准确率的前提下，显著提升其预测概率的可信度。",
            "id": "3103445",
            "problem": "给定一个分类场景，其中一个模型为$N$个样本的$K$个类别生成未归一化的分数（logits）。从具有softmax链接函数的多项式模型的最大似然原理出发，推导每个样本的平均负对数似然作为多类别分类的损失泛函。然后，演示一个案例研究，其中降低此损失会提高准确率但因过度自信的预测而恶化校准，并应用温度缩放来校正校准，同时保持预测的类别排序不变。\n\n你的程序必须实现以下步骤，不得使用任何跳过推导的快捷方式：\n\n- 从多项式分布的最大似然原理和softmax构造出发，推导类别负对数似然损失（通常称为类别交叉熵）作为样本平均值的表达式。\n- 定义并计算分类准确率，即预测类别与真实类别相等的样本所占的比例。\n- 通过预期校准误差（ECE）使用置信度分箱来定义和计算校准度：将区间$[0,1]$划分为$B$个等宽的箱，为每个箱计算平均置信度与平均准确率之间的差异，并按箱中样本的比例聚合加权平均值。所有量必须表示为$[0,1]$范围内的小数。\n\n使用以下由三个案例组成的明确测试套件。对于每个案例，真实标签都是长度为$N=8$、具有$K=3$个类别的相同向量。标签被编码为$\\{0,1,2\\}$中的整数：\n\n- 标签$y$：$[0,1,2,0,1,2,0,2]$。\n\n- 案例$1$（基线logits $L^{(1)}$，具有中等置信度和混合的正确性）：一个形状为$N \\times K$的矩阵，\n  $$\n  L^{(1)} =\n  \\begin{bmatrix}\n  0.8  0.9  0.7 \\\\\n  0.4  0.6  0.5 \\\\\n  0.3  0.4  0.5 \\\\\n  0.6  0.5  0.4 \\\\\n  0.7  0.6  0.5 \\\\\n  0.5  0.6  0.7 \\\\\n  0.2  0.3  0.1 \\\\\n  0.4  0.5  0.45\n  \\end{bmatrix}.\n  $$\n  对ECE使用$B=5$个置信度分箱。\n\n- 案例$2$（过自信的logits $L^{(2)}$，具有更高的准确率但更差的校准）：\n  $$\n  L^{(2)} =\n  \\begin{bmatrix}\n  3.0  1.0  -1.0 \\\\\n  0.0  3.0  -1.0 \\\\\n  -0.5  0.5  2.5 \\\\\n  2.5  0.0  -1.5 \\\\\n  3.0  0.5  -0.5 \\\\\n  -1.0  0.0  3.0 \\\\\n  -1.0  3.0  -2.0 \\\\\n  -1.5  0.0  2.5\n  \\end{bmatrix}.\n  $$\n  对ECE使用$B=5$个置信度分箱。\n\n- 案例$3$（对案例2进行温度缩放）：应用温度缩放，温度$T \\in S$，其中$S = \\{1.0, 1.5, 2.0, 3.0\\}$，方法是在softmax之前将$L^{(2)}$中的每个logit除以$T$。选择使案例2给定数据上的预期校准误差最小化的温度$T^\\star \\in S$（将ECE表示为小数）。计算在$T^\\star$下的准确率、平均类别损失和ECE。\n\n科学真实性约束：\n\n- 推导必须从多项式分类的最大似然和作为链接函数的softmax函数开始，而不是从预先给定的目标公式开始。\n- 置信度定义为每个样本的最大预测类别概率。\n- 温度缩放必须保持类别排序，因为除以一个正常数$T$不会改变logits的顺序。\n\n最终输出要求：\n\n- 对于每个案例，按顺序计算以下值：准确率、平均类别负对数似然损失和预期校准误差（ECE），每个值都表示为$[0,1]$范围内的小数。\n- 你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，顺序如下：\n  $$\n  [\\text{Acc}^{(1)}, \\text{Loss}^{(1)}, \\text{ECE}^{(1)}, \\text{Acc}^{(2)}, \\text{Loss}^{(2)}, \\text{ECE}^{(2)}, \\text{Acc}^{(2,T^\\star)}, \\text{Loss}^{(2,T^\\star)}, \\text{ECE}^{(2,T^\\star)}].\n  $$\n- 将每个输出四舍五入到$6$位小数。所有量必须是小数；不要使用百分号。不涉及物理单位；不要包含任何单位。\n\n你的实现必须是一个完整的、可运行的程序，它不接受任何输入，并精确地打印所需的单行输出。",
            "solution": "该问题是有效的，因为它是一个自洽、有科学依据且定义明确的机器学习练习。它要求从第一性原理出发推导一个基本的损失函数，并将其应用于一个具体的数值场景，以说明模型准确率、置信度和校准等重要概念。所有必要的数据、定义和约束都已提供。我们现在可以开始解决该问题。\n\n解决方案的结构如下：\n1. 从最大似然原理推导多类别分类的平均负对数似然损失。\n2. 性能指标的正式定义：准确率、置信度和预期校准误差（ECE）。\n3. 将这些原理应用于提供的三个测试案例。\n\n**1. 负对数似然损失的推导**\n\n我们考虑一个具有$N$个独立样本和$K$个类别的多类别分类问题。对于每个样本$i$（其中$i \\in \\{1, \\dots, N\\}$），模型接受一个输入向量$x_i$并生成一个未归一化分数（或logits）的向量，$l_i = (l_{i1}, l_{i2}, \\dots, l_{iK})$。\n\n模型对样本$i$的每个类别$k \\in \\{1, \\dots, K\\}$的预测概率是通过对logits应用softmax函数得到的：\n$$\np_{ik} = P(\\text{class}=k | x_i) = \\frac{\\exp(l_{ik})}{\\sum_{j=1}^{K} \\exp(l_{ij})}\n$$\n样本$i$的概率集合是向量$p_i = (p_{i1}, p_{i2}, \\dots, p_{iK})$。注意，对于所有的$i, k$，$\\sum_{k=1}^K p_{ik} = 1$且$p_{ik} > 0$。\n\n样本$i$的真实标签表示为$c_i \\in \\{1, \\dots, K\\}$。我们可以使用一个长度为$K$的one-hot编码向量$y_i$来表示这个标签，其中其第$k$个分量$y_{ik}$由指示函数给出：\n$$\ny_{ik} = \\mathbb{I}(k=c_i) = \\begin{cases} 1  \\text{if } k = c_i \\\\ 0  \\text{if } k \\neq c_i \\end{cases}\n$$\n单个样本的分类结果可以建模为从类别分布（Categorical distribution）中的一次抽样，这是多项式分布在一次试验（$n=1$）下的特例。在给定预测概率$p_i$的情况下，观测到结果$y_i$的概率质量函数是：\n$$\nP(Y_i=y_i) = \\prod_{k=1}^{K} p_{ik}^{y_{ik}}\n$$\n由于$y_{ik}$仅在真实类别$c_i$时为1，否则为0，此表达式简化为$p_{i, c_i}$，即真实类别的预测概率。\n\n最大似然估计（MLE）原理指出，我们应该选择使观测到给定数据集的似然性最大化的模型参数（这些参数决定了logits $l_i$）。假设$N$个样本是独立同分布（i.i.d.）的，数据集的总似然是各个样本似然的乘积：\n$$\n\\mathcal{L} = \\prod_{i=1}^{N} P(Y_i=y_i) = \\prod_{i=1}^{N} \\prod_{k=1}^{K} p_{ik}^{y_{ik}}\n$$\n为了数值稳定性和数学上的便利，我们处理对数似然：\n$$\n\\log \\mathcal{L} = \\log \\left( \\prod_{i=1}^{N} \\prod_{k=1}^{K} p_{ik}^{y_{ik}} \\right) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log p_{ik}\n$$\n在机器学习优化中，通常是最小化一个损失函数，而不是最大化一个似然函数。最大化对数似然等价于最小化负对数似然（NLL）：\n$$\n\\text{NLL} = - \\log \\mathcal{L} = - \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log p_{ik}\n$$\n这个表达式是真实分布（one-hot向量$y_i$）和预测分布（$p_i$）之间的类别交叉熵的定义。作为训练用的损失函数，我们通常使用每个样本的平均NLL：\n$$\nJ = \\frac{1}{N} \\text{NLL} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log p_{ik}\n$$\n再次，由于$y_{ik}$仅对真实类别$c_i$为非零，内层求和可以简化，损失函数简化为：\n$$\nJ = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p_{i, c_i}\n$$\n这就是平均负对数似然，或类别交叉熵损失。通过代入softmax公式，我们可以用logits $l_i$来表示单个样本$i$的损失：\n$$\nJ_i = -\\log p_{i, c_i} = -\\log \\left( \\frac{\\exp(l_{i, c_i})}{\\sum_{j=1}^{K} \\exp(l_{ij})} \\right) = \\log\\left(\\sum_{j=1}^{K} \\exp(l_{ij})\\right) - l_{i, c_i}\n$$\n推导至此完成。\n\n**2. 性能指标**\n\n我们如下定义所需的指标：\n\n-   **准确率**：样本$i$的预测类别为$\\hat{y}_i = \\arg\\max_{k} p_{ik} = \\arg\\max_{k} l_{ik}$。准确率是正确分类样本的比例：\n    $$\n    \\text{Accuracy} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}(\\hat{y}_i = c_i)\n    $$\n-   **置信度**：样本$i$的预测置信度是模型为其预测类别分配的概率，即输出向量$p_i$中的最大概率：\n    $$\n    \\text{conf}_i = \\max_{k} p_{ik}\n    $$\n-   **预期校准误差（ECE）**：ECE衡量置信度与准确率之间的差异。区间$[0,1]$被划分为$B$个等宽的箱，$M_1, M_2, \\dots, M_B$。对于每个箱$M_b$，我们计算所有预测置信度落入该箱的样本的平均准确率和平均置信度。设$I_b$是满足$\\text{conf}_i \\in M_b$的样本索引集合，且$n_b = |I_b|$。若$n_b > 0$：\n    $$\n    \\text{acc}(b) = \\frac{1}{n_b} \\sum_{i \\in I_b} \\mathbb{I}(\\hat{y}_i = c_i)\n    $$\n    $$\n    \\text{conf}(b) = \\frac{1}{n_b} \\sum_{i \\in I_b} \\text{conf}_i\n    $$\n    ECE是这些量在所有箱上的绝对差的加权平均值：\n    $$\n    \\text{ECE} = \\sum_{b=1}^{B} \\frac{n_b}{N} \\left| \\text{acc}(b) - \\text{conf}(b) \\right|\n    $$\n    一个完美校准的模型其ECE为$0$。\n\n-   **温度缩放**：这是一种用于改善模型校准的后处理技术。它通过在softmax函数前用一个正温度参数$T > 0$来缩放logits：\n    $$\n    p_{ik}(T) = \\frac{\\exp(l_{ik}/T)}{\\sum_{j=1}^{K} \\exp(l_{ij}/T)}\n    $$\n    $T>1$的值会“软化”概率，使其更接近均匀分布，从而减少过自信。一个关键特性是，对于任何$T > 0$，温度缩放不会改变logits的$\\arg\\max$，因此它不会改变模型的预测$\\hat{y}_i$或其准确率。最优温度$T^\\star$是通过在验证集上最小化一个校准指标（如ECE）来找到的。\n\n**3. 应用于测试案例**\n\n我们现在将为三个指定案例计算所需的指标（准确率、损失、ECE）。对于所有案例，$N=8$，$K=3$，真实标签为$y = [0, 1, 2, 0, 1, 2, 0, 2]$。\n\n-   **案例1**：基线logits $L^{(1)}$，对ECE使用$B=5$个箱。\n-   **案例2**：过自信的logits $L^{(2)}$，对ECE使用$B=5$个箱。我们预计与案例1相比，会看到更高的准确率、更低的损失，但更高的ECE，这展示了经典的准确率-校准权衡。\n-   **案例3**：应用于案例2的温度缩放。我们将从集合$S = \\{1.0, 1.5, 2.0, 3.0\\}$中找到使案例2数据的ECE最小化的最优温度$T^\\star$。如前所述，案例3的准确率将与案例2的相同。然后，我们将报告用$T^\\star$校准后的模型的所有三个指标。\n\n计算由最终答案中提供的程序执行。该程序实现了如上所述的softmax函数、损失函数、准确率计算以及带分箱的ECE计算。然后，它遍历所有案例并计算所需的值。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving and calculating classification metrics\n    for the provided test cases.\n    \"\"\"\n\n    # --- Data Definition ---\n    y_true = np.array([0, 1, 2, 0, 1, 2, 0, 2])\n    \n    L1 = np.array([\n        [0.8, 0.9, 0.7],\n        [0.4, 0.6, 0.5],\n        [0.3, 0.4, 0.5],\n        [0.6, 0.5, 0.4],\n        [0.7, 0.6, 0.5],\n        [0.5, 0.6, 0.7],\n        [0.2, 0.3, 0.1],\n        [0.4, 0.5, 0.45]\n    ])\n\n    L2 = np.array([\n        [3.0, 1.0, -1.0],\n        [0.0, 3.0, -1.0],\n        [-0.5, 0.5, 2.5],\n        [2.5, 0.0, -1.5],\n        [3.0, 0.5, -0.5],\n        [-1.0, 0.0, 3.0],\n        [-1.0, 3.0, -2.0],\n        [-1.5, 0.0, 2.5]\n    ])\n\n    T_values = [1.0, 1.5, 2.0, 3.0]\n    n_bins = 5\n\n    # --- Helper Functions ---\n    def stable_softmax(logits, T=1.0):\n        \"\"\"\n        Computes softmax probabilities in a numerically stable way.\n        Optionally applies temperature scaling.\n        \"\"\"\n        scaled_logits = logits / T\n        # Subtract max for numerical stability\n        exps = np.exp(scaled_logits - np.max(scaled_logits, axis=1, keepdims=True))\n        return exps / np.sum(exps, axis=1, keepdims=True)\n\n    def calculate_metrics(logits, y, n_bins_ece, T=1.0):\n        \"\"\"\n        Calculates accuracy, average NLL loss, and ECE.\n        \"\"\"\n        N = logits.shape[0]\n        probs = stable_softmax(logits, T)\n\n        # Accuracy\n        y_pred = np.argmax(probs, axis=1)\n        correctness = (y_pred == y)\n        accuracy = np.mean(correctness)\n\n        # Average Categorical Negative Log-Likelihood (Cross-Entropy)\n        # We need the probabilities corresponding to the true classes\n        true_class_probs = probs[np.arange(N), y]\n        # To avoid log(0), add a small epsilon\n        loss = -np.mean(np.log(true_class_probs + 1e-9))\n        \n        # Expected Calibration Error (ECE)\n        confidences = np.max(probs, axis=1)\n        ece = 0.0\n        bin_boundaries = np.linspace(0, 1, n_bins_ece + 1)\n        \n        for b in range(n_bins_ece):\n            bin_lower = bin_boundaries[b]\n            bin_upper = bin_boundaries[b+1]\n            \n            # Define samples in the current bin\n            # The first bin [0, 1/B] is inclusive of 0.\n            if b == 0:\n                in_bin = (confidences >= bin_lower)  (confidences = bin_upper)\n            else:\n                # Subsequent bins ( (k-1)/B, k/B ] are exclusive of the lower bound\n                in_bin = (confidences > bin_lower)  (confidences = bin_upper)\n            \n            n_in_bin = np.sum(in_bin)\n            \n            if n_in_bin > 0:\n                acc_in_bin = np.mean(correctness[in_bin])\n                conf_in_bin = np.mean(confidences[in_bin])\n                ece += (n_in_bin / N) * np.abs(acc_in_bin - conf_in_bin)\n                \n        return accuracy, loss, ece\n\n    # --- Calculations for each case ---\n    results = []\n\n    # Case 1\n    acc1, loss1, ece1 = calculate_metrics(L1, y_true, n_bins)\n    results.extend([acc1, loss1, ece1])\n\n    # Case 2\n    acc2, loss2, ece2 = calculate_metrics(L2, y_true, n_bins)\n    results.extend([acc2, loss2, ece2])\n\n    # Case 3\n    ece_by_T = []\n    metrics_by_T = []\n    for T in T_values:\n        metrics = calculate_metrics(L2, y_true, n_bins, T=T)\n        metrics_by_T.append(metrics)\n        ece_by_T.append(metrics[2])\n\n    # Find the temperature T* that minimizes ECE\n    # np.argmin returns the index of the first minimum in case of a tie.\n    best_T_idx = np.argmin(ece_by_T)\n    acc3, loss3, ece3 = metrics_by_T[best_T_idx]\n    results.extend([acc3, loss3, ece3])\n    \n    # --- Format and Print Output ---\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}