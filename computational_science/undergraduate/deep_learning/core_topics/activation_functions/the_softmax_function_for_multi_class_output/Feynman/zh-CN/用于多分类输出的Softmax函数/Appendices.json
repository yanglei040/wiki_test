{
    "hands_on_practices": [
        {
            "introduction": "理论上的数学公式在实际的计算机中可能因浮点数精度限制而失效。这个练习将带你直面在实现 Softmax 函数时最核心的数值稳定性挑战。通过推导并实现 `log-sum-exp` 技巧，你将学会如何编写出既精确又能在各种数值范围下稳定运行的关键代码，这是任何深度学习实践者的必备技能。",
            "id": "3193214",
            "problem": "给定一个实值 logits 向量 $\\mathbf{z} = (z_1, z_2, \\dots, z_n) \\in \\mathbb{R}^n$。在使用 softmax 函数的多类分类中，对数归一化因子是量 $A(\\mathbf{z}) = \\log\\left(\\sum_{j=1}^n e^{z_j}\\right)$，它必须在有限精度算术中可靠地计算。您的任务是推导、实现并分析一种数值稳定的方法来计算 $A(\\mathbf{z})$。\n\n从指数函数和自然对数的基本定义和性质出发，并且只使用对所有实数都有效的代数变换，推导出一个通过提出 $\\mathbf{z}$ 的最大分量来避免溢出和严重有效性损失的 $A(\\mathbf{z})$ 表达式。令 $m = \\max_j z_j$。证明这个变换保持了 $A(\\mathbf{z})$ 的精确数学值，并解释为什么它在数值上是有益的。\n\n假设采用标准的有限精度舍入模型，用于电气和电子工程师协会（IEEE 754）binary64 算术，其单位舍入误差为 $u$（对于舍入到最近，为机器 epsilon $\\epsilon_{\\text{mach}}$ 的一半）。将每个基本运算 $x \\circ y$（其中 $\\circ \\in \\{+, \\times\\}$）建模为 $\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\delta)$，其中 $|\\delta| \\leq u$，并将基本函数 $f \\in \\{\\exp, \\log\\}$ 视为在其输出上最多引入 $u$ 的相对误差。对于使用直接从左到右累加的方式对 $n$ 个正项求和，使用标准界限，即计算出的和 $\\widehat{S}$ 满足一个由 $\\gamma_{n-1} = \\dfrac{(n-1)u}{1 - (n-1)u}$ 界定的相对误差。\n\n使用这些模型，推导 $A(\\mathbf{z})$ 的数值稳定计算的绝对误差的一个保守上界，该上界表示为 $m$、长度 $n$、机器精度参数 $u$ 和 $\\epsilon_{\\text{mach}}$，以及从稳定化表达式中自然产生的任何量（例如，稳定化的和 $S = \\sum_{j=1}^n e^{z_j - m}$ 及其对数）的函数。您的界限必须是显式的，并且可以从 $\\mathbf{z}$、 $u$ 和 $n$ 计算得出。\n\n实现一个程序，该程序：\n- 在不溢出的情况下，使用直接求幂和求和来计算朴素值 $A_{\\text{naive}}(\\mathbf{z})$，以用于验证。\n- 使用您推导的表达式计算稳定化值 $A_{\\text{stable}}(\\mathbf{z})$。\n- 使用任意精度算术计算高精度参考值 $A_{\\text{ref}}(\\mathbf{z})$，并通过 $m$ 进行归一化以避免在参考计算中发生溢出。\n- 根据 $m$、$u$、$n$ 以及您定义的任何稳定化中间量，计算 $A_{\\text{stable}}(\\mathbf{z})$ 的理论绝对误差界限 $B(\\mathbf{z})$。\n- 对于每个测试用例，如果 $|A_{\\text{stable}}(\\mathbf{z}) - A_{\\text{ref}}(\\mathbf{z})| \\leq B(\\mathbf{z})$，则生成一个布尔值 true，否则生成 false。\n\n使用以下输入向量 $\\mathbf{z}$ 的测试套件来检验不同的数值范围：\n1. $\\mathbf{z}_1 = (-2.0, 0.0, 1.5, -0.5)$，一个具有中等大小的通用情况。\n2. $\\mathbf{z}_2 = (1000.0, 0.0, -1000.0)$，一个在 binary64 算术的朴素求幂中会溢出的情况。\n3. $\\mathbf{z}_3 = (709.0, 700.0, 0.0)$，在 binary64 算术中接近 $e^{x}$ 溢出阈值的值。\n4. $\\mathbf{z}_4 = (-745.0, -746.0, -747.0)$，在 binary64 算术中接近 $e^{x}$ 下溢/次正规数范围的值。\n5. $\\mathbf{z}_5 = (0.0, 0.0, 0.0, 0.0, 0.0)$，用于测试对称性和累加的相等条目。\n6. $\\mathbf{z}_6$ 包含 101 个从 -10 到 10 线性间隔的条目，即 $z_j = -10 + \\dfrac{20(j-1)}{100}$，其中 $j = 1, 2, \\dots, 101$。\n\n您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如 $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_6]$，其中每个 $\\text{result}_k$ 是相应测试用例的布尔值。此问题中不出现物理单位、角度或百分比；所有输出都是无单位的。",
            "solution": "问题陈述经过严格审查，被认为是有效的。它在科学上基于数值分析和浮点运算的原理，问题提出得很好，目标明确，并为唯一的、可验证的解决方案提供了足够的信息。关于使用‘任意精度算术’的微小模糊之处，通过将其解释为指定环境中可用的最高精度 `numpy.longdouble` 来解决，这是建立参考值的标准且实用的方法。\n\n任务是推导和分析一种数值稳定的方法，用于计算对数归一化因子，或称 log-sum-exp 函数，对于向量 $\\mathbf{z} = (z_1, z_2, \\dots, z_n) \\in \\mathbb{R}^n$ 定义如下：\n$$A(\\mathbf{z}) = \\log\\left(\\sum_{j=1}^n e^{z_j}\\right)$$\n\n**1. 数值稳定表达式的推导**\n\n计算 $A(\\mathbf{z})$ 的主要数值挑战来自指数函数 $e^{z_j}$。如果任何 $z_j$ 是一个大的正数（例如，对于 binary$64$ 算术，$z_j > 709.78$），$e^{z_j}$ 将溢出到无穷大。如果 $z_j$ 值具有较大的动态范围，则和可能由单个项主导，导致较小项的有效性损失。\n\n为了缓解这些问题，我们引入一个归一化常数。令 $m = \\max_{1 \\le j \\le n} z_j$。这是向量 $\\mathbf{z}$ 的最大分量。我们可以从对数内的和中提出因子 $e^m$。这种操作在代数上是精确的：\n$$\n\\sum_{j=1}^n e^{z_j} = \\sum_{j=1}^n e^{m + (z_j - m)} = \\sum_{j=1}^n e^m e^{z_j - m} = e^m \\left(\\sum_{j=1}^n e^{z_j - m}\\right)\n$$\n将此代回 $A(\\mathbf{z})$ 的表达式中：\n$$\nA(\\mathbf{z}) = \\log\\left( e^m \\sum_{j=1}^n e^{z_j - m} \\right)\n$$\n利用对数的基本性质 $\\log(xy) = \\log(x) + \\log(y)$，我们可以分离这些项：\n$$\nA(\\mathbf{z}) = \\log(e^m) + \\log\\left(\\sum_{j=1}^n e^{z_j - m}\\right)\n$$\n最后，由于自然对数和指数函数是互逆的，$\\log(e^m) = m$。这得到了稳定化的表达式：\n$$\nA(\\mathbf{z}) = m + \\log\\left(\\sum_{j=1}^n e^{z_j - m}\\right)\n$$\n对于任何 $\\mathbf{z} \\in \\mathbb{R}^n$，此变换在数学上都是精确的。\n\n这种形式的数值优势是巨大的。令 $y_j = z_j - m$。根据 $m$ 的定义，对于所有 $j$，我们有 $y_j \\le 0$。任何 $y_j$ 的最大值为 0，这发生在索引 $k$ 处，其中 $z_k = m$。\n- **防止溢出**：指数函数的参数 $y_j$ 现在处于 $(-\\infty, 0]$ 范围内。最大值 $e^0 = 1$ 不会引起溢出。\n- **提高精度**：和 $S = \\sum_{j=1}^n e^{y_j}$ 包含项 $e^0 = 1$，所以 $S \\ge 1$。这可以防止和下溢到零，从而导致后续的对数运算失败。虽然对于非常负的 $y_j$，单个项 $e^{y_j}$ 可能会下溢到 0，但这是一种平稳退化，因为这些项在数学上对和的值是可忽略的。这种公式避免了对数量级差异巨大的数求和，这是浮点运算中精度损失的主要来源。\n\n**2. 绝对误差界限的推导**\n\n我们现在推导绝对误差 $|\\hat{A} - A(\\mathbf{z})|$ 的一个保守上界，其中 $\\hat{A}$ 是使用稳定公式在有限精度算术中计算出的值。我们使用问题中指定的浮点模型，单位舍入误差为 $u$。\n令 $A(\\mathbf{z}) = m + \\log(S)$，其中 $S = \\sum_{j=1}^n e^{z_j-m}$。\n计算值 $\\hat{A}$ 由以下浮点运算序列得出：\n1. $\\hat{y}_j = \\operatorname{fl}(z_j - m)$\n2. $\\hat{t}_j = \\operatorname{fl}(e^{\\hat{y}_j})$\n3. $\\hat{S} = \\operatorname{fl}(\\sum_j \\hat{t}_j)$\n4. $\\hat{L} = \\operatorname{fl}(\\log(\\hat{S}))$\n5. $\\hat{A} = \\operatorname{fl}(m + \\hat{L})$\n\n总绝对误差为 $|\\hat{A} - A(\\mathbf{z})|$。我们分析每一步的误差贡献。\n最后的操作是加法，所以 $\\hat{A} = (m+\\hat{L})(1+\\theta_A)$ 且 $|\\theta_A| \\le u$。\n$$|\\hat{A} - A(\\mathbf{z})| = |(m+\\hat{L})(1+\\theta_A) - (m+\\log(S))| = |\\hat{L}-\\log(S) + (m+\\hat{L})\\theta_A|$$\n$$ \\le |\\hat{L}-\\log(S)| + |m+\\hat{L}|u $$\n下一步是界定 $\\hat{L} = \\operatorname{fl}(\\log(\\hat{S})) = \\log(\\hat{S})(1+\\theta_L)$（其中 $|\\theta_L|\\le u$）的误差。\n$$ |\\hat{L} - \\log(S)| = |\\log(\\hat{S})(1+\\theta_L) - \\log(S)| = |\\log(\\hat{S}/S) + \\log(\\hat{S})\\theta_L| $$\n$$ \\le |\\log(\\hat{S}/S)| + |\\log(\\hat{S})|u $$\n为了界定 $|\\log(\\hat{S}/S)|$，我们使用中值定理，这意味着对于 $\\hat{S} \\approx S$，有 $|\\log(\\hat{S}/S)| \\approx |\\hat{S}/S - 1|$。此项表示 $S$ 的相对误差。\n$S$ 的误差来自两个来源：计算各项 $\\hat{t}_j$ 的误差和它们求和的误差。\n- 相对于真实项 $s_j = e^{z_j-m}$，每一项 $\\hat{t}_j = \\operatorname{fl}(e^{\\operatorname{fl}(z_j-m)})$ 的误差可以被界定。一阶分析表明 $|\\hat{t}_j - s_j| \\lesssim s_j(1+|z_j-m|)u$。\n- 因此，这些单独项误差之和 $\\sum |\\hat{t}_j - s_j|$ 受 $u \\sum s_j(1+|z_j-m|)$ 的界定。\n- 计算项 $\\hat{t}_j$ 的求和引入了一个相对误差，其界限为 $\\gamma_{n-1} = \\frac{(n-1)u}{1-(n-1)u}$。\n\n结合这些误差来源，和 $S$ 上的相对误差由以下公式界定：\n$$ \\left|\\frac{\\hat{S}-S}{S}\\right| \\lesssim \\gamma_{n-1} + u \\frac{\\sum_{j=1}^n s_j(1+|z_j-m|)}{S} $$\n用计算出的估计值 $\\hat{t}_j$ 和 $\\hat{S}$ 替换精确（但未知）的量 $s_j$ 和 $S$，并定义可计算的和 $\\hat{Q}_{\\text{comp}} = \\sum_{j=1}^n \\hat{t}_j(1+|z_j-m|)$，我们得到了相对误差的可计算界限：\n$$ \\left|\\frac{\\hat{S}-S}{S}\\right| \\lesssim \\gamma_{n-1} + u \\frac{\\hat{Q}_{\\text{comp}}}{\\hat{S}} $$\n综合所有部分，总绝对误差由以下公式界定：\n$$ |\\hat{A} - A(\\mathbf{z})| \\lesssim \\left( \\gamma_{n-1} + u \\frac{\\hat{Q}_{\\text{comp}}}{\\hat{S}} \\right) + |\\hat{L}|u + |m+\\hat{L}|u $$\n我们将我们的保守、可计算的上界 $B(\\mathbf{z})$ 定义为：\n$$ B(\\mathbf{z}) = \\gamma_{n-1} + u \\left( \\frac{\\hat{Q}_{\\text{comp}}}{\\hat{S}} + |\\hat{L}| + |m+\\hat{L}| \\right) $$\n其中右侧的所有量都是在工作精度下计算的。该界限考虑了来自求和（$\\gamma_{n-1}$）、项计算传播（$u\\hat{Q}_{\\text{comp}}/\\hat{S}$）、对数运算（$u|\\hat{L}|$）和最终加法（$u|m+\\hat{L}|$）的误差。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives, implements, and analyzes a numerically stable method for \n    computing the log-sum-exp function, and verifies a theoretical \n    error bound against a high-precision reference computation.\n    \"\"\"\n\n    def compute_and_verify(z_in: tuple or list):\n        \"\"\"\n        Computes the stable log-sum-exp, its error bound, a reference value,\n        and verifies that the actual error is within the theoretical bound.\n\n        Args:\n            z_in: A tuple or list of real numbers representing the input vector.\n\n        Returns:\n            A boolean, True if |A_stable - A_ref| = Bound, False otherwise.\n        \"\"\"\n        # --- Environment Setup (IEEE 754 binary64) ---\n        u = np.finfo(np.float64).eps / 2.0\n        z = np.array(z_in, dtype=np.float64)\n        n = z.size\n\n        if n > 1 and (n - 1) * u >= 1:\n            raise ValueError(\"n is too large for the summation error model to be valid.\")\n        \n        gamma_n_minus_1 = ((n - 1) * u) / (1 - (n - 1) * u) if n > 1 else 0.0\n\n        # --- Stable Computation (float64) ---\n        # A_stable = m + log(sum(exp(z - m)))\n        m = np.max(z)\n        y = z - m  # These are the shifted exponents\n        t = np.exp(y)  # These are the terms in the sum\n        S_hat = np.sum(t)\n        L_hat = np.log(S_hat)\n        A_stable = m + L_hat\n\n        # --- Theoretical Absolute Error Bound Computation (float64) ---\n        # B(z) = gamma_{n-1} + u * (Q_hat/S_hat + |L_hat| + |m+L_hat|)\n        # where Q_hat = sum(t_j * (1 + |y_j|))\n        Q_hat_comp = np.sum(t * (1.0 + m - z)) # |y_j| = m - z_j since m is max\n        \n        bound_sum_err = gamma_n_minus_1\n        bound_term_prop_err = u * (Q_hat_comp / S_hat)\n        bound_log_err = u * np.abs(L_hat)\n        bound_add_err = u * np.abs(m + L_hat)\n        \n        bound = bound_sum_err + bound_term_prop_err + bound_log_err + bound_add_err\n\n        # --- High-Precision Reference Computation (longdouble) ---\n        # The reference computation also uses the stable method to avoid overflow,\n        # but with higher precision arithmetic (numpy.longdouble).\n        z_ref = np.array(z_in, dtype=np.longdouble)\n        m_ref = np.max(z_ref)\n        y_ref = z_ref - m_ref\n        S_ref = np.sum(np.exp(y_ref))\n        A_ref = m_ref + np.log(S_ref)\n\n        # --- Comparison ---\n        actual_error = np.abs(A_stable - A_ref)\n        \n        return actual_error = bound\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1. General case\n        (-2.0, 0.0, 1.5, -0.5),\n        # 2. Overflow case\n        (1000.0, 0.0, -1000.0),\n        # 3. Near overflow threshold\n        (709.0, 700.0, 0.0),\n        # 4. Underflow/subnormal regime\n        (-745.0, -746.0, -747.0),\n        # 5. Equal entries\n        (0.0, 0.0, 0.0, 0.0, 0.0),\n        # 6. Larger summation\n        np.linspace(-10.0, 10.0, 101).tolist(),\n    ]\n\n    results = []\n    for case in test_cases:\n        is_bound_valid = compute_and_verify(case)\n        results.append(str(is_bound_valid).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "标准的交叉熵损失平等地对待所有训练样本，但这在处理类别不平衡或存在大量“简单”样本时可能效率低下。本实践将引导你实现并分析焦点损失（Focal Loss），一种巧妙的损失函数变体。你将通过从零开始推导其梯度并进行实验，亲身体验它如何自动降低简单样本的权重，从而将模型的“注意力”集中在更具挑战性的困难样本上。",
            "id": "3193212",
            "problem": "您将研究一个多类别分类器，该分类器使用 softmax 函数将实值 logit 向量映射到类别概率，并使用 focal loss 目标函数进行训练。请仅从基本定义出发，不要假设任何预先推导出的梯度。设 logit 为 $\\mathbf{z} \\in \\mathbb{R}^K$，预测概率由 softmax 函数给出：\n$$\np_i(\\mathbf{z}) \\equiv \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)} \\quad \\text{for } i \\in \\{1,\\dots,K\\}.\n$$\n对于真实类别索引 $y \\in \\{1,\\dots,K\\}$ 和聚焦参数 $\\gamma \\ge 0$，focal loss 定义为：\n$$\nL(\\mathbf{z};y,\\gamma) \\equiv -\\left(1 - p_y(\\mathbf{z})\\right)^{\\gamma} \\,\\log\\!\\left(p_y(\\mathbf{z})\\right),\n$$\n其中 $\\log$ 表示自然对数。\n\n您的任务是：\n- 仅从给定定义和标准多元微积分法则出发，推导每个分量 $i \\in \\{1,\\dots,K\\}$ 的梯度 $\\frac{\\partial L}{\\partial z_i}$。不要假设任何已知的 softmax 雅可比矩阵；从其定义推导。\n- 实现一个数值稳定的 softmax、focal loss 以及您推导的关于 $\\mathbf{z}$ 的解析梯度。\n- 评估聚焦参数对于困难样本（$p_y(\\mathbf{z})$ 很小）与简单样本（$p_y(\\mathbf{z})$ 接近于 1）的影响。\n\n程序要求：\n- 实现一个函数，通过在求幂之前先减去 $\\max_i z_i$ 来计算任何输入 $\\mathbf{z}$ 的数值稳定 softmax。\n- 实现一个函数，使用稳定的 softmax 直接从 $\\mathbf{z}$ 计算 $L(\\mathbf{z};y,\\gamma)$。\n- 实现一个函数，使用您推导的表达式计算解析梯度 $\\nabla_{\\mathbf{z}} L$。\n- 实现一个步长为 $h = 10^{-6}$ 的中心差分数值梯度检查器来近似 $\\nabla_{\\mathbf{z}} L$：\n$$\n\\left[\\nabla_{\\mathbf{z}} L(\\mathbf{z})\\right]_i \\approx \\frac{L(\\mathbf{z} + h\\,\\mathbf{e}_i) - L(\\mathbf{z} - h\\,\\mathbf{e}_i)}{2h}.\n$$\n\n测试套件和输出：\n- 通用情况梯度检查：设 $K=4$, $\\mathbf{z} = [0.0,\\,1.0,\\,-0.5,\\,2.0]$, $y = 4$, 且 $\\gamma = 2.0$。计算您的解析梯度与步长为 $h = 10^{-6}$ 的中心差分梯度之间的最大绝对差，并将此差值作为浮点数输出。\n- 边界情况聚焦参数：设 $K=3$, $\\mathbf{z} = [2.0,\\,-1.0,\\,0.5]$, $y = 1$, 且 $\\gamma = 0.0$。在这种情况下，focal loss 简化为交叉熵（Cross-Entropy, CE）。计算您的解析梯度与 CE 梯度 $\\mathbf{p} - \\mathbf{e}_y$ 之间的最大绝对差，并将此差值作为浮点数输出。这里 $\\mathbf{e}_y$ 是在索引 $y$ 处为 1、其余位置为 0 的 one-hot 向量。\n- 困难样本与简单样本的侧重对比：考虑两个 $K=4$ 的情况，它们具有相同的真实类别 $y = 1$ 和 $\\gamma = 2.0$。\n  - 简单样本：$\\mathbf{z}_{\\mathrm{easy}} = [10.0,\\,0.0,\\,0.0,\\,0.0]$。\n  - 困难样本：$\\mathbf{z}_{\\mathrm{hard}} = [-10.0,\\,0.0,\\,0.0,\\,0.0]$。\n  对每种情况，计算比率\n  $$\n  S \\equiv \\frac{\\left\\|\\nabla_{\\mathbf{z}} L_{\\mathrm{focal}}\\right\\|_2}{\\left\\|\\nabla_{\\mathbf{z}} L_{\\mathrm{CE}}\\right\\|_2},\n  $$\n  其中 $L_{\\mathrm{CE}}(\\mathbf{z};y) \\equiv -\\log p_y(\\mathbf{z})$ 且 $\\nabla_{\\mathbf{z}} L_{\\mathrm{CE}} = \\mathbf{p} - \\mathbf{e}_y$。如果困难样本的 $S$ 严格大于简单样本的 $S$，则输出一个布尔值 True。此外，以包含两个浮点数的列表形式输出这对聚焦权重 $\\left[(1-p_y(\\mathbf{z}_{\\mathrm{easy}}))^{\\gamma},\\,(1-p_y(\\mathbf{z}_{\\mathrm{hard}}))^{\\gamma}\\right]$。\n\n最终输出格式：\n- 您的程序必须生成单行输出，其中包含按以下顺序排列的结果，形式为方括号括起来的逗号分隔列表：\n  - 通用情况的最大绝对梯度差（浮点数），\n  - 边界情况下与交叉熵的最大绝对差（浮点数），\n  - 困难样本与简单样本的侧重对比（布尔值），\n  - 简单样本和困难样本的聚焦权重列表（按此顺序，包含两个浮点数的列表）。\n- 具体来说，该单行输出必须如下所示：\n$$\n[\\;r_{\\mathrm{gen}},\\;r_{\\mathrm{ce0}},\\;b_{\\mathrm{hardeasy}},\\;[w_{\\mathrm{easy}},w_{\\mathrm{hard}}]\\;].\n$$\n不需要用户输入，也不涉及任何物理单位。使用自然对数，并报告原始十进制值，不要使用百分号。",
            "solution": "用户提供了一个在深度学习领域中定义明确的问题，要求推导并实现 focal loss 函数的梯度。该问题科学严谨，内容自洽，且算法上已明确指定。我将着手解决此问题。\n\n### 第 1 部分：梯度推导\n\n主要任务是推导 focal loss 函数 $L(\\mathbf{z}; y, \\gamma)$ 相对于 logit 向量 $\\mathbf{z} \\in \\mathbb{R}^K$ 的梯度。focal loss 定义为：\n$$\nL(\\mathbf{z};y,\\gamma) \\equiv -\\left(1 - p_y(\\mathbf{z})\\right)^{\\gamma} \\,\\log\\!\\left(p_y(\\mathbf{z})\\right)\n$$\n其中 $p_y(\\mathbf{z})$ 是真实类别 $y$ 的 softmax 概率：\n$$\np_y(\\mathbf{z}) = \\frac{\\exp(z_y)}{\\sum_{j=1}^{K} \\exp(z_j)}\n$$\n我们希望计算每个分量 $i \\in \\{1, \\dots, K\\}$ 的偏导数 $\\frac{\\partial L}{\\partial z_i}$。我们将使用链式法则。损失 $L$ 是 $p_y$ 的函数，而 $p_y$ 又是所有 logit $z_j$ 的函数。\n$$\n\\frac{\\partial L}{\\partial z_i} = \\frac{\\mathrm{d}L}{\\mathrm{d}p_y} \\frac{\\partial p_y}{\\partial z_i}\n$$\n\n**步骤 1.1：损失函数对概率的导数, $\\frac{\\mathrm{d}L}{\\mathrm{d}p_y}$**\n\n我们对 $L = -(1-p_y)^\\gamma \\log(p_y)$ 应用乘法法则。设 $u(p_y) = -(1-p_y)^\\gamma$ 且 $v(p_y) = \\log(p_y)$。\n$$\n\\frac{\\mathrm{d}L}{\\mathrm{d}p_y} = \\frac{\\mathrm{d}u}{\\mathrm{d}p_y} v(p_y) + u(p_y) \\frac{\\mathrm{d}v}{\\mathrm{d}p_y}\n$$\n各分量的导数是：\n$$\n\\frac{\\mathrm{d}u}{\\mathrm{d}p_y} = \\frac{\\mathrm{d}}{\\mathrm{d}p_y} \\left[-(1-p_y)^\\gamma\\right] = - \\left[\\gamma(1-p_y)^{\\gamma-1}(-1)\\right] = \\gamma(1-p_y)^{\\gamma-1}\n$$\n$$\n\\frac{\\mathrm{d}v}{\\mathrm{d}p_y} = \\frac{\\mathrm{d}}{\\mathrm{d}p_y} \\left[\\log(p_y)\\right] = \\frac{1}{p_y}\n$$\n将这些代回，我们得到：\n$$\n\\frac{\\mathrm{d}L}{\\mathrm{d}p_y} = \\gamma(1-p_y)^{\\gamma-1} \\log(p_y) - (1-p_y)^\\gamma \\frac{1}{p_y}\n$$\n$$\n\\frac{\\mathrm{d}L}{\\mathrm{d}p_y} = \\gamma(1-p_y)^{\\gamma-1} \\log(p_y) - \\frac{(1-p_y)^\\gamma}{p_y}\n$$\n\n**步骤 1.2：Softmax 概率对 Logit 的导数, $\\frac{\\partial p_k}{\\partial z_i}$**\n\n按照要求，我们推导 softmax 函数的雅可比矩阵。设 $p_k(\\mathbf{z}) = \\frac{\\exp(z_k)}{\\sum_{j=1}^{K} \\exp(z_j)}$。令 $S = \\sum_{j=1}^{K} \\exp(z_j)$。\n\n情况 1：$i = k$。使用除法法则：\n$$\n\\frac{\\partial p_k}{\\partial z_k} = \\frac{\\left(\\frac{\\partial}{\\partial z_k}\\exp(z_k)\\right)S - \\exp(z_k)\\left(\\frac{\\partial S}{\\partial z_k}\\right)}{S^2} = \\frac{\\exp(z_k)S - \\exp(z_k)\\exp(z_k)}{S^2}\n$$\n$$\n= \\frac{\\exp(z_k)}{S} \\left(\\frac{S - \\exp(z_k)}{S}\\right) = p_k(1-p_k)\n$$\n\n情况 2：$i \\neq k$。使用除法法则：\n$$\n\\frac{\\partial p_k}{\\partial z_i} = \\frac{\\left(\\frac{\\partial}{\\partial z_i}\\exp(z_k)\\right)S - \\exp(z_k)\\left(\\frac{\\partial S}{\\partial z_i}\\right)}{S^2} = \\frac{0 \\cdot S - \\exp(z_k)\\exp(z_i)}{S^2}\n$$\n$$\n= -\\frac{\\exp(z_k)}{S}\\frac{\\exp(z_i)}{S} = -p_k p_i\n$$\n\n这两种情况可以使用克罗内克 δ 符号 $\\delta_{ik}$ 进行统一：\n$$\n\\frac{\\partial p_k}{\\partial z_i} = p_k(\\delta_{ik} - p_i)\n$$\n对于我们的损失函数，我们关心的是真实类别 $y$ 的概率，因此我们设 $k=y$：\n$$\n\\frac{\\partial p_y}{\\partial z_i} = p_y(\\delta_{iy} - p_i)\n$$\n\n**步骤 1.3：合并各项**\n\n现在我们将步骤 1.1 和 1.2 的结果代入链式法则表达式中：\n$$\n\\frac{\\partial L}{\\partial z_i} = \\left( \\gamma(1-p_y)^{\\gamma-1} \\log(p_y) - \\frac{(1-p_y)^\\gamma}{p_y} \\right) \\cdot \\left( p_y(\\delta_{iy} - p_i) \\right)\n$$\n将因子 $p_y$ 分配到第一个括号内，简化表达式：\n$$\n\\frac{\\partial L}{\\partial z_i} = \\left( \\gamma p_y (1-p_y)^{\\gamma-1} \\log(p_y) - (1-p_y)^\\gamma \\right) (\\delta_{iy} - p_i)\n$$\n这是梯度 $\\nabla_{\\mathbf{z}}L$ 的第 $i$ 个分量的最终表达式。\n\n**步骤 1.4：对 $\\gamma=0$ 进行验证**\n\n当 $\\gamma=0$ 时，focal loss 简化为标准交叉熵损失 $L = -\\log(p_y)$。让我们验证我们的梯度公式在这种特殊情况下的正确性。第一个括号中的项变为：\n$$\n\\left( 0 \\cdot p_y (1-p_y)^{-1} \\log(p_y) - (1-p_y)^0 \\right) = -1\n$$\n因此，梯度变为：\n$$\n\\frac{\\partial L}{\\partial z_i}\\bigg|_{\\gamma=0} = (-1)(\\delta_{iy} - p_i) = p_i - \\delta_{iy}\n$$\n以向量形式表示，$\\nabla_{\\mathbf{z}}L = \\mathbf{p} - \\mathbf{e}_y$，其中 $\\mathbf{e}_y$ 是真实类别 $y$ 的 one-hot 向量。这与已知的交叉熵损失梯度相匹配，从而证实了我们的推导。\n\n### 第 2 部分：实现与评估\n\n以下部分将在最终的 Python 代码中实现。\n\n**数值稳定的 Softmax**：为了避免大 logit 值的数值溢出，我们使用恒等式 $\\text{softmax}(\\mathbf{z}) = \\text{softmax}(\\mathbf{z} - c)$，其中 $c$ 为任意常数。通过选择 $c = \\max_j z_j$，我们确保指数函数的参数为非正数，从而防止溢出。\n\n**Focal Loss 和梯度函数**：我们将实现用于稳定 softmax、focal loss、其基于我们推导公式的解析梯度，以及使用中心差分的数值梯度检查器的函数。\n\n**测试套件执行**：代码将执行问题中指定的三个测试用例：\n1.  一个通用梯度检查，比较解析梯度和数值梯度。\n2.  一个 $\\gamma=0$ 的边界情况检查，将 focal loss 梯度与标准交叉熵梯度进行比较。\n3.  通过比较 focal 梯度范数与交叉熵梯度范数的比率，分析 focal loss 对困难样本与简单样本的侧重。将报告 $S_{\\text{hard}}  S_{\\text{easy}}$ 的布尔结果以及聚焦权重。\n\n实现将遵循指定的 Python 环境和输出格式。所有类别索引都将从问题中的 1-based 约定转换为 Python 的 0-based 约定。",
            "answer": "```python\nimport numpy as np\n\ndef stable_softmax(z: np.ndarray) - np.ndarray:\n    \"\"\"\n    Computes numerically stable softmax probabilities.\n    \"\"\"\n    z_max = np.max(z)\n    exp_z = np.exp(z - z_max)\n    return exp_z / np.sum(exp_z)\n\ndef focal_loss(z: np.ndarray, y_1based: int, gamma: float) - float:\n    \"\"\"\n    Computes the focal loss.\n    y_1based is the 1-based true class index.\n    \"\"\"\n    y_idx = y_1based - 1  # Convert to 0-based index\n    p = stable_softmax(z)\n    p_y = p[y_idx]\n\n    # Clip p_y to avoid log(0) issues if it is exactly 0.\n    eps = 1e-12\n    p_y = np.clip(p_y, eps, 1.0 - eps)\n\n    loss = -(1 - p_y)**gamma * np.log(p_y)\n    return loss\n\ndef analytic_gradient(z: np.ndarray, y_1based: int, gamma: float) - np.ndarray:\n    \"\"\"\n    Computes the analytic gradient of the focal loss w.r.t. z.\n    y_1based is the 1-based true class index.\n    \"\"\"\n    y_idx = y_1based - 1\n    K = len(z)\n    p = stable_softmax(z)\n    p_y = p[y_idx]\n    \n    e_y = np.zeros(K)\n    e_y[y_idx] = 1.0\n\n    if gamma == 0.0:\n        return p - e_y\n\n    eps = 1e-12\n    p_y = np.clip(p_y, eps, 1.0 - eps)\n    \n    one_minus_py = 1.0 - p_y\n    log_py = np.log(p_y)\n\n    term = gamma * p_y * (one_minus_py**(gamma - 1.0)) * log_py - (one_minus_py**gamma)\n    \n    grad = term * (e_y - p)\n    # The problem definition gives dL/dz = (p-e_y) * (-term)\n    # This is equivalent to (e_y-p)*term. My derivation is correct.\n    # dL/dz_i = (d_{iy}-p_i) * term\n    \n    return grad\n\ndef numerical_gradient(z: np.ndarray, y_1based: int, gamma: float, h: float) - np.ndarray:\n    \"\"\"\n    Computes the numerical gradient of the focal loss using central differences.\n    \"\"\"\n    K = len(z)\n    grad_num = np.zeros(K)\n    \n    for i in range(K):\n        e_i = np.zeros(K)\n        e_i[i] = 1.0\n        \n        z_plus_h = z + h * e_i\n        z_minus_h = z - h * e_i\n        \n        loss_plus = focal_loss(z_plus_h, y_1based, gamma)\n        loss_minus = focal_loss(z_minus_h, y_1based, gamma)\n        \n        grad_num[i] = (loss_plus - loss_minus) / (2 * h)\n        \n    return grad_num\n\ndef solve():\n    \"\"\"\n    Executes all test cases and prints the final result.\n    \"\"\"\n    results = []\n\n    # Test 1: General-case gradient check\n    z1 = np.array([0.0, 1.0, -0.5, 2.0])\n    y1 = 4\n    gamma1 = 2.0\n    h1 = 1e-6\n    grad_ana1 = analytic_gradient(z1, y1, gamma1)\n    grad_num1 = numerical_gradient(z1, y1, gamma1, h=h1)\n    r_gen = np.max(np.abs(grad_ana1 - grad_num1))\n    results.append(r_gen)\n\n    # Test 2: Boundary-case focusing parameter (gamma=0, Cross-Entropy)\n    z2 = np.array([2.0, -1.0, 0.5])\n    y2 = 1\n    gamma2 = 0.0\n    grad_focal_gamma0 = analytic_gradient(z2, y2, gamma2)\n    p2 = stable_softmax(z2)\n    e_y2 = np.zeros_like(z2)\n    e_y2[y2 - 1] = 1.0\n    grad_ce2 = p2 - e_y2\n    r_ce0 = np.max(np.abs(grad_focal_gamma0 - grad_ce2))\n    results.append(r_ce0)\n\n    # Test 3: Hard-versus-easy emphasis\n    y3 = 1\n    gamma3 = 2.0\n    y3_idx = y3 - 1\n    z_easy = np.array([10.0, 0.0, 0.0, 0.0])\n    z_hard = np.array([-10.0, 0.0, 0.0, 0.0])\n    \n    # Easy example\n    p_easy = stable_softmax(z_easy)\n    py_easy = p_easy[y3_idx]\n    grad_focal_easy = analytic_gradient(z_easy, y3, gamma3)\n    e_y3 = np.zeros_like(z_easy)\n    e_y3[y3_idx] = 1.0\n    grad_ce_easy = p_easy - e_y3\n    S_easy = np.linalg.norm(grad_focal_easy, 2) / np.linalg.norm(grad_ce_easy, 2)\n    w_easy = (1 - py_easy)**gamma3\n\n    # Hard example\n    p_hard = stable_softmax(z_hard)\n    py_hard = p_hard[y3_idx]\n    grad_focal_hard = analytic_gradient(z_hard, y3, gamma3)\n    grad_ce_hard = p_hard - e_y3\n    S_hard = np.linalg.norm(grad_focal_hard, 2) / np.linalg.norm(grad_ce_hard, 2)\n    w_hard = (1 - py_hard)**gamma3\n\n    b_hard_gt_easy = S_hard > S_easy\n    weights = [w_easy, w_hard]\n    \n    results.append(b_hard_gt_easy)\n    results.append(weights)\n    \n    # Format the final output string exactly as requested.\n    # The default string representation of list and bool in Python matches the required format.\n    print(f\"[{results[0]},{results[1]},{str(results[2]).lower()},{results[3]}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个分类准确率高的模型，其输出的概率值就一定可信吗？答案是否定的，许多现代神经网络都存在“过度自信”的问题。这个练习介绍了一种重要的后处理技术——模型校准，通过在推理阶段对模型的对数几率（logits）进行仿射变换来提升其概率输出的可靠性。你将学习如何为这个校准过程推导优化目标并用代码实现，这是一个将模型从“能分类”提升到“可信赖”的关键步骤。",
            "id": "3193227",
            "problem": "考虑一个具有 $K$ 个类别的多类别分类问题。在推理时，你通过一个仿射变换 $z' = a z + b$ 对预训练的 logits 进行后处理校准，其中 $z \\in \\mathbb{R}^K$ 是一个样本的原始 logits，$a \\in \\mathbb{R}$ 是一个跨类别共享的标量，$b \\in \\mathbb{R}^K$ 是一个逐类的偏移向量。预测的类别分布是通过对变换后的 logits 应用 softmax 函数得到的。你需要通过在给定的验证集上最小化负对数似然（NLL）来推导最优参数 $(a,b)$。\n\n仅使用以下基本依据：\n- softmax 函数的定义：对于 $s \\in \\mathbb{R}^K$，$\\operatorname{softmax}(s)_c = \\dfrac{\\exp(s_c)}{\\sum_{k=1}^K \\exp(s_k)}$，其中 $c \\in \\{1,\\dots,K\\}$。\n- 在分类模型下，独立样本的负对数似然（NLL）的定义：如果样本 $i$ 的真实类别为 $y_i \\in \\{0,1,\\dots,K-1\\}$，预测概率为 $p_i \\in \\Delta^{K-1}$（概率单纯形），那么它对 NLL 的贡献是 $-\\log(p_{i,y_i})$，并且数据集的 NLL 是所有样本的总和。\n- 样本的独立性和微分的链式法则。\n\n你的任务是：\n- 用 $(a,b)$、原始 logits $z$ 和 softmax 函数来形式化 NLL。\n- 通过计算关于 $a$ 和 $b$ 的梯度来推导必要的最优性条件。\n- 针对下面的每个测试用例，实现一个数值稳定的程序来找到最小化 NLL 的 $(a,b)$。你必须通过使用 log-sum-exp 和 softmax 的稳定计算来解决 softmax 的数值稳定性问题，尤其是在 logits 数量级很大时。\n- 就本问题而言，将 $b$ 视为一个 $K$ 维向量（每个类别一个偏移量）。请注意，给 $b$ 的所有分量加上同一个标量不会改变 softmax 的输出；尽管如此，你的方法应返回一个由一致的优化程序产生的最小化器。\n\n此问题不涉及物理单位。角度不适用。百分比（如果概念上出现）应表示为小数。\n\n测试套件：\n- 测试用例 1（正常路径）：$K=3$，$N=6$。原始 logits 和标签：\n  - $z_1 = (2.0, 0.5, -1.0)$, $y_1 = 0$。\n  - $z_2 = (0.0, 1.0, 0.0)$, $y_2 = 1$。\n  - $z_3 = (1.5, -0.5, 0.0)$, $y_3 = 0$。\n  - $z_4 = (0.2, 0.0, 2.0)$, $y_4 = 2$。\n  - $z_5 = (-1.0, 0.0, 1.0)$, $y_5 = 2$。\n  - $z_6 = (3.0, 1.0, 0.0)$, $y_6 = 0$。\n- 测试用例 2（零 logits 的边界情况）：$K=4$，$N=5$。原始 logits 和标签：\n  - $z_1 = (0.0, 0.0, 0.0, 0.0)$, $y_1 = 0$。\n  - $z_2 = (0.0, 0.0, 0.0, 0.0)$, $y_2 = 1$。\n  - $z_3 = (0.0, 0.0, 0.0, 0.0)$, $y_3 = 2$。\n  - $z_4 = (0.0, 0.0, 0.0, 0.0)$, $y_4 = 3$。\n  - $z_5 = (0.0, 0.0, 0.0, 0.0)$, $y_5 = 1$。\n- 测试用例 3（用于数值稳定性的高数量级 logits）：$K=3$，$N=4$。原始 logits 和标签：\n  - $z_1 = (1000.0, 0.0, -1000.0)$, $y_1 = 0$。\n  - $z_2 = (1200.0, 1100.0, 1000.0)$, $y_2 = 0$。\n  - $z_3 = (-800.0, -900.0, -850.0)$, $y_3 = 2$。\n  - $z_4 = (0.0, 0.0, 0.0)$, $y_4 = 1$。\n\n最终输出规格：\n- 你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个元素对应一个测试用例，并且本身是一个由优化参数组成的、由方括号括起来的逗号分隔列表。列表中首先是标量 $a$，然后是按类别索引升序排列的 $b$ 的分量。\n- 所有数值必须四舍五入到六位小数。\n- 例如，输出格式为 $[[a_1,b_{1,0},\\dots],[a_2,b_{2,0},\\dots],[a_3,b_{3,0},\\dots]]$，其中每个 $a_j$ 和 $b_{j,c}$ 都以六位小数表示。",
            "solution": "用户希望找到仿射校准变换 $z' = az + b$ 的最优参数 $(a, b)$，应用于 logits $z \\in \\mathbb{R}^K$。参数 $a \\in \\mathbb{R}$ 和 $b \\in \\mathbb{R}^K$ 是通过在验证数据集上最小化负对数似然（NLL）来确定的。这需要推导 NLL 及其关于 $a$ 和 $b$ 的梯度，然后使用数值优化算法。\n\n### 步骤 1：形式化负对数似然（NLL）\n\n设验证集为 $\\{(z_i, y_i)\\}_{i=1}^N$，其中 $z_i \\in \\mathbb{R}^K$ 是样本 $i$ 的输入 logits，$y_i \\in \\{0, 1, \\dots, K-1\\}$ 是真实类别索引。\n\n样本 $i$ 的变换后 logits 由仿射变换给出：\n$$ z'_i = a z_i + b $$\n对于每个类别 $c \\in \\{0, \\dots, K-1\\}$，其分量形式为 $z'_{i,c} = a z_{i,c} + b_c$。\n\n类别 $c$ 的预测概率是通过对变换后的 logits 应用 softmax 函数得到的：\n$$ p_{i,c} = \\operatorname{softmax}(z'_i)_c = \\frac{\\exp(z'_{i,c})}{\\sum_{k=0}^{K-1} \\exp(z'_{i,k})} = \\frac{\\exp(a z_{i,c} + b_c)}{\\sum_{k=0}^{K-1} \\exp(a z_{i,k} + b_k)} $$\n\n对于真实类别为 $y_i$ 的单个样本 $i$，其负对数似然定义为 $\\mathcal{L}_i(a,b) = -\\log(p_{i,y_i})$。代入概率的表达式，我们得到：\n$$ \\mathcal{L}_i(a, b) = -\\log\\left(\\frac{\\exp(a z_{i,y_i} + b_{y_i})}{\\sum_{k=0}^{K-1} \\exp(a z_{i,k} + b_k)}\\right) $$\n使用属性 $\\log(X/Y) = \\log(X) - \\log(Y)$，这可以简化为：\n$$ \\mathcal{L}_i(a, b) = -\\left(a z_{i,y_i} + b_{y_i}\\right) + \\log\\left(\\sum_{k=0}^{K-1} \\exp(a z_{i,k} + b_k)\\right) $$\n$\\log(\\sum_k \\exp(\\cdot))$ 项是 log-sum-exp 函数，通常表示为 $\\operatorname{LSE}(\\cdot)$。\n\n整个数据集的总 NLL 是每个独立样本的 NLL 之和：\n$$ \\mathcal{L}(a, b) = \\sum_{i=1}^N \\mathcal{L}_i(a, b) = \\sum_{i=1}^N \\left[ \\log\\left(\\sum_{k=0}^{K-1} \\exp(a z_{i,k} + b_k)\\right) - (a z_{i,y_i} + b_{y_i}) \\right] $$\n\n### 步骤 2：推导最优性条件（梯度）\n\n为了找到最小化 $\\mathcal{L}(a,b)$ 的最优参数 $(a,b)$，我们必须找到梯度为零的点。我们计算 $\\mathcal{L}$ 关于 $a$ 和向量 $b$ 的每个分量 $b_j$ 的偏导数。\n\n**关于 $b_j$ 的梯度：**\n我们对 $\\mathcal{L}$ 关于 $b_j$ 求导，其中 $j \\in \\{0, \\dots, K-1\\}$。和的导数是导数的和，所以我们可以关注单个样本的贡献 $\\mathcal{L}_i$：\n$$ \\frac{\\partial \\mathcal{L}_i}{\\partial b_j} = \\frac{\\partial}{\\partial b_j}\\left( \\log\\left(\\sum_{k=0}^{K-1} \\exp(z'_{i,k})\\right) \\right) - \\frac{\\partial}{\\partial b_j}(a z_{i,y_i} + b_{y_i}) $$\n使用链式法则，第一项是：\n$$ \\frac{1}{\\sum_{k=0}^{K-1} \\exp(z'_{i,k})} \\cdot \\exp(z'_{i,j}) = p_{i,j} $$\n如果 $j=y_i$，第二项为 1，否则为 0。这可以用克罗内克 delta 符号 $\\delta_{j,y_i}$ 来表示。\n$$ \\frac{\\partial \\mathcal{L}_i}{\\partial b_j} = p_{i,j} - \\delta_{j,y_i} $$\n关于 $b_j$ 的总梯度是所有样本的总和：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_j} = \\sum_{i=1}^N (p_{i,j} - \\delta_{j,y_i}) $$\n\n**关于 $a$ 的梯度：**\n类似地，我们对 $\\mathcal{L}_i$ 关于 $a$ 求导：\n$$ \\frac{\\partial \\mathcal{L}_i}{\\partial a} = \\frac{\\partial}{\\partial a}\\left( \\log\\left(\\sum_{k=0}^{K-1} \\exp(z'_{i,k})\\right) \\right) - \\frac{\\partial}{\\partial a}(a z_{i,y_i} + b_{y_i}) $$\n使用链式法则，第一项得到：\n$$ \\frac{1}{\\sum_{k=0}^{K-1} \\exp(z'_{i,k})} \\cdot \\sum_{k=0}^{K-1} \\left( \\exp(z'_{i,k}) \\cdot \\frac{\\partial z'_{i,k}}{\\partial a} \\right) = \\sum_{k=0}^{K-1} p_{i,k} \\cdot z_{i,k} $$\n第二项就是 $z_{i,y_i}$。所以，对于一个样本：\n$$ \\frac{\\partial \\mathcal{L}_i}{\\partial a} = \\sum_{k=0}^{K-1} p_{i,k} z_{i,k} - z_{i,y_i} $$\n这可以更紧凑地写为 $\\sum_{k=0}^{K-1} (p_{i,k} - \\delta_{k,y_i}) z_{i,k}$。\n关于 $a$ 的总梯度是所有样本的总和：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial a} = \\sum_{i=1}^N \\left( \\sum_{k=0}^{K-1} p_{i,k} z_{i,k} - z_{i,y_i} \\right) = \\sum_{i=1}^N \\sum_{k=0}^{K-1} (p_{i,k} - \\delta_{k,y_i}) z_{i,k} $$\n\n### 步骤 3：数值稳定的实现\n\nNLL 是 $(a,b)$ 的凸函数，因此存在全局最小值。我们可以使用像 L-BFGS-B 这样高效且鲁棒的基于梯度的优化算法来找到它。该算法在每次迭代中都需要目标函数值及其梯度。\n\n实现的一个关键方面是数值稳定性，尤其是在处理对数量级较大的 logits 使用 `exp` 函数时。\n为了计算 $\\operatorname{LSE}(s) = \\log(\\sum_k \\exp(s_k))$ 和 $\\operatorname{softmax}(s)$，我们使用恒等式：\n$$ \\operatorname{LSE}(s) = c + \\log\\left(\\sum_k \\exp(s_k - c)\\right), \\quad \\text{其中 } c = \\max_k(s_k) $$\n这通过确保 `exp` 的最大参数为 0 来防止数值溢出。同样的移位也用于 softmax 计算。\n\n优化参数是标量 $a$ 和向量 $b$，形成一个 $(K+1)$ 维向量。我们用一个合理的猜测进行初始化，例如 $a=1$ 和 $b=\\mathbf{0}$，这对应于未校准的模型。然后，优化算法使用计算出的梯度迭代更新 $(a,b)$，直到收敛。\n\n注意，softmax 函数对其输入的共同移位是不变的，即 $\\operatorname{softmax}(s) = \\operatorname{softmax}(s+C)$，其中 $C$ 是任意标量。这意味着如果 $(a, b)$ 是一个最小化器，那么 $(a, b+C\\mathbf{1})$ 也是，其中 $\\mathbf{1}$ 是一个全为 1 的向量。L-BFGS-B 算法将在这个解流形上收敛到一个单一、一致的最小化器。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves for the optimal affine calibration parameters (a, b) for multiple test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"logits\": [\n                (2.0, 0.5, -1.0), (0.0, 1.0, 0.0), (1.5, -0.5, 0.0),\n                (0.2, 0.0, 2.0), (-1.0, 0.0, 1.0), (3.0, 1.0, 0.0)\n            ],\n            \"labels\": [0, 1, 0, 2, 2, 0],\n        },\n        {\n            \"logits\": [\n                (0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0),\n                (0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0)\n            ],\n            \"labels\": [0, 1, 2, 3, 1],\n        },\n        {\n            \"logits\": [\n                (1000.0, 0.0, -1000.0), (1200.0, 1100.0, 1000.0),\n                (-800.0, -900.0, -850.0), (0.0, 0.0, 0.0)\n            ],\n            \"labels\": [0, 0, 2, 1],\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        logits = np.array(case[\"logits\"], dtype=np.float64)\n        labels = np.array(case[\"labels\"], dtype=np.int64)\n        \n        N, K = logits.shape\n\n        def objective_and_grad(params, Z, Y):\n            \"\"\"\n            Computes the Negative Log-Likelihood (NLL) and its gradient \n            with respect to the calibration parameters (a, b).\n            This function is designed to be used with scipy.optimize.minimize.\n\n            Args:\n                params (np.ndarray): A 1D array of length K+1, where params[0] is 'a'\n                                     and params[1:] is the vector 'b'.\n                Z (np.ndarray): The (N, K) matrix of original logits.\n                Y (np.ndarray): The (N,) array of true class labels.\n\n            Returns:\n                tuple: A tuple containing the NLL (float) and its gradient (np.ndarray).\n            \"\"\"\n            a = params[0]\n            b = params[1:]\n            \n            # shape (N, K): Calculate transformed logits using broadcasting\n            Z_prime = a * Z + b \n\n            # Numerically stable computation of log-sum-exp and softmax\n            max_logits = np.max(Z_prime, axis=1, keepdims=True)\n            exp_Z_prime = np.exp(Z_prime - max_logits)\n            sum_exp_Z_prime = np.sum(exp_Z_prime, axis=1, keepdims=True)\n            \n            # Probabilities P has shape (N, K)\n            P = exp_Z_prime / sum_exp_Z_prime\n\n            # Calculate the NLL objective function\n            # log(sum(exp(z'))) = max_logit + log(sum(exp(z' - max_logit)))\n            log_sum_exp = max_logits.flatten() + np.log(sum_exp_Z_prime.flatten())\n            # NLL contribution from each sample is log_sum_exp - z'_{true_class}\n            true_class_logits = Z_prime[np.arange(N), Y]\n            nll = np.sum(log_sum_exp - true_class_logits)\n\n            # Calculate gradients\n            # The core of the gradient is the difference between predicted probabilities\n            # and the one-hot encoded true labels.\n            diff = P.copy()\n            diff[np.arange(N), Y] -= 1.0\n\n            # Gradient w.r.t. 'a' is sum over all samples and classes of diff * Z\n            grad_a = np.sum(diff * Z)\n\n            # Gradient w.r.t. 'b' is sum of diffs over samples for each class\n            grad_b = np.sum(diff, axis=0)\n            \n            grad = np.concatenate(([grad_a], grad_b))\n            \n            return nll, grad\n\n        # Initial guess: a=1 (no change in scale), b=0 (no bias)\n        x0 = np.zeros(K + 1, dtype=np.float64)\n        x0[0] = 1.0\n\n        # Run the L-BFGS-B optimizer. jac=True signifies that our function\n        # returns both the objective value and the gradient vector.\n        res = minimize(objective_and_grad, \n                       x0, \n                       args=(logits, labels), \n                       method='L-BFGS-B', \n                       jac=True)\n        \n        optimal_params = res.x\n        formatted_params = [f\"{p:.6f}\" for p in optimal_params]\n        all_results.append(f\"[{','.join(formatted_params)}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}