{
    "hands_on_practices": [
        {
            "introduction": "深入理解深度学习的一个关键在于如何控制激活值在网络层间传播时的分布。本练习  提供了一个基础的理论实践。我们将从数学上推导当输入服从正态分布时，ReLU、ELU 和 SELU 激活函数输出的均值和方差，从而揭示为何某些激活函数可能导致信号消失或爆炸，而像 SELU 这样的激活函数则被设计成具有“自归一化”特性。这个练习将为你理解现代权重初始化策略背后的数学直觉奠定基础。",
            "id": "3197614",
            "problem": "设标量预激活值 $x$ 服从正态分布 $x \\sim \\mathcal{N}(0,\\sigma^{2})$。考虑深度学习中常用的三种激活函数：修正线性单元（ReLU）、指数线性单元（ELU）和缩放指数线性单元（SELU），其定义如下：\n$$\n\\mathrm{ReLU}(x) = \\max\\{0,x\\},\n$$\n$$\n\\mathrm{ELU}(x) = \n\\begin{cases}\nx,  x  0,\\\\\n\\alpha\\left(\\exp(x) - 1\\right),  x \\leq 0,\n\\end{cases}\n\\quad \\text{其中 } \\alpha  0,\n$$\n$$\n\\mathrm{SELU}(x) = \\lambda \\cdot \\mathrm{ELU}(x) \\quad \\text{其中 } \\lambda  0 \\text{ 且 } \\alpha  0.\n$$\n仅从实值随机变量的期望和方差的定义以及正态分布的概率密度函数出发，推导均值 $\\mathbb{E}[\\mathrm{ReLU}(x)]$ 和方差 $\\mathrm{Var}[\\mathrm{ReLU}(x)]$ 的闭式表达式（用 $\\sigma$、$\\alpha$、$\\lambda$ 以及标准函数，如标准正态累积分布函数表示）。然后，推导 $\\mathbb{E}[\\mathrm{ELU}(x)]$ 和 $\\mathrm{Var}[\\mathrm{ELU}(x)]$ 以及 $\\mathbb{E}[\\mathrm{SELU}(x)]$ 和 $\\mathrm{Var}[\\mathrm{SELU}(x)]$ 的相应公式。\n\n利用这些表达式，并且除了核心定义之外不使用任何快捷公式，解释每种激活后的输出均值和方差如何依赖于输入标准差 $\\sigma$ 和激活参数 $(\\alpha,\\lambda)$。讨论这对于旨在保持跨层零均值和稳定方差的权重初始化策略的影响，包括在给定 $\\mathbb{E}[x]=0$ 的情况下，如何选择 $(\\alpha,\\lambda)$ 以使得 $\\mathbb{E}[\\mathrm{SELU}(x)]=0$ 和 $\\mathrm{Var}[\\mathrm{SELU}(x)]=1$。\n\n你的最终答案必须是一个单一的闭式解析表达式，将推导出的六个量收集在一个单行矩阵中。如果你选择引入任何特殊函数（例如，标准正态累积分布函数），请在推导过程中明确定义它们。不需要进行数值近似。",
            "solution": "该问题要求当输入 $x$ 是一个均值为 $0$、方差为 $\\sigma^2$ 的正态分布随机变量时，推导三种激活函数——ReLU、ELU 和 SELU——输出的均值和方差。推导必须从第一性原理出发。\n\n设输入预激活值为随机变量 $x \\sim \\mathcal{N}(0, \\sigma^2)$。其概率密度函数（PDF）由下式给出\n$$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$$\n对于连续随机变量 $x$ 的函数 $g(x)$，其期望 $\\mathbb{E}[g(x)]$ 和方差 $\\mathrm{Var}[g(x)]$ 的基本定义为：\n$$\\mathbb{E}[g(x)] = \\int_{-\\infty}^{\\infty} g(x) p(x) dx$$\n$$\\mathrm{Var}[g(x)] = \\mathbb{E}\\left[(g(x) - \\mathbb{E}[g(x)])^2\\right] = \\mathbb{E}[g(x)^2] - (\\mathbb{E}[g(x)])^2$$\n我们引入标准正态随机变量 $z \\sim \\mathcal{N}(0,1)$，其概率密度函数（PDF）为 $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$，其累积分布函数（CDF）为 $\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) dt$。输入 $x$ 可以写成 $x = \\sigma z$。\n\n我们将频繁使用以下从定义中推导出的定积分：\n1. $\\int_0^\\infty x p(x) dx$：该积分表示 $x$ 在正实数轴上的期望。\n$$\\int_0^\\infty x \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) dx = \\frac{\\sigma}{\\sqrt{2\\pi}} \\int_0^\\infty z \\exp\\left(-\\frac{z^2}{2}\\right) dz = \\frac{\\sigma}{\\sqrt{2\\pi}} \\left[-\\exp\\left(-\\frac{z^2}{2}\\right)\\right]_0^\\infty = \\frac{\\sigma}{\\sqrt{2\\pi}}$$\n2. $\\int_0^\\infty x^2 p(x) dx$：$x$ 的总方差为 $\\mathrm{Var}[x] = \\mathbb{E}[x^2] = \\sigma^2$。由于正态分布关于 $0$ 的对称性，在正半轴上的积分是总积分的一半。\n$$\\int_0^\\infty x^2 p(x) dx = \\frac{1}{2}\\int_{-\\infty}^\\infty x^2 p(x) dx = \\frac{\\sigma^2}{2}$$\n3. $\\int_{-\\infty}^0 \\exp(ax) p(x) dx$：该积分可以通过在指数上配方来求解。\n$$\\int_{-\\infty}^0 \\exp(ax) p(x) dx = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\int_{-\\infty}^0 \\exp\\left(ax - \\frac{x^2}{2\\sigma^2}\\right) dx = \\exp\\left(\\frac{a^2\\sigma^2}{2}\\right) \\Phi(-a\\sigma)$$\n4. $\\int_{-\\infty}^0 p(x) dx = P(x \\le 0) = \\frac{1}{2}$，由对称性可知。\n\n利用这些工具，我们为每种激活函数推导所需的量。\n\n**1. 修正线性单元 (ReLU)**\nReLU 函数定义为 $\\mathrm{ReLU}(x) = \\max\\{0, x\\}$。\n\n均值 $\\mathbb{E}[\\mathrm{ReLU}(x)]$：\n$$\\mathbb{E}[\\mathrm{ReLU}(x)] = \\int_{-\\infty}^{\\infty} \\max\\{0, x\\} p(x) dx = \\int_{-\\infty}^0 0 \\cdot p(x) dx + \\int_0^\\infty x p(x) dx$$\n使用积分 1，我们得到：\n$$\\mathbb{E}[\\mathrm{ReLU}(x)] = \\frac{\\sigma}{\\sqrt{2\\pi}}$$\n\n方差 $\\mathrm{Var}[\\mathrm{ReLU}(x)]$：\n首先，我们求二阶矩 $\\mathbb{E}[\\mathrm{ReLU}(x)^2]$。\n$$\\mathbb{E}[\\mathrm{ReLU}(x)^2] = \\int_{-\\infty}^{\\infty} (\\max\\{0, x\\})^2 p(x) dx = \\int_{-\\infty}^0 0^2 \\cdot p(x) dx + \\int_0^\\infty x^2 p(x) dx$$\n使用积分 2，我们得到：\n$$\\mathbb{E}[\\mathrm{ReLU}(x)^2] = \\frac{\\sigma^2}{2}$$\n现在，我们可以计算方差：\n$$\\mathrm{Var}[\\mathrm{ReLU}(x)] = \\mathbb{E}[\\mathrm{ReLU}(x)^2] - (\\mathbb{E}[\\mathrm{ReLU}(x)])^2 = \\frac{\\sigma^2}{2} - \\left(\\frac{\\sigma}{\\sqrt{2\\pi}}\\right)^2 = \\sigma^2\\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right)$$\n\n**2. 指数线性单元 (ELU)**\nELU 函数定义为：当 $x  0$ 时，$\\mathrm{ELU}(x) = x$；当 $x \\leq 0$ 时，$\\mathrm{ELU}(x) = \\alpha(\\exp(x) - 1)$，其中 $\\alpha0$。\n\n均值 $\\mathbb{E}[\\mathrm{ELU}(x)]$：\n$$\\mathbb{E}[\\mathrm{ELU}(x)] = \\int_0^\\infty x p(x) dx + \\int_{-\\infty}^0 \\alpha(\\exp(x) - 1) p(x) dx$$\n第一项是 $\\frac{\\sigma}{\\sqrt{2\\pi}}$。第二项是：\n$$\\alpha \\int_{-\\infty}^0 (\\exp(x) - 1) p(x) dx = \\alpha \\left( \\int_{-\\infty}^0 \\exp(x) p(x) dx - \\int_{-\\infty}^0 p(x) dx \\right)$$\n使用积分 3（$a=1$）和积分 4：\n$$\\alpha \\left( \\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2} \\right)$$\n合并各项得到均值：\n$$\\mathbb{E}[\\mathrm{ELU}(x)] = \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right)$$\n\n方差 $\\mathrm{Var}[\\mathrm{ELU}(x)]$：\n首先，我们求二阶矩 $\\mathbb{E}[\\mathrm{ELU}(x)^2]$。\n$$\\mathbb{E}[\\mathrm{ELU}(x)^2] = \\int_0^\\infty x^2 p(x) dx + \\int_{-\\infty}^0 \\left(\\alpha(\\exp(x) - 1)\\right)^2 p(x) dx$$\n第一项是 $\\frac{\\sigma^2}{2}$。第二项是：\n$$\\alpha^2 \\int_{-\\infty}^0 (\\exp(2x) - 2\\exp(x) + 1) p(x) dx$$\n$$= \\alpha^2 \\left( \\int_{-\\infty}^0 \\exp(2x) p(x) dx - 2\\int_{-\\infty}^0 \\exp(x) p(x) dx + \\int_{-\\infty}^0 p(x) dx \\right)$$\n使用积分 3（$a=2$ 和 $a=1$）和积分 4：\n$$= \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right)$$\n合并各项得到二阶矩：\n$$\\mathbb{E}[\\mathrm{ELU}(x)^2] = \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right)$$\n然后方差为 $\\mathrm{Var}[\\mathrm{ELU}(x)] = \\mathbb{E}[\\mathrm{ELU}(x)^2] - (\\mathbb{E}[\\mathrm{ELU}(x)])^2$，即：\n$$\\mathrm{Var}[\\mathrm{ELU}(x)] = \\left[ \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\dots \\right) \\right] - \\left[ \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left( \\dots \\right) \\right]^2$$\n显式地：\n$$\\mathrm{Var}[\\mathrm{ELU}(x)] = \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) - \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right)^2$$\n\n**3. 缩放指数线性单元 (SELU)**\nSELU 函数定义为 $\\mathrm{SELU}(x) = \\lambda \\cdot \\mathrm{ELU}(x)$，其中 $\\lambda  0$ 且 $\\alpha  0$。其均值和方差可以利用期望和方差关于标量乘法的性质求得。\n\n均值 $\\mathbb{E}[\\mathrm{SELU}(x)]$：\n$$\\mathbb{E}[\\mathrm{SELU}(x)] = \\mathbb{E}[\\lambda \\cdot \\mathrm{ELU}(x)] = \\lambda \\mathbb{E}[\\mathrm{ELU}(x)]$$\n$$\\mathbb{E}[\\mathrm{SELU}(x)] = \\lambda \\left[ \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right]$$\n\n方差 $\\mathrm{Var}[\\mathrm{SELU}(x)]$：\n$$\\mathrm{Var}[\\mathrm{SELU}(x)] = \\mathrm{Var}[\\lambda \\cdot \\mathrm{ELU}(x)] = \\lambda^2 \\mathrm{Var}[\\mathrm{ELU}(x)]$$\n$$\\mathrm{Var}[\\mathrm{SELU}(x)] = \\lambda^2 \\left[ \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) - \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right)^2 \\right]$$\n\n**讨论与启示**\n\n推导出的表达式揭示了输出分布的统计特性如何依赖于输入标准差 $\\sigma$ 和激活参数 $(\\alpha, \\lambda)$。\n\n对于 ReLU，输出均值 $\\mathbb{E}[\\mathrm{ReLU}(x)] = \\sigma/\\sqrt{2\\pi}$ 是严格为正的，并随 $\\sigma$ 缩放。这会引入一个偏离零的“偏置漂移”，可能使优化复杂化。输出方差为 $\\mathrm{Var}[\\mathrm{ReLU}(x)] \\approx 0.34 \\sigma^2$。如果不对这种方差的减小进行补偿，可能会导致深度网络中的梯度消失问题。像“He 初始化”这样的权重初始化策略旨在通过缩放权重来恢复预激活值的方差，以抵消这种影响。\n\n对于 ELU 和 SELU，参数 $\\alpha$ 和 $\\lambda$ 提供了更多的控制能力。SELU 的目标是实现自归一化网络，其中每层的输出会自动收敛到一个具有固定均值和方差的分布，通常是均值为 $0$、方差为 $1$。\n为了对于一个均值为 $0$、方差为 $\\sigma^2$ 的输入，实现 $\\mathbb{E}[\\mathrm{SELU}(x)]=0$ 和 $\\mathrm{Var}[\\mathrm{SELU}(x)]=1$，我们必须为 $\\alpha$ 和 $\\lambda$ 求解以下方程组：\n1. $\\mathbb{E}[\\mathrm{SELU}(x)] = 0$：由于 $\\lambda  0$，这要求 $\\mathbb{E}[\\mathrm{ELU}(x)] = 0$。\n$$\\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) = 0$$\n该方程建立了 $\\alpha$ 与输入方差 $\\sigma^2$ 之间的关系。\n\n2. $\\mathrm{Var}[\\mathrm{SELU}(x)] = 1$：这意味着 $\\lambda^2 \\mathrm{Var}[\\mathrm{ELU}(x)] = 1$。由于我们已经设定 $\\mathbb{E}[\\mathrm{ELU}(x)]=0$，方差简化为二阶矩，即 $\\mathrm{Var}[\\mathrm{ELU}(x)] = \\mathbb{E}[\\mathrm{ELU}(x)^2]$。\n$$\\lambda^2 \\left[ \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) \\right] = 1$$\n\n在自归一化网络的背景下，通常假设层的输入已被归一化为方差 $\\sigma^2=1$。通过求解这两个关于 $\\sigma=1$ 的方程，可以获得 $\\alpha$ 和 $\\lambda$ 的特定数值（约 $\\alpha \\approx 1.6733$ 和 $\\lambda \\approx 1.0507$）。这些特定值定义了标准的 SELU 激活函数，当与特定的权重初始化（“LeCun 正态”）配合使用时，能促进整个网络中神经元激活值收敛到标准正态分布，从而缓解梯度消失/爆炸问题并对训练过程进行正则化。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{\\sigma}{\\sqrt{2\\pi}}  \\sigma^2\\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right)  \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right)  \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) - \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right)^2  \\lambda \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right)  \\lambda^2 \\left( \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) - \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right)^2 \\right) \\end{pmatrix}}$$"
        },
        {
            "introduction": "从理论走向实践，下一个练习  将具体展示“归纳偏置”（inductive bias）的概念。我们将构建一个模拟任务，其数据的真实生成过程使用了 Leaky ReLU。通过训练两个模型——一个使用与之匹配的 Leaky ReLU，另一个使用标准的 ReLU——我们可以直接观察到，拥有正确架构假设的模型能取得显著更优的性能。这个动手编码任务将使选择合适激活函数的抽象优势变得具体可感。",
            "id": "3197626",
            "problem": "考虑一个双特征输入向量 $\\mathbf{x} = \\begin{bmatrix} x_{+} \\\\ x_{-} \\end{bmatrix} \\in \\mathbb{R}^{2}$，其中 $x_{+}$ 代表某个潜在原因的正向证据，$x_{-}$ 代表负向证据（抑制性）。潜在线性得分定义为 $a^{\\star}(\\mathbf{x}) = x_{+} - x_{-}$。为了体现负向证据应部分抑制激活但非完全消除的原则，目标映射通过作用于 $a^{\\star}$ 的带泄露修正线性单元（Leaky Rectified Linear Unit, Leaky ReLU）激活函数定义，其斜率参数为 $\\alpha \\in [0,1]$：\n$$\ny(\\mathbf{x}; \\alpha) = \\phi_{\\text{leaky}, \\alpha}\\!\\left(a^{\\star}(\\mathbf{x})\\right) \\quad \\text{with} \\quad \\phi_{\\text{leaky}, \\alpha}(a) = \\max(a, \\alpha a).\n$$\n作为对比，修正线性单元（Rectified Linear Unit, ReLU）定义为\n$$\n\\phi_{\\text{relu}}(a) = \\max(0,a).\n$$\n对于每个指定的 $\\alpha$，您将训练两个单神经元模型，使用以下架构将 $\\mathbf{x}$ 映射到一个标量预测值 $\\hat{y}$：\n$$\n\\hat{y}(\\mathbf{x}) = v \\cdot \\phi\\!\\left(\\mathbf{w}^{\\top}\\mathbf{x} + b\\right) + c,\n$$\n其中 $\\phi$ 为 $\\phi_{\\text{relu}}$ 或 $\\phi_{\\text{leaky}, \\alpha}$，$\\mathbf{w} \\in \\mathbb{R}^{2}$，$b \\in \\mathbb{R}$，$v \\in \\mathbb{R}$ 和 $c \\in \\mathbb{R}$ 是可训练参数。训练过程通过最小化均方误差（Mean Squared Error, MSE）来进行，对于一个数据集 $\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{N}$，其定义为：\n$$\n\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\hat{y}(\\mathbf{x}_{i}) - y_{i}\\right)^{2}.\n$$\n使用基于全批量梯度下降的经验风险最小化（Empirical Risk Minimization, ERM）来更新 $(\\mathbf{w}, b, v, c)$。\n\n数据集规范：\n- 使用种子为 0 初始化的伪随机数生成器，抽取 $N$ 个独立样本，其中 $N = 2000$。\n- 对每个样本，独立地从 $\\text{Uniform}([0,1])$ 分布中抽取 $x_{+}$ 和 $x_{-}$。\n- 对于给定的 $\\alpha$，计算 $a^{\\star} = x_{+} - x_{-}$ 和 $y = \\phi_{\\text{leaky}, \\alpha}(a^{\\star})$。\n\n训练规范：\n- 使用全批量梯度下降法，学习率为 $\\eta = 0.03$，步数为 $T = 800$。\n- 从正态分布 $\\mathcal{N}(0, \\sigma^{2})$（其中 $\\sigma = 0.1$）中初始化参数 $(\\mathbf{w}, b, v, c)$，每个测试用例使用固定的种子以确保可复现性和公平性：对于测试用例索引 $k$（从 0 开始），将初始化种子设置为 $100 + k$。\n- 确保在每个测试用例中，两个模型（$\\phi=\\phi_{\\text{relu}}$ 的模型和 $\\phi=\\phi_{\\text{leaky}, \\alpha}$ 的模型）都从完全相同的初始参数开始。\n\n测试套件：\n评估负斜率参数 $\\alpha$ 的以下值：\n- $\\alpha = 0.2$: 典型的部分抑制。\n- $\\alpha = 0.0$: 边界情况，此时 Leaky ReLU 退化为 ReLU。\n- $\\alpha = 0.8$: 强的部分抑制。\n- $\\alpha = 1.0$: 在 $a^{\\star}$ 上的恒等激活。\n\n对于每个 $\\alpha$，按照上述规范训练两个模型，并计算它们在训练数据上的 MSE。定义一个容差 $\\delta = 10^{-4}$，并为每个测试用例输出布尔结果\n$$\n\\text{result} = \\left(\\text{MSE}_{\\text{leaky}} + \\delta  \\text{MSE}_{\\text{relu}}\\right).\n$$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如 $[\\text{result}_{1},\\text{result}_{2},\\text{result}_{3},\\text{result}_{4}]$），布尔值的顺序与上面给出的 $\\alpha$ 的顺序一致。\n\n缩略语定义：\n- 修正线性单元 (ReLU): $\\phi_{\\text{relu}}(a) = \\max(0,a)$。\n- 带泄露修正线性单元 (Leaky ReLU): $\\phi_{\\text{leaky}, \\alpha}(a) = \\max(a, \\alpha a)$。\n- 均方误差 (MSE): $\\frac{1}{N}\\sum_{i=1}^{N}(\\hat{y}_{i}-y_{i})^{2}$。\n- 经验风险最小化 (ERM): 通过最小化数据集上的经验损失来估计参数。",
            "solution": "该问题要求训练两个单神经元模型来近似一个由带泄露修正线性单元（Leaky Rectified Linear Unit, Leaky ReLU）生成的目标函数。一个模型使用 Leaky ReLU 激活函数，与真实情况（ground truth）相匹配，而另一个模型使用标准的修正线性单元（Rectified Linear Unit, ReLU）。目标是通过全批量梯度下降法训练它们，并在训练数据上评估其最终的均方误差（Mean Squared Error, MSE），从而比较它们的性能。\n\n### 1. 模型与损失函数\n\n两个模型的架构都是一个单神经元网络，由下式给出：\n$$\n\\hat{y}(\\mathbf{x}) = v \\cdot \\phi(\\mathbf{w}^{\\top}\\mathbf{x} + b) + c\n$$\n其中 $\\mathbf{x} = \\begin{bmatrix} x_{+} \\\\ x_{-} \\end{bmatrix} \\in \\mathbb{R}^{2}$ 是输入向量，$\\mathbf{w} \\in \\mathbb{R}^{2}$，$b \\in \\mathbb{R}$，$v \\in \\mathbb{R}$ 和 $c \\in \\mathbb{R}$ 是可训练参数。函数 $\\phi$ 是激活函数。\n\n这两个模型通过它们对 $\\phi$ 的选择来区分：\n1.  **ReLU 模型**: $\\phi(\\cdot) = \\phi_{\\text{relu}}(a) = \\max(0, a)$。\n2.  **Leaky ReLU 模型**: $\\phi(\\cdot) = \\phi_{\\text{leaky}, \\alpha}(a) = \\max(a, \\alpha a)$，其中 $\\alpha$ 是给定的斜率参数。\n\n模型通过在包含 $N$ 个样本的数据集 $\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{N}$ 上最小化均方误差（MSE）损失函数来进行训练：\n$$\nJ(\\mathbf{w}, b, v, c) = \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\hat{y}(\\mathbf{x}_{i}) - y_{i}\\right)^{2}\n$$\n目标值 $y_i$ 是通过 $y_i = \\phi_{\\text{leaky}, \\alpha}(x_{i,+} - x_{i,-})$ 为一个特定的 $\\alpha$ 生成的。\n\n### 2. 批量梯度下降的梯度推导\n\n为了使用梯度下降法最小化 MSE，我们必须计算损失函数 $J$ 关于每个参数的偏导数。对于任意参数 $\\theta$ 的更新规则是 $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} J$，其中 $\\eta$ 是学习率。我们使用全批量梯度下降，因此梯度是在整个数据集上平均的。\n\n设 $a_i = \\mathbf{w}^{\\top}\\mathbf{x}_i + b$ 为样本 $i$ 的预激活值，$\\hat{y}_i = v \\cdot \\phi(a_i) + c$ 为预测值。单个样本的损失为 $L_i = (\\hat{y}_i - y_i)^2$。应用链式法则，我们求得总损失 $J = \\frac{1}{N} \\sum_{i=1}^{N} L_i$ 的梯度如下：\n\n-   **关于 $c$ 的梯度**：\n    $$\n    \\nabla_c J = \\frac{\\partial J}{\\partial c} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L_i}{\\partial c} = \\frac{1}{N} \\sum_{i=1}^{N} 2(\\hat{y}_i - y_i)\n    $$\n\n-   **关于 $v$ 的梯度**：\n    $$\n    \\nabla_v J = \\frac{\\partial J}{\\partial v} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L_i}{\\partial v} = \\frac{1}{N} \\sum_{i=1}^{N} 2(\\hat{y}_i - y_i) \\phi(a_i)\n    $$\n\n-   **关于 $b$ 的梯度**：\n    $$\n    \\nabla_b J = \\frac{\\partial J}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L_i}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^{N} 2(\\hat{y}_i - y_i) \\cdot v \\cdot \\phi'(a_i)\n    $$\n\n-   **关于 $\\mathbf{w}$ 的梯度**：\n    $$\n    \\nabla_{\\mathbf{w}} J = \\frac{\\partial J}{\\partial \\mathbf{w}} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L_i}{\\partial \\mathbf{w}} = \\frac{1}{N} \\sum_{i=1}^{N} 2(\\hat{y}_i - y_i) \\cdot v \\cdot \\phi'(a_i) \\cdot \\mathbf{x}_i\n    $$\n\n项 $\\phi'(a)$ 是激活函数的导数（或次梯度）。\n\n-   对于 **ReLU**: $\\phi_{\\text{relu}}(a) = \\max(0, a)$。其次梯度为：\n    $$\n    \\phi'_{\\text{relu}}(a) = \\begin{cases} 1  \\text{if } a  0 \\\\ 0  \\text{if } a \\le 0 \\end{cases}\n    $$\n-   对于 **Leaky ReLU**: $\\phi_{\\text{leaky}, \\alpha}(a) = \\max(a, \\alpha a)$。对于 $\\alpha \\in [0, 1]$，这可以写成当 $a \\ge 0$ 时为 $a$，当 $a  0$ 时为 $\\alpha a$。其次梯度为：\n    $$\n    \\phi'_{\\text{leaky}, \\alpha}(a) = \\begin{cases} 1  \\text{if } a \\ge 0 \\\\ \\alpha  \\text{if } a  0 \\end{cases}\n    $$\n这个通用形式正确地处理了 $\\alpha=0$（退化为 ReLU）和 $\\alpha=1$（退化为恒等函数 $a$）的边界情况。\n\n### 3. 模拟过程\n\n该问题要求一个计算实验，遵循以下规范：\n\n1.  **数据集生成**：在开始时生成一个由 $N = 2000$ 个样本组成的输入特征数据集 $\\mathbf{X}$。对每个样本，$x_+$ 和 $x_-$ 独立地从 $\\text{Uniform}([0,1])$ 分布中抽取。使用种子为 $0$ 的伪随机数生成器。\n2.  **测试用例**：实验针对斜率参数 $\\alpha$ 的四个值 $\\alpha \\in \\{0.2, 0.0, 0.8, 1.0\\}$ 运行。\n3.  **各用例设置**：对于每个 $\\alpha$ 值（由从 $0$ 开始的索引 $k$ 标记）：\n    a. 目标向量 $\\mathbf{y}$ 计算为 $y_i = \\phi_{\\text{leaky}, \\alpha}(x_{i,+} - x_{i,-})$。\n    b. 模型参数 $(\\mathbf{w}, b, v, c)$ 通过从正态分布 $\\mathcal{N}(0, \\sigma^2)$（$\\sigma=0.1$）中抽样来初始化，使用固定的种子 $100+k$。\n    c. ReLU 模型和 Leaky ReLU 模型都从这些相同的初始参数开始。\n4.  **训练**：两个模型都使用全批量梯度下降法进行 $T=800$ 步的训练，学习率为 $\\eta=0.03$。上面推导出的梯度用于参数更新。\n5.  **评估**：训练后，在整个训练数据集上为两个模型计算最终的 MSE，得到 $\\text{MSE}_{\\text{leaky}}$ 和 $\\text{MSE}_{\\text{relu}}$。\n6.  **比较**：布尔结果由条件 $\\text{MSE}_{\\text{leaky}} + \\delta  \\text{MSE}_{\\text{relu}}$ 确定，容差为 $\\delta = 10^{-4}$。\n\n这个过程系统地将一个架构与数据生成过程相匹配的模型与一个错误指定的模型进行比较，从而提供了对归纳偏置作用的洞察。当 $\\alpha=0.0$ 时，目标函数是一个 ReLU，使得两个模型在功能上是相同的；因此，它们的 MSE 预计会相等，从而得到 `False` 结果。对于其他的 $\\alpha$ 值，Leaky ReLU 模型具有正确的函数形式，预计会达到更低的 MSE，从而得到 `True`。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef leaky_relu(a, alpha):\n    \"\"\"\n    Computes the Leaky ReLU activation function.\n    Given alpha in [0,1], max(a, alpha*a) is equivalent to:\n    a if a = 0\n    alpha * a if a  0\n    \"\"\"\n    return np.maximum(a, alpha * a)\n\ndef leaky_relu_grad(a, alpha):\n    \"\"\"Computes the gradient of the Leaky ReLU function.\"\"\"\n    grad = np.ones_like(a, dtype=float)\n    grad[a  0] = alpha\n    return grad\n\ndef relu(a):\n    \"\"\"Computes the ReLU activation function.\"\"\"\n    return np.maximum(0, a)\n\ndef relu_grad(a):\n    \"\"\"Computes the gradient of the ReLU function.\"\"\"\n    grad = np.zeros_like(a, dtype=float)\n    grad[a > 0] = 1.0\n    return grad\n\ndef train_model(X, y, initial_params, activation_func, grad_func, T, eta):\n    \"\"\"\n    Trains a single-neuron model using full-batch gradient descent.\n    \"\"\"\n    w, b, v, c = initial_params\n    N = X.shape[0]\n\n    for _ in range(T):\n        # Forward pass\n        a = X @ w + b\n        h = activation_func(a)\n        y_hat = v * h + c\n\n        # Error term\n        e = y_hat - y\n\n        # Gradients\n        grad_c = (2 / N) * np.sum(e)\n        grad_v = (2 / N) * np.sum(e * h)\n        \n        # Common term for w and b gradients\n        g_a_common = (2 / N) * v * e\n        g_a = g_a_common * grad_func(a)\n\n        grad_b = np.sum(g_a)\n        grad_w = X.T @ g_a\n\n        # Update parameters\n        w -= eta * grad_w\n        b -= eta * grad_b\n        v -= eta * grad_v\n        c -= eta * grad_c\n\n    # Compute final MSE on the training data\n    a_final = X @ w + b\n    h_final = activation_func(a_final)\n    y_hat_final = v * h_final + c\n    final_mse = np.mean((y_hat_final - y)**2)\n\n    return final_mse\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and print the results.\n    \"\"\"\n    # Problem specifications\n    N = 2000\n    dataset_seed = 0\n    eta = 0.03\n    T = 800\n    sigma = 0.1\n    delta = 1e-4\n    alphas = [0.2, 0.0, 0.8, 1.0]\n\n    # Generate dataset features X (same for all test cases)\n    rng_dataset = np.random.RandomState(dataset_seed)\n    x_plus = rng_dataset.uniform(0, 1, N)\n    x_minus = rng_dataset.uniform(0, 1, N)\n    X = np.stack([x_plus, x_minus], axis=1)\n\n    results = []\n\n    for k, alpha in enumerate(alphas):\n        # Generate target vector y for the current alpha\n        a_star = x_plus - x_minus\n        y = leaky_relu(a_star, alpha)\n\n        # Generate initial parameters using the per-case seed\n        init_seed = 100 + k\n        rng_init = np.random.RandomState(init_seed)\n        w_init = rng_init.normal(0, sigma, size=2)\n        b_init = rng_init.normal(0, sigma)\n        v_init = rng_init.normal(0, sigma)\n        c_init = rng_init.normal(0, sigma)\n        \n        # Ensure both models start with the exact same parameters\n        # by passing copies\n        params_for_leaky = (w_init.copy(), b_init, v_init, c_init)\n        params_for_relu = (w_init.copy(), b_init, v_init, c_init)\n\n        # Train Leaky ReLU model\n        leaky_activation_with_alpha = lambda a: leaky_relu(a, alpha)\n        leaky_grad_with_alpha = lambda a: leaky_relu_grad(a, alpha)\n        mse_leaky = train_model(\n            X, y, params_for_leaky,\n            leaky_activation_with_alpha, leaky_grad_with_alpha,\n            T, eta\n        )\n\n        # Train ReLU model\n        mse_relu = train_model(\n            X, y, params_for_relu,\n            relu, relu_grad,\n            T, eta\n        )\n        \n        # Compare MSEs and store boolean result\n        result = (mse_leaky + delta  mse_relu)\n        results.append(result)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "我们最后的实践  深入探讨一个更高级的主题：激活函数如何塑造优化过程的损失景观（loss landscape）。我们将使用 Softplus 函数（ReLU 的一个平滑近似），并通过增大其参数 $\\beta$ 来逐步“硬化”它。通过数值计算损失景观的曲率，我们可以直观地看到，随着激活函数逼近不可微的 ReLU，损失景观如何变得更加尖锐、更不平滑，从而深入理解不同激活函数所带来的优化挑战。",
            "id": "3197657",
            "problem": "您将实现并分析一个单隐藏单元回归模型，该模型使用整流线性单元 (ReLU) 激活函数的一个平滑变体，以研究损失景观如何随着激活硬度参数的增加而变化。该模型在标量输入 $x$ 上定义为 $f_{\\theta,\\beta}(x) = w_2 \\, a_\\beta(w_1 x + b_1) + b_2$，其中 $a_\\beta$ 是带有硬度参数 $\\beta$ 的 Softplus 激活函数。数据集固定为标量输入 $x \\in \\{-2,-1,0,1,2\\}$，其目标值 $y = \\max(0,x)$，即整流线性单元 (ReLU)。损失为均方误差 $L_\\beta(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} (f_{\\theta,\\beta}(x_i) - y_i)^2$，其中 $N=5$。您的程序将计算损失函数在固定参数向量和固定方向上的二阶方向导数（曲率）的有限差分近似值。这将针对一系列逐渐硬化激活函数的 $\\beta$ 值进行计算，并报告这些曲率值。\n\n您必须使用的基本依据和定义：\n- 带有硬度参数 $\\beta$ 的 Softplus 是一个平滑激活函数，定义为 $a_\\beta(x) = \\frac{1}{\\beta} \\log(1 + e^{\\beta x})$。当 $\\beta \\to \\infty$ 时，$a_\\beta(x)$ 逼近由 $\\max(0,x)$ 给出的整流线性单元 (ReLU)。\n- 针对目标值 $y_i$ 和预测值 $\\hat{y}_i$ 的均方误差 (MSE) 为 $L = \\frac{1}{N}\\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2$。\n- 标量函数 $L(\\theta)$ 在点 $\\theta$ 沿单位方向 $v$ 的二阶方向导数，可以通过中心有限差分公式 $L''(\\theta;v) \\approx \\frac{L(\\theta + \\varepsilon v) - 2L(\\theta) + L(\\theta - \\varepsilon v)}{\\varepsilon^2}$ 进行近似，其中 $\\varepsilon$ 为小步长。\n\n程序要求：\n- 使用固定数据集 $\\mathcal{D} = \\{(-2,0),(-1,0),(0,0),(1,1),(2,2)\\}$。\n- 使用固定参数向量 $\\theta = (w_1, b_1, w_2, b_2) = (1,0,1,0)$。\n- 使用固定方向 $v = (0.3,-0.2,0.4,0.1)$，并将其归一化为单位长度，即替换为 $v / \\|v\\|_2$。\n- 使用有限差分步长 $\\varepsilon = 10^{-3}$。\n- 对于下面指定的测试套件中的每个硬度值 $\\beta$，使用上述中心差分公式计算 $L_\\beta$ 在 $\\theta$ 处沿 $v$ 方向的曲率 $C_\\beta$。\n- 稳定地实现 Softplus 函数以避免大的 $\\beta$ 值导致数值溢出，这需要依赖 $\\log(1 + e^{z})$ 的数值稳定变换。\n\n测试套件：\n- 评估并报告 $\\beta \\in \\{0.5, 1.0, 2.0, 5.0, 20.0, 50.0\\}$ 的曲率值，严格按照此顺序。\n\n答案规格：\n- 您的程序必须输出单行，其中包含一个 Python 风格的列表 $[C_{0.5}, C_{1.0}, C_{2.0}, C_{5.0}, C_{20.0}, C_{50.0}]$，按此顺序排列，每个值都四舍五入到恰好 $6$ 位小数。\n- 输出必须是单行，没有任何额外文本。\n- 每个报告值都是一个浮点数。不涉及物理单位或角度单位。\n\n科学真实性和推理要求：\n- 该任务评估增加 $\\beta$ 值如何改变激活函数的光滑度，并进而改变经验损失景观的曲率。您的实现应直接遵循上述定义，不使用任何训练过程或未文档化的启发式方法。在计算具有较大 $\\beta$ 值的 Softplus 函数时，请确保数值稳定性。",
            "solution": "该问题要求计算一个单隐藏单元回归模型的均方误差损失函数的二阶方向导数，即曲率。该曲率将针对一系列模型进行评估，在这些模型中，激活函数 Softplus 逐渐“硬化”为整流线性单元 (ReLU)。分析将在参数空间中的一个固定点和沿一个固定方向进行。\n\n首先，我们将问题的各个组成部分形式化。\n模型对标量输入 $x$ 的预测由以下公式给出：\n$$f_{\\theta,\\beta}(x) = w_2 \\, a_\\beta(w_1 x + b_1) + b_2$$\n模型的参数收集在向量 $\\theta = (w_1, b_1, w_2, b_2)$ 中。激活函数 $a_\\beta$ 是带有硬度参数 $\\beta$ 的 Softplus 函数：\n$$a_\\beta(z) = \\frac{1}{\\beta} \\log(1 + e^{\\beta z})$$\n当参数 $\\beta$ 趋近于无穷大时，$a_\\beta(z)$ 逐点收敛于 ReLU 函数 $\\max(0, z)$。\n\n模型在一个包含 $N=5$ 个数据点的固定数据集 $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^{N}$ 上进行评估。输入为 $x \\in \\{-2, -1, 0, 1, 2\\}$，对应的目标输出为 $y_i = \\max(0, x_i)$，从而得到数据集 $\\mathcal{D} = \\{(-2,0), (-1,0), (0,0), (1,1), (2,2)\\}$。\n\n损失函数 $L_\\beta(\\theta)$ 是模型预测值 $f_{\\theta,\\beta}(x_i)$ 与真实目标值 $y_i$ 之间的均方误差 (MSE)：\n$$L_\\beta(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} (f_{\\theta,\\beta}(x_i) - y_i)^2$$\n\n实现 Softplus 函数的一个关键方面是确保数值稳定性，特别是当参数 $\\beta z$ 的值很大时。当 $\\beta z$ 为大的正数时，$e^{\\beta z}$ 可能导致数值溢出。可以推导出一个函数 $g(u) = \\log(1 + e^u)$ 的标准稳定实现。对于正数 $u$，我们写作 $g(u) = \\log(e^u(e^{-u} + 1)) = u + \\log(1 + e^{-u})$。对于负数 $u$，原始形式是稳定的。一个更直接和鲁棒的方法是使用一个库函数来稳定地计算 $\\log(e^a + e^b)$，例如 `numpy.logaddexp(a, b)`。设 $a=0$ 和 $b=\\beta z$，Softplus 激活函数可以计算为：\n$$a_\\beta(z) = \\frac{1}{\\beta} \\text{np.logaddexp}(0, \\beta z)$$\n\n主要目标是计算损失函数 $L_\\beta(\\theta)$ 在固定参数向量 $\\theta_{center}$ 处沿固定单位方向向量 $v$ 的二阶方向导数。这个量记作 $C_\\beta$，代表了损失表面在 $v$ 方向上的曲率。它使用中心有限差分公式进行近似：\n$$C_\\beta = L_\\beta''(\\theta_{center}; v) \\approx \\frac{L_\\beta(\\theta_{center} + \\varepsilon v) - 2L_\\beta(\\theta_{center}) + L_\\beta(\\theta_{center} - \\varepsilon v)}{\\varepsilon^2}$$\n其中 $\\varepsilon$ 是一个小步长。\n\n问题指定了以下固定值：\n- 计算曲率时所用的参数向量：$\\theta_{center} = (w_1, b_1, w_2, b_2) = (1, 0, 1, 0)$。\n- 方向向量，必须被归一化：$v_{raw} = (0.3, -0.2, 0.4, 0.1)$。单位方向向量为 $v = v_{raw} / \\|v_{raw}\\|_2$。$L_2$-范数为 $\\|v_{raw}\\|_2 = \\sqrt{0.3^2 + (-0.2)^2 + 0.4^2 + 0.1^2} = \\sqrt{0.09 + 0.04 + 0.16 + 0.01} = \\sqrt{0.3}$。\n- 有限差分步长：$\\varepsilon = 10^{-3}$。\n\n计算过程如下。对于集合 $\\{0.5, 1.0, 2.0, 5.0, 20.0, 50.0\\}$ 中的每个指定 $\\beta$ 值：\n1. 定义一个函数，用于计算任意给定参数向量 $\\theta$ 的损失 $L_\\beta(\\theta)$。该函数以 $\\theta = (w_1, b_1, w_2, b_2)$ 作为输入，使用数值稳定的 Softplus 实现来计算数据集中所有 $x_i$ 的预测值 $f_{\\theta,\\beta}(x_i)$，并返回均方误差。\n2. 归一化方向向量 $v_{raw}$ 以获得 $v$。\n3. 在参数空间中的三个不同点上评估损失函数： - $L_{center} = L_\\beta(\\theta_{center})$ - $L_{plus} = L_\\beta(\\theta_{center} + \\varepsilon v)$ - $L_{minus} = L_\\beta(\\theta_{center} - \\varepsilon v)$\n4. 将这三个损失值代入中心差分公式，计算曲率近似值 $C_\\beta$。\n\n对每个 $\\beta$ 值重复此过程，并收集得到的曲率值。随着 $\\beta$ 的增加，Softplus 函数成为不可微的 ReLU 函数的一个更锐利的近似。激活函数光滑度的这种变化预计会反映在损失景观的曲率中，而本计算正是为了量化这一现象。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the second directional derivative (curvature) of a loss landscape\n    for a simple neural network. The curvature is evaluated for a sequence of\n    models where the Softplus activation function hardens towards ReLU.\n    \"\"\"\n\n    # --- Fixed parameters and data as per problem statement ---\n    \n    # Dataset: x values and corresponding y = max(0, x) targets\n    x_data = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    y_data = np.maximum(0, x_data)\n\n    # Fixed parameter vector theta = (w1, b1, w2, b2)\n    theta_center = np.array([1.0, 0.0, 1.0, 0.0])\n\n    # Unnormalized direction vector v\n    v_raw = np.array([0.3, -0.2, 0.4, 0.1])\n    \n    # Normalize the direction vector to unit length\n    v_norm = v_raw / np.linalg.norm(v_raw)\n\n    # Finite-difference step size\n    epsilon = 1e-3\n\n    # Hardness parameters for the Softplus function\n    beta_values = [0.5, 1.0, 2.0, 5.0, 20.0, 50.0]\n\n    # List to store the results\n    curvature_results = []\n\n    # --- Main Calculation Loop ---\n    \n    for beta in beta_values:\n        \n        def calculate_loss(theta):\n            \"\"\"\n            Calculates the Mean Squared Error for a given parameter vector theta\n            and the current beta value.\n            \n            Args:\n                theta (np.ndarray): The parameter vector (w1, b1, w2, b2).\n\n            Returns:\n                float: The mean squared error loss.\n            \"\"\"\n            w1, b1, w2, b2 = theta\n            \n            # Calculate pre-activation values\n            z = w1 * x_data + b1\n            \n            # Apply the numerically stable Softplus activation function\n            # a_beta(z) = (1/beta) * log(1 + exp(beta * z))\n            # np.logaddexp(0, x) computes log(exp(0) + exp(x)) = log(1 + exp(x)) stably.\n            activation_out = (1.0 / beta) * np.logaddexp(0, beta * z)\n            \n            # Calculate the final model prediction\n            y_pred = w2 * activation_out + b2\n            \n            # Compute the Mean Squared Error\n            loss = np.mean((y_pred - y_data)**2)\n            \n            return loss\n\n        # Evaluate the loss at the three points required for the central difference formula\n        loss_center = calculate_loss(theta_center)\n        loss_plus = calculate_loss(theta_center + epsilon * v_norm)\n        loss_minus = calculate_loss(theta_center - epsilon * v_norm)\n        \n        # Compute the second directional derivative (curvature)\n        curvature = (loss_plus - 2 * loss_center + loss_minus) / (epsilon**2)\n        \n        curvature_results.append(curvature)\n\n    # Format the results for the final output\n    # Each value is rounded to exactly 6 decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in curvature_results]\n    \n    # Print the final output in the specified format\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Run the solver\nsolve()\n```"
        }
    ]
}