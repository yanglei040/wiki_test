{
    "hands_on_practices": [
        {
            "introduction": "要真正理解ReLU变体存在的原因，我们必须分析它们对网络中传播的激活值的统计特性的影响。本练习要求你从第一性原理出发，严格推导当输入服从正态分布时，ReLU、ELU和SELU的输出均值和方差。通过完成这些计算，你将深刻理解自归一化网络背后的设计原则以及信号传播所面临的挑战。",
            "id": "3197614",
            "problem": "设标量预激活值 $x$ 服从正态随机分布 $x \\sim \\mathcal{N}(0,\\sigma^{2})$。考虑深度学习中三种常用的激活函数：修正线性单元（ReLU）、指数线性单元（ELU）和缩放指数线性单元（SELU），其定义如下\n$$\n\\mathrm{ReLU}(x) = \\max\\{0,x\\},\n$$\n$$\n\\mathrm{ELU}(x) = \n\\begin{cases}\nx,  x > 0,\\\\\n\\alpha\\left(\\exp(x) - 1\\right),  x \\leq 0,\n\\end{cases}\n\\quad \\text{其中 } \\alpha > 0,\n$$\n$$\n\\mathrm{SELU}(x) = \\lambda \\cdot \\mathrm{ELU}(x) \\quad \\text{其中 } \\lambda > 0 \\text{ 且 } \\alpha > 0.\n$$\n仅从实值随机变量的期望和方差的定义以及正态分布的概率密度函数出发，推导均值 $\\mathbb{E}[\\mathrm{ReLU}(x)]$ 和方差 $\\mathrm{Var}[\\mathrm{ReLU}(x)]$ 的闭式表达式（用 $\\sigma$、$\\alpha$、$\\lambda$ 以及标准函数，如标准正态累积分布函数来表示）。然后，推导 $\\mathbb{E}[\\mathrm{ELU}(x)]$ 和 $\\mathrm{Var}[\\mathrm{ELU}(x)]$ 以及 $\\mathbb{E}[\\mathrm{SELU}(x)]$ 和 $\\mathrm{Var}[\\mathrm{SELU}(x)]$ 的相应公式。\n\n利用这些表达式，并且除了核心定义外不使用任何快捷公式，解释每种激活函数输出的均值和方差如何依赖于输入标准差 $\\sigma$ 和激活参数 $(\\alpha,\\lambda)$。讨论这对旨在保持各层间零均值和稳定方差的权重初始化策略有何影响，包括在给定 $\\mathbb{E}[x]=0$ 的情况下，如何选择 $(\\alpha,\\lambda)$ 以使得 $\\mathbb{E}[\\mathrm{SELU}(x)]=0$ 和 $\\mathrm{Var}[\\mathrm{SELU}(x)]=1$。\n\n你的最终答案必须是一个单一的闭式解析表达式，将推导出的六个量收集在一个单行矩阵中。如果你选择引入任何特殊函数（例如，标准正态累积分布函数），请在推导中明确定义它们。不需要进行数值近似。",
            "solution": "该问题要求当输入 $x$ 是一个均值为 $0$、方差为 $\\sigma^2$ 的正态分布随机变量时，推导三种激活函数（ReLU、ELU 和 SELU）输出的均值和方差。推导必须从第一性原理出发。\n\n设输入预激活值为一个随机变量 $x \\sim \\mathcal{N}(0, \\sigma^2)$。其概率密度函数（PDF）由下式给出\n$$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$$\n对于连续随机变量 $x$ 的函数 $g(x)$，其期望 $\\mathbb{E}[g(x)]$ 和方差 $\\mathrm{Var}[g(x)]$ 的基本定义为：\n$$\\mathbb{E}[g(x)] = \\int_{-\\infty}^{\\infty} g(x) p(x) dx$$\n$$\\mathrm{Var}[g(x)] = \\mathbb{E}\\left[(g(x) - \\mathbb{E}[g(x)])^2\\right] = \\mathbb{E}[g(x)^2] - (\\mathbb{E}[g(x)])^2$$\n我们引入标准正态随机变量 $z \\sim \\mathcal{N}(0,1)$，其概率密度函数（PDF）为 $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$，其累积分布函数（CDF）为 $\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) dt$。输入 $x$ 可以写成 $x = \\sigma z$。\n\n我们将频繁使用以下从定义中推导出的定积分：\n1. $\\int_0^\\infty x p(x) dx$：该积分表示 $x$ 在正实数轴上的期望。\n$$\\int_0^\\infty x \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) dx = \\frac{\\sigma}{\\sqrt{2\\pi}} \\int_0^\\infty z \\exp\\left(-\\frac{z^2}{2}\\right) dz = \\frac{\\sigma}{\\sqrt{2\\pi}} \\left[-\\exp\\left(-\\frac{z^2}{2}\\right)\\right]_0^\\infty = \\frac{\\sigma}{\\sqrt{2\\pi}}$$\n2. $\\int_0^\\infty x^2 p(x) dx$：$x$ 的总方差为 $\\mathrm{Var}[x] = \\mathbb{E}[x^2] = \\sigma^2$。由于正态分布关于 $0$ 对称，正半轴上的积分是总积分的一半。\n$$\\int_0^\\infty x^2 p(x) dx = \\frac{1}{2}\\int_{-\\infty}^\\infty x^2 p(x) dx = \\frac{\\sigma^2}{2}$$\n3. $\\int_{-\\infty}^0 \\exp(ax) p(x) dx$：这个积分可以通过在指数上配方来求解。\n$$\\int_{-\\infty}^0 \\exp(ax) p(x) dx = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\int_{-\\infty}^0 \\exp\\left(ax - \\frac{x^2}{2\\sigma^2}\\right) dx = \\exp\\left(\\frac{a^2\\sigma^2}{2}\\right) \\Phi(-a\\sigma)$$\n4. $\\int_{-\\infty}^0 p(x) dx = P(x \\le 0) = \\frac{1}{2}$，由对称性可知。\n\n利用这些工具，我们为每种激活函数推导所需的量。\n\n**1. 修正线性单元 (ReLU)**\nReLU 函数定义为 $\\mathrm{ReLU}(x) = \\max\\{0, x\\}$。\n\n均值 $\\mathbb{E}[\\mathrm{ReLU}(x)]$：\n$$\\mathbb{E}[\\mathrm{ReLU}(x)] = \\int_{-\\infty}^{\\infty} \\max\\{0, x\\} p(x) dx = \\int_{-\\infty}^0 0 \\cdot p(x) dx + \\int_0^\\infty x p(x) dx$$\n使用积分 1，我们得到：\n$$\\mathbb{E}[\\mathrm{ReLU}(x)] = \\frac{\\sigma}{\\sqrt{2\\pi}}$$\n\n方差 $\\mathrm{Var}[\\mathrm{ReLU}(x)]$：\n首先，我们求二阶矩 $\\mathbb{E}[\\mathrm{ReLU}(x)^2]$。\n$$\\mathbb{E}[\\mathrm{ReLU}(x)^2] = \\int_{-\\infty}^{\\infty} (\\max\\{0, x\\})^2 p(x) dx = \\int_{-\\infty}^0 0^2 \\cdot p(x) dx + \\int_0^\\infty x^2 p(x) dx$$\n使用积分 2，我们得到：\n$$\\mathbb{E}[\\mathrm{ReLU}(x)^2] = \\frac{\\sigma^2}{2}$$\n现在，我们可以计算方差：\n$$\\mathrm{Var}[\\mathrm{ReLU}(x)] = \\mathbb{E}[\\mathrm{ReLU}(x)^2] - (\\mathbb{E}[\\mathrm{ReLU}(x)])^2 = \\frac{\\sigma^2}{2} - \\left(\\frac{\\sigma}{\\sqrt{2\\pi}}\\right)^2 = \\sigma^2\\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right)$$\n\n**2. 指数线性单元 (ELU)**\nELU 函数定义为 $x > 0$ 时 $\\mathrm{ELU}(x) = x$，以及 $x \\leq 0$ 时 $\\mathrm{ELU}(x) = \\alpha(\\exp(x) - 1)$，其中 $\\alpha>0$。\n\n均值 $\\mathbb{E}[\\mathrm{ELU}(x)]$：\n$$\\mathbb{E}[\\mathrm{ELU}(x)] = \\int_0^\\infty x p(x) dx + \\int_{-\\infty}^0 \\alpha(\\exp(x) - 1) p(x) dx$$\n第一项是 $\\frac{\\sigma}{\\sqrt{2\\pi}}$。第二项是：\n$$\\alpha \\int_{-\\infty}^0 (\\exp(x) - 1) p(x) dx = \\alpha \\left( \\int_{-\\infty}^0 \\exp(x) p(x) dx - \\int_{-\\infty}^0 p(x) dx \\right)$$\n使用积分 3（取 $a=1$）和 4：\n$$\\alpha \\left( \\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2} \\right)$$\n合并各项得到均值：\n$$\\mathbb{E}[\\mathrm{ELU}(x)] = \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right)$$\n\n方差 $\\mathrm{Var}[\\mathrm{ELU}(x)]$：\n首先，我们求二阶矩 $\\mathbb{E}[\\mathrm{ELU}(x)^2]$。\n$$\\mathbb{E}[\\mathrm{ELU}(x)^2] = \\int_0^\\infty x^2 p(x) dx + \\int_{-\\infty}^0 \\left(\\alpha(\\exp(x) - 1)\\right)^2 p(x) dx$$\n第一项是 $\\frac{\\sigma^2}{2}$。第二项是：\n$$\\alpha^2 \\int_{-\\infty}^0 (\\exp(2x) - 2\\exp(x) + 1) p(x) dx$$\n$$= \\alpha^2 \\left( \\int_{-\\infty}^0 \\exp(2x) p(x) dx - 2\\int_{-\\infty}^0 \\exp(x) p(x) dx + \\int_{-\\infty}^0 p(x) dx \\right)$$\n使用积分 3（取 $a=2$ 和 $a=1$）和 4：\n$$= \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right)$$\n合并各项得到二阶矩：\n$$\\mathbb{E}[\\mathrm{ELU}(x)^2] = \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right)$$\n方差则为 $\\mathrm{Var}[\\mathrm{ELU}(x)] = \\mathbb{E}[\\mathrm{ELU}(x)^2] - (\\mathbb{E}[\\mathrm{ELU}(x)])^2$，可得：\n$$\\mathrm{Var}[\\mathrm{ELU}(x)] = \\left[ \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\dots \\right) \\right] - \\left[ \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left( \\dots \\right) \\right]^2$$\n展开为：\n$$\\mathrm{Var}[\\mathrm{ELU}(x)] = \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) - \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right)^2$$\n\n**3. 缩放指数线性单元 (SELU)**\nSELU 函数定义为 $\\mathrm{SELU}(x) = \\lambda \\cdot \\mathrm{ELU}(x)$，其中 $\\lambda > 0$ 且 $\\alpha > 0$。其均值和方差可以利用期望和方差关于标量乘法的性质求得。\n\n均值 $\\mathbb{E}[\\mathrm{SELU}(x)]$：\n$$\\mathbb{E}[\\mathrm{SELU}(x)] = \\mathbb{E}[\\lambda \\cdot \\mathrm{ELU}(x)] = \\lambda \\mathbb{E}[\\mathrm{ELU}(x)]$$\n$$\\mathbb{E}[\\mathrm{SELU}(x)] = \\lambda \\left[ \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right]$$\n\n方差 $\\mathrm{Var}[\\mathrm{SELU}(x)]$：\n$$\\mathrm{Var}[\\mathrm{SELU}(x)] = \\mathrm{Var}[\\lambda \\cdot \\mathrm{ELU}(x)] = \\lambda^2 \\mathrm{Var}[\\mathrm{ELU}(x)]$$\n$$\\mathrm{Var}[\\mathrm{SELU}(x)] = \\lambda^2 \\left[ \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) - \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right)^2 \\right]$$\n\n**讨论与影响**\n\n推导出的表达式揭示了输出分布的统计特性如何依赖于输入标准差 $\\sigma$ 和激活参数 $(\\alpha, \\lambda)$。\n\n对于 ReLU，输出均值 $\\mathbb{E}[\\mathrm{ReLU}(x)] = \\sigma/\\sqrt{2\\pi}$ 严格为正，并随 $\\sigma$ 缩放。这引入了一个偏离零的“偏置偏移”，可能使优化复杂化。输出方差为 $\\mathrm{Var}[\\mathrm{ReLU}(x)] \\approx 0.34 \\sigma^2$。这种方差的减小，如果不加以补偿，可能导致深度网络中的梯度消失问题。像“He 初始化”这样的权重初始化策略旨在通过缩放权重来恢复预激活值的方差，从而抵消这种影响。\n\n对于 ELU 和 SELU，参数 $\\alpha$ 和 $\\lambda$ 提供了更多的控制。SELU 的目标是实现自归一化网络，其中每层的输出自动收敛到一个具有固定均值和方差的分布，通常是均值为 $0$、方差为 $1$。\n对于均值为 $0$、方差为 $\\sigma^2$ 的输入，要实现 $\\mathbb{E}[\\mathrm{SELU}(x)]=0$ 和 $\\mathrm{Var}[\\mathrm{SELU}(x)]=1$，我们必须为 $\\alpha$ 和 $\\lambda$ 求解以下方程组：\n1. $\\mathbb{E}[\\mathrm{SELU}(x)] = 0$：由于 $\\lambda > 0$，这要求 $\\mathbb{E}[\\mathrm{ELU}(x)] = 0$。\n$$\\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) = 0$$\n这个方程建立了 $\\alpha$ 和输入方差 $\\sigma^2$ 之间的关系。\n\n2. $\\mathrm{Var}[\\mathrm{SELU}(x)] = 1$：这意味着 $\\lambda^2 \\mathrm{Var}[\\mathrm{ELU}(x)] = 1$。由于我们已设定 $\\mathbb{E}[\\mathrm{ELU}(x)]=0$，方差简化为二阶矩，即 $\\mathrm{Var}[\\mathrm{ELU}(x)] = \\mathbb{E}[\\mathrm{ELU}(x)^2]$。\n$$\\lambda^2 \\left[ \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) \\right] = 1$$\n\n在自归一化网络的背景下，通常假设层的输入已被归一化，使其方差为 $\\sigma^2=1$。通过对 $\\sigma=1$ 求解这两个方程，可以获得 $\\alpha$ 和 $\\lambda$ 的特定数值（约 $\\alpha \\approx 1.6733$ 和 $\\lambda \\approx 1.0507$）。这些特定值定义了标准的 SELU 激活函数，当与特定的权重初始化（“LeCun 正态”）配对时，能促进整个网络中神经元激活值收敛到标准正态分布，从而缓解梯度消失/爆炸问题并对训练过程进行正则化。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{\\sigma}{\\sqrt{2\\pi}} & \\sigma^2\\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right) & \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) & \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) - \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right)^2 & \\lambda \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right) & \\lambda^2 \\left( \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) - \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right)^2 \\right) \\end{pmatrix}}$$"
        },
        {
            "introduction": "一个激活函数的理论优势必须能够转化为实际应用中的好处。本编码练习将Leaky ReLU置于一个特定情境中，其独特的函数形式——允许负输入有小的非零梯度——比标准ReLU更适合解决该问题。通过实现和比较这两个模型，你将亲眼见证一个激活函数的归纳偏置如何决定模型学习潜在规律的能力。",
            "id": "3197626",
            "problem": "考虑一个双特征输入向量 $\\mathbf{x} = \\begin{bmatrix} x_{+} \\\\ x_{-} \\end{bmatrix} \\in \\mathbb{R}^{2}$，其中 $x_{+}$ 表示某个潜在原因的正向证据，$x_{-}$ 表示负向（抑制性）证据。潜在线性得分定义为 $a^{\\star}(\\mathbf{x}) = x_{+} - x_{-}$。为了体现“负向证据应部分抑制激活，但非完全消除”这一原则，目标映射通过作用于 $a^{\\star}$ 的带泄露整流线性单元 (Leaky Rectified Linear Unit, Leaky ReLU) 激活函数来定义，其斜率参数为 $\\alpha \\in [0,1]$：\n$$\ny(\\mathbf{x}; \\alpha) = \\phi_{\\text{leaky}, \\alpha}\\!\\left(a^{\\star}(\\mathbf{x})\\right) \\quad \\text{其中} \\quad \\phi_{\\text{leaky}, \\alpha}(a) = \\max(a, \\alpha a).\n$$\n作为对比，整流线性单元 (Rectified Linear Unit, ReLU) 定义为\n$$\n\\phi_{\\text{relu}}(a) = \\max(0,a).\n$$\n对于每个指定的 $\\alpha$，你将训练两个单神经元模型，它们使用以下架构将 $\\mathbf{x}$ 映射到一个标量预测值 $\\hat{y}$：\n$$\n\\hat{y}(\\mathbf{x}) = v \\cdot \\phi\\!\\left(\\mathbf{w}^{\\top}\\mathbf{x} + b\\right) + c,\n$$\n其中 $\\phi$ 是 $\\phi_{\\text{relu}}$ 或 $\\phi_{\\text{leaky}, \\alpha}$，而 $\\mathbf{w} \\in \\mathbb{R}^{2}$、$b \\in \\mathbb{R}$、$v \\in \\mathbb{R}$ 和 $c \\in \\mathbb{R}$ 是可训练参数。训练过程通过最小化均方误差 (Mean Squared Error, MSE) 来进行，对于数据集 $\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{N}$，MSE 定义为\n$$\n\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\hat{y}(\\mathbf{x}_{i}) - y_{i}\\right)^{2}.\n$$\n使用经验风险最小化 (Empirical Risk Minimization, ERM)，通过全批量梯度下降来更新 $(\\mathbf{w}, b, v, c)$。\n\n数据集规格：\n- 使用种子初始化为 0 的伪随机数生成器，抽取 $N = 2000$ 个独立样本。\n- 对每个样本，独立地从 $\\text{Uniform}([0,1])$ 分布中抽取 $x_{+}$ 和 $x_{-}$。\n- 对于给定的 $\\alpha$，计算 $a^{\\star} = x_{+} - x_{-}$ 和 $y = \\phi_{\\text{leaky}, \\alpha}(a^{\\star})$。\n\n训练规格：\n- 使用全批量梯度下降，学习率为 $\\eta = 0.03$，步数为 $T = 800$。\n- 从正态分布 $\\mathcal{N}(0, \\sigma^{2})$（其中 $\\sigma = 0.1$）中初始化参数 $(\\mathbf{w}, b, v, c)$。为确保可复现性和公平性，每个测试用例使用一个固定的种子：对于索引为 $k$（从 $0$ 开始）的测试用例，将初始化种子设置为 $100+k$。\n- 在每个测试用例中，确保两个模型（$\\phi=\\phi_{\\text{relu}}$ 和 $\\phi=\\phi_{\\text{leaky}, \\alpha}$）都从完全相同的初始参数开始。\n\n测试套件：\n评估负斜率参数 $\\alpha$ 的以下值：\n- $\\alpha = 0.2$: 典型的部分抑制。\n- $\\alpha = 0.0$: Leaky ReLU 退化为 ReLU 的边界情况。\n- $\\alpha = 0.8$: 强部分抑制。\n- $\\alpha = 1.0$: 在 $a^{\\star}$ 上的恒等激活。\n\n对于每个 $\\alpha$，按照上述规格训练两个模型，并计算它们在训练数据上的 MSE。定义一个容差 $\\delta = 10^{-4}$，并为每个测试用例输出布尔结果\n$$\n\\text{result} = \\left(\\text{MSE}_{\\text{leaky}} + \\delta  \\text{MSE}_{\\text{relu}}\\right).\n$$\n\n最终输出格式：\n你的程序应生成一行输出，其中包含用方括号括起来的、以逗号分隔的结果列表（例如，$[\\text{result}_{1},\\text{result}_{2},\\text{result}_{3},\\text{result}_{4}]$），布尔值的顺序与上面给出的 $\\alpha$ 顺序一致。\n\n缩略语定义：\n- 整流线性单元 (ReLU): $\\phi_{\\text{relu}}(a) = \\max(0,a)$。\n- 带泄露整流线性单元 (Leaky ReLU): $\\phi_{\\text{leaky}, \\alpha}(a) = \\max(a, \\alpha a)$。\n- 均方误差 (MSE): $\\frac{1}{N}\\sum_{i=1}^{N}(\\hat{y}_{i}-y_{i})^{2}$。\n- 经验风险最小化 (ERM): 通过最小化数据集上的经验损失来估计参数。",
            "solution": "该问题要求训练两个单神经元模型，以近似一个由带泄露整流线性单元 (Leaky ReLU) 生成的目标函数。其中一个模型使用 Leaky ReLU 激活函数，与真实情况（ground truth）匹配；而另一个模型使用标准的整流线性单元 (ReLU)。目标是通过全批量梯度下降法训练它们，并在训练数据上评估其最终均方误差 (MSE)，从而比较它们的性能。\n\n### 1. 模型与损失函数\n\n两个模型的架构都是一个单神经元网络，由下式给出：\n$$\n\\hat{y}(\\mathbf{x}) = v \\cdot \\phi(\\mathbf{w}^{\\top}\\mathbf{x} + b) + c\n$$\n其中 $\\mathbf{x} = \\begin{bmatrix} x_{+} \\\\ x_{-} \\end{bmatrix} \\in \\mathbb{R}^{2}$ 是输入向量，$\\mathbf{w} \\in \\mathbb{R}^{2}$、$b \\in \\mathbb{R}$、$v \\in \\mathbb{R}$ 和 $c \\in \\mathbb{R}$ 是可训练参数。函数 $\\phi$ 是激活函数。\n\n这两个模型通过它们对 $\\phi$ 的选择来区分：\n1.  **ReLU 模型**: $\\phi(\\cdot) = \\phi_{\\text{relu}}(a) = \\max(0, a)$。\n2.  **Leaky ReLU 模型**: $\\phi(\\cdot) = \\phi_{\\text{leaky}, \\alpha}(a) = \\max(a, \\alpha a)$，其中 $\\alpha$ 是一个给定的斜率参数。\n\n模型通过在包含 $N$ 个样本的数据集 $\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{N}$ 上最小化均方误差 (MSE) 损失函数来进行训练：\n$$\nJ(\\mathbf{w}, b, v, c) = \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\hat{y}(\\mathbf{x}_{i}) - y_{i}\\right)^{2}\n$$\n目标值 $y_i$ 是通过 $y_i = \\phi_{\\text{leaky}, \\alpha}(x_{i,+} - x_{i,-})$ 为一个特定的 $\\alpha$ 生成的。\n\n### 2. 批量梯度下降的梯度推导\n\n为了使用梯度下降法最小化 MSE，我们必须计算损失函数 $J$ 关于每个参数的偏导数。对任意参数 $\\theta$ 的更新规则是 $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} J$，其中 $\\eta$ 是学习率。我们使用全批量梯度下降，所以梯度是在整个数据集上求平均得到的。\n\n设 $a_i = \\mathbf{w}^{\\top}\\mathbf{x}_i + b$ 为样本 $i$ 的激活前值，$\\hat{y}_i = v \\cdot \\phi(a_i) + c$ 为预测值。单个样本的损失为 $L_i = (\\hat{y}_i - y_i)^2$。应用链式法则，我们求得总损失 $J = \\frac{1}{N} \\sum_{i=1}^{N} L_i$ 的梯度如下：\n\n-   **关于 $c$ 的梯度**：\n    $$\n    \\nabla_c J = \\frac{\\partial J}{\\partial c} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L_i}{\\partial c} = \\frac{1}{N} \\sum_{i=1}^{N} 2(\\hat{y}_i - y_i)\n    $$\n\n-   **关于 $v$ 的梯度**：\n    $$\n    \\nabla_v J = \\frac{\\partial J}{\\partial v} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L_i}{\\partial v} = \\frac{1}{N} \\sum_{i=1}^{N} 2(\\hat{y}_i - y_i) \\phi(a_i)\n    $$\n\n-   **关于 $b$ 的梯度**：\n    $$\n    \\nabla_b J = \\frac{\\partial J}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L_i}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^{N} 2(\\hat{y}_i - y_i) \\cdot v \\cdot \\phi'(a_i)\n    $$\n\n-   **关于 $\\mathbf{w}$ 的梯度**：\n    $$\n    \\nabla_{\\mathbf{w}} J = \\frac{\\partial J}{\\partial \\mathbf{w}} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L_i}{\\partial \\mathbf{w}} = \\frac{1}{N} \\sum_{i=1}^{N} 2(\\hat{y}_i - y_i) \\cdot v \\cdot \\phi'(a_i) \\cdot \\mathbf{x}_i\n    $$\n\n$\\phi'(a)$ 项是激活函数的导数（或次梯度）。\n\n-   对于 **ReLU**: $\\phi_{\\text{relu}}(a) = \\max(0, a)$。其梯度（次梯度）为：\n    $$\n    \\phi'_{\\text{relu}}(a) = \\begin{cases} 1  \\text{if } a  0 \\\\ 0  \\text{if } a \\le 0 \\end{cases}\n    $$\n-   对于 **Leaky ReLU**: $\\phi_{\\text{leaky}, \\alpha}(a) = \\max(a, \\alpha a)$。对于 $\\alpha \\in [0, 1]$，这可以写为：如果 $a \\ge 0$ 则为 $a$，如果 $a  0$ 则为 $\\alpha a$。其梯度（次梯度）为：\n    $$\n    \\phi'_{\\text{leaky}, \\alpha}(a) = \\begin{cases} 1  \\text{if } a \\ge 0 \\\\ \\alpha  \\text{if } a  0 \\end{cases}\n    $$\n这种通用形式能正确处理 $\\alpha=0$（退化为 ReLU）和 $\\alpha=1$（退化为恒等函数 $a$）的边界情况。\n\n### 3. 模拟流程\n\n该问题要求进行一个遵循以下规格的计算实验：\n\n1.  **数据集生成**：在开始时生成一个包含 $N = 2000$ 个样本的输入特征数据集 $\\mathbf{X}$。对每个样本，$x_+$ 和 $x_-$ 从 $\\text{Uniform}([0,1])$ 分布中独立抽取。使用种子为 0 的伪随机数生成器。\n2.  **测试用例**：实验针对斜率参数 $\\alpha$ 的四个值 $\\{0.2, 0.0, 0.8, 1.0\\}$ 进行。\n3.  **各用例设置**：对于每个 $\\alpha$ 值（由索引 $k$ 表示，从 0 开始）：\n    a. 目标向量 $\\mathbf{y}$ 通过 $y_i = \\phi_{\\text{leaky}, \\alpha}(x_{i,+} - x_{i,-})$ 计算得出。\n    b. 模型参数 $(\\mathbf{w}, b, v, c)$ 通过从正态分布 $\\mathcal{N}(0, \\sigma^2)$（其中 $\\sigma=0.1$）中抽取来初始化，使用固定的种子 $100+k$。\n    c. ReLU 模型和 Leaky ReLU 模型都从这些相同的初始参数开始。\n4.  **训练**：两个模型都使用全批量梯度下降法进行 $T=800$ 步的训练，学习率为 $\\eta=0.03$。上面推导出的梯度用于参数更新。\n5.  **评估**：训练结束后，在整个训练数据集上为两个模型计算最终的 MSE，得到 $\\text{MSE}_{\\text{leaky}}$ 和 $\\text{MSE}_{\\text{relu}}$。\n6.  **比较**：布尔结果由条件 $\\text{MSE}_{\\text{leaky}} + \\delta  \\text{MSE}_{\\text{relu}}$ 决定，容差 $\\delta = 10^{-4}$。\n\n此流程系统地将一个架构与数据生成过程匹配的模型与一个模型设定不当的模型进行比较，从而为理解归纳偏置（inductive bias）的作用提供了见解。对于 $\\alpha=0.0$ 的情况，目标函数是一个 ReLU，这使得两个模型在功能上是相同的；因此，它们的 MSE 预计会相等，从而导致 `False` 结果。对于其他的 $\\alpha$ 值，Leaky ReLU 模型具有正确的函数形式，预计会获得更低的 MSE，从而得到 `True` 结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef leaky_relu(a, alpha):\n    \"\"\"\n    Computes the Leaky ReLU activation function.\n    Given alpha in [0,1], max(a, alpha*a) is equivalent to:\n    a if a >= 0\n    alpha * a if a  0\n    \"\"\"\n    return np.maximum(a, alpha * a)\n\ndef leaky_relu_grad(a, alpha):\n    \"\"\"Computes the gradient of the Leaky ReLU function.\"\"\"\n    grad = np.ones_like(a, dtype=float)\n    grad[a  0] = alpha\n    return grad\n\ndef relu(a):\n    \"\"\"Computes the ReLU activation function.\"\"\"\n    return np.maximum(0, a)\n\ndef relu_grad(a):\n    \"\"\"Computes the gradient of the ReLU function.\"\"\"\n    grad = np.zeros_like(a, dtype=float)\n    grad[a > 0] = 1.0\n    return grad\n\ndef train_model(X, y, initial_params, activation_func, grad_func, T, eta):\n    \"\"\"\n    Trains a single-neuron model using full-batch gradient descent.\n    \"\"\"\n    w, b, v, c = initial_params\n    N = X.shape[0]\n\n    for _ in range(T):\n        # Forward pass\n        a = X @ w + b\n        h = activation_func(a)\n        y_hat = v * h + c\n\n        # Error term\n        e = y_hat - y\n\n        # Gradients\n        grad_c = (2 / N) * np.sum(e)\n        grad_v = (2 / N) * np.sum(e * h)\n        \n        # Common term for w and b gradients\n        g_a_common = (2 / N) * v * e\n        g_a = g_a_common * grad_func(a)\n\n        grad_b = np.sum(g_a)\n        grad_w = X.T @ g_a\n\n        # Update parameters\n        w -= eta * grad_w\n        b -= eta * grad_b\n        v -= eta * grad_v\n        c -= eta * grad_c\n\n    # Compute final MSE on the training data\n    a_final = X @ w + b\n    h_final = activation_func(a_final)\n    y_hat_final = v * h_final + c\n    final_mse = np.mean((y_hat_final - y)**2)\n\n    return final_mse\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and print the results.\n    \"\"\"\n    # Problem specifications\n    N = 2000\n    dataset_seed = 0\n    eta = 0.03\n    T = 800\n    sigma = 0.1\n    delta = 1e-4\n    alphas = [0.2, 0.0, 0.8, 1.0]\n\n    # Generate dataset features X (same for all test cases)\n    rng_dataset = np.random.RandomState(dataset_seed)\n    x_plus = rng_dataset.uniform(0, 1, N)\n    x_minus = rng_dataset.uniform(0, 1, N)\n    X = np.stack([x_plus, x_minus], axis=1)\n\n    results = []\n\n    for k, alpha in enumerate(alphas):\n        # Generate target vector y for the current alpha\n        a_star = x_plus - x_minus\n        y = leaky_relu(a_star, alpha)\n\n        # Generate initial parameters using the per-case seed\n        init_seed = 100 + k\n        rng_init = np.random.RandomState(init_seed)\n        w_init = rng_init.normal(0, sigma, size=2)\n        b_init = rng_init.normal(0, sigma)\n        v_init = rng_init.normal(0, sigma)\n        c_init = rng_init.normal(0, sigma)\n        \n        # Ensure both models start with the exact same parameters\n        # by passing copies\n        params_for_leaky = (w_init.copy(), b_init, v_init, c_init)\n        params_for_relu = (w_init.copy(), b_init, v_init, c_init)\n\n        # Train Leaky ReLU model\n        leaky_activation_with_alpha = lambda a: leaky_relu(a, alpha)\n        leaky_grad_with_alpha = lambda a: leaky_relu_grad(a, alpha)\n        mse_leaky = train_model(\n            X, y, params_for_leaky,\n            leaky_activation_with_alpha, leaky_grad_with_alpha,\n            T, eta\n        )\n\n        # Train ReLU model\n        mse_relu = train_model(\n            X, y, params_for_relu,\n            relu, relu_grad,\n            T, eta\n        )\n        \n        # Compare MSEs and store boolean result\n        result = (mse_leaky + delta  mse_relu)\n        results.append(result)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "激活函数的选择直接塑造了损失函数的景观，进而影响优化的难易程度。本实践通过使用平滑的Softplus函数作为对非平滑ReLU的逼近，来探索激活函数“硬度”与损失景观曲率之间的联系。通过数值计算当Softplus函数变得越来越“硬”时损失曲率的变化，你将对不可微激活函数带来的优化挑战建立更直观的理解。",
            "id": "3197657",
            "problem": "您将实现并分析一个单隐藏单元回归模型，该模型使用修正线性单元（ReLU）激活函数的一个平滑变体，以研究损失景观如何随着激活硬度参数的增加而变化。该模型在标量输入 $x$ 上的定义为 $f_{\\theta,\\beta}(x) = w_2 \\, a_\\beta(w_1 x + b_1) + b_2$，其中 $a_\\beta$ 是带有硬度参数 $\\beta$ 的 Softplus 激活函数。数据集固定为标量输入 $x \\in \\{-2,-1,0,1,2\\}$，其目标值 $y = \\max(0,x)$，即修正线性单元（ReLU）。损失函数为均方误差 $L_\\beta(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} (f_{\\theta,\\beta}(x_i) - y_i)^2$，其中 $N = 5$。您的程序将计算在一系列使激活函数逐渐变硬的 $\\beta$ 值下，损失函数在固定参数矢量上沿固定方向的二阶方向导数（曲率）的有限差分近似，并报告这些曲率值。\n\n您必须使用的基本依据和定义：\n- 带有硬度参数 $\\beta$ 的 Softplus 是一个平滑激活函数，定义为 $a_\\beta(x) = \\frac{1}{\\beta} \\log(1 + e^{\\beta x})$。当 $\\beta \\to \\infty$ 时，$a_\\beta(x)$ 逼近由 $\\max(0,x)$ 给出的修正线性单元（ReLU）。\n- 对于目标值 $y_i$ 和预测值 $\\hat{y}_i$ 的均方误差（MSE）为 $L = \\frac{1}{N}\\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2$。\n- 标量函数 $L(\\theta)$ 在点 $\\theta$ 处沿单位方向 $v$ 的二阶方向导数可以通过中心有限差分公式近似为 $L''(\\theta;v) \\approx \\frac{L(\\theta + \\varepsilon v) - 2L(\\theta) + L(\\theta - \\varepsilon v)}{\\varepsilon^2}$，其中 $\\varepsilon$ 是一个小步长。\n\n程序要求：\n- 使用固定的数据集 $\\mathcal{D} = \\{(-2,0),(-1,0),(0,0),(1,1),(2,2)\\}$。\n- 使用固定的参数矢量 $\\theta = (w_1, b_1, w_2, b_2) = (1,0,1,0)$。\n- 使用固定的方向 $v = (0.3,-0.2,0.4,0.1)$，并将其归一化至单位长度，即用 $v / \\|v\\|_2$ 替换。\n- 使用有限差分步长 $\\varepsilon = 10^{-3}$。\n- 对于下面指定的测试套件中的每个硬度值 $\\beta$，使用上述中心差分公式计算 $L_\\beta$ 在 $\\theta$ 处沿 $v$ 方向的曲率 $C_\\beta$。\n- 稳定地实现 Softplus 函数，以避免当 $\\beta$ 较大时出现数值溢出，这需要依赖于 $\\log(1 + e^{z})$ 的数值稳定变换。\n\n测试套件：\n- 按此确切顺序，评估并报告 $\\beta \\in \\{0.5, 1.0, 2.0, 5.0, 20.0, 50.0\\}$ 的曲率值。\n\n答案规格：\n- 您的程序必须输出单行内容，其中包含一个 Python 风格的列表，按顺序包含曲率值 $[C_{0.5}, C_{1.0}, C_{2.0}, C_{5.0}, C_{20.0}, C_{50.0}]$，每个值都精确到 $6$ 位小数。\n- 输出必须是单行，不含任何额外文本。\n- 每个报告的值都是浮点数。不涉及物理单位或角度单位。\n\n科学真实性和推理要求：\n- 该任务评估增加 $\\beta$ 如何改变激活函数的光滑度，从而改变经验损失景观的曲率。您的实现应直接遵循上述定义，不使用任何训练过程或未记录的启发式方法。在计算具有较大 $\\beta$ 的 Softplus 函数时，请确保数值稳定性。",
            "solution": "该问题要求计算一个单隐藏单元回归模型的均方误差损失函数的二阶方向导数（即曲率）。该曲率将针对一系列模型进行评估，在这些模型中，激活函数 Softplus 会逐渐“硬化”为修正线性单元（ReLU）。分析将在参数空间中的一个固定点上沿一个固定方向进行。\n\n首先，我们将问题中的各个组成部分形式化。\n模型对标量输入 $x$ 的预测由下式给出：\n$$f_{\\theta,\\beta}(x) = w_2 \\, a_\\beta(w_1 x + b_1) + b_2$$\n模型的参数被收集在矢量 $\\theta = (w_1, b_1, w_2, b_2)$ 中。激活函数 $a_\\beta$ 是带有硬度参数 $\\beta$ 的 Softplus 函数：\n$$a_\\beta(z) = \\frac{1}{\\beta} \\log(1 + e^{\\beta z})$$\n当参数 $\\beta$ 趋于无穷大时，$a_\\beta(z)$ 逐点收敛于 ReLU 函数 $\\max(0, z)$。\n\n模型在一个包含 $N=5$ 个数据点的固定数据集 $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^{N}$ 上进行评估。输入为 $x \\in \\{-2, -1, 0, 1, 2\\}$，对应的目标输出为 $y_i = \\max(0, x_i)$，得到数据集 $\\mathcal{D} = \\{(-2,0), (-1,0), (0,0), (1,1), (2,2)\\}$。\n\n损失函数 $L_\\beta(\\theta)$ 是模型预测值 $f_{\\theta,\\beta}(x_i)$ 与真实目标值 $y_i$ 之间的均方误差（MSE）：\n$$L_\\beta(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} (f_{\\theta,\\beta}(x_i) - y_i)^2$$\n\n实现 Softplus 函数的一个关键方面是确保数值稳定性，尤其是在参数 $\\beta z$ 值较大时。当 $\\beta z$ 为较大的正数时，$e^{\\beta z}$ 可能导致数值溢出。可以推导出一个函数 $g(u) = \\log(1 + e^u)$ 的标准稳定实现。对于正数 $u$，我们写作 $g(u) = \\log(e^u(e^{-u} + 1)) = u + \\log(1 + e^{-u})$。对于负数 $u$，原始形式是稳定的。一种更直接且鲁棒的方法是使用能够稳定计算 $\\log(e^a + e^b)$ 的库函数，例如 `numpy.logaddexp(a, b)`。通过设置 $a=0$ 和 $b=\\beta z$，Softplus 激活函数可以计算为：\n$$a_\\beta(z) = \\frac{1}{\\beta} \\text{np.logaddexp}(0, \\beta z)$$\n\n主要目标是计算损失函数 $L_\\beta(\\theta)$ 在一个固定参数矢量 $\\theta_{center}$ 处沿一个固定单位方向矢量 $v$ 的二阶方向导数。这个量记为 $C_\\beta$，表示损失曲面在 $v$ 方向上的曲率。它使用中心有限差分公式进行近似：\n$$C_\\beta = L_\\beta''(\\theta_{center}; v) \\approx \\frac{L_\\beta(\\theta_{center} + \\varepsilon v) - 2L_\\beta(\\theta_{center}) + L_\\beta(\\theta_{center} - \\varepsilon v)}{\\varepsilon^2}$$ 其中 $\\varepsilon$ 是一个小步长。\n\n问题指定了以下固定值：\n- 评估曲率时所用的参数矢量：$\\theta_{center} = (w_1, b_1, w_2, b_2) = (1, 0, 1, 0)$。\n- 方向矢量，必须进行归一化：$v_{raw} = (0.3, -0.2, 0.4, 0.1)$。单位方向矢量为 $v = v_{raw} / \\|v_{raw}\\|_2$。$L_2$-范数为 $\\|v_{raw}\\|_2 = \\sqrt{0.3^2 + (-0.2)^2 + 0.4^2 + 0.1^2} = \\sqrt{0.09 + 0.04 + 0.16 + 0.01} = \\sqrt{0.3}$。\n- 有限差分步长：$\\varepsilon = 10^{-3}$。\n\n计算过程如下。对于集合 $\\{0.5, 1.0, 2.0, 5.0, 20.0, 50.0\\}$ 中的每个指定 $\\beta$ 值：\n1. 定义一个函数，用于为任何给定的参数矢量 $\\theta$ 计算损失 $L_\\beta(\\theta)$。该函数以 $\\theta = (w_1, b_1, w_2, b_2)$ 作为输入，使用数值稳定的 Softplus 实现计算数据集中所有 $x_i$ 的预测值 $f_{\\theta,\\beta}(x_i)$，并返回均方误差。\n2. 归一化方向矢量 $v_{raw}$ 以获得 $v$。\n3. 在参数空间中的三个不同点上评估损失函数：\n   - $L_{center} = L_\\beta(\\theta_{center})$\n   - $L_{plus} = L_\\beta(\\theta_{center} + \\varepsilon v)$\n   - $L_{minus} = L_\\beta(\\theta_{center} - \\varepsilon v)$\n4. 将这三个损失值代入中心差分公式，以计算曲率的近似值 $C_\\beta$。\n\n对每个 $\\beta$ 值重复此过程，并收集得到的曲率值。随着 $\\beta$ 的增加，Softplus 函数成为不可微的 ReLU 函数的一个更尖锐的近似。激活函数光滑度的这种变化预计会反映在损失景观的曲率上，而本计算正是为了量化这一现象。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the second directional derivative (curvature) of a loss landscape\n    for a simple neural network. The curvature is evaluated for a sequence of\n    models where the Softplus activation function hardens towards ReLU.\n    \"\"\"\n\n    # --- Fixed parameters and data as per problem statement ---\n    \n    # Dataset: x values and corresponding y = max(0, x) targets\n    x_data = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    y_data = np.maximum(0, x_data)\n\n    # Fixed parameter vector theta = (w1, b1, w2, b2)\n    theta_center = np.array([1.0, 0.0, 1.0, 0.0])\n\n    # Unnormalized direction vector v\n    v_raw = np.array([0.3, -0.2, 0.4, 0.1])\n    \n    # Normalize the direction vector to unit length\n    v_norm = v_raw / np.linalg.norm(v_raw)\n\n    # Finite-difference step size\n    epsilon = 1e-3\n\n    # Hardness parameters for the Softplus function\n    beta_values = [0.5, 1.0, 2.0, 5.0, 20.0, 50.0]\n\n    # List to store the results\n    curvature_results = []\n\n    # --- Main Calculation Loop ---\n    \n    for beta in beta_values:\n        \n        def calculate_loss(theta):\n            \"\"\"\n            Calculates the Mean Squared Error for a given parameter vector theta\n            and the current beta value.\n            \n            Args:\n                theta (np.ndarray): The parameter vector (w1, b1, w2, b2).\n\n            Returns:\n                float: The mean squared error loss.\n            \"\"\"\n            w1, b1, w2, b2 = theta\n            \n            # Calculate pre-activation values\n            z = w1 * x_data + b1\n            \n            # Apply the numerically stable Softplus activation function\n            # a_beta(z) = (1/beta) * log(1 + exp(beta * z))\n            # np.logaddexp(0, x) computes log(exp(0) + exp(x)) = log(1 + exp(x)) stably.\n            activation_out = (1.0 / beta) * np.logaddexp(0, beta * z)\n            \n            # Calculate the final model prediction\n            y_pred = w2 * activation_out + b2\n            \n            # Compute the Mean Squared Error\n            loss = np.mean((y_pred - y_data)**2)\n            \n            return loss\n\n        # Evaluate the loss at the three points required for the central difference formula\n        loss_center = calculate_loss(theta_center)\n        loss_plus = calculate_loss(theta_center + epsilon * v_norm)\n        loss_minus = calculate_loss(theta_center - epsilon * v_norm)\n        \n        # Compute the second directional derivative (curvature)\n        curvature = (loss_plus - 2 * loss_center + loss_minus) / (epsilon**2)\n        \n        curvature_results.append(curvature)\n\n    # Format the results for the final output\n    # Each value is rounded to exactly 6 decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in curvature_results]\n    \n    # Print the final output in the specified format\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Run the solver\nsolve()\n```"
        }
    ]
}