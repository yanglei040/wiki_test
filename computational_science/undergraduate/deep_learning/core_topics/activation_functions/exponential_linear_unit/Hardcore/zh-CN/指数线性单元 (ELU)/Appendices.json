{
    "hands_on_practices": [
        {
            "introduction": "在有效使用一个激活函数之前，我们必须理解其基本的前向传播特性。这个练习 () 将引导你像一名测试工程师一样思考，设计一个最小的输入集合来探测指数线性单元（ELU）的两个关键属性：其在负值区域的饱和行为以及在原点的连续性。通过这个实践，你将对该函数的形状及其数据转换方式建立起更敏锐的直觉。",
            "id": "3185350",
            "problem": "使用单神经元层来研究带指数线性单元（ELU）激活函数的前向传播。该神经元接收一个标量输入 $x$，通过标准前向规则 $z = w x + b$ 计算预激活值 $z$，并输出 $y = \\phi(z)$，其中 $\\phi$ 是参数为 $\\alpha > 0$ 的 ELU 函数，定义如下\n$$\n\\phi(z) = \\begin{cases}\nz,  z > 0 \\\\\n\\alpha(\\exp(z) - 1),  z \\le 0\n\\end{cases}\n$$\n假设 $w = 1$ 且 $b = 0$，因此对于所有输入都有 $z = x$。当 $\\alpha$ 很大时（例如 $\\alpha = 100$），您想要设计一小组测试输入，以经验性地探究两种不同的前向行为：(i) 负区饱和，以及 (ii) 前向映射在 $x = 0$ 处的连续性。仅使用上述前向传播的定义和连续性的定义（极限等于在该点的函数值），选择一个既提出了合适的输入集，又正确陈述了当 $\\alpha$ 很大时在 $x = 0$ 处连续性的分析事实的选项。\n\nA. 使用输入 $\\{ -10^{1}, -10^{-6}, 0, 10^{-6}, 10^{1} \\}$ 来探究极端负区域和 $0$ 点两侧的紧邻域；对于任何 $\\alpha > 0$（包括大的 $\\alpha$），ELU 前向映射在 $x = 0$ 处都是连续的。\n\nB. 使用输入 $\\{ 10^{3}, 10^{4} \\}$ 因为非常大的正输入足以观察到负饱和；当 $\\alpha$ 很大时，ELU 前向映射在 $x = 0$ 处不连续。\n\nC. 使用输入 $\\{ -10^{2}, -10^{1}, -1 \\}$ 来揭示负饱和，并使用 $\\{ -10^{-6}, 10^{-6} \\}$ 来检查 $0$ 附近的行为（无需包含 $0$ 本身）；只要 $\\alpha \\ne 1$，ELU 前向映射在 $x = 0$ 处就有一个可去间断点。\n\nD. 使用输入 $\\{ -10^{6} \\}$ 可以看到当 $x \\to -\\infty$ 时 ELU 饱和到 $-\\infty$；ELU 前向映射仅在 $\\alpha$ 足够小时才在 $x = 0$ 处连续。",
            "solution": "用户希望对一个关于使用指数线性单元（ELU）激活函数的单个神经元的前向传播行为的问题进行批判性分析。\n\n首先，我将验证问题陈述。\n\n### 第1步：提取已知条件\n- 一个带有标量输入 $x$ 的单神经元层。\n- 预激活函数：$z = w x + b$。\n- 激活函数（输出）：$y = \\phi(z)$，其中 $\\phi$ 是 ELU 函数。\n- ELU 函数定义：\n$$\n\\phi(z) = \\begin{cases}\nz,  z > 0 \\\\\n\\alpha(\\exp(z) - 1),  z \\le 0\n\\end{cases}\n$$\n- 参数约束：$\\alpha > 0$。\n- 特定神经元参数：权重 $w = 1$ 和偏置 $b = 0$。\n- 参数的推论：对于所有输入，$z = x$。输出为 $y = \\phi(x)$。\n- 需要考虑的一个特定条件：$\\alpha$ 很大（例如 $\\alpha = 100$）。\n- 任务是选择一个选项，该选项正确地提出了一组测试输入，以经验性地探究两种行为：(i) 负区饱和，和 (ii) 前向映射在 $x = 0$ 处的连续性。该选项还必须正确陈述关于在 $x = 0$ 处连续性的分析事实。\n\n### 第2步：使用提取的已知条件进行验证\n1.  **科学或事实的合理性**：该问题在科学上是合理的。ELU 函数是深度学习和机器学习中的标准激活函数。其定义是正确的。前向传播、饱和和数学连续性的概念是基础性的，并且被正确地引用。\n2.  **适定性**：该问题是适定的。函数 $\\phi(x)$ 被明确定义，并且要研究的性质（饱和和连续性）是标准的数学概念。问题要求一个合适的测试点集和一个正确的分析陈述，这使得在选项中存在唯一正确的答案。\n3.  **客观性**：问题以客观的数学语言陈述。没有主观或模棱两可的术语。\n4.  **完整性和一致性**：问题陈述是自洽的。所有必要的定义和参数（$w$, $b$, $\\phi(z)$, $\\alpha>0$）都已提供。简化 $z = x$ 被明确说明。没有矛盾之处。\n\n### 第3步：结论和行动\n问题陈述是有效的。这是一个定义明确的练习，旨在分析标准激活函数的性质。我现在将开始解决问题并评估选项。\n\n### 求解推导\n\n给定 $w=1$ 和 $b=0$，预激活值为 $z=x$，输出为 $y = \\phi(x)$。函数为：\n$$\ny = \\phi(x) = \\begin{cases}\nx,  x > 0 \\\\\n\\alpha(\\exp(x) - 1),  x \\le 0\n\\end{cases}\n$$\n我们需要分析两个性质：负区饱和和在 $x=0$ 处的连续性。\n\n**1. 负区饱和**\n负区的饱和指的是当函数输入趋向于 $-\\infty$ 时的行为。我们需要计算当 $x \\to -\\infty$ 时 $\\phi(x)$ 的极限。\n对于 $x  0$，函数为 $\\phi(x) = \\alpha(\\exp(x) - 1)$。\n极限是：\n$$\n\\lim_{x \\to -\\infty} \\phi(x) = \\lim_{x \\to -\\infty} \\alpha\\left(\\exp(x) - 1\\right)\n$$\n因为 $\\lim_{x \\to -\\infty} \\exp(x) = 0$，我们有：\n$$\n\\lim_{x \\to -\\infty} \\phi(x) = \\alpha(0 - 1) = -\\alpha\n$$\n当 $x$ 在负方向上变得非常大时，函数 $\\phi(x)$ 饱和到常数值 $-\\alpha$。为了经验性地探究这一点，我们需要选择大的负值作为输入 $x$，例如 $x = -10$ 或 $x = -100$。对于这样的输入，$\\exp(x)$ 将非常接近 $0$，而 $\\phi(x)$ 将非常接近 $-\\alpha$。\n\n**2. 在 $x=0$ 处的连续性**\n要使一个函数在点 $c$ 处连续，必须满足三个条件：\n(i) $f(c)$ 有定义。\n(ii) $\\lim_{x \\to c} f(x)$ 存在。\n(iii) $\\lim_{x \\to c} f(x) = f(c)$。\n\n让我们检查 $\\phi(x)$ 在 $x=0$ 处是否满足这些条件。\n\n(i) 函数在 $x=0$ 处的值由分段函数的第二种情况定义：\n$$\n\\phi(0) = \\alpha\\left(\\exp(0) - 1\\right) = \\alpha(1 - 1) = 0\n$$\n\n(ii) 为了检查极限是否存在，我们必须评估左极限和右极限，看它们是否相等。\n右极限（当 $x$ 从正侧接近 $0$，$x > 0$）：\n$$\n\\lim_{x \\to 0^+} \\phi(x) = \\lim_{x \\to 0^+} x = 0\n$$\n左极限（当 $x$ 从负侧接近 $0$，$x  0$）：\n$$\n\\lim_{x \\to 0^-} \\phi(x) = \\lim_{x \\to 0^-} \\alpha\\left(\\exp(x) - 1\\right) = \\alpha\\left(\\exp(0) - 1\\right) = \\alpha(1 - 1) = 0\n$$\n由于左极限（$0$）等于右极限（$0$），所以极限存在且等于 $0$。\n$$\n\\lim_{x \\to 0} \\phi(x) = 0\n$$\n\n(iii) 我们将极限与函数值进行比较：\n$$\n\\lim_{x \\to 0} \\phi(x) = 0 \\quad \\text{和} \\quad \\phi(0) = 0\n$$\n因为 $\\lim_{x \\to 0} \\phi(x) = \\phi(0)$，所以函数 $\\phi(x)$ 在 $x=0$ 处是连续的。\n关键是，这个结果对**任何** $\\alpha > 0$ 的值都成立。$\\alpha$ 的大小不影响函数在 $x=0$ 处的连续性，尽管它确实影响函数在该点的导数。\n\n为了经验性地探究在 $x=0$ 处的连续性，一个合适的输入集应包括来自两侧非常接近 $0$ 的点，例如 $-10^{-6}$ 和 $10^{-6}$，以及 $0$ 点本身。\n\n**适合的测试集总结：**\n- 测试饱和：至少一个大的负数（例如 $-10^1$）。\n- 测试在 $0$ 处的连续性：围绕 $0$ 的一组紧凑点（例如 $-10^{-6}, 0, 10^{-6}$）。\n- 测试正线性区域：一个正数（例如 $10^1$）。\n一个全面的集合将是 $\\{ -10^1, -10^{-6}, 0, 10^{-6}, 10^1 \\}$。\n\n### 逐项分析选项\n\n**A. 使用输入 $\\{ -10^{1}, -10^{-6}, 0, 10^{-6}, 10^{1} \\}$ 来探究极端负区域和 $0$ 点两侧的紧邻域；对于任何 $\\alpha > 0$（包括大的 $\\alpha$），ELU 前向映射在 $x = 0$ 处都是连续的。**\n- **输入集：** 建议的输入集非常出色。输入 $-10^1$ 探究了负饱和区域。输入 $-10^{-6}$、$0$ 和 $10^{-6}$ 构成了围绕 $0$ 的一个紧邻域，以经验性地检查连续性。输入 $10^1$ 测试了正输入的线性行为。这个集合是为所述目标精心设计的。\n- **连续性陈述：** 关于“对于任何 $\\alpha > 0$（包括大的 $\\alpha$），ELU 前向映射在 $x = 0$ 处都是连续的”的陈述在分析上是正确的，如上所证。\n- **结论：** 正确。\n\n**B. 使用输入 $\\{ 10^{3}, 10^{4} \\}$ 因为非常大的正输入足以观察到负饱和；当 $\\alpha$ 很大时，ELU 前向映射在 $x = 0$ 处不连续。**\n- **输入集：** 建议的输入是大的正数。它们完全没有提供关于负区域（$x \\le 0$）或饱和行为的任何信息。其理由是荒谬的。\n- **连续性陈述：** 关于当 $\\alpha$ 很大时函数“在 $x = 0$ 处不连续”的陈述是错误的。我们证明了对于所有 $\\alpha > 0$ 都是连续的。\n- **结论：** 不正确。\n\n**C. 使用输入 $\\{ -10^{2}, -10^{1}, -1 \\}$ 来揭示负饱和，并使用 $\\{ -10^{-6}, 10^{-6} \\}$ 来检查 $0$ 附近的行为（无需包含 $0$ 本身）；只要 $\\alpha \\ne 1$，ELU 前向映射在 $x = 0$ 处就有一个可去间断点。**\n- **输入集：** 建议的输入对于探究指定的行为是合理的。\n- **连续性陈述：** 关于函数“只要 $\\alpha \\ne 1$，在 $x = 0$ 处就有一个可去间断点”的陈述是错误的。可去间断点发生在极限存在但不等于函数值的情况下。在这里，极限存在且等于函数值（$0$），意味着函数是连续的。条件 $\\alpha \\ne 1$ 与*导数* $\\phi'(x)$ 在 $x=0$ 处的连续性有关，但问题是关于函数 $\\phi(x)$ 本身的连续性。\n- **结论：** 不正确。\n\n**D. 使用输入 $\\{ -10^{6} \\}$ 可以看到当 $x \\to -\\infty$ 时 ELU 饱和到 $-\\infty$；ELU 前向映射仅在 $\\alpha$ 足够小时才在 $x = 0$ 处连续。**\n- **输入集：** 单个输入点不足以同时测试饱和和在 $0$ 处的连续性。\n- **饱和陈述：** 声称“ELU饱和到 $-\\infty$”是错误的。如所推导，函数饱和到有限值 $-\\alpha$。\n- **连续性陈述：** 声称在 $x = 0$ 处的连续性“仅在 $\\alpha$ 足够小时”才成立是错误的。我们证明了对于所有 $\\alpha > 0$ 都是连续的。\n- **结论：** 不正确。\n\n基于详细分析，选项 A 是唯一一个既提供了合适的实验设计（输入集）又提供了正确分析陈述的选项。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "激活函数的真正效用体现在反向传播过程中，它在其中关键性地塑造了梯度流。本题 () 将让你亲手在一个包含ELU的小型网络中实践多元链式法则。你将计算损失函数关于输入的梯度，并特别关注如何正确处理在零点的不可导“拐点”，这是分段激活函数的一个普遍挑战。",
            "id": "3190277",
            "problem": "考虑一个标量输入 $x \\in \\mathbb{R}$，它被输入到一个参数为 $\\alpha \\in (0, \\infty)$ 的指数线性单元 (ELU) 激活函数 $g(x)$ 中，该函数定义如下\n$$\ng(x) = \n\\begin{cases}\nx,  x > 0 \\\\\n\\alpha(\\exp(x) - 1),  x \\le 0\n\\end{cases}\n$$\n该激活函数扇出到一个双分支网络：\n- 分支 1 计算 $y_{1} = \\sigma\\!\\left(w_{1}\\,g(x) + b_{1}\\right)$，其中 $\\sigma(z)$ 是 logistic sigmoid 函数，$\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$，并且 $w_{1}, b_{1} \\in \\mathbb{R}$ 是固定参数。\n- 分支 2 计算 $y_{2} = \\left(g(x)\\right)^{2}$。\n\n标量损失是平方误差和的一半\n$$\nL = \\frac{1}{2}\\left(y_{1} - t_{1}\\right)^{2} + \\frac{1}{2}\\left(y_{2} - t_{2}\\right)^{2},\n$$\n其中 $t_{1}, t_{2} \\in \\mathbb{R}$ 是固定目标值。\n\n仅使用导数的基本定义和多元链式法则，通过在两个分支上传播导数，推导出当 $x > 0$ 和 $x  0$ 时 $\\frac{\\partial L}{\\partial x}$ 的反向传播表达式。然后，使用次梯度计算在拐点 $x = 0$ 处的 $\\frac{\\partial L}{\\partial x}$。对于不可微点 $x = 0$，采用反向传播选择 ELU 右导数的约定（即，取 $\\frac{d g}{d x}\\big|_{x=0}$ 等于 $x > 0$ 时的导数）。请用 $w_{1}$、$b_{1}$、$t_{1}$ 和标准函数表示在 $x = 0$ 处 $\\frac{\\partial L}{\\partial x}$ 的最终表达式。无需进行数值舍入。",
            "solution": "该问题是有效的。这是一个关于微积分及其在神经网络中应用的适定问题，是深度学习中的一个标准课题。该问题具有科学依据、自洽且客观。为处理 ELU 激活函数的不可微点所提供的约定使得问题明确无歧义。\n\n目标是求出标量损失 $L$ 关于标量输入 $x$ 的导数，记作 $\\frac{\\partial L}{\\partial x}$。损失 $L$ 通过一系列涉及激活函数 $g(x)$ 的中间计算依赖于 $x$。总的依赖关系图如下：\n$x \\rightarrow g(x)$\n$g(x) \\rightarrow y_{1} \\rightarrow L$\n$g(x) \\rightarrow y_{2} \\rightarrow L$\n\n全导数 $\\frac{\\partial L}{\\partial x}$ 可以使用多元链式法则求得。由于输入 $x$ 通过单一中间变量 $g(x)$ 影响损失 $L$，然后该变量再分支出去，因此链式法则可以组织为：\n$$\n\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial g(x)} \\frac{d g(x)}{d x}\n$$\n第一项 $\\frac{\\partial L}{\\partial g(x)}$ 包含了源于 $g(x)$ 的两个分支。它是沿每条路径的偏导数之和：\n$$\n\\frac{\\partial L}{\\partial g(x)} = \\frac{\\partial L}{\\partial y_{1}} \\frac{\\partial y_{1}}{\\partial g(x)} + \\frac{\\partial L}{\\partial y_{2}} \\frac{\\partial y_{2}}{\\partial g(x)}\n$$\n我们将计算该表达式的每个分量。\n\n首先，我们计算损失 $L$ 关于其直接输入 $y_{1}$ 和 $y_{2}$ 的导数。\n损失为 $L = \\frac{1}{2}\\left(y_{1} - t_{1}\\right)^{2} + \\frac{1}{2}\\left(y_{2} - t_{2}\\right)^{2}$。\n偏导数为：\n$$\n\\frac{\\partial L}{\\partial y_{1}} = y_{1} - t_{1}\n$$\n$$\n\\frac{\\partial L}{\\partial y_{2}} = y_{2} - t_{2}\n$$\n\n接下来，我们计算 $y_{1}$ 和 $y_{2}$ 关于 $g(x)$ 的导数。\n对于分支 1，$y_{1} = \\sigma\\!\\left(w_{1}\\,g(x) + b_{1}\\right)$。令 $z_{1} = w_{1}\\,g(x) + b_{1}$。logistic sigmoid 函数 $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ 的导数是 $\\frac{d\\sigma}{dz} = \\sigma(z)(1 - \\sigma(z))$。\n对 $y_{1}$ 使用链式法则：\n$$\n\\frac{\\partial y_{1}}{\\partial g(x)} = \\frac{d\\sigma(z_{1})}{dz_{1}} \\frac{\\partial z_{1}}{\\partial g(x)} = \\sigma(z_{1})(1 - \\sigma(z_{1})) \\cdot w_{1} = y_{1}(1 - y_{1})w_{1}\n$$\n对于分支 2，$y_{2} = \\left(g(x)\\right)^{2}$。其导数为：\n$$\n\\frac{\\partial y_{2}}{\\partial g(x)} = 2 g(x)\n$$\n\n现在我们组合这些部分来求 $\\frac{\\partial L}{\\partial g(x)}$：\n$$\n\\frac{\\partial L}{\\partial g(x)} = (y_{1} - t_{1}) \\cdot [y_{1}(1 - y_{1})w_{1}] + (y_{2} - t_{2}) \\cdot [2 g(x)]\n$$\n$$\n\\frac{\\partial L}{\\partial g(x)} = w_{1}(y_{1} - t_{1})y_{1}(1 - y_{1}) + 2(y_{2} - t_{2})g(x)\n$$\n\n最后一步是确定 ELU 激活函数 $\\frac{d g(x)}{d x}$ 的导数。这取决于 $x$ 的值。\nELU 函数定义为：\n$$\ng(x) = \n\\begin{cases}\nx,  x > 0 \\\\\n\\alpha(\\exp(x) - 1),  x \\le 0\n\\end{cases}\n$$\n- 当 $x > 0$ 时：$g(x) = x$，所以 $\\frac{d g}{d x} = 1$。\n- 当 $x  0$ 时：$g(x) = \\alpha(\\exp(x) - 1)$，所以 $\\frac{d g}{d x} = \\alpha \\exp(x)$。\n- 当 $x = 0$ 时：函数在此点（一个“拐点”）不可微，因为左导数是 $\\alpha \\exp(0) = \\alpha$，而右导数是 $1$。问题指定了一个约定：“采用反向传播选择右导数的约定”。因此，我们必须使用：\n$$\n\\frac{d g}{d x}\\bigg|_{x=0} = 1\n$$\n\n题目要求我们求出在 $x = 0$ 处 $\\frac{\\partial L}{\\partial x}$ 的最终表达式。我们计算在 $x=0$ 处的所有中间量：\n首先，在 $x=0$ 处，激活函数的值是：\n$$\ng(0) = \\alpha(\\exp(0) - 1) = \\alpha(1-1) = 0\n$$\n接下来，我们计算在 $x=0$ 处的 $y_{1}$ 和 $y_{2}$：\n$$\ny_{1}\\big|_{x=0} = \\sigma(w_{1} g(0) + b_{1}) = \\sigma(w_{1}(0) + b_{1}) = \\sigma(b_{1})\n$$\n$$\ny_{2}\\big|_{x=0} = (g(0))^{2} = 0^{2} = 0\n$$\n现在我们可以计算在 $x=0$ 处的梯度项 $\\frac{\\partial L}{\\partial g(x)}$：\n$$\n\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0} = w_{1}(y_{1}\\big|_{x=0} - t_{1})y_{1}\\big|_{x=0}(1 - y_{1}\\big|_{x=0}) + 2(y_{2}\\big|_{x=0} - t_{2})g(0)\n$$\n代入我们刚求出的值：\n$$\n\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0} = w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1})) + 2(0 - t_{2})(0)\n$$\n第二项消失：\n$$\n\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0} = w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1}))\n$$\n最后，我们使用主链式法则公式和为 $\\frac{d g}{d x}$ 指定的约定来计算在 $x=0$ 处的 $\\frac{\\partial L}{\\partial x}$：\n$$\n\\frac{\\partial L}{\\partial x}\\bigg|_{x=0} = \\left(\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0}\\right) \\left(\\frac{d g}{d x}\\bigg|_{x=0}\\right)\n$$\n$$\n\\frac{\\partial L}{\\partial x}\\bigg|_{x=0} = \\left( w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1})) \\right) \\cdot 1\n$$\n$$\n\\frac{\\partial L}{\\partial x}\\bigg|_{x=0} = w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1}))\n$$\n此表达式按要求用所需参数 $w_{1}$、$b_{1}$、$t_{1}$ 和标准 sigmoid 函数 $\\sigma$ 表示。注意，$t_{2}$ 和 $\\alpha$ 没有出现在 $x=0$ 处导数的最终表达式中。",
            "answer": "$$\n\\boxed{w_{1}\\left(\\sigma(b_{1}) - t_{1}\\right)\\sigma(b_{1})\\left(1 - \\sigma(b_{1})\\right)}\n$$"
        },
        {
            "introduction": "为了进一步加深理解，我们可以不将ELU的参数 $\\alpha$ 视为一个固定的超参数，而是将其作为模型自身的一个可学习参数。这个练习 () 将指导你从第一性原理出发，推导损失函数关于 $\\alpha$ 的梯度，这是对其进行优化的关键步骤。此外，本题还引入了参数可辨识性这一重要概念，探讨模型架构的选择如何影响所学得解的唯一性。",
            "id": "3123807",
            "problem": "要求您实现并分析一个使用指数线性单元 (ELU) 的单个隐藏层，其中 ELU 参数是每层的可学习标量。该分析必须基于 ELU 函数的基本定义和导数的链式法则。\n\n考虑一个具有激活前的值 $z \\in \\mathbb{R}^n$ 的单个隐藏层，该层通过一个带有层特定可学习参数 $\\alpha \\in \\mathbb{R}$ 的指数线性单元 (ELU) 激活函数 $f(x;\\alpha)$。ELU 函数分段定义为：当 $x \\ge 0$ 时，$f(x;\\alpha) = x$；当 $x  0$ 时，$f(x;\\alpha) = \\alpha (\\exp(x) - 1)$。该层的输出随后被传递到一个一维线性输出 $y_{\\text{hat}} = w^\\top f(z;\\alpha) + b$，其中权重为 $w \\in \\mathbb{R}^n$，偏置为 $b \\in \\mathbb{R}$。损失函数为均方误差 (MSE) $L = \\frac{1}{2}(y_{\\text{hat}} - y)^2$，目标值为 $y \\in \\mathbb{R}$。\n\n任务：\n- 使用导数的链式法则，从第一性原理推导出梯度 $\\partial L / \\partial \\alpha$ 的闭式表达式，该表达式依赖于 $z$、$w$、$y_{\\text{hat}}$、$y$ 和 $\\alpha$。推导过程必须从 $f(x;\\alpha)$、$y_{\\text{hat}}$ 和 $L$ 的定义开始，并应用链式法则，不使用任何快捷公式。\n- 实现一个完整的程序，该程序：\n  1. 对给定的输入，计算前向传播结果 $y_{\\text{hat}}$、损失 $L$ 和解析梯度 $\\partial L / \\partial \\alpha$。\n  2. 将解析梯度与通过对 $\\alpha$ 应用中心有限差分计算出的数值梯度进行验证。\n  3. 检验在层缩放情况下 $\\alpha$ 的可识别性：讨论并测试当将 $\\alpha$ 缩放因子 $s$ 并同时将后续线性层的权重 $w$ 缩放 $1/s$ 时，何时会产生相同的输出，何时不会。可识别性指的是对于给定的输入，不同的参数设置是否会产生不可区分的模型输出。\n  4. 覆盖 $x=0$ 处的边界行为以及激活前的值全为正的情况。\n\n使用以下特定的测试套件：\n- 用于梯度检查的混合符号情况：\n  - $z_{\\text{mixed}} = [-1.2, 0.8, -0.3]$，$\\alpha = 1.3$，$w = [0.9, -1.1, 0.5]$，$b = 0.2$，$y = 0.7$，有限差分步长 $\\varepsilon = 10^{-6}$。\n  - 报告解析梯度和数值梯度之间的相对误差，该误差为一个浮点数，计算公式为 $\\frac{|\\text{analytical} - \\text{numerical}|}{\\max(10^{-12}, |\\text{analytical}| + |\\text{numerical}|)}$。\n- 零点边界情况：\n  - $z_{0} = [0.0]$，$\\alpha = 1.0$，$w = [1.0]$，$b = 0.0$，$y = 0.0$。\n  - 报告解析梯度 $\\partial L / \\partial \\alpha$（浮点数）。\n- 激活前的值全为负时的可识别性：\n  - $z_{\\text{neg}} = [-1.0, -0.5]$，$\\alpha = 0.7$，$w = [1.2, -0.8]$，$b = 0.3$，缩放因子 $s = 2.5$。\n  - 报告一个布尔值，指示 $w^\\top f(z_{\\text{neg}};\\alpha) + b$ 是否在 $10^{-12}$ 的容差范围内等于 $(w/s)^\\top f(z_{\\text{neg}}; s\\alpha) + b$。\n- 混合符号时可识别性失效：\n  - $z_{\\text{mix2}} = [-1.0, 0.5]$，使用与上述情况相同的 $\\alpha$、$w$、$b$ 和 $s$。\n  - 报告一个布尔值，指示 $w^\\top f(z_{\\text{mix2}};\\alpha) + b$ 是否在 $10^{-12}$ 的容差范围内等于 $(w/s)^\\top f(z_{\\text{mix2}}; s\\alpha) + b$。\n- 激活前的值全为正的情况：\n  - $z_{\\text{pos}} = [0.4, 0.2]$，$\\alpha = 1.7$，$w = [0.6, -0.3]$，$b = -0.1$，$y = 0.5$。\n  - 报告解析梯度 $\\partial L / \\partial \\alpha$（浮点数）。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[result_1,result_2,result_3,result_4,result_5]$），其顺序与上述测试套件中指定的顺序完全一致：混合情况的相对误差（浮点数）、零点边界情况的梯度（浮点数）、全为负的可识别性布尔值、混合符号的可识别性布尔值、全为正情况的梯度（浮点数）。不应打印任何其他文本。",
            "solution": "该问题是有效的。它在科学上基于神经网络理论的原理，在数学上是适定的，并且陈述客观。为得出唯一解所需的所有必要数据和定义均已提供。\n\n### 1. 梯度 $\\partial L / \\partial \\alpha$ 的推导\n\n主要任务是推导损失函数 $L$ 相对于指数线性单元 (ELU) 参数 $\\alpha$ 的梯度的闭式表达式。此推导从所提供的定义出发，并应用微积分的链式法则。\n\n相关量如下：\n- ELU 激活函数，为标量输入 $x$ 和参数 $\\alpha$ 定义：\n$$\nf(x; \\alpha) = \\begin{cases}\nx,  x \\ge 0 \\\\\n\\alpha (\\exp(x) - 1),  x  0\n\\end{cases}\n$$\n- 模型的输出预测 $y_{\\text{hat}}$，对于激活前向量 $z \\in \\mathbb{R}^n$、权重向量 $w \\in \\mathbb{R}^n$ 和标量偏置 $b$：\n$$\ny_{\\text{hat}} = w^\\top f(z; \\alpha) + b = \\sum_{i=1}^n w_i f(z_i; \\alpha) + b\n$$\n其中 $f(z; \\alpha)$ 按元素应用于向量 $z$。\n- 均方误差 (MSE) 损失函数，对于目标值 $y$：\n$$\nL = \\frac{1}{2}(y_{\\text{hat}} - y)^2\n$$\n\n我们通过应用链式法则来计算 $\\frac{\\partial L}{\\partial \\alpha}$：\n$$\n\\frac{\\partial L}{\\partial \\alpha} = \\frac{\\partial L}{\\partial y_{\\text{hat}}} \\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha}\n$$\n\n首先，我们计算损失 $L$ 相对于预测值 $y_{\\text{hat}}$ 的导数：\n$$\n\\frac{\\partial L}{\\partial y_{\\text{hat}}} = \\frac{\\partial}{\\partial y_{\\text{hat}}} \\left( \\frac{1}{2}(y_{\\text{hat}} - y)^2 \\right) = y_{\\text{hat}} - y\n$$\n\n接下来，我们计算预测值 $y_{\\text{hat}}$ 相对于参数 $\\alpha$ 的导数。\n$$\n\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left( \\sum_{i=1}^n w_i f(z_i; \\alpha) + b \\right)\n$$\n由于微分是线性算子，且 $b$ 不是 $\\alpha$ 的函数：\n$$\n\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha} = \\sum_{i=1}^n w_i \\frac{\\partial f(z_i; \\alpha)}{\\partial \\alpha}\n$$\n现在，我们必须求出 ELU 函数 $f(x; \\alpha)$ 相对于 $\\alpha$ 的导数。我们考虑其定义中的两种情况：\n- 如果 $x \\ge 0$，则 $f(x; \\alpha) = x$。相对于 $\\alpha$ 的导数是：\n$$\n\\frac{\\partial f(x; \\alpha)}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha}(x) = 0\n$$\n- 如果 $x  0$，则 $f(x; \\alpha) = \\alpha (\\exp(x) - 1)$。相对于 $\\alpha$ 的导数是：\n$$\n\\frac{\\partial f(x; \\alpha)}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} (\\alpha (\\exp(x) - 1)) = \\exp(x) - 1\n$$\n这可以使用指示函数 $I(c)$ 紧凑地表示，如果条件 $c$ 为真，则 $I(c)$ 为 1，否则为 0：\n$$\n\\frac{\\partial f(x; \\alpha)}{\\partial \\alpha} = (\\exp(x) - 1) \\cdot I(x  0)\n$$\n注意，这个导数在 $x=0$ 处是良定义且连续的，因为 $\\lim_{x\\to0^-}(\\exp(x)-1) = 0$，这与 $x \\ge 0$ 时的导数相匹配。\n\n将其代回到 $\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha}$ 的表达式中：\n$$\n\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha} = \\sum_{i=1}^n w_i (\\exp(z_i) - 1) \\cdot I(z_i  0)\n$$\n这个和仅包含激活前的值 $z_i$ 为负的项。\n\n最后，我们将各部分组合起来，得到 $\\frac{\\partial L}{\\partial \\alpha}$ 的完整表达式：\n$$\n\\frac{\\partial L}{\\partial \\alpha} = (y_{\\text{hat}} - y) \\left( \\sum_{i=1}^n w_i (\\exp(z_i) - 1) \\cdot I(z_i  0) \\right)\n$$\n这就是梯度的闭式表达式，它仅依赖于给定的变量 $z, w, y_{\\text{hat}}, y$ 和 $\\alpha$。\n\n### 2. 特殊情况和可识别性分析\n\n- **激活前的值全为正或零（对所有 $i$ 都有 $z_i \\ge 0$）**：在这种情况下，对所有 $i$ 都有 $I(z_i  0) = 0$。梯度表达式中的和变为零。因此，$\\frac{\\partial L}{\\partial \\alpha} = 0$。这是合乎逻辑的，因为对于非负输入，ELU 函数表现为恒等函数，$f(z_i; \\alpha) = z_i$，这与 $\\alpha$ 无关。因此，模型的输出 $y_{\\text{hat}}$ 和损失 $L$ 不依赖于 $\\alpha$，其梯度必须为零。这在边界情况（$z=[0.0]$）和全为正的情况下进行了测试。\n\n- **缩放下的可识别性**：我们分析将 $\\alpha$ 缩放因子 $s$ 并将 $w$ 缩放 $1/s$ 是否会使模型输出保持不变。设新参数为 $\\alpha' = s\\alpha$ 和 $w' = w/s$。新输出为 $y'_{\\text{hat}} = (w')^\\top f(z; \\alpha') + b$。\n\n- **情况 A：激活前的值全为负（对所有 $i$ 都有 $z_i  0$）**：\n对于任何分量 $i$，$f(z_i; \\alpha) = \\alpha(\\exp(z_i)-1)$。原始输出为：\n$$y_{\\text{hat}} = \\sum_i w_i \\alpha (\\exp(z_i)-1) + b$$\n新输出为：\n$$y'_{\\text{hat}} = \\sum_i w'_i f(z_i; \\alpha') + b = \\sum_i \\frac{w_i}{s} \\left( (s\\alpha)(\\exp(z_i)-1) \\right) + b = \\sum_i w_i \\alpha (\\exp(z_i)-1) + b$$\n在这里，$y'_{\\text{hat}} = y_{\\text{hat}}$。模型是不可识别的，因为对于任何具有全为负分量的输入 $z$，不同的参数集 $(\\alpha, w)$ 和 $(s\\alpha, w/s)$ 会产生相同的输出。\n\n- **情况 B：混合符号的激活前的值**：\n假设某些 $z_j \\ge 0$ 且某些 $z_k  0$。原始输出为：\n$$y_{\\text{hat}} = \\sum_{i \\mid z_i \\ge 0} w_i z_i + \\sum_{i \\mid z_i  0} w_i \\alpha(\\exp(z_i)-1) + b$$\n新输出为：\n$$y'_{\\text{hat}} = \\sum_{i \\mid z_i \\ge 0} w'_i z_i + \\sum_{i \\mid z_i  0} w'_i \\alpha'(\\exp(z_i)-1) + b$$\n$$y'_{\\text{hat}} = \\sum_{i \\mid z_i \\ge 0} \\frac{w_i}{s} z_i + \\sum_{i \\mid z_i  0} \\frac{w_i}{s} (s\\alpha)(\\exp(z_i)-1) + b$$\n$$y'_{\\text{hat}} = \\sum_{i \\mid z_i \\ge 0} \\frac{w_i}{s} z_i + \\sum_{i \\mid z_i  0} w_i \\alpha(\\exp(z_i)-1) + b$$\n比较 $y_{\\text{hat}}$ 和 $y'_{\\text{hat}}$，对应于负值 $z_i$ 的项是相同的，但对应于非负值 $z_i$ 的项被缩放了 $1/s$。除非 $s=1$ 或对于非负部分所有 $w_i z_i=0$，$y'_{\\text{hat}} \\neq y_{\\text{hat}}$。在这种情况下，参数是可识别的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes results for all test cases as specified in the problem statement.\n    \"\"\"\n    \n    def elu(z, alpha):\n        \"\"\"Computes the element-wise ELU activation.\"\"\"\n        return np.where(z >= 0, z, alpha * (np.exp(z) - 1))\n\n    def compute_forward_loss(z, alpha, w, b, y):\n        \"\"\"Computes the forward pass and loss.\"\"\"\n        activations = elu(z, alpha)\n        y_hat = np.dot(w, activations) + b\n        loss = 0.5 * (y_hat - y)**2\n        return y_hat, loss\n\n    def analytical_gradient(z, alpha, w, y_hat, y):\n        \"\"\"Computes the analytical gradient dL/d(alpha).\"\"\"\n        # Derivative of ELU with respect to alpha\n        d_elu_d_alpha = np.where(z >= 0, 0.0, np.exp(z) - 1)\n        \n        # Derivative of y_hat with respect to alpha\n        d_yhat_d_alpha = np.dot(w, d_elu_d_alpha)\n        \n        # Derivative of Loss with respect to y_hat\n        d_L_d_yhat = y_hat - y\n        \n        # Final gradient using the chain rule\n        d_L_d_alpha = d_L_d_yhat * d_yhat_d_alpha\n        return d_L_d_alpha\n\n    def numerical_gradient(z, w, b, y, alpha, epsilon):\n        \"\"\"Computes the numerical gradient using central finite differences.\"\"\"\n        _, loss_plus = compute_forward_loss(z, alpha + epsilon, w, b, y)\n        _, loss_minus = compute_forward_loss(z, alpha - epsilon, w, b, y)\n        return (loss_plus - loss_minus) / (2 * epsilon)\n\n    results = []\n\n    # Test Case 1: Mixed-sign case for gradient check\n    z_mixed = np.array([-1.2, 0.8, -0.3])\n    alpha_mixed = 1.3\n    w_mixed = np.array([0.9, -1.1, 0.5])\n    b_mixed = 0.2\n    y_mixed = 0.7\n    epsilon = 1e-6\n    \n    y_hat_mixed, _ = compute_forward_loss(z_mixed, alpha_mixed, w_mixed, b_mixed, y_mixed)\n    grad_analyt = analytical_gradient(z_mixed, alpha_mixed, w_mixed, y_hat_mixed, y_mixed)\n    grad_numer = numerical_gradient(z_mixed, w_mixed, b_mixed, y_mixed, alpha_mixed, epsilon)\n    \n    rel_error_num = np.abs(grad_analyt - grad_numer)\n    rel_error_den = np.maximum(1e-12, np.abs(grad_analyt) + np.abs(grad_numer))\n    relative_error = rel_error_num / rel_error_den\n    results.append(relative_error)\n    \n    # Test Case 2: Boundary case at zero\n    z_0 = np.array([0.0])\n    alpha_0 = 1.0\n    w_0 = np.array([1.0])\n    b_0 = 0.0\n    y_0 = 0.0\n    \n    y_hat_0, _ = compute_forward_loss(z_0, alpha_0, w_0, b_0, y_0)\n    grad_at_zero = analytical_gradient(z_0, alpha_0, w_0, y_hat_0, y_0)\n    results.append(grad_at_zero)\n    \n    # Test Case 3: Identifiability with all-negative pre-activations\n    z_neg = np.array([-1.0, -0.5])\n    alpha_neg = 0.7\n    w_neg = np.array([1.2, -0.8])\n    b_neg = 0.3\n    s_neg = 2.5\n    \n    y_hat_orig_neg, _ = compute_forward_loss(z_neg, alpha_neg, w_neg, b_neg, 0.0)\n    \n    alpha_scaled_neg = s_neg * alpha_neg\n    w_scaled_neg = w_neg / s_neg\n    y_hat_scaled_neg, _ = compute_forward_loss(z_neg, alpha_scaled_neg, w_scaled_neg, b_neg, 0.0)\n    \n    is_identifiable_neg = np.isclose(y_hat_orig_neg, y_hat_scaled_neg, atol=1e-12)\n    results.append(bool(is_identifiable_neg))\n    \n    # Test Case 4: Identifiability fails for mixed signs\n    z_mix2 = np.array([-1.0, 0.5])\n    # The problem specifies to use the same alpha, w, b, s as in the previous case.\n    alpha_mix2 = alpha_neg #0.7\n    w_mix2 = w_neg #[1.2, -0.8]\n    b_mix2 = b_neg #0.3\n    s_mix2 = s_neg # 2.5\n    \n    y_hat_orig_mix2, _ = compute_forward_loss(z_mix2, alpha_mix2, w_mix2, b_mix2, 0.0)\n    \n    alpha_scaled_mix2 = s_mix2 * alpha_mix2\n    w_scaled_mix2 = w_mix2 / s_mix2\n    y_hat_scaled_mix2, _ = compute_forward_loss(z_mix2, alpha_scaled_mix2, w_scaled_mix2, b_mix2, 0.0)\n    \n    is_identifiable_mix2 = np.isclose(y_hat_orig_mix2, y_hat_scaled_mix2, atol=1e-12)\n    # The question is whether they are equal, so non-identifiability means the outputs are different.\n    # The problem asks for a boolean indicating whether the outputs are equal.\n    results.append(bool(is_identifiable_mix2))\n\n    # Test Case 5: All-positive pre-activations\n    z_pos = np.array([0.4, 0.2])\n    alpha_pos = 1.7\n    w_pos = np.array([0.6, -0.3])\n    b_pos = -0.1\n    y_pos = 0.5\n\n    y_hat_pos, _ = compute_forward_loss(z_pos, alpha_pos, w_pos, b_pos, y_pos)\n    grad_all_pos = analytical_gradient(z_pos, alpha_pos, w_pos, y_hat_pos, y_pos)\n    results.append(grad_all_pos)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{results[0]},{results[1]},{results[2]},{results[3]},{results[4]}]\")\n\nsolve()\n```"
        }
    ]
}