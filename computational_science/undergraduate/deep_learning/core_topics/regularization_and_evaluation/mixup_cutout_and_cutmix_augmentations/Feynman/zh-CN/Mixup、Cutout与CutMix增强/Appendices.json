{
    "hands_on_practices": [
        {
            "introduction": "数据增强的一个核心目标是提升模型对训练数据中未出现过的变化的稳健性。本实践将通过一个具体的编码任务，探讨 Cutout 增强技术如何通过在训练中模拟遮挡，来帮助模型学习更鲁棒的特征。你将亲手构建一个合成数据集，并实施一个完整的机器学习实验，以量化 Cutout 对模型在测试时面对真实遮挡时的性能提升效果 。这个练习将让你深入理解数据增强与模型泛化能力之间的直接联系。",
            "id": "3151871",
            "problem": "考虑一个二元分类问题，其中每个输入是一幅合成卫星图像，表示为像素强度的二维数组，标签指示是否存在目标地物。目标是研究“cutout”增强如何影响模型在测试时对模拟云层遮挡的鲁棒性。从二元逻辑回归的经验风险最小化开始：分类器是一个线性函数，其参数为 $w \\in \\mathbb{R}^d$ 和 $b \\in \\mathbb{R}$，通过逻辑函数 $\\sigma(z) = 1/(1 + e^{-z})$ 将输入向量 $x \\in \\mathbb{R}^d$ 映射到一个概率，其中 $z = w^\\top x + b$。在带有 $\\ell_2$ 正则化的二元交叉熵下，经验风险为\n$$\n\\mathcal{L}(w,b) = \\frac{1}{n}\\sum_{i=1}^n \\left[-y_i \\log \\sigma(w^\\top x_i + b) - (1-y_i)\\log(1 - \\sigma(w^\\top x_i + b))\\right] + \\frac{\\lambda}{2}\\|w\\|_2^2.\n$$\n通过 cutout 进行的数据增强被建模为一个随机变换 $T_f$。给定一张图像 $X \\in \\mathbb{R}^{H \\times W}$ 和一个目标面积分数 $f \\in [0,1]$，该变换会通过将单个轴对齐的矩形区域的像素强度设置为零来对其进行遮蔽，该区域的面积约为 $f \\cdot H \\cdot W$。该矩形具有在 $[0.3, 3.0]$ 内均匀随机的宽高比，以及在图像内部完全包含的约束下均匀随机的位置。使用 cutout 进行训练时，会以概率 $p \\in [0,1]$ 将 $T_f$ 独立地应用于每个训练样本。在测试时，通过对每个测试图像确定性地应用 $T_{f_{\\text{test}}}$ 来模拟遮挡（每个图像的矩形形状和位置具有独立的随机性）。\n\n按如下方式构建一个合成数据集。设图像高度 $H = 16$，宽度 $W = 16$，因此 $d = H \\cdot W = 256$。对于每个样本索引 $i \\in \\{1,\\dots,n\\}$，从 $\\{0,1\\}$ 中均匀随机抽取一个标签 $y_i$。从标准差为 $\\sigma = 0.1$ 的零均值正态分布中抽取基底噪声 $N_i \\in \\mathbb{R}^{H \\times W}$，其元素相互独立。施加一个弱的全局类别依赖偏置：对于 $y_i = 1$，将所有 $N_i$ 的像素值加上常数 $g = 0.1$；对于 $y_i = 0$，则从所有像素值中减去 $g$。施加一个强的局部判别性补丁：对于 $y_i = 1$，在左上角 $s \\times s$（其中 $s=6$）的正方形区域加上常数 $a_p = 1.0$；对于 $y_i = 0$，则从该区域减去 $a_p$。将每张图像展平为 $\\mathbb{R}^{256}$ 中的一个向量。这种构造产生了两个冗余的线索：一个强的局部补丁和一个弱的全局偏置。一个未经遮挡训练的模型可能严重依赖局部补丁，而一个使用 cutout 训练的模型在训练期间会经历偶然的遮挡，并可能为全局偏置分配非零权重，这可能在测试时局部补丁被遮挡时提高鲁棒性。\n\n实现一个程序，该程序：\n- 使用上述过程生成一个包含 $N_{\\text{train}} = 2000$ 个样本的训练集和一个包含 $N_{\\text{test}} = 2000$ 个样本的测试集，每个测试用例使用独立的随机种子以确保可复现性。\n- 使用全批量梯度下降法训练两个二元逻辑回归模型，学习率为 $\\eta = 0.1$，$\\ell_2$ 正则化系数为 $\\lambda = 0.001$，优化步数为 $E = 300$：\n  - 一个未经增强训练的基线模型（即 $p = 0$）。\n  - 一个使用 cutout 增强训练的模型，其增强概率为指定的 $p$，增强面积分数等于该用例的测试时遮挡分数 $f_{\\text{test}}$。\n- 在一个已被遮挡的测试集上评估两个模型，该测试集是通过对每个测试样本应用一次 $T_{f_{\\text{test}}}$ 生成的。必须使用相同的遮挡测试集来评估两个模型，以使性能具有可比性。\n- 计算每个模型的准确率，其中准确率是在预测概率上使用 $0.5$ 的阈值时正确分类的测试样本的比例，并报告改进量，即差值 $A_{\\text{aug}} - A_{\\text{base}}$。\n\n使用以下参数值测试套件，其中每个元组为 $(p, f_{\\text{test}}, \\text{seed})$，并且每个数学实体都已指定：\n- 案例 1：$(p = 0.5, f_{\\text{test}} = 0.0, \\text{seed} = 42)$。\n- 案例 2：$(p = 0.5, f_{\\text{test}} = 0.3, \\text{seed} = 123)$。\n- 案例 3：$(p = 0.9, f_{\\text{test}} = 0.5, \\text{seed} = 7)$。\n- 案例 4：$(p = 0.0, f_{\\text{test}} = 0.3, \\text{seed} = 99)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，“[$r_1,r_2,r_3,r_4$]”）。每个 $r_i$ 必须是表示相应测试用例的 $A_{\\text{aug}} - A_{\\text{base}}$ 的浮点数，四舍五入到 $4$ 位小数。不应打印任何其他输出。",
            "solution": "The user wants to solve a problem about the effect of cutout augmentation on model robustness.\n\n### Step 1: Extract Givens\n- **Problem Type**: Binary classification with a logistic regression model.\n- **Input Data**: Synthetic satellite images of size $H \\times W$, where $H=16$ and $W=16$. The flattened input dimension is $d=256$.\n- **Classifier**: $\\sigma(w^\\top x + b)$, where $\\sigma(z) = 1/(1+e^{-z})$, $w \\in \\mathbb{R}^d$, $b \\in \\mathbb{R}$.\n- **Loss Function**: Binary cross-entropy with $\\ell_2$ regularization:\n$$\n\\mathcal{L}(w,b) = \\frac{1}{n}\\sum_{i=1}^n \\left[-y_i \\log \\sigma(w^\\top x_i + b) - (1-y_i)\\log(1 - \\sigma(w^\\top x_i + b))\\right] + \\frac{\\lambda}{2}\\|w\\|_2^2.\n$$\n- **Data Augmentation (Cutout, $T_f$)**: Masks a single axis-aligned rectangular region with zeros.\n    - Target area fraction: $f \\in [0,1]$.\n    - Approximate area: $f \\cdot H \\cdot W$.\n    - Rectangle aspect ratio: Uniformly random in $[0.3, 3.0]$.\n    - Rectangle location: Uniformly random, fully contained in the image.\n- **Training Procedure**:\n    - Optimizer: Full-batch gradient descent.\n    - Learning rate: $\\eta = 0.1$.\n    - $\\ell_2$ regularization coefficient: $\\lambda = 0.001$.\n    - Optimization steps (epochs): $E = 300$.\n    - Augmented model training: Apply $T_f$ with probability $p$ to each sample at each training step. Training augmentation fraction $f$ is set to the test-time occlusion fraction $f_{\\text{test}}$.\n    - Baseline model training: No augmentation ($p=0$).\n- **Synthetic Data Generation**:\n    - Samples: $N_{\\text{train}} = 2000$, $N_{\\text{test}} = 2000$.\n    - Labels $y_i \\in \\{0,1\\}$: Uniformly random.\n    - Base noise $N_i$: Pixel-wise independent from a normal distribution $\\mathcal{N}(0, \\sigma^2)$ with $\\sigma=0.1$.\n    - Global bias: Add $g=0.1$ for $y_i=1$, subtract $g=0.1$ for $y_i=0$.\n    - Local discriminative patch: A $s \\times s$ square with $s=6$ in the top-left corner. Add $a_p=1.0$ for $y_i=1$, subtract $a_p=1.0$ for $y_i=0$.\n- **Test-Time Evaluation**:\n    - Occlusion: Apply $T_{f_{\\text{test}}}$ deterministically to every test image (i.e., with probability $p=1.0$).\n    - Metric: Accuracy improvement, defined as $A_{\\text{aug}} - A_{\\text{base}}$. Accuracy is the fraction of correct predictions with a decision threshold of $0.5$.\n- **Test Cases**: Tuples of $(p, f_{\\text{test}}, \\text{seed})$.\n    - Case $1$: $(0.5, 0.0, 42)$.\n    - Case $2$: $(0.5, 0.3, 123)$.\n    - Case $3$: $(0.9, 0.5, 7)$.\n    - Case $4$: $(0.0, 0.3, 99)$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem is well-grounded in standard machine learning principles, including logistic regression, gradient descent, regularization, and data augmentation (cutout). The experimental setup to test the hypothesis that cutout improves robustness to occlusion is a common and valid research methodology in the field. The problem is scientifically sound.\n2.  **Non-Formalizable or Irrelevant**: The problem is directly related to the specified topic and is entirely formalizable into a computational task.\n3.  **Incomplete or Contradictory Setup**: All required parameters ($H, W, n, \\lambda, \\eta, E$, data generation parameters, augmentation parameters) are explicitly defined. The procedure is described in sufficient detail to be implemented. There are no contradictions.\n4.  **Unrealistic or Infeasible**: The parameters and setup are computationally feasible and constitute a standard, small-scale machine learning experiment.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The detailed specification of the data generation, training, and evaluation processes ensures that a unique and meaningful numerical result can be computed for each test case.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem addresses a non-trivial concept—the interplay between data augmentation and model robustness—through a concrete, computational experiment. While the setup is synthetic, it is designed to isolate and study a specific phenomenon, which is a valid scientific approach.\n7.  **Outside Scientific Verifiability**: The results are verifiable by implementing the described algorithm and running it with the specified seeds.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with providing a complete solution.\n\n### Algorithmic Solution\nThe solution involves implementing the specified data generation, model training, and evaluation pipeline.\n\n#### 1. Data Generation\nFor a set of $n$ samples, we generate images $X \\in \\mathbb{R}^{n \\times H \\times W}$ and labels $y \\in \\{0, 1\\}^n$.\nFor each sample $i \\in \\{1, \\dots, n\\}$:\n- A label $y_i$ is drawn from a Bernoulli distribution with $p=0.5$.\n- A base noise image $N_i \\in \\mathbb{R}^{H \\times W}$ is drawn, with each pixel $N_{i,jk} \\sim \\mathcal{N}(0, \\sigma^2)$, where $\\sigma = 0.1$.\n- A class-conditional multiplier $c_i = 2y_i - 1$ is defined, which is $+1$ for $y_i=1$ and $-1$ for $y_i=0$.\n- The image $X_i$ is constructed by adding a global bias and a local patch to the noise:\n$$ X_i = N_i + g \\cdot c_i \\cdot \\mathbf{1}_{H \\times W} + a_p \\cdot c_i \\cdot P_s $$\nwhere $\\mathbf{1}_{H \\times W}$ is an $H \\times W$ matrix of ones, and $P_s$ is an $H \\times W$ matrix with ones in the top-left $s \\times s$ block (with $s=6$) and zeros elsewhere. Constants are $g=0.1$ and $a_p=1.0$.\n- Finally, each $X_i$ is flattened into a vector $x_i \\in \\mathbb{R}^d$, where $d=H \\cdot W = 256$.\n\n#### 2. Cutout Augmentation\nThe cutout transformation $T_f$ takes an image and masks a random rectangular region. For an image of size $H \\times W$ and area fraction $f$:\n- The target area is $A = f \\cdot H \\cdot W$.\n- An aspect ratio $r$ is sampled uniformly from $[0.3, 3.0]$.\n- The height and width of the rectangle are calculated as $h = \\sqrt{A/r}$ and $w = \\sqrt{Ar}$. These are rounded to the nearest integer and clipped to be at most $H$ and $W$, respectively.\n- A top-left corner $(y_0, x_0)$ is sampled uniformly from the valid range, $[0, H-h] \\times [0, W-w]$.\n- The pixel intensities in the rectangle $[y_0:y_0+h, x_0:x_0+w]$ are set to $0$.\n\n#### 3. Model Training\nThe logistic regression parameters $w$ and $b$ are optimized using full-batch gradient descent. The gradients of the loss function $\\mathcal{L}(w,b)$ are:\n$$ \\nabla_w \\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(z_i) - y_i) x_i + \\lambda w $$\n$$ \\nabla_b \\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(z_i) - y_i) $$\nwhere $z_i = w^\\top x_i + b$. The parameters are initialized to zero and updated for $E=300$ steps:\n$$ w \\leftarrow w - \\eta \\nabla_w \\mathcal{L} $$\n$$ b \\leftarrow b - \\eta \\nabla_b \\mathcal{L} $$\n- For the **baseline model**, the training data $\\{x_i, y_i\\}$ is used directly.\n- For the **augmented model**, at each of the $E$ steps, a new augmented training set is created. For each sample $x_i$ in the original training batch, the cutout transformation $T_{f_{\\text{test}}}$ is applied with probability $p$. The gradients are then computed using this stochastically augmented batch.\n\n#### 4. Evaluation\n- First, a single occluded test set is created by applying the cutout transformation $T_{f_{\\text{test}}}$ once to every image in the generated test set.\n- Both the baseline model ($w_{\\text{base}}, b_{\\text{base}}$) and the augmented model ($w_{\\text{aug}}, b_{\\text{aug}}$) are used to make predictions on this same occluded test set.\n- A prediction $\\hat{y}_i$ for a test sample $x_i$ is $1$ if $\\sigma(w^\\top x_i + b) \\ge 0.5$, and $0$ otherwise.\n- Accuracy is computed as the fraction of correct predictions: $A = \\frac{1}{N_{\\text{test}}}\\sum_{i=1}^{N_{\\text{test}}} \\mathbb{I}(\\hat{y}_i = y_i)$.\n- The final result for each case is the accuracy improvement, $A_{\\text{aug}} - A_{\\text{base}}$, rounded to $4$ decimal places.\n\nThis entire procedure is repeated for each of the four test cases, with the random number generator re-seeded for each case to ensure reproducibility.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment for all test cases.\n    \"\"\"\n    H, W = 16, 16\n    d = H * W\n    N_train = 2000\n    N_test = 2000\n    noise_std = 0.1\n    g = 0.1\n    s = 6\n    a_p = 1.0\n    eta = 0.1\n    lambda_reg = 0.001\n    epochs = 300\n    aspect_ratio_range = (0.3, 3.0)\n\n    test_cases = [\n        # (p, f_test, seed)\n        (0.5, 0.0, 42),\n        (0.5, 0.3, 123),\n        (0.9, 0.5, 7),\n        (0.0, 0.3, 99),\n    ]\n\n    results = []\n\n    def generate_data(n_samples, rng):\n        \"\"\"Generates synthetic image data.\"\"\"\n        labels = rng.integers(0, 2, size=n_samples)\n        noise = rng.normal(0, noise_std, size=(n_samples, H, W))\n        \n        # c_i = +1 for label 1, -1 for label 0\n        class_multiplier = 2 * labels - 1\n        \n        # Add global bias\n        images = noise + g * class_multiplier[:, np.newaxis, np.newaxis]\n        \n        # Add local patch\n        patch = np.zeros((H, W))\n        patch[0:s, 0:s] = a_p\n        images += class_multiplier[:, np.newaxis, np.newaxis] * patch\n        \n        return images.reshape(n_samples, d), labels\n\n    def apply_cutout_on_batch(images_flat, f, p_aug, rng):\n        \"\"\"Applies cutout augmentation to a batch of images.\"\"\"\n        if p_aug == 0 or f == 0:\n            return images_flat.copy()\n\n        n_samples = images_flat.shape[0]\n        images = images_flat.reshape(n_samples, H, W).copy()\n        \n        target_area = f * H * W\n        \n        # Determine which images to augment\n        augment_indices = rng.random(n_samples)  p_aug\n        \n        for i in range(n_samples):\n            if not augment_indices[i]:\n                continue\n\n            aspect_ratio = rng.uniform(aspect_ratio_range[0], aspect_ratio_range[1])\n            \n            h = int(np.round(np.sqrt(target_area / aspect_ratio)))\n            w = int(np.round(np.sqrt(target_area * aspect_ratio)))\n            \n            h = min(h, H)\n            w = min(w, W)\n\n            if h > 0 and w > 0:\n                y0 = rng.integers(0, H - h + 1)\n                x0 = rng.integers(0, W - w + 1)\n                images[i, y0:y0+h, x0:x0+w] = 0.0\n\n        return images.reshape(n_samples, d)\n        \n    def logistic_sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-np.clip(z, -250, 250)))\n\n    def train_model(X_train, y_train, p_aug, f_aug, rng):\n        \"\"\"Trains a logistic regression model.\"\"\"\n        n_samples, n_features = X_train.shape\n        w = np.zeros(n_features)\n        b = 0.0\n\n        for _ in range(epochs):\n            X_epoch = apply_cutout_on_batch(X_train, f_aug, p_aug, rng)\n            \n            z = X_epoch @ w + b\n            sigma = logistic_sigmoid(z)\n            \n            grad_w = (1/n_samples) * X_epoch.T @ (sigma - y_train) + lambda_reg * w\n            grad_b = (1/n_samples) * np.sum(sigma - y_train)\n            \n            w -= eta * grad_w\n            b -= eta * grad_b\n            \n        return w, b\n\n    def predict(X, w, b):\n        \"\"\"Makes predictions with a trained model.\"\"\"\n        probas = logistic_sigmoid(X @ w + b)\n        return (probas >= 0.5).astype(int)\n\n    def calculate_accuracy(y_true, y_pred):\n        \"\"\"Calculates prediction accuracy.\"\"\"\n        return np.mean(y_true == y_pred)\n\n    for p_case, f_test_case, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # Generate data\n        X_train, y_train = generate_data(N_train, rng)\n        X_test, y_test = generate_data(N_test, rng)\n\n        # Train baseline model (no augmentation)\n        w_base, b_base = train_model(X_train, y_train, p_aug=0.0, f_aug=0.0, rng=rng)\n\n        # Train augmented model\n        # Augmentation fraction f is set to the test occlusion fraction f_test\n        w_aug, b_aug = train_model(X_train, y_train, p_aug=p_case, f_aug=f_test_case, rng=rng)\n\n        # Create occluded test set (deterministic application, p=1.0)\n        X_test_occluded = apply_cutout_on_batch(X_test, f_test_case, p_aug=1.0, rng=rng)\n\n        # Evaluate both models on the same occluded test set\n        preds_base = predict(X_test_occluded, w_base, b_base)\n        acc_base = calculate_accuracy(y_test, preds_base)\n        \n        preds_aug = predict(X_test_occluded, w_aug, b_aug)\n        acc_aug = calculate_accuracy(y_test, preds_aug)\n\n        improvement = round(acc_aug - acc_base, 4)\n        results.append(improvement)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了 Cutout 的基本思想后，我们将转向一种更高级的技术——CutMix，它通过将一张图像的切块粘贴到另一张图像上来合成新样本。本次实践聚焦于一个极具挑战性的现实问题：类别不平衡。我们将通过一个理想化的分析模型，推导在 CutMix 中有策略地从少数类中选择“贡献”图像，如何影响模型的预测置信度和对少数类的召回率 。这个练习旨在培养你分析复杂增强策略对模型行为深层影响的能力。",
            "id": "3151880",
            "problem": "考虑一个包含 $K$ 个类别的监督式多类别分类场景，其中训练集是不平衡的，其类别先验概率 $p_1, p_2, \\dots, p_K$ 满足 $\\sum_{k=1}^{K} p_k = 1$ 且对于所有 $k$ 都有 $p_k \\ge 0$。训练使用 CutMix 增强：对于类别为 $i$ 的基础图像，从另一张图像中提取一个捐赠者图像块并粘贴到基础图像中。令随机变量 $\\alpha \\in [0,1]$ 表示所粘贴的捐赠者图像块相对于总图像面积的面积比例。捐赠者类别从一个满足 $\\sum_{k=1}^{K} q_k = 1$ 且 $q_k \\ge 0$ 的分布 $q_1, q_2, \\dots, q_K$ 中采样，其中可以选择 $q_k$ 以偏好少数类。增强后样本的训练标签是对应于基础类别和捐赠者类别的 one-hot 向量的凸组合，其中基础类别获得权重 $(1 - \\alpha)$，捐赠者类别获得权重 $\\alpha$。\n\n使用以下基本依据：\n- 预测的类别概率与训练标签之间的交叉熵损失，在对相似输入进行平均时，会驱动预测值趋向于观测到的目标值。\n- CutMix 中的标签更新规则使用捐赠者面积比例 $\\alpha$ 作为 one-hot 标签的混合系数。\n- 期望校准误差 (ECE) 聚合了可靠性图中各个分箱内经验准确率与预测置信度之间的绝对差值。\n- 召回率衡量在决策置信度阈值下，检测到的实际正样本所占的比例。\n\n在理想化假设下，即模型经过训练后，其对某个样本真实类别的输出近似于训练期间为该真实类别观测到的期望目标值，请推导一个表达式（用关于 $\\alpha$ 的期望、捐赠者分布 $q_k$ 和类别先验 $p_k$ 表示），用于描述对类别为 $k$ 的样本，分配给其真实类别的预测置信度。然后，使用这个推导出的表达式，构建一个程序，为每个指定的测试用例计算以下量：\n- 期望校准误差 (ECE)，计算方法是将每个类别视为一个独立的分箱，其置信度等于为该类别推导出的真实类别置信度，并且在正确分类的理想化假设下，其经验准确率等于 $1$。按类别先验聚合各分箱的贡献。\n- 宏召回率，定义为各类别召回率的算术平均值，其中如果某类别的推导真实类别置信度不小于决策阈值，则该类别的召回率等于 $1$，否则等于 $0$。\n- 微召回率，定义为各类别召回率的类别先验加权平均值。\n\n对于捐赠者图像块的面积比例 $\\alpha$，假设 $\\alpha$ 服从形状参数为 $a  0$ 和 $b  0$ 的 Beta 分布，并使用其著名性质 $\\mathbb{E}[\\alpha] = \\frac{a}{a + b}$。\n\n您的程序必须为以下测试套件中的每个测试用例实现上述计算，其中 $K = 3$ 且所有用例具有相同的不平衡类别先验：\n- 案例 1（基线，均匀捐赠者采样，中等程度混合）：$p = [0.05, 0.15, 0.80]$，$q = [\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]$，$a = 1$，$b = 1$，决策阈值 $t = 0.85$。\n- 案例 2（偏好少数类的捐赠者采样，中等程度混合）：$p = [0.05, 0.15, 0.80]$，$q = [0.7, 0.2, 0.1]$，$a = 1$，$b = 1$，决策阈值 $t = 0.85$。\n- 案例 3（偏好少数类的捐赠者采样，高程度混合）：$p = [0.05, 0.15, 0.80]$，$q = [0.7, 0.2, 0.1]$，$a = 9$，$b = 1$，决策阈值 $t = 0.95$。\n\n最终输出格式必须是包含三个测试用例串联结果的单行，格式为 $[\\text{ECE}_1,\\text{Macro}_1,\\text{Micro}_1,\\text{ECE}_2,\\text{Macro}_2,\\text{Micro}_2,\\text{ECE}_3,\\text{Macro}_3,\\text{Micro}_3]$。每个数值必须四舍五入到 6 位小数。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如 $[r_1,r_2,\\dots]$）。",
            "solution": "目标是为使用 CutMix 增强训练的模型的预测置信度推导一个表达式，并随后使用该表达式计算特定的评估指标：期望校准误差 (ECE)、宏召回率和微召回率。推导和计算基于问题陈述中提供的一组理想化假设。\n\n分析分两个阶段进行：首先，推导预测的真实类别置信度；其次，应用此结果计算所需的指标。\n\n**1. 预测的真实类别置信度的推导**\n\n给定一个包含 $K$ 个类别的多类别分类问题。类别先验由概率 $p_k$ 给出，其中 $k \\in \\{1, 2, \\dots, K\\}$。训练过程使用 CutMix 增强。\n\n问题的核心依赖于一个理想化假设：训练后，模型对样本真实类别的预测概率近似于训练期间为该真实类别观测到的期望目标值。令 $C_k$ 为分配给类别为 $k$ 的样本的真实类别的预测置信度。根据该假设，此置信度等于增强后训练标签向量的第 $k$ 个分量的期望值，其条件是原始（基础）图像来自类别 $k$。\n\n让我们将其形式化。一个增强样本由真实类别为 $k$ 的基础图像和真实类别为 $j$ 的捐赠者图像创建。基础图像的 one-hot 编码标签为 $e_k \\in \\mathbb{R}^K$，捐赠者图像的为 $e_j \\in \\mathbb{R}^K$。捐赠者图像的类别 $j$ 是从指定分布 $q = (q_1, q_2, \\dots, q_K)$ 中采样的随机变量。粘贴的捐赠者图像块的面积比例 $\\alpha$ 是从参数为 $a$ 和 $b$ 的 Beta 分布中采样的随机变量。增强后的标签 $y_{\\text{aug}}$ 是基础标签和捐赠者标签的凸组合：\n$$\ny_{\\text{aug}} = (1 - \\alpha) e_k + \\alpha e_j\n$$\n该向量的第 $k$ 个分量 $y_{\\text{aug},k}$ 表示真实类别 $k$ 的目标值。我们可以将其表示为：\n$$\ny_{\\text{aug},k} = (1 - \\alpha) (e_k)_k + \\alpha (e_j)_k\n$$\n其中 $(v)_k$ 表示向量 $v$ 的第 $k$ 个分量。根据 one-hot 向量的定义，$(e_k)_k = 1$。如果捐赠者类别 $j$ 与基础类别 $k$ 相同，则项 $(e_j)_k$ 为 $1$，否则为 $0$。这可以用克罗内克 δ (Kronecker delta) $\\delta_{jk}$ 来表示。因此，真实类别的目标值为：\n$$\ny_{\\text{aug},k} = (1 - \\alpha) + \\alpha \\delta_{jk}\n$$\n我们的目标是找到预测置信度 $C_k$，即该量在随机变量 $\\alpha$ 和 $j$ 上的期望。捐赠者类别 $j$ 的选择和图像块面积 $\\alpha$ 的选择是独立事件。\n$$\nC_k = \\mathbb{E}[y_{\\text{aug},k} | \\text{base class is } k] = \\mathbb{E}_{\\alpha \\sim \\text{Beta}(a,b), j \\sim q} \\left[ (1 - \\alpha) + \\alpha \\delta_{jk} \\right]\n$$\n根据期望的线性性，我们有：\n$$\nC_k = \\mathbb{E}[1 - \\alpha] + \\mathbb{E}[\\alpha \\delta_{jk}]\n$$\n由于 $\\alpha$ 和 $j$ 的独立性，它们乘积的期望等于它们期望的乘积：\n$$\n\\mathbb{E}[\\alpha \\delta_{jk}] = \\mathbb{E}[\\alpha] \\mathbb{E}[\\delta_{jk}]\n$$\n令 $\\mu_\\alpha = \\mathbb{E}[\\alpha]$。$C_k$ 的表达式变为：\n$$\nC_k = (1 - \\mu_\\alpha) + \\mu_\\alpha \\mathbb{E}[\\delta_{jk}]\n$$\n期望 $\\mathbb{E}[\\delta_{jk}]$ 是在捐赠者类别 $j$ 的分布上计算的。捐赠者类别为 $j'$ 的概率是 $q_{j'}$。\n$$\n\\mathbb{E}[\\delta_{jk}] = \\sum_{j'=1}^{K} P(\\text{donor class}=j') \\cdot \\delta_{j'k} = \\sum_{j'=1}^{K} q_{j'} \\delta_{j'k}\n$$\n由于所有其他项都为零，此和的计算结果为 $q_k$。将此代回 $C_k$ 的表达式中：\n$$\nC_k = (1 - \\mu_\\alpha) + \\mu_\\alpha q_k = 1 - \\mu_\\alpha(1 - q_k)\n$$\n问题陈述指出 $\\alpha$ 服从参数为 $a  0$ 和 $b  0$ 的 Beta 分布，其期望为 $\\mu_\\alpha = \\mathbb{E}[\\alpha] = \\frac{a}{a+b}$。因此，为真实类别 $k$ 分配的预测置信度的最终推导表达式为：\n$$\nC_k = 1 - \\frac{a}{a+b} (1 - q_k)\n$$\n\n**2. 性能指标的计算**\n\n在推导出 $C_k$ 的表达式后，我们现在定义用于计算所需指标的算法。\n\n- **期望校准误差 (ECE)**：问题为 ECE 定义了一个特定的公式。每个类别 $k$ 被视为一个单独的分箱。分箱 $k$ 的置信度是 $C_k$，准确率假设为 $1$。ECE 是准确率和置信度之间绝对差的平均值，由类别先验概率 $p_k$ 加权。\n$$\n\\text{ECE} = \\sum_{k=1}^{K} p_k |1 - C_k|\n$$\n代入推导出的 $C_k$ 表达式：\n$$\n\\text{ECE} = \\sum_{k=1}^{K} p_k \\left|1 - \\left(1 - \\frac{a}{a+b}(1-q_k)\\right)\\right| = \\sum_{k=1}^{K} p_k \\left|\\frac{a}{a+b}(1-q_k)\\right|\n$$\n由于 $a, b  0$ 且 $q_k \\in [0, 1]$，绝对值内的项是非负的。\n$$\n\\text{ECE} = \\frac{a}{a+b} \\sum_{k=1}^{K} p_k (1-q_k)\n$$\n\n- **召回率指标**：这些指标取决于决策阈值 $t$。如果类别为 $k$ 的样本的真实类别置信度 $C_k$ 达到或超过该阈值，则认为该样本被正确召回。每类召回率 $R_k$ 是一个二元值：\n$$\nR_k = \\begin{cases} 1  \\text{if } C_k \\ge t \\\\ 0  \\text{if } C_k  t \\end{cases}\n$$\n这可以使用指示函数简洁地写为 $R_k = \\mathbb{I}(C_k \\ge t)$。\n\n- **宏召回率**：这是各类别召回率的未加权算术平均值。\n$$\n\\text{Macro Recall} = \\frac{1}{K} \\sum_{k=1}^{K} R_k\n$$\n\n- **微召回率**：这是各类别召回率的加权平均值，其中权重为类别先验 $p_k$。\n$$\n\\text{Micro Recall} = \\sum_{k=1}^{K} p_k R_k\n$$\n\n程序将实现这些公式，首先计算置信度向量 $C = (C_1, \\dots, C_K)$，然后计算每类召回率 $R = (R_1, \\dots, R_K)$，最后为每个测试用例计算聚合指标。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating ECE, Macro Recall, and Micro Recall\n    for three specified test cases of CutMix augmentation.\n    \"\"\"\n    \n    # Define the common and case-specific parameters from the problem statement.\n    # p: class priors, K: number of classes\n    p_common = np.array([0.05, 0.15, 0.80])\n    K = len(p_common)\n    \n    test_cases = [\n        {\n            \"p\": p_common,\n            \"q\": np.array([1/3, 1/3, 1/3]),\n            \"a\": 1,\n            \"b\": 1,\n            \"t\": 0.85,\n        },\n        {\n            \"p\": p_common,\n            \"q\": np.array([0.7, 0.2, 0.1]),\n            \"a\": 1,\n            \"b\": 1,\n            \"t\": 0.85,\n        },\n        {\n            \"p\": p_common,\n            \"q\": np.array([0.7, 0.2, 0.1]),\n            \"a\": 9,\n            \"b\": 1,\n            \"t\": 0.95,\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        p, q, a, b, t = case[\"p\"], case[\"q\"], case[\"a\"], case[\"b\"], case[\"t\"]\n        \n        # 1. Calculate the mean of the patch area distribution alpha.\n        # E[alpha] = a / (a + b)\n        mu_alpha = a / (a + b)\n        \n        # 2. Derive the predicted confidence for each true class k.\n        # C_k = 1 - mu_alpha * (1 - q_k)\n        C = 1 - mu_alpha * (1 - q)\n        \n        # 3. Compute the Expected Calibration Error (ECE).\n        # ECE = sum_k p_k * |1 - C_k|\n        # This simplifies to: ECE = mu_alpha * sum_k p_k * (1 - q_k)\n        ece = np.sum(p * np.abs(1 - C))\n        \n        # 4. Compute per-class recalls based on the decision threshold t.\n        # R_k = 1 if C_k = t, else 0\n        R = (C >= t).astype(float)\n        \n        # 5. Compute Macro Recall (arithmetic mean of per-class recalls).\n        # Macro Recall = (1/K) * sum_k R_k\n        macro_recall = np.mean(R)\n        \n        # 6. Compute Micro Recall (class-prior weighted average of per-class recalls).\n        # Micro Recall = sum_k p_k * R_k\n        micro_recall = np.sum(p * R)\n        \n        # Append the results for the current case.\n        results.extend([ece, macro_recall, micro_recall])\n\n    # Format the final output string as a list of numbers rounded to 6 decimal places.\n    formatted_results = ','.join(f'{x:.6f}' for x in results)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后，我们将探索三种方法中最抽象的 Mixup，它直接对整个图像进行线性插值。这个练习鼓励我们进行批判性思考：随机混合总是有益的吗？我们将引入“语义相似度”的概念，并设计一种自适应的 Mixup 策略，其中混合系数 $ \\lambda $ 由图像间的相似度决定。你将设计一个原则性测试，以验证这种自适应方法是否能有效避免混合语义上不相关的图像对，从而减少潜在的“负面”增强效应 。",
            "id": "3151945",
            "problem": "您收到了一个修改标准线性mixup数据增强的提案，该提案建议将混合系数选择为数据依赖的语义相似性的确定性函数。您的任务是形式化并测试一个自适应规则——该规则将混合系数设置为等于语义相似性——是否能够避免在语义上遥远的配对上产生有害混合。此问题需纯粹通过数学和算法术语来解决，并实现为一个完整的程序。背景是深度学习数据增强：mixup、cutout和cutmix。尽管这里只直接模拟了mixup，但您的推理应基于深度学习分类中使用的核心定义。\n\n基本依据与定义：\n- 考虑一个有 $C$ 个类别的分类问题。设一个输入样本由 $\\mathbb{R}^d$ 中的向量表示，其类别标签由 $\\{0,1\\}^C$ 中的独热向量表示。\n- 标准mixup使用混合系数 $\\lambda \\in [0,1]$ 从一对样本 $(x_i, y_i)$ 和 $(x_j, y_j)$ 生成一个虚拟训练样本。混合输入为 $x_{\\text{mix}} = \\lambda x_i + (1-\\lambda) x_j$，混合目标为凸组合 $y_{\\text{mix}} = \\lambda y_i + (1-\\lambda) y_j$。\n- 定义两个样本之间的语义相似性 $s \\in [0,1]$ 为余弦相似度映射到 $[0,1]$ 的结果，即 $s(x_i, x_j) = \\frac{1 + \\cos(\\theta(x_i,x_j))}{2}$，其中 $\\cos(\\theta(x_i,x_j)) = \\frac{x_i^\\top x_j}{\\|x_i\\|_2 \\|x_j\\|_2}$。\n- 假设一个与语义一致的“教师”目标分布，对于一个混合对，当 $y_i \\neq y_j$ 时，该分布将概率质量 $s$ 分配给 $x_i$ 的类别，将概率质量 $(1-s)$ 分配给 $x_j$ 的类别；当 $y_i=y_j$ 时，将所有概率质量分配给该单一类别。这反映了分类学中一个经过充分检验的原则：目标应遵循数据的语义构成，其中 $s$ 作为构成的代理。\n- 混合目标 $y_{\\text{mix}}$ 的危害性由从教师分布 $p$ 到mixup目标 $q$ 的Kullback–Leibler散度 $D_{\\mathrm{KL}}(p \\| q)$ 来量化。根据 $D_{\\mathrm{KL}}$ 对离散分布的标准定义，计算时会添加一个微小正常数以避免除以零。\n\n目标：\n- 在一个合成数据集上定义并评估三种关于 $\\lambda$ 的混合策略：\n  - 策略A（固定）：$\\lambda = 0.5$。\n  - 策略B（随机Beta）：$\\lambda \\sim \\mathrm{Beta}(\\alpha,\\alpha)$，其中 $\\alpha$ 为给定值。\n  - 策略C（自适应）：$\\lambda = s$。\n- 如果 $y_i \\neq y_j$ 且 $s(x_i,x_j) \\le s_{\\mathrm{cut}}$（对于给定的 $s_{\\mathrm{cut}} \\in [0,1]$），则一对样本 $(x_i,y_i),(x_j,y_j)$ 被视为语义上遥远。\n- 如果一个混合对的 $D_{\\mathrm{KL}}(p \\| q)$ 超过给定阈值 $\\delta  0$，则认为该混合对在某策略下是有害的。\n- 对于每个测试用例，使用 $M$ 个随机抽样的配对，分别对每种策略估计在语义遥远配对子集上有害配对的比例。如果在某个测试用例中没有发现语义遥远的配对（即该集合为空），则将所有策略的有害比例定义为 $0$，并在这种情况下视为自适应策略成功避免了有害混合。\n\n每个测试用例的数据集构建：\n- 为 $c \\in \\{1,\\dots,C\\}$ 选择 $C$ 个类别原型 $\\mu_c \\in \\mathbb{R}^d$。将它们放置为缩放后的标准正交向量，使得 $\\mu_c$ 是一个缩放后的基向量，其缩放比例由一个“分离度”参数给出。\n- 对于每个类别 $c$，从均值为 $\\mu_c$、协方差为各向同性的 $\\sigma^2 I_d$ 的多元正态分布中抽取 $n$ 个样本 $x$。并相应地分配独热标签 $y$。\n\n数值计算说明：\n- 所有角度都是无量纲的，在三角函数内部使用弧度制。\n- 本问题中没有物理单位。\n- 所有概率和比例都必须计算为 $[0,1]$ 区间内的实数。\n\n测试套件：\n对于每个测试用例，产生一个布尔结果，其定义为：“如果自适应策略（策略C）在语义遥远配对上的有害比例严格低于策略A和策略B的有害比例，或者语义遥远配对的数量为零，则为True”，否则为“False”。为保证可复现性，请使用单一固定的随机种子。评估以下四个测试用例：\n\n- 测试用例1（通用“理想路径”）：\n  - $d=16$, $C=3$, $n=50$, $\\sigma=0.6$, separation $=2.5$, $s_{\\mathrm{cut}}=0.3$, $\\delta=0.02$, $M=4000$, pair mode = “all pairs,” $\\alpha=0.4$。\n- 测试用例2（边界：低类间分离度，可能很少或没有遥远配对）：\n  - $d=16$, $C=3$, $n=50$, $\\sigma=0.6$, separation $=0.5$, $s_{\\mathrm{cut}}=0.3$, $\\delta=0.02$, $M=4000$, pair mode = “all pairs,” $\\alpha=0.4$。\n- 测试用例3（边界：高类间分离度）：\n  - $d=16$, $C=3$, $n=50$, $\\sigma=0.3$, separation $=4.0$, $s_{\\mathrm{cut}}=0.3$, $\\delta=0.02$, $M=4000$, pair mode = “all pairs,” $\\alpha=0.4$。\n- 测试用例4（边缘：仅限同类配对）：\n  - $d=16$, $C=3$, $n=50$, $\\sigma=0.6$, separation $=2.0$, $s_{\\mathrm{cut}}=0.3$, $\\delta=0.02$, $M=4000$, pair mode = “same-class only,” $\\alpha=0.4$。\n\n输出格式：\n- 您的程序应生成单行输出，其中包含四个测试用例的四个布尔结果，按顺序排列，格式为用方括号括起来的逗号分隔的Python风格列表。例如，可接受的输出格式形如 $[b_1,b_2,b_3,b_4]$，其中每个 $b_k$ 为True或False。\n- 程序必须是完全自包含的，不得要求任何用户输入、外部文件或网络访问。使用单一固定的随机种子，以确保结果是确定性的。\n\n注意事项：\n- 您的推导应基于上述标准定义。除了已确立的凸组合、余弦相似度、独热编码和Kullback–Leibler散度的定义外，不要使用任何快捷公式。\n- 尽管更广泛的主题包括cutout和cutmix，但本次计算实验专注于mixup。您在构建推理时仍应意识到，cutout通过遮蔽图像区域（将像素设置为零）来实现，而cutmix则在图像之间粘贴补丁；两者都与mixup不同，因为它们的输入混合不是全局凸组合，但cutmix中的目标混合仍是凸组合，而cutout通常保留原始标签。",
            "solution": "我们从分类目标、凸组合、余弦相似度和Kullback–Leibler散度的基本定义出发。目标是形式化自适应mixup，其中标签和输入的混合系数取决于两个样本之间可测量的语义相似性，然后评估这种自适应性是否减少了对语义遥远配对的有害混合。\n\n步骤 1：数据模型与原型\n- 我们考虑 $\\mathbb{R}^d$ 中的 $C$ 个类别。一种简单且经过充分检验的合成类别结构的方法是将原型 $\\mu_c$ 放置在标准正交轴上，并由一个标量“分离度”进行缩放以控制类间距离。具体来说，设 $e_c$ 表示 $\\mathbb{R}^d$ 中的标准基，并为 $c \\in \\{1,\\dots,C\\}$ 定义 $\\mu_c = \\text{separation} \\cdot e_c$，从而使得当 $c \\neq c'$ 时 $\\mu_c^\\top \\mu_{c'} = 0$ 且 $\\|\\mu_c\\|_2 = \\text{separation}$。这确保了在各向同性噪声下类分离度是可控的。\n- 对于每个类别 $c$，我们从分布 $\\mathcal{N}(\\mu_c, \\sigma^2 I_d)$ 中抽取 $n$ 个样本 $x$，分配独热标签 $y$（其中 $y_c=1$，其余为零），并收集数据集 $\\{(x_i,y_i)\\}$。\n\n步骤 2：语义相似性\n- 对于一对样本 $(x_i,y_i)$ 和 $(x_j,y_j)$，定义余弦相似度为 $\\cos(\\theta(x_i,x_j)) = \\frac{x_i^\\top x_j}{\\|x_i\\|_2 \\|x_j\\|_2}$。通过 $s(x_i,x_j) = \\frac{1 + \\cos(\\theta(x_i,x_j))}{2}$ 将其映射到单位区间，以获得语义相似性 $s \\in [0,1]$。此映射是一种保持顺序并限定相似性范围的单调变换，这是度量学习中广泛接受的做法。\n\n步骤 3：与语义对齐的教师分布\n- 我们如下定义一对样本的与语义对齐的教师分布 $p$。如果 $y_i \\neq y_j$，则 $p$ 将概率质量 $s$ 分配给 $y_i$ 的类别，将 $(1-s)$ 分配给 $y_j$ 的类别，而所有其他类别的概率质量为零。如果 $y_i = y_j$，则 $p$ 是该类别上的独热分布（因为混合同一类别的两个样本应保留相同的目标）。\n- 这与一个基本思想相符：在分类中，目标分布的正确性概率应遵循数据的语义内容。变量 $s$ 编码了一个样本的语义对混合输入的贡献分数。\n\n步骤 4：Mixup策略与混合目标\n- 标准mixup将混合目标构造成凸组合 $q_\\lambda = \\lambda y_i + (1-\\lambda) y_j$。我们评估三种策略：\n  - 策略A：$\\lambda = 0.5$（固定对半混合）。\n  - 策略B：$\\lambda \\sim \\mathrm{Beta}(\\alpha,\\alpha)$，这是实践中随机化混合权重的一种常见选择；我们为每个测试用例固定 $\\alpha$，并使用固定的随机种子以保证可复现性。\n  - 策略C：$\\lambda = s$（自适应），使用可测量的语义相似性来设定混合比例。\n- 对于同类配对，由于 $y_i=y_j$，所有策略都产生相同的 $q_\\lambda$，它等于底层的独热标签。\n\n步骤 5：使用Kullback–Leibler散度量化危害\n- 为衡量经语义信息调整的教师分布 $p$ 与混合目标 $q_\\lambda$ 之间的一致性，我们使用Kullback–Leibler散度\n$$\nD_{\\mathrm{KL}}(p \\| q_\\lambda) = \\sum_{c=1}^{C} p_c \\log\\left(\\frac{p_c}{q_{\\lambda,c}}\\right),\n$$\n按照惯例，在对数函数内部向 $p_c$ 和 $q_{\\lambda,c}$ 添加一个微小的正数 $\\varepsilon0$，以避免当概率为零时出现数值问题。这符合 $D_{\\mathrm{KL}}$ 对离散分布的标准定义，并提供了一个定量的非对齐度量。\n- 如果一个混合对的 $D_{\\mathrm{KL}}(p \\| q_\\lambda)  \\delta$（对于给定的阈值 $\\delta  0$），则认为该混合对是有害的。\n\n步骤 6：专注于语义遥远的配对\n- 当样本在语义上遥远时，有害混合尤其值得关注。因此，我们将评估限制在满足 $y_i \\neq y_j$ 和 $s(x_i,x_j) \\le s_{\\mathrm{cut}}$ 的配对上。截断值 $s_{\\mathrm{cut}}$ 形式化了“遥远配对”的概念，并在每个测试用例中指定。\n- 如果在某个测试用例中没有找到这样的配对（例如，当类别重叠严重且相似度很高时），我们为所有策略将有害比例定义为 $0$，并按惯例在这种空洞的情况下宣布自适应策略已避免了有害混合。\n\n步骤 7：测试用例与预期行为\n- 我们设计了四个测试用例来覆盖不同的情况：\n  - 测试用例1：中等分离度和噪声。我们预计会有许多具有不同相似度的跨类配对。策略C设置 $\\lambda=s$，使得在跨类配对上 $q_\\lambda$ 与 $p$ 完全匹配，从而产生 $D_{\\mathrm{KL}}=0$，而当 $s \\neq \\lambda$ 时，策略A和B通常会产生正的散度。\n  - 测试用例2：低分离度。跨类相似度往往较高，可能产生很少或没有遥远配对（$s \\le s_{\\mathrm{cut}}$）。根据我们的规则，在这种边界场景下，自适应策略被认为没有产生有害的遥远混合。\n  - 测试用例3：高分离度与低噪声。许多跨类配对是遥远的。策略C保持与语义对齐（$\\lambda=s$），而策略A和B对大多数遥远配对产生不匹配，导致更大的散度。\n  - 测试用例4：仅限同类配对。由于 $y_i = y_j$，所有策略的结果都一致，并且在遥远的跨类配对的空集上，所有策略的有害比例都为零，这使得自适应策略符合成功标准。\n\n步骤 8：计算与输出\n- 对于每个测试用例，我们根据指定的配对模式（所有配对或仅限同类）抽样 $M$ 个配对，计算 $s$，识别出遥远子集，对每个策略评估 $D_{\\mathrm{KL}}(p\\|q_\\lambda)$，并计算有害比例。然后我们输出一个布尔值，指示自适应策略的有害比例是否严格低于两个基线（或者遥远配对数量为零）。\n- 最终程序将四个测试用例的结果打印为单行：一个包含四个有序布尔值的列表。\n\n关于cutout和cutmix的讨论：\n- Cutout通过将图像输入的连续区域设置为零来遮蔽它们，同时保留原始标签；它不执行输入或标签的凸组合。Cutmix用另一张图像的补丁替换一张图像的某个区域，并根据补丁区域的比例混合标签。相比之下，mixup对整个输入形成全局凸组合，并相应地执行标签插值。本文探讨的自适应思想可以自然地扩展到cutmix，即通过将标签混合系数设置为一个依赖于语义的权重（而不是或除了区域面积之外），而cutout通常保留原始标签，因此根本不会使用 $\\lambda$。",
            "answer": "```python\nimport numpy as np\n\n# Environment: Python 3.12, numpy 1.23.5, scipy 1.11.4 (not used here)\n\ndef generate_prototypes(C, d, separation):\n    \"\"\"\n    Generate class prototypes as scaled standard basis vectors in R^d.\n    \"\"\"\n    prototypes = np.zeros((C, d), dtype=float)\n    for c in range(C):\n        prototypes[c, c] = separation\n    return prototypes\n\ndef generate_dataset(C, d, n_per_class, sigma, separation, rng):\n    \"\"\"\n    Generate synthetic dataset: for each class, sample n_per_class points from N(mu_c, sigma^2 I).\n    \"\"\"\n    mus = generate_prototypes(C, d, separation)\n    X_list = []\n    y_list = []\n    for c in range(C):\n        Xc = rng.normal(loc=0.0, scale=sigma, size=(n_per_class, d)) + mus[c]\n        yc = np.zeros((n_per_class, C), dtype=float)\n        yc[:, c] = 1.0\n        X_list.append(Xc)\n        y_list.append(yc)\n    X = np.vstack(X_list)\n    Y = np.vstack(y_list)\n    labels = np.argmax(Y, axis=1)\n    return X, Y, labels\n\ndef cosine_similarity(x, y):\n    \"\"\"\n    Cosine similarity between two vectors.\n    \"\"\"\n    nx = np.linalg.norm(x)\n    ny = np.linalg.norm(y)\n    if nx == 0.0 or ny == 0.0:\n        return 0.0\n    return float(np.dot(x, y) / (nx * ny))\n\ndef semantic_similarity(x, y):\n    \"\"\"\n    Map cosine similarity to [0,1]: s = (1 + cos) / 2.\n    \"\"\"\n    cos = cosine_similarity(x, y)\n    return (1.0 + cos) / 2.0\n\ndef teacher_distribution(li, lj, s, C):\n    \"\"\"\n    Build teacher distribution p over C classes:\n    - If li != lj: mass s on li and (1-s) on lj.\n    - If li == lj: one-hot on li.\n    \"\"\"\n    p = np.zeros(C, dtype=float)\n    if li == lj:\n        p[li] = 1.0\n    else:\n        p[li] = s\n        p[lj] = 1.0 - s\n    return p\n\ndef mixup_target(li, lj, lam, C):\n    \"\"\"\n    Mixed target q over C classes using mixup coefficient lam.\n    \"\"\"\n    q = np.zeros(C, dtype=float)\n    if li == lj:\n        q[li] = 1.0\n    else:\n        q[li] = lam\n        q[lj] = 1.0 - lam\n    return q\n\ndef kl_divergence(p, q, eps=1e-12):\n    \"\"\"\n    D_KL(p || q) for discrete distributions with epsilon smoothing.\n    \"\"\"\n    p_safe = p + eps\n    q_safe = q + eps\n    p_safe /= p_safe.sum()\n    q_safe /= q_safe.sum()\n    return float(np.sum(p_safe * (np.log(p_safe) - np.log(q_safe))))\n\ndef sample_pairs(labels, M, mode, rng):\n    \"\"\"\n    Sample M pairs of indices according to mode:\n    - 'all': any pairs uniformly at random with replacement\n    - 'same': pairs restricted to same-class\n    - 'cross': pairs restricted to different classes\n    Returns arrays of indices i_idx, j_idx.\n    \"\"\"\n    N = len(labels)\n    i_idx = []\n    j_idx = []\n    # Precompute class indices\n    class_to_indices = {}\n    for c in np.unique(labels):\n        class_to_indices[c] = np.where(labels == c)[0]\n    if mode == 'all':\n        i_idx = rng.integers(0, N, size=M)\n        j_idx = rng.integers(0, N, size=M)\n        return i_idx, j_idx\n    elif mode == 'same-class only':\n        # For each sample, choose j from same class uniformly\n        for _ in range(M):\n            i = int(rng.integers(0, N))\n            ci = labels[i]\n            choices = class_to_indices[ci]\n            j = int(choices[rng.integers(0, len(choices))])\n            i_idx.append(i)\n            j_idx.append(j)\n        return np.array(i_idx, dtype=int), np.array(j_idx, dtype=int)\n    elif mode == 'cross-class only':\n        # For each sample, choose i and j with different classes\n        classes = list(class_to_indices.keys())\n        for _ in range(M):\n            ci = int(classes[rng.integers(0, len(classes))])\n            # choose a different class\n            cj_choices = [c for c in classes if c != ci]\n            cj = int(cj_choices[rng.integers(0, len(cj_choices))])\n            i = int(class_to_indices[ci][rng.integers(0, len(class_to_indices[ci]))])\n            j = int(class_to_indices[cj][rng.integers(0, len(class_to_indices[cj]))])\n            i_idx.append(i)\n            j_idx.append(j)\n        return np.array(i_idx, dtype=int), np.array(j_idx, dtype=int)\n    else:\n        raise ValueError(\"Unknown mode\")\n\ndef evaluate_case(d, C, n, sigma, separation, s_cut, delta, M, pair_mode, alpha, rng):\n    \"\"\"\n    Evaluate one test case and return boolean result as specified.\n    \"\"\"\n    X, Y, labels = generate_dataset(C=C, d=d, n_per_class=n, sigma=sigma, separation=separation, rng=rng)\n\n    i_idx, j_idx = sample_pairs(labels, M, pair_mode, rng)\n    \n    harm_counts = {'A': 0, 'B': 0, 'C': 0}\n    total_distant = 0\n\n    # Pre-sample Beta for policy B\n    if alpha = 0:\n        raise ValueError(\"alpha must be positive\")\n    lam_beta = rng.beta(alpha, alpha, size=M)\n\n    for k in range(M):\n        i = int(i_idx[k])\n        j = int(j_idx[k])\n        li = int(labels[i])\n        lj = int(labels[j])\n        \n        s = semantic_similarity(X[i], X[j])\n        if (li != lj) and (s = s_cut):\n            total_distant += 1\n            # Teacher\n            p = teacher_distribution(li, lj, s, C)\n            # Policy A\n            qA = mixup_target(li, lj, lam=0.5, C=C)\n            dA = kl_divergence(p, qA)\n            if dA > delta:\n                harm_counts['A'] += 1\n            # Policy B\n            qB = mixup_target(li, lj, lam=float(lam_beta[k]), C=C)\n            dB = kl_divergence(p, qB)\n            if dB > delta:\n                harm_counts['B'] += 1\n            # Policy C\n            qC = mixup_target(li, lj, lam=s, C=C)\n            dC = kl_divergence(p, qC)\n            if dC > delta:\n                harm_counts['C'] += 1\n\n    # If no distant pairs, by convention adaptive policy succeeds\n    if total_distant == 0:\n        return True\n\n    fracA = harm_counts['A'] / total_distant\n    fracB = harm_counts['B'] / total_distant\n    fracC = harm_counts['C'] / total_distant\n    # Strictly lower than both baselines\n    return (fracC  fracA) and (fracC  fracB)\n\ndef solve():\n    rng = np.random.default_rng(seed=1337)\n\n    test_cases = [\n        # (d, C, n, sigma, separation, s_cut, delta, M, pair_mode, alpha)\n        (16, 3, 50, 0.6, 2.5, 0.3, 0.02, 4000, 'all pairs', 0.4),        # Case 1\n        (16, 3, 50, 0.6, 0.5, 0.3, 0.02, 4000, 'all pairs', 0.4),        # Case 2\n        (16, 3, 50, 0.3, 4.0, 0.3, 0.02, 4000, 'all pairs', 0.4),        # Case 3\n        (16, 3, 50, 0.6, 2.0, 0.3, 0.02, 4000, 'same-class only', 0.4),       # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        d, C, n, sigma, separation, s_cut, delta, M, pair_mode, alpha = case\n        res = evaluate_case(d, C, n, sigma, separation, s_cut, delta, M, pair_mode, alpha, rng)\n        results.append(res)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}