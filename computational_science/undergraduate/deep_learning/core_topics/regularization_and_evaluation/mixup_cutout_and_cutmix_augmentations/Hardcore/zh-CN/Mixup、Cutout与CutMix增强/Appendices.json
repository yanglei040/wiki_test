{
    "hands_on_practices": [
        {
            "introduction": "Cutout 不仅是一种正则化技术，它还能有效提升模型对真实世界中遮挡问题的鲁棒性。本练习  将通过一个精心设计的合成数据集来模拟这一场景，数据集中同时包含强烈的局部特征和微弱的全局特征。通过训练并比较未使用和使用 Cutout 的模型在局部特征被遮挡时的表现，你将亲身体会到数据增强如何迫使模型学习更加泛化和稳健的特征。",
            "id": "3151871",
            "problem": "考虑一个二元分类问题，其中每个输入是一张合成卫星图像，表示为像素强度的二维数组，其标签指示是否存在目标地物。目标是研究“cutout”增强对测试时模拟云层遮挡的鲁棒性有何影响。从二元逻辑回归的经验风险最小化开始：分类器是一个线性函数，其参数为 $w \\in \\mathbb{R}^d$ 和 $b \\in \\mathbb{R}$，通过逻辑函数 $\\sigma(z) = 1/(1 + e^{-z})$（其中 $z = w^\\top x + b$）将输入向量 $x \\in \\mathbb{R}^d$ 映射到一个概率。在带有 $\\ell_2$ 正则化的二元交叉熵下的经验风险为\n$$\n\\mathcal{L}(w,b) = \\frac{1}{n}\\sum_{i=1}^n \\left[-y_i \\log \\sigma(w^\\top x_i + b) - (1-y_i)\\log(1 - \\sigma(w^\\top x_i + b))\\right] + \\frac{\\lambda}{2}\\|w\\|_2^2.\n$$\n通过 cutout 进行的数据增强被建模为一个随机变换 $T_f$。给定一张图像 $X \\in \\mathbb{R}^{H \\times W}$ 和一个目标面积分数 $f \\in [0,1]$，该变换通过将一个面积约为 $f \\cdot H \\cdot W$ 的单一轴对齐矩形区域的像素强度设置为零来进行遮蔽。该矩形在 $[0.3, 3.0]$ 区间内具有均匀随机的长宽比，并且其位置在图像内完全包含的约束下均匀随机。使用 cutout 进行训练时，以概率 $p \\in [0,1]$ 对每个训练样本独立应用 $T_f$。在测试时，通过对每个测试图像确定性地应用 $T_{f_{\\text{test}}}$ 来模拟遮挡（每个图像的矩形形状和位置具有独立的随机性）。\n\n按如下方式构建一个合成数据集。设图像高度 $H = 16$，宽度 $W = 16$，因此 $d = H \\cdot W = 256$。对于每个样本索引 $i \\in \\{1,\\dots,n\\}$，从 $\\{0,1\\}$ 中均匀随机地抽取一个标签 $y_i$。从标准差为 $\\sigma = 0.1$ 的零均值正态分布中抽取基底噪声 $N_i \\in \\mathbb{R}^{H \\times W}$，其元素相互独立。施加一个弱的全局类别依赖偏置：对于 $y_i = 1$，将常数 $g = 0.1$ 加到 $N_i$ 的所有像素上；对于 $y_i = 0$，从所有像素中减去 $g$。通过对左上角 $s \\times s$（其中 $s = 6$）的正方形区域施加一个强的局部判别性补丁：对于 $y_i = 1$，加上常数 $a_p = 1.0$；对于 $y_i = 0$，减去 $a_p$。将每张图像展平为 $\\mathbb{R}^{256}$ 中的一个向量。这种构造产生了两个冗余的线索：一个强的局部补丁和一个弱的全局偏置。一个未经遮挡训练的模型可能会严重依赖局部补丁，而一个使用 cutout 训练的模型在训练期间会经历偶然的遮挡，并可能为全局偏置分配非零权重，从而可能在测试时补丁被遮挡时提高鲁棒性。\n\n实现一个程序，该程序：\n- 使用上述过程生成一个包含 $N_{\\text{train}} = 2000$ 个样本的训练集和一个包含 $N_{\\text{test}} = 2000$ 个样本的测试集，每个测试用例使用独立的随机种子以确保可复现性。\n- 使用全批量梯度下降法训练两个二元逻辑回归模型，学习率为 $\\eta = 0.1$，$\\ell_2$ 正则化系数为 $\\lambda = 0.001$，优化步数为 $E = 300$：\n  - 一个未经增强（即 $p = 0$）训练的基线模型。\n  - 一个使用 cutout 增强的模型，其训练所用的增强概率为指定的 $p$，增强面积分数等于该用例的测试时遮挡分数 $f_{\\text{test}}$。\n- 在一个测试集上评估两个模型，该测试集已通过对每个测试样本应用一次 $T_{f_{\\text{test}}}$ 进行了遮挡。必须使用相同的被遮挡测试集来评估两个模型，以使性能具有可比性。\n- 计算每个模型的准确率，其中准确率是在预测概率上使用 0.5 阈值时正确分类的测试样本的比例，并以差异 $A_{\\text{aug}} - A_{\\text{base}}$ 的形式报告提升效果。\n\n使用以下参数值测试套件，其中每个元组为 $(p, f_{\\text{test}}, \\text{seed})$，并且每个数学实体都已指定：\n- 用例 1：$(p = 0.5, f_{\\text{test}} = 0.0, \\text{seed} = 42)$。\n- 用例 2：$(p = 0.5, f_{\\text{test}} = 0.3, \\text{seed} = 123)$。\n- 用例 3：$(p = 0.9, f_{\\text{test}} = 0.5, \\text{seed} = 7)$。\n- 用例 4：$(p = 0.0, f_{\\text{test}} = 0.3, \\text{seed} = 99)$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，“[$r_1,r_2,r_3,r_4$]”）。每个 $r_i$ 必须是表示相应测试用例的 $A_{\\text{aug}} - A_{\\text{base}}$ 的浮点数，四舍五入到 4 位小数。不应打印任何其他输出。",
            "solution": "用户想要解决一个关于 cutout 增强对模型鲁棒性影响的问题。\n\n### 步骤 1：提取已知条件\n- **问题类型**：二元分类，使用逻辑回归模型。\n- **输入数据**：尺寸为 $H \\times W$ 的合成卫星图像，其中 $H=16$，$W=16$。展平后的输入维度为 $d=256$。\n- **分类器**：$\\sigma(w^\\top x + b)$，其中 $\\sigma(z) = 1/(1+e^{-z})$，$w \\in \\mathbb{R}^d$，$b \\in \\mathbb{R}$。\n- **损失函数**：带 $\\ell_2$ 正则化的二元交叉熵：\n$$\n\\mathcal{L}(w,b) = \\frac{1}{n}\\sum_{i=1}^n \\left[-y_i \\log \\sigma(w^\\top x_i + b) - (1-y_i)\\log(1 - \\sigma(w^\\top x_i + b))\\right] + \\frac{\\lambda}{2}\\|w\\|_2^2.\n$$\n- **数据增强 (Cutout, $T_f$)**：用零遮蔽一个单一的轴对齐矩形区域。\n    - 目标面积分数：$f \\in [0,1]$。\n    - 近似面积：$f \\cdot H \\cdot W$。\n    - 矩形长宽比：在 $[0.3, 3.0]$ 内均匀随机。\n    - 矩形位置：均匀随机，完全包含在图像内。\n- **训练过程**：\n    - 优化器：全批量梯度下降。\n    - 学习率：$\\eta = 0.1$。\n    - $\\ell_2$ 正则化系数：$\\lambda = 0.001$。\n    - 优化步数（轮次）：$E = 300$。\n    - 增强模型训练：在每个训练步骤中，以概率 $p$ 对每个样本应用 $T_f$。训练增强分数 $f$ 设置为测试时遮挡分数 $f_{\\text{test}}$。\n    - 基线模型训练：无增强（$p=0$）。\n- **合成数据生成**：\n    - 样本：$N_{\\text{train}} = 2000$，$N_{\\text{test}} = 2000$。\n    - 标签 $y_i \\in \\{0,1\\}$：均匀随机。\n    - 基底噪声 $N_i$：像素级独立，来自均值为 0、标准差为 $\\sigma=0.1$ 的正态分布 $\\mathcal{N}(0, \\sigma^2)$。\n    - 全局偏置：对于 $y_i=1$ 加 $g=0.1$，对于 $y_i=0$ 减 $g=0.1$。\n    - 局部判别性补丁：左上角一个 $s \\times s$ 的正方形，其中 $s=6$。对于 $y_i=1$ 加 $a_p=1.0$，对于 $y_i=0$ 减 $a_p=1.0$。\n- **测试时评估**：\n    - 遮挡：对每个测试图像确定性地应用 $T_{f_{\\text{test}}}$（即概率 $p=1.0$）。\n    - 指标：准确率提升，定义为 $A_{\\text{aug}} - A_{\\text{base}}$。准确率是决策阈值为 0.5 时的正确预测比例。\n- **测试用例**：元组 $(p, f_{\\text{test}}, \\text{seed})$。\n    - 用例 1：$(0.5, 0.0, 42)$。\n    - 用例 2：$(0.5, 0.3, 123)$。\n    - 用例 3：$(0.9, 0.5, 7)$。\n    - 用例 4：$(0.0, 0.3, 99)$。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学或事实上的不健全性**：该问题很好地基于标准的机器学习原理，包括逻辑回归、梯度下降、正则化和数据增强（cutout）。用于测试 cutout 改善对遮挡鲁棒性这一假设的实验设置是该领域常见且有效的研究方法。该问题在科学上是合理的。\n2.  **不可形式化或不相关**：该问题与指定主题直接相关，并且完全可以形式化为计算任务。\n3.  **不完整或矛盾的设置**：所有必需的参数（$H, W, n, \\lambda, \\eta, E$，数据生成参数，增强参数）都已明确定义。过程描述得足够详细，可以实现。没有矛盾之处。\n4.  **不现实或不可行**：参数和设置在计算上是可行的，构成了一个标准的小规模机器学习实验。\n5.  **不适定或结构不良**：该问题是适定的。数据生成、训练和评估过程的详细规范确保了可以为每个测试用例计算出唯一且有意义的数值结果。\n6.  **伪深刻、琐碎或同义反复**：该问题通过一个具体的计算实验来探讨一个非凡的概念——数据增强与模型鲁棒性之间的相互作用。虽然设置是合成的，但其设计旨在隔离和研究一个特定现象，这是一种有效的科学方法。\n7.  **超出科学可验证性**：结果可通过实现所述算法并使用指定的种子运行来验证。\n\n### 步骤 3：结论与行动\n问题有效。我将继续提供完整的解决方案。\n\n### 算法解决方案\n该解决方案涉及实现指定的数据生成、模型训练和评估流程。\n\n#### 1. 数据生成\n对于一组 $n$ 个样本，我们生成图像 $X \\in \\mathbb{R}^{n \\times H \\times W}$ 和标签 $y \\in \\{0, 1\\}^n$。\n对于每个样本 $i \\in \\{1, \\dots, n\\}$：\n- 从伯努利分布（$p=0.5$）中抽取一个标签 $y_i$。\n- 抽取一个基底噪声图像 $N_i \\in \\mathbb{R}^{H \\times W}$，其中每个像素 $N_{i,jk} \\sim \\mathcal{N}(0, \\sigma^2)$，$\\sigma = 0.1$。\n- 定义一个类别条件乘数 $c_i = 2y_i - 1$，对于 $y_i=1$ 是 $+1$，对于 $y_i=0$ 是 $-1$。\n- 图像 $X_i$ 通过向噪声中添加全局偏置和局部补丁来构建：\n$$ X_i = N_i + g \\cdot c_i \\cdot \\mathbf{1}_{H \\times W} + a_p \\cdot c_i \\cdot P_s $$\n其中 $\\mathbf{1}_{H \\times W}$ 是一个全为 1 的 $H \\times W$ 矩阵，而 $P_s$ 是一个 $H \\times W$ 矩阵，其左上角 $s \\times s$（$s=6$）块为 1，其他地方为 0。常数为 $g=0.1$ 和 $a_p=1.0$。\n- 最后，每个 $X_i$ 被展平为一个向量 $x_i \\in \\mathbb{R}^d$，其中 $d=H \\cdot W = 256$。\n\n#### 2. Cutout 增强\ncutout 变换 $T_f$ 接收一个图像并遮蔽一个随机的矩形区域。对于一个尺寸为 $H \\times W$ 的图像和面积分数 $f$：\n- 目标面积是 $A = f \\cdot H \\cdot W$。\n- 从 $[0.3, 3.0]$ 中均匀采样一个长宽比 $r$。\n- 矩形的高度和宽度计算为 $h = \\sqrt{A/r}$ 和 $w = \\sqrt{Ar}$。这些值被四舍五入到最近的整数，并被裁剪以确保分别不超过 $H$ 和 $W$。\n- 从有效范围 $[0, H-h] \\times [0, W-w]$ 中均匀采样一个左上角坐标 $(y_0, x_0)$。\n- 矩形 $[y_0:y_0+h, x_0:x_0+w]$ 中的像素强度被设置为 0。\n\n#### 3. 模型训练\n逻辑回归参数 $w$ 和 $b$ 使用全批量梯度下降进行优化。损失函数 $\\mathcal{L}(w,b)$ 的梯度为：\n$$ \\nabla_w \\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(z_i) - y_i) x_i + \\lambda w $$\n$$ \\nabla_b \\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(z_i) - y_i) $$\n其中 $z_i = w^\\top x_i + b$。参数初始化为零，并更新 $E=300$ 步：\n$$ w \\leftarrow w - \\eta \\nabla_w \\mathcal{L} $$\n$$ b \\leftarrow b - \\eta \\nabla_b \\mathcal{L} $$\n- 对于**基线模型**，直接使用训练数据 $\\{x_i, y_i\\}$。\n- 对于**增强模型**，在 $E$ 个步骤中的每一步，都会创建一个新的增强训练集。对于原始训练批次中的每个样本 $x_i$，以概率 $p$ 应用 cutout 变换 $T_{f_{\\text{test}}}$。然后使用这个随机增强的批次计算梯度。\n\n#### 4. 评估\n- 首先，通过对生成的测试集中的每个图像应用一次 cutout 变换 $T_{f_{\\text{test}}}$，创建一个单一的被遮挡的测试集。\n- 基线模型（$w_{\\text{base}}, b_{\\text{base}}$）和增强模型（$w_{\\text{aug}}, b_{\\text{aug}}$）都用于对这个相同的被遮挡测试集进行预测。\n- 对于测试样本 $x_i$ 的预测 $\\hat{y}_i$ 为 $1$（如果 $\\sigma(w^\\top x_i + b) \\ge 0.5$），否则为 $0$。\n- 准确率计算为正确预测的比例：$A = \\frac{1}{N_{\\text{test}}}\\sum_{i=1}^{N_{\\text{test}}} \\mathbb{I}(\\hat{y}_i = y_i)$。\n- 每个用例的最终结果是准确率提升，$A_{\\text{aug}} - A_{\\text{base}}$，四舍五入到 4 位小数。\n\n对四个测试用例中的每一个重复此完整过程，并在每个用例开始时重新设定随机数生成器的种子，以确保可复现性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment for all test cases.\n    \"\"\"\n    H, W = 16, 16\n    d = H * W\n    N_train = 2000\n    N_test = 2000\n    noise_std = 0.1\n    g = 0.1\n    s = 6\n    a_p = 1.0\n    eta = 0.1\n    lambda_reg = 0.001\n    epochs = 300\n    aspect_ratio_range = (0.3, 3.0)\n\n    test_cases = [\n        # (p, f_test, seed)\n        (0.5, 0.0, 42),\n        (0.5, 0.3, 123),\n        (0.9, 0.5, 7),\n        (0.0, 0.3, 99),\n    ]\n\n    results = []\n\n    def generate_data(n_samples, rng):\n        \"\"\"Generates synthetic image data.\"\"\"\n        labels = rng.integers(0, 2, size=n_samples)\n        noise = rng.normal(0, noise_std, size=(n_samples, H, W))\n        \n        # c_i = +1 for label 1, -1 for label 0\n        class_multiplier = 2 * labels - 1\n        \n        # Add global bias\n        images = noise + g * class_multiplier[:, np.newaxis, np.newaxis]\n        \n        # Add local patch\n        patch = np.zeros((H, W))\n        patch[0:s, 0:s] = a_p\n        images += class_multiplier[:, np.newaxis, np.newaxis] * patch\n        \n        return images.reshape(n_samples, d), labels\n\n    def apply_cutout_on_batch(images_flat, f, p_aug, rng):\n        \"\"\"Applies cutout augmentation to a batch of images.\"\"\"\n        if p_aug == 0 or f == 0:\n            return images_flat.copy()\n\n        n_samples = images_flat.shape[0]\n        images = images_flat.reshape(n_samples, H, W).copy()\n        \n        target_area = f * H * W\n        \n        # Determine which images to augment\n        augment_indices = rng.random(n_samples)  p_aug\n        \n        for i in range(n_samples):\n            if not augment_indices[i]:\n                continue\n\n            aspect_ratio = rng.uniform(aspect_ratio_range[0], aspect_ratio_range[1])\n            \n            h = int(np.round(np.sqrt(target_area / aspect_ratio)))\n            w = int(np.round(np.sqrt(target_area * aspect_ratio)))\n            \n            h = min(h, H)\n            w = min(w, W)\n\n            if h > 0 and w > 0:\n                y0 = rng.integers(0, H - h + 1)\n                x0 = rng.integers(0, W - w + 1)\n                images[i, y0:y0+h, x0:x0+w] = 0.0\n\n        return images.reshape(n_samples, d)\n        \n    def logistic_sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-np.clip(z, -250, 250)))\n\n    def train_model(X_train, y_train, p_aug, f_aug, rng):\n        \"\"\"Trains a logistic regression model.\"\"\"\n        n_samples, n_features = X_train.shape\n        w = np.zeros(n_features)\n        b = 0.0\n\n        for _ in range(epochs):\n            X_epoch = apply_cutout_on_batch(X_train, f_aug, p_aug, rng)\n            \n            z = X_epoch @ w + b\n            sigma = logistic_sigmoid(z)\n            \n            grad_w = (1/n_samples) * X_epoch.T @ (sigma - y_train) + lambda_reg * w\n            grad_b = (1/n_samples) * np.sum(sigma - y_train)\n            \n            w -= eta * grad_w\n            b -= eta * grad_b\n            \n        return w, b\n\n    def predict(X, w, b):\n        \"\"\"Makes predictions with a trained model.\"\"\"\n        probas = logistic_sigmoid(X @ w + b)\n        return (probas >= 0.5).astype(int)\n\n    def calculate_accuracy(y_true, y_pred):\n        \"\"\"Calculates prediction accuracy.\"\"\"\n        return np.mean(y_true == y_pred)\n\n    for p_case, f_test_case, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # Generate data\n        X_train, y_train = generate_data(N_train, rng)\n        X_test, y_test = generate_data(N_test, rng)\n\n        # Train baseline model (no augmentation)\n        w_base, b_base = train_model(X_train, y_train, p_aug=0.0, f_aug=0.0, rng=rng)\n\n        # Train augmented model\n        # Augmentation fraction f is set to the test occlusion fraction f_test\n        w_aug, b_aug = train_model(X_train, y_train, p_aug=p_case, f_aug=f_test_case, rng=rng)\n\n        # Create occluded test set (deterministic application, p=1.0)\n        X_test_occluded = apply_cutout_on_batch(X_test, f_test_case, p_aug=1.0, rng=rng)\n\n        # Evaluate both models on the same occluded test set\n        preds_base = predict(X_test_occluded, w_base, b_base)\n        acc_base = calculate_accuracy(y_test, preds_base)\n        \n        preds_aug = predict(X_test_occluded, w_aug, b_aug)\n        acc_aug = calculate_accuracy(y_test, preds_aug)\n\n        improvement = round(acc_aug - acc_base, 4)\n        results.append(improvement)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "标准的 Mixup 策略使用随机的混合系数 $\\lambda$，但这可能导致对语义上毫无关联的图像进行不合理的混合。本练习  探索了一种更智能的自适应方法，即混合系数 $\\lambda$ 由图像对之间的语义相似度 $s$ 决定。通过实现这一策略，你将学习如何让数据增强变得更加感知上下文，并利用 Kullback-Leibler 散度作为量化指标，检验自适应混合是否能有效避免“有害”的混合。",
            "id": "3151945",
            "problem": "给定一个提案，该提案建议通过将混合系数选择为数据依赖的语义相似度的确定性函数来修改标准的线性 mixup 数据增强方法。您的任务是形式化并测试一个自适应规则，该规则将 mixup 系数设置为等于语义相似度，是否能够避免在语义疏远的样本对上进行有害的混合。该问题需要纯粹以数学和算法术语来解决，并实现为一个完整的程序。背景是深度学习数据增强：mixup、cutout 和 cutmix。尽管此处仅直接模拟 mixup，但您的推理应基于深度学习分类中使用的核心定义。\n\n基本原理和定义：\n- 考虑一个具有 $C$ 个类别的分类问题。设一个输入样本由 $\\mathbb{R}^d$ 中的一个向量表示，其类别标签由 $\\{0,1\\}^C$ 中的一个独热（one-hot）向量表示。\n- 标准 mixup 从一对样本 $(x_i, y_i)$ 和 $(x_j, y_j)$ 使用混合系数 $\\lambda \\in [0,1]$ 生成一个虚拟训练样本。混合后的输入为 $x_{\\text{mix}} = \\lambda x_i + (1-\\lambda) x_j$，混合后的目标是凸组合 $y_{\\text{mix}} = \\lambda y_i + (1-\\lambda) y_j$。\n- 设两个样本之间的语义相似度 $s \\in [0,1]$ 定义为余弦相似度映射到 $[0,1]$ 的结果，即 $s(x_i, x_j) = \\frac{1 + \\cos(\\theta(x_i,x_j))}{2}$，其中 $\\cos(\\theta(x_i,x_j)) = \\frac{x_i^\\top x_j}{\\|x_i\\|_2 \\|x_j\\|_2}$。\n- 假设对于一个混合对，存在一个语义一致的“教师”目标分布：当 $y_i \\neq y_j$ 时，该分布将概率质量 $s$ 分配给 $x_i$ 的类别，将概率质量 $(1-s)$ 分配给 $x_j$ 的类别；当 $y_i=y_j$ 时，将所有概率质量分配给该单一类别。这反映了分类中一个经过充分检验的原则，即目标应遵循数据的语义构成，其中 $s$ 作为构成的代理。\n- 混合目标 $y_{\\text{mix}}$ 的危害性由从教师分布 $p$ 到 mixup 目标 $q$ 的 Kullback–Leibler 散度 $D_{\\mathrm{KL}}(p \\| q)$ 来量化，计算时会添加一个小的正常数以避免除以零，这符合 $D_{\\mathrm{KL}}$ 对离散分布的标准定义。\n\n目标：\n- 在一个合成数据集上定义并评估三种关于 $\\lambda$ 的混合策略：\n  - 策略 A (固定)：$\\lambda = 0.5$。\n  - 策略 B (随机 Beta 分布)：$\\lambda \\sim \\mathrm{Beta}(\\alpha,\\alpha)$，其中 $\\alpha$ 为给定值。\n  - 策略 C (自适应)：$\\lambda = s$。\n- 如果一对样本 $(x_i,y_i),(x_j,y_j)$ 满足 $y_i \\neq y_j$ 且 $s(x_i,x_j) \\le s_{\\mathrm{cut}}$，则该对样本被认为是语义疏远的。\n- 如果一个混合对在某个策略下的 $D_{\\mathrm{KL}}(p \\| q)$ 超过给定的阈值 $\\delta > 0$，则认为它是有害的。\n- 对于每个测试用例，使用 $M$ 个随机抽样的样本对，分别对每种策略，估计在语义疏远的样本对子集上有害样本对的比例。如果在某个测试用例中没有发现语义疏远的样本对（即该集合为空），则将所有策略的有害比例定义为 $0$，并在此情况下视作自适应策略已成功避免了有害混合。\n\n每个测试用例的数据集构建：\n- 为 $c \\in \\{1,\\dots,C\\}$ 选择 $C$ 个类别原型 $\\mu_c \\in \\mathbb{R}^d$。将它们放置为缩放后的标准正交向量，使得 $\\mu_c$ 是一个缩放后的基向量，缩放比例由一个“分离度”（separation）参数给出。\n- 对于每个类别 $c$，从均值为 $\\mu_c$、协方差为各向同性 $\\sigma^2 I_d$ 的多元正态分布中抽取 $n$ 个样本 $x$。相应地分配独热标签 $y$。\n\n数值计算说明：\n- 所有角度都是无量纲的，在三角函数内部使用弧度。\n- 本问题中没有物理单位。\n- 所有概率和比例必须计算为 $[0,1]$ 区间内的实数。\n\n测试套件：\n对于每个测试用例，产生一个布尔值结果，其定义为：“如果自适应策略（策略 C）在语义疏远样本对上的有害比例严格低于策略 A 和策略 B 的有害比例，或者语义疏远样本对的数量为零，则为 True”，否则为 “False”。使用单一固定的随机种子以保证可复现性。评估以下四个测试用例：\n\n- 测试用例 $1$ (一般的“理想路径” (happy path))：\n  - $d=16$, $C=3$, $n=50$, $\\sigma=0.6$, separation $=2.5$, $s_{\\mathrm{cut}}=0.3$, $\\delta=0.02$, $M=4000$, 配对模式 = “所有配对”, $\\alpha=0.4$。\n- 测试用例 $2$ (边界情况：低类间分离度，可能很少或没有疏远样本对)：\n  - $d=16$, $C=3$, $n=50$, $\\sigma=0.6$, separation $=0.5$, $s_{\\mathrm{cut}}=0.3$, $\\delta=0.02$, $M=4000$, 配对模式 = “所有配对”, $\\alpha=0.4$。\n- 测试用例 $3$ (边界情况：高类间分离度)：\n  - $d=16$, $C=3$, $n=50$, $\\sigma=0.3$, separation $=4.0$, $s_{\\mathrm{cut}}=0.3$, $\\delta=0.02$, $M=4000$, 配对模式 = “所有配对”, $\\alpha=0.4$。\n- 测试用例 $4$ (边缘情况：仅同类配对)：\n  - $d=16$, $C=3$, $n=50$, $\\sigma=0.6$, separation $=2.0$, $s_{\\mathrm{cut}}=0.3$, $\\delta=0.02$, $M=4000$, 配对模式 = “仅同类配对”, $\\alpha=0.4$。\n\n输出格式：\n- 您的程序应生成单行输出，其中包含四个测试用例的四个布尔值结果，按顺序排列，格式为方括号括起来的 Python 风格列表。例如，一个可接受的输出格式形如 $[b_1,b_2,b_3,b_4]$，其中每个 $b_k$ 为 True 或 False。\n- 程序必须是完全自包含的，不得需要任何用户输入、外部文件或网络访问。使用单一固定的随机种子，以使您的结果是确定性的。\n\n注记：\n- 您的推导应基于上述标准定义。除了已建立的凸组合、余弦相似度、独热编码和 Kullback–Leibler 散度的定义外，不要使用任何快捷公式。\n- 尽管更广泛的主题包括 cutout 和 cutmix，但本计算实验侧重于 mixup。您的推理仍应意识到 cutout 会遮蔽区域（将像素设置为零），而 cutmix 会在图像之间粘贴补丁；两者都与 mixup 不同，因为它们的输入混合不是全局凸组合，但 cutmix 中的目标混合仍然是凸组合，而 cutout 通常保留原始标签。",
            "solution": "我们从分类目标、凸组合、余弦相似度和 Kullback–Leibler 散度的基本定义出发。目标是形式化自适应 mixup，其中标签和输入的混合系数取决于两个样本之间可测量的语义相似度，然后评估这种自适应性是否能减少在语义疏远样本对上的有害混合。\n\n步骤 $1$：数据模型和原型\n- 我们考虑 $\\mathbb{R}^d$ 中的 $C$ 个类别。一种简单且经过充分检验的合成类别结构的方法是将原型 $\\mu_c$ 放置在标准正交轴上，并由一个标量“分离度”（separation）进行缩放以控制类间距离。具体来说，令 $e_c$ 表示 $\\mathbb{R}^d$ 中的标准基向量，并定义 $\\mu_c = \\text{separation} \\cdot e_c$ 对于 $c \\in \\{1,\\dots,C\\}$，这样当 $c \\neq c'$ 时 $\\mu_c^\\top \\mu_{c'} = 0$ 且 $\\|\\mu_c\\|_2 = \\text{separation}$。这确保了在各向同性噪声下受控的类别分离度。\n- 对于每个类别 $c$，我们从 $\\mathcal{N}(\\mu_c, \\sigma^2 I_d)$ 分布中抽取 $n$ 个样本 $x$，分配 $y_c=1$ 其余为零的独热标签 $y$，并收集数据集 $\\{(x_i,y_i)\\}$。\n\n步骤 $2$：语义相似度\n- 对于一对样本 $(x_i,y_i)$ 和 $(x_j,y_j)$，定义余弦相似度 $\\cos(\\theta(x_i,x_j)) = \\frac{x_i^\\top x_j}{\\|x_i\\|_2 \\|x_j\\|_2}$。通过 $s(x_i,x_j) = \\frac{1 + \\cos(\\theta(x_i,x_j))}{2}$ 将其映射到单位区间以获得语义相似度 $s \\in [0,1]$。这种映射是一种单调变换，保留了顺序并限定了相似度的范围，这是度量学习中被广泛接受的做法。\n\n步骤 $3$：与语义对齐的教师分布\n- 我们如下定义一对样本的语义对齐教师分布 $p$。如果 $y_i \\neq y_j$，则 $p$ 将概率质量 $s$ 分配给类别 $y_i$，将 $(1-s)$ 分配给类别 $y_j$，而对所有其他类别分配零质量。如果 $y_i = y_j$，则 $p$ 是该类别上的独热分布（因为混合两个相同类别的样本应保持相同的目标）。\n- 这与一个基本思想相符，即在分类中，目标分布的正确性概率应遵循数据的语义内容。变量 $s$ 编码了一个样本的语义对混合输入的贡献分数。\n\n步骤 $4$：Mixup 策略和混合目标\n- 标准 mixup 将混合目标构造成凸组合 $q_\\lambda = \\lambda y_i + (1-\\lambda) y_j$。我们评估三种策略：\n  - 策略 A：$\\lambda = 0.5$ (固定的对半混合)。\n  - 策略 B：$\\lambda \\sim \\mathrm{Beta}(\\alpha,\\alpha)$，这在实践中是随机化混合权重的常用选择；我们为每个测试用例固定 $\\alpha$，并使用固定的随机种子以保证可复现性。\n  - 策略 C：$\\lambda = s$ (自适应)，使用可测量的语义相似度来设置混合比例。\n- 对于同类样本对，由于 $y_i=y_j$，所有策略都产生相同的 $q_\\lambda$，它等于底层的独热标签。\n\n步骤 $5$：使用 Kullback–Leibler 散度进行危害量化\n- 为了衡量语义感知的教师分布 $p$ 和混合目标 $q_\\lambda$ 之间的对齐程度，我们使用 Kullback–Leibler 散度\n$$\nD_{\\mathrm{KL}}(p \\| q_\\lambda) = \\sum_{c=1}^{C} p_c \\log\\left(\\frac{p_c}{q_{\\lambda,c}}\\right),\n$$\n按照惯例，在对数内对 $p_c$ 和 $q_{\\lambda,c}$ 都加上一个小的 $\\varepsilon0$ 以避免当概率为零时出现数值问题。这符合 $D_{\\mathrm{KL}}$ 对离散分布的标准定义，并提供了错位的量化度量。\n- 如果 $D_{\\mathrm{KL}}(p \\| q_\\lambda)  \\delta$（其中 $\\delta0$ 是给定的阈值），则认为一个混合对是有害的。\n\n步骤 $6$：关注语义疏远的样本对\n- 当样本在语义上疏远时，有害混合尤其令人担忧。因此，我们将评估限制在满足 $y_i \\neq y_j$ 和 $s(x_i,x_j) \\le s_{\\mathrm{cut}}$ 的样本对上。截断值 $s_{\\mathrm{cut}}$ 形式化了“疏远样本对”的概念，并在每个测试用例中指定。\n- 如果在某个测试用例中没有找到这样的样本对（例如，当类别严重重叠且相似度很高时），我们约定将所有策略的有害比例定义为 $0$，并在这种空泛情况下宣布自适应策略已避免了有害混合。\n\n步骤 $7$：测试用例和预期行为\n- 我们设计了四个测试用例来覆盖不同的情景：\n  - 测试用例 $1$：中等的分离度和噪声。我们预计会有许多跨类样本对，其相似度分布在一个范围内。策略 C 设置 $\\lambda=s$，因此在跨类样本对上 $q_\\lambda$ 与 $p$ 完全匹配，从而得到 $D_{\\mathrm{KL}}=0$，而策略 A 和 B 在 $s \\neq \\lambda$ 时通常会产生正的散度。\n  - 测试用例 $2$：低分离度。跨类相似度倾向于较高，可能产生很少或没有疏远样本对（$s \\le s_{\\mathrm{cut}}$）。根据我们的规则，在这种边界场景中，自适应策略被认为没有产生有害的疏远混合。\n  - 测试用例 $3$：高分离度伴随低噪声。许多跨类样本对是疏远的。策略 C 仍然与语义保持一致（$\\lambda=s$），而策略 A 和 B 对大多数疏远样本对产生错配，导致更大的散度。\n  - 测试用例 $4$：仅同类配对。由于 $y_i = y_j$，所有策略的结果都一致，并且在空的疏远跨类样本对集合上，它们的有害比例都为零，这算作自适应策略的一次成功。\n\n步骤 $8$：计算和输出\n- 对于每个测试用例，我们根据指定的配对模式（所有配对或仅同类配对）抽样 $M$ 个样本对，计算 $s$，识别疏远子集，为每个策略评估 $D_{\\mathrm{KL}}(p\\|q_\\lambda)$，并计算有害比例。然后我们输出一个布尔值，指示自适应策略的有害比例是否严格低于两个基线（或者是否存在零个疏远对）。\n- 最终程序将四个测试用例的结果打印为单行：一个包含四个布尔值的有序列表。\n\n关于 cutout 和 cutmix 的讨论：\n- Cutout 通过将图像输入的连续区域设置为零来遮蔽它们，同时保持原始标签；它不执行输入或标签的凸组合。Cutmix 将一张图像的某个区域替换为另一张图像的补丁，并按补丁面积比例混合标签。相比之下，mixup 对整个输入形成全局凸组合，并相应地执行标签插值。这里探讨的自适应思想可以自然地扩展到 cutmix，通过将标签混合系数设置为一个依赖于语义的权重（而不是或除了面积），而 cutout 通常保留原始标签，因此根本不会使用 $\\lambda$。\n\n通过将评估建立在语义感知的教师分布和混合目标之间的 $D_{\\mathrm{KL}}$ 之上，我们遵循了统计学习中的核心定义，而没有引入快捷公式，并且我们提供了一种有原则且可测试的方法来检验自适应策略是否避免了在疏远样本对上的有害混合。",
            "answer": "```python\nimport numpy as np\n\n# Environment: Python 3.12, numpy 1.23.5, scipy 1.11.4 (not used here)\n\ndef generate_prototypes(C, d, separation):\n    \"\"\"\n    Generate class prototypes as scaled standard basis vectors in R^d.\n    \"\"\"\n    prototypes = np.zeros((C, d), dtype=float)\n    for c in range(C):\n        prototypes[c, c] = separation\n    return prototypes\n\ndef generate_dataset(C, d, n_per_class, sigma, separation, rng):\n    \"\"\"\n    Generate synthetic dataset: for each class, sample n_per_class points from N(mu_c, sigma^2 I).\n    \"\"\"\n    mus = generate_prototypes(C, d, separation)\n    X_list = []\n    y_list = []\n    for c in range(C):\n        Xc = rng.normal(loc=0.0, scale=sigma, size=(n_per_class, d)) + mus[c]\n        yc = np.zeros((n_per_class, C), dtype=float)\n        yc[:, c] = 1.0\n        X_list.append(Xc)\n        y_list.append(yc)\n    X = np.vstack(X_list)\n    Y = np.vstack(y_list)\n    labels = np.argmax(Y, axis=1)\n    return X, Y, labels\n\ndef cosine_similarity(x, y):\n    \"\"\"\n    Cosine similarity between two vectors.\n    \"\"\"\n    nx = np.linalg.norm(x)\n    ny = np.linalg.norm(y)\n    if nx == 0.0 or ny == 0.0:\n        return 0.0\n    return float(np.dot(x, y) / (nx * ny))\n\ndef semantic_similarity(x, y):\n    \"\"\"\n    Map cosine similarity to [0,1]: s = (1 + cos) / 2.\n    \"\"\"\n    cos = cosine_similarity(x, y)\n    return (1.0 + cos) / 2.0\n\ndef teacher_distribution(li, lj, s, C):\n    \"\"\"\n    Build teacher distribution p over C classes:\n    - If li != lj: mass s on li and (1-s) on lj.\n    - If li == lj: one-hot on li.\n    \"\"\"\n    p = np.zeros(C, dtype=float)\n    if li == lj:\n        p[li] = 1.0\n    else:\n        p[li] = s\n        p[lj] = 1.0 - s\n    return p\n\ndef mixup_target(li, lj, lam, C):\n    \"\"\"\n    Mixed target q over C classes using mixup coefficient lam.\n    \"\"\"\n    q = np.zeros(C, dtype=float)\n    if li == lj:\n        q[li] = 1.0\n    else:\n        q[li] = lam\n        q[lj] = 1.0 - lam\n    return q\n\ndef kl_divergence(p, q, eps=1e-12):\n    \"\"\"\n    D_KL(p || q) for discrete distributions with epsilon smoothing.\n    \"\"\"\n    p_safe = p + eps\n    q_safe = q + eps\n    p_safe /= p_safe.sum()\n    q_safe /= q_safe.sum()\n    return float(np.sum(p_safe * (np.log(p_safe) - np.log(q_safe))))\n\ndef sample_pairs(labels, M, mode, rng):\n    \"\"\"\n    Sample M pairs of indices according to mode:\n    - 'all': any pairs uniformly at random with replacement\n    - 'same': pairs restricted to same-class\n    - 'cross': pairs restricted to different classes\n    Returns arrays of indices i_idx, j_idx.\n    \"\"\"\n    N = len(labels)\n    i_idx = []\n    j_idx = []\n    # Precompute class indices\n    class_to_indices = {}\n    for c in np.unique(labels):\n        class_to_indices[c] = np.where(labels == c)[0]\n    if mode == 'all':\n        i_idx = rng.integers(0, N, size=M)\n        j_idx = rng.integers(0, N, size=M)\n        return i_idx, j_idx\n    elif mode == 'same':\n        # For each sample, choose j from same class uniformly\n        for _ in range(M):\n            i = int(rng.integers(0, N))\n            ci = labels[i]\n            choices = class_to_indices[ci]\n            j = int(choices[rng.integers(0, len(choices))])\n            i_idx.append(i)\n            j_idx.append(j)\n        return np.array(i_idx, dtype=int), np.array(j_idx, dtype=int)\n    elif mode == 'cross':\n        # For each sample, choose i and j with different classes\n        classes = list(class_to_indices.keys())\n        for _ in range(M):\n            ci = int(classes[rng.integers(0, len(classes))])\n            # choose a different class\n            cj_choices = [c for c in classes if c != ci]\n            cj = int(cj_choices[rng.integers(0, len(cj_choices))])\n            i = int(class_to_indices[ci][rng.integers(0, len(class_to_indices[ci]))])\n            j = int(class_to_indices[cj][rng.integers(0, len(class_to_indices[cj]))])\n            i_idx.append(i)\n            j_idx.append(j)\n        return np.array(i_idx, dtype=int), np.array(j_idx, dtype=int)\n    else:\n        raise ValueError(\"Unknown mode\")\n\ndef evaluate_case(d, C, n, sigma, separation, s_cut, delta, M, pair_mode, alpha, rng):\n    \"\"\"\n    Evaluate one test case and return boolean result as specified.\n    \"\"\"\n    X, Y, labels = generate_dataset(C=C, d=d, n_per_class=n, sigma=sigma, separation=separation, rng=rng)\n    # Determine sampling mode\n    if pair_mode == 'all':\n        mode = 'all'\n    elif pair_mode == 'same':\n        mode = 'same'\n    elif pair_mode == 'cross':\n        mode = 'cross'\n    else:\n        raise ValueError(\"pair_mode must be one of: all, same, cross\")\n\n    i_idx, j_idx = sample_pairs(labels, M, mode, rng)\n    # For policy A: fixed lambda=0.5\n    # For policy B: lambda ~ Beta(alpha,alpha)\n    # For policy C: lambda = s\n    harm_counts = {'A': 0, 'B': 0, 'C': 0}\n    total_distant = 0\n\n    # Pre-sample Beta for policy B\n    if alpha = 0:\n        raise ValueError(\"alpha must be positive\")\n    lam_beta = rng.beta(alpha, alpha, size=M)\n\n    for k in range(M):\n        i = int(i_idx[k])\n        j = int(j_idx[k])\n        li = int(labels[i])\n        lj = int(labels[j])\n        # Only consider distant subset: cross-class and s = s_cut\n        s = semantic_similarity(X[i], X[j])\n        if (li != lj) and (s = s_cut):\n            total_distant += 1\n            # Teacher\n            p = teacher_distribution(li, lj, s, C)\n            # Policy A\n            qA = mixup_target(li, lj, lam=0.5, C=C)\n            dA = kl_divergence(p, qA)\n            if dA > delta:\n                harm_counts['A'] += 1\n            # Policy B\n            qB = mixup_target(li, lj, lam=float(lam_beta[k]), C=C)\n            dB = kl_divergence(p, qB)\n            if dB > delta:\n                harm_counts['B'] += 1\n            # Policy C\n            qC = mixup_target(li, lj, lam=s, C=C)\n            dC = kl_divergence(p, qC)\n            if dC > delta:\n                harm_counts['C'] += 1\n\n    # If no distant pairs, by convention adaptive policy succeeds\n    if total_distant == 0:\n        return True\n\n    fracA = harm_counts['A'] / total_distant\n    fracB = harm_counts['B'] / total_distant\n    fracC = harm_counts['C'] / total_distant\n    # Strictly lower than both baselines\n    return (fracC  fracA) and (fracC  fracB)\n\ndef solve():\n    rng = np.random.default_rng(seed=1337)\n\n    test_cases = [\n        # (d, C, n, sigma, separation, s_cut, delta, M, pair_mode, alpha)\n        (16, 3, 50, 0.6, 2.5, 0.3, 0.02, 4000, 'all', 0.4),        # Case 1\n        (16, 3, 50, 0.6, 0.5, 0.3, 0.02, 4000, 'all', 0.4),        # Case 2\n        (16, 3, 50, 0.3, 4.0, 0.3, 0.02, 4000, 'all', 0.4),        # Case 3\n        (16, 3, 50, 0.6, 2.0, 0.3, 0.02, 4000, 'same', 0.4),       # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        d, C, n, sigma, separation, s_cut, delta, M, pair_mode, alpha = case\n        res = evaluate_case(d, C, n, sigma, separation, s_cut, delta, M, pair_mode, alpha, rng)\n        results.append(res)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "数据增强技术在提升模型性能的同时，也可能带来意想不到的负面影响，例如对少数群体造成不成比例的伤害。本练习  为你提供了一个用于量化此类影响的实用公平性测试框架。你将学习如何通过计算和比较不同子群在增强前后的准确率和校准误差变化，来评估一项增强策略是否存在不公平性，这是负责任的人工智能开发中一项至关重要的实践技能。",
            "id": "3151966",
            "problem": "您的任务是设计并实现一个基于原则的公平性测试，以评估基于输入和标签的凸混合（mixup）以及图像间区域替换（CutMix）的数据增强方法，是否相对于多数子群，对少数子群造成了不成比例的损害。评估必须使用子群准确率和校准度。解决方案必须依赖于基本定义和标准经验估计量。\n\n核心设定是二元分类。对于任何真实标签为 $y \\in \\{0,1\\}$、模型对类别 1 的预测概率得分为 $s \\in [0,1]$ 的样本，定义预测标签 $\\hat{y}$ 为：如果 $s \\ge 0.5$，则 $\\hat{y} = 1$，否则 $\\hat{y} = 0$。定义预测类别的置信度 $c$ 为：如果 $\\hat{y} = 1$，则 $c = s$；如果 $\\hat{y} = 0$，则 $c = 1-s$。对于任何有限子群数据集 $\\mathcal{D} = \\{(y_i,s_i)\\}_{i=1}^n$，经验准确率的计算公式为\n$$\nA(\\mathcal{D}) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\hat{y}_i = y_i\\},\n$$\n以及使用 $B$ 个等宽区间的期望校准误差 (ECE) 的计算公式为\n$$\n\\mathrm{ECE}_B(\\mathcal{D}) = \\sum_{b=1}^B \\frac{n_b}{n} \\left| \\mathrm{acc}_b - \\mathrm{conf}_b \\right|,\n$$\n其中，区间 $b$ 包含所有满足置信度 $c_i$ 落在将 $[0,1]$ 等分为 $B$ 个子区间后第 $b$ 个子区间内的样本索引 $i$，$n_b$ 是区间 $b$ 中的样本数，$\\mathrm{acc}_b = \\frac{1}{n_b} \\sum_{i \\in b} \\mathbf{1}\\{\\hat{y}_i = y_i\\}$ 是区间 $b$ 内的经验准确率，$\\mathrm{conf}_b = \\frac{1}{n_b} \\sum_{i \\in b} c_i$ 是区间 $b$ 内的平均置信度。按照惯例，空区间对总和的贡献为零。这些定义是您进行推导和实现的唯一允许依据。\n\n为测试数据增强方法对少数子群相对于多数子群是否造成不成比例的影响，假设我们对两个子群都有基线（增强前）预测和增强后预测。令少数子群表示为 $\\mathcal{D}^{\\mathrm{base}}_{\\mathrm{min}}$ 和 $\\mathcal{D}^{\\mathrm{aug}}_{\\mathrm{min}}$，多数子群表示为 $\\mathcal{D}^{\\mathrm{base}}_{\\mathrm{maj}}$ 和 $\\mathcal{D}^{\\mathrm{aug}}_{\\mathrm{maj}}$。定义子群 $G \\in \\{\\mathrm{min}, \\mathrm{maj}\\}$ 的准确率损害为\n$$\nH_{\\mathrm{acc}}(G) = \\max\\left(0, \\; A\\!\\left(\\mathcal{D}^{\\mathrm{base}}_{G}\\right) - A\\!\\left(\\mathcal{D}^{\\mathrm{aug}}_{G}\\right) \\right),\n$$\n定义子群 $G$ 的校准度损害为\n$$\nH_{\\mathrm{cal}}(G) = \\max\\left(0, \\; \\mathrm{ECE}_B\\!\\left(\\mathcal{D}^{\\mathrm{aug}}_{G}\\right) - \\mathrm{ECE}_B\\!\\left(\\mathcal{D}^{\\mathrm{base}}_{G}\\right) \\right).\n$$\n给定两个非负阈值 $\\tau_{\\mathrm{acc}}$ 和 $\\tau_{\\mathrm{cal}}$，如果满足以下任一条件，我们则认为存在不成比例的影响：\n$$\nH_{\\mathrm{acc}}(\\mathrm{min}) - H_{\\mathrm{acc}}(\\mathrm{maj}) > \\tau_{\\mathrm{acc}}\n$$\n或\n$$\nH_{\\mathrm{cal}}(\\mathrm{min}) - H_{\\mathrm{cal}}(\\mathrm{maj}) > \\tau_{\\mathrm{cal}}.\n$$\n\n您的程序必须实现此测试，并为每个测试用例返回一个布尔值，以指示是否检测到不成比例的影响。\n\n对所有测试用例使用以下固定设置：\n- 区间数量：$B = 5$。\n- 准确率损害阈值：$\\tau_{\\mathrm{acc}} = 0.05$。\n- 校准度损害阈值：$\\tau_{\\mathrm{cal}} = 0.15$。\n\n实现以下测试套件。每个测试用例都为少数和多数子群提供基线和增强后的真实标签数组和预测概率数组。所有数组都是有序对 $(\\text{labels}, \\text{scores})$，其中标签在 $\\{0,1\\}$ 中，得分在 $[0,1]$ 中。\n\n测试用例 1（旨在模拟 mixup 对少数群体的损害大于多数群体）：\n- 少数子群基线：标签 $[1,0,1,0,1,0]$，得分 $[0.9,0.2,0.8,0.3,0.7,0.4]$。\n- 少数子群增强后：得分 $[0.6,0.4,0.55,0.45,0.6,0.5]$（标签不变）。\n- 多数子群基线：标签 $[1,0,1,0,1,0,1,0,1,0,1,0]$，得分 $[0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05]$。\n- 多数子群增强后：得分 $[0.8,0.2,0.8,0.2,0.8,0.2,0.8,0.2,0.8,0.2,0.8,0.2]$。\n\n测试用例 2（旨在模拟 CutMix 改善了两个群体，没有造成不成比例的损害）：\n- 少数子群基线：标签 $[1,0,1,0,0,1]$，得分 $[0.6,0.6,0.55,0.55,0.4,0.4]$。\n- 少数子群增强后：得分 $[0.85,0.2,0.75,0.25,0.3,0.8]$。\n- 多数子群基线：标签 $[1,0,1,0,1,0,1,0,1,0,1,0]$，得分 $[0.6,0.4,0.6,0.4,0.6,0.4,0.6,0.4,0.6,0.4,0.6,0.4]$。\n- 多数子群增强后：得分 $[0.85,0.15,0.85,0.15,0.85,0.15,0.85,0.15,0.85,0.15,0.85,0.15]$。\n\n测试用例 3（边界情况，两个群体的性能相似地下降，因此在给定阈值下没有不成比例的损害）：\n- 少数子群基线：标签 $[1,1,0,0,1,0]$，得分 $[0.9,0.9,0.1,0.1,0.8,0.2]$。\n- 少数子群增强后：得分 $[0.7,0.6,0.4,0.55,0.6,0.6]$。\n- 多数子群基线：标签 $[1,0,1,0,1,0,1,0,1,0,1,0]$，得分 $[0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05]$。\n- 多数子群增强后：得分 $[0.6,0.6,0.6,0.6,0.49,0.49,0.49,0.49,0.8,0.2,0.8,0.2]$。\n\n在每个测试中，增强前后的所有标签都保持不变。每个测试用例的输出是如上所述计算出的单个布尔值。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个逗号分隔的 Python 风格列表，每个测试用例对应一个布尔值，顺序与上述相同，例如 $[{\\tt True},{\\tt False},{\\tt False}]$。",
            "solution": "该问题要求实现一个特定的公平性测试，以确定数据增强技术是否对少数子群造成了相对于多数子群而言不成比例的损害。该测试基于两个关键指标的变化：准确率和校准误差。我们将首先形式化这些指标的计算，然后构建完整的测试流程。\n\n这是一个二元分类任务，对于每个数据点 $i$，我们给定一个真实标签 $y_i \\in \\{0, 1\\}$ 和一个模型对类别 1 的预测概率得分 $s_i \\in [0, 1]$。\n\n首先，我们必须为每个数据点推导出两个中间量：预测标签 $\\hat{y}_i$ 和预测类别的置信度 $c_i$。\n预测标签 $\\hat{y}_i$ 通过将得分 $s_i$ 在 $0.5$ 处进行阈值化来确定：\n$$\n\\hat{y}_i = \\begin{cases} 1  \\text{if } s_i \\ge 0.5 \\\\ 0  \\text{if } s_i  0.5 \\end{cases}\n$$\n这可以使用指示函数表示为 $\\hat{y}_i = \\mathbf{1}\\{s_i \\ge 0.5\\}$。\n\n预测类别的置信度 $c_i$ 是模型为其预测的类别所报告的概率。如果模型预测为类别 $1$ (即 $\\hat{y}_i=1$)，置信度就是得分 $s_i$。如果它预测为类别 $0$ (即 $\\hat{y}_i=0$)，置信度是 $1-s_i$。形式上：\n$$\nc_i = \\begin{cases} s_i  \\text{if } \\hat{y}_i = 1 \\\\ 1-s_i  \\text{if } \\hat{y}_i = 0 \\end{cases}\n$$\n这可以写成 $c_i = s_i \\cdot \\hat{y}_i + (1-s_i) \\cdot (1-\\hat{y}_i)$。\n\n有了这些量，我们就可以为给定的数据集 $\\mathcal{D} = \\{(y_i, s_i)\\}_{i=1}^n$ 定义两个主要指标。\n\n经验准确率 $A(\\mathcal{D})$ 是正确预测的比例：\n$$\nA(\\mathcal{D}) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\hat{y}_i = y_i\\}\n$$\n其中 $n$ 是 $\\mathcal{D}$ 中的样本数。\n\n期望校准误差 $\\mathrm{ECE}_B(\\mathcal{D})$ 量化了模型的置信度与其准确率之间的差异。它是通过将置信度范围 $[0, 1]$ 划分为 $B$ 个等宽区间来计算的。对于每个区间 $b \\in \\{1, \\dots, B\\}$，我们计算置信度得分 $c_i$ 落入该区间的样本的平均准确率和平均置信度。\n设第 $b$ 个区间对应于区间 $[\\frac{b-1}{B}, \\frac{b}{B})$。最后一个区间 $b=B$ 必须包含上界 1，即 $[\\frac{B-1}{B}, 1]$。\n对于每个区间 $b$，令 $n_b$ 为其中的样本数。如果 $n_b > 0$，我们定义区间准确率 $\\mathrm{acc}_b$ 和区间置信度 $\\mathrm{conf}_b$ 如下：\n$$\n\\mathrm{acc}_b = \\frac{1}{n_b} \\sum_{i \\in b} \\mathbf{1}\\{\\hat{y}_i = y_i\\}\n$$\n$$\n\\mathrm{conf}_b = \\frac{1}{n_b} \\sum_{i \\in b} c_i\n$$\nECE 是所有区间上，区间准确率和区间置信度之间绝对差的加权平均值：\n$$\n\\mathrm{ECE}_B(\\mathcal{D}) = \\sum_{b=1}^B \\frac{n_b}{n} \\left| \\mathrm{acc}_b - \\mathrm{conf}_b \\right|\n$$\n根据问题描述，空区间 ($n_b=0$) 对总和的贡献为 0。对于所有测试用例，区间的数量固定为 $B=5$。\n\n接下来，我们定义子群 $G \\in \\{\\mathrm{min}, \\mathrm{maj}\\}$ 的损害指标。准确率损害 $H_{\\mathrm{acc}}(G)$ 衡量应用增强后准确率的下降程度。它被定义为基线准确率和增强后准确率之间差值的正数部分：\n$$\nH_{\\mathrm{acc}}(G) = \\max\\left(0, \\; A\\!\\left(\\mathcal{D}^{\\mathrm{base}}_{G}\\right) - A\\!\\left(\\mathcal{D}^{\\mathrm{aug}}_{G}\\right) \\right)\n$$\n校准度损害 $H_{\\mathrm{cal}}(G)$ 衡量校准误差的增加程度。它被定义为增强后 ECE 和基线 ECE 之间差值的正数部分：\n$$\nH_{\\mathrm{cal}}(G) = \\max\\left(0, \\; \\mathrm{ECE}_B\\!\\left(\\mathcal{D}^{\\mathrm{aug}}_{G}\\right) - \\mathrm{ECE}_B\\!\\left(\\mathcal{D}^{\\mathrm{base}}_{G}\\right) \\right)\n$$\n\n最后，如果少数群体在准确率或校准度方面所遭受的损害，比多数群体所遭受的损害超出指定的阈值，则检测到不成比例的影响。给定的阈值为 $\\tau_{\\mathrm{acc}} = 0.05$ 和 $\\tau_{\\mathrm{cal}} = 0.15$。不成比例影响的条件是：\n$$\n(H_{\\mathrm{acc}}(\\mathrm{min}) - H_{\\mathrm{acc}}(\\mathrm{maj}) > \\tau_{\\mathrm{acc}}) \\quad \\lor \\quad (H_{\\mathrm{cal}}(\\mathrm{min}) - H_{\\mathrm{cal}}(\\mathrm{maj}) > \\tau_{\\mathrm{cal}})\n$$\n实现将包括一个函数，该函数为给定的数据集计算 $A(\\mathcal{D})$ 和 $\\mathrm{ECE}_B(\\mathcal{D})$。此函数将对一个测试用例中的四个数据集（$\\mathcal{D}^{\\mathrm{base}}_{\\mathrm{min}}$、$\\mathcal{D}^{\\mathrm{aug}}_{\\mathrm{min}}$、$\\mathcal{D}^{\\mathrm{base}}_{\\mathrm{maj}}$ 和 $\\mathcal{D}^{\\mathrm{aug}}_{\\mathrm{maj}}$）分别调用。然后，使用得出的指标来计算损害值并评估最终的布尔条件。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_metrics(labels, scores, B):\n    \"\"\"\n    Calculates empirical accuracy and ECE for a given dataset.\n\n    Args:\n        labels (np.ndarray): Array of true labels {0, 1}.\n        scores (np.ndarray): Array of model probability scores for class 1, in [0, 1].\n        B (int): The number of bins for ECE calculation.\n\n    Returns:\n        tuple[float, float]: A tuple containing (accuracy, ECE).\n    \"\"\"\n    n = len(labels)\n    if n == 0:\n        return 0.0, 0.0\n\n    y_hat = (scores >= 0.5).astype(int)\n    confidence = np.where(y_hat == 1, scores, 1.0 - scores)\n    correct = (y_hat == labels).astype(int)\n    \n    # Calculate accuracy\n    accuracy = np.mean(correct)\n\n    # Calculate ECE\n    ece = 0.0\n    bin_width = 1.0 / B\n    for b in range(B):\n        bin_lower = b * bin_width\n        bin_upper = (b + 1) * bin_width\n\n        # The last bin is inclusive of the upper bound\n        if b == B - 1:\n            in_bin_mask = (confidence >= bin_lower)  (confidence = bin_upper)\n        else:\n            in_bin_mask = (confidence >= bin_lower)  (confidence  bin_upper)\n        \n        n_b = np.sum(in_bin_mask)\n        \n        if n_b > 0:\n            bin_correct = correct[in_bin_mask]\n            bin_confidence = confidence[in_bin_mask]\n            \n            acc_b = np.mean(bin_correct)\n            conf_b = np.mean(bin_confidence)\n            \n            ece += (n_b / n) * np.abs(acc_b - conf_b)\n\n    return accuracy, ece\n\ndef run_fairness_test(case, B, tau_acc, tau_cal):\n    \"\"\"\n    Runs the full disproportionate impact test for a single test case.\n    \n    Args:\n        case (dict): A dictionary containing the data for the test case.\n        B (int): Number of bins for ECE.\n        tau_acc (float): Accuracy harm threshold.\n        tau_cal (float): Calibration harm threshold.\n\n    Returns:\n        bool: True if disproportionate impact is detected, False otherwise.\n    \"\"\"\n    # Minority group calculations\n    min_labels_base = np.array(case[\"min_base\"][0])\n    min_scores_base = np.array(case[\"min_base\"][1])\n    min_labels_aug = np.array(case[\"min_aug\"][0])\n    min_scores_aug = np.array(case[\"min_aug\"][1])\n\n    acc_base_min, ece_base_min = calculate_metrics(min_labels_base, min_scores_base, B)\n    acc_aug_min, ece_aug_min = calculate_metrics(min_labels_aug, min_scores_aug, B)\n    \n    H_acc_min = max(0, acc_base_min - acc_aug_min)\n    H_cal_min = max(0, ece_aug_min - ece_base_min)\n\n    # Majority group calculations\n    maj_labels_base = np.array(case[\"maj_base\"][0])\n    maj_scores_base = np.array(case[\"maj_base\"][1])\n    maj_labels_aug = np.array(case[\"maj_aug\"][0])\n    maj_scores_aug = np.array(case[\"maj_aug\"][1])\n\n    acc_base_maj, ece_base_maj = calculate_metrics(maj_labels_base, maj_scores_base, B)\n    acc_aug_maj, ece_aug_maj = calculate_metrics(maj_labels_aug, maj_scores_aug, B)\n\n    H_acc_maj = max(0, acc_base_maj - acc_aug_maj)\n    H_cal_maj = max(0, ece_aug_maj - ece_base_maj)\n    \n    # Check for disproportionate impact\n    disproportionate_acc_impact = (H_acc_min - H_acc_maj) > tau_acc\n    disproportionate_cal_impact = (H_cal_min - H_cal_maj) > tau_cal\n    \n    return disproportionate_acc_impact or disproportionate_cal_impact\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the fairness test for each.\n    \"\"\"\n    # Fixed settings from the problem statement\n    B = 5\n    tau_acc = 0.05\n    tau_cal = 0.15\n\n    test_cases = [\n        # Test case 1\n        {\n            \"min_base\": ([1,0,1,0,1,0], [0.9,0.2,0.8,0.3,0.7,0.4]),\n            \"min_aug\":  ([1,0,1,0,1,0], [0.6,0.4,0.55,0.45,0.6,0.5]),\n            \"maj_base\": ([1,0,1,0,1,0,1,0,1,0,1,0], [0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05]),\n            \"maj_aug\":  ([1,0,1,0,1,0,1,0,1,0,1,0], [0.8,0.2,0.8,0.2,0.8,0.2,0.8,0.2,0.8,0.2,0.8,0.2])\n        },\n        # Test case 2\n        {\n            \"min_base\": ([1,0,1,0,0,1], [0.6,0.6,0.55,0.55,0.4,0.4]),\n            \"min_aug\":  ([1,0,1,0,0,1], [0.85,0.2,0.75,0.25,0.3,0.8]),\n            \"maj_base\": ([1,0,1,0,1,0,1,0,1,0,1,0], [0.6,0.4,0.6,0.4,0.6,0.4,0.6,0.4,0.6,0.4,0.6,0.4]),\n            \"maj_aug\":  ([1,0,1,0,1,0,1,0,1,0,1,0], [0.85,0.15,0.85,0.15,0.85,0.15,0.85,0.15,0.85,0.15,0.85,0.15])\n        },\n        # Test case 3\n        {\n            \"min_base\": ([1,1,0,0,1,0], [0.9,0.9,0.1,0.1,0.8,0.2]),\n            \"min_aug\":  ([1,1,0,0,1,0], [0.7,0.6,0.4,0.55,0.6,0.6]),\n            \"maj_base\": ([1,0,1,0,1,0,1,0,1,0,1,0], [0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05]),\n            \"maj_aug\":  ([1,0,1,0,1,0,1,0,1,0,1,0], [0.6,0.6,0.6,0.6,0.49,0.49,0.49,0.49,0.8,0.2,0.8,0.2])\n        }\n    ]\n\n    # Reformat test cases to use the same labels for augmented data\n    for case in test_cases:\n        case[\"min_aug\"] = (case[\"min_base\"][0], case[\"min_aug\"][1])\n        case[\"maj_aug\"] = (case[\"maj_base\"][0], case[\"maj_aug\"][1])\n\n    results = []\n    for case in test_cases:\n        result = run_fairness_test(case, B, tau_acc, tau_cal)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}