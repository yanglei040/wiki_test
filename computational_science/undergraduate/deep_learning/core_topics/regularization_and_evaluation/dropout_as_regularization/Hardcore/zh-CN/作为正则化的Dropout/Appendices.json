{
    "hands_on_practices": [
        {
            "introduction": "标准的 dropout 实现被称为“倒置 dropout”（inverted dropout），它在训练期间对保留下来的激活值进行缩放。本练习将通过一个思想实验来探究这一设计的核心理由：如果我们在训练时省略了这个缩放步骤，会发生什么？通过分析这个假设场景，你将从根本上理解为何缩放对于保持训练和测试阶段输出期望的一致性至关重要，并量化由此产生的模型偏差。",
            "id": "3118056",
            "problem": "一个单隐藏单元线性网络在一个无噪声回归任务上进行训练，使用 dropout 作为正则化方法，但其倒置 dropout (inverted dropout) 的实现是错误的（训练时没有进行缩放）。输入特征 $x$ 是一个随机变量，满足 $\\mathbb{E}[x]=0$ 和 $\\operatorname{Var}(x)=\\sigma_x^2$，真实目标是 $y=\\beta x$。网络输出为 $\\hat{y}=v\\,(m\\,u\\,x)$，其中 $u$ 和 $v$ 是权重，$m\\sim\\text{Bernoulli}(p)$ 是一个独立的 dropout 掩码，以保留概率 $p$ 应用于隐藏层激活。通常情况下，倒置 dropout 会在训练时对激活值进行缩放，以使其在训练和测试之间的期望保持不变，但在这里，训练期间没有使用任何缩放。假设在这个错误的训练过程下，对于每个固定的 $x$，优化器已经达到了 $\\mathbb{E}_m[\\hat{y}\\,|\\,x]=y$ 的条件。在测试时，dropout 被移除且不应用任何缩放（即，$m$ 确定性地为 1）。\n\n给定 $p=0.5$，$\\beta=2$ 和 $\\sigma_x^2=4$，哪个选项正确地给出了测试时的均方误差（Mean Squared Error (MSE)）并解释了观察到的训练-测试不匹配的来源？\n\nA. 测试时 MSE 为 $16$，这是由向上偏差引起的：相对于训练时的期望，测试输出被放大了 $1/p$ 倍，因此 $\\mathbb{E}[\\hat{y}_{\\text{test}}\\,|\\,x]=(\\beta/p)\\,x>\\beta x$。\n\nB. 测试时 MSE 为 $4$，这完全是由掩码引入的方差引起的；没有偏差，因为在测试时期望是匹配的。\n\nC. 测试时 MSE 为 $0$，因为将训练期望与目标匹配保证了在测试时的完美预测。\n\nD. 测试时 MSE 为 $4$，这是由向下偏差引起的：相对于训练时的期望，测试输出被衰减了。",
            "solution": "用户希望我验证问题陈述，如果有效，则为一个使用非标准 dropout 实现训练的单单元线性网络推导其测试时均方误差（MSE）。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n-   网络模型：一个单隐藏单元线性网络。\n-   任务：无噪声回归。\n-   输入：一个随机变量 $x$，满足 $\\mathbb{E}[x]=0$ 和 $\\operatorname{Var}(x)=\\sigma_x^2$。\n-   真实目标：$y=\\beta x$。\n-   网络输出：$\\hat{y}=v\\,(m\\,u\\,x)$，其中 $u$ 和 $v$ 是标量权重。\n-   Dropout 掩码：$m\\sim\\text{Bernoulli}(p)$，一个独立的随机变量，保留概率为 $p$。\n-   训练过程：错误的倒置 dropout；训练期间不使用缩放。\n-   训练结果：对于每个固定的 $x$，优化器已经达到了 $\\mathbb{E}_m[\\hat{y}\\,|\\,x]=y$。\n-   测试过程：Dropout 被移除（$m$ 确定性地为 1），且不应用缩放。\n-   常量：$p=0.5$，$\\beta=2$，$\\sigma_x^2=4$。\n-   问题：确定测试时 MSE 和训练-测试不匹配的来源。\n\n**步骤 2：使用提取的已知条件进行验证**\n-   **科学依据**：该问题坚实地基于神经网络和正则化技术（特别是 dropout）的原理。它描述了一个合理但错误的实现场景，以测试概念性理解。所有概念（MSE、期望、方差、伯努利分布、偏差）都是统计学和机器学习中的标准概念。\n-   **适定性**：该问题是适定的。训练目标陈述清晰，可以确定所学参数的有效值。测试时条件也是明确的。可以从给定信息中推导出一个唯一、稳定且有意义的解（测试 MSE）。\n-   **客观性**：该问题以精确、客观的数学语言陈述。\n-   **完整性和一致性**：该问题是自洽的。它提供了求解所需量所必需的所有参数（$p$, $\\beta$, $\\sigma_x^2$）和条件。没有内部矛盾。\n\n**步骤 3：结论和行动**\n问题陈述是**有效**的。我将继续进行推导和求解。\n\n### 推导\n\n**1. 分析训练阶段**\n\n训练期间的网络输出由 $\\hat{y}_{\\text{train}} = v(m u x)$ 给出。权重 $u$ 和 $v$ 由优化器调整，以满足对于任何给定的 $x$ 的条件 $\\mathbb{E}_m[\\hat{y}_{\\text{train}} \\,|\\, x] = y$。让我们计算这个期望。期望是针对随机 dropout 掩码 $m$ 计算的。\n\n$$\n\\mathbb{E}_m[\\hat{y}_{\\text{train}} \\,|\\, x] = \\mathbb{E}_m[v m u x \\,|\\, x]\n$$\n\n由于 $v$、$u$ 和 $x$ 相对于 $m$ 的期望被视为常数，我们有：\n\n$$\n\\mathbb{E}_m[\\hat{y}_{\\text{train}} \\,|\\, x] = v u x \\, \\mathbb{E}[m]\n$$\n\ndropout 掩码 $m$ 服从伯努利分布，$m \\sim \\text{Bernoulli}(p)$。伯努利随机变量的期望是其成功概率，所以 $\\mathbb{E}[m] = p$。\n\n将其代回，我们得到：\n\n$$\n\\mathbb{E}_m[\\hat{y}_{\\text{train}} \\,|\\, x] = p v u x\n$$\n\n训练条件规定，这个期望输出必须等于真实目标 $y = \\beta x$。\n\n$$\np v u x = \\beta x\n$$\n\n为了使此等式对所有 $x$ 值都成立，两边 $x$ 的系数必须相等。设 $W = vu$ 为线性网络的有效权重。\n\n$$\np W = \\beta \\implies W = \\frac{\\beta}{p}\n$$\n\n这是网络在这种特定的（也是不正确的）训练过程中学到的有效权重。\n\n**2. 分析测试阶段**\n\n在测试时，dropout 被禁用，这意味着掩码确定性地设置为 $m=1$。不应用任何缩放。测试时输出 $\\hat{y}_{\\text{test}}$ 为：\n\n$$\n\\hat{y}_{\\text{test}} = v (1 \\cdot u x) = v u x = W x\n$$\n\n代入学习到的有效权重 $W = \\beta/p$：\n\n$$\n\\hat{y}_{\\text{test}} = \\left(\\frac{\\beta}{p}\\right) x\n$$\n\n**3. 计算测试时均方误差 (MSE)**\n\nMSE 是测试时预测与真实目标之间差值的平方的期望，其中期望是针对输入数据 $x$ 的分布计算的。\n\n$$\n\\text{MSE} = \\mathbb{E}_x[(\\hat{y}_{\\text{test}} - y)^2]\n$$\n\n代入 $\\hat{y}_{\\text{test}}$ 和 $y$ 的表达式：\n\n$$\n\\text{MSE} = \\mathbb{E}_x\\left[ \\left( \\left(\\frac{\\beta}{p}\\right)x - \\beta x \\right)^2 \\right]\n$$\n\n提出因子 $\\beta$ 和 $x$：\n\n$$\n\\text{MSE} = \\mathbb{E}_x\\left[ \\left( \\beta \\left(\\frac{1}{p} - 1\\right) x \\right)^2 \\right] = \\mathbb{E}_x\\left[ \\beta^2 \\left(\\frac{1}{p} - 1\\right)^2 x^2 \\right]\n$$\n\n由于 $\\beta$ 和 $p$ 是常数，我们可以将它们从期望中提出：\n\n$$\n\\text{MSE} = \\beta^2 \\left(\\frac{1}{p} - 1\\right)^2 \\mathbb{E}_x[x^2]\n$$\n\n我们已知 $\\operatorname{Var}(x) = \\sigma_x^2$ 且 $\\mathbb{E}[x]=0$。方差定义为 $\\operatorname{Var}(x) = \\mathbb{E}[x^2] - (\\mathbb{E}[x])^2$。\n因此，$\\sigma_x^2 = \\mathbb{E}[x^2] - 0^2$，这意味着 $\\mathbb{E}[x^2] = \\sigma_x^2$。\n\n将此代入 MSE 公式：\n\n$$\n\\text{MSE} = \\beta^2 \\left(\\frac{1}{p} - 1\\right)^2 \\sigma_x^2\n$$\n\n**4. 代入数值**\n\n我们已知 $p=0.5$，$\\beta=2$ 和 $\\sigma_x^2=4$。\n\n$$\n\\text{MSE} = (2)^2 \\left(\\frac{1}{0.5} - 1\\right)^2 (4)\n$$\n\n$$\n\\text{MSE} = 4 \\left(2 - 1\\right)^2 (4)\n$$\n\n$$\n\\text{MSE} = 4 (1)^2 (4)\n$$\n\n$$\n\\text{MSE} = 16\n$$\n\n**5. 分析误差来源（训练-测试不匹配）**\n\n在训练期间，网络的期望输出与目标匹配：$\\mathbb{E}_m[\\hat{y}_{\\text{train}}\\,|\\,x] = y = \\beta x$。\n在测试时，确定性输出为 $\\hat{y}_{\\text{test}} = (\\beta/p)x$。\n由于 $p=0.5$，我们有 $1/p = 2$。所以，$\\hat{y}_{\\text{test}} = (\\beta/0.5)x = 2\\beta x$。真实目标仍然是 $y = \\beta x$。\n\n测试时输出始终比真实目标大 $1/p = 2$ 倍。这种系统性的高估是一种条件偏差，其中 $\\mathbb{E}[\\hat{y}_{\\text{test}} - y \\,|\\, x] = (\\beta/p)x - \\beta x \\neq 0$。发生这种情况的原因是，标准的倒置 dropout 实践是在*训练*期间将激活值乘以 $1/p$ 进行缩放，以确保期望激活值的大小与没有 dropout 时保持相同。由于没有这样做，网络权重 $W=vu$ 被“夸大”了 $1/p$ 倍以作补偿。在测试时，当 dropout 掩码被移除时，这些被夸大的权重导致了放大的输出。这是一种“向上偏差”或输出信号的放大。\n\n### 逐项分析选项\n\n**A. 测试时 MSE 为 $16$，这是由向上偏差引起的：相对于训练时的期望，测试输出被放大了 $1/p$ 倍，因此 $\\mathbb{E}[\\hat{y}_{\\text{test}}\\,|\\,x]=(\\beta/p)\\,x>\\beta x$。**\n-   计算出的测试时 MSE 确实是 $16$。\n-   原因被正确地识别为向上偏差（放大）。\n-   放大因子被正确地识别为 $1/p$。测试输出 $\\hat{y}_{\\text{test}} = (\\beta/p)x$ 与训练目标 $y = \\beta x$（即训练时期望）进行比较。\n-   所示关系 $\\hat{y}_{\\text{test}} = (\\beta/p)x > \\beta x = y$（对于 $x>0$）正确地描述了放大。由于对于给定的 $x$，测试输出是确定性的，因此使用 $\\mathbb{E}[\\hat{y}_{\\text{test}}\\,|\\,x]$ 等同于 $\\hat{y}_{\\text{test}}$。\n-   **结论：正确。**\n\n**B. 测试时 MSE 为 $4$，这完全是由掩码引入的方差引起的；没有偏差，因为在测试时期望是匹配的。**\n-   MSE 是 $16$，而不是 $4$。\n-   误差来源是系统性偏差，而不是方差。测试时不使用 dropout 掩码，因此它不能成为测试时方差的来源。\n-   条件期望在测试时不匹配；$\\hat{y}_{\\text{test}} = (\\beta/p)x \\neq \\beta x = y$。\n-   **结论：不正确。**\n\n**C. 测试时 MSE 为 $0$，因为将训练期望与目标匹配保证了在测试时的完美预测。**\n-   MSE 是 $16$，而不是 $0$。\n-   其推理存在缺陷。如果模型的行为在训练和测试之间发生变化（例如，由于移除了随机掩码而没有进行适当的缩放补偿），那么在训练期间匹配期望输出并不能保证在测试时能做出正确的预测。\n-   **结论：不正确。**\n\n**D. 测试时 MSE 为 $4$，这是由向下偏差引起的：相对于训练时的期望，测试输出被衰减了。**\n-   MSE 是 $16$，而不是 $4$。\n-   偏差是向上的（放大），而不是向下的（衰减），因为 $p=0.5  1$，这使得 $1/p = 2  1$。\n-   **结论：不正确。**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "在构建复杂的深度网络时，我们经常将 dropout 与批量归一化（Batch Normalization, BN）等层结合使用。然而，它们的相对顺序并非无关紧要。本练习旨在探讨 dropout 与 BN 的交互作用，分析将 dropout 置于 BN 之前或之后，会对网络在训练和推理阶段的统计特性产生何种影响。解决这个问题将帮助你理解为何特定层序组合可能导致训练与测试之间的分布偏移，并掌握设计更稳健模型架构的实用原则。",
            "id": "3118023",
            "problem": "给定两个深度神经网络块，它们在其他方面完全相同，唯一的区别在于 Dropout 层相对于批量归一化 (BN) 层的位置。批量归一化 (BN) 指的是一种标准变换，在训练期间，它使用批次均值和批次方差对每个通道进行归一化；在推理期间，它使用训练期间累积的运行（指数平均）均值和方差进行归一化，然后进行一次学习到的仿射变换。Dropout 在训练期间使用常见的“反向”实现：每个单元以概率 $p$ 独立保留并按 $1/p$ 缩放，而在推理期间 Dropout 被禁用。考虑一个通用的激活前的值 $x$ 进入每个块中的 BN 层，并假设 Dropout 掩码与 $x$ 无关。你将在训练模式和推理模式下，对每个块和每个相关张量位置，在留出数据上测量经验性的逐层统计量 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$，以诊断分布偏移。这两个块是：\n- 块 A（“Dropout 在 BN 之前”）：前一层的激活值 $\\to$ 保留概率为 $p$ 且带反向缩放的 Dropout $\\to$ BN $\\to$ 剩余层。\n- 块 B（“Dropout 在 BN 之后”）：前一层的激活值 $\\to$ BN $\\to$ 保留概率为 $p$ 且带反向缩放的 Dropout $\\to$ 剩余层。\n假设这些块的上游输入具有有限的一阶矩和二阶矩，并且训练使用足够大的批次，以使 BN 的运行统计量收敛到相应的训练时批次统计量。在这些假设下，当比较块 A 与块 B 在训练与推理之间，在 BN 输入和 BN 输出处测量的 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$ 时，哪个陈述最能预测你将观察到的模式？\n\nA. 在块 A 中，BN 的运行方差被训练期间遇到的掩码噪声所放大，因此在推理时，BN 输入的方差小于运行方差，BN 输出的方差收缩至约 $p$（小于 $1$），而均值保持在约 $0$。在块 B 中，BN 统计量在训练和推理之间，在 BN 输入和 BN 输出处都非常匹配，因此它们的均值和方差一致；任何训练-推理的方差差异仅出现在 Dropout 层之后，其中训练方差大约大 $1/p$ 倍。\n\nB. 在块 A 中，BN 消除了任何训练-推理的不匹配，因此 BN 输入和 BN 输出在训练和推理之间的均值和方差完全匹配。在块 B 中，将 Dropout 放在 BN 之后会在推理时于 BN 输出处引起一个与 $(1-p)$ 成比例的均值偏移。\n\nC. 在块 A 中，BN 之前的掩码操作使 BN 的运行均值偏向 $0$，因此在推理时，BN 输出存在非零的均值不匹配，而其方差匹配。在块 B 中，BN 之后的掩码操作使 BN 的运行方差向上偏置，因此在推理时，BN 输出的方差膨胀了 $1/p$ 倍。\n\nD. 使用反向 Dropout 时，位置无关紧要：对于任何 $p$，每一层的期望和方差在训练和推理之间都匹配，因此在两个块的 BN 输入和输出处，所有测量的 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$ 都重合。",
            "solution": "用户希望我分析涉及批量归一化 (BN) 和 Dropout 的两个神经网络结构块的统计特性，特别关注训练模式和推理模式之间的分布偏移。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n-   **块 A（“Dropout 在 BN 之前”）：** 前一层的激活值 $\\to$ Dropout $\\to$ BN $\\to$ 剩余层。\n-   **块 B（“Dropout 在 BN 之后”）：** 前一层的激活值 $\\to$ BN $\\to$ Dropout $\\to$ 剩余层。\n-   **批量归一化 (BN)：**\n    -   训练：使用批次均值/方差进行归一化。然后进行一次学习到的仿射变换 $z = \\gamma \\hat{x} + \\beta$。\n    -   推理：使用训练期间累积的运行均值/方差进行归一化。然后进行相同的仿射变换。\n-   **Dropout：**\n    -   “反向”实现。\n    -   训练：每个单元以概率 $p$ 保留，并按 $1/p$ 缩放。否则置零。\n    -   推理：Dropout 是一个恒等函数（被禁用）。\n-   **输入：** 一个通用的激活前的值，我们称之为 $y$，进入该块。Dropout 掩码独立于 $y$。\n-   **任务：** 比较两个块在训练模式与推理模式下，在 BN 输入和 BN 输出处的经验性逐层统计量 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$。\n-   **假设：**\n    1.  上游输入 $y$ 具有有限的一阶矩和二阶矩。设 $\\operatorname{E}[y] = \\mu_y$ 和 $\\operatorname{Var}[y] = \\sigma_y^2$。\n    2.  训练使用足够大的批次，以至于 BN 的运行统计量收敛到相应的训练时批次统计量。这意味着 BN 存储的运行均值 $\\mu_{\\text{run}}$ 和运行方差 $\\sigma^2_{\\text{run}}$ 等于训练期间进入 BN 层的张量的总体均值和方差。\n\n**步骤2：使用提取的已知条件进行验证**\n-   **科学基础：** 该问题在深度学习和统计学原理上有坚实的基础。对批量归一化和反向 Dropout 的描述是标准的。这些层之间的相互作用在设计神经网络架构中是一个已知且重要的实践和理论考量。\n-   **问题明确：** 问题定义清晰。两个块、操作以及分析目标都得到了明确的说明。关于大批次和收敛统计量的假设使得问题在分析上是可解的，并能导出一个唯一的结论。\n-   **客观性：** 问题要求对统计矩进行严格的、定量的比较，这是一项客观的任务。\n\n**步骤3：结论和行动**\n问题陈述是有效的。这是一个在深度学习理论领域中定义明确的问题。我将进行正式的推导。\n\n### 推导\n\n设 $y$ 是块的输入，其 $\\operatorname{E}[y] = \\mu_y$ 和 $\\operatorname{Var}[y] = \\sigma_y^2$。\n设 $m$ 是 Dropout 的伯努利随机变量，$P(m=1) = p$，且独立于 $y$。\nBN 层有可学习的参数 $\\gamma$ 和 $\\beta$。为简单起见且不失一般性，我们将经常分析仿射变换之前的归一化输出（其均值为 $0$，方差为 $1$），然后再考虑 $\\gamma$ 和 $\\beta$ 的影响。因此，训练期间 BN 之后的目标均值和方差分别为 $\\beta$ 和 $\\gamma^2$。\n\n#### 块 A 的分析 (Dropout $\\to$ BN)\n\n设块为 $y \\xrightarrow{\\text{Dropout}} x \\xrightarrow{\\text{BN}} z$。张量 $x$ 是 BN 层的输入。\n\n**1. 训练模式：**\n-   BN 层的输入是 $x_{\\text{train}} = y \\cdot \\frac{m}{p}$。\n-   我们计算其统计量，根据假设，这些统计量将成为 BN 的运行统计量。\n-   **均值：** $\\operatorname{E}[x_{\\text{train}}] = \\operatorname{E}[y \\cdot \\frac{m}{p}] = \\operatorname{E}[y]\\operatorname{E}[\\frac{m}{p}] = \\mu_y \\cdot \\frac{\\operatorname{E}[m]}{p} = \\mu_y \\cdot \\frac{p}{p} = \\mu_y$。\n    运行均值为 $\\mu_{\\text{run}} = \\mu_y$。\n-   **方差：** 为了计算 $\\operatorname{Var}[x_{\\text{train}}]$，我们首先找到 $\\operatorname{E}[x_{\\text{train}}^2]$。\n    $\\operatorname{E}[x_{\\text{train}}^2] = \\operatorname{E}[(y \\frac{m}{p})^2] = \\operatorname{E}[y^2 \\frac{m^2}{p^2}] = \\operatorname{E}[y^2]\\operatorname{E}[\\frac{m^2}{p^2}]$。\n    因为 $m \\in \\{0, 1\\}$，所以 $m^2=m$，因此 $\\operatorname{E}[m^2]=\\operatorname{E}[m]=p$。\n    $\\operatorname{E}[x_{\\text{train}}^2] = \\operatorname{E}[y^2] \\frac{p}{p^2} = \\frac{1}{p}\\operatorname{E}[y^2] = \\frac{1}{p}(\\sigma_y^2 + \\mu_y^2)$。\n    $\\operatorname{Var}[x_{\\text{train}}] = \\operatorname{E}[x_{\\text{train}}^2] - (\\operatorname{E}[x_{\\text{train}}])^2 = \\frac{1}{p}(\\sigma_y^2 + \\mu_y^2) - \\mu_y^2 = \\frac{\\sigma_y^2}{p} + \\mu_y^2(\\frac{1}{p} - 1) = \\frac{\\sigma_y^2}{p} + \\mu_y^2(\\frac{1-p}{p})$。\n    运行方差为 $\\sigma_{\\text{run}}^2 = \\frac{\\sigma_y^2}{p} + \\mu_y^2(\\frac{1-p}{p})$。由于 $1/p  1$，这个方差被 Dropout 噪声“夸大”了。\n-   **BN 输出：** 训练期间的 BN 输出 $z_{\\text{train}}$ 将有 $\\operatorname{E}[z_{\\text{train}}] \\approx \\beta$ 和 $\\operatorname{Var}[z_{\\text{train}}] \\approx \\gamma^2$。\n\n**2. 推理模式：**\n-   Dropout 被禁用。BN 层的输入是 $x_{\\text{inf}} = y$。\n-   **BN 输入处的统计量：** $\\operatorname{E}[x_{\\text{inf}}] = \\mu_y$ 和 $\\operatorname{Var}[x_{\\text{inf}}] = \\sigma_y^2$。\n-   **BN 输入处训练与推理的对比：**\n    -   均值匹配：$\\operatorname{E}[x_{\\text{inf}}] = \\operatorname{E}[x_{\\text{train}}] = \\mu_y$。\n    -   方差不匹配：$\\operatorname{Var}[x_{\\text{inf}}] = \\sigma_y^2  \\sigma_{\\text{run}}^2 = \\operatorname{Var}[x_{\\text{train}}]$。BN 存储的运行方差高估了真实的推理时输入方差。\n-   **BN 输出：** BN 层使用存储的运行统计量对 $x_{\\text{inf}}$ 进行归一化：\n    $z_{\\text{inf}} = \\gamma \\frac{x_{\\text{inf}} - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}} + \\beta = \\gamma \\frac{y - \\mu_y}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}} + \\beta$。\n-   **BN 输出的统计量：**\n    -   均值：$\\operatorname{E}[z_{\\text{inf}}] = \\frac{\\gamma}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}}\\operatorname{E}[y - \\mu_y] + \\beta = \\beta$。输出的均值与训练时一致。\n    -   方差：$\\operatorname{Var}[z_{\\text{inf}}] = \\operatorname{Var}[\\gamma \\frac{y - \\mu_y}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}}] = \\frac{\\gamma^2}{\\sigma_{\\text{run}}^2 + \\epsilon}\\operatorname{Var}[y] = \\frac{\\gamma^2 \\sigma_y^2}{\\sigma_{\\text{run}}^2 + \\epsilon}$。\n    -   由于 $\\sigma_{\\text{run}}^2  \\sigma_y^2$，我们有 $\\operatorname{Var}[z_{\\text{inf}}]  \\gamma^2 = \\operatorname{Var}[z_{\\text{train}}]$。推理时的输出方差小于训练时。\n    -   为简单起见，如果我们假设输入是中心化的（$\\mu_y=0$），则 $\\sigma_{\\text{run}}^2 = \\sigma_y^2/p$。输出方差变为 $\\operatorname{Var}[z_{\\text{inf}}] \\approx \\frac{\\gamma^2 \\sigma_y^2}{\\sigma_y^2/p} = \\gamma^2 p$。方差收缩了因子 $p$。\n\n**块 A 总结：** 出现了训练-推理分布偏移。BN 层使用了来自训练的被夸大的方差估计，导致它在推理数据上进行不正确的归一化，从而使其输出的方差收缩。\n\n#### 块 B 的分析 (BN $\\to$ Dropout)\n\n设块为 $y \\xrightarrow{\\text{BN}} x \\xrightarrow{\\text{Dropout}} z$。张量 $x$ 现在是 BN 层的输出。\n\n**1. 训练模式：**\n-   BN 层的输入是 $y$。\n-   它计算并存储来自 $y$ 的统计量。\n-   运行均值：$\\mu_{\\text{run}} = \\operatorname{E}[y] = \\mu_y$。\n-   运行方差：$\\sigma_{\\text{run}}^2 = \\operatorname{Var}[y] = \\sigma_y^2$。\n-   **BN 输出 / Dropout 输入：** $x_{\\text{train}}$ 将有 $\\operatorname{E}[x_{\\text{train}}] \\approx \\beta$ 和 $\\operatorname{Var}[x_{\\text{train}}] \\approx \\gamma^2$。\n-   **Dropout 输出：** 最终输出为 $z_{\\text{train}} = x_{\\text{train}} \\cdot \\frac{m}{p}$。其方差将被 Dropout 噪声夸大，$\\operatorname{Var}[z_{\\text{train}}] \\approx \\frac{\\gamma^2}{p} + \\beta^2(\\frac{1-p}{p})$。\n\n**2. 推理模式：**\n-   Dropout 被禁用。\n-   **BN 输入：** 输入仍然是 $y$。其统计量与训练时相比没有变化。因此，在 BN 输入处，没有训练-推理分布偏移。\n-   **BN 输出 / Dropout 输入：** BN 层使用 $\\mu_{\\text{run}} = \\mu_y$ 和 $\\sigma_{\\text{run}}^2 = \\sigma_y^2$ 对 $y$ 进行归一化。\n    $x_{\\text{inf}} = \\gamma \\frac{y - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}} + \\beta = \\gamma \\frac{y - \\mu_y}{\\sqrt{\\sigma_y^2 + \\epsilon}} + \\beta$。\n-   **BN 输出的统计量：**\n    -   均值：$\\operatorname{E}[x_{\\text{inf}}] \\approx \\beta$。\n    -   方差：$\\operatorname{Var}[x_{\\text{inf}}] \\approx \\gamma^2$。\n-   **BN 输出处训练与推理的对比：** $x$（BN 输出）的统计量在训练和推理之间匹配：$\\operatorname{E}[x_{\\text{train}}] \\approx \\operatorname{E}[x_{\\text{inf}}]$ 和 $\\operatorname{Var}[x_{\\text{train}}] \\approx \\operatorname{Var}[x_{\\text{inf}}]$。\n-   **Dropout 输出：** Dropout 是一个恒等映射，所以 $z_{\\text{inf}} = x_{\\text{inf}}$。最终输出有 $\\operatorname{E}[z_{\\text{inf}}] \\approx \\beta$ 和 $\\operatorname{Var}[z_{\\text{inf}}] \\approx \\gamma^2$。\n-   训练-推理的差异仅出现在 Dropout 层*之后*，其中 $\\operatorname{Var}[z_{\\text{train}}] > \\operatorname{Var}[z_{\\text{inf}}]$。\n\n**块 B 总结：** 在 BN 层的输入或输出处没有分布偏移。BN 层在训练和测试中看到相同的数据分布，其存储的统计量是合适的。训练-推理的差异被隔离在 Dropout 层本身的输出处。\n\n### 逐项分析\n\n**A. 在块 A 中，BN 的运行方差被训练期间遇到的掩码噪声所放大，因此在推理时，BN 输入的方差小于运行方差，BN 输出的方差收缩至约 $p$（小于 $1$），而均值保持在约 $0$。在块 B 中，BN 统计量在训练和推理之间，在 BN 输入和 BN 输出处都非常匹配，因此它们的均值和方差一致；任何训练-推理的方差差异仅出现在 Dropout 层之后，其中训练方差大约大 $1/p$ 倍。**\n-   对块 A 的描述与我们的分析完全一致。运行方差被噪声夸大。这导致 BN 输出方差在推理时收缩约 $p$ 的因子。均值保持稳定。\n-   对块 B 的描述也完全一致。BN 层的输入和输出统计量在训练和推理之间是稳定的。方差不匹配被隔离在 Dropout 层的输出处，其中训练方差被放大了约 $1/p$ 的因子。\n-   **结论：正确。**\n\n**B. 在块 A 中，BN 消除了任何训练-推理的不匹配，因此 BN 输入和 BN 输出在训练和推理之间的均值和方差完全匹配。在块 B 中，将 Dropout 放在 BN 之后会在推理时于 BN 输出处引起一个与 $(1-p)$ 成比例的均值偏移。**\n-   关于块 A 的陈述不正确。BN 并没有消除不匹配；由于使用了不正确的运行统计量，它反而是其输出处不匹配的来源。在 BN 输出处存在方差不匹配。\n-   关于块 B 的陈述不正确。反向 Dropout 保持了均值。没有均值偏移。\n-   **结论：不正确。**\n\n**C. 在块 A 中，BN 之前的掩码操作使 BN 的运行均值偏向 $0$，因此在推理时，BN 输出存在非零的均值不匹配，而其方差匹配。在块 B 中，BN 之后的掩码操作使 BN 的运行方差向上偏置，因此在推理时，BN 输出的方差膨胀了 $1/p$ 倍。**\n-   关于块 A 的陈述不正确。反向 Dropout 确保运行均值是一个无偏估计（$\\operatorname{E}[x_{\\text{train}}] = \\mu_y$）。它还错误地声称方差匹配，而实际上方差是收缩的。\n-   关于块 B 的陈述不正确。掩码操作发生在 BN *之后*，所以它不能使 BN 的运行方差产生偏置。推理时的 BN 输出方差与训练时匹配，并不会膨胀。\n-   **结论：不正确。**\n\n**D. 使用反向 Dropout 时，位置无关紧要：对于任何 $p$，每一层的期望和方差在训练和推理之间都匹配，因此在两个块的 BN 输入和输出处，所有测量的 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$ 都重合。**\n-   这个陈述显然是错误的。我们的分析表明，位置非常重要。块 A 在 BN 输出处存在方差不匹配，而块 B 则没有。反向 Dropout 保持了一阶矩（均值）但没有保持二阶矩（方差），这是观察到的现象的根本原因。\n-   **结论：不正确。**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "在推理阶段，标准做法是直接移除 dropout 层，这相当于用一个确定性的网络来近似随机的网络集合。这个练习将深入探讨这种确定性近似的理论基础和局限性，特别是当网络中存在非线性激活函数时。你将运用泰勒展开等分析工具，比较确定性推理与更精确的蒙特卡洛（Monte Carlo）方法，从而揭示前者可能引入的偏差。这项实践不仅能加深你对 dropout 推理机制的理解，还能让你领略到其与贝叶斯模型平均之间的深刻联系。",
            "id": "3118065",
            "problem": "考虑一个神经网络中的单个标量隐藏单元，其预激活值为 $a \\in \\mathbb{R}$，非线性函数为 $\\phi:\\mathbb{R}\\to\\mathbb{R}$。在推理时，假设 dropout 通过采样一个伯努利掩码 $z \\sim \\text{Bernoulli}(p)$ 来显式建模，其保留概率为 $p \\in (0,1]$，并使用反向 dropout 缩放，使得随机输出为 $y(z) = \\phi\\!\\left(\\frac{z}{p}\\,a\\right)$。两种常见的推理规则是：\n- 确定性缩放（无采样）：使用 $y_{\\text{det}} = \\phi(a)$。\n- 蒙特卡洛（MC）dropout：通过对 $N \\in \\mathbb{N}$ 个独立采样的掩码进行平均，来近似随机预测均值 $y_{\\star} = \\mathbb{E}_{z}\\big[\\phi\\!\\left(\\frac{z}{p}\\,a\\right)\\big]$，即 $y_{\\text{MC}} = \\frac{1}{N}\\sum_{i=1}^{N} \\phi\\!\\left(\\frac{z_i}{p}\\,a\\right)$。\n\n除非另有说明，假设 $\\phi$ 在 $a$ 的一个邻域内是二次连续可微的。仅使用期望和方差的定义、琴生不等式以及 $\\phi$ 在 $a$ 附近的二阶泰勒近似来推断偏差和曲率。选择所有正确的陈述。\n\nA. 对于一个二次可微的函数 $\\phi$ 且 $\\phi''(a) \\neq 0$，确定性规则 $y_{\\text{det}}$ 相对于 $y_{\\star}$ 是有偏的，其主阶偏差为\n$$\n\\mathbb{E}\\big[\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big] - \\phi(a) \\approx \\tfrac{1}{2}\\,\\mathrm{Var}\\!\\left(\\tfrac{z}{p}\\,a\\right)\\,\\phi''(a) = \\tfrac{1}{2}\\,\\frac{1-p}{p}\\,a^{2}\\,\\phi''(a).\n$$\n\nB. MC 估计量 $y_{\\text{MC}}$ 是有偏的，除非 $N \\to \\infty$；对于凸函数 $\\phi$，其期望值超过 $y_{\\star}$。\n\nC. 如果 $\\phi$ 是线性的，那么对于任何 $p \\in (0,1]$ 和任何 $a \\in \\mathbb{R}$，都有 $y_{\\text{det}} = y_{\\star}$ 精确成立。\n\nD. 保持 $p$ 和 $a$ 不变，确定性偏差的大小 $\\big|y_{\\star} - y_{\\text{det}}\\big|$ 随着曲率 $\\big|\\phi''(a)\\big|$ 的增加而减小，因为平均过程平滑了非线性。\n\nE. 对于修正线性单元 $\\phi(u)=\\max(0,u)$，对于所有的 $a \\in \\mathbb{R}$ 和 $p \\in (0,1]$，都有 $y_{\\text{det}} = y_{\\star}$。\n\nF. 对于任何 $N \\in \\mathbb{N}$，MC 估计量 $y_{\\text{MC}}$ 是 $y_{\\star}$ 的一个无偏估计量，其方差按 $\\mathrm{Var}(y_{\\text{MC}}) = \\mathrm{Var}\\!\\big(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big)/N$ 的比例缩放，因此增加 $N$ 会减少由采样引起的误差，而不会引入偏差；较高的曲率通常会通过非线性对输入变异性的放大作用来增加 $\\mathrm{Var}\\!\\big(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big)$。",
            "solution": "用户希望对一个关于神经网络中 dropout 推理的问题陈述进行批判性分析，并在问题有效的情况下给出解决方案。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n- 一个神经网络中的单个标量隐藏单元，其预激活值为 $a \\in \\mathbb{R}$，非线性函数为 $\\phi:\\mathbb{R}\\to\\mathbb{R}$。\n- 在推理时，通过采样一个伯努利掩码 $z \\sim \\text{Bernoulli}(p)$ 来显式建模，其保留概率为 $p \\in (0,1]$。\n- 使用反向 dropout 缩放，得到随机输出 $y(z) = \\phi\\!\\left(\\frac{z}{p}\\,a\\right)$。\n- 定义了两种推理规则：\n    1. 确定性缩放：$y_{\\text{det}} = \\phi(a)$。\n    2. 蒙特卡洛（MC）dropout：$y_{\\text{MC}} = \\frac{1}{N}\\sum_{i=1}^{N} \\phi\\!\\left(\\frac{z_i}{p}\\,a\\right)$，其中 $N \\in \\mathbb{N}$ 是独立采样的掩码 $z_i$ 的数量。\n- 目标量是随机预测均值：$y_{\\star} = \\mathbb{E}_{z}\\big[\\phi\\!\\left(\\frac{z}{p}\\,a\\right)\\big]$。\n- 假设：除非另有说明，$\\phi$ 在 $a$ 的一个邻域内是二次连续可微的。\n- 允许的工具：期望和方差的定义、琴生不等式以及 $\\phi$ 在 $a$ 附近的二阶泰勒近似。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题描述了对 dropout 的标准理论分析，dropout 是深度学习中广泛使用的正则化技术。反向 dropout、MC dropout、确定性近似等概念，以及通过泰勒级数对它们进行的分析，对于理解 dropout 的行为至关重要。该问题在科学上是合理的。\n- **定义明确：** 所有术语（$y(z)$, $y_{\\text{det}}$, $y_{\\text{MC}}$, $y_{\\star}$, $p$, $a$, $z$）都得到了清晰明确的定义。问题要求基于这些定义评估几个陈述的正确性，这是一个定义明确的任务。\n- **客观性：** 该问题以精确的数学语言陈述，没有主观性或歧义。\n\n**缺陷清单：**\n1.  **科学或事实上的不合理：** 无。该设置是用于分析的标准且有效的简化。\n2.  **非形式化或不相关：** 无。该问题是一个与其所述主题直接相关的形式化数学练习。\n3.  **不完整或矛盾的设置：** 无。所有必要信息都已提供。\n4.  **不切实际或不可行：** 无。这是一个计算过程的理论模型。\n5.  **不适定或结构不良：** 无。\n6.  **故作高深、琐碎或同义反复：** 无。该问题需要对概率论和微积分进行非琐碎的应用。\n7.  **超出科学可验证性范围：** 无。所有陈述在数学上都是可验证的。\n\n**步骤 3：结论与行动**\n问题陈述是**有效的**。开始求解。\n\n### 求解推导\n\n首先，我们确定所涉及的随机变量的属性。非线性函数的输入是随机变量 $X = \\frac{z}{p}a$，其中 $z \\sim \\text{Bernoulli}(p)$。随机变量 $z$ 以概率 $p$ 取值 $1$，以概率 $1-p$ 取值 $0$。\n因此，随机变量 $X$ 以概率 $p$ 取值 $\\frac{1}{p}a$，以概率 $1-p$ 取值 $\\frac{0}{p}a = 0$。\n\n$X$ 的期望是：\n$$ \\mathbb{E}[X] = \\mathbb{E}\\left[\\frac{z}{p}a\\right] = \\frac{a}{p}\\mathbb{E}[z] = \\frac{a}{p} \\cdot p = a $$\n这证实了反向 dropout 缩放保留了训练时预激活值的均值。\n\n$X$ 的方差是：\n$$ \\mathrm{Var}(X) = \\mathrm{Var}\\left(\\frac{z}{p}a\\right) = \\left(\\frac{a}{p}\\right)^2 \\mathrm{Var}(z) = \\frac{a^2}{p^2} \\left(p(1-p)\\right) = a^2 \\frac{1-p}{p} $$\n\n目标量是 $y_{\\star} = \\mathbb{E}[\\phi(X)]$。使用“无意识统计学家法则”（或离散随机变量的期望定义）：\n$$ y_{\\star} = \\phi\\left(\\frac{a}{p}\\right) \\cdot p + \\phi(0) \\cdot (1-p) $$\n\n确定性规则给出 $y_{\\text{det}} = \\phi(a) = \\phi(\\mathbb{E}[X])$。\n\n确定性规则的偏差是 $y_{\\star} - y_{\\text{det}} = \\mathbb{E}[\\phi(X)] - \\phi(\\mathbb{E}[X])$。对于非线性函数 $\\phi$，琴生不等式表明这个偏差是非零的。\n\n现在我们评估每个选项。\n\n**A. 对于一个二次可微的函数 $\\phi$ 且 $\\phi''(a) \\neq 0$，确定性规则 $y_{\\text{det}}$ 相对于 $y_{\\star}$ 是有偏的，其主阶偏差为 $\\mathbb{E}\\big[\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big] - \\phi(a) \\approx \\tfrac{1}{2}\\,\\mathrm{Var}\\!\\left(\\tfrac{z}{p}\\,a\\right)\\,\\phi''(a) = \\tfrac{1}{2}\\,\\frac{1-p}{p}\\,a^{2}\\,\\phi''(a)$。**\n\n为了近似偏差，我们使用 $\\phi(X)$ 在其均值 $\\mathbb{E}[X] = a$ 附近的二阶泰勒展开：\n$$ \\phi(X) \\approx \\phi(a) + (X-a)\\phi'(a) + \\frac{1}{2}(X-a)^2\\phi''(a) $$\n对两边取期望：\n$$ \\mathbb{E}[\\phi(X)] \\approx \\mathbb{E}\\left[\\phi(a) + (X-a)\\phi'(a) + \\frac{1}{2}(X-a)^2\\phi''(a)\\right] $$\n利用期望的线性性质：\n$$ y_{\\star} \\approx \\phi(a) + \\mathbb{E}[X-a]\\phi'(a) + \\frac{1}{2}\\mathbb{E}[(X-a)^2]\\phi''(a) $$\n我们知道 $\\mathbb{E}[X-a] = \\mathbb{E}[X]-a = a-a = 0$，并且根据定义，$\\mathbb{E}[(X-a)^2] = \\mathrm{Var}(X)$。\n将这些代入近似式中：\n$$ y_{\\star} \\approx \\phi(a) + 0 \\cdot \\phi'(a) + \\frac{1}{2}\\mathrm{Var}(X)\\phi''(a) $$\n因此，偏差是：\n$$ \\mathbb{E}\\big[\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big] - \\phi(a) = y_{\\star} - y_{\\text{det}} \\approx \\frac{1}{2}\\mathrm{Var}(X)\\phi''(a) $$\n代入 $\\mathrm{Var}(X) = \\mathrm{Var}\\!\\left(\\tfrac{z}{p}\\,a\\right) = a^2 \\frac{1-p}{p}$ 的表达式：\n$$ \\mathbb{E}\\big[\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big] - \\phi(a) \\approx \\frac{1}{2} \\left(a^2 \\frac{1-p}{p}\\right) \\phi''(a) = \\frac{1}{2}\\,\\frac{1-p}{p}\\,a^{2}\\,\\phi''(a) $$\n该陈述与此推导完全匹配。\n**结论：正确。**\n\n**B. MC 估计量 $y_{\\text{MC}}$ 是有偏的，除非 $N \\to \\infty$；对于凸函数 $\\phi$，其期望值超过 $y_{\\star}$。**\n\nMC 估计量定义为 $y_{\\text{MC}} = \\frac{1}{N}\\sum_{i=1}^{N} \\phi\\!\\left(\\frac{z_i}{p}\\,a\\right)$，其中 $z_i$ 是来自 Bernoulli($p$) 的独立同分布（i.i.d.）样本。令 $Y_i = \\phi\\!\\left(\\frac{z_i}{p}\\,a\\right)$。根据定义，每个 $Y_i$ 的期望是 $y_{\\star}$。\n我们来计算估计量 $y_{\\text{MC}}$ 的期望：\n$$ \\mathbb{E}[y_{\\text{MC}}] = \\mathbb{E}\\left[ \\frac{1}{N}\\sum_{i=1}^{N} Y_i \\right] = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}[Y_i] $$\n由于每个 $Y_i$ 具有相同的分布，$\\mathbb{E}[Y_i] = y_{\\star}$ 对所有 $i = 1, \\dots, N$ 成立。\n$$ \\mathbb{E}[y_{\\text{MC}}] = \\frac{1}{N} \\sum_{i=1}^{N} y_{\\star} = \\frac{1}{N} (N \\cdot y_{\\star}) = y_{\\star} $$\n对于任何有限的 $N \\ge 1$，MC 估计量的期望都精确地等于 $y_{\\star}$。因此，$y_{\\text{MC}}$ 是 $y_{\\star}$ 的无偏估计量。声称它是有偏的，除非 $N \\to \\infty$ 是错误的。陈述的第二部分也是错误的，因为它的期望值等于 $y_{\\star}$，而不是大于它。\n**结论：不正确。**\n\n**C. 如果 $\\phi$ 是线性的，那么对于任何 $p \\in (0,1]$ 和任何 $a \\in \\mathbb{R}$，都有 $y_{\\text{det}} = y_{\\star}$ 精确成立。**\n\n设 $\\phi$ 为线性函数，$\\phi(u) = c u + d$，其中 $c, d \\in \\mathbb{R}$ 是常数。\n目标值 $y_{\\star}$ 是：\n$$ y_{\\star} = \\mathbb{E}[\\phi(X)] = \\mathbb{E}[c X + d] $$\n根据期望的线性性质：\n$$ y_{\\star} = c \\mathbb{E}[X] + d $$\n我们已经证明 $\\mathbb{E}[X] = a$。\n$$ y_{\\star} = c a + d $$\n确定性估计是：\n$$ y_{\\text{det}} = \\phi(a) = c a + d $$\n因此，$y_{\\star} = y_{\\text{det}}$ 精确成立。这个结果也可以从选项 A 的偏差近似中得到。对于线性函数，$\\phi''(u) = 0$ 对所有 $u$ 成立。泰勒展开仅包含一阶项是精确的，所以偏差恰好为 $0$。\n**结论：正确。**\n\n**D. 保持 $p$ 和 $a$ 不变，确定性偏差的大小 $\\big|y_{\\star} - y_{\\text{det}}\\big|$ 随着曲率 $\\big|\\phi''(a)\\big|$ 的增加而减小，因为平均过程平滑了非线性。**\n\n根据 A 中的分析，主阶确定性偏差为：\n$$ \\text{Bias} = y_{\\star} - y_{\\text{det}} \\approx \\frac{1}{2}\\,\\frac{1-p}{p}\\,a^{2}\\,\\phi''(a) $$\n此偏差的大小为：\n$$ |\\text{Bias}| \\approx \\left| \\frac{1}{2}\\,\\frac{1-p}{p}\\,a^{2}\\,\\phi''(a) \\right| = \\left(\\frac{1}{2}\\,\\frac{1-p}{p}\\,a^{2}\\right) |\\phi''(a)| $$\n当 $p$ 和 $a$ 固定时（$a \\neq 0, p \\in (0,1)$），括号中的项是一个正常数。偏差的大小与曲率的大小 $|\\phi''(a)|$ 成正比。因此，随着 $|\\phi''(a)|$ 的增加，偏差的大小*增加*，而不是减小。直观上，一个更弯曲的函数会比其线性近似偏离得更多，因此 $\\mathbb{E}[\\phi(X)]$ 和 $\\phi(\\mathbb{E}[X])$ 之间的差异会变得更大。\n**结论：不正确。**\n\n**E. 对于修正线性单元 $\\phi(u)=\\max(0,u)$，对于所有的 $a \\in \\mathbb{R}$ 和 $p \\in (0,1]$，都有 $y_{\\text{det}} = y_{\\star}$。**\n\nReLU 函数在 $u=0$ 处不是二次可微的，所以我们必须使用 $y_{\\star}$ 的精确定义。\n随机变量 $X = \\frac{z}{p}a$ 以概率 $p$ 取值 $\\frac{a}{p}$，以概率 $1-p$ 取值 $0$。\n$$ y_{\\star} = \\mathbb{E}\\left[\\phi\\left(\\frac{z}{p}a\\right)\\right] = p \\cdot \\phi\\left(\\frac{a}{p}\\right) + (1-p) \\cdot \\phi(0) $$\n由于 $\\phi(0) = \\max(0,0)=0$，这简化为：\n$$ y_{\\star} = p \\cdot \\phi\\left(\\frac{a}{p}\\right) = p \\cdot \\max\\left(0, \\frac{a}{p}\\right) $$\n确定性估计是 $y_{\\text{det}} = \\phi(a) = \\max(0, a)$。\n我们必须检查对于所有的 $a$ 是否有 $y_{\\star} = y_{\\text{det}}$。\n情况 1：$a > 0$。由于 $p \\in (0,1]$，我们有 $a/p \\ge a > 0$。\n- $y_{\\star} = p \\cdot \\max(0, a/p) = p \\cdot (a/p) = a$。\n- $y_{\\text{det}} = \\max(0, a) = a$。\n在这种情况下，$y_{\\star} = y_{\\text{det}}$。\n情况 2：$a \\le 0$。由于 $p > 0$，我们有 $a/p \\le 0$。\n- $y_{\\star} = p \\cdot \\max(0, a/p) = p \\cdot 0 = 0$。\n- $y_{\\text{det}} = \\max(0, a) = 0$。\n在这种情况下，$y_{\\star} = y_{\\text{det}}$。\n由于等式对 $a > 0$ 和 $a \\le 0$ 都成立，所以该陈述对所有 $a \\in \\mathbb{R}$ 都成立。这个特殊性质的出现是因为随机输入的一个状态是 $0$，这是 ReLU 函数的拐点，并且对于正输入，函数是线性的，我们已经知道在这种情况下等式成立。\n**结论：正确。**\n\n**F. 对于任何 $N \\in \\mathbb{N}$，MC 估计量 $y_{\\text{MC}}$ 是 $y_{\\star}$ 的一个无偏估计量，其方差按 $\\mathrm{Var}(y_{\\text{MC}}) = \\mathrm{Var}\\!\\big(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big)/N$ 的比例缩放，因此增加 $N$ 会减少由采样引起的误差，而不会引入偏差；较高的曲率通常会通过非线性对输入变异性的放大作用来增加 $\\mathrm{Var}\\!\\big(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big)$。**\n\n这个陈述包含多个部分。\n1. “$y_{\\text{MC}}$ 是 $y_{\\star}$ 的一个无偏估计量”：正如在选项 B 的分析中所示，$\\mathbb{E}[y_{\\text{MC}}] = y_{\\star}$。这是正确的。\n2. “其方差按 $\\mathrm{Var}(y_{\\text{MC}}) = \\mathrm{Var}\\!\\big(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big)/N$ 的比例缩放”：$y_{\\text{MC}}$ 是 $N$ 个独立同分布的随机变量 $Y_i = \\phi\\!\\left(\\frac{z_i}{p}\\,a\\right)$ 的样本均值。样本均值的方差是总体方差除以样本大小。因此，$\\mathrm{Var}(y_{\\text{MC}}) = \\frac{\\mathrm{Var}(Y_1)}{N} = \\frac{\\mathrm{Var}(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right))}{N}$。这是正确的。\n3. “增加 $N$ 会减少由采样引起的误差，而不会引入偏差”：由于无论 $N$ 为多少，偏差都为 $0$，并且方差以 $1/N$ 的速度减小，这是一个正确的结论。估计量的均方误差是 $\\mathrm{MSE}(y_{\\text{MC}}) = \\mathrm{Var}(y_{\\text{MC}}) + (\\text{Bias})^2 = \\frac{\\mathrm{Var}(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right))}{N} + 0^2$，它随着 $N$ 的增加而减小。\n4. “较高的曲率通常会通过非线性对输入变异性的放大作用来增加 $\\mathrm{Var}\\!\\big(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big)$...”：输出的方差 $\\mathrm{Var}(\\phi(X))$ 取决于输出值 $\\phi(0)$ 和 $\\phi(a/p)$ 的离散程度。与曲率较低的函数相比，曲率较高的函数通常会将输入值 $0$ 和 $a/p$ 映射到距离更远的输出值，从而增加输出的方差。这是一个合理的定性陈述。非线性“放大”了其输入的变异性，而放大的程度与函数的非线性程度（即曲率）有关。\n该陈述的所有部分在事实上和概念上都是正确的。\n**结论：正确。**\n\n最终复核正确选项：A, C, E, F。",
            "answer": "$$\\boxed{ACEF}$$"
        }
    ]
}