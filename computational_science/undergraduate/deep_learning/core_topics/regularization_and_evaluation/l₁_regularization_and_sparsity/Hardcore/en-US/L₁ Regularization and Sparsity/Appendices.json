{
    "hands_on_practices": [
        {
            "introduction": "The most fundamental application of $L_1$ regularization is in linear regression, where it gives rise to the LASSO model. This exercise provides a direct comparison between Ordinary Least Squares (OLS) regression and $L_1$-regularized regression to reveal how the $L_1$ penalty induces sparsity by driving many feature weights to exactly zero. By implementing this from scratch in a synthetic setting where the \"true\" important features are known, you will gain a concrete understanding of how LASSO performs robust feature selection, even in the presence of noise .",
            "id": "3140969",
            "problem": "You are given a synthetic sparse linear regression setting in which only a small subset of input dimensions influences the output. The goal is to implement and compare training with and without $\\ell_{1}$ regularization, and then evaluate how accurately the true sparse support is recovered as the observation noise varies.\n\nStart from the following foundational base:\n- The linear model assumption: the observed response vector $\\mathbf{y} \\in \\mathbb{R}^{n}$ is generated by $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^{\\star} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\mathbf{w}^{\\star} \\in \\mathbb{R}^{p}$ is the true parameter vector, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{n}$ is noise.\n- The concept of sparsity: the true parameter vector $\\mathbf{w}^{\\star}$ has only $k$ nonzero entries (the support size), with all other entries equal to zero.\n- The $\\ell_{1}$ norm: for any vector $\\mathbf{w} \\in \\mathbb{R}^{p}$, the $\\ell_{1}$ norm is $\\|\\mathbf{w}\\|_{1} = \\sum_{j=1}^{p} |w_{j}|$.\n- The $\\ell_{2}$ norm: for any vector $\\mathbf{r} \\in \\mathbb{R}^{n}$, the $\\ell_{2}$ norm is $\\|\\mathbf{r}\\|_{2} = \\left(\\sum_{i=1}^{n} r_{i}^{2}\\right)^{1/2}$.\n\nTask specification:\n1. Data generation:\n   - Use $n = 120$ samples and $p = 100$ features.\n   - Set the true sparsity level to $k = 5$.\n   - Use a fixed random seed for reproducibility so that the true support is constant across all test cases.\n   - Draw $\\mathbf{X}$ with entries independently from a standard normal distribution, then center each column to have zero mean and scale each column so that its empirical second moment equals $1$ (that is, for each column $j$, enforce $\\frac{1}{n}\\sum_{i=1}^{n} X_{ij}^{2} = 1$).\n   - Define the true parameter $\\mathbf{w}^{\\star}$ by selecting a support of size $k$ uniformly at random and setting those entries to $+1$, with all other entries set to $0$.\n   - Generate noise $\\boldsymbol{\\varepsilon}$ with independent entries from a normal distribution with zero mean and standard deviation $\\sigma$, and set $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^{\\star} + \\boldsymbol{\\varepsilon}$.\n2. Training procedures to implement:\n   - With $\\ell_{1}$ regularization: minimize the objective $(1/(2n))\\|\\mathbf{y}-\\mathbf{X}\\mathbf{w}\\|_{2}^{2} + \\lambda \\|\\mathbf{w}\\|_{1}$ to estimate $\\widehat{\\mathbf{w}}_{\\ell_{1}}$. Derive the optimization steps from first principles using optimality conditions and implement a correct algorithm (for example, cyclic coordinate descent using soft-thresholding) that converges to a minimizer.\n   - Without $\\ell_{1}$ regularization: minimize $(1/(2n))\\|\\mathbf{y}-\\mathbf{X}\\mathbf{w}\\|_{2}^{2}$ to estimate $\\widehat{\\mathbf{w}}_{\\mathrm{no}\\ \\ell_{1}}$ using Ordinary Least Squares (OLS), that is, solve the least-squares problem for $\\mathbf{w}$.\n   - For the $\\ell_{1}$ method, set the regularization parameter according to $\\lambda = \\alpha \\cdot \\sigma \\cdot \\sqrt{(2 \\log p)/n}$ with $\\alpha = 1$. This ties the regularization strength to the noise level in a principled way.\n3. Support recovery accuracy:\n   - Define the true support $S^{\\star} \\subset \\{1,\\dots,p\\}$ as the index set of nonzero entries of $\\mathbf{w}^{\\star}$, with $|S^{\\star}| = k$.\n   - For any estimated parameter vector $\\widehat{\\mathbf{w}}$, define its predicted support $\\widehat{S}$ as the index set of the $k$ largest entries of $\\widehat{\\mathbf{w}}$ in absolute value.\n   - Define the support recovery accuracy as the fraction of indices whose support membership is correctly classified: $\\frac{1}{p}\\sum_{j=1}^{p} \\mathbf{1}\\{(j \\in \\widehat{S}) \\Leftrightarrow (j \\in S^{\\star})\\}$, where $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function and $\\Leftrightarrow$ denotes logical equivalence.\n4. Test suite:\n   - Use the noise standard deviation values $\\sigma \\in \\{0.0, 0.1, 0.5, 1.0\\}$.\n   - For each $\\sigma$, train both models and compute the support recovery accuracy for each.\n5. Required final output format:\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\text{acc}_{\\ell_{1}}(\\sigma_{1}), \\text{acc}_{\\mathrm{no}\\ \\ell_{1}}(\\sigma_{1}), \\text{acc}_{\\ell_{1}}(\\sigma_{2}), \\text{acc}_{\\mathrm{no}\\ \\ell_{1}}(\\sigma_{2}), \\text{acc}_{\\ell_{1}}(\\sigma_{3}), \\text{acc}_{\\mathrm{no}\\ \\ell_{1}}(\\sigma_{3}), \\text{acc}_{\\ell_{1}}(\\sigma_{4}), \\text{acc}_{\\mathrm{no}\\ \\ell_{1}}(\\sigma_{4})]$, where each $\\text{acc}$ is a float in $[0,1]$.\n\nNo user input is allowed; all parameters must be set internally according to the above specification. The program must be fully deterministic given the fixed random seed, and must implement the training and evaluation exactly as specified.",
            "solution": "The user wants to compare the performance of Ordinary Least Squares (OLS) and LASSO ($\\ell_1$ regularized regression) in recovering the true sparse support of a parameter vector in a synthetic linear regression problem. The comparison will be performed across varying levels of observation noise.\n\n### 1. Problem Formulation\n\nThe underlying generative model is a linear relationship perturbed by additive Gaussian noise:\n$$\n\\mathbf{y} = \\mathbf{X}\\mathbf{w}^{\\star} + \\boldsymbol{\\varepsilon}\n$$\nwhere $\\mathbf{y} \\in \\mathbb{R}^{n}$ is the vector of observations, $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\mathbf{w}^{\\star} \\in \\mathbb{R}^{p}$ is the true sparse parameter vector with only $k$ non-zero entries, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{n}$ is a vector of i.i.d. noise components drawn from $\\mathcal{N}(0, \\sigma^2)$.\n\nWe aim to estimate $\\mathbf{w}^{\\star}$ using two methods:\n\n1.  **Ordinary Least Squares (OLS, no $\\ell_1$)**: This method minimizes the sum of squared residuals. The objective function is:\n    $$\n    L_{\\mathrm{no}\\ \\ell_{1}}(\\mathbf{w}) = \\frac{1}{2n} \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|_{2}^{2}\n    $$\n2.  **LASSO ($\\ell_1$ regularization)**: This method adds an $\\ell_1$ penalty term to the OLS objective, which encourages sparse solutions. The objective function is:\n    $$\n    L_{\\ell_{1}}(\\mathbf{w}) = \\frac{1}{2n} \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|_{2}^{2} + \\lambda \\|\\mathbf{w}\\|_{1}\n    $$\nwhere $\\|\\mathbf{w}\\|_{1} = \\sum_{j=1}^{p} |w_j|$ is the $\\ell_1$ norm of $\\mathbf{w}$.\n\n### 2. Data Generation\n\nThe synthetic data is generated as specified:\n-   **Dimensions**: Number of samples $n=120$, number of features $p=100$.\n-   **Sparsity**: The true support size is $k=5$.\n-   **Design Matrix $\\mathbf{X}$**: First, a matrix $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p}$ is drawn with entries from a standard normal distribution $\\mathcal{N}(0, 1)$. Each column $\\mathbf{z}_j$ is then centered to have zero mean and scaled to have an empirical second moment of $1$. That is, for each column $j$, the final column $\\mathbf{x}_j$ is computed as:\n    $$\n    \\mathbf{x}_j = \\frac{\\mathbf{z}_j - \\bar{z}_j \\mathbf{1}}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (z_{ij} - \\bar{z}_j)^2}}\n    $$\n    where $\\bar{z}_j = \\frac{1}{n} \\sum_{i=1}^n z_{ij}$ and $\\mathbf{1}$ is a vector of ones. This ensures that for the final matrix $\\mathbf{X}$, $\\frac{1}{n}\\sum_{i=1}^{n} X_{ij} = 0$ and $\\frac{1}{n}\\sum_{i=1}^{n} X_{ij}^2 = 1$ for all $j \\in \\{1,\\dots,p\\}$. Note that the second property implies $\\frac{1}{n}\\mathbf{x}_j^T\\mathbf{x}_j = 1$.\n-   **True Parameter Vector $\\mathbf{w}^{\\star}$**: A set of $k=5$ indices is chosen uniformly at random from $\\{1, \\dots, p\\}$ to form the true support $S^{\\star}$. The corresponding entries in $\\mathbf{w}^{\\star}$ are set to $1$, while all other $p-k$ entries are set to $0$.\n-   **Observation Vector $\\mathbf{y}$**: For a given noise level $\\sigma$, the noise vector $\\boldsymbol{\\varepsilon}$ is generated with i.i.d. entries from $\\mathcal{N}(0, \\sigma^2)$. The final observations are $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^{\\star} + \\boldsymbol{\\varepsilon}$.\n-   **Reproducibility**: A fixed random seed ensures that $\\mathbf{X}$ and $\\mathbf{w}^{\\star}$ are identical across all test cases (i.e., for all values of $\\sigma$).\n\n### 3. Model Training\n\n#### Ordinary Least Squares (OLS) Solution\nThe OLS estimator $\\widehat{\\mathbf{w}}_{\\mathrm{no}\\ \\ell_{1}}$ is found by minimizing $L_{\\mathrm{no}\\ \\ell_{1}}(\\mathbf{w})$. This objective is a convex quadratic function, and its minimum can be found by setting its gradient with respect to $\\mathbf{w}$ to zero.\n$$\n\\nabla_{\\mathbf{w}} L_{\\mathrm{no}\\ \\ell_{1}}(\\mathbf{w}) = \\frac{1}{n} \\nabla_{\\mathbf{w}} \\left( \\frac{1}{2} (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) \\right) = \\frac{1}{n} (-\\mathbf{X}^T\\mathbf{y} + \\mathbf{X}^T\\mathbf{X}\\mathbf{w})\n$$\nSetting the gradient to zero yields the normal equations:\n$$\n\\mathbf{X}^T\\mathbf{X}\\widehat{\\mathbf{w}} = \\mathbf{X}^T\\mathbf{y}\n$$\nSince $n=120 > p=100$ and $\\mathbf{X}$ is generated from a continuous distribution, the matrix $\\mathbf{X}^T\\mathbf{X}$ is invertible with probability $1$. The unique OLS solution is:\n$$\n\\widehat{\\mathbf{w}}_{\\mathrm{no}\\ \\ell_{1}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n\n#### LASSO Solution via Cyclic Coordinate Descent\nThe LASSO objective $L_{\\ell_{1}}(\\mathbf{w})$ is non-differentiable at points where any $w_j=0$ due to the $\\ell_1$ norm. We use an iterative optimization method called Cyclic Coordinate Descent (CCD). In CCD, we optimize the objective with respect to one coordinate $w_j$ at a time, holding all other coordinates fixed.\n\nLet's fix all coordinates $w_k$ for $k \\ne j$. The objective as a function of $w_j$ is:\n$$\nL(w_j) = \\frac{1}{2n} \\left\\|\\mathbf{y} - \\sum_{k \\ne j} \\mathbf{x}_k w_k - \\mathbf{x}_j w_j \\right\\|_2^2 + \\lambda |w_j| + \\text{const}\n$$\nLet $\\mathbf{r}_j = \\mathbf{y} - \\sum_{k \\ne j} \\mathbf{x}_k w_k$ be the partial residual. The objective simplifies to:\n$$\nL(w_j) = \\frac{1}{2n} \\|\\mathbf{r}_j - \\mathbf{x}_j w_j\\|_2^2 + \\lambda |w_j| = \\frac{1}{2n}(\\mathbf{r}_j^T\\mathbf{r}_j - 2w_j\\mathbf{x}_j^T\\mathbf{r}_j + w_j^2\\mathbf{x}_j^T\\mathbf{x}_j) + \\lambda |w_j|\n$$\nBecause of our column normalization, we have $\\mathbf{x}_j^T\\mathbf{x}_j = n$. The objective becomes:\n$$\nL(w_j) = \\frac{1}{2}w_j^2 - (\\frac{1}{n}\\mathbf{x}_j^T\\mathbf{r}_j)w_j + \\lambda|w_j| + \\text{const}\n$$\nThe subgradient of this with respect to $w_j$ must contain zero at the minimum. This condition leads to the update rule defined by the soft-thresholding operator $S_{\\tau}(\\cdot)$:\n$$\nw_j^{\\text{new}} = S_{\\lambda}\\left(\\frac{1}{n}\\mathbf{x}_j^T\\mathbf{r}_j\\right)\n$$\nwhere $S_{\\tau}(z) = \\text{sign}(z)\\max(|z|-\\tau, 0)$. The term $\\frac{1}{n}\\mathbf{x}_j^T\\mathbf{r}_j$ can be expressed in a computationally efficient way for an iterative algorithm. Let $\\mathbf{w}^{\\text{old}}$ be the coefficient vector before the update of $w_j$.\n$$\n\\frac{1}{n}\\mathbf{x}_j^T\\mathbf{r}_j = \\frac{1}{n}\\mathbf{x}_j^T\\left(\\mathbf{y} - \\sum_{k \\ne j} \\mathbf{x}_k w_k^{\\text{old}}\\right) = \\frac{1}{n}\\mathbf{x}_j^T\\left(\\mathbf{y} - \\mathbf{X}\\mathbf{w}^{\\text{old}} + \\mathbf{x}_j w_j^{\\text{old}}\\right) = \\frac{1}{n}\\mathbf{x}_j^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w}^{\\text{old}}) + w_j^{\\text{old}}\n$$\nThe term $\\mathbf{y} - \\mathbf{X}\\mathbf{w}^{\\text{old}}$ is the current full residual. This leads to an efficient CCD algorithm:\n1.  Initialize $\\widehat{\\mathbf{w}}_{\\ell_1} = \\mathbf{0}$.\n2.  Repeat for a fixed number of epochs (e.g., $100$):\n    For each coordinate $j=1, \\dots, p$:\n    a. Let $w_j^{\\text{old}}$ be the current value of the $j$-th coefficient.\n    b. Calculate the correlation of the $j$-th feature with the current partial residual: $\\rho_j = \\frac{1}{n}\\mathbf{x}_j^T(\\mathbf{y} - \\mathbf{X}\\widehat{\\mathbf{w}}_{\\ell_1}) + w_j^{\\text{old}}$. Note that this is efficient if we maintain the full residual $\\mathbf{r} = \\mathbf{y} - \\mathbf{X}\\widehat{\\mathbf{w}}_{\\ell_1}$ and update it.\n    c. Apply the soft-thresholding operator to update the coefficient: $w_j^{\\text{new}} = S_{\\lambda}(\\rho_j)$.\n    d. Update the coefficient vector: $\\widehat{w}_{\\ell_1, j} = w_j^{\\text{new}}$.\n\nThe regularization parameter is set according to the theoretically motivated choice $\\lambda = \\alpha \\sigma \\sqrt{\\frac{2 \\log p}{n}}$, with $\\alpha=1$.\n\n### 4. Support Recovery Accuracy\n\nThe performance is measured by how well the estimated support $\\widehat{S}$ matches the true support $S^{\\star}$.\n-   $S^{\\star} = \\{ j \\mid w_j^{\\star} \\ne 0 \\}$, the set of indices of true non-zero coefficients. $|S^{\\star}| = k$.\n-   $\\widehat{S} = \\{ j \\mid \\widehat{w}_j \\text{ is among the } k \\text{ largest in absolute value} \\}$, the estimated support. $|\\widehat{S}| = k$.\n\nThe accuracy is the fraction of indices whose support membership is correctly identified. An index $j$ is classified correctly if it is either in both $S^{\\star}$ and $\\widehat{S}$ (a true positive) or in neither (a true negative).\n-   True Positives: $TP = |S^{\\star} \\cap \\widehat{S}|$.\n-   True Negatives: $TN = |\\{1,\\dots,p\\} \\setminus (S^{\\star} \\cup \\widehat{S})|$.\nSince $|S^{\\star}|=k$ and $|\\widehat{S}|=k$, we have $|S^{\\star} \\cup \\widehat{S}| = |S^{\\star}| + |\\widehat{S}| - |S^{\\star} \\cap \\widehat{S}| = 2k - TP$.\nTherefore, $TN = p - (2k - TP) = p - 2k + TP$.\nThe accuracy is defined as:\n$$\n\\text{Accuracy} = \\frac{TP + TN}{p} = \\frac{TP + (p - 2k + TP)}{p} = \\frac{p - 2k + 2TP}{p}\n$$\nThis formula is used to evaluate the accuracy for both $\\widehat{\\mathbf{w}}_{\\mathrm{no}\\ \\ell_{1}}$ and $\\widehat{\\mathbf{w}}_{\\ell_{1}}$ for each noise level $\\sigma$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares OLS and LASSO for sparse support recovery.\n    \"\"\"\n    # 1. Problem parameters\n    n = 120  # Number of samples\n    p = 100  # Number of features\n    k = 5    # Sparsity level (number of non-zero coefficients)\n    fixed_seed = 42 # Seed for reproducibility of X and w_star\n\n    # Test suite: noise standard deviations\n    sigmas = [0.0, 0.1, 0.5, 1.0]\n\n    # --- Data Generation (fixed across noise levels) ---\n    rng = np.random.default_rng(seed=fixed_seed)\n    \n    # Generate design matrix X\n    X_raw = rng.standard_normal(size=(n, p))\n    # Center each column\n    X_centered = X_raw - X_raw.mean(axis=0)\n    # Scale each column so its empirical second moment is 1\n    # (equivalent to empirical variance since columns are centered)\n    col_norms = np.sqrt(np.mean(X_centered**2, axis=0))\n    X = X_centered / col_norms\n\n    # Generate true sparse parameter vector w_star\n    w_star = np.zeros(p)\n    support_indices = rng.choice(p, size=k, replace=False)\n    w_star[support_indices] = 1.0\n    S_star = set(support_indices)\n\n    results = []\n\n    # Loop over each noise level\n    for sigma in sigmas:\n        # --- Generate response vector y for the current noise level ---\n        if sigma == 0.0:\n            epsilon = np.zeros(n)\n        else:\n            # Use a different seed for noise generation to ensure it's independent\n            # but still reproducible for a given sigma.\n            noise_rng = np.random.default_rng(seed=int(sigma * 1000))\n            epsilon = noise_rng.normal(0, sigma, n)\n        \n        y = X @ w_star + epsilon\n\n        # --- 2. Training Procedures ---\n\n        # A. Without L1 regularization (Ordinary Least Squares)\n        # Solve the normal equations: (X.T @ X) @ w = X.T @ y\n        XTX = X.T @ X\n        XTy = X.T @ y\n        w_no_l1 = np.linalg.solve(XTX, XTy)\n\n        # B. With L1 regularization (LASSO via Cyclic Coordinate Descent)\n        alpha = 1.0\n        lambda_val = alpha * sigma * np.sqrt(2 * np.log(p) / n)\n        \n        def soft_threshold(rho, lam):\n            \"\"\"Soft-thresholding operator.\"\"\"\n            if rho > lam:\n                return rho - lam\n            elif rho  -lam:\n                return rho + lam\n            else:\n                return 0.0\n\n        w_l1 = np.zeros(p)\n        num_epochs = 100  # Number of CCD cycles\n\n        # Efficient CCD implementation with residual updates\n        residual = y - X @ w_l1\n        for _ in range(num_epochs):\n            for j in range(p):\n                w_old_j = w_l1[j]\n                \n                # Incorrect residual update was in thought process.\n                # The correct efficient update needs to remove the old contribution before computing rho.\n                # rho_j = (X[:, j] @ (residual + X[:, j] * w_old_j)) / n\n                # This formulation from Murphy's textbook (and others) is standard:\n                # Let's use a simpler but correct one to avoid bugs.\n                # Recomputing rho_j from scratch is safer and fast enough for these dimensions.\n                # The argument to soft_threshold is (1/n)*X_j^T*(y - sum_{k!=j} X_k w_k)\n                # = (1/n)*X_j^T*(y - Xw + X_j w_j) = (1/n)*X_j^T*(residual) + w_old_j\n                \n                # We update the residual for efficiency, this is the correct way.\n                # Before updating w_j, the residual does not account for w_j's contribution.\n                # Let's re-add w_j's contribution to get the partial residual.\n                rho = (X[:, j].T @ (residual + X[:, j] * w_old_j))\n                w_new_j = soft_threshold(rho / n, lambda_val)\n                \n                # Update residual with the change in w_j\n                if w_new_j != w_old_j:\n                    residual -= X[:, j] * (w_new_j - w_old_j)\n                    w_l1[j] = w_new_j\n\n\n        # --- 3. Support Recovery Accuracy ---\n        def calculate_accuracy(w_est, S_star_set, p_dim, k_dim):\n            \"\"\"Calculates support recovery accuracy.\"\"\"\n            # Get predicted support: indices of k largest absolute values\n            if np.all(w_est == 0):\n                S_hat = set()\n            else:\n                S_hat = set(np.argsort(np.abs(w_est))[-k_dim:])\n            \n            # True Positives: intersection of true and predicted supports\n            tp = len(S_hat.intersection(S_star_set))\n            \n            # Accuracy = (TP + TN) / p\n            # TN = p - |S* U S_hat| = p - (|S*|+|S_hat|-|S* intersect S_hat|)\n            # TN = p - (k + k - tp) = p - 2k + tp\n            # Accuracy = (p_dim - 2 * k_dim + 2 * tp) / p_dim\n            accuracy = (p_dim - 2 * k_dim + 2 * tp) / p_dim\n            return accuracy\n        \n        acc_l1 = calculate_accuracy(w_l1, S_star, p, k)\n        acc_no_l1 = calculate_accuracy(w_no_l1, S_star, p, k)\n        \n        results.extend([acc_l1, acc_no_l1])\n\n    # --- 4. Final Output ---\n    # Format: [acc_l1(s1), acc_no_l1(s1), acc_l1(s2), acc_no_l1(s2), ...]\n    output_str = \",\".join([f\"{res:.10f}\" for res in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While applying an $L_1$ penalty to individual weights can create sparsity, a more powerful approach in deep learning is to induce *structured* sparsity, such as removing entire channels or filters. This practice explores a clever and effective method for channel pruning by applying the $L_1$ penalty not to the convolutional weights, but to the scaling parameter $\\Gamma$ in a Batch Normalization layer . As you will see, forcing a channel's $\\Gamma$ to zero effectively nullifies that entire channel's contribution, providing a direct mechanism for network compression.",
            "id": "3140994",
            "problem": "You are given a simplified deep learning model with Batch Normalization (BN) that maps an input matrix $X \\in \\mathbb{R}^{N \\times d}$ to a scalar output via one hidden layer with $C$ channels. The hidden pre-activations are $H = XW + \\mathbf{1}b^\\top$ where $W \\in \\mathbb{R}^{d \\times C}$, $b \\in \\mathbb{R}^{C}$, and $\\mathbf{1} \\in \\mathbb{R}^{N}$ is the all-ones vector. For each channel, Batch Normalization (BN) computes a batch mean $\\mu \\in \\mathbb{R}^{C}$ and variance $\\sigma^2 \\in \\mathbb{R}^{C}$, produces normalized activations $Z = (H - \\mu)/\\sqrt{\\sigma^2 + \\epsilon}$, and then scales and shifts with parameters $\\Gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$ to yield $Y = \\Gamma \\odot Z + \\beta$, where $\\odot$ denotes elementwise multiplication. The scalar model output for the $i$-th sample is $s_i = \\sum_{k=1}^{C} Y_{i,k} v_k$ with $v \\in \\mathbb{R}^{C}$. During training, the running mean and variance used at test time are updated with momentum $m \\in (0,1)$ as $\\mu_{\\text{run}} \\leftarrow m \\mu_{\\text{run}} + (1-m)\\mu$ and $\\sigma^2_{\\text{run}} \\leftarrow m \\sigma^2_{\\text{run}} + (1-m)\\sigma^2$.\n\nThe empirical risk is defined using the logistic loss with labels $y_i \\in \\{-1,+1\\}$:\n$$\n\\mathcal{L}_{\\text{logistic}} = \\frac{1}{N}\\sum_{i=1}^{N} \\log\\left(1 + \\exp(-y_i s_i)\\right).\n$$\nWe consider two distinct regularized objectives via empirical risk minimization:\n- Case A (weight sparsity): add an $L_1$ penalty on the weights $W$ with strength $\\alpha$, giving the objective $J_A = \\mathcal{L}_{\\text{logistic}} + \\alpha \\|W\\|_1$.\n- Case B (channel sparsity): add an $L_1$ penalty on the BN scale parameters $\\Gamma$ with the same strength $\\alpha$, giving the objective $J_B = \\mathcal{L}_{\\text{logistic}} + \\alpha \\|\\Gamma\\|_1$.\n\nFor optimization, use gradient descent on the smooth part of the objective with learning rate $\\eta$, and apply the proximal operator associated with the $L_1$ norm to the penalized parameters after each gradient step. The Batch Normalization (BN) gradients must be derived from the BN definitions given above; you must not assume any unproven or ad hoc formulas.\n\nDefine an “inactive channel at test time” as any channel index $k \\in \\{1,\\dots,C\\}$ whose trained BN scale parameter satisfies $|\\Gamma_k| \\le \\tau$, where $\\tau  0$ is a small threshold. The test-time BN transformation uses the running mean $\\mu_{\\text{run}}$ and running variance $\\sigma^2_{\\text{run}}$, but the inactivity criterion depends only on $\\Gamma$.\n\nYour program must:\n- Generate a synthetic binary classification dataset in $\\mathbb{R}^d$ with $N$ samples by drawing $N/2$ samples for $y=+1$ from a multivariate normal distribution with mean vector $\\mu_{+} \\in \\mathbb{R}^d$ and covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$, and $N/2$ samples for $y=-1$ from a multivariate normal distribution with mean vector $\\mu_{-} \\in \\mathbb{R}^d$ and the same covariance $\\Sigma$. All random draws must be reproducible by fixing a seed.\n- Train the model twice for each test case: once with Case A ($L_1$ penalty on $W$) and once with Case B ($L_1$ penalty on $\\Gamma$), using identical initial parameters and hyperparameters except for which parameter receives the $L_1$ penalty.\n- After training, count how many channels are inactive at test time for each case, i.e., the number of $k$ such that $|\\Gamma_k| \\le \\tau$.\n\nStart from the following foundations exclusively:\n- Empirical Risk Minimization with logistic loss as defined above.\n- Batch Normalization (BN) as defined above, including the use of running averages for test-time statistics.\n- Gradient descent for the smooth part of the objective and the proximal operator corresponding to the $L_1$ norm (without assuming any shortcut formulas not derived from the $L_1$ norm definition).\n\nYour program must implement these steps with clear, principled computations.\n\nUse the following fixed hyperparameters unless otherwise stated in the test cases:\n- Input dimension $d = 4$.\n- Number of channels $C = 8$.\n- Training set size $N = 256$.\n- Learning rate $\\eta = 0.05$.\n- BN epsilon $\\epsilon = 10^{-5}$.\n- BN momentum $m = 0.9$.\n- Initial parameters: $W$ initialized with independent normal draws of standard deviation $0.1$, $b$ initialized to $0$, $\\Gamma$ initialized to $1$, $\\beta$ initialized to $0$, $v$ initialized with independent normal draws of standard deviation $0.1$.\n\nImplement a single-epoch full-batch gradient descent loop repeated for the specified number of steps in each test case. You must ensure that both Case A and Case B start from identical initial parameters in each test case to make the comparison fair.\n\nTest Suite:\nProvide the following four test cases, each given as a tuple $(\\alpha, \\tau, S)$ where $\\alpha$ is the $L_1$ strength, $\\tau$ is the inactivity threshold, and $S$ is the number of gradient descent steps:\n1. $(0.0, 10^{-3}, 200)$: No $L_1$ penalty baseline.\n2. $(10^{-3}, 10^{-3}, 400)$: Weak $L_1$ to compare effects.\n3. $(5 \\times 10^{-3}, 10^{-3}, 400)$: Moderate $L_1$ to induce noticeable sparsity.\n4. $(2 \\times 10^{-2}, 10^{-3}, 400)$: Strong $L_1$ to test near-collapse behavior.\n\nRequired Final Output Format:\nYour program should produce a single line of output containing a list of results for the four test cases. Each result is itself a list of two integers $[n_{\\Gamma}, n_{W}]$, where $n_{\\Gamma}$ is the number of inactive channels when penalizing $\\Gamma$ (Case B) and $n_{W}$ is the number of inactive channels when penalizing $W$ (Case A), in that order. The final line must therefore look like:\n$[ [n_{\\Gamma}^{(1)}, n_{W}^{(1)}], [n_{\\Gamma}^{(2)}, n_{W}^{(2)}], [n_{\\Gamma}^{(3)}, n_{W}^{(3)}], [n_{\\Gamma}^{(4)}, n_{W}^{(4)}] ]$,\nwith the four inner lists corresponding to the four test cases in the order listed above.",
            "solution": "The user's request is to analyze and compare the sparsity-inducing effects of applying an $L_1$ penalty to either the weights of a linear layer or the scaling parameters of a subsequent Batch Normalization (BN) layer. This is a well-defined problem in computational science, grounded in the principles of empirical risk minimization and neural network optimization. The problem is valid and will be solved by deriving the necessary gradients and implementing the specified proximal gradient descent algorithm.\n\n### 1. Model and Objective Functions\n\nThe model maps an input matrix $X \\in \\mathbb{R}^{N \\times d}$ to a vector of scalar outputs $s \\in \\mathbb{R}^{N}$ through a single hidden layer with $C$ channels.\n\n**Forward Pass:**\n1.  **Linear Transformation**: The pre-activation for the hidden layer is computed as $H = XW + \\mathbf{1}b^\\top$, where $W \\in \\mathbb{R}^{d \\times C}$ are the weights, $b \\in \\mathbb{R}^{C}$ is the bias vector, and $\\mathbf{1} \\in \\mathbb{R}^{N}$ is a vector of ones.\n2.  **Batch Normalization (BN)**: For each channel $k \\in \\{1, \\dots, C\\}$, the batch mean $\\mu_k$ and variance $\\sigma^2_k$ are calculated over the $N$ samples:\n    $$\n    \\mu_k = \\frac{1}{N} \\sum_{i=1}^{N} H_{i,k} \\quad \\text{and} \\quad \\sigma_k^2 = \\frac{1}{N} \\sum_{i=1}^{N} (H_{i,k} - \\mu_k)^2\n    $$\n    The pre-activations are then normalized:\n    $$\n    Z_{i,k} = \\frac{H_{i,k} - \\mu_k}{\\sqrt{\\sigma_k^2 + \\epsilon}}\n    $$\n    where $\\epsilon$ is a small constant for numerical stability. Finally, a learned affine transformation is applied:\n    $$\n    Y_{i,k} = \\Gamma_k Z_{i,k} + \\beta_k\n    $$\n    using the channel-wise scale parameter $\\Gamma_k$ and shift parameter $\\beta_k$.\n3.  **Output Layer**: The final scalar score for the $i$-th sample is a linear combination of the channel outputs:\n    $$\n    s_i = \\sum_{k=1}^{C} Y_{i,k} v_k\n    $$\n    where $v \\in \\mathbb{R}^{C}$ is the output weight vector.\n\n**Objective Functions:**\nThe base loss is the empirical risk using the logistic loss for binary classification with labels $y_i \\in \\{-1, +1\\}$:\n$$\n\\mathcal{L}_{\\text{logistic}} = \\frac{1}{N} \\sum_{i=1}^{N} \\log(1 + \\exp(-y_i s_i))\n$$\nTwo regularized objectives are considered:\n-   **Case A (Weight Sparsity)**: An $L_1$ penalty is applied to the weight matrix $W$:\n    $$\n    J_A = \\mathcal{L}_{\\text{logistic}} + \\alpha \\|W\\|_1 = \\mathcal{L}_{\\text{logistic}} + \\alpha \\sum_{j=1}^{d} \\sum_{k=1}^{C} |W_{j,k}|\n    $$\n-   **Case B (Channel Sparsity)**: An $L_1$ penalty is applied to the BN scale vector $\\Gamma$:\n    $$\n    J_B = \\mathcal{L}_{\\text{logistic}} + \\alpha \\|\\Gamma\\|_1 = \\mathcal{L}_{\\text{logistic}} + \\alpha \\sum_{k=1}^{C} |\\Gamma_k|\n    $$\n\n### 2. Optimization by Proximal Gradient Descent\n\nThe objectives $J_A$ and $J_B$ are composite, consisting of a smooth part ($\\mathcal{L}_{\\text{logistic}}$) and a non-smooth part (the $L_1$ norm). Such problems are effectively solved using proximal gradient descent. For a general objective $J(\\theta) = f(\\theta) + g(\\theta)$ where $f$ is smooth and $g$ is non-smooth, the update rule is:\n$$\n\\theta_{t+1} = \\text{prox}_{\\eta g}(\\theta_t - \\eta \\nabla f(\\theta_t))\n$$\nThe proximal operator for the scaled $L_1$ norm, $g(\\theta) = \\alpha \\|\\theta\\|_1$, is the soft-thresholding function, applied element-wise:\n$$\n\\text{prox}_{\\eta \\alpha \\|\\cdot\\|_1}(x)_i = \\text{sign}(x_i) \\max(|x_i| - \\eta\\alpha, 0)\n$$\nIn our procedure, we first perform a standard gradient descent step on the parameters using the gradients of $\\mathcal{L}_{\\text{logistic}}$, and then apply the soft-thresholding operator to the specific parameter being penalized ($W$ in Case A, $\\Gamma$ in Case B).\n\n### 3. Gradient Derivation\n\nTo implement the gradient descent step, we need the gradients of $\\mathcal{L}_{\\text{logistic}}$ with respect to all trainable parameters ($W, b, \\Gamma, \\beta, v$). We use the chain rule, starting from the output and propagating backwards. Let $\\nabla_{\\theta}\\mathcal{L} \\equiv \\frac{\\partial \\mathcal{L}_{\\text{logistic}}}{\\partial \\theta}$.\n\n1.  **Gradient w.r.t. scores $s$**:\n    $\\frac{\\partial \\mathcal{L}_{\\text{logistic}}}{\\partial s_i} = \\frac{1}{N} \\frac{-y_i \\exp(-y_i s_i)}{1+\\exp(-y_i s_i)} = \\frac{1}{N} \\frac{-y_i}{1+\\exp(y_i s_i)}$. Let this gradient vector be $\\nabla_s \\mathcal{L} \\in \\mathbb{R}^N$.\n\n2.  **Gradients w.r.t. output layer ($v$) and BN output ($Y$)**:\n    $$\n    \\nabla_v \\mathcal{L} = Y^\\top (\\nabla_s \\mathcal{L}) \\quad \\in \\mathbb{R}^C\n    $$\n    $$\n    \\nabla_Y \\mathcal{L} = (\\nabla_s \\mathcal{L}) v^\\top \\quad \\in \\mathbb{R}^{N \\times C}\n    $$\n    where $\\nabla_s \\mathcal{L}$ is treated as a column vector.\n\n3.  **Gradients w.r.t. BN parameters ($\\Gamma, \\beta$) and BN input ($Z$)**:\n    $$\n    \\nabla_\\Gamma \\mathcal{L} = \\text{sum}(\\nabla_Y \\mathcal{L} \\odot Z, \\text{axis}=0) \\quad \\in \\mathbb{R}^C\n    $$\n    $$\n    \\nabla_\\beta \\mathcal{L} = \\text{sum}(\\nabla_Y \\mathcal{L}, \\text{axis}=0) \\quad \\in \\mathbb{R}^C\n    $$\n    $$\n    \\nabla_Z \\mathcal{L} = \\nabla_Y \\mathcal{L} \\odot \\Gamma^\\top \\quad \\in \\mathbb{R}^{N \\times C}\n    $$\n    where $\\odot$ is element-wise multiplication and $\\Gamma^\\top$ is broadcast across the batch dimension.\n\n4.  **Gradient w.r.t. pre-normalization activations $H$**: This is the most involved step, as each $H_{i,k}$ affects all $Z_{j,k}$ for a given channel $k$ through the batch statistics $\\mu_k$ and $\\sigma_k^2$. The final expression for the gradient with respect to the input matrix $H$ (for a single channel $k$, then generalized) is:\n    $$\n    (\\nabla_H \\mathcal{L})_{i,k} = \\frac{1}{\\sqrt{\\sigma_k^2+\\epsilon}} \\left[ (\\nabla_Z \\mathcal{L})_{i,k} - \\frac{1}{N}\\sum_{j=1}^N (\\nabla_Z \\mathcal{L})_{j,k} - Z_{i,k} \\frac{1}{N}\\sum_{j=1}^N \\left((\\nabla_Z \\mathcal{L})_{j,k} Z_{j,k}\\right) \\right]\n    $$\n    This can be implemented efficiently using vectorized operations.\n\n5.  **Gradients w.r.t. input weights ($W$) and biases ($b$)**:\n    $$\n    \\nabla_W \\mathcal{L} = X^\\top (\\nabla_H \\mathcal{L}) \\quad \\in \\mathbb{R}^{d \\times C}\n    $$\n    $$\n    \\nabla_b \\mathcal{L} = \\text{sum}(\\nabla_H \\mathcal{L}, \\text{axis}=0) \\quad \\in \\mathbb{R}^C\n    $$\n\n### 4. Implementation and Evaluation\n\nThe program implements this process for the specified test cases. A synthetic dataset is generated from two multivariate normal distributions. For each test case, two training runs are performed, starting from identical initial parameters: one for Case A and one for Case B. A full-batch gradient descent loop is executed for the specified number of steps, applying the updates derived above. After training, the number of \"inactive channels\" is counted for each case, defined as channels where the final learned scale parameter $|\\Gamma_k|$ falls below a threshold $\\tau$. The results are then collated and printed.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment and print the results.\n    \"\"\"\n    RNG_SEED = 42\n    np.random.seed(RNG_SEED)\n\n    # Fixed hyperparameters\n    CONSTS = {\n        'd': 4,\n        'C': 8,\n        'N': 256,\n        'eta': 0.05,\n        'epsilon': 1e-5,\n        'm': 0.9,\n    }\n\n    # Test cases: (alpha, tau, S)\n    test_cases = [\n        (0.0, 1e-3, 200),\n        (1e-3, 1e-3, 400),\n        (5e-3, 1e-3, 400),\n        (2e-2, 1e-3, 400),\n    ]\n\n    # Data generation parameters\n    mu_plus = np.full(CONSTS['d'], 0.5)\n    mu_minus = np.full(CONSTS['d'], -0.5)\n    Sigma = np.eye(CONSTS['d'])\n    \n    # Generate dataset\n    N_half = CONSTS['N'] // 2\n    X_plus = np.random.multivariate_normal(mu_plus, Sigma, N_half)\n    X_minus = np.random.multivariate_normal(mu_minus, Sigma, N_half)\n    X = np.vstack([X_plus, X_minus])\n    y = np.hstack([np.ones(N_half), -np.ones(N_half)])\n\n    all_results = []\n    \n    for alpha, tau, S in test_cases:\n        # Generate initial parameters for this test case\n        # Ensures both Case A and B start from the exact same point\n        initial_params = {\n            'W': np.random.normal(0, 0.1, (CONSTS['d'], CONSTS['C'])),\n            'b': np.zeros(CONSTS['C']),\n            'Gamma': np.ones(CONSTS['C']),\n            'beta': np.zeros(CONSTS['C']),\n            'v': np.random.normal(0, 0.1, CONSTS['C']),\n            'mu_run': np.zeros(CONSTS['C']),\n            'sigma2_run': np.ones(CONSTS['C']),\n        }\n\n        # Case A: Penalize W\n        final_params_A = run_training(initial_params, X, y, alpha, S, penalize_W=True, consts=CONSTS)\n        n_W = np.sum(np.abs(final_params_A['Gamma']) = tau)\n\n        # Case B: Penalize Gamma\n        final_params_B = run_training(initial_params, X, y, alpha, S, penalize_W=False, consts=CONSTS)\n        n_Gamma = np.sum(np.abs(final_params_B['Gamma']) = tau)\n        \n        all_results.append([n_Gamma, n_W])\n\n    # Format and print the final output\n    print(str(all_results).replace(' ', ''))\n\n\ndef soft_threshold(x, threshold):\n    \"\"\"Soft-thresholding operator for L1 proximal step.\"\"\"\n    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n\n\ndef run_training(initial_params, X, y, alpha, S, penalize_W, consts):\n    \"\"\"\n    Runs the training loop for S steps.\n    \"\"\"\n    # Make a deep copy to avoid modifying the initial parameters dict\n    params = {k: np.copy(v) for k, v in initial_params.items()}\n\n    for _ in range(S):\n        # 1. Forward pass\n        s, cache = forward_pass(X, params, consts)\n\n        # 2. Update running statistics\n        params['mu_run'] = consts['m'] * params['mu_run'] + (1 - consts['m']) * cache['mu']\n        params['sigma2_run'] = consts['m'] * params['sigma2_run'] + (1 - consts['m']) * cache['var']\n\n        # 3. Backward pass (compute gradients of the smooth part)\n        grads = backward_pass(X, y, s, cache, consts)\n\n        # 4. Parameter update (gradient descent + proximal step)\n        eta_alpha = consts['eta'] * alpha\n\n        # Update non-penalized parameters with standard gradient descent\n        params['v'] -= consts['eta'] * grads['dL_dv']\n        params['beta'] -= consts['eta'] * grads['dL_dbeta']\n        params['b'] -= consts['eta'] * grads['dL_db']\n\n        if penalize_W:\n            # Penalize W (Case A)\n            params['Gamma'] -= consts['eta'] * grads['dL_dGamma']\n            W_temp = params['W'] - consts['eta'] * grads['dL_dW']\n            params['W'] = soft_threshold(W_temp, eta_alpha)\n        else:\n            # Penalize Gamma (Case B)\n            params['W'] -= consts['eta'] * grads['dL_dW']\n            G_temp = params['Gamma'] - consts['eta'] * grads['dL_dGamma']\n            params['Gamma'] = soft_threshold(G_temp, eta_alpha)\n\n    return params\n\n\ndef forward_pass(X, params, consts):\n    \"\"\"Computes the forward pass of the model.\"\"\"\n    W, b = params['W'], params['b']\n    Gamma, beta = params['Gamma'], params['beta']\n    v = params['v']\n    \n    # Linear layer\n    H = X @ W + b\n    \n    # Batch Normalization\n    mu = H.mean(axis=0)\n    var = H.var(axis=0)\n    sigma_eff = np.sqrt(var + consts['epsilon'])\n    H_centered = H - mu\n    Z = H_centered / sigma_eff\n    Y = Gamma * Z + beta\n    \n    # Output layer\n    s = Y @ v\n    \n    cache = {'X': X, 'H': H, 'Z': Z, 'mu': mu, 'var': var, 'sigma_eff': sigma_eff, 'Gamma': Gamma, 'Y': Y, 'v': v}\n    return s, cache\n\n\ndef backward_pass(X, y, s, cache, consts):\n    \"\"\"Computes the gradients for all parameters.\"\"\"\n    N = consts['N']\n    \n    # Gradient of loss w.r.t. scores s\n    g_s = -y / (1 + np.exp(y * s))\n    dL_ds = (1 / N) * g_s\n    \n    # Backprop to v and Y\n    dL_dv = cache['Y'].T @ dL_ds\n    dL_dY = np.outer(dL_ds, cache['v'])\n    \n    # Backprop through BN affine transform to Gamma, beta, and Z\n    dL_dGamma = np.sum(dL_dY * cache['Z'], axis=0)\n    dL_dbeta = np.sum(dL_dY, axis=0)\n    dL_dZ = dL_dY * cache['Gamma']\n    \n    # Backprop through BN normalization to H\n    dLdZ_sum = np.sum(dL_dZ, axis=0)\n    dLdZ_Z_sum = np.sum(dL_dZ * cache['Z'], axis=0)\n    \n    term1 = dL_dZ\n    term2 = (1 / N) * dLdZ_sum\n    term3 = (1 / N) * cache['Z'] * dLdZ_Z_sum\n    \n    dL_dH = (1 / cache['sigma_eff']) * (term1 - term2 - term3)\n    \n    # Backprop through linear layer to W and b\n    dL_dW = X.T @ dL_dH\n    dL_db = np.sum(dL_dH, axis=0)\n    \n    grads = {'dL_dv': dL_dv, 'dL_dGamma': dL_dGamma, 'dL_dbeta': dL_dbeta, 'dL_dW': dL_dW, 'dL_db': dL_db}\n    return grads\n\nsolve()\n\n```"
        },
        {
            "introduction": "The $L_1$ norm is a convex proxy for the non-convex $L_0$ \"norm,\" which directly counts non-zero elements. This final practice dives into the algorithmic difference between these two formulations of sparsity . You will implement and compare Proximal Gradient Descent with soft-thresholding (for the $L_1$ problem) against Iterative Hard-Thresholding (for the $L_0$ problem), gaining insight into how these distinct mathematical objectives lead to different iterative schemes and sparsity outcomes.",
            "id": "3140920",
            "problem": "You are asked to implement and compare two iterative methods that induce sparsity in linear inverse problems. Consider solving a linear system with a design matrix $A \\in \\mathbb{R}^{m \\times n}$ and observation vector $b \\in \\mathbb{R}^{m}$. You will compare:\n- Iterative hard-thresholding, which approximately addresses the problem $\\min_{x} \\tfrac{1}{2}\\lVert A x - b \\rVert_{2}^{2}$ subject to the sparsity constraint $\\lVert x \\rVert_{0} \\leq k$ by alternating a gradient step on the smooth term with a hard-thresholding projection onto the set of $k$-sparse vectors.\n- Proximal gradient descent (PGD) with soft-thresholding, which addresses the $\\ell_{1}$-regularized problem $\\min_{x} \\tfrac{1}{2}\\lVert A x - b \\rVert_{2}^{2} + \\lambda \\lVert x \\rVert_{1}$ by alternating a gradient step on the smooth term with the proximal operator of the $\\ell_{1}$ norm.\n\nStart from the following foundational base:\n- The data fidelity term is the squared loss, a smooth function with gradient $\\nabla g(x) = A^{\\top}(A x - b)$ where $g(x) = \\tfrac{1}{2}\\lVert A x - b \\rVert_{2}^{2}$.\n- Use a constant step size $\\eta = 1/L$, where $L$ is any valid Lipschitz constant of $\\nabla g(x)$. For the squared loss, one such valid choice is the largest eigenvalue of $A^{\\top}A$.\n- For convergence checking, use the relative update criterion $\\lVert x^{t+1} - x^{t} \\rVert_{2} / \\max\\{1,\\lVert x^{t} \\rVert_{2}\\} \\leq \\text{tol}$.\n\nImplementation details to follow exactly:\n- Initialization: $x^{0} = 0$ for both methods.\n- Step size: $\\eta = 1/L$ with $L$ taken as the largest eigenvalue of $A^{\\top}A$ computed numerically.\n- Iterative hard-thresholding: after each gradient step $v^{t} = x^{t} - \\eta \\nabla g(x^{t})$, project by keeping the $k$ entries of largest magnitude in $v^{t}$ and setting the rest to $0$.\n- Proximal gradient with soft-thresholding: after each gradient step $v^{t} = x^{t} - \\eta \\nabla g(x^{t})$, apply the component-wise proximal operator of the $\\ell_{1}$ norm with parameter $\\eta \\lambda$.\n- Stopping criterion: stop at the smallest iteration $t$ satisfying the relative update criterion with tolerance $\\text{tol}$, or after reaching $\\text{max\\_iter}$ if the criterion is not met.\n- Final sparsity is defined as the count of indices $i$ with $\\lvert x_{i} \\rvert  \\tau_{\\text{nz}}$ where $\\tau_{\\text{nz}} = 10^{-8}$.\n- Convergence iterations is the number of iterations taken when the stopping criterion first holds (count the final iterate that satisfies the criterion); if not satisfied within the budget, report $\\text{max\\_iter}$.\n\nData generation for each test case:\n- Use a deterministic random number generator seeded as specified. For each case, generate $A$ with independent entries drawn from the standard normal distribution. Generate a ground-truth $x^{\\star}$ with exactly $k$ nonzero entries at indices sampled uniformly without replacement from $\\{1,\\dots,n\\}$. The nonzero values are drawn independently from the standard normal distribution. Generate noise $\\varepsilon$ with independent entries drawn from the normal distribution with zero mean and standard deviation $\\sigma$. Set $b = A x^{\\star} + \\varepsilon$.\n- Angles and physical units are not involved in this problem.\n\nHyperparameters common to all test cases:\n- Maximum iterations $\\text{max\\_iter} = 1000$.\n- Tolerance $\\text{tol} = 10^{-6}$.\n- Nonzero threshold $\\tau_{\\text{nz}} = 10^{-8}$.\n\nTest suite (three cases designed for coverage):\n- Case $1$ (well-conditioned, mild noise): $m = 60$, $n = 200$, $k = 5$, $\\lambda = 0.1$, $\\sigma = 0.01$, seed $= 0$.\n- Case $2$ (underdetermined, noiseless): $m = 40$, $n = 120$, $k = 10$, $\\lambda = 0.05$, $\\sigma = 0$, seed $= 1$.\n- Case $3$ (square system, strong shrinkage): $m = 80$, $n = 80$, $k = 3$, $\\lambda = 1.0$, $\\sigma = 0.02$, seed $= 2$.\n\nYour program must, for each test case, run both methods with the specified settings and report a list with four integers:\n- Iterative hard-thresholding iterations to convergence,\n- Iterative hard-thresholding final sparsity,\n- Soft-thresholding proximal-gradient iterations to convergence,\n- Soft-thresholding proximal-gradient final sparsity.\n\nFinal output format:\n- Aggregate the results for the three cases in order as a single line string representing a list of lists, with no spaces, e.g., $[ [a_{1},b_{1},c_{1},d_{1}], [a_{2},b_{2},c_{2},d_{2}], [a_{3},b_{3},c_{3},d_{3}] ]$ but printed as a single line with no embedded spaces, and square brackets and commas exactly as shown. Replace symbols by the computed integers. Your program should produce exactly one line of output in this format.",
            "solution": "The problem is valid. It presents a well-defined computational task rooted in the established principles of numerical optimization and sparse signal recovery. All necessary parameters, algorithms, and conditions are provided, ensuring the problem is self-contained, consistent, and admits a unique, verifiable solution.\n\nThe core task is to find a sparse vector $x \\in \\mathbb{R}^{n}$ that is an approximate solution to the linear system $Ax = b$, where $A \\in \\mathbb{R}^{m \\times n}$ is the design matrix and $b \\in \\mathbb{R}^{m}$ is the observation vector. We will implement and compare two prominent iterative algorithms for this purpose: Iterative Hard-Thresholding (IHT) and Proximal Gradient Descent (PGD) using a soft-thresholding operator.\n\nBoth algorithms are based on a common iterative structure that combines a gradient descent step on a smooth data fidelity term with a projection or proximal step that enforces sparsity. The data fidelity term is the squared $\\ell_2$-norm loss, $g(x) = \\frac{1}{2}\\lVert A x - b \\rVert_{2}^{2}$. This function is smooth and convex, with a gradient given by $\\nabla g(x) = A^{\\top}(A x - b)$.\n\nFor an iterative algorithm of the form $x^{t+1} \\leftarrow \\text{Step}(x^t)$, a standard choice for the gradient descent update is $v^{t} = x^{t} - \\eta \\nabla g(x^{t})$, where $\\eta  0$ is the step size. For convergence, $\\eta$ must be chosen appropriately. A common strategy, which we will adopt, is to set $\\eta = 1/L$, where $L$ is the Lipschitz constant of the gradient $\\nabla g(x)$. For the given loss function, the gradient $\\nabla g(x)$ is Lipschitz continuous with constant $L = \\lambda_{\\max}(A^{\\top}A)$, the largest eigenvalue of the matrix $A^{\\top}A$. This choice guarantees the monotonic decrease of the objective function in PGD and is a standard choice for IHT.\n\n**Method 1: Iterative Hard-Thresholding (IHT)**\n\nIHT seeks to solve the $\\ell_0$-constrained optimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2}\\lVert A x - b \\rVert_{2}^{2} \\quad \\text{subject to} \\quad \\lVert x \\rVert_{0} \\leq k\n$$\nHere, $\\lVert x \\rVert_{0}$ is the $\\ell_0$ \"norm,\" which counts the number of non-zero elements in $x$, and $k$ is the desired level of sparsity. Since this problem is non-convex and computationally hard, IHT provides an approximate solution via a simple iterative scheme. Starting with an initial guess $x^0$, each iteration consists of two steps:\n1.  A standard gradient descent step on the data fidelity term: $v^{t} = x^{t} - \\eta \\nabla g(x^{t}) = x^{t} - \\eta A^{\\top}(A x^{t} - b)$.\n2.  A projection step, which enforces the sparsity constraint. This is achieved by applying a hard-thresholding operator, $H_k(\\cdot)$, to the result of the gradient step. The operator $H_k(v)$ keeps the $k$ components of $v$ with the largest absolute values and sets all other components to zero.\n\nThe complete IHT update rule is:\n$$\nx^{t+1} = H_k(x^{t} - \\eta A^{\\top}(A x^{t} - b))\n$$\n\n**Method 2: Proximal Gradient Descent (PGD) with Soft-Thresholding**\n\nPGD addresses the $\\ell_1$-regularized problem, a convex relaxation of the $\\ell_0$ problem, also known as the LASSO (Least Absolute Shrinkage and Selection Operator) problem:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2}\\lVert A x - b \\rVert_{2}^{2} + \\lambda \\lVert x \\rVert_{1} \\right\\}\n$$\nHere, $\\lambda  0$ is a regularization parameter that controls the trade-off between data fidelity and sparsity, with larger values of $\\lambda$ promoting sparser solutions. The objective function is a sum of a smooth term, $g(x) = \\frac{1}{2}\\lVert A x - b \\rVert_{2}^{2}$, and a non-smooth but convex term, $h(x) = \\lambda \\lVert x \\rVert_{1}$. PGD is designed to solve such composite optimization problems. Each iteration involves:\n1.  A gradient descent step on the smooth term $g(x)$: $v^{t} = x^{t} - \\eta \\nabla g(x^{t})$.\n2.  A proximal step, which involves applying the proximal operator of the non-smooth term $h(x)$ to the intermediate vector $v^t$. The proximal operator of the $\\ell_1$ norm is the soft-thresholding operator, $S_{\\alpha}(\\cdot)$, defined component-wise as:\n    $$\n    (S_{\\alpha}(v))_i = \\text{sgn}(v_i) \\max(|v_i| - \\alpha, 0)\n    $$\n    The parameter for the operator is $\\alpha = \\eta\\lambda$.\n\nThe complete PGD update rule, also known as the Iterative Shrinkage-Thresholding Algorithm (ISTA), is:\n$$\nx^{t+1} = S_{\\eta\\lambda}(x^{t} - \\eta A^{\\top}(A x^{t} - b))\n$$\n\n**Implementation and Evaluation**\n\nFor each test case, we will first generate the data $(A, b)$ based on a ground truth sparse vector $x^{\\star}$ as specified. We will then compute the Lipschitz constant $L = \\lambda_{\\max}(A^{\\top}A)$ numerically and set the step size $\\eta = 1/L$. Both algorithms will be initialized with $x^0 = 0$. The iterations will proceed until the relative update criterion, $\\lVert x^{t+1} - x^{t} \\rVert_{2} / \\max\\{1, \\lVert x^{t} \\rVert_{2}\\} \\leq \\text{tol} = 10^{-6}$, is met, or until a maximum of $\\text{max\\_iter} = 1000$ iterations is reached. The number of iterations to convergence and the final sparsity (count of entries with magnitude greater than $\\tau_{\\text{nz}} = 10^{-8}$) will be recorded for both methods.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Iterative Hard-Thresholding (IHT) and\n    Proximal Gradient Descent (PGD) with soft-thresholding for finding\n    sparse solutions to linear inverse problems.\n    \"\"\"\n    \n    # Define hyperparameters common to all test cases.\n    max_iter = 1000\n    tol = 1e-6\n    tau_nz = 1e-8\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (m, n, k, lambda, sigma, seed)\n        (60, 200, 5, 0.1, 0.01, 0),\n        (40, 120, 10, 0.05, 0.0, 1),\n        (80, 80, 3, 1.0, 0.02, 2),\n    ]\n\n    def generate_data(m, n, k, sigma, seed):\n        \"\"\"Generates problem data (A, b) for a given test case.\"\"\"\n        rng = np.random.default_rng(seed)\n        A = rng.standard_normal(size=(m, n))\n        x_star = np.zeros(n)\n        nonzero_indices = rng.choice(n, k, replace=False)\n        x_star[nonzero_indices] = rng.standard_normal(k)\n        noise = rng.normal(loc=0.0, scale=sigma, size=m)\n        b = A @ x_star + noise\n        return A, b\n\n    def iterative_hard_thresholding(A, b, k, eta):\n        \"\"\"Performs the Iterative Hard-Thresholding algorithm.\"\"\"\n        n = A.shape[1]\n        x = np.zeros(n)\n        \n        for t in range(max_iter):\n            grad = A.T @ (A @ x - b)\n            v = x - eta * grad\n            \n            indices_to_keep = np.argsort(np.abs(v))[-k:]\n            x_next = np.zeros(n)\n            x_next[indices_to_keep] = v[indices_to_keep]\n            \n            # Check for convergence\n            denom = max(1.0, np.linalg.norm(x))\n            rel_update = np.linalg.norm(x_next - x) / denom\n            \n            x = x_next\n            \n            if rel_update = tol:\n                num_iters = t + 1\n                sparsity = np.sum(np.abs(x) > tau_nz)\n                return num_iters, int(sparsity)\n        \n        # If convergence criteria is not met within max_iter\n        num_iters = max_iter\n        sparsity = np.sum(np.abs(x) > tau_nz)\n        return num_iters, int(sparsity)\n\n    def proximal_gradient_descent(A, b, lambd, eta):\n        \"\"\"Performs Proximal Gradient Descent with soft-thresholding.\"\"\"\n        n = A.shape[1]\n        x = np.zeros(n)\n        alpha = eta * lambd\n        \n        for t in range(max_iter):\n            grad = A.T @ (A @ x - b)\n            v = x - eta * grad\n            \n            # Soft-thresholding operator\n            x_next = np.sign(v) * np.maximum(np.abs(v) - alpha, 0)\n            \n            # Check for convergence\n            denom = max(1.0, np.linalg.norm(x))\n            rel_update = np.linalg.norm(x_next - x) / denom\n            \n            x = x_next\n\n            if rel_update = tol:\n                num_iters = t + 1\n                sparsity = np.sum(np.abs(x) > tau_nz)\n                return num_iters, int(sparsity)\n        \n        # If convergence criteria is not met within max_iter\n        num_iters = max_iter\n        sparsity = np.sum(np.abs(x) > tau_nz)\n        return num_iters, int(sparsity)\n\n    results = []\n    for m, n, k, lambd, sigma, seed in test_cases:\n        A, b = generate_data(m, n, k, sigma, seed)\n        \n        # Calculate Lipschitz constant and step size\n        # L is the largest eigenvalue of A.T @ A\n        L = np.linalg.eigvalsh(A.T @ A)[-1]\n        eta = 1.0 / L\n        \n        # Run Iterative Hard-Thresholding\n        iht_iters, iht_sparsity = iterative_hard_thresholding(A, b, k, eta)\n        \n        # Run Proximal Gradient Descent\n        pgd_iters, pgd_sparsity = proximal_gradient_descent(A, b, lambd, eta)\n        \n        results.append([iht_iters, iht_sparsity, pgd_iters, pgd_sparsity])\n\n    # Format the final output string without spaces as required.\n    inner_results = [f\"[{','.join(map(str, res_list))}]\" for res_list in results]\n    final_output = f\"[{','.join(inner_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}