## 引言
在深度学习领域，构建能够超越训练数据、在未知数据上表现出色的模型是最终目标。然而，[模型容量](@entry_id:634375)的巨大提升也带来了一个普遍的挑战：过拟合。当模型过度学习训练数据中的噪声和特异性时，其泛化能力会急剧下降。正则化是应对这一挑战的核心策略。本文旨在系统性地剖析正则化这一关键概念，揭示其如何通过约束模型来[防止过拟合](@entry_id:635166)。

我们将通过三个章节的探索，带领读者从理论走向实践。在“原理与机制”一章中，我们将深入挖掘[L2正则化](@entry_id:162880)、Dropout等经典技术的数学本质及其现代变体。随后，在“应用与跨学科联系”一章中，我们将展示正则化思想如何在信号处理、生物信息学等广阔领域中发挥关键作用。最后，“动手实践”部分将提供具体的编码练习，帮助您将理论知识转化为解决实际问题的能力。

## 原理与机制

继前一章对过拟合现象及其对[模型泛化](@entry_id:174365)能力危害的介绍之后，本章将深入探讨正则化的核心原理与机制。正则化并非单一的技术，而是一系列旨在约束[模型复杂度](@entry_id:145563)、防止模型过度拟合训练数据中噪声和特异性的方法的总称。我们将从最经典的正则化形式出发，逐步扩展到更现代、更隐式的正则化策略，并通过一系列精心设计的思想实验和分析，揭示它们在实践中如何以及为何有效。

### 正则化作为对复杂度的惩罚：$L_2$正则化与贝叶斯观点

最直观的[正则化方法](@entry_id:150559)是在模型的[经验风险](@entry_id:633993)（即训练损失）之上，直接添加一个惩罚项。这个惩罚项的设计旨在量化模型的“复杂度”，并通过最小化总[目标函数](@entry_id:267263)来同时优化模型对数据的拟合度和模型的简洁度。

**$L_2$正则化**，又称**[权重衰减](@entry_id:635934)（Weight Decay）**或**[岭回归](@entry_id:140984)（Ridge Regression）**，是这一思想最典型和广泛应用的体现。它通过在[损失函数](@entry_id:634569)中加入模型参数权重的$L_2$范数的平方来实现。对于一个参数为$w$的模型，其带有$L_2$正则化的目标函数$J(w)$通常表示为：

$$J(w) = \mathcal{L}_{emp}(w) + \frac{\lambda}{2} \|w\|_2^2$$

其中，$\mathcal{L}_{emp}(w)$是[经验风险](@entry_id:633993)，例如在$N$个样本上的平均平方误差；$\|w\|_2^2 = \sum_j w_j^2$是权重向量$w$的平方$L_2$范数；$\lambda > 0$是**正则化系数**，它控制着我们对[模型复杂度](@entry_id:145563)的惩罚强度。一个较大的$\lambda$会迫使模型的权重更接近于零，从而得到一个更“简单”的模型。

为了更深刻地理解$L_2$正则化的本质，我们可以从概率的角度出发，揭示它与**[贝叶斯推断](@entry_id:146958)**之间的深刻联系。考虑一个[贝叶斯线性回归](@entry_id:634286)模型，其中我们假设数据由带有[高斯噪声](@entry_id:260752)的线性过程生成，即$y \sim \mathcal{N}(w^{\top}x, \sigma_y^2)$。此外，我们为模型权重$w$设定一个先验分布，该先验分布反映了我们对权重的“信念”。如果我们相信较小的权重更有可能，一个自然的选择是为$w$赋予一个均值为零的球形[高斯先验](@entry_id:749752)，即$w \sim \mathcal{N}(0, \sigma^2 I)$。

根据贝叶斯定理，权重的[后验分布](@entry_id:145605)正比于似然与先验的乘积：$p(w | \mathcal{D}) \propto p(\mathcal{D} | w) p(w)$。在机器学习中，我们常常寻求**最大后验估计（Maximum A Posteriori, MAP）**，即找到最大化[后验概率](@entry_id:153467)的权重$w_{\text{MAP}}$。这等价于最小化负对数后验：

$$-\ln p(w | \mathcal{D}) \propto -\ln p(\mathcal{D} | w) - \ln p(w)$$

将高斯[似然](@entry_id:167119)和[高斯先验](@entry_id:749752)的表达式代入，我们会发现最小化负对数后验等价于最小化以下[目标函数](@entry_id:267263)：

$$J(w) \propto \frac{1}{2\sigma_y^2} \sum_{i=1}^{N} (y_i - w^{\top}x_i)^2 + \frac{1}{2\sigma^2} \|w\|_2^2$$

通过适当的缩放，这个目标函数可以被重写为标准的$L_2$正则化形式$R(w) + \frac{\lambda}{2}\|w\|_2^2$，其中$R(w)$是平均平方误差。通过比较系数，我们可以精确地确定正则化系数$\lambda$与贝叶斯模型超参数之间的关系：$\lambda = \frac{\sigma_y^2}{N\sigma^2}$ 。

这个推导揭示了一个核心观点：**$L_2$正则化可以被视为在模型参数上施加了一个[高斯先验](@entry_id:749752)的[MAP估计](@entry_id:751667)**。正则化系数$\lambda$的大小反映了我们对先验的信任程度（由先验[方差](@entry_id:200758)$\sigma^2$控制）与对数据拟合的信任程度（由噪声[方差](@entry_id:200758)$\sigma_y^2$控制）之间的权衡。

值得注意的是，[MAP估计](@entry_id:751667)只给出了[后验分布](@entry_id:145605)的峰值，即最可能的参数点。一个完整的贝叶斯方法会通过对整个[后验分布](@entry_id:145605)进行积分来做出预测（即后验预测平均）。这样做的好处是，预测结果会包含由[参数不确定性](@entry_id:264387)带来的额外[方差](@entry_id:200758)项。例如，对于一个新输入$x_*$，MAP“插件式”预测的[方差](@entry_id:200758)仅为观测噪声[方差](@entry_id:200758)$\sigma_y^2$，而完整的后验预测[方差](@entry_id:200758)为$\sigma_y^2 + x_*^{\top} \Sigma_w x_*$，其中$\Sigma_w$是权重的[后验协方差矩阵](@entry_id:753631)。这个额外的$x_*^{\top} \Sigma_w x_*$项正体现了模型对自身[参数不确定性](@entry_id:264387)的认知 。尽管如此，[MAP估计](@entry_id:751667)因其计算上的简便性，在[深度学习](@entry_id:142022)中作为一种有效的正则化手段被广泛采用。

### $L_2$正则化的精妙之处与现代观点

尽管$L_2$正则化形式简单，但在现代深度学习实践中，对其正确理解和应用却涉及到一些微妙但至关重要的细节。

#### 尺度不变性问题

一个常被忽视的问题是，标准的$L_2$正则化**不是[尺度不变的](@entry_id:178566)**。这意味着，如果我们将输入特征进行缩放，正则化的效果会发生改变。设想一个线性回归问题，我们将第$j$个特征乘以一个因子$s_j > 0$。为了保持模型的预测不变，相应的权重$w_j$必须除以$s_j$。然而，原始的$L_2$惩罚项$\lambda \sum_j w_j^2$会因此改变。

为了在[特征缩放](@entry_id:271716)后保持正则化效果的一致性，我们需要调整每个权重对应的惩罚系数。一个严谨的分析表明，如果特征被一个[对角矩阵](@entry_id:637782)$S = \operatorname{diag}(s_1, \dots, s_d)$缩放，那么为了保持[优化问题](@entry_id:266749)的解（在$w=Sv$的重参数化下）不变，每个权重$v_j$的正则化系数$\lambda_j$必须与原始的全局系数$\lambda$和缩放因子$s_j$满足关系$\lambda_j = \lambda s_j^2$ 。这意味着，尺度较大的特征（$s_j > 1$）实际上受到了更强的惩罚。这一特性强烈地启示我们，在使用$L_2$正则化时，**对输入特征进行[标准化](@entry_id:637219)**（例如，归一化为零均值和单位[方差](@entry_id:200758)）是一个至关重要的[预处理](@entry_id:141204)步骤。

#### [解耦权重衰减](@entry_id:635953) ([AdamW](@entry_id:163970))

尺度不变性的问题在与**[自适应优化](@entry_id:746259)器（如Adam）**结合时变得更加复杂。Adam等优化器通过记录梯度的一阶和二阶矩来为每个参数计算自适应的[学习率](@entry_id:140210)。在标准的$L_2$正则化下，[目标函数](@entry_id:267263)的梯度包含数据损失的梯度和正则化项的梯度，即$\nabla \mathcal{L}(w) + \lambda w$。[自适应优化](@entry_id:746259)器会用一个对角预条件矩阵$A_t$来缩放这个总梯度。

这导致了一个问题：正则化项的梯度$\lambda w$也被自适应地缩放了。具体来说，对于$L_2$正则化，权重更新中与衰减相关的部分变成了$-\lambda A_t w_t$。由于$A_t$的对角元素通常不相等，这意味着不同权重的衰减率变得不同，且这个衰减率与该权重过去梯度的历史有关。这与最初岭回归中所有权重以相同比例衰减的初衷相去甚远。

**[解耦权重衰减](@entry_id:635953)（Decoupled Weight Decay）**，以其在**[AdamW](@entry_id:163970)**优化器中的应用而闻名，正是为了解决这个问题而提出的。它将[权重衰减](@entry_id:635934)步骤从梯度计算中“解耦”出来。更新规则可以概念化为：
1. 根据数据损失计算梯度更新：$\Delta w_{\text{grad}} = - A_t \nabla \mathcal{L}(w_t)$
2. 应用一个独立的、与$A_t$无关的[权重衰减](@entry_id:635934)：$w_{t+1} = (w_t + \Delta w_{\text{grad}}) - \eta w_t$

在这种形式下，每个权重都以一个统一的速率$\eta$进行衰减，这恢复了$L_2$正则化在SGD等非[自适应优化](@entry_id:746259)器下的行为。在SGD中，$A_t = \alpha I$（其中$\alpha$是[学习率](@entry_id:140210)），$L_2$正则化和[解耦权重衰减](@entry_id:635953)可以通过设置$\eta = \alpha \lambda$来等价。但在Adam中，由于$A_t$的非[均匀性](@entry_id:152612)，两者通常不等价 。

一个具体的例子可以清晰地展示这种差异：假设一个两参数模型，权重$w_t = (1,1)^{\top}$，Adam的预条件矩阵为$A_t = \operatorname{diag}(2\alpha, \alpha/2)$。如果此时损失梯度为零，标准$L_2$正则化会导致第一个坐标的衰减量是第二个的4倍，而[解耦权重衰减](@entry_id:635953)则使两个坐标以相同的量衰减 。这说明[解耦权重衰减](@entry_id:635953)提供了一种更可预测和更符合理论初衷的正则化方式。

#### 参数范数 vs. 函数复杂度

$L_2$正则化的核心假设是，参数范数的大小是[模型复杂度](@entry_id:145563)的良好代理。然而，在深度神经网络中，尤其是在使用如ReLU这样的正齐次[激活函数](@entry_id:141784)的网络中，这种关系可能被打破。

考虑一个简单的[ReLU网络](@entry_id:637021)$f(x) = a \cdot \sigma(bx)$，其中$\sigma(\cdot)$是[ReLU函数](@entry_id:273016)。由于ReLU的[正齐次性](@entry_id:262235)（$\sigma(cz) = c\sigma(z)$ for $c \ge 0$），对于任何$s > 0$，我们可以将参数$(a,b)$重缩放为$(a/s, sb)$，而函数本身对于正输入$x$保持不变：$(a/s) \cdot \sigma((sb)x) = (a/s) \cdot s \cdot \sigma(bx) = a \cdot \sigma(bx)$。

然而，$L_2$权重惩罚项$a^2+b^2$在这种重缩放下却不是不变的。这意味着，对于计算完全相同函数的一族无限多的参数$(a/s, sb)$，正则化项的值可以任意大或小。因此，在这种情况下，权重范数惩罚的不再是函数的内在复杂度，而仅仅是参数的特定分解形式。相比之下，如果我们对模型的输出进行$L_2$正则化，例如$\sum_i (f(x_i))^2$，这个惩罚项只依赖于函数在数据点上的输出值，因此它在这种缩放变换下是保持不变的。一个具体的计算示例可以表明，这两种不同的正则化策略（权重范数惩罚和输出范数惩罚）确实会导致学习到不同的最优函数 。

这个例子深刻地揭示了，在深度网络中，直接惩罚权重范数可能是一种启发式方法，其有效性依赖于[网络结构](@entry_id:265673)和激活函数的性质。理解这些尺度不变性是理解更高级[正则化技术](@entry_id:261393)（如权重标准化或[谱归一化](@entry_id:637347)）的关键。

### 通过随机性进行正则化：Dropout

除了在目标函数中添加显式惩罚项，我们还可以通过在训练过程中引入随机性来正则化模型。**Dropout**是这种思想最成功的范例。

在训练期间，Dropout以某个概率$p$（**保持概率**）保留一个神经元的激活值，并以$1-p$的概率将其输出设置为零。为了在测试时保持激活值的期望尺度不变，一种常见的做法是“倒置Dropout”（inverted dropout），即在训练时将保留下来的激活值乘以$1/p$。在测试阶段，所有神经元都被保留（即不进行Dropout），以使用模型的全部能力进行预测。

直观上看，Dropout迫使网络不能过度依赖于任何一个单一的特征或神经元，因为它随时可能被“丢弃”。这鼓励网络学习更加鲁棒和[分布](@entry_id:182848)式的特征表示。

我们可以通过[数学分析](@entry_id:139664)来更精确地理解Dropout的正则化效应。考虑一个简单的线性模型，其输出层的预测为$\hat{y} = w^{\top}a(x)$，其中$a(x)$是来自前一层的激活向量。如果在$a(x)$上应用Dropout，那么在训练时的随机预测变为$\hat{y}_{\text{do}} = w^{\top} (z \odot a(x))$，其中$z$是一个随机的伯努利掩码向量，其元素以概率$p$为$1/p$，以概率$1-p$为$0$。

当我们最小化关于数据和随机掩码$z$的期望平方损失$\mathbb{E}[(y-\hat{y}_{\text{do}})^2]$时，可以证明这个期望损失能够被分解为两部分：

$$\mathbb{E}[(y-\hat{y}_{\text{do}})^2] = \mathbb{E}[(y - w^{\top}a(x))^2] + \frac{1-p}{p} \mathbb{E}[\sum_j w_j^2 a_j(x)^2]$$

第一项是没有Dropout时的确定性风险。第二项是由于引入噪声而产生的额外项，它完全可以被看作是对权重$w$的正则化惩罚。在某些简化假设下（例如，激活值$a(x)$具有零均值和单位协[方差](@entry_id:200758)），这个惩罚项可以被精确地简化为$\frac{1-p}{p}\|w\|_2^2$ 。

这个结果非常具有启发性：**Dropout在训练过程中引入的随机性，其期望效果等价于对权重施加了一个$L_2$惩罚**。这个惩罚的强度由保持概率$p$决定，当$p$减小时，正则化强度$\frac{1-p}{p}$增大。更进一步的分析表明，这个等效的$L_2$惩罚是“自适应的”，其强度还依赖于输入的二阶矩（[方差](@entry_id:200758)）。例如，对于一个单变量模型$y=wx$，若在输入$x$上应用Dropout，等效的$L_2$惩罚项为$w^2 \sigma_x^2 \frac{1-p}{p}$，其中$\sigma_x^2$是输入的[方差](@entry_id:200758)。这表明Dropout对与[方差](@entry_id:200758)较大（[信息量](@entry_id:272315)可能也更大）的输入相关的权重施加了更强的惩罚 。

### 通过数据操控进行正则化：[数据增强](@entry_id:266029)与[标签平滑](@entry_id:635060)

另一大类[正则化技术](@entry_id:261393)直接作用于训练数据本身，通过创造新的“虚拟”训练样本或修改标签来引导模型学习更具泛化性的特征。

#### [数据增强](@entry_id:266029) (Data Augmentation)

**[数据增强](@entry_id:266029)**通过对现有训练样本应用各种变换来生成新的训练数据。例如，在图像[分类任务](@entry_id:635433)中，常见的增强技术包括随机裁剪、旋转、翻转和颜色[抖动](@entry_id:200248)。其基本假设是，这些变换不应改变样本的真实标签。这种通过变换来扩充数据集的方法，可以有效地将我们关于数据[不变性](@entry_id:140168)的先验知识注入到模型中。

然而，应用[数据增强](@entry_id:266029)时必须遵守一个核心原则：**变换必须是标签不变的**。如果一个增强变换系统性地将一个属于A类的样本变成了看起来像B类的样本，但标签仍然是A，那么它就会向模型注入错误的监督信号，从而损害而不是提升性能。

一个简单的思想实验可以揭示其危害。考虑一个一维[二元分类](@entry_id:142257)问题，真实标签由$y = \mathbb{1}\{x \ge 0\}$决定。假设我们应用一个增强变换$T(x) = -x$，但保持标签$y$不变。这个变换显然违反了标签不变性，因为对于$x>0$, $y=1$，但其变换后的输入$-x$的真实标签应该是$0$。如果以概率$\alpha$应用这种错误的增强，可以证明，当$\alpha > 1/2$时，增强后的训练数据集上的[贝叶斯最优分类器](@entry_id:164732)将变为$f^*(x) = \mathbb{1}\{x  0\}$——一个与真实决策边界完全相反的分类器。这会导致在原始（未增强的）[测试集](@entry_id:637546)上获得灾难性的性能 。

**Mixup**是一种更高级的[数据增强](@entry_id:266029)技术，它通过对两个随机抽取的样本$(x_i, y_i)$和$(x_j, y_j)$进行[线性插值](@entry_id:137092)来创造新的虚拟样本：
$$(\tilde{x}, \tilde{y}) = (\lambda x_i + (1-\lambda) x_j, \lambda y_i + (1-\lambda) y_j)$$
其中$\lambda$通常从一个对称的Beta[分布](@entry_id:182848)中采样。Mixup鼓励模型在其训练样本之间的空间中表现出线性的行为，这可以被看作是一种平滑约束，从而降低了模型对训练数据中[对抗性扰动](@entry_id:746324)的敏感性。

从**偏见-[方差](@entry_id:200758)权衡**的角度分析Mixup，我们可以获得更深的理解。通过鼓励平滑，Mixup限制了模型的复杂度，使其更难拟合训练数据中的噪声，从而有效地**降低了模型的[方差](@entry_id:200758)**。然而，由于真实的底层函数$f^*$不一定是线性的，这种强加的线性假设也**引入了模型的偏见**。

这种权衡关系启发了一种动态的训练策略：**正则化强度退火**。在训练早期，模型非常灵活，[方差](@entry_id:200758)是主要问题，因此使用较强的Mixup（即$\lambda$更集中于$0.5$附近）是有益的。随着训练的进行，模型逐渐收敛，此时偏见成为限制性能的主要因素。因此，我们应逐渐减弱Mixup的强度（让$\lambda$更多地取接近$0$或$1$的值），甚至在训练的最后阶段完全关闭它。这使得模型能够在[后期](@entry_id:165003)专注于减少偏见，学习数据中更精细的结构，从而在不牺牲早期[方差](@entry_id:200758)控制的情况下，达到更好的最终性能 。

#### [标签平滑](@entry_id:635060) (Label Smoothing)

**[标签平滑](@entry_id:635060)**是一种作用于标签的[正则化技术](@entry_id:261393)。在多[分类问题](@entry_id:637153)中，训练数据通常使用“one-hot”编码，即正确类别的标签为$1$，其余为$0$。这种硬标签鼓励模型产生极端自信的预测，即让对应正确类别的logit（softmax前的输入）趋向于正无穷。这可能导致[模型校准](@entry_id:146456)不佳（即其预测的置信度不能反映真实的正确率）和对扰动敏感。

[标签平滑](@entry_id:635060)通过将硬标签替换为软标签来缓解这一问题。它将one-hot标签$y$与一个[均匀分布](@entry_id:194597)$u$进行[凸组合](@entry_id:635830)：$q = (1-\epsilon)y + \epsilon u$，其中$\epsilon$是一个小的平滑参数。例如，对于一个$K$[分类问题](@entry_id:637153)，正确类别的标签从$1$变为$1-\epsilon+\epsilon/K$，而其他类别的标签从$0$变为$\epsilon/K$。

这种简单的修改具有多重深刻的正则化效应 ：
1.  **从损失函数的角度**：使用平滑标签的[交叉熵损失](@entry_id:141524)，可以被分解为原始[交叉熵损失](@entry_id:141524)和一个正则项的加权和：$L_{LS} = (1-\epsilon)H(y,p) + \epsilon H(u,p)$。这等价于在原始损失上添加了一个惩罚模型[预测分布](@entry_id:165741)$p$与[均匀分布](@entry_id:194597)$u$之间[KL散度](@entry_id:140001)的项，即$\epsilon \cdot \mathrm{KL}(u \| p)$。
2.  **从梯度的角度**：对于正确类别$c$的logit $z_c$，其梯度从$p_c-1$变为$p_c-q_c = p_c - (1-\epsilon+\epsilon/K)$。由于目标$q_c$严格小于$1$，梯度在$p_c = q_c$时为零，这阻止了$z_c$被推向无穷大，从而抑制了模型的过度自信。
3.  **从对logit的先验角度**：附加的正则项$\epsilon H(u,p)$可以被看作是对logits向量$z$施加了一个先验。这个先验惩罚项是一个[凸函数](@entry_id:143075)，其最小值在所有logits都相等时取到。因此，[标签平滑](@entry_id:635060)鼓励logits的[分布](@entry_id:182848)更加集中，[方差](@entry_id:200758)更小，从而产生更“软”的预测[概率分布](@entry_id:146404)。

总的来说，[标签平滑](@entry_id:635060)通过阻止模型产生过于绝对的预测，提高了模型的**校准**水平和泛化能力。然而，在类别极度不平衡的数据集上，它也可能因为削弱了对少数类的监督信号而轻微影响其准确率。

### 通过训练过程进行正则化：[早停](@entry_id:633908)

最后一类重要的[正则化技术](@entry_id:261393)并非修改[损失函数](@entry_id:634569)或数据，而是直接干预训练过程本身。**[早停](@entry_id:633908)（Early Stopping）**是其中最基本也是最有效的方法之一。

[深度学习模型](@entry_id:635298)通常具有极高的容量，如果训练时间足够长，它们几乎总能完美地记住训练数据，包括其中的噪声，从而导致严重的过拟合。训练过程可以看作是在一个巨大的[假设空间](@entry_id:635539)中进行搜索。训练的 epoch 数越多，模型探索的函数就越复杂。[早停](@entry_id:633908)的基本思想是，在模型于[验证集](@entry_id:636445)上的性能开始下降时（即开始[过拟合](@entry_id:139093)时）就停止训练。这相当于通过限制优化算法的运行步数，来隐式地限制了模型的[有效容量](@entry_id:748806)。

[早停](@entry_id:633908)的关键挑战在于如何确定“最佳”的停止时机。这是一个[启发式](@entry_id:261307)的问题，因为我们只能通过有限且可能带有噪声的信号来做出判断。主要存在两种策略 ：
1.  **基于[验证集](@entry_id:636445)性能的准则**：这是最常见的方法。我们监控模型在独立[验证集](@entry_id:636445)上的损失。当验证损失在某个“耐心期”（patience）内不再有显著改善时，就停止训练。这种方法的优点是它直接衡量了我们关心的泛化性能。然而，它的一个主要缺点是验证损失的测量值本身可能是有噪声的，尤其是在[验证集](@entry_id:636445)较小或评估过程随机性较强（如使用了Dropout评估）的情况下。过大的噪声可能导致过早或过晚的停止。
2.  **基于训练动态的准则**：这种方法不依赖[验证集](@entry_id:636445)，而是监控优化过程本身的指标。一个常见的指标是训练梯度的范数。在典型的训练过程中，随着模型接近损失函数的平坦区域，梯度的范数会逐渐减小。因此，我们可以设定一个规则，当梯度的移动平均值持续低于某个阈值时，就认为训练已经收敛并停止。这种方法的优点是信号可能比嘈杂的验证损失更稳定，但它只是泛化性能的一个间接代理，其阈值等超参数可能难以设定。

通过一个受控的模拟实验，我们可以观察到这两种策略的权衡。在[验证集](@entry_id:636445)噪声较低时，基于验证损失的策略通常能更准确地找到最佳停止点。然而，当[验证集](@entry_id:636445)噪声非常大时，其性能会显著下降，甚至可能不如一个经过良好调优的、更稳定的基于梯度范数的策略。这表明，在实践中选择哪种[早停](@entry_id:633908)策略，需要考虑具体问题的特性，特别是验证信号的可靠性。

总而言之，从显式的$L_2$惩罚到隐式的[早停](@entry_id:633908)，正则化的原理与机制多种多样，但它们共同的目标都是通过引入某种形式的约束或先验知识，来引导模型在拟合训练数据与保持简洁以实现良好泛化之间取得理想的平衡。