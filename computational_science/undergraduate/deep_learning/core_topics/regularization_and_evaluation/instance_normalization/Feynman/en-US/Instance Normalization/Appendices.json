{
    "hands_on_practices": [
        {
            "introduction": "To truly understand Instance Normalization (IN), we begin with its core mathematical function: transforming the stylistic features of an image. This exercise guides you through the fundamental algebra of IN, showing how its learnable affine parameters, $\\gamma$ and $\\beta$, can be set to precisely map a feature map's mean and standard deviation to match a target style . Mastering this derivation is the first step toward understanding how neural networks can manipulate and transfer artistic styles from one image to another.",
            "id": "3138582",
            "problem": "Consider a single-channel feature map in a Convolutional Neural Network (CNN) for an image of spatial dimensions $H \\times W$. Instance Normalization (IN) computes per-instance, per-channel statistics across the spatial grid. Let the source feature map be denoted by $\\{x_{ij}\\}_{i=1,\\dots,H;\\, j=1,\\dots,W}$ with source mean $\\mu_{s}$ and source standard deviation $\\sigma_{s}$ across the spatial dimensions, where the mean and standard deviation are defined by\n$$\n\\mu_{s} \\triangleq \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_{ij}, \\quad \\sigma_{s} \\triangleq \\sqrt{\\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\left(x_{ij} - \\mu_{s}\\right)^{2}},\n$$\nand assume $\\sigma_{s} > 0$. Suppose we wish to produce a transformed feature map\n$$\ny_{ij} \\triangleq \\gamma \\cdot \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}} + \\beta\n$$\nwhose mean and standard deviation across $(H,W)$ equal specified target style statistics $\\mu_{t}$ and $\\sigma_{t}$, respectively, with $\\sigma_{t} > 0$.\n\nUsing only the core definitions of mean, variance, and standard deviation, determine the values of the affine parameters $(\\gamma, \\beta)$ that achieve target mean $\\mu_{t}$ and target standard deviation $\\sigma_{t}$ across the $(H,W)$ spatial dimensions. For uniqueness, select the positive scaling solution for $\\gamma$. Express your final answer as a closed-form analytic expression for $(\\gamma, \\beta)$.",
            "solution": "The user wants to determine the affine parameters $(\\gamma, \\beta)$ for an Instance Normalization (IN) transformation. The transformation is defined as:\n$$\ny_{ij} \\triangleq \\gamma \\cdot \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}} + \\beta\n$$\nwhere $\\{x_{ij}\\}$ is the source feature map with mean $\\mu_s$ and standard deviation $\\sigma_s$. The goal is for the transformed feature map $\\{y_{ij}\\}$ to have a target mean $\\mu_t$ and a target standard deviation $\\sigma_t$. We are given that $\\sigma_s > 0$ and $\\sigma_t > 0$.\n\nFirst, let us define a standardized variable $\\hat{x}_{ij}$ as the normalized source feature:\n$$\n\\hat{x}_{ij} \\triangleq \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}}\n$$\nThe transformation can now be written more compactly as:\n$$\ny_{ij} = \\gamma \\hat{x}_{ij} + \\beta\n$$\nOur strategy is to first compute the mean and standard deviation of the standardized feature map $\\{\\hat{x}_{ij}\\}$ and then use these properties to find the values of $\\gamma$ and $\\beta$ that satisfy the target conditions for $\\{y_{ij}\\}$.\n\nLet us calculate the mean of $\\{\\hat{x}_{ij}\\}$, which we denote as $\\mu_{\\hat{x}}$.\n$$\n\\mu_{\\hat{x}} = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\hat{x}_{ij}\n$$\nSubstituting the definition of $\\hat{x}_{ij}$:\n$$\n\\mu_{\\hat{x}} = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\left( \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}} \\right)\n$$\nUsing the linearity of summation, we can separate the terms:\n$$\n\\mu_{\\hat{x}} = \\frac{1}{\\sigma_{s}} \\left( \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_{ij} - \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\mu_{s} \\right)\n$$\nBy definition, $\\mu_s = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_{ij}$. The second term simplifies to $\\frac{1}{HW} (HW \\cdot \\mu_s) = \\mu_s$.\n$$\n\\mu_{\\hat{x}} = \\frac{1}{\\sigma_{s}} (\\mu_{s} - \\mu_{s}) = 0\n$$\nThus, the standardized feature map has a mean of $0$.\n\nNext, let us calculate the variance of $\\{\\hat{x}_{ij}\\}$, denoted as $\\sigma_{\\hat{x}}^2$.\n$$\n\\sigma_{\\hat{x}}^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (\\hat{x}_{ij} - \\mu_{\\hat{x}})^2\n$$\nSince $\\mu_{\\hat{x}} = 0$, this simplifies to:\n$$\n\\sigma_{\\hat{x}}^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\hat{x}_{ij}^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\left( \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}} \\right)^2\n$$\n$$\n\\sigma_{\\hat{x}}^2 = \\frac{1}{\\sigma_{s}^2} \\left( \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (x_{ij} - \\mu_{s})^2 \\right)\n$$\nThe term in the parenthesis is the definition of the source variance, $\\sigma_{s}^2$.\n$$\n\\sigma_{\\hat{x}}^2 = \\frac{1}{\\sigma_{s}^2} (\\sigma_{s}^2) = 1\n$$\nThe standard deviation of $\\{\\hat{x}_{ij}\\}$ is $\\sigma_{\\hat{x}} = \\sqrt{1} = 1$. The standardized feature map has a mean of $0$ and a standard deviation of $1$.\n\nNow, we can determine the parameters $\\gamma$ and $\\beta$ by imposing the target statistics on $y_{ij} = \\gamma \\hat{x}_{ij} + \\beta$.\n\nThe mean of the target feature map, $\\mu_y$, must equal $\\mu_t$.\n$$\n\\mu_y = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} y_{ij} = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (\\gamma \\hat{x}_{ij} + \\beta)\n$$\nUsing the linearity of summation again:\n$$\n\\mu_y = \\gamma \\left( \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\hat{x}_{ij} \\right) + \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\beta\n$$\n$$\n\\mu_y = \\gamma \\mu_{\\hat{x}} + \\beta\n$$\nSubstituting $\\mu_{\\hat{x}} = 0$:\n$$\n\\mu_y = \\gamma (0) + \\beta = \\beta\n$$\nWe require $\\mu_y = \\mu_t$, so we find the value for $\\beta$:\n$$\n\\beta = \\mu_t\n$$\n\nThe standard deviation of the target feature map, $\\sigma_y$, must equal $\\sigma_t$. We first compute the variance, $\\sigma_y^2$.\n$$\n\\sigma_y^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (y_{ij} - \\mu_y)^2\n$$\nSubstituting $y_{ij} = \\gamma \\hat{x}_{ij} + \\beta$ and $\\mu_y = \\beta$:\n$$\n\\sigma_y^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} ((\\gamma \\hat{x}_{ij} + \\beta) - \\beta)^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (\\gamma \\hat{x}_{ij})^2\n$$\n$$\n\\sigma_y^2 = \\gamma^2 \\left( \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\hat{x}_{ij}^2 \\right)\n$$\nThe term in the parenthesis is the variance of $\\{\\hat{x}_{ij}\\}$, which is $\\sigma_{\\hat{x}}^2 = 1$.\n$$\n\\sigma_y^2 = \\gamma^2 \\sigma_{\\hat{x}}^2 = \\gamma^2 (1) = \\gamma^2\n$$\nThe standard deviation $\\sigma_y$ is the square root of the variance:\n$$\n\\sigma_y = \\sqrt{\\gamma^2} = |\\gamma|\n$$\nWe require $\\sigma_y = \\sigma_t$. This gives the condition:\n$$\n|\\gamma| = \\sigma_t\n$$\nSince $\\sigma_t > 0$, this yields two possible solutions: $\\gamma = \\sigma_t$ or $\\gamma = -\\sigma_t$. The problem statement specifies selecting the positive scaling solution for $\\gamma$. Therefore, we must choose:\n$$\n\\gamma = \\sigma_t\n$$\n\nThe affine parameters that achieve the target statistics are $(\\gamma, \\beta) = (\\sigma_t, \\mu_t)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sigma_{t} & \\mu_{t}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from theory to practice, this exercise tackles the challenge of implementing Instance Normalization within the multi-dimensional tensor frameworks used in deep learning. A correct implementation hinges on carefully managing tensor axes to ensure statistics are computed on a per-instance, per-channel basis. By building both a correct and a deliberately buggy version of IN, you will develop a practical understanding of broadcasting mechanics and learn to create tests that catch subtle but critical implementation errors .",
            "id": "3138633",
            "problem": "You are asked to implement and test the broadcasting semantics of Instance Normalization (IN) in a four-dimensional tensor setting that is standard in deep learning. Work with a feature tensor $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$, where $B$ is the batch size, $C$ is the number of channels, and $H$, $W$ are spatial dimensions. The goal is to implement a correct per-instance, per-channel normalization and to expose a common implementation pitfall where broadcasting mixes channels by using a per-instance mean across all channels.\n\nStart from the following fundamental base:\n- For any finite set of real numbers, the sample mean is the average of the values, and the sample variance is the average of squared deviations from the mean.\n- For $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$, it is meaningful to compute statistics over the spatial axes for each fixed $(b,c)$ pair. Use a small positive constant $\\epsilon$ added inside the square root to ensure numerical stability.\n\nTasks:\n1. Implement a function that, for each batch index $b \\in \\{0,\\dots,B-1\\}$ and channel index $c \\in \\{0,\\dots,C-1\\}$, computes a mean $\\mu_{b,c} \\in \\mathbb{R}$ and variance $\\sigma^2_{b,c} \\in \\mathbb{R}$ over the spatial dimensions $\\{0,\\dots,H-1\\} \\times \\{0,\\dots,W-1\\}$ only, broadcasts them to shape $\\mathbb{R}^{B \\times C \\times 1 \\times 1}$, and uses them to normalize $X$ and then apply a per-channel affine transform specified by $\\gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$. The normalization must strictly be per-instance, per-channel over spatial dimensions, and the affine parameters must be broadcast over the correct axes, never mixing channels.\n2. Implement a deliberately buggy variant that computes a per-instance mean and variance across all channels and spatial dimensions for each batch index $b$, namely $\\tilde{\\mu}_{b} \\in \\mathbb{R}$ and $\\tilde{\\sigma}^2_{b} \\in \\mathbb{R}$ with shapes $\\mathbb{R}^{B \\times 1 \\times 1 \\times 1}$, broadcasts these across channels, and then applies the same per-channel affine transform. This simulates the broadcasting pitfall that mixes channels by using $\\tilde{\\mu}_{b}$ and $\\tilde{\\sigma}^2_{b}$ in place of $\\mu_{b,c}$ and $\\sigma^2_{b,c}$.\n3. For each test case below, compute the maximum absolute difference between the correctly normalized output and the buggy normalized output. Use $\\epsilon = 10^{-5}$ and ensure all computations are performed in real-valued arithmetic. The affine parameters must broadcast per channel, not per batch nor per spatial location.\n\nDefinitions (do not implement formulas directly here; derive them from the definitions in your solution):\n- For fixed $(b,c)$, the mean over spatial axes is the average of the $H \\times W$ entries $X_{b,c,h,w}$ as $(h,w)$ ranges over all spatial indices.\n- For fixed $(b,c)$, the variance over spatial axes is the average of the squared deviations of $X_{b,c,h,w}$ from the mean for $(h,w)$ over all spatial indices.\n- For the buggy variant, the mean for batch index $b$ is the average of all $C \\times H \\times W$ entries in $X_{b,:,:,:}$, and the variance for batch index $b$ is the average of their squared deviations.\n\nTest Suite:\n- Case A (happy path, different channel statistics):\n  - Dimensions: $B=2$, $C=2$, $H=2$, $W=2$.\n  - Tensor $X$ is specified by:\n    $$X_{0,0,:,:} = \\begin{bmatrix} 0 & 2 \\\\ 4 & 6 \\end{bmatrix}, \\quad X_{0,1,:,:} = \\begin{bmatrix} -6 & -4 \\\\ -2 & 0 \\end{bmatrix},$$\n    $$X_{1,0,:,:} = \\begin{bmatrix} 10 & 12 \\\\ 14 & 16 \\end{bmatrix}, \\quad X_{1,1,:,:} = \\begin{bmatrix} -10 & -12 \\\\ -14 & -16 \\end{bmatrix}.$$\n  - Affine parameters: $\\gamma = [1, 1]$, $\\beta = [0, 0]$.\n- Case B (edge case, one batch, three channels, one spatial dimension degenerate):\n  - Dimensions: $B=1$, $C=3$, $H=1$, $W=4$.\n  - Tensor $X$ is specified by:\n    $$X_{0,0,:,:} = \\begin{bmatrix} 1 & 2 & 3 & 4 \\end{bmatrix}, \\quad X_{0,1,:,:} = \\begin{bmatrix} 10 & 10 & 10 & 10 \\end{bmatrix},$$\n    $$X_{0,2,:,:} = \\begin{bmatrix} -1 & -2 & -3 & -4 \\end{bmatrix}.$$\n  - Affine parameters: $\\gamma = [1, 0.5, 2]$, $\\beta = [0, 1, -1]$.\n- Case C (boundary where buggy broadcasting is harmless because there is only one channel):\n  - Dimensions: $B=1$, $C=1$, $H=2$, $W=2$.\n  - Tensor $X$ is specified by:\n    $$X_{0,0,:,:} = \\begin{bmatrix} 5 & 7 \\\\ 9 & 11 \\end{bmatrix}.$$\n  - Affine parameters: $\\gamma = [1]$, $\\beta = [0]$.\n- Case D (extreme case, single spatial element per channel):\n  - Dimensions: $B=2$, $C=3$, $H=1$, $W=1$.\n  - Tensor $X$ is specified by:\n    $$X_{0,0,0,0} = 0, \\quad X_{0,1,0,0} = 10, \\quad X_{0,2,0,0} = -10,$$\n    $$X_{1,0,0,0} = 5, \\quad X_{1,1,0,0} = 15, \\quad X_{1,2,0,0} = -5.$$\n  - Affine parameters: $\\gamma = [1, 1, 1]$, $\\beta = [0, 0, 0]$.\n\nOutput Specification:\n- Your program should produce a single line of output containing the maximum absolute differences for Cases A, B, C, and D, in this order, as a comma-separated list enclosed in square brackets. For example, the output format must be exactly like:\n- \"Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., [resultA,resultB,resultC,resultD]).\"\n- Each result must be a floating-point number.\n\nNo physical units or angle units are involved in this problem. All numbers are pure real values. Ensure scientific realism and correct broadcasting semantics per the definitions given above; do not mix channels when computing per-channel statistics. Use $\\epsilon = 10^{-5}$ for all computations.",
            "solution": "The problem requires the implementation and comparison of two variants of normalization for a four-dimensional tensor $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$, where $B$ is the batch size, $C$ is the number of channels, and $H, W$ are spatial dimensions. One variant is the correct Instance Normalization (IN), and the other is a deliberately buggy version that incorrectly computes statistics across channels.\n\nLet the input tensor be $X$ with dimensions $(B, C, H, W)$. Let $\\gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$ be the per-channel learnable affine parameters (scale and shift). A small constant $\\epsilon = 10^{-5}$ is used for numerical stability.\n\nFirst, we define the correct Instance Normalization procedure. For each instance $b \\in \\{0, \\dots, B-1\\}$ and each channel $c \\in \\{0, \\dots, C-1\\}$, the mean $\\mu_{b,c}$ and variance $\\sigma^2_{b,c}$ are computed independently over the spatial dimensions $(H, W)$. The number of spatial elements is $N_{sp} = H \\times W$.\n\nThe mean for instance $b$ and channel $c$ is:\n$$ \\mu_{b,c} = \\frac{1}{N_{sp}} \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} X_{b,c,h,w} $$\nThe variance for instance $b$ and channel $c$ is:\n$$ \\sigma^2_{b,c} = \\frac{1}{N_{sp}} \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} (X_{b,c,h,w} - \\mu_{b,c})^2 $$\nUsing these statistics, the tensor $X$ is normalized. Note that for each element $X_{b,c,h,w}$, the corresponding $\\mu_{b,c}$ and $\\sigma^2_{b,c}$ are used. Computationally, this is achieved by broadcasting the $\\mu$ and $\\sigma^2$ tensors, which have an effective shape of $\\mathbb{R}^{B \\times C \\times 1 \\times 1}$, over the full $\\mathbb{R}^{B \\times C \\times H \\times W}$ tensor. The normalized tensor $\\hat{X}$ is given by:\n$$ \\hat{X}_{b,c,h,w} = \\frac{X_{b,c,h,w} - \\mu_{b,c}}{\\sqrt{\\sigma^2_{b,c} + \\epsilon}} $$\nFinally, the per-channel affine transformation is applied. The scale $\\gamma_c$ and shift $\\beta_c$ are broadcast across the batch and spatial dimensions. The correct final output $Y$ is:\n$$ Y_{b,c,h,w} = \\gamma_c \\hat{X}_{b,c,h,w} + \\beta_c $$\n\nSecond, we define the buggy normalization procedure. This variant simulates a common implementation error where statistics are computed across channels instead of per-channel. For each instance $b \\in \\{0, \\dots, B-1\\}$, a single mean $\\tilde{\\mu}_{b}$ and variance $\\tilde{\\sigma}^2_{b}$ are computed over all channels and spatial dimensions $(C, H, W)$. The number of elements per instance is $N_{all} = C \\times H \\times W$.\n\nThe per-instance mean is:\n$$ \\tilde{\\mu}_{b} = \\frac{1}{N_{all}} \\sum_{c=0}^{C-1} \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} X_{b,c,h,w} $$\nThe per-instance variance is:\n$$ \\tilde{\\sigma}^2_{b} = \\frac{1}{N_{all}} \\sum_{c=0}^{C-1} \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} (X_{b,c,h,w} - \\tilde{\\mu}_{b})^2 $$\nThese statistics, with effective shape $\\mathbb{R}^{B \\times 1 \\times 1 \\times 1}$, are then incorrectly broadcast across all channels for normalization. The incorrectly normalized tensor $\\tilde{X}$ is given by:\n$$ \\tilde{X}_{b,c,h,w} = \\frac{X_{b,c,h,w} - \\tilde{\\mu}_{b}}{\\sqrt{\\tilde{\\sigma}^2_{b} + \\epsilon}} $$\nThe affine transformation is applied identically to the correct version, maintaining its per-channel nature. The buggy final output $\\tilde{Y}$ is:\n$$ \\tilde{Y}_{b,c,h,w} = \\gamma_c \\tilde{X}_{b,c,h,w} + \\beta_c $$\n\nThe final task is to compute the maximum absolute difference between the two outputs for each test case:\n$$ \\text{MaxAbsDiff} = \\max_{b,c,h,w} | Y_{b,c,h,w} - \\tilde{Y}_{b,c,h,w} | $$\nIn our implementation, we will use `numpy`'s `mean` and `var` functions with the appropriate `axis` and `keepdims=True` arguments to efficiently compute these statistics and leverage broadcasting. For the affine parameters $\\gamma$ and $\\beta$, given as $1$-D arrays of size $C$, we will reshape them to $(1, C, 1, 1)$ to ensure correct per-channel broadcasting. The difference calculation will then be a straightforward element-wise operation followed by finding the maximum value.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares correct and buggy Instance Normalization\n    for a 4D tensor, and calculates the maximum absolute difference\n    between their outputs for a suite of test cases.\n    \"\"\"\n    epsilon = 1e-5\n\n    test_cases = [\n        # Case A: Happy path, B=2, C=2, H=2, W=2\n        {\n            \"X\": np.array([\n                [[[0, 2], [4, 6]], [[-6, -4], [-2, 0]]],\n                [[[10, 12], [14, 16]], [[-10, -12], [-14, -16]]]\n            ], dtype=float),\n            \"gamma\": np.array([1, 1], dtype=float),\n            \"beta\": np.array([0, 0], dtype=float)\n        },\n        # Case B: Edge case, B=1, C=3, H=1, W=4\n        {\n            \"X\": np.array([\n                [[[1, 2, 3, 4]]],\n                [[[10, 10, 10, 10]]],\n                [[[-1, -2, -3, -4]]]\n            ], dtype=float).reshape(1, 3, 1, 4),\n            \"gamma\": np.array([1, 0.5, 2], dtype=float),\n            \"beta\": np.array([0, 1, -1], dtype=float)\n        },\n        # Case C: Boundary case, B=1, C=1, H=2, W=2\n        {\n            \"X\": np.array([[[[5, 7], [9, 11]]]], dtype=float),\n            \"gamma\": np.array([1], dtype=float),\n            \"beta\": np.array([0], dtype=float)\n        },\n        # Case D: Extreme case, B=2, C=3, H=1, W=1\n        {\n            \"X\": np.array([\n                [[[0]], [[10]], [[-10]]],\n                [[[5]], [[15]], [[-5]]]\n            ], dtype=float),\n            \"gamma\": np.array([1, 1, 1], dtype=float),\n            \"beta\": np.array([0, 0, 0], dtype=float)\n        }\n    ]\n\n    results = []\n\n    def correct_instance_norm(X, gamma, beta, eps):\n        C = X.shape[1]\n        \n        # Per-instance, per-channel statistics over spatial dimensions (axes 2, 3)\n        mu = np.mean(X, axis=(2, 3), keepdims=True)\n        var = np.var(X, axis=(2, 3), keepdims=True)\n\n        X_normalized = (X - mu) / np.sqrt(var + eps)\n\n        # Reshape affine parameters for broadcasting\n        gamma_reshaped = gamma.reshape(1, C, 1, 1)\n        beta_reshaped = beta.reshape(1, C, 1, 1)\n        \n        return gamma_reshaped * X_normalized + beta_reshaped\n\n    def buggy_instance_norm(X, gamma, beta, eps):\n        C = X.shape[1]\n        \n        # Per-instance statistics over channel and spatial dimensions (axes 1, 2, 3)\n        mu = np.mean(X, axis=(1, 2, 3), keepdims=True)\n        var = np.var(X, axis=(1, 2, 3), keepdims=True)\n\n        X_normalized = (X - mu) / np.sqrt(var + eps)\n        \n        # Reshape affine parameters for broadcasting\n        gamma_reshaped = gamma.reshape(1, C, 1, 1)\n        beta_reshaped = beta.reshape(1, C, 1, 1)\n        \n        return gamma_reshaped * X_normalized + beta_reshaped\n\n    for case in test_cases:\n        X = case[\"X\"]\n        gamma = case[\"gamma\"]\n        beta = case[\"beta\"]\n\n        Y_correct = correct_instance_norm(X, gamma, beta, epsilon)\n        Y_buggy = buggy_instance_norm(X, gamma, beta, epsilon)\n\n        max_abs_diff = np.max(np.abs(Y_correct - Y_buggy))\n        results.append(max_abs_diff)\n    \n    # Print the results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A robust algorithm must gracefully handle edge cases, and in neural networks, inputs can sometimes have zero variance (e.g., a blank patch in an image). This advanced problem challenges you to analyze IN's behavior in such a scenario . By working through the forward and backward passes, you will discover the crucial role of the small constant $\\epsilon$ in ensuring numerical stability and see how gradients flow even when the initial input variation is zero, providing a deeper insight into the mechanics of automatic differentiation in modern network layers.",
            "id": "3138668",
            "problem": "Consider a single channel of a convolutional feature map within a deep neural network using Instance Normalization (IN). For one instance and one channel, let the spatial indices be $(h,w)$ over a grid of size $H \\times W$ with $m = H W$ elements. Instance Normalization computes the per-instance, per-channel mean and variance as $ \\mu = \\frac{1}{m} \\sum_{h,w} x_{hw}$ and $ \\sigma^{2} = \\frac{1}{m} \\sum_{h,w} (x_{hw} - \\mu)^{2}$. The normalized activation is defined by $ \\hat{x}_{hw} = \\frac{x_{hw} - \\mu}{\\sqrt{\\sigma^{2} + \\epsilon}}$ with a small constant $ \\epsilon > 0$ added for numerical stability, and the affine transform produces $ y_{hw} = \\gamma \\hat{x}_{hw} + \\beta$, where $\\gamma$ and $\\beta$ are learnable parameters. Let the loss be $ \\mathcal{L}$ and denote the upstream gradient by $ g_{hw} = \\frac{\\partial \\mathcal{L}}{\\partial y_{hw}}$. Assume the edge case $ x_{hw} = c$ for all $(h,w)$, where $c$ is a constant and $m \\ge 2$.\n\nFrom the core definitions above and the chain rule of differentiation, reason about the normalized activations and gradients in this zero-variance regime. Which option best describes the normalized outputs and the gradient distribution with respect to $x_{hw}$, $\\gamma$, and $\\beta$?\n\nA. When $ x_{hw} = c$ for all $(h,w)$, the normalized activations satisfy $ \\hat{x}_{hw} = 0$, so $ y_{hw} = \\beta$. The gradient with respect to inputs is $ \\frac{\\partial \\mathcal{L}}{\\partial x_{hw}} = \\frac{\\gamma}{\\sqrt{\\epsilon}} \\left( g_{hw} - \\frac{1}{m} \\sum_{h',w'} g_{h'w'} \\right)$, the gradient with respect to the scale is $ \\frac{\\partial \\mathcal{L}}{\\partial \\gamma} = \\sum_{h,w} g_{hw} \\hat{x}_{hw} = 0$, and the gradient with respect to the shift is $ \\frac{\\partial \\mathcal{L}}{\\partial \\beta} = \\sum_{h,w} g_{hw}$.\n\nB. When $ x_{hw} = c$ for all $(h,w)$, the normalized activations satisfy $ \\hat{x}_{hw} = \\frac{1}{\\sqrt{\\epsilon}}$, so $ y_{hw} = \\gamma \\frac{1}{\\sqrt{\\epsilon}} + \\beta$. The gradient with respect to inputs is uniform and equals $ \\frac{\\partial \\mathcal{L}}{\\partial x_{hw}} = \\frac{1}{m} \\sum_{h',w'} g_{h'w'}$, the gradient with respect to the scale is $ \\frac{\\partial \\mathcal{L}}{\\partial \\gamma} = \\sum_{h,w} g_{hw}$, and the gradient with respect to the shift is $ \\frac{\\partial \\mathcal{L}}{\\partial \\beta} = 0$.\n\nC. When $ x_{hw} = c$ for all $(h,w)$, the normalized activations are undefined because $ \\sigma^{2} = 0$ causes division by zero; consequently all gradients $ \\frac{\\partial \\mathcal{L}}{\\partial x_{hw}}$, $ \\frac{\\partial \\mathcal{L}}{\\partial \\gamma}$, and $ \\frac{\\partial \\mathcal{L}}{\\partial \\beta}$ are zero.\n\nD. When $ x_{hw} = c$ for all $(h,w)$, the normalized activations satisfy $ \\hat{x}_{hw} = 0$, but the gradient with respect to inputs reduces to $ \\frac{\\partial \\mathcal{L}}{\\partial x_{hw}} = \\frac{\\gamma}{\\sqrt{\\epsilon}} g_{hw}$ with no averaging term; the gradient with respect to the scale is $ \\frac{\\partial \\mathcal{L}}{\\partial \\gamma} = 0$, and the gradient with respect to the shift is $ \\frac{\\partial \\mathcal{L}}{\\partial \\beta} = 0$.\n\nE. When $ x_{hw} = c$ for all $(h,w)$, the normalized activations satisfy $ \\hat{x}_{hw} = 0$ and $ y_{hw} = \\beta$, and the gradient with respect to inputs vanishes $ \\frac{\\partial \\mathcal{L}}{\\partial x_{hw}} = 0$ because subtracting the mean removes all sensitivity; the gradients with respect to the affine parameters satisfy $ \\frac{\\partial \\mathcal{L}}{\\partial \\gamma} = 0$ and $ \\frac{\\partial \\mathcal{L}}{\\partial \\beta} = \\sum_{h,w} g_{hw}$.",
            "solution": "The problem statement is analyzed for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- A single channel of a convolutional feature map with spatial indices $(h,w)$ over a grid of size $H \\times W$.\n- The number of spatial elements is $m = HW$, with the constraint $m \\ge 2$.\n- The input to Instance Normalization for this instance and channel is $x_{hw}$.\n- Per-instance, per-channel mean: $\\mu = \\frac{1}{m} \\sum_{h,w} x_{hw}$.\n- Per-instance, per-channel variance: $\\sigma^{2} = \\frac{1}{m} \\sum_{h,w} (x_{hw} - \\mu)^{2}$.\n- Normalized activation: $\\hat{x}_{hw} = \\frac{x_{hw} - \\mu}{\\sqrt{\\sigma^{2} + \\epsilon}}$, with $\\epsilon > 0$.\n- Affine transformation: $y_{hw} = \\gamma \\hat{x}_{hw} + \\beta$, where $\\gamma$ and $\\beta$ are learnable parameters.\n- Loss function is $\\mathcal{L}$.\n- Upstream gradient: $g_{hw} = \\frac{\\partial \\mathcal{L}}{\\partial y_{hw}}$.\n- Edge case condition: $x_{hw} = c$ for all $(h,w)$, where $c$ is a constant.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is based on the standard definitions of Instance Normalization, a widely used technique in deep learning. The mathematical operations involved are based on standard calculus. This is scientifically sound.\n2.  **Well-Posed**: The problem provides all necessary definitions, variables, and a specific condition ($x_{hw}=c$) to analyze. The goal is to derive specific quantities (outputs and gradients), which is a well-defined mathematical task. The inclusion of $\\epsilon > 0$ and $m \\ge 2$ ensures the problem is mathematically tractable.\n3.  **Objective**: The language is precise, mathematical, and free of any subjective claims.\n4.  **Flaw Check**:\n    - No scientific or factual unsoundness is present.\n    - The problem is directly formalizable and relevant to its stated topic.\n    - The setup is not incomplete or contradictory.\n    - The edge case is not unrealistic or infeasible within the abstract context of neural network activations.\n    - The problem is well-structured and admits a unique solution.\n    - The derivation is non-trivial and requires careful application of the chain rule, particularly in the specified edge case.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full derivation and evaluation of options will be performed.\n\n### Derivation\n\nWe are given the condition $x_{hw} = c$ for all spatial locations $(h,w)$. We will first analyze the forward pass and then the backward pass (gradients).\n\n#### Forward Pass Analysis\n1.  **Mean Calculation**:\n    The mean $\\mu$ is calculated over all $m$ spatial locations.\n    $$ \\mu = \\frac{1}{m} \\sum_{h,w} x_{hw} = \\frac{1}{m} \\sum_{h,w} c = \\frac{1}{m} (m \\cdot c) = c $$\n\n2.  **Variance Calculation**:\n    The variance $\\sigma^2$ is calculated using the computed mean.\n    $$ \\sigma^{2} = \\frac{1}{m} \\sum_{h,w} (x_{hw} - \\mu)^{2} = \\frac{1}{m} \\sum_{h,w} (c - c)^{2} = \\frac{1}{m} \\sum_{h,w} 0 = 0 $$\n\n3.  **Normalization**:\n    The normalized activation $\\hat{x}_{hw}$ is computed. The term $\\epsilon > 0$ is crucial here.\n    $$ \\hat{x}_{hw} = \\frac{x_{hw} - \\mu}{\\sqrt{\\sigma^{2} + \\epsilon}} = \\frac{c - c}{\\sqrt{0 + \\epsilon}} = \\frac{0}{\\sqrt{\\epsilon}} = 0 $$\n    Thus, for all $(h,w)$, the normalized activation is $\\hat{x}_{hw} = 0$.\n\n4.  **Affine Transformation**:\n    The final output $y_{hw}$ is computed using the learnable parameters $\\gamma$ and $\\beta$.\n    $$ y_{hw} = \\gamma \\hat{x}_{hw} + \\beta = \\gamma \\cdot 0 + \\beta = \\beta $$\n    Thus, for all $(h,w)$, the output of the layer is $y_{hw} = \\beta$.\n\n#### Backward Pass Analysis (Gradients)\n\nWe use the chain rule to compute the gradients of the loss $\\mathcal{L}$ with respect to $\\beta$, $\\gamma$, and $x_{hw}$. The upstream gradient is given as $g_{hw} = \\frac{\\partial \\mathcal{L}}{\\partial y_{hw}}$.\n\n1.  **Gradient with respect to $\\beta$ ($\\frac{\\partial \\mathcal{L}}{\\partial \\beta}$)**:\n    The parameter $\\beta$ is a scalar added to each $y_{hw}$.\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial \\beta} = \\sum_{h,w} \\frac{\\partial \\mathcal{L}}{\\partial y_{hw}} \\frac{\\partial y_{hw}}{\\partial \\beta} $$\n    Since $y_{hw} = \\gamma \\hat{x}_{hw} + \\beta$, we have $\\frac{\\partial y_{hw}}{\\partial \\beta} = 1$.\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial \\beta} = \\sum_{h,w} g_{hw} \\cdot 1 = \\sum_{h,w} g_{hw} $$\n\n2.  **Gradient with respect to $\\gamma$ ($\\frac{\\partial \\mathcal{L}}{\\partial \\gamma}$)**:\n    The parameter $\\gamma$ is a scalar that scales each $\\hat{x}_{hw}$.\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial \\gamma} = \\sum_{h,w} \\frac{\\partial \\mathcal{L}}{\\partial y_{hw}} \\frac{\\partial y_{hw}}{\\partial \\gamma} $$\n    Since $y_{hw} = \\gamma \\hat{x}_{hw} + \\beta$, we have $\\frac{\\partial y_{hw}}{\\partial \\gamma} = \\hat{x}_{hw}$.\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial \\gamma} = \\sum_{h,w} g_{hw} \\hat{x}_{hw} $$\n    In our specific case, $\\hat{x}_{hw} = 0$ for all $(h,w)$. Therefore:\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial \\gamma} = \\sum_{h,w} g_{hw} \\cdot 0 = 0 $$\n\n3.  **Gradient with respect to $x_{hw}$ ($\\frac{\\partial \\mathcal{L}}{\\partial x_{hw}}$)**:\n    This is the most involved gradient as each input $x_{h'w'}$ affects all normalized outputs $\\hat{x}_{hw}$ through the mean $\\mu$ and variance $\\sigma^2$. To find $\\frac{\\partial \\mathcal{L}}{\\partial x_{h_0w_0}}$ for a specific input at location $(h_0, w_0)$:\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial x_{h_0w_0}} = \\sum_{h,w} \\frac{\\partial \\mathcal{L}}{\\partial y_{hw}} \\frac{\\partial y_{hw}}{\\partial x_{h_0w_0}} = \\sum_{h,w} g_{hw} \\frac{\\partial (\\gamma \\hat{x}_{hw} + \\beta)}{\\partial x_{h_0w_0}} = \\gamma \\sum_{h,w} g_{hw} \\frac{\\partial \\hat{x}_{hw}}{\\partial x_{h_0w_0}} $$\n    We need to compute $\\frac{\\partial \\hat{x}_{hw}}{\\partial x_{h_0w_0}}$. Let $D = \\sqrt{\\sigma^2 + \\epsilon}$. Then $\\hat{x}_{hw} = \\frac{x_{hw}-\\mu}{D}$.\n    Using the quotient rule:\n    $$ \\frac{\\partial \\hat{x}_{hw}}{\\partial x_{h_0w_0}} = \\frac{D \\cdot \\frac{\\partial(x_{hw}-\\mu)}{\\partial x_{h_0w_0}} - (x_{hw}-\\mu) \\cdot \\frac{\\partial D}{\\partial x_{h_0w_0}}}{D^2} $$\n    Let's evaluate the terms in our edge case ($x_{ij}=c$ for all $i,j$):\n    - $D = \\sqrt{0 + \\epsilon} = \\sqrt{\\epsilon}$.\n    - $x_{hw} - \\mu = c - c = 0$.\n    The expression simplifies because the second term in the numerator is zero:\n    $$ \\frac{\\partial \\hat{x}_{hw}}{\\partial x_{h_0w_0}} = \\frac{\\sqrt{\\epsilon} \\cdot \\frac{\\partial(x_{hw}-\\mu)}{\\partial x_{h_0w_0}} - 0}{\\epsilon} = \\frac{1}{\\sqrt{\\epsilon}} \\frac{\\partial(x_{hw}-\\mu)}{\\partial x_{h_0w_0}} $$\n    Now we compute the partial derivative of $(x_{hw}-\\mu)$:\n    $$ \\frac{\\partial(x_{hw}-\\mu)}{\\partial x_{h_0w_0}} = \\frac{\\partial x_{hw}}{\\partial x_{h_0w_0}} - \\frac{\\partial \\mu}{\\partial x_{h_0w_0}} $$\n    - The term $\\frac{\\partial x_{hw}}{\\partial x_{h_0w_0}}$ is $1$ if $(h,w)=(h_0,w_0)$ and $0$ otherwise. This is the Kronecker delta, $\\delta_{(h,w),(h_0,w_0)}$.\n    - The term $\\frac{\\partial \\mu}{\\partial x_{h_0w_0}} = \\frac{\\partial}{\\partial x_{h_0w_0}} \\left(\\frac{1}{m} \\sum_{h',w'} x_{h'w'}\\right) = \\frac{1}{m}$.\n    So, $\\frac{\\partial(x_{hw}-\\mu)}{\\partial x_{h_0w_0}} = \\delta_{(h,w),(h_0,w_0)} - \\frac{1}{m}$.\n    Plugging this back, we get:\n    $$ \\frac{\\partial \\hat{x}_{hw}}{\\partial x_{h_0w_0}} = \\frac{1}{\\sqrt{\\epsilon}} \\left( \\delta_{(h,w),(h_0,w_0)} - \\frac{1}{m} \\right) $$\n    Finally, we substitute this into the expression for $\\frac{\\partial \\mathcal{L}}{\\partial x_{h_0w_0}}$:\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial x_{h_0w_0}} = \\gamma \\sum_{h,w} g_{hw} \\frac{1}{\\sqrt{\\epsilon}} \\left( \\delta_{(h,w),(h_0,w_0)} - \\frac{1}{m} \\right) $$\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial x_{h_0w_0}} = \\frac{\\gamma}{\\sqrt{\\epsilon}} \\left( \\sum_{h,w} g_{hw} \\delta_{(h,w),(h_0,w_0)} - \\sum_{h,w} g_{hw} \\frac{1}{m} \\right) $$\n    The first summation collapses due to the Kronecker delta, picking out only the term where $(h,w)=(h_0,w_0)$:\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial x_{h_0w_0}} = \\frac{\\gamma}{\\sqrt{\\epsilon}} \\left( g_{h_0w_0} - \\frac{1}{m} \\sum_{h,w} g_{hw} \\right) $$\n    Generalizing from $(h_0,w_0)$ to any $(h,w)$ and using $(h',w')$ as summation indices:\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial x_{hw}} = \\frac{\\gamma}{\\sqrt{\\epsilon}} \\left( g_{hw} - \\frac{1}{m} \\sum_{h',w'} g_{h'w'} \\right) $$\n\n### Option-by-Option Analysis\n\n- **A. When $x_{hw} = c$ for all $(h,w)$, the normalized activations satisfy $\\hat{x}_{hw} = 0$, so $y_{hw} = \\beta$. The gradient with respect to inputs is $\\frac{\\partial \\mathcal{L}}{\\partial x_{hw}} = \\frac{\\gamma}{\\sqrt{\\epsilon}} \\left( g_{hw} - \\frac{1}{m} \\sum_{h',w'} g_{h'w'} \\right)$, the gradient with respect to the scale is $\\frac{\\partial \\mathcal{L}}{\\partial \\gamma} = \\sum_{h,w} g_{hw} \\hat{x}_{hw} = 0$, and the gradient with respect to the shift is $\\frac{\\partial \\mathcal{L}}{\\partial \\beta} = \\sum_{h,w} g_{hw}$.**\n  - The forward pass results $\\hat{x}_{hw} = 0$ and $y_{hw} = \\beta$ are correct.\n  - The gradient $\\frac{\\partial \\mathcal{L}}{\\partial x_{hw}}$ matches our derivation exactly.\n  - The gradient $\\frac{\\partial \\mathcal{L}}{\\partial \\gamma}$ matches our derivation exactly.\n  - The gradient $\\frac{\\partial \\mathcal{L}}{\\partial \\beta}$ matches our derivation exactly.\n  - **Verdict: Correct.**\n\n- **B. When $x_{hw} = c$ for all $(h,w)$, the normalized activations satisfy $\\hat{x}_{hw} = \\frac{1}{\\sqrt{\\epsilon}}$, so $y_{hw} = \\gamma \\frac{1}{\\sqrt{\\epsilon}} + \\beta$. ...**\n  - The statement $\\hat{x}_{hw} = \\frac{1}{\\sqrt{\\epsilon}}$ is incorrect. The numerator of $\\hat{x}_{hw}$ is $x_{hw} - \\mu = c - c = 0$, so $\\hat{x}_{hw} = 0$.\n  - **Verdict: Incorrect.**\n\n- **C. When $x_{hw} = c$ for all $(h,w)$, the normalized activations are undefined because $\\sigma^{2} = 0$ causes division by zero;...**\n  - This is incorrect. The formula for the normalized activation includes $\\epsilon > 0$ in the denominator, $\\sqrt{\\sigma^2 + \\epsilon}$, specifically to prevent division by zero. The denominator is $\\sqrt{\\epsilon}$, which is well-defined.\n  - **Verdict: Incorrect.**\n\n- **D. When $x_{hw} = c$ for all $(h,w)$, the normalized activations satisfy $\\hat{x}_{hw} = 0$, but the gradient with respect to inputs reduces to $\\frac{\\partial \\mathcal{L}}{\\partial x_{hw}} = \\frac{\\gamma}{\\sqrt{\\epsilon}} g_{hw}$ with no averaging term; ... the gradient with respect to the shift is $\\frac{\\partial \\mathcal{L}}{\\partial \\beta} = 0$.**\n  - The expression for $\\frac{\\partial \\mathcal{L}}{\\partial x_{hw}}$ is incorrect. It omits the term $-\\frac{1}{m}\\sum_{h',w'} g_{h'w'}$, which arises from the chain rule through the mean $\\mu$.\n  - The expression for $\\frac{\\partial \\mathcal{L}}{\\partial \\beta}$ is incorrect. It should be $\\sum_{h,w} g_{hw}$.\n  - **Verdict: Incorrect.**\n\n- **E. When $x_{hw} = c$ for all $(h,w)$, ... the gradient with respect to inputs vanishes $\\frac{\\partial \\mathcal{L}}{\\partial x_{hw}} = 0$ because subtracting the mean removes all sensitivity;...**\n  - The claim that $\\frac{\\partial \\mathcal{L}}{\\partial x_{hw}} = 0$ is incorrect. Our derivation shows a non-zero gradient unless the upstream gradient $g_{hw}$ happens to be constant. The reasoning \"subtracting the mean removes all sensitivity\" is an intuitive fallacy that ignores the gradient path through the mean itself.\n  - **Verdict: Incorrect.**\n\nBased on the detailed derivation, only option A accurately describes the forward pass results and all the relevant gradients in the specified zero-variance regime.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}