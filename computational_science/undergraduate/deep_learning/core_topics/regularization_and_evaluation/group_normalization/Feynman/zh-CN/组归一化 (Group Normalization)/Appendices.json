{
    "hands_on_practices": [
        {
            "introduction": "Group Normalization (GN) 是一个非线性操作，理解其基本数学性质至关重要。此练习旨在通过理论推导和经验验证，探索 GN 的一个核心属性：尺度不变性。通过分析在何种条件下 $\\text{GN}(ax) = a \\cdot \\text{GN}(x)$ 成立，您将深入理解 $\\varepsilon$ 和可学习参数 $\\beta$ 如何影响 GN 的行为，并揭示其与线性操作的根本区别。",
            "id": "3134049",
            "problem": "在深度学习中，组归一化 (Group Normalization, GN) 的定义如下。设输入张量表示为 $x \\in \\mathbb{R}^{N \\times C \\times H \\times W}$。设 $G$ 为一个正整数，且 $G$ 能整除 $C$。定义 $C_{\\mathrm{grp}} = C / G$。对于每个样本索引 $n \\in \\{0,1,\\dots,N-1\\}$ 和每个组索引 $g \\in \\{0,1,\\dots,G-1\\}$，定义索引的组集合\n$$\n\\mathcal{I}_{n,g} = \\{(n, c, h, w) \\mid c \\in \\{g \\cdot C_{\\mathrm{grp}}, \\dots, (g+1) \\cdot C_{\\mathrm{grp}} - 1\\},\\ h \\in \\{0,\\dots,H-1\\},\\ w \\in \\{0,\\dots,W-1\\} \\}.\n$$\n定义组均值和方差为\n$$\n\\mu_{n,g}(x) = \\frac{1}{|\\mathcal{I}_{n,g}|} \\sum_{(n,c,h,w) \\in \\mathcal{I}_{n,g}} x_{n,c,h,w}, \\quad\n\\sigma^2_{n,g}(x) = \\frac{1}{|\\mathcal{I}_{n,g}|} \\sum_{(n,c,h,w) \\in \\mathcal{I}_{n,g}} \\left(x_{n,c,h,w} - \\mu_{n,g}(x)\\right)^2.\n$$\n给定一个小的非负常数 $\\varepsilon \\ge 0$，归一化张量 $z$ 按元素定义为\n$$\nz_{n,c,h,w} = \\frac{x_{n,c,h,w} - \\mu_{n,g(c)}(x)}{\\sqrt{\\sigma^2_{n,g(c)}(x) + \\varepsilon}},\n$$\n其中 $g(c) = \\left\\lfloor \\frac{c}{C_{\\mathrm{grp}}} \\right\\rfloor$。使用可学习的逐通道仿射参数 $\\gamma \\in \\mathbb{R}^{C}$ 和 $\\beta \\in \\mathbb{R}^{C}$，组归一化 (Group Normalization) 的输出为\n$$\n\\mathrm{GN}(x)_{n,c,h,w} = \\gamma_c \\, z_{n,c,h,w} + \\beta_c.\n$$\n\n你的任务是：\n1) 仅使用上述定义以及均值和方差的已知缩放性质（对于任意标量 $a \\in \\mathbb{R}$，有 $\\mu_{n,g}(a x) = a \\, \\mu_{n,g}(x)$ 和 $\\sigma^2_{n,g}(a x) = a^2 \\, \\sigma^2_{n,g}(x)$），推导在何种关于 $(\\varepsilon,\\gamma,\\beta)$ 的充分必要条件下，映射 $x \\mapsto \\mathrm{GN}(x)$ 是正一阶齐次的，即对于任意 $a>0$ 和任意 $x$，满足：\n$$\n\\mathrm{GN}(a x) = a \\cdot \\mathrm{GN}(x).\n$$\n你必须根据给定的定义和基本性质进行推理，不得使用任何未经证明的简化公式。\n\n2) 实现一个程序，该程序构建一个固定的测试张量，并在一组改变分组、缩放因子 $a$ 和参数 $(\\varepsilon,\\gamma,\\beta)$ 的测试用例中，根据固定的绝对容差凭经验检查上述等式是否成立。为确保可复现性，实现细节如下：\n- 使用形状为 $(N,C,H,W)$ 的张量，其中 $N=2, C=4, H=2, W=2$。\n- 使用以 $7$ 为种子的伪随机数生成器，从标准正态分布中独立抽样 $x$ 的元素。\n- 对于每个测试用例，使用指定的 $G, \\varepsilon, \\gamma, \\beta$ 计算 $\\mathrm{GN}(x)$ 和 $\\mathrm{GN}(a x)$，然后在绝对容差 $\\tau = 10^{-9}$ 内测试 $\\mathrm{GN}(a x)$ 是否等于 $a \\cdot \\mathrm{GN}(x)$。\n\n使用的测试套件如下：\n- 用例 1：$G=2$, $a=2$, $\\varepsilon=0$, $\\gamma=[1,\\,-\\tfrac{1}{2},\\,2,\\,\\tfrac{3}{10}]$, $\\beta=[0,\\,0,\\,0,\\,0]$。\n- 用例 2：$G=2$, $a=1$, $\\varepsilon=10^{-5}$, $\\gamma=[1,\\,-\\tfrac{1}{2},\\,2,\\,\\tfrac{3}{10}]$, $\\beta=[\\tfrac{1}{10},\\,-\\tfrac{1}{5},\\,0,\\,\\tfrac{1}{2}]$。\n- 用例 3：$G=4$, $a=2$, $\\varepsilon=0$, $\\gamma=[0,\\,0,\\,0,\\,0]$, $\\beta=[0,\\,0,\\,0,\\,0]$。\n- 用例 4：$G=1$, $a=2$, $\\varepsilon=0$, $\\gamma=[1,\\,-\\tfrac{1}{2},\\,2,\\,\\tfrac{3}{10}]$, $\\beta=[\\tfrac{1}{4},\\,\\tfrac{1}{4},\\,\\tfrac{1}{4},\\,\\tfrac{1}{4}]$。\n- 用例 5：$G=1$, $a=2$, $\\varepsilon=10^{-5}$, $\\gamma=[1,\\,-\\tfrac{1}{2},\\,2,\\,\\tfrac{3}{10}]$, $\\beta=[0,\\,0,\\,0,\\,0]$。\n- 用例 6：$G=2$, $a=100$, $\\varepsilon=10^{-5}$, $\\gamma=[1,\\,-\\tfrac{1}{2},\\,2,\\,\\tfrac{3}{10}]$, $\\beta=[0,\\,0,\\,0,\\,0]$。\n\n对于每个用例，你的程序必须输出一个布尔值，表示等式 $\\mathrm{GN}(a x) = a \\cdot \\mathrm{GN}(x)$ 是否在容差 $\\tau$ 内成立。最终输出格式必须是单行、用方括号括起来的逗号分隔的布尔值列表，例如，“[True,False,True,False,True,False]”。此问题不涉及角度或物理单位。你的代码不能要求任何用户输入。",
            "solution": "该问题要求完成两项任务：首先，推导使组归一化 (Group Normalization, GN) 算子成为正一阶齐次的参数 $(\\varepsilon, \\gamma, \\beta)$ 的充分必要条件。其次，为一组给定的参数实现一个关于此性质的经验性测试。\n\n### 第一部分：齐次性条件的推导\n\n一个函数 $f$ 是正一阶齐次的条件是，对于所有可缩放的输入 $x$ 和任意标量 $a > 0$，都有 $f(ax) = a f(x)$。在我们的情况下，$f(x) = \\mathrm{GN}(x)$。我们必须找到使此等式对所有输入张量 $x \\in \\mathbb{R}^{N \\times C \\times H \\times W}$ 都成立的关于 $(\\varepsilon, \\gamma, \\beta)$ 的充分必要条件。\n\n我们来分析齐次性方程 $\\mathrm{GN}(ax) = a \\cdot \\mathrm{GN}(x)$ 的左侧 (LHS) 和右侧 (RHS)。我们将考虑输出张量在索引 $(n,c,h,w)$ 处的一个元素。设 $g = g(c) = \\lfloor c / (C/G) \\rfloor$ 为通道 $c$ 的组索引。为了简洁，在上下文清晰的情况下，我们省略 $\\mu$ 和 $\\sigma^2$ 的参数，例如，$\\mu_{n,g} \\equiv \\mu_{n,g}(x)$。\n\n从 $\\mathrm{GN}(x)$ 的定义直接展开 RHS 可得：\n$$\n\\text{RHS} = a \\cdot \\mathrm{GN}(x)_{n,c,h,w} = a \\left( \\gamma_c \\frac{x_{n,c,h,w} - \\mu_{n,g}}{\\sqrt{\\sigma^2_{n,g} + \\varepsilon}} + \\beta_c \\right) = a \\gamma_c \\frac{x_{n,c,h,w} - \\mu_{n,g}}{\\sqrt{\\sigma^2_{n,g} + \\varepsilon}} + a \\beta_c.\n$$\n\n对于 LHS，我们首先计算缩放后输入 $ax$ 的均值和方差。使用给定的缩放性质 $\\mu_{n,g}(ax) = a \\mu_{n,g}(x)$ 和 $\\sigma^2_{n,g}(ax) = a^2 \\sigma^2_{n,g}(x)$：\n$$\n\\mathrm{GN}(ax)_{n,c,h,w} = \\gamma_c \\frac{(ax)_{n,c,h,w} - \\mu_{n,g}(ax)}{\\sqrt{\\sigma^2_{n,g}(ax) + \\varepsilon}} + \\beta_c = \\gamma_c \\frac{a \\cdot x_{n,c,h,w} - a \\cdot \\mu_{n,g}}{\\sqrt{a^2 \\sigma^2_{n,g} + \\varepsilon}} + \\beta_c.\n$$\n$$\n\\text{LHS} = \\gamma_c \\frac{a (x_{n,c,h,w} - \\mu_{n,g})}{\\sqrt{a^2 \\sigma^2_{n,g} + \\varepsilon}} + \\beta_c.\n$$\n\n现在，我们令 LHS = RHS，此等式必须对所有 $x$ 和所有 $a > 0$ 成立：\n$$\n\\gamma_c \\frac{a (x_{n,c,h,w} - \\mu_{n,g})}{\\sqrt{a^2 \\sigma^2_{n,g} + \\varepsilon}} + \\beta_c = a \\gamma_c \\frac{x_{n,c,h,w} - \\mu_{n,g}}{\\sqrt{\\sigma^2_{n,g} + \\varepsilon}} + a \\beta_c.\n$$\n整理各项，我们得到：\n$$\na \\gamma_c (x_{n,c,h,w} - \\mu_{n,g}) \\left( \\frac{1}{\\sqrt{a^2 \\sigma^2_{n,g} + \\varepsilon}} - \\frac{1}{\\sqrt{\\sigma^2_{n,g} + \\varepsilon}} \\right) = (a-1)\\beta_c.\n$$\n\n这个等式必须对所有输入张量 $x$ 和所有 $a > 0$ 成立。方程的右侧 $(a-1)\\beta_c$ 是一个与输入 $x$ 无关的常数。然而，左侧通过项 $(x_{n,c,h,w} - \\mu_{n,g})$ 和 $\\sigma^2_{n,g}(x)$ 明确地依赖于 $x$。一个依赖于 $x$ 的函数恒等于一个常数的唯一可能是，这个函数本身也是一个常数，并且这个常数等于等式右侧的常数。\n\n我们可以通过选择一个特定的输入 $x$ 来约束这个常数。例如，如果我们选择一个在每个组 $(n, g)$ 内所有激活值都相等的输入 $x_0$，那么对于这个输入，$\\sigma_{n,g}^2(x_0)=0$ 且 $x_{0,n,c,h,w} - \\mu_{n,g}(x_0) = 0$。代入这些值，方程的左侧变为 0。因此，右侧的常数也必须为 0：\n$$\n(a-1)\\beta_c = 0.\n$$\n由于这个条件必须对所有 $a > 0$ 成立，我们可以选择任意 $a \\neq 1$（例如 $a=2$），这立即导出 $\\beta_c = 0$。此结论必须对所有通道 $c \\in \\{0, \\dots, C-1\\}$ 都成立。因此，正一阶齐次性的一个必要条件是 $\\beta = \\vec{0}$。\n\n将 $\\beta = \\vec{0}$ 代回我们整理后的方程，我们得到：\n$$\na \\gamma_c (x_{n,c,h,w} - \\mu_{n,g}) \\left( \\frac{1}{\\sqrt{a^2 \\sigma^2_{n,g} + \\varepsilon}} - \\frac{1}{\\sqrt{\\sigma^2_{n,g} + \\varepsilon}} \\right) = 0.\n$$\n这个等式现在必须对所有 $x$ 和所有 $a > 0$ 成立。这意味着乘积中的至少一个因子必须为零。\n我们可以选择一个输入 $x$ 使得其在组内不全相等，这样 $x_{n,c,h,w} - \\mu_{n,g}$ 和 $\\sigma^2_{n,g}$ 就不全为零。\n\n考虑括号中的项：$\\frac{1}{\\sqrt{a^2 \\sigma^2_{n,g} + \\varepsilon}} - \\frac{1}{\\sqrt{\\sigma^2_{n,g} + \\varepsilon}}$。如果我们选择 $a \\neq 1$ 并且一个 $x$ 使得 $\\sigma^2_{n,g} > 0$，那么 $a^2 \\sigma^2_{n,g} \\neq \\sigma^2_{n,g}$，因此括号中的项通常不为零（除非 $\\varepsilon=0$ 且 $\\sigma_{n,g}=0$，但我们不能假设所有输入方差都为零）。\n\n既然括号内的项和 $(x_{n,c,h,w} - \\mu_{n,g})$ 通常不为零，为了使整个表达式对所有可能的输入 $x$ 和所有 $a>0$ 都恒等于零，唯一的可能性是 $\\gamma_c = 0$。\n\n如果 $\\gamma_c = 0$，那么方程变为 $0=0$，恒成立。如果 $\\beta_c = 0$ 且 $\\gamma_c=0$，则 $\\mathrm{GN}(x)$ 在该通道上恒为 0，齐次性条件 $\\mathrm{GN}(ax) = a \\cdot \\mathrm{GN}(x)$ 也就简化为 $0 = a \\cdot 0$，显然成立。\n\n因此，该等式对所有 $x$ 和 $a>0$ 成立的充分必要条件是，对于每个通道 $c$，其对应的参数都为零。\n\n$$\n\\gamma = \\vec{0} \\quad \\text{且} \\quad \\beta = \\vec{0}.\n$$\n在这些条件下，对于任何输入 $x$，$\\mathrm{GN}(x)$ 恒为零张量。齐次性条件 $\\mathrm{GN}(ax) = a \\cdot \\mathrm{GN}(x)$ 因 $\\vec{0} = a \\cdot \\vec{0}$ 而不证自明。\n\n### 第二部分：经验性验证\n\n以下程序实现了组归一化算子，并针对六个指定用例测试了齐次性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef group_norm(x: np.ndarray, G: int, eps: float, gamma: np.ndarray, beta: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the Group Normalization for a 4D input tensor.\n\n    Args:\n        x: Input tensor of shape (N, C, H, W).\n        G: Number of groups.\n        eps: Epsilon for numerical stability.\n        gamma: Learnable per-channel scaling factor of shape (C,).\n        beta: Learnable per-channel bias factor of shape (C,).\n\n    Returns:\n        The output tensor after Group Normalization.\n    \"\"\"\n    N, C, H, W = x.shape\n    if C % G != 0:\n        raise ValueError(\"Number of channels C must be divisible by number of groups G.\")\n    \n    C_grp = C // G\n    \n    # Reshape for group-wise calculations: (N, C, H, W) -> (N, G, C_grp, H, W)\n    x_reshaped = x.reshape(N, G, C_grp, H, W)\n    \n    # Calculate mean and variance over the group axes (C_grp, H, W)\n    # These are axes (2, 3, 4) in the reshaped tensor.\n    # keepdims=True ensures that the output shapes are broadcastable.\n    mean = x_reshaped.mean(axis=(2, 3, 4), keepdims=True)\n    var = x_reshaped.var(axis=(2, 3, 4), keepdims=True)\n    \n    # Normalize\n    # The reshaped mean and var will broadcast over x_reshaped.\n    x_norm_reshaped = (x_reshaped - mean) / np.sqrt(var + eps)\n    \n    # Reshape back to original tensor layout: (N, G, C_grp, H, W) -> (N, C, H, W)\n    x_norm = x_norm_reshaped.reshape(N, C, H, W)\n    \n    # Apply affine transformation (scale and shift)\n    # Reshape gamma and beta from (C,) to (1, C, 1, 1) for broadcasting.\n    gamma_reshaped = gamma.reshape(1, C, 1, 1)\n    beta_reshaped = beta.reshape(1, C, 1, 1)\n    \n    output = gamma_reshaped * x_norm + beta_reshaped\n    return output\n\ndef solve():\n    \"\"\"\n    Main function to run the empirical tests for homogeneity.\n    \"\"\"\n    # Define reproducible test tensor\n    N, C, H, W = 2, 4, 2, 2\n    seed = 7\n    rng = np.random.default_rng(seed)\n    x = rng.standard_normal(size=(N, C, H, W))\n    \n    # Absolute tolerance for equality check\n    tau = 1e-9\n\n    # Define the 6 test cases from the problem statement.\n    test_cases = [\n        # Case 1: G=2, a=2, eps=0, gamma!=0, beta=0\n        {'G': 2, 'a': 2.0, 'eps': 0.0, 'gamma': np.array([1.0, -0.5, 2.0, 0.3]), 'beta': np.array([0.0, 0.0, 0.0, 0.0])},\n        # Case 2: a=1 (trivial case)\n        {'G': 2, 'a': 1.0, 'eps': 1e-5, 'gamma': np.array([1.0, -0.5, 2.0, 0.3]), 'beta': np.array([0.1, -0.2, 0.0, 0.5])},\n        # Case 3: G=4, a=2, eps=0, gamma=0, beta=0 (trivial GN)\n        {'G': 4, 'a': 2.0, 'eps': 0.0, 'gamma': np.array([0.0, 0.0, 0.0, 0.0]), 'beta': np.array([0.0, 0.0, 0.0, 0.0])},\n        # Case 4: G=1, a=2, eps=0, beta!=0\n        {'G': 1, 'a': 2.0, 'eps': 0.0, 'gamma': np.array([1.0, -0.5, 2.0, 0.3]), 'beta': np.array([0.25, 0.25, 0.25, 0.25])},\n        # Case 5: G=1, a=2, eps!=0, beta=0\n        {'G': 1, 'a': 2.0, 'eps': 1e-5, 'gamma': np.array([1.0, -0.5, 2.0, 0.3]), 'beta': np.array([0.0, 0.0, 0.0, 0.0])},\n        # Case 6: G=2, a=100, eps!=0, beta=0\n        {'G': 2, 'a': 100.0, 'eps': 1e-5, 'gamma': np.array([1.0, -0.5, 2.0, 0.3]), 'beta': np.array([0.0, 0.0, 0.0, 0.0])},\n    ]\n\n    results = []\n    for case in test_cases:\n        G, a, eps, gamma, beta = case['G'], case['a'], case['eps'], case['gamma'], case['beta']\n        \n        # Calculate GN(a*x)\n        gn_ax = group_norm(a * x, G, eps, gamma, beta)\n        \n        # Calculate a * GN(x)\n        a_gn_x = a * group_norm(x, G, eps, gamma, beta)\n        \n        # Check if GN(a*x) is close to a * GN(x) within the absolute tolerance\n        # rtol=0 ensures only absolute tolerance is used for the comparison.\n        is_homogeneous = np.allclose(gn_ax, a_gn_x, atol=tau, rtol=0)\n        results.append(is_homogeneous)\n\n    # Format the final output as a single-line comma-separated list of booleans\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在深度学习模型的训练中，数值稳定性是一个核心问题，尤其是在归一化层中。当一个组内所有激活值的方差趋近于零时，标准的归一化计算可能会导致梯度爆炸。本练习将引导您从第一性原理出发，推导当组方差 $\\sigma_g^2 \\to 0$ 时 GN 的梯度行为，并分析如何通过合理设置超参数 $\\varepsilon$ 来保证训练的稳定性。",
            "id": "3133999",
            "problem": "考虑一个单一训练样本和群组归一化 (Group Normalization, GN) 中的一个群组，群组大小为 $m \\geq 2$。设群组激活值为 $x_{1}, \\dots, x_{m} \\in \\mathbb{R}$，其群组均值为 $\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x_{i}$，其群组方差为 $\\sigma_{g}^{2} = \\frac{1}{m} \\sum_{i=1}^{m} (x_{i} - \\mu)^{2}$。GN 生成归一化激活值 $\\hat{x}_{i} = \\frac{x_{i} - \\mu}{\\sqrt{\\sigma_{g}^{2} + \\epsilon}}$ 并输出 $y_{i} = \\gamma \\hat{x}_{i} + \\beta$，其中 $\\gamma \\in \\mathbb{R}$ 和 $\\beta \\in \\mathbb{R}$ 是在该群组的 $m$ 个元素之间共享的可学习参数。设标量损失为 $L(y_{1}, \\dots, y_{m})$。假设传入的梯度满足一个一致界限 $| \\frac{\\partial L}{\\partial y_{i}} | \\leq G$，对于所有 $i \\in \\{1,\\dots,m\\}$，其中 $G > 0$ 是一个给定的常数。\n\n仅使用上述定义和基本的多变量微积分（链式法则）作为基础：\n\n1) 当 $\\sigma_{g}^{2} \\to 0$ 时，推导梯度 $\\frac{\\partial L}{\\partial x_{i}}$ 的极限形式，用 $\\gamma$、$\\epsilon$ 和传入的梯度表示。\n\n2) 从该极限形式出发，当 $\\sigma_{g}^{2} \\to 0$ 时，推导出 $\\max_{i} \\left| \\frac{\\partial L}{\\partial x_{i}} \\right|$ 的最紧可能上界，用 $m$、$\\gamma$、$\\epsilon$ 和 $G$ 表示。\n\n3) 给定一个目标梯度幅值阈值 $\\tau > 0$，求解保证在 $\\sigma_{g}^{2} \\to 0$ 的极限情况下 $\\max_{i} \\left| \\frac{\\partial L}{\\partial x_{i}} \\right| \\leq \\tau$ 的最小 $\\epsilon$。\n\n4) 提出一种自适应策略，将 $\\epsilon$ 选择为 $\\sigma_{g}^{2}$ 和其他问题参数的函数，以在 $\\sigma_{g}^{2} \\to 0$ 时保持数值稳定性，并解释其工作原理。\n\n你的最终答案必须仅为第 3 部分中最小 $\\epsilon$ 的封闭形式表达式。无需单位。如果你的答案包含常数，请用符号表示。不要四舍五入。",
            "solution": "该解答是基于应用于群组归一化 (Group Normalization, GN) 函数的多变量微积分原理的四部分推导和分析。\n\n### 第 1 部分：梯度 $\\frac{\\partial L}{\\partial x_{i}}$ 极限形式的推导\n\n损失 $L$ 对输入激活值 $x_i$ 的全导数可以通过多变量链式法则求得，即对所有依赖于 $x_i$ 的输出 $y_j$ 进行求和：\n$$ \\frac{\\partial L}{\\partial x_{i}} = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_{j}} \\frac{\\partial y_{j}}{\\partial x_{i}} $$\n输出 $y_j$ 定义为 $y_{j} = \\gamma \\hat{x}_{j} + \\beta$，其中 $\\hat{x}_{j} = \\frac{x_{j} - \\mu}{\\sqrt{\\sigma_{g}^{2} + \\epsilon}}$。因此，$\\frac{\\partial y_{j}}{\\partial x_{i}} = \\gamma \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}}$。\n\n为了求得 $\\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}}$，我们必须考虑到 $\\mu$ 和 $\\sigma_g^2$ 都是群组中所有 $x_k$ 的函数。均值 $\\mu$ 和方差 $\\sigma_g^2$ 关于 $x_i$ 的偏导数是：\n$$ \\frac{\\partial \\mu}{\\partial x_{i}} = \\frac{\\partial}{\\partial x_{i}} \\left( \\frac{1}{m} \\sum_{k=1}^{m} x_{k} \\right) = \\frac{1}{m} $$\n$$ \\frac{\\partial \\sigma_{g}^{2}}{\\partial x_{i}} = \\frac{\\partial}{\\partial x_{i}} \\left( \\frac{1}{m} \\sum_{k=1}^{m} (x_{k} - \\mu)^{2} \\right) = \\frac{1}{m} \\sum_{k=1}^{m} 2(x_{k} - \\mu) \\left( \\frac{\\partial x_{k}}{\\partial x_{i}} - \\frac{\\partial \\mu}{\\partial x_{i}} \\right) $$\n$$ = \\frac{2}{m} \\sum_{k=1}^{m} (x_{k} - \\mu) ( \\delta_{ki} - \\frac{1}{m} ) = \\frac{2}{m} \\left( (x_i - \\mu)(1-\\frac{1}{m}) - \\frac{1}{m}\\sum_{k \\neq i} (x_k - \\mu) \\right) $$\n使用性质 $\\sum_{k=1}^{m} (x_{k} - \\mu) = 0$，我们有 $\\sum_{k \\neq i} (x_{k} - \\mu) = -(x_i - \\mu)$。代入可得：\n$$ \\frac{\\partial \\sigma_{g}^{2}}{\\partial x_{i}} = \\frac{2}{m} \\left( (x_i - \\mu)(1-\\frac{1}{m}) + \\frac{1}{m}(x_i - \\mu) \\right) = \\frac{2}{m} (x_i - \\mu) $$\n现在，我们将商法则应用于 $\\hat{x}_j = \\frac{u}{v}$，其中 $u = x_j - \\mu$ 且 $v = \\sqrt{\\sigma_g^2 + \\epsilon}$。\n$$ \\frac{\\partial u}{\\partial x_i} = \\frac{\\partial (x_j - \\mu)}{\\partial x_i} = \\delta_{ij} - \\frac{1}{m} $$\n$$ \\frac{\\partial v}{\\partial x_i} = \\frac{1}{2\\sqrt{\\sigma_g^2 + \\epsilon}} \\frac{\\partial \\sigma_g^2}{\\partial x_i} = \\frac{1}{2\\sqrt{\\sigma_g^2 + \\epsilon}} \\frac{2}{m}(x_i - \\mu) = \\frac{x_i - \\mu}{m\\sqrt{\\sigma_g^2 + \\epsilon}} $$\n导数 $\\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} = \\frac{v \\frac{\\partial u}{\\partial x_i} - u \\frac{\\partial v}{\\partial x_i}}{v^2}$ 为：\n$$ \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} = \\frac{\\sqrt{\\sigma_g^2 + \\epsilon}(\\delta_{ij} - \\frac{1}{m}) - (x_j-\\mu) \\frac{x_i - \\mu}{m\\sqrt{\\sigma_g^2 + \\epsilon}}}{\\sigma_g^2 + \\epsilon} = \\frac{1}{\\sqrt{\\sigma_g^2 + \\epsilon}} \\left( \\delta_{ij} - \\frac{1}{m} \\right) - \\frac{(x_i - \\mu)(x_j - \\mu)}{m(\\sigma_g^2 + \\epsilon)^{3/2}} $$\n我们关心的是 $\\sigma_g^2 \\to 0$ 时的极限。这个条件意味着对于所有 $k \\in \\{1, \\dots, m\\}$，$(x_k - \\mu) \\to 0$。项 $(x_i-\\mu)(x_j-\\mu)$ 的阶数为 $O(\\sigma_g^2)$。我们来分析一下 $\\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}}$ 表达式中的第二项：\n$$ \\lim_{\\sigma_g^2 \\to 0} \\frac{(x_i - \\mu)(x_j - \\mu)}{m(\\sigma_g^2 + \\epsilon)^{3/2}} = \\lim_{\\sigma_g^2 \\to 0} \\frac{O(\\sigma_g^2)}{m(\\sigma_g^2 + \\epsilon)^{3/2}} = \\frac{0}{m \\epsilon^{3/2}} = 0 $$\n因此，在 $\\sigma_g^2 \\to 0$ 的极限下，该导数简化为：\n$$ \\lim_{\\sigma_g^2 \\to 0} \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} = \\lim_{\\sigma_g^2 \\to 0} \\frac{1}{\\sqrt{\\sigma_g^2 + \\epsilon}} \\left( \\delta_{ij} - \\frac{1}{m} \\right) = \\frac{1}{\\sqrt{\\epsilon}} \\left( \\delta_{ij} - \\frac{1}{m} \\right) $$\n将此结果代回 $\\frac{\\partial L}{\\partial x_i}$ 的表达式，并使用 $g_j = \\frac{\\partial L}{\\partial y_j}$ 来表示传入的梯度：\n$$ \\lim_{\\sigma_g^2 \\to 0} \\frac{\\partial L}{\\partial x_{i}} = \\sum_{j=1}^{m} g_j \\left( \\lim_{\\sigma_g^2 \\to 0} \\gamma \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} \\right) = \\sum_{j=1}^{m} g_j \\frac{\\gamma}{\\sqrt{\\epsilon}} (\\delta_{ij} - \\frac{1}{m}) $$\n$$ = \\frac{\\gamma}{\\sqrt{\\epsilon}} \\left( g_i - \\frac{1}{m} \\sum_{j=1}^{m} g_j \\right) $$\n这就是梯度 $\\frac{\\partial L}{\\partial x_i}$ 的极限形式。\n\n### 第 2 部分：$\\max_{i} \\left| \\frac{\\partial L}{\\partial x_{i}} \\right|$ 的最紧上界\n\n根据第 1 部分，我们必须找到 $\\max_i \\left| \\frac{\\gamma}{\\sqrt{\\epsilon}} \\left( g_i - \\bar{g} \\right) \\right|$ 的最紧上界，其中 $\\bar{g} = \\frac{1}{m} \\sum_j g_j$ 且给定对所有 $j$ 都有 $|g_j| \\leq G$。这等价于最大化 $\\frac{|\\gamma|}{\\sqrt{\\epsilon}} \\max_i |g_i - \\bar{g}|$。我们专注于最大化 $|g_i - \\bar{g}|$。\n$$ |g_i - \\bar{g}| = \\left| g_i - \\frac{1}{m} g_i - \\frac{1}{m} \\sum_{j \\neq i} g_j \\right| = \\left| \\frac{m-1}{m} g_i - \\frac{1}{m} \\sum_{j \\neq i} g_j \\right| $$\n根据三角不等式：\n$$ \\left| \\frac{m-1}{m} g_i - \\frac{1}{m} \\sum_{j \\neq i} g_j \\right| \\leq \\frac{m-1}{m} |g_i| + \\frac{1}{m} \\sum_{j \\neq i} |g_j| $$\n使用界限 $|g_j| \\leq G$：\n$$ \\leq \\frac{m-1}{m} G + \\frac{1}{m} (m-1) G = 2G \\frac{m-1}{m} $$\n这个界是紧的。可以通过将一个梯度设置为一个极端值，而将其他梯度设置为相反的极端值来达到。对于一个固定的 $i$，设 $g_i = G$ 且对所有 $j \\neq i$ 设 $g_j = -G$。\n平均梯度为 $\\bar{g} = \\frac{1}{m} (G + (m-1)(-G)) = \\frac{G(1 - m + 1)}{m} = \\frac{G(2-m)}{m}$。\n那么，$g_i - \\bar{g} = G - G\\frac{2-m}{m} = G \\left( \\frac{m - 2 + m}{m} \\right) = G \\frac{2m-2}{m} = 2G \\frac{m-1}{m}$。\n其幅值为 $|g_i - \\bar{g}| = 2G \\frac{m-1}{m}$。对于任何 $j \\neq i$，$|g_j - \\bar{g}| = |-G - G\\frac{2-m}{m}| = |-G \\frac{m+2-m}{m}| = \\frac{2G}{m}$。\n因为 $m \\geq 2$，所以 $m-1 \\geq 1$，因此 $2G\\frac{m-1}{m} \\geq \\frac{2G}{m}$。\n与均值的最大偏差确实是 $2G \\frac{m-1}{m}$。\n因此，梯度幅值的最紧上界是：\n$$ \\max_i \\left| \\frac{\\partial L}{\\partial x_{i}} \\right| \\leq \\frac{|\\gamma|}{\\sqrt{\\epsilon}} \\left( 2G \\frac{m-1}{m} \\right) $$\n\n### 第 3 部分：求解最小的 $\\epsilon$\n\n我们被给予的要求是，在 $\\sigma_g^2 \\to 0$ 的极限下，$\\max_i \\left| \\frac{\\partial L}{\\partial x_{i}} \\right| \\leq \\tau$。使用第 2 部分得到的最紧界限，我们建立不等式：\n$$ \\frac{2G |\\gamma|}{\\sqrt{\\epsilon}} \\frac{m-1}{m} \\leq \\tau $$\n为了找到保证在任何有效的传入梯度配置下都满足此条件的最小 $\\epsilon > 0$，我们求解 $\\epsilon$：\n$$ \\sqrt{\\epsilon} \\geq \\frac{2G |\\gamma|}{\\tau} \\frac{m-1}{m} $$\n该不等式定义了 $\\sqrt{\\epsilon}$ 的一个下界。当这个不等式取等号时，得到 $\\epsilon$ 的最小值。两边平方，得到所需的最小 $\\epsilon$：\n$$ \\epsilon = \\left( \\frac{2G |\\gamma|}{\\tau} \\frac{m-1}{m} \\right)^2 = \\frac{4 G^2 \\gamma^2}{\\tau^2} \\left( 1 - \\frac{1}{m} \\right)^2 $$\n\n### 第 4 部分：提出选择 $\\epsilon$ 的自适应策略\n\n不稳定性源于梯度表达式中的项 $\\frac{1}{\\sqrt{\\sigma_g^2 + \\epsilon}}$，如果 $\\sigma_g^2$ 和 $\\epsilon$ 都接近于零，该项可能会爆炸。第 3 部分得到的固定 $\\epsilon$ 提供了一个最坏情况下的保证，但可能过于保守。一种自适应策略可以根据当前条件设置 $\\epsilon$，使其刚好足够大以确保稳定性。\n\n一个有原则的自适应策略是动态地强制执行第 3 部分的梯度约束。在对给定群组的每次反向传播中，我们可以访问传入的梯度 $g_j = \\frac{\\partial L}{\\partial y_j}$ 和参数 $\\gamma$。我们可以计算驱动梯度幅值的项：$C_{\\max} = |\\gamma| \\max_j |g_j - \\bar{g}|$。\n梯度幅值的主要部分近似为 $\\frac{C_{\\max}}{\\sqrt{\\sigma_g^2 + \\epsilon}}$。为了将其限制在阈值 $\\tau$ 以下，我们需要：\n$$ \\frac{C_{\\max}}{\\sqrt{\\sigma_g^2 + \\epsilon}} \\leq \\tau \\implies \\sigma_g^2 + \\epsilon \\geq \\left( \\frac{C_{\\max}}{\\tau} \\right)^2 $$\n这引出了一个策略：设置 $\\epsilon$ 以等式形式满足此条件，同时确保其不为负，并包含一个小的下限以增强鲁棒性：\n$$ \\epsilon = \\max\\left(\\epsilon_{\\text{floor}}, \\left(\\frac{C_{\\max}}{\\tau}\\right)^2 - [\\sigma_g^2]_{\\text{sg}}\\right) $$\n在这里，$\\epsilon_{\\text{floor}}$ 是一个小的正常数（例如，$10^{-8}$），$\\tau$ 是所需梯度范数上限的超参数，$[\\sigma_g^2]_{\\text{sg}}$ 表示在计算 $\\epsilon$ 时使用批方差 $\\sigma_g^2$ 的值，而不通过它传播梯度（停止梯度操作）。\n\n这个策略之所以有效，是因为：\n1. 如果 $\\sigma_g^2$ 很大（即 $(\\frac{C_{\\max}}{\\tau})^2 \\leq \\sigma_g^2$），$\\epsilon$ 默认为 $\\epsilon_{\\text{floor}}$。梯度分母 $\\sqrt{\\sigma_g^2+\\epsilon}$ 很大，因此梯度很小且稳定。\n2. 如果 $\\sigma_g^2$ 很小（即 $(\\frac{C_{\\max}}{\\tau})^2 > \\sigma_g^2$），$\\epsilon$ 的选择使得 $\\sigma_g^2 + \\epsilon = (C_{\\max}/\\tau)^2$。梯度分母变为 $\\sqrt{(C_{\\max}/\\tau)^2} = C_{\\max}/\\tau$。因此，梯度幅值被限制在大约 $\\frac{C_{\\max}}{C_{\\max}/\\tau} = \\tau$，从而防止了爆炸。\n这种对 $\\epsilon$ 的自适应选择直接针对数值不稳定性的根源。",
            "answer": "$$\n\\boxed{\\frac{4G^2\\gamma^2}{\\tau^2}\\left(\\frac{m-1}{m}\\right)^2}\n$$"
        },
        {
            "introduction": "标准的归一化方法（如 GN）依赖于均值和方差的经典估计，这使得它们对数据中的异常值非常敏感。本练习将带您超越标准 GN，通过实现并比较一个基于均值中位数 (Median-of-Means) 估计器的“鲁棒”归一化方法，来动手实践如何处理病态数据。通过在包含极端异常值的特定测试案例上进行分析，您将直观地看到鲁棒统计量如何提升归一化在复杂数据场景下的稳定性和有效性。",
            "id": "3134016",
            "problem": "考虑一个表示深度神经网络中特征图小批量的四维张量。设该张量表示为 $X \\in \\mathbb{R}^{N \\times C \\times H \\times W}$，其中批量大小为 $N$，通道数为 $C$，空间维度为 $H$ 和 $W$。任务是实现、分析和比较两种应用于每个样本和每组通道的归一化方案：组归一化（Group Normalization, GN）和一种鲁棒的替代方案，该方案使用均值中位数（Median-of-Means, MoM）替换经典估计量来计算均值和二阶矩。\n\n您必须从以下基本原理出发推导归一化变换：\n\n-   有限实数集 $\\{x_i\\}_{i=1}^m$ 的算术平均值定义为 $\\mu = \\frac{1}{m} \\sum_{i=1}^m x_i$。\n-   （总体）方差定义为 $\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^2$。\n-   原点二阶矩为 $M_2 = \\frac{1}{m} \\sum_{i=1}^m x_i^2$，且满足 $\\sigma^2 = M_2 - \\mu^2$。\n\n分组配置将 $C$ 个通道划分为 $G$ 个大小相等（为 $\\frac{C}{G}$）的非重叠组（假设 $C$ 可被 $G$ 整除）。对于每个样本 $n \\in \\{1,\\dots,N\\}$ 和每个组 $g \\in \\{1,\\dots,G\\}$，收集该组中所有通道及其空间位置上的所有元素；将此多重集表示为 $\\mathcal{S}_{n,g}$，其基数表示为 $m = |\\mathcal{S}_{n,g}|$。组归一化使用在 $\\mathcal{S}_{n,g}$ 上计算的算术平均值和方差，将 $X$ 的元素映射到一个无量纲表示，使得其组内均值为 $0$，组内二阶中心矩等于 $1$（不考虑数值稳定性常数 $\\varepsilon > 0$）。鲁棒替代方案将算术平均值替换为均值中位数估计量，将二阶矩替换为 $M_2$ 的均值中位数估计，然后通过 $\\widehat{\\sigma}^2_{\\text{robust}} = \\max\\{ \\widehat{M}_2 - \\widehat{\\mu}^2, 0 \\}$ 构建一个鲁棒方差，同样使用数值稳定性常数 $\\varepsilon$。\n\n在 $\\mathcal{S}_{n,g}$ 上的均值中位数（MoM）过程定义如下。以固定的确定性顺序将 $\\mathcal{S}_{n,g}$ 扁平化为一个序列 $(x_1,\\dots,x_m)$。选择一个整数块数 $k$，满足 $k = \\min\\{3, m\\}$，并将序列划分为 $k$ 个大小尽可能相等的连续块。对于均值，计算各块的均值，然后取它们的中位数以获得 $\\widehat{\\mu}$。对于二阶矩，计算 $x_i^2$ 的各块平均值，然后取它们的中位数以获得 $\\widehat{M}_2$。\n\n实现这两种归一化方法时，不进行任何仿射重缩放或添加偏置（即，使用 $\\gamma = 1$ 和 $\\beta = 0$），并使用固定的数值稳定性常数 $\\varepsilon = 10^{-5}$。\n\n构建以下确定性测试套件。在所有情况下，$N = 1$，并且 MoM 分区中每个组内元素的排序是默认的行主序扁平化，其中通道变化最慢，空间维度变化最快。\n\n-   测试用例 1（组内的病态主导）：$C = 4$, $H = 2$, $W = 2$, $G = 2$。通道 $\\{0,1\\}$ 构成组 0，通道 $\\{2,3\\}$ 构成组 1。定义空间符号模式 $s(h,w) = 1$（如果 $(h + w)$ 是偶数）和 $s(h,w) = -1$（如果 $(h+w)$ 是奇数），对于 $h \\in \\{0,1\\}$ 和 $w \\in \\{0,1\\}$。将通道值设置为：\n    -   通道 0：$x_{0,h,w} = \\mathbf{1}\\{(h + w) \\text{ is odd}\\}$，\n    -   通道 1：$x_{1,h,w} = 1000 \\cdot s(h,w)$，\n    -   通道 2：$x_{2,h,w} = 0.5 \\cdot s(h,w)$，\n    -   通道 3：$x_{3,h,w} = -1 \\cdot s(h,w)$。\n    使用离群值通道索引 $c_{\\text{out}} = 1$ 和小方差通道索引 $c_{\\text{small}} = 0$。\n\n-   测试用例 2（平衡方差，无极端离群值）：$C = 4$, $H = 2$, $W = 2$, $G = 2$。使用相同的 $s(h,w)$ 并设置振幅 $a_0 = 1$, $a_1 = 1.2$, $a_2 = 0.8$, $a_3 = 1.5$，其中\n    -   通道 $c$：$x_{c,h,w} = a_c \\cdot s(h,w)$，对于 $c \\in \\{0,1,2,3\\}$。\n    使用 $c_{\\text{out}} = 1$ 和 $c_{\\text{small}} = 2$。\n\n-   测试用例 3（边界：组大小为 1）：$C = 4$, $H = 2$, $W = 2$, $G = 4$。使用与测试用例 1 中相同的通道定义。这里每个组只包含一个通道。使用 $c_{\\text{out}} = 1$ 和 $c_{\\text{small}} = 0$。\n\n-   测试用例 4（每组元素最少）：$C = 2$, $H = 1$, $W = 1$, $G = 1$。设置\n    -   通道 0：$x_{0,0,0} = 0.1$，\n    -   通道 1：$x_{1,0,0} = 1000$。\n    使用 $c_{\\text{out}} = 1$ 和 $c_{\\text{small}} = 0$。\n\n对于每个测试用例，计算四个量：\n-   $r_{\\text{den}}$：包含 $c_{\\text{out}}$ 的组的分母之比 $\\frac{\\sqrt{\\sigma^2_{\\text{GN}} + \\varepsilon}}{\\sqrt{\\widehat{\\sigma}^2_{\\text{MoM}} + \\varepsilon}}$，\n-   $r_{\\text{out}}$：离群值通道的最大绝对归一化值之比 $\\frac{\\max |Y_{\\text{GN}}(c_{\\text{out}})|}{\\max |Y_{\\text{MoM}}(c_{\\text{out}})|}$，\n-   $r_{\\text{small}}$：小方差通道的最大绝对归一化值之比 $\\frac{\\max |Y_{\\text{GN}}(c_{\\text{small}})|}{\\max |Y_{\\text{MoM}}(c_{\\text{small}})|}$，\n-   $d_{\\mu}$：包含 $c_{\\text{out}}$ 的组的均值估计量之间的绝对差 $| \\mu_{\\text{GN}} - \\widehat{\\mu}_{\\text{MoM}} |$。\n\n所有这四个量都必须是实数。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个测试用例贡献一个包含四个值的列表，顺序为 $[r_{\\text{den}}, r_{\\text{out}}, r_{\\text{small}}, d_{\\mu}]$。例如，输出格式必须类似于 $[[v_{1,1}, v_{1,2}, v_{1,3}, v_{1,4}], [v_{2,1}, v_{2,2}, v_{2,3}, v_{2,4}], [v_{3,1}, v_{3,2}, v_{3,3}, v_{3,4}], [v_{4,1}, v_{4,2}, v_{4,3}, v_{4,4}]]$，其中包含您的实现所产生的精确数值。",
            "solution": "当前任务要求实现并比较分析两种用于深度学习特征图的归一化方案：标准的组归一化（GN）和一种基于均值中位数（MoM）估计量的鲁棒变体。分析将在一套确定性测试套件上进行，该套件旨在评估这些方法的行为，特别是在存在离群值的情况下。\n\n### 1. 理论基础\n\n设 $X \\in \\mathbb{R}^{N \\times C \\times H \\times W}$ 是一个表示小批量特征图的张量。两种归一化方案都作用于通道组。$C$ 个通道被划分为 $G$ 个组。对于每个样本 $n$ 和组 $g$，我们定义一个多重集 $\\mathcal{S}_{n,g}$，其中包含该组中所有通道的激活值及其空间维度。该集合的大小为 $m = (C/G) \\cdot H \\cdot W$。归一化的目标是标准化每个集合 $\\mathcal{S}_{n,g}$ 内数值分布的矩（例如均值和方差）。\n\n#### 1.1. 标准组归一化 (GN)\n\n组归一化作为一种标准技术，使用经典的统计估计量，这些估计量虽然高效但对离群值敏感。\n\n集合 $\\mathcal{S}_{n,g}$（其元素表示为 $x_i$）的均值和方差计算如下：\n-   **算术平均值**：一阶原点矩（均值）的估计量是样本均值：\n    $$ \\mu_{\\text{GN}} = \\frac{1}{m} \\sum_{i=1}^m x_i $$\n-   **总体方差**：二阶中心矩（方差）的估计量是：\n    $$ \\sigma^2_{\\text{GN}} = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_{\\text{GN}})^2 $$\n    为了计算稳定性和效率，这通常使用二阶原点矩 $M_{2, \\text{GN}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i^2$，通过恒等式 $\\sigma^2_{\\text{GN}} = M_{2, \\text{GN}} - \\mu_{\\text{GN}}^2$ 来计算。\n\n对每个元素 $x_i \\in \\mathcal{S}_{n,g}$ 的归一化变换为：\n$$ y_i = \\frac{x_i - \\mu_{\\text{GN}}}{\\sqrt{\\sigma^2_{\\text{GN}} + \\varepsilon}} $$\n其中 $\\varepsilon$ 是一个小的常数（在此问题中为 $10^{-5}$），用于增加数值稳定性以防止除以零。此变换将每组中的激活值重新缩放，使其均值近似为 $0$，方差近似为 $1$。\n\n#### 1.2. 使用均值中位数（MoM）的鲁棒组归一化\n\n为了提高对离群值的鲁棒性，可以用鲁棒的替代方案替换经典的均值和方差估计量。均值中位数（MoM）是一种简单而有效的均值鲁棒估计量。\n\n对于一个包含 $m$ 个值的集合，MoM 过程如下：\n1.  将 $m$ 个值的集合划分为 $k$ 个非重叠的块，其中 $k$ 被指定为 $k = \\min\\{3, m\\}$。进行分区以使块的大小尽可能相等。\n2.  计算 $k$ 个块中每个块内值的算术平均值。\n3.  MoM 估计量是这 $k$ 个块均值的中位数。\n\n应用此原理，我们定义均值和二阶矩的鲁棒估计量：\n-   **MoM 均值估计量 ($\\widehat{\\mu}_{\\text{MoM}}$)**：数据 $\\{x_i\\}_{i=1}^m$ 被划分为 $k$ 个块。设 $B_1, \\dots, B_k$ 为这些块。均值的 MoM 估计为：\n    $$ \\widehat{\\mu}_{\\text{MoM}} = \\text{median}\\left( \\left\\{ \\text{mean}(B_j) \\right\\}_{j=1}^k \\right) $$\n-   **MoM 二阶矩估计量 ($\\widehat{M}_{2, \\text{MoM}}$)**：将相同的过程应用于平方数据 $\\{x_i^2\\}_{i=1}^m$。将它们划分为 $k$ 个块，二阶矩的 MoM 估计是平方数据块平均值的中位数。\n-   **鲁棒方差估计量 ($\\widehat{\\sigma}^2_{\\text{MoM}}$)**：使用替代原则，通过矩的鲁棒估计量构建一个鲁棒方差估计：\n    $$ \\widehat{\\sigma}^2_{\\text{MoM}} = \\max\\left( \\widehat{M}_{2, \\text{MoM}} - \\widehat{\\mu}_{\\text{MoM}}^2, 0 \\right) $$\n    $\\max(\\cdot, 0)$ 操作确保了非负性，这可能因估计误差而被违反。\n\n对每个元素 $x_i \\in \\mathcal{S}_{n,g}$ 的鲁棒归一化变换与 GN 类似：\n$$ y_i = \\frac{x_i - \\widehat{\\mu}_{\\text{MoM}}}{\\sqrt{\\widehat{\\sigma}^2_{\\text{MoM}} + \\varepsilon}} $$\n\n### 2. 算法实现与分析\n\n实现遵循上述定义。对于每个测试用例，构造一个输入张量 $X$。两个独立的函数分别应用 GN 和基于 MoM 的归一化方案。每个函数遍历 $G$ 个通道组。对于给定的组，它从 $X$ 中提取相应的数据切片，将其扁平化为一维数组，并计算所需的统计量（$\\mu_{\\text{GN}}, \\sigma^2_{\\text{GN}}$ 或 $\\widehat{\\mu}_{\\text{MoM}}, \\widehat{\\sigma}^2_{\\text{MoM}}$）。然后使用这些统计量对该组中的数据进行归一化。\n\nMoM实现中的一个关键细节是块的划分。对于大小为 $m$ 和 $k$ 个块的扁平化数组，使用了 `numpy.array_split`，它创建大小尽可能相等的块，符合问题规范。中位数使用 `numpy.median` 计算，它能正确处理奇数和偶数个数的块统计量。\n\n通过为每个测试用例计算四个指标来进行分析：\n-   $d_{\\mu} = |\\mu_{\\text{GN}} - \\widehat{\\mu}_{\\text{MoM}}|$：这直接衡量了包含离群值通道的组的经典均值估计和鲁棒均值估计之间的差异。一个较大的值表明离群值显著扭曲了经典均值。\n-   $r_{\\text{den}} = \\sqrt{\\sigma^2_{\\text{GN}} + \\varepsilon} / \\sqrt{\\widehat{\\sigma}^2_{\\text{MoM}} + \\varepsilon}$：这比较了尺度估计（归一化分母）。该比率表明与经典方法相比，鲁棒方法感知到的数据离散程度是更大还是更小。\n-   $r_{\\text{out}} = \\max |Y_{\\text{GN}}(c_{\\text{out}})| / \\max |Y_{\\text{MoM}}(c_{\\text{out}})|$：该比率衡量了离群值通道归一化后的相对幅度。大于 $1$ 的值表明基于 MoM 的方法在抑制离群值幅度方面更有效。\n-   $r_{\\text{small}} = \\max |Y_{\\text{GN}}(c_{\\text{small}})| / \\max |Y_{\\text{MoM}}(c_{\\text{small}})|$：这评估了归一化方案对一个“常规”通道的影响。一个理想的鲁棒方法应对非离群数据的影响最小化。\n\n测试用例旨在探究不同的行为：\n-   **测试用例 1** 引入了一个强烈的离群值，以突显鲁棒性的核心差异。\n-   **测试用例 2** 使用没有极端离群值的平衡数据，预计两种方法会产生相似的结果。\n-   **测试用例 3 和 4** 探索了与每组元素数量 ($m$) 相关的边界条件及其对 MoM 估计量的影响，特别是当 $k  3$ 时。当 $m=2, k=2$ 时，MoM的均值和二阶矩估计量与其经典对应物相同，因为两个值的中位数就是它们的平均值。当 $m=4, k=3$ 时，两种方法之间的差异取决于数据分布以及分区的形成方式。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    \n    test_cases_params = [\n        # (C, H, W, G, c_out, c_small, data_generator_id)\n        (4, 2, 2, 2, 1, 0, 1),\n        (4, 2, 2, 2, 1, 2, 2),\n        (4, 2, 2, 4, 1, 0, 1),\n        (2, 1, 1, 1, 1, 0, 3)\n    ]\n\n    EPSILON = 1e-5\n\n    def get_mom_stats(data: np.ndarray):\n        \"\"\"Computes Median-of-Means for mean and second moment.\"\"\"\n        m = data.shape[0]\n        k = min(3, m)\n        \n        # Partition data for mean estimation\n        blocks = np.array_split(data, k)\n        block_means = [np.mean(b) for b in blocks]\n        mu_mom = np.median(block_means)\n        \n        # Partition squared data for M2 estimation\n        squared_data = np.square(data)\n        sq_blocks = np.array_split(squared_data, k)\n        block_m2s = [np.mean(b) for b in sq_blocks]\n        m2_mom = np.median(block_m2s)\n        \n        return mu_mom, m2_mom\n\n    def apply_norm(X: np.ndarray, G: int, method: str):\n        \"\"\"Applies Group Normalization using the specified method.\"\"\"\n        N, C, H, W = X.shape\n        Y = np.zeros_like(X, dtype=np.float64)\n        stats = {}\n        \n        channels_per_group = C // G\n        if C % G != 0:\n            raise ValueError(\"C must be divisible by G.\")\n        \n        for g in range(G):\n            ch_start = g * channels_per_group\n            ch_end = (g + 1) * channels_per_group\n            \n            group_data = X[:, ch_start:ch_end, :, :].flatten()\n            \n            if method == 'gn':\n                mu = np.mean(group_data)\n                # Population variance\n                var = np.var(group_data)\n            elif method == 'mom':\n                mu, m2 = get_mom_stats(group_data)\n                var = max(m2 - mu**2, 0)\n            else:\n                raise ValueError(\"Unknown method.\")\n            \n            stats[g] = {'mean': mu, 'var': var}\n            \n            group_slice = X[:, ch_start:ch_end, :, :]\n            Y[:, ch_start:ch_end, :, :] = (group_slice - mu) / np.sqrt(var + EPSILON)\n            \n        return Y, stats\n\n    def create_tensor(C, H, W, gen_id):\n        \"\"\"Generates the input tensor for a test case.\"\"\"\n        X = np.zeros((1, C, H, W), dtype=np.float64)\n        \n        if gen_id == 1 or gen_id == 2:\n            s = np.zeros((H, W))\n            for h in range(H):\n                for w in range(W):\n                    s[h,w] = 1 if (h + w) % 2 == 0 else -1\n\n            if gen_id == 1:\n                # Test Case 1  3 data\n                X[0, 0, :, :] = np.fromfunction(lambda h, w: (h + w) % 2, (H, W))\n                X[0, 1, :, :] = 1000.0 * s\n                X[0, 2, :, :] = 0.5 * s\n                X[0, 3, :, :] = -1.0 * s\n            elif gen_id == 2:\n                # Test Case 2 data\n                amplitudes = [1.0, 1.2, 0.8, 1.5]\n                for c in range(C):\n                    X[0, c, :, :] = amplitudes[c] * s\n        elif gen_id == 3:\n            # Test Case 4 data\n            X[0, 0, 0, 0] = 0.1\n            X[0, 1, 0, 0] = 1000.0\n            \n        return X\n\n    final_results = []\n    for C, H, W, G, c_out, c_small, gen_id in test_cases_params:\n        X = create_tensor(C, H, W, gen_id)\n        \n        channels_per_group = C // G\n        group_out_idx = c_out // channels_per_group\n        \n        # Apply both normalization methods\n        Y_gn, stats_gn = apply_norm(X, G, 'gn')\n        Y_mom, stats_mom = apply_norm(X, G, 'mom')\n        \n        # Extract stats for the outlier group\n        mu_gn = stats_gn[group_out_idx]['mean']\n        var_gn = stats_gn[group_out_idx]['var']\n        mu_mom = stats_mom[group_out_idx]['mean']\n        var_mom = stats_mom[group_out_idx]['var']\n\n        # Calculate metrics\n        d_mu = np.abs(mu_gn - mu_mom)\n        \n        denom_gn = np.sqrt(var_gn + EPSILON)\n        denom_mom = np.sqrt(var_mom + EPSILON)\n        r_den = denom_gn / denom_mom if denom_mom != 0 else np.inf\n        \n        max_abs_gn_out = np.max(np.abs(Y_gn[0, c_out, :, :]))\n        max_abs_mom_out = np.max(np.abs(Y_mom[0, c_out, :, :]))\n        r_out = max_abs_gn_out / max_abs_mom_out if max_abs_mom_out != 0 else np.inf\n\n        max_abs_gn_small = np.max(np.abs(Y_gn[0, c_small, :, :]))\n        max_abs_mom_small = np.max(np.abs(Y_mom[0, c_small, :, :]))\n        r_small = max_abs_gn_small / max_abs_mom_small if max_abs_mom_small != 0 else np.inf\n        \n        case_results = [r_den, r_out, r_small, d_mu]\n        final_results.append(case_results)\n\n    # Convert results to a string in the required list-of-lists format\n    result_str = \"[\" + \", \".join([f\"[{', '.join(map(str, res))}]\" for res in final_results]) + \"]\"\n    print(result_str)\n\nsolve()\n\n```"
        }
    ]
}