{
    "hands_on_practices": [
        {
            "introduction": "To truly understand a building block like Group Normalization (GN), we must first investigate its fundamental mathematical properties. This exercise guides you to explore how GN responds to a simple scaling of its input, a property known as positive 1-homogeneity. By combining theoretical derivation with empirical verification, you will uncover the precise, and surprisingly strict, conditions under which the identity $\\mathrm{GN}(a x) = a \\cdot \\mathrm{GN}(x)$ holds, providing a crucial insight into the operator's behavior within a deep neural network .",
            "id": "3134049",
            "problem": "You are given the definition of Group Normalization (GN) in deep learning. Let an input tensor be denoted by $x \\in \\mathbb{R}^{N \\times C \\times H \\times W}$. Let $G$ be a positive integer such that $G$ divides $C$. Define $C_{\\mathrm{grp}} = C / G$. For each sample index $n \\in \\{0,1,\\dots,N-1\\}$ and each group index $g \\in \\{0,1,\\dots,G-1\\}$, define the group set of indices\n$$\n\\mathcal{I}_{n,g} = \\{(n, c, h, w) \\mid c \\in \\{g \\cdot C_{\\mathrm{grp}}, \\dots, (g+1) \\cdot C_{\\mathrm{grp}} - 1\\},\\ h \\in \\{0,\\dots,H-1\\},\\ w \\in \\{0,\\dots,W-1\\} \\}.\n$$\nDefine the groupwise mean and variance as\n$$\n\\mu_{n,g}(x) = \\frac{1}{|\\mathcal{I}_{n,g}|} \\sum_{(n,c,h,w) \\in \\mathcal{I}_{n,g}} x_{n,c,h,w}, \\quad\n\\sigma^2_{n,g}(x) = \\frac{1}{|\\mathcal{I}_{n,g}|} \\sum_{(n,c,h,w) \\in \\mathcal{I}_{n,g}} \\left(x_{n,c,h,w} - \\mu_{n,g}(x)\\right)^2.\n$$\nGiven a small nonnegative constant $\\varepsilon \\ge 0$, the normalized tensor $z$ is defined elementwise by\n$$\nz_{n,c,h,w} = \\frac{x_{n,c,h,w} - \\mu_{n,g(c)}(x)}{\\sqrt{\\sigma^2_{n,g(c)}(x) + \\varepsilon}},\n$$\nwhere $g(c) = \\left\\lfloor \\frac{c}{C_{\\mathrm{grp}}} \\right\\rfloor$. With learnable per-channel affine parameters $\\gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$, the Group Normalization output is\n$$\n\\mathrm{GN}(x)_{n,c,h,w} = \\gamma_c \\, z_{n,c,h,w} + \\beta_c.\n$$\n\nYour tasks are:\n1) Using only the definitions above and the well-known scaling properties of the mean and variance (for any scalar $a \\in \\mathbb{R}$, $\\mu_{n,g}(a x) = a \\, \\mu_{n,g}(x)$ and $\\sigma^2_{n,g}(a x) = a^2 \\, \\sigma^2_{n,g}(x)$), derive necessary and sufficient conditions on $(\\varepsilon,\\gamma,\\beta)$ under which the mapping $x \\mapsto \\mathrm{GN}(x)$ is positively $1$-homogeneous, meaning that for every $a>0$ and every $x$,\n$$\n\\mathrm{GN}(a x) = a \\cdot \\mathrm{GN}(x).\n$$\nYou must reason from the given definitions and fundamental properties without appealing to any unproven shortcut formula.\n\n2) Implement a program that constructs a fixed test tensor and empirically checks whether the above equality holds within a fixed absolute tolerance for a set of test cases that vary the grouping, the scale $a$, and the parameters $(\\varepsilon,\\gamma,\\beta)$. The implementation details to ensure reproducibility are:\n- Use a tensor with shape $(N,C,H,W)$ where $N=2$, $C=4$, $H=2$, $W=2$.\n- Sample the entries of $x$ independently from the standard normal distribution using a pseudorandom number generator seeded with $7$.\n- For each test case, compute $\\mathrm{GN}(x)$ and $\\mathrm{GN}(a x)$ using the specified $G$, $\\varepsilon$, $\\gamma$, and $\\beta$, and then test whether $\\mathrm{GN}(a x)$ equals $a \\cdot \\mathrm{GN}(x)$ within absolute tolerance $\\tau = 10^{-9}$.\n\nThe test suite to be used is:\n- Case $1$: $G=2$, $a=2$, $\\varepsilon=0$, $\\gamma=[1,\\,-\\tfrac{1}{2},\\,2,\\,\\tfrac{3}{10}]$, $\\beta=[0,\\,0,\\,0,\\,0]$.\n- Case $2$: $G=2$, $a=1$, $\\varepsilon=10^{-5}$, $\\gamma=[1,\\,-\\tfrac{1}{2},\\,2,\\,\\tfrac{3}{10}]$, $\\beta=[\\tfrac{1}{10},\\,-\\tfrac{1}{5},\\,0,\\,\\tfrac{1}{2}]$.\n- Case $3$: $G=4$, $a=2$, $\\varepsilon=0$, $\\gamma=[0,\\,0,\\,0,\\,0]$, $\\beta=[0,\\,0,\\,0,\\,0]$.\n- Case $4$: $G=1$, $a=2$, $\\varepsilon=0$, $\\gamma=[1,\\,-\\tfrac{1}{2},\\,2,\\,\\tfrac{3}{10}]$, $\\beta=[\\tfrac{1}{4},\\,\\tfrac{1}{4},\\,\\tfrac{1}{4},\\,\\tfrac{1}{4}]$.\n- Case $5$: $G=1$, $a=2$, $\\varepsilon=10^{-5}$, $\\gamma=[1,\\,-\\tfrac{1}{2},\\,2,\\,\\tfrac{3}{10}]$, $\\beta=[0,\\,0,\\,0,\\,0]$.\n- Case $6$: $G=2$, $a=100$, $\\varepsilon=10^{-5}$, $\\gamma=[1,\\,-\\tfrac{1}{2},\\,2,\\,\\tfrac{3}{10}]$, $\\beta=[0,\\,0,\\,0,\\,0]$.\n\nFor each case, your program must output a boolean indicating whether the equality $\\mathrm{GN}(a x) = a \\cdot \\mathrm{GN}(x)$ holds within the tolerance $\\tau$. The final output format must be a single line containing a comma-separated list of these booleans enclosed in square brackets, for example, \"[True,False,True,False,True,False]\". No angles or physical units are involved in this problem. Your code must not require any user input.",
            "solution": "The problem asks for two tasks: first, to derive the necessary and sufficient conditions on the parameters $(\\varepsilon, \\gamma, \\beta)$ for the Group Normalization (GN) operator to be positively $1$-homogeneous. Second, to implement an empirical test of this property for a given set of parameters.\n\n### Part 1: Derivation of Homogeneity Conditions\n\nThe condition for a function $f$ to be positively $1$-homogeneous is $f(ax) = a f(x)$ for all scalable inputs $x$ and any scalar $a > 0$. In our case, $f(x) = \\mathrm{GN}(x)$. We must find the necessary and sufficient conditions on $(\\varepsilon, \\gamma, \\beta)$ for this equality to hold for all input tensors $x \\in \\mathbb{R}^{N \\times C \\times H \\times W}$.\n\nLet us analyze the left-hand side (LHS) and right-hand side (RHS) of the homogeneity equation, $\\mathrm{GN}(ax) = a \\cdot \\mathrm{GN}(x)$. We will consider a single element of the output tensor at indices $(n,c,h,w)$. Let $g = g(c) = \\lfloor c / (C/G) \\rfloor$ be the group index for channel $c$. We also suppress the arguments of $\\mu$ and $\\sigma^2$ for brevity where the context is clear, e.g., $\\mu_{n,g} \\equiv \\mu_{n,g}(x)$.\n\nThe RHS is straightforwardly expanded from the definition of $\\mathrm{GN}(x)$:\n$$\n\\text{RHS} = a \\cdot \\mathrm{GN}(x)_{n,c,h,w} = a \\left( \\gamma_c \\frac{x_{n,c,h,w} - \\mu_{n,g}}{\\sqrt{\\sigma^2_{n,g} + \\varepsilon}} + \\beta_c \\right) = a \\gamma_c \\frac{x_{n,c,h,w} - \\mu_{n,g}}{\\sqrt{\\sigma^2_{n,g} + \\varepsilon}} + a \\beta_c.\n$$\n\nFor the LHS, we first evaluate the mean and variance for the scaled input $ax$. Using the provided scaling properties $\\mu_{n,g}(ax) = a \\mu_{n,g}(x)$ and $\\sigma^2_{n,g}(ax) = a^2 \\sigma^2_{n,g}(x)$:\n$$\n\\mathrm{GN}(ax)_{n,c,h,w} = \\gamma_c \\frac{(ax)_{n,c,h,w} - \\mu_{n,g}(ax)}{\\sqrt{\\sigma^2_{n,g}(ax) + \\varepsilon}} + \\beta_c = \\gamma_c \\frac{a \\cdot x_{n,c,h,w} - a \\cdot \\mu_{n,g}}{\\sqrt{a^2 \\sigma^2_{n,g} + \\varepsilon}} + \\beta_c.\n$$\n$$\n\\text{LHS} = \\gamma_c \\frac{a (x_{n,c,h,w} - \\mu_{n,g})}{\\sqrt{a^2 \\sigma^2_{n,g} + \\varepsilon}} + \\beta_c.\n$$\n\nNow, we set LHS = RHS, which must hold for all $x$ and all $a > 0$:\n$$\n\\gamma_c \\frac{a (x_{n,c,h,w} - \\mu_{n,g})}{\\sqrt{a^2 \\sigma^2_{n,g} + \\varepsilon}} + \\beta_c = a \\gamma_c \\frac{x_{n,c,h,w} - \\mu_{n,g}}{\\sqrt{\\sigma^2_{n,g} + \\varepsilon}} + a \\beta_c.\n$$\nRearranging the terms, we get:\n$$\na \\gamma_c (x_{n,c,h,w} - \\mu_{n,g}) \\left( \\frac{1}{\\sqrt{a^2 \\sigma^2_{n,g} + \\varepsilon}} - \\frac{1}{\\sqrt{\\sigma^2_{n,g} + \\varepsilon}} \\right) = (a-1)\\beta_c.\n$$\n\nThis equation must hold for all input tensors $x$. The right-hand side, $(a-1)\\beta_c$, is a constant with respect to $x$. The left-hand side, however, depends on $x$ through the terms $(x_{n,c,h,w} - \\mu_{n,g})$ and $\\sigma^2_{n,g}(x)$. For an equality between a function of $x$ and a constant to hold for all $x$, the function itself must be constant.\n\nLet's test the LHS with a specific input. Consider an input tensor $x_0$ that is constant within each group $(n,g)$. For such a tensor, $\\sigma^2_{n,g}(x_0) = 0$ and $x_{n,c,h,w} - \\mu_{n,g}(x_0) = 0$. In this case, the LHS of the equation evaluates to $0$. Therefore, the constant on the RHS must also be $0$:\n$$\n(a-1)\\beta_c = 0.\n$$\nSince this must hold for all $a > 0$, we can choose any $a \\neq 1$ (e.g., $a=2$), which leads to the conclusion that $\\beta_c = 0$. This must be true for all channels $c \\in \\{0, \\dots, C-1\\}$. Thus, a necessary condition for positive $1$-homogeneity is $\\beta = \\vec{0}$.\n\nSubstituting $\\beta = \\vec{0}$ back into our rearranged equation gives:\n$$\na \\gamma_c (x_{n,c,h,w} - \\mu_{n,g}) \\left( \\frac{1}{\\sqrt{a^2 \\sigma^2_{n,g} + \\varepsilon}} - \\frac{1}{\\sqrt{\\sigma^2_{n,g} + \\varepsilon}} \\right) = 0.\n$$\nThis equality must hold for all $x$, all $a > 0$, and all indices. This occurs if at least one of the factors is zero.\n1.  $\\gamma_c = 0$: If the scaling parameter for a channel is zero, the equation holds trivially ($0=0$). In this case, $\\mathrm{GN}(x)_{n,c,h,w} = 0 \\cdot z_{n,c,h,w} + 0 = 0$. The homogeneity condition becomes $0 = a \\cdot 0$, which is always true.\n2.  $x_{n,c,h,w} - \\mu_{n,g} = 0$: This is not true for all inputs $x$ and all indices, so we cannot rely on it.\n3.  The term in the parenthesis is zero:\n    $$\n    \\frac{1}{\\sqrt{a^2 \\sigma^2_{n,g} + \\varepsilon}} = \\frac{1}{\\sqrt{\\sigma^2_{n,g} + \\varepsilon}}.\n    $$\n    Squaring both sides yields $a^2 \\sigma^2_{n,g} + \\varepsilon = \\sigma^2_{n,g} + \\varepsilon$, which simplifies to:\n    $$\n    (a^2 - 1) \\sigma^2_{n,g} = 0.\n    $$\n    This must hold for all $a > 0$ (e.g., $a=2$) and for all possible values of $\\sigma^2_{n,g}$ (which can be non-zero). This is a contradiction unless $\\varepsilon \\to \\infty$ or the term is not as it seems.\n\nLet's re-examine the logic. The equality must hold for any $x$ (thus any $\\sigma_g^2 \\ge 0$) and any $a > 0$. If we choose $a \\ne 1$ and an $x$ such that $\\sigma_g^2 > 0$, then $(a^2-1)\\sigma_g^2 \\ne 0$. This means the parenthetical term is non-zero in general.\nTherefore, for the product to be zero, we must have $\\gamma_c (x_{n,c,h,w} - \\mu_{n,g}) = 0$. Since this must be true for any $x$, including those where $x_{n,c,h,w} \\ne \\mu_{n,g}$, we must conclude that $\\gamma_c=0$.\n\nSo, for any given channel $c$, the property holds for all $x$ and $a>0$ only if $\\gamma_c=0$ and $\\beta_c=0$.\nThese conditions are also sufficient. If $\\gamma = \\vec{0}$ and $\\beta = \\vec{0}$, then $\\mathrm{GN}(x)$ is the zero tensor for any input $x$. The homogeneity condition $\\mathrm{GN}(ax) = a \\cdot \\mathrm{GN}(x)$ is then satisfied trivially as $\\vec{0} = a \\cdot \\vec{0}$.\n\nTherefore, the necessary and sufficient conditions are:\n$$\n\\gamma = \\vec{0} \\quad \\text{and} \\quad \\beta = \\vec{0}.\n$$\n\n### Part 2: Empirical Verification\n\nThe following program implements the Group Normalization operator and tests the homogeneity property for the six specified cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef group_norm(x: np.ndarray, G: int, eps: float, gamma: np.ndarray, beta: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the Group Normalization for a 4D input tensor.\n\n    Args:\n        x: Input tensor of shape (N, C, H, W).\n        G: Number of groups.\n        eps: Epsilon for numerical stability.\n        gamma: Learnable per-channel scaling factor of shape (C,).\n        beta: Learnable per-channel bias factor of shape (C,).\n\n    Returns:\n        The output tensor after Group Normalization.\n    \"\"\"\n    N, C, H, W = x.shape\n    if C % G != 0:\n        raise ValueError(\"Number of channels C must be divisible by number of groups G.\")\n    \n    C_grp = C // G\n    \n    # Reshape for group-wise calculations: (N, C, H, W) -> (N, G, C_grp, H, W)\n    x_reshaped = x.reshape(N, G, C_grp, H, W)\n    \n    # Calculate mean and variance over the group axes (C_grp, H, W)\n    # These are axes (2, 3, 4) in the reshaped tensor.\n    # keepdims=True ensures that the output shapes are broadcastable.\n    mean = x_reshaped.mean(axis=(2, 3, 4), keepdims=True)\n    var = x_reshaped.var(axis=(2, 3, 4), keepdims=True)\n    \n    # Normalize\n    # The reshaped mean and var will broadcast over x_reshaped.\n    x_norm_reshaped = (x_reshaped - mean) / np.sqrt(var + eps)\n    \n    # Reshape back to original tensor layout: (N, G, C_grp, H, W) -> (N, C, H, W)\n    x_norm = x_norm_reshaped.reshape(N, C, H, W)\n    \n    # Apply affine transformation (scale and shift)\n    # Reshape gamma and beta from (C,) to (1, C, 1, 1) for broadcasting.\n    gamma_reshaped = gamma.reshape(1, C, 1, 1)\n    beta_reshaped = beta.reshape(1, C, 1, 1)\n    \n    output = gamma_reshaped * x_norm + beta_reshaped\n    return output\n\ndef solve():\n    \"\"\"\n    Main function to run the empirical tests for homogeneity.\n    \"\"\"\n    # Define reproducible test tensor\n    N, C, H, W = 2, 4, 2, 2\n    seed = 7\n    rng = np.random.default_rng(seed)\n    x = rng.standard_normal(size=(N, C, H, W))\n    \n    # Absolute tolerance for equality check\n    tau = 1e-9\n\n    # Define the 6 test cases from the problem statement.\n    test_cases = [\n        # Case 1: G=2, a=2, eps=0, gamma!=0, beta=0\n        {'G': 2, 'a': 2.0, 'eps': 0.0, 'gamma': np.array([1.0, -0.5, 2.0, 0.3]), 'beta': np.array([0.0, 0.0, 0.0, 0.0])},\n        # Case 2: a=1 (trivial case)\n        {'G': 2, 'a': 1.0, 'eps': 1e-5, 'gamma': np.array([1.0, -0.5, 2.0, 0.3]), 'beta': np.array([0.1, -0.2, 0.0, 0.5])},\n        # Case 3: G=4, a=2, eps=0, gamma=0, beta=0 (trivial GN)\n        {'G': 4, 'a': 2.0, 'eps': 0.0, 'gamma': np.array([0.0, 0.0, 0.0, 0.0]), 'beta': np.array([0.0, 0.0, 0.0, 0.0])},\n        # Case 4: G=1, a=2, eps=0, beta!=0\n        {'G': 1, 'a': 2.0, 'eps': 0.0, 'gamma': np.array([1.0, -0.5, 2.0, 0.3]), 'beta': np.array([0.25, 0.25, 0.25, 0.25])},\n        # Case 5: G=1, a=2, eps!=0, beta=0\n        {'G': 1, 'a': 2.0, 'eps': 1e-5, 'gamma': np.array([1.0, -0.5, 2.0, 0.3]), 'beta': np.array([0.0, 0.0, 0.0, 0.0])},\n        # Case 6: G=2, a=100, eps!=0, beta=0\n        {'G': 2, 'a': 100.0, 'eps': 1e-5, 'gamma': np.array([1.0, -0.5, 2.0, 0.3]), 'beta': np.array([0.0, 0.0, 0.0, 0.0])},\n    ]\n\n    results = []\n    for case in test_cases:\n        G, a, eps, gamma, beta = case['G'], case['a'], case['eps'], case['gamma'], case['beta']\n        \n        # Calculate GN(a*x)\n        gn_ax = group_norm(a * x, G, eps, gamma, beta)\n        \n        # Calculate a * GN(x)\n        a_gn_x = a * group_norm(x, G, eps, gamma, beta)\n        \n        # Check if GN(a*x) is close to a * GN(x) within the absolute tolerance\n        # rtol=0 ensures only absolute tolerance is used for the comparison.\n        is_homogeneous = np.allclose(gn_ax, a_gn_x, atol=tau, rtol=0)\n        results.append(is_homogeneous)\n\n    # Format the final output as a single-line comma-separated list of booleans\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from general properties to practical implementation, we now focus on the small but mighty parameter $\\epsilon$. While often introduced merely as a way to prevent division by zero, its true role in training stability is far more significant. This practice challenges you to analyze the backpropagation dynamics of GN, deriving how $\\epsilon$ directly controls the magnitude of gradients when the group variance is small and preventing potential numerical explosions that can derail the training process .",
            "id": "3133999",
            "problem": "Consider a single training sample and a single group in Group Normalization (GN) with group size $m \\geq 2$. Let the group activations be $x_{1}, \\dots, x_{m} \\in \\mathbb{R}$, their group mean be $\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x_{i}$, and their group variance be $\\sigma_{g}^{2} = \\frac{1}{m} \\sum_{i=1}^{m} (x_{i} - \\mu)^{2}$. GN produces normalized activations $\\hat{x}_{i} = \\frac{x_{i} - \\mu}{\\sqrt{\\sigma_{g}^{2} + \\epsilon}}$ and outputs $y_{i} = \\gamma \\hat{x}_{i} + \\beta$, where $\\gamma \\in \\mathbb{R}$ and $\\beta \\in \\mathbb{R}$ are learnable parameters shared across the $m$ elements of the group. Let the scalar loss be $L(y_{1}, \\dots, y_{m})$. Assume the incoming gradients satisfy a uniform bound $| \\frac{\\partial L}{\\partial y_{i}} | \\leq G$ for all $i \\in \\{1,\\dots,m\\}$, where $G > 0$ is a given constant.\n\nUsing only the definitions above and basic multivariable calculus (chain rule) as the fundamental base:\n\n1) Derive the limiting form of the gradient $\\frac{\\partial L}{\\partial x_{i}}$ as $\\sigma_{g}^{2} \\to 0$, expressed in terms of $\\gamma$, $\\epsilon$, and the incoming gradients.\n\n2) From that limiting form, derive the tightest possible upper bound on $\\max_{i} \\left| \\frac{\\partial L}{\\partial x_{i}} \\right|$ in terms of $m$, $\\gamma$, $\\epsilon$, and $G$ when $\\sigma_{g}^{2} \\to 0$.\n\n3) Given a target gradient magnitude threshold $\\tau > 0$, solve for the smallest $\\epsilon$ that guarantees $\\max_{i} \\left| \\frac{\\partial L}{\\partial x_{i}} \\right| \\leq \\tau$ in the limit $\\sigma_{g}^{2} \\to 0$.\n\n4) Propose one adaptive strategy for selecting $\\epsilon$ as a function of $\\sigma_{g}^{2}$ and other problem parameters that preserves numerical stability as $\\sigma_{g}^{2} \\to 0$, and explain why it works.\n\nYour final answer must be only the closed-form expression for the minimal $\\epsilon$ from part 3). No units are needed. If your answer involves constants, express them symbolically. Do not round.",
            "solution": "The solution is a four-part derivation and analysis based on the principles of multivariable calculus applied to the Group Normalization (GN) function.\n\n### Part 1: Derivation of the limiting form of the gradient $\\frac{\\partial L}{\\partial x_{i}}$\n\nThe total derivative of the loss $L$ with respect to an input activation $x_i$ is found using the multivariable chain rule, summing over all outputs $y_j$ that depend on $x_i$:\n$$ \\frac{\\partial L}{\\partial x_{i}} = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_{j}} \\frac{\\partial y_{j}}{\\partial x_{i}} $$\nThe output $y_j$ is defined as $y_{j} = \\gamma \\hat{x}_{j} + \\beta$, where $\\hat{x}_{j} = \\frac{x_{j} - \\mu}{\\sqrt{\\sigma_{g}^{2} + \\epsilon}}$. Therefore, $\\frac{\\partial y_{j}}{\\partial x_{i}} = \\gamma \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}}$.\n\nTo find $\\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}}$, we must consider that both $\\mu$ and $\\sigma_g^2$ are functions of all $x_k$ in the group. The partial derivatives of the mean $\\mu$ and variance $\\sigma_g^2$ with respect to $x_i$ are:\n$$ \\frac{\\partial \\mu}{\\partial x_{i}} = \\frac{\\partial}{\\partial x_{i}} \\left( \\frac{1}{m} \\sum_{k=1}^{m} x_{k} \\right) = \\frac{1}{m} $$\n$$ \\frac{\\partial \\sigma_{g}^{2}}{\\partial x_{i}} = \\frac{\\partial}{\\partial x_{i}} \\left( \\frac{1}{m} \\sum_{k=1}^{m} (x_{k} - \\mu)^{2} \\right) = \\frac{1}{m} \\sum_{k=1}^{m} 2(x_{k} - \\mu) \\left( \\frac{\\partial x_{k}}{\\partial x_{i}} - \\frac{\\partial \\mu}{\\partial x_{i}} \\right) $$\n$$ = \\frac{2}{m} \\sum_{k=1}^{m} (x_{k} - \\mu) ( \\delta_{ki} - \\frac{1}{m} ) = \\frac{2}{m} \\left( (x_i - \\mu)(1-\\frac{1}{m}) - \\frac{1}{m}\\sum_{k \\neq i} (x_k - \\mu) \\right) $$\nUsing the property $\\sum_{k=1}^{m} (x_{k} - \\mu) = 0$, we have $\\sum_{k \\neq i} (x_{k} - \\mu) = -(x_i - \\mu)$. Substituting this gives:\n$$ \\frac{\\partial \\sigma_{g}^{2}}{\\partial x_{i}} = \\frac{2}{m} \\left( (x_i - \\mu)(1-\\frac{1}{m}) + \\frac{1}{m}(x_i - \\mu) \\right) = \\frac{2}{m} (x_i - \\mu) $$\nNow, we apply the quotient rule to $\\hat{x}_j = \\frac{u}{v}$ where $u = x_j - \\mu$ and $v = \\sqrt{\\sigma_g^2 + \\epsilon}$.\n$$ \\frac{\\partial u}{\\partial x_i} = \\frac{\\partial (x_j - \\mu)}{\\partial x_i} = \\delta_{ij} - \\frac{1}{m} $$\n$$ \\frac{\\partial v}{\\partial x_i} = \\frac{1}{2\\sqrt{\\sigma_g^2 + \\epsilon}} \\frac{\\partial \\sigma_g^2}{\\partial x_i} = \\frac{1}{2\\sqrt{\\sigma_g^2 + \\epsilon}} \\frac{2}{m}(x_i - \\mu) = \\frac{x_i - \\mu}{m\\sqrt{\\sigma_g^2 + \\epsilon}} $$\nThe derivative $\\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} = \\frac{v \\frac{\\partial u}{\\partial x_i} - u \\frac{\\partial v}{\\partial x_i}}{v^2}$ is:\n$$ \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} = \\frac{\\sqrt{\\sigma_g^2 + \\epsilon}(\\delta_{ij} - \\frac{1}{m}) - (x_j-\\mu) \\frac{x_i - \\mu}{m\\sqrt{\\sigma_g^2 + \\epsilon}}}{\\sigma_g^2 + \\epsilon} = \\frac{1}{\\sqrt{\\sigma_g^2 + \\epsilon}} \\left( \\delta_{ij} - \\frac{1}{m} \\right) - \\frac{(x_i - \\mu)(x_j - \\mu)}{m(\\sigma_g^2 + \\epsilon)^{3/2}} $$\nWe are interested in the limit as $\\sigma_g^2 \\to 0$. This condition implies that $(x_k - \\mu) \\to 0$ for all $k \\in \\{1, \\dots, m\\}$. The term $(x_i-\\mu)(x_j-\\mu)$ is of order $O(\\sigma_g^2)$. Let us analyze the second term in the expression for $\\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}}$:\n$$ \\lim_{\\sigma_g^2 \\to 0} \\frac{(x_i - \\mu)(x_j - \\mu)}{m(\\sigma_g^2 + \\epsilon)^{3/2}} = \\lim_{\\sigma_g^2 \\to 0} \\frac{O(\\sigma_g^2)}{m(\\sigma_g^2 + \\epsilon)^{3/2}} = \\frac{0}{m \\epsilon^{3/2}} = 0 $$\nTherefore, in the limit $\\sigma_g^2 \\to 0$, the derivative simplifies to:\n$$ \\lim_{\\sigma_g^2 \\to 0} \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} = \\lim_{\\sigma_g^2 \\to 0} \\frac{1}{\\sqrt{\\sigma_g^2 + \\epsilon}} \\left( \\delta_{ij} - \\frac{1}{m} \\right) = \\frac{1}{\\sqrt{\\epsilon}} \\left( \\delta_{ij} - \\frac{1}{m} \\right) $$\nSubstituting this back into the expression for $\\frac{\\partial L}{\\partial x_i}$, and using $g_j = \\frac{\\partial L}{\\partial y_j}$ to denote the incoming gradients:\n$$ \\lim_{\\sigma_g^2 \\to 0} \\frac{\\partial L}{\\partial x_{i}} = \\sum_{j=1}^{m} g_j \\left( \\lim_{\\sigma_g^2 \\to 0} \\gamma \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} \\right) = \\sum_{j=1}^{m} g_j \\frac{\\gamma}{\\sqrt{\\epsilon}} (\\delta_{ij} - \\frac{1}{m}) $$\n$$ = \\frac{\\gamma}{\\sqrt{\\epsilon}} \\left( g_i - \\frac{1}{m} \\sum_{j=1}^{m} g_j \\right) $$\nThis is the limiting form of the gradient $\\frac{\\partial L}{\\partial x_i}$.\n\n### Part 2: Tightest upper bound on $\\max_{i} \\left| \\frac{\\partial L}{\\partial x_{i}} \\right|$\n\nFrom Part 1, we must find the tightest upper bound on $\\max_i \\left| \\frac{\\gamma}{\\sqrt{\\epsilon}} \\left( g_i - \\bar{g} \\right) \\right|$, where $\\bar{g} = \\frac{1}{m} \\sum_j g_j$ and we are given $|g_j| \\leq G$ for all $j$. This is equivalent to maximizing $\\frac{|\\gamma|}{\\sqrt{\\epsilon}} \\max_i |g_i - \\bar{g}|$. We focus on maximizing $|g_i - \\bar{g}|$.\n$$ |g_i - \\bar{g}| = \\left| g_i - \\frac{1}{m} g_i - \\frac{1}{m} \\sum_{j \\neq i} g_j \\right| = \\left| \\frac{m-1}{m} g_i - \\frac{1}{m} \\sum_{j \\neq i} g_j \\right| $$\nBy the triangle inequality:\n$$ \\left| \\frac{m-1}{m} g_i - \\frac{1}{m} \\sum_{j \\neq i} g_j \\right| \\leq \\frac{m-1}{m} |g_i| + \\frac{1}{m} \\sum_{j \\neq i} |g_j| $$\nUsing the bound $|g_j| \\leq G$:\n$$ \\leq \\frac{m-1}{m} G + \\frac{1}{m} (m-1) G = 2G \\frac{m-1}{m} $$\nThis bound is tight. It can be achieved by setting one gradient to an extreme and the others to the opposite extreme. For a fixed $i$, let $g_i = G$ and $g_j = -G$ for all $j \\neq i$.\nThe mean gradient is $\\bar{g} = \\frac{1}{m} (G + (m-1)(-G)) = \\frac{G(1 - (m-1))}{m} = \\frac{G(2-m)}{m}$.\nThen, $|g_i - \\bar{g}| = |G - G\\frac{2-m}{m}| = |G \\left( \\frac{m - 2 + m}{m} \\right)| = |G \\frac{2m-2}{m}| = 2G \\frac{m-1}{m}$.\nFor any $j \\neq i$, $|g_j - \\bar{g}| = |-G - G\\frac{2-m}{m}| = |-G \\frac{m+2-m}{m}| = \\frac{2G}{m}$.\nSince $m \\geq 2$, it follows that $m-1 \\geq 1$, so $2G\\frac{m-1}{m} \\geq \\frac{2G}{m}$.\nThe maximum deviation from the mean is indeed $2G \\frac{m-1}{m}$.\nThus, the tightest upper bound on the gradient magnitude is:\n$$ \\max_i \\left| \\frac{\\partial L}{\\partial x_{i}} \\right| \\leq \\frac{|\\gamma|}{\\sqrt{\\epsilon}} \\left( 2G \\frac{m-1}{m} \\right) $$\n\n### Part 3: Solving for the smallest $\\epsilon$\n\nWe are given the requirement that $\\max_i \\left| \\frac{\\partial L}{\\partial x_{i}} \\right| \\leq \\tau$ in the limit $\\sigma_g^2 \\to 0$. Using the tightest bound from Part 2, we establish the inequality:\n$$ \\frac{2G |\\gamma|}{\\sqrt{\\epsilon}} \\frac{m-1}{m} \\leq \\tau $$\nTo find the smallest $\\epsilon > 0$ that guarantees this condition for any valid configuration of incoming gradients, we solve for $\\epsilon$:\n$$ \\sqrt{\\epsilon} \\geq \\frac{2G |\\gamma|}{\\tau} \\frac{m-1}{m} $$\nThe inequality defines a lower bound for $\\sqrt{\\epsilon}$. The smallest value of $\\epsilon$ is obtained when this inequality is an equality. Squaring both sides yields the minimum required $\\epsilon$:\n$$ \\epsilon = \\left( \\frac{2G |\\gamma|}{\\tau} \\frac{m-1}{m} \\right)^2 = \\frac{4 G^2 \\gamma^2}{\\tau^2} \\left( 1 - \\frac{1}{m} \\right)^2 $$\n\n### Part 4: Proposing an adaptive strategy for selecting $\\epsilon$\n\nThe instability arises from the term $\\frac{1}{\\sqrt{\\sigma_g^2 + \\epsilon}}$ in the gradient expression, which can blow up if both $\\sigma_g^2$ and $\\epsilon$ are close to zero. The fixed $\\epsilon$ from Part 3 provides a worst-case guarantee but may be overly conservative. An adaptive strategy can set $\\epsilon$ based on current conditions to be just large enough to ensure stability.\n\nA principled adaptive strategy is to enforce the gradient constraint from Part 3 dynamically. At each backward pass for a given group, we have access to the incoming gradients $g_j = \\frac{\\partial L}{\\partial y_j}$ and the parameter $\\gamma$. We can compute the term that drives the gradient magnitude: $C_{\\max} = |\\gamma| \\max_j |g_j - \\bar{g}|$.\nThe dominant part of the gradient magnitude is approximately $\\frac{C_{\\max}}{\\sqrt{\\sigma_g^2 + \\epsilon}}$. To cap this at a threshold $\\tau$, we require:\n$$ \\frac{C_{\\max}}{\\sqrt{\\sigma_g^2 + \\epsilon}} \\leq \\tau \\implies \\sigma_g^2 + \\epsilon \\geq \\left( \\frac{C_{\\max}}{\\tau} \\right)^2 $$\nThis leads to the strategy of setting $\\epsilon$ to satisfy this condition with equality, while ensuring it does not become negative and including a small floor for robustness:\n$$ \\epsilon = \\max\\left(\\epsilon_{\\text{floor}}, \\left(\\frac{C_{\\max}}{\\tau}\\right)^2 - [\\sigma_g^2]_{\\text{sg}}\\right) $$\nHere, $\\epsilon_{\\text{floor}}$ is a small positive constant (e.g., $10^{-8}$), $\\tau$ is a hyperparameter for the desired gradient norm cap, and $[\\sigma_g^2]_{\\text{sg}}$ denotes that the value of the batch variance $\\sigma_g^2$ is used without propagating gradients through it in the computation of $\\epsilon$ (a stop-gradient operation).\n\nThis strategy works because:\n1. If $\\sigma_g^2$ is large (i.e., $(\\frac{C_{\\max}}{\\tau})^2 \\leq \\sigma_g^2$), $\\epsilon$ defaults to $\\epsilon_{\\text{floor}}$. The gradient denominator $\\sqrt{\\sigma_g^2+\\epsilon}$ is large, so gradients are small and stable.\n2. If $\\sigma_g^2$ is small (i.e., $(\\frac{C_{\\max}}{\\tau})^2 > \\sigma_g^2$), $\\epsilon$ is chosen such that $\\sigma_g^2 + \\epsilon = (C_{\\max}/\\tau)^2$. The gradient denominator becomes $\\sqrt{(C_{\\max}/\\tau)^2} = C_{\\max}/\\tau$. The gradient magnitude is thus capped at approximately $\\frac{C_{\\max}}{C_{\\max}/\\tau} = \\tau$, preventing explosion.\nThis adaptive selection of $\\epsilon$ directly targets the source of numerical instability.",
            "answer": "$$\n\\boxed{\\frac{4G^2\\gamma^2}{\\tau^2}\\left(\\frac{m-1}{m}\\right)^2}\n$$"
        },
        {
            "introduction": "Standard Group Normalization relies on the arithmetic mean and variance, which are notoriously sensitive to outliers. This final practice explores a key limitation of GN and a powerful method to overcome it by building a more robust normalization layer. You will implement a variant of GN that replaces classical estimators with the Median-of-Means (MoM), a technique from robust statistics, and compare its performance on a pathological case with an extreme outlier, demonstrating how thoughtful design can lead to more resilient models .",
            "id": "3134016",
            "problem": "Consider a four-dimensional tensor representing a mini-batch of feature maps in deep neural networks. Let the tensor be denoted by $X \\in \\mathbb{R}^{N \\times C \\times H \\times W}$, with batch size $N$, number of channels $C$, and spatial dimensions $H$ and $W$. The task is to implement, analyze, and compare two normalization schemes applied per sample and per group of channels: Group Normalization (GN) and a robust alternative that replaces classical estimators with the Median-of-Means (MoM) for both the mean and the second moment.\n\nYou must derive the normalization transformations starting from the following fundamental base:\n\n- The arithmetic mean of a finite set of real numbers $\\{x_i\\}_{i=1}^m$ is defined as $\\mu = \\frac{1}{m} \\sum_{i=1}^m x_i$.\n- The (population) variance is defined as $\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^2$.\n- The second moment about zero is $M_2 = \\frac{1}{m} \\sum_{i=1}^m x_i^2$ and satisfies $\\sigma^2 = M_2 - \\mu^2$.\n\nA group configuration partitions the $C$ channels into $G$ non-overlapping groups of equal size $\\frac{C}{G}$ (assume $C$ is divisible by $G$). For each sample $n \\in \\{1,\\dots,N\\}$ and each group $g \\in \\{1,\\dots,G\\}$, collect all elements across the channels in that group and their spatial locations; denote this multiset by $\\mathcal{S}_{n,g}$ and its cardinality by $m = |\\mathcal{S}_{n,g}|$. Group Normalization uses the arithmetic mean and variance computed over $\\mathcal{S}_{n,g}$ to map elements of $X$ to a dimensionless representation whose group-wise mean is $0$ and group-wise second central moment equals $1$, up to a numerical stability constant $\\varepsilon > 0$. The robust alternative replaces the arithmetic mean with the Median-of-Means estimator and the second moment with a Median-of-Means estimate of $M_2$, and then constructs a robust variance via $\\widehat{\\sigma}^2_{\\text{robust}} = \\max\\{ \\widehat{M}_2 - \\widehat{\\mu}^2, 0 \\}$, also using the same numerical stability constant $\\varepsilon$.\n\nThe Median-of-Means (MoM) procedure on $\\mathcal{S}_{n,g}$ is defined as follows. Flatten $\\mathcal{S}_{n,g}$ into a sequence $(x_1,\\dots,x_m)$ in a fixed, deterministic order. Choose an integer number of blocks $k$ satisfying $k = \\min\\{3, m\\}$, and partition the sequence into $k$ contiguous blocks with sizes as equal as possible. For the mean, compute the block means and take their median to obtain $\\widehat{\\mu}$. For the second moment, compute the block averages of $x_i^2$ and take their median to obtain $\\widehat{M}_2$.\n\nImplement both normalization methods without any affine re-scaling or bias (i.e., use $\\gamma = 1$ and $\\beta = 0$) and use a fixed numerical stability constant $\\varepsilon = 10^{-5}$.\n\nConstruct the following deterministic test suite. In all cases, $N = 1$, and the ordering of elements within each group for the MoM partition is the default row-major flattening with channels varying slowest and spatial dimensions varying fastest.\n\n- Test Case $1$ (Pathological dominance within a group): $C = 4$, $H = 2$, $W = 2$, $G = 2$. Channels $\\{0,1\\}$ form group $0$, channels $\\{2,3\\}$ form group $1$. Define the spatial sign pattern $s(h,w) = 1$ if $(h + w)$ is even and $s(h,w) = -1$ otherwise, for $h \\in \\{0,1\\}$ and $w \\in \\{0,1\\}$. Set channel values as:\n  - Channel $0$: $x_{0,h,w} = \\mathbf{1}\\{(h + w) \\text{ is odd}\\}$,\n  - Channel $1$: $x_{1,h,w} = 1000 \\cdot s(h,w)$,\n  - Channel $2$: $x_{2,h,w} = 0.5 \\cdot s(h,w)$,\n  - Channel $3$: $x_{3,h,w} = -1 \\cdot s(h,w)$.\n  Use outlier channel index $c_{\\text{out}} = 1$ and small-variance channel index $c_{\\text{small}} = 0$.\n\n- Test Case $2$ (Balanced variances, no extreme outliers): $C = 4$, $H = 2$, $W = 2$, $G = 2$. Use the same $s(h,w)$ and set amplitudes $a_0 = 1$, $a_1 = 1.2$, $a_2 = 0.8$, $a_3 = 1.5$ with\n  - Channel $c$: $x_{c,h,w} = a_c \\cdot s(h,w)$ for $c \\in \\{0,1,2,3\\}$.\n  Use $c_{\\text{out}} = 1$ and $c_{\\text{small}} = 2$.\n\n- Test Case $3$ (Boundary: group size $1$): $C = 4$, $H = 2$, $W = 2$, $G = 4$. Use the same channel definitions as in Test Case $1$. Here each group contains exactly one channel. Use $c_{\\text{out}} = 1$ and $c_{\\text{small}} = 0$.\n\n- Test Case $4$ (Minimal elements per group): $C = 2$, $H = 1$, $W = 1$, $G = 1$. Set\n  - Channel $0$: $x_{0,0,0} = 0.1$,\n  - Channel $1$: $x_{1,0,0} = 1000$.\n  Use $c_{\\text{out}} = 1$ and $c_{\\text{small}} = 0$.\n\nFor each test case, compute four quantities:\n- $r_{\\text{den}}$: the ratio of denominators $\\frac{\\sqrt{\\sigma^2_{\\text{GN}} + \\varepsilon}}{\\sqrt{\\widehat{\\sigma}^2_{\\text{MoM}} + \\varepsilon}}$ for the group containing $c_{\\text{out}}$,\n- $r_{\\text{out}}$: the ratio of maximum absolute normalized values for the outlier channel $\\frac{\\max |Y_{\\text{GN}}(c_{\\text{out}})|}{\\max |Y_{\\text{MoM}}(c_{\\text{out}})|}$,\n- $r_{\\text{small}}$: the ratio of maximum absolute normalized values for the small-variance channel $\\frac{\\max |Y_{\\text{GN}}(c_{\\text{small}})|}{\\max |Y_{\\text{MoM}}(c_{\\text{small}})|}$,\n- $d_{\\mu}$: the absolute difference of the group mean estimators $| \\mu_{\\text{GN}} - \\widehat{\\mu}_{\\text{MoM}} |$ for the group containing $c_{\\text{out}}$.\n\nAll four quantities must be real numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of the four values in the order $[r_{\\text{den}}, r_{\\text{out}}, r_{\\text{small}}, d_{\\mu}]$. For example, the output format must be like $[[v_{1,1}, v_{1,2}, v_{1,3}, v_{1,4}], [v_{2,1}, v_{2,2}, v_{2,3}, v_{2,4}], [v_{3,1}, v_{3,2}, v_{3,3}, v_{3,4}], [v_{4,1}, v_{4,2}, v_{4,3}, v_{4,4}]]$ with exact numeric values produced by your implementation.",
            "solution": "The present task requires the implementation and comparative analysis of two normalization schemes for deep learning feature maps: the standard Group Normalization (GN) and a robust variant based on the Median-of-Means (MoM) estimator. The analysis will be conducted on a deterministic test suite designed to evaluate the behavior of these methods, particularly in the presence of outliers.\n\n### 1. Theoretical Foundation\n\nLet $X \\in \\mathbb{R}^{N \\times C \\times H \\times W}$ be a tensor representing a mini-batch of feature maps. Both normalization schemes operate on groups of channels. The $C$ channels are partitioned into $G$ groups. For each sample $n$ and group $g$, we define a multiset $\\mathcal{S}_{n,g}$ containing all activation values from the channels in that group and their spatial dimensions. The size of this set is $m = (C/G) \\cdot H \\cdot W$. Normalization aims to standardize the moments (e.g., mean and variance) of the distribution of values within each set $\\mathcal{S}_{n,g}$.\n\n#### 1.1. Standard Group Normalization (GN)\n\nGroup Normalization, as a standard technique, uses classical statistical estimators which are known to be efficient but sensitive to outliers.\n\nThe mean and variance for the set $\\mathcal{S}_{n,g}$ (with elements denoted by $x_i$) are computed as:\n-   **Arithmetic Mean**: The estimator for the first raw moment (the mean) is the sample mean:\n    $$ \\mu_{\\text{GN}} = \\frac{1}{m} \\sum_{i=1}^m x_i $$\n-   **Population Variance**: The estimator for the second central moment (the variance) is:\n    $$ \\sigma^2_{\\text{GN}} = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_{\\text{GN}})^2 $$\n    For computational stability and efficiency, this is often calculated using the second raw moment, $M_{2, \\text{GN}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i^2$, via the identity $\\sigma^2_{\\text{GN}} = M_{2, \\text{GN}} - \\mu_{\\text{GN}}^2$.\n\nThe normalization transformation for each element $x_i \\in \\mathcal{S}_{n,g}$ is then:\n$$ y_i = \\frac{x_i - \\mu_{\\text{GN}}}{\\sqrt{\\sigma^2_{\\text{GN}} + \\varepsilon}} $$\nwhere $\\varepsilon$ is a small constant ($10^{-5}$ in this problem) added for numerical stability to prevent division by zero. This transformation rescales the activations in each group to have a mean of approximately $0$ and a variance of approximately $1$.\n\n#### 1.2. Robust Group Normalization with Median-of-Means (MoM)\n\nTo improve robustness against outliers, one can replace the classical mean and variance estimators with robust alternatives. The Median-of-Means (MoM) is a simple and effective robust estimator for the mean.\n\nThe MoM procedure for a set of $m$ values is as follows:\n1.  Partition the set of $m$ values into $k$ non-overlapping blocks, where $k$ is specified as $k = \\min\\{3, m\\}$. The partitioning is done to make block sizes as equal as possible.\n2.  Compute the arithmetic mean of the values within each of the $k$ blocks.\n3.  The MoM estimate is the median of these $k$ block means.\n\nApplying this principle, we define robust estimators for the mean and second moment:\n-   **MoM Mean Estimator ($\\widehat{\\mu}_{\\text{MoM}}$)**: The data $\\{x_i\\}_{i=1}^m$ are partitioned into $k$ blocks. Let $B_1, \\dots, B_k$ be these blocks. The MoM estimate of the mean is:\n    $$ \\widehat{\\mu}_{\\text{MoM}} = \\text{median}\\left( \\left\\{ \\text{mean}(B_j) \\right\\}_{j=1}^k \\right) $$\n-   **MoM Second Moment Estimator ($\\widehat{M}_{2, \\text{MoM}}$)**: The same procedure is applied to the squared data $\\{x_i^2\\}_{i=1}^m$. These are partitioned into $k$ blocks, and the MoM estimate of the second moment is the median of the block averages of the squared data.\n-   **Robust Variance Estimator ($\\widehat{\\sigma}^2_{\\text{MoM}}$)**: A robust variance estimate is constructed using the plug-in principle with the robust estimators for the moments:\n    $$ \\widehat{\\sigma}^2_{\\text{MoM}} = \\max\\left( \\widehat{M}_{2, \\text{MoM}} - \\widehat{\\mu}_{\\text{MoM}}^2, 0 \\right) $$\n    The $\\max(\\cdot, 0)$ operation ensures non-negativity, which could be violated due to estimation errors.\n\nThe robust normalization transformation for each element $x_i \\in \\mathcal{S}_{n,g}$ is analogous to GN:\n$$ y_i = \\frac{x_i - \\widehat{\\mu}_{\\text{MoM}}}{\\sqrt{\\widehat{\\sigma}^2_{\\text{MoM}} + \\varepsilon}} $$\n\n### 2. Algorithmic Implementation and Analysis\n\nThe implementation follows the definitions above. For each test case, an input tensor $X$ is constructed. Two separate functions apply the GN and MoM-based normalization schemes. Each function iterates through the $G$ channel groups. For a given group, it extracts the corresponding data slice from $X$, flattens it into a one-dimensional array, and computes the required statistics (either $\\mu_{\\text{GN}}, \\sigma^2_{\\text{GN}}$ or $\\widehat{\\mu}_{\\text{MoM}}, \\widehat{\\sigma}^2_{\\text{MoM}}$). These statistics are then used to normalize the data in that group.\n\nA key detail in the MoM implementation is the block partitioning. For a flattened array of size $m$ and $k$ blocks, `numpy.array_split` is used, which creates blocks of sizes as equal as possible, matching the problem specification. The median is computed using `numpy.median`, which correctly handles both odd and even numbers of block statistics.\n\nThe analysis is performed by computing four metrics for each test case:\n-   $d_{\\mu} = |\\mu_{\\text{GN}} - \\widehat{\\mu}_{\\text{MoM}}|$: This directly measures the difference between the classical and robust mean estimates for the group containing the outlier channel. A large value indicates that the outlier significantly skewed the classical mean.\n-   $r_{\\text{den}} = \\sqrt{\\sigma^2_{\\text{GN}} + \\varepsilon} / \\sqrt{\\widehat{\\sigma}^2_{\\text{MoM}} + \\varepsilon}$: This compares the scale estimates (the normalization denominators). The ratio indicates whether the robust method perceived a larger or smaller spread in the data compared to the classical method.\n-   $r_{\\text{out}} = \\max |Y_{\\text{GN}}(c_{\\text{out}})| / \\max |Y_{\\text{MoM}}(c_{\\text{out}})|$: This ratio measures the relative post-normalization magnitude of the outlier channel. A value greater than $1$ suggests that the MoM-based method was more effective at suppressing the outlier's magnitude.\n-   $r_{\\text{small}} = \\max |Y_{\\text{GN}}(c_{\\text{small}})| / \\max |Y_{\\text{MoM}}(c_{\\text{small}})|$: This assesses the impact of the normalization scheme on a \"regular\" channel. An ideal robust method would have minimal distorting impact on non-outlying data.\n\nThe test cases are designed to probe different behaviors:\n-   **Test Case 1** introduces a strong outlier to highlight the core differences in robustness.\n-   **Test Case 2** uses balanced data without extreme outliers, where both methods are expected to yield similar results.\n-   **Test Cases 3 and 4** explore boundary conditions related to the number of elements per group ($m$) and its effect on the MoM estimator, specifically when $k < 3$. For $m=2$, $k=2$, MoM estimators for mean and second moment become identical to their classical counterparts because the median of two values is their average. For $m=4$, $k=3$, the difference between methods depends on the data distribution and how the partitions are formed.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    \n    test_cases_params = [\n        # (C, H, W, G, c_out, c_small, data_generator_id)\n        (4, 2, 2, 2, 1, 0, 1),\n        (4, 2, 2, 2, 1, 2, 2),\n        (4, 2, 2, 4, 1, 0, 1),\n        (2, 1, 1, 1, 1, 0, 3)\n    ]\n\n    EPSILON = 1e-5\n\n    def get_mom_stats(data: np.ndarray):\n        \"\"\"Computes Median-of-Means for mean and second moment.\"\"\"\n        m = data.shape[0]\n        k = min(3, m)\n        \n        # Partition data for mean estimation\n        blocks = np.array_split(data, k)\n        block_means = [np.mean(b) for b in blocks]\n        mu_mom = np.median(block_means)\n        \n        # Partition squared data for M2 estimation\n        squared_data = np.square(data)\n        sq_blocks = np.array_split(squared_data, k)\n        block_m2s = [np.mean(b) for b in sq_blocks]\n        m2_mom = np.median(block_m2s)\n        \n        return mu_mom, m2_mom\n\n    def apply_norm(X: np.ndarray, G: int, method: str):\n        \"\"\"Applies Group Normalization using the specified method.\"\"\"\n        N, C, H, W = X.shape\n        Y = np.zeros_like(X, dtype=np.float64)\n        stats = {}\n        \n        channels_per_group = C // G\n        if C % G != 0:\n            raise ValueError(\"C must be divisible by G.\")\n        \n        for g in range(G):\n            ch_start = g * channels_per_group\n            ch_end = (g + 1) * channels_per_group\n            \n            group_data = X[:, ch_start:ch_end, :, :].flatten()\n            \n            if method == 'gn':\n                mu = np.mean(group_data)\n                # Population variance\n                var = np.var(group_data)\n            elif method == 'mom':\n                mu, m2 = get_mom_stats(group_data)\n                var = max(m2 - mu**2, 0)\n            else:\n                raise ValueError(\"Unknown method.\")\n            \n            stats[g] = {'mean': mu, 'var': var}\n            \n            group_slice = X[:, ch_start:ch_end, :, :]\n            Y[:, ch_start:ch_end, :, :] = (group_slice - mu) / np.sqrt(var + EPSILON)\n            \n        return Y, stats\n\n    def create_tensor(C, H, W, gen_id):\n        \"\"\"Generates the input tensor for a test case.\"\"\"\n        X = np.zeros((1, C, H, W), dtype=np.float64)\n        \n        if gen_id == 1 or gen_id == 2:\n            s = np.zeros((H, W))\n            for h in range(H):\n                for w in range(W):\n                    s[h,w] = 1 if (h + w) % 2 == 0 else -1\n\n            if gen_id == 1:\n                # Test Case 1  3 data\n                X[0, 0, :, :] = np.fromfunction(lambda h, w: (h + w) % 2, (H, W))\n                X[0, 1, :, :] = 1000.0 * s\n                X[0, 2, :, :] = 0.5 * s\n                X[0, 3, :, :] = -1.0 * s\n            elif gen_id == 2:\n                # Test Case 2 data\n                amplitudes = [1.0, 1.2, 0.8, 1.5]\n                for c in range(C):\n                    X[0, c, :, :] = amplitudes[c] * s\n        elif gen_id == 3:\n            # Test Case 4 data\n            X[0, 0, 0, 0] = 0.1\n            X[0, 1, 0, 0] = 1000.0\n            \n        return X\n\n    final_results = []\n    for C, H, W, G, c_out, c_small, gen_id in test_cases_params:\n        X = create_tensor(C, H, W, gen_id)\n        \n        channels_per_group = C // G\n        group_out_idx = c_out // channels_per_group\n        \n        # Apply both normalization methods\n        Y_gn, stats_gn = apply_norm(X, G, 'gn')\n        Y_mom, stats_mom = apply_norm(X, G, 'mom')\n        \n        # Extract stats for the outlier group\n        mu_gn = stats_gn[group_out_idx]['mean']\n        var_gn = stats_gn[group_out_idx]['var']\n        mu_mom = stats_mom[group_out_idx]['mean']\n        var_mom = stats_mom[group_out_idx]['var']\n\n        # Calculate metrics\n        d_mu = np.abs(mu_gn - mu_mom)\n        \n        denom_gn = np.sqrt(var_gn + EPSILON)\n        denom_mom = np.sqrt(var_mom + EPSILON)\n        r_den = denom_gn / denom_mom\n        \n        max_abs_gn_out = np.max(np.abs(Y_gn[0, c_out, :, :]))\n        max_abs_mom_out = np.max(np.abs(Y_mom[0, c_out, :, :]))\n        r_out = max_abs_gn_out / max_abs_mom_out if max_abs_mom_out != 0 else np.inf\n\n        max_abs_gn_small = np.max(np.abs(Y_gn[0, c_small, :, :]))\n        max_abs_mom_small = np.max(np.abs(Y_mom[0, c_small, :, :]))\n        r_small = max_abs_gn_small / max_abs_mom_small if max_abs_mom_small != 0 else np.inf\n        \n        case_results = [r_den, r_out, r_small, d_mu]\n        final_results.append(case_results)\n\n    # Convert results to a string in the required list-of-lists format\n    result_str = \"[\" + \", \".join([f\"[{', '.join(map(str, res))}]\" for res in final_results]) + \"]\"\n    print(result_str)\n\nsolve()\n\n```"
        }
    ]
}