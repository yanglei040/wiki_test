{
    "hands_on_practices": [
        {
            "introduction": "本节的第一个练习旨在通过动手实践，直接比较几种最常见的参数范数惩罚：$L_2$ 范数、$L_1$ 范数以及近似的 $L_0$ 范数。通过在一个合成的稀疏线性回归任务中实施和测试这些正则化器，您将对它们如何影响模型参数获得一个具体的理解。这个练习  将清晰地揭示为何 $L_1$ 惩罚以其引导稀疏性和执行特征选择的能力而著称，而 $L_2$ 惩罚则不具备此特性，同时它也突显了与 $L_0$ 范数相关的计算挑战。",
            "id": "3161377",
            "problem": "要求您在一个稀疏线性回归的场景中，通过一个完全可复现的计算实验来实施、分析和比较参数范数惩罚。重点是展示在存在稀疏性的情况下，$L_2$ (岭) 惩罚何时会导致欠拟合，并将其与通过硬阈值法实现的 $L_0$ 近似以及 $L_1$ (最小绝对收缩和选择算子 (LASSO)) 惩罚的可处理性进行对比。\n\n给定一个从具有稀疏真实参数矢量的线性模型生成的数据集，您必须基于三种不同的参数范数惩罚构建估计量，并在多个测试案例中定量比较它们的经验性能。\n\n您必须使用的基本定义和事实：\n- 带有平方损失的线性回归的经验风险最小化目标是最小化 $f(\\mathbf{w}) = \\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\rVert_2^2$，其中 $\\mathbf{w} \\in \\mathbb{R}^d$，$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，$\\mathbf{y} \\in \\mathbb{R}^n$，且 $n, d \\in \\mathbb{N}$。\n- 平方损失的梯度为 $\\nabla f(\\mathbf{w}) = -\\frac{1}{n}\\mathbf{X}^\\top(\\mathbf{y} - \\mathbf{X}\\mathbf{w})$。\n- 梯度的 Lipschitz 常数是 $\\frac{1}{n}\\mathbf{X}^\\top\\mathbf{X}$ 的最大特征值。\n- $L_2$ 惩罚问题 (岭回归) 在目标函数上增加了 $\\frac{\\lambda_2}{2}\\lVert \\mathbf{w}\\rVert_2^2$，其中 $\\lambda_2 \\in \\mathbb{R}_{\\ge 0}$。\n- $L_1$ 惩罚问题 (LASSO) 增加了 $\\lambda_1\\lVert \\mathbf{w}\\rVert_1$，其中 $\\lambda_1 \\in \\mathbb{R}_{\\ge 0}$，并且可以使用源于次梯度最优性的软阈值法，通过坐标下降法进行高效优化。\n- $L_0$ 约束问题 $\\min f(\\mathbf{w})$ subject to $\\lVert \\mathbf{w}\\rVert_0 \\le s$ 是非凸和组合性的；一种近似策略是迭代硬阈值法 (IHT)，该方法使用梯度下降步骤，后跟一个硬阈值算子，保留 $s$ 个绝对值最大的条目。\n\n数据生成协议 (必须严格遵守)：\n- 对于每个测试案例，生成一个 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ 矩阵，其条目为独立的标准正态分布。将每一列 $\\mathbf{x}_j$ 进行归一化，以满足 $\\frac{1}{n}\\lVert \\mathbf{x}_j\\rVert_2^2 = 1$。通过均匀随机选择 $k$ 个不同的索引并赋以从零均值正态分布中抽取的非零值，来生成一个 k-稀疏的真实矢量 $\\mathbf{w}^\\star \\in \\mathbb{R}^d$。然后，通过将任何较小的绝对值增加到恰好为 $1.5$（符号保持不变）来强制执行 $1.5$ 的最小绝对幅值。生成响应 $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^\\star + \\boldsymbol{\\varepsilon}$，其中 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)$。\n\n需要实现的估计量：\n- 岭 ($L_2$) 估计量：通过最小化带惩罚权重 $\\lambda_2$ 的惩罚经验风险得到。\n- $L_0$ 近似（通过迭代硬阈值法 IHT）：使用上述的精确梯度，一个严格小于 Lipschitz 常数倒数的恒定步长，以及在每次迭代中保留恰好 $s$ 个条目的硬阈值法。使用固定的迭代次数。\n- LASSO ($L_1$) 估计量（通过循环坐标下降）：使用软阈值更新。使用固定的迭代预算，并基于一个小的变化容差在收敛时提前终止。\n\n在每个测试案例中为每种方法计算的性能指标：\n- 训练均方误差，定义为 $\\mathrm{MSE} = \\frac{1}{n}\\lVert \\mathbf{y} - \\mathbf{X}\\hat{\\mathbf{w}}\\rVert_2^2$。\n- 支持集召回率，定义为 $r = \\frac{\\lvert \\widehat{S} \\cap S^\\star \\rvert}{k}$，其中 $S^\\star = \\{ j : w^\\star_j \\ne 0 \\}$，$ \\widehat{S}$ 是估计的支持集。$\\widehat{S}$ 的定义如下：对于岭回归，取 $| \\hat{w}_j |$ 最大的 $k$ 个索引；对于 IHT，取硬阈值处理后的非零索引；对于 LASSO，取 $|\\hat{w}_j| > \\tau$ 的索引，其中 $\\tau = 10^{-8}$。\n\n测试套件：\n对于每个元组 $(n, d, k, \\sigma, \\lambda_2, \\lambda_1, s, \\text{seed})$，按照规定构建数据集，然后计算三种方法的指标。使用以下四个测试案例，它们分别探讨了一个典型设置、强 $L_2$ 欠拟合、零噪声边界和稀疏度错误指定的情况：\n- 案例 A: $(n, d, k, \\sigma, \\lambda_2, \\lambda_1, s, \\text{seed}) = (\\,80,\\, 100,\\, 5,\\, 0.05,\\, 10.0,\\, 0.05,\\, 5,\\, 1\\,)$。\n- 案例 B: $(\\,80,\\, 100,\\, 5,\\, 0.10,\\, 100.0,\\, 0.10,\\, 5,\\, 2\\,)$。\n- 案例 C: $(\\,80,\\, 100,\\, 5,\\, 0.00,\\, 10.0,\\, 0.01,\\, 5,\\, 3\\,)$。\n- 案例 D: $(\\,80,\\, 100,\\, 5,\\, 0.05,\\, 10.0,\\, 0.05,\\, 3,\\, 4\\,)$。\n\n输出要求：\n- 对于每个案例，按顺序 $[ \\mathrm{MSE}_{\\mathrm{ridge}}, \\mathrm{MSE}_{\\mathrm{IHT}}, \\mathrm{MSE}_{\\mathrm{LASSO}}, r_{\\mathrm{ridge}}, r_{\\mathrm{IHT}}, r_{\\mathrm{LASSO}} ]$ 输出一个包含 6 个浮点数的列表，每个浮点数四舍五入到恰好 4 位小数。\n- 您的程序应生成单行输出，其中包含这些按案例排列的列表，以逗号分隔，行中不含任何空格。例如：\"[[a1,a2,a3,a4,a5,a6],[b1,b2,b3,b4,b5,b6],[c1,c2,c3,c4,c5,c6],[d1,d2,d3,d4,d5,d6]]\"。\n- 不涉及物理单位。所有角度（如果出现）均应以弧度为单位。所有值必须是数值。\n\n您的实现必须在给定种子的情况下是确定性的，仅使用指定的库，并遵循上述确切的数据集构建和指标定义。",
            "solution": "当前任务是进行一个计算实验，比较稀疏线性回归的三种不同参数范数惩罚：$L_2$ (岭回归)、$L_1$ (LASSO) 和 $L_0$ 的一种近似 (迭代硬阈值法)。该分析将在根据指定协议生成的合成数据集上进行，目的是评估每种方法恢复稀疏真实参数矢量 $\\mathbf{w}^\\star$ 的能力。\n\n其基础模型是一个线性关系 $\\mathbf{y} = \\mathbf{X}\\mathbf{w} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ 是特征矩阵，$\\mathbf{w} \\in \\mathbb{R}^d$ 是参数矢量，$\\mathbf{y} \\in \\mathbb{R}^n$ 是响应矢量，$\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n$ 是一个噪声项。需要最小化的经验风险是均方误差：\n$$f(\\mathbf{w}) = \\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\rVert_2^2$$\n通过向该目标函数添加一个惩罚项来引入正则化。我们将实现并分析三种这样的正则化器。\n\n### 数据生成与评估指标\n\n对于每个测试案例，使用固定的程序合成一个数据集以确保可复现性。\n1.  生成一个设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，其条目独立地从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取。然后对每一列 $\\mathbf{x}_j$进行归一化，使其经 $1/n$ 缩放后的 $L_2$ 范数平方为1：$\\frac{1}{n}\\lVert \\mathbf{x}_j\\rVert_2^2 = 1$。\n2.  构建一个 k-稀疏的真实矢量 $\\mathbf{w}^\\star \\in \\mathbb{R}^d$。均匀随机地选择 $k$ 个支持集索引。这些索引处的值从 $\\mathcal{N}(0, 1)$ 中抽取，任何绝对值小于 $1.5$ 的值都被设置为恰好为 $1.5$（同时保留其符号）。$\\mathbf{w}^\\star$ 的所有其他条目均为零。\n3.  响应矢量 $\\mathbf{y}$ 生成为 $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^\\star + \\boldsymbol{\\varepsilon}$，其中噪声 $\\boldsymbol{\\varepsilon}$ 从多元正态分布 $\\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)$ 中抽取。\n\n每个估计量 $\\hat{\\mathbf{w}}$ 的性能由两个指标量化：\n1.  **训练均方误差 (MSE)：** $\\mathrm{MSE} = \\frac{1}{n}\\lVert \\mathbf{y} - \\mathbf{X}\\hat{\\mathbf{w}}\\rVert_2^2$。\n2.  **支持集召回率 (r)：** $r = \\frac{\\lvert \\widehat{S} \\cap S^\\star \\rvert}{k}$，其中 $S^\\star = \\{j \\mid w^\\star_j \\neq 0\\}$ 是真实支持集，$\\widehat{S}$ 是估计的支持集。\n\n### 估计量的实现\n\n**1. 岭回归 ($L_2$ 惩罚)**\n\n岭回归在目标函数中增加了一个平方 $L_2$ 范数惩罚，通过将参数估计向零收缩来帮助防止过拟合。其优化问题为：\n$$\\min_{\\mathbf{w}} \\left\\{ \\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\rVert_2^2 + \\frac{\\lambda_2}{2}\\lVert \\mathbf{w}\\rVert_2^2 \\right\\}$$\n该目标函数是凸且连续可微的。将其关于 $\\mathbf{w}$ 的梯度设为零，可得到一个闭式解：\n$$\\nabla_{\\mathbf{w}} \\left( \\frac{1}{2n}(\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\frac{\\lambda_2}{2}\\mathbf{w}^\\top\\mathbf{w} \\right) = -\\frac{1}{n}\\mathbf{X}^\\top(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\lambda_2\\mathbf{w} = \\mathbf{0}$$\n$$(\\mathbf{X}^\\top\\mathbf{X} + n\\lambda_2\\mathbf{I})\\mathbf{w} = \\mathbf{X}^\\top\\mathbf{y}$$\n$$\\hat{\\mathbf{w}}_{\\text{ridge}} = (\\mathbf{X}^\\top\\mathbf{X} + n\\lambda_2\\mathbf{I})^{-1}\\mathbf{X}^\\top\\mathbf{y}$$\n这个线性系统通过数值方法求解。$L_2$ 惩罚产生稠密的解（即 $\\hat{\\mathbf{w}}_{\\text{ridge}}$ 的大多数条目都是非零的），因此支持集 $\\widehat{S}$ 是通过选择绝对值最大的 $k$ 个系数的索引来估计的。\n\n**2. 迭代硬阈值法 (IHT, $L_0$ 近似)**\n\n$L_0$ “范数” $\\lVert\\mathbf{w}\\rVert_0$ 计算 $\\mathbf{w}$ 中非零元素的数量。$L_0$ 约束问题旨在找到最多有 $s$ 个非零参数的最佳拟合模型：\n$$\\min_{\\mathbf{w}} \\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\rVert_2^2 \\quad \\text{subject to} \\quad \\lVert \\mathbf{w}\\rVert_0 \\le s$$\n这个问题是 NP-hard 的。IHT 是一种提供近似解的迭代算法。它在标准的梯度下降步骤和硬阈值步骤之间交替进行。更新规则是：\n$$\\mathbf{w}_{t+1} = H_s(\\mathbf{w}_t - \\alpha \\nabla f(\\mathbf{w}_t))$$\n其中 $\\nabla f(\\mathbf{w}_t) = -\\frac{1}{n}\\mathbf{X}^\\top(\\mathbf{y} - \\mathbf{X}\\mathbf{w}_t)$ 是损失函数的梯度。算子 $H_s(\\cdot)$ 是硬阈值投影，它保留其矢量参数中绝对值最大的 $s$ 个元素，并将其余所有元素设为零。为确保收敛，必须选择合适的步长 $\\alpha$。一个有效的选择是 $\\alpha  1/L$，其中 $L$ 是梯度的 Lipschitz 常数，由 $\\frac{1}{n}\\mathbf{X}^\\top\\mathbf{X}$ 的最大特征值给出。我们使用步长 $\\alpha = 0.99/L$ 和固定的 $100$ 次迭代。支持集 $\\widehat{S}$ 自然地定义为最终估计 $\\hat{\\mathbf{w}}_{\\text{IHT}}$ 中的非零索引集合。\n\n**3. LASSO (最小绝对收缩和选择算子, $L_1$ 惩罚)**\n\nLASSO 增加了一个 $L_1$ 范数惩罚，该惩罚以在解中诱导稀疏性而闻名。其优化问题是：\n$$\\min_{\\mathbf{w}} \\left\\{ \\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\rVert_2^2 + \\lambda_1\\lVert \\mathbf{w}\\rVert_1 \\right\\}$$\n其中 $\\lVert \\mathbf{w}\\rVert_1 = \\sum_j |w_j|$。该目标函数是凸的，但在任何 $w_j=0$ 的点上是不可微的。我们使用循环坐标下降法来解决这个问题。对于每个坐标 $j$，我们固定所有其他坐标，并求解关于 $w_j$ 的一维问题。这会产生软阈值更新规则。在列归一化 $\\frac{1}{n}\\lVert \\mathbf{x}_j \\rVert_2^2 = 1$ 的条件下，坐标 $j$ 的更新为：\n$$w_j \\leftarrow S_{\\lambda_1}(\\rho_j)$$\n其中 $S_a(z) = \\text{sign}(z)\\max(|z|-a, 0)$ 是软阈值算子，而 $\\rho_j = \\frac{1}{n}\\mathbf{x}_j^\\top(\\mathbf{y} - \\sum_{i \\neq j}\\mathbf{x}_i w_i)$。为高效实现，我们预先计算 $\\mathbf{X}^\\top\\mathbf{X}$ 和 $\\mathbf{X}^\\top\\mathbf{y}$。算法循环迭代所有坐标，直到两次迭代之间 $\\mathbf{w}$ 的变化小于一个容差（$10^{-6}$）或达到最大迭代次数（$1000$）。支持集 $\\widehat{S}$ 被取为 $|\\hat{w}_j| > 10^{-8}$ 的索引集合。\n\n所选的测试案例将说明特定的行为：案例 A 是一个标准设置；案例 B 使用一个非常高的 $\\lambda_2$ 来展示岭回归如何过度平滑并导致“欠拟合”；案例 C 没有噪声，稀疏方法在此应表现出色；案例 D 展示了为 IHT 错误指定稀疏度水平 $s$ 的影响。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Ridge, IHT, and LASSO for sparse linear regression.\n    \"\"\"\n\n    test_cases = [\n        # (n, d, k, sigma, lambda2, lambda1, s, seed)\n        (80, 100, 5, 0.05, 10.0, 0.05, 5, 1), # Case A\n        (80, 100, 5, 0.10, 100.0, 0.10, 5, 2), # Case B\n        (80, 100, 5, 0.00, 10.0, 0.01, 5, 3), # Case C\n        (80, 100, 5, 0.05, 10.0, 0.05, 3, 4), # Case D\n    ]\n    \n    all_results = []\n\n    def generate_data(n, d, k, sigma, seed):\n        rng = np.random.default_rng(seed)\n        X = rng.standard_normal(size=(n, d))\n        \n        # Normalize columns\n        X_col_norms = np.linalg.norm(X, axis=0)\n        scaling_factors = np.sqrt(n) / X_col_norms\n        X = X * scaling_factors\n        \n        w_star = np.zeros(d)\n        support_indices = rng.choice(d, k, replace=False)\n        w_star[support_indices] = rng.standard_normal(k)\n        \n        # Enforce minimum magnitude\n        low_mag_mask = (w_star != 0)  (np.abs(w_star)  1.5)\n        w_star[low_mag_mask] = np.sign(w_star[low_mag_mask]) * 1.5\n        \n        epsilon = rng.normal(loc=0, scale=sigma, size=n)\n        y = X @ w_star + epsilon\n        \n        return X, y, w_star, support_indices\n\n    def calculate_mse(X, y, w):\n        n = X.shape[0]\n        return np.sum((y - X @ w)**2) / n\n\n    def train_ridge(X, y, lambda2, k, true_support):\n        n, d = X.shape\n        XtX = X.T @ X\n        Xty = X.T @ y\n        identity = np.eye(d)\n        \n        w_ridge = np.linalg.solve(XtX + n * lambda2 * identity, Xty)\n        \n        estimated_support = np.argsort(np.abs(w_ridge))[-k:]\n        true_positives = len(np.intersect1d(estimated_support, true_support))\n        recall = true_positives / k\n        \n        mse = calculate_mse(X, y, w_ridge)\n        return mse, recall\n\n    def train_iht(X, y, s, k, true_support, n_iter=100):\n        n, d = X.shape\n        L = np.max(np.linalg.eigvalsh((X.T @ X) / n))\n        alpha = 0.99 / L\n        \n        w = np.zeros(d)\n\n        for _ in range(n_iter):\n            grad = - (X.T @ (y - X @ w)) / n\n            w_temp = w - alpha * grad\n            \n            # Hard thresholding\n            indices_to_keep = np.argsort(np.abs(w_temp))[-s:]\n            w = np.zeros(d)\n            w[indices_to_keep] = w_temp[indices_to_keep]\n\n        estimated_support = np.where(w != 0)[0]\n        true_positives = len(np.intersect1d(estimated_support, true_support))\n        recall = true_positives / k\n        \n        mse = calculate_mse(X, y, w)\n        return mse, recall\n\n    def train_lasso(X, y, lambda1, k, true_support, max_iter=1000, tol=1e-6):\n        n, d = X.shape\n        w = np.zeros(d)\n        \n        XtX = X.T @ X\n        Xty = X.T @ y\n        \n        for i in range(max_iter):\n            w_old = w.copy()\n            for j in range(d):\n                # Using pre-computed matrices for efficiency\n                rho_j_num = Xty[j] - (np.dot(XtX[j, :], w) - XtX[j, j] * w[j])\n                rho_j = rho_j_num / n\n                \n                # Soft-thresholding\n                w[j] = np.sign(rho_j) * np.maximum(np.abs(rho_j) - lambda1, 0)\n            \n            if np.max(np.abs(w - w_old))  tol:\n                break\n        \n        tau = 1e-8\n        estimated_support = np.where(np.abs(w) > tau)[0]\n        true_positives = len(np.intersect1d(estimated_support, true_support))\n        recall = true_positives / k\n        \n        mse = calculate_mse(X, y, w)\n        return mse, recall\n\n\n    for case in test_cases:\n        n, d, k, sigma, lambda2, lambda1, s, seed = case\n        \n        X, y, w_star, true_support = generate_data(n, d, k, sigma, seed)\n        \n        mse_ridge, recall_ridge = train_ridge(X, y, lambda2, k, true_support)\n        mse_iht, recall_iht = train_iht(X, y, s, k, true_support)\n        mse_lasso, recall_lasso = train_lasso(X, y, lambda1, k, true_support)\n        \n        case_results = [\n            round(mse_ridge, 4),\n            round(mse_iht, 4),\n            round(mse_lasso, 4),\n            round(recall_ridge, 4),\n            round(recall_iht, 4),\n            round(recall_lasso, 4)\n        ]\n        all_results.append(case_results)\n\n    case_strings = []\n    for case_res in all_results:\n        case_strings.append(f\"[{','.join(map(str, case_res))}]\")\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "这个练习将超越简单的参数收缩，深入探讨 $\\ell_2$ 正则化在深度学习背景下更微妙且深刻的影响。您将分析一个双层线性网络，并发现对单个权重矩阵施加标准的权重衰减（weight decay）惩罚，实际上等价于惩罚其端到端线性映射的核范数（nuclear norm）。这个练习  揭示了一个简单的参数范数惩罚如何充当一种*隐式正则化器*，鼓励模型学习低秩、结构更简单的函数，这是理解过参数化模型泛化能力的一个关键思想。",
            "id": "3161416",
            "problem": "考虑一个双层线性网络，其输入维度为 $d$，输出维度为 $p$，隐藏层宽度为 $h$。参数为一对矩阵 $(W_2, W_1)$，其中 $W_1 \\in \\mathbb{R}^{h \\times d}$ 且 $W_2 \\in \\mathbb{R}^{p \\times h}$。该网络计算一个线性映射 $M = W_2 W_1 \\in \\mathbb{R}^{p \\times d}$。假设我们的目标是通过最小化一个从基本定义（平方损失和欧几里得范数）构建的正则化经验风险，来近似一个目标线性映射 $T \\in \\mathbb{R}^{p \\times d}$。具体而言，对于一个固定的正则化系数 $\\alpha \\ge 0$，考虑以下目标函数\n$$\n\\mathcal{L}(W_2, W_1) = \\frac{1}{2} \\lVert W_2 W_1 - T \\rVert_F^2 + \\frac{\\alpha}{2}\\left(\\lVert W_1 \\rVert_F^2 + \\lVert W_2 \\rVert_F^2\\right),\n$$\n其中 $\\lVert \\cdot \\rVert_F$ 表示弗罗贝尼乌斯范数（Frobenius norm）。网络可能在 $h \\ge \\min\\{p,d\\}$ 的意义上是过参数化的，但正则化的选择可能会在有效映射 $M$ 中引发降秩，使其成为欠参数化映射。\n\n仅从核心定义和经过充分检验的事实出发，推导出一个能够最小化上述目标函数的最优有效映射 $M^\\star$ 的完整刻画。你的推导应从弗罗贝尼乌斯范数、核范数（nuclear norm）和奇异值分解（SVD）的定义开始，并进而展示对 $(W_2, W_1)$ 的欧几里得范数惩罚项如何在即使 $h$ 导致参数化过完备的情况下，也能在最优解 $M^\\star$ 中引发隐式的秩降低。然后，设计一个算法，该算法能直接根据 $T$ 的奇异值计算出 $M^\\star$ 的奇异值，并返回 $M^\\star$ 的秩。此外，调整你的刻画和算法以适应 $h$ 是有限值并作为 $M^\\star$ 秩的硬上限的情况，并解释该上限如何与正则化相互作用以确定最终的秩。\n\n你的程序必须实现此算法并将其应用于以下测试套件。在所有情况下，取 $p=d=5$，并令 $T$ 为一个对角矩阵，其对角线上的奇异值为指定的 $\\sigma_1, \\dots, \\sigma_5$（因此其奇异值分解使用单位奇异向量）。对于每种情况，计算在隐藏层宽度 $h$ 和正则化强度 $\\alpha$ 的条件下，最小化 $\\mathcal{L}(W_2, W_1)$ 的最优 $M^\\star$ 的整数秩。\n\n测试用例：\n- 测试用例 1（理想情况，无正则化）：$h=10$，$\\alpha=0.0$，$\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$。\n- 测试用例 2（中等正则化导致秩下降）：$h=10$，$\\alpha=1.0$，$\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$。\n- 测试用例 3（较轻正则化）：$h=10$，$\\alpha=0.7$，$\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$。\n- 测试用例 4（强正则化消除所有奇异值）：$h=10$，$\\alpha=3.5$，$\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$。\n- 测试用例 5（宽度受限的边界情况，其中 $h$ 起约束作用）：$h=2$，$\\alpha=0.7$，$\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$。\n- 测试用例 6（零目标映射）：$h=10$，$\\alpha=0.5$，$\\sigma = [0.0, 0.0, 0.0, 0.0, 0.0]$。\n\n你的程序应生成单行输出，其中包含按顺序排列的各用例的秩，格式为逗号分隔的列表并用方括号括起，例如，“[r1,r2,r3,r4,r5,r6]”。不涉及物理单位或角度单位，所有输出均为整数。解决方案必须是完全自包含的，并且可以在任何现代编程语言中运行；然而，最终答案必须按照下文规定以 Python 程序形式提供。",
            "solution": "用户要求对一个带有参数范数惩罚项的双层网络所产生的最优线性映射的秩，给出一个完整的刻画和算法解。\n\n### 问题验证\n\n首先，我将验证问题陈述。\n\n#### 第一步：提取已知条件\n-   **网络结构**：一个由映射 $M = W_2 W_1$ 定义的双层线性网络，其中 $W_1 \\in \\mathbb{R}^{h \\times d}$ 且 $W_2 \\in \\mathbb{R}^{p \\times h}$。维度包括输入 $d$、输出 $p$ 和隐藏层宽度 $h$。\n-   **目标**：一个目标线性映射 $T \\in \\mathbb{R}^{p \\times d}$。\n-   **目标函数**：需要最小化的正则化经验风险为\n    $$\n    \\mathcal{L}(W_2, W_1) = \\frac{1}{2} \\lVert W_2 W_1 - T \\rVert_F^2 + \\frac{\\alpha}{2}\\left(\\lVert W_1 \\rVert_F^2 + \\lVert W_2 \\rVert_F^2\\right)\n    $$\n    其中正则化系数 $\\alpha \\ge 0$。\n-   **范数**：$\\lVert \\cdot \\rVert_F$ 表示弗罗贝尼乌斯范数。\n-   **约束**：宽度 $h$ 对映射 $M$ 的秩施加了硬性约束，因为 $\\text{rank}(M) = \\text{rank}(W_2 W_1) \\le \\min(\\text{rank}(W_2), \\text{rank}(W_1)) \\le h$。\n-   **任务**：刻画由最小化 $\\mathcal{L}(W_2, W_1)$ 产生的最优映射 $M^\\star$，设计一个算法来计算其秩，并将该算法应用于几个测试用例。\n-   **测试数据**：在所有情况下，$p=d=5$，且 $T$ 是一个对角矩阵，其指定的奇异值为 $\\sigma$。\n    -   测试用例 1：$h=10$，$\\alpha=0.0$，$\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$。\n    -   测试用例 2：$h=10$，$\\alpha=1.0$，$\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$。\n    -   测试用例 3：$h=10$，$\\alpha=0.7$，$\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$。\n    -   测试用例 4：$h=10$，$\\alpha=3.5$，$\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$。\n    -   测试用例 5：$h=2$，$\\alpha=0.7$，$\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$。\n    -   测试用例 6：$h=10$，$\\alpha=0.5$，$\\sigma = [0.0, 0.0, 0.0, 0.0, 0.0]$。\n\n#### 第二步：使用提取的已知条件进行验证\n该问题定义明确且具有科学依据。它探讨了深度学习理论中的一个基本问题：由显式参数范数惩罚项所引发的隐式正则化的性质。\n-   **科学合理性**：该问题使用线性代数和优化中的标准、成熟概念进行表述，包括弗罗贝尼乌斯范数、核范数和奇异值分解 (SVD)。对此类损失函数的分析是机器学习研究中的一个常见课题。\n-   **适定性**：一旦根据有效映射 $M$ 重新参数化，目标函数可以被证明是一个凸优化问题，存在唯一的最小化解 $M^\\star$。因此，该问题是适定的。\n-   **客观性**：语言精确且数学化，没有歧义或主观性陈述。\n\n该问题通过了所有验证标准。它是一个有效且有实质内容的理论机器学习问题。\n\n#### 第三步：结论与行动\n问题有效。我现在将着手解决。\n\n### 最优映射 $M^\\star$ 的推导\n\n问题的核心是理解正则化项 $\\frac{\\alpha}{2}(\\lVert W_1 \\rVert_F^2 + \\lVert W_2 \\rVert_F^2)$ 如何影响乘积矩阵 $M = W_2 W_1$ 的性质。我们将证明这种形式的正则化等价于对 $M$ 的核范数进行惩罚。\n\n#### 基本定义\n1.  **弗罗贝尼乌斯范数**：对于一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$，其元素为 $a_{ij}$，弗罗贝尼乌斯范数为 $\\lVert A \\rVert_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n a_{ij}^2} = \\sqrt{\\text{Tr}(A^T A)}$。\n2.  **奇异值分解 (SVD)**：任何矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 都可以分解为 $A = U \\Sigma V^T$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是一个对角矩阵，其对角线上的非负元素 $\\sigma_i(A)$ 称为奇异值。\n3.  **核范数**：矩阵 $A$ 的核范数（或迹范数）是其奇异值之和：$\\lVert A \\rVert_* = \\sum_i \\sigma_i(A) = \\text{Tr}(\\sqrt{A^T A})$。\n\n#### 重构目标函数\n目标函数为\n$$\n\\mathcal{L}(W_2, W_1) = \\frac{1}{2} \\lVert M - T \\rVert_F^2 + \\frac{\\alpha}{2}\\left(\\lVert W_1 \\rVert_F^2 + \\lVert W_2 \\rVert_F^2\\right),\n$$\n其中 $M = W_2 W_1$。这是一个关于参数 $(W_2, W_1)$ 的优化问题，属于非凸问题。然而，我们可以将其改写为一个关于矩阵 $M$ 的优化问题。对于任何固定的矩阵 $M$，最小化 $\\mathcal{L}$ 需要找到能够生成 $M$ 且同时最小化惩罚项 $\\lVert W_1 \\rVert_F^2 + \\lVert W_2 \\rVert_F^2$ 的因子 $W_1, W_2$。\n\n矩阵分析中的一个关键结论指出，对于任何矩阵 $M \\in \\mathbb{R}^{p \\times d}$ 以及任何因子分解 $M = W_2 W_1$（其中 $W_1 \\in \\mathbb{R}^{h \\times d}$ 且 $W_2 \\in \\mathbb{R}^{p \\times h}$），以下不等式成立：\n$$\n\\lVert W_1 \\rVert_F^2 + \\lVert W_2 \\rVert_F^2 \\ge 2 \\lVert M \\rVert_*\n$$\n如果隐藏维度 $h$ 足够大，特别是 $h \\ge \\text{rank}(M)$，这个下界是可以达到的。最小值通过一个源于 $M$ 的 SVD 的“平衡”因子分解实现。设 $M=U \\Sigma V^T$ 的秩为 $k$。一个最小范数分解是 $W_2 = U_k (\\Sigma_k)^{1/2}$ 和 $W_1 = (\\Sigma_k)^{1/2} V_k^T$，其中下标 $k$ 表示 SVD 中对应于 $k$ 个非零奇异值的部分。对于此选择，$\\lVert W_1 \\rVert_F^2 = \\lVert W_2 \\rVert_F^2 = \\text{Tr}(\\Sigma_k) = \\lVert M \\rVert_*$。\n\n因此，在隐藏维度为 $h$ 的所有可能因子 $(W_1, W_2)$ 上最小化 $\\mathcal{L}(W_2, W_1)$ 等价于求解关于 $M$ 的以下问题：\n$$\n\\min_{M \\in \\mathbb{R}^{p \\times d}} \\left( \\frac{1}{2} \\lVert M - T \\rVert_F^2 + \\alpha \\lVert M \\rVert_* \\right) \\quad \\text{subject to} \\quad \\text{rank}(M) \\le h.\n$$\n这是一个带有附加秩约束的凸优化问题（核范数正则化）。\n\n#### 求解最优奇异值\n设目标矩阵的 SVD 为 $T = U_T \\Sigma_T V_T^T$，其中 $\\Sigma_T = \\text{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_r)$，$r = \\min(p, d)$ 且 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$。由于弗罗贝尼乌斯范数和核范数的酉不变性，最优解 $M^\\star$ 必须与 $T$ 共享相同的奇异向量。因此，$M^\\star = U_T \\Sigma_M V_T^T$，其中 $\\Sigma_M = \\text{diag}(s_1, s_2, \\dots, s_r)$ 是某个对角矩阵，其对角元素 $s_1 \\ge s_2 \\ge \\dots \\ge 0$。\n\n将这些形式代入目标函数，它会解耦为一系列标量问题，每个奇异值对应一个：\n$$\n\\min_{s_1, \\dots, s_r} \\sum_{i=1}^r \\left( \\frac{1}{2} (s_i - \\sigma_i)^2 + \\alpha s_i \\right) \\quad \\text{subject to} \\quad s_i \\ge 0 \\quad \\text{and} \\quad |\\{i : s_i  0\\}| \\le h.\n$$\n我们先不考虑秩约束，为每个分量 $i$ 求解最优的 $s_i$。我们需要求解 $\\min_{s_i \\ge 0} f(s_i) = \\frac{1}{2}(s_i - \\sigma_i)^2 + \\alpha s_i$。其导数为 $f'(s_i) = s_i - \\sigma_i + \\alpha$。令其为零得到 $s_i = \\sigma_i - \\alpha$。由于 $s_i$ 必须为非负数，解为 $s_i = \\max(0, \\sigma_i - \\alpha)$。这个操作被称为软阈值算子（soft-thresholding operator）。\n\n#### 考虑有限 $h$ 带来的秩约束\n隐藏层宽度 $h$ 施加了一个硬约束 $\\text{rank}(M) \\le h$，这意味着最多只有 $h$ 个奇异值 $s_i$ 可以非零。为了最小化总目标函数，我们必须决定哪些奇异值要“保留”（允许非零），哪些要“置零”。\n\n对于每个索引 $i$，我们可以设 $s_i=0$ 或 $s_i = \\max(0, \\sigma_i-\\alpha)$。\n-   如果我们设 $s_i=0$，对目标函数的贡献是 $\\frac{1}{2}\\sigma_i^2$。\n-   如果我们设 $s_i = \\max(0, \\sigma_i-\\alpha)$，贡献是：\n    -   如果 $\\sigma_i \\le \\alpha$，则 $s_i=0$，贡献为 $\\frac{1}{2}\\sigma_i^2$。“保留”这个奇异值没有好处。\n    -   如果 $\\sigma_i  \\alpha$，则 $s_i=\\sigma_i-\\alpha  0$。贡献为 $\\frac{1}{2}((\\sigma_i-\\alpha)-\\sigma_i)^2 + \\alpha(\\sigma_i-\\alpha) = \\frac{1}{2}\\alpha^2 + \\alpha\\sigma_i - \\alpha^2 = \\alpha\\sigma_i - \\frac{1}{2}\\alpha^2$。\n\n保留第 $i$ 个奇异值（若 $\\sigma_i  \\alpha$）相对于将其强制归零所带来的损失减少量为 $\\text{Gain}_i = (\\frac{1}{2}\\sigma_i^2) - (\\alpha\\sigma_i - \\frac{1}{2}\\alpha^2) = \\frac{1}{2}(\\sigma_i - \\alpha)^2$。\n\n为了在最多保留 $h$ 个奇异值的预算下最大化总增益，我们应该贪心地选择保留那些提供最大增益的奇异值。由于增益随 $\\sigma_i$ 单调增加（对于 $\\sigma_i  \\alpha$），我们应该保留与最大的 $\\sigma_i$ 值相对应的奇异值，前提是它们大于 $\\alpha$。\n\n#### 最终算法\n这导出了一个简单的算法来确定最优映射 $M^\\star$ 的秩：\n1.  设目标矩阵 $T$ 的奇异值为 $\\{\\sigma_i\\}$。\n2.  计算满足 $\\sigma_i  \\alpha$ 的奇异值的数量，记为 $k$。这些是在无约束情况下通过软阈值操作后能够存活的奇异值。\n3.  最优解 $M^\\star$ 的秩既受正则化（为非零奇异值设定阈值）的限制，也受网络架构（为秩设定硬上限）的限制。\n4.  因此，最终的秩为 $\\text{rank}(M^\\star) = \\min(h, k)$。\n\n这提供了一个完整的刻画。$M^\\star$ 的奇异值由 $s_i^\\star = \\max(0, \\sigma_i - \\alpha)$ 给出，适用于前 $\\min(h, k)$ 个索引（假设 $\\sigma_i$ 按降序排列），而对于所有其他索引，$s_i^\\star = 0$。该算法直接计算这些非零值的数量。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the rank of an optimal linear mapping\n    in a regularized two-layer network.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (h, alpha, sigma_values)\n    test_cases = [\n        (10, 0.0, [3.0, 2.5, 1.0, 0.8, 0.2]),  # Case 1\n        (10, 1.0, [3.0, 2.5, 1.0, 0.8, 0.2]),  # Case 2\n        (10, 0.7, [3.0, 2.5, 1.0, 0.8, 0.2]),  # Case 3\n        (10, 3.5, [3.0, 2.5, 1.0, 0.8, 0.2]),  # Case 4\n        (2, 0.7, [3.0, 2.5, 1.0, 0.8, 0.2]),   # Case 5\n        (10, 0.5, [0.0, 0.0, 0.0, 0.0, 0.0])   # Case 6\n    ]\n\n    results = []\n    \n    for h, alpha, sigma in test_cases:\n        # The derivation shows that minimizing the objective for (W2, W1)\n        # is equivalent to solving a nuclear norm regularized problem for M = W2*W1:\n        # min_M (1/2 * ||M - T||_F^2 + alpha * ||M||_*)\n        # subject to rank(M) = h.\n        #\n        # The solution to this problem is given by singular value thresholding.\n        # The optimal singular values of M, s_i*, are related to the singular\n        # values of T, sigma_i, by the soft-thresholding operator:\n        # s_i* = max(0, sigma_i - alpha).\n        #\n        # The rank of the unconstrained solution is the number of singular\n        # values of T that are greater than the regularization parameter alpha.\n        \n        # Count the number of singular values of T greater than alpha.\n        # This is the rank of the optimal solution if there were no 'h' constraint.\n        k = 0\n        for s_val in sigma:\n            if s_val > alpha:\n                k += 1\n        \n        # The hidden layer width 'h' imposes a hard constraint on the rank of M.\n        # rank(M) = rank(W2 * W1) = min(rank(W1), rank(W2)) = h.\n        # Thus, the final rank is the minimum of the unconstrained rank 'k'\n        # and the architectural rank limit 'h'.\n        rank = min(h, k)\n        results.append(rank)\n\n    # Format the results into the required string format: \"[r1,r2,r3,r4,r5,r6]\"\n    # np.array2string can be used for robust formatting.\n    # The output format requires no spaces and integer representation.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在使用参数范数惩罚时，一个关键的挑战是如何选择正则化超参数 $\\lambda$。最后一个练习  将介绍一种强大且自动化的方法来解决这个问题——双层优化（bilevel optimization）。您将不再把 $\\lambda$ 视为一个需要手动调整的固定值，而是将其作为一个可通过最小化验证损失来学习的参数。这需要您运用隐函数定理推导出“超梯度”（hypergradient），即验证损失相对于 $\\lambda$ 的梯度，从而让您一窥驱动现代自动化机器学习的先进技术。",
            "id": "3161386",
            "problem": "一个带有参数向量 $\\mathbf{w} \\in \\mathbb{R}^{d}$ 的模型在一个训练集 $(\\mathbf{X}_{\\mathrm{tr}}, \\mathbf{y}_{\\mathrm{tr}})$ 上通过经验风险最小化（ERM）进行训练，使用带有参数范数惩罚项（平方欧几里得范数）的平方误差目标函数。惩罚项的权重超参数 $\\lambda \\ge 0$ 在一个双层优化中被视为可学习的参数，其中内层问题最小化训练目标以产生 $\\mathbf{w}^{\\star}(\\lambda)$，外层问题在 $(\\mathbf{X}_{\\mathrm{val}}, \\mathbf{y}_{\\mathrm{val}})$ 上最小化验证损失 $\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda))$。仅从以下定义和第一性原理出发，通过隐式微分推导验证损失如何随 $\\lambda$ 变化，并评估内循环的稳定性。\n\n设训练目标为\n$$\n\\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda) \\;=\\; \\frac{1}{2n}\\|\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{tr}}\\|_{2}^{2} \\;+\\; \\frac{\\lambda}{2}\\|\\mathbf{w}\\|_{2}^{2},\n$$\n验证损失为\n$$\n\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}) \\;=\\; \\frac{1}{2m}\\|\\mathbf{X}_{\\mathrm{val}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{val}}\\|_{2}^{2}.\n$$\n内层问题的最小化器定义为\n$$\n\\mathbf{w}^{\\star}(\\lambda) \\;=\\; \\arg\\min_{\\mathbf{w}\\in\\mathbb{R}^{d}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda).\n$$\n\n任务：\n- 仅使用内层问题的一阶最优性条件和标准线性代数，推导 $\\mathbf{w}^{\\star}(\\lambda)$ 关于 $\\mathbf{X}_{\\mathrm{tr}}$、$\\mathbf{y}_{\\mathrm{tr}}$ 和 $\\lambda$ 的显式公式。\n- 仅使用定义、链式法则和基本矩阵微积分（包括逆矩阵的导数），推导超梯度 $\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda))$ 的闭式表达式。\n- 现在考虑一个具体实例，其中 $n=3$, $d=2$, $m=2$，\n$$\n\\mathbf{X}_{\\mathrm{tr}} \\;=\\; \\begin{pmatrix} 1  0 \\\\[4pt] 0  1 \\\\[4pt] 1  1 \\end{pmatrix}, \n\\quad\n\\mathbf{y}_{\\mathrm{tr}} \\;=\\; \\begin{pmatrix} 1 \\\\[4pt] 1 \\\\[4pt] 2 \\end{pmatrix},\n\\quad\n\\mathbf{X}_{\\mathrm{val}} \\;=\\; \\begin{pmatrix} 1  1 \\\\[4pt] 2  0 \\end{pmatrix},\n\\quad\n\\mathbf{y}_{\\mathrm{val}} \\;=\\; \\begin{pmatrix} 2 \\\\[4pt] 0.8 \\end{pmatrix},\n$$\n并计算 $\\left.\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda))\\right|_{\\lambda=\\frac{1}{3}}$。将你的最终数值答案四舍五入到四位有效数字。\n- 对于内循环稳定性，考虑在 $\\mathcal{L}_{\\mathrm{tr}}$ 上使用固定步长 $\\alpha  0$ 的梯度下降法。使用 $\\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda)$ 的Hessian矩阵的最大特征值，陈述确保局部稳定性（不发散的迭代）的 $\\alpha$ 条件，并数值计算在 $\\lambda=\\frac{1}{3}$ 时 $\\alpha$ 的允许范围。你最终报告的答案必须仅为 $\\left.\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda))\\right|_{\\lambda=\\frac{1}{3}}$ 的值。",
            "solution": "问题陈述已经过验证，被认为是科学上合理的、良定的，并且没有矛盾或歧义。它代表了正则化线性回归背景下用于超参数调整的标准双层优化问题。因此，我们可以进行完整的解答。\n\n以下是此问题的给定条件：\n训练目标是 $\\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda) = \\frac{1}{2n}\\|\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{tr}}\\|_{2}^{2} + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|_{2}^{2}$。\n验证损失是 $\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}) = \\frac{1}{2m}\\|\\mathbf{X}_{\\mathrm{val}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{val}}\\|_{2}^{2}$。\n内循环的最优参数是 $\\mathbf{w}^{\\star}(\\lambda) = \\arg\\min_{\\mathbf{w}\\in\\mathbb{R}^{d}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda)$。\n具体实例的数值为 $n=3$, $d=2$, $m=2$，\n$\\mathbf{X}_{\\mathrm{tr}} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix}$，\n$\\mathbf{y}_{\\mathrm{tr}} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix}$，\n$\\mathbf{X}_{\\mathrm{val}} = \\begin{pmatrix} 1  1 \\\\ 2  0 \\end{pmatrix}$，\n$\\mathbf{y}_{\\mathrm{val}} = \\begin{pmatrix} 2 \\\\ 0.8 \\end{pmatrix}$，\n求值点是 $\\lambda=\\frac{1}{3}$。\n\n问题按要求分为四个部分进行解答。\n\n**第1部分：推导 $\\mathbf{w}^{\\star}(\\lambda)$ 的显式公式**\n\n训练目标 $\\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda)$ 是关于 $\\mathbf{w}$ 的凸函数。通过将关于 $\\mathbf{w}$ 的梯度设置为零向量，可以找到唯一的最小化器 $\\mathbf{w}^{\\star}(\\lambda)$。目标函数可以用矩阵形式写成：\n$$\n\\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda) = \\frac{1}{2n}(\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{tr}})^{T}(\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{tr}}) + \\frac{\\lambda}{2}\\mathbf{w}^{T}\\mathbf{w}\n$$\n关于 $\\mathbf{w}$ 的梯度是：\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda) = \\frac{1}{2n} \\cdot 2\\mathbf{X}_{\\mathrm{tr}}^{T}(\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{tr}}) + \\frac{\\lambda}{2} \\cdot 2\\mathbf{w} = \\frac{1}{n}(\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}}) + \\lambda\\mathbf{w}\n$$\n在 $\\mathbf{w} = \\mathbf{w}^{\\star}(\\lambda)$ 处将梯度设为零：\n$$\n\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w}^{\\star}(\\lambda) - \\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}} + \\lambda\\mathbf{w}^{\\star}(\\lambda) = \\mathbf{0}\n$$\n重新整理各项以求解 $\\mathbf{w}^{\\star}(\\lambda)$：\n$$\n\\left(\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}} + \\lambda\\mathbf{I}\\right)\\mathbf{w}^{\\star}(\\lambda) = \\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}}\n$$\n对于 $\\lambda > 0$，矩阵 $(\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}} + \\lambda\\mathbf{I})$ 保证是可逆的，因为 $\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}}$ 是半正定的。因此，我们可以写出 $\\mathbf{w}^{\\star}(\\lambda)$ 的显式公式：\n$$\n\\mathbf{w}^{\\star}(\\lambda) = \\left(\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}} + \\lambda\\mathbf{I}\\right)^{-1} \\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}}\n$$\n\n**第2部分：推导超梯度 $\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda))$**\n\n我们寻求计算验证损失相对于超参数 $\\lambda$ 的导数。验证损失是 $\\mathbf{w}$ 的函数，而 $\\mathbf{w}$ 又是 $\\lambda$ 的函数。使用链式法则：\n$$\n\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda)) = \\left. \\frac{\\partial\\mathcal{L}_{\\mathrm{val}}}{\\partial\\mathbf{w}^{T}} \\right|_{\\mathbf{w}=\\mathbf{w}^{\\star}(\\lambda)} \\frac{d\\mathbf{w}^{\\star}(\\lambda)}{d\\lambda} = \\left(\\nabla_{\\mathbf{w}}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda))\\right)^{T} \\frac{d\\mathbf{w}^{\\star}(\\lambda)}{d\\lambda}\n$$\n验证损失的梯度是：\n$$\n\\nabla_{\\mathbf{w}}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}) = \\frac{1}{m}\\mathbf{X}_{\\mathrm{val}}^{T}(\\mathbf{X}_{\\mathrm{val}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{val}})\n$$\n为了求出 $\\frac{d\\mathbf{w}^{\\star}(\\lambda)}{d\\lambda}$ 这一项，我们对内层问题的一阶最优性条件使用隐式微分，该条件对所有相关的 $\\lambda$ 都必须成立：\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}^{\\star}(\\lambda), \\lambda) = \\mathbf{0}\n$$\n对这个恒等式关于 $\\lambda$ 求导：\n$$\n\\frac{d}{d\\lambda} \\left[ \\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}^{\\star}(\\lambda), \\lambda) \\right] = \\mathbf{0}\n$$\n应用多元链式法则：\n$$\n\\left(\\nabla_{\\mathbf{w}}^{2} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}^{\\star}(\\lambda), \\lambda)\\right) \\frac{d\\mathbf{w}^{\\star}(\\lambda)}{d\\lambda} + \\frac{\\partial}{\\partial\\lambda}\\left(\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}^{\\star}(\\lambda), \\lambda)\\right) = \\mathbf{0}\n$$\n第一项包含训练损失的Hessian矩阵，$\\mathbf{H}_{\\mathrm{tr}} = \\nabla_{\\mathbf{w}}^{2} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda)$:\n$$\n\\mathbf{H}_{\\mathrm{tr}} = \\nabla_{\\mathbf{w}}\\left(\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}} + \\lambda\\mathbf{w}\\right) = \\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}} + \\lambda\\mathbf{I}\n$$\n第二项是梯度关于 $\\lambda$ 的偏导数：\n$$\n\\frac{\\partial}{\\partial\\lambda}\\left(\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{tr}}\\right) = \\frac{\\partial}{\\partial\\lambda}\\left(\\frac{1}{n}(\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}}) + \\lambda\\mathbf{w}\\right) = \\mathbf{w}\n$$\n将这些代入隐式微分方程并在 $\\mathbf{w} = \\mathbf{w}^{\\star}(\\lambda)$ 处求值：\n$$\n\\mathbf{H}_{\\mathrm{tr}} \\frac{d\\mathbf{w}^{\\star}}{d\\lambda} + \\mathbf{w}^{\\star}(\\lambda) = \\mathbf{0}\n$$\n解出 $\\frac{d\\mathbf{w}^{\\star}}{d\\lambda}$：\n$$\n\\frac{d\\mathbf{w}^{\\star}(\\lambda)}{d\\lambda} = - \\mathbf{H}_{\\mathrm{tr}}^{-1} \\mathbf{w}^{\\star}(\\lambda) = - \\left(\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}} + \\lambda\\mathbf{I}\\right)^{-1} \\mathbf{w}^{\\star}(\\lambda)\n$$\n将此代回超梯度的表达式：\n$$\n\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda)) = - \\left(\\nabla_{\\mathbf{w}}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda))\\right)^{T} \\left(\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}} + \\lambda\\mathbf{I}\\right)^{-1} \\mathbf{w}^{\\star}(\\lambda)\n$$\n\n**第3部分：在 $\\lambda = 1/3$ 时的数值计算**\n\n首先，我们使用提供的数据计算必要的矩阵：\n$$\n\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}\n$$\n$$\n\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}\n$$\n当 $n=3$ 且 $\\lambda=1/3$ 时，训练损失的Hessian矩阵为：\n$$\n\\mathbf{H}_{\\mathrm{tr}} = \\frac{1}{3}\\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} + \\frac{1}{3}\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 3  1 \\\\ 1  3 \\end{pmatrix} = \\begin{pmatrix} 1  1/3 \\\\ 1/3  1 \\end{pmatrix}\n$$\nHessian矩阵的逆是：\n$$\n\\mathbf{H}_{\\mathrm{tr}}^{-1} = \\frac{1}{1 \\cdot 1 - (\\frac{1}{3} \\cdot \\frac{1}{3})} \\begin{pmatrix} 1  -1/3 \\\\ -1/3  1 \\end{pmatrix} = \\frac{1}{8/9} \\begin{pmatrix} 1  -1/3 \\\\ -1/3  1 \\end{pmatrix} = \\frac{9}{8} \\begin{pmatrix} 1  -1/3 \\\\ -1/3  1 \\end{pmatrix}\n$$\n接下来，我们计算在 $\\lambda=1/3$ 时的 $\\mathbf{w}^{\\star}(\\lambda)$：\n$$\n\\mathbf{w}^{\\star}(\\frac{1}{3}) = \\mathbf{H}_{\\mathrm{tr}}^{-1} \\left(\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}}\\right) = \\frac{9}{8}\\begin{pmatrix} 1  -1/3 \\\\ -1/3  1 \\end{pmatrix} \\left(\\frac{1}{3}\\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}\\right) = \\frac{9}{8}\\begin{pmatrix} 1  -1/3 \\\\ -1/3  1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{9}{8}\\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix} = \\begin{pmatrix} 3/4 \\\\ 3/4 \\end{pmatrix}\n$$\n现在，我们计算在 $\\mathbf{w}^{\\star}(1/3)$ 处的验证损失梯度。我们将其表示为 $\\mathbf{g}_{\\mathrm{val}}$：\n$$\n\\mathbf{g}_{\\mathrm{val}} = \\nabla_{\\mathbf{w}}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\frac{1}{3})) = \\frac{1}{m}\\mathbf{X}_{\\mathrm{val}}^{T}\\left(\\mathbf{X}_{\\mathrm{val}}\\mathbf{w}^{\\star}(\\frac{1}{3}) - \\mathbf{y}_{\\mathrm{val}}\\right)\n$$\n当 $m=2$ 时：\n$$\n\\mathbf{X}_{\\mathrm{val}}\\mathbf{w}^{\\star}(\\frac{1}{3}) - \\mathbf{y}_{\\mathrm{val}} = \\begin{pmatrix} 1  1 \\\\ 2  0 \\end{pmatrix}\\begin{pmatrix} 3/4 \\\\ 3/4 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0.8 \\end{pmatrix} = \\begin{pmatrix} 3/4 + 3/4 \\\\ 2(3/4) \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0.8 \\end{pmatrix} = \\begin{pmatrix} 1.5 \\\\ 1.5 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0.8 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ 0.7 \\end{pmatrix}\n$$\n$$\n\\mathbf{g}_{\\mathrm{val}} = \\frac{1}{2}\\begin{pmatrix} 1  2 \\\\ 1  0 \\end{pmatrix}\\begin{pmatrix} -0.5 \\\\ 0.7 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} -0.5+1.4 \\\\ -0.5 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} 0.9 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 0.45 \\\\ -0.25 \\end{pmatrix}\n$$\n最后，我们计算超梯度：\n$$\n\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}} = - \\mathbf{g}_{\\mathrm{val}}^{T} \\mathbf{H}_{\\mathrm{tr}}^{-1} \\mathbf{w}^{\\star}(\\frac{1}{3})\n$$\n我们计算 $\\mathbf{H}_{\\mathrm{tr}}^{-1} \\mathbf{w}^{\\star}(\\frac{1}{3})$：\n$$\n\\mathbf{H}_{\\mathrm{tr}}^{-1} \\mathbf{w}^{\\star}(\\frac{1}{3}) = \\frac{9}{8}\\begin{pmatrix} 1  -1/3 \\\\ -1/3  1 \\end{pmatrix} \\begin{pmatrix} 3/4 \\\\ 3/4 \\end{pmatrix} = \\frac{9}{8}\\begin{pmatrix} 3/4 - 1/4 \\\\ -1/4 + 3/4 \\end{pmatrix} = \\frac{9}{8}\\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 9/16 \\\\ 9/16 \\end{pmatrix}\n$$\n然后，超梯度是：\n$$\n\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}} = - \\begin{pmatrix} 0.45  -0.25 \\end{pmatrix} \\begin{pmatrix} 9/16 \\\\ 9/16 \\end{pmatrix} = - \\left( 0.45 \\cdot \\frac{9}{16} - 0.25 \\cdot \\frac{9}{16} \\right) = - \\frac{9}{16} (0.45 - 0.25) = - \\frac{9}{16} (0.2) = - \\frac{9}{16} \\cdot \\frac{1}{5} = -\\frac{9}{80}\n$$\n以十进制形式表示，这是-0.1125。\n\n**第4部分：内循环稳定性分析**\n\n内循环的梯度下降更新由 $\\mathbf{w}_{k+1} = \\mathbf{w}_{k} - \\alpha \\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}_k, \\lambda)$ 给出。如果更新映射的雅可比矩阵的谱半径小于1，则该过程在最小化器 $\\mathbf{w}^\\star$ 附近是局部稳定的。更新映射为 $\\mathbf{F}(\\mathbf{w}) = \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda)$。其雅可比矩阵为：\n$$\n\\mathbf{J}_{\\mathbf{F}}(\\mathbf{w}) = \\nabla_{\\mathbf{w}}\\mathbf{F}(\\mathbf{w}) = \\mathbf{I} - \\alpha \\nabla_{\\mathbf{w}}^{2} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda) = \\mathbf{I} - \\alpha \\mathbf{H}_{\\mathrm{tr}}\n$$\n稳定性要求 $\\rho(\\mathbf{I} - \\alpha \\mathbf{H}_{\\mathrm{tr}})  1$，其中 $\\rho(\\cdot)$ 是谱半径。$\\mathbf{I} - \\alpha \\mathbf{H}_{\\mathrm{tr}}$ 的特征值是 $1 - \\alpha\\mu_i$，其中 $\\mu_i$ 是 $\\mathbf{H}_{\\mathrm{tr}}$ 的特征值。条件变为对所有 $i$ 都有 $|1 - \\alpha\\mu_i|1$。由于当 $\\lambda > 0$ 时 $\\mathbf{H}_{\\mathrm{tr}}$ 是正定的，所有 $\\mu_i > 0$。该条件简化为 $0  \\alpha  2/\\mu_{\\max}(\\mathbf{H}_{\\mathrm{tr}})$。我们必须找到在 $\\lambda=1/3$ 时 $\\mathbf{H}_{\\mathrm{tr}}$ 的最大特征值。\n$$\n\\mathbf{H}_{\\mathrm{tr}} = \\begin{pmatrix} 1  1/3 \\\\ 1/3  1 \\end{pmatrix}\n$$\n特征方程是 $\\det(\\mathbf{H}_{\\mathrm{tr}} - \\mu\\mathbf{I}) = (1-\\mu)^{2} - (1/3)^{2} = 0$。\n这给出 $1-\\mu = \\pm 1/3$，所以特征值为 $\\mu_1 = 1 - 1/3 = 2/3$ 和 $\\mu_2 = 1 + 1/3 = 4/3$。\n最大特征值为 $\\mu_{\\max} = 4/3$。步长 $\\alpha$ 的稳定性条件是：\n$$\n0  \\alpha  \\frac{2}{4/3} = \\frac{3}{2}\n$$\n为确保局部稳定，$\\alpha$ 的允许范围是 $(0, 1.5)$。\n\n最终报告的答案是第3部分的数值。",
            "answer": "$$\n\\boxed{-0.1125}\n$$"
        }
    ]
}