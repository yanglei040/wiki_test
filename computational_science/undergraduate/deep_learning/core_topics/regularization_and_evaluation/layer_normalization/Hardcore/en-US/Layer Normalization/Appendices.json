{
    "hands_on_practices": [
        {
            "introduction": "The Layer Normalization formula includes a small constant, $\\epsilon$, added to the variance in the denominator. While it may seem like a minor detail, this term is a critical safeguard for numerical stability. This exercise will guide you through a \"pathological\" scenario where an input's feature variance collapses towards zero . By analyzing the gradient in this limit, you will gain a first-hand understanding of how $\\epsilon$ prevents the catastrophic explosion of gradients, ensuring that the learning process remains stable.",
            "id": "3142048",
            "problem": "Consider a single application of Layer Normalization (LN) to a layer with feature dimension $d=3$. For an input vector $x \\in \\mathbb{R}^{3}$, LN computes the mean $\\mu(x)$ and variance $\\sigma^{2}(x)$ over features, and returns the normalized output $y(x)$ with a shared scale parameter $\\gamma$ and shared shift parameter $\\beta$ defined by\n$$\n\\mu(x) = \\frac{1}{3}\\sum_{i=1}^{3} x_{i},\\qquad\n\\sigma^{2}(x) = \\frac{1}{3}\\sum_{i=1}^{3} \\big(x_{i}-\\mu(x)\\big)^{2},\\qquad\ny_{i}(x) = \\gamma \\,\\frac{x_{i}-\\mu(x)}{\\sqrt{\\sigma^{2}(x)+\\epsilon}} + \\beta,\n$$\nwhere $\\epsilon>0$ is a small stabilizing constant.\n\nConstruct a pathological input sequence that approaches a degenerate direction (near-zero variance) by letting\n$$\nx(\\alpha) = c\\,\\mathbf{1} + \\alpha\\,v,\\quad \\text{with}\\quad \\mathbf{1}=\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix},\\quad v=\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix},\\quad c\\in\\mathbb{R},\\quad \\alpha>0,\n$$\nand choose $\\beta=0$ and a shared positive scale $\\gamma=g>0$. Define the scalar loss to be the first LN output coordinate\n$$\nL(x) = y_{1}(x).\n$$\n\nTasks:\n1. Starting only from the definitions of $\\mu(x)$ and $\\sigma^{2}(x)$, show that along $x(\\alpha)$ one has $\\mu\\big(x(\\alpha)\\big)=c$ and $\\sigma^{2}\\big(x(\\alpha)\\big)=\\alpha^{2}\\,s_{v}^{2}$, where $s_{v}^{2}=\\frac{1}{3}\\sum_{i=1}^{3} v_{i}^{2}$.\n2. Using the chain rule and the above core definitions, derive the gradient $\\nabla_{x}L\\big(x(\\alpha)\\big)$ for general $\\alpha>0$ and $\\epsilon>0$.\n3. Evaluate the limit of this gradient as $\\alpha \\to 0^{+}$ (equivalently, as $\\sigma^{2}\\to 0$), and report the limiting gradient vector with respect to $x$ as a row matrix. This row matrix will be your final reported answer.\n4. In your derivation, explain how the presence of $\\epsilon>0$ prevents blow-up of the gradient near degenerate directions by comparing the scaling you obtain when hypothetically $\\epsilon=0$.\n\nNo numerical rounding is required. Your final answer must be the single limiting gradient vector in closed form, expressed symbolically in terms of $g$ and $\\epsilon$, and reported as a row matrix.",
            "solution": "The problem asks for the derivation and analysis of the gradient of a Layer Normalization (LN) output with respect to its input, particularly in the limit of near-zero variance.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n-   **Domain**: Input vector $x \\in \\mathbb{R}^{3}$ (feature dimension $d=3$).\n-   **Layer Normalization Definitions**:\n    -   Mean: $\\mu(x) = \\frac{1}{3}\\sum_{i=1}^{3} x_{i}$\n    -   Variance: $\\sigma^{2}(x) = \\frac{1}{3}\\sum_{i=1}^{3} \\big(x_{i}-\\mu(x)\\big)^{2}$\n    -   Output: $y_{i}(x) = \\gamma \\,\\frac{x_{i}-\\mu(x)}{\\sqrt{\\sigma^{2}(x)+\\epsilon}} + \\beta$\n-   **Stabilizing Constant**: $\\epsilon>0$.\n-   **Pathological Input Sequence**: $x(\\alpha) = c\\,\\mathbf{1} + \\alpha\\,v$, where $\\mathbf{1}=\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}$, $v=\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}$, $c\\in\\mathbb{R}$, and $\\alpha>0$.\n-   **Parameters**: Shared shift $\\beta=0$, shared positive scale $\\gamma=g>0$.\n-   **Loss Function**: $L(x) = y_{1}(x)$.\n-   **Auxiliary Definition**: $s_{v}^{2}=\\frac{1}{3}\\sum_{i=1}^{3} v_{i}^{2}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n-   **Scientifically Grounded**: The problem is rooted in the field of deep learning, a subfield of computer science and applied mathematics. The definitions for Layer Normalization are standard. The analysis of gradients in degenerate cases is a critical aspect of understanding the numerical stability of optimization algorithms for neural networks.\n-   **Well-Posed**: The problem provides all necessary definitions, constraints, and variables to perform the requested derivations and analysis. It asks for a specific limiting gradient, which, as will be shown, exists and is unique under the given conditions.\n-   **Objective**: The problem is stated in precise mathematical language, free of subjective or ambiguous terminology.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. I will proceed with the solution.\n\nThe solution is structured according to the four tasks outlined in the problem. The dimension is fixed at $d=3$.\n\n### Task 1: Mean and Variance of $x(\\alpha)$\nThe input sequence is defined as $x(\\alpha) = c\\,\\mathbf{1} + \\alpha\\,v$. In component form, $x_i(\\alpha) = c + \\alpha v_i$ for $i \\in \\{1, 2, 3\\}$.\n\nFirst, we compute the mean $\\mu\\big(x(\\alpha)\\big)$:\n$$\n\\mu\\big(x(\\alpha)\\big) = \\frac{1}{3}\\sum_{i=1}^{3} x_i(\\alpha) = \\frac{1}{3}\\sum_{i=1}^{3} (c + \\alpha v_i) = \\frac{1}{3} \\left( \\sum_{i=1}^{3} c + \\alpha \\sum_{i=1}^{3} v_i \\right)\n$$\nThe sum of the components of the vector $v = \\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}$ is $\\sum_{i=1}^{3} v_i = 1+0+(-1) = 0$. The sum $\\sum_{i=1}^{3} c = 3c$.\nTherefore, the mean is:\n$$\n\\mu\\big(x(\\alpha)\\big) = \\frac{1}{3} (3c + \\alpha \\cdot 0) = c\n$$\nThis shows that the mean is constant along the defined path and equal to $c$.\n\nNext, we compute the variance $\\sigma^2\\big(x(\\alpha)\\big)$:\n$$\n\\sigma^2\\big(x(\\alpha)\\big) = \\frac{1}{3}\\sum_{i=1}^{3} \\big(x_i(\\alpha) - \\mu(x(\\alpha))\\big)^2\n$$\nUsing the results above, the term inside the square is $x_i(\\alpha) - \\mu(x(\\alpha)) = (c + \\alpha v_i) - c = \\alpha v_i$.\nSubstituting this into the variance definition:\n$$\n\\sigma^2\\big(x(\\alpha)\\big) = \\frac{1}{3}\\sum_{i=1}^{3} (\\alpha v_i)^2 = \\frac{1}{3}\\sum_{i=1}^{3} \\alpha^2 v_i^2 = \\alpha^2 \\left( \\frac{1}{3}\\sum_{i=1}^{3} v_i^2 \\right)\n$$\nBy the problem's definition, $s_v^2 = \\frac{1}{3}\\sum_{i=1}^{3} v_i^2$. This gives the final result:\n$$\n\\sigma^2\\big(x(\\alpha)\\big) = \\alpha^2 s_v^2\n$$\n\n### Task 2: Gradient Derivation\nThe loss is $L(x) = y_1(x)$. The parameters are $\\gamma=g$ and $\\beta=0$. We have:\n$$\nL(x) = y_1(x) = g \\frac{x_1 - \\mu(x)}{\\sqrt{\\sigma^2(x) + \\epsilon}}\n$$\nWe need to find the gradient $\\nabla_x L(x) = \\begin{pmatrix} \\frac{\\partial L}{\\partial x_1} & \\frac{\\partial L}{\\partial x_2} & \\frac{\\partial L}{\\partial x_3} \\end{pmatrix}$. Let's find the partial derivative $\\frac{\\partial L}{\\partial x_k}$ for an arbitrary component $k \\in \\{1, 2, 3\\}$.\n\nLet's introduce intermediate variables for clarity: $n_i(x) = x_i - \\mu(x)$ and $s(x) = \\sqrt{\\sigma^2(x) + \\epsilon}$. The loss is $L(x) = g \\, \\frac{n_1(x)}{s(x)}$.\nUsing the quotient rule, the gradient is:\n$$\n\\nabla_x L(x) = g \\left( \\frac{\\nabla_x n_1(x)}{s(x)} - \\frac{n_1(x)}{s(x)^2} \\nabla_x s(x) \\right)\n$$\nWe need the gradients of $n_1(x)$ and $s(x)$. The dimension is $d=3$.\nThe gradient of the mean $\\mu(x) = \\frac{1}{3}\\sum_j x_j$ is $\\nabla_x \\mu(x) = \\frac{1}{3}\\mathbf{1}^T = \\begin{pmatrix} \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix}$.\nThe gradient of $n_1(x) = x_1 - \\mu(x)$ is $\\nabla_x n_1(x) = \\nabla_x x_1 - \\nabla_x \\mu(x) = e_1^T - \\frac{1}{3}\\mathbf{1}^T$, where $e_1^T = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}$. So, $\\nabla_x n_1(x) = \\begin{pmatrix} \\frac{2}{3} & -\\frac{1}{3} & -\\frac{1}{3} \\end{pmatrix}$.\n\nNext, for $\\nabla_x s(x)$:\n$$\n\\nabla_x s(x) = \\nabla_x \\left( \\sigma^2(x) + \\epsilon \\right)^{1/2} = \\frac{1}{2s(x)} \\nabla_x \\sigma^2(x)\n$$\nThe gradient of the variance $\\sigma^2(x) = \\frac{1}{3}\\sum_j (x_j-\\mu)^2$ is:\n$$\n\\frac{\\partial \\sigma^2}{\\partial x_k} = \\frac{1}{3}\\sum_j 2(x_j-\\mu)\\left(\\frac{\\partial x_j}{\\partial x_k} - \\frac{\\partial \\mu}{\\partial x_k}\\right) = \\frac{2}{3}\\sum_j(x_j-\\mu)(\\delta_{jk} - \\frac{1}{3}) = \\frac{2}{3}\\left((x_k-\\mu) - \\frac{1}{3}\\sum_j(x_j-\\mu)\\right)\n$$\nSince $\\sum_j(x_j-\\mu) = 0$, we have $\\frac{\\partial \\sigma^2}{\\partial x_k} = \\frac{2}{3}(x_k-\\mu) = \\frac{2}{3}n_k(x)$.\nIn vector form, $\\nabla_x \\sigma^2(x) = \\frac{2}{3}n(x)$, where $n(x)$ is the vector with components $n_k(x)$.\nSo, $\\nabla_x s(x) = \\frac{1}{2s(x)} \\frac{2}{3}n(x) = \\frac{n(x)}{3s(x)}$.\n\nSubstituting these gradients back into the expression for $\\nabla_x L(x)$:\n$$\n\\nabla_x L(x) = g \\left( \\frac{e_1^T - \\frac{1}{3}\\mathbf{1}^T}{s(x)} - \\frac{n_1(x)}{s(x)^2} \\frac{n(x)}{3s(x)} \\right) = \\frac{g}{s(x)}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g n_1(x)}{3s(x)^3}n(x)\n$$\nThis is the general gradient expression. Now we evaluate it at $x=x(\\alpha)$:\n-   $n(x(\\alpha)) = x(\\alpha) - \\mu(x(\\alpha))\\mathbf{1} = (c\\mathbf{1} + \\alpha v) - c\\mathbf{1} = \\alpha v$.\n-   $n_1(x(\\alpha)) = \\alpha v_1$.\n-   $s(x(\\alpha)) = \\sqrt{\\sigma^2(x(\\alpha)) + \\epsilon} = \\sqrt{\\alpha^2 s_v^2 + \\epsilon}$.\n\nSubstituting these into the gradient formula:\n$$\n\\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\alpha^2 s_v^2 + \\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g (\\alpha v_1)}{3(\\alpha^2 s_v^2 + \\epsilon)^{3/2}}(\\alpha v)\n$$\n$$\n\\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\alpha^2 s_v^2 + \\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g \\alpha^2 v_1}{3(\\alpha^2 s_v^2 + \\epsilon)^{3/2}}v\n$$\n\n### Task 3: Limiting Gradient as $\\alpha \\to 0^+$\nWe now take the limit of the derived gradient as $\\alpha \\to 0^+$:\n$$\n\\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) = \\lim_{\\alpha \\to 0^+} \\left( \\frac{g}{\\sqrt{\\alpha^2 s_v^2 + \\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g \\alpha^2 v_1}{3(\\alpha^2 s_v^2 + \\epsilon)^{3/2}}v \\right)\n$$\nWe evaluate the limit of each term separately.\nFor the first term, as $\\alpha \\to 0^+$, the denominator becomes $\\sqrt{0 + \\epsilon} = \\sqrt{\\epsilon}$.\n$$\n\\lim_{\\alpha \\to 0^+} \\frac{g}{\\sqrt{\\alpha^2 s_v^2 + \\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) = \\frac{g}{\\sqrt{\\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right)\n$$\nFor the second term, the numerator contains a factor of $\\alpha^2$, which goes to $0$. The denominator goes to $3\\epsilon^{3/2}$, which is a non-zero constant since $\\epsilon > 0$.\n$$\n\\lim_{\\alpha \\to 0^+} \\frac{g \\alpha^2 v_1}{3(\\alpha^2 s_v^2 + \\epsilon)^{3/2}}v = \\frac{g \\cdot 0 \\cdot v_1}{3\\epsilon^{3/2}}v = \\mathbf{0}\n$$\nCombining the limits, we get:\n$$\n\\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right)\n$$\nSubstituting the vectors $e_1^T = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}$ and $\\mathbf{1}^T = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}$:\n$$\n\\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\epsilon}}\\left(\\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} - \\frac{1}{3}\\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}\\right) = \\frac{g}{\\sqrt{\\epsilon}}\\begin{pmatrix} 1-\\frac{1}{3} & 0-\\frac{1}{3} & 0-\\frac{1}{3} \\end{pmatrix}\n$$\n$$\n\\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\epsilon}}\\begin{pmatrix} \\frac{2}{3} & -\\frac{1}{3} & -\\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{2g}{3\\sqrt{\\epsilon}} & -\\frac{g}{3\\sqrt{\\epsilon}} & -\\frac{g}{3\\sqrt{\\epsilon}} \\end{pmatrix}\n$$\nThis is the limiting gradient vector.\n\n### Task 4: Role of $\\epsilon > 0$\nTo understand the role of $\\epsilon$, we compare the behavior of the gradient magnitude as $\\alpha \\to 0^+$ with and without $\\epsilon$.\n\n**Case 1: $\\epsilon > 0$ (as derived above)**\nThe gradient $\\nabla_x L\\big(x(\\alpha)\\big)$ approaches a finite constant vector as $\\alpha \\to 0^+$. The magnitude of the limiting gradient is:\n$$\n\\left\\| \\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) \\right\\| = \\left\\| \\frac{g}{\\sqrt{\\epsilon}}\\begin{pmatrix} \\frac{2}{3} & -\\frac{1}{3} & -\\frac{1}{3} \\end{pmatrix} \\right\\| = \\frac{g}{\\sqrt{\\epsilon}} \\sqrt{(\\frac{2}{3})^2 + (-\\frac{1}{3})^2 + (-\\frac{1}{3})^2} = \\frac{g}{\\sqrt{\\epsilon}}\\sqrt{\\frac{4+1+1}{9}} = \\frac{g\\sqrt{6}}{3\\sqrt{\\epsilon}}\n$$\nThis is a finite, non-zero value. The presence of $\\epsilon>0$ ensures that the denominator in the LN formula, $\\sqrt{\\sigma^2(x)+\\epsilon}$, is bounded below by $\\sqrt{\\epsilon}$, thus preventing the gradient from blowing up.\n\n**Case 2: Hypothetical $\\epsilon = 0$**\nIf we were to set $\\epsilon=0$, the gradient expression becomes:\n$$\n\\nabla_x L\\big(x(\\alpha)\\big) \\Big|_{\\epsilon=0} = \\frac{g}{\\sqrt{\\alpha^2 s_v^2}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g \\alpha^2 v_1}{3(\\alpha^2 s_v^2)^{3/2}}v\n$$\nSince $\\alpha > 0$, $\\sqrt{\\alpha^2 s_v^2} = \\alpha s_v$.\n$$\n\\nabla_x L\\big(x(\\alpha)\\big) \\Big|_{\\epsilon=0} = \\frac{g}{\\alpha s_v}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g \\alpha^2 v_1}{3\\alpha^3 s_v^3}v = \\frac{1}{\\alpha}\\left[ \\frac{g}{s_v}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g v_1}{3 s_v^3}v \\right]\n$$\nThe term in the square brackets is a constant vector that is generally non-zero. The gradient vector's magnitude thus scales with $1/\\alpha$. As $\\alpha \\to 0^+$, the variance $\\sigma^2 = \\alpha^2 s_v^2 \\to 0$, and the magnitude of the gradient diverges:\n$$\n\\left\\| \\nabla_x L\\big(x(\\alpha)\\big) \\Big|_{\\epsilon=0} \\right\\| \\to \\infty\n$$\nThis \"blow-up\" of the gradient corresponds to numerical instability in optimization algorithms. The stabilizing constant $\\epsilon > 0$ is crucial because it ensures that even for inputs with near-zero variance, the normalization denominator remains bounded away from zero, leading to well-behaved, finite gradients.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2g}{3\\sqrt{\\epsilon}} & -\\frac{g}{3\\sqrt{\\epsilon}} & -\\frac{g}{3\\sqrt{\\epsilon}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Real-world data is rarely perfect; it often includes missing values or requires padding to fit into fixed-size tensors, a common practice in Natural Language Processing. Layer Normalization must be adapted to handle such cases by ignoring these invalid entries. This problem challenges you to derive a \"masked\" version of Layer Normalization that operates only on valid features . Through derivation, a concrete numerical example, and stability analysis, you will learn how to correctly implement and reason about normalization in the presence of incomplete data.",
            "id": "3142018",
            "problem": "A single input sample in a deep neural network has feature vector $x \\in \\mathbb{R}^{d}$ together with a binary validity mask $m \\in \\{0,1\\}^{d}$, where $m_{i} = 1$ indicates that feature $x_{i}$ is valid and $m_{i} = 0$ indicates it is invalid or missing. Layer Normalization (LN) is applied per sample across features. Begin from the fundamental definitions of the arithmetic mean and the variance over a finite set of real numbers, together with the population-variance convention that divides by the number of elements being averaged. \n\nTask 1: Derive the expressions for the masked mean $\\mu_{m}$ and masked variance $\\sigma_{m}^{2}$ that use only the valid features indicated by $m$. Your derivation must start from the definitions of arithmetic mean and variance, and must be consistent with averaging over the subset $\\{i : m_{i} = 1\\}$.\n\nTask 2: Consider a masked Layer Normalization that uses the masked statistics from Task 1 and applies normalization only to valid features by performing the following sequence on each valid index $i$: subtract the masked mean, divide by the square root of the masked variance plus a strictly positive smoothing parameter $\\epsilon$, and then apply a scalar affine transform with parameters $\\gamma$ and $\\beta$. Invalid features are left unchanged. For the specific instance $d = 4$ with $x = (2, -1, 3, 0)$, $m = (1, 0, 1, 0)$, $\\gamma = 1$, $\\beta = 0$, and $\\epsilon = 1 \\times 10^{-5}$, compute the value of the normalized output at index $i = 1$. Report your numerical answer rounded to four significant figures.\n\nTask 3: Using your expressions from Task 1, analyze the numerical stability of masked Layer Normalization when $\\sum_{i=1}^{d} m_{i}$ is small. Your analysis should discuss how the estimator of the variance behaves, the role of $\\epsilon$, and the potential magnitude of the normalization factor. No numerical answer is required for Task 3.",
            "solution": "We begin with the fundamental definitions. For a finite multiset of real numbers $\\{z_{1}, z_{2}, \\dots, z_{k}\\}$, the arithmetic mean is given by\n$$\n\\bar{z} \\;=\\; \\frac{1}{k} \\sum_{j=1}^{k} z_{j},\n$$\nand the population variance (the second central moment with division by $k$) is\n$$\n\\operatorname{Var}(z) \\;=\\; \\frac{1}{k} \\sum_{j=1}^{k} (z_{j} - \\bar{z})^{2}.\n$$\nIn our masked setting, let $K = \\sum_{i=1}^{d} m_{i}$ denote the number of valid features and let the valid index set be $S = \\{i \\in \\{1,\\dots,d\\} : m_{i} = 1\\}$. The masked mean aggregates only the valid entries. Using the mean definition on the set $\\{x_{i} : i \\in S\\}$, we obtain\n$$\n\\mu_{m} \\;=\\; \\frac{1}{K} \\sum_{i \\in S} x_{i}.\n$$\nEquivalently, since $m_{i} \\in \\{0,1\\}$ selects valid features,\n$$\n\\mu_{m} \\;=\\; \\frac{\\sum_{i=1}^{d} m_{i} x_{i}}{\\sum_{i=1}^{d} m_{i}}.\n$$\nNext, the masked variance follows directly from the population variance definition applied to the same valid set:\n$$\n\\sigma_{m}^{2} \\;=\\; \\frac{1}{K} \\sum_{i \\in S} (x_{i} - \\mu_{m})^{2} \\;=\\; \\frac{\\sum_{i=1}^{d} m_{i} (x_{i} - \\mu_{m})^{2}}{\\sum_{i=1}^{d} m_{i}}.\n$$\nThis completes Task 1.\n\nFor Task 2, masked Layer Normalization applies the normalization only at indices $i$ with $m_{i} = 1$ by performing the standardized transform with smoothing and affine parameters. Concretely, for valid $i$,\n$$\ny_{i} \\;=\\; \\gamma \\,\\frac{x_{i} - \\mu_{m}}{\\sqrt{\\sigma_{m}^{2} + \\epsilon}} \\;+\\; \\beta,\n$$\nand for invalid $i$ (where $m_{i} = 0$), we set $y_{i} = x_{i}$. With $x = (2, -1, 3, 0)$, $m = (1, 0, 1, 0)$, we have $K = \\sum_{i=1}^{4} m_{i} = 2$. The masked mean is\n$$\n\\mu_{m} \\;=\\; \\frac{m_{1} x_{1} + m_{2} x_{2} + m_{3} x_{3} + m_{4} x_{4}}{m_{1} + m_{2} + m_{3} + m_{4}} \\;=\\; \\frac{1 \\cdot 2 + 0 \\cdot (-1) + 1 \\cdot 3 + 0 \\cdot 0}{2} \\;=\\; \\frac{5}{2} \\;=\\; 2.5.\n$$\nThe masked variance is\n$$\n\\sigma_{m}^{2} \\;=\\; \\frac{m_{1} (x_{1} - \\mu_{m})^{2} + m_{2} (x_{2} - \\mu_{m})^{2} + m_{3} (x_{3} - \\mu_{m})^{2} + m_{4} (x_{4} - \\mu_{m})^{2}}{K}\n\\;=\\; \\frac{(2 - 2.5)^{2} + (3 - 2.5)^{2}}{2}\n\\;=\\; \\frac{0.25 + 0.25}{2}\n\\;=\\; 0.25.\n$$\nWith $\\gamma = 1$, $\\beta = 0$, and $\\epsilon = 1 \\times 10^{-5}$, the denominator is\n$$\n\\sqrt{\\sigma_{m}^{2} + \\epsilon} \\;=\\; \\sqrt{0.25 + 1 \\times 10^{-5}} \\;=\\; \\sqrt{0.25001}.\n$$\nTherefore,\n$$\ny_{1} \\;=\\; 1 \\cdot \\frac{2 - 2.5}{\\sqrt{0.25001}} + 0 \\;=\\; \\frac{-0.5}{\\sqrt{0.25001}}.\n$$\nTo obtain a numerical value, note that $\\sqrt{0.25001}$ is slightly larger than $0.5$. Using the identity $\\sqrt{0.25 + a} = 0.5 \\sqrt{1 + 4a}$, for $a = 10^{-5}$ we have $\\sqrt{0.25001} = 0.5 \\sqrt{1 + 4 \\times 10^{-5}}$. A first-order expansion $\\sqrt{1 + \\delta} \\approx 1 + \\frac{\\delta}{2}$ for small $\\delta$ with $\\delta = 4 \\times 10^{-5}$ gives\n$$\n\\sqrt{0.25001} \\;\\approx\\; 0.5 \\left(1 + 2 \\times 10^{-5}\\right) \\;=\\; 0.50001.\n$$\nThus,\n$$\ny_{1} \\;\\approx\\; \\frac{-0.5}{0.50001} \\;=\\; -\\frac{0.5}{0.50001} \\;=\\; -\\frac{1}{1 + 2 \\times 10^{-5}} \\;\\approx\\; -(1 - 2 \\times 10^{-5}) \\;=\\; -0.99998.\n$$\nRounded to four significant figures, this is $-1.000$.\n\nFor Task 3, we analyze stability when $K = \\sum_{i=1}^{d} m_{i}$ is small. The masked variance\n$$\n\\sigma_{m}^{2} \\;=\\; \\frac{1}{K} \\sum_{i \\in S} (x_{i} - \\mu_{m})^{2}\n$$\nis an estimator computed over $K$ samples. When $K$ is small, this estimator exhibits high variance (in the statistical sense), making the denominator $\\sqrt{\\sigma_{m}^{2} + \\epsilon}$ potentially very small if the valid values are nearly equal, or highly variable across samples. Without the smoothing term $\\epsilon > 0$, the denominator could approach zero, producing extremely large normalized values and unstable gradients. The smoothing $\\epsilon$ lower-bounds the denominator by $\\sqrt{\\epsilon}$, yielding the magnitude bound\n$$\n\\left|\\frac{x_{i} - \\mu_{m}}{\\sqrt{\\sigma_{m}^{2} + \\epsilon}}\\right| \\;\\leq\\; \\frac{|x_{i} - \\mu_{m}|}{\\sqrt{\\epsilon}},\n$$\nso choosing $\\epsilon$ appropriately mitigates blow-up. However, small $K$ also makes $\\mu_{m}$ itself sensitive to individual features, which can amplify fluctuations of $x_{i} - \\mu_{m}$. In the extreme case $K = 1$, we have $\\mu_{m} = x_{i^{\\star}}$ for the lone valid index $i^{\\star}$, yielding $\\sigma_{m}^{2} = 0$ and normalized deviation $x_{i^{\\star}} - \\mu_{m} = 0$, so $y_{i^{\\star}} = \\beta$ is benign; instability arises more when $K$ is small but at least $2$, and the valid values are nearly identical or the variance estimate is noisy. Practical stability therefore hinges on the interplay between $K$, the dispersion of $\\{x_{i} : i \\in S\\}$, and the choice of $\\epsilon$ and $\\gamma$; larger $\\epsilon$ and moderating $\\gamma$ reduce sensitivity when $K$ is small.",
            "answer": "$$\\boxed{-1.000}$$"
        },
        {
            "introduction": "Moving from the mechanics to the principles of design, we ask: is every component of the Layer Normalization formula essential? This exercise explores this question by comparing standard Layer Normalization with a popular and efficient alternative, Root Mean Square Normalization (RMSNorm), which omits the mean-subtraction step. By analyzing how gradients flow differently in each case, you will discover the fundamental trade-offs of the centering operation and develop an intuition for when it helps, and when it might hinder, the optimization process .",
            "id": "3142059",
            "problem": "Consider a single training example with a feature vector $\\mathbf{x} \\in \\mathbb{R}^d$ that is passed through a normalization layer before a linear map with weight vector $\\mathbf{w} \\in \\mathbb{R}^d$ and a scalar target $t \\in \\mathbb{R}$. The loss is $L = \\tfrac{1}{2}(\\mathbf{w}^\\top \\mathbf{z} - t)^2$, where $\\mathbf{z}$ is the normalized output. The upstream gradient with respect to the normalized output is $\\mathbf{g} = \\partial L / \\partial \\mathbf{z} = (\\mathbf{w}^\\top \\mathbf{z} - t)\\,\\mathbf{w}$. Assume the coordinates of $\\mathbf{x}$ are right-skewed with sample mean $\\mu > 0$, sample standard deviation $\\sigma > 0$, and skewness coefficient $\\gamma_1 > 0$. Two alternative normalizations are used in practice: one that subtracts the sample mean and scales by the sample standard deviation (Layer Normalization (LN)), and one that scales by the root mean square without subtracting the mean (Root Mean Square Normalization (RMSNorm)). Using only the definitions of sample mean, variance, and root mean square; the constant vector $\\mathbf{1} \\in \\mathbb{R}^d$; and the multivariate chain rule for gradients, derive the qualitative structure of $\\partial L / \\partial \\mathbf{x}$ under each normalization by decomposing directions into the span of $\\mathbf{1}$ and its orthogonal complement. Based on this structure, determine in which of the following scenarios removing the mean (centering) improves or harms optimization during gradient descent for skewed feature distributions.\n\nA. When $|\\mu| \\gg \\sigma$ and the upstream gradient has negligible mean, i.e., $\\mathbf{1}^\\top \\mathbf{g} \\approx 0$, centering improves conditioning by eliminating a dominant shift and equalizing scales across coordinates.\n\nB. When the upstream gradient is approximately collinear with $\\mathbf{1}$, i.e., $\\mathbf{g} \\approx \\alpha\\,\\mathbf{1}$ for some $\\alpha \\in \\mathbb{R}$, and $\\mu \\neq 0$, centering suppresses the gradient component needed to fit uniform weights or biases, thereby slowing optimization.\n\nC. For right-skewed features with $\\gamma_1 > 0$ and $|\\mu| \\gg \\sigma$, the scaling-only normalization is invariant to additive shifts $\\mathbf{x} \\mapsto \\mathbf{x} + c\\,\\mathbf{1}$, making it preferable to centering whenever skewness induces positive shifts.\n\nD. When $\\mu \\approx 0$ but $\\gamma_1$ is large, centering strictly harms optimization because it discards third-moment information that gradients would otherwise exploit.\n\nSelect all correct statements.",
            "solution": "The problem asks for an analysis of the effect of centering in normalization layers, specifically comparing Layer Normalization (LN) to Root Mean Square Normalization (RMSNorm), on gradient-based optimization for skewed feature distributions. To do this, we must derive the gradient of the loss $L$ with respect to the input feature vector $\\mathbf{x}$, denoted $\\frac{\\partial L}{\\partial \\mathbf{x}}$, for each normalization scheme.\n\nThe problem provides the following:\n- Input feature vector: $\\mathbf{x} \\in \\mathbb{R}^d$\n- Weight vector: $\\mathbf{w} \\in \\mathbb{R}^d$\n- Scalar target: $t \\in \\mathbb{R}$\n- Normalized vector: $\\mathbf{z}$\n- Loss function: $L = \\frac{1}{2}(\\mathbf{w}^\\top \\mathbf{z} - t)^2$\n- Upstream gradient: $\\mathbf{g} = \\frac{\\partial L}{\\partial \\mathbf{z}} = (\\mathbf{w}^\\top \\mathbf{z} - t)\\mathbf{w}$\n\nThe gradient of the loss with respect to the input $\\mathbf{x}$ is given by the multivariate chain rule:\n$$ \\frac{\\partial L}{\\partial \\mathbf{x}} = \\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right)^\\top \\frac{\\partial L}{\\partial \\mathbf{z}} = \\mathbf{J}_{\\mathbf{z}}^\\top \\mathbf{g} $$\nwhere $\\mathbf{J}_{\\mathbf{z}} = \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}$ is the Jacobian matrix of the normalization function $\\mathbf{z}(\\mathbf{x})$.\n\nWe will derive the Jacobian for both Layer Normalization and RMSNorm. Let $\\mathbf{1} \\in \\mathbb{R}^d$ be the vector of all ones. The sample mean of $\\mathbf{x}$ is $\\mu = \\frac{1}{d} \\mathbf{1}^\\top \\mathbf{x}$. The sample variance is $\\sigma^2 = \\frac{1}{d} \\|\\mathbf{x} - \\mu\\mathbf{1}\\|^2$. The root mean square is $s = \\sqrt{\\frac{1}{d} \\|\\mathbf{x}\\|^2}$.\n\n**1. Layer Normalization (LN)**\n\nThe LN transformation is $\\mathbf{z}_{\\text{LN}} = \\frac{\\mathbf{x} - \\mu\\mathbf{1}}{\\sigma}$. We must find its Jacobian, $\\mathbf{J}_{\\text{LN}}$. A component-wise derivation yields:\n$$ (\\mathbf{J}_{\\text{LN}})_{jk} = \\frac{\\partial z_j}{\\partial x_k} = \\frac{1}{\\sigma}\\left(\\delta_{jk} - \\frac{1}{d}\\right) - \\frac{z_j}{d} z_k $$\nIn matrix form, this is:\n$$ \\mathbf{J}_{\\text{LN}} = \\frac{1}{\\sigma} \\left[ \\left(\\mathbf{I} - \\frac{1}{d}\\mathbf{1}\\mathbf{1}^\\top\\right) - \\frac{1}{d}\\mathbf{z}_{\\text{LN}}\\mathbf{z}_{\\text{LN}}^\\top \\right] $$\nwhere $\\mathbf{I}$ is the identity matrix and $\\mathbf{P}_{\\mathbf{1}^\\perp} = \\mathbf{I} - \\frac{1}{d}\\mathbf{1}\\mathbf{1}^\\top$ is the projection matrix onto the subspace orthogonal to $\\mathbf{1}$. The Jacobian matrix is symmetric. The gradient is:\n$$ \\frac{\\partial L}{\\partial \\mathbf{x}}\\bigg|_{\\text{LN}} = \\mathbf{J}_{\\text{LN}}^\\top \\mathbf{g} = \\mathbf{J}_{\\text{LN}} \\mathbf{g} = \\frac{1}{\\sigma} \\left[ \\left(\\mathbf{I} - \\frac{1}{d}\\mathbf{1}\\mathbf{1}^\\top\\right)\\mathbf{g} - \\frac{1}{d}(\\mathbf{z}_{\\text{LN}}^\\top \\mathbf{g})\\mathbf{z}_{\\text{LN}} \\right] $$\nA crucial property of this gradient is its projection onto the $\\mathbf{1}$ vector:\n$$ \\mathbf{1}^\\top \\frac{\\partial L}{\\partial \\mathbf{x}}\\bigg|_{\\text{LN}} = \\frac{1}{\\sigma} \\left[ (\\mathbf{1}^\\top\\mathbf{I} - \\frac{1}{d}\\mathbf{1}^\\top\\mathbf{1}\\mathbf{1}^\\top)\\mathbf{g} - \\frac{1}{d}(\\mathbf{z}_{\\text{LN}}^\\top \\mathbf{g})(\\mathbf{1}^\\top\\mathbf{z}_{\\text{LN}}) \\right] $$\nSince $\\mathbf{1}^\\top \\mathbf{1} = d$, the first term inside the bracket becomes $(\\mathbf{1}^\\top - \\mathbf{1}^\\top)\\mathbf{g} = \\mathbf{0}^\\top\\mathbf{g} = 0$. For the second term, we note that the output of LN is centered: $\\mathbf{1}^\\top\\mathbf{z}_{\\text{LN}} = \\frac{1}{\\sigma}(\\mathbf{1}^\\top\\mathbf{x} - \\mu\\mathbf{1}^\\top\\mathbf{1}) = \\frac{1}{\\sigma}(d\\mu - \\mu d) = 0$. Thus, the second term is also zero.\nThis means $\\mathbf{1}^\\top \\frac{\\partial L}{\\partial \\mathbf{x}}\\big|_{\\text{LN}} = 0$. The gradient vector for LN is always orthogonal to the constant vector $\\mathbf{1}$. Consequently, a gradient descent update, $\\Delta\\mathbf{x} = -\\eta \\frac{\\partial L}{\\partial \\mathbf{x}}\\big|_{\\text{LN}}$, will not change the mean of $\\mathbf{x}$: $\\mu_{\\text{new}} = \\frac{1}{d} \\mathbf{1}^\\top (\\mathbf{x} + \\Delta\\mathbf{x}) = \\mu_{\\text{old}} + 0 = \\mu_{\\text{old}}$. Layer Normalization prevents the optimization from adjusting the mean of the features.\n\n**2. Root Mean Square Normalization (RMSNorm)**\n\nThe RMSNorm transformation is $\\mathbf{z}_{\\text{RMS}} = \\frac{\\mathbf{x}}{s}$, where $s^2 = \\frac{1}{d}\\mathbf{x}^\\top\\mathbf{x}$. The Jacobian, $\\mathbf{J}_{\\text{RMS}}$, is:\n$$ (\\mathbf{J}_{\\text{RMS}})_{jk} = \\frac{\\partial z_j}{\\partial x_k} = \\frac{1}{s}\\left(\\delta_{jk} - \\frac{1}{d}z_j z_k\\right) $$\nIn matrix form:\n$$ \\mathbf{J}_{\\text{RMS}} = \\frac{1}{s}\\left[\\mathbf{I} - \\frac{1}{d}\\mathbf{z}_{\\text{RMS}}\\mathbf{z}_{\\text{RMS}}^\\top\\right] $$\nThis Jacobian is also symmetric. The gradient is:\n$$ \\frac{\\partial L}{\\partial \\mathbf{x}}\\bigg|_{\\text{RMS}} = \\mathbf{J}_{\\text{RMS}}^\\top \\mathbf{g} = \\mathbf{J}_{\\text{RMS}} \\mathbf{g} = \\frac{1}{s}\\left[\\mathbf{g} - \\frac{1}{d}(\\mathbf{z}_{\\text{RMS}}^\\top \\mathbf{g})\\mathbf{z}_{\\text{RMS}}\\right] $$\nUnlike LN, this gradient generally has a non-zero component along $\\mathbf{1}$:\n$$ \\mathbf{1}^\\top \\frac{\\partial L}{\\partial \\mathbf{x}}\\bigg|_{\\text{RMS}} = \\frac{1}{s}\\left[\\mathbf{1}^\\top\\mathbf{g} - \\frac{1}{d}(\\mathbf{z}_{\\text{RMS}}^\\top\\mathbf{g})(\\mathbf{1}^\\top\\mathbf{z}_{\\text{RMS}})\\right] $$\nSince $\\mathbf{z}_{\\text{RMS}}$ is not centered, $\\mathbf{1}^\\top\\mathbf{z}_{\\text{RMS}} = \\frac{1}{s}\\mathbf{1}^\\top\\mathbf{x} = \\frac{d\\mu}{s} \\neq 0$ (for $\\mu \\neq 0$). Thus, the mean of $\\mathbf{x}$ can be updated during optimization.\n\nNow we evaluate the given options.\n\n**A. When $|\\mu| \\gg \\sigma$ and the upstream gradient has negligible mean, i.e., $\\mathbf{1}^\\top \\mathbf{g} \\approx 0$, centering improves conditioning by eliminating a dominant shift and equalizing scales across coordinates.**\nThis scenario describes an input vector $\\mathbf{x}$ dominated by a large common mode $\\mu$, while the loss is sensitive to variations around this mode (since $\\mathbf{1}^\\top \\mathbf{g} \\approx 0$, the loss is insensitive to a uniform change in $\\mathbf{z}$).\n- With RMSNorm, $\\mathbf{z}_{\\text{RMS}} = \\mathbf{x}/s$. Since $|\\mu| \\gg \\sigma$, we have $s^2 = \\mu^2 + \\sigma^2 \\approx \\mu^2$, so $s \\approx |\\mu|$. The variations in $\\mathbf{x}$ (of order $\\sigma$) are scaled down by $s \\approx |\\mu|$, becoming $\\sigma/|\\mu| \\ll 1$. The gradient is approximately $\\frac{\\partial L}{\\partial \\mathbf{x}}|_{\\text{RMS}} \\approx \\frac{1}{s}\\mathbf{g} \\approx \\frac{1}{|\\mu|}\\mathbf{g}$. The gradient magnitude is small.\n- With LN, the dominant shift $\\mu\\mathbf{1}$ is removed first: $\\mathbf{x} - \\mu\\mathbf{1}$. The remaining vector has components of order $\\sigma$. This is then scaled by $1/\\sigma$, so the components of $\\mathbf{z}_{\\text{LN}}$ are of order $1$. The gradient is approximately $\\frac{\\partial L}{\\partial \\mathbf{x}}|_{\\text{LN}} \\approx \\frac{1}{\\sigma}[\\mathbf{g} - \\text{projection}]$. The gradient magnitude is scaled by $1/\\sigma$.\nSince $|\\mu| \\gg \\sigma$, the LN gradient is much larger than the RMSNorm gradient, leading to faster learning of the feature variations. By removing the large, task-irrelevant mean and re-scaling the relevant variations, LN improves the problem's conditioning. The statement accurately describes a key benefit of centering.\n**Verdict: Correct.**\n\n**B. When the upstream gradient is approximately collinear with $\\mathbf{1}$, i.e., $\\mathbf{g} \\approx \\alpha\\,\\mathbf{1}$ for some $\\alpha \\in \\mathbb{R}$, and $\\mu \\neq 0$, centering suppresses the gradient component needed to fit uniform weights or biases, thereby slowing optimization.**\nThis scenario describes a case where the loss is only sensitive to the average value of the normalized activations. This is typical when learning a bias-like component. Let's analyze the LN gradient with $\\mathbf{g} = \\alpha\\mathbf{1}$.\n$$ \\frac{\\partial L}{\\partial \\mathbf{x}}\\bigg|_{\\text{LN}} = \\frac{1}{\\sigma} \\left[ \\left(\\mathbf{I} - \\frac{1}{d}\\mathbf{1}\\mathbf{1}^\\top\\right)(\\alpha\\mathbf{1}) - \\frac{1}{d}(\\mathbf{z}_{\\text{LN}}^\\top (\\alpha\\mathbf{1}))\\mathbf{z}_{\\text{LN}} \\right] $$\nThe first term is $\\alpha(\\mathbf{1} - \\frac{d}{d}\\mathbf{1}) = \\mathbf{0}$. The second term contains $\\mathbf{z}_{\\text{LN}}^\\top\\mathbf{1} = 0$. Thus, $\\frac{\\partial L}{\\partial \\mathbf{x}}|_{\\text{LN}} = \\mathbf{0}$. Layer Normalization completely blocks the gradient signal. In contrast, the RMSNorm gradient, $\\frac{\\partial L}{\\partial \\mathbf{x}}|_{\\text{RMS}}$, would be non-zero, allowing the model to learn. Therefore, centering actively harms (by stopping) optimization in this specific case.\n**Verdict: Correct.**\n\n**C. For right-skewed features with $\\gamma_1 > 0$ and $|\\mu| \\gg \\sigma$, the scaling-only normalization is invariant to additive shifts $\\mathbf{x} \\mapsto \\mathbf{x} + c\\,\\mathbf{1}$, making it preferable to centering whenever skewness induces positive shifts.**\nThis statement claims that RMSNorm (\"scaling-only\") is invariant to an additive shift $\\mathbf{x}' = \\mathbf{x} + c\\mathbf{1}$. Let's test this. The new RMS is $s' = \\sqrt{\\frac{1}{d}\\|\\mathbf{x}+c\\mathbf{1}\\|^2} = \\sqrt{\\frac{1}{d}(\\|\\mathbf{x}\\|^2 + 2c\\mathbf{1}^\\top\\mathbf{x} + c^2\\|\\mathbf{1}\\|^2)} = \\sqrt{s^2 + 2c\\mu + c^2}$. In general, $s' \\neq s$. The new normalized vector is $\\mathbf{z}'_{\\text{RMS}} = \\frac{\\mathbf{x}+c\\mathbf{1}}{s'}$, which is not equal to the original $\\mathbf{z}_{\\text{RMS}} = \\mathbf{x}/s$. Thus, RMSNorm is not invariant to additive shifts. It is the output of Layer Normalization that is invariant to such shifts, as the centering step $\\mathbf{x} \\mapsto \\mathbf{x}-\\mu\\mathbf{1}$ cancels out any uniform shift. The premise of the statement is factually incorrect.\n**Verdict: Incorrect.**\n\n**D. When $\\mu \\approx 0$ but $\\gamma_1$ is large, centering strictly harms optimization because it discards third-moment information that gradients would otherwise exploit.**\nIf $\\mu = 0$, the sample variance $\\sigma^2 = \\frac{1}{d}\\sum(x_i-0)^2 = \\frac{1}{d}\\sum x_i^2$ becomes identical to the squared RMS, $s^2$. Therefore, $\\sigma = s$.\nIn this case, LN and RMSNorm become identical transformations:\n- $\\mathbf{z}_{\\text{LN}} = \\frac{\\mathbf{x} - 0\\cdot\\mathbf{1}}{\\sigma} = \\mathbf{x}/\\sigma$\n- $\\mathbf{z}_{\\text{RMS}} = \\mathbf{x}/s$\nSince $\\sigma=s$, we have $\\mathbf{z}_{\\text{LN}} = \\mathbf{z}_{\\text{RMS}}$. If the normalization functions are identical, their gradients and the resulting optimization trajectories will also be identical. One cannot \"strictly harm\" optimization relative to the other. If $\\mu \\approx 0$, the two methods are nearly identical, and any difference in performance would be negligible, contrary to the strong claim \"strictly harms\". The justification about \"discards third-moment information\" is unsubstantiated; the normalization formulas themselves only depend on the first and second moments of the specific sample vector $\\mathbf{x}$, not the moments of the underlying distribution.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AB}$$"
        }
    ]
}