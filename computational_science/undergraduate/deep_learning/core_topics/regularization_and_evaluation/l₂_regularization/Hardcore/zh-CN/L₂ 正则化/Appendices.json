{
    "hands_on_practices": [
        {
            "introduction": "掌握任何一项技术的第一步都是理解其背后的数学原理。本练习将引导你通过推导，揭示神经网络中的权重衰减（weight decay）与经典统计学方法——岭回归（Ridge Regression）之间的深刻联系。通过解决这个练习 ，你不仅能掌握 L₂ 正则化的闭式解，还能从根本上理解它为何是解决线性模型问题的有效工具。",
            "id": "3169526",
            "problem": "考虑一个单层线性神经网络，其参数向量为 $w \\in \\mathbb{R}^{d}$，它通过 $f_{w}(x) = w^{\\top} x$ 将输入 $x \\in \\mathbb{R}^{d}$ 映射到一个输出。给定训练数据 $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$，它们被堆叠成一个设计矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和一个响应向量 $y \\in \\mathbb{R}^{n}$。训练目标是带有权重衰减惩罚（欧几里得范数（L2）正则化）的平方误差和，定义为\n$$\nJ(w) = \\|y - X w\\|_{2}^{2} + \\lambda \\|w\\|_{2}^{2},\n$$\n其中 $\\lambda > 0$ 是正则化强度。\n\n仅从 $J(w)$ 的定义以及微分和矩阵代数的基本法则出发，推导出最小化 $J(w)$ 的闭式解优化器 $w^{\\star}$，并解释为什么这个优化器建立了岭回归与训练带有权重衰减的单层线性神经网络之间的等价性。\n\n然后，对于具体的数据集\n$$\nX = \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  1\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{pmatrix},\n$$\n和正则化参数 $\\lambda = 1$，计算优化后的权重向量 $w^{\\star}$。将你的最终答案表示为一个行矩阵，其元素为精确的有理数（不要四舍五入）。",
            "solution": "该问题要求完成两个主要任务：首先，推导使用L2正则化（权重衰减）训练的单层线性神经网络权重的闭式解，并解释其与岭回归的等价性；其次，为给定的数据集计算具体的权重向量。\n\n该问题是有效的，因为它在科学上基于标准的机器学习理论，提法良好，信息充分且一致，并且表述客观。\n\n首先，我们推导最优权重向量 $w^{\\star}$。要最小化的目标函数由下式给出：\n$$\nJ(w) = \\|y - X w\\|_{2}^{2} + \\lambda \\|w\\|_{2}^{2}\n$$\n其中 $w \\in \\mathbb{R}^{d}$，$X \\in \\mathbb{R}^{n \\times d}$，$y \\in \\mathbb{R}^{n}$，且 $\\lambda > 0$。\n\n为了找到 $J(w)$ 的最小值，我们必须首先计算它关于 $w$ 的梯度，记为 $\\nabla_{w} J(w)$，并将其设为零。让我们展开 $J(w)$ 中的各项。欧几里得范数的平方 $\\|v\\|_{2}^{2}$ 等价于点积 $v^{\\top}v$。\n\n第一项是平方误差和：\n$$\n\\|y - X w\\|_{2}^{2} = (y - X w)^{\\top} (y - X w) = (y^{\\top} - w^{\\top} X^{\\top})(y - X w) = y^{\\top}y - y^{\\top}Xw - w^{\\top}X^{\\top}y + w^{\\top}X^{\\top}Xw\n$$\n由于 $y^{\\top}Xw$ 是一个标量，它等于其转置 $(y^{\\top}Xw)^{\\top} = w^{\\top}X^{\\top}y$。因此，我们可以合并交叉项：\n$$\n\\|y - X w\\|_{2}^{2} = y^{\\top}y - 2w^{\\top}X^{\\top}y + w^{\\top}X^{\\top}Xw\n$$\n\n第二项是权重衰减惩罚项：\n$$\n\\lambda \\|w\\|_{2}^{2} = \\lambda w^{\\top}w\n$$\n\n将这些结合起来，完整的目标函数是：\n$$\nJ(w) = y^{\\top}y - 2w^{\\top}X^{\\top}y + w^{\\top}X^{\\top}Xw + \\lambda w^{\\top}w\n$$\n\n现在我们对 $J(w)$ 关于向量 $w$ 求导。我们使用以下矩阵微积分的标准法则：\n1.  对于常数 $c$，$\\nabla_{w} (c) = 0$。\n2.  对于向量 $a$，$\\nabla_{w} (w^{\\top}a) = \\nabla_{w} (a^{\\top}w) = a$。\n3.  $\\nabla_{w} (w^{\\top}Mw) = (M + M^{\\top})w$。如果 $M$ 是对称的，则简化为 $2Mw$。\n\n将这些法则应用于 $J(w)$：\n- 项 $y^{\\top}y$ 关于 $w$ 是常数，所以其梯度为 $0$。\n- 对于项 $-2w^{\\top}X^{\\top}y$，向量 $X^{\\top}y$ 关于 $w$ 是常数。所以，$\\nabla_{w}(-2w^{\\top}X^{\\top}y) = -2X^{\\top}y$。\n- 对于项 $w^{\\top}X^{\\top}Xw$，矩阵 $M = X^{\\top}X$ 是对称的。因此，$\\nabla_{w}(w^{\\top}X^{\\top}Xw) = 2(X^{\\top}X)w$。\n- 对于项 $\\lambda w^{\\top}w$，我们可以将其写为 $\\lambda w^{\\top}Iw$，其中 $I$ 是单位矩阵。因为 $I$ 是对称的，所以 $\\nabla_{w}(\\lambda w^{\\top}Iw) = \\lambda(2Iw) = 2\\lambda w$。\n\n将这些梯度相加，我们得到：\n$$\n\\nabla_{w} J(w) = -2X^{\\top}y + 2(X^{\\top}X)w + 2\\lambda w\n$$\n\n为了找到最小化 $J(w)$ 的最优权重 $w^{\\star}$，我们将梯度设为零向量：\n$$\n\\nabla_{w} J(w^{\\star}) = -2X^{\\top}y + 2(X^{\\top}X)w^{\\star} + 2\\lambda w^{\\star} = 0\n$$\n除以 $2$ 并重新整理各项：\n$$\n(X^{\\top}X)w^{\\star} + \\lambda w^{\\star} = X^{\\top}y\n$$\n在左侧提取公因子 $w^{\\star}$（注意 $\\lambda w^{\\star} = \\lambda I w^{\\star}$，其中 $I$ 是 $d \\times d$ 单位矩阵）：\n$$\n(X^{\\top}X + \\lambda I) w^{\\star} = X^{\\top}y\n$$\n为了分离出 $w^{\\star}$，我们乘以矩阵 $(X^{\\top}X + \\lambda I)$ 的逆。这个逆矩阵存在，因为 $X^{\\top}X$ 是一个半正定矩阵，并且对于 $\\lambda > 0$，$\\lambda I$ 是一个正定矩阵。一个半正定矩阵和一个正定矩阵的和总是正定的，因此是可逆的。\n$$\nw^{\\star} = (X^{\\top}X + \\lambda I)^{-1} X^{\\top}y\n$$\n这就是优化器 $w^{\\star}$ 的闭式解。\n\n这个结果建立了与岭回归的等价性。岭回归的标准形式是找到一个系数向量 $w$ 来最小化带惩罚项的平方误差和，这正是目标函数 $J(w)$。推导出的解 $w^{\\star} = (X^{\\top}X + \\lambda I)^{-1} X^{\\top}y$ 是岭回归众所周知的闭式解。因此，使用平方误差和损失以及 L2 权重衰减来训练单层线性神经网络，在数学上等同于执行岭回归。\n\n接下来，我们为给定的数据集计算 $w^{\\star}$：\n$$\nX = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}, \\quad \\lambda = 1\n$$\n权重向量 $w$ 将在 $\\mathbb{R}^{2}$ 中。首先，我们计算所需的矩阵。\n$X$ 的转置是：\n$$\nX^{\\top} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix}\n$$\n接下来，我们计算 $X^{\\top}X$：\n$$\nX^{\\top}X = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} 1(1)+0(0)+1(1)  1(0)+0(1)+1(1) \\\\ 0(1)+1(0)+1(1)  0(0)+1(1)+1(1) \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}\n$$\n现在我们计算项 $(X^{\\top}X + \\lambda I)$。由于 $X^{\\top}X$ 是一个 $2 \\times 2$ 矩阵， $I$ 是 $2 \\times 2$ 单位矩阵。\n$$\nX^{\\top}X + \\lambda I = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} + 1 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 3  1 \\\\ 1  3 \\end{pmatrix}\n$$\n接下来，我们求这个矩阵的逆。对于一个 $2 \\times 2$ 矩阵 $A = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$，其逆为 $A^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$。\n行列式为 $\\det(X^{\\top}X + \\lambda I) = 3(3) - 1(1) = 9 - 1 = 8$。\n逆矩阵是：\n$$\n(X^{\\top}X + \\lambda I)^{-1} = \\frac{1}{8} \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix}\n$$\n现在，我们计算项 $X^{\\top}y$：\n$$\nX^{\\top}y = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1(1)+0(2)+1(3) \\\\ 0(1)+1(2)+1(3) \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix}\n$$\n最后，我们通过将两个结果相乘来计算 $w^{\\star}$：\n$$\nw^{\\star} = (X^{\\top}X + \\lambda I)^{-1} (X^{\\top}y) = \\frac{1}{8} \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 3(4) - 1(5) \\\\ -1(4) + 3(5) \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 12 - 5 \\\\ -4 + 15 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 7 \\\\ 11 \\end{pmatrix}\n$$\n所以，优化后的权重向量是：\n$$\nw^{\\star} = \\begin{pmatrix} \\frac{7}{8} \\\\ \\frac{11}{8} \\end{pmatrix}\n$$\n问题要求最终答案以行矩阵形式表示。\n$$\n(w^{\\star})^{\\top} = \\begin{pmatrix} \\frac{7}{8}  \\frac{11}{8} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7}{8}  \\frac{11}{8}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在理解了 L₂ 正则化的数学基础后，下一步是通过实验直观地感受其威力。本练习  要求你设计一个受控的二元分类实验，亲手验证 L₂ 正则化如何在高维空间中防止模型“记忆”训练数据中的噪声，从而提升其泛化能力。你将通过编程找到一个“临界”正则化强度 $\\lambda$，见证模型从随机猜测到有效泛化的转变。",
            "id": "3141360",
            "problem": "要求您实现一个完整的、可运行的程序，构建一个受控的合成二元分类实验，以研究平方欧几里得范数正则化（也称为 $L_{2}$ 正则化或权重衰减）如何防止对随机标签的记忆，从而提高泛化能力。该程序必须以精确指定的格式产生单行输出。\n\n程序必须遵循以下形式化设计。\n\n1. 数据生成。对于每个测试用例，生成一个训练设计矩阵 $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$ 和一个测试设计矩阵 $X_{\\text{test}} \\in \\mathbb{R}^{m \\times d}$，其中 $d = d_{\\text{sig}} + d_{\\text{noi}}$。$X_{\\text{train}}$ 和 $X_{\\text{test}}$ 的每一行都必须独立地从一个零均值、单位协方差的多元正态分布中抽样。构建一个真实权重向量 $w^{\\star} \\in \\mathbb{R}^{d}$，其前 $d_{\\text{sig}}$ 个条目非零，后 $d_{\\text{noi}}$ 个条目恰好为零；确保 $w^{\\star}$ 具有单位欧几里得范数。通过 $y^{\\text{clean}} = \\operatorname{sign}(X w^{\\star})$ 按元素定义干净标签，取值于 $\\{-1, +1\\}$。对于训练集，通过以概率 $p$ 将每个 $y^{\\text{clean}}_{i}$ 独立地替换为从 $\\{-1, +1\\}$ 中的随机抽取（并以概率 $1 - p$ 保持不变）来构建观测（可能被破坏）的标签 $y_{\\text{train}}$。对于测试集，使用 $y_{\\text{test}} = \\operatorname{sign}(X_{\\text{test}} w^{\\star})$，不进行任何破坏。因此，标签在期望上是平衡的，并且由于对称性，测试集上的随机水平准确率为 $0.5$。\n\n2. 模型类别与学习目标。考虑线性预测器 $f_{w}(x) = x^{\\top} w$，其中 $w \\in \\mathbb{R}^{d}$ 且无偏置项。通过最小化带 $L_{2}$ 正则化的经验风险来训练 $w$：对 $w \\in \\mathbb{R}^{d}$ 进行最小化，\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\left(x_{i}^{\\top} w - y_{\\text{train},i}\\right)^{2} \\;+\\; \\lambda \\,\\lVert w \\rVert_{2}^{2},\n$$\n对于网格中提供的多个正则化强度 $\\lambda \\ge 0$ 值。训练后，通过 $\\hat{y} = \\operatorname{sign}(X_{\\text{test}} w)$ 进行分类，并计算测试准确率，即相对于 $y_{\\text{test}}$ 的正确预测比例。\n\n3. 转变标准与临界 $\\lambda$。定义改进阈值 $\\tau$（严格大于随机水平 $0.5$）。对于给定的测试用例和给定的 $\\lambda$ 值升序网格，将临界 $\\lambda$ 定义为网格中使得相应测试准确率至少为 $\\tau$ 的最小 $\\lambda$。如果所提供网格中没有 $\\lambda$ 能达到至少为 $\\tau$ 的测试准确率，则将临界 $\\lambda$ 定义为 $-1.0$。\n\n4. 确定性。每个测试用例使用一个固定的伪随机数生成器种子。在计算 $\\lambda = 0$ 的解时，以确定性的方式处理正规方程组中潜在的奇异性，从而使训练规则是明确定义且可复现的。\n\n5. 测试套件。您的程序必须执行以下三个测试用例并独立处理每个用例：\n   - 用例 A（$L_{2}$ 抑制随机标签记忆）：$n = 120$，$m = 4000$，$d_{\\text{sig}} = 5$，$d_{\\text{noi}} = 300$，$p = 0.4$，改进阈值 $\\tau = 0.6$，正则化网格\n     $$\n     \\Lambda_{A} = [\\,0.0,\\; 10^{-6},\\; 10^{-5},\\; 3\\cdot 10^{-5},\\; 10^{-4},\\; 3\\cdot 10^{-4},\\; 10^{-3},\\; 3\\cdot 10^{-3},\\; 10^{-2},\\; 3\\cdot 10^{-2},\\; 10^{-1},\\; 3\\cdot 10^{-1},\\; 1.0,\\; 3.0,\\; 10.0\\,].\n     $$\n   - 用例 B（完全随机标签；无法改进）：$n = 120$，$m = 4000$，$d_{\\text{sig}} = 5$，$d_{\\text{noi}} = 300$，$p = 1.0$，改进阈值 $\\tau = 0.6$，正则化网格\n     $$\n     \\Lambda_{B} = \\Lambda_{A}.\n     $$\n   - 用例 C（干净的低维真实信号；立即改进）：$n = 200$，$m = 2000$，$d_{\\text{sig}} = 1$，$d_{\\text{noi}} = 0$，$p = 0.0$，改进阈值 $\\tau = 0.6$，正则化网格\n     $$\n     \\Lambda_{C} = \\Lambda_{A}.\n     $$\n\n   为保证可复现性，请使用各用例的随机种子 $s_{A} = 12345$、$s_{B} = 13345$ 和 $s_{C} = 14345$。\n\n6. 输出规范。您的程序必须产生单行输出，其中包含一个恰好有三个浮点值的列表，按 A、B、C 用例的顺序排列，每个值是该用例的临界正则化强度（如上定义），当未观察到向改进的泛化能力转变时，使用值 $-1.0$。该行必须采用精确格式\n$$\n[\\ell_{A},\\ell_{B},\\ell_{C}]\n$$\n值之间用逗号分隔，无附加文本。不涉及物理单位。\n\n您的目标是以一种直接测试 $L_{2}$ 正则化是否以及何时通过收缩权重向量来防止记忆随机标签的方式实现此实验，并测量对于三个用例中的每一个，测试准确率从随机水平转变为改进泛化能力的最小正则化强度。",
            "solution": "该问题要求实现一个数值实验，以研究 $L_2$ 正则化在二元分类背景下的作用。该实验旨在展示正则化如何防止模型记忆训练标签中的噪声，从而提高其对未见数据的泛化能力。任务的核心是找到“临界”正则化强度 $\\lambda$，在该强度下，模型在测试集上的性能从随机水平转变为达到指定的改进阈值。这将针对三个旨在突出正则化不同方面的独特案例来完成。\n\n模型是一个线性预测器 $f_w(x) = x^{\\top} w$，其中 $x \\in \\mathbb{R}^{d}$ 是特征向量，$w \\in \\mathbb{R}^{d}$ 是权重向量。权重是通过最小化一个基于平方误差损失的正则化经验风险目标函数来学习的。对于一个包含 $n$ 个样本 $\\{(x_i, y_i)\\}_{i=1}^{n}$ 的训练集，目标函数 $L(w)$ 由下式给出：\n$$\nL(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(x_{i}^{\\top} w - y_i\\right)^{2} + \\lambda \\lVert w \\rVert_{2}^{2}\n$$\n这里，$y_i \\in \\{-1, +1\\}$ 是训练标签，$\\lambda \\ge 0$ 是正则化参数，它控制对权重向量的平方欧几里得范数 $\\lVert w \\rVert_{2}^{2}$ 的惩罚。\n\n为了找到最小化 $L(w)$ 的最优权重向量 $\\hat{w}$，我们计算 $L(w)$ 相对于 $w$ 的梯度并将其设为零向量。首先，我们用矩阵表示法来表达目标函数。设 $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$ 为设计矩阵，其第 $i$ 行为 $x_i^{\\top}$，并设 $y_{\\text{train}} \\in \\mathbb{R}^{n}$ 为训练标签向量。目标函数变为：\n$$\nL(w) = \\frac{1}{n} \\lVert X_{\\text{train}}w - y_{\\text{train}} \\rVert_{2}^{2} + \\lambda \\lVert w \\rVert_{2}^{2} = \\frac{1}{n} (X_{\\text{train}}w - y_{\\text{train}})^{\\top}(X_{\\text{train}}w - y_{\\text{train}}) + \\lambda w^{\\top}w\n$$\n$L(w)$ 相对于 $w$ 的梯度为：\n$$\n\\nabla_w L(w) = \\frac{1}{n} \\nabla_w (w^{\\top}X_{\\text{train}}^{\\top}X_{\\text{train}}w - 2y_{\\text{train}}^{\\top}X_{\\text{train}}w + y_{\\text{train}}^{\\top}y_{\\text{train}}) + \\lambda \\nabla_w (w^{\\top}w)\n$$\n使用标准的矩阵微积分恒等式，并注意到 $X_{\\text{train}}^{\\top}X_{\\text{train}}$ 是一个对称矩阵，我们得到：\n$$\n\\nabla_w L(w) = \\frac{1}{n} (2X_{\\text{train}}^{\\top}X_{\\text{train}}w - 2X_{\\text{train}}^{\\top}y_{\\text{train}}) + 2\\lambda w\n$$\n将梯度设为零以求最小值：\n$$\n\\frac{2}{n} X_{\\text{train}}^{\\top}X_{\\text{train}}w - \\frac{2}{n} X_{\\text{train}}^{\\top}y_{\\text{train}} + 2\\lambda w = 0\n$$\n$$\n(X_{\\text{train}}^{\\top}X_{\\text{train}} + n\\lambda I) w = X_{\\text{train}}^{\\top}y_{\\text{train}}\n$$\n其中 $I$ 是 $d \\times d$ 的单位矩阵。\n\n这个线性方程组为最优权重向量 $\\hat{w}$ 提供了求解方法。\n\n对于 $\\lambda > 0$，矩阵 $(X_{\\text{train}}^{\\top}X_{\\text{train}} + n\\lambda I)$ 保证是可逆的。这是因为 $X_{\\text{train}}^{\\top}X_{\\text{train}}$ 是半正定的，而 $n\\lambda I$ 是正定的，使其和为正定矩阵，因此可逆。唯一解是：\n$$\n\\hat{w} = (X_{\\text{train}}^{\\top}X_{\\text{train}} + n\\lambda I)^{-1} X_{\\text{train}}^{\\top} y_{\\text{train}}\n$$\n在数值上，直接求解线性系统比计算矩阵的逆更稳定和高效。\n\n对于 $\\lambda = 0$ 的特殊情况，目标函数变为标准的非正则化最小二乘问题，其正规方程为 $X_{\\text{train}}^{\\top}X_{\\text{train}}w = X_{\\text{train}}^{\\top}y_{\\text{train}}$。在特征数量 $d$ 大于样本数量 $n$ 的高维设置中（如测试用例 A 和 B），矩阵 $X_{\\text{train}}^{\\top}X_{\\text{train}}$ 是奇异的，系统有无限多个解。问题要求对此情况进行确定性处理。标准方法是使用 Moore-Penrose 伪逆，它能产生具有最小欧几里得范数的解。这等同于求解最小二乘问题 $\\min_{w} \\lVert X_{\\text{train}}w - y_{\\text{train}} \\rVert_2^2$，可以使用数值线性代数库轻松完成。\n\n每个测试用例的实验步骤如下：\n1.  使用指定的种子初始化伪随机数生成器以保证可复现性。\n2.  构建真实权重向量 $w^{\\star} \\in \\mathbb{R}^{d}$：从标准正态分布中生成其前 $d_{\\text{sig}}$ 个分量，将此子向量归一化使其范数为1，并将其余 $d_{\\text{noi}}$ 个分量设置为零。这确保了 $\\lVert w^{\\star} \\rVert_{2} = 1$。\n3.  通过从标准正态分布 $\\mathcal{N}(0, 1)$ 中独立抽取每个条目，生成训练数据 $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$ 和测试数据 $X_{\\text{test}} \\in \\mathbb{R}^{m \\times d}$。\n4.  生成标签。真实的测试标签是 $y_{\\text{test}} = \\operatorname{sign}(X_{\\text{test}} w^{\\star})$。训练标签 $y_{\\text{train}}$ 的生成方法是：首先计算干净标签 $y^{\\text{clean}} = \\operatorname{sign}(X_{\\text{train}} w^{\\star})$，然后以概率 $p$ 将每个标签替换为从 $\\{-1, +1\\}$ 中随机抽取的值。`sign` 函数对于输入 $0$ 的输出将确定性地映射到 $+1$。\n5.  遍历所提供的 $\\lambda$ 值升序网格。对于每个 $\\lambda$，计算相应的最优权重向量 $\\hat{w}$。\n6.  使用学习到的 $\\hat{w}$ 在测试集上进行预测：$\\hat{y}_{\\text{test}} = \\operatorname{sign}(X_{\\text{test}}\\hat{w})$。\n7.  计算测试准确率，即匹配预测的比例：$\\frac{1}{m} \\sum_{j=1}^{m} \\mathbb{I}(\\hat{y}_{\\text{test},j} = y_{\\text{test},j})$。\n8.  临界 $\\lambda$ 是网格中第一个使测试准确率大于或等于改进阈值 $\\tau$ 的值。如果没有找到这样的 $\\lambda$，则临界值定义为 $-1.0$。\n\n此过程独立应用于三个测试用例中的每一个，并报告得到的临界 $\\lambda$ 值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Executes a synthetic experiment to study L2 regularization.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    lambda_grid = [0.0, 1e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 0.1, 0.3, 1.0, 3.0, 10.0]\n    \n    test_cases = [\n        # Case A: High-dimensional, noisy labels\n        {'n': 120, 'm': 4000, 'd_sig': 5, 'd_noi': 300, 'p': 0.4, 'tau': 0.6, 'seed': 12345, 'lambda_grid': lambda_grid},\n        # Case B: Fully random labels\n        {'n': 120, 'm': 4000, 'd_sig': 5, 'd_noi': 300, 'p': 1.0, 'tau': 0.6, 'seed': 13345, 'lambda_grid': lambda_grid},\n        # Case C: Clean, low-dimensional signal\n        {'n': 200, 'm': 2000, 'd_sig': 1, 'd_noi': 0, 'p': 0.0, 'tau': 0.6, 'seed': 14345, 'lambda_grid': lambda_grid},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        n, m, d_sig, d_noi, p, tau, seed, current_lambda_grid = \\\n            case['n'], case['m'], case['d_sig'], case['d_noi'], case['p'], case['tau'], case['seed'], case['lambda_grid']\n        \n        d = d_sig + d_noi\n        \n        # 1. Deterministic Setup\n        rng = np.random.default_rng(seed)\n\n        # 2. Data Generation\n        # Construct ground-truth weight vector w_star with unit norm\n        w_star_sig = rng.standard_normal(size=d_sig)\n        w_star_sig /= np.linalg.norm(w_star_sig)\n        w_star = np.concatenate((w_star_sig, np.zeros(d_noi)))\n        \n        # Generate design matrices\n        X_train = rng.standard_normal(size=(n, d))\n        X_test = rng.standard_normal(size=(m, d))\n        \n        # Generate labels\n        # Test labels (clean)\n        y_test = np.sign(X_test @ w_star)\n        y_test[y_test == 0] = 1 # Deterministic handling of sign(0)\n\n        # Training labels (potentially corrupted)\n        y_clean_train = np.sign(X_train @ w_star)\n        y_clean_train[y_clean_train == 0] = 1\n        \n        y_train = y_clean_train.copy()\n        corruption_mask = rng.random(size=n) < p\n        num_corrupt = np.sum(corruption_mask)\n        if num_corrupt > 0:\n            random_labels = rng.choice([-1, 1], size=num_corrupt)\n            y_train[corruption_mask] = random_labels\n\n        # 3. Model Training and Evaluation\n        critical_lambda = -1.0\n        \n        # Pre-compute parts of the normal equations\n        XtX = X_train.T @ X_train\n        Xty = X_train.T @ y_train\n        identity_d = np.identity(d)\n\n        for lam in current_lambda_grid:\n            # Solve for the weight vector w_hat\n            if lam == 0.0:\n                # For lambda=0, use lstsq for the min-norm solution, robust to singularity\n                w_hat = np.linalg.lstsq(X_train, y_train, rcond=None)[0]\n            else:\n                # For lambda > 0, solve the regularized normal equations\n                A = XtX + n * lam * identity_d\n                w_hat = np.linalg.solve(A, Xty)\n            \n            # Predict on test set\n            y_pred = np.sign(X_test @ w_hat)\n            y_pred[y_pred == 0] = 1 # Deterministic handling of sign(0)\n            \n            # Compute test accuracy\n            accuracy = np.mean(y_pred == y_test)\n            \n            # Check for transition\n            if accuracy >= tau:\n                critical_lambda = lam\n                break\n        \n        results.append(critical_lambda)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "我们已经看到 L₂ 正则化的有效性，但其效果高度依赖于正则化强度 $\\lambda$ 的选择。传统的网格搜索等方法费时费力，本练习  将带你进入一个更高级的领域：基于梯度的超参数优化。你将通过隐式微分推导出验证集损失关于 $\\lambda$ 的梯度（即超梯度），并实现一个单步更新算法，从而向自动化调优这一机器学习前沿课题迈出坚实的一步。",
            "id": "3141419",
            "problem": "考虑一个带有参数矢量 $\\,\\mathbf{w} \\in \\mathbb{R}^d\\,$ 的线性模型。训练损失定义为平均平方误差加上欧几里得范数正则化（也称为 $L_2$ 正则化或权重衰减），验证损失定义为在留出集上的平均平方误差。形式上，对于训练数据矩阵 $\\,X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}\\,$ 和训练目标 $\\,\\mathbf{y}_{\\text{train}} \\in \\mathbb{R}^n\\,$，以及验证数据矩阵 $\\,X_{\\text{val}} \\in \\mathbb{R}^{m \\times d}\\,$ 和验证目标 $\\,\\mathbf{y}_{\\text{val}} \\in \\mathbb{R}^m\\,$，定义\n$$\n\\mathcal{L}_{\\text{train}}(\\mathbf{w};\\lambda) \\equiv \\frac{1}{2n}\\left\\| X_{\\text{train}} \\mathbf{w} - \\mathbf{y}_{\\text{train}} \\right\\|_2^2 \\;+\\; \\frac{\\lambda}{2}\\left\\|\\mathbf{w}\\right\\|_2^2,\n$$\n$$\n\\mathcal{L}_{\\text{val}}(\\mathbf{w}) \\equiv \\frac{1}{2m}\\left\\| X_{\\text{val}} \\mathbf{w} - \\mathbf{y}_{\\text{val}} \\right\\|_2^2,\n$$\n其中 $\\,\\lambda \\ge 0\\,$ 是正则化强度。假设 $\\,\\hat{\\mathbf{w}}(\\lambda)\\,$ 是 $\\,\\mathcal{L}_{\\text{train}}(\\mathbf{w};\\lambda)\\,$ 的唯一最小化子，其由一阶最优性条件 $\\,\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{train}}(\\hat{\\mathbf{w}}(\\lambda);\\lambda) = \\mathbf{0}\\,$ 表征。\n\n你的任务是：\n- 从光滑无约束最小化（一阶最优性和可微性）的基本原理和链式法则出发，使用训练最优性条件的隐式微分，推导超梯度 $\\,\\frac{\\partial \\mathcal{L}_{\\text{val}}(\\hat{\\mathbf{w}}(\\lambda))}{\\partial \\lambda}\\,$ 的一个原则性表达式，该表达式用在 $\\,\\hat{\\mathbf{w}}(\\lambda)\\,$ 处求值的量来表示。除了这些原理之外，不要使用任何快捷公式。\n- 使用线性系统的解（不要构造显式矩阵逆）来实现 $\\,\\hat{\\mathbf{w}}(\\lambda)\\,$ 和超梯度的数值稳定计算。\n- 实现单步超参数更新 $\\,\\lambda_{\\text{new}} = \\max\\!\\left(0,\\; \\lambda - \\eta \\,\\frac{\\partial \\mathcal{L}_{\\text{val}}(\\hat{\\mathbf{w}}(\\lambda))}{\\partial \\lambda}\\right)\\,$，其中 $\\,\\eta > 0\\,$ 是超参数学习率，并通过投影到非负实数上来强制 $\\,\\lambda\\,$ 的非负性。\n\n使用以下测试套件。每个案例指定了 $\\,X_{\\text{train}}\\,$, $\\,\\mathbf{y}_{\\text{train}}\\,$, $\\,X_{\\text{val}}\\,$, $\\,\\mathbf{y}_{\\text{val}}\\,$, 初始 $\\,\\lambda\\,$ 和超参数学习率 $\\,\\eta\\,$。\n\n- 案例 $\\,1$（良态，中等正则化，理想路径）：\n  - $X_{\\text{train}}$ 行：\n    - $[\\,1.0,\\,-1.0,\\,2.0\\,]$\n    - $[\\,0.0,\\,3.0,\\,-1.0\\,]$\n    - $[\\,2.0,\\,0.5,\\,0.0\\,]$\n    - $[\\,-1.0,\\,1.5,\\,1.0\\,]$\n    - $[\\,3.0,\\,-2.0,\\,0.5\\,]$\n  - $\\mathbf{y}_{\\text{train}} = [\\,1.0,\\,-2.0,\\,0.5,\\,3.0,\\,-1.0\\,]$\n  - $X_{\\text{val}}$ 行：\n    - $[\\,0.5,\\,-0.5,\\,1.0\\,]$\n    - $[\\,1.0,\\,2.0,\\,-1.0\\,]$\n    - $[\\,-1.0,\\,0.0,\\,0.0\\,]$\n    - $[\\,0.0,\\,1.0,\\,1.0\\,]$\n  - $\\mathbf{y}_{\\text{val}} = [\\,0.0,\\,1.0,\\,-1.0,\\,2.0\\,]$\n  - $\\lambda = 0.1$, $\\eta = 0.05$。\n\n- 案例 $\\,2$（边界正则化 $\\,\\lambda = 0\\,$）：\n  - 与案例 $\\,1$ 相同的 $X_{\\text{train}}$, $\\mathbf{y}_{\\text{train}}$, $X_{\\text{val}}$, $\\mathbf{y}_{\\text{val}}$。\n  - $\\lambda = 0.0$, $\\eta = 0.05$。\n\n- 案例 $\\,3$（非常大的正则化）：\n  - 与案例 $\\,1$ 相同的 $X_{\\text{train}}$, $\\mathbf{y}_{\\text{train}}$, $X_{\\text{val}}$, $\\mathbf{y}_{\\text{val}}$。\n  - $\\lambda = 10.0$, $\\eta = 0.05$。\n\n- 案例 $\\,4$（由小正则化稳定的病态设计）：\n  - $X_{\\text{train}}$ 行：\n    - $[\\,1.0,\\,2.0,\\,3.0\\,]$\n    - $[\\,2.0,\\,4.0,\\,6.0\\,]$\n    - $[\\,3.0,\\,6.1,\\,9.1\\,]$\n    - $[\\,4.0,\\,8.0,\\,12.0\\,]$\n    - $[\\,5.0,\\,10.1,\\,15.2\\,]$\n  - $\\mathbf{y}_{\\text{train}} = [\\,1.0,\\,2.0,\\,3.1,\\,4.0,\\,5.2\\,]$\n  - $X_{\\text{val}}$ 行：\n    - $[\\,6.0,\\,12.0,\\,18.0\\,]$\n    - $[\\,7.0,\\,14.0,\\,21.0\\,]$\n    - $[\\,8.0,\\,16.1,\\,24.2\\,]$\n  - $\\mathbf{y}_{\\text{val}} = [\\,6.2,\\,7.1,\\,8.3\\,]$\n  - $\\lambda = 0.001$, $\\eta = 0.1$。\n\n你的程序应生成单行输出，其中包含所有测试案例的结果，结果为方括号括起来的逗号分隔列表。每个测试案例的结果必须是一个双元素列表 $[\\,g,\\,\\lambda_{\\text{new}}\\,]$，其中 $\\,g\\,$ 是计算出的超梯度 $\\,\\frac{\\partial \\mathcal{L}_{\\text{val}}(\\hat{\\mathbf{w}}(\\lambda))}{\\partial \\lambda}\\,$，而 $\\,\\lambda_{\\text{new}}\\,$ 是一步更新后的正则化强度。例如，最终输出格式应如下所示\n$$\n[\\,[g_1,\\lambda_{\\text{new},1}],\\,[g_2,\\lambda_{\\text{new},2}],\\,[g_3,\\lambda_{\\text{new},3}],\\,[g_4,\\lambda_{\\text{new},4}]\\,].\n$$\n不涉及物理单位或角度单位。所有输出必须是实数（浮点数），并且必须通过投影来强制 $\\,\\lambda_{\\text{new}}\\,$ 的非负性。",
            "solution": "该问题要求在线性模型与$L_2$正则化的背景下，推导并实现一种超参数梯度计算方法。具体来说，我们必须求验证损失相对于正则化参数 $\\lambda$ 的导数，该导数在最小化正则化训练损失的最优模型参数 $\\hat{\\mathbf{w}}(\\lambda)$ 处求值。推导必须从第一性原理出发，即一阶最优性条件和链式法则。\n\n首先，让我们形式化给定的量。参数矢量 $\\mathbf{w} \\in \\mathbb{R}^d$ 的训练损失是：\n$$\n\\mathcal{L}_{\\text{train}}(\\mathbf{w};\\lambda) = \\frac{1}{2n}\\left\\| X_{\\text{train}} \\mathbf{w} - \\mathbf{y}_{\\text{train}} \\right\\|_2^2 + \\frac{\\lambda}{2}\\left\\|\\mathbf{w}\\right\\|_2^2\n$$\n验证损失是：\n$$\n\\mathcal{L}_{\\text{val}}(\\mathbf{w}) = \\frac{1}{2m}\\left\\| X_{\\text{val}} \\mathbf{w} - \\mathbf{y}_{\\text{val}} \\right\\|_2^2\n$$\n对于给定的 $\\lambda \\ge 0$，矢量 $\\hat{\\mathbf{w}}(\\lambda)$ 是训练损失的唯一最小化子。它由一阶最优性条件定义，该条件指出训练损失关于 $\\mathbf{w}$ 的梯度在最优点必须为零：\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{train}}(\\hat{\\mathbf{w}}(\\lambda);\\lambda) = \\mathbf{0}\n$$\n我们的目标是计算超梯度 $\\frac{\\partial \\mathcal{L}_{\\text{val}}(\\hat{\\mathbf{w}}(\\lambda))}{\\partial \\lambda}$。验证损失 $\\mathcal{L}_{\\text{val}}$ 通过最优参数矢量 $\\hat{\\mathbf{w}}(\\lambda)$ 隐式地依赖于 $\\lambda$。因此，我们可以应用链式法则：\n$$\n\\frac{\\partial \\mathcal{L}_{\\text{val}}(\\hat{\\mathbf{w}}(\\lambda))}{\\partial \\lambda} = \\left( \\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{val}}(\\hat{\\mathbf{w}}(\\lambda)) \\right)^T \\frac{\\partial \\hat{\\mathbf{w}}(\\lambda)}{\\partial \\lambda}\n$$\n这个表达式由我们必须确定的两部分组成：验证损失的梯度 $\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{val}}(\\hat{\\mathbf{w}}(\\lambda))$，以及最优参数对超参数的敏感度 $\\frac{\\partial \\hat{\\mathbf{w}}(\\lambda)}{\\partial \\lambda}$。\n\n第一项，即验证损失的梯度，可以很容易地计算出来。使用标准矩阵微积分，$\\mathcal{L}_{\\text{val}}(\\mathbf{w})$ 的梯度是：\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{val}}(\\mathbf{w}) = \\frac{1}{m} X_{\\text{val}}^T (X_{\\text{val}} \\mathbf{w} - \\mathbf{y}_{\\text{val}})\n$$\n我们在 $\\mathbf{w} = \\hat{\\mathbf{w}}(\\lambda)$ 处对此进行求值。\n\n第二项 $\\frac{\\partial \\hat{\\mathbf{w}}(\\lambda)}{\\partial \\lambda}$ 需要对训练最优性条件进行隐式微分。我们首先写出训练损失的梯度：\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{train}}(\\mathbf{w};\\lambda) = \\frac{1}{n} X_{\\text{train}}^T (X_{\\text{train}} \\mathbf{w} - \\mathbf{y}_{\\text{train}}) + \\lambda \\mathbf{w}\n$$\n最优性条件是一个对所有相关的 $\\lambda$ 都成立的恒等式：\n$$\n\\mathbf{G}(\\hat{\\mathbf{w}}(\\lambda), \\lambda) \\equiv \\frac{1}{n} X_{\\text{train}}^T (X_{\\text{train}} \\hat{\\mathbf{w}}(\\lambda) - \\mathbf{y}_{\\text{train}}) + \\lambda \\hat{\\mathbf{w}}(\\lambda) = \\mathbf{0}\n$$\n由于这个恒等式成立，其关于 $\\lambda$ 的全导数也必须为零：\n$$\n\\frac{d \\mathbf{G}}{d \\lambda} = \\frac{\\partial \\mathbf{G}}{\\partial \\mathbf{w}} \\frac{\\partial \\hat{\\mathbf{w}}}{\\partial \\lambda} + \\frac{\\partial \\mathbf{G}}{\\partial \\lambda} = \\mathbf{0}\n$$\n项 $\\frac{\\partial \\mathbf{G}}{\\partial \\mathbf{w}}$ 是梯度的雅可比矩阵，也就是训练损失的海森矩阵，在 $\\hat{\\mathbf{w}}(\\lambda)$ 处求值：\n$$\nH_{\\text{train}}(\\lambda) \\equiv \\nabla^2_{\\mathbf{w}} \\mathcal{L}_{\\text{train}}(\\hat{\\mathbf{w}}(\\lambda);\\lambda) = \\frac{\\partial}{\\partial \\mathbf{w}} \\left( \\frac{1}{n} X_{\\text{train}}^T (X_{\\text{train}} \\mathbf{w} - \\mathbf{y}_{\\text{train}}) + \\lambda \\mathbf{w} \\right) = \\frac{1}{n} X_{\\text{train}}^T X_{\\text{train}} + \\lambda I\n$$\n其中 $I$ 是 $d \\times d$ 的单位矩阵。项 $\\frac{\\partial \\mathbf{G}}{\\partial \\lambda}$ 是梯度函数关于 $\\lambda$ 的偏导数，同样在 $\\hat{\\mathbf{w}}(\\lambda)$ 处求值：\n$$\n\\frac{\\partial \\mathbf{G}}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left( \\frac{1}{n} X_{\\text{train}}^T (X_{\\text{train}} \\mathbf{w} - \\mathbf{y}_{\\text{train}}) + \\lambda \\mathbf{w} \\right) \\Big|_{\\mathbf{w}=\\hat{\\mathbf{w}}(\\lambda)} = \\hat{\\mathbf{w}}(\\lambda)\n$$\n将这些代回到全导数方程中，得到：\n$$\nH_{\\text{train}}(\\lambda) \\frac{\\partial \\hat{\\mathbf{w}}(\\lambda)}{\\partial \\lambda} + \\hat{\\mathbf{w}}(\\lambda) = \\mathbf{0}\n$$\n由于 $H_{\\text{train}}(\\lambda)$ 是可逆的（当 $\\lambda > 0$ 时，以及当 $\\lambda = 0$ 且 $X_{\\text{train}}$ 具有满列秩时，它是正定的），我们可以求解敏感度项：\n$$\n\\frac{\\partial \\hat{\\mathbf{w}}(\\lambda)}{\\partial \\lambda} = - \\left( H_{\\text{train}}(\\lambda) \\right)^{-1} \\hat{\\mathbf{w}}(\\lambda)\n$$\n现在，我们将这个结果代回到我们的超梯度链式法则表达式中：\n$$\n\\frac{\\partial \\mathcal{L}_{\\text{val}}}{\\partial \\lambda} = - \\left( \\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{val}}(\\hat{\\mathbf{w}}) \\right)^T \\left( H_{\\text{train}}(\\lambda) \\right)^{-1} \\hat{\\mathbf{w}}(\\lambda)\n$$\n这个表达式提供了一种计算超梯度的原则性方法。对于数值实现，避免显式计算矩阵逆 $\\left( H_{\\text{train}}(\\lambda) \\right)^{-1}$ 至关重要。相反，我们求解线性系统。首先，为了获得 $\\hat{\\mathbf{w}}(\\lambda)$，我们求解从最优性条件 $\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{train}} = \\mathbf{0}$ 推导出的正规方程：\n$$\n(\\frac{1}{n} X_{\\text{train}}^T X_{\\text{train}} + \\lambda I) \\mathbf{w} = \\frac{1}{n} X_{\\text{train}}^T \\mathbf{y}_{\\text{train}} \\implies (X_{\\text{train}}^T X_{\\text{train}} + n\\lambda I) \\mathbf{w} = X_{\\text{train}}^T \\mathbf{y}_{\\text{train}}\n$$\n设 $A = X_{\\text{train}}^T X_{\\text{train}} + n\\lambda I$ 和 $\\mathbf{b} = X_{\\text{train}}^T \\mathbf{y}_{\\text{train}}$。我们求解 $A \\hat{\\mathbf{w}} = \\mathbf{b}$ 以得到 $\\hat{\\mathbf{w}}$。\n\n为了在不进行求逆的情况下计算超梯度，我们可以重新排列表达式。设 $\\mathbf{g}_{\\text{val}} = \\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{val}}(\\hat{\\mathbf{w}})$。超梯度为 $-\\mathbf{g}_{\\text{val}}^T H_{\\text{train}}^{-1} \\hat{\\mathbf{w}}$。因为 $H_{\\text{train}}$ 是对称的，我们可以将其写为 $-\\left(H_{\\text{train}}^{-1} \\mathbf{g}_{\\text{val}}\\right)^T \\hat{\\mathbf{w}}$。设 $\\mathbf{p} = H_{\\text{train}}^{-1} \\mathbf{g}_{\\text{val}}$。我们可以通过求解线性系统 $H_{\\text{train}} \\mathbf{p} = \\mathbf{g}_{\\text{val}}$ 来找到 $\\mathbf{p}$。注意 $H_{\\text{train}} = \\frac{1}{n} A$。所以系统是 $(\\frac{1}{n} A) \\mathbf{p} = \\mathbf{g}_{\\text{val}}$，或 $A \\mathbf{p} = n \\mathbf{g}_{\\text{val}}$。\n\n完整的、数值稳定的算法如下：\n1.  定义系统矩阵 $A = X_{\\text{train}}^T X_{\\text{train}} + n\\lambda I$ 和矢量 $\\mathbf{b} = X_{\\text{train}}^T \\mathbf{y}_{\\text{train}}$。\n2.  求解线性系统 $A \\hat{\\mathbf{w}} = \\mathbf{b}$ 以找到最优参数 $\\hat{\\mathbf{w}}(\\lambda)$。\n3.  计算验证损失的梯度 $\\mathbf{g}_{\\text{val}} = \\frac{1}{m} X_{\\text{val}}^T (X_{\\text{val}} \\hat{\\mathbf{w}} - \\mathbf{y}_{\\text{val}})$。\n4.  定义右侧矢量 $\\mathbf{c} = n \\mathbf{g}_{\\text{val}}$。\n5.  求解第二个线性系统 $A \\mathbf{p} = \\mathbf{c}$ 以得到矢量 $\\mathbf{p}$。注意，这里重用了相同的矩阵 $A$，这在计算上是高效的。\n6.  超梯度由点积 $g = -\\mathbf{p}^T \\hat{\\mathbf{w}}$ 给出。\n7.  最后，执行单步超参数更新：$\\lambda_{\\text{new}} = \\max\\left(0, \\lambda - \\eta g\\right)$。\n\n此过程允许鲁棒地计算超梯度并随后更新正则化参数 $\\lambda$，构成了基于梯度的超参数优化的基础。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_hypergradient_step(X_train, y_train, X_val, y_val, lambda_val, eta):\n    \"\"\"\n    Computes the optimal weights, the hypergradient, and the updated lambda.\n\n    Args:\n        X_train (np.ndarray): Training data matrix of shape (n, d).\n        y_train (np.ndarray): Training target vector of shape (n,).\n        X_val (np.ndarray): Validation data matrix of shape (m, d).\n        y_val (np.ndarray): Validation target vector of shape (m,).\n        lambda_val (float): The current regularization strength.\n        eta (float): The hyperparameter learning rate.\n\n    Returns:\n        tuple: A tuple containing the hypergradient (g) and the new lambda (lambda_new).\n    \"\"\"\n    n, d = X_train.shape\n    m = X_val.shape[0]\n\n    # Step 1: Define the system matrix A and vector b for finding optimal weights w_hat.\n    # The normal equation is (X_train.T @ X_train + n*lambda*I) @ w = X_train.T @ y_train.\n    A = X_train.T @ X_train + n * lambda_val * np.identity(d)\n    b = X_train.T @ y_train\n\n    # Step 2: Solve the linear system A @ w_hat = b for w_hat.\n    # This finds the weights that minimize the regularized training loss.\n    try:\n        w_hat = np.linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        # For ill-conditioned or singular matrices, use the pseudo-inverse based solver.\n        w_hat = np.linalg.lstsq(A, b, rcond=None)[0]\n\n    # Step 3: Compute the gradient of the validation loss with respect to w, at w_hat.\n    # grad_val = (1/m) * X_val.T @ (X_val @ w_hat - y_val)\n    val_residuals = X_val @ w_hat - y_val\n    grad_val = (1 / m) * (X_val.T @ val_residuals)\n\n    # Step 4: Define the right-hand side vector for the second linear system.\n    # This system will find p = H_train^-1 @ grad_val.\n    # The system is A @ p = n * grad_val.\n    c = n * grad_val\n\n    # Step 5: Solve the second linear system A @ p = c for p.\n    # We reuse the matrix A from the first step for efficiency.\n    try:\n        p = np.linalg.solve(A, c)\n    except np.linalg.LinAlgError:\n        p = np.linalg.lstsq(A, c, rcond=None)[0]\n\n    # Step 6: Compute the hypergradient g.\n    # g = -p.T @ w_hat\n    g = -np.dot(p, w_hat)\n\n    # Step 7: Update lambda using the computed hypergradient.\n    # Enforce non-negativity by projection.\n    lambda_new = max(0.0, lambda_val - eta * g)\n\n    return g, lambda_new\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X_train\": np.array([\n                [1.0, -1.0, 2.0], [0.0, 3.0, -1.0], [2.0, 0.5, 0.0],\n                [-1.0, 1.5, 1.0], [3.0, -2.0, 0.5]\n            ]),\n            \"y_train\": np.array([1.0, -2.0, 0.5, 3.0, -1.0]),\n            \"X_val\": np.array([\n                [0.5, -0.5, 1.0], [1.0, 2.0, -1.0], [-1.0, 0.0, 0.0],\n                [0.0, 1.0, 1.0]\n            ]),\n            \"y_val\": np.array([0.0, 1.0, -1.0, 2.0]),\n            \"lambda_val\": 0.1,\n            \"eta\": 0.05\n        },\n        {\n            \"X_train\": np.array([\n                [1.0, -1.0, 2.0], [0.0, 3.0, -1.0], [2.0, 0.5, 0.0],\n                [-1.0, 1.5, 1.0], [3.0, -2.0, 0.5]\n            ]),\n            \"y_train\": np.array([1.0, -2.0, 0.5, 3.0, -1.0]),\n            \"X_val\": np.array([\n                [0.5, -0.5, 1.0], [1.0, 2.0, -1.0], [-1.0, 0.0, 0.0],\n                [0.0, 1.0, 1.0]\n            ]),\n            \"y_val\": np.array([0.0, 1.0, -1.0, 2.0]),\n            \"lambda_val\": 0.0,\n            \"eta\": 0.05\n        },\n        {\n            \"X_train\": np.array([\n                [1.0, -1.0, 2.0], [0.0, 3.0, -1.0], [2.0, 0.5, 0.0],\n                [-1.0, 1.5, 1.0], [3.0, -2.0, 0.5]\n            ]),\n            \"y_train\": np.array([1.0, -2.0, 0.5, 3.0, -1.0]),\n            \"X_val\": np.array([\n                [0.5, -0.5, 1.0], [1.0, 2.0, -1.0], [-1.0, 0.0, 0.0],\n                [0.0, 1.0, 1.0]\n            ]),\n            \"y_val\": np.array([0.0, 1.0, -1.0, 2.0]),\n            \"lambda_val\": 10.0,\n            \"eta\": 0.05\n        },\n        {\n            \"X_train\": np.array([\n                [1.0, 2.0, 3.0], [2.0, 4.0, 6.0], [3.0, 6.1, 9.1],\n                [4.0, 8.0, 12.0], [5.0, 10.1, 15.2]\n            ]),\n            \"y_train\": np.array([1.0, 2.0, 3.1, 4.0, 5.2]),\n            \"X_val\": np.array([\n                [6.0, 12.0, 18.0], [7.0, 14.0, 21.0], [8.0, 16.1, 24.2]\n            ]),\n            \"y_val\": np.array([6.2, 7.1, 8.3]),\n            \"lambda_val\": 0.001,\n            \"eta\": 0.1\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        g, lambda_new = compute_hypergradient_step(\n            case[\"X_train\"],\n            case[\"y_train\"],\n            case[\"X_val\"],\n            case[\"y_val\"],\n            case[\"lambda_val\"],\n            case[\"eta\"]\n        )\n        results.append([g, lambda_new])\n\n    # Final print statement in the exact required format.\n    str_results = [f\"[{g},{l_new}]\" for g, l_new in results]\n    print(f\"[{','.join(str_results)}]\")\n\nsolve()\n```"
        }
    ]
}