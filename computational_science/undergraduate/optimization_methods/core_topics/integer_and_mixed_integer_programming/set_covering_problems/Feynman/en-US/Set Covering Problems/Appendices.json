{
    "hands_on_practices": [
        {
            "introduction": "The first and most critical skill in optimization is translating a real-world challenge into a precise mathematical language. This practice uses the intuitive scenario of sensor placement to guide you through the formulation of a set cover problem as a Mixed-Integer Program (MIP). By working through this exercise , you will not only master the art of defining variables, objectives, and constraints but also explore the powerful techniques of LP relaxation and valid inequalities, which are fundamental to solving complex optimization problems.",
            "id": "3130513",
            "problem": "A city must deploy a minimum-cost set of environmental sensors to ensure that every critical site is covered by at least one sensor. A sensor placed at a candidate location covers the subset of sites within its sensing radius. Formulate this decision as a set cover problem using the following instance. There are $3$ sites, indexed by $i \\in \\{1,2,3\\}$, and $3$ candidate sensor locations, indexed by $j \\in \\{a,b,c\\}$. The site coverage sets are:\n- Sensor $a$ covers sites $\\{1,2\\}$,\n- Sensor $b$ covers sites $\\{2,3\\}$,\n- Sensor $c$ covers sites $\\{1,3\\}$.\nAll sensors have unit cost, so $c_{a} = c_{b} = c_{c} = 1$. Define a binary decision variable $x_{j}$ indicating whether a sensor is placed at candidate location $j$.\n\nStarting from the formal definition of the set cover problem and the standard elements of Mixed-Integer Programming (MIP), complete the following tasks:\n\n1. Write a complete MIP model for minimizing total cost subject to coverage constraints for this instance, including precise definitions of variables, objective function, and constraints. Use binary variables $x_{j} \\in \\{0,1\\}$.\n\n2. Propose a Linear Programming (LP) relaxation by replacing the integrality constraints with continuous bounds, and then derive a strengthening valid inequality using a capacity-based argument on a subset of sites. State the general form of this inequality and then instantiate it for the given instance to obtain a concrete inequality in terms of $x_{a}$, $x_{b}$, and $x_{c}$.\n\n3. Using the LP relaxation together with your derived valid inequality, compute the optimal objective value of the strengthened LP for this instance. Round your final numerical answer to four significant figures.",
            "solution": "**1. Complete MIP Model**\n\nThe goal is to formulate a Mixed-Integer Programming (MIP) model. This requires defining the decision variables, the objective function, and the constraints.\n\n**Decision Variables:**\nAs defined in the problem, we use binary decision variables $x_j$ for each candidate sensor location $j \\in \\{a, b, c\\}$.\n$x_j = \\begin{cases} 1 & \\text{if a sensor is placed at location } j \\\\ 0 & \\text{otherwise} \\end{cases}$\nThe variables are thus $x_a$, $x_b$, and $x_c$, and they are restricted to be in $\\{0, 1\\}$.\n\n**Objective Function:**\nThe objective is to minimize the total cost of the deployed sensors. The total cost is the sum of the costs of the selected sensors.\nThe cost function is:\n$$ \\text{Minimize } \\sum_{j \\in \\{a,b,c\\}} c_j x_j $$\nGiven that $c_a = c_b = c_c = 1$, the objective function simplifies to:\n$$ \\text{Minimize } Z = x_a + x_b + x_c $$\n\n**Constraints:**\nThe core requirement is that every critical site must be covered by at least one sensor. This translates to a set of constraints, one for each site. A site $i$ is covered if the sum of the decision variables for sensors that cover site $i$ is at least $1$.\n\n-   **Constraint for Site 1:** Site $1$ is covered by sensors $a$ and $c$. Thus, at least one of these must be deployed.\n    $$ x_a + x_c \\ge 1 $$\n-   **Constraint for Site 2:** Site $2$ is covered by sensors $a$ and $b$.\n    $$ x_a + x_b \\ge 1 $$\n-   **Constraint for Site 3:** Site $3$ is covered by sensors $b$ and $c$.\n    $$ x_b + x_c \\ge 1 $$\n\n**Complete MIP Model:**\nThe complete MIP model for this set cover instance is:\n$$ \\begin{aligned} \\text{Minimize } \\quad & Z = x_a + x_b + x_c \\\\ \\text{Subject to } \\quad & x_a + x_c \\ge 1 \\\\ & x_a + x_b \\ge 1 \\\\ & x_b + x_c \\ge 1 \\\\ & x_a, x_b, x_c \\in \\{0, 1\\} \\end{aligned} $$\n\n**2. LP Relaxation and a Strengthening Valid Inequality**\n\n**Linear Programming (LP) Relaxation:**\nThe LP relaxation is obtained by relaxing the integrality constraint on the decision variables, i.e., replacing $x_j \\in \\{0, 1\\}$ with $0 \\le x_j \\le 1$.\n\nThe LP relaxation is:\n$$ \\begin{aligned} \\text{Minimize } \\quad & Z_{LP} = x_a + x_b + x_c \\\\ \\text{Subject to } \\quad & x_a + x_c \\ge 1 \\\\ & x_a + x_b \\ge 1 \\\\ & x_b + x_c \\ge 1 \\\\ & 0 \\le x_a, x_b, x_c \\le 1 \\end{aligned} $$\nBy inspection, a feasible and optimal solution to this LP is $x_a = x_b = x_c = 0.5$, which yields an objective value of $Z_{LP} = 0.5 + 0.5 + 0.5 = 1.5$. This solution is fractional and therefore not a valid solution to the original MIP.\n\n**Strengthening Valid Inequality:**\nA valid inequality is an inequality that is satisfied by all integer feasible solutions. A strengthening valid inequality is one that cuts off some fractional solutions of the LP relaxation. We can derive such an inequality by summing the three coverage constraints:\n$$ (x_a + x_c) + (x_a + x_b) + (x_b + x_c) \\ge 1 + 1 + 1 $$\n$$ 2x_a + 2x_b + 2x_c \\ge 3 $$\n$$ x_a + x_b + x_c \\ge 1.5 $$\nSince the total number of sensors, $x_a + x_b + x_c$, must be an integer in any valid solution to the original MIP, we can strengthen this inequality by rounding up the right-hand side:\n$$ x_a + x_b + x_c \\ge \\lceil 1.5 \\rceil = 2 $$\nThis inequality, $x_a + x_b + x_c \\ge 2$, is a strengthening valid inequality (often called a \"cut\") because it is satisfied by any valid integer solution but is violated by the fractional LP optimal solution where $x_a + x_b + x_c = 1.5$.\n\n**3. Optimal Objective Value of the Strengthened LP**\n\nWe add the derived valid inequality to the LP relaxation to form a strengthened LP:\n$$ \\begin{aligned} \\text{Minimize } \\quad & Z_{SLP} = x_a + x_b + x_c \\\\ \\text{Subject to } \\quad & x_a + x_c \\ge 1 \\\\ & x_a + x_b \\ge 1 \\\\ & x_b + x_c \\ge 1 \\\\ & x_a + x_b + x_c \\ge 2 \\\\ & 0 \\le x_a, x_b, x_c \\le 1 \\end{aligned} $$\nThe objective function is to minimize $Z_{SLP} = x_a + x_b + x_c$. The fourth constraint, $x_a + x_b + x_c \\ge 2$, directly provides a lower bound on the objective value: $Z_{SLP} \\ge 2$.\n\nTo determine if this lower bound can be achieved, we need to find if there exists a feasible solution $(x_a, x_b, x_c)$ that satisfies all constraints and achieves an objective value of $2$. Let's test if there is a feasible point on the plane $x_a + x_b + x_c = 2$.\n\nConsider the point $(x_a, x_b, x_c) = (1, 1, 0)$.\nThe objective value is $1 + 1 + 0 = 2$.\nNow, we check if this point is feasible by verifying all constraints:\n1.  $x_a + x_c = 1 + 0 = 1 \\ge 1$ (satisfied)\n2.  $x_a + x_b = 1 + 1 = 2 \\ge 1$ (satisfied)\n3.  $x_b + x_c = 1 + 0 = 1 \\ge 1$ (satisfied)\n4.  $x_a + x_b + x_c = 1 + 1 + 0 = 2 \\ge 2$ (satisfied)\n5.  $0 \\le 1 \\le 1$, $0 \\le 1 \\le 1$, $0 \\le 0 \\le 1$ (all satisfied)\n\nSince the point $(1, 1, 0)$ is feasible and its objective value $2$ matches the lower bound for the objective function, it is an optimal solution. Therefore, the optimal objective value of the strengthened LP is $2$.\nBy symmetry, the points $(1, 0, 1)$ and $(0, 1, 1)$ are also optimal solutions, yielding the same objective value.\n\nThe optimal objective value of the strengthened LP is exactly $2$. Expressed to four significant figures, this is $2.000$.",
            "answer": "$$\n\\boxed{2.000}\n$$"
        },
        {
            "introduction": "Since the set covering problem is NP-hard, exact solutions are often computationally infeasible for large-scale instances, making heuristics essential tools. This exercise challenges you to critically analyze the performance of the standard greedy algorithm, a natural and popular heuristic approach. By constructing a specific instance and comparing the greedy solution to the true optimum , you will gain a deeper insight into the algorithm's mechanics and its potential pitfalls, learning to quantify its performance and understand the trade-off between speed and solution quality.",
            "id": "3180742",
            "problem": "A set covering problem is defined by a ground set $U$, a collection of subsets $\\{S_i\\}$ of $U$, and a nonnegative cost vector $c \\in \\mathbb{R}^{m}$, where $m$ is the number of sets. The decision variables $x_i \\in \\{0,1\\}$ indicate whether set $S_i$ is selected. Let $A = (a_{ij})$ be the binary incidence matrix with $a_{ij} = 1$ if element $j \\in U$ is contained in set $S_i$, and $a_{ij} = 0$ otherwise. The set covering problem seeks to minimize $\\sum_{i=1}^{m} c_i x_i$ subject to $\\sum_{i=1}^{m} a_{ij} x_i \\geq 1$ for all $j \\in U$. Consider the following instance with $U = \\{1,2,3,4,5,6,7,8,h_1,h_2\\}$ and five sets indexed $i \\in \\{0,1,2,3,4\\}$ with costs $c_0 = 10$, $c_1 = 9$, $c_2 = 9$, $c_3 = 6$, and $c_4 = 6$. The incidence matrix $A$ (rows in the order $i=0,1,2,3,4$ and columns in the order $1,2,3,4,5,6,7,8,h_1,h_2$) is\n$$\nA =\n\\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 \\\\\n1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n\\end{pmatrix}.\n$$\nThus, $S_0$ covers $\\{1,2,3,4,5,6,7,8\\}$, $S_1$ covers $\\{1,2,3,4,h_1,h_2\\}$, $S_2$ covers $\\{5,6,7,8,h_1,h_2\\}$, $S_3$ covers $\\{h_1\\}$, and $S_4$ covers $\\{h_2\\}$. Consider the standard greedy heuristic that iteratively selects the set $i$ that minimizes the ratio $c_i / |\\{j \\in U : a_{ij} = 1 \\text{ and } \\sum_{k} a_{kj} x_k = 0\\}|$, i.e., the cost divided by the number of newly covered elements, until all elements of $U$ are covered. Define the redundancy of a solution $x$ as\n$$\nR(x) \\;=\\; \\sum_{j \\in U} \\left( \\sum_{i=0}^{4} a_{ij} x_i \\;-\\; 1 \\right),\n$$\nwhich sums, over all elements, the number of coverings in excess of one. Let $x^{\\text{greedy}}$ be the solution produced by the greedy heuristic and let $x^{\\text{OPT}}$ be an optimal solution minimizing total cost over the given sets. Compute the ratio\n$$\n\\mathcal{Q} \\;=\\; \\frac{R\\!\\left(x^{\\text{greedy}}\\right)}{R\\!\\left(x^{\\text{OPT}}\\right)}.\n$$\nProvide your final answer as an exact value with no rounding.",
            "solution": "We start from the core definition of the set covering problem: select sets to cover each element in $U$ at least once while minimizing total cost. The greedy heuristic is defined by the rule that at each iteration it selects the set that minimizes the ratio of its cost to the number of newly covered elements (elements not yet covered at that point).\n\nStep 1: Determine the greedy selection sequence.\n\nInitially, all $10$ elements are uncovered. Compute the initial ratios:\n- For $S_0$: it covers $8$ newly (elements $1,2,3,4,5,6,7,8$), so the ratio is $10 / 8 = 1.25$.\n- For $S_1$: it covers $6$ newly (elements $1,2,3,4,h_1,h_2$), so the ratio is $9 / 6 = 1.5$.\n- For $S_2$: it covers $6$ newly (elements $5,6,7,8,h_1,h_2$), so the ratio is $9 / 6 = 1.5$.\n- For $S_3$: it covers $1$ newly ($h_1$), so the ratio is $6 / 1 = 6$.\n- For $S_4$: it covers $1$ newly ($h_2$), so the ratio is $6 / 1 = 6$.\n\nThe greedy rule selects $S_0$ first because $1.25$ is the smallest ratio. After selecting $S_0$, the covered elements are $\\{1,2,3,4,5,6,7,8\\}$, and the uncovered elements are $\\{h_1,h_2\\}$.\n\nNext, we recompute ratios with respect to the uncovered elements $\\{h_1,h_2\\}$:\n- $S_1$ now adds $2$ newly (both $h_1$ and $h_2$), ratio $9 / 2 = 4.5$.\n- $S_2$ also adds $2$ newly, ratio $9 / 2 = 4.5$.\n- $S_3$ adds $1$ newly ($h_1$), ratio $6 / 1 = 6$.\n- $S_4$ adds $1$ newly ($h_2$), ratio $6 / 1 = 6$.\n\nThe minimal ratio is now $4.5$, corresponding to both $S_1$ and $S_2$. Without loss of generality, we select $S_1$. At this point, the remaining elements $\\{h_1,h_2\\}$ are covered. Since all elements in the universe $U$ are now covered, the heuristic terminates. The greedy solution is therefore the selection of sets $\\{S_0, S_1\\}$. This corresponds to the solution vector $x^{\\text{greedy}}$ where $x_0 = 1$, $x_1 = 1$, and $x_2 = x_3 = x_4 = 0$.\n\nStep 2: Determine an optimal solution $x^{\\text{OPT}}$ among the given sets.\n\nWe evaluate candidate covers:\n- $\\{S_0,S_1\\}$ covers all elements, with total cost $10 + 9 = 19$.\n- $\\{S_0,S_2\\}$ similarly covers all elements, cost $10 + 9 = 19$.\n- $\\{S_1,S_2\\}$ covers all elements (since $S_1$ covers $\\{1,2,3,4,h_1,h_2\\}$ and $S_2$ covers $\\{5,6,7,8,h_1,h_2\\}$), with total cost $9 + 9 = 18$.\n- $\\{S_0,S_3,S_4\\}$ covers all elements, cost $10 + 6 + 6 = 22$.\nOther combinations either fail to cover all elements or are more expensive.\n\nThus, the optimal solution is $x^{\\text{OPT}}$ with $x_1 = 1$, $x_2 = 1$, and $x_0 = x_3 = x_4 = 0$, with minimum total cost $18$.\n\nStep 3: Compute the redundancy $R(x)$ for both solutions.\n\nBy definition,\n$$\nR(x) = \\sum_{j \\in U} \\left( \\sum_{i=0}^{4} a_{ij} x_i - 1 \\right),\n$$\nwhich counts, over all elements, the excess coverage beyond one.\n\nCompute $R\\!\\left(x^{\\text{greedy}}\\right)$ for $x^{\\text{greedy}}$ with sets $\\{S_0,S_1\\}$:\n- Elements $1,2,3,4$: covered by both $S_0$ and $S_1$, so coverage multiplicity is $2$ for each, contributing $2 - 1 = 1$ each, total $4$.\n- Elements $5,6,7,8$: covered only by $S_0$, multiplicity $1$, contributing $0$ each, total $0$.\n- Element $h_1$: covered only by $S_1$, multiplicity $1$, contributing $0$.\n- Element $h_2$: covered only by $S_1$, multiplicity $1$, contributing $0$.\n\nTherefore,\n$$\nR\\!\\left(x^{\\text{greedy}}\\right) = 4 + 0 + 0 + 0 = 4.\n$$\n\nCompute $R\\!\\left(x^{\\text{OPT}}\\right)$ for $x^{\\text{OPT}}$ with sets $\\{S_1,S_2\\}$:\n- Elements $1,2,3,4$: covered only by $S_1$, multiplicity $1$, contributing $0$ each, total $0$.\n- Elements $5,6,7,8$: covered only by $S_2$, multiplicity $1$, contributing $0$ each, total $0$.\n- Element $h_1$: covered by both $S_1$ and $S_2$, multiplicity $2$, contributing $1$.\n- Element $h_2$: covered by both $S_1$ and $S_2$, multiplicity $2$, contributing $1$.\n\nTherefore,\n$$\nR\\!\\left(x^{\\text{OPT}}\\right) = 0 + 0 + 1 + 1 = 2.\n$$\n\nStep 4: Compute the requested ratio $\\mathcal{Q}$.\n\nBy definition,\n$$\n\\mathcal{Q} = \\frac{R\\!\\left(x^{\\text{greedy}}\\right)}{R\\!\\left(x^{\\text{OPT}}\\right)} = \\frac{4}{2} = 2.\n$$\n\nThus, the redundancy incurred by the greedy solution is exactly twice that of the optimal solution for this instance.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "To move beyond simple heuristics, approximation algorithms offer solutions with provable guarantees on their quality, and the primal-dual method is a cornerstone for designing them. This practice provides a hands-on walk-through of a primal-dual algorithm for the weighted set cover problem, revealing the elegant theory that powers it. By tracing the algorithm's steps , you will see how dual variables are systematically increased and how complementary slackness conditions are used to select sets, giving you a concrete understanding of the profound relationship between a problem and its dual.",
            "id": "3180697",
            "problem": "A weighted set cover instance is defined by a finite universe $U$, a family of subsets $\\{S_{i}\\}_{i=1}^{m}$ with costs $\\{c_{i}\\}_{i=1}^{m}$, and decision variables $\\{x_{i}\\}_{i=1}^{m}$ that indicate whether a subset is chosen. The standard Linear Programming (LP) relaxation and its dual for the weighted set cover problem are as follows. The primal is\n$$\n\\min \\sum_{i=1}^{m} c_{i} x_{i} \\quad \\text{subject to} \\quad \\sum_{i : j \\in S_{i}} x_{i} \\geq 1 \\ \\text{for all} \\ j \\in U, \\quad x_{i} \\geq 0 \\ \\text{for all} \\ i,\n$$\nand the dual is\n$$\n\\max \\sum_{j \\in U} y_{j} \\quad \\text{subject to} \\quad \\sum_{j \\in S_{i}} y_{j} \\leq c_{i} \\ \\text{for all} \\ i, \\quad y_{j} \\geq 0 \\ \\text{for all} \\ j.\n$$\nComplementary slackness for a pair of primal and dual feasible solutions $(x,y)$ requires that for any $i$, if $x_{i} > 0$ then $\\sum_{j \\in S_{i}} y_{j} = c_{i}$, and for any $j$, if $y_{j} > 0$ then $\\sum_{i : j \\in S_{i}} x_{i} = 1$.\n\nConsider the following instance designed to test how the primal-dual method leverages complementary slackness to select sets exactly when dual constraints saturate. Let the universe be $U = \\{1,2,3\\}$. The family of sets and their costs are:\n- $S_{1} = \\{1,2\\}$ with cost $c_{1} = 3$,\n- $S_{2} = \\{2,3\\}$ with cost $c_{2} = 3$,\n- $S_{3} = \\{1,3\\}$ with cost $c_{3} = 2$.\n\nStarting from the primal-dual framework and the dual feasibility constraints, conceptually perform uniform dual ascent on the uncovered elements: increase $y_{j}$ at the same constant rate for each uncovered $j \\in U$, and whenever a set $S_{i}$ satisfies $\\sum_{j \\in S_{i}} y_{j} = c_{i}$ and covers at least one uncovered element, select it into the cover. If multiple sets become tight simultaneously while covering uncovered elements, break ties by choosing the lowest index. Stop when all elements of $U$ are covered.\n\nDerive the final dual weights $y_{1}$, $y_{2}$, and $y_{3}$ produced by this process for the above instance, and verify that for each selected set $S_{i}$ the equality $\\sum_{j \\in S_{i}} y_{j} = c_{i}$ holds exactly. Report your final numerical answer as the row vector $(y_{1}, y_{2}, y_{3})$. No rounding is required.",
            "solution": "The problem requires the execution of a specific primal-dual algorithm for a given instance of the weighted set cover problem. The core of the algorithm is a dual ascent procedure, where dual variables corresponding to uncovered elements are increased until a set's dual constraint becomes tight.\n\nThe given instance is:\n- Universe $U = \\{1, 2, 3\\}$.\n- Subsets and costs:\n  - $S_1 = \\{1, 2\\}$, $c_1 = 3$.\n  - $S_2 = \\{2, 3\\}$, $c_2 = 3$.\n  - $S_3 = \\{1, 3\\}$, $c_3 = 2$.\n\nThe dual constraints are:\n1.  For $S_1$: $y_1 + y_2 \\leq c_1 = 3$.\n2.  For $S_2$: $y_2 + y_3 \\leq c_2 = 3$.\n3.  For $S_3$: $y_1 + y_3 \\leq c_3 = 2$.\n4.  Non-negativity: $y_j \\geq 0$ for $j \\in \\{1, 2, 3\\}$.\n\nWe will trace the algorithm's execution step by step.\n\n**Initialization:**\nInitially, no sets are chosen for the cover. The set of uncovered elements is the entire universe, $U' = \\{1, 2, 3\\}$. All dual variables are initialized to zero: $y_1 = 0$, $y_2 = 0$, and $y_3 = 0$.\n\n**Phase 1: Dual Ascent on $\\{1, 2, 3\\}$**\nThe algorithm increases the dual variables $y_1$, $y_2$, and $y_3$ at a uniform rate, as all corresponding elements are uncovered. We can represent this by defining $y_1 = y_2 = y_3 = t$ for some non-negative parameter $t$ that we increase from $0$. We monitor the dual constraints to find the smallest value of $t > 0$ for which a constraint becomes tight.\n- For $S_1$: $y_1 + y_2 = t + t = 2t$. The constraint $2t \\leq 3$ becomes tight at $t = \\frac{3}{2} = 1.5$.\n- For $S_2$: $y_2 + y_3 = t + t = 2t$. The constraint $2t \\leq 3$ becomes tight at $t = \\frac{3}{2} = 1.5$.\n- For $S_3$: $y_1 + y_3 = t + t = 2t$. The constraint $2t \\leq 2$ becomes tight at $t = 1$.\n\nThe first constraint to become tight is for $S_3$ at $t=1$. At this moment, we take the following actions:\n1.  Freeze the dual variables at their current values. The increase of $t=1$ is applied to all duals, so we have $(y_1, y_2, y_3) = (1, 1, 1)$.\n2.  Since the now-tight set $S_3$ covers elements $\\{1, 3\\}$, both of which are currently uncovered, we add $S_3$ to our cover.\n3.  Update the set of uncovered elements by removing those covered by $S_3$: $U' = U' \\setminus S_3 = \\{1, 2, 3\\} \\setminus \\{1, 3\\} = \\{2\\}$.\n\nThe algorithm now proceeds to the next phase with an updated state.\n\n**Phase 2: Dual Ascent on $\\{2\\}$**\nThe current state is: duals $(y_1, y_2, y_3) = (1, 1, 1)$, and the only uncovered element is $2$.\nIn this phase, we only increase the dual variable corresponding to the uncovered element, which is $y_2$. The other dual variables, $y_1$ and $y_3$, remain fixed at their current value of $1$. Let the increment to $y_2$ be $\\Delta t \\ge 0$. The duals are now $y_1 = 1$, $y_2 = 1 + \\Delta t$, and $y_3 = 1$.\nWe check the dual constraints for sets that contain the uncovered element $2$, which are $S_1$ and $S_2$.\n- For $S_1$: The sum of duals is $y_1 + y_2 = 1 + (1 + \\Delta t) = 2 + \\Delta t$. The constraint $2 + \\Delta t \\leq 3$ becomes tight when $\\Delta t = 1$.\n- For $S_2$: The sum of duals is $y_2 + y_3 = (1 + \\Delta t) + 1 = 2 + \\Delta t$. The constraint $2 + \\Delta t \\leq 3$ becomes tight when $\\Delta t = 1$.\n\nBoth $S_1$ and $S_2$ become tight simultaneously at an increment of $\\Delta t=1$. The problem provides a tie-breaking rule: choose the set with the lowest index. Between $S_1$ and $S_2$, we choose $S_1$.\n1.  We apply the increment $\\Delta t = 1$ to $y_2$. The final dual variable values are:\n    - $y_1 = 1$ (unchanged).\n    - $y_2 = 1 + \\Delta t = 1 + 1 = 2$.\n    - $y_3 = 1$ (unchanged).\n    So, the final dual vector is $(y_1, y_2, y_3) = (1, 2, 1)$.\n2.  We add $S_1$ to our cover.\n3.  Update the set of uncovered elements: $U' = U' \\setminus S_1 = \\{2\\} \\setminus \\{1, 2\\} = \\emptyset$.\n\nSince the set of uncovered elements is now empty, the algorithm terminates.\n\n**Final Verification:**\nThe algorithm selected sets $S_3$ and $S_1$. The final dual weights are $y_1=1$, $y_2=2$, and $y_3=1$. The problem asks to verify that for each selected set, its dual constraint is met with equality.\n- For selected set $S_1 = \\{1, 2\\}$: The dual sum is $y_1 + y_2 = 1 + 2 = 3$. This is equal to the cost $c_1=3$. The condition is satisfied.\n- For selected set $S_3 = \\{1, 3\\}$: The dual sum is $y_1 + y_3 = 1 + 1 = 2$. This is equal to the cost $c_3=2$. The condition is satisfied.\n\nThe verification is successful. The final dual weights derived from the process are $(y_1, y_2, y_3) = (1, 2, 1)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 2 & 1\n\\end{pmatrix}\n}\n$$"
        }
    ]
}