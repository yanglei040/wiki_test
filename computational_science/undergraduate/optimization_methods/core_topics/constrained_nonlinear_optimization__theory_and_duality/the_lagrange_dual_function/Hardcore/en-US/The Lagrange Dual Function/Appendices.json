{
    "hands_on_practices": [
        {
            "introduction": "We begin with a classic geometric problem: finding the closest point in an affine subspace to an external point. This exercise  is a perfect first step into Lagrange duality, as it demonstrates the core mechanics of deriving the dual function for a quadratic program with only linear equality constraints. By working through this, you will practice forming the Lagrangian and minimizing it with respect to the primal variable to uncover the dual objective function.",
            "id": "2167450",
            "problem": "Consider the problem of finding the projection of a point onto an affine subspace. This is equivalent to finding the point $x$ in the subspace that is closest in Euclidean distance to a given external point $x_0$. The affine subspace is defined by the set of all points $x \\in \\mathbb{R}^n$ that satisfy the linear system $Ax = b$, where $A$ is a given $m \\times n$ matrix and $b$ is a given vector in $\\mathbb{R}^m$. The external point is $x_0 \\in \\mathbb{R}^n$.\n\nTo find this projection, we solve the following optimization problem:\n$$\n\\begin{array}{ll}\n\\text{minimize}  \\frac{1}{2}\\|x - x_0\\|_2^2 \\\\\n\\text{subject to}  Ax = b\n\\end{array}\n$$\nwhere $\\| \\cdot \\|_2$ denotes the standard Euclidean norm.\n\nYour task is to find the objective function of the Lagrange dual problem. This dual objective function, denoted as $g(\\nu)$, is a function of the Lagrange multiplier vector $\\nu \\in \\mathbb{R}^m$ associated with the equality constraints. Express your answer as a symbolic expression in terms of $\\nu$, $A$, $b$, and $x_0$.",
            "solution": "We form the Lagrangian for the equality-constrained problem. With multiplier $\\nu \\in \\mathbb{R}^{m}$ for the constraint $Ax=b$, the Lagrangian is\n$$\nL(x,\\nu)=\\frac{1}{2}\\|x-x_{0}\\|_{2}^{2}+\\nu^{T}(Ax-b).\n$$\nTo compute the dual function $g(\\nu)=\\inf_{x}L(x,\\nu)$, we minimize $L$ over $x$. The stationarity condition is obtained by setting the gradient with respect to $x$ to zero:\n$$\n\\nabla_{x}L(x,\\nu)=(x-x_{0})+A^{T}\\nu=0 \\quad \\Longrightarrow \\quad x^{\\star}(\\nu)=x_{0}-A^{T}\\nu.\n$$\nSubstituting $x^{\\star}(\\nu)$ back into $L$ gives\n$$\ng(\\nu)=L(x^{\\star}(\\nu),\\nu)=\\frac{1}{2}\\|x_{0}-A^{T}\\nu - x_{0}\\|_{2}^{2}+\\nu^{T}\\big(A(x_{0}-A^{T}\\nu)-b\\big).\n$$\nSimplifying term by term,\n$$\n\\frac{1}{2}\\|x_{0}-A^{T}\\nu - x_{0}\\|_{2}^{2}=\\frac{1}{2}\\|A^{T}\\nu\\|_{2}^{2}=\\frac{1}{2}\\nu^{T}AA^{T}\\nu,\n$$\nand\n$$\n\\nu^{T}\\big(A(x_{0}-A^{T}\\nu)-b\\big)=\\nu^{T}Ax_{0}-\\nu^{T}AA^{T}\\nu-\\nu^{T}b.\n$$\nTherefore,\n$$\ng(\\nu)=\\frac{1}{2}\\nu^{T}AA^{T}\\nu+\\nu^{T}Ax_{0}-\\nu^{T}AA^{T}\\nu-\\nu^{T}b\n=-\\frac{1}{2}\\nu^{T}AA^{T}\\nu+\\nu^{T}(Ax_{0}-b).\n$$\nEquivalently, $g(\\nu)=-\\frac{1}{2}\\|A^{T}\\nu\\|_{2}^{2}+(Ax_{0}-b)^{T}\\nu$.",
            "answer": "$$\\boxed{-\\frac{1}{2}\\nu^{T}AA^{T}\\nu+\\nu^{T}(Ax_{0}-b)}$$"
        },
        {
            "introduction": "Next, we advance to a problem involving both equality and inequality constraints, a common scenario in many fields like machine learning and statistics. Projecting a point onto the probability simplex is a fundamental task, and its dual formulation offers profound insights into the solution's structure. This practice  challenges you to handle inequality multipliers and reveals a beautiful connection between the optimal dual variable and the resulting thresholding form of the primal solution.",
            "id": "3191755",
            "problem": "Consider the Euclidean projection of a vector $y \\in \\mathbb{R}^{n}$ onto the probability simplex $\\Delta^{n} = \\{ x \\in \\mathbb{R}^{n} : x \\geq 0, \\ \\mathbf{1}^{\\top} x = 1 \\}$, formulated as the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2} \\| x - y \\|_{2}^{2} \\quad \\text{subject to} \\quad x \\geq 0, \\ \\mathbf{1}^{\\top} x = 1,\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^{n}$ denotes the all-ones vector. Starting from the definition of the Lagrangian and the Lagrange dual function, derive the dual function $g(\\nu)$ associated with the equality multiplier $\\nu \\in \\mathbb{R}$ by eliminating the inequality multipliers via maximization over them. Then, characterize how the optimal scalar $\\nu$ that maximizes $g(\\nu)$ induces a thresholding form for the primal optimizer $x^{\\star}$.\n\nYou must work from core definitions: the Lagrangian, the dual function (defined as the infimum of the Lagrangian over primal variables), and the Karush–Kuhn–Tucker (KKT) conditions (stationarity, primal feasibility, dual feasibility, and complementary slackness). Do not assume any pre-derived projection formula.\n\nYour final answer must be a single closed-form analytical expression for the dual function $g(\\nu)$ in terms of $\\nu$ and $y$. No numerical evaluation is required.",
            "solution": "We begin with the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2} \\| x - y \\|_{2}^{2} \\quad \\text{subject to} \\quad x \\geq 0, \\ \\mathbf{1}^{\\top} x = 1.\n$$\nIntroduce Lagrange multipliers $\\mu \\in \\mathbb{R}^{n}$ for the inequality constraints $x \\geq 0$ and $\\nu \\in \\mathbb{R}$ for the equality constraint $\\mathbf{1}^{\\top} x = 1$. The Lagrangian is\n$$\n\\mathcal{L}(x, \\mu, \\nu) = \\frac{1}{2} \\| x - y \\|_{2}^{2} - \\mu^{\\top} x + \\nu \\left( \\mathbf{1}^{\\top} x - 1 \\right),\n$$\nwith dual feasibility requiring $\\mu \\geq 0$. The Lagrange dual function $g(\\mu, \\nu)$ is defined as the infimum of the Lagrangian over the primal variable $x$:\n$$\ng(\\mu, \\nu) = \\inf_{x \\in \\mathbb{R}^{n}} \\ \\mathcal{L}(x, \\mu, \\nu).\n$$\nSince the objective is strictly convex in $x$, the infimum is attained at the unique stationary point where the gradient with respect to $x$ vanishes. Compute the stationarity condition:\n$$\n\\nabla_{x} \\mathcal{L}(x, \\mu, \\nu) = x - y - \\mu + \\nu \\mathbf{1} = 0 \\quad \\Rightarrow \\quad x^{\\star}(\\mu, \\nu) = y + \\mu - \\nu \\mathbf{1}.\n$$\nSubstitute $x^{\\star}(\\mu, \\nu)$ into the Lagrangian:\n\\begin{align*}\n\\mathcal{L}\\big(x^{\\star}(\\mu, \\nu), \\mu, \\nu\\big)\n= \\frac{1}{2} \\| x^{\\star}(\\mu, \\nu) - y \\|_{2}^{2} - \\mu^{\\top} x^{\\star}(\\mu, \\nu) + \\nu \\left( \\mathbf{1}^{\\top} x^{\\star}(\\mu, \\nu) - 1 \\right) \\\\\n= \\frac{1}{2} \\| \\mu - \\nu \\mathbf{1} \\|_{2}^{2} - \\mu^{\\top} \\big( y + \\mu - \\nu \\mathbf{1} \\big) + \\nu \\big( \\mathbf{1}^{\\top} y + \\mathbf{1}^{\\top} \\mu - n \\nu - 1 \\big).\n\\end{align*}\nExpanding and simplifying term-by-term,\n\\begin{align*}\n\\frac{1}{2} \\| \\mu - \\nu \\mathbf{1} \\|_{2}^{2} = \\frac{1}{2} \\left( \\| \\mu \\|_{2}^{2} - 2 \\nu \\mathbf{1}^{\\top} \\mu + n \\nu^{2} \\right), \\\\\n- \\mu^{\\top} ( y + \\mu - \\nu \\mathbf{1} ) = - \\mu^{\\top} y - \\| \\mu \\|_{2}^{2} + \\nu \\mathbf{1}^{\\top} \\mu, \\\\\n\\nu ( \\mathbf{1}^{\\top} y + \\mathbf{1}^{\\top} \\mu - n \\nu - 1 ) = \\nu \\mathbf{1}^{\\top} y + \\nu \\mathbf{1}^{\\top} \\mu - n \\nu^{2} - \\nu.\n\\end{align*}\nTherefore,\n\\begin{align*}\ng(\\mu, \\nu)\n= \\left[ \\frac{1}{2} \\| \\mu \\|_{2}^{2} - \\nu \\mathbf{1}^{\\top} \\mu + \\frac{1}{2} n \\nu^{2} \\right]\n   + \\left[ - \\mu^{\\top} y - \\| \\mu \\|_{2}^{2} + \\nu \\mathbf{1}^{\\top} \\mu \\right]\n   + \\left[ \\nu \\mathbf{1}^{\\top} y + \\nu \\mathbf{1}^{\\top} \\mu - n \\nu^{2} - \\nu \\right] \\\\\n= - \\frac{1}{2} \\| \\mu \\|_{2}^{2} - \\mu^{\\top} y + \\nu \\mathbf{1}^{\\top} y + \\nu \\mathbf{1}^{\\top} \\mu - \\frac{1}{2} n \\nu^{2} - \\nu.\n\\end{align*}\nIt is convenient to collect terms by components; note that the dependence on $\\mu_{i}$ is separable:\n$$\ng(\\mu, \\nu) = \\sum_{i=1}^{n} \\left( - \\frac{1}{2} \\mu_{i}^{2} - \\mu_{i} y_{i} + \\nu \\mu_{i} \\right) + \\nu \\left( \\sum_{i=1}^{n} y_{i} - 1 \\right) - \\frac{1}{2} n \\nu^{2}.\n$$\nFor fixed $\\nu$, we compute the supremum of $g(\\mu, \\nu)$ over $\\mu \\geq 0$ componentwise. Consider, for each index $i$, the concave quadratic in $\\mu_{i}$:\n$$\n\\phi_{i}(\\mu_{i}; \\nu) = - \\frac{1}{2} \\mu_{i}^{2} - \\mu_{i} ( y_{i} - \\nu ), \\quad \\text{subject to} \\quad \\mu_{i} \\geq 0.\n$$\nThe unconstrained maximizer satisfies $\\frac{\\partial \\phi_{i}}{\\partial \\mu_{i}} = - \\mu_{i} - ( y_{i} - \\nu ) = 0$, hence $\\mu_{i}^{\\star} = - ( y_{i} - \\nu )$. Enforcing $\\mu_{i} \\geq 0$ yields\n$$\n\\mu_{i}^{\\star}(\\nu) = \\max\\{ 0, \\ \\nu - y_{i} \\}.\n$$\nSubstituting $\\mu_{i}^{\\star}$ back into $\\phi_{i}$ gives\n$$\n\\sup_{\\mu_{i} \\geq 0} \\ \\phi_{i}(\\mu_{i}; \\nu)\n= \\begin{cases}\n\\frac{1}{2} ( \\nu - y_{i} )^{2},  \\text{if } \\nu \\geq y_{i}, \\\\\n0,  \\text{if } \\nu  y_{i},\n\\end{cases}\n= \\frac{1}{2} \\big( \\nu - y_{i} \\big)_{+}^{2},\n$$\nwhere $(a)_{+} = \\max\\{ a, 0 \\}$. Therefore, the dual function after maximizing over $\\mu \\geq 0$ reduces to the scalar function\n$$\ng(\\nu) = \\sum_{i=1}^{n} \\frac{1}{2} \\big( \\nu - y_{i} \\big)_{+}^{2} + \\nu \\left( \\sum_{i=1}^{n} y_{i} - 1 \\right) - \\frac{1}{2} n \\nu^{2}.\n$$\nThis $g(\\nu)$ is concave in $\\nu$ because it is a sum of concave functions (each piecewise quadratic-concave) minus a positive quadratic term.\n\nWe now use the Karush–Kuhn–Tucker (KKT) conditions to extract the primal optimizer $x^{\\star}$ in terms of the $\\nu$ that maximizes $g(\\nu)$. From stationarity,\n$$\nx^{\\star} = y + \\mu^{\\star} - \\nu^{\\star} \\mathbf{1},\n$$\nand from the expression for $\\mu^{\\star}$ we have componentwise\n$$\nx_{i}^{\\star} = y_{i} + \\max\\{ 0, \\ \\nu^{\\star} - y_{i} \\} - \\nu^{\\star} = \\max\\{ y_{i} - \\nu^{\\star}, \\ 0 \\}.\n$$\nThus $x^{\\star} = \\big( y - \\nu^{\\star} \\mathbf{1} \\big)_{+}$, which is a thresholding rule parameterized by $\\nu^{\\star}$. Enforcing the equality constraint $\\mathbf{1}^{\\top} x^{\\star} = 1$ yields the scalar equation\n$$\n\\sum_{i=1}^{n} \\max\\{ y_{i} - \\nu^{\\star}, \\ 0 \\} = 1,\n$$\nwhich determines $\\nu^{\\star}$ uniquely in the generic case (strict convexity of the objective guarantees a unique $x^{\\star}$; when multiple indices tie at the threshold, any corresponding $\\nu^{\\star}$ satisfying the equation produces the same $x^{\\star}$). In summary, the dual function is the scalar concave function $g(\\nu)$ above, and the primal solution is a thresholded vector with threshold $\\nu^{\\star}$ that makes the components sum to one.\n\nThe required final answer is the analytical expression of $g(\\nu)$.",
            "answer": "$$\\boxed{g(\\nu)=\\sum_{i=1}^{n}\\frac{1}{2}\\big(\\nu-y_{i}\\big)_{+}^{2}+\\nu\\left(\\sum_{i=1}^{n}y_{i}-1\\right)-\\frac{1}{2}n\\,\\nu^{2}}$$"
        },
        {
            "introduction": "Sometimes the most important concepts are revealed in the simplest settings. This final exercise  uses a basic one-dimensional convex problem to explore the subtle relationship between the primal and dual problems, specifically concerning the attainment of their optimal values. You will investigate a case where strong duality holds (the optimal values are equal), yet the primal infimum is not actually achieved by any feasible point, providing a key insight into the interpretation of dual solutions.",
            "id": "3191751",
            "problem": "Consider the convex optimization problem in one dimension\n$$\n\\begin{aligned}\n\\text{minimize}_{x \\in \\mathbb{R}} \\quad  f(x) \\equiv \\exp(x) \\\\\n\\text{subject to} \\quad  x \\leq 0.\n\\end{aligned}\n$$\nWork from first principles using the definitions of the Lagrangian and the Lagrange dual function.\n\nTasks:\n- Using only the definition of the Lagrangian for inequality constraints, define the Lagrangian and derive explicitly the Lagrange dual function $g(\\lambda)$ for $\\lambda \\geq 0$. Identify all values of $\\lambda$ for which $g(\\lambda)$ is finite.\n- Formulate the dual problem and determine whether a dual maximizer exists. If it does, identify it.\n- Analyze the primal problem to determine its infimum $p^{\\star}$ and whether the infimum is attained by any feasible $x$. Justify your conclusions directly from the properties of $f(x)$ and the feasible set.\n- State the dual optimal value $d^{\\star}$, compare it with $p^{\\star}$, and comment on the duality gap and on attainment in both primal and dual.\n\nAs your final answer, report the dual optimal value $d^{\\star}$ as a single real number. No rounding is required.",
            "solution": "The optimization problem can be written in the standard form as:\n$$\n\\begin{aligned}\n\\text{minimize}_{x \\in \\mathbb{R}} \\quad  f_0(x) \\\\\n\\text{subject to} \\quad  f_1(x) \\leq 0,\n\\end{aligned}\n$$\nwhere the objective function is $f_0(x) = \\exp(x)$ and the inequality constraint function is $f_1(x) = x$.\n\nFirst, we define the Lagrangian and derive the Lagrange dual function. The Lagrangian $L(x, \\lambda)$ is defined as $L(x, \\lambda) = f_0(x) + \\lambda f_1(x)$, where $\\lambda$ is the Lagrange multiplier associated with the inequality constraint. For an inequality constraint of the form $f_1(x) \\leq 0$, the multiplier must be non-negative, i.e., $\\lambda \\geq 0$.\n$$\nL(x, \\lambda) = \\exp(x) + \\lambda x\n$$\nThe Lagrange dual function, $g(\\lambda)$, is defined as the infimum of the Lagrangian over the primal variable $x$:\n$$\ng(\\lambda) = \\inf_{x \\in \\mathbb{R}} L(x, \\lambda) = \\inf_{x \\in \\mathbb{R}} (\\exp(x) + \\lambda x)\n$$\nTo find this infimum, we analyze the function $h(x) = \\exp(x) + \\lambda x$ for a fixed $\\lambda \\geq 0$. We consider two cases for $\\lambda$:\nCase 1: $\\lambda > 0$. To find the infimum of $h(x)$, we examine its derivative with respect to $x$:\n$$\n\\frac{\\partial L}{\\partial x} = \\exp(x) + \\lambda\n$$\nSince $\\exp(x) > 0$ for all $x \\in \\mathbb{R}$ and we assume $\\lambda > 0$, the derivative $\\frac{\\partial L}{\\partial x}$ is strictly positive. This implies that $L(x, \\lambda)$ is a strictly increasing function of $x$. The infimum is thus attained as $x$ approaches $-\\infty$:\n$$\ng(\\lambda) = \\lim_{x \\to -\\infty} (\\exp(x) + \\lambda x) = 0 + \\lambda(-\\infty) = -\\infty\n$$\nCase 2: $\\lambda = 0$. The Lagrangian becomes $L(x, 0) = \\exp(x)$. This is also a strictly increasing function of $x$. Its infimum is:\n$$\ng(0) = \\inf_{x \\in \\mathbb{R}} \\exp(x) = \\lim_{x \\to -\\infty} \\exp(x) = 0\n$$\nCombining these results, the Lagrange dual function for $\\lambda \\geq 0$ is:\n$$\ng(\\lambda) = \\begin{cases} 0  \\text{if } \\lambda=0 \\\\ -\\infty  \\text{if } \\lambda>0 \\end{cases}\n$$\nThe dual function $g(\\lambda)$ is finite only for the value $\\lambda = 0$.\n\nSecond, we formulate and solve the dual problem. The dual problem is to maximize the dual function $g(\\lambda)$ subject to the non-negativity constraint on the multiplier:\n$$\n\\begin{aligned}\n\\text{maximize}_{\\lambda} \\quad  g(\\lambda) \\\\\n\\text{subject to} \\quad  \\lambda \\geq 0.\n\\end{aligned}\n$$\nGiven the derived form of $g(\\lambda)$, we are maximizing over the set of values $\\{0, -\\infty, -\\infty, \\dots\\}$. The maximum value is clearly $0$, which is achieved at $\\lambda = 0$. Therefore, a dual maximizer exists and is unique: $\\lambda^{\\star} = 0$. The optimal value of the dual problem is $d^{\\star} = g(\\lambda^{\\star}) = g(0) = 0$.\n\nThird, we analyze the primal problem. We need to find the infimum of $f(x) = \\exp(x)$ over the feasible set $S = \\{x \\in \\mathbb{R} \\mid x \\leq 0\\}$, which is the interval $(-\\infty, 0]$. The objective function $f(x) = \\exp(x)$ is a strictly increasing function over its entire domain $\\mathbb{R}$. To minimize an increasing function, one must choose the smallest possible value for its argument from the feasible set. Since the feasible set $(-\\infty, 0]$ is unbounded below, the infimum is the limit of the function as $x$ approaches $-\\infty$.\n$$\np^{\\star} = \\inf_{x \\leq 0} \\exp(x) = \\lim_{x \\to -\\infty} \\exp(x) = 0\n$$\nThe primal infimum (or optimal value) is $p^{\\star} = 0$. For this value to be attained, there must exist a feasible point $x^{\\star} \\leq 0$ such that $f(x^{\\star}) = p^{\\star}$. This would require $\\exp(x^{\\star}) = 0$. However, the exponential function is strictly positive for all real $x$. Thus, there is no $x^{\\star}$ that satisfies this condition, and the primal infimum is not attained.\n\nFourth, we state the dual optimal value, compare it with the primal infimum, and comment on the duality gap and attainment.\nThe primal optimal value is $p^{\\star} = 0$.\nThe dual optimal value is $d^{\\star} = 0$.\nThe duality gap is $p^{\\star} - d^{\\star} = 0 - 0 = 0$. This indicates that strong duality holds for this problem. The holding of strong duality is guaranteed by Slater's condition, since the problem is convex and there exists a strictly feasible point (e.g., $x=-1$, for which $-1  0$).\nRegarding attainment, the dual optimal value $d^{\\star}=0$ is attained at the dual maximizer $\\lambda^{\\star}=0$. In contrast, the primal optimal value $p^{\\star}=0$ is not attained by any feasible $x$.",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}