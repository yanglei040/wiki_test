{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最好的检验方式就是亲自动手。本练习旨在通过一个精心设计的数值实验，让您直观地观察到马拉托斯效应。您将为一个带有非线性等式约束的优化问题实施一个基本的序列二次规划（SQP）算法，并观察在靠近最优解时，基于罚函数的线搜索为何会拒绝看似完美的单位步长（$\\alpha=1$）。这个实践将揭示理论与算法实际行为之间的微妙差异，并为理解更高级的优化策略奠定基础。",
            "id": "3147380",
            "problem": "考虑一个带等式约束的非线性优化问题，其目标函数为 $f(x) = x_1^2 + x_2^2$，非线性等式约束为 $c(x) = x_2 - \\sin x_1 = 0$。Karush-Kuhn-Tucker (KKT) 点是满足等式约束下最优性一阶必要条件的解 $x^\\star$。序列二次规划 (Sequential Quadratic Programming, SQP) 方法通过线性化约束并使用拉格朗日函数黑塞矩阵的对称正定近似来构造一个二次子问题。得到的步长 $p_k$ 随后用于带优值函数的线搜索，以确保迭代进程并处理约束。在 $x^\\star$ 附近，Maratos 效应描述了这样一种现象：尽管完整的 SQP 步长 $p_k$ 是一个有效的类牛顿步，若被接受将能实现快速的局部收敛，但由于二阶约束曲率，它仍会被基于优值函数的线搜索所拒绝。\n\n从基本定义开始：\n- 目标梯度为 $\\nabla f(x) = \\begin{bmatrix}2 x_1 \\\\ 2 x_2\\end{bmatrix}$。\n- 约束雅可比矩阵为 $\\nabla c(x) = \\begin{bmatrix}-\\cos x_1  1\\end{bmatrix}$。\n- 精确惩罚优值函数为 $\\phi_\\sigma(x) = f(x) + \\sigma \\lvert c(x) \\rvert$，其中 $\\sigma  0$ 是惩罚参数。\n- Armijo 充分下降条件要求 $\\phi_\\sigma(x + \\alpha p_k) \\le \\phi_\\sigma(x) + m \\alpha d_\\phi(x; p_k)$，其中 $m \\in (0, 1)$ 是充分下降参数，$\\alpha \\in (0, 1]$ 是步长，$d_\\phi(x; p_k)$ 是 $\\phi_\\sigma$ 在点 $x$ 沿方向 $p_k$ 的方向导数。\n- 对于精确惩罚优值函数，在点 $x$ 沿方向 $p$ 的方向导数定义为 $d_\\phi(x; p) = \\nabla f(x)^\\top p + \\sigma \\operatorname{sgn}(c(x)) \\nabla c(x) p$，其中 $\\operatorname{sgn}$ 是符号函数，并且当 $c(x)=0$ 时取值为 $0$。\n\n在点 $x_k$ 处的 SQP 步长 $p_k$ 定义为带有线性化约束的二次规划子问题的解：\n- 最小化 $q(p) = \\nabla f(x_k)^\\top p + \\tfrac{1}{2} p^\\top B_k p$，约束条件为 $\\nabla c(x_k) p + c(x_k) = 0$，\n其中 $B_k$ 是一个近似拉格朗日函数黑塞矩阵的对称正定矩阵。在本问题中，使用 $B_k = 2 I$，其中 $I$ 是单位矩阵。该二次规划的最优性条件导出线性 KKT 系统\n$$\n\\begin{bmatrix}\nB_k  \\nabla c(x_k)^\\top \\\\\n\\nabla c(x_k)  0\n\\end{bmatrix}\n\\begin{bmatrix}\np_k \\\\ \\lambda_k\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n\\nabla f(x_k) \\\\ c(x_k)\n\\end{bmatrix}.\n$$\n\n使用精确惩罚优值函数 $\\phi_\\sigma$ 沿 $p_k$ 方向实现一个回溯线搜索，其中 Armijo 参数 $m = 10^{-4}$，回溯因子 $\\rho = \\tfrac{1}{2}$，惩罚参数 $\\sigma = 1000$。从 $\\alpha = 1$ 开始，重复将 $\\alpha$ 乘以 $\\rho$，直到 Armijo 条件满足，或达到最多 50 次回溯步数。如果方向导数 $d_\\phi(x_k; p_k) \\ge 0$，则该步长不是优值函数的下降方向；在这种情况下，报告 $\\alpha = 0$。\n\n你的程序必须：\n- 通过求解上述定义的带有 $B_k = 2 I$ 和 $\\nabla c(x_k)$ 的 KKT 系统，计算在每个给定测试点 $x_k$ 处的 SQP 步长 $p_k$。\n- 使用精确惩罚优值函数 $\\phi_\\sigma$ 和 Armijo 条件，沿 $p_k$ 方向执行回溯线搜索。\n- 对于每个测试点，返回接受的步长 $\\alpha$。\n\n测试套件：\n1. $x_k = (0.1, \\sin(0.1))$ (在 KKT 点附近的可行点)。\n2. $x_k = (0.05, \\sin(0.05))$ (更接近 KKT 点的可行点)。\n3. $x_k = (0.01, \\sin(0.01))$ (非常接近 KKT 点的可行点)。\n4. $x_k = (0.1, \\sin(0.1) + 10^{-3})$ (在 KKT 点附近的轻微不可行点)。\n5. $x_k = (10^{-6}, \\sin(10^{-6}))$ (极度接近 KKT 点的可行点)。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含五个测试用例接受的步长，形式为逗号分隔的列表并用方括号括起，顺序与测试套件相同 (例如 `[\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4,\\alpha_5]`)。每个 $\\alpha_i$ 必须是浮点数。\n\n该设计旨在探讨 Maratos 效应：对于 KKT 点附近的可行迭代点，线性化约束使得 $\\nabla c(x_k) p_k + c(x_k) = 0$，因此一阶可行性沿 $p_k$ 方向得以保持。然而，真实约束 $c(x)$ 表现出曲率，而 $c(x_k + \\alpha p_k)$ 包含量级为 $\\mathcal{O}(\\alpha^2)$ 的二阶项，这可能导致优值函数 $\\phi_\\sigma$ 增加并违反 $\\alpha = 1$ 时的 Armijo 条件，从而尽管 $p_k$ 接近最优，仍然拒绝完整的 SQP 步长。你的实现必须通过计算出的被接受的步长来从数值上检测到这种行为。",
            "solution": "该问题是有效的。这是一个在非线性优化领域中明确定义的数值练习，专门用于在序列二次规划 (SQP) 方法的背景下演示 Maratos 效应。所有必需的函数、参数和过程都已提供，并且该问题在数学上和算法上都是合理的。\n\n**1. 理论框架**\n\n该问题要求分析一个 SQP 方法的线搜索过程在特定非线性约束优化问题上的行为。核心组成部分是：\n\n- **目标函数**: $f(x) = x_1^2 + x_2^2$\n- **等式约束**: $c(x) = x_2 - \\sin x_1 = 0$\n\n该问题的最优解是 $x^\\star=(0, 0)$，在此处目标函数达到最小值。\n\n**2. SQP 方法**\n\nSQP 方法通过在每个迭代点 $x_k$ 构造并求解一个二次规划 (QP) 子问题来迭代地解决优化问题。该 QP 子问题是通过使用拉格朗日函数的二次模型和线性化约束来构建的。\n\n步长 $p_k$ 通过求解以下问题找到：\n$$\n\\min_{p} \\quad \\nabla f(x_k)^\\top p + \\frac{1}{2} p^\\top B_k p\n$$\n$$\n\\text{s.t.} \\quad \\nabla c(x_k) p + c(x_k) = 0\n$$\n\n这里，$B_k=2I$ 是给定的拉格朗日函数黑塞矩阵的正定近似。这个 QP 子问题的解由 Karush-Kuhn-Tucker (KKT) 线性系统给出：\n$$\n\\begin{bmatrix}\nB_k  \\nabla c(x_k)^\\top \\\\\n\\nabla c(x_k)  0\n\\end{bmatrix}\n\\begin{bmatrix}\np_k \\\\ \\lambda_k\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n\\nabla f(x_k) \\\\ c(x_k)\n\\end{bmatrix}\n$$\n其中 $\\lambda_k$ 是约束的拉格朗日乘子估计值。对于我们的特定问题，该系统是：\n$$\n\\begin{bmatrix}\n2  0  -\\cos x_{k,1} \\\\\n0  2  1 \\\\\n-\\cos x_{k,1}  1  0\n\\end{bmatrix}\n\\begin{bmatrix}\np_{k,1} \\\\ p_{k,2} \\\\ \\lambda_k\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n2x_{k,1} \\\\\n2x_{k,2} \\\\\nx_{k,2} - \\sin x_{k,1}\n\\end{bmatrix}\n$$\n在每个测试点 $x_k$ 求解这个 $3 \\times 3$ 的线性系统，以找到 SQP 步长 $p_k$。\n\n**3. 优值函数与线搜索**\n\n为确保步长 $p_k$ 在朝向约束最优解的方向上取得进展，我们使用了一个优值函数。优值函数将目标函数和约束违反度合并为一个单一的值。问题指定了精确惩罚优值函数：\n$$\n\\phi_\\sigma(x) = f(x) + \\sigma |c(x)|\n$$\n其中惩罚参数 $\\sigma=1000$。沿方向 $p_k$ 执行线搜索，以找到一个步长 $\\alpha \\in (0, 1]$，使得该优值函数有足够的下降。新的迭代点将是 $x_{k+1} = x_k + \\alpha p_k$。\n\n充分下降由 Armijo 条件控制：\n$$\n\\phi_\\sigma(x_k + \\alpha p_k) \\le \\phi_\\sigma(x_k) + m \\alpha d_\\phi(x_k; p_k)\n$$\n其中 $m = 10^{-4}$ 是一个小常数，$d_\\phi(x_k; p_k)$ 是 $\\phi_\\sigma$ 在 $x_k$ 处沿 $p_k$ 方向的方向导数，定义为：\n$$\nd_\\phi(x_k; p_k) = \\nabla f(x_k)^\\top p_k + \\sigma \\operatorname{sgn}(c(x_k)) \\nabla c(x_k) p_k\n$$\n线搜索过程是一个回溯算法：\n1.  初始化 $\\alpha = 1$。\n2.  如果满足 Armijo 条件，则接受此 $\\alpha$。\n3.  如果不满足，则将步长乘以一个因子 $\\rho = 1/2$（即 $\\alpha \\leftarrow \\alpha \\rho$）并重复，最多进行 50 次迭代。\n\n一个至关重要的先决条件是 $p_k$ 必须是优值函数的下降方向，即 $d_\\phi(x_k; p_k)  0$。如果 $d_\\phi(x_k; p_k) \\ge 0$，则没有正步长可以满足 Armijo 条件，我们报告 $\\alpha = 0$。\n\n**4. Maratos 效应**\n\nMaratos 效应是一种现象，即一个“好”的步长（如完整的 SQP 步长，$\\alpha=1$）被线搜索拒绝，从而迫使算法采用短的、低效的步长。这通常发生在解 $x^\\star$ 附近，当迭代点 $x_k$ 位于或非常接近约束流形时 ($c(x_k) \\approx 0$)。\n\n从 QP 子问题可知，步长 $p_k$ 满足线性化约束 $\\nabla c(x_k) p_k + c(x_k) = 0$。如果 $x_k$ 是可行的 ($c(x_k)=0$)，则 $\\nabla c(x_k) p_k = 0$。这意味着步长 $p_k$ 在一阶上与 $x_k$ 处的约束流形相切。然而，约束 $c(x)$ 是非线性的（弯曲的）。当我们走这一步时，新点 $x_k + p_k$ 可能会偏离真实的约束流形。对 $c(x_k+p_k)$ 进行泰勒展开可以揭示这一点：\n$$\nc(x_k+p_k) \\approx c(x_k) + \\nabla c(x_k) p_k + \\frac{1}{2} p_k^\\top \\nabla^2 c(x_k) p_k = \\frac{1}{2} p_k^\\top \\nabla^2 c(x_k) p_k\n$$\n这个二阶项代表了约束的曲率，意味着 $|c(x_k+p_k)| \\ne 0$。对于大的惩罚参数 $\\sigma$，由于这种新的约束违反而导致的优值函数增加量 $\\sigma |c(x_k+p_k)|$ 可能会超过在目标函数中实现的减少量，从而导致 $\\alpha=1$ 时 Armijo 条件失败。算法随后被迫缩短步长，破坏了全步长 SQP 方法的快速二次收敛率。\n\n**5. 实现算法**\n\n对于每个测试点 $x_k=(x_{k,1}, x_{k,2})$：\n1.  **计算问题函数**：计算 $c(x_k) = x_{k,2} - \\sin(x_{k,1})$，$\\nabla f(x_k) = [2x_{k,1}, 2x_{k,2}]^\\top$ 和 $\\nabla c(x_k) = [-\\cos(x_{k,1}), 1]$。\n2.  **求解 SQP 步长**：构造 KKT 矩阵 $A$ 和右侧向量 $b$，并求解线性系统 $A z = b$ 以得到 $z=[p_k^\\top, \\lambda_k]^\\top$。\n3.  **计算方向导数**：计算 $d_\\phi(x_k; p_k) = \\nabla f(x_k)^\\top p_k + \\sigma \\operatorname{sgn}(c(x_k)) (\\nabla c(x_k) p_k)$。\n4.  **检查下降方向**：如果 $d_\\phi(x_k; p_k) \\ge 0$，则将接受的步长设为 $\\alpha_k = 0$ 并终止此测试用例。\n5.  **执行回溯线搜索**：\n    - 初始化 $\\alpha = 1.0$。\n    - 计算初始优值函数值 $\\phi_{\\text{current}} = f(x_k) + \\sigma|c(x_k)|$。\n    - 循环最多 50 次迭代：\n        - 令 $x_{\\text{new}} = x_k + \\alpha p_k$。\n        - 计算新的优值函数值 $\\phi_{\\text{new}} = f(x_{\\text{new}}) + \\sigma|c(x_{\\text{new}})|$。\n        - 检查是否 $\\phi_{\\text{new}} \\le \\phi_{\\text{current}} + m \\alpha d_\\phi(x_k; p_k)$。\n        - 如果条件成立，则跳出循环并接受当前的 $\\alpha$。\n        - 否则，更新 $\\alpha \\leftarrow \\alpha \\times 0.5$。\n6.  **存储结果**：$\\alpha$ 的最终值是测试点 $x_k$ 接受的步长 $\\alpha_k$。\n\n此过程将应用于所提供的五个测试用例中的每一个。结果将从数值上展示 Maratos 效应：对于在可行流形上接近解的点，完整的步长 $\\alpha=1$ 将被拒绝，但是当点变得极度接近时，二阶曲率项变得足够小，使得完整的步长被接受。对于不可行点，步长旨在减少不可行性，这通常有助于满足 Armijo 条件。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the accepted Armijo step length for a series of test cases\n    in an SQP framework, demonstrating the Maratos effect.\n    \"\"\"\n    \n    # Define optimization problem parameters and functions\n    sigma = 1000.0\n    m = 1e-4\n    rho = 0.5\n    max_backtrack_steps = 50\n\n    def f(x):\n        \"\"\"Objective function f(x) = x1^2 + x2^2\"\"\"\n        return x[0]**2 + x[1]**2\n\n    def grad_f(x):\n        \"\"\"Gradient of the objective function\"\"\"\n        return np.array([2 * x[0], 2 * x[1]])\n\n    def c(x):\n        \"\"\"Equality constraint c(x) = x2 - sin(x1)\"\"\"\n        return x[1] - np.sin(x[0])\n\n    def grad_c(x):\n        \"\"\"Jacobian of the equality constraint\"\"\"\n        return np.array([-np.cos(x[0]), 1.0])\n    \n    def merit_function(x_val, sigma_val):\n        \"\"\"Exact penalty merit function\"\"\"\n        return f(x_val) + sigma_val * np.abs(c(x_val))\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.1, np.sin(0.1)),\n        (0.05, np.sin(0.05)),\n        (0.01, np.sin(0.01)),\n        (0.1, np.sin(0.1) + 1e-3),\n        (1e-6, np.sin(1e-6))\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x_k = np.array(case)\n        \n        # 1. Calculate values at the current iterate x_k\n        grad_f_k = grad_f(x_k)\n        c_k = c(x_k)\n        grad_c_k = grad_c(x_k)\n        \n        # 2. Construct and solve the KKT system for the SQP step p_k\n        # [ B_k   grad_c(x_k)^T ] [ p_k     ] = - [ grad_f(x_k) ]\n        # [ grad_c(x_k)    0    ] [ lambda_k]   - [   c(x_k)    ]\n        # B_k = 2*I\n        \n        kkt_matrix = np.array([\n            [2.0, 0.0, grad_c_k[0]],\n            [0.0, 2.0, grad_c_k[1]],\n            [grad_c_k[0], grad_c_k[1], 0.0]\n        ])\n        \n        rhs_vector = -np.array([grad_f_k[0], grad_f_k[1], c_k])\n        \n        try:\n            kkt_solution = np.linalg.solve(kkt_matrix, rhs_vector)\n        except np.linalg.LinAlgError:\n            # Handle cases where the matrix might be singular, though not expected for this problem.\n            results.append(0.0) # No valid step can be computed\n            continue\n\n        p_k = kkt_solution[:2]\n        \n        # 3. Compute directional derivative of the merit function\n        d_phi = grad_f_k @ p_k + sigma * np.sign(c_k) * (grad_c_k @ p_k)\n        \n        # 4. Check if p_k is a descent direction\n        if d_phi >= 0:\n            results.append(0.0)\n            continue\n            \n        # 5. Perform backtracking line search\n        alpha = 1.0\n        current_phi = merit_function(x_k, sigma)\n        \n        accepted_alpha = 0.0\n        for i in range(max_backtrack_steps + 1):\n            x_new = x_k + alpha * p_k\n            new_phi = merit_function(x_new, sigma)\n            \n            armijo_condition_rhs = current_phi + m * alpha * d_phi\n            \n            if new_phi = armijo_condition_rhs:\n                accepted_alpha = alpha\n                break\n            \n            if i == max_backtrack_steps: # Last attempt failed\n                accepted_alpha = alpha * rho # The final alpha if loop terminates\n                break\n\n            alpha *= rho\n            \n        results.append(accepted_alpha)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在第一个练习中观察到马拉托斯效应后，下一个关键步骤是深入理解其背后的数学原理。这个练习将挑战您进行“逆向工程”：从单位步长被持续拒绝的现象出发，推断约束的几何性质。通过分析泰勒展开和优化理论中的基本概念，您将能够精确地指出问题并非出在拉格朗日函数的二阶充分条件上，而是源于线性化模型与非线性约束曲率之间的内在矛盾。",
            "id": "3147430",
            "problem": "给定一个等式约束非线性规划问题：最小化 $f(x)$，约束条件为 $c(x)=0$，其中 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 和 $c:\\mathbb{R}^n\\to\\mathbb{R}^m$ 是二次连续可微的。假设在局部解 $x^*$ 处，线性无关约束规范 (LICQ) 成立，其关联的 Karush–Kuhn–Tucker (KKT) 乘子为 $\\lambda^*$，并且二阶充分条件 (SOSC) 成立，即拉格朗日函数的Hessian矩阵 $\\nabla_{xx}^2 L(x^*,\\lambda^*)$ 在切空间 $\\{p:\\nabla c(x^*)p=0\\}$ 上是正定的。考虑一种序列二次规划 (SQP) 方法，该方法通过求解一个带有线性化约束 $\\nabla c(x_k)d_k=-c(x_k)$ 的二次模型来计算步长 $d_k$，并使用基于价值函数 $\\phi_\\rho(x)=f(x)+\\tfrac{1}{2}\\rho\\|c(x)\\|^2$ 的回溯线搜索，其中罚参数 $\\rho0$。在 $x^*$ 的一个邻域内，即使 $x_k\\to x^*$ 且 $d_k\\to 0$，线搜索仍然反复拒绝完整步长 $\\alpha=1$（即最终无限次地取 $\\alpha_k  1$）。\n\n仅使用第一性原理（泰勒展开、LICQ、KKT 和 SOSC 的定义），逆向分析这种对 $\\alpha=1$ 的持续拒绝揭示了关于约束曲率的什么信息，并提出能够恢复局部超线性步长接受和收敛的算法修正方案。\n\n选择所有与上述现象最一致的陈述。\n\nA. 存在至少一个激活约束分量 $c_i$ 和一个非零切向量 $p$（满足 $\\nabla c(x^*)p=0$），使得 $p^\\top \\nabla^2 c_i(x^*)\\,p\\neq 0$。在 $x_k+d_k$ 处产生的 $O(\\|d_k\\|^2)$ 阶的可行性误差可能会压倒价值函数中模型预测的下降量，并引发 Maratos 效应，导致在 $x^*$ 附近拒绝 $\\alpha=1$。\n\nB. 对 $\\alpha=1$ 的反复拒绝意味着 $\\nabla_{xx}^2 L(x^*,\\lambda^*)$ 在切空间上必须是不定的；否则，只要足够接近 $x^*$，$\\alpha=1$ 将总是被接受。\n\nC. 将罚参数 $\\rho$ 增加到足够大可以保证在不修改算法步长的情况下，在 $x^*$ 附近接受 $\\alpha=1$，从而消除 Maratos 效应。\n\nD. 引入一个二阶校正 (SOC) 步来显式补偿 $O(\\|d_k\\|^2)$ 阶的约束违反（例如，通过求解 $\\nabla c(x_k+d_k)s_k=-c(x_k+d_k)$ 并在行搜索中使用 $d_k+s_k$），或者采用增强了此类 SOC 步的滤子线搜索，可以在 LICQ 和 SOSC 条件下恢复局部的超线性收敛和对 $\\alpha=1$ 的接受。\n\nE. 观察到的行为意味着在切空间上 $\\nabla^2 c(x^*)=0$；约束在 $x^*$ 附近实际上是仿射的，而拒绝必须源于目标模型而非约束的曲率。\n\n选择所有适用的选项。",
            "solution": "用户要求对问题陈述进行严格验证，然后基于第一性原理推导所述现象（Maratos效应）的原因，并提出算法修正建议。最后，必须对每个选项进行评估。\n\n### 第一步：提取已知条件\n- **问题类型**：等式约束非线性规划 (NLP)。\n- **目标**：最小化 $f(x)$，其中 $f: \\mathbb{R}^n \\to \\mathbb{R}$。\n- **约束**：$c(x) = 0$，其中 $c: \\mathbb{R}^n \\to \\mathbb{R}^m$。\n- **光滑性**：$f$ 和 $c$ 是二次连续可微的 ($C^2$)。\n- **解的性质**：\n    - $x^*$ 是一个局部解。\n    - 线性无关约束规范 (LICQ) 在 $x^*$ 处成立，意味着约束梯度向量 $\\{\\nabla c_i(x^*)\\}_{i=1}^m$ 是线性无关的，因此雅可比矩阵 $\\nabla c(x^*)$ 具有满行秩 $m$。\n    - 二阶充分条件 (SOSC) 在 $(x^*, \\lambda^*)$ 处成立，其中 $\\lambda^*$ 是 Karush-Kuhn-Tucker (KKT) 乘子向量。这意味着拉格朗日函数的Hessian矩阵 $\\nabla_{xx}^2 L(x^*, \\lambda^*) = \\nabla^2 f(x^*) + \\sum_{i=1}^m \\lambda_i^* \\nabla^2 c_i(x^*)$ 在切空间 $T(x^*) = \\{p \\in \\mathbb{R}^n \\mid \\nabla c(x^*) p = 0 \\}$ 上是正定的。\n- **算法**：序列二次规划 (SQP) 方法。\n    - **步长计算**：搜索方向 $d_k$ 是通过求解一个涉及拉格朗日函数的二次模型和形如 $\\nabla c(x_k) d_k = -c(x_k)$ 的线性化约束的二次规划 (QP) 计算得到的。\n- **全局化策略**：对 $\\ell_2$ 价值函数 $\\phi_\\rho(x) = f(x) + \\frac{1}{2}\\rho \\|c(x)\\|^2$ 进行回溯线搜索，其中罚参数 $\\rho  0$。\n- **观察到的现象**：在解 $x^*$ 附近，当 $x_k \\to x^*$ 且 $d_k \\to 0$ 时，线搜索持续拒绝完整步长 $\\alpha=1$（即无限次地取 $\\alpha_k  1$）。这阻碍了超线性收敛。\n\n### 第二步：使用提取的已知条件进行验证\n1.  **科学基础**：该问题牢固地植根于成熟的非线性约束优化理论。所描述的现象，即 Maratos 效应，是 SQP 方法中一个经典且被深入理解的问题。所有术语（LICQ、SOSC、SQP、价值函数）都是标准术语。\n2.  **适定性**：该问题是适定的。它要求对一种特定的、可形式化的算法行为进行解释，并提出标准的补救措施，这些都有基于优化理论的确定答案。\n3.  **客观性**：问题以精确、客观的数学语言陈述。\n4.  **一致性与完整性**：问题设置在内部是一致的。所提供的信息（LICQ、SOSC、SQP步长定义、价值函数）足以从第一性原理分析 Maratos 效应。\n\n### 第三步：结论与行动\n问题陈述是**有效的**。我们可以继续进行求解。\n\n### Maratos效应的第一性原理分析\n\n问题的核心在于 QP 子问题线性化模型所预测的进展与非线性价值函数 $\\phi_\\rho(x)$ 实际测量的进展之间存在不匹配。接受完整步长 ($\\alpha=1$) 的线搜索条件通常要求有充分下降，例如 $\\phi_\\rho(x_k+d_k) \\le \\phi_\\rho(x_k) + \\eta \\Delta_k$，其中 $\\eta \\in (0,1)$ 且 $\\Delta_k  0$ 是一个预测的下降量。拒绝 $\\alpha=1$ 意味着这个条件不满足；具体来说，实际的减少量不足，甚至可能价值函数值有所增加，即 $\\phi_\\rho(x_k+d_k)  \\phi_\\rho(x_k)$。\n\n让我们使用在 $x_k$ 处的泰勒展开来分析试探点 $x_k+d_k$ 处的约束值：\n$$c(x_k+d_k) = c(x_k) + \\nabla c(x_k)d_k + \\frac{1}{2}\\mathcal{C}(x_k, d_k) + O(\\|d_k\\|^3)$$\n其中 $\\mathcal{C}(x_k, d_k)$ 是一个向量，其第 $i$ 个分量为 $d_k^\\top \\nabla^2 c_i(x_k) d_k$。\nSQP 步长 $d_k$ 由线性化约束方程 $\\nabla c(x_k)d_k = -c(x_k)$ 定义。将其代入展开式得到：\n$$c(x_k+d_k) = \\frac{1}{2}\\mathcal{C}(x_k, d_k) + O(\\|d_k\\|^3)$$\n这是一个关键结果。SQP 步长通常不会产生一个可行点。新的约束违反量 $c(x_k+d_k)$ 的量级是 $O(\\|d_k\\|^2)$，其值直接由**约束的曲率**决定，该曲率编码在Hessian项 $\\nabla^2 c_i(x_k)$ 中。如果所有约束都是仿射的（即对所有 $i$ 都有 $\\nabla^2 c_i(x) = 0$），那么 $c(x_k+d_k)$ 将为零（直到更高阶项），并且该步长将产生一个可行点。因此，Maratos 效应从根本上是由约束的非线性引起的。\n\nSQP 步长 $d_k$ 是一个朝向解 $(x^*, \\lambda^*)$ 的快速的、类牛顿的步长，意味着 $x_k+d_k$ 是比 $x_k$ 对 $x^*$ 的一个超线性更好的近似。然而，价值函数 $\\phi_\\rho(x)$ 可能无法识别这种进展。算法采用一个步长 $d_k$，将可行性从 $\\|c(x_k)\\| = O(\\|d_k\\|)$ 改善到 $\\|c(x_k+d_k)\\| = O(\\|d_k\\|^2)$。可行性项 $\\frac{1}{2}\\rho\\|c(x)\\|^2$ 的大幅减少与目标项 $f(x)$ 的变化相平衡。\n\n从一个不可行点 $x_k$ 到一个在弯曲约束流形上近似可行的点 $x_k+d_k$ 的路径可能需要相对于目标函数 $f$ \"上坡\"移动。这意味着 $f(x_k+d_k)  f(x_k)$ 是可能的。当这种情况发生时，$f(x)$ 的增加可能会超过 $\\phi_\\rho(x)$ 中可行性项的减少，导致价值函数增加，即 $\\phi_\\rho(x_k+d_k)  \\phi_\\rho(x_k)$，从而导致步长被拒绝。这种权衡正是 Maratos 效应。\n\n### 算法修正\n\n为了克服这一点，必须修改算法以容忍 SQP 步长在解附近的行为。\n1.  **二阶校正 (SOC)**：在计算 SQP 步长 $d_k$ 后，如果试探点 $x_k+d_k$ 未被接受，则计算一个校正步 $s_k$ 以进一步改善可行性。该步长 $s_k$ 通常通过求解一个基于 $x_k+d_k$ 处剩余约束违反量的简化系统来计算。一个常见的选择是通过线性系统 $\\nabla c(x_k) s_k = -c(x_k+d_k)$ 求解 $s_k$。由于 $c(x_k+d_k) = O(\\|d_k\\|^2)$，LICQ 意味着 $s_k=O(\\|d_k\\|^2)$。新的试探点是 $x_k+d_k+s_k$。在这个新点，约束违反量变为 $O(\\|d_k\\|^3)$ 或更小，这是一个小得多的值。这个校正足够小，不会干扰 $d_k$ 的理想性质，但又足够强大，能将步长“弯曲”回可行区域，确保价值函数记录到足够的下降。\n2.  **非单调线搜索 / 滤子法**：这些方法不要求在每次迭代中价值函数都严格下降，而是放宽了接受标准。滤子法维护一组先前“不可接受”的 $(f(x_i), \\|c(x_i)\\|)$ 对，并接受一个新的试探点，只要它不被滤子中的任何点“支配”（即，它充分改善了目标函数值或约束违反量）。这种方法自然地适应了 SQP 步长的行为，因为 SQP 步长可能为了在可行性上获得大的收益而暂时增加目标函数值。为了确保收敛，当 SQP 步长被拒绝时，滤子法通常会辅以 SOC 步。\n\n### 选项评估\n\n**A. 存在至少一个激活约束分量 $c_i$ 和一个非零切向量 $p$（满足 $\\nabla c(x^*)p=0$），使得 $p^\\top \\nabla^2 c_i(x^*)\\,p\\neq 0$。在 $x_k+d_k$ 处产生的 $O(\\|d_k\\|^2)$ 阶的可行性误差可能会压倒价值函数中模型预测的下降量，并引发 Maratos 效应，导致在 $x^*$ 附近拒绝 $\\alpha=1$。**\n这个陈述与我们的第一性原理分析完全一致。对于切向量 $p$，条件 $p^\\top \\nabla^2 c_i(x^*)p \\neq 0$ 是约束在解处存在曲率的数学表达式。这种曲率导致了 $O(\\|d_k\\|^2)$ 阶的剩余约束违反量 $c(x_k+d_k)$，这反过来又可能导致价值函数拒绝单位步长。这正是 Maratos 效应的精确机制。\n**结论：正确。**\n\n**B. 对 $\\alpha=1$ 的反复拒绝意味着 $\\nabla_{xx}^2 L(x^*,\\lambda^*)$ 在切空间上必须是不定的；否则，只要足够接近 $x^*$，$\\alpha=1$ 将总是被接受。**\n这个陈述是错误的。问题明确假设二阶充分条件 (SOSC) 成立，这意味着 $\\nabla_{xx}^2 L(x^*, \\lambda^*)$ 在切空间上是*正定*的。Maratos 效应是在问题满足快速局部收敛的标准条件（LICQ 和 SOSC）*尽管如此*仍然发生的现象。失败之处不在于局部最优性条件，而在于全局化机制（价值函数线搜索）。\n**结论：不正确。**\n\n**C. 将罚参数 $\\rho$ 增加到足够大可以保证在不修改算法步长的情况下，在 $x^*$ 附近接受 $\\alpha=1$，从而消除 Maratos 效应。**\n这个陈述是错误的。增加罚参数$\\rho$通常会加剧Maratos效应，因为它会更加严厉地惩罚由约束曲率引起的$O(\\|d_k\\|^2)$阶的可行性违反，使其更容易压倒目标函数的下降。\n**结论：不正确。**\n\n**D. 引入一个二阶校正 (SOC) 步来显式补偿 $O(\\|d_k\\|^2)$ 阶的约束违反（例如，通过求解 $\\nabla c(x_k+d_k)s_k=-c(x_k+d_k)$ 并在行搜索中使用 $d_k+s_k$），或者采用增强了此类 SOC 步的滤子线搜索，可以在 LICQ 和 SOSC 条件下恢复局部的超线性收敛和对 $\\alpha=1$ 的接受。**\n这个陈述准确地描述了两种主要的、可证明有效的针对 Maratos 效应的补救措施。二阶校正步专门设计用于将 $O(\\|d_k\\|^2)$ 阶的约束违反量减少到更高阶，从而解决了与价值函数的冲突。滤子法提供了另一种全局化策略，它不会以同样的方式受到 Maratos 效应的影响，而且它们通常会用 SOC 步来加强。这些都是标准的、最先进的技术。\n**结论：正确。**\n\n**E. 观察到的行为意味着在切空间上 $\\nabla^2 c(x^*)=0$；约束在 $x^*$ 附近实际上是仿射的，而拒绝必须源于目标模型而非约束的曲率。**\n这个陈述与事实在逻辑上正好相反。如第一性原理分析所示，如果约束是仿射的（$\\nabla^2 c_i=0$），SQP 步将产生一个完全可行的点（即 $c(x_k+d_k)=0$）。在这种情况下，Maratos 效应不会发生。该效应根本上且必然是非零约束曲率的结果。\n**结论：不正确。**",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "识别并理解问题之后，真正的工程师需要找到解决方案。本练习将指导您为标准的SQP算法增加一个强大的功能——二阶校正（SOC），这是一种专门用于克服马拉托斯效应的技术。您将通过编程实现基线SQP算法和增强后的SOC-SQP算法，并在一系列曲率变化的约束问题上量化比较它们的性能，从而亲身体验现代优化算法如何通过引入更高阶的信息来保证快速的局部收敛性。",
            "id": "3147419",
            "problem": "实现一个数值实验，以在一族光滑非线性不等式约束优化问题中诊断并缓解序列二次规划（SQP）的Maratos效应。您的程序必须实现两个求解器：一个使用精确罚函数并进行回溯线搜索的基准SQP求解器，以及一个通过二阶校正（SOC）增强的SQP求解器，该校正用于补偿曲率引起的线性化误差。请在一个参数化的曲面约束上比较它们的迭代次数，并报告由SOC带来的迭代次数改进量，将其作为约束曲率参数的函数。\n\n该优化问题由一个非负曲率参数 $\\,\\kappa \\in \\mathbb{R}_{\\ge 0}\\,$ 参数化：\n- 决策变量：$\\,x = (x_1,x_2) \\in \\mathbb{R}^2$。\n- 目标：最小化 $\\,f(x) = \\tfrac{1}{2}(x_1 - 1)^2 + \\tfrac{1}{2} x_2^2$。\n- 单不等式约束：$\\,c_\\kappa(x) \\le 0\\,$，其中 $\\,c_\\kappa(x) = \\tfrac{1}{2}\\kappa x_1^2 - x_2$。\n\n解释：可行集为 $\\,\\{x \\in \\mathbb{R}^2 \\,:\\, x_2 \\ge \\tfrac{1}{2}\\kappa x_1^2\\}\\,$，即曲率随 $\\,\\kappa\\,$ 增加的抛物线上方的区域。无约束最小化点位于 $\\,x^\\star = (1,0)\\,$，并且随着 $\\,\\kappa$ 的增加而变得不可行。\n\n您必须使用以下基本和核心定义来论证您的算法设计：\n- 不等式约束光滑优化的Karush–Kuhn–Tucker (KKT) 条件定义了局部最小化点的一阶必要条件：对于某个乘子 $\\,\\lambda \\ge 0\\,$，满足驻点条件 $\\,\\nabla f(x) + \\lambda \\nabla c_\\kappa(x) = 0\\,$、原始可行性 $\\,c_\\kappa(x) \\le 0\\,$、对偶可行性 $\\,\\lambda \\ge 0\\,$ 以及互补松弛性 $\\,\\lambda\\,c_\\kappa(x) = 0$。\n- 序列二次规划 (SQP) 通过求解在当前迭代点处拉格朗日函数的二次近似（受限于约束的线性化）来构造步长。\n- Maratos效应指的是这样一种现象：一个线性可行（一阶）的SQP步长，在代入原始非线性约束进行评估时，由于约束泰勒展开中的二阶项而变得不可行，从而导致价值函数下降不佳，并引起线搜索拒绝步长及多次回溯缩减。\n- 二阶校正 (SOC) 通过在试验点处增加一个小的校正量来弥补这一缺陷，该校正量旨在恢复该点的线性化可行性，从而抵消二阶残差。\n\n实现要求：\n- 每次运行都从相同的初始点 $\\,x^{(0)} = (0.5,\\,0.0)\\,$ 开始。\n- 在QP子问题中，使用单位矩阵 $\\,B = I\\,$作为Hessian矩阵的近似。\n- 在每次外层迭代中，构建并求解单约束QP子问题，该子问题最小化 $\\,f\\,$ 的二次模型，并受限于 $\\,c_\\kappa\\,$ 在当前迭代点处的线性化。通过检查无约束步长是否线性可行来使用单不等式情况的精确解；否则，将线性化约束强制为活动约束。\n- 对精确罚函数 $\\,\\phi(x) = f(x) + \\mu \\max\\{0, c_\\kappa(x)\\}\\,$ 使用回溯线搜索，固定惩罚权重 $\\,\\mu = 100\\,$。使用Armijo条件，其中充分下降参数为 $\\,\\sigma = 10^{-4}\\,$，回溯因子为 $\\,\\beta = 0.5\\,$。\n- 对于SOC增强的变体，在 $\\,x\\,$ 处计算出QP步长 $\\,p\\,$ 后，构建试验点 $\\,x^+ = x + p\\,$，如果 $\\,c_\\kappa(x^+)  0\\,$，则通过将线性化不等式作为等式求解：$\\,\\nabla c_\\kappa(x^+)^\\top d = -c_\\kappa(x^+)\\,$，来计算一个恢复 $\\,x^+\\,$ 处线性化可行性的最小范数校正量 $\\,d\\,$。在线搜索中使用组合步长 $\\,s = p + d\\,$。如果 $\\,c_\\kappa(x^+) \\le 0\\,$，则设 $\\,d = 0\\,$ 且 $\\,s = p\\,$。\n- 当KKT残差 $\\,r(x)\\,$ 低于容差 $\\,\\varepsilon = 10^{-6}\\,$ 或迭代次数达到硬上限 $\\,200\\,$ 时终止。将KKT残差定义为\n$$\nr(x) \\;=\\; \\left\\|\\nabla f(x) + \\lambda^\\star(x)\\,\\nabla c_\\kappa(x)\\right\\|_2 \\;+\\; \\max\\{0, c_\\kappa(x)\\},\n$$\n其中 $\\,\\lambda^\\star(x) = \\max\\left\\{0,\\; -\\dfrac{\\nabla f(x)^\\top \\nabla c_\\kappa(x)}{\\|\\nabla c_\\kappa(x)\\|_2^2}\\right\\}\\,$ 是在 $\\,\\lambda \\ge 0\\,$ 上最小化 $\\,\\left\\|\\nabla f(x) + \\lambda \\nabla c_\\kappa(x)\\right\\|_2\\,$ 的非负标量。\n\n测试套件和输出：\n- 对列表 $\\,\\kappa \\in \\{0.0,\\,0.5,\\,2.0,\\,5.0,\\,10.0\\}\\,$ 中的每个曲率参数，运行基准SQP和SOC增强SQP。\n- 对于每个 $\\,\\kappa\\,$，记录基准求解器收敛所需的外层迭代次数，记为 $\\,N_{\\text{base}}(\\kappa)\\,$，以及SOC增强求解器的迭代次数，记为 $\\,N_{\\text{soc}}(\\kappa)\\,$。\n- 对于每个 $\\,\\kappa\\,$，计算整数改进量 $\\,\\Delta(\\kappa) = N_{\\text{base}}(\\kappa) - N_{\\text{soc}}(\\kappa)\\,$。正值表示SOC所需的迭代次数更少；零表示没有变化；负值表示SOC需要更多迭代。\n\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，用方括号括起来（例如，`[3,1,0,2,4]`），对应于 $\\,\\kappa \\in \\{0.0,\\,0.5,\\,2.0,\\,5.0,\\,10.0\\}\\,$ 顺序的 $\\,\\Delta(\\kappa)\\,$ 值。\n\n此问题不涉及物理单位或角度；所有输出均为无单位整数。",
            "solution": "用户提供的问题是有效的。这是一个适定的、有科学依据的数值优化领域的问题。所有必要的定义、参数和算法组件都已提供，并且没有内部矛盾或歧义。该问题是一个标准的教科书示例，用于演示Maratos效应以及序列二次规划（SQP）中二阶校正（SOC）的有效性。\n\n我们接下来提供详细的解决方案。\n\n### 1. 问题表述与预备知识\n\n该问题旨在最小化一个光滑非线性目标函数，其受限于单个光滑非线性不等式约束。问题由一个非负曲率参数 $\\kappa \\in \\mathbb{R}_{\\ge 0}$ 参数化。\n\n- **目标函数：** $f(x) = \\frac{1}{2}(x_1 - 1)^2 + \\frac{1}{2} x_2^2$，其中 $x = (x_1, x_2) \\in \\mathbb{R}^2$。\n- **约束函数：** $c_\\kappa(x) = \\frac{1}{2}\\kappa x_1^2 - x_2 \\le 0$。\n\n这些函数的梯度对于SQP算法至关重要：\n- **目标函数的梯度：** $\\nabla f(x) = \\begin{pmatrix} x_1 - 1 \\\\ x_2 \\end{pmatrix}$。\n- **约束函数的梯度：** $\\nabla c_\\kappa(x) = \\begin{pmatrix} \\kappa x_1 \\\\ -1 \\end{pmatrix}$。\n\nKarush-Kuhn-Tucker (KKT) 条件为点 $x^\\star$ 成为局部最小化点提供了一阶必要条件。必须存在一个拉格朗日乘子 $\\lambda^\\star \\ge 0$ 使得：\n1.  **驻点条件：** $\\nabla f(x^\\star) + \\lambda^\\star \\nabla c_\\kappa(x^\\star) = 0$。\n2.  **原始可行性：** $c_\\kappa(x^\\star) \\le 0$。\n3.  **对偶可行性：** $\\lambda^\\star \\ge 0$。\n4.  **互补松弛性：** $\\lambda^\\star c_\\kappa(x^\\star) = 0$。\n\n### 2. 序列二次规划 (SQP) 框架\n\nSQP是一种迭代方法，它通过一系列二次规划（QP）子问题来对原始非线性问题进行建模。在每次迭代 $x^{(k)}$ 时，我们通过求解一个QP来计算搜索方向 $p^{(k)}$，该QP由拉格朗日函数的二次模型和约束的线性模型组成。\n\n#### 2.1. QP子问题\n\n在给定的迭代点 $x^{(k)}$，QP子问题定义为：\n$$\n\\min_{p \\in \\mathbb{R}^2} \\quad \\frac{1}{2} p^\\top B^{(k)} p + \\nabla f(x^{(k)})^\\top p\n$$\n$$\n\\text{s.t.} \\quad \\nabla c_\\kappa(x^{(k)})^\\top p + c_\\kappa(x^{(k)}) \\le 0\n$$\n根据规定，我们使用单位矩阵作为Hessian近似，即 $B^{(k)} = I$。令 $g^{(k)} = \\nabla f(x^{(k)})$，$A^{(k)} = \\nabla c_\\kappa(x^{(k)})^\\top$，$b^{(k)} = -c_\\kappa(x^{(k)})$。QP简化为：\n$$\n\\min_{p \\in \\mathbb{R}^2} \\quad \\frac{1}{2} p^\\top p + (g^{(k)})^\\top p \\quad \\text{s.t.} \\quad A^{(k)} p \\le b^{(k)}\n$$\nQP目标的无约束最小化点是 $p_{unc} = -g^{(k)}$。我们检查此步长是否满足线性化约束：\n- **情况1：约束非活动。** 如果 $A^{(k)} p_{unc} \\le b^{(k)}$，或等价地 $c_\\kappa(x^{(k)}) \\le \\nabla c_\\kappa(x^{(k)})^\\top \\nabla f(x^{(k)})$，则无约束步长对于QP是可行的。解为 $p^{(k)} = -g^{(k)}$。QP乘子为 $\\nu^{(k)}=0$。\n- **情况2：约束活动。** 如果 $A^{(k)} p_{unc}  b^{(k)}$，解必须位于可行集的边界上，即 $A^{(k)} p = b^{(k)}$。此等式约束QP的解由 $p^{(k)} = -g^{(k)} - \\nu^{(k)} (A^{(k)})^\\top$ 给出，其中QP乘子 $\\nu^{(k)}$ 通过将 $p^{(k)}$ 代入活动约束中求得：\n$$\n\\nu^{(k)} = \\frac{-b^{(k)} - A^{(k)} g^{(k)}}{A^{(k)} (A^{(k)})^\\top} = \\frac{c_\\kappa(x^{(k)}) - \\nabla c_\\kappa(x^{(k)})^\\top \\nabla f(x^{(k)})}{\\|\\nabla c_\\kappa(x^{(k)})\\|_2^2}\n$$\n搜索方向则为 $p^{(k)} = -\\nabla f(x^{(k)}) - \\nu^{(k)} \\nabla c_\\kappa(x^{(k)})$。由于我们处于这种情况，分子为正，确保 $\\nu^{(k)}  0$。\n\n#### 2.2. 通过价值函数和线搜索实现全局化\n\n步长 $p^{(k)}$ 提供了一个良好的局部搜索方向。为确保全局收敛，我们执行回溯线搜索以找到一个步长 $\\alpha^{(k)} \\in (0, 1]$，该步长能在价值函数上产生足够的下降。我们使用精确罚（或 $l_1$）价值函数：\n$$\n\\phi(x) = f(x) + \\mu \\max\\{0, c_\\kappa(x)\\}\n$$\n惩罚参数为 $\\mu = 100$。线搜索旨在满足Armijo条件：\n$$\n\\phi(x^{(k)} + \\alpha s^{(k)}) \\le \\phi(x^{(k)}) + \\sigma \\alpha D^{(k)}\n$$\n其中 $s^{(k)}$ 是搜索方向（待定义），$\\sigma=10^{-4}$ 是充分下降常数，$D^{(k)}$ 是预测的方向导数。在此背景下，$D^{(k)}$ 的一个标准选择是基于激发SQP步长的线性化模型：\n$$\nD^{(k)} = \\nabla f(x^{(k)})^\\top p^{(k)} - \\mu \\max\\{0, c_\\kappa(x^{(k)})\\}\n$$\n对于基准SQP算法，搜索方向就是QP的解，即 $s^{(k)} = p^{(k)}$。\n\n### 3. Maratos效应与二阶校正 (SOC)\n\nMaratos效应发生在迭代点接近最优解但不在约束边界上时。SQP步长 $p^{(k)}$ 提供了一个朝向最优点的极佳移动，减少了拉格朗日模型的值。然而，由于真实约束 $c_\\kappa$ 的曲率，试验点 $x^{(k+1)} = x^{(k)} + p^{(k)}$ 可能会落在可行集之外很远的地方，即 $c_\\kappa(x^{(k+1)})  0$。这种约束违反的增加可能会超过目标函数 $f$ 的减少，导致价值函数 $\\phi$ 增加。线搜索会拒绝完整步长（$\\alpha=1$）并需要多次回溯，从而严重减慢收敛速度。对于较大的 $\\kappa$（对应于较高的约束曲率），这种效应更为显著。\n\n二阶校正（SOC）是一种补救措施。在计算主步长 $p^{(k)}$ 后，我们形成一个试验点 $x^+ = x^{(k)} + p^{(k)}$。然后我们评估该点的真实约束违反值 $c_\\kappa(x^+)$。如果 $c_\\kappa(x^+)  0$，我们计算一个“校正”步长 $d^{(k)}$ 以恢复可行性，至少是一阶可行性。这个校正是满足 $x^+$ 处线性化约束方程的最小范数向量：\n$$\n\\nabla c_\\kappa(x^+)^\\top d = -c_\\kappa(x^+)\n$$\n这个最小范数问题的解是：\n$$\nd^{(k)} = -\\frac{c_\\kappa(x^+)}{\\|\\nabla c_\\kappa(x^+)\\|_2^2} \\nabla c_\\kappa(x^+)\n$$\n线搜索的最终搜索方向是复合步长 $s^{(k)} = p^{(k)} + d^{(k)}$。这个新方向融合了关于约束的二阶信息（通过在 $x^+$ 处求值），旨在以 $\\alpha=1$ 满足Armijo条件，从而恢复快速的局部收敛。如果 $c_\\kappa(x^+) \\le 0$，则不需要校正，我们设 $s^{(k)} = p^{(k)}$。\n\n### 4. 终止准则\n\n当KKT残差低于容差 $\\varepsilon = 10^{-6}$ 时，算法终止。残差 $r(x)$ 衡量KKT条件的违反程度：\n$$\nr(x) = \\underbrace{\\|\\nabla f(x) + \\lambda^\\star(x)\\,\\nabla c_\\kappa(x)\\|_2}_{\\text{驻点误差}} + \\underbrace{\\max\\{0, c_\\kappa(x)\\}}_{\\text{可行性误差}}\n$$\n其中 $\\lambda^\\star(x)$ 是拉格朗日乘子的一个估计。它被选为最小化残差驻点部分的乘子的非负最小二乘解：\n$$\n\\lambda^\\star(x) = \\arg\\min_{\\lambda \\ge 0} \\left\\|\\nabla f(x) + \\lambda\\,\\nabla c_\\kappa(x)\\right\\|_2^2\n$$\n求解这个单变量非负约束最小二乘问题，得到：\n$$\n\\lambda^\\star(x) = \\max\\left\\{0,\\; -\\frac{\\nabla f(x)^\\top \\nabla c_\\kappa(x)}{\\|\\nabla c_\\kappa(x)\\|_2^2}\\right\\}\n$$\n同时还设置了200次迭代的硬上限。\n\n### 5. 算法总结\n\n**基准SQP与SOC-SQP算法：**\n\n1.  初始化 $k=0$，$x^{(0)} = (0.5, 0.0)$，$\\mu=100$，$\\sigma=10^{-4}$，$\\beta=0.5$，$\\varepsilon=10^{-6}$。\n2.  **For** $k = 0, 1, 2, \\dots, 199$:\n    a.  计算 $f(x^{(k)})$、$\\nabla f(x^{(k)})$、$c_\\kappa(x^{(k)})$ 和 $\\nabla c_\\kappa(x^{(k)})$。\n    b.  计算KKT残差 $r(x^{(k)})$。如果 $r(x^{(k)})  \\varepsilon$，终止并返回 $k$。\n    c.  求解QP子问题以找到步长 $p^{(k)}$ 和QP乘子 $\\nu^{(k)}$。\n    d.  **设置搜索方向 $s^{(k)}$**：\n        -   **基准：** $s^{(k)} = p^{(k)}$。\n        -   **SOC增强：**\n            i.   设试验点 $x^+ = x^{(k)} + p^{(k)}$。\n            ii.  如果 $c_\\kappa(x^+)  0$，计算 $d^{(k)} = -\\frac{c_\\kappa(x^+)}{\\|\\nabla c_\\kappa(x^+)\\|_2^2} \\nabla c_\\kappa(x^+)$ 并设置 $s^{(k)} = p^{(k)} + d^{(k)}$。\n            iii. 否则，设置 $s^{(k)} = p^{(k)}$。\n    e.  **回溯线搜索**：\n        i.   初始化步长 $\\alpha = 1$。\n        ii.  计算当前价值函数值 $\\phi(x^{(k)})$。\n        iii. 计算预测方向导数 $D^{(k)} = \\nabla f(x^{(k)})^\\top p^{(k)} - \\mu \\max\\{0, c_\\kappa(x^{(k)})\\} $。\n        iv.  **While** $\\phi(x^{(k)} + \\alpha s^{(k)})  \\phi(x^{(k)}) + \\sigma \\alpha D^{(k)}$:\n             $\\alpha \\leftarrow \\beta \\alpha$。\n    f.  **更新：** $x^{(k+1)} = x^{(k)} + \\alpha s^{(k)}$。\n3.  如果循环完成，终止并返回 $200$。\n\n该实验将对基准版本和SOC增强版本，在指定的 $\\kappa$ 值范围内执行此过程，并报告收敛所需的迭代次数差异。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a numerical experiment to diagnose and mitigate the Maratos effect\n    within Sequential Quadratic Programming (SQP) for a family of smooth nonlinear\n    inequality constrained optimization problems.\n    \"\"\"\n\n    # --- Problem Definition ---\n    def objective_func(x):\n        \"\"\"Computes the objective function value and its gradient.\"\"\"\n        f_val = 0.5 * (x[0] - 1.0)**2 + 0.5 * x[1]**2\n        grad_f = np.array([x[0] - 1.0, x[1]])\n        return f_val, grad_f\n\n    def constraint_func(x, kappa):\n        \"\"\"Computes the constraint function value and its gradient.\"\"\"\n        c_val = 0.5 * kappa * x[0]**2 - x[1]\n        grad_c = np.array([kappa * x[0], -1.0])\n        return c_val, grad_c\n\n    def merit_func(x, kappa, mu):\n        \"\"\"Computes the exact penalty merit function.\"\"\"\n        f_val, _ = objective_func(x)\n        c_val, _ = constraint_func(x, kappa)\n        return f_val + mu * max(0, c_val)\n\n    def kkt_residual(x, kappa):\n        \"\"\"Computes the KKT residual for termination checking.\"\"\"\n        _, grad_f = objective_func(x)\n        c_val, grad_c = constraint_func(x, kappa)\n        \n        grad_c_norm_sq = np.dot(grad_c, grad_c)\n        \n        if grad_c_norm_sq  1e-12:\n            lambda_star = 0.0\n        else:\n            lambda_star = max(0, -np.dot(grad_f, grad_c) / grad_c_norm_sq)\n\n        stationarity_err = np.linalg.norm(grad_f + lambda_star * grad_c)\n        feasibility_err = max(0, c_val)\n        \n        return stationarity_err + feasibility_err\n\n    # --- SQP Solver ---\n    def sqp_solver(kappa, use_soc):\n        \"\"\"\n        Solves the optimization problem for a given kappa using SQP.\n        \n        Args:\n            kappa (float): The curvature parameter.\n            use_soc (bool): If True, enables the Second-Order Correction.\n            \n        Returns:\n            int: The number of iterations to convergence.\n        \"\"\"\n        \n        # Parameters\n        x = np.array([0.5, 0.0])\n        mu = 100.0\n        sigma = 1e-4\n        beta = 0.5\n        tol = 1e-6\n        max_iter = 200\n\n        for i in range(max_iter):\n            # 1. Check for termination\n            if kkt_residual(x, kappa)  tol:\n                return i\n\n            # 2. Evaluate functions and gradients at the current iterate\n            f_val, grad_f = objective_func(x)\n            c_val, grad_c = constraint_func(x, kappa)\n\n            # 3. Solve the QP subproblem for the step p\n            p = np.zeros(2)\n            \n            # Check if the unconstrained step p = -grad_f is feasible for the linearized constraint\n            if np.dot(grad_c, -grad_f) + c_val = 0:\n                p = -grad_f\n            else:\n                # Constraint is active, solve for p on the boundary\n                grad_c_norm_sq = np.dot(grad_c, grad_c)\n                if grad_c_norm_sq  1e-12: return max_iter # Failsafe\n                nu = (c_val - np.dot(grad_c, grad_f)) / grad_c_norm_sq\n                p = -grad_f - nu * grad_c\n\n            # 4. Determine search direction s (with optional SOC)\n            s = p\n            if use_soc:\n                x_plus = x + p\n                c_val_plus, grad_c_plus = constraint_func(x_plus, kappa)\n\n                if c_val_plus > 0:\n                    grad_c_plus_norm_sq = np.dot(grad_c_plus, grad_c_plus)\n                    if grad_c_plus_norm_sq > 1e-12:\n                        d = -(c_val_plus / grad_c_plus_norm_sq) * grad_c_plus\n                        s = p + d\n            \n            # 5. Backtracking Line Search\n            alpha = 1.0\n            phi_x = f_val + mu * max(0, c_val)\n            dir_deriv = np.dot(grad_f, p) - mu * max(0, c_val)\n            \n            if dir_deriv >= 0:\n                 return max_iter\n            \n            alpha_min = 1e-10 \n            \n            while alpha > alpha_min:\n                x_new = x + alpha * s\n                phi_new = merit_func(x_new, kappa, mu)\n\n                if phi_new = phi_x + sigma * alpha * dir_deriv:\n                    break\n                \n                alpha *= beta\n            else:\n                return max_iter\n            \n            # 6. Update iterate\n            x = x + alpha * s\n            \n        return max_iter\n        \n    # --- Main Experiment ---\n    kappa_values = [0.0, 0.5, 2.0, 5.0, 10.0]\n    improvements = []\n    \n    for kappa in kappa_values:\n        n_base = sqp_solver(kappa, use_soc=False)\n        n_soc = sqp_solver(kappa, use_soc=True)\n        delta = n_base - n_soc\n        improvements.append(delta)\n\n    print(f\"[{','.join(map(str, improvements))}]\")\n\nsolve()\n```"
        }
    ]
}