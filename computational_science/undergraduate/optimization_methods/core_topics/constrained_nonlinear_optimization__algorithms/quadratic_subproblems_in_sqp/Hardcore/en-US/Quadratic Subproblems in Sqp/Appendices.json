{
    "hands_on_practices": [
        {
            "introduction": "At the heart of Sequential Quadratic Programming (SQP) lies a powerful simplification: replacing complex nonlinear constraints with manageable linear approximations. While this makes the subproblem solvable, the approximation is rarely perfect. This first exercise  provides a concrete look at the discrepancy, or \"gap,\" between the true nonlinear constraint and its first-order linearization, helping to build crucial intuition about why SQP is an iterative process.",
            "id": "3169627",
            "problem": "Consider Sequential Quadratic Programming (SQP), which constructs at a current iterate $x_k$ a quadratic approximation of the objective and a linearization of the constraints to define a search direction $d$. Let there be a single nonlinear inequality constraint $g(x) \\le 0$ with $x \\in \\mathbb{R}$. At the iterate $x_k = \\frac{3}{10}$, take the constraint to be $g(x) = x^2 - \\frac{1}{10}$. Use a quadratic model of the objective with Hessian $H_k = 1$ and gradient $\\nabla f(x_k) = -1$, so the SQP subproblem is\n$$\n\\min_{d \\in \\mathbb{R}} \\;\\; \\frac{1}{2} d^2 - d \\quad \\text{subject to} \\quad g(x_k) + \\nabla g(x_k)\\, d \\le 0.\n$$\nSolve this subproblem to obtain the step $d_k$, and verify that the linearized constraint is satisfied at $d_k$ while the true constraint $g(x_k + d_k)$ is violated. Then quantify the discrepancy between the true constraint and its linearized prediction at the step by computing\n$$\n\\Delta \\;=\\; g(x_k + d_k) \\;-\\; \\big( g(x_k) + \\nabla g(x_k)\\, d_k \\big).\n$$\nProvide the exact value of $\\Delta$ as a reduced fraction. No rounding is required.",
            "solution": "The user wants to solve a Sequential Quadratic Programming (SQP) subproblem and analyze the discrepancy between the linearized constraint and the true constraint.\n\n### Step 1: Extract Givens\n- **Optimization Method**: Sequential Quadratic Programming (SQP).\n- **Current Iterate**: $x_k = \\frac{3}{10}$, where $x \\in \\mathbb{R}$.\n- **Objective Function Model**:\n    - Hessian: $H_k = 1$.\n    - Gradient: $\\nabla f(x_k) = -1$.\n- **Constraint Function**:\n    - A single nonlinear inequality constraint: $g(x) \\le 0$.\n    - Specific form: $g(x) = x^2 - \\frac{1}{10}$.\n- **SQP Subproblem Definition**:\n    $$ \\min_{d \\in \\mathbb{R}} \\;\\; \\frac{1}{2} d^2 - d \\quad \\text{subject to} \\quad g(x_k) + \\nabla g(x_k)\\, d \\le 0 $$\n- **Tasks**:\n    1.  Solve the subproblem for the step $d_k$.\n    2.  Verify that the linearized constraint $g(x_k) + \\nabla g(x_k)\\, d_k \\le 0$ is satisfied.\n    3.  Verify that the true constraint $g(x_k + d_k) \\le 0$ is violated.\n    4.  Compute the discrepancy: $\\Delta = g(x_k + d_k) - \\big( g(x_k) + \\nabla g(x_k)\\, d_k \\big)$.\n    5.  Provide $\\Delta$ as an exact reduced fraction.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation.\n- **Scientific Groundedness**: The problem is well-grounded in the established theory of nonlinear optimization, specifically the SQP method. The formulation of the quadratic subproblem is standard.\n- **Well-Posedness**: The objective function $\\frac{1}{2} d^2 - d$ is a strictly convex quadratic function (its second derivative is $1 > 0$), and the constraint is linear in the variable $d$. This defines a convex Quadratic Program (QP), which is known to have a unique solution. The problem is well-posed.\n- **Objectivity**: The language is precise and mathematical, with no subjective or ambiguous terms.\n- **Completeness**: All necessary functions ($g(x)$), values ($x_k$), and model parameters ($H_k$, $\\nabla f(x_k)$) are provided to formulate and solve the subproblem.\n- **Other Flaws**: The problem does not exhibit any other flaws listed in the validation criteria. It is a standard textbook exercise.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\nThe solution proceeds by first constructing the specific quadratic programming (QP) subproblem, solving it, and then performing the requested verifications and calculations.\n\n**1. Formulate the QP Subproblem**\n\nWe need to compute the terms for the linearized constraint $g(x_k) + \\nabla g(x_k)\\, d \\le 0$.\nThe constraint function is $g(x) = x^2 - \\frac{1}{10}$.\nThe current iterate is $x_k = \\frac{3}{10}$.\n\nFirst, evaluate the constraint function at $x_k$:\n$$ g(x_k) = g\\left(\\frac{3}{10}\\right) = \\left(\\frac{3}{10}\\right)^2 - \\frac{1}{10} = \\frac{9}{100} - \\frac{10}{100} = -\\frac{1}{100} $$\n\nNext, compute the gradient of the constraint function, $\\nabla g(x)$, and evaluate it at $x_k$:\n$$ \\nabla g(x) = \\frac{d}{dx}\\left(x^2 - \\frac{1}{10}\\right) = 2x $$\n$$ \\nabla g(x_k) = \\nabla g\\left(\\frac{3}{10}\\right) = 2\\left(\\frac{3}{10}\\right) = \\frac{6}{10} = \\frac{3}{5} $$\n\nThe objective function of the subproblem is given as $\\frac{1}{2}d^2 - d$.\nThe linearized constraint is $g(x_k) + \\nabla g(x_k) d \\le 0$, which becomes:\n$$ -\\frac{1}{100} + \\frac{3}{5}d \\le 0 $$\n\nThus, the complete SQP subproblem is:\n$$ \\min_{d \\in \\mathbb{R}} \\;\\; \\frac{1}{2} d^2 - d \\quad \\text{subject to} \\quad \\frac{3}{5}d \\le \\frac{1}{100} $$\n\n**2. Solve the Subproblem for the Step $d_k$**\n\nThis is a constrained optimization problem. We can solve it using the Karush-Kuhn-Tucker (KKT) conditions. Let the objective be $q(d) = \\frac{1}{2}d^2 - d$.\nThe Lagrangian is:\n$$ \\mathcal{L}(d, \\lambda) = \\frac{1}{2}d^2 - d + \\lambda \\left(-\\frac{1}{100} + \\frac{3}{5}d\\right) $$\nwhere $\\lambda \\ge 0$ is the Lagrange multiplier.\n\nThe KKT conditions are:\n1.  **Stationarity**: $\\frac{\\partial \\mathcal{L}}{\\partial d} = d - 1 + \\frac{3}{5}\\lambda = 0$\n2.  **Primal Feasibility**: $-\\frac{1}{100} + \\frac{3}{5}d \\le 0$\n3.  **Dual Feasibility**: $\\lambda \\ge 0$\n4.  **Complementary Slackness**: $\\lambda \\left(-\\frac{1}{100} + \\frac{3}{5}d\\right) = 0$\n\nFrom complementary slackness, we have two cases:\n\nCase A: The constraint is inactive, so $\\lambda = 0$.\nThe stationarity condition gives $d - 1 + 0 = 0$, which implies $d = 1$.\nWe check primal feasibility: $-\\frac{1}{100} + \\frac{3}{5}(1) = -\\frac{1}{100} + \\frac{60}{100} = \\frac{59}{100}$. This is not less than or equal to $0$. So, this case is not the solution. The unconstrained minimum of $q(d)$ occurs at $d=1$, which violates the constraint.\n\nCase B: The constraint is active, so $-\\frac{1}{100} + \\frac{3}{5}d = 0$.\nFrom this equation, we can solve for $d$:\n$$ \\frac{3}{5}d = \\frac{1}{100} \\implies d = \\frac{1}{100} \\cdot \\frac{5}{3} = \\frac{5}{300} = \\frac{1}{60} $$\nThis is our candidate for the step, $d_k = \\frac{1}{60}$.\nNow we use the stationarity condition to find $\\lambda$:\n$$ d - 1 + \\frac{3}{5}\\lambda = 0 \\implies \\frac{1}{60} - 1 + \\frac{3}{5}\\lambda = 0 $$\n$$ -\\frac{59}{60} + \\frac{3}{5}\\lambda = 0 \\implies \\frac{3}{5}\\lambda = \\frac{59}{60} $$\n$$ \\lambda = \\frac{59}{60} \\cdot \\frac{5}{3} = \\frac{59 \\cdot 5}{60 \\cdot 3} = \\frac{59}{12 \\cdot 3} = \\frac{59}{36} $$\nWe check dual feasibility: $\\lambda = \\frac{59}{36} > 0$. This is consistent with an active constraint.\nTherefore, the solution to the subproblem is $d_k = \\frac{1}{60}$.\n\n**3. Verify Constraint Satisfaction and Violation**\n\nLet's verify the two constraints as required.\n- **Linearized Constraint**: We need to check if $g(x_k) + \\nabla g(x_k) d_k \\le 0$.\n$$ g(x_k) + \\nabla g(x_k) d_k = -\\frac{1}{100} + \\frac{3}{5} \\left(\\frac{1}{60}\\right) = -\\frac{1}{100} + \\frac{3}{300} = -\\frac{1}{100} + \\frac{1}{100} = 0 $$\nSince $0 \\le 0$ is true, the linearized constraint is satisfied (it is active).\n\n- **True Constraint**: We need to check if $g(x_k + d_k) \\le 0$.\nFirst, find the new point $x_k + d_k$:\n$$ x_k + d_k = \\frac{3}{10} + \\frac{1}{60} = \\frac{18}{60} + \\frac{1}{60} = \\frac{19}{60} $$\nNow, evaluate the true constraint function $g(x)$ at this new point:\n$$ g(x_k + d_k) = g\\left(\\frac{19}{60}\\right) = \\left(\\frac{19}{60}\\right)^2 - \\frac{1}{10} = \\frac{19^2}{60^2} - \\frac{1}{10} = \\frac{361}{3600} - \\frac{1}{10} $$\n$$ g(x_k + d_k) = \\frac{361}{3600} - \\frac{360}{3600} = \\frac{1}{3600} $$\nThe value of the true constraint is $\\frac{1}{3600}$. Since $\\frac{1}{3600} > 0$, the condition $g(x_k + d_k) \\le 0$ is **violated**.\n\n**4. Compute the Discrepancy $\\Delta$**\n\nThe discrepancy is defined as $\\Delta = g(x_k + d_k) - \\big( g(x_k) + \\nabla g(x_k) d_k \\big)$.\nFrom our previous calculations:\n- $g(x_k + d_k) = \\frac{1}{3600}$\n- $g(x_k) + \\nabla g(x_k) d_k = 0$\n\nSubstituting these values into the expression for $\\Delta$:\n$$ \\Delta = \\frac{1}{3600} - 0 = \\frac{1}{3600} $$\nThis discrepancy arises because the linearization $g(x_k) + \\nabla g(x_k) d$ is a first-order Taylor approximation of $g(x_k+d)$. The error in this approximation is given by the higher-order terms. For a quadratic function like $g(x) = x^2 - \\frac{1}{10}$, the Taylor series is exact and terminates after the second-order term:\n$$ g(x_k + d) = g(x_k) + (\\nabla g(x_k)) d + \\frac{1}{2} (\\nabla^2 g(x_k)) d^2 $$\nThe discrepancy is therefore exactly the second-order term:\n$$ \\Delta = g(x_k+d_k) - (g(x_k) + \\nabla g(x_k) d_k) = \\frac{1}{2} (\\nabla^2 g(x_k)) (d_k)^2 $$\nWe can verify this. The second derivative is $\\nabla^2 g(x) = 2$. So, $\\nabla^2 g(x_k) = 2$.\n$$ \\Delta = \\frac{1}{2} (2) \\left(\\frac{1}{60}\\right)^2 = \\left(\\frac{1}{60}\\right)^2 = \\frac{1}{3600} $$\nThis confirms our result. The fraction $\\frac{1}{3600}$ is already in its most reduced form.",
            "answer": "$$\\boxed{\\frac{1}{3600}}$$"
        },
        {
            "introduction": "Building on the concept of linearization, we now tackle the construction and solution of a complete quadratic subproblem for a nonlinear program with inequality constraints. This practice  is a comprehensive workout that involves defining the quadratic objective and linearized constraints, and then solving the resulting QP to find the search direction. It demonstrates the core mechanics of generating a single SQP step and connects it to globalization strategies through the use of a merit function.",
            "id": "3195671",
            "problem": "Consider the nonlinear program with two inequality constraints:\n$$\n\\begin{aligned}\n\\min_{x \\in \\mathbb{R}^2}\\quad & f(x) \\\\\n\\text{s.t.}\\quad & g_1(x) \\le 0, \\\\\n& g_2(x) \\le 0,\n\\end{aligned}\n$$\nwhere\n$$\nf(x) = (x_1 - 3)^2 + (x_2 - 3)^2,\\quad\ng_1(x) = x_1 + x_2 - 2,\\quad\ng_2(x) = x_1^2 + x_2 - 2.\n$$\nStarting from the point $x^{(0)} = (1,\\;1.5)$, construct a single Sequential Quadratic Programming (SQP) step using the identity matrix $B^{(0)} = I_2$ as the initial symmetric positive definite approximation of the Hessian of the Lagrangian. Use the standard SQP quadratic subproblem formed by the first-order Taylor expansion of the constraints and the quadratic model with $B^{(0)}$ of the objective. Derive this quadratic subproblem explicitly and solve it to obtain the step $p^{(0)}$.\n\nTo assess globalization with an exact penalty merit function, consider the $\\ell_1$ exact penalty merit function\n$$\n\\phi_{\\mu}(x) = f(x) + \\mu\\left(|\\max\\{0, g_1(x)\\}| + |\\max\\{0, g_2(x)\\}|\\right),\n$$\nwith penalty parameter $\\mu \\ge 0$. Compute the directional derivative of $\\phi_{\\mu}$ at $x^{(0)}$ along the SQP step $p^{(0)}$, and determine the smallest threshold value $\\mu_{\\min}$ such that for all $\\mu > \\mu_{\\min}$, the SQP step is a strict descent direction for $\\phi_{\\mu}$ at $x^{(0)}$.\n\nYour final answer must be the single real number $\\mu_{\\min}$. No rounding is required; provide an exact value. Report only $\\mu_{\\min}$ as your final answer.",
            "solution": "The user wants me to solve a multi-part optimization problem.\n\nHere's a breakdown of the tasks:\n1.  **Validate the Problem**: Check if the problem statement is well-posed, scientifically sound, and complete.\n2.  **Construct SQP Subproblem**: Formulate the quadratic programming (QP) subproblem at the initial point $x^{(0)} = (1, 1.5)$. This involves:\n    *   Calculating the gradient of the objective function $f(x)$ at $x^{(0)}$.\n    *   Calculating the values and gradients of the constraint functions $g_1(x)$ and $g_2(x)$ at $x^{(0)}$.\n    *   Using the given approximation of the Hessian of the Lagrangian, $B^{(0)} = I_2$.\n    *   Writing down the QP subproblem in terms of the step $p = (p_1, p_2)$.\n3.  **Solve the QP Subproblem**: Solve the derived QP for the step $p^{(0)}$. This means finding the optimal $p_1$ and $p_2$. This is a constrained optimization problem in itself. I will likely need to use the Karush-Kuhn-Tucker (KKT) conditions for the subproblem.\n4.  **Analyze the Merit Function**:\n    *   Define the $\\ell_1$ exact penalty merit function $\\phi_{\\mu}(x)$.\n    *   Calculate its directional derivative at $x^{(0)}$ in the direction $p^{(0)}$, which is denoted as $D_{p^{(0)}} \\phi_{\\mu}(x^{(0)})$.\n    *   Determine the condition for $p^{(0)}$ to be a strict descent direction, which is $D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) < 0$.\n    *   Solve this inequality for $\\mu$ to find the smallest threshold $\\mu_{\\min}$.\n\n**Part 1: Problem Validation**\n\n*   **Extract Givens**:\n    *   Problem type: Nonlinear program with inequality constraints.\n    *   Objective function: $f(x) = (x_1 - 3)^2 + (x_2 - 3)^2$.\n    *   Constraint 1: $g_1(x) = x_1 + x_2 - 2 \\le 0$.\n    *   Constraint 2: $g_2(x) = x_1^2 + x_2 - 2 \\le 0$.\n    *   Starting point: $x^{(0)} = (1, 1.5)$.\n    *   Initial Hessian approximation: $B^{(0)} = I_2$ (the $2 \\times 2$ identity matrix).\n    *   Task 1: Construct and solve the SQP quadratic subproblem for the step $p^{(0)}$.\n    *   Merit function: $\\phi_{\\mu}(x) = f(x) + \\mu\\left(|\\max\\{0, g_1(x)\\}| + |\\max\\{0, g_2(x)\\}|\\right)$.\n    *   Task 2: Compute the directional derivative of $\\phi_{\\mu}$ at $x^{(0)}$ along $p^{(0)}$.\n    *   Task 3: Find the smallest $\\mu_{\\min}$ such that for $\\mu > \\mu_{\\min}$, $D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) < 0$.\n\n*   **Validate Using Extracted Givens**:\n    *   **Scientifically Grounded**: The problem is a standard exercise in the field of nonlinear optimization, specifically using the Sequential Quadratic Programming method. All functions are well-defined polynomials. The concepts of SQP, merit functions, and directional derivatives are standard in optimization theory. The problem is mathematically sound.\n    *   **Well-Posed**: The problem asks for specific calculations based on given data. The functions are smooth. The QP subproblem is convex and should have a unique solution. The final part asks for a threshold value, which should be uniquely determinable from the preceding calculations.\n    *   **Objective**: The language is precise and mathematical. No subjective terms are used.\n    *   **Completeness**: All necessary information is provided: the functions, the starting point, the Hessian approximation, the definition of the merit function, and the specific tasks to perform. The problem is self-contained.\n    *   **Feasibility/Realism**: The problem is a mathematical construct, so physical realism is not applicable. The values and functions are reasonable for a textbook-style problem.\n    *   **Ill-Posed/Poorly Structured**: No issues here. The structure is logical and guides the solver through a standard procedure.\n    *   **Trivial/Tautological**: The problem requires several steps of calculation and application of optimization theory. It is not trivial.\n    *   **Topic Relevance**: The problem is clearly about *unconstrained and constrained optimization* within the field *optimization methods*. It specifically deals with SQP, a core algorithm for constrained optimization.\n\n*   **Verdict**: The problem is valid. I will proceed with the solution.\n\n**Part 2: Solution Steps**\n\n**Step A: Construct the SQP Subproblem**\n\nThe general form of the SQP subproblem at iteration $k$ is:\n$$\n\\begin{aligned}\n\\min_{p \\in \\mathbb{R}^n}\\quad & f(x^{(k)}) + \\nabla f(x^{(k)})^T p + \\frac{1}{2} p^T B^{(k)} p \\\\\n\\text{s.t.}\\quad & g_i(x^{(k)}) + \\nabla g_i(x^{(k)})^T p \\le 0, \\quad i \\in \\mathcal{I}\n\\end{aligned}\n$$\nHere, $k=0$ and $n=2$. $x^{(0)} = (1, 1.5)$. $B^{(0)} = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n\nFirst, I need to compute the function values and gradients at $x^{(0)}$.\n\n*   **Objective function $f(x)$**:\n    *   $f(x) = (x_1 - 3)^2 + (x_2 - 3)^2$\n    *   $\\nabla f(x) = \\begin{pmatrix} 2(x_1 - 3) \\\\ 2(x_2 - 3) \\end{pmatrix}$\n    *   At $x^{(0)} = (1, 1.5)$:\n        *   $f(x^{(0)}) = (1 - 3)^2 + (1.5 - 3)^2 = (-2)^2 + (-1.5)^2 = 4 + 2.25 = 6.25$.\n        *   $\\nabla f(x^{(0)}) = \\begin{pmatrix} 2(1 - 3) \\\\ 2(1.5 - 3) \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix}$.\n\n*   **Constraint function $g_1(x)$**:\n    *   $g_1(x) = x_1 + x_2 - 2$\n    *   $\\nabla g_1(x) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n    *   At $x^{(0)} = (1, 1.5)$:\n        *   $g_1(x^{(0)}) = 1 + 1.5 - 2 = 0.5$.\n        *   $\\nabla g_1(x^{(0)}) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\n*   **Constraint function $g_2(x)$**:\n    *   $g_2(x) = x_1^2 + x_2 - 2$\n    *   $\\nabla g_2(x) = \\begin{pmatrix} 2x_1 \\\\ 1 \\end{pmatrix}$\n    *   At $x^{(0)} = (1, 1.5)$:\n        *   $g_2(x^{(0)}) = 1^2 + 1.5 - 2 = 1 + 1.5 - 2 = 0.5$.\n        *   $\\nabla g_2(x^{(0)}) = \\begin{pmatrix} 2(1) \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\n\nNow, substitute these into the subproblem template. Let $p = (p_1, p_2)^T$.\nThe objective of the subproblem is:\n$$\n\\min_{p \\in \\mathbb{R}^2} \\quad 6.25 + \\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix}^T \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix}^T \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix}\n$$\nThe constant term $f(x^{(0)})$ does not affect the minimizer $p$, so we can drop it. The objective simplifies to:\n$$\n\\min_{p \\in \\mathbb{R}^2} \\quad -4 p_1 - 3 p_2 + \\frac{1}{2} (p_1^2 + p_2^2)\n$$\nThe constraints of the subproblem are:\n1.  \n    $g_1(x^{(0)}) + \\nabla g_1(x^{(0)})^T p \\le 0$\n    $0.5 + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}^T \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix} \\le 0 \\implies p_1 + p_2 + 0.5 \\le 0$\n2.  \n    $g_2(x^{(0)}) + \\nabla g_2(x^{(0)})^T p \\le 0$\n    $0.5 + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}^T \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix} \\le 0 \\implies 2p_1 + p_2 + 0.5 \\le 0$\n\nSo, the explicit QP subproblem is:\n$$\n\\begin{aligned}\n\\min_{p_1, p_2} \\quad & \\frac{1}{2} p_1^2 + \\frac{1}{2} p_2^2 - 4 p_1 - 3 p_2 \\\\\n\\text{s.t.} \\quad & p_1 + p_2 + 0.5 \\le 0 \\\\\n& 2p_1 + p_2 + 0.5 \\le 0\n\\end{aligned}\n$$\n\n**Step B: Solve the QP Subproblem**\n\nThis is a convex QP. I will use the KKT conditions to solve it.\nLet the objective be $Q(p) = \\frac{1}{2} p_1^2 + \\frac{1}{2} p_2^2 - 4 p_1 - 3 p_2$.\nLet the constraints be $c_1(p) = p_1 + p_2 + 0.5 \\le 0$ and $c_2(p) = 2p_1 + p_2 + 0.5 \\le 0$.\nThe Lagrangian is:\n$$\n\\mathcal{L}(p_1, p_2, \\lambda_1, \\lambda_2) = Q(p) + \\lambda_1 c_1(p) + \\lambda_2 c_2(p)\n$$\nwhere $\\lambda_1, \\lambda_2 \\ge 0$ are the Lagrange multipliers.\n\nThe KKT conditions are:\n1.  **Stationarity**: $\\nabla_p \\mathcal{L} = 0$\n    $$\n    \\nabla_p Q(p) + \\lambda_1 \\nabla_p c_1(p) + \\lambda_2 \\nabla_p c_2(p) = 0\n    $$\n    $$\n    \\begin{pmatrix} p_1 - 4 \\\\ p_2 - 3 \\end{pmatrix} + \\lambda_1 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\lambda_2 \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n    $$\n    This gives two equations:\n    (i) $p_1 - 4 + \\lambda_1 + 2\\lambda_2 = 0 \\implies p_1 = 4 - \\lambda_1 - 2\\lambda_2$\n    (ii) $p_2 - 3 + \\lambda_1 + \\lambda_2 = 0 \\implies p_2 = 3 - \\lambda_1 - \\lambda_2$\n\n2.  **Primal Feasibility**:\n    (iii) $p_1 + p_2 + 0.5 \\le 0$\n    (iv) $2p_1 + p_2 + 0.5 \\le 0$\n\n3.  **Dual Feasibility**:\n    (v) $\\lambda_1 \\ge 0$\n    (vi) $\\lambda_2 \\ge 0$\n\n4.  **Complementary Slackness**:\n    (vii) $\\lambda_1 (p_1 + p_2 + 0.5) = 0$\n    (viii) $\\lambda_2 (2p_1 + p_2 + 0.5) = 0$\n\nLet's test the cases for active constraints.\n\n*   Case 1: No constraints are active. $\\lambda_1 = 0, \\lambda_2 = 0$.\n    From (i), $p_1 = 4$. From (ii), $p_2 = 3$.\n    Check feasibility:\n    $c_1(p) = 4 + 3 + 0.5 = 7.5 \\not\\le 0$. This case is not the solution.\n\n*   Case 2: Constraint 1 is active, constraint 2 is not. $p_1 + p_2 + 0.5 = 0, \\lambda_2 = 0, \\lambda_1 > 0$.\n    From (i), $p_1 = 4 - \\lambda_1$.\n    From (ii), $p_2 = 3 - \\lambda_1$.\n    Substitute into the active constraint:\n    $(4 - \\lambda_1) + (3 - \\lambda_1) + 0.5 = 0$\n    $7.5 - 2\\lambda_1 = 0 \\implies \\lambda_1 = 3.75$.\n    This satisfies $\\lambda_1 > 0$.\n    Now find $p_1, p_2$:\n    $p_1 = 4 - 3.75 = 0.25$.\n    $p_2 = 3 - 3.75 = -0.75$.\n    Check feasibility for constraint 2:\n    $c_2(p) = 2(0.25) + (-0.75) + 0.5 = 0.5 - 0.75 + 0.5 = 0.25 \\not\\le 0$. This case is not the solution.\n\n*   Case 3: Constraint 2 is active, constraint 1 is not. $2p_1 + p_2 + 0.5 = 0, \\lambda_1 = 0, \\lambda_2 > 0$.\n    From (i), $p_1 = 4 - 2\\lambda_2$.\n    From (ii), $p_2 = 3 - \\lambda_2$.\n    Substitute into the active constraint:\n    $2(4 - 2\\lambda_2) + (3 - \\lambda_2) + 0.5 = 0$\n    $8 - 4\\lambda_2 + 3 - \\lambda_2 + 0.5 = 0$\n    $11.5 - 5\\lambda_2 = 0 \\implies \\lambda_2 = 11.5 / 5 = 2.3$.\n    This satisfies $\\lambda_2 > 0$.\n    Now find $p_1, p_2$:\n    $p_1 = 4 - 2(2.3) = 4 - 4.6 = -0.6$.\n    $p_2 = 3 - 2.3 = 0.7$.\n    Check feasibility for constraint 1:\n    $c_1(p) = -0.6 + 0.7 + 0.5 = 0.6 \\not\\le 0$. This case is not the solution.\n\n*   Case 4: Both constraints are active. $p_1 + p_2 + 0.5 = 0, 2p_1 + p_2 + 0.5 = 0, \\lambda_1 > 0, \\lambda_2 > 0$.\n    This is a system of two linear equations for $p_1, p_2$:\n    (a) $p_1 + p_2 = -0.5$\n    (b) $2p_1 + p_2 = -0.5$\n    Subtract (a) from (b): $(2p_1 - p_1) + (p_2 - p_2) = -0.5 - (-0.5) \\implies p_1 = 0$.\n    Substitute $p_1=0$ into (a): $0 + p_2 = -0.5 \\implies p_2 = -0.5$.\n    So $p^{(0)} = (0, -0.5)^T$.\n    Now find $\\lambda_1, \\lambda_2$ using the stationarity conditions (i) and (ii):\n    $p_1 = 0 \\implies 4 - \\lambda_1 - 2\\lambda_2 = 0 \\implies \\lambda_1 + 2\\lambda_2 = 4$.\n    $p_2 = -0.5 \\implies 3 - \\lambda_1 - \\lambda_2 = -0.5 \\implies \\lambda_1 + \\lambda_2 = 3.5$.\n    This is a system for $\\lambda_1, \\lambda_2$.\n    Subtract the second from the first: $(\\lambda_1 - \\lambda_1) + (2\\lambda_2 - \\lambda_2) = 4 - 3.5 \\implies \\lambda_2 = 0.5$.\n    Substitute $\\lambda_2=0.5$ into the second equation: $\\lambda_1 + 0.5 = 3.5 \\implies \\lambda_1 = 3$.\n    Check dual feasibility: $\\lambda_1 = 3 > 0$ and $\\lambda_2 = 0.5 > 0$. All conditions are satisfied.\n\nSo the solution to the QP subproblem is $p^{(0)} = \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix}$.\nThe Lagrange multipliers for the subproblem are $\\lambda^{(1)} = (\\lambda_1, \\lambda_2)^T = (3, 0.5)^T$. These are the estimates for the multipliers of the original problem for the next iteration.\n\nSummary of results so far:\n$x^{(0)} = (1, 1.5)^T$\n$p^{(0)} = (0, -0.5)^T$\n$\\nabla f(x^{(0)}) = (-4, -3)^T$\n$g_1(x^{(0)}) = 0.5$\n$g_2(x^{(0)}) = 0.5$\n$\\nabla g_1(x^{(0)}) = (1, 1)^T$\n$\\nabla g_2(x^{(0)}) = (2, 1)^T$\nMultipliers from QP: $\\lambda_1=3, \\lambda_2=0.5$.\n\n**Step C: Directional Derivative of the Merit Function**\n\nThe $\\ell_1$ exact penalty merit function is:\n$$\n\\phi_{\\mu}(x) = f(x) + \\mu\\left(|\\max\\{0, g_1(x)\\}| + |\\max\\{0, g_2(x)\\}|\\right)\n$$\nWe need to compute the directional derivative of $\\phi_{\\mu}$ at $x^{(0)}$ in the direction $p^{(0)}$.\nThe formula for the directional derivative of $\\phi_{\\mu}$ at a point $x$ in a direction $p$ is:\n$$\nD_p \\phi_{\\mu}(x) = \\nabla f(x)^T p + \\mu D_p \\left( \\sum_{i=1}^m |\\max\\{0, g_i(x)\\}| \\right)\n$$\nAt points where the constraints are violated, i.e., $g_i(x) > 0$, the term $|\\max\\{0, g_i(x)\\}|$ is simply $g_i(x)$, which is differentiable. Its directional derivative is $\\nabla g_i(x)^T p$.\nAt points where the constraints are satisfied, i.e., $g_i(x) \\le 0$, the term $|\\max\\{0, g_i(x)\\}|$ is $0$ in a neighborhood if $g_i(x) < 0$. If $g_i(x)=0$, the directional derivative is $\\max\\{0, \\nabla g_i(x)^T p\\}$.\nIn our case, at $x^{(0)}$, we have $g_1(x^{(0)}) = 0.5 > 0$ and $g_2(x^{(0)}) = 0.5 > 0$.\nSo, $\\phi_{\\mu}(x)$ is differentiable at $x^{(0)}$ and in its neighborhood, it is given by:\n$$\n\\phi_{\\mu}(x) = f(x) + \\mu(g_1(x) + g_2(x))\n$$\nThe directional derivative is therefore:\n$$\nD_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = \\nabla \\phi_{\\mu}(x^{(0)})^T p^{(0)}\n$$\nwhere $\\nabla \\phi_{\\mu}(x^{(0)}) = \\nabla f(x^{(0)}) + \\mu(\\nabla g_1(x^{(0)}) + \\nabla g_2(x^{(0)}))$.\nSo,\n$$\nD_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = (\\nabla f(x^{(0)}) + \\mu(\\nabla g_1(x^{(0)}) + \\nabla g_2(x^{(0)})))^T p^{(0)}\n$$\n$$\nD_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = \\nabla f(x^{(0)})^T p^{(0)} + \\mu(\\nabla g_1(x^{(0)})^T p^{(0)} + \\nabla g_2(x^{(0)})^T p^{(0)})\n$$\nLet's compute the components.\n*   $\\nabla f(x^{(0)})^T p^{(0)} = \\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix}^T \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix} = (-4)(0) + (-3)(-0.5) = 1.5$.\n*   $\\nabla g_1(x^{(0)})^T p^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}^T \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix} = (1)(0) + (1)(-0.5) = -0.5$.\n*   $\\nabla g_2(x^{(0)})^T p^{(0)} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}^T \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix} = (2)(0) + (1)(-0.5) = -0.5$.\n\nSubstituting these into the expression for the directional derivative:\n$$\nD_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = 1.5 + \\mu(-0.5 + (-0.5)) = 1.5 - \\mu\n$$\n\nAlternatively, there's a general formula for the directional derivative of the $\\ell_1$ penalty function. For a general NLP $\\min f(x)$ s.t. $c(x) \\le 0$, the directional derivative of $\\phi_\\mu(x) = f(x) + \\mu \\sum_i \\max(0, c_i(x))$ in direction $p$ is:\n$$ D_p \\phi_\\mu(x) = \\nabla f(x)^T p + \\mu \\sum_{i \\in I_0(x)} \\max(0, \\nabla c_i(x)^T p) + \\mu \\sum_{i \\in I_+(x)} \\nabla c_i(x)^T p $$\nwhere $I_+(x) = \\{i | c_i(x) > 0\\}$ and $I_0(x) = \\{i | c_i(x) = 0\\}$.\nAt our point $x^{(0)}$, $g_1(x^{(0)})=0.5$ and $g_2(x^{(0)})=0.5$. So both are in $I_+(x^{(0)})$. The set $I_0(x^{(0)})$ is empty.\nSo the formula becomes:\n$$ D_{p^{(0)}} \\phi_\\mu(x^{(0)}) = \\nabla f(x^{(0)})^T p^{(0)} + \\mu (\\nabla g_1(x^{(0)})^T p^{(0)} + \\nabla g_2(x^{(0)})^T p^{(0)}) $$\nThis is exactly what I derived earlier. My previous derivation was correct.\nSo, $D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = 1.5 - \\mu$.\n\n**Step D: Find the Threshold $\\mu_{\\min}$**\n\nThe SQP step $p^{(0)}$ is a strict descent direction for $\\phi_{\\mu}$ at $x^{(0)}$ if the directional derivative is strictly negative:\n$$\nD_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) < 0\n$$\nSubstituting the expression we found:\n$$\n1.5 - \\mu < 0\n$$\n$$\n1.5 < \\mu\n$$\nSo, for all $\\mu > 1.5$, the step $p^{(0)}$ is a strict descent direction.\nThe question asks for the smallest threshold value $\\mu_{\\min}$ such that for all $\\mu > \\mu_{\\min}$, the condition holds.\nThis directly gives $\\mu_{\\min} = 1.5$.\n\nLet's double-check the theory.\nThe directional derivative is the key. $p$ is a descent direction if $D_p \\phi < 0$.\nThe problem requires a *strict* descent direction. $D_p \\phi < 0$.\nMy inequality $1.5 - \\mu < 0$ seems correct. This implies $\\mu > 1.5$.\nThe smallest value that works as a lower bound for $\\mu$ is $1.5$.\nSo $\\mu_{\\min}=1.5$.\n\nLet's check this result against a known theorem in numerical optimization (e.g., Nocedal & Wright, \"Numerical Optimization\").\nA common condition for the penalty parameter $\\mu$ is that it must satisfy $\\mu \\ge \\|\\lambda^{(k+1)}\\|_\\infty$, where $\\lambda^{(k+1)}$ are the multipliers from the QP subproblem.\nIn our case, the multipliers are $\\lambda^{(1)} = (\\lambda_1, \\lambda_2)^T = (3, 0.5)^T$.\nThe infinity norm is $\\|\\lambda^{(1)}\\|_\\infty = \\max\\{|\\lambda_1|, |\\lambda_2|\\} = \\max\\{3, 0.5\\} = 3$.\nThis suggests that perhaps $\\mu$ should be greater than $3$. Why is my result different?\n\nLet's re-examine the condition for descent.\nThe objective function of the QP is $Q(p) = \\nabla f(x^{(k)})^T p + \\frac{1}{2} p^T B^{(k)} p$.\nThe constraints are $\\nabla g_i(x^{(k)})^T p \\le -g_i(x^{(k)})$.\nLet $p^{(k)}$ be the solution of the QP and $\\lambda^{(k+1)}$ be the multipliers.\nThe KKT conditions for the QP are:\n$\\nabla f(x^{(k)}) + B^{(k)} p^{(k)} + \\sum_i \\lambda_i^{(k+1)} \\nabla g_i(x^{(k)}) = 0$.\nThis means $\\nabla f(x^{(k)})^T p^{(k)} + (p^{(k)})^T B^{(k)} p^{(k)} + \\sum_i \\lambda_i^{(k+1)} \\nabla g_i(x^{(k)})^T p^{(k)} = 0$. This is multiplying by $(p^{(k)})^T$.\nFrom this, we get $\\nabla f(x^{(k)})^T p^{(k)} = - (p^{(k)})^T B^{(k)} p^{(k)} - \\sum_i \\lambda_i^{(k+1)} \\nabla g_i(x^{(k)})^T p^{(k)}$.\n\nThe directional derivative is:\n$D_{p^{(k)}} \\phi_{\\mu}(x^{(k)}) = \\nabla f(x^{(k)})^T p^{(k)} + \\mu \\sum_{i \\in \\mathcal{I}} D_{p^{(k)}} (|\\max\\{0, g_i(x^{(k)})\\}|)$.\nLet's re-evaluate the derivative part for the constraints.\n$D_{p^{(k)}} \\sum_i |\\max(0,g_i)| = \\sum_{g_i > 0} \\nabla g_i^T p^{(k)} + \\sum_{g_i=0} \\max(0, \\nabla g_i^T p^{(k)})$.\nAt $x^{(0)}$, $g_1=0.5>0$ and $g_2=0.5>0$.\nSo $D = \\nabla f^T p + \\mu(\\nabla g_1^T p + \\nabla g_2^T p)$.\nThis is correct.\nThe linearized constraints are active at the solution $p^{(0)}$:\n$g_1(x^{(0)}) + \\nabla g_1(x^{(0)})^T p^{(0)} = 0 \\implies \\nabla g_1(x^{(0)})^T p^{(0)} = -g_1(x^{(0)}) = -0.5$.\n$g_2(x^{(0)}) + \\nabla g_2(x^{(0)})^T p^{(0)} = 0 \\implies \\nabla g_2(x^{(0)})^T p^{(0)} = -g_2(x^{(0)}) = -0.5$.\n\nMy earlier calculation of these terms was:\n$\\nabla g_1(x^{(0)})^T p^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}^T \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix} = -0.5$. Correct.\n$\\nabla g_2(x^{(0)})^T p^{(0)} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}^T \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix} = -0.5$. Correct.\nThis is consistent with the constraints of the QP being active.\n\nNow let's use the KKT condition for the QP.\n$\\nabla f(x^{(0)}) + B^{(0)} p^{(0)} + \\lambda_1 \\nabla g_1(x^{(0)}) + \\lambda_2 \\nabla g_2(x^{(0)}) = 0$.\n$\\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix} + \\lambda_1 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\lambda_2 \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 0$\n$\\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix} + \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_1 \\end{pmatrix} + \\begin{pmatrix} 2\\lambda_2 \\\\ \\lambda_2 \\end{pmatrix} = 0$\n$\\begin{pmatrix} -4 + \\lambda_1 + 2\\lambda_2 \\\\ -3.5 + \\lambda_1 + \\lambda_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThis is the same system I solved for $\\lambda_1, \\lambda_2$ before using $p_1, p_2$.\n$p_1-4 + \\lambda_1 + 2\\lambda_2 = 0 \\implies 0-4+\\lambda_1+2\\lambda_2=0 \\implies \\lambda_1+2\\lambda_2 = 4$.\n$p_2-3 + \\lambda_1 + \\lambda_2 = 0 \\implies -0.5-3+\\lambda_1+\\lambda_2=0 \\implies \\lambda_1+\\lambda_2 = 3.5$.\nSolution was $\\lambda_1 = 3, \\lambda_2 = 0.5$. Correct.\n\nLet's find a general expression for the directional derivative in terms of the QP solution.\n$D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = \\nabla f(x^{(0)})^T p^{(0)} + \\mu \\left( \\nabla g_1(x^{(0)})^T p^{(0)} + \\nabla g_2(x^{(0)})^T p^{(0)} \\right)$.\nFrom the linearized constraints (which are active):\n$\\nabla g_1(x^{(0)})^T p^{(0)} = -g_1(x^{(0)})$.\n$\\nabla g_2(x^{(0)})^T p^{(0)} = -g_2(x^{(0)})$.\nSo, $D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = \\nabla f(x^{(0)})^T p^{(0)} - \\mu (g_1(x^{(0)}) + g_2(x^{(0)}))$.\n\nNow, let's look at $\\nabla f(x^{(0)})^T p^{(0)}$.\nFrom the KKT condition of the QP, multiply by $(p^{(0)})^T$:\n$(p^{(0)})^T \\nabla f(x^{(0)}) + (p^{(0)})^T B^{(0)} p^{(0)} + \\lambda_1 (p^{(0)})^T \\nabla g_1(x^{(0)}) + \\lambda_2 (p^{(0)})^T \\nabla g_2(x^{(0)}) = 0$.\n$\\nabla f(x^{(0)})^T p^{(0)} = - (p^{(0)})^T B^{(0)} p^{(0)} - \\lambda_1 (\\nabla g_1(x^{(0)})^T p^{(0)}) - \\lambda_2 (\\nabla g_2(x^{(0)})^T p^{(0)})$.\nSubstitute $\\nabla g_i^T p^{(0)} = -g_i(x^{(0)})$:\n$\\nabla f(x^{(0)})^T p^{(0)} = - (p^{(0)})^T B^{(0)} p^{(0)} - \\lambda_1 (-g_1(x^{(0)})) - \\lambda_2 (-g_2(x^{(0)}))$.\n$\\nabla f(x^{(0)})^T p^{(0)} = - p^{(0)T} B^{(0)} p^{(0)} + \\lambda_1 g_1(x^{(0)}) + \\lambda_2 g_2(x^{(0)})$.\n\nSubstitute this expression for $\\nabla f^T p$ back into the directional derivative formula:\n$D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = \\left( - p^{(0)T} B^{(0)} p^{(0)} + \\lambda_1 g_1(x^{(0)}) + \\lambda_2 g_2(x^{(0)}) \\right) - \\mu (g_1(x^{(0)}) + g_2(x^{(0)}))$.\n$D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = - p^{(0)T} B^{(0)} p^{(0)} + (\\lambda_1 - \\mu) g_1(x^{(0)}) + (\\lambda_2 - \\mu) g_2(x^{(0)})$.\n\nLet's calculate the terms:\n*   $p^{(0)} = (0, -0.5)^T$.\n*   $B^{(0)} = I_2$.\n*   $p^{(0)T} B^{(0)} p^{(0)} = p^{(0)T} p^{(0)} = 0^2 + (-0.5)^2 = 0.25$. This term is always non-negative, and strictly positive if $p^{(0)} \\neq 0$.\n*   $g_1(x^{(0)}) = 0.5$.\n*   $g_2(x^{(0)}) = 0.5$.\n*   $\\lambda_1 = 3$.\n*   $\\lambda_2 = 0.5$.\n\nSubstitute these values:\n$D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = -0.25 + (3 - \\mu)(0.5) + (0.5 - \\mu)(0.5)$.\n$D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = -0.25 + 1.5 - 0.5\\mu + 0.25 - 0.5\\mu$.\n$D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = 1.5 - \\mu$.\n\nThis confirms my previous, more direct calculation. The result seems robust.\n$D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = 1.5 - \\mu$.\nWe need this to be strictly negative for a descent direction.\n$1.5 - \\mu < 0 \\implies \\mu > 1.5$.\nSo, $\\mu_{\\min} = 1.5$.\n\nWhy did the rule of thumb $\\mu \\ge \\|\\lambda\\|_\\infty$ give a different number? Let's check the literature again.\nThe condition $\\mu \\ge \\|\\lambda^{(k+1)}\\|_\\infty$ is a *sufficient* condition to ensure that $p^{(k)}$ is a descent direction. It is not always a *necessary* condition. Let's see if we can derive it from the general expression I found.\n$D_p \\phi_\\mu = - p^T B p + \\sum_{i \\in \\mathcal{A}} (\\lambda_i - \\mu) g_i(x)$.\nHere $\\mathcal{A}$ is the active set of constraints for the QP. It's not quite $g_i(x)$ but the linearized constraint violation.\nLet's be more precise.\n$D_{p^{(k)}} \\phi_\\mu(x^{(k)}) = \\nabla f^T p^{(k)} + \\mu \\sum_{i \\in I_+(x^{(k)})} \\nabla g_i^T p^{(k)} + \\mu \\sum_{i \\in I_0(x^{(k)})} \\max(0, \\nabla g_i^T p^{(k)})$.\nUsing $\\nabla f^T p^{(k)} = -p^{(k)T}B^{(k)}p^{(k)} - \\sum_i \\lambda_i^{(k+1)} \\nabla g_i^T p^{(k)}$:\n$D = -p^T B p - \\sum_i \\lambda_i \\nabla g_i^T p + \\mu \\sum_{i \\in I_+} \\nabla g_i^T p + \\mu \\sum_{i \\in I_0} \\max(0, \\nabla g_i^T p)$.\n(I am dropping the iteration superscript for clarity)\n$D = -p^T B p + \\sum_{i \\in I_+} (\\mu - \\lambda_i) \\nabla g_i^T p + \\sum_{i \\in I_0 \\cup I_-} (-\\lambda_i \\nabla g_i^T p) + \\mu \\sum_{i \\in I_0} \\max(0, \\nabla g_i^T p)$.\nRecall QP constraints: $\\nabla g_i^T p \\le -g_i(x)$.\nFor $i\\in I_+$, $g_i(x)>0$, so $\\nabla g_i^T p$ must be negative.\nFor $i\\in I_0$, $g_i(x)=0$, so $\\nabla g_i^T p \\le 0$.\nFor $i \\in I_-$, $g_i(x)<0$. $\\nabla g_i^T p$ can be positive or negative. The corresponding $\\lambda_i$ from the QP subproblem should be 0 if this constraint is not active in the QP.\n\nLet's assume all constraints are in $I_+ \\cup I_0$.\n$g_i > 0 \\implies \\nabla g_i^T p \\le -g_i < 0$.\n$g_i = 0 \\implies \\nabla g_i^T p \\le 0$.\nSo in both cases $\\nabla g_i^T p \\le 0$.\n$D = -p^T B p + \\sum_{i \\in I_+} (\\mu - \\lambda_i) \\nabla g_i^T p + \\sum_{i \\in I_0} [ -\\lambda_i \\nabla g_i^T p + \\mu \\max(0, \\nabla g_i^T p) ]$.\nSince for $i \\in I_0$, $\\nabla g_i^T p \\le 0$, we have $\\max(0, \\nabla g_i^T p) = 0$.\nSo the term for $i \\in I_0$ becomes $-\\lambda_i \\nabla g_i^T p$.\n$D = -p^T B p + \\sum_{i \\in I_+} (\\mu - \\lambda_i) \\nabla g_i^T p - \\sum_{i \\in I_0} \\lambda_i \\nabla g_i^T p$.\n\nNow let's apply this to our problem.\nBoth constraints are in $I_+$. $I_0$ is empty.\n$D = -p^T B p + (\\mu - \\lambda_1) \\nabla g_1^T p + (\\mu - \\lambda_2) \\nabla g_2^T p$.\nWe need $D < 0$.\n$p^T B p = 0.25 > 0$.\n$\\nabla g_1^T p = -0.5 < 0$.\n$\\nabla g_2^T p = -0.5 < 0$.\n$\\lambda_1=3, \\lambda_2=0.5$.\n$D = -0.25 + (\\mu - 3)(-0.5) + (\\mu - 0.5)(-0.5)$.\n$D = -0.25 - 0.5\\mu + 1.5 - 0.5\\mu + 0.25$.\n$D = 1.5 - \\mu$.\nYes, this is definitely the correct expression. My calculations are consistent.\n\nNow, let's understand the $\\mu \\ge \\|\\lambda\\|_\\infty$ condition.\nLet's look at the general expression again:\n$D = -p^T B p + (\\lambda_1 - \\mu)g_1 + (\\lambda_2 - \\mu)g_2$.\nThis is not right. The general formula was:\n$D = -p^T B p + (\\lambda_1 g_1 + \\lambda_2 g_2) - \\mu(g_1+g_2)$ where $g_i$ are evaluated at $x^{(0)}$.\nThis is after substituting $\\nabla f^T p = -p^T B p + \\lambda_1 g_1 + \\lambda_2 g_2$. Let's re-derive this.\n$\\nabla f^T p = -p^T B p - \\lambda_1 \\nabla g_1^T p - \\lambda_2 \\nabla g_2^T p$.\n$\\nabla g_i^T p = -g_i$ because both QP constraints are active.\nSo $\\nabla f^T p = -p^T B p - \\lambda_1(-g_1) - \\lambda_2(-g_2) = -p^T B p + \\lambda_1 g_1 + \\lambda_2 g_2$.\nThis is correct.\nThen $D = \\nabla f^T p - \\mu(g_1+g_2)$.\n$D = -p^T B p + \\lambda_1 g_1 + \\lambda_2 g_2 - \\mu g_1 - \\mu g_2$.\n$D = -p^T B p + (\\lambda_1 - \\mu)g_1 + (\\lambda_2 - \\mu)g_2$.\nThis formula is correct. All $g_i$ are evaluated at $x^{(0)}$.\nLet's plug in numbers into *this* formula.\n$p^T B p = 0.25$.\n$g_1 = 0.5$.\n$g_2 = 0.5$.\n$\\lambda_1=3, \\lambda_2 = 0.5$.\n$D = -0.25 + (3-\\mu)(0.5) + (0.5-\\mu)(0.5)$.\n$D = -0.25 + 1.5 - 0.5\\mu + 0.25 - 0.5\\mu$.\n$D = 1.5 - \\mu$.\nYes, the calculation is consistent from all angles.\n\nNow, let's explore my confusion.\nThe condition $D < 0$ becomes:\n$-p^T B p + (\\lambda_1 - \\mu)g_1 + (\\lambda_2 - \\mu)g_2 < 0$.\nIf we set $\\mu \\ge \\max(\\lambda_1, \\lambda_2)$, which is $\\mu \\ge 3$.\nThen both $(\\lambda_1-\\mu)$ and $(\\lambda_2-\\mu)$ are $\\le 0$.\nSince $g_1>0$ and $g_2>0$, the terms $(\\lambda_i-\\mu)g_i$ are both $\\le 0$.\nAlso $-p^T B p = -0.25 < 0$.\nSo if $\\mu \\ge 3$, then $D < 0$ is guaranteed. This confirms that $\\mu \\ge \\|\\lambda\\|_\\infty$ is a *sufficient* condition.\nHowever, it's not a necessary condition.\nWe need to find the *smallest* threshold $\\mu_{\\min}$.\nThe condition is $1.5 - \\mu < 0$.\nThis is equivalent to $\\mu > 1.5$.\nThe threshold is the infimum of the set of $\\mu$s that satisfy the condition. The set is $(1.5, \\infty)$. The infimum is $1.5$.\nSo $\\mu_{\\min}=1.5$.\nThe question states \"determine the smallest threshold value $\\mu_{\\min}$ such that for all $\\mu > \\mu_{\\min}$...\". This means $\\mu_{\\min}$ is the boundary value.\nThe logic holds.\n\nLet's do a final check of all calculations.\n$x^{(0)} = (1, 1.5)$.\n$f(x) = (x_1-3)^2 + (x_2-3)^2 \\implies \\nabla f = (2x_1-6, 2x_2-6)^T \\implies \\nabla f(x^{(0)}) = (-4, -3)^T$.\n$g_1(x) = x_1+x_2-2 \\implies \\nabla g_1 = (1,1)^T \\implies g_1(x^{(0)})=0.5, \\nabla g_1(x^{(0)})=(1,1)^T$.\n$g_2(x) = x_1^2+x_2-2 \\implies \\nabla g_2 = (2x_1, 1)^T \\implies g_2(x^{(0)})=0.5, \\nabla g_2(x^{(0)})=(2,1)^T$.\nQP subproblem:\n$\\min \\frac{1}{2}p_1^2 + \\frac{1}{2}p_2^2 -4p_1 -3p_2$\ns.t. $p_1+p_2+0.5 \\le 0$\n$2p_1+p_2+0.5 \\le 0$\nUnconstrained min is at $p=(4,3)$. Both constraints violated.\nIf first is active: $p_1+p_2=-0.5$. Stationarity with $\\lambda_2=0$ gives $p_1=4-\\lambda_1, p_2=3-\\lambda_1$. $(4-\\lambda_1)+(3-\\lambda_1)=-0.5 \\implies 7-2\\lambda_1=-0.5 \\implies 2\\lambda_1=7.5 \\implies \\lambda_1=3.75$. $p_1 = 0.25, p_2=-0.75$. Check second constraint: $2(0.25)+(-0.75)+0.5 = 0.5-0.75+0.5=0.25>0$. Violated.\nIf second is active: $2p_1+p_2=-0.5$. Stationarity with $\\lambda_1=0$ gives $p_1=4-2\\lambda_2, p_2=3-\\lambda_2$. $2(4-2\\lambda_2)+(3-\\lambda_2)=-0.5 \\implies 8-4\\lambda_2+3-\\lambda_2=-0.5 \\implies 11-5\\lambda_2 = -0.5 \\implies 5\\lambda_2=11.5 \\implies \\lambda_2=2.3$. $p_1 = 4-4.6=-0.6, p_2=3-2.3=0.7$. Check first constraint: $-0.6+0.7+0.5=0.6>0$. Violated.\nIf both active: $p_1+p_2=-0.5$, $2p_1+p_2=-0.5$. Subtracting gives $p_1=0$. Then $p_2=-0.5$. So $p^{(0)}=(0, -0.5)$.\nStationarity:\n$p_1-4+\\lambda_1+2\\lambda_2=0 \\implies 0-4+\\lambda_1+2\\lambda_2=0 \\implies \\lambda_1+2\\lambda_2=4$.\n$p_2-3+\\lambda_1+\\lambda_2=0 \\implies -0.5-3+\\lambda_1+\\lambda_2=0 \\implies \\lambda_1+\\lambda_2=3.5$.\nSubtracting gives $\\lambda_2=0.5$. Then $\\lambda_1=3$.\n$\\lambda_1>0, \\lambda_2>0$. This is the valid solution.\nAll calculations for the QP part are correct.\n\nDirectional derivative part:\nEverything hinged on the fact that at $x^{(0)}$, both constraints are violated ($g_i > 0$). This simplifies the merit function locally to $f+\\mu(g_1+g_2)$. This simplification is correct.\nThen $D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = \\nabla f^T p^{(0)} + \\mu(\\nabla g_1^T p^{(0)} + \\nabla g_2^T p^{(0)})$. This is correct.\n$\\nabla f^T p^{(0)} = (-4, -3) \\cdot (0, -0.5) = 1.5$. Correct.\n$\\nabla g_1^T p^{(0)} = (1,1) \\cdot (0, -0.5) = -0.5$. Correct.\n$\\nabla g_2^T p^{(0)} = (2,1) \\cdot (0, -0.5) = -0.5$. Correct.\n$D = 1.5 + \\mu(-0.5 - 0.5) = 1.5 - \\mu$. Correct.\nCondition for strict descent direction: $D < 0$.\n$1.5 - \\mu < 0 \\implies \\mu > 1.5$.\nThe threshold is $\\mu_{\\min}=1.5$.\nAll steps seem correct and verified. The logic is sound. The calculations are double checked. The potential confusion with the standard sufficient condition for $\\mu$ has been resolved; the question asks for the sharpest possible bound (the minimum threshold), not just any sufficient bound. My result is the sharpest bound.",
            "answer": "$$\n\\boxed{1.5}\n$$"
        },
        {
            "introduction": "A fundamental requirement for the QP subproblem to be efficiently and reliably solvable is that it must be convex, which means its Hessian matrix must be positive semidefinite. However, in many practical SQP algorithms, the Hessian approximation $B_k$ can become indefinite. This final exercise  delves into a critical numerical safeguard: Hessian regularization. You will investigate how adding a small multiple of the identity matrix, $\\tau I$, can restore convexity and explore how the choice of $\\tau$ impacts the resulting search direction.",
            "id": "3169643",
            "problem": "You are asked to study the role of Hessian regularization in the quadratic programming subproblem that arises in Sequential Quadratic Programming (SQP). Your investigation must be implemented as a complete, runnable program that solves a small family of convex quadratic subproblems obtained by regularizing an indefinite Hessian approximation.\n\nThe fundamental base you must start from is:\n- The second-order Taylor approximation of a smooth objective function around a current iterate, leading to a local quadratic model in the step variable.\n- The first-order (affine) approximation of smooth inequality constraints around a current iterate, leading to a local polyhedral approximation of the feasible region.\n- The requirement that the quadratic subproblem be convex to guarantee the existence of a global minimizer, which is ensured if its quadratic form is positive semidefinite.\n\nConstruct the following toy nonlinear program and its SQP quadratic subproblem at a given iterate.\n\n1) Nonlinear program:\n- Objective: minimize the function $f(x)$ where $x \\in \\mathbb{R}^2$ and\n  $$ f(x) = x_1^3 - 3 x_1 x_2 + x_2^2. $$\n- Inequality constraints:\n  - $$ c_1(x) := x_1^2 + x_2 - 1 \\le 0, $$\n  - $$ c_2(x) := -x_2 \\le 0. $$\n\n2) Linearization point and data for the quadratic subproblem:\n- Current iterate:\n  $$ x_k = \\begin{bmatrix} 0.5 \\\\ 0.4 \\end{bmatrix}. $$\n- Gradient of the objective at $x_k$:\n  $$ \\nabla f(x) = \\begin{bmatrix} 3 x_1^2 - 3 x_2 \\\\ -3 x_1 + 2 x_2 \\end{bmatrix}, \\quad g_k := \\nabla f(x_k). $$\n- An indefinite symmetric Hessian approximation (for example, a quasi-Newton update that has lost positive definiteness):\n  $$ B_k = \\begin{bmatrix} -0.5 & 0.0 \\\\ 0.0 & 0.2 \\end{bmatrix}. $$\n- Linearization of the inequality constraints at $x_k$:\n  - For $c_1(x)$, use the gradient\n    $$ \\nabla c_1(x) = \\begin{bmatrix} 2 x_1 \\\\ 1 \\end{bmatrix}, \\quad \\nabla c_1(x_k), \\quad c_1(x_k), $$\n    and the affine approximation $c_1(x_k) + \\nabla c_1(x_k)^\\top d \\le 0$.\n  - For $c_2(x)$, use the gradient\n    $$ \\nabla c_2(x) = \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}, \\quad \\nabla c_2(x_k), \\quad c_2(x_k), $$\n    and the affine approximation $c_2(x_k) + \\nabla c_2(x_k)^\\top d \\le 0$.\n\n3) Regularized quadratic subproblem:\n- For a given regularization parameter $\\tau \\ge 0$, define the shifted matrix\n  $$ H(\\tau) := B_k + \\tau I. $$\n- Solve the convex quadratic program in the step variable $d \\in \\mathbb{R}^2$:\n  $$ \\min_{d \\in \\mathbb{R}^2} \\; q_\\tau(d) := \\tfrac{1}{2} d^\\top H(\\tau) d + g_k^\\top d, $$\n  subject to the affine inequality constraints\n  $$ c_i(x_k) + \\nabla c_i(x_k)^\\top d \\le 0, \\quad \\text{for } i \\in \\{1,2\\}. $$\n\nScientific realism and convexity requirement:\n- If $H(\\tau)$ is not positive semidefinite, the quadratic subproblem is nonconvex and a global minimizer is not guaranteed. In this case, for the purpose of this exercise, you must treat the step as undefined.\n\nInvestigation goal:\n- For each tested $\\tau$, compute the step $d(\\tau)$ if and only if $H(\\tau)$ is positive semidefinite (within a numerical tolerance of $\\varepsilon = 10^{-10}$ on the smallest eigenvalue). If $H(\\tau)$ fails this test, declare the step undefined.\n- Report the Euclidean norm $\\lVert d(\\tau) \\rVert_2$ when defined; otherwise report the sentinel value $-1.0$.\n- Report two feasibility indicators:\n  - Linearized feasibility: whether the computed step $d(\\tau)$ satisfies all affine constraints $c_i(x_k) + \\nabla c_i(x_k)^\\top d \\le 0$ within a tolerance of $\\delta = 10^{-8}$.\n  - Original feasibility: whether the trial point $x_k + d(\\tau)$ satisfies the original nonlinear constraints $c_i(x_k + d) \\le 0$ within the same tolerance $\\delta$.\n\nTest suite:\n- Use the following values of the regularization parameter\n  $$ \\tau \\in \\{ 0.0, \\; 0.3, \\; 0.5, \\; 2.0 \\}. $$\n- These cover the cases of a nonconvex quadratic form ($\\tau$ too small), a boundary case yielding a positive semidefinite form, and a clearly convex case (larger $\\tau$).\n\nAnswer specification and output format:\n- For each $\\tau$ in the test suite, your program must produce a four-element list\n  $$ [\\tau, \\; \\lVert d(\\tau) \\rVert_2, \\; \\text{is\\_lin\\_feasible}, \\; \\text{is\\_orig\\_feasible}]. $$\n- If $H(\\tau)$ is not positive semidefinite, return $[\\tau, \\; -1.0, \\; \\text{False}, \\; \\text{False}]$.\n- Aggregate the results for all tested $\\tau$ values, in the given order, into a single list, and print exactly one line containing this aggregate list in a comma-separated format enclosed in square brackets, for example\n  $$ [[\\tau_1,\\; \\lVert d(\\tau_1)\\rVert_2,\\; \\dots], [\\tau_2,\\; \\lVert d(\\tau_2)\\rVert_2,\\; \\dots], \\dots]. $$\n- There are no physical units or angles in this problem. All real-valued outputs must be decimal floats. Booleans must be literal `True` or `False`.",
            "solution": "The user wants to analyze a regularized quadratic programming (QP) subproblem arising in Sequential Quadratic Programming (SQP) for a given nonlinear program (NLP).\n\nFirst, we formalize the problem by computing the necessary components for the QP subproblem at the specified iterate $x_k$.\n\nThe NLP is defined as:\nMinimize $f(x) = x_1^3 - 3 x_1 x_2 + x_2^2$\nSubject to:\n$c_1(x) = x_1^2 + x_2 - 1 \\le 0$\n$c_2(x) = -x_2 \\le 0$\n\nThe QP subproblem is constructed at the iterate $x_k = \\begin{bmatrix} 0.5 \\\\ 0.4 \\end{bmatrix}$. The subproblem is a quadratic approximation of the original problem in terms of a step variable $d \\in \\mathbb{R}^2$.\n\nThe objective function for the QP is $q_\\tau(d) = \\tfrac{1}{2} d^\\top H(\\tau) d + g_k^\\top d$.\nThe components are:\n1.  The gradient of the objective function $f(x)$ at $x_k$, denoted $g_k$.\n    The gradient is $\\nabla f(x) = \\begin{bmatrix} 3 x_1^2 - 3 x_2 \\\\ -3 x_1 + 2 x_2 \\end{bmatrix}$.\n    At $x_k = \\begin{bmatrix} 0.5 \\\\ 0.4 \\end{bmatrix}$, we have:\n    $$ g_k = \\nabla f(x_k) = \\begin{bmatrix} 3(0.5)^2 - 3(0.4) \\\\ -3(0.5) + 2(0.4) \\end{bmatrix} = \\begin{bmatrix} 0.75 - 1.2 \\\\ -1.5 + 0.8 \\end{bmatrix} = \\begin{bmatrix} -0.45 \\\\ -0.7 \\end{bmatrix}. $$\n\n2.  The regularized Hessian approximation, $H(\\tau)$.\n    The problem provides an indefinite Hessian approximation $B_k = \\begin{bmatrix} -0.5 & 0.0 \\\\ 0.0 & 0.2 \\end{bmatrix}$.\n    This matrix is regularized using a parameter $\\tau \\ge 0$ to form a new matrix $H(\\tau) = B_k + \\tau I$, where $I$ is the identity matrix.\n    $$ H(\\tau) = \\begin{bmatrix} -0.5 & 0.0 \\\\ 0.0 & 0.2 \\end{bmatrix} + \\tau \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} -0.5 + \\tau & 0.0 \\\\ 0.0 & 0.2 + \\tau \\end{bmatrix}. $$\n\nThe constraints for the QP are affine approximations of the original nonlinear constraints $c_i(x)$. The general form is $c_i(x_k) + \\nabla c_i(x_k)^\\top d \\le 0$.\n\n1.  For constraint $c_1(x)$:\n    The value at $x_k$ is $c_1(x_k) = (0.5)^2 + 0.4 - 1 = 0.25 + 0.4 - 1 = -0.35$.\n    The gradient is $\\nabla c_1(x) = \\begin{bmatrix} 2 x_1 \\\\ 1 \\end{bmatrix}$.\n    At $x_k$, $\\nabla c_1(x_k) = \\begin{bmatrix} 2(0.5) \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n    The linearized constraint is $-0.35 + \\begin{bmatrix} 1 & 1 \\end{bmatrix} d \\le 0$, which simplifies to $d_1 + d_2 \\le 0.35$.\n\n2.  For constraint $c_2(x)$:\n    The value at $x_k$ is $c_2(x_k) = -0.4$.\n    The gradient is $\\nabla c_2(x) = \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}$, which is constant.\n    The linearized constraint is $-0.4 + \\begin{bmatrix} 0 & -1 \\end{bmatrix} d \\le 0$, which simplifies to $-d_2 \\le 0.4$, or $d_2 \\ge -0.4$.\n\nThe complete regularized QP subproblem is:\nMinimize $q_\\tau(d) = \\tfrac{1}{2} d^\\top \\begin{bmatrix} -0.5+\\tau & 0 \\\\ 0 & 0.2+\\tau \\end{bmatrix} d + \\begin{bmatrix} -0.45 \\\\ -0.7 \\end{bmatrix}^\\top d$\nSubject to:\n$d_1 + d_2 \\le 0.35$\n$d_2 \\ge -0.4$\n\nA crucial part of the task is to solve this QP only if it is convex, which requires the Hessian matrix $H(\\tau)$ to be positive semidefinite (PSD). A diagonal matrix is PSD if and only if all its diagonal entries are non-negative. The eigenvalues of $H(\\tau)$ are $\\lambda_1 = -0.5 + \\tau$ and $\\lambda_2 = 0.2 + \\tau$.\nFor $H(\\tau)$ to be PSD, we require $\\lambda_1 \\ge 0$ and $\\lambda_2 \\ge 0$.\n$-0.5 + \\tau \\ge 0 \\implies \\tau \\ge 0.5$.\n$0.2 + \\tau \\ge 0 \\implies \\tau \\ge -0.2$.\nThe combined condition is $\\tau \\ge 0.5$. The problem specifies a numerical tolerance $\\varepsilon = 10^{-10}$, so the check becomes whether the smallest eigenvalue is greater than or equal to $-\\varepsilon$. This effectively confirms the analytical condition $\\tau \\ge 0.5$.\n\nWe now analyze the problem for each value of $\\tau$ in the test suite $\\{ 0.0, 0.3, 0.5, 2.0 \\}$.\n\nCase $\\tau = 0.0$:\n$H(0.0) = \\begin{bmatrix} -0.5 & 0.0 \\\\ 0.0 & 0.2 \\end{bmatrix}$. The smallest eigenvalue is $-0.5$, which is less than $-\\varepsilon$. The matrix is not PSD, so the QP subproblem is nonconvex. As per the problem statement, the step $d(0.0)$ is undefined. The result is $[0.0, -1.0, \\text{False}, \\text{False}]$.\n\nCase $\\tau = 0.3$:\n$H(0.3) = \\begin{bmatrix} -0.2 & 0.0 \\\\ 0.0 & 0.5 \\end{bmatrix}$. The smallest eigenvalue is $-0.2$, which is less than $-\\varepsilon$. The matrix is not PSD. The step $d(0.3)$ is undefined. The result is $[0.3, -1.0, \\text{False}, \\text{False}]$.\n\nCase $\\tau = 0.5$:\n$H(0.5) = \\begin{bmatrix} 0.0 & 0.0 \\\\ 0.0 & 0.7 \\end{bmatrix}$. The smallest eigenvalue is $0.0$, which is $\\ge -\\varepsilon$. The matrix is PSD, so the QP is convex and can be solved. The objective function is $q_{0.5}(d) = \\tfrac{1}{2}(0.7 d_2^2) - 0.45 d_1 - 0.7 d_2$. Since the coefficient of $d_1$ is negative ($-0.45$), we must make $d_1$ as large as possible to minimize the objective. The constraint $d_1 + d_2 \\le 0.35$ bounds $d_1$, so at the optimum, this constraint must be active: $d_1 = 0.35 - d_2$. Substituting this into the objective gives a 1D quadratic problem in $d_2$. The solution to this problem is $d_2 = 5/14 \\approx 0.3571$ and $d_1 = -1/140 \\approx -0.0071$.\nThe step is $d(0.5) \\approx [-0.0071, 0.3571]^\\top$. Its Euclidean norm is $\\lVert d(0.5) \\rVert_2 \\approx 0.3572$.\n-   Linearized feasibility: The step satisfies both $d_1 + d_2 \\le 0.35$ and $d_2 \\ge -0.4$, so it is linearly feasible.\n-   Original feasibility: The new point is $x_k + d(0.5) \\approx [0.4929, 0.7571]^\\top$. Testing this in the original nonlinear constraints gives $c_1(x_k+d) \\approx 5.1 \\times 10^{-5}$, which violates $c_1(x) \\le 0$. Thus, it is not feasible for the original problem. The result is $[0.5, \\lVert d(0.5) \\rVert_2, \\text{True}, \\text{False}]$.\n\nCase $\\tau = 2.0$:\n$H(2.0) = \\begin{bmatrix} 1.5 & 0.0 \\\\ 0.0 & 2.2 \\end{bmatrix}$. The eigenvalues are $1.5$ and $2.2$, so the matrix is positive definite. The QP is strictly convex. The unconstrained minimizer is found by setting the gradient of $q_{2.0}(d)$ to zero, which yields $d_{unc} = [0.3, 7/22]^\\top \\approx [0.3, 0.318]^\\top$. This point violates the first linearized constraint ($d_1 + d_2 \\approx 0.618 > 0.35$). Therefore, the solution must lie on the boundary of the feasible region. Solving the KKT system for the case where the first constraint is active ($d_1+d_2=0.35$) yields the optimal step $d(2.0) \\approx [0.1405, 0.2095]^\\top$. The Euclidean norm is $\\lVert d(2.0) \\rVert_2 \\approx 0.2522$.\n-   Linearized feasibility: By construction, the step is on the boundary of the feasible region and thus satisfies the linearized constraints.\n-   Original feasibility: The new point is $x_k + d(2.0) \\approx [0.6405, 0.6095]^\\top$. For this point, $c_1(x_k+d) \\approx 0.0198 > 0$, violating the first nonlinear constraint. The result is $[2.0, \\lVert d(2.0) \\rVert_2, \\text{True}, \\text{False}]$.\n\nThe implementation will use `numpy` for vector and matrix operations and `scipy.optimize.minimize` with the 'SLSQP' method to solve the convex QP subproblems. The feasibility checks are performed using the specified tolerance $\\delta = 10^{-8}$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves a family of regularized quadratic subproblems from an SQP method\n    for a given nonlinear program and reports the results.\n    \"\"\"\n    \n    # Define problem data and parameters from the problem statement.\n    # Current iterate x_k\n    x_k = np.array([0.5, 0.4])\n    \n    # Indefinite Hessian approximation B_k\n    B_k = np.array([[-0.5, 0.0], [0.0, 0.2]])\n    \n    # Test suite of regularization parameters\n    test_cases = [0.0, 0.3, 0.5, 2.0]\n    \n    # Tolerances\n    epsilon = 1e-10  # For positive-semidefinite check\n    delta = 1e-8     # For feasibility checks\n\n    # --- Pre-calculate quantities that do not depend on tau ---\n    \n    # Gradient of the objective at x_k\n    g_k = np.array([3*x_k[0]**2 - 3*x_k[1], -3*x_k[0] + 2*x_k[1]])\n    \n    # Values and gradients of constraints at x_k\n    # Constraint c1\n    c1_val_at_xk = x_k[0]**2 + x_k[1] - 1.0\n    nabla_c1_at_xk = np.array([2*x_k[0], 1.0])\n    \n    # Constraint c2\n    c2_val_at_xk = -x_k[1]\n    nabla_c2_at_xk = np.array([0.0, -1.0])\n\n    # List to store results for each tau\n    results = []\n\n    # --- Iterate through each tau value ---\n    for tau in test_cases:\n        # 1. Construct the regularized Hessian H(tau)\n        H_tau = B_k + tau * np.identity(2)\n        \n        # 2. Check for positive semidefiniteness (PSD)\n        # We use eigvalsh because H_tau is symmetric.\n        eigenvalues = np.linalg.eigvalsh(H_tau)\n        if np.min(eigenvalues) < -epsilon:\n            # Not PSD, the step is undefined.\n            results.append([tau, -1.0, False, False])\n            continue\n\n        # 3. If PSD, define and solve the convex QP subproblem\n        \n        # QP objective function\n        def qp_objective(d):\n            return 0.5 * d.T @ H_tau @ d + g_k.T @ d\n\n        # QP constraints (for scipy.optimize.minimize, type 'ineq' is f(x) >= 0)\n        constraints = [\n            {'type': 'ineq', 'fun': lambda d: -(c1_val_at_xk + nabla_c1_at_xk.T @ d)},\n            {'type': 'ineq', 'fun': lambda d: -(c2_val_at_xk + nabla_c2_at_xk.T @ d)}\n        ]\n        \n        # Initial guess for the step d\n        d_initial_guess = np.array([0.0, 0.0])\n        \n        # Solve the QP using Sequential Least Squares Programming (SLSQP)\n        qp_result = minimize(\n            qp_objective, \n            d_initial_guess, \n            method='SLSQP', \n            constraints=constraints\n        )\n        \n        d_solution = qp_result.x\n        \n        # 4. Compute required outputs\n        \n        # Euclidean norm of the step d\n        norm_d = np.linalg.norm(d_solution)\n        \n        # Check linearized feasibility\n        is_lin_feasible = (c1_val_at_xk + nabla_c1_at_xk.T @ d_solution <= delta) and \\\n                          (c2_val_at_xk + nabla_c2_at_xk.T @ d_solution <= delta)\n                          \n        # Check original nonlinear feasibility at the new trial point x_k + d\n        x_new = x_k + d_solution\n        c1_at_xnew = x_new[0]**2 + x_new[1] - 1.0\n        c2_at_xnew = -x_new[1]\n        \n        is_orig_feasible = (c1_at_xnew <= delta) and (c2_at_xnew <= delta)\n\n        # Append the full result for this tau\n        results.append([tau, norm_d, bool(is_lin_feasible), bool(is_orig_feasible)])\n\n    # Format the final output string as specified\n    output_str = \"[\" + \",\".join(str(r) for r in results) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}