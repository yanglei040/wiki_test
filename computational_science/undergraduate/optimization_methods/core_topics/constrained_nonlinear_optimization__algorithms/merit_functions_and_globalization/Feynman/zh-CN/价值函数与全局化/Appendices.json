{
    "hands_on_practices": [
        {
            "introduction": "理论是指导，但真正的理解来自于实践。本节将通过一系列动手实践，加深你对价值函数及其在全局化策略中作用的理解。我们将从一个基本但至关重要的问题开始：如何选择惩罚项？  这个练习将引导你比较两种最常见的惩罚函数——平方$\\ell_2$范数和$\\ell_1$范数。通过计算确保搜索方向为下降方向所需的最小惩罚参数，你将亲手揭示它们在处理约束违反时的一个根本性差异，并理解为何$\\ell_1$惩罚通常被认为是“精确的”。",
            "id": "3149223",
            "problem": "考虑一个带单个标量约束的光滑等式约束优化问题。在迭代点 $x_k$ 处，假设我们使用标量罚函数的线搜索来全局化一个候选步长 $p$。定义两个基于相同罚参数 $\\mu  0$ 构建的罚函数：\n- 二范数平方罚函数：$\\phi_{2}(x;\\mu) = f(x) + \\frac{\\mu}{2}\\,c(x)^2$。\n- 一范数罚函数：$\\phi_{1}(x;\\mu) = f(x) + \\mu\\,|c(x)|$。\n\n假设在 $x_k$ 处的局部数据如下：\n- 标量约束违反量为 $c(x_k) = 10^{-3}  0$。\n- 沿候选步长 $p$ 方向，约束的方向导数为 $Jc(x_k)\\,p = -1$，因此沿 $p$ 方向移动会以单位速率减小约束违反量。\n- 沿 $p$ 方向，目标函数的方向导数为 $g(x_k)^{\\top}p = 0.05$，因此沿 $p$ 方向移动会增加目标函数值。\n\n一个标准的回溯 Armijo 线搜索要求罚函数在 $t=0$ 处具有严格为负的方向导数，以便接受足够小的步长。仅使用方向导数的基本定义和链式法则，对每个罚函数确定最小的罚参数 $\\mu$，使得 $p$ 在 $x_k$ 处是一个下降方向。然后计算比率\n$$\nR \\;=\\; \\frac{\\mu_{2,\\min}}{\\mu_{1,\\min}},\n$$\n其中 $\\mu_{2,\\min}$ 对应于 $\\phi_{2}$，$\\mu_{1,\\min}$ 对应于 $\\phi_{1}$。给出 $R$ 的数值，四舍五入到四位有效数字。不需要单位。",
            "solution": "该问题是有效的，因为它科学地基于数值优化的原理，是适定的、客观的且内部一致的。我们可以进行形式化的求解。\n\n令函数 $\\psi(x)$ 定义在 $\\mathbb{R}^n$ 上。$\\psi$ 在点 $x_k$ 处沿向量 $p$ 方向的方向导数由下式给出\n$$\nD_p \\psi(x_k) = \\lim_{t \\to 0^+} \\frac{\\psi(x_k + tp) - \\psi(x_k)}{t}\n$$\n如果 $\\psi$ 在 $x_k$ 处连续可微，其方向导数由 $D_p \\psi(x_k) = \\nabla \\psi(x_k)^\\top p$ 给出。如果 $D_p \\psi(x_k)  0$，则方向 $p$ 是 $\\psi$ 在 $x_k$ 处的下降方向。问题要求我们为两个不同的罚函数找到最小的罚参数 $\\mu$，使得给定的步长 $p$ 是一个下降方向。\n\n我们在迭代点 $x_k$ 处有以下数据：\n-   目标函数梯度分量：$\\nabla f(x_k)^\\top p = 0.05$。\n-   约束函数值：$c(x_k) = 10^{-3}$。\n-   约束梯度分量：$\\nabla c(x_k)^\\top p = -1$。\n\n**二范数平方罚函数 $\\phi_{2}(x;\\mu)$ 的分析**\n\n二范数平方罚函数定义为\n$$\n\\phi_{2}(x;\\mu) = f(x) + \\frac{\\mu}{2}\\,c(x)^2\n$$\n其中 $\\mu  0$ 是罚参数。$f(x)$ 和 $c(x)$ 都是光滑函数，因此 $\\phi_2(x;\\mu)$ 也是光滑的。我们可以使用链式法则求出其在 $x_k$ 处沿方向 $p$ 的方向导数：\n$$\nD_p \\phi_{2}(x_k; \\mu) = D_p f(x_k) + D_p \\left( \\frac{\\mu}{2} c(x)^2 \\right) \\Big|_{x_k}\n$$\n罚项的导数是\n$$\nD_p \\left( \\frac{\\mu}{2} c(x)^2 \\right) \\Big|_{x_k} = \\frac{\\mu}{2} \\cdot 2 c(x_k) \\cdot D_p c(x_k) = \\mu \\, c(x_k) \\, (\\nabla c(x_k)^\\top p)\n$$\n因此，罚函数的方向导数是\n$$\nD_p \\phi_{2}(x_k; \\mu) = \\nabla f(x_k)^\\top p + \\mu \\, c(x_k) \\, (\\nabla c(x_k)^\\top p)\n$$\n为使 $p$ 成为下降方向，必须有 $D_p \\phi_{2}(x_k; \\mu)  0$。代入给定值：\n$$\n0.05 + \\mu \\, (10^{-3}) \\, (-1)  0\n$$\n$$\n0.05 - 10^{-3}\\mu  0\n$$\n$$\n0.05  10^{-3}\\mu\n$$\n$$\n\\mu  \\frac{0.05}{10^{-3}} = 50\n$$\n使 $p$ 成为下降方向的罚参数集合是 $(50, \\infty)$。问题要求的是此集合中的最小参数，即该集合的下确界。\n$$\n\\mu_{2,\\min} = 50\n$$\n\n**一范数罚函数 $\\phi_{1}(x;\\mu)$ 的分析**\n\n一范数罚函数定义为\n$$\n\\phi_{1}(x;\\mu) = f(x) + \\mu\\,|c(x)|\n$$\n为了求方向导数，我们必须处理绝对值项 $|c(x)|$。我们对项 $\\mu\\,|c(x)|$ 使用方向导数的基本定义：\n$$\nD_p (\\mu\\,|c(x)|) \\Big|_{x_k} = \\mu \\lim_{t \\to 0^+} \\frac{|c(x_k+tp)| - |c(x_k)|}{t}\n$$\n我们已知 $c(x_k) = 10^{-3}  0$。由于 $c(x)$ 是一个光滑函数，我们可以对小的 $t  0$ 使用一阶泰勒展开：\n$$\nc(x_k+tp) = c(x_k) + t (\\nabla c(x_k)^\\top p) + O(t^2)\n$$\n代入给定值：\n$$\nc(x_k+tp) = 10^{-3} + t(-1) + O(t^2) = 10^{-3} - t + O(t^2)\n$$\n对于足够小的 $t  0$，$c(x_k+tp)$ 的值将保持为正。因此，对于 $t \\to 0^+$，我们有 $|c(x_k+tp)| = c(x_k+tp)$ 且 $|c(x_k)| = c(x_k)$。极限变为：\n$$\n\\lim_{t \\to 0^+} \\frac{c(x_k+tp) - c(x_k)}{t}\n$$\n这正是 $c(x)$ 在 $x_k$ 处沿方向 $p$ 的方向导数的定义，即 $\\nabla c(x_k)^\\top p$。所以，\n$$\nD_p (\\mu\\,|c(x)|) \\Big|_{x_k} = \\mu (\\nabla c(x_k)^\\top p)\n$$\n完整的罚函数 $\\phi_1$ 的方向导数是：\n$$\nD_p \\phi_{1}(x_k; \\mu) = \\nabla f(x_k)^\\top p + \\mu \\, (\\nabla c(x_k)^\\top p)\n$$\n为使 $p$ 成为下降方向，我们需要 $D_p \\phi_{1}(x_k; \\mu)  0$。代入给定值：\n$$\n0.05 + \\mu(-1)  0\n$$\n$$\n0.05 - \\mu  0\n$$\n$$\n\\mu  0.05\n$$\n最小的罚参数是集合 $(0.05, \\infty)$ 的下确界。\n$$\n\\mu_{1,\\min} = 0.05\n$$\n\n**比率 R 的计算**\n\n问题要求计算比率 $R = \\frac{\\mu_{2,\\min}}{\\mu_{1,\\min}}$。使用我们计算出的值：\n$$\nR = \\frac{50}{0.05} = \\frac{50}{5 \\times 10^{-2}} = \\frac{50}{5} \\times 10^2 = 10 \\times 100 = 1000\n$$\n问题指定答案应四舍五入到四位有效数字。精确值为 $1000$。为了用四位有效数字表示，我们可以将其写成科学记数法形式 $1.000 \\times 10^3$。",
            "answer": "$$\\boxed{1.000 \\times 10^{3}}$$"
        },
        {
            "introduction": "在上一个练习中，我们看到了$\\ell_1$价值函数在选择惩罚参数方面的优势。然而，没有免费的午餐，它的一个主要挑战在于其非光滑性。 这个练习将带你直面这个问题，你将分析$\\ell_1$价值函数在可行点处的“扭结”（kink）现象，并计算它如何影响标准的Armijo线搜索条件。这有助于你理解理论上的优雅与实际应用中的复杂性之间的权衡。",
            "id": "3149246",
            "problem": "考虑具有单个变量 $x \\in \\mathbb{R}$ 的等式约束非线性规划问题：\n最小化 $f(x)$，约束条件为 $g(x) = 0$，\n其中 $f(x) = (x - 2)^{2}$ 且 $g(x) = x^{2} - 1$。设精确罚函数为 $\\Phi_{\\rho}(x) = f(x) + \\rho \\, |g(x)|$，其中罚参数 $\\rho  0$。\n\n任务 A：仅使用绝对值和方向导数的核心定义，解释为什么 $\\Phi_{\\rho}(x)$ 在每个可行点（即满足 $g(x) = 0$ 的任意 $x$）处都有一个不可微的尖点。你的解释应明确引用 $|g(x)|$ 在可行点处的行为，以及它如何影响 $\\Phi_{\\rho}(x)$ 的可微性。\n\n任务 B：通过线搜索实现的全局化依赖于充分下降 (Armijo) 条件。假设我们位于可行点 $x^{\\ast} = 1$，并考虑单位搜索方向 $d = 1$。设罚参数为 $\\rho = \\tfrac{1}{2}$，Armijo 参数为 $c_{1} = 0.1$。如果线搜索使用 Clarke 方向导数（即在 $x^{\\ast}$ 处沿 $d$ 的单侧斜率）作为斜率估计，计算满足 Armijo 检验的最大正步长 $\\alpha_{\\max}$\n$$\n\\Phi_{\\rho}(x^{\\ast} + \\alpha d) \\le \\Phi_{\\rho}(x^{\\ast}) + c_{1} \\, \\alpha \\, s_{g},\n$$\n其中 $s_{g}$ 表示 $\\Phi_{\\rho}$ 在 $x^{\\ast}$ 处沿 $d$ 的方向导数。将你的最终答案表示为一个精确的数。不需要四舍五入。\n\n任务 C：简要提出一种次梯度保障措施，一个实用的线搜索算法可以在可行点附近采用该措施来稳健地处理尖点。你的提议应使用可由 $f$、$g$、$\\rho$ 和搜索方向 $d$ 计算出的量来陈述，并且不应依赖于超出方向导数和次梯度基本定义的任何外部“捷径”公式。",
            "solution": "该问题经验证是自洽的，在非线性优化领域有科学依据，并且是适定的。所有必要的函数、参数和条件都已明确给出，没有内部矛盾。我们可以开始求解。\n\n该问题要求分析一个等式约束非线性规划问题，我们要最小化 $f(x) = (x-2)^2$，约束条件为 $g(x) = x^2-1=0$。该优化问题采用精确罚函数 $\\Phi_{\\rho}(x) = f(x) + \\rho \\, |g(x)|$ 进行求解，其中罚参数 $\\rho  0$。\n\n**任务 A：不可微性的解释**\n\n如果一个函数在某点的方向导数在所有方向上都存在，并且是方向向量的线性函数，则该函数在该点是可微的。函数 $\\Psi(x)$ 在点 $x$ 沿方向 $d$ 的方向导数定义为：\n$$\n\\Psi'(x; d) = \\lim_{\\alpha \\to 0^+} \\frac{\\Psi(x + \\alpha d) - \\Psi(x)}{\\alpha}\n$$\n要使 $\\Psi(x)$ 在 $x$ 处可微，必须对所有方向 $d$ 满足 $\\Psi'(x; d) = -\\Psi'(x; -d)$。\n\n设 $x_{\\text{feas}}$ 为一个可行点，即 $g(x_{\\text{feas}}) = 0$。对于给定问题，可行点是 $x^2-1=0$ 的根，即 $x=1$ 和 $x=-1$。在任意这样的点上，罚函数的值为 $\\Phi_{\\rho}(x_{\\text{feas}}) = f(x_{\\text{feas}}) + \\rho |g(x_{\\text{feas}})| = f(x_{\\text{feas}})$。\n\n我们来计算 $\\Phi_{\\rho}(x)$ 在可行点 $x_{\\text{feas}}$ 沿方向 $d$ 的方向导数。\n$$\n\\Phi_{\\rho}'(x_{\\text{feas}}; d) = \\lim_{\\alpha \\to 0^+} \\frac{\\Phi_{\\rho}(x_{\\text{feas}} + \\alpha d) - \\Phi_{\\rho}(x_{\\text{feas}})}{\\alpha}\n$$\n代入 $\\Phi_{\\rho}$ 的定义并使用 $g(x_{\\text{feas}})=0$：\n$$\n\\Phi_{\\rho}'(x_{\\text{feas}}; d) = \\lim_{\\alpha \\to 0^+} \\frac{[f(x_{\\text{feas}} + \\alpha d) + \\rho |g(x_{\\text{feas}} + \\alpha d)|] - f(x_{\\text{feas}})}{\\alpha}\n$$\n由于 $f$ 和 $g$ 都是可微的，这个极限可以被拆分：\n$$\n\\Phi_{\\rho}'(x_{\\text{feas}}; d) = \\lim_{\\alpha \\to 0^+} \\frac{f(x_{\\text{feas}} + \\alpha d) - f(x_{\\text{feas}})}{\\alpha} + \\rho \\lim_{\\alpha \\to 0^+} \\frac{|g(x_{\\text{feas}} + \\alpha d)|}{\\alpha}\n$$\n第一项是 $f(x)$ 的方向导数的定义，即 $f'(x_{\\text{feas}})d$。\n对于第二项，我们使用 $g(x)$ 在 $x_{\\text{feas}}$ 周围的一阶泰勒展开：\n$g(x_{\\text{feas}} + \\alpha d) = g(x_{\\text{feas}}) + \\alpha \\, g'(x_{\\text{feas}})d + O(\\alpha^2) = \\alpha \\, g'(x_{\\text{feas}})d + O(\\alpha^2)$。\n将此代入极限：\n$$\n\\lim_{\\alpha \\to 0^+} \\frac{|\\alpha \\, g'(x_{\\text{feas}})d + O(\\alpha^2)|}{\\alpha} = \\lim_{\\alpha \\to 0^+} \\frac{\\alpha |g'(x_{\\text{feas}})d + O(\\alpha)|}{\\alpha} = |g'(x_{\\text{feas}})d|\n$$\n合并各项，罚函数在可行点处的方向导数为：\n$$\n\\Phi_{\\rho}'(x_{\\text{feas}}; d) = f'(x_{\\text{feas}})d + \\rho |g'(x_{\\text{feas}})d|\n$$\n项 $|g'(x_{\\text{feas}})d|$ 通常不是关于 $d$ 的线性函数。要使 $\\Phi_{\\rho}$ 在 $x_{\\text{feas}}$ 处可微，我们需要 $\\Phi_{\\rho}'(x_{\\text{feas}}; d) = -\\Phi_{\\rho}'(x_{\\text{feas}}; -d)$。我们来检验这个条件：\n$$\n-\\Phi_{\\rho}'(x_{\\text{feas}}; -d) = -[f'(x_{\\text{feas}})(-d) + \\rho |g'(x_{\\text{feas}})(-d)|] = -[-f'(x_{\\text{feas}})d + \\rho |-g'(x_{\\text{feas}})d|] = f'(x_{\\text{feas}})d - \\rho |g'(x_{\\text{feas}})d|\n$$\n为了可微性，我们需要 $f'(x_{\\text{feas}})d + \\rho |g'(x_{\\text{feas}})d| = f'(x_{\\text{feas}})d - \\rho |g'(x_{\\text{feas}})d|$，这可以简化为 $2\\rho |g'(x_{\\text{feas}})d| = 0$。因为 $\\rho  0$，这要求 $g'(x_{\\text{feas}})d = 0$。\n\n对于我们的具体问题，$g'(x) = 2x$。在可行点 $x=1$ 和 $x=-1$ 处，梯度分别为 $g'(1)=2$ 和 $g'(-1)=-2$，两者都非零。因此，对于任意方向 $d \\ne 0$，我们有 $g'(x_{\\text{feas}})d \\neq 0$。因此，条件 $g'(x_{\\text{feas}})d=0$ 并非对所有方向 $d$ 都成立，所以函数 $\\Phi_{\\rho}(x)$ 在任何可行点处都不可微。项 $|g'(x_{\\text{feas}})d|$ 的存在导致了函数在 $x_{\\text{feas}}$ 处的轮廓上产生一个“尖点”。\n\n**任务 B：Armijo 条件计算**\n\n我们给定点 $x^{\\ast} = 1$，搜索方向 $d=1$，罚参数 $\\rho = \\frac{1}{2}$，以及 Armijo 参数 $c_{1} = 0.1 = \\frac{1}{10}$。我们需要找到满足 Armijo 条件的最大 $\\alpha_{\\max}0$：\n$$\n\\Phi_{\\rho}(x^{\\ast} + \\alpha d) \\le \\Phi_{\\rho}(x^{\\ast}) + c_{1} \\, \\alpha \\, s_{g}\n$$\n首先，我们计算不等式中的每一项。\n\n1.  **在 $x^{\\ast}$ 处的值**：\n    $x^{\\ast}=1$ 是可行点，因为 $g(1) = 1^2 - 1 = 0$。\n    $f(x^{\\ast}) = f(1) = (1-2)^2 = 1$。\n    $\\Phi_{\\rho}(x^{\\ast}) = \\Phi_{\\frac{1}{2}}(1) = f(1) + \\frac{1}{2}|g(1)| = 1 + \\frac{1}{2}|0| = 1$。\n\n2.  **方向导数 $s_{g}$**：\n    $s_{g}$ 是 $\\Phi_{\\rho}$ 在 $x^{\\ast}=1$ 处沿 $d=1$ 的方向导数。使用任务 A 中的公式：\n    $s_{g} = \\Phi_{\\rho}'(1; 1) = f'(1)(1) + \\rho |g'(1)(1)|$。\n    我们有 $f'(x) = 2(x-2)$，所以 $f'(1) = 2(1-2) = -2$。\n    我们有 $g'(x) = 2x$，所以 $g'(1) = 2(1) = 2$。\n    $s_{g} = (-2) + \\frac{1}{2}|2| = -2 + 1 = -1$。\n    这里提到 Clarke 方向导数是一致的，因为对于该函数在该点，它与标准的单侧方向导数重合。\n\n3.  **在试验点处的值**：\n    试验点是 $x^{\\ast} + \\alpha d = 1 + \\alpha$。对于 $\\alpha  0$。\n    $f(1+\\alpha) = ((1+\\alpha) - 2)^2 = (\\alpha - 1)^2 = \\alpha^2 - 2\\alpha + 1$。\n    $g(1+\\alpha) = (1+\\alpha)^2 - 1 = 1 + 2\\alpha + \\alpha^2 - 1 = \\alpha^2 + 2\\alpha$。\n    因为 $\\alpha  0$，$g(1+\\alpha) = \\alpha(\\alpha+2)  0$，所以 $|g(1+\\alpha)| = \\alpha^2 + 2\\alpha$。\n    $\\Phi_{\\rho}(1+\\alpha) = f(1+\\alpha) + \\rho |g(1+\\alpha)| = (\\alpha^2 - 2\\alpha + 1) + \\frac{1}{2}(\\alpha^2 + 2\\alpha) = \\frac{3}{2}\\alpha^2 - \\alpha + 1$。\n\n现在，我们将这些代入 Armijo 不等式：\n$$\n\\frac{3}{2}\\alpha^2 - \\alpha + 1 \\le 1 + \\left(\\frac{1}{10}\\right) \\alpha (-1)\n$$\n$$\n\\frac{3}{2}\\alpha^2 - \\alpha \\le -\\frac{1}{10}\\alpha\n$$\n将所有项移到一边：\n$$\n\\frac{3}{2}\\alpha^2 - \\alpha + \\frac{1}{10}\\alpha \\le 0\n$$\n$$\n\\frac{3}{2}\\alpha^2 - \\frac{9}{10}\\alpha \\le 0\n$$\n提出因子 $\\alpha$：\n$$\n\\alpha \\left(\\frac{3}{2}\\alpha - \\frac{9}{10}\\right) \\le 0\n$$\n由于我们寻找的是正步长 $\\alpha  0$，我们可以用 $\\alpha$ 除以不等式两边而不改变不等号方向：\n$$\n\\frac{3}{2}\\alpha - \\frac{9}{10} \\le 0\n$$\n$$\n\\frac{3}{2}\\alpha \\le \\frac{9}{10}\n$$\n$$\n\\alpha \\le \\frac{9}{10} \\cdot \\frac{2}{3} = \\frac{18}{30} = \\frac{3}{5}\n$$\nArmijo 条件对区间 $(0, \\frac{3}{5}]$ 中的所有 $\\alpha$ 都成立。因此，最大正步长为 $\\alpha_{\\max} = \\frac{3}{5}$。\n\n**任务 C：次梯度保障措施提议**\n\n对于非光滑罚函数，线搜索的挑战出现在当迭代点 $x_k$ 非常接近一个不可微点时（即当 $|g(x_k)|$ 很小时）。在这样一个 $g(x_k) \\neq 0$ 的点 $x_k$ 处，罚函数 $\\Phi_{\\rho}(x)$ 是可微的，其沿方向 $d$ 的方向导数为：\n$$\ns_k = \\nabla\\Phi_{\\rho}(x_k)^Td = f'(x_k)d + \\rho \\, \\text{sign}(g(x_k)) g'(x_k)d\n$$\n如果所取的步长 $\\alpha$ 使得线搜索穿过可行流形（即 $g(x_k)$ 和 $g(x_k+\\alpha d)$ 符号相反），罚函数的梯度会突然改变。斜率 $s_k$ 就不再能很好地预测函数的行为，可能导致线搜索失败或需要多次回溯。\n\n当迭代点接近可行集时，可以通过修改 Armijo 条件中使用的斜率来实现一个稳健的保障措施。该保障措施应使用一个能考虑到尖点的斜率模型。\n\n**提议：** 一个实用的线搜索算法可以采用以下次梯度保障措施：\n\n1.  定义一个小的正容差 $\\tau$。\n2.  在当前迭代点 $x$ 处，计算约束值 $g(x)$ 及其导数 $g'(x)$。\n3.  计算导数 $f'(x)$ 和搜索方向 $d$。\n4.  如果 $|g(x)| \\ge \\tau$，则认为迭代点远离尖点。使用标准方向导数作为 Armijo 检验中的斜率：\n    $$s_{\\text{slope}} = f'(x)d + \\rho \\, \\text{sign}(g(x)) g'(x)d$$\n5.  如果 $|g(x)|  \\tau$，则认为迭代点接近尖点。为防止线搜索被一个可能过于乐观的下降估计所误导，斜率被替换为尖点本身处的方向导数，该导数总是大于或等于标准方向导数：\n    $$s_{\\text{slope}} = f'(x)d + \\rho \\, |g'(x)d|$$\n然后，这个受保障的斜率 $s_{\\text{slope}}$ 用于 Armijo 条件中：$\\Phi_{\\rho}(x + \\alpha d) \\le \\Phi_{\\rho}(x) + c_{1} \\, \\alpha \\, s_{\\text{slope}}$。此过程仅使用可从问题数据（$f, g, \\rho$）和当前迭代点/方向计算出的量，并通过在接近可行集时使用更保守的局部下降模型来稳健地处理尖点的几何形状。",
            "answer": "$$\\boxed{\\frac{3}{5}}$$"
        },
        {
            "introduction": "理论分析和计算练习为我们奠定了基础，现在是时候将这些见解付诸代码了。非光滑价值函数导致的“之字形”下降（zig-zagging）是优化算法中一个经典且棘手的问题。 在这个编程实践中，你将亲手实现一个梯度下降算法，并观察非光滑的$\\ell_1$惩罚如何导致不稳定的收敛路径。更重要的是，你将实现并测试两种平滑技术，直观地比较它们在改善全局收敛性能和“驯服”扭结方面的效果，从而将理论知识转化为强大的实践技能。",
            "id": "3149286",
            "problem": "考虑一个无约束最小化问题，其通过对一个价值函数进行线搜索来实现全局化。该价值函数结合了一个光滑目标函数和一个等式约束的罚函数。设目标函数为 $f(\\mathbf{x}) = (x_1 - 1)^2 + \\kappa (x_2 - 1)^2$，其中 $\\mathbf{x} \\in \\mathbb{R}^2$，单一等式约束的残差为 $c(\\mathbf{x}) = x_1 + x_2 - 1$。一个通用的价值函数为 $\\Phi(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\,\\psi(c(\\mathbf{x}))$，其中 $\\mu  0$ 是一个罚参数，$\\psi$ 是一个从 $\\mathbb{R}$到 $\\mathbb{R}_{\\ge 0}$ 的罚映射。\n\n基本原理和定义：\n- 使用基于梯度的下降法，并结合回溯线搜索和Armijo充分下降条件。在第 $k$ 次迭代中，下降方向为 $p_k = -\\nabla \\Phi(\\mathbf{x}_k)$，步长 $\\alpha_k$ 通过回溯选择，直到对于一个固定的 $\\sigma \\in (0,1)$，Armijo条件 $\\Phi(\\mathbf{x}_k + \\alpha_k p_k) \\le \\Phi(\\mathbf{x}_k) + \\sigma \\alpha_k \\nabla \\Phi(\\mathbf{x}_k)^\\top p_k$ 成立。\n- 回溯过程会以一个常数因子 $\\beta \\in (0,1)$ 缩短步长，直到满足Armijo条件。\n- 全局化指的是使用价值函数和线搜索来确保从任意初始点开始的收敛性。\n\n非光滑和光滑罚函数：\n- 非光滑罚函数：$\\psi_{\\text{ns}}(t) = |t|$，它在 $t = 0$ 处不可微，与基于梯度的方法结合使用时可能导致Z字形震荡。\n- 用于缓解Z字形震荡的光滑罚函数：\n  1. 光滑绝对值：$\\psi_{\\sqrt{}}(t) = \\sqrt{t^2 + \\varepsilon^2}$，其中 $\\varepsilon  0$，是 $|t|$ 的一个可微近似。\n  2. Huber型光滑：$\\psi_{\\text{Huber}}(t) = \\begin{cases} \\dfrac{t^2}{2\\varepsilon},  |t| \\le \\varepsilon \\\\ |t| - \\dfrac{\\varepsilon}{2},  |t|  \\varepsilon \\end{cases}$，其中 $\\varepsilon  0$，它是连续可微的，并且在原点的一个邻域之外与绝对值函数一致。\n\n你的任务：\n- 实现使用Armijo条件的梯度下降和回溯线搜索算法，用于非光滑价值函数 $\\Phi_{\\text{ns}}(\\mathbf{x}) = f(\\mathbf{x}) + \\mu |c(\\mathbf{x})|$ 和光滑价值函数 $\\Phi_{\\text{sm}}(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\psi_{\\text{sm}}(c(\\mathbf{x}))$，其中 $\\psi_{\\text{sm}}$ 根据每个测试用例的指定，为 $\\psi_{\\sqrt{}}$ 或 $\\psi_{\\text{Huber}}$。\n- 使用以下固定的线搜索参数：Armijo参数 $\\sigma = 10^{-4}$，回溯缩减因子 $\\beta = \\tfrac{1}{2}$，每次迭代的最大回溯次数 $M_{\\text{bt}} = 50$，梯度范数的终止容差 $\\tau = 10^{-6}$，以及最大迭代次数 $N_{\\max} = 200$。\n- 对于 $c(\\mathbf{x}) = 0$ 的非光滑情况，为确定起见，选择次梯度 $\\nabla |c| = \\operatorname{sign}(c) \\nabla c$ 并设 $\\operatorname{sign}(0) = 0$。\n\nZ字形震荡和全局化指标：\n- 将Z字形计数定义为迭代次数 $k \\ge 1$ 中，连续下降方向 $p_{k-1}$ 和 $p_k$ 之间的夹角超过 $\\tfrac{\\pi}{2}$ （以弧度计）的次数。角度必须以弧度计算和比较。\n- 将约束符号翻转计数定义为迭代次数 $k \\ge 1$ 中，$\\operatorname{sign}(c(\\mathbf{x}_k)) \\neq \\operatorname{sign}(c(\\mathbf{x}_{k-1}))$ 的次数。\n- 将回溯计数定义为在整个过程中回溯线搜索缩减步长的总次数（即，导致 $\\alpha \\leftarrow \\beta \\alpha$ 的不成功Armijo测试的总次数）。\n- 记录终止时的最终价值函数值 $\\Phi(\\mathbf{x}_{\\text{final}})$。\n\n测试套件：\n对于每个测试用例，运行算法两次：一次使用非光滑价值函数，一次使用指定的光滑价值函数。请严格使用所述参数。\n\n- 测试用例1：$\\mu = 50.0$，$\\kappa = 100.0$，$\\mathbf{x}_0 = (2.0, -1.0)$，$\\varepsilon = 10^{-3}$，光滑方法 $\\psi_{\\sqrt{}}$。\n- 测试用例2：$\\mu = 200.0$，$\\kappa = 10.0$，$\\mathbf{x}_0 = (0.8, 0.8)$，$\\varepsilon = 10^{-3}$，光滑方法 $\\psi_{\\sqrt{}}$。\n- 测试用例3：$\\mu = 100.0$，$\\kappa = 1000.0$，$\\mathbf{x}_0 = (-1.0, 2.0)$，$\\varepsilon = 10^{-4}$，光滑方法 $\\psi_{\\text{Huber}}$。\n- 测试用例4：$\\mu = 20.0$，$\\kappa = 1.0$，$\\mathbf{x}_0 = (0.6, 0.4)$，$\\varepsilon = 10^{-3}$，光滑方法 $\\psi_{\\sqrt{}}$。\n\n输出规格：\n- 对于每个测试用例，按顺序 $[\\text{zigzag}_{\\text{ns}}, \\text{zigzag}_{\\text{sm}}, \\text{signflip}_{\\text{ns}}, \\text{signflip}_{\\text{sm}}, \\text{backtracks}_{\\text{ns}}, \\text{backtracks}_{\\text{sm}}, \\Phi_{\\text{ns, final}}, \\Phi_{\\text{sm, final}}]$ 输出一个包含八个值的列表。所有角度、比较和计数都必须使用弧度。每个值必须是布尔值、整数或浮点数。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素对应一个测试用例，并且本身是一个包含上述八个值的列表（例如，$[[\\dots],[\\dots],[\\dots],[\\dots]]$）。",
            "solution": "该问题要求实现一个带有回溯线搜索的梯度下降算法，以最小化一个价值函数。该价值函数由一个二次目标函数和一个等式约束的罚函数项组成。任务的核心是比较使用非光滑绝对值罚函数与使用其两种不同光滑近似时算法的行为。性能评估基于旨在量化Z字形震荡等收敛困难的指标。\n\n首先，我们定义问题的各个组成部分。\n目标函数为 $f(\\mathbf{x}): \\mathbb{R}^2 \\to \\mathbb{R}$，由下式给出：\n$$ f(\\mathbf{x}) = (x_1 - 1)^2 + \\kappa (x_2 - 1)^2 $$\n其梯度 $\\nabla f(\\mathbf{x})$ 为：\n$$ \\nabla f(\\mathbf{x}) = \\begin{pmatrix} 2(x_1 - 1) \\\\ 2\\kappa(x_2 - 1) \\end{pmatrix} $$\n等式约束通过残差函数 $c(\\mathbf{x}): \\mathbb{R}^2 \\to \\mathbb{R}$ 表示：\n$$ c(\\mathbf{x}) = x_1 + x_2 - 1 $$\n约束残差的梯度 $\\nabla c(\\mathbf{x})$ 是一个常向量：\n$$ \\nabla c(\\mathbf{x}) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n\n价值函数 $\\Phi(\\mathbf{x})$ 结合了目标函数和约束罚函数：\n$$ \\Phi(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\psi(c(\\mathbf{x})) $$\n其中 $\\mu  0$ 是罚参数，$\\psi$ 是一个罚映射。我们分析 $\\psi$ 的三种形式。\n\n1.  **非光滑价值函数 ($\\Phi_{\\text{ns}}$)**\n    罚函数是绝对值函数 $\\psi_{\\text{ns}}(t) = |t|$。\n    价值函数为：\n    $$ \\Phi_{\\text{ns}}(\\mathbf{x}) = (x_1 - 1)^2 + \\kappa (x_2 - 1)^2 + \\mu |x_1 + x_2 - 1| $$\n    此函数在 $c(\\mathbf{x}) = 0$ 处不可微。我们使用其次梯度，该次梯度通过次微分的链式法则定义。$|t|$ 的次导数在 $t \\neq 0$ 时为 $\\operatorname{sign}(t)$，在 $t=0$ 时为区间 $[-1, 1]$。问题指定使用 $\\operatorname{sign}(0) = 0$。\n    次梯度为：\n    $$ \\nabla \\Phi_{\\text{ns}}(\\mathbf{x}) = \\nabla f(\\mathbf{x}) + \\mu \\operatorname{sign}(c(\\mathbf{x})) \\nabla c(\\mathbf{x}) = \\begin{pmatrix} 2(x_1 - 1) \\\\ 2\\kappa(x_2 - 1) \\end{pmatrix} + \\mu \\operatorname{sign}(x_1 + x_2 - 1) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n\n2.  **光滑绝对值价值函数 ($\\Phi_{\\sqrt{}}$)**\n    罚函数是绝对值的光滑近似 $\\psi_{\\sqrt{}}(t) = \\sqrt{t^2 + \\varepsilon^2}$，其中 $\\varepsilon  0$。\n    价值函数为：\n    $$ \\Phi_{\\sqrt{}}(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\sqrt{c(\\mathbf{x})^2 + \\varepsilon^2} $$\n    此函数处处可微。其梯度使用链式法则求得。$\\psi_{\\sqrt{}}(t)$ 的导数为 $\\psi'_{\\sqrt{}}(t) = \\frac{t}{\\sqrt{t^2 + \\varepsilon^2}}$。\n    梯度为：\n    $$ \\nabla \\Phi_{\\sqrt{}}(\\mathbf{x}) = \\nabla f(\\mathbf{x}) + \\mu \\frac{c(\\mathbf{x})}{\\sqrt{c(\\mathbf{x})^2 + \\varepsilon^2}} \\nabla c(\\mathbf{x}) = \\begin{pmatrix} 2(x_1 - 1) \\\\ 2\\kappa(x_2 - 1) \\end{pmatrix} + \\mu \\frac{x_1 + x_2 - 1}{\\sqrt{(x_1 + x_2 - 1)^2 + \\varepsilon^2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n\n3.  **Huber型光滑价值函数 ($\\Phi_{\\text{Huber}}$)**\n    罚函数是Huber损失函数 $\\psi_{\\text{Huber}}(t)$，它是连续可微的。\n    $$ \\psi_{\\text{Huber}}(t) = \\begin{cases} \\frac{t^2}{2\\varepsilon},  |t| \\le \\varepsilon \\\\ |t| - \\frac{\\varepsilon}{2},  |t|  \\varepsilon \\end{cases} $$\n    价值函数为 $\\Phi_{\\text{Huber}}(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\psi_{\\text{Huber}}(c(\\mathbf{x}))$。$\\psi_{\\text{Huber}}(t)$ 的导数为：\n    $$ \\psi'_{\\text{Huber}}(t) = \\begin{cases} \\frac{t}{\\varepsilon},  |t| \\le \\varepsilon \\\\ \\operatorname{sign}(t),  |t|  \\varepsilon \\end{cases} $$\n    价值函数的梯度为：\n    $$ \\nabla \\Phi_{\\text{Huber}}(\\mathbf{x}) = \\nabla f(\\mathbf{x}) + \\mu \\psi'_{\\text{Huber}}(c(\\mathbf{x})) \\nabla c(\\mathbf{x}) = \\begin{pmatrix} 2(x_1 - 1) \\\\ 2\\kappa(x_2 - 1) \\end{pmatrix} + \\mu \\psi'_{\\text{Huber}}(x_1+x_2-1) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n\n优化算法是梯度下降法。在每次迭代 $k$ 中，从初始点 $\\mathbf{x}_0$ 开始，下一个点 $\\mathbf{x}_{k+1}$ 通过下式求得：\n$$ \\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k p_k $$\n其中 $p_k = -\\nabla \\Phi(\\mathbf{x}_k)$ 是下降方向（对于 $\\Phi_{\\text{ns}}$ 则是基于次梯度的方向）。步长 $\\alpha_k$ 由回溯线搜索确定，该搜索从 $\\alpha=1$ 开始，并以因子 $\\beta \\in (0,1)$ 连续减小步长，直到满足Armijo充分下降条件：\n$$ \\Phi(\\mathbf{x}_k + \\alpha p_k) \\le \\Phi(\\mathbf{x}_k) + \\sigma \\alpha \\nabla \\Phi(\\mathbf{x}_k)^\\top p_k $$\n对于一个固定的 $\\sigma \\in (0,1)$。当梯度（或次梯度）的范数低于容差 $\\tau$（即 $\\|\\nabla \\Phi(\\mathbf{x}_k)\\|_2 \\le \\tau$）时，或达到最大迭代次数 $N_{\\max}$ 时，过程终止。\n\n计算以下指标来评估算法的性能：\n- **Z字形计数**：迭代次数 $k \\ge 1$ 中，连续下降方向 $p_{k-1}$ 和 $p_k$ 之间的夹角大于 $\\frac{\\pi}{2}$ 弧度的次数。这计算为满足 $\\arccos\\left(\\frac{p_k^\\top p_{k-1}}{\\|p_k\\|_2 \\|p_{k-1}\\|_2}\\right)  \\frac{\\pi}{2}$ 的 $k \\ge 1$ 的次数。\n- **约束符号翻转计数**：迭代次数 $k \\ge 1$ 中，约束残差的符号发生变化的次数，即 $\\operatorname{sign}(c(\\mathbf{x}_k)) \\neq \\operatorname{sign}(c(\\mathbf{x}_{k-1}))$。\n- **回溯计数**：在整个优化运行期间，步长 $\\alpha$ 被缩减的总次数。\n- **最终价值函数值**：终止点处的 $\\Phi(\\mathbf{x}_{\\text{final}})$ 的值。\n\n实现将包含一个执行梯度下降的主循环，应用指定的价值函数及其梯度。终止后，使用迭代点和下降方向的历史记录来计算所需的指标。对每个测试用例重复此过程，一次用于非光滑罚函数，一次用于指定的光滑罚函数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Global parameters defined in the problem statement\n    SIGMA = 1e-4\n    BETA = 0.5\n    M_BT = 50\n    TAU = 1e-6\n    N_MAX = 200\n\n    # Objective function and its gradient\n    def f(x, kappa):\n        return (x[0] - 1.0)**2 + kappa * (x[1] - 1.0)**2\n\n    def grad_f(x, kappa):\n        return np.array([2.0 * (x[0] - 1.0), 2.0 * kappa * (x[1] - 1.0)])\n\n    # Constraint residual and its gradient\n    def c(x):\n        return x[0] + x[1] - 1.0\n\n    def grad_c():\n        return np.array([1.0, 1.0])\n\n    # --- Merit Function Classes ---\n    class MeritFunction:\n        def __init__(self, kappa, mu):\n            self.kappa = kappa\n            self.mu = mu\n\n        def phi(self, x):\n            raise NotImplementedError\n\n        def grad_phi(self, x):\n            raise NotImplementedError\n\n    class NonsmoothMerit(MeritFunction):\n        def phi(self, x):\n            return f(x, self.kappa) + self.mu * np.abs(c(x))\n\n        def grad_phi(self, x):\n            c_val = c(x)\n            return grad_f(x, self.kappa) + self.mu * np.sign(c_val) * grad_c()\n\n    class SqrtMerit(MeritFunction):\n        def __init__(self, kappa, mu, epsilon):\n            super().__init__(kappa, mu)\n            self.epsilon = epsilon\n            self.epsilon_sq = epsilon**2\n\n        def phi(self, x):\n            c_val = c(x)\n            return f(x, self.kappa) + self.mu * np.sqrt(c_val**2 + self.epsilon_sq)\n\n        def grad_phi(self, x):\n            c_val = c(x)\n            denominator = np.sqrt(c_val**2 + self.epsilon_sq)\n            penalty_grad_factor = self.mu * c_val / denominator if denominator > 0 else 0\n            return grad_f(x, self.kappa) + penalty_grad_factor * grad_c()\n\n    class HuberMerit(MeritFunction):\n        def __init__(self, kappa, mu, epsilon):\n            super().__init__(kappa, mu)\n            self.epsilon = epsilon\n\n        def phi(self, x):\n            c_val = c(x)\n            if np.abs(c_val) = self.epsilon:\n                penalty = c_val**2 / (2.0 * self.epsilon)\n            else:\n                penalty = np.abs(c_val) - self.epsilon / 2.0\n            return f(x, self.kappa) + self.mu * penalty\n\n        def grad_phi(self, x):\n            c_val = c(x)\n            if np.abs(c_val) = self.epsilon:\n                penalty_deriv = c_val / self.epsilon\n            else:\n                penalty_deriv = np.sign(c_val)\n            return grad_f(x, self.kappa) + self.mu * penalty_deriv * grad_c()\n\n    # --- Optimizer ---\n    def run_optimizer(x0, merit_function):\n        x = np.array(x0, dtype=float)\n        \n        x_history = [x]\n        p_history = []\n        total_backtracks = 0\n        \n        for _ in range(N_MAX):\n            grad = merit_function.grad_phi(x)\n            \n            grad_norm = np.linalg.norm(grad)\n            if grad_norm = TAU:\n                break\n                \n            p = -grad\n            p_history.append(p)\n            \n            # Backtracking line search\n            phi_k = merit_function.phi(x)\n            grad_p_dot = np.dot(grad, p)\n            \n            alpha = 1.0\n            num_bt = 0\n            while num_bt  M_BT:\n                x_next = x + alpha * p\n                if merit_function.phi(x_next) = phi_k + SIGMA * alpha * grad_p_dot:\n                    break\n                alpha *= BETA\n                total_backtracks += 1\n                num_bt += 1\n            \n            x = x + alpha * p\n            x_history.append(x)\n            \n        x_final = x_history[-1]\n        phi_final = merit_function.phi(x_final)\n\n        # Zig-zag count\n        zigzag_count = 0\n        if len(p_history) > 1:\n            for i in range(1, len(p_history)):\n                p_prev, p_curr = p_history[i-1], p_history[i]\n                norm_prev, norm_curr = np.linalg.norm(p_prev), np.linalg.norm(p_curr)\n                if norm_prev > 1e-12 and norm_curr > 1e-12:\n                    cos_theta = np.dot(p_curr, p_prev) / (norm_curr * norm_prev)\n                    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n                    angle = np.arccos(cos_theta)\n                    if angle > np.pi / 2.0:\n                        zigzag_count += 1\n        \n        # Sign-flip count\n        signflip_count = 0\n        if len(x_history) > 1:\n            for i in range(1, len(x_history)):\n                c_prev = c(x_history[i-1])\n                c_curr = c(x_history[i])\n                if np.sign(c_curr) != np.sign(c_prev):\n                    signflip_count += 1\n                    \n        return zigzag_count, signflip_count, total_backtracks, phi_final\n\n    # --- Test Suite ---\n    test_cases = [\n        {'mu': 50.0, 'kappa': 100.0, 'x0': (2.0, -1.0), 'epsilon': 1e-3, 'smoothing': 'sqrt'},\n        {'mu': 200.0, 'kappa': 10.0, 'x0': (0.8, 0.8), 'epsilon': 1e-3, 'smoothing': 'sqrt'},\n        {'mu': 100.0, 'kappa': 1000.0, 'x0': (-1.0, 2.0), 'epsilon': 1e-4, 'smoothing': 'huber'},\n        {'mu': 20.0, 'kappa': 1.0, 'x0': (0.6, 0.4), 'epsilon': 1e-3, 'smoothing': 'sqrt'},\n    ]\n    \n    all_results = []\n    \n    for case in test_cases:\n        mu, kappa, x0, epsilon = case['mu'], case['kappa'], case['x0'], case['epsilon']\n        \n        # Run nonsmooth case\n        ns_merit = NonsmoothMerit(kappa, mu)\n        ns_metrics = run_optimizer(x0, ns_merit)\n        \n        # Run smoothed case\n        if case['smoothing'] == 'sqrt':\n            sm_merit = SqrtMerit(kappa, mu, epsilon)\n        else: # huber\n            sm_merit = HuberMerit(kappa, mu, epsilon)\n        \n        sm_metrics = run_optimizer(x0, sm_merit)\n        \n        case_results = [\n            ns_metrics[0], sm_metrics[0],  # zigzag\n            ns_metrics[1], sm_metrics[1],  # signflip\n            ns_metrics[2], sm_metrics[2],  # backtracks\n            ns_metrics[3], sm_metrics[3]   # phi_final\n        ]\n        all_results.append(case_results)\n        \n    formatted_results = []\n    for res_list in all_results:\n        formatted_list = f\"[{','.join(map(str, res_list))}]\"\n        formatted_results.append(formatted_list)\n    \n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}