{
    "hands_on_practices": [
        {
            "introduction": "The core of Sequential Quadratic Programming lies in iteratively solving a simplified model of the original problem. This first practice will guide you through the fundamental process of constructing and solving the very first QP subproblem for a simple nonlinear optimization task . By starting from a non-feasible point, you will learn how to linearize constraints and use the objective's gradient to determine the initial search direction, a crucial first step towards finding a constrained optimum.",
            "id": "2201995",
            "problem": "Consider the optimization problem of finding the point $(x_1, x_2)$ that is closest to the origin, subject to the constraint that the point must lie on the line defined by the equation $x_1 - x_2 + 1 = 0$. This problem can be formulated as minimizing the squared Euclidean distance to the origin:\n$$\n\\begin{aligned}\n\\min_{x_1, x_2} \\quad & f(x_1, x_2) = x_1^2 + x_2^2 \\\\\n\\text{s.t.} \\quad & h(x_1, x_2) = x_1 - x_2 + 1 = 0\n\\end{aligned}\n$$\nYou are to perform one iteration of the Sequential Quadratic Programming (SQP) method to solve this problem. Starting from the initial, non-feasible point $x_0 = (1, 1)^T$, determine the search direction $p_0 = (p_1, p_2)^T$ that is calculated in this first step.\n\nYour final answer should be a row matrix containing the components of the search direction vector, $p_1$ and $p_2$.",
            "solution": "We are minimizing $f(x_{1},x_{2})=x_{1}^{2}+x_{2}^{2}$ subject to the equality constraint $h(x_{1},x_{2})=x_{1}-x_{2}+1=0$. The Lagrangian is $L(x,\\lambda)=f(x)+\\lambda h(x)$. Its Hessian with respect to $x$ is $\\nabla_{xx}^{2}L(x,\\lambda)=\\nabla^{2}f(x)+\\lambda\\nabla^{2}h(x)=2I$, since $h$ is linear.\n\nAt the initial point $x_{0}=(1,1)^{T}$, we have $\\nabla f(x_{0})=2x_{0}=(2,2)^{T}$, $h(x_{0})=1$, and $\\nabla h(x_{0})=(1,-1)^{T}$. The SQP subproblem is\n$$\n\\min_{p}\\; \\frac{1}{2}p^{T}(2I)p+\\nabla f(x_{0})^{T}p\n\\quad\\text{s.t.}\\quad h(x_{0})+\\nabla h(x_{0})^{T}p=0.\n$$\nThis simplifies to\n$$\n\\min_{p_{1},p_{2}}\\; p_{1}^{2}+p_{2}^{2}+2p_{1}+2p_{2}\n\\quad\\text{s.t.}\\quad p_{1}-p_{2}=-1.\n$$\nForm the Lagrangian of the subproblem $Q(p,\\mu)=p_{1}^{2}+p_{2}^{2}+2p_{1}+2p_{2}+\\mu\\,(p_{1}-p_{2}+1)$ and set stationarity conditions:\n$$\n\\frac{\\partial Q}{\\partial p_{1}}=2p_{1}+2+\\mu=0,\\qquad\n\\frac{\\partial Q}{\\partial p_{2}}=2p_{2}+2-\\mu=0,\n$$\nwith the constraint $p_{1}-p_{2}+1=0$. From stationarity, $\\mu=-2p_{1}-2=-(2p_{2}+2)$, hence $p_{1}+p_{2}=-2$. Together with $p_{1}-p_{2}=-1$, solving gives $p_{1}=-\\frac{3}{2}$ and $p_{2}=-\\frac{1}{2}$.\n\nTherefore, the SQP search direction at the first step is $p_{0}=\\left(-\\frac{3}{2},-\\frac{1}{2}\\right)$.",
            "answer": "$$\\boxed{\\begin{pmatrix}-\\frac{3}{2} & -\\frac{1}{2}\\end{pmatrix}}$$"
        },
        {
            "introduction": "While SQP is powerful, it is not without its potential pitfalls, which are crucial to understand for robust application. This exercise explores a scenario where the true Hessian of the Lagrangian is used in the QP subproblem, leading to a model that is not convex . You will discover how this can result in an unbounded subproblem, highlighting why practical SQP implementations often modify the Hessian to ensure the quadratic model has a well-defined minimum.",
            "id": "2202011",
            "problem": "Sequential Quadratic Programming (SQP) is an iterative method for solving constrained Non-Linear Programming (NLP) problems. At each iteration $k$, given a current point $x_k$ and a Lagrange multiplier estimate $\\lambda_k$, a search direction $p$ is found by solving a Quadratic Programming (QP) subproblem.\n\nConsider the following one-dimensional NLP:\nMinimize $f(x) = -x^3$ subject to the constraint $x \\ge 0$.\n\nLet's analyze the first step of an SQP method starting from the feasible point $x_0 = 1$. The QP subproblem to find the search direction $p$ is formulated as:\n$$\n\\begin{align*}\n\\min_{p} \\quad & \\nabla f(x_0)^T p + \\frac{1}{2} p^T (\\nabla_{xx}^2 L(x_0, \\lambda_0)) p \\\\\n\\text{s.t.} \\quad & c(x_0) + \\nabla c(x_0)^T p \\ge 0\n\\end{align*}\n$$\nwhere $L(x, \\lambda) = f(x) - \\lambda c(x)$ is the Lagrangian function for the problem $\\min f(x)$ s.t. $c(x) \\ge 0$. For this problem, we use the true Hessian of the Lagrangian and an initial Lagrange multiplier estimate of $\\lambda_0 = 0$.\n\nWhich of the following statements correctly describes the solution to this QP subproblem?\n\nA. The QP subproblem is infeasible.\n\nB. The QP subproblem has a unique minimizer at $p = 0$.\n\nC. The QP subproblem is unbounded below.\n\nD. The QP subproblem has a unique minimizer at $p = -0.5$.\n\nE. The QP subproblem has a unique minimizer at $p = -1$.",
            "solution": "We define the inequality constraint as $c(x)=x$, so the problem is $\\min f(x)$ subject to $c(x)\\ge 0$. The Lagrangian is $L(x,\\lambda)=f(x)-\\lambda c(x)$.\n\nAt the starting point $x_{0}=1$ with $\\lambda_{0}=0$, compute the ingredients of the QP subproblem:\n- Objective gradient: $\\nabla f(x)=f'(x)=-3x^{2}$, hence $\\nabla f(x_{0})=-3$.\n- Constraint and its gradient: $c(x)=x$, so $c(x_{0})=1$ and $\\nabla c(x_{0})=c'(x_{0})=1$.\n- Hessian of the Lagrangian: $\\nabla_{xx}^{2}L(x,\\lambda)=f''(x)-\\lambda c''(x)$. Since $f''(x)=-6x$ and $c''(x)=0$, we have $\\nabla_{xx}^{2}L(x_{0},\\lambda_{0})=-6$.\n\nTherefore, the QP subproblem in the step direction $p$ is\n$$\n\\min_{p}\\;\\; \\nabla f(x_{0})\\,p+\\frac{1}{2}\\,p^{2}\\,\\nabla_{xx}^{2}L(x_{0},\\lambda_{0})\n=\\min_{p}\\;\\;(-3)p+\\frac{1}{2}(-6)p^{2}\n=\\min_{p}\\;\\; -3p-3p^{2}\n$$\nsubject to the linearized constraint\n$$\nc(x_{0}) + \\nabla c(x_{0})\\,p \\ge 0 \\;\\;\\Longleftrightarrow\\;\\; 1+p\\ge 0 \\;\\;\\Longleftrightarrow\\;\\; p\\ge -1.\n$$\n\nThe feasible set is the half-line $\\{p\\in\\mathbb{R}:p\\ge -1\\}$. The objective is a concave quadratic $\\phi(p)=-3p-3p^{2}$ with leading coefficient $-3<0$. As $p\\to +\\infty$ within the feasible set, $\\phi(p)=-3p^{2}-3p\\to -\\infty$. Hence the QP subproblem is unbounded below.\n\nTherefore, the correct statement is that the QP subproblem is unbounded below.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "This advanced, code-based practice moves beyond calculating a single step to connect SQP theory with practical sensitivity analysis. You will explore the profound relationship between the Lagrange multipliers, which arise from the KKT conditions of the SQP subproblem, and the sensitivity of the optimal objective value to changes in the constraints . By numerically verifying the identity $\\frac{df^*}{db} = -\\lambda^*$, you will gain a tangible understanding of how Lagrange multipliers represent the \"shadow price\" of a constraint, a cornerstone concept in economics and engineering design.",
            "id": "3180254",
            "problem": "Consider a parameterized, strictly convex quadratic optimization problem with a single linear equality constraint and bound constraints. Let the objective function be $f(x) = \\frac{1}{2} x^\\top Q x + c^\\top x$ with $Q = \\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}$, $c = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, decision vector $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$, and constraints $x_1 \\ge 0$, $x_2 \\ge 0$, and $a^\\top x = b$ with $a = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$. The parameter $b \\in \\mathbb{R}$ is the right-hand side of the equality constraint and determines the feasible set. Assume a constraint qualification and second-order sufficient conditions hold at the solution for all tested values of $b$.\n\nStarting from the definitions of the Lagrangian function, Karush-Kuhn-Tucker (KKT) conditions, and local sensitivity principles for parametric constrained optimization, you must:\n\n1. For each given value of $b$, solve the constrained optimization problem using a single iteration of Sequential Quadratic Programming (SQP) applied to this problem class, noting that the quadratic objective with linear constraints makes the SQP subproblem exact in one step. Compute both the optimal solution $x^\\star(b)$ and the associated Lagrange multiplier(s) for the equality constraint.\n2. Use the multiplier corresponding to the equality constraint to form a sensitivity-based prediction of the derivative of the optimal value function $f^\\star(b)$ with respect to $b$.\n3. Independently estimate the derivative of $f^\\star(b)$ with respect to $b$ by finite differences. Use a central difference $\\frac{f^\\star(b+h) - f^\\star(b-h)}{2h}$ when $b-h$ is feasible; otherwise, use a forward difference $\\frac{f^\\star(b+h) - f^\\star(b)}{h}$. Choose a small positive stepsize $h$.\n4. For each test case, compute the absolute difference between the sensitivity-based prediction and the finite-difference estimate. These absolute differences are the quantifiable results.\n\nThere are no physical units or angles in this problem. Your program must implement the above steps for the following test suite of parameter values:\n- Happy path interior case: $b = 1.0$.\n- Active-set case with one bound active: $b = 0.25$.\n- Feasible-domain boundary case: $b = 0.0$.\n- Transition case where the active set changes: $b = 0.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases provided (for example, $[r_1,r_2,r_3,r_4]$), where each $r_i$ is the absolute difference (a nonnegative real number) for the corresponding test case.",
            "solution": "The user-provided problem is a well-posed, scientifically grounded problem in the field of mathematical optimization. It is complete, objective, and its solution is verifiable. Therefore, it is deemed a valid problem.\n\nThe problem requires solving a parameterized quadratic program (QP) and performing a sensitivity analysis on its optimal value with respect to the parameter. We are given the objective function $f(x) = \\frac{1}{2} x^\\top Q x + c^\\top x$, where $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$, $Q = \\begin{bmatrix} 2 & 0 \\\\ 0 & 6 \\end{bmatrix}$, and $c = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$. The constraints are $x_1 \\ge 0$, $x_2 \\ge 0$, and a linear equality constraint $a^\\top x = b$, where $a = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ and $b$ is a parameter.\n\nThe task is to:\n1. Solve the QP for several values of $b$ to find the optimal solution $x^\\star(b)$ and the Lagrange multiplier for the equality constraint, $\\lambda^\\star(b)$. The problem notes that for a QP with linear constraints, a single iteration of Sequential Quadratic Programming (SQP) is equivalent to solving the original QP directly, as the SQP subproblem is an exact representation of the original problem.\n2. Predict the derivative of the optimal value function, $\\frac{df^\\star(b)}{db}$, using the sensitivity result $\\frac{df^\\star(b)}{db} = -\\lambda^\\star(b)$.\n3. Estimate the same derivative using a finite-difference scheme.\n4. Calculate the absolute difference between the prediction and the estimate.\n\nThe Lagrangian function for this problem, treating the bound constraints explicitly, is:\n$$L(x, \\lambda, \\mu) = \\frac{1}{2}x^\\top Q x + c^\\top x + \\lambda(a^\\top x - b) - \\mu^\\top x$$\nwhere $\\lambda$ is the multiplier for the equality constraint $a^\\top x - b = 0$, and $\\mu = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\end{bmatrix}$ contains the multipliers for the non-negativity constraints $-x_1 \\le 0$ and $-x_2 \\le 0$.\n\nThe Karush-Kuhn-Tucker (KKT) conditions for optimality are:\n1.  **Stationarity:** $\\nabla_x L = Qx + c + \\lambda a - \\mu = 0$\n2.  **Primal Feasibility:** $a^\\top x = b$, and $x \\ge 0$ (element-wise)\n3.  **Dual Feasibility:** $\\mu \\ge 0$ (element-wise)\n4.  **Complementary Slackness:** $\\mu_i x_i = 0$ for $i=1,2$.\n\nWe solve the QP by analyzing distinct cases based on the active set of the inequality constraints.\n\n**Case 1: No bound constraints are active ($x_1 > 0, x_2 > 0$)**\nFrom complementary slackness, $\\mu_1 = \\mu_2 = 0$. The KKT system simplifies to a linear system in $x$ and $\\lambda$:\n$$ \\begin{bmatrix} Q & a \\\\ a^\\top & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ \\lambda \\end{bmatrix} = \\begin{bmatrix} -c \\\\ b \\end{bmatrix} \\implies \\begin{bmatrix} 2 & 0 & 1 \\\\ 0 & 6 & 1 \\\\ 1 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\lambda \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ -2 \\\\ b \\end{bmatrix} $$\nSolving this system yields:\n$x_1(b) = \\frac{1+6b}{8}$, $x_2(b) = \\frac{2b-1}{8}$, and $\\lambda(b) = -\\frac{5+6b}{4}$.\nThis solution is valid only when its underlying assumptions hold, i.e., $x_1 > 0$ and $x_2 > 0$.\n$x_1 > 0 \\implies 1+6b > 0 \\implies b > -\\frac{1}{6}$.\n$x_2 > 0 \\implies 2b-1 > 0 \\implies b > \\frac{1}{2}$.\nThus, this internal solution is valid for $b > \\frac{1}{2}$.\n\n**Case 2: The constraint $x_2 \\ge 0$ is active ($x_2 = 0, x_1 > 0$)**\nFrom complementary slackness, $\\mu_1 = 0$. Primal feasibility gives $x_2 = 0$ and $x_1+x_2 = b \\implies x_1 = b$.\nThe stationarity conditions are:\n$2x_1 + 1 + \\lambda - \\mu_1 = 0 \\implies 2b + 1 + \\lambda = 0 \\implies \\lambda = -2b-1$.\n$6x_2 + 2 + \\lambda - \\mu_2 = 0 \\implies 2 + \\lambda - \\mu_2 = 0 \\implies \\mu_2 = 2+\\lambda = 2 + (-2b-1) = 1-2b$.\nThis solution is valid when $x_1 > 0$ (i.e., $b>0$) and $\\mu_2 \\ge 0$ (i.e., $1-2b \\ge 0 \\implies b \\le \\frac{1}{2}$).\nThus, this boundary solution is valid for $0 < b \\le \\frac{1}{2}$.\n\n**Case 3: Both constraints $x_1 \\ge 0, x_2 \\ge 0$ are active ($x_1=0, x_2=0$)**\nPrimal feasibility requires $x_1+x_2=b \\implies b=0$.\nThe problem reduces to finding the optimal solution at $x^\\star = [0, 0]^\\top$. This is the only feasible point.\nThe optimal value is $f^\\star(0) = f(0,0) = 0$. The KKT conditions must still be satisfied by some multipliers. As $b \\to 0^+$ from Case 2, we have $\\lambda \\to -2(0)-1 = -1$. We take this limiting value, $\\lambda^\\star(0) = -1$.\nLet's check: with $\\lambda=-1$, stationarity gives $\\mu = c + \\lambda a = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} + (-1)\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. Since $\\mu \\ge 0$, this is a valid KKT point.\nSo for $b=0$, the solution is $x^\\star(0) = [0, 0]^\\top$ and $\\lambda^\\star(0) = -1$.\n\nSummary of the solution $x^\\star(b)$ and multiplier $\\lambda^\\star(b)$:\n- For $b > \\frac{1}{2}$: $x^\\star(b) = \\left[ \\frac{1+6b}{8}, \\frac{2b-1}{8} \\right]^\\top$, $\\lambda^\\star(b) = -\\frac{5+6b}{4}$.\n- For $0 < b \\le \\frac{1}{2}$: $x^\\star(b) = [b, 0]^\\top$, $\\lambda^\\star(b) = -2b-1$.\n- For $b = 0$: $x^\\star(b) = [0,0]^\\top$, $\\lambda^\\star(b) = -1$.\n\n**Sensitivity Analysis and Finite-Difference Estimation**\nThe sensitivity theorem for constrained optimization states that the derivative of the optimal value function $f^\\star(b)$ with respect to a parameter $b$ in a constraint $h(x,b)=0$ is given by $\\frac{df^\\star}{db} = \\frac{\\partial L}{\\partial b}|_{x^\\star, \\lambda^\\star}$. For our constraint $a^\\top x - b = 0$, the Lagrangian term is $\\lambda(a^\\top x-b)$, so $\\frac{\\partial L}{\\partial b} = -\\lambda$. Thus, the sensitivity-based prediction is $\\frac{df^\\star}{db} = -\\lambda^\\star(b)$.\n\nWe compare this prediction with a numerical estimate from finite differences. A step size of $h=10^{-7}$ is chosen.\n- For $b > 0$, we use the central difference formula $D_c(b,h) = \\frac{f^\\star(b+h) - f^\\star(b-h)}{2h}$, since $b-h$ corresponds to a feasible problem.\n- For $b=0$, we must use the forward difference formula $D_f(b,h) = \\frac{f^\\star(b+h) - f^\\star(b)}{h}$, since $b-h < 0$ corresponds to an infeasible problem (as $x_1, x_2 \\ge 0$ implies $x_1+x_2 \\ge 0$, so $b$ must be non-negative).\n\nThe procedure for each test case is as follows:\n1.  Based on the value of $b$, determine which case applies and find $x^\\star(b)$ and $\\lambda^\\star(b)$.\n2.  Calculate the prediction: $P = -\\lambda^\\star(b)$.\n3.  Define a function $f_{star}(v)$ which solves the QP for a given value $v$ and returns the objective value $f(x^\\star(v))$.\n4.  Calculate the estimate $E$ using the appropriate finite difference formula applied to $f_{star}$.\n5.  Compute the absolute difference $|P - E|$.\n\n- **$b=1.0$**: Falls in Case 1 ($b > 0.5$).\n- **$b=0.25$**: Falls in Case 2 ($0 < b \\le 0.5$).\n- **$b=0.0$**: Is Case 3.\n- **$b=0.5$**: Is the transition point between Case 1 and Case 2. Both formulas yield the same $x^\\star(0.5)=[0.5, 0]^\\top$ and $\\lambda^\\star(0.5)=-2$. The function $f^\\star(b)$ is differentiable at $b=0.5$.\n\nWe will now implement this logic to compute the required absolute differences.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the parameterized QP and performs sensitivity analysis for given test cases.\n    \"\"\"\n    Q = np.array([[2., 0.], [0., 6.]])\n    c = np.array([1., 2.])\n    a = np.array([1., 1.])\n    h = 1e-7\n\n    test_cases = [1.0, 0.25, 0.0, 0.5]\n    results = []\n\n    def solve_qp_and_get_lambda(b_val):\n        \"\"\"\n        Solves the QP for a given parameter b, returning the optimal solution x_star\n        and the Lagrange multiplier lambda_star for the equality constraint.\n\n        Args:\n            b_val (float): The value of the parameter b.\n\n        Returns:\n            tuple: A tuple (x_star, lambda_star). Returns (None, None) if infeasible.\n        \"\"\"\n        if b_val < 0:\n            # Problem is infeasible as x1, x2 >= 0 implies x1+x2 >= 0.\n            return None, None\n        \n        if b_val > 0.5:\n            # Case 1: Interior solution (x1 > 0, x2 > 0)\n            x1 = (1.0 + 6.0 * b_val) / 8.0\n            x2 = (2.0 * b_val - 1.0) / 8.0\n            lambda_star = -(5.0 + 6.0 * b_val) / 4.0\n            x_star = np.array([x1, x2])\n        elif b_val > 0:\n            # Case 2: Boundary solution (x2 = 0)\n            x1 = b_val\n            x2 = 0.0\n            lambda_star = -2.0 * b_val - 1.0\n            x_star = np.array([x1, x2])\n        else: # b_val == 0\n            # Case 3: Boundary solution (x1 = 0, x2 = 0)\n            x_star = np.array([0.0, 0.0])\n            lambda_star = -1.0\n\n        return x_star, lambda_star\n\n    def get_f_star(b_val):\n        \"\"\"\n        Calculates the optimal value of the objective function for a given b.\n\n        Args:\n            b_val (float): The value of the parameter b.\n\n        Returns:\n            float: The optimal objective value f*(b).\n        \"\"\"\n        x_star, _ = solve_qp_and_get_lambda(b_val)\n        if x_star is None:\n            # Should not happen for the given test cases.\n            return float('inf') \n        \n        # f(x) = 0.5 * x.T @ Q @ x + c.T @ x\n        term1 = 0.5 * x_star.T @ Q @ x_star\n        term2 = c.T @ x_star\n        return term1 + term2\n\n    for b in test_cases:\n        # 1. Solve the QP and get the multiplier for the current b\n        x_star, lambda_star = solve_qp_and_get_lambda(b)\n        \n        # 2. Form sensitivity-based prediction\n        prediction = -lambda_star\n        \n        # 3. Estimate derivative using finite differences\n        if b == 0.0:\n            # Use forward difference for the boundary case b=0\n            # as b-h is infeasible\n            f_star_b = get_f_star(b)\n            f_star_b_plus_h = get_f_star(b + h)\n            estimate = (f_star_b_plus_h - f_star_b) / h\n        else:\n            # Use central difference for interior cases\n            f_star_b_plus_h = get_f_star(b + h)\n            f_star_b_minus_h = get_f_star(b - h)\n            estimate = (f_star_b_plus_h - f_star_b_minus_h) / (2.0 * h)\n            \n        # 4. Compute the absolute difference\n        abs_diff = abs(prediction - estimate)\n        results.append(abs_diff)\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.15g}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}