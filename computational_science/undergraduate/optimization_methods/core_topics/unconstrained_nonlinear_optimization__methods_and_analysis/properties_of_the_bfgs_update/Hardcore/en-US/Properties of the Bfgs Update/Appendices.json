{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method, we must start with its core mechanics. This first practice focuses on the direct application of the BFGS update formula for the Hessian approximation $B_k$. By working through a concrete numerical example, you will not only perform the rank-two update but also discover how the fundamental secant condition, $B_{k+1}s_k = y_k$, provides an elegant shortcut for related calculations, reinforcing the deep connection between the algorithm's mechanics and its theoretical foundation .",
            "id": "3166921",
            "problem": "Consider the unconstrained minimization of a twice continuously differentiable function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ given by the quadratic model $f(x)=\\frac{1}{2}x^{\\top}A x$, where $A\\in\\mathbb{R}^{2\\times 2}$ is symmetric positive definite with\n$$\nA=\\begin{pmatrix}4  1 \\\\ 1  3\\end{pmatrix}.\n$$\nLet $x_{k}\\in\\mathbb{R}^{2}$ be a current iterate and $x_{k+1}=x_{k}+s_{k}$ with a given step $s_{k}\\in\\mathbb{R}^{2}$ equal to\n$$\ns_{k}=\\begin{pmatrix}-2 \\\\ 1\\end{pmatrix}.\n$$\nDefine the gradient change $y_{k}=\\nabla f(x_{k+1})-\\nabla f(x_{k})$. For this quadratic $f$, recall that $\\nabla f(x)=A x$, so $y_{k}=A s_{k}$. Let $B_{k}\\in\\mathbb{R}^{2\\times 2}$ be a symmetric positive definite approximation to the Hessian at iteration $k$, and set $B_{k}=I$, where $I$ is the identity matrix. Using the defining principles of the Broyden–Fletcher–Goldfarb–Shanno (BFGS) update—symmetry, the secant condition $B_{k+1}s_{k}=y_{k}$, and minimal deviation from $B_{k}$ in the metric induced by $B_{k}^{-1}$ to preserve positive definiteness—construct $B_{k+1}$ directly from $B_{k}$, $s_{k}$, and $y_{k}$. Then, establish the connection to the inverse approximation $H_{k+1}=B_{k+1}^{-1}$, and use this connection to evaluate the scalar\n$$\ns_{k}^{\\top}H_{k+1}y_{k}.\n$$\nExpress your final answer as a single real number. No rounding is required.",
            "solution": "The problem asks us to consider a quadratic minimization problem and perform one step of the Broyden–Fletcher–Goldfarb–Shanno (BFGS) update for the Hessian approximation. We are then required to evaluate a specific scalar quantity.\n\nThe problem provides the following data and definitions:\nThe objective function is a quadratic form $f(x)=\\frac{1}{2}x^{\\top}A x$, where $x \\in \\mathbb{R}^{2}$.\nThe Hessian matrix $A$ is given as $A=\\begin{pmatrix}4  1 \\\\ 1  3\\end{pmatrix}$. The matrix is symmetric and positive definite as its determinant is $4 \\times 3 - 1 \\times 1 = 11 > 0$ and its trace is $4+3=7>0$.\nThe step from iterate $x_k$ to $x_{k+1}$ is $s_k = x_{k+1} - x_k = \\begin{pmatrix}-2 \\\\ 1\\end{pmatrix}$.\nThe gradient change is $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$. For the given quadratic function, the gradient is $\\nabla f(x) = A x$. Thus, $y_k = A x_{k+1} - A x_k = A(x_{k+1}-x_k) = A s_k$.\nThe initial Hessian approximation is the identity matrix, $B_k = I = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}$.\n\nFirst, we calculate the gradient change vector $y_k$:\n$$\ny_k = A s_k = \\begin{pmatrix}4  1 \\\\ 1  3\\end{pmatrix} \\begin{pmatrix}-2 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}4(-2) + 1(1) \\\\ 1(-2) + 3(1)\\end{pmatrix} = \\begin{pmatrix}-7 \\\\ 1\\end{pmatrix}.\n$$\nThe BFGS update requires that the condition $s_k^{\\top}y_k  0$ is met to ensure that the updated matrix $B_{k+1}$ remains positive definite (given $B_k$ is positive definite). We verify this condition:\n$$\ns_k^{\\top}y_k = \\begin{pmatrix}-2  1\\end{pmatrix} \\begin{pmatrix}-7 \\\\ 1\\end{pmatrix} = (-2)(-7) + (1)(1) = 14 + 1 = 15.\n$$\nSince $15  0$, the condition is satisfied.\n\nThe problem asks to construct the next Hessian approximation, $B_{k+1}$. The BFGS update formula is given by:\n$$\nB_{k+1} = B_k + \\frac{y_k y_k^{\\top}}{y_k^{\\top} s_k} - \\frac{B_k s_k s_k^{\\top} B_k}{s_k^{\\top} B_k s_k}.\n$$\nWe compute the necessary components for this formula. We have $B_k=I$.\nThe vector $B_k s_k$ is:\n$$\nB_k s_k = I s_k = s_k = \\begin{pmatrix}-2 \\\\ 1\\end{pmatrix}.\n$$\nThe scalar $s_k^{\\top} B_k s_k$ is:\n$$\ns_k^{\\top} B_k s_k = s_k^{\\top} I s_k = s_k^{\\top} s_k = (-2)^2 + 1^2 = 5.\n$$\nThe term $y_k^{\\top}s_k$ was already computed and is equal to $15$.\n\nNow we construct the two rank-one update matrices.\nThe first update term is:\n$$\n\\frac{y_k y_k^{\\top}}{y_k^{\\top} s_k} = \\frac{1}{15} \\begin{pmatrix}-7 \\\\ 1\\end{pmatrix} \\begin{pmatrix}-7  1\\end{pmatrix} = \\frac{1}{15} \\begin{pmatrix}49  -7 \\\\ -7  1\\end{pmatrix}.\n$$\nThe second update term is:\n$$\n\\frac{B_k s_k s_k^{\\top} B_k}{s_k^{\\top} B_k s_k} = \\frac{s_k s_k^{\\top}}{s_k^{\\top} s_k} = \\frac{1}{5} \\begin{pmatrix}-2 \\\\ 1\\end{pmatrix} \\begin{pmatrix}-2  1\\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix}4  -2 \\\\ -2  1\\end{pmatrix}.\n$$\nSubstituting these into the BFGS formula for $B_{k+1}$:\n$$\nB_{k+1} = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix} + \\frac{1}{15} \\begin{pmatrix}49  -7 \\\\ -7  1\\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix}4  -2 \\\\ -2  1\\end{pmatrix}\n$$\nTo combine these matrices, we use a common denominator of $15$:\n$$\nB_{k+1} = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix} + \\begin{pmatrix}49/15  -7/15 \\\\ -7/15  1/15\\end{pmatrix} - \\begin{pmatrix}12/15  -6/15 \\\\ -6/15  3/15\\end{pmatrix}\n$$\n$$\nB_{k+1} = \\begin{pmatrix}1 + 49/15 - 12/15  0 - 7/15 + 6/15 \\\\ 0 - 7/15 + 6/15  1 + 1/15 - 3/15\\end{pmatrix} = \\begin{pmatrix}(15+49-12)/15  -1/15 \\\\ -1/15  (15+1-3)/15\\end{pmatrix}\n$$\n$$\nB_{k+1} = \\begin{pmatrix}52/15  -1/15 \\\\ -1/15  13/15\\end{pmatrix} = \\frac{1}{15}\\begin{pmatrix}52  -1 \\\\ -1  13\\end{pmatrix}.\n$$\nHaving constructed $B_{k+1}$, we are next asked to establish the connection to its inverse, $H_{k+1} = B_{k+1}^{-1}$, and use this to evaluate $s_k^{\\top}H_{k+1}y_k$.\n\nA fundamental defining property of the BFGS update is that the new Hessian approximation $B_{k+1}$ must satisfy the secant condition:\n$$\nB_{k+1}s_k = y_k.\n$$\nThis condition ensures that the new quadratic model's gradient matches the observed gradient change along the direction of the step $s_k$.\n\nLet $H_{k+1}$ be the inverse of $B_{k+1}$, i.e., $H_{k+1} = B_{k+1}^{-1}$. Since $B_{k+1}$ is symmetric and positive definite, its inverse exists and is also symmetric and positive definite. We can pre-multiply the secant condition by $H_{k+1}$:\n$$\nH_{k+1}(B_{k+1}s_k) = H_{k+1}y_k.\n$$\nUsing the associativity of matrix multiplication and the definition of the inverse matrix, $H_{k+1}B_{k+1} = I$:\n$$\n(H_{k+1}B_{k+1})s_k = H_{k+1}y_k\n$$\n$$\nI s_k = H_{k+1}y_k\n$$\n$$\ns_k = H_{k+1}y_k.\n$$\nThis relationship, known as the inverse secant condition, is the \"connection\" asked for in the problem. It is a direct consequence of the way the BFGS update is constructed.\n\nWe can now use this result to evaluate the required scalar quantity $s_k^{\\top}H_{k+1}y_k$. We substitute the expression $s_k$ for $H_{k+1}y_k$:\n$$\ns_k^{\\top}H_{k+1}y_k = s_k^{\\top}(H_{k+1}y_k) = s_k^{\\top}s_k.\n$$\nThe problem is reduced to calculating the squared Euclidean norm of the vector $s_k$.\n$$\ns_k^{\\top}s_k = \\begin{pmatrix}-2  1\\end{pmatrix} \\begin{pmatrix}-2 \\\\ 1\\end{pmatrix} = (-2)^2 + (1)^2 = 4 + 1 = 5.\n$$\nTherefore, the value of the scalar expression is $5$. The explicit calculation of $B_{k+1}$ was an intermediate step to demonstrate understanding of the full BFGS procedure, but the final answer is obtained more elegantly by exploiting the fundamental properties of the update.",
            "answer": "$$\n\\boxed{5}\n$$"
        },
        {
            "introduction": "The guaranteed success of the BFGS update, particularly the preservation of positive definiteness in the Hessian approximation, relies on the curvature condition $y_k^\\top s_k \\gt 0$. While this holds in convex settings, real-world problems often involve non-convex regions where this condition can be violated. This exercise guides you through such a scenario, demonstrating how an undamped BFGS step can fail near a saddle point and then introducing a practical damping strategy to restore the necessary curvature, a vital skill for developing robust optimization solvers .",
            "id": "3167001",
            "problem": "Consider the quadratic objective function defined by $f(x) = \\frac{1}{2} x^{\\top} A x$ on $\\mathbb{R}^{2}$, where $A = \\begin{pmatrix}1  0 \\\\ 0  -1\\end{pmatrix}$. Let the current iterate be $x_{k} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ and the next iterate be $x_{k+1} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, so that $s_{k} = x_{k+1} - x_{k}$. Let the current inverse Hessian approximation be $H_{k} = I$, and define $B_{k} = H_{k}^{-1}$. \n\nTasks:\n1. Verify that the point $x = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ is a saddle point of $f$, and compute $s_{k}$ and $y_{k} = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$. Show that $y_{k}^{\\top} s_{k} \\le 0$.\n2. Using the undamped inverse update from the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method, compute $H_{k+1}$ from $H_{k}$, $s_{k}$, and $y_{k}$, and demonstrate that $H_{k+1}$ is not positive definite.\n3. To restore positive definiteness, consider a damping strategy that replaces $y_{k}$ by a damped vector $\\bar{y}_{k} = \\theta y_{k} + (1 - \\theta) B_{k} s_{k}$ with a parameter $\\theta \\in (0,1]$. Impose the curvature condition $s_{k}^{\\top} \\bar{y}_{k} = \\delta \\, s_{k}^{\\top} B_{k} s_{k}$ with $\\delta = \\frac{1}{5}$. Determine the largest $\\theta$ in $(0,1]$ satisfying this equality, and then compute the corresponding damped update $H_{k+1}$ to verify positive definiteness.\n\nAnswer specification: Report the value of $\\theta$ as an exact rational number in simplest terms. No rounding is required.",
            "solution": "The problem is well-posed and grounded in the principles of numerical optimization, specifically concerning the properties of the Broyden–Fletcher–Goldfarb–Shanno (BFGS) update for quasi-Newton methods. We will proceed with a full solution.\n\nThe objective function is $f(x) = \\frac{1}{2} x^{\\top} A x$ for $x \\in \\mathbb{R}^{2}$, with $A = \\begin{pmatrix}1  0 \\\\ 0  -1\\end{pmatrix}$. This can be written as $f(x_1, x_2) = \\frac{1}{2}(x_1^2 - x_2^2)$.\n\n### Task 1: Saddle Point and Curvature Condition\n\nFirst, we compute the gradient $\\nabla f(x)$ and the Hessian $\\nabla^2 f(x)$ of the objective function.\nThe gradient is given by $\\nabla f(x) = A x = \\begin{pmatrix}1  0 \\\\ 0  -1\\end{pmatrix} \\begin{pmatrix}x_1 \\\\ x_2\\end{pmatrix} = \\begin{pmatrix}x_1 \\\\ -x_2\\end{pmatrix}$.\nThe Hessian is the constant matrix $\\nabla^2 f(x) = A = \\begin{pmatrix}1  0 \\\\ 0  -1\\end{pmatrix}$.\n\nTo verify that $x = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ is a saddle point, we check the first- and second-order conditions.\n1.  The gradient at $x = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ is $\\nabla f(\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}) = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$, confirming that it is a critical point.\n2.  The Hessian matrix $A$ has eigenvalues $\\lambda_1 = 1$ and $\\lambda_2 = -1$. Since the eigenvalues have opposite signs, the Hessian is indefinite. A critical point with an indefinite Hessian is a saddle point.\n\nNext, we compute the vectors $s_k$ and $y_k$.\nThe iterates are given as $x_{k} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ and $x_{k+1} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$.\nThe step vector $s_k$ is defined as $s_{k} = x_{k+1} - x_{k}$:\n$$s_k = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$$\nThe vector $y_k$ is defined as the change in gradients, $y_{k} = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$.\n$\\nabla f(x_{k}) = \\nabla f(\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}) = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$.\n$\\nabla f(x_{k+1}) = \\nabla f(\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}) = \\begin{pmatrix}0 \\\\ -1\\endpmatrix}$.\nTherefore,\n$$y_k = \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} - \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -1\\end{pmatrix}$$\nNow we check the sign of the curvature $y_k^\\top s_k$:\n$$y_k^\\top s_k = \\begin{pmatrix}0  -1\\end{pmatrix} \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = (0)(0) + (-1)(1) = -1$$\nSince $y_k^\\top s_k = -1 \\le 0$, the standard curvature condition for the BFGS method, which requires $y_k^\\top s_k  0$ to guarantee positive definiteness of the updated Hessian approximation, is not satisfied.\n\n### Task 2: Undamped BFGS Update\n\nThe inverse BFGS update formula is $H_{k+1} = \\left(I - \\rho_k s_k y_k^\\top\\right) H_k \\left(I - \\rho_k y_k s_k^\\top\\right) + \\rho_k s_k s_k^\\top$, where $\\rho_k = 1/(y_k^\\top s_k)$.\nWe are given the initial inverse Hessian approximation $H_k = I = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}$.\nWe have $y_k^\\top s_k = -1$, so $\\rho_k = -1$.\nWe compute the necessary components for the update:\n$$\nI - \\rho_k s_k y_k^\\top = I - (-1) s_k y_k^\\top = I + \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}\\begin{pmatrix}0  -1\\end{pmatrix} = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix} + \\begin{pmatrix}0  0 \\\\ 0  -1\\end{pmatrix} = \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix}\n$$\n$$\nI - \\rho_k y_k s_k^\\top = I - (-1) y_k s_k^\\top = I + \\begin{pmatrix}0 \\\\ -1\\end{pmatrix}\\begin{pmatrix}0  1\\end{pmatrix} = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix} + \\begin{pmatrix}0  0 \\\\ 0  -1\\end{pmatrix} = \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix}\n$$\nThe first term of the update is:\n$$\n\\left(I - \\rho_k s_k y_k^\\top\\right) H_k \\left(I - \\rho_k y_k s_k^\\top\\right) = \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix} I \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix} = \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix}\n$$\nThe second term is:\n$$\n\\rho_k s_k s_k^\\top = (-1) \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}\\begin{pmatrix}0  1\\end{pmatrix} = \\begin{pmatrix}0  0 \\\\ 0  -1\\end{pmatrix}\n$$\nCombining them, we get:\n$$\nH_{k+1} = \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix} + \\begin{pmatrix}0  0 \\\\ 0  -1\\end{pmatrix} = \\begin{pmatrix}1  0 \\\\ 0  -1\\end{pmatrix}\n$$\nTo check if $H_{k+1}$ is positive definite, we use Sylvester's criterion. The leading principal minors must all be positive.\nThe first minor is $H_{11} = 1 > 0$.\nThe second minor is $\\det(H_{k+1}) = (1)(-1) - (0)(0) = -1$.\nSince the determinant is negative, $H_{k+1}$ is not positive definite.\n\n### Task 3: Damped BFGS Update\n\nTo restore positive definiteness, we use a damping strategy. The vector $y_k$ is replaced by $\\bar{y}_k = \\theta y_k + (1-\\theta) B_k s_k$, where $\\theta \\in (0, 1]$. We are given $H_k = I$, so its inverse, the direct Hessian approximation, is $B_k = H_k^{-1} = I$.\nThe damped vector becomes $\\bar{y}_k = \\theta y_k + (1-\\theta) s_k$.\n\nThe parameter $\\theta$ is determined by imposing the modified curvature condition $s_k^\\top \\bar{y}_k = \\delta s_k^\\top B_k s_k$ with $\\delta = \\frac{1}{5}$.\nSubstituting the expression for $\\bar{y}_k$ and $B_k=I$:\n$$s_k^\\top (\\theta y_k + (1-\\theta) s_k) = \\frac{1}{5} s_k^\\top s_k$$\nExpanding the left side gives:\n$$\\theta (s_k^\\top y_k) + (1-\\theta) (s_k^\\top s_k) = \\frac{1}{5} (s_k^\\top s_k)$$\nWe have the previously computed values $s_k^\\top y_k = -1$ and $s_k^\\top s_k = \\begin{pmatrix}0  1\\end{pmatrix}\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = 1$.\nSubstituting these values into the equation for $\\theta$:\n$$\\theta(-1) + (1-\\theta)(1) = \\frac{1}{5}(1)$$\n$$-\\theta + 1 - \\theta = \\frac{1}{5}$$\n$$1 - 2\\theta = \\frac{1}{5}$$\n$$2\\theta = 1 - \\frac{1}{5} = \\frac{4}{5}$$\n$$\\theta = \\frac{2}{5}$$\nThis value lies in the specified interval $(0, 1]$, so it is the required value.\n\nWe now compute the damped update for $H_{k+1}$ using this $\\theta$.\nFirst, we find the damped vector $\\bar{y}_k$:\n$$\\bar{y}_k = \\frac{2}{5} y_k + \\left(1-\\frac{2}{5}\\right) s_k = \\frac{2}{5} \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\frac{3}{5} \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -2/5\\end{pmatrix} + \\begin{pmatrix}0 \\\\ 3/5\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 1/5\\end{pmatrix}$$\nThe new curvature term is $s_k^\\top \\bar{y}_k = \\begin{pmatrix}0  1\\end{pmatrix} \\begin{pmatrix}0 \\\\ 1/5\\end{pmatrix} = \\frac{1}{5}$. This is positive as required.\n\nWe apply the BFGS update formula with $\\bar{y}_k$. The new $\\rho_k$ is $\\bar{\\rho}_k = 1/(s_k^\\top \\bar{y}_k) = 1/(1/5) = 5$.\n$$H_{k+1} = \\left(I - \\bar{\\rho}_k s_k \\bar{y}_k^\\top\\right) H_k \\left(I - \\bar{\\rho}_k \\bar{y}_k s_k^\\top\\right) + \\bar{\\rho}_k s_k s_k^\\top$$\nLet's compute the components:\n$I - \\bar{\\rho}_k s_k \\bar{y}_k^\\top = I - 5 \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}\\begin{pmatrix}0  1/5\\end{pmatrix} = I - 5\\begin{pmatrix}0  0 \\\\ 0  1/5\\end{pmatrix} = I - \\begin{pmatrix}0  0 \\\\ 0  1\\end{pmatrix} = \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix}$.\nThe first term is $\\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix} I \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix}^\\top = \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix}$.\nThe second term is $\\bar{\\rho}_k s_k s_k^\\top = 5 \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}\\begin{pmatrix}0  1\\end{pmatrix} = \\begin{pmatrix}0  0 \\\\ 0  5\\end{pmatrix}$.\nCombining them:\n$$H_{k+1} = \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix} + \\begin{pmatrix}0  0 \\\\ 0  5\\end{pmatrix} = \\begin{pmatrix}1  0 \\\\ 0  5\\end{pmatrix}$$\nTo verify that this matrix is positive definite, we check its eigenvalues, which are its diagonal entries $1$ and $5$. Since both are positive, the matrix is positive definite. Alternatively, the leading principal minors are $1>0$ and $\\det(H_{k+1}) = 5 > 0$, confirming positive definiteness.\n\nThe value of $\\theta$ determined is $\\frac{2}{5}$.",
            "answer": "$$\\boxed{\\frac{2}{5}}$$"
        },
        {
            "introduction": "Beyond its step-by-step efficiency, the BFGS method possesses a remarkable \"self-correcting\" property, especially on quadratic functions. Even if the initial Hessian approximation $H_0$ is a poor estimate of the true inverse Hessian, subsequent BFGS updates progressively refine it. In this coding exercise, you will implement the BFGS algorithm with an exact line search and witness this property firsthand by numerically tracking the convergence of your Hessian approximation $H_k$ to the true inverse Hessian $A^{-1}$, bringing a powerful theoretical result to life .",
            "id": "3166990",
            "problem": "Consider a smooth unconstrained optimization problem with a convex quadratic objective function defined by $f(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$, where $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $b \\in \\mathbb{R}^{n}$, and $x \\in \\mathbb{R}^{n}$. The gradient is $\\nabla f(x) = A x - b$. Quasi-Newton methods construct iterates $x_{k+1} = x_k + \\alpha_k p_k$ with $p_k = - H_k \\nabla f(x_k)$, where $H_k$ is an approximation to the inverse Hessian. The Broyden–Fletcher–Goldfarb–Shanno (BFGS) method is the quasi-Newton scheme that enforces the secant condition $H_{k+1} y_k = s_k$ with $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$, while updating $H_k$ to $H_{k+1}$ in a symmetric, positive definite-preserving manner via a rank-two change that minimizes the alteration subject to the secant condition.\n\nYour task is to:\n1. Starting from the stated fundamental bases, derive the BFGS inverse-Hessian update that enforces the secant condition $H_{k+1} y_k = s_k$, preserves symmetry and positive definiteness when $y_k^{\\top} s_k  0$, and arises from a least-change principle. Explain why exact line search on a convex quadratic ensures $y_k^{\\top} s_k  0$.\n2. Using your derived update, implement a program that, for each test case below, performs BFGS iterations with exact line search starting from a specified initial vector $x_0$ and initial inverse-Hessian approximation $H_0$. For each test case, run exactly $K$ iterations unless an iteration would be undefined due to $\\| \\nabla f(x_k) \\|$ being sufficiently small or a denominator in the formulas becoming numerically negligible; in such cases, stop early. After finishing, compute the spectral norm decay metric $\\| H_K A - I \\|_2$, where $I$ is the $n \\times n$ identity matrix and $\\| \\cdot \\|_2$ is the matrix operator two-norm (largest singular value). This quantity numerically reflects the self-correcting nature: even with a poor $H_0$, repeated BFGS updates on a convex quadratic drive $H_k$ toward $A^{-1}$.\n3. Produce the final program output as a single line containing a comma-separated list enclosed in square brackets of the final values of $\\| H_K A - I \\|_2$ for the provided test suite, in the order given below. These outputs are dimensionless real numbers.\n\nTest suite:\n- Case $1$ (happy path with poorly scaled $H_0$):\n  - $n = 3$\n  - $$A_1 = \\begin{bmatrix} 4  1  0 \\\\ 1  3  0 \\\\ 0  0  2 \\end{bmatrix}, \\quad b_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}.$$\n  - $$x_{0,1} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad H_{0,1} = 0.1 \\, I_3.$$\n  - Iterations $K_1 = 3$.\n- Case $2$ (boundary case with $H_0 = A^{-1}$):\n  - $n = 3$\n  - $$A_2 = \\begin{bmatrix} 2  0  0 \\\\ 0  5  0 \\\\ 0  0  1 \\end{bmatrix}, \\quad b_2 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}.$$\n  - $$x_{0,2} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad H_{0,2} = A_2^{-1}.$$\n  - Iterations $K_2 = 3$.\n- Case $3$ (ill-conditioned quadratic):\n  - $n = 3$\n  - $$A_3 = \\begin{bmatrix} 10^{-3}  0  0 \\\\ 0  1  0 \\\\ 0  0  10^{3} \\end{bmatrix}, \\quad b_3 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}.$$\n  - $$x_{0,3} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad H_{0,3} = I_3.$$\n  - Iterations $K_3 = 3$.\n- Case $4$ (poorly oriented but symmetric positive definite $H_0$):\n  - $n = 3$\n  - $$A_4 = \\begin{bmatrix} 6  2  1 \\\\ 2  5  0 \\\\ 1  0  3 \\end{bmatrix}, \\quad b_4 = \\begin{bmatrix} 0.5 \\\\ -1 \\\\ 2 \\end{bmatrix}.$$\n  - $$x_{0,4} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad H_{0,4} = \\begin{bmatrix} 3  1  0 \\\\ 1  1  0 \\\\ 0  0  0.5 \\end{bmatrix}.$$\n  - Iterations $K_4 = 3$.\n\nImplementation details:\n- Use exact line search along $p_k$ for a convex quadratic $f$ to determine $\\alpha_k$, which is the unique minimizer of $f(x_k + \\alpha p_k)$ along the direction $p_k$.\n- Stop early if $\\| \\nabla f(x_k) \\|_2 \\leq 10^{-12}$, if $p_k^{\\top} A p_k \\leq 10^{-14}$, or if $y_k^{\\top} s_k \\leq 10^{-14}$ would make the update ill-defined.\n- After the iterations for each case, compute the single float $\\| H_K A - I \\|_2$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[r_1, r_2, r_3, r_4]$, where $r_i$ corresponds to Case $i$.\n\nNo physical units or angle units are involved in this problem. All outputs must be real numbers without units.",
            "solution": "The problem is valid. It is scientifically grounded in the theory of convex optimization, specifically quasi-Newton methods. It is well-posed, providing all necessary definitions, initial conditions, and parameters for the BFGS algorithm to be executed for a specified number of iterations. The matrices defining the convex quadratic objectives are symmetric positive definite, and the initial inverse Hessian approximations are also symmetric positive definite, ensuring the problem setup is consistent and feasible.\n\n### 1. Derivation and Theoretical Background\n\nThe Broyden–Fletcher–Goldfarb–Shanno (BFGS) method is a quasi-Newton algorithm for unconstrained optimization. It iteratively builds an approximation $H_k$ to the inverse of the Hessian matrix. The update from $H_k$ to $H_{k+1}$ is designed to satisfy several key properties.\n\n#### 1.1 The BFGS Inverse Hessian Update\n\nThe BFGS update is derived to satisfy the secant condition, preserve symmetry, and maintain positive definiteness. For a move from $x_k$ to $x_{k+1}$, we define the displacement $s_k$ and the change in gradient $y_k$:\n$$ s_k = x_{k+1} - x_k $$\n$$ y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k) $$\nThe secant condition requires the new inverse Hessian approximation $H_{k+1}$ to map $y_k$ to $s_k$:\n$$ H_{k+1} y_k = s_k $$\nFor a quadratic function $f(x) = \\frac{1}{2}x^{\\top} A x - b^{\\top} x$, the gradient is $\\nabla f(x) = Ax-b$. Thus, $y_k = (Ax_{k+1}-b) - (Ax_k-b) = A(x_{k+1}-x_k) = As_k$. The secant condition then becomes $H_{k+1} A s_k = s_k$, which means that $H_{k+1}$ acts as the inverse of $A$ in the direction of $s_k$.\n\nThe BFGS update for the inverse Hessian $H_k$ is the unique symmetric update that satisfies the secant condition and minimizes a weighted Frobenius norm of the change, $\\|H_k^{-1/2}(H_{k+1}-H_k)H_k^{-1/2}\\|_F$. The resulting update formula is a rank-two correction to $H_k$:\n$$ H_{k+1} = \\left(I - \\frac{s_k y_k^{\\top}}{y_k^{\\top} s_k}\\right) H_k \\left(I - \\frac{y_k s_k^{\\top}}{y_k^{\\top} s_k}\\right) + \\frac{s_k s_k^{\\top}}{y_k^{\\top} s_k} $$\nwhere $I$ is the identity matrix.\n\nLet's verify its properties:\n1.  **Secant Condition**: We must show that $H_{k+1} y_k = s_k$. Let $\\rho_k = 1 / (y_k^{\\top} s_k)$.\n    $$ H_{k+1} y_k = \\left(I - \\rho_k s_k y_k^{\\top}\\right) H_k \\left(I - \\rho_k y_k s_k^{\\top}\\right) y_k + \\rho_k s_k s_k^{\\top} y_k $$\n    The term $\\left(I - \\rho_k y_k s_k^{\\top}\\right) y_k$ simplifies to $y_k - \\rho_k y_k (s_k^{\\top} y_k) = y_k - \\frac{1}{y_k^{\\top} s_k} y_k (y_k^{\\top} s_k) = y_k - y_k = 0$.\n    Therefore, the first part of the expression vanishes:\n    $$ H_{k+1} y_k = 0 + \\rho_k s_k (s_k^{\\top} y_k) = \\frac{1}{y_k^{\\top} s_k} s_k (y_k^{\\top} s_k) = s_k $$\n    The secant condition is satisfied.\n\n2.  **Symmetry**: If $H_k$ is symmetric ($H_k = H_k^{\\top}$), then $H_{k+1}$ must also be symmetric. The term $\\frac{s_k s_k^{\\top}}{y_k^{\\top} s_k}$ is an outer product scaled by a scalar, so it is symmetric. The first term is of the form $M H_k M^{\\top}$ with $M = (I - \\rho_k s_k y_k^{\\top})$, which does not immediately appear symmetric. However, the form is $M H_k (M')^T$ where $M' = (I - \\rho_k s_k y_k^T)$ and not $(I - \\rho_k y_k s_k^T)$. Let's re-examine the formula: $(I - \\rho_k y_k s_k^{\\top})^{\\top} = I - \\rho_k (s_k y_k^{\\top})^{\\top} = I - \\rho_k y_k s_k^{\\top}$. So let's check the transpose of the first term:\n    $$ \\left[ \\left(I - \\rho_k s_k y_k^{\\top}\\right) H_k \\left(I - \\rho_k y_k s_k^{\\top}\\right) \\right]^{\\top} = \\left(I - \\rho_k y_k s_k^{\\top}\\right)^{\\top} H_k^{\\top} \\left(I - \\rho_k s_k y_k^{\\top}\\right)^{\\top} $$\n    $$ = \\left(I - \\rho_k s_k y_k^{\\top}\\right) H_k \\left(I - \\rho_k y_k s_k^{\\top}\\right) $$\n    The first term is symmetric, and thus $H_{k+1}$ is symmetric.\n\n3.  **Positive Definiteness**: If $H_k$ is positive definite and the curvature condition $y_k^{\\top} s_k  0$ holds, then $H_{k+1}$ is also positive definite. For any non-zero vector $z \\in \\mathbb{R}^n$, we have:\n    $$ z^{\\top} H_{k+1} z = z^{\\top}\\left(I - \\rho_k s_k y_k^{\\top}\\right) H_k \\left(I - \\rho_k y_k s_k^{\\top}\\right)z + \\rho_k z^{\\top}s_k s_k^{\\top}z $$\n    Let $v = (I - \\rho_k y_k s_k^{\\top})z$. The first term becomes $v^{\\top} H_k v$. Since $H_k$ is positive definite, $v^{\\top} H_k v \\ge 0$. The second term is $\\rho_k (z^{\\top}s_k)^2 = \\frac{(z^{\\top}s_k)^2}{y_k^{\\top}s_k}$. Since $y_k^{\\top}s_k  0$, this term is also non-negative. The sum is zero only if both terms are zero, which implies $z$ is a multiple of $y_k$ and also orthogonal to $s_k$, which can only happen if $z=0$. Thus $H_{k+1}$ is positive definite.\n\n#### 1.2 The Curvature Condition and Exact Line Search\n\nThe preservation of positive definiteness hinges on the curvature condition $y_k^{\\top} s_k  0$. For a convex quadratic objective $f(x)$ with a symmetric positive definite (SPD) Hessian $A$, this condition is guaranteed by an exact line search.\nThe step is $s_k = \\alpha_k p_k$, where $p_k = -H_k \\nabla f(x_k)$. Since $H_k$ is SPD, $p_k$ is a descent direction if $\\nabla f(x_k) \\ne 0$:\n$$ \\nabla f(x_k)^{\\top} p_k = - \\nabla f(x_k)^{\\top} H_k \\nabla f(x_k)  0 $$\nAs shown before, for a quadratic, $y_k = A s_k$. Therefore:\n$$ y_k^{\\top} s_k = (A s_k)^{\\top} s_k = s_k^{\\top} A s_k $$\nSince $A$ is SPD, $s_k^{\\top} A s_k  0$ for any non-zero $s_k$. The vector $s_k = \\alpha_k p_k$ is non-zero because $p_k \\ne 0$ (if not at optimum) and the step length $\\alpha_k$ from an exact line search will be positive for a descent direction.\n\n#### 1.3 Exact Line Search for a Quadratic Function\n\nThe exact line search step size $\\alpha_k$ minimizes $\\phi(\\alpha) = f(x_k + \\alpha p_k)$.\n$$ \\phi(\\alpha) = \\frac{1}{2}(x_k + \\alpha p_k)^{\\top}A(x_k + \\alpha p_k) - b^{\\top}(x_k + \\alpha p_k) $$\nTo find the minimum, we set the derivative with respect to $\\alpha$ to zero:\n$$ \\frac{d\\phi}{d\\alpha} = \\nabla f(x_k + \\alpha p_k)^{\\top} p_k = 0 $$\nThe gradient at the new point is $\\nabla f(x_k + \\alpha p_k) = A(x_k + \\alpha p_k) - b = (Ax_k - b) + \\alpha A p_k = \\nabla f(x_k) + \\alpha A p_k$.\nSubstituting this into the derivative equation:\n$$ (\\nabla f(x_k) + \\alpha A p_k)^{\\top} p_k = 0 $$\n$$ \\nabla f(x_k)^{\\top} p_k + \\alpha p_k^{\\top} A p_k = 0 $$\nSolving for $\\alpha$ gives the optimal step size $\\alpha_k$:\n$$ \\alpha_k = - \\frac{\\nabla f(x_k)^{\\top} p_k}{p_k^{\\top} A p_k} $$\nSince $p_k$ is a descent direction, the numerator is positive. Since $A$ is SPD, the denominator is positive. Thus, $\\alpha_k  0$.\n\n### 2. Algorithm Summary\n\nFor each test case, the algorithm proceeds as follows:\n1.  Initialize $x_0$ and $H_0$.\n2.  For $k = 0, 1, \\dots, K-1$:\n    a.  Compute the gradient $g_k = \\nabla f(x_k) = A x_k - b$.\n    b.  Check for convergence: if $\\| g_k \\|_2$ is below a tolerance ($10^{-12}$), terminate the loop.\n    c.  Compute the search direction $p_k = -H_k g_k$.\n    d.  Calculate the exact line search step size $\\alpha_k = - (g_k^{\\top} p_k) / (p_k^{\\top} A p_k)$. Check for a near-zero denominator ($10^{-14}$) and terminate if found.\n    e.  Update the position: $x_{k+1} = x_k + \\alpha_k p_k$.\n    f.  Calculate $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$.\n    g.  Check the curvature condition: if $y_k^{\\top} s_k$ is below a tolerance ($10^{-14}$), terminate the loop.\n    h.  Perform the BFGS update to find $H_{k+1}$ using the formula derived above.\n    i.  Set $x_k \\leftarrow x_{k+1}$ and $H_k \\leftarrow H_{k+1}$.\n3.  After the loop finishes, compute the final metric $\\|H_K A - I\\|_2$, where $H_K$ is the final inverse Hessian approximation.\n\nThis procedure is implemented for each test case provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_bfgs(A, b, x0, H0, K_max):\n    \"\"\"\n    Performs BFGS iterations for a convex quadratic with exact line search.\n\n    Args:\n        A (np.ndarray): The symmetric positive definite matrix of the quadratic objective.\n        b (np.ndarray): The linear term vector of the quadratic objective.\n        x0 (np.ndarray): The initial point.\n        H0 (np.ndarray): The initial inverse Hessian approximation.\n        K_max (int): The maximum number of iterations.\n\n    Returns:\n        float: The spectral norm of (H_final * A - I).\n    \"\"\"\n    n = A.shape[0]\n    x_k = x0.copy()\n    H_k = H0.copy()\n    identity = np.identity(n)\n\n    for k in range(K_max):\n        # Step a: Compute gradient\n        grad_k = A @ x_k - b\n\n        # Step b: Check for convergence\n        grad_norm = np.linalg.norm(grad_k)\n        if grad_norm = 1e-12:\n            break\n\n        # Step c: Compute search direction\n        p_k = -H_k @ grad_k\n        \n        # Step d: Compute exact line search step size\n        denom_alpha = p_k.T @ A @ p_k\n        if denom_alpha = 1e-14:\n            break\n            \n        alpha_k = -(grad_k.T @ p_k) / denom_alpha\n\n        # Step e: Update position\n        x_k_plus_1 = x_k + alpha_k * p_k\n\n        # Step f: Define s_k and y_k\n        s_k = x_k_plus_1 - x_k\n        grad_k_plus_1 = A @ x_k_plus_1 - b\n        y_k = grad_k_plus_1 - grad_k\n        \n        # Step g: Check curvature condition\n        y_k_T_s_k = y_k.T @ s_k\n        if y_k_T_s_k = 1e-14:\n            break\n            \n        # Step h: Perform the BFGS update\n        rho_k = 1.0 / y_k_T_s_k\n        \n        term1 = identity - rho_k * np.outer(s_k, y_k)\n        term2 = identity - rho_k * np.outer(y_k, s_k)\n        \n        H_k_plus_1 = term1 @ H_k @ term2 + rho_k * np.outer(s_k, s_k)\n\n        # Step i: Update for next iteration\n        x_k = x_k_plus_1\n        H_k = H_k_plus_1\n\n    # After loop, compute the final metric\n    metric = np.linalg.norm(H_k @ A - identity, ord=2)\n    return metric\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem for all test cases and prints the results.\n    \"\"\"\n    # Case 1\n    A1 = np.array([[4, 1, 0], [1, 3, 0], [0, 0, 2]], dtype=float)\n    b1 = np.array([1, 2, 3], dtype=float)\n    x0_1 = np.array([0, 0, 0], dtype=float)\n    H0_1 = 0.1 * np.identity(3)\n    K1 = 3\n\n    # Case 2\n    A2 = np.array([[2, 0, 0], [0, 5, 0], [0, 0, 1]], dtype=float)\n    b2 = np.array([1, -1, 2], dtype=float)\n    x0_2 = np.array([0, 0, 0], dtype=float)\n    H0_2 = np.linalg.inv(A2)\n    K2 = 3\n\n    # Case 3\n    A3 = np.array([[1e-3, 0, 0], [0, 1, 0], [0, 0, 1e3]], dtype=float)\n    b3 = np.array([1, 1, 1], dtype=float)\n    x0_3 = np.array([0, 0, 0], dtype=float)\n    H0_3 = np.identity(3)\n    K3 = 3\n\n    # Case 4\n    A4 = np.array([[6, 2, 1], [2, 5, 0], [1, 0, 3]], dtype=float)\n    b4 = np.array([0.5, -1, 2], dtype=float)\n    x0_4 = np.array([0, 0, 0], dtype=float)\n    H0_4 = np.array([[3, 1, 0], [1, 1, 0], [0, 0, 0.5]], dtype=float)\n    K4 = 3\n    \n    test_cases = [\n        (A1, b1, x0_1, H0_1, K1),\n        (A2, b2, x0_2, H0_2, K2),\n        (A3, b3, x0_3, H0_3, K3),\n        (A4, b4, x0_4, H0_4, K4)\n    ]\n\n    results = []\n    for A, b, x0, H0, K_max in test_cases:\n        result = run_bfgs(A, b, x0, H0, K_max)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}