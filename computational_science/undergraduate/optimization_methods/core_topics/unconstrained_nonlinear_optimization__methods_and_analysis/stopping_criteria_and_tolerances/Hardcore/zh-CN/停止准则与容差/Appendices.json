{
    "hands_on_practices": [
        {
            "introduction": "在实践中，单一的停止准则往往并不可靠，可能导致过早终止或不必要的计算。本练习将指导您构建一个更稳健的“字典序”停止规则，它按优先级顺序组合了三个关键的收敛指标：梯度范数、步长大小和函数值下降量。通过这个练习，您将学会如何设计一个能够应对不同收敛阶段特征的、结构化的停止策略，这是编写高效优化代码的一项基本技能。",
            "id": "3187944",
            "problem": "要求您为一种结合了梯度范数、步长范数和每次迭代函数下降量的迭代优化方法设计并实现一个字典序停止准则，并在非凸基准函数上对其进行评估。实现必须是一个完整的、可运行的程序。\n\n使用的基本原理是无约束可微优化的一阶必要最优性条件：对于一个可微函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 的局部极小值点 $x^\\star$，其梯度满足 $\\nabla f(x^\\star)=\\mathbf{0}$。在数值计算上，该原理启发我们在梯度范数很小时停止。然而，数值停滞也可以通过小步长或可忽略的函数下降来检测。您的任务是推导一个优先考虑这些标准的字典序停止准则，并在带回溯线搜索的梯度下降法中实现它。\n\n将迭代方法定义如下。给定初始点 $x_0\\in\\mathbb{R}^n$，在第 $k$ 次迭代，当前点为 $x_k$，计算梯度 $g_k=\\nabla f(x_k)$ 及其范数 $G_k=\\lVert g_k\\rVert_2$。从一个初始步长 $t_00$，使用收缩因子 $\\beta\\in(0,1)$ 和 Armijo 参数 $c\\in(0,1)$ 执行回溯线搜索，以找到一个步长 $t_k$ 使得 Armijo 条件成立：\n$$\nf(x_k - t_k g_k) \\le f(x_k) - c\\,t_k\\,\\lVert g_k\\rVert_2^2.\n$$\n更新迭代点 $x_{k+1}=x_k - t_k g_k$。定义步长范数 $S_k=\\lVert x_{k+1}-x_k\\rVert_2$ 和单步函数下降量 $D_k=f(x_k)-f(x_{k+1})$。\n\n设计带有容差 $\\tau_g0$、$\\tau_s0$ 和 $\\tau_f0$ 的字典序停止准则如下，在每次迭代 $k$ 时按所述顺序进行评估：\n- 第一关（梯度范数优先）：如果 $G_k\\le \\tau_g$，停止。\n- 第二关（步长范数优先）：如果 $k\\ge 0$ 且 $S_k\\le \\tau_s$，停止。\n- 第三关（函数下降量优先）：如果 $k\\ge 0$ 且 $D_k\\le \\tau_f$，停止。\n如果所有关卡都未触发，并且达到了最大迭代次数 $N_{\\max}$，则终止。\n\n您必须在带回溯的梯度下降法中为以下非凸基准函数实现此规则：\n\n- 二维 Himmelblau 函数，定义为\n$$\nf_H(x,y)=\\left(x^2+y-11\\right)^2+\\left(x+y^2-7\\right)^2.\n$$\n\n- 二维 Rastrigin 函数，其中 $A=10$，定义为\n$$\nf_R(x,y)=2A+\\left(x^2 - A\\cos(2\\pi x)\\right)+\\left(y^2 - A\\cos(2\\pi y)\\right).\n$$\n\n您的程序必须使用上述的字典序停止准则和回溯线搜索。对于每个测试用例，返回一个整数代码，指示哪个关卡停止了迭代：如果第一关触发，返回 $1$；如果第二关触发，返回 $2$；如果第三关触发，返回 $3$；如果因达到 $N_{\\max}$ 而终止，返回 $4$。\n\n在以下测试集上实现并评估该规则。每个测试用例是一个参数元组 $(f,\\ x_0,\\ \\tau_g,\\ \\tau_s,\\ \\tau_f,\\ t_0,\\ \\beta,\\ c,\\ N_{\\max})$：\n\n- 情况 A（理想路径，预计由梯度门主导）：$f=f_H$，$x_0=(0,0)$，$\\tau_g=10^{-4}$，$\\tau_s=10^{-12}$，$\\tau_f=10^{-12}$，$t_0=1$，$\\beta=0.5$，$c=10^{-4}$，$N_{\\max}=10^4$。\n\n- 情况 B（边缘情况，通过极小的初始步长强制触发步长门）：$f=f_R$，$x_0=(3,3)$，$\\tau_g=10^{-100}$，$\\tau_s=10^{-10}$，$\\tau_f=10^{-100}$，$t_0=10^{-12}$，$\\beta=0.5$，$c=10^{-4}$，$N_{\\max}=10^3$。\n\n- 情况 C（边缘情况，梯度和步长容差极为严格，触发函数下降门）：$f=f_R$，$x_0=(0.3,-0.3)$，$\\tau_g=10^{-20}$，$\\tau_s=10^{-50}$，$\\tau_f=10^{-8}$，$t_0=0.5$，$\\beta=0.5$，$c=10^{-4}$，$N_{\\max}=10^4$。\n\n- 情况 D（边界情况，起始点为驻点）：$f=f_H$，$x_0=(3,2)$，$\\tau_g=10^{-8}$，$\\tau_s=10^{-12}$，$\\tau_f=10^{-12}$，$t_0=1$，$\\beta=0.5$，$c=10^{-4}$，$N_{\\max}=10^3$。\n\n您的程序应生成单行输出，其中包含情况 A、B、C 和 D 的停止原因代码，按此顺序，以逗号分隔的列表形式包含在方括号中，例如 $\\left[1,2,3,1\\right]$。",
            "solution": "用户要求设计并实现一个配备特定字典序停止准则的梯度下降算法。本解决方案将算法形式化，定义必要的数学组件，并概述实现逻辑。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n\n- **迭代方法**：带回溯线搜索的梯度下降法。\n- **更新规则**：$x_{k+1} = x_k - t_k g_k$，其中 $g_k = \\nabla f(x_k)$。\n- **回溯线搜索**：\n    - **Armijo 条件**：$f(x_k - t_k g_k) \\le f(x_k) - c\\,t_k\\,\\lVert g_k\\rVert_2^2$。\n    - **参数**：初始步长 $t_0  0$，收缩因子 $\\beta \\in (0,1)$，Armijo 参数 $c \\in (0,1)$。\n- **用于停止的量**：\n    - 梯度范数：$G_k = \\lVert g_k \\rVert_2$。\n    - 步长范数：$S_k = \\lVert x_{k+1} - x_k \\rVert_2$。\n    - 函数下降量：$D_k = f(x_k) - f(x_{k+1})$。\n- **字典序停止准则**：\n    - **容差**：$\\tau_g  0$，$\\tau_s  0$，$\\tau_f  0$。\n    - **最大迭代次数**：$N_{\\max}$。\n    - **评估顺序**：\n        1.  **第一关**：如果 $G_k \\le \\tau_g$，停止（返回代码 $1$）。\n        2.  **第二关**：如果 $k \\ge 0$ 且 $S_k \\le \\tau_s$，停止（返回代码 $2$）。\n        3.  **第三关**：如果 $k \\ge 0$ 且 $D_k \\le \\tau_f$，停止（返回代码 $3$）。\n        4.  **最大迭代次数**：如果循环完成 $N_{\\max}$ 次迭代，停止（返回代码 $4$）。\n- **基准函数**：\n    - **Himmelblau 函数 ($f_H$)**：$f_H(x,y) = \\left(x^2+y-11\\right)^2+\\left(x+y^2-7\\right)^2$。\n    - **Rastrigin 函数 ($f_R$)**：$f_R(x,y) = 2A+\\left(x^2 - A\\cos(2\\pi x)\\right)+\\left(y^2 - A\\cos(2\\pi y)\\right)$，其中 $A=10$。\n- **测试集**：\n    - **情况 A**：$(f_H, x_0=(0,0), \\tau_g=10^{-4}, \\tau_s=10^{-12}, \\tau_f=10^{-12}, t_0=1, \\beta=0.5, c=10^{-4}, N_{\\max}=10^4)$。\n    - **情况 B**：$(f_R, x_0=(3,3), \\tau_g=10^{-100}, \\tau_s=10^{-10}, \\tau_f=10^{-100}, t_0=10^{-12}, \\beta=0.5, c=10^{-4}, N_{\\max}=10^3)$。\n    - **情况 C**：$(f_R, x_0=(0.3,-0.3), \\tau_g=10^{-20}, \\tau_s=10^{-50}, \\tau_f=10^{-8}, t_0=0.5, \\beta=0.5, c=10^{-4}, N_{\\max}=10^4)$。\n    - **情况 D**：$(f_H, x_0=(3,2), \\tau_g=10^{-8}, \\tau_s=10^{-12}, \\tau_f=10^{-12}, t_0=1, \\beta=0.5, c=10^{-4}, N_{\\max}=10^3)$。\n- **输出格式**：一个以逗号分隔的整数停止原因代码列表，例如 $[1,2,3,1]$。\n\n**第 2 步：使用提取的已知条件进行验证**\n\n- **科学上可靠**：该问题基于数值优化中的基本和标准概念：梯度下降、一阶最优性条件、回溯线搜索（Armijo 准则）和停止准则。基准函数是测试非凸优化程序的标准函数。该问题在科学上是合理的。\n- **适定**：该问题是一个计算任务，具有明确定义的算法、具体的输入和确定的输出格式。存在一个唯一且有意义的解（停止代码序列），并且可以计算得出。\n- **客观**：所有定义和参数都以精确的数学语言给出。没有主观或含糊的陈述。\n- **完整且一致**：提供了实现算法和运行测试用例所需的所有信息。参数已指定，函数已定义，停止规则的逻辑是明确的。第二关和第三关的条件“如果 $k \\ge 0$”在一个标准的从零开始索引的迭代循环中是自然满足的，但正确地暗示了这些检查从第一次迭代开始就有效。\n- **未检测到其他缺陷**：该问题并非不切实际、不适定、微不足道或无法验证。这是一个结构良好的计算数学练习。\n\n**第 3 步：结论与行动**\n\n问题有效。将提供完整的解决方案。\n\n### 基于原理的解决方案设计\n\n问题的核心是实现一个梯度下降算法。该算法通过沿负梯度方向采取步长，迭代地走向函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的局部最小值。\n\n**1. 梯度下降迭代**\n在每次迭代 $k$ 中，从一个点 $x_k$ 开始，下一个点 $x_{k+1}$ 通过以下方式找到：\n$$\nx_{k+1} = x_k - t_k g_k\n$$\n其中 $g_k = \\nabla f(x_k)$ 是 $f$ 在 $x_k$ 处的梯度，而 $t_k  0$ 是步长。\n\n**2. 步长选择：回溯线搜索**\n步长 $t_k$ 是通过回溯线搜索过程确定的，以确保函数值的充分下降。从初始猜测 $t = t_0$ 开始，步长反复乘以一个因子 $\\beta \\in (0,1)$，直到满足 Armijo-Goldstein 条件：\n$$\nf(x_k - t g_k) \\le f(x_k) - c t \\lVert g_k \\rVert_2^2\n$$\n对于一个常数 $c \\in (0,1)$。第一个满足此不等式的 $t$ 值被选为 $t_k$。这保证了该步长提供的目标函数下降量与步长和梯度范数的平方成正比。\n\n**3. 基准函数及其梯度**\n该算法将在两个非凸函数上进行测试。为了实现，需要它们的梯度。\n\n- **Himmelblau 函数**：$f_H(x,y) = (x^2+y-11)^2 + (x+y^2-7)^2$。\n  其梯度 $\\nabla f_H = (\\frac{\\partial f_H}{\\partial x}, \\frac{\\partial f_H}{\\partial y})$ 是：\n  $$\n  \\frac{\\partial f_H}{\\partial x} = 2(x^2+y-11)(2x) + 2(x+y^2-7)(1) = 4x(x^2+y-11) + 2(x+y^2-7)\n  $$\n  $$\n  \\frac{\\partial f_H}{\\partial y} = 2(x^2+y-11)(1) + 2(x+y^2-7)(2y) = 2(x^2+y-11) + 4y(x+y^2-7)\n  $$\n\n- **Rastrigin 函数**：$f_R(x,y) = 2A + (x^2 - A\\cos(2\\pi x)) + (y^2 - A\\cos(2\\pi y))$，其中 $A=10$。\n  其梯度 $\\nabla f_R = (\\frac{\\partial f_R}{\\partial x}, \\frac{\\partial f_R}{\\partial y})$ 是：\n  $$\n  \\frac{\\partial f_R}{\\partial x} = 2x - A(-\\sin(2\\pi x))(2\\pi) = 2x + 2\\pi A \\sin(2\\pi x)\n  $$\n  $$\n  \\frac{\\partial f_R}{\\partial y} = 2y - A(-\\sin(2\\pi y))(2\\pi) = 2y + 2\\pi A \\sin(2\\pi y)\n  $$\n\n**4. 字典序停止准则和实现逻辑**\n算法的主循环最多进行 $N_{\\max}$ 次迭代。在每次迭代 $k$ 中，按精确的字典序检查停止准则。\n\n设 $x_k$ 是当前迭代点。\n1.  计算函数值 $f_k = f(x_k)$ 和梯度 $g_k = \\nabla f(x_k)$。\n2.  计算梯度范数 $G_k = \\lVert g_k \\rVert_2$。\n3.  **检查第一关**：如果 $G_k \\le \\tau_g$，算法终止并返回停止代码 $1$。此检查基于最优性的一阶必要条件 $\\nabla f(x^\\star) = \\mathbf{0}$。\n4.  如果第一关未满足，执行回溯线搜索以找到步长 $t_k$。\n5.  计算下一个迭代点：$x_{k+1} = x_k - t_k g_k$。\n6.  计算步长范数 $S_k = \\lVert x_{k+1} - x_k \\rVert_2 = t_k \\lVert g_k \\rVert_2$。\n7.  计算新的函数值 $f_{k+1} = f(x_{k+1})$ 和函数下降量 $D_k = f_k - f_{k+1}$。\n8.  **检查第二关**：如果 $S_k \\le \\tau_s$，算法终止并返回停止代码 $2$。这表明迭代点不再有显著移动，可能表示数值停滞或收敛。\n9.  **检查第三关**：如果 $D_k \\le \\tau_f$，算法终止并返回停止代码 $3$。这表明每次迭代中目标函数的改善可以忽略不计。\n10. 如果没有一个关卡被触发，循环继续到下一次迭代 $k+1$，以 $x_{k+1}$ 作为新的点。\n11. **检查最大迭代次数**：如果循环在没有任何关卡被触发的情况下完成了 $N_{\\max}$ 次迭代，算法终止并返回停止代码 $4$。\n\n为确定相应的停止代码，将为每个测试用例实现这一系列操作。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the optimization algorithm.\n    \"\"\"\n\n    # Define benchmark functions and their gradients\n    def himmelblau(x):\n        \"\"\"Himmelblau's function.\"\"\"\n        return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n\n    def grad_himmelblau(x):\n        \"\"\"Gradient of Himmelblau's function.\"\"\"\n        dx = 4 * x[0] * (x[0]**2 + x[1] - 11) + 2 * (x[0] + x[1]**2 - 7)\n        dy = 2 * (x[0]**2 + x[1] - 11) + 4 * x[1] * (x[0] + x[1]**2 - 7)\n        return np.array([dx, dy])\n\n    def rastrigin(x):\n        \"\"\"Rastrigin's function.\"\"\"\n        A = 10\n        return 2 * A + (x[0]**2 - A * np.cos(2 * np.pi * x[0])) + \\\n               (x[1]**2 - A * np.cos(2 * np.pi * x[1]))\n\n    def grad_rastrigin(x):\n        \"\"\"Gradient of Rastrigin's function.\"\"\"\n        A = 10\n        dx = 2 * x[0] + 2 * np.pi * A * np.sin(2 * np.pi * x[0])\n        dy = 2 * x[1] + 2 * np.pi * A * np.sin(2 * np.pi * x[1])\n        return np.array([dx, dy])\n\n    def lexicographic_gradient_descent(f, grad_f, x0, tau_g, tau_s, tau_f, t0, beta, c, n_max):\n        \"\"\"\n        Performs gradient descent with backtracking and a lexicographic stopping rule.\n        \n        Returns an integer stop code:\n        1: Gradient norm tolerance met (Gate 1)\n        2: Step norm tolerance met (Gate 2)\n        3: Function decrease tolerance met (Gate 3)\n        4: Maximum iterations reached\n        \"\"\"\n        x_k = np.array(x0, dtype=float)\n\n        for k in range(n_max):\n            f_k = f(x_k)\n            g_k = grad_f(x_k)\n            G_k = np.linalg.norm(g_k)\n\n            # Gate 1: Gradient norm priority\n            if G_k = tau_g:\n                return 1\n\n            # Backtracking line search\n            t_k = t0\n            while f(x_k - t_k * g_k) > f_k - c * t_k * (G_k**2):\n                t_k *= beta\n                # Safety break for excessively small step sizes\n                if t_k  1e-20: \n                    t_k = 0\n                    break\n\n            x_k_plus_1 = x_k - t_k * g_k\n            \n            S_k = np.linalg.norm(x_k_plus_1 - x_k)\n            f_k_plus_1 = f(x_k_plus_1)\n            D_k = f_k - f_k_plus_1\n\n            # Gate 2: Step norm priority\n            if S_k = tau_s:\n                return 2\n\n            # Gate 3: Function decrease priority\n            if D_k = tau_f:\n                return 3\n\n            x_k = x_k_plus_1\n        \n        # Termination due to maximum iterations\n        return 4\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'f': himmelblau, 'grad_f': grad_himmelblau, 'x0': (0, 0), 'tau_g': 1e-4, \n         'tau_s': 1e-12, 'tau_f': 1e-12, 't0': 1, 'beta': 0.5, 'c': 1e-4, 'n_max': 10000},\n        # Case B\n        {'f': rastrigin, 'grad_f': grad_rastrigin, 'x0': (3, 3), 'tau_g': 1e-100, \n         'tau_s': 1e-10, 'tau_f': 1e-100, 't0': 1e-12, 'beta': 0.5, 'c': 1e-4, 'n_max': 1000},\n        # Case C\n        {'f': rastrigin, 'grad_f': grad_rastrigin, 'x0': (0.3, -0.3), 'tau_g': 1e-20, \n         'tau_s': 1e-50, 'tau_f': 1e-8, 't0': 0.5, 'beta': 0.5, 'c': 1e-4, 'n_max': 10000},\n        # Case D\n        {'f': himmelblau, 'grad_f': grad_himmelblau, 'x0': (3, 2), 'tau_g': 1e-8, \n         'tau_s': 1e-12, 'tau_f': 1e-12, 't0': 1, 'beta': 0.5, 'c': 1e-4, 'n_max': 1000}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = lexicographic_gradient_descent(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "优化过程的收敛路径很少是平滑的，单步的进展可能具有欺骗性。本练习引入了“窗口停滞”准则，它通过在最近的多次迭代窗口内评估进展来平滑噪声，从而避免因暂时的停滞而过早终止。您将通过动手实践，学习如何调整窗口大小和容忍度这些超参数，以在算法的响应速度和决策的可靠性之间取得平衡。",
            "id": "3187906",
            "problem": "考虑通过固定步长的梯度下降（GD）方法来最小化一个可微目标函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$。该方法由迭代格式 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$ 定义，其中 $\\alpha  0$ 是一个选定的步长，$x_k \\in \\mathbb{R}^n$ 是在迭代次数 $k \\in \\mathbb{N}$ 时的迭代点。必须有一个停止法则，根据有原则的容差条件来决定迭代过程何时终止。在此问题中，您将实现并评估一个窗口化停滞准则，该准则有两个可调参数：函数下降容差 $\\tau  0$ 和窗口长度 $m \\in \\mathbb{N}$。对于一个固定的 $\\varepsilon  0$，该法则在第一个同时满足以下两个条件的索引 $k \\ge m$ 处终止迭代：\n$$\nf(x_{k-m}) - f(x_k) \\le \\tau\n\\quad\\text{and}\\quad\n\\|x_k - x_{k-m}\\|_2 \\le \\varepsilon.\n$$\n如果在给定的最大迭代次数 $K$ 内不存在这样的 $k$，则该法则不触发，我们设置 $k_{\\mathrm{stop}} = K$。\n\n您的任务是实现带有上述停止法则的GD算法，并按如下方式为每个测试用例在给定的候选有限集中搜索以调整 $(\\tau, m)$。对于每个候选对 $(\\tau, m)$，从指定的初始点运行GD，直到停止法则触发或达到最大迭代次数 $K$，并记录 $k_{\\mathrm{stop}}$ 和相应的迭代点 $x_{k_{\\mathrm{stop}}}$。仅当停止时的质量要求得到满足时，候选对 $(\\tau, m)$ 才被认为是可接受的。在可接受的候选中，选择具有最小 $k_{\\mathrm{stop}}$ 的一对。如果在 $k_{\\mathrm{stop}}$ 上出现平局，选择具有较大 $m$ 的那个；如果仍然平局，则选择具有较小 $\\tau$ 的那个。如果没有可接受的候选，则选择使下述质量要求的违规度最小化的那一对，并使用相同的平局决胜规则。\n\n质量要求和违规度量。对于每个测试用例，都指定了一个基于范数的解邻近度阈值 $\\rho  0$。质量要求是\n$$\n\\|x_{k_{\\mathrm{stop}}}\\|_2 \\le \\rho.\n$$\n如果不满足此要求，则定义违规度量 $v = \\max\\{0, \\|x_{k_{\\mathrm{stop}}}\\|_2 - \\rho\\}$，并选择使 $v$ 最小化的候选（使用上述相同的平局决胜规则）。\n\n为以下三个案例的测试套件实现上述过程。在所有案例中，使用欧几里得范数 $\\|\\cdot\\|_2$，且不涉及物理单位。\n\n测试用例 1（顺利路径，各向异性二次函数）：\n- 目标函数：$f(x) = \\tfrac{1}{2}(x_1^2 + 10 x_2^2)$ on $\\mathbb{R}^2$。\n- 梯度：$\\nabla f(x) = (x_1, 10 x_2)$。\n- 初始点：$x_0 = (5, -5)$。\n- 步长：$\\alpha = 0.15$。\n- 最大迭代次数：$K = 5000$。\n- 停滞移动容差：$\\varepsilon = 10^{-6}$。\n- 候选集：$\\{\\tau\\} = \\{10^{-7}, 10^{-6}, 10^{-5}\\}$ 和 $\\{m\\} = \\{3, 5, 10\\}$。\n- 质量阈值：$\\rho = 10^{-3}$。\n\n测试用例 2（边界条件，小步长各向同性二次函数）：\n- 目标函数：$f(x) = \\tfrac{1}{2}(x_1^2 + x_2^2)$ on $\\mathbb{R}^2$。\n- 梯度：$\\nabla f(x) = (x_1, x_2)$。\n- 初始点：$x_0 = (2, 2)$。\n- 步长：$\\alpha = 10^{-3}$。\n- 最大迭代次数：$K = 20000$。\n- 停滞移动容差：$\\varepsilon = 10^{-7}$。\n- 候选集：$\\{\\tau\\} = \\{10^{-7}, 10^{-6}, 10^{-5}\\}$ 和 $\\{m\\} = \\{3, 10, 50\\}$。\n- 质量阈值：$\\rho = 5 \\times 10^{-2}$。\n\n测试用例 3（边缘案例，平坦平台几何）：\n- 目标函数：$f(x) = \\arctan(\\|x\\|_2)$ on $\\mathbb{R}^2$。\n- 梯度：当 $x \\neq 0$ 时，$\\nabla f(x) = \\dfrac{1}{1+\\|x\\|_2^2} \\dfrac{x}{\\|x\\|_2}$，当 $x = 0$ 时，$\\nabla f(0) = (0, 0)$。\n- 初始点：$x_0 = (100, 0)$。\n- 步长：$\\alpha = 10^{-1}$。\n- 最大迭代次数：$K = 200000$。\n- 停滞移动容差：$\\varepsilon = 10^{-4}$。\n- 候选集：$\\{\\tau\\} = \\{10^{-8}, 10^{-7}, 10^{-6}\\}$ 和 $\\{m\\} = \\{20, 50\\}$。\n- 质量阈值：$\\rho = 10^{-1}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。每个结果必须是对应测试用例所选出的双元素列表 $[\\tau, m]$，并按上述案例的顺序排列。例如，输出格式必须为 $[[\\tau_1,m_1],[\\tau_2,m_2],[\\tau_3,m_3]]$ 的形式，不含空格。",
            "solution": "我们从梯度下降（GD）的核心定义开始。给定一个可微目标函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 和一个固定步长 $\\alpha  0$，GD通过 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$ 生成迭代点。在温和的正则性条件下且 $\\alpha$ 足够小，序列 $(f(x_k))_{k \\in \\mathbb{N}}$ 是非递增的，并且 $(x_k)$ 会趋近于一个一阶平稳点，在凸设定中这通常是一个极小值点。\n\n一个有原则的停止法则应该能检测到停滞的开始。单次迭代的检查，如 $\\|\\nabla f(x_k)\\| \\le \\text{tolerance}$ 或 $\\|x_{k+1}-x_k\\| \\le \\text{tolerance}$，可能对瞬态噪声或方向变化敏感。窗口化准则通过聚合多次迭代的信息来缓解此问题。定义一个窗口长度 $m \\in \\mathbb{N}$、一个用于函数下降的容差 $\\tau  0$ 和一个用于决策空间中移动的容差 $\\varepsilon  0$。窗口化停滞法则在第一个满足以下条件的 $k \\ge m$ 处宣告停止：\n$$\nf(x_{k-m}) - f(x_k) \\le \\tau,\\quad \\|x_k - x_{k-m}\\|_2 \\le \\varepsilon.\n$$\n这将两个信号耦合在一起：窗口内目标函数的垂直下降和参数的水平移动。如果两者都很小，说明算法在最近的 $m$ 次迭代中进展甚微，这表明要么已接近平稳点，要么步长太小无法产生有意义的进展。在这两种情况下，如果伴随有质量检查，停止是合理的。\n\n质量要求。对每个测试用例，我们施加一个解邻近度的度量 $\\|x_{k_{\\mathrm{stop}}}\\|_2 \\le \\rho$，其中 $\\rho  0$ 是选定的。对于测试用例1和2中的二次目标函数，唯一的极小值点位于 $x^\\star = 0$，因此限制 $\\|x_{k_{\\mathrm{stop}}}\\|_2$ 直接衡量了与极小值点的接近程度。对于测试用例3中的 $\\arctan$ 目标函数，极小值点也在 $x^\\star = 0$ 处，因为当 $\\|x\\|_2 \\ge 0$ 时，$\\arctan(\\|x\\|_2)$ 是严格递增的，所以同样的准则适用。\n\n调整 $(\\tau, m)$。这对参数 $(\\tau, m)$ 控制了法则的敏感度：\n- 较大的 $m$ 在更长的时间范围内跟踪进展，减少了由瞬态行为引起的过早停止的风险。\n- 较小的 $\\tau$ 要求在窗口内有更显著的函数下降才能继续，这将延迟停止，直到算法真正趋于平坦。\n- 移动容差 $\\varepsilon$ 确保了即使函数几乎是平的，除非迭代点本身已经稳定下来，否则我们不会停止。\n\n为了调整 $(\\tau, m)$，我们评估所有候选，并选择在满足质量要求 $\\|x_{k_{\\mathrm{stop}}}\\|_2 \\le \\rho$ 的前提下，使停止迭代索引 $k_{\\mathrm{stop}}$ 最小的那一对。这是一个理性的优化目标：如果能够保证足够的质量，那么提早停止是更可取的。在没有候选满足质量要求的情况下，我们选择使违规度 $v = \\max\\{0, \\|x_{k_{\\mathrm{stop}}}\\|_2 - \\rho\\}$ 最小化的那一对，该违规度量化了结果与期望容差的差距。平局通过偏好更大的窗口 $m$（更稳健的聚合）然后是更小的 $\\tau$（更严格的下降要求）来打破，这与保守的停止策略设计相一致。\n\n算法设计。对于每个测试用例：\n1. 初始化 $x_0$ 并计算 $f(x_0)$。\n2. 对于每个候选对 $(\\tau, m)$：\n   a. 使用 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$ 迭代GD，最多进行 $K$ 步。\n   b. 维护历史记录 $(x_j)$ 和 $(f(x_j))$。对于每个 $k \\ge m$，计算\n      $$\n      \\Delta f_k = f(x_{k-m}) - f(x_k),\\quad d_k = \\|x_k - x_{k-m}\\|_2.\n      $$\n      如果 $\\Delta f_k \\le \\tau$ 且 $d_k \\le \\varepsilon$，则声明 $k_{\\mathrm{stop}} = k$ 并停止循环。\n   c. 如果循环完成而未触发，则设置 $k_{\\mathrm{stop}} = K$。\n   d. 评估质量 $\\|x_{k_{\\mathrm{stop}}}\\|_2 \\le \\rho$ 并记录 $k_{\\mathrm{stop}}$ 和必要的违规度 $v$。\n3. 在可接受的候选中选择使 $k_{\\mathrm{stop}}$ 最小的一对；否则选择使违规度 $v$ 最小的一对，并遵循指定的平局决胜规则。\n\n从基本原理上对窗口化准则的论证。在梯度利普希茨连续（参数为 $L  0$，即 $\\|\\nabla f(x) - \\nabla f(y)\\|_2 \\le L \\|x-y\\|_2$）的GD中，对于 $\\alpha \\in (0, 2/L)$，存在经典的下降性质\n$$\nf(x_{k+1}) \\le f(x_k) - \\tfrac{1}{2}\\alpha (2 - \\alpha L) \\|\\nabla f(x_k)\\|_2^2,\n$$\n这确保了 $f(x_k)$ 的单调递减。当一个窗口内的下降量小于 $\\tau$ 时，该窗口内每次迭代的 $f$ 的平均变化量以 $\\tau/m$ 为界，表明梯度范数平均而言很小。同时，如果 $\\|x_k - x_{k-m}\\|_2 \\le \\varepsilon$，迭代点形成一个近似的柯西段，表示在决策空间中已趋于稳定。综合来看，这些反映了近似平稳性：在梯度小、移动小的情况下，GD的进一步进展将是微不足道的。设置容差 $(\\tau, \\varepsilon)$ 和视界 $m$ 可以校准在停止前能容忍多少边际进展，而调整过程则选择能够尽早达到目标质量的值。\n\n这三个测试用例涵盖了：\n- 一个良态的各向异性二次函数，其中GD收敛迅速，代表了顺利路径。\n- 一个小步长的各向同性二次函数，说明了当每次迭代进展缓慢时，对选择 $\\tau$ 和 $m$ 的敏感性。\n- 一个平坦平台的 $\\arctan(\\|x\\|_2)$ 目标函数，在远离极小值点处梯度极小，暴露了如果调整不当以避免假阳性，朴素的容差可能导致过早停止的边缘案例；即使没有候选达到目标质量，基于违规度的回退机制也能确保做出有原则的选择。\n\n该程序枚举候选的 $(\\tau, m)$ 对，使用窗口化停滞法则运行GD，应用选择标准，并以指定的单行格式输出所选的对。",
            "answer": "```python\nimport numpy as np\n\ndef gd_run(f, grad, x0, alpha, K, eps_move, tau, m):\n    \"\"\"\n    Run Gradient Descent with a windowed stagnation stopping rule.\n    Stop at first k >= m such that:\n        f(x_{k-m}) - f(x_k) = tau and ||x_k - x_{k-m}|| = eps_move\n    If not triggered by K, set k_stop = K.\n    Returns (k_stop, x_stop_norm).\n    \"\"\"\n    x_hist = [np.array(x0, dtype=float)]\n    f_hist = [f(x_hist[0])]\n    x = x_hist[0]\n    k_stop = None\n\n    for k in range(1, K + 1):\n        g = grad(x)\n        x = x - alpha * g\n        fx = f(x)\n        x_hist.append(x)\n        f_hist.append(fx)\n\n        if k >= m:\n            df = f_hist[k - m] - fx\n            move = np.linalg.norm(x - x_hist[k - m])\n            if df = tau and move = eps_move:\n                k_stop = k\n                break\n\n    if k_stop is None:\n        k_stop = K\n        x_stop = x_hist[-1]\n    else:\n        x_stop = x_hist[k_stop]\n\n    return k_stop, np.linalg.norm(x_stop)\n\n\ndef select_tau_m(f, grad, x0, alpha, K, eps_move, tau_list, m_list, rho):\n    \"\"\"\n    Enumerate candidates (tau, m), run GD, and select:\n      - among acceptable candidates (||x_stop|| = rho), the one with minimal k_stop;\n      - ties broken by larger m, then smaller tau;\n      - if none acceptable, minimize violation v = max(0, ||x_stop|| - rho) with same tie-breakers.\n    Returns the selected [tau, m].\n    \"\"\"\n    candidates = []\n    for tau in tau_list:\n        for m in m_list:\n            k_stop, xnorm = gd_run(f, grad, x0, alpha, K, eps_move, tau, m)\n            acceptable = xnorm = rho\n            violation = max(0.0, xnorm - rho)\n            candidates.append({\n                \"tau\": tau, \"m\": m, \"k\": k_stop,\n                \"acceptable\": acceptable, \"violation\": violation\n            })\n\n    # Filter acceptable candidates\n    acceptable_cands = [c for c in candidates if c[\"acceptable\"]]\n    if acceptable_cands:\n        # Sort by k ascending, m descending, tau ascending\n        acceptable_cands.sort(key=lambda c: (c[\"k\"], -c[\"m\"], c[\"tau\"]))\n        best = acceptable_cands[0]\n    else:\n        # Sort by violation ascending, k ascending, m descending, tau ascending\n        candidates.sort(key=lambda c: (c[\"violation\"], c[\"k\"], -c[\"m\"], c[\"tau\"]))\n        best = candidates[0]\n\n    return [best[\"tau\"], best[\"m\"]]\n\n\ndef solve():\n    # Define test cases\n    # Test Case 1\n    def f1(x):\n        return 0.5 * (x[0]**2 + 10.0 * x[1]**2)\n    def g1(x):\n        return np.array([x[0], 10.0 * x[1]])\n    x0_1 = np.array([5.0, -5.0])\n    alpha_1 = 0.15\n    K_1 = 5000\n    eps_1 = 1e-6\n    tau_list_1 = [1e-7, 1e-6, 1e-5]\n    m_list_1 = [3, 5, 10]\n    rho_1 = 1e-3\n\n    # Test Case 2\n    def f2(x):\n        return 0.5 * (x[0]**2 + x[1]**2)\n    def g2(x):\n        return np.array([x[0], x[1]])\n    x0_2 = np.array([2.0, 2.0])\n    alpha_2 = 1e-3\n    K_2 = 20000\n    eps_2 = 1e-7\n    tau_list_2 = [1e-7, 1e-6, 1e-5]\n    m_list_2 = [3, 10, 50]\n    rho_2 = 5e-2\n\n    # Test Case 3\n    def f3(x):\n        r = np.linalg.norm(x)\n        return np.arctan(r)\n    def g3(x):\n        r = np.linalg.norm(x)\n        if r == 0.0:\n            return np.array([0.0, 0.0])\n        return (1.0 / (1.0 + r*r)) * (x / r)\n    x0_3 = np.array([100.0, 0.0])\n    alpha_3 = 1e-1\n    K_3 = 200000\n    eps_3 = 1e-4\n    tau_list_3 = [1e-8, 1e-7, 1e-6]\n    m_list_3 = [20, 50]\n    rho_3 = 1e-1\n\n    results = []\n    # Case 1\n    sel1 = select_tau_m(f1, g1, x0_1, alpha_1, K_1, eps_1, tau_list_1, m_list_1, rho_1)\n    results.append(sel1)\n    # Case 2\n    sel2 = select_tau_m(f2, g2, x0_2, alpha_2, K_2, eps_2, tau_list_2, m_list_2, rho_2)\n    results.append(sel2)\n    # Case 3\n    sel3 = select_tau_m(f3, g3, x0_3, alpha_3, K_3, eps_3, tau_list_3, m_list_3, rho_3)\n    results.append(sel3)\n\n    # Format output without spaces\n    def format_pair(p):\n        return \"[\" + f\"{p[0]},{p[1]}\" + \"]\"\n    out = \"[\" + \",\".join(format_pair(p) for p in results) + \"]\"\n    print(out)\n\nsolve()\n```"
        },
        {
            "introduction": "当目标函数的梯度不可用或难以计算时，我们需要依赖于像 Nelder-Mead 这样的无导数方法。本练习将带您探索这类方法的停止准则，并对比两种常见的策略：基于几何的“单纯形直径”和基于函数值的“函数值跨度”。通过分析它们在精心设计的“角落案例”中的表现，您将深刻理解不同准则的适用场景和潜在缺陷，并认识到选择停止准则需要对算法和问题本身的特性有深入的理解。",
            "id": "3187957",
            "problem": "你的任务是实现 Nelder-Mead (NM) 方法，并使用两种不同的停止准则来比较它们的行为和可靠性。Nelder-Mead 方法是一种用于无约束最小化问题的迭代直接搜索算法。该算法维护一个单纯形，即 $\\mathbb{R}^n$ 空间中的一个由 $n+1$ 个点组成的集合，并通过现有顶点的仿射组合生成新的候选点。该算法使用反射、扩张、收缩和压缩变换，将单纯形向目标值更低的区域移动。\n\n基本原理和定义：\n- 目标是最小化一个实值函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$。\n- $\\mathbb{R}^n$ 空间中的一个单纯形 $\\mathcal{S}$ 是由 $n+1$ 个仿射无关点组成的集合 $\\{x_0,x_1,\\ldots,x_n\\}$。\n- 将顶点按其目标值排序后（$x_0$ 为最佳点，$x_n$ 为最差点），除最差点（目标值最大的顶点）外的所有点的形心 $c$ 定义为 $c=\\frac{1}{n}\\sum_{i=0}^{n-1}x_i$。\n- 反射点为 $x_r=c+\\alpha(c-x_n)$，其中 $\\alpha$ 为反射系数。\n- 扩张点为 $x_e=c+\\gamma(x_r-c)$，其中 $\\gamma$ 为扩张系数。\n- 收缩点可以是外收缩点 $x_{co}=c+\\rho(x_r-c)$ 或内收缩点 $x_{ci}=c+\\rho(x_n-c)$，其中 $\\rho$ 为收缩系数。\n- 压缩变换将所有非最佳顶点替换为 $x_i\\leftarrow x_0+\\sigma(x_i-x_0)$，其中 $\\sigma$ 为压缩系数。\n\n待比较的停止准则：\n- 基于单纯形直径的几何准则：直径 $d(\\mathcal{S})$ 定义为单纯形任意两个顶点之间的最大欧几里得距离，\n$$\nd(\\mathcal{S})=\\max_{0\\le i,j\\le n}\\left\\Vert x_i-x_j\\right\\Vert_2.\n$$\n当 $d(\\mathcal{S}_k)\\le \\varepsilon_d$ 时停止。\n- 基于目标值散布的函数值准则：散布 $s(\\mathcal{S})$ 定义为单纯形各顶点上的最大和最小目标值之差，\n$$\ns(\\mathcal{S})=\\max_i f(x_i)-\\min_i f(x_i).\n$$\n当 $s(\\mathcal{S}_k)\\le \\varepsilon_f$ 时停止。\n\n算法参数：\n- 除非另有说明，否则使用 $\\alpha=1$, $\\gamma=2$, $\\rho=\\tfrac{1}{2}$, $\\sigma=\\tfrac{1}{2}$。\n- 必须强制设置最大迭代次数以避免无限循环。\n\n你的程序必须根据上述基本定义实现 Nelder-Mead 方法，并对每个测试用例运行两次：一次使用几何停止准则 $d(\\mathcal{S}_k)\\le \\varepsilon_d$，另一次使用函数值停止准则 $s(\\mathcal{S}_k)\\le \\varepsilon_f$。对于每次运行，报告迭代次数、最终最佳目标值 $\\min_i f(x_i)$、最终直径 $d(\\mathcal{S}_k)$ 和最终散布 $s(\\mathcal{S}_k)$。\n\n测试套件：\n- 用例 $1$（平坦平台角点情况）：\n  - 维度 $n=2$。\n  - 目标函数 $f(x)=\\left(10^{-12}\\right)\\left(x_1^2+x_2^2\\right)$。\n  - 初始单纯形顶点 $(10,10)$, $(10,11)$, $(11,10)$。\n  - 容差 $\\varepsilon_d=10^{-3}$, $\\varepsilon_f=10^{-10}$。\n  - 最大迭代次数 $200$。\n- 用例 $2$（小单纯形的陡峭梯度角点情况）：\n  - 维度 $n=2$。\n  - 目标函数 $f(x)=\\exp\\left(50x_1\\right)+\\left(10^{-3}\\right)x_2^2$。\n  - 初始单纯形顶点 $(0.1,0)$, $(0.1001,0)$, $(0.1,0.0001)$。\n  - 容差 $\\varepsilon_d=5\\times 10^{-4}$, $\\varepsilon_f=10^{-2}$。\n  - 最大迭代次数 $200$。\n- 用例 $3$（理想路径，弯曲谷底）：\n  - 维度 $n=2$。\n  - 目标函数 $f(x)=\\left(1-x_1\\right)^2+100\\left(x_2-x_1^2\\right)^2$。\n  - 初始单纯形顶点 $(-1.2,1)$, $(-1.2,1.2)$, $(-1,1)$。\n  - 容差 $\\varepsilon_d=10^{-3}$, $\\varepsilon_f=10^{-6}$。\n  - 最大迭代次数 $1000$。\n- 用例 $4$（直径上的边界相等情况）：\n  - 维度 $n=2$。\n  - 目标函数 $f(x)=x_1^2+x_2^2$。\n  - 初始单纯形顶点 $(0,0)$, $(1,0)$, $(0,1)$。\n  - 容差 $\\varepsilon_d=\\sqrt{2}$, $\\varepsilon_f=0.5$。\n  - 最大迭代次数 $200$。\n\n每个测试用例的所需输出：\n- 对于每个测试用例，输出列表\n$$\n[\\;I_d,\\;I_f,\\;B,\\;F_d,\\;F_f,\\;D_d,\\;S_d,\\;D_f,\\;S_f\\;]\n$$\n其中，$I_d$ 是按直径停止时的迭代次数，$I_f$ 是按散布停止时的迭代次数，$B$ 是一个布尔值，当 $I_d  I_f$ 时为 True，$F_d$ 和 $F_f$ 分别是按直径和按散布停止时的最终最佳函数值，$D_d$ 和 $S_d$ 是按直径停止时的最终直径和散布，$D_f$ 和 $S_f$ 是按散布停止时的最终直径和散布。输出必须是四个此类列表的列表，以逗号分隔，并用方括号括起来，不含空格，例如 `[[...],[...],[...],[...]]`。",
            "solution": "该问题要求实现 Nelder-Mead 直接搜索优化算法。该实现必须基于所提供的基本定义，并且必须支持两种不同的停止准则：一种基于单纯形的几何直径，另一种基于单纯形顶点处目标值的函数值散布。这两种准则的性能将在四个测试用例中进行比较，每个用例都旨在突显算法的不同行为。\n\n解决方案的核心是执行 Nelder-Mead 方法的函数。该算法是迭代的，对于目标函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$，其过程如下。\n\n给定一个由 $\\mathbb{R}^n$ 空间中 $n+1$ 个顶点组成的初始单纯形 $\\mathcal{S}_0 = \\{x_0, x_1, \\ldots, x_n\\}$。然后算法进入一个循环，在每次迭代 $k=0, 1, 2, \\ldots$ 中：\n$1$。 对当前单纯形 $\\mathcal{S}_k$ 的 $n+1$ 个顶点进行求值和排序，使其目标函数值有序：$f(x_0) \\le f(x_1) \\le \\ldots \\le f(x_n)$。顶点 $x_0$ 和 $x_n$ 分别被指定为最佳顶点和最差顶点。\n\n$2$。 检查停止准则。对每个测试用例执行两次独立的运行。\n   - 对于几何准则的运行，计算单纯形直径 $d(\\mathcal{S}_k) = \\max_{0 \\le i,j \\le n} \\|x_i - x_j\\|_2$。如果 $d(\\mathcal{S}_k) \\le \\varepsilon_d$，算法终止，并报告迭代次数为 $k$。\n   - 对于函数值准则的运行，计算目标值的散布 $s(\\mathcal{S}_k) = f(x_n) - f(x_0)$。如果 $s(\\mathcal{S}_k) \\le \\varepsilon_f$，算法终止，并报告迭代次数为 $k$。\n   - 在两种情况下，如果迭代次数 $k$ 达到指定的最大值 $k_{\\max}$，算法也会终止。\n\n$3$。 如果没有满足停止准则，则生成一个新的单纯形 $\\mathcal{S}_{k+1}$。这从计算 $n$ 个最佳顶点的形心 $c$ 开始：\n$$\nc = \\frac{1}{n} \\sum_{i=0}^{n-1} x_i\n$$\n\n$4$。 尝试一系列变换，用一个更好的点替换最差顶点 $x_n$。这些变换的标准参数是：反射系数 $\\alpha=1$，扩张系数 $\\gamma=2$，收缩系数 $\\rho=\\frac{1}{2}$，以及压缩系数 $\\sigma=\\frac{1}{2}$。\n   - **反射**：生成一个反射点 $x_r = c + \\alpha(c - x_n)$。设其函数值为 $f_r = f(x_r)$。\n   - **扩张**：如果 $f_r$ 优于当前最佳值，即 $f_r  f(x_0)$，算法会通过计算扩张点 $x_e = c + \\gamma(x_r - c)$ 在该方向上进一步探索。如果 $f(x_e)  f_r$，则新顶点为 $x_e$；否则，新顶点为 $x_r$。这个新点将替换 $x_n$。\n   - **接受**：如果反射点不是新的最佳点，但优于次差顶点，即 $f(x_0) \\le f_r  f(x_{n-1})$，则接受 $x_r$ 并用它替换 $x_n$。\n   - **收缩**：如果反射点没有改进，即 $f_r \\ge f(x_{n-1})$，则执行收缩操作。\n     - 如果 $f_r  f(x_n)$，则执行*外收缩*，计算 $x_{co} = c + \\rho(x_r - c)$。如果 $f(x_{co}) \\le f_r$，则用 $x_{co}$ 替换 $x_n$。否则，执行压缩操作。\n     - 如果 $f_r \\ge f(x_n)$，则执行*内收缩*，计算 $x_{ci} = c + \\rho(x_n - c)$。如果 $f(x_{ci})  f(x_n)$，则用 $x_{ci}$ 替换 $x_n$。否则，执行压缩操作。\n   - **压缩**：如果收缩步骤未能产生改进，则将整个单纯形向最佳顶点 $x_0$ 压缩。所有顶点 $x_i$（其中 $i \\in \\{1, 2, \\ldots, n\\}$）都根据规则 $x_i \\leftarrow x_0 + \\sigma(x_i - x_0)$ 进行替换。\n\n此过程生成一个新的单纯形 $\\mathcal{S}_{k+1}$，然后开始下一次迭代。\n\n这些测试用例旨在探究停止准则的相对有效性。\n- **用例 1**：一个近乎平坦的二次函数。即使单纯形顶点相距很远，函数值散布 $s(\\mathcal{S})$ 也可能迅速变得非常小，这可能导致在函数值准则下过早终止。\n- **用例 2**：一个具有非常陡峭的指数分量的函数。在函数值急剧变化的区域，单纯形在几何上可能变得很小（$d(\\mathcal{S})$ 很小），这可能导致在几何准则下过早终止。\n- **用例 3**：Rosenbrock 函数，这是一个具有狭窄弯曲谷底的标准基准测试。它用作对算法稳健性的一般性测试。\n- **用例 4**：一个简单的二次函数，其初始单纯形直径恰好等于容差 $\\varepsilon_d$，用于测试在迭代 $k=0$ 时对边界条件的处理。\n\n程序将执行此逻辑，对每个测试用例运行两次（每种停止准则一次），并将结果——迭代次数（$I_d, I_f$）、布尔比较（$B$）、最终最佳目标值（$F_d, F_f$）以及最终单纯形度量（$D_d, S_d, D_f, S_f$）——整理成指定的列表格式作为最终输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef nelder_mead(func, initial_simplex, tol_d, tol_f, max_iter, stop_criterion):\n    \"\"\"\n    Implements the Nelder-Mead optimization algorithm.\n\n    Args:\n        func: The objective function to minimize.\n        initial_simplex: The starting simplex as a numpy array of shape (n+1, n).\n        tol_d: Tolerance for the geometric (diameter) stopping criterion.\n        tol_f: Tolerance for the functional (spread) stopping criterion.\n        max_iter: The maximum number of iterations.\n        stop_criterion: A string, either 'diameter' or 'spread'.\n\n    Returns:\n        A tuple containing:\n        - k (int): Number of iterations.\n        - best_val (float): The best objective function value found.\n        - diameter (float): The final diameter of the simplex.\n        - spread (float): The final spread of the objective values.\n    \"\"\"\n    # Standard Nelder-Mead parameters\n    alpha, gamma, rho, sigma = 1.0, 2.0, 0.5, 0.5\n    \n    simplex = np.copy(initial_simplex)\n    n = simplex.shape[1]  # Dimension of the problem\n\n    for k in range(max_iter + 1):\n        # 1. Evaluate and order vertices\n        f_values = np.array([func(x) for x in simplex])\n        sorted_indices = np.argsort(f_values)\n        simplex = simplex[sorted_indices]\n        f_values = f_values[sorted_indices]\n\n        # 2. Calculate stopping metrics for the current simplex\n        # Simplex diameter\n        diameter = 0.0\n        for i in range(n + 1):\n            for j in range(i + 1, n + 1):\n                dist = np.linalg.norm(simplex[i] - simplex[j])\n                if dist > diameter:\n                    diameter = dist\n        \n        # Function value spread\n        spread = f_values[-1] - f_values[0]\n\n        # 3. Check for termination\n        if (stop_criterion == 'diameter' and diameter = tol_d) or \\\n           (stop_criterion == 'spread' and spread = tol_f) or \\\n           k == max_iter:\n            return k, f_values[0], diameter, spread\n\n        # Get best, worst, and second-worst points and values\n        x0, xn, xn_1 = simplex[0], simplex[-1], simplex[-2]\n        f0, fn, fn_1 = f_values[0], f_values[-1], f_values[-2]\n\n        # 4. Centroid\n        centroid = np.mean(simplex[:-1], axis=0)\n\n        # 5. Reflection\n        xr = centroid + alpha * (centroid - xn)\n        fr = func(xr)\n\n        new_simplex = np.copy(simplex)\n        shrink = False\n\n        if f0 = fr  fn_1:\n            # Accept reflected point\n            new_simplex[-1] = xr\n        elif fr  f0:\n            # Expansion\n            xe = centroid + gamma * (xr - centroid)\n            fe = func(xe)\n            if fe  fr:\n                new_simplex[-1] = xe\n            else:\n                new_simplex[-1] = xr\n        else: # fr >= fn_1, needs contraction or shrink\n            if fr  fn:\n                # Outside Contraction\n                xco = centroid + rho * (xr - centroid)\n                fco = func(xco)\n                if fco = fr:\n                    new_simplex[-1] = xco\n                else:\n                    shrink = True\n            else: # fr >= fn\n                # Inside Contraction\n                xci = centroid + rho * (xn - centroid)\n                fci = func(xci)\n                if fci  fn:\n                    new_simplex[-1] = xci\n                else:\n                    shrink = True\n        \n        if shrink:\n            # Shrink transformation\n            for i in range(1, n + 1):\n                new_simplex[i] = x0 + sigma * (simplex[i] - x0)\n        \n        simplex = new_simplex\n\n    # This part should not be reached due to loop range and k == max_iter check\n    return max_iter, f_values[0], diameter, spread\n\ndef solve():\n    # Define objective functions\n    def f1(x): return 1e-12 * (x[0]**2 + x[1]**2)\n    def f2(x): return np.exp(50 * x[0]) + 1e-3 * x[1]**2\n    def f3(x): return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n    def f4(x): return x[0]**2 + x[1]**2\n\n    # Test cases from the problem statement\n    test_cases = [\n        {\n            \"func\": f1,\n            \"initial_simplex\": np.array([[10, 10], [10, 11], [11, 10]], dtype=float),\n            \"tol_d\": 1e-3, \"tol_f\": 1e-10, \"max_iter\": 200\n        },\n        {\n            \"func\": f2,\n            \"initial_simplex\": np.array([[0.1, 0], [0.1001, 0], [0.1, 0.0001]], dtype=float),\n            \"tol_d\": 5e-4, \"tol_f\": 1e-2, \"max_iter\": 200\n        },\n        {\n            \"func\": f3,\n            \"initial_simplex\": np.array([[-1.2, 1], [-1.2, 1.2], [-1, 1]], dtype=float),\n            \"tol_d\": 1e-3, \"tol_f\": 1e-6, \"max_iter\": 1000\n        },\n        {\n            \"func\": f4,\n            \"initial_simplex\": np.array([[0, 0], [1, 0], [0, 1]], dtype=float),\n            \"tol_d\": np.sqrt(2), \"tol_f\": 0.5, \"max_iter\": 200\n        }\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        # Run with diameter stopping criterion\n        Id, Fd, Dd, Sd = nelder_mead(\n            case[\"func\"], case[\"initial_simplex\"],\n            case[\"tol_d\"], case[\"tol_f\"], case[\"max_iter\"], 'diameter'\n        )\n\n        # Run with function spread stopping criterion\n        If, Ff, Df, Sf = nelder_mead(\n            case[\"func\"], case[\"initial_simplex\"],\n            case[\"tol_d\"], case[\"tol_f\"], case[\"max_iter\"], 'spread'\n        )\n\n        B = Id  If\n        \n        # Assemble results for the current case\n        case_results = [Id, If, B, Fd, Ff, Dd, Sd, Df, Sf]\n        \n        # Format the list into a string representation without spaces\n        case_str = f\"[{','.join(map(str, case_results))}]\"\n        all_results_str.append(case_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        }
    ]
}