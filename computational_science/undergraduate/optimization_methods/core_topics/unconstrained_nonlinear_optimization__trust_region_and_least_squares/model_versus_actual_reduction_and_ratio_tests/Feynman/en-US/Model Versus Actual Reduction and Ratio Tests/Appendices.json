{
    "hands_on_practices": [
        {
            "introduction": "The ratio test is not just a passive measure of quality; it's an active control mechanism. This first exercise provides a concrete, hands-on calculation to demonstrate this feedback loop in one of its most common forms. By computing the ratio $\\rho_k$ for a damped Newton step, you will see precisely how its value determines the update to the Levenberg-Marquardt damping parameter, regulating the algorithm's behavior from one iteration to the next .",
            "id": "3152688",
            "problem": "Consider minimizing a twice continuously differentiable scalar function $f(x)$ using a damped Newton step computed via the Levenbergâ€“Marquardt (LM) modification. The LM step $s_k$ at iterate $x_k$ with parameter $\\lambda_k$ is defined implicitly by the linear system $(\\nabla^{2} f(x_k) + \\lambda_k I)s_k = -\\nabla f(x_k)$, where $I$ is the identity operator and $\\nabla f(x)$, $\\nabla^{2} f(x)$ denote the gradient and Hessian, respectively. The local quadratic Taylor model of $f$ around $x_k$ is $m_k(s) = f(x_k) + \\nabla f(x_k)^{\\top} s + \\tfrac{1}{2}s^{\\top} \\nabla^{2} f(x_k) s$. The actual reduction is $\\mathrm{AR}_k(s) = f(x_k) - f(x_k + s)$ and the predicted reduction is $\\mathrm{PR}_k(s) = f(x_k) - m_k(s)$. The ratio test uses the quantity $\\rho_k = \\dfrac{\\mathrm{AR}_k(s_k)}{\\mathrm{PR}_k(s_k)}$ to judge the quality of the model relative to the actual reduction.\n\nWork in one dimension with the function $f(x) = \\dfrac{1}{4}x^{4} - x^{3} + \\dfrac{3}{2}x^{2}$. At the current iterate $x_k = \\dfrac{1}{2}$, set the LM parameter to $\\lambda_k = \\dfrac{1}{20}$. Using the fundamental definition of the quadratic Taylor model (truncate the Taylor series of $f$ at second order about $x_k$) and the LM step definition, compute the ratio $\\rho_k$ for the step $s_k$ obtained from $(\\nabla^{2} f(x_k) + \\lambda_k I)s_k = -\\nabla f(x_k)$. Then apply the following ratio-based LM update policy:\n- If $\\rho_k  \\underline{\\eta}$ with $\\underline{\\eta} = \\dfrac{1}{4}$, set $\\lambda_{k+1} = 8 \\lambda_k$.\n- If $\\underline{\\eta} \\le \\rho_k \\le \\overline{\\eta}$ with $\\overline{\\eta} = \\dfrac{3}{4}$, set $\\lambda_{k+1} = \\lambda_k$.\n- If $\\rho_k > \\overline{\\eta}$, set $\\lambda_{k+1} = \\dfrac{\\lambda_k}{2}$.\n\nYour final answer must be the single value of $\\lambda_{k+1}$ as an exact fraction. No rounding is required. The scenario is constructed so that overly aggressive damping (too small $\\lambda_k$) produces a step whose quadratic-model predicted reduction is poor relative to the actual reduction, thereby triggering an increase in $\\lambda_k$.",
            "solution": "The problem is to determine the next Levenberg-Marquardt (LM) parameter, $\\lambda_{k+1}$, based on the ratio $\\rho_k$ of actual to predicted reduction for a specific one-dimensional function $f(x)$ at a given point $x_k$ and with a given initial LM parameter $\\lambda_k$.\n\nFirst, we define the function and compute its first and second derivatives:\nThe function is $f(x) = \\dfrac{1}{4}x^{4} - x^{3} + \\dfrac{3}{2}x^{2}$.\nThe first derivative (gradient in 1D) is $f'(x) = x^{3} - 3x^{2} + 3x$.\nThe second derivative (Hessian in 1D) is $f''(x) = 3x^{2} - 6x + 3 = 3(x-1)^{2}$.\n\nNext, we evaluate the function and its derivatives at the current iterate $x_k = \\dfrac{1}{2}$:\n$f(x_k) = f\\left(\\dfrac{1}{2}\\right) = \\dfrac{1}{4}\\left(\\dfrac{1}{16}\\right) - \\dfrac{1}{8} + \\dfrac{3}{2}\\left(\\dfrac{1}{4}\\right) = \\dfrac{1}{64} - \\dfrac{8}{64} + \\dfrac{24}{64} = \\dfrac{17}{64}$.\n$f'(x_k) = f'\\left(\\dfrac{1}{2}\\right) = \\left(\\dfrac{1}{2}\\right)^{3} - 3\\left(\\dfrac{1}{2}\\right)^{2} + 3\\left(\\dfrac{1}{2}\\right) = \\dfrac{1}{8} - \\dfrac{6}{8} + \\dfrac{12}{8} = \\dfrac{7}{8}$.\n$f''(x_k) = f''\\left(\\dfrac{1}{2}\\right) = 3\\left(\\dfrac{1}{2} - 1\\right)^{2} = 3\\left(-\\dfrac{1}{2}\\right)^{2} = \\dfrac{3}{4}$.\n\nThe LM step $s_k$ is computed using the given parameter $\\lambda_k = \\dfrac{1}{20}$. The LM system in one dimension is:\n$$ (f''(x_k) + \\lambda_k) s_k = -f'(x_k) $$\nSubstituting the computed values:\n$$ \\left(\\dfrac{3}{4} + \\dfrac{1}{20}\\right) s_k = -\\dfrac{7}{8} \\implies \\left(\\dfrac{16}{20}\\right) s_k = -\\dfrac{7}{8} \\implies \\dfrac{4}{5} s_k = -\\dfrac{7}{8} $$\nSolving for $s_k$:\n$$ s_k = -\\dfrac{7}{8} \\cdot \\dfrac{5}{4} = -\\dfrac{35}{32} $$\n\nNow, we compute the ratio $\\rho_k = \\dfrac{\\mathrm{AR}_k(s_k)}{\\mathrm{PR}_k(s_k)}$.\nThe predicted reduction is $\\mathrm{PR}_k(s_k) = f(x_k) - m_k(s_k) = -f'(x_k)s_k - \\dfrac{1}{2}f''(x_k)s_k^2$.\n$$ \\mathrm{PR}_k(s_k) = -\\left(\\dfrac{7}{8}\\right)\\left(-\\dfrac{35}{32}\\right) - \\dfrac{1}{2}\\left(\\dfrac{3}{4}\\right)\\left(-\\dfrac{35}{32}\\right)^2 = \\dfrac{245}{256} - \\dfrac{3}{8}\\left(\\dfrac{1225}{1024}\\right) = \\dfrac{7840 - 3675}{8192} = \\dfrac{4165}{8192} $$\nThe actual reduction $\\mathrm{AR}_k(s_k)$ is related to the predicted reduction through the higher-order terms of the Taylor expansion. Let $R_k(s_k)$ be the remainder term:\n$$ f(x_k + s_k) = m_k(s_k) + R_k(s_k) \\implies \\mathrm{AR}_k(s_k) = f(x_k) - f(x_k + s_k) = \\mathrm{PR}_k(s_k) - R_k(s_k) $$\nThe ratio is therefore $\\rho_k = 1 - \\dfrac{R_k(s_k)}{\\mathrm{PR}_k(s_k)}$.\nThe remainder $R_k(s_k)$ for a quartic polynomial is given by $R_k(s_k) = \\dfrac{1}{6}f'''(x_k)s_k^3 + \\dfrac{1}{24}f^{(4)}(x_k)s_k^4$.\nThe third and fourth derivatives are:\n$f'''(x) = 6x-6 \\implies f'''(\\frac{1}{2}) = 3 - 6 = -3$.\n$f^{(4)}(x) = 6 \\implies f^{(4)}(\\frac{1}{2}) = 6$.\nNow, we compute $R_k(s_k)$:\n$$ R_k(s_k) = \\dfrac{1}{6}(-3)\\left(-\\dfrac{35}{32}\\right)^3 + \\dfrac{1}{24}(6)\\left(-\\dfrac{35}{32}\\right)^4 = -\\dfrac{1}{2}\\left(-\\dfrac{42875}{32768}\\right) + \\dfrac{1}{4}\\left(\\dfrac{1500625}{1048576}\\right) $$\n$$ R_k(s_k) = \\dfrac{42875}{65536} + \\dfrac{1500625}{4194304} = \\dfrac{2744000 + 1500625}{4194304} = \\dfrac{4244625}{4194304} $$\nNow we compute the ratio $\\rho_k$:\n$$ \\rho_k = 1 - \\frac{R_k(s_k)}{\\mathrm{PR}_k(s_k)} = 1 - \\frac{4244625/4194304}{4165/8192} = 1 - \\frac{4244625 \\cdot 8192}{4194304 \\cdot 4165} = 1 - \\frac{4244625}{512 \\cdot 4165} $$\n$$ \\rho_k = 1 - \\frac{4244625}{2132480} = \\frac{2132480 - 4244625}{2132480} = \\frac{-2112145}{2132480} $$\nThe numerator and denominator share a common factor of 245. Simplifying the fraction gives:\n$$ \\rho_k = \\frac{-2112145 \\div 245}{2132480 \\div 245} = -\\frac{8621}{8704} $$\nFinally, we apply the update rule for $\\lambda_{k+1}$. The thresholds are $\\underline{\\eta} = \\dfrac{1}{4}$ and $\\overline{\\eta} = \\dfrac{3}{4}$.\nSince $\\rho_k = -\\dfrac{8621}{8704}$ is a negative number, it is clearly less than $\\underline{\\eta} = \\dfrac{1}{4}$.\nThe rule states: if $\\rho_k  \\underline{\\eta}$, set $\\lambda_{k+1} = 8 \\lambda_k$.\nGiven $\\lambda_k = \\dfrac{1}{20}$, the new parameter is:\n$$ \\lambda_{k+1} = 8 \\cdot \\lambda_k = 8 \\cdot \\dfrac{1}{20} = \\dfrac{8}{20} = \\dfrac{2}{5} $$\nThe poor quality of the quadratic model (indicated by a negative $\\rho_k$) correctly triggers an increase in the damping parameter $\\lambda$, which will lead to a smaller, more conservative step in the next iteration.",
            "answer": "$$\\boxed{\\frac{2}{5}}$$"
        },
        {
            "introduction": "Within a trust-region framework, various strategies exist for computing a trial step, each with different computational costs and approximation qualities. This practice moves from theory to implementation, asking you to compare the performance of the classic Cauchy, dogleg, and exact trust-region steps on a challenging ill-conditioned problem. By examining their respective $\\rho_k$ ratios, you will gain insight into the practical trade-offs and the varying reliability of these fundamental methods .",
            "id": "3152579",
            "problem": "Consider a twice continuously differentiable quadratic objective function defined by $$f(x) = \\tfrac{1}{2} x^\\top H x + b^\\top x + c,$$ where $$H \\in \\mathbb{R}^{n \\times n}$$ is symmetric positive definite, $$b \\in \\mathbb{R}^n,$$ and $$c \\in \\mathbb{R}.$$ At a current point $$x_k,$$ the local quadratic model used in Trust-Region (TR) methods is $$m_k(s) = f(x_k) + g_k^\\top s + \\tfrac{1}{2} s^\\top H s,$$ where $$g_k = \\nabla f(x_k) = H x_k + b.$$ The TR subproblem is to find a step $$s_k$$ that minimizes $$m_k(s)$$ subject to $$\\|s\\|_2 \\le \\Delta_k,$$ where $$\\Delta_k > 0$$ is the trust-region radius. The model-predicted reduction is $$\\mathrm{PR}_k = m_k(0) - m_k(s_k),$$ and the actual reduction is $$\\mathrm{AR}_k = f(x_k) - f(x_k + s_k).$$ The ratio test compares the model and actual reductions via $$\\rho_k = \\frac{\\mathrm{AR}_k}{\\mathrm{PR}_k}.$$\n\nYour task is to implement a program that, for an ill-conditioned quadratic and specified test cases, computes $$\\rho_k$$ using three different TR steps:\n- the Cauchy (steepest descent) step,\n- the dogleg step,\n- the exact TR solution step.\n\nYou must construct the following ill-conditioned quadratic and evaluation settings:\n- Dimension $$n = 3.$$\n- Hessian $$H = \\operatorname{diag}(10^6, 1, 10^{-6}).$$\n- Vector $$b = (0, 0, 0)^\\top,$$ so that $$g_k = H x_k.$$\n- Scalar offset $$c$$ can be treated as an arbitrary constant because it cancels in reduction differences.\n- To emulate roundoff effects in actual function evaluations on ill-conditioned problems, define a perturbed evaluation $$\\tilde{f}(x) = f(x) + \\delta(x)$$ with $$\\delta(x) = \\eta \\, \\sqrt{\\kappa(H)} \\, \\|x\\|_2 \\left(\\left|b^\\top x\\right| + \\tfrac{1}{2}\\left|x^\\top H x\\right|\\right),$$ where $$\\kappa(H)$$ is the condition number of $$H$$ in the $$2$$-norm and $$\\eta > 0$$ is a small scalar supplied per test. In this problem, since $$b = 0,$$ $$\\delta(x) = \\eta \\, \\sqrt{\\kappa(H)} \\, \\|x\\|_2 \\left(\\tfrac{1}{2}\\left|x^\\top H x\\right|\\right).$$\n- The predicted reduction should be computed from the model without perturbation, $$\\mathrm{PR}_k = -\\left(g_k^\\top s_k + \\tfrac{1}{2} s_k^\\top H s_k\\right),$$ and the actual reduction should be computed using $$\\tilde{f}$$, $$\\mathrm{AR}_k = \\tilde{f}(x_k) - \\tilde{f}(x_k + s_k).$$\n\nImplement the three steps as follows based on standard optimization principles:\n- Cauchy step: move along the negative gradient direction and truncate to the trust-region boundary if needed.\n- Dogleg step: combine the Cauchy step and the Newton step (when the Newton step is outside the trust region, take the intersection on the segment connecting these two points with the trust-region boundary).\n- Exact TR solution: solve the TR subproblem by finding the unique Lagrange multiplier $$\\lambda \\ge 0$$ such that $$s(\\lambda) = -(H + \\lambda I)^{-1} g_k$$ satisfies $$\\|s(\\lambda)\\|_2 = \\Delta_k,$$ with the unconstrained Newton step used if it lies within the trust region.\n\nUse the following test suite, where each case provides $$x_k,$$ $$\\Delta_k,$$ and $$\\eta$$:\n- Case $$1$$ (happy path, Newton step inside trust region): $$x_k = (10^{-3}, -10^{-3}, 2 \\cdot 10^{-3})^\\top,$$ $$\\Delta_k = 10^{-1},$$ $$\\eta = 10^{-16}.$$\n- Case $$2$$ (small trust region, Cauchy truncation): $$x_k = (10^{-3}, -10^{-3}, 2 \\cdot 10^{-3})^\\top,$$ $$\\Delta_k = 10^{-4},$$ $$\\eta = 10^{-16}.$$\n- Case $$3$$ (dogleg segment case, Cauchy inside but Newton outside): $$x_k = (2 \\cdot 10^{-1}, -10^{-1}, 5 \\cdot 10^{-2})^\\top,$$ $$\\Delta_k = 2.2 \\cdot 10^{-1},$$ $$\\eta = 10^{-16}.$$\n- Case $$4$$ (ill-conditioning emphasizing small-eigenvalue direction): $$x_k = (0, 0, 10^{3})^\\top,$$ $$\\Delta_k = 10^{-2},$$ $$\\eta = 10^{-16}.$$\n\nYour program must compute, for each case, the list $$[\\rho_k^{\\text{Cauchy}}, \\rho_k^{\\text{dogleg}}, \\rho_k^{\\text{exact}}]$$ and aggregate all case results into a single list. The final output must be a single line containing this aggregated list in a comma-separated, square-bracketed format (for example, $$[ [\\rho_1^{\\text{C}}, \\rho_1^{\\text{D}}, \\rho_1^{\\text{E}}], [\\rho_2^{\\text{C}}, \\rho_2^{\\text{D}}, \\rho_2^{\\text{E}}], \\ldots ]$$), with numerical values printed as standard decimal floats.\n\nNo physical units apply. Angles do not appear. Percentages must not be used; report $$\\rho_k$$ values as decimals.\n\nThe task should be solved in purely mathematical and algorithmic terms, starting from the core definitions and well-tested facts outlined above, without relying on unstated assumptions or shortcuts. Your implementation must be self-contained and must not require any user input.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $$[ [r_{11}, r_{12}, r_{13}], [r_{21}, r_{22}, r_{23}], [r_{31}, r_{32}, r_{33}], [r_{41}, r_{42}, r_{43}] ]$$).",
            "solution": "The problem is valid as it is scientifically grounded in numerical optimization theory, well-posed with all necessary data and definitions, and objective in its formulation. We will proceed to solve it by implementing the specified algorithms.\n\nThe core task is to compute the ratio $\\rho_k = \\frac{\\mathrm{AR}_k}{\\mathrm{PR}_k}$ for three different trust-region (TR) steps: the Cauchy step, the dogleg step, and the exact TR solution. The context is minimizing an ill-conditioned quadratic function.\n\nThe objective function is $f(x) = \\frac{1}{2} x^\\top H x + b^\\top x + c$. Given the problem's specifications:\n- The dimension is $n=3$.\n- The Hessian matrix is $H = \\operatorname{diag}(10^6, 1, 10^{-6})$. This matrix is symmetric and positive definite, with eigenvalues $\\lambda_{\\max} = 10^6$ and $\\lambda_{\\min} = 10^{-6}$.\n- The linear term coefficient is $b = (0, 0, 0)^\\top$.\n- The scalar offset $c$ is constant and cancels in all reduction calculations, so we can set $c=0$.\nThis simplifies the objective function to $f(x) = \\frac{1}{2} x^\\top H x$.\n\nThe gradient at a point $x_k$ is $g_k = \\nabla f(x_k) = Hx_k + b = Hx_k$. The quadratic model of $f$ around $x_k$ is $m_k(s) = f(x_k) + g_k^\\top s + \\frac{1}{2} s^\\top H s$. Since the model uses the exact Hessian $H$, it is equivalent to $f(x_k+s)$.\n\nThe predicted reduction from a step $s_k$ is based on the model:\n$$\n\\mathrm{PR}_k = m_k(0) - m_k(s_k) = f(x_k) - (f(x_k) + g_k^\\top s_k + \\frac{1}{2} s_k^\\top H s_k) = -\\left(g_k^\\top s_k + \\frac{1}{2} s_k^\\top H s_k\\right)\n$$\nSince $H$ is positive definite and the steps $s_k$ are chosen to minimize the model, $\\mathrm{PR}_k$ will be non-negative.\n\nThe actual reduction is computed using a perturbed function evaluation $\\tilde{f}(x)$ to simulate roundoff errors:\n$$\n\\mathrm{AR}_k = \\tilde{f}(x_k) - \\tilde{f}(x_k + s_k)\n$$\nThe perturbed function is defined as $\\tilde{f}(x) = f(x) + \\delta(x)$, where the perturbation $\\delta(x)$ is given by:\n$$\n\\delta(x) = \\eta \\, \\sqrt{\\kappa(H)} \\, \\|x\\|_2 \\left(\\frac{1}{2}\\left|x^\\top H x\\right|\\right)\n$$\nThe 2-norm condition number of the diagonal matrix $H$ is $\\kappa(H) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{10^6}{10^{-6}} = 10^{12}$. Thus, $\\sqrt{\\kappa(H)} = 10^6$. Since $H$ is positive definite, $x^\\top H x \\ge 0$, so $|x^\\top H x| = x^\\top H x = 2f(x)$. The perturbation simplifies to:\n$$\n\\delta(x) = \\eta \\cdot 10^6 \\cdot \\|x\\|_2 \\cdot f(x)\n$$\nAnd the perturbed function is:\n$$\n\\tilde{f}(x) = f(x) \\left(1 + \\eta \\cdot 10^6 \\cdot \\|x\\|_2\\right)\n$$\n\nFor each test case, specified by $(x_k, \\Delta_k, \\eta)$, we compute the three steps and their corresponding $\\rho_k$ values.\n\n**1. The Cauchy Step ($s_C$)**\nThe Cauchy step is the minimizer of the quadratic model $m_k(s)$ along the steepest descent direction, $d = -g_k$, constrained by the trust region $\\|s\\|_2 \\le \\Delta_k$.\nThe unconstrained minimizer along this direction is $s_{UCP} = \\tau^* d$, where $\\tau^* = \\frac{-g_k^\\top d}{d^\\top H d} = \\frac{g_k^\\top g_k}{g_k^\\top H g_k}$.\n- If $\\|s_{UCP}\\|_2 \\le \\Delta_k$, the Cauchy step is $s_C = s_{UCP}$.\n- Otherwise, the step is truncated to the trust-region boundary: $s_C = \\Delta_k \\frac{d}{\\|d\\|_2} = -\\frac{\\Delta_k}{\\|g_k\\|_2} g_k$.\n\n**2. The Dogleg Step ($s_{DL}$)**\nThe dogleg step provides an approximation to the exact TR solution by interpolating between the Cauchy step and the Newton step. The Newton step, $s_N$, is the unconstrained minimizer of $m_k(s)$, given by $s_N = -H^{-1}g_k$. For this problem, $s_N = -H^{-1}(Hx_k) = -x_k$.\nThe dogleg path connects the origin, the unconstrained Cauchy point $s_{UCP}$, and the Newton point $s_N$. The step $s_{DL}$ is the point on this path that is farthest from the origin within the trust region.\n- **Case (a):** If $\\|s_N\\|_2 \\le \\Delta_k$, the Newton step is feasible and optimal. $s_{DL} = s_N$.\n- **Case (b):** If $\\|s_N\\|_2 > \\Delta_k$ and $\\|s_{UCP}\\|_2 \\ge \\Delta_k$, the dogleg path intersects the trust-region boundary along the steepest descent direction. $s_{DL}$ is the truncated Cauchy step, $s_{DL} = -\\frac{\\Delta_k}{\\|g_k\\|_2} g_k$.\n- **Case (c):** If $\\|s_N\\|_2 > \\Delta_k$ and $\\|s_{UCP}\\|_2  \\Delta_k$, the step $s_{DL}$ lies on the line segment connecting $s_{UCP}$ and $s_N$. We find $s_{DL} = s_{UCP} + \\beta(s_N - s_{UCP})$ by solving $\\|s_{UCP} + \\beta(s_N - s_{UCP})\\|_2^2 = \\Delta_k^2$ for $\\beta \\in (0,1]$. This is a quadratic equation in $\\beta$, and since $\\|s_{UCP}\\|_2  \\Delta_k  \\|s_N\\|_2$, a unique solution for $\\beta$ in this range is guaranteed.\n\n**3. The Exact Trust-Region Step ($s_{TR}$)**\nThe exact solution to the TR subproblem $\\min_{\\|s\\|_2 \\le \\Delta_k} m_k(s)$ is characterized by the optimality conditions.\n- If the Newton step $s_N = -x_k$ is feasible (i.e., $\\|s_N\\|_2 \\le \\Delta_k$), then it is the optimal solution, so $s_{TR} = s_N$. This corresponds to a Lagrange multiplier $\\lambda = 0$.\n- If $\\|s_N\\|_2 > \\Delta_k$, the solution lies on the boundary, $\\|s_{TR}\\|_2 = \\Delta_k$. The step is given by $s_{TR}(\\lambda) = -(H + \\lambda I)^{-1} g_k$ for a unique $\\lambda > 0$ that satisfies $\\|s_{TR}(\\lambda)\\|_2 = \\Delta_k$. This requires solving the nonlinear secular equation for $\\lambda$:\n$$\n\\left\\| -(H + \\lambda I)^{-1} g_k \\right\\|_2 = \\Delta_k\n$$\nSince $H$ is diagonal, $H = \\operatorname{diag}(h_1, h_2, h_3)$, the equation becomes:\n$$\n\\sqrt{\\sum_{i=1}^3 \\left(\\frac{(g_k)_i}{h_i + \\lambda}\\right)^2} = \\Delta_k\n$$\nThis one-dimensional root-finding problem for $\\lambda$ can be solved efficiently using numerical methods like Brent's method. The function $\\phi(\\lambda) = \\|s_{TR}(\\lambda)\\|_2$ is monotonically decreasing for $\\lambda \\ge 0$, which ensures a unique solution.\n\nThe implementation will follow these steps for each test case provided.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Computes the model vs. actual reduction ratio (rho) for three types of trust-region steps\n    on an ill-conditioned quadratic function for a suite of test cases.\n    \"\"\"\n    # Define problem constants based on the problem statement\n    H = np.diag([1e6, 1.0, 1e-6])\n    # The condition number kappa(H) = max(eig)/min(eig) = 1e6 / 1e-6 = 1e12\n    sqrt_kappa_H = np.sqrt(1e12)  # This is 1e6\n\n    # Define the test cases\n    test_cases = [\n        {'x_k': np.array([1e-3, -1e-3, 2e-3]), 'delta_k': 1e-1, 'eta': 1e-16},\n        {'x_k': np.array([1e-3, -1e-3, 2e-3]), 'delta_k': 1e-4, 'eta': 1e-16},\n        {'x_k': np.array([2e-1, -1e-1, 5e-2]), 'delta_k': 2.2e-1, 'eta': 1e-16},\n        {'x_k': np.array([0.0, 0.0, 1e3]), 'delta_k': 1e-2, 'eta': 1e-16},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        x_k = case['x_k']\n        delta_k = case['delta_k']\n        eta = case['eta']\n        \n        # Gradient is g_k = Hx_k + b. With b=0, g_k = Hx_k.\n        g_k = H @ x_k\n        norm_g_k = np.linalg.norm(g_k, 2)\n\n        # Helper function for perturbed objective function f_tilde(x)\n        def f_pert(x):\n            # Since H is positive definite, x.T @ H @ x is always non-negative.\n            x_H_x = x.T @ H @ x\n            factor = 1.0 + eta * sqrt_kappa_H * np.linalg.norm(x, 2)\n            return 0.5 * x_H_x * factor\n\n        # Helper function to compute the rho ratio\n        def calculate_rho(s_k):\n            # Predicted reduction: PR_k = -(g_k^T * s_k + 0.5 * s_k^T * H * s_k)\n            pred_k = -(g_k.T @ s_k + 0.5 * s_k.T @ H @ s_k)\n            # Actual reduction: AR_k = f_tilde(x_k) - f_tilde(x_k + s_k)\n            ared_k = f_pert(x_k) - f_pert(x_k + s_k)\n            \n            if np.abs(pred_k)  1e-30:\n                return 1.0 if np.abs(ared_k)  1e-30 else np.sign(ared_k) * np.inf\n            return ared_k / pred_k\n\n        case_rhos = []\n\n        # --- Step 1: Cauchy Step ---\n        g_k_norm_sq = g_k.T @ g_k\n        g_k_H_g_k = g_k.T @ H @ g_k\n        \n        # Tau for unconstrained Cauchy step\n        tau_star = g_k_norm_sq / g_k_H_g_k if g_k_H_g_k > 0 else np.inf\n        s_UCP = -tau_star * g_k\n        norm_s_UCP = np.linalg.norm(s_UCP, 2)\n\n        if norm_s_UCP = delta_k:\n            s_C = s_UCP\n        else:\n            s_C = -(delta_k / norm_g_k) * g_k\n        case_rhos.append(calculate_rho(s_C))\n        \n        # --- Step 2: Dogleg Step ---\n        # Newton step s_N = -H^-1 * g_k. Since g_k = Hx_k, s_N = -x_k.\n        s_N = -x_k\n        norm_s_N = np.linalg.norm(s_N, 2)\n\n        s_DL = None\n        if norm_s_N = delta_k:\n            s_DL = s_N\n        elif norm_s_UCP = delta_k:\n            s_DL = -(delta_k / norm_g_k) * g_k # Same as truncated Cauchy\n        else:\n            # Intersection of dogleg path with trust-region boundary\n            p_U = s_UCP\n            p_N = s_N\n            d = p_N - p_U\n            # Solve ||p_U + beta*d||^2 = delta_k^2 for beta\n            a = d.T @ d\n            b = 2 * (p_U.T @ d)\n            c = p_U.T @ p_U - delta_k**2\n            # Product of roots is c/a  0, so one positive, one negative. We need the positive one.\n            beta = (-b + np.sqrt(b**2 - 4 * a * c)) / (2 * a)\n            s_DL = p_U + beta * d\n        case_rhos.append(calculate_rho(s_DL))\n\n        # --- Step 3: Exact TR Step ---\n        s_TR = None\n        if norm_s_N = delta_k:\n            s_TR = s_N\n        else:\n            H_diag = np.diag(H)\n            def secular_eq(lam):\n                s_lam_components = g_k / (H_diag + lam)\n                return np.linalg.norm(s_lam_components, 2) - delta_k\n\n            # Find a bracketing interval [lam_low, lam_high] for the root finder\n            lam_low = 0.0\n            if secular_eq(1e-9)  0: # Handle cases where lambda is very small\n                 lam_high = 1e-9\n            else:\n                 lam_high = 1.0\n                 while secular_eq(lam_high)  0:\n                     lam_high *= 2.0\n            \n            sol = root_scalar(secular_eq, bracket=[lam_low, lam_high], method='brentq')\n            lam_star = sol.root\n            s_TR = -np.linalg.inv(H + lam_star * np.identity(3)) @ g_k\n        case_rhos.append(calculate_rho(s_TR))\n\n        all_results.append(case_rhos)\n    \n    # Format the final output string exactly as requested.\n    # The default str() for a list of lists matches the problem's example format: [ [r1, r2], [r3, r4] ]\n    print(str(all_results))\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world objective functions are often not perfectly smooth, containing 'kinks' or discontinuities that can violate the assumptions of our quadratic models. This exercise explores such a scenario, where a step crosses a discontinuity in the function's second derivative, leading to a breakdown in the model's predictive power. You will analyze the resulting, and perhaps surprising, value of $\\rho_k$ and use this insight to devise a more robust strategy for updating the trust-region radius, moving beyond naive heuristics .",
            "id": "3152655",
            "problem": "Consider a trust-region method for unconstrained optimization with model $m_k(s) = f(x_k) + \\nabla f(x_k)^\\top s + \\tfrac{1}{2} s^\\top B_k s$, where $B_k$ approximates $\\nabla^2 f(x_k)$. The ratio test uses the actual reduction and the model-predicted reduction, defined by\n$$\\mathrm{AR}_k = f(x_k) - f(x_k + s_k), \\quad \\mathrm{PR}_k = m_k(0) - m_k(s_k) = -\\nabla f(x_k)^\\top s_k - \\tfrac{1}{2} s_k^\\top B_k s_k,$$\nand the ratio\n$$\\rho_k = \\frac{\\mathrm{AR}_k}{\\mathrm{PR}_k}.$$\nSuppose the objective $f:\\mathbb{R}^2 \\to \\mathbb{R}$ is piecewise cubic with a kink along $x_1 = 0$ and a jump in the Hessian across this kink:\n$$\nf(x) = \\begin{cases}\n\\tfrac{1}{3}\\alpha x_1^3 + \\tfrac{1}{2}\\beta_+ x_1^2 + \\gamma x_1 x_2 + \\tfrac{1}{2}\\delta x_2^2,  x_1 \\ge 0, \\\\\n\\tfrac{1}{3}\\alpha x_1^3 + \\tfrac{1}{2}\\beta_- x_1^2 + \\gamma x_1 x_2 + \\tfrac{1}{2}\\delta x_2^2,  x_1  0,\n\\end{cases}\n$$\nwith parameters $\\alpha = 3$, $\\beta_+ = 4$, $\\beta_- = 1$, $\\gamma = -3$, and $\\delta = 2$. Let the current iterate be $x_k = (0.05,\\, 0.2)$ and the trust-region radius be $\\Delta_k = 1.0$. The algorithm returns a step $s_k = (-0.50,\\, -0.73)$, which crosses the kink (since $x_{k,1} + s_{k,1} = -0.45  0$). Take $B_k = \\nabla^2 f(x_k)$ and the acceptance thresholds $\\eta_1 = 0.1$ and $\\eta_2 = 0.75$.\n\nTasks:\n- Compute $\\nabla f(x_k)$ and $B_k$ using the definition of $f$ on the side $x_1 \\ge 0$.\n- Compute $\\mathrm{PR}_k$ and $\\mathrm{AR}_k$.\n- Compute $\\rho_k$ and interpret it using the ratio test.\n- Design an update for the trust-region radius $\\Delta_{k+1}$ that avoids repeated crossings of the kink $x_1 = 0$ after accepting the step $s_k$.\n\nWhich option correctly reports the approximate value of $\\rho_k$ and prescribes a radius update that avoids repeated crossing of $x_1 = 0$?\n\nA. $\\rho_k \\approx 0.24$. Reject the step and set $\\Delta_{k+1} = 0.5\\,\\Delta_k$.\n\nB. $\\rho_k \\approx 1.2$. Accept the step and set $\\Delta_{k+1} = 2\\,\\Delta_k$.\n\nC. $\\rho_k \\approx 40$. Accept the step and set $\\Delta_{k+1} := \\min\\{1.5\\,\\Delta_k,\\, 0.8\\,|x_{k+1,1}|\\}$ to avoid repeated crossing of $x_1 = 0$.\n\nD. $\\rho_k \\approx 40$. Accept the step and set $\\Delta_{k+1} = 2\\,\\Delta_k$ because high $\\rho_k$ always warrants expansion.",
            "solution": "The problem statement is a valid exercise in numerical optimization. It presents a well-defined, self-contained scenario designed to test the behavior of a trust-region algorithm on a nonsmooth objective function. All parameters and conditions are clearly specified, and the question is objective and mathematically formalizable.\n\nWe will proceed with the tasks as outlined.\n\nThe objective function $f(x)$ is given by:\n$$\nf(x) = \\begin{cases}\n\\tfrac{1}{3}\\alpha x_1^3 + \\tfrac{1}{2}\\beta_+ x_1^2 + \\gamma x_1 x_2 + \\tfrac{1}{2}\\delta x_2^2,  x_1 \\ge 0 \\\\\n\\tfrac{1}{3}\\alpha x_1^3 + \\tfrac{1}{2}\\beta_- x_1^2 + \\gamma x_1 x_2 + \\tfrac{1}{2}\\delta x_2^2,  x_1  0\n\\end{cases}\n$$\nWith parameters $\\alpha = 3$, $\\beta_+ = 4$, $\\beta_- = 1$, $\\gamma = -3$, and $\\delta = 2$, this becomes:\n$$\nf(x) = \\begin{cases}\nx_1^3 + 2 x_1^2 - 3 x_1 x_2 + x_2^2,  x_1 \\ge 0 \\\\\nx_1^3 + \\tfrac{1}{2} x_1^2 - 3 x_1 x_2 + x_2^2,  x_1  0\n\\end{cases}\n$$\nThe current iterate is $x_k = (0.05, 0.2)$. The proposed step is $s_k = (-0.50, -0.73)$.\nThe next point would be $x_{k+1} = x_k + s_k = (0.05 - 0.50, 0.2 - 0.73) = (-0.45, -0.53)$.\n\n**1. Compute $\\nabla f(x_k)$ and $B_k$.**\n\nSince $x_{k,1} = 0.05 \\ge 0$, we use the definition of $f(x)$ for $x_1 \\ge 0$ to compute the gradient and Hessian at $x_k$.\nFor $x_1 > 0$, the gradient is:\n$$ \\nabla f(x) = \\begin{pmatrix} 3x_1^2 + 4x_1 - 3x_2 \\\\ -3x_1 + 2x_2 \\end{pmatrix} $$\nEvaluating at $x_k = (0.05, 0.2)$:\n$$ \\nabla f(x_k) = \\begin{pmatrix} 3(0.05)^2 + 4(0.05) - 3(0.2) \\\\ -3(0.05) + 2(0.2) \\end{pmatrix} = \\begin{pmatrix} 3(0.0025) + 0.2 - 0.6 \\\\ -0.15 + 0.4 \\end{pmatrix} = \\begin{pmatrix} 0.0075 - 0.4 \\\\ 0.25 \\end{pmatrix} = \\begin{pmatrix} -0.3925 \\\\ 0.25 \\end{pmatrix} $$\nThe model Hessian $B_k$ is the true Hessian $\\nabla^2 f(x_k)$. For $x_1 > 0$, the Hessian is:\n$$ \\nabla^2 f(x) = \\begin{pmatrix} 6x_1 + 4  -3 \\\\ -3  2 \\end{pmatrix} $$\nEvaluating at $x_k = (0.05, 0.2)$:\n$$ B_k = \\nabla^2 f(x_k) = \\begin{pmatrix} 6(0.05) + 4  -3 \\\\ -3  2 \\end{pmatrix} = \\begin{pmatrix} 0.3 + 4  -3 \\\\ -3  2 \\end{pmatrix} = \\begin{pmatrix} 4.3  -3 \\\\ -3  2 \\end{pmatrix} $$\n\n**2. Compute $\\mathrm{PR}_k$ and $\\mathrm{AR}_k$.**\n\nThe predicted reduction is $\\mathrm{PR}_k = -\\nabla f(x_k)^\\top s_k - \\tfrac{1}{2} s_k^\\top B_k s_k$.\nThe first term is:\n$$ -\\nabla f(x_k)^\\top s_k = - \\begin{pmatrix} -0.3925  0.25 \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} = - ((-0.3925)(-0.50) + (0.25)(-0.73)) = - (0.19625 - 0.1825) = -0.01375 $$\nThe second term involves the quadratic form $s_k^\\top B_k s_k$:\n$$ s_k^\\top B_k s_k = \\begin{pmatrix} -0.50  -0.73 \\end{pmatrix} \\begin{pmatrix} 4.3  -3 \\\\ -3  2 \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} (-0.50)(4.3) + (-0.73)(-3)  (-0.50)(-3) + (-0.73)(2) \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} -2.15 + 2.19  1.5 - 1.46 \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} = \\begin{pmatrix} 0.04  0.04 \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} $$\n$$ = (0.04)(-0.50) + (0.04)(-0.73) = -0.02 - 0.0292 = -0.0492 $$\nSo, the second term of $\\mathrm{PR}_k$ is $-\\tfrac{1}{2} s_k^\\top B_k s_k = -\\tfrac{1}{2}(-0.0492) = 0.0246$.\nThus, the predicted reduction is:\n$$ \\mathrm{PR}_k = -0.01375 + 0.0246 = 0.01085 $$\nThe actual reduction is $\\mathrm{AR}_k = f(x_k) - f(x_k + s_k)$.\nFirst, we compute $f(x_k) = f(0.05, 0.2)$ using the formula for $x_1 \\ge 0$:\n$$ f(0.05, 0.2) = (0.05)^3 + 2(0.05)^2 - 3(0.05)(0.2) + (0.2)^2 = 0.000125 + 2(0.0025) - 0.03 + 0.04 = 0.015125 $$\nNext, we compute $f(x_k + s_k) = f(-0.45, -0.53)$ using the formula for $x_1  0$:\n$$ f(-0.45, -0.53) = (-0.45)^3 + \\tfrac{1}{2}(-0.45)^2 - 3(-0.45)(-0.53) + (-0.53)^2 $$\n$$ = -0.091125 + \\tfrac{1}{2}(0.2025) - 3(0.2385) + 0.2809 $$\n$$ = -0.091125 + 0.10125 - 0.7155 + 0.2809 = 0.010125 - 0.4346 = -0.424475 $$\nThe actual reduction is:\n$$ \\mathrm{AR}_k = 0.015125 - (-0.424475) = 0.4396 $$\n\n**3. Compute $\\rho_k$ and interpret it.**\n\nThe ratio is:\n$$ \\rho_k = \\frac{\\mathrm{AR}_k}{\\mathrm{PR}_k} = \\frac{0.4396}{0.01085} \\approx 40.516 $$\nThis value is approximately $40$.\nThe acceptance thresholds are $\\eta_1 = 0.1$ and $\\eta_2 = 0.75$. Since $\\rho_k \\approx 40 > \\eta_2$, the step is considered very successful and should be accepted: $x_{k+1} = x_k + s_k$.\n\n**4. Design an update for the trust-region radius $\\Delta_{k+1}$.**\n\nThe extremely large value of $\\rho_k$ indicates a severe mismatch between the model $m_k(s)$ and the true function $f(x)$ along the step $s_k$. This mismatch occurred because the model was built at $x_k$ (in the $x_1 > 0$ region) while the step crossed into the $x_1  0$ region, where the function has a different definition (and a different Hessian). A standard trust-region algorithm might expand the radius (e.g., $\\Delta_{k+1} = 2\\Delta_k$) due to the very successful step. However, this would be a naive response. A large trust region at the new point $x_{k+1}$ (where $x_{k+1,1} = -0.45  0$) could easily result in a new step that crosses the kink back into the $x_1 > 0$ region, leading to another poor model prediction and potential oscillations around the kink.\n\nA more sophisticated strategy is to accept the step but then choose a trust-region radius for the next iteration that is small enough to prevent immediately crossing back over the kink. This allows the algorithm to build a new model, $m_{k+1}(s)$, that is accurate for the local region around $x_{k+1}$. A reasonable heuristic is to set the new radius $\\Delta_{k+1}$ to be no larger than the distance from the new point to the kink, with a safety factor. The distance from $x_{k+1}$ to the line $x_1 = 0$ is $|x_{k+1,1}| = |-0.45| = 0.45$. So, we should require $\\Delta_{k+1} \\le c |x_{k+1,1}|$ for some $c  1$ (e.g., $c = 0.8$).\n\nNow we evaluate the options based on these findings.\n\n**Option A: $\\rho_k \\approx 0.24$. Reject the step and set $\\Delta_{k+1} = 0.5\\,\\Delta_k$.**\nThe value of $\\rho_k \\approx 0.24$ is incorrect. Our calculation gives $\\rho_k \\approx 40$. Since $\\rho_k > \\eta_1$, the step should be accepted, not rejected.\n**Verdict: Incorrect.**\n\n**Option B: $\\rho_k \\approx 1.2$. Accept the step and set $\\Delta_{k+1} = 2\\,\\Delta_k$.**\nThe value of $\\rho_k \\approx 1.2$ is incorrect. Our calculation gives $\\rho_k \\approx 40$.\n**Verdict: Incorrect.**\n\n**Option C: $\\rho_k \\approx 40$. Accept the step and set $\\Delta_{k+1} := \\min\\{1.5\\,\\Delta_k,\\, 0.8\\,|x_{k+1,1}|\\}$ to avoid repeated crossing of $x_1 = 0$.**\nThe value $\\rho_k \\approx 40$ matches our calculation. `Accept the step` is the correct action as $\\rho_k > \\eta_2$. The proposed radius update, $\\Delta_{k+1} := \\min\\{1.5\\,\\Delta_k,\\, 0.8\\,|x_{k+1,1}|\\}$, implements the sophisticated heuristic described above. It combines a standard radius expansion for a successful step ($1.5\\,\\Delta_k = 1.5$) with a constraint that prevents crossing the kink ($0.8\\,|x_{k+1,1}| = 0.8 \\cdot 0.45 = 0.36$). The new radius would be $\\Delta_{k+1} = \\min\\{1.5, 0.36\\} = 0.36$. This is an excellent algorithmic choice for this specific problem.\n**Verdict: Correct.**\n\n**Option D: $\\rho_k \\approx 40$. Accept the step and set $\\Delta_{k+1} = 2\\,\\Delta_k$ because high $\\rho_k$ always warrants expansion.**\nThe value $\\rho_k \\approx 40$ and the action `Accept the step` are correct. However, the reasoning and the proposed radius update are flawed. A very high $\\rho_k$ signals a very poor model, and blindly expanding the trust region is likely to perpetuate the problem of crossing the discontinuity. The justification that high $\\rho_k$ `always` warrants expansion is a false and naive heuristic. Option C provides a much more robust strategy.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}