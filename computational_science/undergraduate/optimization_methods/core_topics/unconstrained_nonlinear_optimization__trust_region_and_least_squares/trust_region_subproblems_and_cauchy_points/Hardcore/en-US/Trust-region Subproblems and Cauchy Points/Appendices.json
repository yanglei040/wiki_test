{
    "hands_on_practices": [
        {
            "introduction": "The Cauchy point is valued for its simplicity and its role in guaranteeing the convergence of trust-region algorithms. This practice explores the special condition where this simple step is not just adequate, but is in fact the exact solution to the trust-region subproblem. By analyzing a case where the gradient aligns with an eigenvector of the Hessian, you will see this optimality firsthand and then observe how quickly it vanishes with a small perturbation, highlighting why more sophisticated methods are generally necessary .",
            "id": "3194326",
            "problem": "Consider the trust-region subproblem for a twice continuously differentiable function with quadratic model defined at a current iterate:\n$$m(p) = g^{\\mathsf{T}} p + \\frac{1}{2} p^{\\mathsf{T}} B p,$$\nsubject to the constraint\n$$\\|p\\| \\leq \\Delta,$$\nwhere $B$ is symmetric positive definite, $g$ is the gradient, and $\\Delta0$ is the trust-region radius. The global solution $p^{\\star}$ satisfies the first-order conditions derived from the Karush-Kuhn-Tucker (KKT) conditions, and the Cauchy point $p_{\\mathrm{C}}$ is the minimizer of the quadratic model restricted to the line segment $\\{-t g : t \\geq 0, \\|t g\\| \\leq \\Delta\\}$.\n\nYou will analyze a concrete instance and a perturbation.\n\n1. Construct the case with\n$$B = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix}, \\quad g = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad \\Delta = 3.$$\nUsing only fundamental definitions and first-order optimality conditions, determine the global minimizer $p^{\\star}$ and the Cauchy point $p_{\\mathrm{C}}$, and verify that the Cauchy point is optimal for this trust-region subproblem.\n\n2. Now perturb the gradient slightly to\n$$\\tilde{g} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix},$$\nkeeping the same $B$ and $\\Delta$. Using the same principles, determine the global minimizer $\\tilde{p}^{\\star}$ and the Cauchy point $\\tilde{p}_{\\mathrm{C}}$, and compute the exact difference in the model values\n$$m(\\tilde{p}_{\\mathrm{C}}) - m(\\tilde{p}^{\\star}).$$\nExpress your final answer as an exact fraction. Do not round.",
            "solution": "The problem asks for an analysis of a trust-region subproblem under two different scenarios for the gradient vector. The quadratic model is given by $m(p) = g^{\\mathsf{T}} p + \\frac{1}{2} p^{\\mathsf{T}} B p$, subject to the constraint $\\|p\\| \\leq \\Delta$, where $\\|\\cdot\\|$ is the Euclidean norm.\n\nThe global minimizer $p^{\\star}$ of this problem is characterized by the Karush-Kuhn-Tucker (KKT) conditions. The Lagrangian is $\\mathcal{L}(p, \\lambda) = m(p) + \\frac{\\lambda}{2} (p^{\\mathsf{T}}p - \\Delta^2)$. The first-order optimality conditions are:\n$1$. $\\nabla_p \\mathcal{L}(p, \\lambda) = g + Bp + \\lambda p = 0 \\implies (B + \\lambda I)p = -g$\n$2$. $\\lambda \\geq 0$\n$3$. $\\|p\\| \\leq \\Delta$\n$4$. $\\lambda (\\|p\\|^2 - \\Delta^2) = 0$ (complementary slackness)\n$5$. $B + \\lambda I$ must be positive semi-definite. Since $B$ is given as positive definite, this is satisfied for any $\\lambda \\geq 0$.\n\nIf the unconstrained minimizer $p_U = -B^{-1}g$ satisfies $\\|p_U\\|  \\Delta$, then by setting $\\lambda=0$, all KKT conditions are met, and $p^{\\star} = p_U$. Otherwise, the solution lies on the boundary, i.e., $\\|p^{\\star}\\| = \\Delta$, which implies $\\lambda  0$.\n\nThe Cauchy point $p_{\\mathrm{C}}$ is the minimizer of the model $m(p)$ along the steepest descent direction, restricted to the trust region. That is, we seek to minimize $\\phi(t) = m(-tg)$ for $t \\geq 0$ such that $\\|-tg\\| \\leq \\Delta$. The path is $p(t) = -tg$. The constraint becomes $t\\|g\\| \\leq \\Delta$, so $0 \\leq t \\leq \\Delta/\\|g\\|$.\nSubstituting $p(t)$ into the model:\n$$ \\phi(t) = g^{\\mathsf{T}}(-tg) + \\frac{1}{2}(-tg)^{\\mathsf{T}}B(-tg) = -t\\|g\\|^2 + \\frac{t^2}{2}g^{\\mathsf{T}}Bg $$\nThis is a quadratic in $t$. The unconstrained minimizer is found by setting $\\phi'(t) = -\\|g\\|^2 + t(g^{\\mathsf{T}}Bg) = 0$, which yields $t^* = \\frac{\\|g\\|^2}{g^{\\mathsf{T}}Bg}$.\nSince $B$ is positive definite, $g^{\\mathsf{T}}Bg  0$, so $\\phi(t)$ is a strictly convex function. The minimizer over the interval $[0, \\Delta/\\|g\\|]$ is $t_{\\mathrm{C}} = \\min(t^*, \\Delta/\\|g\\|)$. The Cauchy point is then $p_{\\mathrm{C}} = -t_{\\mathrm{C}}g$.\n\n**Part 1: Analysis of the initial case**\n\nThe given parameters are:\n$$ B = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix}, \\quad g = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad \\Delta = 3 $$\nFirst, we find the global minimizer $p^{\\star}$. We compute the unconstrained minimizer $p_U$. The inverse of $B$ is $B^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  1/4 \\end{pmatrix}$.\n$$ p_U = -B^{-1}g = -\\begin{pmatrix} 1  0 \\\\ 0  1/4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} $$\nWe check its norm: $\\|p_U\\| = \\sqrt{(-2)^2 + 0^2} = 2$.\nSince $\\|p_U\\| = 2  \\Delta = 3$, the unconstrained minimizer is feasible. Therefore, the global minimizer is $p^{\\star} = p_U = \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix}$.\n\nNext, we find the Cauchy point $p_{\\mathrm{C}}$. We minimize $\\phi(t)$ over $t \\in [0, \\Delta/\\|g\\|]$.\nFirst, compute the necessary quantities:\n$$ \\|g\\|^2 = 2^2 + 0^2 = 4 $$\n$$ g^{\\mathsf{T}}Bg = \\begin{pmatrix} 2  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = 4 $$\nThe unconstrained minimizer for $t$ is $t^* = \\frac{\\|g\\|^2}{g^{\\mathsf{T}}Bg} = \\frac{4}{4} = 1$.\nThe feasible interval for $t$ is $[0, \\Delta/\\|g\\|] = [0, 3/2]$. Since $t^*=1$ is within this interval, we have $t_{\\mathrm{C}} = 1$.\nThe Cauchy point is $p_{\\mathrm{C}} = -t_{\\mathrm{C}}g = -1 \\cdot \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix}$.\n\nComparing the results, we find that $p_{\\mathrm{C}} = p^{\\star} = \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix}$. This verifies that the Cauchy point is optimal for this specific subproblem instance. This occurs because the gradient $g$ is an eigenvector of the Hessian $B$.\n\n**Part 2: Analysis of the perturbed case**\n\nThe parameters are now:\n$$ B = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix}, \\quad \\tilde{g} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}, \\quad \\Delta = 3 $$\nFirst, we find the new global minimizer $\\tilde{p}^{\\star}$. We start with the unconstrained minimizer $\\tilde{p}_U$:\n$$ \\tilde{p}_U = -B^{-1}\\tilde{g} = -\\begin{pmatrix} 1  0 \\\\ 0  1/4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -1/4 \\end{pmatrix} $$\nWe check its norm: $\\|\\tilde{p}_U\\| = \\sqrt{(-2)^2 + (-1/4)^2} = \\sqrt{4 + 1/16} = \\sqrt{65/16} = \\frac{\\sqrt{65}}{4}$.\nSince $8^2 = 64$ and $9^2=81$, we have $8  \\sqrt{65}  9$. Thus, $2  \\frac{\\sqrt{65}}{4}  \\frac{9}{4} = 2.25$.\nAs $\\|\\tilde{p}_U\\| \\approx 2.015  \\Delta = 3$, the unconstrained minimizer is feasible. The global minimizer is $\\tilde{p}^{\\star} = \\tilde{p}_U = \\begin{pmatrix} -2 \\\\ -1/4 \\end{pmatrix}$.\n\nNext, we calculate the model value at this point, $m(\\tilde{p}^{\\star})$:\n$$ m(\\tilde{p}^{\\star}) = \\tilde{g}^{\\mathsf{T}}\\tilde{p}^{\\star} + \\frac{1}{2}(\\tilde{p}^{\\star})^{\\mathsf{T}}B\\tilde{p}^{\\star} $$\n$$ \\tilde{g}^{\\mathsf{T}}\\tilde{p}^{\\star} = \\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ -1/4 \\end{pmatrix} = -4 - \\frac{1}{4} = -\\frac{17}{4} $$\n$$ (\\tilde{p}^{\\star})^{\\mathsf{T}}B\\tilde{p}^{\\star} = \\begin{pmatrix} -2  -1/4 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ -1/4 \\end{pmatrix} = \\begin{pmatrix} -2  -1/4 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix} = 4 + \\frac{1}{4} = \\frac{17}{4} $$\n$$ m(\\tilde{p}^{\\star}) = -\\frac{17}{4} + \\frac{1}{2}\\left(\\frac{17}{4}\\right) = -\\frac{17}{8} $$\n\nNow, we find the new Cauchy point $\\tilde{p}_{\\mathrm{C}}$. We compute the necessary quantities for minimizing $\\phi(t) = m(-t\\tilde{g})$:\n$$ \\|\\tilde{g}\\|^2 = 2^2 + 1^2 = 5 $$\n$$ \\tilde{g}^{\\mathsf{T}}B\\tilde{g} = \\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix} = 4 + 4 = 8 $$\nThe unconstrained minimizer for $t$ is $t^* = \\frac{\\|\\tilde{g}\\|^2}{\\tilde{g}^{\\mathsf{T}}B\\tilde{g}} = \\frac{5}{8}$.\nThe feasible interval for $t$ is $[0, \\Delta/\\|\\tilde{g}\\|] = [0, 3/\\sqrt{5}]$.\nWe check if $t^*$ is in the interval: $\\frac{5}{8} \\leq \\frac{3}{\\sqrt{5}} \\iff 5\\sqrt{5} \\leq 24 \\iff \\sqrt{125} \\leq 24 \\iff 125 \\leq 576$, which is true.\nSo, $t_{\\mathrm{C}} = t^* = 5/8$.\nThe Cauchy point is $\\tilde{p}_{\\mathrm{C}} = -t_{\\mathrm{C}}\\tilde{g} = -\\frac{5}{8} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -5/4 \\\\ -5/8 \\end{pmatrix}$.\n\nWe calculate the model value at this point, $m(\\tilde{p}_{\\mathrm{C}})$. We can use the formula $m(\\tilde{p}_{\\mathrm{C}}) = \\phi(t_{\\mathrm{C}}) = -\\frac{1}{2}\\frac{(\\|\\tilde{g}\\|^2)^2}{\\tilde{g}^{\\mathsf{T}}B\\tilde{g}}$ (assuming $m(0)=0$):\n$$ m(\\tilde{p}_{\\mathrm{C}}) = -\\frac{1}{2} \\frac{5^2}{8} = -\\frac{25}{16} $$\n\nFinally, we compute the difference in the model values:\n$$ m(\\tilde{p}_{\\mathrm{C}}) - m(\\tilde{p}^{\\star}) = \\left(-\\frac{25}{16}\\right) - \\left(-\\frac{17}{8}\\right) = -\\frac{25}{16} + \\frac{34}{16} = \\frac{34 - 25}{16} = \\frac{9}{16} $$",
            "answer": "$$\\boxed{\\frac{9}{16}}$$"
        },
        {
            "introduction": "A key task in optimization is exploiting the geometry of the objective function, especially its curvature. This exercise demonstrates a significant limitation of the Cauchy point: its \"blindness\" to curvature information in directions orthogonal to the gradient. By constructing a problem with a strong negative curvature direction that the Cauchy step cannot \"see,\" you will analytically derive the large gap between the reduction achieved by the Cauchy point and the true optimal step, revealing why methods that explore more than just the steepest descent direction are crucial for efficiency .",
            "id": "3194285",
            "problem": "Consider the quadratic trust-region model $m(p) = g^{\\top}p + \\frac{1}{2} p^{\\top} B p$ with gradient $g \\in \\mathbb{R}^{2}$ and symmetric matrix $B \\in \\mathbb{R}^{2 \\times 2}$. The trust-region (TR) subproblem seeks to minimize $m(p)$ subject to the constraint $\\|p\\| \\leq \\Delta$, where $\\Delta  0$ is the TR radius. Let $B = \\mathrm{diag}(\\beta_{1}, -M)$ with $\\beta_{1} \\geq 0$ and $M  0$, and let $g = (\\gamma, 0)^{\\top}$ with $\\gamma \\neq 0$. Note that the negative curvature direction associated with the eigenvalue $-M$ is orthogonal to $g$. Assume $\\Delta$ is sufficiently large so that both inequalities $\\Delta \\geq \\frac{|\\gamma|}{\\beta_{1} + M}$ and $\\Delta \\geq \\frac{|\\gamma|}{\\beta_{1}}$ hold whenever the denominator is nonzero, ensuring that the minimizers derived below are feasible without truncation by the constraint.\n\nStarting only from the definitions of the trust-region model and the Cauchy point (the point obtained by minimizing $m(-\\tau g)$ over scalar step sizes $\\tau$ constrained by $\\|-\\tau g\\| \\leq \\Delta$), carry out the following:\n\n1. Derive the exact global minimizer of the TR subproblem for this $B$ and $g$, and obtain the minimized model value $m(p_{\\mathrm{TR}})$ in closed form. Your reasoning must justify why the exact minimizer lies on the boundary and how it exploits the negative curvature direction.\n2. Derive the Cauchy point $p_{\\mathrm{C}}$ along the negative gradient direction and obtain the minimized model value $m(p_{\\mathrm{C}})$ in closed form, making clear why the negative curvature direction does not affect the Cauchy point in this setup.\n3. Let $R_{\\mathrm{TR}} := -m(p_{\\mathrm{TR}})$ and $R_{\\mathrm{C}} := -m(p_{\\mathrm{C}})$ denote the model reductions achieved by the exact TR step and the Cauchy point, respectively. Compute the multiplicative optimality gap factor \n$$F := \\frac{R_{\\mathrm{TR}}}{R_{\\mathrm{C}}}$$\nas a simplified closed-form analytic expression in terms of $\\beta_{1}$, $M$, $\\gamma$, and $\\Delta$.\n\nExpress your final answer as a single closed-form expression. No rounding is required. No physical units are involved.",
            "solution": "The quadratic model is given by $m(p) = g^{\\top}p + \\frac{1}{2} p^{\\top} B p$, with the optimization problem being $\\min_{p \\in \\mathbb{R}^2} m(p)$ subject to $\\|p\\| \\leq \\Delta$.\nThe given parameters are the gradient $g = (\\gamma, 0)^{\\top}$ with $\\gamma \\neq 0$, and the symmetric Hessian matrix $B = \\mathrm{diag}(\\beta_{1}, -M)$, where $\\beta_{1} \\geq 0$ and $M  0$. The trust-region radius is $\\Delta  0$.\nLet $p = (p_1, p_2)^{\\top}$. The model can be written explicitly as:\n$$m(p) = \\gamma p_1 + \\frac{1}{2} (\\beta_1 p_1^2 - M p_2^2)$$\nThe constraint is $p_1^2 + p_2^2 \\leq \\Delta^2$.\n\n### 1. Global Minimizer of the Trust-Region Subproblem\n\nThe solution $p_{\\mathrm{TR}}$ to the trust-region subproblem must satisfy the Karush-Kuhn-Tucker (KKT) conditions. There exists a Lagrange multiplier $\\lambda \\geq 0$ such that:\n1. $(B + \\lambda I) p_{\\mathrm{TR}} = -g$\n2. $\\lambda (\\Delta - \\|p_{\\mathrm{TR}}\\|) = 0$\n3. $B + \\lambda I$ is positive semi-definite.\n\nThe matrix $B$ has eigenvalues $\\beta_1$ and $-M$. Since $M  0$, $B$ is indefinite. For the matrix $B + \\lambda I = \\mathrm{diag}(\\beta_1 + \\lambda, -M + \\lambda)$ to be positive semi-definite, both of its diagonal entries (which are its eigenvalues) must be non-negative. As $\\beta_1 \\geq 0$ and $\\lambda \\geq 0$, the first eigenvalue $\\beta_1 + \\lambda$ is always non-negative. The second eigenvalue requires $-M + \\lambda \\geq 0$, which implies $\\lambda \\geq M$.\n\nThe model $m(p)$ contains the term $-\\frac{1}{2} M p_2^2$. Since $M  0$, for any fixed $p_1$, the model value can be made arbitrarily negative by increasing $|p_2|$. This means the quadratic function is unbounded below on $\\mathbb{R}^2$, and thus an unconstrained minimizer does not exist. Consequently, any minimizer subject to the constraint $\\|p\\| \\leq \\Delta$ must lie on the boundary, i.e., $\\|p_{\\mathrm{TR}}\\| = \\Delta$.\n\nFrom the complementary slackness condition $\\lambda (\\Delta - \\|p_{\\mathrm{TR}}\\|) = 0$, since $\\|p_{\\mathrm{TR}}\\| = \\Delta  0$, we must have $\\lambda \\geq 0$. We have already established a stronger condition, $\\lambda \\geq M  0$.\n\nLet's analyze the first KKT condition, $(B + \\lambda I) p = -g$:\n$$\n\\begin{pmatrix} \\beta_1 + \\lambda  0 \\\\ 0  -M + \\lambda \\end{pmatrix} \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix} = \\begin{pmatrix} -\\gamma \\\\ 0 \\end{pmatrix}\n$$\nThis decouples into two equations:\n(a) $(\\beta_1 + \\lambda) p_1 = -\\gamma$\n(b) $(-M + \\lambda) p_2 = 0$\n\nWe consider the possibilities for $\\lambda \\geq M$.\nCase 1: $\\lambda  M$. In this case, from equation (b), we must have $p_2=0$. Equation (a) gives $p_1 = -\\frac{\\gamma}{\\beta_1 + \\lambda}$. The solution is on the boundary, so $\\|p_{\\mathrm{TR}}\\|= |p_1| = \\Delta$, which implies $\\frac{|\\gamma|}{\\beta_1 + \\lambda} = \\Delta$, or $\\lambda = \\frac{|\\gamma|}{\\Delta} - \\beta_1$. The condition $\\lambda  M$ becomes $\\frac{|\\gamma|}{\\Delta} - \\beta_1  M$, which simplifies to $\\Delta  \\frac{|\\gamma|}{\\beta_1 + M}$. However, the problem statement assumes $\\Delta \\geq \\frac{|\\gamma|}{\\beta_1 + M}$, so this case is ruled out. This situation is characteristic of the \"hard case\" in trust-region literature.\n\nCase 2: $\\lambda = M$. This is the only remaining possibility.\nEquation (a) gives $(\\beta_1 + M) p_1 = -\\gamma$, so $p_1 = -\\frac{\\gamma}{\\beta_1 + M}$. Since $\\beta_1 \\geq 0$ and $M  0$, the denominator is non-zero.\nEquation (b) becomes $(-M + M) p_2 = 0$, which is $0 \\cdot p_2 = 0$. This equation is satisfied for any value of $p_2$.\nThe solution must lie on the boundary, so $p_1^2 + p_2^2 = \\Delta^2$. We can solve for $p_2^2$:\n$$p_2^2 = \\Delta^2 - p_1^2 = \\Delta^2 - \\left(-\\frac{\\gamma}{\\beta_1 + M}\\right)^2 = \\Delta^2 - \\frac{\\gamma^2}{(\\beta_1 + M)^2}$$\nA real solution for $p_2$ exists if $\\Delta^2 - \\frac{\\gamma^2}{(\\beta_1 + M)^2} \\geq 0$, which is equivalent to $\\Delta \\geq \\frac{|\\gamma|}{\\beta_1 + M}$. This inequality is guaranteed by the problem's assumptions.\nThe solution exploits the negative curvature direction (the eigenvector $(0, 1)^{\\top}$) by having a non-zero component $p_2$. The model value is $m(p) = \\gamma p_1 + \\frac{1}{2}(\\beta_1 p_1^2 - M p_2^2)$. With $p_1$ fixed, the value is minimized by maximizing $p_2^2$, which is what we have done by enforcing the boundary condition. The sign of $p_2$ does not affect the model value. Thus, there are two global minimizers.\nWe denote one such minimizer by $p_{\\mathrm{TR}}$, with components $p_{1, \\mathrm{TR}} = -\\frac{\\gamma}{\\beta_1 + M}$ and $p_{2, \\mathrm{TR}}^2 = \\Delta^2 - \\frac{\\gamma^2}{(\\beta_1 + M)^2}$.\n\nThe minimized model value is:\n$m(p_{\\mathrm{TR}}) = \\gamma p_{1, \\mathrm{TR}} + \\frac{1}{2}(\\beta_1 p_{1, \\mathrm{TR}}^2 - M p_{2, \\mathrm{TR}}^2)$\n$m(p_{\\mathrm{TR}}) = \\gamma \\left(-\\frac{\\gamma}{\\beta_1 + M}\\right) + \\frac{1}{2}\\left[\\beta_1 \\frac{\\gamma^2}{(\\beta_1 + M)^2} - M \\left(\\Delta^2 - \\frac{\\gamma^2}{(\\beta_1 + M)^2}\\right)\\right]$\n$m(p_{\\mathrm{TR}}) = -\\frac{\\gamma^2}{\\beta_1 + M} + \\frac{1}{2}\\left[\\frac{(\\beta_1 + M)\\gamma^2}{(\\beta_1 + M)^2} - M \\Delta^2\\right]$\n$m(p_{\\mathrm{TR}}) = -\\frac{\\gamma^2}{\\beta_1 + M} + \\frac{1}{2}\\left(\\frac{\\gamma^2}{\\beta_1 + M} - M \\Delta^2\\right)$\n$$m(p_{\\mathrm{TR}}) = -\\frac{1}{2} \\frac{\\gamma^2}{\\beta_1 + M} - \\frac{1}{2} M \\Delta^2$$\n\n### 2. The Cauchy Point\n\nThe Cauchy point, $p_{\\mathrm{C}}$, is found by minimizing the model $m(p)$ along the negative gradient direction $p(\\tau) = -\\tau g$ for a step $\\tau \\geq 0$, subject to the trust-region constraint $\\|p(\\tau)\\| \\leq \\Delta$.\nHere, $p(\\tau) = -\\tau (\\gamma, 0)^{\\top} = (-\\tau\\gamma, 0)^{\\top}$. The model as a function of $\\tau$ is:\n$m(p(\\tau)) = g^{\\top}(-\\tau g) + \\frac{1}{2}(-\\tau g)^{\\top}B(-\\tau g) = -\\tau\\|g\\|^2 + \\frac{\\tau^2}{2}g^{\\top}Bg$.\nWe compute the necessary quantities:\n$\\|g\\|^2 = \\gamma^2 + 0^2 = \\gamma^2$.\n$g^{\\top}Bg = \\begin{pmatrix} \\gamma  0 \\end{pmatrix} \\begin{pmatrix} \\beta_1  0 \\\\ 0  -M \\end{pmatrix} \\begin{pmatrix} \\gamma \\\\ 0 \\end{pmatrix} = \\beta_1 \\gamma^2$.\nSo, the one-dimensional model is $m(\\tau) = -\\tau\\gamma^2 + \\frac{\\tau^2}{2}\\beta_1\\gamma^2$.\n\nThe negative curvature direction, associated with the eigenvalue $-M$, is the standard basis vector $e_2 = (0, 1)^{\\top}$. The Cauchy point search direction is $-g = (-\\gamma, 0)^{\\top}$, which is parallel to $e_1 = (1, 0)^{\\top}$. Since $e_1$ is orthogonal to $e_2$, the search for the Cauchy point is restricted to a subspace where the negative curvature has no effect. This is evident as the term $-M$ does not appear in the expression for $m(\\tau)$.\n\nTo find the optimal $\\tau$, we analyze $m(\\tau)$.\nIf $\\beta_1  0$, $m(\\tau)$ is a convex quadratic in $\\tau$. Its unconstrained minimizer is found by setting the derivative to zero: $m'(\\tau) = -\\gamma^2 + \\tau\\beta_1\\gamma^2 = \\gamma^2(\\tau\\beta_1-1) = 0$, which gives $\\tau^* = 1/\\beta_1$.\nIf $\\beta_1 = 0$, $m(\\tau) = -\\tau\\gamma^2$, which is a strictly decreasing function of $\\tau$.\nThe constraint is $\\|p(\\tau)\\| = \\|-\\tau g\\| = \\tau|\\gamma| \\leq \\Delta$, so $0 \\leq \\tau \\leq \\Delta/|\\gamma|$.\n\nThe problem states the assumption $\\Delta \\geq \\frac{|\\gamma|}{\\beta_1}$ holds \"whenever the denominator is nonzero\", i.e., for $\\beta_1  0$. This inequality is equivalent to $\\frac{\\Delta}{|\\gamma|} \\geq \\frac{1}{\\beta_1}$.\nFor $\\beta_1  0$, the Cauchy step $\\tau_{\\mathrm{C}}$ is the constrained minimizer of $m(\\tau)$, which is $\\tau_{\\mathrm{C}} = \\min(\\tau^*, \\Delta/|\\gamma|) = \\min(1/\\beta_1, \\Delta/|\\gamma|)$. Given the assumption, this yields $\\tau_{\\mathrm{C}} = 1/\\beta_1$.\nThe problem formulation thus directs us to the case where $\\beta_1  0$.\nThe Cauchy point is $p_{\\mathrm{C}} = -\\tau_{\\mathrm{C}} g = -\\frac{1}{\\beta_1}g = (-\\frac{\\gamma}{\\beta_1}, 0)^{\\top}$.\nThe model value at the Cauchy point is:\n$m(p_{\\mathrm{C}}) = m(\\tau = 1/\\beta_1) = -\\frac{1}{\\beta_1}\\gamma^2 + \\frac{1}{2}\\left(\\frac{1}{\\beta_1}\\right)^2\\beta_1\\gamma^2 = -\\frac{\\gamma^2}{\\beta_1} + \\frac{1}{2}\\frac{\\gamma^2}{\\beta_1} = -\\frac{1}{2}\\frac{\\gamma^2}{\\beta_1}$.\n\n### 3. Optimality Gap Factor\n\nThe model reduction for the TR minimizer is $R_{\\mathrm{TR}} = m(0) - m(p_{\\mathrm{TR}})$. Since $m(0)=0$, $R_{\\mathrm{TR}} = -m(p_{\\mathrm{TR}})$.\n$$R_{\\mathrm{TR}} = \\frac{1}{2} \\frac{\\gamma^2}{\\beta_1 + M} + \\frac{1}{2} M \\Delta^2$$\nThe model reduction for the Cauchy point is $R_{\\mathrm{C}} = m(0) - m(p_{\\mathrm{C}}) = -m(p_{\\mathrm{C}})$.\n$$R_{\\mathrm{C}} = \\frac{1}{2}\\frac{\\gamma^2}{\\beta_1}$$\nThe optimality gap factor is $F = \\frac{R_{\\mathrm{TR}}}{R_{\\mathrm{C}}}$.\n$$F = \\frac{\\frac{1}{2} \\frac{\\gamma^2}{\\beta_1 + M} + \\frac{1}{2} M \\Delta^2}{\\frac{1}{2}\\frac{\\gamma^2}{\\beta_1}} = \\frac{\\frac{\\gamma^2}{\\beta_1 + M} + M \\Delta^2}{\\frac{\\gamma^2}{\\beta_1}}$$\nWe can split the fraction:\n$$F = \\left(\\frac{\\gamma^2}{\\beta_1 + M}\\right) \\left(\\frac{\\beta_1}{\\gamma^2}\\right) + (M \\Delta^2) \\left(\\frac{\\beta_1}{\\gamma^2}\\right)$$\n$$F = \\frac{\\beta_1}{\\beta_1 + M} + \\frac{M \\beta_1 \\Delta^2}{\\gamma^2}$$\nThis is the final simplified closed-form expression.",
            "answer": "$$\\boxed{\\frac{\\beta_1}{\\beta_1 + M} + \\frac{M \\beta_1 \\Delta^2}{\\gamma^2}}$$"
        },
        {
            "introduction": "Having explored conceptual cases of optimality and failure, this final practice asks you to quantify the performance gap between the Cauchy point and the exact trust-region solution. By implementing a program, you will compute the predicted model reduction for both steps across a range of trust-region radii, from very small to large. This exercise provides a concrete, numerical feel for the trade-off between the computational simplicity of the Cauchy point and the superior quality of the full subproblem solution .",
            "id": "3194333",
            "problem": "Consider a quadratic model in trust-region methods defined by $$m(p)=f+g^{\\top}p+\\tfrac{1}{2}p^{\\top}Bp,$$ where $f\\in\\mathbb{R}$ is a scalar, $g\\in\\mathbb{R}^{n}$ is the gradient at the current iterate, and $B\\in\\mathbb{R}^{n\\times n}$ is a symmetric approximation to the Hessian matrix. The trust-region subproblem seeks $$\\min_{p\\in\\mathbb{R}^{n}}\\,m(p)\\quad\\text{subject to}\\quad\\lVert p\\rVert\\leq\\Delta,$$ where $\\Delta0$ is the trust-region radius and $\\lVert\\cdot\\rVert$ denotes the Euclidean norm. The predicted reduction is defined by $$\\text{predicted reduction}=m(0)-m(p).$$\n\nStarting from the fundamental definitions of the quadratic model and the trust-region subproblem, as well as the standard optimality conditions for quadratic minimization with a Euclidean ball constraint and the definition of the Cauchy point (the minimizer of the quadratic model along the steepest-descent ray restricted to the trust region), implement a program to compute and compare the predicted reduction of:\n- the exact trust-region solution (the global minimizer of the trust-region subproblem), and\n- the Cauchy point (the steepest-descent point truncated to the trust-region boundary if necessary),\n\nfor the specific two-dimensional case with $$B=\\begin{pmatrix}2  0 \\\\ 0  0.2\\end{pmatrix},\\qquad g=\\begin{pmatrix}1\\\\1\\end{pmatrix}.$$\n\nYour program must, for each trust-region radius $\\Delta$ in the test suite below, compute:\n1. The exact trust-region step $p_{\\mathrm{TR}}(\\Delta)$ and its predicted reduction $m(0)-m\\big(p_{\\mathrm{TR}}(\\Delta)\\big)$.\n2. The Cauchy point $p_{\\mathrm{C}}(\\Delta)$ and its predicted reduction $m(0)-m\\big(p_{\\mathrm{C}}(\\Delta)\\big)$.\n\nUse the following test suite for $\\Delta$:\n- $$\\Delta_{1}=0.1$$ (very small radius),\n- $$\\Delta_{2}=1.0$$ (moderate radius below the Cauchy unconstrained threshold),\n- $$\\Delta_{3}=\\frac{10}{11}\\sqrt{2}$$ (exact boundary for the Cauchy ray minimizer),\n- $$\\Delta_{4}=2.0$$ (moderate radius above the Cauchy unconstrained threshold but below the exact unconstrained minimizer norm),\n- $$\\Delta_{5}=\\sqrt{25.25}$$ (exact boundary where the unconstrained quadratic minimizer hits the trust-region boundary),\n- $$\\Delta_{6}=10.0$$ (large radius).\n\nThe final output format must be a single line containing a comma-separated list enclosed in square brackets. Each element of the list must be a sublist in the form $$[\\Delta,\\;R_{\\mathrm{exact}}(\\Delta),\\;R_{\\mathrm{Cauchy}}(\\Delta)],$$ where $R_{\\mathrm{exact}}(\\Delta)$ and $R_{\\mathrm{Cauchy}}(\\Delta)$ denote the predicted reductions of the exact trust-region solution and the Cauchy point, respectively. All numerical values must be floating-point numbers rounded to $10$ decimal places, and the output must contain no spaces. For example, the overall output should look like $$\\big[[\\Delta_{1},R_{\\mathrm{exact}}(\\Delta_{1}),R_{\\mathrm{Cauchy}}(\\Delta_{1})],[\\Delta_{2},R_{\\mathrm{exact}}(\\Delta_{2}),R_{\\mathrm{Cauchy}}(\\Delta_{2})],\\dots\\big]$$ printed as a single line.\n\nNo physical units or angle units are involved in this problem. Your implementation must adhere to the described output format exactly and be self-contained without reading input. The answer for each test case must be presented as the triple of floats in the specified sublist structure.",
            "solution": "The objective is to compute and compare the predicted reduction achieved by two different steps, the Cauchy point $p_{\\mathrm{C}}$ and the exact trust-region solution $p_{\\mathrm{TR}}$, for a given quadratic model and a set of trust-region radii $\\Delta$.\n\nThe quadratic model of the objective function is given by\n$$m(p) = f + g^{\\top}p + \\frac{1}{2}p^{\\top}Bp$$\nwith the specific two-dimensional gradient $g$ and symmetric matrix $B$:\n$$g = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 2  0 \\\\ 0  0.2 \\end{pmatrix}$$\nThe trust-region subproblem is to minimize this model subject to the constraint that the step $p$ remains within a Euclidean ball of radius $\\Delta$:\n$$\\min_{p \\in \\mathbb{R}^2} m(p) \\quad \\text{subject to} \\quad \\|p\\|_2 \\leq \\Delta$$\nThe predicted reduction from a step $p$ is defined as the decrease in the model's value from the origin:\n$$\\text{pred}(p) = m(0) - m(p) = (f) - (f + g^{\\top}p + \\frac{1}{2}p^{\\top}Bp) = -g^{\\top}p - \\frac{1}{2}p^{\\top}Bp$$\nThis expression is independent of the value $f$.\n\nFirst, we analyze the Cauchy point, $p_{\\mathrm{C}}$. This point is defined as the minimizer of the quadratic model $m(p)$ along the steepest-descent direction, $d = -g$, restricted to the trust region. A step along this direction can be written as $p(t) = -tg$ for some scalar $t  0$. The constraint $\\|p(t)\\| \\le \\Delta$ becomes $t\\|g\\| \\le \\Delta$, which implies $t \\in [0, \\Delta/\\|g\\|]$.\nWe substitute $p(t) = -tg$ into the model to obtain a one-dimensional quadratic function of $t$:\n$$\\phi(t) = m(-tg) = f - tg^{\\top}g + \\frac{1}{2}t^2g^{\\top}Bg$$\nTo find the minimizer of $\\phi(t)$ for $t  0$, we examine its derivative:\n$$\\phi'(t) = -g^{\\top}g + t(g^{\\top}Bg)$$\nIf $g^{\\top}Bg  0$, the unconstrained minimizer of $\\phi(t)$ is found by setting $\\phi'(t) = 0$, which yields $t_{\\mathrm{unc}} = \\frac{g^{\\top}g}{g^{\\top}Bg}$. The Cauchy point is then found by projecting this unconstrained minimizer onto the feasible interval $[0, \\Delta/\\|g\\|]$. The step length is $t_{\\mathrm{C}} = \\min(t_{\\mathrm{unc}}, \\frac{\\Delta}{\\|g\\|})$. The Cauchy point is thus $p_{\\mathrm{C}} = -t_{\\mathrm{C}}g$.\nFor the given problem, we compute:\n$$\\|g\\|^2 = g^{\\top}g = 1^2 + 1^2 = 2$$\n$$g^{\\top}Bg = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  0.2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2 + 0.2 = 2.2$$\nSince $g^{\\top}Bg = 2.2  0$, the unconstrained minimizer exists with $t_{\\mathrm{unc}} = \\frac{2}{2.2} = \\frac{10}{11}$.\nThe corresponding unconstrained Cauchy step is $p_{\\mathrm{unc,C}} = -\\frac{10}{11}g$. Its norm is $\\|p_{\\mathrm{unc,C}}\\| = \\frac{10}{11}\\|g\\| = \\frac{10\\sqrt{2}}{11}$. Let us denote this critical radius by $\\Delta_{\\mathrm{C}}^* = \\frac{10\\sqrt{2}}{11}$.\nThe Cauchy point $p_{\\mathrm{C}}(\\Delta)$ depends on the trust-region radius $\\Delta$ as follows:\n1. If $\\Delta  \\Delta_{\\mathrm{C}}^*$, the unconstrained minimizer lies outside the trust region, so $p_{\\mathrm{C}}$ is on the boundary: $p_{\\mathrm{C}}(\\Delta) = -\\frac{\\Delta}{\\|g\\|}g = -\\frac{\\Delta}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n2. If $\\Delta \\ge \\Delta_{\\mathrm{C}}^*$, the unconstrained minimizer is inside or on the boundary of the trust region, so $p_{\\mathrm{C}}(\\Delta) = -\\frac{10}{11}g = -\\frac{10}{11}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nThe predicted reduction for the Cauchy point, $R_{\\mathrm{Cauchy}}(\\Delta) = \\text{pred}(p_{\\mathrm{C}}(\\Delta))$, is:\n1. For $\\Delta  \\Delta_{\\mathrm{C}}^*$: $R_{\\mathrm{Cauchy}}(\\Delta) = -g^{\\top}(-\\frac{\\Delta}{\\sqrt{2}}g) - \\frac{1}{2}(-\\frac{\\Delta}{\\sqrt{2}}g)^{\\top}B(-\\frac{\\Delta}{\\sqrt{2}}g) = \\frac{\\Delta}{\\sqrt{2}}\\|g\\|^2 - \\frac{\\Delta^2}{2\\|g\\|^2}\\frac{1}{2}g^{\\top}Bg = \\Delta\\sqrt{2} - 0.55\\Delta^2$.\n2. For $\\Delta \\ge \\Delta_{\\mathrm{C}}^*$: The step is constant, so the reduction is constant. $R_{\\mathrm{Cauchy}}(\\Delta) = -g^{\\top}(-\\frac{10}{11}g) - \\frac{1}{2}(-\\frac{10}{11}g)^{\\top}B(-\\frac{10}{11}g) = \\frac{10}{11}\\|g\\|^2 - \\frac{1}{2}(\\frac{10}{11})^2g^{\\top}Bg = \\frac{10}{11}(2) - \\frac{1}{2}(\\frac{100}{121})(2.2) = \\frac{20}{11} - \\frac{10}{11} = \\frac{10}{11}$.\n\nNext, we analyze the exact trust-region solution, $p_{\\mathrm{TR}}$. According to the optimality conditions for the trust-region subproblem, a vector $p$ is a global solution if and only if $\\|p\\| \\le \\Delta$ and there exists a scalar $\\lambda \\ge 0$ such that:\n$$(B + \\lambda I)p = -g$$\n$$\\lambda(\\Delta - \\|p\\|) = 0$$\n$$B + \\lambda I \\text{ is positive semidefinite}$$\nSince the given matrix $B$ is positive definite (eigenvalues are $2$ and $0.2$), $B+\\lambda I$ will be positive definite for any $\\lambda \\ge 0$. This simplifies the analysis.\nWe first check the unconstrained minimizer of $m(p)$, which corresponds to $\\lambda=0$:\n$$p_{\\mathrm{B}} = -B^{-1}g = -\\begin{pmatrix} 1/2  0 \\\\ 0  1/0.2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ -5 \\end{pmatrix}$$\nThe norm of this unconstrained minimizer is $\\|p_{\\mathrm{B}}\\| = \\sqrt{(-0.5)^2 + (-5)^2} = \\sqrt{0.25 + 25} = \\sqrt{25.25}$. Let us denote this critical radius by $\\Delta_{\\mathrm{TR}}^* = \\sqrt{25.25}$.\nThe exact solution $p_{\\mathrm{TR}}(\\Delta)$ depends on $\\Delta$ as follows:\n1. If $\\Delta \\ge \\Delta_{\\mathrm{TR}}^*$, the unconstrained minimizer is feasible. Thus, $p_{\\mathrm{TR}}(\\Delta) = p_{\\mathrm{B}} = \\begin{pmatrix} -0.5 \\\\ -5 \\end{pmatrix}$.\n2. If $\\Delta  \\Delta_{\\mathrm{TR}}^*$, the solution must lie on the boundary of the trust region, i.e., $\\|p_{\\mathrm{TR}}\\|=\\Delta$. From the complementarity condition, this implies $\\lambda  0$. The solution has the form $p_{\\mathrm{TR}}(\\lambda) = -(B+\\lambda I)^{-1}g$. We must find the unique $\\lambda0$ that satisfies the secular equation:\n$$\\|-(B+\\lambda I)^{-1}g\\|^2 = \\left(\\frac{1}{2+\\lambda}\\right)^2 + \\left(\\frac{1}{0.2+\\lambda}\\right)^2 = \\Delta^2$$\nThis is a nonlinear equation that can be solved for $\\lambda$ using a numerical root-finding algorithm.\nThe predicted reduction for the exact solution, $R_{\\mathrm{exact}}(\\Delta) = \\text{pred}(p_{\\mathrm{TR}}(\\Delta))$, is:\n1. For $\\Delta \\ge \\Delta_{\\mathrm{TR}}^*$: The step is constant, so reduction is constant.\n$R_{\\mathrm{exact}}(\\Delta) = -g^{\\top}p_{\\mathrm{B}} - \\frac{1}{2}p_{\\mathrm{B}}^{\\top}Bp_{\\mathrm{B}}$. Since $Bp_{\\mathrm{B}}=-g$, this simplifies to $R_{\\mathrm{exact}}(\\Delta) = -g^{\\top}p_{\\mathrm{B}} - \\frac{1}{2}p_{\\mathrm{B}}^{\\top}(-g) = -\\frac{1}{2}g^{\\top}p_{\\mathrm{B}} = \\frac{1}{2}g^{\\top}B^{-1}g = \\frac{1}{2}(\\frac{1^2}{2} + \\frac{1^2}{0.2}) = \\frac{1}{2}(0.5+5) = 2.75$.\n2. For $\\Delta  \\Delta_{\\mathrm{TR}}^*$: After finding the appropriate $\\lambda  0$, the solution is $p_{\\mathrm{TR}}=p_{\\mathrm{TR}}(\\lambda)$. The reduction is given by:\n$$R_{\\mathrm{exact}}(\\Delta) = -g^{\\top}p_{\\mathrm{TR}} - \\frac{1}{2}p_{\\mathrm{TR}}^{\\top}Bp_{\\mathrm{TR}}$$\nUsing $(B+\\lambda I)p_{\\mathrm{TR}} = -g$, we have $Bp_{\\mathrm{TR}} = -g - \\lambda p_{\\mathrm{TR}}$. Substituting this into the reduction formula:\n$$R_{\\mathrm{exact}}(\\Delta) = -g^{\\top}p_{\\mathrm{TR}} - \\frac{1}{2}p_{\\mathrm{TR}}^{\\top}(-g - \\lambda p_{\\mathrm{TR}}) = -\\frac{1}{2}g^{\\top}p_{\\mathrm{TR}} + \\frac{\\lambda}{2}\\|p_{\\mathrm{TR}}\\|^2$$\nSince $\\|p_{\\mathrm{TR}}\\|=\\Delta$ in this case, and $p_{\\mathrm{TR}} = -(B+\\lambda I)^{-1}g$, we get:\n$$R_{\\mathrm{exact}}(\\Delta) = \\frac{1}{2}g^{\\top}(B+\\lambda I)^{-1}g + \\frac{\\lambda}{2}\\Delta^2 = \\frac{1}{2}\\left(\\frac{1}{2+\\lambda} + \\frac{1}{0.2+\\lambda}\\right) + \\frac{\\lambda}{2}\\Delta^2$$\n\nThe algorithm for each $\\Delta$ in the test suite is as follows:\n- Compute $R_{\\mathrm{Cauchy}}(\\Delta)$ by checking if $\\Delta  \\Delta_{\\mathrm{C}}^*$.\n- Compute $R_{\\mathrm{exact}}(\\Delta)$ by checking if $\\Delta  \\Delta_{\\mathrm{TR}}^*$. If it is, solve the secular equation for $\\lambda$ and use the corresponding reduction formula. Otherwise, use the constant reduction value of $2.75$.\n\nThe test cases are:\n- $\\Delta_1=0.1$\n- $\\Delta_2=1.0$\n- $\\Delta_3=\\frac{10\\sqrt{2}}{11} \\approx 1.2859942395$ (which is $\\Delta_{\\mathrm{C}}^*$)\n- $\\Delta_4=2.0$\n- $\\Delta_5=\\sqrt{25.25} \\approx 5.0249378106$ (which is $\\Delta_{\\mathrm{TR}}^*$)\n- $\\Delta_6=10.0$\n\nThis systematic procedure will yield the required comparisons for each value of $\\Delta$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Computes and compares the predicted reduction of the exact trust-region solution\n    and the Cauchy point for a specific 2D quadratic problem.\n    \"\"\"\n    \n    # Define problem constants\n    g = np.array([1.0, 1.0])\n    B = np.diag([2.0, 0.2])\n\n    test_deltas = [\n        0.1,\n        1.0,\n        10.0 / 11.0 * np.sqrt(2.0),\n        2.0,\n        np.sqrt(25.25),\n        10.0\n    ]\n    \n    # Pre-calculate key values\n    g_norm_sq = g @ g\n    g_norm = np.sqrt(g_norm_sq)\n    gBg = g @ B @ g\n\n    # --- Cauchy Point Calculations ---\n    # Threshold delta for Cauchy point\n    # Occurs when the unconstrained minimizer along the steepest descent ray\n    # is on the trust region boundary.\n    delta_cauchy_star = (g_norm_sq / gBg) * g_norm if gBg > 0 else np.inf\n\n    # Predicted reduction if Cauchy point is unconstrained (and inside TR)\n    pred_red_cauchy_unc = (g_norm_sq**2) / (2.0 * gBg) if gBg > 0 else np.inf\n\n    # --- Exact TR Solution Calculations ---\n    # Unconstrained minimizer of the quadratic model and its norm\n    B_inv = np.diag(1.0 / np.diag(B))\n    p_B = -B_inv @ g\n    delta_tr_star = np.linalg.norm(p_B)\n    \n    # Predicted reduction for the unconstrained minimizer\n    pred_red_exact_unc = -0.5 * (g @ p_B)\n\n    # List to store results\n    results = []\n\n    def predicted_reduction(p, g_vec, B_mat):\n        \"\"\"Computes the predicted reduction for a given step p.\"\"\"\n        return - (g_vec @ p) - 0.5 * (p @ B_mat @ p)\n\n    def secular_equation_func(lam, delta, g_vec, B_diag):\n        \"\"\"\n        The secular equation |p(lam)| - delta = 0 for root finding.\n        p(lam) = -(B + lam*I)^-1 * g\n        \"\"\"\n        p_lam_norm_sq = np.sum((g_vec / (B_diag + lam))**2)\n        return np.sqrt(p_lam_norm_sq) - delta\n        \n    for delta in test_deltas:\n        # 1. Compute Cauchy point and its predicted reduction\n        if delta  delta_cauchy_star:\n            # Cauchy point is on the boundary\n            p_C = - (delta / g_norm) * g\n            r_cauchy = predicted_reduction(p_C, g, B)\n        else:\n            # Cauchy point is the unconstrained minimizer along the ray\n            r_cauchy = pred_red_cauchy_unc\n\n        # 2. Compute exact solution and its predicted reduction\n        if delta >= delta_tr_star:\n            # Unconstrained solution is inside the trust region\n            r_exact = pred_red_exact_unc\n        else:\n            # Solution is on the boundary, need to solve for lambda\n            # The search interval for lambda is [0, inf).\n            # We can use a large number as an upper bound.\n            sol = root_scalar(\n                secular_equation_func, \n                args=(delta, g, np.diag(B)), \n                bracket=[0, 100000],  # A sufficiently wide bracket\n                method='brentq'\n            )\n            lam = sol.root\n            \n            # Simplified formula for predicted reduction on the boundary\n            p_lam_term = np.sum(g**2 / (np.diag(B) + lam))\n            r_exact = 0.5 * p_lam_term + 0.5 * lam * delta**2\n\n        results.append([delta, r_exact, r_cauchy])\n\n    # Format the output string exactly as required\n    # [[val1,val2,val3],[val4,val5,val6],...] without spaces\n    \n    # The default str() for a list adds spaces, so we build the string manually.\n    inner_parts = []\n    for R in results:\n        # Format each sublist to a string, rounding to 10 decimal places.\n        inner_parts.append(f\"[{R[0]:.10f},{R[1]:.10f},{R[2]:.10f}]\")\n\n    # Join the sublist strings with commas and enclose in brackets.\n    final_output_str = f\"[{','.join(inner_parts)}]\"\n\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}