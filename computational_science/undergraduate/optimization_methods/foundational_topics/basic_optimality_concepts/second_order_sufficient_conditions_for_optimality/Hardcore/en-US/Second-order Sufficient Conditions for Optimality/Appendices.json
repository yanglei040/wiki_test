{
    "hands_on_practices": [
        {
            "introduction": "To begin, we will tackle a foundational problem involving a polynomial objective function and a simple linear equality constraint. This exercise is designed to provide a step-by-step guide through the essential mechanics of applying the second-order sufficient conditions. By working through this clear example, you will build a solid procedural understanding of identifying a KKT point, constructing the reduced Hessian, and interpreting the result to confirm a strict local minimum .",
            "id": "3176409",
            "problem": "Consider the equality-constrained optimization problem with objective function $f(x,y)=x^{2}y^{2}+x^{2}+y^{2}$ and constraint $g(x,y)=x+y=0$. Work from first principles of equality-constrained optimality, beginning with the definition of a constrained stationary point via the gradient of the Lagrangian and the condition that the constraint Jacobian has full rank. Then, use the definition of the reduced Hessian constructed on an orthonormal basis for the nullspace of the constraint Jacobian to assess second-order sufficiency.\n\nTasks:\n1. Determine the constrained stationary point(s) by solving the first-order necessary conditions of Karush-Kuhn-Tucker (KKT) optimality for equality constraints.\n2. Construct the reduced Hessian of the Lagrangian at the stationary point found in Task $1$ using the orthonormal basis vector $z=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\-1\\end{pmatrix}$ for the nullspace of the constraint Jacobian.\n3. Using the second-order sufficient condition for equality-constrained problems, decide whether the stationary point is a strict local minimum.\n\nReport as your final answer only the scalar value of the reduced Hessian obtained in Task $2$. No rounding is required; provide the exact value. Do not include any units.",
            "solution": "The problem asks for the analysis of an equality-constrained optimization problem. We are given the objective function $f(x,y)=x^{2}y^{2}+x^{2}+y^{2}$ and the equality constraint $g(x,y)=x+y=0$. We will follow the specified tasks to find the constrained stationary points and analyze the second-order conditions.\n\nThe first step is to define the Lagrangian function, $L(x, y, \\lambda)$, for this problem. The Lagrangian is given by $L(x, y, \\lambda) = f(x, y) - \\lambda g(x, y)$.\n$$L(x,y,\\lambda) = x^{2}y^{2}+x^{2}+y^{2} - \\lambda(x+y)$$\nwhere $\\lambda$ is the Lagrange multiplier associated with the constraint.\n\n**Task 1: Determine the constrained stationary point(s).**\n\nWe find the stationary points by applying the first-order necessary conditions, which state that the gradient of the Lagrangian with respect to all variables must be zero. These are the Karush-Kuhn-Tucker (KKT) conditions for equality constraints.\n\nThe gradient of the Lagrangian is:\n$$ \\nabla L(x,y,\\lambda) = \\begin{pmatrix} \\frac{\\partial L}{\\partial x} \\\\ \\frac{\\partial L}{\\partial y} \\\\ \\frac{\\partial L}{\\partial \\lambda} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n\nThis yields the following system of equations:\n1. $\\frac{\\partial L}{\\partial x} = 2xy^{2} + 2x - \\lambda = 0$\n2. $\\frac{\\partial L}{\\partial y} = 2x^{2}y + 2y - \\lambda = 0$\n3. $\\frac{\\partial L}{\\partial \\lambda} = -(x+y) = 0 \\implies x+y=0$\n\nFrom a purely mathematical viewpoint, the condition on $\\lambda$ is that the gradient with respect to the primal variables is zero, and the final equation is the constraint itself. Setting $\\frac{\\partial L}{\\partial \\lambda}=0$ is a convenient way to recover the constraint.\n\nFrom the third equation, we have $y = -x$. We substitute this into the first two equations.\nSubstituting $y=-x$ into equation (1):\n$$ 2x(-x)^{2} + 2x - \\lambda = 0 \\implies 2x^{3} + 2x - \\lambda = 0 $$\n$$ \\lambda = 2x^{3} + 2x $$\nSubstituting $y=-x$ into equation (2):\n$$ 2x^{2}(-x) + 2(-x) - \\lambda = 0 \\implies -2x^{3} - 2x - \\lambda = 0 $$\n$$ \\lambda = -2x^{3} - 2x $$\n\nEquating the two expressions for $\\lambda$:\n$$ 2x^{3} + 2x = -2x^{3} - 2x $$\n$$ 4x^{3} + 4x = 0 $$\n$$ 4x(x^{2} + 1) = 0 $$\n\nThe term $x^{2}+1$ is always positive for any real number $x$. Therefore, the only real solution is $x=0$.\nIf $x=0$, then $y = -x = 0$.\nWe can find the corresponding value of $\\lambda$ using either expression. For instance, using $\\lambda = 2x^{3} + 2x$, we get $\\lambda = 2(0)^{3} + 2(0) = 0$.\n\nThus, there is a single constrained stationary point $(x^*, y^*) = (0,0)$, with the associated Lagrange multiplier $\\lambda^* = 0$.\n\n**Task 2: Construct the reduced Hessian of the Lagrangian.**\n\nThe second-order conditions involve the Hessian of the Lagrangian with respect to the variables $x$ and $y$, denoted as $\\nabla_{xx}^{2} L$.\nFirst, we compute the second partial derivatives of $L(x,y,\\lambda)$:\n$$ \\frac{\\partial^{2} L}{\\partial x^{2}} = \\frac{\\partial}{\\partial x}(2xy^{2} + 2x - \\lambda) = 2y^{2} + 2 $$\n$$ \\frac{\\partial^{2} L}{\\partial y^{2}} = \\frac{\\partial}{\\partial y}(2x^{2}y + 2y - \\lambda) = 2x^{2} + 2 $$\n$$ \\frac{\\partial^{2} L}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(2xy^{2} + 2x - \\lambda) = 4xy $$\n\nThe Hessian matrix of the Lagrangian is:\n$$ \\nabla_{xx}^{2} L(x,y,\\lambda) = \\begin{pmatrix} 2y^{2}+2  4xy \\\\ 4xy  2x^{2}+2 \\end{pmatrix} $$\n\nNext, we evaluate this Hessian at the stationary point $(x^*, y^*, \\lambda^*) = (0,0,0)$:\n$$ H_L = \\nabla_{xx}^{2} L(0,0,0) = \\begin{pmatrix} 2(0)^{2}+2  4(0)(0) \\\\ 4(0)(0)  2(0)^{2}+2 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} $$\n\nThe reduced Hessian is constructed by projecting this Hessian onto the nullspace of the constraint Jacobian. The constraint Jacobian, $A$, is the transpose of the gradient of the constraint function $g(x,y)$:\n$$ \\nabla g(x,y) = \\begin{pmatrix} \\frac{\\partial g}{\\partial x} \\\\ \\frac{\\partial g}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n$$ A(x,y) = \\nabla g(x,y)^{T} = \\begin{pmatrix} 1  1 \\end{pmatrix} $$\nThe nullspace of $A$ consists of all vectors $v = \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix}$ such that $Av=0$, which means $1 \\cdot v_x + 1 \\cdot v_y = 0$, or $v_y = -v_x$. Any vector in the nullspace is a multiple of $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nThe problem provides an orthonormal basis for this nullspace, which we denote by the matrix $Z$. In this case, the nullspace is one-dimensional, so $Z$ is a column vector:\n$$ Z = z = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n\nThe reduced Hessian, $H_R$, is defined as $H_R = Z^{T} H_L Z$. We now compute this product:\n$$ H_R = \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  -1 \\end{pmatrix}\\right) \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\right) $$\n$$ H_R = \\frac{1}{2} \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nFirst, we perform the multiplication of the first two matrices:\n$$ \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} (1)(2)+(-1)(0)  (1)(0)+(-1)(2) \\end{pmatrix} = \\begin{pmatrix} 2  -2 \\end{pmatrix} $$\nNow, we multiply this result by the last vector:\n$$ \\begin{pmatrix} 2  -2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = (2)(1) + (-2)(-1) = 2 + 2 = 4 $$\nFinally, we multiply by the scalar factor $\\frac{1}{2}$:\n$$ H_R = \\frac{1}{2} (4) = 2 $$\nThe reduced Hessian is a $1 \\times 1$ matrix, which is the scalar value $2$.\n\n**Task 3: Decide whether the stationary point is a strict local minimum.**\n\nThe second-order sufficient condition for a point $x^*$ to be a strict local minimum is that the reduced Hessian $H_R = Z^{T}(\\nabla_{xx}^{2} L(x^*, \\lambda^*))Z$ is positive definite.\nIn our case, the reduced Hessian is the scalar $2$. A scalar is positive definite if and only if it is a positive number.\nSince $H_R = 2 > 0$, the reduced Hessian is positive definite.\nTherefore, the stationary point $(x^*, y^*) = (0,0)$ is a strict local minimum for the constrained problem.\n\nThe final answer requested is the scalar value of the reduced Hessian obtained in Task 2.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "This practice explores a crucial concept: a function that is not convex globally can still possess a strict local minimum when confined to a feasible set. We will examine a quadratic function whose Hessian is indefinite, meaning it exhibits both positive and negative curvature. This problem powerfully illustrates why we analyze curvature on the *tangent space* of the constraints, as you will see how a constraint can effectively \"slice away\" the undesirable negative curvature, ensuring the second-order sufficient conditions hold in all feasible directions .",
            "id": "3176387",
            "problem": "Consider the equality-constrained quadratic program in dimension $3$ with objective function $f:\\mathbb{R}^{3}\\to\\mathbb{R}$ given by\n$$\nf(x) \\;=\\; \\frac{1}{2}\\,x^{\\top}Qx \\;+\\; c^{\\top}x,\n$$\nwhere\n$$\nQ \\;=\\; \\begin{pmatrix} 1  0  0 \\\\ 0  -1  0 \\\\ 0  0  2 \\end{pmatrix}, \n\\qquad\nc \\;=\\; \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}.\n$$\nImpose the single linear equality constraint\n$$\nA x \\;=\\; b, \n\\qquad \nA \\;=\\; \\begin{pmatrix} 0  1  0 \\end{pmatrix}, \n\\qquad \nb \\;=\\; 1.\n$$\nUsing only foundational definitions—namely, the definition of a constrained local minimizer, the notion of feasible directions and the tangent space (defined here by the first-order constraint linearization), the Taylor expansion of $f$ up to second order, and the Karush-Kuhn-Tucker (KKT) optimality conditions—do the following:\n\n1) Establish that the symmetric matrix $Q$ is indefinite by analyzing its eigenvalues.\n\n2) Identify the set of feasible directions $d$ at any feasible point $\\bar{x}$ by linearizing the constraint and characterizing the tangent space $\\{d \\in \\mathbb{R}^{3} : A d = 0\\}$.\n\n3) Show that the quadratic form $d^{\\top}Qd$ is strictly positive for all nonzero feasible directions $d$ you identified in part $2$, and explain how this enforces positive curvature in feasible directions even though $Q$ is indefinite in $\\mathbb{R}^{3}$.\n\n4) Derive the first-order stationary conditions for equality-constrained problems from the definition of a constrained local minimizer and the Lagrangian, and solve the resulting linear system to obtain the candidate optimizer $x^{\\star}$.\n\n5) Using the second-order Taylor expansion of $f$ along feasible directions and your result in part $3$, argue that the second-order sufficient conditions for optimality hold at your candidate, thereby proving that $x^{\\star}$ is a strict local minimizer.\n\nYour final answer must be the optimizer $x^{\\star}$ written as a single row vector using the parentheses matrix notation. Express your answer exactly; no rounding is required.",
            "solution": "The problem as stated is mathematically well-posed, internally consistent, and free of any scientific or factual unsoundness. All necessary information is provided to determine a unique solution. We may therefore proceed with a full derivation as requested. The solution will follow the five specified steps.\n\nThe problem is to find a strict local minimizer for the objective function $f:\\mathbb{R}^{3}\\to\\mathbb{R}$,\n$$f(x) = \\frac{1}{2}x^{\\top}Qx + c^{\\top}x$$\nwith\n$$Q = \\begin{pmatrix} 1  0  0 \\\\ 0  -1  0 \\\\ 0  0  2 \\end{pmatrix}, \\quad c = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}$$\nsubject to the linear equality constraint $Ax=b$, where\n$$A = \\begin{pmatrix} 0  1  0 \\end{pmatrix}, \\quad b = 1$$\n\n$1$) We first establish that the symmetric matrix $Q$ is indefinite. A symmetric matrix is indefinite if it has at least one positive eigenvalue and at least one negative eigenvalue. The matrix $Q$ is a diagonal matrix, so its eigenvalues are its diagonal entries. The eigenvalues are $\\lambda_1 = 1$, $\\lambda_2 = -1$, and $\\lambda_3 = 2$. Since $\\lambda_1$ and $\\lambda_3$ are positive and $\\lambda_2$ is negative, the matrix $Q$ is, by definition, indefinite. This implies that the quadratic function $f(x)$ is non-convex and has both positive and negative curvature in $\\mathbb{R}^{3}$. Specifically, the quadratic form $x^{\\top}Qx$ is a saddle.\n\n$2$) Next, we identify the set of feasible directions at any feasible point $\\bar{x}$. A point $\\bar{x}$ is feasible if it satisfies the constraint $A\\bar{x} = b$. For a linear equality constraint, the set of feasible directions at any feasible point is the tangent space, which is constant for all feasible points. A direction $d \\in \\mathbb{R}^{3}$ is a feasible direction if for a feasible point $\\bar{x}$, the point $\\bar{x}+\\alpha d$ also satisfies the constraint for all sufficiently small scalars $\\alpha$. This requires $A(\\bar{x}+\\alpha d) = b$. Expanding this gives $A\\bar{x} + \\alpha Ad = b$. Since $\\bar{x}$ is feasible, $A\\bar{x} = b$, which simplifies the condition to $\\alpha Ad = 0$. For this to hold for any non-zero $\\alpha$, we must have $Ad=0$. Thus, the set of feasible directions is the null space of the matrix $A$.\nGiven $A = \\begin{pmatrix} 0  1  0 \\end{pmatrix}$, we find the null space by solving $Ad=0$:\n$$ \\begin{pmatrix} 0  1  0 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ d_2 \\\\ d_3 \\end{pmatrix} = 0 $$\nThis equation simplifies to $d_2 = 0$. The components $d_1$ and $d_3$ can be any real numbers. Therefore, the tangent space, representing the set of all feasible directions, is\n$$ T = \\{ d \\in \\mathbb{R}^{3} \\mid Ad = 0 \\} = \\left\\{ \\begin{pmatrix} d_1 \\\\ 0 \\\\ d_3 \\end{pmatrix} \\mid d_1, d_3 \\in \\mathbb{R} \\right\\} $$\nThis is a $2$-dimensional subspace of $\\mathbb{R}^{3}$ spanned by the basis vectors $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ and $\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n\n$3$) We now examine the curvature of the objective function along these feasible directions. This is determined by the sign of the quadratic form $d^{\\top}Qd$ for any non-zero feasible direction $d \\in T$. Let $d$ be a non-zero vector in the tangent space, so $d = \\begin{pmatrix} d_1 \\\\ 0 \\\\ d_3 \\end{pmatrix}$ where $d_1$ and $d_3$ are not both zero. We compute $d^{\\top}Qd$:\n$$ d^{\\top}Qd = \\begin{pmatrix} d_1  0  d_3 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  -1  0 \\\\ 0  0  2 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ 0 \\\\ d_3 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} d_1  0  2d_3 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ 0 \\\\ d_3 \\end{pmatrix} = d_1^2 + 2d_3^2 $$\nSince $d_1, d_3 \\in \\mathbb{R}$, we have $d_1^2 \\ge 0$ and $2d_3^2 \\ge 0$. The sum $d_1^2 + 2d_3^2$ is zero if and only if $d_1 = 0$ and $d_3 = 0$, which corresponds to the zero vector $d=0$. As we are considering non-zero feasible directions, we have $d_1^2 + 2d_3^2 > 0$. This demonstrates that the quadratic form $d^{\\top}Qd$ is strictly positive for all nonzero feasible directions. This means the Hessian of the objective function, $Q$, is positive definite on the tangent space $T$. The negative curvature associated with the eigenvalue $\\lambda_2 = -1$ corresponds to the $x_2$ direction. However, the constraint $Ax=b$ (which is $x_2=1$) restricts feasible points to a plane, and feasible directions to the subspace where the second component is zero. This effectively neutralizes the direction of negative curvature, leaving only positive curvature in all allowable directions of movement.\n\n$4$) We derive and solve the first-order necessary conditions for optimality, known as the Karush-Kuhn-Tucker (KKT) conditions for equality constraints. We define the Lagrangian function $L(x, \\lambda) = f(x) - \\lambda (Ax - b)$, where $\\lambda$ is the scalar Lagrange multiplier.\n$$ L(x, \\lambda) = \\frac{1}{2}x^{\\top}Qx + c^{\\top}x - \\lambda(A x - b) $$\nThe first-order necessary conditions for a point $x^{\\star}$ to be a local minimizer are the existence of a multiplier $\\lambda^{\\star}$ such that the gradient of the Lagrangian with respect to $x$ is zero, and the point is feasible:\n$$ \\nabla_x L(x^{\\star}, \\lambda^{\\star}) = Qx^{\\star} + c - A^{\\top}\\lambda^{\\star} = 0 $$\n$$ Ax^{\\star} - b = 0 $$\nSubstituting the given matrices and vectors, we obtain a system of linear equations:\n$$ \\begin{pmatrix} 1  0  0 \\\\ 0  -1  0 \\\\ 0  0  2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\lambda = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n$$ \\begin{pmatrix} 0  1  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = 1 $$\nThe feasibility constraint $Ax=b$ gives $x_2 = 1$. The gradient equation yields a system of three scalar equations:\n$1$) $x_1 + 1 = 0 \\implies x_1 = -1$\n$2$) $-x_2 - 2 - \\lambda = 0$\n$3$) $2x_3 + 3 = 0 \\implies x_3 = -\\frac{3}{2}$\nSubstituting $x_2=1$ into the second equation allows us to find the multiplier $\\lambda$:\n$-1 - 2 - \\lambda = 0 \\implies \\lambda = -3$.\nThe candidate optimizer is the point $x^{\\star}$ with components $x_1 = -1$, $x_2 = 1$, and $x_3 = -\\frac{3}{2}$.\n$$ x^{\\star} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -\\frac{3}{2} \\end{pmatrix} $$\n\n$5$) Finally, we verify that $x^{\\star}$ is a strict local minimizer using the second-order sufficient conditions (SOSC). For equality constraints, the SOSC are satisfied at a KKT point $(x^{\\star}, \\lambda^{\\star})$ if the Hessian of the Lagrangian with respect to $x$, evaluated at $x^{\\star}$, is positive definite on the tangent space $T(x^{\\star})$. That is, $d^{\\top} \\nabla_{xx}^2 L(x^{\\star}, \\lambda^{\\star}) d > 0$ for all nonzero $d \\in T(x^{\\star})$.\nThe Hessian of the Lagrangian is $\\nabla_{xx}^2 L(x, \\lambda) = Q$.\nThe SOSC then requires that $d^{\\top}Qd > 0$ for all nonzero $d$ such that $Ad=0$. This is precisely the condition we verified in part $3$). We showed that for any $d \\in T, d \\neq 0$, the quadratic form evaluates to $d^{\\top}Qd = d_1^2 + 2d_3^2 > 0$.\nTo see why this guarantees a strict local minimum, consider a Taylor expansion of $f(x)$ around $x^{\\star}$ along a feasible direction $d \\in T$. For a point $x = x^{\\star} + \\alpha d$, with $\\alpha \\in \\mathbb{R}$:\n$$ f(x^{\\star}+\\alpha d) - f(x^{\\star}) = \\alpha \\nabla f(x^{\\star})^{\\top}d + \\frac{1}{2}\\alpha^2 d^{\\top} \\nabla^2 f(x^{\\star}) d + o(\\alpha^2) $$\nFrom the KKT condition $\\nabla_x L(x^{\\star}, \\lambda^{\\star}) = 0$, we have $\\nabla f(x^{\\star}) = A^{\\top}\\lambda^{\\star}$. The first-order term is $\\alpha (A^{\\top}\\lambda^{\\star})^{\\top}d = \\alpha (\\lambda^{\\star})^{\\top} (Ad)$. Since $d \\in T$, we have $Ad=0$, so the first-order term vanishes. The expansion becomes:\n$$ f(x^{\\star}+\\alpha d) - f(x^{\\star}) = \\frac{1}{2}\\alpha^2 d^{\\top} Q d + o(\\alpha^2) $$\nAs established, $d^{\\top}Qd > 0$ for any nonzero $d \\in T$. Thus, for any nonzero $\\alpha$, the term $\\frac{1}{2}\\alpha^2 d^{\\top} Q d$ is strictly positive. For $\\alpha$ sufficiently small, this second-order term dominates the higher-order terms $o(\\alpha^2)$, implying that $f(x^{\\star}+\\alpha d) - f(x^{\\star}) > 0$. This holds for any feasible direction $d$. Consequently, $x^{\\star}$ is a strict local minimizer. The candidate point $x^{\\star} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -\\frac{3}{2} \\end{pmatrix}$ is confirmed to be a strict local minimizer of the constrained problem.",
            "answer": "$$\\boxed{\\begin{pmatrix} -1  1  -\\frac{3}{2} \\end{pmatrix}}$$"
        },
        {
            "introduction": "We now advance from equality constraints to the more general case involving inequality constraints, which requires introducing the concept of the critical cone. This problem presents a candidate point where an inequality constraint is active, necessitating a more nuanced analysis than the tangent space used for equalities. This exercise will guide you through deriving the critical cone and testing the Hessian's curvature on this specific set of directions, demonstrating a key scenario where the second-order test can be inconclusive .",
            "id": "3176420",
            "problem": "Consider the nonlinear program: minimize the objective $f(x) = x_1^2 + x_2^2$ subject to the single inequality constraint $h(x) = \\cos(x_1) - x_2 \\le 0$. Let the candidate point be $x^\\star = (0, 1)$.\n\nYour tasks are:\n\n- Starting from the definitions of the Lagrangian $L(x,\\lambda) = f(x) + \\lambda h(x)$ and the Karush–Kuhn–Tucker (KKT) conditions (primal feasibility, dual feasibility, complementary slackness, and stationarity), verify that $x^\\star$ satisfies the first-order conditions under the Mangasarian–Fromovitz Constraint Qualification (MFCQ), and determine the associated Lagrange multiplier $\\lambda^\\star$.\n\n- Using the first-order (linear) approximation of the active constraint and the definition of linearized feasibility, derive the cone of linearly feasible directions at $x^\\star$. Then, using the principle that critical directions are those linearly feasible directions along which the first-order change in the objective is zero, derive the critical cone $C(x^\\star,\\lambda^\\star)$.\n\n- Compute the Hessian of the Lagrangian with respect to $x$, namely $\\nabla_{xx}^2 L(x^\\star,\\lambda^\\star)$, and evaluate the least curvature along critical directions:\n$$\n\\alpha^\\star \\;=\\; \\min \\left\\{ d^\\top \\nabla_{xx}^2 L(x^\\star,\\lambda^\\star)\\, d \\;:\\; d \\in C(x^\\star,\\lambda^\\star),\\ \\|d\\|_2 = 1 \\right\\}.\n$$\n\nGive $\\alpha^\\star$ as your final answer. No rounding is required, and no units are involved. Express your answer as a single real number.",
            "solution": "The problem asks for the analysis of a candidate point $x^\\star = (0, 1)$ for the nonlinear program: minimize $f(x) = x_1^2 + x_2^2$ subject to $h(x) = \\cos(x_1) - x_2 \\le 0$. The analysis involves verifying the Karush–Kuhn–Tucker (KKT) conditions, determining the critical cone, and computing the minimum curvature of the Lagrangian's Hessian along normalized directions in this cone.\n\nFirst, we define the Lagrangian function $L(x, \\lambda)$ for this problem, which is given by\n$$L(x, \\lambda) = f(x) + \\lambda h(x) = x_1^2 + x_2^2 + \\lambda (\\cos(x_1) - x_2).$$\nHere, $x = (x_1, x_2)^\\top$ and $\\lambda$ is the Lagrange multiplier associated with the inequality constraint.\n\nWe begin by verifying the first-order necessary conditions (KKT conditions) at the point $x^\\star = (0, 1)^\\top$.\n\nThe gradients of the objective function $f(x)$ and the constraint function $h(x)$ are:\n$$\\nabla f(x) = \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\end{pmatrix}, \\quad \\nabla h(x) = \\begin{pmatrix} -\\sin(x_1) \\\\ -1 \\end{pmatrix}.$$\nAt the candidate point $x^\\star = (0, 1)^\\top$, these gradients become:\n$$\\nabla f(x^\\star) = \\begin{pmatrix} 2(0) \\\\ 2(1) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}, \\quad \\nabla h(x^\\star) = \\begin{pmatrix} -\\sin(0) \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}.$$\n\nThe KKT conditions consist of primal feasibility, dual feasibility, complementary slackness, and stationarity.\n$1$. **Primal Feasibility**: We check if $x^\\star$ satisfies the constraint $h(x) \\le 0$.\n$$h(x^\\star) = \\cos(0) - 1 = 1 - 1 = 0.$$\nSince $h(x^\\star) = 0$, the constraint is satisfied and is active at $x^\\star$.\n\n$2$. **Mangasarian–Fromovitz Constraint Qualification (MFCQ)**: For the set of active inequality constraints, the gradient vectors must be linearly independent. Here, there is only one active constraint. Its gradient $\\nabla h(x^\\star) = (0, -1)^\\top$ is a non-zero vector, so it is linearly independent. Thus, MFCQ holds at $x^\\star$, which guarantees the existence of a unique Lagrange multiplier $\\lambda^\\star$.\n\n$3$. **Stationarity**: The gradient of the Lagrangian with respect to $x$ must be zero at $(x^\\star, \\lambda^\\star)$:\n$$\\nabla_x L(x^\\star, \\lambda^\\star) = \\nabla f(x^\\star) + \\lambda^\\star \\nabla h(x^\\star) = 0.$$\nSubstituting the gradients we calculated:\n$$\\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + \\lambda^\\star \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.$$\nThis vector equation yields two scalar equations:\n\\begin{align*} 0 + \\lambda^\\star(0) = 0 \\quad \\implies \\quad 0 = 0 \\\\ 2 - \\lambda^\\star = 0 \\quad \\implies \\quad \\lambda^\\star = 2 \\end{align*}\nThe stationarity condition holds with the unique Lagrange multiplier $\\lambda^\\star = 2$.\n\n$4$. **Dual Feasibility**: For an inequality constraint of the form $h(x) \\le 0$, the multiplier must be non-negative. We found $\\lambda^\\star = 2$, which satisfies $\\lambda^\\star \\ge 0$.\n\n$5$. **Complementary Slackness**: This condition requires $\\lambda^\\star h(x^\\star) = 0$. We have $\\lambda^\\star = 2$ and $h(x^\\star) = 0$, so $2 \\cdot 0 = 0$. The condition is satisfied.\nAll KKT conditions are satisfied at $x^\\star = (0, 1)$ with $\\lambda^\\star = 2$.\n\nNext, we derive the critical cone $C(x^\\star, \\lambda^\\star)$. This requires first finding the cone of linearly feasible directions, $LF(x^\\star)$. A direction $d = (d_1, d_2)^\\top$ is in $LF(x^\\star)$ if $\\nabla h_i(x^\\star)^\\top d \\le 0$ for all active inequality constraints $h_i$. Here, only $h(x)$ is active.\n$$\\nabla h(x^\\star)^\\top d \\le 0 \\implies \\begin{pmatrix} 0  -1 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix} \\le 0 \\implies -d_2 \\le 0 \\implies d_2 \\ge 0.$$\nSo, the cone of linearly feasible directions is $LF(x^\\star) = \\{d \\in \\mathbb{R}^2 \\mid d_2 \\ge 0\\}$.\n\nThe critical cone $C(x^\\star, \\lambda^\\star)$ consists of directions $d \\in LF(x^\\star)$ along which the first-order change in the objective is non-positive, i.e., $\\nabla f(x^\\star)^\\top d \\le 0$. Note that due to stationarity, $\\nabla f(x^\\star)^\\top d = -\\lambda^\\star \\nabla h(x^\\star)^\\top d$. Since $\\lambda^\\star > 0$, the condition $\\nabla f(x^\\star)^\\top d \\le 0$ is equivalent to $\\nabla h(x^\\star)^\\top d \\ge 0$. Combined with the linear feasibility condition $\\nabla h(x^\\star)^\\top d \\le 0$, this implies $\\nabla h(x^\\star)^\\top d = 0$.\nLet's verify this using the explicit gradients:\n$$\\nabla f(x^\\star)^\\top d = \\begin{pmatrix} 0  2 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix} = 2d_2.$$\nThe condition $\\nabla f(x^\\star)^\\top d \\le 0$ implies $2d_2 \\le 0$, or $d_2 \\le 0$.\nA direction $d$ is in the critical cone if it satisfies both $d \\in LF(x^\\star)$ (i.e., $d_2 \\ge 0$) and $\\nabla f(x^\\star)^\\top d \\le 0$ (i.e., $d_2 \\le 0$). The only way to satisfy both is to have $d_2 = 0$.\nThe critical cone is therefore:\n$$C(x^\\star, \\lambda^\\star) = \\{d = (d_1, d_2)^\\top \\in \\mathbb{R}^2 \\mid d_2 = 0\\}.$$\nThis represents the $x_1$-axis.\n\nFinally, we compute $\\alpha^\\star$, the minimum curvature of the Hessian of the Lagrangian along normalized critical directions. The Hessian of the Lagrangian with respect to $x$ is:\n$$\\nabla_{xx}^2 L(x, \\lambda) = \\nabla_{xx}^2 \\left( x_1^2 + x_2^2 + \\lambda (\\cos(x_1) - x_2) \\right) = \\begin{pmatrix} 2 - \\lambda \\cos(x_1)  0 \\\\ 0  2 \\end{pmatrix}.$$\nEvaluating this at $(x^\\star, \\lambda^\\star) = ((0, 1), 2)$:\n$$\\nabla_{xx}^2 L(x^\\star, \\lambda^\\star) = \\begin{pmatrix} 2 - 2 \\cos(0)  0 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} 2 - 2(1)  0 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  2 \\end{pmatrix}.$$\nLet $H = \\nabla_{xx}^2 L(x^\\star, \\lambda^\\star)$. We want to find:\n$$\\alpha^\\star = \\min \\left\\{ d^\\top H d \\mid d \\in C(x^\\star,\\lambda^\\star),\\ \\|d\\|_2 = 1 \\right\\}.$$\nA direction $d \\in C(x^\\star, \\lambda^\\star)$ must be of the form $d = (d_1, 0)^\\top$. The normalization condition $\\|d\\|_2 = 1$ implies $\\sqrt{d_1^2 + 0^2} = 1$, so $|d_1| = 1$. This gives two possible vectors for $d$: $(1, 0)^\\top$ and $(-1, 0)^\\top$.\nLet's evaluate the quadratic form $d^\\top H d$ for a generic critical direction $d = (d_1, 0)^\\top$:\n$$d^\\top H d = \\begin{pmatrix} d_1  0 \\end{pmatrix} \\begin{pmatrix} 0  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} d_1  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = 0.$$\nThe value of the quadratic form is $0$ for any vector in the critical cone, including the normalized ones. Therefore, the minimum value is $0$.\n$$\\alpha^\\star = 0.$$\nThis result indicates that the second-order sufficient conditions for a strict local minimum are not satisfied, as they require the curvature to be strictly positive on the critical cone.",
            "answer": "$$\n\\boxed{0}\n$$"
        }
    ]
}