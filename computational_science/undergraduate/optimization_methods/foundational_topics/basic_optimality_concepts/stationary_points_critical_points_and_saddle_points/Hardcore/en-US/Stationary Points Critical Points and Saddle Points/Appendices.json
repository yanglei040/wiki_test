{
    "hands_on_practices": [
        {
            "introduction": "This exercise is a foundational practice in multivariable calculus and optimization, focusing on a simple quadratic function to solidify the core procedure of finding and classifying stationary points. By calculating the gradient to locate the stationary point and then analyzing the Hessian matrix's eigenvalues, you will practice the standard classification workflow. Furthermore, by finding the coordinate rotation that diagonalizes the Hessian, you will develop a deeper geometric intuition for the principal curvatures that define a saddle surface .",
            "id": "3184889",
            "problem": "Consider the twice continuously differentiable function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by $f(x,y)=x^{2}+xy-y^{2}$. Use the fundamental definition that a stationary point is a point where the gradient is zero, and classify stationary points by the definiteness of the Hessian matrix. Then, starting from the fact that any real symmetric matrix is orthogonally diagonalizable, determine a rotation that diagonalizes the Hessian and write the quadratic form in the rotated coordinates to reveal the saddle nature.\n\nTasks:\n- Derive and solve the stationary point conditions $\\nabla f(x,y)=\\mathbf{0}$.\n- Use the Hessian to classify the stationary point by definiteness.\n- Construct an orthogonal rotation matrix $R(\\varphi)$ with angle $\\varphi$ (in radians) that diagonalizes the Hessian, and show that in the rotated coordinates $(u,v)=R(\\varphi)^{\\top}(x,y)$ the quadratic form has no cross term.\n- Provide the exact values of the principal curvatures (the eigenvalues of the Hessian) and the exact rotation angle $\\varphi$.\n\nAnswer specification:\n- Express the angle in radians as an exact symbolic expression.\n- Express the principal curvatures exactly (do not approximate square roots).\n- As your final answer, give a single row matrix containing, in order: the rotation angle $\\varphi$, the larger principal curvature, and the smaller principal curvature. Do not include units inside the final row matrix.",
            "solution": "The problem is to find and classify the stationary points of the function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by $f(x,y)=x^{2}+xy-y^{2}$, and to find a coordinate rotation that diagonalizes the associated quadratic form.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n- Function: $f(x,y)=x^{2}+xy-y^{2}$, twice continuously differentiable.\n- Definition of stationary point: $\\nabla f(x,y)=\\mathbf{0}$.\n- Classification method: Definiteness of the Hessian matrix.\n- Task: Find an orthogonal rotation matrix $R(\\varphi)$ to diagonalize the Hessian.\n- Task: Express the quadratic form in rotated coordinates $(u,v)=R(\\varphi)^{\\top}(x,y)$.\n- Required outputs: The stationary point, its classification, the principal curvatures (eigenvalues), and the rotation angle $\\varphi$.\n- Final answer format: A row matrix containing the rotation angle, the larger principal curvature, and the smaller principal curvature.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in multivariable calculus, involving gradients, Hessians, eigenvalues, and matrix diagonalization. These are core mathematical concepts. The problem is scientifically sound.\n- **Well-Posed:** The function is explicitly defined, and the tasks are unambiguous. The procedures for finding stationary points and diagonalizing symmetric matrices are standard, leading to a unique and meaningful solution. The problem is well-posed.\n- **Objective:** The problem is stated using formal mathematical language, free of any subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We will proceed with a full solution.\n\nThe function is given by $f(x,y) = x^{2}+xy-y^{2}$.\n\n**1. Find the Stationary Point**\nA stationary point $(x_0, y_0)$ is a point where the gradient of the function is the zero vector, i.e., $\\nabla f(x_0, y_0) = \\mathbf{0}$. The gradient of $f$ is:\n$$\n\\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 2x+y \\\\ x-2y \\end{pmatrix}\n$$\nSetting the gradient to zero gives the following system of linear equations:\n$$\n\\begin{cases}\n2x+y = 0 \\\\\nx-2y = 0\n\\end{cases}\n$$\nFrom the second equation, we have $x=2y$. Substituting this into the first equation:\n$$\n2(2y)+y=0 \\implies 4y+y=0 \\implies 5y=0 \\implies y=0\n$$\nSubstituting $y=0$ back into $x=2y$ gives $x=0$.\nThus, the only stationary point is $(0,0)$.\n\n**2. Classify the Stationary Point using the Hessian Matrix**\nThe Hessian matrix $H$ is the matrix of second partial derivatives:\n$$\nH(x,y) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2}  \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x}  \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix}\n$$\nThe second partial derivatives are:\n$$\n\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x}(2x+y) = 2\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(2x+y) = 1\n$$\n$$\n\\frac{\\partial^2 f}{\\partial y^2} = \\frac{\\partial}{\\partial y}(x-2y) = -2\n$$\nSince $f$ is twice continuously differentiable, $\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y} = 1$. The Hessian matrix is constant for all $(x,y)$:\n$$\nH = \\begin{pmatrix} 2  1 \\\\ 1  -2 \\end{pmatrix}\n$$\nTo classify the stationary point $(0,0)$, we analyze the definiteness of $H$. We do this by finding its eigenvalues, which are the principal curvatures. The characteristic equation is $\\det(H-\\lambda I)=0$:\n$$\n\\det\\begin{pmatrix} 2-\\lambda  1 \\\\ 1  -2-\\lambda \\end{pmatrix} = 0\n$$\n$$\n(2-\\lambda)(-2-\\lambda) - (1)(1) = 0\n$$\n$$\n-4-2\\lambda+2\\lambda+\\lambda^2 - 1 = 0\n$$\n$$\n\\lambda^2 - 5 = 0 \\implies \\lambda = \\pm\\sqrt{5}\n$$\nThe eigenvalues (principal curvatures) are $\\lambda_1 = \\sqrt{5}$ and $\\lambda_2 = -\\sqrt{5}$. Since one eigenvalue is positive and the other is negative, the Hessian matrix is indefinite. A stationary point with an indefinite Hessian is a saddle point.\n\n**3. Determine the Rotation to Diagonalize the Hessian**\nWe seek an orthogonal rotation matrix $R(\\varphi) = \\begin{pmatrix} \\cos\\varphi  -\\sin\\varphi \\\\ \\sin\\varphi  \\cos\\varphi \\end{pmatrix}$ that diagonalizes $H$. The columns of $R(\\varphi)$ must be the normalized eigenvectors of $H$. The transformation is given by $D = R(\\varphi)^\\top H R(\\varphi)$, where $D$ is the diagonal matrix of eigenvalues.\nThe angle of rotation $\\varphi$ that aligns the coordinate axes with the principal axes of the quadratic form $Ax^2+Bxy+Cy^2$ is given by $\\tan(2\\varphi) = \\frac{B}{A-C}$. The quadratic form associated with the Hessian $H = \\begin{pmatrix} a  b \\\\ b  c \\end{pmatrix}$ is $q(x,y) = ax^2 + 2bxy + cy^2$. For our Hessian $H = \\begin{pmatrix} 2  1 \\\\ 1  -2 \\end{pmatrix}$, we have $a=2$, $b=1$, $c=-2$. The formula for the rotation angle to diagonalize the matrix itself is $\\tan(2\\varphi)=\\frac{2b}{a-c}$.\n$$\n\\tan(2\\varphi) = \\frac{2(1)}{2 - (-2)} = \\frac{2}{4} = \\frac{1}{2}\n$$\nWe can solve for $\\varphi$. Let us choose $2\\varphi$ to be in the first quadrant, so $\\varphi$ is in $(0, \\pi/4)$. We use the half-angle identity for tangent: $\\tan(2\\varphi) = \\frac{2\\tan\\varphi}{1-\\tan^2\\varphi}$.\n$$\n\\frac{1}{2} = \\frac{2\\tan\\varphi}{1-\\tan^2\\varphi}\n$$\n$$\n1-\\tan^2\\varphi = 4\\tan\\varphi\n$$\n$$\n\\tan^2\\varphi + 4\\tan\\varphi - 1 = 0\n$$\nThis is a quadratic equation for $\\tan\\varphi$. Using the quadratic formula:\n$$\n\\tan\\varphi = \\frac{-4 \\pm \\sqrt{4^2 - 4(1)(-1)}}{2(1)} = \\frac{-4 \\pm \\sqrt{16+4}}{2} = \\frac{-4 \\pm \\sqrt{20}}{2} = \\frac{-4 \\pm 2\\sqrt{5}}{2} = -2 \\pm \\sqrt{5}\n$$\nSince we chose $\\varphi \\in (0, \\pi/4)$, $\\tan\\varphi$ must be positive and less than $1$.\nThe value $\\sqrt{5}-2 \\approx 2.236-2=0.236$ satisfies $0  \\sqrt{5}-2  1$.\nThe other value, $-2-\\sqrt{5}$, is negative.\nThus, we must have $\\tan\\varphi = \\sqrt{5}-2$. The rotation angle is $\\varphi = \\arctan(\\sqrt{5}-2)$.\n\n**4. Express Quadratic Form in Rotated Coordinates**\nThe coordinate transformation is given by $\\begin{pmatrix} x \\\\ y \\end{pmatrix} = R(\\varphi)\\begin{pmatrix} u \\\\ v \\end{pmatrix}$. The quadratic part of the function is the function itself, $f(x,y)=x^2+xy-y^2$. This corresponds to the matrix $A = \\begin{pmatrix} 1  1/2 \\\\ 1/2  -1 \\end{pmatrix}$. Note that $H=2A$. The quadratic form is $\\mathbf{x}^\\top A \\mathbf{x}$.\nIn the new coordinates $(u,v)$, the quadratic form becomes:\n$$\nf(u,v) = \\left(R(\\varphi)\\begin{pmatrix} u \\\\ v \\end{pmatrix}\\right)^\\top A \\left(R(\\varphi)\\begin{pmatrix} u \\\\ v \\end{pmatrix}\\right) = \\begin{pmatrix} u \\\\ v \\end{pmatrix}^\\top (R(\\varphi)^\\top A R(\\varphi)) \\begin{pmatrix} u \\\\ v \\end{pmatrix}\n$$\nThe matrix $R(\\varphi)^\\top A R(\\varphi)$ is a diagonal matrix whose diagonal entries are the eigenvalues of $A$. The eigenvalues of $A = \\frac{1}{2}H$ are half the eigenvalues of $H$, which are $\\frac{\\sqrt{5}}{2}$ and $-\\frac{\\sqrt{5}}{2}$.\nThus, in the rotated coordinate system, the function is:\n$$\nf(u,v) = \\frac{\\sqrt{5}}{2}u^2 - \\frac{\\sqrt{5}}{2}v^2\n$$\nThis form explicitly shows the saddle nature of the function at the origin, with parabolic curvature upwards along the $u$-axis and downwards along the $v$-axis. There is no cross term $uv$.\n\n**Summary of Results**\n- The stationary point is $(0,0)$, which is a saddle point.\n- The principal curvatures are the eigenvalues of the Hessian, which are $\\lambda_{\\text{max}} = \\sqrt{5}$ and $\\lambda_{\\text{min}} = -\\sqrt{5}$.\n- The rotation angle required to diagonalize the Hessian is $\\varphi = \\arctan(\\sqrt{5}-2)$ radians.\n\nThe final answer requires a single row matrix containing the rotation angle $\\varphi$, the larger principal curvature, and the smaller principal curvature, in that order.\n$$\n\\begin{pmatrix} \\varphi  \\lambda_{\\text{max}}  \\lambda_{\\text{min}} \\end{pmatrix} = \\begin{pmatrix} \\arctan(\\sqrt{5}-2)  \\sqrt{5}  -\\sqrt{5} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\arctan(\\sqrt{5}-2)  \\sqrt{5}  -\\sqrt{5} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The standard second-derivative test can be inconclusive, particularly when the Hessian matrix is singular or 'degenerate'. This practice confronts such a scenario, requiring a return to first principles to classify a line of critical points where the Hessian provides incomplete information . By directly examining the function's behavior along different curves passing through a critical point, you will learn a more robust technique for analyzing complex local geometries that standard tests cannot resolve.",
            "id": "3184903",
            "problem": "Let $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ be defined by $f(x,y)=x^{2}y$. Using only core definitions from unconstrained optimization, proceed as follows:\n- Identify the complete set of critical points by solving $\\nabla f(x,y)=\\mathbf{0}$ from first principles.\n- For each critical point, discuss how the Hessian matrix’s rank affects the applicability of the classical second-order test, and explain why the classification may be inconclusive when the Hessian is rank-deficient.\n- Classify the critical points into local minima, local maxima, or saddle points using rigorous arguments based on restrictions of $f$ along curves. In particular, classify the origin by restricting $f$ to rays of the form $(x,y)=t\\mathbf{u}$ with $\\|\\mathbf{u}\\|=1$ and small $t$, and justify your conclusion from first principles without invoking any unproven shortcut formulas.\n- Define the leading-order directional profile $g(u_{x},u_{y})$ by the smallest positive integer $k$ for which the expansion $f(tu_{x},tu_{y})=t^{k}\\,g(u_{x},u_{y})$ holds for all unit directions $(u_{x},u_{y})$ and sufficiently small $t$, and use this $g$ to support your classification of the origin.\n\nReport your final answer as the single closed-form analytic expression for $g(u_{x},u_{y})$. No rounding is required. Do not include units.",
            "solution": "We start from the foundational definitions. A point $(x^{\\star},y^{\\star})$ is a critical point of $f$ if and only if the gradient $\\nabla f(x^{\\star},y^{\\star})=\\mathbf{0}$. The Hessian matrix determines the second-order approximation of $f$ and is used in the classical second-order condition; when the Hessian is singular (rank-deficient), the quadratic form does not capture all nearby variations and the test may be inconclusive, in which case higher-order or directional analyses are required.\n\nStep $1$: Compute the gradient and critical set. The partial derivatives are\n$$\n\\frac{\\partial f}{\\partial x}(x,y)=2xy,\\qquad \\frac{\\partial f}{\\partial y}(x,y)=x^{2}.\n$$\nSetting $\\nabla f(x,y)=\\mathbf{0}$ yields the system\n$$\n2xy=0,\\qquad x^{2}=0.\n$$\nFrom $x^{2}=0$ we obtain $x=0$. Substituting into $2xy=0$ gives $0=0$, which imposes no further restriction on $y$. Therefore, the entire vertical axis is critical:\n$$\n\\mathcal{C}=\\{(0,y):y\\in\\mathbb{R}\\}.\n$$\n\nStep $2$: Examine the Hessian and the second-order test. The Hessian of $f$ is\n$$\n\\nabla^{2} f(x,y)=\\begin{pmatrix}\n\\frac{\\partial^{2}f}{\\partial x^{2}}  \\frac{\\partial^{2}f}{\\partial x\\partial y}\\\\[4pt]\n\\frac{\\partial^{2}f}{\\partial y\\partial x}  \\frac{\\partial^{2}f}{\\partial y^{2}}\n\\end{pmatrix}\n=\\begin{pmatrix}\n2y  2x\\\\\n2x  0\n\\end{pmatrix}.\n$$\nAt a critical point $(0,y_{0})$, this reduces to\n$$\n\\nabla^{2} f(0,y_{0})=\\begin{pmatrix}\n2y_{0}  0\\\\\n0  0\n\\end{pmatrix}.\n$$\nFor $y_{0}\\neq 0$, the Hessian has rank $1$ (one nonzero eigenvalue and one zero eigenvalue), hence it is singular. For $y_{0}=0$ (the origin), the Hessian is the zero matrix and has rank $0$. In all cases on $\\mathcal{C}$ the Hessian is rank-deficient, so the classical second-order test is inconclusive: a positive semidefinite Hessian does not guarantee a local minimum, a negative semidefinite Hessian does not guarantee a local maximum, and a singular Hessian cannot certify a saddle without further analysis.\n\nStep $3$: Classify nonzero critical points $(0,y_{0})$ with $y_{0}\\neq 0$. Fix $y_{0}0$. Consider any sufficiently small neighborhood of $(0,y_{0})$ in which $y0$ is maintained, for instance any ball of radius $r$ with $ry_{0}$. In such a neighborhood, $f(x,y)=x^{2}y\\ge 0$ with equality if and only if $x=0$. Since $f(0,y_{0})=0$, it follows that $(0,y_{0})$ is a local minimum, but not strict, because $f(0,y)=0$ for all $y$ in the neighborhood along the line $x=0$. Similarly, for $y_{0}0$, in a sufficiently small neighborhood one has $y0$, so $f(x,y)=x^{2}y\\le 0$, with equality along $x=0$, implying that $(0,y_{0})$ is a local maximum, again non-strict.\n\nStep $4$: Classify the origin using directional tests beyond second order. At the origin, the Hessian is the zero matrix, so the quadratic approximation vanishes and cannot inform the local behavior. We therefore examine the restriction of $f$ along rays through the origin. Let $\\mathbf{u}=(u_{x},u_{y})$ be any unit direction, so $\\|\\mathbf{u}\\|=1$, and consider the curve $(x,y)=(tu_{x},tu_{y})$ parameterized by $t$. Substituting into $f$ gives\n$$\nf(tu_{x},tu_{y})=(tu_{x})^{2}\\,(tu_{y})=t^{3}\\,u_{x}^{2}\\,u_{y}.\n$$\nThis identity holds exactly for all $t$ because $f$ is a homogeneous polynomial of degree $3$. The leading-order term in $t$ is of order $t^{3}$, and its directional coefficient is the cubic form\n$$\ng(u_{x},u_{y})=u_{x}^{2}\\,u_{y}.\n$$\nBecause there exist unit directions with $g(u_{x},u_{y})0$ (for example, $u_{x}=\\frac{1}{\\sqrt{2}}$, $u_{y}=\\frac{1}{\\sqrt{2}}$) and also unit directions with $g(u_{x},u_{y})0$ (for example, $u_{x}=\\frac{1}{\\sqrt{2}}$, $u_{y}=-\\frac{1}{\\sqrt{2}}$), it follows that in every neighborhood of the origin there are points where $f0$ and points where $f0$. Therefore, the origin is a saddle point. This conclusion illustrates precisely how the rank-deficiency of the Hessian complicates classification: the vanishing quadratic form forces us to appeal to higher-order (here cubic) directional behavior to detect the change of sign that certifies a saddle.\n\nSummary of classifications:\n- For $(0,y_{0})$ with $y_{0}0$: local (non-strict) minimum.\n- For $(0,y_{0})$ with $y_{0}0$: local (non-strict) maximum.\n- For $(0,0)$: saddle point, detected via the directional cubic profile $g(u_{x},u_{y})$.\n\nPer the problem’s reporting requirement, the analytic expression for the leading-order directional profile is $g(u_{x},u_{y})=u_{x}^{2}u_{y}$.",
            "answer": "$$\\boxed{u_{x}^{2}u_{y}}$$"
        },
        {
            "introduction": "Why is the distinction between local minima and saddle points so critical for the performance of optimization algorithms? This computational exercise provides a hands-on answer by simulating how two fundamental methods behave near a strict saddle point. By implementing and comparing the trajectories of Gradient Descent and Newton's method, you will observe firsthand how first-order methods can escape saddle points while pure second-order methods can become trapped, illustrating a key challenge in modern numerical optimization .",
            "id": "3184951",
            "problem": "You are to write a complete, runnable program that compares the local escape dynamics of gradient descent and Newton’s method near a strict saddle point for the bivariate function defined by the rule: given a scalar parameter $\\epsilon \\in \\mathbb{R}$, define $f:\\mathbb{R}^2 \\to \\mathbb{R}$ by\n$$\nf(x,y) = x^2 - y^2 + \\epsilon\\,(x^3 + y^3).\n$$\nStart from core definitions in optimization methods. A stationary point is any point where the gradient equals the zero vector, and a strict saddle point is a stationary point at which the Hessian matrix is indefinite. Use these principles to derive, by analytic differentiation from first principles, the gradient $\\nabla f(x,y)$ and the Hessian matrix $\\nabla^2 f(x,y)$. Based only on these derivatives and the canonical update rules derived from first- and second-order Taylor expansions, implement the following two iterative methods:\n- Gradient descent with a fixed step size $\\alpha  0$: iterates $(x_{k+1},y_{k+1})$ from $(x_k,y_k)$ using the negative gradient direction.\n- Newton’s method with full step size $\\alpha_{\\text{Newton}} = 1$: computes the step by solving a linear system defined by the Hessian and the gradient at the current iterate.\n\nYour program must simulate both methods from the same initial condition and compare their “escape” behavior from a small neighborhood of the origin, which is a strict saddle for small $|\\epsilon|$. For a given escape radius $R  0$ and maximum iteration count $M \\in \\mathbb{N}$:\n- Define the escape time for a method as the smallest integer $k \\in \\{1,2,\\dots,M\\}$ such that $\\sqrt{x_k^2 + y_k^2} \\ge R$. If no such $k$ exists within $M$ iterations, report the escape time as $-1$.\n- Begin counting after performing the first update from the initial point; do not count the starting point as escaped, even if it already satisfies the threshold.\n\nScientific realism requirement: ensure that your implementation directly uses the analytically derived $\\nabla f$ and $\\nabla^2 f$ for updates. When computing the Newton step, solve the $2\\times 2$ linear system exactly for the step direction whenever the Hessian is invertible; if a singularity is encountered numerically, treat that trajectory as non-escaping within the iteration budget and report $-1$ for that method in that test.\n\nAngle units are not involved in this problem. No physical units are involved.\n\nTest suite. Your program must evaluate the following five parameter sets, each specified as a tuple $(\\epsilon,\\alpha,x_0,y_0,R,M)$, where $\\epsilon$ is the cubic coefficient, $\\alpha$ is the gradient descent step size, $(x_0,y_0)$ is the initial point, $R$ is the escape radius, and $M$ is the iteration cap:\n- Case $1$: $(\\epsilon,\\alpha,x_0,y_0,R,M) = (0.01,\\,0.1,\\,0.05,\\,0.05,\\,1.0,\\,200)$.\n- Case $2$: $(\\epsilon,\\alpha,x_0,y_0,R,M) = (0.01,\\,0.1,\\,0.05,\\,0.0,\\,1.0,\\,200)$.\n- Case $3$: $(\\epsilon,\\alpha,x_0,y_0,R,M) = (-0.01,\\,0.1,\\,0.05,\\,0.05,\\,1.0,\\,200)$.\n- Case $4$: $(\\epsilon,\\alpha,x_0,y_0,R,M) = (0.2,\\,0.1,\\,0.05,\\,0.05,\\,1.0,\\,200)$.\n- Case $5$: $(\\epsilon,\\alpha,x_0,y_0,R,M) = (0.01,\\,0.02,\\,0.05,\\,0.05,\\,1.0,\\,200)$.\n\nFor each case, your program must compute two integers: the escape time for gradient descent and the escape time for Newton’s method. The required final output format is a single line containing a list of lists, where each inner list has the form $[\\text{gd\\_steps},\\text{newton\\_steps}]$ for the corresponding case, with no additional text. For example, a line of the form\n$[[\\text{gd}_1,\\text{nt}_1],[\\text{gd}_2,\\text{nt}_2],\\dots,[\\text{gd}_5,\\text{nt}_5]]$\nwhere each $\\text{gd}_i$ and $\\text{nt}_i$ is an integer, must be printed by your program. The numbers should not include any units and should be printed as plain integers.",
            "solution": "The user-provided problem is valid. It is scientifically grounded in the principles of numerical optimization, well-posed with a clear objective and sufficient data, and stated in objective, formal language. We will proceed with a full solution.\n\nThe core of the problem is to compare the local dynamics of two fundamental optimization algorithms—Gradient Descent and Newton's Method—in the vicinity of a strict saddle point. The analysis is performed on the bivariate function $f: \\mathbb{R}^2 \\to \\mathbb{R}$ defined by\n$$\nf(x,y) = x^2 - y^2 + \\epsilon(x^3 + y^3)\n$$\nwhere $\\epsilon$ is a real-valued parameter. A strict saddle point is a stationary point where the Hessian matrix is indefinite (has both positive and negative eigenvalues).\n\nTo implement the algorithms, we must first derive the gradient vector $\\nabla f(x,y)$ and the Hessian matrix $\\nabla^2 f(x,y)$ from first principles.\n\nThe partial derivatives of $f(x,y)$ are:\n$$\n\\frac{\\partial f}{\\partial x} = 2x + 3\\epsilon x^2\n$$\n$$\n\\frac{\\partial f}{\\partial y} = -2y + 3\\epsilon y^2\n$$\nThe gradient vector $\\nabla f(x,y)$ is therefore:\n$$\n\\nabla f(x,y) = \\begin{pmatrix} 2x + 3\\epsilon x^2 \\\\ -2y + 3\\epsilon y^2 \\end{pmatrix}\n$$\nA stationary point occurs where $\\nabla f(x,y) = \\mathbf{0}$. By inspection, the origin $(x,y) = (0,0)$ is a stationary point for any value of $\\epsilon$.\n\nNext, we compute the second-order partial derivatives to form the Hessian matrix:\n$$\n\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x}(2x + 3\\epsilon x^2) = 2 + 6\\epsilon x\n$$\n$$\n\\frac{\\partial^2 f}{\\partial y^2} = \\frac{\\partial}{\\partial y}(-2y + 3\\epsilon y^2) = -2 + 6\\epsilon y\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial x}(-2y + 3\\epsilon y^2) = 0\n$$\nSince $f$ is a polynomial, its mixed partial derivatives are continuous, so $\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y}$. The Hessian matrix $\\nabla^2 f(x,y)$ is:\n$$\n\\nabla^2 f(x,y) = \\begin{pmatrix} 2 + 6\\epsilon x  0 \\\\ 0  -2 + 6\\epsilon y \\end{pmatrix}\n$$\nTo verify that the origin is a strict saddle point, we evaluate the Hessian at $(0,0)$:\n$$\n\\nabla^2 f(0,0) = \\begin{pmatrix} 2  0 \\\\ 0  -2 \\end{pmatrix}\n$$\nThe eigenvalues of this matrix are $\\lambda_1 = 2$ and $\\lambda_2 = -2$. Since one eigenvalue is positive and one is negative, the Hessian is indefinite at the origin, confirming that $(0,0)$ is a strict saddle point for any $\\epsilon$. The quadratic part of the function, $x^2 - y^2$, creates a saddle geometry where the function increases along the $x$-axis (the stable manifold's direction for gradient descent) and decreases along the $y$-axis (the unstable manifold's direction).\n\nWith these derivatives, we can specify the iterative update rules. For an iterate $\\mathbf{x}_k = (x_k, y_k)^T$:\n\n**1. Gradient Descent (GD)**\nThe update rule for gradient descent with a fixed step size $\\alpha  0$ is based on moving in the direction opposite to the gradient:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)\n$$\nIn component form, the updates are:\n$$\nx_{k+1} = x_k - \\alpha (2x_k + 3\\epsilon x_k^2) = x_k(1 - 2\\alpha) - 3\\alpha\\epsilon x_k^2\n$$\n$$\ny_{k+1} = y_k - \\alpha (-2y_k + 3\\epsilon y_k^2) = y_k(1 + 2\\alpha) - 3\\alpha\\epsilon y_k^2\n$$\nNear the origin, the linear terms dominate. The $x$-component is multiplied by a factor of $(1 - 2\\alpha)$. For a small step size like $\\alpha=0.1$, this factor is $0.8$, causing the $x$-component to decay. The $y$-component is multiplied by a factor of $(1 + 2\\alpha)$, which is $1.2$. This factor is greater than $1$, causing the $y$-component to grow exponentially. This exponential growth along the unstable direction (related to the negative eigenvalue of the Hessian) allows the gradient descent iterates to rapidly \"escape\" the saddle point, provided the initial point has a non-zero component in this direction. If an initial point lies on the stable manifold (here, the $x$-axis, where $y_0 = 0$), the iterates will converge to the saddle point.\n\n**2. Newton's Method (NT)**\nNewton's method uses a second-order Taylor approximation of the function. The update step $\\Delta \\mathbf{x}_k$ is found by solving the linear system:\n$$\n\\nabla^2 f(\\mathbf{x}_k) \\Delta \\mathbf{x}_k = - \\nabla f(\\mathbf{x}_k)\n$$\nThe next iterate is then $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\Delta \\mathbf{x}_k$ (using the specified full step size of $1$). Since our Hessian is diagonal, the solution is straightforward, provided the diagonal elements are non-zero:\n$$\n\\Delta x_k = -\\frac{2x_k + 3\\epsilon x_k^2}{2 + 6\\epsilon x_k} \\quad \\text{and} \\quad \\Delta y_k = -\\frac{-2y_k + 3\\epsilon y_k^2}{-2 + 6\\epsilon y_k}\n$$\nThe updates are:\n$$\nx_{k+1} = x_k + \\Delta x_k = x_k - \\frac{2x_k + 3\\epsilon x_k^2}{2 + 6\\epsilon x_k} = \\frac{x_k(2+6\\epsilon x_k) - (2x_k + 3\\epsilon x_k^2)}{2 + 6\\epsilon x_k} = \\frac{3\\epsilon x_k^2}{2 + 6\\epsilon x_k}\n$$\n$$\ny_{k+1} = y_k + \\Delta y_k = y_k - \\frac{-2y_k + 3\\epsilon y_k^2}{-2 + 6\\epsilon y_k} = \\frac{y_k(-2+6\\epsilon y_k) - (-2y_k + 3\\epsilon y_k^2)}{-2 + 6\\epsilon y_k} = \\frac{3\\epsilon y_k^2}{-2 + 6\\epsilon y_k}\n$$\nNear the origin, where $x_k$ and $y_k$ are small, these updates show that $x_{k+1} \\approx \\frac{3\\epsilon}{2} x_k^2$ and $y_{k+1} \\approx -\\frac{3\\epsilon}{2} y_k^2$. Both components converge quadratically to $0$. This demonstrates that the pure form of Newton's method, when applied near a strict saddle point, does not automatically escape. Instead, it is strongly attracted to the saddle point itself. This behavior motivates the development of modified Newton methods (e.g., trust-region or cubic regularization) that are designed to exploit negative curvature for fast escape. The simulation must handle potential singularity if a denominator term $2+6\\epsilon x_k$ or $-2+6\\epsilon y_k$ becomes numerically zero.\n\nThe program will implement two functions, one for each method, that simulate the trajectory from an initial point $(x_0, y_0)$. Each function will iterate up to a maximum of $M$ times, checking at each step $k$ if the distance from the origin, $\\sqrt{x_k^2 + y_k^2}$, has reached or exceeded the escape radius $R$. The first $k \\ge 1$ for which this occurs is the escape time. If no escape occurs by iteration $M$, the escape time is reported as $-1$. This logic will be applied to each test case provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem by simulating Gradient Descent and Newton's\n    method to compare their escape dynamics from a saddle point.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: (epsilon, alpha, x0, y0, R, M)\n        (0.01, 0.1, 0.05, 0.05, 1.0, 200),\n        # Case 2: Initial point on the stable manifold\n        (0.01, 0.1, 0.05, 0.0, 1.0, 200),\n        # Case 3: Negative epsilon\n        (-0.01, 0.1, 0.05, 0.05, 1.0, 200),\n        # Case 4: Larger epsilon\n        (0.2, 0.1, 0.05, 0.05, 1.0, 200),\n        # Case 5: Smaller GD step size\n        (0.01, 0.02, 0.05, 0.05, 1.0, 200),\n    ]\n\n    def simulate_gd(params):\n        \"\"\"\n        Simulates Gradient Descent for a given parameter set.\n        \n        Args:\n            params (tuple): A tuple containing (epsilon, alpha, x0, y0, R, M).\n\n        Returns:\n            int: The escape time, or -1 if not escaped.\n        \"\"\"\n        epsilon, alpha, x0, y0, r_escape, m_iter = params\n        x_k, y_k = float(x0), float(y0)\n        r_escape_sq = r_escape**2\n\n        for k in range(1, int(m_iter) + 1):\n            # Gradient components\n            grad_x = 2.0 * x_k + 3.0 * epsilon * x_k**2\n            grad_y = -2.0 * y_k + 3.0 * epsilon * y_k**2\n            \n            # Update step\n            x_k = x_k - alpha * grad_x\n            y_k = y_k - alpha * grad_y\n\n            # Check for escape\n            if x_k**2 + y_k**2 >= r_escape_sq:\n                return k\n        \n        return -1\n\n    def simulate_newton(params):\n        \"\"\"\n        Simulates Newton's Method for a given parameter set.\n        \n        Args:\n            params (tuple): A tuple containing (epsilon, alpha, x0, y0, R, M).\n                          alpha is unused for Newton's method.\n\n        Returns:\n            int: The escape time, or -1 if not escaped or singular.\n        \"\"\"\n        epsilon, _, x0, y0, r_escape, m_iter = params\n        x_k, y_k = float(x0), float(y0)\n        r_escape_sq = r_escape**2\n        singularity_threshold = 1e-12\n\n        for k in range(1, int(m_iter) + 1):\n            # Hessian diagonal components\n            h11 = 2.0 + 6.0 * epsilon * x_k\n            h22 = -2.0 + 6.0 * epsilon * y_k\n\n            # Check for numerical singularity\n            if abs(h11)  singularity_threshold or abs(h22)  singularity_threshold:\n                return -1\n            \n            # Gradient components\n            grad_x = 2.0 * x_k + 3.0 * epsilon * x_k**2\n            grad_y = -2.0 * y_k + 3.0 * epsilon * y_k**2\n\n            # Newton step\n            delta_x = -grad_x / h11\n            delta_y = -grad_y / h22\n\n            # Update step\n            x_k = x_k + delta_x\n            y_k = y_k + delta_y\n\n            # Check for escape\n            if x_k**2 + y_k**2 >= r_escape_sq:\n                return k\n\n        return -1\n        \n    results = []\n    for case in test_cases:\n        gd_steps = simulate_gd(case)\n        newton_steps = simulate_newton(case)\n        results.append([gd_steps, newton_steps])\n\n    # Final print statement in the exact required format.\n    # The format string converts the list of lists to a string without spaces.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}