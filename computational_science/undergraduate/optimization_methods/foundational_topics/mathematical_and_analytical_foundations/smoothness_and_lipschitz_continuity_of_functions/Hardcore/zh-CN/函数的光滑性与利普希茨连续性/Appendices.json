{
    "hands_on_practices": [
        {
            "introduction": "理论的理解始于实践。这个练习将带你深入研究两种不同类型的利普希茨连续性。我们将为一个在机器学习中常见的非光滑函数（最大化函数）计算其函数本身的利普希茨常数，并为其光滑近似形式（log-sum-exp函数）计算其梯度的利普希茨常数（即光滑度参数）。通过这个练习，你不仅能掌握这些重要参数的计算方法，还能体会到“光滑化”这一将非光滑问题转化为可用梯度方法解决的关键技巧。",
            "id": "3183315",
            "problem": "令 $m=3$ 且 $d=2$。考虑由下式定义的凸函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$\n$$\nf(x)=\\max_{i\\in\\{1,2,3\\}} a_{i}^{\\top}x,\n$$\n其中 $a_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$，$a_{2}=\\begin{pmatrix}0\\\\2\\end{pmatrix}$，以及 $a_{3}=\\begin{pmatrix}-1\\\\-1\\end{pmatrix}$。在欧几里得范数 $\\|\\cdot\\|_{2}$ 下进行计算。对于任意 $\\gamma>0$，定义平滑近似（log-sum-exp 平滑）$f_{\\gamma}:\\mathbb{R}^{2}\\to\\mathbb{R}$ 为\n$$\nf_{\\gamma}(x)=\\gamma\\log\\!\\left(\\sum_{i=1}^{3}\\exp\\!\\left(\\frac{a_{i}^{\\top}x}{\\gamma}\\right)\\right).\n$$\n从利普希茨连续性和梯度利普希茨连续性的基本定义出发，确定：\n1. 关于范数 $\\|\\cdot\\|_{2}$，函数 $f$ 的最小利普希茨常数。\n2. 关于范数 $\\|\\cdot\\|_{2}$，梯度 $\\nabla f_{\\gamma}(x)$ 的最小全局利普希茨常数，表示为 $\\gamma$ 的闭式解析函数 $L(\\gamma)$。\n\n将你的最终答案表示为一个行矩阵，其第一个元素是 $f$ 的利普希茨常数，第二个元素是 $L(\\gamma)$。无需四舍五入。",
            "solution": "用户要求我解决一个分为两部分的问题，该问题与一个特定的凸函数 $f(x)$ 及其平滑近似 $f_\\gamma(x)$ 相关。\n\n**第一部分：$f(x)$ 的利普希茨常数**\n\n一个函数 $f:\\mathbb{R}^{d}\\to\\mathbb{R}$ 被称为关于范数 $\\|\\cdot\\|$ 是 $L_f$-利普希茨连续的，如果存在一个常数 $L_f \\ge 0$，使得对于所有 $x, y \\in \\mathbb{R}^{d}$，不等式成立：\n$$|f(x) - f(y)| \\le L_f \\|x - y\\|$$\n满足条件的最小常数 $L_f$ 是 $f$ 的利普希茨常数。\n\n对于凸函数，其利普希茨常数由该函数次梯度的最大可能范数给出。用于次梯度的范数是用于定义域范数的对偶范数。鉴于我们使用的是自对偶的欧几里得范数 $\\|\\cdot\\|_{2}$，利普希茨常数为：\n$$L_f = \\sup_{x \\in \\mathbb{R}^{d}} \\sup_{g \\in \\partial f(x)} \\|g\\|_{2}$$\n其中 $\\partial f(x)$ 是 $f$ 在 $x$ 处的次微分。\n\n给定的函数是 $f(x) = \\max_{i \\in \\{1, 2, 3\\}} a_{i}^{\\top}x$。这是一个有限个线性函数的最大值，是一个著名的凸函数。$f(x)$ 在点 $x$ 处的次微分是活跃线性函数（即在 $x$ 处达到最大值的函数）梯度的凸包：\n$$\\partial f(x) = \\text{conv}\\{a_i \\mid a_i^\\top x = f(x)\\}$$\n在整个定义域上所有可能的次梯度的集合是所有向量 $a_i$ 的凸包：\n$$\\bigcup_{x \\in \\mathbb{R}^{d}} \\partial f(x) = \\text{conv}\\{a_1, a_2, ..., a_m\\}$$\n在我们的例子中，$m=3$，所以这个集合是 $\\text{conv}\\{a_1, a_2, a_3\\}$。\n\n利普希茨常数是该集合中任意元素范数的上确界：\n$$L_f = \\sup_{g \\in \\text{conv}\\{a_1, a_2, a_3\\}} \\|g\\|_2$$\n由于范数 $\\|\\cdot\\|_2$ 是一个凸函数，它在一个紧凸集（有限点集的凸包是紧凸的）上的最大值在该集合的一个极点上达到。$\\text{conv}\\{a_1, a_2, a_3\\}$ 的极点是 $\\{a_1, a_2, a_3\\}$ 的一个子集。因此，问题简化为在给定向量 $a_i$ 中找到最大范数：\n$$L_f = \\max_{i \\in \\{1, 2, 3\\}} \\|a_i\\|_2$$\n给定的向量是 $a_1=\\begin{pmatrix}1\\\\0\\end{pmatrix}$，$a_2=\\begin{pmatrix}0\\\\2\\end{pmatrix}$，以及 $a_3=\\begin{pmatrix}-1\\\\-1\\end{pmatrix}$。我们计算它们的欧几里得范数：\n$$\\|a_1\\|_2 = \\sqrt{1^2 + 0^2} = \\sqrt{1} = 1$$\n$$\\|a_2\\|_2 = \\sqrt{0^2 + 2^2} = \\sqrt{4} = 2$$\n$$\\|a_3\\|_2 = \\sqrt{(-1)^2 + (-1)^2} = \\sqrt{1+1} = \\sqrt{2}$$\n这些值的最大值是：\n$$L_f = \\max\\{1, 2, \\sqrt{2}\\} = 2$$\n\n**第二部分：$f_\\gamma(x)$ 的梯度利普希茨常数**\n\n如果对于所有 $x, y \\in \\mathbb{R}^{d}$，一个可微函数 $g:\\mathbb{R}^{d}\\to\\mathbb{R}$ 具有带常数 $L$ 的利普希茨连续梯度（也称为 $L$-平滑性），则：\n$$\\|\\nabla g(x) - \\nabla g(y)\\|_2 \\le L \\|x - y\\|_2$$\n对于一个二阶可微函数，最小的此类常数 $L$ 是其海森矩阵在整个定义域上谱范数（对于对称半正定矩阵，即最大特征值）的上确界：\n$$L = \\sup_{x \\in \\mathbb{R}^{d}} \\|\\nabla^2 g(x)\\|_2 = \\sup_{x \\in \\mathbb{R}^{d}} \\lambda_{\\max}(\\nabla^2 g(x))$$\n函数是 $f_{\\gamma}(x)=\\gamma\\log\\left(\\sum_{i=1}^{3}\\exp\\left(\\frac{a_{i}^{\\top}x}{\\gamma}\\right)\\right)$。我们来计算它的梯度和海森矩阵。\n令 $p_i(x) = \\frac{\\exp(a_i^\\top x / \\gamma)}{\\sum_{k=1}^3 \\exp(a_k^\\top x / \\gamma)}$。注意 $p_i(x) > 0$ 且 $\\sum_{i=1}^3 p_i(x) = 1$。\n$f_\\gamma(x)$ 的梯度是：\n$$\\nabla f_\\gamma(x) = \\sum_{i=1}^3 p_i(x) a_i$$\n海森矩阵 $\\nabla^2 f_\\gamma(x)$ 的元素为 $(\\nabla^2 f_\\gamma(x))_{jk} = \\frac{\\partial^2 f_\\gamma(x)}{\\partial x_j \\partial x_k}$。一个标准的计算得出：\n$$\\nabla^2 f_\\gamma(x) = \\frac{1}{\\gamma} \\left( \\sum_{i=1}^3 p_i(x) a_i a_i^\\top - \\left(\\sum_{i=1}^3 p_i(x) a_i\\right) \\left(\\sum_{i=1}^3 p_i(x) a_i\\right)^\\top \\right)$$\n这个矩阵表示一个向量值随机变量 $A$ 的协方差矩阵，该随机变量以概率 $p_i(x)$ 取值 $a_i$，并由 $\\frac{1}{\\gamma}$ 缩放。令 $\\bar{a}(x) = \\mathbb{E}_{p(x)}[A] = \\sum_i p_i(x) a_i$。那么 $\\nabla^2 f_\\gamma(x) = \\frac{1}{\\gamma} \\text{Cov}_{p(x)}(A) = \\frac{1}{\\gamma}(\\mathbb{E}_{p(x)}[AA^\\top] - \\bar{a}(x)\\bar{a}(x)^\\top)$。\n\n梯度的最小全局利普希茨常数 $L(\\gamma)$ 是：\n$$L(\\gamma) = \\sup_{x \\in \\mathbb{R}^2} \\|\\nabla^2 f_\\gamma(x)\\|_2 = \\frac{1}{\\gamma} \\sup_{x \\in \\mathbb{R}^2} \\lambda_{\\max}(\\text{Cov}_{p(x)}(A))$$\n谱范数为 $\\lambda_{\\max}(M) = \\sup_{\\|v\\|_2=1} v^\\top M v$。对于协方差矩阵，我们有：\n$$v^\\top (\\text{Cov}_{p(x)}(A)) v = \\mathbb{E}_{p(x)}[(v^\\top A)^2] - (\\mathbb{E}_{p(x)}[v^\\top A])^2 = \\text{Var}_{p(x)}(a^\\top v)$$\n值 $p(x)$ 可以任意接近概率单纯形 $\\Delta_3 = \\{p \\in \\mathbb{R}^3 \\mid p_i \\ge 0, \\sum p_i = 1\\}$ 中的任何点。因此，我们需要找到在 $p \\in \\Delta_3$ 上的上确界：\n$$L(\\gamma) = \\frac{1}{\\gamma} \\sup_{p \\in \\Delta_3} \\sup_{\\|v\\|_2=1} \\text{Var}_p(a^\\top v) = \\frac{1}{\\gamma} \\sup_{\\|v\\|_2=1} \\sup_{p \\in \\Delta_3} \\text{Var}_p(a^\\top v)$$\n对于一个固定向量 $v$，令 $c_i = a_i^\\top v$。我们想要求 $\\sup_{p \\in \\Delta_3} \\text{Var}_p(c)$。具有有限支撑的随机变量的方差由 $\\frac{1}{4}(\\max(c) - \\min(c))^2$ 界定。当概率质量在最大值和最小值上各分配为 $\\frac{1}{2}$ 时，达到这个最大值。因此：\n$$\\sup_{p \\in \\Delta_3} \\text{Var}_p(c) = \\frac{1}{4} \\max_{i,j} (c_i - c_j)^2 = \\frac{1}{4} \\max_{i,j} ((a_i - a_j)^\\top v)^2$$\n将此代回 $L(\\gamma)$ 的表达式中：\n$$L(\\gamma) = \\frac{1}{\\gamma} \\sup_{\\|v\\|_2=1} \\left( \\frac{1}{4} \\max_{i,j} ((a_i - a_j)^\\top v)^2 \\right) = \\frac{1}{4\\gamma} \\max_{i,j} \\left( \\sup_{\\|v\\|_2=1} ((a_i - a_j)^\\top v)^2 \\right)$$\n根据对偶范数的定义和柯西-施瓦茨不等式，$\\sup_{\\|v\\|_2=1} ((a_i - a_j)^\\top v)^2 = \\|a_i - a_j\\|_2^2$。因此，最小全局利普希茨常数是：\n$$L(\\gamma) = \\frac{1}{4\\gamma} \\max_{i,j \\in \\{1,2,3\\}} \\|a_i - a_j\\|_2^2$$\n我们现在计算向量之间差的平方范数：\n$$a_1 - a_2 = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}0 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}1 \\\\ -2\\end{pmatrix} \\implies \\|a_1 - a_2\\|_2^2 = 1^2 + (-2)^2 = 5$$\n$$a_1 - a_3 = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}-1 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}2 \\\\ 1\\end{pmatrix} \\implies \\|a_1 - a_3\\|_2^2 = 2^2 + 1^2 = 5$$\n$$a_2 - a_3 = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix} - \\begin{pmatrix}-1 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 3\\end{pmatrix} \\implies \\|a_2 - a_3\\|_2^2 = 1^2 + 3^2 = 10$$\n这些值的最大值是 $10$。\n将此代入 $L(\\gamma)$ 的公式中：\n$$L(\\gamma) = \\frac{10}{4\\gamma} = \\frac{5}{2\\gamma}$$\n\n最终答案是一个行矩阵，包含 $f$ 的利普希茨常数 $2$ 和 $\\nabla f_\\gamma$ 的利普希茨常数 $\\frac{5}{2\\gamma}$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & \\frac{5}{2\\gamma}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "掌握了如何计算光滑度常数后，我们更进一步，学习如何主动地调整它以优化算法性能。在优化中，我们常常通过添加正则项来改善问题的“性状”（conditioning），这不仅有助于防止过拟合，还能加速收敛。这个练习将量化这一过程：你将分析添加二次正则项如何同时改变函数的光滑度和强凸性，并在此基础上，通过优化正则化系数来最小化决定算法收敛速度的关键因子——条件数。",
            "id": "3183345",
            "problem": "设 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ 是一个可微凸函数，其梯度是 $L$-利普希茨连续的，即对所有 $x,y\\in\\mathbb{R}^{n}$ 都有 $\\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\,\\|x-y\\|$，并假设 $f$ 是 $\\mu_{0}$-强凸的，即对所有 $x,y\\in\\mathbb{R}^{n}$ 都有 $f(y)\\geq f(x)+\\nabla f(x)^{\\top}(y-x)+\\frac{\\mu_{0}}{2}\\|y-x\\|^{2}$，其中 $L>\\mu_{0}\\geq 0$。考虑添加一个二次正则化项来构成\n$$\nF_{\\alpha}(x)\\;=\\;f(x)\\;+\\;\\frac{\\alpha}{2}\\,\\|x\\|^{2},\n$$\n其中 $\\alpha\\geq 0$ 是一个可调参数。仅使用梯度利普希茨连续性和强凸性的基本定义，确定 $F_{\\alpha}$ 的平滑度常数 $L_{\\alpha}$ 和强凸性模数 $\\mu_{\\alpha}$。\n\n在光滑强凸优化中，Nesterov 加速梯度（NAG）方法的迭代复杂度，在不考虑与 $\\alpha$ 无关的乘法常数的情况下，与 $\\sqrt{L_{\\alpha}/\\mu_{\\alpha}}$ 乘以目标精度的对数因子成正比。为了量化在“不过多改变 $L$”的情况下增加强凸性的权衡，施加工程约束 $L_{\\alpha}\\leq (1+\\rho)\\,L$，其中给定参数 $0 < \\rho < 1$。选择 $\\alpha$ 以在该约束条件下最小化主导的条件数因子 $\\sqrt{L_{\\alpha}/\\mu_{\\alpha}}$，并将最小化的因子表示为关于 $L$、$\\mu_{0}$ 和 $\\rho$ 的单个闭式表达式。不要对您的答案进行四舍五入。",
            "solution": "用户希望我解决一个优化背景下的两部分问题。\n\n**第 1 部分：求正则化函数的光滑度和强凸性常数。**\n\n1.  **分析函数：**新函数为 $F_{\\alpha}(x) = f(x) + \\frac{\\alpha}{2} \\|x\\|^2$。\n2.  **定义性质：**\n    *   **光滑性（L-光滑）：**如果一个函数 $g$ 的梯度是 $L_g$-利普希茨连续的，即 $\\|\\nabla g(x) - \\nabla g(y)\\| \\leq L_g \\|x - y\\|$，则该函数是 $L_g$-光滑的。\n    *   **强凸性（$\\mu$-强凸）：**如果一个可微函数 $g$ 满足 $g(y) \\geq g(x) + \\nabla g(x)^T(y-x) + \\frac{\\mu_g}{2} \\|y-x\\|^2$，则该函数是 $\\mu_g$-强凸的。\n3.  **将这些定义应用于 $F_{\\alpha}(x)$。**\n\n    *   **$F_{\\alpha}$ 的光滑性：**\n        *   首先，我需要 $F_{\\alpha}(x)$ 的梯度。\n        *   $f(x)$ 的梯度是 $\\nabla f(x)$。\n        *   正则化项是 $g(x) = \\frac{\\alpha}{2} \\|x\\|^2 = \\frac{\\alpha}{2} \\sum_{i=1}^n x_i^2$。\n        *   正则化项的梯度是 $\\nabla g(x) = \\alpha x$。\n        *   所以，$\\nabla F_{\\alpha}(x) = \\nabla f(x) + \\alpha x$。\n        *   现在，我需要检验 $\\nabla F_{\\alpha}(x)$ 的利普希茨连续性。\n        *   $\\|\\nabla F_{\\alpha}(x) - \\nabla F_{\\alpha}(y)\\| = \\|(\\nabla f(x) + \\alpha x) - (\\nabla f(y) + \\alpha y)\\|$。\n        *   $= \\|(\\nabla f(x) - \\nabla f(y)) + \\alpha(x - y)\\|$。\n        *   使用三角不等式：$\\|\\nabla F_{\\alpha}(x) - \\nabla F_{\\alpha}(y)\\| \\leq \\|\\nabla f(x) - \\nabla f(y)\\| + \\|\\alpha(x - y)\\|$。\n        *   $= \\|\\nabla f(x) - \\nabla f(y)\\| + |\\alpha| \\|x - y\\|$。\n        *   因为 $\\alpha \\geq 0$，所以这是 $\\|\\nabla f(x) - \\nabla f(y)\\| + \\alpha \\|x - y\\|$。\n        *   我们已知 $\\nabla f$ 是 $L$-利普希茨的，所以 $\\|\\nabla f(x) - \\nabla f(y)\\| \\leq L \\|x - y\\|$。\n        *   将其代入，我们得到 $\\|\\nabla F_{\\alpha}(x) - \\nabla F_{\\alpha}(y)\\| \\leq L \\|x - y\\| + \\alpha \\|x - y\\| = (L + \\alpha) \\|x - y\\|$。\n        *   这意味着 $\\nabla F_{\\alpha}$ 是 $(L+\\alpha)$-利普希茨连续的。\n        *   因此，$F_{\\alpha}$ 的光滑度常数是 $L_{\\alpha} = L + \\alpha$。\n\n    *   **$F_{\\alpha}$ 的强凸性：**\n        *   我需要使用定义：$F_{\\alpha}(y) \\geq F_{\\alpha}(x) + \\nabla F_{\\alpha}(x)^T(y-x) + \\frac{\\mu_{\\alpha}}{2} \\|y-x\\|^2$。\n        *   让我们展开这些项。$F_{\\alpha}(y) = f(y) + \\frac{\\alpha}{2} \\|y\\|^2$。\n        *   $F_{\\alpha}(x) = f(x) + \\frac{\\alpha}{2} \\|x\\|^2$。\n        *   $\\nabla F_{\\alpha}(x) = \\nabla f(x) + \\alpha x$。\n        *   不等式变为：\n            $f(y) + \\frac{\\alpha}{2} \\|y\\|^2 \\geq (f(x) + \\frac{\\alpha}{2} \\|x\\|^2) + (\\nabla f(x) + \\alpha x)^T(y-x) + \\frac{\\mu_{\\alpha}}{2} \\|y-x\\|^2$。\n        *   这看起来很复杂。我们尝试另一种方法。\n        *   一个函数 $g$ 是 $\\mu_g$-强凸的，当且仅当函数 $h(x) = g(x) - \\frac{\\mu_g}{2} \\|x\\|^2$ 是凸的。\n        *   我们直接使用定义，但更巧妙一些。\n        *   设 $g(x) = \\frac{\\alpha}{2} \\|x\\|^2$。这是一个二次函数。\n        *   我们知道 $f$ 是 $\\mu_0$-强凸的：\n            $f(y) \\geq f(x) + \\nabla f(x)^T(y-x) + \\frac{\\mu_0}{2} \\|y-x\\|^2$。\n        *   $g(x)$ 呢？$\\nabla g(x) = \\alpha x$。\n            $g(y) = \\frac{\\alpha}{2} \\|y\\|^2$。\n            $g(x) + \\nabla g(x)^T(y-x) + \\frac{\\alpha}{2} \\|y-x\\|^2 = \\frac{\\alpha}{2} \\|x\\|^2 + (\\alpha x)^T(y-x) + \\frac{\\alpha}{2} (\\|y\\|^2 - 2y^T x + \\|x\\|^2)$。\n            $= \\frac{\\alpha}{2} \\|x\\|^2 + \\alpha y^T x - \\alpha \\|x\\|^2 + \\frac{\\alpha}{2} \\|y\\|^2 - \\alpha y^T x + \\frac{\\alpha}{2} \\|x\\|^2$。\n            $= \\frac{\\alpha}{2} \\|y\\|^2 = g(y)$。\n        *   所以，对于 $g(x) = \\frac{\\alpha}{2} \\|x\\|^2$，我们有等式 $g(y) = g(x) + \\nabla g(x)^T(y-x) + \\frac{\\alpha}{2} \\|y-x\\|^2$。这意味着 $g(x)$ 恰好是 $\\alpha$-强凸的。\n        *   现在考虑 $F_{\\alpha}(x) = f(x) + g(x)$。\n        *   一个 $\\mu_1$-强凸函数与一个 $\\mu_2$-强凸函数的和是 $(\\mu_1 + \\mu_2)$-强凸的。\n        *   我们来证明这一点。设 $h_1$ 是 $\\mu_1$-强凸的，$h_2$ 是 $\\mu_2$-强凸的。\n            $h_1(y) \\geq h_1(x) + \\nabla h_1(x)^T(y-x) + \\frac{\\mu_1}{2} \\|y-x\\|^2$。\n            $h_2(y) \\geq h_2(x) + \\nabla h_2(x)^T(y-x) + \\frac{\\mu_2}{2} \\|y-x\\|^2$。\n        *   将这两个不等式相加：\n            $(h_1(y) + h_2(y)) \\geq (h_1(x) + h_2(x)) + (\\nabla h_1(x) + \\nabla h_2(x))^T(y-x) + \\frac{(\\mu_1 + \\mu_2)}{2} \\|y-x\\|^2$。\n        *   这正是 $h_1+h_2$ 以模数 $\\mu_1 + \\mu_2$ 的强凸性定义。\n        *   在我们的例子中，$f(x)$ 是 $\\mu_0$-强凸的，而 $g(x) = \\frac{\\alpha}{2} \\|x\\|^2$ 是 $\\alpha$-强凸的。\n        *   因此，$F_{\\alpha}(x) = f(x) + g(x)$ 是 $(\\mu_0 + \\alpha)$-强凸的。\n        *   强凸性模数是 $\\mu_{\\alpha} = \\mu_0 + \\alpha$。\n\n    *   **第 1 部分总结：**\n        *   $L_{\\alpha} = L + \\alpha$。\n        *   $\\mu_{\\alpha} = \\mu_0 + \\alpha$。\n        *   这些结果看起来是正确的，并且是按照要求使用基本定义推导出来的。\n\n**第 2 部分：在约束条件下优化条件数。**\n\n1.  **陈述目标：**关于 $\\alpha \\geq 0$ 最小化 $\\sqrt{\\frac{L_{\\alpha}}{\\mu_{\\alpha}}}$。\n2.  **陈述第 1 部分的量：**\n    *   $L_{\\alpha} = L + \\alpha$。\n    *   $\\mu_{\\alpha} = \\mu_0 + \\alpha$。\n3.  **陈述约束条件：**\n    *   $L_{\\alpha} \\leq (1+\\rho)L$。\n    *   代入 $L_{\\alpha}$：$L + \\alpha \\leq (1+\\rho)L = L + \\rho L$。\n    *   这简化为 $\\alpha \\leq \\rho L$。\n    *   我们还有一个隐式约束 $\\alpha \\geq 0$。\n    *   所以，$\\alpha$ 的可行范围是 $0 \\leq \\alpha \\leq \\rho L$。\n4.  **构建优化问题：**\n    *   最小化 $g(\\alpha) = \\sqrt{\\frac{L + \\alpha}{\\mu_0 + \\alpha}}$\n    *   约束条件为 $0 \\leq \\alpha \\leq \\rho L$。\n5.  **分析目标函数 $g(\\alpha)$。**\n    *   对于 $x \\geq 0$，最小化 $\\sqrt{x}$ 等价于最小化 $x$。\n    *   所以，我们来最小化 $h(\\alpha) = \\frac{L + \\alpha}{\\mu_0 + \\alpha}$。\n    *   为了找到 $h(\\alpha)$ 的行为，我们对它求关于 $\\alpha$ 的导数。\n    *   $h'(\\alpha) = \\frac{d}{d\\alpha} \\left( \\frac{L + \\alpha}{\\mu_0 + \\alpha} \\right)$。\n    *   使用商法则：\n        $h'(\\alpha) = \\frac{(1)(\\mu_0 + \\alpha) - (L + \\alpha)(1)}{(\\mu_0 + \\alpha)^2}$。\n        $h'(\\alpha) = \\frac{\\mu_0 + \\alpha - L - \\alpha}{(\\mu_0 + \\alpha)^2} = \\frac{\\mu_0 - L}{(\\mu_0 + \\alpha)^2}$。\n6.  **分析导数的符号。**\n    *   分母 $(\\mu_0 + \\alpha)^2$ 总是正的。\n    *   $h'(\\alpha)$ 的符号由分子 $\\mu_0 - L$ 的符号决定。\n    *   问题陈述中说 $L > \\mu_0 \\ge 0$。\n    *   因此，$\\mu_0 - L < 0$。\n    *   所以，对于定义域内的所有 $\\alpha$，$h'(\\alpha) < 0$。\n7.  **解释结果。**\n    *   由于 $h(\\alpha)$ 的导数总是负的，函数 $h(\\alpha)$ 是一个关于 $\\alpha$ 的严格递减函数。\n    *   函数 $g(\\alpha) = \\sqrt{h(\\alpha)}$ 也是严格递减的。\n    *   为了在一个闭区间 $[a, b]$ 上最小化一个严格递减的函数，我们必须选择区间中可能的最大值。\n    *   $\\alpha$ 的区间是 $[0, \\rho L]$。\n    *   可能的最大值是 $\\alpha = \\rho L$。\n8.  **计算最小化的因子。**\n    *   $\\alpha$ 的最优值是 $\\alpha_{opt} = \\rho L$。\n    *   现在，将这个值代回到条件数因子的表达式中。\n    *   最小化的因子 = $\\sqrt{\\frac{L + \\alpha_{opt}}{\\mu_0 + \\alpha_{opt}}}$。\n    *   $= \\sqrt{\\frac{L + \\rho L}{\\mu_0 + \\rho L}}$。\n    *   $= \\sqrt{\\frac{L(1 + \\rho)}{\\mu_0 + \\rho L}}$。\n9.  **最后检查步骤和约束条件。**\n    *   问题陈述：$f$ 是可微凸函数，$\\nabla f$ 是 $L$-利普希茨的，$f$ 是 $\\mu_0$-强凸的。\n    *   $L > \\mu_0 \\geq 0$。\n    *   $F_{\\alpha}(x) = f(x) + \\frac{\\alpha}{2} \\|x\\|^2$，其中 $\\alpha \\geq 0$。\n    *   约束：$L_{\\alpha} \\leq (1+\\rho)L$，其中 $0 < \\rho < 1$。\n    *   $L_{\\alpha} = L+\\alpha$ 和 $\\mu_{\\alpha} = \\mu_0+\\alpha$ 的推导是可靠的，并且遵循了定义。\n    *   约束 $\\alpha \\leq \\rho L$ 是正确的。\n    *   对目标函数 $\\frac{L+\\alpha}{\\mu_0+\\alpha}$ 的分析是正确的。导数确实是 $\\frac{\\mu_0 - L}{(\\mu_0+\\alpha)^2}$。\n    *   因为 $L > \\mu_0$，导数为负。函数是递减的。\n    *   最小值在 $\\alpha$ 允许的最大值处取得。\n    *   允许的最大值是 $\\alpha = \\rho L$。\n    *   得到的最小值是 $\\sqrt{\\frac{L(1+\\rho)}{\\mu_0 + \\rho L}}$。\n    *   该表达式如要求是关于 $L$、$\\mu_0$ 和 $\\rho$ 的。它是一个单一的闭式表达式。\n    *   所有步骤在逻辑上似乎都是合理的，在数学上也是正确的。",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{L(1+\\rho)}{\\mu_{0}+\\rho L}}}\n$$"
        },
        {
            "introduction": "理论最终要通过实验来验证。本练习将理论与实践无缝对接，让你亲眼见证光滑度在梯度下降算法稳定性中的决定性作用。你将设计一个数值实验，在一个简单的二次规划问题上，通过编程来模拟梯度下降的行为。通过对比理论预测与数值结果，你将直观地观察到，当步长违反了由光滑度常数 $L$ 确定的理论界限时，算法是如何从稳定收敛走向剧烈发散的，从而深刻理解理论指导实践的重要性。",
            "id": "3183363",
            "problem": "你需要设计并实现一个数值实验，通过光滑性和梯度的 Lipschitz 连续性来检验无约束二次最小化问题的梯度下降法的稳定性。实验应完全在二次目标函数 $f(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$ 的框架下进行，其中 $A$ 是一个对称正定（SPD）矩阵，$b$ 是一个向量。$f$ 的梯度由 $\\nabla f(x) = A x - b$ 给出。如果对于定义域中的所有 $x,y$ 都满足 $\\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x-y\\|$（其中 $\\|\\cdot\\|$ 表示欧几里得范数），则称梯度映射是 Lipschitz 连续的，其常数为 $L$。在这个具有 SPD 矩阵 $A$ 的二次函数设定中，梯度的 Lipschitz 常数等于 $A$ 的谱范数，也即 $A$ 的最大特征值。\n\n从基本的梯度下降迭代 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$ 开始，其中 $\\alpha$ 是一个固定的步长，表示相对于最小化点 $x^{\\star}$（满足 $A x^{\\star} = b$）的误差 $e_k = x_k - x^{\\star}$，并分析线性误差动态。迭代的稳定性可以通过一个从 $A$ 和 $\\alpha$ 导出的特定矩阵的谱半径来表征。基于此特性，存在一个与梯度 Lipschitz 常数相关的临界步长边界，它将稳定行为与振荡或发散行为区分开来。你的程序必须：\n\n- 计算梯度的 Lipschitz 常数 $L$，即 $A$ 的最大特征值。\n- 通过检查线性误差传播算子的谱半径是否超过 $1$ 来计算理论上的稳定性/发散性分类。\n- 从指定的初始点开始，对误差动态进行固定次数的梯度下降数值迭代，并确定是否在数值上观察到发散。对于本实验，如果以下两个条件都成立，则定义“观察到的发散”发生：\n  1. 运行过程中的最大误差范数至少是初始误差范数的 $r_{\\text{grow}}$ 倍。\n  2. 最终误差范数至少是初始误差范数的 $r_{\\text{final}}$ 倍。\n  \n使用下面指定的 $N$ 次迭代，以及下面指定的固定阈值 $r_{\\text{grow}}$ 和 $r_{\\text{final}}$。你对每个测试用例的输出必须是一个布尔值，指示观察到的发散是否与理论发散分类相符。\n\n要使用的基本定义：\n- 梯度的 Lipschitz 连续性：$\\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x-y\\|$。\n- 梯度下降更新：$x_{k+1} = x_k - \\alpha \\nabla f(x_k)$。\n- 相对于满足 $A x^{\\star} = b$ 的最小化点 $x^{\\star}$ 的误差动态。\n\n实现以下测试套件。每个情况指定了 $A$、$b$、$x_0$ 和 $\\alpha$。将 $A$ 为 $1 \\times 1$ 矩阵和长度为 $1$ 的向量的一维情况同样处理。对所有情况使用 $N = 200$ 次迭代，$r_{\\text{grow}} = 1.5$ 和 $r_{\\text{final}} = 1.2$。\n\n- 情况 1（一维，步长低于边界）：$A = [5],\\quad b = [5],\\quad x_0 = [10],\\quad \\alpha = 0.30.$\n- 情况 2（一维，步长在边界上）：$A = [5],\\quad b = [5],\\quad x_0 = [10],\\quad \\alpha = 0.40.$\n- 情况 3（一维，步长高于边界）：$A = [5],\\quad b = [5],\\quad x_0 = [10],\\quad \\alpha = 0.41.$\n- 情况 4（二维，步长低于边界）：$A = \\begin{bmatrix}3  1 \\\\ 1  3\\end{bmatrix},\\quad b = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ 4\\end{bmatrix},\\quad \\alpha = 0.49.$\n- 情况 5（二维，步长在边界上）：$A = \\begin{bmatrix}3  1 \\\\ 1  3\\end{bmatrix},\\quad b = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ 4\\end{bmatrix},\\quad \\alpha = 0.50.$\n- 情况 6（二维，步长高于边界）：$A = \\begin{bmatrix}3  1 \\\\ 1  3\\end{bmatrix},\\quad b = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ 4\\end{bmatrix},\\quad \\alpha = 0.51.$\n- 情况 7（三维，步长低于边界）：$A = \\operatorname{diag}(1,3,7),\\quad b = \\begin{bmatrix}2 \\\\ -1 \\\\ 5\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ -3 \\\\ 1\\end{bmatrix},\\quad \\alpha = 0.28.$\n- 情况 8（三维，步长高于边界）：$A = \\operatorname{diag}(1,3,7),\\quad b = \\begin{bmatrix}2 \\\\ -1 \\\\ 5\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ -3 \\\\ 1\\end{bmatrix},\\quad \\alpha = 0.29.$\n\n你的程序必须：\n- 对每种情况，计算 $A$ 的最大特征值 $L$。\n- 通过评估线性误差传播算子的谱半径并检查其是否超过 $1$ 来计算理论发散分类。\n- 使用指定的 $\\alpha$ 对误差动态运行 $N = 200$ 次数值梯度下降，并使用如上定义的 $r_{\\text{grow}} = 1.5$ 和 $r_{\\text{final}} = 1.2$ 来分类是否观察到发散。\n- 对每种情况产生一个布尔值，指示观察到的发散是否与理论分类相符。\n\n最终输出格式：\n你的程序应产生单行输出，包含一个逗号分隔的列表，列表用方括号括起来（例如，`[True,False,...,True]`）。每个 `result_i` 必须是一个布尔值。",
            "solution": "该问题要求对应用于无约束二次最小化问题的梯度下降法的稳定性理论进行数值验证。给定目标函数 $f(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$，其中 $A$ 是对称正定（SPD）矩阵。梯度下降更新规则为 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$，其中 $\\alpha > 0$ 是一个固定步长。\n\n首先，我们通过分析误差动态来建立稳定性的理论框架。目标函数的梯度是 $\\nabla f(x) = A x - b$。唯一最小化点，记为 $x^{\\star}$，是线性系统 $\\nabla f(x^{\\star}) = 0$ 的解，即 $A x^{\\star} = b$。\n\n在第 $k$ 次迭代的误差定义为 $e_k = x_k - x^{\\star}$。为了推导误差的演化过程，我们将梯度和关系式 $b = A x^{\\star}$ 代入梯度下降更新规则中：\n$$x_{k+1} = x_k - \\alpha (A x_k - b)$$\n$$x_{k+1} = x_k - \\alpha (A x_k - A x^{\\star})$$\n两边同时减去 $x^{\\star}$ 得到第 $k+1$ 次迭代的误差：\n$$x_{k+1} - x^{\\star} = (x_k - x^{\\star}) - \\alpha A (x_k - x^{\\star})$$\n$$e_{k+1} = e_k - \\alpha A e_k$$\n这可以简化为关于误差向量的线性动力学系统：\n$$e_{k+1} = (I - \\alpha A) e_k$$\n其中 $I$ 是单位矩阵。令 $B = I - \\alpha A$ 为误差传播矩阵。第 $k$ 次迭代的误差由 $e_k = B^k e_0$ 给出，其中 $e_0$ 是初始误差。\n\n迭代的稳定性，即对于任何初始误差 $e_0$，当 $k \\to \\infty$ 时误差 $e_k$ 收敛到零，由传播矩阵 $B$ 的谱半径决定。谱半径 $\\rho(B)$ 是 $B$ 的特征值绝对值的最大值。当且仅当 $\\rho(B) < 1$ 时，迭代是稳定且收敛的。如果 $\\rho(B) > 1$，则迭代发散。如果 $\\rho(B) = 1$，误差不会收敛到零（除非它已经是零），但其范数不一定会增长到无穷大。\n\n由于 $A$ 是对称的，它有实数特征值 $\\lambda_i$。$B = I - \\alpha A$ 的特征值是 $\\mu_i = 1 - \\alpha \\lambda_i$。因此，谱半径为：\n$$\\rho(B) = \\max_i |1 - \\alpha \\lambda_i|$$\n由于 $A$ 是正定的，所有 $\\lambda_i > 0$。收敛条件 $\\rho(B) < 1$ 等价于对所有 $i$ 都有 $-1 < 1 - \\alpha \\lambda_i < 1$。\n右侧不等式 $1 - \\alpha \\lambda_i < 1$ 简化为 $-\\alpha \\lambda_i < 0$，对于 $\\alpha > 0$ 和 $\\lambda_i > 0$ 恒成立。\n左侧不等式 $-1 < 1 - \\alpha \\lambda_i$ 简化为 $\\alpha \\lambda_i < 2$，或 $\\alpha < 2/\\lambda_i$。这必须对所有特征值 $\\lambda_i$ 都成立。最严格的条件由最大特征值 $\\lambda_{\\max}(A)$ 施加。因此，步长必须满足：\n$$\\alpha < \\frac{2}{\\lambda_{\\max}(A)}$$\n问题指出，梯度的 Lipschitz 常数 $L$ 等于 $\\lambda_{\\max}(A)$。稳定性条件是 $\\alpha < 2/L$。\n\n如果稳定性条件被违反，使得 $\\rho(I - \\alpha A) > 1$，则理论上判定为发散。\n\n数值实验旨在验证这一理论预测。对于每个测试用例，我们执行以下步骤：\n1.  **理论分类**：我们计算矩阵 $A$ 的特征值，找到谱半径 $\\rho(I - \\alpha A)$，并确定它是否大于 $1$。这给出了 `theoretical_divergence`布尔分类。\n2.  **数值模拟**：我们对误差动态 $e_{k+1} = (I - \\alpha A)e_k$ 进行 $N=200$ 次迭代的模拟。\n    - 首先，通过求解 $A x^{\\star} = b$ 来找到最小化点 $x^{\\star}$。\n    - 计算初始误差 $e_0 = x_0 - x^{\\star}$ 及其欧几里得范数 $\\|e_0\\|$。\n    - 我们迭代计算 $e_1, e_2, \\dots, e_N$ 及其范数。\n    - **观察到的发散**：我们检查模拟是否满足指定的发散标准：\n      - (1) 运行期间的最大误差范数 $\\max_{0 \\le k \\le N} \\|e_k\\|$ 至少是初始误差范数 $\\|e_0\\|$ 的 $r_{\\text{grow}} = 1.5$ 倍。\n      - (2) 最终误差范数 $\\|e_N\\|$ 至少是初始误差范数 $\\|e_0\\|$ 的 $r_{\\text{final}} = 1.2$ 倍。\n    - 如果两个条件都满足，则该行为被分类为 `observed_divergence`。\n3.  **比较**：对于每种情况，我们比较理论分类和观察到的分类。预期的结果是它们相匹配，从而产生布尔值 `True`。当 $\\rho(B) > 1$ 时，对应于 $B$ 的主导特征值的误差分量将以 $(\\rho(B))^k$ 的速度呈指数增长。对于足够大的迭代次数 $N$，这种增长将不可避免地超过固定的阈值 $r_{\\text{grow}}$ 和 $r_{\\text{final}}$。相反，当 $\\rho(B) \\le 1$ 时，误差范数不会增长，因此不会满足发散标准。\n\n实现将根据此逻辑处理八个测试用例中的每一个。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the stability of gradient descent for quadratic minimization problems,\n    comparing theoretical predictions with numerical simulations.\n    \"\"\"\n    # Use float for all numerical values to ensure consistency.\n    test_cases = [\n        # Case 1 (1D, stable)\n        {'A': np.array([[5.0]]), 'b': np.array([5.0]), 'x0': np.array([10.0]), 'alpha': 0.30},\n        # Case 2 (1D, boundary)\n        {'A': np.array([[5.0]]), 'b': np.array([5.0]), 'x0': np.array([10.0]), 'alpha': 0.40},\n        # Case 3 (1D, divergent)\n        {'A': np.array([[5.0]]), 'b': np.array([5.0]), 'x0': np.array([10.0]), 'alpha': 0.41},\n        # Case 4 (2D, stable)\n        {'A': np.array([[3.0, 1.0], [1.0, 3.0]]), 'b': np.array([1.0, -2.0]), 'x0': np.array([5.0, 4.0]), 'alpha': 0.49},\n        # Case 5 (2D, boundary)\n        {'A': np.array([[3.0, 1.0], [1.0, 3.0]]), 'b': np.array([1.0, -2.0]), 'x0': np.array([5.0, 4.0]), 'alpha': 0.50},\n        # Case 6 (2D, divergent)\n        {'A': np.array([[3.0, 1.0], [1.0, 3.0]]), 'b': np.array([1.0, -2.0]), 'x0': np.array([5.0, 4.0]), 'alpha': 0.51},\n        # Case 7 (3D, stable)\n        {'A': np.diag([1.0, 3.0, 7.0]), 'b': np.array([2.0, -1.0, 5.0]), 'x0': np.array([5.0, -3.0, 1.0]), 'alpha': 0.28},\n        # Case 8 (3D, divergent)\n        {'A': np.diag([1.0, 3.0, 7.0]), 'b': np.array([2.0, -1.0, 5.0]), 'x0': np.array([5.0, -3.0, 1.0]), 'alpha': 0.29},\n    ]\n\n    results = []\n    \n    # Simulation parameters\n    N = 200\n    r_grow = 1.5\n    r_final = 1.2\n    \n    for case in test_cases:\n        A, b, x0, alpha = case['A'], case['b'], case['x0'], case['alpha']\n        \n        # 1. Theoretical Analysis\n        # Since A is symmetric, eigvalsh is appropriate and returns real eigenvalues.\n        eigenvalues_A = np.linalg.eigvalsh(A)\n        # The eigenvalues of the error propagation matrix B = I - alpha * A\n        eigenvalues_B = 1.0 - alpha * eigenvalues_A\n        # The spectral radius of B\n        rho_B = np.max(np.abs(eigenvalues_B))\n        # Theoretical divergence occurs if the spectral radius is > 1\n        theoretical_divergence = rho_B > 1.0\n        \n        # 2. Numerical Simulation\n        # Solve for the minimizer x_star from A * x_star = b\n        x_star = np.linalg.solve(A, b)\n        \n        # Initial error vector and its norm\n        e_k = x0 - x_star\n        initial_error_norm = np.linalg.norm(e_k)\n        \n        # Handle the edge case where the initial guess is the solution\n        if initial_error_norm  1e-12:\n            observed_divergence = False\n        else:\n            max_error_norm = initial_error_norm\n            \n            # Error propagation matrix B = I - alpha * A\n            B = np.identity(A.shape[0]) - alpha * A\n            \n            # Run the simulation on the error dynamics\n            for _ in range(N):\n                e_k = B @ e_k\n                current_norm = np.linalg.norm(e_k)\n                if current_norm > max_error_norm:\n                    max_error_norm = current_norm\n            \n            final_error_norm = np.linalg.norm(e_k)\n            \n            # Check the conditions for observed divergence\n            cond1 = (max_error_norm >= r_grow * initial_error_norm)\n            cond2 = (final_error_norm >= r_final * initial_error_norm)\n            observed_divergence = cond1 and cond2\n\n        # 3. Compare theoretical prediction with numerical observation\n        # The result is True if they match, False otherwise.\n        match = (observed_divergence == theoretical_divergence)\n        results.append(match)\n\n    # Format the final output as a string representing a list of booleans\n    # The default string representation of Python booleans is 'True' or 'False'.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}