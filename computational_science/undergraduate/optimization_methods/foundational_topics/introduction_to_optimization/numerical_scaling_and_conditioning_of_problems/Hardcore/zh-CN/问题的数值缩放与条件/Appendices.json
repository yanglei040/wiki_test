{
    "hands_on_practices": [
        {
            "introduction": "本章的第一个练习将通过一个直接的编程实践，向你展示一个简单的变量统一缩放如何显著影响常见优化算法的收敛性。通过在同一个二次函数问题上实现并比较梯度下降法、共轭梯度法和牛顿法在不同缩放因子下的表现，你将亲眼看到哪些方法对缩放敏感，哪些则具有稳健性。这个练习旨在强调在应用算法之前，考虑问题本身的尺度是多么重要。",
            "id": "3158924",
            "problem": "您将研究均匀变量缩放如何影响一阶和二阶优化方法在严格凸二次目标函数上的数值行为。考虑由 $f(\\boldsymbol{x}) = \\tfrac{1}{2}\\,\\boldsymbol{x}^{\\top}\\boldsymbol{Q}\\boldsymbol{x} - \\boldsymbol{b}^{\\top}\\boldsymbol{x}$ 定义的二次函数，其中 $\\boldsymbol{Q}\\in\\mathbb{R}^{n\\times n}$ 是对称正定矩阵，$\\boldsymbol{b}\\in\\mathbb{R}^{n}$。通过变量替换 $\\boldsymbol{x} = s\\,\\boldsymbol{y}$（其中 $s = 10^{k}$，$k$ 为整数）定义一族缩放问题，并令 $g_{k}(\\boldsymbol{y}) = f(s\\,\\boldsymbol{y})$。您将在 $g_{k}$上实现三种方法：具有固定步长的梯度下降法 (GD)、应用于二次型的对称正定系统的共轭梯度法 (CG)，以及采用单位步长的牛顿法。您的程序必须根据原始变量 $\\boldsymbol{x}$ 来衡量，经验性地计算达到预设平稳点容差所需的迭代次数。\n\n使用的基本原理和定义：\n- 对称正定矩阵具有正实数特征值；$f$ 的梯度为 $\\nabla f(\\boldsymbol{x}) = \\boldsymbol{Q}\\boldsymbol{x} - \\boldsymbol{b}$，海森矩阵为 $\\nabla^{2} f(\\boldsymbol{x}) = \\boldsymbol{Q}$。\n- 矩阵 2-范数下的谱条件数为 $\\kappa_{2}(\\boldsymbol{Q}) = \\lambda_{\\max}(\\boldsymbol{Q})/\\lambda_{\\min}(\\boldsymbol{Q})$，其中 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 分别是最大和最小特征值。\n- 对于缩放问题 $\\boldsymbol{x} = s\\,\\boldsymbol{y}$，关于 $\\boldsymbol{y}$ 的梯度为 $\\nabla g_{k}(\\boldsymbol{y}) = s^{2}\\boldsymbol{Q}\\boldsymbol{y} - s\\,\\boldsymbol{b}$，海森矩阵为 $\\nabla^{2} g_{k}(\\boldsymbol{y}) = s^{2}\\boldsymbol{Q}$。\n\n实现要求：\n- 使用维度 $n=10$，$\\boldsymbol{Q}$ 为对角矩阵，其对角线上的元素为 $\\lambda_{i} = 10^{4i/9}$，$i = 0,1,2,\\dots,9$。设置 $\\boldsymbol{b} = \\boldsymbol{1}$（全一向量）。\n- 对测试集 $k \\in \\{-6,-3,0,3,6\\}$，使用缩放因子 $s_{k} = 10^{k}$。\n- 所有方法均从 $\\boldsymbol{y}_{0} = \\boldsymbol{0}$ 开始初始化。对于每种方法和每个 $k$，进行迭代直到原始梯度范数满足 $\\|\\nabla f(\\boldsymbol{x})\\|_{2} \\le \\varepsilon$，其中 $\\varepsilon = 10^{-8}$，$\\boldsymbol{x} = s_{k}\\,\\boldsymbol{y}$。所有范数均为欧几里得范数。\n- 梯度下降法 (GD)：应用于 $g_{k}$，固定步长 $\\alpha = 10^{-4}$，即 $\\boldsymbol{y}_{t+1} = \\boldsymbol{y}_{t} - \\alpha\\,\\nabla g_{k}(\\boldsymbol{y}_{t})$。设置迭代次数上限为 $N_{\\max}^{\\mathrm{GD}} = 20000$。为避免发散时出现数值溢出，如果在任何点 $\\|\\nabla f(\\boldsymbol{x}_{t})\\|_{2} > 10^{12}\\,\\|\\nabla f(\\boldsymbol{x}_{0})\\|_{2}$ 或梯度范数变为非有限值，则提前终止 GD，并报告该 $k$ 值的迭代次数为 $N_{\\max}^{\\mathrm{GD}}$。\n- 共轭梯度法 (CG)：将标准线性共轭梯度法应用于 $g_{k}$ 的正规方程，即从 $\\boldsymbol{y}_{0} = \\boldsymbol{0}$ 开始求解 $s_{k}^{2}\\boldsymbol{Q}\\boldsymbol{y} = s_{k}\\boldsymbol{b}$。每次 CG 迭代后，通过注意到残差 $\\boldsymbol{r}_{t} = s_{k}\\boldsymbol{b} - s_{k}^{2}\\boldsymbol{Q}\\boldsymbol{y}_{t}$ 满足 $\\|\\nabla f(\\boldsymbol{x}_{t})\\|_{2} = \\|\\boldsymbol{r}_{t}\\|_{2}/s_{k}$（其中 $\\boldsymbol{x}_{t} = s_{k}\\boldsymbol{y}_{t}$），来检查原始梯度范数准则。使用最大迭代次数 $N_{\\max}^{\\mathrm{CG}} = n$。\n- 牛顿法：应用于 $g_{k}$，采用单位步长，即 $\\boldsymbol{y}_{t+1} = \\boldsymbol{y}_{t} - \\left(\\nabla^{2} g_{k}(\\boldsymbol{y}_{t})\\right)^{-1}\\nabla g_{k}(\\boldsymbol{y}_{t})$。对于严格凸二次函数，该方法从任意起始点都能在一次迭代中收敛。您必须从 $\\boldsymbol{y}_{0} = \\boldsymbol{0}$ 实现一次牛顿迭代，并为每个 $k$ 计为 1 次迭代。\n\n测试集和输出：\n- 测试集是缩放因子集 $k \\in \\{-6,-3,0,3,6\\}$，使用固定参数 $n=10$, $\\varepsilon=10^{-8}$, $\\alpha=10^{-4}$, $N_{\\max}^{\\mathrm{GD}}=20000$ 和 $N_{\\max}^{\\mathrm{CG}}=n$。\n- 对于给定顺序中的每个 $k$，计算迭代次数的三元组 $\\left(N_{\\mathrm{GD}}(k), N_{\\mathrm{CG}}(k), N_{\\mathrm{Newton}}(k)\\right)$，其中每个条目都是一个整数。如果 GD 因发散保护而提前终止或在迭代上限内未能达到容差，则报告 $N_{\\mathrm{GD}}(k)=N_{\\max}^{\\mathrm{GD}}$。\n- 您的程序应生成单行输出，其中包含结果，格式为一个由方括号括起来的、由逗号分隔的三元组列表。格式必须是有效的 Python 列表的列表字面量，例如，对于两个测试用例，格式为 $[\\,[3,2,1],[4,2,1]\\,]$，但应使用此问题实际计算出的整数，并按指定顺序 $k \\in \\{-6,-3,0,3,6\\}$ 包含五个三元组。\n\n您的实现必须是一个完整、可运行的程序，该程序执行所有计算并以指定格式精确打印一行输出，不得有任何多余文本。不需要也不允许用户输入。",
            "solution": "用户的请求是分析均匀变量缩放对三种标准优化算法——梯度下降法 (GD)、共轭梯度法 (CG) 和牛顿法——性能的影响。该分析在一个严格凸二次目标函数上进行。\n\n### 问题验证\n根据指定标准对问题陈述进行验证。\n\n**步骤 1：提取已知条件**\n-   目标函数：$f(\\boldsymbol{x}) = \\tfrac{1}{2}\\,\\boldsymbol{x}^{\\top}\\boldsymbol{Q}\\boldsymbol{x} - \\boldsymbol{b}^{\\top}\\boldsymbol{x}$，其中 $\\boldsymbol{Q}\\in\\mathbb{R}^{n\\times n}$ 是对称正定 (SPD) 矩阵，$\\boldsymbol{b}\\in\\mathbb{R}^{n}$。\n-   缩放后的变量：$\\boldsymbol{x} = s\\,\\boldsymbol{y}$，其中 $s = 10^{k}$，$k$ 为整数。\n-   缩放后的目标函数：$g_{k}(\\boldsymbol{y}) = f(s\\,\\boldsymbol{y})$。\n-   $f$ 的梯度和海森矩阵：$\\nabla f(\\boldsymbol{x}) = \\boldsymbol{Q}\\boldsymbol{x} - \\boldsymbol{b}$，$\\nabla^{2} f(\\boldsymbol{x}) = \\boldsymbol{Q}$。\n-   $g_k$ 的梯度和海森矩阵：$\\nabla g_{k}(\\boldsymbol{y}) = s^{2}\\boldsymbol{Q}\\boldsymbol{y} - s\\,\\boldsymbol{b}$，$\\nabla^{2} g_{k}(\\boldsymbol{y}) = s^{2}\\boldsymbol{Q}$。\n-   参数：$n=10$，$\\boldsymbol{Q} = \\mathrm{diag}(10^{4i/9})$ 其中 $i \\in \\{0, \\dots, 9\\}$，$\\boldsymbol{b} = \\boldsymbol{1}$。\n-   测试集：$k \\in \\{-6, -3, 0, 3, 6\\}$。\n-   初始点：$\\boldsymbol{y}_{0} = \\boldsymbol{0}$。\n-   停止准则：$\\|\\nabla f(\\boldsymbol{x})\\|_{2} \\le \\varepsilon$，其中 $\\varepsilon = 10^{-8}$。\n-   梯度下降法 (GD)：更新规则 $\\boldsymbol{y}_{t+1} = \\boldsymbol{y}_{t} - \\alpha\\,\\nabla g_{k}(\\boldsymbol{y}_{t})$，步长 $\\alpha = 10^{-4}$。最大迭代次数 $N_{\\max}^{\\mathrm{GD}} = 20000$。若 $\\|\\nabla f(\\boldsymbol{x}_{t})\\|_{2} > 10^{12}\\,\\|\\nabla f(\\boldsymbol{x}_{0})\\|_{2}$ 或范数非有限，则判定为发散，报告 $N_{\\max}^{\\mathrm{GD}}$。\n-   共轭梯度法 (CG)：应用于求解 $s_{k}^{2}\\boldsymbol{Q}\\boldsymbol{y} = s_{k}\\boldsymbol{b}$。最大迭代次数 $N_{\\max}^{\\mathrm{CG}} = n = 10$。停止准则基于原始梯度范数 $\\|\\nabla f(\\boldsymbol{x}_{t})\\|_{2} = \\|\\boldsymbol{r}_{t}\\|_{2}/s_{k} \\le \\varepsilon$。\n-   牛顿法：要求从 $\\boldsymbol{y}_{0} = \\boldsymbol{0}$ 进行一次迭代，计为 1 次迭代。\n\n**步骤 2：使用提取的已知条件进行验证**\n该问题在科学上基于已建立的数值优化理论。问题是适定的，所有参数和条件都已明确定义，以确保得到唯一、可计算的结果。语言客观且正式。该问题是分析优化算法数值特性的一个标准练习，并且完全自洽和一致。未发现任何缺陷。\n\n**步骤 3：结论与行动**\n问题有效。下面提供一个合理的解答。\n\n### 基于原理的解答\n\n该问题研究缩放变换 $\\boldsymbol{x} = s\\boldsymbol{y}$ 对优化算法收敛性的影响。原始目标函数为 $f(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^{\\top}\\boldsymbol{Q}\\boldsymbol{x} - \\boldsymbol{b}^{\\top}\\boldsymbol{x}$。以 $\\boldsymbol{y}$ 表示的缩放后目标函数为 $g_k(\\boldsymbol{y}) = \\frac{s^2}{2}\\boldsymbol{y}^{\\top}\\boldsymbol{Q}\\boldsymbol{y} - s\\boldsymbol{b}^{\\top}\\boldsymbol{y}$。这个缩放后问题的海森矩阵是 $\\nabla^2 g_k(\\boldsymbol{y}) = s^2\\boldsymbol{Q}$。算法的行为关键取决于这个缩放后海森矩阵的性质。\n\n指定的矩阵 $\\boldsymbol{Q}$ 是对角矩阵，其特征值为 $\\lambda_i = 10^{4i/9}$，$i=0, \\dots, 9$。\n最小特征值为 $\\lambda_{\\min}(\\boldsymbol{Q}) = 10^{0} = 1$。\n最大特征值为 $\\lambda_{\\max}(\\boldsymbol{Q}) = 10^{4\\cdot9/9} = 10^4$。\n原始海森矩阵的条件数为 $\\kappa_2(\\boldsymbol{Q}) = \\lambda_{\\max}(\\boldsymbol{Q}) / \\lambda_{\\min}(\\boldsymbol{Q}) = 10^4$。\n\n**1. 梯度下降法 (GD)**\n针对缩放问题的 GD 更新规则为 $\\boldsymbol{y}_{t+1} = \\boldsymbol{y}_{t} - \\alpha \\nabla g_k(\\boldsymbol{y}_t)$。对于二次函数，这是一个线性迭代：\n$$ \\boldsymbol{y}_{t+1} = \\boldsymbol{y}_{t} - \\alpha (s^2\\boldsymbol{Q}\\boldsymbol{y}_t - s\\boldsymbol{b}) = (\\boldsymbol{I} - \\alpha s^2\\boldsymbol{Q})\\boldsymbol{y}_t + \\alpha s\\boldsymbol{b} $$\n此迭代收敛当且仅当迭代矩阵 $\\boldsymbol{I} - \\alpha s^2\\boldsymbol{Q}$ 的谱半径小于 1。该矩阵的特征值为 $1 - \\alpha s^2 \\lambda_i(\\boldsymbol{Q})$。条件是对于所有 $i$ 都有 $|1 - \\alpha s^2 \\lambda_i(\\boldsymbol{Q})|  1$，这等价于 $0  \\alpha s^2 \\lambda_i(\\boldsymbol{Q})  2$。由于 $\\alpha > 0$ 且 $\\lambda_i > 0$，我们只需检查最大特征值的上界：\n$$ \\alpha s^2 \\lambda_{\\max}(\\boldsymbol{Q})  2 $$\n给定 $\\alpha = 10^{-4}$ 和 $\\lambda_{\\max}(\\boldsymbol{Q}) = 10^4$，条件变为 $10^{-4} \\cdot s^2 \\cdot 10^4  2$，简化为 $s^2  2$。\n我们对每个 $k$ 检查此条件：\n-   $k \\in \\{-6,-3,0\\}$：$s \\in \\{10^{-6}, 10^{-3}, 1\\}$。在这些情况下，$s^2 \\in \\{10^{-12}, 10^{-6}, 1\\}$，都小于 2。GD 保证收敛。然而，收敛速率由 $\\rho(\\boldsymbol{I} - \\alpha s^2\\boldsymbol{Q}) = \\max_i |1 - \\alpha s^2 \\lambda_i|$ 决定。对于小的 $s$，$\\alpha s^2$ 非常小，使得谱半径极其接近 1。这意味着收敛极其缓慢。鉴于迭代上限为 $N_{\\max}^{\\mathrm{GD}}=20000$，预计算法将不会达到容差 $\\varepsilon=10^{-8}$，并会在达到迭代上限时终止。\n-   $k \\in \\{3,6\\}$：$s \\in \\{10^3, 10^6\\}$。在这些情况下，$s^2 \\in \\{10^6, 10^{12}\\}$，都大于 2。收敛条件被违反，GD 迭代将会发散。发散保护 $\\|\\nabla f(\\boldsymbol{x}_{t})\\|_{2} > 10^{12}\\,\\|\\nabla f(\\boldsymbol{x}_{0})\\|_{2}$ 将被触发，报告的迭代次数将为 $N_{\\max}^{\\mathrm{GD}}$。\n因此，对于所有 $k$ 值，GD 的预期迭代次数均为 $N_{\\max}^{\\mathrm{GD}} = 20000$。\n\n**2. 共轭梯度法 (CG)**\nCG 方法用于求解由最优性条件 $\\nabla g_k(\\boldsymbol{y}) = \\boldsymbol{0}$ 产生的线性系统，即 $s^2\\boldsymbol{Q}\\boldsymbol{y} = s\\boldsymbol{b}$。系统矩阵为 $A_k = s^2\\boldsymbol{Q}$。CG 的收敛速率取决于系统矩阵的条件数 $\\kappa_2(A_k)$。\n$$ \\kappa_2(A_k) = \\kappa_2(s^2\\boldsymbol{Q}) = \\frac{\\lambda_{\\max}(s^2\\boldsymbol{Q})}{\\lambda_{\\min}(s^2\\boldsymbol{Q})} = \\frac{s^2\\lambda_{\\max}(\\boldsymbol{Q})}{s^2\\lambda_{\\min}(\\boldsymbol{Q})} = \\frac{\\lambda_{\\max}(\\boldsymbol{Q})}{\\lambda_{\\min}(\\boldsymbol{Q})} = \\kappa_2(\\boldsymbol{Q}) $$\n条件数不受缩放因子 $s$ 的影响。此外，停止准则为 $\\|\\nabla f(\\boldsymbol{x}_t)\\|_2 \\le \\varepsilon$。CG 的线性系统残差为 $\\boldsymbol{r}_t = s\\boldsymbol{b} - s^2\\boldsymbol{Q}\\boldsymbol{y}_t$。原始函数的梯度为 $\\nabla f(\\boldsymbol{x}_t) = \\boldsymbol{Q}\\boldsymbol{x}_t - \\boldsymbol{b} = \\boldsymbol{Q}(s\\boldsymbol{y}_t) - \\boldsymbol{b} = s\\boldsymbol{Q}\\boldsymbol{y}_t - \\boldsymbol{b}$。我们可以看到 $\\boldsymbol{r}_t = -s(s\\boldsymbol{Q}\\boldsymbol{y}_t - \\boldsymbol{b}) = -s\\nabla f(\\boldsymbol{x}_t)$。因此，$\\|\\boldsymbol{r}_t\\|_2 = s\\|\\nabla f(\\boldsymbol{x}_t)\\|_2$。\n停止准则 $\\|\\nabla f(\\boldsymbol{x}_t)\\|_2 \\le \\varepsilon$ 等价于 $\\|\\boldsymbol{r}_t\\|_2/s \\le \\varepsilon$。我们来分析相对残差的减少。初始残差为 $\\boldsymbol{r}_0 = s\\boldsymbol{b} - A_k\\boldsymbol{y}_0 = s\\boldsymbol{b}$。该条件可以表示为：\n$$ \\frac{\\|\\boldsymbol{r}_t\\|_2}{\\|\\boldsymbol{r}_0\\|_2} \\le \\frac{s\\varepsilon}{\\|s\\boldsymbol{b}\\|_2} = \\frac{s\\varepsilon}{s\\|\\boldsymbol{b}\\|_2} = \\frac{\\varepsilon}{\\|\\boldsymbol{b}\\|_2} $$\n由于系统矩阵的条件数和所需的相对残差减少量都与缩放因子 $s$ 无关，因此达到该准则所需的 CG 迭代次数对于所有测试的 $k$ 值都将是恒定的。在有 $n=10$ 个不同特征值的情况下，CG 在精确算术中保证最多在 10 次迭代内收敛。\n\n**3. 牛顿法**\n用于最小化 $g_k(\\boldsymbol{y})$ 的牛顿法使用以下更新规则：\n$$ \\boldsymbol{y}_{t+1} = \\boldsymbol{y}_{t} - (\\nabla^2 g_k(\\boldsymbol{y}_t))^{-1} \\nabla g_k(\\boldsymbol{y}_t) $$\n对于我们的二次问题，海森矩阵 $\\nabla^2 g_k(\\boldsymbol{y}) = s^2\\boldsymbol{Q}$ 是常数。从 $\\boldsymbol{y}_0 = \\boldsymbol{0}$ 开始，第一次迭代为：\n$$ \\boldsymbol{y}_1 = \\boldsymbol{y}_0 - (s^2\\boldsymbol{Q})^{-1} (s^2\\boldsymbol{Q}\\boldsymbol{y}_0 - s\\boldsymbol{b}) = \\boldsymbol{0} - (s^2\\boldsymbol{Q})^{-1} (-s\\boldsymbol{b}) = s^{-2}\\boldsymbol{Q}^{-1}s\\boldsymbol{b} = s^{-1}\\boldsymbol{Q}^{-1}\\boldsymbol{b} $$\n$g_k(\\boldsymbol{y})$ 的精确最小化点 $\\boldsymbol{y}^*$ 是通过求解 $\\nabla g_k(\\boldsymbol{y}^*) = s^2\\boldsymbol{Q}\\boldsymbol{y}^* - s\\boldsymbol{b} = \\boldsymbol{0}$ 得到的，即 $\\boldsymbol{y}^* = s^{-1}\\boldsymbol{Q}^{-1}\\boldsymbol{b}$。\n因此，牛顿法从任何起始点都能在单次迭代中找到精确解。此性质是仿射不变的，因此不受缩放因子 $s$ 的影响。如问题所述，对于所有 $k$，迭代次数都为 1。\n\n此分析显示了缩放的不同影响：它对调优不佳的 GD 是有害的，与 CG（对于此问题结构是一种尺度不变方法）无关，也与牛顿法（一种仿射不变方法）无关。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem for different scaling factors and reports\n    the number of iterations for Gradient Descent, Conjugate Gradient, and\n    Newton's method.\n    \"\"\"\n    # Problem Parameters\n    n = 10\n    epsilon = 1e-8\n    alpha_gd = 1e-4\n    N_max_GD = 20000\n    N_max_CG = n\n    k_values = [-6, -3, 0, 3, 6]\n\n    # Problem Setup: Q matrix and b vector\n    i = np.arange(n, dtype=float)\n    lambdas = 10.0**(4.0 * i / 9.0)\n    Q = np.diag(lambdas)\n    b = np.ones(n)\n\n    all_results = []\n    \n    # Pre-calculate initial gradient norm for GD divergence check\n    # x0 = 0, so grad_f(x0) = -b\n    grad_f_x0_norm = np.linalg.norm(b)\n\n    for k in k_values:\n        s = 10.0**k\n        iter_counts_for_k = []\n\n        # --- 1. Gradient Descent (GD) ---\n        y = np.zeros(n)\n        gd_iters = N_max_GD\n        divergence_threshold = 1e12 * grad_f_x0_norm\n\n        for t in range(N_max_GD):\n            x = s * y\n            grad_f = Q @ x - b\n            grad_f_norm = np.linalg.norm(grad_f)\n\n            if grad_f_norm <= epsilon:\n                gd_iters = t\n                break\n            \n            if grad_f_norm > divergence_threshold or not np.isfinite(grad_f_norm):\n                # As per instructions, report N_max_GD on divergence\n                gd_iters = N_max_GD\n                break\n            \n            # GD step on the scaled problem g_k(y)\n            grad_g = s*s * (Q @ y) - s * b\n            y = y - alpha_gd * grad_g\n        \n        iter_counts_for_k.append(gd_iters)\n\n        # --- 2. Conjugate Gradient (CG) ---\n        # Solve A*y = c where A = s^2*Q and c = s*b\n        A = (s**2) * Q\n        c = s * b\n        \n        y = np.zeros(n)\n        r = c - A @ y  # Initial residual r_0\n        p = r          # Initial search direction p_0\n        rs_old = np.dot(r, r)\n        \n        cg_iters = N_max_CG\n        \n        # Check an edge case where r_0 is already small, though not for y_0=0\n        if np.linalg.norm(r) / s <= epsilon:\n             cg_iters = 0\n        else:\n            for t in range(N_max_CG):\n                Ap = A @ p\n                alpha_cg = rs_old / np.dot(p, Ap)\n                \n                y = y + alpha_cg * p\n                r = r - alpha_cg * Ap\n                \n                # Check stopping criterion on the original gradient norm\n                # ||grad_f(x_t)||_2 = ||r_t||_2 / s\n                if np.linalg.norm(r) / s <= epsilon:\n                    cg_iters = t + 1\n                    break\n                \n                rs_new = np.dot(r, r)\n                # Update search direction\n                p = r + (rs_new / rs_old) * p\n                rs_old = rs_new\n\n        iter_counts_for_k.append(cg_iters)\n\n        # --- 3. Newton's Method ---\n        # Converges in 1 step for a quadratic function\n        newton_iters = 1\n        iter_counts_for_k.append(newton_iters)\n\n        all_results.append(iter_counts_for_k)\n\n    # Final print statement must match the specified format\n    print(f\"{all_results}\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "在观察到缩放效应之后，这个练习将深入探讨其背后的理论 。你将通过解析计算，分别在特征缩放前后，求解一个函数梯度的利普希茨常数（Lipschitz constant）——这是决定一阶方法收敛速度的关键参数。这项练习将在问题的几何曲率与算法性能的形式化分析之间，建立起一座至关重要的桥梁。",
            "id": "3158950",
            "problem": "考虑二次连续可微函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$，其定义为 $f(x)=\\frac{1}{2}x^{\\top}Qx$，其中 $Q\\in\\mathbb{R}^{2\\times 2}$ 是一个对称正定矩阵，由 $Q=\\begin{pmatrix}4 & 2\\\\2 & 3\\end{pmatrix}$ 给出。设范数为欧几里得范数 $|\\!|\\cdot|\\!|$。如果对于所有的 $x,y\\in\\mathbb{R}^{2}$，都有 $|\\!|\\nabla f(x)-\\nabla f(y)|\\!|\\le L|\\!|x-y|\\!|$ 成立，则称梯度是利普希茨连续的，常数为 $L$。从梯度利普希茨连续性的定义和标准的线性代数知识出发，确定满足此不等式的最小常数 $L$。\n\n现在，使用对角缩放矩阵 $D=\\operatorname{diag}(2,1)$ 进行特征缩放，并定义缩放后的变量 $z=Dx$。考虑缩放后的函数 $g:\\mathbb{R}^{2}\\to\\mathbb{R}$，其由 $g(z)=f(D^{-1}z)$ 给出。使用相同的欧几里得范数 $|\\!|\\cdot|\\!|$ 和相同的梯度利普希茨连续性定义，确定使得 $|\\!|\\nabla g(z)-\\nabla g(w)|\\!|\\le L|\\!|z-w|\\!|$ 对所有 $z,w\\in\\mathbb{R}^{2}$ 成立的最小常数 $L$。\n\n将你的最终答案表示为一个含两个元素的行矩阵，第一个元素等于 $f$ 的最小常数，第二个元素等于 $g$ 的最小常数。请提供精确的解析表达式，无需四舍五入。",
            "solution": "本问题要求解函数 $f(x)$ 及其缩放版本 $g(z)$ 的梯度的最小利普希茨常数。\n\n一个函数的梯度 $\\nabla h(x)$ 是利普希茨连续的，常数为 $L$，如果对于其定义域中的所有 $x, y$，不等式 $|\\!|\\nabla h(x) - \\nabla h(y)|\\!| \\le L|\\!|x-y|\\!|$ 均成立，其中 $|\\!|\\cdot|\\!|$ 是欧几里得范数。对于在凸集上定义的二次连续可微函数 $h$，这样的最小常数 $L$ 由其海森矩阵谱范数的上确界给出，即 $L = \\sup_x |\\!|\\nabla^2 h(x)|\\!|_2$。\n\n首先，我们分析函数 $f(x) = \\frac{1}{2}x^{\\top}Qx$，其中 $x \\in \\mathbb{R}^2$ 且 $Q = \\begin{pmatrix} 4  2 \\\\ 2  3 \\end{pmatrix}$。\n$f(x)$ 关于 $x$ 的梯度由 $\\nabla f(x) = \\frac{1}{2}(Q+Q^{\\top})x$ 给出。由于 $Q$ 是对称的，即 $Q=Q^{\\top}$，所以 $\\nabla f(x) = Qx$。\n$f(x)$ 的海森矩阵是 $\\nabla f(x)$ 的梯度，即 $\\nabla^2 f(x) = Q$。\n由于海森矩阵是常数，$\\nabla f$ 的最小利普希茨常数（我们记为 $L_f$）就是矩阵 $Q$ 的谱范数（或诱导2-范数），即 $L_f = |\\!|Q|\\!|_2$。\n对于对称矩阵，谱范数等于其绝对值最大的特征值。由于题目给出 $Q$ 是正定的，其特征值为正实数。因此，$L_f = \\lambda_{\\max}(Q)$。\n\n为求 $Q$ 的特征值，我们求解特征方程 $\\det(Q - \\lambda I) = 0$：\n$$\n\\det\\begin{pmatrix} 4-\\lambda  2 \\\\ 2  3-\\lambda \\end{pmatrix} = (4-\\lambda)(3-\\lambda) - (2)(2) = 0\n$$\n$$\n\\lambda^2 - 7\\lambda + 12 - 4 = 0\n$$\n$$\n\\lambda^2 - 7\\lambda + 8 = 0\n$$\n使用二次公式，特征值 $\\lambda$ 为：\n$$\n\\lambda = \\frac{-(-7) \\pm \\sqrt{(-7)^2 - 4(1)(8)}}{2(1)} = \\frac{7 \\pm \\sqrt{49 - 32}}{2} = \\frac{7 \\pm \\sqrt{17}}{2}\n$$\n两个特征值是 $\\lambda_1 = \\frac{7 - \\sqrt{17}}{2}$ 和 $\\lambda_2 = \\frac{7 + \\sqrt{17}}{2}$。如预期，两者均为正。\n最大特征值是 $\\lambda_{\\max}(Q) = \\frac{7 + \\sqrt{17}}{2}$。\n因此，$f$ 的梯度的最小利普希茨常数是 $L_f = \\frac{7 + \\sqrt{17}}{2}$。\n\n接下来，我们分析缩放后的函数 $g(z) = f(D^{-1}z)$，其中 $z \\in \\mathbb{R}^2$ 且 $D = \\operatorname{diag}(2,1) = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}$。\n$D$ 的逆矩阵是 $D^{-1} = \\begin{pmatrix} 1/2  0 \\\\ 0  1 \\end{pmatrix}$。\n代入 $f$ 的表达式，我们得到：\n$$\ng(z) = \\frac{1}{2}(D^{-1}z)^{\\top}Q(D^{-1}z) = \\frac{1}{2}z^{\\top}(D^{-1})^{\\top}Q D^{-1}z\n$$\n由于 $D$ 是对角矩阵，所以它是对称的，这意味着其逆矩阵 $D^{-1}$ 也是对称的。所以，$(D^{-1})^{\\top} = D^{-1}$。\n$$\ng(z) = \\frac{1}{2}z^{\\top}(D^{-1}QD^{-1})z\n$$\n这是关于 $z$ 的二次型，其矩阵为 $Q_g = D^{-1}QD^{-1}$。\n$g(z)$ 的梯度是 $\\nabla g(z) = Q_g z$，其海森矩阵是 $\\nabla^2 g(z) = Q_g$。\n由于 $Q$ 和 $D^{-1}$ 都是对称的，矩阵 $Q_g$ 也是对称的。$\\nabla g$ 的最小利普希茨常数（记为 $L_g$）是 $Q_g$ 的谱范数，也就是其最大特征值，因为 $Q_g$ 也将是正定的。$L_g = \\lambda_{\\max}(Q_g)$。\n\n我们现在计算矩阵 $Q_g$：\n$$\nQ_g = D^{-1}QD^{-1} = \\begin{pmatrix} 1/2  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 4  2 \\\\ 2  3 \\end{pmatrix} \\begin{pmatrix} 1/2  0 \\\\ 0  1 \\end{pmatrix}\n$$\n$$\nQ_g = \\begin{pmatrix} 1/2  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 2  2 \\\\ 1  3 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 1  3 \\end{pmatrix}\n$$\n为求 $Q_g$ 的特征值，我们求解其特征方程 $\\det(Q_g - \\mu I) = 0$：\n$$\n\\det\\begin{pmatrix} 1-\\mu  1 \\\\ 1  3-\\mu \\end{pmatrix} = (1-\\mu)(3-\\mu) - (1)(1) = 0\n$$\n$$\n\\mu^2 - 4\\mu + 3 - 1 = 0\n$$\n$$\n\\mu^2 - 4\\mu + 2 = 0\n$$\n使用二次公式，特征值 $\\mu$ 为：\n$$\n\\mu = \\frac{-(-4) \\pm \\sqrt{(-4)^2 - 4(1)(2)}}{2(1)} = \\frac{4 \\pm \\sqrt{16 - 8}}{2} = \\frac{4 \\pm 2\\sqrt{2}}{2} = 2 \\pm \\sqrt{2}\n$$\n两个特征值是 $\\mu_1 = 2 - \\sqrt{2}$ 和 $\\mu_2 = 2 + \\sqrt{2}$。两者均为正。\n最大特征值是 $\\lambda_{\\max}(Q_g) = 2 + \\sqrt{2}$。\n因此，$g$ 的梯度的最小利普希茨常数是 $L_g = 2 + \\sqrt{2}$。\n\n最终答案由两个最小常数 $L_f$ 和 $L_g$ 组成。\n$L_f = \\frac{7 + \\sqrt{17}}{2}$\n$L_g = 2 + \\sqrt{2}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7 + \\sqrt{17}}{2} & 2 + \\sqrt{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "本章的最后一个练习将展示预处理（preconditioning）的最终目标：将一个病态问题（ill-conditioned problem）转变为一个理想条件问题（perfectly conditioned problem）。你将实现一个基于Hessian矩阵特征分解的特定变量变换，它能理想地重塑问题的几何形态。通过比较最速下降法在变换前后的性能，你将见证其惊人的改进，并理解这类技术为何如此强大。",
            "id": "3158940",
            "problem": "要求您编写一个完整、可运行的程序，用于评估数值缩放对二次优化问题条件数的影响，及其对采用精确线搜索的最速下降法迭代次数的影响。考虑无约束二次最小化问题，其目标函数为 $f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\mathsf{T}}\\mathbf{H}\\mathbf{x} - \\mathbf{b}^{\\mathsf{T}}\\mathbf{x}$，其中 $\\mathbf{H}\\in\\mathbb{R}^{n\\times n}$ 是对称正定矩阵，$\\mathbf{b}\\in\\mathbb{R}^{n}$。请在两种参数化下实现采用精确线搜索的最速下降法：原始变量 $\\mathbf{x}$ 和缩放变量 $\\mathbf{z}=\\mathbf{P}\\mathbf{x}$，其中选择 $\\mathbf{P}$ 以展平 $\\mathbf{H}$ 的谱。具体来说，计算特征分解 $\\mathbf{H}=\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\mathsf{T}}$，其中 $\\mathbf{Q}$ 是标准正交矩阵，$\\mathbf{\\Lambda}=\\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{n})$ 是对角矩阵且其对角元均为正，并令 $\\mathbf{P}=\\mathbf{Q}\\mathbf{\\Lambda}^{1/2}$，其中 $\\mathbf{\\Lambda}^{1/2}=\\operatorname{diag}(\\sqrt{\\lambda_{1}},\\dots,\\sqrt{\\lambda_{n}})$。此选择将目标函数转换为一个新问题，其在 $\\mathbf{z}$ 坐标系下的海森矩阵是单位矩阵，从而展平了特征值。\n\n您的程序必须遵循的算法要求：\n- 在 $\\mathbf{x}$ 坐标系下实现采用精确线搜索的最速下降法，具体如下：从 $\\mathbf{x}_{0}$ 开始，迭代 $\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\alpha_{k}\\nabla f(\\mathbf{x}_{k})$，其中 $\\nabla f(\\mathbf{x})=\\mathbf{H}\\mathbf{x}-\\mathbf{b}$，步长 $\\alpha_{k}$ 使一维函数 $\\phi(\\alpha)=f(\\mathbf{x}_{k}-\\alpha\\nabla f(\\mathbf{x}_{k}))$ 在 $\\alpha\\in\\mathbb{R}$ 上最小化。当 $\\|\\nabla f(\\mathbf{x}_{k})\\|_{2}\\leq \\varepsilon \\|\\nabla f(\\mathbf{x}_{0})\\|_{2}$ 或达到最大迭代次数时终止。\n- 在 $\\mathbf{z}$ 坐标系下对变换后的目标函数 $f_{t}(\\mathbf{z})=\\tfrac{1}{2}\\mathbf{z}^{\\mathsf{T}}\\mathbf{I}\\mathbf{z}-\\mathbf{c}^{\\mathsf{T}}\\mathbf{z}$ 实现相同的方法，其中 $\\mathbf{c}=\\mathbf{P}^{-\\mathsf{T}}\\mathbf{b}$，从 $\\mathbf{z}_{0}=\\mathbf{P}\\mathbf{x}_{0}$ 开始，沿负梯度方向 $\\nabla f_{t}(\\mathbf{z})=\\mathbf{z}-\\mathbf{c}$ 使用精确线搜索，并在 $\\|\\nabla f_{t}(\\mathbf{z}_{k})\\|_{2}\\leq \\varepsilon \\|\\nabla f_{t}(\\mathbf{z}_{0})\\|_{2}$ 或达到最大迭代次数时终止。\n\n使用以下包含三个案例的测试套件。对于每个案例，容差为 $\\varepsilon=10^{-8}$，最大迭代次数为 $10^{6}$。所有向量均为列向量，但在代码中您可以将它们表示为一维数组。\n\n- 案例 1（各向同性海森矩阵，边界情况）：\n  - 维度 $n=4$。\n  - $\\mathbf{H}_{1}=\\mu\\mathbf{I}$，其中 $\\mu=7$。\n  - $\\mathbf{b}_{1}=[2,\\,-1,\\,3,\\,0.5]^{\\mathsf{T}}$。\n  - $\\mathbf{x}_{0,1}=\\mathbf{0}$。\n\n- 案例 2（轴对齐但病态）：\n  - 维度 $n=4$。\n  - $\\mathbf{H}_{2}=\\operatorname{diag}(0.1,\\,1,\\,10,\\,100)$。\n  - $\\mathbf{b}_{2}=[1,\\,1,\\,1,\\,1]^{\\mathsf{T}}$。\n  - $\\mathbf{x}_{0,2}=\\mathbf{0}$。\n\n- 案例 3（旋转的、强病态）：\n  - 维度 $n=6$。\n  - 令 $\\mathbf{A}_{3}$ 为\n    $\n    \\begin{bmatrix}\n    1  2  3  4  5  6\\\\\n    2  1  4  3  6  5\\\\\n    3  4  1  2  5  6\\\\\n    4  3  2  1  6  5\\\\\n    5  6  5  6  1  2\\\\\n    6  5  6  5  2  1\n    \\end{bmatrix}\n    $。\n    计算 $\\mathbf{A}_{3}$ 的瘦 $\\mathbf{Q}\\mathbf{R}$ 分解，并取标准正交因子 $\\mathbf{Q}_{3}$。\n  - 定义 $\\mathbf{\\Lambda}_{3}=\\operatorname{diag}(1,\\,2,\\,5,\\,20,\\,50,\\,1000)$ 和 $\\mathbf{H}_{3}=\\mathbf{Q}_{3}\\mathbf{\\Lambda}_{3}\\mathbf{Q}_{3}^{\\mathsf{T}}$。\n  - $\\mathbf{b}_{3}=[1,\\,-2,\\,3,\\,-4,\\,5,\\,-6]^{\\mathsf{T}}$。\n  - $\\mathbf{x}_{0,3}=\\mathbf{0}$。\n\n对于每个案例 $i\\in\\{1,2,3\\}$，您的程序必须计算：\n- $N_{x}^{(i)}$，在 $\\mathbf{x}$ 坐标系下，采用精确线搜索的最速下降法达到终止条件所需的迭代次数。\n- $N_{z}^{(i)}$，在 $\\mathbf{z}$ 坐标系下，采用精确线搜索的最速下降法达到终止条件所需的迭代次数。\n- $\\Delta^{(i)}=N_{x}^{(i)}-N_{z}^{(i)}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个案例的结果，格式为逗号分隔的列表，并用方括号括起来，顺序为 $[N_{x}^{(1)},N_{z}^{(1)},\\Delta^{(1)},N_{x}^{(2)},N_{z}^{(2)},\\Delta^{(2)},N_{x}^{(3)},N_{z}^{(3)},\\Delta^{(3)}]$。\n- 所有输出必须为整数。\n\n您的实现必须是自包含的，并且不能需要任何外部输入。此问题不涉及任何物理单位。不使用角度。在代码中将所有阈值和常数表示为纯数字，并确保所有案例的执行都在指定的迭代上限内完成。程序必须遵守最终答案规范中描述的运行时环境。",
            "solution": "经评估，用户提供的问题是有效的。这是一个在数值优化领域中具有科学依据、自包含且客观的适定问题。它要求实现并比较在有预处理和无预处理情况下，最速下降法应用于二次目标函数的性能。\n\n核心问题是二次函数 $f(\\mathbf{x}): \\mathbb{R}^{n} \\to \\mathbb{R}$ 的无约束最小化：\n$$f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^{\\mathsf{T}}\\mathbf{H}\\mathbf{x} - \\mathbf{b}^{\\mathsf{T}}\\mathbf{x}$$\n其中 $\\mathbf{H} \\in \\mathbb{R}^{n \\times n}$ 是一个对称正定 (SPD) 矩阵，$\\mathbf{b} \\in \\mathbb{R}^{n}$。该函数的梯度为 $\\nabla f(\\mathbf{x}) = \\mathbf{H}\\mathbf{x} - \\mathbf{b}$。\n\n### 原始坐标系（$\\mathbf{x}$）下的最速下降法\n\n最速下降法通过沿负梯度方向（即最速下降方向）移动来迭代更新当前解 $\\mathbf{x}_k$。更新规则为：\n$$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} + \\alpha_k \\mathbf{p}_k = \\mathbf{x}_{k} - \\alpha_k \\nabla f(\\mathbf{x}_k)$$\n其中 $\\mathbf{p}_k = - \\nabla f(\\mathbf{x}_k)$ 是搜索方向，$\\alpha_k > 0$ 是步长。\n\n问题指定使用精确线搜索来确定最优步长 $\\alpha_k$。这涉及最小化关于 $\\alpha$ 的一维函数 $\\phi(\\alpha) = f(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k))$。我们通过将其导数 $\\frac{d\\phi}{d\\alpha}$ 置零来找到最小值。\n令 $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$。\n$$\\phi(\\alpha) = \\frac{1}{2}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k)^{\\mathsf{T}}\\mathbf{H}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k) - \\mathbf{b}^{\\mathsf{T}}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k)$$\n$$\\frac{d\\phi}{d\\alpha} = -\\mathbf{g}_k^{\\mathsf{T}}\\mathbf{H}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k) + \\mathbf{g}_k^{\\mathsf{T}}\\mathbf{b} = -\\mathbf{g}_k^{\\mathsf{T}}(\\mathbf{H}\\mathbf{x}_k - \\mathbf{b}) + \\alpha\\mathbf{g}_k^{\\mathsf{T}}\\mathbf{H}\\mathbf{g}_k$$\n使用梯度定义 $\\mathbf{g}_k = \\mathbf{H}\\mathbf{x}_k - \\mathbf{b}$，我们得到：\n$$\\frac{d\\phi}{d\\alpha} = -\\mathbf{g}_k^{\\mathsf{T}}\\mathbf{g}_k + \\alpha\\mathbf{g}_k^{\\mathsf{T}}\\mathbf{H}\\mathbf{g}_k$$\n将其置为 0 并求解 $\\alpha$，得到最优步长 $\\alpha_k$：\n$$\\alpha_k = \\frac{\\mathbf{g}_k^{\\mathsf{T}}\\mathbf{g}_k}{\\mathbf{g}_k^{\\mathsf{T}}\\mathbf{H}\\mathbf{g}_k}$$\n迭代过程持续进行，直到满足终止条件 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon \\|\\nabla f(\\mathbf{x}_0)\\|_2$ 或达到最大迭代次数。\n\n### 缩放坐标系（$\\mathbf{z}$）下的最速下降法\n\n最速下降法的收敛速率受海森矩阵 $\\mathbf{H}$ 的条件数 $\\kappa(\\mathbf{H}) = \\lambda_{\\max}/\\lambda_{\\min}$ 的限制。如果 $\\kappa(\\mathbf{H}) \\gg 1$，问题是病态的，收敛可能非常缓慢。预处理旨在将问题转换为一个条件更好的问题。\n\n我们使用变量替换 $\\mathbf{z} = \\mathbf{P}\\mathbf{x}$，这意味着 $\\mathbf{x} = \\mathbf{P}^{-1}\\mathbf{z}$。选择缩放矩阵 $\\mathbf{P}$ 来“白化”海森矩阵。基于 SPD 矩阵 $\\mathbf{H} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\mathsf{T}}$ 的特征分解（其中 $\\mathbf{Q}$ 是标准正交的，$\\mathbf{\\Lambda}$ 是正特征值的对角矩阵），我们选择 $\\mathbf{P} = \\mathbf{Q}\\mathbf{\\Lambda}^{1/2}$。\n\n以 $\\mathbf{z}$ 表示的目标函数为 $f_t(\\mathbf{z}) = f(\\mathbf{P}^{-1}\\mathbf{z})$：\n$$f_t(\\mathbf{z}) = \\frac{1}{2}(\\mathbf{P}^{-1}\\mathbf{z})^{\\mathsf{T}}\\mathbf{H}(\\mathbf{P}^{-1}\\mathbf{z}) - \\mathbf{b}^{\\mathsf{T}}(\\mathbf{P}^{-1}\\mathbf{z})$$\n$$f_t(\\mathbf{z}) = \\frac{1}{2}\\mathbf{z}^{\\mathsf{T}}(\\mathbf{P}^{-1})^{\\mathsf{T}}\\mathbf{H}\\mathbf{P}^{-1}\\mathbf{z} - (\\mathbf{P}^{-\\mathsf{T}}\\mathbf{b})^{\\mathsf{T}}\\mathbf{z}$$\n新的海森矩阵是 $\\mathbf{H}_t = (\\mathbf{P}^{-1})^{\\mathsf{T}}\\mathbf{H}\\mathbf{P}^{-1}$。代入 $\\mathbf{P}$ 和 $\\mathbf{H}$：\n$$\\mathbf{H}_t = ((\\mathbf{Q}\\mathbf{\\Lambda}^{1/2})^{-1})^{\\mathsf{T}} (\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\mathsf{T}}) ((\\mathbf{Q}\\mathbf{\\Lambda}^{1/2})^{-1}) = (\\mathbf{\\Lambda}^{-1/2}\\mathbf{Q}^{\\mathsf{T}})^{\\mathsf{T}} (\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\mathsf{T}}) (\\mathbf{\\Lambda}^{-1/2}\\mathbf{Q}^{\\mathsf{T}})$$\n$$\\mathbf{H}_t = (\\mathbf{Q}\\mathbf{\\Lambda}^{-1/2}) (\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\mathsf{T}}) (\\mathbf{\\Lambda}^{-1/2}\\mathbf{Q}^{\\mathsf{T}}) = \\mathbf{Q}\\mathbf{\\Lambda}^{-1/2}\\mathbf{\\Lambda}\\mathbf{\\Lambda}^{-1/2}\\mathbf{Q}^{\\mathsf{T}} = \\mathbf{Q}\\mathbf{I}\\mathbf{Q}^{\\mathsf{T}} = \\mathbf{I}$$\n变换后的海森矩阵是单位矩阵，其条件数为 $\\kappa(\\mathbf{I})=1$。问题是完美条件的。变换后的目标函数为：\n$$f_t(\\mathbf{z}) = \\frac{1}{2}\\mathbf{z}^{\\mathsf{T}}\\mathbf{I}\\mathbf{z} - \\mathbf{c}^{\\mathsf{T}}\\mathbf{z}, \\quad \\text{其中} \\quad \\mathbf{c} = \\mathbf{P}^{-\\mathsf{T}}\\mathbf{b} = \\mathbf{Q}\\mathbf{\\Lambda}^{-1/2}\\mathbf{b}$$\n梯度为 $\\nabla f_t(\\mathbf{z}) = \\mathbf{z} - \\mathbf{c}$。最小值在 $\\nabla f_t(\\mathbf{z}) = \\mathbf{0}$ 处取得，即 $\\mathbf{z}^* = \\mathbf{c}$。\n\n对 $f_t(\\mathbf{z})$ 应用采用精确线搜索的最速下降法：\n- 搜索方向为 $\\mathbf{p}_k = - \\nabla f_t(\\mathbf{z}_k) = -(\\mathbf{z}_k - \\mathbf{c})$。\n- 步长公式给出 $\\alpha_k = \\frac{\\nabla f_t(\\mathbf{z}_k)^{\\mathsf{T}}\\nabla f_t(\\mathbf{z}_k)}{\\nabla f_t(\\mathbf{z}_k)^{\\mathsf{T}}\\mathbf{H}_t\\nabla f_t(\\mathbf{z}_k)} = \\frac{\\nabla f_t(\\mathbf{z}_k)^{\\mathsf{T}}\\nabla f_t(\\mathbf{z}_k)}{\\nabla f_t(\\mathbf{z}_k)^{\\mathsf{T}}\\mathbf{I}\\nabla f_t(\\mathbf{z}_k)} = 1$。\n步长始终为 $\\alpha_k=1$。更新规则变为：\n$$\\mathbf{z}_{k+1} = \\mathbf{z}_k - 1 \\cdot \\nabla f_t(\\mathbf{z}_k) = \\mathbf{z}_k - (\\mathbf{z}_k - \\mathbf{c}) = \\mathbf{c}$$\n因此，从任何 $\\mathbf{z}_0 \\neq \\mathbf{c}$ 开始，算法在一步内收敛到精确解 $\\mathbf{z}^*=\\mathbf{c}$。对于给定的测试案例，$\\mathbf{x}_0 = \\mathbf{0}$，因此 $\\mathbf{z}_0 = \\mathbf{P}\\mathbf{x}_0 = \\mathbf{0}$。初始梯度 $\\nabla f_t(\\mathbf{z}_0) = -\\mathbf{c}$ 是非零的，因此算法将恰好需要一次迭代。因此，对于所有案例 $i=1, 2, 3$，都有 $N_z^{(i)}=1$。\n\n对于案例 1，海森矩阵 $\\mathbf{H}_1=\\mu\\mathbf{I}$ 已经是完美条件的，其条件数为 $\\kappa(\\mathbf{H}_1)=1$。在原始坐标系中的最速下降法也预计在一步内收敛。因此我们预测 $N_x^{(1)}=1$。\n\n对于案例 2 和 3，海森矩阵被构造成病态的，其条件数为 $\\kappa(\\mathbf{H}) = 1000$。这将导致收敛缓慢，我们预计 $N_x^{(2)}$ 和 $N_x^{(3)}$ 将显著大于 1。\n\n### 实现\n\n实现了两个函数：一个用于在 $\\mathbf{x}$ 坐标系下进行最速下降（`steepest_descent_x`），另一个首先计算变换，然后在 $\\mathbf{z}$ 坐标系下进行最速下降（`steepest_descent_z`）。主函数 `solve` 为三个测试案例中的每一个构建矩阵，并调用这两个函数来计算迭代次数 $N_x$ 和 $N_z$。然后为每个案例计算差值 $\\Delta = N_x - N_z$。结果被汇总并以所需格式打印。",
            "answer": "```python\nimport numpy as np\n\ndef steepest_descent_x(H, b, x0, epsilon, max_iter):\n    \"\"\"\n    Performs steepest descent with exact line search in x-coordinates.\n    \"\"\"\n    x = x0.copy()\n    g = H @ x - b\n    g0_norm = np.linalg.norm(g)\n\n    if g0_norm == 0:\n        return 0\n\n    term_thresh = epsilon * g0_norm\n    \n    k = 0\n    while k < max_iter:\n        g_norm = np.linalg.norm(g)\n        if g_norm <= term_thresh:\n            break\n        \n        # Exact line search step size\n        alpha = (g.T @ g) / (g.T @ H @ g)\n        \n        # Update step\n        x = x - alpha * g\n        \n        # Update gradient\n        g = H @ x - b\n        \n        k += 1\n        \n    return k\n\ndef steepest_descent_z(H, b, x0, epsilon, max_iter):\n    \"\"\"\n    Performs steepest descent with exact line search in z-coordinates.\n    \"\"\"\n    # Eigendecomposition of H = Q Lambda Q.T\n    # eigh is for symmetric/hermitian matrices\n    eigvals, Q = np.linalg.eigh(H)\n    \n    # Transformation matrices\n    # P = Q * Lambda^(1/2)\n    # P_inv_T = Q * Lambda^(-1/2) (derived in solution explanation)\n    sqrt_eigvals = np.sqrt(eigvals)\n    inv_sqrt_eigvals = 1.0 / sqrt_eigvals\n    \n    P = Q @ np.diag(sqrt_eigvals)\n    P_inv_T = Q @ np.diag(inv_sqrt_eigvals)\n    \n    # Transformed problem parameters\n    c = P_inv_T @ b\n    z0 = P @ x0\n    \n    z = z0.copy()\n    g_t = z - c # Gradient in z-space\n    g0_norm_t = np.linalg.norm(g_t)\n\n    if g0_norm_t == 0:\n        return 0\n        \n    term_thresh_t = epsilon * g0_norm_t\n\n    k = 0\n    while k < max_iter:\n        g_t_norm = np.linalg.norm(g_t)\n        if g_t_norm <= term_thresh_t:\n            break\n            \n        # Exact line search step size is always 1 for z-space problem\n        alpha = 1.0\n        \n        # Update step\n        z = z - alpha * g_t\n        \n        # Update gradient\n        g_t = z - c\n        \n        k += 1\n        \n    return k\n\ndef solve():\n    \"\"\"\n    Sets up and solves the three test cases as specified in the problem.\n    \"\"\"\n    epsilon = 1e-8\n    max_iter = 10**6\n    results = []\n\n    # Case 1\n    n1 = 4\n    mu = 7.0\n    H1 = np.diag([mu] * n1)\n    b1 = np.array([2.0, -1.0, 3.0, 0.5])\n    x01 = np.zeros(n1)\n    \n    Nx1 = steepest_descent_x(H1, b1, x01, epsilon, max_iter)\n    Nz1 = steepest_descent_z(H1, b1, x01, epsilon, max_iter)\n    D1 = Nx1 - Nz1\n    results.extend([Nx1, Nz1, D1])\n\n    # Case 2\n    n2 = 4\n    H2 = np.diag([0.1, 1.0, 10.0, 100.0])\n    b2 = np.ones(n2)\n    x02 = np.zeros(n2)\n\n    Nx2 = steepest_descent_x(H2, b2, x02, epsilon, max_iter)\n    Nz2 = steepest_descent_z(H2, b2, x02, epsilon, max_iter)\n    D2 = Nx2 - Nz2\n    results.extend([Nx2, Nz2, D2])\n\n    # Case 3\n    n3 = 6\n    A3 = np.array([\n        [1, 2, 3, 4, 5, 6],\n        [2, 1, 4, 3, 6, 5],\n        [3, 4, 1, 2, 5, 6],\n        [4, 3, 2, 1, 6, 5],\n        [5, 6, 5, 6, 1, 2],\n        [6, 5, 6, 5, 2, 1]\n    ], dtype=float)\n    \n    Q3, _ = np.linalg.qr(A3, mode='reduced')\n    Lambda3_diag = np.array([1.0, 2.0, 5.0, 20.0, 50.0, 1000.0])\n    Lambda3 = np.diag(Lambda3_diag)\n    H3 = Q3 @ Lambda3 @ Q3.T\n    \n    b3 = np.array([1.0, -2.0, 3.0, -4.0, 5.0, -6.0])\n    x03 = np.zeros(n3)\n\n    Nx3 = steepest_descent_x(H3, b3, x03, epsilon, max_iter)\n    Nz3 = steepest_descent_z(H3, b3, x03, epsilon, max_iter)\n    D3 = Nx3 - Nz3\n    results.extend([Nx3, Nz3, D3])\n    \n    # Cast to integer as per output format specification\n    results_int = [int(r) for r in results]\n    \n    # Final print statement\n    print(f\"[{','.join(map(str, results_int))}]\")\n\nsolve()\n\n```"
        }
    ]
}