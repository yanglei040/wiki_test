{
    "hands_on_practices": [
        {
            "introduction": "Many real-world optimization problems involve objectives that are not smoothly differentiable, such as modeling costs that only apply above a certain threshold. This exercise explores how to handle piecewise linear convex functions, specifically those involving the $\\max(0, z)$ structure, which is common in machine learning as the \"hinge loss\" or \"ReLU activation.\" By working through this practice, you will learn the essential technique of using auxiliary variables to reformulate a seemingly complex, non-differentiable problem into a standard Linear Program (LP), making it solvable with efficient, widely available algorithms .",
            "id": "3108327",
            "problem": "Consider the optimization problem with decision vector $x \\in \\mathbb{R}^{n}$:\nMinimize $f(x) = \\sum_{i=1}^{n} \\max\\!\\big(0,\\, x_{i} - \\tau\\big)$ subject to $A x = b$, where $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, and $\\tau \\in \\mathbb{R}$ are given data. Using only foundational definitions and closure properties, reason from first principles about the shape and algebraic structure of the objective and the feasible region. Then identify which of the following statements correctly classify the problem and provide a valid equivalent reformulation when appropriate. Select all that apply.\n\n- A. The problem is nonconvex because $\\max(\\cdot)$ is nondifferentiable; it cannot be represented as a Linear Program (LP).\n\n- B. The problem is convex and piecewise linear; it is equivalent to the Linear Program (LP): minimize $\\sum_{i=1}^{n} t_{i}$ over $(x,t) \\in \\mathbb{R}^{n} \\times \\mathbb{R}^{n}$ subject to $A x = b$, $t_{i} \\ge x_{i} - \\tau$, and $t_{i} \\ge 0$ for all $i \\in \\{1,\\dots,n\\}$.\n\n- C. The problem is convex but not piecewise linear; the correct equivalent reformulation is a Quadratic Program (QP): minimize $\\sum_{i=1}^{n} s_{i}^{2}$ subject to $A x = b$, $s_{i} \\ge x_{i} - \\tau$, and $s_{i} \\ge 0$ for all $i \\in \\{1,\\dots,n\\}$.\n\n- D. The problem is linear because each $\\max(\\cdot)$ can be removed by enforcing $x_{i} \\ge \\tau$; equivalently, minimize $\\sum_{i=1}^{n} (x_{i} - \\tau)$ subject to $A x = b$ and $x_{i} \\ge \\tau$ for all $i \\in \\{1,\\dots,n\\}$.\n\n- E. The problem is convex and piecewise linear but requires Mixed-Integer Linear Programming (MILP) with binary variables to express the $\\max(\\cdot)$ terms exactly.",
            "solution": "The problem statement will first be validated for scientific soundness, clarity, and completeness.\n\n**Step 1: Extract Givens**\n- Decision vector: $x \\in \\mathbb{R}^{n}$\n- Objective function: Minimize $f(x) = \\sum_{i=1}^{n} \\max\\!\\big(0,\\, x_{i} - \\tau\\big)$\n- Constraint: $A x = b$\n- Data: $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $\\tau \\in \\mathbb{R}$\n- Question: Classify the problem and identify valid equivalent reformulations.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is mathematically well-defined.\n- **Scientific Grounding:** The problem is a standard form of an optimization problem frequently encountered in fields like statistics (e.g., Lasso with a hinge loss variant) and machine learning. The objective function is a sum of shifted Rectified Linear Unit (ReLU) functions, and the constraints are linear. All components are based on established mathematical principles.\n- **Well-Posed:** The problem is well-posed for the purpose of classification. It presents a clear objective function to be minimized over a clearly defined feasible set. The existence and uniqueness of a minimizer depend on the properties of $A$ and $b$, but the structure of the problem itself is unambiguous.\n- **Objective:** The language is precise and uses standard terminology from the field of optimization theory (e.g., \"decision vector,\" \"objective,\" \"convex,\" \"Linear Program\").\n\nThe problem statement has no scientific or factual unsoundness, is formal and relevant, is complete, is mathematically feasible, is well-posed for classification, and is not trivial.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. We may proceed with the solution.\n\n**Derivation from First Principles**\n\n1.  **Analysis of the Feasible Region:**\n    The feasible region is defined by the set $\\mathcal{F} = \\{x \\in \\mathbb{R}^{n} \\mid A x = b\\}$. This is the solution set to a system of linear equations. Such a set is an affine subspace of $\\mathbb{R}^{n}$. Any affine set is, by definition, a convex set. Furthermore, it is a polyhedron.\n\n2.  **Analysis of the Objective Function:**\n    The objective function is $f(x) = \\sum_{i=1}^{n} f_i(x_i)$, where $f_i(x_i) = \\max(0, x_i - \\tau)$.\n    -   **Shape of $f_i(x_i)$**: The function $f_i(x_i)$ is composed of two linear pieces: it is $0$ for $x_i \\le \\tau$ and $x_i - \\tau$ for $x_i > \\tau$. Therefore, $f_i(x_i)$ is a piecewise linear function.\n    -   **Convexity of $f_i(x_i)$**: A function is convex if its epigraph is a convex set. Alternatively, we can use closure properties. The function $g_1(x_i) = 0$ is a constant function, hence convex. The function $g_2(x_i) = x_i - \\tau$ is an affine function, hence convex. The pointwise maximum of two (or more) convex functions is also a convex function. Thus, $f_i(x_i) = \\max(g_1(x_i), g_2(x_i))$ is a convex function.\n    -   **Properties of the full objective $f(x)$**: The function $f(x)$ is a sum of the functions $f_i(x_i)$.\n        -   The sum of convex functions is a convex function. Since each $f_i(x_i)$ is convex, their sum $f(x)$ is convex.\n        -   The sum of piecewise linear functions is a piecewise linear function. Since each $f_i(x_i)$ is piecewise linear, their sum $f(x)$ is also piecewise linear.\n\n3.  **Overall Problem Classification:**\n    The problem is to minimize a convex, piecewise linear function $f(x)$ over a convex, polyhedral set $\\mathcal{F}$. This is, by definition, a convex optimization problem. Problems of this specific form (piecewise linear convex objective, linear constraints) can be reformulated as Linear Programs (LPs).\n\n4.  **LP Reformulation:**\n    To transform the problem into an LP, we introduce auxiliary variables to handle the $\\max$ operator. For each term $\\max(0, x_i - \\tau)$ in the objective, we introduce a new variable $t_i \\in \\mathbb{R}$. We want to set $t_i = \\max(0, x_i - \\tau)$. In a minimization context, this equality can be replaced by an inequality, $t_i \\ge \\max(0, x_i - \\tau)$, because the minimization of $\\sum t_i$ will drive each $t_i$ down to its smallest possible value, which is exactly $\\max(0, x_i - \\tau)$.\n    The inequality $t_i \\ge \\max(0, x_i - \\tau)$ is equivalent to the pair of linear inequalities:\n    $t_i \\ge 0$\n    $t_i \\ge x_i - \\tau$\n    The original problem can thus be written as:\n    Minimize $\\sum_{i=1}^{n} t_i$\n    subject to:\n    $A x = b$\n    $t_i \\ge x_i - \\tau$, for $i = 1, \\dots, n$\n    $t_i \\ge 0$, for $i = 1, \\dots, n$\n    The new optimization variables are $(x, t) \\in \\mathbb{R}^{n} \\times \\mathbb{R}^{n}$. The new objective function is linear, and all constraints are linear. Therefore, this is a Linear Program (LP).\n\n**Option-by-Option Analysis**\n\n-   **A. The problem is nonconvex because $\\max(\\cdot)$ is nondifferentiable; it cannot be represented as a Linear Program (LP).**\n    This statement makes two incorrect claims.\n    1.  The premise \"nonconvex because $\\max(\\cdot)$ is nondifferentiable\" is false. Convexity does not require differentiability. As shown above, the function $f(x)$ is convex because it is the sum of pointwise maxima of affine functions.\n    2.  The conclusion \"it cannot be represented as a Linear Program (LP)\" is false. As demonstrated in the derivation, the problem is equivalent to an LP.\n    **Verdict: Incorrect.**\n\n-   **B. The problem is convex and piecewise linear; it is equivalent to the Linear Program (LP): minimize $\\sum_{i=1}^{n} t_{i}$ over $(x,t) \\in \\mathbb{R}^{n} \\times \\mathbb{R}^{n}$ subject to $A x = b$, $t_{i} \\ge x_{i} - \\tau$, and $t_{i} \\ge 0$ for all $i \\in \\{1,\\dots,n\\}$.**\n    This statement is fully consistent with our analysis from first principles.\n    1.  The classification \"convex and piecewise linear\" is correct.\n    2.  The provided LP reformulation is precisely the one derived above. It correctly models the original problem.\n    **Verdict: Correct.**\n\n-   **C. The problem is convex but not piecewise linear; the correct equivalent reformulation is a Quadratic Program (QP): minimize $\\sum_{i=1}^{n} s_{i}^{2}$ subject to $A x = b$, $s_{i} \\ge x_{i} - \\tau$, and $s_{i} \\ge 0$ for all $i \\in \\{1,\\dots,n\\}$.**\n    This statement contains multiple errors.\n    1.  The claim \"not piecewise linear\" is false. The objective function is a sum of piecewise linear functions, which is itself piecewise linear.\n    2.  The proposed reformulation is not equivalent to the original problem. Minimizing $\\sum s_i^2$ is fundamentally different from minimizing $\\sum \\max(0, x_i - \\tau)$. A Quadratic Program (QP) has a quadratic objective, which this formulation does, but it does not solve the original problem.\n    **Verdict: Incorrect.**\n\n-   **D. The problem is linear because each $\\max(\\cdot)$ can be removed by enforcing $x_{i} \\ge \\tau$; equivalently, minimize $\\sum_{i=1}^{n} (x_{i} - \\tau)$ subject to $A x = b$ and $x_{i} \\ge \\tau$ for all $i \\in \\{1,\\dots,n\\}$.**\n    This statement proposes an incorrect simplification.\n    1.  The original objective function $f(x) = \\sum \\max(0, x_i-\\tau)$ is not linear; it is piecewise linear.\n    2.  The proposed reformulation is not equivalent. It adds new constraints, $x_i \\ge \\tau$ for all $i$, that are not present in the original problem. This shrinks the feasible set. The optimal solution to the original problem could very well have some components $x_i < \\tau$. This reformulation finds the optimal solution only within a specific subset of the original feasible domain, which does not guarantee finding the true global optimum of the original problem.\n    **Verdict: Incorrect.**\n\n-   **E. The problem is convex and piecewise linear but requires Mixed-Integer Linear Programming (MILP) with binary variables to express the $\\max(\\cdot)$ terms exactly.**\n    This statement is partially correct but ultimately wrong.\n    1.  The classification \"convex and piecewise linear\" is correct.\n    2.  The claim that it \"requires Mixed-Integer Linear Programming (MILP)\" is false. While it is possible to model a $\\max$ function using binary variables, it is not necessary here. The need for integer variables typically arises in modeling non-convex functions or logical disjunctions. Because the convex function $\\sum \\max(0, x_i - \\tau)$ is being minimized, the simple LP formulation with auxiliary variables works perfectly. The optimization process itself ensures the inequalities are tight at the solution.\n    **Verdict: Incorrect.**",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Minimizing squared errors is a cornerstone of data analysis, forming the basis of methods like linear regression. This exercise takes that one step further by incorporating constraints, a common scenario when physical or practical limits apply to the variables. You will classify a classic least-squares problem with box constraints as a Quadratic Program (QP) and, more importantly, analyze how its convexity depends on the properties of the data matrix $A$. This practice solidifies the crucial link between linear algebra concepts, like matrix rank, and optimization theory, helping you understand when a problem is guaranteed to have a single, easily found global minimum .",
            "id": "3108413",
            "problem": "Consider the optimization problem\nminimize over $x \\in \\mathbb{R}^n$: $f(x) = \\lVert A x - b \\rVert_2^2$ subject to $l \\le x \\le u$,\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $l, u \\in \\mathbb{R}^n$ with $l_i \\le u_i$ for all $i \\in \\{1,\\dots,n\\}$. Using only the core definitions that a Quadratic Program (QP) is an optimization problem with a quadratic objective and linear constraints, that a Linear Program (LP) has a linear objective and linear constraints, that a Quadratically Constrained Quadratic Program (QCQP) has at least one quadratic constraint, and that for a twice continuously differentiable function $f$, convexity corresponds to a positive semidefinite Hessian and strict convexity corresponds to a positive definite Hessian on the domain, classify the problem and determine how strict convexity of $f$ depends on $A$. Select the single correct statement.\n\nA. The problem is a convex Quadratic Program (QP) because the objective is quadratic and the box constraints are linear; the objective $f$ is strictly convex if and only if $A$ has full column rank, equivalently $A^\\top A \\succ 0$, and otherwise $f$ is merely convex.\n\nB. The problem is a nonconvex Quadratic Program because $A$ may be rectangular; the objective $f$ is strictly convex if $A$ has full row rank.\n\nC. The problem is a Linear Program (LP) because the constraints are linear; the objective $f$ is never strictly convex regardless of $A$.\n\nD. The problem is a convex Quadratically Constrained Quadratic Program (QCQP) because the norm involves a quadratic expression; the objective $f$ is strictly convex for any nonzero $A$.\n\nE. The problem is a convex Quadratic Program (QP); the objective $f$ is strictly convex if and only if $A$ is square and invertible.",
            "solution": "This problem statement is valid. It presents a standard bounded-variable least squares problem, is mathematically well-defined, uses standard terminology, and provides sufficient information to classify the problem and determine its properties.\n\nFirst, we analyze the objective function $f(x) = \\lVert A x - b \\rVert_2^2$. By definition of the squared Euclidean norm, we have:\n$$ f(x) = (Ax - b)^\\top (Ax - b) $$\nExpanding this expression gives:\n$$ f(x) = (x^\\top A^\\top - b^\\top)(Ax - b) = x^\\top A^\\top A x - x^\\top A^\\top b - b^\\top A x + b^\\top b $$\nSince the term $b^\\top A x$ is a scalar ($1 \\times 1$ matrix), it is equal to its own transpose, $(b^\\top A x)^\\top = x^\\top A^\\top b$. Therefore, we can combine the two linear terms:\n$$ f(x) = x^\\top (A^\\top A) x - 2 b^\\top A x + \\lVert b \\rVert_2^2 $$\nThis expression is a quadratic function of the variable $x \\in \\mathbb{R}^n$. The matrix of the quadratic form is $A^\\top A$, the linear part is determined by the vector $-2 A^\\top b$, and there is a constant term $\\lVert b \\rVert_2^2$.\n\nNext, we analyze the constraints, which are given by $l \\le x \\le u$. These are known as box constraints. This is a shorthand for a set of $2n$ linear inequalities: $l_i \\le x_i$ and $x_i \\le u_i$ for $i \\in \\{1, \\dots, n\\}$. These inequalities are linear in the components of $x$.\n\nBased on the problem's definitions, a Quadratic Program (QP) is an optimization problem with a quadratic objective and linear constraints. Since our objective function is quadratic and our constraints are linear, the problem is a QP.\n\nNow, we determine the convexity of the objective function $f(x)$. For a twice continuously differentiable function, convexity is determined by the properties of its Hessian matrix, $\\nabla^2 f(x)$. First, we compute the gradient of $f(x)$:\n$$ \\nabla f(x) = 2 (A^\\top A) x - 2 A^\\top b = 2A^\\top(Ax-b) $$\nThe Hessian matrix is the Jacobian of the gradient:\n$$ \\nabla^2 f(x) = \\nabla_x \\left( 2 A^\\top A x - 2 A^\\top b \\right) = 2 A^\\top A $$\nThe Hessian matrix is a constant matrix, $2 A^\\top A$.\n\nFor $f(x)$ to be convex, its Hessian must be positive semidefinite (PSD). A matrix $H$ is PSD if $v^\\top H v \\ge 0$ for all vectors $v \\in \\mathbb{R}^n$. Let's test our Hessian $H = 2A^\\top A$:\n$$ v^\\top H v = v^\\top (2 A^\\top A) v = 2 (v^\\top A^\\top) (A v) = 2 (Av)^\\top (Av) = 2 \\lVert Av \\rVert_2^2 $$\nSince the squared Euclidean norm of any vector is always non-negative, $\\lVert Av \\rVert_2^2 \\ge 0$ for all $v$. Thus, the Hessian $2 A^\\top A$ is always positive semidefinite, regardless of the properties of $A$. This implies that the objective function $f(x)$ is always convex. Therefore, the problem is a convex QP.\n\nFor $f(x)$ to be strictly convex, its Hessian must be positive definite (PD). A matrix $H$ is PD if $v^\\top H v > 0$ for all non-zero vectors $v \\in \\mathbb{R}^n, v \\ne 0$. Using our previous result:\n$$ v^\\top H v = 2 \\lVert Av \\rVert_2^2 $$\nFor this to be strictly positive for any $v \\ne 0$, we require that $\\lVert Av \\rVert_2^2 > 0$, which is equivalent to $Av \\ne 0$ for all $v \\ne 0$. The set of vectors $v$ for which $Av = 0$ is the null space of $A$, denoted $\\text{null}(A)$. The condition $Av \\ne 0$ for all $v \\ne 0$ is equivalent to stating that the null space of $A$ contains only the zero vector, i.e., $\\text{null}(A) = \\{0\\}$. This is the definition of the matrix $A$ having linearly independent columns, also known as having full column rank.\n\nThe condition that the Hessian $H = 2A^\\top A$ is positive definite is equivalent to the $n \\times n$ matrix $A^\\top A$ being positive definite ($A^\\top A \\succ 0$). A fundamental result from linear algebra is that $A^\\top A$ is positive definite if and only if $A$ has full column rank. If $A$ does not have full column rank, its null space is non-trivial, so there exists a $v \\ne 0$ such that $Av = 0$, which means $v^\\top (2A^\\top A)v = 0$. In this case, the Hessian is positive semidefinite but not positive definite, and the function $f(x)$ is convex but not strictly convex.\n\n**Option-by-Option Analysis**\n\n*   **A. The problem is a convex Quadratic Program (QP) because the objective is quadratic and the box constraints are linear; the objective $f$ is strictly convex if and only if $A$ has full column rank, equivalently $A^\\top A \\succ 0$, and otherwise $f$ is merely convex.**\n    All parts of this statement are consistent with our derivation. The problem is a convex QP. The condition for strict convexity is correctly identified as $A$ having full column rank, which is correctly stated as being equivalent to the Gram matrix $A^\\top A$ being positive definite ($A^\\top A \\succ 0$). It also correctly states that otherwise, the function is convex but not strictly so.\n    **Verdict: Correct.**\n\n*   **B. The problem is a nonconvex Quadratic Program because $A$ may be rectangular; the objective $f$ is strictly convex if $A$ has full row rank.**\n    This is incorrect. The objective function is always convex. The shape of $A$ does not determine convexity. The condition for strict convexity is full *column* rank, not full *row* rank.\n    **Verdict: Incorrect.**\n\n*   **C. The problem is a Linear Program (LP) because the constraints are linear; the objective $f$ is never strictly convex regardless of $A$.**\n    This is incorrect. The objective is quadratic, not linear, so it is not an LP (unless $A=0$, a trivial case). The objective can be strictly convex.\n    **Verdict: Incorrect.**\n\n*   **D. The problem is a convex Quadratically Constrained Quadratic Program (QCQP) because the norm involves a quadratic expression; the objective $f$ is strictly convex for any nonzero $A$.**\n    This is incorrect. The constraints $l \\le x \\le u$ are linear, not quadratic, so it is not a QCQP. The claim that $f$ is strictly convex for any nonzero $A$ is false. If $A$ is a nonzero matrix without full column rank (e.g., $A = \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}$), the objective is not strictly convex.\n    **Verdict: Incorrect.**\n\n*   **E. The problem is a convex Quadratic Program (QP); the objective $f$ is strictly convex if and only if $A$ is square and invertible.**\n    This is incorrect. While the classification as a convex QP is correct, the condition for strict convexity is too narrow. A matrix being square and invertible is a sufficient condition for having full column rank, but it is not necessary. A non-square matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m > n$ can have full column rank ($n$), in which case the objective is strictly convex. Since the statement uses \"if and only if\", it is false.\n    **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "In modern data science, we often want solutions that are not just accurate but also \"simple.\" This exercise presents a powerful comparison between two different notions of simplicity: finding a solution with the smallest magnitude ($\\|x\\|_2$) versus one that is sparse, meaning it has the fewest non-zero elements ($\\|x\\|_0$). By analyzing these two seemingly similar goals, you will discover the profound impact of the objective function on a problem's classification and its computational difficulty. This practice illuminates the fundamental dividing line in optimization between tractable convex problems and computationally hard non-convex ones, a key insight for designing effective models .",
            "id": "3108380",
            "problem": "Consider two optimization problems defined on the Euclidean space $\\mathbb{R}^{n}$, with a given matrix $B \\in \\mathbb{R}^{m \\times n}$ and vector $c \\in \\mathbb{R}^{m}$:\n- $\\mathcal{P}_{2}$: minimize $f(x) = \\|x\\|_{2}$ subject to $Bx = c$.\n- $\\mathcal{P}_{0}$: minimize $g(x) = \\|x\\|_{0}$ subject to $Bx = c$, where $\\|x\\|_{0}$ denotes the number of nonzero components of $x$.\n\nUsing only foundational definitions and facts about convexity, affine sets, norms, and standard conic models, decide which of the following statements are true. Acronyms are spelled out on first appearance: Second-Order Cone Programming (SOCP), Linear Programming (LP), Mixed-Integer Linear Programming (MILP), and Non-deterministic Polynomial time (NP).\n\nChoose all that apply.\n\nA. $\\mathcal{P}_{2}$ is a convex optimization problem, and it can be equivalently reformulated as a Second-Order Cone Programming (SOCP) problem by introducing a scalar $t \\in \\mathbb{R}$ and using the second-order cone constraint $\\|x\\|_{2} \\le t$, then minimizing $t$ subject to $Bx = c$.\n\nB. $\\mathcal{P}_{0}$ is a convex optimization problem because $\\|\\cdot\\|_{0}$ is a norm that satisfies the triangle inequality, and therefore it can be solved as a Linear Programming (LP) problem.\n\nC. Since the feasible set $\\{x \\in \\mathbb{R}^{n} : Bx = c\\}$ is convex, any objective minimized over it yields a convex optimization problem.\n\nD. $\\mathcal{P}_{2}$ can always be solved as a Linear Programming (LP) problem without any nonlinearity because the Euclidean norm is linear on $\\mathbb{R}^{n}$.\n\nE. $\\mathcal{P}_{0}$ can be encoded as a Mixed-Integer Linear Programming (MILP) problem by introducing binary variables to count nonzero entries, and in general it is combinatorial and nonconvex (NP-hard in the worst case).",
            "solution": "The validity of the problem statement must first be assessed.\n\n### Step 1: Extract Givens\n- The optimization problems are defined on the space $\\mathbb{R}^{n}$.\n- A matrix $B \\in \\mathbb{R}^{m \\times n}$ and a vector $c \\in \\mathbb{R}^{m}$ are given.\n- Problem $\\mathcal{P}_{2}$: minimize $f(x) = \\|x\\|_{2}$ subject to $Bx = c$.\n- Problem $\\mathcal{P}_{0}$: minimize $g(x) = \\|x\\|_{0}$ subject to $Bx = c$.\n- Definition of $\\|x\\|_{0}$: The number of nonzero components of the vector $x$.\n- Acronyms: Second-Order Cone Programming (SOCP), Linear Programming (LP), Mixed-Integer Linear Programming (MILP), Non-deterministic Polynomial time (NP).\n- The analysis must use foundational definitions and facts about convexity, affine sets, norms, and standard conic models.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is well-defined and scientifically grounded.\n- **Scientific Grounding**: The problems $\\mathcal{P}_{2}$ and $\\mathcal{P}_{0}$ are canonical examples in optimization theory, representing least-norm and sparse solution problems, respectively. All terms like convexity, norms, LP, SOCP, MILP, and NP-hardness are standard in mathematics and computer science. The definition of the $L_0$ pseudo-norm is standard. The premises are factually sound and central to the field of optimization methods.\n- **Well-Posedness**: The problems are clearly specified. The question asks for an evaluation of several statements about these problems, which is a well-posed task. A unique verdict (true or false) can be determined for each statement based on established theory.\n- **Objectivity**: The problem is stated using precise, formal mathematical language, with no subjective or ambiguous elements.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full analysis of the options can proceed.\n\n### Analysis of the Options\n\n**A. $\\mathcal{P}_{2}$ is a convex optimization problem, and it can be equivalently reformulated as a Second-Order Cone Programming (SOCP) problem by introducing a scalar $t \\in \\mathbb{R}$ and using the second-order cone constraint $\\|x\\|_{2} \\le t$, then minimizing $t$ subject to $Bx = c$.**\n\nAn optimization problem is convex if its objective function is a convex function and its feasible set is a convex set.\n\n1.  **Feasible Set**: The feasible set of $\\mathcal{P}_{2}$ is $S = \\{x \\in \\mathbb{R}^{n} : Bx = c\\}$. This is an affine set. To verify its convexity, let $x_1, x_2 \\in S$ and let $\\alpha \\in [0, 1]$. Then $B x_1 = c$ and $B x_2 = c$. Consider the point $x_{\\alpha} = \\alpha x_1 + (1-\\alpha) x_2$. Applying the matrix $B$, we get $B x_{\\alpha} = B(\\alpha x_1 + (1-\\alpha) x_2) = \\alpha B x_1 + (1-\\alpha) B x_2 = \\alpha c + (1-\\alpha) c = c$. Thus, $x_{\\alpha} \\in S$, and the set $S$ is convex.\n\n2.  **Objective Function**: The objective function is $f(x) = \\|x\\|_{2}$. Any norm, including the Euclidean norm $\\|\\cdot\\|_{2}$, is a convex function. This follows from the triangle inequality and absolute homogeneity. For any $x_1, x_2 \\in \\mathbb{R}^{n}$ and $\\alpha \\in [0, 1]$, we have:\n    $$f(\\alpha x_1 + (1-\\alpha) x_2) = \\|\\alpha x_1 + (1-\\alpha) x_2\\|_{2} \\le \\|\\alpha x_1\\|_{2} + \\|(1-\\alpha) x_2\\|_{2}$$\n    By absolute homogeneity, this becomes:\n    $$|\\alpha| \\|x_1\\|_{2} + |1-\\alpha| \\|x_2\\|_{2} = \\alpha \\|x_1\\|_{2} + (1-\\alpha) \\|x_2\\|_{2} = \\alpha f(x_1) + (1-\\alpha) f(x_2)$$\n    This inequality, $f(\\alpha x_1 + (1-\\alpha) x_2) \\le \\alpha f(x_1) + (1-\\alpha) f(x_2)$, is the definition of a convex function.\n\nSince both the objective function and the feasible set are convex, $\\mathcal{P}_{2}$ is a convex optimization problem.\n\n3.  **SOCP Reformulation**: Consider the proposed problem: minimize $t$ subject to $Bx=c$ and $\\|x\\|_{2} \\le t$. Let's establish its equivalence to $\\mathcal{P}_2$.\n    - If $x^*$ is an optimal solution to $\\mathcal{P}_2$, let $t^* = \\|x^*\\|_{2}$. The pair $(x^*, t^*)$ is feasible for the new problem because $Bx^*=c$ and $\\|x^*\\|_{2} \\le t^*$ holds with equality.\n    - Conversely, if $(x', t')$ is any feasible solution to the new problem, then $Bx'=c$ (so $x'$ is feasible for $\\mathcal{P}_2$) and $\\|x'\\|_{2} \\le t'$. At an optimal solution $(x^{**}, t^{**})$, we must have $t^{**} = \\|x^{**}\\|_{2}$. If $t^{**}  \\|x^{**}\\|_{2}$, one could reduce $t$ to $\\|x^{**}\\|_{2}$ to obtain a strictly better objective value, contradicting optimality. Therefore, minimizing $t$ is equivalent to minimizing $\\|x\\|_{2}$.\n    The reformulated problem is:\n    $$\\text{minimize } t$$\n    $$\\text{subject to } Bx = c, \\quad \\|x\\|_{2} \\le t$$\n    This is a standard form of a Second-Order Cone Programming (SOCP) problem. The objective function is linear (in the augmented variable space $(x, t)$), and the constraints consist of linear equalities and a second-order cone constraint.\n\nThe statement is entirely correct. Verdict: **Correct**.\n\n**B. $\\mathcal{P}_{0}$ is a convex optimization problem because $\\|\\cdot\\|_{0}$ is a norm that satisfies the triangle inequality, and therefore it can be solved as a Linear Programming (LP) problem.**\n\nThis statement makes several claims that we must check.\n\n1.  **Is $\\|\\cdot\\|_{0}$ a norm?** A function $p(x)$ is a norm if it satisfies: (i) $p(x) \\ge 0$ with $p(x)=0 \\iff x=0$; (ii) $p(\\alpha x) = |\\alpha| p(x)$ (absolute homogeneity); (iii) $p(x+y) \\le p(x)+p(y)$ (triangle inequality).\n    - The $L_0$ pseudo-norm fails absolute homogeneity. Let $x = [1, 0, \\dots, 0]^T \\in \\mathbb{R}^n$. Then $\\|x\\|_{0} = 1$. Let $\\alpha = 2$. Then $\\alpha x = [2, 0, \\dots, 0]^T$. We have $\\|\\alpha x\\|_{0} = 1$. However, $|\\alpha|\\|x\\|_{0} = |2| \\cdot 1 = 2$. Since $1 \\ne 2$, $\\|\\cdot\\|_{0}$ is not a norm. The statement's premise is false.\n\n2.  **Is $\\mathcal{P}_{0}$ a convex optimization problem?** The feasible set $\\{x \\in \\mathbb{R}^{n} : Bx = c\\}$ is convex, as shown in the analysis of option A. We must check if the objective function $g(x) = \\|x\\|_{0}$ is convex.\n    - Let $x_1 = [1, 0]^T$ and $x_2 = [0, 1]^T$. Then $\\|x_1\\|_{0} = 1$ and $\\|x_2\\|_{0} = 1$.\n    - Let's check the convexity condition for $\\alpha = 0.5$:\n    $$g(0.5 x_1 + 0.5 x_2) \\le 0.5 g(x_1) + 0.5 g(x_2)$$\n    The left-hand side is $g(0.5 [1, 0]^T + 0.5 [0, 1]^T) = g([0.5, 0.5]^T) = \\|[0.5, 0.5]^T\\|_{0} = 2$.\n    The right-hand side is $0.5(1) + 0.5(1) = 1$.\n    The condition $2 \\le 1$ is false. Therefore, $\\|x\\|_{0}$ is not a convex function, and $\\mathcal{P}_{0}$ is not a convex optimization problem.\n\n3.  **Can it be solved as an LP?** Since $\\mathcal{P}_{0}$ is not a convex problem, it cannot be modeled as an LP, which is a specific type of convex problem.\n\nThe statement is false on multiple grounds. Verdict: **Incorrect**.\n\n**C. Since the feasible set $\\{x \\in \\mathbb{R}^{n} : Bx = c\\}$ is convex, any objective minimized over it yields a convex optimization problem.**\n\nThis statement is a misrepresentation of the definition of a convex optimization problem. A problem is convex only if **both** the feasible set is convex **and** the objective function is convex. A convex feasible set alone is not sufficient. Problem $\\mathcal{P}_{0}$ serves as a direct counterexample: it has a convex feasible set $\\{x \\in \\mathbb{R}^{n} : Bx = c\\}$ but a non-convex objective function $\\|x\\|_{0}$, making it a non-convex problem. Verdict: **Incorrect**.\n\n**D. $\\mathcal{P}_{2}$ can always be solved as a Linear Programming (LP) problem without any nonlinearity because the Euclidean norm is linear on $\\mathbb{R}^{n}$.**\n\n1.  **Is the Euclidean norm linear?** A function $f(x)$ is linear if $f(x+y) = f(x)+f(y)$ and $f(\\alpha x) = \\alpha f(x)$. The Euclidean norm $f(x) = \\|x\\|_{2}$ is not linear.\n    - It violates additivity: Let $x = [1, 0]^T$ and $y = [0, 1]^T$. $\\|x\\|_{2} = 1$ and $\\|y\\|_{2} = 1$. $\\|x+y\\|_{2} = \\|[1, 1]^T\\|_{2} = \\sqrt{1^2+1^2} = \\sqrt{2}$. Since $\\|x+y\\|_{2} = \\sqrt{2} \\ne \\|x\\|_{2} + \\|y\\|_{2} = 2$, it is not additive.\n    - It violates scalar multiplication: Let $x = [1, 0]^T$ and $\\alpha = -1$. $\\|\\alpha x\\|_{2} = \\|[-1, 0]^T\\|_{2} = 1$. However, $\\alpha \\|x\\|_{2} = (-1) \\cdot 1 = -1$. Since $1 \\ne -1$, it is not linear.\n    The premise that the norm is linear is false.\n\n2.  **Can $\\mathcal{P}_{2}$ be solved as an LP?** An LP must have a linear objective function and linear constraints. The objective function of $\\mathcal{P}_2$ is $\\|x\\|_{2}$, which is non-linear. As established in option A, this problem is an SOCP. While LPs are a subclass of SOCPs, $\\mathcal{P}_{2}$ is not an LP because its objective gives rise to a non-polyhedral epigraph (described by $\\|x\\|_2 \\le t$). It cannot be reformulated as an LP without approximation. Verdict: **Incorrect**.\n\n**E. $\\mathcal{P}_{0}$ can be encoded as a Mixed-Integer Linear Programming (MILP) problem by introducing binary variables to count nonzero entries, and in general it is combinatorial and nonconvex (NP-hard in the worst case).**\n\n1.  **MILP formulation**: The problem $\\mathcal{P}_0$ seeks to minimize the number of non-zero elements of $x$. This is equivalent to minimizing $\\sum_{i=1}^{n} z_i$ where $z_i=1$ if $x_i \\ne 0$ and $z_i=0$ if $x_i=0$. This logic can be enforced using binary variables $z_i \\in \\{0, 1\\}$ and \"big-M\" constraints, assuming a bound $-M \\le x_i \\le M$ exists for all components of any feasible $x$. The formulation is:\n    $$\\text{minimize} \\quad \\sum_{i=1}^{n} z_i$$\n    $$\\text{subject to} \\quad Bx = c$$\n    $$-M z_i \\le x_i \\le M z_i, \\quad \\text{for } i=1, \\dots, n$$\n    $$x \\in \\mathbb{R}^{n}, \\quad z \\in \\{0, 1\\}^{n}$$\n    This is a Mixed-Integer Linear Program because it has a linear objective, linear constraints, continuous variables ($x_i$), and integer (binary) variables ($z_i$). Thus, $\\mathcal{P}_0$ can be encoded as an MILP.\n\n2.  **Combinatorial and Nonconvex nature**: As shown in the analysis of B, the objective function $\\|x\\|_0$ is not convex, so $\\mathcal{P}_0$ is a nonconvex problem. The task of finding the smallest set of non-zero entries is equivalent to finding the smallest subset of columns of $B$ that can represent $c$, which is a combinatorial search problem.\n\n3.  **NP-hardness**: The problem of finding the sparsest solution to a system of linear equations is a classic NP-hard problem. It is known to be computationally difficult, and its worst-case complexity is exponential in the problem size unless P=NP.\n\nAll parts of this statement are correct assertions from optimization theory. Verdict: **Correct**.",
            "answer": "$$\\boxed{AE}$$"
        }
    ]
}