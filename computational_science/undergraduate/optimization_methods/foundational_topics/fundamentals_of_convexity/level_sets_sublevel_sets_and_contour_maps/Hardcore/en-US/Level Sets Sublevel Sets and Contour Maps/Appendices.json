{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of gradient descent is deeply connected to the geometry of an objective function's level sets. In this exercise, we explore a classic pathological case where the level sets form a \"flat valley,\" leading to a gradient that offers no guidance in one direction and stalls the optimization process. By analyzing the function $f(x,y)=x^2$, you will see firsthand how this difficult geometry arises and then discover how regularization techniques reshape the contours, creating a more tractable landscape for gradient-based methods .",
            "id": "3141941",
            "problem": "Consider the function $f:\\mathbb{R}^2\\to\\mathbb{R}$ defined by $f(x,y)=x^2$. Using only the definitions of a level set $\\{(x,y):f(x,y)=c\\}$, a sublevel set $\\{(x,y):f(x,y)\\le c\\}$ for a scalar $c\\in\\mathbb{R}$, and the gradient-based update $z_{k+1}=z_k-\\alpha\\nabla f(z_k)$ for a step size $\\alpha0$ in gradient methods, determine which of the following statements are correct. Also consider adding a small quadratic regularization to obtain $g(x,y)=x^2+\\varepsilon y^2$ with $\\varepsilon0$, and an absolute-value (also called $\\ell_1$) regularization to obtain $h(x,y)=x^2+\\lambda |y|$ with $\\lambda0$.\n\nA. For $c0$, the level set $\\{(x,y):f(x,y)=c\\}$ consists of the two vertical lines $x=\\pm\\sqrt{c}$, and for $c=0$ it is the $y$-axis.\n\nB. For $c\\ge 0$, the sublevel set $\\{(x,y):f(x,y)\\le c\\}$ is the vertical strip $\\{(x,y):|x|\\le \\sqrt{c}\\}$, which is unbounded in $y$.\n\nC. For gradient descent applied to $f$ with any fixed step size $\\alpha0$ and any starting point $(x_0,y_0)$ with $y_0\\ne 0$, the $y$-component of the iterates changes and moves toward $0$ over iterations.\n\nD. For $g(x,y)=x^2+\\varepsilon y^2$ with $\\varepsilon0$, each contour $\\{(x,y):g(x,y)=c\\}$ for $c0$ is an ellipse, and the gradient has a nonzero $y$-component whenever $y\\ne 0$, enabling gradient methods with suitable step sizes to contract both $x$ and $y$ toward $0$.\n\nE. Replacing the quadratic term in $y$ by an absolute-value ($\\ell_1$) term, $h(x,y)=x^2+\\lambda |y|$ with $\\lambda0$, produces smooth circular contours and a gradient with nonzero $y$-component for every $y$.",
            "solution": "We analyze each statement based on the provided functions and definitions. The core functions are $f(x,y) = x^2$, $g(x,y) = x^2 + \\varepsilon y^2$, and $h(x,y) = x^2 + \\lambda |y|$.\n\n**Analysis of Statement A:**\nThe level set of $f(x,y)=x^2$ is defined by $x^2 = c$.\n- If $c > 0$, the solutions are $x = \\sqrt{c}$ and $x = -\\sqrt{c}$. Since $y$ is unconstrained, these equations define two vertical lines.\n- If $c = 0$, the unique solution is $x=0$. This defines the $y$-axis.\nThe statement is a correct description of the level sets for $c \\ge 0$.\n\n**Analysis of Statement B:**\nThe sublevel set of $f(x,y)=x^2$ is defined by $x^2 \\le c$ for $c \\ge 0$. This inequality is equivalent to $\\sqrt{x^2} \\le \\sqrt{c}$, which simplifies to $|x| \\le \\sqrt{c}$. This defines a vertical strip between $x=-\\sqrt{c}$ and $x=\\sqrt{c}$, which is unbounded in the $y$-direction. The statement is correct.\n\n**Analysis of Statement C:**\nThe gradient of $f(x,y)=x^2$ is $\\nabla f(x,y) = \\begin{pmatrix} 2x \\\\ 0 \\end{pmatrix}$. The gradient descent update for the $y$-component is $y_{k+1} = y_k - \\alpha \\cdot 0 = y_k$. The $y$-component does not change during optimization. The statement claims it \"changes and moves toward 0,\" which is incorrect.\n\n**Analysis of Statement D:**\nFor the regularized function $g(x,y) = x^2 + \\varepsilon y^2$ with $\\varepsilon > 0$:\n- The contours $x^2 + \\varepsilon y^2 = c$ (for $c>0$) are standard equations for ellipses centered at the origin.\n- The gradient is $\\nabla g(x,y) = \\begin{pmatrix} 2x \\\\ 2\\varepsilon y \\end{pmatrix}$. The $y$-component, $2\\varepsilon y$, is non-zero whenever $y \\neq 0$.\n- The gradient descent updates are $x_{k+1} = (1-2\\alpha)x_k$ and $y_{k+1} = (1-2\\alpha\\varepsilon)y_k$. For a suitable step size $\\alpha$ (e.g., $0  \\alpha  \\min(1, 1/\\varepsilon)$), both updates are contractive, moving the iterates $(x_k, y_k)$ towards $(0,0)$. The statement is correct.\n\n**Analysis of Statement E:**\nFor the function $h(x,y) = x^2 + \\lambda|y|$ with $\\lambda > 0$:\n- The contours are given by $x^2 + \\lambda|y| = c$. For $y \\ge 0$, this is $y = (c - x^2)/\\lambda$ (a downward-opening parabola). For $y  0$, this is $y = (x^2 - c)/\\lambda$ (an upward-opening parabola). The resulting shape is not circular and has a sharp \"crease\" along the x-axis, so it is not smooth.\n- The function $|y|$ is not differentiable at $y=0$, so the gradient of $h$ is not defined for any point where $y=0$.\nThe statement incorrectly claims the contours are \"smooth circular\" and that the gradient has a non-zero $y$-component \"for every $y$\". The statement is incorrect.\n\nSummary: Statements A, B, and D are correct. Statements C and E are incorrect.",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "While many optimization problems involve smooth functions, a vast number of practical applications, especially in modern machine learning, lead to non-differentiable objectives. This practice introduces a common source of non-smoothness: taking the maximum of several functions. You will investigate a function whose level sets have a distinct \"peanut\" shape with sharp \"creases,\" allowing you to explore the concepts of convexity and minimization in a setting where standard gradients do not always exist .",
            "id": "3141952",
            "problem": "Consider the function $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ defined by $f(x,y)=\\max\\{(x-1)^2+y^2,\\,(x+1)^2+y^2\\}$. Use only the foundational definitions of a level set $\\{(x,y)\\in\\mathbb{R}^2:\\,f(x,y)=c\\}$, a sublevel set $\\{(x,y)\\in\\mathbb{R}^2:\\,f(x,y)\\le c\\}$, convexity, and differentiability to analyze the geometry of the contours (level sets), the structure of sublevel sets, the location and nature of minimizers, and any basin boundary phenomena associated with gradient-based optimization. Select all statements that are true.\n\nA. For any $c\\ge 1$, the sublevel set $\\{(x,y):\\,f(x,y)\\le c\\}$ is exactly the intersection of two disks of radius $\\sqrt{c}$ centered at $(1,0)$ and $(-1,0)$, and it is empty for $c1$.\n\nB. The function $f$ is convex and attains a unique global minimum at $(0,0)$ with minimum value $1$.\n\nC. The set of nondifferentiability of $f$ is precisely the vertical line $\\{(0,y):\\,y\\in\\mathbb{R}\\}$.\n\nD. For any $c1$, the level set $\\{(x,y):\\,f(x,y)=c\\}$ consists of two circular arcs (one from the circle centered at $(1,0)$ of radius $\\sqrt{c}$ and one from the circle centered at $(-1,0)$ of radius $\\sqrt{c}$) meeting at the points $(0,\\pm\\sqrt{c-1})$, forming a “peanut”-shaped closed curve; for $c=1$ the level set is the single point $(0,0)$; for $c1$ it is empty.\n\nE. Gradient descent initialized anywhere in $\\mathbb{R}^2$ exhibits two basins of attraction separated by the line $x=0$, converging to distinct local minima near $(\\pm 1,0)$.",
            "solution": "Let's analyze the function $f(x,y)=\\max\\{f_1(x,y), f_2(x,y)\\}$, where $f_1(x,y) = (x-1)^2+y^2$ and $f_2(x,y) = (x+1)^2+y^2$. The condition $f_1(x,y)=f_2(x,y)$ simplifies to $(x-1)^2=(x+1)^2$, which gives $-2x=2x$, or $x=0$.\n- If $x > 0$, then $(x+1)^2 > (x-1)^2$, so $f(x,y) = f_2(x,y) = (x+1)^2+y^2$.\n- If $x  0$, then $(x-1)^2 > (x+1)^2$, so $f(x,y) = f_1(x,y) = (x-1)^2+y^2$.\n- If $x = 0$, then $f(0,y) = (0-1)^2+y^2 = 1+y^2$.\n\n**Analysis of Statement A:**\nThe sublevel set $\\{(x,y) : f(x,y) \\le c\\}$ is defined by the condition $\\max\\{f_1(x,y), f_2(x,y)\\} \\le c$. This is equivalent to the system of inequalities $f_1(x,y) \\le c$ AND $f_2(x,y) \\le c$.\n- $(x-1)^2+y^2 \\le c$ defines a closed disk of radius $\\sqrt{c}$ centered at $(1,0)$.\n- $(x+1)^2+y^2 \\le c$ defines a closed disk of radius $\\sqrt{c}$ centered at $(-1,0)$.\nThe sublevel set is the intersection of these two disks.\nThe global minimum of $f$ occurs at $(0,0)$, where $f(0,0)=1$. Thus, if $c1$, the sublevel set is empty. For $c \\ge 1$, the set is non-empty. The statement is correct.\n\n**Analysis of Statement B:**\nThe functions $f_1(x,y)$ and $f_2(x,y)$ are convex (their Hessians are $2\\mathbf{I}$). The pointwise maximum of convex functions is also convex, so $f$ is convex.\nA property of convex functions is that any local minimum is a global minimum. We found the minimum value to be $f(0,0)=1$. For any other point $(x,y) \\neq (0,0)$, $f(x,y)>1$. Thus, $(0,0)$ is the unique global minimum. The statement is correct.\n\n**Analysis of Statement C:**\nThe function $f = \\max\\{f_1, f_2\\}$ is non-differentiable at points where $f_1=f_2$ and $\\nabla f_1 \\neq \\nabla f_2$. We know $f_1=f_2$ on the line $x=0$. Let's check the gradients there:\n- $\\nabla f_1(x,y) = \\begin{pmatrix} 2(x-1) \\\\ 2y \\end{pmatrix}$, so $\\nabla f_1(0,y) = \\begin{pmatrix} -2 \\\\ 2y \\end{pmatrix}$.\n- $\\nabla f_2(x,y) = \\begin{pmatrix} 2(x+1) \\\\ 2y \\end{pmatrix}$, so $\\nabla f_2(0,y) = \\begin{pmatrix} 2 \\\\ 2y \\end{pmatrix}$.\nSince $\\nabla f_1(0,y) \\neq \\nabla f_2(0,y)$ for any $y$, the function is non-differentiable at every point on the line $x=0$. At all points where $x \\neq 0$, $f$ is equal to either $f_1$ or $f_2$, both of which are smooth polynomials. Thus, the set of non-differentiability is precisely the line $x=0$. The statement is correct.\n\n**Analysis of Statement D:**\nThe level set is $\\{(x,y) : f(x,y) = c\\}$.\n- For $c1$, it's empty, as $\\min f=1$.\n- For $c=1$, it's the single point $\\{(0,0)\\}$, the unique minimum.\n- For $c>1$:\n    - In the region $x \\ge 0$, the level set is part of the circle $(x+1)^2+y^2 = c$.\n    - In the region $x  0$, the level set is part of the circle $(x-1)^2+y^2 = c$.\n    - These two arcs meet at the line $x=0$. Setting $x=0$ in either equation gives $1+y^2=c$, so $y=\\pm\\sqrt{c-1}$. Since $c>1$, these are two real points.\nThe description of a \"peanut\"-shaped curve formed by two circular arcs meeting at $(0, \\pm\\sqrt{c-1})$ is perfectly accurate. The statement is correct.\n\n**Analysis of Statement E:**\nSince $f$ is a convex function with a unique global minimum at $(0,0)$, there are no other local minima. Gradient descent (or subgradient descent on the non-differentiable line) will always converge to this single minimum, regardless of the starting point. Therefore, the entire plane $\\mathbb{R}^2$ is a single basin of attraction for the minimum at $(0,0)$. The statement's claim of distinct local minima and two basins of attraction is incorrect.\n\nSummary: Statements A, B, C, and D are correct. Statement E is incorrect.",
            "answer": "$$\\boxed{ABCD}$$"
        },
        {
            "introduction": "We often encounter objective functions whose level sets are highly elongated ellipses, a property known as ill-conditioning, which drastically slows down gradient descent. Rather than passively accepting this poor geometry, we can actively reshape it through a change of variables, a technique known as preconditioning. This exercise guides you through designing a linear transformation that converts ill-conditioned elliptical contours into perfectly circular ones, providing a concrete understanding of how preconditioning accelerates optimization by improving the problem's underlying geometry .",
            "id": "3141910",
            "problem": "Consider the twice continuously differentiable function $f(\\mathbf{x})=\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{H}\\,\\mathbf{x}$, where $\\mathbf{H}$ is the symmetric positive-definite matrix\n$$\n\\mathbf{H}=\\mathbf{R}(\\theta)^{\\top}\\begin{pmatrix}9  0 \\\\ 0  1\\end{pmatrix}\\mathbf{R}(\\theta),\n$$\nand $\\mathbf{R}(\\theta)$ is the rotation matrix\n$$\n\\mathbf{R}(\\theta)=\\begin{pmatrix}\\cos(\\theta)  -\\sin(\\theta) \\\\ \\sin(\\theta)  \\cos(\\theta)\\end{pmatrix}\n$$\nwith $\\theta=\\frac{\\pi}{6}$. A linear reparameterization $ \\mathbf{y}=\\mathbf{T}\\mathbf{x} $ induces the transformed function $g(\\mathbf{y})=f(\\mathbf{T}^{-1}\\mathbf{y})$. Using only the foundational definitions of level sets, linear transformations, and the Euclidean norm, analyze how the level sets of $g$ depend on $\\mathbf{T}$ and $\\mathbf{H}$. Then, design an invertible linear map $\\mathbf{T}$ that reduces the anisotropy of the level sets of $g$ as much as possible in the Euclidean geometry, in the sense of minimizing the condition number (ratio of largest to smallest eigenvalue) of the Hessian of $g$.\n\nYour final task is to compute the condition number of the Hessian of $g$ under your designed $\\mathbf{T}$ in exact form. Express your final answer as a single real number. No rounding is required.",
            "solution": "The function is a quadratic form $f(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^{\\top}\\mathbf{H}\\mathbf{x}$. Its Hessian is the matrix $\\mathbf{H}$. The eigenvalues of $\\mathbf{H}$ are the same as the diagonal entries of the matrix $\\mathbf{D} = \\begin{pmatrix}9  0 \\\\ 0  1\\end{pmatrix}$, which are $\\lambda_{\\max}(\\mathbf{H}) = 9$ and $\\lambda_{\\min}(\\mathbf{H}) = 1$. The condition number of $\\mathbf{H}$ is $\\kappa(\\mathbf{H}) = \\frac{9}{1} = 9$, which reflects the elongated elliptical shape of the level sets of $f$.\n\nWe are given a linear reparameterization $\\mathbf{y} = \\mathbf{T}\\mathbf{x}$, so $\\mathbf{x} = \\mathbf{T}^{-1}\\mathbf{y}$. The transformed function is:\n$$\ng(\\mathbf{y}) = f(\\mathbf{T}^{-1}\\mathbf{y}) = \\frac{1}{2}(\\mathbf{T}^{-1}\\mathbf{y})^{\\top}\\mathbf{H}(\\mathbf{T}^{-1}\\mathbf{y}) = \\frac{1}{2}\\mathbf{y}^{\\top}\\left((\\mathbf{T}^{-1})^{\\top}\\mathbf{H}\\mathbf{T}^{-1}\\right)\\mathbf{y}.\n$$\nThis is also a quadratic form. The Hessian of $g$, which we'll call $\\mathbf{H}_g$, is the matrix inside the parentheses:\n$$\n\\mathbf{H}_g = (\\mathbf{T}^{-1})^{\\top}\\mathbf{H}\\mathbf{T}^{-1}.\n$$\nOur goal is to design an invertible matrix $\\mathbf{T}$ to minimize the condition number of $\\mathbf{H}_g$. The smallest possible condition number for a positive-definite matrix is 1, which occurs when all its eigenvalues are equal. This means the matrix must be a scalar multiple of the identity matrix, $\\mathbf{H}_g = \\alpha \\mathbf{I}$ for some $\\alpha > 0$. We can aim for the simplest case, $\\alpha=1$, so our design goal is to find a $\\mathbf{T}$ such that $\\mathbf{H}_g = \\mathbf{I}$.\n$$\n(\\mathbf{T}^{-1})^{\\top}\\mathbf{H}\\mathbf{T}^{-1} = \\mathbf{I}\n$$\nRearranging this equation gives:\n$$\n\\mathbf{H} = \\mathbf{T}^{\\top}\\mathbf{T}\n$$\nThis is the Cholesky-like decomposition of $\\mathbf{H}$. Since $\\mathbf{H}$ is symmetric and positive-definite, a unique symmetric positive-definite matrix \"square root\" $\\mathbf{H}^{1/2}$ exists such that $\\mathbf{H} = \\mathbf{H}^{1/2}\\mathbf{H}^{1/2}$. Let's choose our transformation matrix $\\mathbf{T}$ to be this principal square root:\n$$\n\\mathbf{T} = \\mathbf{H}^{1/2}.\n$$\nSince $\\mathbf{H}^{1/2}$ is symmetric, $\\mathbf{T}^\\top = \\mathbf{T}$, so the condition $\\mathbf{H} = \\mathbf{T}^{\\top}\\mathbf{T}$ is satisfied.\n\nNow, we compute the Hessian $\\mathbf{H}_g$ using this choice of $\\mathbf{T}$:\n$$\n\\mathbf{H}_g = (\\mathbf{T}^{-1})^{\\top}\\mathbf{H}\\mathbf{T}^{-1} = ((\\mathbf{H}^{1/2})^{-1})^{\\top}\\mathbf{H}(\\mathbf{H}^{1/2})^{-1}.\n$$\nSince $\\mathbf{H}^{1/2}$ is symmetric, its inverse is also symmetric, so $((\\mathbf{H}^{1/2})^{-1})^{\\top} = (\\mathbf{H}^{1/2})^{-1}$. The expression becomes:\n$$\n\\mathbf{H}_g = (\\mathbf{H}^{1/2})^{-1}\\mathbf{H}(\\mathbf{H}^{1/2})^{-1} = (\\mathbf{H}^{1/2})^{-1}(\\mathbf{H}^{1/2}\\mathbf{H}^{1/2})(\\mathbf{H}^{1/2})^{-1}.\n$$\nUsing the associativity of matrix multiplication, we group the terms:\n$$\n\\mathbf{H}_g = [(\\mathbf{H}^{1/2})^{-1}\\mathbf{H}^{1/2}][\\mathbf{H}^{1/2}(\\mathbf{H}^{1/2})^{-1}] = \\mathbf{I} \\cdot \\mathbf{I} = \\mathbf{I}.\n$$\nThe Hessian of the transformed function $g$ is the identity matrix $\\mathbf{I}$. The eigenvalues of the identity matrix are all 1.\nThe condition number of $\\mathbf{H}_g$ is therefore:\n$$\n\\kappa(\\mathbf{H}_g) = \\frac{\\lambda_{\\max}(\\mathbf{H}_g)}{\\lambda_{\\min}(\\mathbf{H}_g)} = \\frac{1}{1} = 1.\n$$\nThis is the minimum possible value, achieving the goal of making the level sets of $g$ perfectly circular. The final answer is the value of this condition number.",
            "answer": "$$\n\\boxed{1}\n$$"
        }
    ]
}