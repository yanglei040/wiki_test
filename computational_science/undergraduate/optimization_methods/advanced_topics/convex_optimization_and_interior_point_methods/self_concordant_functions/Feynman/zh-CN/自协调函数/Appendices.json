{
    "hands_on_practices": [
        {
            "introduction": "我们的第一个实践是基础性的。我们将为一个带有对数障碍的线性优化问题计算牛顿方向。这个练习至关重要，因为它具体展示了如何通过计算梯度和Hessian矩阵来推导搜索方向，这是内点法中的一个基本步骤。",
            "id": "3176672",
            "problem": "考虑线性优化中的增广障碍目标函数，其定义为 $g(x) = c^{\\top} x + \\mu f(x)$，其中 $x \\in \\mathbb{R}^{n}_{++}$ 是分量严格为正的向量，$c \\in \\mathbb{R}^{n}$ 是给定的成本向量，$\\mu  0$ 是一个障碍参数，$f(x) = -\\sum_{i=1}^{n} \\ln(x_{i})$ 是针对非负约束的对数障碍函数。从梯度和 Hessian 矩阵的基本定义以及针对二阶连续可微函数的牛顿法出发，推导在当前迭代点 $x$ 处最小化 $g(x)$ 的牛顿方向 $d$。然后，将得到的关于 $d$ 的线性系统完全用松弛变量 $s_{i} := x_{i}$（$i = 1,\\dots,n$）来重写，并以闭式解的形式求解该系统，从而得到一个包含 $s$ 和 $c$ 的 $d$ 的显式表达式。\n\n最后，将你的推导应用于 $n=3$，$c = (2,-1,3)^{\\top}$，$\\mu = 2$ 以及当前迭代点 $x = (1,2,4)^{\\top}$ 的数值实例，并计算精确的牛顿方向向量 $d$。无需四舍五入。请将你的最终数值答案以单行矩阵的形式给出。",
            "solution": "该问题要求推导在线性优化中最小化一个增广障碍目标函数的牛顿方向，并随后将其应用于一个具体的数值实例。\n\n待最小化的目标函数由下式给出\n$$g(x) = c^{\\top} x + \\mu f(x)$$\n其中 $x \\in \\mathbb{R}^{n}_{++}$，$c \\in \\mathbb{R}^{n}$，$\\mu  0$，且对数障碍函数为 $f(x) = -\\sum_{i=1}^{n} \\ln(x_{i})$。定义域 $\\mathbb{R}^{n}_{++}$ 包含 $\\mathbb{R}^{n}$ 中所有分量严格为正的向量，即对于所有 $i=1, \\dots, n$，都有 $x_{i}  0$。\n\n对于最小化一个二阶连续可微函数 $g(x)$，牛顿法通过求解以下线性系统来计算在当前迭代点 $x$ 处的搜索方向 $d$：\n$$\\nabla^2 g(x) d = -\\nabla g(x)$$\n其中 $\\nabla g(x)$ 是 $g(x)$ 的梯度，$\\nabla^2 g(x)$ 是其 Hessian 矩阵。\n\n首先，我们推导 $g(x)$ 的梯度。该函数可以明确地用 $x$ 的分量写成：\n$$g(x) = \\sum_{i=1}^{n} c_i x_i - \\mu \\sum_{i=1}^{n} \\ln(x_i)$$\n关于第 $j$ 个分量 $x_j$ 的偏导数是：\n$$\\frac{\\partial g}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\sum_{i=1}^{n} c_i x_i - \\mu \\sum_{i=1}^{n} \\ln(x_i) \\right) = c_j - \\frac{\\mu}{x_j}$$\n梯度是这些偏导数构成的向量：\n$$\\nabla g(x) = \\begin{pmatrix} c_1 - \\mu/x_1 \\\\ c_2 - \\mu/x_2 \\\\ \\vdots \\\\ c_n - \\mu/x_n \\end{pmatrix} = c - \\mu \\begin{pmatrix} 1/x_1 \\\\ 1/x_2 \\\\ \\vdots \\\\ 1/x_n \\end{pmatrix}$$\n\n接下来，我们推导 $g(x)$ 的 Hessian 矩阵。Hessian 矩阵是二阶偏导数矩阵，其元素为 $(\\nabla^2 g(x))_{jk} = \\frac{\\partial^2 g}{\\partial x_j \\partial x_k}$。\n对于 $j \\neq k$：\n$$\\frac{\\partial^2 g}{\\partial x_j \\partial x_k} = \\frac{\\partial}{\\partial x_j} \\left( \\frac{\\partial g}{\\partial x_k} \\right) = \\frac{\\partial}{\\partial x_j} \\left( c_k - \\frac{\\mu}{x_k} \\right) = 0$$\n对于 $j = k$：\n$$\\frac{\\partial^2 g}{\\partial x_j^2} = \\frac{\\partial}{\\partial x_j} \\left( \\frac{\\partial g}{\\partial x_j} \\right) = \\frac{\\partial}{\\partial x_j} \\left( c_j - \\frac{\\mu}{x_j} \\right) = - \\mu \\left( -\\frac{1}{x_j^2} \\right) = \\frac{\\mu}{x_j^2}$$\n因此，Hessian 矩阵是一个对角矩阵：\n$$\\nabla^2 g(x) = \\begin{pmatrix} \\mu/x_1^2  0  \\cdots  0 \\\\ 0  \\mu/x_2^2  \\cdots  0 \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ 0  0  \\cdots  \\mu/x_n^2 \\end{pmatrix} = \\mu \\cdot \\text{diag}\\left(\\frac{1}{x_1^2}, \\dots, \\frac{1}{x_n^2}\\right)$$\n我们可以将其紧凑地表示为 $\\nabla^2 g(x) = \\mu X^{-2}$，其中 $X = \\text{diag}(x_1, \\dots, x_n)$。\n\n现在，我们构建牛顿系统 $\\nabla^2 g(x) d = -\\nabla g(x)$：\n$$\\mu \\cdot \\text{diag}\\left(\\frac{1}{x_1^2}, \\dots, \\frac{1}{x_n^2}\\right) d = - \\left( c - \\mu \\begin{pmatrix} 1/x_1 \\\\ \\vdots \\\\ 1/x_n \\end{pmatrix} \\right) = \\mu \\begin{pmatrix} 1/x_1 \\\\ \\vdots \\\\ 1/x_n \\end{pmatrix} - c$$\n问题要求用松弛变量 $s_i := x_i$ 来重写这个系统。这是一个直接替换。令 $s$ 为分量为 $s_i$ 的向量。\n$$\\mu \\cdot \\text{diag}\\left(\\frac{1}{s_1^2}, \\dots, \\frac{1}{s_n^2}\\right) d = \\mu \\begin{pmatrix} 1/s_1 \\\\ \\vdots \\\\ 1/s_n \\end{pmatrix} - c$$\n\n为了求解牛顿方向 $d$，我们可以逐分量求解该系统。对于每个 $i \\in \\{1, \\dots, n\\}$，第 $i$ 个方程是：\n$$\\frac{\\mu}{s_i^2} d_i = \\frac{\\mu}{s_i} - c_i$$\n求解 $d_i$：\n$$d_i = \\frac{s_i^2}{\\mu} \\left( \\frac{\\mu}{s_i} - c_i \\right) = s_i - \\frac{s_i^2}{\\mu} c_i$$\n这就是牛顿方向 $d$ 的每个分量关于 $s_i$、$c_i$ 和 $\\mu$ 的闭式表达式。以向量形式，令 $S = \\text{diag}(s_1, \\dots, s_n)$，则表达式为 $d = s - \\frac{1}{\\mu} S^2 c$。\n\n最后，我们将此结果应用于给定的数值实例：\n$n=3$，$c = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}$，$\\mu = 2$，以及当前迭代点为 $x = \\begin{pmatrix} 1 \\\\ 2 \\\\ 4 \\end{pmatrix}$。\n根据问题的符号表示，我们有 $s = x$，因此 $s_1 = 1$，$s_2 = 2$，$s_3 = 4$。\n\n我们计算牛顿方向 $d = \\begin{pmatrix} d_1 \\\\ d_2 \\\\ d_3 \\end{pmatrix}$ 的分量：\n对于 $i=1$：\n$$d_1 = s_1 - \\frac{s_1^2}{\\mu} c_1 = 1 - \\frac{1^2}{2} \\cdot 2 = 1 - \\frac{1}{2} \\cdot 2 = 1 - 1 = 0$$\n对于 $i=2$：\n$$d_2 = s_2 - \\frac{s_2^2}{\\mu} c_2 = 2 - \\frac{2^2}{2} \\cdot (-1) = 2 - \\frac{4}{2} \\cdot (-1) = 2 - 2 \\cdot (-1) = 2 + 2 = 4$$\n对于 $i=3$：\n$$d_3 = s_3 - \\frac{s_3^2}{\\mu} c_3 = 4 - \\frac{4^2}{2} \\cdot 3 = 4 - \\frac{16}{2} \\cdot 3 = 4 - 8 \\cdot 3 = 4 - 24 = -20$$\n\n最终得到的牛顿方向向量是 $d = \\begin{pmatrix} 0 \\\\ 4 \\\\ -20 \\end{pmatrix}$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  4  -20\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "了解了一个典型的自协和函数后，我们现在来探讨如何检验其他函数是否具备此性质。我们将分析在稳健统计中广泛使用的Huber损失函数，以确定它是否是自协和的，以及在何种程度上是。这个实践通过将抽象定义与一个具体的分段函数联系起来，帮助我们建立直觉，并揭示一个“完美”自协和区域的形态。",
            "id": "3176704",
            "problem": "设参数为 $\\delta0$ 的Huber损失对于 $t\\in\\mathbb{R}$ 定义为\n$$\nh(t)=\\begin{cases}\n\\frac{1}{2}t^2,  \\text{if } |t| \\le \\delta, \\\\\n\\delta\\left(|t|-\\frac{\\delta}{2}\\right),  \\text{if } |t|  \\delta.\n\\end{cases}\n$$\n在凸优化中的一维自协和框架下进行。从 $h(t)$ 的定义和微分法则出发，完成以下几项：\n1) 在 $\\mathbb{R}$ 的每个这些导数存在的光滑区域上，计算 $h''(t)$ 和 $h'''(t)$。\n2) 考虑集合 $S=\\{t\\in\\mathbb{R}:\\, h''(t)0\\}$。使用一维自协和的比较关系 $|h'''(t)|$ 与 $\\big(h''(t)\\big)^{3/2}$，确定最小的常数 $\\nu\\ge 0$，使得\n$$\n|h'''(t)|\\le \\nu\\big(h''(t)\\big)^{3/2}\\quad\\text{for all }t\\in S.\n$$\n给出 $\\nu$ 的精确值作为最终答案。无需四舍五入。你可以在推导过程中包含任何关于自协和的定性评论，但你的最终答案必须是一个实数。",
            "solution": "问题要求我们分析Huber损失函数 $h(t)$ 的一维自协和性，该函数对于参数 $\\delta  0$ 和 $t \\in \\mathbb{R}$ 定义为\n$$\nh(t)=\\begin{cases}\n\\frac{1}{2}t^2,  \\text{if } |t| \\le \\delta \\\\\n\\delta\\left(|t|-\\frac{\\delta}{2}\\right),  \\text{if } |t|  \\delta\n\\end{cases}\n$$\n为了方便求导，我们可以将 $h(t)$ 在三个不同区间上显式地写为：\n$$\nh(t) = \\begin{cases}\n-\\delta t - \\frac{\\delta^2}{2},  \\text{for } t  -\\delta \\\\\n\\frac{1}{2} t^2,  \\text{for } -\\delta \\le t \\le \\delta \\\\\n\\delta t - \\frac{\\delta^2}{2},  \\text{for } t  \\delta\n\\end{cases}\n$$\n\n第一步是计算 $h(t)$ 在函数光滑区域上的一阶、二阶和三阶导数。\n\n一阶导数 $h'(t)$ 通过对每一分段求导得到：\n$$\nh'(t) = \\begin{cases}\n-\\delta,  \\text{for } t  -\\delta \\\\\nt,  \\text{for } -\\delta  t  \\delta \\\\\n\\delta,  \\text{for } t  \\delta\n\\end{cases}\n$$\n我们必须检查在点 $t = -\\delta$ 和 $t = \\delta$ 处的连续性。\n在 $t = \\delta$ 处：\n左极限为 $\\lim_{t \\to \\delta^-} h'(t) = \\lim_{t \\to \\delta^-} t = \\delta$。\n右极限为 $\\lim_{t \\to \\delta^+} h'(t) = \\lim_{t \\to \\delta^+} \\delta = \\delta$。\n由于极限相等，$h'(t)$ 在 $t = \\delta$ 处是连续的。\n\n在 $t = -\\delta$ 处：\n左极限为 $\\lim_{t \\to -\\delta^-} h'(t) = \\lim_{t \\to -\\delta^-} (-\\delta) = -\\delta$。\n右极限为 $\\lim_{t \\to -\\delta^+} h'(t) = \\lim_{t \\to -\\delta^+} t = -\\delta$。\n由于极限相等，$h'(t)$ 在 $t = -\\delta$ 处是连续的。\n因此，$h(t)$ 是 $\\mathbb{R}$ 上的一个连续可微函数（$C^1$ 函数）。\n\n二阶导数 $h''(t)$ 是 $h'(t)$ 的导数，定义在开区间 $(-\\infty, -\\delta)$、$(-\\delta, \\delta)$ 和 $(\\delta, \\infty)$ 上：\n$$\nh''(t) = \\begin{cases}\n\\frac{d}{dt}(-\\delta) = 0,  \\text{for } t \\in (-\\infty, -\\delta) \\\\\n\\frac{d}{dt}(t) = 1,  \\text{for } t \\in (-\\delta, \\delta) \\\\\n\\frac{d}{dt}(\\delta) = 0,  \\text{for } t \\in (\\delta, \\infty)\n\\end{cases}\n$$\n所以，$h''(t)$ 是一个分段常数函数。注意，由于存在跳跃间断点，$h''(t)$ 在 $t = \\pm\\delta$ 处未定义。\n\n三阶导数 $h'''(t)$ 是 $h''(t)$ 的导数，定义在相同的开区间上：\n$$\nh'''(t) = \\begin{cases}\n\\frac{d}{dt}(0) = 0,  \\text{for } t \\in (-\\infty, -\\delta) \\\\\n\\frac{d}{dt}(1) = 0,  \\text{for } t \\in (-\\delta, \\delta) \\\\\n\\frac{d}{dt}(0) = 0,  \\text{for } t \\in (\\delta, \\infty)\n\\end{cases}\n$$\n因此，对于所有 $t \\in \\mathbb{R} \\setminus \\{-\\delta, \\delta\\}$，$h'''(t) = 0$。这就完成了问题的第一部分。\n\n接下来，我们考虑集合 $S = \\{t \\in \\mathbb{R} : h''(t)  0\\}$。从我们得到的 $h''(t)$ 的表达式中，我们看到 $h''(t) = 1  0$ 当且仅当 $t \\in (-\\delta, \\delta)$。因此，集合 $S$ 是开区间 $(-\\delta, \\delta)$。\n\n问题的最后一部分是确定最小的常数 $\\nu \\ge 0$，使得不等式\n$$\n|h'''(t)| \\le \\nu \\big(h''(t)\\big)^{3/2}\n$$\n对于所有 $t \\in S$ 都成立。满足条件的最小 $\\nu$ 由以下比率在集合 $S$ 上的上确界给出：\n$$\n\\nu = \\sup_{t \\in S} \\frac{|h'''(t)|}{\\left(h''(t)\\right)^{3/2}}\n$$\n我们对 $t \\in S = (-\\delta, \\delta)$ 计算该表达式。对于该区间内的任何 $t$，我们有：\n$h''(t) = 1$\n$h'''(t) = 0$\n\n将这些值代入比率中，得到：\n$$\n\\frac{|h'''(t)|}{\\left(h''(t)\\right)^{3/2}} = \\frac{|0|}{(1)^{3/2}} = \\frac{0}{1} = 0\n$$\n对于每一个 $t \\in S$，这个比率都恰好为 $0$。一个只包含值 $0$ 的集合的上确界是 $0$。\n因此，最小常数 $\\nu$ 是 $0$。\n\n这个结果与自协和函数的理论是一致的。对于一个函数，若其在某个区间上的自协和参数 $\\nu$ 为 $0$，则它在该区间上的三阶导数必须恒等于零。这是二次函数的特征。Huber损失函数 $h(t)$ 在指定的定义域 $S = (-\\delta, \\delta)$ 上确实是一个二次函数 $h(t) = \\frac{1}{2}t^2$。",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "自协和理论为何如此重要？这个实践通过数值实验给出了一个令人信服的答案。通过比较牛顿法在自协和函数与非自协和函数上的行为，你将亲身验证该理论的关键承诺：算法性能的可预测性和保证性，这是构建稳健优化算法的基石。",
            "id": "3176724",
            "problem": "考虑以下牛顿法应用于两个凸函数的对比，其中一个函数是自协调的，而另一个不是。设 $x \\in \\mathbb{R}^n$，定义牛顿步长方向为 $p(x) = -\\nabla^2 f(x)^{-1}\\nabla f(x)$，牛顿减量为 $\\lambda(x) = \\sqrt{\\nabla f(x)^\\top \\nabla^2 f(x)^{-1}\\nabla f(x)}$。实际的单步下降量为 $\\Delta f = f(x) - f(x^+)$。在自协调分析中，当使用合适的步长时，存在一个仅依赖于 $\\lambda(x)$ 的典范预测函数，该函数为单步下降量提供了一个下界。你的任务是推导出合适的预测函数，然后实现一个程序，通过实验比较在以下两个函数上实际的单步下降量与该预测值。\n\n推导的基本依据：\n- 牛顿法由方向 $p(x)$、梯度 $\\nabla f(x)$ 和 Hessian 矩阵 $\\nabla^2 f(x)$ 定义。\n- 牛顿减量 $\\lambda(x)$ 是一个如上定义的标量，在二阶方法中用于衡量与最优点的接近程度。\n- 对于自协调函数，阻尼牛顿步的分析提供了一个关于函数值下降量的下界，该下界可以表示为一个仅与 $\\lambda(x)$ 有关的函数，且与具体的函数实例无关。\n\n用于比较的函数：\n1. 正象限上的自协调障碍函数：\n   $$f_{\\text{bar}}(x) = -\\sum_{i=1}^n \\log x_i \\quad \\text{定义域为 } x_i  0.$$\n2. 非自协调的光滑凸函数：\n   $$f_{\\text{sqrt}}(x) = \\sum_{i=1}^n \\sqrt{1 + x_i^2}.$$\n\n使用的步长策略：\n- 完整牛顿步：$x^+ = x + p(x)$。\n- 由 $\\lambda(x)$ 控制的阻尼牛顿步：$x^+ = x + \\alpha(x)\\,p(x)$，其中 $\\alpha(x)$ 仅依赖于 $\\lambda(x)$。\n\n任务：\n1. 针对自协调函数，在相应的阻尼牛顿步下，推导为单步下降量提供下界的、关于 $\\lambda(x)$ 的典范预测函数。\n2. 实现一个程序，对下面的每个测试用例，计算：\n   - 牛顿方向 $p(x)$，\n   - 牛顿减量 $\\lambda(x)$，\n   - 由所选策略决定的步长 $\\alpha(x)$，\n   - 新的点 $x^+$，\n   - 实际下降量 $\\Delta f = f(x) - f(x^+)$，\n   - 完全由推导出的预测函数表示的预测下界，\n   - 实际下降量 $\\Delta f$ 与预测下界之比 $r$。\n3. 使用比率 $r$ 来衡量可预测性。若 $r \\ge 1$，则表示实际下降量达到或超过预测值；若 $r  1$，则表示不满足预测。\n\n测试套件：\n- 用例 A：$f_{\\text{bar}}$，$n=2$，$x = [0.8,\\,1.2]$，阻尼步。\n- 用例 B：$f_{\\text{bar}}$，$n=3$，$x = [0.2,\\,0.5,\\,2.0]$，阻尼步。\n- 用例 C：$f_{\\text{sqrt}}$，$n=2$，$x = [2.0,\\,-1.0]$，阻尼步。\n- 用例 D：$f_{\\text{sqrt}}$，$n=2$，$x = [3.0,\\,3.0]$，完整步。\n\n输出规格：\n- 你的程序应产生单行输出，包含四个用例的比率 $r$，按 A、B、C、D 的顺序排列，格式为方括号内由逗号分隔的列表（例如 $[r_A,r_B,r_C,r_D]$）。\n- 每个 $r$ 都必须以浮点数形式输出。无物理单位。\n\n科学真实性：\n- 确保所有中间计算在数学上是一致的，并遵守函数定义域（对于 $f_{\\text{bar}}$，在整个计算过程中保持 $x_i  0$）。",
            "solution": "该问题要求针对牛顿法在自协调函数上的单步下降量，推导其典范预测函数，并随后与一个非自协调函数进行实验比较。\n\n### 第 1 部分：预测函数的推导\n\n对自协调函数的牛顿法分析为函数值的下降量提供了一个有保证的下界，该下界仅依赖于牛顿减量 $\\lambda(x)$，而不依赖于具体的函数或点 $x$。\n\n如果一个函数 $f$ 是凸的、$C^3$ 连续的，并且其三阶导数受其二阶导数的约束，那么它就被定义为自协调函数。形式上，对于定义域中的所有 $x$ 和所有方向 $h \\in \\mathbb{R}^n$，不等式 $|D^3 f(x)[h,h,h]| \\le 2 (D^2 f(x)[h,h])^{3/2}$ 成立。该性质可以导出函数行为相对于其局部二次近似的强界。\n\n对于一个自协调函数 $f$，一个利用点 $x$ 处的信息来约束邻近点 $y$ 处函数值的关键不等式是：\n$$f(y) \\le f(x) + \\nabla f(x)^\\top (y-x) - \\left( \\|y-x\\|_x + \\log(1 - \\|y-x\\|_x) \\right)$$\n其中 $\\|u\\|_x = \\sqrt{u^\\top \\nabla^2 f(x) u}$ 是在点 $x$ 处的局部范数。该不等式对所有满足 $\\|y-x\\|_x  1$ 的 $y$ 成立。\n\n我们分析 $f$ 沿牛顿方向 $p(x) = -\\nabla^2 f(x)^{-1}\\nabla f(x)$ 的变化。设新点为 $x^+ = x + \\alpha p(x)$，其中 $\\alpha \\in [0, 1)$ 是步长。从 $x$ 到 $x^+$ 的向量是 $\\alpha p(x)$。它在局部范数下的长度是：\n$$ \\|\\alpha p(x)\\|_x = \\alpha \\sqrt{p(x)^\\top \\nabla^2 f(x) p(x)} $$\n代入 $p(x)$ 的定义，我们得到：\n$$ p(x)^\\top \\nabla^2 f(x) p(x) = (-\\nabla^2 f(x)^{-1}\\nabla f(x))^\\top \\nabla^2 f(x) (-\\nabla^2 f(x)^{-1}\\nabla f(x)) = \\nabla f(x)^\\top \\nabla^2 f(x)^{-1} \\nabla f(x) = \\lambda(x)^2 $$\n因此，$\\|\\alpha p(x)\\|_x = \\alpha \\lambda(x)$。如果 $\\alpha \\lambda(x)  1$，则自协调不等式适用。\n\n将 $y-x = \\alpha p(x)$ 代入不等式可得：\n$$ f(x+\\alpha p(x)) \\le f(x) + \\alpha \\nabla f(x)^\\top p(x) - \\left( \\alpha\\lambda(x) + \\log(1 - \\alpha\\lambda(x)) \\right) $$\n我们还知道 $\\nabla f(x)^\\top p(x) = -\\lambda(x)^2$。代入此式可得到 $f(x^+)$ 的一个上界：\n$$ f(x+\\alpha p(x)) \\le f(x) - \\alpha\\lambda(x)^2 - \\alpha\\lambda(x) - \\log(1 - \\alpha\\lambda(x)) $$\n对于任何满足 $\\alpha  1/\\lambda(x)$ 的步长 $\\alpha$，此界均成立。阻尼牛顿法的目标是选择 $\\alpha$ 以实现最大的保证下降量，这对应于最小化 $f(x^+)$ 的这个上界。令 $g(\\alpha)$ 为依赖于 $\\alpha$ 的项：\n$$ g(\\alpha) = -\\alpha\\lambda(x)^2 - \\alpha\\lambda(x) - \\log(1 - \\alpha\\lambda(x)) $$\n为了找到最优的 $\\alpha$，我们将 $g(\\alpha)$ 对 $\\alpha$ 求导，并令其导数为零：\n$$ g'(\\alpha) = -\\lambda(x)^2 - \\lambda(x) - \\frac{-\\lambda(x)}{1 - \\alpha\\lambda(x)} = -\\lambda(x)^2 - \\lambda(x) + \\frac{\\lambda(x)}{1 - \\alpha\\lambda(x)} = 0 $$\n假设 $\\lambda(x)  0$，我们可以两边同除以 $\\lambda(x)$：\n$$ -\\lambda(x) - 1 + \\frac{1}{1 - \\alpha\\lambda(x)} = 0 \\implies \\lambda(x) + 1 = \\frac{1}{1 - \\alpha\\lambda(x)} $$\n$$ 1 - \\alpha\\lambda(x) = \\frac{1}{1+\\lambda(x)} \\implies \\alpha\\lambda(x) = 1 - \\frac{1}{1+\\lambda(x)} = \\frac{\\lambda(x)}{1+\\lambda(x)} $$\n这就得出了阻尼牛顿法的最优步长：\n$$ \\alpha(x) = \\frac{1}{1+\\lambda(x)} $$\n该步长始终满足条件 $\\alpha(x) \\lambda(x)  1$。\n\n现在，我们将此最优步长代回到关于 $f(x^+)$ 的不等式中，以求得保证的下降量：\n$$ f(x^+) \\le f(x) - \\left(\\frac{1}{1+\\lambda(x)}\\right)\\lambda(x)^2 - \\left(\\frac{1}{1+\\lambda(x)}\\right)\\lambda(x) - \\log\\left(1 - \\frac{\\lambda(x)}{1+\\lambda(x)}\\right) $$\n$$ f(x^+) \\le f(x) - \\frac{\\lambda(x)^2+\\lambda(x)}{1+\\lambda(x)} - \\log\\left(\\frac{1}{1+\\lambda(x)}\\right) $$\n$$ f(x^+) \\le f(x) - \\lambda(x) + \\log(1+\\lambda(x)) $$\n重新整理此式可得到单步下降量 $\\Delta f = f(x) - f(x^+)$ 的下界：\n$$ \\Delta f \\ge \\lambda(x) - \\log(1+\\lambda(x)) $$\n推导至此完成。下降量下界的典范预测函数是 $\\psi(\\lambda) = \\lambda - \\log(1+\\lambda)$，它在使用阻尼步长 $\\alpha(\\lambda) = 1/(1+\\lambda)$ 时有效。\n\n### 第 2 部分：测试函数分析\n\n我们将此分析应用于给定的两个函数。\n\n1.  **自协调障碍函数：$f_{\\text{bar}}(x) = -\\sum_{i=1}^n \\log x_i$**\n    - 梯度：$\\nabla f_{\\text{bar}}(x)_i = -1/x_i$。\n    - Hessian 矩阵：$\\nabla^2 f_{\\text{bar}}(x)$ 是一个对角矩阵，其对角元素为 $(\\nabla^2 f_{\\text{bar}}(x))_{ii} = 1/x_i^2$。\n    - Hessian 矩阵的逆：$(\\nabla^2 f_{\\text{bar}}(x)^{-1})_{ii} = x_i^2$。\n    - 牛顿步：$p(x) = -(\\nabla^2 f_{\\text{bar}})^{-1} \\nabla f_{\\text{bar}} = -\\text{diag}(x_i^2) \\cdot [-1/x_i] = [x_i]$，因此 $p(x)=x$。\n    - 牛顿减量：$\\lambda(x)^2 = \\nabla f_{\\text{bar}}^\\top (\\nabla^2 f_{\\text{bar}})^{-1} \\nabla f_{\\text{bar}} = \\sum_{i=1}^n (-1/x_i)^2 (x_i^2) = \\sum_{i=1}^n 1 = n$。\n    - 因此，对于此函数，$\\lambda(x) = \\sqrt{n}$ 是一个常数。对于用例 A ($n=2$) 和 B ($n=3$) ，$\\lambda$ 将分别为 $\\sqrt{2}$ 和 $\\sqrt{3}$。理论预测比率 $r = \\Delta f / (\\lambda - \\log(1+\\lambda))$ 将大于或等于 1。\n\n2.  **非自协调函数：$f_{\\text{sqrt}}(x) = \\sum_{i=1}^n \\sqrt{1 + x_i^2}$**\n    - 梯度：$\\nabla f_{\\text{sqrt}}(x)_i = x_i / \\sqrt{1 + x_i^2}$。\n    - Hessian 矩阵：$\\nabla^2 f_{\\text{sqrt}}(x)$ 是一个对角矩阵，其对角元素为 $(\\nabla^2 f_{\\text{sqrt}}(x))_{ii} = (1 + x_i^2)^{-3/2}$。\n    - Hessian 矩阵的逆：$(\\nabla^2 f_{\\text{sqrt}}(x)^{-1})_{ii} = (1 + x_i^2)^{3/2}$。\n    - 牛顿步：$p(x)_i = -(1+x_i^2)^{3/2} \\cdot (x_i/\\sqrt{1+x_i^2}) = -x_i(1+x_i^2)$。\n    - 牛顿减量：$\\lambda(x)^2 = \\sum_{i=1}^n (x_i/\\sqrt{1+x_i^2})^2 (1+x_i^2)^{3/2} = \\sum_{i=1}^n x_i^2\\sqrt{1+x_i^2}$。\n    - 由于此函数非自协调，推导出的预测不保证成立。我们预计会发现比率 $r  1$ 的情况。用例 C 使用阻尼步，其结果可能满足也可能不满足该界。用例 D 使用完整牛顿步 ($\\alpha=1$)，已知对于远离最优点的非自协调函数，这种步长可能是潜在不稳定的。\n\n### 第 3 部分：计算流程\n\n对于每个测试用例，按顺序计算以下量：\n1.  从测试套件中获取向量 $x$、所选函数（$f_{\\text{bar}}$ 或 $f_{\\text{sqrt}}$）以及步长策略。\n2.  根据解析公式计算梯度 $\\nabla f(x)$ 和 Hessian 矩阵 $\\nabla^2 f(x)$。\n3.  通过求解线性系统 $\\nabla^2 f(x) p(x) = -\\nabla f(x)$ 来计算牛顿步。对于给定的函数，由于 Hessian 矩阵是对角阵，计算被简化为 $p(x) = -(\\nabla^2 f(x))^{-1} \\nabla f(x)$。\n4.  牛顿减量计算为 $\\lambda(x) = \\sqrt{-\\nabla f(x)^\\top p(x)}$。\n5.  确定步长 $\\alpha(x)$：对于“阻尼”步，$\\alpha(x) = 1/(1+\\lambda(x))$；对于“完整”步，$\\alpha(x)=1$。\n6.  计算下一次迭代的点：$x^+ = x + \\alpha(x) p(x)$。\n7.  计算函数值的实际下降量：$\\Delta f = f(x) - f(x^+)$。\n8.  使用推导出的公式计算预测下界：$\\Delta f_{\\text{pred}} = \\lambda(x) - \\log(1+\\lambda(x))$。\n9.  计算比率 $r = \\Delta f / \\Delta f_{\\text{pred}}$ 以评估预测的有效性。\n\n程序实现将对所有四个指定的测试用例执行这些步骤。",
            "answer": "```python\nimport numpy as np\n\ndef f_bar(x):\n    \"\"\"\n    Computes value, gradient, and Hessian for f(x) = -sum(log(x_i)).\n    \"\"\"\n    if np.any(x = 0):\n        # The point must be in the domain of the function.\n        raise ValueError(\"Input x must be in the positive orthant.\")\n    val = -np.sum(np.log(x))\n    grad = -1.0 / x\n    hess = np.diag(1.0 / (x**2))\n    return val, grad, hess\n\ndef f_bar_val(x):\n    \"\"\"\n    Computes only the value of f_bar.\n    \"\"\"\n    if np.any(x = 0):\n        return np.inf\n    return -np.sum(np.log(x))\n\ndef f_sqrt(x):\n    \"\"\"\n    Computes value, gradient, and Hessian for f(x) = sum(sqrt(1 + x_i^2)).\n    \"\"\"\n    term = np.sqrt(1 + x**2)\n    val = np.sum(term)\n    grad = x / term\n    hess = np.diag(1.0 / (term**3))\n    return val, grad, hess\n\ndef f_sqrt_val(x):\n    \"\"\"\n    Computes only the value of f_sqrt.\n    \"\"\"\n    term = np.sqrt(1 + x**2)\n    return np.sum(term)\n\ndef solve():\n    \"\"\"\n    Main function to run the Newton's method comparison for the given test cases.\n    \"\"\"\n    test_cases = [\n        {'func_handle': f_bar, 'val_handle': f_bar_val, 'x': np.array([0.8, 1.2]), 'policy': 'damped'},\n        {'func_handle': f_bar, 'val_handle': f_bar_val, 'x': np.array([0.2, 0.5, 2.0]), 'policy': 'damped'},\n        {'func_handle': f_sqrt, 'val_handle': f_sqrt_val, 'x': np.array([2.0, -1.0]), 'policy': 'damped'},\n        {'func_handle': f_sqrt, 'val_handle': f_sqrt_val, 'x': np.array([3.0, 3.0]), 'policy': 'full'}\n    ]\n\n    ratios = []\n\n    for case in test_cases:\n        func = case['func_handle']\n        val_func = case['val_handle']\n        x = case['x']\n        policy = case['policy']\n\n        # 1. Get function value, gradient, and Hessian at x.\n        f_x, grad_x, hess_x = func(x)\n        \n        # 2. Compute Newton step p and decrement lambda.\n        # p = -H^{-1}g. `solve` is more stable than `inv` but for diagonal H, direct inversion is fine.\n        p_x = -np.linalg.solve(hess_x, grad_x)\n        \n        # lambda^2 = -g^T p\n        lambda_sq = -grad_x.T @ p_x\n        # Prevent numerical issues with small negative numbers due to floating point arithmetic.\n        lambda_x = np.sqrt(max(0, lambda_sq))\n\n        # 3. Determine step size alpha.\n        if policy == 'damped':\n            alpha = 1.0 / (1.0 + lambda_x)\n        else:  # 'full'\n            alpha = 1.0\n\n        # 4. Compute new point x_plus.\n        x_plus = x + alpha * p_x\n        \n        # 5. Compute actual decrease delta_f.\n        f_x_plus = val_func(x_plus)\n        delta_f_actual = f_x - f_x_plus\n\n        # 6. Compute predicted lower bound on decrease.\n        # This prediction is derived for self-concordant functions with a damped step.\n        if lambda_x  1e-9: # Avoid log(1) issues, though lambda isn't zero for these cases.\n            delta_f_pred = lambda_x - np.log(1.0 + lambda_x)\n        else:\n            # For lambda - 0, lambda - log(1+lambda) approx lambda - (lambda - lambda^2/2) = lambda^2/2\n            delta_f_pred = 0.5 * lambda_x**2\n\n        # 7. Compute the ratio r.\n        # A ratio r = 1 means the prediction holds.\n        if abs(delta_f_pred)  1e-12:\n            # If predicted decrease is zero, the ratio is conceptually infinite if there's any actual decrease.\n            # This case does not occur in the problem set.\n            ratio = np.inf if delta_f_actual  1e-12 else 1.0\n        else:\n            ratio = delta_f_actual / delta_f_pred\n        \n        ratios.append(ratio)\n\n    # Format and print the final output as specified.\n    print(f\"[{','.join(map(str, ratios))}]\")\n\nsolve()\n```"
        }
    ]
}