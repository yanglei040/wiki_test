{
    "hands_on_practices": [
        {
            "introduction": "分布鲁棒优化不仅是一个抽象的理论框架，更是一种在不确定性量化中推导更紧密、更实用边界的强大工具。本练习旨在通过一个机会约束问题，将一个通用的概率不等式（切比雪夫不等式）与一个特定的分布鲁棒方法进行对比 。通过这个过程，你将亲身体会到，在给定矩信息的情况下，构造一个分布鲁棒模型能够得到比通用方法更为精确和不那么保守的结论，从而理解其在风险管理中的实际价值。",
            "id": "3121629",
            "problem": "考虑一个标量不确定量 $X$（例如，一个线性成本），其分布未知，但前两阶矩已知：均值 $\\mu$ 和方差 $\\sigma^{2}$，其中 $\\sigma \\ge 0$。对于给定的风险水平 $\\epsilon \\in (0,1)$ 和阈值 $\\tau \\in \\mathbb{R}$，机会约束要求 $\\mathbb{P}(X \\le \\tau) \\ge 1 - \\epsilon$。在分布鲁棒的设定下，我们寻求一个 $\\tau$ 值，以保证对于所有均值为 $\\mu$、方差为 $\\sigma^{2}$ 的 $\\mathbb{R}$ 上的概率分布，该机会约束都成立。\n\n仅使用均值、方差和尾部概率的核心定义，以及概率论中经过充分检验的不等式，完成以下任务：\n\n(i) 使用 Chebyshev 不等式，推导一个关于 $\\tau$ 的充分条件（用 $\\mu$、$\\sigma$ 和 $\\epsilon$ 表示），该条件保证对于所有均值为 $\\mu$、方差为 $\\sigma^{2}$ 的分布，都有 $\\mathbb{P}(X \\le \\tau) \\ge 1 - \\epsilon$。将得到的界表示为 $\\tau_{\\mathrm{Cheb}}(\\mu,\\sigma,\\epsilon)$。\n\n(ii) 使用仅依赖于 $\\mu$ 和 $\\sigma^{2}$ 的最紧单边界（即，在所有与给定矩一致的分布上，基于矩的分布鲁棒机会约束的解），推导保证 $\\mathbb{P}(X \\le \\tau) \\ge 1 - \\epsilon$ 对所有此类分布都成立的最小 $\\tau$。将此表示为 $\\tau_{\\mathrm{DRO}}(\\mu,\\sigma,\\epsilon)$。\n\n(iii) 定义改进因子\n$$\nR(\\epsilon) \\equiv \\frac{\\tau_{\\mathrm{Cheb}}(\\mu,\\sigma,\\epsilon) - \\mu}{\\tau_{\\mathrm{DRO}}(\\mu,\\sigma,\\epsilon) - \\mu}.\n$$\n以最简闭式形式计算 $R(\\epsilon)$。你的最终答案必须仅为 $R(\\epsilon)$ 的简化解析表达式。不要提供中间值。无需四舍五入。",
            "solution": "给定一个标量随机变量 $X$，其分布未知但均值 $\\mu$ 和方差 $\\sigma^{2}$ 已知。机会约束 $\\mathbb{P}(X \\le \\tau) \\ge 1 - \\epsilon$可以等价地写成右尾概率的上界 $\\mathbb{P}(X - \\mu \\ge \\tau - \\mu) \\le \\epsilon$。我们寻求一个 $\\tau$ 值，该值对于所有具有给定前两阶矩的分布都一致成立。\n\n步骤 (i)：由 Chebyshev 不等式导出的充分条件。\n\nChebyshev 不等式（一个经过充分检验的概率不等式）指出，对于任何具有有限均值和方差的随机变量，\n$$\n\\mathbb{P}\\left(|X - \\mu| \\ge t\\right) \\le \\frac{\\sigma^{2}}{t^{2}} \\quad \\text{for all } t > 0.\n$$\n由于单边事件 $\\{X - \\mu \\ge t\\}$ 包含于双边事件 $\\{|X - \\mu| \\ge t\\}$ 中，我们有\n$$\n\\mathbb{P}(X - \\mu \\ge t) \\le \\mathbb{P}\\left(|X - \\mu| \\ge t\\right) \\le \\frac{\\sigma^{2}}{t^{2}} \\quad \\text{for all } t > 0.\n$$\n为确保 $\\mathbb{P}(X - \\mu \\ge t) \\le \\epsilon$，只需满足\n$$\n\\frac{\\sigma^{2}}{t^{2}} \\le \\epsilon \\quad \\Longleftrightarrow \\quad t \\ge \\frac{\\sigma}{\\sqrt{\\epsilon}}.\n$$\n令 $t = \\tau - \\mu$，机会约束得以满足的一个充分条件是\n$$\n\\tau - \\mu \\ge \\frac{\\sigma}{\\sqrt{\\epsilon}} \\quad \\Longleftrightarrow \\quad \\tau_{\\mathrm{Cheb}}(\\mu,\\sigma,\\epsilon) = \\mu + \\frac{\\sigma}{\\sqrt{\\epsilon}}.\n$$\n\n步骤 (ii)：最紧的基于矩的单边界（分布鲁棒机会约束）。\n\n仅依赖于 $\\mu$ 和 $\\sigma^{2}$ 的最紧单边尾部概率界由已知的极值矩（单边）不等式给出，该不等式通常被称为 Cantelli 不等式（也称为单边 Chebyshev 不等式）。对于任何 $t > 0$，\n$$\n\\mathbb{P}(X - \\mu \\ge t) \\le \\frac{\\sigma^{2}}{\\sigma^{2} + t^{2}},\n$$\n并且这个界在均值为 $\\mu$、方差为 $\\sigma^{2}$ 的分布族上是紧的。因此，要保证 $\\mathbb{P}(X - \\mu \\ge t) \\le \\epsilon$ 对所有此类分布一致成立，其充分必要条件是\n$$\n\\frac{\\sigma^{2}}{\\sigma^{2} + t^{2}} \\le \\epsilon \\quad \\Longleftrightarrow \\quad \\sigma^{2} \\le \\epsilon(\\sigma^{2} + t^{2})\n\\quad \\Longleftrightarrow \\quad (1 - \\epsilon)\\sigma^{2} \\le \\epsilon t^{2}.\n$$\n解出 $t$ 可得\n$$\nt \\ge \\sigma \\sqrt{\\frac{1 - \\epsilon}{\\epsilon}}.\n$$\n再次令 $t = \\tau - \\mu$，确保分布鲁棒机会约束的最小阈值为\n$$\n\\tau_{\\mathrm{DRO}}(\\mu,\\sigma,\\epsilon) = \\mu + \\sigma \\sqrt{\\frac{1 - \\epsilon}{\\epsilon}}.\n$$\n\n步骤 (iii)：计算改进因子 $R(\\epsilon)$。\n\n根据定义，\n$$\nR(\\epsilon) = \\frac{\\tau_{\\mathrm{Cheb}}(\\mu,\\sigma,\\epsilon) - \\mu}{\\tau_{\\mathrm{DRO}}(\\mu,\\sigma,\\epsilon) - \\mu}\n= \\frac{\\dfrac{\\sigma}{\\sqrt{\\epsilon}}}{\\sigma \\sqrt{\\dfrac{1 - \\epsilon}{\\epsilon}}}.\n$$\n消去公因子 $\\sigma$ 并化简，\n$$\nR(\\epsilon) = \\frac{1/\\sqrt{\\epsilon}}{\\sqrt{(1 - \\epsilon)/\\epsilon}}\n= \\frac{1}{\\sqrt{1 - \\epsilon}}.\n$$\n对于 $\\epsilon \\in (0,1)$，此表达式已是最简闭式形式。",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{1-\\epsilon}}}$$"
        },
        {
            "introduction": "在优化问题中，经典的样本平均近似（SAA）方法有时会因为对训练数据的过度拟合而表现不佳，尤其是在数据含有异常值时。本练习将让你直面这一挑战，通过一个具体的编码任务来比较 SAA 与基于Wasserstein距离的分布鲁棒优化（DRO）的性能 。你将亲手构建一个DRO模型，并验证它如何通过对经验分布进行鲁棒化来有效抑制过拟合，从而获得更好的样本外泛化能力。",
            "id": "3121634",
            "problem": "您将获得一个玩具实例，以研究样本平均近似（SAA）中的过拟合现象以及通过Wasserstein分布鲁棒优化（DRO）获得的泛化改进。考虑一个标量决策变量 $x \\in \\mathbb{R}$ 和一个标量不确定参数 $\\xi \\in \\mathbb{R}$。损失定义为 $f(x,\\xi) = \\tfrac{1}{2} x^2 - x \\xi$，对于所有 $\\xi$，该函数在 $x$ 上是凸的。$\\xi$ 的真实数据生成分布是一个混合分布：以 $0.9$ 的概率从正态分布 $\\mathcal{N}(0,1)$ 中抽取 $\\xi$，以 $0.1$ 的概率将 $\\xi$ 设置为常数 $10$。训练数据集被构造成包含 $20$ 个样本：$16$ 个样本大致来自 $\\mathcal{N}(0,1)$ 分量，以及 $4$ 个位于 $10$ 的离群值。为保证可复现性，请使用以下固定列表\n$$\n\\Xi_{\\text{train}} = [-1.5, 1.5, -1.0, 1.0, -0.7, 0.7, -0.3, 0.3, -2.0, 2.0, -0.2, 0.2, -1.2, 1.2, -0.4, 0.4, 10, 10, 10, 10].\n$$\n采用以底度量 $d(\\xi,\\xi') = |\\xi - \\xi'|$ 定义的 $1$-Wasserstein 距离，将模糊集 $\\mathcal{W}_{\\epsilon}$ 定义为以从训练数据构建的经验分布 $\\hat{P}_N$ 为中心、半径为 $\\epsilon \\ge 0$ 的球。从期望风险和模糊集的基本定义出发，且不假设任何预先推导的鲁棒公式，推导出 SAA 估计量 $x_{\\text{SAA}}$ 和 Wasserstein-DRO 估计量 $x_{\\text{DRO}}(\\epsilon)$，它们分别最小化经验期望损失和在 $\\mathcal{W}_{\\epsilon}$ 上的最坏情况期望损失。然后，在真实分布下评估它们的样本外风险。在此情景下，决策 $x$ 的样本外风险是在上述混合分布下计算的真实期望 $\\mathbb{E}[f(x,\\xi)]$。\n\n您的程序必须：\n- 完全按照所提供的方式构造 $\\Xi_{\\text{train}}$。\n- 计算 $\\Xi_{\\text{train}}$ 上的经验均值 $\\bar{\\xi}$。\n- 通过最小化 $\\tfrac{1}{2} x^2 - x \\bar{\\xi}$ 来计算 SAA 决策 $x_{\\text{SAA}}$。\n- 使用 1-Wasserstein 球 $\\mathcal{W}_{\\epsilon}$ 和底度量 $|\\cdot|$，基于原则性推理为每个给定的 $\\epsilon$ 推导出 Wasserstein-DRO 决策 $x_{\\text{DRO}}(\\epsilon)$，并实现它。\n- 在真实混合分布下，为每个决策 $x_{\\text{DRO}}(\\epsilon)$ 评估样本外风险 $\\mathbb{E}\\left[\\tfrac{1}{2} x^2 - x \\xi\\right]$。\n\n使用以下半径值的测试套件：\n$$\n[\\;0.0,\\;0.2,\\;1.0,\\;2.0,\\;2.5\\;].\n$$\n这些值涵盖了一个常规的“顺利路径”情况（$0.2$）、基线 SAA 情况（$0.0$）、中度鲁棒化（$1.0$）、$\\epsilon$ 等于 $|\\bar{\\xi}|$ 的边界阈值情况（$2.0$）以及极端鲁棒化（$2.5$）。对于列表中的每个 $\\epsilon$，输出在真实混合分布下评估的 $x_{\\text{DRO}}(\\epsilon)$ 对应的样本外风险。真实分布由混合权重和分量参数完全指定；您必须计算混合分布的精确均值并一致地使用它。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如 $[\\text{result}_1,\\text{result}_2,\\dots]$。所有答案必须是实数（浮点数），不带任何物理单位。较小的值表示更好的泛化性能。",
            "solution": "该问题要求针对一个特定的二次损失函数，推导并比较样本平均近似（SAA）估计量和 Wasserstein 分布鲁棒优化（DRO）估计量。这些估计量的性能将根据它们的样本外风险进行评估，该风险是相对于一个已知的真实数据生成分布计算的。\n\n首先，我们定义问题的核心组成部分。决策变量是一个标量 $x \\in \\mathbb{R}$，不确定参数是一个标量 $\\xi \\in \\mathbb{R}$。损失函数由 $f(x, \\xi) = \\frac{1}{2} x^2 - x \\xi$ 给出。训练数据包含 $N=20$ 个样本，以固定列表 $\\Xi_{\\text{train}}$ 的形式提供。经验分布 $\\hat{P}_N$ 是这些 $N$ 个样本上的离散均匀分布。\n\nSAA 估计量，记为 $x_{\\text{SAA}}$，是经验风险最小化问题的解：\n$$\nx_{\\text{SAA}} = \\arg\\min_{x \\in \\mathbb{R}} \\mathbb{E}_{\\hat{P}_N}[f(x, \\xi)]\n$$\n经验风险计算为训练样本上的平均损失：\n$$\n\\mathbb{E}_{\\hat{P}_N}[f(x, \\xi)] = \\frac{1}{N} \\sum_{i=1}^{N} f(x, \\xi_i) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\frac{1}{2} x^2 - x \\xi_i\\right) = \\frac{1}{2} x^2 - x \\left(\\frac{1}{N} \\sum_{i=1}^{N} \\xi_i\\right)\n$$\n令 $\\bar{\\xi} = \\frac{1}{N} \\sum_{i=1}^{N} \\xi_i$ 为训练样本的经验均值。SAA 目标函数简化为 $\\frac{1}{2} x^2 - x \\bar{\\xi}$。这是一个关于 $x$ 的严格凸二次函数。为了找到最小值点，我们将关于 $x$ 的一阶导数设为零：\n$$\n\\frac{d}{dx} \\left(\\frac{1}{2} x^2 - x \\bar{\\xi}\\right) = x - \\bar{\\xi} = 0\n$$\n这得到 SAA 估计量：\n$$\nx_{\\text{SAA}} = \\bar{\\xi}\n$$\n\n接下来，我们处理 Wasserstein-DRO 估计量 $x_{\\text{DRO}}(\\epsilon)$。它是以下极小极大问题的解：\n$$\nx_{\\text{DRO}}(\\epsilon) = \\arg\\min_{x \\in \\mathbb{R}} \\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[f(x, \\xi)]\n$$\n这里，$\\mathcal{W}_{\\epsilon}$ 是围绕经验分布 $\\hat{P}_N$、半径为 $\\epsilon \\geq 0$ 的 1-Wasserstein 球，定义为 $\\mathcal{W}_{\\epsilon} = \\{P \\mid W_1(P, \\hat{P}_N) \\le \\epsilon \\}$，其中底度量为绝对差 $d(\\xi, \\xi') = |\\xi - \\xi'|$。\n\n为了解决这个极小极大问题，我们首先分析对于固定 $x$ 的内部最大化问题：\n$$\n\\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[f(x, \\xi)] = \\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}\\left[\\frac{1}{2} x^2 - x \\xi\\right] = \\frac{1}{2} x^2 + \\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[-x \\xi]\n$$\nKantorovich-Rubinstein 对偶定理指出，1-Wasserstein 距离可以表示为：\n$$\nW_1(P, \\hat{P}_N) = \\sup_{\\phi: \\|\\phi\\|_{\\text{Lip}} \\le 1} \\left( \\mathbb{E}_P[\\phi(\\xi)] - \\mathbb{E}_{\\hat{P}_N}[\\phi(\\xi)] \\right)\n$$\n其中上确界是对所有 1-Lipschitz 函数 $\\phi$ 取的。由此，对于任意函数 $g(\\xi)$，我们有不等式 $\\mathbb{E}_P[g(\\xi)] - \\mathbb{E}_{\\hat{P}_N}[g(\\xi)] \\le W_1(P, \\hat{P}_N) \\cdot L(g)$，其中 $L(g)$ 是 $g$ 的 Lipschitz 常数。对于任意 $P \\in \\mathcal{W}_{\\epsilon}$，这意味着 $\\mathbb{E}_P[g(\\xi)] \\le \\mathbb{E}_{\\hat{P}_N}[g(\\xi)] + \\epsilon L(g)$。已知这个上界是紧的。\n在我们的例子中，我们关心的函数是 $g(\\xi) = -x\\xi$。其 Lipschitz 常数为 $L(g) = \\sup_{\\xi_1\\neq\\xi_2}\\frac{|-x\\xi_1 - (-x\\xi_2)|}{|\\xi_1-\\xi_2|} = |-x| = |x|$。\n因此，最坏情况期望为：\n$$\n\\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[-x \\xi] = \\mathbb{E}_{\\hat{P}_N}[-x \\xi] + \\epsilon |x| = -x \\bar{\\xi} + \\epsilon |x|\n$$\n将此代回 DRO 目标函数，我们需要求解：\n$$\nx_{\\text{DRO}}(\\epsilon) = \\arg\\min_{x \\in \\mathbb{R}} \\left( \\frac{1}{2} x^2 - x \\bar{\\xi} + \\epsilon |x| \\right)\n$$\n目标函数 $J(x) = \\frac{1}{2} x^2 - x \\bar{\\xi} + \\epsilon |x|$ 是凸的，但在 $x=0$ 处不可微。我们可以使用次梯度微积分。最小值点 $x^*$ 必须满足 $0 \\in \\partial J(x^*)$。$|x|$ 的次梯度在 $x \\neq 0$ 时为 $\\text{sgn}(x)$，在 $x=0$ 时为 $[-1, 1]$。\n$J(x)$ 的次梯度为 $\\partial J(x) = x - \\bar{\\xi} + \\epsilon \\cdot \\partial |x|$。\n如果 $x > 0$，我们需要 $x - \\bar{\\xi} + \\epsilon = 0 \\implies x = \\bar{\\xi} - \\epsilon$。这仅在 $\\bar{\\xi} - \\epsilon > 0$，即 $\\bar{\\xi} > \\epsilon$ 时有效。\n如果 $x  0$，我们需要 $x - \\bar{\\xi} - \\epsilon = 0 \\implies x = \\bar{\\xi} + \\epsilon$。这仅在 $\\bar{\\xi} + \\epsilon  0$，即 $\\bar{\\xi}  -\\epsilon$ 时有效。\n如果 $x=0$，我们需要 $0 \\in 0 - \\bar{\\xi} + \\epsilon[-1, 1] = [-\\bar{\\xi}-\\epsilon, -\\bar{\\xi}+\\epsilon]$。这在 $-\\bar{\\xi}-\\epsilon \\le 0 \\le -\\bar{\\xi}+\\epsilon$ 时为真，可简化为 $|\\bar{\\xi}| \\le \\epsilon$。\n\n综合这些条件，解是对 $\\bar{\\xi}$ 应用软阈值算子，阈值为 $\\epsilon$：\n$$\nx_{\\text{DRO}}(\\epsilon) = \\text{sign}(\\bar{\\xi}) \\max(0, |\\bar{\\xi}| - \\epsilon)\n$$\n注意，当 $\\epsilon=0$ 时，我们有 $x_{\\text{DRO}}(0) = \\text{sign}(\\bar{\\xi})\\max(0, |\\bar{\\xi}|) = \\bar{\\xi}$，这正确地恢复了 SAA 估计量 $x_{\\text{SAA}}$。\n\n最后，我们计算给定决策 $x$ 的样本外风险 $R(x)$。这是损失函数在真实数据生成分布 $P_{\\text{true}}$ 下的期望：\n$$\nR(x) = \\mathbb{E}_{P_{\\text{true}}}[f(x, \\xi)] = \\mathbb{E}_{P_{\\text{true}}}\\left[\\frac{1}{2} x^2 - x \\xi\\right] = \\frac{1}{2} x^2 - x \\mathbb{E}_{P_{\\text{true}}}[\\xi]\n$$\n$\\xi$ 的真实分布是一个混合分布：以 $0.9$ 的概率，$\\xi \\sim \\mathcal{N}(0,1)$，以 $0.1$ 的概率，$\\xi=10$。真实均值 $\\mu_{\\text{true}} = \\mathbb{E}_{P_{\\text{true}}}[\\xi]$ 为：\n$$\n\\mu_{\\text{true}} = 0.9 \\cdot \\mathbb{E}[\\mathcal{N}(0,1)] + 0.1 \\cdot 10 = 0.9 \\cdot 0 + 0.1 \\cdot 10 = 1.0\n$$\n因此，样本外风险为 $R(x) = \\frac{1}{2} x^2 - x$。\n\n我们现在进行数值计算。\n训练数据为 $\\Xi_{\\text{train}} = [-1.5, 1.5, -1.0, 1.0, -0.7, 0.7, -0.3, 0.3, -2.0, 2.0, -0.2, 0.2, -1.2, 1.2, -0.4, 0.4, 10, 10, 10, 10]$。\n前16个对称样本的和为 $0$。四个离群值的和为 $4 \\times 10 = 40$。样本总数为 $N=20$。\n经验均值为 $\\bar{\\xi} = \\frac{0 + 40}{20} = 2.0$。\n\n我们为每个给定的 $\\epsilon$ 评估 $x_{\\text{DRO}}(\\epsilon) = \\text{soft}(2.0, \\epsilon)$ 及其风险 $R(x_{\\text{DRO}}(\\epsilon))$：\n1.  $\\epsilon=0.0$: $x_{\\text{DRO}}(0.0) = \\text{soft}(2.0, 0.0) = 2.0$。风险 $R(2.0) = \\frac{1}{2}(2.0)^2 - 2.0 = 2.0 - 2.0 = 0.0$。\n2.  $\\epsilon=0.2$: $x_{\\text{DRO}}(0.2) = \\text{soft}(2.0, 0.2) = 1.8$。风险 $R(1.8) = \\frac{1}{2}(1.8)^2 - 1.8 = 1.62 - 1.8 = -0.18$。\n3.  $\\epsilon=1.0$: $x_{\\text{DRO}}(1.0) = \\text{soft}(2.0, 1.0) = 1.0$。风险 $R(1.0) = \\frac{1}{2}(1.0)^2 - 1.0 = 0.5 - 1.0 = -0.5$。\n4.  $\\epsilon=2.0$: $x_{\\text{DRO}}(2.0) = \\text{soft}(2.0, 2.0) = 0.0$。风险 $R(0.0) = \\frac{1}{2}(0.0)^2 - 0.0 = 0.0$。\n5.  $\\epsilon=2.5$: $x_{\\text{DRO}}(2.5) = \\text{soft}(2.0, 2.5) = 0.0$。风险 $R(0.0) = \\frac{1}{2}(0.0)^2 - 0.0 = 0.0$。\n\nSAA 解 $x_{\\text{SAA}}=2.0$ 对训练数据中的离群值产生了过拟合，因为它远大于真实最优决策 $x^* = \\arg\\min_x R(x) = \\arg\\min_x(\\frac{1}{2}x^2-x) = 1.0$。$\\epsilon=1.0$ 时的 DRO 解完美地修正了经验均值，从而获得了最优的样本外性能。较小的 $\\epsilon$ 值提供了不充分的修正，而较大的 $\\epsilon$ 值则过度正则化，使解收缩得太多，从而降低了性能。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes SAA and Wasserstein-DRO solutions and their out-of-sample risks.\n    \"\"\"\n    \n    # Define the training dataset as specified in the problem statement.\n    Xi_train = np.array([\n        -1.5, 1.5, -1.0, 1.0, -0.7, 0.7, -0.3, 0.3, -2.0, 2.0, \n        -0.2, 0.2, -1.2, 1.2, -0.4, 0.4, 10.0, 10.0, 10.0, 10.0\n    ])\n\n    # Define the test suite of Wasserstein radii.\n    epsilon_values = [0.0, 0.2, 1.0, 2.0, 2.5]\n\n    # Compute the empirical mean of the training data.\n    # This corresponds to the SAA decision x_SAA.\n    xi_bar = np.mean(Xi_train)\n\n    # The true mean of the data-generating distribution for xi.\n    # P(xi) = 0.9 * N(0, 1) + 0.1 * delta_10\n    # E[xi] = 0.9 * E[N(0,1)] + 0.1 * E[delta_10] = 0.9 * 0 + 0.1 * 10 = 1.0\n    mu_true = 1.0\n\n    def soft_threshold(y, T):\n        \"\"\"\n        The soft-thresholding operator, which gives the Wasserstein-DRO solution.\n        soft(y, T) = sign(y) * max(0, |y| - T)\n        \"\"\"\n        if T  0:\n            raise ValueError(\"Threshold T must be non-negative.\")\n        return np.sign(y) * np.maximum(0, np.abs(y) - T)\n\n    def out_of_sample_risk(x):\n        \"\"\"\n        Computes the out-of-sample risk R(x) = E_true[0.5*x^2 - x*xi].\n        R(x) = 0.5*x^2 - x * E_true[xi]\n        \"\"\"\n        return 0.5 * x**2 - x * mu_true\n\n    results = []\n    for epsilon in epsilon_values:\n        # Compute the Wasserstein-DRO decision x_DRO(epsilon).\n        # For epsilon = 0, this is the SAA decision x_SAA.\n        x_dro = soft_threshold(xi_bar, epsilon)\n        \n        # Evaluate the out-of-sample risk for this decision.\n        risk = out_of_sample_risk(x_dro)\n        results.append(risk)\n\n    # Format the output as a comma-separated list in brackets.\n    # The problem specifies that smaller values indicate better generalization.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "对于初学者而言，“最坏情况分布”可能是一个难以捉摸的概念。本练习旨在揭开其神秘面纱，让你直观地理解Wasserstein球内的最坏情况分布是如何运作的 。你将通过计算并可视化数据点如何在损失函数下被“对抗性”地移动，从而对Wasserstein模糊集的工作机制形成一个具体而深刻的认识。这不仅展示了DRO的效果，更揭示了其内在的运作原理。",
            "id": "3121643",
            "problem": "要求您实现一个小型、自包含的程序，用于计算一个分布鲁棒优化问题的最坏情况样本位置。该问题定义在二维数据上，并受 Wasserstein 球约束。考虑以下设置。假设存在一个有限点集 $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^2$，其经验分布在每个点 $x_i$ 上的质量为 $1/n$。设损失为一个线性泛函 $f_w(x) = w^\\top x$，其中 $w \\in \\mathbb{R}^2$ 是给定的参数向量。考虑一个由无穷阶 Wasserstein 球定义的分布不确定性集合，其底层度量为欧几里得范数。该集合包含所有满足 $W_\\infty(Q, P_n) \\le \\epsilon$ 的分布 $Q$，其中 $P_n$ 是 $\\{x_i\\}$ 上的经验分布。在此设置中，最坏情况期望损失定义为在半径为 $\\epsilon$ 的 Wasserstein 球内所有分布上的期望损失的上确界，并受 Wasserstein 距离约束。仅使用基本定义和凸分析原理，推导最坏情况下的扰动样本位置 $\\{x_i^\\star\\}$ 的优化器。这些位置使得当每个原始样本 $x_i$ 被允许在以 $x_i$ 为中心、半径为 $\\epsilon$ 的欧几里得球内移动时，平均损失最大化。您的程序必须为指定的测试套件计算这些最坏情况下的样本位置，并生成确切所需的输出格式。\n\n您必须使用的基本依据和定义：\n- 在 $\\mathbb{R}^2$ 上，对于欧几里得成本，两个分布之间的无穷阶 Wasserstein 距离 $W_\\infty$ 是通过耦合和欧几里得距离的本性上确界来定义的；这意味着每个质量点最多可以移动 $\\epsilon$ 的距离。\n- 欧几里得范数定义为 $\\|v\\|_2 = \\sqrt{v_1^2 + v_2^2}$，对于 $v \\in \\mathbb{R}^2$。\n- 对于线性泛函 $f_w(x) = w^\\top x$，当梯度非零时，其在一个闭合欧几里得球上的最大化器在梯度的方向上于边界处取得。\n- 柯西-施瓦茨不等式：对于任意 $a, b \\in \\mathbb{R}^2$，有 $a^\\top b \\le \\|a\\|_2 \\|b\\|_2$，当且仅当 $a$ 和 $b$ 正向共线时等号成立。\n\n您的任务：\n1. 基于基本依据，推导在给定 $w$ 和约束 $\\|x - x_i\\|_2 \\le \\epsilon$ 下，$f_w(x)$ 的逐样本最大化器 $x_i^\\star$，包括 $\\|w\\|_2 = 0$ 的边界情况。\n2. 实现一个程序，在给定点集 $\\{x_i\\}$、向量 $w$ 和半径 $\\epsilon$ 的情况下，计算所有最坏情况下的扰动样本位置 $\\{x_i^\\star\\}$。\n3. 将您的实现应用于以下测试套件。为了输出的数值稳定性，报告的坐标应四舍五入到六位小数。\n\n测试套件（每个案例的形式为 $(X, w, \\epsilon)$，其中 $X$ 是一个二维点列表）：\n- 案例 1: $X = \\{[1.0, 0.0], [0.0, 1.0]\\}$, $w = [2.0, 1.0]$, $\\epsilon = 0.0$。\n- 案例 2: $X = \\{[1.0, 0.0], [0.0, 1.0]\\}$, $w = [2.0, 1.0]$, $\\epsilon = 0.5$。\n- 案例 3: $X = \\{[1.0, 0.0], [0.0, 1.0]\\}$, $w = [2.0, 1.0]$, $\\epsilon = 1.0$。\n- 案例 4: $X = \\{[-1.0, -1.0], [2.0, 0.0], [0.0, -2.0]\\}$, $w = [0.0, 0.0]$, $\\epsilon = 0.75$。\n- 案例 5: $X = \\{[0.5, -0.5], [1.5, 2.0]\\}$, $w = [-3.0, 4.0]$, $\\epsilon = 0.25$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素对应一个测试用例，并且是该用例计算出的最坏情况位置的列表。每个位置必须是一个双元素列表 $[x_1^\\star, x_2^\\star]$，两个条目都四舍五入到六位小数。例如，格式应如下所示：\n  - 对于两个各有两点的案例： $[[[x_{11}^\\star, x_{12}^\\star], [x_{21}^\\star, x_{22}^\\star]], [[y_{11}^\\star, y_{12}^\\star], [y_{21}^\\star, y_{22}^\\star]]]$。\n- 此问题不涉及任何物理单位。",
            "solution": "用户希望解决一个在分布不确定性下的最坏情况优化问题。\n\n### **问题陈述验证**\n\n**步骤 1：提取已知条件**\n- **数据**：一个包含 $n$ 个点的集合 $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^2$。\n- **经验分布**：$P_n = \\frac{1}{n} \\sum_{i=1}^n \\delta_{x_i}$，其中 $\\delta_{x_i}$ 是在点 $x_i$ 上的狄拉克质量。\n- **损失函数**：一个线性泛函 $f_w(x) = w^\\top x$，其中 $w \\in \\mathbb{R}^2$ 是给定的向量。\n- **不确定性集合**：一个无穷阶 Wasserstein 球，定义为在 $\\mathbb{R}^2$ 上所有满足 Wasserstein 距离 $W_\\infty(Q, P_n) \\le \\epsilon$ 的分布 $Q$ 的集合。底层度量是欧几里得范数 $\\| \\cdot \\|_2$。\n- **目标**：找到最坏情况下的样本位置 $\\{x_i^\\star\\}_{i=1}^n$，这些位置能达到在不确定性集合上期望损失的上确界。数学上表示为找到解决 $\\sup_{Q: W_\\infty(Q, P_n) \\le \\epsilon} \\mathbb{E}_Q[w^\\top x]$ 的位置。\n- **基本定义**：\n    - $W_\\infty$ 距离约束意味着每个质量点 $x_i$ 可以被移动到一个新位置 $x'$，该位置在以 $x_i$ 为中心、半径为 $\\epsilon$ 的欧几里得球内，即 $\\|x' - x_i\\|_2 \\le \\epsilon$。\n    - 欧几里得范数为 $\\|v\\|_2 = \\sqrt{v_1^2 + v_2^2}$。\n    - 线性泛函 $w^\\top x$ 在闭合欧几里得球上的最大化器位于边界上，方向为梯度 $w$ 的方向，前提是 $w \\neq 0$。\n    - 柯西-施瓦茨不等式：对于 $a, b \\in \\mathbb{R}^2$，有 $a^\\top b \\le \\|a\\|_2 \\|b\\|_2$。\n- **任务**：\n    1. 在约束 $\\|x - x_i\\|_2 \\le \\epsilon$ 下，推导 $f_w(x)$ 的逐样本最大化器 $x_i^\\star$，并处理 $\\|w\\|_2 = 0$ 的情况。\n    2. 实现一个程序，根据给定的 $\\{x_i\\}$, $w$, 和 $\\epsilon$ 计算 $\\{x_i^\\star\\}$。\n    3. 在给定的测试套件上运行并格式化输出。\n\n**步骤 2：使用提取的已知条件进行验证**\n根据验证标准对问题进行评估。\n- **科学上成立**：是。该问题是分布鲁棒优化（DRO）中的一个标准表述，基于已建立的数学概念，如 Wasserstein 距离、凸优化和对偶理论。\n- **适定性**：是。目标是在一个紧集（一组闭球）上最大化一个连续（线性）函数。根据极值定理，最大值存在。推导将表明当 $w \\ne 0$ 时，该最大值是唯一的。\n- **目标明确**：是。问题使用精确、无歧义的数学术语进行陈述。\n- **缺陷检查**：\n    1.  **科学上不健全**：无。该表述是 DRO 中一个有效且被广泛研究的问题。\n    2.  **无法形式化/不相关**：无。问题陈述是形式化的，并且是分布鲁棒优化主题的核心。\n    3.  **不完整/矛盾**：无。每个测试用例所需的所有数据（$X, w, \\epsilon$）均已提供。\n    4.  **不现实/不可行**：不适用。该问题是纯数学问题，不涉及物理约束。\n    5.  **病态的**：无。如上所述，存在一个明确定义的解。\n    6.  **伪深刻/琐碎**：无。该问题要求从第一性原理进行正确推导和精确实现，这是一个有效的挑战。\n    7.  **超出科学可验证性**：无。解是可以通过数学推导和计算验证的。\n\n**步骤 3：结论与行动**\n问题有效。开始解决。\n\n### **最坏情况样本位置的推导**\n\n目标是计算最坏情况期望损失：\n$$ \\sup_{Q: W_\\infty(Q, P_n) \\le \\epsilon} \\mathbb{E}_Q[w^\\top x] $$\n分布鲁棒优化中的一个基本结果，通常由 Kantorovich 对偶性推导得出，指出对于无穷阶 Wasserstein 球，函数的最坏情况期望可以通过对每个样本的最坏情况实现独立取上确界来找到。鉴于经验分布 $P_n$ 在每个点 $x_i$ 上的质量为 $1/n$，问题可以解耦为 $n$ 个独立的子问题：\n$$ \\sup_{Q: W_\\infty(Q, P_n) \\le \\epsilon} \\mathbb{E}_Q[w^\\top x] = \\frac{1}{n} \\sum_{i=1}^n \\sup_{x'} \\{ w^\\top x' \\mid \\|x' - x_i\\|_2 \\le \\epsilon \\} $$\n我们的任务是为这 $n$ 个相同的最大化问题中的每一个找到优化器 $x_i^\\star$。让我们分析一个针对一般样本 $x_i$ 的子问题：\n$$ \\max_{x'} w^\\top x' \\quad \\text{subject to} \\quad \\|x' - x_i\\|_2 \\le \\epsilon $$\n设变量转换为 $x' = x_i + \\delta$，其中 $\\delta \\in \\mathbb{R}^2$。对 $x'$ 的约束转化为对扰动 $\\delta$ 的约束：\n$$ \\|(x_i + \\delta) - x_i\\|_2 \\le \\epsilon \\implies \\|\\delta\\|_2 \\le \\epsilon $$\n目标函数变为：\n$$ w^\\top x' = w^\\top (x_i + \\delta) = w^\\top x_i + w^\\top \\delta $$\n由于 $w^\\top x_i$ 相对于优化变量 $\\delta$ 是一个常数，因此最大化 $w^\\top x'$ 等价于最大化 $w^\\top \\delta$：\n$$ \\max_{\\delta} w^\\top \\delta \\quad \\text{subject to} \\quad \\|\\delta\\|_2 \\le \\epsilon $$\n我们可以使用柯西-施瓦茨不等式来确定最优扰动 $\\delta^\\star$，该不等式指出 $a^\\top b \\le \\|a\\|_2 \\|b\\|_2$。应用此不等式，我们得到：\n$$ w^\\top \\delta \\le \\|w\\|_2 \\|\\delta\\|_2 $$\n根据约束 $\\|\\delta\\|_2 \\le \\epsilon$，我们可以进一步界定该表达式：\n$$ w^\\top \\delta \\le \\|w\\|_2 \\epsilon $$\n当 $\\delta$ 与 $w$ 正向共线且具有最大可能的大小（即 $\\epsilon$）时，等号成立（从而达到最大值）。\n\n我们现在分析向量 $w$ 的两种情况：\n\n**情况 1：$w \\ne 0$**\n如果 $w$ 不是零向量，则 $\\|w\\|_2  0$。最大化 $w^\\top \\delta$ 的唯一最优扰动 $\\delta^\\star$ 是在 $w$ 方向上长度为 $\\epsilon$ 的向量：\n$$ \\delta^\\star = \\epsilon \\frac{w}{\\|w\\|_2} $$\n然后，通过将此最优扰动加到原始样本位置 $x_i$ 上，可以找到最坏情况的样本位置 $x_i^\\star$：\n$$ x_i^\\star = x_i + \\delta^\\star = x_i + \\epsilon \\frac{w}{\\|w\\|_2} $$\n这表明，为了最大化线性损失，每个数据点都应沿着损失向量的梯度 $w$ 的方向移动，移动的距离为不确定性集合所允许的最大范围，即 $\\epsilon$。这对每个样本 $i=1, \\dots, n$ 都成立。\n\n**情况 2：$w = 0$**\n如果 $w$ 是零向量，则 $\\|w\\|_2 = 0$。目标函数变为：\n$$ w^\\top \\delta = 0^\\top \\delta = 0 $$\n在这种情况下，只要 $\\delta$ 满足约束 $\\|\\delta\\|_2 \\le \\epsilon$，无论如何选择，目标值都为 $0$。以 $x_i$ 为中心、半径为 $\\epsilon$ 的球内的任何点都是一个优化器。在这种情况下，一个自然且标准的选择是选取具有最小范数的解，这对应于做出最小的改变。这导致选择 $\\delta^\\star = 0$。因此，最坏情况的位置就是原始位置本身：\n$$ x_i^\\star = x_i $$\n注意，如果 $\\|w\\|_2=0$，情况 1 的公式是未定义的，因此必须单独处理这种情况。\n\n结合这两种情况，最坏情况位置 $x_i^\\star$ 的公式为：\n$$ x_i^\\star = \\begin{cases} x_i + \\epsilon \\frac{w}{\\|w\\|_2}  \\text{if } \\|w\\|_2  0 \\\\ x_i  \\text{if } \\|w\\|_2 = 0 \\end{cases} $$\n此公式将被实现以解决给定测试套件的问题。",
            "answer": "```python\nimport numpy as np\n\ndef compute_worst_case_locations(X, w, epsilon):\n    \"\"\"\n    Computes the worst-case sample locations for a linear loss.\n\n    Args:\n        X (list of lists): The original sample locations, [[x1, y1], [x2, y2], ...].\n        w (list): The parameter vector [w1, w2] of the linear loss.\n        epsilon (float): The radius of the Wasserstein ball (and per-sample uncertainty sets).\n\n    Returns:\n        list of lists: The worst-case sample locations.\n    \"\"\"\n    X_np = np.array(X, dtype=float)\n    w_np = np.array(w, dtype=float)\n\n    # Handle the edge case where epsilon is zero. No perturbation is possible.\n    if epsilon == 0.0:\n        return X_np.tolist()\n\n    # Calculate the Euclidean norm of the vector w.\n    norm_w = np.linalg.norm(w_np)\n\n    # If the norm of w is zero (or numerically close to it), the loss is constant.\n    # The optimal perturbation is zero (no change to the points).\n    if norm_w  1e-9:\n        return X_np.tolist()\n    \n    # Calculate the unit vector in the direction of w.\n    u_w = w_np / norm_w\n    \n    # The optimal perturbation is epsilon times the unit vector u_w.\n    # This vector is the same for all points.\n    delta_star = epsilon * u_w\n    \n    # Add the perturbation to all original sample locations.\n    # Numpy's broadcasting handles this efficiently.\n    X_star_np = X_np + delta_star\n    \n    return X_star_np.tolist()\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        ({\"X\": [[1.0, 0.0], [0.0, 1.0]], \"w\": [2.0, 1.0], \"epsilon\": 0.0}),\n        # Case 2\n        ({\"X\": [[1.0, 0.0], [0.0, 1.0]], \"w\": [2.0, 1.0], \"epsilon\": 0.5}),\n        # Case 3\n        ({\"X\": [[1.0, 0.0], [0.0, 1.0]], \"w\": [2.0, 1.0], \"epsilon\": 1.0}),\n        # Case 4\n        ({\"X\": [[-1.0, -1.0], [2.0, 0.0], [0.0, -2.0]], \"w\": [0.0, 0.0], \"epsilon\": 0.75}),\n        # Case 5\n        ({\"X\": [[0.5, -0.5], [1.5, 2.0]], \"w\": [-3.0, 4.0], \"epsilon\": 0.25}),\n    ]\n\n    # Store formatted string results for each test case\n    results_str_list = []\n\n    for case in test_cases:\n        worst_case_locs = compute_worst_case_locations(case[\"X\"], case[\"w\"], case[\"epsilon\"])\n        \n        # Format the result for a single case according to the required output format.\n        # Each point is formatted as [x,y] with 6 decimal places.\n        points_str = [f\"[{p[0]:.6f},{p[1]:.6f}]\" for p in worst_case_locs]\n        case_result_str = f\"[{','.join(points_str)}]\"\n        results_str_list.append(case_result_str)\n\n    # Join the results of all cases into a single string.\n    final_output = f\"[{','.join(results_str_list)}]\"\n    \n    # Final print statement must produce only the specified output format.\n    print(final_output)\n\n# Execute the solver.\nsolve()\n\n```"
        }
    ]
}