{
    "hands_on_practices": [
        {
            "introduction": "The LASSO is a cornerstone of modern statistics and machine learning, but how does it actually produce sparse solutions? This first exercise guides you through the mechanics of coordinate descent, a simple yet powerful algorithm for solving the LASSO problem . By minimizing the objective one variable at a time, you will derive the celebrated soft-thresholding operator from first principles and see exactly how regularization shrinks coefficients towards zero.",
            "id": "3172091",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem, which minimizes the objective function\n$$\nF(w) \\;=\\; \\frac{1}{2}\\,\\|y - X w\\|_{2}^{2} \\;+\\; \\lambda\\,\\|w\\|_{1},\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is a fixed data matrix, $y \\in \\mathbb{R}^{n}$ is a fixed response vector, $w \\in \\mathbb{R}^{p}$ is the decision variable, and $\\lambda > 0$ is a regularization parameter. Let $X_{j} \\in \\mathbb{R}^{n}$ denote the $j$-th column of $X$ and $w_{j}$ the $j$-th component of $w$. Define the soft-thresholding operator $S_{\\alpha}(z)$ by\n$$\nS_{\\alpha}(z) \\;=\\; \\operatorname{sign}(z)\\,\\max\\big(|z| - \\alpha,\\,0\\big),\n$$\nfor $\\alpha \\ge 0$ and $z \\in \\mathbb{R}$.\n\nStarting from the definition of $F(w)$ and the subgradient of the absolute value function, derive the exact coordinate-wise minimizer for $w_{j}$ while holding all other coordinates fixed. Your derivation must start from the one-dimensional reduction obtained by isolating the contribution of coordinate $j$ and use only first principles (convexity and subgradient optimality for the $\\ell_{1}$ norm). Then, using your derived update, compute a concrete coordinate descent update for the following instance:\n$$\nX \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 \\\\\n0 & 1 & 3\n\\end{pmatrix},\\qquad\ny \\;=\\; \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix},\\qquad\nw \\;=\\; \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.1 \\end{pmatrix},\\qquad\n\\lambda \\;=\\; 1.2,\n$$\nfor coordinate $j = 2$. Provide the updated numerical value of $w_{2}$ after one exact coordinate-wise minimization step.\n\nFinally, briefly explain, based on the structure of $F(w)$ and properties of coordinate descent, why cyclic coordinate descent enjoys global convergence for this convex problem and why sparsity in $X$ can accelerate convergence in practice. Your explanation should rely on fundamental properties (convexity, separability of the $\\ell_{1}$ penalty, and coordinate-wise Lipschitz continuity of the smooth part) without invoking unproven claims or shortcut formulas.\n\nExpress the final numerical answer for the updated $w_{2}$ as a single real number. If rounding is necessary, round to four significant figures; otherwise, provide the exact value.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the theory of convex optimization, well-posed with a unique solution for the specified task, and all definitions and data are complete and consistent. We may, therefore, proceed with the solution.\n\nThe problem requires a three-part response: first, a derivation of the coordinate-wise minimizer for the LASSO objective function; second, a numerical application of this result; and third, an explanation of the convergence properties of coordinate descent for this problem.\n\n**Part 1: Derivation of the Coordinate-wise Minimizer**\n\nThe LASSO objective function is given by\n$$\nF(w) = \\frac{1}{2}\\|y - X w\\|_{2}^{2} + \\lambda\\|w\\|_{1}\n$$\nwhere $w \\in \\mathbb{R}^{p}$. We wish to find the minimizer of $F(w)$ with respect to a single coordinate $w_j$, while all other coordinates $w_k$ for $k \\neq j$ are held fixed.\n\nWe can apportion the function $F(w)$ into terms that depend on $w_j$ and terms that are constant with respect to $w_j$. The smooth term is $\\|y - X w\\|_{2}^{2}$. We can separate the contribution of the $j$-th column of $X$ as follows:\n$$\nXw = \\sum_{k=1}^{p} X_k w_k = X_j w_j + \\sum_{k \\neq j} X_k w_k\n$$\nLet us define the partial residual vector $r_{(-j)} \\in \\mathbb{R}^{n}$ as\n$$\nr_{(-j)} = y - \\sum_{k \\neq j} X_k w_k\n$$\nSubstituting this into the objective function, we get\n$$\nF(w) = \\frac{1}{2}\\|r_{(-j)} - X_j w_j\\|_{2}^{2} + \\lambda \\sum_{k=1}^{p} |w_k| = \\frac{1}{2}\\|r_{(-j)} - X_j w_j\\|_{2}^{2} + \\lambda |w_j| + \\lambda \\sum_{k \\neq j} |w_k|\n$$\nTo minimize $F(w)$ with respect to $w_j$, we can ignore all terms not depending on $w_j$. This gives us a one-dimensional optimization problem:\n$$\n\\min_{w_j \\in \\mathbb{R}} f_j(w_j) \\quad \\text{where} \\quad f_j(w_j) = \\frac{1}{2}\\|r_{(-j)} - X_j w_j\\|_{2}^{2} + \\lambda|w_j|\n$$\nLet us expand the squared $\\ell_2$-norm:\n$$\n\\|r_{(-j)} - X_j w_j\\|_{2}^{2} = (r_{(-j)} - X_j w_j)^T(r_{(-j)} - X_j w_j) = \\|r_{(-j)}\\|_2^2 - 2 w_j X_j^T r_{(-j)} + w_j^2 \\|X_j\\|_2^2\n$$\nThe function $f_j(w_j)$ can be written, up to an additive constant, as\n$$\nf_j(w_j) = \\frac{1}{2}\\|X_j\\|_2^2 w_j^2 - (X_j^T r_{(-j)}) w_j + \\lambda|w_j|\n$$\nThis function is convex, being the sum of a quadratic function (which is convex, assuming $X_j \\neq 0$) and the absolute value function multiplied by $\\lambda > 0$ (which is convex). A point $w_j^*$ is a global minimizer of $f_j(w_j)$ if and only if $0$ belongs to the subdifferential of $f_j$ at $w_j^*$, denoted $\\partial f_j(w_j^*)$.\n\nThe subdifferential of $f_j(w_j)$ is given by:\n$$\n\\partial f_j(w_j) = \\|X_j\\|_2^2 w_j - X_j^T r_{(-j)} + \\lambda \\partial|w_j|\n$$\nwhere the subdifferential of the absolute value function is\n$$\n\\partial|z| = \\begin{cases} \\{\\operatorname{sign}(z)\\} & \\text{if } z \\neq 0 \\\\ [-1, 1] & \\text{if } z = 0 \\end{cases}\n$$\nThe optimality condition $0 \\in \\partial f_j(w_j^*)$ thus becomes:\n$$\nX_j^T r_{(-j)} - \\|X_j\\|_2^2 w_j^* \\in \\lambda \\partial|w_j^*|\n$$\nWe analyze this condition by cases for the value of $w_j^*$:\n1.  If $w_j^* > 0$, then $\\partial|w_j^*| = \\{1\\}$. The condition becomes $X_j^T r_{(-j)} - \\|X_j\\|_2^2 w_j^* = \\lambda$, which gives $w_j^* = \\frac{X_j^T r_{(-j)} - \\lambda}{\\|X_j\\|_2^2}$. For this solution to be consistent with the initial assumption $w_j^* > 0$, we must have $X_j^T r_{(-j)} > \\lambda$.\n2.  If $w_j^* < 0$, then $\\partial|w_j^*| = \\{-1\\}$. The condition becomes $X_j^T r_{(-j)} - \\|X_j\\|_2^2 w_j^* = -\\lambda$, which gives $w_j^* = \\frac{X_j^T r_{(-j)} + \\lambda}{\\|X_j\\|_2^2}$. For consistency with $w_j^* < 0$, we must have $X_j^T r_{(-j)} < -\\lambda$.\n3.  If $w_j^* = 0$, then $\\partial|w_j^*| = [-1, 1]$. The condition becomes $X_j^T r_{(-j)} \\in [-\\lambda, \\lambda]$, or $|X_j^T r_{(-j)}| \\le \\lambda$.\n\nLet's consolidate these results. Let $z = X_j^T r_{(-j)}$ and $\\alpha = \\|X_j\\|_2^2$. The optimal $w_j^*$ is:\n$$\nw_j^* = \\begin{cases} (z - \\lambda)/\\alpha & \\text{if } z > \\lambda \\\\ (z + \\lambda)/\\alpha & \\text{if } z < -\\lambda \\\\ 0 & \\text{if } |z| \\le \\lambda \\end{cases}\n$$\nThis expression is precisely the soft-thresholding operator $S_{\\lambda/\\alpha}(z/\\alpha)$. Let's verify:\n$$\nS_{\\lambda/\\alpha}\\left(\\frac{z}{\\alpha}\\right) = \\operatorname{sign}\\left(\\frac{z}{\\alpha}\\right) \\max\\left(\\left|\\frac{z}{\\alpha}\\right| - \\frac{\\lambda}{\\alpha}, 0\\right) = \\frac{1}{\\alpha} \\operatorname{sign}(z) \\max\\left(|z| - \\lambda, 0\\right)\n$$\n- If $z > \\lambda$, the expression gives $\\frac{1}{\\alpha}(1)(z - \\lambda) = (z-\\lambda)/\\alpha$.\n- If $z < -\\lambda$, the expression gives $\\frac{1}{\\alpha}(-1)(-z - \\lambda) = (z+\\lambda)/\\alpha$.\n- If $|z| \\le \\lambda$, the expression gives $\\frac{1}{\\alpha}\\operatorname{sign}(z)(0) = 0$.\nThe derivation is complete. The exact coordinate-wise minimizer for $w_j$ is given by $w_j^* = S_{\\lambda/\\|X_j\\|_2^2}\\left(\\frac{X_j^T r_{(-j)}}{\\|X_j\\|_2^2}\\right)$.\n\n**Part 2: Numerical Calculation**\n\nWe are given the following instance:\n$$\nX = \\begin{pmatrix} 2 & -1 & 0 \\\\ 0 & 1 & 3 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}, \\quad w_{\\text{initial}} = \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.1 \\end{pmatrix}, \\quad \\lambda = 1.2\n$$\nWe need to update the coordinate $j=2$, which corresponds to $w_2$. The other coordinates are fixed at their current values: $w_1 = 0.5$ and $w_3 = 0.1$.\n\nFirst, we identify the relevant column of $X$: $X_2 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\nThe squared norm of this column is $\\|X_2\\|_2^2 = (-1)^2 + 1^2 = 2$.\n\nNext, we calculate the partial residual $r_{(-2)}$:\n$$\nr_{(-2)} = y - X_1 w_1 - X_3 w_3 = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}(0.5) - \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}(0.1)\n$$\n$$\nr_{(-2)} = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 1 - 1 - 0 \\\\ 4 - 0 - 0.3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3.7 \\end{pmatrix}\n$$\nNow, we compute the term $X_2^T r_{(-2)}$:\n$$\nX_2^T r_{(-2)} = \\begin{pmatrix} -1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3.7 \\end{pmatrix} = (-1)(0) + (1)(3.7) = 3.7\n$$\nWe can now apply the soft-thresholding operator. The updated value, $w_2^{\\text{new}}$, is\n$$\nw_2^{\\text{new}} = S_{\\lambda/\\|X_2\\|_2^2}\\left(\\frac{X_2^T r_{(-2)}}{\\|X_2\\|_2^2}\\right)\n$$\nThe threshold parameter is $\\lambda/\\|X_2\\|_2^2 = 1.2/2 = 0.6$. The argument to the operator is $X_2^T r_{(-2)}/\\|X_2\\|_2^2 = 3.7/2 = 1.85$.\nSo, we compute:\n$$\nw_2^{\\text{new}} = S_{0.6}(1.85)\n$$\nUsing the definition $S_{\\alpha}(z) = \\operatorname{sign}(z) \\max(|z|-\\alpha, 0)$:\n$$\nw_2^{\\text{new}} = \\operatorname{sign}(1.85) \\max(|1.85| - 0.6, 0) = 1 \\cdot \\max(1.85 - 0.6, 0) = \\max(1.25, 0) = 1.25\n$$\nThe updated value for $w_2$ is exactly $1.25$.\n\n**Part 3: Convergence Explanation**\n\nThe global convergence of cyclic coordinate descent for the LASSO problem rests on the specific structure of the objective function $F(w)$.\nThe function $F(w) = \\frac{1}{2}\\|y - Xw\\|_2^2 + \\lambda \\|w\\|_1$ is a sum of two components:\n1.  A smooth, differentiable, convex function $f(w) = \\frac{1}{2}\\|y - Xw\\|_2^2$. Its gradient is $\\nabla f(w) = X^T(Xw - y)$. The gradient with respect to a single coordinate, $\\nabla_j f(w) = X_j^T(Xw - y)$, is Lipschitz continuous with a constant $L_j = \\|X_j\\|_2^2$.\n2.  A non-smooth, convex function $g(w) = \\lambda \\|w\\|_1$. Crucially, this function is **separable**, meaning it can be written as a sum of functions of individual coordinates: $g(w) = \\sum_{j=1}^p \\lambda|w_j|$.\n\nThe combination of these properties—a convex objective function composed of a smooth part with coordinate-wise Lipschitz gradients and a separable non-smooth part—is sufficient to guarantee that cyclic coordinate descent converges to a global minimum. At each step, the algorithm performs an exact minimization over one coordinate. Because the function is convex, any local minimum is a global minimum. The sequence of iterates $\\{w^{(k)}\\}$ generated by coordinate descent is guaranteed to have limit points, and any such limit point is a global minimizer of $F(w)$. This is a classical result in non-smooth convex optimization theory.\n\nThe sparsity of the data matrix $X$ can significantly accelerate the convergence in terms of computation time. The core computation in each coordinate update for $w_j$ is the term $X_j^T r_{(-j)}$. This can be calculated as $X_j^T y - \\sum_{k \\neq j} (X_j^T X_k) w_k$. If $X$ is sparse, many of its columns $X_k$ will be sparse vectors. The inner product $X_j^T X_k$ can be computed much faster, as the number of required multiplications is proportional to the number of positions where both vectors have non-zero elements, not their full dimension $n$. In many cases, this inner product will be exactly zero. This reduces the cost of the summation. Alternatively, if one maintains the full residual $r=y-Xw$, the update $w_j \\to w_j^{\\text{new}}$ requires an update to the residual $r \\to r - X_j (w_j^{\\text{new}} - w_j^{\\text{old}})$. If the vector $X_j$ is sparse (has few non-zero entries), this update operation is computationally inexpensive. Thus, each step of coordinate descent completes faster, leading to a faster overall time to convergence.",
            "answer": "$$\n\\boxed{1.25}\n$$"
        },
        {
            "introduction": "While coordinate descent updates one variable at a time, proximal gradient methods update the entire vector at once. This practice introduces the Iterative Soft-Thresholding Algorithm (ISTA), the canonical proximal gradient method for the LASSO . You will move from derivation to implementation, and in the process, tackle the critical challenge of step-size selection by implementing a backtracking line search, a robust technique for adapting the algorithm to the geometry of the problem.",
            "id": "3172033",
            "problem": "Consider the composite optimization problem with an $\\ell_2$ data-fidelity term and an $\\ell_1$ regularization term. Let $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^{m}$. Define the smooth convex function $g:\\mathbb{R}^{n}\\to\\mathbb{R}$ by $g(w) = \\frac{1}{2}\\lVert A w - b\\rVert_2^2$, and the full objective $\\phi:\\mathbb{R}^{n}\\to\\mathbb{R}$ by $\\phi(w) = g(w) + \\lambda \\lVert w\\rVert_1$, where $\\lambda > 0$ and $\\lVert \\cdot \\rVert_1$ denotes the $\\ell_1$ norm. The gradient $\\nabla g$ is Lipschitz continuous with some Lipschitz constant $L$, meaning $g(u) \\le g(w) + \\langle \\nabla g(w), u-w \\rangle + \\frac{L}{2}\\lVert u-w \\rVert_2^2$ for all $u,w \\in \\mathbb{R}^n$.\n\nYour task is to implement the Iterative Soft-Thresholding Algorithm (ISTA) with Backtracking Line Search (BLS) for this composite objective. At iteration $k$, given a current iterate $w^k$ and a current step size $\\eta > 0$, define the quadratic surrogate\n$$\nQ_{\\eta}(w^k, u) = g(w^k) + \\langle \\nabla g(w^k), u - w^k \\rangle + \\frac{1}{2\\eta}\\lVert u - w^k \\rVert_2^2 + \\lambda \\lVert u \\rVert_1.\n$$\nThe Backtracking Line Search proceeds as follows: starting from a given $\\eta$, construct a candidate $u$ as the unique minimizer of $Q_{\\eta}(w^k, \\cdot)$ and check the acceptance criterion $\\phi(u) \\le Q_{\\eta}(w^k, u)$. If the criterion fails, shrink the step size $\\eta \\leftarrow \\beta \\eta$ with $0 < \\beta < 1$, recompute $u$ for the new $\\eta$, and repeat until the criterion holds. Then set $w^{k+1} \\leftarrow u$ and continue. This mechanism is related to estimating the Lipschitz constant (LC) $L$ of $g$ because the acceptance criterion guarantees that $\\frac{1}{\\eta}$ is an empirical upper bound on $L$ whenever the criterion is satisfied.\n\nImplement the above procedure in a self-contained program. Use the following base definitions and facts:\n- The gradient $\\nabla g(w)$ for $g(w) = \\frac{1}{2}\\lVert A w - b\\rVert_2^2$ is $\\nabla g(w) = A^\\top(Aw - b)$.\n- The proximal mapping associated with the $\\ell_1$ norm is used to obtain the minimizer of $Q_{\\eta}(w^k, \\cdot)$.\n- The true Lipschitz constant $L$ of $\\nabla g$ equals the largest eigenvalue of $A^\\top A$, which is the square of the largest singular value of $A$.\n\nInitialize the algorithm at $w^0 = 0$. Use the shrink factor $\\beta = 0.5$ and run for $K = 60$ iterations in all test cases. For numerical comparisons, define the empirical upper bound $\\hat{L}$ as the maximum value of $\\frac{1}{\\eta}$ used at accepted steps across all iterations. Let $L_{\\text{true}}$ be the true Lipschitz constant computed from $A$. For each test case, report the absolute difference $\\lvert \\hat{L} - L_{\\text{true}} \\rvert$.\n\nTest Suite:\nImplement four test cases, each specifying $(m,n,\\text{seed},s,\\lambda,\\eta_{\\text{init}})$, where $s$ is a scaling factor applied to $A$:\n- Case $1$: $(m,n,\\text{seed},s,\\lambda,\\eta_{\\text{init}}) = (40, 20, 17, 1.0, 0.05, \\eta_{\\text{init}})$ with $\\eta_{\\text{init}}$ chosen as $\\eta_{\\text{init}} = \\frac{0.9}{L_{\\text{true}}}$ for the generated $A$.\n- Case $2$: $(40, 20, 23, 1.0, 0.05, 10.0)$.\n- Case $3$: $(50, 30, 42, 30.0, 0.10, 1.0)$.\n- Case $4$: $(50, 30, 99, 0.0, 1.00, 1.0)$, where $A$ is the zero matrix due to $s=0.0$.\n\nData generation:\n- For each case, generate $A$ with independent standard normal entries and then scale by $s$ (i.e., set $A \\leftarrow s \\cdot A$).\n- Generate $b$ with independent standard normal entries.\n- Use the given $\\text{seed}$ to initialize a pseudorandom number generator for reproducibility.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_i$ is the float $\\lvert \\hat{L} - L_{\\text{true}} \\rvert$ for the $i$-th test case in the order listed above. No other text should be printed. Angles and physical units are not used in this problem, so none are required.",
            "solution": "The problem requires the implementation of the Iterative Soft-Thresholding Algorithm (ISTA) with a Backtracking Line Search (BLS) to solve the composite convex optimization problem:\n$$\n\\min_{w \\in \\mathbb{R}^n} \\phi(w) \\quad \\text{where} \\quad \\phi(w) = g(w) + \\lambda \\lVert w \\rVert_1.\n$$\nThe function $g(w)$ is the smooth, convex data-fidelity term, defined as $g(w) = \\frac{1}{2}\\lVert Aw - b \\rVert_2^2$. The second term is the $\\ell_1$-norm regularization, weighted by a parameter $\\lambda > 0$.\n\nThe core of ISTA is the proximal gradient method. At each iteration $k$, the next iterate $w^{k+1}$ is computed by taking a gradient descent step with respect to the smooth part $g(w)$ and then applying the proximal operator of the non-smooth part. The update rule is:\n$$\nw^{k+1} = \\text{prox}_{\\eta_k \\lambda, \\lVert\\cdot\\rVert_1}(w^k - \\eta_k \\nabla g(w^k)),\n$$\nwhere $\\eta_k > 0$ is the step size at iteration $k$. The gradient of $g(w)$ is given by $\\nabla g(w) = A^\\top(Aw - b)$.\n\nThe proximal operator of the $\\ell_1$-norm, scaled by a factor $\\alpha > 0$, is the soft-thresholding operator $S_{\\alpha}$. For a vector $z \\in \\mathbb{R}^n$, it is applied component-wise:\n$$\n(\\text{prox}_{\\alpha, \\lVert\\cdot\\rVert_1}(z))_i = S_{\\alpha}(z_i) = \\text{sign}(z_i) \\max(|z_i| - \\alpha, 0).\n$$\nIn our case, $\\alpha = \\eta_k \\lambda$. Thus, the candidate update $u$ is found by minimizing the quadratic surrogate function $Q_{\\eta}(w^k, u)$, which can be shown to be equivalent to the proximal gradient step:\n$$\nu = \\arg\\min_{u'} Q_{\\eta}(w^k, u') = \\arg\\min_{u'} \\left( \\frac{1}{2\\eta} \\lVert u' - (w^k - \\eta \\nabla g(w^k)) \\rVert_2^2 + \\lambda \\lVert u' \\rVert_1 \\right) = \\text{prox}_{\\eta\\lambda, \\lVert\\cdot\\rVert_1}(w^k - \\eta \\nabla g(w^k)).\n$$\n\nA crucial part of the algorithm is selecting the step size $\\eta_k$. Convergence is guaranteed if $\\eta_k < 1/L$, where $L$ is the Lipschitz constant of $\\nabla g(w)$. The Backtracking Line Search (BLS) is a strategy to find an appropriate $\\eta_k$ at each iteration without needing prior knowledge of $L$. The method proceeds as follows:\n\nAt each iteration $k$, starting with an initial step size guess $\\eta$:\n1. Compute the candidate point $u = \\text{prox}_{\\eta\\lambda, \\lVert\\cdot\\rVert_1}(w^k - \\eta \\nabla g(w^k))$.\n2. Check the acceptance criterion: $\\phi(u) \\le Q_{\\eta}(w^k, u)$. Substituting the definitions of $\\phi$ and $Q_{\\eta}$, this inequality simplifies to:\n$$\ng(u) \\le g(w^k) + \\langle \\nabla g(w^k), u - w^k \\rangle + \\frac{1}{2\\eta} \\lVert u - w^k \\rVert_2^2.\n$$\nThis condition ensures sufficient decrease in the objective function.\n3. If the criterion is met, the step size $\\eta_k = \\eta$ is accepted, and the update is performed: $w^{k+1} = u$.\n4. If the criterion is not met, the step size is reduced, $\\eta \\leftarrow \\beta \\eta$ (with $0 < \\beta < 1$), and steps 1-2 are repeated until the condition is satisfied.\n\nThe problem specifies an initial iterate $w^0=0$, a shrink factor $\\beta=0.5$, and a total of $K=60$ iterations. For each iteration, the BLS starts with a given initial step size $\\eta_{\\text{init}}$.\n\nThe true Lipschitz constant of $\\nabla g(w)$, denoted $L_{\\text{true}}$, is the maximum eigenvalue of the Hessian $\\nabla^2 g(w) = A^\\top A$. This is equivalent to the square of the largest singular value of the matrix $A$:\n$$\nL_{\\text{true}} = \\lambda_{\\max}(A^\\top A) = \\sigma_{\\max}(A)^2.\n$$\nThis can be computed using Singular Value Decomposition (SVD).\n\nThe acceptance criterion ensures that $1/\\eta_k$ serves as a local upper bound on the curvature of $g(w)$. The empirical upper bound for the Lipschitz constant, $\\hat{L}$, is defined as the maximum of these bounds over all iterations:\n$$\n\\hat{L} = \\max_{k=0, \\dots, K-1} \\left\\{ \\frac{1}{\\eta_k} \\right\\},\n$$\nwhere $\\eta_k$ is the accepted step size at iteration $k$. The final task is to compute the absolute difference $|\\hat{L} - L_{\\text{true}}|$ for each test case.\n\nFor the special case where the scaling factor $s=0$, the matrix $A$ becomes the zero matrix. Consequently, $g(w) = \\frac{1}{2}\\lVert b \\rVert_2^2$ is a constant, its gradient $\\nabla g(w)$ is always the zero vector, and its true Lipschitz constant $L_{\\text{true}} = 0$. The algorithm's behavior simplifies, but the procedure remains well-defined.\n\nThe implementation will proceed by setting up each test case, generating the data $A$ and $b$ according to the specified seed and scaling factor, computing $L_{\\text{true}}$, and then executing the ISTA with BLS for $K=60$ iterations. During the execution, the accepted step sizes $\\eta_k$ are recorded to compute $\\hat{L}$ at the end.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the Iterative Soft-Thresholding Algorithm with Backtracking\n    Line Search for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        # (m, n, seed, s, lambda, eta_init)\n        (40, 20, 17, 1.0, 0.05, 'dynamic'),\n        (40, 20, 23, 1.0, 0.05, 10.0),\n        (50, 30, 42, 30.0, 0.10, 1.0),\n        (50, 30, 99, 0.0, 1.00, 1.0),\n    ]\n\n    results = []\n\n    def soft_threshold(z, alpha):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - alpha, 0)\n\n    def g(A, b, w):\n        \"\"\"Computes the value of the smooth function g(w).\"\"\"\n        return 0.5 * np.linalg.norm(A @ w - b)**2\n\n    def grad_g(A, b, w):\n        \"\"\"Computes the gradient of g(w).\"\"\"\n        return A.T @ (A @ w - b)\n\n    for case_params in test_cases:\n        m, n, seed, s, lam, eta_init_val = case_params\n\n        # Data generation\n        rng = np.random.default_rng(seed)\n        A_raw = rng.standard_normal((m, n))\n        b = rng.standard_normal(m)\n        A = s * A_raw\n\n        # Calculate true Lipschitz constant\n        if s == 0.0:\n            L_true = 0.0\n        else:\n            # L_true = sigma_max(A)^2\n            singular_values = np.linalg.svd(A, compute_uv=False)\n            L_true = singular_values[0]**2\n        \n        # Determine initial step size eta_init\n        if eta_init_val == 'dynamic':\n            # Case 1 specific initialization\n            # As s=1.0, A is not the zero matrix, so L_true > 0 with probability 1\n            eta_init = 0.9 / L_true\n        else:\n            eta_init = eta_init_val\n\n        # Algorithm parameters\n        K = 60\n        beta = 0.5\n        w = np.zeros(n)\n        \n        one_over_etas = []\n\n        # Main ISTA loop\n        for _ in range(K):\n            current_grad_g = grad_g(A, b, w)\n            current_g_w = g(A, b, w)\n            \n            eta = eta_init\n\n            # Backtracking Line Search loop\n            while True:\n                # Proximal gradient step\n                z = w - eta * current_grad_g\n                u = soft_threshold(z, eta * lam)\n                \n                # Check acceptance criterion\n                g_u = g(A, b, u)\n                \n                # RHS of the acceptance criterion inequality\n                surrogate_rhs = current_g_w + np.dot(current_grad_g, u - w) + (0.5 / eta) * np.linalg.norm(u - w)**2\n                \n                if g_u <= surrogate_rhs:\n                    break  # Step size accepted\n                else:\n                    eta *= beta # Shrink step size\n            \n            # Store the accepted 1/eta\n            if eta > 0: # Avoid division by zero if eta somehow becomes zero\n                one_over_etas.append(1.0 / eta)\n            \n            # Update w\n            w = u\n            \n        # Calculate empirical Lipschitz constant\n        if not one_over_etas:\n            # This case happens if K=0, but given K=60, the list will not be empty.\n            L_hat = 0.0\n        else:\n            L_hat = max(one_over_etas)\n            \n        # Calculate and store the absolute difference\n        diff = abs(L_hat - L_true)\n        results.append(diff)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "We now explore a powerful and highly general optimization framework: the Alternating Direction Method of Multipliers (ADMM). By reformulating the LASSO problem with an auxiliary variable and a consensus constraint, ADMM breaks the problem into a sequence of simpler subproblems that can be solved efficiently . This exercise will guide you through the derivation and implementation of ADMM, revealing a versatile approach that extends far beyond the LASSO to a vast range of constrained optimization challenges.",
            "id": "3172064",
            "problem": "You are asked to derive and implement the Alternating Direction Method of Multipliers (ADMM) for the Least Absolute Shrinkage and Selection Operator (Lasso) using the splitting variable $z$ with the equality constraint $w=z$. Start from the constrained formulation and fundamental definitions of convex optimization and the augmented Lagrangian method. Derive explicit update formulas for the variables $w$, $z$, and the scaled dual variable $u$ based solely on first principles, namely: the definition of the augmented Lagrangian for equality-constrained convex problems, stationarity conditions for the differentiable subproblems, and the definition of the proximal operator for the $\\ell_{1}$-norm. Then, implement your derived algorithm and evaluate it on the provided test suite.\n\nThe optimization problem is to minimize the objective\n$$\n\\frac{1}{2}\\lVert y - X w \\rVert_{2}^{2} + \\lambda \\lVert z \\rVert_{1}\n$$\nsubject to the constraint\n$$\nw = z,\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is a data matrix, $y \\in \\mathbb{R}^{n}$ is a response vector, $w \\in \\mathbb{R}^{p}$ is the coefficient vector, $\\lambda \\in \\mathbb{R}_{+}$ is the regularization parameter, and $\\lVert \\cdot \\rVert_{1}$ and $\\lVert \\cdot \\rVert_{2}$ denote the $\\ell_{1}$-norm and the Euclidean norm, respectively.\n\nRequirements:\n- Derive the ADMM updates for $w$, $z$, and the scaled dual variable $u$ starting from the augmented Lagrangian. Your derivation must use only the core definitions of the augmented Lagrangian for equality constraints and properties of convex functions and proximal operators. Do not use pre-memorized shortcut formulas.\n- Implement the algorithm with stopping criteria based on the primal and dual residual norms that compare against absolute and relative tolerances.\n\nNumerical implementation details:\n- Use the scaled form of ADMM with penalty parameter $\\rho \\in \\mathbb{R}_{+}$.\n- Use stopping criteria on the primal residual $r^{k} = w^{k} - z^{k}$ and the dual residual $s^{k} = \\rho (z^{k} - z^{k-1})$:\n  - Stop when $\\lVert r^{k} \\rVert_{2} \\le \\varepsilon_{\\mathrm{pri}}$ and $\\lVert s^{k} \\rVert_{2} \\le \\varepsilon_{\\mathrm{dual}}$.\n  - Use $\\varepsilon_{\\mathrm{pri}} = \\sqrt{p}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\{\\lVert w^{k} \\rVert_{2}, \\lVert z^{k} \\rVert_{2}\\}$ and $\\varepsilon_{\\mathrm{dual}} = \\sqrt{p}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\rho \\lVert u^{k} \\rVert_{2}$, where $p$ is the number of columns of $X$.\n- Use $\\varepsilon_{\\mathrm{abs}} = 10^{-6}$, $\\varepsilon_{\\mathrm{rel}} = 10^{-4}$, and maximum iterations $K_{\\max} = 2000$ unless convergence occurs earlier.\n- Initialize $w^{0} = 0$, $z^{0} = 0$, $u^{0} = 0$.\n\nTest suite:\nRun your implementation on the following four cases. For each case, compute the final objective value\n$$\nF(w) = \\frac{1}{2}\\lVert y - X w \\rVert_{2}^{2} + \\lambda \\lVert w \\rVert_{1}\n$$\nat the converged solution $w$ and round it to $6$ decimal places.\n\n- Case A (orthonormal design, checks soft-thresholding behavior):\n  - $X = I_{3}$, the $3 \\times 3$ identity matrix.\n  - $y = [3.0, -1.0, 0.2]^{\\top}$.\n  - $\\lambda = 1.0$.\n  - $\\rho = 1.0$.\n- Case B (very large regularization, boundary case pushing $w$ to zero):\n  - $X = \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -1 \\\\ 1 & 1 & 1 \\\\ 2 & -1 & 0 \\end{bmatrix}$.\n  - $y = [1.0, -2.0, 0.5, 3.0]^{\\top}$.\n  - $\\lambda = 100.0$.\n  - $\\rho = 1.0$.\n- Case C (rank-deficient design, tests numerical stability via the $\\rho I$ term):\n  - $X = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\\\ 3 & 6 \\end{bmatrix}$.\n  - $y = [1.0, 0.0, 1.0]^{\\top}$.\n  - $\\lambda = 0.1$.\n  - $\\rho = 1.0$.\n- Case D (tiny regularization, near least-squares):\n  - $X = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\\\ 2 & 1 \\end{bmatrix}$.\n  - $y = [1.0, 2.0, 2.5, 4.0]^{\\top}$.\n  - $\\lambda = 10^{-6}$.\n  - $\\rho = 1.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the $4$ rounded objective values in the order A, B, C, D, as a comma-separated list enclosed in square brackets (for example, $[v_{A},v_{B},v_{C},v_{D}]$). No additional text should be printed. No physical units or angle units are involved, and all numeric answers are unitless real numbers. The only acceptable outputs are real numbers rounded to $6$ decimal places.",
            "solution": "The user-provided problem is a valid request to derive and implement the Alternating Direction Method of Multipliers (ADMM) for the Lasso optimization problem. All definitions, constraints, and data are scientifically grounded, internally consistent, and well-posed. The task falls squarely within the domain of convex optimization.\n\nThe Lasso optimization problem is formulated with a splitting variable $z$ as:\n$$\n\\text{minimize} \\quad f(w) + g(z) \\quad \\text{subject to} \\quad w - z = 0\n$$\nwhere $f(w) = \\frac{1}{2}\\lVert y - X w \\rVert_{2}^{2}$ and $g(z) = \\lambda \\lVert z \\rVert_{1}$. This is an equality-constrained convex optimization problem.\n\nThe ADMM algorithm solves this by minimizing the augmented Lagrangian, which incorporates the objective function and the equality constraint. For a general problem of the form $\\min_{w,z} f(w) + g(z)$ subject to $Aw + Bz = c$, the augmented Lagrangian is:\n$$\nL_{\\rho}(w, z, \\nu) = f(w) + g(z) + \\nu^\\top(Aw + Bz - c) + \\frac{\\rho}{2}\\lVert Aw + Bz - c \\rVert_{2}^{2}\n$$\nwhere $\\nu$ is the dual variable and $\\rho > 0$ is the penalty parameter.\n\nFor our specific problem, $A = I$, $B = -I$ (where $I$ is the identity matrix of appropriate dimension), and $c = 0$. Substituting these into the general form gives:\n$$\nL_{\\rho}(w, z, \\nu) = \\frac{1}{2}\\lVert y - Xw \\rVert_{2}^{2} + \\lambda \\lVert z \\rVert_{1} + \\nu^\\top(w - z) + \\frac{\\rho}{2}\\lVert w - z \\rVert_{2}^{2}\n$$\n\nThe ADMM algorithm operates in a scaled form where the dual variable is scaled by $\\rho$. Let $u = (1/\\rho)\\nu$, so $\\nu = \\rho u$. The term $\\nu^\\top(w-z) + \\frac{\\rho}{2}\\lVert w-z \\rVert_2^2$ can be rewritten by completing the square:\n$$\n\\rho u^\\top(w-z) + \\frac{\\rho}{2}\\lVert w-z \\rVert_2^2 = \\frac{\\rho}{2} (2u^\\top(w-z) + \\lVert w-z \\rVert_2^2) = \\frac{\\rho}{2}(\\lVert w - z + u \\rVert_{2}^{2} - \\lVert u \\rVert_{2}^{2})\n$$\nThe scaled augmented Lagrangian, ignoring terms constant with respect to $w$ and $z$, is:\n$$\nL_{\\rho}(w, z, u) = \\frac{1}{2}\\lVert y - Xw \\rVert_{2}^{2} + \\lambda \\lVert z \\rVert_{1} + \\frac{\\rho}{2}\\lVert w - z + u \\rVert_{2}^{2}\n$$\n\nADMM is an iterative method that, at each iteration $k+1$, performs three updates in sequence:\n1. Minimizes $L_{\\rho}$ with respect to $w$, keeping $z$ and $u$ fixed at their values from iteration $k$.\n2. Minimizes $L_{\\rho}$ with respect to $z$, using the newly computed $w^{k+1}$ and fixed $u^k$.\n3. Updates the dual variable $u$.\n\nThe explicit derivation for each update step is as follows:\n\n**1. The $w$-update**\n\nThe update for $w$ is found by solving the unconstrained minimization problem:\n$$\nw^{k+1} = \\arg\\min_{w} \\left( \\frac{1}{2}\\lVert y - Xw \\rVert_{2}^{2} + \\frac{\\rho}{2}\\lVert w - z^{k} + u^{k} \\rVert_{2}^{2} \\right)\n$$\nThe objective function is quadratic in $w$ and differentiable. The minimum is found by setting its gradient with respect to $w$ to zero. Let the objective be $J(w)$.\n$$\n\\nabla_{w} J(w) = \\nabla_{w} \\left( \\frac{1}{2}(y - Xw)^\\top(y - Xw) + \\frac{\\rho}{2}(w - (z^k-u^k))^\\top(w - (z^k-u^k)) \\right)\n$$\n$$\n\\nabla_{w} J(w) = -X^\\top(y - Xw) + \\rho(w - (z^k - u^k))\n$$\nSetting $\\nabla_{w} J(w) = 0$:\n$$\n-X^\\top y + X^\\top X w + \\rho w - \\rho(z^k - u^k) = 0\n$$\n$$\n(X^\\top X + \\rho I) w = X^\\top y + \\rho(z^k - u^k)\n$$\nThe matrix $(X^\\top X + \\rho I)$ is positive definite and thus invertible for any $\\rho > 0$, because $X^\\top X$ is positive semi-definite. The update for $w$ is the solution to this linear system:\n$$\nw^{k+1} = (X^\\top X + \\rho I)^{-1} (X^\\top y + \\rho(z^k - u^k))\n$$\n\n**2. The $z$-update**\n\nThe update for $z$ is found by solving:\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\lambda \\lVert z \\rVert_{1} + \\frac{\\rho}{2}\\lVert w^{k+1} - z + u^{k} \\rVert_{2}^{2} \\right)\n$$\nWe can rewrite the quadratic term as $\\frac{\\rho}{2}\\lVert z - (w^{k+1} + u^{k}) \\rVert_{2}^{2}$. The problem becomes:\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\lambda \\lVert z \\rVert_{1} + \\frac{\\rho}{2}\\lVert z - (w^{k+1} + u^{k}) \\rVert_{2}^{2} \\right)\n$$\nThis is the definition of the proximal operator for the scaled $\\ell_1$-norm function, $h(z) = \\lambda \\lVert z \\rVert_{1}$, evaluated at the point $v = w^{k+1} + u^{k}$ with scaling parameter $1/\\rho$:\n$$\nz^{k+1} = \\text{prox}_{\\lambda\\lVert \\cdot \\rVert_1, 1/\\rho}(w^{k+1} + u^k)\n$$\nThe solution to this is given by the soft-thresholding operator, $S_{\\kappa}(a)$, defined element-wise as $S_{\\kappa}(a_i) = \\text{sign}(a_i)\\max(|a_i|-\\kappa, 0)$. In our case, the threshold is $\\kappa = \\lambda/\\rho$. The update is:\n$$\nz^{k+1} = S_{\\lambda/\\rho}(w^{k+1} + u^k)\n$$\n\n**3. The $u$-update**\n\nThe update for the scaled dual variable is:\n$$\nu^{k+1} = u^k + r^{k+1}\n$$\nwhere $r^{k+1} = w^{k+1} - z^{k+1}$ is the primal residual at iteration $k+1$. This update rule ensures that if the primal residual is non-zero, the dual variable is adjusted to enforce the constraint more strongly in subsequent iterations.\n\n**Stopping Criteria**\n\nThe algorithm terminates when the norms of the primal and dual residuals are below specified tolerances. At iteration $k$:\n- The primal residual is $r^k = w^k - z^k$.\n- The dual residual is $s^k = \\rho (z^k - z^{k-1})$.\n- The tolerances are $\\varepsilon_{\\mathrm{pri}} = \\sqrt{p}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\{\\lVert w^{k} \\rVert_{2}, \\lVert z^{k} \\rVert_{2}\\}$ and $\\varepsilon_{\\mathrm{dual}} = \\sqrt{p}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\rho \\lVert u^{k} \\rVert_{2}$.\n- The loop terminates when both $\\lVert r^{k} \\rVert_{2} \\le \\varepsilon_{\\mathrm{pri}}$ and $\\lVert s^{k} \\rVert_{2} \\le \\varepsilon_{\\mathrm{dual}}$.\n\nThe numerical implementation will use these derived updates and stopping criteria. For the $w$-update, the linear system is solved efficiently using a pre-computed Cholesky factorization of $(X^\\top X + \\rho I)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Sets up and runs the ADMM for Lasso on the four test cases specified\n    in the problem statement.\n    \"\"\"\n\n    def solve_admm(X, y, lambda_, rho, eps_abs=1e-6, eps_rel=1e-4, max_iter=2000):\n        \"\"\"\n        Solves the Lasso problem using ADMM.\n\n        The problem is to minimize:\n        0.5 * ||y - Xw||_2^2 + lambda * ||w||_1\n\n        Args:\n            X (np.ndarray): Data matrix (n x p).\n            y (np.ndarray): Response vector (n,).\n            lambda_ (float): L1 regularization parameter.\n            rho (float): ADMM penalty parameter.\n            eps_abs (float): Absolute tolerance for stopping criteria.\n            eps_rel (float): Relative tolerance for stopping criteria.\n            max_iter (int): Maximum number of iterations.\n\n        Returns:\n            np.ndarray: The converged coefficient vector w.\n        \"\"\"\n        n, p = X.shape\n\n        # Initialize variables\n        w = np.zeros(p)\n        z = np.zeros(p)\n        u = np.zeros(p)\n\n        # Pre-computation for w-update\n        X_T_X = X.T @ X\n        X_T_y = X.T @ y\n        # Use Cholesky factorization to solve the linear system in the w-update efficiently\n        # (X^T X + rho * I)w = X^T y + rho * (z - u)\n        L_and_lower = linalg.cho_factor(X_T_X + rho * np.eye(p))\n\n        for k in range(max_iter):\n            # Store z from previous iteration for dual residual calculation\n            z_prev = np.copy(z)\n\n            # 1. w-update\n            rhs = X_T_y + rho * (z - u)\n            w = linalg.cho_solve(L_and_lower, rhs)\n\n            # 2. z-update (soft-thresholding)\n            z_tilde = w + u\n            threshold = lambda_ / rho\n            z = np.sign(z_tilde) * np.maximum(np.abs(z_tilde) - threshold, 0)\n            \n            # 3. u-update\n            u = u + w - z\n\n            # Stopping criteria check\n            # Primal residual\n            r_k = w - z\n            norm_r = np.linalg.norm(r_k)\n            \n            # Dual residual\n            s_k = rho * (z - z_prev)\n            norm_s = np.linalg.norm(s_k)\n\n            # Tolerances\n            eps_pri = np.sqrt(p) * eps_abs + eps_rel * np.maximum(np.linalg.norm(w), np.linalg.norm(z))\n            eps_dual = np.sqrt(p) * eps_abs + eps_rel * rho * np.linalg.norm(u)\n\n            if norm_r <= eps_pri and norm_s <= eps_dual:\n                break\n        \n        return w\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Orthonormal design\n        {\n            'X': np.eye(3),\n            'y': np.array([3.0, -1.0, 0.2]),\n            'lambda': 1.0,\n            'rho': 1.0\n        },\n        # Case B: Very large regularization\n        {\n            'X': np.array([[1, 0, 2], [0, 1, -1], [1, 1, 1], [2, -1, 0]]),\n            'y': np.array([1.0, -2.0, 0.5, 3.0]),\n            'lambda': 100.0,\n            'rho': 1.0\n        },\n        # Case C: Rank-deficient design\n        {\n            'X': np.array([[1, 2], [2, 4], [3, 6]]),\n            'y': np.array([1.0, 0.0, 1.0]),\n            'lambda': 0.1,\n            'rho': 1.0\n        },\n        # Case D: Tiny regularization\n        {\n            'X': np.array([[1, 0], [0, 1], [1, 1], [2, 1]]),\n            'y': np.array([1.0, 2.0, 2.5, 4.0]),\n            'lambda': 1e-6,\n            'rho': 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X, y, lambda_, rho = case['X'], case['y'], case['lambda'], case['rho']\n        \n        w_final = solve_admm(X, y, lambda_, rho)\n        \n        # Calculate the final objective value F(w)\n        objective_value = 0.5 * np.linalg.norm(y - X @ w_final)**2 + lambda_ * np.linalg.norm(w_final, 1)\n        \n        results.append(round(objective_value, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}