{
    "hands_on_practices": [
        {
            "introduction": "Before diving into complex algorithms, it's crucial to understand *why* regularization is so effective. This first practice problem  provides a clear, quantitative look at the celebrated bias-variance trade-off using Ridge regression. By working through a simplified scenario with an orthogonal design matrix, you will derive the precise impact of the regularization parameter $\\lambda$ on the model's expected error, revealing how a small amount of bias can lead to a significant reduction in variance and improved overall performance.",
            "id": "3172142",
            "problem": "Consider the linear model with Gaussian noise: $y = X \\beta + \\varepsilon$, where $X \\in \\mathbb{R}^{5 \\times 5}$ is an orthogonal matrix so that $X^{\\top} X = I_5$ and $X X^{\\top} = I_5$, the noise vector $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_5)$ with $\\sigma^{2} = 1$, and the true parameter vector $\\beta \\in \\mathbb{R}^{5}$ satisfies $\\|\\beta\\|_{2}^{2} = 100$. Consider the ridge-regularized estimator $\\hat{\\beta}_{\\lambda}$ defined as the minimizer of the objective function $\\|y - X b\\|_{2}^{2} + \\lambda \\|b\\|_{2}^{2}$ over $b \\in \\mathbb{R}^{5}$, with $\\lambda \\ge 0$. Let the performance criterion be the expected parameter mean squared error (mean squared error (MSE)) defined by $\\mathbb{E}\\left[\\|\\hat{\\beta}_{\\lambda} - \\beta\\|_{2}^{2}\\right]$.\n\nStarting only from the definitions of the ridge estimator, properties of orthogonal matrices, and the basic properties of expectation and variance of multivariate normal random vectors, derive an explicit expression for $\\mathbb{E}\\left[\\|\\hat{\\beta}_{\\lambda} - \\beta\\|_{2}^{2}\\right]$ as a function of $\\lambda$, and then determine the smallest positive value of $\\lambda$ for which this expected MSE equals that of the unregularized estimator (ordinary least squares (OLS), which corresponds to $\\lambda = 0$). This value quantifies when the bias induced by ridge begins to outweigh its variance reduction in this setting with large $\\|\\beta\\|_{2}$.\n\nProvide the smallest positive $\\lambda$ as your final answer. Round your answer to four significant figures.",
            "solution": "We begin from the definition of the ridge estimator as the minimizer of the strictly convex function $b \\mapsto \\|y - X b\\|_{2}^{2} + \\lambda \\|b\\|_{2}^{2}$. The first-order optimality condition yields the closed-form solution\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I)^{-1} X^{\\top} y.\n$$\nSubstituting the linear model $y = X \\beta + \\varepsilon$ gives\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I)^{-1} X^{\\top} (X \\beta + \\varepsilon) = (X^{\\top} X + \\lambda I)^{-1} (X^{\\top} X \\beta + X^{\\top} \\varepsilon).\n$$\nUsing $X^{\\top} X = I_5$, we obtain\n$$\n\\hat{\\beta}_{\\lambda} = (I_5 + \\lambda I_5)^{-1} (\\beta + X^{\\top} \\varepsilon) = \\frac{1}{1+\\lambda} \\beta + \\frac{1}{1+\\lambda} X^{\\top} \\varepsilon.\n$$\nHence the estimation error decomposes as\n$$\n\\hat{\\beta}_{\\lambda} - \\beta = \\left(\\frac{1}{1+\\lambda} - 1\\right) \\beta + \\frac{1}{1+\\lambda} X^{\\top} \\varepsilon = -\\frac{\\lambda}{1+\\lambda}\\,\\beta + \\frac{1}{1+\\lambda} X^{\\top} \\varepsilon.\n$$\nWe compute the mean and covariance of $\\hat{\\beta}_{\\lambda} - \\beta$. Since $\\mathbb{E}[\\varepsilon] = 0$ and $X$ is deterministic,\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\lambda} - \\beta] = -\\frac{\\lambda}{1+\\lambda}\\,\\beta,\n$$\nso the bias vector is $-\\frac{\\lambda}{1+\\lambda}\\,\\beta$. The covariance of $\\hat{\\beta}_{\\lambda}$ is\n$$\n\\operatorname{Var}(\\hat{\\beta}_{\\lambda}) = \\operatorname{Var}\\!\\left(\\frac{1}{1+\\lambda} X^{\\top} \\varepsilon\\right) = \\frac{1}{(1+\\lambda)^{2}} \\operatorname{Var}(X^{\\top} \\varepsilon).\n$$\nSince $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_5)$ and $X$ is orthogonal, $X^{\\top} \\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} X^{\\top} X) = \\mathcal{N}(0, \\sigma^{2} I_5)$. Therefore,\n$$\n\\operatorname{Var}(\\hat{\\beta}_{\\lambda}) = \\frac{\\sigma^{2}}{(1+\\lambda)^{2}} I_5.\n$$\nBy the bias-variance decomposition for mean squared error,\n$$\n\\mathbb{E}\\left[\\|\\hat{\\beta}_{\\lambda} - \\beta\\|_{2}^{2}\\right] = \\|\\mathbb{E}[\\hat{\\beta}_{\\lambda}] - \\beta\\|_{2}^{2} + \\operatorname{tr}\\left(\\operatorname{Var}(\\hat{\\beta}_{\\lambda})\\right).\n$$\nUsing the expressions above,\n$$\n\\|\\mathbb{E}[\\hat{\\beta}_{\\lambda}] - \\beta\\|_{2}^{2} = \\left\\|-\\frac{\\lambda}{1+\\lambda}\\,\\beta\\right\\|_{2}^{2} = \\frac{\\lambda^{2}}{(1+\\lambda)^{2}} \\|\\beta\\|_{2}^{2},\n$$\nand\n$$\n\\operatorname{tr}\\left(\\operatorname{Var}(\\hat{\\beta}_{\\lambda})\\right) = \\operatorname{tr}\\left(\\frac{\\sigma^{2}}{(1+\\lambda)^{2}} I_5\\right) = \\frac{5 \\sigma^{2}}{(1+\\lambda)^{2}}.\n$$\nHence the expected parameter mean squared error as a function of $\\lambda$ is\n$$\n\\mathbb{E}\\left[\\|\\hat{\\beta}_{\\lambda} - \\beta\\|_{2}^{2}\\right] = \\frac{\\lambda^{2} \\|\\beta\\|_{2}^{2} + 5 \\sigma^{2}}{(1+\\lambda)^{2}}.\n$$\nFor the unregularized estimator (ordinary least squares (OLS)), $\\lambda = 0$, so\n$$\n\\mathbb{E}\\left[\\|\\hat{\\beta}_{0} - \\beta\\|_{2}^{2}\\right] = 5 \\sigma^{2}.\n$$\nWe are asked for the smallest positive $\\lambda$ such that\n$$\n\\frac{\\lambda^{2} \\|\\beta\\|_{2}^{2} + 5 \\sigma^{2}}{(1+\\lambda)^{2}} = 5 \\sigma^{2}.\n$$\nClearing denominators,\n$$\n\\lambda^{2} \\|\\beta\\|_{2}^{2} + 5 \\sigma^{2} = 5 \\sigma^{2} (1 + 2 \\lambda + \\lambda^{2}) = 5 \\sigma^{2} + 10 \\sigma^{2} \\lambda + 5 \\sigma^{2} \\lambda^{2}.\n$$\nCanceling $5 \\sigma^{2}$ on both sides and rearranging,\n$$\n\\lambda^{2} \\|\\beta\\|_{2}^{2} = 10 \\sigma^{2} \\lambda + 5 \\sigma^{2} \\lambda^{2},\n$$\n$$\n\\lambda^{2} \\left(\\|\\beta\\|_{2}^{2} - 5 \\sigma^{2}\\right) = 10 \\sigma^{2} \\lambda.\n$$\nAssuming $\\|\\beta\\|_{2}^{2} > 5 \\sigma^{2}$ (the given data satisfy this), divide by $\\lambda > 0$ to get\n$$\n\\lambda \\left(\\|\\beta\\|_{2}^{2} - 5 \\sigma^{2}\\right) = 10 \\sigma^{2},\n$$\nso the smallest positive solution is\n$$\n\\lambda^{\\star} = \\frac{10 \\sigma^{2}}{\\|\\beta\\|_{2}^{2} - 5 \\sigma^{2}}.\n$$\nWith $\\sigma^{2} = 1$ and $\\|\\beta\\|_{2}^{2} = 100$, this becomes\n$$\n\\lambda^{\\star} = \\frac{10 \\cdot 1}{100 - 5 \\cdot 1} = \\frac{10}{95}.\n$$\nNumerically, $\\frac{10}{95} \\approx 0.105263\\dots$. Rounding to four significant figures gives $0.1053$.\n\nThis value marks the point where the expected mean squared error of ridge returns to the unregularized level; for any larger $\\lambda$, the expected mean squared error exceeds that of OLS, demonstrating that the bias induced by ridge can dominate the variance reduction when the signal $\\|\\beta\\|_{2}$ is large.",
            "answer": "$$\\boxed{0.1053}$$"
        },
        {
            "introduction": "Having established the motivation for regularization, we now turn to solving the optimization problems it creates. This exercise  introduces coordinate descent, an intuitive yet powerful algorithm for tackling the non-differentiable $\\ell_1$ norm in the LASSO objective. You will derive the update rule for a single variable, revealing its connection to the soft-thresholding operator, which is the mechanism responsible for driving coefficients to exactly zero and producing sparse models.",
            "id": "3172091",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem, which minimizes the objective function\n$$\nF(w) \\;=\\; \\frac{1}{2}\\,\\|y - X w\\|_{2}^{2} \\;+\\; \\lambda\\,\\|w\\|_{1},\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is a fixed data matrix, $y \\in \\mathbb{R}^{n}$ is a fixed response vector, $w \\in \\mathbb{R}^{p}$ is the decision variable, and $\\lambda > 0$ is a regularization parameter. Let $X_{j} \\in \\mathbb{R}^{n}$ denote the $j$-th column of $X$ and $w_{j}$ the $j$-th component of $w$. Define the soft-thresholding operator $S_{\\alpha}(z)$ by\n$$\nS_{\\alpha}(z) \\;=\\; \\operatorname{sign}(z)\\,\\max\\big(|z| - \\alpha,\\,0\\big),\n$$\nfor $\\alpha \\ge 0$ and $z \\in \\mathbb{R}$.\n\nStarting from the definition of $F(w)$ and the subgradient of the absolute value function, derive the exact coordinate-wise minimizer for $w_{j}$ while holding all other coordinates fixed. Your derivation must start from the one-dimensional reduction obtained by isolating the contribution of coordinate $j$ and use only first principles (convexity and subgradient optimality for the $\\ell_{1}$ norm). Then, using your derived update, compute a concrete coordinate descent update for the following instance:\n$$\nX \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 \\\\\n0 & 1 & 3\n\\end{pmatrix},\\qquad\ny \\;=\\; \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix},\\qquad\nw \\;=\\; \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.1 \\end{pmatrix},\\qquad\n\\lambda \\;=\\; 1.2,\n$$\nfor coordinate $j = 2$. Provide the updated numerical value of $w_{2}$ after one exact coordinate-wise minimization step.\n\nFinally, briefly explain, based on the structure of $F(w)$ and properties of coordinate descent, why cyclic coordinate descent enjoys global convergence for this convex problem and why sparsity in $X$ can accelerate convergence in practice. Your explanation should rely on fundamental properties (convexity, separability of the $\\ell_{1}$ penalty, and coordinate-wise Lipschitz continuity of the smooth part) without invoking unproven claims or shortcut formulas.\n\nExpress the final numerical answer for the updated $w_{2}$ as a single real number. If rounding is necessary, round to four significant figures; otherwise, provide the exact value.",
            "solution": "**Part 1: Derivation of the Coordinate-wise Minimizer**\n\nThe LASSO objective function is given by\n$$\nF(w) = \\frac{1}{2}\\|y - X w\\|_{2}^{2} + \\lambda\\|w\\|_{1}\n$$\nwhere $w \\in \\mathbb{R}^{p}$. We wish to find the minimizer of $F(w)$ with respect to a single coordinate $w_j$, while all other coordinates $w_k$ for $k \\neq j$ are held fixed.\n\nWe can aportion the function $F(w)$ into terms that depend on $w_j$ and terms that are constant with respect to $w_j$. The smooth term is $\\|y - X w\\|_{2}^{2}$. We can separate the contribution of the $j$-th column of $X$ as follows:\n$$\nXw = \\sum_{k=1}^{p} X_k w_k = X_j w_j + \\sum_{k \\neq j} X_k w_k\n$$\nLet us define the partial residual vector $r_{(-j)} \\in \\mathbb{R}^{n}$ as\n$$\nr_{(-j)} = y - \\sum_{k \\neq j} X_k w_k\n$$\nSubstituting this into the objective function, we get\n$$\nF(w) = \\frac{1}{2}\\|r_{(-j)} - X_j w_j\\|_{2}^{2} + \\lambda \\sum_{k=1}^{p} |w_k| = \\frac{1}{2}\\|r_{(-j)} - X_j w_j\\|_{2}^{2} + \\lambda |w_j| + \\lambda \\sum_{k \\neq j} |w_k|\n$$\nTo minimize $F(w)$ with respect to $w_j$, we can ignore all terms not depending on $w_j$. This gives us a one-dimensional optimization problem:\n$$\n\\min_{w_j \\in \\mathbb{R}} f_j(w_j) \\quad \\text{where} \\quad f_j(w_j) = \\frac{1}{2}\\|r_{(-j)} - X_j w_j\\|_{2}^{2} + \\lambda|w_j|\n$$\nLet us expand the squared $\\ell_2$-norm:\n$$\n\\|r_{(-j)} - X_j w_j\\|_{2}^{2} = (r_{(-j)} - X_j w_j)^T(r_{(-j)} - X_j w_j) = \\|r_{(-j)}\\|_2^2 - 2 w_j X_j^T r_{(-j)} + w_j^2 \\|X_j\\|_2^2\n$$\nThe function $f_j(w_j)$ can be written, up to an additive constant, as\n$$\nf_j(w_j) = \\frac{1}{2}\\|X_j\\|_2^2 w_j^2 - (X_j^T r_{(-j)}) w_j + \\lambda|w_j|\n$$\nThis function is convex, being the sum of a quadratic function (which is convex, assuming $X_j \\neq 0$) and the absolute value function multiplied by $\\lambda > 0$ (which is convex). A point $w_j^*$ is a global minimizer of $f_j(w_j)$ if and only if $0$ belongs to the subdifferential of $f_j$ at $w_j^*$, denoted $\\partial f_j(w_j^*)$.\n\nThe subdifferential of $f_j(w_j)$ is given by:\n$$\n\\partial f_j(w_j) = \\|X_j\\|_2^2 w_j - X_j^T r_{(-j)} + \\lambda \\partial|w_j|\n$$\nwhere the subdifferential of the absolute value function is\n$$\n\\partial|z| = \\begin{cases} \\{\\operatorname{sign}(z)\\} & \\text{if } z \\neq 0 \\\\ [-1, 1] & \\text{if } z = 0 \\end{cases}\n$$\nThe optimality condition $0 \\in \\partial f_j(w_j^*)$ thus becomes:\n$$\nX_j^T r_{(-j)} - \\|X_j\\|_2^2 w_j^* \\in \\lambda \\partial|w_j^*|\n$$\nWe analyze this condition by cases for the value of $w_j^*$:\n1.  If $w_j^* > 0$, then $\\partial|w_j^*| = \\{1\\}$. The condition becomes $X_j^T r_{(-j)} - \\|X_j\\|_2^2 w_j^* = \\lambda$, which gives $w_j^* = \\frac{X_j^T r_{(-j)} - \\lambda}{\\|X_j\\|_2^2}$. For this solution to be consistent with the initial assumption $w_j^* > 0$, we must have $X_j^T r_{(-j)} > \\lambda$.\n2.  If $w_j^* < 0$, then $\\partial|w_j^*| = \\{-1\\}$. The condition becomes $X_j^T r_{(-j)} - \\|X_j\\|_2^2 w_j^* = -\\lambda$, which gives $w_j^* = \\frac{X_j^T r_{(-j)} + \\lambda}{\\|X_j\\|_2^2}$. For consistency with $w_j^* < 0$, we must have $X_j^T r_{(-j)} < -\\lambda$.\n3.  If $w_j^* = 0$, then $\\partial|w_j^*| = [-1, 1]$. The condition becomes $X_j^T r_{(-j)} \\in [-\\lambda, \\lambda]$, or $|X_j^T r_{(-j)}| \\le \\lambda$.\n\nLet's consolidate these results. Let $z = X_j^T r_{(-j)}$ and $\\alpha = \\|X_j\\|_2^2$. The optimal $w_j^*$ is:\n$$\nw_j^* = \\begin{cases} (z - \\lambda)/\\alpha & \\text{if } z > \\lambda \\\\ (z + \\lambda)/\\alpha & \\text{if } z < -\\lambda \\\\ 0 & \\text{if } |z| \\le \\lambda \\end{cases}\n$$\nThis expression is precisely the soft-thresholding operator $S_{\\lambda/\\alpha}(z/\\alpha)$. Let's verify:\n$$\nS_{\\lambda/\\alpha}\\left(\\frac{z}{\\alpha}\\right) = \\operatorname{sign}\\left(\\frac{z}{\\alpha}\\right) \\max\\left(\\left|\\frac{z}{\\alpha}\\right| - \\frac{\\lambda}{\\alpha}, 0\\right) = \\frac{1}{\\alpha} \\operatorname{sign}(z) \\max\\left(|z| - \\lambda, 0\\right)\n$$\n- If $z > \\lambda$, the expression gives $\\frac{1}{\\alpha}(1)(z - \\lambda) = (z-\\lambda)/\\alpha$.\n- If $z < -\\lambda$, the expression gives $\\frac{1}{\\alpha}(-1)(-z - \\lambda) = (z+\\lambda)/\\alpha$.\n- If $|z| \\le \\lambda$, the expression gives $\\frac{1}{\\alpha}\\operatorname{sign}(z)(0) = 0$.\nThe derivation is complete. The exact coordinate-wise minimizer for $w_j$ is given by $w_j^* = S_{\\lambda/\\|X_j\\|_2^2}\\left(\\frac{X_j^T r_{(-j)}}{\\|X_j\\|_2^2}\\right)$.\n\n**Part 2: Numerical Calculation**\n\nWe are given the following instance:\n$$\nX = \\begin{pmatrix} 2 & -1 & 0 \\\\ 0 & 1 & 3 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}, \\quad w_{\\text{initial}} = \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.1 \\end{pmatrix}, \\quad \\lambda = 1.2\n$$\nWe need to update the coordinate $j=2$, which corresponds to $w_2$. The other coordinates are fixed at their current values: $w_1 = 0.5$ and $w_3 = 0.1$.\n\nFirst, we identify the relevant column of $X$: $X_2 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\nThe squared norm of this column is $\\|X_2\\|_2^2 = (-1)^2 + 1^2 = 2$.\n\nNext, we calculate the partial residual $r_{(-2)}$:\n$$\nr_{(-2)} = y - X_1 w_1 - X_3 w_3 = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}(0.5) - \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}(0.1)\n$$\n$$\nr_{(-2)} = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 1 - 1 - 0 \\\\ 4 - 0 - 0.3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3.7 \\end{pmatrix}\n$$\nNow, we compute the term $X_2^T r_{(-2)}$:\n$$\nX_2^T r_{(-2)} = \\begin{pmatrix} -1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3.7 \\end{pmatrix} = (-1)(0) + (1)(3.7) = 3.7\n$$\nWe can now apply the soft-thresholding operator. The updated value, $w_2^{\\text{new}}$, is\n$$\nw_2^{\\text{new}} = S_{\\lambda/\\|X_2\\|_2^2}\\left(\\frac{X_2^T r_{(-2)}}{\\|X_2\\|_2^2}\\right)\n$$\nThe threshold parameter is $\\lambda/\\|X_2\\|_2^2 = 1.2/2 = 0.6$. The argument to the operator is $X_2^T r_{(-2)}/\\|X_2\\|_2^2 = 3.7/2 = 1.85$.\nSo, we compute:\n$$\nw_2^{\\text{new}} = S_{0.6}(1.85)\n$$\nUsing the definition $S_{\\alpha}(z) = \\operatorname{sign}(z) \\max(|z|-\\alpha, 0)$:\n$$\nw_2^{\\text{new}} = \\operatorname{sign}(1.85) \\max(|1.85| - 0.6, 0) = 1 \\cdot \\max(1.85 - 0.6, 0) = \\max(1.25, 0) = 1.25\n$$\nThe updated value for $w_2$ is exactly $1.25$.\n\n**Part 3: Convergence Explanation**\n\nThe global convergence of cyclic coordinate descent for the LASSO problem rests on the specific structure of the objective function $F(w)$.\nThe function $F(w) = \\frac{1}{2}\\|y - Xw\\|_2^2 + \\lambda \\|w\\|_1$ is a sum of two components:\n1.  A smooth, differentiable, convex function $f(w) = \\frac{1}{2}\\|y - Xw\\|_2^2$. Its gradient is $\\nabla f(w) = X^T(Xw - y)$. The gradient with respect to a single coordinate, $\\nabla_j f(w) = X_j^T(Xw - y)$, is Lipschitz continuous with a constant $L_j = \\|X_j\\|_2^2$.\n2.  A non-smooth, convex function $g(w) = \\lambda \\|w\\|_1$. Crucially, this function is **separable**, meaning it can be written as a sum of functions of individual coordinates: $g(w) = \\sum_{j=1}^p \\lambda|w_j|$.\n\nThe combination of these properties—a convex objective function composed of a smooth part with coordinate-wise Lipschitz gradients and a separable non-smooth part—is sufficient to guarantee that cyclic coordinate descent converges to a global minimum. At each step, the algorithm performs an exact minimization over one coordinate. Because the function is convex, any local minimum is a global minimum. The sequence of iterates $\\{w^{(k)}\\}$ generated by coordinate descent is guaranteed to have limit points, and any such limit point is a global minimizer of $F(w)$. This is a classical result in non-smooth convex optimization theory.\n\nThe sparsity of the data matrix $X$ can significantly accelerate the convergence in terms of computation time. The core computation in each coordinate update for $w_j$ is the term $X_j^T r_{(-j)}$. This can be calculated as $X_j^T y - \\sum_{k \\neq j} (X_j^T X_k) w_k$. If $X$ is sparse, many of its columns $X_k$ will be sparse vectors. The inner product $X_j^T X_k$ can be computed much faster, as the number of required multiplications is proportional to the number of positions where both vectors have non-zero elements, not their full dimension $n$. In many cases, this inner product will be exactly zero. This reduces the cost of the summation. Alternatively, if one maintains the full residual $r=y-Xw$, the update $w_j \\to w_j^{\\text{new}}$ requires an update to the residual $r \\to r - X_j (w_j^{\\text{new}} - w_j^{\\text{old}})$. If the vector $X_j$ is sparse (has few non-zero entries), this update operation is computationally inexpensive. Thus, each step of coordinate descent completes faster, leading to a faster overall time to convergence.",
            "answer": "$$\n\\boxed{1.25}\n$$"
        },
        {
            "introduction": "We can generalize the soft-thresholding operator from the previous exercise to a more powerful tool: the proximal operator. This final practice  guides you through deriving the proximal operator for a weighted $\\ell_1$ norm, a fundamental building block in modern optimization. You will then see how this tool can be used within an advanced algorithm like Iterative Reweighted $\\ell_1$ (IRL1) to approximate non-convex penalties, providing a glimpse into how optimizers can solve problems far more complex than the standard LASSO.",
            "id": "3172027",
            "problem": "Let $g:\\mathbb{R}^{n}\\to\\mathbb{R}$ be defined by $g(\\mathbf{w})=\\sum_{i=1}^{n}\\lambda_{i}\\,|w_{i}|$ with $\\lambda_{i}>0$ for all $i$. The proximal operator of a proper, closed, convex function $g$ at a point $\\mathbf{a}\\in\\mathbb{R}^{n}$ is defined by $\\operatorname{prox}_{g}(\\mathbf{a})=\\arg\\min_{\\mathbf{w}\\in\\mathbb{R}^{n}}\\left\\{g(\\mathbf{w})+\\frac{1}{2}\\|\\mathbf{w}-\\mathbf{a}\\|_{2}^{2}\\right\\}$. Starting from this definition and first principles of convex analysis (including separability and subdifferential optimality conditions), derive the componentwise form of $\\operatorname{prox}_{g}(\\mathbf{a})$ when $g(\\mathbf{w})=\\sum_{i=1}^{n}\\lambda_{i}|w_{i}|$.\n\nNext, consider the cardinality surrogate $h(\\mathbf{w})=\\gamma\\sum_{i=1}^{n}\\ln\\!\\big(|w_{i}|+\\epsilon\\big)$ with $\\gamma>0$ and $\\epsilon>0$, which is concave in the nonnegative variables $|w_{i}|$. Using the fact that a concave function is upper-bounded by its first-order Taylor expansion at any point, construct a majorization of $h(\\mathbf{w})$ at $\\mathbf{u}\\in\\mathbb{R}^{n}$ in terms of the variables $|w_{i}|$, and from this majorization derive a single iteration of the Iterative Reweighted $\\ell_{1}$ (IRL1) method that replaces $h(\\mathbf{w})$ by a weighted $\\ell_{1}$ penalty $\\sum_{i}\\lambda_{i}^{(t)}|w_{i}|$. Express the weights $\\lambda_{i}^{(t)}$ in terms of $\\gamma$, $\\epsilon$, and the current iterate $\\mathbf{w}^{(t)}$.\n\nFinally, let $n=4$, $\\mathbf{a}=(2.1,-0.4,0.05,-3.0)$, initial iterate $\\mathbf{w}^{(0)}=(0.5,0,0,4.0)$, and parameters $\\gamma=0.8$, $\\epsilon=0.1$. Using your derived expressions, compute the next IRL1 proximal update $\\mathbf{w}^{(1)}=\\operatorname{prox}_{\\sum_{i}\\lambda_{i}^{(0)}|w_{i}|}(\\mathbf{a})$, where $\\lambda_{i}^{(0)}$ are obtained from $\\mathbf{w}^{(0)}$. Round each entry of $\\mathbf{w}^{(1)}$ to four significant figures. Provide your final answer as the vector $\\mathbf{w}^{(1)}$.",
            "solution": "**Part 1: Derivation of the Proximal Operator of the Weighted $\\ell_1$ norm**\n\nThe proximal operator of $g(\\mathbf{w}) = \\sum_{i=1}^{n}\\lambda_{i}|w_{i}|$ at a point $\\mathbf{a} \\in \\mathbb{R}^n$ is defined as the solution to the following optimization problem:\n$$\n\\operatorname{prox}_{g}(\\mathbf{a})=\\arg\\min_{\\mathbf{w}\\in\\mathbb{R}^{n}}\\left\\{ \\sum_{i=1}^{n}\\lambda_{i}|w_{i}|+\\frac{1}{2}\\|\\mathbf{w}-\\mathbf{a}\\|_{2}^{2} \\right\\}\n$$\nThe objective function is separable with respect to the components $w_i$. This means we can minimize it by minimizing each component's contribution independently:\n$$\n(\\operatorname{prox}_{g}(\\mathbf{a}))_i = \\arg\\min_{w_{i}\\in\\mathbb{R}}\\left\\{ \\lambda_{i}|w_{i}|+\\frac{1}{2}(w_{i}-a_{i})^{2} \\right\\}\n$$\nLet $F_i(w_i) = \\lambda_{i}|w_{i}|+\\frac{1}{2}(w_{i}-a_{i})^{2}$. According to the first-order optimality condition from convex analysis, the minimizer $w_i^*$ must satisfy $0 \\in \\partial F_i(w_i^*)$. The subdifferential of $F_i$ is given by:\n$$\n\\partial F_i(w_i) = \\partial(\\lambda_i|w_i|) + \\partial\\left(\\frac{1}{2}(w_i-a_i)^2\\right) = \\lambda_i \\partial|w_i| + (w_i - a_i)\n$$\nwhere the subdifferential of the absolute value function is:\n$$\n\\partial|w_i| = \n\\begin{cases} \n\\{1\\} & \\text{if } w_i > 0 \\\\\n\\{-1\\} & \\text{if } w_i < 0 \\\\\n[-1, 1] & \\text{if } w_i = 0 \n\\end{cases}\n$$\nWe analyze the optimality condition $0 \\in \\partial F_i(w_i^*)$ by cases:\n1.  If $w_i^* > 0$: The condition becomes $0 = \\lambda_i \\cdot 1 + (w_i^* - a_i)$, which gives $w_i^* = a_i - \\lambda_i$. This case is self-consistent only if $a_i - \\lambda_i > 0$, i.e., $a_i > \\lambda_i$.\n2.  If $w_i^* < 0$: The condition becomes $0 = \\lambda_i \\cdot (-1) + (w_i^* - a_i)$, which gives $w_i^* = a_i + \\lambda_i$. This case is self-consistent only if $a_i + \\lambda_i < 0$, i.e., $a_i < -\\lambda_i$.\n3.  If $w_i^* = 0$: The condition becomes $0 \\in \\lambda_i [-1, 1] + (0 - a_i)$, which simplifies to $a_i \\in [-\\lambda_i, \\lambda_i]$, or $|a_i| \\le \\lambda_i$.\n\nCombining these three cases gives the full component-wise solution:\n$$\n(\\operatorname{prox}_{g}(\\mathbf{a}))_i = \n\\begin{cases} \na_i - \\lambda_i & \\text{if } a_i > \\lambda_i \\\\\na_i + \\lambda_i & \\text{if } a_i < -\\lambda_i \\\\\n0 & \\text{if } |a_i| \\le \\lambda_i\n\\end{cases}\n$$\nThis is the well-known soft-thresholding operator, which can be written compactly as:\n$$\n(\\operatorname{prox}_{g}(\\mathbf{a}))_i = \\operatorname{sign}(a_i) \\max(0, |a_i| - \\lambda_i)\n$$\n\n**Part 2: Derivation of Iterative Reweighted $\\ell_1$ (IRL1) Weights**\n\nWe are given the concave surrogate for cardinality, $h(\\mathbf{w})=\\gamma\\sum_{i=1}^{n}\\ln\\!\\big(|w_{i}|+\\epsilon\\big)$. Let $f(x) = \\gamma \\ln(x+\\epsilon)$ where $x = |w_i| \\ge 0$. This function is concave since its second derivative $f''(x) = -\\gamma(x+\\epsilon)^{-2}$ is negative for $\\gamma > 0$ and $\\epsilon > 0$.\n\nA concave function is upper-bounded by its first-order Taylor expansion at any point. Let's expand $f(|w_i|)$ around the value from the previous iterate, $|w_i^{(t)}|$:\n$$\nf(|w_i|) \\le f(|w_i^{(t)}|) + f'(|w_i^{(t)}|) \\left(|w_i| - |w_i^{(t)}|\\right)\n$$\nThe derivative is $f'(x) = \\frac{\\gamma}{x+\\epsilon}$. Substituting this in, we get:\n$$\n\\gamma \\ln(|w_i|+\\epsilon) \\le \\gamma \\ln(|w_i^{(t)}|+\\epsilon) + \\frac{\\gamma}{|w_i^{(t)}|+\\epsilon} \\left(|w_i| - |w_i^{(t)}|\\right)\n$$\nSumming over all components $i$ gives a majorizer for $h(\\mathbf{w})$:\n$$\nh(\\mathbf{w}) \\le \\sum_{i=1}^n \\left( \\gamma \\ln(|w_i^{(t)}|+\\epsilon) + \\frac{\\gamma}{|w_i^{(t)}|+\\epsilon} \\left(|w_i| - |w_i^{(t)}|\\right) \\right)\n$$\nThe Iterative Reweighted $\\ell_1$ (IRL1) method replaces $h(\\mathbf{w})$ in the objective function with this majorizer. For the purpose of minimization with respect to $\\mathbf{w}$, terms that are constant in $\\mathbf{w}$ can be dropped. The part of the majorizer that depends on $\\mathbf{w}$ is:\n$$\n\\sum_{i=1}^n \\left( \\frac{\\gamma}{|w_i^{(t)}|+\\epsilon} \\right) |w_i|\n$$\nThis expression is a weighted $\\ell_1$ norm of the form $\\sum_{i}\\lambda_{i}^{(t)}|w_{i}|$. By direct comparison, we can identify the weights for the $t$-th iteration as:\n$$\n\\lambda_{i}^{(t)} = \\frac{\\gamma}{|w_{i}^{(t)}|+\\epsilon}\n$$\n\n**Part 3: Numerical Calculation of the IRL1 Proximal Update**\n\nWe need to compute $\\mathbf{w}^{(1)}=\\operatorname{prox}_{\\sum_{i}\\lambda_{i}^{(0)}|w_{i}|}(\\mathbf{a})$. The process involves two steps: first, calculating the weights $\\lambda^{(0)}$ from the initial iterate $\\mathbf{w}^{(0)}$, and second, applying the proximal operator derived in Part 1.\n\nThe given values are:\n$n=4$, $\\mathbf{a}=(2.1, -0.4, 0.05, -3.0)$, $\\mathbf{w}^{(0)}=(0.5, 0, 0, 4.0)$, $\\gamma=0.8$, $\\epsilon=0.1$.\n\n**Step A: Calculate weights $\\lambda_{i}^{(0)}$**\nUsing the formula $\\lambda_{i}^{(0)} = \\frac{\\gamma}{|w_{i}^{(0)}|+\\epsilon}$:\n- For $i=1$: $\\lambda_{1}^{(0)} = \\frac{0.8}{|0.5|+0.1} = \\frac{0.8}{0.6} = \\frac{4}{3}$.\n- For $i=2$: $\\lambda_{2}^{(0)} = \\frac{0.8}{|0|+0.1} = \\frac{0.8}{0.1} = 8$.\n- For $i=3$: $\\lambda_{3}^{(0)} = \\frac{0.8}{|0|+0.1} = \\frac{0.8}{0.1} = 8$.\n- For $i=4$: $\\lambda_{4}^{(0)} = \\frac{0.8}{|4.0|+0.1} = \\frac{0.8}{4.1} = \\frac{8}{41}$.\n\n**Step B: Compute $\\mathbf{w}^{(1)}$ using the soft-thresholding operator**\nUsing the formula $w_i^{(1)} = \\operatorname{sign}(a_i) \\max(0, |a_i| - \\lambda_i^{(0)})$:\n- For $i=1$: $a_1 = 2.1$, $\\lambda_1^{(0)} = 4/3 \\approx 1.333$. Since $|2.1| > 4/3$:\n  $w_1^{(1)} = \\operatorname{sign}(2.1) \\left(|2.1| - \\frac{4}{3}\\right) = \\frac{21}{10} - \\frac{4}{3} = \\frac{63-40}{30} = \\frac{23}{30}$.\n- For $i=2$: $a_2 = -0.4$, $\\lambda_2^{(0)} = 8$. Since $|-0.4| \\le 8$:\n  $w_2^{(1)} = 0$.\n- For $i=3$: $a_3 = 0.05$, $\\lambda_3^{(0)} = 8$. Since $|0.05| \\le 8$:\n  $w_3^{(1)} = 0$.\n- For $i=4$: $a_4 = -3.0$, $\\lambda_4^{(0)} = 8/41 \\approx 0.195$. Since $|-3.0| > 8/41$:\n  $w_4^{(1)} = \\operatorname{sign}(-3.0) \\left(|-3.0| - \\frac{8}{41}\\right) = -1 \\left(3 - \\frac{8}{41}\\right) = -\\left(\\frac{123-8}{41}\\right) = -\\frac{115}{41}$.\n\nThe resulting vector in exact form is $\\mathbf{w}^{(1)} = (23/30, 0, 0, -115/41)$.\n\n**Step C: Round to four significant figures**\n- $w_1^{(1)} = 23/30 \\approx 0.766666... \\to 0.7667$.\n- $w_2^{(1)} = 0 \\to 0$.\n- $w_3^{(1)} = 0 \\to 0$.\n- $w_4^{(1)} = -115/41 \\approx -2.804878... \\to -2.805$.\n\nThe final vector, with each entry rounded to four significant figures, is $(0.7667, 0, 0, -2.805)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.7667 & 0 & 0 & -2.805\n\\end{pmatrix}\n}\n$$"
        }
    ]
}