{
    "hands_on_practices": [
        {
            "introduction": "Understanding the performance of an online algorithm is central to the field, and the most fundamental benchmark is the regret of Online Subgradient Descent (OSD). This exercise is a cornerstone of online optimization, guiding you to both derive the classic $O(\\sqrt{T})$ regret bound from first principles and then verify it through code. By tackling this problem , you will build the foundational skills needed to analyze and implement online learning algorithms.",
            "id": "3188888",
            "problem": "Consider the setting of Online Convex Optimization (OCO). Let the decision set be the closed Euclidean ball $\\mathcal{X} = \\{x \\in \\mathbb{R}^d : \\lVert x \\rVert_2 \\le R\\}$ of radius $R > 0$, centered at the origin. At each round $t \\in \\{1,2,\\dots,T\\}$, an online learner selects a point $x_t \\in \\mathcal{X}$, after which an adversary reveals a convex loss function $f_t : \\mathcal{X} \\to \\mathbb{R}$ that is $G$-Lipschitz with respect to the Euclidean norm, meaning that every subgradient $g_t \\in \\partial f_t(x)$ satisfies $\\lVert g_t \\rVert_2 \\le G$ for all $x \\in \\mathcal{X}$, where $G > 0$. The learner incurs loss $f_t(x_t)$. The cumulative regret after $T$ rounds is defined by\n$$\n\\mathrm{Regret}_T \\triangleq \\sum_{t=1}^T f_t(x_t) - \\min_{x \\in \\mathcal{X}} \\sum_{t=1}^T f_t(x).\n$$\nThe learner uses Online Subgradient Descent (also called Projected Subgradient Descent): starting from $x_1 = 0$, and for a chosen step size $\\eta > 0$, the update is\n$$\nx_{t+1} = \\Pi_{\\mathcal{X}}\\big(x_t - \\eta \\, g_t\\big),\n$$\nwhere $g_t \\in \\partial f_t(x_t)$ is a subgradient at $x_t$ and $\\Pi_{\\mathcal{X}}$ denotes the Euclidean projection onto $\\mathcal{X}$.\n\nTasks:\n1) Starting from the definitions of convexity, subgradients, and Euclidean projection, derive a non-asymptotic upper bound for $\\mathrm{Regret}_T$ that exhibits sublinear dependence on $T$, assuming a constant step size $\\eta$ that you choose as a function of $R$, $G$, and $T$. Your derivation must rely only on fundamental facts: the subgradient inequality for convex functions, the non-expansiveness of Euclidean projection, and elementary algebraic identities. Your final bound must be expressed explicitly in terms of $R$, $G$, and $T$ and must scale like $O(\\sqrt{T})$.\n\n2) Implement a program that instantiates the above algorithm on a synthetic family of linear losses of the form $f_t(x) = g_t^\\top x$ with $\\lVert g_t \\rVert_2 \\le G$ and verifies numerically that the observed regret is bounded by your theoretical upper bound for several test cases. For linear losses, you must compute the comparator $\\arg\\min_{x \\in \\mathcal{X}} \\sum_{t=1}^T f_t(x)$ exactly via the closed form of the minimizer over the Euclidean ball, and thus compute the exact regret. Use the constant step size $\\eta$ you selected in Task 1.\n\nUse the following test suite. In every case, the constant $G$ denotes the norm bound on $g_t$, the dimension is $d$, the horizon is $T$, and the ball radius is $R$. All random number generation must be reproducible using the given seeds, and every generated $g_t$ must satisfy $\\lVert g_t \\rVert_2 = G$.\n\n- Case A (happy path, randomized): $d = 5$, $R = 3.0$, $G = 2.0$, $T = 400$. Generate each $g_t$ by sampling from a zero-mean normal distribution with independent components with variance $1$, then normalize to unit Euclidean norm and scale to norm $G$. Use seed $0$ for the random number generator.\n\n- Case B (boundary, single step): $d = 3$, $R = 1.0$, $G = 1.0$, $T = 1$. Use a deterministic subgradient $g_1 = G \\cdot e_1$, where $e_1$ is the first standard basis vector in $\\mathbb{R}^d$.\n\n- Case C (adversarial, direction follows iterate): $d = 7$, $R = 2.0$, $G = 1.5$, $T = 600$. Define $g_t = G \\cdot \\frac{x_t}{\\max(\\lVert x_t \\rVert_2, 10^{-12})}$ for $t \\ge 1$, with the convention that if $\\lVert x_t \\rVert_2 = 0$, then $g_t = G \\cdot e_1$.\n\n- Case D (alternating fixed direction, near-canceling sum): $d = 10$, $R = 2.0$, $G = 3.0$, $T = 2500$. Draw a fixed nonzero vector $u \\in \\mathbb{R}^d$ from a zero-mean normal distribution with independent components with variance $1$, using seed $42$, then set $u \\leftarrow u / \\lVert u \\rVert_2$. For odd $t$ set $g_t = G u$, and for even $t$ set $g_t = -G u$.\n\n- Case E (strong gradients, frequent projection to boundary): $d = 2$, $R = 0.5$, $G = 10.0$, $T = 200$. Generate each $g_t$ exactly as in Case A, using seed $123$.\n\nFor each case, run Online Subgradient Descent with $x_1 = 0$ and your constant step size $\\eta$, compute the exact regret $\\mathrm{Regret}_T$, and compare it against your explicit upper bound function of $R$, $G$, and $T$. Decide a boolean outcome that is true if and only if $\\mathrm{Regret}_T \\le \\text{Bound}(R,G,T) + \\varepsilon$, where $\\varepsilon = 10^{-9}$.\n\nFinal output format requirement:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is the boolean for the corresponding case in the order A, B, C, D, E. No other output is permitted.",
            "solution": "The problem of deriving a regret bound for the Online Subgradient Descent algorithm is a standard task in the analysis of online convex optimization. The problem statement is valid as it is scientifically grounded in established optimization theory, well-posed, and objective. We proceed with the two tasks.\n\n### Task 1: Derivation of the Regret Bound\n\nOur goal is to derive an upper bound on the regret, defined as $\\mathrm{Regret}_T \\triangleq \\sum_{t=1}^T f_t(x_t) - \\min_{x \\in \\mathcal{X}} \\sum_{t=1}^T f_t(x)$. Let $x^* = \\arg\\min_{x \\in \\mathcal{X}} \\sum_{t=1}^T f_t(x)$ be an optimal solution in hindsight. Since each $f_t$ is convex, their sum is also convex, and since the domain $\\mathcal{X}$ is compact, such a minimizer $x^*$ is guaranteed to exist. The regret can be rewritten as $\\mathrm{Regret}_T = \\sum_{t=1}^T f_t(x_t) - \\sum_{t=1}^T f_t(x^*)$.\n\nThe derivation hinges on three fundamental properties:\n1.  **Subgradient Inequality:** For a convex function $f_t$ and any subgradient $g_t \\in \\partial f_t(x_t)$, we have $f_t(x_t) - f_t(x^*) \\le g_t^\\top(x_t - x^*)$.\n2.  **Projection Property:** The Euclidean projection operator $\\Pi_{\\mathcal{X}}$ is non-expansive. A key consequence for any $y \\in \\mathbb{R}^d$ and any $z \\in \\mathcal{X}$ is $\\lVert \\Pi_{\\mathcal{X}}(y) - z \\rVert_2^2 \\le \\lVert y - z \\rVert_2^2$.\n3.  **Lipschitz Condition:** The loss functions are $G$-Lipschitz, which implies that any subgradient $g_t \\in \\partial f_t(x)$ has a bounded Euclidean norm, $\\lVert g_t \\rVert_2 \\le G$.\n\nLet us analyze the distance between the iterates $x_t$ and the fixed optimal point $x^*$. The update rule is $x_{t+1} = \\Pi_{\\mathcal{X}}(x_t - \\eta g_t)$. Consider the squared Euclidean distance $\\lVert x_{t+1} - x^* \\rVert_2^2$. Since $x^* \\in \\mathcal{X}$, we can apply the projection property:\n$$\n\\lVert x_{t+1} - x^* \\rVert_2^2 = \\lVert \\Pi_{\\mathcal{X}}(x_t - \\eta g_t) - x^* \\rVert_2^2 \\le \\lVert (x_t - \\eta g_t) - x^* \\rVert_2^2\n$$\nExpanding the term on the right-hand side using the identity $\\lVert a - b \\rVert_2^2 = \\lVert a \\rVert_2^2 - 2a^\\top b + \\lVert b \\rVert_2^2$:\n$$\n\\lVert (x_t - x^*) - \\eta g_t \\rVert_2^2 = \\lVert x_t - x^* \\rVert_2^2 - 2\\eta g_t^\\top(x_t - x^*) + \\eta^2 \\lVert g_t \\rVert_2^2\n$$\nCombining these inequalities, we get:\n$$\n\\lVert x_{t+1} - x^* \\rVert_2^2 \\le \\lVert x_t - x^* \\rVert_2^2 - 2\\eta g_t^\\top(x_t - x^*) + \\eta^2 \\lVert g_t \\rVert_2^2\n$$\nThis inequality is central to the analysis. We rearrange it to isolate the term $g_t^\\top(x_t - x^*)$, which appears in the subgradient inequality:\n$$\n2\\eta g_t^\\top(x_t - x^*) \\le \\lVert x_t - x^* \\rVert_2^2 - \\lVert x_{t+1} - x^* \\rVert_2^2 + \\eta^2 \\lVert g_t \\rVert_2^2\n$$\nDividing by $2\\eta$ (since $\\eta > 0$):\n$$\ng_t^\\top(x_t - x^*) \\le \\frac{1}{2\\eta} \\left( \\lVert x_t - x^* \\rVert_2^2 - \\lVert x_{t+1} - x^* \\rVert_2^2 \\right) + \\frac{\\eta}{2} \\lVert g_t \\rVert_2^2\n$$\nNow, we apply the subgradient inequality $f_t(x_t) - f_t(x^*) \\le g_t^\\top(x_t - x^*)$ to bound the per-round regret:\n$$\nf_t(x_t) - f_t(x^*) \\le \\frac{1}{2\\eta} \\left( \\lVert x_t - x^* \\rVert_2^2 - \\lVert x_{t+1} - x^* \\rVert_2^2 \\right) + \\frac{\\eta}{2} \\lVert g_t \\rVert_2^2\n$$\nTo obtain the total regret, we sum this inequality from $t=1$ to $T$:\n$$\n\\sum_{t=1}^T (f_t(x_t) - f_t(x^*)) \\le \\sum_{t=1}^T \\left[ \\frac{1}{2\\eta} \\left( \\lVert x_t - x^* \\rVert_2^2 - \\lVert x_{t+1} - x^* \\rVert_2^2 \\right) + \\frac{\\eta}{2} \\lVert g_t \\rVert_2^2 \\right]\n$$\nThe left side is precisely $\\mathrm{Regret}_T$. The right side can be split into two sums:\n$$\n\\mathrm{Regret}_T \\le \\frac{1}{2\\eta} \\sum_{t=1}^T \\left( \\lVert x_t - x^* \\rVert_2^2 - \\lVert x_{t+1} - x^* \\rVert_2^2 \\right) + \\frac{\\eta}{2} \\sum_{t=1}^T \\lVert g_t \\rVert_2^2\n$$\nThe first summation is a telescoping series:\n$$\n\\sum_{t=1}^T \\left( \\lVert x_t - x^* \\rVert_2^2 - \\lVert x_{t+1} - x^* \\rVert_2^2 \\right) = (\\lVert x_1 - x^* \\rVert_2^2 - \\lVert x_2 - x^* \\rVert_2^2) + \\dots + (\\lVert x_T - x^* \\rVert_2^2 - \\lVert x_{T+1} - x^* \\rVert_2^2) = \\lVert x_1 - x^* \\rVert_2^2 - \\lVert x_{T+1} - x^* \\rVert_2^2\n$$\nSince $\\lVert x_{T+1} - x^* \\rVert_2^2 \\ge 0$, we can upper-bound the telescoping sum by $\\lVert x_1 - x^* \\rVert_2^2$.\nThe regret bound becomes:\n$$\n\\mathrm{Regret}_T \\le \\frac{1}{2\\eta} \\lVert x_1 - x^* \\rVert_2^2 + \\frac{\\eta}{2} \\sum_{t=1}^T \\lVert g_t \\rVert_2^2\n$$\nWe now use the specific conditions from the problem. The initial point is $x_1 = 0$. The comparator $x^*$ is in the decision set $\\mathcal{X} = \\{x \\in \\mathbb{R}^d : \\lVert x \\rVert_2 \\le R\\}$, so $\\lVert x^* \\rVert_2 \\le R$. Therefore, $\\lVert x_1 - x^* \\rVert_2^2 = \\lVert 0 - x^* \\rVert_2^2 = \\lVert x^* \\rVert_2^2 \\le R^2$. The subgradients are bounded by $G$, so $\\lVert g_t \\rVert_2^2 \\le G^2$ for all $t$. Substituting these gives:\n$$\n\\mathrm{Regret}_T \\le \\frac{R^2}{2\\eta} + \\frac{\\eta}{2} \\sum_{t=1}^T G^2 = \\frac{R^2}{2\\eta} + \\frac{\\eta T G^2}{2}\n$$\nThis bound holds for any constant step size $\\eta > 0$. To achieve sublinear regret in $T$, we must choose $\\eta$ as a function of $T$. We select $\\eta$ to minimize this upper bound. Let $B(\\eta) = \\frac{R^2}{2\\eta} + \\frac{\\eta T G^2}{2}$. We find the minimum by setting the derivative with respect to $\\eta$ to zero:\n$$\n\\frac{dB}{d\\eta} = -\\frac{R^2}{2\\eta^2} + \\frac{TG^2}{2} = 0 \\implies \\eta^2 = \\frac{R^2}{TG^2} \\implies \\eta = \\frac{R}{G\\sqrt{T}}\n$$\nSubstituting this optimal constant step size back into the regret bound:\n$$\n\\mathrm{Regret}_T \\le \\frac{R^2}{2} \\left( \\frac{G\\sqrt{T}}{R} \\right) + \\frac{T G^2}{2} \\left( \\frac{R}{G\\sqrt{T}} \\right) = \\frac{RG\\sqrt{T}}{2} + \\frac{RG\\sqrt{T}}{2} = RG\\sqrt{T}\n$$\nThus, a non-asymptotic upper bound for the regret is $\\mathrm{Regret}_T \\le RG\\sqrt{T}$. This bound exhibits the required sublinear $O(\\sqrt{T})$ dependence on the time horizon $T$. For our numerical verification, we will use the step size $\\eta = \\frac{R}{G\\sqrt{T}}$ and the bound $\\mathrm{Bound}(R, G, T) = RG\\sqrt{T}$.\n\n### Task 2: Implementation and Numerical Verification\n\nThe implementation follows the Online Subgradient Descent algorithm. For each test case, we simulate the $T$ rounds of the online learning process with the specified parameters and subgradient generation rule. We compute the exact regret and verify that it is less than or equal to the derived theoretical bound $RG\\sqrt{T}$ plus a small tolerance $\\varepsilon = 10^{-9}$.\n\nFor linear losses $f_t(x) = g_t^\\top x$, the cumulative loss is $\\sum_{t=1}^T f_t(x) = (\\sum_{t=1}^T g_t)^\\top x$. The comparator's cumulative loss is $\\min_{x \\in \\mathcal{X}} (\\sum_{t=1}^T g_t)^\\top x$. Let $S = \\sum_{t=1}^T g_t$. We want to minimize $S^\\top x$ subject to $\\lVert x \\rVert_2 \\le R$. The minimum is achieved when $x$ is in the direction opposite to $S$ and on the boundary of the ball. The minimizer is $x^* = -R \\frac{S}{\\lVert S \\rVert_2}$ (if $S \\neq 0$). The minimum loss is $S^\\top x^* = -R \\frac{S^\\top S}{\\lVert S \\rVert_2} = -R \\lVert S \\rVert_2$. If $S=0$, any $x \\in \\mathcal{X}$ is a minimizer and the minimum loss is $0$.\n\nThe following code implements this procedure for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(d: int, R: float, G: float, T: int, case: str, seed: int | None) -> bool:\n    \"\"\"\n    Runs one instance of the Online Subgradient Descent simulation.\n\n    Args:\n        d: Dimension of the space.\n        R: Radius of the Euclidean ball.\n        G: Lipschitz constant (norm bound on subgradients).\n        T: Time horizon (number of rounds).\n        case: A string identifying the test case ('A', 'B', 'C', 'D', 'E').\n        seed: Random seed for reproducibility.\n\n    Returns:\n        A boolean indicating if the observed regret is within the theoretical bound.\n    \"\"\"\n    if seed is not None:\n        rng = np.random.default_rng(seed)\n\n    # Optimal constant step size from the theoretical derivation\n    if T > 0:\n        eta = R / (G * np.sqrt(T))\n    else: # Handle T=0 case, though not in test suite.\n        eta = 0.0\n\n    # Initialization\n    x = np.zeros(d)  # Initial iterate x_1 = 0\n    learner_loss = 0.0\n    sum_g = np.zeros(d)\n\n    # Pre-computation for case D\n    if case == 'D':\n        u_vec = rng.normal(size=d)\n        u_norm = np.linalg.norm(u_vec)\n        if u_norm > 1e-15:\n            u = u_vec / u_norm\n        else:  # Fallback for the highly unlikely event of a zero vector\n            u = np.zeros(d)\n            u[0] = 1.0\n\n    # Main OCO loop for T rounds\n    for t in range(1, T + 1):\n        # 1. Adversary reveals subgradient g_t at x_t (current x)\n        if case == 'A' or case == 'E':\n            g_vec = rng.normal(size=d)\n            g_norm = np.linalg.norm(g_vec)\n            if g_norm > 1e-15:\n                g = G * g_vec / g_norm\n            else: # Fallback\n                g = np.zeros(d)\n                g[0] = G\n        elif case == 'B':\n            g = np.zeros(d)\n            g[0] = G\n        elif case == 'C':\n            x_norm = np.linalg.norm(x)\n            if x_norm == 0:  # Convention for x_t = 0\n                g = np.zeros(d)\n                g[0] = G\n            else:\n                g = G * x / max(x_norm, 1e-12)\n        elif case == 'D':\n            if t % 2 == 1:  # Odd t\n                g = G * u\n            else:  # Even t\n                g = -G * u\n        else:\n            raise ValueError(f\"Unknown case: {case}\")\n        \n        # 2. Learner incurs loss f_t(x_t) = g_t^T x_t\n        learner_loss += g @ x\n\n        # 3. Update sum of gradients for comparator calculation\n        sum_g += g\n\n        # 4. Learner computes next iterate x_{t+1}\n        # Unprojected update\n        y = x - eta * g\n        # Projection onto the Euclidean ball\n        y_norm = np.linalg.norm(y)\n        if y_norm > R:\n            x = R * y / y_norm\n        else:\n            x = y\n    \n    # After T rounds, calculate the exact regret\n    # 1. Calculate the comparator's total loss\n    sum_g_norm = np.linalg.norm(sum_g)\n    if sum_g_norm > 1e-12:\n        # x_star = -R * sum_g / sum_g_norm\n        # comparator_loss = sum_g @ x_star = -R * ||sum_g||_2\n        comparator_loss = -R * sum_g_norm\n    else:\n        comparator_loss = 0.0\n\n    # 2. Compute the final regret\n    regret = learner_loss - comparator_loss\n\n    # 3. Compute the theoretical upper bound\n    bound = R * G * np.sqrt(T)\n    \n    # 4. Verify if regret is within the bound, with a small numerical tolerance\n    epsilon = 1e-9\n    return regret <= bound + epsilon\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'d': 5, 'R': 3.0, 'G': 2.0, 'T': 400, 'case': 'A', 'seed': 0},\n        {'d': 3, 'R': 1.0, 'G': 1.0, 'T': 1, 'case': 'B', 'seed': None},\n        {'d': 7, 'R': 2.0, 'G': 1.5, 'T': 600, 'case': 'C', 'seed': None},\n        {'d': 10, 'R': 2.0, 'G': 3.0, 'T': 2500, 'case': 'D', 'seed': 42},\n        {'d': 2, 'R': 0.5, 'G': 10.0, 'T': 200, 'case': 'E', 'seed': 123},\n    ]\n\n    results = []\n    for params in test_cases:\n        is_bounded = run_simulation(**params)\n        results.append(is_bounded)\n\n    # Format the final output as a comma-separated list of booleans in lowercase.\n    # Python str(bool) gives 'True', 'False'; problem example is ambiguous.\n    # Standard practice often uses lowercase for JSON/API-style booleans.\n    # To be safe and explicit, we will use lowercase strings.\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While Online Gradient Descent is powerful, its effectiveness is tied to Euclidean geometry. Many real-world problems, such as portfolio management or resource allocation, involve decisions on the probability simplex. This practice  introduces Mirror Descent with the entropic regularizer—an elegant and highly effective algorithm for this setting—allowing you to implement the method and empirically validate its theoretical regret guarantees.",
            "id": "3186873",
            "problem": "You are asked to implement Stochastic Gradient Descent (SGD) for simplex-constrained optimization via mirror descent using the entropic mirror map, and to empirically assess how the cumulative regret scales as order $\\mathcal{O}(\\sqrt{T})$, where $T$ is the number of rounds. The domain is the probability simplex $\\Delta^K = \\{x \\in \\mathbb{R}^K \\mid x_i \\ge 0, \\sum_{i=1}^K x_i = 1\\}$ for a given dimension $K$. At each round $t \\in \\{1,\\dots,T\\}$, an algorithm selects a point $x_t \\in \\Delta^K$, then a stochastic linear loss is realized as $f_t(x_t) = g_t^\\top x_t$, where $g_t \\in \\mathbb{R}^K$ is a random vector with bounded coordinates. The cumulative regret with respect to the best fixed comparator in $\\Delta^K$ is defined as\n$$\nR_T = \\sum_{t=1}^T f_t(x_t) - \\min_{x \\in \\Delta^K} \\sum_{t=1}^T f_t(x).\n$$\n\nThe program you will implement must use mirror descent with the entropic mirror map. The entropic mirror map is defined via the negative Shannon entropy $R(x) = \\sum_{i=1}^K x_i \\log x_i$, and the associated Bregman divergence $D_R(y\\|x) = R(y) - R(x) - \\nabla R(x)^\\top (y - x)$. Your implementation should perform stochastic updates governed by unbiased gradient samples and respect the simplex constraint at every round. The stochastic gradient samples $g_t$ must be generated from a scientifically sound model with bounded coordinates. Specifically, for each test case, construct $g_t$ as a clipped noisy version of a fixed base cost vector $c \\in [0,B]^K$ with additive zero-mean noise, and clip each coordinate of $g_t$ to the interval $[0,B]$ to enforce the bound $\\|g_t\\|_\\infty \\le B$, where $B > 0$ is provided. The initial point must be the uniform distribution on $\\Delta^K$.\n\nYour task is to:\n- Implement the mirror descent algorithm on $\\Delta^K$ using the entropic mirror map and stochastic linear losses as specified above, producing a sequence $\\{x_t\\}_{t=1}^T$ that remains in $\\Delta^K$.\n- For each test case, compute the cumulative regret $R_T$ with respect to the best fixed point in $\\Delta^K$, and then normalize it by $\\sqrt{T}$ to obtain the quantity $R_T / \\sqrt{T}$.\n- Use pseudorandom number generation with the provided seeds to ensure deterministic reproducibility.\n\nFundamental base for the derivation and implementation:\n- Convexity of the loss functions $f_t(x) = g_t^\\top x$ and the definition of the probability simplex $\\Delta^K$.\n- Definition of mirror descent as an optimization method that iteratively updates based on a chosen strictly convex regularizer $R$, using the corresponding Bregman divergence $D_R$.\n- Properties of the negative Shannon entropy as a strictly convex regularizer on $\\Delta^K$.\n- The concept of cumulative regret $R_T$ in online optimization and its scaling with respect to $T$ for appropriately chosen learning rate and bounded gradients.\n\nTest suite specification:\nImplement the program to run the following $5$ test cases. In each case, $K$ is the dimension of the simplex, $T$ is the number of iterations, $B$ is an upper bound on the coordinates of the stochastic gradients, $\\sigma$ is the standard deviation of the additive noise, $c$ is the base cost vector, and $\\text{seed}$ is the pseudorandom seed. All entries of $c$ are within $[0,B]$.\n\n- Case $1$: $K=5$, $T=2000$, $B=1.0$, $\\sigma=0.2$, $c = [0.1, 0.2, 0.3, 0.4, 0.5]$, $\\text{seed}=42$.\n- Case $2$: $K=2$, $T=500$, $B=1.0$, $\\sigma=0.3$, $c = [0.0, 0.5]$, $\\text{seed}=7$.\n- Case $3$: $K=10$, $T=5000$, $B=2.0$, $\\sigma=0.5$, $c$ is the equally spaced vector from $0.1$ to $1.0$ of length $10$, $\\text{seed}=17$.\n- Case $4$: $K=1$, $T=1000$, $B=1.0$, $\\sigma=0.1$, $c = [0.7]$, $\\text{seed}=99$.\n- Case $5$: $K=5$, $T=4000$, $B=1.0$, $\\sigma=0.2$, $c = [0.1, 0.2, 0.3, 0.4, 0.5]$, $\\text{seed}=123$.\n\nOutput requirement:\nYour program should produce a single line of output containing the normalized regrets $R_T / \\sqrt{T}$ for the $5$ test cases as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$). Each $r_i$ must be a real number (a decimal). No additional text or formatting beyond this single line is permitted in the output.\n\nThere are no physical units involved in this problem. Angles are not used. Do not express any quantity with a percentage sign; use decimals when needed.",
            "solution": "The user-provided problem statement has been validated and is determined to be sound, well-posed, and formalizable. It describes a classic problem in online convex optimization: minimizing cumulative regret against a static expert in hindsight, where the decision space is the probability simplex. The specified algorithm, Mirror Descent with an entropic regularizer, is the standard and theoretically well-grounded approach for this setting.\n\nThe problem is to implement the Stochastic Mirror Descent algorithm for a sequence of linear loss functions on the probability simplex $\\Delta^K = \\{x \\in \\mathbb{R}^K \\mid x_i \\ge 0, \\sum_{i=1}^K x_i = 1\\}$. At each time step $t \\in \\{1, \\dots, T\\}$, the algorithm selects a point $x_t \\in \\Delta^K$ and incurs a loss $f_t(x_t) = g_t^\\top x_t$, where $g_t$ is a stochastic gradient. The goal is to compute the normalized cumulative regret, $R_T / \\sqrt{T}$, where $R_T = \\sum_{t=1}^T f_t(x_t) - \\min_{x \\in \\Delta^K} \\sum_{t=1}^T f_t(x)$.\n\nThe core of the solution is the Mirror Descent update rule. This is an iterative optimization method that generalizes standard gradient descent. The update from $x_t$ to $x_{t+1}$ is given by:\n$$\nx_{t+1} = \\arg\\min_{x \\in \\Delta^K} \\left\\{ \\eta_t g_t^\\top x + D_R(x \\| x_t) \\right\\}\n$$\nwhere $\\eta_t$ is the learning rate, $g_t = \\nabla f_t(x_t)$ is the gradient of the loss function, and $D_R(y \\| x)$ is the Bregman divergence associated with a strictly convex regularizer function $R(x)$.\n\nFor this problem, the regularizer is the negative Shannon entropy, defined on the interior of the simplex:\n$$\nR(x) = \\sum_{i=1}^K x_i \\log x_i\n$$\nThe gradient of this regularizer is $\\nabla R(x)$, with components $(\\nabla R(x))_i = \\log x_i + 1$. The Bregman divergence is $D_R(y \\| x) = R(y) - R(x) - \\nabla R(x)^\\top (y - x)$.\n\nSubstituting these into the update rule, and dropping terms constant with respect to the minimization variable $x$, the update step becomes:\n$$\nx_{t+1} = \\arg\\min_{x \\in \\Delta^K} \\left\\{ \\eta_t g_t^\\top x + R(x) - (\\nabla R(x_t))^\\top x \\right\\}\n$$\n$$\nx_{t+1} = \\arg\\min_{x \\in \\Delta^K} \\left\\{ \\sum_{i=1}^K \\eta_t (g_t)_i x_i + \\sum_{i=1}^K x_i \\log x_i - \\sum_{i=1}^K (\\log (x_t)_i + 1) x_i \\right\\}\n$$\nThis is a convex optimization problem over the simplex. Introducing a Lagrange multiplier $\\lambda$ for the constraint $\\sum_i x_i = 1$, the first-order optimality conditions yield a closed-form solution for each component $(x_{t+1})_i$:\n$$\n(x_{t+1})_i = \\frac{(x_t)_i \\exp(-\\eta_t (g_t)_i)}{\\sum_{j=1}^K (x_t)_j \\exp(-\\eta_t (g_t)_j)}\n$$\nThis update rule is known as the Exponentiated Gradient algorithm. It ensures that if $x_t \\in \\Delta^K$, then $x_{t+1}$ also lies in $\\Delta^K$. The initial point is the center of the simplex, $x_1$, where $(x_1)_i = 1/K$ for all $i \\in \\{1, \\dots, K\\}$.\n\nThe choice of learning rate is critical for achieving the desired regret scaling of $\\mathcal{O}(\\sqrt{T})$. For online mirror descent with gradients whose coordinates are bounded, i.e., $\\|g_t\\|_\\infty \\le B$, the theoretical regret bound is given by:\n$$\nR_T \\le \\frac{D_{R, \\text{max}}}{\\eta} + \\frac{\\eta}{2} \\sum_{t=1}^T \\|g_t\\|_*^2\n$$\nFor the entropic regularizer, the relevant range $D_{R, \\text{max}} = \\max_{x \\in \\Delta^K} R(x) - \\min_{x \\in \\Delta^K} R(x) = 0 - (-\\log K) = \\log K$. Using a more specific bound for this setting and a fixed learning rate $\\eta$, the cumulative regret is bounded by $R_T \\le \\frac{\\log K}{\\eta} + \\frac{\\eta T B^2}{2}$. To minimize this upper bound, we choose $\\eta$ to balance the two terms:\n$$\n\\frac{\\log K}{\\eta} = \\frac{\\eta T B^2}{2} \\implies \\eta^2 = \\frac{2 \\log K}{T B^2} \\implies \\eta = \\frac{\\sqrt{2 \\log K}}{B \\sqrt{T}}\n$$\nWith this fixed learning rate, the regret is bounded by $R_T \\le B\\sqrt{2T\\log K}$, which exhibits the required $\\mathcal{O}(\\sqrt{T})$ scaling. The implementation will use this theoretically-motivated learning rate. For the special case $K=1$, $\\log K = 0$, so $\\eta=0$, implying the point never moves from its initial state, which is correct as $\\Delta^1$ is a single point.\n\nThe overall algorithm is as follows for a given test case with parameters $K, T, B, \\sigma, c, \\text{seed}$:\n1.  Initialize a pseudorandom number generator with the given `seed`.\n2.  Initialize the point $x_1$ with $(x_1)_i = 1/K$ for $i=1, \\dots, K$.\n3.  Calculate the learning rate $\\eta = \\frac{\\sqrt{2 \\log K}}{B \\sqrt{T}}$.\n4.  Initialize total algorithm loss $\\mathcal{L}_{alg} = 0$ and total gradient sum vector $G_T = \\mathbf{0} \\in \\mathbb{R}^K$.\n5.  Iterate for $t=1, \\dots, T$:\n    a. Generate the stochastic gradient $g_t$: create a noise vector $\\epsilon_t$ with components drawn from a normal distribution $\\mathcal{N}(0, \\sigma^2)$, then set $g_t = \\text{clip}(c + \\epsilon_t, 0, B)$.\n    b. Incur the loss for the current round: $l_t = g_t^\\top x_t$.\n    c. Update the total loss: $\\mathcal{L}_{alg} = \\mathcal{L}_{alg} + l_t$.\n    d. Update the total gradient sum: $G_T = G_T + g_t$.\n    e. Compute the next point $x_{t+1}$ using the Exponentiated Gradient update rule with learning rate $\\eta$.\n6.  After the loop, calculate the loss of the best fixed point in hindsight. This is the minimum of a linear function over the simplex, which must occur at a vertex: $\\mathcal{L}_{opt} = \\min_{x \\in \\Delta^K} G_T^\\top x = \\min_{i \\in \\{1,\\dots,K\\}} (G_T)_i$.\n7.  Compute the cumulative regret $R_T = \\mathcal{L}_{alg} - \\mathcal{L}_{opt}$.\n8.  Compute the normalized regret $r = R_T / \\sqrt{T}$.\n\nThis procedure will be applied to all $5$ test cases to generate the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Stochastic Mirror Descent with an entropic regularizer (Exponentiated Gradient)\n    on the probability simplex to calculate normalized cumulative regret for several test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'K': 5, 'T': 2000, 'B': 1.0, 'sigma': 0.2, 'c': np.array([0.1, 0.2, 0.3, 0.4, 0.5]), 'seed': 42},\n        {'K': 2, 'T': 500, 'B': 1.0, 'sigma': 0.3, 'c': np.array([0.0, 0.5]), 'seed': 7},\n        {'K': 10, 'T': 5000, 'B': 2.0, 'sigma': 0.5, 'c': np.linspace(0.1, 1.0, 10), 'seed': 17},\n        {'K': 1, 'T': 1000, 'B': 1.0, 'sigma': 0.1, 'c': np.array([0.7]), 'seed': 99},\n        {'K': 5, 'T': 4000, 'B': 1.0, 'sigma': 0.2, 'c': np.array([0.1, 0.2, 0.3, 0.4, 0.5]), 'seed': 123},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        K = case['K']\n        T = case['T']\n        B = case['B']\n        sigma = case['sigma']\n        c = case['c']\n        seed = case['seed']\n\n        rng = np.random.default_rng(seed)\n\n        # Handle the trivial case K=1\n        if K == 1:\n            results.append(0.0)\n            continue\n            \n        # 1. Initialize point x_1 at the center of the simplex\n        x = np.ones(K) / K\n        \n        # 2. Calculate the theoretically optimal fixed learning rate eta\n        eta = np.sqrt(2 * np.log(K) / (T * B**2))\n\n        # 3. Initialize tracking variables\n        total_algorithm_loss = 0.0\n        total_gradient_sum = np.zeros(K)\n\n        # 4. Main loop for T rounds\n        for t in range(T):\n            # a. Generate stochastic gradient g_t\n            noise = rng.normal(0, sigma, size=K)\n            g_t = np.clip(c + noise, 0, B)\n\n            # b. Incur loss and update total loss\n            loss_t = np.dot(g_t, x)\n            total_algorithm_loss += loss_t\n\n            # c. Update total gradient sum\n            total_gradient_sum += g_t\n            \n            # d. Update point x_t to x_{t+1} using Exponentiated Gradient rule\n            # Numerically stable implementation: find max_val and subtract it inside exp\n            # This does not change the result due to normalization.\n            unnormalized_weights = x * np.exp(-eta * g_t)\n            # The check for sum being zero is for extreme cases, very unlikely here.\n            norm_factor = np.sum(unnormalized_weights)\n            if norm_factor > 0:\n                x = unnormalized_weights / norm_factor\n            else:\n                # If all weights become numerically zero, reset to uniform.\n                # This is a safeguard and shouldn't be hit with the chosen eta.\n                x = np.ones(K) / K\n\n        # 5. Calculate loss of the best fixed point in hindsight\n        # This is min_{x in Delta^K} (sum g_t)^T x = min_i (sum_t g_t)_i\n        best_fixed_loss = np.min(total_gradient_sum)\n\n        # 6. Compute cumulative regret\n        regret = total_algorithm_loss - best_fixed_loss\n\n        # 7. Normalize regret and store the result\n        normalized_regret = regret / np.sqrt(T)\n        results.append(normalized_regret)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "To achieve faster convergence, online algorithms can incorporate second-order information, adapting the learning rate for each dimension. This practice  delves into such a second-order method, exploring how to build a preconditioner from past gradients to accelerate learning. Furthermore, it tackles a critical practical issue: robustness, challenging you to implement a filtering mechanism that protects the algorithm from outlier data that could otherwise derail its performance.",
            "id": "3159745",
            "problem": "Consider an Online Convex Optimization (OCO) problem over a compact convex domain with a second-order update. The decision variable is $x \\in \\mathbb{R}^d$ constrained to the hypercube domain $X = [-1,1]^d$. At each round $t \\in \\{1,2,\\dots,T\\}$, a convex loss function $f_t : X \\to \\mathbb{R}$ is revealed after $x_t$ is chosen. The instantaneous loss is defined by the quadratic function\n$$\nf_t(x) = \\frac{1}{2}\\lVert x - a_t \\rVert_2^2,\n$$\nwhere $a_t \\in \\mathbb{R}^d$ is an adversarially generated vector, and the gradient at $x$ is \n$$\n\\nabla f_t(x) = x - a_t.\n$$\nYou will implement two algorithms:\n- A baseline second-order preconditioned update that uses a cumulative aggregation of gradients.\n- A filtered second-order preconditioned update that excludes gradients whose leverage scores exceed a threshold from the aggregation.\n\nThe algorithm maintains a symmetric positive definite matrix $A_t \\in \\mathbb{R}^{d \\times d}$ that aggregates information. At each round, define the gradient at the current iterate $x_t$ as $g_t = \\nabla f_t(x_t)$ and the leverage score\n$$\n\\ell_t = g_t^\\top A_t^{-1} g_t.\n$$\nThe baseline aggregator always updates the matrix by adding $g_t g_t^\\top$ after taking the step. The filtered aggregator only adds $g_t g_t^\\top$ if the leverage score satisfies $\\ell_t \\le \\tau$, where $\\tau > 0$ is a given threshold. Both algorithms take a preconditioned gradient step using $A_t^{-1}$ followed by Euclidean projection onto $X$. The initialization is $x_1 = 0$ and $A_1 = \\lambda I_d$, where $I_d$ is the identity matrix in $\\mathbb{R}^d$ and $\\lambda > 0$.\n\nFor evaluation, compute the static regret\n$$\nR_T = \\sum_{t=1}^T f_t(x_t) \\;-\\; \\min_{x \\in X} \\sum_{t=1}^T f_t(x),\n$$\nfor each algorithm. For the specified quadratic losses over the hypercube, the static comparator minimizing the sum is\n$$\nx^\\star = \\Pi_X\\left( \\frac{1}{T} \\sum_{t=1}^T a_t \\right),\n$$\nwhere $\\Pi_X(\\cdot)$ denotes the Euclidean projection onto $X$, and the minimum value is $\\sum_{t=1}^T f_t(x^\\star)$.\n\nYour program must simulate the sequence $\\{a_t\\}_{t=1}^T$ for each test case as follows. Fix a direction $u \\in \\mathbb{R}^d$ with $\\lVert u \\rVert_2 = 1$ drawn deterministically from a pseudorandom normal generator with a specified seed. For rounds $t \\le T_{\\mathrm{out}}$, set $a_t = M u$. For rounds $t > T_{\\mathrm{out}}$, set\n$$\na_t = \\left( \\frac{t - T_{\\mathrm{out}}}{T - T_{\\mathrm{out}}} \\right) D_{\\mathrm{end}}\\, u \\;+\\; \\xi_t,\n$$\nwhere $\\xi_t \\in \\mathbb{R}^d$ is independent isotropic Gaussian noise with zero mean and standard deviation $\\sigma$ per coordinate generated from the same pseudorandom generator. If $T = T_{\\mathrm{out}}$, then define the drift fraction as zero so that $a_t = \\xi_t$ for all $t$.\n\nImplement both algorithms precisely as described: use $A_t^{-1}$ to precondition the gradient step at round $t$, then project onto $X$. After the step, update $A_{t+1}$ by adding $g_t g_t^\\top$ for the baseline algorithm, and add $g_t g_t^\\top$ only if $\\ell_t \\le \\tau$ for the filtered algorithm.\n\nUsing this setup, simulate the following test suite. Each test case is specified by the tuple of parameters $(T,d,\\eta,\\lambda,\\tau,M,D_{\\mathrm{end}},T_{\\mathrm{out}},\\sigma,\\text{seed})$:\n- Test 1 (outliers present, moderate threshold): $(200, 5, 0.2, 10^{-2}, 0.5, 30.0, 0.8, 20, 0.05, 0)$.\n- Test 2 (no filtering, identical behavior): $(200, 5, 0.2, 10^{-2}, 10^{9}, 30.0, 0.8, 20, 0.05, 1)$.\n- Test 3 (no outliers, very low threshold, aggressive steps): $(150, 5, 0.8, 10^{-2}, 10^{-8}, 0.0, 0.3, 0, 0.0, 2)$.\n- Test 4 (one-dimensional, outliers present): $(120, 1, 0.2, 10^{-2}, 0.1, 45.0, 0.9, 15, 0.02, 42)$.\n\nFor each test case, compute the static regret $R_T$ for the baseline algorithm and for the filtered algorithm. The required output for each test case is a boolean indicating whether the filtered algorithm achieves strictly smaller regret than the baseline algorithm, i.e., whether $R_T^{\\mathrm{filtered}} < R_T^{\\mathrm{baseline}}$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\").\n\nNo physical units are involved. Angles are not used. Percentages must not be used anywhere; all quantities are specified in real numbers.\n\nThe implementation must be a complete, runnable program in Python 3.12 using only the Python standard library and the specified libraries. The program must be self-contained and should not require any input from the user.",
            "solution": "The user wants me to solve an Online Convex Optimization problem.\n\n### Step 1: Extract Givens\n- **Domain**: Decision variable $x \\in X = [-1,1]^d \\subset \\mathbb{R}^d$.\n- **Rounds**: $t \\in \\{1, 2, \\dots, T\\}$.\n- **Loss Function**: At each round $t$, a convex loss function $f_t(x) = \\frac{1}{2}\\lVert x - a_t \\rVert_2^2$ is revealed, where $a_t \\in \\mathbb{R}^d$.\n- **Gradient**: $\\nabla f_t(x) = x - a_t$.\n- **Algorithm State**: Iterate $x_t \\in X$ and a symmetric positive definite matrix $A_t \\in \\mathbb{R}^{d \\times d}$.\n- **Initialization**: $x_1 = 0 \\in \\mathbb{R}^d$, $A_1 = \\lambda I_d$ where $\\lambda > 0$.\n- **Update Rule**: A preconditioned gradient step with step size $\\eta > 0$, followed by projection: $x_{t+1} = \\Pi_X(x_t - \\eta A_t^{-1} g_t)$, where $g_t = \\nabla f_t(x_t)$.\n- **Leverage Score**: $\\ell_t = g_t^\\top A_t^{-1} g_t$.\n- **Baseline Algorithm Preconditioner Update**: $A_{t+1} = A_t + g_t g_t^\\top$.\n- **Filtered Algorithm Preconditioner Update**: If $\\ell_t \\le \\tau$, then $A_{t+1} = A_t + g_t g_t^\\top$. Otherwise, $A_{t+1} = A_t$. $\\tau > 0$ is a threshold.\n- **Evaluation Metric**: Static regret $R_T = \\sum_{t=1}^T f_t(x_t) - \\min_{x \\in X} \\sum_{t=1}^T f_t(x)$.\n- **Optimal Static Comparator**: $x^\\star = \\Pi_X\\left( \\frac{1}{T} \\sum_{t=1}^T a_t \\right)$.\n- **Minimum Loss**: $\\sum_{t=1}^T f_t(x^\\star)$.\n- **Adversarial Sequence $\\{a_t\\}$ Generation**:\n    - A unit vector $u \\in \\mathbb{R}^d$ is generated pseudorandomly from a normal distribution with a given seed.\n    - For $t \\le T_{\\mathrm{out}}$: $a_t = M u$.\n    - For $t > T_{\\mathrm{out}}$: $a_t = \\left( \\frac{t - T_{\\mathrm{out}}}{T - T_{\\mathrm{out}}} \\right) D_{\\mathrm{end}}\\, u \\;+\\; \\xi_t$, where $\\xi_t$ is isotropic Gaussian noise with standard deviation $\\sigma$.\n    - Special Case: If $T = T_{\\mathrm{out}}$, the drift fraction is defined as zero, and $a_t = \\xi_t$ for all $t$.\n- **Test Cases**:\n    1. $(T=200, d=5, \\eta=0.2, \\lambda=10^{-2}, \\tau=0.5, M=30.0, D_{\\mathrm{end}}=0.8, T_{\\mathrm{out}}=20, \\sigma=0.05, \\text{seed}=0)$.\n    2. $(T=200, d=5, \\eta=0.2, \\lambda=10^{-2}, \\tau=10^{9}, M=30.0, D_{\\mathrm{end}}=0.8, T_{\\mathrm{out}}=20, \\sigma=0.05, \\text{seed}=1)$.\n    3. $(T=150, d=5, \\eta=0.8, \\lambda=10^{-2}, \\tau=10^{-8}, M=0.0, D_{\\mathrm{end}}=0.3, T_{\\mathrm{out}}=0, \\sigma=0.0, \\text{seed}=2)$.\n    4. $(T=120, d=1, \\eta=0.2, \\lambda=10^{-2}, \\tau=0.1, M=45.0, D_{\\mathrm{end}}=0.9, T_{\\mathrm{out}}=15, \\sigma=0.02, \\text{seed}=42)$.\n- **Required Output**: For each test case, a boolean value for the condition $R_T^{\\mathrm{filtered}} < R_T^{\\mathrm{baseline}}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is set within the standard framework of Online Convex Optimization (OCO).\n\n- **Scientifically Grounded**: The problem is based on established principles of online learning and convex optimization. The algorithms described are variants of Online Newton Step (ONS), a well-known second-order online learning method. The use of quadratic losses, gradient-based updates, and regret analysis are all standard in the field.\n- **Well-Posed**: The problem is mathematically precise. The algorithms are defined by explicit update rules. The data generation process is deterministic given a seed, which ensures a unique and reproducible simulation. The objective—comparing the regrets of two algorithms—is unambiguous. The formula for the optimal static comparator is correct for the given loss functions and domain. The one minor ambiguity regarding the case $T = T_{\\mathrm{out}}$ does not affect any of the provided test cases and can be interpreted consistently.\n- **Objective**: The problem is stated in formal, objective mathematical language, free from subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined computational task in the field of optimization methods. I will proceed with developing the solution.\n\n### Principle-Based Design of the Solution\n\nThe solution involves simulating two online learning algorithms over a sequence of rounds and comparing their performance using the static regret metric. The core of the problem lies in implementing the per-round updates for the iterate $x_t$ and the preconditioner matrix $A_t$.\n\n**1. Algorithmic Framework**\nFor each test case, we implement a simulation function that runs for $T$ rounds. This function is executed twice: once for the baseline algorithm and once for the filtered algorithm. The regrets from both runs are then compared.\n\n**2. State Variables and Initialization**\nThe state of the learner at the beginning of round $t$ is described by the pair $(x_t, A_t)$, where $x_t \\in \\mathbb{R}^d$ is the current decision and $A_t \\in \\mathbb{R}^{d \\times d}$ is the preconditioner matrix. For computational efficiency, we directly maintain and update the inverse matrix, $A_t^{-1}$.\nThe simulation begins with the initial state as specified:\n- $x_1 = 0$, the zero vector in $\\mathbb{R}^d$.\n- $A_1 = \\lambda I_d$, a scaled identity matrix. Therefore, the initial inverse is $A_1^{-1} = \\frac{1}{\\lambda} I_d$.\n\n**3. Adversarial Sequence Generation**\nThe sequence of vectors $\\{a_t\\}_{t=1}^T$, which defines the loss functions, is generated deterministically for each test case using a specific seed.\n- A fixed direction vector $u \\in \\mathbb{R}^d$ with $\\lVert u \\rVert_2 = 1$ is generated by sampling from a standard multivariate normal distribution and normalizing the result.\n- For the first $T_{\\mathrm{out}}$ rounds, the vectors are large-magnitude outliers: $a_t = M u$.\n- For subsequent rounds $t > T_{\\mathrm{out}}$, the vectors follow a drifting mean plus noise: $a_t = \\left( \\frac{t - T_{\\mathrm{out}}}{T - T_{\\mathrm{out}}} \\right) D_{\\mathrm{end}}\\, u \\;+\\; \\xi_t$. The noise $\\xi_t$ is sampled from an isotropic Gaussian distribution $\\mathcal{N}(0, \\sigma^2 I_d)$.\n\n**4. Per-Round Update Logic**\nThe simulation proceeds in a loop from $t=1$ to $T$. In each round $t$:\n1.  The iterate is $x_t$. The loss $f_t(x_t) = \\frac{1}{2}\\lVert x_t - a_t \\rVert_2^2$ is computed and added to a running total.\n2.  The gradient of the loss at $x_t$ is computed: $g_t = \\nabla f_t(x_t) = x_t - a_t$.\n3.  The next iterate, $x_{t+1}$, is found by applying the preconditioned update and projecting the result back onto the feasible domain $X$:\n    $$x_{t+1} = \\Pi_X\\left(x_t - \\eta A_t^{-1} g_t\\right)$$\n    The projection $\\Pi_X(\\cdot)$ onto the hypercube $X = [-1,1]^d$ is an element-wise clipping operation: for a vector $v$, $(\\Pi_X(v))_i = \\max(-1, \\min(1, v_i))$.\n4.  The preconditioner matrix for the next round, $A_{t+1}$, is constructed based on the information from round $t$.\n\n**5. Preconditioner Update Mechanism**\nThis step distinguishes the two algorithms. To maintain computational efficiency, the update is applied directly to the inverse matrix $A_t^{-1}$ using the Sherman-Morrison formula.\nThe Sherman-Morrison formula states that $(A + vv^\\top)^{-1} = A^{-1} - \\frac{A^{-1}vv^\\top A^{-1}}{1 + v^\\top A^{-1}v}$. Setting $A=A_t$ and $v=g_t$, we get:\n$$A_{t+1}^{-1} = A_t^{-1} - \\frac{A_t^{-1} g_t g_t^\\top A_t^{-1}}{1 + g_t^\\top A_t^{-1} g_t} = A_t^{-1} - \\frac{(A_t^{-1} g_t)(A_t^{-1} g_t)^\\top}{1 + \\ell_t}$$\n- **Baseline Algorithm**: This rank-1 update is performed in every round.\n- **Filtered Algorithm**: The update is conditional. First, the leverage score $\\ell_t = g_t^\\top A_t^{-1} g_t$ is calculated. The update is performed only if $\\ell_t \\le \\tau$. If $\\ell_t > \\tau$, the gradient $g_t$ is deemed an outlier, and the preconditioner is not updated, i.e., $A_{t+1}^{-1} = A_t^{-1}$. This mechanism prevents high-leverage gradients from distorting the geometry captured by the preconditioner.\n\n**6. Regret Calculation**\nUpon completion of the $T$ rounds, the static regret is calculated.\n1.  The total loss incurred by the algorithm is $L_{\\mathrm{alg}} = \\sum_{t=1}^T f_t(x_t)$.\n2.  The optimal static comparator is found by averaging all $a_t$ vectors and projecting onto the domain: $x^\\star = \\Pi_X\\left(\\frac{1}{T}\\sum_{t=1}^T a_t\\right)$.\n3.  The minimum possible cumulative loss in hindsight is $L^\\star = \\sum_{t=1}^T f_t(x^\\star)$.\n4.  The regret is the difference: $R_T = L_{\\mathrm{alg}} - L^\\star$.\n\nThis process yields the regrets $R_T^{\\mathrm{baseline}}$ and $R_T^{\\mathrm{filtered}}$, which are then compared to produce the final boolean output for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the online convex optimization problem for the given test suite.\n    \"\"\"\n    # Test cases parameters are tuples of:\n    # (T, d, eta, lambda, tau, M, D_end, T_out, sigma, seed)\n    test_cases = [\n        (200, 5, 0.2, 1e-2, 0.5, 30.0, 0.8, 20, 0.05, 0),\n        (200, 5, 0.2, 1e-2, 1e9, 30.0, 0.8, 20, 0.05, 1),\n        (150, 5, 0.8, 1e-2, 1e-8, 0.0, 0.3, 0, 0.0, 2),\n        (120, 1, 0.2, 1e-2, 0.1, 45.0, 0.9, 15, 0.02, 42),\n    ]\n\n    def project_hypercube(v):\n        \"\"\"Projects a vector onto the hypercube [-1, 1]^d.\"\"\"\n        return np.clip(v, -1.0, 1.0)\n\n    def run_simulation(params, is_filtered):\n        \"\"\"\n        Runs one simulation for a given set of parameters and algorithm type.\n\n        Args:\n            params (tuple): The parameters for the simulation.\n            is_filtered (bool): True to run the filtered algorithm, False for baseline.\n\n        Returns:\n            float: The computed static regret.\n        \"\"\"\n        T, d, eta, lam, tau, M, D_end, T_out, sigma, seed = params\n        \n        rng = np.random.default_rng(seed)\n        \n        # Generate the fixed direction vector u\n        u_unnormalized = rng.normal(size=d)\n        norm_u = np.linalg.norm(u_unnormalized)\n        if norm_u == 0: # Handle the unlikely event of a zero vector\n            u_unnormalized[0] = 1.0\n            norm_u = 1.0\n        u = u_unnormalized / norm_u\n\n        # Initialization\n        x = np.zeros(d)\n        A_inv = np.identity(d) / lam\n        \n        total_loss = 0.0\n        a_vectors = []\n        \n        # Main simulation loop\n        for t in range(1, T + 1):\n            # Generate the vector a_t for the current round's loss function\n            if t <= T_out:\n                a_t = M * u\n            else:\n                if T == T_out:\n                    # As per problem, the drift fraction is zero if T=T_out.\n                    # This case does not occur in tests, but is handled for robustness.\n                    drift_fraction = 0.0\n                else:\n                    drift_fraction = (t - T_out) / (T - T_out)\n                \n                xi_t = rng.normal(scale=sigma, size=d)\n                a_t = drift_fraction * D_end * u + xi_t\n            \n            a_vectors.append(a_t)\n            \n            # The current iterate is x_t (variable `x`)\n            # 1. Incur loss\n            loss_t = 0.5 * np.sum((x - a_t)**2)\n            total_loss += loss_t\n            \n            # 2. Compute gradient\n            g_t = x - a_t\n            \n            # 3. Determine next iterate x_{t+1}\n            step_dir = A_inv @ g_t\n            x_next = project_hypercube(x - eta * step_dir)\n            \n            # 4. Update the preconditioner matrix for the next round\n            leverage = g_t.T @ step_dir  # This is g_t^T A_t^-1 g_t\n            \n            update_matrix = True\n            if is_filtered:\n                if leverage > tau:\n                    update_matrix = False\n            \n            if update_matrix:\n                # Use the Sherman-Morrison formula to update A_inv\n                numerator_vec = step_dir  # A_t^-1 g_t\n                denominator = 1.0 + leverage\n                A_inv -= np.outer(numerator_vec, numerator_vec) / denominator\n\n            # Move to the next state\n            x = x_next\n            \n        # After T rounds, compute the static regret\n        # 1. Find the optimal static comparator x_star\n        a_mean = np.mean(a_vectors, axis=0)\n        x_star = project_hypercube(a_mean)\n        \n        # 2. Compute the minimum possible total loss\n        min_total_loss = 0.0\n        for a_t_i in a_vectors:\n            min_total_loss += 0.5 * np.sum((x_star - a_t_i)**2)\n            \n        # 3. Regret is the difference\n        regret = total_loss - min_total_loss\n        return regret\n\n    results = []\n    for params in test_cases:\n        regret_filtered = run_simulation(params, is_filtered=True)\n        regret_baseline = run_simulation(params, is_filtered=False)\n        results.append(regret_filtered < regret_baseline)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```"
        }
    ]
}