## 引言
在[凸优化](@entry_id:137441)领域，梯度是理解和求解可微问题的基石，它不仅指明了函数的[最速上升方向](@entry_id:140639)，还为我们提供了全局的线性下界。然而，在机器学习、信号处理和统计学等现代应用中，我们越来越多地遇到如[ReLU激活函数](@entry_id:138370)、[L1范数](@entry_id:143036)和最大值函数这类关键但不可微的[凸函数](@entry_id:143075)。在这些函数的“[尖点](@entry_id:636792)”或“拐角”处，经典梯度的定义失效，这似乎为优化带来了巨大的挑战。我们如何才能在没有梯度的情况下分析并优化这些函数呢？

本文正是为了解决这一知识鸿沟而引入了**次梯度 (subgradient)** 和**[次微分](@entry_id:175641) (subdifferential)** 的概念——这是对梯度思想的深刻推广。通过将梯度最本质的属性（即构造函数的全局下界）提炼出来作为定义，[次梯度](@entry_id:142710)理论成功地将微积分的强大分析能力扩展到了广阔的不可微[凸函数](@entry_id:143075)世界。

为了帮助您系统地掌握这一核心工具，本文将分为三个章节。在“**原理与机制**”中，我们将从次梯度的定义和几何直观出发，学习如何计算常见[不可微函数](@entry_id:143443)的[次微分](@entry_id:175641)，并掌握一套处理复杂函数的计算法则。接着，在“**应用与[交叉](@entry_id:147634)学科联系**”中，我们将探索[次梯度](@entry_id:142710)理论如何在优化、机器学习、信号处理等多个领域中发挥作用，揭示其在广义[最优性条件](@entry_id:634091)、算法设计以及建模[稀疏性](@entry_id:136793)与鲁棒性等问题上的强大威力。最后，在“**动手实践**”部分，您将通过解决一系列具体问题，亲手运用所学知识，加深对[次微分](@entry_id:175641)概念及其计算的理解。

## 原理与机制

在之前的章节中，我们探讨了[凸函数](@entry_id:143075)在优化中的核心地位，尤其是在其可微的情况下。梯度 $\nabla f(x)$ 不仅指明了函数在点 $x$ 处的[最速上升方向](@entry_id:140639)，还通过不等式 $f(y) \ge f(x) + \nabla f(x)^\top(y-x)$ 为整个函数提供了一个全局的线性下界估计。这个线性函数所代表的超平面，即 **[支撑超平面](@entry_id:274981)** (supporting hyperplane)，在几何上于点 $(x, f(x))$ 处“托住”了函数的上境图 (epigraph)，而从不穿越它。这一优美的性质是梯度方法[收敛性分析](@entry_id:151547)的基石。

然而，在现代优化、机器学习和信号处理的众多应用中，我们频繁遇到的许多重要凸函数并非处处可微。例如，[绝对值函数](@entry_id:160606) $f(x) = |x|$ 在 $x=0$ 处存在一个尖点 (kink)；$\ell_1$ 范数 $f(x) = \|x\|_1$ 在坐标轴上不光滑；而形如 $f(x) = \max\{f_1(x), f_2(x)\}$ 的函数在其组分函数相等的点上通常也是不可微的。在这些不可微的点上，梯度的概念不再适用。我们是否必须放弃梯度所带来的强大分析工具？或者，我们能否将梯度的概念进行扩展，以涵盖这些不可微的情形？

本章将引入 **[次梯度](@entry_id:142710) (subgradient)** 和 **[次微分](@entry_id:175641) (subdifferential)** 的概念，它们是对梯度思想的深刻推广。[次梯度](@entry_id:142710)保留了梯度作为全局下界估计构造者的核心特性，从而使我们能够在不要求[函数光滑性](@entry_id:161935)的前提下，建立一套完整而强大的凸[优化理论](@entry_id:144639)和算法。

### 次梯度的定义与几何直观

我们从可微凸函数的梯度不等式中汲取灵感。对于一个在点 $x$ 可微的凸函数 $f$，梯度 $\nabla f(x)$ 满足：
$$
f(y) \ge f(x) + \nabla f(x)^\top (y-x), \quad \forall y \in \text{dom}(f)
$$
这个不等式的本质在于，以梯度为斜率、经过点 $(x, f(x))$ 的[仿射函数](@entry_id:635019) $h(y) = f(x) + \nabla f(x)^\top (y-x)$ 是函数 $f$ 的一个全局下界。

现在，让我们将这个核心性质本身作为定义。对于一个[凸函数](@entry_id:143075) $f$（不一定可微），如果在点 $x$ 存在一个向量 $g$，使得该向量定义的[仿射函数](@entry_id:635019)能够全局地支撑起 $f$，那么我们就称 $g$ 为 $f$ 在点 $x$ 的一个次梯度。

**定义1：[次梯度](@entry_id:142710) (Subgradient)**

> 对于一个[凸函数](@entry_id:143075) $f: \mathbb{R}^n \to \mathbb{R}$ 和其定义域中的一点 $x$，若向量 $g \in \mathbb{R}^n$ 满足如下不等式：
> $$
> f(y) \ge f(x) + g^\top (y-x), \quad \forall y \in \text{dom}(f)
> $$
> 则称 $g$ 是 $f$ 在点 $x$ 的一个 **[次梯度](@entry_id:142710)**。

从几何上看，这个定义意味着超平面 $z = f(x) + g^\top (y-x)$ 是函数 $f$ 的上境图在点 $(x, f(x))$ 的一个[支撑超平面](@entry_id:274981)。

与梯度不同，一个函数在某一点的次梯度可能不是唯一的。在函数的“[尖点](@entry_id:636792)”处，我们可以想象有多个不同倾斜度的超平面都能“托住”这个点。所有这些可能的次梯度向量共同构成的集合，被称为函数的 **[次微分](@entry_id:175641)**。

**定义2：[次微分](@entry_id:175641) (Subdifferential)**

> 函数 $f$ 在点 $x$ 的 **[次微分](@entry_id:175641)**，记作 $\partial f(x)$，是 $f$ 在 $x$ 处所有次梯度的集合：
> $$
> \partial f(x) = \{ g \in \mathbb{R}^n \mid f(y) \ge f(x) + g^\top (y-x), \quad \forall y \in \text{dom}(f) \}
> $$

对于[凸函数](@entry_id:143075) $f$，其[次微分](@entry_id:175641) $\partial f(x)$ 是一个非空、凸且紧的集合。这一性质保证了我们总能找到至少一个[次梯度](@entry_id:142710)。

-   如果 $f$ 在点 $x$ **可微**，那么[次微分](@entry_id:175641)集合中仅包含一个元素，即该点的梯度：$\partial f(x) = \{\nabla f(x)\}$。
-   如果 $f$ 在点 $x$ **不可微**，[次微分](@entry_id:175641)集合将包含多个元素。

### 核心函数的[次微分](@entry_id:175641)计算

为了具体理解[次微分](@entry_id:175641)，我们将计算几个核心凸函数的[次微分](@entry_id:175641)。这些例子构成了我们分析更复杂函数的基础。

#### [绝对值函数](@entry_id:160606)与 ReLU 函数

我们从一维空间中最简单的不可微[凸函数](@entry_id:143075)——[绝对值函数](@entry_id:160606) $f(x)=|x|$ 开始。

-   **当 $x > 0$ 时**：$f(x)=x$。函数在此处可微，梯度为 $f'(x)=1$。因此，$\partial f(x) = \{1\}$。
-   **当 $x  0$ 时**：$f(x)=-x$。函数在此处可微，梯度为 $f'(x)=-1$。因此，$\partial f(x) = \{-1\}$。
-   **当 $x = 0$ 时**：$f(0)=0$。函数在此处不可微。我们根据定义寻找所有满足 $f(y) \ge f(0) + g(y-0)$ 的 $g$，即 $|y| \ge gy$ 对所有 $y \in \mathbb{R}$ 成立。
    -   若取 $y > 0$，不等式变为 $y \ge gy$，两边同除以 $y$ 得 $g \le 1$。
    -   若取 $y  0$，不等式变为 $-y \ge gy$，两边同除以 $y$（并反转不等号）得 $g \ge -1$。
    -   为了使不等式对所有 $y$ 成立，必须同时满足 $g \le 1$ 和 $g \ge -1$。因此，在 $x=0$ 处，[次微分](@entry_id:175641)是整个闭区间：$\partial f(0) = [-1, 1]$。

总结起来，[绝对值函数](@entry_id:160606)的[次微分](@entry_id:175641)为：
$$
\partial |x| = \begin{cases} \{-1\}   \text{if } x  0 \\ [-1, 1]   \text{if } x = 0 \\ \{1\}   \text{if } x > 0 \end{cases}
$$

与此密切相关的是在[神经网](@entry_id:276355)络中广泛应用的 **[修正线性单元](@entry_id:636721) (ReLU)** 函数 $f(x) = \max\{0, x\}$。通过类似的分析 ，我们可以得到其在 $x=0$ 处的[次微分](@entry_id:175641)为区间 $[0, 1]$，而在 $x0$ 和 $x>0$ 处分别为 $\{0\}$ 和 $\{1\}$。

#### $\mathbb{R}^n$ 空间中的范数

现在我们将分析推广到 $\mathbb{R}^n$ 空间中的重要函数——范数。

**[欧几里得范数](@entry_id:172687) ($\ell_2$-范数)**

考虑函数 $f(x) = \|x\|_2 = \sqrt{x^\top x}$。

-   **当 $x \neq 0$ 时**：函数可微，其梯度为 $\nabla f(x) = \frac{x}{\|x\|_2}$，即指向 $x$ 方向的[单位向量](@entry_id:165907)。因此，$\partial \|x\|_2 = \{\frac{x}{\|x\|_2}\}$。
-   **当 $x = 0$ 时**：函数在原点不可微。我们寻找满足 $\|y\|_2 \ge g^\top y$ 对所有 $y \in \mathbb{R}^n$ 成立的 $g$。
    -   根据 **柯西-[施瓦茨不等式](@entry_id:202153) (Cauchy-Schwarz inequality)**，$|g^\top y| \le \|g\|_2 \|y\|_2$。这意味着 $g^\top y \le \|g\|_2 \|y\|_2$。
    -   如果我们希望 $g^\top y \le \|y\|_2$ 对所有 $y$ 都成立，一个充分条件是 $\|g\|_2 \le 1$。
    -   反之，如果 $\|g\|_2 > 1$，我们可以选择一个特定的 $y$ 来破坏这个不等式。令 $y=g$，不等式变为 $\|g\|_2 \ge g^\top g = \|g\|_2^2$。由于 $\|g\|_2 > 1$，此式等价于 $1 \ge \|g\|_2$，与我们的假设矛盾。因此，任何 $\|g\|_2 > 1$ 的向量都不可能是次梯度。
    -   综上所述，在原点的[次微分](@entry_id:175641)是 $\mathbb{R}^n$ 中的闭单位球：$\partial \|0\|_2 = \{g \in \mathbb{R}^n \mid \|g\|_2 \le 1\}$。

**$\ell_1$-范数**

考虑函数 $f(x) = \|x\|_1 = \sum_{i=1}^n |x_i|$。由于该函数是可分的 (separable)，即各坐标分量之和，我们可以独立分析每个分量 $|x_i|$ 的次梯度。

-   对于任意点 $x$，其第 $i$ 个分量 $g_i$ 必须是 $|x_i|$ 在 $x_i$ 处的[次梯度](@entry_id:142710)。
-   结合我们对一维[绝对值函数](@entry_id:160606)的分析，我们有：
    -   如果 $x_i \neq 0$，则 $g_i = \text{sign}(x_i)$。
    -   如果 $x_i = 0$，则 $g_i \in [-1, 1]$。
-   因此，$\|x\|_1$ 的[次微分](@entry_id:175641)是所有满足上述条件的向量 $g$ 的集合。特别地，在 $x=0$ 处，由于所有分量都为零，每个 $g_i$ 都可以取 $[-1, 1]$ 内的任意值。这意味着 $\partial \|0\|_1 = \{g \in \mathbb{R}^n \mid |g_i| \le 1 \text{ for all } i\}$。这个集合等价于 $\{g \in \mathbb{R}^n \mid \|g\|_\infty \le 1\}$，即由 **$\ell_\infty$-范数** 定义的单位球，一个超立方体。

这个结果揭示了一个深刻的对偶关系：$\ell_1$-范数在原点的[次微分](@entry_id:175641)是 $\ell_\infty$-范数单位球。反之亦然。几何上，$\ell_\infty$-范数单位球（[超立方体](@entry_id:273913)）的顶点恰好是 $\ell_1$-范数[单位球](@entry_id:142558)（[交叉多胞体](@entry_id:748072)）各个晶面 (facet) 的外[法线](@entry_id:167651)向量。

**$\ell_\infty$-范数**

考虑函数 $f(x) = \|x\|_\infty = \max_i \{|x_i|\}$。这个函数可以看作是 $2n$ 个函数 $\{x_1, -x_1, \dots, x_n, -x_n\}$ 的逐点最大值。我们将在下一节系统学习这类函数的[次微分](@entry_id:175641)计算法则。现在，我们仅通过一个例子来建立直观认识。 考虑在 $\mathbb{R}^3$ 中，点 $\bar{x} = (2, -2, 1)$。此处 $f(\bar{x}) = \|x\|_\infty = \max\{|2|, |-2|, |1|\} = 2$。注意到，第一个和第二个分量的[绝对值](@entry_id:147688)都达到了最大值。我们称这两个分量是 **激活的 (active)**。可以证明，在这种情况下，[次微分](@entry_id:175641)是所有激活分量对应梯度的凸组合。
-   $|x_1|$ 在 $x_1=2$ 处的梯度是 $e_1 = (1, 0, 0)^\top$。
-   $|x_2|$ 在 $x_2=-2$ 处的梯度是 $-e_2 = (0, -1, 0)^\top$。
因此，$\partial f(\bar{x}) = \text{conv}\{e_1, -e_2\} = \{\theta e_1 + (1-\theta)(-e_2) \mid \theta \in [0, 1]\}$。这是一个连接点 $(1, 0, 0)^\top$ 和 $(0, -1, 0)^\top$ 的线段。

### [次微分](@entry_id:175641)的计算法则

正如梯度有其计算法则（和、积、链式法则），[次微分](@entry_id:175641)也有一套类似的“算术”法则，这使得计算复杂函数的[次微分](@entry_id:175641)成为可能。

#### [标量乘法](@entry_id:155971)与加法

-   **正[标量乘法](@entry_id:155971)**：若 $\alpha > 0$，则 $\partial (\alpha f)(x) = \alpha \partial f(x)$。
-   **加法法则**：对于两个[凸函数](@entry_id:143075) $f_1$ 和 $f_2$，其和的[次微分](@entry_id:175641)是其[次微分](@entry_id:175641)的 **[闵可夫斯基和](@entry_id:176841) (Minkowski sum)**：
    $$
    \partial(f_1 + f_2)(x) = \partial f_1(x) + \partial f_2(x) = \{ g_1 + g_2 \mid g_1 \in \partial f_1(x), g_2 \in \partial f_2(x) \}
    $$
    此法则在非常宽松的条件下成立（例如，只需函数在 $x$ 点附近连续即可），这对于所有我们关心的函数都适用。

**示例：弹性网 (Elastic Net)**
考虑在[统计学习](@entry_id:269475)中常用的弹性网正则项 $f(x) = \|x\|_2 + \lambda \|x\|_1$，其中 $\lambda > 0$。 这是一个典型的应用加法法则的例子。它的[次微分](@entry_id:175641)就是[欧几里得范数](@entry_id:172687)和 $\ell_1$-范数[次微分](@entry_id:175641)的[闵可夫斯基和](@entry_id:176841)。
当 $x \neq 0$ 时，$\partial \|x\|_2 = \{x/\|x\|_2\}$ 是一个单点集。因此，
$$
\partial f(x) = \left\{ \frac{x}{\|x\|_2} + g_1 \;\middle|\; g_1 \in \lambda \partial \|x\|_1 \right\}
$$
而在 $x = 0$ 时，
$$
\partial f(0) = \partial \|0\|_2 + \lambda \partial \|0\|_1 = \{ g_2 + g_1 \mid \|g_2\|_2 \le 1, \|g_1\|_\infty \le \lambda \}
$$

#### 逐点最大值法则

许多[不可微函数](@entry_id:143443)都可以表示为一族更[简单函数](@entry_id:137521)的逐点最大值。对于这类函数，有一个普适的[次微分](@entry_id:175641)计算法则。

**定理 (逐点最大值)**

 设 $f(x) = \max_{i \in I} f_i(x)$，其中 $I$ 是一个有限索引集，每个 $f_i$ 都是[凸函数](@entry_id:143075)。在点 $x$ 处的 **激活集 (active set)** 定义为 $A(x) = \{ i \in I \mid f_i(x) = f(x) \}$。则 $f$ 在 $x$ 处的[次微分](@entry_id:175641)是所有[激活函数](@entry_id:141784)[次微分](@entry_id:175641)的并集的[凸包](@entry_id:262864)：
 $$
 \partial f(x) = \text{conv} \left( \bigcup_{i \in A(x)} \partial f_i(x) \right)
 $$

这个法则是计算[次微分](@entry_id:175641)最强大的工具之一。 如果所有的 $f_i$ 都是可微的，那么 $\partial f_i(x) = \{\nabla f_i(x)\}$，法则简化为：
$$
\partial f(x) = \text{conv} \{ \nabla f_i(x) \mid i \in A(x) \}
$$
即激活函数梯度的凸组合。

**示例：分片线性函数**
考虑函数 $f(x) = \max\{-x_{1} + 2x_{2} + 3,\; 3x_{1} + x_{2} - 1,\; -2x_{1} - x_{2} + 4\}$。我们希望在点 $x^\star = (0, 1)$ 构造一个[支撑超平面](@entry_id:274981)。
首先，计算各分量函数在 $x^\star$ 的值：
- $f_1(0, 1) = -0 + 2(1) + 3 = 5$
- $f_2(0, 1) = 3(0) + 1 - 1 = 0$
- $f_3(0, 1) = -2(0) - 1 + 4 = 3$
$f(x^\star) = \max\{5, 0, 3\} = 5$。因此，激活集 $A(x^\star) = \{1\}$。
由于只有一个激活函数，且该函数是[仿射函数](@entry_id:635019)（因此可微），[次微分](@entry_id:175641)是一个单点集，等于该激活函数的梯度：
$$
\partial f(x^\star) = \{ \nabla f_1(x) \} = \left\{ \begin{pmatrix} -1 \\ 2 \end{pmatrix} \right\}
$$
我们可以选择唯一的[次梯度](@entry_id:142710) $g = (-1, 2)^\top$。由此构造的[支撑超平面](@entry_id:274981)由[仿射函数](@entry_id:635019) $h(x) = f(x^\star) + g^\top(x-x^\star)$ 定义，计算可得 $h(x) = 5 + (-1)(x_1-0) + 2(x_2-1) = -x_1+2x_2+3$。

#### 链式法则

[次微分](@entry_id:175641)的[链式法则](@entry_id:190743)比梯度的[链式法则](@entry_id:190743)更复杂，但对于特定结构，存在简洁的形式。一个重要的特例是与标量[函数复合](@entry_id:144881)。

**定理 (链式法则简化版)**

 设 $h(x) = g(f(x))$，其中 $f: \mathbb{R}^n \to \mathbb{R}$ 是一个[可微函数](@entry_id:144590)，$g: \mathbb{R} \to \mathbb{R}$ 是一个凸函数。则 $h$ 的[次微分](@entry_id:175641)由下式给出：
 $$
 \partial h(x) = \partial g(f(x)) \cdot \nabla f(x) = \{ s \cdot \nabla f(x) \mid s \in \partial g(f(x)) \}
 $$

**示例：Hinge 损失函数**
考虑函数 $h(x) = \max\{0, f(x)\}$，其中 $f$ 是可微[凸函数](@entry_id:143075)。这可以看作是 $h(x)=g(f(x))$，其中 $g(t)=\max\{0,t\}$ 是 ReLU 函数。
-   如果 $f(x) > 0$，则 $g$ 在 $f(x)$ 处可微，$\partial g(f(x))=\{1\}$，因此 $\partial h(x) = \{1 \cdot \nabla f(x)\} = \{\nabla f(x)\}$。
-   如果 $f(x)  0$，则 $g$ 在 $f(x)$ 处可微，$\partial g(f(x))=\{0\}$，因此 $\partial h(x) = \{0 \cdot \nabla f(x)\} = \{0\}$。
-   如果 $f(x) = 0$，则 $g$ 在 $f(x)=0$ 处不可微，$\partial g(0)=[0,1]$。此时，
    $$
    \partial h(x) = [0,1] \cdot \nabla f(x) = \{ \lambda \nabla f(x) \mid \lambda \in [0,1] \}
    $$
    这是连接原点和梯度向量 $\nabla f(x)$ 的线段。

### 次梯度在优化中的应用

[次梯度](@entry_id:142710)的真正威力在于它为不可微凸[优化问题](@entry_id:266749)提供了理论基石和算法思路。

#### [最优性条件](@entry_id:634091)

对于可微[凸函数](@entry_id:143075)，我们知道一个点 $x^\star$ 是全局最小值点的充要条件是 $\nabla f(x^\star) = 0$。这个条件可以完美地推广到不可微情形。

**定理 ([一阶最优性条件](@entry_id:634945))**

 对于一个[凸函数](@entry_id:143075) $f$（不一定可微），点 $x^\star$ 是 $f$ 的一个[全局最小值](@entry_id:165977)点，当且仅当 $0$ 向量属于其在该点的[次微分](@entry_id:175641)：
 $$
 x^\star \in \operatorname{argmin} f(x) \quad \iff \quad 0 \in \partial f(x^\star)
 $$

**证明与直观解释**：
-   ($\Leftarrow$) 如果 $0 \in \partial f(x^\star)$，根据[次梯度](@entry_id:142710)的定义，取次梯度 $g=0$，我们有 $f(y) \ge f(x^\star) + 0^\top(y-x^\star)$，即 $f(y) \ge f(x^\star)$ 对所有 $y$ 成立。这正是 $x^\star$ 是全局最小值的定义。
-   ($\Rightarrow$) 如果 $x^\star$ 是一个全局最小值点，那么 $f(y) \ge f(x^\star)$ 对所有 $y$ 成立。这可以写成 $f(y) \ge f(x^\star) + 0^\top(y-x^\star)$，这恰好说明 $g=0$ 满足[次梯度](@entry_id:142710)的定义，因此 $0 \in \partial f(x^\star)$。

这个条件极其重要。它将一个（可能困难的）[优化问题](@entry_id:266749)，转化为一个（可能更容易的）包含性问题：寻找一个点 $x$，使其对应的[次微分](@entry_id:175641)集合包含原点。

#### [次梯度](@entry_id:142710)与[方向导数](@entry_id:189133)

[次梯度](@entry_id:142710)与方向导数之间存在着深刻的联系。对于[凸函数](@entry_id:143075) $f$，其在点 $x$ 沿方向 $d$ 的（单边）方向导数为：
$$
f'(x; d) = \lim_{t \downarrow 0} \frac{f(x+td) - f(x)}{t}
$$
这个极限总是存在的。[次微分](@entry_id:175641)可以被看作是所有方向导数的“[集合表示](@entry_id:636781)”。具体而言，方向导数是[次微分](@entry_id:175641)在该方向上的[支撑函数](@entry_id:755667)。

**定理 (方向导数与[次微分](@entry_id:175641))**
 $$
 f'(x; d) = \sup_{g \in \partial f(x)} g^\top d
 $$

这个关系为计算[方向导数](@entry_id:189133)提供了一种方式。例如，在  中，我们计算了 $h(x) = \max\{0, x_1^2+x_2^2-x_1+3x_2\}$ 在 $x_0=(1,0)$ 沿方向 $d=(-2,1)$ 的[方向导数](@entry_id:189133)。我们已经求得 $\partial h(x_0) = \{\lambda (1,3)^\top \mid \lambda \in [0,1]\}$。因此，
$$
Dh(x_0; d) = \sup_{\lambda \in [0,1]} (\lambda (1,3)^\top)^\top (-2,1) = \sup_{\lambda \in [0,1]} \lambda(1 \cdot (-2) + 3 \cdot 1) = \sup_{\lambda \in [0,1]} \lambda
$$
其最大值为 $1$。

#### [次梯度](@entry_id:142710)方法

次梯度是[优化算法](@entry_id:147840)的核心。最基本的 **[次梯度](@entry_id:142710)方法 (subgradient method)** 是[梯度下降法](@entry_id:637322)的一个直接推广：
$$
x_{k+1} = x_k - \alpha_k g_k
$$
其中 $g_k$ 是从 $\partial f(x_k)$ 中选出的任意一个次梯度，$\alpha_k > 0$ 是步长。

需要特别注意的是，与梯度下降不同，[次梯度](@entry_id:142710)方向 $-g_k$ 不一定是 $f$ 在 $x_k$ 处的下降方向。即，我们完全有可能发现 $f(x_{k+1}) > f(x_k)$。然而，在合适的步长规则下（例如，$\alpha_k$ 不可求和但平方可求和，即 $\sum \alpha_k = \infty, \sum \alpha_k^2  \infty$），[次梯度](@entry_id:142710)方法可以保证收敛到最优值。这种收敛性源于每一步都保证了到最优[解集](@entry_id:154326)的距离有所减小。

当[次微分](@entry_id:175641)包含多个元素时，我们有选择 $g_k$ 的自由。不同的选择策略可能导致不同的算法轨迹。例如，在  中，当最小化 $f(x)=\max\{0,x\}$ 时，若迭代点恰好在 $x_k=0$（这是一个最优点），[次微分](@entry_id:175641)是 $[0,1]$。
-   若选择 $g_k=0$（最小范数次梯度），则 $x_{k+1}=0$，算法立即停止在最优点。
-   若选择 $g_k=1$，则 $x_{k+1}=-\alpha_k  0$，算法进入最优解集的另一部分。
两种选择都是有效的，并且都符合算法的收敛性保证。

### 高级主题与扩展实例

[次梯度](@entry_id:142710)的概念可以被推广到更广泛和抽象的空间，在现代优化中发挥着关键作用。

#### 指示函数与[法锥](@entry_id:272387)

一个将[约束优化](@entry_id:635027)问题转化为[无约束优化](@entry_id:137083)问题的强大工具是 **[指示函数](@entry_id:186820) (indicator function)**。对于一个非空闭[凸集](@entry_id:155617) $C \subset \mathbb{R}^n$，其[指示函数](@entry_id:186820) $\delta_C(x)$ 定义为：
$$
\delta_C(x) = \begin{cases} 0  \text{if } x \in C \\ +\infty  \text{if } x \notin C \end{cases}
$$
可以证明 $\delta_C(x)$ 是一个凸函数。其在点 $x \in C$ 的[次微分](@entry_id:175641)具有特殊的几何意义。

根据[次梯度](@entry_id:142710)定义，向量 $v$ 是 $\delta_C(x)$ 在 $x \in C$ 的次梯度，当且仅当 $\delta_C(y) \ge \delta_C(x) + v^\top(y-x)$ 对所有 $y$ 成立。由于 $\delta_C(x)=0$，这变为 $\delta_C(y) \ge v^\top(y-x)$。
-   若 $y \notin C$，$\delta_C(y)=+\infty$，不等式自然成立。
-   若 $y \in C$，$\delta_C(y)=0$，不等式变为 $0 \ge v^\top(y-x)$。

因此，$v$ 是[次梯度](@entry_id:142710)的条件是 $v^\top(y-x) \le 0$ 对所有 $y \in C$ 成立。这个向量集合被称为 $C$ 在 $x$ 处的 **[法锥](@entry_id:272387) (normal cone)**，记作 $N_C(x)$。
$$
\partial \delta_C(x) = N_C(x) = \{ v \in \mathbb{R}^n \mid v^\top(y-x) \le 0, \forall y \in C \}
$$
[法锥](@entry_id:272387)中的向量 $v$ 与从 $x$ 指向集合 $C$ 内部的任何[可行方向](@entry_id:635111) $y-x$ 都形成钝角。这为[约束优化](@entry_id:635027)问题的[最优性条件](@entry_id:634091)提供了几何语言。例如，最小化 $f(x)$ s.t. $x \in C$ 的问题等价于最小化 $f(x)+\delta_C(x)$。其[最优性条件](@entry_id:634091)为 $0 \in \partial(f+\delta_C)(x^\star) = \partial f(x^\star) + N_C(x^\star)$。

#### [矩阵空间](@entry_id:261335)中的次梯度

次梯度的概念同样适用于定义在矩阵空间上的函数。这在机器学习、控制理论和金融等领域至关重要。

**最大[特征值](@entry_id:154894)函数**
令 $\mathbb{S}^n$ 为 $n \times n$ [对称矩阵](@entry_id:143130)空间。函数 $f(X) = \lambda_{\max}(X)$，即矩阵 $X$ 的最大[特征值](@entry_id:154894)，是一个[凸函数](@entry_id:143075)。其在点 $X$ 的[次微分](@entry_id:175641)由与最大[特征值](@entry_id:154894) $\lambda_{\max}(X)$ 相关联的归一化[特征向量](@entry_id:151813)给出。 具体地，
$$
\partial \lambda_{\max}(X) = \text{conv} \{ vv^\top \mid \|v\|_2=1, Xv = \lambda_{\max}(X)v \}
$$
即，[次微分](@entry_id:175641)是由对应于最大[特征值](@entry_id:154894)的所有单位[特征向量](@entry_id:151813)构成的秩一[投影矩阵](@entry_id:154479)的[凸包](@entry_id:262864)。如果最大[特征值](@entry_id:154894)是唯一的，则[次微分](@entry_id:175641)是单点集 $\{vv^\top\}$，函数在此处可微。如果最大[特征值](@entry_id:154894)有重根，[次微分](@entry_id:175641)则是一个更复杂的集合。

**[核范数](@entry_id:195543)**
在[矩阵补全](@entry_id:172040)和[推荐系统](@entry_id:172804)等问题中，**核范数 (nuclear norm)** $f(X) = \|X\|_*$（即矩阵 $X$ 的奇异值之和）扮演着核心角色。它也是一个凸函数。其在 $X$ 处的[次微分](@entry_id:175641)可以通过 $X$ 的[奇异值分解 (SVD)](@entry_id:172448) $X=U\Sigma V^\top$ 来刻画。 设 $X$ 的秩为 $r$，$U \in \mathbb{R}^{m \times r}$ 和 $V \in \mathbb{R}^{n \times r}$ 分别是对应于非零奇异值的左[右奇异向量](@entry_id:754365)矩阵。则
$$
\partial \|X\|_* = \{ UV^\top + W \mid U^\top W = 0, WV=0, \|W\|_2 \le 1 \}
$$
其中 $\|W\|_2$ 是[谱范数](@entry_id:143091)。这个表达式表明，次梯度由两部分构成：一部分 $UV^\top$ 在 $X$ 的像空间和行空间上活动，另一部分 $W$ 在其[正交补](@entry_id:149922)空间上活动，并且受到[谱范数](@entry_id:143091)界的约束。特别地，在 $X=0$ 时，[次微分](@entry_id:175641)是整个[谱范数](@entry_id:143091)[单位球](@entry_id:142558) $\{W \mid \|W\|_2 \le 1\}$。

通过本章的学习，我们建立了[次梯度](@entry_id:142710)这一核心概念，它成功地将微积分的分析能力扩展到了广阔而重要的不可微[凸函数](@entry_id:143075)世界，为现代[优化理论](@entry_id:144639)与实践奠定了坚实的基础。