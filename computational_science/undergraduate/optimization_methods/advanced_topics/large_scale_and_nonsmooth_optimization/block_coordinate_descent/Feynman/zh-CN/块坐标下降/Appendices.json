{
    "hands_on_practices": [
        {
            "introduction": "优化算法的效率不仅取决于收敛所需的迭代次数，还取决于每次迭代的计算成本。本练习将引导你量化坐标下降法 (Coordinate Descent, CD) 相对于标准梯度下降法 (Gradient Descent) 的计算优势，特别是在数据科学中常见的大规模稀疏问题上。通过这个分析，你将揭示使用坐标级方法的一个核心动机。",
            "id": "3103367",
            "problem": "考虑最小化最小二乘目标函数 $f(x) = \\frac{1}{2}\\|A x - b\\|^{2}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个稀疏矩阵，其列为 $\\{a_{i}\\}_{i=1}^{n}$，且 $b \\in \\mathbb{R}^{m}$。令 $\\text{nnz}(A)$ 表示 $A$ 中非零元素的总数，并令 $\\text{nnz}_{i}$ 表示列 $a_{i}$ 中非零元素的数量。假设应用块大小为一的块坐标下降 (Block Coordinate Descent, BCD) 方法（也称为坐标下降 (Coordinate Descent, CD)），并始终维护残差 $r = A x - b$。在每次迭代中，从 $\\{1,\\dots,n\\}$ 中均匀随机地选择一个坐标 $i$，计算与 $a_{i}^{\\top} r$ 成比例的标量 $\\alpha_{i}$，并使用列 $a_{i}$ 更新 $x_{i}$ 和 $r$。假设每次迭代的算术成本由乘加运算的次数来衡量，并且与具有列结构 $a_{i}$ 的稀疏向量进行交互的成本与 $\\text{nnz}_{i}$ 成正比。还假设任何像 $\\|a_{i}\\|^{2}$ 这样的量都已预先计算，因此它们不计入每次迭代的成本。\n\n从 $f(x)$ 的基本定义和所描述运算的算术结构出发，推导出用 $\\{\\text{nnz}_{i}\\}_{i=1}^{n}$ 表示的 CD 每次迭代的期望算术成本，并将其与计算完整梯度 $\\nabla f(x) = A^{\\top}(A x - b)$ 的成本进行比较。将此比较表示为 CD 每次迭代的期望成本与完整梯度成本的比率，并简化为仅含 $n$ 的单个封闭形式表达式。无需四舍五入，也不涉及物理单位。提供该比率作为最终答案。",
            "solution": "问题要求推导和比较与最小化最小二乘目标函数 $f(x) = \\frac{1}{2}\\|A x - b\\|^{2}$ 相关的两个过程的计算成本。我们首先分析坐标下降 (CD) 方法单次迭代的计算成本，然后分析完整梯度计算的成本。\n\n首先，我们分析坐标下降法单次迭代中的运算。该方法涉及选择一个坐标 $i \\in \\{1, \\dots, n\\}$，并关于变量 $x_i$ 最小化 $f(x)$，同时保持所有其他坐标 $x_j$ (对于 $j \\neq i$) 固定。设当前迭代值为 $x$，对应的残差为 $r = A x - b$。$x_i$ 的更新可以写为 $x_{i, \\text{new}} = x_i + \\delta$。目标函数变为标量步长 $\\delta$ 的函数：\n$$h(\\delta) = f(x + \\delta e_i) = \\frac{1}{2}\\|A(x + \\delta e_i) - b\\|^2 = \\frac{1}{2}\\|(Ax - b) + \\delta A e_i\\|^2$$\n其中 $e_i$ 是第 $i$ 个标准基向量。由于 $A e_i$ 是 $A$ 的第 $i$ 列，记为 $a_i$，并且 $Ax-b=r$，我们有：\n$$h(\\delta) = \\frac{1}{2}\\|r + \\delta a_i\\|^2 = \\frac{1}{2}(r^\\top r + 2\\delta a_i^\\top r + \\delta^2 a_i^\\top a_i) = \\frac{1}{2}(\\|r\\|^2 + 2\\delta a_i^\\top r + \\delta^2 \\|a_i\\|^2)$$\n为了找到最优步长 $\\delta^*$，我们将 $h(\\delta)$ 关于 $\\delta$ 的导数设为零：\n$$\\frac{dh}{d\\delta} = a_i^\\top r + \\delta \\|a_i\\|^2 = 0$$\n量 $a_i^\\top r$ 是梯度的第 $i$ 个分量 $(\\nabla f(x))_i$。因此，最优步长为：\n$$\\delta^* = -\\frac{a_i^\\top r}{\\|a_i\\|^2}$$\nCD 迭代包括计算 $\\delta^*$，然后更新 $x_i$ 和残差 $r$。问题中说明了残差 $r$ 是始终维护的。更新步骤如下：\n1. $x_i \\leftarrow x_i + \\delta^*$\n2. $r \\leftarrow r + \\delta^* a_i$\n\n我们现在评估对于给定的坐标 $i$，这样一次迭代的算术成本，以乘加运算的次数来衡量。\n该成本基于这样的假设：涉及稀疏列 $a_i$ 的运算成本与其非零元素数量 $\\text{nnz}_i$ 成正比。量 $\\|a_i\\|^2$ 是预先计算的。\n\n- **计算 $\\delta^*$ 的成本**：主要计算是点积 $a_i^\\top r$。向量 $a_i$ 是稀疏的，有 $\\text{nnz}_i$ 个非零元素，而 $r \\in \\mathbb{R}^m$ 被视为稠密的。该点积需要 $\\text{nnz}_i$ 次乘法和 $\\text{nnz}_i - 1$ 次加法。这对应于大约 $\\text{nnz}_i$ 次乘加运算。随后除以预先计算的标量 $\\|a_i\\|^2$ 是一个单一操作，与点积相比，对于典型的 $\\text{nnz}_i$ 值来说其成本可以忽略不计。因此，计算分子 $a_i^\\top r$ 的成本是主要部分，我们将其计为 $\\text{nnz}_i$ 次运算。\n\n- **更新 $x_i$ 和 $r$ 的成本**：更新 $x_i \\leftarrow x_i + \\delta^*$ 是一个单一的标量加法，成本可以忽略不计。更新 $r \\leftarrow r + \\delta^* a_i$ 是一个稀疏向量更新（一个标量乘向量加运算，或 `axpy` 运算）。由于 $a_i$ 有 $\\text{nnz}_i$ 个非零项，此操作涉及将这 $\\text{nnz}_i$ 个项乘以 $\\delta^*$ 并加到 $r$ 的相应分量上。这需要 $\\text{nnz}_i$ 次乘法和 $\\text{nnz}_i$ 次加法，总共为 $\\text{nnz}_i$ 次乘加运算。\n\n在坐标 $i$ 上的单次 CD 迭代的总成本，记为 $C_{\\text{CD}, i}$，是这些成本的总和：\n$$C_{\\text{CD}, i} = (\\text{cost of } a_i^\\top r) + (\\text{cost of } r + \\delta^* a_i) = \\text{nnz}_i + \\text{nnz}_i = 2 \\cdot \\text{nnz}_i$$\n\n问题中说明，坐标 $i$ 是从 $\\{1, \\dots, n\\}$ 中均匀随机选择的。选择任何特定 $i$ 的概率是 $P(i) = \\frac{1}{n}$。CD 每次迭代的期望成本 $E[C_{\\text{CD}}]$ 是 $C_{\\text{CD}, i}$ 在所有可能的 $i$ 选择上的平均值：\n$$E[C_{\\text{CD}}] = \\sum_{i=1}^{n} P(i) \\cdot C_{\\text{CD}, i} = \\sum_{i=1}^{n} \\frac{1}{n} (2 \\cdot \\text{nnz}_i) = \\frac{2}{n} \\sum_{i=1}^{n} \\text{nnz}_i$$\n所有列的非零元素数量之和 $\\sum_{i=1}^{n} \\text{nnz}_i$，根据定义是矩阵 $A$ 中非零元素的总数，记为 $\\text{nnz}(A)$。因此，\n$$E[C_{\\text{CD}}] = \\frac{2 \\cdot \\text{nnz}(A)}{n}$$\n\n接下来，我们确定计算完整梯度 $\\nabla f(x) = A^{\\top}(A x - b) = A^{\\top}r$ 的成本。梯度的第 $i$ 个分量由 $A^\\top$ 的第 $i$ 行与 $r$ 的点积给出。$A^\\top$ 的第 $i$ 行是 $A$ 的第 $i$ 列，即 $a_i$。因此，$(\\nabla f(x))_i = a_i^\\top r$。\n要计算完整的梯度向量，我们必须为每个坐标 $i=1, \\dots, n$ 计算这个点积。计算第 $i$ 个分量的成本是 $\\text{nnz}_i$。计算完整梯度的总成本 $C_{\\nabla f}$ 是所有分量成本的总和：\n$$C_{\\nabla f} = \\sum_{i=1}^{n} (\\text{cost of } a_i^\\top r) = \\sum_{i=1}^{n} \\text{nnz}_i = \\text{nnz}(A)$$\n这等同于执行稀疏矩阵-向量乘积 $A^\\top r$，这需要对 $A^\\top$ (或 $A$) 中的每个非零项进行一次乘加运算。\n\n最后，我们计算 CD 每次迭代的期望成本与完整梯度成本的比率。\n$$\\text{Ratio} = \\frac{E[C_{\\text{CD}}]}{C_{\\nabla f}} = \\frac{\\frac{2 \\cdot \\text{nnz}(A)}{n}}{\\text{nnz}(A)}$$\n假设矩阵 $A$ 不是零矩阵，$\\text{nnz}(A) > 0$，我们可以通过消去 $\\text{nnz}(A)$ 来简化表达式：\n$$\\text{Ratio} = \\frac{2}{n}$$\n这个结果表明，一次 CD 迭代的期望成本比计算完整梯度要便宜 $\\frac{n}{2}$ 倍，这是关于坐标方法在解决大规模问题时效率的一个关键洞见。",
            "answer": "$$\\boxed{\\frac{2}{n}}$$"
        },
        {
            "introduction": "在理解了块坐标下降法 (Block Coordinate Descent, BCD) 的效率优势之后，下一步是设计有效的坐标更新规则。本练习将探索一种强大的 BCD 技术，即应用一维牛顿法来更新单个坐标。通过为机器学习的基石模型——逻辑斯蒂回归——推导此更新规则，你将学会如何利用二阶信息来潜在地加速每个坐标的收敛。",
            "id": "3103337",
            "problem": "考虑一个包含 $m$ 个带标签样本 $\\{(a_{i},y_{i})\\}_{i=1}^{m}$ 的数据集，其中 $a_{i}\\in\\mathbb{R}^{n}$ 且 $y_{i}\\in\\{-1,1\\}$。定义正则化逻辑回归目标函数\n$$\nf(x)\\;=\\;\\sum_{i=1}^{m}\\ln\\!\\big(1+\\exp(-y_{i}\\,a_{i}^{\\top}x)\\big)\\;+\\;\\frac{\\lambda}{2}\\,\\|x\\|^{2},\n$$\n其中 $x\\in\\mathbb{R}^{n}$，$ \\lambda>0$ 是一个正则化参数。在块坐标下降（BCD）方法中，一种常见的策略是每次更新单个坐标 $x_{j}$，同时保持所有其他坐标固定。构建这种一维更新的一种原则性方法是，使用海森矩阵的对角线近似（即用其对角线元素代替完整的海森矩阵），沿坐标 $j$ 应用牛顿法。\n\n从 $f(x)$ 的梯度和海森矩阵的基本定义出发，并且仅使用链式法则和关于逻辑斯谛函数的已知性质，当海森矩阵用其对角线近似时，推导坐标级牛顿更新 $x_{j}^{\\text{new}}$（$x_{j}$ 的更新值）的解析表达式，该表达式应以当前 $x$、数据 $\\{a_{i},y_{i}\\}_{i=1}^{m}$ 和 $\\lambda$ 来表示。您可以将逻辑斯谛函数定义为 $\\sigma(t)=\\frac{1}{1+\\exp(-t)}$ 并在您的推导中使用它。\n\n您的最终答案必须是 $x_{j}^{\\text{new}}$ 的单个闭式表达式。无需四舍五入。",
            "solution": "该问题要求推导在块坐标下降（BCD）框架下，针对正则化逻辑回归目标函数，单个坐标 $x_j$ 的更新规则。\n$$\nf(x) \\;=\\; \\sum_{i=1}^{m}\\ln\\!\\big(1+\\exp(-y_{i}\\,a_{i}^{\\top}x)\\big)\\;+\\;\\frac{\\lambda}{2}\\,\\|x\\|^{2}\n$$\n该更新规则被指定为沿坐标 $x_j$ 的一维牛顿步。对于单变量函数 $g(t)$，从 $t_k$ 到 $t_{k+1}$ 的牛顿步由 $t_{k+1} = t_k - \\frac{g'(t_k)}{g''(t_k)}$ 给出。在我们的情境中，我们正在相对于单个坐标 $x_j$ 优化 $f(x)$，同时保持所有其他坐标 $x_k$ (对于 $k \\neq j$) 固定。因此，$x_j$ 的更新为：\n$$\nx_{j}^{\\text{new}} = x_j - \\frac{\\frac{\\partial f}{\\partial x_j}}{\\frac{\\partial^2 f}{\\partial x_j^2}}\n$$\n此处，$\\frac{\\partial f}{\\partial x_j}$ 是 $f(x)$ 梯度的第 $j$ 个分量，记为 $(\\nabla f(x))_j$。项 $\\frac{\\partial^2 f}{\\partial x_j^2}$ 是 $f(x)$ 海森矩阵的第 $j$ 个对角元素，记为 $(\\nabla^2 f(x))_{jj}$。这与指定的使用海森矩阵对角线的方法完全对应。\n\n我们的推导分两步进行：首先，我们计算偏导数 $(\\nabla f(x))_j$；其次，我们计算二阶偏导数 $(\\nabla^2 f(x))_{jj}$。\n\n问题提供了逻辑斯谛 sigmoid 函数的定义，$\\sigma(t) = \\frac{1}{1+\\exp(-t)}$。我们将使用此函数及其性质。\n\n**步骤 1：计算梯度分量 $(\\nabla f(x))_j$**\n\n目标函数由两部分组成：损失项 $L(x) = \\sum_{i=1}^{m}\\ln(1+\\exp(-y_{i}\\,a_{i}^{\\top}x))$ 和正则化项 $R(x) = \\frac{\\lambda}{2}\\,\\|x\\|^{2} = \\frac{\\lambda}{2} \\sum_{k=1}^n x_k^2$。梯度是这两部分梯度之和。\n\n正则化项相对于 $x_j$ 的偏导数是：\n$$\n\\frac{\\partial R(x)}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left(\\frac{\\lambda}{2} \\sum_{k=1}^n x_k^2\\right) = \\frac{\\lambda}{2} (2x_j) = \\lambda x_j\n$$\n\n对于损失项，我们应用链式法则。令 $t_i(x) = y_i a_i^\\top x$。那么和中的第 $i$ 项是 $\\ln(1+\\exp(-t_i))$。\n$$\n\\frac{\\partial}{\\partial x_j} \\ln(1+\\exp(-t_i)) = \\frac{1}{1+\\exp(-t_i)} \\cdot \\frac{\\partial}{\\partial x_j}(1+\\exp(-t_i))\n$$\n$$\n= \\frac{1}{1+\\exp(-t_i)} \\cdot \\exp(-t_i) \\cdot \\frac{\\partial(-t_i)}{\\partial x_j}\n$$\n参数 $-t_i(x)$ 相对于 $x_j$ 的导数是：\n$$\n\\frac{\\partial(-t_i)}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} (-y_i a_i^\\top x) = \\frac{\\partial}{\\partial x_j} \\left(-y_i \\sum_{k=1}^n a_{ik}x_k\\right) = -y_i a_{ij}\n$$\n将此代回，我们得到：\n$$\n\\frac{\\partial}{\\partial x_j} \\ln(1+\\exp(-t_i)) = \\frac{\\exp(-t_i)}{1+\\exp(-t_i)} \\cdot (-y_i a_{ij}) = -\\frac{\\exp(-y_i a_i^\\top x)}{1+\\exp(-y_i a_i^\\top x)} y_i a_{ij}\n$$\n我们可以用 sigmoid 函数 $\\sigma(t)$ 来表示这个分数。注意到 $\\sigma(t) - 1 = \\frac{1}{1+\\exp(-t)} - 1 = \\frac{1 - (1+\\exp(-t))}{1+\\exp(-t)} = -\\frac{\\exp(-t)}{1+\\exp(-t)}$。\n因此，第 $i$ 个损失项的偏导数是：\n$$\n\\frac{\\partial}{\\partial x_j} \\ln(1+\\exp(-y_i a_i^\\top x)) = (\\sigma(y_i a_i^\\top x) - 1) y_i a_{ij}\n$$\n对所有数据点 $i=1, \\dots, m$求和，得到总损失 $L(x)$ 的偏导数：\n$$\n\\frac{\\partial L(x)}{\\partial x_j} = \\sum_{i=1}^m y_i a_{ij} (\\sigma(y_i a_i^\\top x) - 1)\n$$\n结合损失项和正则化项的导数，我们得到完整的梯度分量：\n$$\n(\\nabla f(x))_j = \\frac{\\partial f(x)}{\\partial x_j} = \\lambda x_j + \\sum_{i=1}^{m} y_{i} a_{ij} (\\sigma(y_{i} a_{i}^{\\top} x) - 1)\n$$\n\n**步骤 2：计算海森矩阵对角元素 $(\\nabla^2 f(x))_{jj}$**\n\n为了找到海森矩阵的对角元素，我们将 $(\\nabla f(x))_j$ 对 $x_j$ 求导：\n$$\n(\\nabla^2 f(x))_{jj} = \\frac{\\partial^2 f(x)}{\\partial x_j^2} = \\frac{\\partial}{\\partial x_j} \\left(\\lambda x_j + \\sum_{i=1}^{m} y_{i} a_{ij} (\\sigma(y_{i} a_{i}^{\\top} x) - 1)\\right)\n$$\n$$\n= \\lambda + \\sum_{i=1}^{m} y_{i} a_{ij} \\frac{\\partial}{\\partial x_j} \\sigma(y_{i} a_{i}^{\\top} x)\n$$\n我们需要 sigmoid 函数的导数 $\\sigma'(t)$。使用商法则或链式法则：\n$$\n\\sigma'(t) = \\frac{d}{dt} \\left( (1+\\exp(-t))^{-1} \\right) = -(1+\\exp(-t))^{-2} \\cdot (-\\exp(-t)) = \\frac{\\exp(-t)}{(1+\\exp(-t))^2}\n$$\n一个关于 $\\sigma'(t)$ 的著名恒等式是 $\\sigma'(t) = \\sigma(t)(1-\\sigma(t))$：\n$$\n\\sigma(t)(1-\\sigma(t)) = \\frac{1}{1+\\exp(-t)} \\left(1 - \\frac{1}{1+\\exp(-t)}\\right) = \\frac{1}{1+\\exp(-t)} \\frac{\\exp(-t)}{1+\\exp(-t)} = \\frac{\\exp(-t)}{(1+\\exp(-t))^2} = \\sigma'(t)\n$$\n对 $\\sigma(y_i a_i^\\top x)$ 使用这个恒等式和链式法则：\n$$\n\\frac{\\partial}{\\partial x_j} \\sigma(y_{i} a_{i}^{\\top} x) = \\sigma'(y_i a_i^\\top x) \\cdot \\frac{\\partial}{\\partial x_j}(y_i a_i^\\top x) = \\sigma'(y_i a_i^\\top x) \\cdot (y_i a_{ij})\n$$\n将此代入二阶导数的表达式中：\n$$\n(\\nabla^2 f(x))_{jj} = \\lambda + \\sum_{i=1}^{m} y_{i} a_{ij} \\left( \\sigma'(y_i a_i^\\top x) \\cdot y_i a_{ij} \\right)\n$$\n$$\n= \\lambda + \\sum_{i=1}^{m} y_{i}^2 a_{ij}^2 \\sigma'(y_i a_i^\\top x)\n$$\n由于 $y_i \\in \\{-1, 1\\}$，我们有 $y_i^2 = 1$。使用 $\\sigma'(t)$ 的恒等式：\n$$\n(\\nabla^2 f(x))_{jj} = \\lambda + \\sum_{i=1}^{m} a_{ij}^2 \\sigma(y_{i}a_{i}^{\\top}x)(1 - \\sigma(y_{i}a_{i}^{\\top}x))\n$$\n\n**步骤 3：组合牛顿更新规则**\n\n现在我们将梯度分量和海森矩阵对角元素的表达式代入牛顿更新公式：\n$$\nx_{j}^{\\text{new}} = x_j - \\frac{(\\nabla f(x))_j}{(\\nabla^2 f(x))_{jj}}\n$$\n$$\nx_{j}^{\\text{new}} = x_j - \\frac{\\lambda x_j + \\sum_{i=1}^{m} a_{ij} y_{i} (\\sigma(y_{i} a_{i}^{\\top} x) - 1)}{\\lambda + \\sum_{i=1}^{m} a_{ij}^2 \\sigma(y_{i}a_{i}^{\\top}x)(1 - \\sigma(y_{i} a_{i}^{\\top} x))}\n$$\n这就是坐标级牛顿更新的最终闭式表达式。",
            "answer": "$$\n\\boxed{x_j - \\frac{\\lambda x_j + \\sum_{i=1}^{m} a_{ij} y_{i} (\\sigma(y_{i} a_{i}^{\\top} x) - 1)}{\\lambda + \\sum_{i=1}^{m} a_{ij}^2 \\sigma(y_{i}a_{i}^{\\top}x)(1 - \\sigma(y_{i} a_{i}^{\\top} x))}}\n$$"
        },
        {
            "introduction": "理论提供了指导，但经验证据能建立真正的直觉。这个编程练习将挑战你实现块坐标下降法 (BCD) 和梯度下降法 (GD)，并观察它们在一个难度可调的二次规划问题上的表现。通过比较当问题变得更加病态 (ill-conditioned) 时两种方法的迭代次数，你将对每种方法的鲁棒性和收敛特性获得切实的理解。",
            "id": "3103362",
            "problem": "给定一个 $n$ 个变量的二次目标函数 $f(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x$，其中对称矩阵 $Q \\in \\mathbb{R}^{n \\times n}$ 定义为 $Q = \\operatorname{diag}(\\kappa, 1, \\dots, 1) + \\rho \\mathbf{1}\\mathbf{1}^\\top$，$\\kappa > 0$，$\\rho \\ge 0$，$\\mathbf{1} \\in \\mathbb{R}^n$ 是全一向量，并且 $b = \\mathbf{1}$。考虑两种迭代方法：梯度下降法 (GD) 和块坐标下降法 (BCD)。在块坐标下降法 (BCD) 中，使用大小为 $1$ 的循环块（即标准坐标下降法），每次沿一个坐标进行精确最小化。在梯度下降法 (GD) 中，使用等于 $Q$ 的最大特征值倒数的固定步长。\n\n您的任务是编写一个完整的、可运行的程序，对于每个指定的测试用例，该程序需要构造 $Q$ 和 $b$，计算 $f(x)$ 的精确极小点 $x^\\star$，然后比较 GD 和 BCD 在参数 $\\kappa$ 增长时达到规定精度所需的迭代次数。请使用以下精确的算法规范：\n\n- 两种方法的初始化：从 $x^{(0)} = \\mathbf{0}$ 开始。\n- 对于梯度下降法 (GD)：在每次迭代 $t$ 中，计算梯度 $\\nabla f(x^{(t)}) = Q x^{(t)} - b$ 并更新 $x^{(t+1)} = x^{(t)} - \\alpha \\nabla f(x^{(t)})$，其中 $\\alpha = 1 / \\lambda_{\\max}(Q)$，$\\lambda_{\\max}(Q)$ 表示 $Q$ 的最大特征值。\n- 对于块大小为 $1$ 的块坐标下降法 (BCD)：对 $j = 1, 2, \\dots, n$ 进行循环坐标更新，对于每个坐标 $j$，在保持其他坐标固定的同时，对 $f$ 关于 $x_j$ 进行最小化。这种精确的一维最小化将第 $j$ 个偏导数设为零，产生更新 $x_j \\leftarrow \\left(b_j - \\sum_{k \\ne j} Q_{jk} x_k \\right) / Q_{jj}$。将按升序对所有 $n$ 个坐标的一次完整扫描记为一次迭代。\n- 两种方法的停止准则：当相对次优性 $(f(x) - f(x^\\star)) / (f(x^{(0)}) - f(x^\\star)) \\le \\varepsilon$ 时停止，容差 $\\varepsilon = 10^{-6}$。\n- 最大迭代次数：对于 GD，强制上限为 $50{,}000$ 次迭代；对于 BCD，强制上限为 $5{,}000$ 次完整扫描。如果未在上限内满足停止准则，则返回上限值。\n\n使用以下参数三元组 $(n, \\kappa, \\rho)$ 的测试套件：\n\n- 情况 1：$(n, \\kappa, \\rho) = (50, 1, 0)$。\n- 情况 2：$(n, \\kappa, \\rho) = (50, 10, 0.1)$。\n- 情况 3：$(n, \\kappa, \\rho) = (50, 1000, 0.1)$。\n- 情况 4：$(n, \\kappa, \\rho) = (5, 100, 10)$。\n\n您的程序必须为每个情况计算并返回两个整数：GD 的迭代次数（梯度步数）和 BCD 的迭代次数（完整扫描次数）。最终输出必须将所有测试用例的结果汇总到一行，格式为方括号内由逗号分隔的列表，无空格，其中每个元素是每个情况的数对，例如，$[[i_{1,\\mathrm{GD}},i_{1,\\mathrm{BCD}}],[i_{2,\\mathrm{GD}},i_{2,\\mathrm{BCD}}],[i_{3,\\mathrm{GD}},i_{3,\\mathrm{BCD}}],[i_{4,\\mathrm{GD}},i_{4,\\mathrm{BCD}}]]$。\n\n不涉及物理单位。不涉及角度。不使用百分比；容差是一个纯小数。您的程序应生成一行输出，其中包含一个用方括号括起来的、以逗号分隔的、无空格的列表，格式与 $[[i_{1,\\mathrm{GD}},i_{1,\\mathrm{BCD}}],[i_{2,\\mathrm{GD}},i_{2,\\mathrm{BCD}}],[i_{3,\\mathrm{GD}},i_{3,\\mathrm{BCD}}],[i_{4,\\mathrm{GD}},i_{4,\\mathrm{BCD}}]]$ 完全一样。",
            "solution": "该问题要求我们实现梯度下降法 (GD) 和块坐标下降法 (BCD)，并比较它们在一个参数化二次目标函数上的收敛性能。以下是解决此问题的完整步骤和实现。\n\n**1. 准备工作**\n目标函数 $f(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x$ 是一个严格凸的二次型，因为其海森矩阵 $Q$ 是正定的。唯一的全局极小点 $x^\\star$ 可通过将梯度 $\\nabla f(x) = Qx - b$ 设为零来找到，这给出了线性系统 $Qx = b$。解为 $x^\\star = Q^{-1}b$。\n\n对于停止准则，我们需要函数在最优点 $f(x^\\star)$ 和初始点 $f(x^{(0)})$ 的值。\n初始点为 $x^{(0)} = \\mathbf{0}$，因此 $f(x^{(0)}) = \\tfrac{1}{2} \\mathbf{0}^\\top Q \\mathbf{0} - b^\\top \\mathbf{0} = 0$。\n最优值为 $f(x^\\star) = \\tfrac{1}{2} (x^\\star)^\\top Q x^\\star - b^\\top x^\\star$。由于 $Qx^\\star = b$，这可以简化为 $f(x^\\star) = \\tfrac{1}{2} (x^\\star)^\\top b - b^\\top x^\\star = -\\tfrac{1}{2} b^\\top x^\\star$。\n在每次迭代 $t$ 的开始，对当前迭代点 $x^{(t)}$ 检查停止准则：\n$$ \\frac{f(x^{(t)}) - f(x^\\star)}{f(x^{(0)}) - f(x^\\star)} \\le \\varepsilon $$\n其中 $\\varepsilon = 10^{-6}$。分母为 $f(x^{(0)}) - f(x^\\star) = 0 - (-\\tfrac{1}{2}b^\\top x^\\star) = \\tfrac{1}{2}b^\\top x^\\star$。该值在整个迭代过程中是恒定的。\n\n**2. 梯度下降法 (GD) 算法**\nGD 算法使用更新规则生成一个迭代点序列 $x^{(t)}$：\n$$ x^{(t+1)} = x^{(t)} - \\alpha \\nabla f(x^{(t)}) = x^{(t)} - \\alpha (Q x^{(t)} - b) $$\n步长 $\\alpha$ 固定为 $Q$ 的最大特征值的倒数，$\\alpha = 1/\\lambda_{\\max}(Q)$。对于二次目标函数，这个选择是标准的，并保证收敛。必须为每个测试用例计算 $\\lambda_{\\max}(Q)$ 的值。一次迭代包括一次梯度计算和一次更新步骤。\n\n**3. 块坐标下降法 (BCD) 算法**\nBCD 算法通过循环遍历坐标，每次沿一个坐标方向迭代地最小化目标函数。对于二次函数，这种最小化可以精确完成。一次完整迭代（或扫描）包括按顺序更新所有 $n$ 个坐标，从 $j=1, 2, \\dots, n$。\n对于第 $j$ 个坐标的更新，保持所有其他坐标 $x_k$ ($k \\ne j$) 固定，是通过求解 $\\frac{\\partial f}{\\partial x_j} = 0$ 导出的。\n偏导数为 $(\\nabla f(x))_j = (Qx)_j - b_j = \\sum_{k=1}^n Q_{jk}x_k - b_j = 0$。\n分离出新的 $x_j$（我们称之为 $x_j^{\\text{new}}$），我们得到：\n$$ Q_{jj} x_j^{\\text{new}} + \\sum_{k \\ne j} Q_{jk} x_k = b_j $$\n$$ x_j^{\\text{new}} = \\frac{1}{Q_{jj}} \\left( b_j - \\sum_{k \\ne j} Q_{jk} x_k \\right) $$\n在循环 BCD 实现中，当更新 $x_j$ 时，对于 $k < j$ 的 $x_k$ 值已在当前扫描中更新，而对于 $k > j$ 的值则来自上一次扫描。$Q$ 的对角线元素是 $Q_{11} = \\kappa + \\rho$ 和 $Q_{jj} = 1 + \\rho$（对于 $j > 1$）。由于 $\\kappa>0$ 和 $\\rho\\ge0$，所有的 $Q_{jj}>0$，所以除法是良定义的。\n\n**4. 实现计划与代码**\n对于每个测试用例，我们构造 $Q$ 和 $b$，计算 $x^\\star$ 和 $f(x^\\star)$，然后分别运行 GD 和 BCD 算法直到满足停止准则或达到最大迭代次数。\n\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem for a series of test cases, comparing\n    Gradient Descent (GD) and Block Coordinate Descent (BCD).\n    \"\"\"\n\n    # Parameter triples (n, kappa, rho) for each test case.\n    test_cases = [\n        (50, 1, 0),\n        (50, 10, 0.1),\n        (50, 1000, 0.1),\n        (5, 100, 10)\n    ]\n\n    results = []\n    \n    # Algorithmic constants\n    EPSILON = 1e-6\n    MAX_ITER_GD = 50000\n    MAX_ITER_BCD = 5000\n\n    for n, kappa, rho in test_cases:\n        # 1. Construct Q and b\n        b = np.ones(n)\n        diag_D = np.array([kappa] + [1.0] * (n - 1))\n        D = np.diag(diag_D)\n        ones_vec = np.ones((n, 1))\n        Q = D + rho * (ones_vec @ ones_vec.T)\n\n        # 2. Compute exact minimizer x_star and f_star\n        x_star = np.linalg.solve(Q, b)\n        f_star = 0.5 * x_star.T @ Q @ x_star - b.T @ x_star\n\n        # 3. Setup for iterations\n        x0 = np.zeros(n)\n        f0 = 0.0\n        \n        f0_minus_f_star = f0 - f_star\n        \n        if f0_minus_f_star  1e-12:\n            results.append([0, 0])\n            continue\n            \n        # --- Gradient Descent (GD) ---\n        iters_gd = MAX_ITER_GD\n        x_gd = x0.copy()\n        \n        eigvals = np.linalg.eigvalsh(Q)\n        lambda_max = np.max(eigvals)\n        alpha = 1.0 / lambda_max\n        \n        for i in range(MAX_ITER_GD + 1):\n            f_gd = 0.5 * x_gd.T @ Q @ x_gd - b.T @ x_gd\n            rel_subopt = (f_gd - f_star) / f0_minus_f_star\n            \n            if rel_subopt = EPSILON:\n                iters_gd = i\n                break\n            \n            if i  MAX_ITER_GD:\n                grad = Q @ x_gd - b\n                x_gd -= alpha * grad\n\n        # --- Block Coordinate Descent (BCD) ---\n        iters_bcd = MAX_ITER_BCD\n        x_bcd = x0.copy()\n        Q_diag = np.diag(Q)\n\n        for i in range(MAX_ITER_BCD + 1):\n            f_bcd = 0.5 * x_bcd.T @ Q @ x_bcd - b.T @ x_bcd\n            rel_subopt = (f_bcd - f_star) / f0_minus_f_star\n\n            if rel_subopt = EPSILON:\n                iters_bcd = i\n                break\n\n            if i  MAX_ITER_BCD:\n                for j in range(n):\n                    sum_term = Q[j, :] @ x_bcd - Q_diag[j] * x_bcd[j]\n                    x_bcd[j] = (b[j] - sum_term) / Q_diag[j]\n\n        results.append([iters_gd, iters_bcd])\n\n    # Format the final output string exactly as specified\n    inner_parts = [f\"[{gd},{bcd}]\" for gd, bcd in results]\n    final_output = f\"[{','.join(inner_parts)}]\"\n    print(final_output)\n\n# This block is for generating the answer string. It's not part of the final code.\n# if __name__ == '__main__':\n#     solve()\n```",
            "answer": "[[1,1],[356,5],[36774,27],[1762,37]]"
        }
    ]
}