{
    "hands_on_practices": [
        {
            "introduction": "要真正理解道格拉斯-拉赫福德（DR）算法，最好的方法莫过于通过一个简单的二维问题来观察其几何行为。这个练习将引导你手动完成DR算法的迭代步骤，即投影与反射，从而建立对算法如何寻找解的核心直觉 。我们将使用两个半空间作为示例，其投影易于可视化和计算，我们还能从中一窥算法在处理多面体集时的有限步收敛特性。",
            "id": "3122413",
            "problem": "考虑在 $\\mathbb{R}^{2}$ 中寻找两个闭凸集 $C$ 和 $D$ 交集中的一个点的问题，其中 $C$ 和 $D$ 是如下定义的半空间\n$$C := \\{(x_{1},x_{2}) \\in \\mathbb{R}^{2} : x_{2} \\le 0\\}, \\quad D := \\{(x_{1},x_{2}) \\in \\mathbb{R}^{2} : x_{1} + x_{2} \\ge 0\\}。$$\n从初始点 $x^{0} = (1,-3)$ 开始，应用标准的 Douglas–Rachford 分裂（DRS）算法求解该可行性问题。该问题被表述为在单位参数下，最小化 $C$ 和 $D$ 的指示函数之和。仅使用基本定义（邻近算子、到半空间的投影以及由投影定义的反射），解析计算前三次 Douglas–Rachford 迭代的结果 $x^{1}$、$x^{2}$ 和 $x^{3}$。解释为什么在该几何排列的 $C$ 和 $D$ 以及该初始化的条件下，该序列会表现出有限步收敛。将 $x^{3}$ 的值以向量形式作为最终答案。无需四舍五入，不涉及单位。",
            "solution": "该问题是使用 Douglas-Rachford 分裂（DRS）算法，在两个闭凸集 $C$ 和 $D$ 的交集中寻找一个点。该问题被表述为最小化两个集合的指示函数之和 $\\iota_C(x) + \\iota_D(x)$。这些集合已事先被验证适用于此应用。没有矛盾或信息缺失。因此，该问题是有效的。\n\n集合由以下公式给出：\n$$C := \\{(x_{1},x_{2}) \\in \\mathbb{R}^{2} : x_{2} \\le 0\\}$$\n$$D := \\{(x_{1},x_{2}) \\in \\mathbb{R}^{2} : x_{1} + x_{2} \\ge 0\\}$$\n\n用于寻找两个集合 $C$ 和 $D$ 交集中一个点的 Douglas-Rachford 分裂算法可以表示为以下迭代形式：\n$$x^{k+1} = T(x^k)$$\n其中算子 $T$ 对于步长参数 $\\lambda$ 定义为：\n$$T(x) = x + \\lambda (P_C(2P_D(x) - x) - P_D(x))$$\n问题指定了单位参数，因此我们设置 $\\lambda=1$。迭代公式为：\n$$x^{k+1} = x^k + P_C(2P_D(x^k) - x^k) - P_D(x^k)$$\n给定起始点 $x^{0} = (1, -3)$。为了计算迭代结果，我们首先需要投影算子 $P_C$ 和 $P_D$ 的解析表达式。\n\n**1. 投影算子**\n\n投影 $P_S(x)$ 给出在闭凸集 $S$ 中，在欧几里得范数下距离 $x$ 最近的唯一一个点。\n\n- **到 $C$ 的投影：** 集合 $C$ 是下半平面，包含 $x_1$ 轴。对于一个点 $x = (x_1, x_2)$，如果 $x_2 \\le 0$，则该点已在 $C$ 中，所以 $P_C(x) = x$。如果 $x_2 > 0$，则 $C$ 中最近的点是通过将第二个分量设为 $0$ 来找到的。\n$$P_C(x_1, x_2) = (x_1, \\min(x_2, 0))$$\n\n- **到 $D$ 的投影：** 集合 $D$ 是由不等式 $x_1 + x_2 \\ge 0$ 定义的半空间。其边界是直线 $x_1 + x_2 = 0$。对于一个点 $x = (x_1, x_2)$，如果 $x_1 + x_2 \\ge 0$，则 $x \\in D$ 且 $P_D(x) = x$。如果 $x_1 + x_2  0$，投影是通过从 $x$ 沿边界的法向量 $n = (1, 1)$ 方向移动找到的。对于半空间 $a^T z \\ge b$，其投影公式为：如果 $a^T x  b$，则 $P(x) = x - \\frac{a^T x - b}{\\|a\\|^2}a$。对于 $D$，我们有 $a=(1,1)^T$ 和 $b=0$。\n$$P_D(x_1, x_2) = \\begin{cases} (x_1, x_2)  \\text{if } x_1 + x_2 \\ge 0 \\\\ (x_1, x_2) - \\frac{x_1+x_2}{1^2+1^2}(1,1) = \\left(\\frac{x_1-x_2}{2}, \\frac{x_2-x_1}{2}\\right)  \\text{if } x_1 + x_2  0 \\end{cases}$$\n\n**2. 迭代计算**\n\n我们从 $x^0 = (1, -3)$ 开始。我们将中间投影 $P_D(x^k)$ 记为 $y^k$。迭代公式为 $x^{k+1} = x^k + P_C(2y^k - x^k) - y^k$。\n\n**迭代 $k=0$：**\n- **初始点：** $x^0 = (1, -3)$。\n- **计算 $y^0 = P_D(x^0)$：**\n$x^0$ 的分量之和为 $1 + (-3) = -2$，小于 $0$。我们使用点在 $D$ 外部的投影公式：\n$$y^0 = P_D(1, -3) = \\left(\\frac{1 - (-3)}{2}, \\frac{-3 - 1}{2}\\right) = \\left(\\frac{4}{2}, \\frac{-4}{2}\\right) = (2, -2)$$\n- **计算 $P_C$ 的参数：**\n参数为 $2y^0 - x^0$。这等价于将 $x^0$ 关于集合 $D$ 做反射，即 $R_D(x^0) = 2P_D(x^0) - x^0$。\n$$2y^0 - x^0 = 2(2, -2) - (1, -3) = (4, -4) - (1, -3) = (3, -1)$$\n- **应用 $P_C$：**\n令 $v^0 = (3, -1)$。$v^0$ 的第二个分量是 $-1$，小于或等于 $0$。因此，$v^0 \\in C$，其投影是它自身：\n$$P_C(3, -1) = (3, -1)$$\n- **计算 $x^1$：**\n$$x^1 = x^0 + P_C(2y^0 - x^0) - y^0 = (1, -3) + (3, -1) - (2, -2)$$\n$$x^1 = (1+3-2, -3-1-(-2)) = (2, -4+2) = (2, -2)$$\n\n因此，第一次迭代的结果是 $x^1 = (2, -2)$。\n\n**迭代 $k=1$：**\n- **从 $x^1 = (2, -2)$ 开始。**\n- **计算 $y^1 = P_D(x^1)$：**\n分量之和为 $2 + (-2) = 0$。由于 $0 \\ge 0$，点 $x^1$ 在 $D$ 中。因此，投影是该点本身：\n$$y^1 = P_D(2, -2) = (2, -2)$$\n- **计算 $P_C$ 的参数：**\n$$2y^1 - x^1 = 2(2, -2) - (2, -2) = (4, -4) - (2, -2) = (2, -2)$$\n- **应用 $P_C$：**\n令 $v^1 = (2, -2)$。第二个分量是 $-2 \\le 0$，所以 $v^1 \\in C$。\n$$P_C(2, -2) = (2, -2)$$\n- **计算 $x^2$：**\n$$x^2 = x^1 + P_C(2y^1 - x^1) - y^1 = (2, -2) + (2, -2) - (2, -2) = (2, -2)$$\n\n第二次迭代的结果是 $x^2 = (2, -2)$。\n\n**迭代 $k=2$：**\n由于 $x^2 = x^1$，点 $(2, -2)$ 是 DRS 算子 $T$ 的一个不动点。所有后续的迭代结果都将相同。\n$$x^3 = T(x^2) = T(x^1) = x^1 = (2, -2)$$\n因此，前三次迭代的结果是 $x^1 = (2, -2)$，$x^2 = (2, -2)$ 和 $x^3 = (2, -2)$。\n\n**3. 有限步收敛的解释**\n\n序列 $(x^k)$ 在单次迭代后收敛到不动点 $x^*=(2,-2)$。在这种情况下，这种有限步收敛是预料之中的，因为集合 $C$ 和 $D$ 是多面体（具体来说是半空间）。对于此类集合，已知 Douglas-Rachford 算法会在有限次迭代内收敛。\n\n对于本问题实例和初始化，其具体机制如下：\n1.  算法的第一步计算出 $y^0 = P_D(x^0) = P_D(1, -3) = (2, -2)$。\n2.  这个点 $y^0$ 恰好位于交集 $C \\cap D$ 中：\n    - 对于 $C$：$y^0$ 的第二个分量是 $-2$，满足 $-2 \\le 0$。所以，$y^0 \\in C$。\n    - 对于 $D$：$y^0$ 是到 $D$ 的投影，所以它必然在 $D$ 中。确实，$2+(-2)=0 \\ge 0$。\n    因此，$y^0 \\in C \\cap D$。到其中一个集合的投影落到了一个可行解上。\n3.  算法变为 $x^1 = x^0 + P_C(2y^0 - x^0) - y^0$。在这一步收敛到 $y^0$ 取决于项 $P_C(2y^0-x^0)$。我们来分析它：\n    $$2y^0 - x^0 = (3, -1)$$\n    由于第二个分量是 $-1 \\le 0$，这个点已经在 $C$ 中。这意味着 $P_C(2y^0-x^0) = 2y^0-x^0$。\n4.  将此代入 $x^1$ 的更新规则中：\n    $$x^1 = x^0 + (2y^0 - x^0) - y^0 = y^0$$\n    所以，$x^1 = y^0 = (2, -2)$。\n5.  由于 $x^1 = (2,-2)$ 是交集 $C \\cap D$ 中的一个点，它是一个有效解。如 $k=1$ 的计算所示，任何点 $x \\in C \\cap D$ 都是 $\\lambda = 1$ 时 DR 迭代的不动点。从这一点开始，序列变为静止的：对于所有 $k \\ge 1$，$x^k = (2, -2)$。\n\n总而言之，对于此初始化，有限步收敛是由于几何排列的特性，即初始投影 $P_D(x^0)$ 已经是一个解，并且反射 $R_D(x^0)=2P_D(x^0)-x^0$ 位于第二个集合 $C$ 内。这种组合在代数上迫使下一次迭代的结果就是那个解，而那个解又是一个不动点。",
            "answer": "$$\\boxed{\\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}}$$"
        },
        {
            "introduction": "在建立了DR算法的几何直觉之后，我们将从纸笔计算转向一个实际的编程问题。本练习将解决数据科学中的一个常见任务：在无穷范数约束下的最小二乘问题 。你将学习如何重构问题、推导必要的近端算子，并完整实现DR分裂循环。通过将DR算法的解与一种简单的“裁剪”启发式方法进行比较，我们将揭示严谨的优化方法所带来的优势。",
            "id": "3122375",
            "problem": "要求您设计并实现一种道格拉斯-拉赫福德分裂法，用以解决一个约束最小残差问题，并将其与一种朴素的裁剪策略进行比较。设 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，且 $\\tau  0$。考虑以下凸优化问题：\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|A x - b\\|_{2} \\quad \\text{subject to} \\quad \\|x\\|_{\\infty} \\le \\tau.\n$$\n使用辅助变量 $y \\in \\mathbb{R}^{m}$，将此问题在乘积空间中表述为：\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, y \\in \\mathbb{R}^{m}} \\ \\|y\\|_{2} \\quad \\text{subject to} \\quad y = A x - b,\\ \\|x\\|_{\\infty} \\le \\tau.\n$$\n令 $C := \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le \\tau\\}$ 且 $M := \\{(x,y) \\in \\mathbb{R}^{n} \\times \\mathbb{R}^{m} : y = A x - b\\}$。定义函数：\n$$\nf(x,y) := \\iota_{M}(x,y), \\qquad g(x,y) := \\iota_{C}(x) + \\|y\\|_{2},\n$$\n其中 $\\iota_{S}$ 表示集合 $S$ 的指示函数。您的任务是：\n\n1) 使用邻近算子和欧几里得投影的基本定义，从第一性原理推导道格拉斯-拉赫福德分裂法所需的以下邻近映射。\n\n- 函数 $g$ 在步长 $\\gamma  0$ 时的邻近映射，即计算：\n$$\n\\operatorname{prox}_{\\gamma g}(x_{0},y_{0}) = \\arg\\min_{x,y} \\ \\iota_{C}(x) + \\|y\\|_{2} + \\frac{1}{2\\gamma}\\|x-x_{0}\\|_{2}^{2} + \\frac{1}{2\\gamma}\\|y-y_{0}\\|_{2}^{2}.\n$$\n证明该问题可分离为 $x_{0}$ 到 $C$ 上的投影和 $y$ 的欧几里得范数的邻近映射，并明确地给出每个分量的表达式。\n\n- 函数 $f$ 在步长 $\\gamma  0$ 时的邻近映射，即计算：\n$$\n\\operatorname{prox}_{\\gamma f}(x_{0},y_{0}) = \\arg\\min_{x,y} \\ \\iota_{M}(x,y) + \\frac{1}{2\\gamma}\\|x-x_{0}\\|_{2}^{2} + \\frac{1}{2\\gamma}\\|y-y_{0}\\|_{2}^{2}.\n$$\n证明这是点 $(x_{0},y_{0})$ 到仿射集 $M$ 上的欧几里得投影，并仅使用基础线性代数（梯度、正规方程以及到仿射子空间投影的刻画）推导其闭式表达式。\n\n2) 在乘积空间 $\\mathbb{R}^{n} \\times \\mathbb{R}^{m}$ 中实现针对和 $f+g$ 的道格拉斯-拉赫福德迭代。对于给定的迭代点 $(x,y)$ 和步长 $\\gamma  0$，执行：\n- 计算 $u = \\operatorname{prox}_{\\gamma g}(z)$，其中 $z = (x,y)$，\n- 计算 $r = 2u - z$，\n- 计算 $v = \\operatorname{prox}_{\\gamma f}(r)$，\n- 更新 $z \\leftarrow z + v - u$。\n当道格拉斯-拉赫福德间隙 $\\|v-u\\|_{2}$ 和可行性间隙 $\\|A u_{x} - b - u_{y}\\|_{2}$ 均小于一个小的容差时停止，其中 $u_{x}$ 和 $u_{y}$ 分别表示 $u$ 的 $x$ 和 $y$ 块。返回候选解 $x_{\\mathrm{DR}} := u_{x}$ 并报告目标函数值 $\\|A x_{\\mathrm{DR}} - b\\|_{2}$。\n\n3) 作为基准，计算无约束最小二乘解 $x_{\\mathrm{LS}} := \\arg\\min_{x}\\|A x - b\\|_{2}$ 和裁剪解 $x_{\\mathrm{clip}} := \\operatorname{proj}_{C}(x_{\\mathrm{LS}})$，其中投影是到 $\\ell_{\\infty}$ 球上的投影。报告目标函数值 $\\|A x_{\\mathrm{clip}} - b\\|_{2}$。\n\n4) 测试套件。使用以下四个测试用例；在每个用例中，$A$ 由其行给出，$b$ 由其条目给出，$\\tau$ 是一个正标量。所有条目均为实数。\n\n- 用例 1：$m = 4$, $n = 3$, $\\tau = 0.5$,\n  $$\n  A = \\begin{bmatrix}\n  1.0  -0.5  0.3 \\\\\n  0.0  1.2  -0.4 \\\\\n  0.8  0.0  1.0 \\\\\n  -0.3  0.6  0.5\n  \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 0.5 \\\\ -1.0 \\\\ 0.2 \\\\ 1.5 \\end{bmatrix}, \\quad\n  \\tau = 0.5.\n  $$\n\n- 用例 2：$m = 5$, $n = 3$, $\\tau = 10.0$,\n  $$\n  A = \\begin{bmatrix}\n  2.0  -1.0  0.0 \\\\\n  0.0  3.0  -1.0 \\\\\n  1.0  0.0  1.0 \\\\\n  0.0  -2.0  1.0 \\\\\n  1.5  0.5  -0.5\n  \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 0.5 \\\\ -1.0 \\\\ 0.0 \\end{bmatrix}, \\quad\n  \\tau = 10.0.\n  $$\n\n- 用例 3：$m = 3$, $n = 4$, $\\tau = 0.8$,\n  $$\n  A = \\begin{bmatrix}\n  1.0  0.0  0.5  -0.2 \\\\\n  0.0  1.0  -0.3  0.7 \\\\\n  0.2  -0.5  0.0  1.0\n  \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}, \\quad\n  \\tau = 0.8.\n  $$\n\n- 用例 4：$m = 4$, $n = 4$, $\\tau = 0.3$，其中列是秩亏的（第三列等于前两列之和），\n  $$\n  A = \\begin{bmatrix}\n  1.0  0.0  1.0  0.5 \\\\\n  0.5  1.0  1.5  -0.3 \\\\\n  -0.2  0.3  0.1  0.0 \\\\\n  0.0  -0.7  -0.7  1.0\n  \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ -0.5 \\\\ 0.2 \\end{bmatrix}, \\quad\n  \\tau = 0.3.\n  $$\n\n5) 最终输出格式。您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果。按顺序为每个测试用例附加三个值：\n- 道格拉斯-拉赫福德算法的目标函数值 $\\|A x_{\\mathrm{DR}} - b\\|_{2}$，四舍五入到六位小数，\n- 裁剪基准方法的目标函数值 $\\|A x_{\\mathrm{clip}} - b\\|_{2}$，四舍五入到六位小数，\n- 一个整数改进标志，如果 $\\|A x_{\\mathrm{DR}} - b\\|_{2} \\le \\|A x_{\\mathrm{clip}} - b\\|_{2} - 10^{-6}$ 则为 $1$，否则为 $0$。\n\n因此，最终打印的行必须如下所示：\n$$\n[\\text{dr}_{1},\\text{clip}_{1},\\text{imp}_{1},\\text{dr}_{2},\\text{clip}_{2},\\text{imp}_{2},\\text{dr}_{3},\\text{clip}_{3},\\text{imp}_{3},\\text{dr}_{4},\\text{clip}_{4},\\text{imp}_{4}],\n$$\n其中每个 $\\text{dr}_{i}$ 和 $\\text{clip}_{i}$ 都格式化为六位小数，每个 $\\text{imp}_{i}$ 为整数 $0$ 或 $1$。不应打印任何其他文本。不涉及角度；不需要物理单位。对两种间隙均使用 $\\gamma = 1$ 和 $10^{-9}$ 的停止容差，并将最大迭代次数限制在 $20000$。",
            "solution": "用户提供了一个有效且适定的凸优化问题。任务是使用道格拉斯-拉赫福德分裂法解决一个约束最小残差问题，并将其性能与一个简单的基准方法进行比较。该问题表述为：\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|A x - b\\|_{2} \\quad \\text{subject to} \\quad \\|x\\|_{\\infty} \\le \\tau\n$$\n通过引入辅助变量 $y \\in \\mathbb{R}^m$，使得 $y=Ax-b$，该问题在乘积空间 $\\mathbb{R}^n \\times \\mathbb{R}^m$ 中被重新表述为：\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, y \\in \\mathbb{R}^{m}} \\ \\|y\\|_{2} \\quad \\text{subject to} \\quad y = A x - b,\\ \\|x\\|_{\\infty} \\le \\tau\n$$\n通过定义两个我们希望最小化其和的函数 $f$ 和 $g$，这种结构适合使用分裂法：\n$$\nf(x,y) := \\iota_{M}(x,y) \\quad \\text{and} \\quad g(x,y) := \\iota_{C}(x) + \\|y\\|_{2}\n$$\n这里，$\\iota_{S}$ 是集合 $S$ 的指示函数。集合 $C := \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le \\tau\\}$ 是对 $x$ 的 $\\ell_{\\infty}$ 球约束，而 $M := \\{(x,y) \\in \\mathbb{R}^{n} \\times \\mathbb{R}^{m} : y = A x - b\\}$ 表示 $x$ 和 $y$ 之间的仿射关系。道格拉斯-拉赫福德算法需要这两个函数的邻近算子。\n\n**1. g 的邻近算子 ($\\operatorname{prox}_{\\gamma g}$) 的推导**\n\n函数 $g$ 在步长 $\\gamma  0$ 时于点 $(x_0, y_0)$ 的邻近算子由以下最小化问题定义：\n$$\n\\operatorname{prox}_{\\gamma g}(x_{0},y_{0}) = \\arg\\min_{x,y} \\left\\{ g(x,y) + \\frac{1}{2\\gamma}\\left(\\|x-x_{0}\\|_{2}^{2} + \\|y-y_{0}\\|_{2}^{2}\\right) \\right\\}\n$$\n代入 $g(x,y) = \\iota_{C}(x) + \\|y\\|_{2}$ 的定义，问题变为：\n$$\n\\arg\\min_{x,y} \\left\\{ \\iota_{C}(x) + \\|y\\|_{2} + \\frac{1}{2\\gamma}\\|x-x_{0}\\|_{2}^{2} + \\frac{1}{2\\gamma}\\|y-y_{0}\\|_{2}^{2} \\right\\}\n$$\n目标函数在 $x$ 和 $y$ 上是可加分离的，这使我们能够独立地求解每个变量。\n\n对于 $x$ 分量，最小化问题为：\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ \\iota_{C}(x) + \\frac{1}{2\\gamma}\\|x-x_{0}\\|_{2}^{2} \\right\\}\n$$\n指示函数 $\\iota_C(x)$ 对于 $x \\in C$ 为 $0$，否则为 $+\\infty$。因此，该问题等价于在集合 $C$ 中寻找在欧几里得距离意义下最接近 $x_0$ 的点。根据定义，这就是 $x_0$ 到 $C$ 上的欧几里得投影：\n$$\nx^{*} = \\operatorname{proj}_{C}(x_0)\n$$\n集合 $C = \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le \\tau\\}$ 是一个由 $-\\tau \\le x_i \\le \\tau$（对每个分量 $i$）定义的超立方体。因此，该投影是一个逐分量的裁剪操作：\n$$\n(x^{*})_i = \\max(-\\tau, \\min((x_0)_i, \\tau))\n$$\n\n对于 $y$ 分量，最小化问题为：\n$$\ny^{*} = \\arg\\min_{y} \\left\\{ \\|y\\|_{2} + \\frac{1}{2\\gamma}\\|y-y_{0}\\|_{2}^{2} \\right\\}\n$$\n这是欧几里得范数邻近算子 $\\operatorname{prox}_{\\gamma \\|\\cdot\\|_2}(y_0)$ 的定义。一阶最优性条件是 $0 \\in \\partial(\\|y^*\\|_2) + \\frac{1}{\\gamma}(y^* - y_0)$，其中 $\\partial$ 是次微分。这意味着 $y_0 - y^* \\in \\gamma \\partial(\\|y^*\\|_2)$。求解此式可得到块软阈值算子：\n$$\ny^{*} = \\max\\left(0, 1 - \\frac{\\gamma}{\\|y_0\\|_2}\\right) y_0\n$$\n为了处理 $y_0=0$ 的情况，可以更稳健地写成：\n$$\ny^{*} = \\left(1 - \\frac{\\gamma}{\\max(\\|y_0\\|_2, \\gamma)}\\right) y_0\n$$\n综合这些结果，g 的邻近算子可分离为两个独立的操作：\n$$\n\\operatorname{prox}_{\\gamma g}(x_{0},y_{0}) = (\\operatorname{proj}_{C}(x_0), \\operatorname{prox}_{\\gamma \\|\\cdot\\|_2}(y_0))\n$$\n\n**2. f 的邻近算子 ($\\operatorname{prox}_{\\gamma f}$) 的推导**\n\n函数 $f$ 的邻近算子定义为：\n$$\n\\operatorname{prox}_{\\gamma f}(x_{0},y_{0}) = \\arg\\min_{x,y} \\left\\{ \\iota_{M}(x,y) + \\frac{1}{2\\gamma}\\left(\\|x-x_{0}\\|_{2}^{2} + \\|y-y_{0}\\|_{2}^{2}\\right) \\right\\}\n$$\n指示函数 $\\iota_M(x,y)$ 将最小化问题限制在仿射子空间 $M$ 上。缩放因子 $1/(2\\gamma)$ 不会改变最小化点，因此该问题等价于寻找点 $(x_0, y_0)$ 到 $M$ 上的欧几里得投影：\n$$\n(x^*, y^*) = \\operatorname{proj}_{M}(x_0, y_0) = \\arg\\min_{(x,y) \\in M} \\left\\{\\|x-x_{0}\\|_{2}^{2} + \\|y-y_{0}\\|_{2}^{2}\\right\\}\n$$\n我们使用约束 $y = Ax - b$ 从目标函数中消去 $y$：\n$$\nx^* = \\arg\\min_{x} \\left\\{ \\|x-x_{0}\\|_{2}^{2} + \\|(Ax - b) - y_{0}\\|_{2}^{2} \\right\\} = \\arg\\min_{x} \\left\\{ \\|x-x_{0}\\|_{2}^{2} + \\|Ax - (b+y_{0})\\|_{2}^{2} \\right\\}\n$$\n这是一个无约束的二次最小化问题。我们通过将目标函数 $L(x)$ 的梯度设为零来求解：\n$$\n\\nabla_x L(x) = 2(x - x_0) + 2A^T(Ax - (b+y_0)) = 0\n$$\n整理各项，我们得到该最小二乘问题的正规方程：\n$$\nx - x_0 + A^T A x - A^T b - A^T y_0 = 0 \\implies (I + A^T A)x = x_0 + A^T(b+y_0)\n$$\n矩阵 $I + A^T A$ 是对称正定的，因此是可逆的。即使 $A$ 是秩亏的，这一结论也成立。$x^*$ 的解为：\n$$\nx^* = (I + A^T A)^{-1} (x_0 + A^T(b+y_0))\n$$\n相应的 $y^*$ 则由子空间约束确定：\n$$\ny^* = A x^* - b\n$$\n这为 $\\operatorname{prox}_{\\gamma f}(x_0, y_0)$ 提供了一个闭式表达式。\n\n**3. 算法实现**\n\n道格拉斯-拉赫福德迭代按规定实现。从初始点 $z_0 = (x_0, y_0)$ 开始，迭代序列由以下方式生成：\n$$\n\\begin{align*}\nu_{k+1} = \\operatorname{prox}_{\\gamma g}(z_k) \\\\\nr_{k+1} = 2u_{k+1} - z_k \\\\\nv_{k+1} = \\operatorname{prox}_{\\gamma f}(r_{k+1}) \\\\\nz_{k+1} = z_k + (v_{k+1} - u_{k+1})\n\\end{align*}\n$$\n当道格拉斯-拉赫福德间隙 $\\|v_{k+1}-u_{k+1}\\|_{2}$ 和可行性间隙 $\\|A (u_{k+1})_x - b - (u_{k+1})_y\\|_{2}$ 均小于指定的容差时，算法终止。解取为 $x_{\\mathrm{DR}} = (u_{k+1})_x$。对于基准方法，使用标准求解器找到无约束最小二乘解 $x_{\\mathrm{LS}} = \\arg\\min_x \\|Ax-b\\|_2$，然后将其裁剪到可行集 $C$，得到 $x_{\\mathrm{clip}} = \\operatorname{proj}_C(x_{\\mathrm{LS}})$。",
            "answer": "```python\nimport numpy as np\n\ndef douglas_rachford_solver(A, b, tau, gamma, tol, max_iter):\n    \"\"\"\n    Solves the constrained least-residual problem using Douglas-Rachford splitting.\n    min ||Ax - b||_2  s.t. ||x||_inf = tau\n    \"\"\"\n    m, n = A.shape\n    \n    # Initialize iterates for the product space (x, y)\n    x = np.zeros(n)\n    y = np.zeros(m)\n    z = np.concatenate((x, y))\n\n    # Precompute the inverse matrix needed for prox_f\n    # (I + A^T A) is always invertible and well-conditioned\n    I_n = np.eye(n)\n    M_inv = np.linalg.inv(I_n + A.T @ A)\n\n    # Main iteration loop\n    for _ in range(max_iter):\n        xk, yk = z[:n], z[n:]\n\n        # --- Step 1: Compute prox_g(z) ---\n        # Part 1: prox for x is projection onto C (clipping)\n        ux = np.clip(xk, -tau, tau)\n        \n        # Part 2: prox for y is L2-norm block soft-thresholding\n        norm_yk = np.linalg.norm(yk)\n        # This formula is robust for yk = 0\n        uy = (1.0 - gamma / max(norm_yk, gamma)) * yk\n        u = np.concatenate((ux, uy))\n        \n        # --- Step 2: Reflection ---\n        r = 2 * u - z\n        rx, ry = r[:n], r[n:]\n\n        # --- Step 3: Compute prox_f(r) ---\n        # This is the projection onto the affine subspace M: y = Ax - b\n        rhs = rx + A.T @ (b + ry)\n        vx = M_inv @ rhs\n        vy = A @ vx - b\n        v = np.concatenate((vx, vy))\n\n        # --- Check stopping criteria ---\n        dr_gap = np.linalg.norm(v - u)\n        feasibility_gap = np.linalg.norm(A @ ux - b - uy)\n        \n        if dr_gap  tol and feasibility_gap  tol:\n            break\n\n        # --- Step 4: Update z ---\n        z = z + v - u\n\n    # The solution x is the x-part of the u iterate\n    x_dr = u[:n]\n    obj_dr = np.linalg.norm(A @ x_dr - b)\n    \n    return obj_dr\n\ndef baseline_clipping_solver(A, b, tau):\n    \"\"\"\n    Solves the unconstrained least-squares problem and clips the solution.\n    \"\"\"\n    # Find the unconstrained least-squares solution x_LS\n    x_ls = np.linalg.lstsq(A, b, rcond=None)[0]\n    \n    # Clip the solution to the feasible set C (l_infinity ball)\n    x_clip = np.clip(x_ls, -tau, tau)\n    \n    # Calculate the objective value for the clipped solution\n    obj_clip = np.linalg.norm(A @ x_clip - b)\n    \n    return obj_clip\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([\n                [1.0, -0.5, 0.3],\n                [0.0, 1.2, -0.4],\n                [0.8, 0.0, 1.0],\n                [-0.3, 0.6, 0.5]\n            ]),\n            \"b\": np.array([0.5, -1.0, 0.2, 1.5]),\n            \"tau\": 0.5\n        },\n        {\n            \"A\": np.array([\n                [2.0, -1.0, 0.0],\n                [0.0, 3.0, -1.0],\n                [1.0, 0.0, 1.0],\n                [0.0, -2.0, 1.0],\n                [1.5, 0.5, -0.5]\n            ]),\n            \"b\": np.array([1.0, -2.0, 0.5, -1.0, 0.0]),\n            \"tau\": 10.0\n        },\n        {\n            \"A\": np.array([\n                [1.0, 0.0, 0.5, -0.2],\n                [0.0, 1.0, -0.3, 0.7],\n                [0.2, -0.5, 0.0, 1.0]\n            ]),\n            \"b\": np.array([0.1, -0.2, 0.3]),\n            \"tau\": 0.8\n        },\n        {\n            \"A\": np.array([\n                [1.0, 0.0, 1.0, 0.5],\n                [0.5, 1.0, 1.5, -0.3],\n                [-0.2, 0.3, 0.1, 0.0],\n                [0.0, -0.7, -0.7, 1.0]\n            ]),\n            \"b\": np.array([0.0, 1.0, -0.5, 0.2]),\n            \"tau\": 0.3\n        }\n    ]\n\n    # Algorithm parameters\n    gamma = 1.0\n    tolerance = 1e-9\n    max_iterations = 20000\n\n    results = []\n    \n    for case in test_cases:\n        A, b, tau = case[\"A\"], case[\"b\"], case[\"tau\"]\n        \n        # Run Douglas-Rachford solver\n        dr_obj = douglas_rachford_solver(A, b, tau, gamma, tolerance, max_iterations)\n        \n        # Run baseline clipping solver\n        clip_obj = baseline_clipping_solver(A, b, tau)\n        \n        # Check for improvement\n        improvement = 1 if dr_obj = clip_obj - 1e-6 else 0\n        \n        # Append results in specified format\n        results.append(f\"{dr_obj:.6f}\")\n        results.append(f\"{clip_obj:.6f}\")\n        results.append(str(improvement))\n\n    # Print the final output string\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "任何迭代算法的一个关键问题是何时停止。这个高级练习将探讨如何设计一个基于“原始-对偶间隙”（primal-dual gap）的、具有理论保证的停止准则 。我们将揭示DR算法的不動点与问题原始-对偶解之间的深刻联系，并为一个 LASSO 类型的问题实现这个间隙的计算。这个练习有助于你更深入地理解收敛性、最优性条件以及如何评估解的质量。",
            "id": "3122381",
            "problem": "你需要研究 Douglas–Rachford 分裂方法，以最小化两个真、闭、凸函数之和，并设计一个基于原始-对偶间隙的停止准则。该准则利用了 Douglas–Rachford 算子的不动点与原始解之间的映射关系。你的任务包含三个部分：从第一性原理出发推导停止准则，为一类特定的凸函数族实现该准则，并在一系列案例中评估该准则的实际可靠性。\n\n考虑凸优化问题\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + g(x),\n$$\n其中函数定义为\n$$\nf(x) = \\tfrac{1}{2}\\|x - c\\|_2^2, \\qquad g(x) = \\lambda \\|x\\|_1,\n$$\n其中向量 $c \\in \\mathbb{R}^n$ 和标量 $\\lambda \\ge 0$ 是给定的。Douglas–Rachford 分裂迭代由邻近算子构建，对于任意真、闭、凸函数 $h$ 和参数 $\\gamma  0$，其定义为\n$$\n\\operatorname{prox}_{\\gamma h}(z) \\in \\arg\\min_{x \\in \\mathbb{R}^n} \\left\\{ h(x) + \\tfrac{1}{2\\gamma}\\|x - z\\|_2^2 \\right\\}。\n$$\n\n任务：\n\n- 从邻近算子和凸次微分的定义出发，推导 Douglas–Rachford 不动点映射，该映射将 Douglas–Rachford 算子的不动点 $z^\\star$ 与优化问题的原始解 $x^\\star$ 关联起来。设计一个基于原始-对偶间隙的停止准则，该准则仅使用可从当前迭代点和给定数据计算出的量进行评估。\n- 使用 Fenchel 共轭和 Fenchel–Young 不等式，为对偶 $(x,s)$ 表示一个可计算的原始-对偶间隙，其中 $x \\in \\mathbb{R}^n$ 是原始候选解，$s \\in \\mathbb{R}^n$ 是受对偶可行性条件约束的对偶候选解。除了定义之外，不要假设任何闭式解的捷径；直接从这些原理推导出必须计算的内容。\n- 通过将原始-对偶间隙容差与到真实最小化点的距离上界相关联，来证明你的停止准则的可靠性，在适用时使用强凸性。\n\n实现细节：\n\n- 仅使用上述定义，实现松弛参数等于 $1$ 的 Douglas–Rachford 分裂算法。对于给定的特定函数 $f$ 和 $g$，从第一性原理推导并实现其邻近算子。\n- 在每次迭代中，通过你推导的不动点映射将当前点映射到原始候选解 $x_k$，并构建一个满足对偶可行性条件的对偶候选解 $s_k$。在每次迭代中计算原始-对偶间隙，当其小于或等于预设容差时停止。\n- 对于这个特定问题，精确的原始最小化点由强凸性唯一确定；使用这一点来评估你的停止准则的可靠性，其量化标准是返回的 $x$ 是否满足间隙容差所蕴含的理论界。\n\n测试套件：\n\n在以下测试案例上运行你的程序。每个案例是一个元组 $(c, \\lambda, \\gamma, \\text{tol}, \\text{max\\_iters})$，其中 $c \\in \\mathbb{R}^n$，$\\lambda \\ge 0$，$\\gamma  0$，并给出了停止容差和最大迭代次数。\n\n- 案例 1 (一般情况)：$c = (3.0, -1.0, 0.5, -0.2, 0.0)$, $\\lambda = 0.8$, $\\gamma = 1.0$, $\\text{tol} = 10^{-4}$, $\\text{max\\_iters} = 10000$。\n- 案例 2 (边界情况 $\\lambda = 0$)：$c = (1.2, -0.7, 0.0, 3.5)$, $\\lambda = 0.0$, $\\gamma = 1.0$, $\\text{tol} = 10^{-6}$, $\\text{max\\_iters} = 10000$。\n- 案例 3 (强正则化导致解趋于零)：$c = (1.0, -1.0, 2.0)$, $\\lambda = 5.0$, $\\gamma = 1.0$, $\\text{tol} = 10^{-6}$, $\\text{max\\_iters} = 10000$。\n- 案例 4 (小步长)：$c = (0.1, -0.05, 0.0)$, $\\lambda = 0.01$, $\\gamma = 0.1$, $\\text{tol} = 10^{-8}$, $\\text{max\\_iters} = 20000$。\n- 案例 5 (混合量级和符号)：$c = (-2.0, 0.5, 4.0, -3.0)$, $\\lambda = 1.5$, $\\gamma = 0.7$, $\\text{tol} = 10^{-5}$, $\\text{max\\_iters} = 15000$。\n\n输出规范：\n\n- 对于每个测试案例，确定基于原始-对偶间隙的停止决策在以下精确意义上是否实际可靠：如果算法在原始-对偶间隙小于或等于容差 $\\text{tol}$ 时停止，则返回的 $x$ 必须满足 $\\|x - x^\\star\\|_2 \\le \\sqrt{2\\,\\text{tol}}$，其中 $x^\\star$ 是给定 $(c,\\lambda)$ 时 $F$ 的精确最小化点。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目是对应测试案例的可靠性条件的布尔真值，顺序与上面列出的案例相同。例如，对于三个案例，输出格式必须与 $[\\text{True},\\text{False},\\text{True}]$ 完全一样。",
            "solution": "用户提供的问题是凸优化领域中一个有效且适定的问题。它要求针对一个 LASSO 型优化的特定实例，推导、实现和评估 Douglas-Rachford 分裂算法。该问题的所有组成部分都具有科学依据、数学上一致且可形式化。\n\n问题是求解：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + g(x)\n$$\n其中 $f(x) = \\tfrac{1}{2}\\|x - c\\|_2^2$ 且 $g(x) = \\lambda \\|x\\|_1$，对于给定的向量 $c \\in \\mathbb{R}^n$ 和标量 $\\lambda \\ge 0$。\n\n### 1. Douglas-Rachford 迭代与不动点映射\n\nDouglas-Rachford (DR) 分裂方法是一种迭代算法，用于最小化两个真、闭、凸函数之和。对于问题 $\\min_x (f(x) + g(x))$，松弛参数等于 $1$ 的一种 DR 迭代形式由下式给出：\n$$\nz_{k+1} = z_k + \\operatorname{prox}_{\\gamma f}\\left(2\\operatorname{prox}_{\\gamma g}(z_k) - z_k\\right) - \\operatorname{prox}_{\\gamma g}(z_k)\n$$\n其中 $\\gamma  0$ 是一个步长参数，$\\operatorname{prox}_{\\gamma h}(\\cdot)$ 是函数 $h$ 的邻近算子。\n\n该优化问题的解可以从这次迭代的一个不动点 $z^\\star$ 中找到。不动点 $z^\\star$ 满足 $z_{k+1} = z_k = z^\\star$，这意味着：\n$$\n\\operatorname{prox}_{\\gamma f}\\left(2\\operatorname{prox}_{\\gamma g}(z^\\star) - z^\\star\\right) - \\operatorname{prox}_{\\gamma g}(z^\\star) = 0\n$$\n让我们将原始解候选定义为 $x^\\star = \\operatorname{prox}_{\\gamma g}(z^\\star)$。将此代入不动点方程得到：\n$$\nx^\\star = \\operatorname{prox}_{\\gamma f}(2x^\\star - z^\\star)\n$$\n邻近算子的定义表明，$p = \\operatorname{prox}_{\\gamma h}(q)$ 等价于变分包含关系 $q \\in p + \\gamma \\partial h(p)$，其中 $\\partial h$ 是 $h$ 的凸次微分。将此应用于我们的两个条件，我们得到：\n1. 从 $x^\\star = \\operatorname{prox}_{\\gamma g}(z^\\star)$ 得：$z^\\star \\in x^\\star + \\gamma \\partial g(x^\\star) \\implies \\frac{z^\\star - x^\\star}{\\gamma} \\in \\partial g(x^\\star)$。\n2. 从 $x^\\star = \\operatorname{prox}_{\\gamma f}(2x^\\star - z^\\star)$ 得：$2x^\\star - z^\\star \\in x^\\star + \\gamma \\partial f(x^\\star) \\implies \\frac{x^\\star - z^\\star}{\\gamma} \\in \\partial f(x^\\star)$。\n\n令 $v = \\frac{x^\\star - z^\\star}{\\gamma}$。那么这两个包含关系变为 $-v \\in \\partial g(x^\\star)$ 和 $v \\in \\partial f(x^\\star)$。将它们相加得到 $0 \\in \\partial f(x^\\star) + \\partial g(x^\\star)$，这是原始问题 $\\min_x F(x)$ 的一阶最优性条件。因此，如果 $z^\\star$ 是 DR 算子的不动点，那么 $x^\\star = \\operatorname{prox}_{\\gamma g}(z^\\star)$ 是一个原始最小化点。这建立了从不动点到原始解所需的映射。\n\n### 2. 原始-对偶间隙与停止准则\n\n为了设计停止准则，我们构建原始-对偶间隙。原始问题是 $P = \\inf_x \\{f(x) + g(x)\\}$。Fenchel 对偶问题是 $D = \\sup_s \\{-f^*(s) - g^*(-s)\\}$，其中 $h^*(s) = \\sup_x \\{\\langle s, x \\rangle - h(x)\\}$ 是 Fenchel 共轭。\n对于原始候选解 $x$ 和对偶候选解 $s$，原始-对偶间隙为：\n$$\n\\text{Gap}(x, s) = (f(x) + g(x)) - (-f^*(s) - g^*(-s)) = f(x) + g(x) + f^*(s) + g^*(-s)\n$$\n根据 Fenchel-Young 不等式，这个间隙总是非负的。\n\n在 DR 算法的每次迭代 $k$ 中，我们生成一个原始候选解 $x_k = \\operatorname{prox}_{\\gamma g}(z_k)$。从上面的推导中，我们得到包含关系 $\\frac{z_k - x_k}{\\gamma} \\in \\partial g(x_k)$。我们可以用它来构造一个对偶候选解。让我们定义 $u_k = \\frac{z_k - x_k}{\\gamma}$。最优性条件要求找到一个 $s^\\star$ 使得 $-s^\\star \\in \\partial f(x^\\star)$ 且 $s^\\star \\in \\partial g(x^\\star)$。这建议我们将对偶候选解设为 $s_k = -u_k = \\frac{x_k-z_k}{\\gamma}$，从而使得 $-s_k \\in \\partial g(x_k)$。\n\n根据 Fenchel-Young 不等式，条件 $-s_k \\in \\partial g(x_k)$ 意味着 $g(x_k) + g^*(-s_k) = \\langle -s_k, x_k \\rangle$。这简化了间隙表达式：\n$$\n\\text{Gap}(x_k, s_k) = f(x_k) + g(x_k) + f^*(s_k) + g^*(-s_k) = f(x_k) + (g(x_k)+g^*(-s_k)) + f^*(s_k) = f(x_k) + \\langle -s_k, x_k \\rangle + f^*(s_k)\n$$\n我们需要计算我们特定函数的共轭：\n- $f(x) = \\frac{1}{2}\\|x - c\\|_2^2$：其共轭为 $f^*(s) = \\frac{1}{2}\\|s\\|_2^2 + \\langle s, c \\rangle$。\n- $g(x) = \\lambda \\|x\\|_1$：其共轭为 $g^*(s) = 0$ 如果 $\\|s\\|_\\infty \\le \\lambda$，否则为 $\\infty$。\n\n对偶候选解 $s_k$ 的选择还有另一个有利的性质。由于 $-s_k \\in \\partial g(x_k) = \\lambda \\partial \\|x_k\\|_1$，其分量受 $\\lambda$ 限制，即 $\\|-s_k\\|_\\infty \\le \\lambda$。这意味着 $g^*(-s_k) = 0$，这确保了对偶目标是有限的。因此，间隙可以计算为：\n$$\n\\text{Gap}_k = f(x_k) + g(x_k) + f^*(s_k)\n$$\n代入 $f$、$g$ 和 $f^*$ 的表达式：\n$$\n\\text{Gap}_k = \\left(\\frac{1}{2}\\|x_k-c\\|_2^2\\right) + \\left(\\lambda\\|x_k\\|_1\\right) + \\left(\\frac{1}{2}\\|s_k\\|_2^2 + \\langle s_k, c \\rangle\\right)\n$$\n当 $s_k = \\frac{x_k-z_k}{\\gamma}$ 时，上式变为：\n$$\n\\text{Gap}_k = \\frac{1}{2}\\|x_k-c\\|_2^2 + \\lambda\\|x_k\\|_1 + \\frac{1}{2\\gamma^2}\\|x_k-z_k\\|_2^2 + \\frac{1}{\\gamma}\\langle x_k-z_k, c \\rangle\n$$\n这个表达式在每次迭代中仅使用迭代点 $z_k$、导出的 $x_k$ 和问题数据即可计算。当 $\\text{Gap}_k \\le \\text{tol}$ 时算法停止。\n\n### 3. 停止准则的可靠性\n\n目标函数 $F(x) = f(x) + g(x)$ 是一个 $1$-强凸函数 ($f$) 和一个凸函数 ($g$) 的和。因此，$F(x)$ 是 $1$-强凸的。对于一个 $\\mu$-强凸函数，以下不等式成立：\n$$\nF(x) - F(x^\\star) \\ge \\frac{\\mu}{2}\\|x - x^\\star\\|_2^2\n$$\n在我们的例子中，$\\mu = 1$。此外，对于任何原始-对偶对 $(x, s)$，间隙为原始目标的次优性提供了一个上界：$\\text{Gap}(x, s) \\ge F(x) - F(x^\\star)$。\n结合这些不等式，我们有：\n$$\n\\text{Gap}_k \\ge F(x_k) - F(x^\\star) \\ge \\frac{1}{2}\\|x_k - x^\\star\\|_2^2\n$$\n这给出了到真实最小化点 $x^\\star$ 的距离的一个界：\n$$\n\\|x_k - x^\\star\\|_2 \\le \\sqrt{2 \\cdot \\text{Gap}_k}\n$$\n如果算法因为 $\\text{Gap}_k \\le \\text{tol}$ 而终止，返回的解 $x_k$ 保证满足：\n$$\n\\|x_k - x^\\star\\|_2 \\le \\sqrt{2 \\cdot \\text{tol}}\n$$\n这为可靠的停止准则提供了理论基础。\n\n### 4. 实现细节：邻近算子和精确解\n\n为了实现算法，我们需要邻近算子的闭式表达式。\n- 对于 $f(x) = \\frac{1}{2}\\|x-c\\|_2^2$：\n$$ \\operatorname{prox}_{\\gamma f}(z) = \\arg\\min_x \\left\\{\\frac{1}{2}\\|x-c\\|_2^2 + \\frac{1}{2\\gamma}\\|x-z\\|_2^2\\right\\} = \\frac{\\gamma c + z}{\\gamma+1} $$\n- 对于 $g(x) = \\lambda\\|x\\|_1$：\n$$ \\operatorname{prox}_{\\gamma g}(z) = \\arg\\min_x \\left\\{\\lambda\\|x\\|_1 + \\frac{1}{2\\gamma}\\|x-z\\|_2^2\\right\\} = S_{\\gamma\\lambda}(z) $$\n其中 $S_\\alpha(z)$ 是分量软阈值算子：$(S_\\alpha(z))_i = \\operatorname{sign}(z_i)\\max(|z_i|-\\alpha, 0)$。\n\n精确解 $x^\\star$ 可以从最优性条件 $0 \\in x^\\star - c + \\lambda\\partial\\|x^\\star\\|_1$ 中找到，这等价于 $x^\\star = \\operatorname{prox}_{\\lambda g}(c) = S_\\lambda(c)$。这允许直接计算真实最小化点，以验证我们停止准则的可靠性。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Douglas-Rachford splitting method for a specific convex\n    optimization problem, evaluates a primal-dual gap-based stopping test,\n    and checks its reliability against a theoretical bound.\n    \"\"\"\n\n    def soft_threshold(z, threshold):\n        \"\"\"Component-wise soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - threshold, 0)\n\n    def prox_f(z_in, c_vec, gamma_val):\n        \"\"\"Proximal operator of f(x) = 1/2*||x-c||_2^2.\"\"\"\n        return (gamma_val * c_vec + z_in) / (gamma_val + 1.0)\n\n    def prox_g(z_in, lambda_val, gamma_val):\n        \"\"\"Proximal operator of g(x) = lambda*||x||_1.\"\"\"\n        return soft_threshold(z_in, gamma_val * lambda_val)\n\n    def run_douglas_rachford(c, lam, gam, tol, max_iters):\n        \"\"\"\n        Executes the Douglas-Rachford algorithm for a given test case.\n        \"\"\"\n        n = c.shape[0]\n        z = np.zeros(n)\n        \n        final_x = np.zeros(n)\n        final_gap = np.inf\n\n        for _ in range(max_iters):\n            # Step 1: Compute primal candidate x_k\n            x = prox_g(z, lam, gam)\n            \n            # Step 2: Compute dual candidate s_k and the primal-dual gap\n            s = (x - z) / gam\n            \n            f_x = 0.5 * np.linalg.norm(x - c)**2\n            g_x = lam * np.linalg.norm(x, 1)\n            f_star_s = 0.5 * np.linalg.norm(s)**2 + np.dot(s, c)\n            \n            gap = f_x + g_x + f_star_s\n            \n            # Step 3: Check stopping condition\n            if gap = tol:\n                final_x = x\n                final_gap = gap\n                return final_x, final_gap\n\n            # Step 4: Perform the Douglas-Rachford update for z\n            y_prime = 2 * x - z\n            x_prime = prox_f(y_prime, c, gam)\n            z = z + x_prime - x\n        \n        # If max_iters is reached, return the last computed values\n        final_x = x\n        final_gap = gap\n        return final_x, final_gap\n\n    # Test suite from the problem statement\n    test_cases_data = [\n        (np.array([3.0, -1.0, 0.5, -0.2, 0.0]), 0.8, 1.0, 1e-4, 10000),\n        (np.array([1.2, -0.7, 0.0, 3.5]), 0.0, 1.0, 1e-6, 10000),\n        (np.array([1.0, -1.0, 2.0]), 5.0, 1.0, 1e-6, 10000),\n        (np.array([0.1, -0.05, 0.0]), 0.01, 0.1, 1e-8, 20000),\n        (np.array([-2.0, 0.5, 4.0, -3.0]), 1.5, 0.7, 1e-5, 15000),\n    ]\n\n    results = []\n    for c, lam, gam, tol, max_iters in test_cases_data:\n        # Run the algorithm\n        x_final, gap_final = run_douglas_rachford(c, lam, gam, tol, max_iters)\n\n        # Compute the exact solution\n        x_star = soft_threshold(c, lam)\n\n        # Check the reliability condition\n        # The test is reliable unless the algorithm stops due to tolerance\n        # AND the error bound is violated.\n        stopped_by_tol = (gap_final = tol)\n        \n        # Note: If gap is negative due to floating point error, it should be treated as 0 for the bound.\n        # But the primal-dual gap is theoretically non-negative.\n        bound_violated = (np.linalg.norm(x_final - x_star) > np.sqrt(2 * tol))\n\n        is_reliable = not (stopped_by_tol and bound_violated)\n        results.append(is_reliable)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}