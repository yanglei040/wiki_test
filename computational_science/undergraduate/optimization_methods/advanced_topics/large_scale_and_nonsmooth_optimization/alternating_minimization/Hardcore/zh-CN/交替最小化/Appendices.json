{
    "hands_on_practices": [
        {
            "introduction": "理论与实践的第一步是理解算法的核心机制。本练习旨在通过一个简单的二次函数，建立梯度下降法与交替最小化之间的桥梁。通过推导并应用坐标级利普希茨常数，你将探索“非精确”更新与精确区块最小化之间的关系，从而揭示在特定条件下，单步梯度下降等同于精确最小化的深刻见解 ()。",
            "id": "3097264",
            "problem": "考虑二阶连续可微函数\n$$\nf(x,y) \\;=\\; \\frac{3}{2}\\,x^{2} \\;+\\; 2\\,x\\,y \\;+\\; \\frac{5}{2}\\,y^{2} \\;-\\; 4\\,x \\;-\\; 6\\,y,\n$$\n其中决策变量 $x \\in \\mathbb{R}$ 和 $y \\in \\mathbb{R}$。根据定义，如果存在一个常数 $L_{x} \\ge 0$，使得对于所有 $x, x' \\in \\mathbb{R}$ 和任意固定的 $y \\in \\mathbb{R}$，偏梯度是坐标级利普希茨连续的，\n$$\n\\big\\|\\nabla_{x} f(x,y) - \\nabla_{x} f(x',y)\\big\\| \\;\\le\\; L_{x}\\,|x-x'|,\n$$\n对于固定的 $x$，关于 $y, y'$ 的 $L_y$ 也类似定义。\n\n任务：\n1) 从第一性原理出发，推导 $f(x,y)$ 的最小有效坐标级利普希茨常数 $L_{x}$ 和 $L_{y}$。\n2) 使用这些常数，构建一个单步非精确交替最小化 (AM) 方案，其中 $x$ 的更新是在 $f(\\cdot,y^{k})$ 上进行一步梯度下降，步长为 $1/L_{x}$；然后 $y$ 的更新是在 $f(x^{k+1},\\cdot)$ 上进行一步梯度下降，步长为 $1/L_{y}$。从 $(x^{0},y^{0})=(1,0)$ 开始，计算该方案生成的 $(x^{1},y^{1})$。\n3) 对于这个二次函数 $f$，推导给定 $y$ 时 $x$ 的精确块最小化器和给定 $x$ 时 $y$ 的精确块最小化器，并将其与步骤2中获得的非精确 AM 更新进行简要比较。\n\n将您的最终答案报告为包含 $(x^{1},y^{1})$ 的行矩阵。无需四舍五入。",
            "solution": "该问题陈述清晰、科学合理且客观。它包含连续优化中涉及凸二次函数的标准任务。所有必要信息均已提供，问题没有矛盾或歧义。因此，该问题是有效的，将提供完整的解答。\n\n所考虑的函数为\n$$\nf(x,y) \\;=\\; \\frac{3}{2}\\,x^{2} \\;+\\; 2\\,x\\,y \\;+\\; \\frac{5}{2}\\,y^{2} \\;-\\; 4\\,x \\;-\\; 6\\,y.\n$$\n\n**1) 坐标级利普希茨常数的推导**\n\n坐标级利普希茨连续偏梯度的定义要求，对于一个常数 $L_x \\ge 0$，不等式 $|\\nabla_{x} f(x,y) - \\nabla_{x} f(x',y)| \\le L_{x}\\,|x-x'|$ 对所有 $x, x' \\in \\mathbb{R}$ 和任意固定的 $y \\in \\mathbb{R}$ 成立。由于 $f$ 是二阶连续可微的，因此最小的此类常数 $L_x$ 由关于 $x$ 的二阶偏导数绝对值的上确界给出。\n\n首先，我们计算关于 $x$ 的偏梯度：\n$$\n\\nabla_{x} f(x,y) = \\frac{\\partial f}{\\partial x} = 3\\,x + 2\\,y - 4.\n$$\n然后，我们计算关于 $x$ 的二阶偏导数：\n$$\n\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x} (3\\,x + 2\\,y - 4) = 3.\n$$\n最小利普希茨常数 $L_x$ 是该导数在所有 $x$ 和 $y$ 上的绝对值的上确界：\n$$\nL_x = \\sup_{x, y \\in \\mathbb{R}} \\left| \\frac{\\partial^2 f}{\\partial x^2}(x,y) \\right| = |3| = 3.\n$$\n类似地，对于 $y$ 坐标，我们首先计算关于 $y$ 的偏梯度：\n$$\n\\nabla_{y} f(x,y) = \\frac{\\partial f}{\\partial y} = 2\\,x + 5\\,y - 6.\n$$\n然后，我们计算关于 $y$ 的二阶偏导数：\n$$\n\\frac{\\partial^2 f}{\\partial y^2} = \\frac{\\partial}{\\partial y} (2\\,x + 5\\,y - 6) = 5.\n$$\n最小利普希茨常数 $L_y$ 是该导数绝对值的上确界：\n$$\nL_y = \\sup_{x, y \\in \\mathbb{R}} \\left| \\frac{\\partial^2 f}{\\partial y^2}(x,y) \\right| = |5| = 5.\n$$\n因此，最小的有效坐标级利普希茨常数是 $L_x = 3$ 和 $L_y = 5$。\n\n**2) 非精确交替最小化 (AM) 步骤**\n\n非精确 AM 方案由以下更新定义，从 $(x^0, y^0) = (1, 0)$ 开始：\n$$\nx^{k+1} = x^{k} - \\frac{1}{L_x} \\nabla_x f(x^k, y^k)\n$$\n$$\ny^{k+1} = y^{k} - \\frac{1}{L_y} \\nabla_y f(x^{k+1}, y^k)\n$$\n我们计算第一次迭代，即当 $k=0$ 时，以求得 $(x^1, y^1)$。\n\n首先，我们执行 $x$ 更新。我们需要偏梯度 $\\nabla_x f(x^0, y^0)$：\n$$\n\\nabla_x f(x^0, y^0) = \\nabla_x f(1, 0) = 3(1) + 2(0) - 4 = -1.\n$$\n使用 $L_x = 3$，对 $x^1$ 的更新为：\n$$\nx^{1} = x^{0} - \\frac{1}{L_x} \\nabla_x f(x^0, y^0) = 1 - \\frac{1}{3}(-1) = 1 + \\frac{1}{3} = \\frac{4}{3}.\n$$\n接下来，我们执行 $y$ 更新。此更新使用新计算出的值 $x^1 = 4/3$ 和之前的值 $y^0 = 0$。我们需要偏梯度 $\\nabla_y f(x^1, y^0)$：\n$$\n\\nabla_y f(x^1, y^0) = \\nabla_y f\\left(\\frac{4}{3}, 0\\right) = 2\\left(\\frac{4}{3}\\right) + 5(0) - 6 = \\frac{8}{3} - 6 = \\frac{8}{3} - \\frac{18}{3} = -\\frac{10}{3}.\n$$\n使用 $L_y = 5$，对 $y^1$ 的更新为：\n$$\ny^{1} = y^{0} - \\frac{1}{L_y} \\nabla_y f(x^1, y^0) = 0 - \\frac{1}{5}\\left(-\\frac{10}{3}\\right) = \\frac{10}{15} = \\frac{2}{3}.\n$$\n因此，该方案一步的结果是 $(x^1, y^1) = \\left(\\frac{4}{3}, \\frac{2}{3}\\right)$。\n\n**3) 精确块最小化器与比较**\n\n为求得给定固定 $y$ 时 $x$ 的精确块最小化器，记为 $x^*(y)$，我们对 $x$ 求解一阶最优性条件 $\\nabla_x f(x,y) = 0$：\n$$\n3\\,x + 2\\,y - 4 = 0 \\implies 3\\,x = 4 - 2\\,y \\implies x^*(y) = \\frac{4 - 2\\,y}{3}.\n$$\n类似地，为求得给定固定 $x$ 时 $y$ 的精确块最小化器，记为 $y^*(x)$，我们对 $y$ 求解 $\\nabla_y f(x,y) = 0$：\n$$\n2\\,x + 5\\,y - 6 = 0 \\implies 5\\,y = 6 - 2\\,x \\implies y^*(x) = \\frac{6 - 2\\,x}{5}.\n$$\n现在我们将这些精确最小化器与步骤2中的“非精确”AM 更新进行比较。让我们从代数上检查更新规则：\n$$\nx^{k+1} = x^k - \\frac{1}{L_x}\\nabla_x f(x^k, y^k) = x^k - \\frac{1}{3}(3x^k + 2y^k - 4) = x^k - x^k - \\frac{2}{3}y^k + \\frac{4}{3} = \\frac{4 - 2y^k}{3}.\n$$\n此表达式与 $x^*(y^k)$ 相同。\n\n让我们检查 $y$ 的更新规则：\n$$\ny^{k+1} = y^k - \\frac{1}{L_y}\\nabla_y f(x^{k+1}, y^k) = y^k - \\frac{1}{5}(2x^{k+1} + 5y^k - 6) = y^k - \\frac{2}{5}x^{k+1} - y^k + \\frac{6}{5} = \\frac{6 - 2x^{k+1}}{5}.\n$$\n此表达式与 $y^*(x^{k+1})$ 相同。\n\n比较表明，对于这个特定的二次函数，所规定的“非精确”AM 方案等同于精确交替最小化方法，也称为块坐标下降法（BCD）。这是因为对于形如 $g(z) = \\frac{1}{2}Az^2 + Bz + C$（其中 $A > 0$）的一维二次子问题，梯度 $g'(z)$ 的利普希茨常数是 $L=A$。使用步长 $1/L$ 的单步梯度下降即可找到精确的最小化器：$z_{k+1} = z_k - \\frac{1}{A}(Az_k + B) = -B/A$。在我们的问题中，$x$ 和 $y$ 的子问题都是二次的，并且所选的步长（$1/L_x$ 和 $1/L_y$）恰好是二阶导数的倒数，这使得每个更新步骤都是对其各自子问题的精确最小化。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{4}{3}  \\frac{2}{3} \\end{pmatrix}}$$"
        },
        {
            "introduction": "交替最小化算法虽然在理论上保证收敛，但在实际应用中其收敛路径可能并非最高效。特别是在目标函数的“狭窄山谷”中，算法常表现出“之字形”振荡行为，降低了收敛速度。本编程练习将引导你模拟这一现象，并通过引入一种称为“阻尼”或“欠松弛”的常用技术，来观察如何有效抑制振荡，实现更平滑、更快速的收敛 ()。",
            "id": "3097286",
            "problem": "考虑一个双变量目标函数，该函数是双凸的：即固定任一变量时，该函数在另一变量上是凸的。设函数定义为 $$f(x,y) = p x^2 + q y^2 + r x y$$，其中 $p > 0$，$q > 0$ 且 $r \\in \\mathbb{R}$。对于任意固定的 $y$，该函数在 $x$ 上是凸的，因为 $x^2$ 的系数 $p$ 是严格为正的，并且固定 $y$ 所产生的任何关于 $x$ 的线性项都不会影响其在 $x$ 上的凸性。同理，对于任意固定的 $x$，该函数在 $y$ 上是凸的，因为 $y^2$ 的系数 $q$ 是严格为正的。\n\n交替最小化 (Alternating Minimization, AM) 算法定义为：给定当前迭代点 $(x_k, y_k)$，通过先对 $x$ 最小化 $f(x,y_k)$ 得到 $x_{k+1}$，然后对 $y$ 最小化 $f(x_{k+1}, y)$ 得到 $y_{k+1}$ 来计算下一个迭代点。为了研究导致“之”字形行为的窄谷效应，实现带有可选阻尼（欠松弛）因子的 AM 算法。该因子由一个标量 $\\lambda \\in (0,1]$ 指定，并按如下方式应用于每个精确最小化器：在计算当前坐标的精确最小化器后，通过当前值与该最小化器的凸组合进行更新，其中权重为 $\\lambda$。具体来说，使用以下规则 $$x_{k+1} = (1 - \\lambda) x_k + \\lambda \\,\\widehat{x}(y_k), \\quad y_{k+1} = (1 - \\lambda) y_k + \\lambda \\,\\widehat{y}(x_{k+1}),$$ 其中 $\\widehat{x}(y_k)$ 表示 $f(x,y_k)$ 关于 $x$ 的精确最小化器，而 $\\widehat{y}(x_{k+1})$ 表示 $f(x_{k+1},y)$ 关于 $y$ 的精确最小化器。$\\lambda = 1$ 的情况对应于无阻尼的 AM 算法。\n\n你的任务：\n- 从凸性和最小化的基本定义出发，推导函数 $f(x,y)$ 的精确最小化器 $\\widehat{x}(y)$ 和 $\\widehat{y}(x)$。\n- 实现上述带阻尼的 AM 迭代过程，并模拟固定步数下的迭代点。\n- 在 $T$ 次迭代中，使用以下指标量化“之”字形行为和收敛性：\n    1. 由带阻尼的 AM 更新所导出的线性迭代矩阵 $M$ 的谱半径 $\\rho(M)$，它控制着该二次模型的线性收敛性。一个较大且接近 $1$ 的 $\\rho(M)$ 表示收敛更慢且振荡行为增加，而 $\\rho(M) < 1$ 表示收缩。\n    2. 序列 $\\{x_k\\}_{k=0}^{T}$ 中符号变化的次数，该指标捕捉了跨越窄谷的“之”字形行为。\n    3. 最终欧几里得范数与初始欧几里得范数之比，定义为 $$\\text{ratio} = \\frac{\\sqrt{x_T^2 + y_T^2}}{\\sqrt{x_0^2 + y_0^2}}.$$ 较小的比值表示更强的收缩。\n\n对于每个测试用例，计算无阻尼算法（$\\lambda = 1$）和带阻尼算法（$\\lambda = 0.5$）的这些指标。\n\n使用以下测试套件，每个套件由 $(p,q,r,x_0,y_0,T)$ 指定：\n- 用例 1（窄谷，对称尺度）：$(p,q,r,x_0,y_0,T) = (1,1,1.9,1,1,30)$。\n- 用例 2（宽谷，对称尺度）：$(p,q,r,x_0,y_0,T) = (1,1,0.5,1,1,30)$。\n- 用例 3（窄谷，非对称尺度）：$(p,q,r,x_0,y_0,T) = (10,0.1,1.9,1,1,30)$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例应按 $[\\rho_{\\lambda=1},\\rho_{\\lambda=0.5},\\text{signChanges}_{\\lambda=1},\\text{signChanges}_{\\lambda=0.5},\\text{finalRatio}_{\\lambda=1},\\text{finalRatio}_{\\lambda=0.5}]$ 的顺序贡献一个列表，最终输出应将这三个列表聚合成一个单一列表，例如 $$[\\,[\\rho_{1},\\rho_{0.5},s_{1},s_{0.5},u_{1},u_{0.5}],\\,[\\dots],\\,[\\dots]\\,].$$ 不涉及物理单位，并且从符号变化中可以推断出的所有角度都纯粹是代数性质的，不需要指定单位。确保输出中的所有数值都以标准十进制格式表示。",
            "solution": "该问题要求对一个双凸二次目标函数，分析带有可选阻尼因子的交替最小化 (AM) 算法。分析内容包括推导迭代的线性算子、计算其谱半径，以及通过模拟算法来衡量收敛速度和振荡行为等性能指标。\n\n目标函数由下式给出\n$$f(x,y) = p x^2 + q y^2 + r x y$$\n参数为 $p > 0$，$q > 0$ 且 $r \\in \\mathbb{R}$。该函数是双凸的，意味着当一个变量保持不变时，它在另一个变量上是凸的。该函数的全局最小点位于 $(x,y) = (0,0)$，前提是该函数是联合凸的，这要求其 Hessian 矩阵是半正定的。Hessian 矩阵为 $H = \\begin{pmatrix} 2p  r \\\\ r  2q \\end{pmatrix}$，如果 $2p > 0$，$2q > 0$ 且 $\\det(H) = 4pq - r^2 \\ge 0$，则其半正定性得到保证。给定的所有测试用例都满足这个更严格的条件，从而确保了唯一的全局最小值。\n\n**1. 精确最小化器的推导**\n\nAM 算法通过每次沿一个坐标轴最小化函数来进行。为了找到固定 $y$ 时的精确最小化器 $\\widehat{x}(y)$，我们求解 $f(x,y)$ 关于 $x$ 的驻点：\n$$ \\frac{\\partial f(x,y)}{\\partial x} = \\frac{\\partial}{\\partial x} (p x^2 + q y^2 + r x y) = 2px + ry = 0 $$\n由于 $f$ 在 $x$ 上是严格凸的（因为 $p > 0$），这个驻点就是唯一的最小化器：\n$$ \\widehat{x}(y) = -\\frac{r}{2p} y $$\n同理，对于固定的 $x$，我们通过求解关于 $y$ 的驻点来找到最小化器 $\\widehat{y}(x)$：\n$$ \\frac{\\partial f(x,y)}{\\partial y} = \\frac{\\partial}{\\partial y} (p x^2 + q y^2 + r x y) = 2qy + rx = 0 $$\n由于 $f$ 在 $y$ 上是严格凸的（因为 $q > 0$），唯一的最小化器是：\n$$ \\widehat{y}(x) = -\\frac{r}{2q} x $$\n\n**2. 带阻尼的交替最小化迭代**\n\n从迭代点 $(x_k, y_k)$ 到 $(x_{k+1}, y_{k+1})$ 的带阻尼 AM 更新是使用阻尼因子 $\\lambda \\in (0,1]$ 顺序定义的。\n\n首先，$x$ 的更新是当前值 $x_k$ 和精确最小化器 $\\widehat{x}(y_k)$ 的凸组合：\n$$ x_{k+1} = (1 - \\lambda) x_k + \\lambda \\,\\widehat{x}(y_k) = (1 - \\lambda) x_k + \\lambda \\left(-\\frac{r}{2p} y_k\\right) = (1 - \\lambda) x_k - \\frac{\\lambda r}{2p} y_k $$\n\n接下来，$y$ 的更新使用新计算出的 $x_{k+1}$：\n$$ y_{k+1} = (1 - \\lambda) y_k + \\lambda \\,\\widehat{y}(x_{k+1}) = (1 - \\lambda) y_k + \\lambda \\left(-\\frac{r}{2q} x_{k+1}\\right) $$\n为了将其表示为对向量 $z_k = \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix}$ 的单一线性更新，我们将 $x_{k+1}$ 的表达式代入 $y_{k+1}$ 的方程中：\n$$ y_{k+1} = (1 - \\lambda) y_k - \\frac{\\lambda r}{2q} \\left( (1 - \\lambda) x_k - \\frac{\\lambda r}{2p} y_k \\right) $$\n$$ y_{k+1} = -\\frac{\\lambda r(1 - \\lambda)}{2q} x_k + \\left( (1 - \\lambda) + \\frac{\\lambda^2 r^2}{4pq} \\right) y_k $$\n完整的迭代可以写成矩阵形式 $z_{k+1} = M z_k$：\n$$ \\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1-\\lambda  -\\frac{\\lambda r}{2p} \\\\ -\\frac{\\lambda r(1-\\lambda)}{2q}  1-\\lambda + \\frac{\\lambda^2 r^2}{4pq} \\end{pmatrix} \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix} $$\n因此，迭代矩阵 $M$ 为：\n$$ M = \\begin{pmatrix} 1-\\lambda  -\\frac{\\lambda r}{2p} \\\\ -\\frac{\\lambda r(1-\\lambda)}{2q}  1-\\lambda + \\frac{\\lambda^2 r^2}{4pq} \\end{pmatrix} $$\n\n**3. 迭代矩阵的谱半径**\n\n线性迭代的收敛速度由谱半径 $\\rho(M) = \\max_i |\\mu_i|$ 决定，其中 $\\mu_i$ 是 $M$ 的特征值。特征值是特征方程 $\\det(M - \\mu I) = 0$ 的根。令 $\\kappa = \\frac{r^2}{4pq}$。特征方程为：\n$$ \\mu^2 - \\text{Tr}(M)\\mu + \\det(M) = 0 $$\n迹为 $\\text{Tr}(M) = (1-\\lambda) + (1-\\lambda + \\lambda^2 \\kappa) = 2(1-\\lambda) + \\lambda^2 \\kappa$。\n行列式为 $\\det(M) = (1-\\lambda)(1-\\lambda + \\lambda^2 \\kappa) - \\left(-\\frac{\\lambda r}{2p}\\right)\\left(-\\frac{\\lambda r(1-\\lambda)}{2q}\\right) = (1-\\lambda)^2 + \\lambda^2\\kappa(1-\\lambda) - \\lambda^2\\kappa(1-\\lambda) = (1-\\lambda)^2$。\n所以，特征方程是：\n$$ \\mu^2 - \\left( 2(1-\\lambda) + \\lambda^2 \\kappa \\right)\\mu + (1-\\lambda)^2 = 0 $$\n这个关于 $\\mu$ 的二次方程的判别式是 $\\Delta = (2(1-\\lambda) + \\lambda^2 \\kappa)^2 - 4(1-\\lambda)^2 = (\\lambda^2 \\kappa)(4(1-\\lambda) + \\lambda^2 \\kappa)$。由于 $\\lambda \\in (0, 1]$，$1-\\lambda \\ge 0$。又因为 $p,q > 0$，所以 $\\kappa \\ge 0$。因此，$\\Delta \\ge 0$，特征值总是实数。\n由于根的乘积 $\\mu_1 \\mu_2 = (1-\\lambda)^2 \\ge 0$ 且根的和 $\\mu_1+\\mu_2 = 2(1-\\lambda) + \\lambda^2 \\kappa \\ge 0$，因此特征值是非负的。\n因此，谱半径是两个特征值中较大的那个：\n$$ \\rho(M) = \\mu_{\\text{max}} = \\frac{ \\left( 2(1-\\lambda) + \\lambda^2 \\kappa \\right) + \\sqrt{ (\\lambda^2 \\kappa)(4(1-\\lambda) + \\lambda^2 \\kappa) } }{2} $$\n对于无阻尼 AM 的特殊情况（$\\lambda=1$），方程得到简化。矩阵变为 $M = \\begin{pmatrix} 0  -r/(2p) \\\\ 0  r^2/(4pq) \\end{pmatrix}$。由于这是一个三角矩阵，其特征值是主对角线上的元素，即 $\\mu_1 = 0$ 和 $\\mu_2 = \\kappa = \\frac{r^2}{4pq}$。谱半径为 $\\rho(M_{\\lambda=1}) = \\kappa$。\n\n**4. 模拟与性能指标**\n\n该算法将通过从 $(x_0, y_0)$ 开始，迭代推导出的更新方程 $T$ 步来实现。对于每组参数 $(p, q, r)$ 和每个 $\\lambda \\in \\{1, 0.5\\}$ 的选择，计算以下指标：\n\n1.  **谱半径** $\\rho(M)$：使用上面推导的解析公式计算。接近 $1$ 的值表示收敛缓慢。\n2.  **符号变化次数**：生成 x 坐标序列 $\\{x_k\\}_{k=0}^T$。统计在 $k \\in \\{1, \\dots, T\\}$ 中，乘积 $x_k x_{k-1}$ 为负的次数。这量化了跨越 $y$ 轴的“之”字形行为。\n3.  **最终范数与初始范数之比**：计算公式为 $\\frac{\\sqrt{x_T^2 + y_T^2}}{\\sqrt{x_0^2 + y_0^2}}$，它提供了 $T$ 次迭代后总收缩程度的度量。\n\n现在将基于这些推导和定义进行实现。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the damped Alternating Minimization problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (narrow valley, symmetric scales)\n        (1, 1, 1.9, 1, 1, 30),\n        # Case 2 (wide valley, symmetric scales)\n        (1, 1, 0.5, 1, 1, 30),\n        # Case 3 (narrow valley, asymmetric scales)\n        (10, 0.1, 1.9, 1, 1, 30),\n    ]\n\n    lambda_vals = [1.0, 0.5]\n    all_results = []\n\n    for p, q, r, x0, y0, T in test_cases:\n        case_results_interleaved = []\n        \n        # Lists to hold results for lambda=1 and lambda=0.5 to be interleaved later.\n        rhos = []\n        sign_changes_list = []\n        ratios = []\n\n        for lmbda in lambda_vals:\n            # 1. Calculate the spectral radius rho(M)\n            # kappa is the convergence factor for the undamped case\n            kappa = r**2 / (4 * p * q)\n            \n            if lmbda == 1.0:\n                rho = kappa\n            else:\n                one_minus_lmbda = 1.0 - lmbda\n                # Numerator of the eigenvalue formula\n                trace_M = 2 * one_minus_lmbda + lmbda**2 * kappa\n                # Term inside the square root\n                discriminant_term = lmbda**2 * kappa * (4 * one_minus_lmbda + lmbda**2 * kappa)\n                sqrt_term = np.sqrt(discriminant_term)\n                \n                # The spectral radius is the larger, positive eigenvalue\n                rho = (trace_M + sqrt_term) / 2.0\n            \n            rhos.append(rho)\n\n            # 2. Run the simulation\n            x_k, y_k = float(x0), float(y0)\n            x_hist = [x_k]\n            \n            # The simulation uses the direct update rules derived from the problem description\n            for _ in range(T):\n                # First, update x\n                x_hat = -r / (2 * p) * y_k\n                x_kp1 = (1 - lmbda) * x_k + lmbda * x_hat\n                \n                # Then, update y using the new x_kp1\n                y_hat = -r / (2 * q) * x_kp1\n                y_kp1 = (1 - lmbda) * y_k + lmbda * y_hat\n                \n                x_k, y_k = x_kp1, y_kp1\n                x_hist.append(x_k)\n\n            # 3. Quantify metrics\n            # 3a. Count sign changes in the x-coordinate\n            sign_changes = 0\n            for i in range(1, len(x_hist)):\n                if x_hist[i] * x_hist[i-1] < 0:\n                    sign_changes += 1\n            sign_changes_list.append(sign_changes)\n\n            # 3b. Calculate final-to-initial norm ratio\n            norm0 = np.sqrt(x0**2 + y0**2)\n            normT = np.sqrt(x_hist[-1]**2 + y_k**2) # y_k is final y\n            \n            ratio = normT / norm0 if norm0 > 0 else 0.0\n            ratios.append(ratio)\n\n        # Interleave results: [rho_1, rho_0.5, sc_1, sc_0.5, r_1, r_0.5]\n        case_results_interleaved = [\n            rhos[0], rhos[1],\n            sign_changes_list[0], sign_changes_list[1],\n            ratios[0], ratios[1]\n        ]\n        all_results.append(case_results_interleaved)\n\n    # Format the final output string as a list of lists.\n    # e.g., [[1,2],[3,4]]\n    outer_list_str = []\n    for res_list in all_results:\n        inner_list_str = f\"[{','.join(map(str, res_list))}]\"\n        outer_list_str.append(inner_list_str)\n    final_output = f\"[{','.join(outer_list_str)}]\"\n    \n    print(final_output)\n\nsolve()\n\n```"
        },
        {
            "introduction": "许多现实世界中的优化问题都包含约束条件，例如物理量的非负性或概率分布的归一化。本练习将交替最小化应用于一个在机器学习和信号处理中常见的约束双线性反问题。你将通过编程实践，学习如何将约束（如投影到概率单纯形上）整合到算法的每一步中，并对比有约束和无约束两种方法的表现，深刻理解在处理实际问题时正确施加约束的关键作用 ()。",
            "id": "3097289",
            "problem": "给定一个定义如下的双线性逆问题。设 $A \\in \\mathbb{R}^{m \\times n}$ 是一个已知矩阵，$b \\in \\mathbb{R}^{m}$ 是一个根据双线性传感模型生成的观测向量\n$$\nb \\;=\\; A\\,(x \\odot y),\n$$\n其中 $x \\in \\mathbb{R}^{n}$ 和 $y \\in \\mathbb{R}^{n}$ 是未知向量，满足正性与归一化约束 $x \\ge 0$、$y \\ge 0$、$\\sum_{i=1}^{n} x_i = 1$ 和 $\\sum_{i=1}^{n} y_i = 1$。此处，$\\odot$ 表示逐元素（哈达玛）积。目标是通过最小化平方残差来估计 $x$ 和 $y$\n$$\nf(x,y) \\;=\\; \\tfrac{1}{2}\\,\\left\\|A\\big(\\operatorname{diag}(y)\\,x\\big) - b\\right\\|_2^2 \\;=\\; \\tfrac{1}{2}\\,\\left\\|A\\big(x \\odot y\\big) - b\\right\\|_2^2,\n$$\n在约束条件 $x \\ge 0$、$y \\ge 0$、$\\sum_{i=1}^{n} x_i = 1$ 和 $\\sum_{i=1}^{n} y_i = 1$ 下。\n\n从最小二乘法的基本定义和到闭凸集上的欧几里得投影出发，实现交替最小化 (Alternating Minimization) 算法：交替地对 $f(x,y)$ 进行最小化：当 $y$ 固定时对 $x$ 最小化，以及当 $x$ 固定时对 $y$ 最小化。在每次块更新时，将双线性项线性化以获得一个最小二乘子问题，然后通过到概率单纯形上的欧几里得投影来施加约束\n$$\n\\Delta^n \\;=\\; \\left\\{z \\in \\mathbb{R}^n \\;:\\; z_i \\ge 0 \\text{ for all } i, \\;\\sum_{i=1}^{n} z_i = 1 \\right\\}。\n$$\n为了研究投影的作用，实现并比较两种变体：\n- 变体 $\\mathsf{proj}$：在每次最小二乘块更新后，将更新后的向量投影到 $\\Delta^n$ 上。\n- 变体 $\\mathsf{unproj}$：执行最小二乘块更新而不进行任何投影。\n\n对于每种变体，在固定次数的迭代后，报告：\n- 最终残差 $r \\;=\\; \\left\\|A\\,(x \\odot y) - b\\right\\|_2$。\n- 总约束违反度 $v$，对于一对 $(x,y)$ 定义为\n$$\nv \\;=\\; \\left(\\sum_{i=1}^{n} \\max\\{0, -x_i\\} \\right) \\;+\\; \\left|\\sum_{i=1}^{n} x_i - 1\\right| \\;+\\; \\left(\\sum_{i=1}^{n} \\max\\{0, -y_i\\} \\right) \\;+\\; \\left|\\sum_{i=1}^{n} y_i - 1\\right|。\n$$\n\n您的程序必须实现这两种变体，并为下面描述的每个测试用例计算指标 $(r, v)$。不涉及物理单位。本问题不涉及角度。\n\n测试套件：\n- 案例 1 (理想情况)：$m = 7$，$n = 5$。使用随机种子 $s_A = 0$ 生成具有独立标准正态分布条目的矩阵 $A$。使用随机种子 $s_\\star = 1$ 通过采样速率为 $1$ 的独立指数分布条目生成 $x^\\star$ 和 $y^\\star$，并将每个向量归一化以使其位于 $\\Delta^n$ 中。设置 $b = A\\,(x^\\star \\odot y^\\star)$。使用 $x^{(0)} = \\tfrac{1}{n}\\,\\mathbf{1}$ 和 $y^{(0)} = \\tfrac{1}{n}\\,\\mathbf{1}$ 初始化算法，并在每种变体中执行 $T = 50$ 次交替更新。\n- 案例 2 (边界分量接近于零)：$m = 7$，$n = 5$。使用随机种子 $s_A = 1$ 生成具有独立标准正态分布条目的矩阵 $A$。选择 $\\epsilon = 10^{-6}$ 并设置\n$$\nx^\\star \\;=\\; \\operatorname{normalize}\\big([\\epsilon,\\, 0.7,\\, 0.299999,\\, \\epsilon,\\, \\epsilon]\\big), \\quad\ny^\\star \\;=\\; \\operatorname{normalize}\\big([0.6,\\, \\epsilon,\\, \\epsilon,\\, 0.4,\\, \\epsilon]\\big),\n$$\n其中 $\\operatorname{normalize}(z)$ 表示 $z/\\sum_i z_i$ 以确保其属于 $\\Delta^n$。设置 $b = A\\,(x^\\star \\odot y^\\star)$，使用 $x^{(0)} = \\tfrac{1}{n}\\,\\mathbf{1}$ 和 $y^{(0)} = \\tfrac{1}{n}\\,\\mathbf{1}$ 初始化，并执行 $T = 50$ 次更新。\n- 案例 3 (病态设计)：$m = 7$，$n = 5$。使用随机种子 $s_A = 2$ 生成具有独立标准正态分布条目的矩阵 $A$，并通过对角矩阵 $D = \\operatorname{diag}([1,\\, 10^{-2},\\, 10^{-3},\\, 1,\\, 10^{-4}])$ 缩放其列。使用随机种子 $s_\\star = 3$ 通过采样速率为 $1$ 的独立指数分布条目生成 $x^\\star$ 和 $y^\\star$，并将每个向量归一化以使其位于 $\\Delta^n$ 中。设置 $b = A\\,(x^\\star \\odot y^\\star)$，使用 $x^{(0)} = \\tfrac{1}{n}\\,\\mathbf{1}$ 和 $y^{(0)} = \\tfrac{1}{n}\\,\\mathbf{1}$ 初始化，并执行 $T = 50$ 次更新。\n\n实现要求：\n- 在每次固定 $y$ 的 $x$ 更新中，将 $f(x,y)$ 简化为形如 $\\min_x \\tfrac{1}{2}\\|B\\,x - b\\|_2^2$ 的最小二乘问题，其中矩阵 $B$ 被适当地定义，然后通过最小二乘解更新 $x$，在变体 $\\mathsf{proj}$ 中随后投影到 $\\Delta^n$ 上，或在变体 $\\mathsf{unproj}$ 中不进行投影。\n- 在每次固定 $x$ 的 $y$ 更新中，类似地简化为 $\\min_y \\tfrac{1}{2}\\|C\\,y - b\\|_2^2$，其中矩阵 $C$ 被适当地定义，在变体 $\\mathsf{proj}$ 中随后进行投影，或在变体 $\\mathsf{unproj}$ 中不进行投影。\n- 完全按照上述规定使用确定性随机种子。\n\n最终输出格式：\n您的程序应生成一行输出，按顺序包含所有十二个结果\n$$\n\\big[r^{\\mathrm{proj}}_1,\\, r^{\\mathrm{unproj}}_1,\\, v^{\\mathrm{proj}}_1,\\, v^{\\mathrm{unproj}}_1,\\, r^{\\mathrm{proj}}_2,\\, r^{\\mathrm{unproj}}_2,\\, v^{\\mathrm{proj}}_2,\\, v^{\\mathrm{unproj}}_2,\\, r^{\\mathrm{proj}}_3,\\, r^{\\mathrm{unproj}}_3,\\, v^{\\mathrm{proj}}_3,\\, v^{\\mathrm{unproj}}_3\\big],\n$$\n以逗号分隔的列表形式打印，并用方括号括起来（例如，$[a,b,c,d,e,f,g,h,i,j,k,l]$），其中每个条目都是一个浮点数。",
            "solution": "当前的问题是，从通过双线性传感模型 $b = A(x \\odot y)$ 获取的观测向量 $b \\in \\mathbb{R}^m$ 中，估计两个未知向量 $x \\in \\mathbb{R}^n$ 和 $y \\in \\mathbb{R}^n$。其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个已知矩阵，$\\odot$ 表示逐元素积。该估计被表述为一个约束优化问题，旨在最小化平方残差，同时对 $x$ 和 $y$ 施加正性和归一化约束。\n\n目标函数由下式给出：\n$$\nf(x,y) = \\tfrac{1}{2}\\,\\left\\|A\\big(x \\odot y\\big) - b\\right\\|_2^2\n$$\n约束要求 $x$ 和 $y$ 都属于概率单纯形 $\\Delta^n$：\n$$\n\\Delta^n = \\left\\{z \\in \\mathbb{R}^n \\mid z_i \\ge 0 \\text{ for all } i, \\text{ and } \\sum_{i=1}^{n} z_i = 1 \\right\\}\n$$\n由于双线性项 $x \\odot y$ 的存在，目标函数 $f(x,y)$ 是非凸的，这使得寻找全局最小值具有挑战性。该问题指定使用交替最小化 (Alternating Minimization, AM) 算法，这是一种用于处理具有多个变量块问题的常用迭代方法。AM 算法通过固定一组变量并对另一组变量最小化目标函数，然后交换它们的作用来进行。\n\n此问题的 AM 算法通过两个主要步骤进行迭代：\n1.  **$x$-最小化**：固定 $y$ 为其当前估计值 $y^{(k)}$ 并求解 $x$。\n2.  **$y$-最小化**：固定 $x$ 为其新计算的估计值 $x^{(k+1)}$ 并求解 $y$。\n\n让我们详细说明每个子问题。\n\n**1. $x$-最小化子问题：**\n当 $y=y^{(k)}$ 保持不变时，目标函数仅是 $x$ 的函数：\n$$\n\\min_x \\tfrac{1}{2}\\,\\left\\|A\\big(x \\odot y^{(k)}\\big) - b\\right\\|_2^2\n$$\n我们可以通过定义一个对角矩阵 $D_y = \\operatorname{diag}(y^{(k)})$，将逐元素积表示为矩阵-向量积。由此，$x \\odot y^{(k)} = D_y x$。于是，该子问题是一个标准的线性最小二乘问题：\n$$\n\\min_x \\tfrac{1}{2}\\,\\left\\| (A D_y) x - b \\right\\|_2^2\n$$\n令 $B_k = A \\operatorname{diag}(y^{(k)})$。无约束最小化解 $x_{LS}$ 是正规方程 $(B_k^T B_k) x = B_k^T b$ 的解。该解在形式上由 $x_{LS} = B_k^\\dagger b$ 给出，其中 $B_k^\\dagger$ 是 $B_k$ 的 Moore-Penrose 伪逆。\n\n**2. $y$-最小化子问题：**\n类似地，在获得 $x$ 的更新估计值（我们记为 $x^{(k+1)}$）后，我们固定它并求解 $y$。子问题是：\n$$\n\\min_y \\tfrac{1}{2}\\,\\left\\|A\\big(x^{(k+1)} \\odot y\\big) - b\\right\\|_2^2\n$$\n使用恒等式 $x^{(k+1)} \\odot y = \\operatorname{diag}(x^{(k+1)}) y$，我们定义 $D_x = \\operatorname{diag}(x^{(k+1)})$ 并得到另一个线性最小二乘问题：\n$$\n\\min_y \\tfrac{1}{2}\\,\\left\\| (A D_x) y - b \\right\\|_2^2\n$$\n令 $C_{k+1} = A \\operatorname{diag}(x^{(k+1)})$。无约束最小化解是 $y_{LS} = C_{k+1}^\\dagger b$。\n\n该问题要求比较两种处理约束的变体。\n\n**变体 $\\mathsf{unproj}$**：在此变体中，迭代更新期间忽略约束。更新步骤就是简单的无约束最小二乘解：\n$$\nx^{(k+1)} = \\left(A \\operatorname{diag}(y^{(k)})\\right)^\\dagger b\n$$\n$$\ny^{(k+1)} = \\left(A \\operatorname{diag}(x^{(k+1)})\\right)^\\dagger b\n$$\n约束未被强制执行，因此最终的迭代结果 $x$ 和 $y$ 不保证位于 $\\Delta^n$ 中。\n\n**变体 $\\mathsf{proj}$**：此变体在每次最小二乘更新后，通过将中间解投影到 $\\Delta^n$ 上来强制执行单纯形约束。步骤如下：\n1.  计算 $x$ 的无约束更新：$x_{LS} = \\left(A \\operatorname{diag}(y^{(k)})\\right)^\\dagger b$。\n2.  投影到单纯形上：$x^{(k+1)} = \\operatorname{proj}_{\\Delta^n}(x_{LS})$。\n3.  计算 $y$ 的无约束更新：$y_{LS} = \\left(A \\operatorname{diag}(x^{(k+1)})\\right)^\\dagger b$。\n4.  投影到单纯形上：$y^{(k+1)} = \\operatorname{proj}_{\\Delta^n}(y_{LS})$。\n\n**到概率单纯形上的欧几里得投影**\n向量 $z \\in \\mathbb{R}^n$ 到 $\\Delta^n$ 上的投影是以下凸优化问题的解：\n$$\n\\operatorname{proj}_{\\Delta^n}(z) = \\arg\\min_{p \\in \\Delta^n} \\tfrac{1}{2} \\|p-z\\|_2^2\n$$\n这个问题可以被高效地求解。Karush-Kuhn-Tucker (KKT) 条件意味着投影后的向量 $p$ 具有形式 $p_i = \\max(0, z_i - \\mu)$（对于所有 $i=1, \\dots, n$），其中 $\\mu$ 是一个拉格朗日乘子，其选择是为了满足约束 $\\sum_i p_i = 1$。这意味着我们必须找到一个 $\\mu$ 使得 $\\sum_{i=1}^n \\max(0, z_i - \\mu) = 1$。函数 $g(\\mu) = \\sum_{i=1}^n \\max(0, z_i - \\mu)$ 是连续、分段线性和单调递减的。找到 $g(\\mu) - 1 = 0$ 的根可以得到正确的 $\\mu$。一个有效的算法将 $z$ 的元素按降序排序，例如 $z_{(1)} \\ge z_{(2)} \\ge \\dots \\ge z_{(n)}$，并找到一个索引 $\\rho$，使得 $\\mu$ 由前 $\\rho$ 个元素确定。该算法如下：\n1. 将 $z$ 排序为 $u$：$u_1 \\ge u_2 \\ge \\dots \\ge u_n$。\n2. 找到 $\\rho = \\max\\left\\{j \\in \\{1, \\dots, n\\} \\mid u_j - \\frac{1}{j}\\left(\\sum_{i=1}^j u_i - 1\\right) > 0\\right\\}$。\n3. 计算阈值 $\\theta = \\frac{1}{\\rho}\\left(\\sum_{i=1}^\\rho u_i - 1\\right)$。\n4. 投影的分量是 $p_i = \\max(0, z_i - \\theta)$。\n\n这个过程使我们能够正确地实现变体 $\\mathsf{proj}$。数值实现将使用 `numpy.linalg.lstsq` 来稳定地求解最小二乘子问题，随后对变体 $\\mathsf{proj}$ 应用投影算法。最后，我们按照规定为每种变体和每个测试用例计算残差 $r$ 和约束违反度 $v$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef project_simplex(v: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Projects a vector v onto the probability simplex.\n    \"\"\"\n    n_features = v.shape[0]\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    rho_candidates = np.nonzero(u * np.arange(1, n_features + 1) > cssv - 1)\n    if len(rho_candidates[0]) == 0:\n        # This case can happen if v has all very negative values\n        return np.ones(n_features) / n_features\n    rho = rho_candidates[0][-1]\n    theta = (cssv[rho] - 1) / (rho + 1.0)\n    return np.maximum(v - theta, 0)\n\ndef calculate_metrics(A: np.ndarray, b: np.ndarray, x: np.ndarray, y: np.ndarray):\n    \"\"\"\n    Calculates the final residual and constraint violation.\n    \"\"\"\n    # Residual\n    residual_vec = A @ (x * y) - b\n    r = np.linalg.norm(residual_vec)\n\n    # Constraint violation\n    v_x = np.sum(np.maximum(0, -x)) + np.abs(np.sum(x) - 1)\n    v_y = np.sum(np.maximum(0, -y)) + np.abs(np.sum(y) - 1)\n    v = v_x + v_y\n    \n    return r, v\n\ndef run_am_variant(A: np.ndarray, b: np.ndarray, x0: np.ndarray, y0: np.ndarray, T: int, variant: str):\n    \"\"\"\n    Runs an Alternating Minimization variant ('proj' or 'unproj').\n    \"\"\"\n    x, y = x0.copy(), y0.copy()\n    n = A.shape[1]\n\n    for _ in range(T):\n        # x-update\n        # Reformulate as || (A * diag(y)) * x - b ||^2\n        if np.all(np.abs(y) < 1e-9): # handle case where y is near zero\n            B = np.zeros_like(A)\n        else:\n            B = A * y.reshape(1, n)\n        \n        x_ls = np.linalg.lstsq(B, b, rcond=None)[0]\n        \n        if variant == 'proj':\n            x = project_simplex(x_ls)\n        else: # unproj\n            x = x_ls\n\n        # y-update\n        # Reformulate as || (A * diag(x)) * y - b ||^2\n        if np.all(np.abs(x) < 1e-9):\n            C = np.zeros_like(A)\n        else:\n            C = A * x.reshape(1, n)\n            \n        y_ls = np.linalg.lstsq(C, b, rcond=None)[0]\n        \n        if variant == 'proj':\n            y = project_simplex(y_ls)\n        else: # unproj\n            y = y_ls\n\n    return calculate_metrics(A, b, x, y)\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and generate the final output.\n    \"\"\"\n    test_cases = []\n    \n    # Case 1 Setup\n    m, n, s_A, s_star, T = 7, 5, 0, 1, 50\n    rng_A = np.random.default_rng(s_A)\n    A1 = rng_A.standard_normal((m, n))\n    rng_star = np.random.default_rng(s_star)\n    x_star_raw = rng_star.exponential(1.0, size=n)\n    x_star1 = x_star_raw / np.sum(x_star_raw)\n    y_star_raw = rng_star.exponential(1.0, size=n)\n    y_star1 = y_star_raw / np.sum(y_star_raw)\n    b1 = A1 @ (x_star1 * y_star1)\n    x0_1 = np.ones(n) / n\n    y0_1 = np.ones(n) / n\n    test_cases.append({'A': A1, 'b': b1, 'x0': x0_1, 'y0': y0_1, 'T': T})\n\n    # Case 2 Setup\n    m, n, s_A, T = 7, 5, 1, 50\n    rng_A = np.random.default_rng(s_A)\n    A2 = rng_A.standard_normal((m, n))\n    eps = 1e-6\n    x_star_raw = np.array([eps, 0.7, 0.299999, eps, eps])\n    x_star2 = x_star_raw / np.sum(x_star_raw)\n    y_star_raw = np.array([0.6, eps, eps, 0.4, eps])\n    y_star2 = y_star_raw / np.sum(y_star_raw)\n    b2 = A2 @ (x_star2 * y_star2)\n    x0_2 = np.ones(n) / n\n    y0_2 = np.ones(n) / n\n    test_cases.append({'A': A2, 'b': b2, 'x0': x0_2, 'y0': y0_2, 'T': T})\n\n    # Case 3 Setup\n    m, n, s_A, s_star, T = 7, 5, 2, 3, 50\n    rng_A = np.random.default_rng(s_A)\n    A_raw = rng_A.standard_normal((m, n))\n    D = np.diag([1.0, 1e-2, 1e-3, 1.0, 1e-4])\n    A3 = A_raw @ D\n    rng_star = np.random.default_rng(s_star)\n    x_star_raw = rng_star.exponential(1.0, size=n)\n    x_star3 = x_star_raw / np.sum(x_star_raw)\n    y_star_raw = rng_star.exponential(1.0, size=n)\n    y_star3 = y_star_raw / np.sum(y_star_raw)\n    b3 = A3 @ (x_star3 * y_star3)\n    x0_3 = np.ones(n) / n\n    y0_3 = np.ones(n) / n\n    test_cases.append({'A': A3, 'b': b3, 'x0': x0_3, 'y0': y0_3, 'T': T})\n\n    results = []\n    for case in test_cases:\n        A, b, x0, y0, T_iter = case['A'], case['b'], case['x0'], case['y0'], case['T']\n        \n        # Run proj variant\n        r_proj, v_proj = run_am_variant(A, b, x0, y0, T_iter, 'proj')\n        \n        # Run unproj variant\n        r_unproj, v_unproj = run_am_variant(A, b, x0, y0, T_iter, 'unproj')\n        \n        results.extend([r_proj, r_unproj, v_proj, v_unproj])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```"
        }
    ]
}