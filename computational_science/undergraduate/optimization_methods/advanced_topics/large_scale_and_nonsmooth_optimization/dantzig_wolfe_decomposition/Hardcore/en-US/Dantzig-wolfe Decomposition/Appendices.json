{
    "hands_on_practices": [
        {
            "introduction": "The power of Dantzig-Wolfe decomposition begins with a thoughtful problem partition. This exercise challenges you to think like an optimization architect, analyzing a linear program's constraint structure to find the most effective way to split it into blocks. By maximizing the number of \"pure\" constraints within each block, you will practice minimizing the complexity of the linking master problem, a key principle for designing efficient decompositions .",
            "id": "3116333",
            "problem": "Consider the following Linear Programming (LP) problem in standard form, with decision variables $x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6} \\geq 0$:\n$$\n\\min \\; c^{\\top} x \\quad \\text{with} \\quad c = (1, 2, 3, 1, 2, 3),\n$$\nsubject to the equality constraints\n$$\n\\begin{aligned}\nx_{1} + x_{2} + x_{3} = 6, \\\\\nx_{4} + x_{5} + x_{6} = 6, \\\\\nx_{1} + x_{2} = 4, \\\\\nx_{4} + x_{5} = 4, \\\\\nx_{3} + x_{6} = 4, \\\\\nx_{2} + x_{4} = 4.\n\\end{aligned}\n$$\nIn the Dantzig-Wolfe decomposition (DWD), an LP is reformulated by partitioning its variables into blocks, which induces a partition of the constraint rows into subproblem-specific constraints and linking constraints. A constraint row is called “pure” if all its nonzero coefficients involve variables from a single block; otherwise it is “linking.” Denote by $A_{i}$ the subproblem matrix collecting the pure constraints assigned to block $i$.\n\nYou are to partition the variables into exactly two blocks of equal size:\n$$\n\\{x_{1}, x_{2}, x_{3}\\} \\cup \\{x_{4}, x_{5}, x_{6}\\},\n$$\nbut you may assign any three variables to block $1$ and the remaining three to block $2$. Define the block separability index (BSI) $S$ as the number of pure constraints under a given partition; equivalently, $S$ counts how many constraint rows can be assigned to the subproblem matrices $A_{1}$ or $A_{2}$ rather than being linking.\n\nAmong all such two-block partitions, determine the maximal possible value of $S$, denoted $S_{\\max}$. Your final answer must be a single integer giving $S_{\\max}$.",
            "solution": "Let the set of all variables be $V = \\{x_1, \\dots, x_6\\}$. We are seeking a partition of $V$ into two disjoint blocks, $B_1$ and $B_2$, such that $|B_1| = |B_2| = 3$. A constraint is pure if all its variables are in a single block. We denote the constraints by the sets of variables they involve:\n- $C_1: \\{x_1, x_2, x_3\\}$\n- $C_2: \\{x_4, x_5, x_6\\}$\n- $C_3: \\{x_1, x_2\\}$\n- $C_4: \\{x_4, x_5\\}$\n- $C_5: \\{x_3, x_6\\}$\n- $C_6: \\{x_2, x_4\\}$\n\nThe goal is to find the partition that maximizes the number of pure constraints, $S$.\n\nFirst, consider the partition suggested by the problem structure: $B_1 = \\{x_1, x_2, x_3\\}$ and $B_2 = \\{x_4, x_5, x_6\\}$. Let's count the pure constraints for this partition:\n- $C_1$ is pure because $\\{x_1, x_2, x_3\\} = B_1$.\n- $C_2$ is pure because $\\{x_4, x_5, x_6\\} = B_2$.\n- $C_3$ is pure because $\\{x_1, x_2\\} \\subseteq B_1$.\n- $C_4$ is pure because $\\{x_4, x_5\\} \\subseteq B_2$.\n- $C_5$ is linking because $x_3 \\in B_1$ and $x_6 \\in B_2$.\n- $C_6$ is linking because $x_2 \\in B_1$ and $x_4 \\in B_2$.\nFor this partition, $S=4$. This establishes a lower bound: $S_{\\max} \\ge 4$.\n\nNext, we must determine if a value of $S > 4$ is achievable. To have $S \\ge 5$, at least one of the constraints $C_1$ or $C_2$ must be pure, because the other four constraints ($C_3, C_4, C_5, C_6$) can contribute at most 4 to $S$.\n\nAssume $C_1$ is pure. Since $|B_1|=3$ and $C_1$ involves three variables, one block must be exactly $B_1 = \\{x_1, x_2, x_3\\}$. This forces the other block to be its complement, $B_2 = \\{x_4, x_5, x_6\\}$. This is the same partition we just analyzed, which gives $S=4$. The same logic applies if we assume $C_2$ is pure; it also forces the same partition, for which $S=4$.\n\nIn any partition where $S \\ge 5$, at least one of $C_1$ or $C_2$ must be pure. This uniquely determines the partition. However, for that unique partition, we have already calculated that $S=4$. This leads to a contradiction ($S \\ge 5$ and $S=4$). Therefore, it is impossible to have more than 4 pure constraints.\n\nThe maximal possible value of $S$ is 4.",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "Once a problem is decomposed, the column generation algorithm is driven by the pricing subproblem, which uses dual variables to find improving columns. This practice problem demystifies this core mechanism by placing it in the tangible context of a logistics network, where dual variables act as shadow prices on congested resources. You will calculate the \"attractiveness\" of different routing options, gaining a concrete understanding of how reduced cost guides the optimization process toward a solution .",
            "id": "3116329",
            "problem": "A logistics planner must route two commodities from a single source $s$ to a single sink $t$ through a directed network with node set $N=\\{s,a,b,t\\}$ and arc set $A=\\{(s,a),(s,b),(a,b),(a,t),(b,t)\\}$. Each unit of commodity $k \\in \\{1,2\\}$ delivered from $s$ to $t$ yields revenue $R_k$ and incurs per-arc operating cost $c_{ij}$ on each used arc $(i,j) \\in A$. Arc $(i,j)$ has capacity $U_{ij}$ that couples the commodities: the total flow of all commodities on $(i,j)$ cannot exceed $U_{ij}$. Commodity $k$ has demand upper bound $d_k$: the total routed units of commodity $k$ cannot exceed $d_k$. The planner solves a linear programming master problem in a Dantzig–Wolfe decomposition that uses $s$–$t$ paths as columns; a column corresponds to sending $1$ unit of a specific commodity $k$ along a specific $s$–$t$ path. The master problem is a maximization of total net revenue subject to the arc capacity and commodity demand constraints.\n\nSuppose a restricted master problem has been solved and produced the following current dual multipliers: for each arc $(i,j) \\in A$, the dual on its capacity constraint is $\\pi_{ij} \\ge 0$, and for each commodity $k$, the dual on its demand constraint is $\\mu_k \\ge 0$. The pricing subproblem for each commodity searches for an $s$–$t$ path that maximizes the column’s attractiveness given these multipliers.\n\nData:\n- Revenues: $R_1 = 8$, $R_2 = 5$.\n- Arc operating costs: $c_{s,a}=1$, $c_{s,b}=2$, $c_{a,b}=1$, $c_{a,t}=3$, $c_{b,t}=2$.\n- Current dual multipliers on capacities: $\\pi_{s,a}=0.5$, $\\pi_{s,b}=1.0$, $\\pi_{a,b}=0.2$, $\\pi_{a,t}=1.2$, $\\pi_{b,t}=0.3$.\n- Current dual multipliers on demands: $\\mu_1=1.5$, $\\mu_2=0.5$.\n\nConsider the three simple $s$–$t$ paths available in this network: $p_1: s \\to a \\to t$, $p_2: s \\to b \\to t$, and $p_3: s \\to a \\to b \\to t$.\n\nWhich option correctly states the economic interpretation of the dual variables $\\pi_{ij}$ and also identifies the single column that the pricing subproblem would select as the most promising new column under the given multipliers?\n\nA. The $\\pi_{ij}$ are shadow prices (marginal values) per unit of capacity on arc $(i,j)$. Pricing would select commodity $1$ on path $p_3: s \\to a \\to b \\to t$.\n\nB. The $\\pi_{ij}$ are penalties per unit of demand for each commodity. Pricing would select commodity $1$ on path $p_2: s \\to b \\to t$ because it has the smallest base operating cost.\n\nC. The $\\pi_{ij}$ are subsidies that reduce the effective cost on congested arcs. Pricing would select commodity $2$ on path $p_1: s \\to a \\to t$ because it yields positive profit before considering prices.\n\nD. The $\\pi_{ij}$ are shadow prices (marginal values) per unit of capacity on arc $(i,j)$. Pricing would find no path with positive attractiveness for any commodity, so no column would be selected.",
            "solution": "We begin from core linear programming and decomposition principles. A Dantzig–Wolfe decomposition expresses a problem with coupling constraints (here, arc capacities) as a master problem over extreme points (here, $s$–$t$ paths) of subproblems (one per commodity), with a pricing step that generates improving columns. In linear programming, for a maximization problem in the form $\\max\\{c^\\top x : Ax \\le b, x \\ge 0\\}$, the dual is $\\min\\{b^\\top y : A^\\top y \\ge c, y \\ge 0\\}$, and the reduced cost of a variable (column) $x_j$ is $r_j = c_j - a_j^\\top y$, where $a_j$ is the $j$-th column of $A$ and $y$ are the current dual multipliers. A column with $r_j  0$ can improve the master objective; the pricing subproblem seeks the column(s) with maximum positive reduced cost.\n\nApply these facts to the present master problem. A column is indexed by a commodity $k$ and a path $p$, representing $1$ unit of commodity $k$ routed along $p$ from $s$ to $t$. Its primal objective coefficient is the net revenue per unit before pricing: this is the commodity revenue minus the sum of arc operating costs on the path, namely\n$$\nc_{k,p} = R_k - \\sum_{(i,j)\\in p} c_{ij}.\n$$\nIts column in the capacity constraints contributes $1$ unit on each arc $(i,j)$ that is in $p$, and in the demand constraint for commodity $k$ it contributes $1$ unit. Therefore, with dual multipliers $\\pi_{ij} \\ge 0$ for arc capacities and $\\mu_k \\ge 0$ for commodity demands, the reduced cost of column $(k,p)$ is\n$$\nr_{k,p} = c_{k,p} - \\sum_{(i,j)\\in p} \\pi_{ij} - \\mu_k \\;=\\; R_k \\;-\\; \\sum_{(i,j)\\in p} \\big(c_{ij} + \\pi_{ij}\\big) \\;-\\; \\mu_k.\n$$\nThus, for each commodity $k$, the pricing subproblem is to find the $s$–$t$ path $p$ that minimizes the path length with respect to arc weights $c_{ij} + \\pi_{ij}$ and then check whether $r_{k,p}  0$. Economically, each $\\pi_{ij}$ is the shadow price (marginal value) of one unit of capacity on arc $(i,j)$: it is the rate at which the optimal objective would improve if $U_{ij}$ were relaxed by $1$ unit. Consequently, in pricing, capacity use on an arc is “charged” at price $\\pi_{ij}$ on top of its operating cost $c_{ij}$.\n\nCompute the price-adjusted arc weights and path totals:\n- Arc weights $c_{ij} + \\pi_{ij}$:\n  - $(s,a)$: $1 + 0.5 = 1.5$,\n  - $(s,b)$: $2 + 1.0 = 3.0$,\n  - $(a,b)$: $1 + 0.2 = 1.2$,\n  - $(a,t)$: $3 + 1.2 = 4.2$,\n  - $(b,t)$: $2 + 0.3 = 2.3$.\n- Path totals $\\sum_{(i,j)\\in p} (c_{ij} + \\pi_{ij})$:\n  - $p_1: s \\to a \\to t$: $1.5 + 4.2 = 5.7$,\n  - $p_2: s \\to b \\to t$: $3.0 + 2.3 = 5.3$,\n  - $p_3: s \\to a \\to b \\to t$: $1.5 + 1.2 + 2.3 = 5.0$.\n\nNow evaluate reduced costs $r_{k,p} = R_k - \\mu_k - \\text{(path total)}$:\n\nFor commodity $1$ ($R_1=8$, $\\mu_1=1.5$):\n- $r_{1,p_1} = 8 - 1.5 - 5.7 = 0.8$,\n- $r_{1,p_2} = 8 - 1.5 - 5.3 = 1.2$,\n- $r_{1,p_3} = 8 - 1.5 - 5.0 = 1.5$.\n\nAll three are positive, and the largest is $r_{1,p_3} = 1.5$ for path $p_3: s \\to a \\to b \\to t$.\n\nFor commodity $2$ ($R_2=5$, $\\mu_2=0.5$):\n- $r_{2,p_1} = 5 - 0.5 - 5.7 = -1.2$,\n- $r_{2,p_2} = 5 - 0.5 - 5.3 = -0.8$,\n- $r_{2,p_3} = 5 - 0.5 - 5.0 = -0.5$.\n\nAll are nonpositive for commodity $2$.\n\nTherefore, the pricing subproblem would select the column for commodity $1$ on path $p_3$ as the most attractive improving column (largest positive reduced cost). The economic interpretation of $\\pi_{ij}$ is indeed that of shadow prices for arc capacities, i.e., marginal values per unit of capacity.\n\nOption-by-option analysis:\n\nA. States that $\\pi_{ij}$ are shadow prices per unit capacity on arc $(i,j)$, which matches the dual interpretation as marginal values of capacity. Identifies commodity $1$ on $p_3: s \\to a \\to b \\to t$ as the selected column, which is correct because it has the largest positive reduced cost $1.5$. Verdict: Correct.\n\nB. Claims that $\\pi_{ij}$ are penalties per unit demand; this confuses capacity duals with demand duals. It also selects $p_2$ based on smallest base operating cost, ignoring the capacity prices and demand dual, which is not the pricing criterion. Verdict: Incorrect.\n\nC. Describes $\\pi_{ij}$ as subsidies that reduce effective cost; in fact, they act as prices added to operating costs in the reduced cost. It also selects commodity $2$ on $p_1$, but all reduced costs for commodity $2$ are negative. Verdict: Incorrect.\n\nD. Correctly interprets $\\pi_{ij}$ as shadow prices, but claims no positive reduced cost exists. In fact, commodity $1$ has positive reduced costs, with $p_3$ being the best. Verdict: Incorrect.\n\nHence, only option A is correct.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Dantzig-Wolfe is a powerful technique, but it is not a universal solution; a poorly chosen decomposition can be counterproductive. This cautionary exercise illustrates a critical trade-off by showing how a seemingly simple subproblem structure can lead to an exponential explosion in the number of columns in the master problem. By working through this example, you will learn to appreciate why the internal structure of the subproblem blocks is a crucial consideration for avoiding computational bottlenecks .",
            "id": "3116357",
            "problem": "Consider the following Linear Programming (LP) problem, where Linear Programming (LP) denotes optimization of a linear objective over a polyhedral feasible region. Let $n$ be an even positive integer, and define the decision vector $x \\in \\mathbb{R}^{n}$ with components $x_{i}$, $i=1,\\dots,n$. The LP is\n$$\n\\min\\ \\sum_{i=1}^{n} c_{i} x_{i}\n\\quad \\text{subject to} \\quad\n\\sum_{i=1}^{n} x_{i} = \\frac{n}{2}, \\quad 0 \\leq x_{i} \\leq 1 \\ \\text{for all } i=1,\\dots,n,\n$$\nwith strictly increasing costs $c_{i}$ satisfying $c_{1}  c_{2}  \\dots  c_{n}$.\n\n1. Using only fundamental definitions from polyhedral theory and linear optimization (extreme points, convex hull, and linear objectives), justify why the original LP can be solved directly by a simple reasoning based on the structure of its feasible region and the monotonicity of the costs.\n\n2. Now, construct a Dantzig-Wolfe decomposition (DWD) of the above LP by partitioning the variables into $m$ blocks of equal size $k$, so that $n = m k$. For each block $j \\in \\{1,\\dots,m\\}$, define the local feasible set\n$$\nX_{j} = \\{ x^{(j)} \\in \\mathbb{R}^{k} : 0 \\leq x^{(j)}_{p} \\leq 1 \\ \\text{for } p=1,\\dots,k \\},\n$$\nand use a single linking constraint that couples all blocks:\n$$\n\\sum_{j=1}^{m} \\sum_{p=1}^{k} x^{(j)}_{p} = \\frac{n}{2}.\n$$\nIn the Dantzig-Wolfe reformulation, each block $j$ is represented by a convex combination of extreme points of $X_{j}$, and the master problem contains one column per extreme point of each block.\n\nDerive, from first principles, the total number of distinct columns that appear in the master problem under this decomposition, expressed as a function of $m$ and $k$.\n\n3. Finally, evaluate your expression numerically for $m=5$ and $k=8$ (so $n=40$ and $\\frac{n}{2}=20$). No rounding is required. Your final numeric answer must be the total number of columns in the Dantzig-Wolfe master problem under this decomposition.\n\nAdditionally, based on your derivation, briefly identify and explain practical criteria to avoid over-decomposition that causes a large number of columns in the Dantzig-Wolfe master problem. Your explanation should be grounded in the structure of extreme points and linking constraints, but your final answer must be the single number requested above.",
            "solution": "This problem has three parts, followed by an additional explanation.\n\n**1. Direct Solution of the LP**\nThe fundamental theorem of linear programming states that if an optimal solution exists, at least one must occur at an extreme point of the feasible region. The feasible region is the intersection of the $n$-dimensional hypercube $[0,1]^n$ and the hyperplane $\\sum_{i=1}^{n} x_{i} = n/2$. The extreme points of this region are binary vectors $x \\in \\{0, 1\\}^n$ that also satisfy the sum constraint, meaning they must have exactly $n/2$ components equal to 1. To see this, any point with a fractional component can be expressed as a convex combination of two other feasible points, and thus cannot be an extreme point.\n\nTo minimize the objective $\\sum c_i x_i$ with strictly increasing costs $c_1  c_2  \\dots  c_n$, we must assign $x_i=1$ to the variables with the smallest costs. To satisfy the constraint that exactly $n/2$ variables are 1, the unique optimal solution is to set the first $n/2$ variables to 1 and the rest to 0:\n$$\nx_i = 1 \\text{ for } i = 1, \\dots, n/2; \\quad x_i = 0 \\text{ for } i = n/2+1, \\dots, n.\n$$\n\n**2. Number of Columns in the Dantzig-Wolfe Master Problem**\nIn the proposed Dantzig-Wolfe decomposition, the problem is split into $m$ blocks, each of size $k$. The feasible region for each block $j$, $X_j$, is a $k$-dimensional hypercube $[0,1]^k$. The Dantzig-Wolfe master problem is formulated using columns that correspond to the extreme points of these block-feasible regions.\n\nThe extreme points of a $k$-dimensional hypercube are its vertices. A $k$-hypercube has $2^k$ vertices. Since there are $m$ independent blocks, and each block contributes its full set of extreme points as potential columns to the master problem, the total number of columns is the product of the number of blocks and the number of extreme points per block.\n$$\n\\text{Total Columns} = (\\text{Number of blocks}) \\times (\\text{Number of extreme points per block}) = m \\times 2^k\n$$\n\n**3. Numerical Evaluation**\nGiven the parameters $m=5$ and $k=8$ (so $n=40$), we can calculate the total number of columns using the formula derived above:\n$$\n\\text{Total Columns} = 5 \\times 2^8 = 5 \\times 256 = 1280\n$$\n\n**Explanation of Practical Criteria:**\nThe total number of columns, $m \\times 2^k$, grows exponentially with the block size $k$. This decomposition is poor because the subproblem blocks $X_j$ are simple hypercubes with no internal constraints, leading to the maximum possible number of extreme points ($2^k$) for their dimension. A good decomposition relies on subproblems with a rich constraint structure that drastically reduces the number of extreme points relative to their dimension. In this case, \"under-decomposition\" (using a small number of large blocks, i.e., small $m$ and large $k$) causes the number of master problem columns to explode, making the approach computationally intractable. The key practical criterion is to define blocks such that their feasible regions ($X_j$) have a manageable number of extreme points, which usually means they are constrained by more than just simple bounds.",
            "answer": "$$\n\\boxed{1280}\n$$"
        }
    ]
}