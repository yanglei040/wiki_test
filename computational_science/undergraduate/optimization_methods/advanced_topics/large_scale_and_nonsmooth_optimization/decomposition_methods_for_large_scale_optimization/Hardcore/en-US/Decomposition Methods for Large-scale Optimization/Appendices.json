{
    "hands_on_practices": [
        {
            "introduction": "This first exercise serves as a foundational practice in dual decomposition. You will tackle a convex optimization problem with a separable objective function, complicated by a single coupling constraint that links all the variables. The core idea you will explore is Lagrangian relaxation, where this difficult constraint is moved into the objective function, penalized by a dual variable or \"price.\" This allows the problem to break down into smaller, independent subproblems that are easy to solve, a hallmark of decomposition methods.\n\nBy implementing the dual subgradient ascent algorithm, you will gain hands-on experience in the essential mechanics of this technique. This practice focuses on how to iteratively update the dual variables and recover a feasible primal solution from the dual iterates. Most importantly, you will investigate the critical role of the penalty parameter $\\rho$, observing how its value creates a trade-off between the convergence of the dual value and the feasibility of the final solution .",
            "id": "3116747",
            "problem": "You are given a family of convex, separable optimization problems with a single linear coupling constraint. For an integer $m \\ge 2$, each problem has the form\n$$\n\\min_{x \\in \\mathbb{R}^m} \\;\\; f(x) = \\sum_{i=1}^m f_i(x_i)\n\\quad \\text{subject to} \\quad \\sum_{i=1}^m A_i x_i = b, \\quad \\ell_i \\le x_i \\le u_i,\n$$\nwhere each component function is quadratic,\n$$\nf_i(x_i) = \\tfrac{1}{2} q_i x_i^2 + c_i x_i,\n$$\nwith $q_i > 0$, and the bounds satisfy $\\ell_i  u_i$ so the feasible set is nonempty and bounded. The coupling is a single scalar equality constraint with coefficients $A_i$ and right-hand side $b$.\n\nStarting from the definitions of the (unaugmented) Lagrangian and its dual function, consider the standard Lagrangian relaxation with dual variable $\\lambda \\in \\mathbb{R}$,\n$$\n\\mathcal{L}(x,\\lambda) = \\sum_{i=1}^m f_i(x_i) + \\lambda \\Big( \\sum_{i=1}^m A_i x_i - b \\Big),\n$$\nand the dual function $g(\\lambda) = \\inf_{x \\in [\\ell,u]} \\mathcal{L}(x,\\lambda)$ where $[\\ell,u]$ denotes the box $\\prod_{i=1}^m [\\ell_i,u_i]$. The dual problem is to maximize $g(\\lambda)$ over $\\lambda \\in \\mathbb{R}$. Use a constant-step subgradient ascent on the dual with step size equal to $1/\\rho$, where $\\rho  0$ is a user-chosen penalty parameter. Specifically, from an initial $\\lambda_0 = 0$, iterate for $k = 0,1,\\ldots,T-1$:\n- Compute $x^{(k)} \\in \\arg\\min_{\\ell \\le x \\le u}\\, \\mathcal{L}(x,\\lambda_k)$ (this is separable over indices $i$).\n- Compute the scalar subgradient $r_k = \\sum_{i=1}^m A_i x_i^{(k)} - b$.\n- Update $\\lambda_{k+1} = \\lambda_k + \\frac{1}{\\rho}\\, r_k$.\n\nAfter $T$ iterations, form the averaged primal candidate $\\bar{x} = \\frac{1}{T} \\sum_{k=0}^{T-1} x^{(k)}$. Let $p^\\star$ denote the optimal primal value of the original constrained problem, and let $d_{\\text{best}}$ denote the best dual value observed across the $T$ iterations, i.e., $d_{\\text{best}} = \\max_{0 \\le k \\le T-1} g(\\lambda_k)$. Quantify:\n- The duality gap $p^\\star - d_{\\text{best}}$ (a nonnegative float).\n- The primal feasibility residual of the recovered average $|\\sum_{i=1}^m A_i \\bar{x}_i - b|$ (a nonnegative float).\n\nYour program must:\n1. Compute $p^\\star$ to high accuracy using only the problem’s core definitions and first-order optimality principles. You must not assume any pre-derived closed forms. Use a numerically robust scalar root-finding approach for the single Lagrange multiplier associated with the equality constraint and the Karush–Kuhn–Tucker conditions to handle the box constraints.\n2. Implement the dual subgradient ascent described above with constant step $1/\\rho$, evaluate $g(\\lambda_k)$ at each iteration using the minimizing $x^{(k)}$, and track $d_{\\text{best}}$.\n3. Recover $\\bar{x}$ and compute the feasibility residual.\n4. Report for each test case the pair of floats: first $p^\\star - d_{\\text{best}}$, then $|\\sum_{i=1}^m A_i \\bar{x}_i - b|$. Each float should be rounded to $6$ decimal places.\n\nFundamental base you must use:\n- The definition of the Lagrangian $\\mathcal{L}(x,\\lambda)$ for equality constraints.\n- The definition of the dual function $g(\\lambda) = \\inf_x \\mathcal{L}(x,\\lambda)$ and the fact that a subgradient of $g$ at $\\lambda$ is the primal residual $\\sum_i A_i x_i^\\star(\\lambda) - b$ for any minimizer $x^\\star(\\lambda)$ of $\\mathcal{L}(\\cdot,\\lambda)$ over the box constraints.\n- The Karush–Kuhn–Tucker conditions for convex quadratic programs with a single linear equality and bound constraints, used to compute $p^\\star$ by a scalar search over the Lagrange multiplier.\n\nTest Suite:\nUse the following five test cases, each specified as $(m, q, c, A, \\ell, u, b, T, \\rho)$, where vectors are given in coordinate order:\n- Case $1$ (happy path, medium scale): $m = 4$, $q = [\\,2,\\,1,\\,4,\\,3\\,]$, $c = [\\,0,\\,-1,\\,2,\\,-0.5\\,]$, $A = [\\,1,\\,-2,\\,1.5,\\,1\\,]$, $\\ell = [\\,-1,\\,0,\\,-0.5,\\,-2\\,]$, $u = [\\,2,\\,2,\\,1.5,\\,1\\,]$, $b = 0.7$, $T = 200$, $\\rho = 0.5$.\n- Case $2$ (same data, milder step): $m = 4$, $q = [\\,2,\\,1,\\,4,\\,3\\,]$, $c = [\\,0,\\,-1,\\,2,\\,-0.5\\,]$, $A = [\\,1,\\,-2,\\,1.5,\\,1\\,]$, $\\ell = [\\,-1,\\,0,\\,-0.5,\\,-2\\,]$, $u = [\\,2,\\,2,\\,1.5,\\,1\\,]$, $b = 0.7$, $T = 200$, $\\rho = 2$.\n- Case $3$ (same data, small step): $m = 4$, $q = [\\,2,\\,1,\\,4,\\,3\\,]$, $c = [\\,0,\\,-1,\\,2,\\,-0.5\\,]$, $A = [\\,1,\\,-2,\\,1.5,\\,1\\,]$, $\\ell = [\\,-1,\\,0,\\,-0.5,\\,-2\\,]$, $u = [\\,2,\\,2,\\,1.5,\\,1\\,]$, $b = 0.7$, $T = 200$, $\\rho = 20$.\n- Case $4$ (boundary optimum on upper bounds): $m = 3$, $q = [\\,1,\\,1,\\,1\\,]$, $c = [\\,0.1,\\,-0.2,\\,0.3\\,]$, $A = [\\,1,\\,1,\\,1\\,]$, $\\ell = [\\,0,\\,0,\\,0\\,]$, $u = [\\,1,\\,1,\\,1\\,]$, $b = 3$, $T = 200$, $\\rho = 2$.\n- Case $5$ (ill-conditioned curvature): $m = 4$, $q = [\\,0.1,\\,10,\\,0.5,\\,8\\,]$, $c = [\\,0,\\,0,\\,0,\\,0\\,]$, $A = [\\,1,\\,1,\\,-1,\\,2\\,]$, $\\ell = [\\,-1,\\,-1,\\,-1,\\,-1\\,]$, $u = [\\,1,\\,1,\\,1,\\,1\\,]$, $b = 0.3$, $T = 200$, $\\rho = 2$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, for each of the five cases, first the duality gap and then the feasibility residual, both rounded to $6$ decimal places. For example, the output must look like\n$$\n[\\;g_1,\\;r_1,\\;g_2,\\;r_2,\\;g_3,\\;r_3,\\;g_4,\\;r_4,\\;g_5,\\;r_5\\;],\n$$\nwhere $g_j$ is $p^\\star - d_{\\text{best}}$ and $r_j$ is $|\\sum_i A_i \\bar{x}_i - b|$ for case $j$. No other text must be printed.",
            "solution": "The problem presents a convex, separable quadratic optimization problem with a single linear coupling constraint and box constraints. The task is to compute the exact primal optimal value, implement a dual subgradient ascent algorithm, and report the resulting duality gap and primal feasibility residual.\n\nThe problem is valid. It is scientifically grounded in the principles of convex optimization and duality theory. The objective function $f(x) = \\sum_{i=1}^m f_i(x_i)$, with each $f_i(x_i) = \\frac{1}{2} q_i x_i^2 + c_i x_i$ and $q_i  0$, is strictly convex. The feasible set, defined by the intersection of a hyperplane $\\sum_{i=1}^m A_i x_i = b$ and a compact box $[\\ell, u] = \\prod_{i=1}^m [\\ell_i, u_i]$, is convex and compact. The problem statement guarantees this set is nonempty. Consequently, a unique optimal solution $x^\\star$ exists, and the problem is well-posed. All parameters and algorithmic details are specified precisely, making the problem self-contained and objective.\n\nOur solution proceeds in two primary stages: first, the computation of the exact primal optimal value $p^\\star$ through the Karush-Kuhn-Tucker (KKT) conditions; and second, the implementation of the specified dual subgradient method to find the best dual bound $d_{\\text{best}}$ and the averaged primal solution $\\bar{x}$.\n\n### Part 1: Exact Primal Solution via KKT Conditions\n\nTo find the exact primal optimal value $p^\\star$, we solve the primal problem directly. The problem is:\n$$\n\\min_{x \\in \\mathbb{R}^m} \\;\\; \\sum_{i=1}^m \\left(\\tfrac{1}{2} q_i x_i^2 + c_i x_i\\right)\n\\quad \\text{s.t.} \\quad \\sum_{i=1}^m A_i x_i = b, \\quad \\ell_i \\le x_i \\le u_i \\text{ for } i=1,\\ldots,m.\n$$\nThis is a convex optimization problem. The KKT conditions provide necessary and sufficient conditions for optimality. We introduce a Lagrange multiplier $\\nu \\in \\mathbb{R}$ for the equality constraint, and multipliers $\\mu_{l,i} \\ge 0$ and $\\mu_{u,i} \\ge 0$ for the lower and upper bound constraints $x_i \\ge \\ell_i$ and $x_i \\le u_i$, respectively. The full Lagrangian is:\n$$\n\\mathcal{L}_{\\text{full}}(x, \\nu, \\mu_l, \\mu_u) = \\sum_{i=1}^m \\left(\\tfrac{1}{2} q_i x_i^2 + c_i x_i\\right) + \\nu \\left(\\sum_{i=1}^m A_i x_i - b\\right) - \\sum_{i=1}^m \\mu_{l,i} (x_i - \\ell_i) + \\sum_{i=1}^m \\mu_{u,i} (u_i - x_i).\n$$\nThe KKT stationarity condition requires $\\nabla_{x_i} \\mathcal{L}_{\\text{full}} = 0$ for each $i=1,\\ldots,m$:\n$$\nq_i x_i + c_i + \\nu A_i - \\mu_{l,i} + \\mu_{u,i} = 0.\n$$\nThe complementary slackness conditions are $\\mu_{l,i} (x_i - \\ell_i) = 0$ and $\\mu_{u,i} (u_i - x_i) = 0$.\nWe analyze the three possibilities for $x_i$ at the optimum:\n1.  If $\\ell_i  x_i  u_i$, then $\\mu_{l,i} = \\mu_{u,i} = 0$, which implies $q_i x_i + c_i + \\nu A_i = 0$, so $x_i = \\frac{-c_i - \\nu A_i}{q_i}$.\n2.  If $x_i = \\ell_i$, then $\\mu_{u,i} = 0$. Stationarity gives $\\mu_{l,i} = q_i \\ell_i + c_i + \\nu A_i$. The dual feasibility $\\mu_{l,i} \\ge 0$ requires $q_i \\ell_i + c_i + \\nu A_i \\ge 0$, or $\\frac{-c_i - \\nu A_i}{q_i} \\le \\ell_i$.\n3.  If $x_i = u_i$, then $\\mu_{l,i} = 0$. Stationarity gives $\\mu_{u,i} = -(q_i u_i + c_i + \\nu A_i)$. The dual feasibility $\\mu_{u,i} \\ge 0$ requires $q_i u_i + c_i + \\nu A_i \\le 0$, or $\\frac{-c_i - \\nu A_i}{q_i} \\ge u_i$.\n\nCombining these cases, for a given optimal multiplier $\\nu^\\star$, each component $x_i^\\star$ of the optimal solution is given by projecting the unconstrained minimizer onto the feasible interval $[\\ell_i, u_i]$:\n$$\nx_i^\\star(\\nu) = \\text{proj}_{[\\ell_i, u_i]} \\left( \\frac{-c_i - \\nu A_i}{q_i} \\right) = \\max\\left(\\ell_i, \\min\\left(u_i, \\frac{-c_i - \\nu A_i}{q_i}\\right)\\right).\n$$\nThe optimal multiplier $\\nu^\\star$ is the value that ensures the primal equality constraint is satisfied:\n$$\n\\phi(\\nu) := \\sum_{i=1}^m A_i x_i^\\star(\\nu) - b = 0.\n$$\nThe function $\\phi(\\nu)$ is continuous and monotonically non-increasing because each component $A_i x_i^\\star(\\nu)$ is a non-increasing function of $\\nu$. The problem guarantees that a feasible solution exists, which implies that the value $b$ lies within the achievable range of $\\sum_{i=1}^m A_i x_i$ for $x \\in [\\ell, u]$. This ensures that the equation $\\phi(\\nu) = 0$ has at least one solution $\\nu^\\star$. We can find this root using a numerical scalar root-finding method, such as Brent's method, on a sufficiently large interval. Once $\\nu^\\star$ is found, the optimal primal solution is $x^\\star = (x_1^\\star(\\nu^\\star), \\ldots, x_m^\\star(\\nu^\\star))$, and the optimal value is $p^\\star = f(x^\\star) = \\sum_{i=1}^m (\\frac{1}{2} q_i (x_i^\\star)^2 + c_i x_i^\\star)$.\n\n### Part 2: Dual Subgradient Ascent Method\n\nThe dual subgradient method operates on the dual problem, which is to maximize the dual function $g(\\lambda)$. The (unaugmented) Lagrangian is:\n$$\n\\mathcal{L}(x,\\lambda) = \\sum_{i=1}^m f_i(x_i) + \\lambda \\left( \\sum_{i=1}^m A_i x_i - b \\right).\n$$\nThe dual function is $g(\\lambda) = \\inf_{\\ell \\le x \\le u} \\mathcal{L}(x,\\lambda)$. The minimization in the definition of $g(\\lambda)$ is separable over the components of $x$:\n$$\ng(\\lambda) = \\sum_{i=1}^m \\left( \\min_{\\ell_i \\le x_i \\le u_i} \\left\\{ \\tfrac{1}{2} q_i x_i^2 + (c_i + \\lambda A_i) x_i \\right\\} \\right) - \\lambda b.\n$$\nFor a given $\\lambda$, the minimizing $x_i(\\lambda)$ is found, as in the KKT analysis, by projecting the unconstrained minimizer of the quadratic term onto $[\\ell_i, u_i]$:\n$$\nx_i(\\lambda) = \\text{proj}_{[\\ell_i, u_i]} \\left( \\frac{-(c_i + \\lambda A_i)}{q_i} \\right).\n$$\nA subgradient of the concave function $g(\\lambda)$ at a point $\\lambda_k$ is given by the primal residual $r_k = \\sum_{i=1}^m A_i x_i^{(k)} - b$, where $x^{(k)}$ is any minimizer $x(\\lambda_k)$. The subgradient ascent algorithm updates the dual variable $\\lambda$ in the direction of a subgradient. Starting with $\\lambda_0 = 0$, the iterative process for $k = 0, 1, \\ldots, T-1$ is:\n1.  Compute the primal minimizer for the current dual variable $\\lambda_k$:\n    $x^{(k)} = (x_1(\\lambda_k), \\ldots, x_m(\\lambda_k))$.\n2.  Compute the dual function value $g(\\lambda_k) = \\mathcal{L}(x^{(k)}, \\lambda_k)$ and update the best-so-far dual value: $d_{\\text{best}} = \\max(d_{\\text{best}}, g(\\lambda_k))$.\n3.  Compute the subgradient (primal residual): $r_k = \\sum_{i=1}^m A_i x_i^{(k)} - b$.\n4.  Update the dual variable using the specified constant step size $1/\\rho$:\n    $\\lambda_{k+1} = \\lambda_k + \\frac{1}{\\rho} r_k$.\n\nAfter $T$ iterations, we obtain the averaged primal candidate $\\bar{x} = \\frac{1}{T} \\sum_{k=0}^{T-1} x^{(k)}$.\n\n### Part 3: Quantified Metrics\n\nWith the values computed above, we calculate the two required metrics:\n1.  **Duality Gap**: This measures the difference between the true primal optimum and the best dual bound found by the algorithm: $p^\\star - d_{\\text{best}}$. By weak duality, this value is always non-negative.\n2.  **Primal Feasibility Residual**: This measures how well the recovered average solution respects the coupling constraint: $|\\sum_{i=1}^m A_i \\bar{x}_i - b|$.\n\nThese calculations are performed for each test case provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results in the required format.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, medium scale)\n        (4, [2.0, 1.0, 4.0, 3.0], [0.0, -1.0, 2.0, -0.5], [1.0, -2.0, 1.5, 1.0], \n         [-1.0, 0.0, -0.5, -2.0], [2.0, 2.0, 1.5, 1.0], 0.7, 200, 0.5),\n        # Case 2 (same data, milder step)\n        (4, [2.0, 1.0, 4.0, 3.0], [0.0, -1.0, 2.0, -0.5], [1.0, -2.0, 1.5, 1.0],\n         [-1.0, 0.0, -0.5, -2.0], [2.0, 2.0, 1.5, 1.0], 0.7, 200, 2.0),\n        # Case 3 (same data, small step)\n        (4, [2.0, 1.0, 4.0, 3.0], [0.0, -1.0, 2.0, -0.5], [1.0, -2.0, 1.5, 1.0],\n         [-1.0, 0.0, -0.5, -2.0], [2.0, 2.0, 1.5, 1.0], 0.7, 200, 20.0),\n        # Case 4 (boundary optimum on upper bounds)\n        (3, [1.0, 1.0, 1.0], [0.1, -0.2, 0.3], [1.0, 1.0, 1.0], \n         [0.0, 0.0, 0.0], [1.0, 1.0, 1.0], 3.0, 200, 2.0),\n        # Case 5 (ill-conditioned curvature)\n        (4, [0.1, 10.0, 0.5, 8.0], [0.0, 0.0, 0.0, 0.0], [1.0, 1.0, -1.0, 2.0],\n         [-1.0, -1.0, -1.0, -1.0], [1.0, 1.0, 1.0, 1.0], 0.3, 200, 2.0),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        m, q, c, A, ll, u, b, T, rho = [np.array(v) if isinstance(v, list) else v for v in case]\n        \n        # --- Part 1: Compute p_star (exact primal optimum) via KKT and root-finding ---\n        \n        def get_x_star_from_nu(nu, q, c, A, ll, u):\n            \"\"\"Calculates x*(nu) = proj_[l,u] ( (-c - nu*A) / q )\"\"\"\n            unconstrained_x = (-c - nu * A) / q\n            return np.maximum(ll, np.minimum(u, unconstrained_x))\n\n        def phi(nu, q, c, A, ll, u, b):\n            \"\"\"The function sum(A_i * x_i*(nu)) - b, whose root is nu_star\"\"\"\n            x_star = get_x_star_from_nu(nu, q, c, A, ll, u)\n            return np.dot(A, x_star) - b\n\n        # Find nu_star by solving phi(nu)=0 using a robust bracketing strategy\n        nu_search_min, nu_search_max = -1e5, 1e5\n        phi_at_min = phi(nu_search_min, q, c, A, ll, u, b)\n        phi_at_max = phi(nu_search_max, q, c, A, ll, u, b)\n\n        # Since feasibility is guaranteed, phi_at_max = 0 = phi_at_min\n        if np.isclose(phi_at_min, 0.0):\n            nu_star = nu_search_min\n        elif np.isclose(phi_at_max, 0.0):\n            nu_star = nu_search_max\n        else:\n            # A root must exist in the interval\n            nu_star = brentq(phi, nu_search_min, nu_search_max, args=(q, c, A, ll, u, b))\n\n        x_star = get_x_star_from_nu(nu_star, q, c, A, ll, u)\n        p_star = np.sum(0.5 * q * x_star**2 + c * x_star)\n\n        # --- Part 2: Dual Subgradient Ascent ---\n        \n        lambda_k = 0.0\n        x_sum = np.zeros(m)\n        d_best = -np.inf\n\n        for _ in range(T):\n            # Compute x_k which minimizes L(x, lambda_k) over the box\n            unconstrained_x = (-c - lambda_k * A) / q\n            x_k = np.maximum(ll, np.minimum(u, unconstrained_x))\n            x_sum += x_k\n\n            # Compute subgradient r_k\n            r_k = np.dot(A, x_k) - b\n            \n            # Compute dual function value g(lambda_k) = inf_x L(x, lambda_k)\n            g_lambda_k = np.sum(0.5 * q * x_k**2 + c * x_k) + lambda_k * r_k\n            if g_lambda_k > d_best:\n                d_best = g_lambda_k\n            \n            # Update lambda\n            lambda_k += (1.0 / rho) * r_k\n        \n        # --- Part 3: Final Computations ---\n        x_bar = x_sum / T\n        \n        duality_gap = p_star - d_best\n        feasibility_residual = np.abs(np.dot(A, x_bar) - b)\n        \n        all_results.extend([duality_gap, feasibility_residual])\n        \n    # Format and print the final output as a single-line string\n    print(f\"[{','.join(f'{r:.6f}' for r in all_results)}]\")\n\n# Execute the self-contained solution process\nsolve()\n```"
        },
        {
            "introduction": "Building upon the fundamentals of dual ascent, this practice introduces a more advanced and widely-used algorithm: the Alternating Direction Method of Multipliers (ADMM). We will apply ADMM to a practical problem from engineering—the optimal power flow (OPF) problem for an electrical grid. Here, the goal is to determine the most cost-effective power generation schedule across different areas of the grid, which are connected by tie-lines.\n\nADMM is a powerful tool for such distributed problems because it blends the decomposability of dual methods with the robust convergence of augmented Lagrangian methods. You will implement a distributed optimization scheme where each \"area\" of the power grid solves its own local problem, coordinating with its neighbors only through shared boundary information. This exercise will provide concrete insights into how decomposition methods enable complex, large-scale systems to be managed in a decentralized manner and will allow you to explore how the penalty parameter $\\rho$ affects the speed at which the different areas reach a consensus .",
            "id": "3116714",
            "problem": "Consider a direct current optimal power flow (DC-OPF) model on a small power network partitioned into two areas, solved with a distributed consensus formulation using the Alternating Direction Method of Multipliers (ADMM). The DC-OPF uses the linearized power flow relation $$P = B \\theta,$$ where $P$ is the vector of net active power injections at buses, $B$ is the bus susceptance Laplacian matrix assembled from line reactances, and $\\theta$ is the vector of bus voltage angles (in radians). The power flows on a line connecting buses $i$ and $j$ are given by $$f_{ij} = \\frac{\\theta_i - \\theta_j}{x_{ij}},$$ with $x_{ij}$ the line reactance. The net injection at bus $i$ satisfies $$P_i = \\sum_{j \\in \\mathcal{N}(i)} \\frac{\\theta_i - \\theta_j}{x_{ij}},$$ where $\\mathcal{N}(i)$ are adjacent buses connected by transmission lines.\n\nArea $1$ contains buses $\\{1,2,3\\}$ with internal lines $(1,2)$ having reactance $x_{12} = 0.1$ and $(2,3)$ having reactance $x_{23} = 0.2$. Area $2$ contains buses $\\{4,5\\}$ with internal line $(4,5)$ having reactance $x_{45} = 0.15$. The tie-line connecting the areas is $(3,4)$ with reactance $x_{34} = 0.25$. Define susceptances $$b_{12} = \\frac{1}{x_{12}}, \\quad b_{23} = \\frac{1}{x_{23}}, \\quad b_{45} = \\frac{1}{x_{45}}, \\quad b_{34} = \\frac{1}{x_{34}}.$$ The loads (in per-unit) are $$d_1 = 0.0, \\quad d_2 = 0.2, \\quad d_3 = 1.0, \\quad d_4 = 0.8, \\quad d_5 = 0.3.$$ Generators are located at buses $1$, $2$, and $5$ with quadratic costs $$C_1(g_1) = c_{2,1} g_1^2 + c_{1,1} g_1, \\quad C_2(g_2) = c_{2,2} g_2^2 + c_{1,2} g_2, \\quad C_5(g_5) = c_{2,5} g_5^2 + c_{1,5} g_5,$$ where $$c_{2,1} = 0.02, \\quad c_{1,1} = 1.0, \\quad c_{2,2} = 0.04, \\quad c_{1,2} = 1.2, \\quad c_{2,5} = 0.03, \\quad c_{1,5} = 1.1.$$ Angle variables are in radians, and power quantities are in per-unit.\n\nArea $1$ uses a slack bus angle constraint $\\theta_1 = 0$ to set the reference. Area $2$ does not fix a slack, and the tie-line consensus will anchor angles across areas. The local DC-OPF constraints are the nodal power balance equations. For Area $1$:\n- At bus $1$: $$b_{12}(\\theta_1 - \\theta_2) = g_1 - d_1.$$\n- At bus $2$: $$b_{12}(\\theta_2 - \\theta_1) + b_{23}(\\theta_2 - \\theta_3) = g_2 - d_2.$$\n- At bus $3$: $$b_{23}(\\theta_3 - \\theta_2) + b_{34}(\\theta_3 - \\phi_4) = 0 - d_3,$$ where $\\phi_4$ is Area $1$'s local copy of bus $4$'s angle, used to compute tie-line power flow.\n\nFor Area $2$:\n- At bus $4$: $$b_{45}(\\theta_4 - \\theta_5) + b_{34}(\\theta_4 - \\phi_3) = 0 - d_4,$$ where $\\phi_3$ is Area $2$'s local copy of bus $3$'s angle.\n- At bus $5$: $$b_{45}(\\theta_5 - \\theta_4) = g_5 - d_5.$$\n\nTo enforce consistency of boundary variables across areas, introduce consensus variables $z_3$ and $z_4$ for buses $3$ and $4$, respectively. The ADMM penalty parameter is denoted by $\\rho  0$. The scaled ADMM iteration at step $k$ consists of:\n- Area $1$ local minimization over its variables $\\theta_1, \\theta_2, \\theta_3, g_1, g_2, \\phi_4$ of the sum of local cost functions and quadratic penalties $$\\frac{\\rho}{2}\\left(\\theta_3 - z_3^{(k)} + u_{3,\\mathrm{A}}^{(k)}\\right)^2 + \\frac{\\rho}{2}\\left(\\phi_4 - z_4^{(k)} + u_{4,\\mathrm{A}}^{(k)}\\right)^2$$ subject to Area $1$'s nodal constraints and the slack constraint $\\theta_1 = 0$.\n- Area $2$ local minimization over its variables $\\theta_4, \\theta_5, g_5, \\phi_3$ of the sum of local cost functions and quadratic penalties $$\\frac{\\rho}{2}\\left(\\theta_4 - z_4^{(k)} + u_{4,\\mathrm{B}}^{(k)}\\right)^2 + \\frac{\\rho}{2}\\left(\\phi_3 - z_3^{(k)} + u_{3,\\mathrm{B}}^{(k)}\\right)^2$$ subject to Area $2$'s nodal constraints.\n- Consensus updates $$z_3^{(k+1)} = \\frac{\\theta_3^{(k+1)} + u_{3,\\mathrm{A}}^{(k)} + \\phi_3^{(k+1)} + u_{3,\\mathrm{B}}^{(k)}}{2}, \\quad z_4^{(k+1)} = \\frac{\\phi_4^{(k+1)} + u_{4,\\mathrm{A}}^{(k)} + \\theta_4^{(k+1)} + u_{4,\\mathrm{B}}^{(k)}}{2}.$$\n- Dual variable updates $$u_{3,\\mathrm{A}}^{(k+1)} = u_{3,\\mathrm{A}}^{(k)} + \\theta_3^{(k+1)} - z_3^{(k+1)}, \\quad u_{3,\\mathrm{B}}^{(k+1)} = u_{3,\\mathrm{B}}^{(k)} + \\phi_3^{(k+1)} - z_3^{(k+1)},$$ $$u_{4,\\mathrm{A}}^{(k+1)} = u_{4,\\mathrm{A}}^{(k)} + \\phi_4^{(k+1)} - z_4^{(k+1)}, \\quad u_{4,\\mathrm{B}}^{(k+1)} = u_{4,\\mathrm{B}}^{(k)} + \\theta_4^{(k+1)} - z_4^{(k+1)}.$$\n\nDefine the boundary primal residual norm at iteration $k$ as $$r^{(k)} = \\sqrt{\\left(\\theta_3^{(k)} - \\phi_3^{(k)}\\right)^2 + \\left(\\phi_4^{(k)} - \\theta_4^{(k)}\\right)^2}.$$ We will measure convergence speed by the number of iterations required for $$r^{(k)} \\le \\varepsilon,$$ with tolerance $\\varepsilon = 10^{-6}$, up to a maximum of $500$ iterations. Angles must be in radians, and all power quantities must be in per-unit.\n\nTask:\n1. Implement the above distributed DC-OPF with ADMM, where each local area solve is a quadratic program with linear equality constraints, solved via the Karush–Kuhn–Tucker (KKT) conditions.\n2. Test how the choice of penalty $\\rho$ affects the convergence speed and boundary mismatch by running ADMM for the penalty values $\\rho \\in \\{0.01, 0.1, 1.0, 10.0, 100.0\\}$.\n3. For each $\\rho$, report two quantities: the integer number of iterations to reach the tolerance (or the maximum number of iterations if tolerance is not reached), and the final boundary residual norm $r$ as a float.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-item list $[n, r]$ for a specific $\\rho$, i.e., $$[[n_1, r_1],[n_2, r_2],[n_3, r_3],[n_4, r_4],[n_5, r_5]].$$\n\nAngle unit: radians. Power unit: per-unit. Express all numeric outputs as plain floats and integers without units and without percentage signs.\n\nTest suite:\n- Penalty values: $\\rho = 0.01$, $\\rho = 0.1$, $\\rho = 1.0$, $\\rho = 10.0$, $\\rho = 100.0$.\n- Maximum iterations: $500$.\n- Tolerance: $\\varepsilon = 10^{-6}$.",
            "solution": "The problem requires the implementation of a distributed optimization algorithm, specifically the Alternating Direction Method of Multipliers (ADMM), to solve a Direct Current Optimal Power Flow (DC-OPF) problem for a small power network. The network is partitioned into two areas, and the solution must be found by coordinating local subproblems.\n\nFirst, we validate the problem statement.\n\n### Step 1: Extract Givens\n-   **Model**: Direct Current Optimal Power Flow (DC-OPF) using the linearized power flow relation $P = B \\theta$.\n-   **Network Partition**: Area $1$ with buses $\\{1,2,3\\}$ and Area $2$ with buses $\\{4,5\\}$.\n-   **Line Reactances (p.u.)**: $x_{12} = 0.1$, $x_{23} = 0.2$, $x_{45} = 0.15$. The tie-line reactance is $x_{34} = 0.25$. Line susceptances are defined as $b_{ij} = 1/x_{ij}$.\n-   **Loads (p.u.)**: $d_1 = 0.0$, $d_2 = 0.2$, $d_3 = 1.0$, $d_4 = 0.8$, $d_5 = 0.3$.\n-   **Generation Costs**: Quadratic costs $C_i(g_i) = c_{2,i} g_i^2 + c_{1,i} g_i$ for generators at buses $1$, $2$, and $5$. The coefficients are given as $c_{2,1} = 0.02, c_{1,1} = 1.0$; $c_{2,2} = 0.04, c_{1,2} = 1.2$; $c_{2,5} = 0.03, c_{1,5} = 1.1$.\n-   **Reference Angle**: Slack bus constraint $\\theta_1 = 0$.\n-   **Nodal Power Balance Equations**: The problem explicitly provides the DC power flow equations for each bus, which serve as linear equality constraints for the optimization subproblems. These equations involve local variables $(\\theta_i, g_i)$ and copies of boundary variables $(\\phi_i)$.\n-   **ADMM Structure**: A consensus ADMM formulation is specified.\n    -   **Consensus Variables**: $z_3$ and $z_4$ for the boundary bus angles.\n    -   **ADMM Penalty Parameter**: $\\rho  0$.\n    -   **Local Subproblems**: Each area solves a local optimization problem that minimizes its own generation costs plus augmented Lagrangian penalty terms. For Area $1$, the objective is $C_1(g_1) + C_2(g_2) + \\frac{\\rho}{2}(\\theta_3 - z_3^{(k)} + u_{3,\\mathrm{A}}^{(k)})^2 + \\frac{\\rho}{2}(\\phi_4 - z_4^{(k)} + u_{4,\\mathrm{A}}^{(k)})^2$. A similar objective is defined for Area $2$.\n    -   **Update Rules**: Explicit formulas are provided for updating the consensus variables $z_3, z_4$ and the scaled dual variables $u$. For instance, $z_3^{(k+1)} = \\frac{1}{2}(\\theta_3^{(k+1)} + u_{3,\\mathrm{A}}^{(k)} + \\phi_3^{(k+1)} + u_{3,\\mathrm{B}}^{(k)})$.\n-   **Task Parameters**:\n    -   **Subproblem Solver**: Karush–Kuhn–Tucker (KKT) conditions.\n    -   **Penalty Values**: $\\rho \\in \\{0.01, 0.1, 1.0, 10.0, 100.0\\}$.\n    -   **Convergence Tolerance**: $\\varepsilon = 10^{-6}$ on the boundary primal residual norm $r^{(k)} = \\sqrt{(\\theta_3^{(k)} - \\phi_3^{(k)})^2 + (\\phi_4^{(k)} - \\theta_4^{(k)})^2}$.\n    -   **Maximum Iterations**: $500$.\n-   **Output Format**: A list of pairs $[n, r]$, where $n$ is the number of iterations and $r$ is the final residual norm, for each value of $\\rho$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound.\n-   **Scientific Grounding**: The DC-OPF model is a standard and widely accepted simplification in power systems engineering for analyzing active power flows and economic dispatch. ADMM is a powerful and theoretically well-founded method for distributed convex optimization, frequently applied to power system problems.\n-   **Well-Posedness**: The overall problem is a convex quadratic program (QP), as the objective function is a sum of convex quadratic costs and the constraints are linear. This ensures a unique global optimum exists. The local subproblems are also convex QPs. For any $\\rho  0$, ADMM is guaranteed to converge to the optimal solution for this class of problems.\n-   **Completeness and Consistency**: The problem statement is self-contained, providing all necessary numerical data, model equations, and algorithmic steps. There are no contradictions in the provided information. The defined variables and equations are consistent with a standard ADMM-based decomposition of the DC-OPF problem.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A step-by-step solution will be developed and implemented.\n\n### Solution Derivation\n\nThe core of the task is to implement the specified ADMM algorithm. The ADMM algorithm iteratively solves local subproblems for each area, followed by updates of the consensus and dual variables that coordinate the solutions.\n\n**Subproblem Formulation**\n\nThe subproblems for Area $1$ and Area $2$ are quadratic programming problems with linear equality constraints. As specified, we solve them by formulating their Karush-Kuhn-Tucker (KKT) conditions, which form a system of linear equations.\n\n**Area 1 Subproblem**\n\nThe optimization variables for Area $1$ are $x_{\\mathrm{A}} = [\\theta_2, \\theta_3, g_1, g_2, \\phi_4]^T$. The slack constraint $\\theta_1=0$ is a fixed value.\nThe objective function to minimize at iteration $k+1$ is:\n$$ L_{\\mathrm{A}}(x_{\\mathrm{A}}) = C_1(g_1) + C_2(g_2) + \\frac{\\rho}{2}(\\theta_3 - z_3^{(k)} + u_{3,\\mathrm{A}}^{(k)})^2 + \\frac{\\rho}{2}(\\phi_4 - z_4^{(k)} + u_{4,\\mathrm{A}}^{(k)})^2 $$\nsubject to the linear constraints:\n\\begin{align*}\nb_{12}\\theta_2 + g_1 = d_1 \\\\\n(b_{12}+b_{23})\\theta_2 - b_{23}\\theta_3 - g_2 = -d_2 \\\\\n-b_{23}\\theta_2 + (b_{23}+b_{34})\\theta_3 - b_{34}\\phi_4 = -d_3\n\\end{align*}\nIntroducing Lagrange multipliers $\\lambda_{\\mathrm{A}} = [\\lambda_{1,1}, \\lambda_{1,2}, \\lambda_{1,3}]^T$ for these constraints, the KKT conditions (stationarity and feasibility) yield a linear system of the form $K_1 y_1 = v_1$, where $y_1 = [x_{\\mathrm{A}}; \\lambda_{\\mathrm{A}}]$. The KKT matrix $K_1$ and vector $v_1$ are:\n$$\nK_1 = \\begin{pmatrix} Q_1  A_1^T \\\\ A_1  0 \\end{pmatrix}, \\quad\nv_1 = \\begin{pmatrix} -c_1 \\\\ b_1 \\end{pmatrix}\n$$\nwhere $Q_1$ is the Hessian of the objective, $A_1$ is the constraint matrix, $c_1$ contains linear terms from the objective, and $b_1$ is the RHS of the constraints. Specifically:\n- $Q_1 = \\mathrm{diag}(0, \\rho, 2c_{2,1}, 2c_{2,2}, \\rho)$\n- $c_1 = [0, -\\rho(z_3^{(k)} - u_{3,\\mathrm{A}}^{(k)}), c_{1,1}, c_{1,2}, -\\rho(z_4^{(k)} - u_{4,\\mathrm{A}}^{(k)})]^T$\n- $b_1 = [d_1, -d_2, -d_3]^T$\n- $A_1 = \\begin{pmatrix} b_{12}  0  1  0  0 \\\\ b_{12}+b_{23}  -b_{23}  0  -1  0 \\\\ -b_{23}  b_{23}+b_{34}  0  0  -b_{34} \\end{pmatrix}$\n\n**Area 2 Subproblem**\n\nThe optimization variables for Area $2$ are $x_{\\mathrm{B}} = [\\theta_4, \\theta_5, g_5, \\phi_3]^T$.\nThe objective function is:\n$$ L_{\\mathrm{B}}(x_{\\mathrm{B}}) = C_5(g_5) + \\frac{\\rho}{2}(\\theta_4 - z_4^{(k)} + u_{4,\\mathrm{B}}^{(k)})^2 + \\frac{\\rho}{2}(\\phi_3 - z_3^{(k)} + u_{3,\\mathrm{B}}^{(k)})^2 $$\nsubject to the linear constraints:\n\\begin{align*}\n(b_{45}+b_{34})\\theta_4 - b_{45}\\theta_5 - b_{34}\\phi_3 = -d_4 \\\\\n-b_{45}\\theta_4 + b_{45}\\theta_5 - g_5 = -d_5\n\\end{align*}\nSimilarly, this forms a KKT system $K_2 y_2 = v_2$, where $y_2 = [x_{\\mathrm{B}}; \\lambda_{\\mathrm{B}}]$ with $\\lambda_{\\mathrm{B}} = [\\lambda_{2,1}, \\lambda_{2,2}]^T$. The components are:\n- $Q_2 = \\mathrm{diag}(\\rho, 0, 2c_{2,5}, \\rho)$\n- $c_2 = [-\\rho(z_4^{(k)} - u_{4,\\mathrm{B}}^{(k)}), 0, c_{1,5}, -\\rho(z_3^{(k)} - u_{3,\\mathrm{B}}^{(k)})]^T$\n- $b_2 = [-d_4, -d_5]^T$\n- $A_2 = \\begin{pmatrix} b_{45}+b_{34}  -b_{45}  0  -b_{34} \\\\ -b_{45}  b_{45}  -1  0 \\end{pmatrix}$\n\nThe KKT matrices $K_1$ and $K_2$ depend on $\\rho$ but are constant throughout the ADMM iterations for a given $\\rho$.\n\n**ADMM Algorithm**\n\nThe algorithm proceeds as follows for each given value of $\\rho$:\n1.  **Initialization**: Set all variables to zero: local variables $(\\theta, g, \\phi)$, consensus variables $(z_3, z_4)$, and scaled dual variables $(u)$.\n2.  **Iteration**: For $k = 0, 1, 2, \\dots$ up to the maximum number of iterations:\n    a. **Local Solves**:\n       i.  Construct the vector $v_1$ using $z^{(k)}$ and $u^{(k)}$. Solve the linear system $K_1 y_1^{(k+1)} = v_1$ to obtain the updated Area $1$ variables, including $\\theta_3^{(k+1)}$ and $\\phi_4^{(k+1)}$.\n       ii. Construct the vector $v_2$ using $z^{(k)}$ and $u^{(k)}$. Solve the linear system $K_2 y_2^{(k+1)} = v_2$ to obtain the updated Area $2$ variables, including $\\theta_4^{(k+1)}$ and $\\phi_3^{(k+1)}$.\n    b. **Convergence Check**: Calculate the boundary primal residual norm $r^{(k+1)} = \\sqrt{(\\theta_3^{(k+1)} - \\phi_3^{(k+1)})^2 + (\\phi_4^{(k+1)} - \\theta_4^{(k+1)})^2}$. If $r^{(k+1)} \\le \\varepsilon$, terminate and report the current iteration count and residual.\n    c. **Consensus Update**: Update the consensus variables using the new local variables and the old dual variables:\n       $$z_3^{(k+1)} = \\frac{1}{2}(\\theta_3^{(k+1)} + u_{3,\\mathrm{A}}^{(k)} + \\phi_3^{(k+1)} + u_{3,\\mathrm{B}}^{(k)})$$\n       $$z_4^{(k+1)} = \\frac{1}{2}(\\phi_4^{(k+1)} + u_{4,\\mathrm{A}}^{(k)} + \\theta_4^{(k+1)} + u_{4,\\mathrm{B}}^{(k)})$$\n    d. **Dual Update**: Update the scaled dual variables:\n       $$u_{3,\\mathrm{A}}^{(k+1)} = u_{3,\\mathrm{A}}^{(k)} + \\theta_3^{(k+1)} - z_3^{(k+1)}$$\n       $$u_{3,\\mathrm{B}}^{(k+1)} = u_{3,\\mathrm{B}}^{(k)} + \\phi_3^{(k+1)} - z_3^{(k+1)}$$\n       $$u_{4,\\mathrm{A}}^{(k+1)} = u_{4,\\mathrm{A}}^{(k)} + \\phi_4^{(k+1)} - z_4^{(k+1)}$$\n       $$u_{4,\\mathrm{B}}^{(k+1)} = u_{4,\\mathrm{B}}^{(k)} + \\theta_4^{(k+1)} - z_4^{(k+1)}$$\n3.  **Termination**: If the loop finishes without meeting the tolerance, report the maximum iteration count and the final calculated residual.\n\nThis procedure is implemented for each value of $\\rho$ to study its effect on convergence.",
            "answer": "```python\nimport numpy as np\n\ndef run_admm_for_rho(rho, max_iter, tol):\n    \"\"\"\n    Solves the distributed DC-OPF problem for a given rho.\n    \"\"\"\n    # --- 1. Define problem data ---\n    # Reactances (p.u.)\n    x12, x23, x45, x34 = 0.1, 0.2, 0.15, 0.25\n    # Susceptances (p.u.)\n    b12, b23, b45, b34 = 1.0/x12, 1.0/x23, 1.0/x45, 1.0/x34\n    \n    # Loads (p.u.)\n    d1, d2, d3, d4, d5 = 0.0, 0.2, 1.0, 0.8, 0.3\n    \n    # Cost coefficients\n    c21, c11 = 0.02, 1.0\n    c22, c12 = 0.04, 1.2\n    c25, c15 = 0.03, 1.1\n\n    # --- 2. Build KKT matrices ---\n    # Area 1 KKT System: K1 * y1 = v1\n    # Variables y1 = [theta2, theta3, g1, g2, phi4, lam11, lam12, lam13]\n    K1 = np.zeros((8, 8))\n    # Q block (Hessian of local objective)\n    K1[1, 1] = rho\n    K1[2, 2] = 2 * c21\n    K1[3, 3] = 2 * c22\n    K1[4, 4] = rho\n    # A^T block (Jacobian of constraints)\n    A1_T = np.array([\n        [b12, b12 + b23, -b23],\n        [0.0, -b23, b23 + b34],\n        [1.0, 0.0, 0.0],\n        [0.0, -1.0, 0.0],\n        [0.0, 0.0, -b34]\n    ])\n    K1[0:5, 5:8] = A1_T\n    # A block\n    K1[5:8, 0:5] = A1_T.T\n\n    # Area 2 KKT System: K2 * y2 = v2\n    # Variables y2 = [theta4, theta5, g5, phi3, lam21, lam22]\n    K2 = np.zeros((6, 6))\n    # Q block\n    K2[0, 0] = rho\n    K2[2, 2] = 2 * c25\n    K2[3, 3] = rho\n    # A^T block\n    A2_T = np.array([\n        [b45 + b34, -b45],\n        [-b45, b45],\n        [0.0, -1.0],\n        [-b34, 0.0]\n    ])\n    K2[0:4, 4:6] = A2_T\n    # A block\n    K2[4:6, 0:4] = A2_T.T\n    \n    # --- 3. Initialize ADMM variables ---\n    # Consensus variables\n    z3, z4 = 0.0, 0.0\n    # Scaled dual variables\n    u3A, u4A = 0.0, 0.0  # Area 1 duals\n    u3B, u4B = 0.0, 0.0  # Area 2 duals\n    \n    final_r = np.inf\n\n    # --- 4. ADMM Iterations ---\n    for k in range(1, max_iter + 1):\n        # --- Area 1 subproblem solve ---\n        v1 = np.array([\n            -c11,\n            -c12,\n            rho * (z3 - u3A),\n            rho * (z4 - u4A),\n            d1,\n            -d2,\n            -d3\n        ])\n        # Reorder to match KKT matrix structure\n        v1_ordered = np.array([v1[2] - A1_T[1,0]*0, v1[2]*0, v1[0], v1[1], v1[3], v1[4], v1[5], v1[6]])\n        v1_ordered[0] = 0.\n        v1_ordered[1] = rho * (z3 - u3A)\n\n\n        y1 = np.linalg.solve(K1, v1_ordered)\n        _theta2, theta3_kplus1, _g1, _g2, phi4_kplus1 = y1[0], y1[1], y1[2], y1[3], y1[4]\n\n        # --- Area 2 subproblem solve ---\n        v2 = np.array([\n            rho * (z4 - u4B),\n            0.0,\n            -c15,\n            rho * (z3 - u3B),\n            -d4,\n            -d5\n        ])\n        y2 = np.linalg.solve(K2, v2)\n        theta4_kplus1, _theta5, _g5, phi3_kplus1 = y2[0], y2[1], y2[2], y2[3]\n\n        # --- Check convergence ---\n        final_r = np.sqrt((theta3_kplus1 - phi3_kplus1)**2 + (phi4_kplus1 - theta4_kplus1)**2)\n        if final_r = tol:\n            return k, final_r\n\n        # --- Consensus (z) update ---\n        z3_kplus1 = (theta3_kplus1 + u3A + phi3_kplus1 + u3B) / 2.0\n        z4_kplus1 = (phi4_kplus1 + u4A + theta4_kplus1 + u4B) / 2.0\n\n        # --- Dual (u) update ---\n        u3A_kplus1 = u3A + theta3_kplus1 - z3_kplus1\n        u4A_kplus1 = u4A + phi4_kplus1 - z4_kplus1\n        u3B_kplus1 = u3B + phi3_kplus1 - z3_kplus1\n        u4B_kplus1 = u4B + theta4_kplus1 - z4_kplus1\n        \n        # --- Update state for next iteration ---\n        z3, z4 = z3_kplus1, z4_kplus1\n        u3A, u4A = u3A_kplus1, u4A_kplus1\n        u3B, u4B = u3B_kplus1, u4B_kplus1\n\n    # If max iterations reached\n    return max_iter, final_r\n\ndef solve():\n    \"\"\"\n    Main function to run the ADMM simulation for different rho values.\n    \"\"\"\n    # Define test parameters from the problem statement\n    test_cases = [0.01, 0.1, 1.0, 10.0, 100.0]\n    max_iterations = 500\n    tolerance = 1e-6\n\n    # KKT matrix and RHS vector construction correction\n    # Correct mapping: ∂L/∂x = Qx + c + Aᵀλ = 0 => Qx + Aᵀλ = -c\n    # KKT System: [Q, Aᵀ; A, 0] [x; λ] = [-c; b]\n    \n    # Area 1 RHS vector construction\n    c11, c12 = 1.0, 1.2\n    d1, d2, d3 = 0.0, 0.2, 1.0\n    def get_v1(rho, z3, z4, u3A, u4A):\n        c1_vec = np.array([0, -rho*(z3-u3A), c11, c12, -rho*(z4-u4A)])\n        b1_vec = np.array([d1, -d2, -d3])\n        return np.concatenate((-c1_vec, b1_vec))\n    \n    # Area 2 RHS vector construction\n    c15 = 1.1\n    d4, d5 = 0.8, 0.3\n    def get_v2(rho, z3, z4, u3B, u4B):\n        c2_vec = np.array([-rho*(z4-u4B), 0, c15, -rho*(z3-u3B)])\n        b2_vec = np.array([-d4, -d5])\n        return np.concatenate((-c2_vec, b2_vec))\n\n    # This corrected logic will replace the one in run_admm_for_rho\n    def run_admm_corrected(rho, max_iter, tol):\n        # Data and KKT matrices setup (same as before)\n        x12, x23, x45, x34 = 0.1, 0.2, 0.15, 0.25\n        b12, b23, b45, b34 = 1.0/x12, 1.0/x23, 1.0/x45, 1.0/x34\n        c21, c22, c25 = 0.02, 0.04, 0.03\n        K1 = np.zeros((8, 8)); K2 = np.zeros((6, 6))\n        K1[1, 1] = rho; K1[2, 2] = 2*c21; K1[3, 3] = 2*c22; K1[4, 4] = rho\n        A1_T = np.array([[b12,b12+b23,-b23],[0,-b23,b23+b34],[1,0,0],[0,-1,0],[0,0,-b34]])\n        K1[0:5, 5:8] = A1_T; K1[5:8, 0:5] = A1_T.T\n        K2[0, 0] = rho; K2[2, 2] = 2*c25; K2[3, 3] = rho\n        A2_T = np.array([[b45+b34,-b45],[-b45,b45],[0,-1],[-b34,0]])\n        K2[0:4, 4:6] = A2_T; K2[4:6, 0:4] = A2_T.T\n\n        # ADMM Variables\n        z3, z4 = 0.0, 0.0\n        u3A, u4A, u3B, u4B = 0.0, 0.0, 0.0, 0.0\n        final_r = np.inf\n\n        for k in range(1, max_iter + 1):\n            # Area 1 Solve\n            v1 = get_v1(rho, z3, z4, u3A, u4A)\n            y1 = np.linalg.solve(K1, v1)\n            theta3_kplus1, phi4_kplus1 = y1[1], y1[4]\n            # Area 2 Solve\n            v2 = get_v2(rho, z3, z4, u3B, u4B)\n            y2 = np.linalg.solve(K2, v2)\n            theta4_kplus1, phi3_kplus1 = y2[0], y2[3]\n\n            final_r = np.sqrt((theta3_kplus1 - phi3_kplus1)**2 + (phi4_kplus1 - theta4_kplus1)**2)\n            if final_r = tol:\n                return k, final_r\n\n            # Consensus Update\n            z3_kplus1 = (theta3_kplus1 + u3A + phi3_kplus1 + u3B) / 2.0\n            z4_kplus1 = (phi4_kplus1 + u4A + theta4_kplus1 + u4B) / 2.0\n            # Dual Update\n            u3A_kplus1 = u3A + theta3_kplus1 - z3_kplus1\n            u4A_kplus1 = u4A + phi4_kplus1 - z4_kplus1\n            u3B_kplus1 = u3B + phi3_kplus1 - z3_kplus1\n            u4B_kplus1 = u4B + theta4_kplus1 - z4_kplus1\n            \n            # State Update\n            z3, z4 = z3_kplus1, z4_kplus1\n            u3A, u4A, u3B, u4B = u3A_kplus1, u4A_kplus1, u3B_kplus1, u4B_kplus1\n\n        return max_iter, final_r\n\n    results = []\n    for rho_val in test_cases:\n        # Use the corrected implementation\n        n_iters, final_r = run_admm_corrected(rho_val, max_iterations, tolerance)\n        results.append([n_iters, final_r])\n\n    # Format output string exactly as required\n    result_str_list = []\n    for item in results:\n        # Ensure standard float representation\n        formatted_r = str(item[1])\n        result_str_list.append(f\"[{item[0]}, {formatted_r}]\")\n        \n    print(f\"[{','.join(result_str_list)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "This final practice shifts our focus to the second major family of decomposition techniques: Benders decomposition. Unlike Lagrangian relaxation, which \"prices out\" complicating constraints, Benders decomposition is designed for problems with complicating *variables*—typically integer or discrete decisions that, once fixed, render the remaining problem much simpler to solve. The method works by iteratively building a series of linear constraints, or \"cuts,\" that approximate the cost of the subproblem as a function of the master problem's decisions.\n\nThis exercise provides a deep, \"from-the-ground-up\" walkthrough of the Benders decomposition algorithm. You will learn to distinguish between and derive the two fundamental types of Benders cuts: optimality cuts, which are generated when the subproblem is feasible, and feasibility cuts, which are generated from the dual of an infeasible subproblem. By deriving these cuts from first principles and using them to solve the master problem, you will gain a robust understanding of the elegant interplay between the primal and dual problems that lies at the heart of Benders decomposition .",
            "id": "3116814",
            "problem": "Consider the following two-stage linear optimization problem amenable to Benders decomposition. The master decision variable is $x \\in [0,3]$. For any fixed $x$, the subproblem is the Linear Programming (LP) problem\n$$\n\\begin{aligned}\n\\text{minimize}_{y_1,y_2} \\quad  y_1 + y_2 \\\\\n\\text{subject to} \\quad  y_1 + y_2 = d(x), \\\\\n y_1 \\leq 2, \\\\\n y_2 \\leq 2, \\\\\n y_1 \\geq 0,\\; y_2 \\geq 0,\n\\end{aligned}\n$$\nwhere $d(x) = 3 - 2x$. The master objective is to minimize $5x + \\theta$, where $\\theta$ is a surrogate for the subproblem cost and will be constrained through Benders cuts. The subproblem is degenerately multiple optimal whenever it is feasible, because any feasible pair $(y_1,y_2)$ with $y_1 + y_2 = d(x)$ achieves the same objective value $y_1 + y_2$.\n\nTasks:\n1. Derive the dual of the subproblem from first principles, using the fundamental primal-dual pairing between linear programs and the principle of weak/strong duality.\n2. Identify an extreme point that yields a finite optimal dual solution (for the case where the subproblem is feasible) and derive the associated Benders optimality cut on $\\theta$ as a function of $x$.\n3. Identify extreme rays of the dual feasible set that cause dual unboundedness corresponding to subproblem infeasibility for some values of $x$, and derive the associated Benders feasibility cuts, explicitly in terms of $x$.\n4. Perform cut redundancy detection via LP post-processing: among the feasibility cuts you derived, identify any cut that is redundant when $x \\in [0,3]$ by formulating and solving the appropriate one-dimensional LP that maximizes the violation of the candidate cut over the master domain.\n5. Using only the non-redundant cuts and the optimality cut, write the resulting master problem and solve it to obtain the optimal objective value of $5x + \\theta$.\n\nYour final answer must be the single real number equal to the optimal value of $5x + \\theta$. No rounding is required and no units are involved. Express only the final number in your answer.",
            "solution": "The user wants me to solve a two-stage linear optimization problem using Benders decomposition. I will proceed by completing the five specified tasks.\n\n### Problem Validation\nThe problem statement describes a standard two-stage linear program with a single first-stage variable $x$ and a second-stage (subproblem) involving variables $y_1, y_2$. The structure is suitable for Benders decomposition. All variables, functions, and constraints are clearly defined. The problem is mathematically well-posed, scientifically grounded in optimization theory, and objectively stated. No flaws are detected. I will proceed with the solution.\n\n### Task 1: Derive the dual of the subproblem\n\nThe subproblem for a fixed $x$ is given by:\n$$\n\\begin{aligned}\n\\text{minimize}_{y_1,y_2} \\quad  y_1 + y_2 \\\\\n\\text{subject to} \\quad  y_1 + y_2 = d(x) \\\\\n y_1 \\leq 2 \\\\\n y_2 \\leq 2 \\\\\n y_1 \\geq 0,\\; y_2 \\geq 0\n\\end{aligned}\n$$\nwhere $d(x) = 3 - 2x$. To derive the dual, we first express this LP in a standard form. We introduce non-negative slack variables $s_1, s_2$ for the upper bounds on $y_1, y_2$. The subproblem becomes:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad  y_1 + y_2 + 0s_1 + 0s_2 \\\\\n\\text{subject to} \\quad  y_1 + y_2 \\quad\\quad\\quad = 3 - 2x \\\\\n y_1 \\quad\\quad + s_1 \\quad = 2 \\\\\n \\quad\\quad y_2 \\quad\\quad + s_2 = 2 \\\\\n y_1, y_2, s_1, s_2 \\geq 0\n\\end{aligned}\n$$\nThis is a standard form LP: $\\min \\{c^T \\mathbf{y} \\mid A\\mathbf{y} = b, \\mathbf{y} \\geq 0\\}$, where $\\mathbf{y} = (y_1, y_2, s_1, s_2)^T$. The corresponding dual problem is $\\max \\{\\pi^T b \\mid A^T\\pi \\leq c\\}$, where $\\pi = (\\pi_0, \\pi_1, \\pi_2)^T$ is the vector of dual variables associated with the three equality constraints, and these dual variables are unrestricted in sign.\n\nThe matrices and vectors are:\n$$ c = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad A = \\begin{pmatrix} 1  1  0  0 \\\\ 1  0  1  0 \\\\ 0  1  0  1 \\end{pmatrix}, \\quad b(x) = \\begin{pmatrix} 3-2x \\\\ 2 \\\\ 2 \\end{pmatrix} $$\nThe dual constraints $A^T\\pi \\leq c$ are:\n$$\n\\begin{pmatrix}\n1  1  0 \\\\\n1  0  1 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix} \\pi_0 \\\\ \\pi_1 \\\\ \\pi_2 \\end{pmatrix}\n\\leq\n\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives the following inequalities:\n1. $\\pi_0 + \\pi_1 \\leq 1$\n2. $\\pi_0 + \\pi_2 \\leq 1$\n3. $\\pi_1 \\leq 0$\n4. $\\pi_2 \\leq 0$\n\nThe dual objective function to maximize is $\\pi^T b(x)$:\n$$ (3-2x)\\pi_0 + 2\\pi_1 + 2\\pi_2 $$\nSo, the dual of the subproblem is:\n$$\n\\begin{aligned}\n\\text{maximize}_{\\pi_0, \\pi_1, \\pi_2} \\quad  (3-2x)\\pi_0 + 2\\pi_1 + 2\\pi_2 \\\\\n\\text{subject to} \\quad  \\pi_0 + \\pi_1 \\leq 1 \\\\\n \\pi_0 + \\pi_2 \\leq 1 \\\\\n \\pi_1 \\leq 0 \\\\\n \\pi_2 \\leq 0\n\\end{aligned}\n$$\n\n### Task 2: Derive the Benders optimality cut\n\nAn optimality cut is generated from an extreme point of the dual feasible set for a value of $x$ where the subproblem is feasible and has a finite optimal value. The primal subproblem is feasible if there exist $y_1, y_2$ such that $y_1+y_2 = d(x)$ and $0 \\leq y_1, y_2 \\leq 2$. The sum $y_1+y_2$ can take any value in $[0,4]$. Thus, the subproblem is feasible if and only if $0 \\leq d(x) \\leq 4$.\n$$ 0 \\leq 3-2x \\implies 2x \\leq 3 \\implies x \\leq 1.5 $$\n$$ 3-2x \\leq 4 \\implies -1 \\leq 2x \\implies x \\geq -0.5 $$\nGiven the master variable domain $x \\in [0,3]$, the subproblem is feasible for $x \\in [0, 1.5]$.\n\nFor $x$ in this range, the dual problem must have a finite optimal solution. Let's find an extreme point of the dual feasible region. An extreme point is a vertex formed by the intersection of at least three binding constraint planes. Let's test the constraints:\n$\\pi_0 + \\pi_1 = 1$, $\\pi_1 = 0$, $\\pi_2 = 0$. This gives $\\pi_0=1$. The point is $(\\pi_0, \\pi_1, \\pi_2) = (1,0,0)$. Let's check if it's feasible:\n- $1 + 0 \\leq 1$ (satisfied)\n- $1 + 0 \\leq 1$ (satisfied)\n- $0 \\leq 0$ (satisfied)\n- $0 \\leq 0$ (satisfied)\nThe point $(1,0,0)$ is an extreme point of the dual feasible set.\nThe dual objective function at this point is $(3-2x)(1)+2(0)+2(0) = 3-2x$.\nWhen the subproblem is feasible, its optimal objective value is given by $Q(x) = d(x) = 3-2x$. By strong duality, this is also the optimal value of the dual. The optimality cut is $\\theta \\geq Q(x)$, where $Q(x)$ is approximated by the value of the dual objective at one of its extreme points. Using the extreme point $\\pi^*=(1,0,0)$, we obtain the Benders optimality cut:\n$$ \\theta \\geq (3-2x)\\pi_0^* + 2\\pi_1^* + 2\\pi_2^* $$\n$$ \\theta \\geq 3-2x $$\n\n### Task 3: Derive Benders feasibility cuts\n\nFeasibility cuts are generated for values of $x$ where the subproblem is infeasible. This occurs when the dual is unbounded. The directions of unboundedness are the extreme rays of the dual's recession cone.\nThe recession cone is defined by the homogeneous system of inequalities:\n$$\n\\begin{aligned}\nd_0 + d_1 \\leq 0 \\\\\nd_0 + d_2 \\leq 0 \\\\\nd_1 \\leq 0 \\\\\nd_2 \\leq 0\n\\end{aligned}\n$$\nA Benders feasibility cut is derived from an extreme ray $r=(\\pi_0, \\pi_1, \\pi_2)$ of this cone if it provides a direction of unboundedness for the dual objective, i.e., $r^T c_D  0$, where $c_D = (3-2x, 2, 2)^T$. The cut is then of the form $r^T b(x) \\leq 0$.\nThe subproblem is infeasible for $x \\in [0,3]$ when $x1.5$ (since $d(x)  0$) or $x-0.5$ (since $d(x)4$).\n\nCase 1: $x  1.5$, which implies $3-2x  0$.\nLet's find an extreme ray that leads to an unbounded objective. Consider the ray $r_1 = (-1, 0, 0)$. It is in the recession cone: $-1+0\\leq0$, $-1+0\\leq0$, $0\\leq0$, $0\\leq0$.\nThe change in the dual objective along this ray is $(3-2x)(-1) + 2(0) + 2(0) = 2x-3$. For $x1.5$, $2x-3  0$, so the dual is unbounded along this ray.\nThe corresponding feasibility cut is $r_1^T b(x) \\leq 0$:\n$$ (-1,0,0) \\cdot (3-2x, 2, 2)^T \\leq 0 \\implies -(3-2x) \\leq 0 \\implies 2x-3 \\leq 0 \\implies x \\leq 1.5 $$\nThis cut removes the region $x  1.5$ from the feasible set of the master problem.\n\nCase 2: $x  -0.5$, which implies $3-2x  4$.\nLet's find another extreme ray. Consider $r_2 = (1, -1, -1)$. It is in the recession cone: $1-1=0\\leq0$, $1-1=0\\leq0$, $-1\\leq0$, $-1\\leq0$.\nThe change in the dual objective along this ray is $(3-2x)(1) + 2(-1) + 2(-1) = 3-2x-4 = -2x-1$.\nFor $x  -0.5$, $-2x  1$, so $-2x-1  0$. The dual is unbounded along this ray.\nThe corresponding feasibility cut is $r_2^T b(x) \\leq 0$:\n$$ (1,-1,-1) \\cdot (3-2x, 2, 2)^T \\leq 0 \\implies (3-2x) - 2 - 2 \\leq 0 \\implies -2x-1 \\leq 0 \\implies x \\geq -0.5 $$\nThis cut removes the region $x  -0.5$.\n\nThe two feasibility cuts are $x \\leq 1.5$ and $x \\geq -0.5$.\n\n### Task 4: Perform cut redundancy detection\n\nThe master problem for $x$ is constrained by its original domain $[0,3]$ and the feasibility cuts. The resulting feasible region for $x$ is given by:\n$$ \\{ x \\mid 0 \\leq x \\leq 3, \\quad x \\leq 1.5, \\quad x \\geq -0.5 \\} $$\nCombining these inequalities, we get $0 \\leq x \\leq 1.5$.\nWe check for redundancy among the feasibility cuts $x \\leq 1.5$ and $x \\geq -0.5$ over the master domain $x \\in [0,3]$.\n\nA cut is redundant if it is implied by other cuts and domain constraints.\nThe constraint $x \\geq -0.5$ is a candidate for redundancy. We check if it is redundant given the master domain constraint $x \\in [0,3]$.\nPer the problem instruction, we maximize the violation of this candidate cut over the domain defined by other constraints. The violation of $x \\geq -0.5$ can be expressed as $-0.5 - x$. We seek to maximize this value.\n$$ \\max_{x} \\quad -0.5 - x \\quad \\text{s.t.} \\quad 0 \\leq x \\leq 1.5 $$\nThe maximum occurs at the minimum value of $x$, which is $x=0$. The maximum violation is $-0.5-0 = -0.5$. Since the maximum violation is not positive, the cut $x \\geq -0.5$ is never violated on the feasible region defined by the other constraints. It is therefore redundant.\n\nThe non-redundant feasibility cut is $x \\leq 1.5$.\n\n### Task 5: Solve the master problem\n\nThe final master problem incorporates the non-redundant feasibility cut and the optimality cut:\n$$\n\\begin{aligned}\n\\text{minimize}_{x, \\theta} \\quad  5x + \\theta \\\\\n\\text{subject to} \\quad  0 \\leq x \\leq 1.5 \\\\\n \\theta \\geq 3-2x\n\\end{aligned}\n$$\nThe objective is to minimize $5x+\\theta$. To achieve this, for any given $x$, we should choose the smallest possible value for $\\theta$, which is $\\theta = 3-2x$ as per the constraint. Substituting this into the objective function, we obtain a problem in terms of $x$ alone:\n$$ \\text{minimize}_{x} \\quad 5x + (3-2x) \\quad \\text{s.t.} \\quad 0 \\leq x \\leq 1.5 $$\nThe objective simplifies to:\n$$ \\text{minimize}_{x} \\quad 3x+3 \\quad \\text{s.t.} \\quad 0 \\leq x \\leq 1.5 $$\nThe function $f(x)=3x+3$ is a linear function with a positive slope ($3$). Therefore, its minimum value over the interval $[0, 1.5]$ is attained at the left endpoint, $x=0$.\nThe optimal value for $x$ is $x^*=0$.\nThe corresponding optimal value of the objective function is $3(0)+3=3$.\n\nThe optimal solution to the overall problem is $x^*=0$, with the subproblem cost being $Q(0)=3-2(0)=3$.\nThe optimal value of the master objective function is $5x^* + Q(x^*) = 5(0) + 3 = 3$.",
            "answer": "$$\n\\boxed{3}\n$$"
        }
    ]
}