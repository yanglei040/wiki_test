{
    "hands_on_practices": [
        {
            "introduction": "在深入研究完整的近端梯度法之前，我们首先必须掌握其核心构件——近端算子。这个基础练习将引导你为一个简单但重要的二次函数推导其近端算子，让你从根本上理解该算子是如何将一个点“拉向”目标函数的最优解的。通过这个计算，你将为理解更复杂的应用打下坚实的数学基础。",
            "id": "2195112",
            "problem": "在数值优化领域，近端算子（proximal operator）是一种基本工具，用于设计求解不可微或约束问题的算法。对于给定的标量函数 $g(x)$ 和一个正的缩放参数 $\\lambda  0$，作用于点 $v$ 的 $\\lambda g$ 的近端算子定义为使一个复合目标函数最小化的 $x$ 值。\n\n其形式化定义如下：\n$$\n\\text{prox}_{\\lambda g}(v) = \\arg\\min_{x \\in \\mathbb{R}} \\left( g(x) + \\frac{1}{2\\lambda} (x-v)^2 \\right)\n$$\n你的任务是求解一个一般二次函数的近端算子。考虑函数 $g(x) = \\frac{1}{2}ax^2 + bx$，其中 $a$ 和 $b$ 是实值常数且 $a  0$。\n\n推导 $\\text{prox}_{\\lambda g}(v)$ 关于参数 $a$、$b$、$v$ 和 $\\lambda$ 的闭式解析表达式。",
            "solution": "我们被要求计算 $g(x)=\\frac{1}{2}a x^{2}+b x$ 在 $a0$ 和 $\\lambda0$ 条件下的 $\\text{prox}_{\\lambda g}(v)$。根据定义，\n$$\n\\text{prox}_{\\lambda g}(v)=\\arg\\min_{x\\in\\mathbb{R}}\\left(\\frac{1}{2}a x^{2}+b x+\\frac{1}{2\\lambda}(x-v)^{2}\\right).\n$$\n定义目标函数\n$$\nJ(x)=\\frac{1}{2}a x^{2}+b x+\\frac{1}{2\\lambda}(x-v)^{2}.\n$$\n由于 $a0$ 且 $\\lambda0$，函数 $J$ 是严格凸的，因为它的二阶导数为\n$$\nJ''(x)=a+\\frac{1}{\\lambda}0,\n$$\n所以它有一个由一阶最优性条件 $J'(x)=0$ 刻画的唯一最小化子。计算其导数：\n$$\nJ'(x)=a x+b+\\frac{1}{\\lambda}(x-v)=(a+\\frac{1}{\\lambda})x+b-\\frac{v}{\\lambda}.\n$$\n令 $J'(x)=0$ 并求解 $x$：\n$$\n(a+\\frac{1}{\\lambda})x+b-\\frac{v}{\\lambda}=0\n\\;\\;\\Longrightarrow\\;\\;\nx=\\frac{\\frac{v}{\\lambda}-b}{a+\\frac{1}{\\lambda}}.\n$$\n分子和分母同乘以 $\\lambda$ 得到\n$$\nx=\\frac{v-b\\lambda}{1+a\\lambda}.\n$$\n因此，\n$$\n\\text{prox}_{\\lambda g}(v)=\\frac{v-b\\lambda}{1+a\\lambda}.\n$$",
            "answer": "$$\\boxed{\\frac{v-b\\lambda}{1+a\\lambda}}$$"
        },
        {
            "introduction": "理解了近端算子的概念后，我们现在来看看它在完整算法中是如何发挥作用的。这个练习将指导你完成单次近端梯度方法的迭代，清晰地展示算法中“梯度下降”与“近端映射”这两个步骤如何协同工作。通过这个具体的计算，抽象的更新规则将变得一目了然。",
            "id": "2195110",
            "problem": "考虑一个优化问题，寻找一个点 $x = (x_1, x_2) \\in \\mathbb{R}^2$，该点最小化函数 $F(x)$，且其分量满足非负约束，即 $x_1 \\ge 0$ 且 $x_2 \\ge 0$。需要最小化的函数是 $x$ 到目标点 $a$ 的欧几里得距离的平方，由 $F(x) = \\frac{1}{2}\\|x - a\\|_2^2$ 给出。\n\n通过将光滑部分定义为 $f(x) = \\frac{1}{2}\\|x - a\\|_2^2$，非光滑部分 $g(x)$ 定义为非负象限的指示函数，可以将此问题转化为近端算法的标准形式 $\\min_{x} f(x) + g(x)$。如果 $x_1 \\ge 0$ 且 $x_2 \\ge 0$，指示函数 $g(x)$ 为零，否则为无穷大。\n\n你的任务是应用近端梯度法来解决这个问题。近端梯度法的迭代更新规则如下：\n$$ x_{k+1} = \\text{prox}_{t g}(x_k - t \\nabla f(x_k)) $$\n其中 $t$ 是步长，$\\text{prox}_{t g}$ 是与函数 $g$ 相关联的近端算子。\n\n给定目标点 $a = (5, -4)$，初始点 $x_0 = (1, 1)$，以及步长 $t = 0.2$，计算下一次迭代的结果 $x_1$。将你的答案表示为一个行向量 $(x_{1,1}, x_{1,2})$，其中 $x_{1,1}$ 和 $x_{1,2}$ 是向量 $x_1$ 的分量。",
            "solution": "我们要在非负象限上最小化 $F(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$。在近端梯度分解中，设 $f(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$ 和 $g(x)=\\iota_{\\mathbb{R}_{+}^{2}}(x)$，其中 $g(x)$ 是可行集 $\\mathbb{R}_{+}^{2}=\\{x\\in\\mathbb{R}^{2}:x_{1}\\ge 0,\\ x_{2}\\ge 0\\}$ 的指示函数。\n\n$f$ 的梯度由下式给出\n$$\n\\nabla f(x)=x-a.\n$$\n$t g$ 在点 $z$ 处的近端算子是到 $\\mathbb{R}_{+}^{2}$ 上的欧几里得投影：\n$$\n\\text{prox}_{t g}(z)=\\operatorname*{arg\\,min}_{y\\in\\mathbb{R}^{2}}\\left\\{\\iota_{\\mathbb{R}_{+}^{2}}(y)+\\frac{1}{2t}\\|y-z\\|_{2}^{2}\\right\\}=P_{\\mathbb{R}_{+}^{2}}(z),\n$$\n也就是逐分量在零处截断：\n$$\nP_{\\mathbb{R}_{+}^{2}}(z)=\\big(\\max\\{z_{1},0\\},\\ \\max\\{z_{2},0\\}\\big).\n$$\n\n当 $a=(5,-4)$，$x_{0}=(1,1)$ 且 $t=0.2$ 时，计算在 $x_0$ 处的梯度：\n$$\n\\nabla f(x_{0})=x_{0}-a=(1,1)-(5,-4)=(-4,5).\n$$\n执行梯度步：\n$$\nx_{0}-t \\nabla f(x_{0})=(1,1)-0.2\\,(-4,5)=(1+0.8,\\ 1-1)=\\left(\\frac{9}{5},\\ 0\\right).\n$$\n应用近端映射，即到 $\\mathbb{R}_{+}^{2}$ 上的投影：\n$$\nx_{1}=\\text{prox}_{t g}\\big(x_{0}-t \\nabla f(x_{0})\\big)=P_{\\mathbb{R}_{+}^{2}}\\left(\\frac{9}{5},0\\right)=\\left(\\frac{9}{5},0\\right),\n$$\n因为两个分量均已为非负。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{9}{5}  0 \\end{pmatrix}}$$"
        },
        {
            "introduction": "这是我们将理论付诸实践的综合性练习。在机器学习和信号处理中，将优化算法转化为可执行代码是解决实际问题的关键一步。这项动手编程挑战要求你完整实现近端梯度法，以解决一个带 $\\ell_1$ 范数约束的最小二乘问题，这其中包括实现一个非平凡的到 $\\ell_1$ 球上的投影算子。",
            "id": "3167378",
            "problem": "给定一个复合凸优化问题，其形式为在 $x \\in \\mathbb{R}^n$ 上最小化 $F(x) = f(x) + g(x)$，其中 $f$ 是可微的且其梯度是 Lipschitz 连续的，$g$ 是一个真、下半连续的凸函数。该问题的基本基础是凸性的定义、欧几里得范数、二次函数的梯度以及到闭凸集上的投影概念。具体来说，考虑以下实例：\n$$\nf(x) = \\tfrac{1}{2}\\|A x - b\\|_2^2, \\quad g(x) = \\iota_{\\mathcal{C}}(x),\n$$\n其中 $\\iota_{\\mathcal{C}}$ 是集合 $\\mathcal{C}$ 的指示函数，定义为\n$$\n\\mathcal{C} = \\{x \\in \\mathbb{R}^n : \\|x\\|_1 \\le \\tau\\}.\n$$\n指示函数定义为：如果 $x \\in \\mathcal{C}$，则 $\\iota_{\\mathcal{C}}(x) = 0$；否则 $\\iota_{\\mathcal{C}}(x) = +\\infty$。向量 $y \\in \\mathbb{R}^n$ 到集合 $\\mathcal{C}$ 上的投影是在欧几里得范数意义下距离 $y$ 最近的 $\\mathcal{C}$ 中的点。\n\n您的任务是实现一个具有固定步长的近端梯度算法，其中每次迭代包括：首先，对 $f$ 进行一次梯度步，步长基于 $\\nabla f$ 的 Lipschitz 常数；其次，将该点投影到集合 $\\mathcal{C}$ 上。$f$ 的梯度定义为 $\\nabla f(x) = A^\\top (A x - b)$。$\\nabla f$ 的 Lipschitz 常数是 $A^\\top A$ 的算子范数，它等于 $\\|A\\|_2^2$，其中 $\\|A\\|_2$ 表示谱范数（最大奇异值）。使用等于此 Lipschitz 常数倒数的固定步长。\n\n根据其基本定义，实现到 $\\ell_1$ 球 $\\mathcal{C}$ 上的投影，作为以下欧几里得投影问题的唯一解：\n$$\n\\operatorname{Proj}_{\\mathcal{C}}(y) = \\arg\\min_{x \\in \\mathbb{R}^n} \\left\\{ \\tfrac{1}{2}\\|x - y\\|_2^2 \\, \\text{ subject to } \\, \\|x\\|_1 \\le \\tau \\right\\}.\n$$\n该实现对于所有 $\\tau \\ge 0$ 都必须是数值稳健的，包括边界情况 $\\tau = 0$ 以及输入 $y$ 已经满足 $\\|y\\|_1 \\le \\tau$ 的情况。\n\n使用以下矩阵 $A \\in \\mathbb{R}^{5 \\times 4}$ 和向量 $b \\in \\mathbb{R}^5$ 的固定数据：\n$$\nA = \\begin{bmatrix}\n3  -1  0  2 \\\\\n0  1  -2  1 \\\\\n1  0  1  -1 \\\\\n2  1  0  1 \\\\\n-1  2  1  0\n\\end{bmatrix}, \\quad\nb = \\begin{bmatrix}\n1 \\\\ -2 \\\\ 0.5 \\\\ 3 \\\\ -1\n\\end{bmatrix}.\n$$\n在 $x^{(0)} = 0 \\in \\mathbb{R}^4$ 处初始化近端梯度法。使用固定步长 $1/L$，其中 $L = \\|A\\|_2^2$ 是 $\\nabla f$ 的 Lipschitz 常数。当相对变化 $\\|x^{(k+1)} - x^{(k)}\\|_2 / \\max\\{1, \\|x^{(k)}\\|_2\\}$ 小于 $10^{-10}$ 或迭代次数达到 $5000$ 次时停止，以先到者为准。\n\n设计一个程序，为以下 $\\tau$ 值的测试套件运行近端梯度法：\n- 案例1（常规收缩）：$\\tau = 0.5$。\n- 案例2（非激活约束，大半径）：$\\tau = 100$。\n- 案例3（边界，零半径）：$\\tau = 0$。\n- 案例4（边界，约束与无约束解重合）：$\\tau = \\|x_{\\mathrm{LS}}\\|_1$，其中 $x_{\\mathrm{LS}}$ 是在 $\\mathbb{R}^n$ 上最小化 $f(x)$ 的无约束最小二乘解。\n\n对于每种情况，令 $x^\\star$ 表示您方法的输出。程序必须为每种情况生成一个布尔结果，定义如下：\n- 案例1：如果 $\\|x^\\star\\|_1 \\le \\tau + 10^{-8}$ 且 $\\left|\\|x^\\star\\|_1 - \\tau\\right| \\le 10^{-6}$，则输出 $\\text{True}$，否则输出 $\\text{False}$。\n- 案例2：如果 $\\|x^\\star - x_{\\mathrm{LS}}\\|_2 \\le 10^{-8}$，则输出 $\\text{True}$，否则输出 $\\text{False}$。\n- 案例3：如果 $\\|x^\\star\\|_2 \\le 10^{-12}$，则输出 $\\text{True}$，否则输出 $\\text{False}$。\n- 案例4：如果 $\\|x^\\star - x_{\\mathrm{LS}}\\|_2 \\le 10^{-8}$，则输出 $\\text{True}$，否则输出 $\\text{False}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[r_1,r_2,r_3,r_4]$），其中每个 $r_i$ 是案例 $i$ 的布尔结果。",
            "solution": "用户为凸优化任务提供了一个定义明确的问题陈述。该问题在科学上基于凸分析和优化算法的原理。所有必要的数据和条件都已提供，没有内部矛盾或歧义。因此，将提供一个解决方案。\n\n问题是在 $x \\in \\mathbb{R}^n$ 上找到复合凸函数 $F(x) = f(x) + g(x)$ 的最小值，其中函数定义为：\n$f(x) = \\frac{1}{2}\\|A x - b\\|_2^2$\n$g(x) = \\iota_{\\mathcal{C}}(x) = \\begin{cases} 0  \\text{if } \\|x\\|_1 \\le \\tau \\\\ +\\infty  \\text{otherwise} \\end{cases}$\n\n这是一个带有 $\\ell_1$ 范数约束的最小二乘问题。指定的算法是近端梯度法，这是一种非常适合此结构的迭代过程。\n\n**近端梯度法**\n近端梯度法解决形如 $\\min_x f(x) + g(x)$ 的优化问题，其中 $f$ 是光滑的（可微且具有 Lipschitz 梯度），而 $g$ 是凸函数但可能非光滑。每次迭代都将对 $f$ 的标准梯度下降步与涉及 $g$ 的“近端”步相结合。更新规则是：\n$$x^{(k+1)} = \\operatorname{prox}_{t g}(x^{(k)} - t \\nabla f(x^{(k)}))$$\n这里，$x^{(k)}$ 是第 $k$ 步的迭代点，$t  0$ 是步长，$\\operatorname{prox}_{t g}$ 是参数为 $t$ 的 $g$ 的近端算子。\n\n**问题特定组成部分**\n1. **$f(x)$的梯度**：函数 $f(x)$ 是一个标准的二次目标函数。其梯度可通过应用向量值函数的链式法则求得：\n    $$f(x) = \\tfrac{1}{2} (Ax-b)^\\top (Ax-b)$$\n    $$\\nabla f(x) = A^\\top (Ax - b)$$\n\n2. **Lipschitz常数和步长**：梯度 $\\nabla f(x)$ 是 Lipschitz 连续的。近端梯度法收敛的一个充分条件是步长 $t$ 满足 $0  t  2/L$，其中 $L$ 是 $\\nabla f$ 的一个 Lipschitz 常数。最小的这样的常数 $L$ 是 $f$ 的 Hessian 矩阵 $\\nabla^2 f(x) = A^\\top A$ 的算子范数。因此，$L = \\|A^\\top A\\|_2 = \\|A\\|_2^2$，其中 $\\|A\\|_2$ 是 $A$ 的谱范数（其最大奇异值）。问题指定了固定步长 $t = 1/L$，这保证了收敛。\n\n3. **$g(x)$的近端算子**：函数 $h(x)$ 的近端算子定义为：\n    $$\\operatorname{prox}_{t h}(y) = \\arg\\min_{x \\in \\mathbb{R}^n} \\left( h(x) + \\tfrac{1}{2t} \\|x - y\\|_2^2 \\right)$$\n    对于我们的函数 $g(x) = \\iota_{\\mathcal{C}}(x)$，近端算子变为：\n    $$\\operatorname{prox}_{t g}(y) = \\arg\\min_{x \\in \\mathbb{R}^n} \\left( \\iota_{\\mathcal{C}}(x) + \\tfrac{1}{2t} \\|x - y\\|_2^2 \\right)$$\n    由于在集合 $\\mathcal{C}$ 之外 $\\iota_{\\mathcal{C}}(x)$ 是无穷大，最小化子 $x$ 必须位于 $\\mathcal{C}$ 内。在 $\\mathcal{C}$ 内部，$\\iota_{\\mathcal{C}}(x)=0$。问题简化为：\n    $$\\operatorname{prox}_{t g}(y) = \\arg\\min_{x \\in \\mathcal{C}} \\tfrac{1}{2t} \\|x - y\\|_2^2 = \\arg\\min_{x \\in \\mathcal{C}} \\|x - y\\|_2^2$$\n    这正是点 $y$ 到凸集 $\\mathcal{C}$ 上的欧几里得投影的定义，记作 $\\operatorname{Proj}_{\\mathcal{C}}(y)$。\n\n**到$\\ell_1$球上的投影**\n算法实现的核心在于高效地计算到 $\\ell_1$ 球 $\\mathcal{C} = \\{x : \\|x\\|_1 \\le \\tau\\}$ 上的投影。给定一个向量 $y$，我们必须求解：\n$$\\operatorname{Proj}_{\\mathcal{C}}(y) = \\arg\\min_x \\left\\{ \\tfrac{1}{2}\\|x - y\\|_2^2 \\text{ subject to } \\|x\\|_1 \\le \\tau \\right\\}$$\n求解逻辑如下：\n- 如果 $y$ 已经在 $\\mathcal{C}$ 中（即 $\\|y\\|_1 \\le \\tau$），则投影就是 $y$ 本身。\n- 如果 $\\|y\\|_1  \\tau$，投影 $x$ 必须位于集合的边界上，即 $\\|x\\|_1 = \\tau$。这个约束问题的解可以通过 Karush-Kuhn-Tucker (KKT) 条件找到。解的形式是一种软阈值操作：\n$$x_i = \\operatorname{sgn}(y_i) \\max(0, |y_i| - \\lambda)$$\n其中 $\\lambda  0$ 是一个阈值，其选择要满足约束 $\\|x\\|_1 = \\tau$。这意味着我们必须找到方程的根 $\\lambda$：\n$$\\sum_{i=1}^n \\max(0, |y_i| - \\lambda) = \\tau$$\n左侧的函数是关于 $\\lambda$ 的一个连续、分段线性和非增函数。存在一种高效的非迭代算法来找到这个 $\\lambda$。它涉及对 $y$ 各分量的绝对值进行排序，并确定在阈值操作后将保持非零的分量的正确数量。\n\n**算法实现**\n整体算法流程如下：\n1. 初始化 $x^{(0)} = 0$。\n2. 计算 Lipschitz 常数 $L = \\|A\\|_2^2$ 并设置步长 $t = 1/L$。\n3. 对于 $k = 0, 1, 2, \\dots$ 直到达到最大迭代次数：\n    a. 执行梯度步：$z^{(k)} = x^{(k)} - t A^\\top(A x^{(k)} - b)$。\n    b. 执行近端步（投影）：$x^{(k+1)} = \\operatorname{Proj}_{\\mathcal{C}}(z^{(k)})$。\n    c. 使用相对变化准则检查收敛性：$\\|x^{(k+1)} - x^{(k)}\\|_2 / \\max\\{1, \\|x^{(k)}\\|_2\\}  \\epsilon$。\n\n**测试案例分析**\n这四个测试案例旨在验证算法在不同场景下的表现：\n- **案例1 ($\\tau = 0.5$)**：无约束最小二乘解 $x_{\\mathrm{LS}}$（仅 $f(x)$ 的最小化子）的 $\\ell_1$ 范数约为 $\\|x_{\\mathrm{LS}}\\|_1 \\approx 1.87$，大于 $\\tau$。因此，约束是激活的，解 $x^\\star$ 必须位于 $\\ell_1$ 球的边界上，即 $\\|x^\\star\\|_1 = \\tau$。\n- **案例2 ($\\tau = 100$)**：这里，$\\tau$ 远大于 $\\|x_{\\mathrm{LS}}\\|_1$。无约束解 $x_{\\mathrm{LS}}$ 已经是可行的。因此，它也是约束问题的解，算法应收敛到 $x_{\\mathrm{LS}}$。\n- **案例3 ($\\tau = 0$)**：可行集 $\\mathcal{C}$ 只包含原点 $\\{0\\}$。唯一可能的解是 $x^\\star = 0$。\n- **案例4 ($\\tau = \\|x_{\\mathrm{LS}}\\|_1$)**：$\\ell_1$ 球的半径被选择为恰好等于无约束解的 $\\ell_1$ 范数。这使得 $x_{\\mathrm{LS}}$ 位于可行集的边界上。与案例2一样，$x_{\\mathrm{LS}}$ 是最优解。\n\n程序将实现这些步骤，并根据每个案例的规定执行检查。",
            "answer": "```python\nimport numpy as np\n\ndef _projection_l1_ball(y, tau):\n    \"\"\"\n    Computes the projection of a vector y onto the L1 ball of radius tau.\n    Proj_C(y) = argmin_{||x||_1 = tau} ||x-y||_2^2\n    This implementation is based on the algorithm by Duchi et al. (2008).\n    \"\"\"\n    if tau  0:\n        raise ValueError(\"Radius tau must be non-negative.\")\n\n    # Trivial cases\n    if tau == 0:\n        return np.zeros_like(y)\n    \n    # If the point is already in the ball, the projection is the point itself.\n    if np.linalg.norm(y, 1) = tau:\n        return y\n\n    # The projection must be on the boundary of the L1 ball.\n    # We need to find a threshold lambda > 0 such that the soft-thresholded\n    # vector x = sgn(y) * max(0, |y| - lambda) has ||x||_1 = tau.\n    \n    # Get absolute values and sort them in descending order\n    u = np.abs(y)\n    n = len(u)\n    \n    # The algorithm requires sorting, which can be done efficiently.\n    u_sorted = np.sort(u)[::-1]\n    \n    # Compute cumulative sums of the sorted values\n    cumsum_u = np.cumsum(u_sorted)\n    \n    # Find the largest k such that u_k - (1/k) * (sum_{i=1 to k} u_i - tau) > 0\n    # This determines the number of non-zero elements in the projected vector.\n    # This can be done in a vectorized way without a Python loop.\n    rhos = np.arange(1, n + 1)\n    # The condition is u_sorted[k-1] > (cumsum_u[k-1] - tau) / k\n    # `np.where` returns indices where condition is true.\n    # The set of such rhos is guaranteed to be non-empty if ||y||_1 > tau and tau > 0.\n    valid_rhos_indices = np.where(u_sorted > (cumsum_u - tau) / rhos)[0]\n    rho = valid_rhos_indices[-1] + 1\n    \n    # The optimal threshold lambda is calculated based on this rho\n    theta = (cumsum_u[rho - 1] - tau) / rho\n    \n    # Apply soft-thresholding with the computed threshold\n    x_proj = np.sign(y) * np.maximum(0, u - theta)\n    \n    return x_proj\n\ndef _run_proximal_gradient(tau, x0, A, b, max_iter, tol):\n    \"\"\"\n    Solves min 0.5*||Ax-b||^2 s.t. ||x||_1 = tau using proximal gradient method.\n    \"\"\"\n    n_features = A.shape[1]\n    x = x0.copy()\n    \n    # The Lipschitz constant of grad(f) is ||A^T A||_2 = ||A||_2^2.\n    L = np.linalg.norm(A, 2)**2\n    step_size = 1.0 / L\n    \n    for _ in range(max_iter):\n        x_old = x.copy()\n        \n        # Gradient descent step for the smooth part f(x)\n        grad_f = A.T @ (A @ x - b)\n        y = x - step_size * grad_f\n        \n        # Proximal operator step for the non-smooth part g(x)\n        # This is the projection onto the L1 ball.\n        x = _projection_l1_ball(y, tau)\n        \n        # Check for convergence\n        norm_x_old = np.linalg.norm(x_old)\n        denominator = max(1.0, norm_x_old)\n        rel_change = np.linalg.norm(x - x_old) / denominator\n        \n        if rel_change  tol:\n            break\n            \n    return x\n    \ndef solve():\n    # Fixed data for the problem\n    A = np.array([\n        [3, -1, 0, 2],\n        [0, 1, -2, 1],\n        [1, 0, 1, -1],\n        [2, 1, 0, 1],\n        [-1, 2, 1, 0]\n    ], dtype=float)\n    \n    b = np.array([1, -2, 0.5, 3, -1], dtype=float)\n\n    # Initial point for the algorithm\n    x0 = np.zeros(A.shape[1], dtype=float)\n\n    # Algorithm parameters\n    max_iter = 5000\n    tol = 1e-10\n\n    # The unconstrained least squares solution is needed for Cases 2 and 4\n    # x_ls minimizes ||Ax-b||^2, solving A^T A x = A^T b\n    # A has full column rank, so A^T A is invertible.\n    x_ls = np.linalg.solve(A.T @ A, A.T @ b)\n\n    # Define the tau values for each test case\n    test_cases_tau = {\n        'case1': 0.5,\n        'case2': 100.0,\n        'case3': 0.0,\n        'case4': np.linalg.norm(x_ls, 1)\n    }\n\n    results = []\n\n    # Case 1: General shrinkage\n    tau1 = test_cases_tau['case1']\n    x_star1 = _run_proximal_gradient(tau1, x0, A, b, max_iter, tol)\n    norm_x1 = np.linalg.norm(x_star1, 1)\n    # Check if constraint is met (with tolerance) and active\n    res1 = (norm_x1 = tau1 + 1e-8) and (np.abs(norm_x1 - tau1) = 1e-6)\n    results.append(res1)\n\n    # Case 2: Inactive constraint\n    tau2 = test_cases_tau['case2']\n    x_star2 = _run_proximal_gradient(tau2, x0, A, b, max_iter, tol)\n    # Check if solution is close to the unconstrained least squares solution\n    res2 = np.linalg.norm(x_star2 - x_ls) = 1e-8\n    results.append(res2)\n\n    # Case 3: Zero radius\n    tau3 = test_cases_tau['case3']\n    x_star3 = _run_proximal_gradient(tau3, x0, A, b, max_iter, tol)\n    # Check if solution is the zero vector\n    res3 = np.linalg.norm(x_star3) = 1e-12\n    results.append(res3)\n\n    # Case 4: Constraint meets unconstrained solution\n    tau4 = test_cases_tau['case4']\n    x_star4 = _run_proximal_gradient(tau4, x0, A, b, max_iter, tol)\n    # Check if solution is close to the unconstrained least squares solution\n    res4 = np.linalg.norm(x_star4 - x_ls) = 1e-8\n    results.append(res4)\n    \n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}