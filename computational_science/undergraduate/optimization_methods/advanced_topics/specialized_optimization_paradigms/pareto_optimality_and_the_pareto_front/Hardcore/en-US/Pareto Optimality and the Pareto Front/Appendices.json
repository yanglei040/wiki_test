{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering Pareto optimality is to develop an intuition for trade-offs using a concrete set of choices. This practice challenges you to identify the Pareto front from a discrete set of design options, making the abstract concept of dominance tangible. By comparing the true set of optimal trade-offs to the single solution found by a simple greedy algorithm, you will directly observe why myopic, single-objective thinking can lead to suboptimal outcomes in a multi-objective world . This exercise builds a foundational understanding of identifying non-dominated solutions and appreciating the richness of the trade-off space.",
            "id": "3160572",
            "problem": "Consider a bi-objective minimization problem in multiobjective optimization (MOO) with objectives $f_1$ and $f_2$. A solution $x$ is said to Pareto dominate a solution $y$ if $f_i(x) \\le f_i(y)$ for all $i \\in \\{1,2\\}$ and $f_j(x)  f_j(y)$ for at least one $j$. The collection of all undominated solutions is called the Pareto front.\n\nYou are given a discrete feasible set of designs $\\{x_A, x_B, x_C, x_D, x_E, x_F\\}$ with objective vectors\n$$\nf(x_A) = (1, 16),\\quad\nf(x_B) = (3, 9.5),\\quad\nf(x_C) = (5, 6.5),\\quad\nf(x_D) = (8, 5.5),\\quad\nf(x_E) = (12, 4.8),\\quad\nf(x_F) = (16, 1).\n$$\nAll objectives are to be minimized.\n\nA greedy procedure “by $f_1$ then $f_2$” selects the design with the smallest $f_1$-value; ties, if any, are broken by choosing the smallest $f_2$-value among those tied.\n\nDefine the “knee” of the Pareto front as the Pareto-efficient design that maximizes the Euclidean perpendicular distance in the objective space from the straight line passing through the two extreme Pareto designs with minimal $f_1$ and minimal $f_2$ respectively. This definition captures the point of greatest trade-off curvature between $f_1$ and $f_2$ based on the extremes.\n\nTasks:\n- Using the definition of Pareto dominance, identify the two extreme Pareto designs corresponding to the smallest $f_1$ and smallest $f_2$.\n- Identify the design selected by the greedy “by $f_1$ then $f_2$” procedure.\n- Identify the knee design according to the definition above.\n- Compute, in exact closed form, the magnitude of the gap in the objective space, defined as the Euclidean distance between the greedy-selected design’s objective vector and the knee design’s objective vector.\n\nYour final answer must be a single closed-form analytic expression. Do not round. No units are required.",
            "solution": "The problem is a bi-objective minimization problem with a discrete feasible set of designs. The objective is to identify specific designs based on given criteria and compute a distance between two of them in the objective space.\n\nFirst, the problem must be validated.\nThe givens are:\n- A bi-objective minimization problem with objectives $f_1$ and $f_2$.\n- The definition of Pareto dominance: A solution $x$ dominates $y$ if $f_i(x) \\le f_i(y)$ for all $i \\in \\{1,2\\}$ and $f_j(x)  f_j(y)$ for at least one $j$.\n- The feasible set of designs is $\\{x_A, x_B, x_C, x_D, x_E, x_F\\}$.\n- The corresponding objective vectors are:\n  $f(x_A) = (1, 16)$\n  $f(x_B) = (3, 9.5)$\n  $f(x_C) = (5, 6.5)$\n  $f(x_D) = (8, 5.5)$\n  $f(x_E) = (12, 4.8)$\n  $f(x_F) = (16, 1)$\n- A greedy selection procedure is defined as selecting the design with the smallest $f_1$-value, with ties broken by the smallest $f_2$-value.\n- The \"knee\" design is defined as the Pareto-efficient design that maximizes the perpendicular distance from the line connecting the two extreme Pareto designs (those with minimal $f_1$ and minimal $f_2$).\n\nThe problem is scientifically grounded in the field of multi-objective optimization, using standard, well-defined concepts. It is well-posed, providing all necessary information and clear definitions to arrive at a unique solution. The data is consistent and does not violate any mathematical principles. The problem is objective and formalizable. Therefore, the problem is deemed valid and a solution will be provided.\n\nLet's denote the objective vectors as points in $\\mathbb{R}^2$: $P_A=(1, 16)$, $P_B=(3, 9.5)$, $P_C=(5, 6.5)$, $P_D=(8, 5.5)$, $P_E=(12, 4.8)$, $P_F=(16, 1)$.\n\nThe first step is to identify the set of non-dominated solutions, which forms the Pareto front. Let's compare the points. The $f_1$ components of the objective vectors are $1, 3, 5, 8, 12, 16$, which is a strictly increasing sequence. The $f_2$ components are $16, 9.5, 6.5, 5.5, 4.8, 1$, which is a strictly decreasing sequence. For any two distinct points $P_i$ and $P_j$, if the first component of $P_i$ is smaller than that of $P_j$, its second component is larger. This means that neither point can dominate the other according to the given definition of Pareto dominance. Consequently, all six designs $\\{x_A, x_B, x_C, x_D, x_E, x_F\\}$ are non-dominated (i.e., Pareto-efficient), and the set of their objective vectors constitutes the Pareto front for this discrete problem.\n\nNow, we proceed with the specified tasks.\n\n**1. Identify the extreme Pareto designs.**\nThe extreme Pareto designs are those with the minimal $f_1$ and minimal $f_2$ values, respectively.\n- The minimum value of $f_1$ among all designs is $1$, which corresponds to design $x_A$. The objective vector is $P_A = (1, 16)$.\n- The minimum value of $f_2$ among all designs is $1$, which corresponds to design $x_F$. The objective vector is $P_F = (16, 1)$.\nThe two extreme Pareto designs are $x_A$ and $x_F$.\n\n**2. Identify the design selected by the greedy procedure.**\nThe greedy procedure selects the design with the smallest $f_1$-value. As established, the minimum $f_1$-value is $1$, corresponding to design $x_A$. Since there are no ties for the minimum $f_1$-value, the tie-breaking rule is not needed. The greedy-selected design is $x_A$, with objective vector $P_A = (1, 16)$.\n\n**3. Identify the knee design.**\nThe knee is the Pareto-efficient design that maximizes the perpendicular distance to the line passing through the two extreme points, $P_A=(1, 16)$ and $P_F=(16, 1)$.\nLet the coordinates of a generic point be $(f_1, f_2)$. The equation of the line passing through $(f_{1,A}, f_{2,A})=(1, 16)$ and $(f_{1,F}, f_{2,F})=(16, 1)$ is found as follows.\nThe slope $m$ is $m = \\frac{f_{2,F} - f_{2,A}}{f_{1,F} - f_{1,A}} = \\frac{1 - 16}{16 - 1} = \\frac{-15}{15} = -1$.\nUsing the point-slope form $f_2 - f_{2,A} = m(f_1 - f_{1,A})$, we get:\n$f_2 - 16 = -1(f_1 - 1)$\n$f_2 - 16 = -f_1 + 1$\n$f_1 + f_2 - 17 = 0$.\nThe perpendicular distance $d$ from a point $(f_{1,0}, f_{2,0})$ to a line $Af_1 + Bf_2 + C = 0$ is given by $d = \\frac{|Af_{1,0} + Bf_{2,0} + C|}{\\sqrt{A^2 + B^2}}$.\nHere, $A=1$, $B=1$, and $C=-17$. So, $d = \\frac{|f_{1,0} + f_{2,0} - 17|}{\\sqrt{1^2 + 1^2}} = \\frac{|f_{1,0} + f_{2,0} - 17|}{\\sqrt{2}}$.\nWe calculate this distance for all Pareto-efficient points, which are all six points in this case.\n- For $P_A=(1, 16)$: $d_A = \\frac{|1 + 16 - 17|}{\\sqrt{2}} = 0$.\n- For $P_B=(3, 9.5)$: $d_B = \\frac{|3 + 9.5 - 17|}{\\sqrt{2}} = \\frac{|12.5 - 17|}{\\sqrt{2}} = \\frac{|-4.5|}{\\sqrt{2}} = \\frac{4.5}{\\sqrt{2}}$.\n- For $P_C=(5, 6.5)$: $d_C = \\frac{|5 + 6.5 - 17|}{\\sqrt{2}} = \\frac{|11.5 - 17|}{\\sqrt{2}} = \\frac{|-5.5|}{\\sqrt{2}} = \\frac{5.5}{\\sqrt{2}}$.\n- For $P_D=(8, 5.5)$: $d_D = \\frac{|8 + 5.5 - 17|}{\\sqrt{2}} = \\frac{|13.5 - 17|}{\\sqrt{2}} = \\frac{|-3.5|}{\\sqrt{2}} = \\frac{3.5}{\\sqrt{2}}$.\n- For $P_E=(12, 4.8)$: $d_E = \\frac{|12 + 4.8 - 17|}{\\sqrt{2}} = \\frac{|16.8 - 17|}{\\sqrt{2}} = \\frac{|-0.2|}{\\sqrt{2}} = \\frac{0.2}{\\sqrt{2}}$.\n- For $P_F=(16, 1)$: $d_F = \\frac{|16 + 1 - 17|}{\\sqrt{2}} = 0$.\nComparing the distances, the maximum is $d_C = \\frac{5.5}{\\sqrt{2}}$. This corresponds to design $x_C$.\nThe knee design is $x_C$, with objective vector $P_C = (5, 6.5)$.\n\n**4. Compute the magnitude of the gap.**\nThis is the Euclidean distance between the greedy-selected design's objective vector and the knee design's objective vector.\n- Greedy objective vector: $P_{greedy} = P_A = (1, 16)$.\n- Knee objective vector: $P_{knee} = P_C = (5, 6.5)$.\nThe Euclidean distance $D$ is given by the formula $D = \\sqrt{(f_{1,knee} - f_{1,greedy})^2 + (f_{2,knee} - f_{2,greedy})^2}$.\n$D = \\sqrt{(5 - 1)^2 + (6.5 - 16)^2}$\n$D = \\sqrt{4^2 + (-9.5)^2}$\nTo compute this in exact form, we use fractions. $6.5 = \\frac{13}{2}$ and $9.5 = \\frac{19}{2}$.\n$D = \\sqrt{16 + \\left(-\\frac{19}{2}\\right)^2}$\n$D = \\sqrt{16 + \\frac{361}{4}}$\n$D = \\sqrt{\\frac{16 \\times 4}{4} + \\frac{361}{4}}$\n$D = \\sqrt{\\frac{64 + 361}{4}}$\n$D = \\sqrt{\\frac{425}{4}}$\n$D = \\frac{\\sqrt{425}}{\\sqrt{4}} = \\frac{\\sqrt{425}}{2}$\nTo simplify $\\sqrt{425}$, we find its prime factors: $425 = 5 \\times 85 = 5 \\times 5 \\times 17 = 5^2 \\times 17$.\nSo, $\\sqrt{425} = \\sqrt{5^2 \\times 17} = 5\\sqrt{17}$.\nThe distance is $D = \\frac{5\\sqrt{17}}{2}$.\nThis is the required exact closed-form expression for the magnitude of the gap.",
            "answer": "$$\\boxed{\\frac{5\\sqrt{17}}{2}}$$"
        },
        {
            "introduction": "Moving from discrete points to a continuous spectrum of solutions allows for a more powerful analysis. This exercise guides you through the analytical derivation of a continuous Pareto front for a classic bi-objective problem, using the weighted-sum scalarization method to parameterize the set of optimal solutions . You will then apply calculus to formalize the concept of a \"knee\"—the point of maximum trade-off—by calculating the curvature of the front, providing a rigorous method for selecting a single, balanced solution from an infinite set of possibilities.",
            "id": "3160527",
            "problem": "Consider a two-objective Multi-Objective Optimization (MOO) problem with decision variable $x \\in [0,1]$ and objective functions $f_{1}(x)=x^{2}$ and $f_{2}(x)=(x-1)^{2}$. Use the standard definitions of Pareto dominance and the Pareto front: a decision $x^{\\ast}$ is Pareto optimal if there is no $x$ such that $f_{i}(x) \\leq f_{i}(x^{\\ast})$ for all $i \\in \\{1,2\\}$ with at least one strict inequality, and the Pareto front is the image of the set of Pareto optimal decisions under the mapping $x \\mapsto (f_{1}(x),f_{2}(x))$.\n\nGround your derivations in the following well-tested facts:\n- For differentiable scalarization $J_{\\lambda}(x)$ on a convex set, the minimizers are characterized by first-order conditions where interior stationary points satisfy $\\frac{\\mathrm{d}}{\\mathrm{d}x}J_{\\lambda}(x)=0$, and boundary points are checked when interior solutions lie outside the feasible set.\n- The curvature of a twice-differentiable plane curve $r(s)=(u(s),v(s))$ is $\\kappa(s)=\\frac{|u'(s)v''(s)-v'(s)u''(s)|}{\\left(u'(s)^{2}+v'(s)^{2}\\right)^{3/2}}$, and a “knee” on the Pareto front can be operationally defined as the point of maximal curvature along the parametric image of the decision set.\n\nTasks:\n- Derive, from first principles, the structure of the Pareto optimal set in the decision space $[0,1]$ and its Pareto front in objective space $\\mathbb{R}^{2}$.\n- Introduce the weighted-sum scalarization $J_{\\lambda}(x)=\\lambda f_{1}(x)+(1-\\lambda)f_{2}(x)$ with $\\lambda \\in [0,1]$. Compute analytically the $\\lambda$-parametrized minimizer $x^{\\ast}(\\lambda)$ on $[0,1]$, and map it to the Pareto front.\n- Treat the image of $x \\mapsto (f_{1}(x),f_{2}(x))$ as a parametric plane curve. Using the curvature formula, compute the decision value $x_{\\text{knee}}$ that maximizes curvature and then determine the corresponding $\\lambda_{\\text{knee}}$ via the $\\lambda$-parametrization.\n- Extend to $\\mathbb{R}^{n}$ by fixing a nonzero vector $\\mathbf{a} \\in \\mathbb{R}^{n}$ and restricting the decision set to the line segment $\\{\\mathbf{x}=t\\mathbf{a}:t \\in [0,1]\\}$. Consider $F_{1}(t)=\\|\\mathbf{x}\\|^{2}=\\|t\\mathbf{a}\\|^{2}$ and $F_{2}(t)=\\|\\mathbf{x}-\\mathbf{a}\\|^{2}=\\|t\\mathbf{a}-\\mathbf{a}\\|^{2}$, apply the weighted-sum scalarization $J_{\\lambda}(t)=\\lambda F_{1}(t)+(1-\\lambda)F_{2}(t)$, and repeat the curvature-based knee computation. Show how the knee, expressed as a value of $\\lambda$, depends (or does not depend) on the dimension $n$ and the choice of $\\mathbf{a}$.\n\nAnswer specification:\nReport only the unique value of $\\lambda \\in [0,1]$ at which the knee occurs. The answer must be a single exact number. No rounding is required, and no units are involved.",
            "solution": "The problem asks for the unique value of the weighted-sum parameter $\\lambda \\in [0,1]$ that corresponds to the \"knee\" of the Pareto front, where the knee is defined as the point of maximum curvature. We will solve this by following the sequence of tasks outlined.\n\nFirst, we analyze the one-dimensional problem with objective functions $f_{1}(x)=x^{2}$ and $f_{2}(x)=(x-1)^{2}$ for a decision variable $x \\in [0,1]$.\n\nThe first step is to characterize the Pareto optimal set. The function $f_{1}(x)=x^{2}$ is strictly increasing on the interval $[0,1]$, while $f_{2}(x)=(x-1)^{2}$ is strictly decreasing on $[0,1]$. For any two distinct decision variables $x_{a}, x_{b} \\in [0,1]$ such that $x_{a}  x_{b}$, we have $f_{1}(x_{a})  f_{1}(x_{b})$ and $f_{2}(x_{a})  f_{2}(x_{b})$. This means that any improvement in one objective function is necessarily accompanied by a degradation in the other. Consequently, no point in the decision space is dominated by another. Therefore, the entire decision set $x \\in [0,1]$ is the Pareto optimal set.\n\nNext, we employ the weighted-sum scalarization method to find the minimizers of the composite objective function $J_{\\lambda}(x)$, which provides a parameterization of the Pareto front. The scalarized function is given by:\n$$J_{\\lambda}(x) = \\lambda f_{1}(x) + (1-\\lambda)f_{2}(x) = \\lambda x^{2} + (1-\\lambda)(x-1)^{2}$$\nwhere $\\lambda \\in [0,1]$. To find the minimizer $x^{\\ast}(\\lambda)$, we apply the first-order necessary condition for an extremum, $\\frac{\\mathrm{d}J_{\\lambda}}{\\mathrm{d}x} = 0$, for interior points.\n$$\\frac{\\mathrm{d}J_{\\lambda}}{\\mathrm{d}x} = \\frac{\\mathrm{d}}{\\mathrm{d}x} \\left( \\lambda x^{2} + (1-\\lambda)(x^{2} - 2x + 1) \\right) = 2\\lambda x + 2(1-\\lambda)(x-1)$$\nSetting the derivative to zero yields:\n$$2\\lambda x + 2(1-\\lambda)x - 2(1-\\lambda) = 0$$\n$$2x(\\lambda + 1 - \\lambda) - 2(1-\\lambda) = 0$$\n$$2x - 2(1-\\lambda) = 0 \\implies x = 1-\\lambda$$\nSince $\\lambda \\in [0,1]$, the solution $x^{\\ast}(\\lambda) = 1-\\lambda$ is always within the feasible domain $[0,1]$. The second derivative is $\\frac{\\mathrm{d}^{2}J_{\\lambda}}{\\mathrm{d}x^{2}} = 2\\lambda + 2(1-\\lambda) = 2  0$, confirming that $x^{\\ast}(\\lambda)$ is a unique minimizer for any $\\lambda \\in [0,1]$. This gives us a mapping from the weight $\\lambda$ to the Pareto optimal solution $x^{\\ast}$.\n\nNow, we determine the knee of the Pareto front, defined as the point of maximum curvature. The Pareto front is the set of points $(f_{1}(x), f_{2}(x))$ for $x \\in [0,1]$. We can treat this as a parametric plane curve $r(x) = (u(x), v(x))$ with parameter $x$, where $u(x) = f_{1}(x) = x^{2}$ and $v(x) = f_{2}(x) = (x-1)^{2}$. The derivatives with respect to $x$ are:\n$$u'(x) = 2x, \\quad v'(x) = 2(x-1)$$\n$$u''(x) = 2, \\quad v''(x) = 2$$\nUsing the given formula for curvature $\\kappa(x)$:\n$$\\kappa(x) = \\frac{|u'(x)v''(x) - v'(x)u''(x)|}{\\left(u'(x)^{2} + v'(x)^{2}\\right)^{3/2}}$$\nThe numerator of the curvature expression is:\n$$|u'v'' - v'u''| = |(2x)(2) - (2(x-1))(2)| = |4x - 4x + 4| = 4$$\nThe denominator is:\n$$\\left(u'(x)^{2} + v'(x)^{2}\\right)^{3/2} = \\left((2x)^{2} + (2(x-1))^{2}\\right)^{3/2} = \\left(4x^{2} + 4(x^{2}-2x+1)\\right)^{3/2}$$\n$$= \\left(8x^{2} - 8x + 4\\right)^{3/2} = \\left(4(2x^{2} - 2x + 1)\\right)^{3/2} = 8\\left(2x^{2} - 2x + 1\\right)^{3/2}$$\nThus, the curvature is:\n$$\\kappa(x) = \\frac{4}{8\\left(2x^{2} - 2x + 1\\right)^{3/2}} = \\frac{1}{2\\left(2x^{2} - 2x + 1\\right)^{3/2}}$$\nTo maximize $\\kappa(x)$, we must minimize the denominator, which is equivalent to minimizing the quadratic function $g(x) = 2x^{2} - 2x + 1$ over $x \\in [0,1]$. The vertex of this upward-opening parabola occurs at $x = -\\frac{-2}{2(2)} = \\frac{1}{2}$. Since this value lies within the domain $[0,1]$, it is the location of the minimum of $g(x)$.\nTherefore, the knee of the Pareto front corresponds to the decision variable $x_{\\text{knee}} = \\frac{1}{2}$.\nWe now find the value of $\\lambda_{\\text{knee}}$ that yields this solution using the relation $x^{\\ast}(\\lambda) = 1-\\lambda$:\n$$x_{\\text{knee}} = 1 - \\lambda_{\\text{knee}} \\implies \\frac{1}{2} = 1 - \\lambda_{\\text{knee}} \\implies \\lambda_{\\text{knee}} = \\frac{1}{2}$$\n\nFinally, we extend the analysis to the $n$-dimensional case. The decision set is $\\{\\mathbf{x} = t\\mathbf{a} : t \\in [0,1]\\}$ for a nonzero vector $\\mathbf{a} \\in \\mathbb{R}^{n}$. The decision variable is $t$. The objective functions are:\n$$F_{1}(t) = \\|\\mathbf{x}\\|^{2} = \\|t\\mathbf{a}\\|^{2} = t^{2}\\|\\mathbf{a}\\|^{2}$$\n$$F_{2}(t) = \\|\\mathbf{x}-\\mathbf{a}\\|^{2} = \\|t\\mathbf{a}-\\mathbf{a}\\|^{2} = \\|(t-1)\\mathbf{a}\\|^{2} = (t-1)^{2}\\|\\mathbf{a}\\|^{2}$$\nLet $C = \\|\\mathbf{a}\\|^{2}$. Since $\\mathbf{a}$ is nonzero, $C  0$. The objective functions are $F_{1}(t) = Ct^{2}$ and $F_{2}(t) = C(t-1)^{2}$. This problem is structurally identical to the one-dimensional case, with the objectives scaled by a constant factor $C$.\n\nThe scalarized function is $J_{\\lambda}(t) = \\lambda F_{1}(t) + (1-\\lambda)F_{2}(t) = C \\left( \\lambda t^{2} + (1-\\lambda)(t-1)^{2} \\right)$. Minimizing $J_{\\lambda}(t)$ with respect to $t$ is equivalent to minimizing the term in the parenthesis, which has the same form as the 1D case. The minimizer is thus $t^{\\ast}(\\lambda) = 1-\\lambda$.\n\nThe Pareto front is parameterized by $t \\in [0,1]$ as $r(t) = (Ct^{2}, C(t-1)^{2})$. The derivatives are $u'(t)=2Ct, v'(t)=2C(t-1), u''(t)=2C, v''(t)=2C$.\nThe curvature is:\n$$\\kappa(t) = \\frac{|(2Ct)(2C) - (2C(t-1))(2C)|}{\\left((2Ct)^{2} + (2C(t-1))^{2}\\right)^{3/2}} = \\frac{|4C^{2}|}{\\left(4C^{2}(t^{2} + (t-1)^{2})\\right)^{3/2}}$$\n$$\\kappa(t) = \\frac{4C^{2}}{8C^{3}\\left(2t^{2}-2t+1\\right)^{3/2}} = \\frac{1}{2C\\left(2t^{2}-2t+1\\right)^{3/2}}$$\nMaximizing $\\kappa(t)$ requires minimizing $g(t) = 2t^{2}-2t+1$, which again occurs at $t_{\\text{knee}} = \\frac{1}{2}$.\nUsing the correspondence $t^{\\ast}(\\lambda)=1-\\lambda$, we find:\n$$t_{\\text{knee}} = 1 - \\lambda_{\\text{knee}} \\implies \\frac{1}{2} = 1 - \\lambda_{\\text{knee}} \\implies \\lambda_{\\text{knee}} = \\frac{1}{2}$$\nThis result is independent of the dimension $n$ and the specific choice of the non-zero vector $\\mathbf{a}$. The unique value of $\\lambda$ at which the knee occurs is $\\frac{1}{2}$.",
            "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$"
        },
        {
            "introduction": "Many real-world optimization problems, like the famous knapsack problem, are combinatorial in nature, involving a discrete and often vast search space. This practice requires you to implement a computational approach to explore such a problem, where you will enumerate the exact Pareto front for a bi-objective knapsack scenario . By comparing the complete set of optimal solutions with the single point generated by a standard greedy heuristic, you will gain practical insight into the limitations of heuristics and the importance of exact methods for capturing the full range of optimal trade-offs in complex, discrete systems.",
            "id": "3160598",
            "problem": "You are given a bi-objective $0$-$1$ knapsack decision problem. There are $n$ items. Each item $i$ has a positive integer weight $w_i$, and two nonnegative integer profits $p_{1i}$ and $p_{2i}$. A decision vector is $x \\in \\{0,1\\}^n$, where $x_i = 1$ means the item is selected and $x_i = 0$ means it is not. A decision $x$ is feasible if $\\sum_{i=1}^n w_i x_i \\leq C$, where $C$ is the capacity. The objective is to maximize the profit vector $(\\sum_{i=1}^n p_{1i} x_i, \\sum_{i=1}^n p_{2i} x_i)$ in the sense of Pareto optimality.\n\nFundamental base:\n- Pareto dominance: For two feasible decisions $x$ and $y$, let $f(x) = (f_1(x), f_2(x))$ and $f(y) = (f_1(y), f_2(y))$ denote their objective vectors, where $f_1(x) = \\sum_{i=1}^n p_{1i} x_i$ and $f_2(x) = \\sum_{i=1}^n p_{2i} x_i$. Then $f(y)$ Pareto-dominates $f(x)$ if $f_1(y) \\ge f_1(x)$ and $f_2(y) \\ge f_2(x)$ and at least one inequality is strict.\n- Pareto optimality and Pareto front: A feasible decision $x$ is Pareto-optimal if no other feasible decision $y$ Pareto-dominates it. The set of Pareto-optimal objective vectors in objective space is called the Pareto front.\n\nYou must implement two components:\n- Exact enumeration of the Pareto front in objective space: enumerate all feasible decisions $x \\in \\{0,1\\}^n$ satisfying $\\sum_{i=1}^n w_i x_i \\le C$, collect their objective vectors, and filter out all dominated vectors to obtain the exact Pareto front.\n- A greedy heuristic based on scalarization and density: fix the scalarization weight $\\lambda = \\frac{1}{2}$. For each item $i$, compute its scalarized value $v_i = \\lambda p_{1i} + (1-\\lambda) p_{2i}$ and define the density $d_i = \\frac{v_i}{w_i}$. Sort items in descending order of $d_i$. Break ties with the following deterministic rule: prefer larger $p_{1i}$, then larger $p_{2i}$, then smaller $w_i$. Traverse the sorted list once and add an item if it fits in the remaining capacity; otherwise skip it. This yields a single feasible selection and its objective vector $(f_1^{\\mathrm{g}}, f_2^{\\mathrm{g}})$.\n\nFor each test case, compute:\n- The exact Pareto front $F^\\star$ in objective space.\n- A boolean $b$ indicating whether the greedy's objective vector $(f_1^{\\mathrm{g}}, f_2^{\\mathrm{g}})$ is on $F^\\star$.\n- An integer $c$ equal to the number of Pareto-optimal objective vectors, that is $c = |F^\\star|$.\n- An integer $m$ equal to the number of Pareto-optimal objective vectors missed by the greedy heuristic, defined as $m = |F^\\star \\setminus \\{(f_1^{\\mathrm{g}}, f_2^{\\mathrm{g}})\\}|$.\n\nTest suite:\n- Case $1$ (happy path with multiple trade-offs): capacity $C = 7$; items given by the triplets $(w_i,p_{1i},p_{2i})$ for $i \\in \\{1,2,3,4,5,6\\}$:\n  - $i=1$: $(w_1,p_{11},p_{21}) = (3,9,1)$\n  - $i=2$: $(w_2,p_{12},p_{22}) = (3,1,9)$\n  - $i=3$: $(w_3,p_{13},p_{23}) = (2,5,4)$\n  - $i=4$: $(w_4,p_{14},p_{24}) = (4,7,6)$\n  - $i=5$: $(w_5,p_{15},p_{25}) = (1,2,5)$\n  - $i=6$: $(w_6,p_{16},p_{26}) = (2,4,2)$\n- Case $2$ (edge case where greedy is dominated due to integrality and leftover capacity): capacity $C = 5$; items for $i \\in \\{1,2,3\\}$:\n  - $i=1$: $(w_1,p_{11},p_{21}) = (5,11,11)$\n  - $i=2$: $(w_2,p_{12},p_{22}) = (3,7,7)$\n  - $i=3$: $(w_3,p_{13},p_{23}) = (2,3,3)$\n- Case $3$ (boundary condition $C=0$): capacity $C = 0$; items for $i \\in \\{1,2,3\\}$:\n  - $i=1$: $(w_1,p_{11},p_{21}) = (2,5,4)$\n  - $i=2$: $(w_2,p_{12},p_{22}) = (3,4,6)$\n  - $i=3$: $(w_3,p_{13},p_{23}) = (1,3,2)$\n- Case $4$ (ties in density and symmetric trade-offs): capacity $C = 5$; items for $i \\in \\{1,2,3,4\\}$:\n  - $i=1$: $(w_1,p_{11},p_{21}) = (2,8,2)$\n  - $i=2$: $(w_2,p_{12},p_{22}) = (2,2,8)$\n  - $i=3$: $(w_3,p_{13},p_{23}) = (3,6,6)$\n  - $i=4$: $(w_4,p_{14},p_{24}) = (1,1,1)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces. Each element corresponds to one test case and is itself a bracketed list $[b,c,m]$, where $b$ is a boolean, and $c$ and $m$ are integers. For example, the overall output should look like $[[\\mathrm{True},5,4],[\\mathrm{False},7,7],\\dots]$ but without spaces.",
            "solution": "We begin from fundamental definitions in multiobjective optimization. Given a feasible decision $x \\in \\{0,1\\}^n$ with capacity constraint $\\sum_{i=1}^n w_i x_i \\le C$, its objective vector is $f(x) = (f_1(x), f_2(x))$ where $f_1(x) = \\sum_{i=1}^n p_{1i} x_i$ and $f_2(x) = \\sum_{i=1}^n p_{2i} x_i$. For two feasible decisions $x$ and $y$, $f(y)$ Pareto-dominates $f(x)$ if $f_1(y) \\ge f_1(x)$ and $f_2(y) \\ge f_2(x)$ and at least one inequality is strict. A decision $x$ is Pareto-optimal if it is not dominated by any other feasible decision. The Pareto front is the set of all nondominated objective vectors.\n\nPrinciple-based exact method:\n- The most direct, principle-faithful approach to obtain the exact Pareto front for small instances is enumeration. Enumerate all $x \\in \\{0,1\\}^n$ and retain those that satisfy $\\sum_{i=1}^n w_i x_i \\le C$. Compute $(f_1(x), f_2(x))$ for each feasible $x$ and collect all such vectors. Remove duplicates in objective space (different decisions can map to the same objective vector). Then filter out dominated vectors using the dominance relation: a vector $(a_1,a_2)$ is dominated if there exists $(b_1,b_2)$ with $b_1 \\ge a_1$ and $b_2 \\ge a_2$ and at least one strict inequality. The remaining set is the exact Pareto front $F^\\star$.\n\nAlgorithmic design for enumeration:\n- The enumeration is $O(2^n)$ in the number of decisions, which is tractable for the tiny instances provided. Dominance filtering is $O(|S|^2)$ in the number $|S|$ of feasible objective vectors before filtering. Because $n$ is small (at most $6$ in the test suite), both costs are acceptable.\n\nGreedy heuristic based on scalarized density:\n- Scalarization transforms the bi-objective into a single-objective by forming $v_i = \\lambda p_{1i} + (1-\\lambda) p_{2i}$. We fix $\\lambda = \\frac{1}{2}$ so $v_i = \\frac{p_{1i} + p_{2i}}{2}$. To mimic single-objective knapsack greedy, compute density $d_i = \\frac{v_i}{w_i}$. Sort items by $d_i$ descending and break ties deterministically by larger $p_{1i}$, then larger $p_{2i}$, then smaller $w_i$. Traverse the sorted items once; add an item if it fits in the remaining capacity. The result is a single feasible selection with objective vector $(f_1^{\\mathrm{g}}, f_2^{\\mathrm{g}})$.\n\nWhy the greedy can miss Pareto points:\n- Fundamental reasoning connects to the nature of the feasible set. The $0$-$1$ knapsack feasible region in decision space is combinatorial and nonconvex; objective space images of feasible decisions are discrete points. Scalarization with a fixed $\\lambda$ can only identify objectives that are optimal for that scalarized objective, and the greedy selection is not guaranteed to solve even the scalarized knapsack optimally because it uses density ordering rather than dynamic programming or exact optimization. Therefore:\n  - It returns only one point, whereas the Pareto front typically contains multiple trade-off points; all other Pareto points are missed by design.\n  - Integrality and leftover capacity can cause a myopic density choice to yield a dominated point. In test case $2$ with capacity $C = 5$ and items $(w_1,p_{11},p_{21})=(5,11,11)$, $(w_2,p_{12},p_{22})=(3,7,7)$, $(w_3,p_{13},p_{23})=(2,3,3)$, the greedy picks item $2$ then item $3$ (densities $\\frac{7}{3}$ then $\\frac{3}{2}$) yielding $(10,10)$, which is Pareto-dominated by item $1$ alone $(11,11)$. The greedy misses the supported, strictly superior point because it cannot reshuffle its earlier choice once capacity is partially consumed.\n  - Even when the greedy point is Pareto-optimal, it will still miss other Pareto-optimal points that realize different trade-offs (for example, test case $4$ contains symmetric points such as $(14,8)$ and $(8,14)$ in addition to the central $(11,11)$).\n\nComputation outputs:\n- For each case, we compute $F^\\star$, check whether $(f_1^{\\mathrm{g}}, f_2^{\\mathrm{g}}) \\in F^\\star$, count $c = |F^\\star|$, and compute $m = c - 1$ if the greedy point is on the front, otherwise $m = c$. These are directly quantifiable. The final output is a single line, formatted as a bracketed list of per-case summaries $[b,c,m]$ with no spaces.\n\nThis design adheres to core definitions of Pareto dominance and bi-objective optimization, and implements a principled exact method alongside a heuristic that is widely used in practice for scalarized knapsack, thereby illustrating what Pareto optimality is, why it holds for the enumerated points, and how heuristic simplifications can fail in a nonconvex discrete setting.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import product\n\ndef enumerate_feasible_points(weights, p1, p2, capacity):\n    \"\"\"\n    Enumerate all feasible 0-1 selections and return the list of objective pairs (p1_sum, p2_sum).\n    Duplicates in objective space are allowed initially; they will be deduplicated later.\n    \"\"\"\n    n = len(weights)\n    points = []\n    for x_bits in product([0, 1], repeat=n):\n        total_w = sum(w * x for w, x in zip(weights, x_bits))\n        if total_w = capacity:\n            sum1 = sum(a * x for a, x in zip(p1, x_bits))\n            sum2 = sum(b * x for b, x in zip(p2, x_bits))\n            points.append((sum1, sum2))\n    return points\n\ndef pareto_front(points):\n    \"\"\"\n    Given a list of objective pairs (p1, p2), return the exact Pareto front (nondominated points)\n    in objective space. Duplicate objective pairs are reduced to unique representatives.\n    \"\"\"\n    unique_points = list(set(points))\n    front = []\n    for i, pi in enumerate(unique_points):\n        dominated = False\n        for j, pj in enumerate(unique_points):\n            if i == j:\n                continue\n            # pj dominates pi if pj = pi in both coords and strictly in at least one\n            if (pj[0] = pi[0] and pj[1] = pi[1]) and (pj[0]  pi[0] or pj[1]  pi[1]):\n                dominated = True\n                break\n        if not dominated:\n            front.append(pi)\n    # Optional: sort for reproducibility\n    front.sort(key=lambda x: (x[0], x[1]))\n    return front\n\ndef greedy_scalar_density(weights, p1, p2, capacity, lam=0.5):\n    \"\"\"\n    Greedy heuristic:\n    - Compute scalarized value v_i = lam * p1_i + (1-lam) * p2_i.\n    - Compute density d_i = v_i / w_i.\n    - Sort by d_i descending; tie-break by larger p1_i, then larger p2_i, then smaller w_i.\n    - Traverse once, add item if it fits.\n    Return the objective pair (sum p1, sum p2) of the selected set.\n    \"\"\"\n    items = []\n    for i in range(len(weights)):\n        w_i = weights[i]\n        v_i = lam * p1[i] + (1.0 - lam) * p2[i]\n        d_i = v_i / w_i if w_i  0 else float('inf')\n        items.append((i, w_i, p1[i], p2[i], d_i))\n    # Sort with deterministic tie-breaking\n    items.sort(key=lambda t: (-t[4], -t[2], -t[3], t[1]))\n    total_w = 0\n    sum1 = 0\n    sum2 = 0\n    for _, w_i, p1_i, p2_i, _ in items:\n        if total_w + w_i = capacity:\n            total_w += w_i\n            sum1 += p1_i\n            sum2 += p2_i\n    return (sum1, sum2)\n\ndef compact_str(obj):\n    \"\"\"\n    Convert nested Python lists/tuples/booleans/ints to a compact string without spaces,\n    using brackets for lists and parentheses for tuples.\n    \"\"\"\n    if isinstance(obj, list):\n        return '[' + ','.join(compact_str(x) for x in obj) + ']'\n    elif isinstance(obj, tuple):\n        return '(' + ','.join(compact_str(x) for x in obj) + ')'\n    elif isinstance(obj, bool):\n        return 'True' if obj else 'False'\n    else:\n        return str(obj)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"weights\": [3, 3, 2, 4, 1, 2],\n            \"p1\":      [9, 1, 5, 7, 2, 4],\n            \"p2\":      [1, 9, 4, 6, 5, 2],\n            \"C\":       7\n        },\n        # Case 2 (greedy dominated)\n        {\n            \"weights\": [5, 3, 2],\n            \"p1\":      [11, 7, 3],\n            \"p2\":      [11, 7, 3],\n            \"C\":       5\n        },\n        # Case 3 (capacity zero)\n        {\n            \"weights\": [2, 3, 1],\n            \"p1\":      [5, 4, 3],\n            \"p2\":      [4, 6, 2],\n            \"C\":       0\n        },\n        # Case 4 (ties and symmetry)\n        {\n            \"weights\": [2, 2, 3, 1],\n            \"p1\":      [8, 2, 6, 1],\n            \"p2\":      [2, 8, 6, 1],\n            \"C\":       5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        weights = case[\"weights\"]\n        p1 = case[\"p1\"]\n        p2 = case[\"p2\"]\n        C = case[\"C\"]\n\n        # Exact Pareto front\n        points = enumerate_feasible_points(weights, p1, p2, C)\n        front = pareto_front(points)\n\n        # Greedy heuristic\n        g_point = greedy_scalar_density(weights, p1, p2, C, lam=0.5)\n\n        # Metrics\n        is_on_front = g_point in front\n        total_pf = len(front)\n        missed = total_pf - (1 if is_on_front else 0)\n        results.append([is_on_front, total_pf, missed])\n\n    # Final print statement in the exact required format: no spaces.\n    print(compact_str(results))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}