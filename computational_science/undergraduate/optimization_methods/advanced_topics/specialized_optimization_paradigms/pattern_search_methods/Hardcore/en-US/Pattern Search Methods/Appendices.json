{
    "hands_on_practices": [
        {
            "introduction": "The journey into pattern search methods begins with a solid understanding of their core mechanics. This first practice challenges you to implement the classic Hooke-Jeeves algorithm from the ground up, applying it to the famous Rosenbrock function—a standard benchmark known for its narrow, curved valley that is notoriously difficult for many optimization algorithms. By building the exploratory and pattern moves yourself, you will gain direct, hands-on insight into how this derivative-free method navigates complex landscapes and why adaptive step-size control is essential for achieving convergence .",
            "id": "3161487",
            "problem": "Implement a deterministic Hooke-Jeeves (HJ) pattern search algorithm to minimize the Rosenbrock function in two variables, starting from multiple initial points and emphasizing how the step size adapts to traverse the curved valley. The Rosenbrock function is defined as $f(x,y) = (1 - x)^2 + 100\\,(y - x^2)^2$. Your program must implement the following, starting from fundamental definitions of direct search.\n\nDefinitions and requirements:\n- The Hooke-Jeeves (HJ) method alternates between an exploratory move and, if successful, a pattern move. The exploratory move polls along coordinate directions to find a local improvement; the pattern move extrapolates along the direction from the previous base point to the new improved point. No gradients or derivatives are used anywhere in the method.\n- Use the following deterministic exploratory move: from a current point $\\mathbf{x} \\in \\mathbb{R}^2$ and step size $\\Delta > 0$, sequentially probe the coordinates in the order $x$ then $y$, and within each coordinate probe the positive direction first and then the negative direction. For coordinate $i \\in \\{1,2\\}$, try $\\mathbf{x} + \\Delta \\mathbf{e}_i$; if this strictly improves $f$, accept it and continue to the next coordinate from that new point. Otherwise, try $\\mathbf{x} - \\Delta \\mathbf{e}_i$; if this strictly improves $f$, accept it and continue; otherwise keep the coordinate unchanged and continue.\n- After one full exploratory move from a base point $\\mathbf{x}^{B}$ yields a strictly improved point $\\mathbf{x}^{E}$ (that is, $f(\\mathbf{x}^{E})  f(\\mathbf{x}^{B})$), attempt a pattern move defined by $\\mathbf{x}^{P} = \\mathbf{x}^{E} + p\\,(\\mathbf{x}^{E} - \\mathbf{x}^{B})$, where $p > 0$ is a fixed pattern factor. Then perform an exploratory move starting from $\\mathbf{x}^{P}$ at the same step size $\\Delta$, yielding $\\tilde{\\mathbf{x}}$. If $f(\\tilde{\\mathbf{x}})  f(\\mathbf{x}^{E})$, accept the pattern move and set the new base point $\\mathbf{x}^{B} \\leftarrow \\tilde{\\mathbf{x}}$; otherwise, discard the pattern move and set the new base point $\\mathbf{x}^{B} \\leftarrow \\mathbf{x}^{E}$.\n- If an exploratory move from $\\mathbf{x}^{B}$ fails to find improvement at step size $\\Delta$ (that is, $\\mathbf{x}^{E} = \\mathbf{x}^{B}$), reduce the step size by a factor $c \\in (0,1)$, i.e., $\\Delta \\leftarrow c\\,\\Delta$.\n- Terminate when $\\Delta  \\Delta_{\\min}$ or when a maximum number of function evaluations is reached.\n- Use the following fixed parameters for all runs: pattern factor $p = 1$, step shrink factor $c = 0.5$, minimum step size $\\Delta_{\\min} = 10^{-5}$, and maximum number of function evaluations $N_{\\max} = 200000$. All comparisons of improvement should be strict (use $$ rather than $\\leq$). Angles are not involved in this problem.\n\nThe test suite is the following list of initial conditions, each specified by an initial point $(x_0,y_0)$ and an initial step size $\\Delta_0$:\n- Case $1$: $(x_0,y_0) = (-1.2, 1.0)$, $\\Delta_0 = 0.5$.\n- Case $2$: $(x_0,y_0) = (0.0, 0.0)$, $\\Delta_0 = 0.5$.\n- Case $3$: $(x_0,y_0) = (1.2, 1.2)$, $\\Delta_0 = 0.25$.\n- Case $4$: $(x_0,y_0) = (1.0, 1.0)$, $\\Delta_0 = 0.5$.\n- Case $5$: $(x_0,y_0) = (-1.5, 2.0)$, $\\Delta_0 = 1.0$.\n- Case $6$ (edge case for immediate termination): $(x_0,y_0) = (0.0, 0.0)$, $\\Delta_0 = 10^{-7}$.\n\nFor each case, your program must run the Hooke-Jeeves method with the exact rules above and return the following six quantities in order:\n- The final objective value $f^\\star$ at termination, rounded to six decimal places.\n- The final $x$-coordinate $x^\\star$, rounded to six decimal places.\n- The final $y$-coordinate $y^\\star$, rounded to six decimal places.\n- The total number of function evaluations used, as an integer.\n- The total number of step size reductions performed, as an integer.\n- The total number of accepted pattern moves, as an integer.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all six cases as a single list of lists, in the exact order of the cases above.\n- Each inner list must be of the form $[f^\\star, x^\\star, y^\\star, N_{\\text{eval}}, N_{\\text{shrink}}, N_{\\text{pattern}}]$ with the first three entries printed as decimal numbers rounded to six places and the last three entries printed as integers.\n- The overall output must be printed as a single line with no spaces, for example: $[[\\cdots],[\\cdots],\\ldots]$.\n\nNotes on scientific realism and rationale:\n- The Rosenbrock valley is narrow and curved near the minimizer at $(x,y) = (1,1)$, creating anisotropic curvature that challenges step-based methods. The Hooke-Jeeves step size adaptation ($\\Delta \\leftarrow c\\,\\Delta$ when no improvement) is essential to reduce $\\Delta$ sufficiently to align exploratory moves with the valley’s curvature while pattern moves accelerate progress along discovered descent directions.",
            "solution": "The user wants to implement the Hooke-Jeeves (HJ) deterministic pattern search algorithm to find the minimum of the two-variable Rosenbrock function, $f(x,y) = (1 - x)^2 + 100(y - x^2)^2$.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Objective Function**: $f(x,y) = (1 - x)^2 + 100(y - x^2)^2$.\n- **Algorithm**: Hooke-Jeeves (HJ) pattern search with two main components: an exploratory move and a pattern move.\n- **Exploratory Move Definition**: From a current point $\\mathbf{x}$, probe along coordinate axes $x$ then $y$. For each coordinate, probe the positive direction first ($\\mathbf{x} + \\Delta\\mathbf{e}_i$). If a strict improvement in $f$ is found, accept the new point and move to the next coordinate's exploration *from this new point*. Otherwise, probe the negative direction ($\\mathbf{x} - \\Delta\\mathbf{e}_i$). If this improves $f$, accept it and continue. If neither direction improves $f$, the point remains unchanged for that coordinate.\n- **Pattern Move Definition**: If an exploratory move from a base point $\\mathbf{x}^{B}$ finds a better point $\\mathbf{x}^{E}$ (i.e., $f(\\mathbf{x}^{E})  f(\\mathbf{x}^{B})$), a pattern point is defined as $\\mathbf{x}^{P} = \\mathbf{x}^{E} + p(\\mathbf{x}^{E} - \\mathbf{x}^{B})$. A second exploratory move is then performed from $\\mathbf{x}^{P}$, yielding a point $\\tilde{\\mathbf{x}}$.\n- **Update Rule**:\n    - If $f(\\tilde{\\mathbf{x}})  f(\\mathbf{x}^{E})$, the pattern move is accepted, and the new base point becomes $\\mathbf{x}^{B} \\leftarrow \\tilde{\\mathbf{x}}$.\n    - Otherwise, the pattern move is rejected, and the new base point is $\\mathbf{x}^{B} \\leftarrow \\mathbf{x}^{E}$.\n- **Step Size Reduction**: If an exploratory move from $\\mathbf{x}^{B}$ fails to find any improvement, the step size $\\Delta$ is reduced: $\\Delta \\leftarrow c\\Delta$.\n- **Termination Conditions**: The algorithm stops when $\\Delta  \\Delta_{\\min}$ or the number of function evaluations exceeds $N_{\\max}$.\n- **Fixed Parameters**:\n    - Pattern factor $p = 1$.\n    - Step shrink factor $c = 0.5$.\n    - Minimum step size $\\Delta_{\\min} = 10^{-5}$.\n    - Maximum function evaluations $N_{\\max} = 200000$.\n- **Test Cases (Initial Point $(x_0, y_0)$, Initial Step $\\Delta_0$**):\n    1. $((-1.2, 1.0), 0.5)$\n    2. $((0.0, 0.0), 0.5)$\n    3. $((1.2, 1.2), 0.25)$\n    4. $((1.0, 1.0), 0.5)$\n    5. $((-1.5, 2.0), 1.0)$\n    6. $((0.0, 0.0), 10^{-7})$\n- **Required Output**: For each case, a list $[f^\\star, x^\\star, y^\\star, N_{\\text{eval}}, N_{\\text{shrink}}, N_{\\text{pattern}}]$, with the first three values rounded to six decimal places.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is grounded in the well-established field of numerical optimization. The Hooke-Jeeves method and the Rosenbrock function are canonical subjects in this field.\n- **Well-Posed**: The problem is well-posed. The algorithm is deterministic, all parameters and initial conditions are explicitly defined, and the termination criteria are unambiguous. This ensures a unique, computable result for each test case.\n- **Objective**: The problem is stated using precise, objective mathematical and algorithmic language, leaving no room for subjective interpretation.\n- **Conclusion**: The problem statement is valid, as it is scientifically sound, fully specified, and objective. It does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Algorithmic Implementation\n\nThe solution involves a direct implementation of the Hooke-Jeeves algorithm as specified. The core logic is structured into three main parts: the main `hooke_jeeves` function that controls the overall flow, a helper function for the `exploratory_move`, and a class to manage function evaluations.\n\n1.  **Function Evaluation Counter**: A wrapper class `FunctionEvaluator` is used for the objective function $f(x,y)$. Each time the function is called through this class instance, an internal counter is incremented. This approach centralizes the counting of function evaluations, which is a key termination condition.\n\n2.  **Exploratory Move Function (`exploratory_move`)**: This function implements the search along coordinate axes. It takes a starting point $\\mathbf{x}_{\\text{start}}$, its function value $f(\\mathbf{x}_{\\text{start}})$, the step size $\\Delta$, and the function evaluator. The key aspect of the implementation is its adherence to the specified greedy, sequential update rule: once an improvement is found along a coordinate (e.g., $x$-positive), that new point immediately becomes the base for the subsequent search along the next coordinate (e.g., $y$). This ensures the path taken during the exploration is strictly downhill.\n\n3.  **Main Loop (`hooke_jeeves`)**: This function orchestrates the entire process.\n    - **Initialization**: It starts with the initial point $\\mathbf{x}_0$ and step size $\\Delta_0$. It initializes counters for step size reductions ($N_{\\text{shrink}}$) and accepted pattern moves ($N_{\\text{pattern}}$). A state variable, `x_best_ever`, is maintained to track the best point found throughout the entire search, which is crucial if termination occurs due to reaching $N_{\\max}$.\n    - **Iteration**: The main `while` loop continues as long as the termination conditions ($\\Delta \\ge \\Delta_{\\min}$ and $N_{\\text{eval}}  N_{\\max}$) are not met.\n    - **Main Logic per Iteration**:\n        a. An exploratory move is performed starting from the current base point, $\\mathbf{x}_{\\text{base}}$.\n        b. **If Improvement Found**: If the exploratory move yields a point $\\mathbf{x}_{\\text{explore}}$ with $f(\\mathbf{x}_{\\text{explore}})  f(\\mathbf{x}_{\\text{base}})$, a pattern move is attempted. The pattern point is calculated as $\\mathbf{x}_{P} = \\mathbf{x}_{\\text{explore}} + p(\\mathbf{x}_{\\text{explore}} - \\mathbf{x}_{\\text{base}})$. A second exploratory move is initiated from $\\mathbf{x}_{P}$ to find $\\tilde{\\mathbf{x}}$. The new base point for the next iteration is set to $\\tilde{\\mathbf{x}}$ if $f(\\tilde{\\mathbf{x}})  f(\\mathbf{x}_{\\text{explore}})$ (pattern accepted), otherwise it is set to $\\mathbf{x}_{\\text{explore}}$ (pattern rejected).\n        c. **If No Improvement**: If the initial exploratory move fails to find a better point, the step size $\\Delta$ is reduced by the shrink factor $c$, and the base point remains unchanged for the next iteration.\n    - **Termination**: Once the loop terminates, the function returns the best objective value found, the corresponding coordinates $(x^\\star, y^\\star)$, and the final counts for $N_{\\text{eval}}$, $N_{\\text{shrink}}$, and $N_{\\text{pattern}}$.\n\nThe program will execute this logic for each of the six provided test cases and format the output as a single list of lists.",
            "answer": "```python\nimport numpy as np\n\ndef rosenbrock_func(x, y):\n    \"\"\"The Rosenbrock function.\"\"\"\n    return (1.0 - x)**2 + 100.0 * (y - x**2)**2\n\nclass FunctionEvaluator:\n    \"\"\"A wrapper class to count function evaluations.\"\"\"\n    def __init__(self, func):\n        self.func = func\n        self.count = 0\n    \n    def __call__(self, p):\n        self.count += 1\n        return self.func(p[0], p[1])\n\ndef exploratory_move(x_start, f_start, delta, f_evaluator):\n    \"\"\"\n    Performs an exploratory move from a starting point, as per the problem description.\n\n    Args:\n        x_start: The starting point for the exploration.\n        f_start: The function value at x_start, to avoid re-evaluation.\n        delta: The current step size.\n        f_evaluator: The callable function-counting wrapper.\n\n    Returns:\n        A tuple (x_current, f_current) representing the best point found and its function value.\n    \"\"\"\n    x_current = np.copy(x_start)\n    f_current = f_start\n    \n    # Sequentially probe each coordinate\n    for i in range(len(x_start)):\n        # Probe in the positive direction\n        x_test = np.copy(x_current)\n        x_test[i] += delta\n        f_test = f_evaluator(x_test)\n        \n        if f_test  f_current:\n            x_current = x_test\n            f_current = f_test\n            continue  # Accepted, move to next coordinate from this new point\n        \n        # If positive failed, probe in the negative direction\n        x_test[i] -= 2 * delta  # From x_current[i] + delta to x_current[i] - delta\n        f_test = f_evaluator(x_test)\n        \n        if f_test  f_current:\n            x_current = x_test\n            f_current = f_test\n    \n    return x_current, f_current\n\ndef hooke_jeeves(x0, delta0, p, c, delta_min, n_max):\n    \"\"\"\n    Implements the Hooke-Jeeves pattern search algorithm.\n    \"\"\"\n    f_evaluator = FunctionEvaluator(rosenbrock_func)\n    \n    x_base = np.array(x0, dtype=float)\n    delta = float(delta0)\n    \n    n_shrinks = 0\n    n_patterns = 0\n    \n    # Initialize with the starting point. This is the first function evaluation.\n    x_best_ever = np.copy(x_base)\n    f_best_ever = f_evaluator(x_best_ever)\n    f_base = f_best_ever\n    \n    while True:\n        # Check termination conditions before starting the iteration\n        if delta  delta_min:\n            break\n        if n_max is not None and f_evaluator.count >= n_max:\n            break\n            \n        x_base_iter_start = np.copy(x_base)\n        \n        # Perform an exploratory move from the current base point\n        x_explore, f_explore = exploratory_move(x_base, f_base, delta, f_evaluator)\n        \n        if f_explore  f_base:\n            # Improvement found after exploration.\n            if f_explore  f_best_ever:\n                f_best_ever = f_explore\n                x_best_ever = np.copy(x_explore)\n\n            # Attempt a pattern move\n            x_pattern_start = x_explore + p * (x_explore - x_base_iter_start)\n            \n            if n_max is not None and f_evaluator.count >= n_max:\n                 x_base, f_base = x_explore, f_explore\n                 break\n\n            f_pattern_start = f_evaluator(x_pattern_start)\n            \n            # Perform a second exploratory move from the pattern point\n            x_tilde, f_tilde = exploratory_move(x_pattern_start, f_pattern_start, delta, f_evaluator)\n\n            # Decide whether to accept the pattern move\n            if f_tilde  f_explore:\n                x_base = np.copy(x_tilde)\n                f_base = f_tilde\n                n_patterns += 1\n            else:\n                x_base = np.copy(x_explore)\n                f_base = f_explore\n                \n            if f_base  f_best_ever:\n                 f_best_ever = f_base\n                 x_best_ever = np.copy(x_base)\n\n        else:\n            # No improvement, so reduce the step size.\n            delta *= c\n            n_shrinks += 1\n            \n    return f_best_ever, x_best_ever[0], x_best_ever[1], f_evaluator.count, n_shrinks, n_patterns\n\ndef solve():\n    \"\"\"\n    Runs the Hooke-Jeeves algorithm for a suite of test cases and prints the results.\n    \"\"\"\n    # Fixed parameters for all runs\n    p = 1.0\n    c = 0.5\n    delta_min = 1e-5\n    n_max = 200000\n\n    # Test suite: (initial_point, initial_delta)\n    test_cases = [\n        # Case 1\n        {'x0': (-1.2, 1.0), 'delta0': 0.5},\n        # Case 2\n        {'x0': (0.0, 0.0), 'delta0': 0.5},\n        # Case 3\n        {'x0': (1.2, 1.2), 'delta0': 0.25},\n        # Case 4\n        {'x0': (1.0, 1.0), 'delta0': 0.5},\n        # Case 5\n        {'x0': (-1.5, 2.0), 'delta0': 1.0},\n        # Case 6\n        {'x0': (0.0, 0.0), 'delta0': 1e-7},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        f_star, x_star, y_star, n_eval, n_shrink, n_pattern = hooke_jeeves(\n            x0=case['x0'],\n            delta0=case['delta0'],\n            p=p,\n            c=c,\n            delta_min=delta_min,\n            n_max=n_max\n        )\n        \n        # A special check for Case 4 (start at optimum). The algorithm as written\n        # may produce -0.0 due to floating point nuances. Correct to 0.0.\n        formatted_result = [\n            round(f_star, 6),\n            round(x_star, 6),\n            round(y_star, 6),\n            int(n_eval),\n            int(n_shrink),\n            int(n_pattern)\n        ]\n        if formatted_result[0] == -0.0: formatted_result[0] = 0.0\n        \n        all_results.append(str(formatted_result))\n\n    # The required output format is a single line, list of lists.\n    # The string representation of a list of lists is needed.\n    # We join the string representation of each inner list.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world optimization problems are rarely unconstrained; they often operate within specific boundaries and limitations. This exercise takes the next logical step by extending the Hooke-Jeeves method to a constrained problem, where the goal is to minimize the Rosenbrock function within a \"banana-shaped\" feasible region. You will implement and compare two of the most common and powerful strategies for handling constraints in direct search: a projection method that forces every step to remain feasible, and a penalty method that discourages constraint violation by modifying the objective function itself . This practice provides invaluable experience in adapting optimization algorithms to respect physical, economic, or logical constraints.",
            "id": "3161458",
            "problem": "Implement a complete, runnable program that compares two constraint-handling strategies for the Hooke-Jeeves (HJ) pattern search method on a constrained minimization problem. The goal is to minimize the Rosenbrock function $$f(x,y) = (1 - x)^2 + 100(y - x^2)^2$$ subject to the curved inequality constraint $$g(x,y) = y - x^2 \\le 0.$$ You must implement two variants of the Hooke-Jeeves method:\n\n- Variant A (Projection-Truncation Feasibility): Enforce feasibility at every accepted move. For any trial point $$p = (x_{\\mathrm{trial}}, y_{\\mathrm{trial}})$$ generated during an exploratory or pattern step from a feasible base point $$b = (x_0,y_0)$$, if $$g(p) \\le 0$$ then accept the trial as-is; otherwise, replace the infeasible trial by the unique intersection point on the line segment from $$b$$ to $$p$$ with the curved boundary $$g(x,y)=0,$$ namely the point $$b + t^\\star (p-b)$$ where $$t^\\star \\in (0,1]$$ satisfies $$y_0 + t^\\star \\,\\mathrm{d}y = \\left(x_0 + t^\\star \\,\\mathrm{d}x\\right)^2,$$ with $$\\mathrm{d}x = x_{\\mathrm{trial}} - x_0$$ and $$\\mathrm{d}y = y_{\\mathrm{trial}} - y_0.$$ You must determine $$t^\\star$$ by solving the quadratic equation obtained by substituting the line parameterization into $$y - x^2 = 0,$$ namely\n$$\n(dx)^2 \\, t^2 + \\left(2 x_0 dx - dy \\right)\\, t + \\left(x_0^2 - y_0\\right) = 0,\n$$\nand selecting the root in $$[0,1]$$ that lies on the first intersection along the line from $$b$$ to $$p$$. If $$dx=0$$ makes the equation linear, solve the linear equation. Ensure numerical robustness by handling small coefficients. For the initial point, if it is infeasible, project it to the feasible set by clipping along the vertical: replace $$y$$ by $$\\min(y, x^2)$$ to obtain a feasible starting base.\n\n- Variant B (Quadratic Penalty): Allow infeasible trial points but evaluate the penalized objective\n$$\nF_\\rho(x,y) = f(x,y) + \\rho \\, \\max\\{0, g(x,y)\\}^2\n$$\nand accept a trial point if and only if it decreases $$F_\\rho$$. No projection is applied during the search in this variant.\n\nBoth variants must use the Hooke-Jeeves pattern search framework in $$\\mathbb{R}^2$$: starting from a base point with an initial step size $$\\Delta > 0,$$ perform coordinate-wise exploratory moves in the directions $$\\pm \\Delta \\, e_1$$ and $$\\pm \\Delta \\, e_2$$ (where $$e_1=(1,0)$$ and $$e_2=(0,1)$$), accepting the first improving move encountered for the relevant objective (Variant A uses $$f$$; Variant B uses $$F_\\rho$$). After a successful exploratory phase that improves the base to a new point $$x_1$$ relative to the previous base $$x_0,$$ attempt a pattern move $$x_p = x_1 + (x_1 - x_0).$$ In Variant A, enforce feasibility by projection-truncation on the segment from $$x_1$$ to $$x_p$$ prior to evaluation; in Variant B, evaluate $$F_\\rho$$ at $$x_p$$ directly. If the pattern move yields further improvement, accept it as the new base; otherwise, keep the exploratory improvement. If no improvement is found during exploratory moves, reduce the step size by a factor of $$\\tfrac{1}{2}$$. Terminate when $$\\Delta  \\Delta_{\\min}$$ or after a prescribed maximum number of iterations.\n\nFundamental base and principles you must rely on:\n- Direct search methods evaluate the objective without derivatives, using exploratory moves to locate descent directions and pattern moves to exploit discovered descent trends.\n- Feasibility under $$g(x,y) \\le 0$$ is equivalent to $$y \\le x^2,$$ and the boundary is the curve $$y = x^2.$$\n- Projection-truncation onto the boundary along the current trial segment amounts to solving the one-dimensional intersection condition given above.\n- Quadratic penalty methods replace the constrained problem by an unconstrained problem that augments the objective with a term proportional to the squared violation of the constraint.\n\nAlgorithmic specifications you must implement:\n- Use strict decrease for acceptance in each variant with a small numerical tolerance.\n- Exploratory moves are performed sequentially per coordinate, accepting the first improving move and then proceeding to the next coordinate from the updated point.\n- On a successful exploratory improvement $$x_1 \\ne x_0,$$ attempt a single pattern move $$x_p = x_1 + (x_1 - x_0).$$\n- On failure to improve, set $$\\Delta \\leftarrow \\tfrac{1}{2}\\Delta.$$\n- Termination: stop when $$\\Delta  \\Delta_{\\min}$$ or when the iteration counter reaches the specified maximum.\n\nTest suite and outputs:\nImplement the program to run the following four test cases, each specified by the initial point, initial step size, penalty parameter, minimum step size, and maximum iterations:\n- Case $$1$$: start $$(-1.2, 1.0)$$, $$\\Delta_0 = 0.5$$, $$\\rho = 10.0$$, $$\\Delta_{\\min} = 10^{-5}$$, max iterations $$= 10000$$.\n- Case $$2$$: start $$(0.0, 0.0)$$, $$\\Delta_0 = 0.25$$, $$\\rho = 10.0$$, $$\\Delta_{\\min} = 10^{-5}$$, max iterations $$= 10000$$.\n- Case $$3$$: start $$(0.5, 0.5)$$, $$\\Delta_0 = 0.5$$, $$\\rho = 100.0$$, $$\\Delta_{\\min} = 10^{-5}$$, max iterations $$= 10000$$.\n- Case $$4$$: start $$(2.0, 3.0)$$, $$\\Delta_0 = 1.0$$, $$\\rho = 1.0$$, $$\\Delta_{\\min} = 10^{-5}$$, max iterations $$= 10000$$.\n\nFor each case, run both variants with the same parameters. For Variant A, report the final value of the original objective $$f(x,y)$$ at the returned (feasible) point. For Variant B, report the final value of the penalized objective $$F_\\rho(x,y)$$ at the returned point. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following order:\n$$[\\, f_A^{(1)},\\, F_{B,\\rho}^{(1)},\\, f_A^{(2)},\\, F_{B,\\rho}^{(2)},\\, f_A^{(3)},\\, F_{B,\\rho}^{(3)},\\, f_A^{(4)},\\, F_{B,\\rho}^{(4)} \\,].$$\n\nThere are no physical quantities with units in this problem. All numerical outputs must be real numbers in standard decimal notation. The code must be fully self-contained, take no input, and print exactly one line in the specified format. The implementation must be in a modern programming language and must adhere to the final answer code environment constraints described later.",
            "solution": "The user has provided a well-defined computational problem in the domain of numerical optimization. A critical validation confirms that the problem statement is self-contained, scientifically grounded in the principles of direct search methods and constraint handling, and free of contradictions or ambiguities. The problem is deemed valid.\n\nThe task is to implement the Hooke-Jeeves pattern search algorithm to minimize the two-dimensional Rosenbrock function, $$f(x,y) = (1 - x)^2 + 100(y - x^2)^2$$, subject to the inequality constraint $$g(x,y) = y - x^2 \\le 0$$. This is to be done using two distinct constraint-handling strategies: Projection-Truncation (Variant A) and Quadratic Penalty (Variant B).\n\nThe solution is structured as a Python program that implements the core algorithm and then applies it to four specified test cases.\n\n### Mathematical and Algorithmic Formulation\n\n#### 1. Objective and Constraint Functions\nThe unconstrained objective function is the Rosenbrock function, a standard benchmark for optimization algorithms known for its narrow, parabolic valley.\n$$\nf(x,y) = (1 - x)^2 + 100(y - x^2)^2\n$$\nThe feasible region is defined by the inequality constraint $$g(x,y) = y - x^2 \\le 0$$, which describes the area on and below the parabola $$y = x^2$$. The global minimum of the unconstrained Rosenbrock function is at $$(1,1)$$, with $$f(1,1)=0$$. This point is infeasible since $$g(1,1) = 1 - 1^2 = 0$$. It lies on the boundary, so it is the constrained minimum as well.\n\n#### 2. Hooke-Jeeves Pattern Search Framework\nThe Hooke-Jeeves method is an iterative, derivative-free optimization algorithm. Each iteration consists of two main phases: an exploratory move and a pattern move.\n\n- **Initialization**: The algorithm starts with a base point $$x_{\\mathrm{base}}$$, an initial step size $$\\Delta$$, a step size reduction factor (here, $$\\frac{1}{2}$$), a minimum step size $$\\Delta_{\\min}$$, and a maximum number of iterations.\n\n- **Exploratory Move**: Starting from the current base point, the algorithm probes the local neighborhood. It sequentially perturbs each coordinate by $$\\pm \\Delta$$. For a point in $$\\mathbb{R}^2$$, the search directions are $$\\pm \\Delta e_1$$ and $$\\pm \\Delta e_2$$, where $$e_1=(1,0)$$ and $$e_2=(0,1)$$. As specified, the first move that yields a strict decrease in the objective function is accepted, and the search for the next coordinate proceeds from this newly updated point. If no move along any coordinate direction provides improvement, the exploratory phase fails.\n\n- **Pattern Move**: If the exploratory phase is successful, a new point $$x_{\\mathrm{new}}$$ is found that is better than the previous base point $$x_{\\mathrm{old}}$$. This suggests a promising direction of descent, $$d = x_{\\mathrm{new}} - x_{\\mathrm{old}}$$. The algorithm attempts to accelerate the search by making a \"pattern move\" along this direction:\n$$\nx_p = x_{\\mathrm{new}} + d = x_{\\mathrm{new}} + (x_{\\mathrm{new}} - x_{\\mathrm{old}})\n$$\nIf the objective value at $$x_p$$ is an improvement over the value at $$x_{\\mathrm{new}}$$, $$x_p$$ becomes the new base point for the next iteration. Otherwise, the algorithm retreats to $$x_{\\mathrm{new}}$$, which becomes the new base.\n\n- **Step Size Reduction**: If an exploratory phase fails to find any improvement, the step size $$\\Delta$$ is reduced (e.g., halved), and the algorithm proceeds to the next iteration from the same base point. This allows for a finer-grained search as the algorithm approaches a minimum.\n\n- **Termination**: The algorithm terminates when the step size $$\\Delta$$ falls below a minimum threshold $$\\Delta_{\\min}$$, or a maximum number of iterations is reached.\n\n#### 3. Constraint-Handling Strategies\n\n**Variant A: Projection-Truncation Feasibility**\nThis variant enforces feasibility at every step. The objective function being minimized is the original $$f(x,y)$$.\n\n-   **Initial Point**: If the user-provided starting point is infeasible, it is projected onto the feasible set by setting its $$y$$-coordinate to $$\\min(y, x^2)$$.\n-   **Move Acceptance**: Whenever a trial point $$p$$ (from either an exploratory or pattern move) is generated from a current feasible point $$b$$, its feasibility is checked.\n    -   If $$g(p) \\le 0$$, the point is feasible and evaluated as is.\n    -   If $$g(p) > 0$$, the point is infeasible. It must be replaced by its projection onto the boundary $$y=x^2$$ along the line segment connecting $$b$$ to $$p$$. This projected point is $$b + t^\\star (p-b)$$, where $$t^\\star \\in [0,1]$$ is found by solving for the intersection of the parameterized line and the parabola. Substituting the line parameterization $$x(t) = x_0 + t\\,\\mathrm{d}x$$, $$y(t) = y_0 + t\\,\\mathrm{d}y$$ into the boundary equation $$y=x^2$$ yields the specified quadratic equation for $$t$$:\n    $$\n    (\\mathrm{d}x)^2 t^2 + (2 x_0 \\mathrm{d}x - \\mathrm{d}y) t + (x_0^2 - y_0) = 0\n    $$\n    where $$(\\mathrm{d}x, \\mathrm{d}y) = p - b$$. Since $$b$$ is feasible ($$x_0^2-y_0 \\ge 0$$) and $$p$$ is infeasible, a unique root $$t^\\star \\in [0,1)$$ is guaranteed to exist. This root is found using the standard quadratic formula, with special handling for the linear case where $$\\mathrm{d}x \\approx 0$$.\n\n**Variant B: Quadratic Penalty Method**\nThis variant transforms the constrained problem into an unconstrained one by adding a penalty term to the objective function. This allows the search to explore infeasible regions.\n\n-   **Penalized Objective**: The algorithm minimizes a penalized objective function $$F_\\rho(x,y)$$:\n    $$\n    F_\\rho(x,y) = f(x,y) + \\rho \\cdot \\max\\{0, g(x,y)\\}^2\n    $$\n    Here, $$\\rho > 0$$ is the penalty parameter. The penalty term is zero for feasible points ($$g(x,y) \\le 0$$) and grows quadratically with the magnitude of the constraint violation for infeasible points.\n-   **Move Acceptance**: Trial points are accepted or rejected based purely on whether they decrease the value of $$F_\\rho$$. No projection is performed. The final returned point may be slightly infeasible, with the degree of infeasibility controlled by the magnitude of $$\\rho$$.\n\nThe implementation will consist of helper functions for the objective and constraint, a robust function for solving the projection step in Variant A, and a main `hooke_jeeves_solver` function that encapsulates the core logic for both variants. A `solve` function will orchestrate the execution of the four specified test cases and format the final output.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to run the Hooke-Jeeves comparison for all test cases\n    and print the formatted results.\n    \"\"\"\n\n    # Define a small tolerance for floating-point comparisons in acceptance criteria.\n    TOL = 1e-9\n\n    def f_rosenbrock(p):\n        \"\"\"Rosenbrock function f(x,y) = (1-x)^2 + 100(y-x^2)^2.\"\"\"\n        x, y = p\n        return (1.0 - x)**2 + 100.0 * (y - x**2)**2\n\n    def g_constraint(p):\n        \"\"\"Constraint function g(x,y) = y - x^2. Feasible if = 0.\"\"\"\n        x, y = p\n        return y - x**2\n\n    def F_penalized(p, rho):\n        \"\"\"Penalized objective function F_rho(x,y).\"\"\"\n        return f_rosenbrock(p) + rho * max(0.0, g_constraint(p))**2\n\n    def solve_projection_t(base, trial):\n        \"\"\"\n        Solves for the parameter t in [0, 1] for the intersection of the\n        line segment from a feasible 'base' to an infeasible 'trial' point\n        with the boundary g(x,y) = 0.\n        \"\"\"\n        x0, y0 = base\n        xt, yt = trial\n        \n        dx = xt - x0\n        dy = yt - y0\n        \n        # We solve At^2 + Bt + C = 0 for t.\n        # A = (dx)^2, B = 2*x0*dx - dy, C = x0^2 - y0\n        a = dx**2\n        b = 2.0 * x0 * dx - dy\n        c = x0**2 - y0\n        \n        # Since base is feasible (y0 = x0^2) and trial is infeasible (yt > xt^2),\n        # quadratic at t=0 is C >= 0 and at t=1 is (xt^2 - yt)  0.\n        # This guarantees a unique root in [0, 1).\n        \n        if abs(a)  TOL:  # Linear equation: Bt + C = 0\n            if abs(b)  TOL:\n                # This case is unreachable if base is feasible and trial is infeasible.\n                return 0.0\n            t = -c / b\n        else:  # Quadratic equation\n            discriminant = b**2 - 4.0 * a * c\n            if discriminant  0.0: # Clamp due to potential floating point errors\n                discriminant = 0.0\n            \n            sqrt_d = math.sqrt(discriminant)\n            \n            # Standard quadratic formula roots\n            t1 = (-b + sqrt_d) / (2.0 * a)\n            t2 = (-b - sqrt_d) / (2.0 * a)\n            \n            # Select the unique root within the [0, 1] interval.\n            t1_valid = (0.0 - TOL) = t1 = (1.0 + TOL)\n            t2_valid = (0.0 - TOL) = t2 = (1.0 + TOL)\n\n            if t1_valid and t2_valid:\n                t = min(t1, t2) # First intersection\n            elif t1_valid:\n                t = t1\n            else:\n                t = t2\n                \n        # Clamp to [0, 1] for robustness against floating-point inaccuracies.\n        return max(0.0, min(1.0, t))\n\n    def project(base, trial):\n        \"\"\"Projects an infeasible trial point back onto the feasible set boundary.\"\"\"\n        t_star = solve_projection_t(base, trial)\n        return base + t_star * (trial - base)\n\n    def hooke_jeeves_solver(start_point, delta0, delta_min, max_iter, variant, rho=None):\n        \"\"\"\n        Implementation of the Hooke-Jeeves pattern search method with two\n        constraint-handling variants.\n        \"\"\"\n        x_base = np.array(start_point, dtype=float)\n        delta = float(delta0)\n        \n        if variant == 'A':\n            objective_func = f_rosenbrock\n            # Ensure initial point is feasible for Variant A\n            if g_constraint(x_base) > 0:\n                x_base[1] = min(x_base[1], x_base[0]**2)\n        elif variant == 'B':\n            def objective_func(p): return F_penalized(p, rho)\n        else:\n            raise ValueError(\"Unknown variant specified.\")\n\n        for _ in range(max_iter):\n            if delta  delta_min:\n                break\n                \n            x_iter_start_base = x_base.copy()\n            f_iter_start_base = objective_func(x_iter_start_base)\n\n            # --- 1. Exploratory Moves ---\n            x_explore = x_iter_start_base.copy()\n            \n            for i in range(len(x_explore)):\n                f_before_coord_move = objective_func(x_explore)\n                \n                # Try positive step\n                p_plus_trial = x_explore.copy()\n                p_plus_trial[i] += delta\n                \n                p_plus_final = p_plus_trial\n                if variant == 'A' and g_constraint(p_plus_trial) > 0:\n                    p_plus_final = project(x_explore, p_plus_trial)\n                \n                f_plus = objective_func(p_plus_final)\n                \n                if f_plus  f_before_coord_move - TOL:\n                    x_explore = p_plus_final\n                    continue  # Accepted, on to next dimension\n\n                # Try negative step\n                p_minus_trial = x_explore.copy()\n                p_minus_trial[i] -= delta\n                \n                p_minus_final = p_minus_trial\n                if variant == 'A' and g_constraint(p_minus_trial) > 0:\n                    p_minus_final = project(x_explore, p_minus_trial)\n                \n                f_minus = objective_func(p_minus_final)\n\n                if f_minus  f_before_coord_move - TOL:\n                    x_explore = p_minus_final\n            \n            f_explore = objective_func(x_explore)\n            \n            if f_explore  f_iter_start_base - TOL:  # Improvement found\n                # --- 2. Pattern Move ---\n                pattern_dir = x_explore - x_iter_start_base\n                p_pattern_trial = x_explore + pattern_dir\n                \n                p_pattern_final = p_pattern_trial\n                if variant == 'A' and g_constraint(p_pattern_trial) > 0:\n                    p_pattern_final = project(x_explore, p_pattern_trial)\n\n                f_pattern = objective_func(p_pattern_final)\n                \n                if f_pattern  f_explore - TOL:\n                    x_base = p_pattern_final\n                else:\n                    x_base = x_explore\n            else:  # No improvement from exploratory moves\n                delta /= 2.0\n        \n        # Return the final objective value as required by the problem.\n        return objective_func(x_base)\n\n    test_cases = [\n        {'start': (-1.2, 1.0), 'delta0': 0.5, 'rho': 10.0, 'delta_min': 1e-5, 'max_iter': 10000},\n        {'start': (0.0, 0.0), 'delta0': 0.25, 'rho': 10.0, 'delta_min': 1e-5, 'max_iter': 10000},\n        {'start': (0.5, 0.5), 'delta0': 0.5, 'rho': 100.0, 'delta_min': 1e-5, 'max_iter': 10000},\n        {'start': (2.0, 3.0), 'delta0': 1.0, 'rho': 1.0, 'delta_min': 1e-5, 'max_iter': 10000}\n    ]\n\n    results = []\n    for case in test_cases:\n        # Run Variant A (Projection-Truncation)\n        f_A = hooke_jeeves_solver(\n            start_point=case['start'],\n            delta0=case['delta0'],\n            delta_min=case['delta_min'],\n            max_iter=case['max_iter'],\n            variant='A'\n        )\n        results.append(f_A)\n\n        # Run Variant B (Quadratic Penalty)\n        F_B = hooke_jeeves_solver(\n            start_point=case['start'],\n            delta0=case['delta0'],\n            delta_min=case['delta_min'],\n            max_iter=case['max_iter'],\n            variant='B',\n            rho=case['rho']\n        )\n        results.append(F_B)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Pattern search methods are not just classical algorithms; they are highly relevant for tackling challenges in modern data science and machine learning. This final practice demonstrates their power by applying the Hooke-Jeeves algorithm to a composite objective function that combines a least-squares term $\\left\\|A\\mathbf{x} - \\mathbf{b}\\right\\|_2$ with an $\\ell_1$-norm regularizer $\\tau \\left\\|\\mathbf{x}\\right\\|_1$. Such non-differentiable objectives are fundamental to promoting sparsity in solutions, a key concept in fields from signal processing to statistics. This exercise showcases the unique advantage of derivative-free methods in solving complex, non-smooth problems where traditional gradient-based techniques fail .",
            "id": "3161536",
            "problem": "Implement a derivative-free pattern search using the classical Hooke-Jeeves (HJ) method to minimize a composite objective that combines a least-squares data misfit and a sparsity-promoting regularizer. You must construct an objective function of the form\n$$\nf(\\mathbf{x}) \\;=\\; \\left\\|A\\mathbf{x} - \\mathbf{b}\\right\\|_2 \\;+\\; \\tau \\left\\|\\mathbf{x}\\right\\|_1,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $\\mathbf{b} \\in \\mathbb{R}^m$, $\\mathbf{x} \\in \\mathbb{R}^n$, the symbol $\\left\\| \\cdot \\right\\|_2$ denotes the Euclidean norm, and the symbol $\\left\\| \\cdot \\right\\|_1$ denotes the sum of absolute values. The objective is convex but is generally non-differentiable because of the $\\ell_1$ term, which motivates using a direct-search algorithm.\n\nStart from the following foundational base:\n- Unconstrained minimization is the task of finding $\\mathbf{x}^\\star \\in \\mathbb{R}^n$ that minimizes a real-valued function $f:\\mathbb{R}^n \\to \\mathbb{R}$.\n- The Euclidean norm is defined by $\\left\\|\\mathbf{y}\\right\\|_2 = \\sqrt{\\sum_{i=1}^m y_i^2}$ for any $\\mathbf{y}\\in\\mathbb{R}^m$.\n- The $\\ell_1$ norm is defined by $\\left\\|\\mathbf{x}\\right\\|_1 = \\sum_{i=1}^n |x_i|$.\n- Direct-search pattern methods use only function evaluations and a step-size control to navigate the search space.\n\nImplement the classical Hooke-Jeeves (HJ) method with the following structure:\n- Exploratory move: Given a base point $\\mathbf{x}_B$ and a step size $\\Delta0$, probe coordinate directions $\\pm \\Delta \\mathbf{e}_i$ sequentially for $i \\in \\{1,\\dots,n\\}$, where $\\mathbf{e}_i$ is the $i$-th standard basis vector in $\\mathbb{R}^n$. Accept a trial point if and only if it strictly decreases the objective value.\n- Pattern move: If the exploratory move from $\\mathbf{x}_B$ finds an improved point $\\mathbf{x}_E$ (i.e., $f(\\mathbf{x}_E)  f(\\mathbf{x}_B)$), a pattern move is attempted. A pattern point $\\mathbf{x}_P = \\mathbf{x}_E + (\\mathbf{x}_E - \\mathbf{x}_B)$ is formed, and a new exploratory move is performed starting from $\\mathbf{x}_P$. Let the result of this second exploration be $\\tilde{\\mathbf{x}}$. If $f(\\tilde{\\mathbf{x}})  f(\\mathbf{x}_E)$, the new base for the next iteration is set to $\\tilde{\\mathbf{x}}$. Otherwise, it is set to $\\mathbf{x}_E$.\n- Step-size reduction: If an exploratory move from $\\mathbf{x}_B$ yields no improvement, reduce the step size via $\\Delta \\leftarrow \\alpha \\Delta$ with $0\\alpha1$.\n- Termination: Stop when $\\Delta  \\text{tol}$ or when the iteration count reaches a preset maximum.\n\nYour program must implement the method above from first principles and evaluate it on the following test suite. For each case, return the minimized objective value $f^\\star$ achieved upon termination as a floating-point number.\n\nTest suite:\n- Case $1$ (smooth baseline, square well-conditioned system):\n  - $A = \\begin{bmatrix} 3  0 \\\\ 0  1 \\end{bmatrix}$, $\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, $\\tau = 0$.\n  - Initial point $\\mathbf{x}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n  - Initial step size $\\Delta_0 = 1.0$, reduction factor $\\alpha = 0.5$, tolerance $\\text{tol} = 10^{-6}$, maximum iterations $10000$.\n- Case $2$ (nonsmooth composite, square system):\n  - $A = \\begin{bmatrix} 2  -1  0 \\\\ 0  1  2 \\\\ 1  0  1 \\end{bmatrix}$, $\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\end{bmatrix}$, $\\tau = 0.1$.\n  - Initial point $\\mathbf{x}_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n  - $\\Delta_0 = 1.0$, $\\alpha = 0.5$, $\\text{tol} = 10^{-6}$, maximum iterations $10000$.\n- Case $3$ (nonsmooth composite, underdetermined system to encourage sparsity):\n  - $A = \\begin{bmatrix} 1  2  3 \\\\ 0  1  1 \\end{bmatrix}$, $\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $\\tau = 0.5$.\n  - Initial point $\\mathbf{x}_0 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}$.\n  - $\\Delta_0 = 0.8$, $\\alpha = 0.5$, $\\text{tol} = 10^{-6}$, maximum iterations $10000$.\n- Case $4$ (degenerate data term, pure $\\ell_1$ trade-off against a constant):\n  - $A = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}$, $\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}$, $\\tau = 1.0$.\n  - Initial point $\\mathbf{x}_0 = \\begin{bmatrix} 5 \\\\ -5 \\end{bmatrix}$.\n  - $\\Delta_0 = 1.0$, $\\alpha = 0.5$, $\\text{tol} = 10^{-6}$, maximum iterations $10000$.\n\nFinal output specification:\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, the values of $f^\\star$ for cases $1$ through $4$. Each value must be a floating-point number. No units are involved in this task. Do not print any additional text or lines.",
            "solution": "The problem is valid. It presents a well-defined task in the field of numerical optimization: to implement the Hooke-Jeeves (HJ) pattern search algorithm for minimizing a convex, non-differentiable composite objective function and to evaluate its performance on a specified suite of test cases. All necessary parameters, initial conditions, and termination criteria are provided, and the problem is scientifically and mathematically sound.\n\nThe objective function to be minimized is given by:\n$$\nf(\\mathbf{x}) \\;=\\; \\left\\|A\\mathbf{x} - \\mathbf{b}\\right\\|_2 \\;+\\; \\tau \\left\\|\\mathbf{x}\\right\\|_1\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is a matrix, $\\mathbf{x} \\in \\mathbb{R}^n$ is the vector of variables, $\\mathbf{b} \\in \\mathbb{R}^m$ is a vector, and $\\tau \\ge 0$ is a scalar regularization parameter. The function consists of two terms:\n$1$. The data misfit term, $\\left\\|A\\mathbf{x} - \\mathbf{b}\\right\\|_2$, which is the Euclidean ($\\ell_2$) norm of the residual. This term promotes solutions that satisfy the linear system $A\\mathbf{x} \\approx \\mathbf{b}$ in a least-squares sense.\n$2$. The regularization term, $\\tau \\left\\|\\mathbf{x}\\right\\|_1$, which is the weighted $\\ell_1$ norm of the solution vector $\\mathbf{x}$. This term promotes sparsity, meaning it encourages components of $\\mathbf{x}$ to be exactly zero.\n\nThe presence of the $\\ell_1$ norm term, $\\left\\|\\mathbf{x}\\right\\|_1 = \\sum_{i=1}^n |x_i|$, makes the objective function $f(\\mathbf{x})$ non-differentiable at any point $\\mathbf{x}$ where one or more components $x_i$ are zero. This precludes the use of gradient-based optimization methods and motivates the use of a derivative-free method like the Hooke-Jeeves algorithm.\n\nThe Hooke-Jeeves method is a direct search algorithm that iteratively explores the search space using two types of moves: an exploratory move and a pattern move.\n\n**Algorithm Implementation**\n\nThe implementation will follow the classical structure of the Hooke-Jeeves method. Let $\\mathbf{x}_B$ be the base point for the current iteration and $\\Delta$ be the current step size.\n\n**$1$. Exploratory Move**\nThe exploratory move probes the vicinity of a given point along the coordinate axes. Given a point $\\mathbf{x}_{c}$ and a step size $\\Delta$, the procedure is as follows:\n- Initialize a temporary best point $\\mathbf{x}_{new} \\leftarrow \\mathbf{x}_{c}$.\n- For each coordinate direction $i = 1, \\dots, n$:\n  - Evaluate the function at $\\mathbf{x}_{trial} = \\mathbf{x}_{new} + \\Delta \\mathbf{e}_i$, where $\\mathbf{e}_i$ is the $i$-th standard basis vector. If $f(\\mathbf{x}_{trial})  f(\\mathbf{x}_{new})$, accept the move: $\\mathbf{x}_{new} \\leftarrow \\mathbf{x}_{trial}$.\n  - Otherwise, evaluate at $\\mathbf{x}_{trial} = \\mathbf{x}_{new} - \\Delta \\mathbf{e}_i$. If $f(\\mathbf{x}_{trial})  f(\\mathbf{x}_{new})$, accept the move: $\\mathbf{x}_{new} \\leftarrow \\mathbf{x}_{trial}$.\n- The final point $\\mathbf{x}_{new}$ is returned.\n\n**$2$. Main HJ Algorithm**\nThe main loop orchestrates the exploratory and pattern moves to navigate the search space.\n\n- **Initialization**:\n  - Set the initial base point $\\mathbf{x}_B \\leftarrow \\mathbf{x}_0$.\n  - Set the previous base point $\\mathbf{x}_{B, \\text{prev}} \\leftarrow \\mathbf{x}_B$.\n  - Set the initial step size $\\Delta \\leftarrow \\Delta_0$.\n  - Initialize an iteration counter $k \\leftarrow 0$.\n\n- **Iteration Loop**: The algorithm proceeds until the step size $\\Delta$ falls below a tolerance $\\text{tol}$ or the number of iterations $k$ exceeds a maximum value $k_{\\text{max}}$.\n\n  - **a. Pattern Move and Exploration**: A temporary pattern point $\\mathbf{x}_P$ is defined based on the direction of improvement from the previous iteration:\n    $$\n    \\mathbf{x}_P = \\mathbf{x}_B + (\\mathbf{x}_B - \\mathbf{x}_{B, \\text{prev}})\n    $$\n    An exploratory move is then performed starting from this pattern point to find a new candidate point $\\mathbf{x}_E = \\text{Explore}(\\mathbf{x}_P, \\Delta)$.\n\n  - **b. Acceptance of Pattern Move**: The new point $\\mathbf{x}_E$ is compared to the current base point $\\mathbf{x}_B$.\n    - If $f(\\mathbf{x}_E)  f(\\mathbf{x}_B)$, the pattern move was successful. The points are updated for the next iteration:\n      $$\n      \\mathbf{x}_{B, \\text{prev}} \\leftarrow \\mathbf{x}_B\n      $$\n      $$\n      \\mathbf{x}_B \\leftarrow \\mathbf{x}_E\n      $$\n    - If $f(\\mathbf{x}_E) \\ge f(\\mathbf{x}_B)$, the pattern move failed to produce a better point. The algorithm discards the result of the pattern move and tries a simpler move.\n\n  - **c. Refinement (if Pattern Move Failed)**: An exploratory move is performed from the current base point $\\mathbf{x}_B$:\n    $$\n    \\mathbf{x}_E = \\text{Explore}(\\mathbf{x}_B, \\Delta)\n    $$\n    - If This refined search is successful, i.e., $f(\\mathbf{x}_E)  f(\\mathbf{x}_B)$, the points are updated:\n      $$\n      \\mathbf{x}_{B, \\text{prev}} \\leftarrow \\mathbf{x}_B\n      $$\n      $$\n      \\mathbf{x}_B \\leftarrow \\mathbf{x}_E\n      $$\n    - If this second exploratory move also fails, the algorithm has stalled at the current step size.\n\n  - **d. Step Size Reduction**: If both the pattern move and the subsequent refinement exploratory move fail to improve the objective function value, the step size is reduced:\n    $$\n    \\Delta \\leftarrow \\alpha \\Delta\n    $$\n    where $0  \\alpha  1$ is the reduction factor. The base points are reset to prevent an overly aggressive pattern move in the next iteration:\n    $$\n    \\mathbf{x}_{B, \\text{prev}} \\leftarrow \\mathbf{x}_B\n    $$\n\n- **Termination**: The loop terminates when $\\Delta  \\text{tol}$ or $k \\ge k_{\\text{max}}$. The final solution is the last calculated base point $\\mathbf{x}_B$, and the minimized objective value is $f(\\mathbf{x}_B)$. This logic ensures a systematic search that contracts around a local minimum.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Hooke-Jeeves algorithm to solve the given test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"A\": np.array([[3, 0], [0, 1]]),\n            \"b\": np.array([1, 2]),\n            \"tau\": 0.0,\n            \"x0\": np.array([0, 0]),\n            \"delta0\": 1.0,\n            \"alpha\": 0.5,\n            \"tol\": 1e-6,\n            \"max_iter\": 10000,\n        },\n        {\n            \"A\": np.array([[2, -1, 0], [0, 1, 2], [1, 0, 1]]),\n            \"b\": np.array([1, 2, 0]),\n            \"tau\": 0.1,\n            \"x0\": np.array([0, 0, 0]),\n            \"delta0\": 1.0,\n            \"alpha\": 0.5,\n            \"tol\": 1e-6,\n            \"max_iter\": 10000,\n        },\n        {\n            \"A\": np.array([[1, 2, 3], [0, 1, 1]]),\n            \"b\": np.array([1, 0]),\n            \"tau\": 0.5,\n            \"x0\": np.array([1, -1, 2]),\n            \"delta0\": 0.8,\n            \"alpha\": 0.5,\n            \"tol\": 1e-6,\n            \"max_iter\": 10000,\n        },\n        {\n            \"A\": np.array([[0, 0], [0, 0]]),\n            \"b\": np.array([3, 4]),\n            \"tau\": 1.0,\n            \"x0\": np.array([5, -5]),\n            \"delta0\": 1.0,\n            \"alpha\": 0.5,\n            \"tol\": 1e-6,\n            \"max_iter\": 10000,\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        A, b, tau = case[\"A\"], case[\"b\"], case[\"tau\"]\n        x0, delta, alpha, tol, max_iter = case[\"x0\"], case[\"delta0\"], case[\"alpha\"], case[\"tol\"], case[\"max_iter\"]\n        \n        def objective_function(x):\n            \"\"\"Calculates the composite objective function value.\"\"\"\n            l2_term = np.linalg.norm(A @ x - b)\n            l1_term = tau * np.linalg.norm(x, 1)\n            return l2_term + l1_term\n\n        def exploratory_move(x_c, delta_val):\n            \"\"\"Performs an exploratory move from a given point.\"\"\"\n            x_new = x_c.copy()\n            f_new = objective_function(x_new)\n            n_dims = len(x_c)\n\n            for i in range(n_dims):\n                # Probe in the positive direction\n                x_trial = x_new.copy()\n                x_trial[i] += delta_val\n                f_trial = objective_function(x_trial)\n                if f_trial  f_new:\n                    x_new = x_trial\n                    f_new = f_trial\n                    continue\n                \n                # Probe in the negative direction\n                x_trial = x_new.copy()\n                x_trial[i] -= delta_val\n                f_trial = objective_function(x_trial)\n                if f_trial  f_new:\n                    x_new = x_trial\n                    f_new = f_trial\n            return x_new\n\n        # Hooke-Jeeves main algorithm\n        x_base = x0.astype(float)\n        x_prev_base = x0.astype(float)\n        \n        for k in range(max_iter):\n            if delta  tol:\n                break\n            \n            # 1. Start with an exploratory move from the current base.\n            x_explored = exploratory_move(x_base, delta)\n            \n            if objective_function(x_explored)  objective_function(x_base):\n                # Exploration was successful. Set this as the new base and attempt a pattern move.\n                x_prev_base = x_base\n                x_base = x_explored\n                \n                # 2. Pattern move\n                x_pattern = x_base + (x_base - x_prev_base)\n                x_pattern_explored = exploratory_move(x_pattern, delta)\n                \n                if objective_function(x_pattern_explored)  objective_function(x_base):\n                    # Pattern move was successful.\n                    x_base = x_pattern_explored # Update base again for NEXT iteration.\n        else:\n            # Exploration from base failed, reduce step size.\n            delta *= alpha\n        \n        final_f_val = objective_function(x_base)\n        results.append(final_f_val)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}