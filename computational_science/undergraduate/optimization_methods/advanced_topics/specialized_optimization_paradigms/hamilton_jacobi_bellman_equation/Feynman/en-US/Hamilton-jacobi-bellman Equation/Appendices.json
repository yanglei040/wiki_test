{
    "hands_on_practices": [
        {
            "introduction": "The heart of solving any Hamilton-Jacobi-Bellman (HJB) equation lies in understanding and working with the Hamiltonian. This function encapsulates the system dynamics and running costs, and its minimization with respect to the control variable yields the optimal policy. This first exercise  provides direct practice in this fundamental step for a standard linear-quadratic problem, allowing you to derive the optimal feedback control and the minimized Hamiltonian as functions of the state and the value function's derivatives.",
            "id": "3080765",
            "problem": "Consider the one-dimensional controlled It么 stochastic differential equation\n$$\n\\mathrm{d}X_{t} = b(X_{t},u_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t},\n$$\nwith drift $b(x,u) = u$, constant diffusion coefficient $\\sigma  0$, and control process $u_{t}$ taking values in $\\mathbb{R}$. Let the running cost be\n$$\nf(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2},\n$$\nwith $r0$ and $q0$. Define the value function $V(t,x)$ for a finite horizon problem with a bounded and continuous terminal cost $g(x)$ by\n$$\nV(t,x) = \\inf_{u} \\,\\mathbb{E}\\!\\left[ \\int_{t}^{T} f(X_{s},u_{s})\\,\\mathrm{d}s + g(X_{T}) \\,\\middle|\\, X_{t}=x \\right],\n$$\nwhere the infimum is over progressively measurable controls taking values in $\\mathbb{R}$. Using the dynamic programming principle and the definition of the Hamiltonian appearing in the Hamilton-Jacobi-Bellman (HJB) equation, the Hamiltonian at a point $(x,p,M)$ is given by the infimum over controls $u \\in \\mathbb{R}$ of the sum of the controlled generator applied to a $C^{2}$ test function and the running cost. In particular, if $p$ denotes the first spatial derivative and $M$ denotes the second spatial derivative, compute:\n- the minimized Hamiltonian as an explicit function of $x$, $p$, $M$, $\\sigma$, $r$, and $q$;\n- the corresponding minimizing state-feedback control as an explicit function of $p$ and $r$.\n\nProvide your final answer as a two-entry row matrix, with the first entry being the minimized Hamiltonian and the second entry being the minimizing feedback control. No numerical approximation is required, and no units are involved.",
            "solution": "The problem asks for the minimized Hamiltonian and the corresponding optimal control for a given stochastic control problem. The Hamiltonian, as defined in the context of the Hamilton-Jacobi-Bellman (HJB) equation, is given by:\n$$\nH(x,p,M) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\mathcal{L}^{u} + f(x,u) \\right\\}\n$$\nwhere $\\mathcal{L}^{u}$ is the generator of the controlled stochastic process, applied to a test function whose first and second spatial derivatives are denoted by $p$ and $M$ respectively, and $f(x,u)$ is the running cost.\n\nFirst, we determine the generator $\\mathcal{L}^{u}$. For a general one-dimensional It么 process given by $\\mathrm{d}X_{t} = \\mu(t, X_{t})\\,\\mathrm{d}t + \\Sigma(t, X_{t})\\,\\mathrm{d}W_{t}$, the generator applied to a $C^{2}$ function $\\phi(x)$ is:\n$$\n\\mathcal{L}\\phi(x) = \\mu(t,x) \\frac{\\partial \\phi}{\\partial x} + \\frac{1}{2} \\Sigma(t,x)^{2} \\frac{\\partial^{2} \\phi}{\\partial x^{2}}\n$$\nIn our specific problem, the SDE is $\\mathrm{d}X_{t} = u_{t}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}$. The drift is $\\mu(t,x,u) = u$ and the diffusion is $\\Sigma(t,x,u) = \\sigma$. Let $\\phi(x)$ be a generic $C^2$ test function. According to the problem statement, we denote its derivatives as $ p = \\frac{\\mathrm{d}\\phi}{\\mathrm{d}x}$ and $M = \\frac{\\mathrm{d}^2\\phi}{\\mathrm{d}x^2}$.\nThe generator for the controlled process, $\\mathcal{L}^{u}$, is therefore:\n$$\n\\mathcal{L}^{u} = u \\cdot p + \\frac{1}{2}\\sigma^{2} \\cdot M\n$$\nNext, we add the running cost $f(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2}$. The expression to be minimized with respect to the control $u \\in \\mathbb{R}$ is:\n$$\nJ(u) = \\mathcal{L}^{u} + f(x,u) = \\left( u p + \\frac{1}{2}\\sigma^{2}M \\right) + \\left( \\frac{1}{2} r\\,u^{2} + q\\,x^{2} \\right)\n$$\nWe can rearrange this expression as a function of $u$:\n$$\nJ(u) = \\frac{1}{2} r\\,u^{2} + p\\,u + \\left( q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M \\right)\n$$\nThis is a quadratic function of $u$. Since the problem states that $r0$, the parabola opens upwards, and the function has a unique global minimum. To find the value of $u$ that minimizes $J(u)$, we compute the first derivative with respect to $u$ and set it to zero.\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}u} = \\frac{\\mathrm{d}}{\\mathrm{d}u} \\left( \\frac{1}{2} r\\,u^{2} + p\\,u + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M \\right) = r\\,u + p\n$$\nSetting the derivative to zero gives the optimal control, which we denote as $u^{\\star}$:\n$$\nr\\,u^{\\star} + p = 0 \\implies u^{\\star} = -\\frac{p}{r}\n$$\nThis is the minimizing state-feedback control as a function of $p$ and $r$. This is the second part of the required answer.\n\nTo find the minimized Hamiltonian, $H(x,p,M)$, we substitute $u^{\\star}$ back into the expression for $J(u)$:\n$$\nH(x,p,M) = J(u^{\\star}) = \\frac{1}{2} r\\left(-\\frac{p}{r}\\right)^{2} + p\\left(-\\frac{p}{r}\\right) + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n$$\nH(x,p,M) = \\frac{1}{2} r\\left(\\frac{p^{2}}{r^{2}}\\right) - \\frac{p^{2}}{r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n$$\nH(x,p,M) = \\frac{p^{2}}{2r} - \\frac{p^{2}}{r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\nCombining the terms involving $p^{2}$:\n$$\nH(x,p,M) = -\\frac{p^{2}}{2r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\nThis is the minimized Hamiltonian as an explicit function of $x$, $p$, $M$, $\\sigma$, $r$, and $q$. This is the first part of the required answer.\n\nThe final answer is presented as a two-entry row matrix.\nFirst entry: Minimized Hamiltonian $H(x,p,M) = q\\,x^{2} - \\frac{p^{2}}{2r} + \\frac{1}{2}\\sigma^{2}M$.\nSecond entry: Minimizing control $u^{\\star} = -\\frac{p}{r}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} q\\,x^{2} - \\frac{p^{2}}{2r} + \\frac{1}{2}\\sigma^{2}M  -\\frac{p}{r} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having mastered the minimization of the Hamiltonian, we can now tackle a complete optimal control problem. For the classic finite-horizon linear-quadratic regulator (LQR), the HJB equation is a partial differential equation that can be solved analytically using a clever 'guess-and-verify' strategy. This practice  guides you through proposing a quadratic form for the value function, which elegantly transforms the PDE into a solvable Riccati ordinary differential equation, yielding the full time-dependent optimal policy.",
            "id": "3080756",
            "problem": "Consider the scalar controlled stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t = \\big(a X_t + b\\,u_t\\big)\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t,\\quad X_0 = x,\n$$\nwhere $W_t$ is a standard Brownian motion, and $u_t$ is a progressively measurable control taking values in $\\mathbb{R}$. For a fixed finite horizon $Tgt;0$, the performance criterion is\n$$\nJ^{u}(t,x) = \\mathbb{E}\\Bigg[\\int_{t}^{T} \\big(q\\,X_s^2 + r\\,u_s^2\\big)\\,\\mathrm{d}s + s_f\\,X_T^2\\ \\Big|\\ X_t = x\\Bigg].\n$$\nUsing the Dynamic Programming Principle (DPP) and It么's formula as the foundational starting point, derive the Hamilton-Jacobi-Bellman (HJB) equation for the value function $V(t,x) = \\inf_{u} J^{u}(t,x)$, state the verification theorem in this setting, and from it derive the optimal feedback law $u^{*}(t,x)$.\n\nThen, specialize to the parameter values $a=0$, $b=1$, $\\sigmagt;0$ arbitrary, $q=1$, $r=1$, and terminal weight $s_f=1$, on the time interval $[0,T]$ with $Tgt;0$. Compute explicitly the optimal feedback control $u^{*}(t,x)$ and the resulting closed-loop drift $\\mu_{\\mathrm{cl}}(t,x)$ of the state SDE under $u^{*}$.\n\nYour final answer must consist of the pair $\\big(u^{*}(t,x),\\,\\mu_{\\mathrm{cl}}(t,x)\\big)$ as a single closed-form analytic expression. No numerical rounding is required. No physical units are involved.",
            "solution": "### Derivation of the Hamilton-Jacobi-Bellman Equation\n\nThe derivation begins with the Dynamic Programming Principle (DPP). For any small time increment $\\delta t  0$, the value function $V(t,x)$ satisfies:\n$$\nV(t,x) = \\inf_{u \\in \\mathcal{U}[t,T]} \\mathbb{E}\\left[ \\int_{t}^{t+\\delta t} (qX_s^2 + r u_s^2)\\,\\mathrm{d}s + V(t+\\delta t, X_{t+\\delta t}) \\mid X_t=x \\right]\n$$\nwhere $\\mathcal{U}[t,T]$ is the set of admissible controls over the interval $[t,T]$. For an infinitesimal interval $[t, t+\\delta t]$, we can approximate the integral and assume the control $u_s=u_t=u$ is constant.\n$$\nV(t,x) \\approx \\inf_{u} \\mathbb{E}\\left[ (q x^2 + r u^2)\\delta t + V(t+\\delta t, X_{t+\\delta t}) \\mid X_t=x \\right]\n$$\nLet's apply It么's formula to the process $V(t,X_t)$. Assuming $V \\in C^{1,2}([0,T) \\times \\mathbb{R})$, its differential is:\n$$\n\\mathrm{d}V(t,X_t) = \\left( \\frac{\\partial V}{\\partial t} + (a X_t + b u_t)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right)\\mathrm{d}t + \\sigma \\frac{\\partial V}{\\partial x} \\mathrm{d}W_t\n$$\nIntegrating from $t$ to $t+\\delta t$ and taking the conditional expectation, the stochastic integral term vanishes because $\\mathbb{E}[\\int_{t}^{t+\\delta t} (\\dots) \\mathrm{d}W_s \\mid \\mathcal{F}_t] = 0$.\n$$\n\\mathbb{E}[V(t+\\delta t, X_{t+\\delta t}) \\mid X_t=x] - V(t,x) = \\mathbb{E}\\left[ \\int_{t}^{t+\\delta t} \\left( \\frac{\\partial V}{\\partial s} + (a X_s + b u_s)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right)\\mathrm{d}s \\mid X_t=x \\right]\n$$\nFor small $\\delta t$, this becomes:\n$$\n\\mathbb{E}[V(t+\\delta t, X_{t+\\delta t}) \\mid X_t=x] \\approx V(t,x) + \\left( \\frac{\\partial V}{\\partial t} + (a x + b u)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right)\\delta t\n$$\nSubstituting this into the DPP expression:\n$$\nV(t,x) \\approx \\inf_{u} \\left\\{ (q x^2 + r u^2)\\delta t + V(t,x) + \\left( \\frac{\\partial V}{\\partial t} + (a x + b u)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right)\\delta t \\right\\}\n$$\nSubtracting $V(t,x)$ from both sides, dividing by $\\delta t$, and taking the limit as $\\delta t \\to 0$, we obtain the Hamilton-Jacobi-Bellman (HJB) equation:\n$$\n0 = \\inf_{u} \\left\\{ \\frac{\\partial V}{\\partial t} + (a x + b u)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 + r u^2 \\right\\}\n$$\nThis is equivalent to:\n$$\n-\\frac{\\partial V}{\\partial t} = \\inf_{u} \\left\\{ (a x + b u)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 + r u^2 \\right\\}\n$$\nThe term to be minimized, let's call it the Hamiltonian $\\mathcal{H}(t,x,u,V_x)$, is a quadratic function of $u$. The minimum is found by setting its derivative with respect to $u$ to zero:\n$$\n\\frac{\\partial \\mathcal{H}}{\\partial u} = b \\frac{\\partial V}{\\partial x} + 2ru = 0\n$$\nSince $r0$ must hold for a meaningful problem (and $r=1$ in the specialized case), we can solve for the optimal feedback control $u^{*}(t,x)$:\n$$\nu^{*}(t,x) = -\\frac{b}{2r} \\frac{\\partial V}{\\partial x}\n$$\nSubstituting this back into the HJB equation gives the PDE for the value function $V(t,x)$:\n$$\n\\frac{\\partial V}{\\partial t} + (a x)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 + b\\left(-\\frac{b}{2r}\\frac{\\partial V}{\\partial x}\\right)\\frac{\\partial V}{\\partial x} + r\\left(-\\frac{b}{2r}\\frac{\\partial V}{\\partial x}\\right)^2 = 0\n$$\n$$\n\\frac{\\partial V}{\\partial t} + (a x)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 - \\frac{b^2}{2r}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 + \\frac{rb^2}{4r^2}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 = 0\n$$\nThis simplifies to the final form of the HJB PDE:\n$$\n\\frac{\\partial V}{\\partial t} + (a x)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 - \\frac{b^2}{4r}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 = 0\n$$\nThis PDE must be solved backwards in time from the terminal condition $V(T,x) = s_f x^2$.\n\n### Verification Theorem\nA verification theorem formally connects the solution of the HJB equation to the value function. In this context, it can be stated as:\n\nLet $W(t,x) \\in C^{1,2}([0,T] \\times \\mathbb{R})$ be a function that satisfies the HJB partial differential equation:\n$$\n\\frac{\\partial W}{\\partial t} + (a x)\\frac{\\partial W}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 W}{\\partial x^2} + q x^2 - \\frac{b^2}{4r}\\left(\\frac{\\partial W}{\\partial x}\\right)^2 = 0\n$$\nwith the terminal condition $W(T,x) = s_f x^2$. Let $u^*(t,x) = -\\frac{b}{2r} \\frac{\\partial W}{\\partial x}(t,x)$. If the SDE for the state $X_t^*$ under this control has a unique strong solution for any initial condition $(t,x)$, and if certain integrability conditions on the cost are met, then $W(t,x)$ is the true value function, i.e., $W(t,x) = V(t,x)$, and $u^*$ is the optimal control.\n\n### Specialization and Solution\nWe now specialize to the parameters $a=0$, $b=1$, $\\sigma0$, $q=1$, $r=1$, and $s_f=1$. The HJB equation becomes:\n$$\n\\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + x^2 - \\frac{1}{4}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 = 0\n$$\nThe terminal condition is $V(T,x) = x^2$. Given the quadratic cost and linear dynamics, we propose a quadratic solution of the form:\n$$\nV(t,x) = P(t)x^2 + S(t)\n$$\nThe partial derivatives are:\n$\\frac{\\partial V}{\\partial t} = \\dot{P}(t)x^2 + \\dot{S}(t)$, $\\frac{\\partial V}{\\partial x} = 2P(t)x$, $\\frac{\\partial^2 V}{\\partial x^2} = 2P(t)$.\nSubstituting these into the HJB equation:\n$$\n(\\dot{P}(t)x^2 + \\dot{S}(t)) + \\frac{1}{2}\\sigma^2(2P(t)) + x^2 - \\frac{1}{4}(2P(t)x)^2 = 0\n$$\n$$\n\\dot{P}(t)x^2 + \\dot{S}(t) + \\sigma^2 P(t) + x^2 - P(t)^2 x^2 = 0\n$$\nGrouping terms by powers of $x^2$:\n$$\n(\\dot{P}(t) - P(t)^2 + 1)x^2 + (\\dot{S}(t) + \\sigma^2 P(t)) = 0\n$$\nFor this to hold for all $x \\in \\mathbb{R}$, the coefficients of the powers of $x$ must be zero. This yields a system of two ordinary differential equations (ODEs):\n1.  $\\dot{P}(t) = P(t)^2 - 1$ (Riccati equation)\n2.  $\\dot{S}(t) = -\\sigma^2 P(t)$\n\nThe terminal condition $V(T,x) = 1 \\cdot x^2$ implies $P(T)x^2 + S(T) = x^2$, which gives the terminal conditions for the ODEs: $P(T)=1$ and $S(T)=0$.\n\nLet's solve for $P(t)$. The ODE is $\\frac{\\mathrm{d}P}{\\mathrm{d}t} = P^2-1$. We observe that $P_c=1$ is a constant (equilibrium) solution since $1^2-1=0$. Since the terminal condition is $P(T)=1$, by the uniqueness of solutions to ODEs, the solution must be the constant function $P(t) = 1$ for all $t \\in [0,T]$.\n\nNow, we solve for $S(t)$. The ODE is $\\dot{S}(t) = -\\sigma^2 P(t)$. Substituting $P(t)=1$:\n$$\n\\dot{S}(t) = -\\sigma^2\n$$\nIntegrating with respect to $t$:\n$$\nS(t) = \\int -\\sigma^2 \\mathrm{d}t = -\\sigma^2 t + C\n$$\nUsing the terminal condition $S(T)=0$:\n$$\nS(T) = -\\sigma^2 T + C = 0 \\implies C = \\sigma^2 T\n$$\nThus, the solution for $S(t)$ is $S(t) = \\sigma^2 T - \\sigma^2 t = \\sigma^2 (T-t)$.\n\nThe value function is $V(t,x) = x^2 + \\sigma^2(T-t)$.\n\n### Optimal Control and Closed-Loop Drift\nThe optimal feedback control is given by the general formula we derived:\n$$\nu^{*}(t,x) = -\\frac{b}{2r} \\frac{\\partial V}{\\partial x}\n$$\nWith $b=1$, $r=1$, and $\\frac{\\partial V}{\\partial x} = 2P(t)x = 2(1)x = 2x$, we get:\n$$\nu^{*}(t,x) = -\\frac{1}{2(1)} (2x) = -x\n$$\nThe closed-loop drift, $\\mu_{\\mathrm{cl}}(t,x)$, is obtained by substituting $u^*(t,x)$ into the drift term of the original SDE, $a x_t + b u_t$. With $a=0$ and $b=1$:\n$$\n\\mu_{\\mathrm{cl}}(t,x) = a x + b u^{*}(t,x) = (0)x + (1)(-x) = -x\n$$\nThe resulting optimal control and closed-loop drift are both $-x$. They are independent of time $t$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -x  -x \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While finite-horizon problems are instructive, many applications require a control strategy that is optimal over an indefinite period. This leads to the infinite-horizon discounted problem, where we seek a stationary, or time-invariant, optimal policy. In this setting, the HJB equation simplifies, and the time-dependent Riccati ODE from our previous exercise  becomes a simpler Algebraic Riccati Equation (ARE). This practice  will show you how to solve this ARE to find the constant coefficients of the stationary value function and derive the corresponding optimal steady-state feedback control.",
            "id": "3080759",
            "problem": "Consider the one-dimensional controlled stochastic differential equation (SDE)\n$$\n\\mathrm{d}x_{t}=\\big(a\\,x_{t}+b\\,u_{t}\\big)\\,\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t},\n$$\nwhere $a=1$, $b=2$, and $\\sigma=1$, and $\\{W_{t}\\}_{t\\ge 0}$ is a standard Wiener process. Let the control $\\{u_{t}\\}_{t\\ge 0}$ be progressively measurable with respect to the filtration generated by $\\{W_{t}\\}$ and satisfy standard integrability conditions ensuring the existence of a unique strong solution. The infinite-horizon discounted cost is\n$$\nJ(u)=\\mathbb{E}\\left[\\int_{0}^{\\infty}\\exp(-\\rho t)\\,\\big(q\\,x_{t}^{2}+r\\,u_{t}^{2}\\big)\\,\\mathrm{d}t\\right],\n$$\nwhere $\\rho=3$, $q=1$, and $r=1$. Assume time-homogeneity and consider stationary Markov feedback controls $u_{t}=u(x_{t})$.\n\nStarting from the dynamic programming principle and the infinitesimal generator of the diffusion, derive the Hamilton-Jacobi-Bellman (HJB) equation for the value function $V(x)$ in the infinite-horizon discounted setting. Under the quadratic ansatz $V(x)=P\\,x^{2}+v_{0}$ with constants $P0$ and $v_{0}\\in\\mathbb{R}$, show that $P$ must satisfy an algebraic Riccati equation, solve explicitly for $P$, and determine the stationary optimal feedback $u^{\\star}(x)$ and the corresponding value function $V(x)$.\n\nProvide your final answer as three expressions in a single row matrix, in the order $(P,\\;u^{\\star}(x),\\;V(x))$. No derivations should appear in the final answer. The final answer must be an exact, closed-form expression. Do not round or approximate. No units are required.",
            "solution": "The value function for the infinite-horizon discounted optimal control problem is defined as\n$$\nV(x) = \\inf_{u} J(u) = \\inf_{u} \\mathbb{E}\\left[\\int_{0}^{\\infty}\\exp(-\\rho t)\\,\\big(q\\,x_{t}^{2}+r\\,u_{t}^{2}\\big)\\,\\mathrm{d}t \\, \\bigg| \\, x_0=x \\right]\n$$\nwhere the infimum is taken over all admissible controls $\\{u_t\\}$. The dynamic programming principle leads to the Hamilton-Jacobi-Bellman (HJB) equation. For a time-homogeneous, infinite-horizon discounted problem, the HJB equation is given by\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ L(x, u) + \\mathcal{A}^u V(x) \\right\\}\n$$\nwhere $\\rho$ is the discount rate, $L(x, u) = q\\,x^{2}+r\\,u^{2}$ is the running cost, and $\\mathcal{A}^u$ is the infinitesimal generator of the controlled diffusion process $x_t$.\n\nThe controlled stochastic differential equation (SDE) is\n$$\n\\mathrm{d}x_{t}=\\big(a\\,x_{t}+b\\,u_{t}\\big)\\,\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t}\n$$\nThe drift coefficient is $\\mu(x, u) = a\\,x+b\\,u$ and the diffusion coefficient is $\\eta(x, u) = \\sigma$. The infinitesimal generator $\\mathcal{A}^u$ acting on a twice-differentiable function $V(x)$ is\n$$\n\\mathcal{A}^u V(x) = \\mu(x,u) \\frac{\\mathrm{d}V}{\\mathrm{d}x} + \\frac{1}{2} \\eta(x,u)^2 \\frac{\\mathrm{d}^2V}{\\mathrm{d}x^2} = (a\\,x+b\\,u)V'(x) + \\frac{1}{2}\\sigma^2 V''(x)\n$$\nSubstituting $L(x, u)$ and $\\mathcal{A}^u V(x)$ into the HJB equation yields\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ q\\,x^2 + r\\,u^2 + (a\\,x+b\\,u)V'(x) + \\frac{1}{2}\\sigma^2 V''(x) \\right\\}\n$$\nTo find the optimal control $u^{\\star}$, we minimize the expression inside the braces with respect to $u$. The expression is a quadratic function of $u$. The first-order condition for a minimum is found by setting the partial derivative with respect to $u$ to zero:\n$$\n\\frac{\\partial}{\\partial u} \\left[ q\\,x^2 + r\\,u^2 + (a\\,x+b\\,u)V'(x) + \\frac{1}{2}\\sigma^2 V''(x) \\right] = 2\\,r\\,u + b\\,V'(x) = 0\n$$\nSolving for $u$ gives the stationary optimal feedback control as a function of the value function's derivative:\n$$\nu^{\\star}(x) = -\\frac{b}{2r}V'(x)\n$$\nThe second derivative is $2r$, which is positive since $r=1  0$, confirming that this is indeed a minimum.\n\nSubstituting $u^{\\star}(x)$ back into the HJB equation eliminates the infimum:\n$$\n\\rho V(x) = q\\,x^2 + r\\left(-\\frac{b}{2r}V'(x)\\right)^2 + \\left(a\\,x+b\\left(-\\frac{b}{2r}V'(x)\\right)\\right)V'(x) + \\frac{1}{2}\\sigma^2 V''(x)\n$$\nSimplifying the terms:\n$$\n\\rho V(x) = q\\,x^2 + r\\frac{b^2}{4r^2}(V'(x))^2 + a\\,xV'(x) - \\frac{b^2}{2r}(V'(x))^2 + \\frac{1}{2}\\sigma^2 V''(x)\n$$\n$$\n\\rho V(x) = q\\,x^2 + a\\,xV'(x) - \\frac{b^2}{4r}(V'(x))^2 + \\frac{1}{2}\\sigma^2 V''(x)\n$$\nThis is a non-linear ordinary differential equation for the value function $V(x)$.\n\nWe now use the quadratic ansatz $V(x) = P\\,x^2 + v_0$, where $P0$ and $v_0$ are constants to be determined. The derivatives are:\n$$\nV'(x) = 2\\,P\\,x\n$$\n$$\nV''(x) = 2\\,P\n$$\nSubstituting these into the HJB equation:\n$$\n\\rho(P\\,x^2+v_0) = q\\,x^2 + a\\,x(2\\,P\\,x) - \\frac{b^2}{4r}(2\\,P\\,x)^2 + \\frac{1}{2}\\sigma^2(2\\,P)\n$$\n$$\n\\rho\\,P\\,x^2 + \\rho\\,v_0 = q\\,x^2 + 2\\,a\\,P\\,x^2 - \\frac{b^2}{4r}(4\\,P^2\\,x^2) + \\sigma^2\\,P\n$$\n$$\n\\rho\\,P\\,x^2 + \\rho\\,v_0 = \\left( q + 2\\,a\\,P - \\frac{b^2}{r}P^2 \\right)x^2 + \\sigma^2\\,P\n$$\nFor this equation to hold for all $x \\in \\mathbb{R}$, we must equate the coefficients of the powers of $x$ on both sides.\nEquating the coefficients of $x^2$:\n$$\n\\rho\\,P = q + 2\\,a\\,P - \\frac{b^2}{r}P^2\n$$\nRearranging this gives the algebraic Riccati equation (ARE) for $P$:\n$$\n\\frac{b^2}{r}P^2 + (\\rho - 2\\,a)P - q = 0\n$$\nEquating the constant terms (coefficients of $x^0$):\n$$\n\\rho\\,v_0 = \\sigma^2\\,P \\quad \\implies \\quad v_0 = \\frac{\\sigma^2\\,P}{\\rho}\n$$\nWe are given the parameter values: $a=1$, $b=2$, $\\sigma=1$, $\\rho=3$, $q=1$, $r=1$. Substituting these values into the ARE:\n$$\n\\frac{2^2}{1}P^2 + (3 - 2 \\cdot 1)P - 1 = 0\n$$\n$$\n4P^2 + P - 1 = 0\n$$\nWe solve this quadratic equation for $P$ using the quadratic formula:\n$$\nP = \\frac{-1 \\pm \\sqrt{1^2 - 4(4)(-1)}}{2(4)} = \\frac{-1 \\pm \\sqrt{1 + 16}}{8} = \\frac{-1 \\pm \\sqrt{17}}{8}\n$$\nThe value function must be convex, which corresponds to $P  0$. Thus, we must choose the positive root:\n$$\nP = \\frac{-1 + \\sqrt{17}}{8}\n$$\nNow we find the constant term $v_0$:\n$$\nv_0 = \\frac{\\sigma^2\\,P}{\\rho} = \\frac{1^2}{3}P = \\frac{1}{3} \\left( \\frac{\\sqrt{17}-1}{8} \\right) = \\frac{\\sqrt{17}-1}{24}\n$$\nThe value function is therefore:\n$$\nV(x) = P\\,x^2 + v_0 = \\left(\\frac{\\sqrt{17}-1}{8}\\right)x^2 + \\frac{\\sqrt{17}-1}{24}\n$$\nFinally, we determine the optimal feedback control $u^{\\star}(x)$:\n$$\nu^{\\star}(x) = -\\frac{b}{2r}V'(x) = -\\frac{b}{2r}(2\\,P\\,x) = -\\frac{b}{r}Px\n$$\nSubstituting the values for $b$, $r$, and the solved $P$:\n$$\nu^{\\star}(x) = -\\frac{2}{1} \\left( \\frac{\\sqrt{17}-1}{8} \\right) x = -\\frac{\\sqrt{17}-1}{4}x\n$$\nThe three requested quantities are:\n$P = \\frac{\\sqrt{17}-1}{8}$\n$u^{\\star}(x) = -\\frac{\\sqrt{17}-1}{4}x$\n$V(x) = \\frac{\\sqrt{17}-1}{8}x^2 + \\frac{\\sqrt{17}-1}{24}$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\sqrt{17}-1}{8}  -\\frac{\\sqrt{17}-1}{4}x  \\frac{\\sqrt{17}-1}{8}x^2 + \\frac{\\sqrt{17}-1}{24} \\end{pmatrix}}\n$$"
        }
    ]
}