{
    "hands_on_practices": [
        {
            "introduction": "The Hamilton-Jacobi-Bellman (HJB) equation's power stems from connecting a complex dynamic optimization problem to a partial differential equation. At the heart of this connection lies the Hamiltonian, which represents a static optimization problem at each point in space and time. This first exercise  focuses on mastering this crucial step by calculating the minimized Hamiltonian and the optimal feedback control for a standard linear-quadratic system, treating the derivatives of the value function as given parameters.",
            "id": "3080765",
            "problem": "Consider the one-dimensional controlled Itô stochastic differential equation\n$$\n\\mathrm{d}X_{t} = b(X_{t},u_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t},\n$$\nwith drift $b(x,u) = u$, constant diffusion coefficient $\\sigma > 0$, and control process $u_{t}$ taking values in $\\mathbb{R}$. Let the running cost be\n$$\n\\ell(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2},\n$$\nwith $r>0$ and $q>0$. Define the value function $V(t,x)$ for a finite horizon problem with a bounded and continuous terminal cost $g(x)$ by\n$$\nV(t,x) = \\inf_{u} \\,\\mathbb{E}\\!\\left[ \\int_{t}^{T} \\ell(X_{s},u_{s})\\,\\mathrm{d}s + g(X_{T}) \\,\\middle|\\, X_{t}=x \\right],\n$$\nwhere the infimum is over progressively measurable controls taking values in $\\mathbb{R}$. Using the dynamic programming principle and the definition of the Hamiltonian appearing in the Hamilton-Jacobi-Bellman (HJB) equation, the Hamiltonian at a point $(x,p,M)$ is given by the infimum over controls $u \\in \\mathbb{R}$ of the sum of the controlled generator applied to a $C^{2}$ test function and the running cost. In particular, if $p$ denotes the first spatial derivative and $M$ denotes the second spatial derivative, compute:\n- the minimized Hamiltonian as an explicit function of $x$, $p$, $M$, $\\sigma$, $r$, and $q$;\n- the corresponding minimizing state-feedback control as an explicit function of $p$ and $r$.\n\nProvide your final answer as a two-entry row matrix, with the first entry being the minimized Hamiltonian and the second entry being the minimizing feedback control. No numerical approximation is required, and no units are involved.",
            "solution": "This problem requires calculating the minimized Hamiltonian and the corresponding optimal control for a given stochastic control problem. The Hamiltonian, as defined in the context of the Hamilton-Jacobi-Bellman (HJB) equation, is given by:\n$$\nH(x,p,M) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\mathcal{L}^{u} + \\ell(x,u) \\right\\}\n$$\nwhere $\\mathcal{L}^{u}$ is the generator of the controlled stochastic process, applied to a test function whose first and second spatial derivatives are denoted by $p$ and $M$ respectively, and $\\ell(x,u)$ is the running cost.\n\nFirst, we determine the generator $\\mathcal{L}^{u}$. For a general one-dimensional Itô process given by $\\mathrm{d}X_{t} = \\mu(t, X_{t})\\,\\mathrm{d}t + \\Sigma(t, X_{t})\\,\\mathrm{d}W_{t}$, the generator applied to a $C^{2}$ function $\\phi(x)$ is:\n$$\n\\mathcal{L}\\phi(x) = \\mu(t,x) \\frac{\\partial \\phi}{\\partial x} + \\frac{1}{2} \\Sigma(t,x)^{2} \\frac{\\partial^{2} \\phi}{\\partial x^{2}}\n$$\nIn our specific problem, the SDE is $\\mathrm{d}X_{t} = u_{t}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}$. The drift is $\\mu(t,x,u) = u$ and the diffusion is $\\Sigma(t,x,u) = \\sigma$. Let $\\phi(x)$ be a generic $C^2$ test function. According to the problem statement, we denote its derivatives as $ p = \\frac{\\mathrm{d}\\phi}{\\mathrm{d}x}$ and $M = \\frac{\\mathrm{d}^2\\phi}{\\mathrm{d}x^2}$.\nThe generator for the controlled process, $\\mathcal{L}^{u}$, is therefore:\n$$\n\\mathcal{L}^{u} = u \\cdot p + \\frac{1}{2}\\sigma^{2} \\cdot M\n$$\nNext, we add the running cost $\\ell(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2}$. The expression to be minimized with respect to the control $u \\in \\mathbb{R}$ is:\n$$\nJ(u) = \\mathcal{L}^{u} + \\ell(x,u) = \\left( u p + \\frac{1}{2}\\sigma^{2}M \\right) + \\left( \\frac{1}{2} r\\,u^{2} + q\\,x^{2} \\right)\n$$\nWe can rearrange this expression as a function of $u$:\n$$\nJ(u) = \\frac{1}{2} r\\,u^{2} + p\\,u + \\left( q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M \\right)\n$$\nThis is a quadratic function of $u$. Since the problem states that $r>0$, the parabola opens upwards, and the function has a unique global minimum. To find the value of $u$ that minimizes $J(u)$, we compute the first derivative with respect to $u$ and set it to zero.\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}u} = \\frac{\\mathrm{d}}{\\mathrm{d}u} \\left( \\frac{1}{2} r\\,u^{2} + p\\,u + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M \\right) = r\\,u + p\n$$\nSetting the derivative to zero gives the optimal control, which we denote as $u^{\\star}$:\n$$\nr\\,u^{\\star} + p = 0 \\implies u^{\\star} = -\\frac{p}{r}\n$$\nThis is the minimizing state-feedback control as a function of $p$ and $r$. This is the second part of the required answer.\n\nTo find the minimized Hamiltonian, $H(x,p,M)$, we substitute $u^{\\star}$ back into the expression for $J(u)$:\n$$\nH(x,p,M) = J(u^{\\star}) = \\frac{1}{2} r\\left(-\\frac{p}{r}\\right)^{2} + p\\left(-\\frac{p}{r}\\right) + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n$$\nH(x,p,M) = \\frac{1}{2} r\\left(\\frac{p^{2}}{r^{2}}\\right) - \\frac{p^{2}}{r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n$$\nH(x,p,M) = \\frac{p^{2}}{2r} - \\frac{p^{2}}{r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\nCombining the terms involving $p^{2}$:\n$$\nH(x,p,M) = -\\frac{p^{2}}{2r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\nThis is the minimized Hamiltonian as an explicit function of $x$, $p$, $M$, $\\sigma$, $r$, and $q$. This is the first part of the required answer.\n\nThe final answer is presented as a two-entry row matrix.\nFirst entry: Minimized Hamiltonian $H(x,p,M) = q\\,x^{2} - \\frac{p^{2}}{2r} + \\frac{1}{2}\\sigma^{2}M$.\nSecond entry: Minimizing control $u^{\\star} = -\\frac{p}{r}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} q\\,x^{2} - \\frac{p^{2}}{2r} + \\frac{1}{2}\\sigma^{2}M & -\\frac{p}{r} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "With the ability to minimize the Hamiltonian, we can now tackle a complete optimal control problem from start to finish. This practice  explores the elegant case of an infinite-horizon, discounted Linear-Quadratic Regulator (LQR). You will see how assuming a simple quadratic form for the value function transforms the complex HJB partial differential equation into a solvable algebraic Riccati equation, yielding a stationary, optimal control law.",
            "id": "3080759",
            "problem": "Consider the one-dimensional controlled stochastic differential equation (SDE)\n$$\n\\mathrm{d}x_{t}=\\big(a\\,x_{t}+b\\,u_{t}\\big)\\,\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t},\n$$\nwhere $a=1$, $b=2$, and $\\sigma=1$, and $\\{W_{t}\\}_{t\\ge 0}$ is a standard Wiener process. Let the control $\\{u_{t}\\}_{t\\ge 0}$ be progressively measurable with respect to the filtration generated by $\\{W_{t}\\}$ and satisfy standard integrability conditions ensuring the existence of a unique strong solution. The infinite-horizon discounted cost is\n$$\nJ(u)=\\mathbb{E}\\left[\\int_{0}^{\\infty}\\exp(-\\rho t)\\,\\big(q\\,x_{t}^{2}+r\\,u_{t}^{2}\\big)\\,\\mathrm{d}t\\right],\n$$\nwhere $\\rho=3$, $q=1$, and $r=1$. Assume time-homogeneity and consider stationary Markov feedback controls $u_{t}=u(x_{t})$.\n\nStarting from the dynamic programming principle and the infinitesimal generator of the diffusion, derive the Hamilton-Jacobi-Bellman (HJB) equation for the value function $V(x)$ in the infinite-horizon discounted setting. Under the quadratic ansatz $V(x)=P\\,x^{2}+v_{0}$ with constants $P>0$ and $v_{0}\\in\\mathbb{R}$, show that $P$ must satisfy an algebraic Riccati equation, solve explicitly for $P$, and determine the stationary optimal feedback $u^{\\star}(x)$ and the corresponding value function $V(x)$.\n\nProvide your final answer as three expressions in a single row matrix, in the order $(P,\\;u^{\\star}(x),\\;V(x))$. No derivations should appear in the final answer. The final answer must be an exact, closed-form expression. Do not round or approximate. No units are required.",
            "solution": "The value function for the infinite-horizon discounted optimal control problem is defined as\n$$\nV(x) = \\inf_{u} J(u) = \\inf_{u} \\mathbb{E}\\left[\\int_{0}^{\\infty}\\exp(-\\rho t)\\,\\big(q\\,x_{t}^{2}+r\\,u_{t}^{2}\\big)\\,\\mathrm{d}t \\, \\bigg| \\, x_0=x \\right]\n$$\nwhere the infimum is taken over all admissible controls $\\{u_t\\}$. The dynamic programming principle leads to the Hamilton-Jacobi-Bellman (HJB) equation. For a time-homogeneous, infinite-horizon discounted problem, the HJB equation is given by\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\ell(x, u) + \\mathcal{L}^u V(x) \\right\\}\n$$\nwhere $\\rho$ is the discount rate, $\\ell(x, u) = q\\,x^{2}+r\\,u^{2}$ is the running cost, and $\\mathcal{L}^u$ is the infinitesimal generator of the controlled diffusion process $x_t$.\n\nThe controlled stochastic differential equation (SDE) is\n$$\n\\mathrm{d}x_{t}=\\big(a\\,x_{t}+b\\,u_{t}\\big)\\,\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t}\n$$\nThe drift coefficient is $\\mu(x, u) = a\\,x+b\\,u$ and the diffusion coefficient is $\\eta(x, u) = \\sigma$. The infinitesimal generator $\\mathcal{L}^u$ acting on a twice-differentiable function $V(x)$ is\n$$\n\\mathcal{L}^u V(x) = \\mu(x,u) \\frac{\\mathrm{d}V}{\\mathrm{d}x} + \\frac{1}{2} \\eta(x,u)^2 \\frac{\\mathrm{d}^2V}{\\mathrm{d}x^2} = (a\\,x+b\\,u)V'(x) + \\frac{1}{2}\\sigma^2 V''(x)\n$$\nSubstituting $\\ell(x, u)$ and $\\mathcal{L}^u V(x)$ into the HJB equation yields\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ q\\,x^2 + r\\,u^2 + (a\\,x+b\\,u)V'(x) + \\frac{1}{2}\\sigma^2 V''(x) \\right\\}\n$$\nTo find the optimal control $u^{\\star}$, we minimize the expression inside the braces with respect to $u$. The expression is a quadratic function of $u$. The first-order condition for a minimum is found by setting the partial derivative with respect to $u$ to zero:\n$$\n\\frac{\\partial}{\\partial u} \\left[ q\\,x^2 + r\\,u^2 + (a\\,x+b\\,u)V'(x) + \\frac{1}{2}\\sigma^2 V''(x) \\right] = 2\\,r\\,u + b\\,V'(x) = 0\n$$\nSolving for $u$ gives the stationary optimal feedback control as a function of the value function's derivative:\n$$\nu^{\\star}(x) = -\\frac{b}{2r}V'(x)\n$$\nThe second derivative is $2r$, which is positive since $r=1 > 0$, confirming that this is indeed a minimum.\n\nSubstituting $u^{\\star}(x)$ back into the HJB equation eliminates the infimum:\n$$\n\\rho V(x) = q\\,x^2 + r\\left(-\\frac{b}{2r}V'(x)\\right)^2 + \\left(a\\,x+b\\left(-\\frac{b}{2r}V'(x)\\right)\\right)V'(x) + \\frac{1}{2}\\sigma^2 V''(x)\n$$\nSimplifying the terms:\n$$\n\\rho V(x) = q\\,x^2 + r\\frac{b^2}{4r^2}(V'(x))^2 + a\\,xV'(x) - \\frac{b^2}{2r}(V'(x))^2 + \\frac{1}{2}\\sigma^2 V''(x)\n$$\n$$\n\\rho V(x) = q\\,x^2 + a\\,xV'(x) - \\frac{b^2}{4r}(V'(x))^2 + \\frac{1}{2}\\sigma^2 V''(x)\n$$\nThis is a non-linear ordinary differential equation for the value function $V(x)$.\n\nWe now use the quadratic ansatz $V(x) = P\\,x^2 + v_0$, where $P>0$ and $v_0$ are constants to be determined. The derivatives are:\n$$\nV'(x) = 2\\,P\\,x\n$$\n$$\nV''(x) = 2\\,P\n$$\nSubstituting these into the HJB equation:\n$$\n\\rho(P\\,x^2+v_0) = q\\,x^2 + a\\,x(2\\,P\\,x) - \\frac{b^2}{4r}(2\\,P\\,x)^2 + \\frac{1}{2}\\sigma^2(2\\,P)\n$$\n$$\n\\rho\\,P\\,x^2 + \\rho\\,v_0 = q\\,x^2 + 2\\,a\\,P\\,x^2 - \\frac{b^2}{4r}(4\\,P^2\\,x^2) + \\sigma^2\\,P\n$$\n$$\n\\rho\\,P\\,x^2 + \\rho\\,v_0 = \\left( q + 2\\,a\\,P - \\frac{b^2}{r}P^2 \\right)x^2 + \\sigma^2\\,P\n$$\nFor this equation to hold for all $x \\in \\mathbb{R}$, we must equate the coefficients of the powers of $x$ on both sides.\nEquating the coefficients of $x^2$:\n$$\n\\rho\\,P = q + 2\\,a\\,P - \\frac{b^2}{r}P^2\n$$\nRearranging this gives the algebraic Riccati equation (ARE) for $P$:\n$$\n\\frac{b^2}{r}P^2 + (\\rho - 2\\,a)P - q = 0\n$$\nEquating the constant terms (coefficients of $x^0$):\n$$\n\\rho\\,v_0 = \\sigma^2\\,P \\quad \\implies \\quad v_0 = \\frac{\\sigma^2\\,P}{\\rho}\n$$\nWe are given the parameter values: $a=1$, $b=2$, $\\sigma=1$, $\\rho=3$, $q=1$, $r=1$. Substituting these values into the ARE:\n$$\n\\frac{2^2}{1}P^2 + (3 - 2 \\cdot 1)P - 1 = 0\n$$\n$$\n4P^2 + P - 1 = 0\n$$\nWe solve this quadratic equation for $P$ using the quadratic formula:\n$$\nP = \\frac{-1 \\pm \\sqrt{1^2 - 4(4)(-1)}}{2(4)} = \\frac{-1 \\pm \\sqrt{1 + 16}}{8} = \\frac{-1 \\pm \\sqrt{17}}{8}\n$$\nThe value function must be convex, which corresponds to $P > 0$. Thus, we must choose the positive root:\n$$\nP = \\frac{-1 + \\sqrt{17}}{8}\n$$\nNow we find the constant term $v_0$:\n$$\nv_0 = \\frac{\\sigma^2\\,P}{\\rho} = \\frac{1^2}{3}P = \\frac{1}{3} \\left( \\frac{\\sqrt{17}-1}{8} \\right) = \\frac{\\sqrt{17}-1}{24}\n$$\nThe value function is therefore:\n$$\nV(x) = P\\,x^2 + v_0 = \\left(\\frac{\\sqrt{17}-1}{8}\\right)x^2 + \\frac{\\sqrt{17}-1}{24}\n$$\nFinally, we determine the optimal feedback control $u^{\\star}(x)$:\n$$\nu^{\\star}(x) = -\\frac{b}{2r}V'(x) = -\\frac{b}{2r}(2\\,P\\,x) = -\\frac{b}{r}Px\n$$\nSubstituting the values for $b$, $r$, and the solved $P$:\n$$\nu^{\\star}(x) = -\\frac{2}{1} \\left( \\frac{\\sqrt{17}-1}{8} \\right) x = -\\frac{\\sqrt{17}-1}{4}x\n$$\nThe three requested quantities are:\n$P = \\frac{\\sqrt{17}-1}{8}$\n$u^{\\star}(x) = -\\frac{\\sqrt{17}-1}{4}x$\n$V(x) = \\frac{\\sqrt{17}-1}{8}x^2 + \\frac{\\sqrt{17}-1}{24}$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\sqrt{17}-1}{8} & -\\frac{\\sqrt{17}-1}{4}x & \\frac{\\sqrt{17}-1}{8}x^2 + \\frac{\\sqrt{17}-1}{24} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While analytical solutions are enlightening, most real-world applications of optimal control require numerical methods. This final practice  bridges the gap between theory and application by guiding you through the implementation of a policy iteration algorithm to solve the HJB equation numerically. By discretizing the state and action spaces, you will transform the continuous PDE into a system of equations and iteratively find the optimal policy, a technique fundamental to modern control engineering and reinforcement learning.",
            "id": "3001638",
            "problem": "Implement from first principles a complete policy iteration scheme for a one-dimensional stationary discounted Hamilton-Jacobi-Bellman equation arising from a controlled stochastic differential equation. The controlled state process is specified by the stochastic differential equation $dX_t = \\left(a X_t + b u_t\\right) dt + \\sigma dW_t$ with discount rate $\\rho > 0$, where $a$, $b$, $\\sigma$, and $\\rho$ are fixed real parameters, $u_t \\in \\mathbb{R}$ is a control, and $W_t$ is a standard Wiener process. The running cost is $\\ell(x,u) = q x^2 + r u^2$ with $q > 0$ and $r > 0$. The objective from an initial state $x$ is to minimize the discounted cost $J^u(x) = \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} \\ell\\left(X_t^u, u_t\\right) dt\\right]$ over all admissible controls. Let $V(x)$ denote the value function.\n\nYour task is to:\n- Derive the stationary discounted Hamilton-Jacobi-Bellman equation satisfied by $V(x)$ on a bounded interval $\\left[-x_{\\max}, x_{\\max}\\right]$ with Dirichlet boundary conditions chosen to match the exact infinite-horizon discounted linear-quadratic solution $V^{\\mathrm{ref}}(x)$ of this problem. The reference solution is known to be quadratic of the form $V^{\\mathrm{ref}}(x) = P x^2 + C$, where the coefficient $P$ is determined by the algebraic Riccati relation implied by the Hamilton-Jacobi-Bellman equation, and $C$ is chosen to satisfy the stationary discounted balance for the constant diffusion term.\n- Discretize the state space uniformly with $N$ grid points on $\\left[-x_{\\max}, x_{\\max}\\right]$ with mesh spacing $h = 2 x_{\\max}/(N-1)$. Approximate the first derivative by the central difference operator and the second derivative by the standard three-point central difference operator at interior nodes. Impose Dirichlet boundary conditions $V(-x_{\\max}) = V^{\\mathrm{ref}}(-x_{\\max})$ and $V(x_{\\max}) = V^{\\mathrm{ref}}(x_{\\max})$.\n- Discretize the action space as a finite, symmetric set $\\mathcal{U}_M = \\left\\{u_j\\right\\}_{j=1}^M$ of $M$ uniformly spaced values in $\\left[-u_{\\max}, u_{\\max}\\right]$, where $u_{\\max}$ is chosen as $u_{\\max} = \\kappa \\left| b P x_{\\max} / r \\right|$ with $\\kappa = 1.25$ so that the discrete action grid covers the continuous optimizer across the computational domain.\n- Implement policy iteration:\n  - Policy evaluation: for a fixed discrete policy $u(x_i) \\in \\mathcal{U}_M$ at each interior grid point $x_i$, solve the linear system that results from the discretized stationary discounted Hamilton-Jacobi-Bellman equation to obtain the value vector on the grid.\n  - Policy improvement: at each interior grid point $x_i$, compute the discrete Hamiltonian minimizer $\\arg\\min_{u \\in \\mathcal{U}_M} \\left\\{ r u^2 + b u \\left(D_x V\\right)(x_i) \\right\\}$ using the current discrete gradient approximation $\\left(D_x V\\right)(x_i)$, and update the policy.\n  - Terminate when both the sup-norm change in the value vector and the sup-norm change in the policy are smaller than a tolerance $\\varepsilon$, or when the number of policy iterations reaches a maximum $K_{\\max}$.\n- For each test case below, compute the uniform error on the grid $\\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty}$, where $V_{\\mathrm{num}}$ is the converged numerical value function and $V^{\\mathrm{ref}}$ is the exact quadratic reference.\n\nUse the following test suite of parameter sets, each provided as a tuple $\\left(a, b, q, r, \\sigma, \\rho, x_{\\max}, N, M\\right)$:\n- Test $1$: $\\left(-0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 3.0, 161, 61\\right)$.\n- Test $2$: $\\left(0.2, 1.0, 1.0, 0.5, 0.1, 1.0, 2.0, 161, 61\\right)$.\n- Test $3$: $\\left(0.3, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 161, 61\\right)$.\n\nImplementation details and requirements:\n- Start policy iteration from the zero policy $u \\equiv 0$ on interior nodes.\n- Use tolerance $\\varepsilon = 10^{-6}$ and maximum iterations $K_{\\max} = 100$.\n- All computations are nondimensional; no physical units are required.\n- Your program must be a single, complete script that carries out the entire computation for all three tests without user input and prints a single line containing the results.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the tests, for example $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$, where each entry is the floating-point value of $\\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty}$ for the corresponding test, computed on the state grid.",
            "solution": "### 1. Hamilton-Jacobi-Bellman Equation and Exact Solution\n\nThe state process is governed by the linear stochastic differential equation:\n$$\ndX_t = \\left(a X_t + b u_t\\right) dt + \\sigma dW_t\n$$\nwhere $a$, $b$, and $\\sigma$ are real constants, $u_t \\in \\mathbb{R}$ is the control, and $W_t$ is a Wiener process. The objective is to minimize the discounted cost functional with a discount rate $\\rho > 0$ and a quadratic running cost $\\ell(x, u) = q x^2 + r u^2$:\n$$\nJ^u(x) = \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} \\ell\\left(X_t^u, u_t\\right) dt \\;\\middle|\\; X_0 = x \\right]\n$$\nThe value function $V(x) = \\inf_u J^u(x)$ satisfies the stationary Hamilton-Jacobi-Bellman (HJB) equation:\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\mathcal{L}^u V(x) + \\ell(x, u) \\right\\}\n$$\nwhere $\\mathcal{L}^u$ is the infinitesimal generator of the process $X_t$, given by $\\mathcal{L}^u V(x) = (a x + b u) V'(x) + \\frac{1}{2} \\sigma^2 V''(x)$. Substituting the expressions for the generator and running cost, the HJB equation becomes:\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ (a x + b u) V'(x) + \\frac{1}{2} \\sigma^2 V''(x) + q x^2 + r u^2 \\right\\}\n$$\nThe infimum over $u$ is found by setting the partial derivative of the term in braces with respect to $u$ to zero:\n$$\nb V'(x) + 2 r u = 0 \\implies u^*(x) = -\\frac{b}{2r} V'(x)\n$$\nSubstituting this optimal control $u^*(x)$ back into the HJB equation yields a nonlinear ordinary differential equation for $V(x)$:\n$$\n\\rho V(x) = a x V'(x) - \\frac{b^2}{4r} (V'(x))^2 + \\frac{1}{2} \\sigma^2 V''(x) + q x^2\n$$\nFor this linear-quadratic problem, the value function is known to be a quadratic function of the state, $V(x) = P x^2 + C$. Its derivatives are $V'(x) = 2 P x$ and $V''(x) = 2 P$. Substituting these into the HJB equation gives:\n$$\n\\rho (P x^2 + C) = a x (2 P x) - \\frac{b^2}{4r} (2 P x)^2 + \\frac{1}{2} \\sigma^2 (2 P) + q x^2\n$$\n$$\n\\rho P x^2 + \\rho C = (2aP - \\frac{b^2}{r}P^2 + q) x^2 + \\sigma^2 P\n$$\nEquating coefficients of like powers of $x$, we obtain two algebraic equations for the unknown constants $P$ and $C$. The coefficient of $x^2$ yields the continuous-time algebraic Riccati equation (ARE):\n$$\n\\frac{b^2}{r} P^2 - (2a - \\rho) P - q = 0\n$$\nSince the cost must be positive definite, we require $P > 0$. The ARE is a quadratic equation for $P$ and admits a unique positive solution, which is given by:\n$$\nP = \\frac{(2a - \\rho) + \\sqrt{(2a - \\rho)^2 + 4(b^2/r)q}}{2(b^2/r)}\n$$\nEquating the constant terms gives the equation for $C$:\n$$\n\\rho C = \\sigma^2 P \\implies C = \\frac{\\sigma^2 P}{\\rho}\n$$\nThis defines the exact reference solution $V^{\\mathrm{ref}}(x) = Px^2 + C$ on the infinite domain.\n\n### 2. Discretization\n\nThe problem is solved on a bounded domain $x \\in [-x_{\\max}, x_{\\max}]$, which is discretized into a uniform grid of $N$ points $\\{x_i\\}_{i=0}^{N-1}$ with spacing $h = 2x_{\\max}/(N-1)$. Let $V_i$ be the numerical approximation of $V(x_i)$. At an interior grid point $x_i$ for $i \\in \\{1, \\dots, N-2\\}$, the derivatives are approximated using central finite differences:\n$$\nV'(x_i) \\approx \\frac{V_{i+1} - V_{i-1}}{2h}, \\qquad V''(x_i) \\approx \\frac{V_{i+1} - 2V_i + V_{i-1}}{h^2}\n$$\nFor a fixed policy $u(x)$, represented discretely as $u_i = u(x_i)$, the HJB equation is linear in $V$. The discretized form at each interior point $x_i$ is:\n$$\n\\rho V_i = (a x_i + b u_i) \\left(\\frac{V_{i+1} - V_{i-1}}{2h}\\right) + \\frac{1}{2}\\sigma^2 \\left(\\frac{V_{i+1} - 2V_i + V_{i-1}}{h^2}\\right) + q x_i^2 + r u_i^2\n$$\nRearranging terms, we obtain a linear equation for $V_{i-1}, V_i, V_{i+1}$:\n$$\n\\left( \\frac{a x_i + b u_i}{2h} - \\frac{\\sigma^2}{2h^2} \\right) V_{i-1} + \\left( \\rho + \\frac{\\sigma^2}{h^2} \\right) V_i - \\left( \\frac{a x_i + b u_i}{2h} + \\frac{\\sigma^2}{2h^2} \\right) V_{i+1} = q x_i^2 + r u_i^2\n$$\nThe boundary conditions are of Dirichlet type, fixed at the values from the reference solution: $V_0 = V^{\\mathrm{ref}}(-x_{\\max})$ and $V_{N-1} = V^{\\mathrm{ref}}(x_{\\max})$.\n\n### 3. Policy Iteration Algorithm\n\nPolicy iteration is an iterative method that alternates between two steps: policy evaluation and policy improvement.\n\n**Initialization**: The algorithm starts with an initial policy, which is set to the zero policy, $u^{(0)}(x_i) = 0$ for all interior grid points $x_i$. The initial value function $V^{(0)}$ is initialized to zero, with boundary values set from the reference solution.\n\n**Policy Evaluation**: For a given policy $u^{(k)}$, we solve for the corresponding value function $V^{(k+1)}$. The set of discretized HJB equations for all interior points $i \\in \\{1, \\dots, N-2\\}$ forms a tridiagonal linear system of equations for the unknown values $\\{V_i^{(k+1)}\\}_{i=1}^{N-2}$. This system, of the form $\\mathbf{A}\\mathbf{V} = \\mathbf{d}$, is solved using a standard linear algebra solver. The matrix $\\mathbf{A}$ and vector $\\mathbf{d}$ depend on the current policy $u^{(k)}$.\n\n**Policy Improvement**: With the new value function $V^{(k+1)}$ computed, the policy is updated to $u^{(k+1)}$ by minimizing the discrete Hamiltonian at each interior grid point $x_i$:\n$$\nu_i^{(k+1)} = \\arg\\min_{u \\in \\mathcal{U}_M} \\left\\{ (a x_i + b u) \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h} + \\frac{1}{2}\\sigma^2 (\\dots) + q x_i^2 + r u^2 \\right\\}\n$$\nThis is equivalent to minimizing $r u^2 + b u \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h}$ over the discrete action set $\\mathcal{U}_M$. The unconstrained minimizer is $u_i^* = -\\frac{b}{2r} \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h}$. The new policy action $u_i^{(k+1)}$ is then chosen as the element in the discrete action set $\\mathcal{U}_M$ that is closest to $u_i^*$.\n\n**Termination**: The iteration between evaluation and improvement continues until the policy and value function converge. The process terminates when the maximum absolute change in the value vector, $\\|V^{(k+1)} - V^{(k)}\\|_{\\infty}$, and the maximum absolute change in the policy vector, $\\|u^{(k+1)} - u^{(k)}\\|_{\\infty}$, are both below a specified tolerance $\\varepsilon = 10^{-6}$, or when a maximum number of iterations $K_{\\max} = 100$ is reached.\n\n### 4. Error Calculation\n\nAfter the policy iteration converges to a final numerical solution $V_{\\mathrm{num}}$, its accuracy is assessed by computing the uniform norm of the error against the exact reference solution $V^{\\mathrm{ref}}$ on the computational grid:\n$$\n\\text{Error} = \\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty} = \\max_{i \\in \\{0, \\dots, N-1\\}} | V_{\\mathrm{num}}(x_i) - V^{\\mathrm{ref}}(x_i) |\n$$\nThis quantity is computed for each provided test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the policy iteration for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (a, b, q, r, sigma, rho, x_max, N, M)\n        (-0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 3.0, 161, 61),\n        (0.2, 1.0, 1.0, 0.5, 0.1, 1.0, 2.0, 161, 61),\n        (0.3, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 161, 61),\n    ]\n\n    results = []\n    for params in test_cases:\n        error = run_policy_iteration(params)\n        results.append(error)\n\n    # Format and print the final output as a single line\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef run_policy_iteration(params):\n    \"\"\"\n    Solves the HJB equation for a single set of parameters using policy iteration.\n    \"\"\"\n    a, b, q, r, sigma, rho, x_max, N, M = params\n    \n    # --- Step A: Pre-computation and Setup ---\n\n    # 1. Solve the Algebraic Riccati Equation for P\n    # The ARE is (b^2/r)P^2 - (2a - rho)P - q = 0\n    A_ric = b**2 / r\n    B_ric = -(2.0*a - rho)\n    C_ric = -q\n    discriminant = B_ric**2 - 4.0 * A_ric * C_ric\n    # We need the positive root for P\n    P = (-B_ric + np.sqrt(discriminant)) / (2.0 * A_ric)\n\n    # 2. Calculate the constant C\n    C = (sigma**2 * P) / rho\n\n    # 3. Define the reference (exact) value function\n    def v_ref(x):\n        return P * x**2 + C\n\n    # 4. Set up the state and action grids\n    x_grid = np.linspace(-x_max, x_max, N)\n    h = x_grid[1] - x_grid[0]\n    v_ref_grid = v_ref(x_grid)\n\n    kappa = 1.25\n    u_max_val = kappa * abs(b * P * x_max / r)\n    \n    if M > 1 and u_max_val > 0:\n        u_grid = np.linspace(-u_max_val, u_max_val, M)\n    else:\n        u_grid = np.zeros(M)\n    u_step = u_grid[1] - u_grid[0] if M > 1 else 0\n\n    # 5. Set up iteration parameters\n    tol = 1e-6\n    max_iter = 100\n    \n    # --- Step B: Policy Iteration Loop ---\n\n    # Initialization\n    # Policy for interior points (size N-2)\n    current_policy = np.zeros(N - 2)\n    # Value function on the full grid (size N)\n    current_V = np.zeros(N)\n    # Set Dirichlet boundary conditions from the exact solution\n    current_V[0] = v_ref_grid[0]\n    current_V[-1] = v_ref_grid[-1]\n    \n    x_interior = x_grid[1:-1]\n    \n    for k in range(max_iter):\n        \n        # --- 1. Policy Evaluation ---\n        # Solve the linear system A * V_interior = d for V\n        \n        drift_coeff = a * x_interior + b * current_policy\n        \n        # Coefficients of the tridiagonal system\n        L = drift_coeff / (2.0 * h) - sigma**2 / (2.0 * h**2)\n        D = rho + sigma**2 / h**2\n        U = -drift_coeff / (2.0 * h) - sigma**2 / (2.0 * h**2)\n        \n        # Construct the (N-2) x (N-2) system matrix A\n        A = np.diag(D * np.ones(N-2)) + np.diag(U[:-1], k=1) + np.diag(L[1:], k=-1)\n        \n        # Construct the right-hand side vector d\n        d = q * x_interior**2 + r * current_policy**2\n        \n        # Adjust d for boundary conditions\n        d[0] -= L[0] * v_ref_grid[0]\n        d[-1] -= U[-1] * v_ref_grid[-1]\n        \n        # Solve for the new interior values of V\n        try:\n            V_interior_new = np.linalg.solve(A, d)\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix, although not expected here\n            return np.inf\n\n        # Form the full new value function vector\n        V_new = np.concatenate(([v_ref_grid[0]], V_interior_new, [v_ref_grid[-1]]))\n        \n        # --- 2. Policy Improvement ---\n        # Update the policy by minimizing the Hamiltonian\n        \n        # Approximate V'(x) at interior points\n        V_prime_interior = (V_new[2:] - V_new[:-2]) / (2.0 * h)\n        \n        # Compute the unconstrained optimal control\n        u_star_interior = -b / (2.0 * r) * V_prime_interior\n        \n        # Find the closest control in the discrete action space\n        new_policy = np.zeros_like(current_policy)\n        if M > 1 and u_step > 0:\n            indices = np.round((u_star_interior - u_grid[0]) / u_step)\n            indices = np.clip(indices, 0, M - 1).astype(int)\n            new_policy = u_grid[indices]\n            \n        # --- 3. Termination Check ---\n        \n        val_change = np.max(np.abs(V_new - current_V))\n        pol_change = np.max(np.abs(new_policy - current_policy))\n        \n        # Update for the next iteration\n        current_V = V_new\n        current_policy = new_policy\n        \n        if val_change < tol and pol_change < tol:\n            break\n            \n    # --- Step C: Final Error Calculation ---\n    \n    # The converged numerical solution is the last computed value function\n    V_num = current_V\n    error = np.max(np.abs(V_num - v_ref_grid))\n    \n    return error\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}