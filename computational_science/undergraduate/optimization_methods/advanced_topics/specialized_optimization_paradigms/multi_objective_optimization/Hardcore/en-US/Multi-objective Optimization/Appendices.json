{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of multi-objective optimization is generating the set of optimal trade-offs, known as the Pareto front. This first exercise provides a foundational, first-principles look at how this is achieved for continuous and convex problems using the Weighted Sum method. By analytically deriving the Pareto front for a simple bi-objective problem, you will build a concrete understanding of how adjusting the weights in a scalarized objective function allows you to trace the curve of optimal solutions.",
            "id": "3154179",
            "problem": "Consider the bi-objective minimization problem in one decision variable $x \\in \\mathbb{R}$ with objectives $f_{1}(x) = (x - 1)^{2} + 1$ and $f_{2}(x) = (x + 1)^{2} + 1$. The task is to characterize the trade-off between these two objectives using first principles from multi-objective optimization. \n\nStarting from the definition of Pareto optimality and the principle that, for convex problems, minimizing a positively weighted sum of objectives generates Pareto-optimal points, use the Weighted Sum (WS) scalarization $J_{\\alpha}(x) = \\alpha f_{1}(x) + (1 - \\alpha) f_{2}(x)$ for a weight $\\alpha \\in [0, 1]$ to derive analytically the parametric form of the Pareto front in objective space. Your derivation must begin from first-order optimality conditions for convex functions and the definition of Pareto dominance.\n\nNext, consider the Hypervolume (HV) indicator for a finite set in objective space under minimization, defined as the Lebesgue measure of the region weakly dominated by the set and bounded above by a reference point that is strictly worse (componentwise larger) than every point in the set. Let the reference point be $r = (6, 6)$, and for each $\\alpha \\in [0, 1]$, let $x^{\\star}(\\alpha)$ be a minimizer of $J_{\\alpha}(x)$. Treat the singleton set $\\{(f_{1}(x^{\\star}(\\alpha)), f_{2}(x^{\\star}(\\alpha)))\\}$ as the approximation of the Pareto front at weight $\\alpha$.\n\nUsing only the definitions and first-order conditions described above, derive a closed-form expression for the hypervolume $HV(\\alpha)$ of this singleton set with respect to the reference point $r = (6, 6)$, expressed as a function of $\\alpha$. Provide your final expression for $HV(\\alpha)$ in exact, simplified analytic form. Do not round your answer. The final answer must be a single expression.",
            "solution": "The user wants me to solve a multi-objective optimization problem.\n\n### Step 1: Extract Givens\n- **Decision Variable:** $x \\in \\mathbb{R}$.\n- **Objective Functions (Minimization):**\n  - $f_{1}(x) = (x - 1)^{2} + 1$\n  - $f_{2}(x) = (x + 1)^{2} + 1$\n- **Scalarization Method:** Weighted Sum (WS) $J_{\\alpha}(x) = \\alpha f_{1}(x) + (1 - \\alpha) f_{2}(x)$.\n- **Weight Parameter:** $\\alpha \\in [0, 1]$.\n- **Optimization Principle:** Minimizing the convex scalarization $J_{\\alpha}(x)$ generates Pareto-optimal points. The derivation must use first-order optimality conditions.\n- **Hypervolume (HV) Indicator Definition:** For a finite set in objective space and a reference point $r$, the HV is the Lebesgue measure of the region weakly dominated by the set and bounded above by $r$.\n- **Reference Point:** $r = (6, 6)$.\n- **Set for HV Calculation:** The singleton set $\\{(f_{1}(x^{\\star}(\\alpha)), f_{2}(x^{\\star}(\\alpha)))\\}$, where $x^{\\star}(\\alpha)$ is a minimizer of $J_{\\alpha}(x)$.\n- **Final Task:** Derive a closed-form expression for the hypervolume $HV(\\alpha)$ as a function of $\\alpha$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Soundness:** The problem is grounded in the standard theory of multi-objective optimization. The concepts of Pareto optimality, the Weighted Sum method for convex problems, and the Hypervolume indicator are all well-established in the field. The provided functions are simple convex quadratics, which are appropriate for this type of analysis. The problem is scientifically and mathematically sound.\n2.  **Non-Formalizable or Irrelevant:** The problem is highly formal and directly related to the topic of multi-objective optimization.\n3.  **Incomplete or Contradictory Setup:** All necessary information is provided. The objectives, constraints, scalarization method, and reference point are explicitly defined. There are no contradictions.\n4.  **Unrealistic or Infeasible:** The problem is a theoretical exercise; physical realism is not a relevant criterion, but the mathematical setup is perfectly feasible.\n5.  **Ill-Posed or Poorly Structured:** The problem is well-posed. The objective functions are convex, ensuring that the weighted sum has a unique minimizer for any $\\alpha \\in (0,1)$, leading to a well-defined Pareto front and a unique value for $HV(\\alpha)$.\n6.  **Pseudo-Profound, Trivial, or Tautological:** The problem requires a multi-step derivation involving calculus and the application of optimization definitions. It is a substantive exercise, not a trivial or tautological one.\n7.  **Outside Scientific Verifiability:** The derivation and final expression are mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\nThe bi-objective minimization problem involves two objective functions: $f_{1}(x) = (x - 1)^{2} + 1$ and $f_{2}(x) = (x + 1)^{2} + 1$. Both functions are convex on $\\mathbb{R}$. The task is to find a parametric representation of the Pareto front using the Weighted Sum (WS) method and then to derive an expression for the Hypervolume indicator as a function of the weight $\\alpha$.\n\nFirst, we define the WS scalarization, which combines the two objectives into a single objective function $J_{\\alpha}(x)$ using a weight $\\alpha \\in [0, 1]$:\n$$J_{\\alpha}(x) = \\alpha f_{1}(x) + (1 - \\alpha) f_{2}(x)$$\nSubstituting the expressions for $f_{1}(x)$ and $f_{2}(x)$:\n$$J_{\\alpha}(x) = \\alpha ((x - 1)^{2} + 1) + (1 - \\alpha) ((x + 1)^{2} + 1)$$\nSince $f_{1}(x)$ and $f_{2}(x)$ are convex functions and $\\alpha \\ge 0$, $1 - \\alpha \\ge 0$, their non-negative weighted sum $J_{\\alpha}(x)$ is also a convex function. For a convex function, the minimum can be found by setting its first derivative with respect to $x$ to zero. This is the first-order optimality condition.\n$$\\frac{dJ_{\\alpha}(x)}{dx} = \\frac{d}{dx} \\left[ \\alpha (x - 1)^{2} + \\alpha + (1 - \\alpha) (x + 1)^{2} + (1 - \\alpha) \\right] = 0$$\n$$\\frac{dJ_{\\alpha}(x)}{dx} = \\alpha [2(x - 1)] + (1 - \\alpha) [2(x + 1)] = 0$$\nDividing by $2$, we get:\n$$\\alpha (x - 1) + (1 - \\alpha) (x + 1) = 0$$\n$$\\alpha x - \\alpha + x + 1 - \\alpha x - \\alpha = 0$$\n$$x + 1 - 2\\alpha = 0$$\nSolving for $x$, we find the optimal decision variable $x^{\\star}$ as a function of $\\alpha$:\n$$x^{\\star}(\\alpha) = 2\\alpha - 1$$\nThis expression gives the Pareto-optimal solution in the decision space for each $\\alpha \\in [0, 1]$. The set of all such solutions, $\\{x | x = 2\\alpha - 1, \\alpha \\in [0, 1]\\}$, corresponds to the interval $[-1, 1]$.\n\nNext, we map these optimal solutions into the objective space to find the Pareto front. The Pareto front is the set of objective vectors $(f_{1}(x^{\\star}(\\alpha)), f_{2}(x^{\\star}(\\alpha)))$ for $\\alpha \\in [0, 1]$. Let's define the point on the Pareto front as $p(\\alpha) = (p_1(\\alpha), p_2(\\alpha))$.\n$$p_1(\\alpha) = f_{1}(x^{\\star}(\\alpha)) = f_{1}(2\\alpha - 1) = ((2\\alpha - 1) - 1)^{2} + 1 = (2\\alpha - 2)^{2} + 1 = 4(\\alpha - 1)^{2} + 1$$\n$$p_2(\\alpha) = f_{2}(x^{\\star}(\\alpha)) = f_{2}(2\\alpha - 1) = ((2\\alpha - 1) + 1)^{2} + 1 = (2\\alpha)^{2} + 1 = 4\\alpha^{2} + 1$$\nThe parametric form of the Pareto front is $(4(\\alpha - 1)^{2} + 1, 4\\alpha^{2} + 1)$ for $\\alpha \\in [0, 1]$.\n\nThe second part of the problem asks for the Hypervolume (HV) indicator of the singleton set $\\{p(\\alpha)\\}$ with respect to the reference point $r = (6, 6)$. The HV, in this two-dimensional case, corresponds to the area of the hyperrectangle formed by the point $p(\\alpha)$ and the reference point $r$.\nThe formula for the HV of a point $(p_1, p_2)$ with respect to a reference point $(r_1, r_2)$ is:\n$$HV = (r_1 - p_1)(r_2 - p_2)$$\nWe must first verify that the reference point $r = (6, 6)$ strictly dominates all points on the Pareto front.\nFor $\\alpha \\in [0, 1]$, the maximum value of $p_1(\\alpha) = 4(\\alpha - 1)^{2} + 1$ occurs at $\\alpha = 0$, giving $p_1(0) = 5$. The maximum value of $p_2(\\alpha) = 4\\alpha^{2} + 1$ occurs at $\\alpha = 1$, giving $p_2(1) = 5$. Thus, for any point $(p_1, p_2)$ on the front, $p_1 \\le 5$ and $p_2 \\le 5$. Since $r_1=6$ and $r_2=6$, the reference point is indeed a valid upper bound.\n\nNow, we compute $HV(\\alpha)$ as a function of $\\alpha$:\n$$HV(\\alpha) = (6 - p_1(\\alpha))(6 - p_2(\\alpha))$$\nWe need to express $p_1(\\alpha)$ and $p_2(\\alpha)$ as polynomials in $\\alpha$:\n$$p_1(\\alpha) = 4(\\alpha^{2} - 2\\alpha + 1) + 1 = 4\\alpha^{2} - 8\\alpha + 5$$\n$$p_2(\\alpha) = 4\\alpha^{2} + 1$$\nSubstitute these into the HV formula:\n$$HV(\\alpha) = (6 - (4\\alpha^{2} - 8\\alpha + 5))(6 - (4\\alpha^{2} + 1))$$\n$$HV(\\alpha) = (1 + 8\\alpha - 4\\alpha^{2})(5 - 4\\alpha^{2})$$\nNow, we expand the product to obtain a simplified polynomial expression for $HV(\\alpha)$:\n$$HV(\\alpha) = 1(5) + 1(-4\\alpha^{2}) + 8\\alpha(5) + 8\\alpha(-4\\alpha^{2}) - 4\\alpha^{2}(5) - 4\\alpha^{2}(-4\\alpha^{2})$$\n$$HV(\\alpha) = 5 - 4\\alpha^{2} + 40\\alpha - 32\\alpha^{3} - 20\\alpha^{2} + 16\\alpha^{4}$$\nFinally, we collect terms by descending powers of $\\alpha$:\n$$HV(\\alpha) = 16\\alpha^{4} - 32\\alpha^{3} + (-4 - 20)\\alpha^{2} + 40\\alpha + 5$$\n$$HV(\\alpha) = 16\\alpha^{4} - 32\\alpha^{3} - 24\\alpha^{2} + 40\\alpha + 5$$\nThis is the closed-form expression for the hypervolume of the singleton Pareto point approximation as a function of the weight $\\alpha$.",
            "answer": "$$\\boxed{16\\alpha^{4} - 32\\alpha^{3} - 24\\alpha^{2} + 40\\alpha + 5}$$"
        },
        {
            "introduction": "While the Weighted Sum method is powerful, its direct application can be misleading when objectives have different units or scales—a common scenario in real-world engineering and economic problems. This hands-on coding practice demonstrates how an unscaled approach can unintentionally favor one objective, leading to a skewed exploration of the solution space. By implementing and comparing results with and without objective normalization, you will grasp a critical and practical technique for ensuring a balanced and meaningful trade-off analysis.",
            "id": "3162727",
            "problem": "Consider a two-objective minimization problem with decision variable $x$ and objectives $f_1(x)$ and $f_2(x)$ that are evaluated over a finite candidate set $X$. Pareto dominance is defined componentwise: a decision $x_a$ dominates a decision $x_b$ if and only if $f_i(x_a) \\le f_i(x_b)$ for all $i \\in \\{1,2\\}$ and there exists at least one index $j \\in \\{1,2\\}$ such that $f_j(x_a)  f_j(x_b)$. The Pareto set is the subset of $X$ consisting of all decisions not dominated by any other decision in $X$. Normalization is performed using the componentwise utopia and nadir points computed over the candidate set $X$, namely $z_i^\\text{utopia} = \\min_{x \\in X} f_i(x)$ and $z_i^\\text{nadir} = \\max_{x \\in X} f_i(x)$, and the normalized objectives are defined by\n$$\nf_i'(x) = \\frac{f_i(x) - z_i^\\text{utopia}}{z_i^\\text{nadir} - z_i^\\text{utopia}}, \\quad i \\in \\{1,2\\}.\n$$\nYour task is to study how normalization affects the Pareto set and the selection produced by an equal-weight linear scalarization. For each test case below, you must:\n(1) Construct the candidate set $X$ as specified, and compute $f_1(x)$ and $f_2(x)$ for all $x \\in X$.\n(2) Compute $z_i^\\text{utopia}$ and $z_i^\\text{nadir}$ from the objective values on $X$ and form the normalized objectives $f_i'(x)$ for all $x \\in X$.\n(3) Determine the Pareto set in $X$ using the original objectives $(f_1,f_2)$ and the Pareto set using the normalized objectives $(f_1',f_2')$. Report whether these two Pareto sets are identical, using strict Pareto dominance as defined above.\n(4) Using equal weights $\\mathbf{w} = (1,1)$, compute the minimizers in $X$ of the unnormalized scalarization $g(x) = w_1 f_1(x) + w_2 f_2(x)$ and of the normalized scalarization $g'(x) = w_1 f_1'(x) + w_2 f_2'(x)$. If multiple minimizers exist, select the smallest $x$ among them.\nFor each test case, produce the result as the list $[\\text{pareto\\_equal}, \\text{pareto\\_count}, x^\\star_{\\text{unnorm}}, x^\\star_{\\text{norm}}]$, where $\\text{pareto\\_equal}$ is a boolean indicating whether the Pareto sets are identical, $\\text{pareto\\_count}$ is the integer count of points in the Pareto set (using the original objectives), and $x^\\star_{\\text{unnorm}}$ and $x^\\star_{\\text{norm}}$ are floats giving the selected minimizers of $g$ and $g'$ respectively. Your program should aggregate the results of all test cases into a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result itself formatted as a comma-separated list enclosed in square brackets, for example $[[\\dots],[\\dots],[\\dots]]$.\nUse the following three scientifically plausible test cases that highlight different facets of normalization:\nTest Case A (scale disparity with strictly monotonic conflict):\n- Candidate set: $X = \\{0, 0.01, 0.02, \\dots, 1.00\\}$.\n- Objectives: $f_1(x) = 1000 x$, $f_2(x) = 1 - \\sqrt{x}$.\nTest Case B (scale disparity with non-monotonic second objective):\n- Candidate set: $X = \\{0, 0.01, 0.02, \\dots, 1.00\\}$.\n- Objectives: $f_1(x) = 1000 x^2$, $f_2(x) = (x - 0.6)^2$.\nTest Case C (coarse grid edge case):\n- Candidate set: $X = \\{0, 0.5, 1.0\\}$.\n- Objectives: $f_1(x) = 1000 x$, $f_2(x) = 1 - x$.\nAll quantities are dimensionless. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[[\\text{result}_1],[\\text{result}_2],[\\text{result}_3]]$). The results for each test case must follow the format described above and consist only of booleans, integers, and floats.",
            "solution": "The user wants me to analyze a two-objective minimization problem, focusing on the effects of normalization on the Pareto set and on the solution obtained via linear scalarization. The analysis must be performed for three specific test cases.\n\nThe problem is first validated for correctness and solvability.\n\n### Step 1: Extract Givens\n- **Decision Variable**: $x$\n- **Candidate Set**: A finite set $X$.\n- **Objectives**: Two functions to be minimized, $f_1(x)$ and $f_2(x)$.\n- **Pareto Dominance**: A decision $x_a$ dominates $x_b$ if and only if $f_i(x_a) \\le f_i(x_b)$ for all $i \\in \\{1,2\\}$ and there exists at least one index $j \\in \\{1,2\\}$ such that $f_j(x_a)  f_j(x_b)$.\n- **Pareto Set**: The subset of $X$ containing all decisions not dominated by any other decision in $X$.\n- **Normalization**:\n    - Utopia point: $z_i^\\text{utopia} = \\min_{x \\in X} f_i(x)$.\n    - Nadir point: $z_i^\\text{nadir} = \\max_{x \\in X} f_i(x)$.\n    - Normalized objective: $f_i'(x) = \\frac{f_i(x) - z_i^\\text{utopia}}{z_i^\\text{nadir} - z_i^\\text{utopia}}$ for $i \\in \\{1,2\\}$.\n- **Linear Scalarization**:\n    - Weights: $\\mathbf{w} = (w_1, w_2) = (1,1)$.\n    - Unnormalized scalarization: $g(x) = w_1 f_1(x) + w_2 f_2(x)$.\n    - Normalized scalarization: $g'(x) = w_1 f_1'(x) + w_2 f_2'(x)$.\n- **Tie-Breaking Rule**: If multiple minimizers exist for a scalarization, select the one with the smallest value of $x$.\n- **Test Cases**:\n    - **A**: $X = \\{0, 0.01, \\dots, 1.00\\}$, $f_1(x) = 1000 x$, $f_2(x) = 1 - \\sqrt{x}$.\n    - **B**: $X = \\{0, 0.01, \\dots, 1.00\\}$, $f_1(x) = 1000 x^2$, $f_2(x) = (x - 0.6)^2$.\n    - **C**: $X = \\{0, 0.5, 1.0\\}$, $f_1(x) = 1000 x$, $f_2(x) = 1 - x$.\n- **Required Output per Case**: A list $[\\text{pareto\\_equal}, \\text{pareto\\_count}, x^\\star_{\\text{unnorm}}, x^\\star_{\\text{norm}}]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It deals with standard, well-defined concepts from multi-objective optimization: Pareto dominance, normalization, and linear scalarization. The problem is self-contained, providing all necessary definitions, functions, and data sets. The candidate sets $X$ are finite, and the objective functions are well-behaved over their respective domains, ensuring that the utopia and nadir points are well-defined. The normalization denominators, $z_i^\\text{nadir} - z_i^\\text{utopia}$, are non-zero for all specified test cases, as none of the objective functions are constant over the candidate set $X$. The tie-breaking rule ensures a unique solution for the scalarized minimizations. The problem is well-posed and does not violate any scientific principles or contain logical contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be developed.\n\n### Principle-Based Solution\nThe core task is to analyze how normalization affects Pareto optimality and the outcome of an equal-weight linear scalarization method. We will first establish a theoretical principle regarding the Pareto set and then outline the computational algorithm to determine the required quantities for each test case.\n\n**1. Invariance of the Pareto Set under Normalization**\n\nThe normalization specified is a component-wise affine transformation of the form $f_i'(x) = a_i f_i(x) + b_i$, where:\n$$ a_i = \\frac{1}{z_i^\\text{nadir} - z_i^\\text{utopia}} \\quad \\text{and} \\quad b_i = \\frac{-z_i^\\text{utopia}}{z_i^\\text{nadir} - z_i^\\text{utopia}} $$\nFor a non-constant objective function over the candidate set $X$, we have $z_i^\\text{nadir}  z_i^\\text{utopia}$, which implies $a_i  0$. An affine transformation with a positive scaling factor ($a_i  0$) is strictly increasing.\n\nLet's examine how this transformation affects the Pareto dominance relation. Suppose a decision $x_a$ dominates a decision $x_b$ in the original objective space. By definition:\n1. $f_i(x_a) \\le f_i(x_b)$ for $i \\in \\{1,2\\}$.\n2. $f_j(x_a)  f_j(x_b)$ for at least one $j \\in \\{1,2\\}$.\n\nSince $a_i  0$, we can multiply the inequalities by $a_i$ without changing their direction:\n1. $a_i f_i(x_a) \\le a_i f_i(x_b)$ for $i \\in \\{1,2\\}$.\n2. $a_j f_j(x_a)  a_j f_j(x_b)$ for at least one $j \\in \\{1,2\\}$.\n\nAdding the constant $b_i$ to both sides also preserves the inequalities:\n1. $a_i f_i(x_a) + b_i \\le a_i f_i(x_b) + b_i \\implies f_i'(x_a) \\le f_i'(x_b)$ for $i \\in \\{1,2\\}$.\n2. $a_j f_j(x_a) + b_j  a_j f_j(x_b) + b_j \\implies f_j'(x_a)  f_j'(x_b)$ for at least one $j \\in \\{1,2\\}$.\n\nThis shows that $x_a$ dominates $x_b$ in the normalized objective space. The reverse implication holds by the same logic. Therefore, the dominance relations are identical for the original and normalized objectives. Consequently, the set of non-dominated solutions—the Pareto set—is invariant under this normalization. This means the boolean value for `pareto_equal` will be `True` for all test cases.\n\n**2. Algorithmic Procedure**\n\nFor each test case, the following procedure is executed:\n- **Step 2.1: Evaluation.** The candidate set $X$ is generated. For each $x \\in X$, the objective vectors $\\mathbf{f}(x) = (f_1(x), f_2(x))$ are computed and stored.\n- **Step 2.2: Pareto Set Identification.** The Pareto set is identified by pairwise comparison. For each candidate solution $x_i$, it is compared against every other candidate $x_j$. If any $x_j$ dominates $x_i$ (according to the definition), $x_i$ is marked as dominated. The set of all candidates not marked as dominated constitutes the Pareto set. The size of this set is `pareto_count`.\n- **Step 2.3: Normalization.** The utopia and nadir points, $z_i^\\text{utopia}$ and $z_i^\\text{nadir}$, are found by taking the minimum and maximum of the evaluated objective values for each objective $i \\in \\{1,2\\}$. The normalized objectives $f_i'(x)$ are then calculated for all $x \\in X$. As established, this does not alter the Pareto set.\n- **Step 2.4: Scalarization and Minimization.** The unnormalized and normalized scalarized objective functions are computed for all $x \\in X$:\n$$ g(x) = f_1(x) + f_2(x) $$\n$$ g'(x) = f_1'(x) + f_2'(x) $$\nThe minimum value of each function, $\\min_{x \\in X} g(x)$ and $\\min_{x \\in X} g'(x)$, is found. The set of all $x$ values that achieve this minimum is identified. According to the tie-breaking rule, the smallest $x$ from this set is selected as the minimizer, yielding $x^\\star_{\\text{unnorm}}$ and $x^\\star_{\\text{norm}}$.\n\n**3. Application to Test Cases**\n\n- **Test Case A**: $f_1(x) = 1000 x$ and $f_2(x) = 1 - \\sqrt{x}$. On $X = \\{0, 0.01, \\dots, 1.00\\}$, $f_1(x)$ is strictly increasing while $f_2(x)$ is strictly decreasing. This creates a direct conflict, meaning no solution dominates another. The entire set $X$ of $101$ points is the Pareto set. The large scale of $f_1$ dominates the unnormalized scalarization $g(x) = 1000x + 1 - \\sqrt{x}$, pushing its minimum towards $x=0$. After normalization, the objectives $f_1'(x) = x$ and $f_2'(x) = 1 - \\sqrt{x}$ are on a comparable scale. The minimum of $g'(x) = x + 1 - \\sqrt{x}$ is found at $x=0.25$.\n- **Test Case B**: $f_1(x) = 1000 x^2$ and $f_2(x) = (x - 0.6)^2$. For $x \\in [0, 0.6]$, $f_1(x)$ increases while $f_2(x)$ decreases, forming a Pareto front. For $x  0.6$, both $f_1(x)$ and $f_2(x)$ increase, so any solution with $x  0.6$ is dominated by the solution at $x=0.6$. The Pareto set is thus $\\{0, 0.01, \\dots, 0.60\\}$, containing $61$ points. The unnormalized scalarization $g(x)$ is again dominated by the high-magnitude $f_1$ term, placing the minimum at $x=0$. Normalization balances the scales, and the minimum of the normalized scalarization $g'(x)$ is found near the analytical minimum of $x \\approx 0.441$, which corresponds to $x=0.44$ on the discrete grid.\n- **Test Case C**: $f_1(x) = 1000 x$ and $f_2(x) = 1 - x$ on the coarse grid $X=\\{0, 0.5, 1.0\\}$. The points are $(0,1)$, $(500, 0.5)$, and $(1000,0)$. All three are mutually non-dominating, so the Pareto set has $3$ points. The unnormalized scalarization $g(x) = 999x + 1$ is minimized at $x=0$. The normalization perfectly balances the objectives, yielding $f_1'(x)=x$ and $f_2'(x)=1-x$. The normalized scalarization becomes $g'(x) = x + (1-x) = 1$, which is constant for all $x \\in X$. All three points are minimizers. The tie-breaking rule selects the smallest $x$, so $x^\\star_{\\text{norm}} = 0$.\n\nThis analysis provides the basis for the computational implementation that follows.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multi-objective optimization problem for three test cases.\n    \"\"\"\n\n    def find_pareto_indices(objectives: np.ndarray) - set:\n        \"\"\"\n        Identifies the indices of Pareto optimal solutions in a set.\n\n        Args:\n            objectives: An (n_candidates, n_objectives) numpy array of objective values.\n\n        Returns:\n            A set of integer indices corresponding to the Pareto optimal solutions.\n        \"\"\"\n        num_candidates = objectives.shape[0]\n        is_dominated = np.zeros(num_candidates, dtype=bool)\n\n        for i in range(num_candidates):\n            # Check if candidate i is dominated by any other candidate j\n            for j in range(num_candidates):\n                if i == j:\n                    continue\n                \n                # Strict Pareto dominance check:\n                # j dominates i if f_k(j) = f_k(i) for all objectives k\n                # and f_k(j)  f_k(i) for at least one objective k.\n                if np.all(objectives[j] = objectives[i]) and np.any(objectives[j]  objectives[i]):\n                    is_dominated[i] = True\n                    break  # i is dominated, no need to check other j's\n\n        pareto_indices = np.where(~is_dominated)[0]\n        return set(pareto_indices)\n\n    def process_case(x_values: np.ndarray, f1_func: callable, f2_func: callable) - list:\n        \"\"\"\n        Performs the full analysis for a single test case.\n\n        Args:\n            x_values: 1D numpy array of decision variable candidates.\n            f1_func: The first objective function.\n            f2_func: The second objective function.\n\n        Returns:\n            A list containing [pareto_equal, pareto_count, x_star_unnorm, x_star_norm].\n        \"\"\"\n        # (1) Compute objectives for all candidates\n        f1_values = f1_func(x_values)\n        f2_values = f2_func(x_values)\n        objectives = np.vstack([f1_values, f2_values]).T\n\n        # (2) Compute normalized objectives\n        z1_utopia, z1_nadir = np.min(f1_values), np.max(f1_values)\n        z2_utopia, z2_nadir = np.min(f2_values), np.max(f2_values)\n\n        # The problem statement ensures denominators are non-zero for these test cases.\n        denom1 = z1_nadir - z1_utopia if z1_nadir  z1_utopia else 1.0\n        denom2 = z2_nadir - z2_utopia if z2_nadir  z2_utopia else 1.0\n\n        f1_prime = (f1_values - z1_utopia) / denom1\n        f2_prime = (f2_values - z2_utopia) / denom2\n        normalized_objectives = np.vstack([f1_prime, f2_prime]).T\n\n        # (3) Determine Pareto sets and compare\n        original_pareto_indices = find_pareto_indices(objectives)\n        # As proven, normalization does not change the Pareto set.\n        # We can computationally verify this, but it must be true.\n        normalized_pareto_indices = find_pareto_indices(normalized_objectives)\n        \n        pareto_equal = (original_pareto_indices == normalized_pareto_indices)\n        pareto_count = len(original_pareto_indices)\n        \n        # (4) Compute minimizers of scalarizations (w = [1, 1])\n        # Unnormalized scalarization\n        g = f1_values + f2_values\n        min_g = np.min(g)\n        min_g_indices = np.where(np.isclose(g, min_g))[0]\n        # Tie-breaking: select smallest x\n        x_star_unnorm = x_values[min_g_indices[0]]\n\n        # Normalized scalarization\n        g_prime = f1_prime + f2_prime\n        min_g_prime = np.min(g_prime)\n        min_g_prime_indices = np.where(np.isclose(g_prime, min_g_prime))[0]\n        # Tie-breaking: select smallest x\n        x_star_norm = x_values[min_g_prime_indices[0]]\n\n        return [pareto_equal, pareto_count, float(x_star_unnorm), float(x_star_norm)]\n\n    # Define the test cases from the problem statement.\n    test_cases_defs = [\n        {\n            \"x_values\": np.linspace(0, 1.0, 101, dtype=float),\n            \"f1\": lambda x: 1000.0 * x,\n            \"f2\": lambda x: 1.0 - np.sqrt(x)\n        },\n        {\n            \"x_values\": np.linspace(0, 1.0, 101, dtype=float),\n            \"f1\": lambda x: 1000.0 * x**2,\n            \"f2\": lambda x: (x - 0.6)**2\n        },\n        {\n            \"x_values\": np.array([0.0, 0.5, 1.0], dtype=float),\n            \"f1\": lambda x: 1000.0 * x,\n            \"f2\": lambda x: 1.0 - x\n        }\n    ]\n\n    results = []\n    for case_def in test_cases_defs:\n        result = process_case(case_def[\"x_values\"], case_def[\"f1\"], case_def[\"f2\"])\n        results.append(result)\n    \n    # Format the results into a single string as specified\n    result_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]) + \"]\"\n\n    # Final print statement in the exact required format.\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Our focus now shifts from continuous problems to the realm of discrete combinatorial optimization, exemplified by the classic knapsack problem. While scalarization methods can find some optimal solutions, they may fail to identify the entire Pareto front for non-convex or discrete problems. This exercise guides you through deriving and implementing a more powerful technique—dynamic programming—to construct the *exact and complete* set of non-dominated solutions, revealing the true shape of the trade-off frontier for this class of problems.",
            "id": "3154185",
            "problem": "Consider the following bi-objective $0$-$1$ knapsack problem formulated as a minimization problem. There is a set of $n$ items indexed by $i \\in \\{1,\\dots,n\\}$. Each item $i$ has a nonnegative value $p_i \\in \\mathbb{R}_{\\ge 0}$ and a nonnegative weight $w_i \\in \\mathbb{R}_{\\ge 0}$. The decision variable $x_i \\in \\{0,1\\}$ indicates whether item $i$ is selected. The capacity of the knapsack is $C \\in \\mathbb{R}_{\\ge 0}$ and enforces the constraint $\\sum_{i=1}^{n} w_i x_i \\le C$. Define the objective vector\n$$\n\\mathbf{f}(\\mathbf{x}) = \\left(f_1(\\mathbf{x}), f_2(\\mathbf{x})\\right) = \\left(-\\sum_{i=1}^{n} p_i x_i,\\; \\sum_{i=1}^{n} w_i x_i\\right).\n$$\nThe optimization task is to minimize both $f_1$ and $f_2$ subject to the capacity constraint, where minimizing $f_1$ is equivalent to maximizing the total value $\\sum_{i=1}^{n} p_i x_i$.\n\nIn multi-objective optimization, a solution $\\mathbf{x}$ with objective vector $\\mathbf{f}(\\mathbf{x}) = (f_1(\\mathbf{x}), f_2(\\mathbf{x}))$ is said to dominate another solution $\\mathbf{y}$ with objective vector $\\mathbf{f}(\\mathbf{y}) = (f_1(\\mathbf{y}), f_2(\\mathbf{y}))$ if $f_1(\\mathbf{x}) \\le f_1(\\mathbf{y})$ and $f_2(\\mathbf{x}) \\le f_2(\\mathbf{y})$ and at least one inequality is strict. A solution is nondominated if no other feasible solution dominates it. The set of all nondominated objective vectors is the Pareto frontier.\n\nStarting from foundational concepts—binary decisions, feasible sets under capacity, Pareto dominance, and the principle of optimality underpinning dynamic programming—derive an algorithmic approach that constructs the nondominated set iteratively by considering items one-by-one and pruning dominated states at each step. Your derivation must justify why dominated states can be safely discarded without losing any Pareto-optimal solutions.\n\nThen, implement the derived dynamic programming algorithm to enumerate the nondominated objective vectors for the following test suite. For each test case, items are given as pairs $(p_i,w_i)$, and the capacity is $C$. The output for each test case must be the list of nondominated objective vectors $[f_1,f_2]$, sorted in ascending order by $f_1$ and, when $f_1$ ties occur, by $f_2$ in ascending order. If duplicate objective vectors occur, include each unique vector only once.\n\nTest suite:\n- Test $1$ (happy path): items $[(9,4),(7,3),(5,2),(6,5),(4,3)]$, capacity $C=10$.\n- Test $2$ (boundary capacity): items $[(3,2),(4,5)]$, capacity $C=0$.\n- Test $3$ (zero-weight item edge case): items $[(2,0),(5,4),(1,1)]$, capacity $C=4$.\n- Test $4$ (duplicate items edge case): items $[(3,2),(3,2)]$, capacity $C=4$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of objective vectors. For example, the final output format should look like\n$$\n[\\text{list\\_for\\_test1},\\text{list\\_for\\_test2},\\text{list\\_for\\_test3},\\text{list\\_for\\_test4}],\n$$\nwith each $\\text{list\\_for\\_testk}$ being a list of lists $[f_1,f_2]$. No physical units or angles are involved, and all numerical outputs are plain integers.",
            "solution": "The problem asks for the derivation and implementation of a dynamic programming algorithm to find the set of nondominated objective vectors for a bi-objective $0$-$1$ knapsack problem. The objectives are to minimize the negative of the total value and to minimize the total weight, subject to a knapsack capacity constraint.\n\nFirst, we formalize the problem. We are given a set of $n$ items, where item $i$ has value $p_i \\ge 0$ and weight $w_i \\ge 0$. The knapsack has a capacity $C$. A solution is a binary vector $\\mathbf{x} = (x_1, \\dots, x_n)$ where $x_i=1$ if item $i$ is selected and $x_i=0$ otherwise. A solution is feasible if it satisfies the capacity constraint:\n$$\n\\sum_{i=1}^{n} w_i x_i \\le C\n$$\nThe objective is to minimize the vector function $\\mathbf{f}(\\mathbf{x}) = (f_1(\\mathbf{x}), f_2(\\mathbf{x}))$, defined as:\n$$\n\\mathbf{f}(\\mathbf{x}) = \\left(-\\sum_{i=1}^{n} p_i x_i, \\sum_{i=1}^{n} w_i x_i\\right)\n$$\nMinimizing $f_1$ is equivalent to maximizing the total value. Minimizing $f_2$ is minimizing the total weight.\n\nA solution $\\mathbf{x}$ dominates another solution $\\mathbf{y}$ if $f_1(\\mathbf{x}) \\le f_1(\\mathbf{y})$ and $f_2(\\mathbf{x}) \\le f_2(\\mathbf{y})$, with at least one inequality being strict. A solution is nondominated (or Pareto-optimal) if no other feasible solution dominates it. The set of objective vectors corresponding to nondominated solutions is called the Pareto frontier.\n\nWe can derive a dynamic programming algorithm by processing items sequentially. The core idea is to maintain the set of nondominated objective vectors achievable at each stage of the process.\n\nLet $S_k$ be the set of nondominated objective vectors that can be formed using a subset of the first $k$ items, i.e., items $\\{1, \\dots, k\\}$. Each vector in $S_k$ is a pair $(f_1, f_2)$ corresponding to some feasible selection of the first $k$ items.\n\n**1. Base Case (Initialization):**\nBefore considering any items (i.e., for $k=0$), the only possible solution is to select no items. This corresponds to the decision vector $\\mathbf{x} = \\mathbf{0}$. The objective vector is $\\mathbf{f}(\\mathbf{0}) = (0, 0)$. Thus, the initial set of nondominated vectors is:\n$$\nS_0 = \\{(0, 0)\\}\n$$\n\n**2. Recursive Step:**\nAssume we have computed $S_{k-1}$, the set of nondominated objective vectors for the first $k-1$ items. We now consider item $k$, with value $p_k$ and weight $w_k$. For each existing nondominated vector $(f_1, f_2) \\in S_{k-1}$, we can form a new potential solution by including item $k$. This is only possible if the resulting total weight does not exceed the capacity $C$.\n\nThe new solution corresponds to an objective vector $(f'_1, f'_2)$ where:\n$$\nf'_1 = f_1 - p_k\n$$\n$$\nf'_2 = f_2 + w_k\n$$\nThis is valid only if $f'_2 \\le C$.\n\nLet $S'_{k}$ be a temporary set of objective vectors generated by considering item $k$. It is formed by two sets of vectors:\n- The vectors in $S_{k-1}$ (corresponding to not selecting item $k$).\n- The newly generated vectors by adding item $k$ to each feasible state in $S_{k-1}$.\nLet $S_{k-1}^{+}$ be the set of newly generated vectors:\n$$\nS_{k-1}^{+} = \\{ (f_1 - p_k, f_2 + w_k) \\mid (f_1, f_2) \\in S_{k-1} \\text{ and } f_2 + w_k \\le C \\}\n$$\nThe combined set of potential vectors for the first $k$ items is the union:\n$$\nT_k = S_{k-1} \\cup S_{k-1}^{+}\n$$\n\n**3. Pruning Step and Justification:**\nThe set $T_k$ may contain vectors that are dominated by other vectors within $T_k$. To obtain $S_k$, we must prune $T_k$ by removing all dominated vectors. $S_k$ is the set of nondominated vectors in $T_k$.\n\nThis pruning is justified by the principle of optimality for multi-objective optimization. Suppose at stage $k$, we have two objective vectors $\\mathbf{u} = (u_1, u_2)$ and $\\mathbf{v} = (v_1, v_2)$ in $T_k$, both corresponding to feasible selections from the first $k$ items, such that $\\mathbf{u}$ dominates $\\mathbf{v}$. This means $u_1 \\le v_1$ and $u_2 \\le v_2$, with at least one strict inequality.\nAny feasible completion of the solution corresponding to $\\mathbf{v}$ (by adding a subset of items $\\{k+1, \\dots, n\\}$ with objective contribution $(\\Delta f_1, \\Delta f_2)$) results in a final objective vector $\\mathbf{f}_{\\mathbf{v}} = (v_1 + \\Delta f_1, v_2 + \\Delta f_2)$. The same completion can be applied to the solution corresponding to $\\mathbf{u}$, resulting in $\\mathbf{f}_{\\mathbf{u}} = (u_1 + \\Delta f_1, u_2 + \\Delta f_2)$. Since $u_2 \\le v_2$, if the completion is feasible for $\\mathbf{v}$ (i.e., $v_2 + \\Delta f_2 \\le C$), it is also feasible for $\\mathbf{u}$ (i.e., $u_2 + \\Delta f_2 \\le v_2 + \\Delta f_2 \\le C$).\nComparing the final vectors, we find $u_1 + \\Delta f_1 \\le v_1 + \\Delta f_1$ and $u_2 + \\Delta f_2 \\le v_2 + \\Delta f_2$, with at least one inequality being strict. Thus, $\\mathbf{f}_{\\mathbf{u}}$ dominates or equals $\\mathbf{f}_{\\mathbf{v}}$. This demonstrates that any Pareto-optimal solution that could potentially be built upon state $\\mathbf{v}$ can be replaced by an equal or better solution built upon state $\\mathbf{u}$. Therefore, the dominated state $\\mathbf{v}$ can be safely discarded at stage $k$ without losing any final nondominated solutions.\n\nThe final Pareto frontier is the set $S_n$ after all $n$ items have been processed.\n\n**Algorithmic Implementation of Pruning:**\nAn efficient way to prune the set $T_k$ is as follows:\n1.  Sort the vectors $(f_1, f_2)$ in $T_k$ primarily by $f_1$ in ascending order, and secondarily by $f_2$ in ascending order.\n2.  Remove any duplicate vectors.\n3.  Iterate through the sorted, unique list to build the new nondominated set $S_k$. Initialize an empty list `pruned_list` and a variable `min_f2_so_far` to infinity.\n4.  For each vector $(f_1, f_2)$ in the sorted list, if $f_2  \\text{min\\_f2\\_so\\_far}$, then this vector is not dominated by any previously accepted vector. Add $(f_1, f_2)$ to `pruned_list` and update $\\text{min\\_f2\\_so\\_far} = f_2$.\n5.  If $f_2 \\ge \\text{min\\_f2\\_so\\_far}$, the current vector is dominated by a previously accepted vector (which had a smaller or equal $f_1$ and a smaller $f_2$) and should be discarded.\n\nThe final algorithm is:\nInitialize $S = \\{(0, 0)\\}$.\nFor each item $(p_i, w_i)$ from $i=1$ to $n$:\n1.  Generate new vectors: $S_{\\text{new}} = \\{ (f_1-p_i, f_2+w_i) \\mid (f_1, f_2) \\in S \\text{ and } f_2+w_i \\le C \\}$.\n2.  Combine and filter: $S = \\text{prune}(S \\cup S_{\\text{new}})$.\nAfter iterating through all items, the resulting set $S$ is the desired Pareto frontier.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the test cases and print the results.\n    \"\"\"\n    \n    def find_nondominated(items, C):\n        \"\"\"\n        Calculates the nondominated set for a bi-objective 0-1 knapsack problem.\n\n        Args:\n            items (list of tuples): A list of (value, weight) pairs for each item.\n            C (float or int): The capacity of the knapsack.\n\n        Returns:\n            list of lists: The set of nondominated objective vectors [f1, f2],\n                          sorted by f1 and then f2.\n        \"\"\"\n        # S is the set of nondominated objective vectors [f1, f2].\n        # f1 = -total_value, f2 = total_weight\n        # Initialize with the state of selecting no items.\n        S = [[0, 0]]\n\n        for p, w in items:\n            # Generate new states by adding the current item to existing states.\n            newly_generated = []\n            for f1, f2 in S:\n                # Check if adding the new item exceeds capacity.\n                if f2 + w = C:\n                    newly_generated.append([f1 - p, f2 + w])\n\n            # Merge the set of states from not adding the item (S)\n            # with the new states from adding the item (newly_generated).\n            combined_S = S + newly_generated\n\n            # Pruning Step:\n            # 1. Sort primarily by f1 (ascending), secondarily by f2 (ascending).\n            # Python's default sort on lists of lists achieves this.\n            combined_S.sort()\n\n            # 2. Filter out dominated and duplicate solutions.\n            pruned_S = []\n            if not combined_S:\n                S = []\n                continue\n\n            # Remove duplicates from the sorted list.\n            unique_S = [combined_S[0]]\n            for i in range(1, len(combined_S)):\n                if combined_S[i] != combined_S[i-1]:\n                    unique_S.append(combined_S[i])\n            \n            # Identify the nondominated front from the unique, sorted candidates.\n            # A point (f1, f2) is kept if its f2 is strictly smaller than the f2\n            # of all previously kept points. Since the list is sorted by f1,\n            # this guarantees nondominance.\n            min_f2_so_far = float('inf')\n            for f1, f2 in unique_S:\n                if f2  min_f2_so_far:\n                    pruned_S.append([f1, f2])\n                    min_f2_so_far = f2\n            \n            S = pruned_S\n\n        # The pruning logic already produces a list sorted by f1.\n        # A final sort ensures compliance with the output format specification.\n        S.sort()\n        return S\n\n    test_cases = [\n        # Test 1 (happy path)\n        ([(9, 4), (7, 3), (5, 2), (6, 5), (4, 3)], 10),\n        # Test 2 (boundary capacity)\n        ([(3, 2), (4, 5)], 0),\n        # Test 3 (zero-weight item edge case)\n        ([(2, 0), (5, 4), (1, 1)], 4),\n        # Test 4 (duplicate items edge case)\n        ([(3, 2), (3, 2)], 4),\n    ]\n\n    results = []\n    for items, C in test_cases:\n        nondominated_set = find_nondominated(items, C)\n        results.append(str(nondominated_set).replace(\" \", \"\"))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}