{
    "hands_on_practices": [
        {
            "introduction": "蚁群优化（ACO）的动态核心在于信息素的增强与蒸发。本练习将蒸发过程单独剥离，通过计算在没有蚂蚁留下新踪迹时信息素 $\\tau$ 如何随时间衰减，帮助你深刻理解这一“遗忘”机制的重要性。掌握信息素的衰减规律是理解算法如何避免过早收敛到局部最优解的第一步。",
            "id": "3097760",
            "problem": "考虑蚁群优化（ACO），其中边上的信息素强度被建模为一个标量变量 $ \\tau $，它由于蒸发和增强而在离散的迭代中发生变化。在此设定中，蒸发由一个广泛使用的规则定义，即在每次迭代中，当前信息素的一个固定比例 $ \\rho $ 会蒸发，其中 $0  \\rho  1$，而增强是蚂蚁根据找到的解的质量沉积新的信息素。假设有一条边，其初始信息素强度为 $ \\tau_0 $，并且在 $ t $ 次迭代的跨度内完全没有增强。\n\n仅使用这些原理，从第一性原理出发，推导出经过 $ t $ 次迭代后的信息素强度的表达式，记为 $ \\tau^{(t)} $，用 $ \\tau_0 $、$ \\rho $ 和 $ t $ 表示。然后，用 $ \\tau_0 = 2.0 $，$ \\rho = 0.1 $ 和 $ t = 10 $ 计算你的表达式的值。将你的最终答案表示为一个保留四位有效数字的实数。",
            "solution": "该问题是有效的，因为它在科学上基于蚁群优化的原理，问题陈述清晰且信息充分，足以得到唯一解，并以客观、正式的语言表达。没有矛盾、歧义或事实上的不健全之处。\n\n问题要求在只有蒸发的条件下，推导单条边经过 $t$ 次迭代后的信息素强度 $\\tau^{(t)}$，然后用具体参数进行数值计算。\n\n设 $\\tau^{(k)}$ 为第 $k$ 次迭代后边上的信息素强度。在开始时，即任何迭代之前，初始条件为：\n$$\n\\tau^{(0)} = \\tau_0\n$$\n问题陈述在每次迭代中，当前信息素的一个固定比例 $\\rho$ 会蒸发。在第 $k+1$ 次迭代期间蒸发的信息素量为 $\\rho \\tau^{(k)}$。因此，剩余的信息素量是迭代开始时的初始量减去蒸发量。更新规则可表示为：\n$$\n\\tau^{(k+1)} = \\tau^{(k)} - \\rho \\tau^{(k)}\n$$\n这可以通过提出公因子 $\\tau^{(k)}$ 来简化：\n$$\n\\tau^{(k+1)} = (1 - \\rho) \\tau^{(k)}\n$$\n这是一个一阶线性齐次递推关系。问题明确指出没有信息素增强。因此，这个递推关系完全描述了信息素水平在迭代过程中的动态变化。\n\n我们可以通过从初始状态 $\\tau^{(0)} = \\tau_0$ 展开来求解这个递推关系。\n\n第一次迭代后（$k=1$）：\n$$\n\\tau^{(1)} = (1 - \\rho) \\tau^{(0)} = (1 - \\rho) \\tau_0\n$$\n\n第二次迭代后（$k=2$）：\n$$\n\\tau^{(2)} = (1 - \\rho) \\tau^{(1)} = (1 - \\rho) [(1 - \\rho) \\tau_0] = (1 - \\rho)^2 \\tau_0\n$$\n\n第三次迭代后（$k=3$）：\n$$\n\\tau^{(3)} = (1 - \\rho) \\tau^{(2)} = (1 - \\rho) [(1 - \\rho)^2 \\tau_0] = (1 - \\rho)^3 \\tau_0\n$$\n\n观察这个模式，我们可以为任意次迭代 $t$ 推广该公式。项 $(1 - \\rho)$ 的指数始终与经过的迭代次数相匹配。因此，经过 $t$ 次迭代后的信息素强度表达式为：\n$$\n\\tau^{(t)} = (1 - \\rho)^t \\tau_0\n$$\n这就是从第一性原理推导出的表达式。\n\n接下来，我们需要用给定的具体值来计算这个表达式：\n初始信息素强度，$\\tau_0 = 2.0$。\n蒸发率，$\\rho = 0.1$。\n迭代次数，$t = 10$。\n\n将这些值代入推导出的公式中：\n$$\n\\tau^{(10)} = (1 - 0.1)^{10} \\times 2.0\n$$\n$$\n\\tau^{(10)} = (0.9)^{10} \\times 2.0\n$$\n首先，我们计算 $(0.9)^{10}$ 的值：\n$$\n(0.9)^{10} \\approx 0.3486784401\n$$\n现在，我们将这个结果乘以初始强度 $\\tau_0 = 2.0$：\n$$\n\\tau^{(10)} \\approx 0.3486784401 \\times 2.0 = 0.6973568802\n$$\n问题要求最终答案表示为一个保留四位有效数字的实数。\n计算出的值为 $0.6973568802$。\n第一个有效数字是 $6$。\n四位有效数字是 $6$、$9$、$7$ 和 $3$。\n第五位有效数字是 $5$。根据标准四舍五入规则，如果下一位数字是 $5$ 或更大，我们对最后一位有效数字进行进位。\n因此，数字 $3$ 进位为 $4$。\n最终的数值答案是 $0.6974$。",
            "answer": "$$\n\\boxed{0.6974}\n$$"
        },
        {
            "introduction": "蚂蚁在路径选择时，总是在“集体智慧”（信息素）和“局部捷径”（启发式信息）之间权衡。本练习构建了一个启发式信息具有误导性的“病态”场景，旨在让你亲手调节参数 $\\alpha$ 和 $\\beta$，探索算法如何在这种冲突下做出决策。这有助于你理解蚁群优化在探索与利用之间的平衡机制，并掌握参数调优的核心思想。",
            "id": "3097720",
            "problem": "您将设计并分析蚁群优化 (ACO) 在旅行商问题 (TSP) 背景下的一个单决策步骤，以构建一个病态案例，在该案例中，启发式信息强烈偏好一条次优的长边。然后，您将衡量调整信息素和启发式信息的指数（分别表示为 $ \\alpha $ 和 $ \\beta $）是减轻还是加剧了这种偏差。\n\n基本原理：\n- 在蚁群优化 (ACO) 中，位于节点 $ i $ 的蚂蚁选择下一个节点 $ j $ 的概率，与信息素强度和启发式分数分别取 $ \\alpha $ 和 $ \\beta $ 次幂后的乘积成正比。对于一个候选集 $ \\mathcal{N}(i) $，转移概率为\n$$\np_{ij} = \\frac{ \\tau_{ij}^{\\alpha} \\, \\eta_{ij}^{\\beta} }{ \\sum_{k \\in \\mathcal{N}(i)} \\tau_{ik}^{\\alpha} \\, \\eta_{ik}^{\\beta} }.\n$$\n- 对于一个无向图，信息素矩阵 $ \\tau_{ij} $ 和启发式矩阵 $ \\eta_{ij} $ 是非负且对称的。\n\n构建图和误导性启发式信息：\n- 考虑一个包含节点 $ \\{0,1,2,3,4\\} $ 的完全无向图，其对称距离 $ d_{ij} $ 由以下给出：\n  - $ d_{01} = 100 $,\n  - $ d_{0,2} = d_{0,3} = d_{0,4} = 2 $,\n  - $ d_{12} = d_{13} = d_{14} = 2 $,\n  - $ d_{23} = d_{24} = d_{34} = 2 $,\n  - 对于所有 $ i $，有 $ d_{ii} = 0 $，且对于所有 $ i \\neq j $，有 $ d_{ij} = d_{ji} $。\n- 按如下方式定义一个病态启发式信息 $ \\eta_{ij} $：\n  - 对于除 $ (0,1) $ 之外的所有边，令 $ \\eta_{ij} = 1 / d_{ij} $。\n  - 对于单䏼边 $ (0,1) $，将其启发式信息覆写为一个较大的值 $ \\eta_{01} = \\eta_{10} = H_{\\mathrm{bad}} $，其中 $ H_{\\mathrm{bad}} = 10 $。\n- 均匀地初始化信息素：对于所有 $ i \\neq j $，有 $ \\tau_{ij} = 1 $。\n\n目标测量量：\n- 考虑一只位于节点 $ 0 $ 的蚂蚁，其可行的下一个节点为 $ \\mathcal{N}(0) = \\{1,2,3,4\\} $。对于给定的配对 $ (\\alpha, \\beta) $，计算该蚂蚁下一步选择节点 $ 1 $ 的单步选择概率，即选择次优长边 $ (0,1) $ 的概率：\n$$\nP_{\\mathrm{bad}}(\\alpha,\\beta) \\equiv p_{0,1} = \\frac{ \\tau_{0,1}^{\\alpha} \\, \\eta_{0,1}^{\\beta} }{ \\sum_{j \\in \\{1,2,3,4\\}} \\tau_{0,j}^{\\alpha} \\, \\eta_{0,j}^{\\beta} }.\n$$\n\n测试套件：\n- 针对以下 $ (\\alpha,\\beta) $ 配对评估 $ P_{\\mathrm{bad}}(\\alpha,\\beta) $，以探究不同方面的影响：\n  1. $ (\\alpha,\\beta) = (1,1) $：一个标准基线。\n  2. $ (\\alpha,\\beta) = (0.1,1) $：降低信息素影响，同时保持启发式信息影响不变。\n  3. $ (\\alpha,\\beta) = (1,5) $：大大增加启发式信息影响。\n  4. $ (\\alpha,\\beta) = (1,0) $：忽略启发式信息，一种边界情况。\n  5. $ (\\alpha,\\beta) = (2,1) $：增加信息素影响，同时保持启发式信息影响不变。\n\n作答要求：\n- 您的程序必须使用上述基本的 ACO 转移概率规则、指定的图、启发式信息和信息素值，为每个测试用例计算 $ P_{\\mathrm{bad}}(\\alpha,\\beta) $。\n- 输出格式：生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表包含与测试套件顺序相同的 5 个结果。每个结果必须是精确到小数点后 6 位的实数。格式示例： \"[$0.123456$,$0.234567$,$0.345678$,$0.456789$,$0.567890$]\"。\n- 无需用户输入，且不得使用外部文件。",
            "solution": "我们从蚁群优化 (ACO) 中概率性转移的核心原理开始。对于当前节点 $ i $ 和候选邻居 $ \\mathcal{N}(i) $，转移概率为\n$$\np_{ij} = \\frac{ \\tau_{ij}^{\\alpha} \\, \\eta_{ij}^{\\beta} }{ \\sum_{k \\in \\mathcal{N}(i)} \\tau_{ik}^{\\alpha} \\, \\eta_{ik}^{\\beta} }.\n$$\n这源于 ACO 的设计原则，即蚂蚁偏好具有更高信息素强度和更高启发式期望值的边，这种偏好由指数 $ \\alpha $ 和 $ \\beta $ 调节，它们分别控制对信息素和启发式信息的敏感度。\n\n我们通过定义一个图来构建一个病态实例，其中真正的长边 $ (0,1) $ 的距离为 $ d_{01} = 100 $，远大于其他地方长度为 $ d = 2 $ 的短边。在传统的 TSP 启发式信息 $ \\eta_{ij} = 1 / d_{ij} $ 中，长边是没有吸引力的。为了制造这种病态情况，我们仅将边 $ (0,1) $ 的启发式信息覆写为一个较大的误导性值 $ H_{\\mathrm{bad}} = 10 $，同时保持所有其他启发式信息为其常规值。信息素被均匀初始化为 $ \\tau_{ij} = 1 $。\n\n我们评估在节点 $ 0 $ 处的单次决策，其可行邻居为 $ \\{1,2,3,4\\} $。由于对所有 $ j $ 都有 $ \\tau_{0j} = 1 $，转移概率简化为\n$$\np_{0j} = \\frac{ 1^{\\alpha} \\eta_{0j}^{\\beta} }{ \\sum_{k \\in \\{1,2,3,4\\}} 1^{\\alpha} \\eta_{0k}^{\\beta} } \n= \\frac{ \\eta_{0j}^{\\beta} }{ \\sum_{k \\in \\{1,2,3,4\\}} \\eta_{0k}^{\\beta} }.\n$$\n因此，在信息素均匀的初始决策中，$ \\alpha $ 不影响概率；只有 $ \\beta $ 起作用。节点 $ 0 $ 处的具体启发式信息值为：\n- $ \\eta_{0,1} = H_{\\mathrm{bad}} = 10 $,\n- $ \\eta_{0,2} = \\eta_{0,3} = \\eta_{0,4} = 1 / 2 = 0.5 $。\n\n因此，\n$$\nP_{\\mathrm{bad}}(\\alpha,\\beta) = p_{0,1} = \\frac{ (10)^{\\beta} }{ (10)^{\\beta} + 3 \\cdot (0.5)^{\\beta} }.\n$$\n现在我们为测试套件计算这个值：\n\n1. 情况 $ (\\alpha,\\beta) = (1,1) $：\n$$\nP_{\\mathrm{bad}} = \\frac{ 10^{1} }{ 10^{1} + 3 \\cdot 0.5^{1} } = \\frac{ 10 }{ 10 + 1.5 } = \\frac{ 20 }{ 23 } \\approx 0.869565.\n$$\n\n2. 情况 $ (\\alpha,\\beta) = (0.1,1) $：\n由于 $ \\tau_{0j} $ 是均匀的，$ \\alpha $ 的影响被消除了。因此，这与情况 1 相等：\n$$\nP_{\\mathrm{bad}} \\approx 0.869565.\n$$\n\n3. 情况 $ (\\alpha,\\beta) = (1,5) $：\n$$\nP_{\\mathrm{bad}} = \\frac{ 10^{5} }{ 10^{5} + 3 \\cdot 0.5^{5} } \n= \\frac{ 100000 }{ 100000 + 3 \\cdot \\frac{1}{32} } \n= \\frac{ 100000 }{ 100000 + 0.09375 } \n\\approx 0.999999.\n$$\n\n4. 情况 $ (\\alpha,\\beta) = (1,0) $：\n此时启发式信息被忽略，因此所有邻居被选择的概率均等：\n$$\nP_{\\mathrm{bad}} = \\frac{ 1 }{ 1 + 3 \\cdot 1 } = \\frac{ 1 }{ 4 } = 0.25.\n$$\n\n5. 情况 $ (\\alpha,\\beta) = (2,1) $：\n同样，$ \\tau_{0j} $ 是均匀的，所以 $ \\alpha $ 不影响初始决策：\n$$\nP_{\\mathrm{bad}} \\approx 0.869565.\n$$\n\n关于偏差校正的解释：\n- 在信息素均匀的情况下，将 $ \\alpha $ 从 $ 1 $ 减小到 $ 0.1 $ 不会改变 $ P_{\\mathrm{bad}} $，因此它在初始决策时不能校正偏差。\n- 将 $ \\beta $ 从 $ 1 $ 增加到 $ 5 $ 会使 $ P_{\\mathrm{bad}} $ 急剧增加并趋近于 $ 1 $，因此它加剧了由误导性启发式信息引入的偏差。\n- 设置 $ \\beta = 0 $ 会完全消除启发式信息的影响，并产生均等的概率，从而在这个单步场景中消除了偏差。\n\n程序将以数值方式计算这些概率，并以要求的单行格式打印它们，每个值都精确到小数点后 6 位，如下所示\n$$\n[\\;0.869565,\\;0.869565,\\;0.999999,\\;0.250000,\\;0.869565\\;].\n$$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_p_bad(alpha: float, beta: float) - float:\n    \"\"\"\n    Compute P_bad(alpha, beta) = probability of choosing the misleading long edge (0-1)\n    at the first decision from node 0, given uniform pheromone and pathological heuristic.\n    \"\"\"\n    # Uniform pheromone on all edges\n    tau = {\n        1: 1.0,\n        2: 1.0,\n        3: 1.0,\n        4: 1.0,\n    }\n    # Pathological heuristic: override (0,1) with a large value; others use 1/d with d=2\n    H_bad = 10.0\n    eta = {\n        1: H_bad,\n        2: 1.0 / 2.0,\n        3: 1.0 / 2.0,\n        4: 1.0 / 2.0,\n    }\n\n    # Candidate neighbors from node 0\n    neighbors = [1, 2, 3, 4]\n\n    # Compute numerator for j=1\n    num = (tau[1] ** alpha) * (eta[1] ** beta)\n\n    # Compute denominator over all neighbors\n    denom = 0.0\n    for j in neighbors:\n        denom += (tau[j] ** alpha) * (eta[j] ** beta)\n\n    # Guard against numerical issues (should not occur with given values)\n    if denom == 0.0:\n        return 0.0\n\n    return num / denom\n\ndef solve():\n    # Define the test cases from the problem statement: (alpha, beta)\n    test_cases = [\n        (1.0, 1.0),    # baseline\n        (0.1, 1.0),    # decreased alpha\n        (1.0, 5.0),    # increased beta\n        (1.0, 0.0),    # heuristic ignored\n        (2.0, 1.0),    # increased alpha\n    ]\n\n    results = []\n    for alpha, beta in test_cases:\n        p_bad = compute_p_bad(alpha, beta)\n        # Round to exactly 6 decimal places as required\n        results.append(f\"{p_bad:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "基础的蚁群算法可以通过与局部搜索方法结合，演变成性能更强的混合算法。本练习将指导你为旅行商问题（TSP）实现一个结合了2-Opt局部搜索的蚁群优化算法。你还将学习使用信息素的“信噪比”（SNR）这一量化指标，来分析局部搜索如何帮助蚁群更快地集中于高质量解，从而深入理解构建高效元启发式算法的设计原则。",
            "id": "3097743",
            "problem": "您需要为一个欧几里得对称旅行商问题（TSP）实现一个完整的蚁群优化（ACO）求解器，并将一个有界的 Two-Opt 局部搜索步骤整合到每只蚂蚁的路径构建过程中。您的实现必须允许每只蚂蚁在每次迭代中接受恰好 $k$ 次局部改进。然后，您将使用下面定义的信噪比来量化允许的局部改进次数 $k$ 如何影响最佳路径周围的信息素浓度。\n\n使用的基础理论：假设已了解概率、期望值和局部搜索下降的定义。您必须从核心定义出发构建 ACO 的构造规则，即由两个独立信号的乘积加权的随机选择通过将权重相乘来建模，并且基于下降的局部搜索在接受改进时会严格减少路径长度。\n\n问题规格：\n- 考虑一个位于平面上的 $n$ 个城市的完全无向图，城市坐标为 $\\{(x_i,y_i)\\}_{i=1}^n$。对于 $i \\neq j$，欧几里得距离 $d_{ij}$ 定义为 $d_{ij} = \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$，并且 $d_{ii} = +\\infty$。\n- 对于 $i \\neq j$，定义启发式强度 $\\eta_{ij} = 1/d_{ij}$。\n- 令 $\\tau_{ij}$ 表示每次迭代开始时边 $(i,j)$ 上的信息素水平，具有对称性 $\\tau_{ij} = \\tau_{ji}$，且不存在自环。\n- 每只蚂蚁从一个均匀随机选择的城市出发构建一个哈密顿回路，在每一步从当前城市 $i$ 选择下一个城市 $j$ 时，从所有未访问的城市集合中选择的概率为\n  $$p_{ij} = \\frac{\\tau_{ij}^{\\alpha}\\,\\eta_{ij}^{\\beta}}{\\sum\\limits_{\\ell \\in \\mathcal{J}_i} \\tau_{i\\ell}^{\\alpha}\\,\\eta_{i\\ell}^{\\beta}},$$\n  其中 $\\mathcal{J}_i$ 是在城市 $i$ 时未访问的城市集合，$\\alpha$ 和 $\\beta$ 是固定的非负指数。\n- 每只蚂蚁完成其路径后，应用 Two-Opt 局部搜索，最多接受 $k$ 次严格改进的移动。一次 Two-Opt 移动会选择当前路径顺序中两条不相邻的边 $(a,b)$ 和 $(c,d)$，如果得到的闭合路径长度严格减小，则用 $(a,c)$ 和 $(b,d)$ 替换它们；这等同于反转两个索引之间的连续子路径。使用首次改进策略：按确定性顺序扫描候选对，并接受第一个改进的移动；然后从头开始继续扫描，直到接受了 $k$ 次改进移动或不再有改进移动为止。\n- 在一次迭代中所有蚂蚁完成其局部搜索后，分两步执行信息素更新：\n  1. 蒸发：对所有 $i \\neq j$，$\\tau_{ij} \\leftarrow (1-\\rho)\\,\\tau_{ij}$。\n  2. 所有蚂蚁沉积信息素：对于每只闭合路径长度为 $L$ 的蚂蚁，为其路径上使用的每条边 $(i,j)$，在 $\\tau_{ij}$ 和 $\\tau_{ji}$ 上都增加 $\\Delta \\tau_{ij} = Q/L$。\n- 跟踪迄今为止的最佳路径 $\\mathcal{B}$，即在所有迭代和所有蚂蚁中观察到的长度最小的路径，以及其边集 $E(\\mathcal{B})$，表示为无方向且无重复。\n- 在最后一次迭代后，计算信息素信噪比为\n  $$\\mathrm{SNR} = \\frac{\\overline{\\tau}_{\\text{on}}}{\\overline{\\tau}_{\\text{off}}},$$\n  其中 $\\overline{\\tau}_{\\text{on}}$ 是 $E(\\mathcal{B})$ 中无向边 $(i,j)$ 上 $\\tau_{ij}$ 的算术平均值，$\\overline{\\tau}_{\\text{off}}$ 是所有不在 $E(\\mathcal{B})$ 中且满足 $i  j$ 的边 $(i,j)$ 上 $\\tau_{ij}$ 的算术平均值。\n- 如果 $\\overline{\\tau}_{\\text{off}} = 0$，则定义 $\\mathrm{SNR} = \\overline{\\tau}_{\\text{on}}/\\varepsilon$，其中 $\\varepsilon = 10^{-12}$ 是一个小的正则化常数。",
            "solution": "用户提供了一个在优化方法领域中定义明确的计算问题。任务是为旅行商问题（TSP）实现一个蚁群优化（ACO）算法，该算法通过有界的局部搜索过程（2-Opt）进行增强，并分析局部搜索预算对信息素浓度的影响。\n\n### 步骤 1：问题验证\n\n根据所需标准对问题陈述进行验证。\n\n- **提取的已知信息**：\n  - **问题**：用 ACO 结合有界的 2-Opt 局部搜索求解欧几里得对称 TSP。\n  - **图**：一个包含 $n=12$ 个城市的完全无向图，具有指定的笛卡尔坐标 $\\{(x_i, y_i)\\}_{i=1}^{12}$。\n  - **距离**：欧几里得距离 $d_{ij} = \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$。\n  - **启发式强度**：$\\eta_{ij} = 1/d_{ij}$。\n  - **路径构建**：标准概率规则 $p_{ij} \\propto \\tau_{ij}^{\\alpha}\\,\\eta_{ij}^{\\beta}$。\n  - **局部搜索**：2-Opt，采用首次改进策略，每只蚂蚁最多接受 $k$ 次严格改进的移动。\n  - **信息素更新**：标准蒸发 $\\tau_{ij} \\leftarrow (1-\\rho)\\,\\tau_{ij}$，然后所有蚂蚁根据各自的路径进行沉积 $\\Delta\\tau_{ij} = Q/L$。\n  - **性能指标**：最终信息素信噪比 $\\mathrm{SNR} = \\overline{\\tau}_{\\text{on}} / \\overline{\\tau}_{\\text{off}}$，对于分母为零的情况，使用稳定性常数 $\\varepsilon = 10^{-12}$。\n  - **参数**：蚂蚁数 $m=12$，迭代次数 $T=40$，指数 $\\alpha=1.0, \\beta=2.0$，蒸发率 $\\rho=0.5$，沉积因子 $Q=1.0$，初始信息素 $\\tau_0=1.0$，随机种子 $=42$。\n  - **测试套件**：四个局部搜索预算案例：$k \\in \\{0, 1, 3, 100\\}$。\n  - **输出**：一个包含四个 SNR 结果值的逗号分隔列表，四舍五入到 6 位小数。\n\n- **验证结论**：\n  - 该问题**有科学依据**。ACO、TSP 和 2-Opt 都是计算机科学和运筹学中标准且经过深入研究的主题。\n  - 该问题是**适定的**。所有参数、初始条件和程序步骤都已明确定义，确保在给定随机种子的情况下可以计算出唯一的、确定性的结果。\n  - 该问题是**客观且可形式化的**。它以精确的数学和算法术语描述，没有歧义或主观解释。\n  - 该问题是**完整、一致且可行的**。没有缺失数据，也不存在矛盾。计算负载是可管理的。\n\n该问题被判定为**有效**。将开发一个解决方案。\n\n### 步骤 2：原则性解决方案设计\n\n该解决方案需要实现一个为 TSP 定制的完整 ACO 元启发式算法。算法的生命周期包括初始化、蚂蚁迭代构建解、通过局部搜索改进解以及信息素介导的学习。最后一步是计算指定的 SNR 指标。\n\n**1. 初始化**\n首先，我们预处理输入数据。使用城市坐标计算对称的欧几里得距离矩阵 $D = [d_{ij}]$。由此，推导出启发式强度矩阵 $H = [\\eta_{ij}]$，其中对于 $i \\neq j$，$\\eta_{ij} = 1/d_{ij}$，且 $\\eta_{ii}=0$。信息素矩阵 $\\mathcal{T} = [\\tau_{ij}]$ 被初始化为所有边的常数值 $\\tau_0$，且 $\\tau_{ii}=0$。全局最佳路径 $\\mathcal{B}$ 及其长度 $L_{\\mathcal{B}}$ 分别初始化为空状态和无穷大。所有随机选择都由一个使用指定种子初始化的伪随机数生成器控制，以确保每个测试案例的可复现性。\n\n**2. 迭代的路径构建与改进**\n算法的核心是一个固定迭代次数 $T$ 的循环。在每次迭代中，一个由 $m$ 只蚂蚁组成的群体独立地构建解。\n\n- **路径构建**：每只蚂蚁从一个均匀随机选择的起始城市开始。从其当前城市 $i$，它从一组未访问的城市 $\\mathcal{J}_i$ 中选择下一个城市 $j$。这个选择是概率性的，由信息素轨迹 $\\tau_{ij}$ 和启发式信息 $\\eta_{ij}$ 共同引导。转移概率由以下公式给出：\n$$p_{ij} = \\frac{[\\tau_{ij}]^{\\alpha}[\\eta_{ij}]^{\\beta}}{\\sum_{\\ell \\in \\mathcal{J}_i} [\\tau_{i\\ell}]^{\\alpha}[\\eta_{i\\ell}]^{\\beta}}$$\n重复此过程，直到形成一个完整的哈密顿回路（一条路径）。这种构建方法体现了 ACO 的基本原则，即有偏向的随机探索，其中过去成功的选择（更强的信息素）和局部最优的选择（更短的距离）更受青睐。\n\n- **局部搜索（有界 2-Opt）**：蚂蚁构建一条路径后，使用 2-Opt 局部搜索对其进行改进。对一条路径进行 2-Opt 移动涉及选择两条不相邻的边，例如 $(t_i, t_{i+1})$ 和 $(t_j, t_{j+1})$，移除它们，并重新连接两条产生的路径以形成一条新的有效路径。这等同于反转城市 $t_{i+1}$ 和城市 $t_j$ 之间的路径段。只有当移动导致总路径长度严格减小时，才接受该移动。问题指定了首次改进策略：按确定性顺序扫描候选移动，并接受找到的第一个有效的改进移动。然后重新启动整个扫描过程。重复此操作，直到最多进行了 $k$ 次改进，或一次完整扫描显示没有进一步的改进移动（达到 2-opt 局部最优）。对于 $k=0$，此步骤被跳过。引入局部搜索将 ACO 的全局搜索特性与强大的局部探索机制相结合，通常能得到更高质量的解。\n\n**3. 信息素更新**\n在一次迭代中，所有 $m$ 只蚂蚁都完成了它们的（可能已改进的）路径后，信息素轨迹将被更新。这种学习机制对于引导后续的蚂蚁至关重要。\n- **蒸发**：所有信息素值都均匀减少：$\\tau_{ij} \\leftarrow (1-\\rho)\\tau_{ij}$。这模仿了信息素的自然衰减，并帮助蚁群“忘记”不好的选择，避免过早收敛到次优的搜索空间区域。\n- **沉积**：$m$ 只蚂蚁中的每一只都在其最终路径的边上沉积一定量的信息素。沉积的数量与路径长度 $L$ 成反比：$\\Delta\\tau_{ij} = Q/L$。较短的路径导致更多的信息素沉积，从而强化构成优良解的边。更新规则为：\n$$\\tau_{ij} \\leftarrow \\tau_{ij} + \\sum_{a=1}^{m} \\Delta\\tau_{ij}^a$$\n其中，如果边 $(i,j)$ 在蚂蚁 $a$ 的路径中，则 $\\Delta\\tau_{ij}^a = Q/L_a$，否则为 $0$。\n\n在整个过程中，存储由任何蚂蚁在任何迭代中找到的全局最佳路径 $\\mathcal{B}$ 及其长度 $L_{\\mathcal{B}}$。\n\n**4. 最终指标计算 (SNR)**\n最后一次迭代后，算法终止。使用最终的信息素矩阵 $\\mathcal{T}$ 和迄今为止的最佳路径 $\\mathcal{B}$ 来计算信噪比。设 $E(\\mathcal{B})$ 为最佳路径中的（无向）边集。\n- “信号” $\\overline{\\tau}_{\\text{on}}$ 是属于最佳路径的边上的平均信息素水平：\n$$\\overline{\\tau}_{\\text{on}} = \\frac{1}{|E(\\mathcal{B})|} \\sum_{(i,j) \\in E(\\mathcal{B})} \\tau_{ij}$$\n- “噪声” $\\overline{\\tau}_{\\text{off}}$ 是所有其他边上的平均信息素水平：\n$$\\overline{\\tau}_{\\text{off}} = \\frac{1}{|\\mathcal{E}| - |E(\\mathcal{B})|} \\sum_{(i,j) \\in \\mathcal{E} \\setminus E(\\mathcal{B})} \\tau_{ij}$$\n其中 $\\mathcal{E}$ 是图中所有可能边的集合。SNR 是它们的比值，$\\mathrm{SNR} = \\overline{\\tau}_{\\text{on}} / \\overline{\\tau}_{\\text{off}}$，它量化了集体学习过程将信息素集中在最优路径上的程度。更高的 SNR 表示更强的收敛性和对最佳解更清晰的“信号”。\n\n对于测试套件中的每个 k 值，都会重复整个过程，并且为每个案例重新初始化随机数生成器，以确保公平比较。",
            "answer": "```python\n# The final answer must be a single, complete, standalone program.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main entry point to solve the problem.\n    It iterates through the test cases for k and prints the final result.\n    \"\"\"\n    problem_data = {\n        'coords': np.array([\n            (0.0, 0.0), (1.0, 5.0), (5.0, 2.0), (6.0, 6.0), (2.0, 7.0),\n            (8.0, 3.0), (7.0, 9.0), (3.5, 8.0), (9.0, 1.0), (4.0, 4.0),\n            (0.5, 9.5), (9.5, 9.5)\n        ]),\n        'params': {\n            'n_ants': 12, 'n_iterations': 40, 'alpha': 1.0, 'beta': 2.0,\n            'rho': 0.5, 'Q': 1.0, 'tau0': 1.0, 'seed': 42\n        },\n        'test_cases_k': [0, 1, 3, 100],\n        'epsilon': 1e-12\n    }\n\n    results = []\n    for k in problem_data['test_cases_k']:\n        snr = run_aco_simulation(k, problem_data)\n        results.append(f\"{snr:.6f}\")\n    \n    print(f\"[{','.join(results)}]\")\n\ndef calculate_distance_matrix(coords):\n    \"\"\"Calculates the Euclidean distance matrix between cities.\"\"\"\n    n_cities = len(coords)\n    dist_matrix = np.zeros((n_cities, n_cities))\n    for i in range(n_cities):\n        for j in range(i + 1, n_cities):\n            dist = np.linalg.norm(coords[i] - coords[j])\n            dist_matrix[i, j] = dist_matrix[j, i] = dist\n    return dist_matrix\n\ndef calculate_tour_length(tour, dist_matrix):\n    \"\"\"Calculates the total length of a tour.\"\"\"\n    length = 0.0\n    n_cities = len(tour)\n    for i in range(n_cities):\n        u, v = tour[i], tour[(i + 1) % n_cities]\n        length += dist_matrix[u, v]\n    return length\n\ndef construct_tour(rng, tau, eta, alpha, beta, n_cities):\n    \"\"\"An ant constructs a tour using the pheromone and heuristic information.\"\"\"\n    start_node = rng.choice(n_cities)\n    tour = [start_node]\n    unvisited = set(range(n_cities))\n    unvisited.remove(start_node)\n    \n    current_node = start_node\n    while unvisited:\n        nodes_in_order = list(unvisited)\n        weights = (tau[current_node, nodes_in_order] ** alpha) * \\\n                  (eta[current_node, nodes_in_order] ** beta)\n        \n        weights_sum = np.sum(weights)\n        if weights_sum == 0.0:\n            next_node = rng.choice(nodes_in_order)\n        else:\n            probs = weights / weights_sum\n            next_node = rng.choice(nodes_in_order, p=probs)\n        \n        tour.append(next_node)\n        unvisited.remove(next_node)\n        current_node = next_node\n        \n    return tuple(tour)\n\ndef apply_2opt(tour, dist_matrix, k, n_cities):\n    \"\"\"Applies bounded 2-Opt local search to a tour.\"\"\"\n    if k == 0:\n        return tour\n\n    tour_list = list(tour)\n    num_improvements = 0\n    while num_improvements  k:\n        improvement_found_in_scan = False\n        for i in range(n_cities - 1):\n            for j in range(i + 2, n_cities):\n                # The edges are (tour[i], tour[i+1]) and (tour[j], tour[(j+1)%n])\n                # Skip if they are adjacent through the wrap-around.\n                if i == 0 and j == n_cities - 1:\n                    continue\n\n                n1, n2 = tour_list[i], tour_list[i + 1]\n                n3, n4 = tour_list[j], tour_list[(j + 1) % n_cities]\n\n                d_old = dist_matrix[n1, n2] + dist_matrix[n3, n4]\n                d_new = dist_matrix[n1, n3] + dist_matrix[n2, n4]\n                \n                # Use a small tolerance for strict improvement\n                if d_new  d_old - 1e-9:\n                    # Reverse the segment tour_list[i+1:j+1]\n                    tour_list[i + 1:j + 1] = tour_list[j:i:-1]\n                    \n                    num_improvements += 1\n                    improvement_found_in_scan = True\n                    break  # Break from j loop to restart scan\n            if improvement_found_in_scan:\n                break  # Break from i loop to restart scan\n        \n        if not improvement_found_in_scan:\n            break  # No improvements in a full scan, local optimum reached\n            \n    return tuple(tour_list)\n\ndef calculate_snr(tau, best_tour, n_cities, epsilon):\n    \"\"\"Calculates the Signal-to-Noise Ratio of the pheromone matrix.\"\"\"\n    best_edges = set()\n    for i in range(n_cities):\n        u, v = best_tour[i], best_tour[(i + 1) % n_cities]\n        best_edges.add(tuple(sorted((u, v))))\n\n    tau_on_sum, tau_on_count = 0.0, 0\n    tau_off_sum, tau_off_count = 0.0, 0\n\n    for i in range(n_cities):\n        for j in range(i + 1, n_cities):\n            if (i, j) in best_edges:\n                tau_on_sum += tau[i, j]\n                tau_on_count += 1\n            else:\n                tau_off_sum += tau[i, j]\n                tau_off_count += 1\n    \n    mean_tau_on = tau_on_sum / tau_on_count if tau_on_count > 0 else 0.0\n    mean_tau_off = tau_off_sum / tau_off_count if tau_off_count > 0 else 0.0\n    \n    if mean_tau_off == 0.0:\n        return mean_tau_on / epsilon\n    else:\n        return mean_tau_on / mean_tau_off\n\ndef run_aco_simulation(k, problem_data):\n    \"\"\"Runs a full ACO simulation for a given k.\"\"\"\n    coords = problem_data['coords']\n    p = problem_data['params']\n    epsilon = problem_data['epsilon']\n    \n    n_cities = len(coords)\n    \n    rng = np.random.default_rng(p['seed'])\n    \n    dist_matrix = calculate_distance_matrix(coords)\n    eta = np.zeros_like(dist_matrix)\n    nonzero_dists = dist_matrix != 0\n    eta[nonzero_dists] = 1.0 / dist_matrix[nonzero_dists]\n    \n    tau = np.full((n_cities, n_cities), p['tau0'])\n    np.fill_diagonal(tau, 0)\n    \n    best_tour_so_far = None\n    best_length_so_far = np.inf\n    \n    for _ in range(p['n_iterations']):\n        ant_tours_data = []\n        \n        for _ in range(p['n_ants']):\n            tour = construct_tour(rng, tau, eta, p['alpha'], p['beta'], n_cities)\n            tour = apply_2opt(tour, dist_matrix, k, n_cities)\n            \n            length = calculate_tour_length(tour, dist_matrix)\n            ant_tours_data.append({'tour': tour, 'length': length})\n            \n            if length  best_length_so_far:\n                best_length_so_far = length\n                best_tour_so_far = tour\n        \n        tau *= (1 - p['rho'])\n        for data in ant_tours_data:\n            tour, length = data['tour'], data['length']\n            for i in range(n_cities):\n                u, v = tour[i], tour[(i + 1) % n_cities]\n                tau[u, v] += p['Q'] / length\n                tau[v, u] += p['Q'] / length\n    \n    snr = calculate_snr(tau, best_tour_so_far, n_cities, epsilon)\n    return snr\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}