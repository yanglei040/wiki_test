{
    "hands_on_practices": [
        {
            "introduction": "在随机全局优化中，一个核心问题是：为了有足够信心找到全局最优解，我们需要进行多少次尝试？这个练习将引导你运用基本的概率论原理，推导出在已知单次尝试成功概率的假设下，达到特定成功置信度所需的最少重启次数。通过这个计算，你可以深刻理解在多重起始法中，计算预算与成功保证之间的量化关系。",
            "id": "3186484",
            "problem": "考虑定义在超矩形（箱形）域 $[-5.12,5.12]^{d}$ 上的 $d$ 维Rastrigin函数 $f(x)=\\sum_{i=1}^{d}\\big(x_{i}^{2}-10\\cos(2\\pi x_{i})+10\\big)$。使用一种随机多起点策略：每次重启都从定义域中独立地随机均匀采样一个初始点 $x^{(0)}$，然后应用一种确定性的局部最优化方法，该方法会收敛到一个局部最小值，其吸引盆包含 $x^{(0)}$。假设全局最小值的吸引盆的体积分数为 $p$，其中 $0  p  1$，并且您希望以至少 $0.95$ 的概率找到全局最小值。推导出找到全局最小值所需的最少重启次数 $m$ 的闭式表达式。",
            "solution": "问题是要求解最小整数重启次数 $m$，对于一个随机多起点最优化方法，以确保至少有一次找到全局最小值的概率不低于 $0.95$。\n\n首先，我们使用概率论将问题形式化。问题陈述提供了以下关键信息：\n1.  每次重启都是一次独立试验。\n2.  重启的初始点是从定义域中随机均匀采样的。\n3.  单次随机初始化的重启收敛到全局最小值的概率由其吸引盆的体积分数给出，记为 $p$。\n\n设 $S$ 为单次重启成功找到全局最小值的事件。该事件的概率为：\n$$P(S) = p$$\n问题陈述中说明 $0  p  1$。\n\n设 $F$ 为单次重启未能找到全局最小值的事件。这是事件 $S$ 的互补事件。失败的概率为：\n$$P(F) = 1 - P(S) = 1 - p$$\n\n我们执行 $m$ 次独立重启。设 $A$ 为在这 $m$ 次重启中至少有一次找到全局最小值的事件。问题要求我们找到最小的整数 $m$ 使得：\n$$P(A) \\ge 0.95$$\n\n从计算上讲，先计算其互补事件 $A^c$ 的概率更为直接。事件 $A^c$ 指的是 $m$ 次重启都没有找到全局最小值，即所有 $m$ 次重启都失败的场景。\n$$A^c = \\text{所有 } m \\text{ 次重启都失败}$$\n\n由于各次重启是独立事件，所有 $m$ 次都失败的概率是它们各自失败概率的乘积：\n$$P(A^c) = \\underbrace{P(F) \\times P(F) \\times \\dots \\times P(F)}_{m \\text{ 次}} = (P(F))^m = (1 - p)^m$$\n\n事件 $A$（至少一次成功）的概率与其互补事件 $A^c$（没有成功）的概率通过公式 $P(A) = 1 - P(A^c)$ 相关联。因此：\n$$P(A) = 1 - (1 - p)^m$$\n\n现在，我们应用问题中指定的条件：\n$$1 - (1 - p)^m \\ge 0.95$$\n\n我们需要解这个关于 $m$ 的不等式。整理各项，我们得到：\n$$1 - 0.95 \\ge (1 - p)^m$$\n$$0.05 \\ge (1 - p)^m$$\n\n为了求解指数 $m$，我们对不等式两边取自然对数。由于自然对数函数 $\\ln(x)$ 是一个严格递增函数，将其应用于不等式两边会保持不等号的方向：\n$$\\ln(0.05) \\ge \\ln((1 - p)^m)$$\n\n使用对数性质 $\\ln(x^y) = y \\ln(x)$，我们有：\n$$\\ln(0.05) \\ge m \\ln(1 - p)$$\n\n为了分离出 $m$，我们必须除以 $\\ln(1 - p)$。除法后不等号的方向取决于 $\\ln(1 - p)$ 的符号。问题陈述中说明 $0  p  1$，这意味着 $0  1 - p  1$。任何介于 $0$ 和 $1$ 之间的数的自然对数都是负数。因此，$\\ln(1 - p)  0$。\n\n当不等式两边同除以一个负数时，不等号的方向必须反转：\n$$\\frac{\\ln(0.05)}{\\ln(1 - p)} \\le m$$\n这可以写作：\n$$m \\ge \\frac{\\ln(0.05)}{\\ln(1 - p)}$$\n注意到 $\\ln(0.05)$ 也是负数，所以比值 $\\frac{\\ln(0.05)}{\\ln(1 - p)}$ 是一个正数，这与试验次数的期望相符。\n\n问题要求满足此条件的最小*整数*值 $m$。大于或等于给定实数 $x$ 的最小整数由向上取整函数（ceiling function）$\\lceil x \\rceil$ 给出。因此，最小整数 $m$ 是：\n$$m = \\left\\lceil \\frac{\\ln(0.05)}{\\ln(1 - p)} \\right\\rceil$$\n\n这个表达式给出了所需的最小整数重启次数 $m$ 的闭式解，它是一个关于概率 $p$ 和期望成功率 $0.95$ 的函数。",
            "answer": "$$\\boxed{\\left\\lceil \\frac{\\ln(0.05)}{\\ln(1 - p)} \\right\\rceil}$$"
        },
        {
            "introduction": "上一个练习假设我们已知单次优化的成功概率 $p$。但在现实世界中，这个值是未知的。本练习将带你动手编写一个程序，通过蒙特卡洛模拟的方法，经验性地估计一个局部最小点吸引盆的大小，从而得到对成功概率 $p$ 的一个实际估计。这让你能将理论计算与实际的求解器行为联系起来，为设定多重起始策略的密度提供数据支持。",
            "id": "3186406",
            "problem": "要求您设计并实现一个完整的、可运行的程序，该程序为一个指定的局部求解器经验性地估计其吸引盆，并利用此估计来确定用于随机全局优化的合适的多起点布种密度。本练习必须从优化和概率论中的核心定义与经过充分检验的事实出发，并避免依赖任何超出这些基础知识可推导范围的快捷公式。\n\n使用的定义和原则：\n- 一个应用于可微目标函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 的确定性局部求解器，对于给定的初始点 $x_0$，试图生成一个点 $\\hat{x}$，使得 $\\nabla f(\\hat{x}) \\approx 0$ 且 $f(\\hat{x}) \\le f(x_0)$。对于该求解器，所有使其轨迹收敛到特定局部最小值 $x^{\\star}$ 的初始点集合，被称为 $x^{\\star}$ 的吸引盆。\n- 吸引行为的经验估计可以通过蒙特卡洛采样获得。如果从一个区域中采样 $S$ 个独立的初始点，并且有 $K$ 次求解器运行依据指定的邻近准则收敛到 $x^{\\star}$，那么蒙特卡洛估计量 $\\hat{p} = K/S$ 是一个从该区域随机抽取的点能收敛到 $x^{\\star}$ 的概率 $p$ 的无偏估计量。\n- 对于多起点法，在有限面积为 $A(D)$ 的有界域 $D \\subset \\mathbb{R}^n$ 上有多个独立均匀分布的起始点，如果一个特定的子区域 $B \\subset D$ 的测度分数为 $p_B = A(B)/A(D)$，那么在 $N$ 个独立起点中至少有一个落在 $B$ 内的概率是 $1 - (1 - p_B)^N$。求解 $1 - (1 - p_B)^N \\ge q$ 以得到 $N$，即可获得达到至少为 $q$ 的覆盖概率所需的最小起点数。\n\n您的程序必须在二维空间 $n=2$ 中实现以下过程。\n\n1. 给定一个二次连续可微的目标函数 $f:\\mathbb{R}^2\\to \\mathbb{R}$、一个候选局部最小值 $m \\in \\mathbb{R}^2$、一个有界矩形域 $D = [\\ell_x,u_x] \\times [\\ell_y,u_y]$ 其面积为 $A(D) = (u_x - \\ell_x)(u_y - \\ell_y)$、一个半径列表 $\\mathcal{R} = \\{r_1,\\dots,r_k\\}$ 其中每个 $r_i  0$、一个样本大小 $S \\in \\mathbb{N}$、一个成功阈值 $\\tau \\in (0,1)$、一个目标多起点覆盖概率 $q \\in (0,1)$ 以及一个欧几里得邻近容差 $\\varepsilon  0$，为一个指定的局部求解器估计经验吸引盆半径，步骤如下：\n   - 对于每个半径 $r \\in \\mathcal{R}$，从圆盘 $B(m,r) = \\{x \\in \\mathbb{R}^2 : \\|x - m\\|_2 \\le r\\}$ 与 $D$ 的交集中独立地均匀采样 $S$ 个点。对于每个采样点 $x_0$，从 $x_0$ 开始对函数 $f$ 运行局部求解器。如果求解器报告成功终止并返回满足 $\\|\\hat{x} - m\\|_2 \\le \\varepsilon$ 的解 $\\hat{x}$，则记为一次成功。计算经验成功率 $\\hat{p}(r) = K(r)/S$，其中 $K(r)$ 是在半径 $r$ 处的成功次数。\n   - 将经验吸引半径 $r^{\\star}$ 定义为满足 $\\hat{p}(r) \\ge \\tau$ 的最小 $r \\in \\mathcal{R}$。如果不存在这样的 $r$，则将 $r^{\\star}$ 定义为使 $\\hat{p}(r)$ 最大化的 $r \\in \\mathcal{R}$，若存在多个这样的 $r$，则选择最小的那个以打破平局。令 $p^{\\star} = \\hat{p}(r^{\\star})$。\n   - 将吸引盆面积近似为 $\\pi (r^{\\star})^2$，并将相应的域分数近似为 $p_{\\mathrm{area}} = \\min\\{1, \\pi (r^{\\star})^2 / A(D)\\}$。利用从 $D$ 中均匀抽样的多起点种子的独立性，要确保至少有一个起点落在近似吸引盆中的概率至少为 $q$，所需的最小起点数 $N$ 满足 $1 - (1 - p_{\\mathrm{area}})^N \\ge q$。求解 $N$，使其为具有此性质的最小整数。\n\n2. 使用标准局部求解器实现上述过程：\n   - 在指定时使用Broyden–Fletcher–Goldfarb–Shanno (BFGS)方法，并在可用时提供解析梯度。\n   - 在指定时使用Nelder–Mead单纯形法，不使用梯度。\n\n3. 在圆盘内采样均匀分布的点必须是无偏的：如果 $U \\sim \\mathrm{Uniform}[0,1]$ 和 $\\Theta \\sim \\mathrm{Uniform}[0,2\\pi)$ 是独立的，那么极坐标为 $(\\rho,\\theta)$ 的点（其中 $\\rho = r \\sqrt{U}$ 且 $\\theta = \\Theta$）在半径为 $r$ 的圆盘上是均匀分布的。\n\n需要计算和报告的量：\n- 对于每个测试用例，计算三元组 $[N, r^{\\star}, p^{\\star}]$。数字 $N$ 必须是满足多起点覆盖条件的最小整数。值 $r^{\\star}$ 必须是经验选择的半径，$p^{\\star}$ 必须是相应的经验成功率。将概率表示为小数（而非百分比）。不涉及物理单位。\n\n测试套件：\n您的程序必须运行以下三个测试用例并汇总其输出。\n\n- 测试用例1（Himmelblau函数，Broyden–Fletcher–Goldfarb–Shanno方法）：\n  - 目标函数：Himmelblau函数 $f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$。\n  - 候选最小值：$m = (3,2)$。\n  - 域：$D = [-6,6] \\times [-6,6]$。\n  - 半径：$\\mathcal{R} = \\{0.15, 0.30, 0.60, 1.20\\}$。\n  - 每个半径的样本数：$S = 120$。\n  - 成功阈值：$\\tau = 0.9$。\n  - 目标覆盖率：$q = 0.95$。\n  - 邻近容差：$\\varepsilon = 0.01$。\n\n- 测试用例2（Himmelblau函数，Nelder–Mead单纯形法）：\n  - 目标函数：同测试用例1。\n  - 候选最小值：$m = (-2.805118, 3.131312)$。\n  - 域：$D = [-6,6] \\times [-6,6]$。\n  - 半径：$\\mathcal{R} = \\{0.10, 0.30, 0.60, 1.00\\}$。\n  - 每个半径的样本数：$S = 100$。\n  - 成功阈值：$\\tau = 0.8$。\n  - 目标覆盖率：$q = 0.90$。\n  - 邻近容差：$\\varepsilon = 0.02$。\n\n- 测试用例3（Rosenbrock函数，Broyden–Fletcher–Goldfarb–Shanno方法）：\n  - 目标函数：Rosenbrock函数 $f(x,y) = (1-x)^2 + 100(y - x^2)^2$。\n  - 候选最小值：$m = (1,1)$。\n  - 域：$D = [-2,2] \\times [-2,2]$。\n  - 半径：$\\mathcal{R} = \\{0.05, 0.10, 0.20, 0.40\\}$。\n  - 每个半径的样本数：$S = 150$。\n  - 成功阈值：$\\tau = 0.95$。\n  - 目标覆盖率：$q = 0.99$。\n  - 邻近容差：$\\varepsilon = 0.01$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内不含空格，每个测试用例对应一个自己的方括号括起来的三元组。例如，包含三个测试用例的输出必须如下所示：\n\"[[N1,r1,p1],[N2,r2,p2],[N3,r3,p3]]\"\n其中每个 $N_i$ 是一个整数，每个 $r_i$ 和 $p_i$ 是十进制数。在打印输出中，将 $r_i$ 和 $p_i$ 四舍五入到小数点后六位。不应打印任何其他文本。",
            "solution": "问题陈述已经过分析并被确定为有效。它具有科学依据，是良定的、客观的，并包含继续求解所需的所有必要信息。任务是实现一个过程，以经验方式估计函数局部最小值的吸引盆，然后确定多起点优化策略为以一定概率找到该最小值所需的起始点数量。\n\n该过程针对目标函数 $f: \\mathbb{R}^2 \\to \\mathbb{R}$ 和候选局部最小值 $m \\in \\mathbb{R}^2$ 分两个主要阶段执行。\n\n首先，我们经验性地估计吸引半径。对于一组测试半径 $\\mathcal{R} = \\{r_1, \\dots, r_k\\}$，我们进行一次蒙特卡洛模拟。对于每个半径 $r \\in \\mathcal{R}$，我们从以 $m$ 为中心、半径为 $r$ 的圆盘 $B(m, r)$ 中均匀采样生成 $S$ 个初始点 $\\{x_0^{(1)}, \\dots, x_0^{(S)}\\}$。问题指明，对于给定的测试用例，这些圆盘完全包含在搜索域 $D$ 内，从而简化了采样过程。圆盘内的均匀采样是通过极坐标下的逆变换采样实现的。一个点的径向坐标 $\\rho$ 和角坐标 $\\theta$ 通过 $\\rho = r \\sqrt{U_1}$ 和 $\\theta = 2\\pi U_2$ 生成，其中 $U_1, U_2$ 是在 $[0,1]$上均匀分布的独立随机变量。这确保了生成的点在圆盘面积上具有均匀的概率密度。\n\n对于每个采样点 $x_0$，启动一个局部优化求解器（根据指定，为 BFGS 或 Nelder-Mead）。如果满足两个条件，则该次尝试被视为“成功”：求解器成功收敛，并且找到的最小值 $\\hat{x}$ 与目标最小值 $m$ 的欧几里得距离在 $\\varepsilon$ 以内，即 $\\|\\hat{x} - m\\|_2 \\le \\varepsilon$。对给定半径 $r$ 的成功次数 $K(r)$ 进行计数。然后，经验收敛概率（或成功率）被估计为 $\\hat{p}(r) = K(r)/S$。\n\n在为所有 $r \\in \\mathcal{R}$ 计算出 $\\hat{p}(r)$ 后，根据指定的规则选择经验吸引半径 $r^{\\star}$。我们找出成功率达到或超过给定阈值 $\\tau$ 的半径集合，即 $\\{r \\in \\mathcal{R} \\mid \\hat{p}(r) \\ge \\tau\\}$。如果此集合非空，$r^{\\star}$ 就被选为该集合中的最小半径。如果没有半径达到该阈值，则选择产生最大观测成功率的半径作为 $r^{\\star}$，并通过选择其中最小的半径来打破平局。与 $r^{\\star}$ 对应的成功率记为 $p^{\\star} = \\hat{p}(r^{\\star})$。\n\n其次，使用这个经验半径 $r^{\\star}$，我们计算多起点法所需的起始点数量 $N$。最小值 $m$ 的吸引盆被近似为一个半径为 $r^{\\star}$ 的圆盘，其面积为 $A_{\\text{basin}} = \\pi (r^{\\star})^2$。总搜索域 $D$ 是一个矩形 $[\\ell_x, u_x] \\times [\\ell_y, u_y]$，其面积为 $A(D) = (u_x - \\ell_x)(u_y - \\ell_y)$。从 $D$ 中随机均匀选择一个点落入近似吸引盆的概率是它们面积的比值，记为 $p_{\\text{area}} = \\min(1, A_{\\text{basin}} / A(D))$。\n\n对于一个从 $D$ 中均匀抽样 $N$ 个独立起始点的多起点法，所有 $N$ 个点都未落入吸引盆的概率是 $(1 - p_{\\text{area}})^N$。因此，至少有一个点落入吸引盆的概率是 $1 - (1 - p_{\\text{area}})^N$。我们必须找到满足目标覆盖概率 $q$ 的最小整数 $N$：\n$$1 - (1 - p_{\\text{area}})^N \\ge q$$\n整理不等式，我们得到 $(1 - p_{\\text{area}})^N \\le 1 - q$。对两边取自然对数得到 $N \\log(1 - p_{\\text{area}}) \\le \\log(1 - q)$。由于 $p_{\\text{area}} \\in (0, 1)$，$\\log(1 - p_{\\text{area}})$ 是负数。除以它会反转不等号：\n$$N \\ge \\frac{\\log(1-q)}{\\log(1-p_{\\text{area}})}$$\n满足此式的最小整数 $N$ 是通过对右侧表达式取上取整得到的，$N = \\left\\lceil \\frac{\\log(1-q)}{\\log(1-p_{\\text{area}})} \\right\\rceil$。在 $p_{\\text{area}} = 1$ 的特殊情况下，单个起始点即可保证覆盖，因此 $N=1$。\n\n实现将使用 `numpy` 进行数值计算和随机数生成，使用 `scipy.optimize.minimize` 进行局部搜索算法。根据规定，BFGS 方法需要解析梯度。\n\n对于 Himmelblau 函数 $f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$，其偏导数为：\n$$ \\frac{\\partial f}{\\partial x} = 4x(x^2 + y - 11) + 2(x + y^2 - 7) $$\n$$ \\frac{\\partial f}{\\partial y} = 2(x^2 + y - 11) + 4y(x + y^2 - 7) $$\n对于 Rosenbrock 函数 $f(x,y) = (1-x)^2 + 100(y - x^2)^2$，其偏导数为：\n$$ \\frac{\\partial f}{\\partial x} = -2(1-x) - 400x(y-x^2) $$\n$$ \\frac{\\partial f}{\\partial y} = 200(y-x^2) $$\n这些函数及其梯度被实现用于解决测试用例。为随机数生成器设置固定种子可确保蒙特卡洛模拟的可复现性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Set a fixed seed for the random number generator for reproducibility.\n    np.random.seed(0)\n\n    # Define objective functions and their gradients.\n    def himmelblau(p):\n        x, y = p\n        return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n\n    def grad_himmelblau(p):\n        x, y = p\n        df_dx = 4 * x * (x**2 + y - 11) + 2 * (x + y**2 - 7)\n        df_dy = 2 * (x**2 + y - 11) + 4 * y * (x + y**2 - 7)\n        return np.array([df_dx, df_dy])\n\n    def rosenbrock(p):\n        x, y = p\n        return (1 - x)**2 + 100 * (y - x**2)**2\n\n    def grad_rosenbrock(p):\n        x, y = p\n        df_dx = -2 * (1 - x) - 400 * x * (y - x**2)\n        df_dy = 200 * (y - x**2)\n        return np.array([df_dx, df_dy])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": himmelblau,\n            \"grad_f\": grad_himmelblau,\n            \"method\": \"BFGS\",\n            \"m\": np.array([3.0, 2.0]),\n            \"D\": [-6.0, 6.0, -6.0, 6.0],\n            \"R\": [0.15, 0.30, 0.60, 1.20],\n            \"S\": 120,\n            \"tau\": 0.9,\n            \"q\": 0.95,\n            \"eps\": 0.01,\n        },\n        {\n            \"f\": himmelblau,\n            \"grad_f\": None,\n            \"method\": \"Nelder-Mead\",\n            \"m\": np.array([-2.805118, 3.131312]),\n            \"D\": [-6.0, 6.0, -6.0, 6.0],\n            \"R\": [0.10, 0.30, 0.60, 1.00],\n            \"S\": 100,\n            \"tau\": 0.8,\n            \"q\": 0.90,\n            \"eps\": 0.02,\n        },\n        {\n            \"f\": rosenbrock,\n            \"grad_f\": grad_rosenbrock,\n            \"method\": \"BFGS\",\n            \"m\": np.array([1.0, 1.0]),\n            \"D\": [-2.0, 2.0, -2.0, 2.0],\n            \"R\": [0.05, 0.10, 0.20, 0.40],\n            \"S\": 150,\n            \"tau\": 0.95,\n            \"q\": 0.99,\n            \"eps\": 0.01,\n        },\n    ]\n\n    def sample_in_disk(center, radius, n_samples):\n        # Sample points uniformly from a disk of given center and radius.\n        r_vals = radius * np.sqrt(np.random.uniform(0, 1, size=n_samples))\n        theta_vals = 2 * np.pi * np.random.uniform(0, 1, size=n_samples)\n        x_coords = center[0] + r_vals * np.cos(theta_vals)\n        y_coords = center[1] + r_vals * np.sin(theta_vals)\n        return np.stack((x_coords, y_coords), axis=-1)\n\n    def run_case(case):\n        # Execute the full procedure for a single test case.\n        f, grad_f, method, m, domain, radii, S, tau, q, eps = (\n            case[\"f\"], case[\"grad_f\"], case[\"method\"], case[\"m\"], case[\"D\"],\n            case[\"R\"], case[\"S\"], case[\"tau\"], case[\"q\"], case[\"eps\"]\n        )\n\n        phat_results = {}\n        for r_test in radii:\n            initial_points = sample_in_disk(m, r_test, S)\n            success_count = 0\n            for x0 in initial_points:\n                res = minimize(f, x0, method=method, jac=grad_f, options={'maxiter': 1000})\n                if res.success and np.linalg.norm(res.x - m) = eps:\n                    success_count += 1\n            phat_results[r_test] = success_count / S\n        \n        # Determine r_star and p_star based on the specified rules.\n        successful_radii = [r for r, p in phat_results.items() if p = tau]\n        \n        if successful_radii:\n            r_star = min(successful_radii)\n        else:\n            max_p = -1.0\n            # Find max probability among all radii\n            for p in phat_results.values():\n                if p > max_p:\n                    max_p = p\n            # Find radii that achieve max_p, then choose the smallest\n            best_radii = [r for r, p in phat_results.items() if p == max_p]\n            r_star = min(best_radii)\n            \n        p_star = phat_results[r_star]\n\n        # Calculate the required number of multistarts, N.\n        lx, ux, ly, uy = domain\n        A_D = (ux - lx) * (uy - ly)\n        A_basin = np.pi * (r_star**2)\n        p_area = min(1.0, A_basin / A_D)\n\n        if p_area >= 1.0:\n            N = 1\n        elif p_area == 0.0:\n            N = float('inf') # Should not happen with r_star > 0\n        else:\n            N = math.ceil(math.log(1 - q) / math.log(1 - p_area))\n        \n        return int(N), r_star, p_star\n\n    results = []\n    for case in test_cases:\n        result = run_case(case)\n        results.append(result)\n\n    # Format the results into the required string format.\n    formatted_results = []\n    for N, r_star, p_star in results:\n        formatted_results.append(f\"[{int(N)},{r_star:.6f},{p_star:.6f}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "当我们运行多次局部优化后，会得到一系列局部最优解。我们不仅关心是否找到了全局最优解，还关心这些解的质量分布。这个练习将我们的视角从“寻找”转向“分析”，它将每次找到的局部最小值深度视为一个随机变量，并利用次序统计量的理论来预测在 $m$ 次重启后我们能期望得到的最佳结果。你还将学习如何使用非参数自助法（bootstrapping），从已有的观测数据中稳健地估计未来的表现，这是一个在数据驱动的优化中非常强大的工具。",
            "id": "3186375",
            "problem": "考虑一个多起点全局优化设定，其中每次独立重启产生一个局部最小值深度，该深度被建模为一个独立同分布的随机变量 $X$，其累积分布函数为 $F$。设 $X_1,\\dots,X_m$ 为从 $F$ 中抽取的 $m$ 个独立样本，并将在 $m$ 次重启后找到的最优深度定义为 $M_m = \\min\\{X_1,\\dots,X_m\\}$。\n\n任务 1 (推导)：仅从累积分布函数、独立性和顺序统计量的定义，以及非负随机变量的尾部积分恒等式出发，推导期望最优深度 $\\,\\mathbb{E}[M_m]\\,$ 关于 $F$ 和 $m$ 的通用表达式。你的推导除了所述定义外，不得对 $F$ 作任何特定的参数形式假设。\n\n任务 2 (特定分布的闭式解)：使用任务 1 的结果，为以下每种分布族推导 $\\,\\mathbb{E}[M_m]\\,$ 的闭式表达式，其中 $X$ 的支撑集为 $[0,\\infty)$ 或其上的一个区间：\n- 在 $[a,b]$ 上的均匀分布，参数为 $a$ 和 $b$，其中 $a  b$ 且 $a \\ge 0$。\n- 率参数为 $\\lambda > 0$ 的指数分布。\n- 形状为 $k > 0$ 且尺度为 $\\lambda > 0$ 的威布尔分布。\n\n任务 3 (在线估计)：假设您已进行 $r$ 次独立重启，并观测到一组局部最小值深度 $\\{x^{\\mathrm{obs}}_1, \\dots, x^{\\mathrm{obs}}_r\\}$。在不作任何参数假设的情况下，描述一个可复现的、基于非参数自助法（nonparametric bootstrap）的数值过程，以估计未来在 $m$ 次新重启中预期的最优深度 $\\mathbb{E}[M_m]$。说明该过程如何使用观测数据来近似底层的未知分布 $F$。对于一个有 $B=20000$ 次重复的自助法，指定使该过程可复现所需的步骤，使用种子 $12345$。\n\n最后，为以下四种情况计算并报告理论期望值和自助法估计值，将所有结果四舍五入至小数点后六位，并以单个逗号分隔的列表形式呈现，不含空格：`[theory1,boot1,theory2,boot2,...]`。\n1. $X \\sim \\mathrm{Uniform}(0, 1)$, $m=5$, 观测值 = `[0.12, 0.34, 0.07, 0.56, 0.91, 0.22, 0.44, 0.03]`\n2. $X \\sim \\mathrm{Exponential}(2.0)$, $m=10$, 观测值 = `[0.10, 0.40, 0.01, 0.25, 0.02, 0.33, 0.12, 0.05]`\n3. $X \\sim \\mathrm{Weibull}(k=2, \\lambda=1.5)$, $m=3$, 观测值 = `[0.20, 1.00, 1.35, 0.50, 2.10, 0.95, 0.75, 0.30]`\n4. $X \\sim \\mathrm{Uniform}(0, 1)$, $m=1$, 观测值 = `[0.80, 0.40, 0.10, 0.60, 0.30]`",
            "solution": "该问题被评估为有效，因为它具有科学依据、问题明确、客观且内部一致。它提出了一个应用于优化领域的概率论和计算统计学中的标准且不平凡的问题。为得到唯一解所需的所有数据和定义均已提供。我们开始进行推导和求解。\n\n### 任务 1：$\\mathbb{E}[M_m]$ 通用表达式的推导\n\n设 $X$ 是一个非负随机变量，表示局部最小值的深度，其累积分布函数 (CDF) 为 $F(x) = P(X \\le x)$。给定 $m$ 个独立同分布 (i.i.d.) 的随机变量 $X_1, \\dots, X_m$，每个的 CDF 均为 $F$。找到的最优深度定义为一阶顺序统计量 $M_m = \\min\\{X_1, \\dots, X_m\\}$。我们的目标是推导 $M_m$ 的期望值（记为 $\\mathbb{E}[M_m]$）的通用表达式。\n\n首先，我们确定 $M_m$ 的 CDF，记为 $F_{M_m}(x)$。根据定义，$F_{M_m}(x) = P(M_m \\le x)$。处理其互补事件通常更容易：\n$$F_{M_m}(x) = 1 - P(M_m > x)$$\n$M_m > x$ 事件等价于所有 $X_i$ 都大于 $x$ 的事件：\n$$P(M_m > x) = P(\\min\\{X_1, \\dots, X_m\\} > x) = P(X_1 > x, X_2 > x, \\dots, X_m > x)$$\n由于随机变量 $X_1, \\dots, X_m$ 是独立的，其联合概率是各个概率的乘积：\n$$P(M_m > x) = \\prod_{i=1}^{m} P(X_i > x)$$\n此外，因为这些变量是同分布的，所以对所有 $i$，$P(X_i > x)$ 都相同。这个概率被称为生存函数 $S(x)$，由 $S(x) = 1 - F(x)$ 给出。因此：\n$$P(M_m > x) = (P(X > x))^m = (1 - F(x))^m$$\n将此代入 $M_m$ 的 CDF 表达式中，我们得到：\n$$F_{M_m}(x) = 1 - (1 - F(x))^m$$\n问题指定 $X$ 是一个非负随机变量，意味着其支撑集在 $[0, \\infty)$ 或其子区间上。因此，$M_m$ 也是一个非负随机变量。对于任何非负随机变量 $Y$，其期望值可以使用尾部积分恒等式计算：\n$$\\mathbb{E}[Y] = \\int_0^\\infty P(Y > y) \\, dy = \\int_0^\\infty (1 - F_Y(y)) \\, dy$$\n将此恒等式应用于 $M_m$，我们得到：\n$$\\mathbb{E}[M_m] = \\int_0^\\infty P(M_m > x) \\, dx$$\n使用我们之前推导的 $P(M_m > x)$ 的表达式，我们得到期望最优深度的通用公式：\n$$\\mathbb{E}[M_m] = \\int_0^\\infty (1 - F(x))^m \\, dx$$\n此表达式对任何具有 CDF $F$ 的非负随机变量 $X$ 均有效。\n\n### 任务 2：特定分布的闭式表达式\n\n现在我们应用通用公式，为三种特定的分布族推导 $\\mathbb{E}[M_m]$ 的闭式表达式，假设它们的支撑集为非负。\n\n**1. $[a, b]$ 上的均匀分布**\n对于这种情况，我们假设 $0 \\le a  b$。$X$ 的 CDF 为：\n$$F(x) = \\begin{cases} 0  x  a \\\\ \\frac{x-a}{b-a}  a \\le x \\le b \\\\ 1  x > b \\end{cases}$$\n生存函数 $S(x) = 1 - F(x)$ 为：\n$$1 - F(x) = \\begin{cases} 1  x  a \\\\ 1 - \\frac{x-a}{b-a} = \\frac{b-x}{b-a}  a \\le x \\le b \\\\ 0  x > b \\end{cases}$$\n$\\mathbb{E}[M_m]$ 的积分必须根据被积函数的支撑集进行分段：\n$$\\mathbb{E}[M_m] = \\int_0^a (1)^m \\, dx + \\int_a^b \\left(\\frac{b-x}{b-a}\\right)^m \\, dx + \\int_b^\\infty (0)^m \\, dx$$\n$$\\mathbb{E}[M_m] = [x]_0^a + \\frac{1}{(b-a)^m} \\int_a^b (b-x)^m \\, dx = a + \\frac{1}{(b-a)^m} \\left[ -\\frac{(b-x)^{m+1}}{m+1} \\right]_a^b$$\n$$\\mathbb{E}[M_m] = a + \\frac{1}{(b-a)^m} \\left( 0 - \\left( -\\frac{(b-a)^{m+1}}{m+1} \\right) \\right) = a + \\frac{(b-a)^{m+1}}{(b-a)^m(m+1)}$$\n这简化为闭式表达式：\n$$\\mathbb{E}[M_m] = a + \\frac{b-a}{m+1}$$\n\n**2. 率参数为 $\\lambda > 0$ 的指数分布**\n支撑集为 $[0, \\infty)$。CDF 为 $F(x) = 1 - e^{-\\lambda x}$（对于 $x \\ge 0$）。\n生存函数为 $1 - F(x) = e^{-\\lambda x}$。\n将此代入通用公式：\n$$\\mathbb{E}[M_m] = \\int_0^\\infty (e^{-\\lambda x})^m \\, dx = \\int_0^\\infty e^{-m\\lambda x} \\, dx$$\n这是一个标准积分：\n$$\\mathbb{E}[M_m] = \\left[ -\\frac{1}{m\\lambda} e^{-m\\lambda x} \\right]_0^\\infty = 0 - \\left( -\\frac{1}{m\\lambda} e^0 \\right) = \\frac{1}{m\\lambda}$$\n闭式表达式为：\n$$\\mathbb{E}[M_m] = \\frac{1}{m\\lambda}$$\n这与已知的结果一致，即 $m$ 个独立同分布的 exponential($\\lambda$) 变量的最小值是一个 exponential($m\\lambda$) 变量，其均值为 $1/(m\\lambda)$。\n\n**3. 形状为 $k > 0$ 且尺度为 $\\lambda > 0$ 的威布尔分布**\n支撑集为 $[0, \\infty)$。CDF 为 $F(x) = 1 - e^{-(x/\\lambda)^k}$（对于 $x \\ge 0$）。\n生存函数为 $1 - F(x) = e^{-(x/\\lambda)^k}$。\n应用通用公式：\n$$\\mathbb{E}[M_m] = \\int_0^\\infty \\left(e^{-(x/\\lambda)^k}\\right)^m \\, dx = \\int_0^\\infty e^{-m(x/\\lambda)^k} \\, dx$$\n被积函数对应于一个形状为 $k$ 且尺度参数被修改的威布尔变量的生存函数。具体来说，$m(x/\\lambda)^k = (x/(\\lambda m^{-1/k}))^k$。这表明 $M_m$ 也是一个威布尔分布的随机变量，其形状参数为 $k$，尺度参数为 $\\lambda_m = \\lambda m^{-1/k}$。\n一个形状为 $k'$、尺度为 $\\lambda'$ 的威布尔变量的期望值由 $\\lambda' \\Gamma(1 + 1/k')$ 给出，其中 $\\Gamma$ 是伽马函数。\n对于 $M_m$，参数为 $k' = k$ 和 $\\lambda' = \\lambda m^{-1/k}$。因此，期望值为：\n$$\\mathbb{E}[M_m] = (\\lambda m^{-1/k}) \\Gamma\\left(1 + \\frac{1}{k}\\right)$$\n这是威布尔分布情况下的闭式表达式。\n\n### 任务 3：通过非参数自助法进行在线估计\n\n给定一组 $r$ 个观测到的局部最小值深度 $\\{x^{\\mathrm{obs}}_1, \\dots, x^{\\mathrm{obs}}_r\\}$，我们可以在不假设底层分布 $F$ 的参数形式的情况下，估计未来预算为 $m$ 次重启的 $\\mathbb{E}[M_m]$。这通过非参数自助法实现。\n\n**1. 分布的经验估计量：**\n未知的真实 CDF $F$ 由经验 CDF (ECDF) 近似，记为 $\\hat{F}_r(x)$。ECDF 是从 $r$ 个观测值构建的，定义为小于或等于 $x$ 的观测值所占的比例：\n$$\\hat{F}_r(x) = \\frac{1}{r} \\sum_{i=1}^r \\mathbb{I}(x^{\\mathrm{obs}}_i \\le x)$$\n其中 $\\mathbb{I}(\\cdot)$ 是指示函数。该函数表示一个离散概率分布，它在每个观测值 $x^{\\mathrm{obs}}_i$ 上放置了 $1/r$ 的概率质量。\n\n**2. 估计量的计算：**\n自助法原理涉及用其经验估计 $\\hat{F}_r$ 替代真实分布 $F$。目标量 $\\mathbb{E}[M_m] = \\mathbb{E}_F[\\min\\{X_1, \\dots, X_m\\}]$ 通过 $\\hat{\\mathbb{E}}[M_m] = \\mathbb{E}_{\\hat{F}_r}[\\min\\{X^*_1, \\dots, X^*_m\\}]$ 来估计，其中每个 $X^*_j$ 是从经验分布 $\\hat{F}_r$ 中抽取的随机变量。从 $\\hat{F}_r$ 中抽样等价于从原始观测集合 $\\{x^{\\mathrm{obs}}_1, \\dots, x^{\\mathrm{obs}}_r\\}$ 中进行有放回抽样。\n\n期望值 $\\mathbb{E}_{\\hat{F}_r}$ 通常使用蒙特卡洛模拟来近似。完整步骤如下：\n- 设置自助法重复次数 $B$（例如，$B=20000$）。\n- 对于从 $1$ 到 $B$ 的每次重复 $j$：\n    - 通过从观测数据 $\\{x^{\\mathrm{obs}}_1, \\dots, x^{\\mathrm{obs}}_r\\}$ 中有放回地抽取 $m$ 个值，生成一个大小为 $m$ 的“自助样本”。设该样本为 $\\{x^*_{j,1}, \\dots, x^*_{j,m}\\}$。\n    - 计算该自助样本的最小值：$M^*_{m,j} = \\min\\{x^*_{j,1}, \\dots, x^*_{j,m}\\}$。\n- $\\mathbb{E}[M_m]$ 的自助法估计值是 $B$ 个自助法最小值的算术平均值：\n$$\\hat{\\mathbb{E}}[M_m] = \\frac{1}{B} \\sum_{j=1}^B M^*_{m,j}$$\n\n**3. 可复现性：**\n重抽样过程依赖于伪随机数生成器 (PRNG)。为确保自助法估计是确定性且可复现的，必须在重抽样循环开始前用一个固定的种子初始化 PRNG。对于本问题，种子被指定为 $12345$。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import gamma\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing theoretical and bootstrap estimates for the expected minimum.\n    \"\"\"\n\n    def theory_uniform(a, b, m):\n        \"\"\"\n        Computes the theoretical expected minimum for a Uniform distribution.\n        Formula: E[M_m] = a + (b-a)/(m+1)\n        \"\"\"\n        return a + (b - a) / (m + 1)\n\n    def theory_exponential(rate, m):\n        \"\"\"\n        Computes the theoretical expected minimum for an Exponential distribution.\n        Formula: E[M_m] = 1 / (m * rate)\n        \"\"\"\n        return 1 / (m * rate)\n\n    def theory_weibull(shape_k, scale_lambda, m):\n        \"\"\"\n        Computes the theoretical expected minimum for a Weibull distribution.\n        Formula: E[M_m] = scale * (m**(-1/k)) * Gamma(1 + 1/k)\n        \"\"\"\n        return scale_lambda * (m**(-1 / shape_k)) * gamma(1 + 1 / shape_k)\n\n    def bootstrap_estimate(observed_data, m, B, seed):\n        \"\"\"\n        Computes the nonparametric bootstrap estimate of the expected minimum.\n        \n        Args:\n            observed_data (list or np.ndarray): The observed minima values.\n            m (int): The number of restarts for which to estimate the expected minimum.\n            B (int): The number of bootstrap replicates.\n            seed (int): The seed for the pseudorandom number generator.\n        \n        Returns:\n            float: The bootstrap estimate of E[M_m].\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        bootstrap_mins = []\n        \n        # Convert to numpy array for efficiency\n        observed_array = np.array(observed_data)\n        \n        for _ in range(B):\n            # Draw a sample of size m with replacement from the observed data\n            bootstrap_sample = rng.choice(observed_array, size=m, replace=True)\n            # Compute the minimum of the bootstrap sample\n            current_min = np.min(bootstrap_sample)\n            bootstrap_mins.append(current_min)\n            \n        # The estimate is the mean of the bootstrap minimums\n        return np.mean(bootstrap_mins)\n\n    # Problem parameters\n    B_REPLICATES = 20000\n    SEED = 12345\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"dist\": \"uniform\",\n            \"params\": {\"a\": 0, \"b\": 1},\n            \"m\": 5,\n            \"observed_minima\": [0.12, 0.34, 0.07, 0.56, 0.91, 0.22, 0.44, 0.03]\n        },\n        {\n            \"dist\": \"exponential\",\n            \"params\": {\"rate\": 2.0},\n            \"m\": 10,\n            \"observed_minima\": [0.10, 0.40, 0.01, 0.25, 0.02, 0.33, 0.12, 0.05]\n        },\n        {\n            \"dist\": \"weibull\",\n            \"params\": {\"shape_k\": 2.0, \"scale_lambda\": 1.5},\n            \"m\": 3,\n            \"observed_minima\": [0.20, 1.00, 1.35, 0.50, 2.10, 0.95, 0.75, 0.30]\n        },\n        {\n            \"dist\": \"uniform\",\n            \"params\": {\"a\": 0, \"b\": 1},\n            \"m\": 1,\n            \"observed_minima\": [0.80, 0.40, 0.10, 0.60, 0.30]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        m = case[\"m\"]\n        observed = case[\"observed_minima\"]\n        \n        # Calculate theoretical value\n        if case[\"dist\"] == \"uniform\":\n            theory_val = theory_uniform(case[\"params\"][\"a\"], case[\"params\"][\"b\"], m)\n        elif case[\"dist\"] == \"exponential\":\n            theory_val = theory_exponential(case[\"params\"][\"rate\"], m)\n        elif case[\"dist\"] == \"weibull\":\n            theory_val = theory_weibull(case[\"params\"][\"shape_k\"], case[\"params\"][\"scale_lambda\"], m)\n        \n        # Calculate bootstrap estimate\n        bootstrap_val = bootstrap_estimate(observed, m, B_REPLICATES, SEED)\n        \n        # Format results to 6 decimal places and append\n        results.append(f\"{theory_val:.6f}\")\n        results.append(f\"{bootstrap_val:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}