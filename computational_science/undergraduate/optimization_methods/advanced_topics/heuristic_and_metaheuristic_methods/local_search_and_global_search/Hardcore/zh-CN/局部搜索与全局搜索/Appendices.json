{
    "hands_on_practices": [
        {
            "introduction": "在尝试解决具有多个局部最优解的复杂问题之前，深入理解这种复杂性是如何产生的，将极具启发性。本练习将引导我们通过一个思想实验，构造一个“对抗性”的优化目标函数 。我们将看到，通过将一个简单的凸函数 $g(y) = \\frac{1}{2}(y - a)^2$ 与一个周期性的“域扭曲”函数 $h(x) = \\cos(kx)$ 复合，可以生成一个布满局部陷阱的复杂景观，从而揭示局部搜索方法失效的根源。",
            "id": "3145555",
            "problem": "考虑一个一维目标函数，它由一个凸外层函数和一个域扭曲内层函数复合而成。设外层函数为 $g(y) = \\tfrac{1}{2}(y - a)^{2}$，其中 $a \\in (-1, 1)$，内层函数为 $h(x) = \\cos(kx)$，其中 $k \\in \\mathbb{N}$ 是一个正整数。定义复合目标函数 $f(x) = g(h(x)) = \\tfrac{1}{2}(\\cos(kx) - a)^{2}$，定义域为 $x \\in \\mathbb{R}$。内层函数 $h$ 将定义域周期性地折叠到值域 $[-1, 1]$ 内，而该复合操作将 $g$ 中的结构在这些折叠上复制。考虑通过梯度下降进行局部搜索，其步长 $\\alpha  0$ 固定且足够小，以确保在严格局部最小值附近的局部收敛性。吸引盆被定义为所有初始点 $x_{0}$ 的集合，这些点的梯度下降迭代序列收敛到同一个局部最小值。\n\n关于函数景观 $f(x)$ 以及由这种域扭曲所产生的吸引盆，以下哪些陈述是正确的？选择所有适用项。\n\nA. 对于任何整数 $k \\ge 1$ 和任何满足 $|a|  1$ 的 $a$，函数 $f(x)$ 在每个长度为 $2\\pi/k$ 的区间内恰好有 $2$ 个局部最小值，位于 $\\cos(kx) = a$ 的点上。\n\nB. 函数 $f(x)$ 的驻点集合仅由 $\\cos(kx) = a$ 的解构成；当 $a \\neq \\pm 1$ 时，没有由 $h'(x) = 0$ 产生的驻点。\n\nC. 在具有足够小的恒定步长 $\\alpha  0$ 的梯度下降法下，几乎每个初始点 $x_{0}$ 都会收敛到 $f(x)$ 的一个局部最小值；吸引盆是由 $\\sin(kx) = 0$ 的点为边界的开区间。\n\nD. 在保持 $\\alpha$ 固定的情况下增加 $k$，会使吸引盆变宽，从而改善全局收敛性，因为梯度大小按 $k$ 的比例缩小。\n\nE. 当 $|a|  1$ 时，在区间 $[0, 2\\pi]$ 上，$f$ 的不同局部最小值的总数为 $2k$。",
            "solution": "问题陈述已经过验证，被认为是数学上合理、适定且无歧义的。它为分析复合目标函数的性质提供了一个清晰的框架，这是优化理论中的一个标准课题。所有必要的定义和约束条件都已提供。因此，我们可以开始解题。\n\n目标函数由复合函数 $f(x) = g(h(x))$ 给出，其中外层函数为 $g(y) = \\tfrac{1}{2}(y - a)^2$（$a \\in (-1, 1)$），内层函数为 $h(x) = \\cos(kx)$（$k \\in \\mathbb{N}$ 为正整数）。因此，函数 $f(x)$ 为 $f(x) = \\tfrac{1}{2}(\\cos(kx) - a)^2$。定义域为 $x \\in \\mathbb{R}$。\n\n为了分析 $f(x)$ 的函数景观，我们首先通过将其关于 $x$ 的一阶导数设为零来找到其驻点。使用链式法则，导数为：\n$$ f'(x) = \\frac{d}{dx} \\left[ \\tfrac{1}{2}(\\cos(kx) - a)^2 \\right] = (\\cos(kx) - a) \\cdot \\frac{d}{dx}(\\cos(kx)) $$\n$$ f'(x) = (\\cos(kx) - a) \\cdot (-k \\sin(kx)) = -k \\sin(kx) (\\cos(kx) - a) $$\n对于驻点，必须有 $f'(x) = 0$。该方程成立当且仅当满足以下两个条件之一：\n1. $\\sin(kx) = 0$\n2. $\\cos(kx) - a = 0$，即 $\\cos(kx) = a$\n\n接下来，我们使用二阶导数检验来对这些驻点进行分类。二阶导数 $f''(x)$ 是通过使用乘法法则对 $f'(x)$ 求导得到的：\n$$ f''(x) = \\frac{d}{dx} [-k \\sin(kx) (\\cos(kx) - a)] $$\n$$ f''(x) = -k \\left[ (\\frac{d}{dx}\\sin(kx))(\\cos(kx) - a) + \\sin(kx)(\\frac{d}{dx}(\\cos(kx) - a)) \\right] $$\n$$ f''(x) = -k \\left[ (k \\cos(kx))(\\cos(kx) - a) + \\sin(kx)(-k \\sin(kx)) \\right] $$\n$$ f''(x) = -k^2 \\left[ \\cos^2(kx) - a\\cos(kx) - \\sin^2(kx) \\right] $$\n使用二倍角恒等式 $\\cos(2\\theta) = \\cos^2(\\theta) - \\sin^2(\\theta)$，上式可简化为：\n$$ f''(x) = -k^2 [\\cos(2kx) - a\\cos(kx)] = k^2 [a\\cos(kx) - \\cos(2kx)] $$\n\n现在我们可以对这两类驻点进行分类：\n\n情况 1：$\\cos(kx) = a$。\n由于给定 $a$ 在区间 $(-1, 1)$ 内，此方程总是有关于 $x$ 的解。\n在这些点上，我们计算二阶导数：\n$$ f''(x) = k^2 [a(a) - \\cos(2kx)] $$\n使用恒等式 $\\cos(2\\theta) = 2\\cos^2(\\theta) - 1$，我们有：\n$$ f''(x) = k^2 [a^2 - (2\\cos^2(kx) - 1)] = k^2 [a^2 - (2a^2 - 1)] = k^2 (1 - a^2) $$\n因为 $k$ 是正整数，所以 $k^2  0$。因为 $a \\in (-1, 1)$，所以我们有 $a^2  1$，这意味着 $1 - a^2  0$。\n因此，$f''(x) = k^2(1 - a^2)  0$。根据二阶导数检验，所有满足 $\\cos(kx) = a$ 的点都是严格局部最小值。在这些点上，函数值为 $f(x) = \\tfrac{1}{2}(a - a)^2 = 0$，这是全局最小值。\n\n情况 2：$\\sin(kx) = 0$。\n这个条件意味着 $kx = n\\pi$，其中 $n \\in \\mathbb{Z}$ 为某个整数。\n在这些点上，$\\cos(kx) = \\cos(n\\pi) = (-1)^n$。并且，$\\cos(2kx) = \\cos(2n\\pi) = 1$。\n我们计算二阶导数：\n$$ f''(x) = k^2 [a\\cos(kx) - \\cos(2kx)] = k^2 [a(-1)^n - 1] $$\n由于 $a \\in (-1, 1)$，我们有 $-1  a  1$。因此可得 $-1  a(-1)^n  1$。所以，$a(-1)^n - 1$ 是一个介于 $-2$ 和 $0$ 之间的数（不包括端点）。\n具体来说，$a(-1)^n - 1  0$。因为 $k^2  0$，我们有 $f''(x)  0$。\n根据二阶导数检验，所有满足 $\\sin(kx) = 0$ 的点都是严格局部最大值。\n\n有了这个关于驻点的完整分类，我们就可以评估每个选项了。\n\nA. 对于任何整数 $k \\ge 1$ 和任何满足 $|a|  1$ 的 $a$，函数 $f(x)$ 在每个长度为 $2\\pi/k$ 的区间内恰好有 $2$ 个局部最小值，位于 $\\cos(kx) = a$ 的点上。\n函数 $f(x) = \\tfrac{1}{2}(\\cos(kx) - a)^2$ 是周期函数，其基本周期为 $P = 2\\pi/k$。为了找出一个周期内的局部最小值数量，我们可以考虑区间 $x \\in [0, 2\\pi/k)$。在这个区间内，自变量 $u = kx$ 的范围是 $[0, 2\\pi)$。我们需要计算在 $u \\in [0, 2\\pi)$ 上方程 $\\cos(u) = a$ 的解的个数。\n由于 $|a|  1$，方程 $\\cos(u) = a$ 在区间 $[0, 2\\pi)$ 内恰好有两个解：一个是 $u_1 = \\arccos(a) \\in (0, \\pi)$，另一个是 $u_2 = 2\\pi - \\arccos(a) \\in (\\pi, 2\\pi)$。这对应于区间 $[0, 2\\pi/k)$ 内的两个不同点 $x_1 = u_1/k$ 和 $x_2 = u_2/k$。如前所述，这些点是局部最小值。\n因此，这个陈述是 **正确的**。\n\nB. 函数 $f(x)$ 的驻点集合仅由 $\\cos(kx) = a$ 的解构成；当 $a \\neq \\pm 1$ 时，没有由 $h'(x) = 0$ 产生的驻点。\n我们对驻点的推导表明，它们出现在 $f'(x) = -k \\sin(kx) (\\cos(kx) - a) = 0$ 的地方。这导致两组解：$\\cos(kx) = a$ 和 $\\sin(kx) = 0$。内层函数是 $h(x) = \\cos(kx)$，所以它的导数是 $h'(x) = -k\\sin(kx)$。条件 $h'(x) = 0$ 正是 $\\sin(kx) = 0$（因为 $k \\neq 0$）。我们已经证明了满足这个条件的点确实是驻点（具体来说，是局部最大值）。该陈述声称没有由 $h'(x)=0$ 产生的驻点，这是错误的。\n因此，这个陈述是 **不正确的**。\n\nC. 在具有足够小的恒定步长 $\\alpha  0$ 的梯度下降法下，几乎每个初始点 $x_{0}$ 都会收敛到 $f(x)$ 的一个局部最小值；吸引盆是由 $\\sin(kx) = 0$ 的点为边界的开区间。\n函数 $f(x)$ 是一个一维、连续可微的函数。它的驻点要么是局部最小值，要么是局部最大值。对于梯度下降法，局部最小值是稳定不动点，而局部最大值是不稳定不动点。收敛到局部最大值的初始点集合仅包含最大值本身。这个集合是可数的，因此其勒贝格测度为零。因此，“几乎所有”初始点（除了最大值点）都将收敛到局部最小值。\n在一维景观中，相邻局部最小值的吸引盆之间的边界是位于它们之间的局部最大值。这些最大值点充当“分界点”或“分水岭”。我们已经证明了 $f(x)$ 的局部最大值出现在 $\\sin(kx) = 0$ 的点上。因此，局部最小值的吸引盆是由这些最大值点为边界的开区间。\n因此，这个陈述是 **正确的**。\n\nD. 在保持 $\\alpha$ 固定的情况下增加 $k$，会使吸引盆变宽，从而改善全局收敛性，因为梯度大小按 $k$ 的比例缩小。\n根据对选项 C 的分析，吸引盆由局部最大值界定，这些最大值位于满足 $kx = n\\pi$ 或 $x = n\\pi/k$ 的 $x$ 值处。一个典型吸引盆的宽度是两个连续最大值之间的距离，即 $|(n+1)\\pi/k - n\\pi/k| = \\pi/k$。随着 $k$ 的增加，宽度 $\\pi/k$ 减小，意味着吸引盆变得更窄，而不是更宽。\n该陈述的第二部分是关于梯度的大小。梯度为 $f'(x) = -k \\sin(kx) (\\cos(kx) - a)$。其大小为 $|f'(x)| = k |\\sin(kx) (\\cos(kx) - a)|$。在其他条件不变的情况下，梯度的大小与 $k$ 成正比，也就是说，它随着 $k$ 的增加而增加。陈述中的两个论断都是错误的。\n因此，这个陈述是 **不正确的**。\n\nE. 当 $|a|  1$ 时，在区间 $[0, 2\\pi]$ 上，$f$ 的不同局部最小值的总数为 $2k$。\n我们需要计算在 $x \\in [0, 2\\pi]$ 上方程 $\\cos(kx) = a$ 的不同解的个数。\n令 $u = kx$。当 $x$ 在 $[0, 2\\pi]$ 范围内变化时，$u$ 在 $[0, 2\\pi k]$ 范围内变化。\n我们在区间 $u \\in [0, 2\\pi k]$ 内计算 $\\cos(u) = a$ 的解的个数。区间 $[0, 2\\pi k]$ 由 $k$ 个余弦函数的完整周期组成，具体为区间 $[0, 2\\pi), [2\\pi, 4\\pi), \\dots, [2\\pi(k-1), 2\\pi k)$。在每个这样的 $k$ 个区间中，方程 $\\cos(u) = a$ 都有恰好 $2$ 个解，因为 $|a|  1$。\n因此，解的总数是 $k \\times 2 = 2k$。这 $2k$ 个关于 $u$ 的解都是不同的，且位于 $[0, 2\\pi k)$ 内。因为 $x=u/k$，它们对应于区间 $[0, 2\\pi)$ 内的 $2k$ 个不同的 $x$ 的解。端点 $x=2\\pi$ 对应于 $u=2\\pi k$，且 $\\cos(2\\pi k) = 1 \\ne a$，因此在端点处没有最小值。\n因此，这个陈述是 **正确的**。",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "面对一个存在大量局部最优解的复杂函数，最直观的全局搜索策略之一就是“多起点”方法。本练习  将以著名的 Rastrigin 函数为例，它是一个典型的非凸、多模态测试基准，我们将从第一性原理出发，分析多起点梯度下降法的性能。通过推导单次运行的成功概率以及达到全局最优所需的最少重启次数，我们可以量化地理解该策略的有效性，并揭示其在处理高维问题时面临的“维度灾难”挑战。",
            "id": "3145569",
            "problem": "考虑定义在维度 $n \\in \\mathbb{N}$ 上的拉斯金函数\n$$\nf(\\mathbf{x}) \\;=\\; 10 n \\;+\\; \\sum_{i=1}^{n} \\left(x_{i}^{2} \\;-\\; 10 \\cos\\!\\left(2 \\pi x_{i}\\right)\\right),\n$$\n其定义域为箱型域\n$$\n\\mathcal{D} \\;=\\; [-5.12,\\, 5.12]^{n}.\n$$\n您将分析多起点 (MS) 梯度下降法，该过程定义如下：每次运行通过从 $\\mathcal{D}$ 中独立均匀地抽样来初始化 $\\mathbf{x}_{0}$，然后应用具有足够小的固定步长的梯度下降法，以使迭代收敛到 $f$ 的一个局部极小值点。对于本研究，采纳以下基于对称性和余弦项周期性的科学合理理想化：局部极小值点的吸引盆与限制在 $\\mathcal{D}$ 内的整数格点 $\\mathbb{Z}^{n}$ 的维诺单元重合，因此每个位于整数格点 $\\mathbf{k} \\in \\mathbb{Z}^{n} \\cap \\mathcal{D}$ 的局部极小值点吸引所有以 $\\mathbf{k}$ 为中心、边长为 $1$ 的超立方体内的点，并且全局极小值点位于 $\\mathbf{0}$。\n\n从优化和概率论的基本定义出发，且不调用任何预先推导的快捷公式，完成以下任务：\n- 利用吸引盆的几何描述和在 $\\mathcal{D}$ 上的均匀初始化，推导单次运行中多起点梯度下降法收敛到全局极小值点 $\\mathbf{0}$ 的成功概率 $p(n)$。\n- 利用重启的独立性和在 $M$ 次独立伯努利试验中至少成功一次的概率定义，推导所需的最小重启次数 $M(n,\\delta)$，使得在 $M$ 次运行中至少有一次达到全局极小值点的概率至少为 $1 - \\delta$，其中 $\\delta \\in (0,1)$ 是一个预设的失败概率。\n\n请以一个关于 $n$ 和 $\\delta$ 的闭式解析表达式的形式给出您的最终答案。不要提供不等式或界；请给出您的推导所隐含的精确表达式。无需四舍五入，也不涉及物理单位。",
            "solution": "首先对问题进行严格的验证过程。\n\n### 第 1 步：提取给定条件\n- **函数**：拉斯金函数定义为 $f(\\mathbf{x}) = 10n + \\sum_{i=1}^{n} \\left(x_{i}^{2} - 10 \\cos(2 \\pi x_{i})\\right)$。\n- **维度**：$n \\in \\mathbb{N}$。\n- **定义域**：$\\mathcal{D} = [-5.12, 5.12]^{n}$。\n- **算法**：多起点 (MS) 梯度下降法。\n- **初始化**：对于每次运行，起始点 $\\mathbf{x}_{0}$ 从定义域 $\\mathcal{D}$ 中独立且均匀地抽取。\n- **收敛假设**：梯度下降法被假定收敛到一个局部极小值点。\n- **吸引盆理想化**：位于整数格点 $\\mathbf{k} \\in \\mathbb{Z}^{n} \\cap \\mathcal{D}$ 的局部极小值点的吸引盆是以 $\\mathbf{k}$ 为中心、边长为 $1$ 的超立方体。\n- **全局极小值点**：全局极小值点位于 $\\mathbf{0}$。\n- **目标 1**：推导收敛到全局极小值点的单次运行成功概率 $p(n)$。\n- **目标 2**：推导在至少有一次找到全局极小值点的概率不小于 $1 - \\delta$ 的情况下，所需的最小重启次数 $M(n,\\delta)$，其中 $\\delta \\in (0,1)$。\n- **输出要求**：最终答案必须是 $M(n,\\delta)$ 的一个闭式解析表达式。\n\n### 第 2 步：使用提取的给定条件进行验证\n根据验证标准对问题进行评估。\n\n- **科学依据**：该问题在数值优化领域有充分的依据。拉斯金函数是用于测试优化算法的标准非凸基准函数。梯度下降、吸引盆和多起点策略等概念都是基础性的。关键要素是关于吸引盆的“科学合理理想化”。对于拉斯金函数，局部最小值位于整数点附近。将吸引盆简化为整数格点的维诺单元（即超立方体）是一种常见且适当的理想化，以便于解析处理。它抓住了函数本质的多模态特性，而没有深入研究真实吸引盆的复杂分形边界。将 $\\mathbf{x}=\\mathbf{0}$ 确定为全局极小值点是正确的。该问题在科学上是合理的。\n- **良定性**：问题陈述清晰，提供了所有必要信息。目标具体，并在给定的假设下能够推导出一个唯一的解。\n- **客观性**：语言精确且正式。假设被明确表述为“理想化”，这是客观科学交流的标志。没有主观或模糊的术语。\n\n该问题没有表现出任何列出的缺陷（例如，事实不健全、不完整、矛盾等）。它代表了全局优化算法分析中的一个标准理论练习。\n\n### 第 3 步：结论与行动\n该问题是**有效的**。将提供一个完整的、有理有据的解答。\n\n### 解题推导\n\n根据问题陈述的要求，解答分两部分进行推导。\n\n**第 1 部分：单次运行成功概率 $p(n)$ 的推导**\n\n单次梯度下降运行的成功定义为收敛到位于 $\\mathbf{x} = \\mathbf{0}$ 的全局极小值点。根据问题的理想化假设，这种情况发生当且仅当起始点 $\\mathbf{x}_{0}$ 在全局极小值点的吸引盆内初始化。\n\n令 $\\mathcal{B}_{\\mathbf{0}}$ 表示位于 $\\mathbf{0}$ 的全局极小值点的吸引盆。问题指出，该吸引盆是以 $\\mathbf{0}$ 为中心、边长为 $1$ 的超立方体。用数学语言描述为：\n$$\n\\mathcal{B}_{\\mathbf{0}} = \\left[-\\frac{1}{2}, \\frac{1}{2}\\right]^n\n$$\n起始点 $\\mathbf{x}_{0}$ 从定义域 $\\mathcal{D} = [-5.12, 5.12]^{n}$ 中均匀抽取。对于一个几何区域上的均匀概率分布，一个事件（即从特定子区域中抽取一个点）的概率是该子区域的体积与总样本空间体积之比。\n\n因此，单次运行的成功概率 $p(n)$ 由下式给出：\n$$\np(n) = \\frac{\\text{Vol}(\\mathcal{B}_{\\mathbf{0}})}{\\text{Vol}(\\mathcal{D})}\n$$\n$n$ 维空间中边长为 $L$ 的超立方体的体积是 $L^n$。\n定义域 $\\mathcal{D}$ 的体积是根据边长 $L_{\\mathcal{D}} = 5.12 - (-5.12) = 10.24$ 计算的。\n$$\n\\text{Vol}(\\mathcal{D}) = (10.24)^n\n$$\n吸引盆 $\\mathcal{B}_{\\mathbf{0}}$ 的体积是根据边长 $L_{\\mathcal{B}_{\\mathbf{0}}} = \\frac{1}{2} - \\left(-\\frac{1}{2}\\right) = 1$ 计算的。\n$$\n\\text{Vol}(\\mathcal{B}_{\\mathbf{0}}) = (1)^n = 1\n$$\n注意，$\\mathcal{B}_{\\mathbf{0}}$ 完全包含在 $\\mathcal{D}$ 中，符合要求。\n将这些体积代入概率公式，我们得到单次运行的成功概率：\n$$\np(n) = \\frac{1}{(10.24)^n} = (10.24)^{-n}\n$$\n\n**第 2 部分：最小重启次数 $M(n,\\delta)$ 的推导**\n\n多起点过程包括 $M$ 次独立的运行。每次运行都是一个伯努利试验，有两种结果：\n- 成功（找到全局最小值），概率为 $p(n)$。\n- 失败（收敛到不同的局部最小值），概率为 $1 - p(n)$。\n\n我们要求在 $M$ 次运行中至少有一次找到全局最小值的概率至少为 $1 - \\delta$。令 $S_M$ 为在 $M$ 次试验中至少有一次成功的事件。分析其补事件 $S_M^c$ 更为方便，该事件表示在 $M$ 次试验中没有成功（即所有 $M$ 次试验都失败）。\n$S_M^c$ 的概率是：\n$$\nP(S_M^c) = P(\\text{试验 1 失败 且 试验 2 失败 且 ... 且 试验 M 失败})\n$$\n由于各次试验是独立的，联合事件的概率是各次独立概率的乘积：\n$$\nP(S_M^c) = \\prod_{i=1}^{M} P(\\text{试验 i 失败}) = (1 - p(n))^M\n$$\n那么至少有一次成功的概率是：\n$$\nP(S_M) = 1 - P(S_M^c) = 1 - (1 - p(n))^M\n$$\n问题要求这个概率至少为 $1 - \\delta$：\n$$\n1 - (1 - p(n))^M \\ge 1 - \\delta\n$$\n我们现在解这个关于 $M$ 的不等式。\n$$\n- (1 - p(n))^M \\ge - \\delta\n$$\n两边乘以 $-1$ 会使不等号反向：\n$$\n(1 - p(n))^M \\le \\delta\n$$\n为了分离出指数 $M$，我们对两边取自然对数。由于 $1-p(n)$ 和 $\\delta$ 都在区间 $(0,1)$ 内，它们的对数是良定义且为负数。\n$$\n\\ln\\left((1 - p(n))^M\\right) \\le \\ln(\\delta)\n$$\n$$\nM \\ln(1 - p(n)) \\le \\ln(\\delta)\n$$\n为了解出 $M$，我们两边除以 $\\ln(1 - p(n))$。由于 $p(n) \\in (0,1)$，我们有 $1-p(n) \\in (0,1)$，这意味着 $\\ln(1 - p(n))  0$。除以一个负数会再次反转不等号：\n$$\nM \\ge \\frac{\\ln(\\delta)}{\\ln(1 - p(n))}\n$$\n重启次数 $M$ 必须是整数。满足此不等式的最小整数 $M$ 是右侧表达式的上取整。上取整函数 $\\lceil z \\rceil$ 给出大于或等于 $z$ 的最小整数。\n因此，最小重启次数 $M(n,\\delta)$ 是：\n$$\nM(n,\\delta) = \\left\\lceil \\frac{\\ln(\\delta)}{\\ln(1 - p(n))} \\right\\rceil\n$$\n最后，我们代入第一部分推导出的 $p(n) = (10.24)^{-n}$ 的表达式：\n$$\nM(n,\\delta) = \\left\\lceil \\frac{\\ln(\\delta)}{\\ln(1 - (10.24)^{-n})} \\right\\rceil\n$$\n这就是所需的最小重启次数的闭式解析表达式。",
            "answer": "$$\\boxed{\\left\\lceil \\frac{\\ln(\\delta)}{\\ln(1 - (10.24)^{-n})} \\right\\rceil}$$"
        },
        {
            "introduction": "简单的全局搜索策略（如多起点法）虽然有效，但往往效率不高。一个更智能的进阶策略是设计一种混合算法，它能够根据当前位置的局部信息，动态地在全局探索和局部优化之间切换 。在本练习中，我们将基于二阶泰勒展开的原理，利用目标函数的梯度范数 $\\lVert \\nabla f(x) \\rVert$ 和 Hessian 矩阵的最小特征值 $\\lambda_{\\min}(\\nabla^2 f(x))$ 来建立一个切换准则。我们将亲手实现一个混合优化器，它能在检测到正曲率的“凸”区域时采用牛顿法进行快速收敛，而在遇到负曲率的“非凸”区域时则主动“逃离”，从而高效地寻找全局最优解。",
            "id": "3145524",
            "problem": "考虑一个二次连续可微的目标函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 的无约束最小化问题。局部搜索方法通常依赖于局部曲率信息，而全局搜索则强调探索，以避免陷入非最小值的驻点。从梯度 $\\nabla f(x)$ 和 Hessian 矩阵 $\\nabla^2 f(x)$ 的基本定义出发，提出一个有原则的全局-局部切换准则。该准则使用 Hessian 矩阵的最小特征值（记为 $\\lambda_{\\min}(\\nabla^2 f(x))$）以及梯度范数 $\\lVert \\nabla f(x) \\rVert$。然后，在一个简单的非凸示例中，通过混合全局-局部搜索算法实现此准则。\n\n这个简单的非凸目标函数是二维的，定义如下：\n$$\nf(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + \\tfrac{1}{2} x y .\n$$\n该目标函数有多个驻点，其中包括非最小化点。该混合方法必须使用以下具有科学依据的切换准则：\n- 如果 $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ 且 $\\lambda_{\\min}(\\nabla^2 f(x)) \\ge \\kappa_{\\text{pos}}$（表明一阶项很小且具有足够大的正曲率），则进入局部牛顿类精化模式。\n- 当 $\\lambda_{\\min}(\\nabla^2 f(x))  -\\kappa_{\\text{neg}}$（表明存在负曲率方向）时，或当 $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ 且 $\\lambda_{\\min}(\\nabla^2 f(x)) \\le \\kappa_{\\text{pos}}$（表明在没有足够正曲率的情况下接近驻点，如在鞍点或平坦区域）时，进入全局逃逸模式。\n\n在局部模式下，使用阻尼牛顿步，通过求解 $\\nabla^2 f(x)\\,p = -\\nabla f(x)$ 定义，然后基于 Armijo 下降条件进行回溯。在全局模式下，沿着对应于 $\\lambda_{\\min}(\\nabla^2 f(x))$ 的单位特征向量 $v_{\\min}$ 移动，并使用信赖域半径和仅接受下降的准则。在这些模式之外，使用带有 Armijo 回溯的梯度下降步。使用固定阈值 $g_{\\text{tol}} = 10^{-3}$、$\\kappa_{\\text{pos}} = 10^{-3}$、$\\kappa_{\\text{neg}} = 10^{-8}$，信赖域半径 $r = 5\\times 10^{-1}$，以及每次运行最多 $N = 200$ 次迭代。\n\n您的程序必须在指定的目标函数上实现上述混合方法，并为以下初始点测试集生成结果：\n- $x_0 = (0,0)$,\n- $x_0 = (1,1)$,\n- $x_0 = (0,1)$,\n- $x_0 = (3,-3)$,\n- $x_0 = (0.57735,\\,0.57735)$.\n\n对于每个初始点，运行优化器 $N$ 次迭代，并返回最终目标值 $f(x_N)$ 作为浮点数。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，即 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 是按上述顺序列出的相应测试用例的最终目标值。不涉及物理单位或角度，输出必须是不带附加文本的纯实数。",
            "solution": "该混合准则源自点 $x \\in \\mathbb{R}^n$ 附近的二阶泰勒模型：\n$$\nf(x+p) \\approx f(x) + \\nabla f(x)^\\top p + \\tfrac{1}{2} p^\\top \\nabla^2 f(x)\\, p .\n$$\n基本事实如下。驻点满足 $\\nabla f(x) = 0$。如果 Hessian 矩阵 $\\nabla^2 f(x)$ 还是正定的，那么对于任何非零的 $p$，二次项都严格为正，这表明 $x$ 是一个严格的局部极小点。如果 $\\nabla^2 f(x)$ 至少有一个负特征值，那么就存在一个负曲率方向，沿着该方向二次项严格为负，这意味着 $x$ 不可能是局部极小点，并且可以通过沿该方向移动来实现下降步。因此，使用梯度范数 $\\lVert \\nabla f(x) \\rVert$ 和最小特征值 $\\lambda_{\\min}(\\nabla^2 f(x))$ 来决定采用局部精化还是全局逃逸是有原则的。\n\n现在我们定义切换准则。设 $g_{\\text{tol}}$、$ \\kappa_{\\text{pos}}$ 和 $\\kappa_{\\text{neg}}$ 为很小的正阈值。如果 $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ 且 $\\lambda_{\\min}(\\nabla^2 f(x)) \\ge \\kappa_{\\text{pos}}$，则二阶模型表明 $x$ 位于候选极小点附近的局部凸区域内；这为进入局部牛顿类模式提供了依据。相反，如果 $\\lambda_{\\min}(\\nabla^2 f(x))  -\\kappa_{\\text{neg}}$，或者如果 $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ 且 $\\lambda_{\\min}(\\nabla^2 f(x)) \\le \\kappa_{\\text{pos}}$，则要么存在负曲率方向，要么该点在没有足够正曲率的情况下接近驻点；这为进入由负曲率方向引导的全局逃逸模式提供了依据。在这些情况之外，使用梯度下降步是合理的，因为大梯度在模型中占主导地位，而一个简单的下降方向 $p = -\\nabla f(x)$ 能有效减小一阶项。\n\n局部步基于牛顿法：求解 $\\nabla^2 f(x)\\, p = -\\nabla f(x)$ 得到 $p$，然后应用回溯线搜索以确保充分下降。Armijo 条件要求存在一个步长 $\\alpha$，使得\n$$\nf(x + \\alpha p) \\le f(x) + c_1 \\alpha \\nabla f(x)^\\top p ,\n$$\n其中 $c_1 \\in (0,1)$ 是一个小常数，通常为 $c_1 = 10^{-4}$。这个条件是基于第一性原理的：它保证了下降量至少与预测的线性减少量成比例。如果线性系统是病态的，则回退到使用梯度下降。\n\n全局逃逸步利用负曲率。设 $v_{\\min}$ 是与 $\\lambda_{\\min}(\\nabla^2 f(x))$ 相关联的单位特征向量。对于任意非零标量 $t$，沿 $p = t v_{\\min}$ 方向的二阶项为 $\\tfrac{1}{2} t^2 \\lambda_{\\min}(\\nabla^2 f(x))$，如果 $\\lambda_{\\min}(\\nabla^2 f(x))  0$，该项严格为负。这确保了沿 $\\pm v_{\\min}$ 方向足够小的步长会减小二次模型的值。为了相对于线性项来偏置下降方向，选择方向 $d = -\\operatorname{sign}(\\nabla f(x)^\\top v_{\\min})\\, v_{\\min}$，并在信赖域半径 $r$ 内对步长 $s$ 进行回溯搜索，以保证实际的函数值下降，即，仅当 $f(x_{\\text{new}})  f(x)$ 时才接受 $x_{\\text{new}} = x + s d$。这是一个安全的、与模型和非凸性一致的仅接受下降的准则。\n\n现在我们指定这个简单的示例。目标函数是\n$$\nf(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + \\tfrac{1}{2} x y ,\n$$\n其梯度为\n$$\n\\nabla f(x,y) = \\begin{bmatrix} 4x(x^2 - 1) + \\tfrac{1}{2} y \\\\ 4y(y^2 - 1) + \\tfrac{1}{2} x \\end{bmatrix}\n$$\nHessian 矩阵为\n$$\n\\nabla^2 f(x,y) = \\begin{bmatrix} 12x^2 - 4  \\tfrac{1}{2} \\\\ \\tfrac{1}{2}  12y^2 - 4 \\end{bmatrix} .\n$$\n$\\nabla^2 f(x,y)$ 的特征结构在定义域上是变化的，产生了正曲率和负曲率区域。该混合算法由以下参数确定：$g_{\\text{tol}} = 10^{-3}$、$\\kappa_{\\text{pos}} = 10^{-3}$、$\\kappa_{\\text{neg}} = 10^{-8}$、信赖域半径 $r = 5 \\times 10^{-1}$、Armijo 参数 $c_1 = 10^{-4}$、回溯因子 $\\beta = 1/2$ 以及最大迭代次数 $N = 200$。\n\n算法设计：\n- 计算 $\\nabla f(x)$ 和 $\\nabla^2 f(x)$。\n- 通过对称特征分解计算 $\\lambda_{\\min}(\\nabla^2 f(x))$ 并获得 $v_{\\min}$。\n- 如果 $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ 且 $\\lambda_{\\min}(\\nabla^2 f(x)) \\ge \\kappa_{\\text{pos}}$，则采用带 Armijo 回溯的阻尼牛顿步。\n- 否则，如果 $\\lambda_{\\min}(\\nabla^2 f(x))  -\\kappa_{\\text{neg}}$ 或 $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ 且 $\\lambda_{\\min}(\\nabla^2 f(x)) \\le \\kappa_{\\text{pos}}$，则沿着 $v_{\\min}$ 方向采用带回溯的、仅接受下降的负曲率逃逸步，步长在半径 $r$ 内。\n- 否则，采用带 Armijo 回溯的梯度下降步。\n\n这种有原则的切换是基于二阶泰勒模型和通过 Hessian 矩阵对驻点进行的分类。测试集包含的初始点旨在测试不同的模式：$(0,0)$ 是一个具有负曲率的驻点；$(1,1)$ 位于一个具有正曲率但梯度非零的局部极小点附近；$(0,1)$ 靠近一个具有混合曲率的鞍点状区域；$(3,-3)$ 远离极小点且梯度较大；$(0.57735,\\,0.57735)$ 靠近曲率符号变化的边界。对于每种情况，算法在 $N$ 次迭代后返回最终目标值 $f(x_N)$ 作为实数。程序以指定的单行列表格式 $[r_1,r_2,r_3,r_4,r_5]$ 输出这五个结果，顺序与测试用例一致。",
            "answer": "```python\nimport numpy as np\n\n# Hybrid global–local optimizer implementing curvature- and gradient-based switching\n# for the toy nonconvex function:\n#     f(x,y) = (x**2 - 1)**2 + (y**2 - 1)**2 + 0.5 * x * y\n\ndef f_val(xy: np.ndarray) - float:\n    x, y = xy\n    return (x**2 - 1)**2 + (y**2 - 1)**2 + 0.5 * x * y\n\ndef grad_f(xy: np.ndarray) - np.ndarray:\n    x, y = xy\n    gx = 4.0 * x * (x**2 - 1.0) + 0.5 * y\n    gy = 4.0 * y * (y**2 - 1.0) + 0.5 * x\n    return np.array([gx, gy])\n\ndef hess_f(xy: np.ndarray) - np.ndarray:\n    x, y = xy\n    hxx = 12.0 * x**2 - 4.0\n    hyy = 12.0 * y**2 - 4.0\n    hxy = 0.5\n    return np.array([[hxx, hxy],\n                     [hxy, hyy]])\n\ndef armijo_line_search(x: np.ndarray, f_x: float, g_x: np.ndarray, p: np.ndarray,\n                       c1: float = 1e-4, alpha0: float = 1.0, beta: float = 0.5,\n                       max_backtracks: int = 50):\n    \"\"\"Armijo backtracking line search. Returns (x_new, f_new, alpha).\"\"\"\n    alpha = alpha0\n    gTp = float(np.dot(g_x, p))\n    for _ in range(max_backtracks):\n        x_new = x + alpha * p\n        f_new = f_val(x_new)\n        if f_new = f_x + c1 * alpha * gTp:\n            return x_new, f_new, alpha\n        alpha *= beta\n    # If no acceptable step found, return original point\n    return x, f_x, 0.0\n\ndef neg_curvature_escape(x: np.ndarray, f_x: float, g_x: np.ndarray, H_x: np.ndarray,\n                         tr_radius: float = 0.5, beta: float = 0.5, max_backtracks: int = 50):\n    \"\"\"Step along the most negative curvature direction within a trust region radius,\n    accepting only if f decreases. Returns (x_new, f_new, step_len).\"\"\"\n    # Symmetric eigendecomposition\n    evals, evecs = np.linalg.eigh(H_x)\n    idx_min = int(np.argmin(evals))\n    vmin = evecs[:, idx_min]\n    # Bias direction by linear term to favor decrease\n    sign = -1.0 if float(np.dot(g_x, vmin)) = 0.0 else 1.0\n    d = sign * vmin  # unit direction (since evecs are normalized)\n    s = tr_radius\n    for _ in range(max_backtracks):\n        x_new = x + s * d\n        f_new = f_val(x_new)\n        if f_new  f_x:\n            return x_new, f_new, s\n        s *= beta\n    return x, f_x, 0.0\n\ndef hybrid_optimize(x0: np.ndarray,\n                    max_iters: int = 200,\n                    g_tol: float = 1e-3,\n                    kappa_pos: float = 1e-3,\n                    kappa_neg: float = 1e-8,\n                    tr_radius: float = 0.5,\n                    c1: float = 1e-4,\n                    beta: float = 0.5) - float:\n    \"\"\"Run the hybrid global–local optimizer from x0 and return final objective value.\"\"\"\n    x = x0.copy().astype(float)\n    for _ in range(max_iters):\n        f_x = f_val(x)\n        g_x = grad_f(x)\n        H_x = hess_f(x)\n        grad_norm = float(np.linalg.norm(g_x))\n        # Compute smallest eigenvalue of Hessian\n        evals = np.linalg.eigvalsh(H_x)\n        lam_min = float(evals[0])\n\n        # Decide regime and take a step\n        if grad_norm = g_tol and lam_min = kappa_pos:\n            # Local Newton-type refinement\n            try:\n                # Newton direction\n                p = np.linalg.solve(H_x, -g_x)\n            except np.linalg.LinAlgError:\n                # Fallback to gradient descent if Hessian is singular\n                p = -g_x\n            # Armijo backtracking\n            x_new, f_new, alpha = armijo_line_search(x, f_x, g_x, p, c1=c1, alpha0=1.0, beta=beta)\n            if alpha == 0.0:\n                # If no progress, break early\n                break\n            x = x_new\n        elif (lam_min  -kappa_neg) or (grad_norm = g_tol and lam_min = kappa_pos):\n            # Global escape via negative curvature direction\n            x_new, f_new, s = neg_curvature_escape(x, f_x, g_x, H_x, tr_radius=tr_radius, beta=beta)\n            if s == 0.0:\n                # If escape step fails to decrease, fallback to gradient descent\n                p = -g_x\n                x_new, f_new, alpha = armijo_line_search(x, f_x, g_x, p, c1=c1, alpha0=1.0, beta=beta)\n                if alpha == 0.0:\n                    break\n            x = x_new\n        else:\n            # Default local descent\n            p = -g_x\n            x_new, f_new, alpha = armijo_line_search(x, f_x, g_x, p, c1=c1, alpha0=1.0, beta=beta)\n            if alpha == 0.0:\n                break\n            x = x_new\n    return f_val(x)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.0, 0.0]),\n        np.array([1.0, 1.0]),\n        np.array([0.0, 1.0]),\n        np.array([3.0, -3.0]),\n        np.array([0.57735, 0.57735]),\n    ]\n\n    results = []\n    for x0 in test_cases:\n        final_f = hybrid_optimize(x0,\n                                  max_iters=200,\n                                  g_tol=1e-3,\n                                  kappa_pos=1e-3,\n                                  kappa_neg=1e-8,\n                                  tr_radius=0.5,\n                                  c1=1e-4,\n                                  beta=0.5)\n        results.append(final_f)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}