{
    "hands_on_practices": [
        {
            "introduction": "多重起始（Multi-Start）法是全局优化中最直观的策略之一。其基本思想是通过从多个随机选择的初始点运行局部搜索，从而增加找到全局最优解的概率。这个练习将指导你分析一个经典的多重起始梯度下降算法，并将其应用于著名的Rastrigin函数 。通过从基本定义出发进行推导，你将亲身体会到搜索空间的维度如何影响找到全局最优解所需的尝试次数，从而对“维度灾难”有更深刻的理解。",
            "id": "3145569",
            "problem": "考虑一个维度为 $n \\in \\mathbb{N}$ 的 Rastrigin 函数，其定义为\n$$\nf(\\mathbf{x}) \\;=\\; 10 n \\;+\\; \\sum_{i=1}^{n} \\left(x_{i}^{2} \\;-\\; 10 \\cos\\!\\left(2 \\pi x_{i}\\right)\\right),\n$$\n在盒式定义域\n$$\n\\mathcal{D} \\;=\\; [-5.12,\\, 5.12]^{n}\n$$\n上。您将分析多起点（MS）梯度下降法，其定义为以下过程：每次运行通过从 $\\mathcal{D}$ 中独立均匀地抽取样本来初始化 $\\mathbf{x}_{0}$，然后应用具有足够小的固定步长的梯度下降法，以使迭代收敛到 $f$ 的一个局部最小值点。在本研究中，采用以下基于对称性和余弦项周期性的科学合理理想化假设：局部最小值点的吸引盆与限制在 $\\mathcal{D}$ 内的整数格点 $\\mathbb{Z}^{n}$ 的 Voronoi 单元重合，因此，位于整数格点 $\\mathbf{k} \\in \\mathbb{Z}^{n} \\cap \\mathcal{D}$ 上的每个局部最小值点吸引以 $\\mathbf{k}$ 为中心、边长为 $1$ 的超立方体内的所有点，并且全局最小值点位于 $\\mathbf{0}$。\n\n从优化和概率论的基本定义出发，并且不使用任何预先推导的捷径公式，完成以下任务：\n- 使用吸引盆的几何描述和在 $\\mathcal{D}$ 上的均匀初始化，推导单次运行中多起点梯度下降法收敛到全局最小值点 $\\mathbf{0}$ 的成功概率 $p(n)$。\n- 利用重启的独立性以及在 $M$ 次独立伯努利试验中至少成功一次的概率定义，推导所需的最小重启次数 $M(n,\\delta)$，以使在 $M$ 次运行中至少有一次达到全局最小值点的概率不小于 $1 - \\delta$，其中 $\\delta \\in (0,1)$ 是一个预设的失败概率。\n\n请以一个关于 $n$ 和 $\\delta$ 的闭式解析表达式的形式给出 $M(n,\\delta)$ 的最终答案。不要提供不等式或界限；请提供您的推导所蕴含的精确表达式。无需四舍五入，也不涉及物理单位。",
            "solution": "首先对问题进行严格的验证过程。\n\n### 第 1 步：提取已知条件\n- **函数**：Rastrigin 函数定义为 $f(\\mathbf{x}) = 10n + \\sum_{i=1}^{n} \\left(x_{i}^{2} - 10 \\cos(2 \\pi x_{i})\\right)$。\n- **维度**：$n \\in \\mathbb{N}$。\n- **定义域**：$\\mathcal{D} = [-5.12, 5.12]^{n}$。\n- **算法**：多起点（MS）梯度下降法。\n- **初始化**：对于每次运行，起始点 $\\mathbf{x}_{0}$ 从定义域 $\\mathcal{D}$ 中独立均匀地抽取。\n- **收敛假设**：假设梯度下降法收敛到一个局部最小值点。\n- **吸引盆理想化**：位于整数格点 $\\mathbf{k} \\in \\mathbb{Z}^{n} \\cap \\mathcal{D}$ 上的局部最小值点的吸引盆是以 $\\mathbf{k}$ 为中心、边长为 1 的超立方体。\n- **全局最小值点**：全局最小值点位于 $\\mathbf{0}$。\n- **目标 1**：推导收敛到全局最小值点的单次运行成功概率 $p(n)$。\n- **目标 2**：推导在至少找到一次全局最小值点的概率不小于 $1 - \\delta$ 的情况下所需的最小重启次数 $M(n,\\delta)$，其中 $\\delta \\in (0,1)$。\n- **输出要求**：最终答案必须是 $M(n,\\delta)$ 的单个闭式解析表达式。\n\n### 第 2 步：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n\n- **科学依据**：该问题在数值优化领域有充分的依据。Rastrigin 函数是用于测试优化算法的标准非凸基准函数。梯度下降、吸引盆和多起点策略等概念都是基础性的。关键要素是关于吸引盆的“科学合理理想化”假设。对于 Rastrigin 函数，局部最小值点位于整数点附近。将吸引盆简化为整数格点的 Voronoi 单元（即超立方体）是一种常见且适用于分析处理的理想化方法。它抓住了函数本质上的多模态特性，而没有深入研究真实吸引盆的复杂分形边界。将 $\\mathbf{x}=\\mathbf{0}$ 确定为全局最小值点是正确的。该问题在科学上是合理的。\n- **适定性**：问题陈述清晰，提供了所有必要信息。目标明确，在给定假设下可以推导出唯一的解。\n- **客观性**：语言精确、正式。假设被明确表述为“理想化”，这是客观科学交流的标志。没有主观或含糊不清的术语。\n\n该问题没有表现出任何列出的缺陷（例如，事实不健全、不完整、矛盾等）。它代表了全局优化算法分析中的一个标准理论练习。\n\n### 第 3 步：结论与行动\n该问题是**有效的**。将提供一个完整的、有理有据的解答。\n\n### 解题推导\n\n根据问题陈述的要求，解题过程分为两部分。\n\n**第 1 部分：推导单次运行成功概率 $p(n)$**\n\n单次梯度下降运行的成功定义为收敛到位于 $\\mathbf{x} = \\mathbf{0}$ 的全局最小值点。根据问题的理想化假设，这种情况发生当且仅当起始点 $\\mathbf{x}_{0}$ 在全局最小值点的吸引盆内初始化。\n\n设 $\\mathcal{B}_{\\mathbf{0}}$ 表示位于 $\\mathbf{0}$ 的全局最小值点的吸引盆。问题指出，该吸引盆是一个以 $\\mathbf{0}$ 为中心、边长为 $1$ 的超立方体。用数学语言描述为：\n$$\n\\mathcal{B}_{\\mathbf{0}} = \\left[-\\frac{1}{2}, \\frac{1}{2}\\right]^n\n$$\n起始点 $\\mathbf{x}_{0}$ 是从定义域 $\\mathcal{D} = [-5.12, 5.12]^{n}$ 中均匀抽取的。对于一个几何区域上的均匀概率分布，某个事件（即从特定子区域中抽取一个点）的概率是该子区域的体积与总样本空间体积之比。\n\n因此，单次运行的成功概率 $p(n)$ 由下式给出：\n$$\np(n) = \\frac{\\text{Vol}(\\mathcal{B}_{\\mathbf{0}})}{\\text{Vol}(\\mathcal{D})}\n$$\n$n$ 维空间中边长为 $L$ 的超立方体的体积是 $L^n$。\n定义域 $\\mathcal{D}$ 的体积是根据边长 $L_{\\mathcal{D}} = 5.12 - (-5.12) = 10.24$ 计算的。\n$$\n\\text{Vol}(\\mathcal{D}) = (10.24)^n\n$$\n吸引盆 $\\mathcal{B}_{\\mathbf{0}}$ 的体积是根据边长 $L_{\\mathcal{B}_{\\mathbf{0}}} = \\frac{1}{2} - \\left(-\\frac{1}{2}\\right) = 1$ 计算的。\n$$\n\\text{Vol}(\\mathcal{B}_{\\mathbf{0}}) = (1)^n = 1\n$$\n注意，根据要求，$\\mathcal{B}_{\\mathbf{0}}$ 完全包含在 $\\mathcal{D}$ 内。\n将这些体积代入概率公式，我们得到单次运行的成功概率：\n$$\np(n) = \\frac{1}{(10.24)^n} = (10.24)^{-n}\n$$\n\n**第 2 部分：推导最小重启次数 $M(n,\\delta)$**\n\n多起点过程包括 $M$ 次独立的运行。每次运行都是一个有两种结果的伯努利试验：\n- 成功（找到全局最小值），概率为 $p(n)$。\n- 失败（收敛到其他局部最小值），概率为 $1 - p(n)$。\n\n我们要求在 $M$ 次运行中至少找到一次全局最小值的概率不小于 $1 - \\delta$。设 $S_M$ 为在 $M$ 次试验中至少有一次成功的事件。分析其补事件 $S_M^c$ 会更方便，即在 $M$ 次试验中没有一次成功（即所有 $M$ 次试验都失败）的事件。\n$S_M^c$ 的概率是：\n$$\nP(S_M^c) = P(\\text{试验 1 失败且试验 2 失败且 ... 且试验 } M \\text{ 失败})\n$$\n由于各次试验是独立的，联合事件的概率是各次独立事件概率的乘积：\n$$\nP(S_M^c) = \\prod_{i=1}^{M} P(\\text{试验 } i \\text{ 失败}) = (1 - p(n))^M\n$$\n那么，至少有一次成功的概率是：\n$$\nP(S_M) = 1 - P(S_M^c) = 1 - (1 - p(n))^M\n$$\n问题要求这个概率至少为 $1 - \\delta$：\n$$\n1 - (1 - p(n))^M \\ge 1 - \\delta\n$$\n我们现在对这个不等式求解 $M$。\n$$\n- (1 - p(n))^M \\ge - \\delta\n$$\n两边乘以 $-1$ 会使不等号反向：\n$$\n(1 - p(n))^M \\le \\delta\n$$\n为了分离出指数 $M$，我们对两边取自然对数。由于 $1-p(n)$ 和 $\\delta$ 都在区间 $(0,1)$ 内，它们的对数都是有定义的且为负数。\n$$\n\\ln\\left((1 - p(n))^M\\right) \\le \\ln(\\delta)\n$$\n$$\nM \\ln(1 - p(n)) \\le \\ln(\\delta)\n$$\n为了解出 $M$，我们两边除以 $\\ln(1 - p(n))$。由于 $p(n) \\in (0,1)$，我们有 $1-p(n) \\in (0,1)$，这意味着 $\\ln(1 - p(n))  0$。除以一个负数会再次使不等号反向：\n$$\nM \\ge \\frac{\\ln(\\delta)}{\\ln(1 - p(n))}\n$$\n重启次数 $M$ 必须是整数。满足此不等式的最小整数 $M$ 是右侧表达式的向上取整。向上取整函数 $\\lceil z \\rceil$ 给出不小于 $z$ 的最小整数。\n因此，最小重启次数 $M(n,\\delta)$ 是：\n$$\nM(n,\\delta) = \\left\\lceil \\frac{\\ln(\\delta)}{\\ln(1 - p(n))} \\right\\rceil\n$$\n最后，我们代入第一部分推导出的 $p(n) = (10.24)^{-n}$ 的表达式：\n$$\nM(n,\\delta) = \\left\\lceil \\frac{\\ln(\\delta)}{\\ln(1 - (10.24)^{-n})} \\right\\rceil\n$$\n这就是所需的最小重启次数的闭式解析表达式。",
            "answer": "$$\\boxed{\\left\\lceil \\frac{\\ln(\\delta)}{\\ln(1 - (10.24)^{-n})} \\right\\rceil}$$"
        },
        {
            "introduction": "纯粹的全局搜索（如多重起始）可能效率低下，而纯粹的局部搜索容易陷入次优解。一个更强大的方法是将两者结合，创建一种混合算法。本练习将引导你设计并实现一个智能的混合搜索算法，该算法在大部分时间里使用高效的梯度下降法进行“利用”（exploitation），但在检测到搜索停滞在平坦区域（plateau）时，会自动切换到随机游走模式进行“探索”（exploration） 。通过这个实践，你将学会如何利用局部信息（如梯度范数）来动态地平衡局部优化和全局探索。",
            "id": "3145499",
            "problem": "考虑一个双变量连续可微目标函数的无约束最小化问题。目标是设计并实现一种混合确定性-随机搜索算法，该算法结合了基于梯度的局部下降与全局随机探索，其中低梯度区域会触发随机游走。程序必须是自包含的，无需用户输入即可执行，并以指定格式产生单行输出。\n\n基本原理：使用多元微积分中梯度的定义、链式法则以及最速下降遵循负梯度方向的原理。该搜索应具有全局性，即能够逃离确定性方法因梯度过小而可能停滞的区域。\n\n在定义域 $\\mathcal{D} = [-5,5]\\times[-5,5]$ 上定义目标函数为\n$$\nf(x,y) \\;=\\; \\sigma\\!\\left(\\sqrt{x^2+y^2};R,k\\right) \\;+\\; c\\,(x^2+y^2) \\;-\\; A_1\\,\\exp\\!\\left(-\\frac{(x-x^\\star)^2+(y-y^\\star)^2}{2s_1^2}\\right) \\;-\\; A_2\\,\\exp\\!\\left(-\\frac{(x-x_2)^2+(y-y_2)^2}{2s_2^2}\\right),\n$$\n其中\n- $\\sigma(r;R,k) = \\dfrac{1}{1+\\exp\\!\\left(-k\\,(r-R)\\right)}$ 是一个 logistic 函数，它通过在 $r\\ll R$ 和 $r\\gg R$ 时饱和到导数很小的值，从而制造出欺骗性的平台区，\n- $(x^\\star,y^\\star) = (1,-1)$ 是全局盆地的位置，\n- $(x_2,y_2) = (-2,2)$ 是一个浅层局部盆地的位置，\n- $R=3$, $k=5$, $c=0.001$, $A_1=1$, $s_1=0.25$, $A_2=0.2$, $s_2=0.5$。\n\n基于梯度的局部搜索必须使用梯度 $\\nabla f(x,y)$ 和回溯步长规则来接受或拒绝确定性更新。全局搜索部分必须是随机游走，当局部梯度大小很小且近期改进可忽略不计时触发。随机游走的提议点必须各向同性地抽取，步长从一个连续分布中抽取，并且必须被裁剪到定义域 $\\mathcal{D}$ 内。该混合算法必须：\n- 解析地计算 $\\nabla f(x,y)$，\n- 当目标没有改善时，执行一个确定性步骤 $z_{t+1} = z_t - \\alpha\\,\\nabla f(z_t)$，并对 $\\alpha$ 进行回溯，\n- 使用梯度大小阈值和近期迭代的最小改进阈值来检测平台区，\n- 触发随机游走，采样 $N$ 个候选步，并在任何候选步的改进超过最小阈值时，接受最佳改进候选步，\n- 当重复的随机游走未能改善时，向上调整随机游走步长尺度，该尺度有最大值上限，并在成功改善后重置该尺度，\n- 将所有提议点裁剪到 $\\mathcal{D}$ 内。\n\n在您的实现中使用以下参数值：\n- 确定性步长的初始值 $\\alpha = 0.5$，回溯因子 $\\beta = 0.5$，每次迭代最多尝试 3 次回溯，\n- 梯度大小平台区阈值 $\\tau_g = 0.02$，\n- 每次迭代的最小改进阈值 $\\tau_f = 10^{-6}$，\n- 平台区触发计数器 $L = 5$ 次连续的低梯度和低改进迭代，\n- 每次触发的随机游走提议点数量 $N = 50$，\n- 初始随机游走步长尺度 $s_{\\mathrm{rw}} = 0.5$，乘法增长因子 $\\gamma_{\\mathrm{rw}}=1.2$，最大步长尺度 $s_{\\mathrm{rw},\\max} = 3.0$，\n- 最大外部迭代次数 $T_{\\max} = 500$。\n\n您的程序必须计算在 $(x^\\star,y^\\star)$ 处的 $f$ 值，以定义参考最优值 $f^\\star = f(x^\\star,y^\\star)$。对于每个测试用例，如果最终优化器状态 $(x_T,y_T)$ 满足 $\\sqrt{(x_T-x^\\star)^2 + (y_T-y^\\star)^2} \\leq \\delta$（容差 $\\delta = 0.2$），程序必须返回一个指示成功的布尔值。\n\n实现所需的解析表达式：\n1. 对于 $r=\\sqrt{x^2+y^2}$，$\\sigma(r;R,k)=\\dfrac{1}{1+\\exp\\!\\left(-k\\,(r-R)\\right)}$，其关于 $r$ 的导数为：\n$$\n\\frac{d\\sigma}{dr} = k\\,\\sigma(r;R,k)\\,\\bigl(1-\\sigma(r;R,k)\\bigr).\n$$\n根据链式法则，对于 $r0$，\n$$\n\\nabla_{x,y}\\,\\sigma(r;R,k) = \\frac{d\\sigma}{dr}\\,\\frac{1}{r}\\,\\begin{bmatrix}x\\\\y\\end{bmatrix},\n$$\n并且对于 $r=0$，定义 $\\nabla_{x,y}\\,\\sigma(0;R,k)=\\begin{bmatrix}0\\\\0\\end{bmatrix}$。\n2. 对于二次项，$\\nabla_{x,y}\\,\\bigl(c\\,(x^2+y^2)\\bigr) = \\begin{bmatrix}2c\\,x\\\\2c\\,y\\end{bmatrix}$。\n3. 对于高斯势阱 $g(x,y) = -A\\,\\exp\\!\\left(-\\dfrac{(x-\\mu_x)^2+(y-\\mu_y)^2}{2s^2}\\right)$，其梯度为\n$$\n\\nabla g(x,y) = A\\,\\exp\\!\\left(-\\frac{(x-\\mu_x)^2+(y-\\mu_y)^2}{2s^2}\\right)\\,\\frac{1}{s^2}\\,\\begin{bmatrix}x-\\mu_x\\\\y-\\mu_y\\end{bmatrix}.\n$$\n将此应用于两个高斯项，并使用指定的参数。\n\n测试套件：\n对以下测试用例实现混合算法，每个用例包含一个初始点和用于随机数生成器（RNG）的种子：\n- 用例 1：初始点 $(x_0,y_0) = (4.5,-4.5)$，种子 $42$，\n- 用例 2：初始点 $(x_0,y_0) = (-2.2,2.2)$，种子 $123$，\n- 用例 3：初始点 $(x_0,y_0) = (1.5,-0.5)$，种子 $7$，\n- 用例 4：初始点 $(x_0,y_0) = (0,0)$，种子 $2024$，\n- 用例 5：初始点 $(x_0,y_0) = (5.0,5.0)$，种子 $99$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的布尔值列表（例如，$\\texttt{[True,False,True,True,False]}$）。不得打印任何额外文本。",
            "solution": "用户提供的问题被评估为有效。这是一个在数值优化领域内适定且有科学依据的问题，为获得唯一且可验证的解提供了所有必要的数据、定义和约束。\n\n该问题要求设计并实现一个混合优化算法，以找到指定二维函数 $f(x,y)$ 的最小值。该算法结合了确定性局部搜索方法（带回溯的梯度下降）和随机全局搜索方法（随机游走）。这两种模式之间的转换由一个平台区检测机制控制。\n\n目标函数在定义域 $\\mathcal{D} = [-5,5]\\times[-5,5]$ 上定义为：\n$$\nf(x,y) \\;=\\; \\sigma\\!\\left(\\sqrt{x^2+y^2};R,k\\right) \\;+\\; c\\,(x^2+y^2) \\;-\\; A_1\\,\\exp\\!\\left(-\\frac{(x-x^\\star)^2+(y-y^\\star)^2}{2s_1^2}\\right) \\;-\\; A_2\\,\\exp\\!\\left(-\\frac{(x-x_2)^2+(y-y_2)^2}{2s_2^2}\\right)\n$$\n该函数由四项组成：\n1. 一个 logistic (sigmoid) 项 $\\sigma(r;R,k) = \\frac{1}{1+\\exp(-k(r-R))}$，其中 $r = \\sqrt{x^2+y^2}$。该项为径向距离 $r \\ll R$ 和 $r \\gg R$ 的情况引入了大的平坦区域（平台区），旨在对优化算法构成挑战。\n2. 一个浅的二次碗型项 $c\\,(x^2+y^2)$，它将解温和地推向原点。\n3. 一个以 $(x^\\star, y^\\star) = (1,-1)$ 为中心的深高斯势阱，它定义了全局最小值。\n4. 一个以 $(x_2, y_2) = (-2,2)$ 为中心的较浅高斯势阱，它作为一个局部最小值（一个欺骗性陷阱）。\n\n函数的参数给定为 $R=3$, $k=5$, $c=0.001$, $A_1=1$, $s_1=0.25$, $A_2=0.2$ 和 $s_2=0.5$。\n\n为了实现该算法，我们首先需要目标函数的解析梯度 $\\nabla f(x,y)$。该梯度是其四个组成项的梯度之和，即 $\\nabla f = \\nabla f_1 + \\nabla f_2 + \\nabla f_3 + \\nabla f_4$。\n\n1. sigmoid 项的梯度，$\\nabla f_1$：\n使用链式法则，$\\nabla_{x,y}\\,\\sigma(r) = \\frac{d\\sigma}{dr}\\nabla_{x,y}(r)$。对于 $r = \\sqrt{x^2+y^2}  0$，我们有 $\\nabla_{x,y}(r) = \\frac{1}{r}[x, y]^T$。导数 $\\frac{d\\sigma}{dr}$ 给出为 $k\\,\\sigma(r)(1-\\sigma(r))$。一个数值稳定的形式是 $\\frac{d\\sigma}{dr} = k\\,\\frac{\\exp(-k(r-R))}{(1+\\exp(-k(r-R)))^2}$。综合这些，对于 $r0$：\n$$\n\\nabla f_1(x,y) = k\\,\\sigma(r)(1-\\sigma(r))\\,\\frac{1}{r}\\begin{bmatrix}x\\\\y\\end{bmatrix}\n$$\n对于 $r=0$，梯度定义为 $\\begin{bmatrix}0\\\\0\\end{bmatrix}$。\n\n2. 二次项的梯度，$\\nabla f_2$：\n这是一个标准结果：\n$$\n\\nabla f_2(x,y) = \\nabla \\left(c\\,(x^2+y^2)\\right) = \\begin{bmatrix}2cx\\\\2cy\\end{bmatrix}\n$$\n\n3. 和 4. 高斯势阱的梯度，$\\nabla f_3$ 和 $\\nabla f_4$：\n使用为 $g(x,y) = -A\\,\\exp\\!\\left(-\\frac{(x-\\mu_x)^2+(y-\\mu_y)^2}{2s^2}\\right)$ 提供的公式，其梯度是：\n$$\n\\nabla g(x,y) = \\frac{A}{s^2}\\,\\exp\\!\\left(-\\frac{(x-\\mu_x)^2+(y-\\mu_y)^2}{2s^2}\\right)\\begin{bmatrix}x-\\mu_x\\\\y-\\mu_y\\end{bmatrix}\n$$\n我们将此公式应用于第三项（使用参数 $A_1, s_1, (\\mu_x, \\mu_y) = (x^\\star, y^\\star)$）和第四项（使用 $A_2, s_2, (\\mu_x, \\mu_y) = (x_2, y_2)$）。\n\n混合算法迭代进行，最多 $T_{\\max} = 500$ 次。在每次迭代 $t$ 中，状态由当前位置 $z_t = (x_t, y_t)$、随机游走尺度 $s_{\\mathrm{rw}}$ 以及一个用于记录连续类平台区迭代的计数器定义。\n\n一次迭代的逻辑如下：\n首先，我们判断算法是否陷入平台区。如果在连续 $L=5$ 次迭代中，梯度大小一直低于阈值 $\\tau_g = 0.02$ 并且目标函数的改进量一直低于阈值 $\\tau_f = 10^{-6}$，则触发平台区状态。\n\n如果检测到平台区（`plateau_counter = L`）：\n执行一次随机游走。通过向当前位置 $z_t$ 添加随机位移向量来生成 $N=50$ 个候选点。这些向量从半径为 $s_{\\mathrm{rw}}$ 的圆盘中各向同性地采样。每个候选点都被裁剪到定义域 $\\mathcal{D}$ 内。对所有候选点计算目标函数值。如果最佳候选点比当前函数值提供了至少 $\\tau_f$ 的改进，算法将移动到该点，并且随机游走尺度 $s_{\\mathrm{rw}}$ 被重置为其初始值 $0.5$。如果没有候选点提供足够的改进，位置保持不变，但 $s_{\\mathrm{rw}}$ 会乘以一个因子 $\\gamma_{\\mathrm{rw}}=1.2$，最高不超过最大值 $s_{\\mathrm{rw}, \\max}=3.0$。当搜索持续停滞时，这种自适应缩放允许搜索探索更大的区域。在随机游走尝试之后，平台区计数器被重置。\n\n如果没有检测到平台区：\n执行一个标准的梯度下降步。提议的下一个位置为 $z_{t+1} = z_t - \\alpha \\nabla f(z_t)$，其中初始步长为 $\\alpha=0.5$。使用回溯线搜索来确保改进。提议的点被裁剪到 $\\mathcal{D}$ 内。如果 $f(z_{t+1})  f(z_t)$，则接受该步骤。否则，$\\alpha$ 乘以一个因子 $\\beta=0.5$ 以减小步长，并提议一个新的点。这个过程最多重复 $3$ 次。如果在所有回溯尝试后仍未找到改进，则在此次迭代中不更新位置。\n\n每次迭代（无论是随机游走还是梯度下降）之后，算法会为下一次迭代更新平台区计数器。如果在迭代开始时梯度大小低于 $\\tau_g$ 且最终的改进量低于 $\\tau_f$，则计数器递增；否则，它被重置为 $0$。\n\n该过程从一个指定的初始点 $(x_0, y_0)$ 开始，并重复 $T_{\\max}$ 次迭代。然后将最终位置 $(x_T, y_T)$ 与真实全局最小值的位置 $(x^\\star, y^\\star)$ 进行比较。如果欧几里得距离 $\\sqrt{(x_T-x^\\star)^2 + (y_T-y^\\star)^2}$ 小于或等于容差 $\\delta=0.2$，则宣告成功。\n\n该实现将对 5 个测试用例中的每一个执行此逻辑，使用指定的初始点和随机数生成器种子以确保可复现性。最终输出是一个布尔值列表，指示每个用例的成功或失败。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run the hybrid optimization algorithm on all test cases.\n    \"\"\"\n    \n    # --- Function and Algorithm Parameters ---\n    # Objective function parameters\n    R, k, c = 3.0, 5.0, 0.001\n    A1, s1 = 1.0, 0.25\n    x_star, y_star = 1.0, -1.0\n    A2, s2 = 0.2, 0.5\n    x2, y2 = -2.0, 2.0\n    \n    # Hybrid algorithm parameters\n    alpha_initial = 0.5\n    beta = 0.5\n    max_backtrack = 3\n    tau_g = 0.02\n    tau_f = 1e-6\n    L = 5\n    N = 50\n    s_rw_initial = 0.5\n    gamma_rw = 1.2\n    s_rw_max = 3.0\n    T_max = 500\n    \n    # Success criterion\n    delta = 0.2\n    \n    # Domain boundaries\n    domain_min, domain_max = -5.0, 5.0\n    x_star_vec = np.array([x_star, y_star])\n\n    def objective_function(pos):\n        \"\"\"Computes the value of the objective function f(x, y).\"\"\"\n        x, y = pos\n        r = np.sqrt(x**2 + y**2)\n        \n        # Term 1: Sigmoid\n        # Use np.exp for safe exponentiation\n        # Note: -k*(r-R) can be large, leading to exp -> 0.\n        # It can also be very negative, leading to exp -> inf.\n        # np.exp handles large arguments by overflowing to inf, which is fine here.\n        sigma_val = 1.0 / (1.0 + np.exp(-k * (r - R)))\n        \n        # Term 2: Quadratic\n        quad_val = c * (x**2 + y**2)\n        \n        # Term 3: Global minimum Gaussian well\n        gauss1_val = -A1 * np.exp(-((x - x_star)**2 + (y - y_star)**2) / (2 * s1**2))\n        \n        # Term 4: Local minimum Gaussian well\n        gauss2_val = -A2 * np.exp(-((x - x2)**2 + (y - y2)**2) / (2 * s2**2))\n        \n        return sigma_val + quad_val + gauss1_val + gauss2_val\n\n    def gradient_function(pos):\n        \"\"\"Computes the analytical gradient of the objective function.\"\"\"\n        x, y = pos\n        grad = np.zeros(2)\n        r = np.sqrt(x**2 + y**2)\n        \n        # Grad Term 1: Sigmoid\n        if r > 1e-9: # Avoid division by zero\n            exp_term = np.exp(-k * (r - R))\n            sigma_val = 1.0 / (1.0 + exp_term)\n            d_sigma_dr = k * sigma_val * (1.0 - sigma_val)\n            grad_sigma = d_sigma_dr * (1.0 / r) * np.array([x, y])\n            grad += grad_sigma\n            \n        # Grad Term 2: Quadratic\n        grad_quad = 2.0 * c * np.array([x, y])\n        grad += grad_quad\n        \n        # Grad Term 3: Global minimum Gaussian\n        exp_gauss1 = np.exp(-((x - x_star)**2 + (y - y_star)**2) / (2 * s1**2))\n        grad_gauss1 = (A1 / s1**2) * exp_gauss1 * np.array([x - x_star, y - y_star])\n        grad += grad_gauss1\n\n        # Grad Term 4: Local minimum Gaussian\n        exp_gauss2 = np.exp(-((x - x2)**2 + (y - y2)**2) / (2 * s2**2))\n        grad_gauss2 = (A2 / s2**2) * exp_gauss2 * np.array([x - x2, y - y2])\n        grad += grad_gauss2\n\n        return grad\n\n    def run_one_case(initial_pos, seed):\n        \"\"\"Runs the hybrid optimization for a single test case.\"\"\"\n        rng = np.random.default_rng(seed)\n        pos = np.array(initial_pos, dtype=float)\n        \n        plateau_counter = 0\n        s_rw = s_rw_initial\n\n        for _ in range(T_max):\n            f_current = objective_function(pos)\n            grad = gradient_function(pos)\n            grad_norm = np.linalg.norm(grad)\n\n            # --- Mode Selection ---\n            if plateau_counter >= L:\n                # Random Walk Mode\n                thetas = rng.uniform(0, 2 * np.pi, N)\n                # Sample radius for uniform distribution in a disk\n                radii = s_rw * np.sqrt(rng.uniform(0, 1, N))\n                \n                proposals = pos + np.column_stack([radii * np.cos(thetas), radii * np.sin(thetas)])\n                proposals = np.clip(proposals, domain_min, domain_max)\n                \n                f_proposals = np.array([objective_function(p) for p in proposals])\n                \n                best_idx = np.argmin(f_proposals)\n                f_best_proposal = f_proposals[best_idx]\n                \n                if f_best_proposal  f_current - tau_f:\n                    pos_next = proposals[best_idx]\n                    s_rw = s_rw_initial  # Reset scale on success\n                else:\n                    pos_next = pos\n                    s_rw = min(s_rw * gamma_rw, s_rw_max) # Increase scale on failure\n                \n                plateau_counter = 0 # RW attempt was made, reset counter\n            else:\n                # Gradient Descent Mode\n                alpha_current = alpha_initial\n                step_accepted = False\n                for _ in range(max_backtrack + 1):\n                    proposal = pos - alpha_current * grad\n                    proposal = np.clip(proposal, domain_min, domain_max)\n                    \n                    if objective_function(proposal)  f_current:\n                        pos_next = proposal\n                        step_accepted = True\n                        break\n                    \n                    alpha_current *= beta # Backtrack\n                \n                if not step_accepted:\n                    pos_next = pos\n\n            # --- Update plateau counter for the next iteration ---\n            f_next = objective_function(pos_next)\n            improvement = f_current - f_next\n            \n            # Check for stagnation\n            if grad_norm  tau_g and improvement >= 0 and improvement  tau_f:\n                plateau_counter += 1\n            else:\n                plateau_counter = 0\n\n            pos = pos_next\n\n        # --- Check success criterion ---\n        dist_to_optimum = np.linalg.norm(pos - x_star_vec)\n        return dist_to_optimum = delta\n\n    test_cases = [\n        ((4.5, -4.5), 42),\n        ((-2.2, 2.2), 123),\n        ((1.5, -0.5), 7),\n        ((0.0, 0.0), 2024),\n        ((5.0, 5.0), 99)\n    ]\n    \n    results = []\n    for initial_pos, seed in test_cases:\n        success = run_one_case(initial_pos, seed)\n        results.append(success)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "前一个练习中的启发式切换规则虽然有效，但我们可以利用更丰富的几何信息来做出更明智的决策。这个高级练习要求你基于二阶导数信息——Hessian矩阵，构建一个更加原理驱动的混合优化器 。Hessian矩阵的曲率信息（特别是其最小特征值 $\\lambda_{\\min}$）揭示了目标函数的局部几何形态：正曲率意味着我们处于一个“谷”中，适合使用牛顿法等强大的局部方法；而负曲率则表明我们可能位于一个鞍点或“山脊”上，此时应主动沿负曲率方向进行逃逸。这个练习将帮助你掌握现代非凸优化中核心的算法思想。",
            "id": "3145524",
            "problem": "考虑一个二次连续可微的目标函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 的无约束最小化问题。局部搜索方法通常依赖于局部曲率信息，而全局搜索则强调探索，以避免陷入非最小值的驻点。从梯度 $\\nabla f(x)$ 和海森矩阵 $\\nabla^2 f(x)$ 的基本定义出发，提出一个有原则的全局-局部切换准则，该准则同时使用海森矩阵的最小特征值（记为 $\\lambda_{\\min}(\\nabla^2 f(x))$）和梯度范数 $\\lVert \\nabla f(x) \\rVert$。然后，在一个简单的非凸示例中，通过混合全局-局部搜索算法实现此准则。\n\n该简单的非凸目标函数是二维的，定义为\n$$\nf(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + \\tfrac{1}{2} x y .\n$$\n该目标函数有多个驻点，包括非最小化点。该混合方法必须使用以下有科学依据的切换准则：\n- 如果 $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ 且 $\\lambda_{\\min}(\\nabla^2 f(x)) \\ge \\kappa_{\\text{pos}}$（表明一阶项较小且有足够大的正曲率），则进入局部牛顿型精化阶段。\n- 当 $\\lambda_{\\min}(\\nabla^2 f(x))  -\\kappa_{\\text{neg}}$（表明存在负曲率方向）时，或当 $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ 且 $\\lambda_{\\min}(\\nabla^2 f(x)) \\le \\kappa_{\\text{pos}}$（表明接近驻点但没有足够大的正曲率，如在鞍点或平坦区域）时，进入全局逃逸阶段。\n\n在局部阶段，使用阻尼牛顿步，通过求解 $\\nabla^2 f(x)\\,p = -\\nabla f(x)$ 定义，然后基于 Armijo 下降条件进行回溯。在全局阶段，沿着对应于 $\\lambda_{\\min}(\\nabla^2 f(x))$ 的单位特征向量 $v_{\\min}$ 移动，采用信赖域半径和仅接受下降值的准则。在这些阶段之外，使用带有 Armijo 回溯的梯度下降步。使用固定阈值 $g_{\\text{tol}} = 10^{-3}$、$\\kappa_{\\text{pos}} = 10^{-3}$、$\\kappa_{\\text{neg}} = 10^{-8}$，信赖域半径 $r = 5\\times 10^{-1}$，以及每次运行最多 $N = 200$ 次迭代。\n\n你的程序必须在指定的目标函数上实现上述混合方法，并为以下初始点测试集生成结果：\n- $x_0 = (0,0)$,\n- $x_0 = (1,1)$,\n- $x_0 = (0,1)$,\n- $x_0 = (3,-3)$,\n- $x_0 = (0.57735,\\,0.57735)$.\n\n对于每个初始点，运行优化器 $N$ 次迭代，并以浮点数形式返回最终目标值 $f(x_N)$。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，即 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 是对应测试用例的最终目标值，顺序与上面列出的一致。不涉及物理单位或角度，输出必须是不带附加文本的纯实数。",
            "solution": "该混合准则源于点 $x \\in \\mathbb{R}^n$ 附近的二阶泰勒模型：\n$$\nf(x+p) \\approx f(x) + \\nabla f(x)^\\top p + \\tfrac{1}{2} p^\\top \\nabla^2 f(x)\\, p .\n$$\n基本事实如下。驻点满足 $\\nabla f(x) = 0$。如果此外 $\\nabla^2 f(x)$ 是正定的，那么对于任何非零的 $p$，二次项都严格为正，表明 $x$ 是一个严格局部极小点。如果 $\\nabla^2 f(x)$ 至少有一个负特征值，则存在一个负曲率方向，沿着该方向二次项严格为负，这意味着 $x$ 不可能是局部极小点，并且可以通过沿该方向移动来实现下降步。因此，同时使用梯度范数 $\\lVert \\nabla f(x) \\rVert$ 和最小特征值 $\\lambda_{\\min}(\\nabla^2 f(x))$ 来决定是进行局部精化还是全局逃逸是合理的。\n\n我们现在定义切换准则。设 $g_{\\text{tol}}$、$\\kappa_{\\text{pos}}$ 和 $\\kappa_{\\text{neg}}$ 为小的正阈值。如果 $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ 且 $\\lambda_{\\min}(\\nabla^2 f(x)) \\ge \\kappa_{\\text{pos}}$，二阶模型表明 $x$ 位于一个候选极小点附近的局部凸区域中；这为进入局部牛顿型阶段提供了理由。相反地，如果 $\\lambda_{\\min}(\\nabla^2 f(x))  -\\kappa_{\\text{neg}}$，或者如果 $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ 且 $\\lambda_{\\min}(\\nabla^2 f(x)) \\le \\kappa_{\\text{pos}}$，那么要么存在一个负曲率方向，要么该点接近驻点但没有足够大的正曲率；这为进入一个由负曲率方向引导的全局逃逸阶段提供了理由。在这些情况之外，使用梯度下降步是合理的，因为大的梯度在模型中占主导地位，一个简单的下降方向 $p = -\\nabla f(x)$ 能有效地减小一阶项。\n\n局部步基于牛顿法：求解 $\\nabla^2 f(x)\\, p = -\\nabla f(x)$ 得到 $p$，然后应用回溯线搜索以确保充分下降。Armijo 条件要求存在一个步长 $\\alpha$ 使得\n$$\nf(x + \\alpha p) \\le f(x) + c_1 \\alpha \\nabla f(x)^\\top p ,\n$$\n其中 $c_1 \\in (0,1)$ 是一个小常数，通常取 $c_1 = 10^{-4}$。这个条件是基于第一性原理的：它保证了下降量至少与预测的线性下降量成正比。如果线性系统是病态的，则回退到使用梯度下降。\n\n全局逃逸步利用负曲率。设 $v_{\\min}$ 是与 $\\lambda_{\\min}(\\nabla^2 f(x))$ 相关联的单位特征向量。对于任何非零标量 $t$，沿着 $p = t v_{\\min}$ 方向的二阶项是 $\\tfrac{1}{2} t^2 \\lambda_{\\min}(\\nabla^2 f(x))$，如果 $\\lambda_{\\min}(\\nabla^2 f(x))  0$，该项严格为负。这确保了沿着 $\\pm v_{\\min}$ 方向的足够小的步长可以减小二次模型的值。为了相对于线性项来偏置下降方向，选择方向 $d = -\\operatorname{sign}(\\nabla f(x)^\\top v_{\\min})\\, v_{\\min}$，并在信赖域半径 $r$ 内对步长 $s$ 进行回溯搜索，以保证实际的函数值下降，即，仅当 $f(x_{\\text{new}})  f(x)$ 时才接受 $x_{\\text{new}} = x + s d$。这是一个与模型和非凸性相一致的安全的仅下降接受准则。\n\n我们现在指定这个简单示例。目标函数是\n$$\nf(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + \\tfrac{1}{2} x y ,\n$$\n其梯度为\n$$\n\\nabla f(x,y) = \\begin{bmatrix} 4x(x^2 - 1) + \\tfrac{1}{2} y \\\\ 4y(y^2 - 1) + \\tfrac{1}{2} x \\end{bmatrix}\n$$\n海森矩阵为\n$$\n\\nabla^2 f(x,y) = \\begin{bmatrix} 12x^2 - 4  \\tfrac{1}{2} \\\\ \\tfrac{1}{2}  12y^2 - 4 \\end{bmatrix} .\n$$\n$\\nabla^2 f(x,y)$ 的特征结构在定义域内变化，从而产生正曲率和负曲率的区域。该混合算法由以下参数确定：$g_{\\text{tol}} = 10^{-3}$、$\\kappa_{\\text{pos}} = 10^{-3}$、$\\kappa_{\\text{neg}} = 10^{-8}$、信赖域半径 $r = 5 \\times 10^{-1}$、Armijo 参数 $c_1 = 10^{-4}$、回溯因子 $\\beta = 1/2$ 以及最大迭代次数 $N = 200$。\n\n算法设计：\n- 计算 $\\nabla f(x)$ 和 $\\nabla^2 f(x)$。\n- 通过对称特征分解计算 $\\lambda_{\\min}(\\nabla^2 f(x))$ 并得到 $v_{\\min}$。\n- 如果 $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ 且 $\\lambda_{\\min}(\\nabla^2 f(x)) \\ge \\kappa_{\\text{pos}}$，则执行带 Armijo 回溯的阻尼牛顿步。\n- 否则，如果 $\\lambda_{\\min}(\\nabla^2 f(x))  -\\kappa_{\\text{neg}}$ 或 $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ 且 $\\lambda_{\\min}(\\nabla^2 f(x)) \\le \\kappa_{\\text{pos}}$，则沿 $v_{\\min}$ 方向执行负曲率逃逸步，并在半径 $r$ 内使用回溯的仅下降接受准则。\n- 否则，执行带 Armijo 回溯的梯度下降步。\n\n这种有原则的切换是基于二阶泰勒模型以及通过海森矩阵对驻点进行的分类。测试集包含为检验不同情况而选择的初始点：$(0,0)$ 是一个具有负曲率的驻点；$(1,1)$ 位于一个具有正曲率但梯度非零的局部极小点附近；$(0,1)$ 位于一个具有混合曲率的类鞍点区域附近；$(3,-3)$ 远离极小点且梯度较大；$(0.57735,\\,0.57735)$ 位于曲率符号变化的边界附近。对于每种情况，算法在 $N$ 次迭代后返回最终目标值 $f(x_N)$，该值为一个实数。程序以指定的单行列表格式 $[r_1,r_2,r_3,r_4,r_5]$ 输出这五个结果，顺序与测试用例的顺序一致。",
            "answer": "```python\nimport numpy as np\n\n# Hybrid global–local optimizer implementing curvature- and gradient-based switching\n# for the toy nonconvex function:\n#     f(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + 0.5 * x * y\n\ndef f_val(xy: np.ndarray) -> float:\n    x, y = xy\n    return (x**2 - 1)**2 + (y**2 - 1)**2 + 0.5 * x * y\n\ndef grad_f(xy: np.ndarray) -> np.ndarray:\n    x, y = xy\n    gx = 4.0 * x * (x**2 - 1.0) + 0.5 * y\n    gy = 4.0 * y * (y**2 - 1.0) + 0.5 * x\n    return np.array([gx, gy])\n\ndef hess_f(xy: np.ndarray) -> np.ndarray:\n    x, y = xy\n    hxx = 12.0 * x**2 - 4.0\n    hyy = 12.0 * y**2 - 4.0\n    hxy = 0.5\n    return np.array([[hxx, hxy],\n                     [hxy, hyy]])\n\ndef armijo_line_search(x: np.ndarray, f_x: float, g_x: np.ndarray, p: np.ndarray,\n                       c1: float = 1e-4, alpha0: float = 1.0, beta: float = 0.5,\n                       max_backtracks: int = 50):\n    \"\"\"Armijo backtracking line search. Returns (x_new, f_new, alpha).\"\"\"\n    alpha = alpha0\n    gTp = float(np.dot(g_x, p))\n    for _ in range(max_backtracks):\n        x_new = x + alpha * p\n        f_new = f_val(x_new)\n        if f_new = f_x + c1 * alpha * gTp:\n            return x_new, f_new, alpha\n        alpha *= beta\n    # If no acceptable step found, return original point\n    return x, f_x, 0.0\n\ndef neg_curvature_escape(x: np.ndarray, f_x: float, g_x: np.ndarray, H_x: np.ndarray,\n                         tr_radius: float = 0.5, beta: float = 0.5, max_backtracks: int = 50):\n    \"\"\"Step along the most negative curvature direction within a trust region radius,\n    accepting only if f decreases. Returns (x_new, f_new, step_len).\"\"\"\n    # Symmetric eigendecomposition\n    evals, evecs = np.linalg.eigh(H_x)\n    idx_min = int(np.argmin(evals))\n    vmin = evecs[:, idx_min]\n    # Bias direction by linear term to favor decrease\n    sign = -1.0 if float(np.dot(g_x, vmin)) >= 0.0 else 1.0\n    d = sign * vmin  # unit direction (since evecs are normalized)\n    s = tr_radius\n    for _ in range(max_backtracks):\n        x_new = x + s * d\n        f_new = f_val(x_new)\n        if f_new  f_x:\n            return x_new, f_new, s\n        s *= beta\n    return x, f_x, 0.0\n\ndef hybrid_optimize(x0: np.ndarray,\n                    max_iters: int = 200,\n                    g_tol: float = 1e-3,\n                    kappa_pos: float = 1e-3,\n                    kappa_neg: float = 1e-8,\n                    tr_radius: float = 0.5,\n                    c1: float = 1e-4,\n                    beta: float = 0.5) -> float:\n    \"\"\"Run the hybrid global–local optimizer from x0 and return final objective value.\"\"\"\n    x = x0.copy().astype(float)\n    for _ in range(max_iters):\n        f_x = f_val(x)\n        g_x = grad_f(x)\n        H_x = hess_f(x)\n        grad_norm = float(np.linalg.norm(g_x))\n        # Compute smallest eigenvalue of Hessian\n        evals = np.linalg.eigvalsh(H_x)\n        lam_min = float(evals[0])\n\n        # Decide regime and take a step\n        if grad_norm = g_tol and lam_min >= kappa_pos:\n            # Local Newton-type refinement\n            try:\n                # Newton direction\n                p = np.linalg.solve(H_x, -g_x)\n            except np.linalg.LinAlgError:\n                # Fallback to gradient descent if Hessian is singular\n                p = -g_x\n            # Armijo backtracking\n            x_new, f_new, alpha = armijo_line_search(x, f_x, g_x, p, c1=c1, alpha0=1.0, beta=beta)\n            if alpha == 0.0:\n                # If no progress, break early\n                break\n            x = x_new\n        elif (lam_min  -kappa_neg) or (grad_norm = g_tol and lam_min = kappa_pos):\n            # Global escape via negative curvature direction\n            x_new, f_new, s = neg_curvature_escape(x, f_x, g_x, H_x, tr_radius=tr_radius, beta=beta)\n            if s == 0.0:\n                # If escape step fails to decrease, fallback to gradient descent\n                p = -g_x\n                x_new, f_new, alpha = armijo_line_search(x, f_x, g_x, p, c1=c1, alpha0=1.0, beta=beta)\n                if alpha == 0.0:\n                    break\n            x = x_new\n        else:\n            # Default local descent\n            p = -g_x\n            x_new, f_new, alpha = armijo_line_search(x, f_x, g_x, p, c1=c1, alpha0=1.0, beta=beta)\n            if alpha == 0.0:\n                break\n            x = x_new\n    return f_val(x)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.0, 0.0]),\n        np.array([1.0, 1.0]),\n        np.array([0.0, 1.0]),\n        np.array([3.0, -3.0]),\n        np.array([0.57735, 0.57735]),\n    ]\n\n    results = []\n    for x0 in test_cases:\n        final_f = hybrid_optimize(x0,\n                                  max_iters=200,\n                                  g_tol=1e-3,\n                                  kappa_pos=1e-3,\n                                  kappa_neg=1e-8,\n                                  tr_radius=0.5,\n                                  c1=1e-4,\n                                  beta=0.5)\n        results.append(final_f)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}