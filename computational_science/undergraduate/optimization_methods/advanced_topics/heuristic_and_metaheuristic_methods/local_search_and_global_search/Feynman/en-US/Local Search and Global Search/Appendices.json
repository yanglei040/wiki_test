{
    "hands_on_practices": [
        {
            "introduction": "Purely local search methods, such as gradient descent, navigate a function's landscape using only local gradient information. While efficient in well-behaved convex regions, they are easily stalled on large, flat \"plateaus\" or trapped in suboptimal local minima. This first hands-on practice challenges you to implement a hybrid algorithm that intelligently switches between a deterministic local search and a stochastic random walk, demonstrating a practical approach to escape these common traps in nonconvex optimization .",
            "id": "3145499",
            "problem": "Consider unconstrained minimization of a continuously differentiable objective function in two variables. The objective is to design and implement a hybrid deterministic–stochastic search that combines local gradient-based descent with global random exploration, where low-gradient regions trigger random walks. The program must be self-contained and must execute without user input, producing a single line of output in a specified format.\n\nFundamental base: Use the definition of gradient in multivariable calculus, the chain rule, and the principle that steepest descent follows the negative gradient direction. The search should be global in the sense that it can escape regions where the deterministic method would stall due to a small gradient.\n\nDefine the objective function on the domain $\\mathcal{D} = [-5,5]\\times[-5,5]$ by\n$$\nf(x,y) \\;=\\; \\sigma\\!\\left(\\sqrt{x^2+y^2};R,k\\right) \\;+\\; c\\,(x^2+y^2) \\;-\\; A_1\\,\\exp\\!\\left(-\\frac{(x-x^\\star)^2+(y-y^\\star)^2}{2s_1^2}\\right) \\;-\\; A_2\\,\\exp\\!\\left(-\\frac{(x-x_2)^2+(y-y_2)^2}{2s_2^2}\\right),\n$$\nwhere\n- $\\sigma(r;R,k) = \\dfrac{1}{1+\\exp\\!\\left(-k\\,(r-R)\\right)}$ is a logistic function that creates deceptive plateaus both for $r\\ll R$ and $r\\gg R$ by saturating to values with small derivative,\n- $(x^\\star,y^\\star) = (1,-1)$ is the location of the global basin,\n- $(x_2,y_2) = (-2,2)$ is a location of a shallow local basin,\n- $R=3$, $k=5$, $c=0.001$, $A_1=1$, $s_1=0.25$, $A_2=0.2$, $s_2=0.5$.\n\nThe gradient-based local search must use the gradient $\\nabla f(x,y)$ and a backtracking step-size rule to accept or reject deterministic updates. The global search component must be a random walk that is triggered when the local gradient magnitude is small and recent improvement is negligible. Random walk proposals must be drawn isotropically, with step lengths drawn from a continuous distribution, and must be clipped to the domain $\\mathcal{D}$. The hybrid algorithm must:\n- compute $\\nabla f(x,y)$ analytically,\n- perform a deterministic step $z_{t+1} = z_t - \\alpha\\,\\nabla f(z_t)$ with backtracking on $\\alpha$ when the objective does not improve,\n- detect plateaus using a gradient magnitude threshold and a minimal improvement threshold over recent iterations,\n- trigger a random walk that samples $N$ candidate steps and accepts the best improving candidate if any improve beyond a minimal threshold,\n- adapt the random walk step scale upward when repeated random walks fail to improve, capped at a maximum scale, and reset the scale after successful improvement,\n- clip all proposals to $\\mathcal{D}$.\n\nUse the following parameter values in your implementation:\n- Deterministic step-size initial value $\\alpha = 0.5$ with backtracking factor $\\beta = 0.5$ and at most $3$ backtracking attempts per iteration,\n- Gradient magnitude plateau threshold $\\tau_g = 0.02$,\n- Minimal improvement threshold per iteration $\\tau_f = 10^{-6}$,\n- Plateau trigger counter $L = 5$ consecutive low-gradient and low-improvement iterations,\n- Random walk number of proposals $N = 50$ per trigger,\n- Initial random walk step scale $s_{\\mathrm{rw}} = 0.5$, multiplicative growth factor $\\gamma_{\\mathrm{rw}}=1.2$, maximum step scale $s_{\\mathrm{rw},\\max} = 3.0$,\n- Maximum number of outer iterations $T_{\\max} = 500$.\n\nYour program must evaluate $f$ at $(x^\\star,y^\\star)$ to define a reference optimum value $f^\\star = f(x^\\star,y^\\star)$. For each test case, the program must return a boolean indicating success if the final optimizer state $(x_T,y_T)$ satisfies $\\sqrt{(x_T-x^\\star)^2 + (y_T-y^\\star)^2} \\leq \\delta$ with tolerance $\\delta = 0.2$.\n\nAnalytical expressions required for implementation:\n1. For $r=\\sqrt{x^2+y^2}$, $\\sigma(r;R,k)=\\dfrac{1}{1+\\exp\\!\\left(-k\\,(r-R)\\right)}$, its derivative with respect to $r$ is:\n$$\n\\frac{d\\sigma}{dr} = k\\,\\sigma(r;R,k)\\,\\bigl(1-\\sigma(r;R,k)\\bigr).\n$$\nBy the chain rule, for $r>0$,\n$$\n\\nabla_{x,y}\\,\\sigma(r;R,k) = \\frac{d\\sigma}{dr}\\,\\frac{1}{r}\\,\\begin{bmatrix}x\\\\y\\end{bmatrix},\n$$\nand for $r=0$, define $\\nabla_{x,y}\\,\\sigma(0;R,k)=\\begin{bmatrix}0\\\\0\\end{bmatrix}$.\n2. For the quadratic term, $\\nabla_{x,y}\\,\\bigl(c\\,(x^2+y^2)\\bigr) = \\begin{bmatrix}2c\\,x\\\\2c\\,y\\end{bmatrix}$.\n3. For a Gaussian well $g(x,y) = -A\\,\\exp\\!\\left(-\\dfrac{(x-\\mu_x)^2+(y-\\mu_y)^2}{2s^2}\\right)$, the gradient is\n$$\n\\nabla g(x,y) = A\\,\\exp\\!\\left(-\\frac{(x-\\mu_x)^2+(y-\\mu_y)^2}{2s^2}\\right)\\,\\frac{1}{s^2}\\,\\begin{bmatrix}x-\\mu_x\\\\y-\\mu_y\\end{bmatrix}.\n$$\nApply this to both Gaussian terms with the specified parameters.\n\nTest suite:\nImplement the hybrid algorithm on the following test cases, each consisting of an initial point and a seed for the Random Number Generator (RNG):\n- Case $1$: initial $(x_0,y_0) = (4.5,-4.5)$, seed $42$,\n- Case $2$: initial $(x_0,y_0) = (-2.2,2.2)$, seed $123$,\n- Case $3$: initial $(x_0,y_0) = (1.5,-0.5)$, seed $7$,\n- Case $4$: initial $(x_0,y_0) = (0,0)$, seed $2024$,\n- Case $5$: initial $(x_0,y_0) = (5.0,5.0)$, seed $99$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of booleans enclosed in square brackets (e.g., $\\texttt{[True,False,True,True,False]}$). No additional text must be printed.",
            "solution": "The user-provided problem is assessed to be valid. It is a well-posed, scientifically grounded problem in the field of numerical optimization, with all necessary data, definitions, and constraints provided for a unique and verifiable solution.\n\nThe problem requires the design and implementation of a hybrid optimization algorithm to find the minimum of a specified two-dimensional function $f(x,y)$. The algorithm combines a deterministic local search method (gradient descent with backtracking) with a stochastic global search method (random walk). The transition between these two modes is governed by a plateau-detection mechanism.\n\nThe objective function is defined on the domain $\\mathcal{D} = [-5,5]\\times[-5,5]$ as:\n$$\nf(x,y) \\;=\\; \\sigma\\!\\left(\\sqrt{x^2+y^2};R,k\\right) \\;+\\; c\\,(x^2+y^2) \\;-\\; A_1\\,\\exp\\!\\left(-\\frac{(x-x^\\star)^2+(y-y^\\star)^2}{2s_1^2}\\right) \\;-\\; A_2\\,\\exp\\!\\left(-\\frac{(x-x_2)^2+(y-y_2)^2}{2s_2^2}\\right)\n$$\nThe function is composed of four terms:\n$1$. A logistic (sigmoid) term $\\sigma(r;R,k) = \\frac{1}{1+\\exp(-k(r-R))}$ with $r = \\sqrt{x^2+y^2}$. This term introduces large, flat regions (plateaus) for radial distances $r \\ll R$ and $r \\gg R$, designed to challenge the optimization algorithm.\n$2$. A shallow quadratic bowl $c\\,(x^2+y^2)$ which gently pushes the solution towards the origin.\n$3$. A deep Gaussian well centered at $(x^\\star, y^\\star) = (1,-1)$, which defines the global minimum.\n$4$. A shallower Gaussian well centered at $(x_2, y_2) = (-2,2)$, which acts as a local minimum (a deceptive trap).\n\nThe parameters for the function are given as $R=3$, $k=5$, $c=0.001$, $A_1=1$, $s_1=0.25$, $A_2=0.2$, and $s_2=0.5$.\n\nTo implement the algorithm, we first need the analytical gradient of the objective function, $\\nabla f(x,y)$. The gradient is the sum of the gradients of its four constituent terms, $\\nabla f = \\nabla f_1 + \\nabla f_2 + \\nabla f_3 + \\nabla f_4$.\n\n$1$. Gradient of the sigmoid term, $\\nabla f_1$:\nUsing the chain rule, $\\nabla_{x,y}\\,\\sigma(r) = \\frac{d\\sigma}{dr}\\nabla_{x,y}(r)$. For $r = \\sqrt{x^2+y^2} > 0$, we have $\\nabla_{x,y}(r) = \\frac{1}{r}[x, y]^T$. The derivative $\\frac{d\\sigma}{dr}$ is given as $k\\,\\sigma(r)(1-\\sigma(r))$. A numerically stable form is $\\frac{d\\sigma}{dr} = k\\,\\frac{\\exp(-k(r-R))}{(1+\\exp(-k(r-R)))^2}$.\nCombining these, for $r>0$:\n$$\n\\nabla f_1(x,y) = k\\,\\sigma(r)(1-\\sigma(r))\\,\\frac{1}{r}\\begin{bmatrix}x\\\\y\\end{bmatrix}\n$$\nFor $r=0$, the gradient is defined to be $\\begin{bmatrix}0\\\\0\\end{bmatrix}$.\n\n$2$. Gradient of the quadratic term, $\\nabla f_2$:\nThis is a standard result:\n$$\n\\nabla f_2(x,y) = \\nabla \\left(c\\,(x^2+y^2)\\right) = \\begin{bmatrix}2cx\\\\2cy\\end{bmatrix}\n$$\n\n$3$. & $4$. Gradient of the Gaussian wells, $\\nabla f_3$ and $\\nabla f_4$:\nUsing the provided formula for $g(x,y) = -A\\,\\exp\\!\\left(-\\frac{(x-\\mu_x)^2+(y-\\mu_y)^2}{2s^2}\\right)$, its gradient is:\n$$\n\\nabla g(x,y) = \\frac{A}{s^2}\\,\\exp\\!\\left(-\\frac{(x-\\mu_x)^2+(y-\\mu_y)^2}{2s^2}\\right)\\begin{bmatrix}x-\\mu_x\\\\y-\\mu_y\\end{bmatrix}\n$$\nWe apply this formula for the third term with parameters $A_1, s_1, (\\mu_x, \\mu_y) = (x^\\star, y^\\star)$ and for the fourth term with $A_2, s_2, (\\mu_x, \\mu_y) = (x_2, y_2)$.\n\nThe hybrid algorithm proceeds iteratively for a maximum of $T_{\\max} = 500$ iterations. At each iteration $t$, the state is defined by the current position $z_t = (x_t, y_t)$, the random walk scale $s_{\\mathrm{rw}}$, and a counter for consecutive plateau-like iterations.\n\nThe logic for an iteration is as follows:\nFirst, we determine if the algorithm is stuck on a plateau. A plateau state is triggered if, for $L=5$ consecutive iterations, both the gradient magnitude has been below a threshold $\\tau_g = 0.02$ and the improvement in the objective function has been below a threshold $\\tau_f = 10^{-6}$.\n\nIf a plateau is detected (`plateau_counter >= L`):\nA random walk is executed. $N=50$ candidate points are generated by adding random displacement vectors to the current position $z_t$. These vectors are sampled isotropically from a disk of radius $s_{\\mathrm{rw}}$. Each candidate point is clipped to the domain $\\mathcal{D}$. The objective function is evaluated for all candidates. If the best candidate offers an improvement of at least $\\tau_f$ over the current function value, the algorithm moves to that point, and the random walk scale $s_{\\mathrm{rw}}$ is reset to its initial value of $0.5$. If no candidate provides sufficient improvement, the position remains unchanged, but $s_{\\mathrm{rw}}$ is increased by a factor of $\\gamma_{\\mathrm{rw}}=1.2$, up to a maximum of $s_{\\mathrm{rw}, \\max}=3.0$. This adaptive scaling allows the search to explore larger regions when it is persistently stuck. After a random walk attempt, the plateau counter is reset.\n\nIf no plateau is detected:\nA standard gradient descent step is performed. The next position is proposed as $z_{t+1} = z_t - \\alpha \\nabla f(z_t)$, where the initial step size is $\\alpha=0.5$. A backtracking line search is used to ensure improvement. The proposed point is clipped to $\\mathcal{D}$. If $f(z_{t+1}) < f(z_t)$, the step is accepted. Otherwise, $\\alpha$ is reduced by a factor of $\\beta=0.5$, and a new point is proposed. This is repeated up to $3$ times. If no improvement is found after all backtracking attempts, the position is not updated for this iteration.\n\nAfter each iteration, whether a random walk or gradient descent, the algorithm updates the plateau counter for the next iteration. If the gradient magnitude at the start of the iteration was below $\\tau_g$ and the resulting improvement was below $\\tau_f$, the counter is incremented; otherwise, it is reset to $0$.\n\nThe process begins from a specified initial point $(x_0, y_0)$ and repeats for $T_{\\max}$ iterations. The final position $(x_T, y_T)$ is then compared to the true global minimum's location $(x^\\star, y^\\star)$. Success is declared if the Euclidean distance $\\sqrt{(x_T-x^\\star)^2 + (y_T-y^\\star)^2}$ is less than or equal to a tolerance $\\delta=0.2$.\n\nThe implementation will execute this logic for each of the $5$ test cases, using the specified initial points and random number generator seeds to ensure reproducibility. The final output is a list of booleans indicating success or failure for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run the hybrid optimization algorithm on all test cases.\n    \"\"\"\n    \n    # --- Function and Algorithm Parameters ---\n    # Objective function parameters\n    R, k, c = 3.0, 5.0, 0.001\n    A1, s1 = 1.0, 0.25\n    x_star, y_star = 1.0, -1.0\n    A2, s2 = 0.2, 0.5\n    x2, y2 = -2.0, 2.0\n    \n    # Hybrid algorithm parameters\n    alpha_initial = 0.5\n    beta = 0.5\n    max_backtrack = 3\n    tau_g = 0.02\n    tau_f = 1e-6\n    L = 5\n    N = 50\n    s_rw_initial = 0.5\n    gamma_rw = 1.2\n    s_rw_max = 3.0\n    T_max = 500\n    \n    # Success criterion\n    delta = 0.2\n    \n    # Domain boundaries\n    domain_min, domain_max = -5.0, 5.0\n    x_star_vec = np.array([x_star, y_star])\n\n    def objective_function(pos):\n        \"\"\"Computes the value of the objective function f(x, y).\"\"\"\n        x, y = pos\n        r = np.sqrt(x**2 + y**2)\n        \n        # Term 1: Sigmoid\n        # Use np.exp for safe exponentiation\n        # Note: -k*(r-R) can be large, leading to exp -> 0.\n        # It can also be very negative, leading to exp -> inf.\n        # np.exp handles large arguments by overflowing to inf, which is fine here.\n        sigma_val = 1.0 / (1.0 + np.exp(-k * (r - R)))\n        \n        # Term 2: Quadratic\n        quad_val = c * (x**2 + y**2)\n        \n        # Term 3: Global minimum Gaussian well\n        gauss1_val = -A1 * np.exp(-((x - x_star)**2 + (y - y_star)**2) / (2 * s1**2))\n        \n        # Term 4: Local minimum Gaussian well\n        gauss2_val = -A2 * np.exp(-((x - x2)**2 + (y - y2)**2) / (2 * s2**2))\n        \n        return sigma_val + quad_val + gauss1_val + gauss2_val\n\n    def gradient_function(pos):\n        \"\"\"Computes the analytical gradient of the objective function.\"\"\"\n        x, y = pos\n        grad = np.zeros(2)\n        r = np.sqrt(x**2 + y**2)\n        \n        # Grad Term 1: Sigmoid\n        if r > 1e-9: # Avoid division by zero\n            exp_term = np.exp(-k * (r - R))\n            sigma_val = 1.0 / (1.0 + exp_term)\n            d_sigma_dr = k * sigma_val * (1.0 - sigma_val)\n            grad_sigma = d_sigma_dr * (1.0 / r) * np.array([x, y])\n            grad += grad_sigma\n            \n        # Grad Term 2: Quadratic\n        grad_quad = 2.0 * c * np.array([x, y])\n        grad += grad_quad\n        \n        # Grad Term 3: Global minimum Gaussian\n        exp_gauss1 = np.exp(-((x - x_star)**2 + (y - y_star)**2) / (2 * s1**2))\n        grad_gauss1 = (A1 / s1**2) * exp_gauss1 * np.array([x - x_star, y - y_star])\n        grad += grad_gauss1\n\n        # Grad Term 4: Local minimum Gaussian\n        exp_gauss2 = np.exp(-((x - x2)**2 + (y - y2)**2) / (2 * s2**2))\n        grad_gauss2 = (A2 / s2**2) * exp_gauss2 * np.array([x - x2, y - y2])\n        grad += grad_gauss2\n\n        return grad\n\n    def run_one_case(initial_pos, seed):\n        \"\"\"Runs the hybrid optimization for a single test case.\"\"\"\n        rng = np.random.default_rng(seed)\n        pos = np.array(initial_pos, dtype=float)\n        \n        plateau_counter = 0\n        s_rw = s_rw_initial\n\n        for _ in range(T_max):\n            f_current = objective_function(pos)\n            grad = gradient_function(pos)\n            grad_norm = np.linalg.norm(grad)\n\n            # --- Mode Selection ---\n            if plateau_counter >= L:\n                # Random Walk Mode\n                thetas = rng.uniform(0, 2 * np.pi, N)\n                # Sample radius for uniform distribution in a disk\n                radii = s_rw * np.sqrt(rng.uniform(0, 1, N))\n                \n                proposals = pos + np.column_stack([radii * np.cos(thetas), radii * np.sin(thetas)])\n                proposals = np.clip(proposals, domain_min, domain_max)\n                \n                f_proposals = np.array([objective_function(p) for p in proposals])\n                \n                best_idx = np.argmin(f_proposals)\n                f_best_proposal = f_proposals[best_idx]\n                \n                if f_best_proposal < f_current - tau_f:\n                    pos_next = proposals[best_idx]\n                    s_rw = s_rw_initial  # Reset scale on success\n                else:\n                    pos_next = pos\n                    s_rw = min(s_rw * gamma_rw, s_rw_max) # Increase scale on failure\n                \n                plateau_counter = 0 # RW attempt was made, reset counter\n            else:\n                # Gradient Descent Mode\n                alpha_current = alpha_initial\n                step_accepted = False\n                for _ in range(max_backtrack + 1):\n                    proposal = pos - alpha_current * grad\n                    proposal = np.clip(proposal, domain_min, domain_max)\n                    \n                    if objective_function(proposal) < f_current:\n                        pos_next = proposal\n                        step_accepted = True\n                        break\n                    \n                    alpha_current *= beta # Backtrack\n                \n                if not step_accepted:\n                    pos_next = pos\n\n            # --- Update plateau counter for the next iteration ---\n            f_next = objective_function(pos_next)\n            improvement = f_current - f_next\n            \n            # Check for stagnation\n            if grad_norm < tau_g and improvement >= 0 and improvement < tau_f:\n                plateau_counter += 1\n            else:\n                plateau_counter = 0\n\n            pos = pos_next\n\n        # --- Check success criterion ---\n        dist_to_optimum = np.linalg.norm(pos - x_star_vec)\n        return dist_to_optimum <= delta\n\n    test_cases = [\n        ((4.5, -4.5), 42),\n        ((-2.2, 2.2), 123),\n        ((1.5, -0.5), 7),\n        ((0.0, 0.0), 2024),\n        ((5.0, 5.0), 99)\n    ]\n    \n    results = []\n    for initial_pos, seed in test_cases:\n        success = run_one_case(initial_pos, seed)\n        results.append(success)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While random exploration can help escape local traps, a purely random search can be extremely inefficient, especially in high-dimensional spaces. A more effective global strategy often involves using heuristics to guide the search toward promising regions. In this exercise, you will confront a \"deceptive\" landscape where the global minimum is intentionally hidden behind a high-curvature barrier, and you will implement a curvature-aware sampling method designed to detect and explore such features, providing a significant advantage over naive uniform sampling .",
            "id": "3145597",
            "problem": "You must write a complete and runnable program that constructs and optimizes a deliberately deceptive objective function in two variables. The goal is to compare a curvature-aware global sampling method against a baseline uniform global sampling method at locating a hidden global minimum that lies inside a high-curvature barrier. All mathematical quantities must be handled exactly as specified below.\n\nDefinition of the objective. Consider the function in two variables $\\mathbf{x} = (x_1,x_2) \\in \\mathbb{R}^2$:\n$$\nf(\\mathbf{x}) \\;=\\; \\alpha \\,\\lVert \\mathbf{x} \\rVert_2^2 \\;+\\; B \\,\\exp\\!\\left(-\\dfrac{\\left(\\lVert \\mathbf{x} \\rVert_2 - r_b\\right)^2}{2\\,\\sigma_b^2}\\right) \\;-\\; A \\,\\exp\\!\\left(-\\dfrac{\\lVert \\mathbf{x}-\\boldsymbol{\\mu}\\rVert_2^2}{2\\,\\sigma_w^2}\\right),\n$$\nwhere $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. The constants are fixed as\n- $\\alpha = 0.02$,\n- $B = 3.0$,\n- $r_b = 1.2$,\n- $A = 5.0$,\n- $\\boldsymbol{\\mu} = (0.25,-0.15)$,\n- $\\sigma_w = 0.10$,\nand the domain is the square $\\mathcal{D} = [-2,2] \\times [-2,2]$. The scalar $\\sigma_b > 0$ controls the width of the ring-shaped high-curvature barrier. The global minimizer is inside the barrier, near $\\boldsymbol{\\mu}$, but the barrier makes locating it by naive sampling difficult.\n\nFoundational definitions to be used. Let $\\nabla f(\\mathbf{x})$ denote the gradient and $H(\\mathbf{x})$ the Hessian matrix. The Laplacian is defined as $\\Delta f(\\mathbf{x}) = \\text{tr}(H(\\mathbf{x}))$. You may approximate derivatives and curvature numerically using Finite Difference (FD) formulas on a uniform grid. For a uniform grid with spacing $h$ in both directions and grid values $f_{i,j}$, the standard $5$-point stencil for the Laplacian at an interior grid node is\n$$\n\\Delta f \\approx \\dfrac{f_{i+1,j} + f_{i-1,j} + f_{i,j+1} + f_{i,j-1} - 4 f_{i,j}}{h^2}.\n$$\nThe central difference for the $k$-th component of the gradient at $\\mathbf{x}$ with step $h_g$ is\n$$\n\\left[\\nabla f(\\mathbf{x})\\right]_k \\approx \\dfrac{f(\\mathbf{x}+h_g\\mathbf{e}_k) - f(\\mathbf{x}-h_g\\mathbf{e}_k)}{2h_g},\n$$\nwhere $\\mathbf{e}_k$ is the $k$-th standard basis vector.\n\nOptimization tasks. Implement two methods that both adhere to a strict function-evaluation budget $B_{\\text{eval}}$ (every call to $f(\\cdot)$ counts as one evaluation):\n\n- Method $0$ (baseline, pure global search): Draw points uniformly at random in $\\mathcal{D}$ until the evaluation budget is exhausted, and report the best function value seen. No local refinement is permitted.\n\n- Method $1$ (curvature-aware global search with local refinement):\n  1. Use approximately a fixed fraction of the budget on a coarse uniform grid over $\\mathcal{D}$ to evaluate $f$ and estimate the discrete Laplacian magnitude $\\lvert \\Delta f \\rvert$ at interior grid nodes using the $5$-point stencil. Select the $K$ highest-curvature grid nodes (largest $\\lvert \\Delta f \\rvert$).\n  2. Estimate a curvature center $\\mathbf{c}$ as the weighted centroid of these $K$ nodes with weights proportional to $\\lvert \\Delta f \\rvert$. From each selected node $\\mathbf{p}$, generate a short ray of samples towards $\\mathbf{c}$ at several equally spaced fractions along the line segment from $\\mathbf{p}$ to $\\mathbf{c}$, consuming a portion of the budget.\n  3. Use any remaining budget fraction for uniform random samples in $\\mathcal{D}$.\n  4. Take the best point found so far and run a simple gradient-based local search (using central FD gradients and a backtracking step-size rule) until either a small gradient-norm condition is met or the local refinement sub-budget is exhausted. Count all function evaluations used by the local search towards the same global budget.\n\nImplementation constraints. Angles are not used directly, so no angle unit specification is needed. No physical units are involved. All randomness must be controlled by a provided integer seed per test case via a reproducible pseudo-random number generator. All sampling must remain within $\\mathcal{D}$ by projection when necessary.\n\nSuccess metric. For a given $\\sigma_b$, the true global minimum value is attained at $\\mathbf{x}^\\star = \\boldsymbol{\\mu}$ because the barrier term is negligible at $\\boldsymbol{\\mu}$ for the given parameters. For grading, compute the reference value\n$$\nf^\\star(\\sigma_b) \\;=\\; \\alpha\\,\\lVert \\boldsymbol{\\mu} \\rVert_2^2 \\;+\\; B\\,\\exp\\!\\left(-\\dfrac{\\left(\\lVert \\boldsymbol{\\mu} \\rVert_2 - r_b\\right)^2}{2\\,\\sigma_b^2}\\right) \\;-\\; A,\n$$\nand declare success if the best value $f_{\\min}$ found by the method satisfies\n$$\nf_{\\min} - f^\\star(\\sigma_b) \\;\\le\\; \\tau,\n$$\nwith tolerance $\\tau = 0.02$.\n\nTest suite. Your program must run the following test cases without any user input. Each test case is a tuple $(\\sigma_b, B_{\\text{eval}}, \\text{method}, \\text{seed})$:\n- Test $1$: $(0.25, 2000, 1, 1)$,\n- Test $2$: $(0.15, 2200, 1, 2)$,\n- Test $3$: $(0.12, 3000, 1, 3)$,\n- Test $4$: $(0.15, 1500, 0, 2)$.\n\nFor each test case, output a boolean indicating whether the method succeeded under the given budget and random seed.\n\nFinal output format. Your program should produce a single line of output containing the results for the four tests as a comma-separated list enclosed in square brackets, for example, \"[True,False,True,True]\". No other text must be printed.",
            "solution": "The problem statement has been critically validated and is deemed **valid**. It presents a well-posed numerical optimization challenge that is scientifically grounded in the field of optimization methods. The objective function is clearly defined, and the optimization tasks, while containing some unspecified hyperparameters, are structured as a standard algorithmic design problem where the implementer is expected to make reasonable, informed choices. Such choices do not constitute a flaw but are part of the exercise. The problem is self-contained, objective, and its success metric is unambiguous.\n\n### 1. Mathematical Formulation\n\nThe core of the problem is the minimization of the objective function $f(\\mathbf{x})$ over the domain $\\mathcal{D} = [-2,2] \\times [-2,2]$, where $\\mathbf{x} = (x_1, x_2) \\in \\mathbb{R}^2$. The function is defined as:\n$$\nf(\\mathbf{x}) \\;=\\; \\alpha \\,\\lVert \\mathbf{x} \\rVert_2^2 \\;+\\; B \\,\\exp\\!\\left(-\\dfrac{\\left(\\lVert \\mathbf{x} \\rVert_2 - r_b\\right)^2}{2\\,\\sigma_b^2}\\right) \\;-\\; A \\,\\exp\\!\\left(-\\dfrac{\\lVert \\mathbf{x}-\\boldsymbol{\\mu}\\rVert_2^2}{2\\,\\sigma_w^2}\\right)\n$$\nwith constants $\\alpha = 0.02$, $B = 3.0$, $r_b = 1.2$, $A = 5.0$, $\\boldsymbol{\\mu} = (0.25, -0.15)$, and $\\sigma_w = 0.10$. This function is composed of three terms:\n1.  A quadratic term $\\alpha \\lVert \\mathbf{x} \\rVert_2^2$, which creates a shallow global minimum at the origin $\\mathbf{x}=(0,0)$.\n2.  A positive Gaussian ring term $B \\exp(\\dots)$, which creates a circular barrier of radius $r_b=1.2$. The parameter $\\sigma_b$ controls the barrier's width and sharpness, with smaller values of $\\sigma_b$ leading to a more challenging, higher-curvature barrier.\n3.  A negative Gaussian well term $-A \\exp(\\dots)$, creating a deep, narrow basin of attraction with its minimum at $\\mathbf{x} = \\boldsymbol{\\mu}$.\n\nThe global minimum is located at $\\mathbf{x}^\\star \\approx \\boldsymbol{\\mu}$, hidden inside the barrier. The reference minimum value is $f^\\star(\\sigma_b) = f(\\boldsymbol{\\mu})$. Success is achieved if a method finds a point $\\mathbf{x}_{\\min}$ such that its function value $f_{\\min} = f(\\mathbf{x}_{\\min})$ satisfies $f_{\\min} - f^\\star(\\sigma_b) \\le \\tau$, for a tolerance $\\tau = 0.02$.\n\n### 2. Algorithmic Implementation\n\nWe implement two distinct optimization strategies, both adhering to a strict function evaluation budget, $B_{\\text{eval}}$.\n\n#### Method 0: Baseline Global Search\nThis method serves as a simple benchmark. It consists of pure uniform random sampling.\n- **Procedure**: Points $\\mathbf{x}$ are drawn independently and uniformly from the domain $\\mathcal{D}$. This process continues until the evaluation budget $B_{\\text{eval}}$ is fully consumed.\n- **Output**: The best function value found among all sampled points is reported.\nThis method is expected to struggle, as random sampling is inefficient for locating a small target region, especially when a barrier obscures it.\n\n#### Method 1: Curvature-Aware Global Search with Local Refinement\nThis is a sophisticated, multi-stage strategy designed to overcome the deceptive nature of the objective function. It requires choosing several hyperparameters, for which the following reasoned selections were made:\n- **Budget Allocation**: The total budget $B_{\\text{eval}}$ is partitioned as follows: $50\\%$ for coarse grid search, $25\\%$ for ray sampling, $5\\%$ for uniform fill, and $20\\%$ for local refinement.\n- **Curvature Hotspots**: We identify the top $K=10$ grid nodes with the highest magnitude of the discrete Laplacian.\n- **Local Search Parameters**: For the gradient-based refinement, we use a central difference step $h_g = 10^{-6}$. The backtracking line search starts with a step size $\\alpha_0=0.1$, uses a reduction factor $\\beta=0.5$, and an Armijo condition constant $c=10^{-4}$. The search terminates if the gradient norm falls below $10^{-5}$ or its budget is exhausted.\n\nThe four stages of Method 1 are:\n1.  **Coarse Grid Scan**: A uniform grid is laid over $\\mathcal{D}$. The grid size $N \\times N$ is determined by the allocated budget, $N = \\lfloor\\sqrt{0.50 \\cdot B_{\\text{eval}}}\\rfloor$. The function $f(\\mathbf{x})$ is evaluated at each of the $N^2$ grid points. For all interior points of the grid, the Laplacian $\\Delta f$ is estimated using the provided $5$-point finite-difference stencil:\n    $$\n    \\Delta f \\approx \\dfrac{f_{i+1,j} + f_{i-1,j} + f_{i,j+1} + f_{i,j-1} - 4 f_{i,j}}{h^2}\n    $$\n    where $h = 4/(N-1)$ is the grid spacing. The $K=10$ grid nodes with the largest values of $|\\Delta f|$ are selected. These nodes are expected to lie on the high-curvature barrier.\n\n2.  **Ray Sampling**: The intuition is that the global minimum is located \"inward\" from the barrier. We estimate the center of the high-curvature region, $\\mathbf{c}$, as the weighted centroid of the $K$ nodes $\\{\\mathbf{p}_i\\}$, with weights given by their corresponding $|\\Delta f_i|$ values:\n    $$\n    \\mathbf{c} = \\frac{\\sum_{i=1}^{K} |\\Delta f_i| \\mathbf{p}_i}{\\sum_{i=1}^{K} |\\Delta f_i|}\n    $$\n    From each high-curvature node $\\mathbf{p}_i$, we generate a ray of samples along the line segment connecting $\\mathbf{p}_i$ to $\\mathbf{c}$. This focuses the search on the region radially inside the barrier, increasing the probability of finding the hidden well.\n\n3.  **Uniform Fill**: A small portion of the budget is used for uniform random sampling to ensure some minimal global coverage and guard against cases where the curvature-based heuristic might fail.\n\n4.  **Local Refinement**: The best point found from all previous sampling stages, $\\mathbf{x}_{\\text{best}}$, is used as the starting point for a gradient descent search. The gradient $\\nabla f(\\mathbf{x})$ is computed numerically using the central difference formula:\n    $$\n    \\left[\\nabla f(\\mathbf{x})\\right]_k \\approx \\dfrac{f(\\mathbf{x}+h_g\\mathbf{e}_k) - f(\\mathbf{x}-h_g\\mathbf{e}_k)}{2h_g}\n    $$\n    A backtracking line search is employed to determine the step size at each iteration, ensuring progress while managing the step length. This final stage is crucial for converging precisely to the local minimum once its basin of attraction has been discovered. All function evaluations, including those for gradient estimation, are counted towards the budget.\n\nAll newly generated points during ray sampling and local search are clipped to remain within the domain $\\mathcal{D}$. A single, global counter tracks all function evaluations to strictly enforce the budget $B_{\\text{eval}}$. The final success of each test case is determined by comparing the best-found function value against $f^\\star(\\sigma_b)$ with the specified tolerance $\\tau = 0.02$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    \n    class ProblemContext:\n        \"\"\"Encapsulates the problem's objective function and state.\"\"\"\n        def __init__(self, sigma_b, budget):\n            # Constants\n            self.alpha = 0.02\n            self.B = 3.0\n            self.r_b = 1.2\n            self.A = 5.0\n            self.mu = np.array([0.25, -0.15])\n            self.sigma_w = 0.10\n            \n            # Per-test parameters\n            self.sigma_b = sigma_b\n            self.budget = budget\n            \n            # State\n            self.eval_count = 0\n            self.best_x = None\n            self.best_f = float('inf')\n            \n            self.domain_min = -2.0\n            self.domain_max = 2.0\n\n        def f(self, x):\n            \"\"\"Evaluates the objective function and tracks evaluations.\"\"\"\n            if self.eval_count >= self.budget:\n                return float('inf') # Budget exceeded\n            \n            self.eval_count += 1\n            \n            norm_x = np.linalg.norm(x)\n            norm_x_mu = np.linalg.norm(x - self.mu)\n            \n            term1 = self.alpha * norm_x**2\n            term2 = self.B * np.exp(-(norm_x - self.r_b)**2 / (2 * self.sigma_b**2))\n            term3 = -self.A * np.exp(-norm_x_mu**2 / (2 * self.sigma_w**2))\n            \n            val = term1 + term2 + term3\n\n            if val < self.best_f:\n                self.best_f = val\n                self.best_x = x.copy()\n                \n            return val\n\n        def get_f_star(self):\n            \"\"\"Computes the reference minimum value.\"\"\"\n            norm_mu = np.linalg.norm(self.mu)\n            term1 = self.alpha * norm_mu**2\n            term2 = self.B * np.exp(-(norm_mu - self.r_b)**2 / (2 * self.sigma_b**2))\n            term3 = -self.A\n            return term1 + term2 + term3\n\n    def method_0(problem, rng):\n        \"\"\"Method 0: Pure uniform random search.\"\"\"\n        while problem.eval_count < problem.budget:\n            x_rand = rng.uniform(problem.domain_min, problem.domain_max, size=2)\n            problem.f(x_rand)\n        return problem.best_x, problem.best_f\n\n    def method_1(problem, rng):\n        \"\"\"Method 1: Curvature-aware search with local refinement.\"\"\"\n        \n        # Hyperparameters\n        frac_grid = 0.50\n        frac_ray = 0.25\n        frac_uniform = 0.05\n        frac_local = 0.20\n        K = 10\n        local_search_grad_h = 1e-6\n        local_search_alpha0 = 0.1\n        local_search_beta = 0.5\n        local_search_c = 1e-4\n        local_search_grad_tol = 1e-5\n        \n        # Budget allocation\n        budget_grid_end = int(frac_grid * problem.budget)\n        budget_ray_end = budget_grid_end + int(frac_ray * problem.budget)\n        budget_uniform_end = budget_ray_end + int(frac_uniform * problem.budget)\n        budget_local_end = problem.budget # Use all remaining budget for local search\n\n        # 1. Coarse Grid Scan & Laplacian Estimation\n        N = int(np.sqrt(budget_grid_end))\n        if N >= 3:\n            h = (problem.domain_max - problem.domain_min) / (N - 1)\n            x_coords = np.linspace(problem.domain_min, problem.domain_max, N)\n            y_coords = np.linspace(problem.domain_min, problem.domain_max, N)\n            f_grid = np.full((N, N), float('inf'))\n\n            for i in range(N):\n                for j in range(N):\n                    if problem.eval_count >= budget_grid_end: break\n                    pt = np.array([x_coords[i], y_coords[j]])\n                    f_grid[i, j] = problem.f(pt)\n                if problem.eval_count >= budget_grid_end: break\n\n            laplacians_info = []\n            for i in range(1, N - 1):\n                for j in range(1, N - 1):\n                    laplacian = (f_grid[i+1, j] + f_grid[i-1, j] + f_grid[i, j+1] + f_grid[i, j-1] - 4 * f_grid[i, j]) / (h**2)\n                    pt = np.array([x_coords[i], y_coords[j]])\n                    laplacians_info.append((abs(laplacian), pt))\n            \n            laplacians_info.sort(key=lambda item: item[0], reverse=True)\n            high_curvature_nodes_info = laplacians_info[:K]\n        else:\n            high_curvature_nodes_info = []\n\n        # 2. Ray Sampling\n        if high_curvature_nodes_info:\n            total_weight = sum(l_abs for l_abs, _ in high_curvature_nodes_info)\n            if total_weight > 1e-9:\n                weighted_sum = sum(l_abs * p for l_abs, p in high_curvature_nodes_info)\n                c = weighted_sum / total_weight\n                \n                n_rays = len(high_curvature_nodes_info)\n                evals_per_ray = (budget_ray_end - problem.eval_count) // n_rays if n_rays > 0 else 0\n\n                for _, p in high_curvature_nodes_info:\n                    if evals_per_ray > 0:\n                        for i in range(1, evals_per_ray + 1):\n                            if problem.eval_count >= budget_ray_end: break\n                            lam = i / (evals_per_ray + 1.0)\n                            ray_pt = np.clip(p + lam * (c - p), problem.domain_min, problem.domain_max)\n                            problem.f(ray_pt)\n                    if problem.eval_count >= budget_ray_end: break\n\n        # 3. Uniform Fill\n        while problem.eval_count < budget_uniform_end:\n            x_rand = rng.uniform(problem.domain_min, problem.domain_max, size=2)\n            problem.f(x_rand)\n\n        # 4. Local Refinement\n        if problem.best_x is not None:\n            x_current = problem.best_x.copy()\n\n            def grad_f(x, h_grad):\n                if problem.eval_count + 4 > budget_local_end: return None\n                grad = np.zeros(2)\n                for k in range(2):\n                    x_plus_h = x.copy(); x_plus_h[k] += h_grad\n                    x_minus_h = x.copy(); x_minus_h[k] -= h_grad\n                    f_plus = problem.f(np.clip(x_plus_h, problem.domain_min, problem.domain_max))\n                    f_minus = problem.f(np.clip(x_minus_h, problem.domain_min, problem.domain_max))\n                    if f_plus == float('inf') or f_minus == float('inf'): return None # Budget hit during grad calc\n                    grad[k] = (f_plus - f_minus) / (2 * h_grad)\n                return grad\n\n            while problem.eval_count < budget_local_end:\n                 if problem.eval_count + 1 > budget_local_end: break\n                 f_current = problem.f(x_current)\n                 if f_current == float('inf'): break\n\n                 grad = grad_f(x_current, local_search_grad_h)\n                 \n                 if grad is None or np.linalg.norm(grad) < local_search_grad_tol:\n                     break\n                 \n                 alpha = local_search_alpha0\n                 p_k = -grad\n\n                 for _ in range(10): # Backtracking steps\n                     if problem.eval_count + 1 > budget_local_end: break\n                     x_new = np.clip(x_current + alpha * p_k, problem.domain_min, problem.domain_max)\n                     f_new = problem.f(x_new)\n                     if f_new == float('inf'): break\n\n                     armijo_cond = f_current + local_search_c * alpha * np.dot(grad, p_k)\n                     if f_new <= armijo_cond:\n                         x_current = x_new\n                         break\n                     else:\n                         alpha *= local_search_beta\n                 else: # Backtracking failed to find a step\n                     break\n\n        return problem.best_x, problem.best_f\n\n    test_cases = [\n        (0.25, 2000, 1, 1),\n        (0.15, 2200, 1, 2),\n        (0.12, 3000, 1, 3),\n        (0.15, 1500, 0, 2),\n    ]\n\n    results = []\n    tau = 0.02\n\n    for sigma_b, B_eval, method_idx, seed in test_cases:\n        rng = np.random.default_rng(seed)\n        problem = ProblemContext(sigma_b=sigma_b, budget=B_eval)\n\n        if method_idx == 0:\n            method_0(problem, rng)\n        elif method_idx == 1:\n            method_1(problem, rng)\n            \n        f_min = problem.best_f\n        f_star = problem.get_f_star()\n        \n        success = (f_min - f_star) <= tau\n        results.append(success)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building upon heuristic-based methods, this final practice introduces a powerful and principled framework for hybrid optimization using second-order information. By analyzing the Hessian matrix, an algorithm can gain a much richer understanding of the local landscape's geometry, including its curvature. You will implement an advanced optimizer that uses the gradient norm and the Hessian's eigenvalues to dynamically switch between three distinct regimes: a rapid Newton-type local search in convex regions, a standard gradient descent, and a principled escape along directions of negative curvature in non-convex regions .",
            "id": "3145524",
            "problem": "Consider unconstrained minimization of a twice continuously differentiable objective function $f:\\mathbb{R}^n\\to\\mathbb{R}$. The method of local search typically relies on local curvature information, while global search emphasizes exploration to avoid getting trapped at non-minimizing stationary points. Starting from the fundamental definitions of the gradient $\\nabla f(x)$ and the Hessian $\\nabla^2 f(x)$, propose a principled global–local switching criterion that uses the smallest eigenvalue of the Hessian, denoted $\\lambda_{\\min}(\\nabla^2 f(x))$, together with the gradient norm $\\lVert \\nabla f(x) \\rVert$. Then implement this criterion in a hybrid global–local search for a toy nonconvex example.\n\nThe toy nonconvex objective is two-dimensional and defined by\n$$\nf(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + \\tfrac{1}{2} x y .\n$$\nThis objective has multiple stationary points including non-minimizers. The hybrid method must use the following scientifically motivated switching criterion:\n- Enter a local Newton-type refinement regime if $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ and $\\lambda_{\\min}(\\nabla^2 f(x)) \\ge \\kappa_{\\text{pos}}$ (indicating small first-order term and sufficiently positive curvature).\n- Enter a global escape regime whenever $\\lambda_{\\min}(\\nabla^2 f(x)) < -\\kappa_{\\text{neg}}$ (indicating the presence of a direction of negative curvature) or whenever $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ and $\\lambda_{\\min}(\\nabla^2 f(x)) \\le \\kappa_{\\text{pos}}$ (indicating near-stationarity without sufficient positive curvature, as in a saddle or flat region).\n\nIn the local regime, use a damped Newton step defined by solving $\\nabla^2 f(x)\\,p = -\\nabla f(x)$ followed by backtracking based on the Armijo decrease condition. In the global regime, move along the unit eigenvector $v_{\\min}$ corresponding to $\\lambda_{\\min}(\\nabla^2 f(x))$ with a trust-region radius and decrease-only acceptance. Outside these regimes, use a gradient descent step with Armijo backtracking. Use fixed thresholds $g_{\\text{tol}} = 10^{-3}$, $\\kappa_{\\text{pos}} = 10^{-3}$, $\\kappa_{\\text{neg}} = 10^{-8}$, a trust-region radius $r = 5\\times 10^{-1}$, and a maximum of $N = 200$ iterations per run.\n\nYour program must implement the above hybrid method on the specified objective and produce results for the following test suite of initial points:\n- $x_0 = (0,0)$,\n- $x_0 = (1,1)$,\n- $x_0 = (0,1)$,\n- $x_0 = (3,-3)$,\n- $x_0 = (0.57735,\\,0.57735)$.\n\nFor each initial point, run the optimizer for $N$ iterations and return the final objective value $f(x_N)$ as a floating-point number. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, i.e., $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is the final objective value for the corresponding test case, in the order listed above. No physical units or angles are involved, and the outputs must be pure real numbers without additional text.",
            "solution": "The hybrid criterion follows from the second-order Taylor model around a point $x \\in \\mathbb{R}^n$:\n$$\nf(x+p) \\approx f(x) + \\nabla f(x)^\\top p + \\tfrac{1}{2} p^\\top \\nabla^2 f(x)\\, p .\n$$\nThe fundamental facts are as follows. A stationary point satisfies $\\nabla f(x) = 0$. If additionally $\\nabla^2 f(x)$ is positive definite, then the quadratic term is strictly positive for any nonzero $p$, indicating that $x$ is a strict local minimizer. If $\\nabla^2 f(x)$ has at least one negative eigenvalue, then there exists a direction of negative curvature along which the quadratic term is strictly negative, implying that $x$ cannot be a local minimum and that a descent step can be realized by moving along that direction. Therefore, it is principled to use both the gradient norm $\\lVert \\nabla f(x) \\rVert$ and the smallest eigenvalue $\\lambda_{\\min}(\\nabla^2 f(x))$ to decide between local refinement and global escape.\n\nWe now define the switching criterion. Let $g_{\\text{tol}}$, $\\kappa_{\\text{pos}}$, and $\\kappa_{\\text{neg}}$ be small positive thresholds. If $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ and $\\lambda_{\\min}(\\nabla^2 f(x)) \\ge \\kappa_{\\text{pos}}$, the second-order model indicates that $x$ is in a locally convex region near a candidate minimizer; this justifies entering a local Newton-type regime. Conversely, if $\\lambda_{\\min}(\\nabla^2 f(x)) < -\\kappa_{\\text{neg}}$, or if $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ and $\\lambda_{\\min}(\\nabla^2 f(x)) \\le \\kappa_{\\text{pos}}$, then either a direction of negative curvature exists or the point is near-stationary without sufficient positive curvature; this justifies entering a global escape regime oriented by the negative curvature direction. Outside these cases, using a gradient descent step is reasonable, since large gradients dominate the model and a simple descent direction $p = -\\nabla f(x)$ effectively reduces the first-order term.\n\nThe local step is based on Newton's method: solve $\\nabla^2 f(x)\\, p = -\\nabla f(x)$ for $p$, and then apply backtracking line search to ensure sufficient decrease. The Armijo condition requires the existence of a step size $\\alpha$ such that\n$$\nf(x + \\alpha p) \\le f(x) + c_1 \\alpha \\nabla f(x)^\\top p ,\n$$\nwith a small constant $c_1 \\in (0,1)$, commonly $c_1 = 10^{-4}$. This condition is grounded in first principles: it guarantees a decrease at least proportional to the predicted linear reduction. If the linear system is ill-conditioned, fallback to gradient descent is used.\n\nThe global escape step exploits negative curvature. Let $v_{\\min}$ be a unit eigenvector associated with $\\lambda_{\\min}(\\nabla^2 f(x))$. For any nonzero scalar $t$, the second-order term along $p = t v_{\\min}$ is $\\tfrac{1}{2} t^2 \\lambda_{\\min}(\\nabla^2 f(x))$, which is strictly negative if $\\lambda_{\\min}(\\nabla^2 f(x)) < 0$. This ensures that sufficiently small steps along $\\pm v_{\\min}$ reduce the quadratic model. To bias the decrease with respect to the linear term, choose the direction $d = -\\operatorname{sign}(\\nabla f(x)^\\top v_{\\min})\\, v_{\\min}$ and perform a backtracking search on the step length $s$ within a trust-region radius $r$ to guarantee actual decrease, i.e., accept $x_{\\text{new}} = x + s d$ only if $f(x_{\\text{new}}) < f(x)$. This is a safe decrease-only acceptance rule consistent with the model and the nonconvexity.\n\nWe now specify the toy example. The objective is\n$$\nf(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + \\tfrac{1}{2} x y ,\n$$\nwith gradient\n$$\n\\nabla f(x,y) = \\begin{bmatrix} 4x(x^2 - 1) + \\tfrac{1}{2} y \\\\ 4y(y^2 - 1) + \\tfrac{1}{2} x \\end{bmatrix}\n$$\nand Hessian\n$$\n\\nabla^2 f(x,y) = \\begin{bmatrix} 12x^2 - 4 & \\tfrac{1}{2} \\\\ \\tfrac{1}{2} & 12y^2 - 4 \\end{bmatrix} .\n$$\nThe eigenstructure of $\\nabla^2 f(x,y)$ varies across the domain, producing regions of positive and negative curvature. The hybrid algorithm is parameterized by $g_{\\text{tol}} = 10^{-3}$, $\\kappa_{\\text{pos}} = 10^{-3}$, $\\kappa_{\\text{neg}} = 10^{-8}$, trust-region radius $r = 5 \\times 10^{-1}$, Armijo parameter $c_1 = 10^{-4}$, backtracking factor $\\beta = 1/2$, and maximum iterations $N = 200$.\n\nAlgorithmic design:\n- Evaluate $\\nabla f(x)$ and $\\nabla^2 f(x)$.\n- Compute $\\lambda_{\\min}(\\nabla^2 f(x))$ via symmetric eigendecomposition and obtain $v_{\\min}$.\n- If $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ and $\\lambda_{\\min}(\\nabla^2 f(x)) \\ge \\kappa_{\\text{pos}}$, take a damped Newton step with Armijo backtracking.\n- Else if $\\lambda_{\\min}(\\nabla^2 f(x)) < -\\kappa_{\\text{neg}}$ or $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ with $\\lambda_{\\min}(\\nabla^2 f(x)) \\le \\kappa_{\\text{pos}}$, take a negative-curvature escape step along $v_{\\min}$ with backtracking decrease-only acceptance within radius $r$.\n- Else, take a gradient descent step with Armijo backtracking.\n\nThis principled switching is grounded in the second-order Taylor model and the classification of stationary points by the Hessian. The test suite consists of initial points chosen to exercise different regimes: $(0,0)$ is a stationary point with negative curvature; $(1,1)$ lies near a local minimizer with positive curvature but nonzero gradient; $(0,1)$ is near a saddle-like region with mixed curvature; $(3,-3)$ is far from a minimizer with large gradients; $(0.57735,\\,0.57735)$ is near a boundary of curvature sign change. For each case, after $N$ iterations the algorithm returns the final objective value $f(x_N)$ as a real number. The program outputs these five results in the specified single-line list format $[r_1,r_2,r_3,r_4,r_5]$, in the order of the test cases.",
            "answer": "```python\nimport numpy as np\n\n# Hybrid global–local optimizer implementing curvature- and gradient-based switching\n# for the toy nonconvex function:\n#     f(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + 0.5 * x * y\n\ndef f_val(xy: np.ndarray) -> float:\n    x, y = xy\n    return (x**2 - 1)**2 + (y**2 - 1)**2 + 0.5 * x * y\n\ndef grad_f(xy: np.ndarray) -> np.ndarray:\n    x, y = xy\n    gx = 4.0 * x * (x**2 - 1.0) + 0.5 * y\n    gy = 4.0 * y * (y**2 - 1.0) + 0.5 * x\n    return np.array([gx, gy])\n\ndef hess_f(xy: np.ndarray) -> np.ndarray:\n    x, y = xy\n    hxx = 12.0 * x**2 - 4.0\n    hyy = 12.0 * y**2 - 4.0\n    hxy = 0.5\n    return np.array([[hxx, hxy],\n                     [hxy, hyy]])\n\ndef armijo_line_search(x: np.ndarray, f_x: float, g_x: np.ndarray, p: np.ndarray,\n                       c1: float = 1e-4, alpha0: float = 1.0, beta: float = 0.5,\n                       max_backtracks: int = 50):\n    \"\"\"Armijo backtracking line search. Returns (x_new, f_new, alpha).\"\"\"\n    alpha = alpha0\n    gTp = float(np.dot(g_x, p))\n    for _ in range(max_backtracks):\n        x_new = x + alpha * p\n        f_new = f_val(x_new)\n        if f_new <= f_x + c1 * alpha * gTp:\n            return x_new, f_new, alpha\n        alpha *= beta\n    # If no acceptable step found, return original point\n    return x, f_x, 0.0\n\ndef neg_curvature_escape(x: np.ndarray, f_x: float, g_x: np.ndarray, H_x: np.ndarray,\n                         tr_radius: float = 0.5, beta: float = 0.5, max_backtracks: int = 50):\n    \"\"\"Step along the most negative curvature direction within a trust region radius,\n    accepting only if f decreases. Returns (x_new, f_new, step_len).\"\"\"\n    # Symmetric eigendecomposition\n    evals, evecs = np.linalg.eigh(H_x)\n    idx_min = int(np.argmin(evals))\n    vmin = evecs[:, idx_min]\n    # Bias direction by linear term to favor decrease\n    sign = -1.0 if float(np.dot(g_x, vmin)) >= 0.0 else 1.0\n    d = sign * vmin  # unit direction (since evecs are normalized)\n    s = tr_radius\n    for _ in range(max_backtracks):\n        x_new = x + s * d\n        f_new = f_val(x_new)\n        if f_new < f_x:\n            return x_new, f_new, s\n        s *= beta\n    return x, f_x, 0.0\n\ndef hybrid_optimize(x0: np.ndarray,\n                    max_iters: int = 200,\n                    g_tol: float = 1e-3,\n                    kappa_pos: float = 1e-3,\n                    kappa_neg: float = 1e-8,\n                    tr_radius: float = 0.5,\n                    c1: float = 1e-4,\n                    beta: float = 0.5) -> float:\n    \"\"\"Run the hybrid global–local optimizer from x0 and return final objective value.\"\"\"\n    x = x0.copy().astype(float)\n    for _ in range(max_iters):\n        f_x = f_val(x)\n        g_x = grad_f(x)\n        H_x = hess_f(x)\n        grad_norm = float(np.linalg.norm(g_x))\n        # Compute smallest eigenvalue of Hessian\n        evals = np.linalg.eigvalsh(H_x)\n        lam_min = float(evals[0])\n\n        # Decide regime and take a step\n        if grad_norm <= g_tol and lam_min >= kappa_pos:\n            # Local Newton-type refinement\n            try:\n                # Newton direction\n                p = np.linalg.solve(H_x, -g_x)\n            except np.linalg.LinAlgError:\n                # Fallback to gradient descent if Hessian is singular\n                p = -g_x\n            # Armijo backtracking\n            x_new, f_new, alpha = armijo_line_search(x, f_x, g_x, p, c1=c1, alpha0=1.0, beta=beta)\n            if alpha == 0.0:\n                # If no progress, break early\n                break\n            x = x_new\n        elif (lam_min < -kappa_neg) or (grad_norm <= g_tol and lam_min <= kappa_pos):\n            # Global escape via negative curvature direction\n            x_new, f_new, s = neg_curvature_escape(x, f_x, g_x, H_x, tr_radius=tr_radius, beta=beta)\n            if s == 0.0:\n                # If escape step fails to decrease, fallback to gradient descent\n                p = -g_x\n                x_new, f_new, alpha = armijo_line_search(x, f_x, g_x, p, c1=c1, alpha0=1.0, beta=beta)\n                if alpha == 0.0:\n                    break\n            x = x_new\n        else:\n            # Default local descent\n            p = -g_x\n            x_new, f_new, alpha = armijo_line_search(x, f_x, g_x, p, c1=c1, alpha0=1.0, beta=beta)\n            if alpha == 0.0:\n                break\n            x = x_new\n    return f_val(x)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.0, 0.0]),\n        np.array([1.0, 1.0]),\n        np.array([0.0, 1.0]),\n        np.array([3.0, -3.0]),\n        np.array([0.57735, 0.57735]),\n    ]\n\n    results = []\n    for x0 in test_cases:\n        final_f = hybrid_optimize(x0,\n                                  max_iters=200,\n                                  g_tol=1e-3,\n                                  kappa_pos=1e-3,\n                                  kappa_neg=1e-8,\n                                  tr_radius=0.5,\n                                  c1=1e-4,\n                                  beta=0.5)\n        results.append(final_f)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}