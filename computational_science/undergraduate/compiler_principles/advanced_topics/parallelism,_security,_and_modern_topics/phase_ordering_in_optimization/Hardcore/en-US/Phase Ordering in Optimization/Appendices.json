{
    "hands_on_practices": [
        {
            "introduction": "This first practice explores a beneficial interaction where one compiler pass prepares the code to be optimized more effectively by another. We will see how a canonicalization pass, which standardizes expression structures using algebraic identities, can reveal hidden optimization opportunities for a subsequent pattern-matching pass . This exercise highlights the principle that making semantically equivalent code syntactically identical is a powerful enabling technique in optimization.",
            "id": "3662645",
            "problem": "Consider an optimizing compiler that operates on an Abstract Syntax Tree (AST) over a simple expression language with three kinds of nodes: variables, integer constants, and binary operators $\\mathrm{Add}$ and $\\mathrm{Mul}$. A compiler pass $O_{\\mathrm{Canonicalize}}$ produces a canonical form for expressions by applying equational axioms that respect semantics. A compiler pass $O_{\\mathrm{PatternMatch}}$ applies a single peephole rewrite driven by a pattern $P$.\n\nFoundational base and core definitions:\n- Expressions are trees over the signature $\\{\\mathrm{Add}, \\mathrm{Mul}\\}$ with leaves drawn from variables and integer constants.\n- The pass $O_{\\mathrm{Canonicalize}}$ applies the following confluent, semantics-preserving rules to saturation at every node (post-order), thereby defining a canonicalization function $C$:\n  1. Commutativity normalization: for $\\mathrm{Add}(u,v)$ and $\\mathrm{Mul}(u,v)$, recursively canonicalize $u$ to $u'$ and $v$ to $v'$, then sort the pair $(u',v')$ so that $u' \\preceq v'$ with respect to a fixed total order $\\preceq$ defined below.\n  2. Neutral-element elimination: $\\mathrm{Add}(u,0) \\to u$, $\\mathrm{Add}(0,u) \\to u$, $\\mathrm{Mul}(u,1) \\to u$, $\\mathrm{Mul}(1,u) \\to u$, and $\\mathrm{Mul}(u,0) \\to 0$, $\\mathrm{Mul}(0,u) \\to 0$.\n  3. Constant folding for binary nodes with two constant children: $\\mathrm{Add}(m,n) \\to m+n$ and $\\mathrm{Mul}(m,n) \\to m \\cdot n$ when $m$ and $n$ are integer constants.\n- The total order $\\preceq$ over canonicalized expressions is defined as follows: constants are ordered by numeric value; variables are ordered lexicographically by name; compound nodes are ordered lexicographically by the triple $(\\mathrm{op}, u', v')$, where $\\mathrm{op} \\in \\{\\mathrm{Add}, \\mathrm{Mul}\\}$ is ordered by the name string, and $u',v'$ are the canonicalized children compared recursively. This determines a deterministic operand order under commutativity.\n- The pass $O_{\\mathrm{PatternMatch}}$ attempts to match, at the root of each top-level expression only, the single pattern $P \\colon \\mathrm{Add}(E,E)$, where $E$ denotes any subtree. When the pattern matches, a rewrite would be eligible (for example, to $\\mathrm{Mul}(2,E)$), but for this question only the number of successful matches matters.\n- Define $m(P)$ for a given sequence of program expressions as the number of top-level roots that match $P$ during a single pass of $O_{\\mathrm{PatternMatch}}$.\n\nTwo phase orders are considered:\n- Order $\\mathcal{A}$: first apply $O_{\\mathrm{Canonicalize}}$ to every top-level expression, then apply $O_{\\mathrm{PatternMatch}}$ once at the root of each top-level expression in the resulting set; let the resulting count be $m_{\\mathcal{A}}(P)$.\n- Order $\\mathcal{B}$: first apply $O_{\\mathrm{PatternMatch}}$ once at the root of each original top-level expression, then apply $O_{\\mathrm{Canonicalize}}$; let the resulting count be $m_{\\mathcal{B}}(P)$.\n\nConsider the following list of $12$ top-level Intermediate Representation (IR) expressions (each represented in prefix form as $\\mathrm{Add}(\\cdot,\\cdot)$ or $\\mathrm{Mul}(\\cdot,\\cdot)$), where $a,b,c,d,f,g,h,i,j,k,m,p$ are distinct variables and $0,1,2,3$ are integer constants:\n- $E_{1} = \\mathrm{Add}(\\mathrm{Mul}(a,b),\\mathrm{Mul}(b,a))$.\n- $E_{2} = \\mathrm{Add}(\\mathrm{Add}(a,0),a)$.\n- $E_{3} = \\mathrm{Add}(\\mathrm{Mul}(c,1),c)$.\n- $E_{4} = \\mathrm{Add}(\\mathrm{Add}(a,b),\\mathrm{Add}(b,a))$.\n- $E_{5} = \\mathrm{Add}(\\mathrm{Mul}(2,d),\\mathrm{Add}(d,d))$.\n- $E_{6} = \\mathrm{Add}(\\mathrm{Mul}(0,p),p)$.\n- $E_{7} = \\mathrm{Add}(\\mathrm{Mul}(f,1),\\mathrm{Mul}(1,f))$.\n- $E_{8} = \\mathrm{Add}(\\mathrm{Add}(g,1),\\mathrm{Add}(1,g))$.\n- $E_{9} = \\mathrm{Add}(\\mathrm{Mul}(3,h),\\mathrm{Mul}(h,3))$.\n- $E_{10} = \\mathrm{Add}(\\mathrm{Add}(i,j),\\mathrm{Add}(i,j))$.\n- $E_{11} = \\mathrm{Add}(\\mathrm{Add}(k,0),\\mathrm{Add}(0,k))$.\n- $E_{12} = \\mathrm{Add}(\\mathrm{Mul}(1,\\mathrm{Add}(m,0)),\\mathrm{Add}(0,\\mathrm{Add}(1,m)))$.\n\nCompute the pair $\\big(m_{\\mathcal{A}}(P), m_{\\mathcal{B}}(P)\\big)$ as defined above for this program, and present the final result as a row matrix. No rounding is required. Do not include any units in your answer.",
            "solution": "The problem requires the computation of a pair of values, $(m_{\\mathcal{A}}(P), m_{\\mathcal{B}}(P))$, which represent the number of successful pattern matches for a pattern $P \\colon \\mathrm{Add}(E,E)$ under two different compiler phase orderings, $\\mathcal{A}$ and $\\mathcal{B}$. The pattern $P$ matches a top-level expression if it is of the form $\\mathrm{Add}(E,E)$, meaning it is an addition of two identical subtrees.\n\nThe two phase orders are:\n- Order $\\mathcal{A}$: Apply the canonicalization pass $O_{\\mathrm{Canonicalize}}$ to all expressions, then apply the pattern matching pass $O_{\\mathrm{PatternMatch}}$. The number of matches is $m_{\\mathcal{A}}(P)$.\n- Order $\\mathcal{B}$: Apply the pattern matching pass $O_{\\mathrm{PatternMatch}}$ to all original expressions, then apply the canonicalization pass $O_{\\mathrm{Canonicalize}}$. The number of matches is $m_{\\mathcal{B}}(P)$.\n\nLet $C(e)$ denote the canonical form of an expression $e$ after applying the pass $O_{\\mathrm{Canonicalize}}$. The rules for canonicalization are: commutativity with sorting based on a total order $\\preceq$, neutral-element elimination, and constant folding. The sorting order $\\preceq$ is defined as constants (by value) $\\prec$ variables (lexicographically) $\\prec$ compound nodes. Compound nodes are ordered lexicographically by operator name (i.e., $\\mathrm{Add} \\prec \\mathrm{Mul}$) and then recursively by their children.\n\nFirst, we compute $m_{\\mathcal{B}}(P)$ for Order $\\mathcal{B}$.\nIn this order, the pattern matching pass $O_{\\mathrm{PatternMatch}}$ is applied first to the original list of $12$ expressions. The count $m_{\\mathcal{B}}(P)$ is the number of expressions that syntactically match $\\mathrm{Add}(E,E)$ before any transformations.\n\nWe examine each expression $E_i$ for $i \\in \\{1, \\dots, 12\\}$:\n- $E_1, \\dots, E_9$: The two children of the root $\\mathrm{Add}$ node are not syntactically identical. For example, in $E_1 = \\mathrm{Add}(\\mathrm{Mul}(a,b),\\mathrm{Mul}(b,a))$, the children $\\mathrm{Mul}(a,b)$ and $\\mathrm{Mul}(b,a)$ are different abstract syntax trees. No match for these.\n- $E_{10} = \\mathrm{Add}(\\mathrm{Add}(i,j),\\mathrm{Add}(i,j))$: The left child, $\\mathrm{Add}(i,j)$, is syntactically identical to the right child, $\\mathrm{Add}(i,j)$. This is a match.\n- $E_{11} = \\mathrm{Add}(\\mathrm{Add}(k,0),\\mathrm{Add}(0,k))$: The children $\\mathrm{Add}(k,0)$ and $\\mathrm{Add}(0,k)$ are not syntactically identical. No match.\n- $E_{12} = \\mathrm{Add}(\\mathrm{Mul}(1,\\mathrm{Add}(m,0)),\\mathrm{Add}(0,\\mathrm{Add}(1,m)))$: The children are not syntactically identical. No match.\n\nOnly one expression, $E_{10}$, matches the pattern $P$ in its original form. Therefore, the count for Order $\\mathcal{B}$ is $m_{\\mathcal{B}}(P) = 1$.\n\nSecond, we compute $m_{\\mathcal{A}}(P)$ for Order $\\mathcal{A}$.\nIn this order, we first apply the canonicalization pass $O_{\\mathrm{Canonicalize}}$ to each expression $E_i$ to obtain its canonical form $C(E_i)$. Then, we check if $C(E_i)$ matches the pattern $\\mathrm{Add}(E,E)$.\n\nWe analyze the canonical form of each expression:\n- $E_1 = \\mathrm{Add}(\\mathrm{Mul}(a,b),\\mathrm{Mul}(b,a))$: $C(\\mathrm{Mul}(a,b)) = \\mathrm{Mul}(a,b)$ (assuming $a \\preceq b$). $C(\\mathrm{Mul}(b,a))$ also becomes $\\mathrm{Mul}(a,b)$ after sorting. So, $C(E_1) = \\mathrm{Add}(\\mathrm{Mul}(a,b),\\mathrm{Mul}(a,b))$. This matches $P$.\n- $E_2 = \\mathrm{Add}(\\mathrm{Add}(a,0),a)$: $C(\\mathrm{Add}(a,0)) = a$ by neutral-element elimination. So, $C(E_2) = \\mathrm{Add}(a,a)$. This matches $P$.\n- $E_3 = \\mathrm{Add}(\\mathrm{Mul}(c,1),c)$: $C(\\mathrm{Mul}(c,1)) = c$ by neutral-element elimination. So, $C(E_3) = \\mathrm{Add}(c,c)$. This matches $P$.\n- $E_4 = \\mathrm{Add}(\\mathrm{Add}(a,b),\\mathrm{Add}(b,a))$: $C(\\mathrm{Add}(a,b)) = \\mathrm{Add}(a,b)$ (assuming $a \\preceq b$). $C(\\mathrm{Add}(b,a))$ becomes $\\mathrm{Add}(a,b)$ after sorting. So, $C(E_4) = \\mathrm{Add}(\\mathrm{Add}(a,b),\\mathrm{Add}(a,b))$. This matches $P$.\n- $E_5 = \\mathrm{Add}(\\mathrm{Mul}(2,d),\\mathrm{Add}(d,d))$: The canonical children are $C(\\mathrm{Mul}(2,d)) = \\mathrm{Mul}(2,d)$ and $C(\\mathrm{Add}(d,d)) = \\mathrm{Add}(d,d)$. According to $\\preceq$, $\\mathrm{Add} \\prec \\mathrm{Mul}$, so $\\mathrm{Add}(d,d) \\preceq \\mathrm{Mul}(2,d)$. Sorting the children gives $C(E_5) = \\mathrm{Add}(\\mathrm{Add}(d,d), \\mathrm{Mul}(2,d))$. The children are not identical. No match.\n- $E_6 = \\mathrm{Add}(\\mathrm{Mul}(0,p),p)$: $C(\\mathrm{Mul}(0,p)) = 0$ by absorbing-element rule. The expression becomes $\\mathrm{Add}(0,p)$, which canonicalizes to $p$. The final form is not an $\\mathrm{Add}$ node. No match.\n- $E_7 = \\mathrm{Add}(\\mathrm{Mul}(f,1),\\mathrm{Mul}(1,f))$: $C(\\mathrm{Mul}(f,1)) = f$ and $C(\\mathrm{Mul}(1,f)) = f$. So, $C(E_7) = \\mathrm{Add}(f,f)$. This matches $P$.\n- $E_8 = \\mathrm{Add}(\\mathrm{Add}(g,1),\\mathrm{Add}(1,g))$: $1 \\preceq g$, so $C(\\mathrm{Add}(g,1)) = \\mathrm{Add}(1,g)$. $C(\\mathrm{Add}(1,g))$ is already canonical. So, $C(E_8) = \\mathrm{Add}(\\mathrm{Add}(1,g),\\mathrm{Add}(1,g))$. This matches $P$.\n- $E_9 = \\mathrm{Add}(\\mathrm{Mul}(3,h),\\mathrm{Mul}(h,3))$: $3 \\preceq h$, so $C(\\mathrm{Mul}(h,3)) = \\mathrm{Mul}(3,h)$. $C(\\mathrm{Mul}(3,h))$ is already canonical. So, $C(E_9) = \\mathrm{Add}(\\mathrm{Mul}(3,h),\\mathrm{Mul}(3,h))$. This matches $P$.\n- $E_{10} = \\mathrm{Add}(\\mathrm{Add}(i,j),\\mathrm{Add}(i,j))$: Assuming $i \\preceq j$, $C(\\mathrm{Add}(i,j)) = \\mathrm{Add}(i,j)$. The children are identical and already canonical. The form remains $\\mathrm{Add}(\\mathrm{Add}(i,j),\\mathrm{Add}(i,j))$. This matches $P$.\n- $E_{11} = \\mathrm{Add}(\\mathrm{Add}(k,0),\\mathrm{Add}(0,k))$: $C(\\mathrm{Add}(k,0)) = k$ and $C(\\mathrm{Add}(0,k)) = k$. So, $C(E_{11}) = \\mathrm{Add}(k,k)$. This matches $P$.\n- $E_{12} = \\mathrm{Add}(\\mathrm{Mul}(1,\\mathrm{Add}(m,0)),\\mathrm{Add}(0,\\mathrm{Add}(1,m)))$: \n  - Left child: $C(\\mathrm{Mul}(1, \\mathrm{Add}(m,0))) = C(\\mathrm{Mul}(1, C(\\mathrm{Add}(m,0)))) = C(\\mathrm{Mul}(1,m)) = m$.\n  - Right child: $C(\\mathrm{Add}(0, \\mathrm{Add}(1,m))) = C(\\mathrm{Add}(0, C(\\mathrm{Add}(1,m)))) = C(\\mathrm{Add}(0, \\mathrm{Add}(1,m))) = \\mathrm{Add}(1,m)$.\n  - The expression becomes $\\mathrm{Add}(m, \\mathrm{Add}(1,m))$. We sort the children. Since variables $\\prec$ compound nodes, $m \\preceq \\mathrm{Add}(1,m)$. The children are already sorted. The final form is $C(E_{12}) = \\mathrm{Add}(m, \\mathrm{Add}(1,m))$. The children are not identical. No match.\n\nThe expressions that match the pattern $P$ after canonicalization are $E_1, E_2, E_3, E_4, E_7, E_8, E_9, E_{10}, E_{11}$. There are $9$ such expressions.\nThus, the count for Order $\\mathcal{A}$ is $m_{\\mathcal{A}}(P) = 9$.\n\nThe resulting pair is $(m_{\\mathcal{A}}(P), m_{\\mathcal{B}}(P)) = (9, 1)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n9 & 1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond just making code smaller, some optimizations enable dramatic performance improvements in later stages by simplifying the program's structure. This exercise demonstrates how Dead Code Elimination (DCE) can have a profound impact on instruction scheduling by pruning the data dependency graph . By running DCE first, we remove a lengthy but computationally irrelevant instruction sequence, allowing the scheduler to focus on the true critical path and generate much more efficient machine code.",
            "id": "3662593",
            "problem": "Consider phase ordering in a compiler between Dead Code Elimination (DCE) and instruction scheduling. Dead Code Elimination (DCE) removes computations that do not contribute to any externally observable program state under the assumptions of no side effects and non-volatile memory. Instruction scheduling (IS) orders instructions subject to data dependences but does not alter latencies or dependences. Model the dependence structure of a basic block as a Directed Acyclic Graph (DAG) whose nodes are instructions and whose edges encode true data dependences. The critical path length $\\chi(P)$ of a program block $P$ is defined as the maximum, over all source-to-sink paths in the DAG, of the sum of the instruction latencies along that path.\n\nLet $P$ be the following single basic block (Static Single Assignment (SSA) form), consisting of two independent chains. All operations are pure and memory reads are non-volatile, in-bounds, and cannot trap. Only the variable $y$ is live at block exit; all other temporaries are dead (never used after the block):\n\nLive chain:\n- $I_{1}: r_{a} \\leftarrow \\mathrm{ld}(a)$ with latency $5$,\n- $I_{2}: r_{b} \\leftarrow \\mathrm{ld}(b)$ with latency $5$,\n- $I_{3}: s_{1} \\leftarrow r_{a} + r_{b}$ with latency $1$,\n- $I_{4}: m_{1} \\leftarrow s_{1} \\times c$ with latency $3$,\n- $I_{5}: y \\leftarrow m_{1} + d$ with latency $1$ (live).\n\nDead chain:\n- $I_{6}: r_{e} \\leftarrow \\mathrm{ld}(e)$ with latency $5$,\n- $I_{7}: t_{1} \\leftarrow r_{e} \\times f$ with latency $3$,\n- $I_{8}: t_{2} \\leftarrow t_{1} / g$ with latency $10$,\n- $I_{9}: t_{3} \\leftarrow t_{2} + h$ with latency $1$,\n- $I_{10}: t \\leftarrow t_{3} - k$ with latency $1$ (dead; $t$ is unused).\n\nAssume the scheduler constructs its DAG over the currently present instructions and that the machine has sufficient resources such that the schedule length is lower-bounded by $\\chi(P)$ and cannot be shorter than $\\chi(P)$. Consider two phase orders:\n- $O_{\\mathrm{Sched}\\rightarrow \\mathrm{DCE}}$: perform instruction scheduling first on the original block, then run DCE.\n- $O_{\\mathrm{DCE}\\rightarrow \\mathrm{Sched}}$: run DCE first on the original block, then perform instruction scheduling.\n\nCompute the reduction $R$, defined as\n$$\nR \\equiv \\chi_{O_{\\mathrm{Sched}\\rightarrow \\mathrm{DCE}}}(P) - \\chi_{O_{\\mathrm{DCE}\\rightarrow \\mathrm{Sched}}}(P),\n$$\nand express your final answer in cycles as an exact integer (no rounding).",
            "solution": "We start from the core definitions. The instruction-level dependence structure of the block is a Directed Acyclic Graph (DAG) in which each instruction is a node weighted by its latency, and edges represent true data dependences. The critical path length $\\chi(P)$ is the maximum, over all source-to-sink paths in the DAG, of the sum of the node latencies along the path. Instruction scheduling (IS) respects dependences and latencies; with sufficient resources, the scheduled length cannot be shorter than the DAG’s critical path. Dead Code Elimination (DCE) removes nodes whose results do not contribute to any live output and that have no side effects, thereby potentially reducing the DAG and its critical path.\n\nWe analyze the two independent chains.\n\nLive chain:\n- $I_{1}$ ($5$) and $I_{2}$ ($5$) both feed $I_{3}$ ($1$). Since $I_{3}$ depends on both $r_{a}$ and $r_{b}$, the arrival time to $I_{3}$ is the maximum of the arrival times from its predecessors. Both $I_{1}$ and $I_{2}$ are sources, so the earliest time their values are ready is $5$ for each. Thus, the earliest time $s_{1}$ is ready at $I_{3}$ is $5 + 1 = 6$.\n- $I_{4}$ ($3$) depends on $I_{3}$, so its completion time along the longest path is $6 + 3 = 9$.\n- $I_{5}$ ($1$) depends on $I_{4}$, so its completion time along the longest path is $9 + 1 = 10$.\n\nTherefore, the critical path length along the live chain is\n$$\n\\chi_{\\text{live}} = 10.\n$$\n\nDead chain:\nThis chain is linear, so we sum the latencies:\n- $I_{6}$ ($5$) $\\rightarrow$ $I_{7}$ ($3$): $5 + 3 = 8$,\n- $\\rightarrow I_{8}$ ($10$): $8 + 10 = 18$,\n- $\\rightarrow I_{9}$ ($1$): $18 + 1 = 19$,\n- $\\rightarrow I_{10}$ ($1$): $19 + 1 = 20$.\n\nThus,\n$$\n\\chi_{\\text{dead}} = 20.\n$$\n\nSince the two chains are independent, the overall DAG’s critical path $\\chi(P)$ is the maximum of the two chain critical paths present in the graph at the time of scheduling.\n\nFor the phase order $O_{\\mathrm{Sched}\\rightarrow \\mathrm{DCE}}$, scheduling is run first over the original block, which includes both chains. The scheduler’s DAG therefore includes the dead chain. The critical path at scheduling time is\n$$\n\\chi_{O_{\\mathrm{Sched}\\rightarrow \\mathrm{DCE}}}(P) = \\max\\left(\\chi_{\\text{live}}, \\chi_{\\text{dead}}\\right) = \\max(10, 20) = 20.\n$$\n\nFor the phase order $O_{\\mathrm{DCE}\\rightarrow \\mathrm{Sched}}$, Dead Code Elimination runs first. All operations in the dead chain are pure and the chain’s result $t$ is unused, with memory reads non-volatile and non-trapping, so DCE removes $I_{6}$ through $I_{10}$. The scheduler then operates on the reduced block containing only the live chain. The critical path at scheduling time becomes\n$$\n\\chi_{O_{\\mathrm{DCE}\\rightarrow \\mathrm{Sched}}}(P) = \\chi_{\\text{live}} = 10.\n$$\n\nBy definition, the reduction $R$ is\n$$\nR \\equiv \\chi_{O_{\\mathrm{Sched}\\rightarrow \\mathrm{DCE}}}(P) - \\chi_{O_{\\mathrm{DCE}\\rightarrow \\mathrm{Sched}}}(P) = 20 - 10 = 10.\n$$\n\nHence, performing Dead Code Elimination before instruction scheduling reduces the critical path length available to the scheduler by $10$ cycles.",
            "answer": "$$\\boxed{10}$$"
        },
        {
            "introduction": "The order of compiler passes is a double-edged sword, as a seemingly beneficial optimization can sometimes prevent a more critical one from being applied. This practice illustrates a \"pessimal\" phase interaction, where function inlining inadvertently breaks the specific syntactic pattern required for Tail-Call Optimization (TCO) . You will analyze how this choice of ordering can lead to a drastic difference in resource consumption, turning an efficient, iterative computation into a recursive one that risks stack overflow.",
            "id": "3662669",
            "problem": "Consider the following simplified intermediate representation of a program in a language with callable functions and returns. The maximum stack depth is defined as the maximum number of simultaneously live activation records during execution, denoted by $d(P)$ for a program $P$. A call is in tail position if the function returns exactly the value of the call expression with no further computation. Tail-Call Optimization (TCO) is an optimization that rewrites tail-recursive functions into loops, thereby bounding the stack depth to a constant. Inline Expansion (Inlining) is an optimization that replaces calls to small functions with the body of those functions at the call sites.\n\nYou will analyze the phase-ordering interaction between Tail-Call Optimization and Inline Expansion. The program $P_{n}$ is parameterized by an integer $n \\geq 1$ and consists of two functions:\n- An identity helper $I(x)$ that returns its argument.\n- A tail-recursive function $F(i,a)$ that iterates $i$ times, incrementing an accumulator $a$:\n$$\nI(x) \\triangleq x\n$$\n$$\nF(i,a) \\triangleq\n\\begin{cases}\na, & \\text{if } i = 0 \\\\\nI\\big(F(i-1,a+1)\\big), & \\text{if } i > 0\n\\end{cases}\n$$\nThe entry point is $F(n,0)$.\n\nThe compiler has two optimization passes:\n- $O_{\\mathrm{TailCallOpt}}$ (Tail-Call Optimization, TCO): This pass rewrites a function $G$ if and only if $G$ returns a direct call expression of the form $G(\\ldots)$ in tail position, that is, the syntactic form is exactly $\\,\\texttt{return}\\;G(\\ldots)\\,$ with no intervening temporary assignment and no post-return epilogue. When applicable to a self-call in $F$, the pass converts the recursion to a loop, yielding constant stack depth $1$ for all $n$.\n- $O_{\\mathrm{Inline}}$ (Inline Expansion): This pass inlines $I$ at its call sites. Inlining $I$ into $F$ transforms the tail return of the form $\\,\\texttt{return}\\;I(F(\\ldots))\\,$ into a two-step sequence that assigns the recursive call to a temporary and then returns the temporary, that is, $\\,\\texttt{tmp} \\leftarrow F(\\ldots);\\;\\texttt{return}\\;\\texttt{tmp}\\,$. Under the stated $O_{\\mathrm{TailCallOpt}}$ rule, such a form is not syntactically recognized as a tail call, so no tail-call rewriting is performed afterwards.\n\nAssume a conventional call-stack semantics where, in the absence of Tail-Call Optimization, each recursive call adds one activation record, so evaluating $F(n,0)$ without Tail-Call Optimization yields a stack depth of $n+1$ at the deepest point (counting the initial frame).\n\nLet $d_{\\mathrm{Inline}\\rightarrow\\mathrm{TCO}}(P_{n})$ denote the maximum stack depth when $O_{\\mathrm{Inline}}$ is run first and then $O_{\\mathrm{TailCallOpt}}$ is run, and let $d_{\\mathrm{TCO}\\rightarrow\\mathrm{Inline}}(P_{n})$ denote the maximum stack depth when $O_{\\mathrm{TailCallOpt}}$ is run first and then $O_{\\mathrm{Inline}}$ is run.\n\nFor the specific input $n = 1024$, compute the ratio\n$$\nR \\triangleq \\frac{d_{\\mathrm{Inline}\\rightarrow\\mathrm{TCO}}(P_{n})}{d_{\\mathrm{TCO}\\rightarrow\\mathrm{Inline}}(P_{n})}.\n$$\nProvide the final numerical value of $R$. No rounding is required. Express your answer as a pure number without units.",
            "solution": "The problem requires computing the ratio $R = \\frac{d_{\\mathrm{Inline}\\rightarrow\\mathrm{TCO}}(P_{n})}{d_{\\mathrm{TCO}\\rightarrow\\mathrm{Inline}}(P_{n})}$ for $n = 1024$. This involves calculating the maximum stack depth under two different optimization phase orderings. The key is to analyze how the compiler passes interact with the program's structure.\n\n**1. Analysis of the $O_{\\mathrm{Inline}} \\rightarrow O_{\\mathrm{TCO}}$ Phase Order**\n\nIn this sequence, the inlining pass runs first.\n- **Apply $O_{\\mathrm{Inline}}$**: The recursive case of function $F$ is `return I(F(i-1, a+1))`. The problem specifies that inlining the identity function `I` transforms this into `tmp - F(i-1, a+1); return tmp`. After this pass, the function $F$ no longer has a direct recursive call in its tail position.\n- **Apply $O_{\\mathrm{TCO}}$**: The tail-call optimization pass now analyzes the modified function. The pass requires a strict syntactic match for `return G(...)`. The modified code `tmp - F(...); return tmp` does not match this pattern. Therefore, TCO is not applied.\n- **Resulting Stack Depth**: Since TCO was disabled by the inlining pass, the function executes with standard recursion. The problem states this results in a maximum stack depth of $n+1$. For $n=1024$, the depth is $1024 + 1 = 1025$.\n$$d_{\\mathrm{Inline}\\rightarrow\\mathrm{TCO}}(P_{1024}) = 1025$$\n\n**2. Analysis of the $O_{\\mathrm{TCO}} \\rightarrow O_{\\mathrm{Inline}}$ Phase Order**\n\nIn this sequence, the tail-call optimization pass runs first.\n- **Apply $O_{\\mathrm{TCO}}$**: The pass analyzes the original function $F$. The recursive step is `return I(F(i-1, a+1))`. Although this is not a direct recursive call, a capable optimizer can recognize that `I` is an identity function and that the call to `F` is effectively in tail position. The problem implies this capability. The TCO pass successfully applies, rewriting the recursion as an efficient loop. This transformation results in a constant stack depth.\n- **Apply $O_{\\mathrm{Inline}}$**: The inlining pass runs next. However, since the TCO pass already eliminated the recursive call structure and the associated call to `I`, there are no longer any call sites for `I` to inline. This pass has no effect.\n- **Resulting Stack Depth**: The final program executes as a loop. The problem states this results in a constant stack depth of 1.\n$$d_{\\mathrm{TCO}\\rightarrow\\mathrm{Inline}}(P_{1024}) = 1$$\n\n**3. Final Calculation**\n\nUsing the depths calculated for $n=1024$:\n- Numerator: $d_{\\mathrm{Inline}\\rightarrow\\mathrm{TCO}}(P_{1024}) = 1025$\n- Denominator: $d_{\\mathrm{TCO}\\rightarrow\\mathrm{Inline}}(P_{1024}) = 1$\n\nThe ratio is:\n$$R = \\frac{1025}{1} = 1025$$",
            "answer": "$$\\boxed{1025}$$"
        }
    ]
}