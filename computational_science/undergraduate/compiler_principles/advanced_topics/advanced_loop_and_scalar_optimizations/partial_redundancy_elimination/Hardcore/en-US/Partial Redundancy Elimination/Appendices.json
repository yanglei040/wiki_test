{
    "hands_on_practices": [
        {
            "introduction": "We begin our exploration of Partial Redundancy Elimination (PRE) with a foundational scenario. This exercise presents a classic 'diamond' control-flow graph where an identical computation occurs on both branches. By analyzing the proposal to hoist this computation to the common dominating block, you will engage with the core principles of PRE, including semantic safety and profitability, and learn to weigh the performance gains against potential costs like increased code size and register pressure .",
            "id": "3661827",
            "problem": "You are given a control-flow graph (CFG) with entry block $B_0$ that immediately branches to two disjoint blocks $B_1$ and $B_2$, each of which then flows into a common successor $B_3$. The program within these blocks is described as follows, where all variables denote integer registers and $\\oplus$ denotes bitwise exclusive OR.\n\n- In $B_0$, both operands $x$ and $y$ are defined and neither $x$ nor $y$ is modified in $B_1$, $B_2$, or along the edges from $B_0$ to $B_1$ or $B_2$. The conditional branch in $B_0$ is determined by a predicate $p$:\n  - In $B_0$: $x := \\text{def}_x$, $y := \\text{def}_y$, and then if $p$ branch to $B_1$ else branch to $B_2$.\n- In $B_1$, there is a use of the expression $x \\oplus y$ that assigns to a local name $u$:\n  - In $B_1$: $u := x \\oplus y$; control then goes to $B_3$.\n- In $B_2$, there is a use of the same expression $x \\oplus y$ that assigns to a local name $v$:\n  - In $B_2$: $v := x \\oplus y$; control then goes to $B_3$.\n- In $B_3$, both $u$ and $v$ may be live depending on subsequent uses.\n\nAssume there are no side effects in evaluating $x$, $y$, or $x \\oplus y$, and that the bitwise exclusive OR operator $\\oplus$ is pure (it does not read or write memory beyond its operands and has no observable effects other than producing its result). Consider applying Partial Redundancy Elimination (PRE), where PRE is defined as the optimization that removes computations that are redundant on some but not necessarily all paths by relocating computations to earlier dominating points and inserting compensation code to preserve correctness, based on dataflow properties such as availability, anticipatability, and transparency.\n\nA proposed PRE transformation introduces a temporary $t$ in $B_0$ to compute $t := x \\oplus y$ once, and then replaces the computations in $B_1$ and $B_2$ with moves $u := t$ and $v := t$, respectively.\n\nWhich of the following statements about the correctness and profitability of this PRE transformation are true?\n\nA. Hoisting $t := x \\oplus y$ into $B_0$ is semantically safe under the stated conditions because $x$ and $y$ are available in $B_0$ and the expression $x \\oplus y$ is transparent along all paths from $B_0$ to its uses in $B_1$ and $B_2$.\n\nB. This transformation strictly reduces code size because it replaces two instances of $x \\oplus y$ with one instance in $B_0$ and does not require any additional instructions.\n\nC. This transformation can increase register pressure and code size due to the insertion of the copies $u := t$ and $v := t$; its profitability depends on a cost model comparing the cost of one computation plus copies against the cost of two computations.\n\nD. If either $x$ or $y$ were redefined on one path between $B_0$ and the corresponding use of $x \\oplus y$, PRE would need to insert compensation code or avoid hoisting to preserve correctness, because $x \\oplus y$ would not be transparent along that path.\n\nE. Hoisting $x \\oplus y$ to $B_0$ could change observable behavior if $x \\oplus y$ had side effects; therefore, PRE must be restricted to pure expressions without side effects, which applies to the given $\\oplus$.\n\nSelect all correct options.",
            "solution": "The user wants a critical validation of the problem statement, followed by a solution and evaluation of the provided options.\n\n### Step 1: Extract Givens\n\n-   **Control-Flow Graph (CFG)**: An entry block $B_0$ branches to two disjoint blocks, $B_1$ and $B_2$. Both $B_1$ and $B_2$ have a common successor, $B_3$. This forms a diamond-shaped CFG structure.\n-   **Block $B_0$ Semantics**:\n    -   Operands $x$ and $y$ are defined: $x := \\text{def}_x$, $y := \\text{def}_y$.\n    -   A conditional branch based on a predicate $p$: if $p$ branch to $B_1$, else branch to $B_2$.\n-   **Path Properties ($B_0 \\to B_1$, $B_0 \\to B_2$)**: Neither $x$ nor $y$ is modified in $B_1$, $B_2$, or on the edges connecting them from $B_0$.\n-   **Block $B_1$ Semantics**: Contains the computation $u := x \\oplus y$, where $u$ is a local name. Control flows to $B_3$.\n-   **Block $B_2$ Semantics**: Contains the computation $v := x \\oplus y$, where $v$ is a local name. Control flows to $B_3$.\n-   **Block $B_3$ Semantics**: Both $u$ and $v$ may be live upon entry to $B_3$.\n-   **Operator Properties**: The operator $\\oplus$ is bitwise exclusive OR. It is specified as pure, meaning it has no side effects.\n-   **Optimization Definition**: Partial Redundancy Elimination (PRE) is defined as an optimization that removes computations redundant on some, but not all, paths, by relocating computations and inserting compensation code. It is based on dataflow properties like availability, anticipatability, and transparency.\n-   **Proposed Transformation**:\n    1.  Insert $t := x \\oplus y$ into block $B_0$.\n    2.  Replace $u := x \\oplus y$ in $B_1$ with $u := t$.\n    3.  Replace $v := x \\oplus y$ in $B_2$ with $v := t$.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is firmly rooted in the principles of compiler design and optimization, specifically dataflow analysis and code transformation. The concepts of CFG, basic blocks, liveness, dataflow properties (availability, anticipatability), and PRE are standard in this field. The scenario described is a canonical example used to teach PRE and related optimizations. The problem is scientifically sound.\n-   **Well-Posed**: The problem provides a clear and complete description of the program structure and the proposed transformation. The question asks for an evaluation of several statements regarding the correctness and profitability of this transformation, which is a well-defined task. A unique analysis is possible based on the provided information.\n-   **Objective**: The problem is stated in precise, objective language. The terms used ($x$, $y$, $B_0$, $\\oplus$, PRE) are either defined or standard within the context of computer science. There are no subjective or opinion-based claims in the problem setup.\n-   **Other Flaws**: The problem is not trivial, contradictory, or unrealistic within its domain. It presents a standard trade-off analysis common in compiler optimization.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. I will proceed with the analysis and solution.\n\n### Derivation and Option Analysis\n\nThe problem describes a classic case for code motion, often handled by Partial Redundancy Elimination (PRE) algorithms. The expression $e = x \\oplus y$ is computed on all paths exiting the basic block $B_0$. In dataflow analysis terms, this means the expression $e$ is *anticipatable* (or *very busy*) at the exit of $B_0$. This is the key property that allows the computation to be hoisted into $B_0$ without introducing the computation on a path that did not previously contain it.\n\nLet's analyze the proposed transformation:\n-   **Before**: The computation $x \\oplus y$ is performed in $B_1$ if predicate $p$ is true, or in $B_2$ if $p$ is false. In any single execution of this code, exactly one computation of $x \\oplus y$ occurs. There are two static instances of the computation in the source code.\n-   **After**: The computation $t := x \\oplus y$ is performed unconditionally in $B_0$. Then, a copy (move) operation is performed in either $B_1$ ($u := t$) or $B_2$ ($v := t$). In any single execution, one computation of $x \\oplus y$ and one copy operation occur.\n\nNow we evaluate each statement.\n\n**A. Hoisting $t := x \\oplus y$ into $B_0$ is semantically safe under the stated conditions because $x$ and $y$ are available in $B_0$ and the expression $x \\oplus y$ is transparent along all paths from $B_0$ to its uses in $B_1$ and $B_2$.**\n\nFor the hoisting transformation to be semantically safe (i.e., to preserve the meaning of the program), two conditions must be met:\n1.  The operands of the hoisted expression must be available at the new location. Here, $x$ and $y$ are defined in $B_0$, so they are available for the computation $t := x \\oplus y$ in $B_0$.\n2.  The value computed must be identical to the value that would have been computed at the original locations. This requires that the operands ($x$ and $y$) are not redefined on any path between the new location and the original locations. This property is called *transparency*. The problem statement explicitly guarantees this: \"neither $x$ nor $y$ is modified in $B_1$, $B_2$, or along the edges from $B_0$ to $B_1$ or $B_2$\".\n\nSince both conditions are met, the hoisting is semantically safe. The reasoning provided in the statement is precisely correct.\nVerdict: **Correct**.\n\n**B. This transformation strictly reduces code size because it replaces two instances of $x \\oplus y$ with one instance in $B_0$ and does not require any additional instructions.**\n\nLet's analyze the static instruction count.\n-   **Before**: One computation instruction in $B_1$ ($u := x \\oplus y$) and one in $B_2$ ($v := x \\oplus y$). Total: $2$ instructions.\n-   **After**: One computation instruction in $B_0$ ($t := x \\oplus y$), one copy instruction in $B_1$ ($u := t$), and one copy instruction in $B_2$ ($v := t$). Total: $3$ instructions.\nThe transformation replaces two computation instructions with one computation and two copy instructions. The claim that it \"does not require any additional instructions\" is false; it adds two copy instructions while removing one computation. The static code size increases from $2$ to $3$ instructions (assuming each operation corresponds to one instruction of similar size).\nVerdict: **Incorrect**.\n\n**C. This transformation can increase register pressure and code size due to the insertion of the copies $u := t$ and $v := t$; its profitability depends on a cost model comparing the cost of one computation plus copies against the cost of two computations.**\n\nLet's break this statement down.\n1.  **Code Size**: As established in the analysis of option B, the static instruction count increases from $2$ to $3$. So, the transformation can (and does) increase code size.\n2.  **Register Pressure**: The new temporary variable $t$ is defined in $B_0$ and is live until its last use in $B_1$ and $B_2$. This means the value of $t$ must be preserved across the conditional branch. This increases the number of live variables on the control-flow edges from $B_0$ to $B_1$ and $B_0$ to $B_2$, thereby increasing register pressure. This could potentially lead to more register spills if the machine is low on registers.\n3.  **Profitability**: Whether an optimization is \"worth it\" (profitable) depends on a cost model. A compiler must weigh the benefits against the drawbacks. For this transformation, the dynamic path cost changes.\n    -   Cost before: $C(B_0) + C(B_{1 \\text{ or } 2}) = C_{B_0} + C_{x \\oplus y}$.\n    -   Cost after: $C'(B_0) + C'(B_{1 \\text{ or } 2}) = (C_{B_0} + C_{x \\oplus y}) + C_{copy}$.\n    In terms of execution speed, this transformation adds the cost of a copy instruction to every execution path, so it appears unprofitable. However, the statement itself doesn't claim it *is* profitable. It correctly states that its profitability *depends* on a cost model. The phrasing \"comparing the cost of one computation plus copies against the cost of two computations\" refers to the static trade-off ($1$ XOR and $2$ MOVs vs $2$ XORs), which is how a cost model might be framed. The overall statement correctly identifies the downsides and the need for a cost-benefit analysis.\nVerdict: **Correct**.\n\n**D. If either $x$ or $y$ were redefined on one path between $B_0$ and the corresponding use of $x \\oplus y$, PRE would need to insert compensation code or avoid hoisting to preserve correctness, because $x \\oplus y$ would not be transparent along that path.**\n\nThis statement posits a hypothetical modification to the program. Let's say $x$ is redefined on the path from $B_0$ to $B_1$.\n-   The value of $x$ at the computation site in $B_1$ would be different from the value of $x$ at the end of $B_0$.\n-   Hoisting $t := x \\oplus y$ to $B_0$ would compute the expression using the value of $x$ from $B_0$.\n-   Using this value of $t$ in $B_1$ would be semantically incorrect, as it would not reflect the redefinition of $x$.\n-   The reason for this failure is precisely that the path is no longer *transparent* for the expression $x \\oplus y$.\n-   A PRE algorithm must ensure correctness. It would detect the lack of transparency and would have to react. The two main strategies are: (1) **avoid hoisting** the expression altogether, since the safety condition is violated, or (2) in more complex scenarios, insert a re-computation of the expression (so-called **compensation code**) after the operand is redefined. This statement accurately describes the situation and the possible actions a correct PRE implementation would take.\nVerdict: **Correct**.\n\n**E. Hoisting $x \\oplus y$ to $B_0$ could change observable behavior if $x \\oplus y$ had side effects; therefore, PRE must be restricted to pure expressions without side effects, which applies to the given $\\oplus$.**\n\nCode motion optimizations like PRE must be careful not to alter program semantics.\n-   **Side Effects**: If an expression causes a side effect (e.g., modifies memory, performs I/O, throws an exception), moving it can change the program's behavior. In a more general PRE case, an expression might be moved to a path where it was not originally executed, thereby incorrectly introducing a side effect. Even in this problem's specific case where the expression is on all paths, moving it from inside the branches ($B_1$, $B_2$) to before the branch ($B_0$) changes *when* the side effect occurs relative to other program operations, which is an observable change.\n-   **Exceptions**: If $x \\oplus y$ could raise an exception (e.g., if it were $x/y$ and $y$ could be $0$), hoisting it to $B_0$ could cause the program to fault on a path that would not have faulted in the original code. For the hoisting to be safe, the exception must be guaranteed to occur on every path from the hoist point, which is true in this case, but not in general for PRE.\n-   **Purity**: To avoid these issues, code motion optimizations like PRE are conservatively restricted to *pure* expressions that are guaranteed not to have side effects or raise spurious exceptions. The problem correctly states that the bitwise XOR operator $\\oplus$ is pure, making it a valid candidate for this optimization. The reasoning presented in the statement is a fundamental principle of safe compiler optimization.\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ACDE}$$"
        },
        {
            "introduction": "Real-world programs often contain 'unsafe' expressions, such as division, which can raise exceptions. This practice challenges you to apply PRE to an expression like $x/y$ while preserving precise exception semantics. You will discover that safely optimizing such code requires hoisting not only the computation but also its protective guard, a technique that allows compilers to eliminate redundancy without introducing new errors . This demonstrates a crucial principle: correctness must always guide optimization.",
            "id": "3661859",
            "problem": "A program in a Control-Flow Graph (CFG) has blocks $B_0 \\rightarrow B_2$ with a conditional split at $B_2$ based on predicate $p$ into two normal-path blocks $B_4$ and $B_5$, both of which can reach a common join block $J$. There is a rare error handler block $E$ that should be reached if and only if $y=0$. The current fragment in both normal-path branches is:\n- In $B_4$: if $y=0$ then goto $E$ else compute $t_1 := x/y$ and use $t_1$; then goto $J$.\n- In $B_5$: if $y=0$ then goto $E$ else compute $t_2 := x/y$ and use $t_2$; then goto $J$.\n\nAssume integer division, where dividing by zero must trigger the error handler $E$ (i.e., the language has precise exception semantics for division by zero and no other side effects on these paths). Also assume that $x$ and $y$ are not modified along the paths $B_2 \\rightarrow B_4$ and $B_2 \\rightarrow B_5$ and are defined before $B_2$, and that $y$ can be zero only on the rare error path. The goal is to apply Partial Redundancy Elimination (PRE) to the expression $x/y$ so that it is computed exactly once on all normal paths, while preserving the programâ€™s original exception behavior and minimizing guard checks.\n\nFrom the fundamental definitions used in compiler optimization:\n- Redundancy of an expression means the expression is computed multiple times along some paths where its operands have not changed.\n- Availability of an expression at a program point means all paths to that point compute the expression, and the operands have not changed since the last computation.\n- Anticipability of an expression at a program point means the expression will be used along all paths from that point before any of its operands are redefined.\n- Dominance in a CFG means a node $D$ dominates a node $N$ if every path from the entry to $N$ goes through $D$.\n- For unsafe expressions (like $x/y$), computing the expression is safe only in regions where a guard guarantees that its evaluation will not raise an exception; otherwise, speculative motion may violate precise exception semantics.\n\nWhich of the following transformations are correct under these principles and meet the stated goals?\n\nA. Introduce a new block $G$ immediately after $B_2$ (so that $G$ dominates both $B_4$ and $B_5$). In $G$, perform a single guard and computation: if $y=0$ then goto $E$ else compute $t := x/y$. Redirect both edges $B_2 \\rightarrow B_4$ and $B_2 \\rightarrow B_5$ through $G$, eliminate the per-branch guards in $B_4$ and $B_5$, and replace $t_1 := x/y$ and $t_2 := x/y$ by uses of $t$. Keep the uses unchanged. No other changes.\n\nB. Insert $t := x/y$ speculatively in $B_2$ without any guard, remove the per-branch guards in $B_4$ and $B_5$, and replace $t_1 := x/y$ and $t_2 := x/y$ by uses of $t$. Leave $E$ reachable only if some later code attempts division-by-zero as before.\n\nC. Insert a guard in a new dominator block $G$ after $B_2$ that sends control to $E$ if $y=0$ and otherwise passes to $B_4$ or $B_5$ unchanged. In addition, precompute $t := x/y$ in $G$ and replace $t_1 := x/y$ and $t_2 := x/y$ by uses of $t$, but retain the original per-branch guards in $B_4$ and $B_5$ to be conservative.\n\nD. Hoist only the guard into a new block $G$ after $B_2$ that sends control to $E$ if $y=0$ and otherwise forwards to $B_4$ and $B_5$, but leave $t_1 := x/y$ in $B_4$ and $t_2 := x/y$ in $B_5$ as they are, with their computations intact.\n\nSelect the option(s) that produce a program that is semantically equivalent to the original (with respect to exception behavior and values), compute $x/y$ exactly once on all normal paths, and minimize the number of guard checks required.",
            "solution": "### Step 1: Extract Givens\nI'll list all the facts and constraints from the problem description.\n*   CFG structure: $B_0 \\rightarrow B_2$.\n*   Conditional split at $B_2$ based on predicate $p$.\n*   Split leads to two normal-path blocks $B_4$ and $B_5$.\n*   $B_4$ and $B_5$ can both reach a common join block $J$.\n*   Error handler block $E$ is reached if and only if $y=0$.\n*   Original code in $B_4$: `if y=0 then goto E else compute t1 := x/y and use t1; then goto J`.\n*   Original code in $B_5$: `if y=0 then goto E else compute t2 := x/y and use t2; then goto J`.\n*   Assumption: Integer division.\n*   Assumption: Division by zero must trigger error handler $E$ (precise exception semantics).\n*   Assumption: $x$ and $y$ are not modified along paths $B_2 \\rightarrow B_4$ and $B_2 \\rightarrow B_5$.\n*   Assumption: $x$ and $y$ are defined before $B_2$.\n*   Assumption: $y$ can be zero only on the rare error path.\n*   Goal: Apply PRE to $x/y$.\n*   Goal requirements:\n    1.  Compute $x/y$ exactly once on all normal paths.\n    2.  Preserve original exception behavior.\n    3.  Minimize guard checks.\n*   Definitions provided: Redundancy, Availability, Anticipability, Dominance, Safety for unsafe expressions.\n\n### Step 2: Validate Using Extracted Givens\nI'll check the problem against my validation criteria.\n*   **Scientifically Grounded?**: Yes, this is a standard problem in compiler theory, specifically data-flow analysis and code motion for PRE. The concepts (CFG, dominance, PRE, precise exceptions) are well-established.\n*   **Well-Posed?**: Yes, the problem describes a specific code structure and transformation goal, with clear constraints. A solution can be reasoned about.\n*   **Objective?**: Yes, the language is precise and technical. No subjective elements.\n*   **Flaw Check**:\n    1.  **Scientific/Factual Unsoundness?**: No. The setup is consistent with compiler principles.\n    2.  **Non-Formalizable?**: No. It's a formal problem about CFG transformations.\n    3.  **Incomplete/Contradictory?**: The assumptions are clear and sufficient. \"The current fragment...\" implies this is a part of a larger program, but the analysis is scoped to this fragment. The assumption \"$y$ can be zero only on the rare error path\" might seem a bit strong, but it's consistent with the goal of sending control to $E$ only when $y=0$. The problem seems self-consistent.\n    4.  **Unrealistic/Infeasible?**: No. This is a very common optimization scenario.\n    5.  **Ill-Posed?**: The goal is to find the *correct* transformation that meets multiple criteria. This is a standard optimization problem. A unique best transformation among the options is expected.\n    6.  **Trivial/Tautological?**: No. It requires understanding the interplay between code motion, redundancy, and exception semantics, which is a core concept in PRE for unsafe expressions.\n    7.  **Unverifiable?**: No. The correctness of the transformation can be formally verified by analyzing the resulting CFG's semantics.\n\n### Step 3: Verdict and Action\nThe problem is **Valid**. I will proceed to the solution.\n\n### Solution Derivation\n*   **Analyze the initial state**:\n    *   The expression `$x/y$` is computed in both branches ($B_4$ and $B_5$) that emerge from the split at $B_2$.\n    *   Since $x$ and $y$ are not modified on these paths, the expression `$x/y$` is partially redundant. It's not fully redundant because it's not computed on the path leading *into* $B_2$.\n    *   The expression `$x/y$` is *unsafe* because division by zero is a possibility.\n    *   The original program correctly handles this by guarding each computation with `if y=0`.\n    *   The expression `$x/y$` is *anticipated* at the exit of $B_2$ (and entry to $B_4$ and $B_5$) because it's used along all paths ($B_4 \\rightarrow J$ and $B_5 \\rightarrow J$) before $x$ or $y$ are redefined.\n    *   The goal of PRE is to move the computation to an earlier point to eliminate the redundancy. The ideal point would be a block that dominates both $B_4$ and $B_5$.\n\n*   **Apply PRE principles for unsafe expressions**:\n    *   Standard PRE would hoist the computation `$t := x/y$` to a common dominator.\n    *   However, `$x/y$` is unsafe. Hoisting it speculatively (i.e., before an original computation point) can introduce new exceptions. For example, moving `$t := x/y$` to $B_2$ would cause a division-by-zero exception if $y=0$, even if the original program path would not have executed the division.\n    *   The problem states the split at $B_2$ leads to $B_4$ and $B_5$. So, the paths are $B_2 \\rightarrow B_4 \\rightarrow J$ and $B_2 \\rightarrow B_5 \\rightarrow J$.\n    *   To perform PRE safely, we cannot just move the computation. We must also ensure that the moved computation is guarded. This is called \"safe PRE\" or \"speculation control\".\n    *   The original code has a guard `if y=0` in both $B_4$ and $B_5$. This guard is also redundant.\n    *   The ideal transformation would hoist both the computation and the guard.\n\n*   **Formulating the ideal transformation**:\n    1.  To hoist the computation and have it execute only once, it must be placed in a block that dominates both $B_4$ and $B_5$. Since the split occurs at $B_2$, the common dominator is $B_2$. To place code \"after\" $B_2$ but \"before\" $B_4$ and $B_5$, a new block $G$ must be introduced, effectively refactoring the control flow.\n    2.  The resulting control flow would be: $... \\rightarrow B_2 \\rightarrow G$.\n    3.  Block $G$ would contain the hoisted, common code. First, the guard for the unsafe expression: `if y = 0 then goto E`. This preserves the exception semantics and meets the goal of minimizing checks by using a single guard.\n    4.  In the `else` path of this guard, it is now safe to compute `$t := x/y$`.\n    5.  Following the computation, the original control flow must be restored. Block $G$ would then execute the conditional branch: `if p then goto B4' else goto B5'`, where $B_4'$ and $B_5'$ are the modified versions of the original blocks.\n    6.  In $B_4'$ and $B_5'$, the original guards and computations (`if y=0...`, `t1 := x/y`, `t2 := x/y`) are removed. The uses of $t_1$ and $t_2$ are replaced by uses of the hoisted temporary variable $t$.\n    7.  This transformation satisfies all stated goals: `$x/y$` is computed exactly once on normal paths (in $G$); exception behavior is identical to the original; and the number of guard checks is minimized from two to one.\n\n### Option-by-Option Analysis\n\n*   **Option A**: \"Introduce a new block $G$ immediately after $B_2$ (so that $G$ dominates both $B_4$ and $B_5$). In $G$, perform a single guard and computation: if $y=0$ then goto $E$ else compute $t := x/y$. Redirect both edges $B_2 \\rightarrow B_4$ and $B_2 \\rightarrow B_5$ through $G$, eliminate the per-branch guards in $B_4$ and $B_5$, and replace $t_1 := x/y$ and $t_2 := x/y$ by uses of $t$. Keep the uses unchanged. No other changes.\"\n    *   This option describes the ideal transformation derived above. A new block $G$ is inserted, which dominates the subsequent blocks. The phrase \"Redirect both edges... through G\" is a slightly informal but common way to describe the refactoring where a new block is inserted to hold code common to several subsequent paths. The actions described are:\n        1. Hoist the guard and computation into the new dominator block $G$.\n        2. Compute `$x/y$` only once.\n        3. Eliminate the now-redundant guards and computations in the original branches.\n        4. Use the new temporary variable $t$.\n    *   This transformation correctly implements safe Partial Redundancy Elimination. It computes `$x/y$` exactly once, preserves the precise exception semantics by hoisting the guard along with the computation, and minimizes the number of guard checks to one.\n    *   **Verdict**: **Correct**. This option accurately describes the desired optimization.\n\n*   **Option B**: \"Insert $t := x/y$ speculatively in $B_2$ without any guard, remove the per-branch guards in $B_4$ and $B_5$, and replace $t_1 := x/y$ and $t_2 := x/y$ by uses of $t$. Leave $E$ reachable only if some later code attempts division-by-zero as before.\"\n    *   This transformation moves the computation `$t := x/y$` to block $B_2$ but removes the protective guard. This is speculative code motion.\n    *   This is incorrect because `$x/y$` is an unsafe expression. If the program enters $B_2$ with $y=0$, this new code will execute `$t := x/y$` and raise an exception. The original program would not have raised an exception at this point, as the division was guarded.\n    *   This violates the constraint of preserving the program's original exception behavior (\"precise exception semantics\"). Introducing a new, unwanted exception is a critical semantic change. The statement \"Leave $E$ reachable...\" is also violated, as the explicit `goto E` is removed and replaced with a runtime-system-level fault.\n    *   **Verdict**: **Incorrect**.\n\n*   **Option C**: \"Insert a guard in a new dominator block $G$ after $B_2$... In addition, precompute $t := x/y$ in $G$ and replace $t_1 := x/y$ and $t_2 := x/y$ by uses of $t$, but retain the original per-branch guards in $B_4$ and $B_5$ to be conservative.\"\n    *   This transformation correctly precomputes the value of `$x/y$` in a new dominating block $G$ after performing a safety check. It computes `$x/y$` once and preserves exception behavior.\n    *   However, it explicitly states that the original guards in $B_4$ and $B_5$ are retained. Once control passes the guard in $G$, it is known that $y \\neq 0$. Therefore, the guards `if y=0` in blocks $B_4$ and $B_5$ are redundant, as their conditions will always evaluate to false.\n    *   Retaining these guards violates the stated goal to \"minimize guard checks\". The resulting program has three guard checks (`1` in $G$, `1` in $B_4$, `1` in $B_5$), whereas the optimal solution (Option A) has only one.\n    *   **Verdict**: **Incorrect**. This transformation is semantically safe but suboptimal and fails to meet all the stated goals.\n\n*   **Option D**: \"Hoist only the guard into a new block $G$ after $B_2$ that sends control to $E$ if $y=0$ and otherwise forwards to $B_4$ and $B_5$, but leave $t_1 := x/y$ in $B_4$ and $t_2 := x/y$ in $B_5$ as they are, with their computations intact.\"\n    *   This transformation only eliminates the redundancy of the guard `if y=0`. It hoists the check into a common dominator block $G$.\n    *   However, it fails to address the redundancy of the computation `$x/y$`. The problem statement explicitly says to \"leave $t_1 := x/y$ in $B_4$ and $t_2 := x/y$ in $B_5$ as they are\".\n    *   This violates the primary goal of applying Partial Redundancy Elimination to the expression `$x/y$`, which is to \"compute $x/y$ exactly once on all normal paths\". In this transformed program, the computation is still performed twice.\n    *   **Verdict**: **Incorrect**.\n\nBased on the analysis, only Option A correctly describes a transformation that satisfies all the given constraints and goals.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Sometimes, the structure of a program's control flow can block otherwise valid optimizations. This advanced problem illustrates a scenario where PRE is prevented because hoisting the computation would lead to forbidden speculative execution. The solution lies not in a simple code motion but in a structural transformation known as code cloning, or tail duplication, which reshapes the control-flow graph to create new optimization opportunities . This exercise reveals the sophisticated interplay between different compiler techniques and how restructuring code can be a prerequisite for eliminating redundancy.",
            "id": "3661855",
            "problem": "A compiler wants to apply partial redundancy elimination (PRE) of the expression $x+y$ in a program that uses Static Single Assignment (SSA) and is represented as a Control Flow Graph (CFG). The program uses integer arithmetic with overflow checks; evaluating $x+y$ may raise an overflow exception and therefore non-speculative PRE is required: the compiler must not introduce evaluation of $x+y$ on control-flow paths where the original program does not evaluate $x+y$.\n\nConsider the following CFG with basic blocks $B_0$, $B_1$, $B_2$, $J$, $S$, $U$, $N$, and $E$:\n\n- $B_0$: initializes $x := a$, $y := b$, then branches on predicate $p$ to $B_1$ or $B_2$.\n- $B_1$: computes $t := x + y$ and then goes to $J$.\n- $B_2$: updates $y := y + 1$ and then goes to $J$.\n- $J$: merges the paths from $B_1$ and $B_2$ with SSA merge functions $\\phi$, producing $x_J := \\phi(x_{B_1}, x_{B_2})$ and $y_J := \\phi(y_{B_1}, y_{B_2})$, then goes to $S$.\n- $S$: branches on predicate $q$ to $U$ (true) or $N$ (false).\n- $U$: computes $u := x_J + y_J$, uses $u$, and then goes to $E$.\n- $N$: performs unrelated work without using $x+y$, then goes to $E$.\n- $E$: exit.\n\nAssumptions:\n- $x$ and $y$ are not modified in $J$, $S$, $U$, or $N$ (except $u$ is defined in $U$ and only used there).\n- The computation $u := x_J + y_J$ in $U$ is the only evaluation of $x+y$ on the $S \\to U$ (true) path; on the $S \\to N$ (false) path there is no evaluation of $x+y$ in the original program.\n- Because $x+y$ may raise an overflow exception, non-speculative PRE must not introduce any new execution of $x+y$ on the $S \\to N$ path.\n\nObservation: On the $B_0 \\to B_1 \\to J \\to S \\to U$ path, the expression $x+y$ is already computed in $B_1$ as $t := x + y$, and along this path $u := x_J + y_J$ is equal to $t$. On the alternative path $B_0 \\to B_2 \\to J \\to S \\to U$, $x+y$ is not computed earlier. Thus, at $U$, $x+y$ is partially redundant: it is available along one incoming path but not the other.\n\nStandard PRE would try to place $x+y$ to make it available along all paths to $U$, enabling the elimination of $u := x_J + y_J$ in $U$. However, because the closest placement that dominates $U$ across both predecessors is at or before $S$ or $J$, moving $x+y$ there would cause its evaluation also when $q$ is false, violating non-speculative constraints.\n\nWhich restructuring, via code cloning (tail duplication), enables a legal PRE placement so that the computation $u := x_J + y_J$ in $U$ becomes fully redundant and can be removed without introducing any new execution of $x+y$ on the $S \\to N$ path?\n\nChoose the best option:\n\nA. Clone the tail that leads to the use: create path-specific copies $J_1, S_1, U_1$ for the $B_1$ path and $J_2, S_2, U_2$ for the $B_2$ path by duplicating $J$ and $S$ and splitting their outgoing $q$-true edges to $U$. Redirect $B_1 \\to J_1 \\to S_1 \\to U_1$ and $B_2 \\to J_2 \\to S_2 \\to U_2$. Insert a computation of $x+y$ only on the $q$-true edge of $S_2$ (just before $U_2$), and reuse $t := x+y$ from $B_1$ along $J_1 \\to S_1 \\to U_1$. Then replace and remove $u := x_J + y_J$ in both $U_1$ and $U_2$. This executes $x+y$ only when $q$ is true and eliminates the $\\phi$-merged operands in the cloned tails.\n\nB. Clone only the join $J$ into $J_1$ and $J_2$, redirect $B_1 \\to J_1$ and $B_2 \\to J_2$, keep $S$ and $U$ shared, and insert $x+y$ at $J_2$ for the $B_2$ path. Remove $u := x_J + y_J$ in $U$.\n\nC. Clone only the predecessor $B_1$ into a new block $B'_1$ that also computes $t := x + y$, keep $J$, $S$, and $U$ unchanged, and rely on the availability of $t$ to remove $u := x_J + y_J$ in $U$.\n\nD. Speculatively hoist $x+y$ to $B_0$ (preheader), delete $u := x_J + y_J$ in $U$, and rely on $x$ and $y$ remaining unchanged until $U$ to ensure correctness without cloning.",
            "solution": "### Step 1: Extract Givens\n\nThe problem statement provides the following information:\n- **Optimization Goal**: Apply non-speculative partial redundancy elimination (PRE) for the expression $x+y$.\n- **Constraint**: The expression $x+y$ may raise an exception (integer overflow), so PRE must be non-speculative. This means the compiler cannot introduce a new evaluation of $x+y$ on any control-flow path that does not evaluate it in the original program.\n- **Program Representation**: A Control Flow Graph (CFG) using Static Single Assignment (SSA).\n- **CFG Structure and Basic Blocks**:\n    - $B_0$: Initializes $x := a$, $y := b$. Branches on predicate $p$ to $B_1$ or $B_2$.\n    - $B_1$: Computes $t := x + y$. Jumps to $J$.\n    - $B_2$: Updates $y := y + 1$. Jumps to $J$.\n    - $J$: A join block that merges paths from $B_1$ and $B_2$. It contains SSA merge functions: $x_J := \\phi(x_{B_1}, x_{B_2})$ and $y_J := \\phi(y_{B_1}, y_{B_2})$. Jumps to $S$.\n    - $S$: Branches on predicate $q$. If $q$ is true, jumps to $U$. If $q$ is false, jumps to $N$.\n    - $U$: Computes $u := x_J + y_J$, uses the result $u$. Jumps to $E$.\n    - $N$: Performs unrelated work, does not use or compute $x+y$. Jumps to $E$.\n    - $E$: Exit block.\n- **Assumptions**:\n    - Variables $x$ and $y$ are not modified in blocks $J, S, U, N$.\n    - The computation $u := x_J + y_J$ in $U$ is the only one on the $S \\to U$ path.\n    - There is no evaluation of $x+y$ on the $S \\to N$ path in the original program.\n- **Problem Observation**: The expression $x+y$ is partially redundant at block $U$. It is available on the path $B_0 \\to B_1 \\to J \\to S \\to U$ (computed in $B_1$) but not on the path $B_0 \\to B_2 \\to J \\to S \\to U$.\n- **Challenge**: Standard PRE would hoist the computation to a block that dominates $U$, such as $J$ or $S$. However, this would cause $x+y$ to be evaluated on the path to $N$ (specifically, $... \\to J \\to S \\to N$), violating the non-speculative constraint.\n- **Question**: Identify the code restructuring (via code cloning/tail duplication) that enables a valid, non-speculative PRE transformation to make $u := x_J + y_J$ fully redundant.\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientifically Grounded**: The problem is firmly rooted in the principles of compiler design. Partial Redundancy Elimination (PRE), Control Flow Graphs (CFGs), Static Single Assignment (SSA), non-speculative execution, and code duplication (tail duplication) are all standard, well-established concepts in compiler optimization theory. The concern about exceptions from arithmetic operations like addition is a practical issue that modern compilers must handle.\n2.  **Well-Posed**: The problem is clearly structured. It defines an initial state (the CFG and data flow), a clear objective (eliminate the redundant computation in $U$), a critical constraint (non-speculation), and asks for a specific transformation technique (code cloning) to achieve the goal. The description allows for a unique, logical solution to be derived.\n3.  **Objective**: The language is precise and unbiased. It uses standard terminology from computer science. The problem setup is a formal representation of a program snippet, free of subjective elements.\n\nThe problem does not violate any of the invalidity criteria. It is not factually unsound, is formalizable, is complete, is realistic within the context of compiler theory, and is not trivial.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. I will proceed with the solution derivation.\n\n### Analysis of the Problem and Derivation of Solution\n\nThe core of the problem is the interaction between data flow (availability of the expression $x+y$) and control flow (the merge at $J$ and the split at $S$).\n- On the path from $B_1$, $x+y$ is computed and its value is available at $U$. Let the values from $B_0$ be $x_0$ and $y_0$. $B_1$ computes $t := x_0 + y_0$. The values entering $J$ from $B_1$ are $(x_0, y_0)$.\n- On the path from $B_2$, the value of $y$ is changed. $B_2$ computes $y' := y_0 + 1$. The values entering $J$ from $B_2$ are $(x_0, y')$. The expression $x+y$ is not computed.\n\nAt block $J$, the SSA $\\phi$-functions merge these values. For simplicity, let's trace the values, assuming we are on the SSA representation.\n- $x_J := \\phi(x_{from\\_B1}, x_{from\\_B2})$. Since $x$ is not changed in $B_1$ or $B_2$, $x_J$ is just the value of $x$ from $B_0$.\n- $y_J := \\phi(y_{from\\_B1}, y_{from\\_B2})$. This means $y_J$ will be the value from $B_1$ if control came from $B_1$, and the value from $B_2$ if control came from $B_2$.\n\nThe computation in $U$ is $u := x_J + y_J$.\n- If the path was $B_1 \\to J \\to S \\to U$, then $x_J+y_J$ is equivalent to the original $x+y$ computed in $B_1$. It is redundant.\n- If the path was $B_2 \\to J \\to S \\to U$, then $x_J+y_J$ is a new computation (with a modified $y$). It is not redundant.\n\nThe expression $x_J + y_J$ is therefore partially redundant. To make it fully redundant, we would need to compute it on the path from $B_2$ before it reaches $U$. The latest point to do this would be in $B_2$. The earliest point would be after the definition of the new $y$. Let's say we insert it in $B_2$. However, the classic PRE algorithm tries to hoist computations as high as possible. Hoisting $x_J+y_J$ to $J$ or $S$ is problematic because of the branch to $N$:\n- Path $B_0 \\to B_1 \\to J \\to S \\to N$\n- Path $B_0 \\to B_2 \\to J \\to S \\to N$\nNeither of these paths originally computes $x+y$ after block $B_0$. Placing the computation in $J$ or $S$ would introduce it on both paths, violating the non-speculative constraint.\n\nThe solution is to restructure the control flow to isolate the path that needs the computation from the path that doesn't. This is achieved by duplicating the \"tail\" of the graph starting from the merge point $J$. This creates separate control flow paths for the predecessors of $J$.\n\nThe required transformation is to duplicate the blocks $J, S, U$ for each of the two paths coming from $B_1$ and $B_2$.\n1.  **Clone for path from $B_1$**: Create copies $J_1, S_1, U_1$. The control flow becomes $B_1 \\to J_1 \\to S_1$. Block $S_1$ branches on $q$: if true, it goes to $U_1$; if false, it can go to the original $N$.\n2.  **Clone for path from $B_2$**: Create copies $J_2, S_2, U_2$. The control flow becomes $B_2 \\to J_2 \\to S_2$. Block $S_2$ branches on $q$: if true, it goes to $U_2$; if false, it can go to the original $N$.\n\nNow let's analyze the data flow on these new, separated paths:\n- **Path $B_1 \\to J_1 \\to S_1 \\to U_1$**: The value $t$ computed in $B_1$ is available along this entire path. Thus, the computation $u := x+y$ in $U_1$ is fully redundant and can be replaced with a move, e.g., $u:=t$.\n- **Path $B_2 \\to J_2 \\to S_2 \\to U_2$**: The expression $x+y$ (with the modified $y$ from $B_2$) is not available. To make it redundant in $U_2$, we must insert a new computation. The crucial point is *where* to insert it. We must not execute it if the path goes to $N$. The branch distinguishing the path to $U_2$ from the path to $N$ occurs in $S_2$. Therefore, the correct place to insert the computation is *after* the branch logic in $S_2$ but *before* the computation in $U_2$. This can be modeled as inserting it on the edge $S_2 \\to U_2$ or at the very beginning of $U_2$. Let's call the new temporary $t_2$. We insert $t_2 := x+y$ on the edge $S_2 \\to U_2$. Then, the computation in $U_2$ becomes fully redundant and can be replaced with $u := t_2$.\n\nThis transformation successfully eliminates the partial redundancy in $U$ without introducing any speculative computation on the paths to $N$.\n\n### Option-by-Option Analysis\n\n- **A. Clone the tail that leads to the use: create path-specific copies $J_1, S_1, U_1$ for the $B_1$ path and $J_2, S_2, U_2$ for the $B_2$ path by duplicating $J$ and $S$ and splitting their outgoing $q$-true edges to $U$. Redirect $B_1 \\to J_1 \\to S_1 \\to U_1$ and $B_2 \\to J_2 \\to S_2 \\to U_2$. Insert a computation of $x+y$ only on the $q$-true edge of $S_2$ (just before $U_2$), and reuse $t := x+y$ from $B_1$ along $J_1 \\to S_1 \\to U_1$. Then replace and remove $u := x_J + y_J$ in both $U_1$ and $U_2$. This executes $x+y$ only when $q$ is true and eliminates the $\\phi$-merged operands in the cloned tails.**\n  - This option correctly identifies the need for tail duplication, creating separate paths for the flows from $B_1$ and $B_2$.\n  - It correctly describes how to handle each path: reuse the existing computation from $B_1$ on one path, and insert a new computation on the other.\n  - Critically, it specifies inserting the new computation \"only on the $q$-true edge of $S_2$\", which is the correct placement to satisfy the non-speculative constraint. This ensures the new computation is only performed when the program is committed to taking the path to $U$, not $N$.\n  - It also correctly notes the side-effect of eliminating the non-trivial $\\phi$-functions at the original join point $J$.\n  - The phrasing \"Redirect $B_1 \\to J_1 \\to S_1 \\to U_1$\" is a slight simplification, as $S_1$ is a conditional branch, but the intent is clear from the context. The overall description of the transformation is accurate and complete.\n  - Verdict: **Correct**.\n\n- **B. Clone only the join $J$ into $J_1$ and $J_2$, redirect $B_1 \\to J_1$ and $B_2 \\to J_2$, keep $S$ and $U$ shared, and insert $x+y$ at $J_2$ for the $B_2$ path. Remove $u := x_J + y_J$ in $U$.**\n  - This transformation is insufficient. The new CFG would have $J_1 \\to S$ and $J_2 \\to S$. Block $S$ would become the new merge point.\n  - The option suggests inserting $x+y$ into $J_2$.\n  - Now consider the path $B_2 \\to J_2 \\to S \\to N$. This path now includes the new computation of $x+y$ from block $J_2$. The original path did not have this computation. This violates the non-speculative constraint.\n  - Verdict: **Incorrect**.\n\n- **C. Clone only the predecessor $B_1$ into a new block $B'_1$ that also computes $t := x + y$, keep $J$, $S$, and $U$ unchanged, and rely on the availability of $t$ to remove $u := x_J + y_J$ in $U$.**\n  - This transformation is nonsensical in this context. Cloning a block that is a predecessor to the merge point does not alter the problematic control flow structure at $J$ (merge) and $S$ (split).\n  - The partial redundancy at $U$ is caused by the path from $B_2$, which lacks the computation. Cloning $B_1$ does nothing to address the path from $B_2$. The expression $x_J+y_J$ would remain partially redundant at $U$.\n  - Verdict: **Incorrect**.\n\n- **D. Speculatively hoist $x+y$ to $B_0$ (preheader), delete $u := x_J + y_J$ in $U$, and rely on $x$ and $y$ remaining unchanged until $U$ to ensure correctness without cloning.**\n  - This option proposes speculative execution. The problem statement explicitly forbids this: \"Because $x+y$ may raise an overflow exception, non-speculative PRE must not introduce any new execution of $x+y$ on the $S \\to N$ path.\"\n  - Hoisting the computation to $B_0$ means it would be executed regardless of which path is taken later. If the path $... \\to S \\to N$ is taken, an unnecessary and potentially faulting computation has been performed. This is a direct violation of the stated constraint.\n  - Verdict: **Incorrect**.\n\nBased on the analysis, only option A correctly describes a valid transformation that achieves the goal under the given constraints.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}