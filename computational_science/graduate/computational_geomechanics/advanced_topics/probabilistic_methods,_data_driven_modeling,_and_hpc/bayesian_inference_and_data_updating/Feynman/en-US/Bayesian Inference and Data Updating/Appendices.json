{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we will derive the posterior distribution for a fundamental geomechanical parameter: Young's modulus. This exercise  uses the elegant and widely applicable Gaussian-Gaussian conjugate model. By working through the derivation from first principles and completing the square, you will gain a deep, analytical understanding of how Bayesian inference mathematically blends prior knowledge with new data to refine our estimate of a physical quantity.",
            "id": "3502888",
            "problem": "A cylindrical rock core from a homogeneous sandstone formation is tested in uniaxial compression under small strains. The focus is on inferring the Young’s modulus, denoted by $E$, which is assumed to be spatially uniform at the scale of the specimen. A prior belief about $E$ is modeled as a Gaussian random variable $E \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$, where $\\mu_{0}$ and $\\sigma_{0}^{2}$ are known constants reflecting prior calibration from a broader formation study. During the test, repeated local secant estimates of stiffness, $\\{y_{i}\\}_{i=1}^{n}$, are computed from linear segments of the stress–strain curve. Due to instrumentation noise and microstructural variability at the segment scale, each $y_{i}$ is modeled as an independent and identically distributed (i.i.d.) Gaussian observation about the true $E$ with known measurement variance $\\sigma^{2}$, that is $y_{i} \\mid E \\sim \\mathcal{N}(E, \\sigma^{2})$ and independent across $i=1,\\dots,n$. Assume that $\\sigma^{2}$ is known from prior sensor calibration, and that the Gaussian prior is a suitable approximation over the range of feasible $E$ so that its small probability mass on negative values is negligible for the physical interpretation.\n\nStarting from Bayes’ theorem and the definition of the Gaussian probability density function, and using only the independence of $\\{y_{i}\\}_{i=1}^{n}$ and the stated stochastic assumptions, derive the fully normalized posterior density $p(E \\mid y_{1},\\dots,y_{n})$ as an explicit function of $E$, $\\mu_{0}$, $\\sigma_{0}^{2}$, $\\sigma^{2}$, $n$, and the data $\\{y_{i}\\}_{i=1}^{n}$. Your derivation must proceed by explicitly forming the product of the prior density and the likelihood, collecting terms in $E$, and completing the square in the exponent to reveal the Gaussian form, without appealing to any pre-memorized conjugacy formulas.\n\nExpress your final result as a single closed-form analytic expression for the posterior density $p(E \\mid y_{1},\\dots,y_{n})$ that is fully normalized with respect to $E$. No numerical evaluation is required, and no rounding should be performed.",
            "solution": "The problem requires the derivation of the posterior probability density function $p(E \\mid y_{1},\\dots,y_{n})$ for the Young's modulus $E$, given a set of $n$ independent measurements $\\{y_{i}\\}_{i=1}^{n}$. The derivation must start from first principles using Bayes' theorem, the specified prior and likelihood distributions, and proceed by completing the square in the exponent of the resulting expression.\n\nFirst, we must validate the problem statement.\n\n**Step 1: Extract Givens**\n- Parameter of interest: Young’s modulus, $E$.\n- Prior distribution for $E$: $p(E) = \\mathcal{N}(E \\mid \\mu_{0}, \\sigma_{0}^{2})$, where $\\mu_{0}$ and $\\sigma_{0}^{2}$ are known constants.\n- Data: A set of $n$ measurements, $\\{y_{i}\\}_{i=1}^{n}$.\n- Likelihood function (data distribution given the parameter): Each measurement $y_i$ is conditionally independent and drawn from a Gaussian distribution, $y_{i} \\mid E \\sim \\mathcal{N}(E, \\sigma^{2})$, for $i=1,\\dots,n$. The measurement variance $\\sigma^{2}$ is known.\n- Assumption: The probability mass of the Gaussian prior on negative values of $E$ is negligible.\n- Task: Derive the fully normalized posterior density $p(E \\mid y_{1},\\dots,y_{n})$ by explicitly forming the product of the prior and likelihood and completing the square.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, modeling a standard engineering task of parameter estimation from noisy data. The use of Gaussian distributions for both the prior and the likelihood represents a common and well-justified approach in Bayesian statistics, particularly for physical parameters where a central tendency and symmetric uncertainty are reasonable approximations. The problem is well-posed, providing all necessary information (prior, likelihood, independence assumptions) to derive a unique posterior distribution. The language is objective and precise. The problem satisfies all criteria for validity; it is not unsound, incomplete, unrealistic, or ill-posed.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We will proceed with the derivation.\n\nThe derivation begins with Bayes' theorem, which states that the posterior probability density is proportional to the product of the likelihood and the prior probability density:\n$$p(E \\mid y_{1},\\dots,y_{n}) \\propto p(y_{1},\\dots,y_{n} \\mid E) p(E)$$\n\nThe prior distribution for $E$ is given as a Gaussian:\n$$p(E) = \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}} \\exp\\left(-\\frac{(E - \\mu_{0})^2}{2\\sigma_{0}^{2}}\\right)$$\n\nThe measurements $\\{y_{i}\\}_{i=1}^{n}$ are independent and identically distributed given $E$. Therefore, the likelihood function is the product of the individual probabilities for each measurement:\n$$p(y_{1},\\dots,y_{n} \\mid E) = \\prod_{i=1}^{n} p(y_{i} \\mid E)$$\nEach measurement $y_{i}$ is drawn from a Gaussian distribution centered at $E$ with variance $\\sigma^{2}$:\n$$p(y_{i} \\mid E) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(y_{i} - E)^2}{2\\sigma^{2}}\\right)$$\nSubstituting this into the product gives the full likelihood:\n$$p(y_{1},\\dots,y_{n} \\mid E) = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\right)^{n} \\exp\\left(-\\sum_{i=1}^{n} \\frac{(y_{i} - E)^2}{2\\sigma^{2}}\\right)$$\n\nNow, we form the unnormalized posterior by multiplying the prior and the likelihood. We can ignore the constant normalization factors for now and focus on the exponential terms, as they contain the dependence on $E$.\n$$p(E \\mid y_{1},\\dots,y_{n}) \\propto \\exp\\left(-\\frac{(E - \\mu_{0})^2}{2\\sigma_{0}^{2}}\\right) \\exp\\left(-\\sum_{i=1}^{n} \\frac{(y_{i} - E)^2}{2\\sigma^{2}}\\right)$$\n$$p(E \\mid y_{1},\\dots,y_{n}) \\propto \\exp\\left(-\\frac{(E - \\mu_{0})^2}{2\\sigma_{0}^{2}} - \\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n} (y_{i} - E)^2 \\right)$$\n\nLet's analyze the argument of the exponential function, which we denote as $Q(E)$:\n$$Q(E) = -\\frac{1}{2} \\left[ \\frac{(E - \\mu_{0})^2}{\\sigma_{0}^{2}} + \\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n} (y_{i} - E)^2 \\right]$$\nTo identify the form of the posterior distribution, we expand the quadratic terms in $E$:\n$$(E - \\mu_{0})^2 = E^2 - 2E\\mu_{0} + \\mu_{0}^2$$\n$$\\sum_{i=1}^{n} (y_{i} - E)^2 = \\sum_{i=1}^{n} (E^2 - 2Ey_{i} + y_{i}^2) = nE^2 - 2E\\sum_{i=1}^{n}y_{i} + \\sum_{i=1}^{n}y_{i}^2$$\nSubstituting these back into the expression for $Q(E)$ and grouping terms by powers of $E$:\n$$Q(E) = -\\frac{1}{2} \\left[ \\frac{E^2 - 2E\\mu_{0} + \\mu_{0}^2}{\\sigma_{0}^{2}} + \\frac{nE^2 - 2E\\sum_{i=1}^{n}y_{i} + \\sum_{i=1}^{n}y_{i}^2}{\\sigma^{2}} \\right]$$\n$$Q(E) = -\\frac{1}{2} \\left[ E^2\\left(\\frac{1}{\\sigma_{0}^{2}} + \\frac{n}{\\sigma^{2}}\\right) - 2E\\left(\\frac{\\mu_{0}}{\\sigma_{0}^{2}} + \\frac{\\sum_{i=1}^{n}y_{i}}{\\sigma^{2}}\\right) + \\left(\\frac{\\mu_{0}^2}{\\sigma_{0}^{2}} + \\frac{\\sum_{i=1}^{n}y_{i}^2}{\\sigma^{2}}\\right) \\right]$$\n\nThe expression inside the brackets is a quadratic function of $E$. This implies that the posterior distribution is also Gaussian. A general unnormalized Gaussian form is $\\exp(-\\frac{1}{2\\sigma_n^2}(E - \\mu_n)^2)$. We proceed to complete the square to find the posterior mean $\\mu_n$ and variance $\\sigma_n^2$.\n\nLet the coefficient of $E^2$ be $\\frac{1}{\\sigma_n^2}$:\n$$\\frac{1}{\\sigma_n^2} = \\frac{1}{\\sigma_{0}^{2}} + \\frac{n}{\\sigma^{2}}$$\nThis defines the posterior variance $\\sigma_n^2$.\nLet the coefficient of $-2E$ be $\\frac{\\mu_n}{\\sigma_n^2}$:\n$$\\frac{\\mu_n}{\\sigma_n^2} = \\frac{\\mu_{0}}{\\sigma_{0}^{2}} + \\frac{\\sum_{i=1}^{n}y_{i}}{\\sigma^{2}}$$\nThis defines the posterior mean $\\mu_n$.\n\nUsing these definitions, we can rewrite $Q(E)$:\n$$Q(E) = -\\frac{1}{2} \\left[ \\frac{1}{\\sigma_n^2}E^2 - 2\\frac{\\mu_n}{\\sigma_n^2}E + C \\right]$$\nwhere $C$ represents terms not dependent on $E$.\nCompleting the square for the terms involving $E$:\n$$\\frac{1}{\\sigma_n^2}E^2 - 2\\frac{\\mu_n}{\\sigma_n^2}E = \\frac{1}{\\sigma_n^2}(E^2 - 2\\mu_n E) = \\frac{1}{\\sigma_n^2}( (E - \\mu_n)^2 - \\mu_n^2 )$$\nSo,\n$$Q(E) = -\\frac{1}{2} \\left[ \\frac{(E - \\mu_n)^2}{\\sigma_n^2} - \\frac{\\mu_n^2}{\\sigma_n^2} + C \\right] = -\\frac{(E - \\mu_n)^2}{2\\sigma_n^2} + \\text{constant}$$\nThe terms not depending on $E$ are absorbed into the normalization constant of the posterior density.\nThus, the posterior is proportional to a Gaussian kernel:\n$$p(E \\mid y_{1},\\dots,y_{n}) \\propto \\exp\\left( -\\frac{(E - \\mu_n)^2}{2\\sigma_n^2} \\right)$$\nThis confirms the posterior is a Gaussian distribution, $p(E \\mid y_{1},\\dots,y_{n}) = \\mathcal{N}(E \\mid \\mu_n, \\sigma_n^2)$.\nThe normalized posterior density is:\n$$p(E \\mid y_{1},\\dots,y_{n}) = \\frac{1}{\\sqrt{2\\pi\\sigma_n^2}}\\exp\\left( -\\frac{(E - \\mu_n)^2}{2\\sigma_n^2} \\right)$$\n\nWe now write the parameters $\\mu_n$ and $\\sigma_n^2$ explicitly in terms of the given quantities.\nThe posterior variance $\\sigma_n^2$ is the inverse of the posterior precision:\n$$\\sigma_n^2 = \\left( \\frac{1}{\\sigma_{0}^{2}} + \\frac{n}{\\sigma^{2}} \\right)^{-1} = \\frac{\\sigma_{0}^{2}\\sigma^{2}}{\\sigma^{2} + n\\sigma_{0}^{2}}$$\nThe posterior mean $\\mu_n$ is:\n$$\\mu_n = \\sigma_n^2 \\left( \\frac{\\mu_{0}}{\\sigma_{0}^{2}} + \\frac{\\sum_{i=1}^{n}y_{i}}{\\sigma^{2}} \\right) = \\frac{\\sigma_{0}^{2}\\sigma^{2}}{\\sigma^{2} + n\\sigma_{0}^{2}} \\left( \\frac{\\mu_{0}\\sigma^{2} + \\sigma_{0}^{2}\\sum_{i=1}^{n}y_{i}}{\\sigma_{0}^{2}\\sigma^{2}} \\right) = \\frac{\\mu_{0}\\sigma^{2} + \\sigma_{0}^{2}\\sum_{i=1}^{n}y_{i}}{\\sigma^{2} + n\\sigma_{0}^{2}}$$\n\nSubstituting these expressions for $\\mu_n$ and $\\sigma_n^2$ back into the normalized Gaussian PDF gives the final result. For a more compact representation, we can express the final answer in terms of the posterior precision, $\\frac{1}{\\sigma_n^2} = \\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma^2}$, and the expression for $\\mu_n$.\n\nThe final expression for the posterior density is:\n$$p(E \\mid y_{1},\\dots,y_{n}) = \\frac{1}{\\sqrt{2\\pi \\left(\\frac{\\sigma_0^2 \\sigma^2}{\\sigma^2 + n\\sigma_0^2}\\right)}} \\exp\\left( -\\frac{\\left(E - \\frac{\\mu_0\\sigma^2 + \\sigma_0^2\\sum_{i=1}^{n}y_i}{\\sigma^2 + n\\sigma_0^2}\\right)^2}{2 \\left(\\frac{\\sigma_0^2 \\sigma^2}{\\sigma^2 + n\\sigma_0^2}\\right)} \\right)$$\nThis can be rewritten by bringing the denominator of the variance into the prefactor and the exponent:\n$$p(E \\mid y_{1},\\dots,y_{n}) = \\sqrt{\\frac{\\sigma^2 + n\\sigma_0^2}{2\\pi \\sigma_0^2 \\sigma^2}} \\exp\\left( -\\frac{\\sigma^2 + n\\sigma_0^2}{2\\sigma_0^2 \\sigma^2}\\left(E - \\frac{\\mu_0\\sigma^2 + \\sigma_0^2\\sum_{i=1}^{n}y_i}{\\sigma^2 + n\\sigma_0^2}\\right)^2 \\right)$$",
            "answer": "$$\n\\boxed{\n\\sqrt{\\frac{1}{2\\pi}\\left(\\frac{1}{\\sigma_{0}^{2}} + \\frac{n}{\\sigma^{2}}\\right)} \\exp\\left( -\\frac{1}{2}\\left(\\frac{1}{\\sigma_{0}^{2}} + \\frac{n}{\\sigma^{2}}\\right) \\left(E - \\frac{\\frac{\\mu_{0}}{\\sigma_{0}^{2}} + \\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n}y_{i}}{\\frac{1}{\\sigma_{0}^{2}} + \\frac{n}{\\sigma^{2}}}\\right)^{2} \\right)\n}\n$$"
        },
        {
            "introduction": "Building on the analytical foundation from the previous exercise, we now transition to a computational setting. In this practice , you will not only implement the Bayesian update for Young's modulus but also investigate a critical aspect of applied Bayesian modeling: sensitivity analysis. By calculating the partial derivatives of the posterior mean with respect to the prior's hyperparameters, you will learn to quantify how strongly your conclusions depend on your initial assumptions, a vital skill for any rigorous scientific investigation.",
            "id": "3502949",
            "problem": "A uniaxial compression test campaign on a sedimentary rock specimen is conducted to estimate the Young's modulus $E$ (units: $\\mathrm{GPa}$) for use in computational geomechanics simulations. The measurement protocol in the linear elastic regime produces $n$ independent ratios $y_i = \\sigma_i / \\varepsilon_i$ that serve as noisy observations of the true modulus $E$, where $\\sigma_i$ is axial stress and $\\varepsilon_i$ is axial strain. The instrumentation and calibration report specifies an approximately Gaussian measurement error model: the conditional distribution of each observation $y_i$ given the true modulus $E$ is $y_i \\mid E \\sim \\mathcal{N}(E,\\sigma^2)$ with known variance $\\sigma^2$ (units: $\\mathrm{GPa}^2$). The prior distribution for $E$ is taken as a Gaussian $E \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$, where $(\\mu_0,\\sigma_0)$ are hyperparameters encoding prior belief about $E$.\n\nStarting from Bayes' theorem and the definitions of the Gaussian likelihood and Gaussian prior, derive an algorithm to:\n- Compute the posterior distribution of $E$ given observations $\\{y_i\\}_{i=1}^n$ and hyperparameters $(\\mu_0,\\sigma_0)$ under the Gaussian-Gaussian model described above.\n- Compute the posterior mean of $E$ (units: $\\mathrm{GPa}$) and the local sensitivity of the posterior mean with respect to the prior hyperparameters $(\\mu_0,\\sigma_0)$, defined as the partial derivatives $\\partial \\mathbb{E}[E \\mid \\{y_i\\}] / \\partial \\mu_0$ and $\\partial \\mathbb{E}[E \\mid \\{y_i\\}] / \\partial \\sigma_0$. These derivatives are dimensionless.\n\nUse the following dataset and measurement variance for the likelihood:\n- Observations (in $\\mathrm{GPa}$): $y = [\\,24.8,\\,25.1,\\,25.5,\\,24.9,\\,25.0,\\,25.3,\\,24.7,\\,25.2\\,]$.\n- Known measurement standard deviation: $\\sigma = 0.5$ (so the measurement variance is $\\sigma^2 = 0.25$ in $\\mathrm{GPa}^2$).\n\nPerform a sensitivity analysis by evaluating the posterior mean and its local sensitivities for the following test suite of prior hyperparameters and sample sizes:\n1. Baseline, moderately informative prior, full sample: $(\\mu_0,\\sigma_0) = (25.0\\,\\mathrm{GPa},\\,2.0\\,\\mathrm{GPa})$, $n = 8$.\n2. Highly informative and biased prior, full sample: $(\\mu_0,\\sigma_0) = (35.0\\,\\mathrm{GPa},\\,0.5\\,\\mathrm{GPa})$, $n = 8$.\n3. Very vague prior, full sample: $(\\mu_0,\\sigma_0) = (20.0\\,\\mathrm{GPa},\\,10.0\\,\\mathrm{GPa})$, $n = 8$.\n4. Baseline prior, small sample edge case (use the first $n$ observations): $(\\mu_0,\\sigma_0) = (25.0\\,\\mathrm{GPa},\\,2.0\\,\\mathrm{GPa})$, $n = 2$.\n5. Near-correct, extremely informative prior, full sample: $(\\mu_0,\\sigma_0) = (25.1\\,\\mathrm{GPa},\\,0.1\\,\\mathrm{GPa})$, $n = 8$.\n\nScientific realism requirements:\n- Treat $E$ as a continuous scalar parameter.\n- Treat $\\sigma^2$ as known and fixed by calibration, and do not estimate it.\n- All computations must be in $\\mathrm{GPa}$ for moduli and $\\mathrm{GPa}^2$ for variances when applicable.\n- The local sensitivities are dimensionless.\n\nYour program must implement the derived algorithm and produce a single line of output containing, for each test case, a list of three floats:\n- The posterior mean of $E$ in $\\mathrm{GPa}$, rounded to six decimal places.\n- The local sensitivity of the posterior mean with respect to $\\mu_0$, rounded to six decimal places.\n- The local sensitivity of the posterior mean with respect to $\\sigma_0$, rounded to six decimal places.\n\nFinal output format:\n- A single line containing a comma-separated list of per-test-case results, each result itself being a list of three floats, all enclosed in square brackets. For example: $[\\,[m_1,s_{\\mu,1},s_{\\sigma_0,1}],\\,[m_2,s_{\\mu,2},s_{\\sigma_0,2}],\\,\\dots\\,]$.\n- The posterior means must be expressed in $\\mathrm{GPa}$, and sensitivities must be dimensionless.",
            "solution": "The problem requires the derivation of an algorithm for Bayesian updating of the Young's modulus parameter $E$ and the subsequent computation of the posterior mean and its sensitivities with respect to prior hyperparameters. The model assumes a Gaussian likelihood for the measurement data and a Gaussian prior for the parameter $E$. This is a classic example of a conjugate prior model.\n\nFirst, we establish the mathematical framework based on Bayes' theorem. The posterior probability distribution of the parameter $E$ given a set of $n$ independent observations $\\{y_i\\}_{i=1}^n$ is proportional to the product of the likelihood of the data given the parameter and the prior distribution of the parameter:\n$$ p(E \\mid \\{y_i\\}_{i=1}^n) \\propto p(\\{y_i\\}_{i=1}^n \\mid E) \\cdot p(E) $$\n\nThe problem specifies the following distributions:\n1.  **Likelihood**: Each observation $y_i$ is drawn from a normal distribution with mean $E$ and known variance $\\sigma^2$. Since the observations are independent, the joint likelihood is the product of individual likelihoods:\n    $$ p(\\{y_i\\} \\mid E) = \\prod_{i=1}^n \\mathcal{N}(y_i \\mid E, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - E)^2}{2\\sigma^2}\\right) $$\n    $$ p(\\{y_i\\} \\mid E) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - E)^2\\right) $$\n\n2.  **Prior**: The prior belief about $E$ is modeled as a normal distribution with hyperparameters $\\mu_0$ (prior mean) and $\\sigma_0^2$ (prior variance):\n    $$ p(E) = \\mathcal{N}(E \\mid \\mu_0, \\sigma_0^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left(-\\frac{(E - \\mu_0)^2}{2\\sigma_0^2}\\right) $$\n\nTo find the posterior distribution, we combine the likelihood and prior. We focus on the exponential terms, as the normalization constants are not dependent on $E$:\n$$ p(E \\mid \\{y_i\\}) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - E)^2\\right) \\exp\\left(-\\frac{(E - \\mu_0)^2}{2\\sigma_0^2}\\right) $$\n$$ p(E \\mid \\{y_i\\}) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{\\sum_{i=1}^n (y_i - E)^2}{\\sigma^2} + \\frac{(E - \\mu_0)^2}{\\sigma_0^2} \\right] \\right) $$\n\nThe expression in the exponent is a quadratic function of $E$. This implies that the posterior distribution is also a normal distribution, let's say $E \\mid \\{y_i\\} \\sim \\mathcal{N}(\\mu_n, \\sigma_n^2)$. To find its parameters $\\mu_n$ and $\\sigma_n^2$, we can complete the square for $E$ in the exponent.\nLet the exponent be $-\\frac{1}{2} Q(E)$. Expanding the terms in $Q(E)$:\n$$ Q(E) = \\frac{1}{\\sigma^2}\\sum_{i=1}^n (y_i^2 - 2y_iE + E^2) + \\frac{1}{\\sigma_0^2}(E^2 - 2E\\mu_0 + \\mu_0^2) $$\nGrouping terms by powers of $E$:\n$$ Q(E) = E^2\\left(\\frac{n}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right) - 2E\\left(\\frac{\\sum y_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right) + \\text{const} $$\nFor a normal distribution $\\mathcal{N}(\\mu_n, \\sigma_n^2)$, the term in its exponent is $-\\frac{(E-\\mu_n)^2}{2\\sigma_n^2} = -\\frac{1}{2}\\left(\\frac{E^2}{\\sigma_n^2} - \\frac{2E\\mu_n}{\\sigma_n^2} + \\frac{\\mu_n^2}{\\sigma_n^2}\\right)$.\nBy matching the coefficients of $E^2$, we find the inverse of the posterior variance (the posterior precision):\n$$ \\frac{1}{\\sigma_n^2} = \\frac{n}{\\sigma^2} + \\frac{1}{\\sigma_0^2} $$\nThe posterior variance $\\sigma_n^2$ is therefore:\n$$ \\sigma_n^2 = \\left(\\frac{n}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)^{-1} = \\frac{\\sigma^2\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2} $$\n\nBy matching the coefficients of $E$, we find the posterior mean $\\mu_n$:\n$$ \\frac{\\mu_n}{\\sigma_n^2} = \\frac{\\sum y_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2} $$\nLet $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$ be the sample mean of the observations.\n$$ \\mu_n = \\sigma_n^2 \\left(\\frac{n\\bar{y}}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right) $$\nSubstituting the expression for $\\sigma_n^2$:\n$$ \\mu_n = \\left(\\frac{\\sigma^2\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2}\\right) \\left(\\frac{n\\bar{y}\\sigma_0^2 + \\mu_0\\sigma^2}{\\sigma^2\\sigma_0^2}\\right) = \\frac{n\\bar{y}\\sigma_0^2 + \\mu_0\\sigma^2}{n\\sigma_0^2 + \\sigma^2} $$\n\nThe posterior mean, denoted as $M(\\mu_0, \\sigma_0) = \\mathbb{E}[E \\mid \\{y_i\\}] = \\mu_n$, is the quantity for which we need to compute local sensitivities.\n$$ M(\\mu_0, \\sigma_0) = \\frac{n\\bar{y}\\sigma_0^2 + \\mu_0\\sigma^2}{n\\sigma_0^2 + \\sigma^2} $$\n\nNext, we derive the partial derivatives of $M$ with respect to the prior hyperparameters $\\mu_0$ and $\\sigma_0$.\n\n1.  **Sensitivity with respect to $\\mu_0$**: We differentiate $M$ with respect to $\\mu_0$, treating all other variables ($n, \\bar{y}, \\sigma^2, \\sigma_0$) as constants.\n    $$ \\frac{\\partial M}{\\partial \\mu_0} = \\frac{\\partial}{\\partial \\mu_0} \\left( \\frac{n\\bar{y}\\sigma_0^2 + \\mu_0\\sigma^2}{n\\sigma_0^2 + \\sigma^2} \\right) = \\frac{1}{n\\sigma_0^2 + \\sigma^2} \\cdot \\frac{\\partial}{\\partial \\mu_0} (n\\bar{y}\\sigma_0^2 + \\mu_0\\sigma^2) $$\n    $$ \\frac{\\partial M}{\\partial \\mu_0} = \\frac{\\sigma^2}{n\\sigma_0^2 + \\sigma^2} $$\n\n2.  **Sensitivity with respect to $\\sigma_0$**: We differentiate $M$ with respect to $\\sigma_0$. This requires the quotient rule. Let $u(\\sigma_0) = n\\bar{y}\\sigma_0^2 + \\mu_0\\sigma^2$ and $v(\\sigma_0) = n\\sigma_0^2 + \\sigma^2$. Their derivatives with respect to $\\sigma_0$ are $\\frac{\\partial u}{\\partial \\sigma_0} = 2n\\bar{y}\\sigma_0$ and $\\frac{\\partial v}{\\partial \\sigma_0} = 2n\\sigma_0$.\n    $$ \\frac{\\partial M}{\\partial \\sigma_0} = \\frac{v \\frac{\\partial u}{\\partial \\sigma_0} - u \\frac{\\partial v}{\\partial \\sigma_0}}{v^2} = \\frac{(n\\sigma_0^2 + \\sigma^2)(2n\\bar{y}\\sigma_0) - (n\\bar{y}\\sigma_0^2 + \\mu_0\\sigma^2)(2n\\sigma_0)}{(n\\sigma_0^2 + \\sigma^2)^2} $$\n    Expanding the numerator:\n    $$ (2n^2\\bar{y}\\sigma_0^3 + 2n\\bar{y}\\sigma_0\\sigma^2) - (2n^2\\bar{y}\\sigma_0^3 + 2n\\mu_0\\sigma_0\\sigma^2) $$\n    $$ = 2n\\bar{y}\\sigma_0\\sigma^2 - 2n\\mu_0\\sigma_0\\sigma^2 = 2n\\sigma_0\\sigma^2(\\bar{y} - \\mu_0) $$\n    The final expression for the sensitivity with respect to $\\sigma_0$ is:\n    $$ \\frac{\\partial M}{\\partial \\sigma_0} = \\frac{2n\\sigma_0\\sigma^2(\\bar{y} - \\mu_0)}{(n\\sigma_0^2 + \\sigma^2)^2} $$\n\nThe algorithm to be implemented is as follows:\nFor each test case with a given set of prior hyperparameters $(\\mu_0, \\sigma_0)$ and sample size $n$:\n1.  Select the first $n$ observations from the dataset and compute the sample mean $\\bar{y}$.\n2.  Use the given measurement standard deviation $\\sigma$ to compute the variance $\\sigma^2$.\n3.  Compute the posterior mean $M$ using the derived formula.\n4.  Compute the sensitivity $\\frac{\\partial M}{\\partial \\mu_0}$.\n5.  Compute the sensitivity $\\frac{\\partial M}{\\partial \\sigma_0}$.\n6.  Store the three resulting floating-point numbers.\nAll units must be consistent ($\\mathrm{GPa}$ for moduli, $\\mathrm{GPa}^2$ for variances). The sensitivities are dimensionless, as can be verified from the derived formulas.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian inference problem for Young's modulus,\n    calculating the posterior mean and its sensitivities for several test cases.\n    \"\"\"\n\n    # Full dataset of observations for Young's modulus E, in GPa.\n    y_full = np.array([24.8, 25.1, 25.5, 24.9, 25.0, 25.3, 24.7, 25.2])\n\n    # Known measurement standard deviation in GPa.\n    sigma = 0.5\n\n    # Test suite of prior hyperparameters (mu_0, sigma_0) and sample sizes (n).\n    test_cases = [\n        # (mu_0 [GPa], sigma_0 [GPa], n)\n        (25.0, 2.0, 8),  # 1. Baseline, moderately informative prior, full sample\n        (35.0, 0.5, 8),  # 2. Highly informative and biased prior, full sample\n        (20.0, 10.0, 8), # 3. Very vague prior, full sample\n        (25.0, 2.0, 2),  # 4. Baseline prior, small sample edge case\n        (25.1, 0.1, 8),  # 5. Near-correct, extremely informative prior, full sample\n    ]\n\n    all_results = []\n\n    for mu_0, sigma_0, n in test_cases:\n        # 1. Prepare data for the current case\n        # Use the first n observations from the full dataset.\n        y_obs = y_full[:n]\n        # Calculate the sample mean (y_bar).\n        y_bar = np.mean(y_obs)\n\n        # 2. Calculate intermediate quantities based on derived formulas\n        # Measurement variance (sigma^2).\n        sigma_sq = sigma**2\n        # Prior variance (sigma_0^2).\n        sigma0_sq = sigma_0**2\n        \n        # Common denominator in the expressions for posterior mean and sensitivities.\n        denominator = n * sigma0_sq + sigma_sq\n        \n        # 3. Compute posterior mean and sensitivities\n        # Posterior mean of E, denoted as M.\n        # M = (n * y_bar * sigma_0^2 + mu_0 * sigma^2) / (n * sigma_0^2 + sigma^2)\n        posterior_mean = (n * y_bar * sigma0_sq + mu_0 * sigma_sq) / denominator\n        \n        # Sensitivity of the posterior mean with respect to the prior mean (mu_0).\n        # S_mu0 = sigma^2 / (n * sigma_0^2 + sigma^2)\n        sensitivity_mu0 = sigma_sq / denominator\n        \n        # Sensitivity of the posterior mean with respect to the prior std. dev. (sigma_0).\n        # S_sigma0 = (2 * n * sigma_0 * sigma^2 * (y_bar - mu_0)) / (n * sigma_0^2 + sigma^2)^2\n        sensitivity_sigma0 = (2 * n * sigma_0 * sigma_sq * (y_bar - mu_0)) / (denominator**2)\n\n        # 4. Round results to six decimal places as required.\n        m_rounded = round(posterior_mean, 6)\n        s_mu0_rounded = round(sensitivity_mu0, 6)\n        s_sigma0_rounded = round(sensitivity_sigma0, 6)\n\n        # Append the list of results for the current test case.\n        all_results.append([m_rounded, s_mu0_rounded, s_sigma0_rounded])\n\n    # 5. Format and print the final output\n    # The required format is a string representation of a list of lists,\n    # with no whitespace. Example: [[a,b,c],[d,e,f]]\n    final_output_string = str(all_results).replace(' ', '')\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice tackles a more complex and realistic scenario common in geomechanics: estimating parameters for the Mohr-Coulomb failure criterion. This problem  moves beyond analytically solvable models, requiring you to define a custom likelihood based on the physics of triaxial tests and combine it with non-conjugate priors. You will use numerical optimization to find the Maximum A Posteriori (MAP) estimate, bridging the gap between textbook examples and the application of Bayesian inference to sophisticated, physics-based computational models.",
            "id": "3502946",
            "problem": "You must implement a complete program to perform Bayesian updating of Mohr–Coulomb parameters for axisymmetric triaxial compression tests by modeling the likelihood of peak strength and a prior informed by geological context, and then computing the Maximum A Posteriori (MAP) estimate of cohesion and friction angle. The unknown parameters are cohesion $c$ (in kilopascals, kPa) and friction angle $\\phi$ (in degrees). The estimation must be performed for multiple datasets as a test suite, each consisting of peak strength observations at specified confining pressures. Your program must process all datasets, compute the posterior mode for $(c,\\phi)$ in physically meaningful units, and produce a single line of output in a specified format.\n\nFundamental base context:\n- The Mohr–Coulomb failure criterion is used as the constitutive basis for peak strength at failure in triaxial compression. For axisymmetric triaxial tests with confining pressure $\\sigma_3$ (in kPa) and deviatoric stress at failure $q = \\sigma_1 - \\sigma_3$ (in kPa), the peak deviatoric stress predicted by Mohr–Coulomb with cohesion $c$ and friction angle $\\phi$ is\n$$\nq(c,\\phi,\\sigma_3) = \\frac{2\\,c\\,\\cos\\phi_r}{1 - \\sin\\phi_r} + \\frac{2\\,\\sigma_3\\,\\sin\\phi_r}{1 - \\sin\\phi_r},\n$$\nwhere $\\phi_r$ is the friction angle expressed in radians and $\\phi$ is expressed in degrees; hence, $\\phi_r = \\phi \\times \\pi/180$.\n- Bayesian inference is performed using Bayes’ rule, which states that the posterior density of $(c,\\phi)$ given data is proportional to the likelihood times the prior. The Maximum A Posteriori (MAP) estimate is the maximizer of the posterior.\n\nModeling assumptions for the likelihood and prior:\n- Likelihood: For each dataset indexed by $j$, with observed peak deviatoric stresses $\\{q_{j,i}^{\\text{obs}}\\}_{i=1}^{n_j}$ at confining pressures $\\{\\sigma_{3,j,i}\\}_{i=1}^{n_j}$, assume independent Gaussian measurement errors with known standard deviation $s_j$ (in kPa), so that\n$$\nq_{j,i}^{\\text{obs}} = q(c,\\phi,\\sigma_{3,j,i}) + \\epsilon_{j,i}, \\quad \\epsilon_{j,i} \\sim \\mathcal{N}(0, s_j^2).\n$$\n- Prior on $c$: Assume a Lognormal prior specified by $\\log c \\sim \\mathcal{N}(\\mu_{c,j}, \\sigma_{c,j}^2)$, which enforces $c > 0$.\n- Prior on $\\phi$: Assume a Gaussian prior $\\phi \\sim \\mathcal{N}(\\mu_{\\phi,j}, \\sigma_{\\phi,j}^2)$ truncated to a physically plausible interval $[\\phi_{\\min,j}, \\phi_{\\max,j}]$ in degrees. Treat $(c,\\phi)$ a priori independent.\n\nYour task:\n- For each dataset in the test suite, compute the MAP estimate $(c_{\\text{MAP}}, \\phi_{\\text{MAP}})$ by numerically maximizing the posterior density over $c > 0$ and $\\phi \\in [\\phi_{\\min}, \\phi_{\\max}]$. If you implement in terms of minimizing the negative log-posterior, ensure appropriate handling of bounds and numerical stability.\n- Express the final values of $c_{\\text{MAP}}$ in kPa and $\\phi_{\\text{MAP}}$ in degrees. Angles must be in degrees in the final output. No unit conversion is needed for inputs as they are already provided in kPa and degrees.\n\nTest suite:\n- Dataset $1$ (sand-like, moderate cohesion):\n    - Confining pressures $\\sigma_{3,1} = [$ $100$, $300$, $600$ $]$ kPa.\n    - Observed peak deviatoric stresses $q_{1}^{\\text{obs}} = [$ $304.0$, $704.0$, $1302.0$ $]$ kPa.\n    - Observation noise standard deviation $s_1 = $ $20.0$ kPa.\n    - Prior hyperparameters: $\\mu_{c,1} = \\log($ $25.0$ $)$, $\\sigma_{c,1} = $ $0.4$; $\\mu_{\\phi,1} = $ $32.0$ degrees, $\\sigma_{\\phi,1} = $ $5.0$ degrees; truncation bounds $[\\phi_{\\min,1}, \\phi_{\\max,1}] = [$ $15.0$, $45.0$ $]$ degrees.\n- Dataset $2$ (clean sand, near-zero cohesion):\n    - Confining pressures $\\sigma_{3,2} = [$ $50$, $150$, $300$ $]$ kPa.\n    - Observed peak deviatoric stresses $q_{2}^{\\text{obs}} = [$ $154.0$, $420.0$, $832.0$ $]$ kPa.\n    - Observation noise standard deviation $s_2 = $ $15.0$ kPa.\n    - Prior hyperparameters: $\\mu_{c,2} = \\log($ $3.0$ $)$, $\\sigma_{c,2} = $ $0.5$; $\\mu_{\\phi,2} = $ $35.0$ degrees, $\\sigma_{\\phi,2} = $ $3.0$ degrees; truncation bounds $[\\phi_{\\min,2}, \\phi_{\\max,2}] = [$ $20.0$, $50.0$ $]$ degrees.\n- Dataset $3$ (clay-like, higher cohesion, lower friction angle, weaker data):\n    - Confining pressures $\\sigma_{3,3} = [$ $50$, $150$, $300$ $]$ kPa.\n    - Observed peak deviatoric stresses $q_{3}^{\\text{obs}} = [$ $340.0$, $430.0$, $610.0$ $]$ kPa.\n    - Observation noise standard deviation $s_3 = $ $50.0$ kPa.\n    - Prior hyperparameters: $\\mu_{c,3} = \\log($ $120.0$ $)$, $\\sigma_{c,3} = $ $0.3$; $\\mu_{\\phi,3} = $ $18.0$ degrees, $\\sigma_{\\phi,3} = $ $4.0$ degrees; truncation bounds $[\\phi_{\\min,3}, \\phi_{\\max,3}] = [$ $12.0$, $35.0$ $]$ degrees.\n\nAlgorithmic and numerical requirements:\n- Use a robust numerical optimizer with bound handling on $c$ and $\\phi$. Use multiple initial guesses derived from prior hyperparameters to mitigate local optima.\n- Implement the log-likelihood, log-prior for $c$ (Lognormal) and $\\phi$ (Gaussian truncated over the given interval), and the resulting log-posterior. You may omit additive constants that do not depend on $(c,\\phi)$ since they do not affect the MAP.\n- Ensure $\\phi$ is converted to radians when used in trigonometric functions and kept in degrees for output.\n- Enforce $c > 0$ and $\\phi_{\\min} \\le \\phi \\le \\phi_{\\max}$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each dataset’s output should be a two-element list $[c_{\\text{MAP}}, \\phi_{\\text{MAP}}]$ with $c_{\\text{MAP}}$ in kPa and $\\phi_{\\text{MAP}}$ in degrees, both rounded to three decimal places. For example, the output must look like\n[$[$ $c_1$ $,$ $\\phi_1$ $]$ $,$ $[$ $c_2$ $,$ $\\phi_2$ $]$ $,$ $[$ $c_3$ $,$ $\\phi_3$ $]$]\nwith actual numeric values substituted.\n\nYour program must be fully self-contained, require no user input, and strictly adhere to the specified execution environment. The final answers produced by your program must be floats or lists of floats as specified.",
            "solution": "The problem requires finding the Maximum A Posteriori (MAP) estimate of the Mohr–Coulomb parameters—cohesion $c$ and friction angle $\\phi$—given experimental data from triaxial tests. This is achieved by maximizing the posterior probability density function $p(c, \\phi | \\text{data})$, which, according to Bayes' rule, is proportional to the product of the likelihood and the prior probability:\n\n$$\np(c, \\phi | \\text{data}) \\propto p(\\text{data} | c, \\phi) \\cdot p(c, \\phi)\n$$\n\nThe parameters $c$ and $\\phi$ are assumed to be a priori independent, so $p(c, \\phi) = p(c) \\cdot p(\\phi)$. Maximizing the posterior density is equivalent to maximizing its logarithm, the log-posterior, or minimizing its negative, the negative log-posterior. The latter is often preferred for numerical stability. The negative log-posterior, which we will use as our objective function $F(c, \\phi)$ to minimize, is:\n\n$$\nF(c, \\phi) = -\\log[p(\\text{data} | c, \\phi)] - \\log[p(c)] - \\log[p(\\phi)]\n$$\n\nLet us define each term for a generic dataset $j$. The parameters to be estimated are $(c, \\phi)$, and the data consists of $n_j$ pairs of observed peak deviatoric stresses $\\{q_{j,i}^{\\text{obs}}\\}$ and corresponding confining pressures $\\{\\sigma_{3,j,i}\\}$.\n\n**1. The Likelihood Term**\n\nThe model for the predicted peak deviatoric stress is given by the Mohr-Coulomb failure criterion:\n$$\nq_{\\text{pred}}(c, \\phi, \\sigma_3) = \\frac{2c\\cos\\phi_r}{1 - \\sin\\phi_r} + \\frac{2\\sigma_3\\sin\\phi_r}{1 - \\sin\\phi_r}\n$$\nwhere $\\phi_r = \\phi \\cdot \\pi/180$ is the friction angle in radians.\n\nObservations are assumed to be corrupted by independent and identically distributed Gaussian noise, $q_{j,i}^{\\text{obs}} = q_{\\text{pred}}(c, \\phi, \\sigma_{3,j,i}) + \\epsilon_{j,i}$, where $\\epsilon_{j,i} \\sim \\mathcal{N}(0, s_j^2)$. The probability density for a single observation $q_{j,i}^{\\text{obs}}$ is:\n$$\np(q_{j,i}^{\\text{obs}} | c, \\phi) = \\frac{1}{\\sqrt{2\\pi s_j^2}} \\exp\\left(-\\frac{(q_{j,i}^{\\text{obs}} - q_{\\text{pred}}(c, \\phi, \\sigma_{3,j,i}))^2}{2s_j^2}\\right)\n$$\nSince observations are independent, the total likelihood is the product of the individual densities. The log-likelihood is the sum of the individual log-densities:\n$$\n\\log[p(\\text{data}_j | c, \\phi)] = \\sum_{i=1}^{n_j} \\left( -\\frac{1}{2}\\log(2\\pi s_j^2) - \\frac{(q_{j,i}^{\\text{obs}} - q_{\\text{pred}}(c, \\phi, \\sigma_{3,j,i}))^2}{2s_j^2} \\right)\n$$\nFor optimization, we can drop the constant term $-\\frac{n_j}{2}\\log(2\\pi s_j^2)$. The negative log-likelihood term to be minimized is proportional to the sum of squared errors:\n$$\n-\\log[p(\\text{data}_j | c, \\phi)] \\propto \\frac{1}{2s_j^2} \\sum_{i=1}^{n_j} (q_{j,i}^{\\text{obs}} - q_{\\text{pred}}(c, \\phi, \\sigma_{3,j,i}))^2\n$$\n\n**2. The Prior Term for Cohesion $c$**\n\nThe cohesion $c$ is assigned a Lognormal prior, specified by $\\log c \\sim \\mathcal{N}(\\mu_{c,j}, \\sigma_{c,j}^2)$. The probability density function for $c$ is:\n$$\np(c) = \\frac{1}{c\\sigma_{c,j}\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\log c - \\mu_{c,j})^2}{2\\sigma_{c,j}^2}\\right) \\quad \\text{for } c > 0\n$$\nThe log-prior is:\n$$\n\\log[p(c)] = -\\log c - \\frac{1}{2}\\log(2\\pi\\sigma_{c,j}^2) - \\frac{(\\log c - \\mu_{c,j})^2}{2\\sigma_{c,j}^2}\n$$\nDropping the constant term $-\\frac{1}{2}\\log(2\\pi\\sigma_{c,j}^2)$, the negative log-prior term for $c$ to be minimized is:\n$$\n-\\log[p(c)] \\propto \\log c + \\frac{(\\log c - \\mu_{c,j})^2}{2\\sigma_{c,j}^2}\n$$\n\n**3. The Prior Term for Friction Angle $\\phi$**\n\nThe friction angle $\\phi$ is given a Gaussian prior, $\\phi \\sim \\mathcal{N}(\\mu_{\\phi,j}, \\sigma_{\\phi,j}^2)$, truncated to the interval $[\\phi_{\\min,j}, \\phi_{\\max,j}]$. The probability density function is:\n$$\np(\\phi) = \\begin{cases} \\frac{1}{Z} \\frac{1}{\\sigma_{\\phi,j}\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\phi - \\mu_{\\phi,j})^2}{2\\sigma_{\\phi,j}^2}\\right) & \\text{if } \\phi \\in [\\phi_{\\min,j}, \\phi_{\\max,j}] \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nwhere $Z$ is a normalization constant that ensures the density integrates to $1$ over the interval. Since $Z$ does not depend on $\\phi$, it is an additive constant in the log-posterior and can be neglected during optimization. The negative log-prior term to be minimized is thus:\n$$\n-\\log[p(\\phi)] \\propto \\frac{(\\phi - \\mu_{\\phi,j})^2}{2\\sigma_{\\phi,j}^2}\n$$\nThe truncation constraint $\\phi \\in [\\phi_{\\min,j}, \\phi_{\\max,j}]$ will be enforced as bounds in the numerical optimization.\n\n**4. The Complete Objective Function and Optimization Strategy**\n\nCombining all terms, the complete objective function $F(c, \\phi)$ to be minimized for each dataset $j$ is:\n$$\nF(c, \\phi) = \\frac{1}{2s_j^2} \\sum_{i=1}^{n_j} (q_{j,i}^{\\text{obs}} - q_{\\text{pred}}(c, \\phi, \\sigma_{3,j,i}))^2 + \\log c + \\frac{(\\log c - \\mu_{c,j})^2}{2\\sigma_{c,j}^2} + \\frac{(\\phi - \\mu_{\\phi,j})^2}{2\\sigma_{\\phi,j}^2}\n$$\nThis minimization is performed subject to the box constraints $c > 0$ and $\\phi_{\\min,j} \\le \\phi \\le \\phi_{\\max,j}$. For numerical implementation, the lower bound for $c$ is set to a small positive value (e.g., $10^{-9}$) to avoid evaluating $\\log(0)$, while the upper bound can be left unconstrained.\n\nWe will use a numerical optimization algorithm capable of handling box constraints, specifically the L-BFGS-B algorithm available in `scipy.optimize.minimize`. To increase the likelihood of finding the global minimum, we will start the optimization from multiple initial points. These points are chosen systematically based on the prior distributions: one at the prior means $(\\exp(\\mu_{c,j}), \\mu_{\\phi,j})$, and others at points one standard deviation away, ensuring they are within the specified bounds. The solution $(c_{\\text{MAP}}, \\phi_{\\text{MAP}})$ is the one that yields the smallest value of the objective function $F(c, \\phi)$ among all attempts. The final results are rounded and presented in the specified format.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Performs Bayesian MAP estimation of Mohr-Coulomb parameters for three datasets.\n    \"\"\"\n\n    # Define the three test cases as specified in the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Dataset 1\",\n            \"sigma3\": np.array([100.0, 300.0, 600.0]),\n            \"q_obs\": np.array([304.0, 704.0, 1302.0]),\n            \"s\": 20.0,\n            \"mu_c\": np.log(25.0),\n            \"sigma_c\": 0.4,\n            \"mu_phi\": 32.0,\n            \"sigma_phi\": 5.0,\n            \"phi_bounds\": (15.0, 45.0),\n        },\n        {\n            \"name\": \"Dataset 2\",\n            \"sigma3\": np.array([50.0, 150.0, 300.0]),\n            \"q_obs\": np.array([154.0, 420.0, 832.0]),\n            \"s\": 15.0,\n            \"mu_c\": np.log(3.0),\n            \"sigma_c\": 0.5,\n            \"mu_phi\": 35.0,\n            \"sigma_phi\": 3.0,\n            \"phi_bounds\": (20.0, 50.0),\n        },\n        {\n            \"name\": \"Dataset 3\",\n            \"sigma3\": np.array([50.0, 150.0, 300.0]),\n            \"q_obs\": np.array([340.0, 430.0, 610.0]),\n            \"s\": 50.0,\n            \"mu_c\": np.log(120.0),\n            \"sigma_c\": 0.3,\n            \"mu_phi\": 18.0,\n            \"sigma_phi\": 4.0,\n            \"phi_bounds\": (12.0, 35.0),\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Extract data for the current case for clarity\n        sigma3_data = case[\"sigma3\"]\n        q_obs_data = case[\"q_obs\"]\n        s = case[\"s\"]\n        mu_c = case[\"mu_c\"]\n        sigma_c = case[\"sigma_c\"]\n        mu_phi = case[\"mu_phi\"]\n        sigma_phi = case[\"sigma_phi\"]\n        phi_min, phi_max = case[\"phi_bounds\"]\n\n        def mohr_coulomb_model(c, phi_deg, sigma3):\n            \"\"\"\n            Calculates predicted deviatoric stress q based on Mohr-Coulomb.\n            c: cohesion (kPa)\n            phi_deg: friction angle (degrees)\n            sigma3: confining pressure (kPa)\n            \"\"\"\n            phi_rad = np.deg2rad(phi_deg)\n            sin_phi = np.sin(phi_rad)\n            cos_phi = np.cos(phi_rad)\n            \n            # Avoid division by zero if phi is 90 degrees\n            if np.isclose(sin_phi, 1.0):\n                return np.inf\n            \n            denominator = 1.0 - sin_phi\n            \n            q_pred = (2 * c * cos_phi + 2 * sigma3 * sin_phi) / denominator\n            return q_pred\n\n        def neg_log_posterior(params):\n            \"\"\"\n            Objective function: negative log-posterior probability.\n            params: a list or array [c, phi]\n            \"\"\"\n            c, phi = params\n\n            # Parameter bounds check (though handled by optimizer, this adds robustness)\n            if c <= 0 or not (phi_min <= phi <= phi_max):\n                return np.inf\n\n            # 1. Negative Log-Likelihood\n            q_pred = mohr_coulomb_model(c, phi, sigma3_data)\n            sse = np.sum((q_obs_data - q_pred)**2)\n            neg_log_likelihood = sse / (2 * s**2)\n\n            # 2. Negative Log-Prior for c (Lognormal)\n            log_c = np.log(c)\n            neg_log_prior_c = log_c + ((log_c - mu_c)**2) / (2 * sigma_c**2)\n            \n            # 3. Negative Log-Prior for phi (Truncated Normal)\n            neg_log_prior_phi = ((phi - mu_phi)**2) / (2 * sigma_phi**2)\n\n            return neg_log_likelihood + neg_log_prior_c + neg_log_prior_phi\n\n        # Define bounds for the optimizer\n        # c > 0 (practically > a small epsilon)\n        # phi_min <= phi <= phi_max\n        bounds = [(1e-9, None), (phi_min, phi_max)]\n\n        # Set up multiple initial guesses to improve robustness against local minima\n        initial_guesses = []\n        # Guess 1: Prior mean\n        c0_mean = np.exp(mu_c)\n        phi0_mean = mu_phi\n        initial_guesses.append([c0_mean, phi0_mean])\n        \n        # Additional guesses based on prior standard deviation\n        c0_plus_std = np.exp(mu_c + sigma_c)\n        phi0_plus_std = np.clip(mu_phi + sigma_phi, phi_min, phi_max)\n        initial_guesses.append([c0_plus_std, phi0_plus_std])\n        \n        c0_minus_std = np.exp(mu_c - sigma_c)\n        phi0_minus_std = np.clip(mu_phi - sigma_phi, phi_min, phi_max)\n        initial_guesses.append([c0_minus_std, phi0_minus_std])\n\n        best_result = None\n        min_f_val = np.inf\n\n        for x0 in initial_guesses:\n            res = minimize(\n                neg_log_posterior,\n                x0=x0,\n                method='L-BFGS-B',\n                bounds=bounds\n            )\n            if res.success and res.fun < min_f_val:\n                min_f_val = res.fun\n                best_result = res.x\n\n        # Store the MAP estimate for c and phi, rounded to three decimal places\n        if best_result is not None:\n            c_map = round(best_result[0], 3)\n            phi_map = round(best_result[1], 3)\n            results.append([c_map, phi_map])\n        else:\n            # Fallback in case of optimization failure, though unlikely with this setup\n            results.append([np.nan, np.nan])\n\n    # Format the output as per the specification\n    # e.g., [[c1,phi1],[c2,phi2],[c3,phi3]]\n    output_str = \"[\" + \",\".join([f\"[{c},{p}]\" for c, p in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}