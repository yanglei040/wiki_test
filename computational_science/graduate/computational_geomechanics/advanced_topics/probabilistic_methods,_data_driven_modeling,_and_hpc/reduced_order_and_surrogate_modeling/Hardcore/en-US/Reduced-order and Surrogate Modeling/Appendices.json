{
    "hands_on_practices": [
        {
            "introduction": "The foundation of Proper Orthogonal Decomposition (POD) lies in its ability to provide an optimal low-rank approximation of a dataset. A critical first step in building any POD-based reduced-order model is deciding how many basis modes, or vectors, to retain. This practice guides you through the analytical derivation of the truncation error, directly linking the singular value spectrum of your data to the fidelity of your reduced model .",
            "id": "3555778",
            "problem": "A high-fidelity finite element model in computational geomechanics generates a snapshot matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times m}$ from parametric simulations of poroelastic consolidation in a stratified soil layer. Consider the Proper Orthogonal Decomposition (POD) basis constructed from the Singular Value Decomposition (SVD) of $\\mathbf{X}$, where $\\mathbf{X} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{\\top}$ and the singular values are $\\{\\sigma_i\\}_{i \\ge 1}$ with $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge 0$. Let the rank-$r$ POD projector be $\\mathbf{P}_r = \\mathbf{U}_r \\mathbf{U}_r^{\\top}$, where $\\mathbf{U}_r$ contains the first $r$ left singular vectors.\n\nYou are tasked with selecting the number of modes $r$ to satisfy a prescribed tolerance in the relative projection error. Define the relative projection error in the snapshot Frobenius norm as\n$$\nE_r \\equiv \\frac{\\|\\mathbf{X} - \\mathbf{P}_r \\mathbf{X}\\|_F}{\\|\\mathbf{X}\\|_F}.\n$$\nStarting from the definitions of the Singular Value Decomposition and properties of the Frobenius norm, derive a rule for the minimal $r$ that guarantees $E_r \\le \\epsilon$ for a given tolerance $\\epsilon \\in (0,1)$. Then, under the scientifically plausible and empirically observed decay profile for many geomechanical datasets that the singular values are geometrically decaying,\n$$\n\\sigma_i = \\sigma_1 \\, q^{\\,i-1}, \\quad \\text{with} \\quad \\sigma_1 = 1 \\quad \\text{and} \\quad q = 0.82,\n$$\nassume the sequence extends indefinitely and compute the residual energy\n$$\n\\sum_{ir} \\sigma_i^2\n$$\nfor the tolerance $\\epsilon = 10^{-3}$.\n\nExpress your final answer as the row matrix containing $(r, \\sum_{ir} \\sigma_i^2)$. Round the residual energy to four significant figures. Do not include units in your final answer.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of linear algebra and reduced-order modeling, specifically Proper Orthogonal Decomposition (POD) via Singular Value Decomposition (SVD), which is a standard technique in computational sciences. The problem is well-posed, with all necessary data and definitions provided to arrive at a unique, verifiable solution.\n\nThe first task is to derive a rule for selecting the minimal number of modes, $r$, to satisfy a given tolerance, $\\epsilon$, on the relative projection error, $E_r$. The error is defined as:\n$$\nE_r = \\frac{\\|\\mathbf{X} - \\mathbf{P}_r \\mathbf{X}\\|_F}{\\|\\mathbf{X}\\|_F}\n$$\nIt is more convenient to work with the square of this quantity:\n$$\nE_r^2 = \\frac{\\|\\mathbf{X} - \\mathbf{P}_r \\mathbf{X}\\|_F^2}{\\|\\mathbf{X}\\|_F^2}\n$$\nThe Frobenius norm of a matrix $\\mathbf{A}$ is related to its singular values $\\{\\sigma_i\\}$ by $\\|\\mathbf{A}\\|_F^2 = \\sum_i \\sigma_i^2$. The singular values of the snapshot matrix $\\mathbf{X}$ are given as $\\{\\sigma_i\\}_{i \\ge 1}$. Thus, the squared norm in the denominator is:\n$$\n\\|\\mathbf{X}\\|_F^2 = \\sum_{i=1}^{k} \\sigma_i^2\n$$\nwhere $k = \\text{rank}(\\mathbf{X})$.\n\nFor the numerator, we first analyze the matrix representing the projection error, $\\mathbf{X} - \\mathbf{P}_r \\mathbf{X}$. The SVD of $\\mathbf{X}$ can be written as a sum of rank-$1$ matrices:\n$$\n\\mathbf{X} = \\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^{\\top}\n$$\nwhere $\\mathbf{u}_i$ and $\\mathbf{v}_i$ are the $i$-th left and right singular vectors, respectively. The projector $\\mathbf{P}_r = \\mathbf{U}_r \\mathbf{U}_r^{\\top}$ is the orthogonal projection onto the subspace spanned by the first $r$ left singular vectors, $\\{\\mathbf{u}_1, \\dots, \\mathbf{u}_r\\}$. Applying this projector to a vector $\\mathbf{w}$ is $\\mathbf{P}_r \\mathbf{w} = \\sum_{i=1}^r (\\mathbf{u}_i^{\\top}\\mathbf{w})\\mathbf{u}_i$. Applying the projector to the matrix $\\mathbf{X}$ yields:\n$$\n\\mathbf{P}_r \\mathbf{X} = \\left(\\sum_{j=1}^r \\mathbf{u}_j \\mathbf{u}_j^{\\top}\\right) \\left(\\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^{\\top}\\right) = \\sum_{j=1}^r \\sum_{i=1}^k \\sigma_i (\\mathbf{u}_j \\mathbf{u}_j^{\\top} \\mathbf{u}_i) \\mathbf{v}_i^{\\top}\n$$\nDue to the orthonormality of the singular vectors, $\\mathbf{u}_j^{\\top} \\mathbf{u}_i = \\delta_{ij}$ (the Kronecker delta). The expression simplifies to:\n$$\n\\mathbf{P}_r \\mathbf{X} = \\sum_{i=1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^{\\top}\n$$\nThis is the well-known Eckart-Young-Mirsky theorem result, which states that $\\mathbf{P}_r\\mathbf{X}$ is the best rank-$r$ approximation of $\\mathbf{X}$ in the Frobenius norm. The error matrix is then:\n$$\n\\mathbf{X} - \\mathbf{P}_r \\mathbf{X} = \\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^{\\top} - \\sum_{i=1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^{\\top} = \\sum_{i=r+1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^{\\top}\n$$\nThe expression on the right is itself an SVD, where the singular values are $\\{\\sigma_{r+1}, \\sigma_{r+2}, \\dots, \\sigma_k\\}$. Therefore, the squared Frobenius norm of the error matrix is:\n$$\n\\|\\mathbf{X} - \\mathbf{P}_r \\mathbf{X}\\|_F^2 = \\sum_{i=r+1}^k \\sigma_i^2\n$$\nThe problem states to assume the singular value sequence extends indefinitely, so we replace the finite upper limit $k$ with $\\infty$. The squared relative error becomes:\n$$\nE_r^2 = \\frac{\\sum_{i=r+1}^\\infty \\sigma_i^2}{\\sum_{i=1}^\\infty \\sigma_i^2}\n$$\nThe condition for choosing $r$ is $E_r \\le \\epsilon$, which is equivalent to $E_r^2 \\le \\epsilon^2$. So, the rule for the minimal $r$ is to find the smallest integer $r$ that satisfies:\n$$\n\\frac{\\sum_{i=r+1}^\\infty \\sigma_i^2}{\\sum_{i=1}^\\infty \\sigma_i^2} \\le \\epsilon^2\n$$\nNow, we apply this rule to the specified singular value decay profile:\n$$\n\\sigma_i = \\sigma_1 q^{i-1} \\quad \\text{with} \\quad \\sigma_1 = 1 \\quad \\text{and} \\quad q = 0.82\n$$\nThe squared singular values are $\\sigma_i^2 = (\\sigma_1^2) (q^2)^{i-1} = (q^2)^{i-1}$ since $\\sigma_1=1$. The sums are infinite geometric series. The total sum (denominator) is:\n$$\n\\sum_{i=1}^\\infty \\sigma_i^2 = \\sum_{i=1}^\\infty (q^2)^{i-1} = \\sum_{j=0}^\\infty (q^2)^j = \\frac{1}{1-q^2}\n$$\nThis series converges because $|q| = 0.82  1$, so $q^2  1$. The sum of the truncated tail (numerator) is:\n$$\n\\sum_{i=r+1}^\\infty \\sigma_i^2 = \\sum_{i=r+1}^\\infty (q^2)^{i-1} = (q^2)^r + (q^2)^{r+1} + \\dots = \\frac{(q^2)^r}{1-q^2}\n$$\nSubstituting these into the inequality for $r$:\n$$\n\\frac{\\frac{(q^2)^r}{1-q^2}}{\\frac{1}{1-q^2}} \\le \\epsilon^2 \\implies (q^2)^r \\le \\epsilon^2\n$$\nTaking the square root of both sides (all quantities are positive) gives a simpler relation:\n$$\nq^r \\le \\epsilon\n$$\nWe need to find the smallest integer $r$ satisfying this. Taking the natural logarithm of both sides:\n$$\nr \\ln(q) \\le \\ln(\\epsilon)\n$$\nSince $q=0.82  1$, $\\ln(q)$ is negative. Dividing by $\\ln(q)$ reverses the inequality direction:\n$$\nr \\ge \\frac{\\ln(\\epsilon)}{\\ln(q)}\n$$\nWe are given $\\epsilon = 10^{-3}$ and $q = 0.82$.\n$$\nr \\ge \\frac{\\ln(10^{-3})}{\\ln(0.82)} = \\frac{-3 \\ln(10)}{\\ln(0.82)}\n$$\nUsing numerical values $\\ln(10) \\approx 2.302585$ and $\\ln(0.82) \\approx -0.198451$:\n$$\nr \\ge \\frac{-3 \\times 2.302585}{-0.198451} \\approx \\frac{-6.907755}{-0.198451} \\approx 34.808\n$$\nSince $r$ must be an integer, the minimal value is $r = \\lceil 34.808 \\rceil = 35$.\n\nThe second task is to compute the residual energy $\\sum_{ir} \\sigma_i^2$ for this value of $r$. Since $r=35$, this is the sum $\\sum_{i=36}^\\infty \\sigma_i^2$. This is the numerator term we calculated earlier, evaluated at $r=35$.\n$$\n\\sum_{i35} \\sigma_i^2 = \\sum_{i=36}^\\infty \\sigma_i^2 = \\frac{(q^2)^{35}}{1-q^2}\n$$\nWe have $q=0.82$, so $q^2 = 0.82^2 = 0.6724$.\n$$\n\\sum_{i35} \\sigma_i^2 = \\frac{(0.6724)^{35}}{1-0.6724} = \\frac{(0.6724)^{35}}{0.3276}\n$$\nCalculating the value:\n$$\n(0.6724)^{35} \\approx 9.245235 \\times 10^{-7}\n$$\n$$\n\\sum_{i35} \\sigma_i^2 \\approx \\frac{9.245235 \\times 10^{-7}}{0.3276} \\approx 2.8221096 \\times 10^{-6}\n$$\nRounding to four significant figures, the residual energy is $2.822 \\times 10^{-6}$.\n\nThe final answer is the row matrix $(r, \\sum_{ir} \\sigma_i^2)$.\nThe two components are $r=35$ and the residual energy $2.822 \\times 10^{-6}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n35  2.822 \\times 10^{-6}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond simply compressing data, reduced-order models can offer profound computational advantages, especially for dynamic systems. This exercise explores one of the most significant benefits for explicit time integration schemes: the relaxation of the stability time step constraint. By analyzing wave propagation in a soil column, you will quantify how truncating high-frequency modes allows a ROM to take much larger time steps than its full-order counterpart, leading to dramatic speedups .",
            "id": "3555710",
            "problem": "A one-dimensional saturated soil column is modeled by Biot-type dynamics. After a standard mixed finite element semi-discretization in space and a block-static condensation of the fluid variable to isolate the wave-propagatory part, the undamped semi-discrete equations for the solid displacement degrees of freedom can be written in the second-order form\n$$\nM \\, \\ddot{u}(t) + K \\, u(t) = 0,\n$$\nwhere $M \\in \\mathbb{R}^{N \\times N}$ is symmetric positive definite and $K \\in \\mathbb{R}^{N \\times N}$ is symmetric positive semidefinite. Consider explicit central difference time integration applied to the homogeneous system above. Let the generalized eigenpairs of the pair $(K,M)$ solve\n$$\nK \\, \\phi_i = \\lambda_i \\, M \\, \\phi_i, \\quad i=1,\\dots,N,\n$$\nwith $0  \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_N$. Assume the eigenvectors are $M$-orthonormal, so $\\phi_i^{\\top} M \\phi_j = \\delta_{ij}$.\n\nA reduced-order model is constructed by a Galerkin projection onto the subspace spanned by the first $r$ generalized eigenvectors, with $r  N$, forming the reduced basis matrix $V = [\\phi_1,\\dots,\\phi_r] \\in \\mathbb{R}^{N \\times r}$ satisfying $V^{\\top} M V = I_r$. The reduced operators are $M_r = V^{\\top} M V = I_r$ and $K_r = V^{\\top} K V$, and the reduced model is $M_r \\ddot{q}(t) + K_r q(t) = 0$.\n\nStarting from fundamental principles for central difference time integration and modal analysis of second-order linear systems, derive the stability time step constraint for the full model and the reduced model, expressed in terms of the largest generalized eigenvalue retained in each case. Then, using the following full-order generalized eigenvalues (in increasing order),\n$$\n\\lambda_1 = 1.44 \\times 10^{6}, \\quad\n\\lambda_2 = 2.25 \\times 10^{6}, \\quad\n\\lambda_3 = 1.00 \\times 10^{7}, \\quad\n\\lambda_4 = 4.00 \\times 10^{7}, \\quad\n\\lambda_5 = 1.60 \\times 10^{8}, \\quad\n\\lambda_6 = 3.60 \\times 10^{8}, \\quad\n\\lambda_7 = 6.25 \\times 10^{8}, \\quad\n\\lambda_8 = 9.00 \\times 10^{8},\n$$\nand taking $r=5$, compute the ratio\n$$\nR \\equiv \\frac{\\Delta t_{\\text{max,ROM}}}{\\Delta t_{\\text{max,FOM}}}.\n$$\nReport $R$ as a dimensionless number rounded to four significant figures.",
            "solution": "The problem requires the derivation of the stability condition for the explicit central difference time integration scheme applied to a second-order linear system, first for a full-order model (FOM) and then for a reduced-order model (ROM). Subsequently, the ratio of the maximum allowable time steps for the ROM and FOM is to be computed.\n\nThe full-order semi-discrete system is given by:\n$$\nM \\, \\ddot{u}(t) + K \\, u(t) = 0\n$$\nwhere $M$ is symmetric positive definite and $K$ is symmetric positive semidefinite.\n\nThe explicit central difference scheme approximates the second time derivative at time step $t_n$ as:\n$$\n\\ddot{u}(t_n) \\approx \\frac{u_{n+1} - 2u_n + u_{n-1}}{(\\Delta t)^2}\n$$\nwhere $u_n \\equiv u(t_n)$ and $\\Delta t$ is the time step size. Substituting this into the governing equation yields the discrete-time update rule:\n$$\nM \\left( \\frac{u_{n+1} - 2u_n + u_{n-1}}{(\\Delta t)^2} \\right) + K u_n = 0\n$$\nTo analyze the stability of this scheme, we perform a modal analysis. The solution $u(t)$ can be expressed as a linear combination of the generalized eigenvectors $\\phi_i$:\n$$\nu(t) = \\sum_{i=1}^{N} \\eta_i(t) \\phi_i\n$$\nwhere $\\eta_i(t)$ are the modal coordinates. Substituting this modal expansion into the original equation gives:\n$$\nM \\sum_{i=1}^{N} \\ddot{\\eta}_i(t) \\phi_i + K \\sum_{i=1}^{N} \\eta_i(t) \\phi_i = 0\n$$\nPremultiplying by an eigenvector transpose $\\phi_j^{\\top}$ yields:\n$$\n\\sum_{i=1}^{N} \\ddot{\\eta}_i(t) (\\phi_j^{\\top} M \\phi_i) + \\sum_{i=1}^{N} \\eta_i(t) (\\phi_j^{\\top} K \\phi_i) = 0\n$$\nUsing the given $M$-orthonormality condition $\\phi_j^{\\top} M \\phi_i = \\delta_{ij}$ and the definition of the generalized eigenproblem $K \\phi_i = \\lambda_i M \\phi_i$, which implies $\\phi_j^{\\top} K \\phi_i = \\lambda_i \\phi_j^{\\top} M \\phi_i = \\lambda_i \\delta_{ij}$, the system decouples into $N$ independent scalar equations:\n$$\n\\ddot{\\eta}_j(t) + \\lambda_j \\eta_j(t) = 0, \\quad j=1, \\dots, N\n$$\nThe natural frequency of the $j$-th mode is $\\omega_j = \\sqrt{\\lambda_j}$. Applying the central difference scheme to this scalar modal equation gives:\n$$\n\\frac{\\eta_{j, n+1} - 2\\eta_{j, n} + \\eta_{j, n-1}}{(\\Delta t)^2} + \\lambda_j \\eta_{j, n} = 0\n$$\nThis can be rewritten as a linear recurrence relation:\n$$\n\\eta_{j, n+1} - (2 - \\lambda_j (\\Delta t)^2) \\eta_{j, n} + \\eta_{j, n-1} = 0\n$$\nFor the numerical solution to be stable (i.e., not grow unboundedly), the magnitude of the roots of the characteristic polynomial of this recurrence must be less than or equal to $1$. Assuming a solution of the form $\\eta_{j,n} = G^n$, we obtain the characteristic equation:\n$$\nG^2 - (2 - \\lambda_j (\\Delta t)^2) G + 1 = 0\n$$\nFor the roots $G$ to have magnitude $|G| \\le 1$, the discriminant of this quadratic equation must be non-positive, which ensures the roots are complex conjugates on the unit circle.\n$$\n(2 - \\lambda_j (\\Delta t)^2)^2 - 4 \\le 0\n$$\nThis inequality is equivalent to:\n$$\n-2 \\le 2 - \\lambda_j (\\Delta t)^2 \\le 2\n$$\nThe right side, $2 - \\lambda_j (\\Delta t)^2 \\le 2$, implies $-\\lambda_j (\\Delta t)^2 \\le 0$, which is always true since $\\lambda_j  0$. The left side imposes the stability constraint:\n$$\n-2 \\le 2 - \\lambda_j (\\Delta t)^2 \\implies \\lambda_j (\\Delta t)^2 \\le 4\n$$\nThis condition must hold for all modes, so it is governed by the largest eigenvalue, $\\lambda_N$:\n$$\n\\lambda_N (\\Delta t)^2 \\le 4 \\implies \\Delta t \\le \\frac{2}{\\sqrt{\\lambda_N}}\n$$\nThe maximum stable time step for the full-order model (FOM) is thus:\n$$\n\\Delta t_{\\text{max,FOM}} = \\frac{2}{\\sqrt{\\lambda_N}}\n$$\nNow, we consider the reduced-order model (ROM). The ROM is constructed via a Galerkin projection onto the subspace spanned by the first $r$ eigenvectors, $V = [\\phi_1, \\dots, \\phi_r]$. The reduced system is:\n$$\nM_r \\ddot{q}(t) + K_r q(t) = 0\n$$\nwhere $M_r = V^{\\top} M V$ and $K_r = V^{\\top} K V$. Based on the $M$-orthonormality of the eigenvectors, the reduced mass matrix is the identity matrix of size $r$:\n$$\n(M_r)_{ij} = \\phi_i^{\\top} M \\phi_j = \\delta_{ij} \\implies M_r = I_r\n$$\nThe reduced stiffness matrix $K_r$ becomes diagonal:\n$$\n(K_r)_{ij} = \\phi_i^{\\top} K \\phi_j = \\phi_i^{\\top} (\\lambda_j M \\phi_j) = \\lambda_j (\\phi_i^{\\top} M \\phi_j) = \\lambda_j \\delta_{ij}\n$$\nSo, $K_r = \\text{diag}(\\lambda_1, \\dots, \\lambda_r)$. The reduced system is already decoupled:\n$$\nI_r \\ddot{q}(t) + \\text{diag}(\\lambda_1, \\dots, \\lambda_r) q(t) = 0\n$$\nThis corresponds to $r$ scalar equations:\n$$\n\\ddot{q}_i(t) + \\lambda_i q_i(t) = 0, \\quad i=1, \\dots, r\n$$\nThe stability analysis for the ROM is identical to that for a single mode of the FOM. The stability condition must hold for all retained modes, and so it is limited by the largest retained eigenvalue, $\\lambda_r$:\n$$\n\\lambda_r (\\Delta t)^2 \\le 4 \\implies \\Delta t \\le \\frac{2}{\\sqrt{\\lambda_r}}\n$$\nThe maximum stable time step for the ROM is:\n$$\n\\Delta t_{\\text{max,ROM}} = \\frac{2}{\\sqrt{\\lambda_r}}\n$$\nWe are asked to compute the ratio $R \\equiv \\frac{\\Delta t_{\\text{max,ROM}}}{\\Delta t_{\\text{max,FOM}}}$.\n$$\nR = \\frac{2 / \\sqrt{\\lambda_r}}{2 / \\sqrt{\\lambda_N}} = \\sqrt{\\frac{\\lambda_N}{\\lambda_r}}\n$$\nThe problem provides a list of $8$ eigenvalues, implying the full model dimension is $N=8$. The largest eigenvalue is $\\lambda_N = \\lambda_8 = 9.00 \\times 10^8$. The ROM is constructed with $r=5$, so the largest retained eigenvalue is $\\lambda_r = \\lambda_5 = 1.60 \\times 10^8$.\n\nSubstituting these values into the expression for $R$:\n$$\nR = \\sqrt{\\frac{9.00 \\times 10^8}{1.60 \\times 10^8}} = \\sqrt{\\frac{9.00}{1.60}} = \\sqrt{5.625}\n$$\nCalculating the numerical value:\n$$\nR \\approx 2.371708245...\n$$\nRounding to four significant figures, we get $R = 2.372$.",
            "answer": "$$\n\\boxed{2.372}\n$$"
        },
        {
            "introduction": "A standard Galerkin projection is not always sufficient to create a stable and accurate reduced-order model, particularly for transport phenomena where advection dominates diffusion. In such cases, the resulting ROM can exhibit spurious oscillations and become physically meaningless. This hands-on coding problem challenges you to implement and compare two Petrov-Galerkin stabilization strategies—Least-Squares and SUPG-inspired—to overcome this instability, demonstrating a crucial aspect of advanced ROM development for complex geomechanical processes .",
            "id": "3555712",
            "problem": "Consider one-dimensional steady contaminant transport in a saturated porous medium along a line segment with coordinate $x \\in [0,1]$. The unknown is the concentration field $c(x)$, governed by advection and diffusion with constant velocity $u  0$ and constant diffusivity $\\kappa  0$. The governing equation and boundary conditions are\n$$ -\\dfrac{d}{dx}\\left(\\kappa \\dfrac{dc}{dx}\\right) + u \\dfrac{dc}{dx} = s(x) \\quad \\text{for } x \\in (0,1), \\qquad c(0) = 1, \\quad c(1) = 0, $$\nwith a source term $s(x) \\equiv 0$. This describes advection-dominated contaminant transport when the cell Peclet number is large.\n\nYour task is to develop two Petrov–Galerkin stabilized reduced-order models starting from a finite-difference full-order discretization and to compare them quantitatively. Use the following fundamental base:\n- Conservation of mass for a passive scalar in a steady state.\n- The definition of the advection–diffusion operator and standard weak form obtained from multiplying by a test function and integrating by parts.\n- The concept of Galerkin and Petrov–Galerkin projection onto low-dimensional trial and test spaces.\n- Proper Orthogonal Decomposition (POD) via Singular Value Decomposition (SVD) to construct a reduced trial basis from solution snapshots.\n\nDiscretize the operator on a uniform grid with $N$ interior nodes, grid spacing $h = \\dfrac{1}{N+1}$, and impose the Dirichlet boundary conditions strongly. Use a central difference approximation for diffusion and a first-order upwind approximation for advection consistent with $u  0$, to assemble a linear system $A \\, \\mathbf{c} = \\mathbf{b}$ for the interior unknown vector $\\mathbf{c} \\in \\mathbb{R}^{N}$, where $\\mathbf{b}$ includes contributions from the boundary conditions.\n\nConstruct a reduced trial basis $\\Phi \\in \\mathbb{R}^{N \\times k}$ using POD on a snapshot matrix whose columns are full-order solutions corresponding to the following training parameter pairs $(u,\\kappa)$:\n- $u \\in \\{2,5,10\\}$,\n- $\\kappa \\in \\{10^{-3}, 5 \\times 10^{-3}, 10^{-2}\\}$,\nthat is, a total of $9$ snapshots. Use the first $k$ left singular vectors as columns of $\\Phi$.\n\nDevelop two Petrov–Galerkin reduced-order models for a given parameter pair $(u,\\kappa)$:\n- A least-squares Petrov–Galerkin model that minimizes the discrete algebraic residual in the Euclidean norm over the reduced trial space. Derive the reduced system from first principles based on the normal equations that enforce optimality of the residual minimization.\n- A Streamline-Upwind/Petrov–Galerkin (SUPG)-inspired reduced model where the reduced test space modifies each test function by adding a streamline term proportional to the directional derivative along the flow. Specifically, use a test space that augments each test function by a multiple of the directional derivative operator consistent with advection in the positive $x$-direction. The stabilization parameter must be chosen based on dimensional analysis and the Courant-like requirement for one-dimensional steady advection–diffusion on a uniform grid, expressed in terms of $h$ and $u$.\n\nFor both reduced models, assemble and solve the reduced systems to obtain reduced solutions $\\mathbf{c}_{\\mathrm{LS}}$ (least-squares Petrov–Galerkin) and $\\mathbf{c}_{\\mathrm{SUPG}}$ (SUPG-inspired). For each, compute the relative discrete $L^{2}$ error with respect to the full-order solution $\\mathbf{c}_{\\mathrm{FOM}}$,\n$$ e_{\\mathrm{LS}} = \\dfrac{\\left(\\, h \\sum_{i=1}^{N} (c_{\\mathrm{LS},i} - c_{\\mathrm{FOM},i})^{2} \\,\\right)^{1/2}}{\\left(\\, h \\sum_{i=1}^{N} c_{\\mathrm{FOM},i}^{2} \\,\\right)^{1/2}}, \\qquad e_{\\mathrm{SUPG}} = \\dfrac{\\left(\\, h \\sum_{i=1}^{N} (c_{\\mathrm{SUPG},i} - c_{\\mathrm{FOM},i})^{2} \\,\\right)^{1/2}}{\\left(\\, h \\sum_{i=1}^{N} c_{\\mathrm{FOM},i}^{2} \\,\\right)^{1/2}}. $$\nReport the improvement factor defined as the ratio $r = \\dfrac{e_{\\mathrm{LS}}}{e_{\\mathrm{SUPG}}}$.\n\nUse the following test suite of parameter values and reduced basis sizes:\n- Test $1$: $u = 5.0$, $\\kappa = 10^{-3}$, $k = 6$.\n- Test $2$: $u = 1.0$, $\\kappa = 10^{-2}$, $k = 6$.\n- Test $3$: $u = 20.0$, $\\kappa = 10^{-4}$, $k = 8$.\n- Test $4$: $u = 10.0$, $\\kappa = 5 \\times 10^{-4}$, $k = 4$.\n\nUse $N = 300$ interior nodes. The source term is $s(x) \\equiv 0$. All computations are dimensionless, so no physical unit conversion is required. Angles do not appear.\n\nYour program must:\n- Assemble the full-order matrix $A$ and vector $\\mathbf{b}$ for each $(u,\\kappa)$ using the specified discretizations and boundary conditions.\n- Build the POD basis $\\Phi$ from the training set and truncate to the specified $k$ for each test.\n- Construct and solve the least-squares Petrov–Galerkin reduced system obtained by residual minimization.\n- Construct and solve the SUPG-inspired Petrov–Galerkin reduced system using a consistent discrete directional derivative operator for the positive $x$-direction and a stabilization parameter expressed in terms of $h$ and $u$.\n- Compute $e_{\\mathrm{LS}}$, $e_{\\mathrm{SUPG}}$, and output the improvement factor $r$ for each test case.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_{1},r_{2},r_{3},r_{4}]$), where $r_{j}$ is the improvement factor for Test $j$ as defined above, represented as floating-point numbers.",
            "solution": "The proposed problem is scientifically and mathematically sound, well-posed, and contains all necessary information for its resolution. It addresses a standard problem in computational transport phenomena—the numerical solution of the steady one-dimensional advection–diffusion equation—and explores advanced model reduction techniques, namely Proper Orthogonal Decomposition (POD) combined with two different Petrov–Galerkin stabilization methods. The formulation is consistent with established principles of numerical analysis and reduced-order modeling. We may therefore proceed with a complete solution.\n\nThe solution process involves the following steps:\n1.  Derivation of the full-order model (FOM) using the specified finite-difference scheme.\n2.  Generation of solution snapshots and construction of the POD basis.\n3.  Formulation of the Least-Squares Petrov–Galerkin (LSPG) reduced-order model (ROM).\n4.  Formulation of the Streamline-Upwind/Petrov–Galerkin (SUPG)-inspired ROM.\n5.  Implementation of the complete numerical procedure to compute the required improvement factors.\n\n**1. Full-Order Model (FOM) Formulation**\n\nThe governing partial differential equation is given for a concentration field $c(x)$ over the domain $x \\in [0, 1]$:\n$$ -\\dfrac{d}{dx}\\left(\\kappa \\dfrac{dc}{dx}\\right) + u \\dfrac{dc}{dx} = 0, \\quad c(0) = 1, \\quad c(1) = 0 $$\nwhere the velocity $u  0$ and diffusivity $\\kappa  0$ are constants.\n\nWe discretize the spatial domain using a uniform grid with $N = 300$ interior nodes $x_i = i h$ for $i=1, \\dots, N$. The grid spacing is $h = \\frac{1}{N+1}$. Let $c_i$ approximate $c(x_i)$. The boundary values are $c_0 = c(0) = 1$ and $c_{N+1} = c(1) = 0$.\n\nAt an interior node $x_i$, we apply the specified finite-difference approximations:\n-   **Diffusion Term (Central Difference)**: $-\\kappa \\dfrac{d^2c}{dx^2}\\Big|_{x_i} \\approx -\\kappa \\dfrac{c_{i+1} - 2c_i + c_{i-1}}{h^2}$\n-   **Advection Term (First-Order Upwind for $u0$)**: $u \\dfrac{dc}{dx}\\Big|_{x_i} \\approx u \\dfrac{c_i - c_{i-1}}{h}$\n\nSubstituting these into the governing equation yields the discrete equation for node $i$:\n$$ -\\kappa \\left(\\dfrac{c_{i+1} - 2c_i + c_{i-1}}{h^2}\\right) + u \\left(\\dfrac{c_i - c_{i-1}}{h}\\right) = 0 $$\nTo assemble the linear system $A \\mathbf{c} = \\mathbf{b}$, where $\\mathbf{c} = [c_1, c_2, \\dots, c_N]^T$ is the vector of unknowns, we rearrange the terms:\n$$ \\left(-\\dfrac{\\kappa}{h^2} - \\dfrac{u}{h}\\right) c_{i-1} + \\left(\\dfrac{2\\kappa}{h^2} + \\dfrac{u}{h}\\right) c_i + \\left(-\\dfrac{\\kappa}{h^2}\\right) c_{i+1} = 0 $$\nThis equation holds for $i=1, \\dots, N$. We incorporate the boundary conditions by moving known terms to the right-hand side.\n\n-   For $i=1$: The term $c_0=1$ is known.\n    $$ \\left(\\dfrac{2\\kappa}{h^2} + \\dfrac{u}{h}\\right) c_1 - \\dfrac{\\kappa}{h^2} c_2 = -\\left(-\\dfrac{\\kappa}{h^2} - \\dfrac{u}{h}\\right) c_0 = \\dfrac{\\kappa}{h^2} + \\dfrac{u}{h} $$\n    This defines the first row of the system. The right-hand side is $b_1 = \\frac{\\kappa}{h^2} + \\frac{u}{h}$.\n\n-   For $i \\in \\{2, \\dots, N-1\\}$: The equation defines the internal rows of the matrix $A$, with the right-hand side being $b_i = 0$.\n\n-   For $i=N$: The term $c_{N+1}=0$ is known.\n    $$ \\left(-\\dfrac{\\kappa}{h^2} - \\dfrac{u}{h}\\right) c_{N-1} + \\left(\\dfrac{2\\kappa}{h^2} + \\dfrac{u}{h}\\right) c_N = \\dfrac{\\kappa}{h^2} c_{N+1} = 0 $$\n    The right-hand side is $b_N = 0$.\n\nThe resulting matrix $A \\in \\mathbb{R}^{N \\times N}$ is a tridiagonal matrix with the following entries:\n-   Main diagonal: $A_{ii} = \\dfrac{2\\kappa}{h^2} + \\dfrac{u}{h}$ for $i=1, \\dots, N$.\n-   Lower diagonal: $A_{i, i-1} = -\\dfrac{\\kappa}{h^2} - \\dfrac{u}{h}$ for $i=2, \\dots, N$.\n-   Upper diagonal: $A_{i, i+1} = -\\dfrac{\\kappa}{h^2}$ for $i=1, \\dots, N-1$.\n\nThe right-hand side vector $\\mathbf{b} \\in \\mathbb{R}^N$ is given by $b_1 = \\frac{\\kappa}{h^2} + \\frac{u}{h}$ and $b_i = 0$ for $i=2, \\dots, N$. The FOM solution is obtained by solving the linear system $A \\mathbf{c}_{\\mathrm{FOM}} = \\mathbf{b}$.\n\n**2. Proper Orthogonal Decomposition (POD) Basis**\n\nWe generate $9$ snapshot solutions, $\\{\\mathbf{c}_{\\mathrm{FOM}}^{(j)}\\}_{j=1}^9$, by solving the FOM system for each parameter pair $(u, \\kappa)$ in the training set: $u \\in \\{2, 5, 10\\}$ and $\\kappa \\in \\{10^{-3}, 5 \\times 10^{-3}, 10^{-2}\\}$. These solution vectors are assembled as columns of the snapshot matrix $S \\in \\mathbb{R}^{N \\times 9}$.\n\nThe POD basis is constructed via the Singular Value Decomposition (SVD) of the snapshot matrix: $S = U \\Sigma V^T$. The columns of the matrix $U \\in \\mathbb{R}^{N \\times 9}$ are the left singular vectors, which are orthonormal and form the POD basis modes. A reduced basis of size $k$ is formed by taking the first $k$ columns of $U$, denoted as $\\Phi = U[:, :k] \\in \\mathbb{R}^{N \\times k}$.\n\nAn approximate solution can then be represented as a linear combination of these basis vectors: $\\mathbf{c}_{\\mathrm{ROM}} = \\Phi \\mathbf{c}_r$, where $\\mathbf{c}_r \\in \\mathbb{R}^k$ is the vector of reduced coordinates.\n\n**3. Least-Squares Petrov–Galerkin (LSPG) ROM**\n\nThe LSPG method seeks the reduced coordinate vector $\\mathbf{c}_r$ that minimizes the Euclidean norm of the algebraic residual of the FOM system, $R(\\mathbf{c}_{\\mathrm{ROM}}) = A \\mathbf{c}_{\\mathrm{ROM}} - \\mathbf{b}$.\n$$ \\mathbf{c}_r^{\\mathrm{LS}} = \\arg \\min_{\\mathbf{z} \\in \\mathbb{R}^k} \\| A \\Phi \\mathbf{z} - \\mathbf{b} \\|_2^2 $$\nThis is a standard linear least-squares problem. The solution is given by the normal equations:\n$$ (A \\Phi)^T (A \\Phi \\mathbf{c}_r^{\\mathrm{LS}}) = (A \\Phi)^T \\mathbf{b} $$\nThis defines the reduced linear system $A_r^{\\mathrm{LS}} \\mathbf{c}_r^{\\mathrm{LS}} = \\mathbf{b}_r^{\\mathrm{LS}}$, where:\n-   Reduced matrix: $A_r^{\\mathrm{LS}} = (A \\Phi)^T (A \\Phi)$\n-   Reduced vector: $\\mathbf{b}_r^{\\mathrm{LS}} = (A \\Phi)^T \\mathbf{b}$\n\nAfter solving for $\\mathbf{c}_r^{\\mathrm{LS}}$, the full-dimensional LSPG solution is reconstructed as $\\mathbf{c}_{\\mathrm{LS}} = \\Phi \\mathbf{c}_r^{\\mathrm{LS}}$. This method is equivalent to a Petrov-Galerkin projection where the test basis is $\\Psi_{\\mathrm{LS}} = A \\Phi$.\n\n**4. SUPG-inspired Petrov–Galerkin ROM**\n\nThe SUPG method introduces stability by modifying the test basis to add diffusion along the flow streamlines. For a trial basis function $\\phi_j$ (a column of $\\Phi$), the corresponding test function $\\psi_j$ is augmented:\n$$ \\psi_j = \\phi_j + \\tau u \\dfrac{d\\phi_j}{dx} $$\nThe stabilization parameter $\\tau$ is chosen based on dimensional analysis for one-dimensional advection-diffusion. A common and simple choice for advection-dominated flows is $\\tau = \\dfrac{h}{2u}$.\n\nThe derivative $\\frac{d\\phi_j}{dx}$ is discretized using a first-order backward difference operator $D \\in \\mathbb{R}^{N \\times N}$, consistent with the upwind direction for $u0$. At node $i$, $(D\\mathbf{v})_i = (v_i - v_{i-1})/h$. For the first node $i=1$, we need $v_0$. Since the POD basis is derived to approximate solutions, and the reduced model itself projects onto a space that does not explicitly enforce boundary constraints, it is standard to assume the basis functions implicitly satisfy homogeneous boundary conditions, i.e., their value at $x=0$ is $0$. Thus, for a basis vector $\\phi_j$, we have $(\\phi_j)_0=0$, and $(D \\phi_j)_1 = (\\phi_{j,1} - 0)/h = \\phi_{j,1}/h$. This defines the matrix $D$ as a lower bidiagonal matrix with $D_{ii} = 1/h$ and $D_{i,i-1} = -1/h$ for $i  1$, and $D_{11}=1/h$.\n\nThe discrete test basis is represented by the matrix $\\Psi_{\\mathrm{SUPG}} = \\Phi + \\tau u (D \\Phi)$. The Petrov-Galerkin projection requires $\\Psi_{\\mathrm{SUPG}}^T (A \\Phi \\mathbf{c}_r^{\\mathrm{SUPG}} - \\mathbf{b}) = 0$. This yields the reduced system $A_r^{\\mathrm{SUPG}} \\mathbf{c}_r^{\\mathrm{SUPG}} = \\mathbf{b}_r^{\\mathrm{SUPG}}$ with:\n-   Reduced matrix: $A_r^{\\mathrm{SUPG}} = (\\Phi + \\tau u D \\Phi)^T (A \\Phi) = \\left(\\Phi + \\frac{h}{2} D \\Phi \\right)^T (A \\Phi)$\n-   Reduced vector: $\\mathbf{b}_r^{\\mathrm{SUPG}} = (\\Phi + \\tau u D \\Phi)^T \\mathbf{b} = \\left(\\Phi + \\frac{h}{2} D \\Phi \\right)^T \\mathbf{b}$\n\nAfter solving for $\\mathbf{c}_r^{\\mathrm{SUPG}}$, the solution is reconstructed as $\\mathbf{c}_{\\mathrm{SUPG}} = \\Phi \\mathbf{c}_r^{\\mathrm{SUPG}}$.\n\n**5. Error Analysis**\n\nFor each test case, we compute the FOM solution $\\mathbf{c}_{\\mathrm{FOM}}$ and the two ROM solutions $\\mathbf{c}_{\\mathrm{LS}}$ and $\\mathbf{c}_{\\mathrm{SUPG}}$. The relative discrete $L^2$ error for each ROM is calculated as:\n$$ e_{\\mathrm{LS}} = \\frac{\\|\\mathbf{c}_{\\mathrm{LS}} - \\mathbf{c}_{\\mathrm{FOM}}\\|_2}{\\|\\mathbf{c}_{\\mathrm{FOM}}\\|_2}, \\quad e_{\\mathrm{SUPG}} = \\frac{\\|\\mathbf{c}_{\\mathrm{SUPG}} - \\mathbf{c}_{\\mathrm{FOM}}\\|_2}{\\|\\mathbf{c}_{\\mathrm{FOM}}\\|_2} $$\nNote that the factor $\\sqrt{h}$ in the problem's definition of the error norm cancels in the relative error calculation. The final quantity of interest is the improvement factor $r = e_{\\mathrm{LS}} / e_{\\mathrm{SUPG}}$. A value $r  1$ indicates that the SUPG-inspired model is more accurate than the LSPG model for that test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a comparison of LSPG and SUPG-inspired reduced-order models\n    for a 1D advection-diffusion problem.\n    \"\"\"\n    N = 300\n    h = 1.0 / (N + 1)\n\n    def assemble_fom(u, kappa, N, h):\n        \"\"\"Assembles the full-order model matrix A and vector b.\"\"\"\n        # Main diagonal\n        diag = np.full(N, (2.0 * kappa / h**2) + (u / h))\n        # Upper diagonal\n        upper_diag = np.full(N - 1, -kappa / h**2)\n        # Lower diagonal\n        lower_diag = np.full(N - 1, (-kappa / h**2) - (u / h))\n        \n        A = np.diag(diag) + np.diag(upper_diag, k=1) + np.diag(lower_diag, k=-1)\n        \n        b = np.zeros(N)\n        b[0] = (kappa / h**2) + (u / h)\n        \n        return A, b\n\n    # --- Training Phase: Generate Snapshots and Build POD Basis ---\n    train_params = [\n        (2.0, 1e-3), (2.0, 5e-3), (2.0, 1e-2),\n        (5.0, 1e-3), (5.0, 5e-3), (5.0, 1e-2),\n        (10.0, 1e-3), (10.0, 5e-3), (10.0, 1e-2)\n    ]\n    \n    snapshot_matrix = np.zeros((N, len(train_params)))\n    \n    for i, (u_train, kappa_train) in enumerate(train_params):\n        A_train, b_train = assemble_fom(u_train, kappa_train, N, h)\n        c_fom = np.linalg.solve(A_train, b_train)\n        snapshot_matrix[:, i] = c_fom\n        \n    # Perform SVD to get POD basis (left singular vectors)\n    U, _, _ = np.linalg.svd(snapshot_matrix, full_matrices=False)\n    \n    # --- Testing Phase ---\n    test_cases = [\n        (5.0, 1e-3, 6),\n        (1.0, 1e-2, 6),\n        (20.0, 1e-4, 8),\n        (10.0, 5e-4, 4)\n    ]\n    \n    results = []\n    \n    # Define discrete backward difference operator D\n    D = np.diag(np.ones(N)) - np.diag(np.ones(N - 1), k=-1)\n    D /= h\n\n    for u_test, kappa_test, k in test_cases:\n        # 1. Get FOM solution for the test case\n        A_test, b_test = assemble_fom(u_test, kappa_test, N, h)\n        c_fom_test = np.linalg.solve(A_test, b_test)\n        norm_fom = np.linalg.norm(c_fom_test)\n\n        # 2. Extract POD basis of size k\n        Phi = U[:, :k]\n        \n        # 3. Solve Least-Squares Petrov-Galerkin (LSPG) ROM\n        A_phi = A_test @ Phi\n        Ar_ls = A_phi.T @ A_phi\n        br_ls = A_phi.T @ b_test\n        cr_ls = np.linalg.solve(Ar_ls, br_ls)\n        c_ls = Phi @ cr_ls\n        \n        # 4. Solve SUPG-inspired ROM\n        tau = h / (2.0 * u_test)\n        # Note: tau * u_test simplification to h/2 is also possible\n        Psi_supg = Phi + (tau * u_test) * (D @ Phi)\n        \n        Ar_supg = Psi_supg.T @ (A_test @ Phi)\n        br_supg = Psi_supg.T @ b_test\n        cr_supg = np.linalg.solve(Ar_supg, br_supg)\n        c_supg = Phi @ cr_supg\n        \n        # 5. Compute errors and improvement factor\n        e_ls = np.linalg.norm(c_ls - c_fom_test) / norm_fom\n        e_supg = np.linalg.norm(c_supg - c_fom_test) / norm_fom\n        \n        # Handle case where e_supg might be zero to avoid division by zero\n        if e_supg == 0:\n            if e_ls == 0:\n                r = 1.0  # Both are perfect\n            else:\n                r = np.inf # SUPG is perfect, LSPG is not\n        else:\n            r = e_ls / e_supg\n            \n        results.append(r)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}