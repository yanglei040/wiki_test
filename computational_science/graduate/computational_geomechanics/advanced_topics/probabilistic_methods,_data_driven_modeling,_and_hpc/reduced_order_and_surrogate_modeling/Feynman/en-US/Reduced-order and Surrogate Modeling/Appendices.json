{
    "hands_on_practices": [
        {
            "introduction": "Proper Orthogonal Decomposition (POD) is a cornerstone of data-driven model reduction, but its power hinges on effective basis truncation. This exercise provides a foundational understanding of how to select the number of POD modes by connecting the singular value spectrum to the projection error. Mastering this principle is essential for building compact yet accurate reduced-order models from simulation data .",
            "id": "3555778",
            "problem": "A high-fidelity finite element model in computational geomechanics generates a snapshot matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times m}$ from parametric simulations of poroelastic consolidation in a stratified soil layer. Consider the Proper Orthogonal Decomposition (POD) basis constructed from the Singular Value Decomposition (SVD) of $\\mathbf{X}$, where $\\mathbf{X} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{\\top}$ and the singular values are $\\{\\sigma_i\\}_{i \\ge 1}$ with $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge 0$. Let the rank-$r$ POD projector be $\\mathbf{P}_r = \\mathbf{U}_r \\mathbf{U}_r^{\\top}$, where $\\mathbf{U}_r$ contains the first $r$ left singular vectors.\n\nYou are tasked with selecting the number of modes $r$ to satisfy a prescribed tolerance in the relative projection error. Define the relative projection error in the snapshot Frobenius norm as\n$$\nE_r \\equiv \\frac{\\|\\mathbf{X} - \\mathbf{P}_r \\mathbf{X}\\|_F}{\\|\\mathbf{X}\\|_F}.\n$$\nStarting from the definitions of the Singular Value Decomposition and properties of the Frobenius norm, derive a rule for the minimal $r$ that guarantees $E_r \\le \\epsilon$ for a given tolerance $\\epsilon \\in (0,1)$. Then, under the scientifically plausible and empirically observed decay profile for many geomechanical datasets that the singular values are geometrically decaying,\n$$\n\\sigma_i = \\sigma_1 \\, q^{\\,i-1}, \\quad \\text{with} \\quad \\sigma_1 = 1 \\quad \\text{and} \\quad q = 0.82,\n$$\nassume the sequence extends indefinitely and compute the residual energy\n$\\sum_{i>r} \\sigma_i^2$\nfor the tolerance $\\epsilon = 10^{-3}$.\n\nExpress your final answer as the row matrix containing $(r, \\sum_{i>r} \\sigma_i^2)$. Round the residual energy to four significant figures. Do not include units in your final answer.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of linear algebra and reduced-order modeling, specifically Proper Orthogonal Decomposition (POD) via Singular Value Decomposition (SVD), which is a standard technique in computational sciences. The problem is well-posed, with all necessary data and definitions provided to arrive at a unique, verifiable solution.\n\nThe first task is to derive a rule for selecting the minimal number of modes, $r$, to satisfy a given tolerance, $\\epsilon$, on the relative projection error, $E_r$. The error is defined as:\n$$\nE_r = \\frac{\\|\\mathbf{X} - \\mathbf{P}_r \\mathbf{X}\\|_F}{\\|\\mathbf{X}\\|_F}\n$$\nIt is more convenient to work with the square of this quantity:\n$$\nE_r^2 = \\frac{\\|\\mathbf{X} - \\mathbf{P}_r \\mathbf{X}\\|_F^2}{\\|\\mathbf{X}\\|_F^2}\n$$\nThe Frobenius norm of a matrix $\\mathbf{A}$ is related to its singular values $\\{\\sigma_i\\}$ by $\\|\\mathbf{A}\\|_F^2 = \\sum_i \\sigma_i^2$. The singular values of the snapshot matrix $\\mathbf{X}$ are given as $\\{\\sigma_i\\}_{i \\ge 1}$. Thus, the squared norm in the denominator is:\n$$\n\\|\\mathbf{X}\\|_F^2 = \\sum_{i=1}^{k} \\sigma_i^2\n$$\nwhere $k = \\text{rank}(\\mathbf{X})$.\n\nFor the numerator, we first analyze the matrix representing the projection error, $\\mathbf{X} - \\mathbf{P}_r \\mathbf{X}$. The SVD of $\\mathbf{X}$ can be written as a sum of rank-$1$ matrices:\n$$\n\\mathbf{X} = \\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^{\\top}\n$$\nwhere $\\mathbf{u}_i$ and $\\mathbf{v}_i$ are the $i$-th left and right singular vectors, respectively. The projector $\\mathbf{P}_r = \\mathbf{U}_r \\mathbf{U}_r^{\\top}$ is the orthogonal projection onto the subspace spanned by the first $r$ left singular vectors, $\\{\\mathbf{u}_1, \\dots, \\mathbf{u}_r\\}$. Applying this projector to a vector $\\mathbf{w}$ is $\\mathbf{P}_r \\mathbf{w} = \\sum_{i=1}^r (\\mathbf{u}_i^{\\top}\\mathbf{w})\\mathbf{u}_i$. Applying the projector to the matrix $\\mathbf{X}$ yields:\n$$\n\\mathbf{P}_r \\mathbf{X} = \\left(\\sum_{j=1}^r \\mathbf{u}_j \\mathbf{u}_j^{\\top}\\right) \\left(\\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^{\\top}\\right) = \\sum_{j=1}^r \\sum_{i=1}^k \\sigma_i (\\mathbf{u}_j \\mathbf{u}_j^{\\top} \\mathbf{u}_i) \\mathbf{v}_i^{\\top}\n$$\nDue to the orthonormality of the singular vectors, $\\mathbf{u}_j^{\\top} \\mathbf{u}_i = \\delta_{ij}$ (the Kronecker delta). The expression simplifies to:\n$$\n\\mathbf{P}_r \\mathbf{X} = \\sum_{i=1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^{\\top}\n$$\nThis is the well-known Eckart-Young-Mirsky theorem result, which states that $\\mathbf{P}_r\\mathbf{X}$ is the best rank-$r$ approximation of $\\mathbf{X}$ in the Frobenius norm. The error matrix is then:\n$$\n\\mathbf{X} - \\mathbf{P}_r \\mathbf{X} = \\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^{\\top} - \\sum_{i=1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^{\\top} = \\sum_{i=r+1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^{\\top}\n$$\nThe expression on the right is itself an SVD, where the singular values are $\\{\\sigma_{r+1}, \\sigma_{r+2}, \\dots, \\sigma_k\\}$. Therefore, the squared Frobenius norm of the error matrix is:\n$$\n\\|\\mathbf{X} - \\mathbf{P}_r \\mathbf{X}\\|_F^2 = \\sum_{i=r+1}^k \\sigma_i^2\n$$\nThe problem states to assume the singular value sequence extends indefinitely, so we replace the finite upper limit $k$ with $\\infty$. The squared relative error becomes:\n$$\nE_r^2 = \\frac{\\sum_{i=r+1}^\\infty \\sigma_i^2}{\\sum_{i=1}^\\infty \\sigma_i^2}\n$$\nThe condition for choosing $r$ is $E_r \\le \\epsilon$, which is equivalent to $E_r^2 \\le \\epsilon^2$. So, the rule for the minimal $r$ is to find the smallest integer $r$ that satisfies:\n$$\n\\frac{\\sum_{i=r+1}^\\infty \\sigma_i^2}{\\sum_{i=1}^\\infty \\sigma_i^2} \\le \\epsilon^2\n$$\nNow, we apply this rule to the specified singular value decay profile:\n$$\n\\sigma_i = \\sigma_1 q^{i-1} \\quad \\text{with} \\quad \\sigma_1 = 1 \\quad \\text{and} \\quad q = 0.82\n$$\nThe squared singular values are $\\sigma_i^2 = (\\sigma_1^2) (q^2)^{i-1} = (q^2)^{i-1}$ since $\\sigma_1=1$. The sums are infinite geometric series. The total sum (denominator) is:\n$$\n\\sum_{i=1}^\\infty \\sigma_i^2 = \\sum_{i=1}^\\infty (q^2)^{i-1} = \\sum_{j=0}^\\infty (q^2)^j = \\frac{1}{1-q^2}\n$$\nThis series converges because $|q| = 0.82 < 1$, so $q^2 < 1$. The sum of the truncated tail (numerator) is:\n$$\n\\sum_{i=r+1}^\\infty \\sigma_i^2 = \\sum_{i=r+1}^\\infty (q^2)^{i-1} = (q^2)^r + (q^2)^{r+1} + \\dots = \\frac{(q^2)^r}{1-q^2}\n$$\nSubstituting these into the inequality for $r$:\n$$\n\\frac{\\frac{(q^2)^r}{1-q^2}}{\\frac{1}{1-q^2}} \\le \\epsilon^2 \\implies (q^2)^r \\le \\epsilon^2\n$$\nTaking the square root of both sides (all quantities are positive) gives a simpler relation:\n$$\nq^r \\le \\epsilon\n$$\nWe need to find the smallest integer $r$ satisfying this. Taking the natural logarithm of both sides:\n$$\nr \\ln(q) \\le \\ln(\\epsilon)\n$$\nSince $q=0.82 < 1$, $\\ln(q)$ is negative. Dividing by $\\ln(q)$ reverses the inequality direction:\n$$\nr \\ge \\frac{\\ln(\\epsilon)}{\\ln(q)}\n$$\nWe are given $\\epsilon = 10^{-3}$ and $q = 0.82$.\n$$\nr \\ge \\frac{\\ln(10^{-3})}{\\ln(0.82)} = \\frac{-3 \\ln(10)}{\\ln(0.82)}\n$$\nUsing numerical values $\\ln(10) \\approx 2.302585$ and $\\ln(0.82) \\approx -0.198451$:\n$$\nr \\ge \\frac{-3 \\times 2.302585}{-0.198451} \\approx \\frac{-6.907755}{-0.198451} \\approx 34.808\n$$\nSince $r$ must be an integer, the minimal value is $r = \\lceil 34.808 \\rceil = 35$.\n\nThe second task is to compute the residual energy $\\sum_{i>r} \\sigma_i^2$ for this value of $r$. Since $r=35$, this is the sum $\\sum_{i=36}^\\infty \\sigma_i^2$. This is the numerator term we calculated earlier, evaluated at $r=35$.\n$$\n\\sum_{i>35} \\sigma_i^2 = \\sum_{i=36}^\\infty \\sigma_i^2 = \\frac{(q^2)^{35}}{1-q^2}\n$$\nWe have $q=0.82$, so $q^2 = 0.82^2 = 0.6724$.\n$$\n\\sum_{i>35} \\sigma_i^2 = \\frac{(0.6724)^{35}}{1-0.6724} = \\frac{(0.6724)^{35}}{0.3276}\n$$\nCalculating the value:\n$$\n(0.6724)^{35} \\approx 9.245235 \\times 10^{-7}\n$$\n$$\n\\sum_{i>35} \\sigma_i^2 \\approx \\frac{9.245235 \\times 10^{-7}}{0.3276} \\approx 2.8221096 \\times 10^{-6}\n$$\nRounding to four significant figures, the residual energy is $2.822 \\times 10^{-6}$.\n\nThe final answer is the row matrix $(r, \\sum_{i>r} \\sigma_i^2)$.\nThe two components are $r=35$ and the residual energy $2.822 \\times 10^{-6}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n35 & 2.822 \\times 10^{-6}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "One of the most significant advantages of model reduction in dynamics is the potential for computational speed-up through larger time steps. This practice demonstrates how truncating high-frequency modes in a reduced-order model relaxes the stability constraints of explicit time integration schemes. By analyzing a wave propagation problem, you will quantify this benefit and gain insight into the trade-off between model size and computational efficiency .",
            "id": "3555710",
            "problem": "A one-dimensional saturated soil column is modeled by Biot-type dynamics. After a standard mixed finite element semi-discretization in space and a block-static condensation of the fluid variable to isolate the wave-propagatory part, the undamped semi-discrete equations for the solid displacement degrees of freedom can be written in the second-order form\n$$\nM \\, \\ddot{u}(t) + K \\, u(t) = 0,\n$$\nwhere $M \\in \\mathbb{R}^{N \\times N}$ is symmetric positive definite and $K \\in \\mathbb{R}^{N \\times N}$ is symmetric positive semidefinite. Consider explicit central difference time integration applied to the homogeneous system above. Let the generalized eigenpairs of the pair $(K,M)$ solve\n$$\nK \\, \\phi_i = \\lambda_i \\, M \\, \\phi_i, \\quad i=1,\\dots,N,\n$$\nwith $0 < \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_N$. Assume the eigenvectors are $M$-orthonormal, so $\\phi_i^{\\top} M \\phi_j = \\delta_{ij}$.\n\nA reduced-order model is constructed by a Galerkin projection onto the subspace spanned by the first $r$ generalized eigenvectors, with $r < N$, forming the reduced basis matrix $V = [\\phi_1,\\dots,\\phi_r] \\in \\mathbb{R}^{N \\times r}$ satisfying $V^{\\top} M V = I_r$. The reduced operators are $M_r = V^{\\top} M V = I_r$ and $K_r = V^{\\top} K V$, and the reduced model is $M_r \\ddot{q}(t) + K_r q(t) = 0$.\n\nStarting from fundamental principles for central difference time integration and modal analysis of second-order linear systems, derive the stability time step constraint for the full model and the reduced model, expressed in terms of the largest generalized eigenvalue retained in each case. Then, using the following full-order generalized eigenvalues (in increasing order),\n$$\n\\lambda_1 = 1.44 \\times 10^{6}, \\quad\n\\lambda_2 = 2.25 \\times 10^{6}, \\quad\n\\lambda_3 = 1.00 \\times 10^{7}, \\quad\n\\lambda_4 = 4.00 \\times 10^{7}, \\quad\n\\lambda_5 = 1.60 \\times 10^{8}, \\quad\n\\lambda_6 = 3.60 \\times 10^{8}, \\quad\n\\lambda_7 = 6.25 \\times 10^{8}, \\quad\n\\lambda_8 = 9.00 \\times 10^{8},\n$$\nand taking $r=5$, compute the ratio\n$$\nR \\equiv \\frac{\\Delta t_{\\text{max,ROM}}}{\\Delta t_{\\text{max,FOM}}}.\n$$\nReport $R$ as a dimensionless number rounded to four significant figures.",
            "solution": "The problem requires the derivation of the stability condition for the explicit central difference time integration scheme applied to a second-order linear system, first for a full-order model (FOM) and then for a reduced-order model (ROM). Subsequently, the ratio of the maximum allowable time steps for the ROM and FOM is to be computed.\n\nThe full-order semi-discrete system is given by:\n$$\nM \\, \\ddot{u}(t) + K \\, u(t) = 0\n$$\nwhere $M$ is symmetric positive definite and $K$ is symmetric positive semidefinite.\n\nThe explicit central difference scheme approximates the second time derivative at time step $t_n$ as:\n$$\n\\ddot{u}(t_n) \\approx \\frac{u_{n+1} - 2u_n + u_{n-1}}{(\\Delta t)^2}\n$$\nwhere $u_n \\equiv u(t_n)$ and $\\Delta t$ is the time step size. Substituting this into the governing equation yields the discrete-time update rule:\n$$\nM \\left( \\frac{u_{n+1} - 2u_n + u_{n-1}}{(\\Delta t)^2} \\right) + K u_n = 0\n$$\nTo analyze the stability of this scheme, we perform a modal analysis. The solution $u(t)$ can be expressed as a linear combination of the generalized eigenvectors $\\phi_i$:\n$$\nu(t) = \\sum_{i=1}^{N} \\eta_i(t) \\phi_i\n$$\nwhere $\\eta_i(t)$ are the modal coordinates. Substituting this modal expansion into the original equation gives:\n$$\nM \\sum_{i=1}^{N} \\ddot{\\eta}_i(t) \\phi_i + K \\sum_{i=1}^{N} \\eta_i(t) \\phi_i = 0\n$$\nPremultiplying by an eigenvector transpose $\\phi_j^{\\top}$ yields:\n$$\n\\sum_{i=1}^{N} \\ddot{\\eta}_i(t) (\\phi_j^{\\top} M \\phi_i) + \\sum_{i=1}^{N} \\eta_i(t) (\\phi_j^{\\top} K \\phi_i) = 0\n$$\nUsing the given $M$-orthonormality condition $\\phi_j^{\\top} M \\phi_i = \\delta_{ij}$ and the definition of the generalized eigenproblem $K \\phi_i = \\lambda_i M \\phi_i$, which implies $\\phi_j^{\\top} K \\phi_i = \\lambda_i \\phi_j^{\\top} M \\phi_i = \\lambda_i \\delta_{ij}$, the system decouples into $N$ independent scalar equations:\n$$\n\\ddot{\\eta}_j(t) + \\lambda_j \\eta_j(t) = 0, \\quad j=1, \\dots, N\n$$\nThe natural frequency of the $j$-th mode is $\\omega_j = \\sqrt{\\lambda_j}$. Applying the central difference scheme to this scalar modal equation gives:\n$$\n\\frac{\\eta_{j, n+1} - 2\\eta_{j, n} + \\eta_{j, n-1}}{(\\Delta t)^2} + \\lambda_j \\eta_{j, n} = 0\n$$\nThis can be rewritten as a linear recurrence relation:\n$$\n\\eta_{j, n+1} - (2 - \\lambda_j (\\Delta t)^2) \\eta_{j, n} + \\eta_{j, n-1} = 0\n$$\nFor the numerical solution to be stable (i.e., not grow unboundedly), the magnitude of the roots of the characteristic polynomial of this recurrence must be less than or equal to $1$. Assuming a solution of the form $\\eta_{j,n} = G^n$, we obtain the characteristic equation:\n$$\nG^2 - (2 - \\lambda_j (\\Delta t)^2) G + 1 = 0\n$$\nFor the roots $G$ to have magnitude $|G| \\le 1$, the discriminant of this quadratic equation must be non-positive, which ensures the roots are complex conjugates on the unit circle.\n$$\n(2 - \\lambda_j (\\Delta t)^2)^2 - 4 \\le 0\n$$\nThis inequality is equivalent to:\n$$\n-2 \\le 2 - \\lambda_j (\\Delta t)^2 \\le 2\n$$\nThe right side, $2 - \\lambda_j (\\Delta t)^2 \\le 2$, implies $-\\lambda_j (\\Delta t)^2 \\le 0$, which is always true since $\\lambda_j > 0$. The left side imposes the stability constraint:\n$$\n-2 \\le 2 - \\lambda_j (\\Delta t)^2 \\implies \\lambda_j (\\Delta t)^2 \\le 4\n$$\nThis condition must hold for all modes, so it is governed by the largest eigenvalue, $\\lambda_N$:\n$$\n\\lambda_N (\\Delta t)^2 \\le 4 \\implies \\Delta t \\le \\frac{2}{\\sqrt{\\lambda_N}}\n$$\nThe maximum stable time step for the full-order model (FOM) is thus:\n$$\n\\Delta t_{\\text{max,FOM}} = \\frac{2}{\\sqrt{\\lambda_N}}\n$$\nNow, we consider the reduced-order model (ROM). The ROM is constructed via a Galerkin projection onto the subspace spanned by the first $r$ eigenvectors, $V = [\\phi_1, \\dots, \\phi_r]$. The reduced system is:\n$$\nM_r \\ddot{q}(t) + K_r q(t) = 0\n$$\nwhere $M_r = V^{\\top} M V$ and $K_r = V^{\\top} K V$. Based on the $M$-orthonormality of the eigenvectors, the reduced mass matrix is the identity matrix of size $r$:\n$$\n(M_r)_{ij} = \\phi_i^{\\top} M \\phi_j = \\delta_{ij} \\implies M_r = I_r\n$$\nThe reduced stiffness matrix $K_r$ becomes diagonal:\n$$\n(K_r)_{ij} = \\phi_i^{\\top} K \\phi_j = \\phi_i^{\\top} (\\lambda_j M \\phi_j) = \\lambda_j (\\phi_i^{\\top} M \\phi_j) = \\lambda_j \\delta_{ij}\n$$\nSo, $K_r = \\text{diag}(\\lambda_1, \\dots, \\lambda_r)$. The reduced system is already decoupled:\n$$\nI_r \\ddot{q}(t) + \\text{diag}(\\lambda_1, \\dots, \\lambda_r) q(t) = 0\n$$\nThis corresponds to $r$ scalar equations:\n$$\n\\ddot{q}_i(t) + \\lambda_i q_i(t) = 0, \\quad i=1, \\dots, r\n$$\nThe stability analysis for the ROM is identical to that for a single mode of the FOM. The stability condition must hold for all retained modes, and so it is limited by the largest retained eigenvalue, $\\lambda_r$:\n$$\n\\lambda_r (\\Delta t)^2 \\le 4 \\implies \\Delta t \\le \\frac{2}{\\sqrt{\\lambda_r}}\n$$\nThe maximum stable time step for the ROM is:\n$$\n\\Delta t_{\\text{max,ROM}} = \\frac{2}{\\sqrt{\\lambda_r}}\n$$\nWe are asked to compute the ratio $R \\equiv \\frac{\\Delta t_{\\text{max,ROM}}}{\\Delta t_{\\text{max,FOM}}}$.\n$$\nR = \\frac{2 / \\sqrt{\\lambda_r}}{2 / \\sqrt{\\lambda_N}} = \\sqrt{\\frac{\\lambda_N}{\\lambda_r}}\n$$\nThe problem provides a list of $8$ eigenvalues, implying the full model dimension is $N=8$. The largest eigenvalue is $\\lambda_N = \\lambda_8 = 9.00 \\times 10^8$. The ROM is constructed with $r=5$, so the largest retained eigenvalue is $\\lambda_r = \\lambda_5 = 1.60 \\times 10^8$.\n\nSubstituting these values into the expression for $R$:\n$$\nR = \\sqrt{\\frac{9.00 \\times 10^8}{1.60 \\times 10^8}} = \\sqrt{\\frac{9.00}{1.60}} = \\sqrt{5.625}\n$$\nCalculating the numerical value:\n$$\nR \\approx 2.371708245...\n$$\nRounding to four significant figures, we get $R = 2.372$.",
            "answer": "$$\n\\boxed{2.372}\n$$"
        },
        {
            "introduction": "For problems dependent on varying physical parameters, a robust reduced basis must provide accuracy across the entire parameter space; a simple POD on a few snapshots may not suffice, necessitating more intelligent basis construction strategies. This hands-on coding challenge introduces the powerful greedy algorithm for building a reduced basis, guided by an error estimator to ensure reliability. You will implement this advanced technique and compare its efficiency against a simpler uniform sampling method, highlighting its superiority in creating compact and accurate parametric surrogates .",
            "id": "3555733",
            "problem": "Consider a one-dimensional, non-dimensional, steady pore-pressure diffusion problem on the closed interval $[0,1]$ governed by the conservation of mass and Darcyâ€™s law. Let the pore pressure field be $p(x;\\mu)$, with a spatially varying non-dimensional permeability $k(x;\\mu)$ that depends on a scalar parameter $\\mu \\in [-0.9,0.9]$. The governing equation is the second-order linear elliptic boundary value problem\n$$\n-\\frac{d}{dx}\\left(k(x;\\mu)\\frac{dp}{dx}(x;\\mu)\\right) = s(x),\n$$\nwith Dirichlet boundary conditions $p(0;\\mu) = p_L$ and $p(1;\\mu) = p_R$, where all quantities are non-dimensional. Assume the permeability is given by\n$$\nk(x;\\mu) = \\exp\\big(\\mu \\sin(2\\pi x)\\big),\n$$\nand the source term is prescribed as\n$$\ns(x) = s_0 \\cos(2\\pi x),\n$$\nwhere $s_0$ is a non-dimensional amplitude.\n\nDiscretize the domain into $N$ equally spaced nodes with spacing $\\Delta x = 1/(N-1)$, and approximate the flux at half nodes using arithmetic averaging. For interior nodes $i = 2,\\dots,N-1$ (using $1$-based indexing), define $k_{i+1/2} = (k_i + k_{i+1})/2$ and $k_{i-1/2} = (k_i + k_{i-1})/2$, and enforce the discrete balance\n$$\n\\frac{1}{\\Delta x^2}\\Big( k_{i+\\frac{1}{2}}\\,p_{i+1} - \\big(k_{i+\\frac{1}{2}} + k_{i-\\frac{1}{2}}\\big)\\,p_i + k_{i-\\frac{1}{2}}\\,p_{i-1} \\Big) = s_i,\n$$\nwith boundary conditions $p_1 = p_L$ and $p_N = p_R$. This yields a linear system $A(\\mu)\\,p(\\mu) = b(\\mu)$ for each parameter $\\mu$.\n\nConstruct a Reduced Basis (RB) surrogate by representing the solution as $p^r(x;\\mu) = g(x) + u^r(x;\\mu)$, where $g(x)$ is any function satisfying $g(0) = p_L$ and $g(1) = p_R$, and $u^r(x;\\mu)$ is expanded in a basis $V$ whose columns are zero at both boundaries. Use a Galerkin projection onto the span of $V$ to compute $u^r(\\mu)$ by solving\n$$\nV^\\top A(\\mu) V\\, a(\\mu) = V^\\top\\big(b(\\mu) - A(\\mu) g\\big), \\quad u^r(\\mu) = V a(\\mu).\n$$\nDefine the residual as $r(\\mu) = A(\\mu)\\,p^r(\\mu) - b(\\mu)$.\n\nImplement a greedy snapshot selection strategy driven by a residual-based error estimator targeted at pore pressure at critical node locations. Let the set of training parameters be $\\{\\mu_j\\}_{j=1}^{M}$ with $M$ evenly spaced points over $[-0.9,0.9]$. Given a set of critical locations $\\{x_c\\}$, define a diagonal weight vector $w \\in \\mathbb{R}^N$ with entries $w_i$ that significantly upweight the residual contributions at the rows corresponding to the discrete equations associated with the critical locations and their immediate neighbors. Use the weighted residual norm\n$$\n\\varepsilon(\\mu) = \\left( \\sum_{i=1}^{N} w_i\\, r_i(\\mu)^2 \\right)^{1/2}\n$$\nas the greedy selection criterion: at each greedy iteration, add to the basis the snapshot at the parameter $\\mu$ that maximizes $\\varepsilon(\\mu)$ among the remaining training parameters.\n\nAs a baseline, implement uniform snapshot selection, where $r$ snapshots are taken at $r$ evenly spaced parameters over $[-0.9,0.9]$.\n\nFor both greedy and uniform strategies, evaluate the surrogate on a test set of parameters and report the mean absolute error of the pore pressure at the specified critical node locations, averaged over the test parameters.\n\nAll quantities are non-dimensional, and all computations must be performed using the finite difference discretization described above. The Reduced Basis must enforce zero values at the boundary nodes for the fluctuation field $u^r$. The arithmetic averaging for $k_{i\\pm1/2}$ must be used.\n\nYour program must implement the following three test cases, each with a specified problem size, boundary data, source amplitude, Reduced Basis size, critical locations, training set, and test set, as follows:\n\n- Test Case $A$ (general case):\n    - Grid size: $N = 50$.\n    - Boundary values: $p_L = 1.0$, $p_R = 0.0$.\n    - Source amplitude: $s_0 = 0.2$.\n    - Reduced Basis size: $r = 5$.\n    - Critical locations: $x_c \\in \\{0.3, 0.7\\}$.\n    - Training set: $M = 21$ parameters uniformly spaced over $[-0.9, 0.9]$.\n    - Test parameters: $\\{-0.9, -0.45, 0.0, 0.45, 0.9\\}$.\n\n- Test Case $B$ (boundary-dominated case with no source):\n    - Grid size: $N = 80$.\n    - Boundary values: $p_L = -1.0$, $p_R = 1.0$.\n    - Source amplitude: $s_0 = 0.0$.\n    - Reduced Basis size: $r = 7$.\n    - Critical locations: $x_c \\in \\{0.95\\}$.\n    - Training set: $M = 21$ parameters uniformly spaced over $[-0.9, 0.9]$.\n    - Test parameters: $\\{-0.9, -0.45, 0.0, 0.45, 0.9\\}$.\n\n- Test Case $C$ (source-dominated case with equal boundary values):\n    - Grid size: $N = 100$.\n    - Boundary values: $p_L = 0.5$, $p_R = 0.5$.\n    - Source amplitude: $s_0 = 0.3$.\n    - Reduced Basis size: $r = 6$.\n    - Critical locations: $x_c \\in \\{0.5\\}$.\n    - Training set: $M = 21$ parameters uniformly spaced over $[-0.9, 0.9]$.\n    - Test parameters: $\\{-0.9, -0.45, 0.0, 0.45, 0.9\\}$.\n\nFor the targeted residual-based error estimator, use weights $w_i$ equal to $1$ everywhere except at the rows corresponding to the critical locations, where $w_i$ must be set to a large value $\\alpha$, and at their immediate neighbors, where $w_i$ must be set to $\\alpha/2$. Use $\\alpha = 50$.\n\nYour program should produce a single line of output containing six floating-point numbers rounded to six decimal places in a comma-separated list enclosed in square brackets, corresponding to the mean absolute errors at the critical locations over the test parameters for the greedy and uniform strategies in Test Case $A$, Test Case $B$, and Test Case $C$, respectively, in the order\n$$\n[\\text{err}_{A,\\mathrm{greedy}}, \\text{err}_{A,\\mathrm{uniform}}, \\text{err}_{B,\\mathrm{greedy}}, \\text{err}_{B,\\mathrm{uniform}}, \\text{err}_{C,\\mathrm{greedy}}, \\text{err}_{C,\\mathrm{uniform}}].\n$$",
            "solution": "The problem is deemed valid as it is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard exercise in the field of computational geomechanics, specifically focusing on reduced-order modeling for a parametric partial differential equation.\n\n### 1. Problem Formulation and Discretization\n\nThe physical problem is a one-dimensional, steady-state pore-pressure diffusion equation on the domain $x \\in [0, 1]$. The governing equation is a second-order linear elliptic partial differential equation (PDE):\n$$\n-\\frac{d}{dx}\\left(k(x;\\mu)\\frac{dp}{dx}(x;\\mu)\\right) = s(x)\n$$\nHere, $p(x;\\mu)$ is the pore pressure, which depends on the spatial coordinate $x$ and a scalar parameter $\\mu$. The permeability $k(x;\\mu) = \\exp\\big(\\mu \\sin(2\\pi x)\\big)$ is a spatially heterogeneous function that also depends on $\\mu \\in [-0.9, 0.9]$. The source term is $s(x) = s_0 \\cos(2\\pi x)$, and Dirichlet boundary conditions are imposed: $p(0;\\mu) = p_L$ and $p(1;\\mu) = p_R$.\n\nTo solve this problem numerically, we employ a finite difference method on a uniform grid of $N$ nodes $x_i = (i-1)\\Delta x$ for $i=1,\\dots,N$ (using $1$-based indexing), where the grid spacing is $\\Delta x = 1/(N-1)$. The problem specifies the discrete balance equation for any interior node $i=2,\\dots,N-1$:\n$$\n\\frac{1}{\\Delta x^2}\\Big( k_{i+\\frac{1}{2}}\\,p_{i+1} - \\big(k_{i+\\frac{1}{2}} + k_{i-\\frac{1}{2}}\\big)\\,p_i + k_{i-\\frac{1}{2}}\\,p_{i-1} \\Big) = s_i\n$$\nwhere $p_i$ is the approximation of $p(x_i;\\mu)$ and $s_i = s(x_i)$. The inter-nodal permeability is calculated using an arithmetic mean: $k_{i\\pm1/2} = (k_i + k_{i\\pm 1})/2$.\n\n### 2. Full-Order Model (FOM)\n\nThe discrete equations for the interior nodes, combined with the boundary conditions $p_1 = p_L$ and $p_N = p_R$, form a system of $N$ linear algebraic equations. This system can be written in matrix form as:\n$$\nA(\\mu)\\,p(\\mu) = b(\\mu)\n$$\nwhere $p(\\mu) \\in \\mathbb{R}^N$ is the vector of nodal pressures, $A(\\mu) \\in \\mathbb{R}^{N \\times N}$ is the system matrix, and $b(\\mu) \\in \\mathbb{R}^N$ is the right-hand side vector. Using $0$-based indexing for implementation (nodes $i=0,\\dots,N-1$), the matrix $A(\\mu)$ and vector $b(\\mu)$ are constructed as follows:\n\nFor the boundary nodes:\n- Row $i=0$: $A_{0,0} = 1$, and all other $A_{0,j}=0$. The corresponding right-hand side is $b_0 = p_L$.\n- Row $i=N-1$: $A_{N-1,N-1} = 1$, and all other $A_{N-1,j}=0$. The corresponding right-hand side is $b_{N-1} = p_R$.\n\nFor the interior nodes $i=1,\\dots,N-2$:\n- $A_{i,i-1} = \\frac{k_{i-1/2}}{\\Delta x^2} = \\frac{k_{i-1} + k_i}{2\\Delta x^2}$\n- $A_{i,i} = -\\frac{k_{i-1/2} + k_{i+1/2}}{\\Delta x^2} = -\\frac{k_{i-1} + 2k_i + k_{i+1}}{2\\Delta x^2}$\n- $A_{i,i+1} = \\frac{k_{i+1/2}}{\\Delta x^2} = \\frac{k_i + k_{i+1}}{2\\Delta x^2}$\n- All other $A_{i,j}=0$. The right-hand side is $b_i = s_i = s_0 \\cos(2\\pi x_i)$.\n\nFor any given parameter $\\mu$, this linear system can be solved to obtain the high-fidelity or \"truth\" solution, which we refer to as the Full-Order Model (FOM) solution.\n\n### 3. Reduced-Order Model (ROM) via Galerkin Projection\n\nThe core idea of the Reduced Basis method is to find a low-dimensional subspace that effectively captures the behavior of the solution manifold $\\{p(\\mu) \\mid \\mu \\in [-0.9, 0.9]\\}$. To handle non-homogeneous boundary conditions, the solution is decomposed as:\n$$\np^r(x;\\mu) = g(x) + u^r(x;\\mu)\n$$\nwhere $g(x)$ is a \"lifting\" function that satisfies the boundary conditions, i.e., $g(0) = p_L$ and $g(1) = p_R$. A simple linear function $g(x) = p_L + (p_R - p_L)x$ suffices. The fluctuation field $u^r(x;\\mu)$ then has homogeneous boundary conditions, $u^r(0;\\mu) = u^r(1;\\mu) = 0$.\n\nThe field $u^r(\\mu)$ is approximated as a linear combination of basis vectors stored as columns in a matrix $V \\in \\mathbb{R}^{N \\times r}$, where $r \\ll N$:\n$$\nu^r(\\mu) = V a(\\mu)\n$$\nThe columns of $V$ are orthonormal and are zero at the boundary nodes, ensuring $u^r(\\mu)$ satisfies its homogeneous boundary conditions. The vector $a(\\mu) \\in \\mathbb{R}^r$ contains the unknown coefficients.\n\nSubstituting the ROM approximation into the FOM system gives a residual $r(\\mu) = A(\\mu)(g+Va(\\mu)) - b(\\mu)$. The Galerkin projection method requires this residual to be orthogonal to the basis subspace, i.e., $V^\\top r(\\mu)=0$. This leads to a much smaller linear system for the coefficients $a(\\mu)$:\n$$\nV^\\top A(\\mu) V a(\\mu) = V^\\top (b(\\mu) - A(\\mu)g)\n$$\nThis is the reduced system of size $r \\times r$. Solving for $a(\\mu)$ and reconstructing the solution is computationally much cheaper than solving the original $N \\times N$ FOM system.\n\n### 4. Basis Construction Strategies\n\nThe quality of the ROM heavily depends on the choice of the basis $V$. We implement and compare two strategies.\n\n#### 4.1. Uniform Snapshot Selection\nThis is a straightforward approach where the basis is constructed from solutions at pre-selected parameter values. We select $r$ parameters $\\{\\mu_j\\}_{j=1}^r$ uniformly spaced in the interval $[-0.9, 0.9]$. For each $\\mu_j$, we compute the FOM solution $p(\\mu_j)$, obtain the fluctuation part $u_j = p(\\mu_j) - g$, and collect these \"snapshots\". The set of snapshots $\\{u_j\\}_{j=1}^r$ is then orthogonalized (e.g., using a QR decomposition) to form the orthonormal basis $V$.\n\n#### 4.2. Greedy Snapshot Selection\nThis is an adaptive strategy that aims to build a more optimal basis. It iteratively selects snapshots that are poorly represented by the current basis. The selection is driven by a residual-based error estimator. The algorithm proceeds as follows:\n1.  Initialize an empty basis $V$ and a set of training parameters $\\mathcal{M}_{train}$.\n2.  For $k=1, \\dots, r$:\n    a. For each parameter $\\mu \\in \\mathcal{M}_{train}$, compute the ROM solution $p^r(\\mu)$ using the current basis $V$. If $V$ is empty, $p^r(\\mu) = g$.\n    b. Calculate the FOM residual $r(\\mu) = A(\\mu)p^r(\\mu) - b(\\mu)$.\n    c. Evaluate the error estimator, which is a weighted norm of the residual: $\\varepsilon(\\mu) = \\left( \\sum_{i=0}^{N-1} w_i\\, r_i(\\mu)^2 \\right)^{1/2}$. The weights $w_i$ are set to $\\alpha=50$ at rows corresponding to critical locations, $\\alpha/2=25$ at their immediate neighbors, and $1$ elsewhere. This targets solution accuracy at specified points of interest.\n    d. Find the parameter $\\mu^*$ that maximizes the error estimator: $\\mu^* = \\arg\\max_{\\mu \\in \\mathcal{M}_{train}} \\varepsilon(\\mu)$.\n    e. Compute the FOM snapshot $p(\\mu^*)$ and its fluctuation part $u^* = p(\\mu^*) - g$.\n    f. Augment the basis $V$ by orthogonalizing $u^*$ against the existing basis vectors and adding the resulting normalized vector.\n    g. Remove $\\mu^*$ from $\\mathcal{M}_{train}$.\n\n### 5. Error Evaluation\n\nThe accuracy of each ROM is assessed on a set of test parameters $\\mathcal{M}_{test}$. For each $\\mu_{test} \\in \\mathcal{M}_{test}$, we compute the ROM solution $p^r(\\mu_{test})$ and the true FOM solution $p(\\mu_{test})$. The error for that parameter is the average absolute difference at the set of critical node indices $\\mathcal{I}_c$:\n$$\nE(\\mu_{test}) = \\frac{1}{|\\mathcal{I}_c|} \\sum_{i \\in \\mathcal{I}_c} |p_i(\\mu_{test}) - p^r_i(\\mu_{test})|\n$$\nThe final reported error is the mean of these values over all test parameters:\n$$\n\\text{Error} = \\frac{1}{|\\mathcal{M}_{test}|} \\sum_{\\mu_{test} \\in \\mathcal{M}_{test}} E(\\mu_{test})\n$$\nThis procedure is carried out for each test case, for both the greedy and uniform basis selection strategies.",
            "answer": "```python\nimport numpy as np\n\ndef assemble_fom(N, p_L, p_R, s_0, mu):\n    \"\"\"Assembles the Full-Order Model (FOM) system A*p = b.\"\"\"\n    x = np.linspace(0.0, 1.0, N)\n    dx = 1.0 / (N - 1)\n    \n    k = np.exp(mu * np.sin(2 * np.pi * x))\n    s = s_0 * np.cos(2 * np.pi * x)\n    \n    A = np.zeros((N, N))\n    b = np.zeros(N)\n    \n    # Boundary conditions\n    A[0, 0] = 1.0\n    b[0] = p_L\n    A[N - 1, N - 1] = 1.0\n    b[N - 1] = p_R\n    \n    # Interior nodes (0-based indexing)\n    for i in range(1, N - 1):\n        k_minus_half = (k[i - 1] + k[i]) / 2.0\n        k_plus_half = (k[i] + k[i + 1]) / 2.0\n        \n        A[i, i - 1] = k_minus_half / (dx**2)\n        A[i, i] = -(k_minus_half + k_plus_half) / (dx**2)\n        A[i, i + 1] = k_plus_half / (dx**2)\n        b[i] = s[i]\n        \n    return A, b\n\ndef solve_fom(N, p_L, p_R, s_0, mu):\n    \"\"\"Solves the FOM system for a given parameter mu.\"\"\"\n    A, b = assemble_fom(N, p_L, p_R, s_0, mu)\n    p_fom = np.linalg.solve(A, b)\n    return p_fom\n\ndef get_orthonormal_basis(vectors):\n    \"\"\"Computes an orthonormal basis from a set of vectors using QR decomposition.\"\"\"\n    if not vectors:\n        return np.array([]).reshape(len(vectors[0]) if vectors else 0, 0)\n    Q, _ = np.linalg.qr(np.array(vectors).T)\n    return Q\n\ndef solve_rom(A, b, g, V):\n    \"\"\"Solves the Reduced-Order Model (ROM) for a given basis V.\"\"\"\n    if V.shape[1] == 0:\n        return g\n    \n    A_r = V.T @ A @ V\n    b_r = V.T @ (b - A @ g)\n    \n    try:\n        a = np.linalg.solve(A_r, b_r)\n    except np.linalg.LinAlgError:\n         # In case reduced matrix is singular\n        a = np.linalg.lstsq(A_r, b_r, rcond=None)[0]\n\n    u_r = V @ a\n    p_r = g + u_r\n    return p_r\n\ndef run_test_case(N, p_L, p_R, s_0, r_size, x_c, M, test_mus):\n    \"\"\"Runs a single test case for both greedy and uniform strategies.\"\"\"\n    x = np.linspace(0.0, 1.0, N)\n    dx = 1.0 / (N - 1)\n    \n    # Critical location indices\n    crit_indices = [np.argmin(np.abs(x - val)) for val in x_c]\n    \n    # Lifting function\n    g = p_L + (p_R - p_L) * x\n    \n    # Training parameters\n    train_mus = np.linspace(-0.9, 0.9, M)\n\n    # --- Greedy Strategy ---\n    V_greedy = np.zeros((N, 0))\n    greedy_mus = list(train_mus)\n    \n    w = np.ones(N)\n    alpha = 50.0\n    for idx in crit_indices:\n        w[idx] = alpha\n        if idx > 0:\n            w[idx-1] = max(w[idx-1], alpha/2.0)\n        if idx < N-1:\n            w[idx+1] = max(w[idx+1], alpha/2.0)\n            \n    for _ in range(r_size):\n        max_err = -1.0\n        best_mu = -1.0\n        \n        for mu in greedy_mus:\n            A, b = assemble_fom(N, p_L, p_R, s_0, mu)\n            p_r = solve_rom(A, b, g, V_greedy)\n            \n            # Calculate weighted residual norm\n            residual = A @ p_r - b\n            # Zero out BC rows as they are exactly satisfied by construction\n            residual[0] = 0\n            residual[-1] = 0\n            err_indicator = np.linalg.norm(np.sqrt(w) * residual)\n\n            if err_indicator > max_err:\n                max_err = err_indicator\n                best_mu = mu\n        \n        # Add snapshot for best_mu to basis\n        p_fom_best = solve_fom(N, p_L, p_R, s_0, best_mu)\n        u_new = p_fom_best - g\n        \n        # Orthogonalize and add to basis\n        proj = V_greedy @ (V_greedy.T @ u_new)\n        u_orth = u_new - proj\n        norm_u_orth = np.linalg.norm(u_orth)\n        if norm_u_orth > 1e-10:\n            V_greedy = np.c_[V_greedy, u_orth / norm_u_orth]\n        \n        greedy_mus.remove(best_mu)\n\n    # --- Uniform Strategy ---\n    uniform_mus = np.linspace(-0.9, 0.9, r_size)\n    uniform_snapshots_u = []\n    for mu in uniform_mus:\n        p_fom = solve_fom(N, p_L, p_R, s_0, mu)\n        u_snapshot = p_fom - g\n        uniform_snapshots_u.append(u_snapshot)\n    \n    V_uniform = get_orthonormal_basis(uniform_snapshots_u)\n\n    # --- Error Evaluation ---\n    def evaluate_error(V, test_mus):\n        total_abs_err = 0.0\n        for mu_test in test_mus:\n            A, b = assemble_fom(N, p_L, p_R, s_0, mu_test)\n            p_fom = solve_fom(N, p_L, p_R, s_0, mu_test)\n            p_r = solve_rom(A, b, g, V)\n            \n            abs_err_at_crit = np.abs(p_fom[crit_indices] - p_r[crit_indices])\n            mean_abs_err_per_mu = np.mean(abs_err_at_crit)\n            total_abs_err += mean_abs_err_per_mu\n\n        return total_abs_err / len(test_mus)\n\n    err_greedy = evaluate_error(V_greedy, test_mus)\n    err_uniform = evaluate_error(V_uniform, test_mus)\n\n    return err_greedy, err_uniform\n\n\ndef solve():\n    # Define test cases\n    test_cases = {\n        'A': {\n            'N': 50, 'p_L': 1.0, 'p_R': 0.0, 's_0': 0.2, 'r': 5,\n            'x_c': [0.3, 0.7], 'M': 21,\n            'test_mus': [-0.9, -0.45, 0.0, 0.45, 0.9]\n        },\n        'B': {\n            'N': 80, 'p_L': -1.0, 'p_R': 1.0, 's_0': 0.0, 'r': 7,\n            'x_c': [0.95], 'M': 21,\n            'test_mus': [-0.9, -0.45, 0.0, 0.45, 0.9]\n        },\n        'C': {\n            'N': 100, 'p_L': 0.5, 'p_R': 0.5, 's_0': 0.3, 'r': 6,\n            'x_c': [0.5], 'M': 21,\n            'test_mus': [-0.9, -0.45, 0.0, 0.45, 0.9]\n        }\n    }\n\n    results = []\n    \n    # Case A\n    params_A = test_cases['A']\n    err_A_g, err_A_u = run_test_case(\n        params_A['N'], params_A['p_L'], params_A['p_R'], params_A['s_0'],\n        params_A['r'], params_A['x_c'], params_A['M'], params_A['test_mus']\n    )\n    results.extend([err_A_g, err_A_u])\n    \n    # Case B\n    params_B = test_cases['B']\n    err_B_g, err_B_u = run_test_case(\n        params_B['N'], params_B['p_L'], params_B['p_R'], params_B['s_0'],\n        params_B['r'], params_B['x_c'], params_B['M'], params_B['test_mus']\n    )\n    results.extend([err_B_g, err_B_u])\n\n    # Case C\n    params_C = test_cases['C']\n    err_C_g, err_C_u = run_test_case(\n        params_C['N'], params_C['p_L'], params_C['p_R'], params_C['s_0'],\n        params_C['r'], params_C['x_c'], params_C['M'], params_C['test_mus']\n    )\n    results.extend([err_C_g, err_C_u])\n    \n    # Format and print the final output\n    formatted_results = [f'{res:.6f}' for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}