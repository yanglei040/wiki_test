{
    "hands_on_practices": [
        {
            "introduction": "The foundation of direct methods for symmetric positive definite (SPD) systems, ubiquitous in geomechanics, is the Cholesky factorization. Before relying on software libraries, it is essential to understand the mechanics of this factorization by hand. This exercise guides you through the step-by-step process of factorizing a small stiffness matrix and solving the resulting triangular systems, reinforcing the core algorithm that underpins many direct solvers.",
            "id": "3517762",
            "problem": "A three-degree-of-freedom linear system arises from the finite element discretization of a small-strain, linear elastic one-dimensional bar segment in computational geomechanics. By the principle of minimum potential energy and the coercivity of the elastic bilinear form, the assembled stiffness matrix is symmetric positive definite. You are given the stiffness matrix $A \\in \\mathbb{R}^{3 \\times 3}$ and the load vector $b \\in \\mathbb{R}^{3}$:\n$$\nA \\;=\\; \\begin{pmatrix}\n4  2  0 \\\\\n2  10  6 \\\\\n0  6  5\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n8 \\\\\n40 \\\\\n27\n\\end{pmatrix}.\n$$\nUsing only the definition of the Cholesky factorization as $A = L L^{\\top}$ where $L$ is lower triangular with positive diagonal entries, factor $A$ and then solve the linear system $A x = b$ by forward and backward substitution. Report the third component $x_{3}$ of the solution vector $x$. Express your final answer as an exact number. Do not include units. No rounding is required.",
            "solution": "The problem is to solve the linear system $A x = b$ for the unknown vector $x \\in \\mathbb{R}^3$, where the matrix $A$ and vector $b$ are given by:\n$$\nA \\;=\\; \\begin{pmatrix}\n4  2  0 \\\\\n2  10  6 \\\\\n0  6  5\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n8 \\\\\n40 \\\\\n27\n\\end{pmatrix}.\n$$\nThe solution method is specified as Cholesky factorization.\n\nFirst, a validation of the problem statement is performed. The problem provides all necessary data ($A$ and $b$) and specifies a clear, objective task. The context is scientifically sound. A critical premise is that the matrix $A$ is symmetric positive definite (SPD), which is a necessary and sufficient condition for the existence of a unique Cholesky factorization.\n\nWe verify this premise.\nSymmetry: The matrix $A$ is symmetric since $A_{ij} = A_{ji}$ for all $i, j \\in \\{1, 2, 3\\}$. For instance, $A_{12} = 2 = A_{21}$ and $A_{23} = 6 = A_{32}$.\n\nPositive definiteness: We check the leading principal minors of $A$.\nThe first leading principal minor is $\\Delta_1 = |4| = 4 > 0$.\nThe second leading principal minor is $\\det\\begin{pmatrix} 4  2 \\\\ 2  10 \\end{pmatrix} = (4)(10) - (2)(2) = 40 - 4 = 36 > 0$.\nThe third leading principal minor is the determinant of $A$:\n$$\n\\det(A) = 4 \\det\\begin{pmatrix} 10  6 \\\\ 6  5 \\end{pmatrix} - 2 \\det\\begin{pmatrix} 2  6 \\\\ 0  5 \\end{pmatrix} + 0 = 4(50 - 36) - 2(10 - 0) = 4(14) - 20 = 56 - 20 = 36 > 0.\n$$\nSince all leading principal minors are positive, the matrix $A$ is indeed symmetric positive definite. The problem is valid.\n\nWe proceed to find the Cholesky factorization $A = L L^{\\top}$, where $L$ is a lower triangular matrix with positive diagonal entries:\n$$\nL = \\begin{pmatrix} l_{11}  0  0 \\\\ l_{21}  l_{22}  0 \\\\ l_{31}  l_{32}  l_{33} \\end{pmatrix}\n$$\nThe equation $A = L L^{\\top}$ is:\n$$\n\\begin{pmatrix} 4  2  0 \\\\ 2  10  6 \\\\ 0  6  5 \\end{pmatrix} = \\begin{pmatrix} l_{11}  0  0 \\\\ l_{21}  l_{22}  0 \\\\ l_{31}  l_{32}  l_{33} \\end{pmatrix} \\begin{pmatrix} l_{11}  l_{21}  l_{31} \\\\ 0  l_{22}  l_{32} \\\\ 0  0  l_{33} \\end{pmatrix} = \\begin{pmatrix} l_{11}^2  l_{11}l_{21}  l_{11}l_{31} \\\\ l_{21}l_{11}  l_{21}^2+l_{22}^2  l_{21}l_{31}+l_{22}l_{32} \\\\ l_{31}l_{11}  l_{31}l_{21}+l_{32}l_{22}  l_{31}^2+l_{32}^2+l_{33}^2 \\end{pmatrix}\n$$\nEquating the entries of $A$ with the entries of $L L^{\\top}$:\n\nFor the first column of $L$:\n$l_{11}^2 = 4 \\implies l_{11} = \\sqrt{4} = 2$.\n$l_{11}l_{21} = 2 \\implies 2 l_{21} = 2 \\implies l_{21} = 1$.\n$l_{11}l_{31} = 0 \\implies 2 l_{31} = 0 \\implies l_{31} = 0$.\n\nFor the second column of $L$:\n$l_{21}^2 + l_{22}^2 = 10 \\implies 1^2 + l_{22}^2 = 10 \\implies l_{22}^2 = 9 \\implies l_{22} = \\sqrt{9} = 3$.\n$l_{21}l_{31} + l_{22}l_{32} = 6 \\implies (1)(0) + (3)l_{32} = 6 \\implies 3l_{32} = 6 \\implies l_{32} = 2$.\n\nFor the third column of $L$:\n$l_{31}^2 + l_{32}^2 + l_{33}^2 = 5 \\implies 0^2 + 2^2 + l_{33}^2 = 5 \\implies 4 + l_{33}^2 = 5 \\implies l_{33}^2 = 1 \\implies l_{33} = \\sqrt{1} = 1$.\n\nThus, the Cholesky factor $L$ is:\n$$\nL = \\begin{pmatrix} 2  0  0 \\\\ 1  3  0 \\\\ 0  2  1 \\end{pmatrix}\n$$\nThe linear system $A x = b$ becomes $L L^{\\top} x = b$. We solve this in two steps. First, let $y = L^{\\top} x$ and solve the lower triangular system $L y = b$ for $y$ using forward substitution.\n$$\n\\begin{pmatrix} 2  0  0 \\\\ 1  3  0 \\\\ 0  2  1 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 40 \\\\ 27 \\end{pmatrix}\n$$\nThe first equation gives $2y_1 = 8 \\implies y_1 = 4$.\nThe second equation gives $y_1 + 3y_2 = 40 \\implies 4 + 3y_2 = 40 \\implies 3y_2 = 36 \\implies y_2 = 12$.\nThe third equation gives $2y_2 + y_3 = 27 \\implies 2(12) + y_3 = 27 \\implies 24 + y_3 = 27 \\implies y_3 = 3$.\nSo, the intermediate vector is $y = \\begin{pmatrix} 4 \\\\ 12 \\\\ 3 \\end{pmatrix}$.\n\nSecond, we solve the upper triangular system $L^{\\top} x = y$ for $x$ using backward substitution.\n$$\nL^{\\top} = \\begin{pmatrix} 2  1  0 \\\\ 0  3  2 \\\\ 0  0  1 \\end{pmatrix}\n$$\nThe system is:\n$$\n\\begin{pmatrix} 2  1  0 \\\\ 0  3  2 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 12 \\\\ 3 \\end{pmatrix}\n$$\nThe third equation gives $x_3 = 3$.\nThe second equation gives $3x_2 + 2x_3 = 12 \\implies 3x_2 + 2(3) = 12 \\implies 3x_2 + 6 = 12 \\implies 3x_2 = 6 \\implies x_2 = 2$.\nThe first equation gives $2x_1 + x_2 = 4 \\implies 2x_1 + 2 = 4 \\implies 2x_1 = 2 \\implies x_1 = 1$.\n\nThe solution vector is $x = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$.\nThe problem asks for the third component of the solution vector, which is $x_3$.\nThe value is $x_3 = 3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "While Cholesky factorization is elegant, its application to the large, sparse matrices from finite element models presents a major challenge: fill-in. This practice explores how the ordering of equations dramatically affects solver efficiency by connecting matrix structure to the underlying mesh graph. By finding a permutation that minimizes matrix profile and fill-in, you will gain practical insight into the reordering algorithms that are essential for making large-scale direct solves feasible.",
            "id": "3517776",
            "problem": "In computational geomechanics, the finite element discretization of a quasi-static, linear elastic body under plane strain leads to a symmetric positive definite stiffness matrix $A$, whose sparsity structure is governed by the mesh adjacency graph: vertices represent nodal unknowns and undirected edges represent coupling due to shared elements. The choice of permutation of the unknowns affects the matrix envelope, bandwidth, and fill-in during a direct solve such as the Cholesky factorization. Consider a small mesh whose adjacency graph $G$ has vertex set $V=\\{1,2,3,4,5,6\\}$ and edge set\n$$\nE=\\{(1,2),(2,3),(3,4),(4,1),(2,5),(4,6)\\}.\n$$\nThis is a chordless cycle $\\{1,2,3,4\\}$ with two leaves $\\{5\\}$ and $\\{6\\}$ attached to $2$ and $4$, respectively. The symmetric positive definite matrix $A$ arising from this mesh has nonzero pattern corresponding to $E$ together with the diagonal.\n\nFor a permutation $\\pi$ of $\\{1,2,3,4,5,6\\}$, define the permuted matrix $P A P^{\\top}$ where $P$ is the permutation matrix associated with $\\pi$, and define the matrix profile $\\mathcal{P}(\\pi)$ by\n$$\n\\mathcal{P}(\\pi)=\\sum_{i=1}^{6}\\left(i-\\ell_{i}(\\pi)\\right),\n$$\nwhere $\\ell_{i}(\\pi)=\\min\\{\\,j \\,\\mid\\, (P A P^{\\top})_{i j}\\neq 0\\,\\}$ is the leftmost column index of a nonzero in row $i$ (including the diagonal). Define the half-bandwidth $\\beta(\\pi)$ as\n$$\n\\beta(\\pi)=\\max\\{|i-j| \\,\\mid\\, (P A P^{\\top})_{i j}\\neq 0\\}.\n$$\nConsider the Cholesky factorization (no pivoting) of $P A P^{\\top}$ using the elimination order given by $\\pi$, and let $F(\\pi)$ be the total number of subdiagonal fill-in entries in the Cholesky factor $L$ that are zero in the corresponding positions of $P A P^{\\top}$.\n\nStarting from fundamental definitions of graph-induced sparsity and the elimination graph viewpoint, determine a permutation $\\pi^{\\star}$ that minimizes the matrix profile $\\mathcal{P}(\\pi)$ for the given graph $G$ and, for that permutation, compute:\n- the minimized matrix profile $\\mathcal{P}(\\pi^{\\star})$,\n- the half-bandwidth $\\beta(\\pi^{\\star})$, and\n- the fill-in count $F(\\pi^{\\star})$ in $L$.\n\nExpress the final answer as a row matrix containing $\\mathcal{P}(\\pi^{\\star})$, $\\beta(\\pi^{\\star})$, and $F(\\pi^{\\star})$. No rounding is required and no units are involved in the final answer.",
            "solution": "The user wants me to solve a sparse matrix problem related to computational geomechanics.\n\n### Step 1: Problem Validation\n\nFirst, I will validate the provided problem statement.\n\n#### Step 1.1: Extract Givens\n\n-   **Context**: Finite element discretization of a quasi-static, linear elastic body under plane strain.\n-   **Matrix**: A symmetric positive definite (SPD) stiffness matrix $A$ of size $6 \\times 6$.\n-   **Graph**: The sparsity pattern of $A$ corresponds to an adjacency graph $G$ with:\n    -   Vertex set $V=\\{1,2,3,4,5,6\\}$.\n    -   Edge set $E=\\{(1,2),(2,3),(3,4),(4,1),(2,5),(4,6)\\}$.\n    -   The non-zero pattern of $A$ is defined by $(A_{ij} \\neq 0)$ if $i=j$ or $(i,j) \\in E$.\n-   **Permutation**: A permutation $\\pi$ of $\\{1,2,3,4,5,6\\}$ defines a permuted matrix $P A P^{\\top}$.\n-   **Matrix Profile**: $\\mathcal{P}(\\pi)=\\sum_{i=1}^{6}\\left(i-\\ell_{i}(\\pi)\\right)$, where $\\ell_{i}(\\pi)=\\min\\{j \\mid (P A P^{\\top})_{i j}\\neq 0\\,\\}$.\n-   **Half-Bandwidth**: $\\beta(\\pi)=\\max\\{|i-j| \\mid (P A P^{\\top})_{i j}\\neq 0\\}$.\n-   **Fill-in**: $F(\\pi)$ is the number of subdiagonal entries that are zero in $P A P^{\\top}$ but become non-zero in its Cholesky factor $L$.\n\n#### Step 1.2: Validate Using Extracted Givens\n\n-   **Scientific Grounding**: The problem is grounded in the well-established field of numerical linear algebra, specifically sparse matrix techniques used in finite element analysis. All concepts (stiffness matrix, sparsity, Cholesky factorization, profile, bandwidth, fill-in, reordering algorithms) are standard and scientifically sound.\n-   **Well-Posedness**: The problem is well-posed. It asks to find a permutation that minimizes a defined objective function ($\\mathcal{P}(\\pi)$) over a finite set of permutations and then compute other well-defined quantities. A minimum is guaranteed to exist.\n-   **Objectivity**: The language is precise and mathematical, with no subjective or ambiguous terms.\n-   **Conclusion**: The problem statement is valid. It is scientifically sound, well-posed, objective, and complete.\n\n### Step 2: Solution\n\nThe problem asks for a permutation $\\pi^\\star$ that minimizes the matrix profile $\\mathcal{P}(\\pi)$, and for this permutation, to compute the profile $\\mathcal{P}(\\pi^\\star)$, half-bandwidth $\\beta(\\pi^\\star)$, and fill-in count $F(\\pi^\\star)$.\n\n#### Finding the Optimal Permutation $\\pi^\\star$\n\nMinimizing the matrix profile is a classic problem in sparse matrix reordering. While finding the absolute minimum is an NP-complete problem in general, the Reverse Cuthill-McKee (RCM) algorithm is a widely used and effective heuristic for this purpose. For small, structured graphs like the one given, RCM is very likely to yield the optimal ordering.\n\n1.  **Analyze the graph $G$**: The graph consists of a 4-cycle $\\{1,2,3,4\\}$ with two pendant vertices $\\{5\\}$ and $\\{6\\}$ attached to vertices $2$ and $4$, respectively. Let's find the eccentricities of all vertices. The distance $d(u,v)$ is the length of the shortest path between $u$ and $v$.\n    -   $d(1,v)$: $\\{d(1,1), d(1,2), ..., d(1,6)\\} = \\{0, 1, 2, 1, 2, 2\\}$. Eccentricity $e(1)=2$.\n    -   $d(2,v)$: $\\{1, 0, 1, 2, 1, 3\\}$. Eccentricity $e(2)=3$.\n    -   $d(3,v)$: $\\{2, 1, 0, 1, 2, 2\\}$. Eccentricity $e(3)=2$.\n    -   $d(4,v)$: $\\{1, 2, 1, 0, 3, 1\\}$. Eccentricity $e(4)=3$.\n    -   $d(5,v)$: $\\{2, 1, 2, 3, 0, 4\\}$. Eccentricity $e(5)=4$.\n    -   $d(6,v)$: $\\{2, 3, 2, 1, 4, 0\\}$. Eccentricity $e(6)=4$.\n    The diameter of the graph is $\\max(e(v)) = 4$. The peripheral nodes are $\\{5, 6\\}$.\n\n2.  **Apply Cuthill-McKee (CM)**: We start a BFS from a peripheral node, say $5$. In each level of the BFS, nodes are ordered by increasing degree.\n    -   Degrees of vertices: $\\deg(1)=2$, $\\deg(2)=3$, $\\deg(3)=2$, $\\deg(4)=3$, $\\deg(5)=1$, $\\deg(6)=1$.\n    -   Level 0: $\\{5\\}$ (queue: $[5]$)\n    -   Level 1: Neighbors of $5$ is $\\{2\\}$. (queue: $[2]$)\n    -   Level 2: Neighbors of $2$ are $\\{1,3,5\\}$. $5$ is visited. Add $\\{1,3\\}$. Both have degree $2$. Add them in numerical order: $1, 3$. (queue: $[1,3]$)\n    -   Level 3: Neighbors of $1$ and $3$. Neighbors of $1$ are $\\{2,4\\}$, $2$ is visited. Add $4$. Neighbors of $3$ are $\\{2,4\\}$, both visited. Add $\\{4\\}$. (queue: $[4]$)\n    -   Level 4: Neighbors of $4$ are $\\{1,3,6\\}$. $1,3$ are visited. Add $\\{6\\}$. (queue: $[6]$)\n    The CM ordering is $\\pi_{CM} = (5, 2, 1, 3, 4, 6)$.\n\n3.  **Reverse Cuthill-McKee (RCM)**: The RCM ordering is the reverse of the CM ordering.\n    $$\n    \\pi^\\star = \\text{reverse}(\\pi_{CM}) = (6, 4, 3, 1, 2, 5)\n    $$\n    This permutation means we relabel the vertices such that the original vertex $6$ becomes new vertex $1$, original $4$ becomes new $2$, and so on. Due to the graph's symmetry, starting with peripheral node $6$ would yield an equivalent ordering, e.g., $(5, 2, 1, 3, 4, 6)$, which gives the same results for the required metrics. We will proceed with $\\pi^\\star = (6, 4, 3, 1, 2, 5)$.\n\n#### Computing Metrics for $\\pi^\\star$\n\nLet's denote the permuted matrix by $A' = PAP^\\top$. Its structure is determined by the adjacency of the newly labeled vertices.\n-   New 1 (Old 6) is adjacent to Old 4 (New 2).\n-   New 2 (Old 4) is adjacent to Old {1,3,6} (New {4,3,1}).\n-   New 3 (Old 3) is adjacent to Old {2,4} (New {5,2}).\n-   New 4 (Old 1) is adjacent to Old {2,4} (New {5,2}).\n-   New 5 (Old 2) is adjacent to Old {1,3,5} (New {4,3,6}).\n-   New 6 (Old 5) is adjacent to Old 2 (New 5).\n\nThe non-zero off-diagonal structure of $A'$ is given by the edges (symmetric):\n$(1,2), (2,3), (2,4), (3,5), (4,5), (5,6)$.\n\n##### 1. Minimized Matrix Profile $\\mathcal{P}(\\pi^\\star)$\n\nThe profile is $\\mathcal{P}(\\pi^\\star) = \\sum_{i=1}^{6}(i - \\ell_i(\\pi^\\star))$, where $\\ell_i$ is the column index of the first non-zero entry in row $i$ of $A'$.\n-   Row 1 (Old 6): Adjacent to New 2. First non-zero is diagonal. $\\ell_1 = 1$.\n-   Row 2 (Old 4): Adjacent to New {1,3,4}. First non-zero is in column 1. $\\ell_2 = 1$.\n-   Row 3 (Old 3): Adjacent to New {2,5}. First non-zero is in column 2. $\\ell_3 = 2$.\n-   Row 4 (Old 1): Adjacent to New {2,5}. First non-zero is in column 2. $\\ell_4 = 2$.\n-   Row 5 (Old 2): Adjacent to New {3,4,6}. First non-zero is in column 3. $\\ell_5 = 3$.\n-   Row 6 (Old 5): Adjacent to New 5. First non-zero is in column 5. $\\ell_6 = 5$.\nThe profile is:\n$$\n\\mathcal{P}(\\pi^\\star) = (1-1) + (2-1) + (3-2) + (4-2) + (5-3) + (6-5) = 0 + 1 + 1 + 2 + 2 + 1 = 7\n$$\n\n##### 2. Half-Bandwidth $\\beta(\\pi^\\star)$\n\nThe half-bandwidth is $\\beta(\\pi^\\star) = \\max\\{|i-j| \\mid A'_{ij} \\neq 0\\}$. We check the absolute difference of indices for all edges in the new labeling:\n-   Edge (1,2): $|1-2| = 1$.\n-   Edge (2,3): $|2-3| = 1$. Edge (2,4): $|2-4| = 2$.\n-   Edge (3,5): $|3-5| = 2$.\n-   Edge (4,5): $|4-5| = 1$.\n-   Edge (5,6): $|5-6| = 1$.\nThe maximum difference is $2$.\n$$\n\\beta(\\pi^\\star) = 2\n$$\n\n##### 3. Fill-in Count $F(\\pi^\\star)$\n\nFill-in during Cholesky factorization ($A' = LL^\\top$) can be determined using the elimination graph model. We process the vertices of the graph of $A'$ in their natural order $1, 2, ..., 6$. When a vertex $k$ is eliminated, its neighbors in the remaining graph must form a clique. Any edge added to complete the clique is a fill-in.\n\n-   **Eliminate vertex 1**: Its only neighbor is vertex $2$. This is a trivial clique. No fill-in occurs.\n-   **Eliminate vertex 2**: Its neighbors in the current graph are $\\{3,4\\}$. The edge $(3,4)$ does not exist in the original graph of $A'$. Therefore, an edge $(3,4)$ must be added to make $\\{3,4\\}$ a clique. This creates one fill-in entry, corresponding to $L_{43}$ (the subdiagonal entry).\n-   **Eliminate vertex 3**: Its neighbors in the current graph are $\\{2,5\\}$ (from original graph) and $\\{4\\}$ (from fill-in). Since vertex $2$ is already eliminated, its neighbors are $\\{4,5\\}$. The edge $(4,5)$ already exists in the graph of $A'$. Thus, the neighbor set $\\{4,5\\}$ is already a clique. No fill-in occurs.\n-   **Eliminate vertex 4**: Its neighbors in the current graph are $\\{2,5\\}$ (original) and $\\{3\\}$ (fill-in). Vertices $2$ and $3$ are eliminated. The only remaining neighbor is $\\{5\\}$. This is a trivial clique. No fill-in occurs.\n-   **Eliminate vertex 5**: Its neighbors in the current graph are $\\{3,4,6\\}$. Vertices $3$ and $4$ are eliminated. The only remaining neighbor is $\\{6\\}$. This is a trivial clique. No fill-in occurs.\n\nThe elimination process introduces exactly one new edge, $(3,4)$. This corresponds to a single subdiagonal fill-in entry.\n$$\nF(\\pi^\\star) = 1\n$$\n\nIn summary, for the optimal permutation $\\pi^\\star=(6, 4, 3, 1, 2, 5)$, the computed values are: minimized profile $\\mathcal{P}(\\pi^\\star)=7$, half-bandwidth $\\beta(\\pi^\\star)=2$, and fill-in count $F(\\pi^\\star)=1$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n7  2  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "When system sizes become exceptionally large, iterative methods like the Conjugate Gradient (CG) algorithm offer a powerful alternative to direct solvers. Instead of factoring the matrix, these methods generate a sequence of approximate solutions that converge to the true solution. This hands-on coding exercise challenges you to implement the Preconditioned Conjugate Gradient (PCG) method, demonstrating its mechanics and the crucial role of preconditioning in accelerating convergence for realistic geomechanical problems.",
            "id": "3517819",
            "problem": "Consider linear systems arising in computational geomechanics, where the discrete equilibrium of a linear elastic body under small deformation yields a symmetric positive definite (SPD) stiffness matrix. For an SPD matrix $A \\in \\mathbb{R}^{n \\times n}$ and a right-hand side vector $b \\in \\mathbb{R}^{n}$, the solution $x \\in \\mathbb{R}^{n}$ minimizes the quadratic energy functional $\\phi(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$. Iterative methods such as the Conjugate Gradient (CG) method and its preconditioned variant (Preconditioned Conjugate Gradient, PCG) exploit this structure by generating search directions that are $A$-orthogonal and performing exact line minimization of $\\phi(x)$ along these directions. A left preconditioner $M$ that is SPD approximates $A$ and defines the weighted inner product associated with the preconditioned residual through $z = M^{-1} r$, where $r = b - A x$ is the residual.\n\nYour task is to carry out exactly two iterations of the left-preconditioned Conjugate Gradient method starting from a given initial guess $x^{(0)}$, using a specified SPD preconditioner $M$ for each of the following test cases. After performing two iterations, report the updated solution $x^{(2)}$ and the Euclidean norms of the residuals after the first and second iterations, namely $\\| r^{(1)} \\|_{2}$ and $\\| r^{(2)} \\|_{2}$, where $r^{(k)} = b - A x^{(k)}$. All floating-point outputs must be rounded to eight decimal places.\n\nDefinitions and constraints:\n- $A$ must be SPD.\n- $M$ must be SPD and used as a left preconditioner, meaning the preconditioned residual is $z^{(k)} = M^{-1} r^{(k)}$.\n- The initial residual is $r^{(0)} = b - A x^{(0)}$.\n- Perform exactly two iterations, producing $x^{(1)}$ and $x^{(2)}$.\n- Compute and report $\\| r^{(1)} \\|_{2}$ and $\\| r^{(2)} \\|_{2}$.\n- Express all floating-point outputs rounded to eight decimal places.\n\nTest suite:\n- Test case $1$ (one-dimensional diffusion-like stiffness with Jacobi preconditioner):\n  - $$A_1 = \\begin{bmatrix} 2  -1  0  0 \\\\ -1  2  -1  0 \\\\ 0  -1  2  -1 \\\\ 0  0  -1  2 \\end{bmatrix}, \\quad b_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad x_1^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad M_1 = \\operatorname{diag}(A_1) = \\begin{bmatrix} 2  0  0  0 \\\\ 0  2  0  0 \\\\ 0  0  2  0 \\\\ 0  0  0  2 \\end{bmatrix}.$$\n- Test case $2$ (two-dimensional elasticity-like $2\\times 2$ block structure with block-Jacobi preconditioner and nonzero initial guess):\n  - $$A_2 = \\begin{bmatrix} 4  1.2  0.5  0 \\\\ 1.2  3.5  0  0.3 \\\\ 0.5  0  2.8  1.1 \\\\ 0  0.3  1.1  2.5 \\end{bmatrix}, \\quad b_2 = \\begin{bmatrix} 1 \\\\ 0.5 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}, \\quad x_2^{(0)} = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\\\ 0.4 \\end{bmatrix},$$\n  - $$M_2 = \\begin{bmatrix} 4  1.2  0  0 \\\\ 1.2  3.5  0  0 \\\\ 0  0  2.8  1.1 \\\\ 0  0  1.1  2.5 \\end{bmatrix},$$\n  that is, $M_2$ equals the block-diagonal part of $A_2$ consisting of the upper-left and lower-right $2\\times 2$ blocks.\n- Test case $3$ (ill-conditioned SPD system constructed via Gram matrix with identity preconditioner):\n  - Let $$Q = \\begin{bmatrix} 1  0.999  0  0 \\\\ 0  0.001  1  0.999 \\\\ 0  0  0.001  1 \\\\ 0.001  0  0  0.001 \\end{bmatrix}, \\quad \\gamma = 10^{-6}, \\quad A_3 = Q^{\\top} Q + \\gamma I,$$\n  - $$b_3 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{bmatrix}, \\quad x_3^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad M_3 = I.$$\n\nOutput specification:\n- For each test case $i \\in \\{1,2,3\\}$, produce the list $[x^{(2)}_0, x^{(2)}_1, x^{(2)}_2, x^{(2)}_3, \\| r^{(1)} \\|_{2}, \\| r^{(2)} \\|_{2}]$, where $x^{(2)}_j$ denotes the $j$-th component of $x^{(2)}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result as its own sublist, and no spaces. For example, the final output format must be like $$[ [\\cdots], [\\cdots], [\\cdots] ]$$ but with actual numeric values, represented exactly as in Python list syntax and rounded to eight decimal places, with no spaces anywhere in the line.",
            "solution": "The problem is valid. It presents a well-defined computational task based on the standard left-preconditioned Conjugate Gradient (PCG) method, a cornerstone of numerical linear algebra for solving symmetric positive definite (SPD) linear systems. All provided matrices, vectors, and initial conditions are mathematically consistent, scientifically grounded in the context of computational mechanics, and sufficient to produce a unique, verifiable solution. The matrices $A_i$ and preconditioners $M_i$ for each test case have been verified to be SPD as required.\n\nThe left-preconditioned Conjugate Gradient algorithm is an iterative method for solving $Ax=b$ where $A$ is an SPD matrix and the preconditioner $M$ is also SPD. The algorithm can be summarized as follows:\n\nLet $x^{(0)}$ be the initial guess.\nCompute the initial residual:\n$r^{(0)} = b - A x^{(0)}$\nSolve for the preconditioned residual:\n$M z^{(0)} = r^{(0)}$\nSet the initial search direction:\n$p^{(0)} = z^{(0)}$\nCompute the initial squared preconditioned residual norm:\n$\\rho_0 = (r^{(0)})^{\\top} z^{(0)}$\n\nFor each iteration $k = 0, 1, 2, \\dots$:\n1. Compute the matrix-vector product: $w^{(k)} = A p^{(k)}$\n2. Compute the step size: $\\alpha_k = \\frac{\\rho_k}{(p^{(k)})^{\\top} w^{(k)}}$\n3. Update the solution: $x^{(k+1)} = x^{(k)} + \\alpha_k p^{(k)}$\n4. Update the residual: $r^{(k+1)} = r^{(k)} - \\alpha_k w^{(k)}$\n5. Solve for the new preconditioned residual: $M z^{(k+1)} = r^{(k+1)}$\n6. Update the squared preconditioned residual norm: $\\rho_{k+1} = (r^{(k+1)})^{\\top} z^{(k+1)}$\n7. Compute the improvement factor: $\\beta_k = \\frac{\\rho_{k+1}}{\\rho_k}$\n8. Update the search direction: $p^{(k+1)} = z^{(k+1)} + \\beta_k p^{(k)}$\n\nWe will now apply exactly two iterations ($k=0$ and $k=1$) of this algorithm to each test case.\n\n### Test Case 1\n\nGiven:\n$A_1 = \\begin{bmatrix} 2  -1  0  0 \\\\ -1  2  -1  0 \\\\ 0  -1  2  -1 \\\\ 0  0  -1  2 \\end{bmatrix}$, $b_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, $x_1^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $M_1 = \\begin{bmatrix} 2  0  0  0 \\\\ 0  2  0  0 \\\\ 0  0  2  0 \\\\ 0  0  0  2 \\end{bmatrix}$.\n$M_1^{-1} = 0.5 I$.\n\n**Initialization ($k=0$):**\n- $r^{(0)} = b_1 - A_1 x^{(0)} = [1, 0, 1, 0]^{\\top}$\n- $z^{(0)} = M_1^{-1} r^{(0)} = [0.5, 0, 0.5, 0]^{\\top}$\n- $p^{(0)} = z^{(0)}$\n- $\\rho_0 = (r^{(0)})^{\\top} z^{(0)} = 1.0$\n\n**Iteration 1 ($k=0 \\to k=1$):**\n- $w^{(0)} = A_1 p^{(0)} = [1.0, -1.0, 1.0, -0.5]^{\\top}$\n- $\\alpha_0 = \\rho_0 / ((p^{(0)})^{\\top} w^{(0)}) = 1.0 / 1.0 = 1.0$\n- $x^{(1)} = x^{(0)} + \\alpha_0 p^{(0)} = [0.5, 0.0, 0.5, 0.0]^{\\top}$\n- $r^{(1)} = r^{(0)} - \\alpha_0 w^{(0)} = [0.0, 1.0, 0.0, 0.5]^{\\top}$\n- $\\|r^{(1)}\\|_2 = \\sqrt{1.0^2 + 0.5^2} = \\sqrt{1.25} \\approx 1.11803399$\n- $z^{(1)} = M_1^{-1} r^{(1)} = [0.0, 0.5, 0.0, 0.25]^{\\top}$\n- $\\rho_1 = (r^{(1)})^{\\top} z^{(1)} = 0.625$\n- $\\beta_0 = \\rho_1 / \\rho_0 = 0.625$\n- $p^{(1)} = z^{(1)} + \\beta_0 p^{(0)} = [0.3125, 0.5, 0.3125, 0.25]^{\\top}$\n\n**Iteration 2 ($k=1 \\to k=2$):**\n- $w^{(1)} = A_1 p^{(1)} = [0.125, 0.375, -0.125, 0.1875]^{\\top}$\n- $\\alpha_1 = \\rho_1 / ((p^{(1)})^{\\top} w^{(1)}) = 0.625 / 0.234375 = 2.66666667...$\n- $x^{(2)} = x^{(1)} + \\alpha_1 p^{(1)} = [1.33333333, 1.33333333, 1.33333333, 0.66666667]^{\\top}$\n- $r^{(2)} = r^{(1)} - \\alpha_1 w^{(1)} = [-0.33333333, 0.0, 0.33333333, 0.0]^{\\top}$\n- $\\|r^{(2)}\\|_2 = \\sqrt{(-1/3)^2 + (1/3)^2} = \\sqrt{2/9} \\approx 0.47140452$\n\n**Result for Case 1:**\n- $x^{(2)} = [1.33333333, 1.33333333, 1.33333333, 0.66666667]^{\\top}$\n- $\\|r^{(1)}\\|_2 = 1.11803399$\n- $\\|r^{(2)}\\|_2 = 0.47140452$\n\n### Test Case 2\n\nGiven:\n$A_2 = \\begin{bmatrix} 4  1.2  0.5  0 \\\\ 1.2  3.5  0  0.3 \\\\ 0.5  0  2.8  1.1 \\\\ 0  0.3  1.1  2.5 \\end{bmatrix}$, $b_2 = \\begin{bmatrix} 1 \\\\ 0.5 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$, $x_2^{(0)} = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\\\ 0.4 \\end{bmatrix}$, $M_2 = \\begin{bmatrix} 4  1.2  0  0 \\\\ 1.2  3.5  0  0 \\\\ 0  0  2.8  1.1 \\\\ 0  0  1.1  2.5 \\end{bmatrix}$.\n\n**Initialization ($k=0$):**\n- $A_2 x^{(0)} = [0.185, -0.46, 0.63, 0.995]^{\\top}$\n- $r^{(0)} = b_2 - A_2 x^{(0)} = [0.815, 0.96, -0.83, -0.695]^{\\top}$\n- Solving $M_2 z^{(0)} = r^{(0)}$ gives $z^{(0)} = [0.13539013, 0.22786624, -0.22633851, -0.17841105]^{\\top}$\n- $p^{(0)} = z^{(0)}$\n- $\\rho_0 = (r^{(0)})^{\\top} z^{(0)} \\approx 0.64100000$\n\n**Iteration 1 ($k=0 \\to k=1$):**\n- $w^{(0)} = A_2 p^{(0)} = [0.42839172, 1.05537580, -0.28203799, 0.22748359]^{\\top}$\n- $\\alpha_0 = \\rho_0 / ((p^{(0)})^{\\top} w^{(0)}) \\approx 0.64100000 / 0.28711809 \\approx 2.23243930$\n- $x^{(1)} = x^{(0)} + \\alpha_0 p^{(0)} = [0.40237730, 0.30906233, -0.45524800, 0.00199121]^{\\top}$\n- $r^{(1)} = r^{(0)} - \\alpha_0 w^{(0)} = [-0.14197395, -1.39700995, -0.20016480, -1.20239209]^{\\top}$\n- $\\|r^{(1)}\\|_2 \\approx 1.83407987$\n- Solving $M_2 z^{(1)} = r^{(1)}$ gives $z^{(1)} = [0.25208447, -0.45780517, 0.08832790, -0.47895475]^{\\top}$\n- $\\rho_1 = (r^{(1)})^{\\top} z^{(1)} \\approx 0.82522851$\n- $\\beta_0 = \\rho_1 / \\rho_0 \\approx 0.82522851 / 0.64100000 \\approx 1.28740797$\n- $p^{(1)} = z^{(1)} + \\beta_0 p^{(0)} = [0.42621453, -0.16377755, -0.10300854, -0.70823616]^{\\top}$\n\n**Iteration 2 ($k=1 \\to k=2$):**\n- $w^{(1)} = A_2 p^{(1)} = [1.40192778, -0.92728955, -0.98592358, -2.00032585]^{\\top}$\n- $\\alpha_1 = \\rho_1 / ((p^{(1)})^{\\top} w^{(1)}) \\approx 0.82522851 / 2.30138096 \\approx 0.35857022$\n- $x^{(2)} = x^{(1)} + \\alpha_1 p^{(1)} = [0.55529555, 0.25042894, -0.49219356, -0.25265636]^{\\top}$\n- $r^{(2)} = r^{(1)} - \\alpha_1 w^{(1)} = [-0.64491950, -1.06450417, 0.15286297, -0.48512534]^{\\top}$\n- $\\|r^{(2)}\\|_2 \\approx 1.34007870$\n\n**Result for Case 2:**\n- $x^{(2)} = [0.55529555, 0.25042894, -0.49219356, -0.25265636]^{\\top}$\n- $\\|r^{(1)}\\|_2 = 1.83407987$\n- $\\|r^{(2)}\\|_2 = 1.34007870$\n\n### Test Case 3\nGiven:\n$A_3 = Q^{\\top} Q + (10^{-6}) I$ with $Q$ as defined, $b_3 = [1, -1, 1, -1]^{\\top}$, $x_3^{(0)} = [0, 0, 0, 0]^{\\top}$, $M_3 = I$.\nFor $M_3 = I$, the PCG algorithm reduces to the standard CG algorithm, with $z^{(k)} = r^{(k)}$.\n\n**Initialization ($k=0$):**\n- $r^{(0)} = b_3 = [1, -1, 1, -1]^{\\top}$\n- $z^{(0)} = r^{(0)}$\n- $p^{(0)} = r^{(0)}$\n- $\\rho_0 = (r^{(0)})^{\\top} r^{(0)} = 4.0$\n\n**Iteration 1 ($k=0 \\to k=1$):**\n- $w^{(0)} = A_3 p^{(0)} = [1.000001, -1.996001, 2.996001, -1.996001]^{\\top}$\n- $\\alpha_0 = \\rho_0 / ((p^{(0)})^{\\top} w^{(0)}) = 4.0 / 7.988004 \\approx 0.50075050$\n- $x^{(1)} = x^{(0)} + \\alpha_0 p^{(0)} = [0.50075050, -0.50075050, 0.50075050, -0.50075050]^{\\top}$\n- $r^{(1)} = r^{(0)} - \\alpha_0 w^{(0)} = [0.49924850, -0.00199700, -0.49924850, -0.00199700]^{\\top}$\n- $\\|r^{(1)}\\|_2 \\approx 0.70603723$\n- $z^{(1)} = r^{(1)}$\n- $\\rho_1 = (r^{(1)})^{\\top} r^{(1)} \\approx 0.49848882$\n- $\\beta_0 = \\rho_1 / \\rho_0 \\approx 0.49848882 / 4.0 \\approx 0.12462221$\n- $p^{(1)} = z^{(1)} + \\beta_0 p^{(0)} = [0.62387271, -0.12661921, 0.37462630, -0.12661921]^{\\top}$\n\n**Iteration 2 ($k=1 \\to k=2$):**\n- $w^{(1)} = A_3 p^{(1)} = [0.37340402, -0.25200159, 0.50116639, -0.25200159]^{\\top}$\n- $\\alpha_1 = \\rho_1 / ((p^{(1)})^{\\top} w^{(1)}) \\approx 0.49848882 / 0.23351336 \\approx 2.13471018$\n- $x^{(2)} = x^{(1)} + \\alpha_1 p^{(1)} = [1.83363320, -0.77197369, 1.30396071, -0.77197369]^{\\top}$\n- $r^{(2)} = r^{(1)} - \\alpha_1 w^{(1)} = [-0.29810501, 0.25055811, -1.56942700, 0.25055811]^{\\top}$\n- $\\|r^{(2)}\\|_2 \\approx 1.63189035$\n\n**Result for Case 3:**\n- $x^{(2)} = [1.83363320, -0.77197369, 1.30396071, -0.77197369]^{\\top}$\n- $\\|r^{(1)}\\|_2 = 0.70603723$\n- $\\|r^{(2)}\\|_2 = 1.63189035$\nThe increase in residual norm from iteration $1$ to $2$ is plausible for CG on ill-conditioned systems. The method guarantees convergence in the $A$-norm of the error, not necessarily monotonic decrease of the Euclidean residual norm.",
            "answer": "```\n[[1.33333333,1.33333333,1.33333333,0.66666667,1.11803399,0.47140452],[0.55529555,0.25042894,-0.49219356,-0.25265636,1.83407987,1.34007870],[1.83363320,-0.77197369,1.30396071,-0.77197369,0.70603723,1.63189035]]\n```"
        }
    ]
}