{
    "hands_on_practices": [
        {
            "introduction": "Mastering any numerical method begins with a solid grasp of its fundamental steps. This first practice invites you to perform a single iteration of the full Newton-Raphson scheme by hand for a simple two-degree-of-freedom system. By explicitly forming the residual vector and the tangent matrix, you will see how the method transforms a complex nonlinear problem into a sequence of solvable linear ones. This foundational exercise is crucial for demystifying the algorithm and building intuition for its mechanics before applying it to more complex, large-scale problems.",
            "id": "3526509",
            "problem": "A two-degree-of-freedom nonlinear equilibrium problem in computational geomechanics is characterized by the residual vector $\\mathbf{r}(\\mathbf{u}) \\in \\mathbb{R}^{2}$, where $\\mathbf{u} = \\begin{bmatrix} u_{1} \\\\ u_{2} \\end{bmatrix}$ represents generalized displacements. The equilibrium condition requires $\\mathbf{r}(\\mathbf{u}) = \\mathbf{0}$, with the residual defined by\n$$\n\\mathbf{r}(\\mathbf{u}) = \\begin{bmatrix}\nu_{1}^{2} + u_{2} - 3 \\\\\n\\sin(u_{1}) + u_{2}^{3} - 1\n\\end{bmatrix}.\n$$\nWithin the Full Newton–Raphson scheme, the tangent matrix (also called the consistent Jacobian or tangent stiffness) is defined as $ \\mathbf{K}_{t}(\\mathbf{u}) = \\dfrac{\\partial \\mathbf{r}}{\\partial \\mathbf{u}}(\\mathbf{u})$, and each iteration solves the linearized equilibrium update $\\mathbf{K}_{t}(\\mathbf{u}^{(i)}) \\, \\Delta \\mathbf{u}^{(i)} = -\\mathbf{r}(\\mathbf{u}^{(i)})$. In contrast, the Modified Newton–Raphson scheme freezes the tangent matrix across iterations while still updating the residual, but both schemes coincide at the first iteration when starting from the same initial guess.\n\nStarting from the initial guess $\\mathbf{u}^{(0)} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, perform one Full Newton–Raphson iteration:\n- Explicitly form the tangent matrix $\\mathbf{K}_{t}(\\mathbf{u}^{(0)})$ by linearizing $\\mathbf{r}(\\mathbf{u})$.\n- Solve for the increment $\\Delta \\mathbf{u}^{(0)}$ in the linear system $\\mathbf{K}_{t}(\\mathbf{u}^{(0)}) \\, \\Delta \\mathbf{u}^{(0)} = -\\mathbf{r}(\\mathbf{u}^{(0)})$.\n\nExpress the final answer as a single analytic expression for the row vector $\\Delta \\mathbf{u}^{(0)}$ using standard mathematical functions, without numerical rounding and without units.",
            "solution": "The problem is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. Therefore, it is deemed valid. We proceed with the solution by applying one iteration of the Full Newton-Raphson scheme.\n\nThe problem defines a nonlinear system of equations $\\mathbf{r}(\\mathbf{u}) = \\mathbf{0}$, where the displacement vector is $\\mathbf{u} = \\begin{bmatrix} u_{1} \\\\ u_{2} \\end{bmatrix}$ and the residual vector is\n$$\n\\mathbf{r}(\\mathbf{u}) = \\begin{bmatrix} r_1(\\mathbf{u}) \\\\ r_2(\\mathbf{u}) \\end{bmatrix} = \\begin{bmatrix}\nu_{1}^{2} + u_{2} - 3 \\\\\n\\sin(u_{1}) + u_{2}^{3} - 1\n\\end{bmatrix}.\n$$\nThe Full Newton–Raphson method is an iterative procedure to find the root of $\\mathbf{r}(\\mathbf{u})$. Each iteration refines the current guess $\\mathbf{u}^{(i)}$ by computing an increment $\\Delta \\mathbf{u}^{(i)}$ and updating the solution as $\\mathbf{u}^{(i+1)} = \\mathbf{u}^{(i)} + \\Delta \\mathbf{u}^{(i)}$. The increment is found by solving the linear system\n$$\n\\mathbf{K}_{t}(\\mathbf{u}^{(i)}) \\, \\Delta \\mathbf{u}^{(i)} = -\\mathbf{r}(\\mathbf{u}^{(i)}),\n$$\nwhere $\\mathbf{K}_{t}(\\mathbf{u})$ is the tangent matrix (Jacobian) of the residual vector $\\mathbf{r}(\\mathbf{u})$.\n\nWe are asked to perform one iteration starting from the initial guess $\\mathbf{u}^{(0)} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n\nFirst, we evaluate the residual vector $\\mathbf{r}$ at the initial guess $\\mathbf{u}^{(0)}$:\n$$\n\\mathbf{r}(\\mathbf{u}^{(0)}) = \\begin{bmatrix}\n(1)^{2} + (0) - 3 \\\\\n\\sin(1) + (0)^{3} - 1\n\\end{bmatrix} = \\begin{bmatrix}\n-2 \\\\\n\\sin(1) - 1\n\\end{bmatrix}.\n$$\n\nSecond, we formulate the tangent matrix $\\mathbf{K}_{t}(\\mathbf{u})$ by taking the partial derivatives of the components of $\\mathbf{r}(\\mathbf{u})$ with respect to the components of $\\mathbf{u}$:\n$$\n\\mathbf{K}_{t}(\\mathbf{u}) = \\frac{\\partial \\mathbf{r}}{\\partial \\mathbf{u}}(\\mathbf{u}) = \\begin{bmatrix}\n\\frac{\\partial r_1}{\\partial u_1} & \\frac{\\partial r_1}{\\partial u_2} \\\\\n\\frac{\\partial r_2}{\\partial u_1} & \\frac{\\partial r_2}{\\partial u_2}\n\\end{bmatrix}.\n$$\nThe individual partial derivatives are:\n$$\n\\frac{\\partial r_1}{\\partial u_1} = \\frac{\\partial}{\\partial u_1}(u_{1}^{2} + u_{2} - 3) = 2u_{1}\n$$\n$$\n\\frac{\\partial r_1}{\\partial u_2} = \\frac{\\partial}{\\partial u_2}(u_{1}^{2} + u_{2} - 3) = 1\n$$\n$$\n\\frac{\\partial r_2}{\\partial u_1} = \\frac{\\partial}{\\partial u_1}(\\sin(u_{1}) + u_{2}^{3} - 1) = \\cos(u_{1})\n$$\n$$\n\\frac{\\partial r_2}{\\partial u_2} = \\frac{\\partial}{\\partial u_2}(\\sin(u_{1}) + u_{2}^{3} - 1) = 3u_{2}^{2}\n$$\nThus, the tangent matrix is:\n$$\n\\mathbf{K}_{t}(\\mathbf{u}) = \\begin{bmatrix}\n2u_{1} & 1 \\\\\n\\cos(u_{1}) & 3u_{2}^{2}\n\\end{bmatrix}.\n$$\n\nThird, we evaluate this tangent matrix at the initial guess $\\mathbf{u}^{(0)} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$:\n$$\n\\mathbf{K}_{t}(\\mathbf{u}^{(0)}) = \\begin{bmatrix}\n2(1) & 1 \\\\\n\\cos(1) & 3(0)^{2}\n\\end{bmatrix} = \\begin{bmatrix}\n2 & 1 \\\\\n\\cos(1) & 0\n\\end{bmatrix}.\n$$\nThe determinant of this matrix is $\\det(\\mathbf{K}_{t}(\\mathbf{u}^{(0)})) = (2)(0) - (1)(\\cos(1)) = -\\cos(1)$. Since $1$ is not an odd multiple of $\\frac{\\pi}{2}$, $\\cos(1) \\neq 0$, so the matrix is invertible and a unique solution for the increment exists.\n\nFinally, we solve the linear system for the increment $\\Delta \\mathbf{u}^{(0)} = \\begin{bmatrix} \\Delta u_{1}^{(0)} \\\\ \\Delta u_{2}^{(0)} \\end{bmatrix}$:\n$$\n\\mathbf{K}_{t}(\\mathbf{u}^{(0)}) \\, \\Delta \\mathbf{u}^{(0)} = -\\mathbf{r}(\\mathbf{u}^{(0)})\n$$\n$$\n\\begin{bmatrix}\n2 & 1 \\\\\n\\cos(1) & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta u_{1}^{(0)} \\\\\n\\Delta u_{2}^{(0)}\n\\end{bmatrix}\n= -\\begin{bmatrix}\n-2 \\\\\n\\sin(1) - 1\n\\end{bmatrix}\n= \\begin{bmatrix}\n2 \\\\\n1 - \\sin(1)\n\\end{bmatrix}.\n$$\nThis matrix equation corresponds to the following system of two linear equations:\n$$\n\\begin{cases}\n2\\Delta u_{1}^{(0)} + \\Delta u_{2}^{(0)} = 2 & (1) \\\\\n\\cos(1)\\Delta u_{1}^{(0)} = 1 - \\sin(1) & (2)\n\\end{cases}\n$$\nFrom equation $(2)$, we solve for $\\Delta u_{1}^{(0)}$:\n$$\n\\Delta u_{1}^{(0)} = \\frac{1 - \\sin(1)}{\\cos(1)} = \\frac{1}{\\cos(1)} - \\frac{\\sin(1)}{\\cos(1)} = \\sec(1) - \\tan(1).\n$$\nSubstituting this expression into equation $(1)$ allows us to solve for $\\Delta u_{2}^{(0)}$:\n$$\n2(\\sec(1) - \\tan(1)) + \\Delta u_{2}^{(0)} = 2\n$$\n$$\n\\Delta u_{2}^{(0)} = 2 - 2(\\sec(1) - \\tan(1)) = 2(1 - \\sec(1) + \\tan(1)).\n$$\nThe resulting displacement increment is the vector $\\Delta \\mathbf{u}^{(0)} = \\begin{bmatrix} \\sec(1) - \\tan(1) \\\\ 2(1 - \\sec(1) + \\tan(1)) \\end{bmatrix}$. The problem requests the answer as a row vector.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\sec(1) - \\tan(1) & 2(1 - \\sec(1) + \\tan(1)) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A key advantage of the full Newton-Raphson method is its rapid, quadratic rate of convergence near a solution. This exercise moves from the mechanics of a single step to analyzing the overall performance of the iterative sequence. You will derive a general formula to estimate the observed order of convergence from a series of residual norms and then apply it to data from a simulation of an elastoplastic soil. This practice provides a powerful tool for validating solver implementations and understanding the hallmark behavior that makes the Newton-Raphson scheme so effective.",
            "id": "3526519",
            "problem": "A drained triaxial compression increment is solved using a nonlinear finite element (Finite Element (FE)) model of an elastoplastic soil obeying a pressure-dependent yield condition. The discrete static equilibrium is written as the residual equation $r(u) = f_{\\mathrm{int}}(u) - f_{\\mathrm{ext}} = 0$, where $u$ is the global displacement vector, $f_{\\mathrm{int}}(u)$ is the internal force vector, and $f_{\\mathrm{ext}}$ is the external force vector. Two iterative schemes are considered: the full Newton–Raphson method with the consistent elastoplastic tangent updated every iteration, and the modified Newton–Raphson method with the tangent matrix frozen at its value at the start of the load step.\n\nYou are given the Euclidean norms of the residual for the full Newton–Raphson iterations at a fixed load increment:\n$k=0: \\|r(u^{(0)})\\| = 1.0 \\times 10^{-1}$,\n$k=1: \\|r(u^{(1)})\\| = 1.0 \\times 10^{-2}$,\n$k=2: \\|r(u^{(2)})\\| = 1.0 \\times 10^{-4}$,\n$k=3: \\|r(u^{(3)})\\| = 1.0 \\times 10^{-8}$.\n\nStarting from first principles, use the definition of local convergence order $p$ based on the asymptotic model $\\|r(u^{(k+1)})\\| \\approx C \\|r(u^{(k)})\\|^{p}$ for some constant $C>0$ near the solution, and the linearization of the residual about the exact solution, to derive a computable estimator for the observed order $p$ from three consecutive iterates. Then, using the data above for the full Newton–Raphson scheme, compute the observed $p$.\n\nRound your final numerical answer to $4$ significant figures. The final reported value is dimensionless. In your reasoning, also state whether the observed behavior is compatible with quadratic, superlinear, or linear convergence in the sense of the local order $p$, but report only the estimated $p$ as your final answer.",
            "solution": "The problem requires the derivation of a computable estimator for the local order of convergence, $p$, and its calculation for a given set of residual norms from a full Newton-Raphson (FNR) scheme.\n\nThe analysis of convergence is typically formulated in terms of the error in the solution vector, $e^{(k)} = u^{(k)} - u^*$, where $u^{(k)}$ is the iterate at step $k$ and $u^*$ is the exact solution satisfying $r(u^*) = 0$. The order of convergence, $p$, is defined by the asymptotic relationship:\n$$ \\|e^{(k+1)}\\| \\approx C_e \\|e^{(k)}\\|^p $$\nfor some constant $C_e > 0$ as $k \\to \\infty$.\n\nThe problem provides data for the Euclidean norm of the residual vector, $\\|r(u^{(k)})\\|$, and posits a similar relationship:\n$$ \\|r(u^{(k+1)})\\| \\approx C \\|r(u^{(k)})\\|^p $$\nTo justify this model, we linearize the residual function $r(u)$ about the exact solution $u^*$. Using a first-order Taylor series expansion, we have:\n$$ r(u^{(k)}) = r(u^*) + \\left. \\frac{\\partial r}{\\partial u} \\right|_{u=u^*} (u^{(k)} - u^*) + O(\\|u^{(k)}-u^*\\|^2) $$\nThe term $\\frac{\\partial r}{\\partial u}$ is the tangent stiffness matrix, $K_T(u)$. At the solution $u^*$, we have $r(u^*) = 0$. Thus, for an iterate $u^{(k)}$ sufficiently close to $u^*$, we can write:\n$$ r(u^{(k)}) \\approx K_T(u^*) (u^{(k)} - u^*) = K_T^* e^{(k)} $$\nAssuming the tangent stiffness matrix at the solution, $K_T^*$, is non-singular, we can take the vector norms of both sides. This relates the norm of the residual to the norm of the error:\n$$ \\|r(u^{(k)})\\| \\approx \\|K_T^* e^{(k)}\\| \\le \\|K_T^*\\| \\|e^{(k)}\\| $$\nAnd similarly, $\\|e^{(k)}\\| = \\|(K_T^*)^{-1} r(u^{(k)})\\| \\le \\|(K_T^*)^{-1}\\| \\|r(u^{(k)})\\|$. This establishes that, in the vicinity of the solution, the norm of the residual is proportional to the norm of the error, i.e., $\\|r(u^{(k)})\\| \\propto \\|e^{(k)}\\|$. Let us write this as $\\|r(u^{(k)})\\| \\approx C_r \\|e^{(k)}\\|$ for some constant $C_r > 0$.\n\nSubstituting this relationship into the definition of convergence order for the error vector:\n$$ \\frac{\\|r(u^{(k+1)})\\|}{C_r} \\approx C_e \\left(\\frac{\\|r(u^{(k)})\\|}{C_r}\\right)^p $$\n$$ \\|r(u^{(k+1)})\\| \\approx (C_e C_r^{1-p}) \\|r(u^{(k)})\\|^p $$\nThis confirms the assumed model $\\|r(u^{(k+1)})\\| \\approx C \\|r(u^{(k)})\\|^p$, where the constant $C$ is given by $C_e C_r^{1-p}$. This derivation from first principles legitimizes using the sequence of residual norms to estimate the order of convergence.\n\nNow, we derive the computable estimator for $p$. Let $R_k = \\|r(u^{(k)})\\|$. From the model, we have two relations for consecutive iterations $k$ and $k+1$:\n$$ R_{k+1} \\approx C R_k^p $$\n$$ R_k \\approx C R_{k-1}^p $$\nThese relations are valid when the iteration is close to the solution, so the asymptotic behavior holds. To eliminate the unknown constant $C$, we take the natural logarithm of both equations:\n$$ \\ln(R_{k+1}) \\approx \\ln(C) + p \\ln(R_k) $$\n$$ \\ln(R_k) \\approx \\ln(C) + p \\ln(R_{k-1}) $$\nSubtracting the second logarithmic equation from the first yields:\n$$ \\ln(R_{k+1}) - \\ln(R_k) \\approx p (\\ln(R_k) - \\ln(R_{k-1})) $$\nUsing the property of logarithms, $\\ln(a) - \\ln(b) = \\ln(a/b)$, we can write:\n$$ \\ln\\left(\\frac{R_{k+1}}{R_k}\\right) \\approx p \\ln\\left(\\frac{R_k}{R_{k-1}}\\right) $$\nSolving for $p$, we obtain the estimator for the observed order of convergence from three consecutive iterates $(k-1, k, k+1)$:\n$$ p \\approx \\frac{\\ln(R_{k+1} / R_k)}{\\ln(R_k / R_{k-1})} $$\nNext, we use the provided data for the full Newton-Raphson scheme to compute $p$. The given residual norms are:\n$R_0 = \\|r(u^{(0)})\\| = 1.0 \\times 10^{-1}$\n$R_1 = \\|r(u^{(1)})\\| = 1.0 \\times 10^{-2}$\n$R_2 = \\|r(u^{(2)})\\| = 1.0 \\times 10^{-4}$\n$R_3 = \\|r(u^{(3)})\\| = 1.0 \\times 10^{-8}$\n\nWe can compute an estimate for $p$ using the latest three available iterates, which are for $k=1, 2, 3$. This corresponds to setting $k=2$ in our derived formula.\n$$ p \\approx \\frac{\\ln(R_3 / R_2)}{\\ln(R_2 / R_1)} $$\nFirst, we compute the ratios of the norms:\n$$ \\frac{R_3}{R_2} = \\frac{1.0 \\times 10^{-8}}{1.0 \\times 10^{-4}} = 10^{-4} $$\n$$ \\frac{R_2}{R_1} = \\frac{1.0 \\times 10^{-4}}{1.0 \\times 10^{-2}} = 10^{-2} $$\nSubstituting these values into the estimator for $p$:\n$$ p \\approx \\frac{\\ln(10^{-4})}{\\ln(10^{-2})} = \\frac{-4 \\ln(10)}{-2 \\ln(10)} = 2 $$\nThe computed observed order of convergence is exactly $p=2$.\n\nThis result is consistent with the theory of the Newton-Raphson method. An order of convergence $p=2$ signifies quadratic convergence. For a nonlinear system $r(u)=0$, the FNR method with a consistently derived tangent matrix $K_T = \\frac{\\partial r}{\\partial u}$ is expected to exhibit quadratic convergence in the vicinity of a simple root. The provided data for the elastoplastic model, which is a highly nonlinear problem, shows this ideal behavior, indicating that the chosen load increment and starting guess $u^{(0)}$ were appropriate to fall within the basin of attraction and that the consistent tangent was correctly implemented. The convergence behavior is therefore classified as quadratic.\n\nThe final answer must be reported to $4$ significant figures.\n$$ p = 2.000 $$",
            "answer": "$$\n\\boxed{2.000}\n$$"
        },
        {
            "introduction": "This coding exercise demonstrates a critical failure mode of the Newton-Raphson method in the context of strain localization, a common phenomenon in geomechanics. You will implement and compare the full and modified Newton-Raphson schemes, observe their divergence in a softening material, and then successfully restore convergence by implementing a gradient regularization technique. This practice bridges theory and computational reality, highlighting the importance of numerical robustness and advanced modeling techniques for tackling challenging nonlinear problems.",
            "id": "3526589",
            "problem": "Consider a one-dimensional surrogate for strain localization in computational geomechanics consisting of two serial subdomains (treated as axial springs) occupying lengths $L_1$ and $L_2$, with $L_1 + L_2 = L$. Let $u(x)$ denote the axial displacement and $\\varepsilon_i$ the uniform axial strain in subdomain $i \\in \\{1,2\\}$. The subdomains are in series under a prescribed end-to-end displacement $U$, so compatibility requires $L_1 \\varepsilon_1 + L_2 \\varepsilon_2 = U$, and force equilibrium requires the axial stress to be equal in both subdomains. Subdomain $2$ is linear elastic with modulus $E_2$, so $\\sigma_2(\\varepsilon_2) = E_2 \\varepsilon_2$. Subdomain $1$ is a softening material modeling a localization-prone band. Its stress-strain law is given by a piecewise polynomial:\n- For $\\varepsilon_1 \\le \\varepsilon_y$, $\\sigma_1(\\varepsilon_1) = E_1 \\varepsilon_1$.\n- For $\\varepsilon_1 > \\varepsilon_y$, $\\sigma_1(\\varepsilon_1) = \\sigma_y + H_1 (\\varepsilon_1 - \\varepsilon_y) + H_3 (\\varepsilon_1 - \\varepsilon_y)^3$, where $\\sigma_y = E_1 \\varepsilon_y$, and $H_1$ and $H_3$ are softening parameters with $H_1 < 0$ and $H_3 \\ge 0$.\n\nLet the incremental acoustic tensor in one dimension be identified with the scalar algorithmic tangent modulus $A = \\partial \\sigma_1 / \\partial \\varepsilon_1$. In one dimension and in the absence of inertia, loss of ellipticity is indicated by $\\det A = A = 0$ and ill-posed incremental response by $A \\le 0$.\n\nTo regularize the problem, augment the local law in subdomain $1$ by a gradient term that penalizes strain gradients with a material length scale. In the Fourier representation of a localization mode with wavenumber $k$ in subdomain $1$, this is approximated by an additive stabilization in the tangent, $A_{\\text{reg}} = A + \\kappa k^2$, where $\\kappa$ is the gradient coefficient. In the discretized surrogate, this enters the equilibrium residual of subdomain $1$ through an additional term $\\kappa k^2 \\varepsilon_1$.\n\nYou are to formulate and solve the two-equation nonlinear system for the unknowns $\\varepsilon_1$ and $\\varepsilon_2$,\n- Force equilibrium: $\\sigma_1(\\varepsilon_1) - \\sigma_2(\\varepsilon_2) + \\kappa k^2 \\varepsilon_1 = 0$, where the regularization term is absent if $\\kappa = 0$.\n- Compatibility: $L_1 \\varepsilon_1 + L_2 \\varepsilon_2 - U = 0$.\n\nImplement two solvers:\n- Full Newton-Raphson: update the Jacobian at every iteration.\n- Modified Newton-Raphson: freeze the Jacobian at the initial iterate for the entire load step.\n\nIn each iteration, update $\\boldsymbol{\\varepsilon}^{(m+1)} = \\boldsymbol{\\varepsilon}^{(m)} - \\mathbf{J}^{-1} \\mathbf{R}$, where $\\mathbf{R}$ is the residual vector and $\\mathbf{J}$ is the Jacobian matrix. For the regularized case, the Jacobian entry for the subdomain $1$ equilibrium includes the stabilizing $+\\kappa k^2$ in addition to the material tangent $A$.\n\nInitialization must use an elastic guess $\\varepsilon_1^{(0)} = \\varepsilon_2^{(0)} = U/L$, with $L = L_1 + L_2$. Declare convergence when the Euclidean norm of the residual vector is less than a tolerance $\\tau = 10^{-10}$, or declare divergence if the Jacobian is singular at any iteration or the iteration count exceeds a maximum $M = 50$.\n\nTrack and report the acoustic tensor determinant at the final available iterate in subdomain $1$ in both unregularized and regularized forms, namely $\\det A = A$ and $\\det A_{\\text{reg}} = A + \\kappa k^2$. Report these in megapascal (MPa). State whether each solver converged.\n\nUse the following test suite (each line denotes one test case):\n- Case $1$ (no regularization, singular Jacobian on entry into softening): $E_1 = 20000$ MPa, $E_2 = 20000$ MPa, $\\varepsilon_y = 5.0 \\times 10^{-4}$, $H_1 = -\\dfrac{20000 \\times 0.1}{0.9} = -2222.222222222222$ MPa, $H_3 = 0$ MPa, $L_1 = 0.1$ m, $L_2 = 0.9$ m, $U = 0.004$ (dimensionless), $\\kappa = 0$ MPa·m$^2$, $k = \\dfrac{\\pi}{L_1}$ rad/m.\n- Case $2$ (same softening but regularized): identical to Case $1$ except $\\kappa = 0.5$ MPa·m$^2$.\n- Case $3$ (milder softening, no regularization): $E_1 = 20000$ MPa, $E_2 = 20000$ MPa, $\\varepsilon_y = 5.0 \\times 10^{-4}$, $H_1 = -2000$ MPa, $H_3 = 0$ MPa, $L_1 = 0.1$ m, $L_2 = 0.9$ m, $U = 0.004$ (dimensionless), $\\kappa = 0$ MPa·m$^2$, $k = \\dfrac{\\pi}{L_1}$ rad/m.\n\nAngle units must be in radians, strains are dimensionless, lengths are in meters (m), stiffnesses and acoustic tensor determinants are in megapascal (MPa), and $\\kappa$ has units of MPa·m$^2$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list with six entries in the following order:\n- Full Newton-Raphson converged (boolean),\n- Modified Newton-Raphson converged (boolean),\n- $\\det A$ for subdomain $1$ in MPa (float),\n- $\\det A_{\\text{reg}}$ for subdomain $1$ in MPa (float),\n- Number of iterations taken by full Newton-Raphson (integer),\n- Number of iterations taken by modified Newton-Raphson (integer).\n\nFor example, your output must have the form $[[b_1,b_2,a_1,a_2,n_1,n_2],[\\dots],[\\dots]]$ with booleans, floats, and integers replacing the placeholders.",
            "solution": "The problem requires the formulation and implementation of two Newton-Raphson schemes, full (FNR) and modified (MNR), to solve a nonlinear system of equations modeling strain localization in a one-dimensional, two-bar system. The solution must be validated against three distinct test cases, and the results, including convergence status, iteration counts, and final acoustic tensor determinants, must be reported in a specific format.\n\nThe system consists of two subdomains (springs) in series, with lengths $L_1$ and $L_2$, subjected to a total elongation $U$. The state variables are the strains in each subdomain, $\\varepsilon_1$ and $\\varepsilon_2$. The governing equations are derived from force equilibrium and kinematic compatibility.\n\nLet the state vector be $\\boldsymbol{\\varepsilon} = \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\end{pmatrix}$. The problem is to find the root of the residual vector $\\mathbf{R}(\\boldsymbol{\\varepsilon}) = \\mathbf{0}$, where:\n$$ \\mathbf{R}(\\boldsymbol{\\varepsilon}) = \\begin{pmatrix} R_1(\\varepsilon_1, \\varepsilon_2) \\\\ R_2(\\varepsilon_1, \\varepsilon_2) \\end{pmatrix} = \\begin{pmatrix} \\sigma_1(\\varepsilon_1) + \\kappa k^2 \\varepsilon_1 - \\sigma_2(\\varepsilon_2) \\\\ L_1 \\varepsilon_1 + L_2 \\varepsilon_2 - U \\end{pmatrix} $$\n\nThe constitutive laws for the stresses $\\sigma_1$ and $\\sigma_2$ are given as:\n1.  **Subdomain 2 (Linear Elastic):** $\\sigma_2(\\varepsilon_2) = E_2 \\varepsilon_2$.\n2.  **Subdomain 1 (Softening):** A piecewise law dependent on the yield strain $\\varepsilon_y$. With $\\sigma_y = E_1 \\varepsilon_y$:\n$$ \\sigma_1(\\varepsilon_1) = \\begin{cases} E_1 \\varepsilon_1 & \\text{if } \\varepsilon_1 \\le \\varepsilon_y \\\\ \\sigma_y + H_1 (\\varepsilon_1 - \\varepsilon_y) + H_3 (\\varepsilon_1 - \\varepsilon_y)^3 & \\text{if } \\varepsilon_1 > \\varepsilon_y \\end{cases} $$\nThe term $\\kappa k^2 \\varepsilon_1$ in the equilibrium residual $R_1$ represents a gradient-based regularization, active when the coefficient $\\kappa > 0$.\n\nThe Newton-Raphson method finds the solution iteratively. Starting from an initial guess $\\boldsymbol{\\varepsilon}^{(0)}$, subsequent iterates are found by solving the linear system $\\mathbf{J}(\\boldsymbol{\\varepsilon}^{(m)}) \\Delta \\boldsymbol{\\varepsilon}^{(m)} = -\\mathbf{R}(\\boldsymbol{\\varepsilon}^{(m)})$ for the update $\\Delta \\boldsymbol{\\varepsilon}^{(m)} = \\boldsymbol{\\varepsilon}^{(m+1)} - \\boldsymbol{\\varepsilon}^{(m)}$. Here, $\\mathbf{J}$ is the Jacobian matrix of the residual vector.\n\nThe Jacobian matrix $\\mathbf{J} = \\frac{\\partial \\mathbf{R}}{\\partial \\boldsymbol{\\varepsilon}}$ is a $2 \\times 2$ matrix with components:\n*   $J_{11} = \\frac{\\partial R_1}{\\partial \\varepsilon_1} = \\frac{\\partial \\sigma_1}{\\partial \\varepsilon_1} + \\kappa k^2 = A(\\varepsilon_1) + \\kappa k^2$\n*   $J_{12} = \\frac{\\partial R_1}{\\partial \\varepsilon_2} = -\\frac{\\partial \\sigma_2}{\\partial \\varepsilon_2} = -E_2$\n*   $J_{21} = \\frac{\\partial R_2}{\\partial \\varepsilon_1} = L_1$\n*   $J_{22} = \\frac{\\partial R_2}{\\partial \\varepsilon_2} = L_2$\n\nHere, $A(\\varepsilon_1) = \\frac{\\partial \\sigma_1}{\\partial \\varepsilon_1}$ is the algorithmic tangent modulus for subdomain $1$, which acts as the unregularized acoustic tensor determinant in this one-dimensional problem. Its form is derived from the stress-strain law:\n$$ A(\\varepsilon_1) = \\begin{cases} E_1 & \\text{if } \\varepsilon_1 \\le \\varepsilon_y \\\\ H_1 + 3 H_3 (\\varepsilon_1 - \\varepsilon_y)^2 & \\text{if } \\varepsilon_1 > \\varepsilon_y \\end{cases} $$\nThe full Jacobian matrix is:\n$$ \\mathbf{J}(\\varepsilon_1) = \\begin{pmatrix} A(\\varepsilon_1) + \\kappa k^2 & -E_2 \\\\ L_1 & L_2 \\end{pmatrix} $$\n\nThe two required solvers are distinguished by their handling of the Jacobian:\n*   **Full Newton-Raphson (FNR):** The Jacobian $\\mathbf{J}(\\varepsilon_1^{(m)})$ is re-evaluated at each iteration $m$.\n*   **Modified Newton-Raphson (MNR):** The Jacobian is evaluated only once using the initial guess, $\\mathbf{J}(\\varepsilon_1^{(0)})$, and then \"frozen\" for all subsequent iterations.\n\nThe algorithm for each test case and solver is as follows:\n1.  Initialize parameters and calculate derived constants ($L=L_1+L_2$, $\\sigma_y=E_1\\varepsilon_y$, $k=\\pi/L_1$).\n2.  Set the initial guess for the strain vector: $\\boldsymbol{\\varepsilon}^{(0)} = [U/L, U/L]^T$.\n3.  Initialize iteration counter $m=0$.\n4.  For MNR, compute and store the initial Jacobian $\\mathbf{J}^{(0)} = \\mathbf{J}(\\varepsilon_1^{(0)})$. If $\\det(\\mathbf{J}^{(0)})$ is near zero, declare divergence.\n5.  Begin the iteration loop, which terminates if the iteration count $m$ exceeds the maximum $M=50$.\n6.  Inside the loop, compute the residual vector $\\mathbf{R}^{(m)} = \\mathbf{R}(\\boldsymbol{\\varepsilon}^{(m)})$.\n7.  Check for convergence by testing if the Euclidean norm $||\\mathbf{R}^{(m)}||_2$ is below the tolerance $\\tau=10^{-10}$. If so, the solution is found.\n8.  For FNR, compute the current Jacobian $\\mathbf{J}^{(m)} = \\mathbf{J}(\\varepsilon_1^{(m)})$. If $\\det(\\mathbf{J}^{(m)})$ is near zero, declare divergence.\n9.  Solve the linear system $\\mathbf{J} \\Delta\\boldsymbol{\\varepsilon}^{(m)} = -\\mathbf{R}^{(m)}$ for the strain increment $\\Delta\\boldsymbol{\\varepsilon}^{(m)}$.\n10. Update the strain vector: $\\boldsymbol{\\varepsilon}^{(m+1)} = \\boldsymbol{\\varepsilon}^{(m)} + \\Delta\\boldsymbol{\\varepsilon}^{(m)}$.\n11. Increment the iteration counter $m$.\n12. After the loop terminates (due to convergence or divergence), report the convergence status, the final iteration count, and the values of the unregularized acoustic tensor $A(\\varepsilon_1)$ and the regularized acoustic tensor $A_{\\text{reg}} = A(\\varepsilon_1) + \\kappa k^2$ at the final available strain iterate $\\varepsilon_1$. Since the output format requires a single value for $A$ and $A_{\\text{reg}}$ per test case, the result from the FNR solver is prioritized. If FNR fails to converge, the result from the MNR solver is used. If both fail, FNR's final state is used for reporting.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a 1D strain localization problem using Full and Modified Newton-Raphson methods.\n    \"\"\"\n    \n    # Define solver parameters\n    TOL = 1e-10\n    MAX_ITER = 50\n    JAC_TOL = 1e-12\n\n    # Define test cases as a list of dictionaries for clarity\n    test_cases = [\n        {\n            # Case 1: No regularization, singular Jacobian at softening onset\n            \"E1\": 20000.0, \"E2\": 20000.0, \"eps_y\": 5.0e-4,\n            \"H1\": -20000.0 * 0.1 / 0.9, \"H3\": 0.0,\n            \"L1\": 0.1, \"L2\": 0.9, \"U\": 0.004,\n            \"kappa\": 0.0\n        },\n        {\n            # Case 2: Same as Case 1 but regularized, preventing singularity\n            \"E1\": 20000.0, \"E2\": 20000.0, \"eps_y\": 5.0e-4,\n            \"H1\": -20000.0 * 0.1 / 0.9, \"H3\": 0.0,\n            \"L1\": 0.1, \"L2\": 0.9, \"U\": 0.004,\n            \"kappa\": 0.5\n        },\n        {\n            # Case 3: Milder softening, non-singular Jacobian without regularization\n            \"E1\": 20000.0, \"E2\": 20000.0, \"eps_y\": 5.0e-4,\n            \"H1\": -2000.0, \"H3\": 0.0,\n            \"L1\": 0.1, \"L2\": 0.9, \"U\": 0.004,\n            \"kappa\": 0.0\n        }\n    ]\n\n    def run_newton_solver(params, solver_type):\n        \"\"\"\n        A unified Newton-Raphson solver for the 2x2 system.\n\n        Args:\n            params (dict): Dictionary of material and geometric parameters.\n            solver_type (str): 'full' for Full Newton-Raphson, 'modified' for Modified NR.\n\n        Returns:\n            tuple: (converged, num_iter, A_final, A_reg_final)\n        \"\"\"\n        # Unpack parameters\n        E1, E2 = params[\"E1\"], params[\"E2\"]\n        eps_y, H1, H3 = params[\"eps_y\"], params[\"H1\"], params[\"H3\"]\n        L1, L2, U, kappa = params[\"L1\"], params[\"L2\"], params[\"U\"], params[\"kappa\"]\n        \n        # Derived constants\n        L = L1 + L2\n        sigma_y = E1 * eps_y\n        k = np.pi / L1\n\n        def get_sigma1_and_A(eps1):\n            if eps1 <= eps_y:\n                sigma1 = E1 * eps1\n                A = E1\n            else:\n                delta_eps = eps1 - eps_y\n                sigma1 = sigma_y + H1 * delta_eps + H3 * delta_eps**3\n                A = H1 + 3 * H3 * delta_eps**2\n            return sigma1, A\n\n        def get_residual(eps_vec):\n            eps1, eps2 = eps_vec\n            sigma1, _ = get_sigma1_and_A(eps1)\n            sigma2 = E2 * eps2\n            R1 = sigma1 - sigma2 + kappa * k**2 * eps1\n            R2 = L1 * eps1 + L2 * eps2 - U\n            return np.array([R1, R2])\n\n        def get_jacobian(eps1):\n            _, A = get_sigma1_and_A(eps1)\n            J00 = A + kappa * k**2\n            J01 = -E2\n            J10 = L1\n            J11 = L2\n            return np.array([[J00, J01], [J10, J11]])\n\n        # Initialization\n        eps = np.array([U / L, U / L])\n        num_iter = 0\n        converged = False\n        \n        # For Modified NR, compute Jacobian once\n        if solver_type == 'modified':\n            J = get_jacobian(eps[0])\n            if abs(np.linalg.det(J)) < JAC_TOL:\n                # Divergence due to initial singular Jacobian\n                _, A_final = get_sigma1_and_A(eps[0])\n                A_reg_final = A_final + kappa * k**2\n                return False, 0, A_final, A_reg_final\n\n        # Iteration loop\n        while num_iter < MAX_ITER:\n            R = get_residual(eps)\n            if np.linalg.norm(R) < TOL:\n                converged = True\n                break\n            \n            # For Full NR, recompute Jacobian\n            if solver_type == 'full':\n                J = get_jacobian(eps[0])\n                if abs(np.linalg.det(J)) < JAC_TOL:\n                    break  # Divergence due to singular Jacobian\n\n            # Solve for increment and update\n            try:\n                delta_eps = np.linalg.solve(J, -R)\n            except np.linalg.LinAlgError:\n                break # Divergence due to singular matrix from np.linalg.solve\n                \n            eps += delta_eps\n            num_iter += 1\n\n        # Report values at final available iterate\n        _, A_final = get_sigma1_and_A(eps[0])\n        A_reg_final = A_final + kappa * k**2\n\n        return converged, num_iter, A_final, A_reg_final\n\n    all_results = []\n    for case_params in test_cases:\n        fnr_conv, fnr_iter, fnr_A, fnr_A_reg = run_newton_solver(case_params, 'full')\n        mnr_conv, mnr_iter, mnr_A, mnr_A_reg = run_newton_solver(case_params, 'modified')\n\n        # Select the acoustic tensor values based on a convergence priority rule\n        if fnr_conv:\n            final_A = fnr_A\n            final_A_reg = fnr_A_reg\n        elif mnr_conv:\n            final_A = mnr_A\n            final_A_reg = mnr_A_reg\n        else: # Both diverged, use FNR's final state for consistency\n            final_A = fnr_A\n            final_A_reg = fnr_A_reg\n            \n        case_result = [\n            fnr_conv, mnr_conv, \n            float(final_A), float(final_A_reg),\n            int(fnr_iter), int(mnr_iter)\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string exactly as required\n    result_str_list = []\n    for res in all_results:\n        # Manually construct string to control boolean format and spacing\n        s = f\"[{str(res[0]).lower()},{str(res[1]).lower()},{res[2]},{res[3]},{res[4]},{res[5]}]\"\n        result_str_list.append(s)\n    \n    final_output = f\"[{','.join(result_str_list)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}