{
    "hands_on_practices": [
        {
            "introduction": "Finite element discretizations in geomechanics yield large, sparse linear systems. While direct solvers like Cholesky factorization are robust, their efficiency depends critically on limiting \"fill-in\"—the creation of new non-zero entries during elimination. This hands-on exercise challenges you to manually trace the factorization process on a small mesh to visualize how fill-in occurs and, more importantly, how a clever reordering of equations can dramatically reduce it . Mastering this concept provides fundamental insight into the performance of sparse direct solvers.",
            "id": "3538782",
            "problem": "Consider a small unstructured two-dimensional triangular mesh arising in computational geomechanics from a scalar diffusion problem (e.g., steady seepage), discretized with linear finite elements on interior nodes after applying Dirichlet boundary conditions so that the assembled global stiffness matrix $K$ is real, symmetric, and positive definite. Let there be $7$ interior nodes labeled $1$ through $7$, with element connectivity given by the following triangles:\n- $(1,2,3)$, $(1,3,4)$, $(1,4,5)$, $(1,5,6)$, $(1,6,7)$, $(1,7,2)$.\nAssume a single degree of freedom per node. Under the standard finite element assembly, the symmetric sparsity pattern of $K$ is determined by nodal co-membership in an element: if two nodes appear together in at least one triangle, the corresponding off-diagonal entry in $K$ is structurally nonzero.\n\nUsing only fundamental definitions (graph of $K$, symmetric elimination graph model for Cholesky fill, and the idea that elimination of a node introduces edges to make its current neighbors a clique), perform the following:\n\n1) Derive the symmetric sparsity (adjacency) graph of $K$ implied by the above connectivity. Then, under the natural ordering $1,2,\\dots,7$, carry out a symbolic Cholesky elimination (graph fill-in) by successively eliminating nodes $1$ through $7$. Identify all fill edges that appear and, for each elimination step $j$, determine the number $l_j$ of neighbors of node $j$ among the yet-uneliminated nodes at the moment of its elimination.\n\n2) Construct an Approximate Minimum Degree (AMD) ordering by applying the minimum-degree heuristic on the evolving elimination graph, breaking ties by choosing the smallest-index node. Repeat the symbolic elimination under this AMD ordering, list the fill edges introduced, and determine the sequence of $l_j$ values in that permuted order.\n\n3) Using a first-principles operation-count estimate for sparse Cholesky factorization based on the outer-product view of elimination, justify that the dominant multiply-add count is proportional to $\\sum_{j=1}^{7} l_j^2$. Let $S_{\\mathrm{nat}}$ denote this sum under the natural ordering and $S_{\\mathrm{amd}}$ under AMD. Compute the exact ratio $S_{\\mathrm{amd}}/S_{\\mathrm{nat}}$ and express it as a reduced rational number. No rounding is required and no units are to be reported. Your final answer must be a single number.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. We begin by formalizing the problem setup.\n\nThe set of interior nodes is $V = \\{1, 2, 3, 4, 5, 6, 7\\}$. The symmetric sparsity pattern of the stiffness matrix $K$ is represented by an undirected graph $G = (V, E)$, where an edge $(i, j) \\in E$ exists if nodes $i$ and $j$ appear in the same triangular element. The given element connectivity is: $(1,2,3)$, $(1,3,4)$, $(1,4,5)$, $(1,5,6)$, $(1,6,7)$, and $(1,7,2)$.\n\nFirst, we construct the initial adjacency graph $G_0 = (V, E_0)$. The edges are formed by taking all pairs of nodes within each triangle:\n- From $(1,2,3)$: edges $(1,2), (1,3), (2,3)$.\n- From $(1,3,4)$: edges $(1,3), (1,4), (3,4)$.\n- From $(1,4,5)$: edges $(1,4), (1,5), (4,5)$.\n- From $(1,5,6)$: edges $(1,5), (1,6), (5,6)$.\n- From $(1,6,7)$: edges $(1,6), (1,7), (6,7)$.\n- From $(1,7,2)$: edges $(1,7), (1,2), (7,2)$.\n\nThe set of unique edges $E_0$ is $\\{(1,2), (1,3), (1,4), (1,5), (1,6), (1,7), (2,3), (3,4), (4,5), (5,6), (6,7), (7,2)\\}$. This graph is a wheel graph $W_7$ with node $1$ as the central hub and nodes $2$ through $7$ forming the outer rim in a cycle. The initial degrees of the nodes are:\n- $d(1) = 6$\n- $d(2) = d(3) = d(4) = d(5) = d(6) = d(7) = 3$\n\nThe quantity $l_j$ for an eliminated node $j$ is the number of its neighbors in the current elimination graph, considering only the set of yet-uneliminated nodes. The symbolic elimination of node $j$ adds fill edges to the graph such that the neighbors of $j$ form a clique.\n\n**1) Natural Ordering Elimination**\n\nThe natural ordering is $(1, 2, 3, 4, 5, 6, 7)$. We perform symbolic elimination on $G_0$.\n\n- **Eliminate node $j=1$**: The uneliminated nodes are $\\{1, 2, 3, 4, 5, 6, 7\\}$. The neighbors of node $1$ are $\\{2, 3, 4, 5, 6, 7\\}$. Thus, $l_1 = 6$. The elimination of node $1$ makes its neighbors a clique. The neighbors form a cycle $2-3-4-5-6-7-2$. To make this a clique ($K_6$), we must add all chords. The fill edges introduced are: $(2,4)$, $(2,5)$, $(2,6)$, $(3,5)$, $(3,6)$, $(3,7)$, $(4,6)$, $(4,7)$, and $(5,7)$. There are $9$ fill edges. The resulting elimination graph on the remaining nodes $\\{2, ..., 7\\}$ is a complete graph $K_6$.\n\n- **Eliminate node $j=2$**: The uneliminated nodes are $\\{2, 3, 4, 5, 6, 7\\}$, which form a $K_6$. The neighbors of node $2$ are $\\{3, 4, 5, 6, 7\\}$. Thus, $l_2 = 5$. Since these nodes are part of a $K_6$, they already form a clique ($K_5$). No new fill edges are introduced.\n\n- **Eliminate node $j=3$**: The uneliminated nodes are $\\{3, 4, 5, 6, 7\\}$, forming a $K_5$. The neighbors of node $3$ are $\\{4, 5, 6, 7\\}$. Thus, $l_3 = 4$. No fill is introduced.\n\n- **Eliminate node $j=4$**: The uneliminated nodes form a $K_4$. The neighbors of node $4$ are $\\{5, 6, 7\\}$. Thus, $l_4 = 3$. No fill.\n\n- **Eliminate node $j=5$**: The uneliminated nodes form a $K_3$. The neighbors of node $5$ are $\\{6, 7\\}$. Thus, $l_5 = 2$. No fill.\n\n- **Eliminate node $j=6$**: The uneliminated nodes form a $K_2$. The only neighbor of node $6$ is $\\{7\\}$. Thus, $l_6 = 1$. No fill.\n\n- **Eliminate node $j=7$**: Node $7$ is the last node. It has no neighbors among uneliminated nodes. Thus, $l_7 = 0$.\n\nFor the natural ordering, the sequence of neighbor counts is $(l_1, l_2, l_3, l_4, l_5, l_6, l_7) = (6, 5, 4, 3, 2, 1, 0)$.\n\n**2) Approximate Minimum Degree (AMD) Ordering Elimination**\n\nWe apply the minimum-degree heuristic to the evolving elimination graph, choosing the node with the minimum degree among the currently uneliminated nodes. Ties are broken by selecting the node with the smallest index.\n\n- **Step 1**: In $G_0$, the minimum degree is $3$, shared by nodes $\\{2, 3, 4, 5, 6, 7\\}$. We choose node $2$. The first node in the ordering is $p_1 = 2$. $l_2 = 3$. Its neighbors are $\\{1, 3, 7\\}$. A fill edge $(3,7)$ is added to the graph.\n\n- **Step 2**: The uneliminated nodes are $\\{1, 3, 4, 5, 6, 7\\}$. In the updated graph, the degrees are $d(1)=5$, and $d(3)=d(4)=d(5)=d(6)=d(7)=3$. The minimum degree is $3$, shared by $\\{3, 4, 5, 6, 7\\}$. We choose node $3$. The second node is $p_2 = 3$. $l_3 = 3$. Its neighbors are $\\{1, 4, 7\\}$. A fill edge $(4,7)$ is added.\n\n- **Step 3**: The uneliminated nodes are $\\{1, 4, 5, 6, 7\\}$. Degrees are $d(1)=4$, and $d(4)=d(5)=d(6)=d(7)=3$. The minimum degree is $3$, shared by $\\{4, 5, 6, 7\\}$. We choose node $4$. The third node is $p_3 = 4$. $l_4 = 3$. Its neighbors are $\\{1, 5, 7\\}$. A fill edge $(5,7)$ is added.\n\n- **Step 4**: The uneliminated nodes are $\\{1, 5, 6, 7\\}$. In the updated graph, due to previous fill-in, all these nodes $\\{1, 5, 6, 7\\}$ now have a degree of $3$. We choose node $1$ by the tie-breaking rule. The fourth node is $p_4 = 1$. $l_1 = 3$. Its neighbors $\\{5, 6, 7\\}$ are already a clique (edges $(5,6)$ and $(6,7)$ are original, and $(5,7)$ was fill from the previous step). No new fill is introduced.\n\n- **Step 5**: The uneliminated nodes are $\\{5, 6, 7\\}$, which form a clique ($K_3$). All have degree $2$. We choose node $5$. The fifth node is $p_5 = 5$. $l_5 = 2$. No new fill.\n\n- **Step 6**: The uneliminated nodes are $\\{6, 7\\}$, forming a $K_2$. Both have degree $1$. We choose node $6$. The sixth node is $p_6 = 6$. $l_6 = 1$. No new fill.\n\n- **Step 7**: The last node is $p_7 = 7$. $l_7 = 0$.\n\nThe AMD ordering is $(2, 3, 4, 1, 5, 6, 7)$. The fill edges introduced are $(3,7)$, $(4,7)$, and $(5,7)$, totaling $3$ edges. The neighbor counts $l_j$ for each node $j$ at the moment of its elimination under this ordering are:\n$l_1=3$, $l_2=3$, $l_3=3$, $l_4=3$, $l_5=2$, $l_6=1$, $l_7=0$.\n\n**3) Operation Count and Ratio**\n\nThe justification for the operation count being proportional to $\\sum_{j=1}^{n} l_j^2$ comes from the outer-product formulation of Cholesky factorization. At step $j$, the algorithm eliminates node $j$. This corresponds to a rank-$1$ update of the submatrix associated with the neighbors of node $j$. In the filled graph, these $l_j$ neighbors form a clique, corresponding to a dense submatrix of size $l_j \\times l_j$. The number of floating-point operations (multiply-adds) required to perform this update is proportional to the number of entries in this submatrix, which is of order $l_j^2$. Summing over all elimination steps gives a total operation count proportional to $\\sum_{j=1}^{n} l_j^2$.\n\nNow, we compute the sums for both orderings.\n\nFor the natural ordering:\n$$S_{\\mathrm{nat}} = \\sum_{j=1}^{7} l_j^2 = 6^2 + 5^2 + 4^2 + 3^2 + 2^2 + 1^2 + 0^2 = 36 + 25 + 16 + 9 + 4 + 1 + 0 = 91$$\n\nFor the AMD ordering, using the values of $l_j$ determined in part 2:\n$$S_{\\mathrm{amd}} = \\sum_{j=1}^{7} l_j^2 = l_1^2 + l_2^2 + l_3^2 + l_4^2 + l_5^2 + l_6^2 + l_7^2$$\n$$S_{\\mathrm{amd}} = 3^2 + 3^2 + 3^2 + 3^2 + 2^2 + 1^2 + 0^2 = 9 + 9 + 9 + 9 + 4 + 1 + 0 = 41$$\n\nThe ratio is:\n$$\\frac{S_{\\mathrm{amd}}}{S_{\\mathrm{nat}}} = \\frac{41}{91}$$\nThe number $41$ is prime. The prime factorization of $91$ is $7 \\times 13$. Since $41$ is not divisible by $7$ or $13$, the fraction is irreducible.",
            "answer": "$$\\boxed{\\frac{41}{91}}$$"
        },
        {
            "introduction": "For the largest-scale problems, iterative methods like the Conjugate Gradient (CG) algorithm become indispensable. Unlike direct solvers, they refine an approximate solution until a convergence criterion is met, typically based on the norm of the residual vector. A critical question for any engineer is how this mathematical criterion relates to the actual error in the physical quantities of interest, such as displacement. This practice guides you through a first-principles derivation to establish a rigorous bound between the easily-computable preconditioned residual norm and the physically-meaningful energy norm of the error, a cornerstone for building trust in iterative solutions .",
            "id": "3538755",
            "problem": "Consider small-strain linear elasticity on a bounded Lipschitz domain, discretized by the Finite Element Method (FEM). The discrete equilibrium under homogeneous Dirichlet boundary conditions can be written as the linear system $A u = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is the symmetric positive definite (SPD) stiffness matrix assembled from the standard bilinear form of linear elasticity by the principle of virtual work, $u \\in \\mathbb{R}^{n}$ is the nodal displacement vector, and $b \\in \\mathbb{R}^{n}$ is the consistent load vector. Let $u^{\\star}$ denote the exact discrete solution of the system and $u_{k}$ be an iterate produced by a Krylov solver such as the Conjugate Gradient (CG) method. Define the displacement error $e_{k} = u^{\\star} - u_{k}$ and the algebraic residual $r_{k} = b - A u_{k}$. Consider the energy norm induced by the stiffness matrix, $\\|v\\|_{A} = \\sqrt{v^{\\top} A v}$, and the preconditioned residual norm defined by an SPD preconditioner $M \\in \\mathbb{R}^{n \\times n}$, $\\|w\\|_{M^{-1}} = \\sqrt{w^{\\top} M^{-1} w}$. Assume the preconditioner is spectrally equivalent to the stiffness matrix in the sense that there exist positive constants $\\alpha$ and $\\beta$ such that for all $v \\in \\mathbb{R}^{n}$,\n$$\n\\alpha \\, v^{\\top} A v \\leq v^{\\top} M v \\leq \\beta \\, v^{\\top} A v.\n$$\nA residual norm-based stopping criterion $\\|r_{k}\\|_{M^{-1}} \\leq \\varepsilon$ is often used in practice, whereas application-driven verification prefers to control the energy norm of the displacement error $\\|e_{k}\\|_{A}$. Starting only from the above fundamental definitions and facts, derive the best possible constant $C$ (in terms of $\\alpha$ and $\\beta$ only) such that the inequality\n$$\n\\|e_{k}\\|_{A} \\leq C \\, \\|r_{k}\\|_{M^{-1}}\n$$\nholds for all iterates $u_{k}$ and all right-hand sides $b$. Your final answer must be a single closed-form analytic expression for $C$ in terms of $\\alpha$ and $\\beta$. No numerical evaluation is required and no units are involved.",
            "solution": "The problem requires the derivation of the best possible constant $C$ such that $\\|e_{k}\\|_{A} \\leq C \\, \\|r_{k}\\|_{M^{-1}}$. We begin by validating the problem statement.\n\n### Step 1: Extract Givens\n- **Linear System**: $A u = b$, where $A$ is a symmetric positive definite (SPD) matrix in $\\mathbb{R}^{n \\times n}$.\n- **Exact Solution**: $u^{\\star}$ such that $A u^{\\star} = b$.\n- **Iterate**: $u_{k}$.\n- **Displacement Error**: $e_{k} = u^{\\star} - u_{k}$.\n- **Algebraic Residual**: $r_{k} = b - A u_{k}$.\n- **Energy Norm**: $\\|v\\|_{A} = \\sqrt{v^{\\top} A v}$.\n- **Preconditioner**: $M \\in \\mathbb{R}^{n \\times n}$ is an SPD matrix.\n- **Preconditioned Residual Norm**: $\\|w\\|_{M^{-1}} = \\sqrt{w^{\\top} M^{-1} w}$. Note that since $M$ is SPD, $M^{-1}$ is also SPD, so this norm is well-defined.\n- **Spectral Equivalence**: There exist positive constants $\\alpha$ and $\\beta$ such that for all $v \\in \\mathbb{R}^{n}$, the following inequality holds:\n$$\n\\alpha \\, v^{\\top} A v \\leq v^{\\top} M v \\leq \\beta \\, v^{\\top} A v.\n$$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is a standard problem in the analysis of iterative solvers for finite element methods. The given information is self-contained and mathematically consistent. The definitions of norms, matrices, and spectral equivalence are standard in numerical linear algebra and computational mechanics. No flaws are identified.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed to the solution.\n\nWe seek the smallest constant $C$ satisfying the inequality $\\|e_{k}\\|_{A} \\leq C \\, \\|r_{k}\\|_{M^{-1}}$ for all possible iterates $u_k$ and right-hand sides $b$. It is more convenient to work with the squares of the norms. The inequality is equivalent to finding the smallest $C^2$ such that $\\|e_{k}\\|_{A}^2 \\leq C^2 \\, \\|r_{k}\\|_{M^{-1}}^2$. This constant $C^2$ can be expressed as the supremum of the ratio of the squared norms over all possible non-zero errors and residuals:\n$$\nC^2 = \\sup_{u_k \\neq u^{\\star}} \\frac{\\|e_{k}\\|_{A}^2}{\\|r_{k}\\|_{M^{-1}}^2}\n$$\nFirst, we establish a relationship between the error $e_k$ and the residual $r_k$. By their definitions:\n$$\nr_{k} = b - A u_{k}\n$$\nSince $b = A u^{\\star}$, we can substitute this into the expression for the residual:\n$$\nr_{k} = A u^{\\star} - A u_{k} = A (u^{\\star} - u_{k}) = A e_{k}\n$$\nThis fundamental relationship, $r_k = A e_k$, connects the residual to the error. Since the stiffness matrix $A$ is invertible (as it is SPD), we can also write $e_k = A^{-1} r_k$.\n\nNow, let us express the squared energy norm of the error, $\\|e_{k}\\|_{A}^2$, in terms of the residual $r_k$:\n$$\n\\|e_{k}\\|_{A}^2 = e_{k}^{\\top} A e_{k} = (A^{-1} r_{k})^{\\top} A (A^{-1} r_{k}) = r_{k}^{\\top} (A^{-1})^{\\top} A A^{-1} r_{k}\n$$\nSince $A$ is symmetric, its inverse $A^{-1}$ is also symmetric, so $(A^{-1})^{\\top} = A^{-1}$. The expression simplifies to:\n$$\n\\|e_{k}\\|_{A}^2 = r_{k}^{\\top} A^{-1} A A^{-1} r_{k} = r_{k}^{\\top} A^{-1} r_{k}\n$$\nThe squared preconditioned residual norm is given by its definition:\n$$\n\\|r_{k}\\|_{M^{-1}}^2 = r_{k}^{\\top} M^{-1} r_{k}\n$$\nSubstituting these expressions into the formula for $C^2$:\n$$\nC^2 = \\sup_{r_{k} \\neq 0} \\frac{r_{k}^{\\top} A^{-1} r_{k}}{r_{k}^{\\top} M^{-1} r_{k}}\n$$\nNote that if $u_k \\neq u^{\\star}$, then $e_k \\neq 0$, and since $A$ is invertible, $r_k = A e_k \\neq 0$. Thus, the supremum is taken over all non-zero residual vectors $r_k \\in \\mathbb{R}^n$.\n\nThe expression for $C^2$ is a generalized Rayleigh quotient for the matrices $A^{-1}$ and $M^{-1}$. The supremum of this quotient is the largest eigenvalue $\\lambda_{\\max}$ of the generalized eigenvalue problem:\n$$\nA^{-1} w = \\lambda M^{-1} w\n$$\nSince $M^{-1}$ is SPD and hence invertible, we can pre-multiply by $M$:\n$$\nM A^{-1} w = \\lambda w\n$$\nThis shows that $C^2$ is the largest eigenvalue of the matrix $M A^{-1}$. We must now relate the eigenvalues of $M A^{-1}$ to the given spectral equivalence constants $\\alpha$ and $\\beta$.\n\nThe spectral equivalence condition $\\alpha \\, v^{\\top} A v \\leq v^{\\top} M v \\leq \\beta \\, v^{\\top} A v$ is equivalent to stating that the eigenvalues of the generalized eigenvalue problem $M v = \\gamma A v$ lie in the interval $[\\alpha, \\beta]$. Let's demonstrate this connection. The eigenvalues $\\gamma$ of the pair $(M, A)$ are the same as the eigenvalues of the matrix $A^{-1/2} M A^{-1/2}$, since $Mv = \\gamma Av$ is equivalent to $(A^{-1/2}MA^{-1/2})(A^{1/2}v) = \\gamma(A^{1/2}v)$. The given inequality, after setting $v = A^{-1/2}x$, becomes $\\alpha x^\\top x \\leq x^\\top (A^{-1/2} M A^{-1/2}) x \\leq \\beta x^\\top x$, which by the Rayleigh quotient principle for the symmetric matrix $A^{-1/2} M A^{-1/2}$ confirms that its eigenvalues are in $[\\alpha, \\beta]$.\n\nNow, consider the eigenvalue problem for our matrix of interest, $M A^{-1} w = \\lambda w$. Let us define a new vector $v = A^{-1} w$. Since $A$ is invertible, any vector $v$ can be represented this way. Substituting $w = A v$ into the eigenvalue equation gives:\n$$\nM A^{-1} (A v) = \\lambda (A v)\n$$\n$$\nM v = \\lambda A v\n$$\nThis is precisely the generalized eigenvalue problem for the pair $(M, A)$. This demonstrates that the eigenvalues $\\lambda$ of the matrix $M A^{-1}$ are identical to the generalized eigenvalues $\\gamma$ of the pair $(M, A)$.\n\nTherefore, the eigenvalues of $M A^{-1}$ are contained in the interval $[\\alpha, \\beta]$. The largest eigenvalue of $M A^{-1}$ is thus bounded above by $\\beta$:\n$$\n\\lambda_{\\max}(M A^{-1}) \\leq \\beta\n$$\nThis implies that $C^2 = \\sup_{r_{k} \\neq 0} \\frac{r_{k}^{\\top} A^{-1} r_{k}}{r_{k}^{\\top} M^{-1} r_{k}} = \\lambda_{\\max}(M A^{-1}) \\leq \\beta$.\n\nTo be the \"best possible\" constant, the supremum must be attainable. The definition of spectral equivalence implies that for any given $\\alpha, \\beta  0$, one can construct matrices $A$ and $M$ such that the spectrum of the associated generalized eigenvalue problem $Mv = \\lambda Av$ exactly spans the interval $[\\alpha, \\beta]$. In such a case, the largest eigenvalue would be precisely $\\beta$. Therefore, the tightest possible upper bound for $\\lambda_{\\max}(M A^{-1})$ is $\\beta$.\n\nWe conclude that the best possible constant $C^2$ is $\\beta$.\n$$\nC^2 = \\beta\n$$\nTaking the square root gives the constant $C$:\n$$\nC = \\sqrt{\\beta}\n$$\nThis constant guarantees that $\\|e_{k}\\|_{A} \\leq \\sqrt{\\beta} \\, \\|r_{k}\\|_{M^{-1}}$ for any problem satisfying the given conditions, and no smaller constant provides the same universal guarantee.",
            "answer": "$$\n\\boxed{\\sqrt{\\beta}}\n$$"
        },
        {
            "introduction": "Many critical geomechanics problems, such as consolidation or creep, are transient and require solving a sequence of similar linear systems over time. Solving each system from scratch is computationally wasteful, as it ignores the strong temporal coherence between consecutive steps. This advanced practice introduces the concept of recycling Krylov methods, which reuse information from previous solves to accelerate the current one . By formulating a quantitative model based on convergence theory and subspace perturbation analysis, you will learn how to estimate the benefits of this technique and make informed decisions about designing efficient solvers for time-dependent simulations.",
            "id": "3538766",
            "problem": "Consider a quasi-static one-dimensional consolidation process in linear poroelasticity, discretized by the Finite Element Method (FEM). The semi-discrete governing equation is $M \\frac{d p}{dt} + K p = f(t)$, where $M$ is the symmetric positive definite (SPD) mass matrix, $K$ is the SPD stiffness matrix, $p$ is the nodal pore pressure vector, and $f(t)$ is the time-dependent load vector. Using backward Euler with time step $\\Delta t$, the implicit update at time $t_n$ reads $\\left(\\frac{M}{\\Delta t} + K\\right) p^n = \\frac{M}{\\Delta t} p^{n-1} + f^n$. In computational geomechanics, slowly varying loading and permeability often induce temporal coherence, meaning that the linear systems at successive timesteps are similar. Let the symmetric positive definite operator at time $t_n$ be $A_n \\in \\mathbb{R}^{N \\times N}$ and the right-hand side be $b_n \\in \\mathbb{R}^N$, so the linear system is $A_n x_n = b_n$. Assume $A_n$ changes slowly as $A_n = A_{n-1} + E_n$, with $\\|E_n\\|_2$ small.\n\nWe focus on solving $A_n x_n = b_n$ using the Conjugate Gradient (CG) method, or equivalently the Preconditioned Conjugate Gradient (PCG) method with identity preconditioner for analysis. Recycling Krylov subspaces between timesteps is achieved by building an augmentation (deflation) subspace from $k$ Ritz vectors $\\{u_1,\\dots,u_k\\}$ computed at time $t_{n-1}$, which approximate the eigenvectors associated with the $k$ smallest eigenvalues of $A_{n-1}$. The augmented CG constructs a projected operator that effectively deflates these components, improving convergence at time $t_n$. Denote by $\\lambda_1 \\le \\lambda_2 \\le \\cdots \\le \\lambda_N$ the eigenvalues of $A_n$. If exact deflation of the first $k$ eigenmodes is realized, the effective condition number becomes $\\kappa_{\\mathrm{eff}}(k) = \\frac{\\lambda_N}{\\lambda_{k+1}}$, whereas the baseline condition number is $\\kappa_0 = \\frac{\\lambda_N}{\\lambda_1}$.\n\nFor the Conjugate Gradient (CG) method, a classical bound on the energy-norm error after $m$ iterations is\n$$\n\\frac{\\|e_m\\|_{A}}{\\|e_0\\|_{A}} \\le 2 \\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^m,\n$$\nwhere $\\kappa$ is the spectral condition number of the (preconditioned) operator. For a target tolerance $\\varepsilon$ in the energy norm, the iteration count $m(\\kappa,\\varepsilon)$ is defined as the smallest integer $m$ such that $2 \\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^m \\le \\varepsilon$.\n\nTemporal coherence ensures that recycling Ritz vectors from time $t_{n-1}$ remains effective at time $t_n$ provided the principal angles between the invariant subspaces are small. For a cluster $\\mathcal{U}$ of the first $k$ eigenvectors and its complement $\\mathcal{U}^{\\perp}$, the Davis–Kahan sine-theta theorem bounds the largest principal angle $\\theta$ by\n$$\n\\sin(\\theta) \\le \\frac{\\|E_n\\|_2}{\\mathrm{gap}_k},\n$$\nwhere $\\mathrm{gap}_k = \\lambda_{k+1}-\\lambda_k$ is the spectral gap at the boundary between the deflated cluster and the remaining spectrum. A practical coherence criterion is $\\sin(\\theta) \\le s_{\\max}$ for some prescribed $s_{\\max} \\in (0,1)$, which requires $\\mathrm{gap}_k \\ge \\frac{\\|E_n\\|_2}{s_{\\max}}$.\n\nIn this problem, suppose the eigenvalues of $A_n$ are approximated by a linear spacing between $\\lambda_1$ and $\\lambda_N$, i.e.,\n$$\n\\lambda_i = \\lambda_1 + (i-1)\\frac{\\lambda_N - \\lambda_1}{N-1}, \\quad i=1,\\dots,N,\n$$\nso that the spectral gap $\\mathrm{gap}_k = \\lambda_{k+1} - \\lambda_k = \\frac{\\lambda_N - \\lambda_1}{N-1}$ is constant with respect to $k$. For a given tolerance $\\varepsilon$, dimension $N$, eigenvalue bounds $\\lambda_1$ and $\\lambda_N$, coherence parameters $\\|E_n\\|_2$ and $s_{\\max}$, define the baseline iteration count $m_0 = m(\\kappa_0,\\varepsilon)$ and the target halved iteration count $m_{\\mathrm{half}} = \\lceil m_0/2 \\rceil$. Your task is to estimate the minimal number of Ritz vectors $k \\in \\{0,1,\\dots,N-2\\}$ to recycle such that:\n- the coherence constraint holds, i.e., $\\mathrm{gap}_k \\ge \\frac{\\|E_n\\|_2}{s_{\\max}}$,\n- the augmented CG bound achieves halved iteration count, i.e., $m(\\kappa_{\\mathrm{eff}}(k),\\varepsilon) \\le m_{\\mathrm{half}}$.\n\nIf no value of $k$ simultaneously satisfies both, return $-1$.\n\nTest suite:\n- Case $1$: $N=50$, $\\lambda_1=1.0$, $\\lambda_N=100.0$, $\\|E_n\\|_2=0.3$, $s_{\\max}=0.2$, $\\varepsilon=10^{-6}$.\n- Case $2$: $N=100$, $\\lambda_1=0.1$, $\\lambda_N=1000.0$, $\\|E_n\\|_2=0.2$, $s_{\\max}=0.1$, $\\varepsilon=10^{-6}$.\n- Case $3$: $N=30$, $\\lambda_1=10.0$, $\\lambda_N=50.0$, $\\|E_n\\|_2=0.2$, $s_{\\max}=0.15$, $\\varepsilon=10^{-6}$.\n- Case $4$: $N=60$, $\\lambda_1=0.5$, $\\lambda_N=200.0$, $\\|E_n\\|_2=4.0$, $s_{\\max}=0.05$, $\\varepsilon=10^{-6}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[$k_1,k_2,k_3,k_4$]\"), where each $k_i$ is the minimal integer number of Ritz vectors to recycle for Case $i$, or $-1$ if no such $k$ exists. No other output should be produced. Angles are handled via $\\sin(\\theta)$ and require no explicit unit, and there are no physical units in the final answer as it is purely an integer count per case.",
            "solution": "The problem requires finding the minimal number of recycled Ritz vectors, denoted by $k$, that satisfies two specific conditions for accelerating the Conjugate Gradient (CG) method in a sequence of linear systems arising from a poroelasticity model. The allowed range for $k$ is $\\{0, 1, \\dots, N-2\\}$. If no such $k$ exists, the answer is $-1$. We will analyze each condition separately based on the provided parameters for each test case.\n\nThe key parameters are the matrix dimension $N$, the smallest eigenvalue $\\lambda_1$, the largest eigenvalue $\\lambda_N$, the perturbation norm $\\|E_n\\|_2$, a coherence tolerance $s_{\\max}$, and a solver tolerance $\\varepsilon$. The eigenvalues of the system matrix $A_n$ are modeled with a linear spacing:\n$$\n\\lambda_i = \\lambda_1 + (i-1)\\frac{\\lambda_N - \\lambda_1}{N-1}, \\quad i=1,\\dots,N\n$$\n\n**Condition 1: Coherence Constraint**\n\nThe first constraint ensures that the recycled subspace from the previous timestep remains a good approximation. This is quantified by the Davis-Kahan theorem, which leads to the requirement:\n$$\n\\mathrm{gap}_k \\ge \\frac{\\|E_n\\|_2}{s_{\\max}}\n$$\nwhere $\\mathrm{gap}_k = \\lambda_{k+1} - \\lambda_k$ is the spectral gap. Using the given linear model for the eigenvalues, we can compute the gap:\n$$\n\\mathrm{gap}_k = \\left(\\lambda_1 + k\\frac{\\lambda_N - \\lambda_1}{N-1}\\right) - \\left(\\lambda_1 + (k-1)\\frac{\\lambda_N - \\lambda_1}{N-1}\\right) = \\frac{\\lambda_N - \\lambda_1}{N-1}\n$$\nThis result is independent of $k$. Therefore, the coherence constraint simplifies to a single condition for the entire problem:\n$$\n\\frac{\\lambda_N - \\lambda_1}{N-1} \\ge \\frac{\\|E_n\\|_2}{s_{\\max}}\n$$\nIf this inequality does not hold for a given test case, no value of $k$ can satisfy the coherence constraint, and the solution for that case is immediately $-1$. If the inequality holds, this constraint is satisfied for all possible values of $k$, and we must proceed to the second condition.\n\n**Condition 2: Convergence Constraint**\n\nThe second constraint requires that the number of iterations for the augmented CG method, $m(\\kappa_{\\mathrm{eff}}(k), \\varepsilon)$, is no more than half of the baseline number of iterations, $m_0$.\n$$\nm(\\kappa_{\\mathrm{eff}}(k), \\varepsilon) \\le m_{\\mathrm{half}} = \\lceil m_0 / 2 \\rceil\n$$\nThe number of iterations $m(\\kappa, \\varepsilon)$ is the smallest integer $m$ satisfying the CG error bound:\n$$\n2 \\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^m \\le \\varepsilon\n$$\nSolving for $m$ yields:\n$$\nm(\\kappa, \\varepsilon) = \\left\\lceil \\frac{\\ln(\\varepsilon/2)}{\\ln\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)} \\right\\rceil\n$$\nThe baseline iteration count $m_0$ is calculated using the baseline condition number $\\kappa_0 = \\lambda_N / \\lambda_1$.\nThe effective condition number after deflating $k$ eigenmodes, $\\kappa_{\\mathrm{eff}}(k)$, is given by $\\kappa_{\\mathrm{eff}}(k) = \\lambda_N / \\lambda_{k+1}$. Using the formula for $\\lambda_{k+1}$, we get:\n$$\n\\kappa_{\\mathrm{eff}}(k) = \\frac{\\lambda_N}{\\lambda_1 + k \\frac{\\lambda_N - \\lambda_1}{N-1}}\n$$\nThe iteration count $m(\\kappa, \\varepsilon)$ is a monotonically non-decreasing function of the condition number $\\kappa$. As $k$ increases, $\\lambda_{k+1}$ increases, causing $\\kappa_{\\mathrm{eff}}(k)$ to decrease. Consequently, the iteration count $m(\\kappa_{\\mathrm{eff}}(k), \\varepsilon)$ is a monotonically non-increasing function of $k$.\n\n**Algorithmic Procedure**\n\nTo find the minimum $k$ that satisfies both conditions, we follow a two-step procedure for each test case:\n1.  First, check the coherence constraint $\\frac{\\lambda_N - \\lambda_1}{N-1} \\ge \\frac{\\|E_n\\|_2}{s_{\\max}}$. If it fails, the answer is $-1$.\n2.  If the coherence constraint is satisfied, we proceed to find the minimal $k$ for the convergence constraint. We calculate the baseline iteration count $m_0 = m(\\lambda_N/\\lambda_1, \\varepsilon)$ and the target $m_{\\mathrm{half}} = \\lceil m_0/2 \\rceil$.\n3.  We then iterate through $k$ starting from $k=0$ up to $k=N-2$. For each $k$, we compute $\\kappa_{\\mathrm{eff}}(k)$ and the corresponding iteration count $m_k = m(\\kappa_{\\mathrm{eff}}(k), \\varepsilon)$.\n4.  The first value of $k$ for which $m_k \\le m_{\\mathrm{half}}$ is the minimum required number of Ritz vectors. We record this value and stop the search for the current case.\n5.  If the loop completes without finding any $k$ that satisfies the condition, it means even the maximum possible deflation is insufficient. In this scenario, the answer is $-1$.\n\nThis procedure guarantees finding the minimal valid $k$ if one exists. We will implement this logic to solve the provided test suite.\n\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the minimal number of Ritz vectors to recycle for a series of\n    computational geomechanics problems.\n    \"\"\"\n    test_cases = [\n        # Case 1: (N, lambda1, lambdaN, En_norm, s_max, eps)\n        (50, 1.0, 100.0, 0.3, 0.2, 1e-6),\n        # Case 2:\n        (100, 0.1, 1000.0, 0.2, 0.1, 1e-6),\n        # Case 3:\n        (30, 10.0, 50.0, 0.2, 0.15, 1e-6),\n        # Case 4:\n        (60, 0.5, 200.0, 4.0, 0.05, 1e-6),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N, lambda1, lambdaN, En_norm, s_max, eps = case\n\n        # 1. Check the coherence constraint. Because the eigenvalue gap is constant\n        # for a linear distribution, this check is independent of k.\n        gap = (lambdaN - lambda1) / (N - 1)\n        gap_req = En_norm / s_max\n        if gap  gap_req:\n            results.append(-1)\n            continue\n        \n        # Helper function to calculate the theoretical minimum number of CG iterations.\n        def count_iters(kappa, epsilon):\n            if kappa = 1.0:\n                # A condition number of 1 implies the matrix is a multiple of the\n                # identity. CG converges in 1 iteration.\n                return 1\n            \n            sqrt_kappa = np.sqrt(kappa)\n            term = (sqrt_kappa - 1) / (sqrt_kappa + 1)\n            \n            # The CG error bound is 2 * (term)^m = epsilon, which rearranges to:\n            # m >= log(epsilon / 2) / log(term)\n            # Both numerator and denominator are negative.\n            raw_m = np.log(epsilon / 2.0) / np.log(term)\n            return int(np.ceil(raw_m))\n\n        # 2. Calculate the baseline and target number of iterations.\n        kappa0 = lambdaN / lambda1\n        m0 = count_iters(kappa0, eps)\n        m_half = int(np.ceil(m0 / 2.0))\n\n        # 3. Search for the minimal k that meets the iteration target.\n        min_k = -1\n        # Iterate k from 0 to N-2, as specified by the problem.\n        for k in range(N - 1):\n            lambda_k_plus_1 = lambda1 + k * gap\n\n            kappa_eff = lambdaN / lambda_k_plus_1\n            m_k = count_iters(kappa_eff, eps)\n\n            if m_k = m_half:\n                min_k = k\n                break # Found the minimal k, so we can stop searching.\n        \n        results.append(min_k)\n\n    # Print the final result in the specified format \"[k1,k2,k3,k4]\".\n    # This line would be executed if running the script, but is not part of the answer itself.\n    # print(f\"[{','.join(map(str, results))}]\")\n\n# The function call is commented out as it's not part of the solution logic,\n# but was used to generate the answer.\n# solve()\n```",
            "answer": "[2,1,0,-1]"
        }
    ]
}