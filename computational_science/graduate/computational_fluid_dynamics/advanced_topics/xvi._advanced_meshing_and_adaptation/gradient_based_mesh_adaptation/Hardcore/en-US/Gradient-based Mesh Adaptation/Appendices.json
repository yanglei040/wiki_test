{
    "hands_on_practices": [
        {
            "introduction": "The first step in adapting a mesh is to identify where the numerical error is largest. Gradient-based adaptation strategies typically use the error in the gradient of the solution as a proxy for the total discretization error. This exercise provides a concrete, first-principles calculation of a gradient-based error indicator for a single finite element. By comparing the gradient of an exact solution to that of its linear interpolant, you will build a fundamental understanding of how error is quantified locally, which is the cornerstone of all a posteriori error estimation techniques. ",
            "id": "3325351",
            "problem": "Consider a single triangular finite element with vertices ordered counterclockwise at $\\mathbf{x}_1=(0,0)$, $\\mathbf{x}_2=(1,0)$, and $\\mathbf{x}_3=(0,1)$ in the plane. Let the scalar field be $u(x,y)=\\exp(x)\\sin(y)$, and let $u_h$ be the nodal interpolant in the space of affine linear polynomials (also called $\\mathbb{P}_1$ finite elements) constructed from the pointwise samples $u(\\mathbf{x}_i)$ at the three vertices.\n\nUsing only foundational definitions from interpolation theory and vector calculus, perform the following tasks:\n\n1. Define the elementwise gradient-based error indicator $\\eta_K$ by approximating the $L^2$ norm of the gradient error over the element $K$ via first-order centroid quadrature. That is, treat the integral of the squared gradient error over $K$ as the element area times the squared pointwise error of the gradient at the centroid. Then derive and compute the numerical value of $\\eta_K$ for this element.\n\n2. For anisotropic refinement driven by the magnitude of the directional derivative across each edge, predict which edge should be refined most intensely. For an edge $e$ with outward unit normal $\\mathbf{n}_e$, the across-edge directional derivative at the element centroid $\\mathbf{x}_c$ is the scalar $|\\nabla u(\\mathbf{x}_c)\\cdot \\mathbf{n}_e|$. Use the absolute value to remove any dependence on the choice of normal orientation. Enumerate edges as $e_1=\\overline{\\mathbf{x}_1\\mathbf{x}_2}$, $e_2=\\overline{\\mathbf{x}_2\\mathbf{x}_3}$, and $e_3=\\overline{\\mathbf{x}_3\\mathbf{x}_1}$, and select the index $k\\in\\{1,2,3\\}$ attaining the maximum.\n\nReport your final answer as a $1\\times 2$ row matrix containing:\n- the value of $\\eta_K$ (dimensionless), rounded to $4$ significant figures, followed by\n- the edge index $k$ as an integer.\n\nNo units should be included in the final boxed answer.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\nThe givens are:\n- A triangular finite element $K$ with vertices $\\mathbf{x}_1=(0,0)$, $\\mathbf{x}_2=(1,0)$, and $\\mathbf{x}_3=(0,1)$.\n- A scalar field $u(x,y)=\\exp(x)\\sin(y)$.\n- The nodal interpolant $u_h$ is a $\\mathbb{P}_1$ polynomial.\n- The isotropic error indicator is defined as $\\eta_K = \\left( |K| \\cdot |\\nabla u(\\mathbf{x}_c) - \\nabla u_h(\\mathbf{x}_c)|^2 \\right)^{1/2}$, where $|K|$ is the area of the triangle and $\\mathbf{x}_c$ is its centroid.\n- The anisotropic refinement criterion for an edge $e$ is $|\\nabla u(\\mathbf{x}_c)\\cdot \\mathbf{n}_e|$, where $\\mathbf{n}_e$ is the outward unit normal.\n- The edges are enumerated as $e_1=\\overline{\\mathbf{x}_1\\mathbf{x}_2}$, $e_2=\\overline{\\mathbf{x}_2\\mathbf{x}_3}$, and $e_3=\\overline{\\mathbf{x}_3\\mathbf{x}_1}$.\n\nThe problem is a standard exercise in the field of a posteriori error estimation for the finite element method. All terms are well-defined, the functions are smooth, and the geometry is simple. The problem is based on established principles of numerical analysis and vector calculus. Hence, the problem is valid.\n\nWe will solve the two parts of the problem sequentially.\n\n**Part 1: Calculation of the error indicator $\\eta_K$**\n\nFirst, we determine the geometric properties of the triangular element $K$. The vertices are $\\mathbf{x}_1=(0,0)$, $\\mathbf{x}_2=(1,0)$, and $\\mathbf{x}_3=(0,1)$. This is a right-angled triangle.\nThe area of the element, $|K|$, is given by $|K| = \\frac{1}{2} \\times \\text{base} \\times \\text{height} = \\frac{1}{2} \\times 1 \\times 1 = \\frac{1}{2}$.\nThe centroid of the triangle, $\\mathbf{x}_c$, is the average of its vertices' coordinates:\n$$\n\\mathbf{x}_c = \\frac{1}{3}(\\mathbf{x}_1 + \\mathbf{x}_2 + \\mathbf{x}_3) = \\frac{1}{3}((0,0) + (1,0) + (0,1)) = \\left(\\frac{1}{3}, \\frac{1}{3}\\right)\n$$\n\nNext, we compute the gradient of the true scalar field $u(x,y) = \\exp(x)\\sin(y)$.\n$$\n\\nabla u(x,y) = \\left(\\frac{\\partial u}{\\partial x}, \\frac{\\partial u}{\\partial y}\\right) = (\\exp(x)\\sin(y), \\exp(x)\\cos(y))\n$$\nWe evaluate this gradient at the element centroid $\\mathbf{x}_c = (1/3, 1/3)$:\n$$\n\\nabla u(\\mathbf{x}_c) = \\left(\\exp\\left(\\frac{1}{3}\\right)\\sin\\left(\\frac{1}{3}\\right), \\exp\\left(\\frac{1}{3}\\right)\\cos\\left(\\frac{1}{3}\\right)\\right)\n$$\n\nNow, we must find the $\\mathbb{P}_1$ nodal interpolant $u_h(x,y)$. A $\\mathbb{P}_1$ function is an affine linear polynomial of the form $u_h(x,y) = a + bx + cy$. Its coefficients are determined by enforcing that $u_h$ matches $u$ at the vertices.\nThe nodal values of $u$ are:\n$u_1 = u(\\mathbf{x}_1) = u(0,0) = \\exp(0)\\sin(0) = 0$.\n$u_2 = u(\\mathbf{x}_2) = u(1,0) = \\exp(1)\\sin(0) = 0$.\n$u_3 = u(\\mathbf{x}_3) = u(0,1) = \\exp(0)\\sin(1) = \\sin(1)$.\n\nThe interpolant $u_h$ can be written using the linear basis functions $\\phi_i$ of the triangle:\n$u_h(x,y) = u_1 \\phi_1(x,y) + u_2 \\phi_2(x,y) + u_3 \\phi_3(x,y)$.\nFor this reference triangle, the basis functions are $\\phi_1(x,y) = 1-x-y$, $\\phi_2(x,y) = x$, and $\\phi_3(x,y) = y$.\nSubstituting the nodal values:\n$$\nu_h(x,y) = 0 \\cdot (1-x-y) + 0 \\cdot x + \\sin(1) \\cdot y = y\\sin(1)\n$$\nThe gradient of the interpolant $\\nabla u_h$ is constant over the element:\n$$\n\\nabla u_h(x,y) = \\left(\\frac{\\partial}{\\partial x}(y\\sin(1)), \\frac{\\partial}{\\partial y}(y\\sin(1))\\right) = (0, \\sin(1))\n$$\nSince $\\nabla u_h$ is constant, its value at the centroid $\\mathbf{x}_c$ is $\\nabla u_h(\\mathbf{x}_c) = (0, \\sin(1))$.\n\nNow we compute the gradient error vector at the centroid:\n$$\n\\nabla u(\\mathbf{x}_c) - \\nabla u_h(\\mathbf{x}_c) = \\left(\\exp\\left(\\frac{1}{3}\\right)\\sin\\left(\\frac{1}{3}\\right), \\exp\\left(\\frac{1}{3}\\right)\\cos\\left(\\frac{1}{3}\\right) - \\sin(1)\\right)\n$$\nThe squared magnitude of this vector is:\n$$\n|\\nabla u(\\mathbf{x}_c) - \\nabla u_h(\\mathbf{x}_c)|^2 = \\left(\\exp\\left(\\frac{1}{3}\\right)\\sin\\left(\\frac{1}{3}\\right)\\right)^2 + \\left(\\exp\\left(\\frac{1}{3}\\right)\\cos\\left(\\frac{1}{3}\\right) - \\sin(1)\\right)^2\n$$\n$$\n= \\exp\\left(\\frac{2}{3}\\right)\\sin^2\\left(\\frac{1}{3}\\right) + \\exp\\left(\\frac{2}{3}\\right)\\cos^2\\left(\\frac{1}{3}\\right) - 2\\exp\\left(\\frac{1}{3}\\right)\\cos\\left(\\frac{1}{3}\\right)\\sin(1) + \\sin^2(1)\n$$\n$$\n= \\exp\\left(\\frac{2}{3}\\right)\\left(\\sin^2\\left(\\frac{1}{3}\\right) + \\cos^2\\left(\\frac{1}{3}\\right)\\right) - 2\\exp\\left(\\frac{1}{3}\\right)\\cos\\left(\\frac{1}{3}\\right)\\sin(1) + \\sin^2(1)\n$$\n$$\n= \\exp\\left(\\frac{2}{3}\\right) - 2\\exp\\left(\\frac{1}{3}\\right)\\cos\\left(\\frac{1}{3}\\right)\\sin(1) + \\sin^2(1)\n$$\nThe error indicator $\\eta_K$ is defined as:\n$$\n\\eta_K = \\sqrt{|K|} \\cdot |\\nabla u(\\mathbf{x}_c) - \\nabla u_h(\\mathbf{x}_c)| = \\sqrt{\\frac{1}{2}} \\sqrt{\\exp\\left(\\frac{2}{3}\\right) - 2\\exp\\left(\\frac{1}{3}\\right)\\cos\\left(\\frac{1}{3}\\right)\\sin(1) + \\sin^2(1)}\n$$\nWe now compute the numerical value (using radians for trigonometric functions):\n$\\exp(2/3) \\approx 1.947734$\n$2\\exp(1/3)\\cos(1/3)\\sin(1) \\approx 2 \\cdot (1.395612) \\cdot (0.944957) \\cdot (0.841471) \\approx 2.219516$\n$\\sin^2(1) \\approx (0.841471)^2 \\approx 0.708073$\nSo, the term inside the square root is approximately $1.947734 - 2.219516 + 0.708073 \\approx 0.436291$.\n$$\n\\eta_K \\approx \\sqrt{\\frac{1}{2} \\cdot 0.436291} = \\sqrt{0.2181455} \\approx 0.4670609\n$$\nRounded to $4$ significant figures, $\\eta_K \\approx 0.4671$.\n\n**Part 2: Identification of the edge for refinement**\n\nWe need to compute the indicator $|\\nabla u(\\mathbf{x}_c)\\cdot \\mathbf{n}_e|$ for each edge $e \\in \\{e_1, e_2, e_3\\}$. The vertices are ordered counter-clockwise.\nThe edge vectors are:\n$e_1 = \\overline{\\mathbf{x}_1\\mathbf{x}_2}$: vector $\\mathbf{v}_1 = \\mathbf{x}_2 - \\mathbf{x}_1 = (1,0)$.\n$e_2 = \\overline{\\mathbf{x}_2\\mathbf{x}_3}$: vector $\\mathbf{v}_2 = \\mathbf{x}_3 - \\mathbf{x}_2 = (-1,1)$.\n$e_3 = \\overline{\\mathbf{x}_3\\mathbf{x}_1}$: vector $\\mathbf{v}_3 = \\mathbf{x}_1 - \\mathbf{x}_3 = (0,-1)$.\n\nThe corresponding outward unit normal vectors $\\mathbf{n}_e$ are found by rotating the edge vectors by $-90^\\circ$ (which maps a vector $(a,b)$ to $(b,-a)$) and normalizing.\nFor $e_1$: The normal direction is $(0,-1)$. This is already a unit vector. So, $\\mathbf{n}_{e_1} = (0,-1)$.\nFor $e_2$: The normal direction is $(1,1)$. The length is $\\sqrt{1^2+1^2}=\\sqrt{2}$. So, $\\mathbf{n}_{e_2} = (1/\\sqrt{2}, 1/\\sqrt{2})$.\nFor $e_3$: The normal direction is $(-1,0)$. This is already a unit vector. So, $\\mathbf{n}_{e_3} = (-1,0)$.\n\nNow we compute the dot product of the gradient $\\nabla u(\\mathbf{x}_c) = (\\exp(1/3)\\sin(1/3), \\exp(1/3)\\cos(1/3))$ with each normal vector.\nLet $g_x = \\exp(1/3)\\sin(1/3)$ and $g_y = \\exp(1/3)\\cos(1/3)$.\n$\\nabla u(\\mathbf{x}_c) = (g_x, g_y)$.\n\nFor edge $e_1$:\nIndicator$_1 = |\\nabla u(\\mathbf{x}_c) \\cdot \\mathbf{n}_{e_1}| = |(g_x, g_y) \\cdot (0, -1)| = |-g_y| = g_y = \\exp(1/3)\\cos(1/3)$.\nIndicator$_1 \\approx 1.395612 \\cdot 0.944957 \\approx 1.31876$.\n\nFor edge $e_2$:\nIndicator$_2 = |\\nabla u(\\mathbf{x}_c) \\cdot \\mathbf{n}_{e_2}| = |(g_x, g_y) \\cdot (1/\\sqrt{2}, 1/\\sqrt{2})| = \\frac{1}{\\sqrt{2}}|g_x+g_y|$. Since $g_x, g_y > 0$, this is $\\frac{1}{\\sqrt{2}}(g_x+g_y)$.\nIndicator$_2 = \\frac{\\exp(1/3)}{\\sqrt{2}}(\\sin(1/3) + \\cos(1/3))$.\nIndicator$_2 \\approx \\frac{1.395612}{\\sqrt{2}}(0.327195 + 0.944957) \\approx 0.986845 \\cdot 1.272152 \\approx 1.25544$.\n\nFor edge $e_3$:\nIndicator$_3 = |\\nabla u(\\mathbf{x}_c) \\cdot \\mathbf{n}_{e_3}| = |(g_x, g_y) \\cdot (-1, 0)| = |-g_x| = g_x = \\exp(1/3)\\sin(1/3)$.\nIndicator$_3 \\approx 1.395612 \\cdot 0.327195 \\approx 0.45659$.\n\nComparing the three values:\nIndicator$_1 \\approx 1.31876$\nIndicator$_2 \\approx 1.25544$\nIndicator$_3 \\approx 0.45659$\nThe maximum value is Indicator$_1$, which corresponds to edge $e_1$. Therefore, the index of the edge that should be refined most intensely is $k=1$.\n\nThe final answer consists of the numerical value of $\\eta_K$ rounded to $4$ significant figures and the edge index $k$.\n$\\eta_K \\approx 0.4671$\n$k = 1$\nThese are to be reported as a $1 \\times 2$ row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.4671  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the previous exercise used the exact solution's gradient, practical CFD solvers must approximate this gradient from discrete data. The accuracy of this approximation is highly dependent on the quality of the computational mesh. This practice delves into the mechanics of the widely used Green-Gauss gradient reconstruction method on a non-orthogonal (skewed) mesh. Through a direct analytical derivation, you will discover how mesh skewness introduces a specific, non-physical bias into the computed gradient, a crucial insight for understanding and diagnosing discretization errors in real-world simulations. ",
            "id": "3325359",
            "problem": "Consider two-dimensional Computational Fluid Dynamics (CFD) on a uniform, skewed, parallelogram control-volume tessellation. Let the reference control volume be centered at the origin with edge vectors $\\mathbf{e}_{1} = (h, 0)$ and $\\mathbf{e}_{2} = \\left(h \\cos\\varphi, h \\sin\\varphi\\right)$, where $h0$ and $0\\varphi\\pi$ is the interior angle between the edges. The control volume area is $A = h^{2}\\sin\\varphi$. The faces are the pairs of parallel edges aligned with $\\mathbf{e}_{1}$ and $\\mathbf{e}_{2}$. Let the outward unit normal for the faces aligned with $\\mathbf{e}_{1}$ be $\\mathbf{n}_{1} = (0, 1)$ on the “top” face and $-\\mathbf{n}_{1}$ on the “bottom” face, and the outward unit normal for the faces aligned with $\\mathbf{e}_{2}$ be $\\mathbf{n}_{2} = \\left(\\sin\\varphi, -\\cos\\varphi\\right)$ on the “right” face and $-\\mathbf{n}_{2}$ on the “left” face. The neighboring cell centers across faces are located at $\\pm \\mathbf{e}_{1}$ and $\\pm \\mathbf{e}_{2}$, corresponding to the faces aligned with $\\mathbf{e}_{2}$ and $\\mathbf{e}_{1}$, respectively.\n\nLet the scalar field be\n$$\nu(x,y) = a\\,x + b\\,y + \\kappa\\,x^{3},\n$$\nwith constants $a$, $b$, and $\\kappa$. The exact gradient at the origin is $\\nabla u(0,0) = (a, b)$. Using the Green–Gauss theorem, construct a face-based gradient estimator of $\\nabla u$ on the reference control volume by the flux formulation\n$$\n\\nabla u \\approx \\frac{1}{A} \\sum_{f} \\bar{u}_{f}\\,\\mathbf{n}_{f}\\,|\\Gamma_{f}|,\n$$\nwhere $|\\Gamma_{f}|$ is the face length, $\\mathbf{n}_{f}$ is the outward unit normal, and $\\bar{u}_{f}$ is the approximate face-average reconstructed from cell-centered values by the arithmetic average of the two adjacent cell centers on either side of the face, i.e.,\n$$\n\\bar{u}_{f} \\equiv \\frac{1}{2}\\left(u_{\\text{left of }f} + u_{\\text{right of }f}\\right).\n$$\n\nStarting from the divergence theorem and Taylor expansions about the origin up to the minimal order necessary for a leading error characterization, derive the leading-order asymptotic expression for the estimator’s bias vector\n$$\n\\mathbf{E}(\\varphi,h,\\kappa) \\equiv \\nabla u_{\\text{GG}}(0,0) - \\nabla u(0,0),\n$$\nwhere $\\nabla u_{\\text{GG}}$ denotes the Green–Gauss estimator defined above. Your derivation must expose how the mesh non-orthogonality angle $\\varphi$ enters the error. Express the final answer as a single closed-form analytic expression in terms of $\\varphi$, $h$, and $\\kappa$. No numerical evaluation is required. Do not round. The final answer must be the expression for $\\mathbf{E}(\\varphi,h,\\kappa)$.",
            "solution": "The problem requires the derivation of the leading-order asymptotic expression for the bias vector of a face-based Green-Gauss gradient estimator on a skewed parallelogram mesh. The bias vector is defined as $\\mathbf{E}(\\varphi,h,\\kappa) \\equiv \\nabla u_{\\text{GG}}(0,0) - \\nabla u(0,0)$.\n\nThe Green-Gauss theorem in two dimensions states that for a vector field $\\mathbf{F}$, the integral over a region $\\Omega$ with boundary $\\partial\\Omega$ is given by $\\iint_{\\Omega} \\nabla\\cdot\\mathbf{F} \\,dA = \\oint_{\\partial\\Omega} \\mathbf{F}\\cdot\\mathbf{n}\\,ds$. By applying this theorem to the scalar field $u$ multiplied by a constant vector $\\mathbf{c}$, we have $\\mathbf{F} = u\\mathbf{c}$. The divergence is $\\nabla\\cdot(u\\mathbf{c}) = (\\nabla u)\\cdot\\mathbf{c}$. The integral relation becomes $\\iint_{\\Omega} (\\nabla u)\\cdot\\mathbf{c} \\,dA = \\oint_{\\partial\\Omega} u\\mathbf{c}\\cdot\\mathbf{n}\\,ds$. Since $\\mathbf{c}$ is an arbitrary constant vector, this implies the gradient identity $\\iint_{\\Omega} \\nabla u \\,dA = \\oint_{\\partial\\Omega} u\\mathbf{n}\\,ds$.\nFor a small control volume with area $A$, the average gradient is $\\langle\\nabla u\\rangle \\approx \\frac{1}{A} \\oint_{\\partial\\Omega} u\\mathbf{n}\\,ds$. The provided Green-Gauss estimator is a discrete approximation of this identity:\n$$\n\\nabla u_{\\text{GG}} = \\frac{1}{A} \\sum_{f} \\bar{u}_{f}\\,\\mathbf{n}_{f}\\,|\\Gamma_{f}|\n$$\nwhere $f$ indexes the faces of the control volume, $\\bar{u}_f$ is the approximated value of $u$ on face $f$, $\\mathbf{n}_f$ is the outward unit normal, and $|\\Gamma_f|$ is the face length. The area of the parallelogram control volume is given by $A = h^2\\sin\\varphi$.\n\nWe first identify the properties of the four faces of the reference control volume centered at the origin $\\mathbf{c}_0 = (0,0)$.\nLet the faces be denoted as Right ($f_R$), Left ($f_L$), Top ($f_T$), and Bottom ($f_B$).\n\n1.  **Right face ($f_R$)**: Aligned with $\\mathbf{e}_2$. The outward normal is $\\mathbf{n}_{f_R} = \\mathbf{n}_2 = (\\sin\\varphi, -\\cos\\varphi)$. The face length is $|\\Gamma_{f_R}| = |\\mathbf{e}_2| = h$. The adjacent neighboring cell center is at $\\mathbf{c}_R = \\mathbf{c}_0 + \\mathbf{e}_1 = \\mathbf{e}_1 = (h,0)$.\n2.  **Left face ($f_L$)**: Aligned with $\\mathbf{e}_2$. The outward normal is $\\mathbf{n}_{f_L} = -\\mathbf{n}_2 = (-\\sin\\varphi, \\cos\\varphi)$. The face length is $|\\Gamma_{f_L}| = |\\mathbf{e}_2| = h$. The neighbor is at $\\mathbf{c}_L = \\mathbf{c}_0 - \\mathbf{e}_1 = -\\mathbf{e}_1 = (-h,0)$.\n3.  **Top face ($f_T$)**: Aligned with $\\mathbf{e}_1$. The outward normal is $\\mathbf{n}_{f_T} = \\mathbf{n}_1 = (0, 1)$. The face length is $|\\Gamma_{f_T}| = |\\mathbf{e}_1| = h$. The neighbor is at $\\mathbf{c}_T = \\mathbf{c}_0 + \\mathbf{e}_2 = \\mathbf{e}_2 = (h\\cos\\varphi, h\\sin\\varphi)$.\n4.  **Bottom face ($f_B$)**: Aligned with $\\mathbf{e}_1$. The outward normal is $\\mathbf{n}_{f_B} = -\\mathbf{n}_1 = (0, -1)$. The face length is $|\\Gamma_{f_B}| = |\\mathbf{e}_1| = h$. The neighbor is at $\\mathbf{c}_B = \\mathbf{c}_0 - \\mathbf{e}_2 = -\\mathbf{e}_2 = (-h\\cos\\varphi, -h\\sin\\varphi)$.\n\nThe value of the scalar field at the face, $\\bar{u}_f$, is approximated by the arithmetic average of the values at the two adjacent cell centers. The central cell value is $u_0 = u(0,0)$. The scalar field is $u(x,y) = a x + b y + \\kappa x^3$. Thus, $u_0 = 0$.\n\nLet's compute $\\bar{u}_f$ for each face:\n-   For $f_R$: $\\bar{u}_{f_R} = \\frac{1}{2}(u(\\mathbf{c}_0) + u(\\mathbf{c}_R)) = \\frac{1}{2}(u(0,0) + u(h,0)) = \\frac{1}{2}(0 + ah + \\kappa h^3) = \\frac{1}{2}(ah + \\kappa h^3)$.\n-   For $f_L$: $\\bar{u}_{f_L} = \\frac{1}{2}(u(\\mathbf{c}_0) + u(\\mathbf{c}_L)) = \\frac{1}{2}(u(0,0) + u(-h,0)) = \\frac{1}{2}(0 - ah - \\kappa h^3) = -\\frac{1}{2}(ah + \\kappa h^3)$. Note that $\\bar{u}_{f_L} = -\\bar{u}_{f_R}$.\n-   For $f_T$: $\\bar{u}_{f_T} = \\frac{1}{2}(u(\\mathbf{c}_0) + u(\\mathbf{c}_T)) = \\frac{1}{2}(u(0,0) + u(h\\cos\\varphi, h\\sin\\varphi)) = \\frac{1}{2}(a(h\\cos\\varphi) + b(h\\sin\\varphi) + \\kappa(h\\cos\\varphi)^3) = \\frac{1}{2}(ah\\cos\\varphi + bh\\sin\\varphi + \\kappa h^3\\cos^3\\varphi)$.\n-   For $f_B$: $\\bar{u}_{f_B} = \\frac{1}{2}(u(\\mathbf{c}_0) + u(\\mathbf{c}_B)) = \\frac{1}{2}(u(0,0) + u(-h\\cos\\varphi, -h\\sin\\varphi)) = \\frac{1}{2}(-ah\\cos\\varphi - bh\\sin\\varphi - \\kappa h^3\\cos^3\\varphi)$. Note that $\\bar{u}_{f_B} = -\\bar{u}_{f_T}$.\n\nNow, we compute the sum for the estimator. Let $\\mathbf{S} = \\sum_{f} \\bar{u}_{f}\\,\\mathbf{n}_{f}\\,|\\Gamma_{f}|$. Since $|\\Gamma_f|=h$ for all faces:\n$$\n\\mathbf{S} = h(\\bar{u}_{f_R}\\mathbf{n}_{f_R} + \\bar{u}_{f_L}\\mathbf{n}_{f_L} + \\bar{u}_{f_T}\\mathbf{n}_{f_T} + \\bar{u}_{f_B}\\mathbf{n}_{f_B})\n$$\nUsing the symmetries $\\mathbf{n}_{f_L}=-\\mathbf{n}_{f_R}$, $\\bar{u}_{f_L}=-\\bar{u}_{f_R}$, $\\mathbf{n}_{f_B}=-\\mathbf{n}_{f_T}$, and $\\bar{u}_{f_B}=-\\bar{u}_{f_T}$:\n$$\n\\mathbf{S} = h(\\bar{u}_{f_R}\\mathbf{n}_{f_R} + (-\\bar{u}_{f_R})(-\\mathbf{n}_{f_R}) + \\bar{u}_{f_T}\\mathbf{n}_{f_T} + (-\\bar{u}_{f_T})(-\\mathbf{n}_{f_T})) = 2h(\\bar{u}_{f_R}\\mathbf{n}_{f_R} + \\bar{u}_{f_T}\\mathbf{n}_{f_T})\n$$\nSubstitute the expressions for $\\bar{u}_f$ and $\\mathbf{n}_f$:\n$$\n\\mathbf{S} = 2h \\left[ \\frac{1}{2}(ah + \\kappa h^3)\\mathbf{n}_2 + \\frac{1}{2}(ah\\cos\\varphi + bh\\sin\\varphi + \\kappa h^3\\cos^3\\varphi)\\mathbf{n}_1 \\right]\n$$\n$$\n\\mathbf{S} = h(ah + \\kappa h^3)\\mathbf{n}_2 + h(ah\\cos\\varphi + bh\\sin\\varphi + \\kappa h^3\\cos^3\\varphi)\\mathbf{n}_1\n$$\nLet's analyze the components of $\\mathbf{S}=(S_x, S_y)$ using $\\mathbf{n}_1=(0,1)$ and $\\mathbf{n}_2=(\\sin\\varphi, -\\cos\\varphi)$.\n$$\nS_x = h(ah + \\kappa h^3)\\sin\\varphi + h(ah\\cos\\varphi + bh\\sin\\varphi + \\kappa h^3\\cos^3\\varphi)(0) = (ah^2 + \\kappa h^4)\\sin\\varphi\n$$\n$$\nS_y = h(ah + \\kappa h^3)(-\\cos\\varphi) + h(ah\\cos\\varphi + bh\\sin\\varphi + \\kappa h^3\\cos^3\\varphi)(1)\n$$\n$$\nS_y = -ah^2\\cos\\varphi - \\kappa h^4\\cos\\varphi + ah^2\\cos\\varphi + bh^2\\sin\\varphi + \\kappa h^4\\cos^3\\varphi\n$$\n$$\nS_y = bh^2\\sin\\varphi + \\kappa h^4(\\cos^3\\varphi - \\cos\\varphi) = bh^2\\sin\\varphi + \\kappa h^4\\cos\\varphi(\\cos^2\\varphi-1)\n$$\nUsing the identity $\\sin^2\\varphi + \\cos^2\\varphi = 1$, we get $\\cos^2\\varphi-1 = -\\sin^2\\varphi$.\n$$\nS_y = bh^2\\sin\\varphi - \\kappa h^4\\cos\\varphi\\sin^2\\varphi\n$$\nThe estimated gradient is $\\nabla u_{\\text{GG}} = \\mathbf{S}/A$:\n$$\n(\\nabla u_{\\text{GG}})_x = \\frac{S_x}{A} = \\frac{(ah^2 + \\kappa h^4)\\sin\\varphi}{h^2\\sin\\varphi} = a + \\kappa h^2\n$$\n$$\n(\\nabla u_{\\text{GG}})_y = \\frac{S_y}{A} = \\frac{bh^2\\sin\\varphi - \\kappa h^4\\cos\\varphi\\sin^2\\varphi}{h^2\\sin\\varphi} = b - \\kappa h^2\\cos\\varphi\\sin\\varphi\n$$\nSo, the estimated gradient vector is $\\nabla u_{\\text{GG}}(0,0) = (a + \\kappa h^2, \\quad b - \\kappa h^2 \\cos\\varphi \\sin\\varphi)$.\n\nThe exact gradient at the origin is found by direct differentiation of $u(x,y) = ax + by + \\kappa x^3$:\n$$\n\\nabla u = \\left(\\frac{\\partial u}{\\partial x}, \\frac{\\partial u}{\\partial y}\\right) = (a + 3\\kappa x^2, \\quad b)\n$$\nEvaluating at the origin $(0,0)$, we get $\\nabla u(0,0) = (a, b)$.\n\nThe bias (error) vector is $\\mathbf{E} = \\nabla u_{\\text{GG}}(0,0) - \\nabla u(0,0)$:\n$$\n\\mathbf{E} = (a + \\kappa h^2, \\quad b - \\kappa h^2 \\cos\\varphi \\sin\\varphi) - (a, b)\n$$\n$$\n\\mathbf{E} = (\\kappa h^2, \\quad -\\kappa h^2 \\cos\\varphi \\sin\\varphi)\n$$\nThis expression is the leading-order asymptotic error. The estimator is exact for the linear part of the field, $ax+by$. The error arises from the cubic term $\\kappa x^3$. The order of the error is $O(h^2)$, as shown by the common factor $h^2$.\n\nThe influence of the mesh non-orthogonality angle $\\varphi$ is evident in the y-component of the error, $E_y = -\\kappa h^2 \\cos\\varphi \\sin\\varphi$. The term $\\cos\\varphi$ represents the deviation from orthogonality. For an orthogonal mesh where $\\varphi=\\pi/2$, we have $\\cos(\\pi/2)=0$, and this error component vanishes, $E_y=0$. For any non-orthogonal mesh ($\\varphi \\neq \\pi/2$), a non-zero error is introduced into the y-component of the gradient, even though the underlying field variation is purely in the x-direction (from the $x^3$ term). This demonstrates the mechanism by which grid skewness introduces discretization errors that couple different spatial directions.\n\nThe final expression for the bias vector is:\n$$\n\\mathbf{E}(\\varphi,h,\\kappa) = \\kappa h^2 (1, -\\sin\\varphi\\cos\\varphi)\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\kappa h^{2}  -\\kappa h^{2} \\sin\\varphi \\cos\\varphi\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "High-resolution numerical schemes in CFD often employ \"limiters\" to enforce physical principles, such as ensuring a quantity like density remains positive or preventing unphysical oscillations near sharp features. However, these limiters can interact with the components of an adaptation scheme in subtle ways. This exercise presents a scenario where a monotonicity-preserving limiter is active at a local extremum of the solution. You will calculate the \"limited\" gradient and explore the profound consequences for mesh adaptation, leading to the critical conclusion that a naive gradient-based metric can fail precisely where refinement is most needed. This practice highlights the necessity of using more sophisticated metrics, often based on second derivatives (the Hessian), to create a robust adaptation strategy. ",
            "id": "3325297",
            "problem": "Consider a two-dimensional finite volume cell with centroid at the origin $(x,y)=(0,0)$ in a computational fluid dynamics setting. Let a scalar conserved variable $u$ satisfy the scalar conservation law $\\partial_{t} u + \\nabla \\cdot \\mathbf{F}(u) = 0$, where $\\mathbf{F}(u)$ is a differentiable flux function. In a finite volume reconstruction, the cell-centered value $u_{P}$ at the centroid is approximated in a neighborhood using a linear reconstruction $u(\\mathbf{r}) \\approx u_{P} + \\nabla u \\cdot \\mathbf{r}$, where $\\nabla u$ is a constant gradient over the cell, and $\\mathbf{r}$ denotes the position vector relative to the centroid. The unconstrained gradient $\\nabla u$ is computed by a least-squares fit minimizing the squared residuals of the relation $u_{i} - u_{P} \\approx \\nabla u \\cdot \\mathbf{r}_{i}$ over the cell’s neighbor stencil $\\{\\mathbf{r}_{i}, u_{i}\\}$, with equal weights.\n\nYou are given the neighbor centroid positions and cell-center values for the stencil:\n- $\\mathbf{r}_{1} = (1,0)$, $u_{1} = 1.6$,\n- $\\mathbf{r}_{2} = (0,1)$, $u_{2} = 1.7$,\n- $\\mathbf{r}_{3} = (-1,0)$, $u_{3} = 1.8$,\n- $\\mathbf{r}_{4} = (0,-1)$, $u_{4} = 1.9$,\nand the target cell-center value $u_{P} = 2.0$.\n\nAn extremum detector has flagged the target cell as a local maximum, meaning $u_{P} \\geq u_{i}$ for all stencil points. To enforce a discrete maximum principle consistent with monotonicity, introduce a scalar limiter $\\phi \\in [0,1]$ and define the limited gradient $\\nabla u^{\\ell} = \\phi \\nabla u_{\\mathrm{ls}}$, where $\\nabla u_{\\mathrm{ls}}$ denotes the least-squares gradient. The limiter must be chosen to ensure that the reconstructed values at neighbor centroids remain within the stencil bounds, i.e., $u_{P} + \\phi \\, \\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_{i} \\in [u_{\\min}, u_{\\max}]$ for all $i$, with $u_{\\min} = \\min\\{u_{P}, u_{1}, u_{2}, u_{3}, u_{4}\\}$ and $u_{\\max} = \\max\\{u_{P}, u_{1}, u_{2}, u_{3}, u_{4}\\}$.\n\nUsing these data and constraints:\n1. Compute the unconstrained least-squares gradient $\\nabla u_{\\mathrm{ls}}$.\n2. Determine the maximal $\\phi \\in [0,1]$ that satisfies the monotonicity constraints for all stencil points.\n3. Compute the limited gradient $\\nabla u^{\\ell}$.\n\nExpress the final limited gradient components in nondimensional units as a row vector. No rounding is required. In addition, in your solution, discuss the implications of the limited gradient at a detected extremum for the construction of gradient-based mesh adaptation metrics and contrast this with Hessian-based approaches, starting from the underlying requirement that interpolation error control should be consistent with the local smoothness of $u$.",
            "solution": "The user-provided problem is evaluated for validity.\n\n### Step 1: Extract Givens\n- **Governing Law**: A scalar conserved variable $u$ satisfies the conservation law $\\partial_{t} u + \\nabla \\cdot \\mathbf{F}(u) = 0$.\n- **Cell Geometry**: A two-dimensional finite volume cell with centroid at the origin $(x,y)=(0,0)$.\n- **Linear Reconstruction**: The cell-centered value $u_{P}$ is approximated within the cell as $u(\\mathbf{r}) \\approx u_{P} + \\nabla u \\cdot \\mathbf{r}$, where $\\mathbf{r}$ is the position vector from the cell centroid.\n- **Gradient Computation**: The unconstrained gradient $\\nabla u$ is computed via a least-squares fit minimizing the squared residuals of $u_{i} - u_{P} \\approx \\nabla u \\cdot \\mathbf{r}_{i}$ over a neighbor stencil $\\{\\mathbf{r}_{i}, u_{i}\\}$ with equal weights.\n- **Stencil Data**:\n    - Neighbor 1: $\\mathbf{r}_{1} = (1,0)$, $u_{1} = 1.6$.\n    - Neighbor 2: $\\mathbf{r}_{2} = (0,1)$, $u_{2} = 1.7$.\n    - Neighbor 3: $\\mathbf{r}_{3} = (-1,0)$, $u_{3} = 1.8$.\n    - Neighbor 4: $\\mathbf{r}_{4} = (0,-1)$, $u_{4} = 1.9$.\n- **Target Cell Data**: $u_{P} = 2.0$.\n- **Extremum Condition**: The target cell is a local maximum, satisfying $u_{P} \\geq u_{i}$ for all stencil points $i$.\n- **Gradient Limiting**: The limited gradient is $\\nabla u^{\\ell} = \\phi \\nabla u_{\\mathrm{ls}}$, where $\\nabla u_{\\mathrm{ls}}$ is the least-squares gradient and $\\phi \\in [0,1]$ is a scalar limiter.\n- **Monotonicity Constraint**: The limiter $\\phi$ must ensure the reconstructed values at neighbor centroids, $u(\\mathbf{r}_{i}) = u_{P} + \\phi \\, \\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_{i}$, remain within the bounds $[u_{\\min}, u_{\\max}]$.\n- **Stencil Bounds**: $u_{\\min} = \\min\\{u_{P}, u_{1}, u_{2}, u_{3}, u_{4}\\}$ and $u_{\\max} = \\max\\{u_{P}, u_{1}, u_{2}, u_{3}, u_{4}\\}$.\n- **Tasks**:\n    1. Compute the unconstrained least-squares gradient $\\nabla u_{\\mathrm{ls}}$.\n    2. Determine the maximal $\\phi \\in [0,1]$ satisfying the monotonicity constraints.\n    3. Compute the limited gradient $\\nabla u^{\\ell}$.\n    4. Discuss implications of the limited gradient for gradient-based mesh adaptation.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem describes the standard Barth-Jespersen limiter procedure, a well-established technique in finite volume methods for enforcing monotonicity in second-order reconstructions. The use of least-squares for gradient computation is also a standard practice. The principles are scientifically sound and central to the field of computational fluid dynamics.\n- **Well-Posedness**: The problem provides all necessary data and constraints to compute a unique solution for the gradient and the limiter. The data is self-consistent; the condition $u_P \\geq u_i$ is met by the given values ($2.0 \\ge \\{1.6, 1.7, 1.8, 1.9\\}$).\n- **Objectivity**: The problem is stated using precise, objective mathematical and technical terms.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically grounded, well-posed, and objective. A complete solution will be provided.\n\n### Part 1: Computation of the Unconstrained Least-Squares Gradient $\\nabla u_{\\mathrm{ls}}$\nLet the unconstrained gradient be $\\nabla u_{\\mathrm{ls}} = (g_x, g_y)$. The objective is to find $(g_x, g_y)$ that minimizes the sum of squared residuals, $S$:\n$$S(g_x, g_y) = \\sum_{i=1}^{4} \\left( (\\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_i) - (u_i - u_P) \\right)^2 = \\sum_{i=1}^{4} \\left( g_x x_i + g_y y_i - (u_i - u_P) \\right)^2$$\nTo minimize $S$, we set its partial derivatives with respect to $g_x$ and $g_y$ to zero:\n$$ \\frac{\\partial S}{\\partial g_x} = 2 \\sum_{i=1}^{4} \\left( g_x x_i + g_y y_i - (u_i - u_P) \\right) x_i = 0 $$\n$$ \\frac{\\partial S}{\\partial g_y} = 2 \\sum_{i=1}^{4} \\left( g_x x_i + g_y y_i - (u_i - u_P) \\right) y_i = 0 $$\nThis leads to the normal equations, a system of linear equations for $(g_x, g_y)$:\n$$ \\begin{pmatrix} \\sum x_i^2  \\sum x_i y_i \\\\ \\sum x_i y_i  \\sum y_i^2 \\end{pmatrix} \\begin{pmatrix} g_x \\\\ g_y \\end{pmatrix} = \\begin{pmatrix} \\sum x_i (u_i - u_P) \\\\ \\sum y_i (u_i - u_P) \\end{pmatrix} $$\nFirst, we compute the coefficients of the matrix. The neighbor positions are $\\mathbf{r}_{1} = (1,0)$, $\\mathbf{r}_{2} = (0,1)$, $\\mathbf{r}_{3} = (-1,0)$, and $\\mathbf{r}_{4} = (0,-1)$.\n$$ \\sum x_i^2 = 1^2 + 0^2 + (-1)^2 + 0^2 = 2 $$\n$$ \\sum y_i^2 = 0^2 + 1^2 + 0^2 + (-1)^2 = 2 $$\n$$ \\sum x_i y_i = (1)(0) + (0)(1) + (-1)(0) + (0)(-1) = 0 $$\nThe system matrix is diagonal due to the orthogonality of the stencil positions.\nNext, we compute the right-hand side vector. The differences $u_i - u_P$ are:\n- $u_1 - u_P = 1.6 - 2.0 = -0.4$\n- $u_2 - u_P = 1.7 - 2.0 = -0.3$\n- $u_3 - u_P = 1.8 - 2.0 = -0.2$\n- $u_4 - u_P = 1.9 - 2.0 = -0.1$\nThe sums are:\n$$ \\sum x_i (u_i - u_P) = (1)(-0.4) + (0)(-0.3) + (-1)(-0.2) + (0)(-0.1) = -0.4 + 0.2 = -0.2 $$\n$$ \\sum y_i (u_i - u_P) = (0)(-0.4) + (1)(-0.3) + (0)(-0.2) + (-1)(-0.1) = -0.3 + 0.1 = -0.2 $$\nThe linear system is:\n$$ \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} g_x \\\\ g_y \\end{pmatrix} = \\begin{pmatrix} -0.2 \\\\ -0.2 \\end{pmatrix} $$\nSolving for $g_x$ and $g_y$:\n$$ 2g_x = -0.2 \\implies g_x = -0.1 $$\n$$ 2g_y = -0.2 \\implies g_y = -0.1 $$\nThus, the unconstrained least-squares gradient is $\\nabla u_{\\mathrm{ls}} = (-0.1, -0.1)$.\n\n### Part 2: Determination of the Maximal Limiter $\\phi$\nThe monotonicity constraint requires that the reconstructed value at each neighbor's centroid, $u(\\mathbf{r}_i) = u_{P} + \\phi \\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_i$, lies within the range $[u_{\\min}, u_{\\max}]$.\nFirst, we determine the bounds from the given stencil data $\\{2.0, 1.6, 1.7, 1.8, 1.9\\}$:\n$$ u_{\\min} = 1.6 $$\n$$ u_{\\max} = 2.0 $$\nThe constraint is $1.6 \\le u(\\mathbf{r}_i) \\le 2.0$. Substituting $u_P = 2.0$, this becomes:\n$$ 1.6 \\le 2.0 + \\phi \\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_i \\le 2.0 $$\nThis simplifies to two inequalities:\n1. $2.0 + \\phi \\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_i \\le 2.0 \\implies \\phi \\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_i \\le 0$\n2. $1.6 \\le 2.0 + \\phi \\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_i \\implies -0.4 \\le \\phi \\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_i$\nWe evaluate $\\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_i$ for each neighbor:\n- $\\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_1 = (-0.1)(1) + (-0.1)(0) = -0.1$\n- $\\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_2 = (-0.1)(0) + (-0.1)(1) = -0.1$\n- $\\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_3 = (-0.1)(-1) + (-0.1)(0) = 0.1$\n- $\\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_4 = (-0.1)(0) + (-0.1)(-1) = 0.1$\n\nWe seek the largest $\\phi \\in [0,1]$ that satisfies the constraints for all $i=1, 2, 3, 4$.\n- For $i=1$: $\\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_1 = -0.1$.\n    - Constraint 1: $\\phi(-0.1) \\le 0$, which is true for $\\phi \\ge 0$.\n    - Constraint 2: $-0.4 \\le \\phi(-0.1)$, which means $0.4 \\ge 0.1\\phi$, so $\\phi \\le 4$.\n    - The net constraint for $i=1$ is $0 \\le \\phi \\le 4$.\n- For $i=2$: $\\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_2 = -0.1$.\n    - The constraints are identical to the case $i=1$, yielding $0 \\le \\phi \\le 4$.\n- For $i=3$: $\\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_3 = 0.1$.\n    - Constraint 1: $\\phi(0.1) \\le 0$. Since $\\phi \\ge 0$, this requires $\\phi \\le 0$.\n    - Constraint 2: $-0.4 \\le \\phi(0.1)$, which means $\\phi \\ge -4$.\n    - The net constraint for $i=3$ is $\\phi=0$.\n- For $i=4$: $\\nabla u_{\\mathrm{ls}} \\cdot \\mathbf{r}_4 = 0.1$.\n    - The constraints are identical to the case $i=3$, yielding $\\phi=0$.\n\nTo satisfy the constraints for all neighbors simultaneously, we must take the most restrictive condition. The maximal value of $\\phi$ that satisfies $\\phi \\le 4$ and $\\phi=0$ while also being in the interval $[0,1]$ is $\\phi = 0$.\nThe physical interpretation is that the unconstrained linear reconstruction $u(\\mathbf{r}) = 2.0 - 0.1 x - 0.1 y$ would produce values $u(\\mathbf{r}_3) = 2.1$ and $u(\\mathbf{r}_4)=2.1$, which are greater than $u_{\\max} = 2.0$. This creation of a new, spurious extremum violates the discrete maximum principle. To prevent this, the gradient must be flattened entirely, which corresponds to setting $\\phi=0$.\n\n### Part 3: Computation of the Limited Gradient $\\nabla u^{\\ell}$\nThe limited gradient is given by $\\nabla u^{\\ell} = \\phi \\nabla u_{\\mathrm{ls}}$. Using the results from the previous parts:\n$$ \\nabla u^{\\ell} = (0) \\times (-0.1, -0.1) = (0, 0) $$\nThe limited gradient is the zero vector.\n\n### Discussion: Implications for Gradient-Based Mesh Adaptation\nThe result $\\nabla u^{\\ell} = (0,0)$ at a detected local extremum has profound implications for gradient-based mesh adaptation. Mesh adaptation techniques aim to improve solution accuracy by refining the computational grid in regions of high solution error and coarsening it elsewhere. The error is typically estimated using a metric, or sensor, derived from the computed solution.\n\nA common and simple adaptation metric is based on the magnitude of the solution gradient, $|\\nabla u|$. The strategy is to refine the mesh where $|\\nabla u|$ is large to better resolve sharp changes. However, if one were to naively use the *limited* gradient, $\\nabla u^{\\ell}$, from the flow solver's reconstruction as the adaptation metric, a severe logical flaw would occur. As demonstrated, at a local extremum, the monotonicity enforcement required by the numerical scheme forces $\\nabla u^{\\ell}$ to be zero. A gradient-based metric would thus interpret this region as perfectly resolved and requiring no refinement. This is fundamentally incorrect, as extrema are critical solution features where the underlying function, while having a zero gradient, exhibits significant curvature. Failing to refine at extrema would lead to a poor representation of solution peaks, troughs, and vortices.\n\nThis highlights the conflict between the needs of a stable numerical discretization (which may require limiting) and the needs of an accurate error estimation for mesh adaptation. The local smoothness of a function $u$ near an extremum is characterized not by its first derivatives, but by its second derivatives, encapsulated in the Hessian matrix, $H(u)$. For a piecewise linear approximation, the interpolation error is proportional to $h^2|H(u)|$, where $h$ is the mesh size.\n\nTherefore, a robust mesh adaptation strategy must be consistent with the behavior of the interpolation error. Hessian-based adaptation metrics, which are constructed from estimates of the solution's second derivatives, are conceptually superior in this regard. A Hessian-based metric would correctly identify the large curvature at the extremum in this problem—even though the gradient is zero—and trigger mesh refinement. This ensures that the local interpolation error is controlled and that important physical features are not smoothed away or aliased due to insufficient resolution. In contrast, a scheme relying on the limited gradient is blind to such features and is therefore an inadequate basis for a general-purpose adaptation strategy.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} 0  0 \\end{pmatrix}\n}\n$$"
        }
    ]
}