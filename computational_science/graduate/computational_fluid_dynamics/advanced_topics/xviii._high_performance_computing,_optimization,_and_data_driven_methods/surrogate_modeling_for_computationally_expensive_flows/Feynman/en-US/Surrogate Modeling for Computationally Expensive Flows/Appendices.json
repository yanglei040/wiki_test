{
    "hands_on_practices": [
        {
            "introduction": "A common first step in building a surrogate for high-dimensional fields is to find a low-dimensional representation that captures the essential dynamics. Proper Orthogonal Decomposition (POD) provides an optimal linear basis for this task by extracting the most energetic coherent structures from a set of flow snapshots. This practice provides hands-on experience with the mechanics of POD by having you project a new velocity field onto a given basis and compute the resulting approximation error, while also prompting you to think critically about how the diversity of the training data impacts model quality .",
            "id": "3369133",
            "problem": "Consider a nondimensional, incompressible flow field discretized on a uniform mesh so that the discrete $L^{2}$ inner product coincides with the Euclidean dot product on $\\mathbb{R}^{3}$. A Proper Orthogonal Decomposition (POD) basis of rank $r=2$ has been constructed from a snapshot ensemble using the Singular Value Decomposition (SVD) of the snapshot matrix. The resulting $L^{2}$-orthonormal POD modes are given by the columns of the matrix\n$$\nU_{r} \\;=\\; \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n0 & 0\n\\end{pmatrix},\n$$\nso that $\\phi_{1} = \\left(\\frac{1}{\\sqrt{2}},\\, \\frac{1}{\\sqrt{2}},\\, 0\\right)^{\\top}$ and $\\phi_{2} = \\left(-\\frac{1}{\\sqrt{2}},\\, \\frac{1}{\\sqrt{2}},\\, 0\\right)^{\\top}$. Let $P_{r}$ denote the $L^{2}$-orthogonal projection onto $\\operatorname{span}\\{\\phi_{1},\\phi_{2}\\}$.\n\nYou are given a nondimensional discrete velocity field\n$$\nu \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\\\ \\sqrt{2} \\end{pmatrix}.\n$$\n\nStarting from the definitions of the $L^{2}$ inner product, orthonormality, and the orthogonal projection operator, compute the projection error norm $\\|u - P_{r} u\\|_{L^{2}}$.\n\nAdditionally, using first principles from the construction of POD via the Singular Value Decomposition (SVD), discuss how the diversity of snapshots (e.g., broad parametric variation, multiple coherent structures, or multi-regime dynamics) affects the decay rate of the singular values associated with the snapshot matrix. Your discussion must be reasoned from the properties of the snapshot correlation operator and the optimality of POD, not from heuristic rules. The final answer for the projection error must be expressed exactly and does not require rounding.",
            "solution": "The problem consists of two parts. The first part requires the computation of a projection error norm for a given discrete velocity field onto a subspace spanned by a given set of Proper Orthogonal Decomposition (POD) modes. The second part requires a theoretical discussion on the relationship between snapshot diversity and the decay rate of singular values in the context of POD.\n\n**Part 1: Computation of the Projection Error Norm**\n\nThe problem is set in a finite-dimensional vector space $\\mathbb{R}^{3}$, where the discrete $L^{2}$ inner product is defined as the standard Euclidean dot product, $\\langle u, v \\rangle_{L^{2}} = u^{\\top}v$. The associated norm is the Euclidean norm, $\\|u\\|_{L^{2}} = \\sqrt{u^{\\top}u}$.\n\nWe are given a rank-$r=2$ POD basis, which is an orthonormal set of vectors $\\{\\phi_{1}, \\phi_{2}\\}$. These are given as columns of the matrix $U_{r}$:\n$$\n\\phi_{1} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}, \\quad \\phi_{2} = \\begin{pmatrix} -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}\n$$\nThe orthonormality of these basis vectors is confirmed by checking the inner products:\n$$\n\\langle \\phi_{1}, \\phi_{1} \\rangle = \\left(\\frac{1}{\\sqrt{2}}\\right)^{2} + \\left(\\frac{1}{\\sqrt{2}}\\right)^{2} + 0^{2} = \\frac{1}{2} + \\frac{1}{2} = 1\n$$\n$$\n\\langle \\phi_{2}, \\phi_{2} \\rangle = \\left(-\\frac{1}{\\sqrt{2}}\\right)^{2} + \\left(\\frac{1}{\\sqrt{2}}\\right)^{2} + 0^{2} = \\frac{1}{2} + \\frac{1}{2} = 1\n$$\n$$\n\\langle \\phi_{1}, \\phi_{2} \\rangle = \\left(\\frac{1}{\\sqrt{2}}\\right)\\left(-\\frac{1}{\\sqrt{2}}\\right) + \\left(\\frac{1}{\\sqrt{2}}\\right)\\left(\\frac{1}{\\sqrt{2}}\\right) + (0)(0) = -\\frac{1}{2} + \\frac{1}{2} = 0\n$$\nThe set $\\{\\phi_{1}, \\phi_{2}\\}$ is indeed orthonormal with respect to the specified inner product.\n\nThe orthogonal projection of a vector $u$ onto the subspace $W = \\operatorname{span}\\{\\phi_{1}, \\phi_{2}\\}$ is given by the operator $P_{r}$, defined as:\n$$\nP_{r} u = \\sum_{i=1}^{r} \\langle u, \\phi_{i} \\rangle_{L^{2}} \\phi_{i}\n$$\nIn our case, $r=2$, so the projection of the given velocity field $u = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\sqrt{2} \\end{pmatrix}$ is:\n$$\nP_{2} u = \\langle u, \\phi_{1} \\rangle_{L^{2}} \\phi_{1} + \\langle u, \\phi_{2} \\rangle_{L^{2}} \\phi_{2}\n$$\nFirst, we compute the coefficients, which are the inner products of $u$ with the basis vectors:\n$$\n\\alpha_{1} = \\langle u, \\phi_{1} \\rangle_{L^{2}} = u^{\\top}\\phi_{1} = \\begin{pmatrix} 1 & 0 & \\sqrt{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix} = (1)\\left(\\frac{1}{\\sqrt{2}}\\right) + (0)\\left(\\frac{1}{\\sqrt{2}}\\right) + (\\sqrt{2})(0) = \\frac{1}{\\sqrt{2}}\n$$\n$$\n\\alpha_{2} = \\langle u, \\phi_{2} \\rangle_{L^{2}} = u^{\\top}\\phi_{2} = \\begin{pmatrix} 1 & 0 & \\sqrt{2} \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix} = (1)\\left(-\\frac{1}{\\sqrt{2}}\\right) + (0)\\left(\\frac{1}{\\sqrt{2}}\\right) + (\\sqrt{2})(0) = -\\frac{1}{\\sqrt{2}}\n$$\nNow, we construct the projected vector $P_{2}u$:\n$$\nP_{2} u = \\alpha_{1}\\phi_{1} + \\alpha_{2}\\phi_{2} = \\left(\\frac{1}{\\sqrt{2}}\\right) \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix} + \\left(-\\frac{1}{\\sqrt{2}}\\right) \\begin{pmatrix} -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe projection error is the difference between the original vector $u$ and its projection $P_{2}u$:\n$$\nu - P_{2} u = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\sqrt{2} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\sqrt{2} \\end{pmatrix}\n$$\nFinally, we compute the $L^{2}$-norm of this error vector:\n$$\n\\|u - P_{2} u\\|_{L^{2}} = \\left\\| \\begin{pmatrix} 0 \\\\ 0 \\\\ \\sqrt{2} \\end{pmatrix} \\right\\|_{L^{2}} = \\sqrt{0^{2} + 0^{2} + (\\sqrt{2})^{2}} = \\sqrt{2}\n$$\nAlternatively, by the Pythagorean theorem for orthogonal projections, $\\|u - P_r u\\|_{L^2}^2 = \\|u\\|_{L^2}^2 - \\|P_r u\\|_{L^2}^2$.\nWe have $\\|u\\|_{L^2}^2 = 1^2 + 0^2 + (\\sqrt{2})^2 = 3$. The norm of the projection is $\\|P_r u\\|_{L^2}^2 = |\\alpha_1|^2 + |\\alpha_2|^2 = (\\frac{1}{\\sqrt{2}})^2 + (-\\frac{1}{\\sqrt{2}})^2 = \\frac{1}{2} + \\frac{1}{2} = 1$.\nTherefore, $\\|u - P_r u\\|_{L^2}^2 = 3 - 1 = 2$, which gives $\\|u - P_r u\\|_{L^2} = \\sqrt{2}$.\n\n**Part 2: Discussion on Snapshot Diversity and Singular Value Decay**\n\nProper Orthogonal Decomposition (POD) provides an optimal basis for representing an ensemble of data in a least-squares sense. The POD basis vectors $\\{\\phi_i\\}_{i=1}^N$ are derived from the Singular Value Decomposition (SVD) of a snapshot matrix $S \\in \\mathbb{R}^{N \\times M}$, where $N$ is the number of spatial degrees of freedom and $M$ is the number of snapshots. The columns of $S$ are the discrete snapshot vectors $\\{s_j\\}_{j=1}^M$. The SVD of $S$ is $S = U \\Sigma V^{\\top}$, where the columns of $U \\in \\mathbb{R}^{N \\times N}$ are the POD modes $\\phi_i$ and $\\Sigma \\in \\mathbb{R}^{N \\times M}$ is the diagonal matrix of singular values $\\sigma_i \\geq 0$.\n\nThe optimality of POD is linked to the singular values. For any rank $r$, the POD basis $\\{\\phi_i\\}_{i=1}^r$ minimizes the average projection error over the entire snapshot ensemble:\n$$\n\\min_{\\{\\psi_i\\}_{i=1}^r} \\sum_{j=1}^{M} \\left\\| s_j - \\sum_{i=1}^{r} \\langle s_j, \\psi_i \\rangle \\psi_i \\right\\|^2 = \\sum_{j=1}^{M} \\left\\| s_j - \\sum_{i=1}^{r} \\langle s_j, \\phi_i \\rangle \\phi_i \\right\\|^2 = \\sum_{i=r+1}^{k} \\sigma_i^2\n$$\nwhere $k=\\min(N,M)$. The quantity $\\sigma_i^2$ can be interpreted as the \"energy\" or variance captured by the $i$-th mode $\\phi_i$. The total energy of the ensemble is $\\sum_{j=1}^M \\|s_j\\|^2 = \\operatorname{tr}(S^{\\top}S) = \\sum_{i=1}^k \\sigma_i^2$.\n\nThe decay rate of the singular values $\\sigma_i$ is determined by how this total energy is distributed among the modes. This distribution is directly related to the linear dependence or correlation among the snapshots, which is a measure of their \"diversity\". Let us consider the snapshot correlation operator, whose matrix representation is the Gramian matrix $C = S^{\\top}S \\in \\mathbb{R}^{M \\times M}$. The element $C_{ij} = \\langle s_i, s_j \\rangle$ measures the correlation between snapshot $s_i$ and snapshot $s_j$. The eigenvalues of $C$ are $\\lambda_i = \\sigma_i^2$.\n\n1.  **Low Snapshot Diversity:** If the snapshots are not diverse (e.g., they represent minor fluctuations around a steady state or depict a single, dominant coherent structure), the snapshot vectors $\\{s_j\\}$ are highly correlated. This means the vectors are nearly linearly dependent, and the snapshot matrix $S$ has a low effective rank. The correlation matrix $C$ will have large off-diagonal elements, reflecting the high correlation. In this case, the eigenvalue spectrum of $C$ is highly concentrated. A few dominant eigenvalues $\\lambda_i$ (and thus singular values $\\sigma_i$) will capture most of the system's energy, while the remaining values will be very small. This results in a **rapid decay** of the singular values. The system is considered to be of low intrinsic dimensionality.\n\n2.  **High Snapshot Diversity:** If the snapshots are highly diverse (e.g., they are sampled from different dynamical regimes, across a wide range of parameters, or from a turbulent flow with many active scales), the snapshot vectors will be less correlated and will tend to be more orthogonal to each other in the state space $\\mathbb{R}^N$. Consequently, the off-diagonal elements of the correlation matrix $C = S^{\\top}S$ will be small relative to its diagonal elements ($C_{jj} = \\|s_j\\|^2$). The energy of the ensemble is distributed more evenly among a larger set of underlying structures. An information-theoretic interpretation is that the entropy of the snapshot set is higher. This requires more basis functions to capture a given percentage of the total energy. The eigenvalues $\\lambda_i$ of $C$ will be more evenly distributed, leading to a **slow decay** of the singular values $\\sigma_i$. This indicates that the system is of high intrinsic dimensionality.\n\nIn summary, from first principles, the diversity of the snapshot set governs the structure of the snapshot correlation matrix $C$. Higher diversity leads to a more \"spread out\" set of snapshot vectors, making $C$ more diagonally dominant and its eigenvalue spectrum flatter. Since the eigenvalues of $C$ are the squares of the singular values of $S$, greater snapshot diversity directly corresponds to a slower decay rate of the singular values.",
            "answer": "$$\\boxed{\\sqrt{2}}$$"
        },
        {
            "introduction": "Once we have identified the key outputs to predict, we need a robust regression model to map input parameters to those outputs. Gaussian Processes (GPs) are a cornerstone of modern surrogate modeling, offering not only predictions but also principled uncertainty quantification. This exercise guides you through the implementation of the core GP prediction algorithm, focusing on a numerically stable approach using Cholesky factorization rather than direct matrix inversion . Mastering this technique is essential for building reliable and efficient GP surrogates in practice.",
            "id": "3369162",
            "problem": "Consider a surrogate modeling scenario for an expensive Computational Fluid Dynamics (CFD) flow simulation where a Gaussian Process (GP) prior is used on a scalar quantity of interest. The training outputs are collected in a vector $y \\in \\mathbb{R}^n$ and are modeled as noisy observations of latent function values with independent Gaussian noise of variance $\\sigma^2$. The training covariance matrix is $K \\in \\mathbb{R}^{n \\times n}$, the cross-covariance vector between the training inputs and a single test input $x_*$ is $k_* \\in \\mathbb{R}^n$, and the self-covariance at the test input is $k(x_*,x_*) \\in \\mathbb{R}$. All quantities are dimensionless. Your task is to compute the posterior predictive mean $m_*(x_*)$ and variance $s_*^2(x_*)$ at $x_*$ based on conditioning properties of multivariate Gaussian distributions, using a numerically stable algorithm that does not explicitly invert any matrix.\n\nFundamental base: The posterior distribution for a Gaussian Process under Gaussian noise can be obtained by conditioning a joint multivariate Gaussian distribution. The joint prior over the noisy training observations and the noiseless test function value is Gaussian with mean zero and a block covariance constructed from $K$, $k_*$, and $k(x_*,x_*)$, and with the observation noise entering as $\\sigma^2 I$ added to the training block.\n\nDesign and implementation requirements:\n- Compute $m_*(x_*)$ and $s_*^2(x_*)$ using linear solves and the Cholesky factorization of the symmetric positive definite matrix $K + \\sigma^2 I$. Do not perform explicit matrix inversion. Ensure numerical stability and enforce $s_*^2(x_*) \\ge 0$ by clamping to zero if necessary due to numerical roundoff.\n- Express all angles, if any occur, in radians. In this problem, all quantities are dimensionless and no physical units are required.\n- The final output should aggregate the results of all provided test cases into a single line. Each test case result should be a list of two floats $[m_*, s_*^2]$, and the total output should be a list of these lists, printed as a single Python-style list on one line, for example $[[m_1,s_1],[m_2,s_2]]$.\n\nTest suite:\nProvide numerical values for four test cases that exercise different aspects of the computation:\n\n- Case A (general, well-conditioned):\n  - $K = \\begin{bmatrix}\n  1.000000 & 0.726149 & 0.056135 \\\\\n  0.726149 & 1.000000 & 0.278037 \\\\\n  0.056135 & 0.278037 & 1.000000\n  \\end{bmatrix}$\n  - $k_* = \\begin{bmatrix} 0.278037 \\\\ 0.726149 \\\\ 0.726149 \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 0.5 \\\\ 0.9 \\\\ -0.2 \\end{bmatrix}$\n  - $\\sigma^2 = 0.0001$\n  - $k(x_*,x_*) = 1.000000$\n\n- Case B (boundary, single training point):\n  - $K = \\begin{bmatrix} 1.000000 \\end{bmatrix}$\n  - $k_* = \\begin{bmatrix} 0.600000 \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 1.250000 \\end{bmatrix}$\n  - $\\sigma^2 = 0.00000001$\n  - $k(x_*,x_*) = 1.000000$\n\n- Case C (near-singular $K$ stabilized by noise):\n  - $K = \\begin{bmatrix}\n  1.000000 & 0.9999995 \\\\\n  0.9999995 & 1.000000\n  \\end{bmatrix}$\n  - $k_* = \\begin{bmatrix} 0.9999995 \\\\ 0.9999995 \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 0.200000 \\\\ -0.200000 \\end{bmatrix}$\n  - $\\sigma^2 = 0.000001$\n  - $k(x_*,x_*) = 1.000000$\n\n- Case D (moderate noise and larger training set):\n  - $K = \\begin{bmatrix}\n  1.000000 & 0.800737 & 0.065810 & 0.000003 \\\\\n  0.800737 & 1.000000 & 0.249352 & 0.000084 \\\\\n  0.065810 & 0.249352 & 1.000000 & 0.028700 \\\\\n  0.000003 & 0.000084 & 0.028700 & 1.000000\n  \\end{bmatrix}$\n  - $k_* = \\begin{bmatrix} 0.135335 \\\\ 0.411789 \\\\ 0.945994 \\\\ 0.011109 \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 0.000000 \\\\ 1.000000 \\\\ -0.500000 \\\\ 0.200000 \\end{bmatrix}$\n  - $\\sigma^2 = 0.050000$\n  - $k(x_*,x_*) = 1.000000$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact format:\n[[$m_A, s_A^2$],[$m_B, s_B^2$],[$m_C, s_C^2$],[$m_D, s_D^2$]]\nwhere [$m_X, s_X^2$] corresponds to Case $X \\in \\{A,B,C,D\\}$.",
            "solution": "## Principle-Based Solution\n\nThe foundation of Gaussian Process regression lies in the properties of multivariate Gaussian distributions. We model the joint distribution of the observed noisy training data, $y$, and the latent function value at the test point, $f_* = f(x_*)$, as a multivariate Gaussian.\n\nThe prior on the latent function values is a GP, so the vector of latent values at the training points, $f$, and the latent value at the test point, $f_*$, have a joint Gaussian distribution with zero mean:\n$$\n\\begin{pmatrix} f \\\\ f_* \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{0} \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} K & k_* \\\\ k_*^T & k(x_*,x_*) \\end{pmatrix} \\right)\n$$\nThe observations $y$ are noisy versions of $f$, modeled as $y = f + \\epsilon$, where the noise $\\epsilon$ is independent and identically distributed, $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$. The covariance of the observations is $\\text{Cov}(y) = \\text{Cov}(f + \\epsilon) = \\text{Cov}(f) + \\text{Cov}(\\epsilon) = K + \\sigma^2 I$. The cross-covariance between the observations $y$ and the test value $f_*$ is $\\text{Cov}(y, f_*) = \\text{Cov}(f + \\epsilon, f_*) = \\text{Cov}(f, f_*) = k_*$.\n\nTherefore, the joint distribution of the observed data $y$ and the latent test value $f_*$ is:\n$$\n\\begin{pmatrix} y \\\\ f_* \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{0} \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} K + \\sigma^2 I & k_* \\\\ k_*^T & k(x_*,x_*) \\end{pmatrix} \\right)\n$$\nThe posterior distribution $p(f_* | y)$ is obtained by conditioning this joint Gaussian distribution. The standard formulas for the conditional mean and variance of a partitioned Gaussian distribution yield the posterior predictive mean $m_*(x_*)$ and variance $s_*^2(x_*)$:\n$$\nm_*(x_*) = k_*^T (K + \\sigma^2 I)^{-1} y\n$$\n$$\ns_*^2(x_*) = k(x_*,x_*) - k_*^T (K + \\sigma^2 I)^{-1} k_*\n$$\n\n### Numerically Stable Implementation\nDirect computation of the matrix inverse $(K + \\sigma^2 I)^{-1}$ is numerically unstable and computationally expensive ($O(n^3)$). The problem rightly forbids this. A stable and efficient algorithm based on Cholesky factorization is employed.\n\nLet $A = K + \\sigma^2 I$. Since $A$ is symmetric and positive definite, it has a unique Cholesky factorization $A = L L^T$, where $L$ is a lower-triangular matrix.\n\n**1. Posterior Mean Calculation:**\nThe mean is $m_*(x_*) = k_*^T (A^{-1} y)$. We can define a vector $\\alpha = A^{-1} y$. Then, the mean is simply the dot product $m_*(x_*) = k_*^T \\alpha$. To find $\\alpha$ without inversion, we solve the linear system $A \\alpha = y$:\n$$\nL L^T \\alpha = y\n$$\nThis is solved in two steps using triangular substitution, which is numerically stable and efficient ($O(n^2)$):\n-   First, solve $L z = y$ for $z$ using forward substitution.\n-   Then, solve $L^T \\alpha = z$ for $\\alpha$ using backward substitution.\n\n**2. Posterior Variance Calculation:**\nThe variance is $s_*^2(x_*) = k(x_*,x_*) - k_*^T A^{-1} k_*$. The quadratic form $k_*^T A^{-1} k_*$ can be computed more stably. Using the Cholesky factor $L$:\n$$\nk_*^T A^{-1} k_* = k_*^T (L L^T)^{-1} k_* = k_*^T (L^T)^{-1} L^{-1} k_* = (L^{-1} k_*)^T (L^{-1} k_*)\n$$\nLet a vector $v$ be defined as the solution to the triangular system $L v = k_*$. This can be found via forward substitution. Then, the quadratic form is simply the squared Euclidean norm of $v$:\n$$\nk_*^T A^{-1} k_* = v^T v = \\|v\\|^2_2\n$$\nThe posterior variance is thus:\n$$\ns_*^2(x_*) = k(x_*,x_*) - v^T v\n$$\nThis approach requires only one triangular solve ($O(n^2)$) and avoids a second full linear system solve, in addition to being more numerically stable than forming the product $k_*^T (A^{-1} k_*)$.\n\nFinally, due to potential floating-point round-off errors, the computed variance could be a small negative number. To ensure physical validity, it is clamped to zero:\n$$\ns_*^2(x_*) = \\max(0, s_*^2(x_*))\n$$\n\n### Algorithmic Summary\n1.  Construct the matrix $A = K + \\sigma^2 I$.\n2.  Compute the Cholesky factorization $L = \\text{cholesky}(A)$, where $L$ is lower triangular.\n3.  Calculate the posterior mean $m_*(x_*)$:\n    a. Solve $L z = y$ for $z$ using forward substitution.\n    b. Solve $L^T \\alpha = z$ for $\\alpha$ using backward substitution.\n    c. Compute $m_*(x_*) = k_*^T \\alpha$.\n4.  Calculate the posterior variance $s_*^2(x_*)$:\n    a. Solve $L v = k_*$ for $v$ using forward substitution.\n    b. Compute the variance term as $v^T v$.\n    c. Compute $s_*^2(x_*) = k(x_*,x_*) - v^T v$.\n5.  Enforce non-negativity: $s_*^2(x_*) = \\max(0, s_*^2(x_*))$.\n6.  Return the pair $[m_*(x_*), s_*^2(x_*)]$.\n\nThis algorithm is implemented for each of the four provided test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef compute_posterior_mean_variance(K, k_star, y, sigma2, k_xx):\n    \"\"\"\n    Computes the posterior predictive mean and variance for a Gaussian Process.\n\n    This function uses a numerically stable algorithm based on Cholesky\n    factorization, avoiding explicit matrix inversion.\n\n    Args:\n        K (np.ndarray): The n x n training covariance matrix.\n        k_star (np.ndarray): The n x 1 cross-covariance vector.\n        y (np.ndarray): The n x 1 vector of training outputs.\n        sigma2 (float): The variance of the observation noise.\n        k_xx (float): The self-covariance at the test point.\n\n    Returns:\n        list: A list containing the posterior mean and posterior variance, [m_star, s2_star].\n    \"\"\"\n    # Ensure inputs are numpy arrays of appropriate dimension\n    K = np.atleast_2d(K)\n    k_star = np.atleast_1d(k_star)\n    y = np.atleast_1d(y)\n    n = K.shape[0]\n\n    # Step 1: Construct the matrix A = K + sigma^2 * I\n    A = K + sigma2 * np.eye(n)\n\n    # Step 2: Compute the Cholesky factorization A = L L^T\n    try:\n        L = cholesky(A, lower=True)\n    except np.linalg.LinAlgError:\n        # This should not happen for valid inputs as A is positive definite\n        return [np.nan, np.nan]\n\n    # Step 3: Calculate the posterior mean m_star\n    # Solve L z = y for z\n    z = solve_triangular(L, y, lower=True)\n    # Solve L^T alpha = z for alpha\n    alpha = solve_triangular(L.T, z, lower=False)\n    # Compute mean m_star = k_star^T * alpha\n    m_star = k_star.T @ alpha\n\n    # Step 4: Calculate the posterior variance s2_star\n    # Solve L v = k_star for v\n    v = solve_triangular(L, k_star, lower=True)\n    # Compute variance s2_star = k(x*,x*) - v^T v\n    s2_star = v.T @ v\n\n    # Step 5: Enforce non-negativity of variance\n    s2_star = max(0.0, k_xx - s2_star)\n\n    return [m_star, s2_star]\n\ndef solve():\n    \"\"\"\n    Defines test cases and computes the results, printing them in the required format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: general, well-conditioned\n        {\n            \"K\": np.array([\n                [1.000000, 0.726149, 0.056135],\n                [0.726149, 1.000000, 0.278037],\n                [0.056135, 0.278037, 1.000000]\n            ]),\n            \"k_star\": np.array([0.278037, 0.726149, 0.726149]),\n            \"y\": np.array([0.5, 0.9, -0.2]),\n            \"sigma2\": 0.0001,\n            \"k_xx\": 1.000000\n        },\n        # Case B: boundary, single training point\n        {\n            \"K\": np.array([[1.000000]]),\n            \"k_star\": np.array([0.600000]),\n            \"y\": np.array([1.250000]),\n            \"sigma2\": 0.00000001,\n            \"k_xx\": 1.000000\n        },\n        # Case C: near-singular K stabilized by noise\n        {\n            \"K\": np.array([\n                [1.000000, 0.9999995],\n                [0.9999995, 1.000000]\n            ]),\n            \"k_star\": np.array([0.9999995, 0.9999995]),\n            \"y\": np.array([0.200000, -0.200000]),\n            \"sigma2\": 0.000001,\n            \"k_xx\": 1.000000\n        },\n        # Case D: moderate noise and larger training set\n        {\n            \"K\": np.array([\n                [1.000000, 0.800737, 0.065810, 0.000003],\n                [0.800737, 1.000000, 0.249352, 0.000084],\n                [0.065810, 0.249352, 1.000000, 0.028700],\n                [0.000003, 0.000084, 0.028700, 1.000000]\n            ]),\n            \"k_star\": np.array([0.135335, 0.411789, 0.945994, 0.011109]),\n            \"y\": np.array([0.000000, 1.000000, -0.500000, 0.200000]),\n            \"sigma2\": 0.050000,\n            \"k_xx\": 1.000000\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_posterior_mean_variance(\n            case[\"K\"], case[\"k_star\"], case[\"y\"], case[\"sigma2\"], case[\"k_xx\"]\n        )\n        results.append(result)\n\n    # Format the output string as a list of lists.\n    # The default str() for a list includes spaces, which is a standard\n    # Python-style representation and matches the template structure.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Modern deep learning methods, such as Fourier Neural Operators (FNOs), have shown immense promise for learning the solution operators of entire PDEs, enabling rapid predictions of time-dependent flows. A critical challenge in deploying these models is ensuring long-term stability when they are applied autoregressively over many time steps, a phenomenon known as \"roll-out\" instability. This advanced practice addresses this issue directly, tasking you with implementing a physics-informed stability constraint, analogous to a spectral CFL condition, to prevent unphysical error growth . This exercise offers a practical introduction to the vital research area of physics-informed machine learning.",
            "id": "3369161",
            "problem": "Consider the one-dimensional linear advection equation on a periodic domain, given by $\\partial_t u + c \\partial_x u = 0$, where $u(x,t)$ is a scalar field, $c$ is the constant advection speed, $x \\in [0,L)$, and $t \\ge 0$. The fundamental base is the spatial Fourier transform and the fact that under exact Fourier evolution the mode with wavenumber $k$ advances as $u_k(t+\\Delta t) = \\exp(-\\mathrm{i} k c \\Delta t) u_k(t)$ with unit magnitude for the complex amplification factor. A Fourier surrogate model that attempts to predict the per-mode update may produce complex amplification factors whose magnitude exceeds unity or whose phase increment is inconsistent with advection, leading to roll-out instabilities (norm blow-ups) under iterated application over many steps.\n\nYour task is to design and implement a program that instantiates a simple Fourier neural operator surrogate for the per-mode complex amplification factor and constrains it using a spectral Courant–Friedrichs–Lewy (CFL)-like condition to empirically map stability regions. The program will:\n\n1. Construct a spatial grid with $N$ points over a periodic domain of length $L$, with spacing $\\Delta x = L/N$. Consider dimensionless fields, and treat angles in radians.\n\n2. Define the exact per-mode complex amplification $G_k^{\\mathrm{true}} = \\exp(-\\mathrm{i} k c \\Delta t)$ for integer wavenumbers $k$ consistent with a Fourier grid on $[0,L)$.\n\n3. Define a noisy surrogate prediction per mode, modeled as $G_k^{\\mathrm{pred}} = G_k^{\\mathrm{true}} \\cdot A_k \\cdot \\exp(\\mathrm{i} \\Phi_k)$, where $A_k$ and $\\Phi_k$ represent amplitude and phase prediction errors drawn independently per mode from prescribed distributions with mean and standard deviation parameters. The amplitude perturbation should be modeled such that $A_k \\ge 0$ and centered near unity, while the phase perturbation is centered near zero and specified in radians.\n\n4. Implement two roll-out variants over $T$ steps starting from a random initial field $u(x,0)$:\n   - Unconstrained: Apply $G_k^{\\mathrm{pred}}$ at each step in spectral space to obtain $u(x,t)$.\n   - Constrained: Modify $G_k^{\\mathrm{pred}}$ per mode to enforce a spectral CFL-like condition before roll-out. The constrained amplification $G_k^{\\mathrm{cfl}}$ must satisfy all of the following qualitative requirements:\n     a) Amplitude bound: enforce $|G_k^{\\mathrm{cfl}}| \\le 1$.\n     b) Phase increment bound: enforce $|\\Delta \\phi_k| \\le |k c \\Delta t|$, where $\\Delta \\phi_k$ is the per-step phase for mode $k$ and the bound is expressed in radians.\n     c) Additional damping when the non-dimensional CFL ratio $r = |c| \\Delta t / \\Delta x$ exceeds $1$: introduce a monotone damping factor that increases with both the exceedance $r-1$ and the normalized wavenumber magnitude $|k|/k_{\\max}$, where $k_{\\max}$ is the largest resolved wavenumber magnitude on the grid.\n\n5. Define stability for a roll-out as the discrete $\\ell^2$ norm of $u(x,t)$ remaining bounded relative to the initial norm. Specifically, at each step compute the discrete norm $||u^n||_2$ and declare blow-up if $||u^n||_2 / ||u^0||_2$ exceeds a fixed growth threshold $G_{\\mathrm{thr}}$. A simulation is considered stable if no blow-up occurs within $T$ steps. The discrete norm should be computed in dimensionless units consistent with the grid.\n\n6. To empirically map stability regions, run the above for the specified test suite of parameter values. For each case, record a boolean pair $[\\mathrm{stable\\_unconstrained}, \\mathrm{stable\\_constrained}]$.\n\nUse the following test suite, where all angles are in radians and the advection speed $c$ is in meters per second while $L$ is in meters. Compute $\\Delta t$ from the specified non-dimensional CFL ratio $r$ via $\\Delta t = r \\Delta x / |c|$. Use a periodic domain length $L = 2\\pi$, $N = 64$ grid points, and $T = 100$ steps in all cases. Let $G_{\\mathrm{thr}} = 100$. The amplitude error parameters $(\\mu_a,\\sigma_a)$ define $A_k$ centered near $1+\\mu_a$ with variability $\\sigma_a$, and the phase error standard deviation is $\\sigma_\\phi$.\n\n- Case $1$: $c = 1$, $r = 0.5$, $\\mu_a = -0.02$, $\\sigma_a = 0.02$, $\\sigma_\\phi = 0.05$.\n- Case $2$: $c = 1$, $r = 1.0$, $\\mu_a = 0.05$, $\\sigma_a = 0.05$, $\\sigma_\\phi = 0.10$.\n- Case $3$: $c = 1$, $r = 2.0$, $\\mu_a = 0.10$, $\\sigma_a = 0.05$, $\\sigma_\\phi = 0.15$.\n- Case $4$: $c = 1$, $r = 3.0$, $\\mu_a = 0.20$, $\\sigma_a = 0.10$, $\\sigma_\\phi = 0.20$.\n- Case $5$: $c = 1$, $r = 0.1$, $\\mu_a = 0.0$, $\\sigma_a = 0.0$, $\\sigma_\\phi = 0.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry being a boolean pair for the corresponding test case, in the exact format: [[$b_1^{\\mathrm{uncon}}$,$b_1^{\\mathrm{con}}$],[$b_2^{\\mathrm{uncon}}$,$b_2^{\\mathrm{con}}$],$\\dots$,[$b_5^{\\mathrm{uncon}}$,$b_5^{\\mathrm{con}}$]], where each $b_i$ is either $\\mathrm{True}$ or $\\mathrm{False}$.",
            "solution": "The problem is scientifically grounded in the principles of computational fluid dynamics and numerical analysis, specifically concerning the stability of surrogate models for hyperbolic partial differential equations. The problem is well-posed, objective, and contains sufficient information to construct a unique solution, given reasonable and standard interpretations for the specified qualitative requirements.\n\nThe core task is to investigate the stability of a Fourier neural operator surrogate for the one-dimensional linear advection equation, $\\partial_t u + c \\partial_x u = 0$. The analysis is performed by comparing an unconstrained roll-out of the surrogate with a constrained version that enforces stability criteria analogous to the Courant–Friedrichs–Lewy (CFL) condition in spectral space.\n\nFirst, we establish the computational domain and discretization. The physical domain is a periodic interval $x \\in [0, L)$ with $L=2\\pi$. This domain is discretized into $N=64$ grid points, leading to a grid spacing of $\\Delta x = L/N = 2\\pi/64 = \\pi/32$. The discrete spatial coordinates are $x_j = j \\Delta x$ for $j \\in \\{0, 1, \\dots, N-1\\}$.\n\nThe solution $u(x,t)$ is represented in the Fourier basis. The discrete Fourier transform (DFT) and its inverse are used to switch between physical and spectral representations. The wavenumbers $k$ corresponding to the grid are given by $k = 2\\pi f$, where $f$ are the discrete frequencies from `numpy.fft.fftfreq(N, d=Δx)`. For $L=2\\pi$, these wavenumbers are integers ranging from $-(N/2)$ to $(N/2)-1$. The maximum resolved wavenumber magnitude is the Nyquist wavenumber, $k_{\\max} = \\pi / \\Delta x = N/2$.\n\nThe exact solution to the advection equation evolves in the Fourier domain according to the rule $u_k(t+\\Delta t) = G_k^{\\mathrm{true}} u_k(t)$, where $u_k(t)$ is the Fourier coefficient of mode $k$ at time $t$. The exact complex amplification factor is a pure phase rotation:\n$$\nG_k^{\\mathrm{true}} = \\exp(-\\mathrm{i} k c \\Delta t)\n$$\nwhere $\\mathrm{i} = \\sqrt{-1}$. The magnitude $|G_k^{\\mathrm{true}}| = 1$ for all $k$, which ensures that the norm of the solution is conserved over time. The time step $\\Delta t$ is determined by the non-dimensional CFL ratio $r = |c|\\Delta t / \\Delta x$, yielding $\\Delta t = r \\Delta x / |c|$.\n\nA surrogate model introduces errors. We model the surrogate's predicted amplification factor $G_k^{\\mathrm{pred}}$ as a perturbation of the true one:\n$$\nG_k^{\\mathrm{pred}} = G_k^{\\mathrm{true}} \\cdot A_k \\cdot \\exp(\\mathrm{i} \\Phi_k)\n$$\nHere, $A_k$ is a multiplicative amplitude error and $\\Phi_k$ is an additive phase error. Based on the problem description, we model these errors as random variables drawn from specified distributions:\n-   The amplitude perturbation $A_k$ is modeled by drawing from a normal distribution $\\mathcal{N}(1+\\mu_a, \\sigma_a^2)$ and clipping at zero to ensure non-negativity: $A_k = \\max(0, \\mathcal{N}(1+\\mu_a, \\sigma_a^2))$.\n-   The phase error $\\Phi_k$ is drawn from a zero-mean normal distribution: $\\Phi_k \\sim \\mathcal{N}(0, \\sigma_\\phi^2)$.\n\nA simulation roll-out starts with a random initial condition $u(x,0)$, generated from a standard normal distribution. The stability of the roll-out over $T=100$ steps is determined by monitoring the discrete $\\ell^2$ norm, defined here as the root-mean-square norm $||u^n||_2 = \\sqrt{\\frac{1}{N}\\sum_{j=0}^{N-1} |u_j^n|^2}$. A simulation is declared unstable if the norm amplification exceeds a threshold $G_{\\mathrm{thr}} = 100$, i.e., if $||u^n||_2 / ||u^0||_2 > G_{\\mathrm{thr}}$ for any step $n \\in \\{1, \\dots, T\\}$.\n\nTwo roll-out scenarios are simulated for each test case:\n1.  **Unconstrained Roll-out**: The Fourier coefficients are updated at each step using the noisy prediction $u_k^{n+1} = G_k^{\\mathrm{pred}} u_k^n$. This method is susceptible to instability if any $|G_k^{\\mathrm{pred}}| > 1$ or if phase errors accumulate destructively.\n\n2.  **Constrained Roll-out**: The predicted amplification factor $G_k^{\\mathrm{pred}}$ is modified to a constrained version $G_k^{\\mathrm{cfl}}$ before each update step, $u_k^{n+1} = G_k^{\\mathrm{cfl}} u_k^n$. The design of $G_k^{\\mathrm{cfl}}$ adheres to the specified qualitative requirements:\n    a.  **Amplitude Bound**: The magnitude of the amplification factor must not exceed unity. Let $G_k^{\\mathrm{pred}} = |G_k^{\\mathrm{pred}}| \\exp(\\mathrm{i} \\psi_k^{\\mathrm{pred}})$. We enforce $|G_k^{\\mathrm{cfl}}| \\le 1$ by clipping the predicted magnitude: $|G_k^{\\mathrm{cfl}}|' = \\min(|G_k^{\\mathrm{pred}}|, 1)$.\n    b.  **Phase Increment Bound**: The phase of the amplification factor, $\\psi_k$, must satisfy $|\\psi_k| \\le |k c \\Delta t|$. This is a spectral CFL condition, ensuring that the numerical phase velocity for any mode does not exceed the physical advection speed. The predicted phase is $\\psi_k^{\\mathrm{pred}} = -kc\\Delta t + \\Phi_k$. We enforce the constraint by clipping the phase: $\\psi_k^{\\mathrm{cfl}} = \\text{clip}(\\psi_k^{\\mathrm{pred}}, -|kc\\Delta t|, |kc\\Delta t|)$.\n    c.  **Additional Damping**: For CFL ratios $r > 1$, an additional damping factor is introduced that increases with the CFL exceedance $r-1$ and the normalized wavenumber $|k|/k_{\\max}$. We formulate this as a multiplicative factor $D_k$ applied to the amplitude:\n        $$\n        D_k = \n        \\begin{cases} \n        \\max\\left(0, 1 - (r-1) \\frac{|k|}{k_{\\max}}\\right) & \\text{if } r > 1 \\\\\n        1 & \\text{if } r \\le 1 \n        \\end{cases}\n        $$\n    The final constrained amplification factor is constructed by combining these modifications:\n    $$\n    G_k^{\\mathrm{cfl}} = \\min(|G_k^{\\mathrm{pred}}|, 1) \\cdot D_k \\cdot \\exp(\\mathrm{i} \\cdot \\text{clip}(\\psi_k^{\\mathrm{pred}}, -|kc\\Delta t|, |kc\\Delta t|))\n    $$\nThis design ensures that the constrained roll-out remains stable by construction, preventing both amplitude blow-up and unphysical information propagation speeds. The program will execute these simulations for the provided test suite and report the stability of both the unconstrained and constrained roll-outs for each case. A fixed random seed is used for reproducibility.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of mapping stability regions for a Fourier surrogate model\n    of the 1D linear advection equation.\n    \"\"\"\n\n    # --- Simulation Parameters ---\n    L = 2.0 * np.pi  # Domain length (meters)\n    N = 64           # Number of grid points\n    T = 100          # Number of time steps for roll-out\n    G_thr = 100.0    # Norm growth threshold for instability\n\n    # --- Test Suite ---\n    test_cases = [\n        # c, r, mu_a, sigma_a, sigma_phi\n        (1.0, 0.5, -0.02, 0.02, 0.05),\n        (1.0, 1.0, 0.05, 0.05, 0.10),\n        (1.0, 2.0, 0.10, 0.05, 0.15),\n        (1.0, 3.0, 0.20, 0.10, 0.20),\n        (1.0, 0.1, 0.0, 0.0, 0.0),\n    ]\n\n    # Use a fixed random seed for reproducibility\n    np.random.seed(42)\n\n    # --- Pre-computation for the grid ---\n    dx = L / N\n    k = 2.0 * np.pi * np.fft.fftfreq(N, d=dx)\n    k_max_abs = np.pi / dx # Nyquist wavenumber\n\n    # Generate a single random initial condition for all runs\n    u0 = np.random.randn(N)\n    norm_u0 = np.linalg.norm(u0) / np.sqrt(N)\n    u0_hat = np.fft.fft(u0)\n\n    results = []\n    for case in test_cases:\n        c, r, mu_a, sigma_a, sigma_phi = case\n        \n        if c == 0:\n            # Avoid division by zero, although not in test cases\n            dt = 0\n        else:\n            dt = r * dx / abs(c)\n\n        # --- Generate Surrogate Errors ---\n        # Generate noise once per case for both roll-outs\n        # Amplitude error factor\n        A_k_noise = np.maximum(0, np.random.normal(1.0 + mu_a, sigma_a, N))\n        # Phase error (in radians)\n        Phi_k_noise = np.random.normal(0.0, sigma_phi, N)\n\n        # --- Define Amplification Factors ---\n        G_k_true = np.exp(-1j * k * c * dt)\n        G_k_pred = G_k_true * A_k_noise * np.exp(1j * Phi_k_noise)\n\n        # --- Define Constrained Amplification Factor (G_k_cfl) ---\n        # a) Amplitude bound\n        mag_pred = np.abs(G_k_pred)\n        mag_cfl_1 = np.minimum(mag_pred, 1.0)\n        \n        # b) Phase increment bound\n        phase_pred = np.angle(G_k_pred)\n        phase_bound = np.abs(k * c * dt)\n        phase_cfl = np.clip(phase_pred, -phase_bound, phase_bound)\n        \n        # c) Additional damping for r > 1\n        mag_cfl_2 = mag_cfl_1\n        if r > 1.0:\n            # Damping factor is 0 for k=k_max, 1 for k=0, and linear in between\n            damping_factor = np.maximum(0, 1.0 - (r - 1.0) * np.abs(k) / k_max_abs)\n            mag_cfl_2 = mag_cfl_1 * damping_factor\n        \n        G_k_cfl = mag_cfl_2 * np.exp(1j * phase_cfl)\n\n        # --- Run Simulations ---\n        case_results = []\n        for constrained in [False, True]:\n            u_hat = u0_hat.copy()\n            is_stable = True\n            \n            G_k = G_k_cfl if constrained else G_k_pred\n\n            for _ in range(T):\n                u_hat = G_k * u_hat\n                u_n = np.fft.ifft(u_hat)\n                norm_u_n = np.linalg.norm(u_n) / np.sqrt(N)\n                \n                if norm_u0 > 1e-9 and norm_u_n / norm_u0 > G_thr:\n                    is_stable = False\n                    break\n            \n            case_results.append(is_stable)\n        \n        results.append(case_results)\n\n    # Format the final output string to be exactly as specified\n    # e.g., [[False,True],[False,True],...]\n    result_strings = [str(pair).replace(\" \", \"\") for pair in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}