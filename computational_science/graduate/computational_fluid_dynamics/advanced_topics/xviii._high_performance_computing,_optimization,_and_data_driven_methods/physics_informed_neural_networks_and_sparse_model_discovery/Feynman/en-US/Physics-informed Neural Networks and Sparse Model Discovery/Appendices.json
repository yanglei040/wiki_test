{
    "hands_on_practices": [
        {
            "introduction": "Before deploying complex Physics-Informed Neural Networks (PINNs), especially for demanding CFD problems like the Navier-Stokes equations, it is essential to understand their computational budget. This exercise guides you through estimating the time and memory complexity associated with computing the high-order derivatives required by the physics-based residual, using reverse-over-forward Automatic Differentiation (AD). By deriving these scaling laws, you will gain practical insight into the trade-offs between model size, batch size, and available hardware, a crucial skill for designing and training effective PINNs .",
            "id": "3351988",
            "problem": "A Physics-Informed Neural Network (PINN) is trained to approximate incompressible two-dimensional Navier–Stokes fields, where the viscosity term requires second-order spatial derivatives of the velocity components for the residual of the partial differential equation. Consider a fully connected feedforward network with $L$ affine layers, hidden-layer width $W$, input dimension $d=3$ (spatial variables $x$, $y$, and time $t$), and output dimension $m=3$ (velocity components $u$, $v$, and pressure $p$). The hidden layers all have width $W$, and the output layer maps width $W$ to $m$. The network uses Automatic Differentiation (AD), specifically the reverse-over-forward strategy, to compute diagonal second-order spatial derivatives $\\frac{\\partial^{2} u}{\\partial x^{2}}$, $\\frac{\\partial^{2} u}{\\partial y^{2}}$, $\\frac{\\partial^{2} v}{\\partial x^{2}}$, and $\\frac{\\partial^{2} v}{\\partial y^{2}}$, needed for the viscosity term in the residual. Assume the following:\n\n- The cost of a forward evaluation through one affine layer of width $W$ scales proportionally to the number of multiply-adds in a dense matrix–vector product, which is $W^{2}$ when both input and output widths are $W$. Across $L$ layers, the forward cost scales as $c_{\\mathrm{f}} L W^{2}$ for some constant $c_{\\mathrm{f}}$.\n- For reverse-mode AD computing the gradient of a scalar output, the time complexity is proportional to the forward cost with constant factor $c_{\\mathrm{rev}}$ (which includes the forward tape creation and the backward sweep).\n- For second-order diagonal spatial derivatives via reverse-over-forward AD for a single scalar output, the time complexity is proportional to $d_{s} c_{\\mathrm{rev}} L W^{2}$, where $d_{s}=2$ is the number of spatial input directions ($x$ and $y$). To compute the four diagonal second derivatives for the two velocity outputs $u$ and $v$, scale appropriately by the number of outputs.\n- Activation tapes for reverse mode require storing both preactivations and activations for each hidden layer. Let the per-hidden-layer storage be $c_{\\mathrm{act}} W$ floats per sample. Additional AD workspace for second derivatives requires $c_{\\mathrm{AD}} W$ floats per hidden layer per sample. Per-sample memory thus scales as $(c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W$ floats for hidden layers only. Each float is $4$ bytes (single precision).\n- The total parameter count (weights plus biases) is\n$$\nN_{p} = d W + (L-2)W^{2} + W m + (L-1)W + m,\n$$\nand parameters are kept in memory independent of batch size.\n\nTasks:\n\n1. Derive an asymptotic expression for the total time complexity $T(B,L,W)$, in units proportional to multiply-add operations, to compute the set of diagonal second-order spatial derivatives $\\left\\{\\frac{\\partial^{2} u}{\\partial x^{2}}, \\frac{\\partial^{2} u}{\\partial y^{2}}, \\frac{\\partial^{2} v}{\\partial x^{2}}, \\frac{\\partial^{2} v}{\\partial y^{2}}\\right\\}$ for a batch of size $B$ using reverse-over-forward AD, in terms of $B$, $L$, $W$, $c_{\\mathrm{f}}$, and $c_{\\mathrm{rev}}$.\n2. Derive a leading-order expression for the total memory complexity $M(B,L,W)$ (in bytes) as a function of $B$, $L$, and $W$, including both parameter memory and per-sample AD workspace, in terms of $N_{p}$, $c_{\\mathrm{act}}$, and $c_{\\mathrm{AD}}$.\n3. Evaluate the maximum batch size $B_{\\max}$ that fits into a graphics processing unit (GPU) with total available memory of $8 \\times 2^{30}$ bytes, assuming $L=12$, $W=1024$, $d=3$, $m=3$, $c_{\\mathrm{f}}=1$, $c_{\\mathrm{rev}}=2$, $c_{\\mathrm{act}}=2$, $c_{\\mathrm{AD}}=4$, single-precision floats ($4$ bytes each), and that optimizer state and gradient checkpointing are disabled. Ignore any additional framework overhead beyond what is specified. Express your final answer for $B_{\\max}$ as an integer. Also discuss, qualitatively, the implications of these complexity estimates for the choice of batch size in PINN training when second-order derivatives are required.",
            "solution": "The problem asks for an analysis of the computational and memory complexity of a Physics-Informed Neural Network (PINN) used for incompressible Navier–Stokes equations, focusing on the calculation of second-order spatial derivatives via reverse-over-forward Automatic Differentiation (AD). The problem is well-posed, scientifically grounded, and contains sufficient information for a unique solution.\n\nThe analysis is divided into three tasks:\n1. Derivation of the total time complexity $T(B,L,W)$ for computing the required second derivatives for a batch of size $B$.\n2. Derivation of the total memory complexity $M(B,L,W)$ including network parameters and AD workspace.\n3. Calculation of the maximum batch size $B_{\\max}$ for a given memory budget and a qualitative discussion of the implications.\n\n**Task 1: Time Complexity $T(B,L,W)$**\n\nThe goal is to find the total time complexity to compute the set of four diagonal second-order spatial derivatives: $\\left\\{\\frac{\\partial^{2} u}{\\partial x^{2}}, \\frac{\\partial^{2} u}{\\partial y^{2}}, \\frac{\\partial^{2} v}{\\partial x^{2}}, \\frac{\\partial^{2} v}{\\partial y^{2}}\\right\\}$.\n\nThe problem statement provides the following scaling law for the computation: The time complexity for computing the set of diagonal second derivatives for a single scalar output with respect to $d_s$ spatial input directions, using reverse-over-forward AD, is proportional to $d_{s} c_{\\mathrm{rev}} L W^{2}$ for a single sample.\n\nIn this problem:\n- The network outputs are $(u, v, p)$. We are interested in the velocity components $u$ and $v$.\n- The spatial input dimensions are $x$ and $y$, so the number of spatial directions is $d_s=2$.\n\nFirst, consider the derivatives for the velocity component $u$. We need to compute $\\frac{\\partial^{2} u}{\\partial x^{2}}$ and $\\frac{\\partial^{2} u}{\\partial y^{2}}$. This corresponds to one scalar output, $u$, and $d_s=2$ spatial directions. According to the provided scaling law, the time complexity for computing this set of derivatives for a single sample is:\n$$ T_{u, \\text{sample}} = d_s c_{\\mathrm{rev}} L W^{2} = 2 c_{\\mathrm{rev}} L W^{2} $$\n\nNext, consider the derivatives for the velocity component $v$. We need to compute $\\frac{\\partial^{2} v}{\\partial x^{2}}$ and $\\frac{\\partial^{2} v}{\\partial y^{2}}$. This again corresponds to one scalar output, $v$, and $d_s=2$ spatial directions. The computation for $v$ is independent of that for $u$. Therefore, the time complexity is the same:\n$$ T_{v, \\text{sample}} = d_s c_{\\mathrm{rev}} L W^{2} = 2 c_{\\mathrm{rev}} L W^{2} $$\n\nThe total time complexity to compute all four required derivatives for a single sample is the sum of the complexities for $u$ and $v$:\n$$ T_{\\text{sample}} = T_{u, \\text{sample}} + T_{v, \\text{sample}} = 2 c_{\\mathrm{rev}} L W^{2} + 2 c_{\\mathrm{rev}} L W^{2} = 4 c_{\\mathrm{rev}} L W^{2} $$\nThis aligns with the instruction to \"scale appropriately by the number of outputs,\" which is $2$ in this case ($u$ and $v$).\n\nThis complexity is for a single input sample. For a batch of size $B$, assuming each sample is processed independently, the total time complexity $T(B,L,W)$ is the per-sample complexity multiplied by the batch size $B$:\n$$ T(B,L,W) = B \\cdot T_{\\text{sample}} = 4 B c_{\\mathrm{rev}} L W^{2} $$\n\n**Task 2: Memory Complexity $M(B,L,W)$**\n\nThe total memory complexity $M(B,L,W)$ is the sum of two components: the memory for storing the network parameters, $M_{\\text{params}}$, and the memory for the per-sample computational workspace (e.g., AD tapes), $M_{\\text{batch}}$.\n\n1.  **Parameter Memory ($M_{\\text{params}}$):** This memory is required to store the weights and biases of the neural network. Its size is independent of the batch size $B$. The problem provides the formula for the total number of parameters, $N_{p}$:\n    $$ N_{p} = d W + (L-2)W^{2} + W m + (L-1)W + m $$\n    Each parameter is stored as a single-precision float, which occupies $4$ bytes. Therefore, the parameter memory in bytes is:\n    $$ M_{\\text{params}} = 4 N_{p} $$\n\n2.  **Per-Sample AD Workspace Memory ($M_{\\text{batch}}$):** This memory scales linearly with the batch size $B$. The problem states that the per-sample memory for hidden layers, which includes activation tapes and AD workspace for second derivatives, is $(c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W$ floats.\n    For a single sample, the memory in bytes is $4 (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W$.\n    For a batch of size $B$, the total workspace memory is:\n    $$ M_{\\text{batch}} = 4 B (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W $$\n\nThe total memory complexity $M(B,L,W)$ is the sum of these two components:\n$$ M(B,L,W) = M_{\\text{params}} + M_{\\text{batch}} = 4 N_{p} + 4 B (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W $$\nThis expression represents the leading-order memory complexity as specified, incorporating the provided terms for parameter count and batch-dependent workspace.\n\n**Task 3: Maximum Batch Size $B_{\\max}$ and Discussion**\n\nThe maximum batch size $B_{\\max}$ is limited by the total available GPU memory, $M_{\\text{GPU}}$. We must solve for $B$ in the inequality $M(B,L,W) \\le M_{\\text{GPU}}$.\n$$ 4 N_{p} + 4 B (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W \\le M_{\\text{GPU}} $$\nSolving for $B$:\n$$ 4 B (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W \\le M_{\\text{GPU}} - 4 N_{p} $$\n$$ B \\le \\frac{M_{\\text{GPU}} - 4 N_{p}}{4 (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W} $$\nThe maximum batch size $B_{\\max}$ is the largest integer satisfying this inequality:\n$$ B_{\\max} = \\left\\lfloor \\frac{M_{\\text{GPU}} - 4 N_{p}}{4 (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W} \\right\\rfloor $$\nWe are given the following values:\n- $L=12$\n- $W=1024$\n- $d=3$\n- $m=3$\n- $c_{\\mathrm{act}}=2$\n- $c_{\\mathrm{AD}}=4$\n- $M_{\\text{GPU}} = 8 \\times 2^{30}$ bytes $= 8589934592$ bytes\n\nFirst, we calculate the number of parameters $N_{p}$:\n$$ N_{p} = (3)(1024) + (12-2)(1024)^{2} + (1024)(3) + (12-1)(1024) + 3 $$\n$$ N_{p} = 3072 + 10(1048576) + 3072 + 11(1024) + 3 $$\n$$ N_{p} = 3072 + 10485760 + 3072 + 11264 + 3 $$\n$$ N_{p} = 10503171 $$\nThe parameter memory is:\n$$ M_{\\text{params}} = 4 N_{p} = 4 \\times 10503171 = 42012684 \\text{ bytes} $$\nNext, we calculate the per-sample memory consumption, which is the denominator of the fraction for $B$:\n$$ M_{\\text{per\\_sample}} = 4 (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W = 4 (2 + 4)(12-1)(1024) $$\n$$ M_{\\text{per\\_sample}} = 4(6)(11)(1024) = 264 \\times 1024 = 270336 \\text{ bytes/sample} $$\nNow we can find the maximum batch size $B_{\\max}$:\n$$ B_{\\max} = \\left\\lfloor \\frac{8589934592 - 42012684}{270336} \\right\\rfloor $$\n$$ B_{\\max} = \\left\\lfloor \\frac{8547921908}{270336} \\right\\rfloor $$\n$$ B_{\\max} = \\lfloor 31619.46... \\rfloor = 31619 $$\n\n**Qualitative Discussion:**\nThese complexity estimates have significant implications for training PINNs that require second-order derivatives.\nThe total memory $M(B,L,W)$ is a sum of a batch-independent term (parameters, $M_{\\text{params}} \\propto L W^{2}$) and a batch-dependent term (AD workspace, $M_{\\text{batch}} \\propto B L W$).\nFor large networks (large $L$ and $W$), the parameter memory is already substantial. However, the critical constraint often comes from the per-sample AD workspace. The need to compute second-order derivatives requires storing a larger computation graph or additional intermediate products, which is reflected in the $c_{\\mathrm{AD}}$ term. In our example, the per-sample memory footprint is over $270$ kilobytes.\nThis large per-sample memory cost creates a severe trade-off. To fit a large, expressive model (high $L, W$) into a fixed GPU memory, the batch size $B$ must be reduced. This is evident from the formula for $B_{\\max}$, where the large per-sample memory term $4 (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W$ is in the denominator.\nUsing a smaller batch size can have negative consequences for training, such as introducing higher variance into the stochastic gradient estimates. This may slow down convergence and require more careful tuning of the optimizer, for instance, by using smaller learning rates. While larger batches are generally preferred for stable training and maximizing parallel computation on GPUs, the memory overhead from second-order AD in PINNs forces a compromise, pushing practitioners towards smaller batches than otherwise optimal. This motivates the development of memory-efficient AD techniques, such as activation checkpointing (which was explicitly excluded in this problem), where activations are recomputed during the backward pass to save memory at the cost of additional computation.",
            "answer": "$$\n\\boxed{\n\\pmatrix{\n4 B c_{\\mathrm{rev}} L W^{2} & 4 N_{p} + 4 B (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1)W & 31619\n}\n}\n$$"
        },
        {
            "introduction": "One of the most powerful applications of this chapter's techniques is the discovery of governing equations directly from experimental or numerical data. In this practice, you will implement a complete sparse model discovery pipeline to identify the structure of the pressure Poisson equation, a cornerstone of incompressible fluid dynamics, from a noisy velocity field. You will learn to construct a library of candidate physical terms using finite differences, manage noisy data, and apply a sparse regression algorithm to determine which terms are physically relevant, providing a foundational workflow for data-driven modeling .",
            "id": "3351987",
            "problem": "You are given a dimensionless, two-dimensional incompressible flow on a square domain and asked to discover the structure of the pressure Poisson equation using physics-informed sparse regression. The goal is to identify the coefficient multiplying the interior convective source term and to determine whether a boundary source term proportional to a wall-normal gradient is present. Your program must be a complete, runnable program that constructs synthetic data, computes feature libraries from noisy velocity fields, performs sequentially thresholded least squares sparse regression, and reports the discovered coefficients for each provided test case.\n\nStart from the following fundamental base: the incompressible continuity equation $\\nabla\\cdot\\mathbf{u}=0$ and the incompressible momentum equation for unit density (dimensionless variables) $\n\\partial_t \\mathbf{u} + \\mathbf{u}\\cdot\\nabla \\mathbf{u} = -\\nabla p + \\nu \\nabla^2 \\mathbf{u}\n$, where $\\mathbf{u}=(u,v)$ is the velocity field and $p$ is the pressure. The pressure field $p$ is defined up to a constant, and the pressure Poisson equation relates $\\nabla^2 p$ to velocity gradients. Boundary conditions at walls relate the normal derivative of pressure to kinematic quantities. You must not assume any analytic form for the target model coefficients; instead, you must recover them by sparse regression from data constructed solely from the velocity field and its gradients.\n\nYour program must:\n- Construct a uniform Cartesian grid on the unit square $[0,1]\\times[0,1]$ with $N_x=N_y=N$ points for each test case. Use $x_i=i\\,\\Delta x$ and $y_j=j\\,\\Delta y$ with $\\Delta x=\\Delta y=1/(N-1)$ for $i\\in\\{0,\\dots,N-1\\}$ and $j\\in\\{0,\\dots,N-1\\}$.\n- Generate an incompressible velocity field $\\mathbf{u}(x,y)$ by differentiating a stream function $\\psi(x,y)$, where $\\psi(x,y)=\\sin(\\pi x)\\sin(\\pi y)$ and $\\mathbf{u}=(\\partial_y\\psi,-\\partial_x\\psi)$. In test cases where a zero flow is specified, set $u=v=0$ identically.\n- Corrupt the velocity components with independent zero-mean Gaussian noise of standard deviation $\\sigma$ (dimensionless), as specified per test case.\n- Build a feature library from the noisy velocity field using second-order accurate finite differences in the interior and first-order one-sided finite differences at the boundaries:\n  1. The interior convective source feature $f_{\\mathrm{conv}}(x,y)$ given by the discrete approximation of $-\\nabla\\cdot\\left(\\mathbf{u}\\cdot\\nabla \\mathbf{u}\\right)$, computed by first forming the convective acceleration components $a_x=u\\,\\partial_x u + v\\,\\partial_y u$ and $a_y=u\\,\\partial_x v + v\\,\\partial_y v$, and then taking the discrete divergence $-\\left(\\partial_x a_x + \\partial_y a_y\\right)$.\n  2. A boundary source candidate feature $f_{\\mathrm{bnd}}(x,y)$ constructed from wall-normal gradients of wall-normal velocity at grid points adjacent to the domain boundary: for bottom and top walls, use one-sided differences of $v$ in the $y$-direction at $j=0$ and $j=N-1$ to define contributions at $j=1$ and $j=N-2$, respectively; for left and right walls, use one-sided differences of $u$ in the $x$-direction at $i=0$ and $i=N-1$ to define contributions at $i=1$ and $i=N-2$, respectively. Sum the contributions for points adjacent to each wall to obtain $f_{\\mathrm{bnd}}(x,y)$, and set $f_{\\mathrm{bnd}}(x,y)=0$ for all other grid points.\n- Construct a synthetic observed target field $z(x,y)$ for each test case by linearly combining the features as $z=f_{\\mathrm{conv}}+\\alpha\\,f_{\\mathrm{bnd}}$, where $\\alpha$ is a scalar specified per test case. This emulates a scenario in which the pressure Poisson equation interior source is determined by the convective acceleration divergence, and an additional boundary-localized source is present with unknown strength.\n- Perform sparse linear regression to fit $z\\approx c_1\\,f_{\\mathrm{conv}}+c_2\\,f_{\\mathrm{bnd}}$ using sequentially thresholded least squares: compute the least squares solution for $(c_1,c_2)$ from all grid points, set any coefficient with magnitude below the threshold $\\lambda$ to zero, and refit using only the remaining nonzero coefficients. If both coefficients are thresholded to zero, return $(0,0)$.\n\nAll computations are dimensionless, and no physical units are to be reported. Angles, where they appear inside trigonometric functions, are in radians.\n\nTest Suite:\n- Case $1$ (general case): $N=64$, $\\sigma=0.01$, $\\alpha=0.0$, $\\lambda=0.05$, use the streamfunction-generated flow.\n- Case $2$ (boundary-influenced case): $N=64$, $\\sigma=0.02$, $\\alpha=0.4$, $\\lambda=0.05$, use the streamfunction-generated flow.\n- Case $3$ (edge case, zero flow): $N=32$, $\\sigma=0.0$, $\\alpha=1.0$, $\\lambda=0.01$, set $u=v=0$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the discovered regression coefficients for each test case as a comma-separated list of lists, with each inner list in the order $[c_1,c_2]$. For example, for three test cases, print in the exact format: [[c1_case1,c2_case1],[c1_case2,c2_case2],[c1_case3,c2_case3]].",
            "solution": "The problem statement has been critically validated and is deemed to be scientifically grounded, well-posed, and self-contained. It presents a clear and solvable computational task in the domain of physics-informed model discovery for fluid dynamics. All necessary data, definitions, and procedures are provided, and there are no internal contradictions or violations of fundamental principles. We may, therefore, proceed with the solution.\n\nThe core of the problem is to reverse-engineer the structure of a partial differential equation from noisy data. Specifically, we aim to discover the coefficients $c_1$ and $c_2$ in the posited pressure Poisson-like equation\n$$\n\\nabla^2 p \\approx c_1 f_{\\mathrm{conv}} + c_2 f_{\\mathrm{bnd}}\n$$\nwhere the left-hand side is represented by a synthetic \"observed\" target field $z$, and the right-hand side consists of candidate feature terms constructed from a noisy velocity field $\\mathbf{u}$. The true coefficients are implicitly defined by the construction of $z = f_{\\mathrm{conv}} + \\alpha f_{\\mathrm{bnd}}$, meaning the target values are $(c_1, c_2) = (1, \\alpha)$.\n\nThe solution is implemented in the following steps:\n1.  **Grid and Velocity Field Generation**: For each test case, we construct a uniform Cartesian grid on the domain $[0, 1] \\times [0, 1]$ with $N \\times N$ points. The grid spacing is $\\Delta x = \\Delta y = 1/(N-1)$. An incompressible velocity field $\\mathbf{u}=(u,v)$ is generated from the stream function $\\psi(x,y) = \\sin(\\pi x)\\sin(\\pi y)$, which yields $u = \\partial_y \\psi = \\pi \\sin(\\pi x)\\cos(\\pi y)$ and $v = -\\partial_x \\psi = -\\pi \\cos(\\pi x)\\sin(\\pi y)$. This field is analytically divergence-free ($\\nabla \\cdot \\mathbf{u} = 0$). For the zero-flow test case, we set $u=v=0$. The velocity components are then corrupted by adding independent, zero-mean Gaussian noise with a specified standard deviation $\\sigma$.\n\n2.  **Numerical Differentiation**: A crucial component is the computation of spatial derivatives from the discrete, noisy velocity field. As specified, we use second-order central differences for interior grid points and first-order one-sided differences (forward/backward) for boundary points. For a generic field $g(x_i, y_j)$, the derivatives are approximated as:\n$$\n\\left(\\frac{\\partial g}{\\partial x}\\right)_{i,j} =\n\\begin{cases}\n(g_{i+1,j} - g_{i,j}) / \\Delta x & \\text{if } i=0 \\\\\n(g_{i+1,j} - g_{i-1,j}) / (2\\Delta x) & \\text{if } 0 < i < N-1 \\\\\n(g_{i,j} - g_{i-1,j}) / \\Delta x & \\text{if } i=N-1\n\\end{cases}\n$$\nAnalogous formulas are used for the $y$-derivative, $\\partial g / \\partial y$.\n\n3.  **Feature Library Construction**: We construct two feature fields from the noisy velocity data, $\\mathbf{u}_{\\mathrm{noisy}}$.\n    -   **Interior Convective Feature $f_{\\mathrm{conv}}$**: This feature represents the divergence of the convective acceleration, which is the source term in the standard pressure Poisson equation. First, we compute the components of the acceleration vector $\\mathbf{a} = \\mathbf{u}_{\\mathrm{noisy}} \\cdot \\nabla \\mathbf{u}_{\\mathrm{noisy}}$:\n        $$\n        a_x = u_{\\mathrm{noisy}} \\frac{\\partial u_{\\mathrm{noisy}}}{\\partial x} + v_{\\mathrm{noisy}} \\frac{\\partial u_{\\mathrm{noisy}}}{\\partial y}\n        $$\n        $$\n        a_y = u_{\\mathrm{noisy}} \\frac{\\partial v_{\\mathrm{noisy}}}{\\partial x} + v_{\\mathrm{noisy}} \\frac{\\partial v_{\\mathrm{noisy}}}{\\partial y}\n        $$\n        The feature is then the negative divergence of this acceleration field:\n        $$\n        f_{\\mathrm{conv}} = - \\nabla \\cdot \\mathbf{a} = -\\left( \\frac{\\partial a_x}{\\partial x} + \\frac{\\partial a_y}{\\partial y} \\right)\n        $$\n        All derivatives are computed using the mixed-order finite difference scheme described above.\n\n    -   **Boundary Source Feature $f_{\\mathrm{bnd}}$**: This is a candidate feature designed to capture potential physics localized at the domain boundaries. It is defined only at grid points adjacent to the walls and is zero elsewhere. Its value is constructed from the wall-normal gradients of the wall-normal velocity component. For instance, at points $(x_i, y_1)$ adjacent to the bottom wall ($y=0$), the feature value is based on the normal gradient $(\\partial v / \\partial y)|_{y=0}$, approximated by a one-sided difference. The full definition is a sum of contributions from any adjacent walls:\n        -   At $j=1$ (adjacent to bottom wall): $f_{\\mathrm{bnd}}[i, 1] \\mathrel{+}= (v_{\\mathrm{noisy}}[i, 1] - v_{\\mathrm{noisy}}[i, 0]) / \\Delta y$.\n        -   At $j=N-2$ (adjacent to top wall): $f_{\\mathrm{bnd}}[i, N-2] \\mathrel{+}= (v_{\\mathrm{noisy}}[i, N-1] - v_{\\mathrm{noisy}}[i, N-2]) / \\Delta y$.\n        -   At $i=1$ (adjacent to left wall): $f_{\\mathrm{bnd}}[1, j] \\mathrel{+}= (u_{\\mathrm{noisy}}[1, j] - u_{\\mathrm{noisy}}[0, j]) / \\Delta x$.\n        -   At $i=N-2$ (adjacent to right wall): $f_{\\mathrm{bnd}}[N-2, j] \\mathrel{+}= (u_{\\mathrm{noisy}}[N-1, j] - u_{\\mathrm{noisy}}[N-2, j]) / \\Delta x$.\n        The use of `+=` correctly handles corner points adjacent to two walls.\n\n4.  **Sparse Regression**: The core of the model discovery is sequentially thresholded least squares (STLS).\n    -   **Synthetic Target**: First, we create the synthetic \"measured\" data, $z$, using the true coefficients $(1, \\alpha)$ provided for each test case: $z = f_{\\mathrm{conv}} + \\alpha f_{\\mathrm{bnd}}$.\n    -   **Linear System**: We formulate a linear system of equations, $\\mathbf{A}\\mathbf{c} \\approx \\mathbf{b}$, where $\\mathbf{A}$ is the feature matrix whose columns are the flattened arrays of $f_{\\mathrm{conv}}$ and $f_{\\mathrm{bnd}}$, $\\mathbf{b}$ is the flattened target array $z$, and $\\mathbf{c} = [c_1, c_2]^T$ is the vector of unknown coefficients we seek.\n    -   **STLS Algorithm**:\n        1.  Solve the least-squares problem for an initial estimate of $\\mathbf{c}$. This is done using `np.linalg.lstsq`.\n        2.  Identify coefficients whose magnitudes are below the specified threshold $\\lambda$ (i.e., $|c_i| < \\lambda$). These are considered negligible and are marked for removal.\n        3.  If any coefficients were marked, form a new, sparser feature matrix $\\mathbf{A}_{\\mathrm{sparse}}$ by removing the corresponding columns. Solve the least-squares problem again for this reduced system to get the refined coefficients $\\mathbf{c}_{\\mathrm{sparse}}$. The final coefficient vector is formed by placing the refined values back in their original positions and setting the thresholded coefficients to $0$. If all coefficients are thresholded, the result is $[0, 0]$. If no coefficients are thresholded, the initial least-squares solution is the final solution.\n\nThis procedure, when applied to each test case, allows us to recover the most plausible model structure given the data and the candidate library. For Case $1$ ($\\alpha=0$), we expect to recover $(c_1, c_2) \\approx (1, 0)$. For Case $2$ ($\\alpha=0.4$), we expect $(c_1, c_2) \\approx (1, 0.4)$. For Case $3$ (zero flow), all features and the target are identically zero, leading to the trivial solution $(0, 0)$. The code implements this full pipeline.",
            "answer": "```python\nimport numpy as np\n\ndef gradient_2d(f, dx, dy):\n    \"\"\"\n    Computes the gradient of a 2D field using second-order central differences\n    in the interior and first-order one-sided differences at the boundaries.\n    \"\"\"\n    N = f.shape[0]\n    dfdx = np.zeros_like(f)\n    dfdy = np.zeros_like(f)\n\n    # Compute df/dx\n    dfdx[:, 1:-1] = (f[:, 2:] - f[:, :-2]) / (2 * dx)\n    dfdx[:, 0] = (f[:, 1] - f[:, 0]) / dx\n    dfdx[:, -1] = (f[:, -1] - f[:, -2]) / dx\n\n    # Compute df/dy\n    dfdy[1:-1, :] = (f[2:, :] - f[:-2, :]) / (2 * dy)\n    dfdy[0, :] = (f[1, :] - f[0, :]) / dy\n    dfdy[-1, :] = (f[-1, :] - f[-2, :]) / dy\n    \n    return dfdx, dfdy\n\ndef solve():\n    \"\"\"\n    Main function to run the sparse model discovery for each test case.\n    \"\"\"\n    np.random.seed(0) # for reproducibility\n\n    test_cases = [\n        {'N': 64, 'sigma': 0.01, 'alpha': 0.0, 'lambda': 0.05, 'flow_type': 'stream_func'},\n        {'N': 64, 'sigma': 0.02, 'alpha': 0.4, 'lambda': 0.05, 'flow_type': 'stream_func'},\n        {'N': 32, 'sigma': 0.0, 'alpha': 1.0, 'lambda': 0.01, 'flow_type': 'zero_flow'}\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        N = case['N']\n        sigma = case['sigma']\n        alpha = case['alpha']\n        lambda_thresh = case['lambda']\n        flow_type = case['flow_type']\n\n        # 1. Grid and Velocity Field Generation\n        dx = 1.0 / (N - 1)\n        dy = 1.0 / (N - 1)\n        x = np.linspace(0, 1, N)\n        y = np.linspace(0, 1, N)\n        xx, yy = np.meshgrid(x, y, indexing='ij')\n\n        u, v = np.zeros((N, N)), np.zeros((N, N))\n        if flow_type == 'stream_func':\n            u = np.pi * np.sin(np.pi * xx) * np.cos(np.pi * yy)\n            v = -np.pi * np.cos(np.pi * xx) * np.sin(np.pi * yy)\n\n        u_noisy = u + np.random.normal(0, sigma, (N, N))\n        v_noisy = v + np.random.normal(0, sigma, (N, N))\n        \n        # 2. Feature Library Construction\n        du_dx, du_dy = gradient_2d(u_noisy, dx, dy)\n        dv_dx, dv_dy = gradient_2d(v_noisy, dx, dy)\n\n        # Feature 1: f_conv\n        ax = u_noisy * du_dx + v_noisy * du_dy\n        ay = u_noisy * dv_dx + v_noisy * dv_dy\n        dax_dx, _ = gradient_2d(ax, dx, dy)\n        _, day_dy = gradient_2d(ay, dx, dy)\n        f_conv = -(dax_dx + day_dy)\n        \n        # Feature 2: f_bnd\n        f_bnd = np.zeros((N, N))\n        \n        # Bottom wall (y=0), affects j=1\n        grad_v_y_bottom = (v_noisy[1, :] - v_noisy[0, :]) / dy\n        f_bnd[1, :] += grad_v_y_bottom\n        \n        # Top wall (y=1), affects j=N-2\n        grad_v_y_top = (v_noisy[N - 1, :] - v_noisy[N - 2, :]) / dy\n        f_bnd[N - 2, :] += grad_v_y_top\n        \n        # Left wall (x=0), affects i=1\n        grad_u_x_left = (u_noisy[:, 1] - u_noisy[:, 0]) / dx\n        f_bnd[:, 1] += grad_u_x_left\n        \n        # Right wall (x=1), affects i=N-2\n        grad_u_x_right = (u_noisy[:, N - 1] - u_noisy[:, N - 2]) / dx\n        f_bnd[:, N - 2] += grad_u_x_right\n        \n        # 3. Synthetic Target Field\n        z = f_conv + alpha * f_bnd\n\n        # 4. Sparse Regression (STLS)\n        z_vec = z.flatten()\n        f_conv_vec = f_conv.flatten()\n        f_bnd_vec = f_bnd.flatten()\n        \n        A = np.stack([f_conv_vec, f_bnd_vec], axis=1)\n        \n        # Step 1: Full least squares\n        try:\n          c_initial, _, _, _ = np.linalg.lstsq(A, z_vec, rcond=None)\n        except np.linalg.LinAlgError:\n          c_initial = np.array([0.0, 0.0])\n\n        if np.all(np.isclose(A.T @ A, 0)): # Handle zero matrix for zero flow case\n            c_initial = np.array([0.0, 0.0])\n\n        # Step 2: Thresholding\n        active_indices = [i for i, c in enumerate(c_initial) if abs(c) >= lambda_thresh]\n        \n        # Step 3: Refit\n        c_final = np.zeros(2)\n        if len(active_indices) > 0:\n            A_sparse = A[:, active_indices]\n            try:\n              c_sparse, _, _, _ = np.linalg.lstsq(A_sparse, z_vec, rcond=None)\n            except np.linalg.LinAlgError:\n               c_sparse = np.zeros(len(active_indices))\n\n            for i, idx in enumerate(active_indices):\n                c_final[idx] = c_sparse[i]\n        \n        final_results.append([c_final[0], c_final[1]])\n        \n    # Format and print the final output\n    result_strings = [f\"[{res[0]},{res[1]}]\" for res in final_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While powerful, PINNs are not without their challenges, and training can become notoriously difficult for multi-scale or stiff problems, which are common in CFD. This exercise explores a critical failure mode by analyzing the one-dimensional viscous Burgers' equation, a prototypical convection-diffusion problem, as viscosity $\\nu$ decreases. You will quantify how the balance between different physical terms in the loss function's gradient breaks down as the flow becomes advection-dominated. By computing metrics such as gradient imbalance and the Jacobian condition number, you will develop the practical ability to diagnose and predict training pathologies, a vital skill for robustly applying PINNs to complex fluid dynamics simulations .",
            "id": "3352001",
            "problem": "You are asked to analyze the sensitivity and potential training failure of Physics-Informed Neural Networks (PINNs) for the one-dimensional viscous Burgers equation. Your analysis must be implemented as a complete, runnable program that computes the residual magnitudes and gradient norms for a fixed parametric trial field and predicts failure due to gradient pathologies as viscosity decreases. The underlying governing law is the one-dimensional viscous Burgers equation, a prototypical model for convection-diffusion in Computational Fluid Dynamics (CFD), written as the balance law of nonlinear advection and viscous diffusion: $$u_{t} + u\\,u_{x} - \\nu\\,u_{xx} = 0,$$ where $u=u(x,t)$, $x$ is position, $t$ is time, and $\\nu$ is the kinematic viscosity. Your numerical computations will be conducted nondimensionally, so no physical units are required. The test suite consists of a set of viscosity values $\\nu$ that span from moderate to vanishingly small.\n\nYour program must proceed from the following fundamental base and definitions. Consider a fixed parametric trial field that mimics a small neural network with linear output layer and fixed basis features, $$u(x,t;\\boldsymbol{\\theta}) = \\sum_{m=1}^{3}\\theta_{m}\\,\\phi_{m}(x,t),$$ where the basis functions are $$\\phi_{1}(x,t)=\\sin(\\pi x)\\,e^{-t},\\quad \\phi_{2}(x,t)=\\sin(2\\pi x)\\,e^{-t},\\quad \\phi_{3}(x,t)=\\cos(\\pi x)\\,e^{-2t},$$ and the parameter vector is fixed as $$\\boldsymbol{\\theta}=\\begin{bmatrix}\\theta_{1}\\\\\\theta_{2}\\\\\\theta_{3}\\end{bmatrix}=\\begin{bmatrix}1.0\\\\-0.5\\\\0.75\\end{bmatrix}.$$ From the fundamental definitions of derivatives, compute $$u_{t}=\\sum_{m=1}^{3}\\theta_{m}\\,\\partial_{t}\\phi_{m},\\quad u_{x}=\\sum_{m=1}^{3}\\theta_{m}\\,\\partial_{x}\\phi_{m},\\quad u_{xx}=\\sum_{m=1}^{3}\\theta_{m}\\,\\partial_{xx}\\phi_{m},$$ and define the pointwise partial differential equation residual $$\\mathcal{R}(x,t;\\nu,\\boldsymbol{\\theta}) = u_{t} + u\\,u_{x} - \\nu\\,u_{xx}.$$ Define the mean-squared residual over a rectangular spatio-temporal collocation grid as $$\\mathcal{L}(\\nu,\\boldsymbol{\\theta}) = \\frac{1}{N}\\sum_{i=1}^{N}\\mathcal{R}(x_{i},t_{i};\\nu,\\boldsymbol{\\theta})^{2},$$ where $(x_{i},t_{i})$ are the collocation points and $N$ is the total number of points. The gradient of the loss with respect to the parameters, by the chain rule, is $$\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\nu,\\boldsymbol{\\theta}) = \\frac{2}{N}\\sum_{i=1}^{N}\\mathcal{R}(x_{i},t_{i};\\nu,\\boldsymbol{\\theta})\\,\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R}(x_{i},t_{i};\\nu,\\boldsymbol{\\theta}).$$ To analyze imbalance across physics terms, decompose the residual as $$\\mathcal{R}=\\mathcal{T}+\\mathcal{A}-\\mathcal{D},\\quad \\text{with}\\quad \\mathcal{T}=u_{t},\\ \\mathcal{A}=u\\,u_{x},\\ \\mathcal{D}=\\nu\\,u_{xx}.$$ Then the residual Jacobian with respect to the parameters admits the exact decomposition $$\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R} = \\underbrace{\\nabla_{\\boldsymbol{\\theta}}\\mathcal{T}}_{\\text{time}} + \\underbrace{\\nabla_{\\boldsymbol{\\theta}}\\mathcal{A}}_{\\text{advection}} - \\underbrace{\\nabla_{\\boldsymbol{\\theta}}\\mathcal{D}}_{\\text{diffusion}}.$$ Using the linearity of $u$ in $\\boldsymbol{\\theta}$, and the product rule for $\\mathcal{A}=u\\,u_{x}$, obtain $$\\nabla_{\\boldsymbol{\\theta}}\\mathcal{T}=\\begin{bmatrix}\\partial_{t}\\phi_{1}\\\\ \\partial_{t}\\phi_{2}\\\\ \\partial_{t}\\phi_{3}\\end{bmatrix},\\quad \\nabla_{\\boldsymbol{\\theta}}\\mathcal{A}=\\begin{bmatrix}\\phi_{1}\\,u_{x}+u\\,\\partial_{x}\\phi_{1}\\\\ \\phi_{2}\\,u_{x}+u\\,\\partial_{x}\\phi_{2}\\\\ \\phi_{3}\\,u_{x}+u\\,\\partial_{x}\\phi_{3}\\end{bmatrix},\\quad \\nabla_{\\boldsymbol{\\theta}}\\mathcal{D}=\\nu\\begin{bmatrix}\\partial_{xx}\\phi_{1}\\\\ \\partial_{xx}\\phi_{2}\\\\ \\partial_{xx}\\phi_{3}\\end{bmatrix}.$$ Consequently, the loss gradient splits additively into three components, $$\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}=\\mathbf{g}_{\\mathcal{T}}+\\mathbf{g}_{\\mathcal{A}}+\\mathbf{g}_{\\mathcal{D}},\\quad \\text{where}\\quad \\mathbf{g}_{\\mathcal{X}}=\\frac{2}{N}\\sum_{i=1}^{N}\\mathcal{R}(x_{i},t_{i})\\,\\nabla_{\\boldsymbol{\\theta}}\\mathcal{X}(x_{i},t_{i}),$$ for each $\\mathcal{X}\\in\\{\\mathcal{T},\\mathcal{A},-\\mathcal{D}\\}$, with the sign convention absorbed into the definition of $\\mathbf{g}_{\\mathcal{D}}$.\n\nYour tasks are:\n- Use the above definitions to compute, for each prescribed viscosity $\\nu$, the following quantities over a uniform grid with $x\\in[0,1]$, $t\\in[0,1]$, $N_{x}=64$, $N_{t}=64$, and $N=N_{x}N_{t}$:\n  - The root-mean-square residual $$\\mathrm{RMS}(\\nu)=\\sqrt{\\mathcal{L}(\\nu,\\boldsymbol{\\theta})}.$$\n  - The Euclidean norms of the loss-gradient components $\\lVert \\mathbf{g}_{\\mathcal{T}}\\rVert_{2}$, $\\lVert \\mathbf{g}_{\\mathcal{A}}\\rVert_{2}$, and $\\lVert \\mathbf{g}_{\\mathcal{D}}\\rVert_{2}$, and the advection-diffusion gradient imbalance ratio $$\\rho(\\nu)=\\frac{\\lVert \\mathbf{g}_{\\mathcal{A}}\\rVert_{2}}{\\max\\{\\lVert \\mathbf{g}_{\\mathcal{D}}\\rVert_{2},\\varepsilon\\}},$$ with $\\varepsilon=10^{-16}$ to avoid division by zero.\n  - The condition number of the residual Jacobian matrix $$\\mathbf{J}(\\nu)\\in\\mathbb{R}^{N\\times 3},\\quad \\mathbf{J}_{i,m}=\\left[\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R}(x_{i},t_{i};\\nu,\\boldsymbol{\\theta})\\right]_{m},$$ defined as $$\\kappa(\\mathbf{J})=\\frac{\\sigma_{\\max}(\\mathbf{J})}{\\sigma_{\\min}(\\mathbf{J})},$$ where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values of $\\mathbf{J}$.\n- Predict a training failure for PINNs due to gradient pathologies if either of the following holds: $$\\rho(\\nu)>\\kappa_{\\mathrm{th}} \\quad \\text{or} \\quad \\kappa(\\mathbf{J}(\\nu))>\\chi_{\\mathrm{th}},$$ with thresholds $\\kappa_{\\mathrm{th}}=10^{3}$ and $\\chi_{\\mathrm{th}}=10^{8}$.\n\nTest suite and inputs:\n- Use the fixed parameter vector $\\boldsymbol{\\theta}=\\begin{bmatrix}1.0\\\\-0.5\\\\0.75\\end{bmatrix}$.\n- Use the viscosities $\\nu\\in\\{1.0,\\,0.1,\\,0.01,\\,0.001,\\,0.0\\}$.\n\nRequired outputs and formatting:\n- For each $\\nu$ in the test suite, compute and collect the list $$[\\nu,\\ \\mathrm{RMS}(\\nu),\\ \\rho(\\nu),\\ f(\\nu)],$$ where $f(\\nu)$ is $1$ if training failure is predicted and $0$ otherwise.\n- Your program should produce a single line of output containing the results as a comma-separated list of such lists, with no whitespace and each floating-point number formatted to six significant figures, for example, $$[[1,0.123456,789.123,0],\\ldots].$$ The actual numbers will differ; this format is mandatory.",
            "solution": "The objective of this problem is to analyze the sensitivity of a Physics-Informed Neural Network (PINN) training process for the one-dimensional viscous Burgers equation as the viscosity parameter $\\nu$ decreases. This analysis is performed by computing several key metrics for a fixed, simplified trial solution that mimics the output of a neural network. The metrics include the root-mean-square (RMS) of the PDE residual, the norms of individual components of the loss function's gradient, and the condition number of the residual Jacobian. These quantities are used to predict potential training failures arising from gradient pathologies, specifically the imbalance between different physical terms (advection vs. diffusion) and the ill-conditioning of the optimization landscape.\n\nThe analysis proceeds through the following steps:\n\n1.  **Analytical Formulation**: We begin with the analytical definitions provided. The trial solution $u(x,t;\\boldsymbol{\\theta})$ is a linear combination of three predefined basis functions $\\phi_m(x,t)$ with a fixed parameter vector $\\boldsymbol{\\theta}$. The first and second partial derivatives of $u$ with respect to space, $u_x$ and $u_{xx}$, and time, $u_t$, are computed analytically by differentiating the sum term-by-term. These analytical expressions are crucial for accurately evaluating the PDE residual and its gradients without introducing numerical differentiation errors.\n\n2.  **Grid Discretization**: The continuous spatio-temporal domain $(x,t) \\in [0,1] \\times [0,1]$ is discretized into a uniform grid of $N_x \\times N_t = 64 \\times 64 = 4096$ collocation points. All subsequent calculations are performed on this grid. The use of vectorized operations in `NumPy` allows for efficient computation across all points simultaneously.\n\n3.  **Residual and Gradient Computation**: For each viscosity value $\\nu$ in the test suite, we compute the following quantities at each grid point:\n    -   The trial solution $u$ and its derivatives $u_t$, $u_x$, and $u_{xx}$.\n    -   The PDE residual, $\\mathcal{R} = u_{t} + u\\,u_{x} - \\nu\\,u_{xx}$, which measures how well the trial solution satisfies the Burgers equation.\n    -   The gradients of the residual with respect to the parameters $\\boldsymbol{\\theta}$, known as the residual Jacobian $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R}$. These are decomposed into contributions from the temporal ($\\mathcal{T}=u_t$), advection ($\\mathcal{A}=u\\,u_x$), and diffusion ($\\mathcal{D}=\\nu\\,u_{xx}$) terms. The linearity of the trial solution in $\\boldsymbol{\\theta}$ simplifies these gradient calculations significantly. For instance, $\\nabla_{\\boldsymbol{\\theta}} u_t$ is simply the vector of the time derivatives of the basis functions, $[\\partial_t\\phi_1, \\partial_t\\phi_2, \\partial_t\\phi_3]^T$. The gradient of the nonlinear advection term is computed using the product rule.\n\n4.  **Metric Calculation**: Using the pointwise quantities computed above, we aggregate them into the required global metrics:\n    -   **RMS Residual, $\\mathrm{RMS}(\\nu)$**: This is the square root of the mean of the squared residuals over all collocation points, $\\sqrt{\\frac{1}{N}\\sum_i \\mathcal{R}_i^2}$. It provides an overall measure of the solution's accuracy.\n    -   **Loss Gradient Component Norms**: The gradient of the mean-squared error loss function, $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}$, is additively composed of three vectors, $\\mathbf{g}_{\\mathcal{T}}$, $\\mathbf{g}_{\\mathcal{A}}$, and $\\mathbf{g}_{\\mathcal{D}}$, corresponding to the temporal, advection, and diffusion terms. Each vector is computed by summing the product of the residual $\\mathcal{R}$ and the respective component of the residual Jacobian over all grid points, i.e., $\\mathbf{g}_{\\mathcal{X}} \\propto \\sum_i \\mathcal{R}_i \\nabla_{\\boldsymbol{\\theta}}\\mathcal{X}_i$. We then compute the Euclidean norm for each of these three vectors.\n    -   **Advection-Diffusion Imbalance Ratio, $\\rho(\\nu)$**: This ratio, $\\lVert\\mathbf{g}_\\mathcal{A}\\rVert_2 / \\max(\\lVert\\mathbf{g}_\\mathcal{D}\\rVert_2, \\varepsilon)$, quantifies the relative magnitude of the advection gradient component to the diffusion gradient component. A large value indicates that the training dynamics are dominated by the advection term, which can be problematic as viscosity vanishes and the problem becomes convection-dominated.\n    -   **Condition Number, $\\kappa(\\mathbf{J})$**: The residual Jacobian matrix $\\mathbf{J} \\in \\mathbb{R}^{N\\times 3}$ is formed by stacking the gradient vectors $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R}$ from each of the $N$ collocation points. Its condition number, the ratio of its largest to smallest singular value, measures the sensitivity of the residual to changes in the parameters. A high condition number indicates an ill-conditioned problem, where small changes in parameters can lead to large changes in the residual, often slowing down or stalling gradient-based optimization.\n\n5.  **Failure Prediction**: Based on the computed metrics, a training failure is predicted if the imbalance ratio $\\rho(\\nu)$ exceeds a threshold $\\kappa_{\\mathrm{th}}=10^3$ or if the condition number $\\kappa(\\mathbf{J}(\\nu))$ exceeds a threshold $\\chi_{\\mathrm{th}}=10^8$. These conditions signal that the underlying optimization problem is pathological, either due to severe gradient imbalance or ill-conditioning, both of which are common failure modes for PINNs in multi-scale problems.\n\nThe implementation encapsulates these steps into a single program that iterates through the provided list of viscosity values, performs the calculations for each, and formats the results according to the specified output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes sensitivity and predicts training failure for a PINN model of the\n    1D viscous Burgers equation for a range of viscosity values.\n    \"\"\"\n    \n    # Define problem constants and parameters from the problem statement.\n    THETA = np.array([1.0, -0.5, 0.75])\n    NU_VALUES = [1.0, 0.1, 0.01, 0.001, 0.0]\n    NX, NT = 64, 64\n    N = NX * NT\n    KAPPA_TH = 1e3\n    CHI_TH = 1e8\n    EPSILON = 1e-16\n\n    # Create the spatio-temporal grid.\n    x_coords = np.linspace(0, 1, NX)\n    t_coords = np.linspace(0, 1, NT)\n    X, T = np.meshgrid(x_coords, t_coords)\n\n    # --- Pre-compute basis functions and their symbolic derivatives on the grid ---\n    pi = np.pi\n    \n    # phi_1(x,t) = sin(pi*x) * exp(-t)\n    phi1 = np.sin(pi * X) * np.exp(-T)\n    phi1_t = -phi1\n    phi1_x = pi * np.cos(pi * X) * np.exp(-T)\n    phi1_xx = -pi**2 * phi1\n    \n    # phi_2(x,t) = sin(2*pi*x) * exp(-t)\n    phi2 = np.sin(2 * pi * X) * np.exp(-T)\n    phi2_t = -phi2\n    phi2_x = 2 * pi * np.cos(2 * pi * X) * np.exp(-T)\n    phi2_xx = - (2 * pi)**2 * phi2\n\n    # phi_3(x,t) = cos(pi*x) * exp(-2t)\n    phi3 = np.cos(pi * X) * np.exp(-2 * T)\n    phi3_t = -2 * phi3\n    phi3_x = -pi * np.sin(pi * X) * np.exp(-2 * T)\n    phi3_xx = -pi**2 * phi3\n\n    # Store basis functions and their derivatives in arrays for efficient computation.\n    # Shape: (3, NT, NX) where 3 is the number of basis functions.\n    PHI = np.array([phi1, phi2, phi3])\n    PHI_t = np.array([phi1_t, phi2_t, phi3_t])\n    PHI_x = np.array([phi1_x, phi2_x, phi3_x])\n    PHI_xx = np.array([phi1_xx, phi2_xx, phi3_xx])\n\n    # --- Main calculation loop over viscosity values ---\n    results_list = []\n    \n    for nu in NU_VALUES:\n        # Calculate the trial solution u and its derivatives using Einstein summation.\n        # This computes the sum over the first axis (m=1,2,3).\n        u = np.einsum('i,ijk->jk', THETA, PHI)\n        u_t = np.einsum('i,ijk->jk', THETA, PHI_t)\n        u_x = np.einsum('i,ijk->jk', THETA, PHI_x)\n        u_xx = np.einsum('i,ijk->jk', THETA, PHI_xx)\n        \n        # Calculate components of the PDE residual.\n        term_T = u_t\n        term_A = u * u_x\n        term_D = nu * u_xx\n        \n        # Calculate the pointwise PDE residual R = u_t + u*u_x - nu*u_xx.\n        R = term_T + term_A - term_D\n        \n        # Task 1: Compute root-mean-square residual, RMS(nu).\n        rms_val = np.sqrt(np.mean(R**2))\n        \n        # --- Compute gradients with respect to parameters theta ---\n        \n        # Gradient of the temporal term, grad_theta(T). Shape: (NT, NX, 3)\n        grad_T_theta = np.moveaxis(PHI_t, 0, -1)\n        \n        # Gradient of the advection term, grad_theta(A).\n        # grad_A_theta_m = phi_m * u_x + u * phi_x_m\n        grad_A_theta = np.zeros((NT, NX, 3))\n        for m in range(3):\n            grad_A_theta[:, :, m] = PHI[m, :, :] * u_x + u * PHI_x[m, :, :]\n            \n        # Gradient of the diffusion term, grad_theta(D).\n        grad_D_theta = nu * np.moveaxis(PHI_xx, 0, -1)\n        \n        # --- Compute components of the loss gradient: g_T, g_A, g_D ---\n        # g_X = (2/N) * sum_{i,j} (R * grad_X_theta)\n        R_reshaped = R[:, :, np.newaxis] # Reshape for broadcasting\n        \n        g_T = (2 / N) * np.sum(R_reshaped * grad_T_theta, axis=(0, 1))\n        g_A = (2 / N) * np.sum(R_reshaped * grad_A_theta, axis=(0, 1))\n        # g_D is defined with a negative sign: based on grad_theta(-D)\n        g_D = (2 / N) * np.sum(R_reshaped * (-grad_D_theta), axis=(0, 1))\n        \n        # Task 2: Compute norms of gradient components and the imbalance ratio rho(nu).\n        norm_g_A = np.linalg.norm(g_A)\n        norm_g_D = np.linalg.norm(g_D)\n        \n        rho_val = norm_g_A / max(norm_g_D, EPSILON)\n        \n        # Task 3: Compute the condition number of the residual Jacobian matrix J.\n        # J combines gradients from all terms: J = grad_T + grad_A - grad_D\n        grad_R_theta = grad_T_theta + grad_A_theta - grad_D_theta\n        J_matrix = grad_R_theta.reshape((N, 3))\n        cond_J = np.linalg.cond(J_matrix)\n        \n        # Task 4: Predict training failure based on the given thresholds.\n        failure_predicted = 1 if (rho_val > KAPPA_TH) or (cond_J > CHI_TH) else 0\n        \n        results_list.append([nu, rms_val, rho_val, failure_predicted])\n        \n    # --- Format the output as a single-line string ---\n    formatted_results = []\n    for res in results_list:\n        nu_str = f\"{res[0]:.6g}\"\n        rms_str = f\"{res[1]:.6g}\"\n        rho_str = f\"{res[2]:.6g}\"\n        fail_str = str(res[3])\n        formatted_results.append(f\"[{nu_str},{rms_str},{rho_str},{fail_str}]\")\n        \n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}