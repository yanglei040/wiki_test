{
    "hands_on_practices": [
        {
            "introduction": "The continuous adjoint method, or the \"optimize-then-discretize\" approach, begins with deriving the adjoint partial differential equation. This exercise guides you through this fundamental process using the Lagrangian framework for an unsteady convection-diffusion problem. By applying integration by parts, you will uncover how the adjoint system's structure, particularly its terminal condition, is directly determined by the form of the objective functional, a key insight for time-dependent optimization. ",
            "id": "3304882",
            "problem": "Consider a scalar unsteady convection-diffusion process on a bounded, Lipschitz domain $\\Omega \\subset \\mathbb{R}^{d}$, with $d \\in \\{1,2,3\\}$, over a fixed time horizon $[0,T]$. The state $u:\\Omega \\times [0,T] \\to \\mathbb{R}$ satisfies the initial-boundary value problem\n$$\n\\partial_{t} u + \\boldsymbol{v}(\\boldsymbol{x},t) \\cdot \\nabla u - \\nabla \\cdot \\left( \\kappa \\nabla u \\right) = q(\\boldsymbol{x},t) \\quad \\text{in } \\Omega \\times (0,T),\n$$\nwith homogeneous Dirichlet boundary condition $u=0$ on $\\partial \\Omega \\times (0,T)$ and prescribed initial condition $u(\\boldsymbol{x},0)=u_{0}(\\boldsymbol{x})$ on $\\Omega$. Assume $\\kappa>0$ is a positive constant, $\\boldsymbol{v}(\\boldsymbol{x},t)$ is divergence-free, i.e., $\\nabla \\cdot \\boldsymbol{v}(\\boldsymbol{x},t)=0$ for all $(\\boldsymbol{x},t)$, and $q$ and $u_{0}$ are sufficiently smooth for all formal manipulations below to be justified.\n\nDefine the performance functional\n$$\nJ(u) \\;=\\; \\int_{0}^{T} g\\!\\left(u(\\cdot,t)\\right) \\,\\mathrm{d}t \\;+\\; R\\!\\left(u(\\cdot,T)\\right),\n$$\nwhere\n$$\ng\\!\\left(u(\\cdot,t)\\right) \\;=\\; \\int_{\\Omega} \\ell\\!\\left(u(\\boldsymbol{x},t)\\right) \\,\\mathrm{d}\\boldsymbol{x}, \n\\qquad\nR\\!\\left(u(\\cdot,T)\\right) \\;=\\; \\int_{\\Omega} r\\!\\left(u(\\boldsymbol{x},T)\\right) \\,\\mathrm{d}\\boldsymbol{x},\n$$\nand $\\ell:\\mathbb{R}\\to\\mathbb{R}$, $r:\\mathbb{R}\\to\\mathbb{R}$ are continuously differentiable with derivatives denoted $\\ell_{u}$ and $r_{u}$, respectively.\n\nStarting from the strong form of the state equation and the definition of $J(u)$, use a Lagrangian construction and integration by parts to derive the continuous adjoint partial differential equation for the adjoint field $\\psi:\\Omega \\times [0,T]\\to \\mathbb{R}$, together with its associated boundary and terminal-in-time conditions. Assume that the Dirichlet boundary condition on $u$ is enforced strongly, so that the corresponding variation of $u$ vanishes on $\\partial \\Omega \\times (0,T)$.\n\nExplain, based on the structure of $J(u)$, when a terminal adjoint condition is present or absent, and how it depends on the presence of a terminal contribution $R(u(\\cdot,T))$ versus a purely running contribution $g(u(\\cdot,t))$.\n\nYour final answer must be the closed-form analytic expression for the terminal adjoint condition $\\psi(\\boldsymbol{x},T)$ in terms of $r_{u}$ and $u(\\boldsymbol{x},T)$. If no terminal contribution $R$ is present, interpret your expression accordingly. No numerical evaluation is required, and no units are needed.",
            "solution": "The problem asks for the derivation of the continuous adjoint equation, boundary conditions, and terminal condition associated with an unsteady convection-diffusion process, using a Lagrangian framework. The goal is to determine the expression for the terminal adjoint condition and explain its dependence on the performance functional $J(u)$.\n\nThe problem statement is validated as scientifically grounded, well-posed, and objective. It represents a standard application of calculus of variations for deriving adjoint systems in the context of partial differential equations, a cornerstone of sensitivity analysis and optimization in computational science. All necessary information is provided, and the premises are consistent. We may therefore proceed with the solution.\n\nThe state equation is given by\n$$\n\\mathcal{N}(u) := \\partial_{t} u + \\boldsymbol{v} \\cdot \\nabla u - \\nabla \\cdot \\left( \\kappa \\nabla u \\right) - q = 0 \\quad \\text{in } \\Omega \\times (0,T)\n$$\nwith initial condition $u(\\boldsymbol{x},0) = u_{0}(\\boldsymbol{x})$ and boundary condition $u|_{\\partial\\Omega \\times (0,T)} = 0$. The performance functional is\n$$\nJ(u) = \\int_{0}^{T} \\int_{\\Omega} \\ell(u(\\boldsymbol{x},t)) \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t + \\int_{\\Omega} r(u(\\boldsymbol{x},T)) \\,\\mathrm{d}\\boldsymbol{x}\n$$\nWe introduce the adjoint field $\\psi(\\boldsymbol{x},t)$ as a Lagrange multiplier to enforce the state equation as a constraint. The Lagrangian functional $\\mathcal{L}(u, \\psi)$ is defined as:\n$$\n\\mathcal{L}(u, \\psi) = J(u) + \\int_{0}^{T} \\int_{\\Omega} \\psi \\, \\mathcal{N}(u) \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t\n$$\nSubstituting the expressions for $J(u)$ and $\\mathcal{N}(u)$:\n$$\n\\mathcal{L}(u, \\psi) = \\int_{0}^{T} \\int_{\\Omega} \\ell(u) \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t + \\int_{\\Omega} r(u(\\cdot,T)) \\,\\mathrm{d}\\boldsymbol{x} + \\int_{0}^{T} \\int_{\\Omega} \\psi \\left( \\partial_{t} u + \\boldsymbol{v} \\cdot \\nabla u - \\nabla \\cdot (\\kappa \\nabla u) - q \\right) \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t\n$$\nTo find the adjoint equations, we compute the first variation of $\\mathcal{L}$ with respect to $u$ in an arbitrary, admissible direction $\\delta u$, and set it to zero. A perturbation $\\delta u$ is admissible if it satisfies homogeneous initial and boundary conditions, i.e., $\\delta u(\\boldsymbol{x},0) = 0$ on $\\Omega$ and $\\delta u = 0$ on $\\partial\\Omega \\times (0,T)$.\nThe first variation is $\\delta\\mathcal{L}[\\delta u] = \\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}\\Big|_{\\epsilon=0} \\mathcal{L}(u+\\epsilon\\delta u, \\psi)$.\n\nThe variation of the performance functional $J(u)$ is:\n$$\n\\delta J = \\int_{0}^{T} \\int_{\\Omega} \\ell_{u}(u) \\delta u \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t + \\int_{\\Omega} r_{u}(u(\\cdot,T)) \\delta u(\\cdot,T) \\,\\mathrm{d}\\boldsymbol{x}\n$$\nThe variation of the constraint term is:\n$$\n\\delta \\left( \\int_{0}^{T} \\int_{\\Omega} \\psi \\, \\mathcal{N}(u) \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t \\right) = \\int_{0}^{T} \\int_{\\Omega} \\psi \\left( \\partial_{t} \\delta u + \\boldsymbol{v} \\cdot \\nabla \\delta u - \\nabla \\cdot (\\kappa \\nabla \\delta u) \\right) \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t\n$$\nWe apply integration by parts to each term in the expression above to transfer all differential operators from the variation $\\delta u$ to the adjoint field $\\psi$.\n\n1.  **Time derivative term:**\n    $$\n    \\int_{0}^{T} \\int_{\\Omega} \\psi \\, \\partial_{t} \\delta u \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t = \\int_{\\Omega} \\left[ \\psi \\delta u \\right]_{t=0}^{t=T} \\,\\mathrm{d}\\boldsymbol{x} - \\int_{0}^{T} \\int_{\\Omega} (\\partial_{t}\\psi) \\delta u \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t\n    $$\n    This becomes $\\int_{\\Omega} \\psi(\\boldsymbol{x},T) \\delta u(\\boldsymbol{x},T) \\,\\mathrm{d}\\boldsymbol{x} - \\int_{0}^{T} \\int_{\\Omega} (\\partial_{t}\\psi) \\delta u \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t$, since $\\delta u(\\boldsymbol{x},0) = 0$.\n\n2.  **Convection term:** Given the divergence-free condition $\\nabla \\cdot \\boldsymbol{v} = 0$, we have $\\psi (\\boldsymbol{v} \\cdot \\nabla \\delta u) = \\psi \\nabla \\cdot (\\delta u \\boldsymbol{v}) = \\nabla \\cdot (\\psi \\delta u \\boldsymbol{v}) - \\delta u (\\boldsymbol{v} \\cdot \\nabla \\psi)$.\n    $$\n    \\int_{0}^{T} \\int_{\\Omega} \\psi (\\boldsymbol{v} \\cdot \\nabla \\delta u) \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t = \\int_{0}^{T} \\int_{\\Omega} \\left( \\nabla \\cdot (\\psi \\delta u \\boldsymbol{v}) - \\delta u (\\boldsymbol{v} \\cdot \\nabla \\psi) \\right) \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t\n    $$\n    By the Divergence Theorem, $\\int_{\\Omega} \\nabla \\cdot (\\psi \\delta u \\boldsymbol{v}) \\,\\mathrm{d}\\boldsymbol{x} = \\int_{\\partial\\Omega} \\psi \\delta u (\\boldsymbol{v} \\cdot \\boldsymbol{n}) \\,\\mathrm{d}S = 0$, because $\\delta u = 0$ on $\\partial\\Omega$. The term reduces to $-\\int_{0}^{T} \\int_{\\Omega} \\delta u (\\boldsymbol{v} \\cdot \\nabla \\psi) \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t$.\n\n3.  **Diffusion term:** We apply integration by parts twice.\n    $$\n    -\\int_{0}^{T} \\int_{\\Omega} \\psi \\nabla \\cdot (\\kappa \\nabla \\delta u) \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t = -\\int_{0}^{T} \\left( \\int_{\\partial\\Omega} \\psi \\kappa (\\nabla\\delta u \\cdot \\boldsymbol{n}) \\,\\mathrm{d}S - \\int_{\\Omega} \\nabla\\psi \\cdot (\\kappa \\nabla\\delta u) \\,\\mathrm{d}\\boldsymbol{x} \\right) \\mathrm{d}t\n    $$\n    Applying integration by parts to the second term in the parenthesis:\n    $$\n    \\int_{\\Omega} \\nabla\\psi \\cdot (\\kappa \\nabla\\delta u) \\,\\mathrm{d}\\boldsymbol{x} = \\int_{\\partial\\Omega} \\delta u (\\kappa \\nabla\\psi \\cdot \\boldsymbol{n}) \\,\\mathrm{d}S - \\int_{\\Omega} \\delta u \\nabla\\cdot(\\kappa \\nabla\\psi) \\,\\mathrm{d}\\boldsymbol{x}\n    $$\n    Since $\\delta u = 0$ on $\\partial\\Omega$, the new boundary integral vanishes. Combining results, the diffusion term becomes:\n    $$\n    -\\int_{0}^{T} \\int_{\\partial\\Omega} \\psi \\kappa (\\nabla\\delta u \\cdot \\boldsymbol{n}) \\,\\mathrm{d}S \\,\\mathrm{d}t - \\int_{0}^{T} \\int_{\\Omega} \\delta u \\nabla\\cdot(\\kappa \\nabla\\psi) \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t\n    $$\n    For the variation $\\delta\\mathcal{L}$ to be independent of the choice of $\\delta u$, the boundary term must vanish for any admissible $\\delta u$. Since $\\nabla\\delta u \\cdot \\boldsymbol{n}$ is not necessarily zero on the boundary, we must enforce the homogeneous Dirichlet boundary condition $\\psi=0$ on $\\partial\\Omega \\times (0,T)$. This yields the adjoint boundary condition. With this choice, the diffusion term simplifies to $-\\int_{0}^{T} \\int_{\\Omega} \\delta u \\nabla\\cdot(\\kappa \\nabla\\psi) \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t$. Since $\\kappa$ is a constant, $\\nabla\\cdot(\\kappa \\nabla\\psi) = \\kappa \\nabla^{2}\\psi$.\n\nCombining all terms, the total variation $\\delta\\mathcal{L}$ is:\n$$\n\\delta\\mathcal{L} = \\int_{0}^{T} \\int_{\\Omega} \\left[ \\ell_{u}(u) - \\partial_{t}\\psi - \\boldsymbol{v} \\cdot \\nabla \\psi - \\kappa \\nabla^{2}\\psi \\right] \\delta u \\,\\mathrm{d}\\boldsymbol{x} \\,\\mathrm{d}t + \\int_{\\Omega} \\left[ r_{u}(u(\\cdot,T)) + \\psi(\\boldsymbol{x},T) \\right] \\delta u(\\boldsymbol{x},T) \\,\\mathrm{d}\\boldsymbol{x} = 0\n$$\nBy the fundamental lemma of calculus of variations, for $\\delta\\mathcal{L}$ to be zero for all admissible variations $\\delta u$, the integrands must be zero. This yields the adjoint system:\n- **Adjoint PDE:** The term in the space-time integral must be zero, giving the partial differential equation for $\\psi$:\n  $$\n  -\\partial_{t}\\psi - \\boldsymbol{v} \\cdot \\nabla \\psi - \\kappa \\nabla^{2}\\psi = -\\ell_{u}(u) \\quad \\text{in } \\Omega \\times (0,T)\n  $$\n- **Adjoint Boundary Condition:** As derived from integration by parts:\n  $$\n  \\psi = 0 \\quad \\text{on } \\partial\\Omega \\times (0,T)\n  $$\n- **Adjoint Terminal Condition:** The term in the final-time integral must be zero:\n  $$\n  \\psi(\\boldsymbol{x},T) + r_{u}(u(\\boldsymbol{x},T)) = 0 \\quad \\text{on } \\Omega\n  $$\n  This gives the terminal-in-time condition for the adjoint field $\\psi$:\n  $$\n  \\psi(\\boldsymbol{x},T) = -r_{u}(u(\\boldsymbol{x},T))\n  $$\n\nThe adjoint PDE is a terminal-value problem, as it is integrated backward in time from a condition specified at the final time $t=T$.\n\nThe structure of this terminal condition directly addresses the second part of the problem. The function $r_u$ is the derivative of the integrand of the terminal cost term, $R(u(\\cdot,T)) = \\int_{\\Omega} r(u(\\boldsymbol{x},T)) \\,\\mathrm{d}\\boldsymbol{x}$.\n- If the performance functional $J(u)$ includes a terminal cost contribution $R(u(\\cdot,T))$, then $r(u)$ is a non-trivial function. Its derivative $r_u(u)$ is generally non-zero, leading to a non-homogeneous terminal condition $\\psi(\\boldsymbol{x},T) = -r_u(u(\\boldsymbol{x},T))$.\n- If the performance functional consists solely of a running cost contribution (i.e., $R(u(\\cdot,T))=0$), we can formally set $r(u) \\equiv 0$. In this case, its derivative $r_u$ is also zero. The terminal condition for the adjoint equation simplifies to a homogeneous condition: $\\psi(\\boldsymbol{x},T) = 0$.\n\nThus, the presence of a terminal cost directly sources the adjoint system at the final time, providing the \"initial\" condition for the backward-in-time integration. The absence of a terminal cost results in a zero terminal condition for the adjoint. The final answer required is the analytical expression for this terminal condition.",
            "answer": "$$\n\\boxed{-r_{u}\\left(u(\\boldsymbol{x},T)\\right)}\n$$"
        },
        {
            "introduction": "In the \"discretize-then-optimize\" paradigm, the governing PDEs are first transformed into a system of algebraic equations. This practice moves our focus from calculus of variations to linear algebra, asking you to define the adjoint of a matrix operator with respect to a non-Euclidean inner product induced by the finite element mass matrix. This exercise is crucial for understanding that the discrete adjoint is not simply the matrix transpose, but is fundamentally tied to the underlying function space and its chosen inner product. ",
            "id": "3304955",
            "problem": "Consider a linear, steady, scalar partial differential equation on a bounded domain with sufficiently smooth boundary, discretized by a Galerkin Finite Element Method (FEM) within the context of Computational Fluid Dynamics (CFD). Let the resulting algebraic system be written as $K u = f$, where $K \\in \\mathbb{R}^{n \\times n}$ is the stiffness matrix arising from the bilinear form associated with the weak formulation, and $M \\in \\mathbb{R}^{n \\times n}$ is the symmetric positive definite mass matrix induced by the $L^2$ inner product on the finite element space, i.e., $M_{ij} = \\int_{\\Omega} \\phi_{i} \\phi_{j} \\,\\mathrm{d}x$ for the basis functions $\\{\\phi_{i}\\}_{i=1}^{n}$. For coefficient vectors $x, y \\in \\mathbb{R}^{n}$, define the $M$-weighted inner product by $\\langle x, y \\rangle_{M} := x^{\\mathsf{T}} M y$.\n\nIn discrete adjoint analysis, one defines the adjoint operator of $K$ with respect to the $M$-weighted inner product as the unique linear operator $K^{\\dagger}$ such that $\\langle x, K y \\rangle_{M} = \\langle K^{\\dagger} x, y \\rangle_{M}$ for all $x, y \\in \\mathbb{R}^{n}$. Using only the defining property of an adjoint under a given inner product and the properties of $M$, derive an explicit expression for the matrix $K^{\\dagger}$ in terms of $K$, $M$, and the Euclidean transpose. Your final answer must be a single closed-form analytic expression for $K^{\\dagger}$ in terms of $K$ and $M$ (no equations or inequalities). No numerical rounding is required and no units are involved.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of linear algebra and numerical analysis, specifically within the context of the finite element method and adjoint operator theory. It is well-posed, objective, and contains all necessary information for a unique solution.\n\nThe objective is to derive an explicit expression for the discrete adjoint operator $K^{\\dagger}$ in terms of the stiffness matrix $K$, the mass matrix $M$, and the Euclidean transpose operation. The starting point is the defining property of the adjoint operator $K^{\\dagger}$ with respect to the $M$-weighted inner product $\\langle \\cdot, \\cdot \\rangle_{M}$. This property is given as:\n$$\n\\langle x, K y \\rangle_{M} = \\langle K^{\\dagger} x, y \\rangle_{M}\n$$\nThis relation must hold for all vectors $x, y \\in \\mathbb{R}^{n}$.\n\nThe definition of the $M$-weighted inner product is provided as $\\langle u, v \\rangle_{M} := u^{\\mathsf{T}} M v$ for any vectors $u, v \\in \\mathbb{R}^{n}$. We substitute this definition into both sides of the defining equation for the adjoint.\n\nThe left-hand side (LHS) becomes:\n$$\n\\langle x, K y \\rangle_{M} = x^{\\mathsf{T}} M (K y)\n$$\nUsing the associativity of matrix multiplication, this can be written as:\n$$\n\\text{LHS} = x^{\\mathsf{T}} M K y\n$$\nThe right-hand side (RHS) becomes:\n$$\n\\langle K^{\\dagger} x, y \\rangle_{M} = (K^{\\dagger} x)^{\\mathsf{T}} M y\n$$\nTo simplify the RHS, we use the property of the transpose of a matrix-vector product, which is $(AB)^{\\mathsf{T}} = B^{\\mathsf{T}} A^{\\mathsf{T}}$. Applying this to the term $(K^{\\dagger} x)^{\\mathsf{T}}$, we get:\n$$\n(K^{\\dagger} x)^{\\mathsf{T}} = x^{\\mathsf{T}} (K^{\\dagger})^{\\mathsf{T}}\n$$\nSubstituting this back into the expression for the RHS gives:\n$$\n\\text{RHS} = x^{\\mathsf{T}} (K^{\\dagger})^{\\mathsf{T}} M y\n$$\nNow, we equate the expressions for the LHS and RHS:\n$$\nx^{\\mathsf{T}} M K y = x^{\\mathsf{T}} (K^{\\dagger})^{\\mathsf{T}} M y\n$$\nThis equation must be true for all vectors $x \\in \\mathbb{R}^{n}$ and $y \\in \\mathbb{R}^{n}$. If $x^{\\mathsf{T}} A y = x^{\\mathsf{T}} B y$ for all $x, y$, then it must be that the matrices $A$ and $B$ are equal. Therefore, we can equate the matrix products:\n$$\nM K = (K^{\\dagger})^{\\mathsf{T}} M\n$$\nOur goal is to solve for the matrix $K^{\\dagger}$. The problem states that $M$ is the mass matrix, which is symmetric and positive definite. A positive definite matrix is, by definition, invertible. We can therefore right-multiply both sides of the equation by the inverse of $M$, denoted as $M^{-1}$:\n$$\n(M K) M^{-1} = ((K^{\\dagger})^{\\mathsf{T}} M) M^{-1}\n$$\n$$\nM K M^{-1} = (K^{\\dagger})^{\\mathsf{T}} (M M^{-1})\n$$\nSince $M M^{-1} = I$, where $I$ is the identity matrix, the equation simplifies to:\n$$\nM K M^{-1} = (K^{\\dagger})^{\\mathsf{T}}\n$$\nTo obtain an expression for $K^{\\dagger}$, we take the transpose of both sides of this equation:\n$$\n(M K M^{-1})^{\\mathsf{T}} = ((K^{\\dagger})^{\\mathsf{T}})^{\\mathsf{T}}\n$$\nThe transpose of a transpose of a matrix returns the original matrix, so $((K^{\\dagger})^{\\mathsf{T}})^{\\mathsf{T}} = K^{\\dagger}$. This gives:\n$$\nK^{\\dagger} = (M K M^{-1})^{\\mathsf{T}}\n$$\nNext, we use the property that the transpose of a product of matrices is the product of their transposes in reverse order: $(ABC)^{\\mathsf{T}} = C^{\\mathsf{T}} B^{\\mathsf{T}} A^{\\mathsf{T}}$. Applying this rule, we get:\n$$\nK^{\\dagger} = (M^{-1})^{\\mathsf{T}} K^{\\mathsf{T}} M^{\\mathsf{T}}\n$$\nThe problem states that $M$ is a symmetric matrix, which means $M^{\\mathsf{T}} = M$. The inverse of a symmetric matrix is also symmetric, so we have $(M^{-1})^{\\mathsf{T}} = M^{-1}$. Substituting these properties into the expression for $K^{\\dagger}$:\n$$\nK^{\\dagger} = M^{-1} K^{\\mathsf{T}} M\n$$\nThis is the final, explicit expression for the discrete adjoint matrix $K^{\\dagger}$ in terms of $K$, $M$, and the transpose operation.",
            "answer": "$$\n\\boxed{M^{-1} K^{\\mathsf{T}} M}\n$$"
        },
        {
            "introduction": "While the continuous and discrete adjoint approaches both aim to compute sensitivities, they do not always yield the same gradient. This exercise presents a carefully constructed model problem to demonstrate analytically how a discrepancy can arise from an inconsistent numerical treatment of the objective functional. By working through this example, you will gain a critical appreciation for how the order of operations—discretization and optimization—can lead to different outcomes, a vital consideration in practical applications. ",
            "id": "3304943",
            "problem": "Consider the following model problem in the field of computational fluid dynamics at the level of an advanced graduate course, designed to expose the difference between the continuous adjoint gradient for an objective functional and the gradient of a discretized objective arising from an inconsistent quadrature.\n\nLet the state be the solution $u(x; p)$ of the linear partial differential equation (PDE)\n$$\n- \\frac{d^{2} u}{dx^{2}}(x; p) \\;=\\; p,\\quad x \\in (0, 1),\n$$\nsubject to Dirichlet boundary conditions\n$$\nu(0; p) \\;=\\; 0, \\qquad u(1; p) \\;=\\; 0,\n$$\nwhere $p \\in \\mathbb{R}$ is a scalar control parameter. The continuous objective is the linear functional\n$$\nJ(u) \\;=\\; \\int_{0}^{1} u(x; p)\\, dx.\n$$\nFor the discrete problem, consider a uniform mesh with $N$ interior nodes, grid spacing $h = \\frac{1}{N+1}$, and grid points $x_{i} = i h$ for $i = 1, \\dots, N$. Discretize the PDE by the standard second-order centered finite difference (FD) method to obtain\n$$\n- \\frac{U_{i-1} - 2 U_{i} + U_{i+1}}{h^{2}} \\;=\\; p, \\qquad i = 1, \\dots, N,\n$$\nwith $U_{0} = 0$ and $U_{N+1} = 0$. Define the discretized objective by an inconsistent quadrature that omits the last interior node:\n$$\nJ_{h}(U) \\;=\\; h \\sum_{i=1}^{N-1} U_{i}.\n$$\n\nUsing the continuous adjoint method, derive the continuous gradient $\\frac{dJ}{dp}$ in closed form. Using the discrete adjoint method, derive the discrete gradient $\\frac{dJ_{h}}{dp}$ in closed form for arbitrary integer $N \\geq 2$, making explicit use of the linear algebraic adjoint system that corresponds to the FD discretization and the above definition of $J_{h}$. Show that the two gradients differ due solely to the inconsistent quadrature in $J_{h}$, and compute the exact difference\n$$\n\\Delta(N) \\;=\\; \\frac{dJ}{dp} \\;-\\; \\frac{dJ_{h}}{dp}\n$$\nas a single closed-form analytic expression in terms of $N$. Express the final answer as a simplified analytic expression in terms of $N$. Do not round your result. No units are required.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. It represents a standard pedagogical problem in the field of computational methods for optimization and is suitable for rigorous analysis. We proceed with the solution.\n\nThe objective is to compute the difference $\\Delta(N) = \\frac{dJ}{dp} - \\frac{dJ_{h}}{dp}$ between the continuous and discrete gradients of an objective functional with respect to a control parameter $p$.\n\nFirst, we will find the continuous gradient $\\frac{dJ}{dp}$ using the continuous adjoint method.\nThe state equation is the Poisson equation on the domain $x \\in (0, 1)$:\n$$ - \\frac{d^{2} u}{dx^{2}}(x; p) = p $$\nwith homogeneous Dirichlet boundary conditions $u(0; p) = 0$ and $u(1; p) = 0$.\nThe objective functional is:\n$$ J(u) = \\int_{0}^{1} u(x; p)\\, dx $$\nTo find the gradient $\\frac{dJ}{dp}$, we introduce the Lagrangian $\\mathcal{L}$ by augmenting the objective functional with the state equation using an adjoint function $\\lambda(x)$ as a Lagrange multiplier:\n$$ \\mathcal{L}(u, \\lambda, p) = J(u) - \\int_{0}^{1} \\lambda(x) \\left( - \\frac{d^{2} u}{dx^{2}} - p \\right) dx $$\nFor the gradient of the Lagrangian with respect to the state, $\\frac{\\delta \\mathcal{L}}{\\delta u}$, to be zero for any arbitrary admissible variation $\\delta u$, we must find the adjoint equation.\n$$ \\frac{\\delta \\mathcal{L}}{\\delta u} \\cdot \\delta u = \\int_{0}^{1} \\delta u(x) \\,dx - \\int_{0}^{1} \\lambda(x) \\left( - \\frac{d^{2} (\\delta u)}{dx^{2}} \\right) dx = 0 $$\nIntegrating the second term by parts twice, we get:\n$$ -\\int_{0}^{1} \\lambda \\frac{d^{2} (\\delta u)}{dx^{2}} dx = - \\left[ \\lambda \\frac{d(\\delta u)}{dx} \\right]_{0}^{1} + \\left[ \\frac{d\\lambda}{dx} \\delta u \\right]_{0}^{1} - \\int_{0}^{1} \\frac{d^{2}\\lambda}{dx^{2}} \\delta u \\, dx $$\nThe variations $\\delta u$ must satisfy the same homogeneous boundary conditions as $u$, so $\\delta u(0) = 0$ and $\\delta u(1) = 0$. By enforcing homogeneous boundary conditions on the adjoint variable $\\lambda$ as well, $\\lambda(0) = 0$ and $\\lambda(1) = 0$, all boundary terms vanish. The stationarity condition becomes:\n$$ \\int_{0}^{1} \\left( 1 - \\frac{d^{2}\\lambda}{dx^{2}} \\right) \\delta u(x) \\,dx = 0 $$\nSince this must hold for all admissible variations $\\delta u$, the integrand must be zero, which gives the adjoint equation:\n$$ - \\frac{d^{2}\\lambda}{dx^{2}} = 1, \\quad x \\in (0, 1) $$\nwith boundary conditions $\\lambda(0) = 0$ and $\\lambda(1) = 0$. This is a Poisson equation for $\\lambda$. Integrating twice and applying the boundary conditions yields the solution:\n$$ \\lambda(x) = \\frac{1}{2}x(1-x) $$\nThe total derivative of $J$ with respect to $p$ is then the partial derivative of the Lagrangian with respect to $p$, as the terms involving derivatives of the state variable vanish by construction of the adjoint:\n$$ \\frac{dJ}{dp} = \\frac{\\partial \\mathcal{L}}{\\partial p} = \\frac{\\partial}{\\partial p} \\int_{0}^{1} \\lambda(x) p \\, dx = \\int_{0}^{1} \\lambda(x) \\, dx $$\nSubstituting the expression for $\\lambda(x)$ and integrating:\n$$ \\frac{dJ}{dp} = \\int_{0}^{1} \\frac{1}{2}x(1-x) \\, dx = \\frac{1}{2} \\left[ \\frac{x^2}{2} - \\frac{x^3}{3} \\right]_{0}^{1} = \\frac{1}{2} \\left( \\frac{1}{2} - \\frac{1}{3} \\right) = \\frac{1}{12} $$\nSo the continuous gradient is $\\frac{dJ}{dp} = \\frac{1}{12}$.\n\nNext, we derive the discrete gradient $\\frac{dJ_{h}}{dp}$ using the discrete adjoint method.\nThe discrete state equation is given by the finite difference system:\n$$ - \\frac{U_{i-1} - 2 U_{i} + U_{i+1}}{h^{2}} = p, \\quad i = 1, \\dots, N $$\nwith $U_{0} = 0$ and $U_{N+1} = 0$. This can be written in matrix form as $A_{h} U = p \\mathbf{1}$, where $U = [U_1, \\dots, U_N]^T$, $\\mathbf{1}$ is the vector of all ones, and $A_h$ is the $N \\times N$ symmetric, positive-definite matrix:\n$$ A_h = \\frac{1}{h^2} \\begin{pmatrix} 2 & -1 & & \\\\ -1 & 2 & -1 & \\\\ & \\ddots & \\ddots & \\ddots \\\\ & & -1 & 2 & -1 \\\\ & & & -1 & 2 \\end{pmatrix} $$\nThe discretized objective functional is:\n$$ J_{h}(U) = h \\sum_{i=1}^{N-1} U_{i} $$\nThis can be written in vector form as $J_h(U) = c_h^T U$, where $c_h = h [1, 1, \\dots, 1, 0]^T$. The quadrature is inconsistent because a consistent trapezoidal rule would include the node $U_N$.\nThe discrete Lagrangian is:\n$$ \\mathcal{L}_h(U, \\Lambda, p) = J_h(U) - \\Lambda^T (A_h U - p \\mathbf{1}) = c_h^T U - \\Lambda^T (A_h U - p \\mathbf{1}) $$\nwhere $\\Lambda$ is the discrete adjoint vector. The discrete adjoint equation is obtained by setting the gradient of $\\mathcal{L}_h$ with respect to $U$ to zero:\n$$ \\nabla_U \\mathcal{L}_h = c_h - A_h^T \\Lambda = 0 \\implies A_h \\Lambda = c_h $$\nsince $A_h$ is symmetric.\nThe discrete gradient $\\frac{dJ_h}{dp}$ is the partial derivative of $\\mathcal{L}_h$ with respect to $p$:\n$$ \\frac{dJ_h}{dp} = \\frac{\\partial \\mathcal{L}_h}{\\partial p} = \\Lambda^T \\mathbf{1} = \\sum_{i=1}^{N} \\Lambda_i $$\nTo find this sum, we could solve for $\\Lambda = A_h^{-1} c_h$ and sum its components. However, a more direct approach is to differentiate the expression for $J_h$ with respect to $p$ directly. First, we find the discrete state $U$.\n$U = A_h^{-1} (p \\mathbf{1}) = p (A_h^{-1} \\mathbf{1})$.\nThe vector $V = A_h^{-1} \\mathbf{1}$ is the solution to the discrete Poisson equation $A_h V = \\mathbf{1}$. The continuous problem $-v''=1$ with $v(0)=v(1)=0$ has the solution $v(x) = \\frac{1}{2}x(1-x)$. Since the second-order finite difference stencil is exact for quadratic polynomials, the discrete solution is simply the exact solution evaluated at the grid points: $V_i = v(x_i) = \\frac{1}{2}x_i(1-x_i)$.\nThus, $U_i = p V_i = p \\frac{1}{2} x_i (1-x_i)$.\nNow we substitute this into $J_h$:\n$$ J_h(p) = h \\sum_{i=1}^{N-1} U_i = h \\sum_{i=1}^{N-1} p \\frac{1}{2} x_i(1-x_i) $$\nThe gradient is then:\n$$ \\frac{dJ_h}{dp} = h \\sum_{i=1}^{N-1} \\frac{1}{2} x_i(1-x_i) = \\frac{h}{2} \\sum_{i=1}^{N-1} (ih - i^2h^2) = \\frac{h^2}{2} \\sum_{i=1}^{N-1} i - \\frac{h^3}{2} \\sum_{i=1}^{N-1} i^2 $$\nUsing the formulas for sums of powers, $\\sum_{i=1}^{k} i = \\frac{k(k+1)}{2}$ and $\\sum_{i=1}^{k} i^2 = \\frac{k(k+1)(2k+1)}{6}$, with $k=N-1$:\n$$ \\frac{dJ_h}{dp} = \\frac{h^2}{2} \\frac{(N-1)N}{2} - \\frac{h^3}{2} \\frac{(N-1)N(2(N-1)+1)}{6} = \\frac{h^2 N(N-1)}{4} - \\frac{h^3 N(N-1)(2N-1)}{12} $$\nSubstituting $h = \\frac{1}{N+1}$:\n$$ \\frac{dJ_h}{dp} = \\frac{N(N-1)}{4(N+1)^2} - \\frac{N(N-1)(2N-1)}{12(N+1)^3} = \\frac{3N(N-1)(N+1) - N(N-1)(2N-1)}{12(N+1)^3} $$\n$$ = \\frac{N(N-1) [3(N+1)-(2N-1)]}{12(N+1)^3} = \\frac{N(N-1)(3N+3-2N+1)}{12(N+1)^3} = \\frac{N(N-1)(N+4)}{12(N+1)^3} $$\nThis shows that the difference between the continuous and discrete gradients is non-zero due to the combination of discretization error and the inconsistent quadrature.\n\nFinally, we compute the exact difference $\\Delta(N)$:\n$$ \\Delta(N) = \\frac{dJ}{dp} - \\frac{dJ_h}{dp} = \\frac{1}{12} - \\frac{N(N-1)(N+4)}{12(N+1)^3} $$\n$$ \\Delta(N) = \\frac{1}{12} \\left[ 1 - \\frac{N(N-1)(N+4)}{(N+1)^3} \\right] = \\frac{(N+1)^3 - N(N^2+3N-4)}{12(N+1)^3} $$\nLet's expand the terms in the numerator:\n$(N+1)^3 = N^3 + 3N^2 + 3N + 1$\n$N(N^2+3N-4) = N^3 + 3N^2 - 4N$\nThe difference is $(N^3 + 3N^2 + 3N + 1) - (N^3 + 3N^2 - 4N) = 7N+1$.\nTherefore, the exact difference is:\n$$ \\Delta(N) = \\frac{7N+1}{12(N+1)^3} $$\nAs $N \\to \\infty$, $h \\to 0$, we can see that $\\frac{dJ_h}{dp} \\to \\frac{N^3}{12N^3} = \\frac{1}{12}$, so the discrete gradient converges to the continuous gradient. The difference $\\Delta(N)$ is of order $O(N/N^3) = O(1/N^2) = O(h^2)$. The inconsistency in the quadrature introduces an error term that has the same order as the discretization error of the gradient for a consistent scheme.",
            "answer": "$$ \\boxed{\\frac{7N+1}{12(N+1)^3}} $$"
        }
    ]
}