{
    "hands_on_practices": [
        {
            "introduction": "这第一个实践提供了使用加权最小二乘法计算梯度的基础练习。通过从第一性原理推导并求解不同权重方案下的正规方程，您将对如何确定邻近点贡献的优先级有一个具体的理解。此练习还介绍了估计量不确定性的概念，展示了权重的选择如何影响重构梯度的置信度。",
            "id": "3339283",
            "problem": "标量场 $u(\\boldsymbol{x})$ 在一个围绕中心控制体形心 $\\boldsymbol{x}_{0}=(0,0)$ 的非结构化二维模板上被采样。邻近点的偏移量为 $\\boldsymbol{r}_{1}=(1,0)$，$\\boldsymbol{r}_{2}=(0,1)$，$\\boldsymbol{r}_{3}=(-1,0)$，$\\boldsymbol{r}_{4}=(0,-1)$ 和 $\\boldsymbol{r}_{5}=(1,1)$。相对于中心的测量增量为 $\\Delta u_{1}=1.25$，$\\Delta u_{2}=-0.72$，$\\Delta u_{3}=-1.17$，$\\Delta u_{4}=0.69$ 和 $\\Delta u_{5}=0.54$，均为无量纲值。使用线性重构模型 $\\Delta u_{i}\\approx \\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}$，其中 $\\boldsymbol{g}=(g_{x},g_{y})^{\\mathsf{T}}$ 是形心处的梯度，并构建加权最小二乘目标函数 $J(\\boldsymbol{g})=\\sum_{i=1}^{5}w_{i}\\left(\\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}-\\Delta u_{i}\\right)^{2}$。\n\n从该模型和加权最小二乘的定义出发，从第一性原理推导正规方程，并求解它们以在以下两种权重选择下获得重构梯度 $\\boldsymbol{g}$：\n- 均匀权重 $W_{1}=\\mathrm{diag}(1,1,1,1,1)$。\n- 基于距离的权重 $W_{2}=\\mathrm{diag}(1,1,1,1,1/2)$。\n\n假设测量误差是独立的且具有共同方差，因此 $\\Delta u_{i}= \\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}_{\\mathrm{true}}+\\varepsilon_{i}$，其中 $\\mathbb{E}[\\varepsilon_{i}]=0$ 且 $\\mathrm{Cov}(\\varepsilon)=\\sigma^{2}I$。在此假设下，使用从上述目标函数推导出的加权最小二乘估计量的协方差的一般表达式，计算使用 $W_{2}$ 与 $W_{1}$ 所得的 $\\boldsymbol{g}$ 的协方差矩阵的行列式之比 $\\det\\left(\\mathrm{Cov}_{W_{2}}(\\boldsymbol{g})\\right)/\\det\\left(\\mathrm{Cov}_{W_{1}}(\\boldsymbol{g})\\right)$。\n\n将您的最终答案表示为包含以下五个条目的单行矩阵：使用 $W_1$ 时的 $g_{x}$，使用 $W_1$ 时的 $g_{y}$，使用 $W_2$ 时的 $g_{x}$，使用 $W_2$ 时的 $g_{y}$，以及行列式之比 $\\det\\left(\\mathrm{Cov}_{W_{2}}(\\boldsymbol{g})\\right)/\\det\\left(\\mathrm{Cov}_{W_{1}}(\\boldsymbol{g})\\right)$。将您的数值条目四舍五入至四位有效数字。不需要物理单位。",
            "solution": "用户想要解决一个加权最小二乘问题来估计一个梯度向量。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- 标量场表示为 $u(\\boldsymbol{x})$。\n- 中心点位于 $\\boldsymbol{x}_{0}=(0,0)$。\n- 邻近点偏移量为 $\\boldsymbol{r}_{1}=(1,0)$，$\\boldsymbol{r}_{2}=(0,1)$，$\\boldsymbol{r}_{3}=(-1,0)$，$\\boldsymbol{r}_{4}=(0,-1)$ 和 $\\boldsymbol{r}_{5}=(1,1)$。\n- 测量增量为 $\\Delta u_{1}=1.25$，$\\Delta u_{2}=-0.72$，$\\Delta u_{3}=-1.17$，$\\Delta u_{4}=0.69$ 和 $\\Delta u_{5}=0.54$。\n- 线性化模型为 $\\Delta u_{i}\\approx \\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}$，其中 $\\boldsymbol{g}=(g_{x},g_{y})^{\\mathsf{T}}$ 是梯度。\n- 加权最小二乘目标函数为 $J(\\boldsymbol{g})=\\sum_{i=1}^{5}w_{i}\\left(\\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}-\\Delta u_{i}\\right)^{2}$。\n- 指定了两个权重矩阵：\n    1. 均匀权重：$W_{1}=\\mathrm{diag}(1,1,1,1,1)$。\n    2. 基于距离的权重：$W_{2}=\\mathrm{diag}(1,1,1,1,1/2)$。\n- 误差模型为 $\\Delta u_{i}= \\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}_{\\mathrm{true}}+\\varepsilon_{i}$，其中 $\\mathbb{E}[\\varepsilon_{i}]=0$ 且 $\\mathrm{Cov}(\\varepsilon)=\\sigma^{2}I$。\n- 任务是推导正规方程，求解两种权重集下的 $\\boldsymbol{g}$，并计算估计量协方差矩阵的行列式之比 $\\det\\left(\\mathrm{Cov}_{W_{2}}(\\boldsymbol{g})\\right)/\\det\\left(\\mathrm{Cov}_{W_{1}}(\\boldsymbol{g})\\right)$。\n- 数值结果应四舍五入到四位有效数字。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学性：** 该问题描述了用于从散乱数据重构梯度的加权最小二乘法。这是计算流体动力学、有限元分析和其他计算科学领域中一种标准且广泛使用的技术。其基本原理基于泰勒级数展开和统计估计，这些都是成熟的理论。\n- **适定性：** 该问题被构造为一个线性最小二乘问题。设计矩阵 $\\boldsymbol{A}$（其行为 $\\boldsymbol{r}_i^{\\mathsf{T}}$）是列满秩的，权重矩阵 $\\boldsymbol{W}_1$ 和 $\\boldsymbol{W}_2$ 是正定的。这确保了矩阵 $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}$ 是可逆的，从而保证了梯度 $\\boldsymbol{g}$ 存在唯一解。\n- **客观性：** 该问题使用精确的数学语言和定义进行陈述。所有量都有明确的定义。不存在主观或基于观点的陈述。\n- 该问题是自洽的，提供了所有必要的数据和模型。它没有矛盾、不切实际、不适定或过于简单。\n\n**步骤3：结论与行动**\n问题是有效的。将提供完整的解决方案。\n\n### 推导与求解\n\n问题在于找到梯度向量 $\\boldsymbol{g} = (g_x, g_y)^{\\mathsf{T}}$，使得加权最小二乘目标函数最小化：\n$$ J(\\boldsymbol{g}) = \\sum_{i=1}^{5} w_i (\\boldsymbol{r}_i \\cdot \\boldsymbol{g} - \\Delta u_i)^2 $$\n这可以表示为矩阵形式。设设计矩阵 $\\boldsymbol{A}$ 是一个 $5 \\times 2$ 矩阵，其行是向量 $\\boldsymbol{r}_i^{\\mathsf{T}}$，设 $\\boldsymbol{b}$ 是一个包含测量值 $\\Delta u_i$ 的 $5 \\times 1$ 向量。\n$$\n\\boldsymbol{A} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ -1  0 \\\\ 0  -1 \\\\ 1  1 \\end{pmatrix}, \\quad \\boldsymbol{b} = \\begin{pmatrix} 1.25 \\\\ -0.72 \\\\ -1.17 \\\\ 0.69 \\\\ 0.54 \\end{pmatrix}\n$$\n方程组为 $\\boldsymbol{A}\\boldsymbol{g} \\approx \\boldsymbol{b}$。目标函数的矩阵形式是：\n$$ J(\\boldsymbol{g}) = (\\boldsymbol{A}\\boldsymbol{g} - \\boldsymbol{b})^{\\mathsf{T}} \\boldsymbol{W} (\\boldsymbol{A}\\boldsymbol{g} - \\boldsymbol{b}) $$\n其中 $\\boldsymbol{W}$ 是由权重 $w_i$ 构成的对角矩阵。为了最小化 $J(\\boldsymbol{g})$，我们将其对 $\\boldsymbol{g}$ 的梯度设为零。首先，展开目标函数：\n$$ J(\\boldsymbol{g}) = (\\boldsymbol{g}^{\\mathsf{T}}\\boldsymbol{A}^{\\mathsf{T}} - \\boldsymbol{b}^{\\mathsf{T}}) \\boldsymbol{W} (\\boldsymbol{A}\\boldsymbol{g} - \\boldsymbol{b}) = \\boldsymbol{g}^{\\mathsf{T}}\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}\\boldsymbol{g} - \\boldsymbol{g}^{\\mathsf{T}}\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b} - \\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}\\boldsymbol{g} + \\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b} $$\n因为 $\\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}\\boldsymbol{g}$ 是一个标量，它等于其转置 $(\\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}\\boldsymbol{g})^{\\mathsf{T}} = \\boldsymbol{g}^{\\mathsf{T}}\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}^{\\mathsf{T}}\\boldsymbol{b}$。由于 $\\boldsymbol{W}$ 是对角矩阵，$\\boldsymbol{W}^{\\mathsf{T}} = \\boldsymbol{W}$。因此，中间两项是相同的。\n$$ J(\\boldsymbol{g}) = \\boldsymbol{g}^{\\mathsf{T}}(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})\\boldsymbol{g} - 2\\boldsymbol{g}^{\\mathsf{T}}(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b}) + \\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b} $$\n对 $\\boldsymbol{g}$ 求导并令结果为零，得到：\n$$ \\frac{\\partial J}{\\partial \\boldsymbol{g}} = 2(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})\\boldsymbol{g} - 2(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b}) = \\boldsymbol{0} $$\n这就得到了**正规方程**：\n$$ (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})\\boldsymbol{g} = \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b} $$\n梯度估计 $\\boldsymbol{g}$ 的解为：\n$$ \\boldsymbol{g} = (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b}) $$\n\n**情况1：均匀权重** $\\boldsymbol{W}_1 = \\mathrm{diag}(1,1,1,1,1) = \\boldsymbol{I}$\n正规方程简化为 $(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})\\boldsymbol{g}_1 = \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{b}$。\n首先，我们计算 $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A}$：\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A} = \\begin{pmatrix} 1  0  -1  0  1 \\\\ 0  1  0  -1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ -1  0 \\\\ 0  -1 \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} 1^2+(-1)^2+1^2  1(1) \\\\ 1(1)  1^2+(-1)^2+1^2 \\end{pmatrix} = \\begin{pmatrix} 3  1 \\\\ 1  3 \\end{pmatrix} $$\n接下来，我们计算 $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{b}$：\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{b} = \\begin{pmatrix} 1  0  -1  0  1 \\\\ 0  1  0  -1  1 \\end{pmatrix} \\begin{pmatrix} 1.25 \\\\ -0.72 \\\\ -1.17 \\\\ 0.69 \\\\ 0.54 \\end{pmatrix} = \\begin{pmatrix} 1.25 - (-1.17) + 0.54 \\\\ -0.72 - 0.69 + 0.54 \\end{pmatrix} = \\begin{pmatrix} 2.96 \\\\ -0.87 \\end{pmatrix} $$\n$\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A}$ 的逆矩阵是：\n$$ (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1} = \\frac{1}{3(3)-1(1)} \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix} $$\n现在我们求解 $\\boldsymbol{g}_1$：\n$$ \\boldsymbol{g}_1 = \\frac{1}{8} \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix} \\begin{pmatrix} 2.96 \\\\ -0.87 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 3(2.96) - 1(-0.87) \\\\ -1(2.96) + 3(-0.87) \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 8.88 + 0.87 \\\\ -2.96 - 2.61 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 9.75 \\\\ -5.57 \\end{pmatrix} = \\begin{pmatrix} 1.21875 \\\\ -0.69625 \\end{pmatrix} $$\n四舍五入到四位有效数字，$g_{x,1} = 1.219$ 且 $g_{y,1} = -0.6963$。\n\n**情况2：基于距离的权重** $\\boldsymbol{W}_2 = \\mathrm{diag}(1,1,1,1,1/2)$\n我们计算 $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A}$：\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A} = \\sum_{i=1}^5 w_i \\boldsymbol{r}_i \\boldsymbol{r}_i^{\\mathsf{T}} = 1\\begin{pmatrix}1\\\\0\\end{pmatrix}\\begin{pmatrix}1  0\\end{pmatrix} + 1\\begin{pmatrix}0\\\\1\\end{pmatrix}\\begin{pmatrix}0  1\\end{pmatrix} + 1\\begin{pmatrix}-1\\\\0\\end{pmatrix}\\begin{pmatrix}-1  0\\end{pmatrix} + 1\\begin{pmatrix}0\\\\-1\\end{pmatrix}\\begin{pmatrix}0  -1\\end{pmatrix} + \\frac{1}{2}\\begin{pmatrix}1\\\\1\\end{pmatrix}\\begin{pmatrix}1  1\\end{pmatrix} $$\n$$ = \\begin{pmatrix}10\\\\00\\end{pmatrix} + \\begin{pmatrix}00\\\\01\\end{pmatrix} + \\begin{pmatrix}10\\\\00\\end{pmatrix} + \\begin{pmatrix}00\\\\01\\end{pmatrix} + \\begin{pmatrix}0.50.5\\\\0.50.5\\end{pmatrix} = \\begin{pmatrix} 2.5  0.5 \\\\ 0.5  2.5 \\end{pmatrix} $$\n接下来，我们计算 $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{b}$：\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{b} = \\sum_{i=1}^5 w_i \\boldsymbol{r}_i \\Delta u_i = 1\\begin{pmatrix}1\\\\0\\end{pmatrix}(1.25) + 1\\begin{pmatrix}0\\\\1\\end{pmatrix}(-0.72) + 1\\begin{pmatrix}-1\\\\0\\end{pmatrix}(-1.17) + 1\\begin{pmatrix}0\\\\-1\\end{pmatrix}(0.69) + \\frac{1}{2}\\begin{pmatrix}1\\\\1\\end{pmatrix}(0.54) $$\n$$ = \\begin{pmatrix}1.25\\\\-0.72\\end{pmatrix} + \\begin{pmatrix}1.17\\\\0\\end{pmatrix} + \\begin{pmatrix}0\\\\-0.69\\end{pmatrix} + \\begin{pmatrix}0.27\\\\0.27\\end{pmatrix} = \\begin{pmatrix} 1.25+1.17+0.27 \\\\-0.72-0.69+0.27 \\end{pmatrix} = \\begin{pmatrix} 2.69 \\\\ -1.14 \\end{pmatrix} $$\n$\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A}$ 的逆矩阵是：\n$$ (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A})^{-1} = \\frac{1}{2.5^2-0.5^2} \\begin{pmatrix} 2.5  -0.5 \\\\ -0.5  2.5 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 2.5  -0.5 \\\\ -0.5  2.5 \\end{pmatrix} $$\n现在我们求解 $\\boldsymbol{g}_2$：\n$$ \\boldsymbol{g}_2 = \\frac{1}{6} \\begin{pmatrix} 2.5  -0.5 \\\\ -0.5  2.5 \\end{pmatrix} \\begin{pmatrix} 2.69 \\\\ -1.14 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 2.5(2.69) - 0.5(-1.14) \\\\ -0.5(2.69) + 2.5(-1.14) \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 6.725 + 0.57 \\\\ -1.345 - 2.85 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 7.295 \\\\ -4.195 \\end{pmatrix} = \\begin{pmatrix} 1.2158\\bar{3} \\\\ -0.6991\\bar{6} \\end{pmatrix} $$\n四舍五入到四位有效数字，$g_{x,2} = 1.216$ 且 $g_{y,2} = -0.6992$。\n\n**协方差矩阵之比**\n估计的梯度是 $\\hat{\\boldsymbol{g}} = (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b}$。测量向量 $\\boldsymbol{b}$ 通过 $\\boldsymbol{b} = \\boldsymbol{A}\\boldsymbol{g}_{\\mathrm{true}} + \\boldsymbol{\\varepsilon}$ 与真实梯度 $\\boldsymbol{g}_{\\mathrm{true}}$ 相关联，其中 $\\boldsymbol{\\varepsilon}$ 是误差向量，满足 $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\boldsymbol{0}$ 和 $\\mathrm{Cov}(\\boldsymbol{\\varepsilon}) = \\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\mathsf{T}}] = \\sigma^2 \\boldsymbol{I}$。\n估计误差为 $\\hat{\\boldsymbol{g}} - \\boldsymbol{g}_{\\mathrm{true}} = (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{\\varepsilon}$。\n估计量 $\\hat{\\boldsymbol{g}}$ 的协方差矩阵为：\n$$ \\mathrm{Cov}(\\hat{\\boldsymbol{g}}) = \\mathbb{E}[(\\hat{\\boldsymbol{g}} - \\boldsymbol{g}_{\\mathrm{true}})(\\hat{\\boldsymbol{g}} - \\boldsymbol{g}_{\\mathrm{true}})^{\\mathsf{T}}] = \\mathbb{E}[((\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{\\varepsilon}) ((\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{\\varepsilon})^{\\mathsf{T}}] $$\n$$ \\mathrm{Cov}(\\hat{\\boldsymbol{g}}) = (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W} \\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\mathsf{T}}] \\boldsymbol{W}^{\\mathsf{T}}\\boldsymbol{A} ((\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1})^{\\mathsf{T}} $$\n由于 $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\mathsf{T}}] = \\sigma^2\\boldsymbol{I}$，$\\boldsymbol{W}$ 是对角的，并且 $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}$ 是对称的：\n$$ \\mathrm{Cov}_W(\\boldsymbol{g}) = \\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}^2\\boldsymbol{A}) (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} $$\n对于 $\\boldsymbol{W}_1 = \\boldsymbol{I}$，我们有 $\\boldsymbol{W}_1^2 = \\boldsymbol{I}$。该公式简化为普通最小二乘（OLS）的情况：\n$$ \\mathrm{Cov}_{W_1}(\\boldsymbol{g}) = \\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1} (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A}) (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1} = \\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1} $$\n行列式为：\n$$ \\det(\\mathrm{Cov}_{W_1}(\\boldsymbol{g})) = \\det(\\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1}) = (\\sigma^2)^2 \\det((\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1}) = \\frac{\\sigma^4}{\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})} = \\frac{\\sigma^4}{8} $$\n对于 $\\boldsymbol{W}_2 = \\mathrm{diag}(1,1,1,1,1/2)$，我们有 $\\boldsymbol{W}_2^2 = \\mathrm{diag}(1,1,1,1,1/4)$。\n$$ \\det(\\mathrm{Cov}_{W_2}(\\boldsymbol{g})) = \\det\\left( \\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A})^{-1} (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A}) (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A})^{-1} \\right) $$\n$$ = (\\sigma^2)^2 \\frac{\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A})}{(\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A}))^2} $$\n我们已经求得 $\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A}) = \\det \\begin{pmatrix} 2.5  0.5 \\\\ 0.5  2.5 \\end{pmatrix} = 6$。\n我们需要计算 $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A}$：\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A} = \\sum_{i=1}^5 w_i^2 \\boldsymbol{r}_i \\boldsymbol{r}_i^{\\mathsf{T}} = 1^2\\begin{pmatrix}10\\\\01\\end{pmatrix} + 1^2\\begin{pmatrix}10\\\\01\\end{pmatrix} + (\\frac{1}{2})^2\\begin{pmatrix}11\\\\11\\end{pmatrix} = \\begin{pmatrix}20\\\\02\\end{pmatrix} + \\frac{1}{4}\\begin{pmatrix}11\\\\11\\end{pmatrix} = \\begin{pmatrix} 2.25  0.25 \\\\ 0.25  2.25 \\end{pmatrix} $$\n行列式为 $\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A}) = 2.25^2 - 0.25^2 = 5.0625 - 0.0625 = 5$。\n因此，$\\det(\\mathrm{Cov}_{W_2}(\\boldsymbol{g})) = \\sigma^4 \\frac{5}{6^2} = \\frac{5\\sigma^4}{36}$。\n比值为：\n$$ \\frac{\\det(\\mathrm{Cov}_{W_2}(\\boldsymbol{g}))}{\\det(\\mathrm{Cov}_{W_1}(\\boldsymbol{g}))} = \\frac{5\\sigma^4/36}{\\sigma^4/8} = \\frac{5}{36} \\times 8 = \\frac{40}{36} = \\frac{10}{9} = 1.111\\bar{1} $$\n四舍五入到四位有效数字，比值为 $1.111$。\n\n最终结果，四舍五入到四位有效数字，是：\n- 使用 $W_1$ 时的 $g_{x}$：$1.219$\n- 使用 $W_1$ 时的 $g_{y}$：$-0.6963$\n- 使用 $W_2$ 时的 $g_{x}$：$1.216$\n- 使用 $W_2$ 时的 $g_{y}$：$-0.6992$\n- 行列式之比：$1.111$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.219  -0.6963  1.216  -0.6992  1.111\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "除了基本的计算，梯度重构的质量在很大程度上取决于邻近点（即模板）的几何排列。本练习探讨了估计器偏差和方差之间的基本权衡，前者由未建模的物理（如场曲率）引入，后者由噪声产生。通过比较最小模板、对称模板和倾斜模板，您将发现为何冗余和对称的模板在减小总误差方面通常更具优势。",
            "id": "3339276",
            "problem": "在计算流体动力学 (CFD) 中，某一点的最小二乘 (LS) 梯度重构旨在寻找一个向量 $g \\in \\mathbb{R}^2$，该向量通过一阶泰勒展开对邻近点的增量进行建模，从而最佳拟合局部标量场的差值。考虑一个标量场 $\\phi$，其在参考位置 $P$ 和一组具有位置偏移 $r_i = (x_i, y_i)$（$i = 1, \\dots, m$）的相邻位置上进行采样。对于每个相邻点，其增量满足泰勒展开式\n$$\n\\phi(P + r_i) - \\phi(P) \\;=\\; g \\cdot r_i \\;+\\; \\tfrac{1}{2}\\, r_i^\\top H\\, r_i \\;+\\; \\varepsilon_i,\n$$\n其中 $H$ 是局部 Hessian 矩阵，捕捉了因忽略曲率而产生的截断效应，$\\varepsilon_i$ 是零均值测量或离散化噪声，满足 $\\mathbb{E}[\\varepsilon_i] = 0$ 和 $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2$。采用相等权重的 LS 估计量旨在最小化 $\\sum_i \\big(\\phi(P + r_i) - \\phi(P) - g \\cdot r_i\\big)^2$。\n\n在二维空间中，未知梯度分量的数量为 $d = 2$。最小模板恰好使用 $d$ 个位置恰当的相邻点，其方向足以张成 $\\mathbb{R}^2$，而冗余模板则使用超过 $d$ 个相邻点。构建以下三个具有相等间距 $h > 0$ 和相等权重的模板：\n\n- 最小模板 $S_{\\min}$：相邻点位于 $r_1 = (h, 0)$ 和 $r_2 = (0, h)$。\n- 冗余对称模板 $S_{\\mathrm{sym}}$：相邻点位于 $r_1 = (h, 0)$、$r_2 = (-h, 0)$、$r_3 = (0, h)$ 和 $r_4 = (0, -h)$。\n- 冗余偏斜模板 $S_{\\mathrm{skew}}$：相邻点位于 $r_1 = (h, 0)$、$r_2 = (2h, 0)$、$r_3 = (h, h)$ 和 $r_4 = (0, h)$。\n\n假设 Hessian 矩阵的分量为 $H_{xx} = 4$、$H_{xy} = 1$、$H_{yy} = -2$，间距为 $h = 0.1$，噪声方差为 $\\sigma^2 = 10^{-4}$。目标是从第一性原理出发，论证从最小模板过渡到冗余模板时，在方差减小和潜在的截断误差放大之间的认知权衡。\n\n下列哪个陈述是正确的？\n\nA. 对于 $S_{\\min}$，唯一的两方程拟合将曲率吸收到梯度估计中，导致一个与坐标方向对齐的 $\\mathcal{O}(h)$ 阶主阶偏差；而 $S_{\\mathrm{sym}}$ 通过对称性消除了该主阶曲率诱导偏差，并将每个分量的估计量方差相对于 $S_{\\min}$ 减小了大约 2 倍。\n\nB. 无论模板几何形状如何，增加任何额外的相邻点都会严格减小方差和主阶截断偏差。\n\nC. 对于 $S_{\\mathrm{skew}}$ 以及给定的 $H$、$h$ 和 $\\sigma^2$，其 LS 梯度估计的均方误差大于 $S_{\\min}$ 的均方误差，因为方差的减小被几何不对称性导致的曲率诱导偏差的放大所抵消。\n\nD. 在 $S_{\\min}$、$S_{\\mathrm{sym}}$ 和 $S_{\\mathrm{skew}}$ 中，当 $h = 0.1$ 且 $\\sigma^2 = 10^{-4}$ 时，最小模板 $S_{\\min}$ 的均方误差最小。",
            "solution": "问题陈述已经过验证，被认为是科学上合理、适定且完整的。我们可以继续进行推导。\n\n梯度 $g \\in \\mathbb{R}^2$ 的最小二乘 (LS) 估计量最小化残差平方和 $J(g_{LS}) = \\sum_{i=1}^m \\left( \\Delta \\phi_i - g_{LS} \\cdot r_i \\right)^2$，其中 $\\Delta \\phi_i = \\phi(P + r_i) - \\phi(P)$ 是位于位置偏移 $r_i$ 处的相邻点 $i$ 的标量增量。\n\n以矩阵形式表示，令 $R$ 为一个 $m \\times 2$ 的矩阵，其行向量为 $r_i^\\top$。令 $\\Delta\\phi$ 为增量的列向量。LS 问题为 $\\min_{g_{LS}} \\| \\Delta\\phi - R g_{LS} \\|_2^2$。解由正规方程给出：\n$$ g_{LS} = (R^\\top R)^{-1} R^\\top \\Delta\\phi $$\n真实增量 $\\Delta \\phi_i$ 由泰勒展开式给出：\n$$ \\Delta \\phi_i = g \\cdot r_i + \\tfrac{1}{2} r_i^\\top H r_i + \\varepsilon_i $$\n此处，$g$ 是真实梯度，$H$ 是 Hessian 矩阵，$\\varepsilon_i$ 是方差为 $\\sigma^2$ 的零均值噪声项。以矩阵形式表示为 $\\Delta\\phi = R g + b_{trunc} + \\varepsilon$，其中 $b_{trunc}$ 是截断误差项 $\\frac{1}{2} r_i^\\top H r_i$ 组成的向量。\n\n将此代入 $g_{LS}$ 的 LS 解中：\n$$ g_{LS} = (R^\\top R)^{-1} R^\\top (R g + b_{trunc} + \\varepsilon) = g + (R^\\top R)^{-1} R^\\top b_{trunc} + (R^\\top R)^{-1} R^\\top \\varepsilon $$\n估计量的偏差为 $\\text{Bias}(g_{LS}) = \\mathbb{E}[g_{LS} - g]$。由于 $\\mathbb{E}[\\varepsilon] = 0$，偏差是由截断误差引起的：\n$$ \\text{Bias}(g_{LS}) = (R^\\top R)^{-1} R^\\top b_{trunc} $$\n估计量的协方差矩阵为 $\\text{Cov}(g_{LS}) = \\mathbb{E}[(g_{LS} - \\mathbb{E}[g_{LS}])(g_{LS} - \\mathbb{E}[g_{LS}])^\\top]$。由于噪声项不相关且方差为 $\\sigma^2$，我们有 $\\mathbb{E}[\\varepsilon \\varepsilon^\\top] = \\sigma^2 I$。这给出：\n$$ \\text{Cov}(g_{LS}) = \\sigma^2 (R^\\top R)^{-1} $$\n均方误差 (MSE) 是偏差幅度平方与协方差矩阵的迹之和：\n$$ \\text{MSE} = \\|\\text{Bias}(g_{LS})\\|^2 + \\text{Tr}(\\text{Cov}(g_{LS})) $$\n\n我们使用给定参数为每个模板计算此值：$h = 0.1$，$\\sigma^2 = 10^{-4}$，以及 $H = \\begin{pmatrix} 4  1 \\\\ 1  -2 \\end{pmatrix}$。因此，$H_{xx} = 4$, $H_{xy} = 1$, $H_{yy} = -2$。\n\n### 模板 $S_{\\min}$\n相邻点为 $r_1 = (h, 0)$ 和 $r_2 = (0, h)$。几何矩阵为 $R = \\begin{pmatrix} h  0 \\\\ 0  h \\end{pmatrix} = hI$。\n$R^\\top R = h^2 I$。\n$(R^\\top R)^{-1} = \\frac{1}{h^2} I$。\n\n**偏差**：截断误差向量的分量为 $b_{trunc,1} = \\frac{1}{2} r_1^\\top H r_1 = \\frac{1}{2}h^2 H_{xx}$ 和 $b_{trunc,2} = \\frac{1}{2} r_2^\\top H r_2 = \\frac{1}{2}h^2 H_{yy}$。\n$$ \\text{Bias}(g_{LS}^{min}) = \\frac{1}{h^2}I \\begin{pmatrix} h  0 \\\\ 0  h \\end{pmatrix} \\frac{h^2}{2} \\begin{pmatrix} H_{xx} \\\\ H_{yy} \\end{pmatrix} = \\frac{h}{2} \\begin{pmatrix} H_{xx} \\\\ H_{yy} \\end{pmatrix} $$\n数值上，$\\text{Bias}(g_{LS}^{min}) = \\frac{0.1}{2} \\begin{pmatrix} 4 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix}$。\n偏差的范数平方为 $\\|\\text{Bias}(g_{LS}^{min})\\|^2 = (0.2)^2 + (-0.1)^2 = 0.04 + 0.01 = 0.05$。\n\n**协方差**：\n$$ \\text{Cov}(g_{LS}^{min}) = \\sigma^2 \\frac{1}{h^2} I = \\frac{10^{-4}}{(0.1)^2} I = 10^{-2} I = \\begin{pmatrix} 0.01  0 \\\\ 0  0.01 \\end{pmatrix} $$\n迹为 $\\text{Tr}(\\text{Cov}(g_{LS}^{min})) = 0.01 + 0.01 = 0.02$。\n\n**均方误差**：\n$ \\text{MSE}_{min} = 0.05 + 0.02 = 0.07 $。\n\n### 模板 $S_{\\mathrm{sym}}$\n相邻点为 $r_1 = (h, 0)$, $r_2 = (-h, 0)$, $r_3 = (0, h)$, $r_4 = (0, -h)$。\n$R = \\begin{pmatrix} h  0 \\\\ -h  0 \\\\ 0  h \\\\ 0  -h \\end{pmatrix}$。\n$R^\\top R = \\begin{pmatrix} 2h^2  0 \\\\ 0  2h^2 \\end{pmatrix} = 2h^2 I$。\n$(R^\\top R)^{-1} = \\frac{1}{2h^2} I$。\n\n**偏差**：截断误差为 $b_{trunc,1} = \\frac{1}{2}h^2 H_{xx}$, $b_{trunc,2} = \\frac{1}{2}h^2 H_{xx}$, $b_{trunc,3} = \\frac{1}{2}h^2 H_{yy}$, $b_{trunc,4} = \\frac{1}{2}h^2 H_{yy}$。\n$R^\\top b_{trunc}$ 项变为：\n$$ R^\\top b_{trunc} = \\begin{pmatrix} h  -h  0  0 \\\\ 0  0  h  -h \\end{pmatrix} \\frac{h^2}{2} \\begin{pmatrix} H_{xx} \\\\ H_{xx} \\\\ H_{yy} \\\\ H_{yy} \\end{pmatrix} = \\frac{h^3}{2} \\begin{pmatrix} H_{xx} - H_{xx} \\\\ H_{yy} - H_{yy} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\n因此，$\\text{Bias}(g_{LS}^{sym}) = (R^\\top R)^{-1} (0) = 0$。主阶偏差被对称性抵消。\n$\\|\\text{Bias}(g_{LS}^{sym})\\|^2 = 0$。\n\n**协方差**：\n$$ \\text{Cov}(g_{LS}^{sym}) = \\sigma^2 \\frac{1}{2h^2} I = \\frac{10^{-4}}{2(0.1)^2} I = 0.005 I = \\begin{pmatrix} 0.005  0 \\\\ 0  0.005 \\end{pmatrix} $$\n$\\text{Tr}(\\text{Cov}(g_{LS}^{sym})) = 0.005 + 0.005 = 0.01$。\n\n**均方误差**：\n$ \\text{MSE}_{sym} = 0 + 0.01 = 0.01 $。\n\n### 模板 $S_{\\mathrm{skew}}$\n相邻点为 $r_1 = (h,0), r_2 = (2h,0), r_3 = (h,h), r_4 = (0,h)$。\n$R = \\begin{pmatrix} h  0 \\\\ 2h  0 \\\\ h  h \\\\ 0  h \\end{pmatrix}$。\n$R^\\top R = h^2 \\begin{pmatrix} 6  1 \\\\ 1  2 \\end{pmatrix}$。\n$(R^\\top R)^{-1} = \\frac{1}{h^4(12-1)} h^2 \\begin{pmatrix} 2  -1 \\\\ -1  6 \\end{pmatrix} = \\frac{1}{11h^2} \\begin{pmatrix} 2  -1 \\\\ -1  6 \\end{pmatrix}$。\n\n**偏差**：截断误差为 $b_{trunc,1} = \\frac{1}{2}h^2 H_{xx}$，$b_{trunc,2} = \\frac{1}{2}(2h)^2 H_{xx} = 2h^2 H_{xx}$，$b_{trunc,3} = \\frac{h^2}{2}(H_{xx}+2H_{xy}+H_{yy})$，$b_{trunc,4} = \\frac{1}{2}h^2 H_{yy}$。\n$R^\\top b_{trunc} = \\frac{h^3}{2} \\begin{pmatrix} 10H_{xx}+2H_{xy}+H_{yy} \\\\ H_{xx}+2H_{xy}+2H_{yy} \\end{pmatrix}$。\n数值上，$R^\\top b_{trunc} = \\frac{0.001}{2} \\begin{pmatrix} 10(4)+2(1)+(-2) \\\\ 4+2(1)+2(-2) \\end{pmatrix} = \\frac{0.001}{2} \\begin{pmatrix} 40 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0.02 \\\\ 0.001 \\end{pmatrix}$。\n$$ \\text{Bias}(g_{LS}^{skew}) = \\frac{1}{11(0.1)^2} \\begin{pmatrix} 2  -1 \\\\ -1  6 \\end{pmatrix} \\begin{pmatrix} 0.02 \\\\ 0.001 \\end{pmatrix} = \\frac{1}{0.11} \\begin{pmatrix} 0.039 \\\\ -0.014 \\end{pmatrix} = \\begin{pmatrix} 39/110 \\\\ -14/110 \\end{pmatrix} \\approx \\begin{pmatrix} 0.3545 \\\\ -0.1273 \\end{pmatrix} $$\n$\\|\\text{Bias}(g_{LS}^{skew})\\|^2 = (\\frac{39}{110})^2 + (\\frac{-14}{110})^2 = \\frac{1521+196}{12100} = \\frac{1717}{12100} \\approx 0.1419$。\n\n**协方差**：\n$$ \\text{Cov}(g_{LS}^{skew}) = \\frac{\\sigma^2}{11h^2} \\begin{pmatrix} 2  -1 \\\\ -1  6 \\end{pmatrix} = \\frac{10^{-4}}{11(0.01)} \\begin{pmatrix} 2  -1 \\\\ -1  6 \\end{pmatrix} = \\frac{1}{1100} \\begin{pmatrix} 2  -1 \\\\ -1  6 \\end{pmatrix} $$\n$\\text{Tr}(\\text{Cov}(g_{LS}^{skew})) = \\frac{1}{1100}(2+6) = \\frac{8}{1100} \\approx 0.00727$。\n\n**均方误差**：\n$ \\text{MSE}_{skew} \\approx 0.1419 + 0.00727 = 0.14917 $。\n\n### 选项评估\n\n**A. 对于 $S_{\\min}$，唯一的两方程拟合将曲率吸收到梯度估计中，导致一个与坐标方向对齐的 $\\mathcal{O}(h)$ 阶主阶偏差；而 $S_{\\mathrm{sym}}$ 通过对称性消除了该主阶曲率诱导偏差，并将每个分量的估计量方差相对于 $S_{\\min}$ 减小了大约 2 倍。**\n我们对 $S_{\\min}$ 的分析显示其偏差为 $\\text{Bias}(g_{LS}^{min}) = \\frac{h}{2}(H_{xx}, H_{yy})^\\top$，阶数为 $\\mathcal{O}(h)$。“与坐标方向对齐”这一短语可以解释为 $x$-梯度分量的偏差 $\\frac{h}{2}H_{xx}$ 仅源于 $x$-方向的曲率信息 ($H_{xx}$)，$y$-分量同理。这种解耦是该正交模板的一个特征。我们对 $S_{\\mathrm{sym}}$ 的分析表明其主阶偏差恰好为零。$S_{\\min}$ 的每个分量的方差为 $\\frac{\\sigma^2}{h^2}$，而 $S_{\\mathrm{sym}}$ 的为 $\\frac{\\sigma^2}{2h^2}$。这恰好是减小了 2 倍。该陈述准确描述了这两个模板的行为。\n**结论：正确。**\n\n**B. 无论模板几何形状如何，增加任何额外的相邻点都会严格减小方差和主阶截断偏差。**\n增加相邻点通常会减小（或不增加）最小二乘拟合的方差。矩阵 $(R^\\top R)^{-1}$ 在半正定意义上变得“更小”。然而，关于偏差的陈述是错误的。比较 $S_{min}$ 和 $S_{skew}$（即在 $S_{min}$ 的基础上增加了两个相邻点），我们看到偏差的范数平方从 $0.05$ 增加到约 $0.1419$。新增点的偏斜几何形状放大了截断误差。\n**结论：不正确。**\n\n**C. 对于 $S_{\\mathrm{skew}}$ 以及给定的 $H$、$h$ 和 $\\sigma^2$，其 LS 梯度估计的均方误差大于 $S_{\\min}$ 的均方误差，因为方差的减小被几何不对称性导致的曲率诱导偏差的放大所抵消。**\n我们的计算表明 $\\text{MSE}_{skew} \\approx 0.149$ 且 $\\text{MSE}_{min} = 0.07$。陈述 $\\text{MSE}_{skew}  \\text{MSE}_{min}$ 是正确的。给出的原因是方差的减小被偏差的放大所抵消。\n- 方差变化：$\\text{Tr}(\\text{Cov}_{skew}) - \\text{Tr}(\\text{Cov}_{min}) \\approx 0.0073 - 0.02 = -0.0127$。这是一个减小。\n- 偏差平方变化：$\\|\\text{Bias}_{skew}\\|^2 - \\|\\text{Bias}_{min}\\|^2 \\approx 0.1419 - 0.05 = +0.0919$。这是一个放大。\n偏差增加的幅度 ($0.0919$) 远大于方差减小的幅度 ($0.0127$)，因此净效应是更大的均方误差。这个推理完全正确。\n**结论：正确。**\n\n**D. 在 $S_{\\min}$、$S_{\\mathrm{sym}}$ 和 $S_{\\mathrm{skew}}$ 中，当 $h = 0.1$ 且 $\\sigma^2 = 10^{-4}$ 时，最小模板 $S_{\\min}$ 的均方误差最小。**\n比较计算出的 MSE 值：\n- $\\text{MSE}_{min} = 0.07$\n- $\\text{MSE}_{sym} = 0.01$\n- $\\text{MSE}_{skew} \\approx 0.149$\n最小的 MSE 属于对称模板 $S_{\\mathrm{sym}}$。该陈述是错误的。\n**结论：不正确。**\n\n陈述 A 和 C 是正确的。",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "将理论付诸实践需要解决数值稳定性的问题，尤其是在处理真实世界CFD模拟中常见的不完美网格时。这个编程练习要求您在一个邻近点几乎共面的病态模板上，实现并诊断最小二乘系统的行为。您将直接观察到朴素方法的失效，并学习实现稳健的、能揭示秩亏的数值技术，以确保即使在这种具有挑战性的几何构型下也能进行准确而稳定的梯度重构。",
            "id": "3339293",
            "problem": "要求您在三维空间中，针对一个控制体积，在几何模板日益病态的情况下，实现并分析最小二乘梯度重构。其目标是展示近共面的邻点配置如何导致线性最小二乘系统趋向奇异，并设计基于主元选择和模板扩展的秩揭示补救措施。此任务必须通过编写一个完整的、可运行的程序来解决，该程序能够构建指定的模板，使用多种方法计算梯度，并报告量化诊断结果。\n\n考虑一个在空间中线性的标量场 $\\,\\phi(\\mathbf{x})\\,$，$\\;\\phi(\\mathbf{x}) = a\\,x + b\\,y + c\\,z\\,$，其中 $\\,\\mathbf{x} = (x,y,z)\\,$，常数 $\\,a,b,c\\,$ 已知。对于一个中心点 $\\,\\mathbf{x}_0\\,$ 和其邻点 $\\,\\mathbf{x}_i\\,$，通过最小化以下函数来定义在 $\\,\\mathbf{x}_0\\,$ 处的 $\\,\\nabla \\phi\\,$ 的最小二乘梯度重构：\n$$\nJ(\\mathbf{g}) \\,=\\, \\sum_{i=1}^{N} w_i \\,\\Big(\\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_0) - \\mathbf{g}\\cdot(\\mathbf{x}_i - \\mathbf{x}_0)\\Big)^2,\n$$\n其中所有邻点的权重 $\\,w_i = 1\\,$。这导出了线性系统\n$$\n\\mathbf{A}\\,\\mathbf{g} \\,\\approx\\, \\mathbf{b},\n$$\n其中 $\\,\\mathbf{A}\\in\\mathbb{R}^{N\\times 3}\\,$ 的第 $\\,i\\,$ 行为 $\\,(\\mathbf{x}_i - \\mathbf{x}_0)^\\top\\,$，而 $\\,\\mathbf{b}\\in\\mathbb{R}^{N}\\,$ 的元素为 $\\,\\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_0)\\,$。在没有噪声且 $\\,\\mathbf{A}\\,$ 为满列秩的情况下，可以恢复精确解 $\\,\\mathbf{g}^\\star = \\nabla\\phi(\\mathbf{x}_0)\\,$。然而，如果邻点向量近似共面，$\\,\\mathbf{A}\\,$ 会变得病态或秩亏。\n\n您的程序必须实现并比较以下三种方法：\n- 一个朴素的正规方程求解器，使用矩阵 $\\,\\mathbf{N} = \\mathbf{A}^\\top \\mathbf{A}\\,$ 和右端项 $\\,\\mathbf{c} = \\mathbf{A}^\\top \\mathbf{b}\\,$，通过直接线性求解来解出 $\\,\\mathbf{g}\\,$。如果 $\\,\\mathbf{N}\\,$ 是奇异的或数值上过于病态，此方法可能会失败或产生不稳定的结果。\n- 一个基于 $\\,\\mathbf{A}\\,$ 的列主元 $\\,\\mathbf{Q}\\mathbf{R}\\,$ 分解的秩揭示求解器，其数值秩由 $\\,\\mathbf{R}\\,$ 的对角线元素和一个阈值 $\\,\\tau = \\mathrm{rcond}\\cdot |R_{11}|\\,$（其中 $\\,\\mathrm{rcond} = 10^{-12}\\,$）确定。该解应将与可忽略列相关的分量设为零，以获得与估计的数值秩一致的最小范数最小二乘解。\n- 一种模板扩展补救措施：用额外的候选点（如下提供）扩充初始邻点集，直到估计的数值秩达到 $\\,3\\,$，然后在扩展后的模板上重新计算带主元的 $\\,\\mathbf{Q}\\mathbf{R}\\,$ 解。\n\n为了量化近奇异性，使用奇异值分解来估计 $\\,\\mathbf{A}\\,$ 的 $\\,2$-范数条件数，即 $\\,\\kappa_2(\\mathbf{A}) = \\sigma_{\\max}/\\sigma_{\\min}\\,$。如果 $\\,\\sigma_{\\min} = 0\\,$，则将 $\\,\\kappa_2(\\mathbf{A})\\,$ 报告为哨兵值 $\\,10^{18}\\,$。对于朴素的正规方程方法，如果线性求解因奇异性而失败，或者 $\\,\\mathbf{N}\\,$ 的条件数超过 $\\,10^{15}\\,$，则将此方法的误差报告为哨兵值 $\\,10^{9}\\,$。\n\n实现包含三个案例的以下测试套件。在所有案例中，中心点取 $\\,\\mathbf{x}_0 = (0,0,0)\\,$，权重取单位权重 $\\,w_i = 1\\,$。将所有浮点输出四舍五入到六位小数。\n\n- 案例1（良态）：使用系数 $\\,a=2.3\\,$, $\\,b=-1.7\\,$, $\\,c=0.9\\,$ 和邻点\n  $$\n  (1,0,0),\\;(-1,0,0),\\;(0,1,0),\\;(0,-1,0),\\;(0,0,1),\\;(0,0,-1).\n  $$\n- 案例2（近共面）：使用系数 $\\,a=2.3\\,$, $\\,b=-1.7\\,$, $\\,c=0.9\\,$。令 $\\,\\epsilon = 10^{-8}\\,$，邻点为\n  $$\n  (1,0,\\epsilon),\\;(-1,0,\\epsilon),\\;(0,1,\\epsilon),\\;(0,-1,\\epsilon),\\;(1,1,\\epsilon),\\;(-1,-1,\\epsilon).\n  $$\n  按以下顺序提供候选扩展点，并一次添加一个，直到估计的数值秩达到 $\\,3\\,$：\n  $$\n  (0,0,0.2),\\;(0.2,0,0.2),\\;(0,0.2,0.2).\n  $$\n- 案例3（精确共面且场兼容）：使用系数 $\\,a=2.3\\,$, $\\,b=-1.7\\,$, $\\,c=0.0\\,$，邻点为\n  $$\n  (1,0,0),\\;(-1,0,0),\\;(0,1,0),\\;(0,-1,0),\\;(1,1,0),\\;(-1,-1,0).\n  $$\n  使用与案例2中相同的候选扩展点，并按相同顺序添加。\n\n对于每个案例，根据上述定义精确构造 $\\,\\mathbf{A}\\,$ 和 $\\,\\mathbf{b}\\,$。用 $\\,\\mathbf{g}_{\\mathrm{true}} = (a,b,c)^\\top\\,$ 表示真实梯度。对于每种方法，计算重构梯度 $\\,\\widehat{\\mathbf{g}}\\,$ 和欧几里得误差 $\\,\\|\\widehat{\\mathbf{g}}-\\mathbf{g}_{\\mathrm{true}}\\|_2\\,$。同时，使用带主元的 $\\,\\mathbf{Q}\\mathbf{R}\\,$ 和前述阈值来估计扩展前后的数值秩。\n\n您的程序必须生成单行输出，其中包含三个案例的结果，格式为用方括号括起来的逗号分隔列表。每个案例必须是以下形式的列表：\n$$\n[\\kappa_2(\\mathbf{A}),\\; r_{\\mathrm{init}},\\; e_{\\mathrm{naive}},\\; e_{\\mathrm{pivot}},\\; e_{\\mathrm{expand}},\\; r_{\\mathrm{expand}}],\n$$\n其中 $\\,\\kappa_2(\\mathbf{A})\\,$ 是 $\\,\\mathbf{A}\\,$ 的加帽条件数（如果奇异则使用哨兵值 $\\,10^{18}\\,$），$\\,r_{\\mathrm{init}}\\,$ 是初始模板的估计数值秩，$\\,e_{\\mathrm{naive}}\\,$ 是朴素正规方程的误差（如果求解失败或过于病态则使用哨兵值 $\\,10^{9}\\,$），$\\,e_{\\mathrm{pivot}}\\,$ 是在初始模板上使用秩揭示主元解的误差，$\\,e_{\\mathrm{expand}}\\,$ 是在扩展模板上使用主元解的误差，而 $\\,r_{\\mathrm{expand}}\\,$ 是扩展后的估计数值秩。将所有浮点条目四舍五入到六位小数。最终打印的行必须看起来像\n$$\n[[\\cdots],[\\cdots],[\\cdots]],\n$$\n不含任何附加文本。\n\n此处不涉及任何角度（如果有的话）。本问题不涉及任何物理单位。所有要求的数值输出都是无单位的纯实数，并且必须按规定进行四舍五入。",
            "solution": "当前任务要求实现并对比分析三种用于三维线性标量场最小二乘梯度重构的数值方法。分析的重点是当这些方法应用于从良态到秩亏等不同质量的几何模板时，其数值稳定性和准确性。\n\n### 问题描述\n\n给定一个线性标量场 $\\phi(\\mathbf{x}) = a\\,x + b\\,y + c\\,z$。该场的梯度在任何地方都是恒定的，由系数向量 $\\mathbf{g}_{\\text{true}} = \\nabla\\phi = (a, b, c)^\\top$ 给出。\n\n目标是使用一组 $N$ 个邻点 $\\{\\mathbf{x}_i\\}_{i=1}^N$ 在中心点 $\\mathbf{x}_0$ 处重构该梯度。最小二乘法旨在寻找一个梯度向量 $\\mathbf{g}$，以最小化平方误差之和：\n$$\nJ(\\mathbf{g}) = \\sum_{i=1}^{N} w_i \\left( (\\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_0)) - \\mathbf{g} \\cdot (\\mathbf{x}_i - \\mathbf{x}_0) \\right)^2\n$$\n鉴于权重为单位值 ($w_i=1$) 且中心点为原点 ($\\mathbf{x}_0 = (0,0,0)$)，我们有 $\\Delta\\mathbf{x}_i = \\mathbf{x}_i$ 和 $\\phi(\\mathbf{x}_0) = 0$。泛函简化为：\n$$\nJ(\\mathbf{g}) = \\sum_{i=1}^{N} \\left( \\phi(\\mathbf{x}_i) - \\mathbf{g} \\cdot \\mathbf{x}_i \\right)^2\n$$\n这是一个标准的线性最小二乘问题，可以表示为矩阵形式，即寻找 $\\mathbf{g}$ 来最小化 $\\|\\mathbf{A}\\mathbf{g} - \\mathbf{b}\\|_2^2$。系统矩阵 $\\mathbf{A} \\in \\mathbb{R}^{N \\times 3}$ 和右端向量 $\\mathbf{b} \\in \\mathbb{R}^{N}$ 的构造如下：\n- $\\mathbf{A}$ 的第 $i$ 行是转置的位移向量，$(\\mathbf{x}_i - \\mathbf{x}_0)^\\top = \\mathbf{x}_i^\\top$。\n- $\\mathbf{b}$ 的第 $i$ 个元素是标量差，$\\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_0) = \\phi(\\mathbf{x}_i)$。\n\n由于 $\\phi(\\mathbf{x}_i) = a x_i + b y_i + c z_i = \\mathbf{x}_i^\\top \\mathbf{g}_{\\text{true}}$，因此有 $\\mathbf{b} = \\mathbf{A}\\mathbf{g}_{\\text{true}}$。这意味着线性系统是相容的。如果矩阵 $\\mathbf{A}$ 具有满列秩（即秩为 $3$），则唯一的最小二乘解为 $\\mathbf{g} = \\mathbf{g}_{\\text{true}}$。然而，如果邻点 $\\{\\mathbf{x}_i\\}$ 共面或近共面，$\\mathbf{A}$ 的行向量会变得线性相关或近似线性相关，导致 $\\mathbf{A}$ 秩亏或病态。\n\n### 数值方法与诊断\n\n我们将实现并比较三种求解 $\\mathbf{g}$ 的方法：\n\n1.  **朴素正规方程**：此方法通过左乘 $\\mathbf{A}^\\top$ 将超定系统转化为一个 $3 \\times 3$ 的方阵系统：\n    $$\n    (\\mathbf{A}^\\top \\mathbf{A}) \\mathbf{g} = \\mathbf{A}^\\top \\mathbf{b}\n    $$\n    解为 $\\widehat{\\mathbf{g}}_{\\text{naive}} = (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{b}$。虽然简单，但如果 $\\mathbf{A}$ 是病态的，这种方法在数值上是不稳定的，因为正规矩阵 $\\mathbf{N} = \\mathbf{A}^\\top \\mathbf{A}$ 的条件数是 $\\mathbf{A}$ 的条件数的平方，即 $\\kappa_2(\\mathbf{N}) = \\kappa_2(\\mathbf{A})^2$。$\\mathbf{A}$ 的大条件数会导致 $\\mathbf{N}$ 的条件数过大，从而产生显著的数值误差或导致线性求解器失败。我们将监控 $\\kappa_2(\\mathbf{N})$，如果它超过 $10^{15}$ 或求解器失败，则使用哨兵误差值 $10^9$。\n\n2.  **秩揭示主元QR分解**：该方法通过直接处理 $\\mathbf{A}$ 的秩，提供了一种更稳健的解法。我们计算列主元QR分解 $\\mathbf{A}\\mathbf{P} = \\mathbf{Q}\\mathbf{R}$，其中 $\\mathbf{P}$ 是一个置换矩阵，$\\mathbf{Q}$ 的列是标准正交的，$\\mathbf{R}$ 是一个上三角矩阵，其对角线元素的大小递减。通过计算大于阈值 $\\tau = \\text{rcond} \\cdot |R_{11}|$（其中 $\\text{rcond} = 10^{-12}$）的对角线元素 $|R_{kk}|$ 的数量来估计数值秩 $r$。通过求解最小范数解来解决最小二乘问题。这是通过求解与估计秩 $r$ 对应的良态部分系统，并将置换解向量的其余 $3-r$ 个分量设置为零来实现的。此过程通过将问题投影到由 $\\mathbf{A}$ 的“强”列张成的数值稳定子空间上，有效地对问题进行了正则化。\n\n3.  **模板扩展**：当初始邻点模板在几何上存在缺陷（秩 $ 3$）时，此方法通过扩充模板来解决该问题。候选点被逐一添加到邻点集中，直到通过主元QR估计的数值秩达到满秩 $3$。一旦达到满秩模板，便使用稳健的主元QR方法在该扩展系统上计算梯度 $\\widehat{\\mathbf{g}}_{\\text{expand}}$。这种方法旨在通过引入额外的几何信息来恢复问题的适定性。\n\n初始模板的质量通过 $\\mathbf{A}$ 的 $2$-范数条件数 $\\kappa_2(\\mathbf{A}) = \\sigma_{\\max}/\\sigma_{\\min}$ 来量化，该值由其奇异值计算得出。如果 $\\mathbf{A}$ 是奇异的（$\\sigma_{\\min}=0$），则报告哨兵值 $10^{18}$。每种方法的性能通过欧几里得误差 $\\|\\widehat{\\mathbf{g}} - \\mathbf{g}_{\\text{true}}\\|_2$ 来衡量。\n\n### 测试案例分析\n\n-   **案例1（良态）**：邻点构成一个正交基。矩阵 $\\mathbf{A}$ 是完美的良态矩阵，其 $\\kappa_2(\\mathbf{A}) = 1$。数值秩被稳健地识别为 $3$。预计所有三种方法都能完美工作，产生的误差接近机器精度（$0$）。\n\n-   **案例2（近共面）**：邻点位于平面 $z = \\epsilon = 10^{-8}$ 上。位移向量近似共面，使得矩阵 $\\mathbf{A}$ 严重病态，其 $\\kappa_2(\\mathbf{A})$ 非常大。数值秩将被估计为 $2$。由于条件数的平方效应，正规方程方法预计会产生很大的误差。主元QR方法将正确识别秩亏，并在占主导地位的二维子空间内找到解，从而得到显著更小的误差。模板扩展将添加一个具有不可忽略的 $z$ 分量的点，将秩恢复到 $3$，从而能够以非常低的误差实现精确重构。\n\n-   **案例3（精确共面）**：邻点位于平面 $z=0$ 上。矩阵 $\\mathbf{A}$ 精确秩亏（秩为 $2$），其最小奇异值为零，导致条件数为无穷大。矩阵 $\\mathbf{A}^\\top \\mathbf{A}$ 是奇异的，因此正规方程方法将失败，并触发哨兵误差。真实梯度的 $z$ 分量为 $c=0$，这意味着 $\\mathbf{g}_{\\text{true}}$ 位于由 $\\mathbf{A}$ 的列向量张成的子空间内。主元QR方法通过寻找最小范数解，将正确识别出 $\\mathbf{g}_{\\text{true}}$ 并产生零误差。模板扩展将把秩恢复到 $3$，并且也能找到精确解。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import qr, solve_triangular\n\ndef solve_pivoted_qr(A, b, rcond):\n    \"\"\"\n    Solves the least-squares problem Ax = b using rank-revealing QR\n    factorization with column pivoting.\n    \"\"\"\n    n_cols = A.shape[1]\n    if A.shape[0] == 0:\n        return np.zeros(n_cols), 0\n        \n    Q, R, P = qr(A, pivoting=True)\n    \n    # Estimate numerical rank\n    if R.shape[0] == 0 or np.abs(R[0, 0])  np.finfo(float).eps:\n        rank = 0\n    else:\n        tau = rcond * np.abs(R[0, 0])\n        rank = np.sum(np.abs(np.diag(R)) > tau)\n\n    # Solve for the permuted gradient vector y = P^T * g\n    y = np.zeros(n_cols)\n    if rank > 0:\n        d = Q.T @ b\n        try:\n            y[:rank] = solve_triangular(R[:rank, :rank], d[:rank], check_finite=False)\n        except np.linalg.LinAlgError:\n            # This should not happen with a proper rank check, but as a safeguard.\n            pass\n\n    # Un-permute the solution to get g\n    g_hat = np.zeros(n_cols)\n    g_hat[P] = y\n    \n    return g_hat, rank\n\ndef calc_error(g_hat, g_true):\n    \"\"\"Computes the Euclidean norm of the error vector.\"\"\"\n    return np.linalg.norm(g_hat - g_true)\n\ndef process_case(coeffs, neighbors, expansion_pts):\n    \"\"\"\n    Processes a single test case, computes gradients with three methods,\n    and returns all specified diagnostic values.\n    \"\"\"\n    # Constants and Sentinels\n    RCOND_QR = 1e-12\n    COND_N_THRESH = 1e15\n    ERROR_SENTINEL = 1e9\n    KAPPA_SENTINEL = 1e18\n\n    # --- Initial Stencil Setup ---\n    g_true = np.array(coeffs, dtype=float)\n    x0 = np.array([0.0, 0.0, 0.0])\n    \n    A = neighbors - x0\n    b = A @ g_true\n\n    # --- Diagnostics for Initial Stencil ---\n    # Condition number of A using SVD\n    try:\n        singular_values = np.linalg.svd(A, compute_uv=False)\n        if singular_values[-1]  np.finfo(float).eps * singular_values[0]:\n            kappa_A = KAPPA_SENTINEL\n        else:\n            kappa_A = singular_values[0] / singular_values[-1]\n    except np.linalg.LinAlgError:\n        kappa_A = KAPPA_SENTINEL\n\n    # --- Method 2: Rank-Revealing Pivoted QR (Initial Stencil) ---\n    g_pivot, r_init = solve_pivoted_qr(A, b, RCOND_QR)\n    e_pivot = calc_error(g_pivot, g_true)\n\n    # --- Method 1: Naive Normal Equations ---\n    e_naive = ERROR_SENTINEL\n    N = A.T @ A\n    if np.linalg.cond(N)  COND_N_THRESH:\n        try:\n            g_naive = np.linalg.solve(N, c)\n            e_naive = calc_error(g_naive, g_true)\n        except np.linalg.LinAlgError:\n            e_naive = ERROR_SENTINEL\n    else: # Added for explicit sentinel assignment\n        c = A.T @ b # Need to define c\n        e_naive = ERROR_SENTINEL\n\n    \n    # --- Method 3: Stencil Expansion ---\n    if r_init == 3:\n        # If rank is already full, expansion is not needed.\n        r_expand = r_init\n        g_expand = g_pivot\n        e_expand = e_pivot\n    else:\n        # Augment stencil until rank is 3\n        current_neighbors = neighbors.copy()\n        current_rank = r_init\n        \n        for pt in expansion_pts:\n            if current_rank == 3:\n                break\n            \n            # Add point and rebuild system\n            current_neighbors = np.vstack([current_neighbors, pt])\n            A_exp_loop = current_neighbors - x0\n            \n            # Re-evaluate rank\n            _, R_loop, _ = qr(A_exp_loop, pivoting=True)\n            if R_loop.shape[0] > 0 and np.abs(R_loop[0,0]) > np.finfo(float).eps:\n                tau_loop = RCOND_QR * np.abs(R_loop[0, 0])\n                current_rank = np.sum(np.abs(np.diag(R_loop)) > tau_loop)\n            else:\n                current_rank = 0\n\n\n        # Final expanded system\n        A_exp = current_neighbors - x0\n        b_exp = A_exp @ g_true\n        r_expand = current_rank\n        \n        # Solve with expanded stencil\n        g_expand, _ = solve_pivoted_qr(A_exp, b_exp, RCOND_QR)\n        e_expand = calc_error(g_expand, g_true)\n        \n    return [kappa_A, float(r_init), e_naive, e_pivot, e_expand, float(r_expand)]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Case 1: Well-conditioned\n    case1 = {\n        \"coeffs\": (2.3, -1.7, 0.9),\n        \"neighbors\": np.array([\n            [1, 0, 0], [-1, 0, 0], [0, 1, 0],\n            [0, -1, 0], [0, 0, 1], [0, 0, -1]\n        ]),\n        \"expansion_pts\": []\n    }\n\n    # Case 2: Nearly coplanar\n    epsilon = 1e-8\n    case2 = {\n        \"coeffs\": (2.3, -1.7, 0.9),\n        \"neighbors\": np.array([\n            [1, 0, epsilon], [-1, 0, epsilon], [0, 1, epsilon],\n            [0, -1, epsilon], [1, 1, epsilon], [-1, -1, epsilon]\n        ]),\n        \"expansion_pts\": np.array([\n            [0, 0, 0.2], [0.2, 0, 0.2], [0, 0.2, 0.2]\n        ])\n    }\n\n    # Case 3: Exactly coplanar\n    case3 = {\n        \"coeffs\": (2.3, -1.7, 0.0),\n        \"neighbors\": np.array([\n            [1, 0, 0], [-1, 0, 0], [0, 1, 0],\n            [0, -1, 0], [1, 1, 0], [-1, -1, 0]\n        ]),\n        \"expansion_pts\": np.array([\n            [0, 0, 0.2], [0.2, 0, 0.2], [0, 0.2, 0.2]\n        ])\n    }\n    \n    test_cases = [case1, case2, case3]\n    all_results = [process_case(**params) for params in test_cases]\n\n    # Format output string\n    output_parts = []\n    for result in all_results:\n        # Correctly format kappa_A and e_naive as float for consistency\n        result[0] = float(result[0])\n        result[2] = float(result[2])\n        formatted_result = [f\"{val:.6f}\" for val in result]\n        output_parts.append(f\"[{','.join(formatted_result)}]\")\n    \n    print(f\"[[{output_parts[0][1:-1]}],[{output_parts[1][1:-1]}],[{output_parts[2][1:-1]}]]\")\n\n# It is good practice to wrap the main execution logic in a standard Python entry point.\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}