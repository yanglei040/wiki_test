{
    "hands_on_practices": [
        {
            "introduction": "Mastery of Gaussian elimination begins with a firm grasp of its fundamental mechanics. This first exercise provides a concrete opportunity to perform the forward elimination steps on a small, nonsymmetric matrix that is representative of systems arising from convection-diffusion problems in CFD. By executing this calculation by hand, you will solidify your understanding of how to compute multipliers and sequentially transform a matrix into its upper-triangular form, a foundational skill for analyzing and implementing linear solvers .",
            "id": "3322964",
            "problem": "A one-dimensional steady convectionâ€“diffusion boundary-value problem on a uniform grid with three interior nodes is discretized using central differences for diffusion and first-order upwinding for convection, yielding a nonsymmetric tridiagonal linear system for the interior unknown vector $\\boldsymbol{T} \\in \\mathbb{R}^{3}$. The resulting system has the form $A \\boldsymbol{T} = \\boldsymbol{b}$ with\n$$\nA \\;=\\; \\begin{pmatrix}\n5 & -1 & 0 \\\\\n-4 & 5 & -1 \\\\\n0 & -4 & 5\n\\end{pmatrix}, \n\\qquad\n\\boldsymbol{b} \\;=\\; \\begin{pmatrix}\n1 \\\\ 0 \\\\ 2\n\\end{pmatrix}.\n$$\nIn the context of Computational Fluid Dynamics (CFD), the forward elimination phase of Gaussian elimination transforms $A$ into an upper-triangular matrix $U$ by applying elementary row operations that eliminate subdiagonal entries in column-major order. The row operations used in forward elimination can be parameterized by multipliers $m_{21}$, $m_{31}$, and $m_{32}$, where each multiplier $m_{ij}$ scales the pivot row at step $j$ to eliminate the entry in position $(i,j)$.\n\nStarting from the core definition of linear systems and the rule that elementary row operations preserve the solution set, perform forward elimination without pivoting to transform $A$ into an upper-triangular matrix $U$ and to obtain the transformed right-hand side $\\widehat{\\boldsymbol{b}}$ corresponding to the same sequence of row operations. Explicitly identify the multipliers $m_{21}$, $m_{31}$, and $m_{32}$, and report the resulting $U$ and $\\widehat{\\boldsymbol{b}}$. \n\nFinally, provide the exact value of the second-stage multiplier $m_{32}$ as your answer. Express your final answer in exact rational form. No rounding is required and no units are involved.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- The linear system is of the form $A \\boldsymbol{T} = \\boldsymbol{b}$.\n- The coefficient matrix $A$ is given by:\n$$\nA \\;=\\; \\begin{pmatrix}\n5 & -1 & 0 \\\\\n-4 & 5 & -1 \\\\\n0 & -4 & 5\n\\end{pmatrix}\n$$\n- The right-hand side vector $\\boldsymbol{b}$ is given by:\n$$\n\\boldsymbol{b} \\;=\\; \\begin{pmatrix}\n1 \\\\ 0 \\\\ 2\n\\end{pmatrix}\n$$\n- The task is to perform forward elimination without pivoting on the system $A \\boldsymbol{T} = \\boldsymbol{b}$ to obtain an upper-triangular system $U \\boldsymbol{T} = \\widehat{\\boldsymbol{b}}$.\n- The process involves identifying multipliers $m_{21}$, $m_{31}$, and $m_{32}$, where $m_{ij}$ is used to eliminate the entry in position $(i,j)$ by scaling the pivot row $j$.\n- The final answer required is the exact value of the multiplier $m_{32}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the specified criteria:\n- **Scientifically Grounded:** The problem is a standard exercise in numerical linear algebra, specifically the application of Gaussian elimination. This is a fundamental algorithm in scientific computing, including Computational Fluid Dynamics (CFD). The context provided about the physical origin of the system (convection-diffusion equation) is appropriate and realistic. The problem is mathematically and scientifically sound.\n- **Well-Posed:** The problem is well-posed. The matrix $A$ is non-singular (its determinant is $5(25 - 4) - (-1)(-20) = 5(21) - 20 = 85 \\neq 0$), guaranteeing a unique solution to the linear system. The instructions to perform forward elimination without pivoting constitute a well-defined, unambiguous algorithm.\n- **Objective:** The problem is stated in precise, objective mathematical language.\n- **Completeness and Consistency:** All necessary information (matrix $A$, vector $\\boldsymbol{b}$, and the procedure) is provided. There are no contradictions.\n- **No other flaws are present.** The problem is neither unrealistic, ill-posed, trivial, nor unverifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\nThe objective is to transform the linear system $A \\boldsymbol{T} = \\boldsymbol{b}$ into an equivalent upper-triangular system $U \\boldsymbol{T} = \\widehat{\\boldsymbol{b}}$ using forward elimination. This is achieved by applying elementary row operations to the augmented matrix $[A | \\boldsymbol{b}]$.\n\nThe initial augmented matrix is:\n$$\n[A | \\boldsymbol{b}]^{(0)} = \\left[\n\\begin{array}{ccc|c}\n5 & -1 & 0 & 1 \\\\\n-4 & 5 & -1 & 0 \\\\\n0 & -4 & 5 & 2\n\\end{array}\n\\right]\n$$\n\n**Step 1: Elimination in column $j=1$**\n\nThe goal is to eliminate the subdiagonal entries in the first column. The pivot element is $a_{11} = 5$.\n\nTo eliminate the entry $a_{21} = -4$, we define the multiplier $m_{21} = \\frac{a_{21}}{a_{11}}$.\n$$\nm_{21} = \\frac{-4}{5}\n$$\nThe corresponding row operation is $R_2 \\leftarrow R_2 - m_{21} R_1$, or $R_2 \\leftarrow R_2 + \\frac{4}{5} R_1$.\nApplying this to the second row of the augmented matrix:\n- New element in position $(2,1)$: $-4 + \\frac{4}{5}(5) = -4 + 4 = 0$.\n- New element in position $(2,2)$: $5 + \\frac{4}{5}(-1) = 5 - \\frac{4}{5} = \\frac{25-4}{5} = \\frac{21}{5}$.\n- New element in position $(2,3)$: $-1 + \\frac{4}{5}(0) = -1$.\n- New element in the right-hand side vector: $0 + \\frac{4}{5}(1) = \\frac{4}{5}$.\n\nThe entry $a_{31}$ is already $0$. Therefore, no row operation is needed to eliminate it. The corresponding multiplier is:\n$$\nm_{31} = \\frac{a_{31}}{a_{11}} = \\frac{0}{5} = 0\n$$\n\nAfter the first step of elimination, the augmented matrix becomes:\n$$\n[A | \\boldsymbol{b}]^{(1)} = \\left[\n\\begin{array}{ccc|c}\n5 & -1 & 0 & 1 \\\\\n0 & \\frac{21}{5} & -1 & \\frac{4}{5} \\\\\n0 & -4 & 5 & 2\n\\end{array}\n\\right]\n$$\n\n**Step 2: Elimination in column $j=2$**\n\nThe goal is to eliminate the subdiagonal entry in the second column. The pivot element is the new entry in the $(2,2)$ position, $a'_{22} = \\frac{21}{5}$.\n\nTo eliminate the entry $a'_{32} = -4$, we define the multiplier $m_{32} = \\frac{a'_{32}}{a'_{22}}$.\n$$\nm_{32} = \\frac{-4}{\\frac{21}{5}} = -4 \\cdot \\frac{5}{21} = -\\frac{20}{21}\n$$\nThe corresponding row operation is $R_3 \\leftarrow R_3 - m_{32} R_2$, or $R_3 \\leftarrow R_3 + \\frac{20}{21} R_2$.\nApplying this to the third row of the matrix $[A | \\boldsymbol{b}]^{(1)}$:\n- New element in position $(3,1)$: $0 + \\frac{20}{21}(0) = 0$.\n- New element in position $(3,2)$: $-4 + \\frac{20}{21}\\left(\\frac{21}{5}\\right) = -4 + \\frac{20}{5} = -4 + 4 = 0$.\n- New element in position $(3,3)$: $5 + \\frac{20}{21}(-1) = 5 - \\frac{20}{21} = \\frac{105-20}{21} = \\frac{85}{21}$.\n- New element in the right-hand side vector: $2 + \\frac{20}{21}\\left(\\frac{4}{5}\\right) = 2 + \\frac{80}{105} = 2 + \\frac{16}{21} = \\frac{42+16}{21} = \\frac{58}{21}$.\n\nThe forward elimination process is now complete. The final augmented matrix $[U | \\widehat{\\boldsymbol{b}}]$ is:\n$$\n[U | \\widehat{\\boldsymbol{b}}] = \\left[\n\\begin{array}{ccc|c}\n5 & -1 & 0 & 1 \\\\\n0 & \\frac{21}{5} & -1 & \\frac{4}{5} \\\\\n0 & 0 & \\frac{85}{21} & \\frac{58}{21}\n\\end{array}\n\\right]\n$$\n\nThe resulting upper-triangular matrix $U$ and transformed right-hand side vector $\\widehat{\\boldsymbol{b}}$ are:\n$$\nU = \\begin{pmatrix}\n5 & -1 & 0 \\\\\n0 & \\frac{21}{5} & -1 \\\\\n0 & 0 & \\frac{85}{21}\n\\end{pmatrix}, \\qquad\n\\widehat{\\boldsymbol{b}} = \\begin{pmatrix}\n1 \\\\\n\\frac{4}{5} \\\\\n\\frac{58}{21}\n\\end{pmatrix}\n$$\n\nThe multipliers identified during the process are:\n- $m_{21} = -\\frac{4}{5}$\n- $m_{31} = 0$\n- $m_{32} = -\\frac{20}{21}$\n\nThe problem asks for the exact value of the second-stage multiplier $m_{32}$.\nThe value is $-\\frac{20}{21}$.",
            "answer": "$$\n\\boxed{-\\frac{20}{21}}\n$$"
        },
        {
            "introduction": "Beyond the mechanics of the algorithm, a crucial aspect of numerical computation is assessing the quality of the solution. This next practice explores the concept of backward error and provides a stark demonstration of why pivoting is not merely an optional refinement but a critical component for numerical stability. By comparing the residuals of solutions obtained for an ill-conditioned system, you will gain a tangible appreciation for how pivoting strategies protect against the catastrophic amplification of round-off errors .",
            "id": "3322970",
            "problem": "A common verification step in Computational Fluid Dynamics (CFD) linear solvers for elliptic operators is to assess solver robustness on ill-conditioned systems. Consider the following symmetric positive definite but ill-conditioned test system of size $3 \\times 3$:\n$$\nA \\, \\hat{x} \\approx b, \\quad A \\in \\mathbb{R}^{3 \\times 3}, \\quad b \\in \\mathbb{R}^{3},\n$$\nwhere\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & \\tfrac{1}{2} & \\tfrac{1}{3} \\\\\n\\tfrac{1}{2} & \\tfrac{1}{3} & \\tfrac{1}{4} \\\\\n\\tfrac{1}{3} & \\tfrac{1}{4} & \\tfrac{1}{5}\n\\end{pmatrix}, \n\\qquad x_{\\mathrm{true}} \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix},\n\\qquad b \\;=\\; A \\, x_{\\mathrm{true}}.\n$$\nTwo Gaussian-elimination-based solvers are applied: one without pivoting and one with partial pivoting. They return the approximations\n$$\n\\hat{x}_{\\mathrm{np}} \\;=\\; \\begin{pmatrix} 1.02 \\\\ 0.98 \\\\ 1.00 \\end{pmatrix},\n\\qquad\n\\hat{x}_{\\mathrm{pp}} \\;=\\; \\begin{pmatrix} 1.001 \\\\ 0.999 \\\\ 1.000 \\end{pmatrix}.\n$$\nUsing the Euclidean norm, form the normalized residual (backward error) for each solver as the ratio of the residual norm to the right-hand-side norm and compute the ratio\n$$\n\\rho \\;=\\; \\frac{\\|\\,b - A \\hat{x}_{\\mathrm{np}}\\,\\|_{2} / \\|\\,b\\,\\|_{2}}{\\|\\,b - A \\hat{x}_{\\mathrm{pp}}\\,\\|_{2} / \\|\\,b\\,\\|_{2}}.\n$$\nExpress your final answer as a pure number with no units, rounded to four significant figures.",
            "solution": "The problem statement is validated as scientifically sound, well-posed, and complete. The context and values are standard for demonstrating the numerical properties of linear solvers on ill-conditioned systems like the Hilbert matrix. All information required for the calculation is provided. I will proceed with the solution.\n\nThe problem asks for the computation of the ratio of normalized residuals:\n$$\n\\rho \\;=\\; \\frac{\\|\\,b - A \\hat{x}_{\\mathrm{np}}\\,\\|_{2} / \\|\\,b\\,\\|_{2}}{\\|\\,b - A \\hat{x}_{\\mathrm{pp}}\\,\\|_{2} / \\|\\,b\\,\\|_{2}}.\n$$\nSince the term $\\|\\,b\\,\\|_{2}$ is present in both the numerator and the denominator, it cancels out, simplifying the expression to:\n$$\n\\rho \\;=\\; \\frac{\\|\\,b - A \\hat{x}_{\\mathrm{np}}\\,\\|_{2}}{\\|\\,b - A \\hat{x}_{\\mathrm{pp}}\\,\\|_{2}}.\n$$\nLet's denote the residuals for the no-pivoting and partial-pivoting solutions as $r_{\\mathrm{np}} = b - A \\hat{x}_{\\mathrm{np}}$ and $r_{\\mathrm{pp}} = b - A \\hat{x}_{\\mathrm{pp}}$, respectively. Using the fact that $b = A x_{\\mathrm{true}}$, we can express the residuals in terms of the solution errors. Let the error vectors be $\\delta x_{\\mathrm{np}} = \\hat{x}_{\\mathrm{np}} - x_{\\mathrm{true}}$ and $\\delta x_{\\mathrm{pp}} = \\hat{x}_{\\mathrm{pp}} - x_{\\mathrm{true}}$.\nThe residual for the no-pivoting case is:\n$$\nr_{\\mathrm{np}} = A x_{\\mathrm{true}} - A \\hat{x}_{\\mathrm{np}} = A (x_{\\mathrm{true}} - \\hat{x}_{\\mathrm{np}}) = -A (\\hat{x}_{\\mathrm{np}} - x_{\\mathrm{true}}) = -A \\, \\delta x_{\\mathrm{np}}.\n$$\nSimilarly, for the partial-pivoting case:\n$$\nr_{\\mathrm{pp}} = -A \\, \\delta x_{\\mathrm{pp}}.\n$$\nLet's compute the error vectors from the given values:\n$$\n\\delta x_{\\mathrm{np}} = \\hat{x}_{\\mathrm{np}} - x_{\\mathrm{true}} = \\begin{pmatrix} 1.02 \\\\ 0.98 \\\\ 1.00 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.02 \\\\ -0.02 \\\\ 0 \\end{pmatrix}.\n$$\n$$\n\\delta x_{\\mathrm{pp}} = \\hat{x}_{\\mathrm{pp}} - x_{\\mathrm{true}} = \\begin{pmatrix} 1.001 \\\\ 0.999 \\\\ 1.000 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.001 \\\\ -0.001 \\\\ 0 \\end{pmatrix}.\n$$\nBy comparing the two error vectors, we find a simple proportional relationship:\n$$\n\\delta x_{\\mathrm{np}} = \\begin{pmatrix} 20 \\times 0.001 \\\\ 20 \\times (-0.001) \\\\ 20 \\times 0 \\end{pmatrix} = 20 \\, \\delta x_{\\mathrm{pp}}.\n$$\nNow we can substitute this relationship into the expression for the residual $r_{\\mathrm{np}}$:\n$$\nr_{\\mathrm{np}} = -A \\, \\delta x_{\\mathrm{np}} = -A (20 \\, \\delta x_{\\mathrm{pp}}).\n$$\nBy the linearity of matrix-vector multiplication, we can factor out the scalar $20$:\n$$\nr_{\\mathrm{np}} = 20 (-A \\, \\delta x_{\\mathrm{pp}}) = 20 \\, r_{\\mathrm{pp}}.\n$$\nThis reveals that the residual vector from the no-pivoting solver is exactly $20$ times the residual vector from the partial-pivoting solver. We can now compute the ratio of their Euclidean norms:\n$$\n\\rho = \\frac{\\|r_{\\mathrm{np}}\\|_{2}}{\\|r_{\\mathrm{pp}}\\|_{2}} = \\frac{\\|20 \\, r_{\\mathrm{pp}}\\|_{2}}{\\|r_{\\mathrm{pp}}\\|_{2}}.\n$$\nUsing the norm property $\\|c \\mathbf{v}\\|_2 = |c| \\|\\mathbf{v}\\|_2$:\n$$\n\\rho = \\frac{|20| \\, \\|r_{\\mathrm{pp}}\\|_{2}}{\\|r_{\\mathrm{pp}}\\|_{2}}.\n$$\nSince $\\delta x_{\\mathrm{pp}}$ is a non-zero vector and $A$ (a Hilbert matrix) is invertible, $r_{\\mathrm{pp}} = -A \\delta x_{\\mathrm{pp}}$ is also a non-zero vector, so its norm $\\|r_{\\mathrm{pp}}\\|_{2}$ is non-zero. We can safely cancel the norms:\n$$\n\\rho = 20.\n$$\nThe problem requires the answer to be expressed with four significant figures.\n$$\n\\rho = 20.00.\n$$",
            "answer": "$$\n\\boxed{20.00}\n$$"
        },
        {
            "introduction": "Gaussian elimination can be more than just a solution method; it can also be a powerful diagnostic tool for understanding the properties of a linear system. This final exercise bridges the gap between the algebraic process of LU factorization and the underlying physics of a discretized model, specifically the singularity arising from pure Neumann boundary conditions common in CFD. By symbolically analyzing a perturbed version of such a system, you will see precisely how the rank deficiency of the continuous operator manifests as a vanishing pivot during elimination, offering deep insight into the structure of the problem .",
            "id": "3322974",
            "problem": "Consider the steady one-dimensional diffusion operator representative of the pressure Poisson equation in incompressible Computational Fluid Dynamics (CFD), given by the second derivative operator $\\partial_{xx}$ on the interval $[0,1]$ with homogeneous Neumann boundary conditions at both ends. A standard centered second-order finite-difference discretization on $n$ uniformly spaced grid points preserves the constant nullspace of the continuous operator under pure Neumann boundary conditions. For $n=4$ grid points (including the boundary points), the resulting scaled discrete operator (with the common factor $1/h^{2}$ suppressed for algebraic clarity) is the $4 \\times 4$ matrix\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1 & -1 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 \\\\\n0 & -1 & 2 & -1 \\\\\n0 & 0 & -1 & 1\n\\end{pmatrix}.\n$$\nThis matrix models a pure Neumann problem and is singular with a one-dimensional nullspace spanned by the constant vector. To mimic a common CFD practice of weakly pinning the pressure (or potential) to regularize the singular operator while minimally perturbing the physics, consider the perturbed matrix\n$$\nA_{\\varepsilon} \\;=\\; A \\;+\\; \\varepsilon \\, e_{1} e_{1}^{\\top},\n$$\nwhere $\\varepsilon>0$ is small and $e_{1}$ is the first canonical basis vector. Perform Gaussian elimination with partial pivoting on $A_{\\varepsilon}$, and at each stage justify the pivot selection from first principles (i.e., by comparing entry magnitudes in the active pivot column). Derive symbolically the exact expression for the final pivot (the $(4,4)$ diagonal entry of the upper-triangular factor produced by Gaussian elimination with partial pivoting), denoted $u_{44}(\\varepsilon)$, as a function of $\\varepsilon$.\n\nYour final answer must be the single simplified closed-form expression for $u_{44}(\\varepsilon)$. No numerical rounding is required and no units are needed. Explain why the limiting behavior of $u_{44}(\\varepsilon)$ as $\\varepsilon \\to 0^{+}$ reflects the rank deficiency of $A$.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. All necessary information is provided to perform the specified algebraic procedure and analyze the result. The context provided, relating the matrix to a finite-difference discretization of a differential operator from Computational Fluid Dynamics, is physically and mathematically sound. The matrix $A$ possesses the stated properties of being singular with a one-dimensional nullspace spanned by a constant vector. The regularization technique is a standard practice. Therefore, I will proceed with the solution.\n\nThe problem asks to perform Gaussian elimination with partial pivoting on the perturbed matrix $A_{\\varepsilon}$ and find the final pivot $u_{44}(\\varepsilon)$. The matrix $A$ is given by\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1 & -1 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 \\\\\n0 & -1 & 2 & -1 \\\\\n0 & 0 & -1 & 1\n\\end{pmatrix}.\n$$\nThe perturbation is defined by $A_{\\varepsilon} = A + \\varepsilon \\, e_{1} e_{1}^{\\top}$, where $\\varepsilon>0$ is a small parameter and $e_{1} = (1, 0, 0, 0)^{\\top}$ is the first canonical basis vector. The perturbation term is\n$$\n\\varepsilon \\, e_{1} e_{1}^{\\top} \\;=\\; \\varepsilon \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix} \\;=\\; \\begin{pmatrix} \\varepsilon & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}.\n$$\nThus, the matrix $A_{\\varepsilon}$ is\n$$\nA_{\\varepsilon} \\;=\\; \\begin{pmatrix} 1+\\varepsilon & -1 & 0 & 0 \\\\ -1 & 2 & -1 & 0 \\\\ 0 & -1 & 2 & -1 \\\\ 0 & 0 & -1 & 1 \\end{pmatrix}.\n$$\nLet this matrix be denoted as $A^{(1)}$. We now perform Gaussian elimination with partial pivoting.\n\n**Step 1: Elimination in Column 1**\nThe first column of $A^{(1)}$ is $(1+\\varepsilon, -1, 0, 0)^{\\top}$. We must select the entry with the largest absolute value as the pivot. The candidates are $|1+\\varepsilon|$ and $|-1|=1$. Since $\\varepsilon>0$, we have $1+\\varepsilon > 1$, so $|1+\\varepsilon| > |-1|$. The pivot is $a^{(1)}_{11} = 1+\\varepsilon$. No row interchange is necessary.\nWe eliminate the entry $a^{(1)}_{21}=-1$ by computing the multiplier $m_{21} = \\frac{a^{(1)}_{21}}{a^{(1)}_{11}} = \\frac{-1}{1+\\varepsilon}$.\nThe second row is updated by the operation $R_2 \\leftarrow R_2 - m_{21} R_1$.\nThe new entries of the second row are:\n$a^{(2)}_{21} = a^{(1)}_{21} - m_{21}a^{(1)}_{11} = -1 - \\left(\\frac{-1}{1+\\varepsilon}\\right)(1+\\varepsilon) = -1+1=0$.\n$a^{(2)}_{22} = a^{(1)}_{22} - m_{21}a^{(1)}_{12} = 2 - \\left(\\frac{-1}{1+\\varepsilon}\\right)(-1) = 2 - \\frac{1}{1+\\varepsilon} = \\frac{2(1+\\varepsilon)-1}{1+\\varepsilon} = \\frac{1+2\\varepsilon}{1+\\varepsilon}$.\n$a^{(2)}_{23} = a^{(1)}_{23} - m_{21}a^{(1)}_{13} = -1 - \\left(\\frac{-1}{1+\\varepsilon}\\right)(0) = -1$.\n$a^{(2)}_{24} = a^{(1)}_{24} - m_{21}a^{(1)}_{14} = 0 - \\left(\\frac{-1}{1+\\varepsilon}\\right)(0) = 0$.\nThe matrix after the first step is\n$$\nA^{(2)} \\;=\\; \\begin{pmatrix} 1+\\varepsilon & -1 & 0 & 0 \\\\ 0 & \\frac{1+2\\varepsilon}{1+\\varepsilon} & -1 & 0 \\\\ 0 & -1 & 2 & -1 \\\\ 0 & 0 & -1 & 1 \\end{pmatrix}.\n$$\n\n**Step 2: Elimination in Column 2**\nThe active pivot column is the second column, below the diagonal: $(\\frac{1+2\\varepsilon}{1+\\varepsilon}, -1)^{\\top}$. We compare magnitudes to select the pivot.\n$|a^{(2)}_{22}| = \\left|\\frac{1+2\\varepsilon}{1+\\varepsilon}\\right| = \\frac{1+2\\varepsilon}{1+\\varepsilon}$ since $\\varepsilon>0$.\n$|a^{(2)}_{32}| = |-1| = 1$.\nFor $\\varepsilon>0$, $1+2\\varepsilon > 1+\\varepsilon$, which implies $\\frac{1+2\\varepsilon}{1+\\varepsilon} > 1$. Thus, $|a^{(2)}_{22}| > |a^{(2)}_{32}|$. The pivot is $a^{(2)}_{22} = \\frac{1+2\\varepsilon}{1+\\varepsilon}$. No row interchange is necessary.\nWe eliminate $a^{(2)}_{32}=-1$ with multiplier $m_{32} = \\frac{a^{(2)}_{32}}{a^{(2)}_{22}} = \\frac{-1}{(1+2\\varepsilon)/(1+\\varepsilon)} = -\\frac{1+\\varepsilon}{1+2\\varepsilon}$.\nThe third row is updated by $R_3 \\leftarrow R_3 - m_{32} R_2$.\nNew entries for the third row:\n$a^{(3)}_{32} = a^{(2)}_{32} - m_{32}a^{(2)}_{22} = -1 - \\left(-\\frac{1+\\varepsilon}{1+2\\varepsilon}\\right)\\left(\\frac{1+2\\varepsilon}{1+\\varepsilon}\\right) = -1+1 = 0$.\n$a^{(3)}_{33} = a^{(2)}_{33} - m_{32}a^{(2)}_{23} = 2 - \\left(-\\frac{1+\\varepsilon}{1+2\\varepsilon}\\right)(-1) = 2 - \\frac{1+\\varepsilon}{1+2\\varepsilon} = \\frac{2(1+2\\varepsilon)-(1+\\varepsilon)}{1+2\\varepsilon} = \\frac{1+3\\varepsilon}{1+2\\varepsilon}$.\n$a^{(3)}_{34} = a^{(2)}_{34} - m_{32}a^{(2)}_{24} = -1 - \\left(-\\frac{1+\\varepsilon}{1+2\\varepsilon}\\right)(0) = -1$.\nThe matrix after the second step is\n$$\nA^{(3)} \\;=\\; \\begin{pmatrix} 1+\\varepsilon & -1 & 0 & 0 \\\\ 0 & \\frac{1+2\\varepsilon}{1+\\varepsilon} & -1 & 0 \\\\ 0 & 0 & \\frac{1+3\\varepsilon}{1+2\\varepsilon} & -1 \\\\ 0 & 0 & -1 & 1 \\end{pmatrix}.\n$$\n\n**Step 3: Elimination in Column 3**\nThe active pivot column is the third column, below the diagonal: $(\\frac{1+3\\varepsilon}{1+2\\varepsilon}, -1)^{\\top}$. We compare magnitudes.\n$|a^{(3)}_{33}| = \\left|\\frac{1+3\\varepsilon}{1+2\\varepsilon}\\right| = \\frac{1+3\\varepsilon}{1+2\\varepsilon}$ since $\\varepsilon>0$.\n$|a^{(3)}_{43}| = |-1| = 1$.\nFor $\\varepsilon>0$, $1+3\\varepsilon > 1+2\\varepsilon$, which implies $\\frac{1+3\\varepsilon}{1+2\\varepsilon} > 1$. Thus, $|a^{(3)}_{33}| > |a^{(3)}_{43}|$. The pivot is $a^{(3)}_{33} = \\frac{1+3\\varepsilon}{1+2\\varepsilon}$. No row interchange is necessary.\nWe eliminate $a^{(3)}_{43}=-1$ with multiplier $m_{43} = \\frac{a^{(3)}_{43}}{a^{(3)}_{33}} = \\frac{-1}{(1+3\\varepsilon)/(1+2\\varepsilon)} = -\\frac{1+2\\varepsilon}{1+3\\varepsilon}$.\nThe fourth row is updated by $R_4 \\leftarrow R_4 - m_{43} R_3$.\nThe new entry for the fourth row, fourth column is:\n$a^{(4)}_{44} = a^{(3)}_{44} - m_{43}a^{(3)}_{34} = 1 - \\left(-\\frac{1+2\\varepsilon}{1+3\\varepsilon}\\right)(-1) = 1 - \\frac{1+2\\varepsilon}{1+3varepsilon} = \\frac{(1+3\\varepsilon)-(1+2\\varepsilon)}{1+3\\varepsilon} = \\frac{\\varepsilon}{1+3\\varepsilon}$.\nAfter this final step, the process of Gaussian elimination is complete. The resulting upper-triangular matrix, denoted $U$, is\n$$\nU \\;=\\; \\begin{pmatrix} 1+\\varepsilon & -1 & 0 & 0 \\\\ 0 & \\frac{1+2\\varepsilon}{1+\\varepsilon} & -1 & 0 \\\\ 0 & 0 & \\frac{1+3\\varepsilon}{1+2\\varepsilon} & -1 \\\\ 0 & 0 & 0 & \\frac{\\varepsilon}{1+3\\varepsilon} \\end{pmatrix}.\n$$\nThe final pivot is the $(4,4)$ diagonal entry of $U$, which is $u_{44}(\\varepsilon)$.\n$$\nu_{44}(\\varepsilon) = \\frac{\\varepsilon}{1+3\\varepsilon}.\n$$\n\n**Analysis of the Limiting Behavior**\nThe limiting behavior of $u_{44}(\\varepsilon)$ as $\\varepsilon \\to 0^{+}$ is\n$$\n\\lim_{\\varepsilon \\to 0^{+}} u_{44}(\\varepsilon) = \\lim_{\\varepsilon \\to 0^{+}} \\frac{\\varepsilon}{1+3\\varepsilon} = \\frac{0}{1+0} = 0.\n$$\nThis behavior is a direct consequence of the singularity of the original matrix $A$. The determinant of a matrix is the product of the pivots obtained during Gaussian elimination (up to a sign determined by row swaps). Since no row swaps were needed, $\\det(A_{\\varepsilon}) = u_{11}u_{22}u_{33}u_{44}$.\n$$\n\\det(A_{\\varepsilon}) = (1+\\varepsilon)\\left(\\frac{1+2\\varepsilon}{1+\\varepsilon}\\right)\\left(\\frac{1+3\\varepsilon}{1+2\\varepsilon}\\right)\\left(\\frac{\\varepsilon}{1+3\\varepsilon}\\right) = \\varepsilon.\n$$\nAs $\\varepsilon \\to 0^{+}$, $A_{\\varepsilon} \\to A$, and $\\det(A_{\\varepsilon}) \\to \\det(A)$. The limit $\\lim_{\\varepsilon \\to 0^{+}} \\det(A_{\\varepsilon})=0$ confirms that $\\det(A)=0$, so $A$ is a singular matrix.\nThe rank of a matrix is equal to the number of non-zero pivots produced by Gaussian elimination. For any $\\varepsilon>0$, all four pivots of $A_{\\varepsilon}$ are non-zero, so $\\text{rank}(A_{\\varepsilon})=4$. However, in the limit $\\varepsilon \\to 0^{+}$, the pivots of $A_{\\varepsilon}$ converge to the pivots of $A$:\n$$\nu_{11} \\to 1, \\quad u_{22} \\to 1, \\quad u_{33} \\to 1, \\quad u_{44} \\to 0.\n$$\nThe emergence of a zero pivot in the final position for the matrix $A$ indicates that its rank is $3$. For an $n \\times n$ matrix, a rank of $r < n$ implies that the dimension of its nullspace is $n-r$. For the $4 \\times 4$ matrix $A$, a rank of $3$ means its nullspace has dimension $4-3=1$. This is perfectly consistent with the problem statement that $A$ is rank-deficient with a one-dimensional nullspace. The quantity $u_{44}(\\varepsilon)$ approaching zero is the numerical signature of $A_{\\varepsilon}$ being a small perturbation of a singular matrix.",
            "answer": "$$\\boxed{\\frac{\\varepsilon}{1+3\\varepsilon}}$$"
        }
    ]
}