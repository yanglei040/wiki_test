{
    "hands_on_practices": [
        {
            "introduction": "The foundational promise of compressed sensing is to recover a sparse signal from a small number of measurements. This exercise explores the fundamental question of *when* such a recovery is even possible and unique. By engaging with the concept of a matrix's spark, you will derive one of the core theoretical guarantees for unique sparse recovery and see precisely why this condition is sharp by constructing a counterexample .",
            "id": "3460579",
            "problem": "You are given a linear measurement model in compressed sensing of the form $y = A x_{\\star}$, where $A \\in \\mathbb{R}^{m \\times n}$ and $x_{\\star} \\in \\mathbb{R}^{n}$ is $k$-sparse, meaning that the number of nonzero entries of $x_{\\star}$ is at most $k$. The spark of a matrix $A$, denoted $\\operatorname{spark}(A)$, is defined as the smallest number of columns of $A$ that are linearly dependent. Use only fundamental definitions from linear algebra and sparse modeling to address the following.\n\n- Using only the definitions of linear independence, the null space of a matrix, $k$-sparsity, and $\\operatorname{spark}(A)$, show that if $\\operatorname{spark}(A) > 2k$, then the vector $x_{\\star}$ is the unique $k$-sparse solution to $A x = y$.\n\n- Explain why the boundary case $\\operatorname{spark}(A) = 2k$ allows non-uniqueness, and construct a minimal-dimensional counterexample in which there exist two distinct $k$-sparse vectors $x^{(1)} \\neq x^{(2)}$ with $A x^{(1)} = A x^{(2)}$. Here, “minimal-dimensional” means you should use $m = 2k - 1$ and the smallest $n$ for which $\\operatorname{spark}(A) = 2k$ is possible. Then instantiate the construction concretely for $k = 2$ by taking $m = 3$, $n = 4$, and\n  - $a_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$, $a_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$, $a_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$, $a_{4} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix}$ as the columns of $A$,\n  - $x^{(1)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ and $x^{(2)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n\nVerify in this instance that $\\operatorname{spark}(A) = 4$, that $A x^{(1)} = A x^{(2)}$, and compute the scalar value of $\\|x^{(1)} - x^{(2)}\\|_{2}^{2}$. Report this scalar as your final answer. The final answer is exact and requires no rounding.",
            "solution": "We begin from the fundamental definitions. A vector $x \\in \\mathbb{R}^{n}$ is $k$-sparse if it has at most $k$ nonzero entries. The $\\operatorname{spark}$ of a matrix $A \\in \\mathbb{R}^{m \\times n}$, denoted $\\operatorname{spark}(A)$, is the smallest number of columns of $A$ that form a linearly dependent set. Equivalently, any set of fewer than $\\operatorname{spark}(A)$ columns is linearly independent. The null space $\\mathcal{N}(A)$ is the set $\\{h \\in \\mathbb{R}^{n} : A h = 0\\}$.\n\nUniqueness when $\\operatorname{spark}(A) > 2k$. Suppose $y = A x_{\\star}$ with $x_{\\star}$ $k$-sparse. Assume there exists another $k$-sparse solution $x$ with $A x = y$ and $x \\neq x_{\\star}$. Define $h = x - x_{\\star} \\neq 0$. Then $A h = A x - A x_{\\star} = 0$, so $h \\in \\mathcal{N}(A) \\setminus \\{0\\}$. Let $S = \\operatorname{supp}(x)$ and $T = \\operatorname{supp}(x_{\\star})$ denote their supports. Then $\\operatorname{supp}(h) \\subseteq S \\cup T$, so $|\\operatorname{supp}(h)| \\leq |S| + |T| \\leq k + k = 2k$. Because $h \\in \\mathcal{N}(A) \\setminus \\{0\\}$, the columns of $A$ indexed by $\\operatorname{supp}(h)$ must be linearly dependent. By definition of $\\operatorname{spark}(A)$, any linearly dependent set of columns has size at least $\\operatorname{spark}(A)$. Therefore $|\\operatorname{supp}(h)| \\geq \\operatorname{spark}(A)$. Combining yields $\\operatorname{spark}(A) \\leq |\\operatorname{supp}(h)| \\leq 2k$. If $\\operatorname{spark}(A) > 2k$, this is impossible. Hence no such $x \\neq x_{\\star}$ exists, and $x_{\\star}$ is the unique $k$-sparse solution.\n\nNon-uniqueness at the boundary $\\operatorname{spark}(A) = 2k$ and minimal counterexample. If $\\operatorname{spark}(A) = 2k$, then by definition there exists a nonzero vector $z \\in \\mathcal{N}(A)$ whose support has cardinality exactly $2k$, and any strict subset of these $2k$ columns is linearly independent. Partition the support of $z$ into disjoint sets $S$ and $T$ such that $|S| = |T| = k$ and write $z = z_{S} - z_{T}$ where $z_{S}$ and $z_{T}$ are supported on $S$ and $T$ respectively and match the signs of $z$ on their supports. Then $A z = 0$ implies $A z_{S} = A z_{T}$. Both $z_{S}$ and $z_{T}$ are $k$-sparse and distinct, so they furnish two different $k$-sparse solutions to the same measurement $y = A z_{S} = A z_{T}$. Minimal dimension arises from the general bound $\\operatorname{spark}(A) \\leq m + 1$. To achieve $\\operatorname{spark}(A) = 2k$, one needs $m \\geq 2k - 1$, and the minimal choice is $m = 2k - 1$. The smallest $n$ for which $\\operatorname{spark}(A) = 2k$ is possible is $n \\geq 2k$, so that a set of $2k$ columns can be linearly dependent while any $2k - 1$ columns remain independent, realizing $\\operatorname{spark}(A) = 2k$.\n\nConcrete instance for $k = 2$. Take $m = 3$, $n = 4$, and let the columns of $A$ be\n$$\na_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix},\\quad\na_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix},\\quad\na_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix},\\quad\na_{4} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix}.\n$$\nWe verify $\\operatorname{spark}(A) = 4$. Any set of fewer than $4$ columns must be linearly independent to have $\\operatorname{spark}(A)$ equal to $4$. The three-column submatrices are:\n- Columns $\\{a_{1}, a_{2}, a_{3}\\}$ form the identity matrix in $\\mathbb{R}^{3}$, hence independent.\n- Columns $\\{a_{1}, a_{2}, a_{4}\\}$ give the matrix\n$$\n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & -1\n\\end{pmatrix},\n$$\nwhose determinant is $-1$, hence independent.\n- Columns $\\{a_{1}, a_{3}, a_{4}\\}$ give the matrix\n$$\n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & -1\n\\end{pmatrix},\n$$\nwhose determinant equals $-1$, hence independent.\n- Columns $\\{a_{2}, a_{3}, a_{4}\\}$ give the matrix\n$$\n\\begin{pmatrix}\n0 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & -1\n\\end{pmatrix},\n$$\nwhose determinant equals $1$, hence independent.\nSince any set of $3$ columns is independent in $\\mathbb{R}^{3}$ and any set of $4$ columns is necessarily dependent, we have $\\operatorname{spark}(A) = 4 = 2k$.\n\nNext, define\n$$\nx^{(1)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\nx^{(2)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\nBoth $x^{(1)}$ and $x^{(2)}$ are $2$-sparse. Compute the measurements:\n$$\nA x^{(1)} = 1 \\cdot a_{1} + 1 \\cdot a_{2} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix},\n$$\nand\n$$\nA x^{(2)} = 1 \\cdot a_{3} + 1 \\cdot a_{4} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\nHence $A x^{(1)} = A x^{(2)}$, demonstrating non-uniqueness at $\\operatorname{spark}(A) = 2k$.\n\nFinally, compute the requested scalar:\n$$\n\\|x^{(1)} - x^{(2)}\\|_{2}^{2} = \\left\\| \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right\\|_{2}^{2} = \\left\\| \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{pmatrix} \\right\\|_{2}^{2} = 1^{2} + 1^{2} + (-1)^{2} + (-1)^{2} = 4.\n$$\nThis exact value is the final answer.",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "Knowing that a unique sparse solution can exist is one thing; finding it is another. This practice shifts our focus to the computational side by introducing the Least Absolute Shrinkage and Selection Operator (LASSO), a cornerstone optimization problem for sparse recovery. You will derive the Karush-Kuhn-Tucker (KKT) optimality conditions, which provide a precise mathematical characterization of the solution and form the basis for many powerful compressed sensing algorithms .",
            "id": "3460541",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem in the context of compressed sensing, where one seeks a sparse solution by balancing data fidelity and an entrywise sparsity-promoting regularizer. Let $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $\\lambda > 0$. \n\n(a) Formally define the LASSO optimization problem in terms of minimizing a sum of a quadratic data fidelity term and an $\\ell_{1}$-norm regularizer, explicitly specifying the objective function in terms of $A$, $y$, and $\\lambda$.\n\n(b) Using only the foundational facts that (i) the subdifferential of a proper, closed, convex function characterizes its first-order optimality through Fermat’s rule, and (ii) the subdifferential of the $\\ell_{1}$ norm is the Cartesian product of the subdifferentials of the absolute value at each coordinate, derive necessary and sufficient Karush–Kuhn–Tucker (KKT) optimality conditions for the LASSO problem in terms of the subgradient of the $\\ell_{1}$ norm. Your derivation should be componentwise and should explicitly distinguish the cases where a coordinate is zero or nonzero.\n\n(c) Specialize your result to the candidate solution $x^{\\star} = 0 \\in \\mathbb{R}^{n}$. Derive a necessary and sufficient condition on $\\lambda$ under which $x^{\\star} = 0$ is optimal, expressed only in terms of $A$, $y$, and $\\lambda$. Then, for the concrete data\n$$\nA \\,=\\, \\begin{pmatrix}\n1 & -1 & 2 & 0 \\\\\n0 & 2 & -1 & 1 \\\\\n1 & 0 & 1 & -2\n\\end{pmatrix}, \n\\qquad\ny \\,=\\, \\begin{pmatrix}\n2 \\\\\n-1 \\\\\n3\n\\end{pmatrix},\n$$\ncompute the smallest value of $\\lambda$ for which $x^{\\star} = 0$ satisfies your KKT conditions. Express your final answer as an exact real number. No rounding is required.",
            "solution": "(a) The Least Absolute Shrinkage and Selection Operator (LASSO) problem seeks to find a vector $x \\in \\mathbb{R}^{n}$ that minimizes an objective function composed of a data fidelity term and a regularization term. The data fidelity term measures the misfit between the model prediction $Ax$ and the observed data $y$. It is typically given by the squared $\\ell_{2}$-norm of the residual, $\\|Ax - y\\|_{2}^{2}$. The regularization term promotes sparsity in the solution vector $x$ and is given by the $\\ell_{1}$-norm of $x$, $\\|x\\|_{1}$. The parameter $\\lambda > 0$ controls the trade-off between these two terms. A factor of $\\frac{1}{2}$ is conventionally included with the data fidelity term for mathematical convenience when taking derivatives.\n\nThe $\\ell_{1}$-norm of a vector $x \\in \\mathbb{R}^{n}$ is defined as $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$. The squared $\\ell_{2}$-norm of a vector $v \\in \\mathbb{R}^{m}$ is $\\|v\\|_{2}^{2} = \\sum_{j=1}^{m} v_{j}^{2}$.\n\nThus, the LASSO objective function, which we denote by $J(x)$, is:\n$$\nJ(x) = \\frac{1}{2} \\|Ax - y\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\nThe LASSO optimization problem is formally stated as finding a vector $x^{\\star}$ that solves:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\left( \\frac{1}{2} \\|Ax - y\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right)\n$$\n\n(b) To derive the Karush-Kuhn-Tucker (KKT) optimality conditions, we use subdifferential calculus. The objective function $J(x)$ is a sum of two convex functions: $f(x) = \\frac{1}{2} \\|Ax - y\\|_{2}^{2}$ and $g(x) = \\lambda \\|x\\|_{1}$. The function $f(x)$ is differentiable, while $g(x)$ is not differentiable wherever any component of $x$ is zero. Since $J(x)$ is a proper, closed, and convex function, the necessary and sufficient first-order optimality condition for a point $x^{\\star}$ to be a minimizer is given by Fermat's rule:\n$$\n0 \\in \\partial J(x^{\\star})\n$$\nwhere $\\partial J(x^{\\star})$ is the subdifferential of $J$ at $x^{\\star}$.\n\nUsing the sum rule for subdifferentials, where one function is differentiable, we have $\\partial J(x) = \\nabla f(x) + \\partial g(x)$.\nThe gradient of $f(x)$ is:\n$$\n\\nabla f(x) = A^{\\intercal}(Ax - y)\n$$\nThe subdifferential of $g(x) = \\lambda \\|x\\|_{1}$ is $\\partial g(x) = \\lambda \\partial \\|x\\|_{1}$. We are given that the subdifferential of the $\\ell_{1}$-norm is the Cartesian product of the subdifferentials of the absolute value function at each coordinate. Let $\\phi(t) = |t|$. Its subdifferential is:\n$$\n\\partial \\phi(t) = \n\\begin{cases} \n\\{ \\text{sgn}(t) \\} & \\text{if } t \\neq 0 \\\\\n[-1, 1] & \\text{if } t = 0 \n\\end{cases}\n$$\nwhere $\\text{sgn}(t)$ is the sign function, taking value $1$ for $t > 0$ and $-1$ for $t < 0$.\nSo, the subdifferential $\\partial \\|x\\|_{1}$ is a set of vectors $v \\in \\mathbb{R}^{n}$ where each component $v_{i}$ satisfies $v_{i} \\in \\partial |x_{i}|$.\n\nThe optimality condition $0 \\in \\partial J(x^{\\star})$ becomes $0 \\in \\nabla f(x^{\\star}) + \\lambda \\partial \\|x^{\\star}\\|_{1}$. This means there must exist a vector $s \\in \\partial \\|x^{\\star}\\|_{1}$ such that:\n$$\n\\nabla f(x^{\\star}) + \\lambda s = 0 \\implies A^{\\intercal}(Ax^{\\star} - y) = -\\lambda s\n$$\nWe now write this condition component-wise for $i = 1, \\dots, n$. Let $(v)_{i}$ denote the $i$-th component of a vector $v$.\n$$\n(A^{\\intercal}(Ax^{\\star} - y))_{i} = -\\lambda s_{i}, \\quad \\text{where } s_{i} \\in \\partial |x^{\\star}_{i}|\n$$\nWe distinguish between two cases for each component $x^{\\star}_{i}$:\n1.  If $x^{\\star}_{i} \\neq 0$, then $\\partial |x^{\\star}_{i}| = \\{ \\text{sgn}(x^{\\star}_{i}) \\}$. The condition becomes:\n    $$\n    (A^{\\intercal}(Ax^{\\star} - y))_{i} = -\\lambda \\, \\text{sgn}(x^{\\star}_{i})\n    $$\n2.  If $x^{\\star}_{i} = 0$, then $\\partial |x^{\\star}_{i}| = [-1, 1]$. The condition is that there exists some $s_{i} \\in [-1, 1]$ such that $(A^{\\intercal}(Ax^{\\star} - y))_{i} = -\\lambda s_{i}$. This is equivalent to:\n    $$\n    \\left| (A^{\\intercal}(Ax^{\\star} - y))_{i} \\right| \\leq \\lambda\n    $$\nThese two sets of conditions are the necessary and sufficient KKT conditions for the LASSO problem.\n\n(c) We now specialize these conditions for the candidate solution $x^{\\star} = 0 \\in \\mathbb{R}^{n}$. For this solution, every component is zero, so $x^{\\star}_{i} = 0$ for all $i = 1, \\dots, n$. Therefore, we are in the second case for all components.\nSubstituting $x^{\\star} = 0$ into the condition $| (A^{\\intercal}(Ax^{\\star} - y))_{i} | \\leq \\lambda$:\n$$\n\\left| (A^{\\intercal}(A \\cdot 0 - y))_{i} \\right| \\leq \\lambda\n$$\n$$\n\\left| (A^{\\intercal}(-y))_{i} \\right| \\leq \\lambda\n$$\n$$\n\\left| -(A^{\\intercal}y)_{i} \\right| \\leq \\lambda\n$$\n$$\n\\left| (A^{\\intercal}y)_{i} \\right| \\leq \\lambda\n$$\nThis condition must hold for all $i = 1, \\dots, n$. This can be expressed compactly using the infinity norm ($\\ell_{\\infty}$-norm), which is the maximum absolute value of a vector's components. The necessary and sufficient condition for $x^{\\star} = 0$ to be an optimal solution is:\n$$\n\\|A^{\\intercal}y\\|_{\\infty} \\leq \\lambda\n$$\nSince the problem states $\\lambda > 0$, the smallest value of $\\lambda$ for which $x^{\\star} = 0$ is optimal is $\\lambda_{\\min} = \\|A^{\\intercal}y\\|_{\\infty}$, assuming $A^{\\intercal}y \\neq 0$.\n\nNow, we compute this value for the given data:\n$$\nA = \\begin{pmatrix}\n1 & -1 & 2 & 0 \\\\\n0 & 2 & -1 & 1 \\\\\n1 & 0 & 1 & -2\n\\end{pmatrix}, \n\\qquad\ny = \\begin{pmatrix}\n2 \\\\\n-1 \\\\\n3\n\\end{pmatrix}\n$$\nFirst, we find the transpose of $A$:\n$$\nA^{\\intercal} = \\begin{pmatrix}\n1 & 0 & 1 \\\\\n-1 & 2 & 0 \\\\\n2 & -1 & 1 \\\\\n0 & 1 & -2\n\\end{pmatrix}\n$$\nNext, we compute the product $A^{\\intercal}y$:\n$$\nA^{\\intercal}y = \\begin{pmatrix}\n1 & 0 & 1 \\\\\n-1 & 2 & 0 \\\\\n2 & -1 & 1 \\\\\n0 & 1 & -2\n\\end{pmatrix}\n\\begin{pmatrix}\n2 \\\\\n-1 \\\\\n3\n\\end{pmatrix} = \\begin{pmatrix}\n(1)(2) + (0)(-1) + (1)(3) \\\\\n(-1)(2) + (2)(-1) + (0)(3) \\\\\n(2)(2) + (-1)(-1) + (1)(3) \\\\\n(0)(2) + (1)(-1) + (-2)(3)\n\\end{pmatrix} = \\begin{pmatrix}\n2 + 0 + 3 \\\\\n-2 - 2 + 0 \\\\\n4 + 1 + 3 \\\\\n0 - 1 - 6\n\\end{pmatrix} = \\begin{pmatrix}\n5 \\\\\n-4 \\\\\n8 \\\\\n-7\n\\end{pmatrix}\n$$\nFinally, we compute the infinity norm of this vector:\n$$\n\\|A^{\\intercal}y\\|_{\\infty} = \\max \\{ |5|, |-4|, |8|, |-7| \\} = \\max \\{ 5, 4, 8, 7 \\} = 8\n$$\nTherefore, the smallest value of $\\lambda$ for which $x^{\\star} = 0$ is an optimal solution is $8$.",
            "answer": "$$\n\\boxed{8}\n$$"
        },
        {
            "introduction": "Compressed sensing is effective because many real-world signals are not strictly sparse but \"compressible,\" meaning they can be well-approximated by a sparse signal. This exercise provides a hands-on analysis of this crucial concept by quantifying the compressibility of a signal with power-law decaying coefficients. By calculating the best $k$-term approximation error, you will develop a concrete understanding of how a signal's structure enables its efficient representation, which is the central motivation behind the entire field .",
            "id": "3460575",
            "problem": "Consider the standard compressed sensing setup where one quantifies the compressibility of a signal by its best $k$-term approximation error. For a sequence $x \\in \\ell_{2}$ with entries $x_{i}$ sorted in nonincreasing magnitude, the best $k$-term error in the $\\ell_{2}$ norm is defined as\n$$\n\\sigma_{k}(x)_{2} \\triangleq \\inf\\left\\{\\|x - z\\|_{2} : z \\in \\mathbb{R}^{\\mathbb{N}},\\, \\|z\\|_{0} \\leq k\\right\\},\n$$\nwhere $\\|z\\|_{0}$ denotes the number of nonzero entries of $z$, and $\\|x\\|_{2}$ is the Euclidean norm of $x$. This quantity formalizes the notion that signals with rapidly decaying coefficients are well-approximated by sparse representations, which is central to the motivation and formal guarantees in compressed sensing.\n\nLet $x$ be the sequence defined by $x_{i} = i^{-\\alpha}$ for $i \\in \\mathbb{N}$ with a fixed exponent $\\alpha > 1$. Starting only from the definitions above and standard facts about monotone series and integrals, derive the asymptotic scaling of $\\sigma_{k}(x)_{2}$ as a function of $k$ and $\\alpha$. Your final answer must be a single closed-form analytic expression giving the leading-order behavior for large $k$ (include the leading constant). No numerical evaluation is required.",
            "solution": "The problem asks for the asymptotic scaling of the best $k$-term approximation error $\\sigma_{k}(x)_{2}$ for a specific sequence $x \\in \\ell_{2}$. The sequence is defined by its components $x_{i} = i^{-\\alpha}$ for $i \\in \\mathbb{N} = \\{1, 2, 3, \\dots\\}$ with a fixed exponent $\\alpha > 1$. The condition $\\alpha > 1$ ensures that $\\|x\\|_2^2 = \\sum_{i=1}^{\\infty} i^{-2\\alpha}$ converges, as it is a p-series with exponent $2\\alpha > 2$, so $x$ is indeed in $\\ell_2$.\n\nThe best $k$-term approximation error in the $\\ell_2$ norm is given by\n$$\n\\sigma_{k}(x)_{2} \\triangleq \\inf\\left\\{\\|x - z\\|_{2} : z \\in \\mathbb{R}^{\\mathbb{N}},\\, \\|z\\|_{0} \\leq k\\right\\}\n$$\nThe infimum is achieved by choosing $z$ to be a vector that retains the $k$ largest-magnitude components of $x$ and sets all other components to zero. Let this optimal sparse approximation be denoted by $x^{(k)}$. The error is then the $\\ell_2$ norm of the residual vector $x - x^{(k)}$, which consists of the components of $x$ that were set to zero.\n\nFor the given sequence $x_i = i^{-\\alpha}$, the value of $\\alpha$ is greater than $1$, which implies $\\alpha > 0$. The function $f(i) = i^{-\\alpha}$ is positive and strictly decreasing for $i \\geq 1$. Consequently, the components $x_i$ are already sorted in nonincreasing order of magnitude: $|x_1| > |x_2| > |x_3| > \\dots$. The $k$ largest-magnitude components are therefore the first $k$ components, $\\{x_1, x_2, \\dots, x_k\\}$.\n\nThe optimal $k$-term approximation $x^{(k)}$ is thus:\n$$\n(x^{(k)})_i =\n\\begin{cases}\nx_i & \\text{if } i \\leq k \\\\\n0 & \\text{if } i > k\n\\end{cases}\n$$\nThe approximation error $\\sigma_{k}(x)_{2}$ is the norm of the \"tail\" of the sequence:\n$$\n\\sigma_{k}(x)_{2} = \\left( \\sum_{i=k+1}^{\\infty} |x_i|^2 \\right)^{1/2}\n$$\nSubstituting the expression for $x_i$, the squared error is\n$$\n\\sigma_{k}(x)_{2}^2 = \\sum_{i=k+1}^{\\infty} (i^{-\\alpha})^2 = \\sum_{i=k+1}^{\\infty} i^{-2\\alpha}\n$$\nTo find the asymptotic behavior of this sum for large $k$, we can approximate it using an integral. The function $f(t) = t^{-2\\alpha}$ is a positive, continuous, and decreasing function for $t > 0$ because its derivative, $f'(t) = -2\\alpha t^{-2\\alpha-1}$, is negative. For such a function, the sum can be bounded by integrals as follows:\n$$\n\\int_{k+1}^{\\infty} t^{-2\\alpha} dt \\leq \\sum_{i=k+1}^{\\infty} i^{-2\\alpha} \\leq \\int_{k}^{\\infty} t^{-2\\alpha} dt\n$$\nWe evaluate the integral. The condition $\\alpha > 1$ implies $2\\alpha > 2$, so $2\\alpha - 1 > 1$.\n$$\n\\int_{a}^{\\infty} t^{-2\\alpha} dt = \\left[ \\frac{t^{-2\\alpha+1}}{-2\\alpha+1} \\right]_{a}^{\\infty} = 0 - \\frac{a^{1-2\\alpha}}{1-2\\alpha} = \\frac{a^{1-2\\alpha}}{2\\alpha-1}\n$$\nApplying this result to our bounds for $\\sigma_{k}(x)_{2}^2$:\n$$\n\\frac{(k+1)^{1-2\\alpha}}{2\\alpha-1} \\leq \\sigma_{k}(x)_{2}^2 \\leq \\frac{k^{1-2\\alpha}}{2\\alpha-1}\n$$\nTo find the leading-order asymptotic behavior, we examine the ratio of the upper and lower bounds as $k \\to \\infty$:\n$$\n\\lim_{k\\to\\infty} \\frac{\\frac{(k+1)^{1-2\\alpha}}{2\\alpha-1}}{\\frac{k^{1-2\\alpha}}{2\\alpha-1}} = \\lim_{k\\to\\infty} \\left(\\frac{k+1}{k}\\right)^{1-2\\alpha} = \\lim_{k\\to\\infty} \\left(1 + \\frac{1}{k}\\right)^{1-2\\alpha} = 1\n$$\nSince both the lower and upper bounds have the same asymptotic behavior, the Squeeze Theorem implies that the sum itself shares this behavior. Thus, for large $k$, we can write the asymptotic equivalence:\n$$\n\\sigma_{k}(x)_{2}^2 \\sim \\frac{k^{1-2\\alpha}}{2\\alpha-1}\n$$\nwhere $f(k) \\sim g(k)$ means $\\lim_{k\\to\\infty} f(k)/g(k) = 1$. Taking the square root of both sides gives the asymptotic scaling for $\\sigma_{k}(x)_{2}$:\n$$\n\\sigma_{k}(x)_{2} \\sim \\sqrt{\\frac{k^{1-2\\alpha}}{2\\alpha-1}} = \\frac{(k^{1-2\\alpha})^{1/2}}{\\sqrt{2\\alpha-1}} = \\frac{k^{(1-2\\alpha)/2}}{\\sqrt{2\\alpha-1}}\n$$\nThis simplifies to the final expression for the leading-order behavior of the approximation error:\n$$\n\\sigma_{k}(x)_{2} \\sim \\frac{k^{1/2 - \\alpha}}{\\sqrt{2\\alpha-1}}\n$$\nThis expression gives the scaling with $k$ as $k^{1/2 - \\alpha}$ and includes the leading constant $(2\\alpha-1)^{-1/2}$.",
            "answer": "$$\\boxed{\\frac{k^{1/2 - \\alpha}}{\\sqrt{2\\alpha-1}}}$$"
        }
    ]
}