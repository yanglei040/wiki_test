{
    "hands_on_practices": [
        {
            "introduction": "A robust understanding of Signal-to-Noise Ratio (SNR) begins with its fundamental definition and the logarithmic decibel (dB) scale used to express it. This exercise takes you back to first principles to derive the correct dB conversion for power and amplitude ratios, clarifying the common confusion between the $10\\log_{10}(\\cdot)$ and $20\\log_{10}(\\cdot)$ formulas. By grounding these definitions in a canonical compressed sensing model with Gaussian noise, you will develop the essential skill of calculating and interpreting SNR in a practical context .",
            "id": "3462108",
            "problem": "Consider the linear measurement model in compressed sensing, $\\mathbf{y}=\\mathbf{A}\\mathbf{x}+\\mathbf{w}$, where $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ has independent and identically distributed (i.i.d.) entries $A_{ij}\\sim\\mathcal{N}(0,1/m)$, the unknown signal $\\mathbf{x}\\in\\mathbb{R}^{n}$ is $K$-sparse with exactly $K$ nonzero entries each equal to an amplitude $a>0$, and the noise $\\mathbf{w}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I}_{m})$ is Additive White Gaussian Noise (AWGN). The Signal-to-Noise Ratio (SNR) is defined as the ratio of average signal power to average noise power, and decibel (dB) is a logarithmic measure of ratios. \n\n(a) Starting from the fundamental definitions that (i) power is proportional to the square of root-mean-square (RMS) amplitude in linear systems, and (ii) the bel is the base-$10$ logarithm of a power ratio while the decibel (dB) is one-tenth of a bel, derive the decibel representation for the Signal-to-Noise Ratio, $\\mathrm{SNR}$. Then, using only these definitions, explain precisely when expressions of the form $10\\log_{10}(\\cdot)$ versus $20\\log_{10}(\\cdot)$ should be used in compressed sensing contexts that involve either power ratios or RMS amplitude ratios (for example, ratios of $\\ell_{2}$ energies versus ratios of $\\ell_{2}$ norms). Do not assume any specialized formula beyond these definitions.\n\n(b) For the model above, compute the expected linear-scale $\\mathrm{SNR}$ in terms of $K$, $a$, $m$, and $\\sigma^{2}$.\n\n(c) Using the result of part (b) and the appropriate decibel conversion established in part (a), evaluate the SNR in decibels for the specific parameters $m=200$, $K=10$, $a=0.5$, and $\\sigma^{2}=10^{-3}$. Express the final SNR in dB and round your answer to four significant figures.",
            "solution": "We begin from first principles about power, amplitude, and logarithmic ratio measures.\n\nPart (a). The Signal-to-Noise Ratio (SNR) is defined as a ratio of powers:\n$$\n\\mathrm{SNR} \\equiv \\frac{P_{\\text{signal}}}{P_{\\text{noise}}}.\n$$\nThe bel is defined as the base-$10$ logarithm of a power ratio. If $R$ is a power ratio, then the number of bels is $\\log_{10}(R)$. The decibel (dB) is one-tenth of a bel, so a power ratio $R$ measured in dB is\n$$\n\\mathrm{dB}(R) = 10\\log_{10}(R).\n$$\nTherefore, the decibel representation of the Signal-to-Noise Ratio is\n$$\n\\mathrm{SNR}_{\\mathrm{dB}} = 10\\log_{10}(\\mathrm{SNR}).\n$$\nNow, consider an amplitude ratio, for example an RMS amplitude ratio $r \\equiv A_{\\text{signal,RMS}}/A_{\\text{noise,RMS}}$. In linear systems, power is proportional to the square of RMS amplitude. Hence the corresponding power ratio is $R = r^{2}$. The decibel representation becomes\n$$\n\\mathrm{dB}(r) = 10\\log_{10}(r^{2}) = 20\\log_{10}(r).\n$$\nThus, $10\\log_{10}(\\cdot)$ must be used for power ratios (for example, ratios of $\\ell_{2}$ energies such as $\\|\\cdot\\|_{2}^{2}$), while $20\\log_{10}(\\cdot)$ is appropriate for amplitude ratios (for example, ratios of $\\ell_{2}$ norms or RMS amplitudes). In compressed sensing practice, when SNR is defined as $\\|\\mathbf{A}\\mathbf{x}\\|_{2}^{2}/\\|\\mathbf{w}\\|_{2}^{2}$ one uses $10\\log_{10}$, whereas if one works with $\\|\\mathbf{A}\\mathbf{x}\\|_{2}/\\|\\mathbf{w}\\|_{2}$ one uses $20\\log_{10}$; these are equivalent because of the square relationship between power and amplitude.\n\nPart (b). We compute the expected linear-scale SNR for the given random model. The signal component is $\\mathbf{s}=\\mathbf{A}\\mathbf{x}$ and the noise is $\\mathbf{w}$. The signal power is the expected squared $\\ell_{2}$ norm:\n$$\n\\mathbb{E}[\\|\\mathbf{A}\\mathbf{x}\\|_{2}^{2}]\n= \\mathbb{E}[\\mathbf{x}^{\\top}\\mathbf{A}^{\\top}\\mathbf{A}\\mathbf{x}]\n= \\mathbf{x}^{\\top}\\mathbb{E}[\\mathbf{A}^{\\top}\\mathbf{A}]\\mathbf{x}.\n$$\nFor i.i.d. entries $A_{ij}\\sim\\mathcal{N}(0,1/m)$, it is a standard fact that $\\mathbb{E}[\\mathbf{A}^{\\top}\\mathbf{A}]=\\mathbf{I}_{n}$, hence\n$$\n\\mathbb{E}[\\|\\mathbf{A}\\mathbf{x}\\|_{2}^{2}] = \\|\\mathbf{x}\\|_{2}^{2}.\n$$\nThe noise power is\n$$\n\\mathbb{E}[\\|\\mathbf{w}\\|_{2}^{2}] = \\sum_{i=1}^{m}\\mathbb{E}[w_{i}^{2}] = m\\sigma^{2}.\n$$\nTherefore, the expected linear-scale SNR is\n$$\n\\mathrm{SNR} \\equiv \\frac{\\mathbb{E}[\\|\\mathbf{A}\\mathbf{x}\\|_{2}^{2}]}{\\mathbb{E}[\\|\\mathbf{w}\\|_{2}^{2}]}\n= \\frac{\\|\\mathbf{x}\\|_{2}^{2}}{m\\sigma^{2}}.\n$$\nGiven that $\\mathbf{x}$ has exactly $K$ nonzeros each equal to $a$, we have\n$$\n\\|\\mathbf{x}\\|_{2}^{2} = Ka^{2},\n$$\nso\n$$\n\\mathrm{SNR} = \\frac{Ka^{2}}{m\\sigma^{2}}.\n$$\n\nPart (c). Insert the specified parameters $m=200$, $K=10$, $a=0.5$, and $\\sigma^{2}=10^{-3}$ into the linear-scale SNR:\n$$\n\\mathrm{SNR} = \\frac{10\\cdot(0.5)^{2}}{200\\cdot 10^{-3}} = \\frac{10\\cdot 0.25}{0.2} = \\frac{2.5}{0.2} = 12.5.\n$$\nBecause this is a power ratio, the appropriate conversion is $10\\log_{10}(\\cdot)$. Hence\n$$\n\\mathrm{SNR}_{\\mathrm{dB}} = 10\\log_{10}(12.5).\n$$\nFor a numerical value, note that $\\log_{10}(12.5)=\\log_{10}(25)-\\log_{10}(2)$, with $\\log_{10}(25) \\approx 1.397940$ and $\\log_{10}(2) \\approx 0.301030$, yielding\n$$\n\\mathrm{SNR}_{\\mathrm{dB}} = 10(1.397940 - 0.301030) = 10(1.096910) \\approx 10.9691\n$$\nRounded to four significant figures and expressed in decibels (dB), the result is $10.97$ dB.",
            "answer": "$$\\boxed{10.97}$$"
        },
        {
            "introduction": "A cornerstone of compressed sensing is the ability of certain random matrices to preserve geometric structure, a concept formalized by the Restricted Isometry Property (RIP). This practice explores a closely related phenomenon: how the SNR is preserved, in an asymptotic sense, through the random measurement process. By applying concentration of measure principles to both the signal and noise energies, you will rigorously demonstrate why the output SNR faithfully reflects the input SNR as the number of measurements grows large .",
            "id": "3462035",
            "problem": "Consider a measurement model in compressed sensing with a random sensing matrix and additive measurement noise. Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ have independent and identically distributed entries $a_{ij} \\sim \\mathcal{N}(0, 1/m)$, and let $\\mathbf{x} \\in \\mathbb{R}^{n}$ be a fixed deterministic vector. Define $\\mathbf{y} = \\mathbf{A} \\mathbf{x} + \\mathbf{e}$, where $\\mathbf{e} \\in \\mathbb{R}^{m}$ has independent, zero-mean, variance $\\sigma^{2}$ components that are sub-Gaussian in the sense that for each $i \\in \\{1, \\dots, m\\}$ and all $t \\in \\mathbb{R}$, $\\mathbb{E}[\\exp(t e_{i})] \\le \\exp(\\sigma^{2} t^{2} / 2)$. The Signal-to-Noise Ratio (SNR) is defined as the ratio of signal energy to noise energy. For the input and output of the sensing system, define\n$$\n\\mathrm{SNR}_{\\mathrm{in}} = \\frac{\\|\\mathbf{x}\\|_{2}^{2}}{m \\sigma^{2}}, \\qquad \\mathrm{SNR}_{\\mathrm{out}} = \\frac{\\|\\mathbf{A} \\mathbf{x}\\|_{2}^{2}}{\\|\\mathbf{e}\\|_{2}^{2}}.\n$$\nStarting only from the definitions of sub-Gaussian random variables and well-tested concentration facts for Gaussian and sub-Gaussian sums, derive why the random matrix $\\mathbf{A}$ approximately preserves Euclidean norms with high probability (that is, for fixed $\\mathbf{x}$, the quantity $\\|\\mathbf{A} \\mathbf{x}\\|_{2}^{2}$ concentrates around $\\|\\mathbf{x}\\|_{2}^{2}$), and explain how $\\|\\mathbf{e}\\|_{2}^{2}$ concentrates around $m \\sigma^{2}$. Using these two concentration phenomena as a fundamental base, determine the almost sure limiting value of the ratio\n$$\nR_{m} = \\frac{\\mathrm{SNR}_{\\mathrm{out}}}{\\mathrm{SNR}_{\\mathrm{in}}}\n$$\nas $m \\to \\infty$ with $n$ fixed and $\\mathbf{x}$ fixed. Express your final answer as a single real number with no units. No rounding is required.",
            "solution": "The problem is well-posed and scientifically grounded in the theory of compressed sensing and high-dimensional probability. We shall proceed with a rigorous derivation of the requested limiting value.\n\nThe quantity of interest is the almost sure limiting value of the ratio $R_m = \\frac{\\mathrm{SNR}_{\\mathrm{out}}}{\\mathrm{SNR}_{\\mathrm{in}}}$ as the number of measurements $m$ approaches infinity, while the signal dimension $n$ and the signal vector $\\mathbf{x}$ remain fixed. We assume that the signal is non-trivial, i.e., $\\mathbf{x} \\neq \\mathbf{0}$, and that there is noise, i.e., $\\sigma^2 > 0$. Otherwise, the Signal-to-Noise Ratio (SNR) definitions are degenerate.\n\nFirst, we write the explicit expression for $R_m$ by substituting the given definitions of $\\mathrm{SNR}_{\\mathrm{in}}$ and $\\mathrm{SNR}_{\\mathrm{out}}$:\n$$\nR_m = \\frac{\\mathrm{SNR}_{\\mathrm{out}}}{\\mathrm{SNR}_{\\mathrm{in}}} = \\frac{\\|\\mathbf{A} \\mathbf{x}\\|_{2}^{2} / \\|\\mathbf{e}\\|_{2}^{2}}{\\|\\mathbf{x}\\|_{2}^{2} / (m \\sigma^{2})}\n$$\nRearranging the terms, we get a product of two factors:\n$$\nR_m = \\left( \\frac{\\|\\mathbf{A} \\mathbf{x}\\|_{2}^{2}}{\\|\\mathbf{x}\\|_{2}^{2}} \\right) \\left( \\frac{m \\sigma^{2}}{\\|\\mathbf{e}\\|_{2}^{2}} \\right)\n$$\nWe will analyze the almost sure limit of each factor separately as $m \\to \\infty$.\n\n1. Analysis of the first factor: $\\frac{\\|\\mathbf{A} \\mathbf{x}\\|_{2}^{2}}{\\|\\mathbf{x}\\|_{2}^{2}}$\n\nThis factor relates to the norm-preserving property of the random matrix $\\mathbf{A}$. Let $\\mathbf{z} = \\mathbf{A}\\mathbf{x} \\in \\mathbb{R}^{m}$. The $i$-th component of $\\mathbf{z}$ is given by $z_i = \\sum_{j=1}^{n} a_{ij} x_j$. Since the entries $a_{ij}$ of the matrix $\\mathbf{A}$ are independent and identically distributed (i.i.d.) Gaussian random variables with $a_{ij} \\sim \\mathcal{N}(0, 1/m)$, and $\\mathbf{x}$ is a fixed vector, each $z_i$ is a linear combination of Gaussian random variables, and is therefore itself a Gaussian random variable.\n\nThe mean of $z_i$ is:\n$$\n\\mathbb{E}[z_i] = \\mathbb{E}\\left[\\sum_{j=1}^{n} a_{ij} x_j\\right] = \\sum_{j=1}^{n} x_j \\mathbb{E}[a_{ij}] = \\sum_{j=1}^{n} x_j \\cdot 0 = 0\n$$\nThe variance of $z_i$ is, using the independence of the $a_{ij}$ for different $j$:\n$$\n\\mathrm{Var}(z_i) = \\mathrm{Var}\\left(\\sum_{j=1}^{n} a_{ij} x_j\\right) = \\sum_{j=1}^{n} x_j^2 \\mathrm{Var}(a_{ij}) = \\sum_{j=1}^{n} x_j^2 \\left(\\frac{1}{m}\\right) = \\frac{1}{m} \\|\\mathbf{x}\\|_{2}^{2}\n$$\nSo, $z_i \\sim \\mathcal{N}(0, \\|\\mathbf{x}\\|_{2}^{2}/m)$. Since the rows of $\\mathbf{A}$ are independent, the components $z_i$ for $i=1, \\dots, m$ are i.i.d.\n\nThe squared Euclidean norm is $\\|\\mathbf{A}\\mathbf{x}\\|_{2}^{2} = \\|\\mathbf{z}\\|_{2}^{2} = \\sum_{i=1}^{m} z_i^2$. This is a sum of $m$ i.i.d. random variables whose distribution depends on $m$. To apply the Strong Law of Large Numbers (SLLN), it is more convenient to define a rescaled set of variables whose distribution does not depend on $m$. Let $W_i = \\sqrt{m} z_i$.\nThe variables $W_i$ for $i=1, \\dots, m$ are i.i.d. Gaussian random variables. Their mean is $\\mathbb{E}[W_i] = \\sqrt{m}\\mathbb{E}[z_i] = 0$, and their variance is $\\mathrm{Var}(W_i) = m \\mathrm{Var}(z_i) = m \\cdot \\frac{\\|\\mathbf{x}\\|_{2}^{2}}{m} = \\|\\mathbf{x}\\|_{2}^{2}$. Thus, $W_i \\sim \\mathcal{N}(0, \\|\\mathbf{x}\\|_{2}^{2})$.\n\nWe can now express $\\|\\mathbf{A}\\mathbf{x}\\|_{2}^{2}$ in terms of the $W_i$:\n$$\n\\|\\mathbf{A}\\mathbf{x}\\|_{2}^{2} = \\sum_{i=1}^{m} z_i^2 = \\sum_{i=1}^{m} \\left(\\frac{W_i}{\\sqrt{m}}\\right)^2 = \\frac{1}{m} \\sum_{i=1}^{m} W_i^2\n$$\nThe quantity $\\frac{1}{m} \\sum_{i=1}^{m} W_i^2$ is the sample mean of the i.i.d. random variables $\\{W_i^2\\}_{i=1}^m$. The expected value of each $W_i^2$ is $\\mathbb{E}[W_i^2] = \\mathrm{Var}(W_i) + (\\mathbb{E}[W_i])^2 = \\|\\mathbf{x}\\|_{2}^{2} + 0^2 = \\|\\mathbf{x}\\|_{2}^{2}$. Since $\\mathbb{E}[|W_i^2|] = \\|\\mathbf{x}\\|_2^2  \\infty$ (because $\\mathbf{x}$ is a fixed vector), the SLLN applies.\nBy the SLLN, as $m \\to \\infty$:\n$$\n\\|\\mathbf{A}\\mathbf{x}\\|_{2}^{2} = \\frac{1}{m} \\sum_{i=1}^{m} W_i^2 \\xrightarrow{a.s.} \\mathbb{E}[W_1^2] = \\|\\mathbf{x}\\|_{2}^{2}\n$$\nThis demonstrates the concentration of $\\|\\mathbf{A}\\mathbf{x}\\|_{2}^{2}$ around $\\|\\mathbf{x}\\|_{2}^{2}$ for large $m$, which is a key property of such random projections. Consequently, for the first factor in $R_m$, assuming $\\|\\mathbf{x}\\|_2^2 > 0$:\n$$\n\\lim_{m \\to \\infty} \\frac{\\|\\mathbf{A} \\mathbf{x}\\|_{2}^{2}}{\\|\\mathbf{x}\\|_{2}^{2}} \\xrightarrow{a.s.} \\frac{\\|\\mathbf{x}\\|_{2}^{2}}{\\|\\mathbf{x}\\|_{2}^{2}} = 1\n$$\n\n2. Analysis of the second factor: $\\frac{m \\sigma^{2}}{\\|\\mathbf{e}\\|_{2}^{2}}$\n\nThis factor involves the energy of the noise vector $\\mathbf{e}$. We are given that $\\|\\mathbf{e}\\|_{2}^{2} = \\sum_{i=1}^{m} e_i^2$, where the components $e_i$ are i.i.d. random variables with mean $\\mathbb{E}[e_i] = 0$ and variance $\\mathrm{Var}(e_i) = \\sigma^2$.\nLet's consider the sequence of i.i.d. random variables $\\{e_i^2\\}_{i=1}^{m}$. The expectation of each term is:\n$$\n\\mathbb{E}[e_i^2] = \\mathrm{Var}(e_i) + (\\mathbb{E}[e_i])^2 = \\sigma^2 + 0^2 = \\sigma^2\n$$\nThe problem states that the $e_i$ are sub-Gaussian, which implies that all of their moments are finite. Thus, $\\mathbb{E}[|e_i^2|] = \\mathbb{E}[e_i^2] = \\sigma^2  \\infty$. The condition for the SLLN is satisfied.\nApplying the SLLN to the sample mean of the $e_i^2$ variables:\n$$\n\\frac{1}{m} \\|\\mathbf{e}\\|_{2}^{2} = \\frac{1}{m} \\sum_{i=1}^{m} e_i^2 \\xrightarrow{a.s.} \\mathbb{E}[e_1^2] = \\sigma^2\n$$\nThis explains how $\\|\\mathbf{e}\\|_{2}^{2}$ concentrates around the value $m\\sigma^2$. For the term in our ratio, assuming $\\sigma^2 > 0$, we have:\n$$\n\\lim_{m \\to \\infty} \\frac{\\|\\mathbf{e}\\|_{2}^{2}}{m \\sigma^{2}} = \\frac{1}{\\sigma^2} \\lim_{m \\to \\infty} \\left(\\frac{1}{m} \\sum_{i=1}^{m} e_i^2 \\right) \\xrightarrow{a.s.} \\frac{1}{\\sigma^2} \\cdot \\sigma^2 = 1\n$$\nBy the continuous mapping theorem, the limit of the reciprocal is the reciprocal of the limit:\n$$\n\\lim_{m \\to \\infty} \\frac{m \\sigma^{2}}{\\|\\mathbf{e}\\|_{2}^{2}} \\xrightarrow{a.s.} 1\n$$\n\n3. Synthesis and Final Result\n\nWe have established the almost sure limits of both factors in the expression for $R_m$. Since the almost sure limit of a product of random variables is the product of their almost sure limits (provided they exist and are finite), we can combine our results:\n$$\n\\lim_{m \\to \\infty} R_m = \\left( \\lim_{m \\to \\infty} \\frac{\\|\\mathbf{A} \\mathbf{x}\\|_{2}^{2}}{\\|\\mathbf{x}\\|_{2}^{2}} \\right) \\left( \\lim_{m \\to \\infty} \\frac{m \\sigma^{2}}{\\|\\mathbf{e}\\|_{2}^{2}} \\right)\n$$\nSubstituting the limits we derived:\n$$\n\\lim_{m \\to \\infty} R_m \\xrightarrow{a.s.} 1 \\cdot 1 = 1\n$$\nThe almost sure limiting value of the ratio of the output SNR to the input SNR is $1$. This indicates that for a large number of measurements, the random sensing process, on average, preserves the Signal-to-Noise Ratio.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Moving from analysis to application, this exercise demonstrates how a precise noise model enables the design of principled algorithms for sparse signal recovery. You will frame the challenge of identifying the non-zero signal components—the \"support\"—as a formal multiple hypothesis testing problem . This powerful statistical perspective allows you to derive an explicit detection threshold that controls the probability of false discoveries, creating a direct and quantitative link between the noise variance $\\sigma^2$ and algorithm performance.",
            "id": "3462105",
            "problem": "Consider a linear inverse problem in compressed sensing with measurement model $\\mathbf{y} = \\mathbf{A} \\mathbf{x} + \\mathbf{w}$, where $\\mathbf{y} \\in \\mathbb{R}^{m}$, $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $\\mathbf{x} \\in \\mathbb{R}^{n}$ is an unknown sparse vector, and $\\mathbf{w} \\in \\mathbb{R}^{m}$ is additive noise. Assume the columns of $\\mathbf{A}$ are orthonormal so that $\\mathbf{A}^{\\top} \\mathbf{A} = \\mathbf{I}_{n}$, and the noise is white Gaussian, $\\mathbf{w} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{m})$, with known variance $\\sigma^{2} > 0$. Define the proxy statistic $\\mathbf{t} = \\mathbf{A}^{\\top} \\mathbf{y} \\in \\mathbb{R}^{n}$ and consider support recovery as simultaneous hypothesis testing of coordinate-wise nulls $H_{0,j}: x_{j} = 0$, for $j = 1, \\dots, n$, using the two-sided rule that declares coordinate $j$ active if $|t_{j}| \\geq \\tau$ for some threshold $\\tau > 0$. \n\nUsing first principles—the Gaussian noise model, the transformation properties of multivariate Gaussian distributions under linear maps, and the union bound—formulate this as a multiple testing problem on the coordinates of $\\mathbf{A}^{\\top} \\mathbf{y}$ and derive the smallest Bonferroni-calibrated threshold $\\tau$ that guarantees the Family-Wise Error Rate (FWER; probability of at least one false rejection among all true nulls) is at most a desired level $\\alpha \\in (0,1)$, uniformly over all sparse supports. Express your final answer as a closed-form analytic expression in terms of $\\sigma$, $\\alpha$, and $n$. No numerical approximation is required.",
            "solution": "The objective is to find the smallest threshold $\\tau$ such that the Family-Wise Error Rate (FWER) for the multiple testing problem is no greater than a specified level $\\alpha$.\n\nFirst, we analyze the distribution of the proxy statistic $\\mathbf{t} = \\mathbf{A}^{\\top} \\mathbf{y}$. Substituting the measurement model $\\mathbf{y} = \\mathbf{A} \\mathbf{x} + \\mathbf{w}$, we get:\n$$\\mathbf{t} = \\mathbf{A}^{\\top}(\\mathbf{A} \\mathbf{x} + \\mathbf{w}) = \\mathbf{A}^{\\top} \\mathbf{A} \\mathbf{x} + \\mathbf{A}^{\\top} \\mathbf{w}$$\nUsing the given condition that the columns of $\\mathbf{A}$ are orthonormal, $\\mathbf{A}^{\\top} \\mathbf{A} = \\mathbf{I}_{n}$ (the $n \\times n$ identity matrix), the expression simplifies to:\n$$\\mathbf{t} = \\mathbf{I}_{n} \\mathbf{x} + \\mathbf{A}^{\\top} \\mathbf{w} = \\mathbf{x} + \\mathbf{A}^{\\top} \\mathbf{w}$$\nLet's define a new noise vector $\\mathbf{v} = \\mathbf{A}^{\\top} \\mathbf{w}$. Since $\\mathbf{w}$ is a multivariate Gaussian random vector, $\\mathbf{v}$ is also a multivariate Gaussian random vector, as it is a linear transformation of $\\mathbf{w}$. We determine its distribution. The mean of $\\mathbf{v}$ is:\n$$\\mathbb{E}[\\mathbf{v}] = \\mathbb{E}[\\mathbf{A}^{\\top} \\mathbf{w}] = \\mathbf{A}^{\\top} \\mathbb{E}[\\mathbf{w}] = \\mathbf{A}^{\\top} \\mathbf{0} = \\mathbf{0}$$\nThe covariance matrix of $\\mathbf{v}$ is:\n$$\\text{Cov}(\\mathbf{v}) = \\mathbb{E}[\\mathbf{v} \\mathbf{v}^{\\top}] - \\mathbb{E}[\\mathbf{v}]\\mathbb{E}[\\mathbf{v}]^{\\top} = \\mathbb{E}[(\\mathbf{A}^{\\top} \\mathbf{w})(\\mathbf{A}^{\\top} \\mathbf{w})^{\\top}] = \\mathbb{E}[\\mathbf{A}^{\\top} \\mathbf{w} \\mathbf{w}^{\\top} \\mathbf{A}] = \\mathbf{A}^{\\top} \\mathbb{E}[\\mathbf{w} \\mathbf{w}^{\\top}] \\mathbf{A}$$\nGiven that $\\mathbf{w} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{m})$, its covariance matrix is $\\mathbb{E}[\\mathbf{w} \\mathbf{w}^{\\top}] = \\sigma^{2} \\mathbf{I}_{m}$. Substituting this, we obtain:\n$$\\text{Cov}(\\mathbf{v}) = \\mathbf{A}^{\\top} (\\sigma^{2} \\mathbf{I}_{m}) \\mathbf{A} = \\sigma^{2} (\\mathbf{A}^{\\top} \\mathbf{A}) = \\sigma^{2} \\mathbf{I}_{n}$$\nThus, the transformed noise vector is $\\mathbf{v} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{n})$. This implies that the components $v_j$ of $\\mathbf{v}$ are independent and identically distributed (i.i.d.) as $v_j \\sim \\mathcal{N}(0, \\sigma^{2})$.\nThe proxy statistic can be written component-wise as $t_j = x_j + v_j$ for $j = 1, \\dots, n$.\n\nNext, we consider the hypothesis test for each coordinate $j$. The null hypothesis is $H_{0,j}: x_j = 0$. Under this null hypothesis, the test statistic $t_j$ has the distribution of the noise component $v_j$:\n$$t_j | H_{0,j} \\sim \\mathcal{N}(0, \\sigma^{2})$$\nA Type I error for the $j$-th test occurs if we reject $H_{0,j}$ when it is true. The rejection rule is $|t_j| \\ge \\tau$. The probability of this event, let's call it $p_j$, is:\n$$p_j = P(|t_j| \\ge \\tau | H_{0,j})$$\nTo compute this probability, we can standardize the random variable $t_j$ by dividing by its standard deviation $\\sigma$. Let $Z_j = t_j/\\sigma$, where $Z_j | H_{0,j} \\sim \\mathcal{N}(0, 1)$.\n$$p_j = P(|Z_j| \\ge \\frac{\\tau}{\\sigma}) = P\\left(Z_j \\ge \\frac{\\tau}{\\sigma}\\right) + P\\left(Z_j \\le -\\frac{\\tau}{\\sigma}\\right)$$\nLet $\\Phi(z)$ denote the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0, 1)$. Then $P(Z_j \\ge z) = 1 - \\Phi(z)$ and, by symmetry, $P(Z_j \\le -z) = \\Phi(-z) = 1 - \\Phi(z)$. Therefore:\n$$p_j = \\left(1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\\right) + \\left(1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\\right) = 2\\left(1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\\right)$$\nThis probability is the same for all $j$ for which $H_{0,j}$ is true.\n\nThe FWER is the probability of making at least one Type I error. Let $S_0 = \\{j \\mid x_j = 0\\}$ be the index set of true null hypotheses. Let $E_j$ be the event that $H_{0,j}$ is falsely rejected, i.e., $E_j = \\{|t_j| \\ge \\tau\\}$ for $j \\in S_0$.\n$$\\text{FWER} = P\\left(\\bigcup_{j \\in S_0} E_j\\right)$$\nWe apply the union bound (also known as Boole's inequality), which is the basis for the Bonferroni correction:\n$$\\text{FWER} = P\\left(\\bigcup_{j \\in S_0} E_j\\right) \\le \\sum_{j \\in S_0} P(E_j)$$\nSince $P(E_j) = p_j$ for all $j \\in S_0$, and this probability is independent of $j$, we have:\n$$\\text{FWER} \\le \\sum_{j \\in S_0} p_j = |S_0| \\cdot 2\\left(1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\\right)$$\nwhere $|S_0|$ is the number of true null hypotheses.\n\nThe problem requires that the FWER control holds uniformly over all sparse supports. This means we must bound the FWER for any possible configuration of zero and non-zero entries in $\\mathbf{x}$. The bound $|S_0| \\cdot p_j$ depends on the unknown size of the true null set, $|S_0|$. To ensure the guarantee is uniform, we must consider the worst-case scenario for this bound. The bound is maximized when $|S_0|$ is as large as possible. The maximum possible value for $|S_0|$ is $n$ (this occurs when $\\mathbf{x}=\\mathbf{0}$, i.e., all hypotheses are null).\nTherefore, to guarantee $\\text{FWER} \\le \\alpha$ for any support, we must enforce the worst-case bound:\n$$n \\cdot 2\\left(1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\\right) \\le \\alpha$$\nWe now solve for the threshold $\\tau$. To find the smallest $\\tau$ that satisfies this inequality, we can treat it as an equality:\n$$n \\cdot 2\\left(1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\\right) = \\alpha$$\n$$\\implies 1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right) = \\frac{\\alpha}{2n}$$\n$$\\implies \\Phi\\left(\\frac{\\tau}{\\sigma}\\right) = 1 - \\frac{\\alpha}{2n}$$\nTo isolate $\\tau$, we apply the inverse of the standard normal CDF, which is the quantile function, denoted $\\Phi^{-1}$:\n$$\\frac{\\tau}{\\sigma} = \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2n}\\right)$$\nFinally, solving for $\\tau$ yields the desired expression:\n$$\\tau = \\sigma \\cdot \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2n}\\right)$$\nThis is the Bonferroni-calibrated threshold that guarantees the FWER is at most $\\alpha$, uniformly over all possible supports of the sparse signal $\\mathbf{x}$.",
            "answer": "$$\\boxed{\\sigma \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2n}\\right)}$$"
        }
    ]
}