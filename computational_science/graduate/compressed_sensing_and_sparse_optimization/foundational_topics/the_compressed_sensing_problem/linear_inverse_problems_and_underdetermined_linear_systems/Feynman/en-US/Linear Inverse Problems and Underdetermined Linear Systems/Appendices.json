{
    "hands_on_practices": [
        {
            "introduction": "To truly understand how LASSO achieves sparsity, it is invaluable to trace its behavior step-by-step. This exercise  guides you through computing the entire solution path for a small-scale problem, revealing how the set of active (non-zero) coefficients changes as the regularization parameter $\\lambda$ varies. By manually applying the Karush-Kuhn-Tucker (KKT) optimality conditions, you will gain a concrete understanding of how these conditions define the piecewise-linear path of the coefficients, demystifying the process of variable selection and shrinkage.",
            "id": "3457315",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem with measurement matrix $A \\in \\mathbb{R}^{2 \\times 4}$, unknown coefficient vector $x \\in \\mathbb{R}^{4}$, and observation $y \\in \\mathbb{R}^{2}$:\n$$\n\\min_{x \\in \\mathbb{R}^{4}} \\ \\frac{1}{2} \\| y - A x \\|_{2}^{2} + \\lambda \\| x \\|_{1},\n$$\nwhere the columns of $A$ are unit-norm atoms defined by\n$$\na_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad a_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad a_{3} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad a_{4} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix},\n$$\nand the observation is\n$$\ny = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}.\n$$\nLet $x(\\lambda)$ denote the unique LASSO solution for each $\\lambda \\geq 0$. Starting from the fundamental Karush-Kuhn-Tucker (KKT) conditions for LASSO optimality, determine all breakpoints $\\lambda$ at which the active set and/or sign pattern of $x(\\lambda)$ changes, and derive the exact, piecewise-linear expressions for $x(\\lambda)$ on each segment for $\\lambda$ decreasing from the largest breakpoint down to $0$. For each open segment and at each breakpoint, verify the KKT conditions explicitly for both active and inactive coordinates.\n\nExpress all quantities in exact closed form. As your final answer, report the vector of breakpoints in descending order as a row matrix. Do not round your final answer. The final answer must be given as a single expression without units.",
            "solution": "The user wants to find the solution path for a specific LASSO problem.\n\nThe LASSO problem is given by:\n$$ \\min_{x \\in \\mathbb{R}^{4}} \\ \\frac{1}{2} \\| y - A x \\|_{2}^{2} + \\lambda \\| x \\|_{1} $$\nThe Karush-Kuhn-Tucker (KKT) conditions for optimality state that the zero vector must be in the subdifferential of the objective function. This implies the existence of a vector $s \\in \\partial \\|x\\|_1$ such that:\n$$ A^T(y - Ax) = \\lambda s $$\nwhere $s$ is the subgradient of the $\\ell_1$-norm, with components $s_i$ satisfying:\n$$\ns_i = \\begin{cases}\n\\text{sign}(x_i)  \\text{if } x_i \\neq 0 \\\\\n\\in [-1, 1]  \\text{if } x_i = 0\n\\end{cases}\n$$\nLet $\\mathcal{I} = \\{i \\mid x_i \\neq 0\\}$ be the active set. The KKT conditions can be split into two parts:\n1.  For active indices $i \\in \\mathcal{I}$: $a_i^T(y - Ax) = \\lambda \\cdot \\text{sign}(x_i)$.\n2.  For inactive indices $j \\notin \\mathcal{I}$: $|a_j^T(y - Ax)| \\leq \\lambda$.\n\nWe trace the solution path $x(\\lambda)$ as $\\lambda$ decreases from a large value.\n\n**Segment 1: Finding the first breakpoint $\\lambda_1$**\n\nFor a sufficiently large $\\lambda$, the $\\ell_1$ penalty dominates, forcing the solution to be $x(\\lambda) = 0$. In this case, the active set is empty, $\\mathcal{I} = \\emptyset$. The KKT conditions reduce to $|a_i^T(y-A\\cdot 0)| \\leq \\lambda$ for all $i=1, 2, 3, 4$. This is equivalent to $\\lambda \\geq \\max_i |a_i^T y|$.\nLet's compute the correlations $c = A^T y$:\n$A = \\begin{pmatrix} 1  0  \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\\\ 0  1  \\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}} \\end{pmatrix}$, $y = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\n$c_1 = a_1^T y = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 2$.\n$c_2 = a_2^T y = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 1$.\n$c_3 = a_3^T y = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\frac{3}{\\sqrt{2}}$.\n$c_4 = a_4^T y = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}$.\n\nThe first (largest) breakpoint $\\lambda_1$ is the value of $\\lambda$ where one of these inequalities becomes an equality:\n$$ \\lambda_1 = \\max_{i} |c_i| = \\max \\left\\{ 2, 1, \\frac{3}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}} \\right\\} = \\frac{3}{\\sqrt{2}} $$\nFor all $\\lambda \\geq \\lambda_1$, the solution is $x(\\lambda) = 0$. At $\\lambda_1 = 3/\\sqrt{2}$, atom $a_3$ becomes active. The sign of $x_3$ will be $\\text{sign}(c_3) = +1$.\n\n**Segment 2: Path for $\\lambda \\in (\\lambda_2, \\lambda_1)$**\n\nFor $\\lambda  \\lambda_1$, the active set is $\\mathcal{I} = \\{3\\}$ and the sign of $x_3$ is $s_3=1$. The solution vector is of the form $x(\\lambda) = (0, 0, x_3(\\lambda), 0)^T$ with $x_3(\\lambda) > 0$.\nThe active set KKT condition is $a_3^T(y - a_3 x_3) = \\lambda s_3 = \\lambda$.\nSince the atoms are unit-norm ($a_3^T a_3 = 1$), this becomes $a_3^T y - x_3 = \\lambda$, which gives:\n$$ x_3(\\lambda) = a_3^T y - \\lambda = \\frac{3}{\\sqrt{2}} - \\lambda $$\nThis path is valid as long as $x_3(\\lambda)>0$ (i.e., $\\lambda  \\lambda_1$) and the KKT conditions for the inactive atoms $i \\in \\{1, 2, 4\\}$ are satisfied: $|a_i^T(y - a_3 x_3(\\lambda))| \\leq \\lambda$.\nLet $r(\\lambda) = y - a_3 x_3(\\lambda)$. The correlations are $a_i^T r(\\lambda) = a_i^T y - (a_i^T a_3) x_3(\\lambda)$.\nWe need the inner products: $a_1^T a_3 = 1/\\sqrt{2}$, $a_2^T a_3 = 1/\\sqrt{2}$, $a_4^T a_3 = 0$.\n\nFor $i=1$: $a_1^T r(\\lambda) = c_1 - (a_1^T a_3)x_3(\\lambda) = 2 - \\frac{1}{\\sqrt{2}}(\\frac{3}{\\sqrt{2}} - \\lambda) = 2 - \\frac{3}{2} + \\frac{\\lambda}{\\sqrt{2}} = \\frac{1}{2} + \\frac{\\lambda}{\\sqrt{2}}$.\nThe condition $|a_1^T r(\\lambda)| \\leq \\lambda$ becomes $\\frac{1}{2} + \\frac{\\lambda}{\\sqrt{2}} \\leq \\lambda$ (since the term is positive for $\\lambda>0$).\nThis yields $\\frac{1}{2} \\leq \\lambda(1 - \\frac{1}{\\sqrt{2}})$, so $\\lambda \\geq \\frac{1/2}{1-1/\\sqrt{2}} = \\frac{\\sqrt{2}}{2(\\sqrt{2}-1)} = \\frac{\\sqrt{2}(\\sqrt{2}+1)}{2} = \\frac{2+\\sqrt{2}}{2}$.\n\nFor $i=2$: $a_2^T r(\\lambda) = c_2 - (a_2^T a_3)x_3(\\lambda) = 1 - \\frac{1}{\\sqrt{2}}(\\frac{3}{\\sqrt{2}} - \\lambda) = 1 - \\frac{3}{2} + \\frac{\\lambda}{\\sqrt{2}} = -\\frac{1}{2} + \\frac{\\lambda}{\\sqrt{2}}$.\nThe condition is $|-\\frac{1}{2} + \\frac{\\lambda}{\\sqrt{2}}| \\leq \\lambda$. This gives two inequalities:\n(a) $-\\frac{1}{2} + \\frac{\\lambda}{\\sqrt{2}} \\leq \\lambda \\implies -\\frac{1}{2} \\leq \\lambda(1-\\frac{1}{\\sqrt{2}})$, which is true for $\\lambda>0$.\n(b) $-\\frac{1}{2} + \\frac{\\lambda}{\\sqrt{2}} \\geq -\\lambda \\implies \\frac{1}{2} \\leq \\lambda(1+\\frac{1}{\\sqrt{2}})$, so $\\lambda \\geq \\frac{1/2}{1+1/\\sqrt{2}} = \\frac{\\sqrt{2}}{2(\\sqrt{2}+1)} = \\frac{2-\\sqrt{2}}{2}$.\n\nFor $i=4$: $a_4^T r(\\lambda) = c_4 - (a_4^T a_3)x_3(\\lambda) = \\frac{1}{\\sqrt{2}} - 0 = \\frac{1}{\\sqrt{2}}$.\nThe condition is $|\\frac{1}{\\sqrt{2}}| \\leq \\lambda$, so $\\lambda \\geq \\frac{1}{\\sqrt{2}}$.\n\nThe next breakpoint $\\lambda_2$ is the point where one of these inequalities becomes an equality. This happens for the largest $\\lambda  \\lambda_1$ among the boundary values:\n$$ \\lambda_2 = \\max\\left\\{ \\frac{2+\\sqrt{2}}{2}, \\frac{2-\\sqrt{2}}{2}, \\frac{1}{\\sqrt{2}} \\right\\} $$\nNumerical values are approx. $1.707$, $0.293$, $0.707$. The maximum is $\\frac{2+\\sqrt{2}}{2}$.\nThis value is less than $\\lambda_1 = 3/\\sqrt{2} \\approx 2.121$.\nSo, the second breakpoint is $\\lambda_2 = \\frac{2+\\sqrt{2}}{2} = 1 + \\frac{\\sqrt{2}}{2}$.\nAt $\\lambda_2$, we have $a_1^T r(\\lambda_2) = \\lambda_2$ and atom $a_1$ enters the active set. Its sign is $\\text{sign}(a_1^T r(\\lambda_2)) = +1$.\n\n**Segment 3: Path for $\\lambda \\in (0, \\lambda_2)$**\n\nFor $\\lambda  \\lambda_2$, the active set is $\\mathcal{I}=\\{1, 3\\}$ with signs $s_1=1, s_3=1$. The solution has the form $x(\\lambda)=(x_1(\\lambda), 0, x_3(\\lambda), 0)^T$.\nThe active set KKT condition is $A_{\\mathcal{I}}^T(y - A_{\\mathcal{I}}x_{\\mathcal{I}}) = \\lambda s_{\\mathcal{I}}$, where $A_{\\mathcal{I}}=[a_1, a_3]$ and $x_{\\mathcal{I}} = [x_1, x_3]^T$.\nThis gives $A_{\\mathcal{I}}^T A_{\\mathcal{I}} x_{\\mathcal{I}} = A_{\\mathcal{I}}^T y - \\lambda s_{\\mathcal{I}}$.\n$A_{\\mathcal{I}}^T A_{\\mathcal{I}} = \\begin{pmatrix} a_1^T a_1  a_1^T a_3 \\\\ a_3^T a_1  a_3^T a_3 \\end{pmatrix} = \\begin{pmatrix} 1  1/\\sqrt{2} \\\\ 1/\\sqrt{2}  1 \\end{pmatrix}$.\nThe inverse is $(A_{\\mathcal{I}}^T A_{\\mathcal{I}})^{-1} = \\frac{1}{1-1/2}\\begin{pmatrix} 1  -1/\\sqrt{2} \\\\ -1/\\sqrt{2}  1 \\end{pmatrix} = \\begin{pmatrix} 2  -\\sqrt{2} \\\\ -\\sqrt{2}  2 \\end{pmatrix}$.\n$A_{\\mathcal{I}}^T y = \\begin{pmatrix} c_1 \\\\ c_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3/\\sqrt{2} \\end{pmatrix}$ and $s_{\\mathcal{I}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nSo, $x_{\\mathcal{I}}(\\lambda) = (A_{\\mathcal{I}}^T A_{\\mathcal{I}})^{-1} (A_{\\mathcal{I}}^T y - \\lambda s_{\\mathcal{I}}) = \\begin{pmatrix} 2  -\\sqrt{2} \\\\ -\\sqrt{2}  2 \\end{pmatrix} \\begin{pmatrix} 2-\\lambda \\\\ 3/\\sqrt{2}-\\lambda \\end{pmatrix}$.\n$x_1(\\lambda) = 2(2-\\lambda) - \\sqrt{2}(3/\\sqrt{2}-\\lambda) = 4 - 2\\lambda - 3 + \\sqrt{2}\\lambda = 1 + (\\sqrt{2}-2)\\lambda$.\n$x_3(\\lambda) = -\\sqrt{2}(2-\\lambda) + 2(3/\\sqrt{2}-\\lambda) = -2\\sqrt{2} + \\sqrt{2}\\lambda + 3\\sqrt{2} - 2\\lambda = \\sqrt{2} + (\\sqrt{2}-2)\\lambda$.\n\nWe check for the next breakpoint. This occurs if an active coefficient goes to zero, or an inactive atom's correlation reaches $\\pm \\lambda$.\n- `Active coefficients`:\n  $x_1(\\lambda) = 1+(\\sqrt{2}-2)\\lambda > 0 \\implies 1 > (2-\\sqrt{2})\\lambda \\implies \\lambda  \\frac{1}{2-\\sqrt{2}} = \\frac{2+\\sqrt{2}}{2} = \\lambda_2$. So for $\\lambda  \\lambda_2$, $x_1(\\lambda)>0$.\n  $x_3(\\lambda) = \\sqrt{2}+(\\sqrt{2}-2)\\lambda > 0 \\implies \\sqrt{2} > (2-\\sqrt{2})\\lambda \\implies \\lambda  \\frac{\\sqrt{2}}{2-\\sqrt{2}} = \\frac{\\sqrt{2}(2+\\sqrt{2})}{2} = 1+\\sqrt{2}$. Since $1+\\sqrt{2} > \\lambda_2$, $x_3(\\lambda)$ remains positive for $\\lambda \\in (0, \\lambda_2)$.\n  The signs remain consistent and no active coefficient becomes zero for $\\lambda \\in (0, \\lambda_2)$.\n\n- `Inactive coefficients` $i \\in \\{2, 4\\}$:\n  The residual is $r(\\lambda) = y - a_1x_1(\\lambda) - a_3x_3(\\lambda)$.\n  For $i=2$: $a_2^T r(\\lambda) = c_2 - (a_2^T a_1)x_1(\\lambda) - (a_2^T a_3)x_3(\\lambda) = 1 - 0 - \\frac{1}{\\sqrt{2}}x_3(\\lambda)$.\n  $a_2^T r(\\lambda) = 1 - \\frac{1}{\\sqrt{2}}(\\sqrt{2}+(\\sqrt{2}-2)\\lambda) = 1 - (1+\\frac{\\sqrt{2}-2}{\\sqrt{2}}\\lambda) = - (1-\\sqrt{2})\\lambda = (\\sqrt{2}-1)\\lambda$.\n  The condition is $|a_2^T r(\\lambda)| \\leq \\lambda \\implies |(\\sqrt{2}-1)\\lambda| \\leq \\lambda \\implies (\\sqrt{2}-1)\\lambda \\leq \\lambda$, which simplifies to $\\sqrt{2}-1 \\leq 1$ or $\\sqrt{2} \\leq 2$. This inequality is strict for $\\lambda>0$, so atom 2 never becomes active.\n  For $i=4$: $a_4^T r(\\lambda) = c_4 - (a_4^T a_1)x_1(\\lambda) - (a_4^T a_3)x_3(\\lambda) = \\frac{1}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}}x_1(\\lambda) - 0$.\n  $a_4^T r(\\lambda) = \\frac{1}{\\sqrt{2}}(1-x_1(\\lambda)) = \\frac{1}{\\sqrt{2}}(1 - (1+(\\sqrt{2}-2)\\lambda)) = \\frac{1}{\\sqrt{2}}(-(\\sqrt{2}-2)\\lambda) = \\frac{2-\\sqrt{2}}{\\sqrt{2}}\\lambda = (\\sqrt{2}-1)\\lambda$.\n  This is the same correlation as for atom 2, which is also strictly less than $\\lambda$ for $\\lambda > 0$.\n\nNo new event occurs for $\\lambda \\in (0, \\lambda_2)$. The active set and sign pattern remain fixed down to $\\lambda=0$.\nThus, there are only two breakpoints for $\\lambda>0$.\n\nThe breakpoints in descending order are:\n$\\lambda_1 = \\frac{3}{\\sqrt{2}} = \\frac{3\\sqrt{2}}{2}$\n$\\lambda_2 = 1 + \\frac{\\sqrt{2}}{2} = \\frac{2+\\sqrt{2}}{2}$\n\n**Summary of the solution path $x(\\lambda)$:**\n1.  For $\\lambda \\geq \\frac{3\\sqrt{2}}{2}$: $x(\\lambda) = (0, 0, 0, 0)^T$.\n2.  For $\\lambda \\in [\\frac{2+\\sqrt{2}}{2}, \\frac{3\\sqrt{2}}{2})$: $x(\\lambda) = (0, 0, \\frac{3}{\\sqrt{2}}-\\lambda, 0)^T$.\n3.  For $\\lambda \\in [0, \\frac{2+\\sqrt{2}}{2})$: $x(\\lambda) = (1+(\\sqrt{2}-2)\\lambda, 0, \\sqrt{2}+(\\sqrt{2}-2)\\lambda, 0)^T$.\n\nThe question asks for the vector of breakpoints.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{3\\sqrt{2}}{2}  \\frac{2+\\sqrt{2}}{2} \\end{pmatrix} } $$"
        },
        {
            "introduction": "Moving beyond simple sparsity, many applications require selecting variables in predefined groups, a concept known as structured sparsity. The Group LASSO is the canonical method for this task, encouraging entire blocks of coefficients to be either zero or non-zero together. This problem  challenges you to derive the fundamental machinery of the Group LASSO, from its first-order optimality conditions to its core algorithmic component, the block soft-thresholding operator, providing insight into how sparsity-inducing regularizers can be adapted to more complex structural priors.",
            "id": "3457329",
            "problem": "Consider an underdetermined linear inverse problem with data matrix $A \\in \\mathbb{R}^{m \\times n}$ and observation vector $b \\in \\mathbb{R}^{m}$, where $m  n$. Let $\\mathcal{G}$ be a partition of $\\{1,2,\\ldots,n\\}$ into disjoint groups, and for any $x \\in \\mathbb{R}^{n}$ write $x = [x_{g}]_{g \\in \\mathcal{G}}$ according to this partition. Define the group lasso norm by $\\|x\\|_{2,1} = \\sum_{g \\in \\mathcal{G}} \\|x_{g}\\|_{2}$. Consider the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{2,1},\n$$\nwith regularization parameter $\\lambda  0$.\n\nTasks:\n1) Using only the first-order optimality condition for convex functions and the definition of the subdifferential of a norm, derive the Karush-Kuhn-Tucker (KKT) conditions for a minimizer $x^{\\star}$ of the above problem. Express the conditions groupwise for each $g \\in \\mathcal{G}$.\n\n2) From the subdifferential characterization, derive the block soft-thresholding rule that characterizes the proximal operator of the function $x \\mapsto \\lambda \\|x\\|_{2,1}$, and then give the proximal-gradient iteration for minimizing the objective. Your derivation must start from the first-order necessary and sufficient optimality condition for a proximal subproblem and proceed by minimizing over each block.\n\n3) Now specialize to the following concrete instance with $m = 3$, $n = 4$, and two groups $\\mathcal{G} = \\{g_{1}, g_{2}\\}$ where $g_{1} = \\{1,4\\}$ and $g_{2} = \\{2,3\\}$. Let the columns of $A$ be\n$$\na_{1} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}, \\quad\na_{2} = \\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix}, \\quad\na_{3} = \\begin{pmatrix}0 \\\\ 0 \\\\ 1\\end{pmatrix}, \\quad\na_{4} = \\begin{pmatrix}1 \\\\ 1 \\\\ 0\\end{pmatrix},\n$$\nso that $A = [a_{1} \\; a_{2} \\; a_{3} \\; a_{4}] \\in \\mathbb{R}^{3 \\times 4}$, and let $b = \\begin{pmatrix}1 \\\\ 2 \\\\ 3\\end{pmatrix} \\in \\mathbb{R}^{3}$. Using your KKT conditions, determine the smallest value $\\lambda_{0}$ such that $x^{\\star} = 0$ satisfies the optimality conditions for the problem. Express your final answer as a single exact analytic expression. Do not round.",
            "solution": "The problem asks for the derivation of the Karush-Kuhn-Tucker (KKT) conditions for the group lasso optimization problem, the derivation of its associated proximal operator, and the solution for a specific instance. The objective function is $F(x) = f(x) + h(x)$, where $f(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2}$ is a differentiable, convex function and $h(x) = \\lambda \\|x\\|_{2,1}$ is a non-differentiable, convex function. Here $\\|x\\|_{2,1} = \\sum_{g \\in \\mathcal{G}} \\|x_{g}\\|_{2}$.\n\n**1) Derivation of the KKT Conditions**\n\nA vector $x^{\\star} \\in \\mathbb{R}^{n}$ is a minimizer of the convex function $F(x)$ if and only if it satisfies the first-order optimality condition:\n$$\n0 \\in \\partial F(x^{\\star})\n$$\nSince $f(x)$ is differentiable, the subdifferential of the sum $F(x) = f(x) + h(x)$ is given by the sum of the gradient of $f(x)$ and the subdifferential of $h(x)$:\n$$\n\\partial F(x^{\\star}) = \\nabla f(x^{\\star}) + \\partial h(x^{\\star})\n$$\nThe gradient of $f(x)$ is $\\nabla f(x) = A^T(Ax - b)$. The optimality condition becomes:\n$$\n0 \\in A^T(Ax^{\\star} - b) + \\partial h(x^{\\star})\n$$\nThis is equivalent to:\n$$\n-A^T(Ax^{\\star} - b) \\in \\partial h(x^{\\star})\n$$\nThe function $h(x) = \\lambda \\|x\\|_{2,1} = \\lambda \\sum_{g \\in \\mathcal{G}} \\|x_g\\|_2$ is separable with respect to the groups $\\mathcal{G}$. Its subdifferential is the Cartesian product of the subdifferentials of each term:\n$$\n\\partial h(x) = \\partial \\left(\\lambda \\sum_{g \\in \\mathcal{G}} \\|x_g\\|_2\\right) = \\lambda \\prod_{g \\in \\mathcal{G}} \\partial \\|x_g\\|_2\n$$\nThe subdifferential of the Euclidean norm $\\|z\\|_2$ for a vector $z$ is:\n$$\n\\partial \\|z\\|_2 = \\begin{cases} \\{z / \\|z\\|_2\\}  \\text{if } z \\neq 0 \\\\ \\{u \\mid \\|u\\|_2 \\le 1\\}  \\text{if } z = 0 \\end{cases}\n$$\nwhere $\\{u \\mid \\|u\\|_2 \\le 1\\}$ is the closed unit ball in the appropriate dimension.\n\nLet $r^{\\star} = A^T(b - Ax^{\\star})$ be the residual-related vector. The optimality condition $-A^T(Ax^{\\star} - b) \\in \\partial h(x^{\\star})$ can be written as $r^{\\star} \\in \\lambda \\partial \\|x^{\\star}\\|_{2,1}$.\nLet's partition $r^{\\star}$ according to the groups $\\mathcal{G}$, $r^{\\star} = [r^{\\star}_g]_{g \\in \\mathcal{G}}$. The submatrix of $A$ corresponding to the indices in group $g$ is denoted by $A_g$. Then $r^{\\star}_g = A_g^T (b - Ax^{\\star})$. The optimality condition can be expressed for each group $g \\in \\mathcal{G}$ as:\n$$\nA_g^T (b - Ax^{\\star}) \\in \\lambda \\partial \\|x_g^{\\star}\\|_2\n$$\nWe analyze this condition for two cases for each group $g$:\n\nCase 1: $x_g^{\\star} \\ne 0$.\nIn this case, $\\partial \\|x_g^{\\star}\\|_2 = \\{x_g^{\\star} / \\|x_g^{\\star}\\|_2\\}$. The condition becomes:\n$$\nA_g^T (b - Ax^{\\star}) = \\lambda \\frac{x_g^{\\star}}{\\|x_g^{\\star}\\|_2}\n$$\nTaking the Euclidean norm of both sides gives $\\|A_g^T (b - Ax^{\\star})\\|_2 = \\lambda$.\n\nCase 2: $x_g^{\\star} = 0$.\nIn this case, $\\partial \\|x_g^{\\star}\\|_2 = \\{u_g \\mid \\|u_g\\|_2 \\le 1\\}$. The condition becomes:\n$$\nA_g^T (b - Ax^{\\star}) \\in \\lambda \\{u_g \\mid \\|u_g\\|_2 \\le 1\\}\n$$\nThis is equivalent to stating that the norm of the vector on the left must be less than or equal to $\\lambda$:\n$$\n\\|A_g^T (b - Ax^{\\star})\\|_2 \\le \\lambda\n$$\nThese are the group-wise KKT conditions for the minimizer $x^{\\star}$.\n\n**2) Proximal Operator and Proximal-Gradient Iteration**\n\nThe proximal operator of a function $\\psi(x)$ is defined as $\\text{prox}_{\\psi}(v) = \\arg\\min_{x} \\left(\\psi(x) + \\frac{1}{2}\\|x - v\\|_2^2\\right)$.\nWe are interested in the proximal operator of $h(x) = \\lambda \\|x\\|_{2,1}$. Let $\\alpha > 0$ be a step-size. We need to compute $\\text{prox}_{\\alpha h}(v)$.\n$$\n\\text{prox}_{\\alpha h}(v) = \\arg\\min_{x} \\left(\\alpha\\lambda \\|x\\|_{2,1} + \\frac{1}{2}\\|x - v\\|_2^2\\right)\n$$\nThe objective function is separable across the groups $g \\in \\mathcal{G}$:\n$$\n\\alpha\\lambda \\sum_{g \\in \\mathcal{G}} \\|x_g\\|_2 + \\frac{1}{2} \\sum_{g \\in \\mathcal{G}} \\|x_g - v_g\\|_2^2\n$$\nWe can minimize for each block $x_g$ independently:\n$$\n\\hat{x}_g = \\arg\\min_{x_g} \\left(\\alpha\\lambda \\|x_g\\|_2 + \\frac{1}{2}\\|x_g - v_g\\|_2^2\\right)\n$$\nLet $\\mu = \\alpha\\lambda$. The first-order optimality condition for this subproblem is:\n$$\n0 \\in \\mu \\partial \\|\\hat{x}_g\\|_2 + (\\hat{x}_g - v_g) \\quad \\iff \\quad v_g - \\hat{x}_g \\in \\mu \\partial \\|\\hat{x}_g\\|_2\n$$\nCase A: $\\hat{x}_g \\ne 0$. The condition is $v_g - \\hat{x}_g = \\mu \\frac{\\hat{x}_g}{\\|\\hat{x}_g\\|_2}$.\nThis implies $v_g = \\hat{x}_g \\left(1 + \\frac{\\mu}{\\|\\hat{x}_g\\|_2}\\right)$. From this, we see that $\\hat{x}_g$ must be a positive scalar multiple of $v_g$. Taking the norm on both sides:\n$\\|v_g\\|_2 = \\|\\hat{x}_g\\|_2 \\left(1 + \\frac{\\mu}{\\|\\hat{x}_g\\|_2}\\right) = \\|\\hat{x}_g\\|_2 + \\mu$.\nThus, $\\|\\hat{x}_g\\|_2 = \\|v_g\\|_2 - \\mu$. Since $\\|\\hat{x}_g\\|_2 > 0$, this case is only possible if $\\|v_g\\|_2 > \\mu$.\nSubstituting this back, we get $v_g = \\hat{x}_g \\frac{\\|v_g\\|_2}{\\|v_g\\|_2 - \\mu}$, which gives $\\hat{x}_g = \\frac{\\|v_g\\|_2 - \\mu}{\\|v_g\\|_2} v_g = \\left(1 - \\frac{\\mu}{\\|v_g\\|_2}\\right) v_g$.\n\nCase B: $\\hat{x}_g = 0$. The condition is $v_g \\in \\mu \\partial \\|0\\|_2$, which means $\\|v_g\\|_2 \\le \\mu$.\n\nCombining these two cases, the solution for each block is:\n$$\n\\hat{x}_g = \\begin{cases} \\left(1 - \\frac{\\alpha\\lambda}{\\|v_g\\|_2}\\right) v_g  \\text{if } \\|v_g\\|_2  \\alpha\\lambda \\\\ 0  \\text{if } \\|v_g\\|_2 \\le \\alpha\\lambda \\end{cases}\n$$\nThis can be written compactly using the positive part function $(\\cdot)_+ = \\max(0, \\cdot)$ as:\n$$\n\\hat{x}_g = \\left(1 - \\frac{\\alpha\\lambda}{\\|v_g\\|_2}\\right)_+ v_g\n$$\nThis is the block soft-thresholding rule.\n\nThe proximal-gradient method for minimizing $f(x) + h(x)$ is an iterative algorithm given by:\n$$\nx^{(k+1)} = \\text{prox}_{\\alpha h}\\left(x^{(k)} - \\alpha \\nabla f(x^{(k)})\\right)\n$$\nwhere $\\alpha > 0$ is the step size. For our problem, this becomes:\n$$\nx^{(k+1)} = \\text{prox}_{\\alpha\\lambda \\|\\cdot\\|_{2,1}}\\left(x^{(k)} - \\alpha A^T(Ax^{(k)} - b)\\right)\n$$\nApplying the block soft-thresholding operator we derived, let $v^{(k)} = x^{(k)} - \\alpha A^T(Ax^{(k)} - b)$. The update for each group $g \\in \\mathcal{G}$ is:\n$$\nx_g^{(k+1)} = \\left(1 - \\frac{\\alpha\\lambda}{\\|v_g^{(k)}\\|_2}\\right)_+ v_g^{(k)}\n$$\n\n**3) Smallest $\\lambda$ for Zero Solution**\n\nWe seek the smallest $\\lambda_0 > 0$ such that $x^{\\star} = 0$ is a minimizer. Using the KKT conditions from Part 1, we must satisfy the condition for inactive groups (i.e., $x_g^{\\star}=0$). Since all groups are inactive for $x^{\\star}=0$, we must have for all $g \\in \\mathcal{G}$:\n$$\n\\|A_g^T (b - A \\cdot 0)\\|_2 \\le \\lambda\n$$\nThis simplifies to $\\|A_g^T b\\|_2 \\le \\lambda$ for all $g \\in \\mathcal{G}$.\nTo satisfy this for all groups, $\\lambda$ must be greater than or equal to the maximum of these norms. The smallest such $\\lambda$ is therefore:\n$$\n\\lambda_0 = \\max_{g \\in \\mathcal{G}} \\|A_g^T b\\|_2\n$$\nNow, we use the specific instance data: $m=3$, $n=4$.\n$A = \\begin{pmatrix} 1  0  0  1 \\\\ 0  1  0  1 \\\\ 0  0  1  0 \\end{pmatrix}$, $b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$.\nThe groups are $g_1 = \\{1,4\\}$ and $g_2 = \\{2,3\\}$.\n\nThe column submatrices are:\n$A_{g_1}$ (columns $1$ and $4$ of $A$): $A_{g_1} = \\begin{pmatrix} 1  1 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix}$.\n$A_{g_2}$ (columns $2$ and $3$ of $A$): $A_{g_2} = \\begin{pmatrix} 0  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix}$.\n\nNext, we compute $A_g^T b$ for each group.\nFor group $g_1$:\n$$\nA_{g_1}^T b = \\begin{pmatrix} 1  0  0 \\\\ 1  1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} (1)(1)+(0)(2)+(0)(3) \\\\ (1)(1)+(1)(2)+(0)(3) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}\n$$\nThe Euclidean norm is $\\|A_{g_1}^T b\\|_2 = \\sqrt{1^2 + 3^2} = \\sqrt{1+9} = \\sqrt{10}$.\n\nFor group $g_2$:\n$$\nA_{g_2}^T b = \\begin{pmatrix} 0  1  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} (0)(1)+(1)(2)+(0)(3) \\\\ (0)(1)+(0)(2)+(1)(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\nThe Euclidean norm is $\\|A_{g_2}^T b\\|_2 = \\sqrt{2^2 + 3^2} = \\sqrt{4+9} = \\sqrt{13}$.\n\nThe smallest $\\lambda_0$ is the maximum of these values:\n$$\n\\lambda_0 = \\max \\left( \\|A_{g_1}^T b\\|_2, \\|A_{g_2}^T b\\|_2 \\right) = \\max(\\sqrt{10}, \\sqrt{13}) = \\sqrt{13}\n$$",
            "answer": "$$\n\\boxed{\\sqrt{13}}\n$$"
        },
        {
            "introduction": "While solving the LASSO problem for a fixed regularization parameter $\\lambda$ is a question of optimization, choosing the optimal $\\lambda$ itself is a question of statistical model selection. This practice problem  bridges this gap by introducing Stein’s Unbiased Risk Estimate (SURE), a powerful statistical tool for estimating a model's true prediction error. By deriving the SURE formula for LASSO in a simple setting and using it to select an optimal $\\lambda$ for a given dataset, you will learn a principled, data-driven approach to tuning one of the most critical hyperparameters in sparse modeling.",
            "id": "3457312",
            "problem": "Consider the linear inverse problem with identity sensing matrix, where observations are modeled as $y = x_{0} + w \\in \\mathbb{R}^{n}$ with $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Let the estimator $\\widehat{x}_{\\lambda}(y)$ be the solution of the Least Absolute Shrinkage and Selection Operator (LASSO), namely the minimizer of $\\frac{1}{2} \\|y - x\\|_{2}^{2} + \\lambda \\|x\\|_{1}$, which coincides componentwise with the soft-thresholding operator at level $\\lambda$, denoted $S_{\\lambda}(y)$, so that $\\widehat{x}_{\\lambda}(y) = S_{\\lambda}(y)$ with $(S_{\\lambda}(y))_{i} = \\operatorname{sign}(y_{i}) \\max\\{|y_{i}| - \\lambda, 0\\}$. Using Stein’s Unbiased Risk Estimate (SURE), derive from first principles an unbiased estimator of the mean-squared prediction error with respect to the noiseless response, that is, an unbiased estimator of $\\mathbb{E}\\big[\\|S_{\\lambda}(y) - x_{0}\\|_{2}^{2}\\big]$ expressed purely in terms of $y$, $\\sigma^{2}$, and the divergence of $S_{\\lambda}(\\cdot)$. Show that this SURE can be written explicitly using the divergence of the soft-thresholding operator.\n\nThen, using your derived expression, determine the value of $\\lambda$ that minimizes the SURE for the following data:\n- Dimension $n = 6$,\n- Noise variance $\\sigma^{2} = 0.25$,\n- Observation vector $y \\in \\mathbb{R}^{6}$ given by $y = (3.10,\\,-2.40,\\,0.95,\\,-1.80,\\,0.12,\\,-0.42)^{\\top}$.\n\nReport only the numerical value of the minimizer $\\lambda^{\\star}$. Do not round the final answer.",
            "solution": "We begin with the Gaussian sequence model $y = x_{0} + w \\in \\mathbb{R}^{n}$ where $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. The LASSO estimator under the identity design is the soft-thresholding operator $S_{\\lambda}(y)$ defined componentwise by\n$$\n(S_{\\lambda}(y))_{i} = \\operatorname{sign}(y_{i}) \\max\\{|y_{i}| - \\lambda, 0\\}.\n$$\nThe quantity of interest is the mean-squared prediction error with respect to the noiseless response $x_{0}$,\n$$\nR(\\lambda) \\equiv \\mathbb{E}\\big[\\|S_{\\lambda}(y) - x_{0}\\|_{2}^{2}\\big].\n$$\nStein’s Unbiased Risk Estimate (SURE) provides, under mild regularity conditions (weak differentiability and integrability) satisfied by $S_{\\lambda}$ almost everywhere, an unbiased estimator of $R(\\lambda)$:\n$$\n\\operatorname{SURE}(\\lambda; y) \\equiv \\|S_{\\lambda}(y) - y\\|_{2}^{2} + 2 \\sigma^{2} \\,\\operatorname{div}_{y} S_{\\lambda}(y) - n \\sigma^{2}.\n$$\nwhich satisfies $\\mathbb{E}[\\operatorname{SURE}(\\lambda; y)] = R(\\lambda)$.\n\nWe now compute the divergence of the soft-thresholding operator. For each component $i \\in \\{1,\\dots,n\\}$, the mapping $y_{i} \\mapsto (S_{\\lambda}(y))_{i}$ is differentiable for $|y_{i}| \\neq \\lambda$, with derivative\n$$\n\\frac{\\partial (S_{\\lambda}(y))_{i}}{\\partial y_{i}} =\n\\begin{cases}\n1,  \\text{if } |y_{i}|  \\lambda,\\\\\n0,  \\text{if } |y_{i}|  \\lambda.\n\\end{cases}\n$$\nAt the nondifferentiable kinks $|y_{i}| = \\lambda$, a set of Lebesgue measure zero under the Gaussian law, Stein’s formula remains valid. Therefore, the divergence is\n$$\n\\operatorname{div}_{y} S_{\\lambda}(y) = \\sum_{i=1}^{n} \\frac{\\partial (S_{\\lambda}(y))_{i}}{\\partial y_{i}} = \\#\\{i: |y_{i}|  \\lambda\\} \\equiv k(\\lambda),\n$$\nthe number of active (nonzero) soft-thresholded components.\n\nNext, we simplify the residual term $\\|S_{\\lambda}(y) - y\\|_{2}^{2}$. For any coordinate $i$:\n- If $|y_{i}|  \\lambda$ (active), then $(S_{\\lambda}(y))_{i} = y_{i} - \\operatorname{sign}(y_{i}) \\lambda$, hence $(S_{\\lambda}(y))_{i} - y_{i} = -\\operatorname{sign}(y_{i}) \\lambda$, and $((S_{\\lambda}(y))_{i} - y_{i})^{2} = \\lambda^{2}$.\n- If $|y_{i}| \\leq \\lambda$ (inactive), then $(S_{\\lambda}(y))_{i} = 0$, hence $(S_{\\lambda}(y))_{i} - y_{i} = -y_{i}$, and $((S_{\\lambda}(y))_{i} - y_{i})^{2} = y_{i}^{2}$.\n\nTherefore,\n$$\n\\|S_{\\lambda}(y) - y\\|_{2}^{2} = \\sum_{i: |y_{i}|  \\lambda} \\lambda^{2} + \\sum_{i: |y_{i}| \\leq \\lambda} y_{i}^{2} = \\sum_{i=1}^{n} \\min\\{y_{i}^{2}, \\lambda^{2}\\}.\n$$\nCombining with the divergence, we obtain the explicit SURE:\n$$\n\\operatorname{SURE}(\\lambda; y) = \\sum_{i=1}^{n} \\min\\{y_{i}^{2}, \\lambda^{2}\\} + 2 \\sigma^{2} k(\\lambda) - n \\sigma^{2}.\n$$\n\nWe analyze the structure of $\\operatorname{SURE}(\\lambda; y)$ as a function of $\\lambda$. Let $s_{(1)} \\geq s_{(2)} \\geq \\cdots \\geq s_{(n)}$ be the order statistics of $\\{|y_{i}|\\}_{i=1}^{n}$. For $\\lambda \\in (s_{(t+1)}, s_{(t)}]$ with the convention $s_{(n+1)} \\equiv 0$, the number of active coefficients is $k(\\lambda) = t$, and\n$$\n\\operatorname{SURE}(\\lambda; y) = t \\lambda^{2} + \\sum_{i=t+1}^{n} s_{(i)}^{2} + (2t - n) \\sigma^{2}.\n$$\nWithin each open interval $(s_{(t+1)}, s_{(t)})$, the derivative is $\\frac{d}{d\\lambda} \\operatorname{SURE}(\\lambda; y) = 2 t \\lambda \\geq 0$, so $\\operatorname{SURE}(\\lambda; y)$ is nondecreasing in $\\lambda$ on each such interval. Hence any minimizer must occur at the left endpoint of some interval, that is, at $\\lambda \\in \\{0, s_{(n)}, s_{(n-1)}, \\dots, s_{(1)}\\}$.\n\nWe now apply this to the given data. Here $n = 6$, $\\sigma^{2} = 0.25$, and\n$$\ny = (3.10,\\,-2.40,\\,0.95,\\,-1.80,\\,0.12,\\,-0.42)^{\\top},\n$$\nso the sorted absolute values are\n$$\ns_{(1)} = 3.10,\\quad s_{(2)} = 2.40,\\quad s_{(3)} = 1.80,\\quad s_{(4)} = 0.95,\\quad s_{(5)} = 0.42,\\quad s_{(6)} = 0.12.\n$$\nWe evaluate $\\operatorname{SURE}(\\lambda; y)$ at the candidate points $\\lambda \\in \\{0,\\,0.12,\\,0.42,\\,0.95,\\,1.80,\\,2.40,\\,3.10\\}$.\n\nFirst, at $\\lambda = 0$, all components are active so $k(0) = 6$ and\n$$\n\\operatorname{SURE}(0; y) = \\sum_{i=1}^{n} \\min\\{y_{i}^{2}, 0\\} + (2 \\cdot 6 - 6)\\sigma^{2} = 0 + 6 \\cdot 0.25 = 1.5.\n$$\n\nFor $\\lambda = s_{(t)}$ with $t \\in \\{1,\\dots,6\\}$, right after the threshold the active set has size $t-1$, and we have\n$$\n\\operatorname{SURE}(s_{(t)}; y) = (t-1) s_{(t)}^{2} + \\sum_{i=t}^{n} s_{(i)}^{2} + \\big(2(t-1) - n\\big) \\sigma^{2}.\n$$\nWe precompute the squares and tails:\n$$\ns_{(1)}^{2} = 9.61,\\quad s_{(2)}^{2} = 5.76,\\quad s_{(3)}^{2} = 3.24,\\quad s_{(4)}^{2} = 0.9025,\\quad s_{(5)}^{2} = 0.1764,\\quad s_{(6)}^{2} = 0.0144,\n$$\nand the tail sums\n$$\n\\sum_{i=1}^{6} s_{(i)}^{2} = 19.7033,\\quad \\sum_{i=2}^{6} s_{(i)}^{2} = 10.0933,\\quad \\sum_{i=3}^{6} s_{(i)}^{2} = 4.3333,\\quad \\sum_{i=4}^{6} s_{(i)}^{2} = 1.0933,\\quad \\sum_{i=5}^{6} s_{(i)}^{2} = 0.1908,\\quad \\sum_{i=6}^{6} s_{(i)}^{2} = 0.0144.\n$$\nWe also note $(2(t-1) - n) \\sigma^{2} = (2t - 8) \\cdot 0.25$.\n\nNow compute each candidate:\n- For $t = 6$ (i.e., $\\lambda = 0.12$): $(t-1) s_{(t)}^{2} + \\sum_{i=t}^{n} s_{(i)}^{2} = 5 \\cdot 0.0144 + 0.0144 = 0.0864$, and $(2t - 8) \\cdot 0.25 = (12 - 8) \\cdot 0.25 = 1.0$, so $\\operatorname{SURE}(0.12; y) = 0.0864 + 1.0 = 1.0864$.\n- For $t = 5$ (i.e., $\\lambda = 0.42$): $4 \\cdot 0.1764 + 0.1908 = 0.8964$, and $(10 - 8) \\cdot 0.25 = 0.5$, so $\\operatorname{SURE}(0.42; y) = 0.8964 + 0.5 = 1.3964$.\n- For $t = 4$ (i.e., $\\lambda = 0.95$): $3 \\cdot 0.9025 + 1.0933 = 3.8008$, and $(8 - 8) \\cdot 0.25 = 0$, so $\\operatorname{SURE}(0.95; y) = 3.8008$.\n- For $t = 3$ (i.e., $\\lambda = 1.80$): $2 \\cdot 3.24 + 4.3333 = 10.8133$, and $(6 - 8) \\cdot 0.25 = -0.5$, so $\\operatorname{SURE}(1.80; y) = 10.3133$.\n- For $t = 2$ (i.e., $\\lambda = 2.40$): $1 \\cdot 5.76 + 10.0933 = 15.8533$, and $(4 - 8) \\cdot 0.25 = -1.0$, so $\\operatorname{SURE}(2.40; y) = 14.8533$.\n- For $t = 1$ (i.e., $\\lambda = 3.10$): $0 + 19.7033$, and $(2 - 8) \\cdot 0.25 = -1.5$, so $\\operatorname{SURE}(3.10; y) = 18.2033$.\n\nComparing all candidates, we find\n$$\n\\operatorname{SURE}(0; y) = 1.5,\\quad \\operatorname{SURE}(0.12; y) = 1.0864,\\quad \\operatorname{SURE}(0.42; y) = 1.3964,\n$$\nand the remaining candidates yield larger values. Since $\\operatorname{SURE}(\\lambda; y)$ is nondecreasing between breakpoints, the global minimizer is attained at $\\lambda^{\\star} = 0.12$.\n\nTherefore, the value of $\\lambda$ that minimizes SURE for the given data is $\\lambda^{\\star} = 0.12$.",
            "answer": "$$\\boxed{0.12}$$"
        }
    ]
}