{
    "hands_on_practices": [
        {
            "introduction": "Understanding the numerical stability of a problem often begins with its condition number. This exercise provides foundational practice by calculating the 2-norm condition number of a pseudoinverse, $\\kappa_2(A^\\dagger)$. By working through the singular value decomposition, you will demonstrate the crucial identity $\\kappa_2(A^\\dagger) = \\kappa_2(A)$ and solidify your understanding of how singular values govern a matrix's sensitivity to perturbations .",
            "id": "959945",
            "problem": "Compute the 2-norm condition number $\\kappa_2$ of the pseudoinverse of the matrix  \n$$  \nA = \\begin{bmatrix}  \n2 & 1 \\\\  \n1 & 2 \\\\  \n1 & 1  \n\\end{bmatrix}.  \n$$  \nThe pseudoinverse of a matrix $B$ is denoted $B^\\dagger$. The 2-norm condition number is defined as $\\kappa_2(B) = \\|B\\|_2 \\|B^\\dagger\\|_2$. For the pseudoinverse of $A$, compute $\\kappa_2(A^\\dagger)$.",
            "solution": "To compute the 2-norm condition number $\\kappa_2(A^\\dagger)$ of the pseudoinverse of matrix $A$, we use the following key steps:\n\n\n### Step 1: Relationship between $\\kappa_2(A^\\dagger)$ and $\\kappa_2(A)$  \nThe 2-norm condition number of a matrix $B$ is defined as $\\kappa_2(B) = \\|B\\|_2 \\|B^\\dagger\\|_2$. For $B = A^\\dagger$, this becomes:  \n\n$$\n\\kappa_2(A^\\dagger) = \\|A^\\dagger\\|_2 \\|(A^\\dagger)^\\dagger\\|_2\n$$\n  \nBy the property of the Moore-Penrose pseudoinverse, $(A^\\dagger)^\\dagger = A$. Thus:  \n\n$$\n\\kappa_2(A^\\dagger) = \\|A^\\dagger\\|_2 \\|A\\|_2\n$$\n  \nThis is exactly the 2-norm condition number of $A$, so $\\kappa_2(A^\\dagger) = \\kappa_2(A)$.\n\n\n### Step 2: Singular values of $A$  \nThe 2-norm condition number of $A$ is $\\kappa_2(A) = \\frac{\\sigma_1}{\\sigma_r}$, where $\\sigma_1$ is the largest singular value of $A$ and $\\sigma_r$ is the smallest non-zero singular value. Singular values of $A$ are the square roots of the eigenvalues of $A^\\top A$.\n\nFor $A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{bmatrix}$, compute $A^\\top A$:  \n\n$$\nA^\\top A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 1 & 2 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 6 & 5 \\\\ 5 & 6 \\end{bmatrix}\n$$\n\n\n\n### Step 3: Eigenvalues of $A^\\top A$  \nThe characteristic equation of $A^\\top A$ is $\\det(A^\\top A - \\lambda I) = 0$:  \n\n$$\n\\det\\left(\\begin{bmatrix} 6-\\lambda & 5 \\\\ 5 & 6-\\lambda \\end{bmatrix}\\right) = (6-\\lambda)^2 - 5^2 = (\\lambda - 11)(\\lambda - 1) = 0\n$$\n  \nThus, the eigenvalues are $\\lambda_1 = 11$ and $\\lambda_2 = 1$.\n\n\n### Step 4: Singular values and condition number  \nThe singular values of $A$ are $\\sigma_1 = \\sqrt{\\lambda_1} = \\sqrt{11}$ (largest) and $\\sigma_2 = \\sqrt{\\lambda_2} = 1$ (smallest non-zero). Thus:  \n\n$$\n\\kappa_2(A) = \\frac{\\sigma_1}{\\sigma_2} = \\frac{\\sqrt{11}}{1} = \\sqrt{11}\n$$\n  \n\n\nSince $\\kappa_2(A^\\dagger) = \\kappa_2(A)$, we conclude $\\kappa_2(A^\\dagger) = \\sqrt{11}$.",
            "answer": "$$\\boxed{\\sqrt{11}}$$"
        },
        {
            "introduction": "Beyond single matrices, random matrix theory provides powerful insights into the typical behavior of linear systems. This problem explores the expected noise amplification in a least-squares setting where the sensing matrix $A$ has random Gaussian entries. By calculating the expected squared Frobenius norm of the pseudoinverse, $\\mathbb{E}\\! \\left[\\|A^{\\dagger}\\|_{F}^{2}\\right]$, you will uncover how the problem's dimensions, $m$ and $n$, dictate the average-case stability of the solution and its susceptibility to noise .",
            "id": "3471138",
            "problem": "Consider a linear measurement model in compressed sensing and sparse optimization where a tall sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m > n$ is used to recover an $n$-dimensional signal from $m$ measurements. Assume that $A$ has independent and identically distributed Gaussian entries with distribution $\\mathcal{N}(0,1)$, and that $m > n + 1$ so that the relevant expectations exist. Let $A^{\\dagger}$ denote the Moore–Penrose (MP) pseudoinverse of $A$, and recall that the squared Frobenius norm of a matrix $M$ is $\\|M\\|_{F}^{2} = \\operatorname{trace}(M M^{\\top})$.\n\nUsing only fundamental definitions and well-tested facts about Gaussian matrices and their Gram matrices, compute the exact closed-form expression for the expectation $\\mathbb{E}\\!\\left[\\|A^{\\dagger}\\|_{F}^{2}\\right]$ as a function of $m$ and $n$. Then, interpret how its scaling with $m$ and $n$ relates to the amplification of additive white noise by the least-squares estimator $x \\mapsto A^{\\dagger} y$ when $y = A x + w$ with $w$ being zero-mean white Gaussian noise with covariance $\\sigma^{2} I_{m}$. Express your final answer as a closed-form expression in $m$ and $n$. No rounding is required.",
            "solution": "The problem asks for the expected squared Frobenius norm of the Moore-Penrose pseudoinverse, $\\mathbb{E}\\!\\left[\\|A^{\\dagger}\\|_{F}^{2}\\right]$, of a tall Gaussian matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m > n$, and for an interpretation of this quantity in the context of noise amplification.\n\nFirst, we establish the expression for $\\|A^{\\dagger}\\|_{F}^{2}$. The matrix $A$ has dimensions $m \\times n$ where $m > n$. Its entries are drawn from a continuous distribution, so it is of full column rank $n$ with probability $1$. For a matrix of full column rank, the Moore-Penrose pseudoinverse $A^{\\dagger}$ is given by the formula for the left inverse:\n$$\nA^{\\dagger} = (A^{\\top} A)^{-1} A^{\\top}\n$$\nThe squared Frobenius norm of any matrix $M$ is defined as $\\|M\\|_{F}^{2} = \\operatorname{trace}(M M^{\\top})$. Applying this to $A^{\\dagger}$:\n$$\n\\|A^{\\dagger}\\|_{F}^{2} = \\operatorname{trace}(A^{\\dagger} (A^{\\dagger})^{\\top})\n$$\nWe find the transpose of $A^{\\dagger}$:\n$$\n(A^{\\dagger})^{\\top} = \\left((A^{\\top} A)^{-1} A^{\\top}\\right)^{\\top} = (A^{\\top})^{\\top} \\left((A^{\\top} A)^{-1}\\right)^{\\top} = A (A^{\\top} A)^{-1}\n$$\nThe last equality holds because the Gram matrix $A^{\\top} A$ is symmetric, and the inverse of a symmetric matrix is also symmetric.\n\nNow, we compute the product $A^{\\dagger} (A^{\\dagger})^{\\top}$:\n$$\nA^{\\dagger} (A^{\\dagger})^{\\top} = \\left((A^{\\top} A)^{-1} A^{\\top}\\right) \\left(A (A^{\\top} A)^{-1}\\right) = (A^{\\top} A)^{-1} (A^{\\top} A) (A^{\\top} A)^{-1}\n$$\nSince $A^{\\top} A$ is an invertible $n \\times n$ matrix, $(A^{\\top} A)^{-1} (A^{\\top} A) = I_{n}$, where $I_{n}$ is the $n \\times n$ identity matrix. The expression simplifies to:\n$$\nA^{\\dagger} (A^{\\dagger})^{\\top} = I_{n} (A^{\\top} A)^{-1} = (A^{\\top} A)^{-1}\n$$\nSubstituting this back into the formula for the squared Frobenius norm, we get:\n$$\n\\|A^{\\dagger}\\|_{F}^{2} = \\operatorname{trace}\\left((A^{\\top} A)^{-1}\\right)\n$$\nOur task is now to compute the expectation of this quantity: $\\mathbb{E}\\!\\left[\\operatorname{trace}\\left((A^{\\top} A)^{-1}\\right)\\right]$.\n\nThe matrix $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed (i.i.d.) entries with distribution $\\mathcal{N}(0, 1)$. Let us define the matrix $W = A^{\\top} A$. The matrix $W$ is an $n \\times n$ matrix. Let the rows of $A$ be denoted by $a_1^{\\top}, \\dots, a_m^{\\top}$, where each $a_i \\in \\mathbb{R}^n$ is a random vector whose components are i.i.d. $\\mathcal{N}(0, 1)$. Thus, each $a_i$ follows a multivariate normal distribution $a_i \\sim \\mathcal{N}_n(0, I_n)$. The Gram matrix $W$ can be written as the sum:\n$$\nW = A^{\\top} A = \\sum_{i=1}^{m} a_i a_i^{\\top}\n$$\nBy definition, this sum follows a standard Wishart distribution with $m$ degrees of freedom and dimension $n$. We denote this as $W \\sim \\mathcal{W}_n(m, I_n)$.\n\nThe problem reduces to computing $\\mathbb{E}\\!\\left[\\operatorname{trace}(W^{-1})\\right]$ for $W \\sim \\mathcal{W}_n(m, I_n)$. Due to the linearity of both the trace and expectation operators, we can commute them:\n$$\n\\mathbb{E}\\!\\left[\\operatorname{trace}(W^{-1})\\right] = \\operatorname{trace}\\left(\\mathbb{E}[W^{-1}]\\right)\n$$\nThe matrix $W^{-1}$ follows an inverse-Wishart distribution. A standard result from multivariate statistics states that for a Wishart matrix $W \\sim \\mathcal{W}_p(k, \\Sigma)$, its inverse has the expectation:\n$$\n\\mathbb{E}[W^{-1}] = \\frac{\\Sigma^{-1}}{k - p - 1}\n$$\nThis expectation is defined if and only if $k - p - 1 > 0$. In our case, the dimension is $p=n$, the degrees of freedom are $k=m$, and the covariance matrix is $\\Sigma = I_n$. The condition for the existence of the expectation is $m - n - 1 > 0$, or $m > n + 1$, which is explicitly given in the problem statement.\n\nApplying this result to our matrix $W = A^{\\top} A \\sim \\mathcal{W}_n(m, I_n)$, we get:\n$$\n\\mathbb{E}[W^{-1}] = \\mathbb{E}[(A^{\\top}A)^{-1}] = \\frac{I_n^{-1}}{m - n - 1} = \\frac{I_n}{m - n - 1}\n$$\nWe now take the trace of this expected matrix:\n$$\n\\operatorname{trace}\\left(\\frac{I_n}{m - n - 1}\\right) = \\frac{1}{m - n - 1} \\operatorname{trace}(I_n)\n$$\nThe trace of the $n \\times n$ identity matrix $I_n$ is simply $n$. Therefore, we arrive at the final expression for the expectation:\n$$\n\\mathbb{E}\\!\\left[\\|A^{\\dagger}\\|_{F}^{2}\\right] = \\frac{n}{m - n - 1}\n$$\nFor the interpretation, consider the linear model $y = A x + w$, where $w \\in \\mathbb{R}^m$ is additive white Gaussian noise with zero mean and covariance $\\mathbb{E}[w w^{\\top}] = \\sigma^2 I_m$. The least-squares estimate of the signal $x$ is given by $\\hat{x} = A^{\\dagger} y$. The estimation error is $e = \\hat{x} - x$.\n$$\ne = A^{\\dagger} y - x = A^{\\dagger}(A x + w) - x = (A^{\\dagger} A) x + A^{\\dagger} w - x\n$$\nFor a tall matrix $A$ with full column rank, $A^{\\dagger} A = I_n$. Thus, the error simplifies to:\n$$\ne = I_n x + A^{\\dagger} w - x = A^{\\dagger} w\n$$\nThe Mean Squared Error (MSE) is the expected squared Euclidean norm of the error vector, $MSE = \\mathbb{E}[\\|e\\|_2^2]$. The expectation is over both the randomness in $A$ and in $w$.\n$$\nMSE = \\mathbb{E}[\\|A^{\\dagger} w\\|_2^2] = \\mathbb{E}[w^{\\top}(A^{\\dagger})^{\\top}A^{\\dagger}w]\n$$\nUsing the law of total expectation, we condition on $A$: $\\mathbb{E}[\\cdot] = \\mathbb{E}_A[\\mathbb{E}_w[\\cdot|A]]$.\n$$\nMSE = \\mathbb{E}_A\\left[\\mathbb{E}_w\\left[w^{\\top}(A^{\\dagger})^{\\top}A^{\\dagger}w \\mid A\\right]\\right]\n$$\nUsing the trace identity $v^{\\top} M v = \\operatorname{trace}(Mvv^{\\top})$, the inner expectation becomes:\n$$\n\\mathbb{E}_w\\left[\\operatorname{trace}\\left((A^{\\dagger})^{\\top}A^{\\dagger}ww^{\\top}\\right) \\mid A\\right] = \\operatorname{trace}\\left((A^{\\dagger})^{\\top}A^{\\dagger}\\mathbb{E}_w[ww^{\\top}]\\right)\n$$\nSince $\\mathbb{E}_w[ww^{\\top}] = \\sigma^2 I_m$, this is:\n$$\n\\operatorname{trace}\\left((A^{\\dagger})^{\\top}A^{\\dagger}(\\sigma^2 I_m)\\right) = \\sigma^2 \\operatorname{trace}\\left((A^{\\dagger})^{\\top}A^{\\dagger}\\right) = \\sigma^2 \\|A^{\\dagger}\\|_{F}^2\n$$\nThe total MSE is the expectation over $A$:\n$$\nMSE = \\mathbb{E}_A[\\sigma^2 \\|A^{\\dagger}\\|_{F}^2] = \\sigma^2 \\mathbb{E}[\\|A^{\\dagger}\\|_{F}^2]\n$$\nSubstituting our derived result:\n$$\nMSE = \\sigma^2 \\frac{n}{m - n - 1}\n$$\nThis result provides a clear interpretation: $\\mathbb{E}[\\|A^{\\dagger}\\|_{F}^2]$ is the factor by which the input noise power $\\sigma^2$ is amplified to yield the total mean squared error in the signal estimate. The scaling of this factor, $\\frac{n}{m-n-1}$, reveals two key behaviors:\n1. As the number of measurements $m$ increases for a fixed signal dimension $n$, the denominator $m-n-1$ grows, and the amplification factor decreases. This is intuitive: more data leads to a less noisy estimate.\n2. For a fixed $m$, as the signal dimension $n$ approaches the critical value $m-1$, the denominator $m-n-1$ approaches $0$ from above. This causes the amplification factor, and thus the MSE, to diverge to infinity. This indicates that when the number of measurements is close to the number of unknowns, the problem of inverting the system becomes extremely ill-conditioned, leading to catastrophic amplification of noise.",
            "answer": "$$\n\\boxed{\\frac{n}{m - n - 1}}\n$$"
        },
        {
            "introduction": "Analyzing ill-conditioning is critical, but devising practical strategies to overcome it is the ultimate goal for engineers and scientists. This hands-on coding exercise challenges you to implement an early-stopping rule for the LSQR algorithm, a powerful iterative solver for least-squares problems. By dynamically monitoring the residual norm and estimates of $\\|A^\\dagger\\|_{2}$, you will create a regularization scheme that balances signal fidelity against noise amplification, a core task in solving real-world inverse problems .",
            "id": "3471119",
            "problem": "You are given a linear inverse problem in the setting of compressed sensing and sparse optimization. Let $A \\in \\mathbb{R}^{m \\times n}$, $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$, and noisy data $y \\in \\mathbb{R}^{m}$ generated by $y = A x_{\\mathrm{true}} + e$, where the entries of the noise vector $e$ are independent and identically distributed zero-mean Gaussian random variables with variance $\\sigma^{2}$. Consider solving the least-squares problem $\\min_{x \\in \\mathbb{R}^{n}} \\|y - A x\\|_{2}$ using Least Squares QR (LSQR), an iterative Krylov subspace method.\n\nThe core challenge is to design a principled early-stopping rule for LSQR that balances the amplification of noise by ill-conditioning against the approximation error. The rule must use only quantities available from the LSQR iterates: the residual norms and an estimate of the spectral norm of the Moore–Penrose pseudoinverse (MPP) of $A$, denoted $\\|A^{\\dagger}\\|_{2}$. The spectral norm of the pseudoinverse is defined by $\\|A^{\\dagger}\\|_{2} = 1/\\sigma_{\\min}(A)$, and the two-norm condition number of $A$ is defined by $\\kappa_{2}(A) = \\|A\\|_{2} \\,\\|A^{\\dagger}\\|_{2}$. LSQR provides reliable estimates of $\\|A\\|_{2}$ and $\\kappa_{2}(A)$ at each iteration, from which you may estimate $\\|A^{\\dagger}\\|_{2}$ by $\\|A^{\\dagger}\\|_{2} \\approx \\kappa_{2}(A) / \\|A\\|_{2}$.\n\nLet $x_{k}$ denote the LSQR iterate after $k$ iterations, and $r_{k} = y - A x_{k}$ its residual. Let $\\delta = \\sqrt{m}\\,\\sigma$ denote the typical scale of the noise norm $\\|e\\|_{2}$. You must implement the following stopping rule:\n\n- Choose fixed constants $\\tau \\ge 1$ and $\\alpha \\in (0,1]$. For this problem, use $\\tau = 1.1$ and $\\alpha = 1.0$.\n- Define the estimated pseudoinverse norm at iteration $k$ by $\\widehat{\\|A^{\\dagger}\\|}_{2,k} = \\widehat{\\kappa_{2}(A)}_{k} / \\widehat{\\|A\\|}_{2,k}$, using LSQR’s internal estimates at iteration $k$.\n- Stop at the smallest iteration $k \\ge 2$ such that both\n  $$\n  \\|r_{k}\\|_{2} \\le \\tau\\,\\delta\n  \\quad\\text{and}\\quad\n  \\|x_{k} - x_{k-1}\\|_{2} \\le \\alpha\\,\\widehat{\\|A^{\\dagger}\\|}_{2,k}\\,\\delta.\n  $$\nIf no such $k$ is found up to a maximum iteration budget $k_{\\max}$, return $k = k_{\\max}$ and the corresponding quantities.\n\nYour program must implement LSQR with the above stopping rule and produce results for the following test suite. For reproducibility, use a fixed pseudorandom generator seed as specified. All matrices and vectors are purely dimensionless; there are no physical units in this problem.\n\nTest suite (four cases), each case specified by $(m,n,\\text{profile},\\sigma,s,\\text{seed})$:\n- Case $1$: $(40,60,\\text{mild\\_ill},10^{-2},5,0)$.\n- Case $2$: $(60,60,\\text{well},5 \\cdot 10^{-3},10,1)$.\n- Case $3$: $(40,60,\\text{severe\\_ill},10^{-2},5,2)$.\n- Case $4$: $(50,200,\\text{severe\\_ill},5 \\cdot 10^{-3},10,3)$.\n\nMatrix construction:\n- For $\\text{well}$: let $A$ have independent and identically distributed entries $A_{ij} \\sim \\mathcal{N}(0,1/m)$, with no column scaling.\n- For $\\text{mild\\_ill}$: let $A$ be as above, but scale the last $\\lfloor n/2 \\rfloor$ columns by $10^{-2}$.\n- For $\\text{severe\\_ill}$: let $A$ be as above, but scale column $j$ by $10^{-\\gamma}$ where $\\gamma = 8 \\cdot \\frac{j-1}{n-1}$, producing singular values spanning approximately $[1,10^{-8}]$.\n\nSignal construction:\n- Let $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ be $s$-sparse with nonzero entries chosen uniformly at random positions and values sampled i.i.d. from $\\mathcal{N}(0,1)$.\n\nNoise:\n- Let $e \\in \\mathbb{R}^{m}$ have entries sampled i.i.d. from $\\mathcal{N}(0,\\sigma^{2})$.\n\nFor each case, compute $y = A x_{\\mathrm{true}} + e$, set $\\delta = \\sqrt{m}\\,\\sigma$, run LSQR with the stopping rule above, and cap the maximum iteration count by $k_{\\max} = \\min\\{m,n\\}$. For LSQR, use zero absolute and relative tolerances and no condition-limit stopping, i.e., $\\text{atol} = 0$, $\\text{btol} = 0$, and $\\text{conlim} = +\\infty$, and enforce the iteration limit exactly at $k$ when extracting $x_{k}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, the result is the list $[k^{\\star}, \\widehat{\\|A^{\\dagger}\\|}_{2,k^{\\star}}, \\|r_{k^{\\star}}\\|_{2}]$, where $k^{\\star}$ is the selected stopping iteration, $\\widehat{\\|A^{\\dagger}\\|}_{2,k^{\\star}}$ is the estimated pseudoinverse norm at stop, and $\\|r_{k^{\\star}}\\|_{2}$ is the residual norm at stop. Thus the final output must be of the form\n$\n\\big[\n[k_{1}, \\widehat{\\|A^{\\dagger}\\|}_{2,1}, \\|r_{1}\\|_{2}],\n[k_{2}, \\widehat{\\|A^{\\dagger}\\|}_{2,2}, \\|r_{2}\\|_{2}],\n[k_{3}, \\widehat{\\|A^{\\dagger}\\|}_{2,3}, \\|r_{3}\\|_{2}],\n[k_{4}, \\widehat{\\|A^{\\dagger}\\|}_{2,4}, \\|r_{4}\\|_{2}]\n\\big]\n$ and must be printed as a single line.",
            "solution": "The problem requires the implementation of a specific early-stopping rule for the Least Squares QR (LSQR) algorithm, a Krylov subspace method used to solve the linear least-squares problem $\\min_{x \\in \\mathbb{R}^{n}} \\|y - A x\\|_{2}$. The system arises from a linear inverse problem $y = A x_{\\mathrm{true}} + e$, where $A \\in \\mathbb{R}^{m \\times n}$ is a given matrix, $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ is the unknown sparse signal, and $e \\in \\mathbb{R}^{m}$ is a noise vector with independent and identically distributed entries from $\\mathcal{N}(0, \\sigma^2)$.\n\nThe fundamental challenge in solving such inverse problems, especially when the matrix $A$ is ill-conditioned, is to prevent the amplification of noise in the solution. Iterative methods like LSQR offer a form of regularization through early stopping: terminating the iteration before it starts to excessively fit the noise. The problem specifies a principled stopping rule designed to balance the trade-off between approximation error (how well $A x_k$ fits $y$) and solution stability (preventing noise amplification).\n\nThe stopping rule is defined as follows: for given constants $\\tau \\ge 1$ and $\\alpha \\in (0,1]$, the iteration is stopped at the smallest integer $k \\ge 2$ that satisfies two conditions simultaneously:\n1.  $\\|r_{k}\\|_{2} \\le \\tau\\,\\delta$\n2.  $\\|x_{k} - x_{k-1}\\|_{2} \\le \\alpha\\,\\widehat{\\|A^{\\dagger}\\|}_{2,k}\\,\\delta$\n\nHere, $x_k$ is the solution estimate at iteration $k$, and $r_k = y - A x_k$ is the corresponding residual. The parameter $\\delta = \\sqrt{m}\\,\\sigma$ represents the expected Euclidean norm of the noise vector $e$. The first condition is a form of the discrepancy principle; it dictates that the iteration should stop when the residual's norm is on the order of the noise norm. Further reduction of the residual would likely correspond to fitting the noise $e$ rather than the underlying signal structure in $A x_{\\mathrm{true}}$. For this problem, $\\tau = 1.1$.\n\nThe second condition introduces a solution stabilization check. The term $\\widehat{\\|A^{\\dagger}\\|}_{2,k}$ is an estimate of the spectral norm of the Moore-Penrose pseudoinverse of $A$, computed at iteration $k$ from LSQR's internal estimates of the condition number $\\widehat{\\kappa_{2}(A)}_{k}$ and spectral norm $\\widehat{\\|A\\|}_{2,k}$ as $\\widehat{\\|A^{\\dagger}\\|}_{2,k} = \\widehat{\\kappa_{2}(A)}_{k} / \\widehat{\\|A\\|}_{2,k}$. The quantity $\\widehat{\\|A^{\\dagger}\\|}_{2,k}\\,\\delta$ can be interpreted as an estimate of the magnitude of noise propagated into the solution space. The condition thus stops the iteration when the change in the solution vector, $\\|x_k - x_{k-1}\\|_2$, becomes smaller than this noise amplification estimate, suggesting that subsequent updates are unlikely to yield significant improvements to the true signal. For this problem, $\\alpha = 1.0$.\n\nIf these conditions are not met by the maximum allowed iteration count, $k_{\\max} = \\min\\{m,n\\}$, the process terminates and returns the results from iteration $k_{\\max}$.\n\nThe implementation proceeds by first generating the test data for each case as specified. A matrix $A$ of size $m \\times n$ is constructed with entries from $\\mathcal{N}(0, 1/m)$, and its columns are scaled according to the specified ill-conditioning profile ('well', 'mild_ill', or 'severe_ill'). An $s$-sparse signal vector $x_{\\mathrm{true}}$ is generated with random support and normally distributed non-zero values. The measurement vector $y$ is then computed as $y = A x_{\\mathrm{true}} + e$, where $e$ is a realization of the Gaussian noise.\n\nTo apply the custom stopping rule, we must access quantities from each iteration of LSQR. A direct approach is to run the standard `scipy.sparse.linalg.lsqr` function within a loop. For each iteration $k$ from $2$ to $k_{\\max}$, we execute `lsqr` with `iter_lim=k` to obtain $x_k$ and the estimates $\\widehat{\\|A\\|}_{2,k}$ and $\\widehat{\\kappa_{2}(A)}_{k}$. We also require $x_{k-1}$, which is retained from the previous step in the loop. With these quantities, both stopping conditions can be evaluated at each $k$. The first $k$ that satisfies both is chosen as the stopping iteration $k^\\star$. This iterative procedure, while computationally more intensive than a single `lsqr` call, correctly implements the specified logic by providing the necessary history of iterates.\n\nThe specified parameters for the `lsqr` solver are an absolute tolerance `atol` of $0$, a relative tolerance `btol` of $0$, and an unlimited condition number `conlim` of $+\\infty$, ensuring that the solver's termination is controlled solely by our external loop and the iteration limit `iter_lim`. For each test case, the final output consists of the stopping iteration $k^{\\star}$, the estimated pseudoinverse norm $\\widehat{\\|A^{\\dagger}\\|}_{2,k^{\\star}}$, and the residual norm $\\|r_{k^{\\star}}\\|_2$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse.linalg import lsqr\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (m, n, profile, sigma, s, seed)\n        (40, 60, 'mild_ill', 1e-2, 5, 0),\n        (60, 60, 'well', 5e-3, 10, 1),\n        (40, 60, 'severe_ill', 1e-2, 5, 2),\n        (50, 200, 'severe_ill', 5e-3, 10, 3)\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(*case)\n        results.append(result)\n\n    # Format the final output string as specified, removing whitespace.\n    formatted_results = [str(res).replace(' ', '') for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef process_case(m, n, profile, sigma, s, seed):\n    \"\"\"\n    Generates data for a single test case and runs the LSQR with the custom stopping rule.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate matrix A\n    A = rng.normal(0, np.sqrt(1/m), size=(m, n))\n    if profile == 'mild_ill':\n        num_cols_to_scale = int(np.floor(n / 2))\n        scaling_factor = 1e-2\n        A[:, -num_cols_to_scale:] *= scaling_factor\n    elif profile == 'severe_ill':\n        for j in range(n):\n            gamma = 8.0 * j / (n - 1)\n            scaling_factor = 10**(-gamma)\n            A[:, j] *= scaling_factor\n\n    # 2. Generate true signal x_true\n    x_true = np.zeros(n)\n    support = rng.choice(n, s, replace=False)\n    x_true[support] = rng.normal(0, 1, size=s)\n\n    # 3. Generate noise e and measurement y\n    e = rng.normal(0, sigma, size=m)\n    y = A @ x_true + e\n\n    # 4. Implement LSQR with the specified stopping rule\n    tau = 1.1\n    alpha = 1.0\n    delta = np.sqrt(m) * sigma\n    k_max = min(m, n)\n\n    k_star = k_max\n    final_report = None\n\n    # Get x_{k-1} for the first iteration of the loop (k=2)\n    # This corresponds to x_1\n    res_prev = lsqr(A, y, damp=0.0, atol=0, btol=0, conlim=float('inf'), iter_lim=1)\n    x_prev = res_prev[0]\n\n    for k in range(2, k_max + 1):\n        res_k = lsqr(A, y, damp=0.0, atol=0, btol=0, conlim=float('inf'), iter_lim=k)\n        x_k = res_k[0]\n        r_norm_k = res_k[3]\n        anorm_k = res_k[5]\n        acond_k = res_k[6]\n        \n        if anorm_k > 0:\n            est_A_dagger_norm_k = acond_k / anorm_k\n        else:\n            est_A_dagger_norm_k = float('inf')\n\n        x_diff_norm_k = np.linalg.norm(x_k - x_prev)\n\n        cond1 = r_norm_k = tau * delta\n        cond2 = x_diff_norm_k = alpha * est_A_dagger_norm_k * delta\n        \n        if cond1 and cond2:\n            k_star = k\n            final_report = [k_star, est_A_dagger_norm_k, r_norm_k]\n            break\n\n        # Update x_prev for the next iteration step\n        x_prev = x_k\n\n    # If the loop completes without stopping, use results from k_max\n    if final_report is None:\n        # The last iteration of the loop was k = k_max. res_k holds its results.\n        res_kmax = lsqr(A, y, damp=0.0, atol=0, btol=0, conlim=float('inf'), iter_lim=k_max)\n        r_norm_kmax = res_kmax[3]\n        anorm_kmax = res_kmax[5]\n        acond_kmax = res_kmax[6]\n        if anorm_kmax > 0:\n            est_A_dagger_norm_kmax = acond_kmax / anorm_kmax\n        else:\n            est_A_dagger_norm_kmax = float('inf')\n        \n        final_report = [k_max, est_A_dagger_norm_kmax, r_norm_kmax]\n\n    return final_report\n\nsolve()\n```"
        }
    ]
}