{
    "hands_on_practices": [
        {
            "introduction": "The most fundamental way to understand the adjoint is to derive it directly from its defining inner product relationship, $\\langle Ax, y \\rangle = \\langle x, A^* y \\rangle$. This first exercise  provides an opportunity to do just that for a simple yet essential operator in compressed sensing: the subsampling or masking operator. By working through this problem, you will solidify your grasp of the definition and discover that the adjoint of subsampling has an intuitive and elegant structure as a zero-padding or embedding operator.",
            "id": "3457713",
            "problem": "In the context of compressed sensing and sparse optimization, consider the linear operator $M : \\mathbb{C}^{n} \\to \\mathbb{C}^{m}$ defined by subsampling coordinates indexed by an index set. Let $\\Omega = \\{\\omega_{1}, \\omega_{2}, \\ldots, \\omega_{m}\\} \\subset \\{1,2,\\ldots,n\\}$ with $\\omega_{1} < \\omega_{2} < \\cdots < \\omega_{m}$, and define $M$ by\n$$(Mx)_{k} = x_{\\omega_{k}} \\quad \\text{for all } k \\in \\{1,\\ldots,m\\},$$\nfor any $x \\in \\mathbb{C}^{n}$. Equip $\\mathbb{C}^{n}$ and $\\mathbb{C}^{m}$ with the standard complex Euclidean inner products, namely $\\langle u, v \\rangle = v^{*} u$ for vectors $u$ and $v$ of matching dimension, where $v^{*}$ denotes the conjugate transpose of $v$.\n\nUsing only the definition of the adjoint operator $M^{*}$, which is the unique linear operator $M^{*} : \\mathbb{C}^{m} \\to \\mathbb{C}^{n}$ satisfying\n$$\\langle Mx, y \\rangle = \\langle x, M^{*} y \\rangle \\quad \\text{for all } x \\in \\mathbb{C}^{n}, \\ y \\in \\mathbb{C}^{m},$$\nderive an explicit expression for $M^{*} y$ in closed form in terms of $\\Omega$ and $y \\in \\mathbb{C}^{m}$. Express your final answer as a single analytic expression. No numerical approximation is required.",
            "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Operator:** A linear operator $M : \\mathbb{C}^{n} \\to \\mathbb{C}^{m}$.\n- **Index Set:** $\\Omega = \\{\\omega_{1}, \\omega_{2}, \\ldots, \\omega_{m}\\} \\subset \\{1,2,\\ldots,n\\}$ with $\\omega_{1} < \\omega_{2} < \\cdots < \\omega_{m}$.\n- **Operator Definition:** $(Mx)_{k} = x_{\\omega_{k}}$ for all $k \\in \\{1,\\ldots,m\\}$ and for any $x \\in \\mathbb{C}^{n}$.\n- **Vector Spaces:** The domain is $\\mathbb{C}^{n}$ and the codomain is $\\mathbb{C}^{m}$.\n- **Inner Product:** For vectors $u, v$, the inner product is $\\langle u, v \\rangle = v^{*} u$, where $v^{*}$ is the conjugate transpose of $v$. This is the standard complex Euclidean inner product.\n- **Adjoint Definition:** The adjoint operator $M^{*} : \\mathbb{C}^{m} \\to \\mathbb{C}^{n}$ is the unique linear operator satisfying $\\langle Mx, y \\rangle = \\langle x, M^{*} y \\rangle$ for all $x \\in \\mathbb{C}^{n}$ and $y \\in \\mathbb{C}^{m}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is mathematically well-defined and internally consistent.\n- **Scientifically Grounded:** The concepts of linear operators, vector spaces, inner products, and adjoints are fundamental in linear algebra and functional analysis. The operator $M$ described is a standard subsampling or restriction operator, commonly used in fields like signal processing and compressed sensing. The problem is based on established mathematical principles.\n- **Well-Posed:** The existence and uniqueness of the adjoint operator for a linear map between finite-dimensional Hilbert spaces (like $\\mathbb{C}^{n}$ and $\\mathbb{C}^{m}$ with the given inner product) is a standard theorem. The problem asks for the derivation of this unique operator, which is a well-posed task.\n- **Objective:** The problem is stated in precise, formal mathematical language with no subjective or ambiguous terms. All necessary information is provided symbolically, appropriate for a general derivation.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Solution\nThe adjoint operator $M^{*} : \\mathbb{C}^{m} \\to \\mathbb{C}^{n}$ is defined by the relation\n$$ \\langle Mx, y \\rangle = \\langle x, M^{*} y \\rangle $$\nfor all vectors $x \\in \\mathbb{C}^{n}$ and $y \\in \\mathbb{C}^{m}$. The inner product on the left-hand side is in $\\mathbb{C}^{m}$, while the inner product on the right-hand side is in $\\mathbb{C}^{n}$.\n\nWe begin by expanding the left-hand side (LHS) using the definition of the inner product, $\\langle u, v \\rangle = v^{*}u = \\sum_{k} \\overline{v_{k}} u_{k}$.\n$$ \\langle Mx, y \\rangle = y^{*} (Mx) $$\nThe vector $Mx \\in \\mathbb{C}^{m}$ has components $(Mx)_{k} = x_{\\omega_{k}}$ for $k \\in \\{1, \\ldots, m\\}$. Thus,\n$$ \\langle Mx, y \\rangle = \\sum_{k=1}^{m} \\overline{y_{k}} (Mx)_{k} = \\sum_{k=1}^{m} \\overline{y_{k}} x_{\\omega_{k}}. $$\n\nNext, we expand the right-hand side (RHS). Let $z = M^{*}y$. Since $M^{*}$ maps from $\\mathbb{C}^{m}$ to $\\mathbb{C}^{n}$, the vector $z$ is in $\\mathbb{C}^{n}$.\n$$ \\langle x, M^{*} y \\rangle = \\langle x, z \\rangle = z^{*} x = \\sum_{j=1}^{n} \\overline{z_{j}} x_{j}. $$\nThe components of $z$ are precisely what we want to determine, i.e., $z_{j} = (M^{*}y)_{j}$.\n\nEquating the expanded LHS and RHS, we have\n$$ \\sum_{k=1}^{m} \\overline{y_{k}} x_{\\omega_{k}} = \\sum_{j=1}^{n} \\overline{(M^{*}y)_{j}} x_{j}. $$\nThis equality must hold for any choice of vector $x \\in \\mathbb{C}^{n}$. This allows us to determine the components $(M^{*}y)_{j}$ by comparing the coefficients of each $x_{j}$ on both sides of the equation.\n\nLet us rearrange the summation on the LHS to be over the index set $\\{1, \\ldots, n\\}$. The indices appearing on the LHS are $\\omega_{1}, \\ldots, \\omega_{m}$, which form the set $\\Omega$. For any index $j \\in \\{1, \\ldots, n\\}$, we can analyze its contribution.\n\nIf $j \\notin \\Omega$, then $x_{j}$ does not appear on the LHS. This is equivalent to its coefficient being $0$.\nIf $j \\in \\Omega$, then $j = \\omega_{k}$ for exactly one $k \\in \\{1, \\ldots, m\\}$ because the elements of $\\Omega$ are distinct. In this case, the term $\\overline{y_{k}}x_{\\omega_{k}}$ is present on the LHS, which is $\\overline{y_{k}}x_{j}$.\n\nBy comparing the coefficients of $x_{j}$ for each $j \\in \\{1, \\ldots, n\\}$, we can deduce the value of $\\overline{(M^{*}y)_{j}}$.\nFor a specific $j \\in \\{1, \\ldots, n\\}$:\n$1$. If $j \\notin \\Omega$, the coefficient of $x_{j}$ on the LHS is $0$. Therefore, $\\overline{(M^{*}y)_{j}} = 0$, which implies $(M^{*}y)_{j} = 0$.\n$2$. If $j \\in \\Omega$, there's a unique $k$ such that $j = \\omega_{k}$. The coefficient of $x_{j}$ (which is $x_{\\omega_k}$) on the LHS is $\\overline{y_{k}}$. Therefore, $\\overline{(M^{*}y)_{j}} = \\overline{y_{k}}$, which implies $(M^{*}y)_{j} = y_{k}$.\n\nCombining these two cases gives a complete component-wise description of the vector $M^{*}y$. The operator $M^{*}$ takes a vector $y \\in \\mathbb{C}^{m}$ and produces a vector in $\\mathbb{C}^{n}$ whose entries are zero except at the indices specified by $\\Omega$. At the index $\\omega_{k} \\in \\Omega$, the value is $y_{k}$. This operation is often referred to as zero-padding or embedding.\n\nWe can write this result in a compact vector form. Let $e_{j}$ denote the $j$-th standard basis vector in $\\mathbb{C}^{n}$, which has a $1$ in the $j$-th position and zeros elsewhere. The vector $y_{k} e_{\\omega_{k}}$ is a vector in $\\mathbb{C}^{n}$ that has the value $y_{k}$ at index $\\omega_{k}$ and is zero everywhere else. The vector $M^{*}y$ is the sum of such vectors for all components of $y$:\n$$ M^{*}y = y_{1} e_{\\omega_{1}} + y_{2} e_{\\omega_{2}} + \\cdots + y_{m} e_{\\omega_{m}}. $$\nThis can be written in summation notation, which provides the final closed-form expression for the adjoint operator acting on a vector $y$.\n$$ M^{*}y = \\sum_{k=1}^{m} y_{k} e_{\\omega_{k}}. $$\nHere, $e_{\\omega_k}$ represents the standard basis vector in $\\mathbb{C}^n$ corresponding to the index $\\omega_k \\in \\Omega$.",
            "answer": "$$\\boxed{\\sum_{k=1}^{m} y_{k} e_{\\omega_{k}}}$$"
        },
        {
            "introduction": "Building on the foundational definition, we now turn to one of the most important classes of linear operators in science and engineering: convolution. This practice problem  moves from the finite-dimensional setting of vectors to the infinite-dimensional space of square-summable sequences, $\\ell^2(\\mathbb{Z})$. By deriving the adjoint of a discrete convolution operator, you will uncover a profound and celebrated duality: the adjoint of convolution is correlation. This result is a cornerstone of signal processing, filter theory, and the analysis of linear time-invariant systems.",
            "id": "3457685",
            "problem": "Consider the linear operator $C_{h} : \\ell^{2}(\\mathbb{Z}) \\to \\ell^{2}(\\mathbb{Z})$ defined by discrete convolution with a sequence $h \\in \\ell^{1}(\\mathbb{Z})$, namely\n$$\n(C_{h} x)[n] \\;=\\; \\sum_{k \\in \\mathbb{Z}} h[k]\\, x[n - k], \\quad n \\in \\mathbb{Z}.\n$$\nAssume $x, y : \\mathbb{Z} \\to \\mathbb{C}$ and use the standard $\\ell^{2}$ inner product\n$$\n\\langle u, v \\rangle \\;=\\; \\sum_{n \\in \\mathbb{Z}} u[n]\\, \\overline{v[n]}.\n$$\nStarting only from the core definitions of the $\\ell^{2}$ inner product, linear adjoint, and convolution on $\\mathbb{Z}$, derive the adjoint operator $C_{h}^{\\ast}$ and express it as a convolution by a sequence $\\tilde{h} \\in \\ell^{1}(\\mathbb{Z})$. Provide a complete justification for any interchange of infinite sums by establishing absolute convergence with inequalities grounded in first principles. Your final answer must be a single closed-form analytic expression for the entries of $\\tilde{h}$ in terms of $h$; do not invoke any transform-domain characterization or external theorems beyond the aforementioned core definitions. If you introduce any auxiliary indexing transformations, clearly state and justify them.",
            "solution": "The problem statement is a valid, well-posed mathematical exercise in functional analysis. It is scientifically grounded in the theory of linear operators on Hilbert spaces, specifically the space of square-summable sequences $\\ell^{2}(\\mathbb{Z})$. All terms are defined formally and without ambiguity. The conditions provided, namely $h \\in \\ell^{1}(\\mathbb{Z})$ and $x, y \\in \\ell^{2}(\\mathbb{Z})$, are precisely what is needed to ensure the operator is well-defined, bounded, and thus possesses a unique adjoint. The problem is self-contained and free of contradictions.\n\nThe adjoint operator $C_{h}^{\\ast}$ of the linear operator $C_{h} : \\ell^{2}(\\mathbb{Z}) \\to \\ell^{2}(\\mathbb{Z})$ is uniquely defined by the relation\n$$\n\\langle C_{h} x, y \\rangle = \\langle x, C_{h}^{\\ast} y \\rangle\n$$\nfor all $x, y \\in \\ell^{2}(\\mathbb{Z})$. We begin by expanding the left-hand side using the provided definitions of the operator $C_h$ and the inner product $\\langle \\cdot, \\cdot \\rangle$.\n\nLet $x, y \\in \\ell^{2}(\\mathbb{Z})$. The inner product $\\langle C_{h} x, y \\rangle$ is given by:\n$$\n\\langle C_{h} x, y \\rangle = \\sum_{n \\in \\mathbb{Z}} (C_{h} x)[n]\\, \\overline{y[n]}\n$$\nSubstituting the definition of the convolution operator $(C_{h} x)[n] = \\sum_{k \\in \\mathbb{Z}} h[k]\\, x[n - k]$, we obtain a double summation:\n$$\n\\langle C_{h} x, y \\rangle = \\sum_{n \\in \\mathbb{Z}} \\left( \\sum_{k \\in \\mathbb{Z}} h[k]\\, x[n - k] \\right) \\overline{y[n]} = \\sum_{n \\in \\mathbb{Z}} \\sum_{k \\in \\mathbb{Z}} h[k]\\, x[n - k]\\, \\overline{y[n]}\n$$\nTo proceed, we must be able to interchange the order of these infinite summations. This is permissible if the sum converges absolutely. We verify this condition by considering the sum of the absolute values of the terms:\n$$\n\\sum_{n \\in \\mathbb{Z}} \\sum_{k \\in \\mathbb{Z}} |h[k]\\, x[n - k]\\, \\overline{y[n]}| = \\sum_{n \\in \\mathbb{Z}} \\sum_{k \\in \\mathbb{Z}} |h[k]|\\, |x[n - k]|\\, |y[n]|\n$$\nWe can rearrange this sum as:\n$$\n\\sum_{k \\in \\mathbb{Z}} |h[k]| \\left( \\sum_{n \\in \\mathbb{Z}} |x[n - k]|\\, |y[n]| \\right)\n$$\nThe inner sum, $\\sum_{n \\in \\mathbb{Z}} |x[n - k]|\\, |y[n]|$, can be bounded using the Cauchy-Schwarz inequality for sequences. This sum is the inner product of the sequences $(|x[n-k]|)_{n \\in \\mathbb{Z}}$ and $(|y[n]|)_{n \\in \\mathbb{Z}}$. Thus,\n$$\n\\sum_{n \\in \\mathbb{Z}} |x[n - k]|\\, |y[n]| \\le \\left( \\sum_{n \\in \\mathbb{Z}} |x[n - k]|^2 \\right)^{1/2} \\left( \\sum_{n \\in \\mathbb{Z}} |y[n]|^2 \\right)^{1/2}\n$$\nThe $\\ell^{2}$-norm is invariant under translation of the index, so $\\left( \\sum_{n \\in \\mathbb{Z}} |x[n - k]|^2 \\right)^{1/2} = \\|x\\|_{\\ell^2}$. The second term is $\\|y\\|_{\\ell^2}$. Therefore, the inner sum is bounded by $\\|x\\|_{\\ell^2} \\|y\\|_{\\ell^2}$.\nSubstituting this bound back into the expression for the total sum gives:\n$$\n\\sum_{n \\in \\mathbb{Z}} \\sum_{k \\in \\mathbb{Z}} |h[k]\\, x[n - k]\\, \\overline{y[n]}| \\le \\sum_{k \\in \\mathbb{Z}} |h[k]| (\\|x\\|_{\\ell^2} \\|y\\|_{\\ell^2}) = \\|y\\|_{\\ell^2} \\|x\\|_{\\ell^2} \\sum_{k \\in \\mathbb{Z}} |h[k]| = \\|h\\|_{\\ell^1} \\|x\\|_{\\ell^2} \\|y\\|_{\\ell^2}\n$$\nSince $h \\in \\ell^{1}(\\mathbb{Z})$ and $x, y \\in \\ell^{2}(\\mathbb{Z})$, the norms $\\|h\\|_{\\ell^1}$, $\\|x\\|_{\\ell^2}$, and $\\|y\\|_{\\ell^2}$ are all finite. Thus, the total sum is finite, which establishes absolute convergence. By the Fubini-Tonelli theorem for sums, we are justified in interchanging the order of summation.\n\nProceeding with the manipulation of $\\langle C_h x, y \\rangle$:\n$$\n\\langle C_{h} x, y \\rangle = \\sum_{k \\in \\mathbb{Z}} \\sum_{n \\in \\mathbb{Z}} h[k]\\, x[n - k]\\, \\overline{y[n]}\n$$\nTo group terms with $x$ and $y$ separately, we introduce an index transformation. Let $m = n - k$. This implies $n = m + k$. For any fixed integer $k$, the mapping $n \\mapsto m$ is a bijection from $\\mathbb{Z}$ to $\\mathbb{Z}$. Substituting $n = m+k$ and summing over $m$ instead of $n$:\n$$\n\\langle C_{h} x, y \\rangle = \\sum_{k \\in \\mathbb{Z}} \\sum_{m \\in \\mathbb{Z}} h[k]\\, x[m]\\, \\overline{y[m + k]}\n$$\nAgain, by the established absolute convergence, we can interchange the order of summation:\n$$\n\\langle C_{h} x, y \\rangle = \\sum_{m \\in \\mathbb{Z}} \\sum_{k \\in \\mathbb{Z}} x[m]\\, h[k]\\, \\overline{y[m + k]}\n$$\nWe can factor $x[m]$ out of the inner sum and use the property of the complex conjugate $\\overline{a}b = \\overline{a\\overline{b}}$:\n$$\n\\langle C_{h} x, y \\rangle = \\sum_{m \\in \\mathbb{Z}} x[m] \\left( \\sum_{k \\in \\mathbb{Z}} h[k]\\, \\overline{y[m + k]} \\right) = \\sum_{m \\in \\mathbb{Z}} x[m] \\overline{\\left( \\sum_{k \\in \\mathbb{Z}} \\overline{h[k]}\\, y[m + k] \\right)}\n$$\nThis expression is now in the form of $\\langle x, C_{h}^{\\ast} y \\rangle = \\sum_{m \\in \\mathbb{Z}} x[m] \\overline{(C_{h}^{\\ast} y)[m]}$. By comparing the two forms, we identify the action of the adjoint operator on $y$:\n$$\n(C_{h}^{\\ast} y)[m] = \\sum_{k \\in \\mathbb{Z}} \\overline{h[k]}\\, y[m + k]\n$$\nThe problem requires expressing $C_{h}^{\\ast}$ as a convolution operator $C_{\\tilde{h}}$, which has the form $(C_{\\tilde{h}} y)[m] = \\sum_{j \\in \\mathbb{Z}} \\tilde{h}[j]\\, y[m - j]$. To match this form, we introduce another index transformation in the expression for $(C_h^* y)[m]$. Let $j = -k$. This implies $k = -j$. This mapping is also a bijection from $\\mathbb{Z}$ to $\\mathbb{Z}$. Substituting $k = -j$:\n$$\n(C_{h}^{\\ast} y)[m] = \\sum_{j \\in \\mathbb{Z}} \\overline{h[-j]}\\, y[m - j]\n$$\nComparing this with the definition of $C_{\\tilde{h}}$, we find that the operator $C_{h}^{\\ast}$ is indeed a convolution operator $C_{\\tilde{h}}$ where the sequence $\\tilde{h}$ is defined by its entries:\n$$\n\\tilde{h}[j] = \\overline{h[-j]} \\quad \\text{for any } j \\in \\mathbb{Z}.\n$$\nThis is the so-called time-reversed conjugate of the sequence $h$. The problem implies that $\\tilde{h}$ should also be in $\\ell^1(\\mathbb{Z})$. We can verify this:\n$$\n\\|\\tilde{h}\\|_{\\ell^1} = \\sum_{j \\in \\mathbb{Z}} |\\tilde{h}[j]| = \\sum_{j \\in \\mathbb{Z}} |\\overline{h[-j]}| = \\sum_{j \\in \\mathbb{Z}} |h[-j]|\n$$\nLetting $k = -j$, this sum becomes $\\sum_{k \\in \\mathbb{Z}} |h[k]| = \\|h\\|_{\\ell^1}$. Since $h \\in \\ell^1(\\mathbb{Z})$, it follows that $\\|\\tilde{h}\\|_{\\ell^1} < \\infty$, so $\\tilde{h} \\in \\ell^1(\\mathbb{Z})$.\n\nThe analytical expression for the entries of the sequence $\\tilde{h}$, using $n$ as the index variable, is thus $\\overline{h[-n]}$.",
            "answer": "$$\\boxed{\\overline{h[-n]}}$$"
        },
        {
            "introduction": "After deriving the structure of adjoints, the next step is to use them as building blocks in practical algorithms. In many large-scale applications, linear operators are not stored as explicit matrices but are instead defined by functions that compute their action on a vector. This \"matrix-free\" paradigm requires algorithms that can operate using only applications of the operator $A$ and its adjoint $A^*$. This problem  challenges you to design and implement one such fundamental algorithm—the power iteration method to estimate an operator's spectral norm—thereby demonstrating how the abstract concept of the adjoint becomes an indispensable tool for computational science.",
            "id": "3457681",
            "problem": "Let $A : \\mathbb{R}^n \\to \\mathbb{R}^m$ be a linear operator between finite-dimensional Euclidean spaces equipped with the standard inner product and induced Euclidean norm. The adjoint operator $A^* : \\mathbb{R}^m \\to \\mathbb{R}^n$ is defined by the relation $\\langle A x, y \\rangle = \\langle x, A^* y \\rangle$ for all $x \\in \\mathbb{R}^n$ and $y \\in \\mathbb{R}^m$. The spectral norm of $A$ is $\\|A\\|_2 = \\sup_{x \\neq 0} \\frac{\\|A x\\|_2}{\\|x\\|_2}$.\n\nYour task is to design and implement a power-iteration estimator that approximates $\\|A\\|_2$ using only repeated applications of $A$ and $A^*$, without forming $A^*A$ explicitly and without using any decomposition of $A$. The design must be justified from first principles: start from the definitions of the adjoint and the spectral norm, and use only well-established facts such as the existence and properties of the singular value decomposition (SVD) and the spectral theorem for symmetric positive semidefinite operators. Your algorithm must:\n- Operate on function handles for $A$ and $A^*$ that realize the action of the operator and its adjoint on vectors.\n- Use a randomized nonzero initialization to avoid adversarial alignment with the null space, with a fixed seed for reproducibility.\n- Include a stopping rule based on relative change below a tolerance $\\varepsilon$, and a maximum iteration cap $K$.\n- Robustly detect the zero operator by returning $0$ if the first application of $A$ to a unit-norm vector yields the zero vector.\n\nIn the context of compressed sensing and sparse optimization, many sensing operators are used only through their action and the action of their adjoints (for example, subsampled orthonormal transforms and random Gaussian matrices). You will validate your estimator on the following test suite. In each case, implement $A$ and $A^*$ as functions and compute an independent ground-truth spectral norm using a mathematically justified method. Use the following fixed parameters for the estimator in all cases: tolerance $\\varepsilon = 10^{-12}$, maximum iterations $K = 1000$, and a per-case initialization seed as specified. All outputs are to be real numbers without physical units.\n\nTest suite:\n- Case $1$ (dense Gaussian, rectangular): Let $m = 8$, $n = 5$, and construct a matrix $G \\in \\mathbb{R}^{m \\times n}$ with independent standard normal entries using the pseudorandom seed $12345$. Define $A(x) = G x$ and $A^*(y) = G^\\top y$. The ground-truth spectral norm is computed as the matrix $2$-norm of $G$.\n- Case $2$ (zero operator): Let $m = 3$, $n = 7$, and define $A(x) = 0 \\in \\mathbb{R}^m$ and $A^*(y) = 0 \\in \\mathbb{R}^n$. The ground-truth spectral norm is $0$.\n- Case $3$ (subsampled orthonormal transform): Let $n = 64$ and consider the orthonormal type-$2$ Discrete Cosine Transform (DCT) $F : \\mathbb{R}^n \\to \\mathbb{R}^n$ with orthonormal normalization. Let $S \\subset \\{0,1,\\dots,n-1\\}$ be the even indices $S = \\{0,2,4,\\dots,62\\}$, and define $A(x) = (F x)_S$ (selection of the coordinates in $S$) and $A^*(y)$ as the adjoint action corresponding to zero-padding back to length $n$ at the indices $S$ followed by the inverse transform. For this subsampled orthonormal operator, the ground-truth spectral norm is $1$.\n- Case $4$ (diagonal operator): Let $n = m = 4$ and define $A$ as the diagonal operator with entries $\\{0.1, 2.5, -3.7, 1.2\\}$. Then $A^* = A$, and the ground-truth spectral norm is the maximum absolute value of the diagonal entries, namely $3.7$.\n- Case $5$ (scaled dense Gaussian): Let $m = 10$, $n = 4$. Construct a matrix $H \\in \\mathbb{R}^{m \\times n}$ with independent standard normal entries using the pseudorandom seed $2023$, and set $c = 7.3$. Define $A(x) = (c H) x$ and $A^*(y) = (c H)^\\top y$. The ground-truth spectral norm is the matrix $2$-norm of $c H$.\n\nFor each case, run your estimator with the following initialization seeds for the randomized starting vector: seed $0$ for Case $1$, seed $1$ for Case $2$, seed $2$ for Case $3$, seed $3$ for Case $4$, and seed $4$ for Case $5$. Compare your estimate $\\widehat{\\|A\\|_2}$ to the ground-truth $\\|A\\|_2$ using an absolute tolerance of $\\delta = 10^{-8}$ and produce a boolean result that is true if and only if $|\\widehat{\\|A\\|_2} - \\|A\\|_2| \\le \\delta$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where each result is the boolean outcome for the corresponding case in the order Cases $1$ through $5$.",
            "solution": "The task is to design and implement a power iteration based-estimator for the spectral norm $\\|A\\|_2$ of a linear operator $A : \\mathbb{R}^n \\to \\mathbb{R}^m$, using function handles for $A$ and its adjoint $A^*$ without access to their matrix representations. The design must be justified from first principles.\n\n**1. Theoretical Foundation**\n\nThe spectral norm of a linear operator $A$ between finite-dimensional Euclidean spaces is defined as:\n$$\n\\|A\\|_2 = \\sup_{x \\in \\mathbb{R}^n, x \\neq 0} \\frac{\\|Ax\\|_2}{\\|x\\|_2}\n$$\nBy definition, the spectral norm is equal to the largest singular value of $A$, denoted $\\sigma_{\\max}(A)$. The singular values $\\sigma_i(A)$ of an operator $A$ are the non-negative square roots of the eigenvalues of the operator $A^*A$, where $A^*: \\mathbb{R}^m \\to \\mathbb{R}^n$ is the adjoint of $A$ defined by the relation $\\langle Ax, y \\rangle = \\langle x, A^*y \\rangle$ for all $x \\in \\mathbb{R}^n$ and $y \\in \\mathbb{R}^m$.\n\nThe operator $B = A^*A : \\mathbb{R}^n \\to \\mathbb{R}^n$ is self-adjoint and positive semidefinite:\n- **Self-adjoint:** For any $x, z \\in \\mathbb{R}^n$, we have $\\langle Bx, z \\rangle = \\langle (A^*A)x, z \\rangle = \\langle Ax, Az \\rangle = \\langle x, A^*(Az) \\rangle = \\langle x, (A^*A)z \\rangle = \\langle x, Bz \\rangle$.\n- **Positive semidefinite:** For any $x \\in \\mathbb{R}^n$, we have $\\langle Bx, x \\rangle = \\langle (A^*A)x, x \\rangle = \\langle Ax, Ax \\rangle = \\|Ax\\|_2^2 \\ge 0$.\n\nAccording to the spectral theorem, a self-adjoint operator on a finite-dimensional space has a complete orthonormal basis of eigenvectors with corresponding real eigenvalues. For a positive semidefinite operator such as $A^*A$, these eigenvalues are non-negative. Let the eigenvalues of $A^*A$ be $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$. The relationship between the singular values of $A$ and the eigenvalues of $A^*A$ is $\\sigma_i(A)^2 = \\lambda_i(A^*A)$.\n\nConsequently, the square of the spectral norm of $A$ is the largest eigenvalue of $A^*A$:\n$$\n\\|A\\|_2^2 = (\\sigma_{\\max}(A))^2 = \\lambda_{\\max}(A^*A)\n$$\nThis reduces the problem of finding $\\|A\\|_2$ to finding the square root of the largest eigenvalue of the operator $A^*A$.\n\n**2. Algorithm Design: Power Iteration**\n\nThe power iteration method is a standard algorithm for finding the eigenvalue of largest magnitude of an operator. When applied to a self-adjoint, positive semidefinite operator like $B = A^*A$, it converges to the largest eigenvalue, $\\lambda_{\\max}(B)$.\n\nThe algorithm proceeds as follows:\n1. Start with a non-zero initial vector $v_0 \\in \\mathbb{R}^n$. To avoid adversarial alignment with eigenspaces corresponding to smaller eigenvalues, $v_0$ is chosen randomly. It is then normalized to have unit norm, $\\|v_0\\|_2 = 1$.\n2. Iteratively generate a sequence of vectors using the relation:\n   $$\n   v_{k+1} = \\frac{B v_k}{\\|B v_k\\|_2} = \\frac{(A^*A) v_k}{\\| (A^*A) v_k \\|_2}\n   $$\nAs $k \\to \\infty$, the vector $v_k$ converges to the unit-norm eigenvector of $B$ corresponding to $\\lambda_{\\max}(B)$, provided $v_0$ has a non-zero component in the direction of this eigenvector (which is true with probability $1$ for a random $v_0$).\n\nThe crucial insight is that the application of $B = A^*A$ to a vector $v$ can be performed in two stages, using only the operators $A$ and $A^*$:\n- First, compute $u = A v$.\n- Second, compute $w = A^* u$.\nThe result is $w = A^*(Av) = (A^*A)v = Bv$. This avoids the need to explicitly form the operator $B$, which is often computationally expensive or infeasible in large-scale settings.\n\nThe corresponding largest eigenvalue $\\lambda_{\\max}(B)$ can be estimated from the iterates. Using the Rayleigh quotient, we have:\n$$\n\\lambda_k = \\langle (A^*A) v_k, v_k \\rangle = \\langle A v_k, A v_k \\rangle = \\|A v_k\\|_2^2\n$$\nAs $v_k$ converges to the dominant eigenvector, $\\lambda_k$ converges to $\\lambda_{\\max}(A^*A)$. Therefore, the sequence of scalars $s_k = \\|A v_k\\|_2$ converges to $\\sqrt{\\lambda_{\\max}(A^*A)} = \\|A\\|_2$. This provides a direct estimate for the spectral norm.\n\n**3. Final Algorithm Specification**\n\nBased on the principles above, the estimator is designed as follows:\n\n1.  **Initialization**:\n    - Select a random vector $v \\in \\mathbb{R}^n$ using a seeded pseudorandom number generator for reproducibility.\n    - Normalize the vector: $v \\leftarrow v / \\|v\\|_2$.\n    - Initialize the previous norm estimate $\\widehat{\\sigma}_{\\text{old}} = 0.0$.\n2.  **Iteration**: For $k = 1, \\dots, K$ (where $K$ is the maximum number of iterations):\n    a. Apply the operator $A$: $u = A(v)$.\n    b. Calculate the new norm estimate: $\\widehat{\\sigma}_{\\text{new}} = \\|u\\|_2$.\n    c. **Zero Operator Detection**: If $k=1$ and $\\widehat{\\sigma}_{\\text{new}} = 0$, the operator $A$ is the zero operator. Terminate and return $0.0$.\n    d. **Convergence Check**: If $\\widehat{\\sigma}_{\\text{new}} > 0$ and the relative change is below the tolerance $\\varepsilon$, i.e., $|\\widehat{\\sigma}_{\\text{new}} - \\widehat{\\sigma}_{\\text{old}}| < \\varepsilon \\cdot \\widehat{\\sigma}_{\\text{new}}$, the estimate has converged. Terminate and return $\\widehat{\\sigma}_{\\text{new}}$.\n    e. Update the previous estimate: $\\widehat{\\sigma}_{\\text{old}} = \\widehat{\\sigma}_{\\text{new}}$.\n    f. Apply the adjoint operator $A^*$: $w = A^*(u)$.\n    g. **Stability Check**: If $\\|w\\|_2 = 0$, the iteration has converged to the null space of $A^*A$. Terminate and return the current estimate $\\widehat{\\sigma}_{\\text{new}}$.\n    h. Normalize to obtain the next iterate: $v \\leftarrow w / \\|w\\|_2$.\n3.  **Termination**: If the loop completes after $K$ iterations without converging, return the final estimate $\\widehat{\\sigma}_{\\text{new}}$.\n\nThis design meets all problem requirements, providing a robust method to estimate the spectral norm using only operator and adjoint applications.",
            "answer": "```python\nimport numpy as np\nfrom scipy.fft import dct, idct\n\ndef power_iteration_estimator(A, A_star, n, K, epsilon, seed):\n    \"\"\"\n    Estimates the spectral norm of a linear operator A using power iteration.\n\n    Args:\n        A (callable): A function that applies the operator A to a vector.\n        A_star (callable): A function that applies the adjoint operator A* to a vector.\n        n (int): The dimension of the domain of A.\n        K (int): The maximum number of iterations.\n        epsilon (float): The tolerance for the stopping criterion.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        float: The estimated spectral norm of A.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    v = rng.standard_normal(n)\n    v_norm = np.linalg.norm(v)\n    if v_norm == 0.0:\n        # This is extremely unlikely but good practice to handle.\n        v = np.ones(n)\n        v_norm = np.linalg.norm(v)\n    v = v / v_norm\n\n    norm_est = 0.0\n    for k in range(K):\n        # Apply the operator A\n        u = A(v)\n        \n        # Calculate the new estimate for the spectral norm\n        norm_est_new = np.linalg.norm(u)\n\n        # Robustly detect the zero operator on the first iteration\n        if k == 0 and norm_est_new == 0.0:\n            return 0.0\n\n        # Check for convergence based on relative change\n        if norm_est_new > 0.0 and abs(norm_est_new - norm_est) < epsilon * norm_est_new:\n            return norm_est_new\n        \n        norm_est = norm_est_new\n\n        # Apply the adjoint operator A*\n        w = A_star(u)\n        norm_w = np.linalg.norm(w)\n\n        # If w is the zero vector, the algorithm has converged or hit a null space\n        if norm_w == 0.0:\n            return norm_est\n            \n        # Normalize to get the next iterate\n        v = w / norm_w\n    \n    return norm_est\n\ndef solve():\n    \"\"\"\n    Runs the test suite for the power iteration estimator.\n    \"\"\"\n    # Fixed parameters for the estimator\n    EPSILON = 1e-12\n    K = 1000\n    # Absolute tolerance for comparison with ground truth\n    DELTA = 1e-8\n\n    results = []\n\n    # --- Case 1: Dense Gaussian, rectangular ---\n    m1, n1, seed_G, seed_est_1 = 8, 5, 12345, 0\n    rng1 = np.random.default_rng(seed_G)\n    G = rng1.standard_normal((m1, n1))\n    A1 = lambda x: G @ x\n    A1_star = lambda y: G.T @ y\n    gt_1 = np.linalg.norm(G, 2)\n    est_1 = power_iteration_estimator(A1, A1_star, n1, K, EPSILON, seed_est_1)\n    results.append(abs(est_1 - gt_1) <= DELTA)\n\n    # --- Case 2: Zero operator ---\n    m2, n2, seed_est_2 = 3, 7, 1\n    A2 = lambda x: np.zeros(m2)\n    A2_star = lambda y: np.zeros(n2)\n    gt_2 = 0.0\n    est_2 = power_iteration_estimator(A2, A2_star, n2, K, EPSILON, seed_est_2)\n    results.append(abs(est_2 - gt_2) <= DELTA)\n\n    # --- Case 3: Subsampled orthonormal transform (DCT) ---\n    n3, seed_est_3 = 64, 2\n    S = np.arange(0, n3, 2, dtype=int)\n    A3 = lambda x: dct(x, type=2, norm='ortho')[S]\n    def A3_star(y):\n        z = np.zeros(n3)\n        z[S] = y\n        return idct(z, type=2, norm='ortho')\n    gt_3 = 1.0\n    est_3 = power_iteration_estimator(A3, A3_star, n3, K, EPSILON, seed_est_3)\n    results.append(abs(est_3 - gt_3) <= DELTA)\n\n    # --- Case 4: Diagonal operator ---\n    n4, seed_est_4 = 4, 3\n    d = np.array([0.1, 2.5, -3.7, 1.2])\n    A4 = lambda x: d * x\n    A4_star = A4  # Real diagonal operator is self-adjoint\n    gt_4 = np.max(np.abs(d))\n    est_4 = power_iteration_estimator(A4, A4_star, n4, K, EPSILON, seed_est_4)\n    results.append(abs(est_4 - gt_4) <= DELTA)\n\n    # --- Case 5: Scaled dense Gaussian ---\n    m5, n5, seed_H, seed_est_5 = 10, 4, 2023, 4\n    c = 7.3\n    rng5 = np.random.default_rng(seed_H)\n    H = rng5.standard_normal((m5, n5))\n    cH = c * H\n    A5 = lambda x: cH @ x\n    A5_star = lambda y: cH.T @ y\n    gt_5 = np.linalg.norm(cH, 2)\n    est_5 = power_iteration_estimator(A5, A5_star, n5, K, EPSILON, seed_est_5)\n    results.append(abs(est_5 - gt_5) <= DELTA)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}