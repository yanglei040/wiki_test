{
    "hands_on_practices": [
        {
            "introduction": "瑞利商（Rayleigh quotient）是连接二次型与矩阵特征值的基本工具。在稀疏优化的背景下，它为我们提供了一种量化测量算子 $A$ 在特定稀疏子空间上行为的方法。本练习将引导您从瑞利商的基本定义出发，推导出其上确界如何由格拉姆（Gram）子矩阵 $G_S = A_S^\\top A_S$ 的最大特征值 $\\lambda_{\\max}(G_S)$ 决定，并基于此构建一个有原则的支撑集选择规则。通过分析一个特殊构造的场景，您将深入理解谱属性与模型选择挑战之间的内在联系 。",
            "id": "3445867",
            "problem": "考虑一个测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$，其各列均为单位范数向量，因此 $A^{\\top} A$ 是一个对称半正定格拉姆矩阵。对于一个非零向量 $x \\in \\mathbb{R}^{n}$，定义瑞利商为 $R(x) = \\dfrac{x^{\\top} A^{\\top} A x}{\\|x\\|^{2}}$。在压缩感知和稀疏优化的背景下，假设我们希望通过在支撑集受限于一个基数为 $k$ 的候选索引集 $S \\subset \\{1,2,\\dots,n\\}$ ($|S| = k$) 的非零向量上最大化瑞利商，来选择一个基数为 $k$ 的支撑集。令 $A_{S}$ 表示由 $A$ 中索引集 $S$ 对应的列构成的子矩阵，并令 $G_{S} = A_{S}^{\\top} A_{S}$ 为其格拉姆子矩阵。\n\n从对称半正定矩阵的基本性质和谱定理出发，推导在所有支撑于给定集合 $S$ 上的非零向量 $x$ 上 $R(x)$ 的上确界（用 $G_{S}$ 表示），并据此阐明一个有原则的支撑集选择规则。\n\n然后，构建一个科学上合理的情景，在该情景下，根据此规则在两个不相交的候选支撑集之间进行选择变得无法区分。具体来说，假设存在两个不相交的支撑集 $S_{1}, S_{2} \\subset \\{1,2,\\dots,n\\}$，每个基数都为 $k$，满足以下条件：\n- 对于任意不同的 $i,j \\in S_{\\ell}$ (其中 $\\ell \\in \\{1,2\\}$)，对应列的内积满足 $\\langle A_{\\cdot,i}, A_{\\cdot,j} \\rangle = \\mu$，其中 $\\mu$ 是一个固定的值且 $\\mu \\in [0,1)$，并且 $\\|A_{\\cdot,i}\\| = \\|A_{\\cdot,j}\\| = 1$。\n- 对于任意 $i \\in S_{1}$ 和 $j \\in S_{2}$，内积满足 $\\langle A_{\\cdot,i}, A_{\\cdot,j} \\rangle = 0$。\n\n在这些假设下，$G_{S_{1}}$ 和 $G_{S_{2}}$ 都具有恒为 $1$ 的对角线元素和恒为 $\\mu$ 的非对角线元素。计算在所有支撑于 $S_{1}$ 或 $S_{2}$ 上的非零向量 $x$ 上 $R(x)$ 的上确界的精确值，以 $\\mu$ 和 $k$ 的函数形式给出闭式解。你的最终答案应该是一个关于 $\\mu$ 和 $k$ 的单一闭式解析表达式，且不进行任何舍入。",
            "solution": "用户提供了一个来自应用于稀疏优化和压缩感知的线性代数领域的有效且良态的问题陈述。我将进行完整的推导和解答。\n\n首先，我们分析在具有特定支撑集的向量上最大化瑞利商 $R(x)$ 的优化问题。设 $S \\subset \\{1, 2, \\dots, n\\}$ 是一个基数为 $|S|=k$ 的索引集。如果一个向量 $x \\in \\mathbb{R}^n$ 的非零项仅限于索引集 $S$ 中的索引，即对于所有 $j \\notin S$ 都有 $x_j = 0$，则称该向量 $x$ 支撑于 $S$。令 $x_S \\in \\mathbb{R}^k$ 为仅包含 $x$ 中由 $S$ 索引的元素的向量。那么 $x$ 的欧几里得范数的平方为 $\\|x\\|^2 = \\sum_{j=1}^n x_j^2 = \\sum_{j \\in S} x_j^2 = \\|x_S\\|^2$。\n\n瑞利商分子中的项 $A x$ 可以被简化。令 $A_S \\in \\mathbb{R}^{m \\times k}$ 是通过选择索引在 $S$ 中的列而构成的 $A$ 的子矩阵。那么，根据矩阵向量乘法的定义，有 $A x = A_S x_S$。\n将这些代入瑞利商的表达式中，我们得到：\n$$ R(x) = \\frac{x^{\\top} A^{\\top} A x}{\\|x\\|^2} = \\frac{(A x)^{\\top} (A x)}{\\|x_S\\|^2} = \\frac{(A_S x_S)^{\\top} (A_S x_S)}{\\|x_S\\|^2} = \\frac{x_S^{\\top} A_S^{\\top} A_S x_S}{\\|x_S\\|^2} $$\n矩阵 $G_S = A_S^{\\top} A_S$ 是与支撑集 $S$ 相关联的格拉姆子矩阵。该表达式变为矩阵 $G_S$ 和向量 $x_S \\in \\mathbb{R}^k$ 的瑞利商：\n$$ R(x) = \\frac{x_S^{\\top} G_S x_S}{\\|x_S\\|^2} $$\n问题在于求这个量在所有支撑于 $S$ 上的非零向量 $x$ 上的上确界，这等价于求在所有非零 $x_S \\in \\mathbb{R}^k$ 上的上确界。\n根据瑞利-里兹定理（或对称矩阵的谱定理），对称矩阵的瑞利商的上确界是其最大特征值。由于 $G_S$ 是一个格拉姆矩阵，它是对称且半正定的。因此，\n$$ \\sup_{x \\ne 0, \\, \\text{supp}(x) \\subseteq S} R(x) = \\sup_{x_S \\ne 0} \\frac{x_S^{\\top} G_S x_S}{\\|x_S\\|^2} = \\lambda_{\\max}(G_S) $$\n其中 $\\lambda_{\\max}(G_S)$ 表示矩阵 $G_S$ 的最大特征值。\n\n基于此推导，一个有原则的支撑集选择规则是选择基数为 $k$ 的支撑集 $S$，使其对应的格拉姆子矩阵的最大特征值最大化。也就是说，如果 $\\mathcal{S}_k$ 是所有大小为 $k$ 的可能支撑集的集合，则最优支撑集 $S^*$ 由下式给出：\n$$ S^* = \\arg\\max_{S \\in \\mathcal{S}_k} \\lambda_{\\max}(G_S) $$\n这个规则旨在寻找 $A$ 的列的一个子集，该子集在其格拉姆矩阵的最大特征值所捕捉的意义上是“能量最大”或“相关性最强”的。\n\n现在，我们来处理问题中描述的特定情景。我们给定两个不相交的支撑集 $S_1$ 和 $S_2$，每个基数均为 $k$。对于每个支撑集 $S_{\\ell}$ (其中 $\\ell \\in \\{1, 2\\}$)，对应的格拉姆子矩阵 $G_{S_{\\ell}}$ 是一个 $k \\times k$ 矩阵。$G_{S_{\\ell}}$ 的元素由 $(G_{S_{\\ell}})_{ij} = \\langle A_{\\cdot,i}, A_{\\cdot,j} \\rangle$ 给出。\n根据问题陈述：\n- 对角线元素为 $(G_{S_{\\ell}})_{ii} = \\|A_{\\cdot,i}\\|^2 = 1^2 = 1$。\n- 对于不同的 $i, j \\in S_{\\ell}$，非对角线元素为 $(G_{S_{\\ell}})_{ij} = \\langle A_{\\cdot,i}, A_{\\cdot,j} \\rangle = \\mu$。\n\n因此，$G_{S_1}$ 和 $G_{S_2}$ 具有相同的结构。我们用 $G$ 表示这个通用的 $k \\times k$ 矩阵：\n$$ G = \\begin{pmatrix} 1  \\mu  \\cdots  \\mu \\\\ \\mu  1  \\cdots  \\mu \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ \\mu  \\mu  \\cdots  1 \\end{pmatrix} $$\n这个矩阵可以紧凑地写成 $G = (1-\\mu)I_k + \\mu J_k$，其中 $I_k$ 是 $k \\times k$ 的单位矩阵，$J_k$ 是 $k \\times k$ 的全1矩阵。\n\n为了找到 $\\lambda_{\\max}(G)$，我们计算 $G$ 的特征值。$G$ 的特征值与 $J_k$ 的特征值直接相关。矩阵 $J_k$ 的秩为1。\n- 全1向量 $\\mathbf{1} \\in \\mathbb{R}^k$ 是 $J_k$ 的一个特征向量，其特征值为 $k$，因为 $J_k\\mathbf{1} = k\\mathbf{1}$。\n- 对应于特征值 $0$ 的特征空间是 $J_k$ 的零空间，即与 $\\mathbf{1}$ 正交的所有向量的集合。这个空间的维数是 $k-1$。\n\n如果 $v$ 是 $J_k$ 的一个特征向量，其特征值为 $\\lambda_J$，那么它也是 $G$ 的一个特征向量：\n$$ Gv = ((1-\\mu)I_k + \\mu J_k)v = (1-\\mu)I_k v + \\mu J_k v = (1-\\mu)v + \\mu(\\lambda_J v) = (1-\\mu + \\mu\\lambda_J)v $$\n所以，$G$ 的特征值为 $\\lambda_G = 1-\\mu+\\mu\\lambda_J$。\n\n利用 $J_k$ 的特征值，我们求出 $G$ 的特征值：\n1. 对于特征值 $\\lambda_J = k$（特征向量为 $\\mathbf{1}$），$G$ 对应的特征值为：\n   $$ \\lambda_1 = 1-\\mu+\\mu k = 1 + (k-1)\\mu $$\n2. 对于特征值 $\\lambda_J = 0$（重数为 $k-1$），$G$ 对应的特征值为：\n   $$ \\lambda_2 = 1-\\mu+\\mu(0) = 1-\\mu $$\n这个特征值的重数为 $k-1$。\n\n$G$ 的特征值集合是 $\\{1 + (k-1)\\mu, 1-\\mu \\text{ (重数 } k-1)\\}$。我们必须找出其中最大的一个。我们比较 $\\lambda_1$ 和 $\\lambda_2$。\n差值为 $\\lambda_1 - \\lambda_2 = (1 + (k-1)\\mu) - (1-\\mu) = k\\mu$。\n问题陈述中说明 $k$ 是一个基数，所以 $k \\ge 1$。它还说明了 $\\mu \\in [0, 1)$。\n- 如果 $\\mu=0$，那么 $\\lambda_1 - \\lambda_2 = 0$，所以 $\\lambda_1 = \\lambda_2 = 1$。\n- 如果 $\\mu > 0$，并且由于 $k \\ge 1$，那么 $k\\mu > 0$，这意味着 $\\lambda_1 > \\lambda_2$。\n\n在所有 $\\mu \\in [0, 1)$ 的情况下，我们都有 $\\lambda_1 \\ge \\lambda_2$。因此，最大特征值是 $\\lambda_{\\max}(G) = \\lambda_1 = 1 + (k-1)\\mu$。\n\n该情景的构建使得根据推导出的规则，在支撑集 $S_1$ 和 $S_2$ 之间的选择是无法区分的，因为 $\\lambda_{\\max}(G_{S_1}) = \\lambda_{\\max}(G_{S_2})$。对于任一支撑集，其上确界的值都是这个共同的最大特征值。\n\n在所有支撑于 $S_1$ 上的非零向量 $x$ 上 $R(x)$ 的上确界是 $\\lambda_{\\max}(G_{S_1}) = 1 + (k-1)\\mu$。\n在所有支撑于 $S_2$ 上的非零向量 $x$ 上 $R(x)$ 的上确界是 $\\lambda_{\\max}(G_{S_2}) = 1 + (k-1)\\mu$。\n问题要求的是在所有支撑于 $S_1$ *或* $S_2$ 上的非零向量 $x$ 上 $R(x)$ 的上确界，这在支撑集选择问题的背景下，意味着一个 $k$-稀疏向量在这两个支撑集中的任意一个上可实现的最大值。由于两者产生的最大值相同，因此该值即为答案。\n\n最终值为 $1 + (k-1)\\mu$。",
            "answer": "$$\n\\boxed{1 + (k-1)\\mu}\n$$"
        },
        {
            "introduction": "在前一个练习的基础上，我们知道 $\\lambda_{\\max}(G_S)$ 至关重要，但许多算法的稳定性更依赖于 $G_S$ 的整个谱分布。本练习聚焦于谱条件数 $\\kappa_2(G_S) = \\lambda_{\\max}(G_S) / \\lambda_{\\min}(G_S)$，它直接影响着稀疏恢复流程中一个常见步骤——最小二乘去偏（least-squares debiasing）的数值稳定性和噪声敏感性。通过对一种典型相关性结构进行精确计算，您将量化列相关性如何影响算法的性能，并将抽象的谱理论与实际的算法鲁棒性联系起来 。",
            "id": "3445779",
            "problem": "在稀疏恢复流程中，通常在初步的支持集选择（例如，通过最小绝对收缩和选择算子（LASSO））之后执行支持集限制的去偏（debiasing）操作。考虑一个传感矩阵，其列向量均为单位范数，并设 $S$ 是一个大小为 $s=4$ 的选定支持集。假设由 $S$ 索引的列是等相关的：每个非对角线上的内积都等于一个公共的相干水平 $\\mu \\in [0,1)$。因此，其格拉姆（Gram）矩阵为\n$$\nG \\equiv A_{S}^{\\top}A_{S} = (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top},\n$$\n其中 $I_{4}$ 是 $4 \\times 4$ 的单位矩阵，$\\mathbf{1}$ 表示 4 维的全一向量。仅使用线性代数的核心定义和半正定矩阵的性质，计算谱条件数 $\\kappa_{2}(G)$，其定义为 $G$ 的最大特征值与最小特征值之比，并以 $\\mu$ 的函数形式给出其闭式解。然后，从基本原理出发，论证所计算出的 $\\kappa_{2}(G)$ 对于在加性高斯白噪声（AWGN）下，在支持集 $S$ 上进行的支持集限制的最小二乘去偏的数值精度和噪声敏感性意味着什么，并与标准正交情况进行比较。最终答案必须是您用 $\\mu$ 表示的 $\\kappa_{2}(G)$ 的闭式表达式。",
            "solution": "问题要求计算一个特定格拉姆矩阵的谱条件数，并解释其在稀疏恢复背景下的含义。我将首先验证问题的陈述。问题提供了一个定义明确的数学对象，即矩阵 $G = (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top}$（其中 $\\mu \\in [0,1)$），并要求计算一个标准的、定义明确的量，即谱条件数 $\\kappa_{2}(G)$。稀疏恢复和最小二乘去偏的背景在科学上是合理的，并且是该领域的标准内容。所有术语都定义清晰，不存在矛盾或信息缺失。因此，该问题是有效的。\n\n为了计算谱条件数 $\\kappa_{2}(G)$，我们必须找到 $G$ 的最大特征值与最小特征值之比。我们直接从核心定义 $G\\mathbf{v} = \\lambda\\mathbf{v}$ 出发寻找 $G$ 的特征值，其中 $\\lambda$ 是一个特征值，$\\mathbf{v}$ 是其对应的非零特征向量。\n\n给定的矩阵是 $G = (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top}$，其中 $I_{4}$ 是 $4 \\times 4$ 的单位矩阵，$\\mathbf{1}$ 是 $4 \\times 1$ 的全一向量。特征值方程为：\n$$\n\\left( (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top} \\right) \\mathbf{v} = \\lambda \\mathbf{v}\n$$\n将各项应用于向量 $\\mathbf{v}$：\n$$\n(1-\\mu)\\mathbf{v} + \\mu (\\mathbf{1}^{\\top}\\mathbf{v})\\mathbf{1} = \\lambda\\mathbf{v}\n$$\n重新整理各项，我们得到：\n$$\n\\mu (\\mathbf{1}^{\\top}\\mathbf{v})\\mathbf{1} = (\\lambda - (1-\\mu))\\mathbf{v}\n$$\n$$\n\\mu (\\mathbf{1}^{\\top}\\mathbf{v})\\mathbf{1} = (\\lambda - 1 + \\mu)\\mathbf{v}\n$$\n我们针对特征向量 $\\mathbf{v}$ 的两种不同情况来分析这个方程。\n\n情况 1：特征向量 $\\mathbf{v}$ 与全一向量 $\\mathbf{1}$ 正交。\n在这种情况下，内积 $\\mathbf{1}^{\\top}\\mathbf{v} = 0$。方程的左侧变为零向量：\n$$\n\\mathbf{0} = (\\lambda - 1 + \\mu)\\mathbf{v}\n$$\n由于 $\\mathbf{v}$ 是一个特征向量，它必须是非零的（$\\mathbf{v} \\neq \\mathbf{0}$）。因此，标量乘数必须为零：\n$$\n\\lambda - 1 + \\mu = 0 \\implies \\lambda = 1 - \\mu\n$$\n在 $\\mathbb{R}^4$ 中与 $\\mathbf{1}$ 正交的向量空间是一个维度为 $4-1=3$ 的子空间。因此，存在 3 个与特征值 $\\lambda = 1 - \\mu$ 相关联的线性无关的特征向量。该特征值的重数为 3。\n\n情况 2：特征向量 $\\mathbf{v}$ 不与 $\\mathbf{1}$ 正交。\n从方程 $\\mu (\\mathbf{1}^{\\top}\\mathbf{v})\\mathbf{1} = (\\lambda - 1 + \\mu)\\mathbf{v}$ 中，我们可以看到，如果等式两边的量都非零，那么 $\\mathbf{v}$ 必须是 $\\mathbf{1}$ 的标量倍。我们来检验向量 $\\mathbf{v} = \\mathbf{1}$ 本身。\n内积为 $\\mathbf{1}^{\\top}\\mathbf{v} = \\mathbf{1}^{\\top}\\mathbf{1} = 1^2 + 1^2 + 1^2 + 1^2 = 4$。\n将 $\\mathbf{v}=\\mathbf{1}$ 代入特征值方程：\n$$\nG\\mathbf{1} = \\left( (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top} \\right) \\mathbf{1} = (1-\\mu)\\mathbf{1} + \\mu\\mathbf{1}(\\mathbf{1}^{\\top}\\mathbf{1}) = (1-\\mu)\\mathbf{1} + \\mu\\mathbf{1}(4)\n$$\n$$\nG\\mathbf{1} = (1-\\mu+4\\mu)\\mathbf{1} = (1+3\\mu)\\mathbf{1}\n$$\n这表明 $\\mathbf{1}$ 是一个特征向量，其对应的特征值为 $\\lambda = 1+3\\mu$。\n\n矩阵 $G$ 是一个 $4 \\times 4$ 矩阵，所以它有 4 个特征值。我们已经找到了所有这些特征值：\n$$\n\\lambda_{1} = 1+3\\mu \\quad (\\text{重数 } 1)\n$$\n$$\n\\lambda_{2,3,4} = 1-\\mu \\quad (\\text{重数 } 3)\n$$\n问题规定 $\\mu \\in [0, 1)$。对于此区间内的任何 $\\mu$，$1+3\\mu \\geq 1$ 且 $1-\\mu > 0$。因此，所有特征值都严格为正，这证实了 $G$ 是一个正定矩阵。\n\n为了求谱条件数 $\\kappa_2(G)$，我们需要确定最大和最小特征值 $\\lambda_{\\max}(G)$ 和 $\\lambda_{\\min}(G)$。我们比较两个不同的特征值表达式：$1+3\\mu$ 和 $1-\\mu$。\n对于 $\\mu \\in [0,1)$，我们有 $3\\mu \\geq 0$ 和 $-\\mu \\leq 0$。因此，$1+3\\mu \\geq 1$ 和 $1-\\mu \\leq 1$。\n更直接地说，对于 $\\mu \\geq 0$，$3\\mu \\geq -\\mu$，这意味着 $1+3\\mu \\geq 1-\\mu$。仅当 $\\mu=0$ 时等号成立。\n所以，对于 $\\mu \\in [0, 1)$：\n$$\n\\lambda_{\\max}(G) = 1+3\\mu\n$$\n$$\n\\lambda_{\\min}(G) = 1-\\mu\n$$\n谱条件数定义为它们的比率：\n$$\n\\kappa_{2}(G) = \\frac{\\lambda_{\\max}(G)}{\\lambda_{\\min}(G)} = \\frac{1+3\\mu}{1-\\mu}\n$$\n\n现在，我们必须论证这个计算出的 $\\kappa_{2}(G)$ 对支持集限制的最小二乘去偏问题意味着什么。去偏步骤涉及求解限制在支持集 $S$ 上的标准最小二乘问题：\n$$\n\\min_{\\mathbf{x}_S} \\|\\mathbf{y} - A_S \\mathbf{x}_S\\|_2^2\n$$\n其中 $A_S$ 是由 $S$ 索引的 $A$ 的列组成的矩阵。解由正规方程给出：\n$$\n(A_S^{\\top}A_S)\\mathbf{x}_S = A_S^{\\top}\\mathbf{y} \\implies G\\mathbf{x}_S = A_S^{\\top}\\mathbf{y}\n$$\n这个线性系统的性质由条件数 $\\kappa_{2}(G)$ 决定。\n\n标准正交情况对应于 $A_S$ 的列是标准正交的，这意味着 $G = A_S^{\\top}A_S = I_4$。这等同于在我们的模型中设置 $\\mu=0$。在这种情况下，$\\lambda_{\\max}(G) = 1$ 且 $\\lambda_{\\min}(G)=1$，所以 $\\kappa_{2}(G) = \\frac{1+0}{1-0} = 1$。条件数为 1 是最优的、可能的最小值，表示一个完全良态的问题。\n\n随着相干性 $\\mu$ 从 0 增加到 1，条件数 $\\kappa_{2}(G) = \\frac{1+3\\mu}{1-\\mu}$ 单调增加。当 $\\mu \\to 1$ 时，$\\kappa_{2}(G) \\to \\infty$。大的条件数表明矩阵 $G$ 是病态的，意味着它接近于奇异。与标准正交情况相比，这有两个关键影响：\n\n1.  **数值精度：** 当使用有限精度算术（例如，在计算机上）求解系统 $G\\mathbf{x}_S = A_S^{\\top}\\mathbf{y}$ 时，计算解中的相对误差以 $\\kappa_2(G)$ 为界。较大的 $\\kappa_{2}(G)$ 意味着微小的浮点表示误差会被放大，导致数值计算出的解可能与真实的数学解有显著差异。随着 $\\mu$ 的增加，去偏步骤在数值上变得不稳定，结果也变得不那么可靠。\n\n2.  **噪声敏感性：** 在一个带有加性高斯白噪声（AWGN）的模型 $\\mathbf{y} = A_S\\mathbf{x}_{S,\\text{true}} + \\mathbf{n}$ 中，最小二乘估计为 $\\mathbf{x}_S = \\mathbf{x}_{S,\\text{true}} + G^{-1}A_S^{\\top}\\mathbf{n}$。误差项 $\\mathbf{e} = G^{-1}A_S^{\\top}\\mathbf{n}$ 是测量噪声 $\\mathbf{n}$ 通过系统传播的结果。这种噪声的放大程度由矩阵 $G^{-1}$ 决定。$G^{-1}$ 的特征值是 $G$ 的特征值的倒数，具体为 $1/\\lambda_{\\min}(G) = 1/(1-\\mu)$ 和 $1/\\lambda_{\\max}(G) = 1/(1+3\\mu)$。大的 $\\kappa_2(G)$ 意味着 $\\lambda_{\\min}(G)$ 非常小。因此，$G^{-1}$ 的最大特征值非常大。这意味着与对应于 $\\lambda_{\\min}(G)$ 的特征向量对齐的噪声分量被急剧放大。因此，高相干性 $\\mu$ 会导致高条件数，这反过来又导致对测量噪声的极端敏感性，使得去偏估计 $\\mathbf{x}_S$ 与在标准正交（$\\mu=0$）情况下获得的稳健、抗噪声的估计相比，变得高度不准确和不稳定。\n\n总而言之，$\\kappa_{2}(G)$ 的值可以作为一种精确的度量，衡量随着所选支持集上的传感矩阵列向量变得更加相关，最小二乘去偏过程在数值稳定性和统计效率（噪声放大）方面的退化程度。",
            "answer": "$$\n\\boxed{\\frac{1+3\\mu}{1-\\mu}}\n$$"
        },
        {
            "introduction": "理论分析常依赖于具有清晰结构的理想化矩阵，但现实世界中的矩阵往往更为复杂，需要借助数值方法来分析其谱特性。本练习将带领您从解析计算过渡到数值实践，实现瑞利商迭代（Rayleigh Quotient Iteration）算法来近似计算格拉姆子矩阵的最小特征值 $\\lambda_{\\min}(G_S)$。这个值直接决定了在给定支撑集上的稀疏信号是否能够被唯一且稳定地恢复，从而将理论上的可恢复性条件转化为可计算和可验证的指标 。",
            "id": "3445866",
            "problem": "给定一个感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和一个支撑集 $S \\subseteq \\{0,1,\\dots,n-1\\}$，该支撑集选择了一部分列。对于由 $A$ 中索引为 $S$ 的列构成的子矩阵 $A_S \\in \\mathbb{R}^{m \\times |S|}$，考虑其格拉姆矩阵 $G_S = A_S^{\\top} A_S$，该矩阵是对称半正定的。其最小特征值 $\\lambda_{\\min}(G_S)$ 等于 $A_S$ 的最小奇异值的平方。在压缩感知和稀疏优化中，当支撑集已知或被假设时，$\\lambda_{\\min}(G_S)$ 的性质对于重构的唯一性和稳定性至关重要。\n\n从以下基本原理出发：\n- 对于任意非零向量 $x \\in \\mathbb{R}^{|S|}$，瑞利商定义为 $R(x) = \\dfrac{x^{\\top} G_S x}{x^{\\top} x}$。\n- 对于对称矩阵 $G_S$， $R(x)$ 在 $x \\neq 0$ 上的最小值恰好是 $\\lambda_{\\min}(G_S)$，该最小值在与 $\\lambda_{\\min}(G_S)$ 相关联的 $G_S$ 的特征向量处取得。\n- 对称半正定矩阵的所有特征值均为非负，而 $G_S = A_S^{\\top} A_S$ 根据其构造是半正定的。\n\n任务：使用基于瑞利商的迭代法，实现 $\\lambda_{\\min}(G_S)$ 的数值近似。使用带瑞利商更新的反迭代法来近似最小特征值和特征向量，并为可能奇异或近奇异的半正定矩阵设置适当的保障措施。然后将该近似值与重构条件相关联，并计算以下量：\n1. 通过您的瑞利商迭代法获得的 $\\lambda_{\\min}(G_S)$ 的数值近似值。\n2. 通过直接对称特征分解计算出的精确 $\\lambda_{\\min}(G_S)$，作为基准真实值。\n3. 一个布尔值，指示在无噪声情况下，支撑集 $S$ 上的重构是否唯一，定义为 $A_S$ 具有满列秩。使用判据 $\\lambda_{\\min}(G_S) > \\tau$，容差 $\\tau = 10^{-8}$。\n4. 一个稳定性因子 $c_S$，定义为 $c_S = 1 / \\sigma_{\\min}(A_S) = 1 / \\sqrt{\\lambda_{\\min}(G_S)}$，其中较大的 $c_S$ 表示在约束于 $S$ 的最小二乘重构中噪声放大更严重。\n5. 当 $A$ 的所有列都被归一化为单位 $\\ell_2$ 范数时，子集限制等距偏差 $\\delta_S$ 定义为 $\\delta_S = \\max_{i} \\left| \\lambda_i(G_S) - 1 \\right|$。这量化了 $A_S$ 在其列所张成的子空间上偏离等距同构的程度。\n6. 一个布尔值，指示经典的类限制等距检查是否在子集上通过，定义为 $\\delta_S < \\sqrt{2} - 1$。\n\n构建并评估以下测试套件。在所有情况下，在形成 $A_S$ 和 $G_S$ 之前，将 $A$ 的每一列归一化为单位 $\\ell_2$ 范数。\n\n- 测试用例 1（通用、随机、良态）：\n  - 维度：$m = 8$，$n = 10$。\n  - 随机种子：$0$。\n  - 构造：从独立标准正态分布中抽取 $A$ 的元素，然后将每列归一化为单位 $\\ell_2$ 范数。\n  - 支撑集：$S = [0, 3, 7]$。\n\n- 测试用例 2（近共線性导致的近奇异性）：\n  - 维度：$m = 6$，$n = 8$。\n  - 随机种子：$1$。\n  - 构造：\n    - 生成一个具有独立标准正态分布元素的随机基础矩阵。\n    - 通过归一化一个随机列来形成一个单位向量 $u \\in \\mathbb{R}^{m}$。\n    - 生成一个随机向量 $v \\in \\mathbb{R}^{m}$，通过 $v \\leftarrow v - (u^{\\top} v) u$ 对其进行相对于 $u$ 的正交化，然后归一化 $v$。\n    - 将 $A$ 的第 $0$ 列设置为 $u$，将 $A$ 的第 $1$ 列设置为 $u + \\varepsilon v$ 的归一化形式，其中 $\\varepsilon = 10^{-3}$，将 $A$ 的第 $2$ 列设置为一个随机向量的归一化形式，其他列保持为随机。最后将 $A$ 的所有列归一化为单位 $\\ell_2$ 范数。\n  - 支撑集：$S = [0, 1, 2]$。\n\n- 测试用例 3（单列子集）：\n  - 维度：$m = 5$，$n = 7$。\n  - 随机种子：$123$。\n  - 构造：从独立标准正态分布中抽取 $A$ 的元素，然后将每列归一化为单位 $\\ell_2$ 范数。\n  - 支撑集：$S = [5]$。\n\n- 测试用例 4（病态但满秩）：\n  - 维度：$m = 6$，$n = 8$。\n  - 随机种子：$42$。\n  - 构造：\n    - 生成一个具有独立标准正态分布元素的随机基础矩阵。\n    - 归一化 $A$ 的所有列。\n    - 修改第 $7$ 列，使其为 $A[:, 1] + \\eta r$ 的归一化形式，其中 $r$ 是一个新的随机向量，$\\eta = 10^{-2}$，然后重新归一化所有列。\n  - 支撑集：$S = [1, 4, 6, 7]$。\n\n您的程序应为每个测试用例输出一个列表，其中包含：\n- 近似的 $\\lambda_{\\min}(G_S)$（浮点数），\n- 精确的 $\\lambda_{\\min}(G_S)$（浮点数），\n- 唯一性布尔值，\n- 稳定性因子 $c_S$（浮点数），\n- 偏差 $\\delta_S$（浮点数），\n- 限制等距检查布尔值。\n\n您的程序应生成单行输出，其中包含所有测试用例的结果列表，格式为逗号分隔并用方括号括起来，例如 $$[ [r_1], [r_2], [r_3], [r_4] ]$$ 其中每个 $[r_i]$ 本身是包含上述六个项目的列表。",
            "solution": "该解决方案的设计基于数值线性代数的原理及其在压缩感知中的应用。问题的核心是分析与子矩阵 $A_S$ 相关的格拉姆矩阵 $G_S = A_S^\\top A_S$。最小特征值 $\\lambda_{\\min}(G_S)$ 至关重要，因为它决定了在已知支撑集 $S$ 上稀疏重构问题的条件数。\n\n**$\\lambda_{\\min}(G_S)$ 的近似：**\n问题要求使用“带瑞利商更新的反迭代法”，这是一种更广为人知的强大算法，即瑞利商迭代法（RQI）。对于对称矩阵 $G_S$ 和一个近似的特征向量 $x_k$，RQI 包括两个步骤：\n1. 计算瑞利商：$\\mu_k = \\frac{x_k^\\top G_S x_k}{x_k^\\top x_k}$。对于给定的 $x_k$，这是最佳的特征值估计。\n2. 通过求解 $(G_S - \\mu_k I) y_{k+1} = x_k$ 并归一化来更新特征向量估计：$x_{k+1} = y_{k+1} / \\|y_{k+1}\\|_2$。此步骤是反迭代法的一种形式，其中移位量 $\\mu_k$ 是动态更新的。\n\nRQI 表现出三次收敛速度，但其能否成功找到特定特征值（本例中为最小特征值）取决于初始向量 $x_0$。一个朴素的随机起点可能导致收敛到任意一个特征值。为了可靠地定位 $\\lambda_{\\min}(G_S)$，需要一个更鲁棒的初始化方法。我们采用一个预处理步骤，在一个轻微移位的矩阵 $(G_S + \\epsilon I)$（其中 $\\epsilon$ 是一个小的正数）上使用简单的反幂迭代法。应用几次求解 $(G_S + \\epsilon I)y = x$ 的迭代会将向量 $x$ 引导向与 $G_S$ 的最接近 $-\\epsilon$ 的特征值（即 $\\lambda_{\\min}(G_S)$）相对应的特征向量。这个精炼后的向量随后作为 RQI 的高质量起始点，确保收敛到期望的特征值。\n\nRQI 的一个关键方面是处理线性系统求解。当 $\\mu_k$ 接近一个真实的特征值时，矩阵 $(G_S - \\mu_k I)$ 会变得病态或奇异。这是该算法的一个特性，而非缺陷；解向量 $y_{k+1}$ 的巨大模长正是加速收敛的原因。我们的实现将捕获由此奇异性引起的 `numpy.linalg.LinAlgError`，并正确地将其解释为收敛的标志，从而终止迭代。\n\n**相关指标的计算：**\n一旦确定了 $\\lambda_{\\min}(G_S)$，我们计算几个与压缩感知理论相关的核心量：\n\n1. **基准真实特征值：** 我们使用一个标准的、高度优化的对称特征求解器（`numpy.linalg.eigh`）来计算 $G_S$ 的所有特征值，以获得 $\\lambda_{\\min}(G_S)$ 的精确参考值并用于计算 $\\delta_S$。\n2. **重构的唯一性：** 对于无噪声测量 $y=Ax$，如果真实的稀疏向量 $x$ 的支撑集为 $S$，那么 $x$ 的唯一可重构性（在已知 $S$ 的情况下）等价于能够唯一求解 $y = A_S x_S$。这要求 $A_S$ 具有满列秩，这当且仅当 $G_S=A_S^\\top A_S$ 可逆时成立，即 $\\lambda_{\\min}(G_S) > 0$。我们使用数值容差 $\\tau = 10^{-8}$ 来检验此条件。\n3. **稳定性因子 ($c_S$)：** 在有噪声的情况下，最小二乘解的稳定性取决于 $A_S$ 的条件数。一个关键度量是 $c_S = 1/\\sigma_{\\min}(A_S)$，其中 $\\sigma_{\\min}(A_S)$ 是 $A_S$ 的最小奇异值。利用恒等式 $\\lambda_{\\min}(G_S) = \\sigma_{\\min}(A_S)^2$，我们得到 $c_S = 1/\\sqrt{\\lambda_{\\min}(G_S)}$。一个大的 $c_S$ 表示对噪声高度敏感。如果 $\\lambda_{\\min}(G_S)=0$，该因子为无穷大。\n4. **子集限制等距偏差 ($\\delta_S$)：** 限制等距性质（RIP）是压缩感知理论的基石。虽然计算真实的 RIP 常数是 NP-难的，但我们可以针对特定子集 $S$ 检验一个相关性质。如果 $A$ 的所有列都是单位归一化的，若 $A_S$ 是一个等距同构，则算子 $A_S^\\top A_S$ 在其定义域上表现为单位算子。偏差 $\\delta_S = \\max_i |\\lambda_i(G_S) - 1|$ 量化了 $G_S$ 与单位矩阵 $I$ 的差异程度。\n5. **经典的类 RIP 检查：** 许多重构保证的一个充分（但不必要）条件是，对于 $2k$-稀疏信号，$\\delta_{2k} < \\sqrt{2}-1$。我们将此检查应用于我们的特定子集 $S$，评估是否 $\\delta_S < \\sqrt{2}-1$。\n\n整个过程对一套测试用例进行自动化处理，每个用例旨在探究问题的不同方面，从良态的随机矩阵到特意构造的病态情况。使用固定的随机种子确保了所有构造和结果的可复现性。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import lu_factor, lu_solve\n\ndef approximate_lambda_min(G, rng, num_init_iter=10, num_rqi_iter=15):\n    \"\"\"\n    Approximates the smallest eigenvalue of a symmetric positive semidefinite matrix G\n    using inverse iteration followed by Rayleigh Quotient Iteration (RQI).\n    \n    Args:\n        G (np.ndarray): The symmetric positive semidefinite matrix.\n        rng (np.random.Generator): A random number generator.\n        num_init_iter (int): Number of iterations for the initial inverse power method.\n        num_rqi_iter (int): Number of iterations for RQI.\n\n    Returns:\n        float: The approximated smallest eigenvalue.\n    \"\"\"\n    k = G.shape[0]\n    if k == 0:\n        return np.nan\n    if k == 1:\n        return G[0, 0]\n\n    # Initialize a random vector\n    x = rng.standard_normal(size=k)\n    x /= np.linalg.norm(x)\n\n    # 1. Initial phase: Shifted inverse iteration to find a good starting vector.\n    # This guides the vector towards the eigenvector of the smallest eigenvalue.\n    # The shift makes the system well-conditioned.\n    shift = 1e-12\n    G_shifted = G + shift * np.identity(k)\n    try:\n        # Pre-compute LU factorization for efficiency in the loop\n        lu, piv = lu_factor(G_shifted)\n        for _ in range(num_init_iter):\n            y = lu_solve((lu, piv), x)\n            norm_y = np.linalg.norm(y)\n            if norm_y  1e-20:\n                # Vector collapsed; restart with new random vector\n                x = rng.standard_normal(size=k)\n                x /= np.linalg.norm(x)\n                lu, piv = lu_factor(G + shift * np.identity(k)) # Recalculate LU\n                continue\n            x = y / norm_y\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if LU fails (extremely unlikely with the shift)\n        G_pinv = np.linalg.pinv(G_shifted)\n        for _ in range(num_init_iter):\n            y = G_pinv @ x\n            norm_y = np.linalg.norm(y)\n            if norm_y  1e-20:\n                x = rng.standard_normal(size=k)\n                x /= np.linalg.norm(x)\n                continue\n            x = y / norm_y\n\n    # 2. Refinement phase: Rayleigh Quotient Iteration\n    lambda_approx = np.dot(x, G @ x)\n\n    for _ in range(num_rqi_iter):\n        M = G - lambda_approx * np.identity(k)\n        try:\n            y = np.linalg.solve(M, x)\n        except np.linalg.LinAlgError:\n            # Expected condition: M is singular, so lambda_approx is close to an eigenvalue.\n            return lambda_approx\n        \n        norm_y = np.linalg.norm(y)\n        if norm_y  1e-20:\n            return lambda_approx\n        x = y / norm_y\n        lambda_approx = np.dot(x, G @ x)\n        \n    return lambda_approx\n\ndef run_test_case(m, n, seed, S, A_construction_func):\n    \"\"\"\n    Sets up and runs a single test case, computing all required metrics.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    A = A_construction_func(m, n, rng)\n\n    A_S = A[:, S]\n    G_S = A_S.T @ A_S\n\n    # 1. Approximate lambda_min using RQI\n    approx_lambda_min = approximate_lambda_min(G_S, rng)\n\n    # 2. Exact eigenvalues for ground truth and other metrics\n    if G_S.shape[0] == 0:\n        all_eigenvalues = np.array([])\n        exact_lambda_min = np.nan\n    else:\n\n        all_eigenvalues = np.linalg.eigh(G_S)[0]\n        exact_lambda_min = all_eigenvalues[0] if len(all_eigenvalues) > 0 else np.nan\n\n\n    # 3. Uniqueness boolean\n    tau = 1e-8\n    is_unique = exact_lambda_min > tau if not np.isnan(exact_lambda_min) else False\n\n    # 4. Stability factor\n    if not np.isnan(exact_lambda_min) and exact_lambda_min > 1e-15:\n        stability_factor_c_S = 1.0 / np.sqrt(exact_lambda_min)\n    else:\n        stability_factor_c_S = np.inf\n        \n    # 5. Restricted Isometry Deviation\n    if G_S.shape[0] > 0:\n        rip_deviation_delta_S = np.max(np.abs(all_eigenvalues - 1.0))\n    else:\n        rip_deviation_delta_S = 0.0\n\n    # 6. Restricted Isometry Check\n    rip_check = rip_deviation_delta_S  (np.sqrt(2.0) - 1.0)\n\n    return [\n        approx_lambda_min,\n        exact_lambda_min,\n        is_unique,\n        stability_factor_c_S,\n        rip_deviation_delta_S,\n        rip_check,\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and print results for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"m\": 8, \"n\": 10, \"seed\": 0, \"S\": [0, 3, 7],\n            \"construct\": lambda m, n, rng: (\n                lambda A: A / np.linalg.norm(A, axis=0)\n            )(rng.standard_normal(size=(m, n)))\n        },\n        {\n            \"m\": 6, \"n\": 8, \"seed\": 1, \"S\": [0, 1, 2],\n            \"construct\": lambda m, n, rng: (\n                lambda A: (\n                    A.__setitem__((slice(None), 0), (lambda u_rand: u_rand / np.linalg.norm(u_rand))(rng.standard_normal(size=m))),\n                    A.__setitem__((slice(None), 1), (lambda u, v, eps: (u + eps * v) / np.linalg.norm(u + eps * v))(\n                        A[:, 0], \n                        (lambda u, v_rand: (v_rand - np.dot(u, v_rand) * u) / np.linalg.norm(v_rand - np.dot(u, v_rand) * u))(A[:, 0], rng.standard_normal(size=m)),\n                        1e-3\n                    )),\n                    A.__setitem__((slice(None), 2), (lambda r: r / np.linalg.norm(r))(rng.standard_normal(size=m))),\n                    A / np.linalg.norm(A, axis=0)\n                )[-1]\n            )(rng.standard_normal(size=(m, n)))\n        },\n        {\n            \"m\": 5, \"n\": 7, \"seed\": 123, \"S\": [5],\n            \"construct\": lambda m, n, rng: (\n                lambda A: A / np.linalg.norm(A, axis=0)\n            )(rng.standard_normal(size=(m, n)))\n        },\n        {\n            \"m\": 6, \"n\": 8, \"seed\": 42, \"S\": [1, 4, 6, 7],\n            \"construct\": lambda m, n, rng: (\n                lambda A: (\n                    A.__setitem__((slice(None), 7), (lambda col: col / np.linalg.norm(col))(A[:, 1] + 1e-2 * rng.standard_normal(size=m))),\n                    A / np.linalg.norm(A, axis=0)\n                )[-1]\n            )((lambda B: B / np.linalg.norm(B, axis=0))(rng.standard_normal(size=(m, n))))\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case[\"m\"], case[\"n\"], case[\"seed\"], case[\"S\"], case[\"construct\"])\n        results.append(result)\n\n    # Format output as a string representing a list of lists.\n    # The template `f\"[{','.join(map(str, results))}]\"` handles this structure correctly.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}