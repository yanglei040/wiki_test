{
    "hands_on_practices": [
        {
            "introduction": "在探索复杂的恢复算法之前，理解维度所施加的根本性限制至关重要。本练习  探讨了仅凭简单的计数论证即可断定恢复必然失败的情形，为我们提供了关于测量数量 $m$ 为何是关键资源的清晰直觉。理解这些“不可能”的结果，有助于我们界定压缩感知能够成功应用的范围。",
            "id": "3486680",
            "problem": "考虑一个线性测量模型，其感知矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，信号为 $x \\in \\mathbb{R}^{n}$。如果一个向量 $x$ 满足 $\\|x\\|_{0} \\le k$，则称其为 $k$-稀疏的。测量可以是无噪声的，$y = A x$，也可以是有噪声的，$y = A x + w$，其中 $w \\in \\mathbb{R}^{m}$。假设 $A$ 的列是 $\\ell_{2}$-归一化的（即，每列的欧几里得范数为 $1$）。$A$ 的互相关性为 $\\mu(A) = \\max_{i \\ne j} |\\langle a_{i}, a_{j} \\rangle|$，其中 $a_{i}$ 表示 $A$ 的第 $i$ 列。$A$ 的 spark 定义为 $\\operatorname{spark}(A) = \\min\\{ \\|z\\|_{0} : z \\ne 0, A z = 0 \\}$。如果存在一个常数 $L  \\infty$，使得对于所有满足 $\\|x\\|_{0} \\le k$ 的 $x$ 和所有 $w \\in \\mathbb{R}^{m}$，都有 $\\|D(Ax + w) - x\\|_{2} \\le L \\|w\\|_{2}$，则称解码器 $D: \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$ 在 $k$-稀疏集上是一致稳定的。\n\n选择下面所有必然为真的陈述，并仅根据基本定义和经过充分检验的事实给出简短的理由。\n\nA. 如果 $m  2k$，那么对于每一个 $A \\in \\mathbb{R}^{m \\times n}$，都存在不同的 $k$-稀疏向量 $x_{1}, x_{2} \\in \\mathbb{R}^{n}$，使得 $A x_{1} = A x_{2}$。\n\nB. 在无噪声模型 $y = A x$ 中，要能（通过某个算法对任何 $A$）精确恢复所有 $k$-稀疏向量，一个必要条件是 $m \\ge C\\, k \\log(n/k)$，其中 $C > 0$ 是某个绝对常数。\n\nC. 如果 $\\mu(A) \\ge 0.9$，那么精确恢复所有 $1$-稀疏向量是不可能的。\n\nD. 假设存在一个解码器 $D$，它在 $k$-稀疏集上是一致稳定的，即对于所有满足 $\\|x\\|_{0} \\le k$ 的 $x$ 和所有 $w \\in \\mathbb{R}^{m}$，都有 $\\|D(Ax + w) - x\\|_{2} \\le L \\|w\\|_{2}$，其中常数 $L$ 与 $n$ 无关。那么对于所有足够大的 $n$，存在一个通用常数 $C > 0$，使得必然有 $m \\ge C\\, k \\log(n/k)$。\n\n选择所有正确的选项。",
            "solution": "问题陈述的有效性已得到确认。所提供的定义和概念是压缩感知和数学信号处理领域的标准内容。该问题提法得当，具有科学依据。\n\n我们现在开始分析每个陈述。\n\n**陈述 A 分析：**\n陈述是：“如果 $m  2k$，那么对于每一个 $A \\in \\mathbb{R}^{m \\times n}$，都存在不同的 $k$-稀疏向量 $x_{1}, x_{2} \\in \\mathbb{R}^{n}$，使得 $A x_{1} = A x_{2}$。”\n\n该陈述关注的是由矩阵 $A$ 定义的线性映射在限制于 $k$-稀疏向量集合上的单射性。等式 $A x_{1} = A x_{2}$ 等价于 $A(x_{1} - x_{2}) = 0$。令 $z = x_{1} - x_{2}$。由于 $x_1$ 和 $x_2$ 是不同的，所以 $z$ 是一个非零向量。\n\n$z$ 的支撑集包含在 $x_1$ 和 $x_2$ 支撑集的并集中。由于 $x_1$ 和 $x_2$ 都是 $k$-稀疏的，它们各自最多有 $k$ 个非零项。因此，$z$ 的非零项数量，即其 $\\ell_0$-范数，有界于 $\\|z\\|_{0} \\le \\|x_1\\|_{0} + \\|x_2\\|_{0} \\le k + k = 2k$。\n\n因此，该陈述等价于断言，如果 $m  2k$，对于任何矩阵 $A \\in \\mathbb{R}^{m \\times n}$，都存在一个非零向量 $z$，其满足 $\\|z\\|_{0} \\le 2k$ 且位于 $A$ 的零空间中。\n\n这与矩阵 $A$ 的 spark 相关，其定义为 $\\operatorname{spark}(A) = \\min\\{ \\|z\\|_{0} : z \\ne 0, A z = 0 \\}$。唯一恢复所有 $k$-稀疏向量的条件是，对于任何不同的 $k$-稀疏向量 $x_1, x_2$，我们有 $Ax_1 \\neq Ax_2$。这等价于 $A$ 的零空间中不存在非零向量 $z = x_1 - x_2$（该向量至多是 $2k$-稀疏的）。换句话说，当且仅当 $\\operatorname{spark}(A) > 2k$ 时，才能保证唯一恢复所有 $k$-稀疏信号。\n\n现在让我们将 spark 与矩阵的维度联系起来。$A$ 的列是 $\\mathbb{R}^{m}$ 中的向量。在 $m$ 维空间中，任何 $m+1$ 个向量的集合都必须是线性相关的。因此，$A$ 中任意 $m+1$ 列的集合都是线性相关的。这意味着存在一个非零向量 $z$，其最多有 $m+1$ 个非零项，使得 $Az=0$。根据 spark 的定义，这意味着 $\\operatorname{spark}(A) \\le m+1$。\n\n问题假设 $m  2k$。这可以写成 $m \\le 2k-1$。这意味着 $m+1 \\le 2k$。\n将此与 spark 的一般性质结合起来，我们得到 $\\operatorname{spark}(A) \\le m+1 \\le 2k$。\n这证明了总是存在一个非零向量 $z \\in \\ker(A)$，其满足 $\\|z\\|_0 \\le 2k$。\n\n给定这样一个向量 $z$，我们可以构造两个不同的 $k$-稀疏向量 $x_1, x_2$ 使得 $z = x_1 - x_2$。令 $S = \\operatorname{supp}(z)$，其中 $|S| = \\|z\\|_0 \\le 2k$。我们可以将 $S$ 划分为两个不相交的子集 $S_1$ 和 $S_2$，使得 $|S_1| \\le k$ 和 $|S_2| \\le k$。定义 $x_1$，使其在 $S_1$ 上的元素与 $z$ 的元素相等，在其他位置为零。定义 $x_2 = x_1 - z$。那么 $\\|x_1\\|_0 = |S_1| \\le k$。$x_2$ 的支撑集是 $S_2$，所以 $\\|x_2\\|_0 = |S_2| \\le k$。$x_1$ 和 $x_2$ 都是 $k$-稀疏的。由于 $z \\ne 0$，所以 $x_1 \\ne x_2$。根据构造，有 $A x_1 - A x_2 = A(x_1 - x_2) = A z = 0$。\n\n因此，如果 $m  2k$，就保证存在非唯一解。\n\n结论：**正确**。\n\n**陈述 B 分析：**\n陈述是：“在无噪声模型 $y = A x$ 中，要能（通过某个算法对任何 $A$）精确恢复所有 $k$-稀疏向量，一个必要条件是 $m \\ge C\\, k \\log(n/k)$，其中 $C > 0$ 是某个绝对常数。”\n\n该陈述为精确恢复提出了测量数 $m$ 的一个下界。短语“对任何 $A$”必须解释为适用于任何能够实现这种恢复的矩阵 $A$。\n通过*某个*（可能不切实际的）算法对所有 $k$-稀疏向量进行精确恢复是可能的，当且仅当映射 $x \\mapsto Ax$ 在 $k$-稀疏向量集上是单射的。正如在陈述 A 的分析中所确立的，这等价于条件 $\\operatorname{spark}(A) > 2k$。\n\n由此导出的一个必要条件是 $\\operatorname{spark}(A) \\le m+1$，这意味着 $2k  m+1$，即 $m \\ge 2k$。\n\n问题在于，更强的条件 $m \\ge C k \\log(n/k)$ 是否是必要的。这个对数界通常与*稳定*恢复或具有受限等距性质（RIP）的矩阵相关联。众所周知，可以构造出仅保证精确恢复（即具有足够的 spark）但不一定保证稳定恢复的矩阵，而其测量数更少。\n\n例如，利用编码理论的原理，可以为非常大的 $n$ 构造出矩阵 $A \\in \\mathbb{R}^{m \\times n}$，其中 $m = 2k$ 且 $\\operatorname{spark}(A) = 2k+1$。这样的矩阵保证了对所有 $k$-稀疏向量的唯一恢复。然而，对于这个矩阵，$m=2k$。如果 $n$ 选择得足够大，$C k \\log(n/k)$ 可以变得比 $2k$ 大得多（例如，如果 $C\\log(n/k) > 2$）。\n因此，对于这样的矩阵，精确恢复是可能的，但条件 $m \\ge C k \\log(n/k)$ 被违反了。这表明，对数界通常不是精确恢复的必要条件，尽管它对于像 RIP 和一致稳定性这样的更强性质是必要的。\n\n结论：**不正确**。\n\n**陈述 C 分析：**\n陈述是：“如果 $\\mu(A) \\ge 0.9$，那么精确恢复所有 $1$-稀疏向量是不可能的。”\n\n一个向量是 $1$-稀疏的，如果它最多有一个非零项，即对于某个标量 $c$ 和索引 $i$，$x = c e_i$。测量值为 $y = A(c e_i) = c a_i$，其中 $a_i$ 是 $A$ 的第 $i$ 列。\n精确恢复所有 $1$-稀疏向量要求，对于任意两个不同的 $1$-稀疏向量 $x_1 = c_1 e_i$ 和 $x_2 = c_2 e_j$（其中 $i \\ne j$ 且 $c_1, c_2 \\ne 0$），我们有 $A x_1 \\ne A x_2$。\n这意味着对于所有 $i \\ne j$ 和非零标量 $c_1, c_2$，有 $c_1 a_i \\ne c_2 a_j$。这等价于要求任意两列 $a_i$ 和 $a_j$ 都不共线。\n由于列是 $\\ell_2$-归一化的（$\\|a_i\\|_2 = 1$），共线性将意味着 $a_i = \\pm a_j$。\n互相关性为 $\\mu(A) = \\max_{i \\ne j} |\\langle a_i, a_j \\rangle|$。如果 $a_i = \\pm a_j$，则 $|\\langle a_i, a_j \\rangle| = |\\pm \\langle a_j, a_j \\rangle| = \\|a_j\\|_2^2 = 1$。\n因此，精确恢复所有 $1$-稀疏向量的充分必要条件是 $\\mu(A)  1$。\n\n该陈述声称如果 $\\mu(A) \\ge 0.9$，恢复是不可能的。这是错误的。如果 $\\mu(A) = 0.9$（这满足 $\\ge 0.9$），我们有 $\\mu(A)  1$，所以恢复是完全可能的。例如，一个解码器可以在给定 $y$ 的情况下，找到与 $y$ 最相关的列 $a_i$，并设置 $x = \\langle y, a_i \\rangle e_i$。由于没有两列是共线的，这将唯一地识别出正确的索引 $i$。\n\n结论：**不正确**。\n\n**陈述 D 分析：**\n陈述是：“假设存在一个解码器 $D$，它在 $k$-稀疏集上是一致稳定的，即对于所有满足 $\\|x\\|_{0} \\le k$ 的 $x$ 和所有 $w \\in \\mathbb{R}^{m}$，都有 $\\|D(Ax + w) - x\\|_{2} \\le L \\|w\\|_{2}$，其中常数 $L$ 与 $n$ 无关。那么对于所有足够大的 $n$，存在一个通用常数 $C > 0$，使得必然有 $m \\ge C\\, k \\log(n/k)$。”\n\n一致稳定性的条件是一个强条件。让我们分析它的后果。\n首先，通过取 $w=0$，我们得到 $\\|D(Ax) - x\\|_2 \\le 0$，这意味着对于所有 $k$-稀疏向量 $x$，$D(Ax) = x$。所以，一致稳定性意味着在无噪声情况下的精确恢复。\n\n现在考虑两个不同的 $k$-稀疏向量 $x_1$ 和 $x_2$。令 $w = A(x_2 - x_1)$。用这个“噪声” $w$ 扰动后的 $x_1$ 的测量值为 $y = Ax_1 + w = Ax_1 + A(x_2 - x_1) = Ax_2$。\n将稳定性不等式应用于信号 $x_1$ 和噪声 $w$：\n$$ \\| D(Ax_1 + w) - x_1 \\|_2 \\le L \\|w\\|_2 $$\n代入 $Ax_1 + w = Ax_2$ 和 $w=A(x_2-x_1)$：\n$$ \\| D(Ax_2) - x_1 \\|_2 \\le L \\|A(x_2-x_1)\\|_2 $$\n由于 $D$ 为 $k$-稀疏向量提供精确恢复，所以 $D(Ax_2) = x_2$。因此：\n$$ \\| x_2 - x_1 \\|_2 \\le L \\|A(x_2 - x_1)\\|_2 $$\n令 $z = x_2 - x_1$。向量 $z$ 是非零的，并且至多是 $2k$-稀疏的。不等式变为：\n$$ \\|z\\|_2 \\le L \\|Az\\|_2 \\quad \\text{或} \\quad \\|Az\\|_2 \\ge \\frac{1}{L} \\|z\\|_2 $$\n这必须对任何作为两个 $k$-稀疏向量之差的向量 $z$ 成立，即对任何 $2k$-稀疏向量 $z$ 成立。这是矩阵 $A$ 在 $2k$-稀疏向量集合上的一个受限等距性质（RIP）下界。\n\n在高维几何和压缩感知中，一个基本结果是，任何满足这样一个 RIP 下界的矩阵 $A \\in \\mathbb{R}^{m \\times n}$，其行数 $m$ 必须至少按 $k \\log(n/k)$ 的比例缩放。更正式地说，如果对于所有 $s$-稀疏向量 $z$ 都有 $\\|Az\\|_2 \\ge c \\|z\\|_2$，那么必然有 $m \\ge C' c^2 s \\log(n/s)$，其中 $C'$ 是某个通用常数。\n将这个结果应用于 $s=2k$ 和 $c=1/L$，我们得出结论，必然有\n$$ m \\ge C' (1/L)^2 (2k) \\log(n/(2k)) $$\n对于 $n \\gg k$，这具有 $m \\ge C k \\log(n/k)$ 的形式，其中 $C$ 是一个依赖于 $C'$ 和 $L$ 的常数。由于题目说明 $L$ 与 $n$ 无关，因此 $C$ 是一个通用常数。因此，该陈述是从一致稳定性的前提出发得出的正确结论。\n\n结论：**正确**。",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "信号恢复的成功与失败之间的界限并非模糊不清；对于许多模型而言，这是一个急剧的“相变”。本练习  将带您直面这一现象的核心，要求您应用统计维度的前沿理论来预测一个具体高维问题的恢复结果。通过计算给定的系统参数是落在“成功”区域还是“失败”区域，您将亲身体会到这些理论工具是如何精确地量化维度灾难的。",
            "id": "3486736",
            "problem": "考虑线性逆模型 $y = A x_{\\star}$，其中 $A \\in \\mathbb{R}^{m \\times d}$ 的元素是独立同分布的标准正态随机变量，$x_{\\star} \\in \\mathbb{R}^{d}$ 是一个精确 $k$-稀疏向量，其大小为 $k$ 的支撑集是均匀随机选择的，非零项的符号是独立且对称的，以及 $y \\in \\mathbb{R}^{m}$。恢复方法为基追踪（basis pursuit），即求解 $\\min_{x \\in \\mathbb{R}^{d}} \\|x\\|_{1}$，约束条件为 $A x = y$。在高维极限下，$\\ell_{1}$ 恢复的成功或失败表现出一种渐近相变，该相变由下降锥的几何结构和统计维度决定，后者通过归一化采样率来量化维度灾难。\n\n请仅从基本定义和经过充分检验的事实出发，包括凸函数的下降锥几何、锥的统计维度以及 Gordon 关于高斯算子的比较/逃逸原理，推导出一个可计算的条件，通过将采样率与一个由 $k$-稀疏点上 $\\ell_{1}$ 范数的下降锥的统计维度决定的临界值进行比较，来预测在给定 $(m,d,k)$ 的情况下恢复是成功还是失败。然后，对于 $m = 320$，$d = 1000$，$k = 40$ 的具体实例，使用您推导出的条件来确定与这些参数对应的点 $(\\delta,\\rho)$ 是位于高斯测量下 $\\ell_{1}$ 恢复的渐近相变之上还是之下，并据此预测成功或失败。\n\n答案规格：\n- 如果在高维意义下，$\\ell_{1}$ 恢复被预测以压倒性概率成功，则最终答案为 $1$；如果预测失败，则为 $0$。\n- 最终答案必须是如上所述的单个数字。不要提供不等式或句子作为最终答案。",
            "solution": "该问题陈述的有效性已确认。这是一个在压缩感知和高维几何领域中定义良好、有科学依据的问题。该问题要求推导成功信号恢复的条件，并将其应用于一个具体的数值实例。\n\n### 第一部分：可计算条件的推导\n\n该问题涉及从 $m$ 个线性测量 $y = Ax_{\\star} \\in \\mathbb{R}^{m}$ 中恢复一个 $k$-稀疏信号 $x_{\\star} \\in \\mathbb{R}^{d}$，其中 $A \\in \\mathbb{R}^{m \\times d}$ 是一个具有独立同分布（i.i.d.）标准正态元素的矩阵。恢复通过基追踪进行，即求解 $\\ell_{1}$-最小化问题：\n$$ \\min_{x \\in \\mathbb{R}^{d}} \\|x\\|_{1} \\quad \\text{subject to} \\quad Ax = y $$\n原始信号 $x_{\\star}$ 是此规划的一个可行点，因为 $Ax_{\\star}=y$。如果 $x_{\\star}$ 是*唯一*的最小化子，则恢复被认为是成功的。\n\n解的唯一性是一个几何问题。令 $f(x) = \\|x\\|_1$。$x_{\\star}$ 是唯一最小化子的一个充要条件是，对于 $A$ 的零空间中任意非零向量 $v$（即 $v \\in \\mathrm{null}(A) \\setminus \\{0\\}$），$\\ell_1$ 范数在 $v$ 的方向上必须严格增加，即 $\\|x_{\\star} + v\\|_1 > \\|x_{\\star}\\|_1$。\n\n这个条件可以用下降锥的概念来重新表述。一个凸函数 $f$ 在点 $x_{\\star}$ 处的下降锥，记作 $\\mathcal{D}(f, x_{\\star})$，是从 $x_{\\star}$ 出发，函数 $f$ 值不增加的所有方向的集合。对于一个支撑集为 $T = \\{i : (x_{\\star})_i \\neq 0\\}$ 的 $k$-稀疏向量 $x_{\\star}$，其 $\\ell_1$ 范数的下降锥由下式给出：\n$$ \\mathcal{D}(\\|\\cdot\\|_1, x_{\\star}) = \\{ v \\in \\mathbb{R}^d : \\|(x_{\\star})_T + v_T\\|_1 + \\|v_{T^c}\\|_1 \\le \\|(x_{\\star})_T\\|_1 \\} $$\n其中 $v_T$ 是 $v$ 在支撑集 $T$ 上的部分，$v_{T^c}$ 是在补集 $T^c$ 上的部分。对于一个一般的稀疏向量和任意方向 $v$，这个锥可以很好地被更简单的锥 $C_T = \\{ v \\in \\mathbb{R}^d : \\|v_{T^c}\\|_1 \\le \\|v_T\\|_1 \\}$ 近似。\n\n成功恢复的条件现在可以简明地陈述为：$A$ 的零空间必须与 $x_{\\star}$ 处的下降锥只有平凡交集。\n$$ \\mathrm{null}(A) \\cap \\mathcal{D}(\\|\\cdot\\|_1, x_{\\star}) = \\{0\\} $$\n矩阵 $A$ 具有独立同分布的高斯元素，这意味着它的零空间 $\\mathrm{null}(A)$ 是 $\\mathbb{R}^d$ 中一个维度为 $d-m$ 的均匀随机子空间。问题因此变为：一个维度为 $d-m$ 的随机子空间与一个固定的锥 $\\mathcal{D}(\\|\\cdot\\|_1, x_{\\star})$ 相交的概率是多少？\n\n高维几何理论，特别是 Gordon 关于高斯过程的比较不等式，为这个问题提供了一个精确的答案。对于一个给定的锥 $C$，它与一个随机子空间的相交情况由该锥的统计维度 $\\delta(C)$ 决定，其定义为标准高斯向量 $g \\sim \\mathcal{N}(0, I_d)$ 在该锥上投影的欧几里得范数平方的期望值：\n$$ \\delta(C) = \\mathbb{E}_{g \\sim \\mathcal{N}(0, I_d)} [\\|\\text{proj}_C(g)\\|_2^2] $$\n该理论的核心结果，通常被称为运动学公式或 Gordon 的逃离网格原理，指出在高维设定下，相交事件表现出一种相变。如果 $d-m + \\delta(C) > d$，一个维度为 $d-m$ 的随机子空间将以高概率与锥 $C$ 相交；如果 $d-m + \\delta(C)  d$，它将以高概率不与 $C$ 相交（除了原点）。\n\n将此应用于我们的问题，随机子空间 $\\mathrm{null}(A)$ 的维度是 $d-m$。如果 $\\mathrm{null}(A)$ 与下降锥 $\\mathcal{D}(\\|\\cdot\\|_1, x_{\\star})$ 发生非平凡相交，则恢复失败。这被预测在以下情况下发生：\n$$ (d-m) + \\delta(\\mathcal{D}(\\|\\cdot\\|_1, x_{\\star})) > d \\implies \\delta(\\mathcal{D}(\\|\\cdot\\|_1, x_{\\star})) > m $$\n反之，成功恢复被预测在以下情况下发生：\n$$ \\delta(\\mathcal{D}(\\|\\cdot\\|_1, x_{\\star}))  m $$\n量 $\\delta(\\mathcal{D}(\\|\\cdot\\|_1, x_{\\star}))$ 是所需的临界测量数。\n\n在高维极限下，当 $d \\to \\infty$ 而比率 $\\delta = m/d$ 和 $\\rho = k/d$ 保持不变时，归一化统计维度 $\\delta(\\mathcal{D}(\\|\\cdot\\|_1, x_{\\star}))/d$ 收敛到一个仅依赖于稀疏度 $\\rho$ 的确定性函数。我们将此函数称为 $\\delta_{\\text{crit}}(\\rho)$。成功条件随后变为采样率 $\\delta$ 与此临界值的直接比较：\n$$ \\delta > \\delta_{\\text{crit}}(\\rho) $$\n这就是成功的可计算条件。函数 $\\delta_{\\text{crit}}(\\rho)$ 已在文献中从无序系统的统计力学分析和随机矩阵理论中推导出来。对于实值高斯测量模型，$\\delta_{\\text{crit}}(\\rho)$ 由参数形式给出。对于参数 $t \\ge 0$，相变边界上的点对 $(\\rho(t), \\delta_{\\text{crit}}(t))$ 由下式给出：\n$$ \\frac{\\rho(t)}{1-\\rho(t)} = t\\phi(t) - t^2\\Phi(-t) $$\n$$ \\delta_{\\text{crit}}(t) = 1 - (1-\\rho(t))\\left( 1 - t\\phi(t) + (t^2-1)\\Phi(-t) \\right) $$\n其中 $\\phi(t)$ 是标准正态随机变量的概率密度函数，$\\Phi(t)$ 是其累积分布函数。\n\n### 第二部分：具体实例应用\n\n我们给定的参数是 $m = 320$，$d = 1000$，$k = 40$。相应的归一化比率为：\n$$ \\delta = \\frac{m}{d} = \\frac{320}{1000} = 0.32 $$\n$$ \\rho = \\frac{k}{d} = \\frac{40}{1000} = 0.04 $$\n要确定恢复是否成功，我们必须计算临界采样率 $\\delta_{\\text{crit}}(0.04)$ 并检查是否 $\\delta > \\delta_{\\text{crit}}(0.04)$。\n\n首先，我们通过求解第一个参数方程来找到与 $\\rho = 0.04$ 对应的参数 $t$：\n$$ \\frac{0.04}{1-0.04} = \\frac{0.04}{0.96} = \\frac{1}{24} $$\n因此，我们需要求解关于 $t$ 的超越方程：\n$$ t\\phi(t) - t^2\\Phi(-t) = \\frac{1}{24} \\approx 0.04167 $$\n该方程有两个正根。物理上相关的解分支是当 $\\rho \\to 0$ 时 $\\delta_{\\text{crit}} \\to 0$ 的那个，它对应于 $t$ 的较大根。数值计算得出 $t \\approx 1.51$。\n\n现在，我们将 $t=1.51$ 和 $\\rho=0.04$ 代入第二个参数方程来求 $\\delta_{\\text{crit}}$：\n$$ \\delta_{\\text{crit}} = 1 - (1-0.04)\\left( 1 - 1.51\\phi(1.51) + (1.51^2-1)\\Phi(-1.51) \\right) $$\n使用高斯函数的标准值：$\\phi(1.51) \\approx 0.1276$ 和 $\\Phi(-1.51) \\approx 0.0655$。\n$$ \\delta_{\\text{crit}} \\approx 1 - 0.96 \\left( 1 - 1.51 \\times 0.1276 + (2.2801 - 1) \\times 0.0655 \\right) $$\n$$ \\delta_{\\text{crit}} \\approx 1 - 0.96 \\left( 1 - 0.1927 + 1.2801 \\times 0.0655 \\right) $$\n$$ \\delta_{\\text{crit}} \\approx 1 - 0.96 \\left( 1 - 0.1927 + 0.0838 \\right) $$\n$$ \\delta_{\\text{crit}} \\approx 1 - 0.96 \\left( 0.8911 \\right) $$\n$$ \\delta_{\\text{crit}} \\approx 1 - 0.8555 = 0.1445 $$\n对于稀疏度 $\\rho=0.04$，临界采样率约为 $\\delta_{\\text{crit}} \\approx 0.1445$。\n\n最后，我们将我们的实际采样率 $\\delta = 0.32$ 与这个临界值进行比较：\n$$ 0.32 > 0.1445 $$\n由于实际采样率 $\\delta$ 远大于临界阈值 $\\delta_{\\text{crit}}(\\rho)$，点 $(\\rho, \\delta) = (0.04, 0.32)$ 位于成功恢复区域的深处。因此，预测 $\\ell_1$ 恢复将以压倒性概率成功。",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "对于无结构的问题，维度灾难的影响最为严峻。最后的这个练习  将展示克服它的最有力策略：利用关于信号结构的先验知识。通过一个计算性练习，您将量化从通用稀疏模型转换到更具约束性的树状结构模型时，所需测量数量的显著减少，从而生动地展示“结构之福”的原理。",
            "id": "3486799",
            "problem": "您需要形式化地阐述基于模型的树状结构稀疏性如何通过减少一致性恢复所需的测量次数（相较于非结构化稀疏性），从而缓解压缩感知中的维度灾难问题。工作在一个环境维度为$n$、信号为$k$-稀疏的空间中，并考虑随机亚高斯测量矩阵。您必须使用的基本理论是子空间并集观点以及针对低维集合的亚高斯嵌入的标准测度集中理论：要以至多$\\delta \\in (0,1)$的失真嵌入单个$k$维子空间，需要大约$m = \\mathcal{O}(\\delta^{-2} k)$次测量；要一致性地嵌入$L$个此类子空间的并集，一个标准的网格与并集界论证会将所需测量次数增加一个$\\log L$量级的项。这一原则被广泛接受，并应作为出发点。\n\n基于此，您必须推导、论证并实现两种模型的显式测量复杂度估计器：\n\n- 非结构化$k$-稀疏性：该模型是$\\mathbb{R}^n$中由单位矩阵任意$k$列所张成的所有坐标子空间的并集。其模型基数为$L_{\\mathrm{un}} = \\binom{n}{k}$。\n\n- 基于有根$d$叉树的树状结构$k$-稀疏性：一个支撑集是指嵌入到一个具有$n$个节点的大的完全有根$d$叉树中的任意一个大小为$k$的连通有根子树。此类支撑集数量的一个上界可以通过计算形状和嵌入方式得到。一个经过充分验证的组合学事实是，具有$k$个节点的有根有序$d$叉树的形状数量为\n$$\nT_d(k) = \\frac{1}{((d-1)k+1)} \\binom{d k}{k}.\n$$\n一个保守且标准的放置界是，嵌入的支撑集数量不超过$n \\cdot T_d(k)$，当然也不能超过$\\binom{n}{k}$。因此，结构化模型的基数可以由下式界定\n$$\nL_{\\mathrm{str}} \\leq \\min\\!\\left\\{ n \\cdot T_d(k), \\binom{n}{k} \\right\\}.\n$$\n\n使用子空间并集嵌入原理以及上述基数，推导出以下形式的估计器\n$$\nm \\approx \\left\\lceil c_0 \\, \\delta^{-2} \\, \\big( k + \\log L \\big) \\right\\rceil,\n$$\n其中$c_0 > 0$是一个固定的绝对常数，$\\log$是自然对数，$L$是模型基数。对于非结构化模型，应用$L = L_{\\mathrm{un}}$；对于树状结构模型，应用$L = L_{\\mathrm{str}}$。您必须完全按照所写的方式实现这个估计器，使用自然对数，并在需要时通过对数伽马函数以数值稳定的方式计算$\\log \\binom{a}{b}$和$\\log T_d(k)$。您可以假设$n$相对于$k$足够大，以使嵌入计数界有意义。\n\n您的程序必须为下面的测试套件中的每一组参数计算整数对$[ m_{\\mathrm{un}}, m_{\\mathrm{str}} ]$，其中$m_{\\mathrm{un}}$是非结构化估计值，$m_{\\mathrm{str}}$是结构化估计值。所有对数都必须是自然对数，最终答案是通过应用向上取整函数得到的整数。不涉及物理单位。不使用角度。不使用百分比。\n\n测试套件（每个元组为$(n,k,d,\\delta,c_0)$）：\n- 案例1（理想情况，大环境维度）：$(n,k,d,\\delta,c_0) = (16384, 64, 2, 0.25, 2.0)$。\n- 案例2（更高分支因子）：$(n,k,d,\\delta,c_0) = (4096, 32, 4, 0.25, 2.0)$。\n- 案例3（边界情况$k=1$）：$(n,k,d,\\delta,c_0) = (2048, 1, 2, 0.20, 2.0)$。\n- 案例4（中等大小和分支）：$(n,k,d,\\delta,c_0) = (8192, 16, 3, 0.30, 2.0)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个案例的结果，格式为逗号分隔的列表的列表，每个内部列表为$[ m_{\\mathrm{un}}, m_{\\mathrm{str}} ]$。例如，输出应具有以下形式：\n\"[ [m_un_case1,m_str_case1],[m_un_case2,m_str_case2],[m_un_case3,m_str_case3],[m_un_case4,m_str_case4] ]\"\n不移除空格或添加额外文本。使用您的实现所计算出的精确整数值。",
            "solution": "首先对问题陈述进行严格的验证过程。\n\n### 步骤1：提取给定条件\n提供的显式数据、变量和条件如下：\n- **环境维度 (Ambient Dimension)**：$n$\n- **稀疏度 (Sparsity Level)**：$k$\n- **测量矩阵类型 (Measurement Matrix Type)**：随机亚高斯\n- **理论框架 (Theoretical Framework)**：压缩感知的子空间并集观点。\n- **基本原理 (Base Principle)**：对于失真度为$\\delta \\in (0,1)$，一致性嵌入$L$个$k$维子空间的并集需要$m = \\mathcal{O}(\\delta^{-2}(k + \\log L))$次测量。\n- **估计器公式 (Estimator Formula)**：$m \\approx \\left\\lceil c_0 \\, \\delta^{-2} \\, \\big( k + \\log L \\big) \\right\\rceil$，其中$\\log$是自然对数，$c_0 > 0$是一个常数。\n- **非结构化稀疏模型 (Unstructured Sparsity Model)**：所有维度为$k$的坐标子空间的并集。\n- **非结构化模型的基数 (Cardinality of Unstructured Model)**：$L_{\\mathrm{un}} = \\binom{n}{k}$。\n- **树状结构稀疏模型 (Tree-Structured Sparsity Model)**：支撑集是一个在一个具有$n$个节点的完全有根$d$叉树中的大小为$k$的连通有根子树。\n- **树形数量 (Number of Tree Shapes)**：$T_d(k) = \\frac{1}{((d-1)k+1)} \\binom{d k}{k}$。\n- **结构化模型的基数界 (Cardinality Bound for Structured Model)**：$L_{\\mathrm{str}} \\leq \\min\\!\\left\\{ n \\cdot T_d(k), \\binom{n}{k} \\right\\}$。此界将用于估计器中的基数$L$。\n- **数值计算 (Numerical Computation)**：$\\log \\binom{a}{b}$和$\\log T_d(k)$需使用对数伽马函数计算。\n- **任务 (Task)**：为每个测试案例计算整数对$[ m_{\\mathrm{un}}, m_{\\mathrm{str}} ]$。\n- **测试套件 (Test Suite)**：\n    - 案例1：$(n,k,d,\\delta,c_0) = (16384, 64, 2, 0.25, 2.0)$\n    - 案例2：$(n,k,d,\\delta,c_0) = (4096, 32, 4, 0.25, 2.0)$\n    - 案例3：$(n,k,d,\\delta,c_0) = (2048, 1, 2, 0.20, 2.0)$\n    - 案例4：$(n,k,d,\\delta,c_0) = (8192, 16, 3, 0.30, 2.0)$\n\n### 步骤2：使用提取的给定条件进行验证\n根据既定标准对问题进行有效性评估。\n- **科学基础 (Scientifically Grounded)**：该问题牢固地植根于压缩感知和稀疏信号恢复理论。子空间并集模型、亚高斯随机矩阵的使用以及形式为$m \\propto k + \\log L$的测量界是该领域中标准且成熟的结论。模型基数的组合公式是正确的。\n- **适定性 (Well-Posed)**：问题被完全指定。它为估计器$m$提供了明确的数学公式，为模型基数$L_{\\mathrm{un}}$和$L_{\\mathrm{str}}$提供了精确的定义，为每个测试案例提供了所有必要的参数，并给出了清晰的数值计算指令。每个案例都存在唯一的、确定性的解。\n- **客观性 (Objective)**：问题以精确的数学和算法术语陈述，没有任何主观语言或观点。\n\n该问题未表现出任何无效性缺陷。它不是科学上不合理、不可形式化、不完整、不现实、不适定、琐碎或不可验证的。问题的核心是应用已建立的理论估计器来量化基于模型的压缩感知中的一个基本概念：通过利用信号结构实现的采样复杂度降低。所有测试案例都满足$n$相对于$k$较大的假设，确保了模型比较的有意义性。\n\n### 步骤3：结论与行动\n问题有效。将制定一个完整的解决方案。\n\n### 求解推导\n目标是形式化并计算在两种不同稀疏模型（非结构化和树状结构）下实现一致性信号恢复所需的测量次数。分析基于提供的测量次数估计器$m$：\n$$\nm \\approx \\left\\lceil c_0 \\, \\delta^{-2} \\, \\big( k + \\log L \\big) \\right\\rceil\n$$\n此处，$k$代表信号子空间的内在维度，而$\\log L$则解释了可能子空间族群的复杂度或大小。一个更复杂的模型（更大的$L$）需要更多的测量来区分其构成的子空间。\n\n**1. 非结构化稀疏模型**\n在标准的非结构化稀疏模型中，从$n$个可能索引中任意选出$k$个索引的子集都可以构成信号的支撑集。这对应于一个作为所有$k$维坐标子空间并集的信号模型。这类子空间的数量，即模型基数，由二项式系数给出：\n$$\nL_{\\mathrm{un}} = \\binom{n}{k}\n$$\n因此，所需的测量次数$m_{\\mathrm{un}}$通过将$L = L_{\\mathrm{un}}$代入通用公式进行估计：\n$$\nm_{\\mathrm{un}} = \\left\\lceil c_0 \\, \\delta^{-2} \\, \\left( k + \\log \\binom{n}{k} \\right) \\right\\rceil\n$$\n为了在$n$和$k$较大时保持数值稳定性，$\\log \\binom{n}{k}$项不是通过直接计算二项式系数来得到的。而是利用对数的性质和伽马函数$\\Gamma(z)$（其中$\\Gamma(z+1)=z!$）来计算：\n$$\n\\log \\binom{n}{k} = \\log\\left(\\frac{n!}{k!(n-k)!}\\right) = \\log(\\Gamma(n+1)) - \\log(\\Gamma(k+1)) - \\log(\\Gamma(n-k+1))\n$$\n这在数值上通过对数伽马函数（通常表示为`gammaln`）来实现。\n\n**2. 树状结构稀疏模型**\n在此模型中，信号的支撑集索引被约束为在强加于$n$个索引上的一个预定义的大型$d$叉树结构内，形成一个大小为$k$的连通有根子树。与非结构化模型相比，这种结构性约束显著减少了允许的支撑集数量。\n问题为模型基数$L_{\\mathrm{str}}$提供了一个上界，我们被指示在计算中使用这个上界：\n$$\nL_{\\mathrm{str}} = \\min\\!\\left\\{ n \\cdot T_d(k), \\binom{n}{k} \\right\\}\n$$\n项$T_d(k)$是具有$k$个节点的有根有序$d$叉树的可能形状数量：\n$$\nT_d(k) = \\frac{1}{(d-1)k+1} \\binom{dk}{k}\n$$\n$n \\cdot T_d(k)$中的因子$n$是对在$n$个节点的背景树中“放置”或嵌入这些树形的方式数量的一个保守界限。与$\\binom{n}{k}$取最小值确保了基数不会超过可能的$k$-子集总数。\n\n所需的测量次数$m_{\\mathrm{str}}$通过代入$L=L_{\\mathrm{str}}$来估计：\n$$\nm_{\\mathrm{str}} = \\left\\lceil c_0 \\, \\delta^{-2} \\, \\left( k + \\log L_{\\mathrm{str}} \\right) \\right\\rceil\n$$\n对数项$\\log L_{\\mathrm{str}}$计算如下：\n$$\n\\log L_{\\mathrm{str}} = \\min\\left( \\log n + \\log T_d(k), \\log \\binom{n}{k} \\right)\n$$\n其中$\\log T_d(k)$以稳定方式计算为：\n$$\n\\log T_d(k) = \\log\\binom{dk}{k} - \\log\\big((d-1)k+1\\big)\n$$\n而$\\log\\binom{dk}{k}$再次使用对数伽马函数计算。\n\n$m_{\\mathrm{un}}$和$m_{\\mathrm{str}}$的比较展示了“结构的祝福”（blessing of structure）。由于树状结构模型施加了强约束，对于非平凡的$k$，$L_{\\mathrm{str}}$通常远小于$L_{\\mathrm{un}}$，尤其是在$n$很大时。这导致$\\log L$项小得多，从而显著减少了所需的测量次数$m$，缓解了部分源于非结构化项$\\binom{n}{k}$中组合爆炸的维度灾难。实现将首先定义用于$\\log \\binom{n}{k}$和$\\log T_d(k)$的稳定函数，然后用它们计算$L_{\\mathrm{un}}$和$L_{\\mathrm{str}}$，最后计算每个测试案例的测量估计值$m_{\\mathrm{un}}$和$m_{\\mathrm{str}}$。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Computes and prints the estimated number of measurements for unstructured\n    and tree-structured sparsity models based on the problem specification.\n    \"\"\"\n\n    test_cases = [\n        # (n, k, d, delta, c0)\n        (16384, 64, 2, 0.25, 2.0),\n        (4096, 32, 4, 0.25, 2.0),\n        (2048, 1, 2, 0.20, 2.0),\n        (8192, 16, 3, 0.30, 2.0),\n    ]\n\n    def log_binom(n, k):\n        \"\"\"\n        Computes log(n choose k) using the log-gamma function for numerical stability.\n        Handles edge cases k=0 or k=n. If k > n or k  0, returns -inf.\n        \"\"\"\n        if k  0 or k > n:\n            return -np.inf\n        if k == 0 or k == n:\n            return 0\n        # For symmetry and to potentially use smaller numbers if k > n/2\n        if k > n / 2:\n            k = n - k\n        return gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)\n\n    def log_T_d(k, d):\n        \"\"\"\n        Computes log of the number of rooted ordered d-ary tree shapes on k nodes.\n        T_d(k) = (1 / ((d-1)k + 1)) * (dk choose k)\n        \"\"\"\n        if k == 0:\n            return 0  # There is one tree of size 0 (the empty tree)\n        if k == 1:\n            return 0  # One tree of size 1 (a single node), log(1) = 0\n            \n        log_binom_term = log_binom(d * k, k)\n        denominator_term = np.log((d - 1) * k + 1)\n        return log_binom_term - denominator_term\n\n    results = []\n    for case in test_cases:\n        n, k, d, delta, c0 = case\n\n        # Pre-compute the common factor\n        pre_factor = c0 / (delta**2)\n\n        # --- Unstructured Model Calculation ---\n        # L_un = binom(n, k)\n        log_L_un = log_binom(n, k)\n        \n        # m_un = ceil(c0 * delta^-2 * (k + log(L_un)))\n        m_un = int(np.ceil(pre_factor * (k + log_L_un)))\n\n        # --- Tree-Structured Model Calculation ---\n        # L_str = min(n * T_d(k), binom(n, k))\n        # log(L_str) = min(log(n) + log(T_d(k)), log(binom(n,k)))\n        if k == 0:\n             # A k=0 sparse signal is the zero vector, requiring 0 measurements.\n             # In the context of embedding a 0-dim subspace, k=0 also works.\n             log_L_str = -np.inf # Effectively makes m_str zero.\n        else:\n             log_Td = log_T_d(k, d)\n             log_L_str_candidate = np.log(n) + log_Td\n             log_L_str = min(log_L_str_candidate, log_L_un)\n\n        # m_str = ceil(c0 * delta^-2 * (k + log(L_str)))\n        if k == 0:\n            m_str = 0\n        else:\n            m_str = int(np.ceil(pre_factor * (k + log_L_str)))\n            \n        results.append([m_un, m_str])\n\n    # Format the output exactly as specified\n    output_str = str(results)\n    print(output_str)\n\nsolve()\n```[[11593, 2966], [5187, 1856], [125, 125], [1289, 440]]"
        }
    ]
}