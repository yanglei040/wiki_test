{
    "hands_on_practices": [
        {
            "introduction": "Our first practice delves into the theoretical heart of the matter. A central result in compressed sensing is that the number of measurements $m$ required to recover a $k$-sparse signal in $\\mathbb{R}^n$ often scales as $m \\approx C k \\log(n/k)$. This exercise challenges you to derive this fundamental scaling law from first principles, connecting the required number of measurements to the statistical dimension of the $\\ell_1$ norm's descent cone . By working through the asymptotic analysis, you will gain a deep appreciation for how the geometry of high-dimensional cones gives rise to this logarithmic dependence on the ambient dimension.",
            "id": "3486664",
            "problem": "Consider a fixed vector $x_{0} \\in \\mathbb{R}^{n}$ that is $k$-sparse with support $S \\subset \\{1,\\dots,n\\}$ and $|S|=k$, and consider linear measurements $y = A x_{0}$ with a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ whose entries are independent and identically distributed (i.i.d.) standard normal random variables. Let $\\|\\cdot\\|_{1}$ denote the $\\ell_{1}$ norm on $\\mathbb{R}^{n}$. Define the descent cone of $\\|\\cdot\\|_{1}$ at $x_{0}$ by\n$$\n\\mathcal{D} := \\left\\{ d \\in \\mathbb{R}^{n} : \\exists\\, t > 0 \\text{ such that } \\|x_{0} + t d\\|_{1} \\le \\|x_{0}\\|_{1} \\right\\}.\n$$\nLet the statistical dimension of a closed convex cone $\\mathcal{C} \\subset \\mathbb{R}^{n}$ be defined by\n$$\n\\delta(\\mathcal{C}) := \\mathbb{E}\\left[\\|\\Pi_{\\mathcal{C}}(g)\\|_{2}^{2}\\right],\n$$\nwhere $\\Pi_{\\mathcal{C}}$ is the Euclidean projection onto $\\mathcal{C}$ and $g \\sim \\mathcal{N}(0, I_{n})$ is a standard normal vector in $\\mathbb{R}^{n}$. It is a well-tested fact from conic integral geometry that for convex recovery via $\\ell_{1}$ minimization, there is a sharp success/failure threshold that occurs near $m \\approx \\delta(\\mathcal{D})$.\n\nStarting from first principles and core definitions, perform the following steps:\n\n1. Express the normal cone $\\mathcal{N}$ of $\\|\\cdot\\|_{1}$ at $x_{0}$ in terms of the subdifferential $\\partial \\|\\cdot\\|_{1}(x_{0})$, and use the relationship between a cone and its polar cone to rewrite $\\delta(\\mathcal{D})$ in terms of the expected squared distance of a standard normal vector to a scaled version of $\\partial \\|\\cdot\\|_{1}(x_{0})$.\n\n2. Using only basic properties of the standard normal distribution, derive a one-parameter variational representation for $\\delta(\\mathcal{D})$ of the form\n$$\n\\delta(\\mathcal{D}) = \\inf_{\\tau \\ge 0} \\Psi_{n,k}(\\tau),\n$$\nwhere $\\Psi_{n,k}(\\tau)$ is an explicit expression involving the standard normal probability density function and tail distribution.\n\n3. Analyze the asymptotic regime $n \\to \\infty$, $k \\to \\infty$, with $k/n \\to 0$. Using Laplace-type asymptotics for Gaussian tails, determine the leading-order behavior of $\\delta(\\mathcal{D})$ as an explicit function of $n$ and $k$.\n\nYour final answer must be the leading-order asymptotic term for $\\delta(\\mathcal{D})$ in the specified regime, given as a single closed-form analytic expression in terms of $n$ and $k$. Do not include lower-order terms. No rounding is required. Express all logarithms using the natural logarithm $\\ln(\\cdot)$.",
            "solution": "The problem asks for the leading-order asymptotic behavior of the statistical dimension $\\delta(\\mathcal{D})$ of the descent cone $\\mathcal{D}$ associated with the $\\ell_1$-norm at a $k$-sparse vector $x_0 \\in \\mathbb{R}^n$.\n\n### Part 1: Relating Statistical Dimension to the Normal Cone\n\nLet $f(x) = \\|x\\|_1$. The descent cone of $f$ at $x_0$ is given by the set of directions $d$ for which the directional derivative is non-positive:\n$$\n\\mathcal{D} = \\{d \\in \\mathbb{R}^n : f'(x_0; d) \\le 0\\}\n$$\nThe directional derivative of a convex function is the support function of its subdifferential: $f'(x_0; d) = \\max_{z \\in \\partial f(x_0)} \\langle z, d \\rangle$. Thus,\n$$\n\\mathcal{D} = \\{d \\in \\mathbb{R}^n : \\langle z, d \\rangle \\le 0 \\text{ for all } z \\in \\partial f(x_0)\\}\n$$\nLet $K = \\partial \\|x_0\\|_1$ be the subdifferential of the $\\ell_1$-norm at $x_0$. The normal cone to the sublevel set $\\{x : \\|x\\|_1 \\le \\|x_0\\|_1\\}$ at $x_0$ is the conic hull of the subdifferential, $\\mathcal{N} = \\text{cone}(K)$.\nThe set $\\mathcal{D}$ is, by its definition above, the polar cone of the set $K$. Since $K$ itself is not necessarily a cone, we consider its conic hull $\\mathcal{N}$. The polar cone of $\\mathcal{N}$ is:\n$$\n\\mathcal{N}^* = \\{ d \\in \\mathbb{R}^n : \\langle v, d \\rangle \\le 0 \\text{ for all } v \\in \\mathcal{N} \\}\n$$\nSince any $v \\in \\mathcal{N}$ can be written as $v = \\lambda z$ for some $\\lambda \\ge 0$ and $z \\in K$, the condition $\\langle v, d \\rangle \\le 0$ for all $v \\in \\mathcal{N}$ is equivalent to $\\langle z, d \\rangle \\le 0$ for all $z \\in K$. Therefore, $\\mathcal{D} = \\mathcal{N}^*$.\n\nA key identity relates the statistical dimension of a closed convex cone $\\mathcal{C}$ to its polar cone $\\mathcal{C}^*$:\n$$\n\\delta(\\mathcal{C}) = \\mathbb{E}\\left[ \\text{dist}(g, \\mathcal{C}^*)^2 \\right]\n$$\nwhere $g \\sim \\mathcal{N}(0, I_n)$ and $\\text{dist}(g, A) = \\inf_{a \\in A} \\|g - a\\|_2$ is the Euclidean distance from a point $g$ to a set $A$. This identity follows from Moreau's decomposition theorem, $g = \\Pi_{\\mathcal{C}}(g) + \\Pi_{\\mathcal{C}^*}(g)$, which implies $\\|\\Pi_{\\mathcal{C}}(g)\\|_2 = \\|g - \\Pi_{\\mathcal{C}^*}(g)\\|_2 = \\text{dist}(g, \\mathcal{C}^*)$.\n\nApplying this identity to our descent cone $\\mathcal{D}$, we use the fact that $\\mathcal{D}^* = (\\mathcal{N}^*)^* = \\mathcal{N}$ because $\\mathcal{N}$ is a closed convex cone.\n$$\n\\delta(\\mathcal{D}) = \\mathbb{E}\\left[ \\text{dist}(g, \\mathcal{D}^*)^2 \\right] = \\mathbb{E}\\left[ \\text{dist}(g, \\mathcal{N})^2 \\right]\n$$\nThe normal cone is $\\mathcal{N} = \\text{cone}(K) = \\{\\lambda z : \\lambda \\ge 0, z \\in K\\}$. The squared distance of $g$ to $\\mathcal{N}$ is\n$$\n\\text{dist}(g, \\mathcal{N})^2 = \\min_{v \\in \\mathcal{N}} \\|g - v\\|_2^2 = \\min_{\\lambda \\ge 0, z \\in K} \\|g - \\lambda z\\|_2^2 = \\min_{\\lambda \\ge 0} \\min_{z \\in K} \\|g - \\lambda z\\|_2^2 = \\min_{\\lambda \\ge 0} \\text{dist}(g, \\lambda K)^2\n$$\nHere, $\\lambda K$ is a \"scaled version\" of $\\partial \\|x_0\\|_1 = K$. So we have expressed $\\delta(\\mathcal{D})$ in terms of the expectation of a quantity involving the distance to a scaled version of the subdifferential.\n\n### Part 2: Variational Representation\n\nA powerful result from the theory of Gaussian processes (Gordon's inequality, or related principles) allows for the interchange of expectation and minimization in this context.\n$$\n\\delta(\\mathcal{D}) = \\mathbb{E}\\left[ \\min_{\\tau \\ge 0} \\text{dist}(g, \\tau K)^2 \\right] = \\min_{\\tau \\ge 0} \\mathbb{E}\\left[ \\text{dist}(g, \\tau K)^2 \\right]\n$$\nWe define $\\Psi_{n,k}(\\tau) := \\mathbb{E}\\left[\\text{dist}(g, \\tau K)^2\\right]$ and find an explicit expression for it.\nThe subdifferential $K = \\partial \\|x_0\\|_1$ is given by $K = \\{ z \\in \\mathbb{R}^n : z_S = s, \\|z_{S^c}\\|_\\infty \\le 1 \\}$, where $S$ is the support of $x_0$ with $|S|=k$, $S^c$ is its complement, and $s = \\text{sign}((x_0)_S)$.\nThe squared distance is\n$$\n\\text{dist}(g, \\tau K)^2 = \\min_{z \\in K} \\|g - \\tau z\\|_2^2 = \\|g_S - \\tau s\\|_2^2 + \\min_{\\|z_{S^c}\\|_\\infty \\le 1} \\|g_{S^c} - \\tau z_{S^c}\\|_2^2\n$$\nThe minimization over $z_{S^c}$ separates coordinate-wise. For each $i \\in S^c$, we minimize $(g_i - \\tau z_i)^2$ subject to $|z_i| \\le 1$. This is equivalent to minimizing $(g_i - w_i)^2$ subject to $w_i \\in [-\\tau, \\tau]$. The minimum is achieved by projecting $g_i$ onto the interval $[-\\tau, \\tau]$, and the squared distance is $\\text{dist}(g_i, [-\\tau, \\tau])^2 = (\\max(0, |g_i|-\\tau))^2 = (|g_i|-\\tau)_+^2$.\nSo, $\\text{dist}(g, \\tau K)^2 = \\|g_S - \\tau s\\|_2^2 + \\sum_{i \\in S^c} (|g_i|-\\tau)_+^2$.\n\nWe now take the expectation over $g \\sim \\mathcal{N}(0, I_n)$.\nThe first term: $\\mathbb{E}[\\|g_S - \\tau s\\|_2^2] = \\sum_{i \\in S} \\mathbb{E}[(g_i - \\tau s_i)^2]$. Since $s_i^2 = 1$ and $g_i \\sim \\mathcal{N}(0,1)$ are independent with $\\mathbb{E}[g_i]=0, \\mathbb{E}[g_i^2]=1$, we have $\\mathbb{E}[(g_i - \\tau s_i)^2] = \\mathbb{E}[g_i^2] - 2\\tau s_i \\mathbb{E}[g_i] + \\tau^2 s_i^2 = 1 - 0 + \\tau^2 = 1+\\tau^2$. Summing over $k$ indices in $S$ gives $k(1+\\tau^2)$.\n\nThe second term: $\\sum_{i \\in S^c} \\mathbb{E}[(|g_i|-\\tau)_+^2] = (n-k)\\mathbb{E}[(|Z|-\\tau)_+^2]$ for $Z \\sim \\mathcal{N}(0,1)$.\nLet $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$ be the standard normal PDF and $Q(\\tau) = \\int_\\tau^\\infty \\phi(z)dz$ be the tail probability.\n$$\n\\mathbb{E}[(|Z|-\\tau)_+^2] = \\int_{-\\infty}^\\infty (\\max(0, |z|-\\tau))^2 \\phi(z)dz = 2 \\int_\\tau^\\infty (z-\\tau)^2 \\phi(z)dz\n$$\nExpanding the square and integrating (using identities $\\int_\\tau^\\infty z\\phi(z)dz = \\phi(\\tau)$ and $\\int_\\tau^\\infty z^2\\phi(z)dz = \\tau\\phi(\\tau)+Q(\\tau)$) yields:\n$$\n\\mathbb{E}[(|Z|-\\tau)_+^2] = 2 \\left[ (\\tau\\phi(\\tau)+Q(\\tau)) - 2\\tau(\\phi(\\tau)) + \\tau^2 Q(\\tau) \\right] = 2(1+\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau)\n$$\nCombining the terms, we get the variational form:\n$$\n\\Psi_{n,k}(\\tau) = k(1+\\tau^2) + (n-k) \\left[ 2(1+\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau) \\right]\n$$\nwith $\\delta(\\mathcal{D}) = \\inf_{\\tau \\ge 0} \\Psi_{n,k}(\\tau)$.\n\n### Part 3: Asymptotic Analysis\n\nTo find the infimum, we differentiate $\\Psi_{n,k}(\\tau)$ with respect to $\\tau$ and set it to zero. Let $\\tau^*$ be the minimizer.\n$$\n\\frac{d\\Psi_{n,k}}{d\\tau} = 2k\\tau + (n-k) \\left[ 4\\tau Q(\\tau) - 4\\phi(\\tau) \\right] = 0\n$$\nThis gives the first-order condition (FOC) for $\\tau^* > 0$:\n$$\nk\\tau^* = 2(n-k) \\left[ \\phi(\\tau^*) - \\tau^*Q(\\tau^*) \\right]\n$$\nWe substitute this condition back into the expression for $\\Psi_{n,k}(\\tau^*)$ to simplify it.\n$$\n\\delta(\\mathcal{D}) = \\Psi_{n,k}(\\tau^*) = k(1+\\tau^{*2}) + (n-k)\\left[ 2(1+\\tau^{*2})Q(\\tau^*) - 2\\tau^*\\phi(\\tau^*) \\right]\n$$\nFrom the FOC, we can express $(n-k)$ in terms of $k$ and $\\tau^*$: $(n-k) = \\frac{k\\tau^*}{2(\\phi(\\tau^*)-\\tau^*Q(\\tau^*))}$. Substituting this:\n\\begin{align*}\n\\Psi_{n,k}(\\tau^*) = k(1+\\tau^{*2}) + \\frac{k\\tau^*}{2(\\phi-\\tau^*Q)} \\left[ 2(1+\\tau^{*2})Q - 2\\tau^*\\phi \\right] \\\\\n= k(1+\\tau^{*2}) - \\frac{k\\tau^*}{(\\phi-\\tau^*Q)} \\left[ \\tau^*\\phi - (1+\\tau^{*2})Q \\right] \\\\\n= k \\left[ \\frac{(1+\\tau^{*2})(\\phi-\\tau^*Q) - \\tau^{*2}\\phi + \\tau^*(1+\\tau^{*2})Q}{\\phi-\\tau^*Q} \\right] \\\\\n= k \\left[ \\frac{\\phi + \\tau^{*2}\\phi - \\tau^*Q - \\tau^{*3}Q - \\tau^{*2}\\phi + \\tau^*Q + \\tau^{*3}Q}{\\phi-\\tau^*Q} \\right] \\\\\n= k \\frac{\\phi(\\tau^*)}{\\phi(\\tau^*) - \\tau^*Q(\\tau^*)}\n\\end{align*}\nIn the regime $n\\to\\infty, k\\to\\infty, k/n\\to 0$, the ratio $(n-k)/k$ is large, which implies from the FOC that $\\tau^*$ must be large. For large $\\tau$, we use the asymptotic expansion for the Mills ratio: $Q(\\tau) \\sim \\phi(\\tau)(\\frac{1}{\\tau} - \\frac{1}{\\tau^3} + \\dots)$.\nThis gives $\\phi(\\tau) - \\tau Q(\\tau) \\sim \\phi(\\tau) - \\tau \\phi(\\tau)(\\frac{1}{\\tau} - \\frac{1}{\\tau^3}) = \\frac{\\phi(\\tau)}{\\tau^2}$.\nSubstituting this leading-order behavior into the simplified expression for $\\delta(\\mathcal{D})$:\n$$\n\\delta(\\mathcal{D}) \\approx k \\frac{\\phi(\\tau^*)}{\\phi(\\tau^*)/\\tau^{*2}} = k\\tau^{*2}\n$$\nNow we find the leading-order behavior of $\\tau^{*2}$. We use the asymptotic form in the FOC:\n$$\nk\\tau^* \\approx 2(n-k) \\frac{\\phi(\\tau^*)}{\\tau^{*2}} = 2(n-k) \\frac{1}{\\tau^{*2}\\sqrt{2\\pi}} \\exp\\left(-\\frac{\\tau^{*2}}{2}\\right)\n$$\nTaking the natural logarithm of both sides:\n$$\n\\ln(k) + 3\\ln(\\tau^*) \\approx \\ln(2(n-k)) - \\frac{\\tau^{*2}}{2} - \\frac{1}{2}\\ln(2\\pi)\n$$\nRearranging for $\\tau^{*2}$:\n$$\n\\frac{\\tau^{*2}}{2} + 3\\ln(\\tau^*) \\approx \\ln\\left(\\frac{n-k}{k}\\right) + \\text{const}\n$$\nIn the limit $n/k \\to \\infty$, $\\ln(n/k)$ is large. For large $\\tau^*$, the $\\tau^{*2}/2$ term dominates the left-hand side. The leading-order balance is:\n$$\n\\frac{\\tau^{*2}}{2} \\approx \\ln\\left(\\frac{n}{k}\\right) \\implies \\tau^{*2} \\approx 2\\ln\\left(\\frac{n}{k}\\right)\n$$\nCombining this with $\\delta(\\mathcal{D}) \\approx k\\tau^{*2}$, we arrive at the leading-order asymptotic behavior:\n$$\n\\delta(\\mathcal{D}) \\approx k \\left( 2\\ln\\left(\\frac{n}{k}\\right) \\right) = 2k\\ln\\left(\\frac{n}{k}\\right)\n$$\nThis expression represents the leading term in the asymptotic expansion for the statistical dimension of the descent cone.",
            "answer": "$$\n\\boxed{2k\\ln\\left(\\frac{n}{k}\\right)}\n$$"
        },
        {
            "introduction": "While asymptotic laws provide crucial scaling insights, practical engineering requires precise predictions for finite-dimensional systems. This practice moves from scaling behavior to sharp, computable thresholds based on the theory of phase transitions . You will apply the advanced geometric framework of statistical dimension to determine whether, for a specific problem instance defined by $(m, d, k)$, $\\ell_1$ recovery is predicted to succeed or fail. This exercise demonstrates how abstract mathematical concepts translate into a powerful predictive tool for algorithm performance.",
            "id": "3486736",
            "problem": "Consider the linear inverse model $y = A x_{\\star}$, where $A \\in \\mathbb{R}^{m \\times d}$ has independent and identically distributed standard normal entries, $x_{\\star} \\in \\mathbb{R}^{d}$ is exactly $k$-sparse with support chosen uniformly at random of size $k$ and with nonzero signs independent and symmetric, and $y \\in \\mathbb{R}^{m}$. The recovery method is basis pursuit, i.e., solve $\\min_{x \\in \\mathbb{R}^{d}} \\|x\\|_{1}$ subject to $A x = y$. In the high-dimensional limit, the success or failure of $\\ell_{1}$ recovery exhibits an asymptotic phase transition governed by the geometry of descent cones and the statistical dimension, which quantifies the curse of dimensionality via a normalized sampling ratio.\n\nStarting only from fundamental definitions and well-tested facts, including the geometry of descent cones of convex functions, the statistical dimension of a cone, and Gordonâ€™s comparison/escape principles for Gaussian operators, derive a computable condition to predict success or failure from $(m,d,k)$ by comparing the sampling ratio to a critical value determined by the statistical dimension of the descent cone of the $\\ell_{1}$ norm at a $k$-sparse point. Then, for the specific instance with $m = 320$, $d = 1000$, and $k = 40$, use your derived condition to determine whether the point $(\\delta,\\rho)$ corresponding to these parameters lies above or below the asymptotic phase transition for $\\ell_{1}$ recovery under Gaussian measurements, and predict success or failure accordingly.\n\nAnswer specification:\n- Let the final answer be $1$ if $\\ell_{1}$ recovery is predicted to succeed with overwhelming probability in the high-dimensional sense, and $0$ if it is predicted to fail.\n- The final answer must be a single number as specified above. Do not provide an inequality or a sentence as the final answer.",
            "solution": "The validity of the problem statement is confirmed. It is a well-posed, scientifically grounded problem in the field of compressed sensing and high-dimensional geometry. The problem asks for the derivation of a condition for successful signal recovery and its application to a specific numerical instance.\n\n### Part 1: Derivation of the Computable Condition\n\nThe problem concerns the recovery of a $k$-sparse signal $x_{\\star} \\in \\mathbb{R}^{d}$ from $m$ linear measurements $y = Ax_{\\star} \\in \\mathbb{R}^{m}$, where $A \\in \\mathbb{R}^{m \\times d}$ is a matrix with independent and identically distributed (i.i.d.) standard normal entries. The recovery is performed using basis pursuit, which is the $\\ell_{1}$-minimization problem:\n$$ \\min_{x \\in \\mathbb{R}^{d}} \\|x\\|_{1} \\quad \\text{subject to} \\quad Ax = y $$\nThe original signal $x_{\\star}$ is a feasible point for this program since $Ax_{\\star}=y$. Recovery is deemed successful if $x_{\\star}$ is the *unique* minimizer.\n\nThe uniqueness of the solution is a geometric question. Let $f(x) = \\|x\\|_1$. A necessary and sufficient condition for $x_{\\star}$ to be the unique minimizer is that for any non-zero vector $v$ in the null space of $A$ (i.e., $v \\in \\mathrm{null}(A) \\setminus \\{0\\}$), the $\\ell_1$ norm must strictly increase in the direction of $v$, i.e., $\\|x_{\\star} + v\\|_1 > \\|x_{\\star}\\|_1$.\n\nThis condition can be rephrased using the concept of a descent cone. The descent cone of a convex function $f$ at a point $x_{\\star}$, denoted $\\mathcal{D}(f, x_{\\star})$, is the set of directions from $x_{\\star}$ in which $f$ does not increase. For the $\\ell_1$ norm at a $k$-sparse vector $x_{\\star}$ with support $T = \\{i : (x_{\\star})_i \\neq 0\\}$, the descent cone is given by:\n$$ \\mathcal{D}(\\|\\cdot\\|_1, x_{\\star}) = \\{ v \\in \\mathbb{R}^d : \\|(x_{\\star})_T + v_T\\|_1 + \\|v_{T^c}\\|_1 \\le \\|(x_{\\star})_T\\|_1 \\} $$\nwhere $v_T$ is the part of $v$ on the support $T$, and $v_{T^c}$ is the part on the complement $T^c$. For a generic sparse vector and any direction $v$, this cone is well-approximated by the simpler cone $C_T = \\{ v \\in \\mathbb{R}^d : \\|v_{T^c}\\|_1 \\le \\|v_T\\|_1 \\}$.\n\nThe condition for successful recovery can now be stated succinctly: the null space of $A$ must have a trivial intersection with the descent cone at $x_{\\star}$.\n$$ \\mathrm{null}(A) \\cap \\mathcal{D}(\\|\\cdot\\|_1, x_{\\star}) = \\{0\\} $$\nThe matrix $A$ has i.i.d. Gaussian entries, which means its null space, $\\mathrm{null}(A)$, is a uniformly random subspace of $\\mathbb{R}^d$ of dimension $d-m$. The problem thus becomes: what is the probability that a random subspace of dimension $d-m$ intersects a fixed cone $\\mathcal{D}(\\|\\cdot\\|_1, x_{\\star})$?\n\nThe theory of high-dimensional geometry, particularly Gordon's comparison inequality for Gaussian processes, provides a sharp answer to this question. For a given cone $C$, the intersection with a random subspace is determined by the cone's statistical dimension, $\\delta(C)$, defined as the expected squared Euclidean norm of the projection of a standard Gaussian vector $g \\sim \\mathcal{N}(0, I_d)$ onto the cone:\n$$ \\delta(C) = \\mathbb{E}_{g \\sim \\mathcal{N}(0, I_d)} [\\|\\text{proj}_C(g)\\|_2^2] $$\nThe central result of this theory, often called the kinematic formula or Gordon's escape-from-the-mesh principle, states that in the high-dimensional setting, the intersection event exhibits a phase transition. A random subspace of dimension $n$ will intersect the cone $C$ with high probability if $n + \\delta(C) > d$, and will not intersect $C$ (other than the origin) with high probability if $n + \\delta(C)  d$.\n\nApplying this to our problem, the dimension of the random subspace $\\mathrm{null}(A)$ is $n = d-m$. Failure to recover occurs if $\\mathrm{null}(A)$ intersects the descent cone $\\mathcal{D}(\\|\\cdot\\|_1, x_{\\star})$ non-trivially. This is predicted to happen if:\n$$ (d-m) + \\delta(\\mathcal{D}(\\|\\cdot\\|_1, x_{\\star})) > d \\implies \\delta(\\mathcal{D}(\\|\\cdot\\|_1, x_{\\star})) > m $$\nConversely, successful recovery is predicted if:\n$$ \\delta(\\mathcal{D}(\\|\\cdot\\|_1, x_{\\star}))  m $$\nThe quantity $\\delta(\\mathcal{D}(\\|\\cdot\\|_1, x_{\\star}))$ is the critical number of measurements required.\n\nIn the high-dimensional limit where $d \\to \\infty$ while the ratios $\\delta = m/d$ and $\\rho = k/d$ remain constant, the normalized statistical dimension $\\delta(\\mathcal{D}(\\|\\cdot\\|_1, x_{\\star}))/d$ converges to a deterministic function that depends only on the sparsity ratio $\\rho$. Let's call this function $\\delta_{\\text{crit}}(\\rho)$. The success condition then becomes a direct comparison of the sampling ratio $\\delta$ with this critical value:\n$$ \\delta > \\delta_{\\text{crit}}(\\rho) $$\nThis is the computable condition for success. The function $\\delta_{\\text{crit}}(\\rho)$ has been derived in the literature from the statistical mechanical analysis of disordered systems and random matrix theory. For the real-valued Gaussian measurement model, $\\delta_{\\text{crit}}(\\rho)$ is given parametrically. For a parameter $t \\ge 0$, the pair $(\\rho(t), \\delta_{\\text{crit}}(t))$ on the phase transition boundary is given by:\n$$ \\frac{\\rho(t)}{1-\\rho(t)} = t\\phi(t) - t^2\\Phi(-t) $$\n$$ \\delta_{\\text{crit}}(t) = 1 - (1-\\rho(t))\\left( 1 - t\\phi(t) + (t^2-1)\\Phi(-t) \\right) $$\nwhere $\\phi(t)$ is the probability density function and $\\Phi(t)$ is the cumulative distribution function of a standard normal random variable.\n\n### Part 2: Application to the Specific Instance\n\nWe are given the parameters $m = 320$, $d = 1000$, and $k = 40$. The corresponding normalized ratios are:\n$$ \\delta = \\frac{m}{d} = \\frac{320}{1000} = 0.32 $$\n$$ \\rho = \\frac{k}{d} = \\frac{40}{1000} = 0.04 $$\nTo determine if recovery is successful, we must compute the critical sampling ratio $\\delta_{\\text{crit}}(0.04)$ and check if $\\delta > \\delta_{\\text{crit}}(0.04)$.\n\nFirst, we find the parameter $t$ corresponding to $\\rho = 0.04$ by solving the first parametric equation:\n$$ \\frac{0.04}{1-0.04} = \\frac{0.04}{0.96} = \\frac{1}{24} $$\nSo we need to solve the transcendental equation for $t$:\n$$ t\\phi(t) - t^2\\Phi(-t) = \\frac{1}{24} \\approx 0.04167 $$\nThis equation has two positive roots. The physically relevant solution branch is the one for which $\\delta_{\\text{crit}} \\to 0$ as $\\rho \\to 0$, which corresponds to the larger root for $t$. Numerical evaluation yields $t \\approx 1.51$.\n\nNow, we substitute $t=1.51$ and $\\rho=0.04$ into the second parametric equation to find $\\delta_{\\text{crit}}$:\n$$ \\delta_{\\text{crit}} = 1 - (1-0.04)\\left( 1 - 1.51\\phi(1.51) + (1.51^2-1)\\Phi(-1.51) \\right) $$\nUsing standard values for the Gaussian functions: $\\phi(1.51) \\approx 0.1276$ and $\\Phi(-1.51) \\approx 0.0655$.\n$$ \\delta_{\\text{crit}} \\approx 1 - 0.96 \\left( 1 - 1.51 \\times 0.1276 + (2.2801 - 1) \\times 0.0655 \\right) $$\n$$ \\delta_{\\text{crit}} \\approx 1 - 0.96 \\left( 1 - 0.1927 + 1.2801 \\times 0.0655 \\right) $$\n$$ \\delta_{\\text{crit}} \\approx 1 - 0.96 \\left( 1 - 0.1927 + 0.0838 \\right) $$\n$$ \\delta_{\\text{crit}} \\approx 1 - 0.96 \\left( 0.8911 \\right) $$\n$$ \\delta_{\\text{crit}} \\approx 1 - 0.8555 = 0.1445 $$\nThe critical sampling ratio for a sparsity of $\\rho=0.04$ is approximately $\\delta_{\\text{crit}} \\approx 0.1445$.\n\nFinally, we compare our actual sampling ratio $\\delta = 0.32$ with this critical value:\n$$ 0.32 > 0.1445 $$\nSince the actual sampling ratio $\\delta$ is substantially larger than the critical threshold $\\delta_{\\text{crit}}(\\rho)$, the point $(\\rho, \\delta) = (0.04, 0.32)$ lies deep within the region of successful recovery. Therefore, $\\ell_1$ recovery is predicted to succeed with overwhelming probability.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "The curse of dimensionality is not an insurmountable barrier but a challenge that motivates the search for more refined signal models. This final practice explores how incorporating prior knowledge about signal structure can dramatically reduce sampling requirements, a principle known as model-based compressed sensing . By comparing the measurement needs for unstructured sparsity versus a constrained, tree-based model, you will quantify the \"blessing of structure\" and see how reducing model complexity tames the combinatorial explosion that underpins the curse.",
            "id": "3486799",
            "problem": "You are asked to formalize how model-based, tree-structured sparsity mitigates the curse of dimensionality in compressed sensing by reducing the number of measurements required for uniform recovery compared to unstructured sparsity. Work in an ambient dimension of $n$ with $k$-sparse signals, and consider random sub-Gaussian measurement matrices. The fundamental base you must use is the union-of-subspaces viewpoint and standard concentration-of-measure for sub-Gaussian embeddings of low-dimensional sets: to embed a single $k$-dimensional subspace with distortion at most $ \\delta \\in (0,1) $, one requires on the order of $m = \\mathcal{O}(\\delta^{-2} k)$ measurements; to embed a union of $L$ such subspaces uniformly, a standard net-and-union-bound argument inflates the requirement by a term on the order of $ \\log L $. This principle is widely accepted and is to be taken as the starting point.\n\nFrom this base, you must derive, justify, and implement explicit measurement complexity estimators for two models:\n\n- Unstructured $k$-sparsity: the model is the union of all coordinate subspaces spanned by any $k$ columns of the identity in $ \\mathbb{R}^n $. Its model cardinality is $ L_{\\mathrm{un}} = \\binom{n}{k} $.\n\n- Tree-structured $k$-sparsity on a rooted $d$-ary tree: a support is any connected rooted subtree of size $k$ embedded in a large complete rooted $d$-ary tree with $n$ nodes. An upper bound on the number of such supports is obtained by counting shapes and embeddings. A well-tested combinatorial fact is that the number of rooted ordered $d$-ary tree shapes on $k$ nodes equals\n$$\nT_d(k) = \\frac{1}{((d-1)k+1)} \\binom{d k}{k}.\n$$\nA conservative and standard placement bound is that the number of embedded supports does not exceed $ n \\cdot T_d(k) $, and of course cannot exceed $ \\binom{n}{k} $. Hence, the structured model cardinality can be bounded by\n$$\nL_{\\mathrm{str}} \\leq \\min\\!\\left\\{ n \\cdot T_d(k), \\binom{n}{k} \\right\\}.\n$$\n\nUsing the union-of-subspaces embedding principle together with the above cardinalities, derive an estimator of the form\n$$\nm \\approx \\left\\lceil c_0 \\, \\delta^{-2} \\, \\big( k + \\log L \\big) \\right\\rceil,\n$$\nwhere $c_0 > 0$ is a fixed absolute constant, $\\log$ is the natural logarithm, and $L$ is the model cardinality. This formula is to be applied with $ L = L_{\\mathrm{un}} $ for the unstructured model and with $ L = L_{\\mathrm{str}} $ for the tree-structured model. You must implement this estimator exactly as written, using the natural logarithm, with $ \\log \\binom{a}{b} $ and $ \\log T_d(k) $ computed numerically in a stable manner via the logarithm of the gamma function when needed. You may assume that $n$ is sufficiently large relative to $k$ so that the embedding count bound is meaningful.\n\nYour program must compute, for each parameter set in the test suite below, the pair of integers $[ m_{\\mathrm{un}}, m_{\\mathrm{str}} ]$, where $ m_{\\mathrm{un}} $ is the unstructured estimate and $ m_{\\mathrm{str}} $ is the structured estimate. All logarithms must be natural logarithms, and the final answers are integers obtained by applying the ceiling function. No physical units are involved. Angles are not used. Percentages are not used.\n\nTest Suite (each tuple is $ (n,k,d,\\delta,c_0) $):\n- Case $1$ (happy path, large ambient dimension): $(n,k,d,\\delta,c_0) = (16384, 64, 2, 0.25, 2.0)$.\n- Case $2$ (higher branching factor): $(n,k,d,\\delta,c_0) = (4096, 32, 4, 0.25, 2.0)$.\n- Case $3$ (boundary case $k=1$): $(n,k,d,\\delta,c_0) = (2048, 1, 2, 0.20, 2.0)$.\n- Case $4$ (moderate size and branching): $(n,k,d,\\delta,c_0) = (8192, 16, 3, 0.30, 2.0)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the four cases as a comma-separated list of lists, each inner list being $[ m_{\\mathrm{un}}, m_{\\mathrm{str}} ]$. For example, the output should have the form\n\"[ [m_un_case1,m_str_case1],[m_un_case2,m_str_case2],[m_un_case3,m_str_case3],[m_un_case4,m_str_case4] ]\"\nwith no spaces removed or additional text added. Use exact integer values as computed by your implementation.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\nThe explicit data, variables, and conditions provided are as follows:\n- **Ambient Dimension**: $n$\n- **Sparsity Level**: $k$\n- **Measurement Matrix Type**: Random sub-Gaussian\n- **Theoretical Framework**: Union-of-subspaces viewpoint for compressed sensing.\n- **Base Principle**: Uniform embedding of a union of $L$ $k$-dimensional subspaces requires $m = \\mathcal{O}(\\delta^{-2}(k + \\log L))$ measurements for a distortion $\\delta \\in (0,1)$.\n- **Estimator Formula**: $m \\approx \\left\\lceil c_0 \\, \\delta^{-2} \\, \\big( k + \\log L \\big) \\right\\rceil$, where $\\log$ is the natural logarithm and $c_0 > 0$ is a constant.\n- **Unstructured Sparsity Model**: Union of all coordinate subspaces of dimension $k$.\n- **Cardinality of Unstructured Model**: $L_{\\mathrm{un}} = \\binom{n}{k}$.\n- **Tree-Structured Sparsity Model**: A support is a connected rooted subtree of size $k$ in a complete rooted $d$-ary tree with $n$ nodes.\n- **Number of Tree Shapes**: $T_d(k) = \\frac{1}{((d-1)k+1)} \\binom{d k}{k}$.\n- **Cardinality Bound for Structured Model**: $L_{\\mathrm{str}} \\leq \\min\\!\\left\\{ n \\cdot T_d(k), \\binom{n}{k} \\right\\}$. This bound is to be used for the cardinality $L$ in the estimator.\n- **Numerical Computation**: $\\log \\binom{a}{b}$ and $\\log T_d(k)$ are to be computed using the logarithm of the gamma function.\n- **Task**: Compute the pair of integers $[ m_{\\mathrm{un}}, m_{\\mathrm{str}} ]$ for each test case.\n- **Test Suite**:\n    - Case 1: $(n,k,d,\\delta,c_0) = (16384, 64, 2, 0.25, 2.0)$\n    - Case 2: $(n,k,d,\\delta,c_0) = (4096, 32, 4, 0.25, 2.0)$\n    - Case 3: $(n,k,d,\\delta,c_0) = (2048, 1, 2, 0.20, 2.0)$\n    - Case 4: $(n,k,d,\\delta,c_0) = (8192, 16, 3, 0.30, 2.0)$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity against the established criteria.\n- **Scientifically Grounded**: The problem is firmly rooted in the theory of compressed sensing and sparse signal recovery. The union-of-subspaces model, the use of sub-Gaussian random matrices, and the resulting measurement bounds of the form $m \\propto k + \\log L$ are standard and well-established results in the field. The combinatorial formulas for the model cardinalities are correct.\n- **Well-Posed**: The problem is fully specified. It provides an explicit mathematical formula for the estimator $m$, precise definitions for the model cardinalities $L_{\\mathrm{un}}$ and $L_{\\mathrm{str}}$, all necessary parameters for each test case, and clear instructions for numerical computation. A unique, deterministic solution exists for each case.\n- **Objective**: The problem is stated in precise mathematical and algorithmic terms, devoid of any subjective language or opinion.\n\nThe problem does not exhibit any of the invalidity flaws. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, trivial, or unverifiable. The core of the problem is to apply established theoretical estimators to quantify a fundamental concept in model-based compressed sensing: the reduction in sampling complexity achieved by leveraging signal structure. The assumption that $n$ is large relative to $k$ is met in all test cases, ensuring the model comparison is meaningful.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be developed.\n\n### Solution Derivation\nThe objective is to formalize and compute the number of measurements required for uniform signal recovery under two different sparsity models: unstructured and tree-structured. The analysis is based on the provided estimator for the number of measurements, $m$:\n$$\nm \\approx \\left\\lceil c_0 \\, \\delta^{-2} \\, \\big( k + \\log L \\big) \\right\\rceil\n$$\nHere, $k$ represents the intrinsic dimensionality of the signal subspaces, while $\\log L$ accounts for the complexity, or size, of the family of possible subspaces. A more complex model (larger $L$) requires more measurements to distinguish between its constituent subspaces.\n\n**1. Unstructured Sparsity Model**\nIn the standard unstructured sparsity model, any subset of $k$ indices out of a possible $n$ can form the support of the signal. This corresponds to a signal model that is the union of all $k$-dimensional coordinate subspaces. The number of such subspaces, which is the model cardinality, is given by the binomial coefficient:\n$$\nL_{\\mathrm{un}} = \\binom{n}{k}\n$$\nThe required number of measurements, $m_{\\mathrm{un}}$, is therefore estimated by substituting $L = L_{\\mathrm{un}}$ into the general formula:\n$$\nm_{\\mathrm{un}} = \\left\\lceil c_0 \\, \\delta^{-2} \\, \\left( k + \\log \\binom{n}{k} \\right) \\right\\rceil\n$$\nFor numerical stability with large $n$ and $k$, the term $\\log \\binom{n}{k}$ is not computed by evaluating the binomial coefficient directly. Instead, it is computed using the properties of the logarithm and the gamma function $\\Gamma(z)$, where $\\Gamma(z+1)=z!$:\n$$\n\\log \\binom{n}{k} = \\log\\left(\\frac{n!}{k!(n-k)!}\\right) = \\log(\\Gamma(n+1)) - \\log(\\Gamma(k+1)) - \\log(\\Gamma(n-k+1))\n$$\nThis is implemented numerically using the log-gamma function, often denoted `gammaln`.\n\n**2. Tree-Structured Sparsity Model**\nIn this model, the signal's support indices are constrained to form a connected rooted subtree of size $k$ within a larger, predefined $d$-ary tree structure imposed on the $n$ indices. This structural constraint significantly reduces the number of permissible supports compared to the unstructured model.\nThe problem provides an upper bound for the model cardinality, $L_{\\mathrm{str}}$, which we are instructed to use in our calculation:\n$$\nL_{\\mathrm{str}} = \\min\\!\\left\\{ n \\cdot T_d(k), \\binom{n}{k} \\right\\}\n$$\nThe term $T_d(k)$ is the number of possible shapes for a rooted ordered $d$-ary tree with $k$ nodes:\n$$\nT_d(k) = \\frac{1}{(d-1)k+1} \\binom{dk}{k}\n$$\nThe factor of $n$ in $n \\cdot T_d(k)$ is a conservative bound on the number of ways to \"place\" or embed these tree shapes within the larger $n$-node ambient tree. The $\\min$ with $\\binom{n}{k}$ ensures the cardinality does not exceed the total number of possible $k$-subsets.\n\nThe required number of measurements, $m_{\\mathrm{str}}$, is estimated by substituting $L=L_{\\mathrm{str}}$:\n$$\nm_{\\mathrm{str}} = \\left\\lceil c_0 \\, \\delta^{-2} \\, \\left( k + \\log L_{\\mathrm{str}} \\right) \\right\\rceil\n$$\nThe logarithmic term $\\log L_{\\mathrm{str}}$ is computed as:\n$$\n\\log L_{\\mathrm{str}} = \\min\\left( \\log n + \\log T_d(k), \\log \\binom{n}{k} \\right)\n$$\nwhere $\\log T_d(k)$ is calculated stably as:\n$$\n\\log T_d(k) = \\log\\binom{dk}{k} - \\log\\big((d-1)k+1\\big)\n$$\nand $\\log\\binom{dk}{k}$ is again computed using the log-gamma function.\n\nThe comparison of $m_{\\mathrm{un}}$ and $m_{\\mathrm{str}}$ demonstrates the \"blessing of structure.\" Because the tree-structured model imposes strong constraints, $L_{\\mathrm{str}}$ is typically vastly smaller than $L_{\\mathrm{un}}$ for non-trivial $k$, especially when $n$ is large. This leads to a much smaller $\\log L$ term and, consequently, a significant reduction in the required number of measurements $m$, mitigating the curse of dimensionality which partly arises from the combinatorial explosion of possibilities in the unstructured term $\\binom{n}{k}$. The implementation will proceed by first defining stable functions for $\\log \\binom{n}{k}$ and $\\log T_d(k)$, then using these to compute $L_{\\mathrm{un}}$ and $L_{\\mathrm{str}}$ and finally the measurement estimates $m_{\\mathrm{un}}$ and $m_{\\mathrm{str}}$ for each test case.",
            "answer": "`[[17240,5063],[7908,4375],[432,432],[3176,1323]]`"
        }
    ]
}