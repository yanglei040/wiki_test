{
    "hands_on_practices": [
        {
            "introduction": "The effectiveness of a sparsifying transform is fundamentally tied to how well its underlying structure matches the signal's properties. This first practice explores the critical role of boundary conditions in choosing between the Discrete Fourier Transform (DFT) and the Discrete Cosine Transform (DCT) . By analyzing how each transform's implicit signal extension affects coefficient decay rates, you will develop a core intuition for selecting the optimal basis for piecewise smooth signals.",
            "id": "3478629",
            "problem": "You are given a real, finite-length signal $x \\in \\mathbb{R}^N$ obtained by uniform sampling of a function $f:[0,1]\\to\\mathbb{R}$ at $N$ equispaced points. Suppose $f$ is piecewise continuously differentiable on $(0,1)$, with $f' \\in \\mathrm{BV}(0,1)$ (bounded variation), and that all jump discontinuities of $f$ are confined to the domain boundaries in the following sense: $f$ is continuous on $(0,1)$ and any mismatch occurs only between the one-sided limits $f(0^+)$ and $f(1^-)$ that are adjacent under periodic wrap-around. There are no interior jumps on $(0,1)$. You wish to choose a sparsifying transform for compressed sensing recovery that minimizes the number of significant transform coefficients for a target approximation error. Consider the Discrete Fourier Transform (DFT) and the Discrete Cosine Transform (DCT) of type II, viewed as orthonormal bases on $\\mathbb{R}^N$ derived, respectively, from a periodic extension and an even extension of $f$.\n\nUsing only the core definitions of the DFT and DCT as orthonormal expansions associated with, respectively, periodic and even boundary conditions, and the standard relationship between smoothness, boundary conditions, and decay rates of Fourier-type coefficients derived by integration by parts and jump contributions, decide which of the following criteria are correct for choosing between the DFT and the DCT in the setting described. Select all that apply.\n\nA. When $f$ is nonperiodic and its only edge discontinuities are localized at $x=0$ and $x=1$, the DFT’s implicit periodic extension introduces a wrap-around jump, yielding Fourier coefficients that decay like $O(k^{-1})$, whereas the DCT’s even extension removes that artificial jump and, under $f' \\in \\mathrm{BV}(0,1)$, produces cosine coefficients that decay like $O(k^{-2})$. Therefore, the DCT is typically sparser than the DFT in this case.\n\nB. If $f$ is exactly periodic on $[0,1]$ with matched traces across the boundary (for example, $f^{(r)}(0^+) = f^{(r)}(1^-)$ for some $r \\ge 0$), then the DFT aligns with the true boundary conditions and does not incur a wrap-around discontinuity. In this situation, the DFT is at least as sparse as the DCT for general periodic content, and can be preferable.\n\nC. For uniform time-domain subsampling in compressed sensing, the mutual coherence between the sampling basis and the transform basis is identical for the DFT and the DCT, so the transform choice cannot affect recoverability; only the sampling pattern matters.\n\nD. For a fixed coefficient budget $m$, truncated DCT expansions exhibit reduced boundary Gibbs oscillations compared to truncated DFT expansions when discontinuities are confined to the endpoints, so fewer large coefficients are required by the DCT to achieve a target mean-squared error.\n\nE. In $2$-dimensional images with sharp edges that meet and cross the image frame boundary, a separable $2$-dimensional DCT tends to produce sparser coefficient sets than a separable $2$-dimensional DFT under the same sampling, because the even-symmetric extension mitigates wrap-around discontinuities along the frame boundary.\n\nProvide your selection and justify it by reasoning strictly from the transform definitions, the nature of the boundary extensions they impose, and coefficient decay implications that follow from smoothness and jump contributions. Do not assume any unproven shortcut formulas beyond the standard integration by parts argument and the fact that jump discontinuities induce $O(k^{-1})$ decay in Fourier-type series.",
            "solution": "This problem is scientifically grounded, well-posed, and objective, testing a fundamental concept in the application of sparsifying transforms. The provided information is complete and sufficient for a rigorous analysis. We proceed with the solution.\n\nThe core of the problem lies in understanding how the implicit boundary conditions of the DFT and DCT affect the smoothness of the extended signal, which in turn governs the decay rate of the transform coefficients. A faster decay rate implies a sparser representation.\n\n-   **Discrete Fourier Transform (DFT):** The DFT operates on the assumption of periodic boundary conditions. It treats the finite signal $x$ as one period of an infinite periodic sequence. If the original continuous function $f(t)$ is not periodic on $[0,1]$ (i.e., $f(0^+) \\neq f(1^-)$), this periodic extension creates a jump discontinuity at the boundaries. The presence of a jump discontinuity in a function (a $C^{-1}$ property) limits the decay rate of its Fourier-type coefficients to $O(k^{-1})$, where $k$ is the frequency index.\n\n-   **Discrete Cosine Transform (DCT-II):** The DCT-II operates on the assumption of even-symmetric boundary conditions. It implicitly creates an extended signal that is an even reflection of the original. For a function $f(t)$ on $[0,1]$, this creates a new function on $[-1,1]$ that is continuous at $t=0$ (since $f_e(0^+) = f(0^+)$ and $f_e(0^-) = f(-0^+) = f(0^+)$). The full periodic extension of this even function is continuous everywhere. However, its derivative may not be. The derivative of the even-extended function has jump discontinuities at the points of reflection unless the original function's derivative is zero at the boundaries. Given that $f$ is continuous on $(0,1)$ and $f' \\in \\mathrm{BV}(0,1)$, the even-extended function is continuous with a derivative of bounded variation. This property (a $C^0$ function with a BV first derivative) leads to a coefficient decay rate of $O(k^{-2})$.\n\n### Evaluation of Options\n\n**A. When $f$ is nonperiodic and its only edge discontinuities are localized at $x=0$ and $x=1$, the DFT’s implicit periodic extension introduces a wrap-around jump, yielding Fourier coefficients that decay like $O(k^{-1})$, whereas the DCT’s even extension removes that artificial jump and, under $f' \\in \\mathrm{BV}(0,1)$, produces cosine coefficients that decay like $O(k^{-2})$. Therefore, the DCT is typically sparser than the DFT in this case.**\nThis statement is a correct summary of the analysis above. The DFT's periodic assumption introduces a discontinuity for a non-periodic signal, leading to slow $O(k^{-1})$ decay. The DCT's even extension preserves continuity, and for a function with a bounded-variation derivative, achieves a much faster $O(k^{-2})$ decay. Faster decay directly translates to better compressibility and sparsity.\n**Verdict: Correct.**\n\n**B. If $f$ is exactly periodic on $[0,1]$ with matched traces across the boundary (for example, $f^{(r)}(0^+) = f^{(r)}(1^-)$ for some $r \\ge 0$), then the DFT aligns with the true boundary conditions and does not incur a wrap-around discontinuity. In this situation, the DFT is at least as sparse as the DCT for general periodic content, and can be preferable.**\nThis is also correct. If the signal is inherently periodic, the DFT is the natural transform. Its implicit boundary conditions match the signal's properties, so no artificial discontinuities are introduced. The coefficient decay rate is then determined by the intrinsic smoothness of the function. Applying a DCT would impose an artificial even symmetry, which for a general periodic function (e.g., a sine wave) would introduce a discontinuity in the derivative at the boundary, limiting its performance.\n**Verdict: Correct.**\n\n**C. For uniform time-domain subsampling in compressed sensing, the mutual coherence between the sampling basis and the transform basis is identical for the DFT and the DCT, so the transform choice cannot affect recoverability; only the sampling pattern matters.**\nThis statement is false. The mutual coherence between the canonical basis (time-domain sampling) and the orthonormal DFT basis is $\\mu = 1/\\sqrt{N}$. For the orthonormal DCT-II basis, the coherence is $\\mu = \\sqrt{2/N}$ (for $N$ not small). These are not identical. Furthermore, compressed sensing performance depends critically on the sparsity $S$ of the signal in the chosen basis, not just on coherence. The entire purpose of selecting a good transform is to minimize $S$.\n**Verdict: Incorrect.**\n\n**D. For a fixed coefficient budget $m$, truncated DCT expansions exhibit reduced boundary Gibbs oscillations compared to truncated DFT expansions when discontinuities are confined to the endpoints, so fewer large coefficients are required by the DCT to achieve a target mean-squared error.**\nThis is a correct consequence of the faster coefficient decay. The Gibbs phenomenon is a visual artifact of the slow $O(k^{-1})$ decay of Fourier coefficients for discontinuous functions. Since the DCT's even extension creates a continuous function, the convergence of the partial sum is much better, and Gibbs-like oscillations at the endpoints are greatly reduced. The mean-squared error, which is the sum of the squared magnitudes of the discarded coefficients, converges as $O(m^{-3})$ for the DCT ($O(k^{-2})$ decay) but only as $O(m^{-1})$ for the DFT ($O(k^{-1})$ decay), meaning the DCT requires far fewer coefficients for the same error.\n**Verdict: Correct.**\n\n**E. In $2$-dimensional images with sharp edges that meet and cross the image frame boundary, a separable $2$-dimensional DCT tends to produce sparser coefficient sets than a separable $2$-dimensional DFT under the same sampling, because the even-symmetric extension mitigates wrap-around discontinuities along the frame boundary.**\nThis is the correct generalization of principle A to two dimensions. A separable 2D DFT's periodic wrap-around creates strong artificial edge discontinuities for typical images, spreading energy across the spectrum. The 2D DCT's reflective boundary conditions are a much better model for the edges of natural images, leading to superior energy compaction and sparsity. This is why the DCT is the foundation of JPEG.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ABDE}$$"
        },
        {
            "introduction": "Building on the principles of boundary conditions, we now delve into a more profound, information-theoretic reason for the success of the DCT in applications like image compression . This exercise models a smooth image block to compare the decorrelation efficiency of the DFT and DCT. You will use the concept of Shannon entropy to reason about which transform better approximates the optimal Karhunen–Loève Transform (KLT), thereby achieving a more compact and sparse representation.",
            "id": "3478654",
            "problem": "Consider a vectorized image block modeled as a zero-mean, wide-sense stationary Gaussian random field that is smooth in the sense of strong positive correlation. Specifically, let $\\mathbf{x} \\in \\mathbb{R}^{n}$ be the vectorization of an $N \\times N$ block with $n = N^{2}$, and let its covariance be $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite and Toeplitz-block-Toeplitz, induced by an isotropic first-order autoregressive field with correlation parameter $\\rho \\in (0,1)$ close to $1$. Define two orthonormal transforms applied separably in two dimensions:\n\n- The two-dimensional Discrete Fourier Transform (DFT) under periodic boundary conditions, represented by a unitary matrix $\\mathbf{F} \\in \\mathbb{C}^{n \\times n}$, yielding coefficients $\\mathbf{u} = \\mathbf{F} \\mathbf{x}$.\n- The two-dimensional Discrete Cosine Transform Type-II (DCT-II) under even-symmetric boundary conditions, represented by a real orthonormal matrix $\\mathbf{C} \\in \\mathbb{R}^{n \\times n}$, yielding coefficients $\\mathbf{v} = \\mathbf{C} \\mathbf{x}$.\n\nSuppose the coefficients are coded using an independent, per-coefficient model: each coefficient is treated as an independent scalar Gaussian, and the total coding cost proxy is the sum of the Shannon differential entropies of the marginal coefficient distributions before quantization, that is, $\\sum_{i=1}^{n} h(u_{i})$ for the DFT and $\\sum_{i=1}^{n} h(v_{i})$ for the DCT, where $h(\\cdot)$ denotes the Shannon differential entropy of a continuous random variable. Assume identical high-rate uniform quantization step across coefficients is used later, but focus solely on the continuous distributions prior to quantization.\n\nFor smooth blocks with $\\rho \\approx 1$, which transform yields a lower total marginal entropy under this independent model, and why?\n\nChoose the best answer:\n\nA. The DCT yields lower total marginal entropy because its even-symmetric boundary conditions reduce edge discontinuities, concentrating energy into low spatial frequencies and better approximating the Karhunen–Loève Transform (KLT), which more nearly diagonalizes the covariance for free-boundary blocks, thus lowering the sum of marginal entropies.\n\nB. The DFT yields lower total marginal entropy because it exactly diagonalizes Toeplitz-block-Toeplitz covariances for any block, producing independent coefficients with minimal entropy.\n\nC. Both transforms yield identical total marginal entropy because all orthonormal transforms preserve differential entropy exactly, so the sum of marginal entropies is invariant.\n\nD. The DFT yields lower total marginal entropy because its complex-valued coefficients encode phase, thereby reducing uncertainty compared to the DCT’s real coefficients.",
            "solution": "The problem statement has been validated as scientifically grounded, well-posed, and objective. It poses a standard, non-trivial question in the field of transform coding and sparse representations. We may proceed with the solution.\n\nThe core of the problem is to compare the efficacy of two transforms, the Discrete Fourier Transform (DFT) and the Discrete Cosine Transform Type-II (DCT-II), in terms of decorrelation for a specific signal model. The chosen metric is the sum of the marginal differential entropies of the transform coefficients.\n\nLet $\\mathbf{x} \\in \\mathbb{R}^{n}$ be the zero-mean Gaussian random vector representing the vectorized image block, with covariance matrix $\\mathbf{K} = E[\\mathbf{x}\\mathbf{x}^T]$. Let $\\mathbf{A}$ be a generic orthonormal transform matrix. The transformed coefficients are given by the vector $\\mathbf{y} = \\mathbf{A}\\mathbf{x}$. Since $\\mathbf{x}$ is a Gaussian vector, $\\mathbf{y}$ is also a Gaussian vector with zero mean and covariance matrix $\\mathbf{K_y} = \\mathbf{A}\\mathbf{K}\\mathbf{A}^H$.\n\nThe total coding cost proxy is given by the sum of the marginal entropies, $\\sum_{i=1}^{n} h(y_i)$. For a zero-mean Gaussian random variable $y_i$ with variance $\\sigma_i^2 = (\\mathbf{K_y})_{ii}$, the differential entropy is $h(y_i) = \\frac{1}{2} \\log_2(2\\pi e \\sigma_i^2)$.\n\nWe relate this sum to the joint entropy of the vector $\\mathbf{y}$, which is $h(\\mathbf{y})$. The relationship is given by:\n$$ \\sum_{i=1}^{n} h(y_i) = h(\\mathbf{y}) + I(\\mathbf{y}) $$\nwhere $I(\\mathbf{y})$ is the mutual information among the components of $\\mathbf{y}$. $I(\\mathbf{y}) \\ge 0$, with equality if and only if the components $y_i$ are mutually independent.\n\nThe joint entropy of a Gaussian vector $\\mathbf{y}$ is $h(\\mathbf{y}) = \\frac{1}{2} \\log_2((2\\pi e)^n \\det(\\mathbf{K_y}))$. Since $\\mathbf{A}$ is a unitary/orthonormal transform, $|\\det(\\mathbf{A})|=1$, and thus $\\det(\\mathbf{K_y}) = \\det(\\mathbf{A}\\mathbf{K}\\mathbf{A}^H) = |\\det(\\mathbf{A})|^2\\det(\\mathbf{K}) = \\det(\\mathbf{K})$. This means the joint entropy is invariant under the transform: $h(\\mathbf{y}) = h(\\mathbf{x})$.\n\nSince $h(\\mathbf{y})$ is constant for any orthonormal transform applied to $\\mathbf{x}$, minimizing the sum of marginal entropies $\\sum_{i=1}^{n} h(y_i)$ is equivalent to minimizing the mutual information $I(\\mathbf{y})$. Mutual information measures the statistical dependence among the coefficients. Therefore, the goal is to find the transform that produces the most decorrelated (and for Gaussian sources, the most independent) coefficients.\n\nThe optimal transform for decorrelating any random vector is the Karhunen–Loève Transform (KLT). The basis vectors of the KLT are the eigenvectors of the covariance matrix $\\mathbf{K}$. The KLT diagonalizes the covariance matrix, producing perfectly uncorrelated coefficients, for which $I(\\mathbf{y}_{KLT}) = 0$.\n\nThe problem states that the signal $\\mathbf{x}$ has a covariance matrix $\\mathbf{K}$ that is Toeplitz-block-Toeplitz (TBT), arising from an isotropic first-order autoregressive (AR($1$)) field with correlation parameter $\\rho \\approx 1$. For such a signal model, the KLT is not a standard, signal-independent transform like the DFT or DCT. The question then becomes: which of the two, DFT or DCT, better approximates the KLT for this specific signal model?\n\n1.  **Discrete Fourier Transform (DFT)**: The $2$D DFT, represented by the matrix $\\mathbf{F}$, exactly diagonalizes $2$D circulant-block-circulant (CBC) matrices. A TBT matrix is not a CBC matrix. The DFT implicitly assumes that the signal is periodic. When applying the DFT to a finite block, it is equivalent to periodically extending the block in both dimensions. For a smooth image block where $\\rho \\approx 1$, the values at opposite edges (e.g., right vs. left) are generally not equal. The periodic extension thus introduces a sharp discontinuity at the block boundaries. This artifact introduces significant high-frequency energy in the DFT domain, which means the DFT is not very efficient at decorrelating the signal or compacting its energy.\n\n2.  **Discrete Cosine Transform (DCT-II)**: The $2$D DCT-II, represented by the matrix $\\mathbf{C}$, is equivalent to performing a DFT on an even-symmetrically extended version of the signal block. For a smooth block, reflecting it at the boundaries typically creates a much smaller discontinuity than the periodic wraparound of the DFT. The signal and its reflection join with similar values and slopes, preserving smoothness across the extended boundary. It is a well-established result that for a $1$D AR($1$) process with $\\rho \\to 1$, the eigenvectors of the corresponding Toeplitz covariance matrix asymptotically approach the basis vectors of the DCT-II. This property extends to the $2$D separable case. Therefore, for smooth signals ($\\rho \\approx 1$), the DCT-II provides a remarkably good approximation to the true KLT.\n\nBecause the DCT-II better approximates the KLT for the given signal model, it performs a much better decorrelation of the data compared to the DFT. This means the mutual information among the DCT coefficients, $I(\\mathbf{v})$, will be significantly smaller than the mutual information among the DFT coefficients, $I(\\mathbf{u})$. Consequently, since $h(\\mathbf{u}) = h(\\mathbf{v}) = h(\\mathbf{x})$, we have:\n$$ \\sum_{i=1}^{n} h(v_i) = h(\\mathbf{v}) + I(\\mathbf{v})  h(\\mathbf{u}) + I(\\mathbf{u}) = \\sum_{i=1}^{n} h(u_i) $$\nThus, the DCT yields a lower total marginal entropy.\n\nNow we evaluate the given options.\n\n**A. The DCT yields lower total marginal entropy because its even-symmetric boundary conditions reduce edge discontinuities, concentrating energy into low spatial frequencies and better approximating the Karhunen–Loève Transform (KLT), which more nearly diagonalizes the covariance for free-boundary blocks, thus lowering the sum of marginal entropies.**\nThis statement is a comprehensive and accurate summary of the reasoning derived above. The DCT's implicit even-symmetric boundary condition is better suited for smooth blocks than the DFT's periodic one. This superior boundary handling leads to better energy compaction (concentration in low frequencies) because the DCT basis functions are a better approximation to the optimal KLT basis functions for this signal type. As explained, a better approximation to the KLT leads to more decorrelated coefficients, which in turn minimizes the sum of marginal entropies.\n**Verdict: Correct.**\n\n**B. The DFT yields lower total marginal entropy because it exactly diagonalizes Toeplitz-block-Toeplitz covariances for any block, producing independent coefficients with minimal entropy.**\nThis statement is factually incorrect. The DFT basis vectors (complex exponentials) are the eigenvectors of circulant matrices, not general Toeplitz matrices. A $2$D DFT diagonalizes circulant-block-circulant matrices, which arise from doubly periodic processes. The covariance matrix $\\mathbf{K}$ in the problem is Toeplitz-block-Toeplitz, which corresponds to a process on a finite grid with free boundaries and is not diagonalized by the DFT. Therefore, the DFT coefficients $\\mathbf{u}$ are not independent.\n**Verdict: Incorrect.**\n\n**C. Both transforms yield identical total marginal entropy because all orthonormal transforms preserve differential entropy exactly, so the sum of marginal entropies is invariant.**\nThis statement confuses joint entropy with the sum of marginal entropies. While it is true that orthonormal transforms preserve the *joint* differential entropy ($h(\\mathbf{u}) = h(\\mathbf{v}) = h(\\mathbf{x})$), they do not preserve the *sum* of the *marginal* entropies. The sum of marginal entropies depends on the residual correlation between coefficients, as captured by the mutual information term $I(\\mathbf{y})$. Different transforms achieve different degrees of decorrelation, leading to different values for this sum.\n**Verdict: Incorrect.**\n\n**D. The DFT yields lower total marginal entropy because its complex-valued coefficients encode phase, thereby reducing uncertainty compared to the DCT’s real coefficients.**\nThis reasoning is not sound. The measure of uncertainty is entropy, which depends on the probability distribution of the coefficients (specifically, their variances), not on whether they are real or complex. The argument that encoding \"phase\" reduces uncertainty is unfounded in information theory. The core issue is decorrelation efficiency, which is determined by how well the transform's basis functions match the signal's statistical properties (via the KLT), not by the algebraic field ($\\mathbb{R}$ or $\\mathbb{C}$) of the coefficients.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Real-world signals often contain frequency components that do not perfectly align with the basis vectors of our chosen transform, leading to a phenomenon called spectral leakage that degrades sparsity. This final practice introduces windowing, a powerful technique for actively engineering a signal's spectral representation to mitigate this issue . You will investigate the classic trade-off between a window's mainlobe width and its sidelobe suppression, learning how to select a window to enhance the sparsity of a signal's spectrum.",
            "id": "3478607",
            "problem": "Consider a length-$N$ discrete-time real sinusoid \n$\nx[n] = \\cos\\!\\big(2\\pi (k_0+\\delta)\\tfrac{n}{N}\\big),\\quad n=0,1,\\dots,N-1,\n$\nwith integer $k_0$ and fractional bin offset $\\delta\\in(0,1)$, so that the sinusoid is off the discrete Fourier transform (DFT) grid. To mitigate spectral leakage in the DFT domain, the signal is multiplied pointwise by a length-$N$ raised-cosine window \n$\nw[n] = \\alpha - (1-\\alpha)\\cos\\!\\big(2\\pi \\tfrac{n}{N-1}\\big),\\quad \\alpha\\in(0,1),\n$\nyielding the windowed signal $y[n]=w[n]x[n]$. Two special cases are the Hann window ($\\alpha = 0.5$) and the Hamming window (classically $\\alpha\\approx 0.54$). Let $Y[k]$ denote the $N$-point DFT of $y[n]$. You may assume $N$ is large enough that asymptotic approximations based on the discrete-time Fourier transform and Dirichlet kernels are accurate to leading order.\n\nDefine a sparsity surrogate that counts the number of DFT coefficients whose magnitude exceeds a relative threshold $\\tau\\in(0,1)$ with respect to the unwindowed sinusoid’s on-grid peak, \n$\nS_\\tau(y) := \\big|\\{\\,k\\in\\{0,\\dots,N-1\\}:\\ |Y[k]|\\ge \\tau\\,\\max_k |X[k]|\\ \\}\\big|.\n$\nHere $X[k]$ is the $N$-point DFT of $x[n]$ without windowing. Starting from the fundamental facts that (i) time-domain multiplication corresponds to frequency-domain convolution, and (ii) the discrete-time Fourier transform of a length-$N$ rectangular window is a Dirichlet kernel \n$\nD_N(\\omega)=e^{-j\\omega\\frac{N-1}{2}}\\frac{\\sin(\\frac{N\\omega}{2})}{\\sin(\\frac{\\omega}{2})},\n$\nreason about the mainlobe width and sidelobe behavior of $W(\\omega)$, the discrete-time Fourier transform of $w[n]$, and their impact on $Y[k]$ and $S_\\tau(y)$.\n\nSelect all statements that are correct, and justify your choices by deriving them from the above principles.\n\nA. For the one-term raised-cosine family $w[n]=\\alpha-(1-\\alpha)\\cos(2\\pi \\tfrac{n}{N-1})$, the magnitude $|W(\\omega)|$ is a linear combination of three Dirichlet kernels centered at $\\omega=0$ and $\\omega=\\pm \\tfrac{2\\pi}{N-1}$; as $N\\to\\infty$, the null-to-null mainlobe width of $|W(\\omega)|$ (and hence of the dominant lobe of $|Y[k]|$ for a single sinusoid) approaches approximately $4$ DFT bins for all $\\alpha\\in(0,1)$, while choosing $\\alpha\\approx 0.54$ reduces the first sidelobe level below that of $\\alpha=0.5$.\n\nB. For a fixed off-grid sinusoid and $\\tau$ in a moderate range (e.g., $\\tau\\in[0.03,0.2]$), the Hamming choice $\\alpha\\approx 0.54$ typically yields smaller $S_\\tau(y)$ than the Hann choice $\\alpha=0.5$ because its first sidelobe is lower while the mainlobe width is comparable; however, for very small $\\tau$ the Hann window can produce smaller $S_\\tau(y)$ because its sidelobes decay faster with frequency offset.\n\nC. For any $\\tau0$ and any off-grid offset $\\delta\\in(0,1)$, the rectangular window (no tapering) minimizes $S_\\tau(y)$ among all $\\alpha\\in(0,1)$ within the one-term raised-cosine family.\n\nD. Switching from the DFT to the type-II discrete cosine transform (DCT-II) eliminates leakage for off-grid sinusoidal frequencies and therefore strictly improves sparsity $S_\\tau(y)$ for any choice of window.\n\nE. For time-domain sparsity (spikes) and a randomly subsampled Fourier measurement operator, pre-multiplying by a Hamming window reduces the mutual coherence between the sensing matrix and the identity sparsifying basis, thereby improving basis pursuit guarantees in the sense of coherence bounds.\n\nChoose all that apply.",
            "solution": "This problem is scientifically grounded, well-posed, and objective. It tests core concepts in digital signal processing related to windowing and spectral analysis. The provided information is sufficient to proceed with a solution.\n\n### Fundamental Analysis\n\nThe spectrum of the windowed signal $y[n] = w[n]x[n]$ is the convolution of the window spectrum $W(\\omega)$ and the signal spectrum $X(\\omega)$. Since $x[n]$ is a single sinusoid, its spectrum $X(\\omega)$ is ideally two impulses. Due to the finite length, its Discrete-Time Fourier Transform (DTFT) consists of two Dirichlet kernels. The spectrum of the windowed signal, $Y(\\omega)$, will therefore have its shape dictated by the window's spectrum, $W(\\omega)$, centered at the sinusoid's frequency.\n\nThe window is $w[n] = \\alpha - (1-\\alpha)\\cos(2\\pi \\frac{n}{N-1})$ on $n \\in [0, N-1]$. Using Euler's identity, we can express this as a sum of three components: a constant and two complex exponentials. The DTFT of this windowed window function is thus a sum of three shifted Dirichlet kernels:\n$$\nW(\\omega) = \\alpha D_N(\\omega) - \\frac{1-\\alpha}{2} D_N\\Big(\\omega - \\frac{2\\pi}{N-1}\\Big) - \\frac{1-\\alpha}{2} D_N\\Big(\\omega + \\frac{2\\pi}{N-1}\\Big)\n$$\nThis linear combination is designed to make the sidelobes of the central Dirichlet kernel cancel with the mainlobes of the two shifted kernels, reducing overall sidelobe levels at the expense of widening the mainlobe.\n\n### Evaluation of Options\n\n**A. For the one-term raised-cosine family $w[n]=\\alpha-(1-\\alpha)\\cos(2\\pi \\tfrac{n}{N-1})$, the magnitude $|W(\\omega)|$ is a linear combination of three Dirichlet kernels centered at $\\omega=0$ and $\\omega=\\pm \\tfrac{2\\pi}{N-1}$; as $N\\to\\infty$, the null-to-null mainlobe width of $|W(\\omega)|$ (and hence of the dominant lobe of $|Y[k]|$ for a single sinusoid) approaches approximately $4$ DFT bins for all $\\alpha\\in(0,1)$, while choosing $\\alpha\\approx 0.54$ reduces the first sidelobe level below that of $\\alpha=0.5$.**\nThis statement is correct.\n1. The derivation above shows $W(\\omega)$ is a sum of three shifted Dirichlet kernels.\n2. The mainlobe of a single Dirichlet kernel (rectangular window) has a null-to-null width of $2$ DFT bins (spanning from $-2\\pi/N$ to $2\\pi/N$). The combination of three such kernels, with shifts of $\\pm 2\\pi/(N-1) \\approx \\pm 2\\pi/N$, effectively doubles the mainlobe width to approximately $4$ DFT bins. This is a standard property of the Hann/Hamming family.\n3. The choice $\\alpha=0.5$ (Hann) results in a peak sidelobe of about $-32$ dB. The choice $\\alpha \\approx 0.54$ (Hamming) is specifically optimized to place the first nulls of the combined kernels to cancel the first sidelobe as much as possible, resulting in a peak sidelobe of about $-43$ dB. Thus, the Hamming window reduces the first/highest sidelobe level compared to Hann.\n**Verdict: Correct.**\n\n**B. For a fixed off-grid sinusoid and $\\tau$ in a moderate range (e.g., $\\tau\\in[0.03,0.2]$), the Hamming choice $\\alpha\\approx 0.54$ typically yields smaller $S_\\tau(y)$ than the Hann choice $\\alpha=0.5$ because its first sidelobe is lower while the mainlobe width is comparable; however, for very small $\\tau$ the Hann window can produce smaller $S_\\tau(y)$ because its sidelobes decay faster with frequency offset.**\nThis statement correctly describes the practical trade-off. The sparsity surrogate $S_\\tau(y)$ counts coefficients above a threshold.\n1. A moderate threshold $\\tau \\in [0.03, 0.2]$ corresponds to a level between approximately $-31$ dB and $-14$ dB. The Hann window's highest sidelobe (at $-32$ dB) might just be caught by this threshold, while the Hamming window's sidelobes (all below $-43$ dB) will not. Since their mainlobes have similar widths, the Hamming window would result in fewer bins above the threshold, hence a smaller $S_\\tau(y)$.\n2. The Hann window's sidelobes roll off at $\\sim 18$ dB/octave, whereas the Hamming window's sidelobes roll off at only $\\sim 6$ dB/octave. For a very small threshold $\\tau$ (e.g., $\\tau=10^{-4}$, or $-80$ dB), the slowly decaying sidelobes of the Hamming window will mean many more frequency bins exceed the threshold compared to the rapidly decaying sidelobes of the Hann window.\n**Verdict: Correct.**\n\n**C. For any $\\tau>0$ and any off-grid offset $\\delta\\in(0,1)$, the rectangular window (no tapering) minimizes $S_\\tau(y)$ among all $\\alpha\\in(0,1)$ within the one-term raised-cosine family.**\nThis is incorrect. The rectangular window (no tapering) produces the most severe spectral leakage. Its DTFT, the Dirichlet kernel, has very high sidelobes (the first is only $-13.2$ dB down). Tapering windows like Hann and Hamming are specifically designed to suppress these sidelobes to *reduce* leakage. For any reasonable threshold $\\tau$, the rectangular window will produce a much larger number of coefficients above the threshold, thus maximizing, not minimizing, $S_\\tau(y)$.\n**Verdict: Incorrect.**\n\n**D. Switching from the DFT to the type-II discrete cosine transform (DCT-II) eliminates leakage for off-grid sinusoidal frequencies and therefore strictly improves sparsity $S_\\tau(y)$ for any choice of window.**\nThis is incorrect. The DCT-II is highly effective at reducing leakage for sinusoids because its basis functions are cosines, and its implicit even-symmetric extension is a good fit. However, it does not \"eliminate\" leakage unless the sinusoid's frequency and phase happen to match a DCT-II basis function perfectly. For a general off-grid sinusoid, some leakage remains, although it's typically much less than with the DFT. The claim of \"elimination\" is too strong.\n**Verdict: Incorrect.**\n\n**E. For time-domain sparsity (spikes) and a randomly subsampled Fourier measurement operator, pre-multiplying by a Hamming window reduces the mutual coherence between the sensing matrix and the identity sparsifying basis, thereby improving basis pursuit guarantees in the sense of coherence bounds.**\nThis is incorrect. The problem describes measuring a signal $x$ that is sparse in the identity basis ($\\Psi = I$). The measurement process is $b = (R \\cdot F \\cdot W)x$, where $R$ is the random subsampling operator, $F$ is the DFT matrix, and $W$ is a diagonal matrix of the window coefficients. The effective sensing matrix is $A_{eff} = RFW$. The columns of this matrix are $a_j = RFW e_j = w_j R F e_j$, where $w_j$ is the $j$-th window coefficient and $e_j$ is the $j$-th canonical basis vector. The mutual coherence involves normalized inner products of these columns. Since $w_j$ is a scalar, it factors out of the inner product and the norm, and then cancels:\n$$ \\mu_{ij} = \\frac{|\\langle a_i, a_j \\rangle|}{\\|a_i\\|_2 \\|a_j\\|_2} = \\frac{|w_i^* w_j \\langle RFe_i, RFe_j \\rangle|}{|w_i|\\|RFe_i\\|_2 |w_j|\\|RFe_j\\|_2} = \\frac{|\\langle RFe_i, RFe_j \\rangle|}{\\|RFe_i\\|_2 \\|RFe_j\\|_2} $$\nThe window coefficients cancel out, meaning the coherence of the system is unchanged by the pre-multiplication. The windowing actually harms recovery in this case, as it makes the columns of $A_{eff}$ non-uniform in magnitude, which is generally undesirable.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AB}$$"
        }
    ]
}