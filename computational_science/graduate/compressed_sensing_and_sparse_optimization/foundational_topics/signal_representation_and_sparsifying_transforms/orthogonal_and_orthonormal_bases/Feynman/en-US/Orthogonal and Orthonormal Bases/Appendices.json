{
    "hands_on_practices": [
        {
            "introduction": "The concept of orthogonality is geometrically intuitive, representing perpendicularity between vectors. This fundamental exercise bridges the geometric picture with the algebraic definition, using the inner product to determine if two vectors are orthogonal, especially in the context of numerical tolerances common in real-world computations . It serves as a crucial first step in understanding how the columns of sensing matrices are evaluated for this key property.",
            "id": "3464396",
            "problem": "Let $n \\in \\mathbb{N}$ and consider a sensing dictionary in compressed sensing whose columns aim to form an orthonormal basis in $\\mathbb{R}^{n}$. Orthogonality of columns is fundamental to low mutual coherence and stable sparse recovery: two columns $u$ and $v$ are orthogonal if their inner product is zero and they are orthonormal if, in addition, each has unit norm. From the definition of the Euclidean inner product and the Euclidean norm, and invoking the Cauchy–Schwarz inequality, the principal angle $\\theta \\in [0,\\pi]$ between two non-zero vectors $u,v \\in \\mathbb{R}^{n}$ is defined by the relation between the inner product and the norms. \n\nYou are given $n=8$ and the two column vectors\n$$\nu=\\begin{pmatrix}\n1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{pmatrix},\\qquad\nv=\\begin{pmatrix}\n2\\times 10^{-4} \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\nUsing only first principles—namely, the definition of the Euclidean inner product and the Euclidean norm together with the equality case of the Cauchy–Schwarz inequality—derive the expression for the principal angle $\\theta$ between $u$ and $v$ and compute its value. In addition, decide whether $u$ and $v$ are orthogonal up to the numerical tolerance $\\tau=3\\times 10^{-4}$, meaning whether the normalized inner product magnitude satisfies\n$$\n\\frac{|u^{\\top}v|}{\\|u\\|_{2}\\,\\|v\\|_{2}} \\le \\tau.\n$$\nExpress the angle $\\theta$ in radians and round your numerical answer to six significant figures. The final answer must be the value of $\\theta$ only.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard exercise in linear algebra with direct relevance to concepts in compressed sensing. All necessary information is provided, and the problem is free of contradictions or ambiguities.\n\nThe task is to derive and compute the principal angle $\\theta$ between two vectors $u, v \\in \\mathbb{R}^{n}$ for $n=8$, and to determine if they are orthogonal up to a specified numerical tolerance. The derivation must be based on first principles: the Euclidean inner product and the Euclidean norm.\n\nThe given vectors are:\n$$\nu=\\begin{pmatrix}\n1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{pmatrix},\\qquad\nv=\\begin{pmatrix}\n2\\times 10^{-4} \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\nThe numerical tolerance for orthogonality is given as $\\tau=3\\times 10^{-4}$.\n\nFirst, we define the necessary concepts from first principles.\nFor any two vectors $x = (x_1, \\dots, x_n)^\\top$ and $y = (y_1, \\dots, y_n)^\\top$ in $\\mathbb{R}^{n}$, the Euclidean inner product is defined as:\n$$\nx^\\top y = \\sum_{i=1}^{n} x_i y_i\n$$\nThe Euclidean norm (or $L_2$-norm) of a vector $x$ is defined as:\n$$\n\\|x\\|_{2} = \\sqrt{x^\\top x} = \\sqrt{\\sum_{i=1}^{n} x_i^2}\n$$\nThe principal angle $\\theta \\in [0, \\pi]$ between two non-zero vectors $u$ and $v$ is defined by the relation:\n$$\n\\cos(\\theta) = \\frac{u^\\top v}{\\|u\\|_{2} \\|v\\|_{2}}\n$$\nThis definition is well-posed because the Cauchy-Schwarz inequality, $|u^\\top v| \\le \\|u\\|_{2} \\|v\\|_{2}$, guarantees that the right-hand side is always in the interval $[-1, 1]$.\n\nWe now apply these definitions to the given vectors $u$ and $v$.\n\n1.  Calculate the inner product $u^\\top v$:\n$$\nu^\\top v = (1)(2\\times 10^{-4}) + (0)(1) + (0)(0) + (0)(0) + (0)(0) + (0)(0) + (0)(0) + (0)(0)\n$$\n$$\nu^\\top v = 2 \\times 10^{-4}\n$$\n\n2.  Calculate the Euclidean norm of $u$:\n$$\n\\|u\\|_{2} = \\sqrt{1^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2} = \\sqrt{1} = 1\n$$\n\n3.  Calculate the Euclidean norm of $v$:\n$$\n\\|v\\|_{2} = \\sqrt{(2\\times 10^{-4})^2 + 1^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2}\n$$\n$$\n\\|v\\|_{2} = \\sqrt{4 \\times 10^{-8} + 1}\n$$\n\nNow, we substitute these results into the formula for $\\cos(\\theta)$:\n$$\n\\cos(\\theta) = \\frac{2 \\times 10^{-4}}{1 \\cdot \\sqrt{1 + 4 \\times 10^{-8}}} = \\frac{2 \\times 10^{-4}}{\\sqrt{1.00000004}}\n$$\nTo find the angle $\\theta$, we take the arccosine of this value:\n$$\n\\theta = \\arccos\\left(\\frac{2 \\times 10^{-4}}{\\sqrt{1.00000004}}\\right)\n$$\nLet's compute the numerical value. The argument of the arccosine is:\n$$\n\\frac{2 \\times 10^{-4}}{1.00000002} \\approx 1.99999996 \\times 10^{-4}\n$$\nSince this value is very close to $0$, the angle $\\theta$ will be very close to $\\frac{\\pi}{2}$.\n$$\n\\theta = \\arccos(1.9999999600... \\times 10^{-4}) \\approx 1.57059632683 \\text{ radians}\n$$\nRounding to six significant figures, we get:\n$$\n\\theta \\approx 1.57060 \\text{ radians}\n$$\n\nNext, we must decide whether $u$ and $v$ are orthogonal up to the numerical tolerance $\\tau=3\\times 10^{-4}$. The condition is:\n$$\n\\frac{|u^{\\top}v|}{\\|u\\|_{2}\\,\\|v\\|_{2}} \\le \\tau\n$$\nThe left-hand side of this inequality is precisely $|\\cos(\\theta)|$. So, we need to check if $|\\cos(\\theta)| \\le \\tau$.\nWe have:\n$$\n|\\cos(\\theta)| = \\left| \\frac{2 \\times 10^{-4}}{\\sqrt{1 + 4 \\times 10^{-8}}} \\right| \\approx 1.99999996 \\times 10^{-4}\n$$\nThe tolerance is $\\tau = 3 \\times 10^{-4}$.\nWe compare the two values:\n$$\n1.99999996 \\times 10^{-4} \\le 3 \\times 10^{-4}\n$$\nThis inequality is true. Therefore, the vectors $u$ and $v$ are considered orthogonal up to the given tolerance $\\tau$. The problem, however, only asks for the value of $\\theta$ in the final answer.",
            "answer": "$$ \\boxed{1.57060} $$"
        },
        {
            "introduction": "While verifying orthogonality is essential, constructing an orthonormal basis from an arbitrary set of linearly independent vectors is a cornerstone of numerical linear algebra. This practice guides you through implementing the QR factorization using Householder reflections, a stable and powerful method to build the orthonormal columns of $Q$ that are fundamental to many algorithms in signal processing and optimization . By building this tool from first principles, you will gain a deeper appreciation for the structure and uniqueness of orthonormal systems when applied to full-rank matrices.",
            "id": "3464402",
            "problem": "You are given a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$. Construct a program that computes an orthogonal-triangular (QR) factorization of $A$ using Householder reflections, producing $A = Q R$ where $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns and $R \\in \\mathbb{R}^{n \\times n}$ is upper triangular. Your program must be designed from the following foundational base:\n- A matrix $Q$ has orthonormal columns if and only if $Q^\\top Q = I_n$, where $I_n$ is the $n \\times n$ identity matrix.\n- A Householder reflection is a linear transformation that reflects vectors across a hyperplane orthogonal to a unit vector. Such a transformation $H$ satisfies $H^\\top H = I$ and $H = H^\\top$.\n- The product of orthogonal matrices is orthogonal.\n- Applying successive reflections to eliminate subdiagonal entries in a column yields an upper triangular form.\n\nYour program must:\n1. Construct the factorization $A = Q R$ by successively applying Householder reflections to eliminate subdiagonal entries, without using any library function that directly returns a QR factorization of $A$ for the primary construction.\n2. Enforce the convention that the diagonal entries of $R$ are nonnegative whenever they are nonzero by appropriately adjusting the signs of columns of $Q$ and the corresponding rows of $R$ (so that $A = Q R$ remains valid).\n3. Verify the following properties for each test case:\n   - Orthonormality: $Q^\\top Q \\approx I_n$ within a specified tolerance $t$.\n   - Upper triangularity: All entries of $R$ strictly below the main diagonal have absolute value at most $t$.\n   - Uniqueness when full column rank: If $\\operatorname{rank}(A) = n$, then $R$ with the nonnegative diagonal convention is uniquely determined. Numerically verify this by independently computing another orthogonal-triangular factorization $\\tilde{Q} \\tilde{R}$ of the same $A$ (using any correct and distinct method from your Householder construction), enforcing the same nonnegative diagonal convention on $\\tilde{R}$, and checking whether $R \\approx \\tilde{R}$ within the tolerance $t$. If $\\operatorname{rank}(A) < n$, set this uniqueness indicator to the boolean value $False$.\n4. Use the absolute tolerance $t = 10^{-10}$ for all approximate comparisons, and use radians for any angle computations if they were needed (none are explicitly required here).\n5. Apply your program to the following test suite of matrices, each specified as a list of rows (with entries given as real numbers); for each, $m \\ge n$:\n   - Test $1$: $A \\in \\mathbb{R}^{6 \\times 4}$,\n     $$\n     A = \\begin{bmatrix}\n     2 & -1 & 0 & 3\\\\\n     0 & 4 & 1 & -2\\\\\n     1 & 2 & 0 & 0\\\\\n     3 & -3 & 5 & 1\\\\\n     0 & 2 & -1 & 4\\\\\n     1 & 0 & 2 & -1\n     \\end{bmatrix}.\n     $$\n   - Test $2$: $A \\in \\mathbb{R}^{4 \\times 4}$ (Vandermonde-type with distinct nodes),\n     $$\n     A = \\begin{bmatrix}\n     1 & 1 & 1 & 1\\\\\n     1 & 2 & 4 & 8\\\\\n     1 & 3 & 9 & 27\\\\\n     1 & 4 & 16 & 64\n     \\end{bmatrix}.\n     $$\n   - Test $3$: $A \\in \\mathbb{R}^{5 \\times 3}$ with linearly dependent columns (rank equal to $2$),\n     $$\n     A = \\begin{bmatrix}\n     1 & 0 & 1\\\\\n     0 & 1 & 1\\\\\n     2 & 1 & 3\\\\\n     0 & 3 & 3\\\\\n     1 & 0 & 1\n     \\end{bmatrix}.\n     $$\n   - Test $4$: $A \\in \\mathbb{R}^{3 \\times 1}$ (a single nonzero column),\n     $$\n     A = \\begin{bmatrix}\n     3\\\\\n     4\\\\\n     0\n     \\end{bmatrix}.\n     $$\n   - Test $5$: $A \\in \\mathbb{R}^{7 \\times 5}$,\n     $$\n     A = \\begin{bmatrix}\n     1 & 0 & 2 & 1 & -1\\\\\n     0 & 1 & 1 & -1 & 2\\\\\n     2 & 1 & 0 & 3 & 1\\\\\n     1 & -1 & 3 & 0 & 2\\\\\n     0 & 2 & -1 & 1 & 1\\\\\n     3 & 0 & 1 & 2 & 0\\\\\n     1 & 1 & 1 & 1 & 1\n     \\end{bmatrix}.\n     $$\n6. For each test case, produce a list with three boolean outputs in the order: orthonormality check, upper triangularity check, uniqueness check as defined above. Aggregate the results for all test cases into a single line of output containing the results as a comma-separated list of these three-boolean lists, enclosed in square brackets. For example, the final output must look like\n   $$\n   [ [b_{1,1}, b_{1,2}, b_{1,3}], [b_{2,1}, b_{2,2}, b_{2,3}], \\dots ]\n   $$\n   but printed in a single line with no additional text. Here each $b_{i,j}$ is either the literal $True$ or $False$.\n\nYour implementation should be numerically stable and adhere to the specified tolerance. The intended context for this construction is to emphasize the role of orthogonal and orthonormal bases in compressed sensing and sparse optimization, where stable orthogonal factorizations are used to form numerically robust bases for column spaces and to analyze identifiability and uniqueness properties under full column rank. No physical units are involved in this problem. Angles should be in radians if any trigonometric operations are used.",
            "solution": "The problem of constructing an orthogonal-triangular ($QR$) factorization of a matrix $A \\in \\mathbb{R}^{m \\times n}$ where $m \\ge n$ is a fundamental task in numerical linear algebra. The desired factorization is $A = QR$, where $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns (i.e., $Q^\\top Q = I_n$) and $R \\in \\mathbb{R}^{n \\times n}$ is an upper triangular matrix. The solution is developed by implementing the method of Householder reflections, adhering to the principles outlined in the problem statement.\n\n### Principle-Based Algorithmic Design\n\nThe algorithm is designed based on the following core principles:\n\n1.  **Decomposition via Orthogonal Transformations**: The core idea is to find a sequence of orthogonal matrices $H_0, H_1, \\dots, H_{n-1}$ that, when applied to $A$, progressively introduce zeros below the main diagonal, transforming $A$ into an upper trapezoidal form.\n    $$\n    R_{m \\times n} = H_{n-1} \\dots H_1 H_0 A\n    $$\n    Since each $H_j$ is orthogonal ($H_j^\\top H_j = I$), their product is also orthogonal. The full orthogonal matrix $Q_{m \\times m}$ is given by the product of the reflections themselves, as $H_j = H_j^{-1} = H_j^\\top$.\n    $$\n    A = (H_0 H_1 \\dots H_{n-1}) R_{m \\times n}\n    $$\n    Letting $Q_{full} = H_0 H_1 \\dots H_{n-1}$, we have $A = Q_{full} R_{m \\times n}$. The \"thin\" $QR$ factorization is obtained by taking the first $n$ columns of $Q_{full}$ to form $Q \\in \\mathbb{R}^{m \\times n}$ and the first $n$ rows of $R_{m \\times n}$ to form $R \\in \\mathbb{R}^{n \\times n}$.\n\n2.  **Householder Reflections**: Each transformation $H_j$ is a Householder reflection. For a given nonzero vector $x$, a Householder reflection can map $x$ to a vector parallel to a standard basis vector, such as $e_1$. The reflection is across a hyperplane orthogonal to a vector $u$. The transformation matrix is $P = I - 2 \\frac{u u^\\top}{u^\\top u}$. For a vector $x \\in \\mathbb{R}^k$, to map it to a multiple of $e_1 \\in \\mathbb{R}^k$, the reflection vector $u$ is chosen for numerical stability as:\n    $$\n    u = x + \\text{sign}(x_1) \\|x\\|_2 e_1\n    $$\n    where $x_1$ is the first component of $x$. This choice avoids subtractive cancellation when $x$ is nearly parallel to $e_1$. This transformation maps $x$ to $-\\text{sign}(x_1) \\|x\\|_2 e_1$.\n\n3.  **Algorithmic Implementation**: The implementation consists of two main phases:\n    -   **Phase 1: Triangularization of $A$ into $R$**: The algorithm iterates from $j = 0$ to $n-1$. In each iteration $j$, we consider the sub-column vector $x = A_j[j:m, j]$, where $A_j$ is the matrix after $j$ reflections. We compute the corresponding Householder vector $v_j$ (the normalized version of $u$) of size $m-j$. This reflection is applied to the submatrix $A_j[j:m, j:n]$ to zero out the entries below the diagonal in the $j$-th column. The computed Householder vectors $\\{v_j\\}_{j=0}^{n-1}$ are stored.\n    -   **Phase 2: Formation of $Q$**: The full orthogonal matrix $Q_{full} \\in \\mathbb{R}^{m \\times m}$ is constructed by applying the transformations to the identity matrix $I_m$. Since $Q_{full} = H_0 H_1 \\dots H_{n-1}$, the transformations are applied in reverse order to what might be naively expected: $Q_{full} = H_0(H_1(\\dots(H_{n-1}I_m)\\dots))$. The implementation starts with $Q=I_m$ and successively applies the reflections $H_{n-1}, \\dots, H_0$. At step $j$ (iterating downwards from $n-1$ to $0$), the reflection defined by $v_j$ is applied to the rows $j$ through $m-1$ of the current $Q$ matrix. After all reflections are applied, the thin matrix $Q \\in \\mathbb{R}^{m \\times n}$ is extracted as the first $n$ columns.\n\n4.  **Uniqueness and Normalization**: For a full-rank matrix $A$, the $QR$ factorization is unique if the diagonal entries of $R$ are restricted (e.g., to be positive). The problem requires a convention where $R_{jj} \\ge 0$ for all $j$ where $R_{jj}$ is non-zero. Our stable Householder implementation yields diagonal entries $R_{jj}$ that can be negative. A post-processing step inspects the signs of the diagonal entries of $R$. If $R_{jj} < 0$, the $j$-th row of $R$ and the $j$-th column of $Q$ are multiplied by $-1$. This operation preserves the product $A = QR$ since $(Q D)(D^{-1} R) = Q R$ for a diagonal sign matrix $D$ with $D=D^{-1}$.\n\n5.  **Verification Logic**:\n    -   **Orthonormality**: Verified by checking if $Q^\\top Q$ is close to the identity matrix $I_n$, specifically if $\\max_{i,j} |(Q^\\top Q - I_n)_{ij}| \\le t$ for a tolerance $t=10^{-10}$.\n    -   **Upper Triangularity**: Verified by ensuring all elements of $R$ strictly below the main diagonal have an absolute value no greater than $t$.\n    -   **Uniqueness**: The uniqueness of $R$ (with the nonnegative diagonal convention) holds if and only if $A$ has full column rank. We first assess the rank of $A$ by computing its singular values. If the smallest singular value is greater than the tolerance $t$, the matrix is considered full rank. If so, a reference factorization $(\\tilde{Q}, \\tilde{R})$ is computed using a distinct, trusted library function (`scipy.linalg.qr`). After enforcing the same nonnegative diagonal convention on $\\tilde{R}$, we check if $R \\approx \\tilde{R}$ element-wise within tolerance $t$. If $A$ is not full rank, the uniqueness check result is `False`.\n\nThis structured, principle-based approach ensures a correct and numerically stable implementation of the Householder $QR$ factorization, complete with the verifications required by the problem.",
            "answer": "```python\n# The final stand-alone Python code for the problem.\nimport numpy as np\nfrom scipy.linalg import qr as scipy_qr\n\ndef householder_qr(A, tol=1e-10):\n    \"\"\"\n    Computes the thin QR factorization of a matrix A using Householder reflections.\n    A = QR, where Q is m x n with orthonormal columns and R is n x n upper triangular.\n    This function is built from first principles and does not use a library QR function.\n    \"\"\"\n    A_work = A.copy().astype(np.float64)\n    m, n = A_work.shape\n    \n    householder_vectors = []\n\n    # Phase 1: Transform A to R, storing Householder vectors.\n    for j in range(n):\n        x = A_work[j:, j]\n        norm_x = np.linalg.norm(x)\n        \n        if norm_x < tol:\n            # Column is (close to) zeroed out, no reflection needed.\n            v = np.zeros_like(x)\n            householder_vectors.append(v)\n            continue\n            \n        sign_x0 = np.copysign(1.0, x[0]) if x[0] != 0 else 1.0\n        \n        # Standard numerically stable choice for Householder vector u.\n        u = x.copy()\n        u[0] += sign_x0 * norm_x\n        \n        norm_u = np.linalg.norm(u)\n        if norm_u < tol:\n            # If u is zero vector, reflection is identity.\n            v = np.zeros_like(u)\n        else:\n            v = u / norm_u\n        \n        householder_vectors.append(v)\n        \n        # Apply reflection P = I - 2*v*v^T to the remaining submatrix of A.\n        # P * B = B - 2 * v * (v^T * B)\n        sub_matrix = A_work[j:, j:]\n        A_work[j:, j:] = sub_matrix - 2 * np.outer(v, v.T @ sub_matrix)\n        \n    R = A_work[:n, :]\n\n    # Phase 2: Form Q by applying reflections to the identity matrix.\n    # Q = H_0 * H_1 * ... * H_{n-1}. We apply them backwards to Identity.\n    Q = np.eye(m, dtype=np.float64)\n    for j in range(n - 1, -1, -1):\n        v = householder_vectors[j]\n        if np.linalg.norm(v) < tol:\n            continue\n            \n        # Apply P_j to the relevant submatrix of Q.\n        # Q_new = H_j * Q_old => P_j applies to rows j..m of Q_old.\n        sub_Q = Q[j:, :]\n        Q[j:, :] = sub_Q - 2 * np.outer(v, v.T @ sub_Q)\n        \n    return Q[:, :n], R\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    t = 1e-10\n\n    test_cases = [\n        np.array([\n            [2, -1, 0, 3], [0, 4, 1, -2], [1, 2, 0, 0], \n            [3, -3, 5, 1], [0, 2, -1, 4], [1, 0, 2, -1]\n        ], dtype=float),\n        np.array([\n            [1, 1, 1, 1], [1, 2, 4, 8], [1, 3, 9, 27], [1, 4, 16, 64]\n        ], dtype=float),\n        np.array([\n            [1, 0, 1], [0, 1, 1], [2, 1, 3], [0, 3, 3], [1, 0, 1]\n        ], dtype=float),\n        np.array([\n            [3], [4], [0]\n        ], dtype=float),\n        np.array([\n            [1, 0, 2, 1, -1], [0, 1, 1, -1, 2], [2, 1, 0, 3, 1],\n            [1, -1, 3, 0, 2], [0, 2, -1, 1, 1], [3, 0, 1, 2, 0],\n            [1, 1, 1, 1, 1]\n        ], dtype=float)\n    ]\n    \n    results = []\n    \n    for A in test_cases:\n        m, n = A.shape\n        \n        # 1. Construct QR factorization using the custom Householder function.\n        Q, R = householder_qr(A, tol=t)\n        \n        # 2. Enforce nonnegative diagonals on R.\n        for j in range(n):\n            if R[j, j] < 0 and abs(R[j, j]) > t:\n                R[j, :] *= -1\n                Q[:, j] *= -1\n        \n        # 3. Perform verification checks.\n        \n        # Check 1: Orthonormality of Q (Q^T * Q approx I_n).\n        ortho_check = np.allclose(Q.T @ Q, np.eye(n), atol=t, rtol=0) if n > 0 else True\n\n        # Check 2: Upper triangularity of R.\n        tri_check = np.max(np.abs(np.tril(R, k=-1))) <= t if n > 0 else True\n\n        # Check 3: Uniqueness of R (if A has full column rank).\n        # Determine rank by checking singular values.\n        singular_values = np.linalg.svd(A, compute_uv=False)\n        rank_is_full = (n == 0) or (len(singular_values) >= n and singular_values[n-1] > t)\n        \n        if rank_is_full:\n            # Compute a reference QR factorization using a distinct method.\n            Q_ref, R_ref = scipy_qr(A, mode='economic')\n            \n            # Enforce nonnegative diagonals on the reference factorization.\n            for j in range(n):\n                if R_ref[j, j] < 0 and abs(R_ref[j, j]) > t:\n                    R_ref[j, :] *= -1\n                    Q_ref[:, j] *= -1\n            \n            # Compare our R with the reference R.\n            uniqueness_check = np.allclose(R, R_ref, atol=t, rtol=0)\n        else:\n            uniqueness_check = False\n            \n        results.append([ortho_check, tri_check, uniqueness_check])\n\n    # 4. Format and print the final output as a single-line string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In compressed sensing, perfect orthonormality is often a luxury; more commonly, we work with dictionaries that are 'nearly' orthonormal. This simulation-based practice explores the practical consequences of this deviation, measured by mutual coherence, on the performance of the Orthogonal Matching Pursuit (OMP) algorithm . You will empirically test theoretical recovery guarantees and observe how the algorithm's reliability degrades as column correlations increase, providing a clear link between matrix properties and algorithmic success.",
            "id": "3464443",
            "problem": "You must write a complete, runnable program that constructs explicit dictionaries with columns that are nearly orthonormal, runs Orthogonal Matching Pursuit (OMP) on fixed sparse signals, and quantifies how residual correlations behave as mutual coherence increases. The analysis must be rooted in fundamental definitions from compressed sensing and sparse optimization. Specifically, use the definition of mutual coherence, the unit-norm column assumption for dictionaries, and the selection rule of Orthogonal Matching Pursuit (OMP). The goal is to measure how residual correlation with already-selected atoms and with true versus false atoms changes as the dictionary deviates from an orthonormal basis, and to compare these empirical behaviors against the mutual coherence sufficient condition for correct support selection.\n\nDefinitions that must be used:\n- A dictionary is a matrix $D \\in \\mathbb{R}^{m \\times n}$ whose columns $\\{d_j\\}_{j=1}^n$ are unit norm, i.e., $\\|d_j\\|_2 = 1$ for all $j$.\n- The mutual coherence of $D$ is $\\mu(D) = \\max_{i \\neq j} |\\langle d_i, d_j \\rangle|$, where $\\langle \\cdot, \\cdot \\rangle$ denotes the standard inner product.\n- Orthogonal Matching Pursuit (OMP) starts with residual $r_0 = y$ and iteratively selects an index $j_t$ that maximizes $|\\langle d_{j}, r_{t-1} \\rangle|$ among indices not yet selected. After selecting the index, OMP refits the coefficients on the selected set via least squares, producing a new residual $r_t = y - D_{S_t} x_{S_t}^{\\text{LS}}$, where $S_t$ is the set of selected indices and $x_{S_t}^{\\text{LS}}$ denotes the least squares solution. In exact arithmetic, the normal equations yield $D_{S_t}^\\top r_t = 0$.\n\nProgram requirements:\n1. Construct dictionaries $D$ with $m = n$ and unit-norm columns by starting from the standard basis in $\\mathbb{R}^n$ and introducing controlled pairwise interactions between selected column pairs. For a specified pair $(p,q)$, replace the columns as $d_p \\leftarrow \\frac{e_p + \\varepsilon e_q}{\\|e_p + \\varepsilon e_q\\|_2}$ and $d_q \\leftarrow \\frac{e_q + \\varepsilon e_p}{\\|e_q + \\varepsilon e_p\\|_2}$, where $e_j$ denotes the $j$-th standard basis vector and $\\varepsilon \\ge 0$ is a small real number. All other columns must remain as $e_j$ and all columns must be renormalized to unit norm.\n2. For each test case, form a $k$-sparse signal $x \\in \\mathbb{R}^n$ with exactly $k$ nonzero entries equal to $1$ on a specified support $T \\subset \\{0,1,\\dots,n-1\\}$, and compute noiseless measurements $y = D x$.\n3. Implement Orthogonal Matching Pursuit (OMP) for exactly $k$ iterations using the standard selection rule $j_t = \\arg\\max_{j \\notin S_{t-1}} |\\langle d_j, r_{t-1} \\rangle|$ and the residual update $r_t = y - D_{S_t} x_{S_t}^{\\text{LS}}$ using least squares on the selected columns. Ensure that indices are not reselected.\n4. For each iteration $t$, compute the following quantities:\n   - The residual orthogonality defect with respect to the selected set, defined as $\\delta_t = \\max_{j \\in S_t} |\\langle d_j, r_t \\rangle|$. In exact arithmetic, $\\delta_t = 0$, but numerical computation will introduce nonzero values. Summarize this as $\\delta_{\\max} = \\max_t \\delta_t$ over all iterations.\n   - The true versus false correlation margin, defined at each iteration $t$ as $\\rho_t = \\left( \\max_{j \\in T \\setminus S_t} |\\langle d_j, r_t \\rangle| \\right) - \\left( \\max_{j \\notin T} |\\langle d_j, r_t \\rangle| \\right)$, restricted to iterations with $T \\setminus S_t \\neq \\emptyset$. Summarize this as $\\rho_{\\min} = \\min_t \\rho_t$ over those iterations. A negative $\\rho_{\\min}$ indicates that at some iteration the largest correlation belongs to a false atom, which risks misselection.\n5. Compute the mutual coherence $\\mu(D)$ empirically from the dictionary via pairwise column inner products.\n6. Compare to the classical mutual coherence sufficient condition for exact support selection for signals with equal-magnitude nonzeros, which states that if $\\mu(D) < \\frac{1}{2k - 1}$ then OMP selects a correct atom at each step. Compute the threshold $\\tau = \\frac{1}{2k - 1}$ and report whether the final selected support equals the true support, i.e., whether $S_k = T$.\n\nTest suite:\nUse $m = n = 32$ and consider the following four cases, each specified by $(\\varepsilon, \\text{pairs}, k, T)$:\n- Case A (orthonormal baseline): $\\varepsilon = 0$, $\\text{pairs} = \\{(0,1),(2,3)\\}$, $k = 3$, $T = \\{0,2,5\\}$.\n- Case B (happy path, small coherence): $\\varepsilon = 0.05$, $\\text{pairs} = \\{(0,1),(2,3)\\}$, $k = 3$, $T = \\{0,2,5\\}$.\n- Case C (near the mutual coherence threshold): $\\varepsilon = 0.1$, $\\text{pairs} = \\{(0,1),(2,3)\\}$, $k = 3$, $T = \\{0,2,5\\}$.\n- Case D (significant coherence, likely failure): $\\varepsilon = 0.45$, $\\text{pairs} = \\{(0,1),(2,3)\\}$, $k = 3$, $T = \\{0,2,5\\}$.\n\nFor each case, your program must:\n- Build $D$, form $x$ and $y$, run OMP for $k$ iterations, compute $\\delta_{\\max}$, $\\rho_{\\min}$, $\\mu(D)$, $\\tau$, and a boolean indicating whether $S_k = T$.\n- Produce a single line of output containing a list of results, one per case, where each case’s result is the list $[\\text{success}, \\delta_{\\max}, \\mu(D), \\tau, \\rho_{\\min}]$. The final printed line must be exactly a comma-separated list enclosed in square brackets, for example, $[[\\text{True},0.0,0.0,0.2,0.0],[\\dots],\\dots]$. No other text may be printed.",
            "solution": "The user-provided problem is valid. It is scientifically grounded in the principles of compressed sensing and sparse optimization, well-posed with all necessary definitions and parameters, and objective in its formulation. The task is to perform a numerical experiment to analyze the performance of the Orthogonal Matching Pursuit (OMP) algorithm under varying levels of dictionary mutual coherence.\n\nThe core of the analysis is to observe how OMP's ability to correctly identify the support of a sparse signal degrades as the dictionary deviates from an orthonormal basis. This deviation is quantified by the mutual coherence $\\mu(D)$, and its effect is measured through the evolution of residual correlations.\n\nThe solution proceeds in four main stages: dictionary construction, signal simulation, OMP execution, and metric computation.\n\n**1. Dictionary Construction**\n\nThe dictionary $D \\in \\mathbb{R}^{n \\times n}$ is constructed by introducing controlled correlations into an initial orthonormal basis, which is the standard basis represented by the identity matrix $I = [e_0, e_1, \\dots, e_{n-1}]$. For a given pair of indices $(p, q)$ and a perturbation parameter $\\varepsilon \\ge 0$, the corresponding columns $e_p$ and $e_q$ are replaced by new vectors, which are then normalized. The new columns, $d_p$ and $d_q$, are defined as:\n$$\nd_p = \\frac{e_p + \\varepsilon e_q}{\\|e_p + \\varepsilon e_q\\|_2} \\quad \\text{and} \\quad d_q = \\frac{e_q + \\varepsilon e_p}{\\|e_q + \\varepsilon e_p\\|_2}\n$$\nThe norm in the denominator is calculated as $\\|e_j + \\varepsilon e_k\\|_2 = \\sqrt{\\langle e_j + \\varepsilon e_k, e_j + \\varepsilon e_k \\rangle} = \\sqrt{\\|e_j\\|_2^2 + 2\\varepsilon\\langle e_j, e_k \\rangle + \\varepsilon^2\\|e_k\\|_2^2}$. Since $\\{e_j\\}$ are orthonormal, this simplifies to $\\sqrt{1^2 + 0 + \\varepsilon^2} = \\sqrt{1 + \\varepsilon^2}$. Thus, the new columns are:\n$$\nd_p = \\frac{1}{\\sqrt{1+\\varepsilon^2}}(e_p + \\varepsilon e_q) \\quad \\text{and} \\quad d_q = \\frac{1}{\\sqrt{1+\\varepsilon^2}}(e_q + \\varepsilon e_p)\n$$\nAll other columns $d_j$ for $j \\notin \\{p, q\\}$ remain as the standard basis vectors $e_j$. This construction ensures all columns are of unit norm, i.e., $\\|d_j\\|_2 = 1$.\n\nThe mutual coherence, $\\mu(D) = \\max_{i \\neq j} |\\langle d_i, d_j \\rangle|$, is dominated by the inner product of the coupled columns:\n$$\n\\langle d_p, d_q \\rangle = \\left\\langle \\frac{e_p + \\varepsilon e_q}{\\sqrt{1+\\varepsilon^2}}, \\frac{e_q + \\varepsilon e_p}{\\sqrt{1+\\varepsilon^2}} \\right\\rangle = \\frac{1}{1+\\varepsilon^2} \\left( \\langle e_p, e_q \\rangle + \\langle e_p, \\varepsilon e_p \\rangle + \\langle \\varepsilon e_q, e_q \\rangle + \\langle \\varepsilon e_q, \\varepsilon e_p \\rangle \\right) = \\frac{2\\varepsilon}{1+\\varepsilon^2}\n$$\nAs $\\varepsilon$ increases, so does the mutual coherence, making the dictionary columns less orthogonal. For $\\varepsilon=0$, the dictionary is the identity matrix, and $\\mu(D)=0$.\n\n**2. Sparse Signal Model**\n\nFor a given sparsity level $k$ and support set $T \\subset \\{0, 1, \\dots, n-1\\}$ with $|T|=k$, we define a $k$-sparse signal $x \\in \\mathbb{R}^n$. The non-zero entries of $x$ are set to $1$ on the support $T$:\n$$\nx_j = \\begin{cases} 1 & \\text{if } j \\in T \\\\ 0 & \\text{if } j \\notin T \\end{cases}\n$$\nNoiseless measurements are then synthesized as $y = Dx = \\sum_{j \\in T} d_j$.\n\n**3. Orthogonal Matching Pursuit (OMP)**\n\nOMP is an iterative greedy algorithm for sparse recovery. It identifies the support of $x$ one index at a time. Starting with an initial residual $r_0 = y$ and an empty support set $S_0 = \\emptyset$, it performs $k$ iterations. At iteration $t = 1, \\dots, k$:\n1.  **Atom Selection:** It finds the column of $D$ most correlated with the current residual, among those not yet selected:\n    $$\n    j_t = \\arg\\max_{j \\notin S_{t-1}} |\\langle d_j, r_{t-1} \\rangle|\n    $$\n2.  **Support Update:** The selected index is added to the support set: $S_t = S_{t-1} \\cup \\{j_t\\}$.\n3.  **Signal Refitting:** The signal coefficients are re-estimated on the current support set $S_t$ via a least-squares fit:\n    $$\n    x_{S_t}^{\\text{LS}} = \\arg\\min_{z \\in \\mathbb{R}^{|S_t|}} \\|y - D_{S_t} z\\|_2^2 = (D_{S_t}^\\top D_{S_t})^{-1} D_{S_t}^\\top y\n    $$\n    where $D_{S_t}$ is the sub-dictionary with columns indexed by $S_t$.\n4.  **Residual Update:** The residual is updated to be the component of $y$ orthogonal to the subspace spanned by the columns in $D_{S_t}$:\n    $$\n    r_t = y - D_{S_t} x_{S_t}^{\\text{LS}}\n    $$\n    In exact arithmetic, this ensures that the new residual is orthogonal to all previously selected atoms, i.e., $D_{S_t}^\\top r_t = 0$.\n\n**4. Performance Metrics**\n\nTo analyze the algorithm's behavior, several quantities are computed at each iteration $t$:\n-   **Mutual Coherence Threshold ($\\tau$):** A well-known sufficient condition for OMP to correctly identify one new support element at each step, for signals with equal magnitude non-zero entries, is $\\mu(D) < \\frac{1}{2k-1}$. We compute this threshold as $\\tau = \\frac{1}{2k-1}$. If $\\mu(D) < \\tau$, OMP is guaranteed to succeed. If $\\mu(D) \\ge \\tau$, success is not guaranteed and failure becomes possible.\n-   **Residual Orthogonality Defect ($\\delta_{\\max}$):** In practice, due to floating-point arithmetic, the condition $D_{S_t}^\\top r_t = 0$ is only met approximately. We measure the maximum deviation over all iterations: $\\delta_{\\max} = \\max_{t=1,\\dots,k} \\left( \\max_{j \\in S_t} |\\langle d_j, r_t \\rangle| \\right)$. This quantifies the numerical precision of the orthogonalization step.\n-   **True vs. False Correlation Margin ($\\rho_{\\min}$):** The key to OMP's success is that the correlation of the residual with the next correct (true) atom is larger than with any incorrect (false) atom. We measure this margin after each step $t$:\n    $$\n    \\rho_t = \\left( \\max_{j \\in T \\setminus S_t} |\\langle d_j, r_t \\rangle| \\right) - \\left( \\max_{j \\notin T} |\\langle d_j, r_t \\rangle| \\right)\n    $$\n    This is calculated for iterations where true atoms remain to be found ($T \\setminus S_t \\neq \\emptyset$). A positive $\\rho_t$ implies the next selection step is well-conditioned for success. A negative $\\rho_t$ indicates that a false atom is more correlated with the residual than any remaining true atom, foreshadowing a likely error in the next selection. We summarize this by $\\rho_{\\min} = \\min_t \\rho_t$.\n-   **Success Flag:** The ultimate measure of performance is whether the final estimated support $S_k$ matches the true support $T$.\n\nThe provided test cases are designed to probe OMP's performance from an ideal orthonormal case ($\\varepsilon=0$) to a high-coherence case ($\\varepsilon=0.45$) where $\\mu(D)$ significantly exceeds the theoretical threshold $\\tau$. By observing the metrics, especially $\\rho_{\\min}$, we can connect the theoretical condition to the practical behavior of the algorithm.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the OMP analysis for all test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (epsilon, pairs_to_correlate, k, True_Support)\n        (0.0,  {(0, 1), (2, 3)}, 3, {0, 2, 5}),  # Case A: Orthonormal baseline\n        (0.05, {(0, 1), (2, 3)}, 3, {0, 2, 5}),  # Case B: Small coherence\n        (0.1,  {(0, 1), (2, 3)}, 3, {0, 2, 5}),  # Case C: Near threshold\n        (0.45, {(0, 1), (2, 3)}, 3, {0, 2, 5}),  # Case D: Significant coherence\n    ]\n\n    m = n = 32\n    results = []\n\n    for case in test_cases:\n        epsilon, pairs, k, T = case\n\n        # 1. Construct the dictionary D\n        D = np.identity(n)\n        e = np.identity(n)\n        if epsilon > 0: # Avoid division by zero if epsilon is 0\n            norm_factor = np.sqrt(1 + epsilon**2)\n            for p, q in pairs:\n                d_p = (e[:, p] + epsilon * e[:, q]) / norm_factor\n                d_q = (e[:, q] + epsilon * e[:, p]) / norm_factor\n                D[:, p] = d_p\n                D[:, q] = d_q\n\n        # 2. Form the sparse signal x and measurements y\n        x = np.zeros(n)\n        x[list(T)] = 1.0\n        y = D @ x\n\n        # 3. Implement OMP and compute metrics\n        S = set()\n        S_list = []\n        r = y.copy()\n        \n        delta_list = []\n        rho_list = []\n        \n        all_indices = set(range(n))\n        false_atom_indices = list(all_indices - T)\n\n        # OMP loop for k iterations\n        for t in range(k):\n            # Selection step (using residual from previous iteration, r_t-1 which is `r`)\n            correlations = np.abs(D.T @ r)\n            \n            # Exclude already selected indices\n            if S:\n                correlations[list(S)] = -1.0\n            \n            j_t = np.argmax(correlations)\n            \n            # Update support set\n            S.add(j_t)\n            S_list.append(j_t)\n            \n            # Least squares refitting\n            D_S = D[:, S_list]\n            try:\n                # Use least squares to find coefficients on the selected support\n                x_S, _, _, _ = np.linalg.lstsq(D_S, y, rcond=None)\n            except np.linalg.LinAlgError:\n                # This should not happen with this problem's setup\n                # but is good practice for singular D_S\n                success = False\n                delta_max = float('nan')\n                rho_min = float('nan')\n                break\n\n            # Update residual (r_t)\n            r = y - D_S @ x_S\n\n            # 4. Compute per-iteration metrics\n            # delta_t: residual orthogonality defect\n            delta_t = np.max(np.abs(D_S.T @ r))\n            delta_list.append(delta_t)\n            \n            # rho_t: true vs false correlation margin\n            unselected_true_indices = T - S\n            if unselected_true_indices:\n                # Correlation with remaining true atoms\n                true_corrs = np.abs(D[:, list(unselected_true_indices)].T @ r)\n                max_true_corr = np.max(true_corrs)\n                \n                # Correlation with all false atoms\n                false_corrs = np.abs(D[:, false_atom_indices].T @ r)\n                max_false_corr = np.max(false_corrs) if false_atom_indices else 0.0\n\n                rho_t = max_true_corr - max_false_corr\n                rho_list.append(rho_t)\n        \n        else: # This block runs if the loop completes without `break`\n            # Summarize metrics\n            delta_max = max(delta_list) if delta_list else 0.0\n            rho_min = min(rho_list) if rho_list else float('inf') # Should not be empty for k > 1\n            success = (S == T)\n\n        # 5. Compute mutual coherence mu(D)\n        G = D.T @ D\n        np.fill_diagonal(G, 0)\n        mu_D = np.max(np.abs(G))\n\n        # 6. Compute mutual coherence threshold tau\n        tau = 1.0 / (2 * k - 1)\n\n        # Append results for this case\n        results.append([success, delta_max, mu_D, tau, rho_min])\n\n    # Final print statement in the exact required format\n    # The format requires python bools to be capitalized, which is default.\n    output_str = \"[\" + \",\".join([\n        f\"[{s},{d:.6e},{m:.6e},{t:.6e},{r:.6e}]\" for s, d, m, t, r in results\n    ]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}