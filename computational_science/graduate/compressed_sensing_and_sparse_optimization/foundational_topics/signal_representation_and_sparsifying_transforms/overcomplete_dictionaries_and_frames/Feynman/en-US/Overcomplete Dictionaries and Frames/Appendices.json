{
    "hands_on_practices": [
        {
            "introduction": "In the study of sparse representations, precision in our definitions is paramount. This first exercise  challenges you to move beyond intuitive notions and rigorously apply the formal definitions of a basis, a frame, and an overcomplete dictionary. By evaluating several candidate definitions and examples, you will solidify your understanding of the key properties—linear independence, spanning, and redundancy—that distinguish these fundamental structures.",
            "id": "3434627",
            "problem": "You are asked to rigorously evaluate candidate statements about foundational structures used for change of basis and sparse representations in compressed sensing and sparse optimization. Each option simultaneously proposes a formal definition and an explicit example in $\\mathbb{R}^2$, together with claimed properties such as linear independence, spanning, and redundancy. Select all options that are fully correct: the definition must be precise, the example must satisfy the definition, and the stated properties must hold.\n\nA. A basis of $\\mathbb{R}^n$ is a set $\\{b_i\\}_{i=1}^n \\subset \\mathbb{R}^n$ that is linearly independent and spans $\\mathbb{R}^n$. Example in $\\mathbb{R}^2$: $\\{(1,0)^\\top,(0,1)^\\top\\}$. Properties: the set is linearly independent, spans $\\mathbb{R}^2$, and is not redundant.\n\nB. A frame with bounds $(A,B)$ in $\\mathbb{R}^n$ is a family $\\{f_i\\}_{i=1}^m \\subset \\mathbb{R}^n$ such that for all $x \\in \\mathbb{R}^n$,\n$$\nA \\|x\\|_2^2 \\le \\sum_{i=1}^m \\big|\\langle x, f_i \\rangle\\big|^2 \\le B \\|x\\|_2^2.\n$$\nExample in $\\mathbb{R}^2$: $\\{(1,0)^\\top,(0,1)^\\top,(1,1)^\\top\\}$ has frame bounds $(A,B)=(1,3)$. Properties: the set spans $\\mathbb{R}^2$, is not linearly independent (hence redundant), and is not a basis.\n\nC. An overcomplete dictionary for $\\mathbb{R}^n$ is a collection $\\mathcal{D} \\subset \\mathbb{R}^n$ whose linear span is all of $\\mathbb{R}^n$ but which is not linearly independent (equivalently, it contains linear dependencies or has cardinality exceeding $n$). Example in $\\mathbb{R}^2$: $\\{(1,0)^\\top,(0,1)^\\top,(2,0)^\\top\\}$. Properties: the set spans $\\mathbb{R}^2$, is linearly dependent (redundant), and is not a basis.\n\nD. A frame with bounds $(A,B)$ in $\\mathbb{R}^n$ is any set $\\{f_i\\}_{i=1}^m$ obeying\n$$\nA \\|x\\|_2 \\le \\sum_{i=1}^m \\|f_i\\|_2 \\le B \\|x\\|_2 \\quad \\text{for all } x \\in \\mathbb{R}^n.\n$$\nExample in $\\mathbb{R}^2$: $\\{(1,0)^\\top,(0,2)^\\top\\}$ with $(A,B)=(1,1)$. Properties: the set is a basis of $\\mathbb{R}^2$.\n\nE. An overcomplete dictionary in $\\mathbb{R}^n$ is any finite set in $\\mathbb{R}^n$ with more than $n$ elements. Example in $\\mathbb{R}^2$: $\\{(1,0)^\\top,(1,0)^\\top,(1,0)^\\top\\}$. Properties: the set is redundant and spans $\\mathbb{R}^2$.\n\nSelect all correct options.",
            "solution": "The problem requires a rigorous evaluation of five statements, each comprising a definition, an example, and a set of properties related to vector space structures used in sparse representations. An option is deemed correct only if all its constituent parts—the definition, the example's adherence to that definition, and the stated properties of the example—are accurate.\n\n**Analysis of Option A**\n\nThe definition provided for a basis is: \"A basis of $\\mathbb{R}^n$ is a set $\\{b_i\\}_{i=1}^n \\subset \\mathbb{R}^n$ that is linearly independent and spans $\\mathbb{R}^n$.\" This is the precise and standard definition of a basis for an $n$-dimensional vector space. The notation correctly implies a cardinality of $n$.\n\nThe example given in $\\mathbb{R}^2$ is the set $S_A = \\{(1,0)^\\top, (0,1)^\\top\\}$. Let $b_1 = (1,0)^\\top$ and $b_2 = (0,1)^\\top$.\n\nThe stated properties are evaluated as follows:\n1.  **Linear Independence**: The linear combination $c_1 b_1 + c_2 b_2 = c_1(1,0)^\\top + c_2(0,1)^\\top = (c_1, c_2)^\\top$ equals the zero vector $(0,0)^\\top$ if and only if $c_1=0$ and $c_2=0$. Thus, the set is linearly independent. This property is correct.\n2.  **Spanning $\\mathbb{R}^2$**: Any vector $x = (x_1, x_2)^\\top \\in \\mathbb{R}^2$ can be expressed as a linear combination $x = x_1 b_1 + x_2 b_2$. Therefore, the set spans $\\mathbb{R}^2$. This property is correct.\n3.  **Not Redundant**: A set is redundant if it is linearly dependent. Since the set is linearly independent, it is not redundant. A basis is a minimal spanning set, which is inherently non-redundant. This property is correct.\n\nSince the definition is correct, the example is a valid instance of a basis, and all its stated properties hold, this option is fully correct.\n\n**Verdict for A: Correct**\n\n**Analysis of Option B**\n\nThe definition of a frame is given as: \"A frame with bounds $(A,B)$ in $\\mathbb{R}^n$ is a family $\\{f_i\\}_{i=1}^m \\subset \\mathbb{R}^n$ such that for all $x \\in \\mathbb{R}^n$, $A \\|x\\|_2^2 \\le \\sum_{i=1}^m \\big|\\langle x, f_i \\rangle\\big|^2 \\le B \\|x\\|_2^2$.\" This is the correct standard definition for a finite frame in a Hilbert space, where $A$ and $B$ are positive constants.\n\nThe example in $\\mathbb{R}^2$ is the set $S_B = \\{f_1, f_2, f_3\\} = \\{(1,0)^\\top, (0,1)^\\top, (1,1)^\\top\\}$, with claimed frame bounds $(A,B)=(1,3)$. To verify this, let $x=(x_1, x_2)^\\top$. The frame operator sum is:\n$$ \\sum_{i=1}^3 \\big|\\langle x, f_i \\rangle\\big|^2 = \\big|\\langle x, (1,0)^\\top \\rangle\\big|^2 + \\big|\\langle x, (0,1)^\\top \\rangle\\big|^2 + \\big|\\langle x, (1,1)^\\top \\rangle\\big|^2 $$\n$$ = |x_1|^2 + |x_2|^2 + |x_1+x_2|^2 = x_1^2 + x_2^2 + (x_1^2 + 2x_1x_2 + x_2^2) = 2x_1^2 + 2x_2^2 + 2x_1x_2 $$\nThis quadratic form can be written as $x^\\top S x$ where $S = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$ is the frame operator. The frame bounds $A$ and $B$ are the minimum and maximum eigenvalues of $S$. The characteristic equation is $\\det(S - \\lambda I) = (2-\\lambda)^2 - 1 = 0$, which yields $2-\\lambda = \\pm 1$. The eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = 3$. Thus, the optimal frame bounds are $(A,B)=(1,3)$. The example and its claimed bounds are correct.\n\nThe stated properties are evaluated as follows:\n1.  **Spans $\\mathbb{R}^2$**: A set of vectors forms a frame for a space if and only if it spans that space. Since $A=10$, the set is a frame and therefore spans $\\mathbb{R}^2$. This property is correct.\n2.  **Not Linearly Independent (hence redundant)**: The set $S_B$ contains $3$ vectors in the $2$-dimensional space $\\mathbb{R}^2$. Any set of more than $n$ vectors in an $n$-dimensional space must be linearly dependent. Indeed, $(1,1)^\\top = 1 \\cdot (1,0)^\\top + 1 \\cdot (0,1)^\\top$. Thus, the set is not linearly independent, i.e., it is redundant. This property is correct.\n3.  **Not a basis**: A basis must be linearly independent. Since this set is linearly dependent, it cannot be a basis for $\\mathbb{R}^2$. This property is correct.\n\nSince the definition is correct, the example is a valid frame with the asserted bounds, and all properties are correctly stated, this option is fully correct.\n\n**Verdict for B: Correct**\n\n**Analysis of Option C**\n\nThe definition provided for an overcomplete dictionary is: \"An overcomplete dictionary for $\\mathbb{R}^n$ is a collection $\\mathcal{D} \\subset \\mathbb{R}^n$ whose linear span is all of $\\mathbb{R}^n$ but which is not linearly independent\". This is a correct and standard definition. An overcomplete dictionary is a spanning set that is redundant.\n\nThe example in $\\mathbb{R}^2$ is the set $S_C = \\{(1,0)^\\top, (0,1)^\\top, (2,0)^\\top\\}$.\nTo check if this is an overcomplete dictionary, we verify the two conditions from the definition:\n1.  **Spanning**: The subset $\\{(1,0)^\\top, (0,1)^\\top\\}$ is the standard basis for $\\mathbb{R}^2$ and thus spans $\\mathbb{R}^2$. The full set $S_C$ therefore also spans $\\mathbb{R}^2$.\n2.  **Linear Dependence**: The vector $(2,0)^\\top$ is a scalar multiple of $(1,0)^\\top$, specifically $(2,0)^\\top = 2 \\cdot (1,0)^\\top$. This gives a non-trivial linear combination that equals the zero vector: $2(1,0)^\\top + 0(0,1)^\\top - 1(2,0)^\\top = 0$. Thus, the set is linearly dependent.\nThe example correctly fits the definition of an overcomplete dictionary.\n\nThe stated properties (\"spans $\\mathbb{R}^2$\", \"is linearly dependent (redundant)\", \"is not a basis\") are the defining characteristics we just verified. They are all correct.\n\nSince the definition is correct, the example fits the definition, and its properties are correctly stated, this option is fully correct.\n\n**Verdict for C: Correct**\n\n**Analysis of Option D**\n\nThe definition of a frame is given as: \"... any set $\\{f_i\\}_{i=1}^m$ obeying $A \\|x\\|_2 \\le \\sum_{i=1}^m \\|f_i\\|_2 \\le B \\|x\\|_2$ for all $x \\in \\mathbb{R}^n$.\" This definition is fundamentally incorrect. The term $\\sum_{i=1}^m \\|f_i\\|_2$ is a constant value for a given set of vectors, let's call it $C$. The inequality becomes $A \\|x\\|_2 \\le C \\le B \\|x\\|_2$. For this to hold for all $x \\in \\mathbb{R}^n$ (including $x$ with arbitrarily large or small norms), if $x \\neq 0$, it would require $A \\le C/\\|x\\|_2$ and $C/\\|x\\|_2 \\le B$. This cannot hold for a fixed positive $A$ as $\\|x\\|_2 \\to \\infty$, nor for a fixed $B$ as $\\|x\\|_2 \\to 0$ unless $C=0$. The standard frame definition involves the sum of squared inner products, $\\sum_i |\\langle x, f_i \\rangle|^2$, and relates it to $\\|x\\|_2^2$.\n\nThe definition is invalid, so the option is incorrect regardless of the example or properties. For completeness, we note that the example set $\\{(1,0)^\\top,(0,2)^\\top\\}$ with claimed $(A,B)=(1,1)$ does not satisfy the flawed inequality. The sum of norms is $\\|(1,0)^\\top\\|_2 + \\|(0,2)^\\top\\|_2 = 1+2=3$. The inequality becomes $\\|x\\|_2 \\le 3 \\le \\|x\\|_2$, which would imply $\\|x\\|_2=3$. This is not true for all $x \\in \\mathbb{R}^2$. The property that the set is a basis for $\\mathbb{R}^2$ is factually correct, but this cannot salvage an option based on an invalid definition.\n\n**Verdict for D: Incorrect**\n\n**Analysis of Option E**\n\nThe definition of an overcomplete dictionary is given as: \"... any finite set in $\\mathbb{R}^n$ with more than $n$ elements.\" This definition is incorrect because it is incomplete. While a set with more than $n$ elements in $\\mathbb{R}^n$ is guaranteed to be linearly dependent (a necessary property), it is not guaranteed to span $\\mathbb{R}^n$. A key requirement for a dictionary is that it must span the entire space. For instance, the set $\\{(1,0,0)^\\top, (2,0,0)^\\top, (3,0,0)^\\top, (4,0,0)^\\top\\}$ in $\\mathbb{R}^3$ has more than $3$ elements but only spans a $1$-dimensional subspace.\n\nThe example is $S_E = \\{(1,0)^\\top, (1,0)^\\top, (1,0)^\\top\\}$. This set has $3  2$ elements, so it fits the flawed definition.\n\nThe stated properties are \"redundant and spans $\\mathbb{R}^2$\".\n1.  **Redundant**: The set is clearly linearly dependent, so it is redundant. This part is correct.\n2.  **Spans $\\mathbb{R}^2$**: The span of $S_E$ is the set of all scalar multiples of $(1,0)^\\top$, which is the $x$-axis. This is a $1$-dimensional subspace and does not span the $2$-dimensional space $\\mathbb{R}^2$. This property is incorrect.\n\nThe definition is incorrect, and one of the claimed properties for the example is false. Therefore, this option is incorrect.\n\n**Verdict for E: Incorrect**\n\nFinal summary: Options A, B, and C are fully correct in all aspects (definition, example, and properties). Options D and E are incorrect due to flawed definitions and/or incorrect claims about the properties of the examples.",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "Overcomplete dictionaries provide flexible representations but introduce a critical choice between two modeling paradigms: synthesis and analysis sparsity. This practice  presents a carefully constructed scenario where these two models diverge, leading to different recovery outcomes from the same measurement. By working through this concrete calculation, you will gain a deeper appreciation for the geometric differences between the synthesis and analysis $\\ell_1$ programs and understand why they are not always equivalent for a general overcomplete frame.",
            "id": "3465123",
            "problem": "Consider the following explicit overcomplete frame-based compressed sensing setup in $\\mathbb{R}^{2}$. Let the overcomplete dictionary (frame) be the matrix $D \\in \\mathbb{R}^{2 \\times 3}$ with columns $d_{1}$, $d_{2}$, and $d_{3}$ given by $d_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $d_{2} = \\begin{pmatrix} 0 \\\\ \\tfrac{3}{2} \\end{pmatrix}$, and $d_{3} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. Let the sensing matrix be $A \\in \\mathbb{R}^{1 \\times 2}$ given by $A = \\begin{pmatrix} 1  1 \\end{pmatrix}$. The ground-truth signal is $x^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and the measurement is $y = A x^{\\star} = 1$.\n\nTwo convex formulations are considered for recovery:\n- Synthesis $\\ell_{1}$ program: minimize $\\|\\alpha\\|_{1}$ subject to $A D \\alpha = y$, and then reconstruct $x_{\\mathrm{synth}} = D \\alpha$.\n- Analysis $\\ell_{1}$ program: minimize $\\|D^{\\top} x\\|_{1}$ subject to $A x = y$, and output $x_{\\mathrm{an}}$.\n\nStarting only from the definitions above and general convex optimization principles, derive both solutions. Show that in this setup, the analysis $\\ell_{1}$ program recovers $x^{\\star}$ exactly, while the synthesis $\\ell_{1}$ program does not.\n\nCompute the squared Euclidean error of the signal produced by the synthesis $\\ell_{1}$ program relative to the ground truth, namely\n$$\nE \\equiv \\|x_{\\mathrm{synth}} - x^{\\star}\\|_{2}^{2}.\n$$\nExpress your final answer as an exact number. No rounding is required.\n\nIn your derivation, explain why the divergence between the two programs occurs in this example, tying the reasoning to properties of overcomplete frames and the geometric action of the analysis versus synthesis $\\ell_{1}$ objectives under the shared linear measurement constraint.",
            "solution": "The problem statement is first validated.\n\n**Step 1: Extract Givens**\n- Overcomplete dictionary (frame) $D \\in \\mathbb{R}^{2 \\times 3}$: The columns are $d_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $d_{2} = \\begin{pmatrix} 0 \\\\ \\frac{3}{2} \\end{pmatrix}$, and $d_{3} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. Thus, $D = \\begin{pmatrix} 1  0  1 \\\\ 0  \\frac{3}{2}  1 \\end{pmatrix}$.\n- Sensing matrix $A \\in \\mathbb{R}^{1 \\times 2}$: $A = \\begin{pmatrix} 1  1 \\end{pmatrix}$.\n- Ground-truth signal $x^{\\star} \\in \\mathbb{R}^{2}$: $x^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n- Measurement $y \\in \\mathbb{R}$: $y = A x^{\\star} = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$.\n- Synthesis $\\ell_{1}$ program: Minimize $\\|\\alpha\\|_{1}$ subject to $A D \\alpha = y$. Reconstruct $x_{\\mathrm{synth}} = D \\alpha$.\n- Analysis $\\ell_{1}$ program: Minimize $\\|D^{\\top} x\\|_{1}$ subject to $A x = y$. Output $x_{\\mathrm{an}}$.\n- Quantity to compute: Squared Euclidean error $E = \\|x_{\\mathrm{synth}} - x^{\\star}\\|_{2}^{2}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is a standard exercise in compressed sensing and sparse optimization, dealing with well-established concepts like overcomplete frames, synthesis and analysis priors, and $\\ell_1$-norm minimization. It is scientifically and mathematically sound.\n- **Well-Posed**: The problems are convex optimization programs (minimizing a convex function over a convex set), which are guaranteed to have solutions. The problem is self-contained, with all necessary data provided.\n- **Objective**: The problem is stated using precise mathematical language, free from ambiguity or subjective claims.\n- The problem is not flawed by any of the listed criteria (e.g., factual unsoundness, incompleteness, contradiction, unrealism).\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be derived.\n\n**Derivation of the Synthesis Solution**\n\nThe synthesis $\\ell_1$ program is:\n$$\n\\min_{\\alpha \\in \\mathbb{R}^3} \\|\\alpha\\|_1 \\quad \\text{subject to} \\quad AD\\alpha = y\n$$\nFirst, we compute the matrix product $AD$:\n$$\nAD = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1  0  1 \\\\ 0  \\frac{3}{2}  1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 1 \\cdot 0  1 \\cdot 0 + 1 \\cdot \\frac{3}{2}  1 \\cdot 1 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1  \\frac{3}{2}  2 \\end{pmatrix}\n$$\nWith $y = 1$, the constraint $AD\\alpha = y$ becomes a single linear equation for $\\alpha = (\\alpha_1, \\alpha_2, \\alpha_3)^\\top$:\n$$\n\\alpha_1 + \\frac{3}{2}\\alpha_2 + 2\\alpha_3 = 1\n$$\nWe need to find the point on this plane in $\\mathbb{R}^3$ that has the minimum $\\ell_1$-norm, $\\|\\alpha\\|_1 = |\\alpha_1| + |\\alpha_2| + |\\alpha_3|$. A fundamental result of $\\ell_1$-minimization states that the solution to $\\min \\|\\alpha\\|_1$ subject to $\\Phi \\alpha = y$ will be sparse. For a single constraint, the solution will have at most one non-zero component. We can find this solution by setting all but one component of $\\alpha$ to zero and solving for the remaining component.\n1. If $\\alpha_2 = \\alpha_3 = 0$, then $\\alpha_1 = 1$. The solution is $\\alpha = (1, 0, 0)^\\top$, with $\\|\\alpha\\|_1 = 1$.\n2. If $\\alpha_1 = \\alpha_3 = 0$, then $\\frac{3}{2}\\alpha_2 = 1 \\implies \\alpha_2 = \\frac{2}{3}$. The solution is $\\alpha = (0, \\frac{2}{3}, 0)^\\top$, with $\\|\\alpha\\|_1 = \\frac{2}{3}$.\n3. If $\\alpha_1 = \\alpha_2 = 0$, then $2\\alpha_3 = 1 \\implies \\alpha_3 = \\frac{1}{2}$. The solution is $\\alpha = (0, 0, \\frac{1}{2})^\\top$, with $\\|\\alpha\\|_1 = \\frac{1}{2}$.\n\nComparing the $\\ell_1$-norms, the minimum is $\\frac{1}{2}$. Thus, the optimal coefficient vector is $\\alpha_{\\mathrm{synth}} = (0, 0, \\frac{1}{2})^\\top$.\nThe reconstructed signal is $x_{\\mathrm{synth}} = D \\alpha_{\\mathrm{synth}}$:\n$$\nx_{\\mathrm{synth}} = \\begin{pmatrix} 1  0  1 \\\\ 0  \\frac{3}{2}  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix} = \\frac{1}{2} d_3 = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}\n$$\nThe ground-truth signal is $x^{\\star} = (1, 0)^\\top$. Clearly, $x_{\\mathrm{synth}} \\neq x^{\\star}$. The synthesis program fails to recover the ground-truth signal.\n\n**Derivation of the Analysis Solution**\n\nThe analysis $\\ell_1$ program is:\n$$\n\\min_{x \\in \\mathbb{R}^2} \\|D^{\\top}x\\|_1 \\quad \\text{subject to} \\quad Ax = y\n$$\nLet $x = (x_1, x_2)^\\top$. The constraint $Ax=y$ is $\\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 1$, which simplifies to the line $x_1 + x_2 = 1$.\n\nThe objective function involves the term $D^\\top x$:\n$$\nD^{\\top}x = \\begin{pmatrix} 1  0 \\\\ 0  \\frac{3}{2} \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ \\frac{3}{2}x_2 \\\\ x_1+x_2 \\end{pmatrix}\n$$\nThe objective is to minimize $\\|D^{\\top}x\\|_1 = |x_1| + |\\frac{3}{2}x_2| + |x_1+x_2|$.\nUsing the constraint $x_1+x_2=1$, the objective function simplifies to:\n$$\n\\|D^{\\top}x\\|_1 = |x_1| + |\\frac{3}{2}x_2| + |1| = |x_1| + \\frac{3}{2}|x_2| + 1\n$$\nWe need to minimize this quantity subject to $x_1 + x_2 = 1$. We can express the problem as a one-dimensional minimization by substituting $x_2 = 1-x_1$. Let $g(x_1)$ be the function to minimize (ignoring the constant $1$):\n$$\ng(x_1) = |x_1| + \\frac{3}{2}|1-x_1|\n$$\nThis function is convex and piecewise linear, with non-differentiable points at $x_1=0$ and $x_1=1$. We analyze its derivative in the three regions defined by these points:\n- For $x_1  0$: $g(x_1) = -x_1 + \\frac{3}{2}(1-x_1) = -\\frac{5}{2}x_1 + \\frac{3}{2}$. The slope is $g'(x_1) = -\\frac{5}{2}$.\n- For $0  x_1  1$: $g(x_1) = x_1 + \\frac{3}{2}(1-x_1) = -\\frac{1}{2}x_1 + \\frac{3}{2}$. The slope is $g'(x_1) = -\\frac{1}{2}$.\n- For $x_1  1$: $g(x_1) = x_1 - \\frac{3}{2}(1-x_1) = \\frac{5}{2}x_1 - \\frac{3}{2}$. The slope is $g'(x_1) = \\frac{5}{2}$.\n\nThe slope of $g(x_1)$ changes from negative to positive at $x_1=1$. Therefore, the global minimum of $g(x_1)$ occurs at $x_1 = 1$.\nFor $x_1=1$, the constraint $x_1+x_2=1$ gives $x_2=0$.\nThe solution to the analysis program is $x_{\\mathrm{an}} = (1, 0)^\\top$.\nThis is exactly the ground-truth signal, $x_{\\mathrm{an}} = x^{\\star}$. The analysis program successfully recovers the signal.\n\n**Explanation of the Divergence**\n\nThe divergence between the synthesis and analysis programs arises because for a general overcomplete dictionary $D$, the two models promote different notions of sparsity, and their respective cost functions are not equivalent.\n\nThe ground-truth signal is $x^{\\star} = d_1$. It has a perfectly sparse representation in the synthesis model, $x^{\\star} = D\\alpha^{\\star}$ with $\\alpha^{\\star} = (1, 0, 0)^{\\top}$. The $\\ell_1$-norm of this true sparse representation is $\\|\\alpha^{\\star}\\|_1 = 1$. However, this is not the only representation. Any signal of the form $x = D\\alpha$ can also be written as $x = D(\\alpha + v)$ for any $v$ in the null space of $D$, $\\ker(D)$. The synthesis program seeks the single coefficient vector $\\alpha$ which satisfies the measurement constraint $AD\\alpha=y$ and has the globally minimum $\\ell_1$-norm. This search is performed over *all* possible signals that can be synthesized from the dictionary.\n\nIn this problem, the synthesis program found a different coefficient vector, $\\alpha_{\\mathrm{synth}} = (0, 0, 1/2)^\\top$, which also satisfies the measurement constraint ($AD\\alpha_{\\mathrm{synth}} = 1$). This vector has an $\\ell_1$-norm of $\\|\\alpha_{\\mathrm{synth}}\\|_1 = 1/2$. Since $1/2  1$, the synthesis program chooses $\\alpha_{\\mathrm{synth}}$ over the true representation $\\alpha^{\\star}$, leading to an incorrect signal $x_{\\mathrm{synth}} = D\\alpha_{\\mathrm{synth}}$. The synthesis $\\ell_1$-minimization is misled by the existence of another signal, $x_{\\text{synth}}$, which is consistent with the measurements and has a \"sparser\" representation (in the sense of a smaller representative $\\ell_1$-norm) than the true signal $x^{\\star}$.\n\nThe analysis program, in contrast, works directly with the signal $x$. It minimizes the analysis-domain sparsity measure, $\\|D^{\\top}x\\|_1$, over the set of all signals satisfying the measurement constraint $Ax=y$. For this specific problem, the true signal $x^{\\star}$ happens to be the point on the constraint line $x_1+x_2=1$ that yields the minimal value for the analysis cost function $\\|D^\\top x\\|_1 = |x_1| + \\frac{3}{2}|x_2| + 1$. The analysis model is not concerned with how signals are represented by dictionary atoms, but rather with how they are evaluated by the analysis operators (the rows of $D^\\top$).\n\nThis example illustrates that for an overcomplete frame that is not a tight frame, the synthesis \"atomic norm\" ($\\min_{\\alpha: D\\alpha = x} \\|\\alpha\\|_1$) and the analysis norm ($\\|D^\\top x\\|_1$) are different functions, which can have different minimizers over a given constraint set.\n\n**Computation of the Error**\n\nThe squared Euclidean error of the synthesis solution is calculated as $E = \\|x_{\\mathrm{synth}} - x^{\\star}\\|_2^2$.\n$$\nx_{\\mathrm{synth}} - x^{\\star} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} - 1 \\\\ \\frac{1}{2} - 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}\n$$\n$$\nE = \\left\\| \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\right\\|_2^2 = \\left(-\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}\n$$",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "The success of sparse recovery algorithms like Orthogonal Matching Pursuit (OMP) and Basis Pursuit (BP) depends critically on the properties of the dictionary. This exercise  provides a direct, quantitative link between a dictionary's geometric structure and the performance of these algorithms. You will derive the mutual coherence for a common dictionary type—the union of two orthonormal bases—and use it to establish a precise threshold on signal sparsity that guarantees exact recovery.",
            "id": "3465086",
            "problem": "Consider two orthonormal bases $\\mathcal{A}=\\{a_{1},\\dots,a_{n}\\}$ and $\\mathcal{B}=\\{b_{1},\\dots,b_{n}\\}$ in $\\mathbb{R}^{n}$, and form the dictionary $D\\in\\mathbb{R}^{n\\times 2n}$ by concatenation $D=[A\\ B]$, where $A=[a_{1}\\ \\dots\\ a_{n}]$ and $B=[b_{1}\\ \\dots\\ b_{n}]$. Assume all columns of $D$ are unit norm. Define the cross-incoherence level\n$$\n\\alpha \\triangleq \\max_{i\\in\\{1,\\dots,n\\},\\,j\\in\\{1,\\dots,n\\}} |\\langle a_{i}, b_{j}\\rangle|,\n$$\nand suppose $0\\alpha1$.\n\nStarting from the definitions of mutual coherence $\\mu(D)=\\max_{p\\neq q}|\\langle d_{p}, d_{q}\\rangle|$ and cumulative coherence (also called the Babel function) $\\mu_{1}(k)=\\max_{|\\Lambda|=k}\\ \\max_{p\\notin\\Lambda}\\ \\sum_{q\\in\\Lambda} |\\langle d_{p}, d_{q}\\rangle|$, derive closed-form expressions for $\\mu(D)$ and $\\mu_{1}(k)$ for $k\\leq n$ in terms of $\\alpha$. Then, using standard coherence-based exact recovery guarantees for Orthogonal Matching Pursuit (OMP) and Basis Pursuit (BP) in the noiseless setting $y=Dx$ with a $k$-sparse $x$, determine the mutual-coherence sparsity threshold $k^{\\star}(\\alpha)$ such that all $k$-sparse signals are guaranteed to be recovered when $kk^{\\star}(\\alpha)$.\n\nExpress your final answer as a single closed-form expression for $k^{\\star}(\\alpha)$, in terms of $\\alpha$. No numerical evaluation is required.",
            "solution": "The problem asks for the mutual-coherence sparsity threshold $k^{\\star}(\\alpha)$ for a dictionary $D$ constructed by concatenating two orthonormal bases, $\\mathcal{A}=\\{a_{1},\\dots,a_{n}\\}$ and $\\mathcal{B}=\\{b_{1},\\dots,b_{n}\\}$, in $\\mathbb{R}^{n}$. The solution requires three main steps:\n1.  Derive the mutual coherence $\\mu(D)$ in terms of the cross-incoherence level $\\alpha$.\n2.  Derive the cumulative coherence (Babel function) $\\mu_{1}(k)$ in terms of $\\alpha$. While not strictly necessary for the final answer as posed, it provides a consistency check.\n3.  Apply the standard mutual-coherence-based recovery condition for Orthogonal Matching Pursuit (OMP) and Basis Pursuit (BP) to find the sparsity threshold $k^{\\star}(\\alpha)$.\n\nLet the dictionary be $D = [A \\ B] \\in \\mathbb{R}^{n \\times 2n}$, where $A=[a_{1}\\ \\dots\\ a_{n}]$ and $B=[b_{1}\\ \\dots\\ b_{n}]$. The columns of $D$ are denoted by $d_p$, for $p \\in \\{1, \\dots, 2n\\}$. Specifically, $d_i = a_i$ for $i \\in \\{1, \\dots, n\\}$ and $d_{n+j} = b_j$ for $j \\in \\{1, \\dots, n\\}$. All columns are unit-norm, i.e., $\\|d_p\\|_2 = 1$ for all $p$.\n\n**Step 1: Derivation of the Mutual Coherence $\\mu(D)$**\n\nThe mutual coherence of a dictionary $D$ is defined as the maximum absolute inner product between any two distinct columns:\n$$\n\\mu(D) \\triangleq \\max_{p \\neq q} |\\langle d_p, d_q \\rangle|\n$$\nTo calculate this for our dictionary $D=[A \\ B]$, we consider three cases for the pair of distinct columns $(d_p, d_q)$:\n\nCase 1: Both columns are from basis $\\mathcal{A}$.\nLet $d_p = a_i$ and $d_q = a_j$ for $i,j \\in \\{1,\\dots,n\\}$ with $i \\neq j$. Since $\\mathcal{A}$ is an orthonormal basis, the inner product is $\\langle a_i, a_j \\rangle = \\delta_{ij} = 0$.\n\nCase 2: Both columns are from basis $\\mathcal{B}$.\nLet $d_p = b_i$ and $d_q = b_j$ for $i,j \\in \\{1,\\dots,n\\}$ with $i \\neq j$. Since $\\mathcal{B}$ is an orthonormal basis, the inner product is $\\langle b_i, b_j \\rangle = \\delta_{ij} = 0$.\n\nCase 3: One column is from $\\mathcal{A}$ and the other is from $\\mathcal{B}$.\nLet $d_p = a_i$ and $d_q = b_j$ for any $i,j \\in \\{1,\\dots,n\\}$. The absolute value of their inner product is $|\\langle a_i, b_j \\rangle|$. The maximum value of this quantity is defined in the problem as the cross-incoherence level $\\alpha$:\n$$\n\\alpha \\triangleq \\max_{i\\in\\{1,\\dots,n\\},\\,j\\in\\{1,\\dots,n\\}} |\\langle a_{i}, b_{j}\\rangle|\n$$\n\nThe mutual coherence $\\mu(D)$ is the maximum value obtained across these three cases.\n$$\n\\mu(D) = \\max \\left( \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|, \\max_{i \\neq j} |\\langle b_i, b_j \\rangle|, \\max_{i,j} |\\langle a_i, b_j \\rangle| \\right) = \\max(0, 0, \\alpha)\n$$\nTherefore, the mutual coherence of the dictionary is simply the cross-incoherence level:\n$$\n\\mu(D) = \\alpha\n$$\n\n**Step 2: Derivation of the Cumulative Coherence $\\mu_1(k)$**\n\nThe cumulative coherence, or Babel function, $\\mu_1(k)$ is defined as:\n$$\n\\mu_{1}(k) \\triangleq \\max_{|\\Lambda|=k} \\max_{p \\notin \\Lambda} \\sum_{q \\in \\Lambda} |\\langle d_p, d_q \\rangle|\n$$\nwhere $\\Lambda$ is an index set of size $k$. Let's analyze the inner sum for a fixed atom $d_p$ and a fixed set $\\Lambda$. Let $p \\notin \\Lambda$.\n\nLet's consider the case where $d_p$ is a column from basis $\\mathcal{A}$, so $d_p = a_i$ for some $i \\in \\{1, \\dots, n\\}$. The index set $\\Lambda$ of size $k$ can be partitioned into $\\Lambda_A$, whose indices point to columns in $A$, and $\\Lambda_B$, whose indices point to columns in $B$. Let $|\\Lambda_A| = k_A$ and $|\\Lambda_B| = k_B$, with $k_A + k_B = k$. The sum can be written as:\n$$\n\\sum_{q \\in \\Lambda} |\\langle a_i, d_q \\rangle| = \\sum_{j: j \\in \\Lambda_A} |\\langle a_i, a_j \\rangle| + \\sum_{j: n+j \\in \\Lambda_B} |\\langle a_i, b_j \\rangle|\n$$\nSince $p=i \\notin \\Lambda$, it follows that $i$ is not in the set of indices corresponding to $\\Lambda_A$. Because $\\mathcal{A}$ is an orthonormal basis, $\\langle a_i, a_j \\rangle = 0$ for all $j \\neq i$. Thus, the first part of the sum is zero.\n$$\n\\sum_{q \\in \\Lambda} |\\langle a_i, d_q \\rangle| = \\sum_{j: n+j \\in \\Lambda_B} |\\langle a_i, b_j \\rangle|\n$$\nBy definition, each term $|\\langle a_i, b_j \\rangle| \\leq \\alpha$. Thus, the sum is bounded by:\n$$\n\\sum_{j: n+j \\in \\Lambda_B} |\\langle a_i, b_j \\rangle| \\leq k_B \\alpha\n$$\nTo maximize this sum over the choice of $\\Lambda$ (with $|\\Lambda|=k$), we must maximize $k_B$. The maximum value for $k_B$ is $k$, which occurs when $k_A=0$. This means the set $\\Lambda$ should be chosen to consist entirely of indices corresponding to columns from basis $\\mathcal{B}$. In this case, the sum is bounded by $k \\alpha$.\n\nA symmetric argument holds if we start by choosing $d_p$ from basis $\\mathcal{B}$, say $d_p = b_j$. The sum would be maximized by choosing all $k$ atoms for $\\Lambda$ from basis $\\mathcal{A}$, and the sum would also be bounded by $k\\alpha$.\n\nThe definition of $\\mu_1(k)$ involves taking the maximum over all possible choices of $p$ and $\\Lambda$. The maximum value of the sum is therefore achieved by choosing an atom $d_p$ from one basis, and the set $\\Lambda$ of $k$ atoms entirely from the other basis, such that the sum of inner products is maximized. This maximum value is $k\\alpha$.\n$$\n\\mu_1(k) = k \\alpha\n$$\nThis result is valid for $k \\le n$, as specified.\n\n**Step 3: Determining the Sparsity Threshold $k^{\\star}(\\alpha)$**\n\nStandard coherence-based guarantees ensure that any $k$-sparse signal $x$ can be uniquely recovered from noiseless measurements $y = Dx$ by algorithms like OMP and BP, provided the sparsity $k$ is sufficiently small. The most common of these guarantees is based on the mutual coherence $\\mu(D)$.\n\nFor both OMP and BP, a sufficient condition for the exact recovery of any $k$-sparse signal is:\n$$\nk  \\frac{1}{2} \\left(1 + \\frac{1}{\\mu(D)}\\right)\n$$\nThe problem asks for the mutual-coherence sparsity threshold $k^{\\star}(\\alpha)$ such that all $k$-sparse signals are guaranteed to be recovered when $k  k^{\\star}(\\alpha)$. This threshold is therefore the right-hand side of the inequality.\n$$\nk^{\\star}(\\alpha) = \\frac{1}{2} \\left(1 + \\frac{1}{\\mu(D)}\\right)\n$$\nSubstituting the result from Step 1, $\\mu(D) = \\alpha$, we get:\n$$\nk^{\\star}(\\alpha) = \\frac{1}{2} \\left(1 + \\frac{1}{\\alpha}\\right)\n$$\n\nAs a verification, we can use a stronger recovery condition based on the Babel function, known as the Exact Recovery Condition (ERC) for BP:\n$$\n\\mu_1(k) + \\mu_1(k-1)  1\n$$\nSubstituting our result from Step 2, $\\mu_1(k) = k\\alpha$:\n$$\nk\\alpha + (k-1)\\alpha  1\n$$\n$$\n(2k-1)\\alpha  1\n$$\nSince $0  \\alpha  1$, we can divide by $\\alpha$ without changing the inequality direction:\n$$\n2k-1  \\frac{1}{\\alpha}\n$$\n$$\n2k  1 + \\frac{1}{\\alpha}\n$$\n$$\nk  \\frac{1}{2} \\left(1 + \\frac{1}{\\alpha}\\right)\n$$\nThis confirms that for the specific dictionary structure given, the general mutual coherence bound is not loose and gives the same threshold as the more refined Babel function analysis.\n\nThe problem explicitly requests the \"mutual-coherence sparsity threshold\", which directly points to the standard formula involving $\\mu(D)$. The final expression is the desired threshold $k^{\\star}(\\alpha)$.",
            "answer": "$$\n\\boxed{\\frac{1}{2}\\left(1 + \\frac{1}{\\alpha}\\right)}\n$$"
        }
    ]
}