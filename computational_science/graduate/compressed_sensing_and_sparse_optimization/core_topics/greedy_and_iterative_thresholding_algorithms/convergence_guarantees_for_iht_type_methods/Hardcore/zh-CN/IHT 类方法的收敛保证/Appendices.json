{
    "hands_on_practices": [
        {
            "introduction": "在分析算法的全局收敛性之前，首先必须对其单步迭代过程有扎实的理解。第一个练习将引导你亲手完成迭代硬阈值（IHT）算法的一个基本步骤。通过手动计算一次迭代，你将清晰地看到梯度下降更新如何试图最小化误差，以及硬阈值算子如何强制施加关键的稀疏性约束 。",
            "id": "3438851",
            "problem": "考虑一个稀疏优化问题，即在稀疏性约束下，通过最小化最小二乘损失从线性测量中恢复一个 $k$-稀疏信号。设最小二乘目标函数为 $f(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2}$，其中测量矩阵为 $A \\in \\mathbb{R}^{3 \\times 3}$，测量值为 $y \\in \\mathbb{R}^{3}$，初始迭代点为 $x_{0} \\in \\mathbb{R}^{3}$。迭代硬阈值（IHT）方法通过首先对 $f$ 进行一次梯度步长，然后通过硬阈值算子 $H_{k}$ 强制施加 $k$-稀疏性来更新迭代点。该算子返回欧几里得范数下最近的 $k$-稀疏向量。硬阈值算子 $H_{k}$ 选择其参数中绝对值最大的 $k$ 个分量，并将其余所有分量置为零。\n\n给定\n$$\nA = \\begin{pmatrix}\n2  0  1 \\\\\n0  1  1 \\\\\n1  1  0\n\\end{pmatrix},\\quad\ny = \\begin{pmatrix}\n3 \\\\\n0 \\\\\n2\n\\end{pmatrix},\\quad\nk = 2,\\quad\n\\mu = \\frac{1}{4},\\quad\nx_{0} = \\begin{pmatrix}\n1 \\\\\n-1 \\\\\n0\n\\end{pmatrix}.\n$$\n从最小二乘梯度的定义和将 $H_k$ 表征为到非凸 $k$-稀疏向量集上的欧几里得投影出发，显式计算从 $x_0$ 开始进行一步梯度下降，然后进行硬阈值处理后得到的第一个 IHT 迭代点 $x_1$。此外，在 IHT 类方法收敛性保证的背景下，解释 $H_{k}$ 在此计算中的作用，包括说明为什么其投影性质对于在控制误差增长的同时保持稀疏性至关重要。\n\n将你的最终答案以对应于 $x_{1}$ 的 3 分量行矩阵形式给出。无需四舍五入。",
            "solution": "该问题是有效的。这是一个在稀疏优化领域中定义明确的计算和概念性问题。所提供的数据在压缩感知的数学框架内是完整的、一致的且有科学依据的。任务是应用标准算法——迭代硬阈值（IHT）进行一次迭代，并解释其一个组成部分的作用，这是该领域的一个标准概念性问题。\n\n给定条件如下：\n- 目标函数：$f(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2}$\n- 测量矩阵：$A = \\begin{pmatrix} 2  0  1 \\\\ 0  1  1 \\\\ 1  1  0 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 3}$\n- 测量向量：$y = \\begin{pmatrix} 3 \\\\ 0 \\\\ 2 \\end{pmatrix} \\in \\mathbb{R}^{3}$\n- 稀疏度：$k = 2$\n- 步长：$\\mu = \\frac{1}{4}$\n- 初始迭代点：$x_{0} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^{3}$\n- 硬阈值算子：$H_{k}(v)$ 将向量 $v$ 中除了绝对值最大的 $k$ 个分量外的所有分量置零。\n\n迭代硬阈值（IHT）的更新规则为：\n$$x_{i+1} = H_k(x_i - \\mu \\nabla f(x_i))$$\n其中 $H_k$ 是硬阈值算子，它将其参数投影到 $k$-稀疏向量的集合上。目标函数为 $f(x) = \\frac{1}{2}\\|y - Ax\\|_2^2$。其梯度为 $\\nabla f(x) = A^T(Ax - y)$。\n\n首先，我们计算初始迭代点 $x_0$ 处的梯度。给定的矩阵 $A$ 是对称的，所以 $A^T = A$。\n我们计算项 $Ax_0$：\n$$Ax_{0} = \\begin{pmatrix} 2  0  1 \\\\ 0  1  1 \\\\ 1  1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2(1) + 0(-1) + 1(0) \\\\ 0(1) + 1(-1) + 1(0) \\\\ 1(1) + 1(-1) + 0(0) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}$$\n接下来，我们计算残差向量 $Ax_0 - y$：\n$$Ax_{0} - y = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1 \\\\ -2 \\end{pmatrix}$$\n现在，我们计算梯度 $\\nabla f(x_0) = A^T(Ax_0 - y) = A(Ax_0 - y)$：\n$$\\nabla f(x_{0}) = \\begin{pmatrix} 2  0  1 \\\\ 0  1  1 \\\\ 1  1  0 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 2(-1) + 0(-1) + 1(-2) \\\\ 0(-1) + 1(-1) + 1(-2) \\\\ 1(-1) + 1(-1) + 0(-2) \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ -3 \\\\ -2 \\end{pmatrix}$$\n\n下一步是执行梯度下降更新，以获得一个我们记为 $z_0$ 的中间向量：\n$$z_0 = x_0 - \\mu \\nabla f(x_0)$$\n代入给定值 $\\mu = \\frac{1}{4}$ 和计算出的梯度：\n$$z_{0} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix} -4 \\\\ -3 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1 - (-1) \\\\ -1 - (-\\frac{3}{4}) \\\\ 0 - (-\\frac{1}{2}) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}$$\n\n最后，我们通过对 $z_0$ 应用硬阈值算子 $H_k$（其中 $k=2$）来计算第一个 IHT 迭代点 $x_1$。算子 $H_2$ 识别出 $z_0$ 中绝对值最大的两个分量，并将其余所有分量置零。\n$z_0$ 各分量的绝对值为：\n$$|z_{0,1}| = |2| = 2$$\n$$|z_{0,2}| = \\left|-\\frac{1}{4}\\right| = 0.25$$\n$$|z_{0,3}| = \\left|\\frac{1}{2}\\right| = 0.5$$\n两个最大的绝对值是 $2$ 和 $0.5$，对应于第一个和第三个分量。因此，$H_2(z_0)$ 保留这两个分量，并将第二个分量置零。\n$$x_1 = H_2(z_0) = H_2\\left(\\begin{pmatrix} 2 \\\\ -\\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}\\right) = \\begin{pmatrix} 2 \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix}$$\n\n关于 $H_k$ 的作用，它是一个非线性算子，将其输入向量投影到 $k$-稀疏向量的非凸集合 $\\Sigma_k = \\{x \\in \\mathbb{R}^n : \\|x\\|_0 \\le k\\}$ 上。该投影是根据欧几里得（$\\ell_2$）范数定义的，这意味着 $H_k(z)$ 是集合 $\\Sigma_k$ 中离 $z$ 最近的向量。\n\n$H_k$ 的作用对于 IHT 类方法的收敛性保证至关重要，主要有两个原因：\n1.  **稀疏性强制**：梯度步长 $x_i - \\mu \\nabla f(x_i)$ 最小化了最小二乘误差，但通常会产生一个稠密向量，从而违反了稀疏性约束。$H_k$ 算子通过将向量投影回可行集 $\\Sigma_k$ 来纠正这一点。这种强制是必须的，因为 IHT 的理论保证依赖于迭代点保持稀疏性。\n2.  **收敛性证明中的误差控制**：IHT 的收敛性通常是在测量矩阵 $A$ 满足限制等距性质（RIP）的假设下证明的。RIP 指出，对于所有 $S$-稀疏向量 $v$ 和某个小常数 $\\delta_S  1$， $A$ 近似地保持稀疏向量的范数，即 $(1-\\delta_S)\\|v\\|_2^2 \\le \\|Av\\|_2^2 \\le (1+\\delta_S)\\|v\\|_2^2$。IHT 的证明分析了误差 $\\|x_i - x^*\\|_2$ 的演变，其中 $x^*$ 是真实的 $k$-稀疏解。该分析依赖于差分向量 $x_i - x^*$ 是 $2k$-稀疏的这一事实（因为 $x_i$ 和 $x^*$ 都是 $k$-稀疏的）。阶数为 $2k$（或更高，取决于证明）的 RIP 确保了当梯度更新步骤被限制在稀疏向量的子空间上时，它表现为一个压缩映射，从而将迭代点拉近解。$H_k$ 的投影性质对于界定误差 $\\|x_{i+1} - x^*\\|_2 = \\|H_k(z_i) - x^*\\|_2$ 至关重要。虽然 $\\Sigma_k$ 是非凸的，并且 $H_k$ 不像到凸集上的投影那样具有非扩张性，但可以证明该投影不会有害地放大误差。具体来说，$H_k(z_i)$ 是中间向量 $z_i$ 的*最佳* $k$-稀疏近似这一事实，是证明整个迭代过程收敛的关键因素。它确保了只要步长 $\\mu$ 相对于 $A$ 的限制等距常数被恰当地选择，算法就能在 $\\ell_2$ 意义上朝着真实解取得进展。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 2  0  \\frac{1}{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "基于受限等距性质（RIP）的IHT收敛理论为我们提供了关于误差的递归界。这个界虽然抽象，但对算法性能有着具体的意义。本练习旨在连接理论与实践，要求你使用该理论界来计算达到特定精度所需的迭代次数，从而让你对线性收敛速率有一个更切实的感受 。",
            "id": "3438867",
            "problem": "考虑线性测量模型 $y = A x^{*} + e$，其中感知矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，未知信号为 $k$-稀疏信号 $x^{*} \\in \\mathbb{R}^{n}$。假设 $A$ 满足阶数为 $2k$、常数为 $\\delta_{2k}$ 的约束等距性质 (Restricted Isometry Property, RIP)，即对于任意 $2k$-稀疏向量 $z \\in \\mathbb{R}^{n}$，不等式 $(1 - \\delta_{2k}) \\|z\\|_{2}^{2} \\le \\|A z\\|_{2}^{2} \\le (1 + \\delta_{2k}) \\|z\\|_{2}^{2}$ 均成立。考虑迭代硬阈值 (Iterative Hard Thresholding, IHT) 方法，其定义为 $x_{t+1} = H_{k}\\!\\left(x_{t} - \\mu A^{\\top}(A x_{t} - y)\\right)$，其中 $H_{k}$ 表示硬阈值算子，它保留幅度最大的 $k$ 个元素并将余下元素置零，而 $\\mu > 0$ 是一个固定的步长。\n\n根据在 RIP 条件下的标准 IHT 收敛性分析可知，存在仅依赖于 $\\mu$ 和 $\\delta_{2k}$ 的常数 $\\alpha$ 和 $\\beta$，使得误差序列 $E_{t} := \\|x_{t} - x^{*}\\|_{2}$ 对所有 $t \\ge 0$ 都遵循 $E_{t+1} \\le \\alpha E_{t} + \\beta \\|e\\|_{2}$ 形式的线性递归界。\n\n在本问题中，取感知矩阵的参数为 $\\delta_{2k} = 0.2$，步长为 $\\mu = 1$。对于此配置，假设分析指定的关联收缩常数和噪声增益常数分别为 $\\alpha = 0.4$ 和 $\\beta = 1$。从初始误差 $E_{0} = \\|x_{0} - x^{*}\\|_{2} = 10$ 和噪声范数 $\\|e\\|_{2} = 1$ 开始，确定使 $\\|x_{T} - x^{*}\\|_{2} \\le 2 \\beta \\|e\\|_{2}$ 成立的最小整数迭代次数 $T$。\n\n你的最终答案必须是代表 $T$ 的单个实数值。除了确定满足所述要求的最小整数 $T$ 之外，无需进行额外的取整。",
            "solution": "该问题要求寻找迭代硬阈值 (IHT) 算法达到指定误差容限所需的最小整数迭代次数 $T$。误差动态由一个给定的线性递归不等式控制。\n\n首先，我们形式化给定的信息。在第 $t$ 次迭代时的误差，表示为 $E_{t} = \\|x_{t} - x^{*}\\|_{2}$，受以下递归关系约束：\n$$E_{t+1} \\le \\alpha E_{t} + \\beta \\|e\\|_{2}$$\n提供的具体参数如下：\n- 初始误差：$E_{0} = \\|x_{0} - x^{*}\\|_{2} = 10$。\n- 收敛速率常数：$\\alpha = 0.4$。\n- 噪声增益常数：$\\beta = 1$。\n- 噪声向量的范数：$\\|e\\|_{2} = 1$。\n\n目标是找到满足误差 $E_{T}$ 条件的最小整数 $T$：\n$$E_{T} \\le 2 \\beta \\|e\\|_{2}$$\n代入给定值，目标误差水平为 $2 \\times 1 \\times 1 = 2$。因此，我们必须找到满足 $E_{T} \\le 2$ 的最小整数 $T$。\n\n为解决此问题，我们首先需要找到 $E_{t}$ 上界的闭式表达式。该递归关系的形式为 $E_{t+1} \\le \\alpha E_{t} + C$，其中 $C = \\beta \\|e\\|_{2} = 1 \\times 1 = 1$。我们可以展开这个递归，以获得关于 $E_{0}$ 的 $E_{t}$ 的界。\n当 $t=1$ 时：\n$$E_{1} \\le \\alpha E_{0} + C$$\n当 $t=2$ 时：\n$$E_{2} \\le \\alpha E_{1} + C \\le \\alpha (\\alpha E_{0} + C) + C = \\alpha^{2} E_{0} + (\\alpha + 1)C$$\n当 $t=3$ 时：\n$$E_{3} \\le \\alpha E_{2} + C \\le \\alpha (\\alpha^{2} E_{0} + (\\alpha + 1)C) + C = \\alpha^{3} E_{0} + (\\alpha^{2} + \\alpha + 1)C$$\n通过归纳法，对于一个通用整数 $t > 0$，误差 $E_{t}$ 的界为：\n$$E_{t} \\le \\alpha^{t} E_{0} + C \\sum_{i=0}^{t-1} \\alpha^{i}$$\n求和项是一个有限几何级数。由于 $\\alpha = 0.4 \\neq 1$，我们可以使用几何级数求和公式：\n$$\\sum_{i=0}^{t-1} \\alpha^{i} = \\frac{1 - \\alpha^{t}}{1 - \\alpha}$$\n将此代回关于 $E_{t}$ 的不等式，得到：\n$$E_{t} \\le \\alpha^{t} E_{0} + C \\left( \\frac{1 - \\alpha^{t}}{1 - \\alpha} \\right)$$\n这个表达式可以重新排列，以清晰地显示对初始误差和稳态误差界的依赖关系：\n$$E_{t} \\le \\alpha^{t} \\left( E_{0} - \\frac{C}{1 - \\alpha} \\right) + \\frac{C}{1 - \\alpha}$$\n现在我们将给定的数值代入此表达式。我们有 $E_{0} = 10$，$\\alpha = 0.4$ 和 $C = 1$。项 $\\frac{C}{1-\\alpha}$ 为：\n$$\\frac{C}{1 - \\alpha} = \\frac{1}{1 - 0.4} = \\frac{1}{0.6} = \\frac{10}{6} = \\frac{5}{3}$$\n关于 $E_{t}$ 的不等式变为：\n$$E_{t} \\le (0.4)^{t} \\left( 10 - \\frac{5}{3} \\right) + \\frac{5}{3}$$\n化简括号中的项：\n$$10 - \\frac{5}{3} = \\frac{30}{3} - \\frac{5}{3} = \\frac{25}{3}$$\n因此，误差界为：\n$$E_{t} \\le (0.4)^{t} \\left( \\frac{25}{3} \\right) + \\frac{5}{3}$$\n我们需要找到满足 $E_{T} \\le 2$ 的最小整数 $T$。我们将这个条件施加到我们推导出的上界上，这是 $E_T \\le 2$ 的一个充分条件：\n$$(0.4)^{T} \\left( \\frac{25}{3} \\right) + \\frac{5}{3} \\le 2$$\n现在，我们求解 $T$。首先，分离含有 $T$ 的项：\n$$(0.4)^{T} \\left( \\frac{25}{3} \\right) \\le 2 - \\frac{5}{3} = \\frac{6-5}{3} = \\frac{1}{3}$$\n$$(0.4)^{T} \\le \\frac{1}{3} \\cdot \\frac{3}{25}$$\n$$(0.4)^{T} \\le \\frac{1}{25}$$\n为了求解 $T$，我们对两边取自然对数。注意 $0.4 = \\frac{2}{5}$ 并且 $\\frac{1}{25} = 5^{-2}$。\n$$\\ln\\left( (0.4)^{T} \\right) \\le \\ln\\left( \\frac{1}{25} \\right)$$\n$$T \\ln(0.4) \\le -\\ln(25)$$\n由于 $0.4  1$，其自然对数 $\\ln(0.4)$ 是一个负数。因此，当我们除以 $\\ln(0.4)$ 时，必须反转不等号的方向：\n$$T \\ge \\frac{-\\ln(25)}{\\ln(0.4)} = \\frac{\\ln(25)}{-\\ln(0.4)} = \\frac{\\ln(25)}{\\ln(1/0.4)} = \\frac{\\ln(25)}{\\ln(2.5)}$$\n为清晰起见，我们可以用素数的对数来表示它：\n$$T \\ge \\frac{\\ln(5^2)}{\\ln(5/2)} = \\frac{2\\ln(5)}{\\ln(5) - \\ln(2)}$$\n为了求数值，我们使用 $\\ln(5) \\approx 1.609438$ 和 $\\ln(2) \\approx 0.693147$：\n$$T \\ge \\frac{2 \\times 1.609438}{1.609438 - 0.693147} = \\frac{3.218876}{0.916291} \\approx 3.5126$$\n由于 $T$ 必须是整数，我们必须找到大于或等于 $3.5126$ 的最小整数。这个整数是 $4$。\n我们可以通过在不等式 $(0.4)^T \\le \\frac{1}{25} = 0.04$ 中检查 $T$ 的整数值来验证这一点。\n当 $T=3$ 时：$(0.4)^{3} = 0.064$。不等式 $0.064 \\le 0.04$ 不成立。\n当 $T=4$ 时：$(0.4)^{4} = 0.0256$。不等式 $0.0256 \\le 0.04$ 成立。\n因此，最小的整数迭代次数是 $T=4$。",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "对于一个给定的问题，理论收敛速率是固定的，但实际的迭代次数在很大程度上取决于初始点。这个编程练习将探讨一种强大的优化策略——“热启动”，即利用一个好的初始猜测来加速收敛。你将量化使用另一种方法（Lasso）的解来初始化IHT相对于随机初始化的显著性能优势，这体现了实用算法设计中的一个关键原则 。",
            "id": "3438852",
            "problem": "给定一个迭代硬阈值（Iterative Hard Thresholding, IHT）方法的收敛模型，该方法以两种方式初始化：随机初始化和通过采用 $k$-稀疏化的最小绝对收缩和选择算子（Lasso）估计器获得的温启动。假设两种初始化使用相同的测量矩阵，并且该矩阵满足限制等距性质（Restricted Isometry Property, RIP）条件，足以确保 IHT 的线性收敛上界：存在常数 $0  \\rho  1$ 和 $\\eta \\ge 0$（两者都仅取决于测量矩阵和噪声水平），使得迭代误差序列 $\\{E_t\\}_{t \\ge 0}$（定义为 $E_t = \\lVert x^t - x^\\star \\rVert_2$，其中 $x^\\star$ 是真实的 $k$-稀疏向量）遵循一致上界\n$$\nE_{t+1} \\le \\rho \\, E_t + \\eta \\quad \\text{for all } t \\ge 0.\n$$\n这种形式是在限制等距性质（RIP）下，限制强凸性/平滑性的一个广泛使用的推论。对于两种初始化，常数 $\\rho$ 和 $\\eta$ 是相同的，因为测量矩阵和噪声是相同的。设目标容差为 $\\tau > 0$，并定义由递推关系所暗示的误差下限 $F = \\eta / (1 - \\rho)$。如果 $\\tau \\le F$，则该上界排除了达到误差 $\\le \\tau$ 的可能性。\n\n对于给定的初始误差为 $E_0$ 的初始化，使用上述上界来保证 $E_t \\le \\tau$ 所需的最小迭代次数 $t^\\star$ 可以通过精确求解递推上界得到。如果 $E_0 \\le \\tau$，则 $t^\\star = 0$。如果 $E_0 > \\tau$ 但 $\\tau > F$，则满足\n$$\n\\rho^t \\, (E_0 - F) + F \\le \\tau\n$$\n的最小整数 $t^\\star$ 即为所需的迭代次数。这适用于任何初始化。在本问题中，您将量化在相同的 $(\\rho,\\eta)$ 条件下，从 $k$-稀疏化的 Lasso 温启动开始相较于随机初始化所减少的迭代次数。\n\n使用的缩略词：\n- 迭代硬阈值 (Iterative Hard Thresholding, IHT)。\n- 限制等距性质 (Restricted Isometry Property, RIP)。\n- 最小绝对收缩和选择算子 (Least Absolute Shrinkage and Selection Operator, Lasso)。\n\n您的任务是编写一个程序，为每个测试用例计算迭代次数的减少量\n$$\n\\Delta t = t^\\star_{\\text{rand}} - t^\\star_{\\text{warm}},\n$$\n其中 $t^\\star_{\\text{rand}}$ 使用 $E_0 = E_{0,\\text{rand}}$（随机初始化），$t^\\star_{\\text{warm}}$ 使用 $E_0 = E_{0,\\text{warm}}$（$k$-稀疏化 Lasso 初始化）。如果 $\\tau \\le F$，则输出 $-1$ 表示该上界不允许达到目标。否则，根据递推上界所暗示的公式计算 $t^\\star$：\n$$\nt^\\star =\n\\begin{cases}\n0,  \\text{if } E_0 \\le \\tau,\\\\\n\\left\\lceil \\dfrac{\\ln\\left(\\dfrac{\\tau - F}{E_0 - F}\\right)}{\\ln(\\rho)} \\right\\rceil,  \\text{if } E_0 > \\tau \\text{ and } \\tau > F,\n\\end{cases}\n$$\n其中所有对数均为自然对数，并且由于 $0  \\rho  1$，分母 $\\ln(\\rho)$ 为负。请注意，当 $E_0 > \\tau > F$ 时，分子是 $(0,1)$ 范围内数字的对数，因此该比值为正，符合要求。\n\n请使用以下测试套件。每个测试用例提供 $(\\rho,\\eta,\\tau,E_{0,\\text{rand}},E_{0,\\text{warm}})$：\n\n- 情况 A（一般情况）：$(\\rho,\\eta,\\tau,E_{0,\\text{rand}},E_{0,\\text{warm}}) = (0.6, 0.05, 0.2, 5.0, 1.0)$。\n- 情况 B（边界情况，目标不可达）：$(\\rho,\\eta,\\tau,E_{0,\\text{rand}},E_{0,\\text{warm}}) = (0.8, 0.1, 0.5, 1.0, 0.9)$。此处 $\\tau$ 等于下限 $F$。\n- 情况 C（温启动已在容差内）：$(\\rho,\\eta,\\tau,E_{0,\\text{rand}},E_{0,\\text{warm}}) = (0.5, 0.04, 0.1, 1.0, 0.08)$。\n- 情况 D（初始误差相等）：$(\\rho,\\eta,\\tau,E_{0,\\text{rand}},E_{0,\\text{warm}}) = (0.7, 0.03, 0.3, 1.0, 1.0)$。\n- 情况 E（慢收缩，紧容差）：$(\\rho,\\eta,\\tau,E_{0,\\text{rand}},E_{0,\\text{warm}}) = (0.9, 0.05, 0.55, 2.0, 0.8)$。\n\n最终输出格式：您的程序应生成单行输出，其中包含按上述测试套件顺序排列的结果，形式为逗号分隔的列表并用方括号括起，例如 $\\texttt{[r_A,r_B,r_C,r_D,r_E]}$，其中每个 $r_\\cdot$ 是该情况下的整数 $\\Delta t$，如果该情况下的目标不可达，则为 $-1$。此问题不涉及任何物理单位。",
            "solution": "该问题要求计算迭代硬阈值（IHT）方法在使用温启动初始化与随机初始化相比时迭代次数的减少量。该分析基于所提供的误差序列收敛的一致上界。\n\n收敛模型由 $\\ell_2$-范数误差 $E_t = \\lVert x^t - x^\\star \\rVert_2$ 的线性递推关系给出：\n$$\nE_{t+1} \\le \\rho \\, E_t + \\eta\n$$\n其中 $t \\ge 0$ 是迭代索引，$E_0$ 是初始误差，$\\rho$ 是满足 $0  \\rho  1$ 的收缩因子，$\\eta \\ge 0$ 是一个加性误差项，通常与测量噪声有关。此递推关系描述了向真实解的一个邻域线性收敛的过程。\n\n为了找到 $E_t$ 的一个显式上界，我们可以展开这个递推式：\n$E_1 \\le \\rho E_0 + \\eta$\n$E_2 \\le \\rho E_1 + \\eta \\le \\rho (\\rho E_0 + \\eta) + \\eta = \\rho^2 E_0 + \\eta(1+\\rho)$\n...\n$E_t \\le \\rho^t E_0 + \\eta \\sum_{i=0}^{t-1} \\rho^i = \\rho^t E_0 + \\eta \\left( \\frac{1 - \\rho^t}{1 - \\rho} \\right)$\n\n通过引入误差下限 $F = \\frac{\\eta}{1 - \\rho}$，可以重新整理这个表达式。该下限代表了当 $t \\to \\infty$ 时误差上界的极限。上界变为：\n$$\nE_t \\le \\rho^t E_0 + (1 - \\rho^t)F = \\rho^t (E_0 - F) + F\n$$\n这就是问题陈述中提供的闭式上界。我们的目标是找到最小的迭代次数 $t^\\star$，使得对于给定的容差 $\\tau > 0$，该上界能保证 $E_{t^\\star} \\le \\tau$。\n\n首先，我们必须评估在此上界下目标容差 $\\tau$ 是否可以达到。当 $t \\to \\infty$ 时，上界趋近于 $F$。因此，如果 $\\tau \\le F$，该上界永远不能保证误差会降到 $\\tau$ 以下。问题规定，在这种情况下，结果应指定为 $-1$。\n\n如果 $\\tau > F$，目标是可以达到的。我们寻求满足以下条件的最小整数 $t$：\n$$\n\\rho^t (E_0 - F) + F \\le \\tau\n$$\n如果初始状态已经满足容差，$E_0 \\le \\tau$，则不需要迭代，因此 $t^\\star = 0$。\n\n如果 $E_0 > \\tau$，这也意味着 $E_0 > F$（因为 $\\tau > F$），我们必须求解 $t$：\n$$\n\\rho^t (E_0 - F) \\le \\tau - F\n$$\n因为 $E_0 > F$，所以 $E_0 - F > 0$，我们可以直接相除而不改变不等号方向。\n$$\n\\rho^t \\le \\frac{\\tau - F}{E_0 - F}\n$$\n因为 $0  \\rho  1$，对数 $\\ln(\\rho)$ 是负数。对两边取自然对数会反转不等号：\n$$\nt \\ln(\\rho) \\ge \\ln\\left(\\frac{\\tau - F}{E_0 - F}\\right)\n$$\n$$\nt \\ge \\frac{\\ln\\left(\\frac{\\tau - F}{E_0 - F}\\right)}{\\ln(\\rho)}\n$$\n注意，由于 $E_0 > \\tau > F$，项 $\\frac{\\tau - F}{E_0 - F}$ 在 $0$ 和 $1$ 之间，使其对数为负。两个负数之比为正，这对于迭代次数是必需的。满足此条件的最小整数 $t$ 由向上取整函数给出：\n$$\nt^\\star = \\left\\lceil \\frac{\\ln\\left(\\frac{\\tau - F}{E_0 - F}\\right)}{\\ln(\\rho)} \\right\\rceil\n$$\n这证实了所提供的公式。总体任务是为每个测试用例计算 $\\Delta t = t^\\star_{\\text{rand}} - t^\\star_{\\text{warm}}$。\n\n我们现在将此过程应用于每个测试用例。\n\n情况 A：$(\\rho,\\eta,\\tau,E_{0,\\text{rand}},E_{0,\\text{warm}}) = (0.6, 0.05, 0.2, 5.0, 1.0)$\n- 误差下限：$F = \\frac{0.05}{1 - 0.6} = \\frac{0.05}{0.4} = 0.125$。\n- 可达性：$\\tau = 0.2 > F = 0.125$。目标可达。\n- 对于随机初始化 ($E_{0,\\text{rand}} = 5.0$)：$E_{0,\\text{rand}} > \\tau$。\n  $t^\\star_{\\text{rand}} = \\left\\lceil \\frac{\\ln((0.2 - 0.125)/(5.0 - 0.125))}{\\ln(0.6)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.075 / 4.875)}{\\ln(0.6)} \\right\\rceil \\approx \\lceil 8.171 \\rceil = 9$。\n- 对于温启动初始化 ($E_{0,\\text{warm}} = 1.0$)：$E_{0,\\text{warm}} > \\tau$。\n  $t^\\star_{\\text{warm}} = \\left\\lceil \\frac{\\ln((0.2 - 0.125)/(1.0 - 0.125))}{\\ln(0.6)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.075 / 0.875)}{\\ln(0.6)} \\right\\rceil \\approx \\lceil 4.809 \\rceil = 5$。\n- 迭代次数减少量：$\\Delta t = t^\\star_{\\text{rand}} - t^\\star_{\\text{warm}} = 9 - 5 = 4$。\n\n情况 B：$(\\rho,\\eta,\\tau,E_{0,\\text{rand}},E_{0,\\text{warm}}) = (0.8, 0.1, 0.5, 1.0, 0.9)$\n- 误差下限：$F = \\frac{0.1}{1 - 0.8} = \\frac{0.1}{0.2} = 0.5$。\n- 可达性：$\\tau = 0.5$，这意味着 $\\tau \\le F$ 成立。该上界不保证收敛到目标。\n- 结果：-1。\n\n情况 C：$(\\rho,\\eta,\\tau,E_{0,\\text{rand}},E_{0,\\text{warm}}) = (0.5, 0.04, 0.1, 1.0, 0.08)$\n- 误差下限：$F = \\frac{0.04}{1 - 0.5} = \\frac{0.04}{0.5} = 0.08$。\n- 可达性：$\\tau = 0.1 > F = 0.08$。目标可达。\n- 对于随机初始化 ($E_{0,\\text{rand}} = 1.0$)：$E_{0,\\text{rand}} > \\tau$。\n  $t^\\star_{\\text{rand}} = \\left\\lceil \\frac{\\ln((0.1 - 0.08)/(1.0 - 0.08))}{\\ln(0.5)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.02 / 0.92)}{\\ln(0.5)} \\right\\rceil \\approx \\lceil 5.523 \\rceil = 6$。\n- 对于温启动初始化 ($E_{0,\\text{warm}} = 0.08$)：$E_{0,\\text{warm}} = 0.08 \\le \\tau = 0.1$。初始误差已经在容差范围内。\n  $t^\\star_{\\text{warm}} = 0$。\n- 迭代次数减少量：$\\Delta t = t^\\star_{\\text{rand}} - t^\\star_{\\text{warm}} = 6 - 0 = 6$。\n\n情况 D：$(\\rho,\\eta,\\tau,E_{0,\\text{rand}},E_{0,\\text{warm}}) = (0.7, 0.03, 0.3, 1.0, 1.0)$\n- 误差下限：$F = \\frac{0.03}{1 - 0.7} = \\frac{0.03}{0.3} = 0.1$。\n- 可达性：$\\tau = 0.3 > F = 0.1$。目标可达。\n- 初始误差相同：$E_{0,\\text{rand}} = E_{0,\\text{warm}} = 1.0$。\n- 由于所有其他参数 $(\\rho, \\eta, \\tau)$ 相同，两种初始化所需的迭代次数将完全相同，即 $t^\\star_{\\text{rand}} = t^\\star_{\\text{warm}}$。\n- 迭代次数减少量：$\\Delta t = 0$。\n\n情况 E：$(\\rho,\\eta,\\tau,E_{0,\\text{rand}},E_{0,\\text{warm}}) = (0.9, 0.05, 0.55, 2.0, 0.8)$\n- 误差下限：$F = \\frac{0.05}{1 - 0.9} = \\frac{0.05}{0.1} = 0.5$。\n- 可达性：$\\tau = 0.55 > F = 0.5$。目标可达。\n- 对于随机初始化 ($E_{0,\\text{rand}} = 2.0$)：$E_{0,\\text{rand}} > \\tau$。\n  $t^\\star_{\\text{rand}} = \\left\\lceil \\frac{\\ln((0.55 - 0.5)/(2.0 - 0.5))}{\\ln(0.9)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.05 / 1.5)}{\\ln(0.9)} \\right\\rceil \\approx \\lceil 32.28 \\rceil = 33$。\n- 对于温启动初始化 ($E_{0,\\text{warm}} = 0.8$)：$E_{0,\\text{warm}} > \\tau$。\n  $t^\\star_{\\text{warm}} = \\left\\lceil \\frac{\\ln((0.55 - 0.5)/(0.8 - 0.5))}{\\ln(0.9)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.05 / 0.3)}{\\ln(0.9)} \\right\\rceil \\approx \\lceil 17.005 \\rceil = 18$。\n- 迭代次数减少量：$\\Delta t = t^\\star_{\\text{rand}} - t^\\star_{\\text{warm}} = 33 - 18 = 15$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the reduction in IHT iteration counts due to warm-starting\n    for a series of test cases.\n    \"\"\"\n    \n    # Each tuple represents a test case with parameters:\n    # (rho, eta, tau, E_0_rand, E_0_warm)\n    test_cases = [\n        (0.6, 0.05, 0.2, 5.0, 1.0),    # Case A\n        (0.8, 0.1, 0.5, 1.0, 0.9),    # Case B\n        (0.5, 0.04, 0.1, 1.0, 0.08),   # Case C\n        (0.7, 0.03, 0.3, 1.0, 1.0),    # Case D\n        (0.9, 0.05, 0.55, 2.0, 0.8),   # Case E\n    ]\n\n    def compute_iterations(rho, F, tau, E0):\n        \"\"\"\n        Calculates the number of iterations t_star required to reach tolerance tau.\n        This function assumes the attainability condition (tau > F) is already checked.\n        \"\"\"\n        # If initial error is already within tolerance, no iterations are needed.\n        if E0 = tau:\n            return 0\n        \n        # If E0 > tau, apply the formula derived from the convergence bound.\n        # The arguments to the logarithm are guaranteed to be positive under\n        # the conditions E0 > tau > F.\n        numerator = np.log((tau - F) / (E0 - F))\n        denominator = np.log(rho)\n        \n        # The result must be an integer, hence the ceiling function.\n        return int(np.ceil(numerator / denominator))\n\n    results = []\n    for case in test_cases:\n        rho, eta, tau, E0_rand, E0_warm = case\n        \n        # The contraction factor rho must be in (0, 1) for convergence.\n        # If rho is 1, 1-rho is 0, leading to division by zero.\n        if rho == 1.0:\n            # This case is ill-defined in the model\n            # but we can handle it defensively.\n            results.append(-1)\n            continue\n            \n        # Calculate the error floor F.\n        F = eta / (1 - rho)\n        \n        # Check the attainability condition. If tau = F, the bound does not\n        # guarantee the error can be reduced to the target tolerance.\n        if tau = F:\n            results.append(-1)\n            continue\n            \n        # If the target is attainable, compute the iteration counts for both\n        # random and warm-start initializations.\n        t_rand = compute_iterations(rho, F, tau, E0_rand)\n        t_warm = compute_iterations(rho, F, tau, E0_warm)\n        \n        # The result for the case is the reduction in iterations.\n        delta_t = t_rand - t_warm\n        results.append(delta_t)\n\n    # Format the final output as a comma-separated list in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}