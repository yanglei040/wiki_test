{
    "hands_on_practices": [
        {
            "introduction": "The best way to understand an algorithm is to perform its operations by hand. This first practice provides a concrete, step-by-step walkthrough of a single Stagewise Orthogonal Matching Pursuit (StOMP) stage, from the initial correlation computation to the final residual update. By working through this small-scale example , you will solidify your grasp of the core mechanics of the algorithm and see how the thresholding and least-squares steps interact to identify a candidate support set.",
            "id": "3481077",
            "problem": "Consider the linear measurement model $y = A x^{\\star} + w$ with additive white Gaussian noise $w \\sim \\mathcal{N}(0, \\sigma^{2} I)$, and suppose we apply the first stage of Stagewise Orthogonal Matching Pursuit (StOMP). Let the sensing matrix $A \\in \\mathbb{R}^{4 \\times 4}$ have unit-norm columns given by the normalized Hadamard system\n$$\nA \\;=\\; \\frac{1}{2}\\begin{bmatrix}\n1  1  1  1\\\\\n1  1  -1  -1\\\\\n1  -1  1  -1\\\\\n1  -1  -1  1\n\\end{bmatrix}.\n$$\nLet the observed data be\n$$\ny \\;=\\; \\begin{bmatrix} 2 \\\\ 0 \\\\ 0.5 \\\\ -1 \\end{bmatrix},\n$$\nand the noise standard deviation be $\\sigma = 0.2$. At the initial stage, StOMP uses the residual $r^{(0)} = y$, the correlation vector $c^{(0)} = A^{\\top} r^{(0)}$, a universal Gaussian threshold\n$$\n\\tau_0 \\;=\\; \\sigma \\sqrt{2 \\ln n},\n$$\nwhere $n$ is the number of columns of $A$, and it selects the index set\n$$\nJ_0 \\;=\\; \\{\\, j \\in \\{1,\\dots,n\\} \\;\\colon\\; |c^{(0)}_j|  \\tau_0 \\,\\}.\n$$\nThen it performs a least-squares update on the support $S_0 = J_0$ to obtain $x^{(1)}_{S_0} = \\arg\\min_{x \\in \\mathbb{R}^{|S_0|}} \\|\\, y - A_{S_0} x \\,\\|_{2}^{2}$ (and zeros outside $S_0$), followed by the new residual $r^{(1)} = y - A_{S_0} x^{(1)}_{S_0}$.\n\nStarting from these definitions and the assumption of Gaussian noise with variance $\\sigma^{2}$ per entry, compute explicitly:\n- $c^{(0)}$,\n- $\\tau_0$,\n- $J_0$,\n- the least-squares estimate on $S_0$,\n- and the new residual $r^{(1)}$.\n\nFinally, report the single scalar quantity $\\| r^{(1)} \\|_{2}^{2}$. Provide your answer as an exact real number (no rounding).",
            "solution": "The problem requires the computation of the squared $L_2$-norm of the residual after the first stage of the Stagewise Orthogonal Matching Pursuit (StOMP) algorithm. We will proceed by calculating the intermediate quantities as defined in the problem description.\n\nThe given linear model is $y = A x^{\\star} + w$, with sensing matrix $A \\in \\mathbb{R}^{4 \\times 4}$, observed data $y$, and noise standard deviation $\\sigma$.\n$$\nA = \\frac{1}{2}\\begin{bmatrix}\n1  1  1  1\\\\\n1  1  -1  -1\\\\\n1  -1  1  -1\\\\\n1  -1  -1  1\n\\end{bmatrix}, \\quad y = \\begin{bmatrix} 2 \\\\ 0 \\\\ 0.5 \\\\ -1 \\end{bmatrix}, \\quad \\sigma = 0.2\n$$\nThe number of columns of $A$ is $n=4$. The matrix $A$ is a scaled Hadamard matrix. We can verify that it is symmetric, $A^{\\top} = A$, and that its columns are orthonormal, meaning $A^{\\top}A = I$. This makes $A$ an orthogonal matrix.\n\n- **Step 1: Compute the initial correlation vector $c^{(0)}$**\nThe initial residual is $r^{(0)} = y$. The correlation vector is $c^{(0)} = A^{\\top} r^{(0)} = A^{\\top} y$. Since $A$ is symmetric, $c^{(0)} = A y$.\n$$\nc^{(0)} = \\frac{1}{2}\\begin{bmatrix}\n1  1  1  1\\\\\n1  1  -1  -1\\\\\n1  -1  1  -1\\\\\n1  -1  -1  1\n\\end{bmatrix}\n\\begin{bmatrix} 2 \\\\ 0 \\\\ 0.5 \\\\ -1 \\end{bmatrix}\n= \\frac{1}{2}\\begin{bmatrix}\n1(2) + 1(0) + 1(0.5) + 1(-1) \\\\\n1(2) + 1(0) - 1(0.5) - 1(-1) \\\\\n1(2) - 1(0) + 1(0.5) - 1(-1) \\\\\n1(2) - 1(0) - 1(0.5) + 1(-1)\n\\end{bmatrix}\n= \\frac{1}{2}\\begin{bmatrix}\n1.5 \\\\\n2.5 \\\\\n3.5 \\\\\n0.5\n\\end{bmatrix}\n= \\begin{bmatrix}\n0.75 \\\\\n1.25 \\\\\n1.75 \\\\\n0.25\n\\end{bmatrix}\n$$\nIn fractional form, which is exact, $c^{(0)} = \\begin{bmatrix} 3/4 \\\\ 5/4 \\\\ 7/4 \\\\ 1/4 \\end{bmatrix}$.\n\n- **Step 2: Compute the threshold $\\tau_0$**\nThe threshold is defined as $\\tau_0 = \\sigma \\sqrt{2 \\ln n}$. With $\\sigma = 0.2$ and $n=4$, we have:\n$$\n\\tau_0 = 0.2 \\sqrt{2 \\ln 4} = 0.2 \\sqrt{2 \\ln(2^2)} = 0.2 \\sqrt{4 \\ln 2} = 0.4 \\sqrt{\\ln 2}\n$$\n\n- **Step 3: Determine the index set $J_0$**\nThe index set $J_0$ contains indices $j$ for which $|c^{(0)}_j|  \\tau_0$. We compare the squared magnitudes $|c^{(0)}_j|^2$ with $\\tau_0^2 = (0.4)^2 \\ln 2 = 0.16 \\ln 2$.\nUsing the approximation $\\ln 2 \\approx 0.6931$, $\\tau_0^2 \\approx 0.16 \\times 0.6931 \\approx 0.1109$.\nThe squared magnitudes of the correlations are:\n$|c^{(0)}_1|^2 = (0.75)^2 = 0.5625$\n$|c^{(0)}_2|^2 = (1.25)^2 = 1.5625$\n$|c^{(0)}_3|^2 = (1.75)^2 = 3.0625$\n$|c^{(0)}_4|^2 = (0.25)^2 = 0.0625$\n\nComparing these values to $\\tau_0^2 \\approx 0.1109$:\n$|c^{(0)}_1|^2  \\tau_0^2$\n$|c^{(0)}_2|^2  \\tau_0^2$\n$|c^{(0)}_3|^2  \\tau_0^2$\n$|c^{(0)}_4|^2  \\tau_0^2$\nThus, the selected index set is $J_0 = \\{1, 2, 3\\}$. The support set for the first iteration is $S_0 = J_0 = \\{1, 2, 3\\}$.\n\n- **Step 4: Compute the least-squares estimate $x^{(1)}_{S_0}$ and the new residual $r^{(1)}$**\nThe least-squares estimate on the support $S_0$ is given by $x^{(1)}_{S_0} = \\arg\\min_{x} \\| y - A_{S_0} x \\|_{2}^{2}$, where $A_{S_0}$ is the matrix formed by columns $\\{1, 2, 3\\}$ of $A$. The solution is given by the normal equations: $x^{(1)}_{S_0} = (A_{S_0}^{\\top} A_{S_0})^{-1} A_{S_0}^{\\top} y$.\nAs noted earlier, the columns of $A$ are orthonormal. Let $a_j$ denote the $j$-th column of $A$. Then $a_i^{\\top} a_j = \\delta_{ij}$ (the Kronecker delta).\nFor $S_0 = \\{1, 2, 3\\}$, the submatrix is $A_{S_0} = [a_1, a_2, a_3]$. The Gram matrix is $A_{S_0}^{\\top} A_{S_0} = I_3$, the $3 \\times 3$ identity matrix.\nThe least-squares solution simplifies to $x^{(1)}_{S_0} = A_{S_0}^{\\top} y$. The components of this vector are $(A_{S_0}^{\\top} y)_i = a_i^{\\top} y = c_i^{(0)}$ for $i \\in S_0$.\nSo, $x^{(1)}_{S_0} = \\begin{bmatrix} c_1^{(0)} \\\\ c_2^{(0)} \\\\ c_3^{(0)} \\end{bmatrix} = \\begin{bmatrix} 0.75 \\\\ 1.25 \\\\ 1.75 \\end{bmatrix}$.\n\nThe new residual is $r^{(1)} = y - A_{S_0} x^{(1)}_{S_0}$. The term $A_{S_0} x^{(1)}_{S_0} = A_{S_0} A_{S_0}^{\\top} y$ is the orthogonal projection of $y$ onto the subspace spanned by the columns in $A_{S_0}$, i.e., $\\mathrm{span}\\{a_1, a_2, a_3\\}$.\nThe residual $r^{(1)}$ is therefore the projection of $y$ onto the orthogonal complement of this subspace. Since $\\{a_1, a_2, a_3, a_4\\}$ forms an orthonormal basis for $\\mathbb{R}^4$, the orthogonal complement is spanned by the vector $a_4$.\nThe projection of $y$ onto the subspace spanned by $a_4$ is given by:\n$$ r^{(1)} = (a_4^{\\top} y) a_4 $$\nThe term $a_4^{\\top} y$ is precisely the fourth component of the correlation vector, $c_4^{(0)}$.\nFrom Step 1, $c_4^{(0)} = 0.25 = \\frac{1}{4}$.\nSo, $r^{(1)} = c_4^{(0)} a_4 = \\frac{1}{4} a_4$.\n\n- **Step 5: Compute the final quantity $\\| r^{(1)} \\|_{2}^{2}$**\nWe need to find the squared $L_2$-norm of the residual $r^{(1)}$.\n$$\n\\|r^{(1)}\\|_2^2 = \\left\\| \\frac{1}{4} a_4 \\right\\|_2^2 = \\left(\\frac{1}{4}\\right)^2 \\|a_4\\|_2^2\n$$\nSince the columns of $A$ are unit-norm by construction (as stated in the problem and verified), we have $\\|a_4\\|_2^2 = 1$.\nTherefore, the squared norm of the residual simplifies to:\n$$\n\\|r^{(1)}\\|_2^2 = \\left(\\frac{1}{4}\\right)^2 \\times 1 = \\frac{1}{16}\n$$\nThe final answer is an exact real number.\nAs a fraction, $\\frac{1}{16}$, or as a decimal, $0.0625$. We will provide the fractional form.",
            "answer": "$$\n\\boxed{\\frac{1}{16}}\n$$"
        },
        {
            "introduction": "The true test of a recovery algorithm lies in its performance on \"ill-behaved\" dictionaries with highly correlated atoms, a common challenge in practice. This exercise  presents a carefully constructed scenario to compare StOMP's batch selection strategy against the one-at-a-time approach of Orthogonal Matching Pursuit (OMP). Analyzing this case will reveal the potential pitfalls and advantages of including multiple atoms per stage, offering crucial insights into why StOMP can be both faster and sometimes less precise than OMP.",
            "id": "3481088",
            "problem": "Consider a sensing matrix (dictionary) with $m=3$ and $n=4$ whose unit-norm columns $\\{a_j\\}_{j=1}^4 \\subset \\mathbb{R}^3$ are given explicitly by\n$$\na_1=\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix},\\quad\na_2=\\begin{bmatrix}0.9\\\\ \\sqrt{1-0.9^2}\\\\ 0\\end{bmatrix},\\quad\na_3=\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix},\\quad\na_4=\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}.\n$$\nLet the unknown signal be $x^\\star\\in\\mathbb{R}^4$ with $k$-sparse support $\\operatorname{supp}(x^\\star)=\\{1,3\\}$ and values $x^\\star_1=1$, $x^\\star_3=1$, and $x^\\star_2=x^\\star_4=0$. The noiseless measurement is $y=Ax^\\star=a_1+a_3=\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}$.\n\nTwo greedy methods are compared:\n\n- Orthogonal Matching Pursuit (OMP): At each iteration $t$, compute the residual $r^{(t)}=y-A_{S^{(t)}}\\hat{c}^{(t)}$, where $S^{(t)}$ is the selected support after $t$ iterations and $\\hat{c}^{(t)}$ is the least-squares solution over $S^{(t)}$; then select the new index $j^{(t+1)}\\in\\arg\\max_j |\\langle a_j,r^{(t)}\\rangle|$, augment $S^{(t+1)}=S^{(t)}\\cup\\{j^{(t+1)}\\}$, refit by least squares on $S^{(t+1)}$, and repeat. Assume ties in $\\arg\\max$ are broken by choosing the smallest index.\n\n- Stagewise Orthogonal Matching Pursuit (StOMP): In stage $1$, compute the correlations $c_j=\\langle a_j,y\\rangle$ for all $j$, and select the batch $S=\\{j:\\,|c_j|\\ge \\alpha \\max_k |c_k|\\}$ with a fixed threshold factor $\\alpha=0.85$. Then perform a single least-squares refit over $S$ to obtain coefficients $\\hat{c}_S$, set the residual $r=y-A_S\\hat{c}_S$, and terminate if $r=0$.\n\nStarting from these definitions and the linear-algebraic properties of inner products and orthogonal projections, analyze the first stage/iterations of both methods on the given $A$ and $y$. Which of the following statements is/are correct?\n\nA. StOMP’s first stage selects $S=\\{1,2,3\\}$ because $|a_1^\\top y|=|a_3^\\top y|=1$ and $|a_2^\\top y|=0.9\\ge 0.85\\cdot 1$, and its least-squares refit over $S$ returns coefficients with $\\hat{c}_2=0$, yielding exact recovery of the true support $\\{1,3\\}$ in one stage; OMP, with the stated tie-break, selects $1$ in the first iteration, projects the residual onto the orthogonal complement of $\\operatorname{span}\\{a_1\\}$ to obtain $r^{(1)}=a_3$, then selects $3$ next, also yielding exact recovery.\n\nB. OMP may select $2$ in the first iteration because $|a_2^\\top y|$ equals $1$, which ties with $|a_1^\\top y|$ and $|a_3^\\top y|$.\n\nC. The least-squares problem over StOMP’s batch $S=\\{1,2,3\\}$ is singular due to the high coherence between $a_1$ and $a_2$, so StOMP cannot uniquely recover coefficients in this stage.\n\nD. Because StOMP includes a highly coherent but spurious atom $a_2$ in the first stage, its residual after least squares cannot be zero in the noiseless setting, whereas OMP can drive the residual to zero.\n\nE. If OMP breaks the initial tie by selecting $3$ first, then in the second iteration it must select $2$ before $1$ because $a_2$ remains more correlated with the updated residual than $a_1$.\n\nSelect all that apply.",
            "solution": "The user has provided a problem in the field of compressed sensing, asking for an analysis of the first stage/iteration of two greedy recovery algorithms: Stagewise Orthogonal Matching Pursuit (StOMP) and Orthogonal Matching Pursuit (OMP).\n\n### Problem Statement Validation\n\nFirst, I will validate the problem statement.\n\n**Step 1: Extract Givens**\n- Sensing matrix (dictionary) $A \\in \\mathbb{R}^{3 \\times 4}$ with $m=3$ and $n=4$.\n- The columns of $A$, denoted $\\{a_j\\}_{j=1}^4$, are unit-norm vectors in $\\mathbb{R}^3$:\n$$a_1=\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix},\\quad a_2=\\begin{bmatrix}0.9\\\\ \\sqrt{1-0.9^2}\\\\ 0\\end{bmatrix},\\quad a_3=\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix},\\quad a_4=\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}.$$\nNote that $\\sqrt{1-0.9^2} = \\sqrt{1-0.81} = \\sqrt{0.19}$.\n- The unknown signal is $x^\\star\\in\\mathbb{R}^4$.\n- The signal is $k$-sparse with $k=2$.\n- The support is $\\operatorname{supp}(x^\\star)=\\{1,3\\}$.\n- The non-zero values are $x^\\star_1=1$ and $x^\\star_3=1$. The zero values are $x^\\star_2=x^\\star_4=0$.\n- The noiseless measurement vector is $y=Ax^\\star=a_1x^\\star_1 + a_3x^\\star_3 = a_1+a_3=\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}$.\n- OMP algorithm definition: At each iteration $t$, compute residual $r^{(t)}=y-A_{S^{(t)}}\\hat{c}^{(t)}$, select next index $j^{(t+1)}\\in\\arg\\max_j |\\langle a_j,r^{(t)}\\rangle|$, augment support $S^{(t+1)}=S^{(t)}\\cup\\{j^{(t+1)}\\}$, and refit. Ties are broken by choosing the smallest index.\n- StOMP algorithm definition: In stage $1$, compute correlations $c_j=\\langle a_j,y\\rangle$, select batch $S=\\{j:\\,|c_j|\\ge \\alpha \\max_k |c_k|\\}$ with threshold factor $\\alpha=0.85$. Perform a single least-squares refit on $S$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard, well-defined exercise in sparse signal recovery, a subfield of signal processing and optimization. The algorithms OMP and StOMP are canonical methods in this area. The underlying mathematics are based on linear algebra. The problem is sound.\n- **Well-Posed:** All necessary data (matrix $A$, signal $x^\\star$, measurement $y$) and algorithm specifications (parameter $\\alpha$, tie-breaking rule) are provided. The question asks for a direct analysis of the algorithms' behavior, which is a deterministic process given the inputs.\n- **Objective:** The problem is stated using precise mathematical language, free from any subjective or ambiguous terminology.\n- **Completeness and Consistency:** Let's check the given values. The columns are stated to be unit-norm.\n  - $\\|a_1\\|_2 = \\sqrt{1^2+0^2+0^2}=1$.\n  - $\\|a_2\\|_2 = \\sqrt{0.9^2 + (\\sqrt{0.19})^2 + 0^2} = \\sqrt{0.81+0.19} = \\sqrt{1} = 1$.\n  - $\\|a_3\\|_2 = \\sqrt{0^2+0^2+1^2}=1$.\n  - $\\|a_4\\|_2 = \\sqrt{0^2+1^2+0^2}=1$.\nThe columns are indeed unit-norm. The measurement vector calculation $y = a_1+a_3 = [1,0,0]^T + [0,0,1]^T = [1,0,1]^T$ is correct. All data are consistent and properly defined.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, objective, complete, and internally consistent. I will proceed with the analysis of the algorithms.\n\n### Derivation and Analysis\n\nWe will analyze StOMP and OMP separately based on the provided data.\n\n**Analysis of Stagewise Orthogonal Matching Pursuit (StOMP)**\n\nThe StOMP algorithm proceeds in stages. We analyze the first stage.\n1.  **Compute Correlations:** We compute the inner product of each dictionary atom $a_j$ with the measurement vector $y=\\begin{bmatrix}101\\end{bmatrix}^T$.\n    - $c_1 = \\langle a_1, y \\rangle = a_1^T y = \\begin{bmatrix}100\\end{bmatrix} \\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} = 1$.\n    - $c_2 = \\langle a_2, y \\rangle = a_2^T y = \\begin{bmatrix}0.9\\sqrt{0.19}0\\end{bmatrix} \\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} = 0.9$.\n    - $c_3 = \\langle a_3, y \\rangle = a_3^T y = \\begin{bmatrix}001\\end{bmatrix} \\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} = 1$.\n    - $c_4 = \\langle a_4, y \\rangle = a_4^T y = \\begin{bmatrix}010\\end{bmatrix} \\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} = 0$.\n\n2.  **Thresholding:** We identify the set of indices $S$ for which the absolute correlation $|c_j|$ exceeds a threshold.\n    - The maximum absolute correlation is $\\max_k |c_k| = \\max\\{|1|, |0.9|, |1|, |0|\\} = 1$.\n    - The threshold factor is given as $\\alpha = 0.85$.\n    - The threshold value is $\\alpha \\max_k |c_k| = 0.85 \\cdot 1 = 0.85$.\n    - We select indices $j$ such that $|c_j| \\ge 0.85$:\n        - $|c_1| = 1 \\ge 0.85 \\implies 1 \\in S$.\n        - $|c_2| = 0.9 \\ge 0.85 \\implies 2 \\in S$.\n        - $|c_3| = 1 \\ge 0.85 \\implies 3 \\in S$.\n        - $|c_4| = 0  0.85 \\implies 4 \\notin S$.\n    - The selected batch of indices is $S = \\{1, 2, 3\\}$.\n\n3.  **Least-Squares Refit:** We solve for the coefficients $\\hat{c}_S$ that minimize $\\|y - A_S c\\|_2^2$, where $A_S = \\begin{bmatrix}a_1  a_2  a_3\\end{bmatrix}$.\n    - $A_S = \\begin{bmatrix} 1  0.9  0 \\\\ 0  \\sqrt{0.19}  0 \\\\ 0  0  1 \\end{bmatrix}$.\n    - The least-squares problem is to solve $A_S \\hat{c}_S = y$. Since $A_S$ is a $3 \\times 3$ matrix, we can check if it is invertible. The determinant is $\\det(A_S) = 1 \\cdot (\\sqrt{0.19} \\cdot 1 - 0 \\cdot 0) = \\sqrt{0.19} \\neq 0$. The matrix is invertible, so there is a unique solution.\n    - We solve the system of linear equations for $\\hat{c}_S = \\begin{bmatrix}\\hat{c}_1  \\hat{c}_2  \\hat{c}_3\\end{bmatrix}^T$:\n    $$ \\begin{bmatrix} 1  0.9  0 \\\\ 0  \\sqrt{0.19}  0 \\\\ 0  0  1 \\end{bmatrix} \\begin{bmatrix} \\hat{c}_1 \\\\ \\hat{c}_2 \\\\ \\hat{c}_3 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} $$\n    - From the third row: $\\hat{c}_3 = 1$.\n    - From the second row: $\\sqrt{0.19} \\cdot \\hat{c}_2 = 0 \\implies \\hat{c}_2 = 0$.\n    - From the first row: $1 \\cdot \\hat{c}_1 + 0.9 \\cdot \\hat{c}_2 = 1 \\implies \\hat{c}_1 + 0.9 \\cdot 0 = 1 \\implies \\hat{c}_1 = 1$.\n    - The coefficient vector on the support $S$ is $\\hat{c}_S = \\begin{bmatrix}1  0  1\\end{bmatrix}^T$.\n    - The residual is $r = y - A_S \\hat{c}_S = y - (1 \\cdot a_1 + 0 \\cdot a_2 + 1 \\cdot a_3) = y - (a_1+a_3)$. Since $y=a_1+a_3$, the residual $r=0$.\n    - The recovered signal has non-zero entries at indices $\\{1, 3\\}$, which exactly matches the true support $\\operatorname{supp}(x^\\star)$. Thus, StOMP achieves exact recovery in a single stage.\n\n**Analysis of Orthogonal Matching Pursuit (OMP)**\n\nThe OMP algorithm proceeds iteratively.\n- **Initialization:** $S^{(0)} = \\emptyset$, residual $r^{(0)} = y = \\begin{bmatrix}101\\end{bmatrix}^T$.\n\n- **Iteration 1:**\n    1. **Selection:** Find the index $j^{(1)}$ that maximizes $|\\langle a_j, r^{(0)} \\rangle|$. The correlations are the same as in the StOMP analysis: $|\\langle a_1, y \\rangle| = 1$, $|\\langle a_2, y \\rangle| = 0.9$, $|\\langle a_3, y \\rangle| = 1$, $|\\langle a_4, y \\rangle| = 0$.\n    2. The maximum value is $1$, and there is a tie between indices $j=1$ and $j=3$. The problem states that ties are broken by choosing the smallest index. Therefore, $j^{(1)} = 1$.\n    3. **Augment Support:** $S^{(1)} = S^{(0)} \\cup \\{1\\} = \\{1\\}$.\n    4. **Refit:** Solve for $\\hat{c}^{(1)}$ minimizing $\\|y - A_{S^{(1)}} c\\|_2^2$, where $A_{S^{(1)}} = a_1$. The solution is $\\hat{c}_1^{(1)} = \\frac{\\langle a_1, y \\rangle}{\\|a_1\\|_2^2} = \\frac{1}{1} = 1$.\n    5. **Update Residual:** $r^{(1)} = y - A_{S^{(1)}}\\hat{c}^{(1)} = y - 1 \\cdot a_1 = (a_1+a_3) - a_1 = a_3$. So, $r^{(1)} = \\begin{bmatrix}001\\end{bmatrix}^T$.\n\n- **Iteration 2:**\n    1. **Selection:** Find the index $j^{(2)}$ that maximizes $|\\langle a_j, r^{(1)} \\rangle|$.\n       - $|\\langle a_1, r^{(1)} \\rangle| = |\\langle a_1, a_3 \\rangle| = 0$.\n       - $|\\langle a_2, r^{(1)} \\rangle| = |\\langle a_2, a_3 \\rangle| = \\left| \\begin{bmatrix}0.9\\sqrt{0.19}0\\end{bmatrix} \\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix} \\right| = 0$.\n       - $|\\langle a_3, r^{(1)} \\rangle| = |\\langle a_3, a_3 \\rangle| = \\|a_3\\|_2^2 = 1$.\n       - $|\\langle a_4, r^{(1)} \\rangle| = |\\langle a_4, a_3 \\rangle| = \\left| \\begin{bmatrix}010\\end{bmatrix} \\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix} \\right| = 0$.\n    2. The maximum correlation is unambiguously $1$, for index $j^{(2)} = 3$.\n    3. **Augment Support:** $S^{(2)} = S^{(1)} \\cup \\{3\\} = \\{1, 3\\}$.\n    4. **Refit:** Solve for $\\hat{c}^{(2)}$ minimizing $\\|y - A_{S^{(2)}} c\\|_2^2$, where $A_{S^{(2)}} = \\begin{bmatrix} a_1  a_3 \\end{bmatrix}$. The normal equations are $(A_{S^{(2)}})^T A_{S^{(2)}} \\hat{c}^{(2)} = (A_{S^{(2)}})^T y$.\n       Since $a_1$ and $a_3$ are orthogonal ($\\langle a_1, a_3 \\rangle = 0$), $(A_{S^{(2)}})^T A_{S^{(2)}} = \\begin{bmatrix} \\|a_1\\|^2  0 \\\\ 0  \\|a_3\\|^2 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$.\n       The solution is $\\hat{c}^{(2)} = (A_{S^{(2)}})^T y = \\begin{bmatrix} \\langle a_1, y \\rangle \\\\ \\langle a_3, y \\rangle \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n    5. **Update Residual:** $r^{(2)} = y - A_{S^{(2)}}\\hat{c}^{(2)} = y - (1 \\cdot a_1 + 1 \\cdot a_3) = (a_1+a_3) - (a_1+a_3) = 0$.\n    - The residual is now zero, so OMP terminates. It has correctly identified the support $\\{1, 3\\}$ and the coefficients, achieving exact recovery in $2$ iterations.\n\n### Option-by-Option Analysis\n\n**A. StOMP’s first stage selects $S=\\{1,2,3\\}$ because $|a_1^\\top y|=|a_3^\\top y|=1$ and $|a_2^\\top y|=0.9\\ge 0.85\\cdot 1$, and its least-squares refit over $S$ returns coefficients with $\\hat{c}_2=0$, yielding exact recovery of the true support $\\{1,3\\}$ in one stage; OMP, with the stated tie-break, selects $1$ in the first iteration, projects the residual onto the orthogonal complement of $\\operatorname{span}\\{a_1\\}$ to obtain $r^{(1)}=a_3$, then selects $3$ next, also yielding exact recovery.**\n- The analysis of StOMP showed that $S=\\{1,2,3\\}$ is selected. The reason given is correct.\n- The least-squares refit for StOMP resulted in $\\hat{c}_S = \\begin{bmatrix}101\\end{bmatrix}^T$, so $\\hat{c}_2=0$. This recovers the true sparse support $\\{1,3\\}$.\n- The analysis of OMP showed that due to the tie-break, index $1$ is selected first.\n- The updated residual after the first OMP step is $r^{(1)} = y - \\text{Proj}_{\\text{span}\\{a_1\\}}(y) = y - a_1 = a_3$.\n- The second OMP iteration then selects index $3$, leading to exact recovery.\n- Every claim in this statement is consistent with our derivation.\n- Verdict: **Correct**.\n\n**B. OMP may select $2$ in the first iteration because $|a_2^\\top y|$ equals $1$, which ties with $|a_1^\\top y|$ and $|a_3^\\top y|$.**\n- In the first iteration of OMP, we calculated the correlations with $y$. We found $|\\langle a_2, y \\rangle| = 0.9$.\n- The values for indices $1$ and $3$ were $|\\langle a_1, y \\rangle|=1$ and $|\\langle a_3, y \\rangle|=1$.\n- Since $0.9 \\neq 1$, the claim that $|\\langle a_2, y \\rangle|$ equals $1$ is false. Therefore, OMP cannot select index $2$ in the first iteration.\n- Verdict: **Incorrect**.\n\n**C. The least-squares problem over StOMP’s batch $S=\\{1,2,3\\}$ is singular due to the high coherence between $a_1$ and $a_2$, so StOMP cannot uniquely recover coefficients in this stage.**\n- The least-squares problem involves the matrix $A_S = \\begin{bmatrix}a_1  a_2  a_3\\end{bmatrix}$. We calculated its determinant to be $\\det(A_S) = \\sqrt{0.19} \\neq 0$.\n- A square matrix with a non-zero determinant is non-singular (invertible). Thus, the columns $\\{a_1, a_2, a_3\\}$ are linearly independent.\n- A non-singular matrix ensures that the least-squares problem has a unique solution. The problem is not singular. While the coherence $|\\langle a_1, a_2 \\rangle| = 0.9$ is high, it is less than $1$, so the vectors are not collinear and the set $\\{a_1, a_2, a_3\\}$ is linearly independent.\n- Verdict: **Incorrect**.\n\n**D. Because StOMP includes a highly coherent but spurious atom $a_2$ in the first stage, its residual after least squares cannot be zero in the noiseless setting, whereas OMP can drive the residual to zero.**\n- Our analysis of StOMP showed that the least-squares fit on the support $S=\\{1,2,3\\}$ yields coefficients $\\hat{c}_S = \\begin{bmatrix}101\\end{bmatrix}^T$.\n- The reconstructed signal is $A_S \\hat{c}_S = 1 \\cdot a_1 + 0 \\cdot a_2 + 1 \\cdot a_3 = a_1 + a_3 = y$.\n- The residual is $r = y - A_S \\hat{c}_S = y-y=0$.\n- The statement that the residual \"cannot be zero\" is false. When the true signal $y$ lies in the span of the selected atoms (which is true here, as $y=a_1+a_3$ is in $\\text{span}\\{a_1, a_2, a_3\\}$), the least-squares projection of $y$ onto that span is $y$ itself, and the residual is zero.\n- Verdict: **Incorrect**.\n\n**E. If OMP breaks the initial tie by selecting $3$ first, then in the second iteration it must select $2$ before $1$ because $a_2$ remains more correlated with the updated residual than $a_1$.**\n- Let's analyze this hypothetical scenario.\n- **Iteration 1 (hypothetical):** Select $j^{(1)}=3$. The residual is $r'^{(1)} = y - \\text{Proj}_{\\text{span}\\{a_3\\}}(y) = y - \\frac{\\langle a_3, y \\rangle}{\\|a_3\\|^2}a_3 = (a_1+a_3) - \\frac{1}{1}a_3 = a_1$.\n- **Iteration 2 (hypothetical):** We compute correlations with the new residual $r'^{(1)} = a_1$. The remaining atoms of interest are $1$ and $2$.\n  - $|\\langle a_1, r'^{(1)} \\rangle| = |\\langle a_1, a_1 \\rangle| = \\|a_1\\|^2 = 1$.\n  - $|\\langle a_2, r'^{(1)} \\rangle| = |\\langle a_2, a_1 \\rangle| = |0.9|$.\n- Comparing the correlations, $1  0.9$. OMP would select index $1$, not index $2$.\n- The claim that \"$a_2$ remains more correlated with the updated residual than $a_1$\" is false.\n- Verdict: **Incorrect**.\n\nBased on the detailed analysis, only statement A is correct.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Beyond specific examples, it is vital to understand the fundamental conditions under which an algorithm can succeed. This problem  guides you through a theoretical analysis to quantify the \"separation margin\"—the gap between correlations with true support atoms and those with spurious ones. By deriving this margin for an adversarial coherence pattern, you will uncover a classic performance bound that connects the algorithm's success to the sensing matrix's mutual coherence $\\mu$ and the signal's sparsity $k$.",
            "id": "3481124",
            "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ have unit-norm columns $\\{a_{i}\\}_{i=1}^{n}$ and mutual coherence $\\mu(A) = \\max_{i \\neq j} |\\langle a_{i}, a_{j} \\rangle| = \\mu$, with $0  \\mu  1$. Consider a $k$-sparse signal $x^{\\star} \\in \\mathbb{R}^{n}$ with support $S \\subset \\{1,\\dots,n\\}$, $|S| = k$, and entries $(x^{\\star})_{i} = 1$ for $i \\in S$, $(x^{\\star})_{j} = 0$ for $j \\notin S$. The measurement is $y = A x^{\\star}$.\n\nSuppose an adversarial coherence pattern of $A$ is realized as follows:\n- For each $j \\notin S$, the inner products with the support columns satisfy $\\langle a_{j}, a_{s} \\rangle = \\mu$ for all $s \\in S$.\n- For each $i \\in S$, the inner products with the other support columns satisfy $\\langle a_{i}, a_{s} \\rangle = -\\mu$ for all $s \\in S \\setminus \\{i\\}$.\n\nIn the first iteration, Orthogonal Matching Pursuit (OMP) selects the single index maximizing $|\\langle a_{i}, y \\rangle|$, while Stagewise Orthogonal Matching Pursuit (StOMP) selects all indices $i$ such that $|\\langle a_{i}, y \\rangle| \\geq \\tau$, with a naive fixed threshold $\\tau  0$ that does not adapt to $y$ or the iteration.\n\nStarting from the fundamental definitions of mutual coherence and the stated selection rules, derive the expressions for the minimal first-iteration correlation magnitude among indices in $S$ and the maximal first-iteration correlation magnitude among indices in $S^{c}$ under the adversarial pattern. Then, define the separation margin $\\Delta(\\mu, k)$ as the difference between these two quantities. Your final answer must be a single closed-form analytic expression for $\\Delta(\\mu, k)$ in terms of $\\mu$ and $k$. Do not introduce any extraneous parameters. No rounding is required, and no units are involved. Express the final answer as a single closed-form expression.",
            "solution": "The problem will first be validated for scientific soundness, consistency, and completeness before a solution is attempted.\n\n### Step 1: Extract Givens\n- A matrix $A \\in \\mathbb{R}^{m \\times n}$ has columns $\\{a_{i}\\}_{i=1}^{n}$ that are of unit norm, i.e., $\\|a_i\\|_2 = 1$ for all $i$.\n- The mutual coherence of $A$ is $\\mu(A) = \\max_{i \\neq j} |\\langle a_{i}, a_{j} \\rangle| = \\mu$, with $0  \\mu  1$.\n- A signal $x^{\\star} \\in \\mathbb{R}^{n}$ is $k$-sparse.\n- The support of $x^{\\star}$ is $S \\subset \\{1,\\dots,n\\}$ with $|S| = k$.\n- The non-zero entries of $x^\\star$ are $(x^{\\star})_{i} = 1$ for all $i \\in S$.\n- The measurement vector is $y = A x^{\\star}$.\n- An adversarial coherence pattern is specified:\n    1. For each $j \\notin S$, $\\langle a_{j}, a_{s} \\rangle = \\mu$ for all $s \\in S$.\n    2. For each $i \\in S$, $\\langle a_{i}, a_{s} \\rangle = -\\mu$ for all $s \\in S \\setminus \\{i\\}$.\n- The task is to compute the separation margin $\\Delta(\\mu, k)$, defined as the difference between the minimal first-iteration correlation magnitude for indices in $S$ and the maximal first-iteration correlation magnitude for indices in $S^c$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the mathematical framework of compressed sensing and sparse approximation. It uses standard definitions of mutual coherence, sparse signals, and the initial step of matching pursuit algorithms. The language is objective and precise. The problem is self-contained.\n\nA critical point of validation is whether the specified \"adversarial coherence pattern\" is physically possible for a set of vectors in $\\mathbb{R}^{m}$. The existence of a set of vectors $\\{a_s\\}_{s \\in S}$ with the prescribed inner products requires that their Gram matrix, $G$, be positive semi-definite. Let $A_S$ be the submatrix of $A$ containing the columns indexed by $S$. The Gram matrix is $G = A_S^T A_S$. The entries of this $k \\times k$ matrix are $G_{ij} = \\langle a_i, a_j \\rangle$ for $i,j \\in S$.\nBased on the problem statement:\n- The diagonal entries are $G_{ii} = \\langle a_i, a_i \\rangle = \\|a_i\\|_2^2 = 1$.\n- The off-diagonal entries are $G_{ij} = \\langle a_i, a_j \\rangle = -\\mu$ for $i \\neq j$.\n\nThe matrix $G$ can be written as $G = (1+\\mu)I_k - \\mu J_k$, where $I_k$ is the $k \\times k$ identity matrix and $J_k$ is the $k \\times k$ matrix of all ones. For $G$ to be a valid Gram matrix corresponding to vectors in a real space, it must be positive semi-definite, meaning all its eigenvalues must be non-negative.\nThe eigenvalues of $J_k$ are $k$ (with multiplicity $1$) and $0$ (with multiplicity $k-1$).\nThe eigenvalues of $G$ are therefore:\n- $\\lambda_1 = (1+\\mu) - \\mu(k) = 1 + \\mu - k\\mu = 1 - (k-1)\\mu$.\n- $\\lambda_2, \\dots, \\lambda_k = (1+\\mu) - \\mu(0) = 1+\\mu$.\n\nFor these eigenvalues to be non-negative, we require:\n1. $1+\\mu \\ge 0$. This is satisfied since $\\mu  0$.\n2. $1 - (k-1)\\mu \\ge 0$. This implies $\\mu \\le \\frac{1}{k-1}$ for $k  1$.\n\nThis is a necessary condition for the problem's setup to be consistent. It is not an arbitrary assumption but a mathematical consequence of the problem statement. Therefore, the problem is valid and this condition must hold.\n\n### Step 3: Verdict and Action\nThe problem is valid and well-posed. The validation process has revealed a crucial implicit constraint on the parameters. We proceed with the solution.\n\n### Solution Derivation\nThe goal is to find $\\Delta(\\mu, k) = \\min_{i \\in S} |\\langle a_i, y \\rangle| - \\max_{j \\notin S} |\\langle a_j, y \\rangle|$.\n\nFirst, we express the measurement vector $y$. Given that $(x^{\\star})_{i} = 1$ for $i \\in S$ and $(x^{\\star})_{j} = 0$ for $j \\notin S$, we have:\n$$y = A x^{\\star} = \\sum_{i=1}^{n} a_i (x^{\\star})_i = \\sum_{s \\in S} a_s (x^{\\star})_s = \\sum_{s \\in S} a_s$$\nThe measurement vector is the sum of the columns of $A$ corresponding to the support set $S$.\n\nNext, we calculate the correlation $\\langle a_i, y \\rangle$ for an index $i$ belonging to the support set $S$.\n$$ \\langle a_i, y \\rangle = \\left\\langle a_i, \\sum_{s \\in S} a_s \\right\\rangle = \\sum_{s \\in S} \\langle a_i, a_s \\rangle $$\nWe split the sum into the term where $s=i$ and the terms where $s \\neq i$:\n$$ \\langle a_i, y \\rangle = \\langle a_i, a_i \\rangle + \\sum_{s \\in S, s \\neq i} \\langle a_i, a_s \\rangle $$\nUsing the given information:\n- The columns are unit-norm, so $\\langle a_i, a_i \\rangle = \\|a_i\\|_2^2 = 1$.\n- For $s \\in S \\setminus \\{i\\}$, we are given $\\langle a_i, a_s \\rangle = -\\mu$.\n- The number of terms in the summation is $|S| - 1 = k - 1$.\n\nSubstituting these values, we get:\n$$ \\langle a_i, y \\rangle = 1 + \\sum_{s \\in S, s \\neq i} (-\\mu) = 1 + (k-1)(-\\mu) = 1 - (k-1)\\mu $$\nThis value is the same for all indices $i \\in S$. The magnitude is $|\\langle a_i, y \\rangle| = |1 - (k-1)\\mu|$. From our validation step, we established that for the problem to be consistent, we must have $1 - (k-1)\\mu \\ge 0$. Therefore, we can remove the absolute value signs:\n$$ |\\langle a_i, y \\rangle| = 1 - (k-1)\\mu $$\nSince this holds for all $i \\in S$, the minimum correlation magnitude among indices in $S$ is:\n$$ \\min_{i \\in S} |\\langle a_i, y \\rangle| = 1 - (k-1)\\mu $$\n\nNow, we calculate the correlation $\\langle a_j, y \\rangle$ for an index $j$ not in the support set, i.e., $j \\in S^c = \\{1,\\dots,n\\} \\setminus S$.\n$$ \\langle a_j, y \\rangle = \\left\\langle a_j, \\sum_{s \\in S} a_s \\right\\rangle = \\sum_{s \\in S} \\langle a_j, a_s \\rangle $$\nFrom the problem statement, for any $j \\notin S$, we have $\\langle a_j, a_s \\rangle = \\mu$ for all $s \\in S$. The sum runs over all $k$ elements of $S$:\n$$ \\langle a_j, y \\rangle = \\sum_{s \\in S} \\mu = k\\mu $$\nThis value is the same for all indices $j \\notin S$. The magnitude is $|\\langle a_j, y \\rangle| = |k\\mu|$. Since $k  0$ and $\\mu  0$, this is simply $k\\mu$.\nThus, the maximum correlation magnitude among indices not in $S$ is:\n$$ \\max_{j \\notin S} |\\langle a_j, y \\rangle| = k\\mu $$\n\nFinally, we compute the separation margin $\\Delta(\\mu, k)$ by taking the difference between the minimal on-support magnitude and the maximal off-support magnitude:\n$$ \\Delta(\\mu, k) = \\min_{i \\in S} |\\langle a_i, y \\rangle| - \\max_{j \\notin S} |\\langle a_j, y \\rangle| $$\n$$ \\Delta(\\mu, k) = (1 - (k-1)\\mu) - k\\mu $$\nExpanding and simplifying the expression:\n$$ \\Delta(\\mu, k) = 1 - k\\mu + \\mu - k\\mu $$\n$$ \\Delta(\\mu, k) = 1 - (2k-1)\\mu $$\nThis is the final closed-form expression for the separation margin under the specified adversarial conditions.",
            "answer": "$$\\boxed{1 - (2k-1)\\mu}$$"
        }
    ]
}