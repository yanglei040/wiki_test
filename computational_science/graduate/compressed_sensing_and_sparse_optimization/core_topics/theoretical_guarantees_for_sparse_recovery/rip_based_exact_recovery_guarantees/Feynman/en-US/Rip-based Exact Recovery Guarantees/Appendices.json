{
    "hands_on_practices": [
        {
            "introduction": "Understanding a formal definition like the Restricted Isometry Property (RIP) begins with direct application. This foundational exercise guides you through a manual, step-by-step calculation of the Restricted Isometry Constants (RICs) $\\delta_1$ and $\\delta_2$ for a small, concrete matrix . By explicitly enumerating all relevant submatrices and computing the eigenvalues of their Gram matrices, you will solidify your grasp of the connection between the abstract RIP inequality and the spectral properties of the sensing matrix.",
            "id": "3474584",
            "problem": "Let $A \\in \\mathbb{R}^{2 \\times 3}$ be a sensing matrix with unit-norm columns, given explicitly by\n$$\nA \\;=\\; \\begin{bmatrix}\n1  \\tfrac{1}{2}  \\tfrac{1}{2} \\\\\n0  \\tfrac{\\sqrt{3}}{2}  -\\tfrac{\\sqrt{3}}{2}\n\\end{bmatrix}.\n$$\nConsider the Restricted Isometry Property (RIP) in compressed sensing, defined via the Restricted Isometry Constant (RIC) $\\delta_k$ as the smallest $\\delta \\geq 0$ such that for all supports $S \\subset \\{1,2,3\\}$ with $|S| \\leq k$ and all vectors $x \\in \\mathbb{R}^{|S|}$,\n$$\n(1 - \\delta)\\,\\|x\\|_{2}^{2} \\;\\leq\\; \\|A_{S} x\\|_{2}^{2} \\;\\leq\\; (1 + \\delta)\\,\\|x\\|_{2}^{2}.\n$$\nHere $A_{S}$ denotes the submatrix of $A$ formed by the columns of $A$ indexed by $S$. Starting only from this definition and fundamental facts about singular values and eigenvalues of symmetric matrices, compute the exact Restricted Isometry Constants $\\delta_{1}$ and $\\delta_{2}$ for the given matrix by explicitly enumerating all supports $S$ with $|S| \\leq 2$ and the extremal singular values of $A_{S}$. Provide your final answer as the ordered pair $(\\delta_{1}, \\delta_{2})$. Give exact values; no rounding is required.",
            "solution": "The user wants to compute the Restricted Isometry Constants (RICs) $\\delta_{1}$ and $\\delta_{2}$ for the given matrix $A \\in \\mathbb{R}^{2 \\times 3}$.\n\nThe problem is valid. It is scientifically sound, well-posed, and objective. It provides a clear definition and all necessary data to compute a unique, verifiable answer. The matrix is given, its properties are correctly stated (unit-norm columns), and the task is a direct application of the definition of the Restricted Isometry Property (RIP).\n\nThe Restricted Isometry Property is defined by the inequality\n$$ (1 - \\delta_k) \\|x\\|_2^2 \\leq \\|A_S x\\|_2^2 \\leq (1 + \\delta_k) \\|x\\|_2^2 $$\nwhich must hold for all supports $S \\subset \\{1, 2, 3\\}$ with cardinality $|S| \\leq k$ and all vectors $x \\in \\mathbb{R}^{|S|}$. The constant $\\delta_k$ is the smallest non-negative number for which this holds.\n\nWe can rewrite the expression $\\|A_S x\\|_2^2$ as $x^T A_S^T A_S x$. The matrix $A_S^T A_S$ is a symmetric Gram matrix. The inequality can be framed using the Rayleigh quotient:\n$$ 1 - \\delta_k \\leq \\frac{x^T A_S^T A_S x}{x^T x} \\leq 1 + \\delta_k $$\nBy the Rayleigh-Ritz theorem, the minimum and maximum values of this quotient for a given $S$ are the minimum and maximum eigenvalues of the matrix $A_S^T A_S$, denoted $\\lambda_{\\min}(A_S^T A_S)$ and $\\lambda_{\\max}(A_S^T A_S)$, respectively. These eigenvalues are the squared singular values of $A_S$.\n\nFor the RIP inequality to hold for all supports $S$ with $|S| \\leq k$, the bounds must accommodate the most extreme eigenvalues over all such supports. Let\n$$ \\lambda_{\\min}^{(k)} = \\min_{S: |S| \\leq k} \\lambda_{\\min}(A_S^T A_S) $$\n$$ \\lambda_{\\max}^{(k)} = \\max_{S: |S| \\leq k} \\lambda_{\\max}(A_S^T A_S) $$\nThe RIP constant $\\delta_k$ is then determined by the larger of the two deviations from $1$:\n$$ \\delta_k = \\max\\left(1 - \\lambda_{\\min}^{(k)}, \\lambda_{\\max}^{(k)} - 1\\right) $$\n\nLet the columns of $A$ be $a_1, a_2, a_3$:\n$$ a_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad a_2 = \\begin{bmatrix} \\tfrac{1}{2} \\\\ \\tfrac{\\sqrt{3}}{2} \\end{bmatrix}, \\quad a_3 = \\begin{bmatrix} \\tfrac{1}{2} \\\\ -\\tfrac{\\sqrt{3}}{2} \\end{bmatrix} $$\nThe Gram matrix for any submatrix $A_S$ will have entries $(A_S^T A_S)_{ij} = a_{s_i}^T a_{s_j}$. Since all columns are unit-norm, the diagonal entries of all such Gram matrices will be $1$.\n\n**Computation of $\\delta_1$**\n\nFor $k=1$, we consider all supports $S$ with $|S| \\leq 1$. We only need to consider non-empty supports, so we analyze the case $|S|=1$. The possible supports are $S_1 = \\{1\\}$, $S_2 = \\{2\\}$, and $S_3 = \\{3\\}$.\n\nFor any support $S_i = \\{i\\}$, the submatrix $A_{S_i}$ is the column $a_i$. The corresponding Gram matrix is a $1 \\times 1$ matrix:\n$$ A_{S_i}^T A_{S_i} = [a_i^T a_i] = [\\|a_i\\|_2^2] $$\nAs stated in the problem (and verified), the columns of $A$ have unit norm, so $\\|a_i\\|_2^2 = 1$ for $i=1,2,3$.\nThus, for all three supports of size $1$, the Gram matrix is $[1]$, which has a single eigenvalue of $\\lambda=1$.\n\nTherefore, for $k=1$:\n$$ \\lambda_{\\min}^{(1)} = \\min(1, 1, 1) = 1 $$\n$$ \\lambda_{\\max}^{(1)} = \\max(1, 1, 1) = 1 $$\nThe RIC $\\delta_1$ is then:\n$$ \\delta_1 = \\max(1 - 1, 1 - 1) = 0 $$\n\n**Computation of $\\delta_2$**\n\nFor $k=2$, we must consider all supports $S$ with $|S| \\leq 2$. This includes the supports of size $1$ (for which we already know the eigenvalues are all $1$) and the supports of size $2$. The supports of size $2$ are $S_{12} = \\{1,2\\}$, $S_{13} = \\{1,3\\}$, and $S_{23} = \\{2,3\\}$. We must find the eigenvalues of the Gram matrices for these supports.\n\nFirst, we calculate the required off-diagonal entries (inner products):\n$$ a_1^T a_2 = \\begin{bmatrix} 1  0 \\end{bmatrix} \\begin{bmatrix} \\tfrac{1}{2} \\\\ \\tfrac{\\sqrt{3}}{2} \\end{bmatrix} = \\frac{1}{2} $$\n$$ a_1^T a_3 = \\begin{bmatrix} 1  0 \\end{bmatrix} \\begin{bmatrix} \\tfrac{1}{2} \\\\ -\\tfrac{\\sqrt{3}}{2} \\end{bmatrix} = \\frac{1}{2} $$\n$$ a_2^T a_3 = \\begin{bmatrix} \\tfrac{1}{2}  \\tfrac{\\sqrt{3}}{2} \\end{bmatrix} \\begin{bmatrix} \\tfrac{1}{2} \\\\ -\\tfrac{\\sqrt{3}}{2} \\end{bmatrix} = \\frac{1}{4} - \\frac{3}{4} = -\\frac{1}{2} $$\n\nCase 1: $S = S_{12} = \\{1,2\\}$\nThe Gram matrix is $G_{12} = A_{S_{12}}^T A_{S_{12}} = \\begin{bmatrix} a_1^T a_1  a_1^T a_2 \\\\ a_2^T a_1  a_2^T a_2 \\end{bmatrix} = \\begin{bmatrix} 1  \\tfrac{1}{2} \\\\ \\tfrac{1}{2}  1 \\end{bmatrix}$.\nThe characteristic equation is $\\det(G_{12} - \\lambda I) = 0$:\n$$ \\det\\begin{pmatrix} 1-\\lambda  \\tfrac{1}{2} \\\\ \\tfrac{1}{2}  1-\\lambda \\end{pmatrix} = (1-\\lambda)^2 - \\left(\\frac{1}{2}\\right)^2 = 0 $$\n$$ (1-\\lambda)^2 = \\frac{1}{4} \\implies 1-\\lambda = \\pm \\frac{1}{2} \\implies \\lambda = 1 \\mp \\frac{1}{2} $$\nThe eigenvalues are $\\lambda_{min} = \\frac{1}{2}$ and $\\lambda_{max} = \\frac{3}{2}$.\n\nCase 2: $S = S_{13} = \\{1,3\\}$\nThe Gram matrix is $G_{13} = A_{S_{13}}^T A_{S_{13}} = \\begin{bmatrix} a_1^T a_1  a_1^T a_3 \\\\ a_3^T a_1  a_3^T a_3 \\end{bmatrix} = \\begin{bmatrix} 1  \\tfrac{1}{2} \\\\ \\tfrac{1}{2}  1 \\end{bmatrix}$.\nThis is identical to $G_{12}$, so the eigenvalues are also $\\frac{1}{2}$ and $\\frac{3}{2}$.\n\nCase 3: $S = S_{23} = \\{2,3\\}$\nThe Gram matrix is $G_{23} = A_{S_{23}}^T A_{S_{23}} = \\begin{bmatrix} a_2^T a_2  a_2^T a_3 \\\\ a_3^T a_2  a_3^T a_3 \\end{bmatrix} = \\begin{bmatrix} 1  -\\tfrac{1}{2} \\\\ -\\tfrac{1}{2}  1 \\end{bmatrix}$.\nThe characteristic equation is $\\det(G_{23} - \\lambda I) = 0$:\n$$ \\det\\begin{pmatrix} 1-\\lambda  -\\tfrac{1}{2} \\\\ -\\tfrac{1}{2}  1-\\lambda \\end{pmatrix} = (1-\\lambda)^2 - \\left(-\\frac{1}{2}\\right)^2 = 0 $$\n$$ (1-\\lambda)^2 = \\frac{1}{4} \\implies 1-\\lambda = \\pm \\frac{1}{2} \\implies \\lambda = 1 \\mp \\frac{1}{2} $$\nThe eigenvalues are again $\\lambda_{min} = \\frac{1}{2}$ and $\\lambda_{max} = \\frac{3}{2}$.\n\nNow, we must find the overall minimum and maximum eigenvalues over all supports $S$ with $|S| \\leq 2$.\nThe set of eigenvalues for $|S|=1$ is $\\{1\\}$.\nThe set of eigenvalues for $|S|=2$ is $\\{\\frac{1}{2}, \\frac{3}{2}\\}$.\nCombining these, the full set of eigenvalues to consider for $k=2$ is $\\{ \\frac{1}{2}, 1, \\frac{3}{2} \\}$.\nThe global minimum and maximum are:\n$$ \\lambda_{\\min}^{(2)} = \\min \\left\\{ \\frac{1}{2}, 1, \\frac{3}{2} \\right\\} = \\frac{1}{2} $$\n$$ \\lambda_{\\max}^{(2)} = \\max \\left\\{ \\frac{1}{2}, 1, \\frac{3}{2} \\right\\} = \\frac{3}{2} $$\nThe RIC $\\delta_2$ is then:\n$$ \\delta_2 = \\max\\left(1 - \\lambda_{\\min}^{(2)}, \\lambda_{\\max}^{(2)} - 1\\right) = \\max\\left(1 - \\frac{1}{2}, \\frac{3}{2} - 1\\right) = \\max\\left(\\frac{1}{2}, \\frac{1}{2}\\right) = \\frac{1}{2} $$\n\nThe computed Restricted Isometry Constants are $\\delta_1 = 0$ and $\\delta_2 = \\frac{1}{2}$. The problem asks for the ordered pair $(\\delta_1, \\delta_2)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  \\frac{1}{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While manual calculations are invaluable for building intuition, they are impractical for all but the smallest matrices. This practice bridges the gap between theoretical understanding and computational skill by asking you to implement an algorithm that computes the RIP constant $\\delta_2$ exactly . Automating the process of enumerating supports and finding extremal eigenvalues will not only reinforce the definition of RIP but also equip you with a practical tool to analyze and verify the properties of custom sensing matrices.",
            "id": "3474609",
            "problem": "You are tasked with deriving and implementing an exact computation of the order-$2$ Restricted Isometry Property (RIP) constant for a small matrix with $n=6$ columns. The Restricted Isometry Property (RIP) constant of order $k$, denoted $\\delta_k$, is defined as the smallest nonnegative real number $\\delta$ such that for every $k$-sparse vector $x \\in \\mathbb{R}^n$,\n$$(1 - \\delta)\\,\\|x\\|_2^2 \\le \\|A x\\|_2^2 \\le (1 + \\delta)\\,\\|x\\|_2^2,$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is a real matrix, $\\|\\cdot\\|_2$ denotes the Euclidean norm, and a $k$-sparse vector has at most $k$ nonzero entries. The Restricted Isometry Property (RIP) is a central concept in compressed sensing and sparse optimization because it quantifies how close the linear map $A$ is to an isometry when restricted to sparse vectors. In particular, $\\delta_k$ controls stability and uniqueness in recovery, and, through eigenvalue bounds on submatrix Gram matrices, ties directly to exact recovery guarantees.\n\nStarting from the above definition and fundamental linear algebra facts about Gram matrices, singular values, and eigenvalues, derive a procedure that computes $\\delta_2$ exactly by checking all supports of size $2$ and the extremal eigenvalues of the corresponding Gram matrices. Specifically, for each subset $S \\subset \\{1,2,\\dots,6\\}$ with $|S|=2$, let $A_S$ denote the submatrix of $A$ formed by the columns indexed by $S$. Use the spectral properties of the symmetric positive semidefinite Gram matrix $G_S = A_S^\\top A_S$ to determine, for each $S$, the smallest $\\delta$ such that\n$$(1 - \\delta)\\,\\|x\\|_2^2 \\le \\|A_S x\\|_2^2 \\le (1 + \\delta)\\,\\|x\\|_2^2 \\quad \\text{for all } x \\in \\mathbb{R}^2.$$\nAggregate this over all $\\binom{6}{2}$ supports to obtain the exact $\\delta_2$ for the given $A$.\n\nYour program must:\n- Enumerate all $2$-element supports $S$ from $\\{1,2,\\dots,6\\}$.\n- For each support $S$, construct $A_S$, compute $G_S = A_S^\\top A_S$, and find the extremal eigenvalues of $G_S$.\n- For each $S$, compute the deviation from isometry as the maximum absolute difference between the eigenvalues of $G_S$ and $1$.\n- Return $\\delta_2$ as the maximum of these deviations over all supports $S$ of size $2$.\n\nTest Suite:\nCompute $\\delta_2$ for each of the following matrices, all with $n=6$ columns:\n\n- Case $1$ (happy path, exact isometry): $A_1 \\in \\mathbb{R}^{6 \\times 6}$ defined by $A_1 = I_6$, the $6 \\times 6$ identity matrix.\n\n- Case $2$ (boundary condition, duplicate columns yielding the borderline $\\delta_2 = 1$): $A_2 \\in \\mathbb{R}^{6 \\times 6}$ whose columns are $[e_1, e_1, e_2, e_3, e_4, e_5]$, where $e_i \\in \\mathbb{R}^6$ is the $i$-th standard basis vector.\n\n- Case $3$ (unit-norm columns with one controlled correlation): $A_3 \\in \\mathbb{R}^{6 \\times 6}$ with columns $[e_1, c_2, e_3, e_4, e_5, e_6]$ where $c_2 = 0.4\\,e_1 + \\sqrt{1 - 0.16}\\,e_2$, and $e_i \\in \\mathbb{R}^6$ are standard basis vectors. Note that $c_2$ is unit norm and has inner product $0.4$ with $e_1$.\n\n- Case $4$ (non-normalized columns, diagonal scaling): $A_4 \\in \\mathbb{R}^{6 \\times 6}$ defined by $A_4 = \\operatorname{diag}(1.2, 0.9, 1.0, 1.0, 1.0, 1.0)$.\n\n- Case $5$ (general tall matrix with multiple correlations): $A_5 \\in \\mathbb{R}^{4 \\times 6}$ with columns\n$$\nc_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\nc_2 = \\begin{bmatrix} 0.6 \\\\ 0.8 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\nc_3 = \\begin{bmatrix} 0 \\\\ 0.3 \\\\ \\sqrt{1 - 0.09} \\\\ 0 \\end{bmatrix},\\quad\nc_4 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0.4 \\\\ \\sqrt{1 - 0.16} \\end{bmatrix},\\quad\nc_5 = \\begin{bmatrix} 0.2 \\\\ 0.1 \\\\ 0.2 \\\\ \\sqrt{1 - 0.04 - 0.01 - 0.04} \\end{bmatrix},\\quad\nc_6 = \\begin{bmatrix} 0.1 \\\\ 0.2 \\\\ 0.1 \\\\ -\\sqrt{1 - 0.01 - 0.04 - 0.01} \\end{bmatrix}.\n$$\nAll columns of $A_5$ are unit norm by construction, and $A_5 = [c_1\\, c_2\\, c_3\\, c_4\\, c_5\\, c_6]$.\n\nAnswer specification:\n- For each matrix $A_i$, the computed $\\delta_2$ must be returned as a real number (float).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4,r_5]$, where $r_i$ is the $\\delta_2$ for $A_i$.\n\nNo physical units or angle units are involved in this problem. Ensure numerical computations are stable and use exact enumeration over all supports of size $2$ to guarantee correctness for $n=6$.",
            "solution": "The problem is well-posed, scientifically grounded, and contains sufficient information for a unique solution. It is a direct application of the definition of the Restricted Isometry Property (RIP) constant, a fundamental concept in compressed sensing.\n\nThe Restricted Isometry Property (RIP) constant of order $k$, denoted $\\delta_k$, for a matrix $A \\in \\mathbb{R}^{m \\times n}$ is the smallest non-negative number $\\delta_k$ such that the inequality\n$$ (1 - \\delta_k) \\|x\\|_2^2 \\le \\|Ax\\|_2^2 \\le (1 + \\delta_k) \\|x\\|_2^2 $$\nholds for all $k$-sparse vectors $x \\in \\mathbb{R}^n$. A vector $x$ is $k$-sparse if it has at most $k$ non-zero entries.\n\nTo compute the order-$2$ RIP constant, $\\delta_2$, we must consider all vectors $x$ that are $2$-sparse. A $2$-sparse vector $x$ has its non-zero entries located at indices corresponding to a support set $S \\subset \\{1, 2, \\dots, n\\}$ with $|S|=2$. Let $S = \\{i, j\\}$ with $i \\neq j$. Let $x_S \\in \\mathbb{R}^2$ be the vector containing the two non-zero entries of $x$, and let $A_S \\in \\mathbb{R}^{m \\times 2}$ be the submatrix of $A$ formed by columns $i$ and $j$. The product $Ax$ can be written as $A_S x_S$, and since the other entries of $x$ are zero, $\\|x\\|_2^2 = \\|x_S\\|_2^2$.\n\nThe RIP inequality for this specific support $S$ is\n$$ (1 - \\delta) \\|x_S\\|_2^2 \\le \\|A_S x_S\\|_2^2 \\le (1 + \\delta) \\|x_S\\|_2^2 $$\nfor all $x_S \\in \\mathbb{R}^2$. Dividing by $\\|x_S\\|_2^2$ (for $x_S \\neq 0$), we get\n$$ 1 - \\delta \\le \\frac{\\|A_S x_S\\|_2^2}{\\|x_S\\|_2^2} \\le 1 + \\delta $$\nThe expression in the middle is the Rayleigh quotient of the Gram matrix $G_S = A_S^\\top A_S$ associated with the vector $x_S$. The matrix $G_S$ is a $2 \\times 2$ symmetric positive semidefinite matrix. The Rayleigh-Ritz theorem states that the minimum and maximum values of this quotient are the minimum and maximum eigenvalues of $G_S$, which we denote by $\\lambda_{\\min}(G_S)$ and $\\lambda_{\\max}(G_S)$, respectively.\n\nThus, for the inequality to hold for all non-zero $x_S \\in \\mathbb{R}^2$, we must satisfy\n$$ 1 - \\delta \\le \\lambda_{\\min}(G_S) \\quad \\text{and} \\quad \\lambda_{\\max}(G_S) \\le 1 + \\delta $$\nThese conditions can be rearranged to find the smallest non-negative $\\delta$ that works for the support $S$, which we denote $\\delta_S$:\n$$ \\delta \\ge 1 - \\lambda_{\\min}(G_S) \\quad \\text{and} \\quad \\delta \\ge \\lambda_{\\max}(G_S) - 1 $$\nTherefore, for a given support $S$, the smallest valid $\\delta_S$ is\n$$ \\delta_S = \\max(1 - \\lambda_{\\min}(G_S), \\lambda_{\\max}(G_S) - 1) $$\nThis value represents the maximum deviation of the eigenvalues of $G_S$ from $1$.\n\nThe overall RIP constant $\\delta_2$ must satisfy the condition for all possible $2$-sparse vectors, which means it must hold for all possible supports $S$ of size $2$. Consequently, $\\delta_2$ is the maximum of all such $\\delta_S$:\n$$ \\delta_2 = \\max_{S \\subset \\{1, \\dots, n\\}, |S|=2} \\delta_S $$\n\nFor the given problem with $n=6$, the procedure is as follows:\n1.  Enumerate all $\\binom{6}{2} = 15$ unique pairs of column indices $\\{i, j\\}$ from $\\{1, 2, 3, 4, 5, 6\\}$. Each pair forms a support $S$.\n2.  For each support $S=\\{i, j\\}$, construct the submatrix $A_S$ using columns $i$ and $j$ of $A$.\n3.  Compute the $2 \\times 2$ Gram matrix $G_S = A_S^\\top A_S$.\n4.  Calculate the two eigenvalues of $G_S$, $\\lambda_{\\min}(G_S)$ and $\\lambda_{\\max}(G_S)$.\n5.  Compute the deviation for this support: $\\delta_S = \\max( \\lambda_{\\max}(G_S) - 1, 1 - \\lambda_{\\min}(G_S) )$.\n6.  The RIP constant $\\delta_2$ is the maximum value of $\\delta_S$ found across all $15$ supports.\n\nThis exact enumeration guarantees the correct computation of $\\delta_2$ for the given matrices. The implementation will follow this procedure for each test case provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef compute_delta2(A):\n    \"\"\"\n    Computes the order-2 Restricted Isometry Property (RIP) constant delta_2 for a given matrix A.\n\n    Args:\n        A (np.ndarray): The matrix A of size m x n.\n\n    Returns:\n        float: The RIP constant delta_2.\n    \"\"\"\n    m, n = A.shape\n    if n  2:\n        return 0.0\n\n    max_deviation = 0.0\n\n    # Enumerate all 2-element supports S from {0, 1, ..., n-1}\n    # Note: indices are 0-based\n    for s_indices in itertools.combinations(range(n), 2):\n        # Construct the submatrix A_S\n        A_S = A[:, s_indices]\n\n        # Compute the Gram matrix G_S = A_S^T * A_S\n        G_S = A_S.T @ A_S\n\n        # Find the eigenvalues of the 2x2 symmetric matrix G_S.\n        # np.linalg.eigvalsh is efficient and numerically stable for Hermitian (or real symmetric) matrices.\n        eigenvalues = np.linalg.eigvalsh(G_S)\n        lambda_min = eigenvalues[0]\n        lambda_max = eigenvalues[1]\n\n        # Compute the deviation from isometry for this support\n        deviation_S = max(lambda_max - 1, 1 - lambda_min)\n\n        # Update the overall maximum deviation\n        if deviation_S  max_deviation:\n            max_deviation = deviation_S\n            \n    return max_deviation\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing delta_2 for each test case matrix.\n    \"\"\"\n    # Case 1: A1 = I_6, the 6x6 identity matrix.\n    A1 = np.identity(6)\n\n    # Case 2: A2 with columns [e1, e1, e2, e3, e4, e5].\n    A2 = np.zeros((6, 6))\n    A2[:, 0] = np.array([1, 0, 0, 0, 0, 0])\n    A2[:, 1] = np.array([1, 0, 0, 0, 0, 0])\n    A2[:, 2] = np.array([0, 1, 0, 0, 0, 0])\n    A2[:, 3] = np.array([0, 0, 1, 0, 0, 0])\n    A2[:, 4] = np.array([0, 0, 0, 1, 0, 0])\n    A2[:, 5] = np.array([0, 0, 0, 0, 1, 0])\n    \n    # Case 3: A3 with columns [e1, c2, e3, e4, e5, e6]\n    A3 = np.zeros((6, 6))\n    e1, e2, e3, e4, e5, e6 = np.identity(6)\n    c2 = 0.4 * e1 + np.sqrt(1 - 0.16) * e2\n    A3[:, 0] = e1\n    A3[:, 1] = c2\n    A3[:, 2] = e3\n    A3[:, 3] = e4\n    A3[:, 4] = e5\n    A3[:, 5] = e6\n\n    # Case 4: A4 = diag(1.2, 0.9, 1.0, 1.0, 1.0, 1.0)\n    A4 = np.diag([1.2, 0.9, 1.0, 1.0, 1.0, 1.0])\n\n    # Case 5: A5, a general 4x6 matrix\n    A5 = np.zeros((4, 6))\n    c1 = np.array([1, 0, 0, 0])\n    c2 = np.array([0.6, 0.8, 0, 0])\n    c3 = np.array([0, 0.3, np.sqrt(1 - 0.09), 0])\n    c4 = np.array([0, 0, 0.4, np.sqrt(1 - 0.16)])\n    c5 = np.array([0.2, 0.1, 0.2, np.sqrt(1 - 0.04 - 0.01 - 0.04)])\n    c6 = np.array([0.1, 0.2, 0.1, -np.sqrt(1 - 0.01 - 0.04 - 0.01)])\n    A5 = np.column_stack([c1, c2, c3, c4, c5, c6])\n\n    test_cases = [A1, A2, A3, A4, A5]\n    results = []\n    \n    for A in test_cases:\n        delta_2 = compute_delta2(A)\n        results.append(delta_2)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate goal of computing RIP constants is to assess the quality of sensing matrices used in real applications. This exercise advances your skills by focusing on partial Fourier matrices, a class of structured matrices central to compressed sensing in signal processing and imaging . By implementing a procedure to calculate $\\delta_k$ for these matrices and observing how different frequency selection strategies affect the result, you will gain insight into the practical aspects of matrix design and its direct impact on recovery guarantees.",
            "id": "3474598",
            "problem": "You are given a family of structured sensing matrices built from partial rows of the Discrete Fourier Transform (DFT). For a positive integer $n$, let the $n \\times n$ DFT matrix $F$ have entries $F_{f,c} = \\exp(-2\\pi i f c / n)$, with $f \\in \\{0,1,\\dots,n-1\\}$ indexing frequency and $c \\in \\{0,1,\\dots,n-1\\}$ indexing the canonical coordinate basis. For a subset of frequency indices $\\Omega \\subset \\{0,1,\\dots,n-1\\}$ with cardinality $m = |\\Omega|$, define the $m \\times n$ partial Fourier matrix $A$ by selecting the rows of $F$ indexed by $\\Omega$ and normalizing each column to unit Euclidean norm. Concretely, the entries of $A$ are $A_{r,c} = \\exp(-2\\pi i f_r c / n) / \\sqrt{m}$ where $(f_r)_{r=0}^{m-1}$ enumerates the elements of $\\Omega$.\n\nA vector $x \\in \\mathbb{C}^n$ is called $k$-sparse if it has at most $k$ nonzero entries. The Restricted Isometry Property (RIP) requires that a sensing matrix acts approximately as an isometry on all $k$-sparse vectors. For a given $k$, the restricted isometry constant $\\delta_k$ is defined to be the smallest nonnegative real number such that for all $k$-sparse $x \\in \\mathbb{C}^n$,\n$$(1 - \\delta_k)\\,\\|x\\|_2^2 \\le \\|A x\\|_2^2 \\le (1 + \\delta_k)\\,\\|x\\|_2^2.$$\n\nYour task is to implement a program that, for each specified test case $(n,\\Omega,k)$, numerically estimates $\\delta_k$ exactly by exhaustive search over all supports of size $k$ and extremal distortion on those supports. You must:\n- Construct $A$ from $(n,\\Omega)$ as above, ensuring unit-norm columns.\n- For each support set $S \\subset \\{0,\\dots,n-1\\}$ with $|S| = k$, consider the $k$-dimensional subspace of vectors supported on $S$ and determine the most contracted and the most expanded unit vector in that subspace under the linear map $A$. Use these extremes to compute the distortion factor for $S$ and take the worst case over all such $S$ to produce $\\delta_k$.\n\nIn addition to computing $\\delta_k$, this numerical study should be designed to reveal how the frequency selection pattern $\\Omega$ and the resulting correlations between columns affect the isometry constants. Use the following test suite of moderate sizes to ensure tractability and to probe different structural regimes:\n- Test case $\\mathbf{1}$ (spread frequencies on a prime length): $n = 17$, $\\Omega = \\{0,2,4,6,8,10,12,14\\}$, $k = 2$.\n- Test case $\\mathbf{2}$ (consecutive low frequencies on an even length): $n = 16$, $\\Omega = \\{0,1,2,3,4,5,6,7\\}$, $k = 2$.\n- Test case $\\mathbf{3}$ (boundary case): $n = 16$, $\\Omega = \\{0,1,2,3,4,5,6,7\\}$, $k = 1$.\n- Test case $\\mathbf{4}$ (aliased selection inducing identical columns modulo periodicity): $n = 21$, $\\Omega = \\{0,3,6,9,12,15,18\\}$, $k = 3$.\n- Test case $\\mathbf{5}$ (large $k$ relative to $m$): $n = 12$, $\\Omega = \\{0,1,2,3,4,5\\}$, $k = 5$.\n\nAll angles are dimensionless phases measured in radians via the complex exponential; there are no physical units in this problem.\n\nYour program must:\n- Compute $\\delta_k$ exactly for each test case by exhaustive evaluation as described, using numerically stable linear algebra for the extremal distortions on each support.\n- Produce a single line of output containing the results for the test cases in the order $\\mathbf{1}$ through $\\mathbf{5}$ as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal places (for example, $[0.123456,0.000000,0.314159,0.500000,0.271828]$). The answer for each test case must be a single floating-point number.\n\nNotes:\n- Design your implementation starting from the formal definition of the Restricted Isometry Property and the interpretation of extremal distortion on a fixed support as an extremal quadratic form induced by the sensing matrix restricted to that support.\n- Ensure scientific realism by strictly following the construction of $A$ and by using exact exhaustive search over supports for the given moderate parameters.",
            "solution": "We begin from the definition of the Restricted Isometry Property (RIP). For a matrix $A \\in \\mathbb{C}^{m \\times n}$ and a sparsity level $k$, the $k$-restricted isometry constant $\\delta_k$ is the smallest $\\delta \\ge 0$ such that for all $k$-sparse vectors $x \\in \\mathbb{C}^n$,\n$$(1 - \\delta)\\,\\|x\\|_2^2 \\le \\|A x\\|_2^2 \\le (1 + \\delta)\\,\\|x\\|_2^2.$$\nTo compute $\\delta_k$ exactly for a fixed finite matrix $A$, we reduce the problem to examining $k$-dimensional coordinate subspaces. Let $S \\subset \\{0,1,\\dots,n-1\\}$ be an index set with $|S| = k$, and let $A_S \\in \\mathbb{C}^{m \\times k}$ denote the submatrix of $A$ formed by taking the columns indexed by $S$. Any $k$-sparse vector supported on $S$ can be written as $x = z$ embedded in the coordinates $S$, with $z \\in \\mathbb{C}^k$. Then\n$$\\|A x\\|_2^2 = \\|A_S z\\|_2^2 = z^* (A_S^* A_S)\\, z.$$\nThe quantity $A_S^* A_S$ is the $k \\times k$ Gram matrix of the selected columns and is Hermitian positive semidefinite. Over unit vectors $z$ with $\\|z\\|_2 = 1$, the minimum and maximum values of the quadratic form $z^* (A_S^* A_S) z$ are the smallest and largest eigenvalues of $A_S^* A_S$, respectively. Therefore, for a fixed $S$, the tightest two-sided inequality constants are determined by the extremal eigenvalues $\\lambda_{\\min}(A_S^* A_S)$ and $\\lambda_{\\max}(A_S^* A_S)$, and the distortion relative to an exact isometry is captured by\n$$\\max\\big(1 - \\lambda_{\\min}(A_S^* A_S),\\ \\lambda_{\\max}(A_S^* A_S) - 1\\big).$$\nTaking the worst case over all supports $S$ with $|S| = k$ yields\n$$\\delta_k = \\max_{S:\\ |S|=k} \\max\\big(1 - \\lambda_{\\min}(A_S^* A_S),\\ \\lambda_{\\max}(A_S^* A_S) - 1\\big).$$\nThis follows directly from the definition of RIP and the spectral characterization of extremal quadratic forms on unit spheres in finite-dimensional inner product spaces.\n\nFor the matrices in this problem, $A$ is a column-normalized partial Fourier matrix. Specifically, for a given $n$ and a set of frequency indices $\\Omega = \\{f_0,\\dots,f_{m-1}\\}$, we define $A \\in \\mathbb{C}^{m \\times n}$ by\n$$A_{r,c} = \\frac{1}{\\sqrt{m}} \\exp\\!\\left(-\\frac{2\\pi i}{n}\\, f_r\\, c\\right),\\quad r \\in \\{0,\\dots,m-1\\},\\ c \\in \\{0,\\dots,n-1\\}.$$\nEach column has $m$ entries of unit modulus and is scaled by $1/\\sqrt{m}$, hence has unit Euclidean norm, implying $\\delta_1 = 0$ because any single column is mapped with exact norm preservation. The off-diagonal entries of $A^* A$ are normalized sums of complex exponentials that encode correlations between columns. The selection pattern $\\Omega$ determines these correlations via discrete sums resembling Dirichlet kernels; clustered low frequencies produce slowly decaying correlation with respect to column index differences, while certain arithmetic progressions can induce exact aliasing (identical columns) when the step size and $n$ share nontrivial modular structure.\n\nAlgorithmic design:\n- For each test case $(n,\\Omega,k)$, construct $A$ using the formula above.\n- Enumerate all $\\binom{n}{k}$ index sets $S$ of size $k$ (feasible for the provided small $n$ and $k$).\n- For each $S$, form $G_S = A_S^* A_S$ and compute its eigenvalues using a Hermitian solver.\n- Compute the support distortion $\\max(1 - \\lambda_{\\min}(G_S),\\ \\lambda_{\\max}(G_S) - 1)$.\n- Take the maximum over all $S$ to obtain $\\delta_k$.\n- Repeat for all test cases and output the values rounded to $6$ decimals.\n\nInterpretation relative to frequency selection and column correlations:\n- For $k = 1$, column normalization enforces $\\delta_1 = 0$ since $G_S = [1]$ and both extremal eigenvalues equal $1$.\n- When $\\Omega$ consists of consecutive low frequencies on an even length $n$, the correlation between different columns can be substantial due to the shape of the discrete low-pass kernel, increasing $\\delta_2$.\n- When $\\Omega$ is an arithmetic progression whose step shares a nontrivial factor with $n$, there exist pairs of column indices differing by a specific period that become identical under the selected frequencies. In such cases, for supports containing such pairs, $G_S$ has off-diagonal entries equal to $1$, producing repeated or highly collinear columns, driving $\\lambda_{\\max}$ well above $1$ and yielding $\\delta_k \\ge 1$ for suitable $k$.\n- When $n$ is prime and $\\Omega$ is spread (for example, an arithmetic progression with a step that is invertible modulo $n$), exact aliasing is avoided, and the discrete sums for off-diagonal correlations distribute more evenly on the unit circle, generally reducing the magnitude of off-diagonal Gram entries and thereby reducing $\\delta_2$ relative to clustered selections. Nonetheless, without randomness, deterministically spread patterns still exhibit structured correlations; the numerical value reflects this structure.\n\nThe implementation follows these principles exactly, using exhaustive enumeration and spectral computations to provide the precise $\\delta_k$ for each test case, which directly quantifies the interplay between frequency selection, induced column correlations, and the resulting restricted isometry behavior.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\n\ndef partial_fourier_matrix(n, omega):\n    \"\"\"\n    Construct an m x n partial Fourier matrix with unit-norm columns.\n    Entries: A[r, c] = exp(-2π i * f_r * c / n) / sqrt(m), where f_r in omega.\n    \"\"\"\n    m = len(omega)\n    # Create a grid of shape (m, n) for frequencies and column indices\n    freqs = np.array(omega, dtype=np.float64).reshape(-1, 1)  # (m, 1)\n    cols = np.arange(n, dtype=np.float64).reshape(1, -1)      # (1, n)\n    phases = -2.0 * np.pi * freqs * cols / float(n)           # (m, n)\n    A = np.exp(1j * phases) / np.sqrt(m)                      # (m, n), complex\n    return A\n\ndef delta_k_exact(A, k):\n    \"\"\"\n    Compute the exact restricted isometry constant δ_k for matrix A\n    by exhaustive enumeration of all supports of size k.\n    A is m x n with unit-norm columns assumed.\n    \"\"\"\n    m, n = A.shape\n    if k == 0:\n        return 0.0\n    # Precompute list of all combinations of column indices of size k\n    max_delta = 0.0\n    # For numerical stability when comparing, ensure real parts are used for eigenvalues\n    for S in combinations(range(n), k):\n        AS = A[:, S]  # m x k\n        G = (AS.conj().T) @ AS  # k x k Hermitian Gram matrix\n        # Use Hermitian eigenvalue solver\n        eigvals = np.linalg.eigvalsh(G)\n        # Numerical guard: project tiny imaginary parts away and clip tiny negatives due to rounding\n        eigvals = np.real(eigvals)\n        # Compute distortion for this support\n        local_delta = max(abs(1.0 - eigvals.min()), abs(eigvals.max() - 1.0))\n        if local_delta  max_delta:\n            max_delta = local_delta\n    return float(max_delta)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (n, omega_list, k)\n    test_cases = [\n        (17, [0,2,4,6,8,10,12,14], 2),            # Case 1: spread frequencies on prime n\n        (16, [0,1,2,3,4,5,6,7], 2),               # Case 2: consecutive low freqs on even n\n        (16, [0,1,2,3,4,5,6,7], 1),               # Case 3: boundary k=1\n        (21, [0,3,6,9,12,15,18], 3),              # Case 4: aliased selection (step 3) on n=21\n        (12, [0,1,2,3,4,5], 5),                   # Case 5: large k relative to m\n    ]\n\n    results = []\n    for n, omega, k in test_cases:\n        A = partial_fourier_matrix(n, omega)\n        delta = delta_k_exact(A, k)\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}