{
    "hands_on_practices": [
        {
            "introduction": "This first exercise takes you through the foundational derivation of the classic coherence-based recovery guarantee. Starting from the Exact Recovery Condition (ERC), you will use fundamental matrix norm inequalities to establish the famous threshold $s \\lt \\frac{1}{2}(1 + 1/\\mu)$, solidifying your understanding of how matrix geometry dictates recovery performance. ",
            "id": "3435250",
            "problem": "You are given a real matrix with unit-norm columns and asked to analyze the classical mutual-coherence-based exact recovery threshold for sparse recovery, together with its limiting behavior as the mutual coherence vanishes. Work entirely in the setting of linear algebra over the real numbers with finite dimensions. All angles are in radians and no physical units apply.\n\nFoundational base:\n- Mutual coherence of a matrix with unit-norm columns is defined by $\\,\\mu(A) \\triangleq \\max_{i\\neq j}\\,|\\langle a_i,a_j\\rangle|\\,$, where $\\,a_i\\,$ denotes the $\\,i$-th column and $\\,\\langle\\cdot,\\cdot\\rangle\\,$ is the standard inner product.\n- The Gram matrix of a column submatrix $\\,A_S\\,$ is $\\,G_S \\triangleq A_S^\\top A_S\\,$.\n- The Exact Recovery Condition (ERC) from sparse optimization states that a sufficient condition for unique recovery of any $\\,s$-sparse vector via $\\ell_1$-minimization or Orthogonal Matching Pursuit is\n$$\n\\max_{j\\notin S}\\,\\|A_S^+ a_j\\|_1 \\;\\; 1,\n$$\nfor every support set $\\,S\\subset\\{1,\\dots,n\\}\\,$ with $\\,|S|=s\\,$, where $\\,A_S^+ \\triangleq (A_S^\\top A_S)^{-1}A_S^\\top\\,$ denotes the Moore–Penrose pseudoinverse for full-column-rank $\\,A_S\\,$.\n- You may appeal to classical matrix norm inequalities (submultiplicativity of matrix norms, relationships between vector and matrix norms), the Neumann series bound for inverses $\\,\\|(I - E)^{-1}\\|\\leq 1/(1-\\|E\\|)\\,$ when $\\,\\|E\\|1\\,$, and the Gershgorin disc theorem to argue invertibility of $\\,G_S\\,$.\n\nTask A (derivation): Starting only from the above foundational base, derive a coherence-only sufficient threshold in terms of $\\,\\mu(A)\\,$ that guarantees the ERC for all supports of size $\\,s\\,$. Then analyze the limit $\\,\\mu(A)\\to 0\\,$ to show that the threshold scales like\n$$\ns \\;\\approx\\; \\tfrac{1}{2}\\!\\left(1+\\tfrac{1}{\\mu(A)}\\right),\n$$\nin the precise sense that the largest integer $\\,s\\,$ obeying the strict inequality implied by your bound satisfies\n$$\n\\lim_{\\mu\\to 0^+}\\; \\mu \\cdot s(\\mu) \\;=\\; \\tfrac{1}{2},\\qquad\n\\lim_{\\mu\\to 0^+}\\; \\frac{s(\\mu)}{\\tfrac{1}{2}(1+\\tfrac{1}{\\mu})} \\;=\\; 1.\n$$\nYour derivation must proceed from the definitions and the listed well-tested facts; do not invoke any result that presupposes the target threshold as an input.\n\nTask B (numerical illustration): Consider the explicit $\\,\\varepsilon$-parametrized family of dictionaries $\\,A(\\varepsilon)\\in\\mathbb{R}^{n\\times n}\\,$ with $\\,n\\geq 2\\,$, defined as follows. Let $\\,\\{e_1,\\dots,e_n\\}\\,$ be the standard basis of $\\,\\mathbb{R}^n\\,$. Define\n- $\\,a_1(\\varepsilon) \\triangleq e_1\\,$,\n- for every $\\,j\\in\\{2,\\dots,n\\}\\,$, $\\,a_j(\\varepsilon) \\triangleq \\varepsilon\\,e_1 + \\sqrt{1-\\varepsilon^2}\\,e_j\\,$,\nand assemble the columns $\\,a_j(\\varepsilon)\\,$ into $\\,A(\\varepsilon)\\,$. Show that for every $\\,\\varepsilon\\in(0,1]\\,$ the columns of $\\,A(\\varepsilon)\\,$ have unit norm and that\n$$\n\\mu\\big(A(\\varepsilon)\\big) \\;=\\; \\max\\{\\varepsilon,\\,\\varepsilon^2\\} \\;=\\; \\varepsilon.\n$$\n\nTask C (program specification): Implement a program that performs the following for each test case $\\,(\\varepsilon, n)\\,$ with $\\,n\\in\\mathbb{N}\\,$ and $\\,\\varepsilon\\in(0,1]\\,$:\n- Constructs $\\,A(\\varepsilon)\\in\\mathbb{R}^{n\\times n}\\,$ as above and computes $\\,\\mu\\big(A(\\varepsilon)\\big)\\,$.\n- Computes the real-valued coherence threshold\n$$\ns_{\\mathrm{cont}}(\\mu) \\;\\triangleq\\; \\tfrac{1}{2}\\!\\left(1+\\tfrac{1}{\\mu}\\right),\n$$\nand the largest integer satisfying the strict inequality implied by your derivation, encoded as\n$$\ns_{\\mathrm{int}}(\\mu) \\;\\triangleq\\; \\left\\lceil s_{\\mathrm{cont}}(\\mu)\\right\\rceil - 1.\n$$\n- Reports the scaled product $\\,\\mu\\cdot s_{\\mathrm{int}}(\\mu)\\,$ and the relative deviation\n$$\n\\mathrm{rel\\_dev} \\;\\triangleq\\; \\frac{\\big|s_{\\mathrm{cont}}(\\mu)-s_{\\mathrm{int}}(\\mu)\\big|}{s_{\\mathrm{cont}}(\\mu)}.\n$$\n- Optionally verifies the Exact Recovery Condition numerically at $\\,s_{\\mathrm{int}}(\\mu)\\,$ by exhaustive support search: define\n$$\nT_{A,s} \\;\\triangleq\\; \\max_{S\\subset\\{1,\\dots,n\\},\\,|S|=s}\\;\\max_{j\\notin S}\\; \\big\\|A_S^+\\,a_j\\big\\|_1.\n$$\nIf $\\,s_{\\mathrm{int}}(\\mu)\\leq n-1\\,$, compute $\\,T_{A,s_{\\mathrm{int}}}\\,$ exactly by enumeration and set an indicator\n$$\n\\mathrm{erc\\_ok} \\;=\\; \\begin{cases}\n1  \\text{if } T_{A,s_{\\mathrm{int}}}  1,\\\\\n0  \\text{otherwise.}\n\\end{cases}\n$$\nIf $\\,s_{\\mathrm{int}}(\\mu)  n-1\\,$, skip verification and output $\\,\\mathrm{erc\\_ok}=-1\\,$.\n- Round all floating-point outputs to $\\,6\\,$ decimal places.\n\nTest suite:\n- Use $\\,n=8\\,$ for all cases and the following $\\,\\varepsilon\\,$ values:\n  - $\\,\\varepsilon = 0.5\\,$,\n  - $\\,\\varepsilon = \\tfrac{1}{3}\\,$,\n  - $\\,\\varepsilon = 0.2\\,$,\n  - $\\,\\varepsilon = 0.15\\,$,\n  - $\\,\\varepsilon = 0.1\\,$,\n  - $\\,\\varepsilon = 0.05\\,$.\nFor each case, the program must output a list\n$$\n\\big[\\mu,\\, s_{\\mathrm{int}},\\, \\mu\\cdot s_{\\mathrm{int}},\\, \\mathrm{rel\\_dev},\\, \\mathrm{erc\\_ok}\\big],\n$$\nwith the real numbers rounded to $\\,6\\,$ decimals as specified.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the per-case lists, enclosed in square brackets, for example\n$$\n\\big[\\,[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\;[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\;\\dots\\big].\n$$\nNo other text should be printed.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard exercise in the field of compressed sensing and sparse optimization, with all necessary components defined and a clear set of tasks.\n\n### Task A: Derivation of the Coherence-Based Recovery Threshold\n\nThe primary objective is to derive a sufficient condition on the sparsity level $s$ in terms of the mutual coherence $\\mu(A)$ that guarantees the Exact Recovery Condition (ERC). The ERC is given by\n$$\n\\max_{S:|S|=s} \\max_{j\\notin S} \\|A_S^+ a_j\\|_1  1\n$$\nwhere $A$ is a matrix with unit-norm columns, $S$ is a support set of size $s$, $a_j$ is the $j$-th column of $A$, and $A_S^+$ is the Moore-Penrose pseudoinverse of the submatrix $A_S$ formed by columns with indices in $S$. We are given $A_S^+ = (A_S^\\top A_S)^{-1}A_S^\\top$.\n\nLet's fix a support set $S$ with $|S|=s$ and an index $j \\notin S$. We need to bound $\\|A_S^+ a_j\\|_1$.\nLet $v_j = A_S^+ a_j = (A_S^\\top A_S)^{-1} (A_S^\\top a_j)$.\nLet $G_S = A_S^\\top A_S$ be the Gram matrix of $A_S$. This is an $s \\times s$ matrix. Let $c_j = A_S^\\top a_j$ be an $s$-dimensional vector. Then $v_j = G_S^{-1} c_j$.\n\nUsing standard matrix and vector norm inequalities, we have:\n$$\n\\|v_j\\|_1 = \\|G_S^{-1} c_j\\|_1 \\le \\|G_S^{-1}\\|_1 \\|c_j\\|_1\n$$\nWe need to bound the two terms on the right-hand side.\n\n1.  **Bounding $\\|c_j\\|_1$**: The vector $c_j$ has components $(c_j)_i = \\langle a_i, a_j \\rangle$ for each $i \\in S$. The mutual coherence $\\mu(A)$ is defined as $\\mu(A) \\triangleq \\max_{i\\neq k}\\,|\\langle a_i, a_k\\rangle|$. Since $j \\notin S$, for every $i \\in S$, we have $i \\ne j$. Therefore, $|\\langle a_i, a_j \\rangle| \\le \\mu(A)$.\n    The $\\ell_1$-norm of $c_j$ is the sum of the absolute values of its $s$ components:\n    $$\n    \\|c_j\\|_1 = \\sum_{i \\in S} |\\langle a_i, a_j \\rangle| \\le \\sum_{i \\in S} \\mu(A) = s \\mu(A)\n    $$\n\n2.  **Bounding $\\|G_S^{-1}\\|_1$**: The Gram matrix $G_S$ can be written as $G_S = I + E$, where $I$ is the $s \\times s$ identity matrix and $E$ is a matrix containing the off-diagonal inner products. The diagonal entries of $G_S$ are $(G_S)_{ii} = \\langle a_i, a_i \\rangle = 1$ since all columns are unit-norm. The off-diagonal entries are $(G_S)_{ik} = \\langle a_i, a_k \\rangle$ for $i, k \\in S, i \\ne k$.\n    Thus, $E$ has zeros on its diagonal, and its off-diagonal entries are $(E)_{ik} = \\langle a_i, a_k \\rangle$, so $|(E)_{ik}| \\le \\mu(A)$ for $i \\ne k$.\n\n    To ensure $G_S$ is invertible and to bound its inverse, we can use the Gershgorin circle theorem or the Neumann series. Let's use the latter, which requires a bound on a norm of $E$. The matrix $\\ell_\\infty$-norm is given by the maximum absolute row sum:\n    $$\n    \\|E\\|_\\infty = \\max_{i \\in S} \\sum_{k \\in S} |E_{ik}| = \\max_{i \\in S} \\sum_{k \\in S, k \\ne i} |\\langle a_i, a_k \\rangle| \\le \\max_{i \\in S} \\sum_{k \\in S, k \\ne i} \\mu(A) = (s-1)\\mu(A)\n    $$\n    If $\\|E\\|_\\infty  1$, i.e., $(s-1)\\mu(A)  1$, the matrix $G_S = I+E$ is invertible, and we can bound its inverse norm:\n    $$\n    \\|G_S^{-1}\\|_\\infty = \\|(I+E)^{-1}\\|_\\infty \\le \\frac{1}{1 - \\|E\\|_\\infty} \\le \\frac{1}{1 - (s-1)\\mu(A)}\n    $$\n    Since $G_S$ is a Gram matrix, it is symmetric. Its inverse $G_S^{-1}$ is also symmetric. For any symmetric matrix $M$, $\\|M\\|_1 = \\|M\\|_\\infty$. Therefore,\n    $$\n    \\|G_S^{-1}\\|_1 = \\|G_S^{-1}\\|_\\infty \\le \\frac{1}{1 - (s-1)\\mu(A)}\n    $$\n\nCombining these bounds, we get:\n$$\n\\|A_S^+ a_j\\|_1 \\le \\|G_S^{-1}\\|_1 \\|c_j\\|_1 \\le \\frac{s \\mu(A)}{1 - (s-1)\\mu(A)}\n$$\nFor the ERC to hold, we require this upper bound to be strictly less than $1$:\n$$\n\\frac{s \\mu(A)}{1 - (s-1)\\mu(A)}  1\n$$\nThis inequality is contingent on the denominator being positive, which is the condition $(s-1)\\mu(A)  1$ we already imposed. Assuming this holds, we can multiply both sides by the denominator:\n$$\ns \\mu(A)  1 - (s-1)\\mu(A) \\implies s \\mu(A)  1 - s\\mu(A) + \\mu(A)\n$$\nRearranging the terms gives:\n$$\n2s\\mu(A) - \\mu(A)  1 \\implies (2s-1)\\mu(A)  1\n$$\nAssuming $\\mu(A)  0$, we can divide to find the condition on $s$:\n$$\n2s-1  \\frac{1}{\\mu(A)} \\implies 2s  1 + \\frac{1}{\\mu(A)} \\implies s  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu(A)}\\right)\n$$\nThis is the well-known coherence-based sufficient condition for exact sparse recovery.\n\nNow, we analyze the limiting behavior as $\\mu \\to 0^+$. Let $s(\\mu)$ be the largest integer satisfying the strict inequality $s  \\frac{1}{2}(1 + 1/\\mu)$. This is given by $s(\\mu) = \\lceil \\frac{1}{2}(1 + 1/\\mu) \\rceil - 1$.\nLet $f(\\mu) = \\frac{1}{2}(1 + 1/\\mu)$. By definition of the ceiling function, we have $f(\\mu) \\le \\lceil f(\\mu) \\rceil  f(\\mu)+1$. Subtracting $1$ gives $f(\\mu)-1 \\le s(\\mu)  f(\\mu)$.\n$\\frac{1}{2}(1 + 1/\\mu) - 1  s(\\mu)  \\frac{1}{2}(1 + 1/\\mu)$.\n$\\frac{1}{2\\mu} - \\frac{1}{2}  s(\\mu)  \\frac{1}{2\\mu} + \\frac{1}{2}$.\n\n1.  **Limit of $\\mu \\cdot s(\\mu)$**: Multiply the inequality by $\\mu  0$:\n    $$\n    \\mu\\left(\\frac{1}{2\\mu} - \\frac{1}{2}\\right)  \\mu \\cdot s(\\mu)  \\mu\\left(\\frac{1}{2\\mu} + \\frac{1}{2}\\right) \\implies \\frac{1}{2} - \\frac{\\mu}{2}  \\mu \\cdot s(\\mu)  \\frac{1}{2} + \\frac{\\mu}{2}\n    $$\n    As $\\mu \\to 0^+$, both the lower and upper bounds converge to $1/2$. By the Squeeze Theorem, $\\lim_{\\mu\\to 0^+} \\mu \\cdot s(\\mu) = \\frac{1}{2}$.\n\n2.  **Limit of the ratio**: Divide the inequality $f(\\mu)-1  s(\\mu)  f(\\mu)$ by $f(\\mu)$:\n    $$\n    \\frac{f(\\mu)-1}{f(\\mu)}  \\frac{s(\\mu)}{f(\\mu)}  \\frac{f(\\mu)}{f(\\mu)} \\implies 1 - \\frac{1}{f(\\mu)}  \\frac{s(\\mu)}{f(\\mu)}  1\n    $$\n    As $\\mu \\to 0^+$, $f(\\mu) = \\frac{1}{2}(1+1/\\mu) \\to \\infty$, so $1/f(\\mu) \\to 0$. The lower bound converges to $1$. By the Squeeze Theorem, $\\lim_{\\mu\\to 0^+} \\frac{s(\\mu)}{f(\\mu)} = 1$.\n\n### Task B: Analysis of the Matrix Family $A(\\varepsilon)$\n\nWe are given $A(\\varepsilon) \\in \\mathbb{R}^{n \\times n}$ for $n \\ge 2, \\varepsilon \\in (0,1]$, with columns $a_1(\\varepsilon) = e_1$ and $a_j(\\varepsilon) = \\varepsilon e_1 + \\sqrt{1-\\varepsilon^2} e_j$ for $j \\ge 2$.\n\n1.  **Unit Column Norms**:\n    - For column $1$: $\\|a_1\\|^2 = \\|e_1\\|^2 = 1^2 = 1$.\n    - For columns $j \\in \\{2, \\dots, n\\}$: The vectors $e_1$ and $e_j$ are orthogonal. By the Pythagorean theorem:\n      $\\|a_j\\|^2 = \\|\\varepsilon e_1 + \\sqrt{1-\\varepsilon^2} e_j\\|^2 = (\\varepsilon)^2 + (\\sqrt{1-\\varepsilon^2})^2 = \\varepsilon^2 + 1 - \\varepsilon^2 = 1$.\n    All columns have unit $\\ell_2$-norm.\n\n2.  **Mutual Coherence**: We compute the inner products between distinct columns.\n    - For $j \\in \\{2, \\dots, n\\}$:\n      $\\langle a_1, a_j \\rangle = \\langle e_1, \\varepsilon e_1 + \\sqrt{1-\\varepsilon^2} e_j \\rangle = \\varepsilon \\langle e_1, e_1 \\rangle + \\sqrt{1-\\varepsilon^2} \\langle e_1, e_j \\rangle = \\varepsilon \\cdot 1 + 0 = \\varepsilon$.\n    - For $i,j \\in \\{2, \\dots, n\\}$ with $i \\ne j$:\n      $\\langle a_i, a_j \\rangle = \\langle \\varepsilon e_1 + \\sqrt{1-\\varepsilon^2} e_i, \\varepsilon e_1 + \\sqrt{1-\\varepsilon^2} e_j \\rangle$.\n      Expanding this using orthogonality of $\\{e_k\\}$:\n      $\\langle a_i, a_j \\rangle = \\varepsilon^2 \\langle e_1, e_1 \\rangle + (1-\\varepsilon^2) \\langle e_i, e_j \\rangle = \\varepsilon^2 \\cdot 1 + (1-\\varepsilon^2) \\cdot 0 = \\varepsilon^2$.\n    The set of absolute values of off-diagonal inner products is $\\{\\varepsilon, \\varepsilon^2\\}$. The mutual coherence is the maximum of these:\n    $\\mu(A(\\varepsilon)) = \\max \\{|\\varepsilon|, |\\varepsilon^2|\\}$. Since $\\varepsilon \\in (0,1]$, we have $\\varepsilon  0$ and $\\varepsilon^2 \\le \\varepsilon$. Thus, $\\mu(A(\\varepsilon)) = \\varepsilon$.\n\n### Task C: Program Specification Logic\n\nThe program will implement the specified calculations for a test suite where $n=8$ and $\\varepsilon$ takes several values in $(0,1]$. For each test case $(\\varepsilon, n)$:\n1.  The mutual coherence is computed as $\\mu = \\varepsilon$, per the derivation in Task B.\n2.  The continuous and integer sparsity thresholds are calculated:\n    $s_{\\mathrm{cont}}(\\mu) = \\frac{1}{2}(1+1/\\mu)$ and $s_{\\mathrm{int}}(\\mu) = \\lceil s_{\\mathrm{cont}}(\\mu)\\rceil - 1$.\n3.  The scaled product $\\mu \\cdot s_{\\mathrm{int}}(\\mu)$ and relative deviation $\\mathrm{rel\\_dev} = |s_{\\mathrm{cont}}(\\mu)-s_{\\mathrm{int}}(\\mu)|/s_{\\mathrm{cont}}(\\mu)$ are computed.\n4.  The ERC is numerically verified for sparsity $s = s_{\\mathrm{int}}(\\mu)$.\n    - If $s_{\\mathrm{int}}(\\mu)  n-1$, verification is not possible, and $\\mathrm{erc\\_ok}$ is set to $-1$.\n    - Otherwise, the matrix $A(\\varepsilon)$ of size $n \\times n$ is constructed. The program then exhaustively iterates through all support sets $S \\subset \\{1, \\dots, n\\}$ of size $s_{\\mathrm{int}}(\\mu)$ using `itertools.combinations`.\n    - For each support $S$, the program computes the maximum of $\\|A_S^+ a_j\\|_1$ over all $j \\notin S$. The pseudoinverse $A_S^+$ is computed using `numpy.linalg.pinv`.\n    - The overall maximum value across all supports $S$, denoted $T_{A,s_{\\mathrm{int}}}$, is determined.\n    - The indicator $\\mathrm{erc\\_ok}$ is set to $1$ if $T_{A,s_{\\mathrm{int}}}  1$ and $0$ otherwise.\n5.  Finally, a list containing $[\\mu, s_{\\mathrm{int}}, \\mu \\cdot s_{\\mathrm{int}}, \\mathrm{rel\\_dev}, \\mathrm{erc\\_ok}]$, with all floating-point values rounded to $6$ decimal places, is generated for the test case. The results for all test cases are aggregated into a list of lists for the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy  # Per library specification, though not strictly used.\nimport math\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing a parametrized matrix family A(eps)\n    and verifying the derived coherence-based exact recovery threshold.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (epsilon, n)\n        (0.5, 8),\n        (1/3, 8),\n        (0.2, 8),\n        (0.15, 8),\n        (0.1, 8),\n        (0.05, 8),\n    ]\n\n    all_results = []\n    for eps, n in test_cases:\n        # Per Task B, mu(A(eps)) = eps for eps in (0, 1].\n        mu = eps\n\n        # Compute coherence-based thresholds\n        s_cont = 0.5 * (1.0 + 1.0 / mu)\n        s_int = math.ceil(s_cont) - 1\n\n        # Compute derived quantities\n        mu_s_int = mu * s_int\n        if s_cont  0:\n            rel_dev = abs(s_cont - s_int) / s_cont\n        else:\n            rel_dev = 0.0\n\n        # Numerically verify the Exact Recovery Condition (ERC)\n        erc_ok = -1\n        if s_int  n - 1:\n            erc_ok = -1\n        else:\n            # Construct the matrix A(eps)\n            A = np.zeros((n, n), dtype=float)\n            e1 = np.zeros(n, dtype=float)\n            e1[0] = 1.0\n            A[:, 0] = e1\n            for j in range(1, n):\n                ej = np.zeros(n, dtype=float)\n                ej[j] = 1.0\n                A[:, j] = eps * e1 + math.sqrt(1 - eps**2) * ej\n\n            # Exhaustively check all supports of size s_int\n            max_erc_val = 0.0\n            all_indices = set(range(n))\n            \n            # The number of combinations can be large, but for n=8 and s=7 it's feasible.\n            s = int(s_int)\n            # s=0 is trivial, ERC holds if defined. Let's assume s = 1.\n            if s = 1:\n                for s_indices in combinations(range(n), s):\n                    S = list(s_indices)\n                    J = list(all_indices - set(S))\n                    \n                    A_S = A[:, S]\n                    \n                    # Compute the Moore-Penrose pseudoinverse\n                    A_S_plus = np.linalg.pinv(A_S)\n                    \n                    for j in J:\n                        a_j = A[:, j]\n                        \n                        # Calculate the l1-norm of the projection\n                        norm_val = np.linalg.norm(A_S_plus @ a_j, ord=1)\n                        \n                        if norm_val  max_erc_val:\n                            max_erc_val = norm_val\n            \n            # The ERC is T_{A,s}  1\n            erc_ok = 1 if max_erc_val  1.0 else 0\n\n        # Assemble the list of results for this case, with rounding\n        case_result = [\n            round(mu, 6),\n            s_int,\n            round(mu_s_int, 6),\n            round(rel_dev, 6),\n            erc_ok\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string as a list of lists\n    # e.g., [[val1, val2], [val3, val4]]\n    # Using str() on a list of lists produces the desired format with spaces.\n    # The problem asks for comma-separated list of lists, without specifying space behavior.\n    # Using a join of stringified lists is safer.\n    final_output_str = f\"[{','.join([str(res) for res in all_results])}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output_str.replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "A theoretical guarantee is only as useful as it is sharp. This practice challenges you to construct an Equiangular Tight Frame—a matrix that is maximally incoherent—and use it to demonstrate the tightness of the coherence bound by creating explicit scenarios where recovery fails precisely at the boundary. ",
            "id": "3435242",
            "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ be a sensing matrix with unit-norm columns $\\{a_i\\}_{i=1}^n$. The mutual coherence is defined as $\\mu(A) \\triangleq \\max_{i \\neq j} \\left|\\langle a_i, a_j \\rangle \\right|$. Consider the sparse recovery problem: given a measurement vector $y = A x^\\star$, where $x^\\star \\in \\mathbb{R}^n$ is $s$-sparse (has at most $s$ nonzero entries), attempt to recover $x^\\star$ via Basis Pursuit (BP), which is the $\\ell_1$-minimization program\n$$\n\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1 \\quad \\text{subject to} \\quad A x = y.\n$$\nYou will construct a specific matrix $A$ that is an Equiangular Tight Frame (ETF), derive a coherence-based exact recovery guarantee from first principles, and examine its tightness by explicit boundary cases.\n\nFundamental base:\n- Use only the following foundational concepts.\n    1. The mutual coherence definition $\\mu(A)$.\n    2. The $\\ell_1$-minimization formulation of Basis Pursuit (BP).\n    3. The Null Space Property (NSP): a matrix $A$ guarantees exact recovery of all $s$-sparse vectors by BP if and only if for all $h \\in \\ker(A)$ and all index sets $S \\subset \\{1,\\dots,n\\}$ with $|S| \\le s$, one has $\\|h_S\\|_1  \\|h_{S^c}\\|_1$, where $S^c$ is the complement of $S$.\n    4. The Welch bound fact: for unit-norm $\\{a_i\\}_{i=1}^n \\subset \\mathbb{R}^m$, $\\mu(A) \\ge \\sqrt{\\frac{n-m}{m(n-1)}}$, and equality is achieved by equiangular tight frames.\n- Do not use or assume any pre-derived sparsity threshold formulas. Derive the coherence-based exact recovery threshold starting from the Null Space Property and the mutual coherence definition.\n\nConstruction task:\n- Build a real Equiangular Tight Frame (ETF) in $\\mathbb{R}^m$ with $n = m+1$ columns using the regular simplex construction:\n    1. Let $n = m+1$ and consider the vectors $w_i = e_i - \\frac{1}{n} \\mathbf{1} \\in \\mathbb{R}^n$, where $e_i$ are the standard basis vectors and $\\mathbf{1}$ is the all-ones vector.\n    2. Let $Y \\in \\mathbb{R}^{n \\times m}$ have orthonormal columns forming a basis of the subspace orthogonal to $\\mathbf{1}$.\n    3. Define $A = \\alpha Y^\\top [w_1,\\dots,w_n] \\in \\mathbb{R}^{m \\times n}$ and choose the scalar $\\alpha$ so that the columns of $A$ have unit norm.\n- Verify numerically that the resulting frame is equiangular and tight by computing all pairwise inner products of the columns and the frame operator $A A^\\top$.\n\nDerivation task:\n- Starting from the Null Space Property and only the mutual coherence bound $\\left|\\langle a_i, a_j \\rangle\\right| \\le \\mu(A)$ for $i \\neq j$, derive a sufficient condition on the sparsity level $s$ (expressed as an inequality involving $\\mu(A)$ and $s$) under which BP exactly recovers any $s$-sparse $x^\\star$ from $y = A x^\\star$.\n\nTightness and boundary cases:\n- Using the constructed ETF with $n=m+1$ columns, explicitly exhibit boundary cases where BP succeeds and fails. Your boundary cases must hinge on the exact dependence relation among the columns of the simplex ETF (not on random trials). In particular, use the identity $\\sum_{i=1}^n a_i = 0$ to build measurement vectors $y$ that admit two sparse representations with different $\\ell_1$ norms when $m$ is even (so that the two set sizes differ), and verify whether BP selects the lower $\\ell_1$ representation.\n\nTest suite:\n- Implement the above for the following parameter values and supports; in each case, set the ground-truth coefficients on the specified support equal to $+1$ and zero elsewhere. Let $y = A x^\\star$.\n    1. $m=4$, $n=5$, support $S=\\{0,1\\}$ (happy path strictly below the derived threshold).\n    2. $m=4$, $n=5$, support $S=\\{2,3,4\\}$ (boundary case; complement has size $2$).\n    3. $m=6$, $n=7$, support $S=\\{0,1,2}$ (happy path strictly below the derived threshold).\n    4. $m=6$, $n=7$, support $S=\\{3,4,5,6\\}$ (boundary case; complement has size $3$).\n- For each case:\n    1. Compute the mutual coherence $\\mu(A)$ numerically from the constructed $A$.\n    2. Check whether the derived coherence-based sparsity condition is strictly satisfied by $s = |S|$.\n    3. Solve BP via linear programming to recover $\\hat{x}$ and determine success as a boolean: return true if $\\|\\hat{x} - x^\\star\\|_\\infty \\le 10^{-6}$, and false otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of per-test-case results. Each result must be a list of four entries in the following order:\n    1. The numerical mutual coherence $\\mu(A)$ as a float.\n    2. The sparsity $s$ as an integer.\n    3. A boolean indicating whether the derived coherence-based condition is strictly satisfied.\n    4. A boolean indicating whether BP succeeded for the specified ground truth.\n- The final output must be printed as a single list of these four-entry lists, in the exact string format “[case1,case2,case3,case4]”, where each case is itself printed as a Python-style list.",
            "solution": "We begin with the fundamental definitions. Let $A \\in \\mathbb{R}^{m \\times n}$ have unit-norm columns $\\{a_i\\}_{i=1}^n$. The mutual coherence is $\\mu(A) \\triangleq \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$. The $\\ell_1$-minimization program Basis Pursuit (BP) seeks $\\hat{x} = \\arg\\min \\|x\\|_1$ subject to $A x = y$, with $y = A x^\\star$ where $x^\\star$ is $s$-sparse. The Null Space Property (NSP) states that BP exactly recovers all $s$-sparse signals if and only if for every $h \\in \\ker(A)$ and every index set $S$ with $|S| \\le s$, we have $\\|h_S\\|_1  \\|h_{S^c}\\|_1$.\n\nWe derive a sufficient coherence-based condition for exact recovery by BP. Let $h \\in \\ker(A)$ be nonzero. Then $A h = 0$ implies $A^\\top A h = 0$. For each $i \\in \\{1,\\dots,n\\}$,\n$$\n0 = \\langle a_i, A h \\rangle = \\sum_{j=1}^n \\langle a_i, a_j \\rangle h_j = h_i + \\sum_{j \\ne i} \\langle a_i, a_j \\rangle h_j,\n$$\nwhere we used $\\|a_i\\|_2 = 1$ so that $\\langle a_i, a_i \\rangle = 1$. Hence\n$$\nh_i = - \\sum_{j \\ne i} \\langle a_i, a_j \\rangle h_j.\n$$\nTaking absolute values and using $|\\langle a_i, a_j \\rangle| \\le \\mu(A)$ for $i \\ne j$, we have\n$$\n|h_i| \\le \\sum_{j \\ne i} |\\langle a_i, a_j \\rangle| \\, |h_j| \\le \\mu(A) \\sum_{j \\ne i} |h_j|.\n$$\nFix a support set $S$ with $|S| \\le s$. Summing the above inequality over $i \\in S$ yields\n$$\n\\sum_{i \\in S} |h_i| \\le \\mu(A) \\sum_{i \\in S} \\sum_{j \\ne i} |h_j| = \\mu(A) \\left( \\sum_{i \\in S} \\sum_{j \\in S, j \\ne i} |h_j| + \\sum_{i \\in S} \\sum_{j \\in S^c} |h_j| \\right).\n$$\nWe bound the two terms separately. For the intra-support part,\n$$\n\\sum_{i \\in S} \\sum_{j \\in S, j \\ne i} |h_j| = \\sum_{j \\in S} |h_j| \\cdot \\left(|S| - 1\\right) = \\left(|S| - 1\\right) \\|h_S\\|_1.\n$$\nFor the cross-support part,\n$$\n\\sum_{i \\in S} \\sum_{j \\in S^c} |h_j| = |S| \\cdot \\|h_{S^c}\\|_1.\n$$\nCombining gives\n$$\n\\|h_S\\|_1 \\le \\mu(A) \\left( \\left(|S| - 1\\right) \\|h_S\\|_1 + |S| \\cdot \\|h_{S^c}\\|_1 \\right).\n$$\nRearranging (assuming $\\mu(A)\\left(|S| - 1\\right)  1$ to avoid degeneracy),\n$$\n\\left(1 - \\mu(A)\\left(|S| - 1\\right)\\right) \\|h_S\\|_1 \\le \\mu(A) |S| \\cdot \\|h_{S^c}\\|_1,\n$$\nand thus\n$$\n\\|h_S\\|_1 \\le \\frac{\\mu(A) |S|}{1 - \\mu(A)\\left(|S| - 1\\right)} \\, \\|h_{S^c}\\|_1.\n$$\nA sufficient condition for the Null Space Property is that the factor on the right is strictly less than $1$ for all $|S| \\le s$, i.e.,\n$$\n\\frac{\\mu(A) s}{1 - \\mu(A)\\left(s - 1\\right)}  1.\n$$\nSolving this inequality for $s$ gives\n$$\n\\mu(A) s  1 - \\mu(A)\\left(s - 1\\right)\n\\quad \\Longleftrightarrow \\quad\n\\mu(A) s  1 - \\mu(A) s + \\mu(A)\n\\quad \\Longleftrightarrow \\quad\n2 \\mu(A) s  1 + \\mu(A)\n\\quad \\Longleftrightarrow \\quad\ns  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu(A)}\\right).\n$$\nThis is a coherence-based sufficient condition for exact recovery of all $s$-sparse signals by BP.\n\nNext, we construct an Equiangular Tight Frame (ETF) that attains minimal coherence (the Welch bound) with $n = m+1$ columns, using a regular simplex. Let $n = m+1$, and define the vectors $w_i = e_i - \\frac{1}{n} \\mathbf{1} \\in \\mathbb{R}^n$. These lie in the subspace orthogonal to $\\mathbf{1}$. Choose an orthonormal basis $Y \\in \\mathbb{R}^{n \\times m}$ for this subspace, and define $A = \\alpha Y^\\top [w_1,\\dots,w_n] \\in \\mathbb{R}^{m \\times n}$. The scalar $\\alpha$ is chosen so that columns of $A$ have unit norm. Because $Y$ has orthonormal columns and the $w_i$ lie in the span of $Y$, inner products are preserved by $Y^\\top$. The norms satisfy $\\|w_i\\|_2^2 = 1 - \\frac{1}{n} = \\frac{n-1}{n}$ and $w_i^\\top w_j = -\\frac{1}{n}$ for $i \\ne j$. After scaling by $\\alpha = \\sqrt{\\frac{n}{n-1}}$, the columns of $A$ are unit norm and have pairwise inner products equal to $-\\frac{1}{n-1} = -\\frac{1}{m}$, so $\\mu(A) = \\frac{1}{m}$. Moreover, $A A^\\top = \\frac{n}{m} I_m$, which shows tightness.\n\nWe now explain the tightness of the coherence-based threshold for ETFs via explicit boundary cases. For the simplex ETF, the columns satisfy $\\sum_{i=1}^n a_i = 0$. Consider $m$ even so that $n = m+1$ is odd. For a subset $S \\subset \\{1,\\dots,n\\}$ with $|S| = s = \\left\\lceil \\frac{m+1}{2} \\right\\rceil$, its complement has size $n - s = \\left\\lfloor \\frac{m+1}{2} \\right\\rfloor$. Then\n$$\n\\sum_{i \\in S} a_i = - \\sum_{j \\in S^c} a_j.\n$$\nDefine $x^\\star$ by setting $x^\\star_i = 1$ for $i \\in S$ and $0$ otherwise; then $y = A x^\\star = \\sum_{i \\in S} a_i = - \\sum_{j \\in S^c} a_j = A z$ with $z_j = -1$ for $j \\in S^c$ and $0$ otherwise. The two feasible representations have $\\ell_1$ norms $\\|x^\\star\\|_1 = |S|$ and $\\|z\\|_1 = |S^c|$. When $m$ is even, $|S|  |S^c|$, so BP, which minimizes the $\\ell_1$ norm under $A x = y$, will strictly prefer the complement representation, thereby failing to recover $x^\\star$. This concretely demonstrates failure at the boundary value while the strict inequality condition still guarantees success below the threshold. For $s$ strictly below the derived threshold, the NSP holds, and BP succeeds for all $s$-sparse $x^\\star$.\n\nAlgorithmic design for the program:\n- Construct the simplex ETF $A$ for $m \\in \\{4,6\\}$ and $n = m+1$ by building an orthonormal basis of the subspace orthogonal to $\\mathbf{1}$ via a thin $QR$ factorization of the matrix whose columns are $e_k - e_n$ for $k = 1, \\dots, n-1$, then form $A = \\alpha Y^\\top [w_1,\\dots,w_n]$ with $\\alpha = \\sqrt{\\frac{n}{n-1}}$.\n- Compute $\\mu(A)$ numerically as the maximum absolute off-diagonal entry of $A^\\top A$.\n- Derive the strict coherence-based sparsity condition $s  \\frac{1}{2} \\left(1 + \\frac{1}{\\mu(A)}\\right)$ and evaluate it for each test case.\n- Solve BP via linear programming. Introduce auxiliary variables $t \\in \\mathbb{R}^n$ with constraints $-t \\le x \\le t$ and $t \\ge 0$, and minimize $\\sum_i t_i$ subject to $A x = y$. This is a standard $\\ell_1$-to-linear-program reduction. Use a numerical tolerance to declare success.\n- Use the specified supports and unit coefficients for the ground truths:\n    1. $m=4$, $S=\\{0,1\\}$.\n    2. $m=4$, $S=\\{2,3,4\\}$.\n    3. $m=6$, $S=\\{0,1,2\\}$.\n    4. $m=6$, $S=\\{3,4,5,6\\}$.\n- Return for each case the tuple $[\\mu(A), s, \\text{strict\\_condition}, \\text{bp\\_success}]$ in the specified single-line output format.\n\nThis design faithfully integrates the fundamental Null Space Property and mutual coherence bounds to derive the condition, constructs an Equiangular Tight Frame that attains the Welch bound, and exhibits boundary cases proving the tightness of the derived threshold via explicit sparse alternatives with strictly smaller $\\ell_1$ norm when $m$ is even.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef simplex_etf(m: int) - np.ndarray:\n    \"\"\"\n    Construct an m x (m+1) real Equiangular Tight Frame (ETF) via the regular simplex.\n    Returns a matrix A with unit-norm equiangular columns and tight frame property.\n    \"\"\"\n    n = m + 1\n    # Build vectors w_i = e_i - (1/n) * 1 in R^n\n    W = np.eye(n) - (1.0 / n) * np.ones((n, n))  # columns are w_i\n    # Build an orthonormal basis Y of the subspace orthogonal to 1 (dimension m)\n    # Use B whose columns span 1^\\perp: B_k = e_k - e_n for k=1,...,n-1\n    B = np.zeros((n, m))\n    for k in range(m):\n        B[k, k] = 1.0\n        B[-1, k] = -1.0\n    # Thin QR to get orthonormal basis of span(B)\n    Q, R = np.linalg.qr(B, mode='reduced')  # Q is n x m with orthonormal columns\n    Y = Q  # columns of Y form an orthonormal basis of 1^\\perp\n    # Coordinates of w_i in the basis Y: A0 = Y^T W\n    A0 = Y.T @ W\n    # Normalize columns to unit norm using alpha = sqrt(n/(n-1))\n    alpha = np.sqrt(n / (n - 1))\n    A = alpha * A0\n    return A\n\ndef mutual_coherence(A: np.ndarray) - float:\n    \"\"\"\n    Compute mutual coherence mu(A) as max absolute off-diagonal entry of Gram matrix.\n    Assumes columns of A are unit-norm.\n    \"\"\"\n    G = A.T @ A\n    # mask off-diagonals\n    off_diag = G - np.eye(G.shape[0])\n    mu = np.max(np.abs(off_diag))\n    return float(mu)\n\ndef basis_pursuit(A: np.ndarray, y: np.ndarray) - np.ndarray:\n    \"\"\"\n    Solve Basis Pursuit: minimize ||x||_1 subject to A x = y via linear programming.\n    Uses variable splitting with auxiliary t = |x|.\n    Returns the recovered x.\n    \"\"\"\n    m, n = A.shape\n    # Variables: [x (n), t (n)]\n    num_vars = 2 * n\n    c = np.zeros(num_vars)\n    c[n:] = 1.0  # objective: minimize sum(t)\n    # Equality constraints: A x = y\n    A_eq = np.hstack([A, np.zeros((m, n))])\n    b_eq = y.copy()\n    # Inequality constraints: x - t = 0 and -x - t = 0\n    A_ub = np.zeros((2 * n, num_vars))\n    b_ub = np.zeros(2 * n)\n    # x - t = 0\n    for i in range(n):\n        A_ub[i, i] = 1.0\n        A_ub[i, n + i] = -1.0\n    # -x - t = 0  = -(x) - t = 0\n    for i in range(n):\n        A_ub[n + i, i] = -1.0\n        A_ub[n + i, n + i] = -1.0\n    # Bounds: x free, t = 0\n    bounds = []\n    for i in range(n):\n        bounds.append((None, None))  # x_i free\n    for i in range(n):\n        bounds.append((0.0, None))   # t_i = 0\n    # Solve LP\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n    if not res.success:\n        raise RuntimeError(\"Basis Pursuit LP did not converge: \" + res.message)\n    x_rec = res.x[:n]\n    return x_rec\n\ndef run_case(m: int, support: list) - list:\n    \"\"\"\n    Run a single test case: construct ETF, compute mu, threshold condition, and BP recovery.\n    Returns [mu, s, strict_condition, bp_success].\n    \"\"\"\n    A = simplex_etf(m)\n    mu = mutual_coherence(A)\n    n = m + 1\n    s = len(support)\n    # Ground truth x*: ones on support, zeros elsewhere\n    x_star = np.zeros(n)\n    x_star[support] = 1.0\n    y = A @ x_star\n    # Derived coherence-based strict condition: s  0.5 * (1 + 1 / mu)\n    threshold = 0.5 * (1.0 + 1.0 / mu)\n    strict_condition = (s  threshold)\n    # Solve BP\n    x_hat = basis_pursuit(A, y)\n    bp_success = bool(np.allclose(x_hat, x_star, atol=1e-6))\n    return [mu, s, strict_condition, bp_success]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (4, [0, 1]),            # Happy path: s=2 below threshold (2.5)\n        (4, [2, 3, 4]),         # Boundary failure: s=3, complement size 2, BP prefers complement\n        (6, [0, 1, 2]),         # Happy path: s=3 below threshold (3.5)\n        (6, [3, 4, 5, 6]),      # Boundary failure: s=4, complement size 3\n    ]\n\n    results = []\n    for m, support in test_cases:\n        result = run_case(m, support)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Each case is a list [mu, s, strict_condition, bp_success]\n    # Print as a single list of these lists.\n    def format_entry(entry):\n        # Ensure booleans print as True/False and floats/ints as default\n        return \"[\" + \",\".join([\n            f\"{entry[0]}\",  # mu float\n            f\"{entry[1]}\",  # s int\n            \"True\" if entry[2] else \"False\",  # strict_condition\n            \"True\" if entry[3] else \"False\"   # bp_success\n        ]) + \"]\"\n    print(\"[\" + \",\".join(format_entry(e) for e in results) + \"]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While mutual coherence provides a powerful and simple guarantee, it does not tell the whole story. This exercise demonstrates the limitations of coherence by asking you to compare two dictionaries that are identical in terms of mutual coherence but behave differently under Orthogonal Matching Pursuit (OMP), highlighting the role of more subtle spectral properties. ",
            "id": "3435245",
            "problem": "Construct two explicit real dictionaries $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{m \\times n}$ with identical mutual coherence but different spectral properties of carefully chosen sub-Gram matrices, and empirically compare exact recovery by Orthogonal Matching Pursuit (OMP) for several sparse signals. Work entirely in exact arithmetic over real numbers with no physical units. Angles need not be introduced. The task is to derive and implement, from first principles, the following pipeline:\n\n1. Definitions and core objects to be used.\n   - Mutual coherence $\\mu(D)$ of a dictionary $D$ with unit-norm columns is defined as $\\mu(D) \\triangleq \\max_{i \\neq j} \\left| \\langle d_i, d_j \\rangle \\right|$, where $d_i$ denotes the $i$-th column of $D$. For non-unit columns, the inner products in the Gram matrix must be computed after column-wise normalization.\n   - The sub-Gram matrix of a subset of columns indexed by a finite set $T \\subset \\{1,\\dots,n\\}$ is $G_T(D) \\triangleq D_T^\\top D_T$, where $D_T$ is the submatrix formed by the columns with indices in $T$.\n   - Orthogonal Matching Pursuit (OMP) is a greedy algorithm for sparse recovery. Given a dictionary $D$, a measurement vector $y$, and a target sparsity level $k$, OMP iteratively:\n     - Selects an index maximizing the absolute correlation with the current residual.\n     - Augments the active set.\n     - Recomputes the least-squares fit on the active set and updates the residual.\n     - Repeats exactly $k$ iterations and returns the selected support set.\n   - Exact recovery for a given instance means that the returned support set equals the true support set.\n\n2. Concrete dictionaries. Use $m = 4$ and $n = 5$. Define the columns of $A$ and $B$ as follows, where $\\mathbf{e}_1,\\mathbf{e}_2,\\mathbf{e}_3,\\mathbf{e}_4$ are the standard basis vectors in $\\mathbb{R}^4$:\n   - For $A$:\n     - $a_1 = \\mathbf{e}_1 = [1,0,0,0]^\\top$,\n     - $a_2 = \\mathbf{e}_2 = [0,1,0,0]^\\top$,\n     - $a_3 = \\mathbf{e}_3 = [0,0,1,0]^\\top$,\n     - $a_4 = \\left[\\frac{1}{2}, -\\frac{1}{2}, 0, \\frac{1}{\\sqrt{2}}\\right]^\\top$,\n     - $a_5 = \\left[\\frac{1}{2}, -\\frac{1}{2}, 0, -\\frac{1}{\\sqrt{2}}\\right]^\\top$.\n   - For $B$:\n     - $b_1 = \\mathbf{e}_1 = [1,0,0,0]^\\top$,\n     - $b_2 = \\mathbf{e}_2 = [0,1,0,0]^\\top$,\n     - $b_3 = \\mathbf{e}_3 = [0,0,1,0]^\\top$,\n     - $b_4 = \\left[\\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}\\right]^\\top$,\n     - $b_5 = -\\mathbf{e}_4 = [0,0,0,-1]^\\top$.\n   All columns are unit-norm. The mutual coherence of both $A$ and $B$ is $\\mu = \\frac{1}{2}$, since the maximum absolute off-diagonal entry in the respective column-normalized Gram matrices equals $\\frac{1}{2}$.\n\n3. Sub-Gram matrices to compare spectrally. Let the target support set be $S = \\{1,2,3\\}$ and define $T = S \\cup \\{4\\} = \\{1,2,3,4\\}$. You must compute the minimum eigenvalue of $G_T(A)$ and $G_T(B)$. These capture how differently the collections of columns $A_T$ and $B_T$ are conditioned, despite $A$ and $B$ having identical mutual coherence.\n\n4. Test suite of sparse signals. For each test, create a coefficient vector $x \\in \\mathbb{R}^n$ and use $y = D x$ with the specified $D \\in \\{A,B\\}$. For all tests, the true support is $S = \\{1,2,3\\}$.\n   - Case $1$ (happy path versus failure mode): $k=3$, $x = [1, 1, 1, 0, 0]^\\top$.\n   - Case $2$ (boundary case from coherence-based guarantees): $k=1$, $x = [1, 0, 0, 0, 0]^\\top$.\n   - Case $3$ (small equal coefficients, aggregating correlation challenge): $k=3$, $x = [0.2, 0.2, 0.2, 0, 0]^\\top$.\n   For each case, run OMP for exactly $k$ iterations with $D=A$ and $D=B$, and report whether the recovered support equals $S$.\n\n5. Required computations and outputs.\n   - Compute $\\mu(A)$ and $\\mu(B)$ and verify whether they are identical within numerical tolerance $\\tau = 10^{-12}$, reported as a boolean per test case.\n   - Compute the minimum eigenvalues $\\lambda_{\\min}\\!\\left(G_T(A)\\right)$ and $\\lambda_{\\min}\\!\\left(G_T(B)\\right)$.\n   - For each case, run OMP as specified and report two booleans indicating exact support recovery using $A$ and using $B$.\n   - For each case, aggregate the results into a list:\n     - $\\left[\\text{mu\\_equal}, \\lambda_{\\min}\\!\\left(G_T(A)\\right), \\lambda_{\\min}\\!\\left(G_T(B)\\right), \\text{omp\\_success\\_A}, \\text{omp\\_success\\_B}\\right]$,\n     where $\\text{mu\\_equal}$, $\\text{omp\\_success\\_A}$, and $\\text{omp\\_success\\_B}$ are booleans, and the eigenvalues are floats rounded to $6$ decimal places.\n   - Final output format: Your program should produce a single line of output containing the list of the three per-case result lists, i.e., a single line that looks like $[\\text{case1\\_result},\\text{case2\\_result},\\text{case3\\_result}]$ with no extra whitespace or text. For example: $[[\\text{...}],[\\text{...}],[\\text{...}]]$.\n\nEnsure your implementation does not read input and uses deterministic linear algebra with a fixed tolerance for numerical comparisons. Implement Orthogonal Matching Pursuit (OMP) exactly as described above, stopping after exactly $k$ iterations in each case. All inner products and Gram matrices must be computed after column normalization when evaluating mutual coherence. Round floating-point eigenvalues to $6$ decimal places as specified.",
            "solution": "The problem requires the construction and analysis of two dictionaries, $A$ and $B$, to demonstrate how spectral properties of sub-Gram matrices, rather than just mutual coherence, influence the success of sparse recovery algorithms like Orthogonal Matching Pursuit (OMP). We will proceed by first defining the mathematical objects, then calculating the required metrics, and finally simulating OMP to verify the theoretical predictions.\n\nThe dictionaries $A, B \\in \\mathbb{R}^{m \\times n}$ are given for $m=4$ and $n=5$. Let $\\mathbf{e}_i$ be the $i$-th standard basis vector in $\\mathbb{R}^4$.\n\nThe columns of dictionary $A$ are:\n- $a_1 = [1, 0, 0, 0]^\\top$\n- $a_2 = [0, 1, 0, 0]^\\top$\n- $a_3 = [0, 0, 1, 0]^\\top$\n- $a_4 = \\left[\\frac{1}{2}, -\\frac{1}{2}, 0, \\frac{1}{\\sqrt{2}}\\right]^\\top$\n- $a_5 = \\left[\\frac{1}{2}, -\\frac{1}{2}, 0, -\\frac{1}{\\sqrt{2}}\\right]^\\top$\n\nThe columns of dictionary $B$ are:\n- $b_1 = [1, 0, 0, 0]^\\top$\n- $b_2 = [0, 1, 0, 0]^\\top$\n- $b_3 = [0, 0, 1, 0]^\\top$\n- $b_4 = \\left[\\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}\\right]^\\top$\n- $b_5 = [0, 0, 0, -1]^\\top$\n\nAll columns of both dictionaries are verified to have a Euclidean norm of $1$.\n\nThe mutual coherence $\\mu(D)$ of a dictionary $D$ with unit-norm columns $d_i$ is $\\mu(D) = \\max_{i \\neq j} |\\langle d_i, d_j \\rangle|$. This is the largest absolute off-diagonal element of the Gram matrix $G(D) = D^\\top D$.\n\nFor dictionary $A$, the Gram matrix $G(A) = A^\\top A$ is:\n$$ G(A) = \\begin{pmatrix}\n1  0  0  1/2  1/2 \\\\\n0  1  0  -1/2  -1/2 \\\\\n0  0  1  0  0 \\\\\n1/2  -1/2  0  1  0 \\\\\n1/2  -1/2  0  0  1\n\\end{pmatrix} $$\nThe maximum absolute off-diagonal value is $|\\pm 1/2| = 1/2$. Thus, $\\mu(A) = 1/2$.\n\nFor dictionary $B$, the Gram matrix $G(B) = B^\\top B$ is:\n$$ G(B) = \\begin{pmatrix}\n1  0  0  1/2  0 \\\\\n0  1  0  1/2  0 \\\\\n0  0  1  1/2  0 \\\\\n1/2  1/2  1/2  1  0 \\\\\n0  0  0  0  1\n\\end{pmatrix} $$\nThe maximum absolute off-diagonal value is $1/2$. Thus, $\\mu(B) = 1/2$.\nThe mutual coherences are identical: $\\mu(A) = \\mu(B) = 1/2$.\n\nNext, we analyze the spectral properties of sub-Gram matrices. The specified index set is $T = \\{1, 2, 3, 4\\}$, containing the true support $S = \\{1, 2, 3\\}$ and one distractor atom. The sub-Gram matrices are $G_T(A) = A_T^\\top A_T$ and $G_T(B) = B_T^\\top B_T$.\n\nThe submatrix $A_T$ consists of columns $a_1, a_2, a_3, a_4$. Its Gram matrix is:\n$$ G_T(A) = \\begin{pmatrix}\n1  0  0  1/2 \\\\\n0  1  0  -1/2 \\\\\n0  0  1  0 \\\\\n1/2  -1/2  0  1\n\\end{pmatrix} $$\nThe characteristic polynomial is $\\det(G_T(A) - \\lambda I) = (1-\\lambda)^2 ((1-\\lambda)^2 - 1/2)$. The eigenvalues are $\\lambda \\in \\{1, 1, 1 - 1/\\sqrt{2}, 1 + 1/\\sqrt{2}\\}$. The minimum eigenvalue is $\\lambda_{\\min}(G_T(A)) = 1 - 1/\\sqrt{2} \\approx 0.292893$.\n\nThe submatrix $B_T$ consists of columns $b_1, b_2, b_3, b_4$. Its Gram matrix is:\n$$ G_T(B) = \\begin{pmatrix}\n1  0  0  1/2 \\\\\n0  1  0  1/2 \\\\\n0  0  1  1/2 \\\\\n1/2  1/2  1/2  1\n\\end{pmatrix} $$\nThe characteristic polynomial is $\\det(G_T(B) - \\lambda I) = (1-\\lambda)^2 ((1-\\lambda)^2 - 3/4)$. The eigenvalues are $\\lambda \\in \\{1, 1, 1 - \\sqrt{3}/2, 1 + \\sqrt{3}/2\\}$. The minimum eigenvalue is $\\lambda_{\\min}(G_T(B)) = 1 - \\sqrt{3}/2 \\approx 0.133975$.\n\nThe minimum eigenvalue of a Gram matrix quantifies the linear independence of its columns. A smaller value indicates that the columns are \"closer\" to being linearly dependent. Here, $\\lambda_{\\min}(G_T(B))  \\lambda_{\\min}(G_T(A))$, suggesting that the set of atoms $\\{b_1, b_2, b_3, b_4\\}$ is more poorly conditioned than $\\{a_1, a_2, a_3, a_4\\}$. This is the key to explaining OMP's differing performance.\n\nOrthogonal Matching Pursuit (OMP) is an iterative greedy algorithm. For a given sparsity $k$, dictionary $D$, and measurement $y = Dx$, it operates as follows for $i=1, \\dots, k$:\n1. Initialization: Residual $r_0 = y$, support set $\\Lambda_0 = \\emptyset$.\n2. Atom selection: Find index $j_i = \\arg\\max_{j} |\\langle d_j, r_{i-1} \\rangle|$.\n3. Support update: $\\Lambda_i = \\Lambda_{i-1} \\cup \\{j_i\\}$.\n4. Signal update and residual calculation: Solve the least-squares problem $x_i = \\arg\\min_{z} \\|y - D_{\\Lambda_i} z\\|_2^2$. The new residual is $r_i = y - D_{\\Lambda_i} x_i$. This is equivalent to projecting $y$ onto the orthogonal complement of the subspace spanned by $D_{\\Lambda_i}$, i.e., $r_i = (I - P_{\\Lambda_i})y$ where $P_{\\Lambda_i}$ is the projection matrix.\nThe final recovered support is $\\Lambda_k$.\n\nWe analyze the test cases using this procedure. Note that indices are 0-based in implementation, so $S=\\{0,1,2\\}$.\n\nCase 1: $k=3$, $x=[1, 1, 1, 0, 0]^\\top$. True support is $S=\\{0,1,2\\}$.\n- For dictionary $A$: $y_A = A x = a_1 + a_2 + a_3 = [1, 1, 1, 0]^\\top$. OMP starts with residual $r_0 = y_A$. The correlations are $|\\langle a_1, r_0 \\rangle|=1$, $|\\langle a_2, r_0 \\rangle|=1$, $|\\langle a_3, r_0 \\rangle|=1$, and others are $0$. Assuming tie-breaking picks the smallest index, OMP will select atoms $0, 1, 2$ in sequence. The recovered support is $\\{0, 1, 2\\}$, which equals $S$. Thus, recovery is successful.\n- For dictionary $B$: $y_B = B x = b_1 + b_2 + b_3 = [1, 1, 1, 0]^\\top$. The initial residual is $r_0=y_B$. The correlations are $|\\langle b_1, r_0 \\rangle|=1$, $|\\langle b_2, r_0 \\rangle|=1$, $|\\langle b_3, r_0 \\rangle|=1$, but $|\\langle b_4, r_0 \\rangle| = |\\langle b_4, b_1+b_2+b_3 \\rangle| = |\\frac{1}{2}+\\frac{1}{2}+\\frac{1}{2}|=1.5$. OMP incorrectly selects atom $3$ (corresponding to $b_4$) in the first step because it has the highest correlation. The final support cannot be $S$. Recovery fails. This failure is a direct consequence of the poor conditioning of $B_T$ as indicated by its smaller $\\lambda_{\\min}$. Atom $b_4$ is highly coherent with the linear combination of the true atoms $b_1+b_2+b_3$.\n\nCase 2: $k=1$, $x=[1, 0, 0, 0, 0]^\\top$. The true support is $\\{0\\}$.\nThe problem specifies that the recovered support should be compared against $S=\\{0,1,2\\}$.\n- For dictionary $A$: $y_A = a_1 = [1, 0, 0, 0]^\\top$. For $k=1$, OMP selects the atom with the highest correlation with $y_A$. This is clearly $a_1$ (index $0$), with correlation $1$. The recovered support is $\\{0\\}$. Since $\\{0\\} \\neq \\{0, 1, 2\\}$, the test is considered a failure.\n- For dictionary $B$: $y_B = b_1 = [1, 0, 0, 0]^\\top$. Similarly, OMP selects atom $b_1$ (index $0$). The recovered support is $\\{0\\}$, which is not equal to $S=\\{0, 1, 2\\}$. The test fails.\nIn both sub-cases, OMP correctly identifies the true $1$-sparse support, but fails the test due to the specified comparison against a $3$-element set.\n\nCase 3: $k=3$, $x=[0.2, 0.2, 0.2, 0, 0]^\\top$. True support is $S=\\{0,1,2\\}$.\nThis case is a scaled version of Case 1. The measurement vector $y$ is simply $0.2$ times the vector from Case 1. The OMP selection criterion $\\arg\\max_{j} |\\langle d_j, r_{i-1} \\rangle|$ is invariant to a positive scaling of the measurement vector (and thus all subsequent residuals). Therefore, the sequence of selected atoms will be identical to Case 1 for both dictionaries.\n- For dictionary $A$: Recovery is successful, yielding support $\\{0, 1, 2\\}$.\n- For dictionary $B$: Recovery fails, as atom $3$ is selected first.\n\nThe results confirm that even with identical mutual coherence, the distribution of correlations among subsets of atoms, captured by the spectral properties of sub-Gram matrices, is a more decisive factor for sparse recovery.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the compressed sensing problem by defining two dictionaries,\n    calculating their properties, and running Orthogonal Matching Pursuit (OMP).\n    \"\"\"\n\n    # 1. Define Dictionaries A and B\n    sqrt2 = np.sqrt(2.0)\n    A = np.array([\n        [1.0, 0.0, 0.0, 0.5, 0.5],\n        [0.0, 1.0, 0.0, -0.5, -0.5],\n        [0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0 / sqrt2, -1.0 / sqrt2]\n    ])\n\n    B = np.array([\n        [1.0, 0.0, 0.0, 0.5, 0.0],\n        [0.0, 1.0, 0.0, 0.5, 0.0],\n        [0.0, 0.0, 1.0, 0.5, 0.0],\n        [0.0, 0.0, 0.0, 0.5, -1.0]\n    ])\n\n    def get_mutual_coherence(D):\n        \"\"\"Computes the mutual coherence of a dictionary.\"\"\"\n        # Normalize columns to be safe, as per problem description\n        D_norm = D / np.linalg.norm(D, axis=0)\n        gram_matrix = D_norm.T @ D_norm\n        np.fill_diagonal(gram_matrix, 0)\n        return np.max(np.abs(gram_matrix))\n\n    def get_min_eigenvalue_subgram(D, T_indices):\n        \"\"\"Computes the minimum eigenvalue of a sub-Gram matrix.\"\"\"\n        D_T = D[:, T_indices]\n        G_T = D_T.T @ D_T\n        # use eigvalsh for Hermitian matrices\n        eigenvalues = np.linalg.eigvalsh(G_T)\n        return np.min(eigenvalues)\n\n    def omp(D, y, k):\n        \"\"\"\n        Orthogonal Matching Pursuit algorithm.\n        Stops after exactly k iterations.\n        \"\"\"\n        num_atoms = D.shape[1]\n        residual = np.copy(y)\n        support = []\n        \n        for _ in range(k):\n            correlations = np.abs(D.T @ residual)\n            # Mask already selected atoms\n            correlations[support] = -1\n            \n            # Find the best atom (smallest index in case of a tie)\n            best_atom_idx = np.argmax(correlations)\n            support.append(best_atom_idx)\n\n            # Update residual using projection\n            D_support = D[:, support]\n            \n            # Solve least squares: x = (D_S^T D_S)^-1 D_S^T y\n            # Using pseudoinverse for stability, although solve would also work\n            # as submatrices are full rank here.\n            x_support = np.linalg.pinv(D_support) @ y\n            \n            # Update residual\n            residual = y - D_support @ x_support\n            \n        return set(support)\n\n    # 2. Define Test Cases and Fixed Parameters\n    T_indices = [0, 1, 2, 3] # Indices for sub-Gram matrix, 1-based {1,2,3,4}\n    S_target = {0, 1, 2}     # Target support set, 1-based {1,2,3}\n    \n    test_cases = [\n        # k, x_coeffs\n        (3, np.array([1.0, 1.0, 1.0, 0.0, 0.0])),\n        (1, np.array([1.0, 0.0, 0.0, 0.0, 0.0])),\n        (3, np.array([0.2, 0.2, 0.2, 0.0, 0.0])),\n    ]\n\n    # Pre-compute dictionary properties as they are the same for all cases\n    mu_A = get_mutual_coherence(A)\n    mu_B = get_mutual_coherence(B)\n    mu_equal = abs(mu_A - mu_B)  1e-12\n\n    lambda_min_A = get_min_eigenvalue_subgram(A, T_indices)\n    lambda_min_B = get_min_eigenvalue_subgram(B, T_indices)\n\n    results = []\n    for k, x in test_cases:\n        # Generate measurement vectors\n        y_A = A @ x\n        y_B = B @ x\n\n        # Run OMP\n        recovered_support_A = omp(A, y_A, k)\n        recovered_support_B = omp(B, y_B, k)\n\n        # Check for exact recovery against the specified target set S\n        omp_success_A = (recovered_support_A == S_target)\n        omp_success_B = (recovered_support_B == S_target)\n\n        # Aggregate results for the case\n        case_result = [\n            mu_equal,\n            round(lambda_min_A, 6),\n            round(lambda_min_B, 6),\n            omp_success_A,\n            omp_success_B\n        ]\n        results.append(case_result)\n        \n    # Final print statement in the exact required format.\n    # The string representation of list of lists is \"[...], [...]\"\n    # We join these with commas and wrap in an outer \"[]\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}