{
    "hands_on_practices": [
        {
            "introduction": "The ability to uniquely identify a sparse signal begins with the fundamental properties of the sensing matrix $A$. This first practice explores the most intuitive failure of identifiability, which occurs when the columns of $A$—the atoms in our dictionary—are not distinct enough. By examining the simple case of a $1$-sparse signal , you will connect the abstract concept of identifiability to the concrete geometric property of collinearity and see how this leads to a formal condition for uniqueness.",
            "id": "3492076",
            "problem": "Consider the setting of Compressed Sensing (CS), where one seeks a $k$-sparse vector $x \\in \\mathbb{R}^{n}$ that satisfies a measurement equation $y = A x$ for a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$. A $k$-sparse vector has at most $k$ nonzero components. Starting from the definition of the spark of a matrix and the definitions of linear dependence and $k$-sparsity, derive a condition under which a $1$-sparse representation of $y$ is unique. Then, address the scenario in which the columns of $A$ contain nontrivial duplicates or are collinear (i.e., scalar multiples of one another), explaining why a single measurement $y$ cannot uniquely identify which column generated $y$ when $k=1$. Propose a principled, sign-invariant column-clustering criterion based on unit-norm direction that detects this non-identifiability for $k=1$, and use it to determine the size of the ambiguous support set for a specific instance.\n\nLet the sensing matrix be\n$$\nA = \\begin{bmatrix}\n1  2  3  0  0  1  -2  0 \\\\\n0  0  0  1  2  1  -2  0 \\\\\n0  0  0  0  0  0  0  1\n\\end{bmatrix},\n$$\nso that the columns are\n$$\na_{1} = \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix},\\quad\na_{2} = \\begin{bmatrix}2 \\\\ 0 \\\\ 0\\end{bmatrix},\\quad\na_{3} = \\begin{bmatrix}3 \\\\ 0 \\\\ 0\\end{bmatrix},\\quad\na_{4} = \\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix},\\quad\na_{5} = \\begin{bmatrix}0 \\\\ 2 \\\\ 0\\end{bmatrix},\\quad\na_{6} = \\begin{bmatrix}1 \\\\ 1 \\\\ 0\\end{bmatrix},\\quad\na_{7} = \\begin{bmatrix}-2 \\\\ -2 \\\\ 0\\end{bmatrix},\\quad\na_{8} = \\begin{bmatrix}0 \\\\ 0 \\\\ 1\\end{bmatrix}.\n$$\nYou observe a single measurement\n$$\ny = \\begin{bmatrix}9 \\\\ 0 \\\\ 0\\end{bmatrix}.\n$$\nDefine the ambiguous support set for $k=1$ as\n$$\nS(y) \\triangleq \\left\\{ j \\in \\{1,\\dots,n\\} \\,\\big|\\, \\exists\\, \\alpha \\in \\mathbb{R}\\setminus\\{0\\} \\text{ such that } y = \\alpha\\, a_{j} \\right\\}.\n$$\nUsing a sign-invariant unit-direction clustering rule that groups columns $a_{j}$ and $a_{\\ell}$ into the same cluster if and only if $\\left| \\frac{a_j^T a_\\ell}{\\|a_j\\| \\|a_\\ell\\|} \\right| = 1$, determine the cardinality $\\left|S(y)\\right|$ for the given $A$ and $y$. Provide your final answer as a single integer. No rounding is required.",
            "solution": "The problem asks for a derivation of uniqueness conditions for $1$-sparse solutions, an explanation of non-identifiability, and a specific calculation of the size of an ambiguous support set.\n\nFirst, we address the condition for a $1$-sparse representation to be unique. A vector $x \\in \\mathbb{R}^n$ is $k$-sparse if it has at most $k$ non-zero components. The measurement equation is $y = Ax$, where $A \\in \\mathbb{R}^{m \\times n}$ is the sensing matrix and $y \\in \\mathbb{R}^m$ is the measurement vector. For a $1$-sparse solution with $k=1$, the vector $x$ has only one non-zero component, say $x_j \\neq 0$. We can write $x$ as $x = x_j e_j$, where $e_j$ is the $j$-th standard basis vector in $\\mathbb{R}^n$. The measurement equation becomes $y = A(x_j e_j) = x_j (A e_j) = x_j a_j$, where $a_j$ is the $j$-th column of $A$. This means that a $1$-sparse solution exists if and only if the measurement vector $y$ is a scalar multiple of one of the columns of $A$.\n\nFor this solution to be unique, we must ensure that if $y$ can be represented as a multiple of one column, it cannot be represented as a multiple of a different column. Suppose there exist two distinct $1$-sparse solutions, $x$ and $x'$, that produce the same measurement $y$. Let the support of $x$ be $\\{j\\}$ and the support of $x'$ be $\\{l\\}$, with $j \\neq l$. Then we have:\n$$y = x_j a_j$$\n$$y = x'_l a_l$$\nwhere $x_j \\neq 0$ and $x'_l \\neq 0$. Equating these expressions gives $x_j a_j = x'_l a_l$, which can be rearranged to:\n$$x_j a_j - x'_l a_l = \\mathbf{0}$$\nSince $x_j$ and $x'_l$ are non-zero, this equation is a non-trivial linear combination of the columns $a_j$ and $a_l$ that equals the zero vector. This implies that the set of columns $\\{a_j, a_l\\}$ is linearly dependent.\n\nTo guarantee the uniqueness of any $1$-sparse solution, we must require that no two distinct columns of $A$ are linearly dependent. In other words, for any distinct pair of indices $j, l \\in \\{1, \\dots, n\\}$, the set $\\{a_j, a_l\\}$ must be linearly independent. This condition is equivalent to stating that the spark of the matrix $A$, denoted $\\operatorname{spark}(A)$, must be greater than $2$. The spark of a matrix is defined as the smallest number of columns that are linearly dependent. The general condition for the uniqueness of a $k$-sparse solution is $\\operatorname{spark}(A) > 2k$. For $k=1$, this specializes to $\\operatorname{spark}(A) > 2$.\n\nNext, we address the scenario where columns of $A$ are collinear. If two columns, say $a_i$ and $a_j$ with $i \\neq j$, are collinear, it signifies that one is a scalar multiple of the other. Let's assume no column is the zero vector, so there exists a non-zero scalar $c$ such that $a_i = c a_j$. If a measurement $y$ is generated by a $1$-sparse vector $x$ with its single non-zero element at index $i$, then $y = x_i a_i$. Substituting the collinearity relation, we find $y = x_i(c a_j) = (c x_i) a_j$. Let $\\alpha = c x_i$. Since $c \\neq 0$ and $x_i \\neq 0$, we have $\\alpha \\neq 0$. The expression $y = \\alpha a_j$ corresponds to a different $1$-sparse vector $x'$ whose only non-zero component is $x'_j = \\alpha$ at index $j$. Because both the $1$-sparse vector $x$ (with support $\\{i\\}$) and the $1$-sparse vector $x'$ (with support $\\{j\\}$) produce the exact same measurement $y$, it is impossible to uniquely determine which column generated the observation. This is the source of non-identifiability for $k=1$.\n\nThe problem proposes a column-clustering criterion based on unit-norm direction, which formalizes this concept. Two columns $a_j$ and $a_\\ell$ are grouped into the same cluster if and only if $\\left| \\frac{a_j^T a_\\ell}{\\|a_j\\| \\|a_\\ell\\|} \\right| = 1$. Let $\\hat{a}_j = a_j/\\|a_j\\|$ be the unit vector in the direction of $a_j$. The condition is $|\\hat{a}_j^\\top \\hat{a}_\\ell| = 1$. From the definition of the dot product, $\\hat{a}_j^\\top \\hat{a}_\\ell = \\|\\hat{a}_j\\| \\|\\hat{a}_\\ell\\| \\cos\\theta_{j\\ell}$, where $\\theta_{j\\ell}$ is the angle between the vectors. Since $\\|\\hat{a}_j\\| = 1$ and $\\|\\hat{a}_\\ell\\| = 1$, the condition simplifies to $|\\cos\\theta_{j\\ell}| = 1$. This holds if and only if $\\theta_{j\\ell} = 0$ or $\\theta_{j\\ell} = \\pi$, which means the vectors $a_j$ and $a_\\ell$ are collinear (pointing in the same or opposite directions). This criterion is an equivalence relation that partitions the columns of $A$ into disjoint sets, where each set contains mutually collinear vectors.\n\nFinally, we must determine the cardinality of the ambiguous support set $S(y)$ for the given matrix $A$ and measurement $y$.\nThe matrix is $A = \\begin{bmatrix} 1  2  3  0  0  1  -2  0 \\\\ 0  0  0  1  2  1  -2  0 \\\\ 0  0  0  0  0  0  0  1 \\end{bmatrix}$ and the measurement is $y = \\begin{bmatrix} 9 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\nThe columns of $A$ are:\n$a_{1} = \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix}, a_{2} = \\begin{bmatrix}2 \\\\ 0 \\\\ 0\\end{bmatrix}, a_{3} = \\begin{bmatrix}3 \\\\ 0 \\\\ 0\\end{bmatrix}, a_{4} = \\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix}, a_{5} = \\begin{bmatrix}0 \\\\ 2 \\\\ 0\\end{bmatrix}, a_{6} = \\begin{bmatrix}1 \\\\ 1 \\\\ 0\\end{bmatrix}, a_{7} = \\begin{bmatrix}-2 \\\\ -2 \\\\ 0\\end{bmatrix}, a_{8} = \\begin{bmatrix}0 \\\\ 0 \\\\ 1\\end{bmatrix}$.\n\nThe ambiguous support set is defined as $S(y) \\triangleq \\left\\{ j \\in \\{1,\\dots,n\\} \\,\\big|\\, \\exists\\, \\alpha \\in \\mathbb{R}\\setminus\\{0\\} \\text{ such that } y = \\alpha\\, a_{j} \\right\\}$.\nWe need to find all indices $j$ such that the column $a_j$ is collinear with the vector $y$. We test each column:\n\\begin{itemize}\n    \\item $j=1$: Is $y = \\alpha a_1$? $\\begin{bmatrix} 9 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\alpha \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$. This holds for $\\alpha = 9$. Thus, $1 \\in S(y)$.\n    \\item $j=2$: Is $y = \\alpha a_2$? $\\begin{bmatrix} 9 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\alpha \\begin{bmatrix} 2 \\\\ 0 \\\\ 0 \\end{bmatrix}$. This holds for $\\alpha = \\frac{9}{2}$. Thus, $2 \\in S(y)$.\n    \\item $j=3$: Is $y = \\alpha a_3$? $\\begin{bmatrix} 9 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\alpha \\begin{bmatrix} 3 \\\\ 0 \\\\ 0 \\end{bmatrix}$. This holds for $\\alpha = 3$. Thus, $3 \\in S(y)$.\n    \\item $j=4$: Is $y = \\alpha a_4$? $\\begin{bmatrix} 9 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\alpha \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$. The first component gives $9 = 0$, which is false. Thus, $4 \\notin S(y)$.\n    \\item $j=5$: Is $y = \\alpha a_5$? $\\begin{bmatrix} 9 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\alpha \\begin{bmatrix} 0 \\\\ 2 \\\\ 0 \\end{bmatrix}$. The first component gives $9 = 0$, which is false. Thus, $5 \\notin S(y)$.\n    \\item $j=6$: Is $y = \\alpha a_6$? $\\begin{bmatrix} 9 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\alpha \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}$. This would require $9 = \\alpha$ and $0 = \\alpha$, a contradiction. Thus, $6 \\notin S(y)$.\n    \\item $j=7$: Is $y = \\alpha a_7$? $\\begin{bmatrix} 9 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\alpha \\begin{bmatrix} -2 \\\\ -2 \\\\ 0 \\end{bmatrix}$. This would require $9 = -2\\alpha$ and $0 = -2\\alpha$, which implies $\\alpha=0$, contradicting $\\alpha\\neq 0$. Thus, $7 \\notin S(y)$.\n    \\item $j=8$: Is $y = \\alpha a_8$? $\\begin{bmatrix} 9 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\alpha \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$. The first component gives $9 = 0$, which is false. Thus, $8 \\notin S(y)$.\n\\end{itemize}\nThe set of indices is $S(y) = \\{1, 2, 3\\}$. The cardinality of this set is $|S(y)| = 3$. This result aligns with the clustering criterion, as the vector $y$ lies in the direction defined by the cluster $\\{a_1, a_2, a_3\\}$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "Building on the intuition from the $1$-sparse case, this exercise generalizes the problem of non-uniqueness to signals with sparsity $k > 1$. The key lies in understanding that linear dependence among any set of columns in the sensing matrix can create ambiguity, a property captured by the matrix's spark. You will work through a concrete example  to construct two distinct $2$-sparse vectors that yield identical measurements, thereby demonstrating a direct violation of the fundamental identifiability condition $\\operatorname{spark}(A) > 2k$.",
            "id": "3492120",
            "problem": "Let $A \\in \\mathbb{R}^{2 \\times 3}$ and $y \\in \\mathbb{R}^{2}$ be given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix},\n\\qquad\ny \\;=\\; \\begin{pmatrix}\n1 \\\\ 1\n\\end{pmatrix},\n$$\nand let $k = 2$. Using only the fundamental definitions of $k$-sparsity (the number of nonzero entries of a vector is at most $k$), the spark of a matrix (the smallest number of columns of $A$ that are linearly dependent), and elementary linear algebra, do the following:\n\n1. Exhibit $2$ distinct vectors $x^{(1)}, x^{(2)} \\in \\mathbb{R}^{3}$ with $\\|x^{(i)}\\|_{0} \\le k$ such that $A x^{(1)} = A x^{(2)} = y$.\n\n2. Starting from the definitions, explain why the existence of such $x^{(1)} \\neq x^{(2)}$ demonstrates a failure of a standard identifiability condition for unique $k$-sparse recovery, and state that condition precisely in terms of the $\\operatorname{spark}$ of $A$.\n\n3. Compute the $\\operatorname{spark}$ of $A$.\n\nProvide as your final answer the value of $\\operatorname{spark}(A)$ as a single integer. No rounding is required.",
            "solution": "The problem asks for three tasks related to the uniqueness of sparse solutions to a linear system. We will address each part in sequence using the provided definitions and elementary linear algebra.\n\nThe given linear system is $Ax = y$, where\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix} \\in \\mathbb{R}^{2 \\times 3},\n\\qquad\ny \\;=\\; \\begin{pmatrix}\n1 \\\\ 1\n\\end{pmatrix} \\in \\mathbb{R}^{2},\n\\qquad\nx \\;=\\; \\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} \\in \\mathbb{R}^{3}.\n$$\nThe system of equations is:\n$$\nx_1 + x_3 = 1 \\\\\nx_2 + x_3 = 1\n$$\nWe are also given the sparsity level $k=2$.\n\n**1. Exhibit two distinct $k$-sparse vectors**\n\nWe need to find two distinct vectors, $x^{(1)}$ and $x^{(2)}$, such that they are solutions to $Ax=y$ and their $\\ell_0$-norm (number of non-zero entries) is at most $k=2$.\nFrom the system of equations, we can express $x_1$ and $x_2$ in terms of $x_3$:\n$$\nx_1 = 1 - x_3 \\\\\nx_2 = 1 - x_3\n$$\nLet's search for sparse solutions by setting one component to $0$.\n\nCase 1: Let $x_3 = 0$.\nThe equations yield $x_1 = 1 - 0 = 1$ and $x_2 = 1 - 0 = 1$.\nThis gives a solution vector $x^{(1)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nLet's verify this solution: $A x^{(1)} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 1 + 1 \\cdot 0 \\\\ 0 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = y$.\nThe number of non-zero entries is $\\|x^{(1)}\\|_0 = 2$. Since $2 \\le k=2$, this vector is $2$-sparse.\n\nCase 2: Let $x_1 = 0$.\nThe first equation gives $0 + x_3 = 1$, so $x_3 = 1$.\nThe second equation then gives $x_2 = 1 - x_3 = 1 - 1 = 0$.\nThis gives a solution vector $x^{(2)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\nLet's verify this solution: $A x^{(2)} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 0 + 0 \\cdot 0 + 1 \\cdot 1 \\\\ 0 \\cdot 0 + 1 \\cdot 0 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = y$.\nThe number of non-zero entries is $\\|x^{(2)}\\|_0 = 1$. Since $1 \\le k=2$, this vector is also $2$-sparse (in fact, it is $1$-sparse).\n\nWe have found two distinct vectors, $x^{(1)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $x^{(2)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$, both of which are $2$-sparse solutions to $Ax = y$.\n\n**2. Explanation of identifiability failure**\n\nThe existence of two distinct $k$-sparse solutions $x^{(1)}$ and $x^{(2)}$ for the same measurement vector $y$ implies that the sparse solution is not uniquely identifiable from the measurements. This is a failure of unique sparse recovery.\n\nLet's analyze the difference vector $z = x^{(1)} - x^{(2)}$:\n$$\nz = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix}.\n$$\nThis vector $z$ is non-zero. Let's apply the matrix $A$ to $z$:\n$$\nA z = A(x^{(1)} - x^{(2)}) = A x^{(1)} - A x^{(2)} = y - y = 0.\n$$\nThis shows that $z$ is a non-zero vector in the null space of $A$, i.e., $z \\in \\ker(A) \\setminus \\{0\\}$. The sparsity of this vector is $\\|z\\|_0 = 3$.\n\nThe fundamental definition of the spark of a matrix $A$, denoted $\\operatorname{spark}(A)$, is the smallest number of columns of $A$ that are linearly dependent. An equivalent definition is that $\\operatorname{spark}(A)$ is the minimum possible value of $\\|v\\|_0$ for any non-zero vector $v \\in \\ker(A)$.\nSince we found a vector $z \\in \\ker(A)$ with $\\|z\\|_0=3$, it must be that $\\operatorname{spark}(A) \\le 3$.\n\nA standard identifiability condition for sparse recovery states that for a given sparsity level $k$, any system $Ax=y$ has at most one $k$-sparse solution if and only if\n$$\n\\operatorname{spark}(A) > 2k.\n$$\nThe reasoning is as follows: if there were two distinct $k$-sparse solutions $x^{(1)}$ and $x^{(2)}$, their difference $z=x^{(1)}-x^{(2)}$ would be a non-zero vector in $\\ker(A)$. By the triangle inequality for the $\\ell_0$ pseudo-norm, its sparsity would be bounded by $\\|z\\|_0 \\le \\|x^{(1)}\\|_0 + \\|x^{(2)}\\|_0 \\le k+k = 2k$. This would imply $\\operatorname{spark}(A) \\le 2k$. Therefore, to guarantee uniqueness, we must have $\\operatorname{spark}(A) > 2k$.\n\nIn our problem, $k=2$. The condition for unique recovery would be $\\operatorname{spark}(A) > 2(2) = 4$.\nHowever, our demonstration in part 1 produced two distinct $2$-sparse solutions. Their difference $z$ is a vector in $\\ker(A)$ with $\\|z\\|_0 = 3$. This directly shows that $\\operatorname{spark}(A) \\le 3$. Since $3 \\ngtr 4$, the identifiability condition is violated, which explains why a unique $2$-sparse solution is not guaranteed and, in this case, does not exist.\n\n**3. Computation of the spark of A**\n\nWe compute $\\operatorname{spark}(A)$ by finding the size of the smallest linearly dependent set of columns of $A$. Let the columns of $A$ be $a_1, a_2, a_3$:\n$$\na_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad a_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad a_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nWe check for linear dependence in subsets of columns of increasing size.\n\n-   **Subsets of size 1**: A single column is linearly dependent if and only if it is the zero vector. None of $a_1, a_2, a_3$ is the zero vector. Therefore, $\\operatorname{spark}(A) > 1$.\n\n-   **Subsets of size 2**: We check all pairs of columns.\n    -   $\\{a_1, a_2\\}$: These are the columns of the $2 \\times 2$ identity matrix, which are linearly independent.\n    -   $\\{a_1, a_3\\}$: The matrix $\\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}$ has determinant $1 \\neq 0$, so these columns are linearly independent.\n    -   $\\{a_2, a_3\\}$: The matrix $\\begin{pmatrix} 0  1 \\\\ 1  1 \\end{pmatrix}$ has determinant $-1 \\neq 0$, so these columns are linearly independent.\n    Since every pair of columns is linearly independent, $\\operatorname{spark}(A) > 2$.\n\n-   **Subset of size 3**: We check the set $\\{a_1, a_2, a_3\\}$. We are looking for scalars $c_1, c_2, c_3$, not all zero, such that $c_1 a_1 + c_2 a_2 + c_3 a_3 = 0$.\n    $$\n    c_1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + c_2 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + c_3 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n    $$\n    This vector equation corresponds to the linear system:\n    $$\n    c_1 + c_3 = 0 \\\\\n    c_2 + c_3 = 0\n    $$\n    This gives $c_1 = -c_3$ and $c_2 = -c_3$. We can choose a non-zero value for $c_3$, for instance $c_3 = -1$. This yields $c_1 = 1$ and $c_2 = 1$. A non-trivial solution is $(c_1, c_2, c_3) = (1, 1, -1)$.\n    Indeed, $1 \\cdot a_1 + 1 \\cdot a_2 - 1 \\cdot a_3 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n    Since the three columns of $A$ are linearly dependent, and no smaller subset of columns is, the smallest number of linearly dependent columns is $3$.\n\nBy definition, the spark of $A$ is $3$.\n$$\n\\operatorname{spark}(A) = 3.\n$$\nThis result is consistent with our findings in part 2, where we established that $\\operatorname{spark}(A) \\le 3$.",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "While matrix properties like the spark provide global guarantees on uniqueness, we can also analyze a specific solution from a local, optimization-centric viewpoint. This practice shifts focus to the Karush-Kuhn-Tucker (KKT) conditions, the fundamental certificate of optimality in constrained convex problems. By explicitly constructing a dual solution for a candidate sparse vector , you will not only verify its optimality for the $\\ell_1$-minimization problem but also use the properties of this dual vector to rigorously determine if the solution is unique.",
            "id": "3492080",
            "problem": "Consider the equality-constrained convex optimization problem of finding the sparsest feasible vector in the sense of the $\\ell_{1}$ norm: minimize $\\,\\|\\mathbf{x}\\|_{1}\\,$ subject to $\\,\\mathbf{A}\\mathbf{x}=\\mathbf{y}\\,$, where the data are\n$$\n\\mathbf{A}=\\begin{pmatrix}\n1  0  1 \\\\\n0  1  \\tfrac{1}{2}\n\\end{pmatrix},\\quad\n\\mathbf{y}=\\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}.\n$$\nSuppose the candidate sparse solution is\n$$\n\\hat{\\mathbf{x}}=\\begin{pmatrix}\n1 \\\\\n-1 \\\\\n0\n\\end{pmatrix},\n$$\nwith support $S=\\{1,2\\}$ and complement $S^{c}=\\{3\\}$. Using only first principles for convex optimization (definitions of subgradients, feasibility, and the Karush–Kuhn–Tucker conditions for equality-constrained convex problems), explicitly verify the optimality conditions for $\\hat{\\mathbf{x}}$ and determine whether the $\\ell_{1}$ solution is unique for the given instance $(\\mathbf{A},\\mathbf{y},S)$. Your reasoning should be based on fundamental definitions and should not rely on any pre-stated uniqueness theorems. Express the final answer as the optimal primal solution vector $\\hat{\\mathbf{x}}$ written as a $3\\times 1$ column vector. No rounding is required, and no physical units apply.",
            "solution": "The problem is to solve the convex optimization problem:\n$$\n\\text{minimize} \\quad \\|\\mathbf{x}\\|_1 \\quad \\text{subject to} \\quad \\mathbf{A}\\mathbf{x} = \\mathbf{y}\n$$\nwhere $\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^3 |x_i|$, and the given data are:\n$$\n\\mathbf{A} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  \\frac{1}{2} \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\nA candidate solution is provided:\n$$\n\\hat{\\mathbf{x}} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\nThe task is to verify if $\\hat{\\mathbf{x}}$ is the optimal solution and whether it is unique, using first principles.\n\n### Part 1: Verify Optimality of $\\hat{\\mathbf{x}}$\n\nWe use the Karush–Kuhn–Tucker (KKT) conditions for an equality-constrained convex problem. For a point $\\hat{\\mathbf{x}}$ to be optimal, two conditions must be met:\n1.  **Primal Feasibility**: $\\hat{\\mathbf{x}}$ must satisfy the constraints.\n2.  **Stationarity**: There must exist a dual variable vector $\\hat{\\mathbf{\\nu}}$ such that the gradient (or a subgradient) of the Lagrangian with respect to $\\mathbf{x}$ is zero at $\\hat{\\mathbf{x}}$.\n\n**1. Primal Feasibility:**\nWe check if $\\mathbf{A}\\hat{\\mathbf{x}} = \\mathbf{y}$:\n$$\n\\mathbf{A}\\hat{\\mathbf{x}} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (0)(-1) + (1)(0) \\\\ (0)(1) + (1)(-1) + (\\frac{1}{2})(0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\nSince this equals $\\mathbf{y}$, the candidate solution $\\hat{\\mathbf{x}}$ is feasible.\n\n**2. Stationarity:**\nThe Lagrangian is $L(\\mathbf{x}, \\mathbf{\\nu}) = \\|\\mathbf{x}\\|_1 + \\mathbf{\\nu}^T(\\mathbf{A}\\mathbf{x} - \\mathbf{y})$. The stationarity condition states that $0$ must be in the subgradient of the Lagrangian with respect to $\\mathbf{x}$ at the optimal point.\n$$\n\\mathbf{0} \\in \\partial_x L(\\hat{\\mathbf{x}}, \\hat{\\mathbf{\\nu}}) = \\partial \\|\\hat{\\mathbf{x}}\\|_1 + \\mathbf{A}^T\\hat{\\mathbf{\\nu}}\n$$\nThis is equivalent to finding a dual variable $\\hat{\\mathbf{\\nu}} \\in \\mathbb{R}^2$ such that $-\\mathbf{A}^T\\hat{\\mathbf{\\nu}} \\in \\partial \\|\\hat{\\mathbf{x}}\\|_1$.\n\nThe subgradient of the $\\ell_1$-norm, $f(\\mathbf{x})=\\|\\mathbf{x}\\|_1$, is the set of vectors $\\mathbf{g}$ with components:\n$$\ng_i = \\begin{cases} \\text{sgn}(x_i)  \\text{if } x_i \\neq 0 \\\\ c  \\text{if } x_i = 0, \\text{ where } c \\in [-1, 1] \\end{cases}\n$$\nFor the candidate solution $\\hat{\\mathbf{x}} = (1, -1, 0)^T$:\n- $\\hat{x}_1 = 1 \\neq 0$, so the first component must be $\\text{sgn}(1) = 1$.\n- $\\hat{x}_2 = -1 \\neq 0$, so the second component must be $\\text{sgn}(-1) = -1$.\n- $\\hat{x}_3 = 0$, so the third component can be any value in $[-1, 1]$.\nThus, the subgradient at $\\hat{\\mathbf{x}}$ is the set:\n$$\n\\partial \\|\\hat{\\mathbf{x}}\\|_1 = \\left\\{ \\begin{pmatrix} 1 \\\\ -1 \\\\ z \\end{pmatrix} : z \\in [-1, 1] \\right\\}\n$$\nNow, we must find a $\\hat{\\mathbf{\\nu}} = (\\hat{\\nu}_1, \\hat{\\nu}_2)^T$ that satisfies the stationarity condition. We compute $-\\mathbf{A}^T\\hat{\\mathbf{\\nu}}$:\n$$\n-\\mathbf{A}^T\\hat{\\mathbf{\\nu}} = - \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\hat{\\nu}_1 \\\\ \\hat{\\nu}_2 \\end{pmatrix} = \\begin{pmatrix} -\\hat{\\nu}_1 \\\\ -\\hat{\\nu}_2 \\\\ -\\hat{\\nu}_1 - \\frac{1}{2}\\hat{\\nu}_2 \\end{pmatrix}\n$$\nFor this vector to be in $\\partial \\|\\hat{\\mathbf{x}}\\|_1$, its components must match the required structure:\n1.  $-\\hat{\\nu}_1 = 1 \\implies \\hat{\\nu}_1 = -1$.\n2.  $-\\hat{\\nu}_2 = -1 \\implies \\hat{\\nu}_2 = 1$.\n3.  $-\\hat{\\nu}_1 - \\frac{1}{2}\\hat{\\nu}_2 \\in [-1, 1]$.\n\nSubstituting the values for $\\hat{\\nu}_1$ and $\\hat{\\nu}_2$ into the third condition:\n$$\n-(-1) - \\frac{1}{2}(1) = 1 - \\frac{1}{2} = \\frac{1}{2}\n$$\nSince $\\frac{1}{2}$ is indeed in the interval $[-1, 1]$, the condition is satisfied.\nWe have found a valid dual variable $\\hat{\\mathbf{\\nu}} = (-1, 1)^T$. Since both primal feasibility and stationarity conditions are met, the candidate solution $\\hat{\\mathbf{x}}$ is an optimal solution to the problem.\n\n### Part 2: Determine Uniqueness of the Solution\n\nTo determine if the solution is unique from first principles, we investigate if another optimal solution $\\mathbf{x}^* \\neq \\hat{\\mathbf{x}}$ can exist.\nLet $(\\hat{\\mathbf{x}}, \\hat{\\mathbf{\\nu}})$ be the optimal primal-dual pair we found. Let $\\mathbf{g} = -\\mathbf{A}^T\\hat{\\mathbf{\\nu}} = (1, -1, 1/2)^T$. We know $\\mathbf{g} \\in \\partial\\|\\hat{\\mathbf{x}}\\|_1$.\n\nA fundamental property of convex functions is the subgradient inequality: for any $\\mathbf{x}$ and any $\\mathbf{g} \\in \\partial f(\\hat{\\mathbf{x}})$, we have $f(\\mathbf{x}) \\ge f(\\hat{\\mathbf{x}}) + \\mathbf{g}^T(\\mathbf{x}-\\hat{\\mathbf{x}})$.\nLet $\\mathbf{x}^*$ be any other feasible solution, so $\\mathbf{A}\\mathbf{x}^* = \\mathbf{y}$. Applying the inequality:\n$$\n\\|\\mathbf{x}^*\\|_1 \\ge \\|\\hat{\\mathbf{x}}\\|_1 + \\mathbf{g}^T(\\mathbf{x}^* - \\hat{\\mathbf{x}})\n$$\nLet's analyze the term $\\mathbf{g}^T(\\mathbf{x}^* - \\hat{\\mathbf{x}})$:\n$$\n\\mathbf{g}^T(\\mathbf{x}^* - \\hat{\\mathbf{x}}) = (-\\mathbf{A}^T\\hat{\\mathbf{\\nu}})^T(\\mathbf{x}^* - \\hat{\\mathbf{x}}) = -\\hat{\\mathbf{\\nu}}^T\\mathbf{A}(\\mathbf{x}^* - \\hat{\\mathbf{x}}) = -\\hat{\\mathbf{\\nu}}^T(\\mathbf{A}\\mathbf{x}^* - \\mathbf{A}\\hat{\\mathbf{x}})\n$$\nSince both $\\mathbf{x}^*$ and $\\hat{\\mathbf{x}}$ are feasible, $\\mathbf{A}\\mathbf{x}^*=\\mathbf{y}$ and $\\mathbf{A}\\hat{\\mathbf{x}}=\\mathbf{y}$. Therefore:\n$$\n\\mathbf{g}^T(\\mathbf{x}^* - \\hat{\\mathbf{x}}) = -\\hat{\\mathbf{\\nu}}^T(\\mathbf{y} - \\mathbf{y}) = 0\n$$\nThe subgradient inequality thus simplifies to $\\|\\mathbf{x}^*\\|_1 \\ge \\|\\hat{\\mathbf{x}}\\|_1$, confirming the optimality of $\\hat{\\mathbf{x}}$ for any feasible $\\mathbf{x}^*$.\n\nFor $\\mathbf{x}^*$ to be another optimal solution, it must satisfy $\\|\\mathbf{x}^*\\|_1 = \\|\\hat{\\mathbf{x}}\\|_1$. This means the equality must hold in the subgradient inequality:\n$$\n\\|\\mathbf{x}^*\\|_1 = \\|\\hat{\\mathbf{x}}\\|_1 + \\mathbf{g}^T(\\mathbf{x}^* - \\hat{\\mathbf{x}})\n$$\nThis condition implies that $\\mathbf{x}^*$ must also be a minimizer of the function $h(\\mathbf{x}) = \\|\\mathbf{x}\\|_1 - \\mathbf{g}^T\\mathbf{x}$. Let's find the set of minimizers for this function.\n$$\nh(\\mathbf{x}) = \\sum_{i=1}^3 \\left( |x_i| - g_i x_i \\right) = (|x_1| - 1 \\cdot x_1) + (|x_2| - (-1) \\cdot x_2) + (|x_3| - \\frac{1}{2} x_3)\n$$\nEach term in the sum is non-negative:\n- $|x_1| - x_1 \\ge 0$. Equality holds if and only if $x_1 \\ge 0$.\n- $|x_2| + x_2 \\ge 0$. Equality holds if and only if $x_2 \\le 0$.\n- $|x_3| - \\frac{1}{2}x_3$. Since $|g_3| = |\\frac{1}{2}|  1$, this term is strictly positive for any $x_3 \\neq 0$. If $x_3 > 0$, it is $\\frac{1}{2}x_3 > 0$. If $x_3  0$, it is $-\\frac{3}{2}x_3 > 0$. The term is zero if and only if $x_3 = 0$.\n\nFor $h(\\mathbf{x})$ to be minimized (i.e., equal to $0$), all three terms must be zero. This requires:\n1.  $x_1 \\ge 0$\n2.  $x_2 \\le 0$\n3.  $x_3 = 0$\n\nAny optimal solution $\\mathbf{x}^*$ must satisfy these conditions and also be feasible, i.e., $\\mathbf{A}\\mathbf{x}^*=\\mathbf{y}$. Let's solve the system of equations for $\\mathbf{x}^*=(x_1^*, x_2^*, x_3^*)^T$:\n$$\n\\begin{pmatrix} 1  0  1 \\\\ 0  1  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} x_1^* \\\\ x_2^* \\\\ x_3^* \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\nWe must impose the condition $x_3^*=0$:\n$$\nx_1^* + 0 = 1 \\implies x_1^* = 1\n$$\n$$\nx_2^* + \\frac{1}{2}(0) = -1 \\implies x_2^* = -1\n$$\nThis gives the unique solution $\\mathbf{x}^* = (1, -1, 0)^T$. This vector must also satisfy the sign conditions: $x_1^* = 1 \\ge 0$ (satisfied) and $x_2^* = -1 \\le 0$ (satisfied).\n\nThe only point that satisfies both the feasibility constraints and the conditions for being an optimal solution is $\\mathbf{x}^* = (1, -1, 0)^T$, which is our original candidate solution $\\hat{\\mathbf{x}}$. Therefore, no other optimal solution exists, and $\\hat{\\mathbf{x}}$ is the unique solution. The crucial step enabling this conclusion was the strict inequality $|g_3|1$, which enforced $x_3^*=0$ for any optimal solution.\n\nThe optimal primal solution vector is $\\hat{\\mathbf{x}}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}}\n$$"
        }
    ]
}