{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of the Null Space Property (NSP), it is invaluable to start with a concrete example. The following exercise challenges you to analyze a simple, low-dimensional matrix and directly compute its NSP constant. This practice will help you distinguish between the weaker condition of injectivity on sparse vectors—which merely guarantees a unique sparse solution in the noiseless case—and the stronger NSP, which is necessary for the stability and robustness of $\\ell_1$-minimization . By working through the calculations, you will gain a hands-on feel for what it means for a matrix to satisfy, or fail, this crucial property.",
            "id": "3489369",
            "problem": "Let $s \\in \\mathbb{N}$ denote the sparsity level and consider the matrix $A \\in \\mathbb{R}^{2 \\times 3}$ with columns $a_{1}, a_{2}, a_{3}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n0.4  1  0 \\\\\n0.6  0  1\n\\end{pmatrix}\n\\;=\\; \\big[\\, a_{1} \\; a_{2} \\; a_{3} \\,\\big], \\quad\na_{1} \\;=\\; \\begin{pmatrix} 0.4 \\\\ 0.6 \\end{pmatrix},\\;\na_{2} \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix},\\;\na_{3} \\;=\\; \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\nYou may use the following foundational definitions.\n- A vector $x \\in \\mathbb{R}^{3}$ is called $s$-sparse if $\\|x\\|_{0} \\leq s$, where $\\|x\\|_{0}$ denotes the number of nonzero entries of $x$.\n- The matrix $A$ is injective on $s$-sparse vectors if $Ax = Ax'$ for $x, x'$ with $\\|x\\|_{0}, \\|x'\\|_{0} \\leq s$ implies $x = x'$.\n- The $\\ell_{1}$-Null Space Property (NSP) of order $s$ holds for $A$ if for all nonzero $h \\in \\ker(A)$ and all index sets $S \\subset \\{1,2,3\\}$ with $|S| \\leq s$,\n$$\n\\|h_{S}\\|_{1} \\;\\; \\|h_{S^{c}}\\|_{1}.\n$$\nDefine the NSP constant of order $s$ by\n$$\n\\rho_{s}(A) \\;=\\; \\sup_{h \\in \\ker(A)\\setminus\\{0\\}} \\;\\sup_{S:\\,|S|\\leq s} \\;\\frac{\\|h_{S}\\|_{1}}{\\|h_{S^{c}}\\|_{1}}.\n$$\nTasks:\n- Using only the above definitions and elementary linear algebra, establish that for $s = 1$, the matrix $A$ is injective on $1$-sparse vectors.\n- Compute the exact value of $\\rho_{1}(A)$ and conclude whether the $\\ell_{1}$-NSP of order $1$ holds for $A$.\n- Briefly explain, using the definition of the Restricted Isometry Property (RIP) of order $k$ for a matrix $A$ as the existence of $\\delta_{k} \\in [0,1)$ such that\n$$\n(1 - \\delta_{k})\\|x\\|_{2}^{2} \\;\\leq\\; \\|Ax\\|_{2}^{2} \\;\\leq\\; (1 + \\delta_{k})\\|x\\|_{2}^{2} \\quad \\text{for all $k$-sparse $x$,}\n$$\nwhy a matrix that fails the $\\ell_{1}$-NSP of order $s$ cannot enjoy any RIP level that is known to imply the $\\ell_{1}$-NSP of order $s$ and hence cannot guarantee robust $\\ell_{1}$-recovery, even if it is injective on $s$-sparse vectors.\n\nReport only the exact value of $\\rho_{1}(A)$ as your final answer. No rounding is required.",
            "solution": "The problem as stated is well-posed, mathematically consistent, and grounded in the established theory of compressed sensing. All definitions and data required for the solution are provided. We may therefore proceed with the analysis.\n\nThe problem asks for three tasks to be completed for a sparsity level $s=1$ and the matrix $A \\in \\mathbb{R}^{2 \\times 3}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n0.4  1  0 \\\\\n0.6  0  1\n\\end{pmatrix}\n\\;=\\; \\big[\\, a_{1} \\; a_{2} \\; a_{3} \\,\\big].\n$$\n\nFirst, we establish that for $s=1$, the matrix $A$ is injective on $1$-sparse vectors.\nThe definition states that $A$ is injective on $1$-sparse vectors if for any two $1$-sparse vectors $x, x' \\in \\mathbb{R}^{3}$, the condition $Ax = Ax'$ implies $x=x'$. This can be rewritten as $A(x-x')=0$. The vector $v = x-x'$ is the difference of two $1$-sparse vectors. If the non-zero entries of $x$ and $x'$ are at different positions, $v$ is $2$-sparse. If they are at the same position, $v$ is $1$-sparse or the zero vector. Thus, injectivity on $1$-sparse vectors is equivalent to the condition that for any non-zero vector $v$ with $\\|v\\|_{0} \\leq 2$, we have $Av \\neq 0$. This, in turn, is equivalent to stating that any two columns of $A$ are linearly independent.\n\nLet's verify this for the columns of $A$:\n$a_{1} = \\begin{pmatrix} 0.4 \\\\ 0.6 \\end{pmatrix}$, $a_{2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $a_{3} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\n1.  Consider columns $a_{1}$ and $a_{2}$. The matrix formed by these columns is $\\begin{pmatrix} 0.4  1 \\\\ 0.6  0 \\end{pmatrix}$. Its determinant is $(0.4)(0) - (1)(0.6) = -0.6 \\neq 0$. Thus, $a_{1}$ and $a_{2}$ are linearly independent.\n2.  Consider columns $a_{1}$ and $a_{3}$. The matrix formed by these columns is $\\begin{pmatrix} 0.4  0 \\\\ 0.6  1 \\end{pmatrix}$. Its determinant is $(0.4)(1) - (0)(0.6) = 0.4 \\neq 0$. Thus, $a_{1}$ and $a_{3}$ are linearly independent.\n3.  Consider columns $a_{2}$ and $a_{3}$. The matrix formed by these columns is $\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$, which is the identity matrix. Its determinant is $1 \\neq 0$. Thus, $a_{2}$ and $a_{3}$ are linearly independent.\n\nSince any pair of columns of $A$ is linearly independent, no non-zero vector with at most two non-zero entries lies in the null space of $A$. This proves that $A$ is injective on $1$-sparse vectors.\n\nNext, we compute the exact value of the Null Space Property (NSP) constant $\\rho_{1}(A)$ and check if the $\\ell_{1}$-NSP of order $1$ holds.\nFirst, we find the null space, $\\ker(A)$, of the matrix $A$. A vector $h = (h_{1}, h_{2}, h_{3})^{T} \\in \\mathbb{R}^{3}$ belongs to $\\ker(A)$ if $Ah=0$.\n$$\n\\begin{pmatrix}\n0.4  1  0 \\\\\n0.6  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\nh_{1} \\\\ h_{2} \\\\ h_{3}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\ 0\n\\end{pmatrix}\n$$\nThis gives the system of linear equations:\n$0.4 h_{1} + h_{2} = 0 \\implies h_{2} = -0.4 h_{1}$\n$0.6 h_{1} + h_{3} = 0 \\implies h_{3} = -0.6 h_{1}$\nThe null space is a one-dimensional subspace spanned by any vector of the form $c(1, -0.4, -0.6)^{T}$ for any non-zero scalar $c \\in \\mathbb{R}$.\n\nThe NSP constant $\\rho_{1}(A)$ is defined as:\n$$\n\\rho_{1}(A) \\;=\\; \\sup_{h \\in \\ker(A)\\setminus\\{0\\}} \\;\\sup_{S:\\,|S|\\leq 1} \\;\\frac{\\|h_{S}\\|_{1}}{\\|h_{S^{c}}\\|_{1}}\n$$\nSince the ratio $\\frac{\\|h_{S}\\|_{1}}{\\|h_{S^{c}}\\|_{1}}$ is invariant to scaling of $h$ by a non-zero constant, we can choose a representative vector from $\\ker(A)$, for instance, $h^{*} = (1, -0.4, -0.6)^{T}$.\n\nWe need to check all index sets $S \\subset \\{1,2,3\\}$ with $|S| \\leq 1$.\nThe possible sets are $S = \\emptyset$, $S=\\{1\\}$, $S=\\{2\\}$, and $S=\\{3\\}$.\n\nCase $1$: $S = \\emptyset$. Here $|S|=0$.\n$S^{c} = \\{1,2,3\\}$.\n$\\|h^{*}_{S}\\|_{1} = 0$.\n$\\|h^{*}_{S^{c}}\\|_{1} = |1| + |-0.4| + |-0.6| = 1 + 0.4 + 0.6 = 2$.\nThe ratio is $\\frac{0}{2} = 0$.\n\nCase $2$: $S = \\{1\\}$. Here $|S|=1$.\n$S^{c} = \\{2,3\\}$.\n$\\|h^{*}_{S}\\|_{1} = |h^{*}_{1}| = |1| = 1$.\n$\\|h^{*}_{S^{c}}\\|_{1} = |h^{*}_{2}| + |h^{*}_{3}| = |-0.4| + |-0.6| = 0.4 + 0.6 = 1$.\nThe ratio is $\\frac{1}{1} = 1$.\n\nCase $3$: $S = \\{2\\}$. Here $|S|=1$.\n$S^{c} = \\{1,3\\}$.\n$\\|h^{*}_{S}\\|_{1} = |h^{*}_{2}| = |-0.4| = 0.4$.\n$\\|h^{*}_{S^{c}}\\|_{1} = |h^{*}_{1}| + |h^{*}_{3}| = |1| + |-0.6| = 1 + 0.6 = 1.6$.\nThe ratio is $\\frac{0.4}{1.6} = \\frac{4}{16} = \\frac{1}{4}$.\n\nCase $4$: $S = \\{3\\}$. Here $|S|=1$.\n$S^{c} = \\{1,2\\}$.\n$\\|h^{*}_{S}\\|_{1} = |h^{*}_{3}| = |-0.6| = 0.6$.\n$\\|h^{*}_{S^{c}}\\|_{1} = |h^{*}_{1}| + |h^{*}_{2}| = |1| + |-0.4| = 1 + 0.4 = 1.4$.\nThe ratio is $\\frac{0.6}{1.4} = \\frac{6}{14} = \\frac{3}{7}$.\n\nThe set of computed ratios is $\\{0, 1, \\frac{1}{4}, \\frac{3}{7}\\}$. The supremum of this set is $1$.\nTherefore, the NSP constant of order $1$ is $\\rho_{1}(A) = 1$.\n\nThe $\\ell_{1}$-NSP of order $s$ holds if $\\rho_{s}(A)  1$. Since we found $\\rho_{1}(A) = 1$, the strict inequality is not satisfied. Thus, the $\\ell_{1}$-NSP of order $1$ does not hold for the matrix $A$.\n\nFinally, we briefly explain the relationship between failing the NSP and the Restricted Isometry Property (RIP).\nThe theory of compressed sensing establishes a hierarchy of conditions. Specifically, it is a known theorem that if a matrix $A$ satisfies the RIP of order $2s$ with a sufficiently small constant $\\delta_{2s}$ (e.g., $\\delta_{2s}  \\sqrt{2}-1$, or other similar bounds), then $A$ is guaranteed to satisfy the $\\ell_{1}$-NSP of order $s$. This can be written as an implication:\n$$\n\\text{RIP of order } 2s \\text{ with small enough } \\delta_{2s} \\implies \\ell_{1}\\text{-NSP of order } s\n$$\nBy contraposition, if a matrix fails the $\\ell_{1}$-NSP of order $s$, it cannot possibly satisfy any RIP condition that is strong enough to imply it.\nIn our case, the matrix $A$ fails the $\\ell_{1}$-NSP of order $s=1$. Therefore, $A$ cannot satisfy any condition on the RIP of order $2s=2$ that would be sufficient to imply the $\\ell_{1}$-NSP of order $1$. For instance, $A$ cannot have a RIP constant $\\delta_{2}  \\sqrt{2}-1$.\nThe $\\ell_{1}$-NSP of order $s$ is a necessary and sufficient condition for the uniform robust recovery of all $s$-sparse signals via $\\ell_{1}$-minimization. Since $A$ fails the NSP of order $1$, it cannot guarantee such robust recovery for $1$-sparse signals, even though it is injective on $1$-sparse vectors, a property which only guarantees the uniqueness of $1$-sparse solutions to $y=Ax$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "While the NSP provides the tightest conditions for sparse recovery, proving it directly can be difficult. In practice, we often rely on other, more tractable matrix properties that imply the NSP. This exercise explores the rich theoretical landscape connecting the NSP to two famous conditions: the Restricted Isometry Property (RIP) and mutual coherence . You will derive how a bound on coherence can imply RIP, which in turn implies NSP, and compare this chain of inequalities to a more direct guarantee from coherence alone, revealing a \"gap\" between the conditions. This analysis will deepen your understanding of the hierarchy of theoretical guarantees in compressed sensing and the trade-offs involved in using them.",
            "id": "3489384",
            "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ have unit $\\ell_{2}$-norm columns $\\{a_{j}\\}_{j=1}^{n}$. Define the mutual coherence $\\mu(A)$ by $\\mu(A) \\triangleq \\max_{i \\neq j} |\\langle a_{i}, a_{j} \\rangle|$. The order-$k$ Restricted Isometry Property (RIP) constant $\\delta_{k}(A)$ is defined as the smallest $\\delta \\geq 0$ such that for all $k$-sparse $x \\in \\mathbb{R}^{n}$,\n$$\n(1 - \\delta) \\|x\\|_{2}^{2} \\leq \\|A x\\|_{2}^{2} \\leq (1 + \\delta) \\|x\\|_{2}^{2}.\n$$\nThe order-$s$ Null Space Property (NSP) for $\\ell_{1}$-minimization requires that for every $h \\in \\ker(A) \\setminus \\{0\\}$ and every index set $S \\subset [n]$ with $|S| \\leq s$, one has $\\|h_{S}\\|_{1}  \\|h_{S^{c}}\\|_{1}$.\n\nWork from fundamental definitions and well-tested facts to address the following.\n\n1. Using only the definitions and classical Gershgorin-type arguments on Gram submatrices, bound $\\delta_{k}(A)$ in terms of $\\mu(A)$ and $k$.\n\n2. It is a well-tested sufficient condition that $\\delta_{2s}(A)  \\sqrt{2} - 1$ implies the order-$s$ NSP for $\\ell_{1}$-minimization. Combine your bound from part 1 with this RIP-based sufficiency to translate the RIP condition into a coherence upper bound of the form $\\mu(A)  c_{\\mathrm{RIP}}(s)$.\n\n3. Independently, it is a well-tested sufficient condition that $\\mu(A)  \\frac{1}{2s - 1}$ implies the order-$s$ NSP for $\\ell_{1}$-minimization. Explain briefly, starting from the definitions, why a coherence upper bound of this form can imply the order-$s$ NSP.\n\n4. Construct an explicit parameter regime that exhibits a strict gap between the coherence-based NSP condition and the RIP-based NSP condition derived via part 2. Concretely, consider a dictionary formed by the union of two orthonormal bases in $\\mathbb{R}^{m}$ with coherence $\\mu(A) = \\frac{1}{\\sqrt{m}}$ and choose $s$ and $m$ so that\n$$\n\\frac{\\sqrt{2} - 1}{2s - 1}  \\frac{1}{\\sqrt{m}}  \\frac{1}{2s - 1}.\n$$\nArgue that for such a choice, the coherence-based condition certifies NSP but the RIP-based route via your bound in part 2 does not. You do not need to exhibit concrete matrices beyond identifying a valid pair $(m,s)$ that satisfies the displayed inequality.\n\nFinally, quantify precisely the gap in constants between the two sufficient routes to NSP by computing the exact ratio\n$$\nR \\triangleq \\frac{c_{\\mathrm{coh}}(s)}{c_{\\mathrm{RIP}}(s)},\n$$\nwhere $c_{\\mathrm{coh}}(s)$ is the largest $\\mu(A)$ allowed by the coherence-based sufficient condition in part 3 and $c_{\\mathrm{RIP}}(s)$ is the largest $\\mu(A)$ allowed by the RIP-based sufficient condition obtained in part 2. Give your final answer for $R$ as a closed-form analytic expression. No rounding is required.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is based on standard definitions and results in the field of compressed sensing and sparse optimization. The tasks are formalizable and lead to a verifiable solution.\n\nThe solution proceeds in four parts as requested, followed by the final calculation of the ratio.\n\n1. Bounding the Restricted Isometry Property (RIP) constant $\\delta_{k}(A)$ in terms of the mutual coherence $\\mu(A)$ and the sparsity level $k$.\n\nLet $x \\in \\mathbb{R}^{n}$ be a $k$-sparse vector, meaning it has at most $k$ non-zero entries. Let $S = \\mathrm{supp}(x)$ be the index set of its non-zero components, so $|S| \\leq k$. Let $A_{S}$ be the $m \\times |S|$ submatrix of $A$ formed by the columns indexed by $S$, and let $x_{S}$ be the subvector of $x$ containing the non-zero entries. Then, the product $Ax$ can be written as $A_{S}x_{S}$.\n\nThe squared $\\ell_2$-norm of $Ax$ is given by:\n$$\n\\|Ax\\|_{2}^{2} = \\|A_{S}x_{S}\\|_{2}^{2} = x_{S}^{T}A_{S}^{T}A_{S}x_{S}\n$$\nThe RIP constant $\\delta_{k}(A)$ is defined by the tightest bounds on $\\|Ax\\|_{2}^{2}$ relative to $\\|x\\|_{2}^{2} = \\|x_{S}\\|_{2}^{2}$. These bounds are determined by the eigenvalues of the Gram matrices $G_{S} \\triangleq A_{S}^{T}A_{S}$ for all possible supports $S$ with $|S| \\leq k$. Specifically,\n$$\n1 - \\delta_{k}(A) = \\min_{|S| \\leq k} \\lambda_{\\min}(A_{S}^{T}A_{S})\n$$\n$$\n1 + \\delta_{k}(A) = \\max_{|S| \\leq k} \\lambda_{\\max}(A_{S}^{T}A_{S})\n$$\nLet's analyze the properties of an arbitrary such Gram matrix $G_{S}$, which is a square matrix of size at most $k \\times k$. The columns of $A$, denoted $\\{a_j\\}$, are given to have unit $\\ell_2$-norm, i.e., $\\|a_j\\|_2 = 1$ for all $j \\in \\{1, \\dots, n\\}$.\nThe diagonal entries of $G_{S}$ are $(G_{S})_{ii} = \\langle a_{i}, a_{i} \\rangle = \\|a_{i}\\|_{2}^{2} = 1$ for $i \\in S$.\nThe off-diagonal entries are $(G_{S})_{ij} = \\langle a_{i}, a_{j} \\rangle$ for $i, j \\in S$ and $i \\neq j$. By the definition of mutual coherence, we have $|(G_{S})_{ij}| = |\\langle a_{i}, a_{j} \\rangle| \\leq \\mu(A)$.\n\nWe use the Gershgorin Circle Theorem to bound the eigenvalues of $G_{S}$. For a square matrix $M$, the theorem states that every eigenvalue lies within at least one of the Gershgorin discs $D(M_{ii}, R_{i})$, where $R_{i} = \\sum_{j \\neq i} |M_{ij}|$.\nFor our matrix $G_{S}$, the centers of all discs are $1$. The radius for the $i$-th row is the sum of the magnitudes of the off-diagonal entries in that row:\n$$\nR_{i} = \\sum_{j \\in S, j \\neq i} |(G_{S})_{ij}| = \\sum_{j \\in S, j \\neq i} |\\langle a_{i}, a_{j} \\rangle| \\leq \\sum_{j \\in S, j \\neq i} \\mu(A)\n$$\nSince $|S| \\leq k$, there are at most $k-1$ terms in this sum. Thus, $R_{i} \\leq (k-1)\\mu(A)$.\nSince $G_{S}$ is real and symmetric, its eigenvalues $\\lambda$ are real. The Gershgorin theorem implies that all eigenvalues must lie in the union of the intervals $[1 - R_i, 1 + R_i]$. This means for any eigenvalue $\\lambda$ of $G_{S}$:\n$$\n|\\lambda - 1| \\leq \\max_{i} R_i \\leq (k-1)\\mu(A)\n$$\nThis gives the interval for the eigenvalues:\n$$\n1 - (k-1)\\mu(A) \\leq \\lambda \\leq 1 + (k-1)\\mu(A)\n$$\nThis holds for any eigenvalue of any Gram submatrix $A_{S}^{T}A_{S}$ with $|S| \\leq k$. Therefore, we have bounds on the extremal eigenvalues:\n$$\n\\min_{|S| \\leq k} \\lambda_{\\min}(A_{S}^{T}A_{S}) \\geq 1 - (k-1)\\mu(A)\n$$\n$$\n\\max_{|S| \\leq k} \\lambda_{\\max}(A_{S}^{T}A_{S}) \\leq 1 + (k-1)\\mu(A)\n$$\nFrom the definition of $\\delta_{k}(A)$, this implies $(1 - \\delta_k) \\geq 1 - (k-1)\\mu(A)$ and $(1 + \\delta_k) \\leq 1 + (k-1)\\mu(A)$. Both inequalities lead to the same upper bound on $\\delta_k(A)$:\n$$\n\\delta_{k}(A) \\leq (k-1)\\mu(A)\n$$\n\n2. Translating the RIP-based sufficiency condition into a coherence upper bound.\n\nWe are given that $\\delta_{2s}(A)  \\sqrt{2} - 1$ is a sufficient condition for the order-$s$ Null Space Property (NSP). We can combine this with the bound derived in part 1.\nUsing our bound with $k=2s$, we have:\n$$\n\\delta_{2s}(A) \\leq (2s-1)\\mu(A)\n$$\nTherefore, if we impose a condition on $\\mu(A)$ that is strong enough to guarantee $\\delta_{2s}(A)  \\sqrt{2} - 1$, then this condition on $\\mu(A)$ will also be sufficient for the order-$s$ NSP. We enforce the inequality:\n$$\n(2s-1)\\mu(A)  \\sqrt{2} - 1\n$$\nSolving for $\\mu(A)$, we get the sufficient condition:\n$$\n\\mu(A)  \\frac{\\sqrt{2} - 1}{2s - 1}\n$$\nThis is of the form $\\mu(A)  c_{\\mathrm{RIP}}(s)$, where $c_{\\mathrm{RIP}}(s) = \\frac{\\sqrt{2} - 1}{2s - 1}$.\n\n3. Explaining why a coherence upper bound implies the order-$s$ NSP.\n\nThe order-$s$ NSP requires that for any non-zero vector $h \\in \\ker(A)$ and any index set $S$ with $|S| \\leq s$, we have $\\|h_{S}\\|_{1}  \\|h_{S^{c}}\\|_{1}$.\nLet $h \\in \\ker(A)$ be a non-zero vector, so $Ah = \\sum_{j=1}^{n} h_j a_j = 0$.\nFor any index $i \\in \\{1, \\dots, n\\}$, we can isolate the term $h_i a_i$:\n$$\nh_i a_i = - \\sum_{j \\neq i} h_j a_j\n$$\nTaking the inner product of both sides with $a_i$ and using $\\|a_i\\|_2^2=1$:\n$$\nh_i \\|a_i\\|_2^2 = - \\sum_{j \\neq i} h_j \\langle a_j, a_i \\rangle \\implies h_i = - \\sum_{j \\neq i} h_j \\langle a_j, a_i \\rangle\n$$\nTaking the absolute value and applying the triangle inequality:\n$$\n|h_i| = \\left| \\sum_{j \\neq i} h_j \\langle a_j, a_i \\rangle \\right| \\leq \\sum_{j \\neq i} |h_j| |\\langle a_j, a_i \\rangle|\n$$\nUsing the definition of mutual coherence, $|\\langle a_j, a_i \\rangle| \\leq \\mu(A)$ for $j \\neq i$:\n$$\n|h_i| \\leq \\sum_{j \\neq i} |h_j| \\mu(A) = \\mu(A) \\sum_{j \\neq i} |h_j| = \\mu(A) \\|h_{\\{i\\}^c}\\|_1\n$$\nNow, let $S$ be an arbitrary index set with $|S| \\leq s$. We sum the inequality above over all $i \\in S$:\n$$\n\\|h_S\\|_1 = \\sum_{i \\in S} |h_i| \\leq \\sum_{i \\in S} \\mu(A) \\|h_{\\{i\\}^c}\\|_1 = \\mu(A) \\sum_{i \\in S} \\left( \\sum_{j \\in S, j \\neq i} |h_j| + \\sum_{j \\in S^c} |h_j| \\right)\n$$\nThe first inner sum is $\\sum_{j \\in S, j \\neq i} |h_j| = \\|h_S\\|_1 - |h_i|$. The second is $\\|h_{S^c}\\|_1$.\n$$\n\\|h_S\\|_1 \\leq \\mu(A) \\sum_{i \\in S} (\\|h_S\\|_1 - |h_i| + \\|h_{S^c}\\|_1)\n$$\n$$\n\\|h_S\\|_1 \\leq \\mu(A) \\left( \\sum_{i \\in S} (\\|h_S\\|_1 - |h_i|) + \\sum_{i \\in S} \\|h_{S^c}\\|_1 \\right)\n$$\n$$\n\\|h_S\\|_1 \\leq \\mu(A) \\left( |S|\\|h_S\\|_1 - \\|h_S\\|_1 + |S|\\|h_{S^c}\\|_1 \\right) = \\mu(A) \\left( (|S|-1)\\|h_S\\|_1 + |S|\\|h_{S^c}\\|_1 \\right)\n$$\nSince $|S| \\leq s$, we can write a looser but sufficient inequality:\n$$\n\\|h_S\\|_1 \\leq \\mu(A) \\left( (s-1)\\|h_S\\|_1 + s\\|h_{S^c}\\|_1 \\right)\n$$\nRearranging to solve for $\\|h_S\\|_1$:\n$$\n\\|h_S\\|_1 (1 - \\mu(A)(s-1)) \\leq \\mu(A)s \\|h_{S^c}\\|_1\n$$\nTo ensure the NSP condition $\\|h_S\\|_1  \\|h_{S^c}\\|_1$, we can require the coefficient on $\\|h_{S^c}\\|_1$ to be less than $1$. Assuming $1 - \\mu(A)(s-1)  0$, we require:\n$$\n\\frac{\\mu(A)s}{1 - \\mu(A)(s-1)}  1\n$$\nThis inequality is stricter than necessary, as it holds for any $h$. A more careful analysis on the sum for $|S|$ is used. Let's use the bound for $|S|$ instead of $s$.\n$\\|h_S\\|_1 (1 - \\mu(A)(|S|-1)) \\leq \\mu(A)|S| \\|h_{S^c}\\|_1$.\nWe need $\\frac{\\mu(A)|S|}{1 - \\mu(A)(|S|-1)}  1$, which implies $\\mu(A)|S|  1 - \\mu(A)(|S|-1)$.\nThis simplifies to $\\mu(A)(2|S|-1)  1$, or $\\mu(A)  \\frac{1}{2|S|-1}$.\nThis must hold for any $S$ with $|S| \\leq s$. The condition becomes more restrictive as $|S|$ increases. Thus, the most stringent requirement is for $|S|=s$:\n$$\n\\mu(A)  \\frac{1}{2s-1}\n$$\nThis condition guarantees the NSP for order $s$.\n\n4. Constructing a parameter regime exhibiting a gap.\n\nWe are asked to find integers $m$ and $s$ such that for a matrix with $\\mu(A) = \\frac{1}{\\sqrt{m}}$, we have:\n$$\n\\frac{\\sqrt{2} - 1}{2s - 1}  \\frac{1}{\\sqrt{m}}  \\frac{1}{2s - 1}\n$$\nThis inequality chain represents a scenario where the direct coherence condition for NSP (the right inequality) is satisfied, but the RIP-based condition derived from the Gershgorin bound (the left inequality) is not met.\nLet's manipulate the inequalities. The right inequality is $\\frac{1}{\\sqrt{m}}  \\frac{1}{2s-1}$, which is equivalent to $\\sqrt{m} > 2s-1$, or $m > (2s-1)^2$.\nThe left inequality is $\\frac{\\sqrt{2}-1}{2s-1}  \\frac{1}{\\sqrt{m}}$, equivalent to $\\sqrt{m}  \\frac{2s-1}{\\sqrt{2}-1}$.\nWe can rationalize the denominator: $\\frac{1}{\\sqrt{2}-1} = \\frac{\\sqrt{2}+1}{(\\sqrt{2}-1)(\\sqrt{2}+1)} = \\sqrt{2}+1$.\nSo, $\\sqrt{m}  (2s-1)(\\sqrt{2}+1)$, which means $m  (2s-1)^2 (\\sqrt{2}+1)^2 = (2s-1)^2 (3+2\\sqrt{2})$.\nCombining these, we need to find integers $(m, s)$ such that:\n$$\n(2s-1)^2  m  (3+2\\sqrt{2})(2s-1)^2\n$$\nLet's approximate the factor $3+2\\sqrt{2} \\approx 3 + 2(1.4142) = 3 + 2.8284 = 5.8284$.\nThe condition is $(2s-1)^2  m  5.8284 \\times (2s-1)^2$.\nFor any integer $s \\geq 1$, the length of this interval for $m$ is $(2+2\\sqrt{2})(2s-1)^2$, which is greater than $1$, so an integer $m$ always exists.\nLet's choose a simple value for $s$, for instance $s=2$.\nFor $s=2$, $2s-1 = 3$, and the condition on $m$ becomes:\n$$\n3^2  m  (3+2\\sqrt{2}) \\times 3^2 \\implies 9  m  9(3+2\\sqrt{2}) = 27 + 18\\sqrt{2}\n$$\nSince $18\\sqrt{2} \\approx 25.456$, this is approximately $9  m  52.456$.\nWe can choose any integer $m$ in this range, for example, $m=10$.\nSo, the pair $(m,s)=(10,2)$ is a valid choice.\n\nWith this choice, we have a matrix with coherence $\\mu(A) = 1/\\sqrt{10}$.\nThe direct coherence condition for NSP is $\\mu(A)  \\frac{1}{2s-1}$, which is $\\frac{1}{\\sqrt{10}}  \\frac{1}{2(2)-1} = \\frac{1}{3}$. This is true, as $\\sqrt{10}  3$ (since $10  9$). Thus, the coherence-based condition certifies the order-$2$ NSP.\nThe RIP-based pathway requires $\\mu(A)  c_{\\mathrm{RIP}}(s) = \\frac{\\sqrt{2}-1}{2s-1}$. For $s=2$, this is $\\frac{1}{\\sqrt{10}}  \\frac{\\sqrt{2}-1}{3}$. This is equivalent to $3  \\sqrt{10}(\\sqrt{2}-1) = \\sqrt{20}-\\sqrt{10}$. We estimate $\\sqrt{20} \\approx 4.47$ and $\\sqrt{10} \\approx 3.16$. So, the condition is $3  4.47 - 3.16 = 1.31$, which is false.\nThus, for the regime $(m,s)=(10,2)$, the coherence-based test for NSP passes, while the test derived from the RIP and Gershgorin bounds fails. This demonstrates the gap between the two sufficient conditions. The failure of the second test does not mean NSP fails, but that the bound used ($\\delta_{2s} \\leq (2s-1)\\mu$) is too loose to provide a guarantee in this regime.\n\nFinally, we quantify the gap by computing the ratio $R$.\nThe sufficient condition from the direct coherence argument is $\\mu(A)  c_{\\mathrm{coh}}(s)$, where $c_{\\mathrm{coh}}(s) = \\frac{1}{2s-1}$.\nThe sufficient condition from the RIP-based argument is $\\mu(A)  c_{\\mathrm{RIP}}(s)$, where $c_{\\mathrm{RIP}}(s) = \\frac{\\sqrt{2}-1}{2s-1}$.\nThe ratio $R$ is:\n$$\nR = \\frac{c_{\\mathrm{coh}}(s)}{c_{\\mathrm{RIP}}(s)} = \\frac{\\frac{1}{2s-1}}{\\frac{\\sqrt{2}-1}{2s-1}} = \\frac{1}{\\sqrt{2}-1}\n$$\nRationalizing the denominator gives:\n$$\nR = \\frac{1}{\\sqrt{2}-1} \\times \\frac{\\sqrt{2}+1}{\\sqrt{2}+1} = \\frac{\\sqrt{2}+1}{2-1} = \\sqrt{2}+1\n$$",
            "answer": "$$\n\\boxed{\\sqrt{2}+1}\n$$"
        },
        {
            "introduction": "The true power of properties like the NSP is their ability to provide performance guarantees for recovery algorithms in realistic scenarios, particularly those involving noise. This final practice bridges the gap between abstract theory and practical application by focusing on the Robust Null Space Property (RNSP), a version of the NSP tailored for noisy measurements. Starting from the definition of the RNSP, you will derive a formal upper bound on the reconstruction error for a signal recovered via constrained $\\ell_1$-minimization . This derivation is a cornerstone of compressed sensing theory, demonstrating precisely how a geometric property of a sensing matrix translates into a robust recovery guarantee.",
            "id": "3489375",
            "problem": "Consider a measurement model $y = A x + e$ with an unknown signal $x \\in \\mathbb{R}^{n}$, a known sensing matrix $A \\in \\mathbb{R}^{m \\times n}$, and a noise vector $e \\in \\mathbb{R}^{m}$ satisfying $\\|e\\|_{2} \\le \\varepsilon$. Assume that $A$ satisfies the Robust Null Space Property (RNSP) of order $s$ with parameters $(\\rho,\\tau)$, meaning that for all $v \\in \\mathbb{R}^{n}$ and all index sets $S \\subset \\{1,\\dots,n\\}$ with $|S| \\le s$, one has\n$$\n\\|v_{S}\\|_{2} \\le \\frac{\\rho}{\\sqrt{s}} \\|v_{S^{c}}\\|_{1} + \\tau \\|A v\\|_{2},\n$$\nwhere $0  \\rho  1$ and $\\tau  0$. This property is known to be implied by the Restricted Isometry Property (RIP) under suitable bounds on the RIP constants; you may assume this implication without proof.\n\nLet $\\hat{x}$ be any solution to the constrained $\\ell_{1}$-minimization problem\n$$\n\\min\\{\\|x\\|_{1} : \\|A x - y\\|_{2} \\le \\varepsilon\\}.\n$$\nDefine the best $s$-term approximation error $\\sigma_{s}(x)_{1} := \\min\\{\\|x - z\\|_{1} : \\|z\\|_{0} \\le s\\}$. Starting from the above definitions and standard norm inequalities, derive an upper bound of the form\n$$\n\\|\\hat{x} - x\\|_{1} \\le C_{0} \\,\\sigma_{s}(x)_{1} + C_{1} \\,\\sqrt{s}\\,\\varepsilon,\n$$\nwhere $C_{0}$ and $C_{1}$ are constants that depend only on $\\rho$ and $\\tau$. Provide $C_{0}$ and $C_{1}$ as simplified closed-form expressions in terms of $\\rho$ and $\\tau$. Your final answer should consist solely of the explicit expressions for $C_{0}$ and $C_{1}$.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard derivation within the field of compressed sensing and sparse optimization theory. All necessary givens are provided and are consistent. I will now proceed with the solution.\n\nOur goal is to derive an upper bound on the reconstruction error $\\|\\hat{x} - x\\|_{1}$, where $x \\in \\mathbb{R}^{n}$ is the true signal and $\\hat{x}$ is a solution to the constrained $\\ell_1$-minimization problem:\n$$ \\min_{z \\in \\mathbb{R}^n} \\|z\\|_{1} \\quad \\text{subject to} \\quad \\|A z - y\\|_{2} \\le \\varepsilon $$\nThe measurements are given by $y = A x + e$, with $\\|e\\|_{2} \\le \\varepsilon$.\n\nLet the error vector be $h = \\hat{x} - x$. We seek to bound $\\|h\\|_{1}$.\n\nFirst, we establish a key property arising from the optimality of $\\hat{x}$. Since $\\hat{x}$ is a feasible solution, it satisfies $\\|A\\hat{x} - y\\|_{2} \\le \\varepsilon$. The true signal $x$ is also a feasible solution, because:\n$$ \\|A x - y\\|_{2} = \\|A x - (A x + e)\\|_{2} = \\|-e\\|_{2} = \\|e\\|_{2} \\le \\varepsilon $$\nSince $\\hat{x}$ is a minimizer of the $\\ell_1$-norm over the feasible set, its $\\ell_1$-norm must be less than or equal to that of any other feasible point, including $x$. Thus, we have:\n$$ \\|\\hat{x}\\|_{1} \\le \\|x\\|_{1} $$\nSubstituting $\\hat{x} = x + h$, we get $\\|x + h\\|_{1} \\le \\|x\\|_{1}$.\n\nLet $S_0$ be the index set corresponding to the $s$ largest entries of $x$ in magnitude. Let $S_0^c$ be its complement. Then any vector $z$ which is $s$-sparse has a support of size at most $s$. The best $s$-term approximation error in the $\\ell_1$-norm is defined as $\\sigma_s(x)_1 = \\min_{\\|z\\|_0 \\le s} \\|x-z\\|_1$. This minimum is achieved by setting $z$ to be equal to $x$ on the set $S_0$ and zero elsewhere. Thus, $\\sigma_s(x)_1 = \\|x_{S_0^c}\\|_1$.\n\nWe can decompose the $\\ell_1$-norm of a vector over disjoint index sets. The inequality $\\|x+h\\|_1 \\le \\|x\\|_1$ becomes:\n$$ \\|(x+h)_{S_0}\\|_1 + \\|(x+h)_{S_0^c}\\|_1 \\le \\|x_{S_0}\\|_1 + \\|x_{S_0^c}\\|_1 $$\nUsing the reverse triangle inequality, $\\|a+b\\|_1 \\ge \\|a\\|_1 - \\|b\\|_1$, on both terms on the left:\n$$ (\\|x_{S_0}\\|_1 - \\|h_{S_0}\\|_1) + (\\|h_{S_0^c}\\|_1 - \\|x_{S_0^c}\\|_1) \\le \\|(x+h)_{S_0}\\|_1 + \\|(x+h)_{S_0^c}\\|_1 $$\nCombining these gives:\n$$ \\|x_{S_0}\\|_1 - \\|h_{S_0}\\|_1 + \\|h_{S_0^c}\\|_1 - \\|x_{S_0^c}\\|_1 \\le \\|x_{S_0}\\|_1 + \\|x_{S_0^c}\\|_1 $$\nSimplifying this inequality, we obtain a relation between the error components on $S_0$ and $S_0^c$:\n$$ \\|h_{S_0^c}\\|_1 \\le \\|h_{S_0}\\|_1 + 2\\|x_{S_0^c}\\|_1 $$\nSubstituting $\\sigma_s(x)_1 = \\|x_{S_0^c}\\|_1$, we have:\n$$ \\|h_{S_0^c}\\|_1 \\le \\|h_{S_0}\\|_1 + 2\\sigma_s(x)_1 \\quad (*)$$\n\nNext, we use the Robust Null Space Property (RNSP). The problem states that for any vector $v \\in \\mathbb{R}^n$ and any set $S$ with $|S| \\le s$, the following holds:\n$$ \\|v_S\\|_2 \\le \\frac{\\rho}{\\sqrt{s}} \\|v_{S^c}\\|_1 + \\tau \\|Av\\|_2 $$\nWe apply this property to our error vector $h = \\hat{x} - x$ and the index set $S_0$ defined above. Since $|S_0| \\le s$, this is a valid choice.\n$$ \\|h_{S_0}\\|_2 \\le \\frac{\\rho}{\\sqrt{s}} \\|h_{S_0^c}\\|_1 + \\tau \\|Ah\\|_2 $$\nTo relate this to $\\|h_{S_0}\\|_1$, we use the Cauchy-Schwarz inequality, $\\|v\\|_1 \\le \\sqrt{k}\\|v\\|_2$ for a vector $v \\in \\mathbb{R}^k$. Here, $k = |S_0| \\le s$. Thus, $\\|h_{S_0}\\|_1 \\le \\sqrt{|S_0|}\\|h_{S_0}\\|_2 \\le \\sqrt{s}\\|h_{S_0}\\|_2$.\n$$ \\|h_{S_0}\\|_1 \\le \\sqrt{s} \\left( \\frac{\\rho}{\\sqrt{s}} \\|h_{S_0^c}\\|_1 + \\tau \\|Ah\\|_2 \\right) = \\rho \\|h_{S_0^c}\\|_1 + \\tau\\sqrt{s}\\|Ah\\|_2 $$\nNow we need to bound $\\|Ah\\|_2$. Using the triangle inequality:\n$$ \\|Ah\\|_2 = \\|A\\hat{x} - Ax\\|_2 = \\|(A\\hat{x} - y) - (Ax - y)\\|_2 \\le \\|A\\hat{x} - y\\|_2 + \\|Ax - y\\|_2 $$\nWe know $\\|A\\hat{x} - y\\|_2 \\le \\varepsilon$ from the problem definition, and we showed $\\|Ax - y\\|_2 = \\|e\\|_2 \\le \\varepsilon$. Therefore:\n$$ \\|Ah\\|_2 \\le \\varepsilon + \\varepsilon = 2\\varepsilon $$\nSubstituting this bound into our inequality for $\\|h_{S_0}\\|_1$:\n$$ \\|h_{S_0}\\|_1 \\le \\rho \\|h_{S_0^c}\\|_1 + 2\\tau\\sqrt{s}\\varepsilon \\quad (**) $$\n\nWe now have a system of two inequalities, $(*)$ and $(**)$, for the variables $\\|h_{S_0}\\|_1$ and $\\|h_{S_0^c}\\|_1$. Our goal is to bound $\\|h\\|_1 = \\|h_{S_0}\\|_1 + \\|h_{S_0^c}\\|_1$. We can achieve this by bounding each component separately.\n\nSubstitute inequality $(**)$ into $(*)$:\n$$ \\|h_{S_0^c}\\|_1 \\le (\\rho \\|h_{S_0^c}\\|_1 + 2\\tau\\sqrt{s}\\varepsilon) + 2\\sigma_s(x)_1 $$\nRearranging the terms to solve for $\\|h_{S_0^c}\\|_1$:\n$$ (1-\\rho)\\|h_{S_0^c}\\|_1 \\le 2\\sigma_s(x)_1 + 2\\tau\\sqrt{s}\\varepsilon $$\nSince $0  \\rho  1$, the term $1-\\rho$ is positive, so we can divide by it:\n$$ \\|h_{S_0^c}\\|_1 \\le \\frac{2}{1-\\rho} \\sigma_s(x)_1 + \\frac{2\\tau}{1-\\rho} \\sqrt{s}\\varepsilon $$\n\nNow we find a bound for $\\|h_{S_0}\\|_1$. We substitute the bound we just found for $\\|h_{S_0^c}\\|_1$ into inequality $(**)$:\n$$ \\|h_{S_0}\\|_1 \\le \\rho \\left( \\frac{2}{1-\\rho} \\sigma_s(x)_1 + \\frac{2\\tau}{1-\\rho} \\sqrt{s}\\varepsilon \\right) + 2\\tau\\sqrt{s}\\varepsilon $$\n$$ \\|h_{S_0}\\|_1 \\le \\frac{2\\rho}{1-\\rho} \\sigma_s(x)_1 + \\left( \\frac{2\\rho\\tau}{1-\\rho} + 2\\tau \\right) \\sqrt{s}\\varepsilon $$\nSimplifying the coefficient of the noise term:\n$$ \\frac{2\\rho\\tau}{1-\\rho} + 2\\tau = 2\\tau \\left( \\frac{\\rho}{1-\\rho} + 1 \\right) = 2\\tau \\left( \\frac{\\rho + 1 - \\rho}{1-\\rho} \\right) = \\frac{2\\tau}{1-\\rho} $$\nSo the bound on $\\|h_{S_0}\\|_1$ is:\n$$ \\|h_{S_0}\\|_1 \\le \\frac{2\\rho}{1-\\rho} \\sigma_s(x)_1 + \\frac{2\\tau}{1-\\rho} \\sqrt{s}\\varepsilon $$\n\nFinally, we find the bound for the total error $\\|h\\|_1$ by summing the bounds for its components:\n$$ \\|h\\|_1 = \\|h_{S_0}\\|_1 + \\|h_{S_0^c}\\|_1 $$\n$$ \\|h\\|_1 \\le \\left( \\frac{2\\rho}{1-\\rho} \\sigma_s(x)_1 + \\frac{2\\tau}{1-\\rho} \\sqrt{s}\\varepsilon \\right) + \\left( \\frac{2}{1-\\rho} \\sigma_s(x)_1 + \\frac{2\\tau}{1-\\rho} \\sqrt{s}\\varepsilon \\right) $$\nCombine the terms with $\\sigma_s(x)_1$ and the terms with $\\sqrt{s}\\varepsilon$:\n$$ \\|h\\|_1 \\le \\left( \\frac{2\\rho}{1-\\rho} + \\frac{2}{1-\\rho} \\right) \\sigma_s(x)_1 + \\left( \\frac{2\\tau}{1-\\rho} + \\frac{2\\tau}{1-\\rho} \\right) \\sqrt{s}\\varepsilon $$\n$$ \\|h\\|_1 \\le \\left( \\frac{2(\\rho+1)}{1-\\rho} \\right) \\sigma_s(x)_1 + \\left( \\frac{4\\tau}{1-\\rho} \\right) \\sqrt{s}\\varepsilon $$\n\nThis is the desired upper bound of the form $\\|\\hat{x} - x\\|_{1} \\le C_{0} \\,\\sigma_{s}(x)_{1} + C_{1} \\,\\sqrt{s}\\,\\varepsilon$. By comparing the coefficients, we identify the constants $C_0$ and $C_1$:\n$$ C_0 = \\frac{2(1+\\rho)}{1-\\rho} $$\n$$ C_1 = \\frac{4\\tau}{1-\\rho} $$\nThese constants depend only on $\\rho$ and $\\tau$, as required.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2(1+\\rho)}{1-\\rho}  \\frac{4\\tau}{1-\\rho} \\end{pmatrix}}\n$$"
        }
    ]
}