{
    "hands_on_practices": [
        {
            "introduction": "在实现任何数值算法之前，理解其计算成本是至关重要的一步。这有助于我们评估算法在不同规模问题上的可行性，并识别潜在的计算瓶颈。本练习将引导你对应用于LASSO问题的单次FISTA迭代进行详细的“浮点运算（flop）”计数分析，从而让你对算法的每个组成部分的计算开销有具体而深入的认识。",
            "id": "3446947",
            "problem": "考虑带有稠密矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和数据向量 $b \\in \\mathbb{R}^{m}$ 的最小绝对收缩和选择算子 (LASSO) 问题：\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\n其中 $\\lambda > 0$。快速迭代收缩阈值算法 (FISTA) 是一种近端梯度法，在每次迭代中，它将最近两次的迭代点进行线性组合以形成一个动量点，计算该动量点处光滑二次项的梯度，然后应用 $\\ell_{1}$ 范数的近端算子（软阈值）。\n\n仅使用以下基本要素：\n- 光滑部分为 $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$，其梯度为 $\\nabla f(x) = A^{\\mathsf{T}}(A x - b)$。\n- 非光滑部分为 $g(x) = \\lambda \\|x\\|_{1}$，其在步长 $t > 0$ 时的近端算子是阈值为 $\\tau = \\lambda t$ 的软阈值映射 $S_{\\tau}$，逐分量应用。\n\n假设使用以下计算与计数模型：\n- 矩阵 $A$ 是稠密的，矩阵向量乘积通过朴素的逐行和逐列点积计算：每个长度为 $n$ 的点积使用 $n$ 次乘法和 $n - 1$ 次加法；每个长度为 $m$ 的点积使用 $m$ 次乘法和 $m - 1$ 次加法。\n- 将一次浮点加法或乘法精确地计为一次浮点运算 (flop)。预计算的标量（如 $t$、$1/t$ 或 $\\tau$）可无额外成本地获得。\n- 比较、分支以及绝对值或符号的确定不计入 flop。任何标量除法都通过乘以其倒数来实现。\n- 在最坏情况的算术意义上处理软阈值步骤：对于每个分量，如果它是活跃的（即未被映射到零），它会产生恰好一次与 $\\tau$ 的加法；如果它被映射到零，则不产生算术 flop。为了进行此估算，考虑所有 $n$ 个分量都活跃的最坏情况。\n- 忽略除固定步长 $t = 1/L$（其中 $L$ 是 $\\nabla f$ 的 Lipschitz 常数）之外的任何线搜索或步长更新；忽略任何停止检查。\n\n在这些假设下，一次针对 LASSO 的 FISTA 迭代可以分解为：\n- 将两个 $n$ 维向量进行线性组合以形成动量。\n- 两次矩阵向量乘法，一次与 $A$ 相乘，一次与 $A^{\\mathsf{T}}$ 相乘。\n- 从一个 $m$ 维向量中减去 $m$ 维向量 $b$。\n- 一个梯度步，涉及对一个 $n$ 维向量进行缩放并从一个 $n$ 维向量中减去它。\n- 在阈值 $\\tau$ 下对一个 $n$ 维向量逐分量应用软阈值映射。\n\n根据基本原理和上述计数模型，推导出每次 FISTA 迭代的总 flop 数量，表示为关于 $m$ 和 $n$ 的闭式表达式。你的最终答案必须是单个解析表达式。不需要四舍五入。",
            "solution": "目标是根据指定的计算模型，确定将快速迭代收缩阈值算法 (FISTA) 应用于 LASSO 问题时单次迭代所需的浮点运算 (flop) 总数。FISTA 迭代被分解为五个不同的步骤，我们将计算每个步骤的 flop 数量，然后将它们相加以求得总数。\n\n问题定义为 $\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是稠密矩阵，$b \\in \\mathbb{R}^{m}$ 和 $x \\in \\mathbb{R}^{n}$ 是向量。\n\n我们将分析所提供的 FISTA 迭代分解中的每一步。\n\n1.  **将两个 $n$ 维向量进行线性组合以形成动量：**\n    设最近的两次迭代点为 $x_k \\in \\mathbb{R}^n$ 和 $x_{k-1} \\in \\mathbb{R}^n$。动量点 $y_k$ 通常形成为 $y_k = x_k + \\alpha_k (x_k - x_{k-1})$，其中 $\\alpha_k$ 是一个预计算的标量。这可以分为三个部分来分析：\n    -   向量减法：$d = x_k - x_{k-1}$。这涉及 $n$ 次减法，耗费 $n$ 个 flop。\n    -   标量-向量乘法：$\\alpha_k d$。这涉及 $n$ 次乘法，耗费 $n$ 个 flop。\n    -   向量加法：$x_k + (\\alpha_k d)$。这涉及 $n$ 次加法，耗费 $n$ 个 flop。\n    两种解释都得出此步骤总共需要 $3n$ 个 flop。\n\n2.  **两次矩阵向量乘法和一次向量减法：**\n    这些操作用于计算光滑项的梯度 $\\nabla f(y_k) = A^{\\mathsf{T}}(A y_k - b)$。计算过程如下：\n    -   第一次矩阵向量乘积，$v = A y_k$：矩阵 $A$ 的大小为 $m \\times n$，$y_k$ 是一个 $n$ 维向量。这需要计算 $m$ 个长度为 $n$ 的点积。根据指定的模型，一个长度为 $n$ 的点积耗费 $n$ 次乘法和 $n-1$ 次加法，总计 $2n-1$ 个 flop。此操作的总成本是 $m(2n-1) = 2mn - m$ 个 flop。\n    -   向量减法，$u = v - b$：这里，$v$ 和 $b$ 都是 $m$ 维向量。此操作需要 $m$ 次减法，耗费 $m$ 个 flop。\n    -   第二次矩阵向量乘积，$g = A^{\\mathsf{T}} u$：矩阵 $A^{\\mathsf{T}}$ 的大小为 $n \\times m$，$u$ 是一个 $m$ 维向量。这需要计算 $n$ 个长度为 $m$ 的点积。一个长度为 $m$ 的点积耗费 $m$ 次乘法和 $m-1$ 次加法，总计 $2m-1$ 个 flop。总成本是 $n(2m-1) = 2mn - n$ 个 flop。\n    这三个相关操作的总成本是 $(2mn - m) + m + (2mn - n) = 4mn - n$ 个 flop。\n\n3.  **一个梯度步，涉及对一个 $n$ 维向量进行缩放并从一个 $n$ 维向量中减去它：**\n    此步骤对应于用梯度信息更新动量点：$z_k = y_k - t \\nabla f(y_k)$，其中 $t$ 是步长（一个预计算的标量），$\\nabla f(y_k)$ 是先前计算的 $n$ 维向量。\n    -   标量-向量乘法：$t \\nabla f(y_k)$。这涉及用一个标量缩放一个 $n$ 维向量，耗费 $n$ 次乘法。\n    -   向量减法：$y_k - (t \\nabla f(y_k))$。这涉及两个 $n$ 维向量相减，耗费 $n$ 次减法。\n    此步骤的总成本是 $n + n = 2n$ 个 flop。\n\n4.  **对一个 $n$ 维向量逐分量应用软阈值映射：**\n    这是近端步骤，$x_{k+1} = S_{\\tau}(z_k)$，其中 $\\tau = \\lambda t$。问题陈述为该操作提供了一个特定的计数模型：在所有 $n$ 个分量都活跃（即未被映射到零）的最坏情况下，每个分量的成本恰好是一次加法/减法。任何其他操作，如比较、绝对值或符号确定，都不计算在内。\n    因此，在最坏情况的假设下，此步骤的总成本是 $n \\times 1 = n$ 个 flop。\n\n**总 Flop 数量：**\n每次 FISTA 迭代的总 flop 数量是这些步骤中每个步骤 flop 的总和。\n总 Flops = (动量形成) + (梯度计算) + (梯度步) + (软阈值)\n总 Flops = $(3n) + (4mn - n) + (2n) + (n)$\n总 Flops = $4mn + (3 - 1 + 2 + 1)n$\n总 Flops = $4mn + 5n$\n因此，在给定的模型和假设下，每次 FISTA 迭代的精确总 flop 数量是 $4mn + 5n$。",
            "answer": "$$\n\\boxed{4mn + 5n}\n$$"
        },
        {
            "introduction": "FISTA的收敛性理论与一个关键参数紧密相关：光滑项梯度的利普希茨常数$L$。理论上，为保证收敛，步长$t$应满足$t \\le 1/L$。这个动手实践旨在通过数值模拟，让你直观地探索步长选择对算法稳定性的影响，特别是当你低估$L$（导致步长过大）时，你将观察到算法可能出现的发散行为。",
            "id": "3446919",
            "problem": "考虑压缩感知中的稀疏优化问题，其复合目标由一个光滑的二次数据拟合项和一个非光滑的稀疏性促进项组成。设光滑部分由矩阵-向量映射及其梯度定义，非光滑部分为逐项绝对值惩罚。迭代收縮阈值算法 (ISTA) 和快速迭代收縮阈值算法 (FISTA) 是近端梯度方法，其步長选择取决于光滑部分梯度的 Lipschitz 常数。您的任务是根据指定的奇异值计算 Lipschitz 常数，然后模拟低估或高估此常数对 ISTA 和 FISTA 步骤稳定性的影响。\n\n从以下基本基础开始：\n- 光滑部分是二次函数 $f(x) = \\tfrac{1}{2}\\lVert A x - b \\rVert_2^2$，其梯度 $\\nabla f(x)$ 对所有 $x$ 都存在。\n- 复合目标是 $F(x) = f(x) + \\lambda \\lVert x \\rVert_1$，其中 $\\lambda > 0$ 是一个标量。\n- 近端梯度方法论确保，对于 ISTA 算法作用于 $F(x)$，步长 $t$ 至多为 $\\nabla f$ 的 Lipschitz 常数的倒数时，能够保证其下降性质。\n\n您必须：\n- 仅使用提供的 $A$ 的奇异值，基于关于二次型梯度和奇异值与算子范数之间关系的基本事实，计算 $\\nabla f$ 的 Lipschitz 常数 $L$。\n- 使用标准正交因子构造一个具有指定奇异值的大小为 $m \\times n$ 的矩阵 $A$，并生成一个稀疏的真实值 $x^\\star$ 和带有少量加性噪声的观测向量 $b$。\n- 从基本原理出发，将迭代收缩阈值算法 (ISTA) 和快速迭代收缩阈值算法 (FISTA) 实现为近端梯度方法。\n- 对于给定的估计 Lipschitz 常数 $\\widehat{L}$，在 ISTA 和 FISTA 中均使用步长 $t = 1 / \\widehat{L}$。\n- 为每种算法定义一个稳定性指标如下：如果目标值序列 $\\{F(x_k)\\}_{k=0}^{K}$ 在数值容差 $\\tau$ 内非递增，即对于所有 $k \\in \\{0, 1, \\dots, K-1\\}$ 都有 $F(x_{k+1}) \\leq F(x_k) + \\tau$，其中 $\\tau = 10^{-12}$，则该序列是稳定的。\n- 分析对应于精确、高估和低估的 $\\widehat{L}$ 值的步长选择对 ISTA 和 FISTA 稳定性的影响。\n\n矩阵构造和数据生成细节：\n- 对所有测试用例使用 $m = 10$ 和 $n = 10$。构造 $A = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是标准正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是对角矩阵，其对角线上为指定的奇异值。通过对使用固定种子的伪随机高斯矩阵进行标准正交化来生成 $U$ 和 $V$。\n- 使用稀疏真实值 $x^\\star \\in \\mathbb{R}^{n}$，其在索引 $0$、$3$ 和 $7$ 处的非零项值分别为 $3.0$、$-2.0$ 和 $1.5$，其余项为零。即，$x^\\star = [3.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0.0]$。\n- 生成 $b = A x^\\star + \\eta$，其中 $\\eta \\in \\mathbb{R}^{m}$ 是均值为 $0$、标准差为 $\\sigma = 0.01$ 的高斯噪声。\n- 使用正则化参数 $\\lambda = 0.1$。\n\n要实现的算法：\n- 为目标函数 $F(x) = \\tfrac{1}{2}\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1$ 实现 ISTA 和 FISTA，ISTA 的初始化为 $x_0 = 0$，FISTA 的初始化为 $x_0 = y_0 = 0$，两者的最大迭代次数均为 $K = 200$。对于 FISTA，动量参数使用标准的加速序列。\n\n测试套件：\n提供以下三个测试用例，每个用例都指定了 $A$ 的奇异值列表和一个用于可复现性的伪随机种子，并评估四种 $\\widehat{L}$ 的选择：精确值 $\\widehat{L} = L$，高估值 $\\widehat{L} = 2 L$，以及低估值 $\\widehat{L} = 0.5 L$ 和 $\\widehat{L} = 0.2 L$。\n\n- 测试用例 1 (良态)：\n  - 奇异值：$[1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1.0, 0.9, 0.8, 0.7]$。\n  - 用于矩阵构造和噪声的种子：$0$。\n- 测试用例 2 (病态)：\n  - 奇异值：$[3.0, 1.0, 0.3, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]$。\n  - 用于矩阵构造和噪声的种子：$1$。\n- 测试用例 3 (秩亏)：\n  - 奇异值：$[2.0, 1.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$。\n  - 用于矩阵构造和噪声的种子：$2$。\n\n对于每个测试用例，从奇异值计算 $L$，并对于 $\\widehat{L} \\in \\{L, 2 L, 0.5 L, 0.2 L\\}$ 的每种选择，使用 $t = 1 / \\widehat{L}$ 运行 ISTA 和 FISTA。对于每次运行，计算目标序列 $\\{F(x_k)\\}_{k=0}^{K}$ 和上面定义的稳定性指标。\n\n最终输出格式：\n您的程序应生成包含三个测试用例结果列表的单行输出。每个测试用例的结果必须是一个包含九个条目的列表：\n$[L,\\ \\text{ISTA\\_stable\\_exact},\\ \\text{FISTA\\_stable\\_exact},\\ \\text{ISTA\\_stable\\_over},\\ \\text{FISTA\\_stable\\_over},\\ \\text{ISTA\\_stable\\_under0.5},\\ \\text{FISTA\\_stable\\_under0.5},\\ \\text{ISTA\\_stable\\_under0.2},\\ \\text{FISTA\\_stable\\_under0.2}]$，\n其中 $L$ 是一个浮点数，每个稳定性指标是一个布尔值。程序必须将这三个按测试用例组织的列表作为逗号分隔的列表打印出来，并用方括号括起来，例如 `[[\\dots],[\\dots],[\\dots]]`。",
            "solution": "该任务是分析两种近端梯度方法，即迭代收缩阈值算法 (ISTA) 和快速迭代收缩阈值算法 (FISTA)，在应用于稀疏优化问题时的稳定性。稳定性评估是针对步长的选择进行的，步长由问题光滑部分梯度的 Lipschitz 常数 $L$ 的估计值 $\\widehat{L}$ 确定。\n\n复合目标函数由 $F(x) = f(x) + g(x)$ 给出，其中 $f(x)$ 是光滑的数据保真项，$g(x)$ 是非光滑的正则化项。\n- 光滑部分是最小二乘代价函数：$f(x) = \\frac{1}{2}\\lVert A x - b \\rVert_2^2$，其中矩阵 $A \\in \\mathbb{R}^{m \\times n}$，观测向量 $b \\in \\mathbb{R}^{m}$，变量 $x \\in \\mathbb{R}^{n}$。\n- 非光滑部分是 $\\ell_1$-范数，它促进稀疏性：$g(x) = \\lambda \\lVert x \\rVert_1$，正则化参数为 $\\lambda > 0$。\n\n解决方案分三个阶段进行：首先，我们推导 Lipschitz 常数 $L$ 的公式；其次，我们具体说明 ISTA 和 FISTA 算法；第三，我们概述用于测试稳定性的数值实验。\n\n**1. Lipschitz 常数的推导**\n\n像 ISTA 和 FISTA 这样的近端梯度方法的收敛性依赖于合适的步长选择，其上界为 $\\nabla f$ 的 Lipschitz 常数的倒数。一个函数 $\\nabla f: \\mathbb{R}^n \\to \\mathbb{R}^n$ 是 Lipschitz 连续的，如果对于所有 $x, y \\in \\mathbb{R}^n$，存在常数 $L$ 使得：\n$$ \\lVert \\nabla f(x) - \\nabla f(y) \\rVert_2 \\leq L \\lVert x - y \\rVert_2 $$\n满足此条件的最小 $L$ 即为 Lipschitz 常数。\n\n首先，我们计算 $f(x)$ 的梯度：\n$$ f(x) = \\frac{1}{2}(Ax-b)^\\top(Ax-b) = \\frac{1}{2}(x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b) $$\n关于 $x$ 的梯度是：\n$$ \\nabla f(x) = A^\\top A x - A^\\top b = A^\\top(Ax-b) $$\n接下来，我们计算差值 $\\nabla f(x) - \\nabla f(y)$：\n$$ \\nabla f(x) - \\nabla f(y) = (A^\\top A x - A^\\top b) - (A^\\top A y - A^\\top b) = A^\\top A (x - y) $$\n将此代入 Lipschitz 条件中：\n$$ \\lVert A^\\top A (x - y) \\rVert_2 \\leq L \\lVert x - y \\rVert_2 $$\n这个不等式必须对所有 $x, y$ 成立。这等价于求矩阵 $A^\\top A$ 的算子范数（或谱范数）：\n$$ L = \\lVert A^\\top A \\rVert_2 = \\sup_{z \\neq 0} \\frac{\\lVert A^\\top A z \\rVert_2}{\\lVert z \\rVert_2} $$\n像 $A^\\top A$ 这样的对称半正定矩阵的算子范数是其最大特征值。设 $A$ 的奇异值为 $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_{\\min(m,n)} \\geq 0$。$A^\\top A$ 的特征值是 $A$ 的奇异值的平方。因此，$A^\\top A$ 的最大特征值是 $A$ 的最大奇异值的平方，记为 $\\sigma_{\\max}(A)$。\n所以，Lipschitz 常数是：\n$$ L = \\sigma_{\\max}(A)^2 $$\n对于每个测试用例，将通过从提供的奇异值列表中取最大值并将其平方来计算 $L$。\n\n**2. ISTA 和 FISTA 算法**\n\nISTA 和 FISTA 都是近端梯度方法的实例，该方法生成一个收敛到 $F(x)$ 最小值的迭代序列 $\\{x_k\\}$。该方法的核心是近端算子。对于 $g(x) = \\lambda \\lVert x \\rVert_1$，近端算子是软阈值函数 $S_{\\gamma}(z)$：\n$$ \\text{prox}_{\\gamma g}(z) = S_{\\gamma}(z) $$\n其中，对于向量 $z$ 的每个分量 $i$，$(S_{\\gamma}(z))_i = \\text{sign}(z_i) \\max(|z_i| - \\gamma, 0)$。\n\n一般的更新步骤包括在光滑部分 $f(x)$ 上进行梯度下降步骤，然后在非光滑部分 $g(x)$ 上进行近端步骤。两种算法都使用固定的步长 $t = 1/\\widehat{L}$，其中 $\\widehat{L}$ 是真实 Lipschitz 常数 $L$ 的一个估计值。\n\n**迭代收缩阈值算法 (ISTA)**\n给定初始猜测 $x_0$，ISTA 的更新规则是：\n$$ x_{k+1} = S_{t\\lambda}(x_k - t \\nabla f(x_k)) = S_{t\\lambda}(x_k - t A^\\top(Ax_k - b)) $$\n对于步长 $t \\leq 1/L$，ISTA 被保证为下降算法，即 $F(x_{k+1}) \\leq F(x_k)$。\n\n**快速迭代收缩阈值算法 (FISTA)**\nFISTA 通过引入动量项来加速 ISTA。它使用一个辅助序列 $\\{y_k\\}$ 和一个动量参数序列 $\\{\\theta_k\\}$。\n初始化 $x_0 = y_0$ 和 $\\theta_0=1$：\n\\begin{align*}\nx_{k+1} = S_{t\\lambda}(y_k - t \\nabla f(y_k)) = S_{t\\lambda}(y_k - t A^\\top(Ay_k - b)) \\\\\n\\theta_{k+1} = \\frac{1 + \\sqrt{1 + 4\\theta_k^2}}{2} \\\\\ny_{k+1} = x_{k+1} + \\frac{\\theta_k - 1}{\\theta_{k+1}}(x_{k+1} - x_k)\n\\end{align*}\n只要 $t \\leq 1/L$，FISTA 在目标值误差上能达到更快的收敛速率 $O(1/k^2)$。然而，与 ISTA 不同，FISTA 不是一个下降方法；即使使用有效的步长，目标值序列 $\\{F(x_k)\\}$ 也不能保证是非递增的。可能会出现振荡，尽管总体趋势是朝向最小值下降。\n\n**3. 模拟与稳定性分析**\n\n问题要求构造一个具有指定奇异值的矩阵 $A \\in \\mathbb{R}^{10 \\times 10}$。这通过构建奇异值分解 $A = U \\Sigma V^\\top$ 来完成，其中 $\\Sigma$ 是指定奇异值的对角矩阵，$U, V$ 是通过对高斯随机矩阵进行 QR 分解生成的随机标准正交矩阵。定义一个稀疏的真实值向量 $x^\\star$，并将测量向量 $b$ 生成为 $b = A x^\\star + \\eta$，其中 $\\eta$ 是一个小型高斯噪声向量。\n\n对于三个测试用例（良态、病态和秩亏）中的每一个，我们执行以下分析：\n1. 计算真实的 Lipschitz 常数 $L = \\sigma_{\\max}(A)^2$。\n2. 对于每个估计值 $\\widehat{L} \\in \\{L, 2L, 0.5L, 0.2L\\}$，设置步长 $t = 1/\\widehat{L}$。\n3. 从 $x_0 = 0$ (以及 FISTA 的 $y_0 = 0$) 开始，运行 ISTA 和 FISTA $K=200$ 次迭代。\n4. 对每次运行，记录目标函数值序列 $\\{F(x_k)\\}_{k=0}^{K}$。\n5. 使用给定的标准评估此序列的稳定性：如果对于所有 $k \\in \\{0, 1, \\dots, K-1\\}$ 都有 $F(x_{k+1}) \\leq F(x_k) + \\tau$，其中 $\\tau = 10^{-12}$，则该序列是稳定的。\n\n预期的结果是，对于 $\\widehat{L} \\geq L$ (即 $t \\leq 1/L$)，算法将是稳定的，因为这满足了收敛的理论条件。对于 $\\widehat{L}  L$ (即 $t  1/L$)，步长过大，违反了收敛条件，预计算法将变得不稳定，在目标函数值上表现出振荡或发散行为。如前所述，由于其非单调性，FISTA 即使在 $t \\le 1/L$ 的情况下也可能无法通过严格的稳定性测试。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of analyzing ISTA and FISTA stability for different step-sizes.\n    \"\"\"\n\n    def soft_threshold(z, gamma):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - gamma, 0)\n\n    def objective_function(x, A, b, lambda_reg):\n        \"\"\"Computes the objective function value F(x).\"\"\"\n        return 0.5 * np.linalg.norm(A @ x - b)**2 + lambda_reg * np.linalg.norm(x, 1)\n\n    def ista(A, b, lambda_reg, t, K):\n        \"\"\"Iterative Shrinkage-Thresholding Algorithm.\"\"\"\n        n = A.shape[1]\n        x_k = np.zeros(n)\n        obj_vals = [objective_function(x_k, A, b, lambda_reg)]\n        for _ in range(K):\n            grad_x = A.T @ (A @ x_k - b)\n            x_k = soft_threshold(x_k - t * grad_x, t * lambda_reg)\n            obj_vals.append(objective_function(x_k, A, b, lambda_reg))\n        return obj_vals\n\n    def fista(A, b, lambda_reg, t, K):\n        \"\"\"Fast Iterative Shrinkage-Thresholding Algorithm.\"\"\"\n        n = A.shape[1]\n        x_k = np.zeros(n)\n        y_k = np.zeros(n)\n        theta_k = 1.0\n        obj_vals = [objective_function(x_k, A, b, lambda_reg)]\n        for _ in range(K):\n            x_prev = x_k\n            grad_y = A.T @ (A @ y_k - b)\n            x_k = soft_threshold(y_k - t * grad_y, t * lambda_reg)\n            \n            theta_k_plus_1 = (1.0 + np.sqrt(1.0 + 4.0 * theta_k**2)) / 2.0\n            y_k = x_k + ((theta_k - 1.0) / theta_k_plus_1) * (x_k - x_prev)\n            \n            theta_k = theta_k_plus_1\n            obj_vals.append(objective_function(x_k, A, b, lambda_reg))\n        return obj_vals\n\n    def check_stability(obj_vals, K, tau):\n        \"\"\"Checks if the objective function sequence is non-increasing.\"\"\"\n        for k in range(K):\n            if obj_vals[k+1] > obj_vals[k] + tau:\n                return False\n        return True\n\n    m, n = 10, 10\n    x_star = np.zeros(n)\n    x_star[0] = 3.0\n    x_star[3] = -2.0\n    x_star[7] = 1.5\n    sigma = 0.01\n    lambda_reg = 0.1\n    K = 200\n    tau = 1e-12\n\n    test_cases = [\n        # Test Case 1: well-conditioned\n        ([1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1.0, 0.9, 0.8, 0.7], 0),\n        # Test Case 2: ill-conditioned\n        ([3.0, 1.0, 0.3, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001], 1),\n        # Test Case 3: rank-deficient\n        ([2.0, 1.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2)\n    ]\n\n    all_results = []\n\n    for s_vals, seed in test_cases:\n        np.random.seed(seed)\n        \n        # Matrix Construction from specified singular values\n        s_vals_np = np.array(s_vals)\n        Sigma = np.zeros((m, n))\n        np.fill_diagonal(Sigma, s_vals_np)\n        \n        U, _ = np.linalg.qr(np.random.randn(m, m))\n        V, _ = np.linalg.qr(np.random.randn(n, n))\n        A = U @ Sigma @ V.T\n\n        # Data Generation\n        eta = np.random.normal(0, sigma, size=m)\n        b = A @ x_star + eta\n\n        # Compute true Lipschitz constant\n        L = np.max(s_vals_np)**2 if len(s_vals_np) > 0 else 0.0\n\n        # Define estimated Lipschitz constants to test\n        L_hat_factors = [1.0, 2.0, 0.5, 0.2]\n        \n        case_results = [L]\n\n        for factor in L_hat_factors:\n            L_hat = factor * L\n            \n            # A zero Lipschitz constant implies a zero matrix A (if b=0), or constant gradient\n            # step-size would be infinite. We consider this case unstable.\n            if L_hat  1e-15:\n                ista_stable = False\n                fista_stable = False\n            else:\n                t = 1.0 / L_hat\n\n                # Run ISTA and check stability\n                ista_obj_vals = ista(A, b, lambda_reg, t, K)\n                ista_stable = check_stability(ista_obj_vals, K, tau)\n\n                # Run FISTA and check stability\n                fista_obj_vals = fista(A, b, lambda_reg, t, K)\n                fista_stable = check_stability(fista_obj_vals, K, tau)\n            \n            case_results.extend([ista_stable, fista_stable])\n\n        all_results.append(case_results)\n\n    # Format and print the final output as a single-line string representation of the list\n    print(str(all_results))\n\nsolve()\n```"
        },
        {
            "introduction": "在许多实际应用中，计算精确的利普希茨常数$L$本身可能就是一个难题，甚至比求解原问题还要昂贵。为了解决这个问题，带有回溯线搜索功能的FISTA应运而生。本练习将指导你实现这一更为通用和稳健的算法版本，它能够自适应地寻找合适的步长，从而无需预先知道$L$的精确值。",
            "id": "3446933",
            "problem": "你需要实现一个基于凸分析的数值方法，用于解决一系列合成的稀疏重构问题实例。考虑最小绝对收缩和选择算子 (LASSO) 的目标函数\n$$\n\\min_{x \\in \\mathbb{R}^n} F(x) \\equiv g(x) + h(x),\n$$\n其中光滑部分为\n$$\ng(x) \\equiv \\tfrac{1}{2}\\|A x - b\\|_2^2\n$$\n其梯度为\n$$\n\\nabla g(x) = A^\\top (A x - b),\n$$\n非光滑正则化项是缩放的1-范数\n$$\nh(x) \\equiv \\lambda \\|x\\|_1.\n$$\n$g$ 的梯度是全局利普希茨连续的，存在某个常数 $L^\\star \\in (0,\\infty)$ 满足\n$$\n\\|\\nabla g(x) - \\nabla g(y)\\|_2 \\le L^\\star \\|x - y\\|_2\n$$\n对所有 $x,y \\in \\mathbb{R}^n$ 成立。在此背景下，带回溯的快速迭代收缩阈值算法 (FISTA) 使用以下基本要素：\n- 由 $\\nabla g$ 的利普希茨连续性所蕴含的二次上界，它为接受候选步长提供了充分下降条件的依据。\n- 缩放的1-范数的近端算子，\n$$\n\\operatorname{prox}_{\\tau \\|\\cdot\\|_1}(v) = \\arg\\min_{x \\in \\mathbb{R}^n}\\left\\{\\tfrac{1}{2}\\|x - v\\|_2^2 + \\tau \\|x\\|_1\\right\\},\n$$\n即软阈值映射。\n- Nesterov加速序列，其中 $t_1 = 1$，并根据 $t_k$ 构建外推。\n\n你的任务是：\n- 从上述基础出发，根据 $\\nabla g$ 的利普希茨性质推导出用于回溯的充分下降条件，以验证当前外推点周围 $g$ 的局部二次上界模型。\n- 针对 LASSO 目标函数 $F(x)$，实现带回溯的快速迭代收缩阈值算法 (FISTA)，并假设利普希茨常数 $L^\\star$ 未知。使用 $x_1 = 0$、$y_1 = x_1$ 和 $t_1 = 1$进行初始化。在每个外部迭代 $k \\in \\{1,2,\\dots\\}$ 中，执行回溯线搜索，该搜索从当前估计值 $L_k$ 开始，并将 $L_k$ 乘以一个因子 $\\eta  1$，直到满足充分下降条件。使用与 $h$ 相关的近端映射，根据外推点和当前的 $L_k$ 形成近端梯度候选点。使用 $t_{k+1} = \\tfrac{1 + \\sqrt{1 + 4 t_k^2}}{2}$ 和常规外推法来维持标准的 FISTA 加速更新。\n- 在迭代 $k$ 被接受之前，每次回溯线搜索将 $L_k$ 乘以 $\\eta$ 时，记为一次“$L_k$ 的增加”。记录在前 $10$ 次外部迭代（即 $k \\in \\{1,\\dots,10\\}$）期间发生的此类增加的总次数。\n\n不涉及物理单位。所有角度（如有）均假定为弧度。所有答案必须是实数值。\n\n测试套件和要求输出：\n实现你的程序以运行以下三个合成测试用例。在每个用例中，通过 $b = A x_{\\mathrm{true}}$ 从一个植入向量 $x_{\\mathrm{true}}$ 生成 $b$（无噪声）。对每个用例，运行带回溯的 FISTA 恰好 $10$ 次外部迭代，并报告在这 $10$ 次迭代中发生的 $L_k$ 增加的总次数。\n\n- 用例 1（理想情况，适度回溯）：\n  - $A \\in \\mathbb{R}^{6 \\times 10}$:\n    $$\n    A = \\begin{bmatrix}\n    1  0  0  0  0  0  0.5  -0.2  0.3  0.1 \\\\\n    0  1  0  0  0  0  -0.1  0.4  0.2  -0.3 \\\\\n    0  0  1  0  0  0  0.3  -0.4  0.1  0.2 \\\\\n    0  0  0  1  0  0  -0.2  0.1  0.5  -0.1 \\\\\n    0  0  0  0  1  0  0.4  0.3  -0.2  0.2 \\\\\n    0  0  0  0  0  1  -0.3  0.2  0.1  0.4\n    \\end{bmatrix}.\n    $$\n  - $x_{\\mathrm{true}} \\in \\mathbb{R}^{10}$ 其条目为\n    $$\n    x_{\\mathrm{true}} = \\begin{bmatrix} 0  1.2  0  -0.7  0  0  0  0  2.0  0 \\end{bmatrix}^\\top.\n    $$\n  - $b = A x_{\\mathrm{true}}$。\n  - $\\lambda = 0.05$，初始 $L_1 = 0.1$，回溯因子 $\\eta = 2.0$。\n\n- 用例 2（边界情况，无需回溯）：\n  - $A$ 和 $x_{\\mathrm{true}}$ 与用例 1 中相同，且 $b = A x_{\\mathrm{true}}$。\n  - $\\lambda = 0.05$，初始 $L_1 = 100.0$，回溯因子 $\\eta = 2.0$。\n\n- 用例 3（边缘情况，由于 $\\eta$ 值较小和算子范数较大导致多次小幅增加）：\n  - $A \\in \\mathbb{R}^{8 \\times 12}$:\n    $$\n    A = \\begin{bmatrix}\n    3.0  -2.0  1.0  0.0  1.0  -1.0  2.0  -3.0  1.5  -0.5  0.5  -1.5 \\\\\n    0.0  1.0  -1.0  2.0  -2.0  1.0  -1.5  2.5  -1.0  0.5  -0.5  1.0 \\\\\n    1.5  -1.0  0.5  -0.5  1.0  -1.5  2.0  -2.0  1.0  -1.0  0.5  -0.5 \\\\\n    -1.0  2.0  -2.0  1.0  0.0  1.0  -2.5  3.0  -1.5  1.0  -1.0  0.5 \\\\\n    2.0  -1.0  1.5  -1.0  2.0  -2.0  1.0  -1.0  0.5  -0.5  1.0  -1.5 \\\\\n    -2.0  1.0  -0.5  1.5  -1.0  2.0  -1.0  1.0  -0.5  0.5  -1.0  1.0 \\\\\n    1.0  0.5  -1.5  2.0  -2.5  1.5  -1.0  2.0  -1.0  0.5  -0.5  1.0 \\\\\n    -1.5  2.5  -1.0  0.5  1.0  -1.0  2.0  -3.0  1.5  -1.0  1.0  -0.5\n    \\end{bmatrix}.\n    $$\n  - $x_{\\mathrm{true}} \\in \\mathbb{R}^{12}$ 其条目为\n    $$\n    x_{\\mathrm{true}} = \\begin{bmatrix} 0  0  2.0  -1.0  0  0  0  3.0  0  0  0  0.5 \\end{bmatrix}^\\top.\n    $$\n  - $b = A x_{\\mathrm{true}}$。\n  - $\\lambda = 0.1$，初始 $L_1 = 1.0$，回溯因子 $\\eta = 1.1$。\n\n对每个用例，运行带回溯的 FISTA 恰好 $10$ 次外部迭代，并计算在这 $10$ 次迭代中 $L_k$ 乘以 $\\eta$ 的总次数。你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$）。每个结果必须是代表其相应测试用例计数的整数。不应打印任何附加文本。",
            "solution": "该问题要求实现带回溯线搜索的快速迭代收缩阈值算法 (FISTA)，以解决 LASSO 优化问题。主要任务是计算在固定迭代次数内，回溯过程中利普希茨常数估计值增加的次数。\n\nLASSO 目标函数由 $F(x) = g(x) + h(x)$ 给出，其中 $g(x) = \\frac{1}{2}\\|A x - b\\|_2^2$ 是光滑的数据保真项，$h(x) = \\lambda \\|x\\|_1$ 是非光滑的稀疏性诱导正则化项。\n\n首先，我们推导回溯线搜索中使用的充分下降条件。光滑项的梯度为 $\\nabla g(x) = A^\\top (A x - b)$。已知 $\\nabla g$ 是利普希茨连续的，常数为 $L^\\star$，这意味着对于任何 $L \\ge L^\\star$ 以下不等式成立：\n$$\ng(z) \\le g(y) + \\langle \\nabla g(y), z - y \\rangle + \\frac{L}{2} \\|z - y\\|_2^2\n$$\n这个不等式被称为下降引理，它为函数 $g$ 在点 $y$ 附近的点 $z$ 处提供了一个二次上界。\n\nFISTA 是一种近端梯度方法，它将标准的近端梯度步与 Nesterov 风格的加速相结合。在每次迭代 $k$ 中，给定一个外推点 $y_k$，算法通过最小化 $F(x)$ 在 $y_k$ 周围的二次近似来寻找新点 $x_k$。这种最小化过程导出了近端梯度更新：\n$$\nx_k = \\operatorname{prox}_{h/L}(y_k - \\frac{1}{L}\\nabla g(y_k))\n$$\n其中 $L$ 是利普希茨常数估计值，$\\operatorname{prox}_{\\tau h}(\\cdot)$ 是函数 $\\tau h$ 的近端算子。对于 $h(x) = \\lambda \\|x\\|_1$，近端步骤涉及软阈值算子 $S_{\\tau}(v)_i = \\operatorname{sign}(v_i)\\max(|v_i|-\\tau, 0)$，因此：\n$$\nx_k = S_{\\lambda/L}\\left(y_k - \\frac{1}{L}A^\\top(Ay_k - b)\\right)\n$$\n由于真实的利普希茨常数 $L^\\star$ 未知，因此采用回溯线搜索在每次迭代 $k$ 中寻找合适的步长 $1/L_k$。从 $L_k$ 的一个估计值（例如，前一次迭代的值 $L_{k-1}$）开始，我们检查二次上界是否对我们的候选点 $x_k$ 成立。通过在下降引理中代入 $z=x_k$ 和 $y=y_k$，直接推导出充分下降条件。我们要求所选的 $L_k$ 满足：\n$$\ng(x_k) \\le g(y_k) + \\langle \\nabla g(y_k), x_k - y_k \\rangle + \\frac{L_k}{2} \\|x_k - y_k\\|_2^2\n$$\n如果此条件不满足，则估计值 $L_k$ 过小。我们将其乘以一个因子 $\\eta  1$（即 $L_k \\leftarrow \\eta L_k$），用新的 $L_k$ 重新计算候选点 $x_k$，并再次检查该条件。重复此过程直到条件满足。每次乘以 $\\eta$ 都构成一次我们必须计数的“增加”。\n\n完整的带回溯的 FISTA 算法流程如下：\n\n1.  **初始化**：给定利普希茨常数的初始猜测值 $L_0$、回溯因子 $\\eta  1$。设置 $x_0 = 0$，$y_1 = x_0$ 和 $t_1 = 1$。设增加的总次数为 $C = 0$。\n\n2.  **迭代**：对于 $k = 1, 2, \\ldots, 10$：\n    a. **回溯线搜索**：\n        i. 从一个试验利普希茨常数开始，例如 $L_{trial} = L_{k-1}$。\n        ii. 计算候选点 $x_{k, trial} = S_{\\lambda/L_{trial}}\\left(y_k - \\frac{1}{L_{trial}}\\nabla g(y_k)\\right)$。\n        iii. 检查充分下降条件：如果 $g(x_{k, trial})  g(y_k) + \\langle \\nabla g(y_k), x_{k, trial} - y_k \\rangle + \\frac{L_{trial}}{2} \\|x_{k, trial} - y_k\\|_2^2$，则更新 $L_{trial} \\leftarrow \\eta L_{trial}$，计数器 $C \\leftarrow C+1$，然后返回步骤 (ii)。\n        iv. 一旦条件满足，设置 $L_k = L_{trial}$ 和 $x_k = x_{k, trial}$。\n\n    b. **加速步骤**：更新动量项：\n        i. $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$。\n        ii. $y_{k+1} = x_k + \\frac{t_k - 1}{t_{k+1}}(x_k - x_{k-1})$。\n\n3.  经过 $10$ 次迭代后，计数器 $C$ 的最终值即为给定测试用例的结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs FISTA with backtracking on a suite of test cases and reports the\n    total number of backtracking steps (increases of L).\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"A\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, -0.2, 0.3, 0.1],\n                [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -0.1, 0.4, 0.2, -0.3],\n                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, -0.4, 0.1, 0.2],\n                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, -0.2, 0.1, 0.5, -0.1],\n                [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.3, -0.2, 0.2],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -0.3, 0.2, 0.1, 0.4]\n            ]),\n            \"x_true\": np.array([0.0, 1.2, 0.0, -0.7, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0]),\n            \"lam\": 0.05,\n            \"L_initial\": 0.1,\n            \"eta\": 2.0\n        },\n        {\n            \"name\": \"Case 2\",\n            \"A\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, -0.2, 0.3, 0.1],\n                [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -0.1, 0.4, 0.2, -0.3],\n                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, -0.4, 0.1, 0.2],\n                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, -0.2, 0.1, 0.5, -0.1],\n                [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.3, -0.2, 0.2],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -0.3, 0.2, 0.1, 0.4]\n            ]),\n            \"x_true\": np.array([0.0, 1.2, 0.0, -0.7, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0]),\n            \"lam\": 0.05,\n            \"L_initial\": 100.0,\n            \"eta\": 2.0\n        },\n        {\n            \"name\": \"Case 3\",\n            \"A\": np.array([\n                [3.0, -2.0, 1.0, 0.0, 1.0, -1.0, 2.0, -3.0, 1.5, -0.5, 0.5, -1.5],\n                [0.0, 1.0, -1.0, 2.0, -2.0, 1.0, -1.5, 2.5, -1.0, 0.5, -0.5, 1.0],\n                [1.5, -1.0, 0.5, -0.5, 1.0, -1.5, 2.0, -2.0, 1.0, -1.0, 0.5, -0.5],\n                [-1.0, 2.0, -2.0, 1.0, 0.0, 1.0, -2.5, 3.0, -1.5, 1.0, -1.0, 0.5],\n                [2.0, -1.0, 1.5, -1.0, 2.0, -2.0, 1.0, -1.0, 0.5, -0.5, 1.0, -1.5],\n                [-2.0, 1.0, -0.5, 1.5, -1.0, 2.0, -1.0, 1.0, -0.5, 0.5, -1.0, 1.0],\n                [1.0, 0.5, -1.5, 2.0, -2.5, 1.5, -1.0, 2.0, -1.0, 0.5, -0.5, 1.0],\n                [-1.5, 2.5, -1.0, 0.5, 1.0, -1.0, 2.0, -3.0, 1.5, -1.0, 1.0, -0.5]\n            ]),\n            \"x_true\": np.array([0.0, 0.0, 2.0, -1.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.5]),\n            \"lam\": 0.1,\n            \"L_initial\": 1.0,\n            \"eta\": 1.1\n        }\n    ]\n\n    results = []\n\n    def soft_threshold(v, tau):\n        return np.sign(v) * np.maximum(np.abs(v) - tau, 0.0)\n\n    for case in test_cases:\n        A = case[\"A\"]\n        x_true = case[\"x_true\"]\n        lam = case[\"lam\"]\n        L_initial = case[\"L_initial\"]\n        eta = case[\"eta\"]\n        n_iters = 10\n\n        b = A @ x_true\n        n = A.shape[1]\n        \n        # Following standard FISTA notation: x_0, y_1, t_1\n        x_k = np.zeros(n)      # Corresponds to x_{k-1} in iteration k\n        x_km1 = np.zeros(n)    # Corresponds to x_{k-2} in iteration k\n        y_k = x_k              # y_1 = x_0\n        t_k = 1.0              # t_1\n        L = L_initial          # L_0\n        total_increases = 0\n\n        # Loop for k = 1, ..., 10\n        for _ in range(1, n_iters + 1):\n            L_inner = L\n            while True:\n                grad_g_y = A.T @ (A @ y_k - b)\n                x_k_candidate = soft_threshold(y_k - (1.0 / L_inner) * grad_g_y, lam / L_inner)\n\n                g_x = 0.5 * np.linalg.norm(A @ x_k_candidate - b)**2\n                g_y = 0.5 * np.linalg.norm(A @ y_k - b)**2\n                rhs = g_y + np.dot(grad_g_y, x_k_candidate - y_k) + \\\n                      (L_inner / 2.0) * np.linalg.norm(x_k_candidate - y_k)**2\n\n                if g_x = rhs:\n                    L = L_inner\n                    break\n                else:\n                    L_inner *= eta\n                    total_increases += 1\n            \n            # Found a valid L and x_k_candidate (which is now x_k)\n            x_km1 = x_k\n            x_k = x_k_candidate\n            \n            # Update momentum terms\n            t_kp1 = (1.0 + np.sqrt(1.0 + 4.0 * t_k**2)) / 2.0\n            y_kp1 = x_k + ((t_k - 1.0) / t_kp1) * (x_k - x_km1)\n            \n            # Update state for next iteration\n            t_k = t_kp1\n            y_k = y_kp1\n\n        results.append(total_increases)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}