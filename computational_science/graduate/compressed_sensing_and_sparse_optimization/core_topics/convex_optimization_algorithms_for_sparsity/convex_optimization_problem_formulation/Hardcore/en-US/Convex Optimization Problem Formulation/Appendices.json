{
    "hands_on_practices": [
        {
            "introduction": "The choice of norm to model measurement error has profound implications for the structure of an optimization problem. While the standard LASSO uses an $\\ell_2$-norm fidelity term, many practical scenarios involve noise that is better described as uniformly bounded, leading to an $\\ell_\\infty$-norm constraint. This exercise demonstrates how this robust sparse recovery formulation can be elegantly converted into a Linear Program (LP), one of the most fundamental and efficiently solvable classes of optimization problems. This practice hones the core skill of linearizing non-smooth norms through the introduction of auxiliary variables. ",
            "id": "3439983",
            "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $\\epsilon \\in \\mathbb{R}_{+}$. Consider the robust sparse recovery problem defined by minimizing the $\\ell_{1}$ norm of the coefficient vector subject to an $\\ell_{\\infty}$-bounded residual:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - y\\|_{\\infty} \\le \\epsilon.\n$$\nUse only fundamental definitions of norms and absolute values to reformulate this problem as a Linear Program (LP), where a Linear Program (LP) is given in the canonical inequality form\n$$\n\\min_{z \\in \\mathbb{R}^{p}} \\ c^{\\top} z \\quad \\text{subject to} \\quad G z \\le h,\n$$\nwith $G \\in \\mathbb{R}^{q \\times p}$ and $h \\in \\mathbb{R}^{q}$. Your tasks are:\n1. Derive an equivalent LP by specifying an explicit decision vector $z$, objective vector $c$, constraint matrix $G$, and right-hand side vector $h$, expressed in terms of $A$, $y$, $\\epsilon$, and standard basis blocks (such as identity matrices and zero blocks). You must justify each modeling step starting from the definitions of the $\\ell_{1}$ norm and the $\\ell_{\\infty}$ norm.\n2. Compute a closed-form expression for the total number of scalar inequality constraints and the total number of decision variables in your LP representation as functions of $m$ and $n$. \n\nProvide your final answer as a single $1 \\times 2$ row vector $(N_{\\text{var}}, N_{\\le})$, where $N_{\\text{var}}$ is the number of decision variables and $N_{\\le}$ is the number of scalar inequality constraints. No rounding is required, and no physical units are involved. Express your final answer symbolically in terms of $m$ and $n$ only.",
            "solution": "The problem is to reformulate the given robust sparse recovery optimization problem into a canonical Linear Program (LP). The original problem is stated as:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - y\\|_{\\infty} \\le \\epsilon\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $\\epsilon \\in \\mathbb{R}_{+}$. A Linear Program in canonical inequality form is given by:\n$$\n\\min_{z \\in \\mathbb{R}^{p}} \\ c^{\\top} z \\quad \\text{subject to} \\quad G z \\le h\n$$\nWe must determine the decision vector $z$, the objective vector $c$, the constraint matrix $G$, and the right-hand side vector $h$. We will also calculate the number of decision variables, $N_{\\text{var}} = p$, and the number of scalar inequality constraints, $N_{\\le} = q$, where $G \\in \\mathbb{R}^{q \\times p}$.\n\nThe reformulation proceeds in two main parts: linearizing the objective function and linearizing the constraint.\n\n**1. Reformulation of the Objective Function**\n\nThe objective function is the $\\ell_1$ norm of $x$, defined as $\\|x\\|_1 = \\sum_{i=1}^{n} |x_i|$. The presence of the absolute value function $|x_i|$ makes the objective function non-linear. To convert this into a linear form, we employ a standard technique by representing the vector $x$ as the difference of two non-negative vectors. Let $x = u - v$, where $u, v \\in \\mathbb{R}^n$, and we enforce the component-wise constraints $u \\ge 0$ and $v \\ge 0$.\n\nFor any real number $x_i$, the representation $x_i = u_i - v_i$ with $u_i \\ge 0, v_i \\ge 0$ allows us to express its absolute value. Specifically, if we seek to minimize a sum involving $|x_i|$, we can replace $|x_i|$ with $u_i + v_i$. The minimum value of $u_i + v_i$ subject to $x_i = u_i - v_i$ and $u_i,v_i \\ge 0$ is precisely $|x_i|$. This minimum is achieved by setting $u_i = \\max(x_i, 0)$ and $v_i = \\max(-x_i, 0)$.\n\nTherefore, the objective of minimizing $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$ is equivalent to minimizing $\\sum_{i=1}^n (u_i + v_i)$, subject to $x = u - v$ and the non-negativity of $u$ and $v$.\n\nThe new decision variables for our LP are the components of $u$ and $v$. We define a new, larger decision vector $z \\in \\mathbb{R}^{2n}$ by concatenating $u$ and $v$:\n$$\nz = \\begin{pmatrix} u \\\\ v \\end{pmatrix}\n$$\nThe objective function $\\sum_{i=1}^n (u_i + v_i)$ can be written in vector form as $\\mathbf{1}_n^\\top u + \\mathbf{1}_n^\\top v$, where $\\mathbf{1}_n$ is an $n \\times 1$ vector of ones. In terms of $z$, this is $c^\\top z$, where the objective vector $c \\in \\mathbb{R}^{2n}$ is:\n$$\nc = \\begin{pmatrix} \\mathbf{1}_n \\\\ \\mathbf{1}_n \\end{pmatrix}\n$$\n\n**2. Reformulation of the Constraints**\n\nThe problem has two types of constraints: the primary constraint $\\|A x - y\\|_{\\infty} \\le \\epsilon$ and the implicit non-negativity constraints on our new variables $u$ and $v$.\n\nFirst, we address $\\|A x - y\\|_{\\infty} \\le \\epsilon$. Substituting $x = u - v$, we get:\n$$\n\\|A(u - v) - y\\|_{\\infty} \\le \\epsilon\n$$\nThe $\\ell_\\infty$ norm of a vector $r \\in \\mathbb{R}^m$ is defined as $\\|r\\|_{\\infty} = \\max_{j=1, \\dots, m} |r_j|$. Applying this definition, our constraint becomes:\n$$\n\\max_{j=1, \\dots, m} |(A(u-v) - y)_j| \\le \\epsilon\n$$\nThis single inequality is equivalent to a set of $m$ inequalities:\n$$\n|(A(u-v) - y)_j| \\le \\epsilon \\quad \\text{for each } j = 1, \\dots, m\n$$\nBased on the fundamental definition of the absolute value, an inequality of the form $|a| \\le b$ (for $b \\ge 0$) is equivalent to the pair of linear inequalities $-b \\le a \\le b$. Applying this, each of the $m$ inequalities above expands to:\n$$\n-\\epsilon \\le (A(u-v) - y)_j \\le \\epsilon\n$$\nLet $A_j$ denote the $j$-th row of $A$ and $y_j$ be the $j$-th element of $y$. The term $(A(u-v) - y)_j$ is $A_j u - A_j v - y_j$. The pair of inequalities for each $j$ is:\n1. $A_j u - A_j v - y_j \\le \\epsilon \\quad \\implies \\quad A_j u - A_j v \\le y_j + \\epsilon$\n2. $A_j u - A_j v - y_j \\ge -\\epsilon \\quad \\implies \\quad -(A_j u - A_j v) \\le \\epsilon - y_j$\n\nThese $2m$ inequalities can be expressed in matrix form. The first set of $m$ inequalities is $A u - A v \\le y + \\epsilon \\mathbf{1}_m$. The second set is $-A u + A v \\le \\epsilon \\mathbf{1}_m - y$.\n\nNext, we include the non-negativity constraints $u \\ge 0$ and $v \\ge 0$. To match the canonical form $Gz \\le h$, we write these as:\n- $-u \\le 0_n$\n- $-v \\le 0_n$\nwhere $0_n$ is the $n \\times 1$ zero vector.\n\n**3. Assembling the Linear Program**\n\nWe now assemble the complete LP. The decision vector is $z = \\begin{pmatrix} u \\\\ v \\end{pmatrix}$.\nThe problem is to minimize $c^\\top z$ where $c = \\begin{pmatrix} \\mathbf{1}_n \\\\ \\mathbf{1}_n \\end{pmatrix}$.\nThe constraints are collected into the single matrix inequality $Gz \\le h$:\n$$\n\\begin{pmatrix}\nA & -A \\\\\n-A & A \\\\\n-I_n & 0_{n \\times n} \\\\\n0_{n \\times n} & -I_n\n\\end{pmatrix}\n\\begin{pmatrix} u \\\\ v \\end{pmatrix}\n\\le\n\\begin{pmatrix}\ny + \\epsilon \\mathbf{1}_m \\\\\n\\epsilon \\mathbf{1}_m - y \\\\\n0_n \\\\\n0_n\n\\end{pmatrix}\n$$\nHere, $I_n$ is the $n \\times n$ identity matrix and $0_{n \\times n}$ is the $n \\times n$ zero matrix. This provides the explicit forms for $G$ and $h$.\n\n**4. Counting Variables and Constraints**\n\nThe final task is to determine the total number of decision variables ($N_{\\text{var}}$) and scalar inequality constraints ($N_{\\le}$).\n\n- **Number of variables ($N_{\\text{var}}$):** The decision vector $z$ is the concatenation of $u \\in \\mathbb{R}^n$ and $v \\in \\mathbb{R}^n$. Thus, the total number of scalar variables is:\n$$\nN_{\\text{var}} = p = n + n = 2n\n$$\n\n- **Number of constraints ($N_{\\le}$):** The number of constraints is the number of rows in the matrix $G$. We sum the number of scalar inequalities from each block:\n- $A u - A v \\le y + \\epsilon \\mathbf{1}_m$: $m$ inequalities\n- $-A u + A v \\le \\epsilon \\mathbf{1}_m - y$: $m$ inequalities\n- $-u \\le 0_n$: $n$ inequalities\n- $-v \\le 0_n$: $n$ inequalities\nThe total number of scalar inequality constraints is:\n$$\nN_{\\le} = q = m + m + n + n = 2m + 2n\n$$\nThe final result is $(N_{\\text{var}}, N_{\\le}) = (2n, 2m+2n)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 2n & 2m+2n \\end{pmatrix}}$$"
        },
        {
            "introduction": "Building on the concept of reformulation, we now turn to problems involving the Euclidean ($\\ell_2$) norm. The square-root LASSO, for instance, uses an $\\ell_2$ data-fit term without the square, which confers certain statistical advantages, such as independence from the unknown noise variance. This exercise will guide you in converting such a problem into a Second-Order Cone Program (SOCP), a powerful generalization of LPs. You will practice using the epigraph trick to represent both the $\\ell_1$ and $\\ell_2$ norms as conic constraints, a versatile technique in modern convex optimization. ",
            "id": "3439974",
            "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $\\lambda \\in \\mathbb{R}_{++}$. Consider the Least Absolute Shrinkage and Selection Operator (LASSO) objective with a square-root data-fit, namely the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\lambda \\|x\\|_{1} + \\|A x - y\\|_{2}.\n$$\nFormulate this problem as a Second-Order Cone Program (SOCP), using only the epigraph of the $\\ell_{1}$-norm and the quadratic (second-order) cone representation of the Euclidean norm. Your formulation must be written in terms of standard second-order cones of the form\n$$\n\\mathcal{Q}^{k} \\triangleq \\{(t,z) \\in \\mathbb{R} \\times \\mathbb{R}^{k-1} : \\|z\\|_{2} \\leq t\\},\n$$\ntogether with linear equalities and nonnegativity constraints as needed. Then, identify precisely how many second-order cones appear and what their dimensions are as functions of $m$ and $n$. Finally, let $D(m,n)$ denote the total dimension of the product cone that is the Cartesian product of all second-order cones you used. Derive $D(m,n)$ in closed form in terms of $m$ and $n$. Provide your final answer as a single simplified analytical expression for $D(m,n)$ (no units).",
            "solution": "The problem is to reformulate the given convex optimization problem as a Second-Order Cone Program (SOCP). The objective function is:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\lambda \\|x\\|_{1} + \\|A x - y\\|_{2}\n$$\nThis is a convex optimization problem as it is a sum of convex functions (the $\\ell_1$-norm and the $\\ell_2$-norm composed with an affine map) with a positive weight $\\lambda$. An SOCP minimizes a linear objective function over a set of linear equality/inequality constraints and second-order cone constraints.\n\n**Step 1: Epigraph Formulation**\n\nWe introduce two scalar auxiliary variables, $t_1, t_2 \\in \\mathbb{R}$, to serve as epigraphs for the two terms in the objective function. The original problem is equivalent to the following constrained problem:\n$$\n\\begin{array}{ll}\n\\text{minimize} & \\lambda t_1 + t_2 \\\\\n\\text{subject to} & \\|x\\|_{1} \\leq t_1 \\\\\n& \\|A x - y\\|_{2} \\leq t_2\n\\end{array}\n$$\nThe new optimization variables are $x \\in \\mathbb{R}^n$, $t_1 \\in \\mathbb{R}$, and $t_2 \\in \\mathbb{R}$. The objective function $\\lambda t_1 + t_2$ is now linear. We need to express the two inequality constraints in the standard SOCP form.\n\n**Step 2: Formulating the $\\ell_2$-norm Constraint**\n\nThe second constraint, $\\|A x - y\\|_{2} \\leq t_2$, is already in the form of a second-order cone constraint. The vector $Ax - y$ belongs to $\\mathbb{R}^m$. Using the problem's definition of a standard second-order cone, $\\mathcal{Q}^{k} \\triangleq \\{(t,z) \\in \\mathbb{R} \\times \\mathbb{R}^{k-1} : \\|z\\|_{2} \\leq t\\}$, we can directly express this constraint by setting the scalar part to $t_2$ and the vector part to $Ax-y$:\n$$\n(t_2, A x - y) \\in \\mathcal{Q}^{m+1}\n$$\nThis constraint introduces one second-order cone of dimension $k = m+1$.\n\n**Step 3: Formulating the $\\ell_1$-norm Constraint**\n\nThe first constraint, $\\|x\\|_{1} \\leq t_1$, is defined as $\\sum_{i=1}^{n} |x_i| \\leq t_1$. This is not a direct SOCP constraint. The standard technique is to introduce an auxiliary vector $u \\in \\mathbb{R}^n$ and decompose the single inequality into a system of simpler constraints:\n1. $|x_i| \\leq u_i$ for each $i = 1, \\dots, n$.\n2. $\\sum_{i=1}^{n} u_i \\leq t_1$.\n\nThe constraint $\\sum_{i=1}^{n} u_i \\leq t_1$ is a linear inequality, which is a valid type of constraint in a standard SOCP formulation.\nEach of the first $n$ constraints, $|x_i| \\leq u_i$, can be written as a second-order cone constraint. Since $x_i$ is a scalar, its Euclidean norm is its absolute value, $\\|x_i\\|_2 = |x_i|$. Therefore, the constraint is equivalent to $\\|x_i\\|_2 \\leq u_i$, which fits the definition of a 2-dimensional second-order cone:\n$$\n(u_i, x_i) \\in \\mathcal{Q}^2 \\quad \\text{for } i = 1, \\dots, n\n$$\nThis decomposition introduces $n$ second-order cones, each of dimension $k=2$.\n\n**Step 4: Assembling the SOCP and Counting Dimensions**\n\nBy combining these reformulations, the complete SOCP formulation involves the variables $(x, u, t_1, t_2)$. The problem is:\n$$\n\\begin{array}{ll}\n\\text{minimize} & \\lambda t_1 + t_2 \\\\\n\\text{subject to} & (t_2, Ax - y) \\in \\mathcal{Q}^{m+1} \\\\\n& (u_i, x_i) \\in \\mathcal{Q}^2 \\quad \\text{for } i=1, \\dots, n \\\\\n& \\sum_{i=1}^{n} u_i - t_1 \\le 0\n\\end{array}\n$$\nFrom this formulation, we identify the second-order cones used:\n- One cone of dimension $m+1$.\n- $n$ cones, each of dimension $2$.\n\nThe total number of second-order cones is $n+1$. The problem asks for $D(m,n)$, the total dimension of the product cone formed by the Cartesian product of all second-order cones. The dimension of a product space is the sum of the dimensions of its constituent spaces. The dimension of the cone $\\mathcal{Q}^k$ is $k$. Therefore, $D(m,n)$ is the sum of the dimensions of all these cones:\n$$\nD(m,n) = (\\text{dimension of } \\mathcal{Q}^{m+1}) + \\sum_{i=1}^{n} (\\text{dimension of } \\mathcal{Q}^{2})\n$$\n$$\nD(m,n) = (m+1) + n \\times 2\n$$\nSimplifying this expression gives the final result:\n$$\nD(m,n) = m + 2n + 1\n$$",
            "answer": "$$\n\\boxed{m + 2n + 1}\n$$"
        },
        {
            "introduction": "Many applications in signal processing, communications, and physics, such as Magnetic Resonance Imaging (MRI), involve signals and measurements that are naturally complex-valued. To apply the wide array of standard optimization solvers, which are typically designed for real-valued problems, we must first reformulate the problem. This exercise focuses on translating the complex LASSO problem into an equivalent real-valued convex program. You will learn how to systematically map complex variables, matrices, and norms into their real block-structured counterparts, a crucial skill for bridging the gap between theoretical models and practical computation. ",
            "id": "3439932",
            "problem": "Consider a linear sensing model in the complex domain with measurements $y \\in \\mathbb{C}^{m}$, sensing matrix $A \\in \\mathbb{C}^{m \\times n}$, and signal $x \\in \\mathbb{C}^{n}$. The Least Absolute Shrinkage and Selection Operator (LASSO) seeks a signal $x$ that balances fidelity to the measurements and sparsity of the signal. Starting from the following fundamental bases:\n- The complex Euclidean norm is defined by $\\|z\\|_{2} = \\sqrt{\\sum_{j} |z_{j}|^{2}}$, where $|z_{j}|$ denotes the complex modulus.\n- The modulus of a complex scalar $x_{i} = u_{i} + \\mathrm{i} v_{i}$ equals $|x_{i}| = \\sqrt{u_{i}^{2} + v_{i}^{2}}$.\n- Complex matrix-vector multiplication distributes over real and imaginary parts: if $A = A_{R} + \\mathrm{i} A_{I}$ and $x = u + \\mathrm{i} v$ with $A_{R}, A_{I} \\in \\mathbb{R}^{m \\times n}$ and $u, v \\in \\mathbb{R}^{n}$, then $A x = (A_{R} u - A_{I} v) + \\mathrm{i} (A_{R} v + A_{I} u)$.\n- For any complex vector $r = r_{R} + \\mathrm{i} r_{I}$ with $r_{R}, r_{I} \\in \\mathbb{R}^{m}$, the identity $\\|r\\|_{2}^{2} = \\|r_{R}\\|_{2}^{2} + \\|r_{I}\\|_{2}^{2}$ holds.\n\nUsing these principles, formulate the complex LASSO objective as a convex function of $x \\in \\mathbb{C}^{n}$ and then rewrite it as a real-valued convex program in $\\mathbb{R}^{2n}$ by defining appropriate real variables for the real and imaginary parts of $x$, and a real block matrix and vector built from the real and imaginary parts of $A$ and $y$. Explicitly construct the stacked variable, the block matrix, and the stacked measurement vector, and express the resulting real-valued objective in terms of these quantities and the standard Euclidean norm on $\\mathbb{R}^{2m}$. Your final answer must be a single analytic expression for the real-valued objective written purely in terms of the stacked real variables and the block-structured quantities. No numerical computation is required, and no rounding is necessary.",
            "solution": "We begin by recalling the standard convex formulation of the Least Absolute Shrinkage and Selection Operator (LASSO) objective in the complex domain. Given measurements $y \\in \\mathbb{C}^{m}$, sensing matrix $A \\in \\mathbb{C}^{m \\times n}$, and decision variable $x \\in \\mathbb{C}^{n}$, the complex LASSO objective is the sum of a quadratic data fidelity term and a sparsity-promoting penalty defined by the sum of complex moduli:\n$$\nf(x) \\;=\\; \\frac{1}{2} \\,\\|A x - y\\|_{2}^{2} \\;+\\; \\lambda \\sum_{i=1}^{n} |x_{i}|,\n$$\nwhere $\\lambda > 0$ is a regularization parameter. By construction, $\\|A x - y\\|_{2}^{2}$ is convex in $x$, and the sum $\\sum_{i=1}^{n} |x_{i}|$ is convex because each $|x_{i}|$ is a norm on $\\mathbb{C}$ and a sum of convex functions is convex.\n\nTo rewrite this as a real-valued convex program in $\\mathbb{R}^{2n}$, we separate all quantities into real and imaginary parts. Let\n$$\nA = A_{R} + \\mathrm{i} A_{I}, \\quad y = y_{R} + \\mathrm{i} y_{I}, \\quad x = u + \\mathrm{i} v,\n$$\nwhere $A_{R}, A_{I} \\in \\mathbb{R}^{m \\times n}$, $y_{R}, y_{I} \\in \\mathbb{R}^{m}$, and $u, v \\in \\mathbb{R}^{n}$.\n\nUsing distributivity of complex multiplication,\n$$\nA x = (A_{R} + \\mathrm{i} A_{I})(u + \\mathrm{i} v)\n= A_{R} u + \\mathrm{i} A_{R} v + \\mathrm{i} A_{I} u + \\mathrm{i}^{2} A_{I} v\n= (A_{R} u - A_{I} v) + \\mathrm{i} (A_{R} v + A_{I} u).\n$$\nThus, the residual $r = A x - y$ decomposes into its real and imaginary parts:\n$$\nr_{R} = A_{R} u - A_{I} v - y_{R}, \\qquad\nr_{I} = A_{R} v + A_{I} u - y_{I}.\n$$\nBy the identity for complex Euclidean norm,\n$$\n\\|A x - y\\|_{2}^{2} = \\|r\\|_{2}^{2} = \\|r_{R}\\|_{2}^{2} + \\|r_{I}\\|_{2}^{2}.\n$$\n\nNext, consider the sparsity penalty. For each coordinate $x_{i} = u_{i} + \\mathrm{i} v_{i} \\in \\mathbb{C}$, the modulus is\n$$\n|x_{i}| = \\sqrt{u_{i}^{2} + v_{i}^{2}} = \\big\\|(u_{i}, v_{i})\\big\\|_{2},\n$$\nwhich is the Euclidean norm of the $2$-dimensional real vector formed by the real and imaginary parts of $x_{i}$. Therefore,\n$$\n\\sum_{i=1}^{n} |x_{i}| = \\sum_{i=1}^{n} \\sqrt{u_{i}^{2} + v_{i}^{2}}.\n$$\nThis expresses the complex $\\ell_{1}$-type sparsity as a sum of $\\ell_{2}$ norms over pairs $(u_{i}, v_{i})$, also known as a group norm with groups of size $2$. Each term $\\sqrt{u_{i}^{2} + v_{i}^{2}}$ is convex in $(u_{i}, v_{i})$, so the overall penalty remains convex.\n\nTo compactly represent the data fidelity term in $\\mathbb{R}^{2m}$, define the stacked real variable and measurements:\n$$\n\\tilde{x} = \\begin{pmatrix} u \\\\ v \\end{pmatrix} \\in \\mathbb{R}^{2n}, \\qquad\n\\tilde{y} = \\begin{pmatrix} y_{R} \\\\ y_{I} \\end{pmatrix} \\in \\mathbb{R}^{2m},\n$$\nand the block-structured real matrix\n$$\n\\tilde{A} =\n\\begin{pmatrix}\nA_{R} & -A_{I} \\\\\nA_{I} & \\; A_{R}\n\\end{pmatrix} \\in \\mathbb{R}^{2m \\times 2n}.\n$$\nBy construction,\n$$\n\\tilde{A} \\tilde{x} - \\tilde{y} =\n\\begin{pmatrix}\nA_{R} u - A_{I} v - y_{R} \\\\\nA_{I} u + A_{R} v - y_{I}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nr_{R} \\\\\nr_{I}\n\\end{pmatrix},\n$$\nand therefore\n$$\n\\|A x - y\\|_{2}^{2} = \\|r_{R}\\|_{2}^{2} + \\|r_{I}\\|_{2}^{2}\n= \\left\\|\\tilde{A} \\tilde{x} - \\tilde{y}\\right\\|_{2}^{2}.\n$$\n\nPutting these pieces together, the complex-domain LASSO objective\n$$\n\\frac{1}{2} \\,\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} |x_{i}|\n$$\nis equivalently represented in the real domain as\n$$\n\\frac{1}{2} \\left\\|\\tilde{A} \\tilde{x} - \\tilde{y}\\right\\|_{2}^{2}\n+ \\lambda \\sum_{i=1}^{n} \\sqrt{u_{i}^{2} + v_{i}^{2}}.\n$$\nThis is a convex objective in the variable $\\tilde{x} \\in \\mathbb{R}^{2n}$ because it is the sum of a convex quadratic term and a sum of convex norms. Hence, the requested real-valued convex program in $\\mathbb{R}^{2n}$ is obtained by minimizing this objective with respect to $\\tilde{x}$. The single analytic expression of the objective, expressed in terms of the stacked variables and block-structured quantities, is the final answer.",
            "answer": "$$\\boxed{\\frac{1}{2}\\left\\|\\begin{pmatrix} A_{R} & -A_{I} \\\\[4pt] A_{I} & A_{R} \\end{pmatrix}\\begin{pmatrix} u \\\\[2pt] v \\end{pmatrix}-\\begin{pmatrix} y_{R} \\\\[2pt] y_{I} \\end{pmatrix}\\right\\|_{2}^{2}+\\lambda\\sum_{i=1}^{n}\\sqrt{u_{i}^{2}+v_{i}^{2}}}$$"
        }
    ]
}