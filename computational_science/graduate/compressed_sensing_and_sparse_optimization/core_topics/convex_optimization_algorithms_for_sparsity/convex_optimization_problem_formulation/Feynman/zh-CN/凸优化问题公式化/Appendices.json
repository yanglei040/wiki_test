{
    "hands_on_practices": [
        {
            "introduction": "应用优化中的一项基石技能，是将包含非光滑范数（如 $\\ell_1$ 和 $\\ell_\\infty$ 范数）的问题转化为标准的线性规划（LP）形式。本练习  将指导您运用变量替换和不等式展开等基本技巧，以实现这种精确的重构，从而使问题能够被任何标准的 LP 求解器求解。",
            "id": "3439983",
            "problem": "设 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，且 $\\epsilon \\in \\mathbb{R}_{+}$。考虑鲁棒稀疏恢复问题，其定义为最小化系数向量的 $\\ell_{1}$ 范数，约束条件为残差的 $\\ell_{\\infty}$ 范数有界：\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - y\\|_{\\infty} \\le \\epsilon.\n$$\n仅使用范数和绝对值的基本定义，将此问题重构为一个线性规划 (LP)，其中线性规划 (LP) 以标准不等式形式给出\n$$\n\\min_{z \\in \\mathbb{R}^{p}} \\ c^{\\top} z \\quad \\text{subject to} \\quad G z \\le h,\n$$\n其中 $G \\in \\mathbb{R}^{q \\times p}$ 且 $h \\in \\mathbb{R}^{q}$。您的任务是：\n1. 通过明确指定决策向量 $z$、目标向量 $c$、约束矩阵 $G$ 和右侧向量 $h$，推导一个等价的 LP，并用 $A$、$y$、$\\epsilon$ 和标准基块（例如单位矩阵和零矩阵块）表示。您必须从 $\\ell_{1}$ 范数和 $\\ell_{\\infty}$ 范数的定义出发，证明每个建模步骤的合理性。\n2. 计算您的 LP 表示中纯量不等式约束的总数和决策变量的总数的闭式表达式，作为 $m$ 和 $n$ 的函数。\n\n将您的最终答案表示为单个 $1 \\times 2$ 的行向量 $(N_{\\text{var}}, N_{\\le})$，其中 $N_{\\text{var}}$ 是决策变量的数量，$N_{\\le}$ 是纯量不等式约束的数量。不需要四舍五入，也不涉及物理单位。仅用 $m$ 和 $n$ 的符号形式表示您的最终答案。",
            "solution": "问题在于将给定的鲁棒稀疏恢复优化问题重构为标准线性规划 (LP)。原始问题表述为：\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - y\\|_{\\infty} \\le \\epsilon\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，且 $\\epsilon \\in \\mathbb{R}_{+}$。标准不等式形式的线性规划由以下公式给出：\n$$\n\\min_{z \\in \\mathbb{R}^{p}} \\ c^{\\top} z \\quad \\text{subject to} \\quad G z \\le h\n$$\n我们必须确定决策向量 $z$、目标向量 $c$、约束矩阵 $G$ 和右侧向量 $h$。我们还将计算决策变量的数量 $N_{\\text{var}} = p$ 和纯量不等式约束的数量 $N_{\\le} = q$，其中 $G \\in \\mathbb{R}^{q \\times p}$。\n\n重构过程分为两个主要部分：线性化目标函数和线性化约束。\n\n**1. 目标函数的重构**\n\n目标函数是 $x$ 的 $\\ell_1$ 范数，定义为 $\\|x\\|_1 = \\sum_{i=1}^{n} |x_i|$。绝对值函数 $|x_i|$ 的存在使得目标函数非线性。为了将其转换为线性形式，我们采用一种标准技巧，将向量 $x$ 表示为两个非负向量的差。设 $x = u - v$，其中 $u, v \\in \\mathbb{R}^n$，并且我们强制施加分量级约束 $u \\ge 0$ 和 $v \\ge 0$。\n\n对于任何实数 $x_i$，使用 $u_i \\ge 0, v_i \\ge 0$ 的表示 $x_i = u_i - v_i$ 允许我们表达其绝对值。具体来说，如果我们希望最小化一个包含 $|x_i|$ 的和，我们可以用 $u_i + v_i$ 替换 $|x_i|$。在约束条件 $x_i = u_i - v_i$ 和 $u_i,v_i \\ge 0$ 下，$u_i + v_i$ 的最小值恰好是 $|x_i|$。这个最小值通过设置 $u_i = \\max(x_i, 0)$ 和 $v_i = \\max(-x_i, 0)$ 来实现。\n\n因此，最小化 $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$ 的目标等价于在 $x = u - v$ 以及 $u$ 和 $v$ 非负的约束下，最小化 $\\sum_{i=1}^n (u_i + v_i)$。\n\n我们 LP 的新决策变量是 $u$ 和 $v$ 的分量。我们通过拼接 $u$ 和 $v$ 定义一个新的、更大的决策向量 $z \\in \\mathbb{R}^{2n}$：\n$$\nz = \\begin{pmatrix} u \\\\ v \\end{pmatrix}\n$$\n目标函数 $\\sum_{i=1}^n (u_i + v_i)$ 可以写成向量形式 $\\mathbf{1}_n^\\top u + \\mathbf{1}_n^\\top v$，其中 $\\mathbf{1}_n$ 是一个 $n \\times 1$ 的全一向量。用 $z$ 表示，这就是 $c^\\top z$，其中目标向量 $c \\in \\mathbb{R}^{2n}$ 是：\n$$\nc = \\begin{pmatrix} \\mathbf{1}_n \\\\ \\mathbf{1}_n \\end{pmatrix}\n$$\n\n**2. 约束的重构**\n\n该问题有两类约束：主约束 $\\|A x - y\\|_{\\infty} \\le \\epsilon$ 和对我们的新变量 $u$ 和 $v$ 的隐含非负性约束。\n\n首先，我们处理 $\\|A x - y\\|_{\\infty} \\le \\epsilon$。代入 $x = u - v$，我们得到：\n$$\n\\|A(u - v) - y\\|_{\\infty} \\le \\epsilon\n$$\n向量 $r \\in \\mathbb{R}^m$ 的 $\\ell_\\infty$ 范数定义为 $\\|r\\|_{\\infty} = \\max_{j=1, \\dots, m} |r_j|$。应用这个定义，我们的约束变为：\n$$\n\\max_{j=1, \\dots, m} |(A(u-v) - y)_j| \\le \\epsilon\n$$\n这个单一的不等式等价于一组 $m$ 个不等式：\n$$\n|(A(u-v) - y)_j| \\le \\epsilon \\quad \\text{for each } j = 1, \\dots, m\n$$\n根据绝对值的基本定义，形式为 $|a| \\le b$ (对于 $b \\ge 0$) 的不等式等价于一对线性不等式 $-b \\le a \\le b$。应用此规则，上述 $m$ 个不等式中的每一个都展开为：\n$$\n-\\epsilon \\le (A(u-v) - y)_j \\le \\epsilon\n$$\n令 $A_j$ 表示 $A$ 的第 $j$ 行，$y_j$ 表示 $y$ 的第 $j$ 个元素。项 $(A(u-v) - y)_j$ 是 $A_j u - A_j v - y_j$。对于每个 $j$ 的不等式对是：\n1. $A_j u - A_j v - y_j \\le \\epsilon \\quad \\implies \\quad A_j u - A_j v \\le y_j + \\epsilon$\n2. $A_j u - A_j v - y_j \\ge -\\epsilon \\quad \\implies \\quad -(A_j u - A_j v - y_j) \\le \\epsilon \\quad \\implies \\quad -A_j u + A_j v \\le \\epsilon - y_j$\n\n这 $2m$ 个不等式可以用矩阵形式表示。第一组 $m$ 个不等式是 $A u - A v \\le y + \\epsilon \\mathbf{1}_m$。第二组是 $-A u + A v \\le \\epsilon \\mathbf{1}_m - y$。\n\n接下来，我们包含非负性约束 $u \\ge 0$ 和 $v \\ge 0$。为了匹配标准形式 $Gz \\le h$，我们将其写为：\n-$-u \\le 0_n$\n-$-v \\le 0_n$\n其中 $0_n$ 是 $n \\times 1$ 的零向量。\n\n**3. 组合线性规划**\n\n我们现在组合完整的 LP。决策向量是 $z = \\begin{pmatrix} u \\\\ v \\end{pmatrix}$。\n问题是最小化 $c^\\top z$，其中 $c = \\begin{pmatrix} \\mathbf{1}_n \\\\ \\mathbf{1}_n \\end{pmatrix}$。\n这些约束被收集到单个矩阵不等式 $Gz \\le h$ 中：\n$$\n\\begin{pmatrix}\nA & -A \\\\\n-A & A \\\\\n-I_n & 0_{n \\times n} \\\\\n0_{n \\times n} & -I_n\n\\end{pmatrix}\n\\begin{pmatrix} u \\\\ v \\end{pmatrix}\n\\le\n\\begin{pmatrix}\ny + \\epsilon \\mathbf{1}_m \\\\\n\\epsilon \\mathbf{1}_m - y \\\\\n0_n \\\\\n0_n\n\\end{pmatrix}\n$$\n这里，$I_n$ 是 $n \\times n$ 的单位矩阵，$0_{n \\times n}$ 是 $n \\times n$ 的零矩阵。这就给出了 $G$ 和 $h$ 的显式形式。\n\n**4. 计算变量和约束的数量**\n\n最后的任务是确定决策变量的总数 ($N_{\\text{var}}$) 和纯量不等式约束的总数 ($N_{\\le}$)。\n\n- **变量数量 ($N_{\\text{var}}$)：** 决策向量 $z$ 是 $u \\in \\mathbb{R}^n$ 和 $v \\in \\mathbb{R}^n$ 的拼接。因此，纯量变量的总数为：\n$$\nN_{\\text{var}} = p = n + n = 2n\n$$\n\n- **约束数量 ($N_{\\le}$)：** 约束的数量是矩阵 $G$ 的行数。我们对每个块的纯量不等式数量求和：\n- $A u - A v \\le y + \\epsilon \\mathbf{1}_m$：$m$ 个不等式\n- $-A u + A v \\le \\epsilon \\mathbf{1}_m - y$：$m$ 个不等式\n- $-u \\le 0_n$：$n$ 个不等式\n- $-v \\le 0_n$：$n$ 个不等式\n纯量不等式约束的总数是：\n$$\nN_{\\le} = q = m + m + n + n = 2m + 2n\n$$\n最终结果是 $(N_{\\text{var}}, N_{\\le}) = (2n, 2m+2n)$。",
            "answer": "$$\\boxed{\\begin{pmatrix} 2n & 2m+2n \\end{pmatrix}}$$"
        },
        {
            "introduction": "当优化问题涉及欧几里得（$\\ell_2$）范数时，它们通常能自然地契合二阶锥规划（SOCP）的结构。在此练习  中，您将学习如何使用“epigraph”技术，将 $\\ell_1$ 范数正则化项和 $\\ell_2$ 范数数据保真项均表示为锥约束，这对于一大类问题都是一种强大的方法。",
            "id": "3439974",
            "problem": "设 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，且 $\\lambda \\in \\mathbb{R}_{++}$。考虑带有平方根数据拟合的最小绝对收缩和选择算子 (LASSO) 目标，即以下凸优化问题\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\lambda \\|x\\|_{1} + \\|A x - y\\|_{2}.\n$$\n请将此问题表述为二阶锥规划 (SOCP)，仅使用 $\\ell_{1}$-范数的上镜图和欧几里得范数的二次（二阶）锥表示。您的表述必须用以下形式的标准二阶锥写出\n$$\n\\mathcal{Q}^{k} \\triangleq \\{(t,z) \\in \\mathbb{R} \\times \\mathbb{R}^{k-1} : \\|z\\|_{2} \\leq t\\},\n$$\n并根据需要使用线性等式和非负性约束。然后，精确地确定出现了多少个二阶锥，以及它们的维度作为 $m$ 和 $n$ 的函数是什么。最后，令 $D(m,n)$ 表示您所使用的所有二阶锥的笛卡尔积所形成的乘积锥的总维度。请以 $m$ 和 $n$ 的闭式形式推导出 $D(m,n)$。请将您的最终答案表示为 $D(m,n)$ 的单一简化解析表达式（无单位）。",
            "solution": "给定的优化问题是\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\lambda \\|x\\|_{1} + \\|A x - y\\|_{2}\n$$\n这是一个凸优化问题，因为目标函数是两个凸函数之和（$\\ell_1$-范数和 $\\ell_2$-范数是凸的，与仿射映射的复合保留了凸性），并且 $\\lambda$ 是一个正标量。为了将其表述为二阶锥规划 (SOCP)，我们需要将问题重写为最小化一个线性目标函数，并受一组线性等式/不等式约束和二阶锥约束的限制。\n\n我们首先引入两个标量辅助变量 $t_1 \\in \\mathbb{R}$ 和 $t_2 \\in \\mathbb{R}$，作为目标函数中两项的上镜图变量。原始问题等价于以下约束问题：\n$$\n\\begin{array}{ll}\n\\text{最小化} & \\lambda t_1 + t_2 \\\\\n\\text{满足约束} & \\|x\\|_{1} \\leq t_1 \\\\\n& \\|A x - y\\|_{2} \\leq t_2\n\\end{array}\n$$\n现在的优化是针对变量 $x \\in \\mathbb{R}^n$，$t_1 \\in \\mathbb{R}$ 和 $t_2 \\in \\mathbb{R}$。目标函数 $\\lambda t_1 + t_2$ 对于新的优化变量是线性的。我们现在需要将这两个不等式约束转换为与 SOCP 兼容的形式。\n\n首先，考虑涉及欧几里得范数（$\\ell_2$-范数）的约束：\n$$\n\\|A x - y\\|_{2} \\leq t_2\n$$\n向量 $Ax - y$ 属于 $\\mathbb{R}^m$。根据问题对标准二阶锥的定义 $\\mathcal{Q}^{k} \\triangleq \\{(t,z) \\in \\mathbb{R} \\times \\mathbb{R}^{k-1} : \\|z\\|_{2} \\leq t\\}$，我们可以直接将此约束表示为二阶锥约束。通过将标量部分设为 $t_2$，向量部分设为 $Ax-y$，我们得到：\n$$\n(t_2, A x - y) \\in \\mathcal{Q}^{m+1}\n$$\n这个约束涉及一个维度为 $k = m+1$ 的二阶锥。\n\n接下来，我们处理涉及 $\\ell_1$-范数的约束：\n$$\n\\|x\\|_{1} \\leq t_1\n$$\n$\\ell_1$-范数定义为 $\\|x\\|_1 = \\sum_{i=1}^{n} |x_i|$。这个单一约束的形式不能直接由单个二阶锥表示。标准方法是引入一个辅助向量 $u = (u_1, u_2, \\dots, u_n)^T \\in \\mathbb{R}^n$。不等式 $\\|x\\|_1 \\le t_1$ 等价于以下约束系统：\n$$\n\\sum_{i=1}^{n} u_i \\leq t_1\n$$\n和\n$$\n|x_i| \\leq u_i \\quad \\text{对于 } i = 1, 2, \\dots, n。\n$$\n约束 $|x_i| \\leq u_i$ 涉及向量 $x$ 的单个分量 $x_i$。因为 $x_i$ 是一个标量，它的欧几里得范数就是其绝对值，即 $\\|x_i\\|_2 = |x_i|$。因此，约束 $|x_i| \\leq u_i$ 等价于 $\\|x_i\\|_2 \\leq u_i$。这可以使用 $\\mathcal{Q}^2$ 的定义写成一个二阶锥约束：\n$$\n(u_i, x_i) \\in \\mathcal{Q}^2\n$$\n由于这对从 $1$ 到 $n$ 的每个 $i$ 都必须成立，这个分解引入了 $n$ 个二阶锥，每个维度为 $k=2$。剩余的约束 $\\sum_{i=1}^{n} u_i \\leq t_1$ 是一个线性不等式。问题陈述允许使用线性等式和非负性约束。线性不等式在标准 SOCP 表述中是一种有效的约束类型。\n\n通过结合这些重构，我们得到了原始问题的完整 SOCP 表述。完整的优化变量集合是 $(x, u, t_1, t_2) \\in \\mathbb{R}^n \\times \\mathbb{R}^n \\times \\mathbb{R} \\times \\mathbb{R}$。\n\n该 SOCP 为：\n$$\n\\begin{array}{lll}\n\\text{最小化} & \\lambda t_1 + t_2  \\\\\n\\text{满足约束} & (t_2, Ax-y) \\in \\mathcal{Q}^{m+1}  \\\\\n& (u_i, x_i) \\in \\mathcal{Q}^2  \\text{对于 } i=1, \\dots, n \\\\\n& \\sum_{i=1}^{n} u_i - t_1 \\leq 0 \n\\end{array}\n$$\n\n从这个表述中，我们可以确定所使用的二阶锥：\n1.  一个维度为 $m+1$ 的锥 $\\mathcal{Q}^{m+1}$。\n2.  $n$ 个维度为 $2$ 的锥，每个都是 $\\mathcal{Q}^{2}$。\n\n二阶锥的总数为 $n+1$。\n\n问题将 $D(m,n)$ 定义为表述中使用的所有二阶锥的笛卡尔积所形成的乘积锥的总维度。这个乘积锥是 $\\mathcal{K} = \\mathcal{Q}^{m+1} \\times \\mathcal{Q}^{2} \\times \\dots \\times \\mathcal{Q}^{2}$（其中 $\\mathcal{Q}^2$ 出现 $n$ 次）。乘积空间的维度是其构成空间维度的总和。\n$\\mathcal{Q}^k$ 的维度是 $k$。因此，$D(m,n)$ 是所有这些锥的维度之和：\n$$\nD(m,n) = (\\mathcal{Q}^{m+1} \\text{的维度}) + \\sum_{i=1}^{n} (\\mathcal{Q}^{2} \\text{的维度})\n$$\n$$\nD(m,n) = (m+1) + n \\times 2\n$$\n简化此表达式得到最终结果：\n$$\nD(m,n) = m + 2n + 1\n$$\n这就是 SOCP 表述所需的所有二阶锥乘积的总维度。",
            "answer": "$$\n\\boxed{m + 2n + 1}\n$$"
        },
        {
            "introduction": "虽然一个恢复问题的约束形式和惩罚形式通常被认为是“等价的”，但这种等价性可能非常微妙。本练习  设计了一个精巧的场景，以证明即使在关键参数匹配的情况下，这两种形式也可能选出不同的解。这次探索将加深您对每种形式的精确行为和隐含偏好的理解，尤其是在解不唯一的情况下。",
            "id": "3439951",
            "problem": "考虑一个在压缩感知中使用退化测量算子的线性逆模型。设测量矩阵为 $A \\in \\mathbb{R}^{2 \\times 2}$，由下式给出\n$$\nA = \\begin{pmatrix} 1  1 \\\\ 0  0 \\end{pmatrix},\n$$\n并设观测数据为 $y \\in \\mathbb{R}^{2}$，由下式给出\n$$\ny = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\n决策变量为 $x \\in \\mathbb{R}^{2}$。考虑以下两种用于恢复稀疏 $x$ 的凸规划：\n1. 约束 $\\ell_{1}$ 最小化（也称为基追踪去噪）：在满足 $\\ell_{2}$ 数据保真度约束的条件下，最小化 $x$ 的 $\\ell_{1}$ 范数，\n$$\n\\text{(C)} \\quad \\min_{x \\in \\mathbb{R}^{2}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - y\\|_{2} \\leq \\varepsilon,\n$$\n其中 $\\varepsilon > 0$ 是一个预设的容差。\n2. 惩罚 $\\ell_{1}$ 正则化最小二乘（也称为最小绝对收缩和选择算子）：最小化残差平方和与 $\\ell_{1}$ 范数的加权和，\n$$\n\\text{(P)} \\quad \\min_{x \\in \\mathbb{R}^{2}} \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\n其中 $\\lambda > 0$ 是一个正则化参数。\n\n这两个问题都是凸问题。矩阵 $A$ 是退化的，因为它秩为一且有非平凡的零空间，因此最小化子不必是唯一的。为了研究在非唯一性情况下的选择问题，采用以下选择规则：\n- 在 (C) 中，从所有最小化子中，选择支撑集大小最小（非零项最少）的一个，如果仍然存在非唯一性，则选择 $\\mathbb{R}^{2}$ 中字典序最小的向量。\n- 在 (P) 中，从所有最小化子中，选择 $\\ell_{2}$ 范数最小的一个。\n\n从凸性、$\\ell_{1}$ 范数和次微分最优性条件（Karush–Kuhn–Tucker 条件）的定义出发，根据第一性原理推导：\n- 对于给定的 $A$ 和 $y$，问题 (C) 和 (P) 的解集的结构。\n- 选择一个作为 $\\varepsilon$ 函数的 $\\lambda$，使得从 (C) 和 (P) 中选择的解具有匹配的残差范数，即 $\\|A x_{\\text{C}} - y\\|_{2} = \\|A x_{\\text{P}} - y\\|_{2}$。\n- 明确证明，在所述的选择规则下，当 $\\varepsilon \\in (0,1)$ 时，尽管残差范数匹配，选择的解 $x_{\\text{C}}$ 和 $x_{\\text{P}}$ 仍是不同的向量。\n\n在这些规则下，能够实现残差范数匹配的 $\\lambda$ 值（表示为 $\\varepsilon$ 的封闭形式表达式）是多少？请精确表达你的最终答案，无需四舍五入。",
            "solution": "该问题要求对一个特定的退化线性逆模型，分析两个凸优化问题 (C) 和 (P)。我们必须推导出解集的结构，找到一个使所选解的残差范数相匹配的参数关系 $\\lambda(\\varepsilon)$，并证明这些选定的解是不同的。\n\n首先，我们简化涉及给定矩阵 $A = \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix}$ 和向量 $y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 的表达式。对于任意向量 $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$，乘积 $Ax$ 为：\n$$\nAx = \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} x_1 + x_2 \\\\ 0 \\end{pmatrix}\n$$\n残差向量为 $Ax - y = \\begin{pmatrix} x_1 + x_2 - 1 \\\\ 0 \\end{pmatrix}$。\n因此，残差的 $\\ell_2$ 范数为 $\\|Ax - y\\|_{2} = \\sqrt{(x_1+x_2-1)^2 + 0^2} = |x_1+x_2-1|$。\n$x$ 的 $\\ell_1$ 范数为 $\\|x\\|_1 = |x_1| + |x_2|$。\n\n通过这些简化，我们可以分析每个问题。\n\n**问题 (C) 的分析：约束 $\\ell_1$ 最小化**\n\n问题表述为：\n$$\n\\text{(C)} \\quad \\min_{x \\in \\mathbb{R}^{2}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - y\\|_{2} \\leq \\varepsilon\n$$\n使用我们简化的残差范数表达式，这变为：\n$$\n\\min_{x_1, x_2} |x_1| + |x_2| \\quad \\text{subject to} \\quad |x_1 + x_2 - 1| \\leq \\varepsilon\n$$\n该约束等价于 $-\\varepsilon \\leq x_1 + x_2 - 1 \\leq \\varepsilon$，可以写成 $1-\\varepsilon \\leq x_1 + x_2 \\leq 1+\\varepsilon$。令 $s = x_1+x_2$。该约束将 $s$ 限制在区间 $[1-\\varepsilon, 1+\\varepsilon]$ 内。\n\n根据三角不等式，目标函数的下界是其和的绝对值：$\\|x\\|_1 = |x_1| + |x_2| \\geq |x_1+x_2| = |s|$。为了最小化 $\\|x\\|_1$，我们必须首先在其可行范围 $s \\in [1-\\varepsilon, 1+\\varepsilon]$ 上最小化其下界 $|s|$。\n给定 $\\varepsilon \\in (0,1)$，我们有 $1-\\varepsilon > 0$。因此，任何可行的 $s$ 都是正的。在区间 $[1-\\varepsilon, 1+\\varepsilon]$ 上， $|s|$（也就是 $s$）的最小值在左端点达到，即 $s = 1-\\varepsilon$。\n因此，目标函数的最小可能值为 $1-\\varepsilon$。\n\n对于一个向量 $x$，当且仅当满足两个条件时才能达到这个最小值：\n1.  $|x_1+x_2| = 1-\\varepsilon$。由于 $s=x_1+x_2$ 必须在 $[1-\\varepsilon, 1+\\varepsilon]$ 内，这迫使 $x_1+x_2 = 1-\\varepsilon$。\n2.  $\\|x\\|_1 = |x_1+x_2|$。三角不等式中的这个等式成立，当且仅当 $x_1$ 和 $x_2$ 同号或其中一个为零，即 $x_1x_2 \\geq 0$。\n\n由于和 $x_1+x_2=1-\\varepsilon$ 是正数，所以 $x_1$ 和 $x_2$ 不可能都为负数。因此，我们必须有 $x_1 \\geq 0$ 和 $x_2 \\geq 0$。\n(C) 的最小化子集合，记为 $S_C$，是满足这些条件的所有点的集合：\n$$\nS_C = \\{ x = (x_1, x_2) \\in \\mathbb{R}^2 \\mid x_1+x_2=1-\\varepsilon, x_1 \\geq 0, x_2 \\geq 0 \\}\n$$\n这是第一象限中连接点 $(1-\\varepsilon, 0)$ 和 $(0, 1-\\varepsilon)$ 的线段。\n\n现在，我们应用 (C) 的选择规则。\n1.  最小支撑集大小：向量的支撑集是其非零项的数量。对于线段 $S_C$ 内部的任何点， $x_1$ 和 $x_2$ 都为正，因此支撑集大小为 2。在端点 $(1-\\varepsilon, 0)$ 和 $(0, 1-\\varepsilon)$ 处，一个分量为零，因此支撑集大小为 1。最小支撑集大小为 1。候选解为 $(1-\\varepsilon, 0)$ 和 $(0, 1-\\varepsilon)$。\n2.  字典序最小：我们比较两个候选解。如果向量 $u$ 的第一个不同分量小于向量 $v$ 的相应分量，则 $u$ 在字典序上小于 $v$。比较 $(1-\\varepsilon, 0)$ 和 $(0, 1-\\varepsilon)$，第一个分量分别是 $1-\\varepsilon$ 和 $0$。由于 $\\varepsilon \\in (0,1)$，我们有 $1-\\varepsilon > 0$。因此，$0 < 1-\\varepsilon$，向量 $(0, 1-\\varepsilon)$ 在字典序上更小。\n\n(C) 的选定解是 $x_C = \\begin{pmatrix} 0 \\\\ 1-\\varepsilon \\end{pmatrix}$。\n此解的残差范数为 $\\|Ax_C - y\\|_2 = |0 + (1-\\varepsilon) - 1| = |-\\varepsilon| = \\varepsilon$。\n\n**问题 (P) 的分析：惩罚 $\\ell_1$ 正则化最小二乘**\n\n问题表述为：\n$$\n\\text{(P)} \\quad \\min_{x \\in \\mathbb{R}^{2}} \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\n使用简化后的表达式，即为：\n$$\n\\min_{x_1, x_2} f(x) = \\frac{1}{2}(x_1+x_2-1)^2 + \\lambda(|x_1|+|x_2|)\n$$\n由于 $f(x)$ 是凸函数，一个向量 $x^*$ 是最小化子当且仅当零向量在 $f$ 于 $x^*$ 处的次微分中，即 $0 \\in \\partial f(x^*)$。\n次微分为 $\\partial f(x) = \\nabla(\\frac{1}{2}(x_1+x_2-1)^2) + \\lambda \\partial(\\|x\\|_1)$。\n$$\n\\partial f(x) = \\begin{pmatrix} x_1+x_2-1 \\\\ x_1+x_2-1 \\end{pmatrix} + \\lambda \\begin{pmatrix} \\partial|x_1| \\\\ \\partial|x_2| \\end{pmatrix}\n$$\n其中，当 $t \\neq 0$ 时，$\\partial|t|$ 是符号函数 $\\text{sgn}(t)$；当 $t=0$ 时，是区间 $[-1, 1]$。\n设 $x=(x_1, x_2)$ 是一个最小化子，并令 $s=x_1+x_2$。最优性条件 $0 \\in \\partial f(x)$ 变为：\n$$\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\in \\begin{pmatrix} s-1 \\\\ s-1 \\end{pmatrix} + \\lambda \\begin{pmatrix} g_1 \\\\ g_2 \\end{pmatrix} \\quad \\text{其中 } g_1 \\in \\partial|x_1|, g_2 \\in \\partial|x_2|\n$$\n这意味着 $-(s-1) = \\lambda g_1$ 且 $-(s-1) = \\lambda g_2$，因此 $g_1=g_2$。令 $g=g_1=g_2$。\n条件是存在一个 $g$，使得 $g = -(s-1)/\\lambda$，$g \\in \\partial|x_1|$ 且 $g \\in \\partial|x_2|$。\n-   如果 $x_1>0$ 且 $x_2>0$，则 $g_1=g_2=1$。这意味着 $g=1$，所以 $1 = -(s-1)/\\lambda$，得到 $s=1-\\lambda$。要使 $s$ 为正数之和，需要 $s>0$，因此 $1-\\lambda > 0$，意味着 $\\lambda < 1$。\n-   如果 $x_1<0$ 且 $x_2<0$，则 $g_1=g_2=-1$。这意味着 $g=-1$，所以 $-1 = -(s-1)/\\lambda$，得到 $s=1+\\lambda$。要使 $s$ 为负数之和，需要 $s<0$，因此 $1+\\lambda < 0$，但由于 $\\lambda>0$，这是不可能的。\n-   如果 $x_1$ 和 $x_2$ 符号相反，比如 $x_1>0, x_2<0$，则需要 $g_1=1$ 和 $g_2=-1$，但这与我们要求的 $g_1=g_2$ 矛盾。不可能。\n-   如果一个分量为零，例如 $x_1>0, x_2=0$，则需要 $g_1=1$ 且 $g_2 \\in [-1,1]$。这意味着 $g=1$，这是相容的。条件 $s=1-\\lambda$ 且 $\\lambda<1$ 成立。\n-   如果 $x_1=x_2=0$，则 $s=0$。我们需要 $g_{1,2} \\in [-1,1]$。条件是 $g = -(0-1)/\\lambda = 1/\\lambda$。所以我们需要 $1/\\lambda \\in [-1,1]$，这意味着 $|1/\\lambda| \\leq 1$，因此 $\\lambda \\geq 1$。\n\n解集 $S_P$ 的结构取决于 $\\lambda$：\n- 当 $0 < \\lambda < 1$ 时：条件是 $s=1-\\lambda$ 且 $x_1, x_2 \\ge 0$。解集是线段：\n  $S_P = \\{ x = (x_1, x_2) \\in \\mathbb{R}^2 \\mid x_1+x_2=1-\\lambda, x_1 \\geq 0, x_2 \\geq 0 \\}$。\n- 当 $\\lambda \\geq 1$ 时：唯一的解是 $x_1=x_2=0$，所以 $S_P = \\{ (0,0) \\}$。\n\n现在，我们应用 (P) 的选择规则：最小 $\\ell_2$ 范数。\n- 当 $0 < \\lambda < 1$ 时：我们必须在集合 $S_P$ 上最小化 $\\|x\\|_2^2 = x_1^2+x_2^2$。这是一个线段上的二次函数。最小值在离原点最近的点达到，即线段的中点：$x_1=x_2=(1-\\lambda)/2$。所以选定的解是 $x_P = \\begin{pmatrix} (1-\\lambda)/2 \\\\ (1-\\lambda)/2 \\end{pmatrix}$。\n- 当 $\\lambda \\geq 1$ 时：解是唯一的，所以 $x_P = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n\n**匹配残差并确定 $\\lambda(\\varepsilon)$**\n\n问题要求找到 $\\lambda$ 使得 $\\|Ax_C - y\\|_2 = \\|Ax_P - y\\|_2$。我们已经求得 $\\|Ax_C - y\\|_2 = \\varepsilon$。\n我们来计算选定解 $x_P$ 的残差范数。\n- 当 $0 < \\lambda < 1$ 时，$x_P = ((1-\\lambda)/2, (1-\\lambda)/2)^T$。分量之和为 $x_{P1}+x_{P2} = 1-\\lambda$。残差范数为 $\\|Ax_P - y\\|_2 = |(x_{P1}+x_{P2})-1| = |(1-\\lambda)-1| = |-\\lambda| = \\lambda$。\n- 当 $\\lambda \\geq 1$ 时，$x_P=(0,0)^T$。和为 $0$。残差范数为 $\\|Ax_P - y\\|_2 = |0-1|=1$。\n\n我们已知 $\\varepsilon \\in (0,1)$。我们令残差范数相等：$\\varepsilon = \\|Ax_P-y\\|_2$。\n- 如果我们处于 $0 < \\lambda < 1$ 的情况，这得出 $\\varepsilon = \\lambda$。这是一个一致的解，因为 $\\varepsilon \\in (0,1)$ 意味着 $\\lambda \\in (0,1)$。\n- 如果我们处于 $\\lambda \\geq 1$ 的情况，这得出 $\\varepsilon = 1$。这与给定的条件 $\\varepsilon \\in (0,1)$ 矛盾。\n\n因此，唯一有效的关系是 $\\lambda = \\varepsilon$。\n\n**选定解的比较**\n\n在 $\\lambda = \\varepsilon$ 的条件下，我们比较对于 $\\varepsilon \\in (0,1)$ 时从 (C) 中选定的解和从 (P) 中选定的解。\n(C) 的解是 $x_C = \\begin{pmatrix} 0 \\\\ 1-\\varepsilon \\end{pmatrix}$。\n(P) 的解是 $x_P = \\begin{pmatrix} (1-\\lambda)/2 \\\\ (1-\\lambda)/2 \\end{pmatrix} = \\begin{pmatrix} (1-\\varepsilon)/2 \\\\ (1-\\varepsilon)/2 \\end{pmatrix}$。\n\n为了检查这些解是否相同，我们令 $x_C = x_P$：\n$$\n\\begin{pmatrix} 0 \\\\ 1-\\varepsilon \\end{pmatrix} = \\begin{pmatrix} (1-\\varepsilon)/2 \\\\ (1-\\varepsilon)/2 \\end{pmatrix}\n$$\n第一个分量给出 $0 = (1-\\varepsilon)/2$，这意味着 $1-\\varepsilon=0$，所以 $\\varepsilon=1$。\n第二个分量给出 $1-\\varepsilon = (1-\\varepsilon)/2$，这也意味着 $1-\\varepsilon=0$，所以 $\\varepsilon=1$。\n解 $x_C$ 和 $x_P$ 仅在 $\\varepsilon=1$ 时相同。然而，问题规定我们必须考虑 $\\varepsilon \\in (0,1)$。对于这个开区间中的任何值，$1-\\varepsilon \\neq 0$，所以 $(1-\\varepsilon)/2 \\neq 0$。因此，$x_C$ 和 $x_P$ 的第一个分量是不同的。\n这明确地证明了，在给定的选择规则和残差匹配的条件下，对于所有的 $\\varepsilon \\in (0,1)$，这两种规划的选定解是不同的。\n\n能够实现残差范数匹配的 $\\lambda$ 值（表示为 $\\varepsilon$ 的封闭形式表达式）是 $\\lambda=\\varepsilon$。",
            "answer": "$$\\boxed{\\varepsilon}$$"
        }
    ]
}