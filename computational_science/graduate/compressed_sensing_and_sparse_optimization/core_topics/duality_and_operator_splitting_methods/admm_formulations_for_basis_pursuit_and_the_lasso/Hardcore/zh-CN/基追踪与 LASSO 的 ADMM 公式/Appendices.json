{
    "hands_on_practices": [
        {
            "introduction": "ADMM的强大之处不仅在于处理可分离的惩罚项，还在于其能优雅地融入硬约束。此练习将指导您如何通过指示函数将非负性等约束条件纳入LASSO框架。您将推导出相应的ADMM更新步骤，并发现它最终简化为一个直观的投影操作，这揭示了近端算子与几何投影之间的深刻联系。",
            "id": "3429989",
            "problem": "考虑在压缩感知背景下的带有非负性约束的最小绝对收缩和选择算子 (LASSO)：最小化二次数据保真项和稀疏性诱导正则化项之和，并满足分量非负性约束。设 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，且 $\\lambda > 0$。将该问题表述为在一致性约束下最小化两个函数之和：\n$$\\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{n}} f(x) + g(z) \\quad \\text{subject to } x = z,$$\n其中 $f(x)$ 是非负象限的指示函数，$g(z)$ 是二次数据保真项和 $\\ell_{1}$范数正则化项之和。使用交替方向乘子法 (ADMM) 分裂和一个缩放对偶变量，仅根据当前值 $z^{k}$ 和 $u^{k}$，在给定惩罚参数 $\\rho > 0$ 的情况下，推导在迭代 $k+1$ 时 $x$ 的更新。您的推导必须从与一致性公式相关的增广拉格朗日量以及指示函数和投影算子的基本性质开始，不得假设任何预先推导的近端公式。\n\n在推导出 $x$ 更新的解析表达式后，使用以下具体输入进行计算：\n$$z^{k} = \\begin{pmatrix} 1.2 \\\\ -0.7 \\\\ 0.0 \\\\ 3.1 \\\\ -2.4 \\end{pmatrix}, \\quad u^{k} = \\begin{pmatrix} -0.5 \\\\ 0.9 \\\\ -0.3 \\\\ 1.2 \\\\ -1.1 \\end{pmatrix}, \\quad \\rho = 1.$$\n以单行矩阵的形式提供 $x^{k+1}$ 的最终数值。无需四舍五入，且无物理单位。",
            "solution": "该问题要求推导非负 LASSO 问题特定表述下，交替方向乘子法 (ADMM) 算法中 $x$ 的更新步骤，然后进行数值计算。\n\n首先，我们验证问题陈述的有效性。\n问题是在约束条件下最小化一个函数，该函数为 ADMM 方法而构建。\n设原始问题为非负 LASSO：\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2}\\|Ax-y\\|_2^2 + \\lambda \\|x\\|_1 \\quad \\text{subject to } x \\ge 0. $$\n给定的一致性公式为：\n$$ \\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{n}} f(x) + g(z) \\quad \\text{subject to } x = z, $$\n其中给定 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，且 $\\lambda > 0$。函数定义如下：\n$f(x) = I_{\\mathbb{R}_+^n}(x)$，这是非负象限的指示函数。集合 $C$ 的指示函数 $I_C(x)$ 定义为：如果 $x \\in C$，则 $I_C(x) = 0$；如果 $x \\notin C$，则 $I_C(x) = \\infty$。因此，$f(x)$ 强制执行了非负性约束 $x \\ge 0$。\n$g(z)$ 是二次数据保真项和 $\\ell_1$范数正则化项之和，这对应于不含非负性约束的原始 LASSO 目标函数：\n$$ g(z) = \\frac{1}{2}\\|Az-y\\|_2^2 + \\lambda \\|z\\|_1. $$\n该问题在科学上是合理的、适定的和客观的。这是 ADMM 在计算数学和信号处理中的一个标准应用。推导和计算所需的所有信息都已提供。该问题是有效的。\n\n我们现在进行推导。ADMM 算法基于增广拉格朗日量。对于约束问题 $\\min f(x) + g(z)$，约束条件为 $x-z=0$，带有缩放对偶变量 $u$ 的增广拉格朗日量由下式给出：\n$$ L_\\rho(x, z, u) = f(x) + g(z) + \\frac{\\rho}{2}\\|x-z+u\\|_2^2 - \\frac{\\rho}{2}\\|u\\|_2^2. $$\n该形式由标准增广拉格朗日量 $L_\\rho(x, z, \\nu) = f(x) + g(z) + \\nu^T(x-z) + \\frac{\\rho}{2}\\|x-z\\|_2^2$ 通过设置对偶变量 $\\nu = \\rho u$ 并配方法推导得出。项 $-\\frac{\\rho}{2}\\|u\\|_2^2$ 对于 $x$ 和 $z$ 是一个常数，在最小化步骤中可以忽略。\n\nADMM 算法在每次迭代 $k$ 中包含三个更新步骤：\n$1$. $x^{k+1} = \\arg\\min_x L_\\rho(x, z^k, u^k)$\n$2$. $z^{k+1} = \\arg\\min_z L_\\rho(x^{k+1}, z, u^k)$\n$3$. $u^{k+1} = u^k + x^{k+1} - z^{k+1}$\n\n我们的任务是推导在迭代 $k+1$ 时 $x$ 的更新。这是 ADMM 序列的第一步：\n$$ x^{k+1} = \\arg\\min_{x} L_\\rho(x, z^k, u^k). $$\n代入增广拉格朗日量的表达式：\n$$ x^{k+1} = \\arg\\min_{x} \\left( f(x) + g(z^k) + \\frac{\\rho}{2}\\|x-z^k+u^k\\|_2^2 \\right). $$\n在这个最小化问题中，变量是 $x$。项 $g(z^k)$ 相对于 $x$ 是常数，可以从目标函数中去掉而不改变最小化子：\n$$ x^{k+1} = \\arg\\min_{x} \\left( f(x) + \\frac{\\rho}{2}\\|x-z^k+u^k\\|_2^2 \\right). $$\n我们定义一个辅助向量 $v^k = z^k - u^k$。表达式简化为：\n$$ x^{k+1} = \\arg\\min_{x} \\left( f(x) + \\frac{\\rho}{2}\\|x-v^k\\|_2^2 \\right). $$\n函数 $f(x)$ 是非负象限的指示函数 $I_{\\mathbb{R}_+^n}(x)$。所以，该最小化问题等价于：\n$$ x^{k+1} = \\arg\\min_{x \\ge 0} \\frac{\\rho}{2}\\|x-v^k\\|_2^2. $$\n正常数因子 $\\frac{\\rho}{2}$ 不影响最小值的位置，因此我们可以进一步简化问题为：\n$$ x^{k+1} = \\arg\\min_{x \\ge 0} \\|x-v^k\\|_2^2. $$\n这是一个求向量 $v^k$ 到凸集 $\\mathbb{R}_+^n$ 上的欧几里得投影的问题。平方欧几里得范数在向量的分量上是可分的：\n$$ \\|x-v^k\\|_2^2 = \\sum_{i=1}^{n} (x_i - v_i^k)^2. $$\n因此，可以对每个分量 $x_i$ 独立进行最小化：\n$$ x_i^{k+1} = \\arg\\min_{x_i \\ge 0} (x_i - v_i^k)^2 \\quad \\text{for } i=1, \\dots, n. $$\n对于每个分量 $i$，我们考虑一元二次函数 $h(x_i) = (x_i - v_i^k)^2$。$h(x_i)$ 的无约束最小化子位于 $x_i = v_i^k$。\n我们必须满足约束 $x_i \\ge 0$。\n情况1：如果 $v_i^k \\ge 0$，无约束最小化子 $x_i=v_i^k$ 位于可行域 $[0, \\infty)$ 内。因此，解是 $x_i^{k+1} = v_i^k$。\n情况2：如果 $v_i^k  0$，无约束最小化子在可行域之外。函数 $h(x_i)$ 是一个开口向上、顶点在 $v_i^k  0$ 的抛物线。在区间 $[0, \\infty)$ 上，函数 $h(x_i)$ 是严格递增的。因此，该区间上的最小值在边界点 $x_i = 0$ 处取得。解是 $x_i^{k+1} = 0$。\n综合两种情况，每个分量的解由下式给出：\n$$ x_i^{k+1} = \\max(0, v_i^k). $$\n用向量形式，这可以写成 $x^{k+1} = (v^k)_+$，其中 $(\\cdot)_+$ 算子表示与 $0$ 逐元素取最大值。代回 $v^k = z^k - u^k$，我们得到 $x$ 更新的最终解析表达式：\n$$ x^{k+1} = (z^k - u^k)_+. $$\n\n现在，我们使用给定的输入来计算这个表达式：\n$$ z^{k} = \\begin{pmatrix} 1.2 \\\\ -0.7 \\\\ 0.0 \\\\ 3.1 \\\\ -2.4 \\end{pmatrix}, \\quad u^{k} = \\begin{pmatrix} -0.5 \\\\ 0.9 \\\\ -0.3 \\\\ 1.2 \\\\ -1.1 \\end{pmatrix}. $$\n惩罚参数 $\\rho=1$ 在此特定的更新步骤中不需要，因为它在最小化过程中被消去了。\n首先，我们计算向量 $v^k = z^k - u^k$:\n$$ v^k = z^k - u^k = \\begin{pmatrix} 1.2 \\\\ -0.7 \\\\ 0.0 \\\\ 3.1 \\\\ -2.4 \\end{pmatrix} - \\begin{pmatrix} -0.5 \\\\ 0.9 \\\\ -0.3 \\\\ 1.2 \\\\ -1.1 \\end{pmatrix} = \\begin{pmatrix} 1.2 - (-0.5) \\\\ -0.7 - 0.9 \\\\ 0.0 - (-0.3) \\\\ 3.1 - 1.2 \\\\ -2.4 - (-1.1) \\end{pmatrix} = \\begin{pmatrix} 1.7 \\\\ -1.6 \\\\ 0.3 \\\\ 1.9 \\\\ -1.3 \\end{pmatrix}. $$\n接下来，我们应用逐元素到非负象限的投影，$x^{k+1} = (v^k)_+$:\n$$ x^{k+1} = \\left( \\begin{pmatrix} 1.7 \\\\ -1.6 \\\\ 0.3 \\\\ 1.9 \\\\ -1.3 \\end{pmatrix} \\right)_+ = \\begin{pmatrix} \\max(0, 1.7) \\\\ \\max(0, -1.6) \\\\ \\max(0, 0.3) \\\\ \\max(0, 1.9) \\\\ \\max(0, -1.3) \\end{pmatrix} = \\begin{pmatrix} 1.7 \\\\ 0.0 \\\\ 0.3 \\\\ 1.9 \\\\ 0.0 \\end{pmatrix}. $$\n问题要求以单行矩阵的形式给出答案。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.7  0.0  0.3  1.9  0.0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在掌握了标准LASSO的ADMM解法后，一个自然的延伸是探索在现实世界信号和图像处理中更为普遍的结构化稀疏问题。本练习引入了组稀疏（Group LASSO）和融合稀疏（Fused LASSO）正则化器，它们能促进变量的分组或平滑。通过为这个更复杂的模型设计一个多变量分裂的ADMM算法，您将亲身体验该框架的灵活性和模块化特性。",
            "id": "3429996",
            "problem": "考虑结合了组稀疏性和一维融合稀疏性的结构化稀疏回归问题。设 $x \\in \\mathbb{R}^{n}$，$A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，$\\lambda_{g} \\ge 0$，$\\lambda_{f} \\ge 0$，且令 $\\{G_{1},\\dots,G_{K}\\}$ 是 $\\{1,\\dots,n\\}$ 的一个划分，定义了不重叠的组。定义组正则化项 $R_{g}(x) \\triangleq \\sum_{k=1}^{K} \\|x_{G_{k}}\\|_{2}$ 和融合正则化项 $R_{f}(x) \\triangleq \\|D x\\|_{1}$，其中 $D \\in \\mathbb{R}^{(n-1)\\times n}$ 是一阶差分算子，其行在连续位置上为 $[0,\\dots,0,-1,1,0,\\dots,0]$。考虑凸优化问题\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} \\ + \\ \\lambda_{g} \\, R_{g}(x) \\ + \\ \\lambda_{f} \\, R_{f}(x).\n$$\n任务：\n1) 从正常、闭、凸函数 $\\phi$ 的邻近算子定义出发，即 $\\mathrm{prox}_{\\tau \\phi}(v) \\triangleq \\arg\\min_{z} \\left\\{\\frac{1}{2}\\|z - v\\|_{2}^{2} + \\tau \\, \\phi(z)\\right\\}$，推导作用于向量 $v \\in \\mathbb{R}^{n}$ 的组正则化项 $R_{g}$ 的邻近算子，并用块 $v_{G_{k}}$ 明确表示。\n2) 引入辅助变量 $z_{g} \\in \\mathbb{R}^{n}$ 和 $z_{f} \\in \\mathbb{R}^{n-1}$，通过线性约束 $x = z_{g}$ 和 $D x = z_{f}$ 来拆分问题。使用交替方向乘子法（ADMM），写出带有缩放对偶变量的增广拉格朗日量，并推导 $x$、$z_{g}$、$z_{f}$ 以及缩放对偶变量的一般 ADMM 更新步骤，将每次更新识别为邻近算子或源于一阶最优性条件的线性系统求解。你的推导必须从邻近算子和乘子法的定义开始。\n3) 考虑投影到 $D$ 的融合线性约束图上的问题：\n$$\n\\min_{x \\in \\mathbb{R}^{n}, \\, u \\in \\mathbb{R}^{n-1}} \\ \\frac{1}{2}\\|x - \\hat{x}\\|_{2}^{2} + \\frac{1}{2}\\|u - \\hat{u}\\|_{2}^{2} \\quad \\text{subject to} \\quad u = D x.\n$$\n通过消去 $u$ 并求解得到的正规方程，推导投影 $(x^{\\star}, u^{\\star})$ 关于 $(\\hat{x}, \\hat{u})$、$D$ 和单位矩阵的闭式表达式。\n4) 现在，特化为 $n = 3$，$m = 3$，$A = I_{3}$，以及\n$$\nD \\ = \\ \\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\end{pmatrix}, \\quad y \\ = \\ \\begin{pmatrix} 3 \\\\ 1 \\\\ -1 \\end{pmatrix}.\n$$\n令组为 $G_{1} = \\{1,2\\}$ 和 $G_{2} = \\{3\\}$，因此 $R_{g}(x) = \\| (x_{1}, x_{2}) \\|_{2} + |x_{3}|$。设置 $\\lambda_{g} = 1$，$\\lambda_{f} = 1$，ADMM 惩罚参数 $\\rho_{g} = 1$，$\\rho_{f} = 1$。初始化 $z_{g}^{(0)} = 0$，$z_{f}^{(0)} = 0$，$u_{g}^{(0)} = 0$，$u_{f}^{(0)} = 0$。使用你在第 2 部分中推导的 ADMM 更新规则，计算第一个 $x$ 的迭代结果 $x^{(1)}$。将你的最终答案以单行向量的形式报告。不要四舍五入；提供精确值。",
            "solution": "将首先通过提取所有给定信息，然后评估其科学和数学完整性来验证问题。\n\n### 第 1 步：提取给定信息\n\n- **优化变量和数据：**\n  - $x \\in \\mathbb{R}^{n}$\n  - $A \\in \\mathbb{R}^{m \\times n}$\n  - $y \\in \\mathbb{R}^{m}$\n  - $\\lambda_{g} \\ge 0$\n  - $\\lambda_{f} \\ge 0$\n- **问题结构：**\n  - $\\{1,\\dots,n\\}$ 到不重叠组 $\\{G_{1},\\dots,G_{K}\\}$ 的一个划分。\n  - 组正则化项：$R_{g}(x) \\triangleq \\sum_{k=1}^{K} \\|x_{G_{k}}\\|_{2}$。\n  - 融合正则化项：$R_{f}(x) \\triangleq \\|D x\\|_{1}$，其中 $D \\in \\mathbb{R}^{(n-1)\\times n}$ 是一阶差分算子。\n- **优化问题：**\n  $$ \\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} \\ + \\ \\lambda_{g} \\, R_{g}(x) \\ + \\ \\lambda_{f} \\, R_{f}(x) $$\n- **任务 1 定义：**\n  - 邻近算子：$\\mathrm{prox}_{\\tau \\phi}(v) \\triangleq \\arg\\min_{z} \\left\\{\\frac{1}{2}\\|z - v\\|_{2}^{2} + \\tau \\, \\phi(z)\\right\\}$。\n- **任务 2 定义：**\n  - 辅助变量：$z_{g} \\in \\mathbb{R}^{n}$，$z_{f} \\in \\mathbb{R}^{n-1}$。\n  - 线性约束：$x = z_{g}$，$D x = z_{f}$。\n- **任务 3 问题：**\n  $$ \\min_{x \\in \\mathbb{R}^{n}, \\, u \\in \\mathbb{R}^{n-1}} \\ \\frac{1}{2}\\|x - \\hat{x}\\|_{2}^{2} + \\frac{1}{2}\\|u - \\hat{u}\\|_{2}^{2} \\quad \\text{subject to} \\quad u = D x $$\n- **任务 4 具体值：**\n  - $n = 3$，$m = 3$，$A = I_{3}$\n  - $D \\ = \\ \\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\end{pmatrix}$\n  - $y \\ = \\ \\begin{pmatrix} 3 \\\\ 1 \\\\ -1 \\end{pmatrix}$\n  - 组：$G_{1} = \\{1,2\\}$，$G_{2} = \\{3\\}$。\n  - 正则化参数：$\\lambda_{g} = 1$，$\\lambda_{f} = 1$。\n  - ADMM 惩罚参数：$\\rho_{g} = 1$，$\\rho_{f} = 1$。\n  - 初始值：$z_{g}^{(0)} = 0$，$z_{f}^{(0)} = 0$，$u_{g}^{(0)} = 0$，$u_{f}^{(0)} = 0$。\n\n### 第 2 步：使用提取的给定信息进行验证\n\n1.  **科学基础：** 该问题是凸优化领域的一个标准练习，特别是在用于线性回归的结构化稀疏正则化方法方面。目标函数结合了最小二乘数据保真项、组 LASSO 和融合 LASSO 正则化项。交替方向乘子法（ADMM）是解决此类问题的成熟且强大的算法。所有概念（邻近算子、增广拉格朗日量等）都是数学优化和信号处理领域的基础。该问题牢固地建立在既定理论之上。\n2.  **适定性：** 目标函数是三个凸函数的和：一个二次函数、组 $\\ell_{2,1}$-范数和一个与线性算子复合的 $\\ell_1$-范数。因此，这个和是凸的。如果 $A$ 的零空间和正则化项的零空间适当不相交，则该最小化问题是适定的，并有唯一解；更一般地，由于当 $A$ 具有满列秩时这是一个严格凸问题。问题具体、无歧义，并能根据所提供的数据得出唯一确定的结果。\n3.  **客观性：** 该问题使用精确的数学语言和符号陈述，没有任何主观性或偏见。\n\n### 第 3 步：结论与行动\n\n该问题是 **有效的**。它是计算优化领域一个适定、有科学依据且客观的问题。我将继续进行求解。\n\n***\n\n### 第 1 部分：组正则化项的邻近算子\n\n函数 $\\tau R_{g}$ 在点 $v \\in \\mathbb{R}^{n}$ 处的邻近算子定义为：\n$$ \\mathrm{prox}_{\\tau R_{g}}(v) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2}\\|z - v\\|_{2}^{2} + \\tau R_{g}(z) \\right\\} $$\n代入 $R_{g}(z) = \\sum_{k=1}^{K} \\|z_{G_{k}}\\|_{2}$ 的定义，目标函数变为：\n$$ \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2}\\sum_{i=1}^{n} (z_i - v_i)^2 + \\tau \\sum_{k=1}^{K} \\|z_{G_{k}}\\|_{2} \\right\\} $$\n由于组 $\\{G_k\\}$ 构成了索引的划分，平方欧几里得范数项可以按这些组进行分离：$\\|z - v\\|_{2}^{2} = \\sum_{k=1}^{K} \\|z_{G_k} - v_{G_k}\\|_{2}^{2}$。因此，最小化问题分解为 $K$ 个独立的子问题，每个组一个：\n$$ \\left[\\mathrm{prox}_{\\tau R_{g}}(v)\\right]_{G_k} = \\arg\\min_{z_{G_k} \\in \\mathbb{R}^{|G_k|}} \\left\\{ \\frac{1}{2}\\|z_{G_k} - v_{G_k}\\|_{2}^{2} + \\tau \\|z_{G_k}\\|_{2} \\right\\} $$\n这就是函数 $\\phi(u) = \\tau \\|u\\|_{2}$ 的邻近算子的定义。让我们对一个通用向量 $w = v_{G_k}$ 和变量 $u = z_{G_k}$ 求解这个问题。问题是找到 $u^\\star = \\arg\\min_{u} \\{\\frac{1}{2}\\|u - w\\|_{2}^{2} + \\tau \\|u\\|_{2}\\}$。\n一阶最优性条件是 $0 \\in \\nabla_u \\left(\\frac{1}{2}\\|u - w\\|_{2}^{2}\\right) + \\partial_u(\\tau \\|u\\|_{2})$，这给出 $0 \\in (u - w) + \\tau \\partial\\|u\\|_{2}$。\n欧几里得范数的次梯度为：如果 $u \\ne 0$，则 $\\partial\\|u\\|_{2} = \\{u/\\|u\\|_{2}\\}$；如果 $u = 0$，则 $\\partial\\|0\\|_{2} = \\{g \\in \\mathbb{R}^{|G_k|} : \\|g\\|_{2} \\le 1\\}$。\n\n情况 1：$u \\ne 0$。条件是 $w - u = \\tau \\frac{u}{\\|u\\|_{2}}$。这意味着 $w = u(1 + \\frac{\\tau}{\\|u\\|_{2}})$，表明 $u$ 必须是 $w$ 的一个正数倍缩放，比如 $u = \\alpha w$，其中 $\\alpha > 0$。取范数，我们得到 $\\|w\\|_{2} = \\|u\\|_{2} + \\tau$。所以，$\\|u\\|_{2} = \\|w\\|_{2} - \\tau$。这仅在 $\\|w\\|_{2} > \\tau$ 时有效。在这种情况下， $u = w \\frac{\\|u\\|_{2}}{\\|w\\|_{2}} = w \\frac{\\|w\\|_{2} - \\tau}{\\|w\\|_{2}} = \\left(1 - \\frac{\\tau}{\\|w\\|_{2}}\\right)w$。\n\n情况 2：如果我们假设 $\\|w\\|_{2} \\le \\tau$，我们来检验解 $u=0$。最优性条件变为 $0 \\in -w + \\tau \\partial\\|0\\|_{2}$，这意味着 $w \\in \\tau \\partial\\|0\\|_{2}$，即 $\\|w\\|_{2} \\le \\tau$。这与我们的假设一致。因此，如果 $\\|w\\|_{2} \\le \\tau$，最小化者是 $u=0$。\n\n结合两种情况，解（称为块软阈值算子）是：\n$$ u^\\star = \\max\\left(0, 1 - \\frac{\\tau}{\\|w\\|_{2}}\\right)w $$\n将此结果应用于每个组块 $G_k$，我们得到 $R_g$ 的邻近算子：\n$$ \\left[\\mathrm{prox}_{\\tau R_{g}}(v)\\right]_{G_k} = \\left(1 - \\frac{\\tau}{\\|v_{G_k}\\|_{2}}\\right)_{+} v_{G_k} = \\max\\left(0, 1 - \\frac{\\tau}{\\|v_{G_k}\\|_{2}}\\right) v_{G_k} $$\n\n### 第 2 部分：ADMM 公式化与更新\n\n通过引入辅助变量 $z_g$ 和 $z_f$ 来拆分原问题。问题重写为：\n$$ \\min_{x, z_g, z_f} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda_{g} R_{g}(z_g) + \\lambda_{f} \\|z_f\\|_{1} \\quad \\text{s.t.} \\quad x-z_g = 0, \\ Dx - z_f = 0 $$\n使用缩放对偶变量 $u_g, u_f$ 的增广拉格朗日量是：\n$$ L_{\\rho}(x, z_g, z_f, u_g, u_f) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda_{g} R_{g}(z_g) + \\lambda_{f} \\|z_f\\|_{1} + \\frac{\\rho_g}{2}\\|x - z_g + u_g\\|_{2}^{2} - \\frac{\\rho_g}{2}\\|u_g\\|_{2}^{2} + \\frac{\\rho_f}{2}\\|Dx - z_f + u_f\\|_{2}^{2} - \\frac{\\rho_f}{2}\\|u_f\\|_{2}^{2} $$\nADMM 算法包括针对每个主变量迭代最小化 $L_{\\rho}$，然后更新对偶变量。在第 $k+1$ 次迭代时：\n\n1.  **$x$-更新：**\n    $x^{(k+1)} = \\arg\\min_{x} \\left\\{ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{\\rho_g}{2}\\|x - z_g^{(k)} + u_g^{(k)}\\|_{2}^{2} + \\frac{\\rho_f}{2}\\|Dx - z_f^{(k)} + u_f^{(k)}\\|_{2}^{2} \\right\\}$。\n    这是一个二次最小化问题。将关于 $x$ 的梯度设为零：\n    $A^T(Ax - y) + \\rho_g(x - z_g^{(k)} + u_g^{(k)}) + D^T\\rho_f(Dx - z_f^{(k)} + u_f^{(k)}) = 0$。\n    重新整理各项，得到关于 $x$ 的线性系统：\n    $(A^T A + \\rho_g I + \\rho_f D^T D) x = A^T y + \\rho_g(z_g^{(k)} - u_g^{(k)}) + \\rho_f D^T(z_f^{(k)} - u_f^{(k)})$。\n    这是一个 **线性系统求解**。\n\n2.  **$z_g$-更新：**\n    $z_g^{(k+1)} = \\arg\\min_{z_g} \\left\\{ \\lambda_{g} R_{g}(z_g) + \\frac{\\rho_g}{2}\\|x^{(k+1)} - z_g + u_g^{(k)}\\|_{2}^{2} \\right\\}$。\n    重新整理二次项：\n    $z_g^{(k+1)} = \\arg\\min_{z_g} \\left\\{ \\frac{\\rho_g}{2}\\|z_g - (x^{(k+1)} + u_g^{(k)})\\|_{2}^{2} + \\lambda_{g} R_{g}(z_g) \\right\\}$。\n    这是 **邻近算子** 的定义：\n    $z_g^{(k+1)} = \\mathrm{prox}_{\\lambda_g/\\rho_g R_g}(x^{(k+1)} + u_g^{(k)})$。\n\n3.  **$z_f$-更新：**\n    $z_f^{(k+1)} = \\arg\\min_{z_f} \\left\\{ \\lambda_{f} \\|z_f\\|_{1} + \\frac{\\rho_f}{2}\\|Dx^{(k+1)} - z_f + u_f^{(k)}\\|_{2}^{2} \\right\\}$。\n    重新整理二次项：\n    $z_f^{(k+1)} = \\arg\\min_{z_f} \\left\\{ \\frac{\\rho_f}{2}\\|z_f - (Dx^{(k+1)} + u_f^{(k)})\\|_{2}^{2} + \\lambda_{f} \\|z_f\\|_{1} \\right\\}$。\n    这是 $\\ell_1$-范数的 **邻近算子** 的定义（逐元素软阈值）：\n    $z_f^{(k+1)} = \\mathrm{prox}_{\\lambda_f/\\rho_f \\|\\cdot\\|_1}(Dx^{(k+1)} + u_f^{(k)}) = S_{\\lambda_f/\\rho_f}(Dx^{(k+1)} + u_f^{(k)})$。\n\n4.  **对偶变量更新：**\n    $u_g^{(k+1)} = u_g^{(k)} + x^{(k+1)} - z_g^{(k+1)}$。\n    $u_f^{(k+1)} = u_f^{(k)} + Dx^{(k+1)} - z_f^{(k+1)}$。\n\n### 第 3 部分：投影到融合约束图上\n\n问题是找到解 $(x^\\star, u^\\star)$：\n$$ \\min_{x \\in \\mathbb{R}^{n}, \\, u \\in \\mathbb{R}^{n-1}} \\ \\frac{1}{2}\\|x - \\hat{x}\\|_{2}^{2} + \\frac{1}{2}\\|u - \\hat{u}\\|_{2}^{2} \\quad \\text{subject to} \\quad u = D x $$\n我们可以通过将约束 $u = Dx$ 代入目标函数来消去 $u$。这得到了一个只关于 $x$ 的无约束最小化问题：\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\ J(x) = \\frac{1}{2}\\|x - \\hat{x}\\|_{2}^{2} + \\frac{1}{2}\\|Dx - \\hat{u}\\|_{2}^{2} $$\n函数 $J(x)$ 是二次且凸的。我们通过将梯度 $\\nabla J(x)$ 设为零来找到最小化者 $x^\\star$。\n$$ \\nabla J(x) = (x - \\hat{x}) + D^T(Dx - \\hat{u}) = 0 $$\n重新整理各项以求解 $x$：\n$$ x + D^T D x = \\hat{x} + D^T \\hat{u} $$\n$$ (I + D^T D) x = \\hat{x} + D^T \\hat{u} $$\n矩阵 $(I + D^T D)$ 是对称正定的，因此是可逆的。$x^\\star$ 的解是：\n$$ x^{\\star} = (I + D^T D)^{-1} (\\hat{x} + D^T \\hat{u}) $$\n相应的最优 $u^\\star$ 可由约束条件得出：\n$$ u^{\\star} = D x^{\\star} = D(I + D^T D)^{-1} (\\hat{x} + D^T \\hat{u}) $$\n\n### 第 4 部分：第一次 ADMM 迭代计算\n\n我们需要使用第 2 部分的更新规则来计算 $x^{(1)}$。初始值为 $z_g^{(0)}=0$，$z_f^{(0)}=0$，$u_g^{(0)}=0$，$u_f^{(0)}=0$。\n$x$-更新为：\n$$ (A^T A + \\rho_g I + \\rho_f D^T D) x^{(1)} = A^T y + \\rho_g(z_g^{(0)} - u_g^{(0)}) + \\rho_f D^T(z_f^{(0)} - u_f^{(0)}) $$\n代入初始值，右侧（RHS）显著简化：\n$$ \\text{RHS} = A^T y + \\rho_g(0 - 0) + \\rho_f D^T(0 - 0) = A^T y $$\n给定 $A=I_3$，RHS 为 $I_3 y = y = \\begin{pmatrix} 3 \\\\ 1 \\\\ -1 \\end{pmatrix}$。\n左侧（LHS）的矩阵是 $M = A^T A + \\rho_g I + \\rho_f D^T D$。\n当 $A = I_3$，$\\rho_g = 1$ 且 $\\rho_f = 1$ 时，这变为 $M = I_3^T I_3 + 1 \\cdot I_3 + 1 \\cdot D^T D = 2I_3 + D^T D$。\n首先，计算 $D^T D$：\n$$ D = \\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\end{pmatrix}, \\quad D^T = \\begin{pmatrix} -1  0 \\\\ 1  -1 \\\\ 0  1 \\end{pmatrix} $$\n$$ D^T D = \\begin{pmatrix} -1  0 \\\\ 1  -1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\end{pmatrix} = \\begin{pmatrix} 1  -1  0 \\\\ -1  1+1  -1 \\\\ 0  -1  1 \\end{pmatrix} = \\begin{pmatrix} 1  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  1 \\end{pmatrix} $$\n现在计算完整矩阵 $M$：\n$$ M = 2I_3 + D^T D = \\begin{pmatrix} 2  0  0 \\\\ 0  2  0 \\\\ 0  0  2 \\end{pmatrix} + \\begin{pmatrix} 1  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  1 \\end{pmatrix} = \\begin{pmatrix} 3  -1  0 \\\\ -1  4  -1 \\\\ 0  -1  3 \\end{pmatrix} $$\n我们必须求解线性系统 $M x^{(1)} = y$：\n$$ \\begin{pmatrix} 3  -1  0 \\\\ -1  4  -1 \\\\ 0  -1  3 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\\\ -1 \\end{pmatrix} $$\n这对应于方程组：\n1. $3x_1 - x_2 = 3$\n2. $-x_1 + 4x_2 - x_3 = 1$\n3. $-x_2 + 3x_3 = -1$\n\n从(1)式，我们有 $x_2 = 3x_1 - 3$。\n从(3)式，我们有 $3x_3 = x_2 - 1$。代入 $x_2$：$3x_3 = (3x_1 - 3) - 1 = 3x_1 - 4$，所以 $x_3 = x_1 - \\frac{4}{3}$。\n将 $x_2$ 和 $x_3$ 代入 (2)式：\n$$ -x_1 + 4(3x_1 - 3) - (x_1 - \\frac{4}{3}) = 1 $$\n$$ -x_1 + 12x_1 - 12 - x_1 + \\frac{4}{3} = 1 $$\n$$ 10x_1 = 1 + 12 - \\frac{4}{3} = 13 - \\frac{4}{3} = \\frac{39-4}{3} = \\frac{35}{3} $$\n$$ x_1 = \\frac{35}{30} = \\frac{7}{6} $$\n现在反向代入以求得 $x_2$ 和 $x_3$：\n$$ x_2 = 3\\left(\\frac{7}{6}\\right) - 3 = \\frac{7}{2} - \\frac{6}{2} = \\frac{1}{2} $$\n$$ x_3 = \\frac{7}{6} - \\frac{4}{3} = \\frac{7}{6} - \\frac{8}{6} = -\\frac{1}{6} $$\n因此，第一个 $x$ 的迭代结果是 $x^{(1)} = \\begin{pmatrix} 7/6 \\\\ 1/2 \\\\ -1/6 \\end{pmatrix}$。",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{7}{6}  \\frac{1}{2}  -\\frac{1}{6} \\end{pmatrix} } $$"
        },
        {
            "introduction": "一个稳健的数值算法应该对其输入数据的任意缩放不敏感。本练习从理论转向实践，探讨了ADMM的一个关键问题：数据缩放如何影响算法参数的选择和收敛行为。通过分析数据尺度、正则化参数 $\\lambda$ 和ADMM罚参数 $\\rho$ 之间的相互作用，您将理解为何数据标准化是确保算法稳定、可预测收敛的关键一步。",
            "id": "3429991",
            "problem": "考虑在压缩感知设置中的最小绝对值收敛和选择算子 (LASSO) 问题：对于一个向量 $x \\in \\mathbb{R}^n$，最小化目标函数 $J(x) = \\tfrac{1}{2}\\,\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1$，其中数据矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，测量值为 $b \\in \\mathbb{R}^m$，正则化参数为 $\\lambda  0$。假设您使用一个辅助变量 $z \\in \\mathbb{R}^n$ 和惩罚参数 $\\rho  0$，应用交替方向乘子法 (ADMM, Alternating Direction Method of Multipliers) 的标准变量分裂形式。$z$ 的更新涉及一个软阈值步骤，其阈值大小取决于一个结合了 $\\lambda$ 和 $\\rho$ 的比率。现在考虑对数据进行均匀重缩放 $(A,b) \\mapsto (s A, s b)$，因子为 $s  0$，而不改变变量 $x$。您希望保持未缩放问题的最优解，并针对此类重缩放稳定 ADMM $z$-更新中使用的软阈值大小。\n\n以下哪个陈述最好地描述了重缩放对 $z$-更新中使用的有效软阈值大小的影响，并推荐了一种归一化策略来确保在不同 $s$ 选择下的稳定性？\n\nA. 在 $(A,b) \\mapsto (s A, s b)$ 的重缩放下，为了保持相同的最优解，必须将 $\\lambda$ 替换为 $\\lambda' = s^2 \\lambda$。为了保持 $z$-更新中使用的软阈值大小并使整个 ADMM 迭代对此重缩放不变，也应设置 $\\rho' = s^2 \\rho$，这保持了关键比率。一种实用的归一化方法是对 $A$ 进行列归一化，使每列具有单位 $\\ell_2$-范数，并选择 $\\rho$ 与 $\\|A\\|_2^2$ 成正比，这在测量单位变化时能保持软阈值大小在数值上的稳定性。\n\nB. $z$-更新的阈值不依赖于 $A$ 或 $b$，因此 $(A,b) \\mapsto (s A, s b)$ 对软阈值大小没有影响；所以不需要调整 $\\lambda$ 或 $\\rho$，并且对 $A$ 或 $b$ 进行归一化是不必要的，且可能会使解产生偏差。\n\nC. 为了在 $(A,b) \\mapsto (s A, s b)$ 后维持相同的软阈值大小，设置 $\\lambda' = s \\lambda$ 并保持 $\\rho$ 不变，并将 $A$ 的行标准化为单位 $\\ell_1$-范数，以便稳定阈值。\n\nD. 选择 $\\rho$ 与 $\\|A\\|_2^2$ 成反比，同时保持 $\\lambda$ 固定，可以保证在 $(A,b) \\mapsto (s A, s b)$ 的重缩放下软阈值大小按 $s^2$ 的比例减小，这是可取的，因为它增强了数据保真度并加速了收敛；不需要进一步的归一化。",
            "solution": "用户需要一份关于将交替方向乘子法 (ADMM) 应用于 LASSO 问题的分析，特别是关于数据重缩放对算法参数和行为的影响。\n\n### 步骤 1：提取已知条件\n\n-   **目标函数 (LASSO):** $J(x) = \\tfrac{1}{2}\\,\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1$\n-   **变量和数据:** $x \\in \\mathbb{R}^n$ (优化变量), $A \\in \\mathbb{R}^{m \\times n}$ (数据矩阵), $b \\in \\mathbb{R}^m$ (测量值)。\n-   **参数:** $\\lambda  0$ (正则化参数), $\\rho  0$ (ADMM 惩罚参数)。\n-   **算法:** ADMM，使用一个辅助变量 $z \\in \\mathbb{R}^n$ 的标准变量分裂形式。\n-   **变换:** 数据的均匀重缩放 $(A,b) \\mapsto (s A, s b)$，缩放因子为 $s  0$。\n-   **目标:**\n    1.  保持未缩放问题的最优解。\n    2.  确保 ADMM $z$-更新中软阈值的大小对此重缩放具有稳定性。\n-   **问题:** 描述重缩放的影响并推荐一种归一化策略。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n问题陈述在科学上和数学上是合理的。LASSO 目标函数是统计学、信号处理和机器学习领域一个标准且被充分研究的问题。ADMM 是解决该问题的经典算法，所描述的变量分裂形式是最常用的方法。关于算法参数应如何响应数据缩放进行调整的问题，是数值优化领域一个基础且实际的问题。该问题提法得当、客观，并包含足够的信息以进行严谨的分析。未检测到缺陷。\n\n### 步骤 3：结论和行动\n\n问题陈述是**有效的**。开始求解。\n\n### 推导与分析\n\n**1. LASSO 的 ADMM 形式**\n\nLASSO 问题是：\n$$ \\underset{x \\in \\mathbb{R}^n}{\\text{minimize}} \\quad \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1 $$\n为了应用 ADMM，我们使用变量分裂。我们引入一个辅助变量 $z \\in \\mathbb{R}^n$，并将问题重构为一个约束优化问题：\n$$ \\underset{x, z \\in \\mathbb{R}^n}{\\text{minimize}} \\quad \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|z\\|_1 \\quad \\text{subject to} \\quad x - z = 0 $$\n该问题的增广拉格朗日量（以其缩放对偶变量形式表示）是：\n$$ L_\\rho(x, z, y) = \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|z\\|_1 + \\frac{\\rho}{2}\\|x - z + y\\|_2^2 - \\frac{\\rho}{2}\\|y\\|_2^2 $$\n其中 $y$ 是缩放对偶变量。ADMM 算法包括交替地对 $L_\\rho$ 关于 $x$ 和 $z$ 进行最小化，然后更新 $y$：\n1.  **x-更新:** $x^{k+1} = \\underset{x}{\\arg\\min} \\left( \\frac{1}{2}\\|Ax - b\\|_2^2 + \\frac{\\rho}{2}\\|x - z^k + y^k\\|_2^2 \\right)$\n2.  **z-更新:** $z^{k+1} = \\underset{z}{\\arg\\min} \\left( \\lambda \\|z\\|_1 + \\frac{\\rho}{2}\\|x^{k+1} - z + y^k\\|_2^2 \\right)$\n3.  **y-更新:** $y^{k+1} = y^k + x^{k+1} - z^{k+1}$\n\nz-更新是 $\\ell_1$-范数的近端算子。其解由软阈值函数 $S_K(\\cdot)$ 给出：\n$$ z^{k+1} = S_{\\lambda/\\rho}(x^{k+1} + y^k) $$\n**软阈值大小**是关键参数 $K = \\lambda/\\rho$。\n\n**2. 数据重缩放对最优解的影响**\n\n考虑数据重缩放 $(A, b) \\mapsto (A', b') = (s A, s b)$，其中 $s > 0$。新的目标函数，带有一个可能是新的正则化参数 $\\lambda'$，是：\n$$ J'(x) = \\frac{1}{2}\\|A'x - b'\\|_2^2 + \\lambda' \\|x\\|_1 = \\frac{1}{2}\\|sAx - sb\\|_2^2 + \\lambda' \\|x\\|_1 = \\frac{s^2}{2}\\|Ax - b\\|_2^2 + \\lambda' \\|x\\|_1 $$\n为了保持原问题的最优解，新目标函数 $J'(x)$ 必须等价于原目标函数 $J(x)$，即它必须是 $J(x)$ 的一个正常数倍。让我们将 $J(x)$ 乘以 $s^2$：\n$$ s^2 J(x) = s^2 \\left( \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1 \\right) = \\frac{s^2}{2}\\|Ax - b\\|_2^2 + s^2 \\lambda \\|x\\|_1 $$\n通过比较 $J'(x)$ 和 $s^2 J(x)$，我们看到要使它们是同一个函数（从而有相同的最小化器），我们必须设置：\n$$ \\lambda' = s^2 \\lambda $$\n这是在数据重缩放下为保持 LASSO 问题解所需的对 $\\lambda$ 的缩放。\n\n**3. 重缩放对 ADMM 和阈值稳定性的影响**\n\n我们现在希望选择新的惩罚参数 $\\rho'$ 来稳定 ADMM 算法。问题明确要求保持软阈值的大小。\n原始阈值大小为 $K = \\lambda/\\rho$。\n新的阈值大小为 $K' = \\lambda'/\\rho'$。\n为保持大小不变，我们设置 $K' = K$：\n$$ \\frac{\\lambda'}{\\rho'} = \\frac{\\lambda}{\\rho} $$\n代入 $\\lambda' = s^2 \\lambda$，我们得到：\n$$ \\frac{s^2 \\lambda}{\\rho'} = \\frac{\\lambda}{\\rho} \\implies \\rho' = s^2 \\rho $$\n因此，为保持恒定的软阈值大小，惩罚参数 $\\rho$ 也必须按 $s^2$ 缩放。\n\n让我们验证这个选择 $(\\lambda, \\rho) \\mapsto (\\lambda', \\rho') = (s^2\\lambda, s^2\\rho)$ 是否使整个 ADMM 迭代不变。新的 x-更新是线性系统的解：\n$$ ((A')^T A' + \\rho'I) x = (A')^T b' + \\rho'(z^k - y^k) $$\n代入 $A'=sA$、$b'=sb$ 以及 $\\rho'=s^2\\rho$：\n$$ ((sA)^T(sA) + s^2\\rho I) x = (sA)^T(sb) + s^2\\rho(z^k - y^k) $$\n$$ (s^2 A^T A + s^2\\rho I) x = s^2 A^T b + s^2\\rho(z^k - y^k) $$\n两边同时除以 $s^2$ (因为 $s>0$)，我们得到：\n$$ (A^T A + \\rho I) x = A^T b + \\rho(z^k - y^k) $$\n这与原始的 x-更新方程完全相同。因此，如果迭代变量 $(z^k, y^k)$ 相同，新的迭代变量 $x^{k+1}$ 也将相同。\n新的 z-更新是 $z^{k+1} = S_{\\lambda'/\\rho'}(x^{k+1}+y^k)$。因为 $\\lambda'/\\rho' = (s^2\\lambda)/(s^2\\rho) = \\lambda/\\rho$ 并且 x-更新是不变的，所以 z-更新也是不变的。\ny-更新仅依赖于原始变量 $x$ 和 $z$，因此它也是不变的。\n结论是，在数据重缩放 $(A,b) \\mapsto (sA, sb)$ 下，整个 ADMM 迭代序列保持不变，当且仅当参数按 $(\\lambda, \\rho) \\mapsto (s^2\\lambda, s^2\\rho)$ 进行缩放。\n\n**4. 实用的归一化策略**\n\n上述分析揭示了数据缩放与算法参数的最优选择之间存在强耦合关系。一个鲁棒的实现应该对输入数据的任意缩放不敏感。实现这一点的一个常用方法是首先对数据进行归一化。对于 LASSO，标准做法是将 $A$ 的列归一化，使其具有单位 $\\ell_2$-范数。这能确保所有特征都在可比较的尺度上。\n\n此外，选择 $\\rho$ 的一个常用启发式方法是平衡 x-更新中的各项。x-更新涉及对矩阵 $(A^TA + \\rho I)$ 求逆。为了使该矩阵良态并平衡两项的影响，$\\rho$ 的数量级应与 $A^TA$ 的特征值相同。$A^TA$ 的最大特征值是 $\\|A\\|_2^2$。因此，一个合理的启发式方法是选择 $\\rho$ 与 $\\|A\\|_2^2$ 成正比。\n\n让我们检查这个启发式方法是否与我们的缩放规则一致。假设我们采用规则 $\\rho = c \\|A\\|_2^2$ (对于某个常数 $c$)。如果我们将 $A$ 重缩放为 $A' = sA$，该规则建议新的惩罚参数应为 $\\rho' = c \\|A'\\|_2^2 = c \\|sA\\|_2^2 = c s^2 \\|A\\|_2^2 = s^2 (c \\|A\\|_2^2) = s^2 \\rho$。这与我们为迭代不变性推导出的要求完全一致。因此，这种实用策略自动地执行了正确的缩放。\n\n### 选项评估\n\n*   **A. 在 $(A,b) \\mapsto (s A, s b)$ 的重缩放下，为了保持相同的最优解，必须将 $\\lambda$ 替换为 $\\lambda' = s^2 \\lambda$。为了保持 $z$-更新中使用的软阈值大小并使整个 ADMM 迭代对此重缩放不变，也应设置 $\\rho' = s^2 \\rho$，这保持了关键比率。一种实用的归一化方法是对 $A$ 进行列归一化，使每列具有单位 $\\ell_2$-范数，并选择 $\\rho$ 与 $\\|A\\|_2^2$ 成正比，这在测量单位变化时能保持软阈值大小在数值上的稳定性。**\n    该陈述与我们的推导完全一致。缩放规则 $\\lambda' = s^2 \\lambda$ 和 $\\rho' = s^2 \\rho$ 是正确的。保持比率 $\\lambda/\\rho$ 不变是正确的。推荐的实用策略，即列归一化和选择 $\\rho \\propto \\|A\\|_2^2$，是标准的、合理的，并与理论发现一致。\n    **结论：正确。**\n\n*   **B. $z$-更新的阈值不依赖于 $A$ 或 $b$，因此 $(A,b) \\mapsto (s A, s b)$ 对软阈值大小没有影响；所以不需要调整 $\\lambda$ 或 $\\rho$，并且对 $A$ 或 $b$ 进行归一化是不必要的，且可能会使解产生偏差。**\n    这是不正确的。虽然阈值公式 $\\lambda/\\rho$ 没有显式包含 $A$ 或 $b$，但问题背景是由 $A$ 和 $b$ 定义的。为了保持问题的解，$\\lambda$ 必须调整。如果 $\\lambda$ 被调整，$\\rho$ 也必须被调整以保持阈值不变。不调整 $\\lambda$ 会改变所求解的问题。声称归一化不必要与数值优化的最佳实践相悖。\n    **结论：不正确。**\n\n*   **C. 为了在 $(A,b) \\mapsto (s A, s b)$ 后维持相同的软阈值大小，设置 $\\lambda' = s \\lambda$ 并保持 $\\rho$ 不变，并将 $A$ 的行标准化为单位 $\\ell_1$-范数，以便稳定阈值。**\n    缩放 $\\lambda' = s\\lambda$ 是不正确的；为了保持最优解，它应该是 $\\lambda' = s^2\\lambda$。因此，建立在这个错误前提之上的陈述的其余部分也是不正确的。在改变 $\\lambda$ 的同时保持 $\\rho$ 不变会改变阈值的大小。\n    **结论：不正确。**\n\n*   **D. 选择 $\\rho$ 与 $\\|A\\|_2^2$ 成反比，同时保持 $\\lambda$ 固定，可以保证在 $(A,b) \\mapsto (s A, s b)$ 的重缩放下软阈值大小按 $s^2$ 的比例减小，这是可取的，因为它增强了数据保真度并加速了收敛；不需要进一步的归一化。**\n    这个陈述包含多个错误。首先，在缩放数据的同时保持 $\\lambda$ 固定会改变问题的解，这违背了陈述的目标之一。其次，如果 $\\rho \\propto 1/\\|A\\|_2^2$，那么 $\\rho' = \\rho/s^2$。在 $\\lambda'=\\lambda$ 的情况下，新的阈值将是 $K' = \\lambda'/\\rho' = \\lambda/(\\rho/s^2) = s^2(\\lambda/\\rho)$，这是一个 $s^2$ 倍的*增加*，而不是减少。第三，更大的阈值意味着更强的正则化（更低的数据保真度），这与其声明相矛盾。\n    **结论：不正确。**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}