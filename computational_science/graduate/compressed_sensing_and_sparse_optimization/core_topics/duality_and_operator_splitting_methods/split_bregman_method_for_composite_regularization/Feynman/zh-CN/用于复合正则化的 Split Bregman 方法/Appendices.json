{
    "hands_on_practices": [
        {
            "introduction": "分裂Bregman方法将一个复杂的优化问题分解为一系列交替进行的更简单的子问题。我们将要处理的第一个子问题是主变量 $u$ 的更新，这一步通常是一个二次最小化问题，可以通过找到梯度为零的点来求解，从而得到一组我们熟悉的称为正规方程的线性方程组。 这个练习将帮助你熟练推导该算法的这一基本组成部分。",
            "id": "3480428",
            "problem": "考虑压缩感知中的复合正则化问题，该问题旨在通过最小化一个最小二乘数据保真度项和多个通过线性算子施加的非光滑正则化项之和，从数据 $b \\in \\mathbb{R}^{m}$ 中寻求一个估计 $u \\in \\mathbb{R}^{n}$。具体来说，设 $A \\in \\mathbb{R}^{m \\times n}$ 为一个已知的感知矩阵，并设 $\\{K_{i}\\}_{i=1}^{r}$ 为一组已知的线性算子，其中 $K_{i} \\in \\mathbb{R}^{p_{i} \\times n}$。数据保真度项为 $f(u) = \\tfrac{1}{2}\\|A u - b\\|_{2}^{2}$。为了处理具有多个变换的非光滑项，应用了分裂 Bregman 方法，引入了分裂变量 $\\{d_{i}\\}_{i=1}^{r}$ 和 Bregman 变量 $\\{b_{i}\\}_{i=1}^{r}$。在第 $k$ 次迭代时，给定固定的 $\\{d_{i}^{k}\\}$ 和 $\\{b_{i}^{k}\\}$，更新 $u$ 的步骤需要求解以下严格凸二次子问题\n$$\nu^{k+1} \\in \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\tfrac{1}{2}\\|A u - b\\|_{2}^{2} + \\tfrac{\\mu}{2} \\sum_{i=1}^{r} \\|K_{i} u - d_{i}^{k} + b_{i}^{k}\\|_{2}^{2} \\right\\},\n$$\n其中 $\\mu > 0$ 是一个固定的罚参数。假设矩阵\n$$\nA^{\\top} A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top} K_{i}\n$$\n是可逆的，例如，如果 $A$ 具有满列秩或者 $\\sum_{i=1}^{r} K_{i}^{\\top} K_{i}$ 是正定的，则该条件可以得到保证。从多元微积分和线性代数的基本事实出发——即可微的严格凸函数在其梯度为零处达到其唯一的极小值点，以及对于任何维度兼容的矩阵 $M$ 和向量 $x,y$，有 $\\nabla_{x} \\left(\\tfrac{1}{2}\\|M x - y\\|_{2}^{2}\\right) = M^{\\top} (M x - y)$——推导用于更新 $u$ 的正规方程，并显式地解出 $u^{k+1}$。\n\n你的最终答案必须是 $u^{k+1}$ 关于 $A$、$b$、$\\{K_{i}\\}$、$\\{d_{i}^{k}\\}$、$\\{b_{i}^{k}\\}$ 和 $\\mu$ 的单一闭式解析表达式。不需要进行数值近似或四舍五入。",
            "solution": "该问题要求为在复合正则化的分裂 Bregman 方法中出现的特定目标函数推导一个使其最小化的向量 $u^{k+1}$ 的闭式表达式。\n\n需要对 $u \\in \\mathbb{R}^{n}$ 最小化的目标函数由下式给出\n$$\n\\mathcal{L}(u) = \\frac{1}{2}\\|A u - b\\|_{2}^{2} + \\frac{\\mu}{2} \\sum_{i=1}^{r} \\|K_{i} u - d_{i}^{k} + b_{i}^{k}\\|_{2}^{2}\n$$\n其中 $A$、$b$、$\\mu$、$\\{K_i\\}$、$\\{d_i^k\\}$ 和 $\\{b_i^k\\}$ 是给定​​量。问题陈述该函数关于 $u$ 是严格凸和可微的。根据微积分的基本原理，一个严格凸可微函数的唯一极小值点 $u^{k+1}$ 可以在其关于目标变量的梯度等于零向量的点找到。因此，我们必须求解方程 $\\nabla_{u} \\mathcal{L}(u^{k+1}) = 0$。\n\n我们可以通过分别求每一项的梯度来计算 $\\mathcal{L}(u)$ 的梯度。目标函数是两个分量的和。\n第一个分量是数据保真度项，$\\mathcal{L}_1(u) = \\frac{1}{2}\\|A u - b\\|_{2}^{2}$。\n第二个分量是正则化项，$\\mathcal{L}_2(u) = \\frac{\\mu}{2} \\sum_{i=1}^{r} \\|K_{i} u - d_{i}^{k} + b_{i}^{k}\\|_{2}^{2}$。\n\n问题提供了二次型的梯度公式：对于矩阵 $M$ 和向量 $x, y$，我们有 $\\nabla_{x} \\left(\\frac{1}{2}\\|M x - y\\|_{2}^{2}\\right) = M^{\\top} (M x - y)$。\n\n将此公式应用于第一个分量 $\\mathcal{L}_1(u)$，其中 $M=A$，$x=u$，$y=b$，我们得到其梯度：\n$$\n\\nabla_{u} \\mathcal{L}_1(u) = A^{\\top}(A u - b)\n$$\n\n对于第二个分量 $\\mathcal{L}_2(u)$，我们利用梯度算子的线性性质。和的梯度是梯度的和：\n$$\n\\nabla_{u} \\mathcal{L}_2(u) = \\nabla_{u} \\left( \\frac{\\mu}{2} \\sum_{i=1}^{r} \\|K_{i} u - d_{i}^{k} + b_{i}^{k}\\|_{2}^{2} \\right) = \\frac{\\mu}{2} \\sum_{i=1}^{r} \\nabla_{u} \\|K_{i} u - (d_{i}^{k} - b_{i}^{k})\\|_{2}^{2}\n$$\n将提供的梯度公式应用于求和中的每一项，其中 $M = K_{i}$，$x = u$，$y = d_{i}^{k} - b_{i}^{k}$，我们得到：\n$$\n\\nabla_{u} \\|K_{i} u - (d_{i}^{k} - b_{i}^{k})\\|_{2}^{2} = 2 K_{i}^{\\top}(K_{i} u - (d_{i}^{k} - b_{i}^{k}))\n$$\n将其代回 $\\nabla_{u} \\mathcal{L}_2(u)$ 的表达式中：\n$$\n\\nabla_{u} \\mathcal{L}_2(u) = \\frac{\\mu}{2} \\sum_{i=1}^{r} 2 K_{i}^{\\top}(K_{i} u - d_{i}^{k} + b_{i}^{k}) = \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(K_{i} u - d_{i}^{k} + b_{i}^{k})\n$$\n\n$\\mathcal{L}(u)$ 的总梯度是其各分量梯度之和：\n$$\n\\nabla_{u} \\mathcal{L}(u) = \\nabla_{u} \\mathcal{L}_1(u) + \\nabla_{u} \\mathcal{L}_2(u) = A^{\\top}(A u - b) + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(K_{i} u - d_{i}^{k} + b_{i}^{k})\n$$\n在 $u = u^{k+1}$ 处将梯度设为零，得到用于更新 $u$ 的正规方程：\n$$\nA^{\\top}(A u^{k+1} - b) + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(K_{i} u^{k+1} - d_{i}^{k} + b_{i}^{k}) = 0\n$$\n为了解出 $u^{k+1}$，我们通过分配各项并将包含 $u^{k+1}$ 的项组合在一起，来重新整理这个方程：\n$$\nA^{\\top}A u^{k+1} - A^{\\top}b + \\mu \\sum_{i=1}^{r} \\left( K_{i}^{\\top}K_{i} u^{k+1} - K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k}) \\right) = 0\n$$\n$$\nA^{\\top}A u^{k+1} + \\mu \\left( \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right) u^{k+1} - A^{\\top}b - \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k}) = 0\n$$\n将所有不含 $u^{k+1}$ 的项移到方程右边：\n$$\nA^{\\top}A u^{k+1} + \\mu \\left( \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right) u^{k+1} = A^{\\top}b + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k})\n$$\n在方程左边提出 $u^{k+1}$：\n$$\n\\left( A^{\\top}A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right) u^{k+1} = A^{\\top}b + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k})\n$$\n问题陈述保证了矩阵 $\\left( A^{\\top}A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right)$ 是可逆的。因此，我们可以用它的逆矩阵左乘方程两边来分离出 $u^{k+1}$：\n$$\nu^{k+1} = \\left( A^{\\top}A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right)^{-1} \\left( A^{\\top}b + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k}) \\right)\n$$\n这个表达式就是 $u$-更新 $u^{k+1}$ 的显式闭式解。",
            "answer": "$$\n\\boxed{\\left( A^{\\top} A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top} K_{i} \\right)^{-1} \\left( A^{\\top} b + \\mu \\sum_{i=1}^{r} K_{i}^{\\top} (d_{i}^{k} - b_{i}^{k}) \\right)}\n$$"
        },
        {
            "introduction": "在更新了变量 $u$ 之后，下一步是更新辅助变量 $d$，这一步将非光滑正则化项分离出来。这个问题引入了邻近算子（proximal operator）这一强大概念，它是处理此类项的关键工具。你将推导出最常见且最重要的邻近算子——加权 $\\ell_1$ 范数的邻近算子，并会发现它对应于一个直观的操作，即软阈值（soft-thresholding）。",
            "id": "3480434",
            "problem": "考虑压缩感知中的复合正则化最小二乘问题，其包含一个线性测量算子 $A \\in \\mathbb{R}^{m \\times n}$，数据 $y \\in \\mathbb{R}^{m}$，以及一个作用于辅助分裂变量 $d \\in \\mathbb{R}^{p}$ 的稀疏性促进惩罚项：\n$$\n\\min_{u \\in \\mathbb{R}^{n},\\, d \\in \\mathbb{R}^{p}} \\; \\frac{1}{2}\\|Au - y\\|_{2}^{2} \\;+\\; \\lambda \\sum_{j=1}^{p} w_{j} |d_{j}| \\quad \\text{subject to} \\quad d = Ku,\n$$\n其中 $K \\in \\mathbb{R}^{p \\times n}$ 是一个固定的线性算子，且对所有 $j$ 都有 $w_{j} > 0$。使用变量分裂和分裂 Bregman 方法（已知其等价于交替方向乘子法（ADMM）的一种特定形式），在第 $k$ 次迭代中，对于固定的 $u^{k}$ 和 Bregman/对偶变量 $b^{k}$，通过最小化以下表达式来获得 $d$ 的更新：\n$$\n\\min_{d \\in \\mathbb{R}^{p}} \\; \\lambda \\sum_{j=1}^{p} w_{j} |d_{j}| \\;+\\; \\frac{\\mu}{2} \\left\\| d - \\left(Ku^{k} + b^{k}\\right) \\right\\|_{2}^{2},\n$$\n其中 $\\mu > 0$ 是一个固定的惩罚参数。从一个正常闭凸函数 $g$ 的近端算子的定义出发，\n$$\n\\mathrm{prox}_{\\tau g}(z) \\;=\\; \\arg\\min_{x} \\left\\{ g(x) + \\frac{1}{2\\tau}\\|x - z\\|_{2}^{2} \\right\\},\n$$\n并仅使用基本的凸分析原理（次微分最优性条件和可分性），推导加权 $\\ell_{1}$ 函数 $g(d) = \\sum_{j=1}^{p} w_{j}|d_{j}|$ 的近端算子的闭式表达式，并证明 $d$-更新等于 $\\mathrm{prox}_{(\\lambda/\\mu) g}\\!\\left(Ku^{k}+b^{k}\\right)$。\n\n然后，将您推导出的公式应用于以下 $p = 4$ 的具体实例：\n- 权重 $w = (w_{1}, w_{2}, w_{3}, w_{4}) = (1, 2, 0.5, 3)$,\n- 参数 $\\lambda = 1$ 和 $\\mu = 2$,\n- 自变量 $z = Ku^{k} + b^{k} = (2, -0.3, 0, 1.5)$。\n\n计算更新后的 $d^{k+1}$。使用标准数学表示法将您的最终答案表示为单个行向量。无需四舍五入，且不涉及物理单位。",
            "solution": "该问题要求完成两个主要任务：首先，推导加权 $\\ell_1$ 范数的近端算子的闭式表达式，并将其与分裂 Bregman 方法的 $d$-更新步骤联系起来；其次，将此公式应用于一个具体的数值实例。\n\n**第1部分：近端算子的推导及其与分裂 Bregman 更新的关系**\n\n设 $g(d) = \\sum_{j=1}^{p} w_{j} |d_{j}|$，其中 $w_j > 0$。带有参数 $\\tau > 0$ 的 $g$ 的近端算子定义为：\n$$\n\\mathrm{prox}_{\\tau g}(z) = \\arg\\min_{x \\in \\mathbb{R}^{p}} \\left\\{ \\tau g(x) + \\frac{1}{2}\\|x - z\\|_{2}^{2} \\right\\}\n$$\n代入 $g(x)$ 的表达式，目标函数为：\n$$\nF(x) = \\tau \\sum_{j=1}^{p} w_{j} |x_{j}| + \\frac{1}{2} \\sum_{j=1}^{p} (x_{j} - z_{j})^{2}\n$$\n该目标函数关于分量 $x_j$ 是可分的。因此，可以对每个分量独立地进行最小化：\n$$\n(\\mathrm{prox}_{\\tau g}(z))_j = \\arg\\min_{x_j \\in \\mathbb{R}} \\left\\{ \\tau w_j |x_j| + \\frac{1}{2}(x_j - z_j)^2 \\right\\}\n$$\n设 $x_j^*$ 是最小化点。由于目标函数是凸的，其最优性条件是在 $x_j^*$ 处，0 必须位于目标函数的次微分中。$|x_j|$ 的次微分是：\n$$\n\\partial|x_j| = \\begin{cases} \\{\\mathrm{sgn}(x_j)\\}  \\text{if } x_j \\neq 0 \\\\ [-1, 1]  \\text{if } x_j = 0 \\end{cases}\n$$\n最优性条件是在 $x_j^*$ 处求值的 $0 \\in \\partial \\left( \\tau w_j |x_j| + \\frac{1}{2}(x_j - z_j)^2 \\right)$。这得到：\n$$\n0 \\in \\tau w_j \\partial|x_j^*| + (x_j^* - z_j)\n$$\n重新整理，我们得到：\n$$\nz_j - x_j^* \\in \\tau w_j \\partial|x_j^*|\n$$\n我们根据 $x_j^*$ 的值分三种情况分析此条件：\n\n1.  **情况1：$x_j^* > 0$**。此时 $\\partial|x_j^*| = \\{1\\}$。条件变为 $z_j - x_j^* = \\tau w_j$，这意味着 $x_j^* = z_j - \\tau w_j$。这种情况仅在 $x_j^* > 0$ 时成立，因此我们需要 $z_j - \\tau w_j > 0$，即 $z_j > \\tau w_j$。\n\n2.  **情况2：$x_j^*  0$**。此时 $\\partial|x_j^*| = \\{-1\\}$。条件变为 $z_j - x_j^* = -\\tau w_j$，这意味着 $x_j^* = z_j + \\tau w_j$。这种情况仅在 $x_j^*  0$ 时成立，因此我们需要 $z_j + \\tau w_j  0$，即 $z_j  -\\tau w_j$。\n\n3.  **情况3：$x_j^* = 0$**。此时 $\\partial|x_j^*| = [-1, 1]$。条件变为 $z_j - 0 \\in \\tau w_j [-1, 1]$，这等价于 $|z_j| \\le \\tau w_j$。\n\n结合这三种情况，我们得到对于任意 $z_j$ 的解 $x_j^*$：\n$$\nx_j^* = \\begin{cases} z_j - \\tau w_j  \\text{if } z_j  \\tau w_j \\\\ 0  \\text{if } |z_j| \\le \\tau w_j \\\\ z_j + \\tau w_j  \\text{if } z_j  -\\tau w_j \\end{cases}\n$$\n这就是著名的逐元素软阈值算子，可以紧凑地写为：\n$$\nx_j^* = \\mathrm{sgn}(z_j) \\max(0, |z_j| - \\tau w_j)\n$$\n因此，加权 $\\ell_1$ 范数的近端算子是一个向量值函数，其第 $j$ 个分量是对 $z_j$ 进行阈值为 $\\tau w_j$ 的软阈值操作。\n\n接下来，我们将其与分裂 Bregman 方法中的 $d$-更新步骤联系起来。该更新由以下问题的解给出：\n$$\nd^{k+1} = \\arg\\min_{d \\in \\mathbb{R}^{p}} \\; \\lambda \\sum_{j=1}^{p} w_{j} |d_{j}| \\;+\\; \\frac{\\mu}{2} \\left\\| d - z \\right\\|_{2}^{2}\n$$\n其中我们记 $z = Ku^{k} + b^{k}$。我们可以通过将目标函数除以常数 $\\mu  0$ 来重写它，这样做不会改变其最小化点：\n$$\nd^{k+1} = \\arg\\min_{d \\in \\mathbb{R}^{p}} \\; \\frac{\\lambda}{\\mu} \\sum_{j=1}^{p} w_{j} |d_{j}| \\;+\\; \\frac{1}{2} \\left\\| d - z \\right\\|_{2}^{2}\n$$\n这个表达式正是 $\\mathrm{prox}_{\\tau g}(z)$ 的定义，其中 $g(d) = \\sum_{j=1}^{p} w_j |d_j|$ 且参数 $\\tau = \\lambda/\\mu$。因此，分裂 Bregman 方法的 $d$-更新确实是由一个近端算子给出的：\n$$\nd^{k+1} = \\mathrm{prox}_{(\\lambda/\\mu) g}\\!\\left(Ku^{k}+b^{k}\\right)\n$$\n\n**第2部分：应用于具体实例**\n\n我们已知以下数值：\n- 权重：$w = (1, 2, 0.5, 3)$\n- 参数：$\\lambda = 1$, $\\mu = 2$\n- 自变量向量：$z = Ku^{k} + b^{k} = (2, -0.3, 0, 1.5)$\n\n首先，我们计算近端算子参数 $\\tau$：\n$$\n\\tau = \\frac{\\lambda}{\\mu} = \\frac{1}{2} = 0.5\n$$\n接下来，我们计算阈值向量 $T$，其中 $T_j = \\tau w_j$：\n$$\nT = \\tau w = 0.5 \\times (1, 2, 0.5, 3) = (0.5, 1.0, 0.25, 1.5)\n$$\n现在我们将软阈值公式 $d_j^{k+1} = \\mathrm{sgn}(z_j) \\max(0, |z_j| - T_j)$ 应用于每个分量 $j=1, 2, 3, 4$：\n\n- 对于 $j=1$：$z_1 = 2$ 且 $T_1 = 0.5$。由于 $z_1 > T_1$，我们有：\n  $$ d_1^{k+1} = z_1 - T_1 = 2 - 0.5 = 1.5 = \\frac{3}{2} $$\n\n- 对于 $j=2$：$z_2 = -0.3$ 且 $T_2 = 1.0$。由于 $|z_2| = 0.3 \\le T_2$，我们有：\n  $$ d_2^{k+1} = 0 $$\n\n- 对于 $j=3$：$z_3 = 0$ 且 $T_3 = 0.25$。由于 $|z_3| = 0 \\le T_3$，我们有：\n  $$ d_3^{k+1} = 0 $$\n\n- 对于 $j=4$：$z_4 = 1.5$ 且 $T_4 = 1.5$。由于 $|z_4| = 1.5 \\le T_4$，我们有：\n  $$ d_4^{k+1} = 0 $$\n\n综合这些分量，更新后的向量 $d^{k+1}$ 是：\n$$\nd^{k+1} = \\left(\\frac{3}{2}, 0, 0, 0\\right)\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{2}  0  0  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "邻近算子框架的威力并不仅限于处理简单的 $\\ell_1$ 范数。这个练习将这一概念推广到弹性网络（elastic net）正则化项，它结合了 $\\ell_1$ 范数和平方 $\\ell_2$ 范数。通过推导其邻近算子，你将理解分裂Bregman方法如何优雅地处理更复杂的复合正则化项，以及底层的更新规则是如何被相应修正的。",
            "id": "3480393",
            "problem": "考虑弹性网络正则化项 $g:\\mathbb{R}^{n}\\to\\mathbb{R}$，其定义为 $g(d)=\\alpha\\|d\\|_{1}+\\frac{\\beta}{2}\\|d\\|_{2}^{2}$，其中 $\\alpha0$ 且 $\\beta\\geq 0$。设正常闭凸函数 $h$ 的近端算子定义为 $\\mathrm{prox}_{\\tau h}(z)=\\arg\\min_{x\\in\\mathbb{R}^{n}}\\left\\{\\frac{1}{2}\\|x-z\\|_{2}^{2}+\\tau h(x)\\right\\}$，其中 $\\tau0$。给定一个带约束的复合正则化最小二乘模型，其中包含线性分裂变量 $d\\in\\mathbb{R}^{m}$ 和线性算子 $K:\\mathbb{R}^{n}\\to\\mathbb{R}^{m}$：\n\n$$\n\\min_{u\\in\\mathbb{R}^{n},\\,d\\in\\mathbb{R}^{m}} \\;\\; \\frac{1}{2}\\|Au-b\\|_{2}^{2} + g(d) \\quad \\text{subject to} \\quad Ku=d,\n$$\n\n其中 $A\\in\\mathbb{R}^{p\\times n}$ 和 $b\\in\\mathbb{R}^{p}$ 是给定的。在分裂 Bregman (SB) 方法中，对于罚参数 $\\mu0$ 和当前的 Bregman 变量 $b^{k}\\in\\mathbb{R}^{m}$，第 $k$ 次迭代的 $d$-更新步骤形式如下\n\n$$\nd^{k+1}=\\arg\\min_{d\\in\\mathbb{R}^{m}} \\left\\{ g(d) + \\frac{\\mu}{2}\\left\\|d - \\bigl(Ku^{k+1}+b^{k}\\bigr)\\right\\|_{2}^{2} \\right\\}。\n$$\n\n从近端算子的定义和凸函数的基本次微分最优性条件出发，推导近端算子 $\\mathrm{prox}_{\\tau g}(z)$ (对于 $z\\in\\mathbb{R}^{n}$ 和 $\\tau0$) 的闭式表达式，该表达式需用按分量定义的软阈值算子 $\\mathrm{soft}(z,\\kappa)$ 来表示，其中 $\\bigl[\\mathrm{soft}(z,\\kappa)\\bigr]_{i}=\\mathrm{sign}(z_{i})\\max\\{|z_{i}|-\\kappa,0\\}$。然后，解释该表达式如何修正 SB 迭代中的 $d$-更新步骤，并用 $\\alpha$、$\\beta$ 和 $\\mu$ 明确指出有效的收缩水平和缩放因子。\n\n你的最终答案必须是使用软阈值表示法给出的 $\\mathrm{prox}_{\\tau g}(z)$ 的单个闭式解析表达式。不需要进行数值近似。",
            "solution": "该问题要求两个主要结果：首先，推导弹性网络正则化项的近端算子 $\\mathrm{prox}_{\\tau g}(z)$ 的闭式表达式；其次，解释这如何应用于分裂 Bregman 迭代中的 $d$-更新步骤。\n\n**第 1 部分：$\\mathrm{prox}_{\\tau g}(z)$ 的推导**\n\n根据定义，$g$ 的近端算子由下式给出：\n$$\n\\mathrm{prox}_{\\tau g}(z) = \\arg\\min_{x\\in\\mathbb{R}^{n}}\\left\\{\\frac{1}{2}\\|x-z\\|_{2}^{2}+\\tau g(x)\\right\\}\n$$\n我们将弹性网络正则化项 $g(x) = \\alpha\\|x\\|_{1}+\\frac{\\beta}{2}\\|x\\|_{2}^{2}$ 的表达式代入目标函数。我们将目标函数称为 $F(x)$。\n$$\nF(x) = \\frac{1}{2}\\|x-z\\|_{2}^{2} + \\tau \\left(\\alpha\\|x\\|_{1} + \\frac{\\beta}{2}\\|x\\|_{2}^{2}\\right)\n$$\n我们想要找到 $x^* = \\arg\\min_{x} F(x)$。让我们重新排列 $F(x)$ 中的各项：\n$$\nF(x) = \\tau\\alpha\\|x\\|_{1} + \\frac{1}{2}\\|x-z\\|_{2}^{2} + \\frac{\\tau\\beta}{2}\\|x\\|_{2}^{2}\n$$\n展开二次项 $\\|x-z\\|_{2}^{2} = x^T x - 2x^T z + z^T z = \\|x\\|_2^2 - 2x^T z + \\|z\\|_2^2$，我们得到：\n$$\nF(x) = \\tau\\alpha\\|x\\|_{1} + \\frac{1}{2}(\\|x\\|_{2}^{2} - 2x^T z + \\|z\\|_{2}^{2}) + \\frac{\\tau\\beta}{2}\\|x\\|_{2}^{2}\n$$\n合并与 $x$ 相关的项：\n$$\nF(x) = \\tau\\alpha\\|x\\|_{1} + \\frac{1+\\tau\\beta}{2}\\|x\\|_{2}^{2} - x^T z + \\frac{1}{2}\\|z\\|_{2}^{2}\n$$\n由于项 $\\frac{1}{2}\\|z\\|_{2}^{2}$ 关于 $x$ 是常数，我们可以在最小化过程中忽略它。问题变为：\n$$\nx^* = \\arg\\min_{x} \\left\\{ \\tau\\alpha\\|x\\|_{1} + \\frac{1+\\tau\\beta}{2}\\|x\\|_{2}^{2} - x^T z \\right\\}\n$$\n我们可以对包含 $x$ 的二次项进行配方：\n$$\n\\frac{1+\\tau\\beta}{2}\\|x\\|_{2}^{2} - x^T z = \\frac{1+\\tau\\beta}{2} \\left( \\|x\\|_{2}^{2} - \\frac{2}{1+\\tau\\beta}x^T z \\right) = \\frac{1+\\tau\\beta}{2} \\left\\| x - \\frac{1}{1+\\tau\\beta}z \\right\\|_{2}^{2} - \\frac{1}{2(1+\\tau\\beta)}\\|z\\|_{2}^{2}\n$$\n将此代回并再次舍去关于 $x$ 的常数项，最小化等价于：\n$$\nx^* = \\arg\\min_{x} \\left\\{ \\tau\\alpha\\|x\\|_{1} + \\frac{1+\\tau\\beta}{2} \\left\\| x - \\frac{1}{1+\\tau\\beta}z \\right\\|_{2}^{2} \\right\\}\n$$\n我们可以将目标函数除以正常数 $(1+\\tau\\beta)$ 而不改变最小值点：\n$$\nx^* = \\arg\\min_{x} \\left\\{ \\frac{\\tau\\alpha}{1+\\tau\\beta}\\|x\\|_{1} + \\frac{1}{2} \\left\\| x - \\frac{1}{1+\\tau\\beta}z \\right\\|_{2}^{2} \\right\\}\n$$\n这就是 $\\ell_1$-范数近端算子的定义，$\\mathrm{prox}_{\\lambda \\|\\cdot\\|_1}(y) = \\mathrm{soft}(y,\\lambda)$，其参数为 $\\lambda = \\frac{\\tau\\alpha}{1+\\tau\\beta}$ 和 $y = \\frac{1}{1+\\tau\\beta}z$。\n因此，解为：\n$$\nx^* = \\mathrm{soft}\\left(\\frac{1}{1+\\tau\\beta}z, \\frac{\\tau\\alpha}{1+\\tau\\beta}\\right)\n$$\n让我们按分量分析。由于 $1+\\tau\\beta  0$，$\\frac{z_i}{1+\\tau\\beta}$ 的符号与 $z_i$ 的符号相同。\n\\begin{align*}\nx_i^* = \\mathrm{sign}\\left(\\frac{z_i}{1+\\tau\\beta}\\right) \\max\\left\\{\\left|\\frac{z_i}{1+\\tau\\beta}\\right| - \\frac{\\tau\\alpha}{1+\\tau\\beta}, 0\\right\\} \\\\\n= \\mathrm{sign}(z_i) \\frac{1}{1+\\tau\\beta} \\max\\left\\{|z_i| - \\tau\\alpha, 0\\right\\} \\\\\n= \\frac{1}{1+\\tau\\beta} \\left( \\mathrm{sign}(z_i) \\max\\left\\{|z_i| - \\tau\\alpha, 0\\right\\} \\right) \\\\\n= \\frac{1}{1+\\tau\\beta} \\bigl[\\mathrm{soft}(z, \\tau\\alpha)\\bigr]_i\n\\end{align*}\n因此，近端算子的闭式表达式为：\n$$\n\\mathrm{prox}_{\\tau g}(z) = \\frac{1}{1+\\tau\\beta} \\mathrm{soft}(z, \\tau\\alpha)\n$$\n\n**第 2 部分：应用于分裂 Bregman 的 $d$-更新**\n\n分裂 Bregman 迭代中的 $d$-更新由下式给出：\n$$\nd^{k+1}=\\arg\\min_{d\\in\\mathbb{R}^{m}} \\left\\{ g(d) + \\frac{\\mu}{2}\\left\\|d - \\bigl(Ku^{k+1}+b^{k}\\bigr)\\right\\|_{2}^{2} \\right\\}\n$$\n为了将其与近端算子的定义联系起来，我们可以将目标函数乘以 $1/\\mu$，这不会改变最小值点：\n$$\nd^{k+1}=\\arg\\min_{d\\in\\mathbb{R}^{m}} \\left\\{ \\frac{1}{\\mu}g(d) + \\frac{1}{2}\\left\\|d - \\bigl(Ku^{k+1}+b^{k}\\bigr)\\right\\|_{2}^{2} \\right\\}\n$$\n该表达式与近端算子 $\\mathrm{prox}_{\\tau h}(z)$ 的定义完全匹配，对应关系如下：\n-   函数为 $h=g$。\n-   参数为 $\\tau = \\frac{1}{\\mu}$。\n-   输入向量为 $z = Ku^{k+1}+b^{k}$。\n\n因此，$d$-更新可以简洁地写成一个近端步骤：\n$$\nd^{k+1} = \\mathrm{prox}_{\\frac{1}{\\mu}g}\\left(Ku^{k+1}+b^{k}\\right)\n$$\n现在，我们可以使用第一部分推导出的 $\\mathrm{prox}_{\\tau g}(z)$ 的闭式表达式，通过代入 $\\tau = 1/\\mu$ 和 $z = Ku^{k+1}+b^{k}$：\n$$\nd^{k+1} = \\frac{1}{1+\\left(\\frac{1}{\\mu}\\right)\\beta} \\mathrm{soft}\\left(Ku^{k+1}+b^{k}, \\frac{1}{\\mu}\\alpha\\right)\n$$\n化简缩放因子：\n$$\nd^{k+1} = \\frac{1}{ \\frac{\\mu+\\beta}{\\mu} } \\mathrm{soft}\\left(Ku^{k+1}+b^{k}, \\frac{\\alpha}{\\mu}\\right) = \\frac{\\mu}{\\mu+\\beta} \\mathrm{soft}\\left(Ku^{k+1}+b^{k}, \\frac{\\alpha}{\\mu}\\right)\n$$\n该表达式修正了用于 $\\ell_1$ 正则化（其中 $\\beta=0$）方法中的标准软阈值更新。弹性网络的 $\\ell_2^2$ 部分引入了一个缩放因子。\n-   **有效收缩水平**（或阈值）是软阈值算子的第二个参数，即 $\\kappa = \\frac{\\alpha}{\\mu}$。\n-   **缩放因子**是阈值处理后应用的乘法项，即 $\\frac{\\mu}{\\mu+\\beta}$。\n\n总之，弹性网络模型的分裂 Bregman 方法中的 $d$-子问题通过对向量 $Ku^{k+1}+b^{k}$ 应用软阈值来求解，其阈值与 $\\alpha$ 成正比，与 $\\mu$ 成反比，然后将结果乘以一个取决于 $\\mu$ 与 $\\beta$ 之比的因子。注意，如果 $\\beta=0$（纯 $\\ell_1$ 情况），缩放因子变为 $1$，我们就恢复到标准的软阈值更新。",
            "answer": "$$\\boxed{\\frac{1}{1+\\tau\\beta}\\mathrm{soft}(z, \\tau\\alpha)}$$"
        }
    ]
}