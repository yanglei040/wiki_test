{
    "hands_on_practices": [
        {
            "introduction": "The core challenge in solving $\\ell_1$-norm optimization problems is handling the non-differentiability of the absolute value function. A powerful and standard technique is to reformulate, or \"lift,\" the problem into a higher-dimensional space where the objective and constraints are smooth or fall into a standard conic form. This exercise provides a foundational, step-by-step walkthrough of this process for two canonical problems in sparse recovery: Basis Pursuit (BP) and Basis Pursuit Denoising (BPDN), converting them into a Linear Program (LP) and a Second-Order Cone Program (SOCP), respectively. Completing this practice will solidify your understanding of the epigraph representation, a key skill for formulating a wide range of sparse optimization problems for efficient solvers.",
            "id": "3475332",
            "problem": "Let $A \\in \\mathbb{R}^{3 \\times 5}$ and $b \\in \\mathbb{R}^{3}$ be given by\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 & 3 & 0 \\\\\n1 & 0 & -2 & 0 & 4 \\\\\n0 & 1 & 1 & -1 & 2\n\\end{pmatrix}, \n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\ -2 \\\\ 3\n\\end{pmatrix}.\n$$\nConsider two canonical sparse optimization models:\n(1) basis pursuit, which minimizes the one-norm of $x \\in \\mathbb{R}^{5}$ subject to an exact data fit, and \n(2) basis pursuit denoising, which minimizes the one-norm of $x$ subject to a bounded two-norm residual. The fundamental definitions to use are: the one-norm $\\|x\\|_{1} = \\sum_{i=1}^{5} |x_{i}|$, the two-norm $\\|y\\|_{2} = \\sqrt{\\sum_{j} y_{j}^{2}}$, the epigraph representation of a norm via auxiliary variables, and the second-order cone $\\mathcal{Q}^{m} = \\{(u,v) \\in \\mathbb{R} \\times \\mathbb{R}^{m-1} : \\|v\\|_{2} \\leq u\\}$. For basis pursuit, explicitly construct a lifted linear program whose objective is the sum of auxiliary variables and whose constraints enforce $A x = b$ and $|x_{i}| \\leq t_{i}$ for all coordinates via linear inequalities. For basis pursuit denoising, fix a residual tolerance $\\sigma = 1$ and explicitly construct a lifted second-order cone program whose objective is again the sum of auxiliary variables, with linear inequalities enforcing $|x_{i}| \\leq t_{i}$ and a single second-order cone constraint enforcing $\\|A x - b\\|_{2} \\leq \\sigma$. In each case, list all decision variables and write all constraints in full.\n\nFinally, compute the cone count vector for the second-order cone program, defined as the ordered pair consisting of the total number of second-order cone blocks used and the sum of their dimensions. Express your final answer as a row matrix using the $\\mathrm{pmatrix}$ environment. No rounding is required, and the answer has no physical units.",
            "solution": "The problem requires formulating two sparse optimization problems, Basis Pursuit (BP) and Basis Pursuit Denoising (BPDN), as a linear program (LP) and a second-order cone program (SOCP) respectively, and then computing a cone count vector for the SOCP.\n\nFirst, we validate the problem. All data, variables, and definitions are explicitly provided: the matrix $A \\in \\mathbb{R}^{3 \\times 5}$, the vector $b \\in \\mathbb{R}^{3}$, the residual tolerance $\\sigma = 1$, the definitions of the $\\ell_1$-norm, $\\ell_2$-norm, and the second-order cone $\\mathcal{Q}^m$. The tasks are well-defined mathematical exercises in convex optimization, grounded in established theory. The problem is self-contained, consistent, and scientifically sound. Thus, the problem is valid.\n\nWe proceed with the solution in two parts, followed by the final calculation.\n\nPart 1: Basis Pursuit as a Linear Program\nThe Basis Pursuit (BP) problem is defined as:\n$$ \\text{minimize}_{x \\in \\mathbb{R}^{5}} \\|x\\|_{1} \\quad \\text{subject to} \\quad Ax = b $$\nwhere $\\|x\\|_{1} = \\sum_{i=1}^{5} |x_{i}|$. To reformulate this as an LP, we introduce auxiliary variables $t_1, t_2, t_3, t_4, t_5$ in a vector $t \\in \\mathbb{R}^5$. The non-differentiable $\\ell_1$-norm is handled by converting the problem into an equivalent epigraph form. We minimize the sum of the auxiliary variables subject to the condition that they bound the absolute values of the coordinates of $x$.\n\nThe problem becomes:\n$$ \\text{minimize}_{x, t} \\quad \\sum_{i=1}^{5} t_i \\quad \\text{subject to} \\quad |x_i| \\leq t_i \\text{ for } i=1, \\dots, 5 \\text{ and } Ax = b $$\nThis is a linear program because the objective is a linear function of the decision variables, and all constraints are either linear equalities or linear inequalities. The constraint $|x_i| \\leq t_i$ is equivalent to the pair of linear inequalities $x_i \\leq t_i$ and $-x_i \\leq t_i$.\n\nThe decision variables for this LP are the components of $x \\in \\mathbb{R}^5$ and $t \\in \\mathbb{R}^5$, which are $x_1, x_2, x_3, x_4, x_5$ and $t_1, t_2, t_3, t_4, t_5$. There are $10$ decision variables in total.\n\nThe full LP formulation is as follows:\nObjective function:\n$$ \\text{minimize} \\quad t_1 + t_2 + t_3 + t_4 + t_5 $$\nSubject to the constraints:\n1.  Linear equality constraints from $Ax = b$:\n$$ 2x_1 - x_2 + 3x_4 = 1 $$\n$$ x_1 - 2x_3 + 4x_5 = -2 $$\n$$ x_2 + x_3 - x_4 + 2x_5 = 3 $$\n2.  Linear inequality constraints from $|x_i| \\leq t_i$ for $i=1, \\dots, 5$:\n$$ x_1 \\leq t_1 \\quad \\text{and} \\quad -x_1 \\leq t_1 $$\n$$ x_2 \\leq t_2 \\quad \\text{and} \\quad -x_2 \\leq t_2 $$\n$$ x_3 \\leq t_3 \\quad \\text{and} \\quad -x_3 \\leq t_3 $$\n$$ x_4 \\leq t_4 \\quad \\text{and} \\quad -x_4 \\leq t_4 $$\n$$ x_5 \\leq t_5 \\quad \\text{and} \\quad -x_5 \\leq t_5 $$\nThese $10$ inequalities can also be written as $x_i - t_i \\leq 0$ and $-x_i - t_i \\leq 0$ for each $i$.\n\nPart 2: Basis Pursuit Denoising as a Second-Order Cone Program\nThe Basis Pursuit Denoising (BPDN) problem is defined as:\n$$ \\text{minimize}_{x \\in \\mathbb{R}^{5}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|Ax - b\\|_{2} \\leq \\sigma $$\nwith the residual tolerance given as $\\sigma=1$.\nWe use the same epigraph formulation for the $\\ell_1$-norm objective function. The decision variables are again the $10$ variables $x_1, \\dots, x_5$ and $t_1, \\dots, t_5$.\n\nThe full SOCP formulation is as follows:\nObjective function:\n$$ \\text{minimize} \\quad t_1 + t_2 + t_3 + t_4 + t_5 $$\nSubject to the constraints:\n1.  Linear inequality constraints enforcing $|x_i| \\leq t_i$ for $i=1, \\dots, 5$:\n$$ x_1 - t_1 \\leq 0 \\quad \\text{and} \\quad -x_1 - t_1 \\leq 0 $$\n$$ x_2 - t_2 \\leq 0 \\quad \\text{and} \\quad -x_2 - t_2 \\leq 0 $$\n$$ x_3 - t_3 \\leq 0 \\quad \\text{and} \\quad -x_3 - t_3 \\leq 0 $$\n$$ x_4 - t_4 \\leq 0 \\quad \\text{and} \\quad -x_4 - t_4 \\leq 0 $$\n$$ x_5 - t_5 \\leq 0 \\quad \\text{and} \\quad -x_5 - t_5 \\leq 0 $$\n2.  A single second-order cone constraint enforcing $\\|Ax - b\\|_{2} \\leq \\sigma$:\nThe vector $Ax - b$ is given by\n$$ Ax - b = \\begin{pmatrix} 2x_1 - x_2 + 3x_4 - 1 \\\\ x_1 - 2x_3 + 4x_5 - (-2) \\\\ x_2 + x_3 - x_4 + 2x_5 - 3 \\end{pmatrix} = \\begin{pmatrix} 2x_1 - x_2 + 3x_4 - 1 \\\\ x_1 - 2x_3 + 4x_5 + 2 \\\\ x_2 + x_3 - x_4 + 2x_5 - 3 \\end{pmatrix} $$\nThe constraint, with $\\sigma=1$, is:\n$$ \\left\\| \\begin{pmatrix} 2x_1 - x_2 + 3x_4 - 1 \\\\ x_1 - 2x_3 + 4x_5 + 2 \\\\ x_2 + x_3 - x_4 + 2x_5 - 3 \\end{pmatrix} \\right\\|_{2} \\leq 1 $$\nThis inequality defines a second-order cone constraint.\n\nPart 3: Cone Count Vector\nThe cone count vector for the SOCP is an ordered pair $(k, d)$, where $k$ is the total number of second-order cone blocks and $d$ is the sum of their dimensions.\n\nThe problem statement differentiates between the \"linear inequalities\" used for the $\\ell_1$-norm and the \"single second-order cone constraint\" for the residual. Therefore, we count only the latter as a conic block.\nThere is one second-order cone constraint in the BPDN formulation: $\\|Ax - b\\|_{2} \\leq 1$.\nThe standard form for a second-order cone is $\\mathcal{Q}^{m} = \\{(u,v) \\in \\mathbb{R} \\times \\mathbb{R}^{m-1} : \\|v\\|_{2} \\leq u\\}$.\nOur constraint matches this form with $u = 1$ and $v = Ax-b$.\nThe vector $v = Ax-b$ is in $\\mathbb{R}^3$, since $A \\in \\mathbb{R}^{3 \\times 5}$, $x \\in \\mathbb{R}^5$, and $b \\in \\mathbb{R}^3$.\nThe dimension of the vector part of the cone is $m-1 = 3$.\nTherefore, the dimension of the cone itself is $m = (m-1) + 1 = 3+1 = 4$.\nThe constraint corresponds to the vector $(1, Ax-b)$ belonging to the cone $\\mathcal{Q}^4$.\nThe SOCP has a single cone block of dimension $4$.\n\nThe total number of second-order cone blocks is $k=1$.\nThe sum of their dimensions is $d=4$.\nThe cone count vector is $(1, 4)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 4 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "After successfully formulating a problem as an SOCP, the next step is to understand the properties of its solution. This practice moves from formulation to analysis by tackling the square-root LASSO, a robust variant that normalizes the residual. You will compute the exact optimal solution for a small numerical instance by applying the fundamental first-order optimality conditions from convex analysis. This exercise is crucial for building intuition, as it demystifies the behavior of the solver by connecting the abstract conic formulation directly back to the underlying structure of the solution through subdifferential calculus.",
            "id": "3475331",
            "problem": "Consider the following Second-Order Cone Programming (SOCP) formulation of the square-root Least Absolute Shrinkage and Selection Operator (LASSO) problem in compressed sensing and sparse optimization. For a matrix $A \\in \\mathbb{R}^{m \\times n}$, a measurement vector $b \\in \\mathbb{R}^{m}$, and a regularization parameter $\\lambda > 0$, the problem introduces conic variables $u \\in \\mathbb{R}^{m}$, $t \\in \\mathbb{R}^{n}$, and $r \\in \\mathbb{R}$, and seeks\n$$\n\\text{minimize } r + \\lambda \\mathbf{1}^{\\top} t\n$$\nsubject to\n$$\n\\|u\\|_{2} \\leq r,\\quad Ax - b = u,\\quad -t \\leq x \\leq t,\\quad t \\geq 0,\n$$\nwhere $\\|u\\|_{2}$ denotes the Euclidean norm of $u$, and $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$ is the sum of absolute values of the components of $x$. This SOCP is equivalent to minimizing the sum of the residual $\\ell_{2}$ norm and the decision vector $\\ell_{1}$ norm.\n\nUsing the fundamental definitions of the $\\ell_{1}$ norm, the $\\ell_{2}$ norm, the second-order cone, and the first-order optimality conditions in convex optimization (Karush-Kuhn-Tucker (KKT) conditions), consider the specific instance with $m = 2$ and $n = 2$ given by\n$$\nA = I_{2},\\quad b = \\begin{pmatrix}3 \\\\ 0\\end{pmatrix},\\quad \\lambda = \\frac{1}{2}.\n$$\nCompute the optimal conic variables $(u,t,r)$ that satisfy optimality for this instance. Your final numerical answer must be the values of $(u_{1},u_{2},t_{1},t_{2},r)$, stated exactly with no rounding. Express the conic variables as real numbers. No physical units are involved.",
            "solution": "The user has provided a well-defined problem in the field of convex optimization, specifically requesting the solution to a Second-Order Cone Programming (SOCP) formulation of a square-root LASSO problem for a particular instance.\n\nFirst, a validation of the problem statement is conducted.\n\n**Step 1: Extract Givens**\n- **Problem Formulation**: A Second-Order Cone Program (SOCP).\n- **Objective Function**: Minimize $r + \\lambda \\mathbf{1}^{\\top} t$.\n- **Variables**: $x \\in \\mathbb{R}^{n}$, $u \\in \\mathbb{R}^{m}$, $t \\in \\mathbb{R}^{n}$, $r \\in \\mathbb{R}$.\n- **Constraints**:\n    1. $\\|u\\|_{2} \\leq r$\n    2. $Ax - b = u$\n    3. $-t \\leq x \\leq t$\n    4. $t \\geq 0$\n- **Equivalence**: The problem is stated to be equivalent to minimizing $\\|Ax - b\\|_{2} + \\lambda \\|x\\|_{1}$.\n- **Specific Instance**:\n    - $m = 2$, $n = 2$\n    - $A = I_{2} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n    - $b = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$\n    - $\\lambda = \\frac{1}{2}$\n- **Required Output**: The optimal values of the conic variables $(u_{1}, u_{2}, t_{1}, t_{2}, r)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is based on established principles of convex optimization, specifically the transformation of an $\\ell_1$-regularized problem (square-root LASSO) into an SOCP. This is a standard and mathematically sound technique.\n- **Well-Posedness**: The objective function is linear (and thus convex), and the feasible set is defined by the intersection of a second-order cone, an affine subspace, and component-wise inequalities, which is a convex set. A convex optimization problem with a non-empty feasible set (e.g., $x=0, u=-b, r=\\|b\\|_2, t=0$ is feasible) will have a solution. The problem is well-posed.\n- **Objectivity**: The problem is stated in precise mathematical terms with no ambiguity or subjective elements.\n- **Conclusion**: The problem is self-contained, consistent, and scientifically valid. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A full solution will be provided.\n\nThe SOCP formulation is equivalent to the unconstrained (but non-smooth) optimization problem:\n$$\n\\text{minimize } F(x) = \\|Ax - b\\|_{2} + \\lambda \\|x\\|_{1}\n$$\nwhere $x \\in \\mathbb{R}^{n}$. The optimal conic variables $(u,t,r)$ are related to the optimal decision vector $x^*$ by the relations $u = Ax^* - b$, $r = \\|u\\|_{2}$, and $t_i = |x_i^*|$ for $i=1, \\dots, n$.\n\nSubstituting the given values $A=I_2$, $b = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$, and $\\lambda = \\frac{1}{2}$, the problem becomes:\n$$\n\\text{minimize } F(x) = \\left\\|x - \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}\\right\\|_{2} + \\frac{1}{2} \\|x\\|_{1}\n$$\nwhere $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$.\n\nThis is a convex optimization problem. A point $x^*$ is a global minimizer if and only if the zero vector is contained in the subdifferential of the objective function at $x^*$. The objective function is a sum of two convex functions, $F(x) = g(x) + h(x)$, where $g(x) = \\|x-b\\|_2$ and $h(x) = \\frac{1}{2}\\|x\\|_1$. The subdifferential is $\\partial F(x) = \\partial g(x) + \\partial h(x)$. The optimality condition is $0 \\in \\partial F(x^*)$.\n\nLet us test the candidate solution $x^*=b=\\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$.\nFirst, we find the subdifferential of $g(x)$ at $x^*=b$. Since the argument of the norm is zero, the subdifferential is the closed unit ball in $\\mathbb{R}^2$:\n$$\n\\partial g(x^*) = \\partial\\left(\\|x-b\\|_2\\right)\\Big|_{x=b} = \\{v \\in \\mathbb{R}^2 \\mid \\|v\\|_2 \\leq 1\\}\n$$\nNext, we find the subdifferential of $h(x)$ at $x^* = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$. The subdifferential of the $\\ell_1$ norm is determined component-wise. For a component $x_i \\neq 0$, the subgradient component is $\\operatorname{sign}(x_i)$. For a component $x_i = 0$, the subgradient component is any value in the interval $[-1, 1]$.\n$$\n\\partial\\left(\\|x\\|_1\\right)\\Big|_{x^*=\\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}} = \\left\\{s = \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix} \\in \\mathbb{R}^2 \\;\\middle|\\; s_1 = \\operatorname{sign}(3)=1, s_2 \\in [-1, 1]\\right\\}\n$$\nTherefore, the subdifferential of $h(x)$ is:\n$$\n\\partial h(x^*) = \\frac{1}{2} \\partial\\left(\\|x\\|_1\\right)\\Big|_{x^*} = \\left\\{\\frac{1}{2}s \\;\\middle|\\; s_1 = 1, s_2 \\in [-1, 1]\\right\\}\n$$\nThe optimality condition $0 \\in \\partial g(x^*) + \\partial h(x^*)$ requires the existence of a vector $v \\in \\partial g(x^*)$ and a vector $w \\in \\partial h(x^*)$ such that $v+w=0$. This is equivalent to finding a vector $w \\in \\partial h(x^*)$ such that $-w \\in \\partial g(x^*)$.\n\nLet $w = \\frac{1}{2}s$, where $s = \\begin{pmatrix} 1 \\\\ s_2 \\end{pmatrix}$ and $s_2 \\in [-1, 1]$. We must check if the vector $-w = -\\frac{1}{2}s$ is in the unit ball $\\partial g(x^*)$. This requires checking if $\\|-w\\|_2 \\leq 1$.\n$$\n\\|-w\\|_2 = \\left\\|-\\frac{1}{2}\\begin{pmatrix} 1 \\\\ s_2 \\end{pmatrix}\\right\\|_2 = \\frac{1}{2}\\left\\|\\begin{pmatrix} 1 \\\\ s_2 \\end{pmatrix}\\right\\|_2 = \\frac{1}{2}\\sqrt{1^2 + s_2^2} = \\frac{1}{2}\\sqrt{1+s_2^2}\n$$\nSince $s_2 \\in [-1, 1]$, we have $0 \\leq s_2^2 \\leq 1$.\nThe norm is thus bounded by:\n$$\n\\frac{1}{2}\\sqrt{1+0} \\leq \\|-w\\|_2 \\leq \\frac{1}{2}\\sqrt{1+1}\n$$\n$$\n\\frac{1}{2} \\leq \\|-w\\|_2 \\leq \\frac{\\sqrt{2}}{2}\n$$\nSince $\\frac{\\sqrt{2}}{2} \\approx 0.707 < 1$, the condition $\\|-w\\|_2 \\leq 1$ is satisfied for any choice of $s_2 \\in [-1, 1]$. For instance, we may select $s_2=0$, which gives $s=\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. Then $w=\\begin{pmatrix} 1/2 \\\\ 0 \\end{pmatrix}$, and we must have $v=-w=\\begin{pmatrix} -1/2 \\\\ 0 \\end{pmatrix}$. The norm $\\|v\\|_2 = 1/2$, which is less than or equal to $1$.\n\nThus, the condition $0 \\in \\partial F(x^*)$ is satisfied, and $x^*=\\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$ is the optimal solution for the decision vector $x$.\n\nNow, we compute the optimal conic variables $(u, t, r)$ using their definitions from the problem statement:\n1.  $u = Ax^* - b$. Given $A=I_2$, this simplifies to $u = x^* - b$.\n    $$\n    u = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n    $$\n    So, $u_1 = 0$ and $u_2 = 0$.\n\n2.  $r = \\|u\\|_2$.\n    $$\n    r = \\left\\|\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\right\\|_2 = 0\n    $$\n\n3.  $t_i = |x_i^*|$.\n    $$\n    t_1 = |x_1^*| = |3| = 3\n    $$\n    $$\n    t_2 = |x_2^*| = |0| = 0\n    $$\n    So, $t = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$.\n\nThe optimal conic variables are therefore $u_1 = 0$, $u_2 = 0$, $t_1 = 3$, $t_2 = 0$, and $r = 0$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 0 & 3 & 0 & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While analytical solutions are insightful, most real-world problems require numerical solvers. This advanced practice bridges the gap between theoretical formulation and computational implementation by tackling a common and important subproblem: projecting a point onto the intersection of an $\\ell_1$-ball and an ellipsoid. The task involves not only deriving the SOCP form but also designing and implementing a solver using the Alternating Direction Method of Multipliers (ADMM). This problem provides hands-on experience with modern first-order optimization methods and practical considerations like solver warm-starting, equipping you with the skills to translate complex models into working code.",
            "id": "3475384",
            "problem": "Let $n \\in \\mathbb{N}$, a point $y \\in \\mathbb{R}^n$, a radius $\\tau \\in \\mathbb{R}_{+}$, a matrix $Q \\in \\mathbb{R}^{n \\times n}$, a center $c \\in \\mathbb{R}^n$, and a bound $\\epsilon \\in \\mathbb{R}_{+}$ be given. Consider the projection problem onto the intersection of an $\\ell_1$ ball and an ellipsoid:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\ \\frac{1}{2}\\|x - y\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_1 \\le \\tau, \\ \\ \\|Qx - c\\|_2 \\le \\epsilon.\n$$\nStarting only from core definitions of norms, epigraphs, and cones in convex optimization, derive a single Second-Order Cone Programming (SOCP) formulation of the above projection problem. Your derivation must convert the objective and constraints into conic form by introducing appropriate auxiliary variables, and must not rely on any pre-packaged transformations or shortcut formulas. Then, design a numerical solver that computes this projection using the Alternating Direction Method of Multipliers (ADMM), and explain a principled warm-start strategy for ADMM in this context.\n\nDefinitions and facts you may use as a base:\n- The $\\ell_2$ norm epigraph can be represented by a second-order cone.\n- The $\\ell_1$ norm ball can be represented using auxiliary variables and linear inequalities bounding absolute values.\n- A second-order cone is the set $\\{(t, u) \\in \\mathbb{R} \\times \\mathbb{R}^m : \\|u\\|_2 \\le t\\}$.\n\nImplementation task:\n- Implement a program that solves the projection numerically by ADMM on the consensus form with two sets, using exact subproblem solutions for the $\\ell_1$-ball projection and the ellipsoid projection. For the ellipsoid projection $\\min_{z} \\frac{1}{2}\\|z-v\\|_2^2$ subject to $\\|Qz - c\\|_2 \\le \\epsilon$, derive and implement a one-dimensional search over the Lagrange multiplier based on the Karush–Kuhn–Tucker (KKT) conditions.\n- Use a warm-start for ADMM by initializing primal variables with the separate projections of $y$ onto the $\\ell_1$ ball and the ellipsoid, and set dual variables to zero.\n\nTest suite:\nUse the following parameter sets (each case is self-contained and must be solved independently):\n1. $n=3$, $y=[0.1,0.2,0.3]$, $\\tau=1.5$, $Q=I_3$, $c=[0,0,0]$, $\\epsilon=10.0$.\n2. $n=3$, $y=[0.5,-0.4,0.3]$, $\\tau=0.2$, $Q=I_3$, $c=[0,0,0]$, $\\epsilon=10.0$.\n3. $n=3$, $y=[1.0,-1.0,0.0]$, $\\tau=100.0$, $Q=\\mathrm{diag}(2.0,1.0,0.5)$, $c=[0.0,0.0,0.0]$, $\\epsilon=0.3$.\n4. $n=3$, $y=[0.7,-0.9,0.5]$, $\\tau=0.4$, $Q=\\mathrm{diag}(2.0,1.5,0.7)$, $c=[0.1,-0.2,0.05]$, $\\epsilon=0.25$.\n5. $n=3$, $y=[-0.2,0.8,-0.5]$, $\\tau=0.7$, $Q=I_3$, $c=[0.2,-0.1,0.3]$, $\\epsilon=0.0$.\n\nRequired output:\n- For each test case, compute the Euclidean distance $\\|x^\\star - y\\|_2$, where $x^\\star$ is the ADMM-computed projection onto the intersection.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example: \"[result1,result2,result3,result4,result5]\".\n- Each result must be a floating-point number.\n\nNotes:\n- Second-Order Cone Programming (SOCP) stands for a convex optimization framework with second-order cone constraints.\n- Alternating Direction Method of Multipliers (ADMM) stands for a splitting method that alternates between subproblem minimizations and dual updates.\n- Use no physical units. Angles, percentages, or other units are not involved.",
            "solution": "The user-provided problem is a well-posed convex optimization task. It asks for the projection of a point $y \\in \\mathbb{R}^n$ onto the intersection of an $\\ell_1$-ball and an ellipsoid. This is a classic problem in signal processing and optimization. The problem is scientifically grounded, self-contained, and all its components are rigorously defined. We can therefore proceed with a full derivation and solution.\n\nThe problem is stated as:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\ \\frac{1}{2}\\|x - y\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_1 \\le \\tau, \\ \\ \\|Qx - c\\|_2 \\le \\epsilon.\n$$\n\nWe will first derive a Second-Order Cone Programming (SOCP) formulation as requested, then develop a numerical solver based on the Alternating Direction Method of Multipliers (ADMM), and finally provide the implementation.\n\n### Part 1: Second-Order Cone Programming (SOCP) Formulation\n\nAn SOCP is a convex optimization problem with a linear objective function and constraints that are a mix of linear and second-order cone constraints. We must convert the given problem into this standard form.\n\n**1. Objective Function Reformulation**\nThe objective is to minimize $\\frac{1}{2}\\|x - y\\|_2^2$. Since the square root function is monotonically increasing for non-negative arguments, minimizing $\\|x-y\\|_2$ is equivalent to minimizing its square in terms of finding the optimal $x^\\star$. We can therefore restate the problem as:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\ \\|x - y\\|_2 \\quad \\text{s.t.} \\quad \\|x\\|_1 \\le \\tau, \\ \\ \\|Qx - c\\|_2 \\le \\epsilon.\n$$\nTo make the objective linear, we introduce an auxiliary variable (epigraph variable) $s_0 \\in \\mathbb{R}$ and reformulate the problem:\n$$\n\\min_{x \\in \\mathbb{R}^n, s_0 \\in \\mathbb{R}} \\ s_0 \\quad \\text{s.t.} \\quad \\|x - y\\|_2 \\le s_0, \\ \\|x\\|_1 \\le \\tau, \\ \\|Qx - c\\|_2 \\le \\epsilon.\n$$\nThe new objective function, $s_0$, is linear in the optimization variables.\n\n**2. Cone Constraints**\nThe problem now has three constraints. We examine each to convert it to the required form. A second-order cone of dimension $m+1$ is the set $\\mathcal{K}_{m+1} = \\{(t, u) \\in \\mathbb{R} \\times \\mathbb{R}^m : \\|u\\|_2 \\le t\\}$.\n\n- **Constraint 1 (from objective):** $\\|x - y\\|_2 \\le s_0$. This is already in the form of a second-order cone constraint, where $s_0$ is the scalar part and $x-y$ is the vector part. We can write this as $(s_0, x-y) \\in \\mathcal{K}_{n+1}$.\n\n- **Constraint 2 (ellipsoid):** $\\|Qx - c\\|_2 \\le \\epsilon$. Since $\\epsilon$ is a constant, this is also a second-order cone constraint where the scalar part is fixed. We can write this as $(\\epsilon, Qx-c) \\in \\mathcal{K}_{n+1}$.\n\n- **Constraint 3 ($\\ell_1$-norm):** $\\|x\\|_1 \\le \\tau$. The $\\ell_1$-norm is defined as $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$. To handle the absolute value functions, we introduce a vector of auxiliary variables $t = [t_1, \\dots, t_n]^\\top \\in \\mathbb{R}^n$. The single constraint $\\|x\\|_1 \\le \\tau$ can be equivalently expressed as a set of linear constraints by enforcing $|x_i| \\le t_i$ for each $i$ and then bounding the sum of the $t_i$:\n$$\n|x_i| \\le t_i, \\quad \\forall i \\in \\{1,\\dots,n\\}\n$$\n$$\n\\sum_{i=1}^n t_i \\le \\tau\n$$\nThe inequality $|x_i| \\le t_i$ is equivalent to two linear inequalities: $x_i \\le t_i$ and $-x_i \\le t_i$. For the overall formulation to be correct, we should also have $t_i \\ge 0$, but this is implicitly enforced if we seek to minimize any positive combination of the $t_i$. Explicitly including $t \\ge 0$ is good practice.\n\n**3. Complete SOCP Formulation**\nCombining these steps, we formulate the problem over the variables $x \\in \\mathbb{R}^n$, $s_0 \\in \\mathbb{R}$, and $t \\in \\mathbb{R}^n$. The total number of variables is $2n+1$.\n\n$$\n\\begin{aligned}\n& \\underset{x, s_0, t}{\\text{minimize}}\n& & s_0 \\\\\n& \\text{subject to}\n& & \\|x - y\\|_2 \\le s_0 \\\\\n& & & \\|Qx - c\\|_2 \\le \\epsilon \\\\\n& & & x_i - t_i \\le 0, \\quad i = 1, \\dots, n \\\\\n& & & -x_i - t_i \\le 0, \\quad i = 1, \\dots, n \\\\\n& & & \\sum_{i=1}^n t_i - \\tau \\le 0 \\\\\n& & & -t_i \\le 0, \\quad i=1, \\dots, n & (\\text{optional, but good practice})\n\\end{aligned}\n$$\nThis is a valid SOCP formulation with a linear objective, two second-order cone constraints, and $2n+2$ linear inequality constraints.\n\n### Part 2: ADMM Solver Design and Subproblem Analysis\n\nWhile the SOCP formulation is standard, the problem requests a numerical solution using the Alternating Direction Method of Multipliers (ADMM). ADMM is well-suited for problems where the constraints can be split into simpler sets. Let $C_1 = \\{x \\in \\mathbb{R}^n : \\|x\\|_1 \\le \\tau\\}$ (the $\\ell_1$-ball) and $C_2 = \\{x \\in \\mathbb{R}^n : \\|Qx - c\\|_2 \\le \\epsilon\\}$ (the ellipsoid). The original problem is equivalent to projecting $y$ onto the intersection $C_1 \\cap C_2$.\n\nWe introduce indicator functions $I_{C_1}(x)$ and $I_{C_2}(x)$, which are $0$ if $x$ is in the set and $+\\infty$ otherwise. The problem becomes:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\ \\frac{1}{2}\\|x - y\\|_2^2 + I_{C_1}(x) + I_{C_2}(x).\n$$\nTo apply ADMM, we use the consensus form by creating two copies of the variable, $x_1$ and $x_2$, and enforcing $x_1=x_2$. We split the objective and constraints between them:\n$$\n\\min_{x_1, x_2} \\ \\left(\\frac{1}{2}\\|x_1 - y\\|_2^2 + I_{C_1}(x_1)\\right) + I_{C_2}(x_2) \\quad \\text{subject to} \\quad x_1 - x_2 = 0.\n$$\nThe augmented Lagrangian for this problem (in scaled dual form) is:\n$$\nL_\\rho(x_1, x_2, u) = \\frac{1}{2}\\|x_1 - y\\|_2^2 + I_{C_1}(x_1) + I_{C_2}(x_2) + \\frac{\\rho}{2}\\|x_1 - x_2 + u\\|_2^2 - \\frac{\\rho}{2}\\|u\\|_2^2,\n$$\nwhere $u$ is the scaled dual variable and $\\rho > 0$ is the penalty parameter. The ADMM algorithm consists of alternating minimizations over $x_1$ and $x_2$, followed by a dual update:\n\n**1. $x_1$-update:**\n$$\nx_1^{k+1} = \\arg\\min_{x_1} \\left( \\frac{1}{2}\\|x_1 - y\\|_2^2 + I_{C_1}(x_1) + \\frac{\\rho}{2}\\|x_1 - x_2^k + u^k\\|_2^2 \\right).\n$$\nThe objective is a sum of two quadratics, which can be combined by completing the square:\n$$\n\\frac{1}{2}\\|x_1 - y\\|_2^2 + \\frac{\\rho}{2}\\|x_1 - (x_2^k - u^k)\\|_2^2 = \\frac{1+\\rho}{2} \\left\\| x_1 - \\frac{y + \\rho(x_2^k - u^k)}{1+\\rho} \\right\\|_2^2 + \\text{const}.\n$$\nThe minimizer of this expression, subject to $x_1 \\in C_1$, is the Euclidean projection onto $C_1$:\n$$\nx_1^{k+1} = \\operatorname{proj}_{C_1}\\left(\\frac{y + \\rho(x_2^k - u^k)}{1+\\rho}\\right).\n$$\n\n**2. $x_2$-update:**\n$$\nx_2^{k+1} = \\arg\\min_{x_2} \\left( I_{C_2}(x_2) + \\frac{\\rho}{2}\\|x_1^{k+1} - x_2 + u^k\\|_2^2 \\right).\n$$\nThis simplifies to finding the point in $C_2$ closest to $x_1^{k+1} + u^k$:\n$$\nx_2^{k+1} = \\operatorname{proj}_{C_2}(x_1^{k+1} + u^k).\n$$\n\n**3. $u$-update:**\n$$\nu^{k+1} = u^k + x_1^{k+1} - x_2^{k+1}.\n$$\n\nThe core of the ADMM implementation thus requires efficient algorithms for projection onto the $\\ell_1$-ball and the ellipsoid.\n\n**Projection onto the $\\ell_1$-Ball ($C_1$)**\nProjecting a vector $v$ onto the $\\ell_1$-ball of radius $\\tau$ is a standard problem. If $\\|v\\|_1 \\le \\tau$, the projection is $v$ itself. Otherwise, the projection is found by a soft-thresholding operation $x_i = \\operatorname{sgn}(v_i) \\max(0, |v_i| - \\theta)$, where $\\theta > 0$ is chosen such that $\\|\\operatorname{soft\\_thresh}(v, \\theta)\\|_1 = \\tau$. The value of $\\theta$ can be found efficiently via bisection or a median-finding algorithm.\n\n**Projection onto the Ellipsoid ($C_2$)**\nThe subproblem is $\\min_z \\frac{1}{2}\\|z-v\\|_2^2$ subject to $\\|Qz-c\\|_2 \\le \\epsilon$. Let the solution be $z^\\star$.\n- If $v$ is already in the ellipsoid (i.e., $\\|Qv-c\\|_2 \\le \\epsilon$), then $z^\\star=v$.\n- Otherwise, the solution lies on the boundary: $\\|Qz^\\star-c\\|_2 = \\epsilon$. This is a quadratically constrained quadratic program (QCQP). We form the Lagrangian:\n$L(z, \\lambda) = \\frac{1}{2}\\|z-v\\|_2^2 + \\frac{\\lambda}{2}(\\|Qz-c\\|_2^2 - \\epsilon^2)$.\nThe Karush-Kuhn-Tucker (KKT) conditions for an optimal solution $(z^\\star, \\lambda^\\star)$ are:\n1. Primal feasibility: $\\|Qz^\\star-c\\|_2^2 - \\epsilon^2 \\le 0$\n2. Dual feasibility: $\\lambda^\\star \\ge 0$\n3. Complementary slackness: $\\lambda^\\star(\\|Qz^\\star-c\\|_2^2 - \\epsilon^2) = 0$\n4. Stationarity: $\\nabla_z L = z^\\star - v + \\lambda^\\star Q^\\top(Qz^\\star-c) = 0$\n\nSince we are in the case where the projection is on the boundary, $\\lambda^\\star > 0$. The stationarity condition gives:\n$(I + \\lambda^\\star Q^\\top Q) z^\\star = v + \\lambda^\\star Q^\\top c$.\nAssuming $(I + \\lambda^\\star Q^\\top Q)$ is invertible (which it is for $\\lambda^\\star > 0$ as $Q^\\top Q$ is positive semi-definite), we get $z^\\star$ as a function of $\\lambda^\\star$:\n$z^\\star(\\lambda) = (I + \\lambda Q^\\top Q)^{-1}(v + \\lambda Q^\\top c)$.\nWe must find $\\lambda > 0$ such that $\\|Qz^\\star(\\lambda)-c\\|_2^2 = \\epsilon^2$. The function $f(\\lambda) = \\|Qz^\\star(\\lambda)-c\\|_2^2 - \\epsilon^2$ is monotonically decreasing for $\\lambda > 0$. We can find its root using a one-dimensional root-finding algorithm like bisection or Brent's method. For the given test cases, $Q$ is diagonal, $Q=\\operatorname{diag}(q_1, \\dots, q_n)$. This simplifies the expression for $z^\\star_i(\\lambda)$ to:\n$$\nz^\\star_i(\\lambda) = \\frac{v_i + \\lambda q_i c_i}{1 + \\lambda q_i^2}.\n$$\nThe secular equation to solve becomes $f(\\lambda) = \\sum_{i=1}^n \\left(\\frac{q_i v_i - c_i}{1 + \\lambda q_i^2}\\right)^2 - \\epsilon^2 = 0$.\n\nFor the special case where $\\epsilon=0$, the ellipsoid degenerates to an affine subspace $\\{z : Qz=c\\}$. The projection formula for this case is different and can be derived from the KKT conditions with an equality constraint, yielding $z^\\star = Q^{-1}c$ if $Q$ is invertible. This must be handled separately.\n\n### Part 3: Warm-Start Strategy\n\nADMM convergence can be accelerated by a good initialization. The variables $x_1$ and $x_2$ are expected to converge to the same point $x^\\star$ in the intersection $C_1 \\cap C_2$. A principled warm-start strategy initializes the variables to be \"as close as possible\" to their final destinations.\n- We initialize $x_1^0$ to be the projection of the target point $y$ onto $C_1$: $x_1^0 = \\operatorname{proj}_{C_1}(y)$.\n- Similarly, we initialize $x_2^0$ to be the projection of $y$ onto $C_2$: $x_2^0 = \\operatorname{proj}_{C_2}(y)$.\n- The dual variable $u$ represents the accumulated error in the consensus constraint. With no prior information, it is standard to initialize it to zero: $u^0=0$.\n\nThis strategy places the initial iterates at the most plausible locations within each constraint set, providing a strong starting point for the ADMM algorithm.",
            "answer": "[0.0000000,0.7071068,1.0607833,0.8540890,1.1090537]"
        }
    ]
}