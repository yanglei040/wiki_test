{
    "hands_on_practices": [
        {
            "introduction": "Mastering the translation of $\\ell_1$-norm problems into standard conic forms is a cornerstone of modern optimization. This first exercise provides a concrete, step-by-step guide to this process for two canonical problems: Basis Pursuit (BP) and Basis Pursuit Denoising (BPDN). By explicitly constructing the equivalent Linear Program and Second-Order Cone Program for a small-scale example, you will gain hands-on experience with the epigraph trick, the fundamental technique for handling non-differentiable norms in a way that is tractable for standard solvers .",
            "id": "3475332",
            "problem": "Let $A \\in \\mathbb{R}^{3 \\times 5}$ and $b \\in \\mathbb{R}^{3}$ be given by\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 & 3 & 0 \\\\\n1 & 0 & -2 & 0 & 4 \\\\\n0 & 1 & 1 & -1 & 2\n\\end{pmatrix}, \n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\ -2 \\\\ 3\n\\end{pmatrix}.\n$$\nConsider two canonical sparse optimization models:\n(1) basis pursuit, which minimizes the one-norm of $x \\in \\mathbb{R}^{5}$ subject to an exact data fit, and \n(2) basis pursuit denoising, which minimizes the one-norm of $x$ subject to a bounded two-norm residual. The fundamental definitions to use are: the one-norm $\\|x\\|_{1} = \\sum_{i=1}^{5} |x_{i}|$, the two-norm $\\|y\\|_{2} = \\sqrt{\\sum_{j} y_{j}^{2}}$, the epigraph representation of a norm via auxiliary variables, and the second-order cone $\\mathcal{Q}^{m} = \\{(u,v) \\in \\mathbb{R} \\times \\mathbb{R}^{m-1} : \\|v\\|_{2} \\leq u\\}$. For basis pursuit, explicitly construct a lifted linear program whose objective is the sum of auxiliary variables and whose constraints enforce $A x = b$ and $|x_{i}| \\leq t_{i}$ for all coordinates via linear inequalities. For basis pursuit denoising, fix a residual tolerance $\\sigma = 1$ and explicitly construct a lifted second-order cone program whose objective is again the sum of auxiliary variables, with linear inequalities enforcing $|x_{i}| \\leq t_{i}$ and a single second-order cone constraint enforcing $\\|A x - b\\|_{2} \\leq \\sigma$. In each case, list all decision variables and write all constraints in full.\n\nFinally, compute the cone count vector for the second-order cone program, defined as the ordered pair consisting of the total number of second-order cone blocks used and the sum of their dimensions. Express your final answer as a row matrix using the $\\mathrm{pmatrix}$ environment. No rounding is required, and the answer has no physical units.",
            "solution": "The problem requires formulating two sparse optimization problems, Basis Pursuit (BP) and Basis Pursuit Denoising (BPDN), as a linear program (LP) and a second-order cone program (SOCP) respectively, and then computing a cone count vector for the SOCP.\n\nFirst, we validate the problem. All data, variables, and definitions are explicitly provided: the matrix $A \\in \\mathbb{R}^{3 \\times 5}$, the vector $b \\in \\mathbb{R}^{3}$, the residual tolerance $\\sigma = 1$, the definitions of the $\\ell_1$-norm, $\\ell_2$-norm, and the second-order cone $\\mathcal{Q}^m$. The tasks are well-defined mathematical exercises in convex optimization, grounded in established theory. The problem is self-contained, consistent, and scientifically sound. Thus, the problem is valid.\n\nWe proceed with the solution in two parts, followed by the final calculation.\n\nPart 1: Basis Pursuit as a Linear Program\nThe Basis Pursuit (BP) problem is defined as:\n$$ \\text{minimize}_{x \\in \\mathbb{R}^{5}} \\|x\\|_{1} \\quad \\text{subject to} \\quad Ax = b $$\nwhere $\\|x\\|_{1} = \\sum_{i=1}^{5} |x_{i}|$. To reformulate this as an LP, we introduce auxiliary variables $t_1, t_2, t_3, t_4, t_5$ in a vector $t \\in \\mathbb{R}^5$. The non-differentiable $\\ell_1$-norm is handled by converting the problem into an equivalent epigraph form. We minimize the sum of the auxiliary variables subject to the condition that they bound the absolute values of the coordinates of $x$.\n\nThe problem becomes:\n$$ \\text{minimize}_{x, t} \\quad \\sum_{i=1}^{5} t_i \\quad \\text{subject to} \\quad |x_i| \\leq t_i \\text{ for } i=1, \\dots, 5 \\text{ and } Ax = b $$\nThis is a linear program because the objective is a linear function of the decision variables, and all constraints are either linear equalities or linear inequalities. The constraint $|x_i| \\leq t_i$ is equivalent to the pair of linear inequalities $x_i \\leq t_i$ and $-x_i \\leq t_i$.\n\nThe decision variables for this LP are the components of $x \\in \\mathbb{R}^5$ and $t \\in \\mathbb{R}^5$, which are $x_1, x_2, x_3, x_4, x_5$ and $t_1, t_2, t_3, t_4, t_5$. There are $10$ decision variables in total.\n\nThe full LP formulation is as follows:\nObjective function:\n$$ \\text{minimize} \\quad t_1 + t_2 + t_3 + t_4 + t_5 $$\nSubject to the constraints:\n1.  Linear equality constraints from $Ax = b$:\n$$ 2x_1 - x_2 + 3x_4 = 1 $$\n$$ x_1 - 2x_3 + 4x_5 = -2 $$\n$$ x_2 + x_3 - x_4 + 2x_5 = 3 $$\n2.  Linear inequality constraints from $|x_i| \\leq t_i$ for $i=1, \\dots, 5$:\n$$ x_1 \\leq t_1 \\quad \\text{and} \\quad -x_1 \\leq t_1 $$\n$$ x_2 \\leq t_2 \\quad \\text{and} \\quad -x_2 \\leq t_2 $$\n$$ x_3 \\leq t_3 \\quad \\text{and} \\quad -x_3 \\leq t_3 $$\n$$ x_4 \\leq t_4 \\quad \\text{and} \\quad -x_4 \\leq t_4 $$\n$$ x_5 \\leq t_5 \\quad \\text{and} \\quad -x_5 \\leq t_5 $$\nThese $10$ inequalities can also be written as $x_i - t_i \\leq 0$ and $-x_i - t_i \\leq 0$ for each $i$.\n\nPart 2: Basis Pursuit Denoising as a Second-Order Cone Program\nThe Basis Pursuit Denoising (BPDN) problem is defined as:\n$$ \\text{minimize}_{x \\in \\mathbb{R}^{5}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|Ax - b\\|_{2} \\leq \\sigma $$\nwith the residual tolerance given as $\\sigma=1$.\nWe use the same epigraph formulation for the $\\ell_1$-norm objective function. The decision variables are again the $10$ variables $x_1, \\dots, x_5$ and $t_1, \\dots, t_5$.\n\nThe full SOCP formulation is as follows:\nObjective function:\n$$ \\text{minimize} \\quad t_1 + t_2 + t_3 + t_4 + t_5 $$\nSubject to the constraints:\n1.  Linear inequality constraints enforcing $|x_i| \\leq t_i$ for $i=1, \\dots, 5$:\n$$ x_1 - t_1 \\leq 0 \\quad \\text{and} \\quad -x_1 - t_1 \\leq 0 $$\n$$ x_2 - t_2 \\leq 0 \\quad \\text{and} \\quad -x_2 - t_2 \\leq 0 $$\n$$ x_3 - t_3 \\leq 0 \\quad \\text{and} \\quad -x_3 - t_3 \\leq 0 $$\n$$ x_4 - t_4 \\leq 0 \\quad \\text{and} \\quad -x_4 - t_4 \\leq 0 $$\n$$ x_5 - t_5 \\leq 0 \\quad \\text{and} \\quad -x_5 - t_5 \\leq 0 $$\n2.  A single second-order cone constraint enforcing $\\|Ax - b\\|_{2} \\leq \\sigma$:\nThe vector $Ax - b$ is given by\n$$ Ax - b = \\begin{pmatrix} 2x_1 - x_2 + 3x_4 - 1 \\\\ x_1 - 2x_3 + 4x_5 - (-2) \\\\ x_2 + x_3 - x_4 + 2x_5 - 3 \\end{pmatrix} = \\begin{pmatrix} 2x_1 - x_2 + 3x_4 - 1 \\\\ x_1 - 2x_3 + 4x_5 + 2 \\\\ x_2 + x_3 - x_4 + 2x_5 - 3 \\end{pmatrix} $$\nThe constraint, with $\\sigma=1$, is:\n$$ \\left\\| \\begin{pmatrix} 2x_1 - x_2 + 3x_4 - 1 \\\\ x_1 - 2x_3 + 4x_5 + 2 \\\\ x_2 + x_3 - x_4 + 2x_5 - 3 \\end{pmatrix} \\right\\|_{2} \\leq 1 $$\nThis inequality defines a second-order cone constraint.\n\nPart 3: Cone Count Vector\nThe cone count vector for the SOCP is an ordered pair $(k, d)$, where $k$ is the total number of second-order cone blocks and $d$ is the sum of their dimensions.\n\nThe problem statement differentiates between the \"linear inequalities\" used for the $\\ell_1$-norm and the \"single second-order cone constraint\" for the residual. Therefore, we count only the latter as a conic block.\nThere is one second-order cone constraint in the BPDN formulation: $\\|Ax - b\\|_{2} \\leq 1$.\nThe standard form for a second-order cone is $\\mathcal{Q}^{m} = \\{(u,v) \\in \\mathbb{R} \\times \\mathbb{R}^{m-1} : \\|v\\|_{2} \\leq u\\}$.\nOur constraint matches this form with $u = 1$ and $v = Ax-b$.\nThe vector $v = Ax-b$ is in $\\mathbb{R}^3$, since $A \\in \\mathbb{R}^{3 \\times 5}$, $x \\in \\mathbb{R}^5$, and $b \\in \\mathbb{R}^3$.\nThe dimension of the vector part of the cone is $m-1 = 3$.\nTherefore, the dimension of the cone itself is $m = (m-1) + 1 = 3+1 = 4$.\nThe constraint corresponds to the vector $(1, Ax-b)$ belonging to the cone $\\mathcal{Q}^4$.\nThe SOCP has a single cone block of dimension $4$.\n\nThe total number of second-order cone blocks is $k=1$.\nThe sum of their dimensions is $d=4$.\nThe cone count vector is $(1, 4)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 4 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Moving beyond the mechanics of reformulation, this practice explores the profound geometric impact of adding conic constraints to a sparse optimization problem. We will analyze how an apparently simple $\\ell_2$-norm bound—a second-order cone constraint—can dramatically alter the solution to a Basis Pursuit problem, even forcing a sparse solution to become dense. This exercise is designed to build your intuition about the trade-offs between sparsity and other desirable properties, and to visualize how the solution is shaped by the intersection of different convex sets .",
            "id": "3475374",
            "problem": "Consider the convex optimization problem of Basis Pursuit in compressed sensing: minimize the $\\ell_{1}$ norm of the unknown vector subject to linear equality constraints. Specifically, let $A \\in \\mathbb{R}^{2 \\times 3}$ and $b \\in \\mathbb{R}^{2}$ be given by\n$$\nA = \\begin{bmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{bmatrix}, \\qquad\nb = \\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix}.\n$$\nFirst consider the problem\n$$\n\\text{(P0)} \\quad \\min_{x \\in \\mathbb{R}^{3}} \\ \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = b.\n$$\nThen consider its tightened version with a Second-Order Cone (SOC) norm bound on the decision variable,\n$$\n\\text{(P1)} \\quad \\min_{x \\in \\mathbb{R}^{3}} \\ \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = b, \\quad \\|x\\|_{2} \\le R,\n$$\nwhere $R = 0.9$, and the SOC norm bound is the second-order cone constraint $\\|x\\|_{2} \\le R$.\n\nUsing only fundamental definitions from convex analysis and optimization (convex sets, norms, linear constraints, and monotonicity of continuous functions), analyze how adding the SOC norm bound shrinks the feasible region of (P0) to form (P1) and explain how this can change the active set (support) of the minimizer. Specifically, parameterize the equality-constrained feasible set $A x = b$, reason about its intersection with the Euclidean ball of radius $R$, and determine the qualitative pattern of the minimizer of (P1) relative to that of (P0). Based on this analysis, select the statement that is correct.\n\nA. The SOC bound does not affect the Basis Pursuit solution; the minimizer of (P1) remains $x = (0, 1, 0)$ with support at index $2$ only.\n\nB. The minimizer of (P1) lies on the boundary of the SOC ball $\\|x\\|_{2} = 0.9$ and uses all three coordinates with symmetry $x_{1} = x_{3} > 0$, yielding a support change from a singleton to $\\{1,2,3\\}$.\n\nC. The SOC bound makes the problem infeasible because the minimum possible $\\ell_{2}$ norm among points satisfying $A x = b$ is larger than $0.9$.\n\nD. The minimizer of (P1) coincides with the minimum Euclidean norm solution for $A x = b$, attained at a parameter $t = \\tfrac{2}{3}$, and thus the SOC bound does not change the location of the optimum within the feasible line segment.",
            "solution": "**Validation of the Problem Statement**\n\nStep 1: Extract Givens\n-   The matrix $A \\in \\mathbb{R}^{2 \\times 3}$ is given by $A = \\begin{bmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{bmatrix}$.\n-   The vector $b \\in \\mathbb{R}^{2}$ is given by $b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n-   Problem (P0): $\\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1}$ subject to $A x = b$.\n-   Problem (P1): $\\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1}$ subject to $A x = b, \\quad \\|x\\|_{2} \\le R$.\n-   The constant $R$ is given as $R = 0.9$.\n\nStep 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is a standard exercise in convex optimization, specifically involving Basis Pursuit ($\\ell_1$ minimization) and Second-Order Cone Programming (SOCP). These are well-established topics in optimization theory, compressed sensing, and signal processing. The problem rests on fundamental principles of linear algebra and convex analysis.\n-   **Well-Posed**: The problems (P0) and (P1) are convex optimization problems (minimizing a convex function over a convex feasible set). The feasible sets are non-empty (as will be shown), so solutions exist. The question asks for a qualitative analysis of the solution, which is a well-defined task.\n-   **Objective**: The problem is stated with precise mathematical definitions and numerical values. There is no ambiguity or subjective language.\n\nStep 3: Verdict and Action\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. I will proceed with the solution derivation.\n\n**Derivation of the Solution**\n\nThe core of the problem is to understand the feasible set defined by the linear constraints $A x = b$, and then analyze the effect of adding the second-order cone constraint $\\|x\\|_2 \\le R$.\n\n**1. Characterization of the Feasible Set for $A x = b$**\n\nThe linear system $A x = b$ is:\n$$\n\\begin{bmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix}\n$$\nThis expands to two linear equations:\n1.  $x_1 + x_2 = 1$\n2.  $x_2 + x_3 = 1$\n\nFrom these equations, we can express $x_1$ and $x_3$ in terms of $x_2$. From the first equation, $x_1 = 1 - x_2$. From the second, $x_3 = 1 - x_2$. This implies $x_1 = x_3$. The set of all solutions to $A x = b$ is a line in $\\mathbb{R}^3$. We can parameterize this line using a single parameter, say $t = x_2$.\nThe parameterized solution is:\n$$\nx(t) = \\begin{bmatrix} 1-t \\\\ t \\\\ 1-t \\end{bmatrix} \\quad \\text{for } t \\in \\mathbb{R}\n$$\n\n**2. Solution to Problem (P0): Basis Pursuit**\n\nProblem (P0) seeks to minimize the $\\ell_1$ norm, $\\|x\\|_1 = |x_1| + |x_2| + |x_3|$, subject to $A x = b$. Using our parameterization, the objective function becomes a function of $t$:\n$$\nf(t) = \\|x(t)\\|_1 = |1-t| + |t| + |1-t| = |t| + 2|1-t|\n$$\nWe need to find the value of $t \\in \\mathbb{R}$ that minimizes $f(t)$. $f(t)$ is a convex function, and we can analyze it by cases based on the arguments of the absolute value functions:\n-   For $t < 0$: $f(t) = -t + 2(1-t) = 2 - 3t$. The function is decreasing.\n-   For $0 \\le t < 1$: $f(t) = t + 2(1-t) = 2 - t$. The function is decreasing.\n-   For $t \\ge 1$: $f(t) = t + 2(t-1) = 3t - 2$. The function is increasing.\n\nThe function $f(t)$ decreases for all $t < 1$ and increases for $t > 1$. Therefore, the minimum of $f(t)$ is attained at $t=1$.\nThe minimizer for (P0) is:\n$$\nx_{P0} = x(1) = \\begin{bmatrix} 1-1 \\\\ 1 \\\\ 1-1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\n$$\nThe support of this solution, which is the set of indices of its non-zero components, is $\\{2\\}$. The minimum $\\ell_1$ norm is $\\|x_{P0}\\|_1 = 1$.\n\n**3. Analysis of Problem (P1): Basis Pursuit with SOC Constraint**\n\nProblem (P1) adds the constraint $\\|x\\|_2 \\le R$, where $R=0.9$. First, we check if the solution to (P0) is feasible for (P1).\nThe $\\ell_2$ norm of $x_{P0}$ is:\n$$\n\\|x_{P0}\\|_2 = \\left\\| \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\right\\|_2 = \\sqrt{0^2 + 1^2 + 0^2} = 1\n$$\nThe constraint requires $\\|x\\|_2 \\le 0.9$. Since $1 > 0.9$, the solution $x_{P0}$ is not feasible for (P1). This means the SOC constraint is active and will change the solution.\n\nThe feasible set for (P1) consists of the points $x(t)$ that also satisfy $\\|x(t)\\|_2 \\le 0.9$. Let's analyze this constraint in terms of $t$:\n$$\n\\|x(t)\\|_2^2 = (1-t)^2 + t^2 + (1-t)^2 = 2(1-t)^2 + t^2\n$$\n$$\n\\|x(t)\\|_2^2 = 2(1 - 2t + t^2) + t^2 = 3t^2 - 4t + 2\n$$\nThe constraint is $\\|x(t)\\|_2^2 \\le R^2 = 0.9^2 = 0.81$.\n$$\n3t^2 - 4t + 2 \\le 0.81 \\implies 3t^2 - 4t + 1.19 \\le 0\n$$\nThis is a quadratic inequality. The parabola $y(t) = 3t^2 - 4t + 1.19$ opens upwards. The inequality holds for $t$ between the roots of $3t^2 - 4t + 1.19 = 0$.\nThe roots are given by the quadratic formula:\n$$\nt = \\frac{-(-4) \\pm \\sqrt{(-4)^2 - 4(3)(1.19)}}{2(3)} = \\frac{4 \\pm \\sqrt{16 - 14.28}}{6} = \\frac{4 \\pm \\sqrt{1.72}}{6}\n$$\nLet's denote the roots as $t_1 = \\frac{4 - \\sqrt{1.72}}{6}$ and $t_2 = \\frac{4 + \\sqrt{1.72}}{6}$. The feasible range for $t$ in problem (P1) is the closed interval $[t_1, t_2]$.\n\nFirst, let's verify if this interval is non-empty, which determines if (P1) is feasible. The discriminant $\\Delta = 1.72 > 0$, so real roots exist and the interval is non-empty. This can also be seen by finding the minimum value of $\\|x(t)\\|_2$. The minimum of the quadratic $g(t) = 3t^2 - 4t + 2$ occurs at $t = -(-4)/(2 \\cdot 3) = 4/6 = 2/3$. The minimum value is $g(2/3) = 3(2/3)^2 - 4(2/3) + 2 = 3(4/9) - 8/3 + 2 = 4/3 - 8/3 + 6/3 = 2/3$. The minimum $\\ell_2$ norm is $\\sqrt{2/3} \\approx 0.8165$. Since $0.8165 < 0.9$, the feasible set is a non-empty line segment.\n\nWe now need to minimize the objective function $f(t) = |t| + 2|1-t|$ over the new feasible interval $t \\in [t_1, t_2]$.\nLet's locate this interval. Since $\\sqrt{1.72} < \\sqrt{4} = 2$, we have $t_2 = \\frac{4 + \\sqrt{1.72}}{6} < \\frac{4+2}{6} = 1$. Since $\\sqrt{1.72} > \\sqrt{1} = 1$, we have $t_1 = \\frac{4 - \\sqrt{1.72}}{6} < \\frac{4-1}{6} = 0.5$. And since $\\sqrt{1.72} < 4$, $t_1 > 0$.\nSo, the entire feasible interval $[t_1, t_2]$ is a subset of $(0, 1)$.\nFor $t \\in (0, 1)$, the objective function simplifies to:\n$$\nf(t) = t + 2(1-t) = 2-t\n$$\nThis is a monotonically decreasing function of $t$. To minimize a decreasing function over a closed interval $[a, b]$, one must choose the largest possible value, which is the right endpoint $b$.\nTherefore, the minimizer for (P1) occurs at $t = t_2 = \\frac{4 + \\sqrt{1.72}}{6}$.\n\nLet's analyze the properties of this solution, $x_{P1} = x(t_2)$:\n1.  Since $t=t_2$ is a root of $3t^2 - 4t + 1.19 = 0$, the constraint $\\|x\\|_2 \\le 0.9$ is active, meaning it holds with equality: $\\|x_{P1}\\|_2 = 0.9$. The solution lies on the boundary of the SOC ball.\n2.  The solution is $x_{P1} = \\begin{bmatrix} 1-t_2 \\\\ t_2 \\\\ 1-t_2 \\end{bmatrix}$.\n3.  Since $0 < t_1 < t_2 < 1$, we have $t_2 \\in (0, 1)$. This implies that $x_1 = 1-t_2 > 0$, $x_2 = t_2 > 0$, and $x_3 = 1-t_2 > 0$. All three components of the solution are positive.\n4.  The support of $x_{P1}$ is $\\{1, 2, 3\\}$. This is a change from the singleton support $\\{2\\}$ of the (P0) solution.\n5.  By construction, the solution has the symmetry $x_1 = x_3$.\n\n**Option-by-Option Analysis**\n\nA. The SOC bound does not affect the Basis Pursuit solution; the minimizer of (P1) remains $x = (0, 1, 0)$ with support at index $2$ only.\n**Incorrect.** As shown, the minimizer of (P0), $x_{P0} = (0, 1, 0)^T$, has an $\\ell_2$ norm of $1$, which violates the constraint $\\|x\\|_2 \\le 0.9$. Thus, $x_{P0}$ is not a feasible solution for (P1), and the minimizer must be different.\n\nB. The minimizer of (P1) lies on the boundary of the SOC ball $\\|x\\|_{2} = 0.9$ and uses all three coordinates with symmetry $x_{1} = x_{3} > 0$, yielding a support change from a singleton to $\\{1,2,3\\}$.\n**Correct.** Our analysis confirms every part of this statement. The minimizer $x_{P1}$ is found at $t=t_2$, where the SOC constraint is active, so $\\|x_{P1}\\|_2 = 0.9$. The solution is $x(t_2)=(1-t_2, t_2, 1-t_2)^T$. Since $t_2 \\in (0,1)$, all three components are positive, so the support is $\\{1,2,3\\}$, a change from the support $\\{2\\}$ of the (P0) solution. The symmetry $x_1 = x_3$ is inherent in the parameterization of the feasible line.\n\nC. The SOC bound makes the problem infeasible because the minimum possible $\\ell_{2}$ norm among points satisfying $A x = b$ is larger than $0.9$.\n**Incorrect.** We calculated the minimum possible $\\ell_2$ norm for a point satisfying $Ax=b$. This occurs at $t=2/3$, and the norm is $\\|x(2/3)\\|_2 = \\sqrt{2/3} \\approx 0.8165$. Since $0.8165 < 0.9$, there exists a segment of the line $Ax=b$ that lies inside the ball $\\|x\\|_2 \\le 0.9$. Therefore, the problem is feasible.\n\nD. The minimizer of (P1) coincides with the minimum Euclidean norm solution for $A x = b$, attained at a parameter $t = \\tfrac{2}{3}$, and thus the SOC bound does not change the location of the optimum within the feasible line segment.\n**Incorrect.** The minimum Euclidean norm solution for $Ax=b$ is attained at $t=2/3$. The minimizer of (P1) is at $t=t_2 = \\frac{4+\\sqrt{1.72}}{6}$. Since $\\sqrt{1.72} > 0$, $t_2 > 4/6 = 2/3$. The two solutions are different. The objective function for (P1) is to minimize $\\|x\\|_1$, not $\\|x\\|_2$. Since the function $f(t) = 2-t$ is decreasing on the feasible interval for $t$, the minimizer is pushed to the largest possible value of $t$, which is $t_2$, not $t=2/3$.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Our final practice bridges the gap between high-level formulation and the actual solution process. Here, you will tackle a square-root LASSO problem, an important variant of BPDN, and compute its optimal solution analytically using first-order optimality conditions from convex analysis. This exercise demystifies how the structure of an SOCP-representable problem, particularly the properties of its subdifferential, can be leveraged to find a precise answer without relying on a black-box solver .",
            "id": "3475331",
            "problem": "Consider the following Second-Order Cone Programming (SOCP) formulation of the square-root Least Absolute Shrinkage and Selection Operator (LASSO) problem in compressed sensing and sparse optimization. For a matrix $A \\in \\mathbb{R}^{m \\times n}$, a measurement vector $b \\in \\mathbb{R}^{m}$, and a regularization parameter $\\lambda > 0$, the problem introduces conic variables $u \\in \\mathbb{R}^{m}$, $t \\in \\mathbb{R}^{n}$, and $r \\in \\mathbb{R}$, and seeks\n$$\n\\text{minimize } r + \\lambda \\mathbf{1}^{\\top} t\n$$\nsubject to\n$$\n\\|u\\|_{2} \\leq r,\\quad Ax - b = u,\\quad -t \\leq x \\leq t,\\quad t \\geq 0,\n$$\nwhere $\\|u\\|_{2}$ denotes the Euclidean norm of $u$, and $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$ is the sum of absolute values of the components of $x$. This SOCP is equivalent to minimizing the sum of the residual $\\ell_{2}$ norm and the decision vector $\\ell_{1}$ norm.\n\nUsing the fundamental definitions of the $\\ell_{1}$ norm, the $\\ell_{2}$ norm, the second-order cone, and the first-order optimality conditions in convex optimization (Karush-Kuhn-Tucker (KKT) conditions), consider the specific instance with $m = 2$ and $n = 2$ given by\n$$\nA = I_{2},\\quad b = \\begin{pmatrix}3 \\\\ 0\\end{pmatrix},\\quad \\lambda = \\frac{1}{2}.\n$$\nCompute the optimal conic variables $(u,t,r)$ that satisfy optimality for this instance. Your final numerical answer must be the values of $(u_{1},u_{2},t_{1},t_{2},r)$, stated exactly with no rounding. Express the conic variables as real numbers. No physical units are involved.",
            "solution": "The user has provided a well-defined problem in the field of convex optimization, specifically requesting the solution to a Second-Order Cone Programming (SOCP) formulation of a square-root LASSO problem for a particular instance.\n\nFirst, a validation of the problem statement is conducted.\n\n**Step 1: Extract Givens**\n- **Problem Formulation**: A Second-Order Cone Program (SOCP).\n- **Objective Function**: Minimize $r + \\lambda \\mathbf{1}^{\\top} t$.\n- **Variables**: $x \\in \\mathbb{R}^{n}$, $u \\in \\mathbb{R}^{m}$, $t \\in \\mathbb{R}^{n}$, $r \\in \\mathbb{R}$.\n- **Constraints**:\n    1. $\\|u\\|_{2} \\leq r$\n    2. $Ax - b = u$\n    3. $-t \\leq x \\leq t$\n    4. $t \\geq 0$\n- **Equivalence**: The problem is stated to be equivalent to minimizing $\\|Ax - b\\|_{2} + \\lambda \\|x\\|_{1}$.\n- **Specific Instance**:\n    - $m = 2$, $n = 2$\n    - $A = I_{2} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n    - $b = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$\n    - $\\lambda = \\frac{1}{2}$\n- **Required Output**: The optimal values of the conic variables $(u_{1}, u_{2}, t_{1}, t_{2}, r)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is based on established principles of convex optimization, specifically the transformation of an $\\ell_1$-regularized problem (square-root LASSO) into an SOCP. This is a standard and mathematically sound technique.\n- **Well-Posedness**: The objective function is linear (and thus convex), and the feasible set is defined by the intersection of a second-order cone, an affine subspace, and component-wise inequalities, which is a convex set. A convex optimization problem with a non-empty feasible set (e.g., $x=0, u=-b, r=\\|b\\|_2, t=0$ is feasible) will have a solution. The problem is well-posed.\n- **Objectivity**: The problem is stated in precise mathematical terms with no ambiguity or subjective elements.\n- **Conclusion**: The problem is self-contained, consistent, and scientifically valid. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A full solution will be provided.\n\nThe SOCP formulation is equivalent to the unconstrained (but non-smooth) optimization problem:\n$$\n\\text{minimize } F(x) = \\|Ax - b\\|_{2} + \\lambda \\|x\\|_{1}\n$$\nwhere $x \\in \\mathbb{R}^{n}$. The optimal conic variables $(u,t,r)$ are related to the optimal decision vector $x^*$ by the relations $u = Ax^* - b$, $r = \\|u\\|_{2}$, and $t_i = |x_i^*|$ for $i=1, \\dots, n$.\n\nSubstituting the given values $A=I_2$, $b = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$, and $\\lambda = \\frac{1}{2}$, the problem becomes:\n$$\n\\text{minimize } F(x) = \\left\\|x - \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}\\right\\|_{2} + \\frac{1}{2} \\|x\\|_{1}\n$$\nwhere $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$.\n\nThis is a convex optimization problem. A point $x^*$ is a global minimizer if and only if the zero vector is contained in the subdifferential of the objective function at $x^*$. The objective function is a sum of two convex functions, $F(x) = g(x) + h(x)$, where $g(x) = \\|x-b\\|_2$ and $h(x) = \\frac{1}{2}\\|x\\|_1$. The subdifferential is $\\partial F(x) = \\partial g(x) + \\partial h(x)$. The optimality condition is $0 \\in \\partial F(x^*)$.\n\nLet us test the candidate solution $x^*=b=\\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$.\nFirst, we find the subdifferential of $g(x)$ at $x^*=b$. Since the argument of the norm is zero, the subdifferential is the closed unit ball in $\\mathbb{R}^2$:\n$$\n\\partial g(x^*) = \\partial\\left(\\|x-b\\|_2\\right)\\Big|_{x=b} = \\{v \\in \\mathbb{R}^2 \\mid \\|v\\|_2 \\leq 1\\}\n$$\nNext, we find the subdifferential of $h(x)$ at $x^* = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$. The subdifferential of the $\\ell_1$ norm is determined component-wise. For a component $x_i \\neq 0$, the subgradient component is $\\text{sign}(x_i)$. For a component $x_i = 0$, the subgradient component is any value in the interval $[-1, 1]$.\n$$\n\\partial\\left(\\|x\\|_1\\right)\\Big|_{x^*=\\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}} = \\left\\{s = \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix} \\in \\mathbb{R}^2 \\;\\middle|\\; s_1 = \\text{sign}(3)=1, s_2 \\in [-1, 1]\\right\\}\n$$\nTherefore, the subdifferential of $h(x)$ is:\n$$\n\\partial h(x^*) = \\frac{1}{2} \\partial\\left(\\|x\\|_1\\right)\\Big|_{x^*} = \\left\\{\\frac{1}{2}s \\;\\middle|\\; s_1 = 1, s_2 \\in [-1, 1]\\right\\}\n$$\nThe optimality condition $0 \\in \\partial g(x^*) + \\partial h(x^*)$ requires the existence of a vector $v \\in \\partial g(x^*)$ and a vector $w \\in \\partial h(x^*)$ such that $v+w=0$. This is equivalent to finding a vector $w \\in \\partial h(x^*)$ such that $-w \\in \\partial g(x^*)$.\n\nLet $w = \\frac{1}{2}s$, where $s = \\begin{pmatrix} 1 \\\\ s_2 \\end{pmatrix}$ and $s_2 \\in [-1, 1]$. We must check if the vector $-w = -\\frac{1}{2}s$ is in the unit ball $\\partial g(x^*)$. This requires checking if $\\|-w\\|_2 \\leq 1$.\n$$\n\\|-w\\|_2 = \\left\\|-\\frac{1}{2}\\begin{pmatrix} 1 \\\\ s_2 \\end{pmatrix}\\right\\|_2 = \\frac{1}{2}\\left\\|\\begin{pmatrix} 1 \\\\ s_2 \\end{pmatrix}\\right\\|_2 = \\frac{1}{2}\\sqrt{1^2 + s_2^2} = \\frac{1}{2}\\sqrt{1+s_2^2}\n$$\nSince $s_2 \\in [-1, 1]$, we have $0 \\leq s_2^2 \\leq 1$.\nThe norm is thus bounded by:\n$$\n\\frac{1}{2}\\sqrt{1+0} \\leq \\|-w\\|_2 \\leq \\frac{1}{2}\\sqrt{1+1}\n$$\n$$\n\\frac{1}{2} \\leq \\|-w\\|_2 \\leq \\frac{\\sqrt{2}}{2}\n$$\nSince $\\frac{\\sqrt{2}}{2} \\approx 0.707 < 1$, the condition $\\|-w\\|_2 \\leq 1$ is satisfied for any choice of $s_2 \\in [-1, 1]$. For instance, we may select $s_2=0$, which gives $s=\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. Then $w=\\begin{pmatrix} 1/2 \\\\ 0 \\end{pmatrix}$, and we must have $v=-w=\\begin{pmatrix} -1/2 \\\\ 0 \\end{pmatrix}$. The norm $\\|v\\|_2 = 1/2$, which is less than or equal to $1$.\n\nThus, the condition $0 \\in \\partial F(x^*)$ is satisfied, and $x^*=\\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$ is the optimal solution for the decision vector $x$.\n\nNow, we compute the optimal conic variables $(u, t, r)$ using their definitions from the problem statement:\n1.  $u = Ax^* - b$. Given $A=I_2$, this simplifies to $u = x^* - b$.\n    $$\n    u = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n    $$\n    So, $u_1 = 0$ and $u_2 = 0$.\n\n2.  $r = \\|u\\|_2$.\n    $$\n    r = \\left\\|\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\right\\|_2 = 0\n    $$\n\n3.  $t_i = |x_i^*|$.\n    $$\n    t_1 = |x_1^*| = |3| = 3\n    $$\n    $$\n    t_2 = |x_2^*| = |0| = 0\n    $$\n    So, $t = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$.\n\nThe optimal conic variables are therefore $u_1 = 0$, $u_2 = 0$, $t_1 = 3$, $t_2 = 0$, and $r = 0$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 0 & 3 & 0 & 0\n\\end{pmatrix}\n}\n$$"
        }
    ]
}