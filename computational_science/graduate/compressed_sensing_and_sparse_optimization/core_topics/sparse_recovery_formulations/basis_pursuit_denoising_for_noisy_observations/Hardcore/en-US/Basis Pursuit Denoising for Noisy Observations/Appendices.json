{
    "hands_on_practices": [
        {
            "introduction": "Before relying on sophisticated solvers, it is crucial to understand the fundamental geometry of an optimization problem. This exercise challenges you to solve a small-scale Basis Pursuit Denoising (BPDN) problem from first principles by systematically exploring candidate solutions of increasing sparsity . This hands-on process of enumerating supports and checking feasibility will build a concrete intuition for the interplay between the $\\ell_1$-norm objective and the data fidelity constraint, which lies at the heart of sparse recovery.",
            "id": "3433445",
            "problem": "Consider the Basis Pursuit Denoising (BPDN) problem, defined as minimizing the $\\ell_{1}$-norm of a coefficient vector subject to a quadratic data fidelity constraint. Specifically, given a sensing matrix $A \\in \\mathbb{R}^{3 \\times 5}$, an observation vector $y \\in \\mathbb{R}^{3}$, and a noise tolerance $\\epsilon > 0$, the BPDN problem is\n$$\n\\min_{x \\in \\mathbb{R}^{5}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - y\\|_{2} \\leq \\epsilon.\n$$\nLet\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 0 & 1\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n1 \\\\ 0.01 \\\\ 0\n\\end{pmatrix}, \\quad \\epsilon = 0.02.\n$$\nUsing first principles—namely, the definition of the BPDN feasible set, enumeration of candidate supports (subsets of $\\{1,2,3,4,5\\}$), and rigorous feasibility and optimality checks based on convexity and subgradient optimality—determine the optimal objective value $\\min \\|x\\|_{1}$ for this instance. You must not use any off-the-shelf solver or shortcut formulas; instead, reason from the BPDN formulation, the structure of $A$, and the residual constraint $\\|A x - y\\|_{2} \\leq \\epsilon$.\n\nReport the single real number that is the optimal value of the BPDN objective $\\min \\|x\\|_{1}$. No rounding is required; express your final answer in exact form.",
            "solution": "The problem is an instance of Basis Pursuit Denoising (BPDN), a convex optimization problem formulated as:\n$$\n\\min_{x \\in \\mathbb{R}^{5}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - y\\|_{2} \\leq \\epsilon\n$$\nThe given data are:\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 0 & 1\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n1 \\\\ 0.01 \\\\ 0\n\\end{pmatrix}, \\quad \\epsilon = 0.02\n$$\nThe columns of $A$ are $a_1, a_2, a_3, a_4, a_5 \\in \\mathbb{R}^3$. The vector $y$ can be expressed as $y = a_1 + 0.01 a_2$. The problem seekscoeffs an $x$ that is sparse and approximates $y$ well. We proceed by examining candidate solutions with increasing sparsity (number of non-zero entries, denoted by $k=\\|x\\|_0$).\n\n**Case $k=0$:**\nIf $x=0$, the residual is $\\|A \\cdot 0 - y\\|_{2} = \\|y\\|_{2} = \\sqrt{1^2 + 0.01^2 + 0^2} = \\sqrt{1.0001}$.\nThe constraint is $\\|y\\|_2 \\leq \\epsilon$, which is $\\sqrt{1.0001} \\leq 0.02$. This is equivalent to $1.0001 \\leq 0.02^2 = 0.0004$, which is false. Thus, $x=0$ is not a feasible solution.\n\n**Case $k=1$:**\nLet's assume the solution $x$ has only one non-zero component, $x_i$, with support $S=\\{i\\}$. We need to find $\\min |x_i|$ subject to $\\|x_i a_i - y\\|_2 \\leq \\epsilon$. A solution exists if the line spanned by $a_i$ intersects the $\\ell_2$-ball of radius $\\epsilon$ around $y$. This is true if the orthogonal projection of $y$ onto the line, $\\text{proj}_{a_i}(y)$, is such that $\\|y - \\text{proj}_{a_i}(y)\\|_2 \\leq \\epsilon$.\n\nLet's check this for each column $a_i$:\n- For $i=1$: $a_1=(1,0,0)^T$. $a_1^T a_1=1$, $a_1^T y = 1$. $\\text{proj}_{a_1}(y) = \\frac{1}{1}a_1 = (1,0,0)^T$.\nThe distance is $\\|y - \\text{proj}_{a_1}(y)\\|_2 = \\|(0, 0.01, 0)^T\\|_2 = 0.01$. Since $0.01 \\leq \\epsilon=0.02$, a solution exists.\nThe constraint becomes $\\|(x_1-1, -0.01, 0)\\|_2^2 \\leq 0.02^2$, which is $(x_1-1)^2 + 0.0001 \\leq 0.0004$, or $(x_1-1)^2 \\leq 0.0003$.\nThis gives $1-\\sqrt{0.0003} \\leq x_1 \\leq 1+\\sqrt{0.0003}$. Since both bounds are positive, the minimum of $|x_1|$ is $1-\\sqrt{0.0003}$.\n\n- For $i=2$: $a_2=(0,1,0)^T$. $a_2^T a_2=1$, $a_2^T y = 0.01$. $\\text{proj}_{a_2}(y) = 0.01 a_2$.\nThe distance is $\\|y - \\text{proj}_{a_2}(y)\\|_2 = \\|(1,0,0)^T\\|_2=1$. $1 > 0.02$, so no feasible solution on this support.\n\n- For $i=3$: $a_3=(0,0,1)^T$. $a_3^T y=0$. The distance is $\\|y\\|_2=\\sqrt{1.0001} > 0.02$. No feasible solution.\n\n- For $i=4$: $a_4=(1,1,0)^T$. $a_4^T a_4=2$, $a_4^T y=1.01$. $\\text{proj}_{a_4}(y) = \\frac{1.01}{2}a_4$.\nThe distance is $\\|y - \\frac{1.01}{2}a_4\\|_2 = \\|(1-0.505, 0.01-0.505, 0)\\|_2 = \\|(0.495, -0.495, 0)\\|_2 = \\sqrt{2 \\cdot 0.495^2} = 0.495\\sqrt{2} > 0.02$. No feasible solution.\n\n- For $i=5$: $a_5=(0,1,1)^T$. $a_5^T y=0.01$. The distance is $\\sqrt{1^2+0.005^2+(-0.005)^2} = \\sqrt{1.00005} > 0.02$. No feasible solution.\n\nThe only feasible $1$-sparse solution is on support $\\{1\\}$, yielding an objective value of $1-\\sqrt{0.0003}$.\n$\\sqrt{0.0003}=\\sqrt{3 \\times 10^{-4}} = 10^{-2}\\sqrt{3} \\approx 0.01732$. So, the objective value is approximately $1-0.01732 = 0.98268$.\n\n**Case $k=2$:**\nLet's analyze promising supports of size $2$. A good support $S=\\{i,j\\}$ corresponds to a subspace $\\text{span}(a_i, a_j)$ that is \"close\" to $y$. The vector $y=(1, 0.01, 0)^T$ lies in the $xy$-plane, which is spanned by $\\{a_1, a_2\\}$.\nNotice that $a_4=a_1+a_2$, so $\\text{span}(a_1, a_4) = \\text{span}(a_1, a_2)$. Let's analyze the support $S=\\{1,4\\}$.\nLet $x=(x_1, 0, 0, x_4, 0)^T$. The constraint is $\\|x_1 a_1 + x_4 a_4 - y\\|_2 \\leq \\epsilon$.\n$x_1 a_1 + x_4 a_4 = x_1(1,0,0)^T + x_4(1,1,0)^T = (x_1+x_4, x_4, 0)^T$.\nThe constraint becomes $\\|(x_1+x_4 - 1, x_4 - 0.01, 0)\\|_2 \\leq 0.02$, which simplifies to $(x_1+x_4 - 1)^2 + (x_4 - 0.01)^2 \\leq 0.02^2 = 0.0004$.\nWe want to minimize $\\|x\\|_1 = |x_1|+|x_4|$ subject to this constraint.\nLet $z_1 = x_1+x_4$ and $z_2 = x_4$. Then $x_1 = z_1 - z_2$.\nThe problem transforms to minimizing $|z_1-z_2|+|z_2|$ subject to $(z_1-1)^2 + (z_2-0.01)^2 \\leq 0.0004$.\nThe feasible region in the $(z_1, z_2)$ plane is a disk centered at $(1, 0.01)$ with radius $0.02$.\nFor any point in this disk, $z_1 \\in [1-0.02, 1+0.02] = [0.98, 1.02]$ and $z_2 \\in [0.01-0.02, 0.01+0.02] = [-0.01, 0.03]$.\nThe term $z_1-z_2$ has a minimum value of $z_{1, \\min} - z_{2, \\max} = 0.98 - 0.03 = 0.95$, which is positive. So $|z_1-z_2|=z_1-z_2$.\nThe objective function is $z_1-z_2+|z_2|$.\nWe analyze two sub-cases based on the sign of $z_2$:\n1. If $z_2 \\geq 0$: The objective is $z_1-z_2+z_2=z_1$. To minimize $z_1$ in the feasible region, we take its minimum possible value, which is $0.98$. This occurs at the point $(z_1, z_2) = (0.98, 0.01)$. This point is in the feasible region and satisfies $z_2 \\geq 0$. The objective value is $0.98$.\n2. If $z_2 < 0$: The objective is $z_1-z_2-z_2=z_1-2z_2$. This is a linear function. Its minimum over the part of the disk where $z_2<0$ must occur on the boundary. The boundary consists of a circular arc and a line segment on the $z_1$-axis ($z_2=0$). The minimum of $z_1-2z_2$ on the circular arc can be found using calculus, but it can be shown that the minimum over the entire $z_2<0$ region occurs on the segment where $z_2=0$. On this segment, the objective is $z_1$, and from $(z_1-1)^2 \\le 0.0003$, the minimum value for $z_1$ is $1-\\sqrt{0.0003}$.\n\nComparing the minimum values from both cases: $0.98$ and $1-\\sqrt{0.0003} \\approx 0.98268$. The smaller value is $0.98$.\nThis minimum is achieved at $(z_1, z_2) = (0.98, 0.01)$.\nThis corresponds to $x_4=z_2=0.01$ and $x_1=z_1-z_2=0.98-0.01=0.97$.\nThe solution candidate is $x^*=(0.97, 0, 0, 0.01, 0)^T$.\nThe objective value is $\\|x^*\\|_1 = |0.97|+|0.01| = 0.98$.\nLet's check feasibility: $Ax^* = 0.97 a_1 + 0.01 a_4 = (0.97,0,0)^T + (0.01,0.01,0)^T = (0.98, 0.01, 0)^T$.\n$\\|Ax^*-y\\|_2 = \\|(0.98-1, 0.01-0.01, 0-0)\\|_2 = \\|(-0.02, 0, 0)\\|_2 = 0.02$.\nThe constraint $\\|Ax^*-y\\|_2 \\leq 0.02$ is satisfied.\nThe objective value $0.98$ is less than $1-\\sqrt{0.0003} \\approx 0.98268$. So this $2$-sparse solution is better than the $1$-sparse one.\n\n**Optimality Check:**\nFor a convex problem like BPDN, the Karush-Kuhn-Tucker (KKT) conditions are sufficient for optimality. A solution $x^*$ is optimal if there exists a dual vector $\\nu \\in \\mathbb{R}^3$ such that $A^T\\nu$ is a subgradient of the $\\ell_1$-norm at $x^*$, i.e., $A^T\\nu \\in \\partial\\|x^*\\|_1$, and $\\nu = c(y-Ax^*)$ for some constant $c > 0$ (since the constraint is active).\nFor our candidate $x^*=(0.97, 0, 0, 0.01, 0)^T$, the support is $S=\\{1,4\\}$.\nThe residual is $r = y - Ax^* = (1, 0.01, 0)^T - (0.98, 0.01, 0)^T = (0.02, 0, 0)^T$.\nLet's choose $\\nu = c r = c(0.02, 0, 0)^T$.\nThe subgradient candidate is $s = A^T\\nu$:\n$$ s = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0.02c \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0.02c \\\\ 0 \\\\ 0 \\\\ 0.02c \\\\ 0 \\end{pmatrix} $$\nThe subgradient conditions are:\n- $s_i = \\text{sgn}(x_i^*)$ for $i \\in S$.\n  - $s_1 = \\text{sgn}(0.97) = 1 \\implies 0.02c = 1 \\implies c=50$.\n  - $s_4 = \\text{sgn}(0.01) = 1 \\implies 0.02c = 1 \\implies c=50$.\n- $|s_i| \\leq 1$ for $i \\notin S$.\n  - $|s_2|=0 \\leq 1$. (Holds)\n  - $|s_3|=0 \\leq 1$. (Holds)\n  - $|s_5|=0 \\leq 1$. (Holds)\nAll conditions are satisfied with $c=50 > 0$. Therefore, $x^*=(0.97, 0, 0, 0.01, 0)^T$ is the optimal solution.\nThe optimal objective value is $\\|x^*\\|_1 = 0.98$.\nIn exact fraction form, this is $\\frac{98}{100} = \\frac{49}{50}$.\n\nFinal comparison of objective values found:\n- $1$-sparse solution on $\\{1\\}$: $1-\\sqrt{0.0003} \\approx 0.98268$\n- $2$-sparse solution on $\\{1,4\\}$: $0.98$\n\nThe minimum value is $0.98$.\nFurther checks for supports of size $k \\ge 3$ will not yield a better result, as confirmed by the KKT conditions. For instance, considering the basis $S=\\{1,2,3\\}$ leads to a soft-thresholding problem on $y$ which results in the same $1$-sparse solution found earlier.\nThe optimal objective value is therefore $0.98$.\nThe final answer must be in exact form.\n$0.98 = \\frac{98}{100} = \\frac{49}{50}$.",
            "answer": "$$\\boxed{\\frac{49}{50}}$$"
        },
        {
            "introduction": "In convex optimization, finding a candidate solution is only half the battle; proving it is truly the global optimum is paramount. This practice introduces the Karush-Kuhn-Tucker (KKT) conditions, which provide a formal certificate of optimality for problems like BPDN . By rigorously verifying that a given point satisfies primal feasibility, stationarity, and complementary slackness, you will gain proficiency in applying the core theoretical tool for validating solutions to constrained convex programs.",
            "id": "3433457",
            "problem": "Consider the Basis Pursuit Denoising (BPDN) problem with noisy observations, formulated as minimizing the convex functional under a second-order cone constraint:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - y\\|_{2} \\leq \\varepsilon,\n$$\nwhere $A \\in \\mathbb{R}^{2 \\times 3}$, $y \\in \\mathbb{R}^{2}$, and $\\varepsilon \\in \\mathbb{R}_{+}$. Let the data be\n$$\nA = \\begin{pmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 0 \\end{pmatrix}, \\quad\ny = \\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix}, \\quad \\varepsilon = 1,\n$$\nand consider the candidate primal point and dual vector\n$$\n\\hat{x} = \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\qquad\n\\hat{z} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nUsing first principles from convex analysis (subdifferentials and normal cones), verify the Karush–Kuhn–Tucker (KKT) conditions—namely, primal feasibility, dual feasibility, stationarity, and complementary slackness—for the BPDN problem. Conclude that $\\hat{x}$ is optimal. Then compute the optimal objective value $\\|\\hat{x}\\|_{1}$. Provide the final value as an exact real number (no rounding).",
            "solution": "The problem asks to verify that the candidate point $\\hat{x}$ is an optimal solution to the Basis Pursuit Denoising (BPDN) problem by checking the Karush–Kuhn–Tucker (KKT) conditions. The BPDN problem is a convex optimization problem formulated as:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - y\\|_{2} \\leq \\varepsilon\n$$\nThe objective function $f(x) = \\|x\\|_{1}$ is convex, and the feasible set $C = \\{x \\in \\mathbb{R}^{3} \\mid \\|A x - y\\|_{2} \\leq \\varepsilon\\}$ is a convex set. For a convex optimization problem, the KKT conditions are sufficient for global optimality.\n\nLet's define the problem in the standard form $\\min_{x} f(x)$ subject to $g(x) \\leq 0$, with $f(x) = \\|x\\|_{1}$ and $g(x) = \\|A x - y\\|_{2} - \\varepsilon$.\nFrom first principles of convex analysis, a point $\\hat{x} \\in C$ is optimal if and only if it satisfies the condition $0 \\in \\partial f(\\hat{x}) + N_C(\\hat{x})$, where $\\partial f(\\hat{x})$ is the subdifferential of $f$ at $\\hat{x}$ and $N_C(\\hat{x})$ is the normal cone to the feasible set $C$ at $\\hat{x}$.\n\nThe normal cone $N_C(\\hat{x})$ depends on whether the constraint is active at $\\hat{x}$:\n- If $g(\\hat{x}) < 0$, then $\\hat{x}$ is in the interior of $C$, and $N_C(\\hat{x}) = \\{0\\}$.\n- If $g(\\hat{x}) = 0$, then $\\hat{x}$ is on the boundary of $C$, and $N_C(\\hat{x}) = \\{ \\lambda v \\mid \\lambda \\geq 0, v \\in \\partial g(\\hat{x}) \\}$.\n\nThe subdifferential of $g(x)$ at $\\hat{x}$ is, by the chain rule, $\\partial g(\\hat{x}) = A^T \\partial_u(\\|u\\|_2)|_{u=A\\hat{x}-y}$.\n\nWe first verify the primal feasibility of the candidate point $\\hat{x} = \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix}$.\nThe given data are $A = \\begin{pmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 0 \\end{pmatrix}$, $y = \\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix}$, and $\\varepsilon = 1$.\nThe residual vector is computed as:\n$$\nA\\hat{x} - y = \\begin{pmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(2) + (0)(0) + (-1)(-1) \\\\ (0)(2) + (1)(0) + (0)(-1) \\end{pmatrix} - \\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\n$$\nThe $\\ell_2$-norm of the residual is:\n$$\n\\|A\\hat{x} - y\\|_{2} = \\left\\| \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\sqrt{(-1)^{2} + 0^{2}} = 1\n$$\nThe primal feasibility condition is $\\|A\\hat{x} - y\\|_{2} \\leq \\varepsilon$, which is $1 \\leq 1$. This is true, so $\\hat{x}$ is a feasible point.\nFurthermore, since $\\|A\\hat{x} - y\\|_{2} = \\varepsilon$, the constraint is active. This means $\\hat{x}$ lies on the boundary of the feasible set, and the normal cone $N_C(\\hat{x})$ is non-trivial.\n\nSince the residual $A\\hat{x}-y \\neq 0$, the subdifferential of $\\|u\\|_2$ at $u=A\\hat{x}-y$ is the unique vector $\\frac{A\\hat{x}-y}{\\|A\\hat{x}-y\\|_2}$.\nThus, $\\partial g(\\hat{x})$ is the singleton set containing the vector:\n$$\nA^T \\frac{A\\hat{x}-y}{\\|A\\hat{x}-y\\|_2} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ -1 & 0 \\end{pmatrix} \\frac{\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}}{1} = \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nThe normal cone at $\\hat{x}$ is $N_C(\\hat{x}) = \\left\\{ \\lambda \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\mid \\lambda \\geq 0 \\right\\}$.\nThe optimality condition $0 \\in \\partial f(\\hat{x}) + N_C(\\hat{x})$ becomes: there must exist a $\\lambda \\geq 0$ such that\n$$\n-\\lambda \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\in \\partial \\|\\hat{x}\\|_1\n$$\nLet us define a dual variable $\\hat{z} = \\lambda \\frac{y-A\\hat{x}}{\\|y-A\\hat{x}\\|_2}$. With this definition, the optimality conditions can be stated in terms of the primal-dual pair $(\\hat{x}, \\hat{z})$:\n1.  **Primal Feasibility**: $\\|A\\hat{x} - y\\|_2 \\leq \\varepsilon$.\n2.  **Stationarity**: $A^T \\hat{z} \\in \\partial \\|\\hat{x}\\|_1$.\n3.  **Complementary Slackness**: $\\langle \\hat{z}, y-A\\hat{x} \\rangle = \\varepsilon \\|\\hat{z}\\|_2$.\n\nWe proceed to verify these three conditions for the given $\\hat{x} = \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix}$ and $\\hat{z} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\n**1. Primal Feasibility:**\nAs calculated before, $\\|A\\hat{x} - y\\|_2 = 1$ and $\\varepsilon = 1$. The condition $1 \\leq 1$ is satisfied.\n\n**2. Stationarity:**\nWe check if $A^T \\hat{z}$ is an element of the subdifferential of $\\|x\\|_1$ at $\\hat{x}$.\nFirst, we compute the vector $A^T \\hat{z}$:\n$$\nA^T \\hat{z} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}\n$$\nNext, we determine the subdifferential $\\partial \\|\\hat{x}\\|_1$. For a vector $s = (s_1, s_2, s_3)^T$ to be in $\\partial \\|\\hat{x}\\|_1$, its components must satisfy $s_i = \\text{sign}(\\hat{x}_i)$ if $\\hat{x}_i \\neq 0$, and $s_i \\in [-1, 1]$ if $\\hat{x}_i = 0$.\nFor $\\hat{x} = (2, 0, -1)^T$:\n- $s_1 = \\text{sign}(2) = 1$.\n- $s_2 \\in [-1, 1]$.\n- $s_3 = \\text{sign}(-1) = -1$.\nThe vector $A^T \\hat{z} = (1, 0, -1)^T$ satisfies these conditions: its first component is $1$, its second component $0$ is in the interval $[-1, 1]$, and its third component is $-1$.\nTherefore, $A^T \\hat{z} \\in \\partial \\|\\hat{x}\\|_1$, and the stationarity condition is satisfied.\n\n**3. Complementary Slackness:**\nWe must check if $\\langle \\hat{z}, y - A\\hat{x} \\rangle = \\varepsilon \\|\\hat{z}\\|_2$.\nThe vector $y - A\\hat{x} = -(A\\hat{x} - y) = - \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe left-hand side is:\n$$\n\\langle \\hat{z}, y - A\\hat{x} \\rangle = \\left\\langle \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\rangle = (1)(1) + (0)(0) = 1\n$$\nThe right-hand side is:\n$$\n\\varepsilon \\|\\hat{z}\\|_2 = 1 \\cdot \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\|_2 = 1 \\cdot \\sqrt{1^2 + 0^2} = 1\n$$\nSince both sides equal $1$, the complementary slackness condition is satisfied.\n\nAll three KKT conditions are satisfied for the given primal point $\\hat{x}$ and dual vector $\\hat{z}$. As the BPDN problem is convex, satisfaction of the KKT conditions is sufficient to prove optimality. We conclude that $\\hat{x}$ is an optimal solution.\n\nFinally, we compute the optimal objective value, which is $\\|\\hat{x}\\|_1$:\n$$\n\\|\\hat{x}\\|_1 = \\|(2, 0, -1)^T\\|_1 = |2| + |0| + |-1| = 2 + 0 + 1 = 3\n$$\nThe optimal value of the objective function is $3$.",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "While direct enumeration is instructive, it is computationally infeasible for realistic problems. This exercise shifts our focus to practical algorithms by examining the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), a cornerstone of modern sparse optimization . By executing a single iteration, you will deconstruct its two key components—a standard gradient descent step on the smooth part of the objective and a proximal step that promotes sparsity—and analyze the theoretical properties that guarantee its rapid convergence.",
            "id": "3433513",
            "problem": "Consider the basis pursuit denoising (BPDN) problem with squared-error data fidelity and an $\\ell_{1}$ penalty: minimize over $x \\in \\mathbb{R}^{4}$ the composite objective $F(x) \\equiv f(x) + g(x)$, where $f(x) = \\tfrac{1}{2}\\|A x - y\\|_{2}^{2}$ and $g(x) = \\lambda \\|x\\|_{1}$. Let the sensing matrix be \n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \n\\end{pmatrix},\n$$\nthe observation vector be $y = \\begin{pmatrix} 3 \\\\ -1 \\\\ \\tfrac{1}{2} \\end{pmatrix}$, and the regularization weight be $\\lambda = \\tfrac{3}{4}$. Start the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) with $x^{0} = 0 \\in \\mathbb{R}^{4}$, $z^{0} = x^{0}$, and $t_{0} = 1$. Use the standard FISTA step-size choice $L$ equal to the Lipschitz constant of $\\nabla f$, and perform exactly one full FISTA iteration. \n\nTasks:\n- Determine $L$ from first principles and use it as the step size.\n- Write explicitly the momentum update and the proximal step for this instance and compute $x^{1}$.\n- Provide a concise, first-principles explanation of why the acceleration in FISTA does not change the set of fixed points of the underlying proximal gradient operator for this BPDN problem.\n\nAnswer format:\n- Report the iterate $x^{1}$ as a $1 \\times 4$ row vector using exact values (fractions where appropriate). Do not round.",
            "solution": "The problem is to perform one iteration of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) for the Basis Pursuit Denoising (BPDN) problem and to provide a theoretical explanation regarding its fixed points. The objective function to minimize is $F(x) = f(x) + g(x)$, where $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ is the smooth data-fidelity term and $g(x) = \\lambda \\|x\\|_{1}$ is the non-smooth regularization term.\n\nFirst, we determine the step size, which is set to $\\gamma = \\frac{1}{L}$, where $L$ is the Lipschitz constant of the gradient of $f(x)$, denoted $\\nabla f(x)$. The gradient of $f(x)$ is:\n$$\n\\nabla f(x) = A^{T}(A x - y)\n$$\nThe Lipschitz constant $L$ of $\\nabla f(x)$ is the spectral norm of its Hessian matrix, $\\nabla^2 f(x)$. Since the gradient is affine in $x$, the Hessian is constant:\n$$\n\\nabla^{2} f(x) = A^{T}A\n$$\nThus, $L = \\|A^{T}A\\|_{2}$, where $\\|\\cdot\\|_{2}$ denotes the spectral norm (largest singular value). For a positive semi-definite matrix like $A^{T}A$, this is its largest eigenvalue. We are given:\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \n\\end{pmatrix}\n$$\nIts transpose is:\n$$\nA^{T} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n$$\nThe Hessian is then:\n$$\nA^{T}A = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \n\\end{pmatrix} = \n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 \n\\end{pmatrix}\n$$\nThis is a diagonal matrix. Its eigenvalues are the diagonal entries: $1$, $1$, $1$, and $0$. The largest eigenvalue is $1$. Therefore, the Lipschitz constant is $L = 1$, and the step size is $\\gamma = \\frac{1}{L} = 1$.\n\nNext, we perform one iteration of FISTA. The algorithm updates are:\n1. $x^{k+1} = \\text{prox}_{\\gamma g}(z^k - \\gamma \\nabla f(z^k))$\n2. $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$\n3. $z^{k+1} = x^{k+1} + \\frac{t_k - 1}{t_{k+1}}(x^{k+1} - x^k)$\n\nThe initial conditions are given as $x^{0} = \\mathbf{0} \\in \\mathbb{R}^{4}$, $z^{0} = x^{0} = \\mathbf{0}$, and $t_{0} = 1$. We compute the first iterate $x^{1}$ (for $k=0$):\n$$\nx^{1} = \\text{prox}_{\\gamma g}(z^0 - \\gamma \\nabla f(z^0))\n$$\nWith $z^{0} = \\mathbf{0}$ and $\\gamma = 1$, the argument of the proximal operator is:\n$$\nv = z^0 - \\gamma \\nabla f(z^0) = \\mathbf{0} - 1 \\cdot A^{T}(A\\mathbf{0} - y) = -A^{T}(-y) = A^{T}y\n$$\nWe are given $y = \\begin{pmatrix} 3 \\\\ -1 \\\\ \\frac{1}{2} \\end{pmatrix}$. So we compute $v$:\n$$\nv = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\begin{pmatrix} 3 \\\\ -1 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix}\n$$\nThe proximal operator for $g(x) = \\lambda \\|x\\|_{1}$ is the soft-thresholding operator, $\\text{Shrink}(v, \\tau)$, which acts element-wise as $\\text{Shrink}(v_i, \\tau) = \\text{sign}(v_i) \\max(|v_i| - \\tau, 0)$. The threshold is $\\tau = \\gamma \\lambda = 1 \\cdot \\frac{3}{4} = \\frac{3}{4}$.\n\nWe apply this to each component of $v$:\n$$\nx_1^1 = \\text{Shrink}\\left(3, \\frac{3}{4}\\right) = \\text{sign}(3) \\max\\left(|3| - \\frac{3}{4}, 0\\right) = 1 \\cdot \\left(3 - \\frac{3}{4}\\right) = \\frac{12-3}{4} = \\frac{9}{4}\n$$\n$$\nx_2^1 = \\text{Shrink}\\left(-1, \\frac{3}{4}\\right) = \\text{sign}(-1) \\max\\left(|-1| - \\frac{3}{4}, 0\\right) = -1 \\cdot \\left(1 - \\frac{3}{4}\\right) = -\\frac{1}{4}\n$$\n$$\nx_3^1 = \\text{Shrink}\\left(\\frac{1}{2}, \\frac{3}{4}\\right) = \\text{sign}\\left(\\frac{1}{2}\\right) \\max\\left(\\left|\\frac{1}{2}\\right| - \\frac{3}{4}, 0\\right) = 1 \\cdot \\max\\left(-\\frac{1}{4}, 0\\right) = 0\n$$\n$$\nx_4^1 = \\text{Shrink}\\left(0, \\frac{3}{4}\\right) = \\text{sign}(0) \\max\\left(|0| - \\frac{3}{4}, 0\\right) = 0\n$$\nThus, the first iterate is $x^{1} = \\begin{pmatrix} \\frac{9}{4} \\\\ -\\frac{1}{4} \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nFinally, we explain why the acceleration in FISTA does not change the set of fixed points.\nA point $x^*$ is a minimizer of the composite objective function $F(x)=f(x)+g(x)$ if and only if it satisfies the first-order optimality condition:\n$$\n\\mathbf{0} \\in \\nabla f(x^*) + \\partial g(x^*)\n$$\nwhere $\\partial g(x^*)$ is the subdifferential of $g$ at $x^*$. This condition is equivalent to the fixed-point equation for the proximal gradient operator $T(z) = \\text{prox}_{\\gamma g}(z - \\gamma \\nabla f(z))$:\n$$\nx^* = T(x^*) = \\text{prox}_{\\gamma g}(x^* - \\gamma \\nabla f(x^*))\n$$\nThe set of minimizers of $F(x)$ is therefore precisely the set of fixed points of the operator $T$.\n\nNow, consider the FISTA algorithm. A fixed point of the FISTA iteration is a point $(x^*, z^*)$ such that if $x^k = x^{k-1} = x^*$, then $x^{k+1} = x^*$ and $z^{k+1} = z^k = z^*$.\nThe update for $z$ is $z^{k+1} = x^{k+1} + \\frac{t_k - 1}{t_{k+1}}(x^{k+1} - x^k)$. At a fixed point, $x^{k+1} = x^k = x^*$, so the second term vanishes, yielding $z^{k+1} = x^{k+1}$. This implies that at any fixed point of the algorithm, we must have $z^* = x^*$.\n\nThe update for $x$ is $x^{k+1} = T(z^k)$. At the fixed point $(x^*, z^*)$, this becomes $x^* = T(z^*)$.\nSubstituting the condition $z^* = x^*$ into this equation, we get:\n$$\nx^* = T(x^*)\n$$\nThis is the identical fixed-point condition as for the standard (non-accelerated) proximal gradient method. Therefore, FISTA and the proximal gradient method share the same set of fixed points. The acceleration scheme in FISTA, which involves evaluating the gradient at an extrapolated point $z^k$ instead of $x^k$, alters the path of the iterates toward a solution but does not change the set of possible solutions (the fixed points).",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{9}{4} & -\\frac{1}{4} & 0 & 0\n\\end{pmatrix}\n}\n$$"
        }
    ]
}