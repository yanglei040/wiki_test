{
    "hands_on_practices": [
        {
            "introduction": "Understanding the LASSO solution path is fundamental to grasping its feature selection mechanism. This first exercise provides a direct, hands-on experience in tracing this path by applying the Karush–Kuhn–Tucker (KKT) optimality conditions to a small, concrete problem . By manually calculating the critical values of the regularization parameter $\\lambda$ where the set of non-zero coefficients changes, you will build a solid intuition for how sparsity is induced and controlled.",
            "id": "3488574",
            "problem": "Consider the least absolute shrinkage and selection operator (LASSO) problem in sparse linear regression with $n=2$ samples and $p=3$ features. Let the design matrix be\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix},\n$$\nand the response vector be\n$$\ny \\;=\\; \\begin{pmatrix}\n1 \\\\\n0.5\n\\end{pmatrix}.\n$$\nThe LASSO estimator $\\beta \\in \\mathbb{R}^{3}$ is defined as the minimizer of\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\;\\; \\frac{1}{2}\\,\\|\\,y - X\\beta\\,\\|_{2}^{2} \\;+\\; \\lambda\\,\\|\\beta\\|_{1},\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter and $\\|\\cdot\\|_{1}$ denotes the $\\ell_1$ norm. Starting from first principles in convex optimization and subgradient calculus, write out explicitly the Karush–Kuhn–Tucker (KKT) conditions for optimality for this problem and this specific $(X,y)$. Use these conditions to determine, as $\\lambda$ varies over $\\lambda \\ge 0$, the intervals of $\\lambda$ over which the support (the set of indices of nonzero coordinates of the minimizer) remains constant. Report, as your final answer, the ordered list of the finite critical values of $\\lambda$ at which the support changes. Express all mathematical entities in standard mathematical notation. No rounding is required, and no physical units are involved. The answer must be a single row vector containing the critical $\\lambda$ values in descending order.",
            "solution": "The problem as stated is a well-posed application of convex optimization to the LASSO regression model. All necessary data and definitions are provided, and the problem is internally consistent and scientifically grounded. We may therefore proceed with the solution.\n\nThe LASSO optimization problem is defined as finding the vector $\\beta \\in \\mathbb{R}^{3}$ that minimizes the objective function:\n$$\nL(\\beta) \\;=\\; \\frac{1}{2}\\,\\|\\,y - X\\beta\\,\\|_{2}^{2} \\;+\\; \\lambda\\,\\|\\beta\\|_{1}\n$$\nHere, the least-squares term $f(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2}$ is a differentiable convex function, and the penalty term $g(\\beta) = \\lambda\\|\\beta\\|_{1}$ is a non-differentiable convex function. The minimizer $\\beta$ must satisfy the Karush–Kuhn–Tucker (KKT) optimality conditions. For a problem of this form, the KKT conditions are given by the subgradient optimality condition $0 \\in \\nabla f(\\beta) + \\partial g(\\beta)$.\n\nThe gradient of the least-squares term is:\n$$\n\\nabla f(\\beta) = -X^\\top(y - X\\beta) = X^\\top X\\beta - X^\\top y\n$$\nThe subgradient of the $\\ell_1$-norm penalty term is $\\lambda \\partial \\|\\beta\\|_{1}$, where $\\partial \\|\\beta\\|_{1}$ is a vector $s \\in \\mathbb{R}^{3}$ with components $s_j$ such that:\n$$\ns_j = \\begin{cases}\n\\text{sign}(\\beta_j) & \\text{if } \\beta_j \\neq 0 \\\\\nv_j \\in [-1, 1] & \\text{if } \\beta_j = 0\n\\end{cases}\n$$\nThus, the KKT conditions require the existence of a subgradient vector $s$ satisfying the above properties such that $\\nabla f(\\beta) + \\lambda s = 0$, which can be written as:\n$$\nX^\\top y - X^\\top X\\beta = \\lambda s\n$$\nComponent-wise, let $c=X^\\top y$ and $H=X^\\top X$. The KKT conditions for each component $j \\in \\{1, 2, 3\\}$ are:\n1. If $\\beta_j \\neq 0$, then $(c - H\\beta)_j = \\lambda \\, \\text{sign}(\\beta_j)$. This implies the set of non-zero coefficients (the support or active set) determines a system of linear equations.\n2. If $\\beta_j = 0$, then $|(c - H\\beta)_j| \\le \\lambda$. This condition ensures that no coefficient currently outside the active set has a strong enough correlation with the residual to enter the model.\n\nFirst, we compute the required matrices for the given data:\n$X = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}$, $y = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}$.\n$$\nc = X^\\top y = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix}\n$$\n$$\nH = X^\\top X = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 2 \\end{pmatrix}\n$$\nWe now trace the LASSO solution path by starting with a very large $\\lambda$ and decreasing it.\n\n**Region 1: The first coefficient enters**\nFor a sufficiently large $\\lambda$, the $\\ell_1$ penalty dominates, forcing the solution to be $\\beta = (0, 0, 0)^\\top$. In this case, the KKT condition for inactive coefficients must hold for all $j$: $|(c - H\\cdot 0)_j| \\leq \\lambda$, which simplifies to $|c_j| \\le \\lambda$. The first coefficient will become non-zero when $\\lambda$ decreases to the point where this inequality is first violated. This occurs at:\n$$\n\\lambda_1 = \\max_{j} |c_j| = \\max\\{|1|, |\\frac{1}{2}|, |\\frac{3}{2}|\\} = \\frac{3}{2}\n$$\nThus, for all $\\lambda \\ge \\frac{3}{2}$, the solution is $\\beta=0$ and the support is the empty set $\\emptyset$. At $\\lambda_1 = \\frac{3}{2}$, the third coefficient, $\\beta_3$, is poised to enter the model. This is the first critical value of $\\lambda$.\n\n**Region 2: Support is $\\{3\\}$**\nFor $\\lambda < \\frac{3}{2}$, the active set is $\\mathcal{A}=\\{3\\}$, so $\\beta_1=0$ and $\\beta_2=0$. The sign of $\\beta_3$ will be the sign of the corresponding correlation, $\\text{sign}(c_3)=+1$. The KKT condition for the active coefficient $\\beta_3$ is:\n$$\nc_3 - (H_{31}\\beta_1 + H_{32}\\beta_2 + H_{33}\\beta_3) = \\lambda \\cdot \\text{sign}(\\beta_3)\n$$\n$$\n\\frac{3}{2} - (1 \\cdot 0 + 1 \\cdot 0 + 2 \\beta_3) = \\lambda \\cdot (+1) \\implies \\frac{3}{2} - 2\\beta_3 = \\lambda \\implies \\beta_3 = \\frac{\\frac{3}{2} - \\lambda}{2} = \\frac{3 - 2\\lambda}{4}\n$$\nThis solution is valid as long as the KKT conditions for the inactive coefficients $\\beta_1$ and $\\beta_2$ are met:\n- For $j=1$: $|c_1 - H_{13}\\beta_3| \\le \\lambda \\implies |1 - 1 \\cdot \\beta_3| \\le \\lambda$.\nSubstituting $\\beta_3$: $|1 - \\frac{3-2\\lambda}{4}| = |\\frac{4 - 3 + 2\\lambda}{4}| = |\\frac{1+2\\lambda}{4}|$. Since $\\lambda \\ge 0$, this becomes $\\frac{1+2\\lambda}{4} \\le \\lambda \\implies 1+2\\lambda \\le 4\\lambda \\implies 1 \\le 2\\lambda \\implies \\lambda \\ge \\frac{1}{2}$.\n- For $j=2$: $|c_2 - H_{23}\\beta_3| \\le \\lambda \\implies |\\frac{1}{2} - 1 \\cdot \\beta_3| \\le \\lambda$.\nSubstituting $\\beta_3$: $|\\frac{1}{2} - \\frac{3-2\\lambda}{4}| = |\\frac{2 - 3 + 2\\lambda}{4}| = |\\frac{2\\lambda-1}{4}|$. We need $|\\frac{2\\lambda-1}{4}| \\le \\lambda$.\nIf $\\lambda \\ge \\frac{1}{2}$, this is $\\frac{2\\lambda-1}{4} \\le \\lambda \\implies 2\\lambda-1 \\le 4\\lambda \\implies -1 \\le 2\\lambda \\implies \\lambda \\ge -\\frac{1}{2}$, which is true.\nIf $\\lambda < \\frac{1}{2}$, this is $\\frac{1-2\\lambda}{4} \\le \\lambda \\implies 1-2\\lambda \\le 4\\lambda \\implies 1 \\le 6\\lambda \\implies \\lambda \\ge \\frac{1}{6}$.\nThe path for support $\\{3\\}$ is valid as long as all conditions hold, i.e., $\\lambda \\ge \\frac{1}{2}$ and $\\lambda \\ge \\frac{1}{6}$. The binding constraint is $\\lambda \\ge \\frac{1}{2}$. A new event occurs when this constraint becomes an equality.\nThe next critical value is $\\lambda_2 = \\frac{1}{2}$, at which point coefficient $\\beta_1$ enters the model.\n\n**Region 3: Support is $\\{1, 3\\}$**\nFor $\\lambda < \\frac{1}{2}$, the active set is $\\mathcal{A}=\\{1, 3\\}$, so $\\beta_2=0$. The sign of $\\beta_3$ remains $+1$. The sign of the entering coefficient $\\beta_1$ is given by the sign of the correlation term at the moment of entry: $\\text{sign}(c_1-H_{13}\\beta_3)|_{\\lambda=1/2} = \\text{sign}(\\frac{1+2(1/2)}{4}) = \\text{sign}(\\frac{1}{2}) = +1$.\nThe KKT conditions for the active set are:\n$$\nc_1 - (H_{11}\\beta_1 + H_{13}\\beta_3) = \\lambda \\cdot (+1) \\implies 1 - (\\beta_1 + \\beta_3) = \\lambda\n$$\n$$\nc_3 - (H_{31}\\beta_1 + H_{33}\\beta_3) = \\lambda \\cdot (+1) \\implies \\frac{3}{2} - (\\beta_1 + 2\\beta_3) = \\lambda\n$$\nWe solve this $2 \\times 2$ system for $\\beta_1$ and $\\beta_3$:\n$$\n\\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_3 \\end{pmatrix} = \\begin{pmatrix} 1 - \\lambda \\\\ \\frac{3}{2} - \\lambda \\end{pmatrix}\n$$\nThe inverse of the matrix is $\\begin{pmatrix} 2 & -1 \\\\ -1 & 1 \\end{pmatrix}$.\n$$\n\\begin{pmatrix} \\beta_1 \\\\ \\beta_3 \\end{pmatrix} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 - \\lambda \\\\ \\frac{3}{2} - \\lambda \\end{pmatrix}\n$$\nSolving yields:\n$$\n\\beta_1 = 2(1-\\lambda) - (\\frac{3}{2} - \\lambda) = 2 - 2\\lambda - \\frac{3}{2} + \\lambda = \\frac{1}{2} - \\lambda\n$$\n$$\n\\beta_3 = -(1-\\lambda) + (\\frac{3}{2} - \\lambda) = -1 + \\lambda + \\frac{3}{2} - \\lambda = \\frac{1}{2}\n$$\nFor these solutions to be valid, we need to check the KKT condition for the inactive coefficient $\\beta_2=0$:\n$|c_2 - (H_{21}\\beta_1 + H_{23}\\beta_3)| \\le \\lambda$.\n$|\\frac{1}{2} - (0 \\cdot \\beta_1 + 1 \\cdot \\beta_3)| = |\\frac{1}{2} - \\beta_3| = |\\frac{1}{2} - \\frac{1}{2}| = |0| = 0$.\nThe condition is $0 \\le \\lambda$, which is true for all $\\lambda > 0$. Equality holds at $\\lambda=0$.\nAlso, for the support to be $\\{1,3\\}$, we require $\\beta_1$ and $\\beta_3$ to be non-zero. $\\beta_3=\\frac{1}{2}$ is always non-zero. $\\beta_1 = \\frac{1}{2}-\\lambda > 0$ requires $\\lambda < \\frac{1}{2}$.\nNo new coefficient enters and no active coefficient drops out of the model for any $\\lambda \\in (0, \\frac{1}{2})$. The support set remains $\\{1,3\\}$ throughout this interval. At $\\lambda=0$, the solution becomes $\\beta = (\\frac{1}{2}, 0, \\frac{1}{2})^\\top$, which also has support $\\{1,3\\}$.\nTherefore, the support only changes at a finite number of points for $\\lambda \\ge 0$.\n\nThe intervals of constant support are:\n- For $\\lambda \\in [\\frac{3}{2}, \\infty)$: Support is $\\emptyset$.\n- For $\\lambda \\in [\\frac{1}{2}, \\frac{3}{2})$: Support is $\\{3\\}$.\n- For $\\lambda \\in [0, \\frac{1}{2})$: Support is $\\{1, 3\\}$.\n\nThe support set changes when $\\lambda$ crosses the values $\\frac{3}{2}$ and $\\frac{1}{2}$. These are the finite critical values of $\\lambda$. The list of these values in descending order is $(\\frac{3}{2}, \\frac{1}{2})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{2} & \\frac{1}{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the KKT conditions define the LASSO solution, practical algorithms are needed to find it for larger problems. This practice delves into the mechanics of coordinate descent, a widely used and efficient algorithm for LASSO . You will derive the coordinate-wise update rule and, in doing so, uncover the crucial role of feature standardization, a pre-processing step that ensures the $\\ell_1$ penalty is applied equitably across all predictors.",
            "id": "3488572",
            "problem": "Consider a linear model with design matrix $X \\in \\mathbb{R}^{n \\times p}$ having columns $X_j \\in \\mathbb{R}^{n}$ for $j \\in \\{1,\\dots,p\\}$ and response vector $y \\in \\mathbb{R}^{n}$. The Least Absolute Shrinkage and Selection Operator (LASSO) estimates $\\beta \\in \\mathbb{R}^{p}$ by minimizing the convex objective\n$$\n\\frac{1}{2}\\,\\|y - X\\beta\\|_2^2 + \\lambda \\,\\|\\beta\\|_1,\n$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm, $\\|\\cdot\\|_1$ denotes the $\\ell_1$ norm, and $\\lambda > 0$ is the regularization parameter. In a cyclic coordinate descent scheme, each step updates one coordinate $\\beta_j$ while holding the others fixed. Starting only from the definition of the objective above, the definition of the Euclidean norm, and the subgradient of the absolute value function, derive the one-dimensional subproblem for a single coordinate update and solve for the update in terms of the residual that excludes the contribution of $X_j$. Then analyze the special case in which each feature is standardized so that $\\|X_j\\|_2^2 = n$ for all $j$, and contrast it with the case where columns are not standardized. Using your derivations and reasoning, select all statements below that are correct.\n\nThroughout, let $r^{(j)} = y - \\sum_{k \\neq j} X_k \\beta_k$ denote the partial residual excluding the contribution of the $j$-th feature, and let $S(u,t) = \\mathrm{sign}(u)\\,(|u| - t)_+$ denote the soft-thresholding operator.\n\nA. Under the condition $\\|X_j\\|_2^2 = n$ for all $j$, the coordinate-wise minimizer along coordinate $j$ can be written as\n$$\n\\beta_j \\leftarrow S\\!\\left(\\frac{1}{n}\\,X_j^\\top r^{(j)},\\,\\frac{\\lambda}{n}\\right),\n$$\nwhich shows that a single global $\\lambda$ acts as a uniform soft-threshold on normalized correlations $\\frac{1}{n}\\,X_j^\\top r^{(j)}$ across all features.\n\nB. Without standardization, a single fixed $\\lambda$ induces a uniform level of shrinkage across all coordinates because the $\\ell_1$ penalty $\\|\\beta\\|_1$ is scale-free with respect to the columns of $X$, making tuning $\\lambda$ equally interpretable for all features.\n\nC. Under the condition $\\|X_j\\|_2^2 = n$ for all $j$, the Karush–Kuhn–Tucker (KKT) stationarity condition for a zero coefficient on coordinate $j$ simplifies to\n$$\n\\left|X_j^\\top r^{(j)}\\right| \\le \\lambda,\n$$\nso testing whether $\\beta_j = 0$ reduces to checking whether the absolute unnormalized correlation between $X_j$ and $r^{(j)}$ is below the single threshold $\\lambda$.\n\nD. If columns are not standardized, features with larger $\\|X_j\\|_2$ require smaller changes in $\\beta_j$ to produce the same change in the linear predictor $X\\beta$, thereby incurring a smaller $\\ell_1$ penalty per unit predictive effect; for a fixed $\\lambda$, such features are preferentially selected relative to features with smaller $\\|X_j\\|_2$.\n\nE. With the standardization $\\|X_j\\|_2^2 = n$, choosing a single fixed $\\lambda$ yields the same expected sparsity level regardless of the sample size $n$, because the decision rule for setting coefficients to zero does not depend on $n$.\n\nSelect all correct options (one or more).",
            "solution": "The user has requested a detailed derivation and analysis of the coordinate descent update for the LASSO optimization problem, followed by an evaluation of several statements.\n\n### Problem Validation\n\nI will first validate the problem statement.\n\n**Step 1: Extract Givens**\n- **Model:** A linear model.\n- **Design Matrix:** $X \\in \\mathbb{R}^{n \\times p}$ with columns $X_j \\in \\mathbb{R}^{n}$ for $j \\in \\{1,\\dots,p\\}$.\n- **Response Vector:** $y \\in \\mathbb{R}^{n}$.\n- **Coefficient Vector:** $\\beta \\in \\mathbb{R}^{p}$.\n- **LASSO Objective Function:** $L(\\beta) = \\frac{1}{2}\\,\\|y - X\\beta\\|_2^2 + \\lambda \\,\\|\\beta\\|_1$.\n- **Norms:** $\\|\\cdot\\|_2$ is the Euclidean norm, $\\|\\cdot\\|_1$ is the $\\ell_1$ norm.\n- **Regularization Parameter:** $\\lambda > 0$.\n- **Algorithm:** Cyclic coordinate descent, updating one coordinate $\\beta_j$ at a time.\n- **Required Derivation:** Derive the one-dimensional subproblem and its solution for the $\\beta_j$ update.\n- **Special Case:** Analyze the case where features are standardized such that $\\|X_j\\|_2^2 = n$ for all $j$.\n- **Definitions:**\n    - Partial residual: $r^{(j)} = y - \\sum_{k \\neq j} X_k \\beta_k$.\n    - Soft-thresholding operator: $S(u,t) = \\mathrm{sign}(u)\\,(|u| - t)_+$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem describes the LASSO, a fundamental and widely used technique in high-dimensional statistics and machine learning. The objective function, definitions, and the use of coordinate descent are all standard and scientifically sound.\n- **Well-Posedness:** The problem asks for a specific derivation and analysis, which is a well-defined mathematical task. The LASSO objective function is convex, ensuring that a global minimum exists, which makes the optimization problem well-posed.\n- **Objectivity:** The problem is stated in precise mathematical language, free of ambiguity or subjective content.\n- **Flaw Checklist:**\n    1.  **Scientific Unsoundness:** None. The formulation is correct.\n    2.  **Non-Formalizable:** None. The problem is a standard mathematical derivation.\n    3.  **Incomplete/Contradictory:** None. All necessary information is provided.\n    4.  **Unrealistic:** None. The model and the standardization condition are common in practice.\n    5.  **Ill-Posed:** None. The derivation leads to a unique update rule.\n    6.  **Trivial/Tautological:** None. The derivation requires a proper application of subgradient calculus and is a non-trivial exercise.\n    7.  **Unverifiable:** None. The results are mathematically verifiable.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the derivation and evaluation.\n\n### Derivation of the Coordinate-wise Update\n\nThe LASSO objective function is:\n$$\nL(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1\n$$\nIn a coordinate descent step for the $j$-th coordinate, we fix all other coefficients $\\beta_k$ for $k \\neq j$ and minimize $L(\\beta)$ with respect to $\\beta_j$. We can rewrite the linear predictor $X\\beta$ as:\n$$\nX\\beta = \\sum_{k=1}^p X_k \\beta_k = X_j \\beta_j + \\sum_{k \\neq j} X_k \\beta_k\n$$\nSubstituting this into the objective function and grouping terms that do not depend on $\\beta_j$ into a constant, we get the one-dimensional objective for $\\beta_j$:\n$$\nL_j(\\beta_j) = \\frac{1}{2}\\left\\|y - \\sum_{k \\neq j} X_k \\beta_k - X_j \\beta_j\\right\\|_2^2 + \\lambda |\\beta_j| + \\lambda \\sum_{k \\neq j} |\\beta_k|\n$$\nUsing the definition of the partial residual $r^{(j)} = y - \\sum_{k \\neq j} X_k \\beta_k$, the objective to minimize with respect to $\\beta_j$ is:\n$$\n\\arg\\min_{\\beta_j} \\left( \\frac{1}{2}\\|r^{(j)} - X_j \\beta_j\\|_2^2 + \\lambda |\\beta_j| \\right)\n$$\nLet's expand the squared norm term:\n$$\n\\|r^{(j)} - X_j \\beta_j\\|_2^2 = (r^{(j)} - X_j \\beta_j)^\\top (r^{(j)} - X_j \\beta_j) = \\|r^{(j)}\\|_2^2 - 2\\beta_j X_j^\\top r^{(j)} + \\beta_j^2 \\|X_j\\|_2^2\n$$\nDropping the term $\\|r^{(j)}\\|_2^2$ which is constant with respect to $\\beta_j$, the minimization problem is equivalent to:\n$$\n\\arg\\min_{\\beta_j} \\left( \\frac{1}{2}\\beta_j^2 \\|X_j\\|_2^2 - \\beta_j X_j^\\top r^{(j)} + \\lambda |\\beta_j| \\right)\n$$\nThis is a convex function. We find the minimum by setting its subgradient with respect to $\\beta_j$ to $0$. The subgradient is:\n$$\n\\partial_{\\beta_j} L_j(\\beta_j) = \\beta_j \\|X_j\\|_2^2 - X_j^\\top r^{(j)} + \\lambda \\cdot \\partial |\\beta_j|\n$$\nwhere $\\partial |\\beta_j|$ is the subgradient of the absolute value function:\n$$\n\\partial |\\beta_j| = \\begin{cases} \\{\\mathrm{sign}(\\beta_j)\\} & \\text{if } \\beta_j \\neq 0 \\\\ [-1, 1] & \\text{if } \\beta_j = 0 \\end{cases}\n$$\nThe optimality condition is $0 \\in \\partial_{\\beta_j} L_j(\\beta_j)$. We analyze three cases for the optimal $\\beta_j$:\n\n**Case 1: $\\beta_j > 0$**\nThe subgradient is single-valued: $\\partial |\\beta_j| = \\{1\\}$. The optimality condition becomes:\n$$\n\\beta_j \\|X_j\\|_2^2 - X_j^\\top r^{(j)} + \\lambda = 0 \\implies \\beta_j = \\frac{X_j^\\top r^{(j)} - \\lambda}{\\|X_j\\|_2^2}\n$$\nFor this solution to be consistent with the assumption $\\beta_j > 0$, we must have $X_j^\\top r^{(j)} > \\lambda$.\n\n**Case 2: $\\beta_j < 0$**\nThe subgradient is single-valued: $\\partial |\\beta_j| = \\{-1\\}$. The optimality condition becomes:\n$$\n\\beta_j \\|X_j\\|_2^2 - X_j^\\top r^{(j)} - \\lambda = 0 \\implies \\beta_j = \\frac{X_j^\\top r^{(j)} + \\lambda}{\\|X_j\\|_2^2}\n$$\nFor this solution to be consistent with $\\beta_j < 0$, we must have $X_j^\\top r^{(j)} < -\\lambda$.\n\n**Case 3: $\\beta_j = 0$**\nThe subgradient is the interval $[-1, 1]$. The optimality condition is:\n$$\n0 \\in 0 \\cdot \\|X_j\\|_2^2 - X_j^\\top r^{(j)} + \\lambda [-1, 1] \\implies 0 \\in -X_j^\\top r^{(j)} + [-\\lambda, \\lambda]\n$$\nThis means $X_j^\\top r^{(j)} \\in [-\\lambda, \\lambda]$, which is equivalent to $|X_j^\\top r^{(j)}| \\le \\lambda$.\n\nCombining these three cases, we can write the solution compactly using the soft-thresholding operator $S(u,t) = \\mathrm{sign}(u)(|u|-t)_+$. Let $u = \\frac{X_j^\\top r^{(j)}}{\\|X_j\\|_2^2}$ and $t = \\frac{\\lambda}{\\|X_j\\|_2^2}$. Then the solution is:\n$$\n\\hat{\\beta}_j = S\\left(\\frac{X_j^\\top r^{(j)}}{\\|X_j\\|_2^2}, \\frac{\\lambda}{\\|X_j\\|_2^2}\\right)\n$$\nThis is the general coordinate-wise update rule.\n\nNow, we analyze the special case with standardization $\\|X_j\\|_2^2 = n$ for all $j=1, \\dots, p$. The update rule simplifies to:\n$$\n\\hat{\\beta}_j = S\\left(\\frac{X_j^\\top r^{(j)}}{n}, \\frac{\\lambda}{n}\\right)\n$$\n\n### Evaluation of Options\n\n**A. Under the condition $\\|X_j\\|_2^2 = n$ for all $j$, the coordinate-wise minimizer along coordinate $j$ can be written as $\\beta_j \\leftarrow S\\!\\left(\\frac{1}{n}\\,X_j^\\top r^{(j)},\\,\\frac{\\lambda}{n}\\right)$, which shows that a single global $\\lambda$ acts as a uniform soft-threshold on normalized correlations $\\frac{1}{n}\\,X_j^\\top r^{(j)}$ across all features.**\n\nOur derivation for the standardized case confirms the update formula $\\beta_j \\leftarrow S\\left(\\frac{X_j^\\top r^{(j)}}{n}, \\frac{\\lambda}{n}\\right)$. The term $\\frac{1}{n}X_j^\\top r^{(j)}$ is the sample covariance between feature $j$ and the partial residual. Because $\\|X_j\\|_2^2 = n$, the feature $X_j$ is standardized to have a root-mean-square value of $1$. The quantity is therefore a normalized measure of correlation. The soft-thresholding is applied to this quantity with a threshold of $\\frac{\\lambda}{n}$. Since $n$ and $\\lambda$ are the same for all features, this threshold value is uniform across all coordinates $j$. This standardization ensures that the penalty is applied equitably to all features, as their \"predictive potential\" (the correlation term) is measured on a common scale before being thresholded. The statement accurately describes this situation.\n\n**Verdict: Correct**\n\n**B. Without standardization, a single fixed $\\lambda$ induces a uniform level of shrinkage across all coordinates because the $\\ell_1$ penalty $\\|\\beta\\|_1$ is scale-free with respect to the columns of $X$, making tuning $\\lambda$ equally interpretable for all features.**\n\nFrom our general derivation, the update rule without standardization is $\\beta_j \\leftarrow S\\left(\\frac{X_j^\\top r^{(j)}}{\\|X_j\\|_2^2}, \\frac{\\lambda}{\\|X_j\\|_2^2}\\right)$. The effective threshold applied to the normalized correlation $\\frac{X_j^\\top r^{(j)}}{\\|X_j\\|_2^2}$ is $\\frac{\\lambda}{\\|X_j\\|_2^2}$. This threshold explicitly depends on the norm of the column $X_j$. If the columns have different norms, the shrinkage will not be uniform. Columns with larger norms will have a smaller effective threshold and will be penalized less, and vice-versa.\nFurthermore, the claim that the $\\ell_1$ penalty is \"scale-free\" is incorrect. If we scale a column $X_j$ by a factor $c > 0$ to get $X'_j = cX_j$, to keep the model's prediction $X_j\\beta_j$ unchanged, the coefficient must be rescaled to $\\beta'_j = \\beta_j/c$. The penalty term for this coefficient changes from $\\lambda|\\beta_j|$ to $\\lambda|\\beta_j/c|$. This demonstrates that the LASSO solution is sensitive to the scale of the predictors. Therefore, every premise in this statement is false.\n\n**Verdict: Incorrect**\n\n**C. Under the condition $\\|X_j\\|_2^2 = n$ for all $j$, the Karush–Kuhn–Tucker (KKT) stationarity condition for a zero coefficient on coordinate $j$ simplifies to $|X_j^\\top r^{(j)}| \\le \\lambda$, so testing whether $\\beta_j = 0$ reduces to checking whether the absolute unnormalized correlation between $X_j$ and $r^{(j)}$ is below the single threshold $\\lambda$.**\n\nFrom our derivation (Case 3), the condition for $\\beta_j=0$ to be the coordinate-wise minimizer is $|X_j^\\top r^{(j)}| \\le \\lambda$. This is a direct consequence of the subgradient optimality condition and holds for the general LASSO problem, irrespective of whether the columns of $X$ are standardized. At the global minimum, this condition must hold for all $j$ such that $\\hat{\\beta}_j = 0$. The quantity $X_j^\\top r^{(j)}$ is the dot product (unnormalized covariance or \"unnormalized correlation\") between the feature vector $X_j$ and the partial residual. The condition correctly states that the magnitude of this quantity must be less than or equal to the single, global regularization parameter $\\lambda$. While the standardization condition $\\|X_j\\|_2^2 = n$ is mentioned, the KKT condition itself is general. However, the statement's conclusion and the formula are correct. The condition provides a clear rule for when a feature is excluded from the model.\n\n**Verdict: Correct**\n\n**D. If columns are not standardized, features with larger $\\|X_j\\|_2$ require smaller changes in $\\beta_j$ to produce the same change in the linear predictor $X\\beta$, thereby incurring a smaller $\\ell_1$ penalty per unit predictive effect; for a fixed $\\lambda$, such features are preferentially selected relative to features with smaller $\\|X_j\\|_2$.**\n\nThis statement accurately describes the effect of feature scaling on LASSO. Let's analyze it piece by piece.\n1.  A change $\\Delta \\beta_j$ in a coefficient results in a change $X_j \\Delta \\beta_j$ in the predictor. The magnitude of this change is $\\|\\Delta(X\\beta)\\|_2 = |\\Delta \\beta_j|\\,\\|X_j\\|_2$. To achieve a fixed magnitude of change $C$, we need $|\\Delta \\beta_j| = C/\\|X_j\\|_2$. Thus, if $\\|X_j\\|_2$ is larger, a smaller $|\\Delta \\beta_j|$ is needed. This is correct.\n2.  The $\\ell_1$ penalty associated with the coefficient $\\beta_j$ is $\\lambda |\\beta_j|$. A smaller coefficient magnitude $|\\beta_j|$ results in a smaller penalty.\n3.  Combining these points: for a given desired \"predictive effect\" (change in $X\\beta$), a feature $X_j$ with a larger norm $\\|X_j\\|_2$ can achieve it with a smaller coefficient $\\beta_j$, thereby incurring a smaller $\\ell_1$ penalty.\n4.  Consequently, the LASSO optimization, which balances predictive fit against the $\\ell_1$ penalty, will preferentially shrink coefficients of features with small norms towards zero and retain features with large norms. This statement correctly identifies this bias.\n\n**Verdict: Correct**\n\n**E. With the standardization $\\|X_j\\|_2^2 = n$, choosing a single fixed $\\lambda$ yields the same expected sparsity level regardless of the sample size $n$, because the decision rule for setting coefficients to zero does not depend on $n$.**\n\nThe decision rule to set a coefficient to zero, derived from the coordinate-wise update, is to check if $|\\frac{1}{n}X_j^\\top r^{(j)}| \\le \\frac{\\lambda}{n}$, which is equivalent to $|X_j^\\top r^{(j)}| \\le \\lambda$. Let's consider the statistical behavior of the term $X_j^\\top r^{(j)} = \\sum_{i=1}^n X_{ij} r_i^{(j)}$ as the sample size $n$ increases. Assuming the data points are drawn i.i.d., this sum's magnitude generally grows with $n$. For a feature that is truly correlated with the response, this sum will typically grow at a rate of $O(n)$, while for an uncorrelated feature it would grow at a rate of $O(\\sqrt{n})$ by the Central Limit Theorem. If $\\lambda$ is held fixed, as $n$ increases, the value of $|X_j^\\top r^{(j)}|$ will eventually exceed the fixed threshold $\\lambda$ for more and more features. This means the model will become denser (less sparse) as $n$ increases for a fixed $\\lambda$. Therefore, a fixed $\\lambda$ does not yield the same expected sparsity level. To maintain a certain sparsity or achieve desirable statistical properties like selection consistency, $\\lambda$ must be chosen as a function of $n$ (e.g., $\\lambda \\sim \\sqrt{n \\log p}$). The premise of the statement is false.\n\n**Verdict: Incorrect**\n\nFinal correct options are A, C, and D.",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "The behavior of LASSO becomes particularly interesting in the presence of highly correlated features, a common occurrence in real-world data. This final exercise explores a challenging scenario where predictor collinearity leads to non-unique solutions, revealing a key subtlety of the $\\ell_1$ penalty . By implementing different tie-breaking strategies, you will see firsthand how algorithmic choices can determine which correlated features are selected, even when the overall model fit remains identical.",
            "id": "3488577",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) regression problem with a fixed design matrix and no intercept. The estimator $\\hat{\\boldsymbol{\\beta}}(\\lambda)$ solves the convex optimization problem\n$$\n\\min_{\\boldsymbol{\\beta}\\in\\mathbb{R}^p} \\ \\frac{1}{2}\\left\\|\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\beta}\\right\\|_2^2 + \\lambda \\left\\|\\boldsymbol{\\beta}\\right\\|_1,\n$$\nwhere $\\boldsymbol{y}\\in\\mathbb{R}^n$ is the response, $\\mathbf{X}\\in\\mathbb{R}^{n\\times p}$ is the predictor matrix, $\\lambda\\ge 0$ is the regularization parameter, $\\left\\|\\cdot\\right\\|_2$ denotes the Euclidean norm, and $\\left\\|\\cdot\\right\\|_1$ denotes the $\\ell_1$ norm. The Karush-Kuhn-Tucker (KKT) conditions for optimality state that at an optimal solution $\\hat{\\boldsymbol{\\beta}}$ with residual $\\hat{\\boldsymbol{r}}=\\boldsymbol{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}}$, there exists a subgradient vector $\\boldsymbol{z}\\in\\mathbb{R}^p$ satisfying\n$$\n\\mathbf{X}^\\top \\hat{\\boldsymbol{r}} = \\lambda \\boldsymbol{z}, \\quad \\text{where} \\quad z_j\\in\n\\begin{cases}\n\\{\\operatorname{sign}(\\hat{\\beta}_j)\\} & \\text{if } \\hat{\\beta}_j\\ne 0,\\\\\n[-1,1] & \\text{if } \\hat{\\beta}_j=0.\n\\end{cases}\n$$\nThis subgradient is set-valued when $\\hat{\\beta}_j=0$, and ties in the absolute correlations $|\\mathbf{X}_j^\\top \\hat{\\boldsymbol{r}}|$ can induce non-uniqueness in the subgradient and, depending on $\\mathbf{X}$, possibly in the coefficient vector itself.\n\nConstruct a scenario with exact ties in correlations by letting $\\mathbf{X}$ have two identical columns. Let $n=5$ and define the vector\n$$\n\\boldsymbol{v}=\\begin{bmatrix}1\\\\2\\\\-1\\\\0\\\\1\\end{bmatrix}, \\quad \\text{and} \\quad \\mathbf{X}=\\begin{bmatrix}\\boldsymbol{v} & \\boldsymbol{v}\\end{bmatrix}\\in\\mathbb{R}^{5\\times 2}.\n$$\nFor any $\\boldsymbol{y}$, both predictors $\\mathbf{X}_1$ and $\\mathbf{X}_2$ have equal correlation with any residual $\\boldsymbol{r}$ because $\\mathbf{X}_1=\\mathbf{X}_2=\\boldsymbol{v}$. In this duplicated-column setting, the LASSO objective can be reduced to a one-dimensional problem in the aggregate coefficient $s=\\beta_1+\\beta_2$:\n$$\n\\min_{s\\in\\mathbb{R}} \\ \\frac{1}{2}\\left\\|\\boldsymbol{y}-\\boldsymbol{v}\\,s\\right\\|_2^2 + \\lambda |s|.\n$$\nThe fit depends only on $s$, while the decomposition of $s$ into $(\\beta_1,\\beta_2)$ is not uniquely determined when $s\\ne 0$ because many pairs $(\\beta_1,\\beta_2)$ share the same $s=\\beta_1+\\beta_2$ and yield the same objective value. This creates a non-unique subgradient assignment across the tied predictors.\n\nYou must implement two selection rules that resolve ties differently and trace their effects on the resulting coefficient path, even though the aggregate fit is the same:\n- Least Angle Regression (LARS) style equal-sharing rule: when $s\\ne 0$, assign $\\beta_1=\\beta_2=s/2$ so that tied predictors enter and evolve symmetrically.\n- Deterministic tie-breaking rule: when $s\\ne 0$, assign all magnitude to the first predictor, i.e., $\\beta_1=s$, $\\beta_2=0$.\n\nFrom first principles, use the KKT conditions specialized to this duplicated-column model to derive the computation of the optimal aggregate coefficient $s^\\star(\\lambda)$ and then map it to $(\\beta_1,\\beta_2)$ under each selection rule. For each case, also compute the residual correlation value $c_{\\text{res}}=\\boldsymbol{v}^\\top(\\boldsymbol{y}-\\boldsymbol{v}s^\\star)$, which is the common correlation of both tied predictors with the residual at the computed solution.\n\nImplement a program that, for the following test suite, computes and outputs for each test case the list\n$$\n\\left[\\beta_1^{\\text{equal}},\\ \\beta_2^{\\text{equal}},\\ \\beta_1^{\\text{tie}},\\ \\beta_2^{\\text{tie}},\\ s^\\star,\\ c_{\\text{res}}\\right],\n$$\nwhere $(\\beta_1^{\\text{equal}},\\beta_2^{\\text{equal}})$ are the coefficients under the LARS equal-sharing rule, $(\\beta_1^{\\text{tie}},\\beta_2^{\\text{tie}})$ are the coefficients under the deterministic tie-breaking rule, $s^\\star$ is the optimal aggregate coefficient, and $c_{\\text{res}}$ is the residual correlation value.\n\nUse the following test suite to assess different regimes:\n- Case $1$ (happy path, nonzero solution): $\\boldsymbol{y}=3\\boldsymbol{v}$ and $\\lambda=0.5\\cdot |\\boldsymbol{v}^\\top \\boldsymbol{y}|$.\n- Case $2$ (boundary condition, threshold): $\\boldsymbol{y}=3\\boldsymbol{v}$ and $\\lambda=|\\boldsymbol{v}^\\top \\boldsymbol{y}|$.\n- Case $3$ (strong regularization, zero solution): $\\boldsymbol{y}=3\\boldsymbol{v}$ and $\\lambda=1.5\\cdot |\\boldsymbol{v}^\\top \\boldsymbol{y}|$.\n- Case $4$ (sign flip, nonzero solution with negative correlation): $\\boldsymbol{y}=-3\\boldsymbol{v}$ and $\\lambda=0.5\\cdot |\\boldsymbol{v}^\\top \\boldsymbol{y}|$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each element being the case-specific list specified above, in the order of cases $1$ through $4$. For example, the output format must be\n$$\n\\left[\\left[\\cdots\\right],\\left[\\cdots\\right],\\left[\\cdots\\right],\\left[\\cdots\\right]\\right].\n$$\nAll quantities are dimensionless real numbers; no physical units are involved. The output values must be computed exactly by your program; do not hard-code the results.",
            "solution": "The problem is valid as it presents a well-posed and scientifically grounded mathematical exercise in the context of LASSO regression, a standard method in statistics and machine learning. All necessary information is provided, and the setup is internally consistent and free of ambiguity.\n\nThe task is to analyze the LASSO solution for a specific case where the predictor matrix $\\mathbf{X} \\in \\mathbb{R}^{5\\times 2}$ has two identical columns, $\\mathbf{X}_1 = \\mathbf{X}_2 = \\boldsymbol{v}$, where $\\boldsymbol{v}=\\begin{bmatrix}1 & 2 & -1 & 0 & 1\\end{bmatrix}^\\top$. The LASSO problem is\n$$\n\\min_{\\beta_1,\\beta_2\\in\\mathbb{R}} \\ \\frac{1}{2}\\left\\|\\boldsymbol{y}-\\mathbf{X}_1\\beta_1 - \\mathbf{X}_2\\beta_2\\right\\|_2^2 + \\lambda \\left(|\\beta_1| + |\\beta_2|\\right).\n$$\nSubstituting $\\mathbf{X}_1 = \\mathbf{X}_2 = \\boldsymbol{v}$ and defining the aggregate coefficient $s = \\beta_1+\\beta_2$, the objective function becomes\n$$\n\\frac{1}{2}\\left\\|\\boldsymbol{y}-\\boldsymbol{v}s\\right\\|_2^2 + \\lambda (|\\beta_1| + |\\beta_2|).\n$$\nFor a fixed sum $s$, the term $|\\beta_1| + |\\beta_2|$ is minimized subject to $\\beta_1+\\beta_2=s$ when its value is $|s|$. This is achieved when $\\beta_1$ and $\\beta_2$ have the same sign (or one or both are zero). Thus, the LASSO problem is equivalent to first solving for the optimal aggregate coefficient $s^\\star$:\n$$\ns^\\star = \\arg\\min_{s\\in\\mathbb{R}} \\ \\frac{1}{2}\\left\\|\\boldsymbol{y}-\\boldsymbol{v}s\\right\\|_2^2 + \\lambda |s|.\n$$\nOnce $s^\\star$ is found, the individual coefficients $(\\beta_1, \\beta_2)$ are determined by the specific tie-breaking rule, while ensuring $\\beta_1+\\beta_2=s^\\star$ and $|\\beta_1|+|\\beta_2|=|s^\\star|$.\n\nWe derive the solution for $s^\\star$ from the Karush-Kuhn-Tucker (KKT) conditions of the original $2$-dimensional problem. The KKT conditions state that for an optimal solution $\\hat{\\boldsymbol{\\beta}}=[\\hat{\\beta}_1, \\hat{\\beta}_2]^\\top$, the subgradient of the objective at $\\hat{\\boldsymbol{\\beta}}$ must contain zero. The gradient of the least-squares term is $\\mathbf{X}^\\top(\\mathbf{X}\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{y})$. The subgradient of the $\\ell_1$ penalty is $\\lambda\\boldsymbol{z}$, where $z_j \\in \\partial |\\beta_j|$. Thus, the KKT stationarity condition is:\n$$\n\\mathbf{X}^\\top(\\boldsymbol{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = \\lambda\\boldsymbol{z}.\n$$\nGiven $\\mathbf{X}=[\\boldsymbol{v}, \\boldsymbol{v}]$ and $\\mathbf{X}\\hat{\\boldsymbol{\\beta}}=\\boldsymbol{v}(\\hat{\\beta}_1+\\hat{\\beta}_2)=\\boldsymbol{v}s$, the condition becomes:\n$$\n\\begin{bmatrix} \\boldsymbol{v}^\\top \\\\ \\boldsymbol{v}^\\top \\end{bmatrix}(\\boldsymbol{y}-\\boldsymbol{v}s) = \\lambda \\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix}.\n$$\nThis yields two identical equations for the residual correlation $c_{\\text{res}} = \\boldsymbol{v}^\\top(\\boldsymbol{y}-\\boldsymbol{v}s) = \\boldsymbol{v}^\\top\\boldsymbol{y} - s(\\boldsymbol{v}^\\top\\boldsymbol{v})$:\n$$\nc_{\\text{res}} = \\lambda z_1 \\quad \\text{and} \\quad c_{\\text{res}} = \\lambda z_2.\n$$\nThis implies that $z_1=z_2$, which we denote by $z$. The subgradient components must be equal. The condition on $s^\\star$ is therefore\n$$\n\\boldsymbol{v}^\\top\\boldsymbol{y} - s^\\star(\\boldsymbol{v}^\\top\\boldsymbol{v}) = \\lambda z,\n$$\nwhere $z$ must be compatible with the signs of the optimal coefficients $\\hat{\\beta}_1, \\hat{\\beta}_2$. Since $|\\hat{\\beta}_1|+|\\hat{\\beta}_2|=|s^\\star|$, $\\hat{\\beta}_1$ and $\\hat{\\beta}_2$ must have the same sign (or be zero). Therefore, if any $\\hat{\\beta}_j$ is non-zero, they will have the same sign as $s^\\star$. This leads to $z_j = \\operatorname{sign}(s^\\star)$ for any non-zero $\\hat{\\beta}_j$, so $z=\\operatorname{sign}(s^\\star)$.\n\nWe analyze three cases for $s^\\star$:\n1.  **$s^\\star > 0$**: The condition implies $\\hat{\\beta}_1, \\hat{\\beta}_2 \\ge 0$ with at least one being positive. Thus, $z_1=z_2=1$. The KKT equation becomes $\\boldsymbol{v}^\\top\\boldsymbol{y} - s^\\star(\\boldsymbol{v}^\\top\\boldsymbol{v}) = \\lambda$. Solving for $s^\\star$:\n    $$\n    s^\\star = \\frac{\\boldsymbol{v}^\\top\\boldsymbol{y} - \\lambda}{\\boldsymbol{v}^\\top\\boldsymbol{v}}.\n    $$\n    This is consistent only if $s^\\star > 0$, which requires $\\boldsymbol{v}^\\top\\boldsymbol{y} > \\lambda$.\n\n2.  **$s^\\star < 0$**: The condition implies $\\hat{\\beta}_1, \\hat{\\beta}_2 \\le 0$ with at least one being negative. Thus, $z_1=z_2=-1$. The KKT equation becomes $\\boldsymbol{v}^\\top\\boldsymbol{y} - s^\\star(\\boldsymbol{v}^\\top\\boldsymbol{v}) = -\\lambda$. Solving for $s^\\star$:\n    $$\n    s^\\star = \\frac{\\boldsymbol{v}^\\top\\boldsymbol{y} + \\lambda}{\\boldsymbol{v}^\\top\\boldsymbol{v}}.\n    $$\n    This is consistent only if $s^\\star < 0$, which requires $\\boldsymbol{v}^\\top\\boldsymbol{y} < -\\lambda$.\n\n3.  **$s^\\star = 0$**: This implies $\\hat{\\beta}_1=\\hat{\\beta}_2=0$. The subgradient components $z_1, z_2$ can be any value in $[-1,1]$. With $s^\\star=0$, the KKT equation is $\\boldsymbol{v}^\\top\\boldsymbol{y} = \\lambda z$. This requires $z=\\frac{\\boldsymbol{v}^\\top\\boldsymbol{y}}{\\lambda}$ (assuming $\\lambda>0$). Consistency requires $z \\in [-1,1]$, which is equivalent to $|\\boldsymbol{v}^\\top\\boldsymbol{y}| \\le \\lambda$.\n\nCombining these cases gives the solution for $s^\\star$ via the soft-thresholding operator:\n$$\ns^\\star = \\frac{S_\\lambda(\\boldsymbol{v}^\\top\\boldsymbol{y})}{\\boldsymbol{v}^\\top\\boldsymbol{v}}, \\quad \\text{where} \\quad S_\\lambda(c) = \\operatorname{sign}(c)\\max(|c|-\\lambda, 0).\n$$\nOnce $s^\\star$ is computed, the coefficients $(\\hat{\\beta}_1, \\hat{\\beta}_2)$ are assigned based on the two specified rules:\n- **LARS equal-sharing rule**: $\\beta_1^{\\text{equal}} = s^\\star/2$, $\\beta_2^{\\text{equal}} = s^\\star/2$.\n- **Deterministic tie-breaking rule**: $\\beta_1^{\\text{tie}} = s^\\star$, $\\beta_2^{\\text{tie}} = 0$.\n\nThe residual correlation is computed as $c_{\\text{res}} = \\boldsymbol{v}^\\top\\boldsymbol{y} - s^\\star(\\boldsymbol{v}^\\top\\boldsymbol{v})$.\n\nCalculations for the test cases proceed as follows. First, we compute $\\boldsymbol{v}^\\top\\boldsymbol{v} = 1^2+2^2+(-1)^2+0^2+1^2 = 7$.\n\n- **Case 1**: $\\boldsymbol{y}=3\\boldsymbol{v}$, $\\lambda=0.5|\\boldsymbol{v}^\\top\\boldsymbol{y}|$.\n  $\\boldsymbol{v}^\\top\\boldsymbol{y} = 3(\\boldsymbol{v}^\\top\\boldsymbol{v}) = 3(7) = 21$.\n  $\\lambda = 0.5|21| = 10.5$.\n  Since $\\boldsymbol{v}^\\top\\boldsymbol{y}=21 > \\lambda=10.5$, we have $s^\\star = (21-10.5)/7 = 1.5$.\n  $\\beta^{\\text{equal}}=(0.75, 0.75)$, $\\beta^{\\text{tie}}=(1.5, 0)$.\n  $c_{\\text{res}} = 21 - 1.5(7) = 10.5$.\n  Result: $[0.75, 0.75, 1.5, 0.0, 1.5, 10.5]$.\n\n- **Case 2**: $\\boldsymbol{y}=3\\boldsymbol{v}$, $\\lambda=|\\boldsymbol{v}^\\top\\boldsymbol{y}|$.\n  $\\boldsymbol{v}^\\top\\boldsymbol{y} = 21$, $\\lambda = 21$.\n  Since $|\\boldsymbol{v}^\\top\\boldsymbol{y}| = \\lambda$, we have $s^\\star=0$.\n  $\\beta^{\\text{equal}}=(0, 0)$, $\\beta^{\\text{tie}}=(0, 0)$.\n  $c_{\\text{res}} = 21 - 0(7) = 21$.\n  Result: $[0.0, 0.0, 0.0, 0.0, 0.0, 21.0]$.\n\n- **Case 3**: $\\boldsymbol{y}=3\\boldsymbol{v}$, $\\lambda=1.5|\\boldsymbol{v}^\\top\\boldsymbol{y}|$.\n  $\\boldsymbol{v}^\\top\\boldsymbol{y} = 21$, $\\lambda = 1.5(21) = 31.5$.\n  Since $|\\boldsymbol{v}^\\top\\boldsymbol{y}| < \\lambda$, we have $s^\\star=0$.\n  $\\beta^{\\text{equal}}=(0, 0)$, $\\beta^{\\text{tie}}=(0, 0)$.\n  $c_{\\text{res}} = 21 - 0(7) = 21$.\n  Result: $[0.0, 0.0, 0.0, 0.0, 0.0, 21.0]$.\n\n- **Case 4**: $\\boldsymbol{y}=-3\\boldsymbol{v}$, $\\lambda=0.5|\\boldsymbol{v}^\\top\\boldsymbol{y}|$.\n  $\\boldsymbol{v}^\\top\\boldsymbol{y} = -3(\\boldsymbol{v}^\\top\\boldsymbol{v}) = -21$.\n  $\\lambda = 0.5|-21| = 10.5$.\n  Since $\\boldsymbol{v}^\\top\\boldsymbol{y}=-21 < -\\lambda=-10.5$, we have $s^\\star = (-21+10.5)/7 = -1.5$.\n  $\\beta^{\\text{equal}}=(-0.75, -0.75)$, $\\beta^{\\text{tie}}=(-1.5, 0)$.\n  $c_{\\text{res}} = -21 - (-1.5)(7) = -10.5$.\n  Result: $[-0.75, -0.75, -1.5, 0.0, -1.5, -10.5]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes LASSO solutions for a duplicated-column model under two tie-breaking rules.\n    \"\"\"\n    # Define vector v as per the problem statement.\n    v = np.array([1, 2, -1, 0, 1])\n\n    # Pre-compute the squared L2 norm of v.\n    v_dot_v = float(np.dot(v, v))\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (y_factor, lambda_factor).\n    # y = y_factor * v\n    # lambda = lambda_factor * |v.T @ y|\n    test_cases = [\n        (3.0, 0.5),    # Case 1\n        (3.0, 1.0),    # Case 2\n        (3.0, 1.5),    # Case 3\n        (-3.0, 0.5),   # Case 4\n    ]\n\n    results = []\n    for y_factor, lambda_factor in test_cases:\n        # Calculate y and the correlation term v.T @ y.\n        # y = y_factor * v # y is not explicitly needed\n        v_dot_y = y_factor * v_dot_v\n\n        # Calculate the regularization parameter lambda.\n        lda = lambda_factor * abs(v_dot_y)\n\n        # Calculate the optimal aggregate coefficient s_star using the soft-thresholding formula.\n        s_star = 0.0\n        if v_dot_y > lda:\n            s_star = (v_dot_y - lda) / v_dot_v\n        elif v_dot_y < -lda:\n            s_star = (v_dot_y + lda) / v_dot_v\n        \n        # Rule 1: LARS equal-sharing rule.\n        b1_equal = s_star / 2.0\n        b2_equal = s_star / 2.0\n\n        # Rule 2: Deterministic tie-breaking rule.\n        b1_tie = s_star\n        b2_tie = 0.0\n        \n        # Compute the residual correlation c_res = v.T @ (y - v * s_star)\n        c_res = v_dot_y - s_star * v_dot_v\n        \n        # Store the results for the current case.\n        case_result = [b1_equal, b2_equal, b1_tie, b2_tie, s_star, c_res]\n        results.append(case_result)\n\n    # Format the final output string as a list of lists.\n    # e.g., [[val1,val2,...],[val1,val2,...],...]\n    output_str = f\"[{','.join([f'[{\",\".join(map(str, r))}]' for r in results])}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}