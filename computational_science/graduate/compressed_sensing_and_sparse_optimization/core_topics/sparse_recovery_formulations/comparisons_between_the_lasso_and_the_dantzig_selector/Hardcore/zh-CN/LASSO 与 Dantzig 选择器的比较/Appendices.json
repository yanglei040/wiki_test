{
    "hands_on_practices": [
        {
            "introduction": "为了深入理解 LASSO 和 Dantzig 选择算子的内在机制，我们首先从一个理想化的场景入手：正交设计。在此设定下，两种估计器都存在优美的闭式解，使得直接比较成为可能。通过推导这些解，您将亲手揭示它们与软阈值算子的深刻联系，并理解它们的正则化参数如何相互对应。",
            "id": "3435559",
            "problem": "考虑线性模型 $y = X \\beta^{\\star} + w$，其中 $X \\in \\mathbb{R}^{n \\times p}$ 的列是标准正交的，即 $X^{\\top} X = I_{p}$。在压缩感知和稀疏优化背景下，您需要比较 $\\beta^{\\star}$ 的两个凸估计量：最小绝对收缩和选择算子 (LASSO) 估计量和丹齐格选择器 (Dantzig selector)。LASSO 估计量 $\\hat{\\beta}_{\\mathrm{LASSO}}$ 定义为目标函数\n$$\\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},$$\n的最小化子，其中 $\\lambda > 0$ 是一个正则化参数。丹齐格选择器 $\\hat{\\beta}_{\\mathrm{DS}}$ 定义为在可行性约束\n$$\\|X^{\\top}(y - X \\beta)\\|_{\\infty} \\leq t,$$\n下最小化 $\\|\\beta\\|_{1}$ 的解，其中 $t > 0$ 是一个调节参数。\n\n从 LASSO 的凸优化最优性条件和丹齐格选择器的可行性几何出发，在标准正交设计条件 $X^{\\top} X = I_{p}$ 下，推导 $\\hat{\\beta}_{\\mathrm{LASSO}}$ 和 $\\hat{\\beta}_{\\mathrm{DS}}$ 的闭式表达式。然后，对于 $p=5$ 的特定实例，给定充分统计量 $X^{\\top} y = z$ 为\n$$z = \\begin{pmatrix} 2.4 \\\\ -0.3 \\\\ 1.1 \\\\ -1.8 \\\\ 0.0 \\end{pmatrix},$$\n以及参数 $\\lambda = 0.9$ 和 $t = 0.7$，计算这两个估计量之间的平方 $\\ell_{2}$ 距离，\n$$\\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2}.$$\n\n将您的最终数值答案四舍五入到四位有效数字。您的最终答案不带单位。",
            "solution": "问题要求在标准正交设计矩阵的特定条件下，计算 LASSO 和丹齐格选择器估计量之间的平方 $\\ell_2$ 距离。我们首先在该条件下推导这两种估计量的闭式解。\n\n令充分统计量定义为 $z = X^{\\top}y$。标准正交设计条件为 $X^{\\top}X = I_p$，其中 $I_p$ 是 $p \\times p$ 的单位矩阵。\n\n首先，我们分析 LASSO 估计量 $\\hat{\\beta}_{\\mathrm{LASSO}}$，它是以下最小化问题的解：\n$$ \\hat{\\beta}_{\\mathrm{LASSO}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\left( \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right) $$\n目标函数中的二次项可以展开为：\n$$ \\|y - X \\beta\\|_{2}^{2} = (y - X \\beta)^{\\top}(y - X \\beta) = y^{\\top}y - 2y^{\\top}X\\beta + \\beta^{\\top}X^{\\top}X\\beta $$\n使用条件 $X^{\\top}X = I_p$ 和定义 $z = X^{\\top}y$，上式可简化为：\n$$ \\|y - X \\beta\\|_{2}^{2} = y^{\\top}y - 2z^{\\top}\\beta + \\beta^{\\top}\\beta $$\n项 $y^{\\top}y$ 相对于 $\\beta$ 是常数，可以从最小化中去掉。我们也可以加上常数项 $\\frac{1}{2}z^{\\top}z$ 而不改变最小化子。因此，目标函数等价于最小化：\n$$ L(\\beta) = \\frac{1}{2}(\\beta^{\\top}\\beta - 2z^{\\top}\\beta + z^{\\top}z) + \\lambda \\|\\beta\\|_{1} = \\frac{1}{2}\\|\\beta - z\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} $$\n这个目标函数是可分的，意味着它可以写成关于 $\\beta$ 各个分量的函数之和：\n$$ L(\\beta) = \\sum_{j=1}^{p} \\left( \\frac{1}{2}(\\beta_j - z_j)^2 + \\lambda |\\beta_j| \\right) $$\n因此，我们可以通过独立地最小化每个分量项来找到最优的 $\\hat{\\beta}_{\\mathrm{LASSO}}$：\n$$ \\hat{\\beta}_{j, \\mathrm{LASSO}} = \\arg \\min_{\\beta_j \\in \\mathbb{R}} \\left( \\frac{1}{2}(\\beta_j - z_j)^2 + \\lambda |\\beta_j| \\right) $$\n这是 $\\ell_1$ 范数的近端算子，其结果为软阈值函数。每个分量的解是：\n$$ \\hat{\\beta}_{j, \\mathrm{LASSO}} = \\mathrm{sign}(z_j) \\cdot \\max(|z_j| - \\lambda, 0) $$\n这可以表示为 $\\hat{\\beta}_{\\mathrm{LASSO}} = S_{\\lambda}(z)$，其中 $S_{\\lambda}$ 是阈值为 $\\lambda$ 的逐元素软阈值算子。\n\n接下来，我们分析丹齐格选择器 $\\hat{\\beta}_{\\mathrm{DS}}$，它是以下约束优化问题的解：\n$$ \\hat{\\beta}_{\\mathrm{DS}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|X^{\\top}(y - X \\beta)\\|_{\\infty} \\leq t $$\n我们使用标准正交设计属性 $X^{\\top}X = I_p$ 和定义 $z = X^{\\top}y$ 来简化约束条件：\n$$ X^{\\top}(y - X \\beta) = X^{\\top}y - X^{\\top}X\\beta = z - \\beta $$\n因此约束变为 $\\|z - \\beta\\|_{\\infty} \\leq t$。这等价于一组逐分量的约束：\n$$ |z_j - \\beta_j| \\leq t \\quad \\text{for all } j \\in \\{1, \\dots, p\\} $$\n每个不等式可以重写为 $z_j - t \\leq \\beta_j \\leq z_j + t$。优化问题现在是在这些箱式约束下最小化 $\\|\\beta\\|_{1} = \\sum_{j=1}^{p} |\\beta_j|$。由于目标函数和约束都是可分的，我们可以独立求解每个分量：\n$$ \\hat{\\beta}_{j, \\mathrm{DS}} = \\arg \\min_{\\beta_j} |\\beta_j| \\quad \\text{subject to} \\quad z_j - t \\leq \\beta_j \\leq z_j + t $$\n为了找到最小值，我们考虑区间 $[z_j - t, z_j + t]$ 相对于 $0$ 的位置：\n1. 如果区间包含 $0$，即 $z_j - t \\leq 0 \\leq z_j + t$，这等价于 $|z_j| \\leq t$，则 $|\\beta_j|$ 的最小值在 $\\hat{\\beta}_{j, \\mathrm{DS}} = 0$ 处取得。\n2. 如果区间完全为正，即 $z_j - t > 0$ 或 $z_j > t$，区间中离 $0$ 最近的点是左端点。因此，$\\hat{\\beta}_{j, \\mathrm{DS}} = z_j - t$。\n3. 如果区间完全为负，即 $z_j + t  0$ 或 $z_j  -t$，区间中离 $0$ 最近的点是右端点。因此，$\\hat{\\beta}_{j, \\mathrm{DS}} = z_j + t$。\n这三种情况可以由同一个软阈值函数总结：\n$$ \\hat{\\beta}_{j, \\mathrm{DS}} = \\mathrm{sign}(z_j) \\cdot \\max(|z_j| - t, 0) $$\n这可以表示为 $\\hat{\\beta}_{\\mathrm{DS}} = S_{t}(z)$。\n\n现在我们使用给定的数值：\n$$ z = \\begin{pmatrix} 2.4 \\\\ -0.3 \\\\ 1.1 \\\\ -1.8 \\\\ 0.0 \\end{pmatrix}, \\quad \\lambda = 0.9, \\quad t = 0.7 $$\n我们计算 LASSO 估计 $\\hat{\\beta}_{\\mathrm{LASSO}} = S_{0.9}(z)$:\n$$ \\hat{\\beta}_{1, \\mathrm{LASSO}} = \\mathrm{sign}(2.4) \\max(|2.4| - 0.9, 0) = 1.5 $$\n$$ \\hat{\\beta}_{2, \\mathrm{LASSO}} = \\mathrm{sign}(-0.3) \\max(|-0.3| - 0.9, 0) = 0 $$\n$$ \\hat{\\beta}_{3, \\mathrm{LASSO}} = \\mathrm{sign}(1.1) \\max(|1.1| - 0.9, 0) = 0.2 $$\n$$ \\hat{\\beta}_{4, \\mathrm{LASSO}} = \\mathrm{sign}(-1.8) \\max(|-1.8| - 0.9, 0) = -0.9 $$\n$$ \\hat{\\beta}_{5, \\mathrm{LASSO}} = \\mathrm{sign}(0.0) \\max(|0.0| - 0.9, 0) = 0 $$\n所以，$\\hat{\\beta}_{\\mathrm{LASSO}} = \\begin{pmatrix} 1.5 \\\\ 0 \\\\ 0.2 \\\\ -0.9 \\\\ 0 \\end{pmatrix}$。\n\n接下来，我们计算丹齐格选择器估计 $\\hat{\\beta}_{\\mathrm{DS}} = S_{0.7}(z)$:\n$$ \\hat{\\beta}_{1, \\mathrm{DS}} = \\mathrm{sign}(2.4) \\max(|2.4| - 0.7, 0) = 1.7 $$\n$$ \\hat{\\beta}_{2, \\mathrm{DS}} = \\mathrm{sign}(-0.3) \\max(|-0.3| - 0.7, 0) = 0 $$\n$$ \\hat{\\beta}_{3, \\mathrm{DS}} = \\mathrm{sign(1.1)} \\max(|1.1| - 0.7, 0) = 0.4 $$\n$$ \\hat{\\beta}_{4, \\mathrm{DS}} = \\mathrm{sign}(-1.8) \\max(|-1.8| - 0.7, 0) = -1.1 $$\n$$ \\hat{\\beta}_{5, \\mathrm{DS}} = \\mathrm{sign}(0.0) \\max(|0.0| - 0.7, 0) = 0 $$\n所以，$\\hat{\\beta}_{\\mathrm{DS}} = \\begin{pmatrix} 1.7 \\\\ 0 \\\\ 0.4 \\\\ -1.1 \\\\ 0 \\end{pmatrix}$。\n\n最后，我们计算两个估计量之间的平方 $\\ell_2$ 距离 $\\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2}$：\n$$ \\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}} = \\begin{pmatrix} 1.5 - 1.7 \\\\ 0 - 0 \\\\ 0.2 - 0.4 \\\\ -0.9 - (-1.1) \\\\ 0 - 0 \\end{pmatrix} = \\begin{pmatrix} -0.2 \\\\ 0 \\\\ -0.2 \\\\ 0.2 \\\\ 0 \\end{pmatrix} $$\n平方 $\\ell_2$ 范数是这个差向量各分量平方的和：\n$$ \\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2} = (-0.2)^2 + 0^2 + (-0.2)^2 + (0.2)^2 + 0^2 $$\n$$ \\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2} = 0.04 + 0 + 0.04 + 0.04 + 0 = 0.12 $$\n问题要求答案四舍五入到四位有效数字。计算值为 $0.12$，可以写成 $0.1200$ 以反映此精度。",
            "answer": "$$\\boxed{0.1200}$$"
        },
        {
            "introduction": "理论上的相似性有时会掩盖实践中的差异。本练习通过一个简单的二维实例，直观地展示了 LASSO 和 Dantzig 选择算子在几何上的本质区别。您将发现，即便在一个估计器的解满足另一个估计器的约束条件时，由于它们各自优化不同的目标函数，最终的解也可能不同，这有助于您建立对两种方法几何特性的深刻理解。",
            "id": "3435555",
            "problem": "考虑一个有限维线性逆问题，其设计矩阵为 $A \\in \\mathbb{R}^{n \\times p}$，观测向量为 $y \\in \\mathbb{R}^{n}$。此处，最小绝对收缩和选择算子 (LASSO) 以其约束形式定义为以下凸规划的解集：\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} \\ \\ \\text{subject to} \\ \\ \\|x\\|_{1} \\leq t,\n$$\n其中给定预算 $t  0$。Dantzig 选择算子 (DS) 定义为以下凸规划的解集：\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\ \\|x\\|_{1} \\ \\ \\text{subject to} \\ \\ \\|A^{\\top}(A x - y)\\|_{\\infty} \\leq \\lambda,\n$$\n其中给定容差 $\\lambda  0$。\n\n在 $n = p = 2$ 的情况下，取 $A = I_{2}$（$2 \\times 2$ 单位矩阵）。设 $y = (3,1)^{\\top}$，$t = 2.5$，$\\lambda = 0.8$。仅从上述定义和标准的凸分析原理出发，完成以下任务：\n\n- 推导此实例的 LASSO 极小化子 $x_{\\mathrm{L}}$ 和 Dantzig 选择算子极小化子 $x_{\\mathrm{D}}$，并明确验证 LASSO 解集和 Dantzig 选择算子可行集相交，但产生不同的极小化子。\n- 从 $\\mathbb{R}^{2}$ 中水平集和可行域的形状角度，提供一个几何解释，说明为何即使 LASSO 极小化子是 Dantzig 可行的，两者得到的极小化子也不同。\n\n最后，计算欧几里得距离 $\\|x_{\\mathrm{L}} - x_{\\mathrm{D}}\\|_{2}$，并以简化的精确表达式给出最终答案。不要对答案进行四舍五入。",
            "solution": "该问题指定了 $n = p = 2$ 的情况，设计矩阵为单位矩阵 $A = I_2$，观测向量为 $y = (3, 1)^{\\top}$，LASSO 预算为 $t = 2.5$，Dantzig 选择算子容差为 $\\lambda = 0.8$。\n\n**1. 推导 LASSO 极小化子 $x_{\\mathrm{L}}$**\n\nLASSO 问题为\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} \\ \\ \\text{subject to} \\ \\ \\|x\\|_{1} \\leq t\n$$\n代入给定值，设 $x = (x_1, x_2)^{\\top}$：\n$$\n\\min_{x_1, x_2} \\ \\frac{1}{2}\\|x - y\\|_{2}^{2} = \\frac{1}{2}((x_1 - 3)^2 + (x_2 - 1)^2) \\ \\ \\text{subject to} \\ \\ |x_1| + |x_2| \\leq 2.5\n$$\n这个问题等价于求点 $y=(3,1)$ 在以原点为中心、半径为 $t=2.5$ 的 $\\ell_1$ 球上的欧几里得投影。\n首先，我们检查无约束极小化子 $x_{unc} = y = (3,1)$ 是否可行。我们计算其 $\\ell_1$ 范数：$\\|y\\|_1 = |3| + |1| = 4$。由于 $4  2.5$，无约束解是不可行的，因此 LASSO 解 $x_{\\mathrm{L}}$ 必须位于可行域的边界上，即 $\\|x_{\\mathrm{L}}\\|_1 = 2.5$。\n\n由于 $y$ 在第一象限，我们预计解 $x_{\\mathrm{L}}$ 也将在第一象限，所以 $x_1 \\geq 0$ 且 $x_2 \\geq 0$。约束变为 $x_1 + x_2 = 2.5$。几何上，解是线段 $x_1 + x_2 = 2.5$（其中 $x_1, x_2 \\ge 0$）上距离 $y=(3,1)$ 最近的点。从解 $x_{\\mathrm{L}}$ 到 $y$ 的向量必须与该线段正交。直线 $x_1+x_2=2.5$ 的方向向量是 $(1, -1)$。因此，向量 $y - x_{\\mathrm{L}} = (3-x_1, 1-x_2)$ 必须与 $(1, -1)$ 正交。\n正交条件由点积给出：\n$$\n(3-x_1, 1-x_2) \\cdot (1, -1) = 0\n$$\n$$\n(3-x_1) - (1-x_2) = 0 \\implies 2 - x_1 + x_2 = 0 \\implies x_1 - x_2 = 2\n$$\n现在我们求解以下线性方程组：\n\\begin{enumerate}\n    \\item $x_1 + x_2 = 2.5$\n    \\item $x_1 - x_2 = 2$\n\\end{enumerate}\n两方程相加得 $2x_1 = 4.5$，所以 $x_1 = 2.25$。\n将 $x_1$ 代入第一个方程得 $2.25 + x_2 = 2.5$，所以 $x_2 = 0.25$。\n由于 $x_1 = 2.25  0$ 且 $x_2 = 0.25  0$，我们的假设是正确的。LASSO 极小化子为 $x_{\\mathrm{L}} = (2.25, 0.25)^{\\top}$。\n\n**2. 推导 Dantzig 选择算子极小化子 $x_{\\mathrm{D}}$**\n\nDantzig 选择算子 (DS) 问题为\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\ \\|x\\|_{1} \\ \\ \\text{subject to} \\ \\ \\|A^{\\top}(A x - y)\\|_{\\infty} \\leq \\lambda\n$$\n代入给定值，$A=I_2$：\n$$\n\\min_{x_1, x_2} \\ |x_1| + |x_2| \\ \\ \\text{subject to} \\ \\ \\|x - y\\|_{\\infty} \\leq \\lambda\n$$\n约束 $\\|x-y\\|_{\\infty} \\leq 0.8$ 展开为 $\\max(|x_1 - 3|, |x_2 - 1|) \\leq 0.8$。这等价于以下不等式组：\n$$\n|x_1 - 3| \\leq 0.8 \\implies 3 - 0.8 \\leq x_1 \\leq 3 + 0.8 \\implies 2.2 \\leq x_1 \\leq 3.8\n$$\n$$\n|x_2 - 1| \\leq 0.8 \\implies 1 - 0.8 \\leq x_2 \\leq 1 + 0.8 \\implies 0.2 \\leq x_2 \\leq 1.8\n$$\n可行域是矩形 $[2.2, 3.8] \\times [0.2, 1.8]$。由于整个区域都在第一象限，目标函数简化为最小化 $x_1 + x_2$。这是一个线性规划问题。线性函数在凸多胞体（此处为矩形）上的最小值必然出现在某个顶点处。这些顶点是 $(2.2, 0.2)$, $(3.8, 0.2)$, $(2.2, 1.8)$ 和 $(3.8, 1.8)$。在每个顶点上计算目标函数 $x_1+x_2$ 的值：\n\\begin{itemize}\n    \\item 在 $(2.2, 0.2)$ 处: $x_1 + x_2 = 2.2 + 0.2 = 2.4$\n    \\item 在 $(3.8, 0.2)$ 处: $x_1 + x_2 = 3.8 + 0.2 = 4.0$\n    \\item 在 $(2.2, 1.8)$ 处: $x_1 + x_2 = 2.2 + 1.8 = 4.0$\n    \\item 在 $(3.8, 1.8)$ 处: $x_1 + x_2 = 3.8 + 1.8 = 5.6$\n\\end{itemize}\n最小值为 $2.4$，在顶点 $(2.2, 0.2)$ 处取得。\n因此，Dantzig 选择算子极小化子为 $x_{\\mathrm{D}} = (2.2, 0.2)^{\\top}$。\n\n**3. 验证与几何解释**\n\n极小化子分别为 $x_{\\mathrm{L}} = (2.25, 0.25)^{\\top}$ 和 $x_{\\mathrm{D}} = (2.2, 0.2)^{\\top}$，它们是不同的。\n我们验证 LASSO 极小化子 $x_{\\mathrm{L}}$ 对于 Dantzig 选择算子问题是可行的。我们检查是否满足 $\\|x_{\\mathrm{L}} - y\\|_{\\infty} \\leq \\lambda$：\n$$\n\\|x_{\\mathrm{L}} - y\\|_{\\infty} = \\|(2.25 - 3, 0.25 - 1)\\|_{\\infty} = \\|(-0.75, -0.75)\\|_{\\infty} = \\max(|-0.75|, |-0.75|) = 0.75\n$$\n由于 $0.75 \\leq 0.8$， $x_{\\mathrm{L}}$ 确实在 Dantzig 选择算子的可行集内。这证实了 LASSO 解集和 Dantzig 选择算子可行集是相交的。然而，$\\|x_{\\mathrm{L}}\\|_1 = 2.25 + 0.25 = 2.5$，而 $\\|x_{\\mathrm{D}}\\|_1 = 2.2+0.2=2.4$。因为 $\\|x_{\\mathrm{D}}\\|_1  \\|x_{\\mathrm{L}}\\|_1$，所以 $x_{\\mathrm{L}}$ 不是 Dantzig 极小化子。\n\n几何解释：\n- LASSO 解 $x_{\\mathrm{L}}$ 是 $\\ell_1$ 球 $\\|x\\|_1 \\leq 2.5$（一个以原点为中心的菱形）上与点 $y=(3,1)$ 欧几里得距离最近的点。LASSO 目标函数的水平集是以 $y$ 为中心的圆。通过从 $y$ 点开始扩张一个圆，直到它首次接触到菱形，即可找到解。这个切点就是 $y$ 在菱形上的欧几里得投影。\n- Dantzig 选择算子解 $x_{\\mathrm{D}}$ 是 $\\ell_{\\infty}$ 球 $\\|x-y\\|_{\\infty} \\leq 0.8$（一个以 $y$ 为中心的正方形）内具有最小 $\\ell_1$ 范数的点。DS 目标函数的水平集是以原点为中心的菱形 $\\|x\\|_1=c$。通过从原点开始扩张一个菱形，直到它首次接触到可行正方形，即可找到解。\n- 这两种方法产生不同结果，因为它们在不同的可行域上优化不同的目标。LASSO 在一个 $\\ell_1$ 约束集上最小化 $\\ell_2$ 距离，而 Dantzig 选择算子在一个 $\\ell_{\\infty}$ 约束集上最小化 $\\ell_1$ 范数。点 $x_{\\mathrm{L}}=(2.25, 0.25)$ 在欧几里得距离上更接近 $y$，但点 $x_{\\mathrm{D}}=(2.2, 0.2)$ 在 $\\ell_1$ 度量下“更接近”原点。目标函数和约束条件的不同几何形状导致了不同的最优性条件，从而得到不同的解。\n\n**4. 最终计算**\n\n我们计算欧几里得距离 $\\|x_{\\mathrm{L}} - x_{\\mathrm{D}}\\|_{2}$。\n$$\nx_{\\mathrm{L}} = (2.25, 0.25)^{\\top} = \\left(\\frac{9}{4}, \\frac{1}{4}\\right)^{\\top}\n$$\n$$\nx_{\\mathrm{D}} = (2.2, 0.2)^{\\top} = \\left(\\frac{11}{5}, \\frac{1}{5}\\right)^{\\top}\n$$\n差向量为：\n$$\nx_{\\mathrm{L}} - x_{\\mathrm{D}} = \\left(\\frac{9}{4} - \\frac{11}{5}, \\frac{1}{4} - \\frac{1}{5}\\right) = \\left(\\frac{45 - 44}{20}, \\frac{5 - 4}{20}\\right) = \\left(\\frac{1}{20}, \\frac{1}{20}\\right)^{\\top}\n$$\n欧几里得距离为：\n$$\n\\|x_{\\mathrm{L}} - x_{\\mathrm{D}}\\|_{2} = \\sqrt{\\left(\\frac{1}{20}\\right)^2 + \\left(\\frac{1}{20}\\right)^2} = \\sqrt{2 \\cdot \\left(\\frac{1}{20}\\right)^2} = \\sqrt{\\frac{2}{400}} = \\frac{\\sqrt{2}}{20}\n$$",
            "answer": "$$\\boxed{\\frac{\\sqrt{2}}{20}}$$"
        },
        {
            "introduction": "多重共线性是现实世界数据分析中一个常见且棘手的难题。本练习将引导您探讨 LASSO 和 Dantzig 选择算子在面对完美共线性时的不同表现。通过在一个特制的共线性设计矩阵上计算两种估计量，您将观察到它们在变量选择上截然不同的行为，从而理解它们如何以各自独特的方式解决由预测变量相关性所引发的系数不确定性问题。",
            "id": "3435595",
            "problem": "考虑一个压缩感知中的线性回归模型，其设计矩阵 $X \\in \\mathbb{R}^{2 \\times 3}$ 和响应向量 $y \\in \\mathbb{R}^{2}$ 由下式给出：\n$$\nX = \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix}, \n\\quad\ny = \\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}.\n$$\n注意到第三列满足精确的线性关系 $x_{3} = x_{1} + x_{2}$，因此设计矩阵表现出完全共线性。定义最小绝对收缩和选择算子 (Least Absolute Shrinkage and Selection Operator, LASSO) 估计量 $\\hat{\\beta}^{\\text{L}} \\in \\mathbb{R}^{3}$ 为以下问题的解：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda_{\\text{L}}\\|\\beta\\|_{1},\n$$\n以及丹齐格选择器 (Dantzig selector) 估计量 $\\hat{\\beta}^{\\text{D}} \\in \\mathbb{R}^{3}$ 为以下问题的解：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\|\\beta\\|_{1} \\quad \\text{约束条件为} \\quad \\|X^{\\top}(y - X\\beta)\\|_{\\infty} \\leq \\lambda_{\\text{D}}.\n$$\n取 LASSO 的正则化水平为 $\\lambda_{\\text{L}} = 1$，丹齐格选择器的正则化水平为 $\\lambda_{\\text{D}} = \\frac{3}{4}$。从以上核心定义出发，精确计算这两个估计量，并解释完全共线性 $x_{3} = x_{1} + x_{2}$ 如何在这两种方法下导致不同的支撑集恢复结果。将连接后的估计量向量以单行矩阵的形式报告：首先是 LASSO 系数，顺序为 $(\\beta_{1}, \\beta_{2}, \\beta_{3})$，然后是丹齐格选择器系数，顺序相同。无需四舍五入。",
            "solution": "### LASSO 估计量计算\nLASSO 目标函数为 $L(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda_{\\text{L}}\\|\\beta\\|_{1}$。当 $\\lambda_{\\text{L}} = 1$ 时，表达式为：\n$$\nL(\\beta) = \\frac{1}{2}\\left\\| \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix} \\right\\|_{2}^{2} + \\sum_{j=1}^{3} |\\beta_j|\n$$\n$$\nL(\\beta) = \\frac{1}{2} \\left[ (1 - \\beta_1 - \\beta_3)^2 + (-1 - \\beta_2 - \\beta_3)^2 \\right] + |\\beta_1| + |\\beta_2| + |\\beta_3|\n$$\nKarush-Kuhn-Tucker (KKT) 条件表明，在最小值 $\\hat{\\beta}$ 处，零向量必须在目标函数的次梯度中：$0 \\in \\partial L(\\hat{\\beta})$。这给出了条件：\n$$\n-X^{\\top}(y - X\\hat{\\beta}) + \\lambda_{\\text{L}} s = 0, \\quad \\text{其中 } s \\in \\partial\\|\\hat{\\beta}\\|_{1}\n$$\n这里，$s$ 是一个向量，使得如果 $\\hat{\\beta}_j \\neq 0$，则 $s_j = \\text{sign}(\\hat{\\beta}_j)$；如果 $\\hat{\\beta}_j = 0$，则 $s_j \\in [-1, 1]$。当 $\\lambda_{\\text{L}}=1$ 时，条件为 $X^{\\top}X\\hat{\\beta} - X^{\\top}y + s = 0$。\n我们计算所需的矩阵：\n$$\nX^{\\top}X = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\end{pmatrix}\n$$\n$$\nX^{\\top}y = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\nKKT 条件构成一个方程组：\n1. $\\hat{\\beta}_1 + \\hat{\\beta}_3 - 1 + s_1 = 0$\n2. $\\hat{\\beta}_2 + \\hat{\\beta}_3 + 1 + s_2 = 0$\n3. $\\hat{\\beta}_1 + \\hat{\\beta}_2 + 2\\hat{\\beta}_3 + s_3 = 0$\n\n将前两个方程相加得到 $\\hat{\\beta}_1 + \\hat{\\beta}_2 + 2\\hat{\\beta}_3 + s_1 + s_2 = 0$。将其与第三个方程比较，揭示了由共线性施加的一致性条件：$s_3 = s_1 + s_2$。\n\n我们来检验假设 $\\hat{\\beta}^{\\text{L}} = (0, 0, 0)$。如果 $\\hat{\\beta} = 0$，我们可以选择任意 $s_j \\in [-1, 1]$。KKT 条件变为：\n1. $0 + 0 - 1 + s_1 = 0 \\implies s_1 = 1$\n2. $0 + 0 + 1 + s_2 = 0 \\implies s_2 = -1$\n3. $0 + 0 + 0 + s_3 = 0 \\implies s_3 = 0$\n次梯度向量为 $s = (1, -1, 0)$。它的所有分量都在 $[-1, 1]$ 内，所以 $s \\in \\partial\\|0\\|_1$。此外，一致性条件 $s_3 = s_1 + s_2$ 也得到满足，因为 $0 = 1 + (-1)$。因此，$\\hat{\\beta}^{\\text{L}} = (0, 0, 0)$ 是一个有效的解。由于 LASSO 目标函数是凸的，这是全局最小值。\n所以，$\\hat{\\beta}^{\\text{L}} = (0, 0, 0)$。\n\n### 丹齐格选择器估计量计算\n丹齐格选择器问题是：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\|\\beta\\|_{1} \\quad \\text{约束条件为} \\quad \\|X^{\\top}(y - X\\beta)\\|_{\\infty} \\leq \\lambda_{\\text{D}}\n$$\n其中 $\\lambda_{\\text{D}} = \\frac{3}{4}$。约束条件为 $|(X^{\\top}(y - X\\beta))_j| \\le \\lambda_{\\text{D}}$，对 $j=1,2,3$ 成立。\n我们定义 $\\gamma_1 = \\beta_1 + \\beta_3$ 和 $\\gamma_2 = \\beta_2 + \\beta_3$。$X\\beta$ 项可以被重新参数化为：\n$$\nX\\beta = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 + x_2) = (\\beta_1 + \\beta_3)x_1 + (\\beta_2 + \\beta_3)x_2 = \\gamma_1 x_1 + \\gamma_2 x_2\n$$\n对残差相关性的约束是：\n1. $|\\gamma_1 - 1| \\le \\frac{3}{4} \\implies \\frac{1}{4} \\le \\gamma_1 \\le \\frac{7}{4}$。\n2. $|\\gamma_2 - (-1)| \\le \\frac{3}{4} \\implies -\\frac{7}{4} \\le \\gamma_2 \\le -\\frac{1}{4}$。\n3. $|\\gamma_1 + \\gamma_2 - 0| \\le \\frac{3}{4} \\implies -\\frac{3}{4} \\le \\gamma_1 + \\gamma_2 \\le \\frac{3}{4}$。\n\n目标是最小化 $\\|\\beta\\|_1 = |\\beta_1| + |\\beta_2| + |\\beta_3| = |\\gamma_1 - \\beta_3| + |\\gamma_2 - \\beta_3| + |\\beta_3|$。对于任何固定的 $(\\gamma_1, \\gamma_2)$，通过选择 $\\beta_3$ 为 $\\{0, \\gamma_1, \\gamma_2\\}$ 的中位数来最小化该项。\n从约束条件可知，任何可行的 $\\gamma_1$ 都是正的，任何可行的 $\\gamma_2$ 都是负的。因此，有序集合总是 $\\{\\gamma_2, 0, \\gamma_1\\}$。中位数是 $0$，所以最优选择是 $\\hat{\\beta}_3 = 0$。\n当 $\\hat{\\beta}_3=0$ 时，我们有 $\\beta_1 = \\gamma_1$ 和 $\\beta_2 = \\gamma_2$。目标函数变为最小化 $|\\gamma_1| + |\\gamma_2|$。由于 $\\gamma_1 > 0$ 和 $\\gamma_2  0$，这等价于最小化 $\\gamma_1 - \\gamma_2$。\n\n我们现在有一个线性规划问题：\n最小化 $\\gamma_1 - \\gamma_2$，约束条件为：\n$$\n\\frac{1}{4} \\le \\gamma_1 \\le \\frac{7}{4}\n$$\n$$\n-\\frac{7}{4} \\le \\gamma_2 \\le -\\frac{1}{4}\n$$\n$$\n-\\frac{3}{4} \\le \\gamma_1 + \\gamma_2 \\le \\frac{3}{4}\n$$\n为了最小化 $\\gamma_1 - \\gamma_2$，我们必须选择尽可能小的 $\\gamma_1$ 和尽可能大的 $\\gamma_2$。从前两个约束条件可知，它们是 $\\gamma_1 = \\frac{1}{4}$ 和 $\\gamma_2 = -\\frac{1}{4}$。\n我们验证这对值是否满足第三个约束条件：$\\gamma_1 + \\gamma_2 = \\frac{1}{4} + (-\\frac{1}{4}) = 0$。\n由于 $-\\frac{3}{4} \\le 0 \\le \\frac{3}{4}$，该点是可行的。\n因此，解是 $\\hat{\\gamma}_1 = \\frac{1}{4}$ 和 $\\hat{\\gamma}_2 = -\\frac{1}{4}$。\n丹齐格选择器估计量的系数是：\n$$\n\\hat{\\beta}_3^{\\text{D}} = 0\n$$\n$$\n\\hat{\\beta}_1^{\\text{D}} = \\hat{\\gamma}_1 - \\hat{\\beta}_3^{\\text{D}} = \\frac{1}{4}\n$$\n$$\n\\hat{\\beta}_2^{\\text{D}} = \\hat{\\gamma}_2 - \\hat{\\beta}_3^{\\text{D}} = -\\frac{1}{4}\n$$\n所以，$\\hat{\\beta}^{\\text{D}} = (\\frac{1}{4}, -\\frac{1}{4}, 0)$。\n\n### 不同结果的解释\n完全共线性 $x_3 = x_1 + x_2$ 意味着对于任何向量 $\\beta$，如果我们将 $\\beta$ 替换为 $\\beta' = (\\beta_1 + \\alpha, \\beta_2 + \\alpha, \\beta_3 - \\alpha)$（对于任何标量 $\\alpha$），预测值 $X\\beta$ 保持不变。这两种方法以不同的方式解决这种模糊性。\n\n- **LASSO**：目标函数将平方误差损失与 $l_1$ 惩罚项相结合。正则化水平 $\\lambda_{\\text{L}} = 1$ 足够高，可以将有效系数 $\\gamma_1 = \\beta_1 + \\beta_3$ 和 $\\gamma_2 = \\beta_2 + \\beta_3$ 收缩到零。这使得最优拟合为 $X\\beta = 0$。在给定此拟合的情况下，LASSO 必须从产生它的无限个系数向量族 $(\\alpha, \\alpha, -\\alpha)$ 中进行选择。它通过最小化惩罚项 $\\|\\beta\\|_1=|\\alpha|+|\\alpha|+|-\\alpha|=3|\\alpha|$ 来实现这一点。该项在 $\\alpha=0$ 处最小化，得到唯一解 $\\hat{\\beta}^{\\text{L}} = (0, 0, 0)$。LASSO 无法区分共线的预测变量，并且在这种正则化水平下，它将所有这些变量都消除了。\n\n- **丹齐格选择器**：该方法将模型拟合与系数正则化分开。拟合由约束条件处理，这些约束条件要求每个特征与残差之间的相关性要小 ($|\\cdot| \\le \\lambda_{\\text{D}}$)。其目标仅仅是在这个可行集内找到具有最小 $l_1$ 范数的向量 $\\beta$。共线性导致了约束条件中的依赖关系。然而，正则化水平 $\\lambda_{\\text{D}} = \\frac{3}{4}$ 定义了一个允许非零系数的可行集。优化过程正确地识别出，在给定约束下要最小化 $\\|\\beta\\|_1$，解必须满足 $\\beta_3=0$。然后，它找到 $\\beta_1$ 和 $\\beta_2$ 的非零系数，这些系数在满足相关性边界的同时具有最小的 $l_1$ 范数。结果是一个非零的稀疏解 $\\hat{\\beta}^{\\text{D}} = (\\frac{1}{4}, -\\frac{1}{4}, 0)$，它正确地识别了两个基预测变量 $x_1$ 和 $x_2$。\n\n本质上，丹齐格选择器的结构允许它容忍一定量的残差相关性以找到稀疏解，而 LASSO 的统一目标函数，在选定的 $\\lambda_{\\text{L}}$ 下，以将所有系数设置为零为代价，迫使解具有零残差相关性。\n\n连接后的向量为 $(0, 0, 0, \\frac{1}{4}, -\\frac{1}{4}, 0)$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  0  0  \\frac{1}{4}  -\\frac{1}{4}  0 \\end{pmatrix}}\n$$"
        }
    ]
}