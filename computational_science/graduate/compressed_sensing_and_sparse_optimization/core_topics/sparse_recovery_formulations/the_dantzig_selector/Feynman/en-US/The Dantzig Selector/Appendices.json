{
    "hands_on_practices": [
        {
            "introduction": "To build a strong intuition for the Dantzig selector, we begin with a hands-on calculation in a simple two-dimensional setting. This practice guides you through translating the abstract definition of the selector's constraint, $\\|A^{\\top}(y - A x)\\|_{\\infty} \\le \\delta$, into a concrete set of linear inequalities. By characterizing this feasible region as a polyhedron, you will see how minimizing the $\\ell_1$-norm geometrically corresponds to finding a specific vertex of this shape .",
            "id": "3487278",
            "problem": "Consider a linear model with design matrix $A \\in \\mathbb{R}^{2 \\times 2}$ and observation vector $y \\in \\mathbb{R}^{2}$. The Dantzig selector imposes the constraint $\\|A^{\\top}(y - A x)\\|_{\\infty} \\le \\delta$ on the coefficient vector $x \\in \\mathbb{R}^{2}$ for a given tolerance $\\delta > 0$. Let\n$$\nA = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad \\delta = 1.\n$$\nStarting from the definition of the Dantzig selector constraint $\\|A^{\\top}(y - A x)\\|_{\\infty} \\le \\delta$ and fundamental linear algebra identities, do the following:\n- Derive an explicit system of linear inequalities in the components of $x$ that characterizes the feasible region in $\\mathbb{R}^{2}$.\n- Compute all extreme points (vertices) of this feasible polyhedral region by solving the appropriate boundary equalities.\n- Identify the extreme point that minimizes the objective $\\|x\\|_{1}$ over the feasible region, and justify your selection by analyzing the values of $\\|x\\|_{1}$ attained at the vertices (and, if necessary, along edges).\n\nReport the coordinates of the minimizing extreme point as your final answer in the form of a single row vector. No rounding is required, and no units are involved.",
            "solution": "The problem requires finding the solution to the Dantzig selector optimization problem, which aims to minimize the $\\ell_1$-norm of a coefficient vector $x$ subject to a constraint on the correlation between the columns of the design matrix and the residual.\n\nFirst, we validate the problem statement.\n\n### Step 1: Extract Givens\n- Design matrix: $A = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 2}$\n- Observation vector: $y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} \\in \\mathbb{R}^{2}$\n- Coefficient vector: $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^{2}$\n- Tolerance: $\\delta = 1$\n- Constraint: $\\|A^{\\top}(y - A x)\\|_{\\infty} \\le \\delta$\n- Objective: Minimize $\\|x\\|_{1}$ over the feasible region defined by the constraint.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the field of sparse optimization and compressed sensing. The Dantzig selector is a well-established method. The problem is well-posed, with all necessary matrices, vectors, and parameters provided. The data are numerically and dimensionally consistent. The language is objective and precise. The task is clearly defined and involves standard mathematical procedures (linear algebra, convex optimization). No flaws are detected.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\nThe core of the problem is to characterize the feasible region defined by the Dantzig selector constraint $\\|A^{\\top}(y - A x)\\|_{\\infty} \\le \\delta$, and then find the point within this region that has the minimum $\\ell_1$-norm.\n\nLet's begin by computing the expression inside the norm. The transpose of $A$ is:\n$$\nA^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}\n$$\nThe term $Ax$ is:\n$$\nAx = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} x_1 + x_2 \\\\ x_2 \\end{pmatrix}\n$$\nThe residual vector $r = y - Ax$ is:\n$$\nr = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} x_1 + x_2 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3 - x_1 - x_2 \\\\ 1 - x_2 \\end{pmatrix}\n$$\nNow, we compute the product $A^{\\top}r$:\n$$\nA^{\\top}r = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 3 - x_1 - x_2 \\\\ 1 - x_2 \\end{pmatrix} = \\begin{pmatrix} 1(3 - x_1 - x_2) + 0(1 - x_2) \\\\ 1(3 - x_1 - x_2) + 1(1 - x_2) \\end{pmatrix} = \\begin{pmatrix} 3 - x_1 - x_2 \\\\ 4 - x_1 - 2x_2 \\end{pmatrix}\n$$\nThe Dantzig selector constraint is $\\|A^{\\top}(y - A x)\\|_{\\infty} \\le \\delta$. With $\\delta = 1$, this becomes:\n$$\n\\left\\| \\begin{pmatrix} 3 - x_1 - x_2 \\\\ 4 - x_1 - 2x_2 \\end{pmatrix} \\right\\|_{\\infty} \\le 1\n$$\nThe infinity norm, $\\|\\cdot\\|_{\\infty}$, of a vector is the maximum absolute value of its components. Therefore, the constraint is equivalent to the following two inequalities:\n$$\n|3 - x_1 - x_2| \\le 1 \\quad \\text{and} \\quad |4 - x_1 - 2x_2| \\le 1\n$$\nWe can expand each absolute value inequality into a pair of linear inequalities.\nFor the first one, $|3 - x_1 - x_2| \\le 1$:\n$$\n-1 \\le 3 - x_1 - x_2 \\le 1\n$$\nThis gives us:\n1.  $3 - x_1 - x_2 \\le 1 \\implies 2 \\le x_1 + x_2$\n2.  $3 - x_1 - x_2 \\ge -1 \\implies 4 \\ge x_1 + x_2$\n\nFor the second one, $|4 - x_1 - 2x_2| \\le 1$:\n$$\n-1 \\le 4 - x_1 - 2x_2 \\le 1\n$$\nThis gives us:\n3.  $4 - x_1 - 2x_2 \\le 1 \\implies 3 \\le x_1 + 2x_2$\n4.  $4 - x_1 - 2x_2 \\ge -1 \\implies 5 \\ge x_1 + 2x_2$\n\nThus, the feasible region is a polyhedron in $\\mathbb{R}^2$ defined by the intersection of four half-spaces:\n$$\n\\begin{cases}\nx_1 + x_2 \\ge 2 \\\\\nx_1 + x_2 \\le 4 \\\\\nx_1 + 2x_2 \\ge 3 \\\\\nx_1 + 2x_2 \\le 5\n\\end{cases}\n$$\nThis region is a closed and bounded convex set (a parallelogram). The extreme points (vertices) of this region are found by finding the intersection of the boundary lines. The boundary lines are:\n$L_1: x_1 + x_2 = 2$\n$L_2: x_1 + x_2 = 4$\n$L_3: x_1 + 2x_2 = 3$\n$L_4: x_1 + 2x_2 = 5$\n\nLines $L_1$ and $L_2$ are parallel. Lines $L_3$ and $L_4$ are parallel. The vertices are the four points of intersection between one line from $\\{L_1, L_2\\}$ and one line from $\\{L_3, L_4\\}$.\n\nVertex 1 (Intersection of $L_1$ and $L_3$):\n$$\n\\begin{cases} x_1 + x_2 = 2 \\\\ x_1 + 2x_2 = 3 \\end{cases} \\implies (x_1 + 2x_2) - (x_1 + x_2) = 3 - 2 \\implies x_2 = 1.\n$$\nSubstituting $x_2 = 1$ into $x_1 + x_2 = 2$ gives $x_1 = 1$. The vertex is $V_1 = (1, 1)$.\n\nVertex 2 (Intersection of $L_1$ and $L_4$):\n$$\n\\begin{cases} x_1 + x_2 = 2 \\\\ x_1 + 2x_2 = 5 \\end{cases} \\implies (x_1 + 2x_2) - (x_1 + x_2) = 5 - 2 \\implies x_2 = 3.\n$$\nSubstituting $x_2 = 3$ into $x_1 + x_2 = 2$ gives $x_1 = -1$. The vertex is $V_2 = (-1, 3)$.\n\nVertex 3 (Intersection of $L_2$ and $L_4$):\n$$\n\\begin{cases} x_1 + x_2 = 4 \\\\ x_1 + 2x_2 = 5 \\end{cases} \\implies (x_1 + 2x_2) - (x_1 + x_2) = 5 - 4 \\implies x_2 = 1.\n$$\nSubstituting $x_2 = 1$ into $x_1 + x_2 = 4$ gives $x_1 = 3$. The vertex is $V_3 = (3, 1)$.\n\nVertex 4 (Intersection of $L_2$ and $L_3$):\n$$\n\\begin{cases} x_1 + x_2 = 4 \\\\ x_1 + 2x_2 = 3 \\end{cases} \\implies (x_1 + 2x_2) - (x_1 + x_2) = 3 - 4 \\implies x_2 = -1.\n$$\nSubstituting $x_2 = -1$ into $x_1 + x_2 = 4$ gives $x_1 = 5$. The vertex is $V_4 = (5, -1)$.\n\nThe four extreme points of the feasible region are $V_1=(1, 1)$, $V_2=(-1, 3)$, $V_3=(3, 1)$, and $V_4=(5, -1)$.\n\nThe objective is to minimize the $\\ell_1$-norm, $\\|x\\|_{1} = |x_1| + |x_2|$, over this feasible region. Since the objective function $\\|x\\|_1$ is a convex function and the feasible region is a compact convex set, the minimum value must be attained at one or more of its extreme points (vertices). We evaluate the objective function at each vertex:\n- For $V_1 = (1, 1)$: $\\|V_1\\|_1 = |1| + |1| = 2$.\n- For $V_2 = (-1, 3)$: $\\|V_2\\|_1 = |-1| + |3| = 1 + 3 = 4$.\n- For $V_3 = (3, 1)$: $\\|V_3\\|_1 = |3| + |1| = 3 + 1 = 4$.\n- For $V_4 = (5, -1)$: $\\|V_4\\|_1 = |5| + |-1| = 5 + 1 = 6$.\n\nComparing the values, the minimum value of $\\|x\\|_1$ is $2$, which is attained at the vertex $V_1 = (1, 1)$. The problem asks for the extreme point that minimizes the objective. From our analysis of the extreme points, this is uniquely determined.\n\nTherefore, the extreme point that minimizes the $\\ell_1$-norm over the feasible region is $(1, 1)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1 & 1 \\end{pmatrix}}$$"
        },
        {
            "introduction": "We now move from a numerical example to an important analytical case that reveals the Dantzig selector's core mechanics. By assuming an orthonormal design matrix, where $X^{\\top}X = I$, the optimization problem simplifies remarkably, allowing for a closed-form solution. This exercise demonstrates that, under these idealized conditions, the Dantzig selector is equivalent to the well-known soft-thresholding operator, providing deep insight into its shrinkage properties and enabling a precise analysis of its bias .",
            "id": "3487307",
            "problem": "Let $X \\in \\mathbb{R}^{n \\times p}$ be a design matrix with $X^\\top X = I$, and let $y \\in \\mathbb{R}^{n}$ be an observed response vector. Define $b = X^\\top y \\in \\mathbb{R}^{p}$. The Dantzig selector is the estimator $\\hat{\\beta}(\\lambda) \\in \\mathbb{R}^{p}$ defined by the convex program\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|X^\\top(y - X\\beta)\\|_{\\infty} \\le \\lambda,\n$$\nwhere $\\lambda > 0$ is a tuning parameter. Starting only from the above definition and the orthonormality condition $X^\\top X = I$, derive the closed-form expression of the minimizer $\\hat{\\beta}(\\lambda)$ in terms of $b$ and $\\lambda$ by reducing the problem to separable coordinate-wise subproblems. Then, define the bias vector $\\Delta(\\lambda) = \\hat{\\beta}(\\lambda) - b$ and compute the analytic expressions for the $\\ell_{2}$ and $\\ell_{\\infty}$ biases, namely $\\|\\Delta(\\lambda)\\|_{2}$ and $\\|\\Delta(\\lambda)\\|_{\\infty}$, as functions of $\\lambda$ and $b$. Express your final answer as a single row matrix containing the two closed-form expressions in terms of $b$ and $\\lambda$. No numerical approximation or rounding is required.",
            "solution": "The problem statement is evaluated as valid based on the specified criteria. It is scientifically grounded in the field of statistical optimization, well-posed, objective, and contains all necessary information for a unique solution.\n\nThe problem asks for the closed-form solution to the Dantzig selector optimization problem under the condition of an orthonormal design matrix, and for the subsequent calculation of the $\\ell_{2}$ and $\\ell_{\\infty}$ norms of the bias vector.\n\nThe Dantzig selector is defined as the solution $\\hat{\\beta}(\\lambda)$ to the convex program:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|X^\\top(y - X\\beta)\\|_{\\infty} \\le \\lambda\n$$\nWe are given the design matrix $X \\in \\mathbb{R}^{n \\times p}$ satisfies $X^\\top X = I$, where $I$ is the $p \\times p$ identity matrix. We are also given the definition $b = X^\\top y \\in \\mathbb{R}^{p}$.\n\nFirst, we simplify the constraint of the optimization problem using the given information:\n$$\nX^\\top(y - X\\beta) = X^\\top y - (X^\\top X)\\beta\n$$\nSubstituting $b = X^\\top y$ and $X^\\top X = I$, we get:\n$$\nX^\\top(y - X\\beta) = b - \\beta\n$$\nThe constraint $\\|X^\\top(y - X\\beta)\\|_{\\infty} \\le \\lambda$ thus simplifies to:\n$$\n\\|b - \\beta\\|_{\\infty} \\le \\lambda\n$$\nThe $\\ell_{\\infty}$-norm of a vector $v \\in \\mathbb{R}^{p}$ is defined as $\\|v\\|_{\\infty} = \\max_{j=1,\\dots,p} |v_j|$. Therefore, the constraint is equivalent to a set of $p$ component-wise inequalities:\n$$\n|b_j - \\beta_j| \\le \\lambda \\quad \\text{for each } j = 1, \\dots, p\n$$\n\nThe objective function is the $\\ell_1$-norm, which is additively separable across the components of $\\beta$:\n$$\n\\|\\beta\\|_{1} = \\sum_{j=1}^{p} |\\beta_j|\n$$\nSince both the objective function and the constraints are separable, we can decompose the $p$-dimensional optimization problem into $p$ independent one-dimensional optimization problems. For each component $j \\in \\{1, \\dots, p\\}$, we solve:\n$$\n\\hat{\\beta}_j(\\lambda) = \\arg\\min_{\\beta_j \\in \\mathbb{R}} |\\beta_j| \\quad \\text{subject to} \\quad |b_j - \\beta_j| \\le \\lambda\n$$\nThe constraint $|b_j - \\beta_j| \\le \\lambda$ is equivalent to $-\\lambda \\le b_j - \\beta_j \\le \\lambda$, which defines the closed interval $[b_j - \\lambda, b_j + \\lambda]$ for $\\beta_j$. We are thus looking for the element in this interval with the minimum absolute value. This is equivalent to finding the point in the interval $[b_j - \\lambda, b_j + \\lambda]$ that is closest to the origin $0$.\n\nWe analyze the solution by considering the position of this interval relative to $0$:\n1.  If the interval contains $0$, i.e., $b_j - \\lambda \\le 0 \\le b_j + \\lambda$, which is equivalent to $|b_j| \\le \\lambda$, the point with the minimum absolute value is $\\hat{\\beta}_j(\\lambda) = 0$.\n2.  If the interval is entirely to the right of $0$, i.e., $b_j - \\lambda > 0$, which is equivalent to $b_j > \\lambda$, the point with the minimum absolute value is the left endpoint of the interval, $\\hat{\\beta}_j(\\lambda) = b_j - \\lambda$.\n3.  If the interval is entirely to the left of $0$, i.e., $b_j + \\lambda < 0$, which is equivalent to $b_j < -\\lambda$, the point with the minimum absolute value is the right endpoint of the interval, $\\hat{\\beta}_j(\\lambda) = b_j + \\lambda$.\n\nThese three cases can be expressed compactly using the soft-thresholding operator $S_{\\lambda}(\\cdot)$:\n$$\n\\hat{\\beta}_j(\\lambda) = S_{\\lambda}(b_j) = \\text{sgn}(b_j) \\max(0, |b_j| - \\lambda)\n$$\nThe vector solution is obtained by applying this operator element-wise to the vector $b$: $\\hat{\\beta}(\\lambda) = S_{\\lambda}(b)$.\n\nNext, we compute the bias vector $\\Delta(\\lambda) = \\hat{\\beta}(\\lambda) - b$. The $j$-th component is $\\Delta_j(\\lambda) = \\hat{\\beta}_j(\\lambda) - b_j$. We use the same case analysis:\n1.  If $|b_j| \\le \\lambda$, then $\\hat{\\beta}_j(\\lambda) = 0$, so $\\Delta_j(\\lambda) = 0 - b_j = -b_j$.\n2.  If $b_j > \\lambda$, then $\\hat{\\beta}_j(\\lambda) = b_j - \\lambda$, so $\\Delta_j(\\lambda) = (b_j - \\lambda) - b_j = -\\lambda$.\n3.  If $b_j < -\\lambda$, then $\\hat{\\beta}_j(\\lambda) = b_j + \\lambda$, so $\\Delta_j(\\lambda) = (b_j + \\lambda) - b_j = \\lambda$.\n\nThese can be summarized as:\n$$\n\\Delta_j(\\lambda) =\n\\begin{cases}\n-b_j & \\text{if } |b_j| \\le \\lambda \\\\\n-\\lambda \\cdot \\text{sgn}(b_j) & \\text{if } |b_j| > \\lambda\n\\end{cases}\n$$\n\nNow we compute the required norms of the bias vector $\\Delta(\\lambda)$.\n\nFirst, the $\\ell_2$-norm, $\\|\\Delta(\\lambda)\\|_2$. We compute its square:\n$$\n\\|\\Delta(\\lambda)\\|_2^2 = \\sum_{j=1}^{p} \\Delta_j(\\lambda)^2\n$$\nWe split the sum into two parts based on the condition on $|b_j|$:\n$$\n\\|\\Delta(\\lambda)\\|_2^2 = \\sum_{j: |b_j| \\le \\lambda} \\Delta_j(\\lambda)^2 + \\sum_{j: |b_j| > \\lambda} \\Delta_j(\\lambda)^2\n$$\nFor indices $j$ where $|b_j| \\le \\lambda$, we have $\\Delta_j(\\lambda)^2 = (-b_j)^2 = b_j^2$.\nFor indices $j$ where $|b_j| > \\lambda$, we have $\\Delta_j(\\lambda)^2 = (-\\lambda \\cdot \\text{sgn}(b_j))^2 = \\lambda^2$.\nSo, the squared norm is:\n$$\n\\|\\Delta(\\lambda)\\|_2^2 = \\sum_{j: |b_j| \\le \\lambda} b_j^2 + \\sum_{j: |b_j| > \\lambda} \\lambda^2\n$$\nThe second sum is over all indices where $|b_j| > \\lambda$. If we denote the number of such indices by $|\\{j : |b_j| > \\lambda\\}|$, this sum equals $\\lambda^2 |\\{j : |b_j| > \\lambda\\}|$.\nTaking the square root gives the $\\ell_2$-norm:\n$$\n\\|\\Delta(\\lambda)\\|_2 = \\sqrt{\\sum_{j: |b_j| \\le \\lambda} b_j^2 + \\lambda^2 |\\{j : |b_j| > \\lambda\\}|}\n$$\nThis can be written using indicator functions $\\mathbb{I}(\\cdot)$ as:\n$$\n\\|\\Delta(\\lambda)\\|_2 = \\sqrt{\\sum_{j=1}^{p} b_j^2 \\mathbb{I}(|b_j| \\le \\lambda) + \\lambda^{2} \\sum_{j=1}^{p} \\mathbb{I}(|b_j| > \\lambda)}\n$$\n\nSecond, the $\\ell_\\infty$-norm, $\\|\\Delta(\\lambda)\\|_\\infty = \\max_{j=1,\\dots,p} |\\Delta_j(\\lambda)|$. We analyze the magnitudes $|\\Delta_j(\\lambda)|$:\n1.  If $|b_j| \\le \\lambda$, then $|\\Delta_j(\\lambda)| = |-b_j| = |b_j|$. In this case, $|\\Delta_j(\\lambda)| \\le \\lambda$.\n2.  If $|b_j| > \\lambda$, then $|\\Delta_j(\\lambda)| = |-\\lambda \\cdot \\text{sgn}(b_j)| = \\lambda$.\n\nWe need to find the maximum value among all $|\\Delta_j(\\lambda)|$. The set of values is $\\{|b_j| \\text{ for } j \\text{ s.t. } |b_j| \\le \\lambda\\} \\cup \\{\\lambda \\text{ for } j \\text{ s.t. } |b_j| > \\lambda\\}$.\nLet's consider two cases based on the relationship between $\\lambda$ and $\\|b\\|_\\infty = \\max_j |b_j|$:\n1.  If $\\|b\\|_\\infty \\le \\lambda$, then $|b_j| \\le \\lambda$ for all $j=1, \\dots, p$. In this case, $\\Delta_j(\\lambda) = -b_j$ for all $j$. Then $\\|\\Delta(\\lambda)\\|_\\infty = \\max_j |-b_j| = \\max_j |b_j| = \\|b\\|_\\infty$.\n2.  If $\\|b\\|_\\infty > \\lambda$, then there exists at least one index $k$ such that $|b_k| > \\lambda$. For any such $k$, $|\\Delta_k(\\lambda)| = \\lambda$. For any index $j$ where $|b_j| \\le \\lambda$, we have $|\\Delta_j(\\lambda)| = |b_j| \\le \\lambda$. Therefore, the maximum over all components is exactly $\\lambda$.\n\nCombining these two cases, we find:\n$$\n\\|\\Delta(\\lambda)\\|_\\infty =\n\\begin{cases}\n\\|b\\|_\\infty & \\text{if } \\|b\\|_\\infty \\le \\lambda \\\\\n\\lambda & \\text{if } \\|b\\|_\\infty > \\lambda\n\\end{cases}\n$$\nThis is precisely the definition of $\\min(\\lambda, \\|b\\|_\\infty)$.\n\nThe two requested expressions are $\\|\\Delta(\\lambda)\\|_2$ and $\\|\\Delta(\\lambda)\\|_{\\infty}$. We will present them in a row matrix as requested.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{\\sum_{j=1}^{p} b_j^2 \\mathbb{I}(|b_j| \\le \\lambda) + \\lambda^{2} \\sum_{j=1}^{p} \\mathbb{I}(|b_j| > \\lambda)} & \\min(\\lambda, \\|b\\|_{\\infty}) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A crucial property of any statistical estimator is its behavior under simple data transformations, such as scaling the predictors. This practice explores the concept of scaling equivariance for the Dantzig selector, asking you to determine how the problem's objective and constraints must be re-weighted to ensure the underlying solution remains consistent when a column of the design matrix is rescaled. Mastering this concept is key to applying the method correctly and understanding its relationship to other estimators like the LASSO, which possesses this property naturally .",
            "id": "3487301",
            "problem": "Consider a linear observation model $y = A \\beta^{\\star} + \\varepsilon$ with design matrix $A \\in \\mathbb{R}^{n \\times p}$, parameter vector $\\beta^{\\star} \\in \\mathbb{R}^{p}$, and noise $\\varepsilon \\in \\mathbb{R}^{n}$. The Dantzig selector is defined as the solution to\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|A^{\\top}(y - A \\beta)\\|_{\\infty} \\le \\tau,\n$$\nwhere $\\tau > 0$ is a given tolerance. Now, modify the design matrix by scaling a single column $A_{j} \\in \\mathbb{R}^{n}$ by a nonzero scalar $c \\in \\mathbb{R} \\setminus \\{0\\}$, producing $A^{(c)}$ whose $j$-th column is $A_{j}^{(c)} = c A_{j}$ and all other columns are unchanged. Define the diagonal scaling matrix $S(c) \\in \\mathbb{R}^{p \\times p}$ by $S(c)_{jj} = c$ and $S(c)_{kk} = 1$ for $k \\ne j$. Consider the following weighted Dantzig selector posed with the scaled design:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\sum_{k=1}^{p} w_{k}(c)\\,|\\beta_{k}| \\quad \\text{subject to} \\quad \\left\\|U(c)^{-1}\\,(A^{(c)})^{\\top}\\bigl(y - A^{(c)} \\beta\\bigr)\\right\\|_{\\infty} \\le \\tau,\n$$\nwhere $w_{k}(c) > 0$ are given objective weights and $U(c) \\in \\mathbb{R}^{p \\times p}$ is a diagonal matrix with diagonal entries $u_{k}(c) > 0$. Your task is to determine explicit expressions for the pair of weights on the scaled coordinate, $w_{j}(c)$ and $u_{j}(c)$, such that the weighted problem with $(A^{(c)}, w(c), U(c))$ is solution-invariant to column scaling in the following sense:\n1) For any $y \\in \\mathbb{R}^{n}$ and any $\\tau > 0$, if $\\widehat{\\beta}$ solves the unweighted problem with $A$ above, then $\\widehat{\\beta}^{(c)} := S(c)^{-1}\\widehat{\\beta}$ solves the weighted problem with $A^{(c)}$, $w(c)$, and $U(c)$.\n2) The invariance must hold without changing $\\tau$ and without modifying any data other than $A_{j} \\leftarrow c A_{j}$ and the specified weights.\n\nAssume the weights on all unscaled coordinates $k \\ne j$ are fixed to $w_{k}(c) = 1$ and $u_{k}(c) = 1$. Provide your final answer as the pair of analytic expressions for the scaled-coordinate weights $\\bigl(w_{j}(c), u_{j}(c)\\bigr)$, written as a single $1 \\times 2$ row matrix. No numerical approximation is required or permitted, and no units are involved.",
            "solution": "The user wants to find the weights $w_j(c)$ and $u_j(c)$ that make the Dantzig selector optimization problem invariant to the scaling of a single column of the design matrix $A$.\n\nLet the original Dantzig selector problem be denoted by $\\text{P}_1$:\n$$\n\\text{P}_1: \\quad \\min_{\\beta \\in \\mathbb{R}^{p}} \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|A^{\\top}(y - A \\beta)\\|_{\\infty} \\le \\tau\n$$\nLet $\\widehat{\\beta}$ be a solution to $\\text{P}_1$.\n\nThe modified design matrix $A^{(c)}$ is formed by scaling the $j$-th column of $A$ by a non-zero scalar $c$. That is, $A_{j}^{(c)} = c A_{j}$, and $A_{k}^{(c)} = A_{k}$ for all $k \\ne j$. Let $S(c)$ be the diagonal matrix with $S(c)_{jj} = c$ and $S(c)_{kk} = 1$ for $k \\ne j$. Then the modified design matrix can be expressed compactly as $A^{(c)} = A S(c)$.\n\nThe weighted Dantzig selector problem with the scaled design matrix, which we denote by $\\text{P}_2$, is given as:\n$$\n\\text{P}_2: \\quad \\min_{\\tilde{\\beta} \\in \\mathbb{R}^{p}} \\sum_{k=1}^{p} w_{k}(c)\\,|\\tilde{\\beta}_{k}| \\quad \\text{subject to} \\quad \\left\\|U(c)^{-1}\\,(A^{(c)})^{\\top}\\bigl(y - A^{(c)} \\tilde{\\beta}\\bigr)\\right\\|_{\\infty} \\le \\tau\n$$\nI use $\\tilde{\\beta}$ here to distinguish the optimization variable of $\\text{P}_2$ from that of $\\text{P}_1$. We are given that for $k \\ne j$, the weights are fixed at $w_{k}(c) = 1$ and $u_{k}(c) = 1$. The diagonal matrix $U(c)$ has entries $U(c)_{kk} = u_k(c)$.\n\nThe problem requires that if $\\widehat{\\beta}$ is a solution to $\\text{P}_1$, then $\\widehat{\\beta}^{(c)} = S(c)^{-1}\\widehat{\\beta}$ must be a solution to $\\text{P}_2$. This implies that the two optimization problems must be equivalent under the change of variables $\\beta = S(c)\\tilde{\\beta}$ (which is equivalent to $\\tilde{\\beta} = S(c)^{-1}\\beta$).\n\nWe will transform problem $\\text{P}_1$ with the substitution $\\beta = S(c)\\tilde{\\beta}$ and require the resulting problem in the variable $\\tilde{\\beta}$ to be identical to $\\text{P}_2$.\n\n**1. Transform the objective function:**\nThe objective function of $\\text{P}_1$ is $\\|\\beta\\|_{1}$. Substituting $\\beta = S(c)\\tilde{\\beta}$, we get:\n$$\n\\|\\beta\\|_{1} = \\|S(c)\\tilde{\\beta}\\|_{1} = \\sum_{k=1}^{p} |(S(c)\\tilde{\\beta})_{k}| = \\sum_{k=1}^{p} |S(c)_{kk} \\tilde{\\beta}_{k}|\n$$\nSince $S(c)_{jj} = c$ and $S(c)_{kk} = 1$ for $k \\ne j$, the objective becomes:\n$$\n|S(c)_{jj} \\tilde{\\beta}_{j}| + \\sum_{k \\ne j} |S(c)_{kk} \\tilde{\\beta}_{k}| = |c||\\tilde{\\beta}_{j}| + \\sum_{k \\ne j} |\\tilde{\\beta}_{k}|\n$$\nNow, we compare this to the objective function of $\\text{P}_2$:\n$$\n\\sum_{k=1}^{p} w_{k}(c)\\,|\\tilde{\\beta}_{k}| = w_{j}(c)|\\tilde{\\beta}_{j}| + \\sum_{k \\ne j} w_{k}(c)|\\tilde{\\beta}_{k}|\n$$\nGiven $w_{k}(c) = 1$ for $k \\ne j$, this is $w_{j}(c)|\\tilde{\\beta}_{j}| + \\sum_{k \\ne j} |\\tilde{\\beta}_{k}|$.\nFor the two objectives to be identical for any choice of $\\tilde{\\beta}$, their coefficients must match. This yields the condition for the weight $w_j(c)$:\n$$\nw_{j}(c) = |c|\n$$\nSince $c \\in \\mathbb{R} \\setminus \\{0\\}$, we have $|c|>0$, which is consistent with the requirement $w_k(c) > 0$.\n\n**2. Transform the constraint:**\nThe constraint of $\\text{P}_1$ is $\\|A^{\\top}(y - A \\beta)\\|_{\\infty} \\le \\tau$. We substitute $\\beta = S(c)\\tilde{\\beta}$:\n$$\n\\|A^{\\top}(y - A S(c)\\tilde{\\beta})\\|_{\\infty} \\le \\tau\n$$\nUsing the relation $A^{(c)} = A S(c)$, the term inside the parenthesis becomes $y - A^{(c)}\\tilde{\\beta}$. So the transformed constraint is:\n$$\n\\|A^{\\top}(y - A^{(c)}\\tilde{\\beta})\\|_{\\infty} \\le \\tau\n$$\nNow, let's compare this with the constraint of $\\text{P}_2$:\n$$\n\\left\\|U(c)^{-1}\\,(A^{(c)})^{\\top}\\bigl(y - A^{(c)} \\tilde{\\beta}\\bigr)\\right\\|_{\\infty} \\le \\tau\n$$\nTo relate the two constraints, we examine the relationship between $A^{\\top}$ and $(A^{(c)})^{\\top}$.\n$$\n(A^{(c)})^{\\top} = (A S(c))^{\\top} = S(c)^{\\top}A^{\\top}\n$$\nSince $S(c)$ is a diagonal matrix, it is symmetric, so $S(c)^{\\top} = S(c)$. Therefore:\n$$\n(A^{(c)})^{\\top} = S(c)A^{\\top} \\implies A^{\\top} = S(c)^{-1}(A^{(c)})^{\\top}\n$$\nwhere $S(c)^{-1}$ is a diagonal matrix with $(S(c)^{-1})_{jj} = 1/c$ and $(S(c)^{-1})_{kk} = 1$ for $k \\ne j$.\n\nSubstituting $A^{\\top} = S(c)^{-1}(A^{(c)})^{\\top}$ into the transformed constraint from $\\text{P}_1$:\n$$\n\\|S(c)^{-1}(A^{(c)})^{\\top}(y - A^{(c)}\\tilde{\\beta})\\|_{\\infty} \\le \\tau\n$$\nFor this constraint to be equivalent to the constraint of $\\text{P}_2$, which is $\\|U(c)^{-1}(A^{(c)})^{\\top}(y - A^{(c)}\\tilde{\\beta})\\|_{\\infty} \\le \\tau$, for any $\\tilde{\\beta}$, the linear operators on the vector $v = (A^{(c)})^{\\top}(y - A^{(c)}\\tilde{\\beta})$ inside the norm must be equivalent in a way that preserves the $\\ell_\\infty$-norm ball.\nSpecifically, we must have the feasible sets defined by the two constraints be identical. This means that for any vector $v \\in \\mathbb{R}^p$, we must have $\\|S(c)^{-1} v\\|_{\\infty} = \\|U(c)^{-1} v\\|_{\\infty}$.\n$$\n\\max_{k} |(S(c)^{-1}v)_k| = \\max_{k} |(U(c)^{-1}v)_k|\n$$\n$$\n\\max_{k} \\frac{1}{|s_k(c)|}|v_k| = \\max_{k} \\frac{1}{u_k(c)}|v_k|\n$$\nwhere $s_k(c)$ are the diagonal entries of $S(c)$ and we have used $u_k(c)>0$. This equality must hold for any vector $v$, which implies the coefficients must be equal for each component $k$:\n$$\n\\frac{1}{|s_k(c)|} = \\frac{1}{u_k(c)} \\implies u_k(c) = |s_k(c)|\n$$\nWe check this condition for all $k$:\n- For $k \\ne j$, $s_k(c) = 1$, so we require $u_k(c) = |1| = 1$. This matches the given condition $u_k(c)=1$ for unscaled coordinates.\n- For $k = j$, $s_j(c) = c$, so we require $u_j(c) = |c|$.\n\nThis gives the expression for the weight $u_j(c)$. Since $c \\ne 0$, we have $|c|>0$, consistent with the requirement $u_k(c)>0$.\n\nIn summary, for the weighted Dantzig selector to be solution-invariant to column scaling in the specified sense, the weights on the scaled coordinate $j$ must be:\n$$w_j(c) = |c|$$\n$$u_j(c) = |c|$$\n\nThe pair of weights $(w_j(c), u_j(c))$ is therefore $(|c|, |c|)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n|c| & |c|\n\\end{pmatrix}\n}\n$$"
        }
    ]
}