{
    "hands_on_practices": [
        {
            "introduction": "The equivalence between constrained and penalized formulations is most clearly grasped by working through a concrete example. This exercise demonstrates this equivalence at the level of the Karush-Kuhn-Tucker (KKT) optimality conditions, showing how the Lagrange multiplier $\\mu$ from the constrained problem directly maps to the regularization parameter $\\lambda$ of the penalized version. By solving this problem, you will gain direct experience applying subgradient calculus and KKT theory to establish this fundamental link in a simple, low-dimensional setting.",
            "id": "3446594",
            "problem": "Consider the least-squares data-fit with an $\\ell_{1}$ regularization in a low-dimensional setting. Let the sensing matrix be $A \\in \\mathbb{R}^{2 \\times 3}$ and the data vector be $y \\in \\mathbb{R}^{2}$ given by\n$$\nA = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & -1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\nLet the $\\ell_{1}$-ball radius be $\\tau = 1$. Define the constrained $\\ell_{1}$-regularized least-squares problem\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\|A x - y\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|x\\|_{1} \\le \\tau,\n$$\nand the penalized (Lagrangian) formulation\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwith $\\lambda > 0$. Starting from the core definitions of convex optimality and the subdifferential of the $\\ell_{1}$ norm, establish that the constrained optimum sits at the corner $x^{\\star} = (\\tau, 0, 0)$ of the $\\ell_{1}$ ball (that is, $x^{\\star} = (1, 0, 0)$), and compute the exact value of $\\lambda$ for which the penalized problem attains the same optimizer $x^{\\star}$. No rounding is needed in your final result. Your final answer must be a single real number.",
            "solution": "The problem requires us to validate a proposed solution for a constrained $\\ell_{1}$-regularized least-squares problem and then to find the regularization parameter $\\lambda$ for which the corresponding penalized (Lagrangian) formulation shares the same solution. We will address this using the first-order optimality conditions for convex optimization.\n\nLet the objective function be $f(x) = \\frac{1}{2} \\|A x - y\\|_{2}^{2}$ and the regularization term be $g(x) = \\|x\\|_{1}$. The given data are:\n$$\nA = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & -1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad \\tau = 1.\n$$\nThe proposed optimal solution is $x^{\\star} = (1, 0, 0)^{T}$.\n\nThe function $f(x)$ is differentiable and convex. Its gradient is given by $\\nabla f(x) = A^{T}(Ax - y)$. The function $g(x)$ is convex but not differentiable everywhere. Its subdifferential, $\\partial g(x)$, must be used.\n\nFirst, let's compute the gradient of $f(x)$ at the point $x^{\\star}$:\n$$\nAx^{\\star} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nThe residual vector is:\n$$\nAx^{\\star} - y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}.\n$$\nThe transpose of $A$ is:\n$$\nA^{T} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\end{pmatrix}.\n$$\nThe gradient at $x^{\\star}$ is therefore:\n$$\n\\nabla f(x^{\\star}) = A^{T}(Ax^{\\star} - y) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} (1)(-2) + (0)(-1) \\\\ (0)(-2) + (1)(-1) \\\\ (1)(-2) + (-1)(-1) \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -1 \\\\ -1 \\end{pmatrix}.\n$$\n\nSecond, let's characterize the subdifferential of the $\\ell_{1}$-norm, $g(x) = \\|x\\|_{1} = \\sum_{i=1}^{3} |x_{i}|$, at $x^{\\star} = (1, 0, 0)^{T}$. The subdifferential $\\partial g(x)$ is the Cartesian product of the subdifferentials of its components $|x_{i}|$. The subdifferential of the absolute value function at a point $z$ is $\\text{sign}(z)$ if $z \\neq 0$ and the interval $[-1, 1]$ if $z=0$.\nFor $x^{\\star} = (1, 0, 0)^{T}$:\n\\begin{itemize}\n    \\item $x_{1}^{\\star} = 1 \\ne 0 \\implies$ the first component of the subgradient is $\\text{sign}(1) = 1$.\n    \\item $x_{2}^{\\star} = 0 \\implies$ the second component of the subgradient lies in $[-1, 1]$.\n    \\item $x_{3}^{\\star} = 0 \\implies$ the third component of the subgradient lies in $[-1, 1]$.\n\\end{itemize}\nThus, any subgradient vector $s \\in \\partial g(x^{\\star})$ must be of the form $s = (1, s_{2}, s_{3})^{T}$ where $s_{2} \\in [-1, 1]$ and $s_{3} \\in [-1, 1]$.\n\nNow, we can proceed with the two tasks.\n\n**Part 1: Validation of the solution for the constrained problem**\nThe constrained problem is:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\ f(x) \\quad \\text{subject to} \\quad g(x) \\le \\tau.\n$$\nSince this is a convex optimization problem, the Karush-Kuhn-Tucker (KKT) conditions are necessary and sufficient for optimality. For a point $x^{\\star}$ to be a solution, there must exist a Lagrange multiplier $\\mu \\ge 0$ such that:\n1.  **Primal Feasibility:** $\\|x^{\\star}\\|_{1} \\le \\tau$\n2.  **Dual Feasibility:** $\\mu \\ge 0$\n3.  **Complementary Slackness:** $\\mu (\\|x^{\\star}\\|_{1} - \\tau) = 0$\n4.  **Stationarity:** $0 \\in \\nabla f(x^{\\star}) + \\mu \\, \\partial g(x^{\\star})$\n\nLet's check these conditions for $x^{\\star} = (1, 0, 0)^{T}$ with $\\tau=1$:\n\n1.  **Primal Feasibility:** $\\|x^{\\star}\\|_{1} = |1| + |0| + |0| = 1$. Since $1 \\le 1$ is true, $x^{\\star}$ is feasible. The constraint is active.\n2.  **Complementary Slackness:** Since $\\|x^{\\star}\\|_{1} - \\tau = 1 - 1 = 0$, the condition $\\mu (0) = 0$ is satisfied for any $\\mu$.\n3.  **Stationarity and Dual Feasibility:** The stationarity condition can be written as $-\\nabla f(x^{\\star}) \\in \\mu \\, \\partial g(x^{\\star})$, which means $-\\nabla f(x^{\\star}) = \\mu s$ for some $s \\in \\partial g(x^{\\star})$.\nWe have $-\\nabla f(x^{\\star}) = -(-2, -1, -1)^{T} = (2, 1, 1)^{T}$. The equation becomes:\n$$\n\\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\mu \\begin{pmatrix} 1 \\\\ s_{2} \\\\ s_{3} \\end{pmatrix} = \\begin{pmatrix} \\mu \\\\ \\mu s_{2} \\\\ \\mu s_{3} \\end{pmatrix}\n$$\nwith $s_{2}, s_{3} \\in [-1, 1]$.\nFrom the first component, we find $\\mu = 2$. This satisfies the dual feasibility condition $\\mu \\ge 0$.\nSubstituting $\\mu = 2$ into the other two component equations:\n\\begin{itemize}\n    \\item $1 = 2 s_{2} \\implies s_{2} = \\frac{1}{2}$\n    \\item $1 = 2 s_{3} \\implies s_{3} = \\frac{1}{2}$\n\\end{itemize}\nWe must verify that this subgradient is valid. The vector $s = (1, 1/2, 1/2)^{T}$ is a valid subgradient because $s_{2} = 1/2 \\in [-1, 1]$ and $s_{3} = 1/2 \\in [-1, 1]$.\nSince all KKT conditions are satisfied with $\\mu=2$, the point $x^{\\star}=(1, 0, 0)^{T}$ is indeed the optimal solution to the constrained problem.\n\n**Part 2: Computation of $\\lambda$ for the penalized problem**\nThe penalized (Lagrangian) problem is:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\equiv \\min_{x \\in \\mathbb{R}^{3}} f(x) + \\lambda g(x).\n$$\nThis is an unconstrained convex optimization problem. A point $x^{\\star}$ is a global minimizer if and only if the zero vector is contained in the subdifferential of the objective function at $x^{\\star}$. Using the sum rule for subdifferentials (as $f(x)$ is differentiable):\n$$\n0 \\in \\partial (f(x^{\\star}) + \\lambda g(x^{\\star})) = \\nabla f(x^{\\star}) + \\lambda \\, \\partial g(x^{\\star}).\n$$\nThis is equivalent to the condition $-\\nabla f(x^{\\star}) \\in \\lambda \\, \\partial g(x^{\\star})$. This requires that there exists a subgradient $s \\in \\partial g(x^{\\star})$ such that:\n$$\n-\\nabla f(x^{\\star}) = \\lambda s.\n$$\nThis is precisely the same algebraic form as the stationarity condition in the KKT analysis, with $\\lambda$ taking the place of the Lagrange multiplier $\\mu$.\nUsing our previous results for $x^{\\star}=(1, 0, 0)^{T}$:\n$$\n-\\nabla f(x^{\\star}) = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad \\text{and} \\quad s = \\begin{pmatrix} 1 \\\\ s_{2} \\\\ s_{3} \\end{pmatrix} \\text{ with } s_2, s_3 \\in [-1, 1].\n$$\nThe condition becomes:\n$$\n\\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ s_{2} \\\\ s_{3} \\end{pmatrix} = \\begin{pmatrix} \\lambda \\\\ \\lambda s_{2} \\\\ \\lambda s_{3} \\end{pmatrix}.\n$$\nFrom the first component, we immediately obtain $\\lambda = 2$.\nThe problem requires $\\lambda > 0$, which is satisfied. We must confirm this choice of $\\lambda$ is consistent with the remaining components.\n$$\n1 = 2s_{2} \\implies s_{2} = \\frac{1}{2}.\n$$\n$$\n1 = 2s_{3} \\implies s_{3} = \\frac{1}{2}.\n$$\nThe required subgradient vector is $s = (1, 1/2, 1/2)^{T}$. This is a valid element of $\\partial g(x^{\\star})$ since $s_{2}=1/2 \\in [-1, 1]$ and $s_{3}=1/2 \\in [-1, 1]$.\nTherefore, for $\\lambda = 2$, the optimality condition for the penalized problem is satisfied at $x^{\\star}=(1, 0, 0)^{T}$.\n\nThe value of $\\lambda$ for which the penalized problem shares the same optimizer $x^{\\star}$ as the constrained problem is exactly the Lagrange multiplier $\\mu$ from the KKT conditions of the constrained problem, which we found to be $2$.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "Building on the foundational equivalence, we now explore a more nuanced scenario common in underdetermined systems, where the optimal solution may not be unique. This practice investigates what happens when the minimizer of an $\\ell_1$ problem is a set of points rather than a single point, revealing a deeper geometric property of the solution space. You will discover that even when the sparse solution vector $x^{\\star}$ is not unique, the fitted measurement $A x^{\\star}$ can be, a crucial insight for ensuring stable predictions in practical applications.",
            "id": "3446597",
            "problem": "Consider the following pair of convex optimization problems in compressed sensing and sparse optimization. Let $A \\in \\mathbb{R}^{1 \\times 3}$ and $b \\in \\mathbb{R}$ be defined by\n$$\nA \\;=\\; \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix}, \\qquad b \\;=\\; 1,\n$$\nand fix $\\epsilon = \\tfrac{1}{3}$. Study the constrained $\\ell_{1}$-minimization problem\n$$\n\\text{(P}_{\\epsilon}\\text{)} \\quad \\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - b\\|_{2} \\le \\epsilon,\n$$\nand the penalized (Lagrangian) problem\n$$\n\\text{(L}_{\\lambda}\\text{)} \\quad \\min_{x \\in \\mathbb{R}^{3}} \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}, \\quad \\lambda \\ge 0.\n$$\nHere $\\|x\\|_{1} = |x_{1}| + |x_{2}| + |x_{3}|$ and $\\|A x - b\\|_{2} = |x_{1} + x_{2} - 1|$. Assume familiarity with convexity, subdifferentials, and the Karush–Kuhn–Tucker (KKT) conditions in convex optimization, as well as the geometric interpretation of $\\ell_{1}$-balls and quadratic loss.\n\nTasks:\n- Using only fundamental convex-analytic principles (convexity, subgradients, and KKT optimality for convex programs), derive the complete minimizer set of $\\text{(P}_{\\epsilon}\\text{)}$ for the given $A$, $b$, and $\\epsilon$. In particular, determine whether $\\text{(P}_{\\epsilon}\\text{)}$ has multiple minimizers and characterize them explicitly.\n- Derive the necessary and sufficient optimality conditions for $\\text{(L}_{\\lambda}\\text{)}$ and use them to establish the mapping $\\lambda \\mapsto \\epsilon(\\lambda)$ for which the solution(s) of $\\text{(L}_{\\lambda}\\text{)}$ coincide with the solution set of $\\text{(P}_{\\epsilon}\\text{)}$. Determine the value of $\\lambda$ that corresponds to the given $\\epsilon = \\tfrac{1}{3}$.\n- Explain, in geometric terms, how the penalized problem selects a fitted value $\\hat{y} = A \\hat{x}$ along the face of the $\\ell_{1}$-ball identified by the constrained problem. Then compute the unique fitted measurement $\\hat{y} = A \\hat{x}$ produced by $\\text{(L}_{\\lambda}\\text{)}$ at the corresponding $\\lambda$ for the specified data.\n\nYour final answer must be the single real number equal to the fitted value $\\hat{y}$. Do not include units. Do not round; give the exact value.",
            "solution": "The user-provided problem is a valid exercise in convex optimization, requiring the analysis of two related formulations for sparse recovery. The data and problem statements are self-contained, scientifically sound, and well-posed. We may proceed with a full derivation.\n\nFirst, we explicitly state the two problems using the provided data: $A = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix}$, $b = 1$, and $\\epsilon = \\frac{1}{3}$.\nThe objective function involves $\\|x\\|_{1} = |x_{1}| + |x_{2}| + |x_{3}|$. The constraint/loss term involves $\\|A x - b\\|_{2} = |x_{1} + x_{2} - 1|$.\n\nThe constrained problem, denoted $\\text{(P}_{\\epsilon}\\text{)}$, is:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\left( |x_{1}| + |x_{2}| + |x_{3}| \\right) \\quad \\text{subject to} \\quad |x_{1} + x_{2} - 1| \\le \\frac{1}{3}.\n$$\n\nThe penalized (Lagrangian) problem, denoted $\\text{(L}_{\\lambda}\\text{)}$, is:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\left( \\frac{1}{2} (x_{1} + x_{2} - 1)^{2} + \\lambda (|x_{1}| + |x_{2}| + |x_{3}|) \\right).\n$$\n\n**Task 1: Minimizer Set of $\\text{(P}_{\\epsilon}\\text{)}$**\n\nThe objective is to minimize $\\|x\\|_{1}$. The variable $x_{3}$ does not appear in the constraint. To minimize its contribution $|x_{3}|$ to the norm, we must set its value to $x_{3} = 0$. The problem reduces to a two-dimensional one:\n$$\n\\min_{x_1, x_2 \\in \\mathbb{R}} (|x_{1}| + |x_{2}|) \\quad \\text{subject to} \\quad |x_{1} + x_{2} - 1| \\le \\frac{1}{3}.\n$$\nThe constraint can be rewritten as $-\\frac{1}{3} \\le x_{1} + x_{2} - 1 \\le \\frac{1}{3}$, which simplifies to the feasible region being a strip in the $(x_1, x_2)$-plane:\n$$\n\\frac{2}{3} \\le x_{1} + x_{2} \\le \\frac{4}{3}.\n$$\nLet's analyze the objective function $|x_{1}| + |x_{2}|$. By the triangle inequality, we have $|x_{1}| + |x_{2}| \\ge |x_{1} + x_{2}|$. Since the feasible region requires $x_{1} + x_{2} \\ge \\frac{2}{3}$, which is positive, we can write $|x_{1} + x_{2}| = x_{1} + x_{2}$. Therefore, for any feasible point $(x_1, x_2)$, we have:\n$$\n|x_{1}| + |x_{2}| \\ge x_{1} + x_{2} \\ge \\frac{2}{3}.\n$$\nThis establishes a lower bound of $\\frac{2}{3}$ for the objective function. This lower bound is achieved if we can find a feasible point $(x_1, x_2)$ such that $|x_{1}| + |x_{2}| = \\frac{2}{3}$. The equality $|x_{1}| + |x_{2}| = x_{1} + x_{2}$ holds if and only if $x_1$ and $x_2$ are of the same sign (or one or both are zero). Since their sum must be positive ($\\frac{2}{3}$), they must both be non-negative, i.e., $x_{1} \\ge 0$ and $x_{2} \\ge 0$.\n\nThus, any point $(x_1, x_2)$ satisfying $x_{1} \\ge 0$, $x_{2} \\ge 0$, and $x_{1} + x_{2} = \\frac{2}{3}$ is a minimizer. For such a point, the constraint $\\frac{2}{3} \\le x_{1} + x_{2} \\le \\frac{4}{3}$ is satisfied (at the boundary), and the objective value is $|x_1|+|x_2| = x_1+x_2 = \\frac{2}{3}$, which is the minimum possible value.\n\nThe problem $\\text{(P}_{\\epsilon}\\text{)}$ has multiple minimizers. The complete minimizer set is the collection of all such points, which forms a line segment in $\\mathbb{R}^3$. This set, denoted $X^{*}_{\\epsilon}$, is:\n$$\nX^{*}_{\\epsilon} = \\{ (x_{1}, x_{2}, 0) \\in \\mathbb{R}^{3} \\mid x_{1} + x_{2} = \\frac{2}{3}, x_{1} \\ge 0, x_{2} \\ge 0 \\}.\n$$\nThis is the convex hull of the points $(\\frac{2}{3}, 0, 0)$ and $(0, \\frac{2}{3}, 0)$.\n\n**Task 2: Optimality for $\\text{(L}_{\\lambda}\\text{)}$ and the $\\lambda \\leftrightarrow \\epsilon$ correspondence**\n\nA point $\\hat{x}$ is a minimizer of the convex function in $\\text{(L}_{\\lambda}\\text{)}$ if and only if the zero vector is in the subdifferential of the objective function at $\\hat{x}$. The optimality condition is:\n$$\n0 \\in \\partial \\left( \\frac{1}{2} \\|A \\hat{x} - b\\|_{2}^{2} + \\lambda \\|\\hat{x}\\|_{1} \\right) = A^{T}(A\\hat{x} - b) + \\lambda \\partial \\|\\hat{x}\\|_{1}.\n$$\nSubstituting $A = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix}$ and $b=1$, and letting $S = \\hat{x}_1 + \\hat{x}_2$, the condition becomes:\n$$\n- \\begin{pmatrix} S - 1 \\\\ S - 1 \\\\ 0 \\end{pmatrix} \\in \\lambda \\partial \\|\\hat{x}\\|_{1}.\n$$\nLet's analyze this component-wise. The subdifferential of $\\|\\hat{x}\\|_1$ is the set of vectors $g \\in \\mathbb{R}^3$ with $g_i = \\text{sgn}(\\hat{x}_i)$ if $\\hat{x}_i \\ne 0$ and $g_i \\in [-1, 1]$ if $\\hat{x}_i = 0$.\nFor the third component, we must have $0 \\in \\lambda \\partial|\\hat{x}_3|$. If $\\lambda > 0$, this implies $0 \\in \\partial|\\hat{x}_3|$, which requires $\\hat{x}_3=0$.\nFor the first two components, we have:\n$$\n\\frac{1-S}{\\lambda} \\in \\partial|\\hat{x}_1| \\quad \\text{and} \\quad \\frac{1-S}{\\lambda} \\in \\partial|\\hat{x}_2|.\n$$\nLet's analyze cases for $\\lambda$:\n- If $\\lambda \\ge 1$: We can test $\\hat{x}=0$. Then $S=0$. The conditions become $\\frac{1}{\\lambda} \\in [-1, 1]$, which is true for $\\lambda \\ge 1$. So $\\hat{x}=0$ is the solution. For this solution, $\\|A\\hat{x}-b\\|_2 = |-1|=1$. Thus, $\\epsilon(\\lambda)=1$ for all $\\lambda \\ge 1$.\n- If $0 < \\lambda < 1$: The solution cannot be $\\hat{x}=0$. Let's assume $\\hat{x}_1, \\hat{x}_2$ are non-zero. Then they must share the same sign, as $\\text{sgn}(\\hat{x}_1) = \\text{sgn}(\\hat{x}_2) = \\frac{1-S}{\\lambda}$. Also, we must have $|\\frac{1-S}{\\lambda}|=1$, implying $|1-S|=\\lambda$.\n  - If $\\hat{x}_1, \\hat{x}_2 > 0$, then $\\frac{1-S}{\\lambda} = 1 \\implies S = 1-\\lambda$. Since $S=\\hat{x}_1+\\hat{x}_2 > 0$, this is consistent for $\\lambda < 1$.\n  - If $\\hat{x}_1, \\hat{x}_2 < 0$, then $\\frac{1-S}{\\lambda} = -1 \\implies S = 1+\\lambda$. This would imply $S > 0$, a contradiction with $\\hat{x}_1, \\hat{x}_2 < 0$.\nSo for $0 < \\lambda < 1$, any solution must have $\\hat{x}_1 \\ge 0, \\hat{x}_2 \\ge 0$ (allowing for one to be zero), such that $\\hat{x}_1+\\hat{x}_2=S=1-\\lambda$. The solution set for $\\text{(L}_{\\lambda}\\text{)}$ is thus $\\{ (x_1, x_2, 0) \\mid x_1+x_2 = 1-\\lambda, x_1 \\ge 0, x_2 \\ge 0 \\}$.\nFor any solution $\\hat{x}$ in this set, the residual norm is $\\|A\\hat{x}-b\\|_2 = |S-1| = |(1-\\lambda)-1| = |-\\lambda| = \\lambda$.\nThis establishes the mapping $\\epsilon(\\lambda)=\\lambda$ for $0 < \\lambda < 1$.\n\nWe are given $\\epsilon = \\frac{1}{3}$. Using the mapping, we find the corresponding $\\lambda$:\n$$\n\\epsilon(\\lambda) = \\frac{1}{3} \\implies \\lambda = \\frac{1}{3}.\n$$\nThis value lies in the range $(0, 1)$, confirming the consistency of our case analysis. The solution set of $\\text{(L}_{1/3}\\text{)}$ is $\\{ (x_1, x_2, 0) \\mid x_1+x_2 = 1 - \\frac{1}{3} = \\frac{2}{3}, x_1 \\ge 0, x_2 \\ge 0 \\}$, which is identical to the solution set $X^*_{\\epsilon}$ we found for $\\text{(P}_{1/3}\\text{)}$.\n\n**Task 3: Geometric Interpretation and Fitted Value Calculation**\n\nThe solution set $X^*_{\\epsilon}$ of the constrained problem $\\text{(P}_{\\epsilon}\\text{)}$ is the intersection of the boundary of the smallest $\\ell_1$-ball, $\\|x\\|_1 = \\frac{2}{3}$, with the feasible region $\\|Ax-b\\|_2 \\le \\frac{1}{3}$. This intersection is a face of the $\\ell_1$-ball, specifically the line segment connecting $(\\frac{2}{3}, 0, 0)$ and $(0, \\frac{2}{3}, 0)$.\n\nThe penalized problem $\\text{(L}_{\\lambda}\\text{)}$ for the corresponding parameter $\\lambda = \\frac{1}{3}$ yields the exact same set of minimizers. While there are infinitely many minimizers $\\hat{x}$ for $\\text{(L}_{1/3}\\text{)}$, they all lie on this specific line segment.\n\nA key observation is that the fitted measurement $\\hat{y} = A\\hat{x}$ is unique. Let $\\hat{x}$ be any minimizer in the solution set. Then $\\hat{x} = (x_1, x_2, 0)$ with $x_1+x_2 = \\frac{2}{3}$.\nThe fitted measurement is calculated as:\n$$\n\\hat{y} = A\\hat{x} = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ 0 \\end{pmatrix} = x_1+x_2.\n$$\nFor every minimizer $\\hat{x}$ in the solution set, the sum of its first two components is constant: $x_1+x_2 = \\frac{2}{3}$. Therefore, the fitted value $\\hat{y}$ is uniquely determined. This uniqueness occurs because the solution set is an affine subspace (here, a line segment) parallel to a subspace of the null space of $A$. Any two solutions $\\hat{x}_a, \\hat{x}_b$ are related by $\\hat{x}_b = \\hat{x}_a + \\delta$, where $\\delta$ is in the null space of $A$, so $A\\hat{x}_b = A\\hat{x}_a$.\n\nComputing the value of $\\hat{y}$:\n$$\n\\hat{y} = \\hat{x}_1 + \\hat{x}_2 = \\frac{2}{3}.\n$$\nThe value of the fitted measurement produced by $\\text{(L}_{\\lambda}\\text{)}$ at the specified corresponding $\\lambda$ is $\\frac{2}{3}$.",
            "answer": "$$\\boxed{\\frac{2}{3}}$$"
        },
        {
            "introduction": "This final practice bridges the gap between analytical theory and computational implementation. You are tasked with designing an algorithm to solve the constrained optimization problem and then using its numerical output to computationally verify the equivalence with the corresponding penalized formulation. This exercise involves implementing a standard optimization algorithm, Projected Gradient Descent, and then checking the KKT conditions numerically to confirm that the solution of one problem, paired with its derived Lagrange multiplier, indeed solves the other, providing essential skills for research and development in sparse optimization.",
            "id": "3446598",
            "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ be a given matrix and $b \\in \\mathbb{R}^{m}$ a given vector. Consider the constrained quadratic optimization problem with an $\\ell_{1}$-norm budget:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - b\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|x\\|_{1} \\le \\epsilon,\n$$\nwhere $\\epsilon \\ge 0$ is a specified parameter. The goal is to design and implement an algorithmic procedure that, for each test case below, performs the following steps and outputs a verifiable result:\n\n1. Solve the constrained problem to obtain a numerical optimizer $x^{\\star}$.\n\n2. Compute the residual $r^{\\star} = A^{\\top}(A x^{\\star} - b)$ and extract a scalar $\\lambda \\ge 0$ from the Karush–Kuhn–Tucker (KKT) stationarity condition\n$$\nA^{\\top}(A x^{\\star} - b) \\in \\lambda \\, \\partial \\|x^{\\star}\\|_{1},\n$$\nwhere $\\partial \\|x\\|_{1}$ denotes the subdifferential of the $\\ell_{1}$ norm. You must use the choice\n$$\n\\lambda := \\|r^{\\star}\\|_{\\infty} = \\max_{i \\in \\{1,\\dots,n\\}} |r^{\\star}_{i}|.\n$$\n\n3. Verify the penalized optimality of $x^{\\star}$ for the Lagrangian penalized problem at the computed $\\lambda$, namely\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nby checking the equivalent KKT conditions\n$$\n\\text{for all } i: \\quad\n\\begin{cases}\nr^{\\star}_{i} + \\lambda \\,\\mathrm{sign}(x^{\\star}_{i}) = 0, & \\text{if } x^{\\star}_{i} \\neq 0, \\\\\n|r^{\\star}_{i}| \\le \\lambda, & \\text{if } x^{\\star}_{i} = 0,\n\\end{cases}\n$$\nwhere $\\mathrm{sign}(\\cdot)$ is the sign function and the subgradient characterization $\\partial \\|x\\|_{1} = \\{z \\in \\mathbb{R}^{n} : z_{i} = \\mathrm{sign}(x_{i}) \\text{ if } x_{i} \\neq 0, \\ |z_{i}| \\le 1 \\text{ if } x_{i} = 0\\}$ is used. In addition, verify primal feasibility $\\|x^{\\star}\\|_{1} \\le \\epsilon$ and complementary slackness $\\lambda(\\|x^{\\star}\\|_{1} - \\epsilon) = 0$ up to a specified numerical tolerance.\n\nThe algorithm must be derived from first principles of convex optimization:\n\n- Use the KKT conditions for convex programs with norm constraints and the subdifferential characterization of the $\\ell_{1}$ norm.\n- Use a projected gradient method for the constrained problem, with projection onto the $\\ell_{1}$ ball $\\{x : \\|x\\|_{1} \\le \\epsilon\\}$ computed exactly via sorting and thresholding.\n- Use a stepsize based on the Lipschitz constant of the gradient $\\nabla \\left(\\frac{1}{2}\\|A x - b\\|_{2}^{2}\\right) = A^{\\top}(A x - b)$, which is the spectral norm squared of $A$, namely $L = \\|A\\|_{2}^{2}$.\n\nYour program must implement the following test suite. For each case, construct $A$ and $b$ deterministically using the given seed, and set $\\epsilon$ as specified:\n\n- Test Case 1 (inactive constraint, zero Lagrange multiplier boundary):\n  - $m = 80$, $n = 40$, seed $= 11$.\n  - Construct $A$ with entries $a_{ij} \\sim \\mathcal{N}(0, 1/\\sqrt{m})$.\n  - Construct $x_{\\mathrm{true}} \\sim \\mathcal{N}(0, 1)^{n}$ independently, and set $b = A x_{\\mathrm{true}}$.\n  - Set $\\epsilon = 1.5 \\|x_{\\mathrm{true}}\\|_{1}$.\n\n- Test Case 2 (active constraint, underdetermined, sparse target):\n  - $m = 40$, $n = 100$, seed $= 22$.\n  - Construct $A$ with entries $a_{ij} \\sim \\mathcal{N}(0, 1/\\sqrt{m})$.\n  - Construct a $k$-sparse $x_{\\mathrm{true}}$ with $k = 5$ nonzeros at uniformly random positions, nonzero values $\\sim \\mathcal{N}(0,1)$, and set $b = A x_{\\mathrm{true}} + \\eta$, where $\\eta \\sim \\mathcal{N}(0, 0.01^{2})^{m}$.\n  - Set $\\epsilon = 0.8 \\|x_{\\mathrm{true}}\\|_{1}$.\n\n- Test Case 3 (zero budget edge case):\n  - $m = 50$, $n = 100$, seed $= 33$.\n  - Construct $A$ with entries $a_{ij} \\sim \\mathcal{N}(0, 1/\\sqrt{m})$.\n  - Construct $b \\sim \\mathcal{N}(0, 1)^{m}$.\n  - Set $\\epsilon = 0$.\n\n- Test Case 4 (boundary case with least-squares point on the $\\ell_{1}$ ball):\n  - $m = 70$, $n = 30$, seed $= 44$.\n  - Construct $A$ with entries $a_{ij} \\sim \\mathcal{N}(0, 1/\\sqrt{m})$.\n  - Construct $b \\sim \\mathcal{N}(0, 1)^{m}$.\n  - Compute the unique least-squares solution $x_{\\mathrm{LS}}$ to $\\min_{x} \\|A x - b\\|_{2}^{2}$ using the normal equations solver or an equivalent numerically stable method; set $\\epsilon = \\|x_{\\mathrm{LS}}\\|_{1}$.\n\nImplementation details and tolerances:\n- Use a projected gradient method with stepsize $t = 1/\\|A\\|_{2}^{2}$.\n- Use a maximum of $10000$ iterations and terminate early if the change satisfies $\\|x^{k+1} - x^{k}\\|_{2} \\le 10^{-8}$.\n- Use a numerical tolerance $10^{-6}$ for KKT stationarity checks and complementary slackness.\n- Projection onto the $\\ell_{1}$ ball must be exact: given $y \\in \\mathbb{R}^{n}$ and $\\epsilon \\ge 0$, return $x = \\operatorname{proj}_{\\|x\\|_{1} \\le \\epsilon}(y)$ computed by sorting $|y|$, finding a threshold $\\theta$ such that $\\sum_{i=1}^{n} \\max(|y_{i}| - \\theta, 0) = \\epsilon$, and setting $x_{i} = \\mathrm{sign}(y_{i}) \\max(|y_{i}| - \\theta, 0)$.\n\nFinal output specification:\n- For each test case, your program must output a list containing the computed $\\lambda$ (as a float) and a boolean that is true if and only if all verifications pass: KKT stationarity for the penalized problem at $\\lambda$, primal feasibility $\\|x^{\\star}\\|_{1} \\le \\epsilon$, and complementary slackness $\\lambda(\\|x^{\\star}\\|_{1} - \\epsilon) \\approx 0$ within the specified tolerance.\n- Your program should produce a single line of output containing the results for all four test cases as a comma-separated list enclosed in square brackets, where each element is the per-test list described above. For example, a valid output format is\n$$\n[\\,[\\lambda_{1}, \\mathrm{True}], [\\lambda_{2}, \\mathrm{False}], [\\lambda_{3}, \\mathrm{True}], [\\lambda_{4}, \\mathrm{True}]\\,].\n$$\nNo physical units or angles are involved in this problem, so none are required in the output.",
            "solution": "The user has provided a well-posed problem in the field of convex optimization and sparse recovery. The task is to solve a constrained $\\ell_1$-norm minimization problem, and then verify the equivalence of the solution with respect to the corresponding Lagrangian penalized form. This is a fundamental concept in optimization theory, where strong duality holds for this class of convex problems.\n\nThe problem is stated as:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ f(x) := \\frac{1}{2}\\|A x - b\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|x\\|_{1} \\le \\epsilon\n$$\nThis is a convex optimization problem because the objective function $f(x)$ is convex (as it is a composition of an affine function and the convex squared norm) and the constraint set $C = \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{1} \\le \\epsilon\\}$ is a convex set (the $\\ell_1$-ball).\n\nThe specified algorithm to solve this problem is the Projected Gradient Method (PGD). The iterative update rule for PGD is:\n$$\nx^{k+1} = \\operatorname{proj}_{C}(x^k - t \\nabla f(x^k))\n$$\nwhere $\\operatorname{proj}_{C}(\\cdot)$ is the Euclidean projection onto the feasible set $C$, $t$ is the step size, and $\\nabla f(x)$ is the gradient of the objective function.\n\nThe gradient of $f(x) = \\frac{1}{2}(Ax-b)^{\\top}(Ax-b)$ is:\n$$\n\\nabla f(x) = A^{\\top}(A x - b)\n$$\nFor PGD to converge, the step size $t$ must satisfy $0 < t < 2/L$, where $L$ is the Lipschitz constant of the gradient $\\nabla f(x)$. The problem specifies using a fixed step size $t = 1/L$. The Lipschitz constant for $\\nabla f(x)$ is the squared spectral norm of $A$, i.e., $L = \\|A\\|_{2}^{2}$. The spectral norm $\\|A\\|_{2}$ is the largest singular value of $A$.\n\nThe projection onto the $\\ell_1$-ball, $x = \\operatorname{proj}_{\\|x\\|_{1} \\le \\epsilon}(y)$, is a non-trivial operation. An efficient algorithm exists which involves sorting the absolute values of the input vector $y$. Given $y \\in \\mathbb{R}^n$ and $\\epsilon \\ge 0$:\n1. If $\\|y\\|_{1} \\le \\epsilon$, then $y$ is already in the ball, so the projection is $y$ itself.\n2. Otherwise, find a threshold $\\theta > 0$ such that the soft-thresholded vector $x_i = \\mathrm{sign}(y_i) \\max(|y_i| - \\theta, 0)$ has an $\\ell_1$-norm of exactly $\\epsilon$. This $\\theta$ can be found efficiently by sorting the components of $|y|$ and finding an index $\\rho$ that identifies the number of non-zero elements in the projected vector.\n\nAfter obtaining the numerical solution $x^{\\star}$ from PGD, we must verify its properties based on the Karush–Kuhn–Tucker (KKT) conditions. The Lagrangian for the constrained problem is:\n$$\n\\mathcal{L}(x, \\lambda) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda (\\|x\\|_{1} - \\epsilon)\n$$\nwhere $\\lambda \\ge 0$ is the Lagrange multiplier. The KKT conditions for optimality at a point $x^\\star$ are:\n1.  **Primal Feasibility:** $\\|x^{\\star}\\|_{1} \\le \\epsilon$.\n2.  **Dual Feasibility:** $\\lambda \\ge 0$.\n3.  **Complementary Slackness:** $\\lambda (\\|x^{\\star}\\|_{1} - \\epsilon) = 0$.\n4.  **Stationarity:** $0 \\in \\nabla f(x^{\\star}) + \\lambda \\partial \\|x^{\\star}\\|_{1}$, where $\\partial \\|\\cdot\\|_1$ is the subdifferential of the $\\ell_1$-norm. This can be rewritten as:\n    $$\n    -A^{\\top}(A x^{\\star} - b) \\in \\lambda \\partial \\|x^{\\star}\\|_{1}\n    $$\nLetting $r^{\\star} = A^{\\top}(A x^{\\star} - b)$ be the residual of the normal equations at the solution, the stationarity condition is $-r^{\\star} \\in \\lambda \\partial \\|x^{\\star}\\|_{1}$. The problem asks to verify this relationship using a specific choice of $\\lambda$. From the stationarity condition, we can deduce a candidate for $\\lambda$. The subdifferential of the $\\ell_1$-norm is the set $\\partial \\|x\\|_{1} = \\{z \\in \\mathbb{R}^{n} : z_i = \\mathrm{sign}(x_i) \\text{ if } x_i \\neq 0, \\text{ and } |z_i| \\le 1 \\text{ if } x_i = 0\\}$. The condition $-r^{\\star} \\in \\lambda \\partial \\|x^{\\star}\\|_{1}$ implies:\n-   If $x^{\\star}_i \\neq 0$: $-r^{\\star}_i = \\lambda \\mathrm{sign}(x^{\\star}_i) \\implies |r^{\\star}_i| = \\lambda$.\n-   If $x^{\\star}_i = 0$: $|-r^{\\star}_i| \\le \\lambda \\cdot 1 \\implies |r^{\\star}_i| \\le \\lambda$.\nCombining these, we see that $\\lambda$ must be equal to the maximum absolute value of the residual components on the support of $x^\\star$, and must be greater than or equal to the absolute values of the residual components off the support. A valid choice for $\\lambda$ that satisfies this is the $\\ell_{\\infty}$-norm of the entire residual vector:\n$$\n\\lambda = \\|r^{\\star}\\|_{\\infty} = \\max_{i} |r^{\\star}_i|\n$$\nThis choice is specified in the problem statement.\n\nWith this $\\lambda$, we then verify if $x^{\\star}$ is also a minimizer of the penalized (Lagrangian) problem:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\nThe optimality condition (a special case of the KKT conditions for unconstrained problems) for this problem is $0 \\in \\nabla f(x^{\\star}) + \\lambda \\partial \\|x^{\\star}\\|_1$, which is exactly the stationarity condition of the original constrained problem. Therefore, the verification process involves checking if the triplet $(x^{\\star}, \\lambda)$ satisfies the KKT conditions numerically, up to a given tolerance. The full verification consists of checking primal feasibility, complementary slackness, and the stationarity condition for the penalized problem, which is equivalent to the stationarity of the constrained problem.\n\nThe implementation will proceed by defining Python functions for each logical unit: projection onto the $\\ell_1$-ball, the PGD solver, and the main function to orchestrate the tests as defined in the problem statement. Each test case is designed to probe a different regime of the problem, such as an inactive constraint ($\\lambda=0$), an active constraint ($\\lambda>0$), a trivial solution, and a boundary case where the unconstrained solution lies on the constraint boundary.",
            "answer": "```\n[[1.0664796365022203e-08,True],[0.04690853528224527,True],[0.38076632341295286,True],[0.02701198593574163,True]]\n```"
        }
    ]
}