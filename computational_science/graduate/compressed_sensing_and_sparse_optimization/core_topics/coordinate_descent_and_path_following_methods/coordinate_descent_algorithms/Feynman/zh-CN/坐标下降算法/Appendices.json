{
    "hands_on_practices": [
        {
            "introduction": "掌握坐标下降法的第一步是为基本的凸优化问题（如无约束最小二乘）实现该算法。这个实践  旨在从第一性原理出发，推导并实现一个高效的坐标下降求解器。通过维护残差向量，你将学习如何利用稀疏性将每次迭代的计算复杂度降低到与矩阵列的非零元素数量成正比，这是处理大规模数据集的一项关键技术。",
            "id": "3441217",
            "problem": "考虑无约束二次最小二乘目标函数 $f(x)=\\tfrac{1}{2}\\lVert Ax - y\\rVert_2^2$，其中 $A \\in \\mathbb{R}^{n \\times p}$，$x \\in \\mathbb{R}^{p}$，$y \\in \\mathbb{R}^{n}$。循环坐标下降法迭代地选择一个坐标 $i \\in \\{1,\\dots,p\\}$，并沿着第 $i$ 个标准向量所张成的一维子空间进行精确最小化。在压缩感知和稀疏优化的背景下，利用 $A$ 列的稀疏性对于实现每次坐标更新的计算量与所选列的非零元素数量成正比至关重要。定义残差 $r = y - Ax$。您的任务是，从第一性原理（一维二次函数最小化的微积分）出发，推导出精确的坐标更新法则。该法则通过维护和更新残差 $r$ 而无需从头重新计算 $Ax$，从而实现每次坐标更新的时间复杂度为 $O(\\lVert a_i\\rVert_0)$，其中 $a_i$ 表示 $A$ 的第 $i$ 列，$\\lVert a_i\\rVert_0$ 表示其非零元素的数量。然后，实现一个循环坐标下降算法，该算法使用此残差维护策略，并对所有坐标执行指定次数的完整遍历。\n\n您必须使用压缩稀疏列（CSC）存储格式来实现该算法，以便高效地访问列 $a_i$ 及其非零模式，并且通过仅更新 $a_i$ 非零位置对应的残差项，使得每次坐标更新的计算时间达到 $O(\\lVert a_i\\rVert_0)$。程序还应计算，对于指定的坐标 $i$，从 $x=0$ 和 $r=y$ 开始执行单次坐标更新时，残差 $r$ 中有多少个条目发生变化。这将反映有效的稀疏感知更新计数（该整数将小于或等于 $\\lVert a_i\\rVert_0$，并且当更新步长在所有非零位置上都非零时等于 $\\lVert a_i\\rVert_0$）。\n\n您的推导必须从 $f(x)$ 和残差 $r$ 的基本定义，以及最小化单变量二次函数的微分基本法则开始。不要使用任何快捷公式；通过将沿所选坐标方向的导数设为零，并用 $a_i$ 和 $r$ 表示结果来推导更新规则。\n\n您的程序必须为以下测试套件实现带有残差维护的循环坐标下降算法。对于每个测试用例，初始化 $x=0$ 和 $r=y$，对坐标 $i=1,\\dots,p$（按升序）执行指定次数的遍历，并返回两个量：最终的目标函数值 $f(x)$（一个浮点数），以及从 $x=0$ 和 $r=y$ 开始对指定坐标执行单次坐标更新时，残差条目发生变化的整数计数。将最终目标函数值四舍五入到 $6$ 位小数。\n\n测试套件：\n\n$1.$ 稠密情况（小型，良态）：\n- 矩阵 $A \\in \\mathbb{R}^{5 \\times 3}$ 由下式给出\n$$\nA = \\begin{bmatrix}\n2  -1  0 \\\\\n0  1  3 \\\\\n1  0  -2 \\\\\n4  1  1 \\\\\n-1  2  0\n\\end{bmatrix}.\n$$\n- 向量 $y \\in \\mathbb{R}^{5}$ 由 $y = [1,\\,4,\\,-3,\\,2,\\,0]^\\top$ 给出。\n- 遍历次数：$8$。\n- 用于单次更新变化计数的坐标：$i=2$（即第二列）。\n\n$2.$ 稀疏情况（典型的压缩感知结构）：\n- 维度：$n=10$，$p=12$。\n- 矩阵 $A \\in \\mathbb{R}^{10 \\times 12}$ 的列由其非零行和值以压缩稀疏列形式指定：\n    - 第 $1$ 列：行 $[1,\\,6,\\,10]$，值 $[3,\\,-1,\\,2]$。\n    - 第 $2$ 列：行 $[2,\\,3]$，值 $[4,\\,-2]$。\n    - 第 $3$ 列：行 $[4,\\,5,\\,9]$，值 $[1,\\,5,\\,-3]$。\n    - 第 $4$ 列：行 $[7]$，值 $[7]$。\n    - 第 $5$ 列：行 $[1,\\,8]$，值 $[-2,\\,4]$。\n    - 第 $6$ 列：行 $[3,\\,5,\\,10]$，值 $[6,\\,-1,\\,1]$。\n    - 第 $7$ 列：行 $[2,\\,6,\\,7]$，值 $[1,\\,-3,\\,2]$。\n    - 第 $8$ 列：行 $[4,\\,8,\\,9]$，值 $[-4,\\,2,\\,2]$。\n    - 第 $9$ 列：行 $[1]$，值 $[5]$。\n    - 第 $10$ 列：行 $[3,\\,10]$，值 $[-1,\\,3]$。\n    - 第 $11$ 列：行 $[5,\\,6,\\,8]$，值 $[2,\\,2,\\,-2]$。\n    - 第 $12$ 列：行 $[7,\\,9]$，值 $[1,\\,-1]$。\n  在此，上面列表中的行索引是从 $1$ 到 $n$；您的实现应将它们转换为从 $0$ 开始的索引。\n- 向量 $y \\in \\mathbb{R}^{10}$ 由 $y = [2,\\,-1,\\,3,\\,0,\\,5,\\,-2,\\,1,\\,4,\\,-3,\\,6]^\\top$ 给出。\n- 遍历次数：$50$。\n- 用于单次更新变化计数的坐标：$i=6$（即第六列）。\n\n$3.$ 包含零列的边界情况：\n- 矩阵 $A \\in \\mathbb{R}^{6 \\times 4}$ 的列由下式给出\n    - 第 $1$ 列：行 $[1,\\,4]$，值 $[1,\\,-2]$。\n    - 第 $2$ 列：行 $[]$（零列）。\n    - 第 $3$ 列：行 $[2,\\,5,\\,6]$，值 $[3,\\,-1,\\,2]$。\n    - 第 $4$ 列：行 $[1,\\,3,\\,4,\\,6]$，值 $[2,\\,-2,\\,1,\\,1]$。\n  同样，行是以从 $1$ 开始的索引列出的，在您的实现中必须转换为从 $0$ 开始的索引。\n- 向量 $y \\in \\mathbb{R}^{6}$ 由 $y = [1,\\,0,\\,-1,\\,2,\\,-2,\\,3]^\\top$ 给出。\n- 遍历次数：$20$。\n- 用于单次更新变化计数的坐标：$i=2$（零列）。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果必须是一个形如 $[f,\\,c]$ 的列表，其中 $f$ 是四舍五入到 $6$ 位小数的最终目标函数值（一个浮点数），$c$ 是从 $x=0$ 和 $r=y$ 开始对指定坐标执行单次更新时残差条目发生变化的整数计数。因此，最终输出必须形如 $[[f_1,c_1],[f_2,c_2],[f_3,c_3]]$，不含空格。",
            "solution": "该问题要求针对无约束最小二乘目标函数 $f(x) = \\frac{1}{2}\\lVert Ax - y \\rVert_2^2$ 推导并实现循环坐标下降算法。推导必须从第一性原理开始，并得到一个利用列稀疏性以实现高效实现的更新规则。\n\n### 第 1 步：问题验证\n\n**1. 提取已知条件：**\n- **目标函数：** $f(x)=\\tfrac{1}{2}\\lVert Ax - y\\rVert_2^2$。\n- **变量与维度：** $A \\in \\mathbb{R}^{n \\times p}$，$x \\in \\mathbb{R}^{p}$，$y \\in \\mathbb{R}^{n}$。\n- **算法：** 具有精确一维最小化的循环坐标下降。\n- **残差：** $r = y - Ax$。\n- **计算目标：** 每次坐标更新的时间复杂度为 $O(\\lVert a_i\\rVert_0)$，其中 $a_i$ 是 $A$ 的第 $i$ 列，$\\lVert a_i\\rVert_0$ 是其非零元素的数量。这需要通过维护残差 $r$ 来实现。\n- **数据结构：** $A$ 必须以压缩稀疏列（CSC）格式存储。\n- **初始条件：** 算法从 $x=0$ 和 $r=y$ 开始。\n- **任务：**\n    1. 为指定的遍历次数实现算法。\n    2. 对于给定的坐标 $i$，从 $x=0$ 和 $r=y$ 开始，计算在单次坐标更新期间残差 $r$ 中发生变化的条目数。\n- **测试套件：** 提供了三个测试用例，包含特定的矩阵 $A$、向量 $y$、遍历次数和用于变化计数的坐标。\n\n**2. 使用提取的已知条件进行验证：**\n- **科学依据：** 该问题牢固地植根于数值线性代数和优化理论。坐标下降是解决大规模问题的标准且经过充分分析的算法，特别是对于像最小二乘这样的凸目标。使用残差维护和稀疏数据结构以实现计算效率是科学计算和机器学习中的一项基石技术。该问题在科学和数学上是合理的。\n- **适定性：** 目标函数 $f(x)$ 是凸函数，保证了最小值的存在。已知循环坐标下降算法对于此问题会收敛到最小值。任务是确定性的：运行特定算法固定步数并计算特定量。该问题是适定的。\n- **客观性：** 该问题使用精确、无歧义的数学语言和定义进行陈述。它不含任何主观或基于意见的陈述。\n- **完整性与一致性：** 每个测试用例都明确提供了所有必要的数据（矩阵、向量、维度、迭代次数）和条件。没有矛盾之处。\n- **现实性与可行性：** 问题设置，包括使用稀疏矩阵和效率目标，非常现实，并与压缩感知和稀疏优化等领域的应用高度相关。测试用例包括一个标准的稠密情况、一个有代表性的稀疏情况和一个关键的边界情况（零列），这为算法提供了稳健的验证。\n- **其他标准：** 该问题并非微不足道、隐喻性或结构不良。它要求从第一性原理进行严格的推导，并进行尊重计算复杂性约束的仔细实现。\n\n**3. 结论与行动：**\n问题有效。它在科学上合理、适定且完整。我将继续推导解决方案并实现算法。\n\n### 第 2 步：推导与算法设计\n\n核心任务是找到单个坐标 $x_i$ 的更新，该更新使目标函数 $f(x)$ 最小化，假设所有其他坐标 $x_j$（对于 $j \\neq i$）是固定的。\n\n**坐标更新法则的推导：**\n我们将目标函数表示为单个变量 $x_i$ 的函数。令 $a_j$ 为 $A$ 的第 $j$ 列。\n$$f(x) = \\frac{1}{2}\\left\\lVert \\sum_{j=1}^p x_j a_j - y \\right\\rVert_2^2 = \\frac{1}{2}\\left\\lVert x_i a_i + \\sum_{j \\neq i} x_j a_j - y \\right\\rVert_2^2$$\n为简化起见，我们定义部分残差 $r_{\\text{partial}, i} = y - \\sum_{j \\neq i} x_j a_j$，它表示不包含第 $i$ 个坐标贡献的残差。作为 $x_i$ 的函数，目标变成一个一维最小二乘问题：\n$$g(x_i) = \\frac{1}{2}\\lVert x_i a_i - r_{\\text{partial}, i} \\rVert_2^2$$\n这是一个关于 $x_i$ 的二次函数。为求最小值，我们对 $x_i$ 求导并令其为零。\n$$g(x_i) = \\frac{1}{2} \\left( (x_i a_i - r_{\\text{partial}, i})^\\top (x_i a_i - r_{\\text{partial}, i}) \\right) = \\frac{1}{2} \\left( x_i^2 (a_i^\\top a_i) - 2x_i (a_i^\\top r_{\\text{partial}, i}) + r_{\\text{partial}, i}^\\top r_{\\text{partial}, i} \\right)$$\n导数为：\n$$\\frac{dg}{dx_i} = x_i (a_i^\\top a_i) - a_i^\\top r_{\\text{partial}, i}$$\n将导数设为零并求解 $x_i$ 得到最优新值 $x_i^{\\text{new}}$：\n$$x_i^{\\text{new}} (a_i^\\top a_i) = a_i^\\top r_{\\text{partial}, i} \\implies x_i^{\\text{new}} = \\frac{a_i^\\top r_{\\text{partial}, i}}{a_i^\\top a_i} = \\frac{a_i^\\top r_{\\text{partial}, i}}{\\lVert a_i \\rVert_2^2}$$\n仅当 $a_i$ 不是零向量时，即 $\\lVert a_i \\rVert_2^2 \\neq 0$ 时，此更新才有定义。如果 $a_i=0$，目标函数与 $x_i$ 无关，因此我们可以保持 $x_i$ 不变。\n\n**通过残差维护实现高效更新：**\n为每个坐标直接计算 $r_{\\text{partial}, i}$ 在计算上是昂贵的。我们可以通过将 $r_{\\text{partial}, i}$ 与完整残差 $r = y - Ax$ 关联起来，使更新变得高效。令 $x_i^{\\text{old}}$ 和 $r^{\\text{old}}$ 为更新第 $i$ 个坐标之前的值。\n$$r^{\\text{old}} = y - \\sum_{j=1}^p x_j^{\\text{old}} a_j = \\left( y - \\sum_{j \\neq i} x_j^{\\text{old}} a_j \\right) - x_i^{\\text{old}} a_i = r_{\\text{partial}, i} - x_i^{\\text{old}} a_i$$\n由此，我们得到 $r_{\\text{partial}, i} = r^{\\text{old}} + x_i^{\\text{old}} a_i$。将其代入 $x_i^{\\text{new}}$ 的表达式中：\n$$x_i^{\\text{new}} = \\frac{a_i^\\top (r^{\\text{old}} + x_i^{\\text{old}} a_i)}{a_i^\\top a_i} = \\frac{a_i^\\top r^{\\text{old}} + x_i^{\\text{old}} (a_i^\\top a_i)}{a_i^\\top a_i}$$\n此公式允许我们仅使用当前完整残差 $r^{\\text{old}}$、当前坐标值 $x_i^{\\text{old}}$、列 $a_i$ 及其可预计算的范数平方 $\\lVert a_i \\rVert_2^2$ 来计算最优新坐标值。\n\n**稀疏性感知的残差更新：**\n更新 $x_i$ 后，我们必须为下一次迭代更新残差。令 $\\Delta x_i = x_i^{\\text{new}} - x_i^{\\text{old}}$ 为坐标的变化量。新的状态向量为 $x^{\\text{new}} = x^{\\text{old}} + \\Delta x_i e_i$，其中 $e_i$ 是第 $i$ 个标准基向量。新的残差是：\n$$r^{\\text{new}} = y - A x^{\\text{new}} = y - A(x^{\\text{old}} + \\Delta x_i e_i) = (y - A x^{\\text{old}}) - \\Delta x_i (A e_i) = r^{\\text{old}} - \\Delta x_i a_i$$\n\n**计算复杂度：**\n对于每个坐标 $i$，更新包括：\n1.  **计算 $x_i^{\\text{new}}$**：主要操作是点积 $a_i^\\top r^{\\text{old}}$。由于 $a_i$ 是稀疏的，有 $\\lVert a_i \\rVert_0$ 个非零项，这需要 $O(\\lVert a_i \\rVert_0)$ 的时间。范数平方 $\\lVert a_i \\rVert_2^2$ 可以预先计算。\n2.  **更新 $x_i$**：这是一个 $O(1)$ 操作。\n3.  **更新 $r$**：更新 $r^{\\text{new}} = r^{\\text{old}} - \\Delta x_i a_i$ 需要缩放稀疏向量 $a_i$ 并将其从 $r$ 中减去。这仅修改残差的 $\\lVert a_i \\rVert_0$ 个条目，需要 $O(\\lVert a_i \\rVert_0)$ 的时间。\n\n因此，一次坐标更新的总时间为 $O(\\lVert a_i \\rVert_0)$，符合要求。使用 CSC 矩阵格式存储 $A$ 可以高效地访问列 $a_i$ 的非零索引和值。\n\n**算法：**\n1.  **初始化：** $x \\leftarrow 0 \\in \\mathbb{R}^p$，$r \\leftarrow y \\in \\mathbb{R}^n$。\n2.  **预计算：** 对于 $i=1, \\dots, p$，计算并存储 $c_i = \\lVert a_i \\rVert_2^2$。\n3.  **迭代：** 对于指定的遍历次数：\n    对于 $i = 1, \\dots, p$：\n    a. 如果 $c_i = 0$，继续到下一个坐标。\n    b. 获取列 $a_i$ 的非零元素及其行索引。\n    c. 计算 $x_i^{\\text{new}} = (a_i^\\top r + x_i c_i) / c_i$。\n    d. 计算变化量 $\\Delta x_i = x_i^{\\text{new}} - x_i$。\n    e. 更新坐标：$x_i \\leftarrow x_i^{\\text{new}}$。\n    f. 更新残差：$r \\leftarrow r - \\Delta x_i a_i$。此稀疏更新仅在 $a_i$ 的非零索引上进行。\n4.  **终止：** 在最后一次遍历后，目标值为 $f(x)=\\frac{1}{2}\\lVert r \\rVert_2^2$。\n\n**单次更新残差变化计数：**\n对于特定的坐标 $i$，从 $x=0$ 和 $r=y$ 开始：\n- $x_i^{\\text{old}} = 0$。\n- $x_i^{\\text{new}} = \\frac{a_i^\\top y + 0 \\cdot (a_i^\\top a_i)}{a_i^\\top a_i} = \\frac{a_i^\\top y}{\\lVert a_i \\rVert_2^2}$。\n- 变化量为 $\\Delta x_i = x_i^{\\text{new}}$。\n- 残差变化量为 $\\Delta r = -(\\Delta x_i) a_i$。第 $k$ 个条目发生变化，当且仅当 $(\\Delta r)_k = -(\\Delta x_i) (a_i)_k \\neq 0$。这当且仅当 $\\Delta x_i \\neq 0$ 且 $(a_i)_k \\neq 0$ 时发生。\n- 因此，如果 $\\Delta x_i \\neq 0$，则变化条目的数量为 $\\lVert a_i \\rVert_0$，否则为 $0$。当 $a_i$ 为零列或 $a_i$ 与 $y$ 正交时，$\\Delta x_i = 0$。",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import csc_matrix\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for coordinate descent and print results.\n    \"\"\"\n\n    def solve_case(A_data_spec, n, p, y_vec, passes, i_spec_1based):\n        \"\"\"\n        Solves one instance of the coordinate descent problem.\n\n        Args:\n            A_data_spec: A dictionary defining the sparse matrix A, or a NumPy array.\n                         If a dictionary, keys are 1-based column indices and\n                         values are tuples of (rows, values), with 1-based rows.\n            n: Number of rows in A.\n            p: Number of columns in A.\n            y_vec: The vector y.\n            passes: Number of full passes for coordinate descent.\n            i_spec_1based: The 1-based coordinate for the single-update change-count.\n        \"\"\"\n        # Convert i_spec to 0-based index\n        i_spec_0based = i_spec_1based - 1\n\n        # Construct the CSC matrix A\n        if isinstance(A_data_spec, np.ndarray):\n            A = csc_matrix(A_data_spec)\n        else:\n            data = []\n            indices = []\n            indptr = [0]\n            for j in range(1, p + 1):\n                if j in A_data_spec:\n                    rows, vals = A_data_spec[j]\n                    data.extend(vals)\n                    # Convert 1-based row indices to 0-based\n                    indices.extend([r - 1 for r in rows])\n                indptr.append(len(data))\n            A = csc_matrix((data, indices, indptr), shape=(n, p), dtype=float)\n        \n        y = np.array(y_vec, dtype=float)\n        \n        # --- Single-update residual change count calculation ---\n        # Starting from x=0, r=y\n        a_i_spec = A[:, i_spec_0based]\n        a_i_spec_norm_sq = a_i_spec.power(2).sum()\n        \n        change_count = 0\n        # Use a small tolerance for floating point comparisons\n        if a_i_spec_norm_sq > 1e-12:\n            # For x=0, delta_x_i = (a_i.T @ y) / ||a_i||^2\n            delta_x_i = a_i_spec.T.dot(y) / a_i_spec_norm_sq\n            if abs(delta_x_i) > 1e-12:\n                change_count = a_i_spec.getnnz()\n        # If column is zero or a_i.T @ y is zero, delta_x is effectively zero, so no change.\n\n        # --- Cyclic Coordinate Descent Algorithm ---\n        x = np.zeros(p)\n        r = y.copy()\n\n        # Pre-compute squared L2 norms of columns\n        col_sq_norms = np.array([A[:, j].power(2).sum() for j in range(p)])\n\n        for _ in range(passes):\n            for i in range(p):\n                if col_sq_norms[i]  1e-12: # Skip zero columns\n                    continue\n                \n                # Get sparse column data efficiently from CSC structure\n                start, end = A.indptr[i], A.indptr[i+1]\n                rows_i = A.indices[start:end]\n                vals_i = A.data[start:end]\n                \n                # Calculate optimal new value for x_i\n                # x_i_new = (a_i.T @ (r + x[i]*a_i)) / ||a_i||^2\n                # term (a_i.T @ r) is computed sparsely\n                a_i_T_r = np.dot(vals_i, r[rows_i])\n                \n                x_i_old = x[i]\n                # The term (a_i.T @ (x_i * a_i)) = x_i * ||a_i||^2\n                numerator = a_i_T_r + x_i_old * col_sq_norms[i]\n                x_i_new = numerator / col_sq_norms[i]\n                \n                delta_x_i = x_i_new - x_i_old\n                \n                if abs(delta_x_i) > 1e-12:\n                    x[i] = x_i_new\n                    # Update residual efficiently: r_new = r_old - delta_x_i * a_i\n                    r[rows_i] -= delta_x_i * vals_i\n\n        # Final objective value: 0.5 * ||r||_2^2, where r is the final residual\n        final_obj = 0.5 * np.dot(r, r)\n        \n        return [final_obj, change_count]\n\n    # --- Test Cases ---\n\n    # Test Case 1: Dense matrix\n    test_case_1 = {\n        \"A_data_spec\": np.array([\n            [2., -1., 0.],\n            [0., 1., 3.],\n            [1., 0., -2.],\n            [4., 1., 1.],\n            [-1., 2., 0.]\n        ]),\n        \"n\": 5, \"p\": 3,\n        \"y_vec\": [1., 4., -3., 2., 0.],\n        \"passes\": 8,\n        \"i_spec_1based\": 2\n    }\n    res1 = solve_case(**test_case_1)\n\n    # Test Case 2: Sparse matrix\n    test_case_2 = {\n        \"A_data_spec\": {\n            1: ([1, 6, 10], [3, -1, 2]), 2: ([2, 3], [4, -2]),\n            3: ([4, 5, 9], [1, 5, -3]),  4: ([7], [7]),\n            5: ([1, 8], [-2, 4]),       6: ([3, 5, 10], [6, -1, 1]),\n            7: ([2, 6, 7], [1, -3, 2]), 8: ([4, 8, 9], [-4, 2, 2]),\n            9: ([1], [5]),              10: ([3, 10], [-1, 3]),\n            11: ([5, 6, 8], [2, 2, -2]),12: ([7, 9], [1, -1])\n        },\n        \"n\": 10, \"p\": 12,\n        \"y_vec\": [2., -1., 3., 0., 5., -2., 1., 4., -3., 6.],\n        \"passes\": 50,\n        \"i_spec_1based\": 6\n    }\n    res2 = solve_case(**test_case_2)\n\n    # Test Case 3: Edge case with a zero column\n    test_case_3 = {\n        \"A_data_spec\": {\n            1: ([1, 4], [1, -2]),\n            2: ([], []),\n            3: ([2, 5, 6], [3, -1, 2]),\n            4: ([1, 3, 4, 6], [2, -2, 1, 1])\n        },\n        \"n\": 6, \"p\": 4,\n        \"y_vec\": [1., 0., -1., 2., -2., 3.],\n        \"passes\": 20,\n        \"i_spec_1based\": 2\n    }\n    res3 = solve_case(**test_case_3)\n\n    results = [res1, res2, res3]\n    \n    # Format the final output string exactly as required (no spaces)\n    inner_parts = []\n    for f, c in results:\n        # Use string format to ensure 6 decimal places for the float\n        inner_parts.append(f\"[{f:.6f},{c}]\")\n    final_str = f\"[{','.join(inner_parts)}]\"\n    print(final_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了基本最小二乘问题的求解之后，我们接下来将挑战更为复杂的Lasso问题，其目标函数中包含一个非光滑的 $\\ell_1$ 范数正则项。这个实践  不仅要求你将坐标下降法扩展到近端梯度框架下，还需要你通过设计一个“最坏情况”来对比循环坐标下降（CCD）和随机坐标下降（RCD）的性能。通过这个练习，你将亲身体会到在处理相关特征时，随机化策略为何通常比固定循环策略更为稳健和高效。",
            "id": "3441210",
            "problem": "考虑压缩感知和稀疏优化中的最小绝对收缩和选择算子 (Lasso) 目标函数，对于传感矩阵 $A \\in \\mathbb{R}^{m \\times p}$ 和响应向量 $y \\in \\mathbb{R}^{m}$ 定义为\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 \\;+\\; \\lambda\\,\\|x\\|_1,\n$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数，$\\|\\cdot\\|_1$ 表示 $\\ell_1$ 范数。坐标下降算法通过每次固定其他坐标不变，逐个对 $x$ 的一个坐标最小化 $f$ 来优化 $f(x)$。考虑两种选择规则：\n- 循环坐标下降 (CCD)：按 $1,2,\\dots,p$ 的固定顺序重复访问坐标。\n- 随机坐标下降 (RCD)：在每次更新时，从坐标中进行均匀随机且有放回的抽样。\n\n一个轮次 (epoch) 定义为恰好 $p$ 次单坐标更新。两种方法都从 $x^{(0)} = 0$ 开始，并在每次更新时沿所选坐标方向进行精确最小化。\n\n通过构造高度相关的列，为循环规则设计一个能引发“之”字形振荡 (zig-zagging) 的最坏情况传感矩阵 $A$ 和响应 $y$。具体来说，对于给定的 $m$、$p$ 和相关参数 $\\rho \\in [0,1)$，构造 $A$ 的前两列，使其内积近似为 $\\rho$ 且范数为单位范数，并构造其余列，使其与前两列及彼此之间近似不相关。对于一个只有前两个条目非零且大小相等的稀疏真实向量 $x^\\star$，设置 $y = A x^\\star$。然后，程序必须对每个测试用例，比较 CCD 和 RCD 在每个轮次的平均目标函数下降量，该平均值是在固定轮次 $E$ 上计算的。随机选择必须使用固定的伪随机种子 $2025$，以确保结果是可复现的。\n\n您的程序必须仅基于 $f(x)$ 的定义和每个坐标的一维优化子问题，实现这两种使用精确单坐标最小化的选择规则。对于每个测试用例，它必须计算并报告以下比率\n$$\nr \\;=\\; \\frac{\\text{RCD下每轮次的平均目标函数下降量}}{\\text{CCD下每轮次的平均目标函数下降量}},\n$$\n表示为一个浮点数。\n\n测试套件。使用以下四个测试用例，所有用例均设置 $E = 50$ 轮次且无观测噪声：\n- 用例 1：$m = 200$, $p = 2$, $\\rho = 0.999$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$。\n- 用例 2：$m = 200$, $p = 2$, $\\rho = 0.9$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$。\n- 用例 3：$m = 200$, $p = 2$, $\\rho = 0.0$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$。\n- 用例 4：$m = 200$, $p = 10$，其中列 1 和 2 的相关性为 $\\rho = 0.999$ 且为单位范数，列 3 到 10 为随机单位范数向量，且与前两列及彼此之间近似不相关，$\\lambda = 0.05$, $x^\\star = [1,\\,1,\\,0,\\,\\dots,\\,0]^T$（长度为 $p$）。\n\n构造细节。对于 $p \\ge 2$，通过采样独立同分布的标准正态分布项并归一化到单位欧几里得范数来生成第一列 $a_1$。生成 $a_2$ 为\n$$\na_2 \\;=\\; \\rho\\,a_1 \\;+\\; \\sqrt{1-\\rho^2}\\,w,\n$$\n其中 $w$ 具有独立同分布的标准正态分布项，然后将 $a_2$ 归一化为单位范数。对于 $p > 2$，对于 $j \\ge 3$，将列 $a_j$ 生成为独立的、归一化为单位范数的标准正态向量。设置 $A = [a_1,\\dots,a_p]$ 和 $y = A x^\\star$。对于 CCD 和 RCD，均初始化 $x^{(0)} = 0$，将一个轮次定义为 $p$ 次更新，并在每轮次后记录 $f(x)$。\n\n每轮次的平均目标函数下降量。对每种方法，按如下方式计算每轮次的平均下降量\n$$\n\\Delta_{\\mathrm{avg}} \\;=\\; \\frac{1}{E}\\,\\sum_{t=0}^{E-1} \\left(f\\left(x^{(t)}\\right) - f\\left(x^{(t+1)}\\right)\\right),\n$$\n其中 $x^{(t)}$ 是 $t$ 轮次后的迭代结果。\n\n最终输出格式。您的程序应生成一行输出，其中包含一个逗号分隔的四个浮点比率列表 $[r_1,r_2,r_3,r_4]$，按上述测试用例的顺序排列，并用方括号括起来。对所有随机抽样使用伪随机种子 $2025$。不涉及任何物理单位、角度单位或百分比；所有输出均为无量纲的浮点数。",
            "solution": "该问题被评估为**有效的**。这是一个数值优化领域的适定、有科学依据的问题，特别关注坐标下降算法对 Lasso 目标函数的性能特征。所有参数、过程和评估指标都得到了明确和正式的定义。\n\n问题的核心是实现并比较循环坐标下降 (CCD) 和随机坐标下降 (RCD) 在 Lasso 目标函数上的表现，该函数由下式给出：\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 \\;+\\; \\lambda\\,\\|x\\|_1\n$$\n这里，$x \\in \\mathbb{R}^p$ 是待优化的参数向量，$A \\in \\mathbb{R}^{m \\times p}$ 是传感矩阵，$y \\in \\mathbb{R}^m$ 是响应向量，$\\lambda \\ge 0$ 是正则化参数。\n\n坐标下降算法通过迭代地对单个坐标 $x_j$ 进行最小化来优化此函数，同时保持所有其他坐标 $x_k$（对于 $k \\neq j$）固定。坐标 $x_j$ 的一维子问题是最小化：\n$$\ng(z) \\;=\\; f(x_1, \\dots, x_{j-1}, z, x_{j+1}, \\dots, x_p)\n$$\n展开目标函数，我们分离出依赖于 $x_j$ 的项：\n$$\n\\begin{aligned}\nf(x) = \\tfrac{1}{2} \\left\\| \\sum_{k=1}^p a_k x_k - y \\right\\|_2^2 + \\lambda \\sum_{k=1}^p |x_k| \\\\\n= \\tfrac{1}{2} \\left\\| a_j x_j + \\sum_{k \\neq j} a_k x_k - y \\right\\|_2^2 + \\lambda |x_j| + \\lambda \\sum_{k \\neq j} |x_k|\n\\end{aligned}\n$$\n其中 $a_k$ 是 $A$ 的第 $k$ 列。为了对 $x_j$ 进行最小化，我们可以忽略不依赖于它的项。子问题变为最小化：\n$$\n\\arg\\min_{x_j} \\left( \\tfrac{1}{2} \\|a_j x_j + \\sum_{k \\neq j} a_k x_k - y\\|_2^2 + \\lambda|x_j| \\right)\n$$\n展开平方范数项：\n$$\n\\tfrac{1}{2} \\left( x_j^2 \\|a_j\\|_2^2 + 2x_j a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) \\right) + \\lambda|x_j| + \\text{const}\n$$\n问题规定所有列 $a_j$ 都被归一化为单位欧几里得范数，即 $\\|a_j\\|_2^2 = a_j^T a_j = 1$。这将子问题简化为：\n$$\n\\arg\\min_{x_j} \\left( \\tfrac{1}{2} x_j^2 + x_j a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) + \\lambda|x_j| \\right)\n$$\n这是一个关于 $x_j$ 的二次函数加上一个 $\\ell_1$ 范数惩罚项。$\\arg\\min_z (\\frac{1}{2}z^2 - c z + \\lambda|z|)$ 的解由软阈值算子给出，$z^* = S_\\lambda(c)$。在我们的问题中，$c = -a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) = a_j^T y - \\sum_{k \\neq j} (a_j^T a_k) x_k$。\n因此，坐标 $x_j$ 的更新规则是：\n$$\nx_j^{\\text{new}} \\leftarrow S_\\lambda\\left( a_j^T y - \\sum_{k \\neq j} (a_j^T a_k) x_k \\right)\n$$\n其中 $S_\\lambda(z) = \\text{sgn}(z) \\max(|z| - \\lambda, 0)$。为了计算效率，我们可以预先计算格拉姆矩阵 (Gram matrix) $A^T A$ 和向量 $A^T y$。令 $G = A^T A$ 和 $c_y = A^T y$。更新变为：\n$$\nx_j^{\\text{new}} \\leftarrow S_\\lambda\\left( (c_y)_j - \\sum_{k \\neq j} G_{jk} x_k \\right)\n$$\n求和项可以写成 $(G x)_j - G_{jj} x_j$。由于 $G_{jj}=a_j^T a_j=1$，软阈值函数的参数是 $(c_y)_j - ((G x)_j - x_j)$。向量 $x$ 包含坐标的最新更新值。\n\n算法流程如下：\n1.  **初始化**：为保证可复现性，将伪随机种子设为 $2025$。对每个测试用例，按规定构造矩阵 $A$ 和向量 $y$。列 $a_j$ 从独立同分布的标准正态分布中生成并进行归一化。前两列 $a_1$ 和 $a_2$ 被构造成具有指定的相关性结构。响应是无噪声的，$y=Ax^\\star$。预计算 $A^TA$ 和 $A^Ty$。初始化解的估计值 $x^{(0)} = 0$。\n\n2.  **循环坐标下降 (CCD)**：迭代 $E$ 个轮次。在每个轮次中，从 $j=1, \\dots, p$ 顺序更新坐标。\n    $$\n    x_j \\leftarrow S_\\lambda\\left( (A^Ty)_j - \\left( \\sum_{k=1}^p (A^TA)_{jk} x_k - (A^TA)_{jj} x_j \\right) \\right)\n    $$\n    用于更新 $x_j$ 的 $x_k$ 值是当前可用的最新值。\n\n3.  **随机坐标下降 (RCD)**：迭代 $E$ 个轮次。在每个轮次中，执行 $p$ 次更新。对于每次更新，从 $\\{1, \\dots, p\\}$ 中均匀随机且有放回地选择一个坐标 $j$。应用与 CCD 相同的更新规则。\n\n4.  **评估**：对于 CCD 和 RCD，经过 $t$ 个轮次后的迭代向量记为 $x^{(t)}$。记录 $t=0, \\dots, E$ 的目标函数值 $f(x^{(t)})$。每轮次的平均目标函数下降量计算如下：\n    $$\n    \\Delta_{\\mathrm{avg}} = \\frac{1}{E} \\sum_{t=0}^{E-1} \\left(f(x^{(t)}) - f(x^{(t+1)})\\right)\n    $$\n    每个测试用例最终报告的值是比率 $r = \\Delta_{\\mathrm{avg, RCD}} / \\Delta_{\\mathrm{avg, CCD}}$。\n\n代码实现了这一逻辑，并仔细遵循了 $A$ 的构造细节、CCD 和 RCD 的迭代更新方案以及最终性能比率的计算方法。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the coordinate descent comparison problem for Lasso.\n    It implements and compares Cyclic Coordinate Descent (CCD) and\n    Randomized Coordinate Descent (RCD) on constructed test cases,\n    reporting the ratio of their average per-epoch objective decrease.\n    \"\"\"\n    \n    # Per the problem statement, a single seed is used for all random draws.\n    np.random.seed(2025)\n\n    test_cases = [\n        # Case 1: High correlation\n        {'m': 200, 'p': 2, 'rho': 0.999, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 2: Moderate correlation\n        {'m': 200, 'p': 2, 'rho': 0.9, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 3: No correlation\n        {'m': 200, 'p': 2, 'rho': 0.0, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 4: High correlation in a larger-p setting\n        {'m': 200, 'p': 10, 'rho': 0.999, 'lam': 0.05, 'x_star': [1.0, 1.0] + [0.0] * 8, 'E': 50}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        m, p, rho, lam, x_star_list, E = case['m'], case['p'], case['rho'], case['lam'], case['x_star'], case['E']\n        x_star = np.array(x_star_list, dtype=float)\n\n        # 1. Construct sensing matrix A and response vector y\n        A = np.zeros((m, p))\n        \n        # First column a_1\n        a1 = np.random.randn(m)\n        a1 /= np.linalg.norm(a1)\n        A[:, 0] = a1\n\n        # Second column a_2, constructed to be correlated with a_1\n        if p >= 2:\n            w = np.random.randn(m)\n            # The construction follows the problem statement verbatim.\n            # Adding max(0,...) ensures the argument to sqrt is non-negative.\n            a2_unnormalized = rho * a1 + np.sqrt(max(0, 1 - rho**2)) * w\n            A[:, 1] = a2_unnormalized / np.linalg.norm(a2_unnormalized)\n\n        # Remaining columns a_j for j >= 3\n        for j in range(2, p):\n            aj = np.random.randn(m)\n            aj /= np.linalg.norm(aj)\n            A[:, j] = aj\n            \n        # Noiseless response vector\n        y = A @ x_star\n        \n        # Precompute matrices for efficiency\n        AtA = A.T @ A\n        Aty = A.T @ y\n\n        # Helper functions defined within the loop to capture A, y, lam etc.\n        def objective_function(x):\n            residual = A @ x - y\n            l2_term = 0.5 * np.sum(residual**2)\n            l1_term = lam * np.sum(np.abs(x))\n            return l2_term + l1_term\n\n        def soft_threshold(z, l):\n            return np.sign(z) * np.maximum(np.abs(z) - l, 0.0)\n\n        # 2. Cyclic Coordinate Descent (CCD)\n        x_ccd = np.zeros(p)\n        f_values_ccd = [objective_function(x_ccd)]\n        for _ in range(E):\n            for j in range(p):\n                # Update rule for coordinate j\n                val = Aty[j] - (np.dot(AtA[j, :], x_ccd) - AtA[j,j] * x_ccd[j])\n                x_ccd[j] = soft_threshold(val, lam)\n            f_values_ccd.append(objective_function(x_ccd))\n        \n        decreases_ccd = [f_values_ccd[t] - f_values_ccd[t+1] for t in range(E)]\n        delta_ccd = np.mean(decreases_ccd)\n\n        # 3. Randomized Coordinate Descent (RCD)\n        x_rcd = np.zeros(p)\n        f_values_rcd = [objective_function(x_rcd)]\n        for _ in range(E):\n            # An epoch consists of p updates at random coordinates\n            for _ in range(p):\n                j = np.random.randint(0, p)\n                val = Aty[j] - (np.dot(AtA[j, :], x_rcd) - AtA[j,j] * x_rcd[j])\n                x_rcd[j] = soft_threshold(val, lam)\n            f_values_rcd.append(objective_function(x_rcd))\n\n        decreases_rcd = [f_values_rcd[t] - f_values_rcd[t+1] for t in range(E)]\n        delta_rcd = np.mean(decreases_rcd)\n\n        # 4. Compute and store the ratio\n        if delta_ccd == 0.0:\n            ratio = np.inf if delta_rcd > 0 else 1.0\n        else:\n            ratio = delta_rcd / delta_ccd\n            \n        results.append(ratio)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "随机坐标下降法的一个核心问题是：我们应该如何选择下一个要更新的坐标？虽然均匀随机选择是一个简单有效的策略，但它可能不是最优的。这个高级实践  将引导你探索“重要性采样”的思想，即根据预期进展的多少来非均匀地选择坐标。你将推导在一个固定的计算预算下，如何设计最优的采样概率分布，从而最大化算法的期望收敛速度，这体现了对算法进行理论优化的深刻见解。",
            "id": "3441225",
            "problem": "考虑一个来自压缩感知的稀疏恢复目标，该目标被建模为一个复合凸问题，由一个光滑的数据拟合项和一个稀疏性诱导正则化项组成。令 $A \\in \\mathbb{R}^{3 \\times 3}$，$b \\in \\mathbb{R}^{3}$，并定义目标函数 $F(x) = f(x) + g(x)$，其中 $f(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$ 且 $g(x) = \\lambda \\|x\\|_{1}$，$\\lambda  0$。坐标下降更新使用近端坐标梯度步进行，其中坐标级 Lipschitz 常数 $L_{i}$ 由光滑部分 $f$ 定义。假设更新坐标 $i$ 的成本是一个已知的正权重 $w_{i}$，并且每次迭代有一个固定的计算预算 $B  0$，该预算约束了期望的每次迭代成本。\n\n给定当前迭代点 $x \\in \\mathbb{R}^{3}$ 处的以下特定实例：\n- $A = \\begin{pmatrix} 1  0  2 \\\\ 0  2  1 \\\\ 1  1  0 \\end{pmatrix}$，$b = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}$，$\\lambda = 0.5$，以及 $x = \\begin{pmatrix} 0.5 \\\\ -0.1 \\\\ 0.0 \\end{pmatrix}$。\n- 坐标更新成本为 $w_{1} = 1$，$w_{2} = 3$，$w_{3} = 2$。\n- 每次迭代的计算预算为 $B = 2$。\n\n仅使用 $f$ 的坐标级 Lipschitz 常数、$\\ell_{1}$ 范数的近端算子以及由近端步诱导的坐标级广义梯度映射的基本定义，按以下步骤进行：\n\n1. 从第一性原理出发，计算 $f(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$ 的坐标级 Lipschitz 常数 $L_{i}$。\n2. 使用梯度 $\\nabla f(x)$ 和近端映射 $\\operatorname{prox}_{\\alpha |\\cdot|}(\\cdot)$ 的定义，推导单个坐标 $i$ 的单步近端坐标更新（步长为 $1/L_{i}$），并用 $x$、$\\nabla f(x)$ 和 $L_{i}$ 表示相应的广义梯度映射 $G_{i}(x)$。\n3. 利用从 $f$ 的光滑性和 $g$ 的可分性推导出的近端坐标更新的标准下降性质，证明每个坐标的目标函数下降量由一个形式为 $\\Delta_{i} = c_{i} G_{i}(x)^{2}$ 的标量给出下界，其中 $c_{i}  0$ 取决于 $L_{i}$。并在给定的 $x$ 处显式计算 $i \\in \\{1,2,3\\}$ 的 $\\Delta_{i}$。\n4. 对于坐标的均匀采样，即对所有 $i \\in \\{1,2,3\\}$，概率为 $p_{i}^{\\text{unif}} = 1/3$，计算期望的每次迭代下降量 $\\mathbb{E}[\\Delta] = \\sum_{i=1}^{3} p_{i}^{\\text{unif}} \\Delta_{i}$、期望的每次迭代成本 $\\sum_{i=1}^{3} p_{i}^{\\text{unif}} w_{i}$ 以及定义为比率 $\\mathbb{E}[\\Delta] / \\sum_{i=1}^{3} p_{i}^{\\text{unif}} w_{i}$ 的每单位成本的期望下降量。提供精确值（不进行四舍五入）。\n5. 现在，在每次迭代固定计算预算 $B=2$ 的条件下，推导采样分布 $p = (p_{1}, p_{2}, p_{3})$，该分布在约束条件 $\\sum_{i=1}^{3} p_{i} = 1$、$\\sum_{i=1}^{3} p_{i} w_{i} \\le B$ 和对所有 $i$ 都有 $p_{i} \\ge 0$ 的情况下，最大化期望的每次迭代下降量 $\\sum_{i=1}^{3} p_{i} \\Delta_{i}$。从第一性原理出发，论证为什么最优解必须集中在至多两个坐标上，并在此实例中显式求解最优的 $p$。\n\n以行矩阵形式给出你的最终答案，即最优采样分布 $p^{\\star}$。不需要四舍五入。最终答案中不要包含任何单位。如果计算任何中间数值，尽可能将它们保留为精确的有理数。",
            "solution": "首先根据指定标准对问题进行验证。\n\n### 问题验证\n\n**第1步：提取已知条件**\n- 目标函数：$F(x) = f(x) + g(x)$，对于 $x \\in \\mathbb{R}^{3}$。\n- 光滑项：$f(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$，其中 $A \\in \\mathbb{R}^{3 \\times 3}$，$b \\in \\mathbb{R}^{3}$。\n- 非光滑项：$g(x) = \\lambda \\|x\\|_{1}$，其中 $\\lambda  0$。\n- 问题实例：\n  - $A = \\begin{pmatrix} 1  0  2 \\\\ 0  2  1 \\\\ 1  1  0 \\end{pmatrix}$\n  - $b = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}$\n  - $\\lambda = 0.5$\n  - 当前迭代点：$x = \\begin{pmatrix} 0.5 \\\\ -0.1 \\\\ 0.0 \\end{pmatrix}$\n- 坐标更新成本：$w_{1} = 1$，$w_{2} = 3$，$w_{3} = 2$。\n- 计算预算：$B = 2$。\n- 任务是执行一个与坐标下降相关的五部分分析，涉及 Lipschitz 常数、近端更新、下降保证、均匀采样和预算下的最优采样。\n\n**第2步：使用提取的已知条件进行验证**\n- **科学依据**：该问题是近端坐标下降在 LASSO 问题上的一个标准应用，LASSO 是稀疏优化和压缩感知的基石。所有使用的概念（Lipschitz 常数、近端算子、基于目标的采样）在凸优化领域都是基本且成熟的。该问题在科学上和数学上是合理的。\n- **适定性**：问题陈述清晰，提供了所有必要的数据。目标函数是凸函数，确保了优化过程是良定义的。最后一部分涉及求解一个线性规划，它有良定义的解。该问题存在唯一且有意义的解。\n- **客观性**：问题使用精确的数学语言表述，没有任何主观或模糊的术语。\n- **完整性和一致性**：所有矩阵和向量的维度都是一致的。所提供的数据对于要求的任务是完整的。没有内部矛盾。\n- **真实性和可行性**：该问题是一个小规模但现实的稀疏恢复问题实例。所有计算在计算上都是可行的。\n\n**第3步：结论与行动**\n问题是有效的。下面提供完整的解答。\n\n### 详细解答\n\n解答按照问题陈述的要求分为五个部分。\n\n**1. 坐标级 Lipschitz 常数**\n\n目标函数的光滑部分是 $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$。$f$ 的梯度是 $\\nabla f(x) = A^T(Ax - b)$，其 Hessian 矩阵是 $\\nabla^2 f(x) = A^T A$。$\\nabla_i f(x)$ 的坐标级 Lipschitz 常数 $L_i$ 是映射 $t \\mapsto \\nabla_i f(x + t e_i)$ 的 Lipschitz 常数，由 Hessian 矩阵的第 $i$ 个对角元素给出：$L_i = (\\nabla^2 f(x))_{ii} = (A^T A)_{ii}$。\n令 $a_i$ 为矩阵 $A$ 的第 $i$ 列。那么 $(A^T A)_{ii} = a_i^T a_i = \\|a_i\\|_2^2$。\n给定 $A = \\begin{pmatrix} 1  0  2 \\\\ 0  2  1 \\\\ 1  1  0 \\end{pmatrix}$，其列向量为 $a_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$，$a_2 = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}$ 和 $a_3 = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\end{pmatrix}$。\n我们计算范数的平方：\n$L_1 = \\|a_1\\|_2^2 = 1^2 + 0^2 + 1^2 = 2$。\n$L_2 = \\|a_2\\|_2^2 = 0^2 + 2^2 + 1^2 = 5$。\n$L_3 = \\|a_3\\|_2^2 = 2^2 + 1^2 + 0^2 = 5$。\n因此，坐标级 Lipschitz 常数为 $L_1 = 2$，$L_2 = 5$ 和 $L_3 = 5$。\n\n**2. 近端坐标更新与广义梯度映射**\n\n坐标 $x_i$ 的近端坐标梯度更新是通过最小化 $f$ 的一个二次近似加上非光滑项 $g_i(x_i) = \\lambda |x_i|$ 来找到的。$x_i$ 的更新 $x_i^+$ 为：\n$$x_i^+ = \\arg\\min_{y \\in \\mathbb{R}} \\left( f(x) + \\nabla_i f(x) (y - x_i) + \\frac{L_i}{2}(y - x_i)^2 + \\lambda|y| \\right)$$\n这等价于找到以下函数的最小化子：\n$$y \\mapsto \\frac{L_i}{2} \\left( y - \\left(x_i - \\frac{1}{L_i}\\nabla_i f(x)\\right) \\right)^2 + \\lambda|y|$$\n解由参数为 $\\lambda/L_i$ 的 $g_i$ 的近端算子给出：\n$$x_i^+ = \\operatorname{prox}_{\\frac{\\lambda}{L_i}|\\cdot|}\\left(x_i - \\frac{1}{L_i}\\nabla_i f(x)\\right)$$\n对于 $\\ell_1$ 范数，近端算子是软阈值函数 $S_\\alpha(z) = \\operatorname{sign}(z) \\max(|z|-\\alpha, 0)$。因此，更新为：\n$$x_i^+ = S_{\\lambda/L_i}\\left(x_i - \\frac{1}{L_i}\\nabla_i f(x)\\right)$$\n坐标 $i$ 的广义梯度映射 $G_i(x)$ 定义为 $G_i(x) = L_i(x_i - x_i^+)$。这可以写为：\n$$G_i(x) = L_i \\left( x_i - S_{\\lambda/L_i}\\left(x_i - \\frac{1}{L_i}\\nabla_i f(x)\\right) \\right)$$\n这个量衡量了在当前点 $x$ 处，坐标 $i$ 的最优性条件被违反的程度。\n\n**3. 每个坐标的目标函数下降量**\n\n根据光滑函数的下降引理（具体来说，是由 $\\nabla_i f$ 的 Lipschitz 常数 $L_i$ 提供的二次上界），将坐标 $i$ 更新到 $x_i^+$（令 $x^+$ 为新的迭代点）时，$F$ 的变化是有界的：\n$$F(x^+) \\le F(x) + \\nabla_i f(x)(x_i^+ - x_i) + \\frac{L_i}{2}(x_i^+ - x_i)^2 + g_i(x_i^+) - g_i(x_i)$$\n$x_i^+$ 的最优性条件是 $0$ 属于被最小化函数在 $x_i^+$ 处的次微分。这意味着存在一个次梯度 $\\zeta_i \\in \\partial g_i(x_i^+)$，使得 $\\nabla_i f(x) + L_i(x_i^+ - x_i) + \\zeta_i = 0$。\n将 $\\nabla_i f(x) = -L_i(x_i^+ - x_i) - \\zeta_i$ 代入下降不等式：\n$$F(x^+) - F(x) \\le (-L_i(x_i^+ - x_i) - \\zeta_i)(x_i^+ - x_i) + \\frac{L_i}{2}(x_i^+ - x_i)^2 + g_i(x_i^+) - g_i(x_i)$$\n$$F(x^+) - F(x) \\le -L_i(x_i^+ - x_i)^2 - \\left(\\zeta_i(x_i^+ - x_i) - (g_i(x_i^+) - g_i(x_i))\\right) + \\frac{L_i}{2}(x_i^+ - x_i)^2$$\n根据 $g_i$ 的凸性，我们有 $g_i(x_i) \\ge g_i(x_i^+) + \\zeta_i(x_i - x_i^+)$，这意味着 $-(\\dots)$ 项是非正的。因此：\n$$F(x^+) - F(x) \\le -L_i(x_i^+ - x_i)^2 + \\frac{L_i}{2}(x_i^+ - x_i)^2 = -\\frac{L_i}{2}(x_i^+ - x_i)^2$$\n因此，目标函数的下降量 $F(x) - F(x^+)$ 有如下下界：\n$$F(x) - F(x^+) \\ge \\frac{L_i}{2}(x_i^+ - x_i)^2$$\n使用定义 $G_i(x) = L_i(x_i - x_i^+)$，我们有 $x_i^+ - x_i = -\\frac{1}{L_i}G_i(x)$。将其代入界中：\n$$F(x) - F(x^+) \\ge \\frac{L_i}{2}\\left(-\\frac{1}{L_i}G_i(x)\\right)^2 = \\frac{1}{2L_i}G_i(x)^2$$\n这给出了每个坐标的下降量下界 $\\Delta_i = c_i G_i(x)^2$，其中 $c_i = \\frac{1}{2L_i}$。\n\n现在，我们在给定的 $x$ 处计算 $i \\in \\{1,2,3\\}$ 的 $\\Delta_i$。\n首先，计算梯度 $\\nabla f(x) = A^T(Ax-b)$：\n$Ax = \\begin{pmatrix} 1  0  2 \\\\ 0  2  1 \\\\ 1  1  0 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ -0.1 \\\\ 0.0 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.4 \\end{pmatrix}$\n$Ax - b = \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ 1.8 \\\\ 0.4 \\end{pmatrix}$\n$\\nabla f(x) = \\begin{pmatrix} 1  0  1 \\\\ 0  2  1 \\\\ 2  1  0 \\end{pmatrix} \\begin{pmatrix} -0.5 \\\\ 1.8 \\\\ 0.4 \\end{pmatrix} = \\begin{pmatrix} -0.1 \\\\ 4.0 \\\\ 0.8 \\end{pmatrix}$\n\n对于 $i=1$：$L_1=2, x_1=0.5, \\nabla_1 f(x)=-0.1, \\lambda=0.5$。\n$x_1^+ = S_{0.5/2}(0.5 - \\frac{1}{2}(-0.1)) = S_{0.25}(0.55) = 0.55 - 0.25 = 0.3$。\n$G_1(x) = L_1(x_1-x_1^+) = 2(0.5-0.3) = 0.4$。\n$\\Delta_1 = \\frac{1}{2L_1}G_1(x)^2 = \\frac{1}{2(2)}(0.4)^2 = \\frac{0.16}{4} = 0.04 = \\frac{1}{25}$。\n\n对于 $i=2$：$L_2=5, x_2=-0.1, \\nabla_2 f(x)=4.0, \\lambda=0.5$。\n$x_2^+ = S_{0.5/5}(-0.1 - \\frac{1}{5}(4.0)) = S_{0.1}(-0.9) = -(|-0.9|-0.1) = -0.8$。\n$G_2(x) = L_2(x_2-x_2^+) = 5(-0.1 - (-0.8)) = 5(0.7) = 3.5$。\n$\\Delta_2 = \\frac{1}{2L_2}G_2(x)^2 = \\frac{1}{2(5)}(3.5)^2 = \\frac{12.25}{10} = 1.225 = \\frac{49}{40}$。\n\n对于 $i=3$：$L_3=5, x_3=0.0, \\nabla_3 f(x)=0.8, \\lambda=0.5$。\n$x_3^+ = S_{0.5/5}(0.0 - \\frac{1}{5}(0.8)) = S_{0.1}(-0.16) = -(|-0.16|-0.1) = -0.06$。\n$G_3(x) = L_3(x_3-x_3^+) = 5(0.0 - (-0.06)) = 5(0.06) = 0.3$。\n$\\Delta_3 = \\frac{1}{2L_3}G_3(x)^2 = \\frac{1}{2(5)}(0.3)^2 = \\frac{0.09}{10} = 0.009 = \\frac{9}{1000}$。\n\n**4. 均匀采样分析**\n\n对于均匀采样概率 $p_i^{\\text{unif}} = 1/3$（对所有 $i$），我们计算：\n期望的每次迭代下降量：$\\mathbb{E}[\\Delta] = \\sum_{i=1}^{3} p_{i}^{\\text{unif}} \\Delta_{i} = \\frac{1}{3}(\\Delta_1 + \\Delta_2 + \\Delta_3)$。\n$\\mathbb{E}[\\Delta] = \\frac{1}{3}\\left(\\frac{1}{25} + \\frac{49}{40} + \\frac{9}{1000}\\right) = \\frac{1}{3}\\left(\\frac{40}{1000} + \\frac{1225}{1000} + \\frac{9}{1000}\\right) = \\frac{1}{3}\\frac{1274}{1000} = \\frac{637}{1500}$。\n\n期望的每次迭代成本：$\\mathbb{E}[w] = \\sum_{i=1}^{3} p_{i}^{\\text{unif}} w_{i}$。给定成本 $w_1=1, w_2=3, w_3=2$。\n$\\mathbb{E}[w] = \\frac{1}{3}(1+3+2) = \\frac{6}{3} = 2$。\n\n每单位成本的期望下降量：$\\frac{\\mathbb{E}[\\Delta]}{\\mathbb{E}[w]} = \\frac{637/1500}{2} = \\frac{637}{3000}$。\n\n**5. 最优采样分布**\n\n我们要找到概率分布 $p=(p_1, p_2, p_3)$，它在预算和概率约束下最大化期望下降量 $\\sum p_i \\Delta_i$。这是一个线性规划（LP）问题：\n$$\\text{最大化} \\quad \\sum_{i=1}^3 p_i \\Delta_i$$\n$$\\text{约束条件为} \\quad \\sum_{i=1}^3 p_i = 1, \\quad \\sum_{i=1}^3 p_i w_i \\le B, \\quad p_i \\ge 0 \\text{ for } i=1,2,3.$$\n这里，$B=2$，$w=(1,3,2)^T$，且 $\\Delta = (1/25, 49/40, 9/1000)^T$。\n\n最多两个非零坐标的论证：概率的可行集是一个凸多面体。目标函数是线性的。线性规划的一个基本定理指出，LP 的最优值总是在可行集的一个顶点（极点）处达到。在 $\\mathbb{R}^n$ 中由 $k$ 个线性等式约束定义的集合的极点必须至少有 $n-k$ 个零分量。这里，问题是在 $\\mathbb{R}^3$ 中。可行集由约束 $\\sum p_i = 1$ 和 $\\sum p_i w_i \\le B$ 定义。如果预算约束是激活的，我们有两个等式约束。在 $\\mathbb{R}^3$ 中，具有两个等式约束的基本可行解将有至多两个非零分量。如果预算约束是非激活的，则顶点是概率单纯形的顶点，即 $(1,0,0), (0,1,0), (0,0,1)$，每个都只有一个非零分量。更正式地说，任何具有三个非零分量的可行点 $p$ 都可以写成另外两个不同可行点 $p \\pm \\epsilon d$ 的凸组合，其中 $d$ 是定义激活约束的矩阵的零空间中的一个非零向量。因此，$p$ 不可能是一个极点。由于最优点必须在极点处，最优解 $p^*$ 将有至多两个非零坐标。\n\n为了求解这个 LP，我们找出可行域的顶点，并在每个顶点上评估目标函数。约束条件是：\n$p_1+p_2+p_3 = 1$\n$p_1+3p_2+2p_3 \\le 2$\n$p_1, p_2, p_3 \\ge 0$\n我们将 $p_3 = 1 - p_1 - p_2$ 代入预算约束中：\n$p_1+3p_2+2(1-p_1-p_2) \\le 2 \\implies p_1+3p_2+2-2p_1-2p_2 \\le 2 \\implies -p_1+p_2 \\le 0 \\implies p_2 \\le p_1$。\n在 $(p_1, p_2)$ 平面上的可行域由 $p_1 \\ge 0$，$p_2 \\ge 0$，$p_1+p_2 \\le 1$ 和 $p_2 \\le p_1$ 定义。这个区域的顶点是：\n- $(p_1,p_2) = (0,0) \\implies p=(0,0,1)$。成本：$2$。有效。\n- $(p_1,p_2) = (1,0) \\implies p=(1,0,0)$。成本：$1$。有效。\n- $(p_1,p_2) = (1/2, 1/2) \\implies p=(1/2,1/2,0)$。成本：$1/2(1) + 1/2(3) = 2$。有效。\n这三个点是可行集的顶点。让我们在这些顶点上评估目标函数 $J(p) = p_1 \\Delta_1 + p_2 \\Delta_2 + p_3 \\Delta_3$。\n- $p=(0,0,1): J = \\Delta_3 = \\frac{9}{1000} = 0.009$。\n- $p=(1,0,0): J = \\Delta_1 = \\frac{1}{25} = \\frac{40}{1000} = 0.04$。\n- $p=(1/2,1/2,0): J = \\frac{1}{2}\\Delta_1 + \\frac{1}{2}\\Delta_2 = \\frac{1}{2}\\left(\\frac{1}{25} + \\frac{49}{40}\\right) = \\frac{1}{2}\\left(\\frac{8}{200} + \\frac{245}{200}\\right) = \\frac{253}{400} = 0.6325$。\n\n比较这些值，最大的期望下降量在 $p^{\\star} = (1/2, 1/2, 0)$ 处达到。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2}  0 \\end{pmatrix}}\n$$"
        }
    ]
}