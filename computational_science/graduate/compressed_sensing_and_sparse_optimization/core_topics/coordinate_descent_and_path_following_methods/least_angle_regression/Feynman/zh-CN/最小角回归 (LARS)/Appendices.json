{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握最小角回归（LARS），第一步是理解其几何核心：等角方向。这个练习旨在剥离完整算法的迭代复杂性，让您专注于从第一性原理出发，推导出一个关键向量——该向量与所有活动预测变量保持相等的相关性。通过在一个简洁的二维案例  中进行推导，您将为LARS算法背后的几何直觉打下坚实的基础。",
            "id": "3456886",
            "problem": "考虑一个线性模型，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times 2}$，其列 $x_{1}$ 和 $x_{2}$ 经过标准化，使得格拉姆矩阵 $X^{\\top}X$ 等于\n$$\nG \\equiv X^{\\top}X \\;=\\; \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix},\n$$\n其中 $0  \\rho  1$。在最小角回归（Least Angle Regression, LARS）算法的某一步，活动集为 $A = \\{1,2\\}$，符号为 $s_{A} = (1,1)^{\\top}$。定义等角方向 $u_{A}$ 为在 $X_{A} = [x_{1}, x_{2}]$ 的张成空间中的唯一向量，其欧几里得范数为单位1，且与活动预测变量具有相等的带符号相关性，即对于某个标量 $A_{A} > 0$，有 $X_{A}^{\\top} u_{A} = A_{A} s_{A}$。令 $a \\equiv X^{\\top} u_{A}$ 表示所有预测变量与该等角方向的相关性向量。\n\n仅从这些定义和基础线性代数（格拉姆矩阵、欧几里得范数和线性无关性）出发，推导以下各项关于 $\\rho$ 的显式表达式：\n- 系数向量 $w_{A} \\in \\mathbb{R}^{2}$，使得 $u_{A} = X_{A} w_{A}$，\n- 向量 $a \\in \\mathbb{R}^{2}$。\n\n将您的最终答案表示为一个单行矩阵，其中包含 $w_{A}$ 的两个分量，后跟 $a$ 的两个分量，按此顺序排列。不需要进行数值四舍五入，也不涉及物理单位。",
            "solution": "该问题已经过有效性核查，被认定为科学上合理、提法明确、客观且内部一致。我们可以继续进行解答。\n\n问题要求在最小角回归（LARS）算法的特定步骤中，给出与等角方向 $u_A$ 相关联的系数向量 $w_A$ 和相关性向量 $a$ 的显式表达式。\n\n我们已知以下定义和条件：\n1. 设计矩阵为 $X \\in \\mathbb{R}^{n \\times 2}$，其列为 $x_1$ 和 $x_2$。活动集为 $A = \\{1, 2\\}$，因此活动预测变量矩阵为 $X_A = X$。\n2. 格拉姆矩阵为 $G = X_A^{\\top}X_A = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$，其中 $0  \\rho  1$。\n3. 等角方向 $u_A$ 在 $X_A$ 的列向量的张成空间中，因此对于某个向量 $w_A \\in \\mathbb{R}^2$，有 $u_A = X_A w_A$。\n4. 向量 $u_A$ 的欧几里得范数为单位1：$\\|u_A\\|_2^2 = 1$。\n5. 向量 $u_A$ 相对于活动预测变量是等角的，其符号由 $s_A = (1, 1)^{\\top}$ 给出。这意味着对于某个标量 $A_A > 0$，有 $X_A^{\\top} u_A = A_A s_A$。\n6. 相关性向量定义为 $a = X^{\\top} u_A$。由于 $X_A = X$，因此 $a = X_A^{\\top} u_A$。\n\n根据定义6和5，我们可以立即将 $a$ 与 $A_A$ 和 $s_A$ 联系起来：\n$$a = X_A^{\\top} u_A = A_A s_A$$\n为了求出 $a$，我们必须首先确定标量 $A_A$ 的值。\n\n让我们结合给定的定义。将 $u_A = X_A w_A$ 代入等角条件，得到：\n$$X_A^{\\top} (X_A w_A) = A_A s_A$$\n$$(X_A^{\\top} X_A) w_A = A_A s_A$$\n使用格拉姆矩阵 $G$ 的定义，这变成了一个关于 $w_A$ 的线性系统：\n$$G w_A = A_A s_A$$\n$G$ 的行列式是 $\\det(G) = 1 \\cdot 1 - \\rho \\cdot \\rho = 1 - \\rho^2$。由于已知 $0  \\rho  1$，因此 $0  1 - \\rho^2  1$，所以 $\\det(G) \\neq 0$。因此，$G$ 是可逆的，我们可以解出 $w_A$：\n$$w_A = G^{-1} (A_A s_A) = A_A (G^{-1} s_A)$$\n这将 $w_A$ 表示为关于未知标量 $A_A$ 的形式。\n\n为了求 $A_A$，我们使用单位范数条件 $\\|u_A\\|_2^2 = 1$。让我们用 $w_A$ 来表示范数：\n$$\\|u_A\\|_2^2 = (X_A w_A)^{\\top} (X_A w_A) = w_A^{\\top} (X_A^{\\top} X_A) w_A = w_A^{\\top} G w_A = 1$$\n现在，将表达式 $w_A = A_A G^{-1} s_A$ 代入这个范数方程：\n$$(A_A G^{-1} s_A)^{\\top} G (A_A G^{-1} s_A) = 1$$\n$$A_A^2 (s_A^{\\top} (G^{-1})^{\\top}) G (G^{-1} s_A) = 1$$\n格拉姆矩阵 $G$ 是对称的，所以它的逆 $G^{-1}$ 也是对称的，即 $(G^{-1})^{\\top} = G^{-1}$。方程简化为：\n$$A_A^2 s_A^{\\top} G^{-1} G G^{-1} s_A = 1$$\n$$A_A^2 s_A^{\\top} G^{-1} s_A = 1$$\n我们可以解出 $A_A^2$：\n$$A_A^2 = \\frac{1}{s_A^{\\top} G^{-1} s_A}$$\n为了继续，我们必须计算 $G^{-1}$ 和二次型 $s_A^{\\top} G^{-1} s_A$。$2 \\times 2$ 矩阵 $G$ 的逆是：\n$$G^{-1} = \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix}$$\n现在，我们计算向量 $G^{-1}s_A$，其中 $s_A = (1, 1)^{\\top}$：\n$$G^{-1} s_A = \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1 - \\rho \\\\ 1 - \\rho \\end{pmatrix}$$\n将 $1 - \\rho^2 = (1-\\rho)(1+\\rho)$ 因式分解，并注意到 $1-\\rho \\neq 0$，我们简化得到：\n$$G^{-1} s_A = \\frac{1-\\rho}{(1-\\rho)(1+\\rho)} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{1+\\rho} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n接下来，我们计算标量 $s_A^{\\top} G^{-1} s_A$：\n$$s_A^{\\top} G^{-1} s_A = \\begin{pmatrix} 1  1 \\end{pmatrix} \\left( \\frac{1}{1+\\rho} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{1+\\rho} (1 \\cdot 1 + 1 \\cdot 1) = \\frac{2}{1+\\rho}$$\n将此结果代回 $A_A^2$ 的表达式中：\n$$A_A^2 = \\frac{1}{2/(1+\\rho)} = \\frac{1+\\rho}{2}$$\n由于已知 $A_A > 0$，我们取正平方根：\n$$A_A = \\sqrt{\\frac{1+\\rho}{2}}$$\n现在，我们已经确定了 $A_A$，可以找到 $w_A$ 和 $a$ 的显式表达式。\n\n对于系数向量 $w_A$：\n$$w_A = A_A (G^{-1}s_A) = \\left(\\sqrt{\\frac{1+\\rho}{2}}\\right) \\left(\\frac{1}{1+\\rho} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\right)$$\n$$w_A = \\frac{\\sqrt{1+\\rho}}{\\sqrt{2}} \\frac{1}{(\\sqrt{1+\\rho})^2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}\\sqrt{1+\\rho}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2(1+\\rho)}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n所以，$w_A$ 的分量是 $w_1 = \\frac{1}{\\sqrt{2(1+\\rho)}}$ 和 $w_2 = \\frac{1}{\\sqrt{2(1+\\rho)}}$。\n\n对于相关性向量 $a$：\n$$a = A_A s_A = \\sqrt{\\frac{1+\\rho}{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n所以，$a$ 的分量是 $a_1 = \\sqrt{\\frac{1+\\rho}{2}}$ 和 $a_2 = \\sqrt{\\frac{1+\\rho}{2}}$。\n\n最终答案由四个分量 $[w_1, w_2, a_1, a_2]$ 排列成一个行矩阵。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2(1+\\rho)}}  \\frac{1}{\\sqrt{2(1+\\rho)}}  \\sqrt{\\frac{1+\\rho}{2}}  \\sqrt{\\frac{1+\\rho}{2}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在理解了等角方向的概念之后，下一步自然是将其付诸实践，运行完整的LARS算法。这个练习  将挑战您应用完整的LARS流程，包括计算残差、识别下一个进入模型的变量，以及精确确定步长$ \\gamma $。通过手动执行这些步骤，您将亲身体验模型系数路径是如何动态演进的，从而加深对算法每一步背后机制的理解。",
            "id": "3456897",
            "problem": "考虑一个线性模型，其设计矩阵为 $X \\in \\mathbb{R}^{3 \\times 3}$，响应向量为 $y \\in \\mathbb{R}^{3}$。设\n$$\nX \\;=\\; \\begin{pmatrix}\n1  1  0 \\\\\n0  1  1 \\\\\n1  0  1\n\\end{pmatrix},\n\\qquad\ny \\;=\\; \\begin{pmatrix}\n3 \\\\ 1 \\\\ 0\n\\end{pmatrix}.\n$$\n假设 $X$ 的列向量已被归一化，具有单位欧几里得范数。使用经典的最小角回归 (LAR) 算法（无最小绝对收缩和选择算子 (LASSO) 修改），从零系数向量开始，执行 LAR 的前两个步骤。根据核心定义进行计算：在每一步中，选择与当前残差具有最大绝对相关性的预测变量作为活动集，将拟合值沿相对于活动预测变量的等角方向移动，并选择步长，使得一个新的预测变量与活动集达到相等的绝对相关性。对于前两个步骤中的每一步，计算：\n- 等角方向 $u_{A}$，\n- 达到下一个事件的步长 $\\gamma$，\n- 以及采取该步长后产生的系数更新 $\\beta$。\n\n为所有中间量提供精确的解析值。对于最终答案，报告第二步步长的精确值，记为 $\\gamma_{2}$。不要近似或四舍五入；请提供 $\\gamma_{2}$ 的精确闭式表达式。",
            "solution": "该问题陈述经核实具有科学依据、问题明确且客观。它是最小角回归 (LAR) 算法的直接应用。我们现在开始求解。\n\n首先，根据规定，我们必须将设计矩阵 $X$ 的列向量归一化，使其具有单位欧几里得范数。给定的矩阵是\n$$\nX_{unnormalized} = \\begin{pmatrix} 1  1  0 \\\\ 0  1  1 \\\\ 1  0  1 \\end{pmatrix}\n$$\n其列向量为 $x_{un,1} = (1, 0, 1)^\\top$、$x_{un,2} = (1, 1, 0)^\\top$ 和 $x_{un,3} = (0, 1, 1)^\\top$。每个列向量的欧几里得范数为 $\\|x_{un,j}\\|_2 = \\sqrt{1^2+1^2} = \\sqrt{2}$。我们将每个列向量除以 $\\sqrt{2}$，得到归一化后的设计矩阵 $X$：\n$$\nX = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  1  0 \\\\ 0  1  1 \\\\ 1  0  1 \\end{pmatrix}\n$$\n设归一化后的列向量记为 $x_1$、$x_2$ 和 $x_3$。响应向量为 $y = (3, 1, 0)^\\top$。\n\nLAR 算法从零系数向量 $\\beta_0 = (0, 0, 0)^\\top$ 开始。初始拟合值为 $\\mu_0 = X\\beta_0 = (0, 0, 0)^\\top$，初始残差为 $r_0 = y - \\mu_0 = y = (3, 1, 0)^\\top$。\n\n**第 1 步：LAR 第一步**\n\n首先，我们计算预测变量与初始残差 $r_0$ 的相关性。相关性向量为 $c = X^\\top r_0$：\n$$\nc = X^\\top r_0 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  0  1 \\\\ 1  1  0 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 \\\\ 4 \\\\ 1 \\end{pmatrix}\n$$\n绝对相关性分别为 $|\\frac{3}{\\sqrt{2}}|$、$|\\frac{4}{\\sqrt{2}}|$ 和 $|\\frac{1}{\\sqrt{2}}|$。最大绝对相关性是 $|c_2| = \\frac{4}{\\sqrt{2}}$。因此，初始活动集为 $\\mathcal{A}_1 = \\{2\\}$。\n\n拟合值的等角方向 $u_{\\mathcal{A}_1}$ 与相关性最强的预测变量对齐，并由其相关性的符号决定。由于 $c_2 > 0$，该方向就是单位向量 $x_2$。\n$$\nu_{\\mathcal{A}_1} = x_2 = \\frac{1}{\\sqrt{2}}(1, 1, 0)^\\top\n$$\n我们将拟合值从 $\\mu_0$ 沿此方向移动：$\\mu(\\gamma) = \\mu_0 + \\gamma u_{\\mathcal{A}_1} = \\gamma x_2$。残差变为 $r(\\gamma) = r_0 - \\gamma x_2$。\n步长 $\\gamma_1$ 是最小的正数 $\\gamma$，使得某个其他预测变量 $j \\notin \\mathcal{A}_1$ 与 $r(\\gamma)$ 的相关性大小与预测变量 $2$ 的相同：$|x_j^\\top r(\\gamma)| = |x_2^\\top r(\\gamma)|$。\n相关性演变为 $c_j(\\gamma) = x_j^\\top (r_0 - \\gamma x_2) = c_j - \\gamma (x_j^\\top x_2)$。\n我们需要计算 $j \\in \\{1, 3\\}$ 时内积 $x_j^\\top x_2$：\n$x_1^\\top x_2 = \\frac{1}{2}(1 \\cdot 1 + 0 \\cdot 1 + 1 \\cdot 0) = \\frac{1}{2}$。\n$x_3^\\top x_2 = \\frac{1}{2}(0 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 0) = \\frac{1}{2}$。\n由于 $x_2$ 是归一化的，所以 $x_2^\\top x_2 = 1$。\n我们需要在以下方程中求解 $\\gamma > 0$：\n1. 对于 $j=1$：$|c_1 - \\gamma (x_1^\\top x_2)| = |c_2 - \\gamma (x_2^\\top x_2)| \\implies |\\frac{3}{\\sqrt{2}} - \\frac{\\gamma}{2}| = |\\frac{4}{\\sqrt{2}} - \\gamma|$。\n   - $\\frac{3}{\\sqrt{2}} - \\frac{\\gamma}{2} = \\frac{4}{\\sqrt{2}} - \\gamma \\implies \\frac{\\gamma}{2} = \\frac{1}{\\sqrt{2}} \\implies \\gamma = \\sqrt{2}$。\n   - $\\frac{3}{\\sqrt{2}} - \\frac{\\gamma}{2} = -(\\frac{4}{\\sqrt{2}} - \\gamma) \\implies \\frac{7}{\\sqrt{2}} = \\frac{3\\gamma}{2} \\implies \\gamma = \\frac{14}{3\\sqrt{2}} = \\frac{7\\sqrt{2}}{3}$。\n2. 对于 $j=3$：$|c_3 - \\gamma (x_3^\\top x_2)| = |c_2 - \\gamma (x_2^\\top x_2)| \\implies |\\frac{1}{\\sqrt{2}} - \\frac{\\gamma}{2}| = |\\frac{4}{\\sqrt{2}} - \\gamma|$。\n   - $\\frac{1}{\\sqrt{2}} - \\frac{\\gamma}{2} = \\frac{4}{\\sqrt{2}} - \\gamma \\implies \\frac{\\gamma}{2} = \\frac{3}{\\sqrt{2}} \\implies \\gamma = 3\\sqrt{2}$。\n   - $\\frac{1}{\\sqrt{2}} - \\frac{\\gamma}{2} = -(\\frac{4}{\\sqrt{2}} - \\gamma) \\implies \\frac{5}{\\sqrt{2}} = \\frac{3\\gamma}{2} \\implies \\gamma = \\frac{10}{3\\sqrt{2}} = \\frac{5\\sqrt{2}}{3}$。\n最小的正值为 $\\gamma_1 = \\sqrt{2}$。\n\n步长为 $\\gamma_1 = \\sqrt{2}$。在此步结束时，拟合值为 $\\mu_1 = \\mu_0 + \\gamma_1 u_{\\mathcal{A}_1} = \\sqrt{2} x_2 = (1, 1, 0)^\\top$。系数向量 $\\beta_1$ 必须满足 $X\\beta_1 = \\mu_1 = \\sqrt{2} x_2$。这意味着 $\\beta_1$ 仅在索引 2 处有非零元素，其值为 $\\sqrt{2}$。\n$$\n\\beta_1 = (0, \\sqrt{2}, 0)^\\top\n$$\n\n**第 2 步：LAR 第二步**\n\n在第二步开始时，活动集为 $\\mathcal{A}_2 = \\{1, 2\\}$，因为预测变量 1 此时已达到与预测变量 2 相同的绝对相关性。\n残差为 $r_1 = y - \\mu_1 = (3, 1, 0)^\\top - (1, 1, 0)^\\top = (2, 0, 0)^\\top$。\n我们来验证与 $r_1$ 的相关性：\n$c(r_1) = X^\\top r_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  0  1 \\\\ 1  1  0 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ \\sqrt{2} \\\\ 0 \\end{pmatrix}$。\n正如预期， $|c_1| = |c_2| > |c_3|$。活动集的相关性符号为 $s_{\\mathcal{A}_2} = (\\text{sign}(c_1), \\text{sign}(c_2))^\\top = (1, 1)^\\top$。\n\n新的等角方向 $u_{\\mathcal{A}_2}$ 是一个与 $s_1 x_1$ 和 $s_2 x_2$ 夹角相等的单位向量。它由 $u_{\\mathcal{A}_2} \\propto X_{\\mathcal{A}_2} (X_{\\mathcal{A}_2}^\\top X_{\\mathcal{A}_2})^{-1} s_{\\mathcal{A}_2}$ 给出。令 $X_{\\mathcal{A}_2} = [x_1, x_2]$。\n$X_{\\mathcal{A}_2}^\\top X_{\\mathcal{A}_2} = \\begin{pmatrix} x_1^\\top x_1  x_1^\\top x_2 \\\\ x_2^\\top x_1  x_2^\\top x_2 \\end{pmatrix} = \\begin{pmatrix} 1  1/2 \\\\ 1/2  1 \\end{pmatrix}$。\n其逆矩阵为 $(X_{\\mathcal{A}_2}^\\top X_{\\mathcal{A}_2})^{-1} = \\frac{1}{1-(1/2)^2} \\begin{pmatrix} 1  -1/2 \\\\ -1/2  1 \\end{pmatrix} = \\frac{4}{3} \\begin{pmatrix} 1  -1/2 \\\\ -1/2  1 \\end{pmatrix} = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix}$。\n方向的权重为 $w = (X_{\\mathcal{A}_2}^\\top X_{\\mathcal{A}_2})^{-1} s_{\\mathcal{A}_2} = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix}$。\n未归一化的方向是 $u'_{\\mathcal{A}_2} = X_{\\mathcal{A}_2} w = \\frac{2}{3}x_1 + \\frac{2}{3}x_2 = \\frac{2}{3} (\\frac{1}{\\sqrt{2}}(1,0,1)^\\top + \\frac{1}{\\sqrt{2}}(1,1,0)^\\top) = \\frac{\\sqrt{2}}{3}(2,1,1)^\\top$。\n其范数为 $\\|u'_{\\mathcal{A}_2}\\| = \\frac{\\sqrt{2}}{3} \\sqrt{2^2+1^2+1^2} = \\frac{\\sqrt{2}\\sqrt{6}}{3} = \\frac{2\\sqrt{3}}{3}$。\n归一化后的等角方向为 $u_{\\mathcal{A}_2} = \\frac{u'_{\\mathcal{A}_2}}{\\|u'_{\\mathcal{A}_2}\\|} = \\frac{\\frac{\\sqrt{2}}{3}(2,1,1)^\\top}{2\\sqrt{3}/3} = \\frac{\\sqrt{2}}{2\\sqrt{3}}(2,1,1)^\\top = \\frac{1}{\\sqrt{6}}(2,1,1)^\\top$。\n\n步长 $\\gamma_2$ 通过确定非活动预测变量 $x_3$ 何时达到与活动集相同的绝对相关性来找到。拟合值移动方式为 $\\mu(\\gamma) = \\mu_1+\\gamma u_{\\mathcal{A}_2}$，残差移动方式为 $r(\\gamma) = r_1 - \\gamma u_{\\mathcal{A}_2}$。\n相关性演变为 $c_j(\\gamma) = c_j(r_1) - \\gamma(x_j^\\top u_{\\mathcal{A}_2})$。\n我们需要内积 $x_j^\\top u_{\\mathcal{A}_2}$：\n$x_1^\\top u_{\\mathcal{A}_2} = \\frac{1}{\\sqrt{2}}(1,0,1)^\\top \\cdot \\frac{1}{\\sqrt{6}}(2,1,1)^\\top = \\frac{3}{\\sqrt{12}} = \\frac{\\sqrt{3}}{2}$。\n$x_2^\\top u_{\\mathcal{A}_2} = \\frac{1}{\\sqrt{2}}(1,1,0)^\\top \\cdot \\frac{1}{\\sqrt{6}}(2,1,1)^\\top = \\frac{3}{\\sqrt{12}} = \\frac{\\sqrt{3}}{2}$。\n$x_3^\\top u_{\\mathcal{A}_2} = \\frac{1}{\\sqrt{2}}(0,1,1)^\\top \\cdot \\frac{1}{\\sqrt{6}}(2,1,1)^\\top = \\frac{2}{\\sqrt{12}} = \\frac{1}{\\sqrt{3}}$。\n我们令 $|c_3(\\gamma)| = |c_1(\\gamma)|$：\n$|c_3(r_1) - \\gamma(x_3^\\top u_{\\mathcal{A}_2})| = |c_1(r_1) - \\gamma(x_1^\\top u_{\\mathcal{A}_2})|$\n$|0 - \\gamma \\frac{1}{\\sqrt{3}}| = |\\sqrt{2} - \\gamma \\frac{\\sqrt{3}}{2}|$。\n假设 $\\gamma$ 足够小，使得 $\\sqrt{2} - \\gamma \\frac{\\sqrt{3}}{2} > 0$，我们求解：\n$\\frac{\\gamma}{\\sqrt{3}} = \\sqrt{2} - \\gamma \\frac{\\sqrt{3}}{2} \\implies \\gamma (\\frac{1}{\\sqrt{3}} + \\frac{\\sqrt{3}}{2}) = \\sqrt{2} \\implies \\gamma (\\frac{2+3}{2\\sqrt{3}}) = \\sqrt{2} \\implies \\gamma \\frac{5}{2\\sqrt{3}} = \\sqrt{2}$。\n这得到 $\\gamma = \\frac{2\\sqrt{6}}{5}$。\n另一种可能性是 $\\frac{\\gamma}{\\sqrt{3}} = -(\\sqrt{2} - \\gamma\\frac{\\sqrt{3}}{2})$，这会得到 $\\gamma=2\\sqrt{6}$。步长 $\\gamma_2$ 是这些正解中的最小值。\n因此，第二步的步长为 $\\gamma_2 = \\frac{2\\sqrt{6}}{5}$。\n\n最后，我们求出此步后的系数向量 $\\beta_2$。活动系数 $(\\beta_1, \\beta_2)$ 的变化与权重 $w = (2/3, 2/3)^\\top$ 成正比。总的系数更新为 $\\Delta\\beta_{\\mathcal{A}_2} = \\frac{\\gamma_2}{\\|u'_{\\mathcal{A}_2}\\|} w = \\frac{2\\sqrt{6}/5}{2\\sqrt{3}/3} \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix} = \\frac{3\\sqrt{2}}{5} \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix} = \\begin{pmatrix} 2\\sqrt{2}/5 \\\\ 2\\sqrt{2}/5 \\end{pmatrix}$。\n新的系数向量为 $\\beta_2 = \\beta_1 + \\Delta\\beta = (0, \\sqrt{2}, 0)^\\top + (\\frac{2\\sqrt{2}}{5}, \\frac{2\\sqrt{2}}{5}, 0)^\\top$。\n$$\n\\beta_2 = \\left(\\frac{2\\sqrt{2}}{5}, \\sqrt{2}+\\frac{2\\sqrt{2}}{5}, 0\\right)^\\top = \\left(\\frac{2\\sqrt{2}}{5}, \\frac{7\\sqrt{2}}{5}, 0\\right)^\\top\n$$\n问题要求的是 $\\gamma_2$ 的值。\n\n前两步计算量的总结：\n第 1 步：\n- 等角方向 $u_{\\mathcal{A}_1} = \\frac{1}{\\sqrt{2}}(1, 1, 0)^\\top$\n- 步长 $\\gamma_1 = \\sqrt{2}$\n- 系数向量 $\\beta_1 = (0, \\sqrt{2}, 0)^\\top$\n\n第 2 步：\n- 等角方向 $u_{\\mathcal{A}_2} = \\frac{1}{\\sqrt{6}}(2, 1, 1)^\\top$\n- 步长 $\\gamma_2 = \\frac{2\\sqrt{6}}{5}$\n- 系数向量 $\\beta_2 = (\\frac{2\\sqrt{2}}{5}, \\frac{7\\sqrt{2}}{5}, 0)^\\top$",
            "answer": "$$\n\\boxed{\\frac{2\\sqrt{6}}{5}}\n$$"
        },
        {
            "introduction": "对于已经掌握了LARS基础知识的学习者来说，这项高级练习旨在培养对稀疏建模领域的深刻理解。LARS是一系列相关算法中的一员，本问题  探讨了它与其近亲——前向分段回归（Forward Stagewise regression）之间的微妙关系。通过构建一个它们的解路径发生分歧的特定场景，您将认识到算法定义上的微小差异如何导致不同的模型构建决策，这对于高级应用和算法选择至关重要。",
            "id": "3456905",
            "problem": "考虑一个线性回归问题，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times 3}$，其列 $x_{1}, x_{2}, x_{3}$ 经过中心化和缩放，使得对任意 $j \\in \\{1,2,3\\}$ 都有 $\\|x_{j}\\|_{2} = 1$。格拉姆矩阵 $G = X^{\\top} X \\in \\mathbb{R}^{3 \\times 3}$ 为\n$$\nG \\;=\\; \\begin{pmatrix}\n1  0.5  0.5 \\\\\n0.5  1  -0.4 \\\\\n0.5  -0.4  1\n\\end{pmatrix}.\n$$\n假设与响应变量的初始相关性严格为正，并按 $X^{\\top} y = (c_{1}^{(0)}, c_{2}^{(0)}, c_{3}^{(0)})^{\\top}$ 排序，其中 $c_{1}^{(0)}  c_{2}^{(0)}  c_{3}^{(0)}  0$，因此变量按 $1$、然后 $2$、然后 $3$ 的顺序进入活动集，并且进入时的符号均为正。在无穷小步长极限下，同时考虑最小角回归的Lasso修改版（LARS-Lasso）和无穷小前向分步回归，并通过$\\ell_1$弧长$t$对系数路径进行参数化，使得每个无穷小方向向量都被归一化为单位 $\\ell_{1}$ 速率。\n\n在第三个变量刚进入后的瞬间，活动集为 $A = \\{1,2,3\\}$，其符号向量为 $s_{A} = (1,1,1)^{\\top}$。令 $d_{\\mathrm{LARS}} \\in \\mathbb{R}^{3}$ 为系数空间中的 LARS-Lasso 等角方向，定义为 $G d_{\\mathrm{LARS}} = s_{A}$ 的解。令 $d_{\\mathrm{FS}} \\in \\mathbb{R}^{3}$ 为无穷小前向分步方向，定义为满足 $d_{\\mathrm{FS}} \\ge 0$ 的非负最小二乘方向，且在所有此类方向中，该方向在活动集上保持绝对相关性相等；等价地，$d_{\\mathrm{FS}}$ 是通过在一个子集 $B \\subseteq A$ 上求解 $G_{BB} d_{B} = s_{B}$ 得到的，其中 $d_{B}  0$ 且 $d_{A \\setminus B} = 0$，并选择该子集以满足非负性约束。\n\n将两个方向都归一化为单位 $\\ell_{1}$ 速率，\n$$\nv_{\\mathrm{LARS}} \\;=\\; \\frac{d_{\\mathrm{LARS}}}{\\|d_{\\mathrm{LARS}}\\|_{1}}, \\qquad v_{\\mathrm{FS}} \\;=\\; \\frac{d_{\\mathrm{FS}}}{\\|d_{\\mathrm{FS}}\\|_{1}}.\n$$\n将此拐点处基于曲率的差异定义为这两个单位 $\\ell_{1}$ 速率切向之间的转角，\n$$\n\\theta \\;=\\; \\arccos\\!\\left( \\frac{\\langle v_{\\mathrm{LARS}}, v_{\\mathrm{FS}} \\rangle}{\\|v_{\\mathrm{LARS}}\\|_{2} \\, \\|v_{\\mathrm{FS}}\\|_{2}} \\right),\n$$\n它被诠释为系数路径在分岔事件处的集中曲率。\n\n精确计算 $\\theta$。将最终答案表示为以弧度为单位的闭式解析表达式（不要进行数值取整）。",
            "solution": "用户希望我按照指定的步骤解决问题。\n\n### 第 1 步：提取已知条件\n- 设计矩阵 $X \\in \\mathbb{R}^{n \\times 3}$，其列为 $x_{1}, x_{2}, x_{3}$。\n- 列经过中心化和缩放：$\\|x_{j}\\|_{2} = 1$ 对 $j \\in \\{1,2,3\\}$ 成立。\n- 格拉姆矩阵 $G = X^{\\top} X \\in \\mathbb{R}^{3 \\times 3}$ 为 $G = \\begin{pmatrix} 1  0.5  0.5 \\\\ 0.5  1  -0.4 \\\\ 0.5  -0.4  1 \\end{pmatrix}$。\n- 初始相关性 $X^{\\top} y = (c_{1}^{(0)}, c_{2}^{(0)}, c_{3}^{(0)})^{\\top}$，其中 $c_{1}^{(0)}  c_{2}^{(0)}  c_{3}^{(0)}  0$。\n- 变量按 $1$、然后 $2$、然后 $3$ 的顺序进入活动集。所有进入时的符号均为正。\n- 在关注的瞬间，活动集为 $A = \\{1,2,3\\}$，符号向量为 $s_{A} = (1,1,1)^{\\top}$。\n- LARS-Lasso 方向 $d_{\\mathrm{LARS}}$ 是方程 $G d_{\\mathrm{LARS}} = s_{A}$ 的解。\n- 无穷小前向分步方向 $d_{\\mathrm{FS}}$ 是通过在子集 $B \\subseteq A$ 上求解 $G_{BB} d_{B} = s_{B}$ 求得的，使得 $d_{B}  0$ 且 $d_{A \\setminus B} = 0$，并选择该子集以满足非负性约束 $d_{\\mathrm{FS}} \\ge 0$。\n- 方向被归一化为单位 $\\ell_{1}$ 速率：$v_{\\mathrm{LARS}} = \\frac{d_{\\mathrm{LARS}}}{\\|d_{\\mathrm{LARS}}\\|_{1}}$ 和 $v_{\\mathrm{FS}} = \\frac{d_{\\mathrm{FS}}}{\\|d_{\\mathrm{FS}}\\|_{1}}$。\n- 要计算的角度是 $\\theta = \\arccos\\left( \\frac{\\langle v_{\\mathrm{LARS}}, v_{\\mathrm{FS}} \\rangle}{\\|v_{\\mathrm{LARS}}\\|_{2} \\|v_{\\mathrm{FS}}\\|_{2}} \\right)$。\n\n### 第 2 步：使用提取的已知条件进行验证\n1.  **科学或事实不健全性**：该问题基于稀疏线性回归理论，特别是比较 LARS-Lasso 和前向分步回归。定义和设置与该领域的标准文献一致。给定的格拉姆矩阵 $G$ 必须是对称半正定的。它是对称的。为了检查正定性，我们对主子式使用 Sylvester 准则：\n    - $M_1 = 1  0$。\n    - $M_2 = \\det \\begin{pmatrix} 1  0.5 \\\\ 0.5  1 \\end{pmatrix} = 1 - (0.5)^2 = 0.75  0$。\n    - $M_3 = \\det(G) = 1(1 - (-0.4)^2) - 0.5(0.5 - (0.5)(-0.4)) + 0.5((0.5)(-0.4) - 1(0.5)) = 1(1-0.16) - 0.5(0.5+0.2) + 0.5(-0.2-0.5) = 0.84 - 0.5(0.7) - 0.5(0.7) = 0.84 - 0.35 - 0.35 = 0.14  0$。\n    由于所有主子式都为正，G 是正定的，因此是一个有效的格拉姆矩阵。该问题在科学上是健全的。\n2.  **不可形式化或不相关**：该问题是指定主题内的一个形式化数学问题。\n3 . **不完整或矛盾的设置**：问题是自洽的，并提供了所有必要的信息。\n4.  **不切实际或不可行**：该设置是用于研究算法行为的标准简化场景，并非不切实际。\n5.  **不适定或结构不良**：问题是适定的。待计算的量由所提供的信息唯一确定。\n6.  **伪深刻、平凡或同义反复**：该问题需要特定的知识和计算，并非平凡问题。\n7.  **超出科学可验证性范围**：该问题是一个完全可验证的数学计算。\n\n### 第 3 步：结论和行动\n问题是有效的。我们继续进行求解。\n\n解题过程分为四个阶段：\n1.  计算 LARS-Lasso 方向向量 $d_{\\mathrm{LARS}}$。\n2.  计算无穷小前向分步方向向量 $d_{\\mathrm{FS}}$。\n3.  归一化两个向量得到 $v_{\\mathrm{LARS}}$ 和 $v_{\\mathrm{FS}}$。\n4.  计算归一化向量之间的角度 $\\theta$。\n\n**1. 计算 $d_{\\mathrm{LARS}}$**\nLARS-Lasso 方向 $d_{\\mathrm{LARS}}$ 由等角条件 $G d_{\\mathrm{LARS}} = s_{A}$ 定义，其中 $G$ 是格拉姆矩阵，$s_{A} = (1, 1, 1)^{\\top}$ 是活动集 $A=\\{1,2,3\\}$ 的符号向量。为了找到 $d_{\\mathrm{LARS}}$，我们通过计算 $G^{-1}$ 来求解这个线性系统。\n格拉姆矩阵为 $G = \\begin{pmatrix} 1  0.5  0.5 \\\\ 0.5  1  -0.4 \\\\ 0.5  -0.4  1 \\end{pmatrix}$。\n在验证过程中计算出 $G$ 的行列式为 $\\det(G) = 0.14$。\n逆矩阵为 $G^{-1} = \\frac{1}{\\det(G)} \\text{adj}(G)$。伴随矩阵 $\\text{adj}(G)$ 是代数余子式矩阵的转置。由于 $G$ 是对称的，其代数余子式矩阵也是对称的，因此 $\\text{adj}(G)$ 就是代数余子式矩阵本身。\n代数余子式为：\n$C_{11} = 1 - (-0.4)^2 = 1 - 0.16 = 0.84$\n$C_{12} = -(0.5 - (0.5)(-0.4)) = -(0.5 + 0.2) = -0.7$\n$C_{13} = (0.5)(-0.4) - (1)(0.5) = -0.2 - 0.5 = -0.7$\n$C_{22} = 1 - (0.5)^2 = 1 - 0.25 = 0.75$\n$C_{23} = -(-0.4 - (0.5)(0.5)) = -(-0.4 - 0.25) = 0.65$\n$C_{33} = 1 - (0.5)^2 = 1 - 0.25 = 0.75$\n根据对称性，$C_{21}=C_{12}$，$C_{31}=C_{13}$，$C_{32}=C_{23}$。\n所以，$G^{-1} = \\frac{1}{0.14} \\begin{pmatrix} 0.84  -0.7  -0.7 \\\\ -0.7  0.75  0.65 \\\\ -0.7  0.65  0.75 \\end{pmatrix}$。\n现在我们计算 $d_{\\mathrm{LARS}} = G^{-1} s_{A}$：\n$d_{\\mathrm{LARS}} = \\frac{1}{0.14} \\begin{pmatrix} 0.84  -0.7  -0.7 \\\\ -0.7  0.75  0.65 \\\\ -0.7  0.65  0.75 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{0.14} \\begin{pmatrix} 0.84 - 0.7 - 0.7 \\\\ -0.7 + 0.75 + 0.65 \\\\ -0.7 + 0.65 + 0.75 \\end{pmatrix} = \\frac{1}{0.14} \\begin{pmatrix} -0.56 \\\\ 0.70 \\\\ 0.70 \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ 5 \\\\ 5 \\end{pmatrix}$。\n该方向与 $(-4, 5, 5)^{\\top}$ 成比例。我们可以使用这个向量，因为任何正的缩放因子在角度计算的归一化步骤中都会被抵消。令 $d_{\\mathrm{LARS}} = (-4, 5, 5)^{\\top}$。\n\n**2. 计算 $d_{\\mathrm{FS}}$**\n无穷小前向分步（FS）回归在系数非递减的约束下构建模型。在这里，由于系数以正号进入，其路径值必须为非负。因此，移动方向 $d_{\\mathrm{FS}}$ 必须是非负的，即 $d_{\\mathrm{FS}} \\ge 0$。\nLARS 方向 $d_{\\mathrm{LARS}} = (-4, 5, 5)^{\\top}$ 的第一个分量是负的。这意味着为了在所有三个变量中保持等角条件，第一个系数 $\\beta_1$ 将不得不减小。这在 FS 中是不允许的。因此，在这一点上，FS 路径必须与 LARS-Lasso 路径分岔。\nLARS 方向违反了约束 $d_{\\mathrm{FS},1} \\ge 0$。对于 FS，我们必须设置 $d_{\\mathrm{FS},1} = 0$。FS 的活动集现在减少为 $B=\\{2,3\\}$，并且方向 $d_{B}$ 必须为该子集保持等角条件。\n我们求解 $G_{BB} d_{B} = s_{B}$，其中 $B=\\{2,3\\}$，$G_{BB} = \\begin{pmatrix} 1  -0.4 \\\\ -0.4  1 \\end{pmatrix}$，以及 $s_{B} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n$\\det(G_{BB}) = 1 - (-0.4)^2 = 1 - 0.16 = 0.84$。\n$G_{BB}^{-1} = \\frac{1}{0.84} \\begin{pmatrix} 1  0.4 \\\\ 0.4  1 \\end{pmatrix}$。\n$d_{B} = \\frac{1}{0.84} \\begin{pmatrix} 1  0.4 \\\\ 0.4  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{0.84} \\begin{pmatrix} 1.4 \\\\ 1.4 \\end{pmatrix} = \\frac{1.4}{0.84} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{140}{84} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{5}{3} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n这些分量是正的，因此这是一个有效的 FS 方向。完整的方向向量是 $d_{\\mathrm{FS}} = (0, 5/3, 5/3)^{\\top}$。我们可以使用一个更简单的成比例的向量，$d_{\\mathrm{FS}} = (0, 1, 1)^{\\top}$。\n\n**3. 方向向量的归一化**\n我们将 $d_{\\mathrm{LARS}}$ 和 $d_{\\mathrm{FS}}$ 归一化为单位 $\\ell_1$ 范数。\n对于 LARS-Lasso：\n$d_{\\mathrm{LARS}} = (-4, 5, 5)^{\\top}$\n$\\|d_{\\mathrm{LARS}}\\|_{1} = |-4| + |5| + |5| = 14$。\n$v_{\\mathrm{LARS}} = \\frac{d_{\\mathrm{LARS}}}{\\|d_{\\mathrm{LARS}}\\|_{1}} = \\frac{1}{14} (-4, 5, 5)^{\\top} = (-\\frac{2}{7}, \\frac{5}{14}, \\frac{5}{14})^{\\top}$。\n\n对于前向分步回归：\n$d_{\\mathrm{FS}} = (0, 1, 1)^{\\top}$\n$\\|d_{\\mathrm{FS}}\\|_{1} = |0| + |1| + |1| = 2$。\n$v_{\\mathrm{FS}} = \\frac{d_{\\mathrm{FS}}}{\\|d_{\\mathrm{FS}}\\|_{1}} = \\frac{1}{2} (0, 1, 1)^{\\top} = (0, \\frac{1}{2}, \\frac{1}{2})^{\\top}$。\n\n**4. 计算角度 $\\theta$**\n角度 $\\theta$ 由公式 $\\theta = \\arccos\\left( \\frac{\\langle v_{\\mathrm{LARS}}, v_{\\mathrm{FS}} \\rangle}{\\|v_{\\mathrm{LARS}}\\|_{2} \\|v_{\\mathrm{FS}}\\|_{2}} \\right)$ 给出。\n首先，我们计算内积：\n$\\langle v_{\\mathrm{LARS}}, v_{\\mathrm{FS}} \\rangle = (-\\frac{2}{7})(0) + (\\frac{5}{14})(\\frac{1}{2}) + (\\frac{5}{14})(\\frac{1}{2}) = 0 + \\frac{5}{28} + \\frac{5}{28} = \\frac{10}{28} = \\frac{5}{14}$。\n接下来，我们计算 $\\ell_2$ 范数：\n$\\|v_{\\mathrm{LARS}}\\|_{2}^2 = (-\\frac{2}{7})^2 + (\\frac{5}{14})^2 + (\\frac{5}{14})^2 = \\frac{4}{49} + \\frac{25}{196} + \\frac{25}{196} = \\frac{16}{196} + \\frac{25}{196} + \\frac{25}{196} = \\frac{66}{196} = \\frac{33}{98}$。\n$\\|v_{\\mathrm{LARS}}\\|_{2} = \\sqrt{\\frac{33}{98}} = \\frac{\\sqrt{33}}{\\sqrt{49 \\times 2}} = \\frac{\\sqrt{33}}{7\\sqrt{2}}$。\n$\\|v_{\\mathrm{FS}}\\|_{2}^2 = 0^2 + (\\frac{1}{2})^2 + (\\frac{1}{2})^2 = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$。\n$\\|v_{\\mathrm{FS}}\\|_{2} = \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}}$。\n现在，我们计算角度的余弦值：\n$\\cos(\\theta) = \\frac{\\langle v_{\\mathrm{LARS}}, v_{\\mathrm{FS}} \\rangle}{\\|v_{\\mathrm{LARS}}\\|_{2} \\|v_{\\mathrm{FS}}\\|_{2}} = \\frac{\\frac{5}{14}}{\\left(\\frac{\\sqrt{33}}{7\\sqrt{2}}\\right) \\left(\\frac{1}{\\sqrt{2}}\\right)} = \\frac{\\frac{5}{14}}{\\frac{\\sqrt{33}}{7 \\times 2}} = \\frac{\\frac{5}{14}}{\\frac{\\sqrt{33}}{14}} = \\frac{5}{\\sqrt{33}}$。\n最后，角度 $\\theta$ 是：\n$\\theta = \\arccos\\left(\\frac{5}{\\sqrt{33}}\\right)$。",
            "answer": "$$\\boxed{\\arccos\\left(\\frac{5}{\\sqrt{33}}\\right)}$$"
        }
    ]
}