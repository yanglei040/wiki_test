{
    "hands_on_practices": [
        {
            "introduction": "To ground our understanding, we first connect the modern frameworks of Plug-and-Play (PnP) and Regularization by Denoising (RED) to classical methods. This practice reveals that for a simple quadratic prior, both PnP-ADMM and RED are equivalent to the well-known Tikhonov regularization, demonstrating that these advanced techniques are elegant generalizations of established principles . By deriving the effective regularization matrix in each case, you will gain a deeper appreciation for the implicit regularization being performed.",
            "id": "3466500",
            "problem": "Consider the linear inverse problem with measurements modeled as $y \\in \\mathbb{R}^{m}$ and a forward operator $A \\in \\mathbb{R}^{m \\times n}$ under additive white Gaussian noise of variance $\\sigma^{2}$. The data-fidelity term is $f(x) = \\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2}$ with $x \\in \\mathbb{R}^{n}$. Let $L \\in \\mathbb{R}^{n \\times n}$ be a symmetric positive semidefinite matrix and define the quadratic potential $\\phi(x) = \\frac{1}{2} x^{\\top} L x$. Let $\\tau  0$, $\\rho  0$, and $\\beta  0$ be given scalars.\n\n- In the Plug-and-Play (PnP) method implemented via the Alternating Direction Method of Multipliers (ADMM), the $z$-update uses a denoiser that is the proximal operator of $\\tau \\phi$, that is, for any $v \\in \\mathbb{R}^{n}$,\n$$\nD_{\\tau}(v) = \\operatorname{prox}_{\\tau \\phi}(v) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|z - v\\|_{2}^{2} + \\tau \\phi(z) \\right\\}.\n$$\nAssume the ADMM penalty parameter is $\\rho$ and consider the standard variable splitting $x=z$.\n\n- In Regularization by Denoising (RED), consider the variational formulation\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; f(x) + \\beta R(x),\n$$\nwhere the RED regularizer associated with the same denoiser $D_{\\tau}$ is\n$$\nR(x) = \\frac{1}{2} x^{\\top} \\left( x - D_{\\tau}(x) \\right),\n$$\nand assume the denoiser is linear, symmetric, and nonexpansive.\n\nStarting from first principles and core definitions only (proximal operator for convex functions, the ADMM splitting for $x=z$ with penalty parameter $\\rho$, and the RED construction), derive the explicit closed-form expression for the effective Tikhonov regularization matrix in each case. Concretely, show that both PnP-ADMM with the proximal denoiser of $\\tau \\phi$ and RED with the same denoiser reduce to classical Tikhonov regularization of the form\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2} + \\frac{1}{2} x^{\\top} \\Gamma x,\n$$\nand compute each method’s effective regularization matrix $\\Gamma$ explicitly in terms of $\\rho$, $\\tau$, $\\beta$, and $L$. \n\nYour final answer must be the pair consisting of the two effective matrices in a single row matrix, in the order $\\left(\\Gamma_{\\mathrm{PnP}}, \\Gamma_{\\mathrm{RED}}\\right)$, as a closed-form analytic expression. No numerical approximation is required, and no units are involved.",
            "solution": "The supplied problem is valid. It is scientifically grounded in the principles of computational imaging and optimization theory, well-posed, objective, and contains all necessary information for a unique solution. We proceed with the derivation.\n\nThe objective is to demonstrate that both Plug-and-Play Alternating Direction Method of Multipliers (PnP-ADMM) and Regularization by Denoising (RED), under the specified conditions, are equivalent to a Tikhonov regularization problem of the form\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2} + \\frac{1}{2} x^{\\top} \\Gamma x\n$$\nand to determine the explicit expressions for the effective regularization matrix $\\Gamma$ in each case.\n\nFirst, we derive the explicit form of the denoiser $D_{\\tau}(v)$. The denoiser is defined as the proximal operator of the function $\\tau \\phi(z)$, where $\\phi(z) = \\frac{1}{2} z^{\\top} L z$. By definition:\n$$\nD_{\\tau}(v) = \\operatorname{prox}_{\\tau \\phi}(v) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|z - v\\|_{2}^{2} + \\tau \\phi(z) \\right\\} = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|z - v\\|_{2}^{2} + \\frac{\\tau}{2} z^{\\top} L z \\right\\}.\n$$\nThe objective function is quadratic and strictly convex, since $L$ is positive semidefinite and $\\tau  0$, ensuring the Hessian $I + \\tau L$ is positive definite. The unique minimizer is found by setting the gradient with respect to $z$ to zero:\n$$\n\\nabla_{z} \\left( \\frac{1}{2} (z - v)^{\\top}(z - v) + \\frac{\\tau}{2} z^{\\top} L z \\right) = (z - v) + \\tau L z = 0.\n$$\nRearranging the terms, we get $(I + \\tau L)z = v$. Since $I + \\tau L$ is invertible, we can solve for $z$:\n$$\nz = (I + \\tau L)^{-1} v.\n$$\nThus, the denoiser is a linear operator represented by the matrix $(I + \\tau L)^{-1}$:\n$$\nD_{\\tau}(v) = (I + \\tau L)^{-1} v.\n$$\n\nWe will now analyze each method separately.\n\n**Part 1: Plug-and-Play ADMM (PnP-ADMM)**\n\nThe PnP-ADMM algorithm is applied to solve an inverse problem of the form $\\min_x f(x) + g(x)$ by using variable splitting $x=z$. The standard ADMM algorithm involves iterations on $x$, $z$, and a scaled dual variable $u$. The problem states that the $z$-update, which would typically be $\\operatorname{prox}_{g/\\rho}$, is replaced by a denoiser $D_{\\tau}$. The fixed point $(x^*, z^*, u^*)$ of this algorithm, to which the iterates are presumed to converge, must satisfy the following conditions:\n1. $x$-update: $x^* = \\arg\\min_x \\left\\{ f(x) + \\frac{\\rho}{2} \\|x - (z^* - u^*)\\|_2^2 \\right\\}$. The first-order optimality condition for this step is $\\nabla f(x^*) + \\rho(x^* - z^* + u^*) = 0$.\n2. $z$-update (PnP step): $z^* = D_{\\tau}(x^* + u^*)$.\n3. Dual update: $u^* = u^* + x^* - z^*$, which implies the fixed-point constraint $x^* = z^*$.\n\nWe use these fixed-point relations to find the implicit optimization problem that the algorithm solves.\nSubstituting $x^* = z^*$ into the first-order condition from the $x$-update:\n$$\n\\nabla f(x^*) + \\rho(x^* - x^* + u^*) = 0 \\implies \\nabla f(x^*) + \\rho u^* = 0.\n$$\nThis gives an expression for the dual variable at the fixed point:\n$$\nu^* = -\\frac{1}{\\rho} \\nabla f(x^*).\n$$\nNow, substitute $z^* = x^*$ and the expression for $u^*$ into the $z$-update equation:\n$$\nx^* = D_{\\tau}\\left(x^* - \\frac{1}{\\rho} \\nabla f(x^*)\\right).\n$$\nThis is the fixed-point equation that characterizes the solution $x^*$. We now substitute the derived expression for the denoiser $D_{\\tau}(v) = (I + \\tau L)^{-1} v$:\n$$\nx^* = (I + \\tau L)^{-1} \\left(x^* - \\frac{1}{\\rho} \\nabla f(x^*)\\right).\n$$\nMultiplying both sides by the invertible matrix $(I + \\tau L)$:\n$$\n(I + \\tau L)x^* = x^* - \\frac{1}{\\rho} \\nabla f(x^*).\n$$\nExpanding the left side yields $x^* + \\tau L x^* = x^* - \\frac{1}{\\rho} \\nabla f(x^*)$.\nSimplifying, we have $\\tau L x^* = -\\frac{1}{\\rho} \\nabla f(x^*)$, which can be rewritten as:\n$$\n\\nabla f(x^*) + \\rho \\tau L x^* = 0.\n$$\nThis equation is the first-order necessary and sufficient optimality condition for a convex minimization problem. We can identify this problem by finding a function whose gradient corresponds to the left-hand side. Let this function be $J_{\\mathrm{PnP}}(x)$:\n$$\n\\nabla J_{\\mathrm{PnP}}(x) = \\nabla f(x) + \\rho \\tau L x.\n$$\nIntegrating with respect to $x$, we find the objective function (up to a constant):\n$$\nJ_{\\mathrm{PnP}}(x) = f(x) + \\frac{1}{2} x^{\\top} (\\rho \\tau L) x = \\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2} + \\frac{\\rho \\tau}{2} x^{\\top} L x.\n$$\nThis is a Tikhonov regularization problem. By comparing it with the target form $\\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2} + \\frac{1}{2} x^{\\top} \\Gamma_{\\mathrm{PnP}} x$, we can directly identify the effective regularization matrix for PnP-ADMM:\n$$\n\\Gamma_{\\mathrm{PnP}} = \\rho \\tau L.\n$$\n\n**Part 2: Regularization by Denoising (RED)**\n\nThe RED framework posits a variational problem explicitly. The problem is given as:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; f(x) + \\beta R(x),\n$$\nwhere $f(x) = \\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2}$ and the RED regularizer is $R(x) = \\frac{1}{2} x^{\\top} (x - D_{\\tau}(x))$.\nTo find the effective Tikhonov matrix, we substitute the expressions for $f(x)$ and $R(x)$ into the minimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2} + \\frac{\\beta}{2} x^{\\top} (x - D_{\\tau}(x)).\n$$\nWe use the same denoiser expression as before: $D_{\\tau}(x) = (I + \\tau L)^{-1} x$. Substituting this into the regularization term gives:\n$$\n\\frac{\\beta}{2} x^{\\top} (x - (I + \\tau L)^{-1} x) = \\frac{\\beta}{2} x^{\\top} (I - (I + \\tau L)^{-1}) x.\n$$\nWe simplify the matrix expression $I - (I + \\tau L)^{-1}$:\n$$\nI - (I + \\tau L)^{-1} = (I + \\tau L)(I + \\tau L)^{-1} - (I + \\tau L)^{-1} = ((I + \\tau L) - I)(I + \\tau L)^{-1} = \\tau L (I + \\tau L)^{-1}.\n$$\nSubstituting this simplified matrix back into the regularization term, we get:\n$$\n\\frac{\\beta}{2} x^{\\top} (\\tau L (I + \\tau L)^{-1}) x = \\frac{\\beta \\tau}{2} x^{\\top} L (I + \\tau L)^{-1} x.\n$$\nThus, the full RED optimization problem is:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2} + \\frac{1}{2} x^{\\top} \\left( \\beta \\tau L (I + \\tau L)^{-1} \\right) x.\n$$\nThis is also in the form of Tikhonov regularization. By comparing it with the target form $\\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2} + \\frac{1}{2} x^{\\top} \\Gamma_{\\mathrm{RED}} x$, we identify the effective regularization matrix for RED:\n$$\n\\Gamma_{\\mathrm{RED}} = \\beta \\tau L (I + \\tau L)^{-1}.\n$$\nSince $L$ is a symmetric matrix, both derived regularization matrices $\\Gamma_{\\mathrm{PnP}}$ and $\\Gamma_{\\mathrm{RED}}$ are symmetric, consistent with the quadratic form of Tikhonov regularization.\n\n**Conclusion**\n\nWe have shown that both PnP-ADMM and RED, with the specified quadratic potential, reduce to Tikhonov regularization. The effective regularization matrices are $\\Gamma_{\\mathrm{PnP}} = \\rho \\tau L$ and $\\Gamma_{\\mathrm{RED}} = \\beta \\tau L (I + \\tau L)^{-1}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\rho \\tau L  \\beta \\tau L(I + \\tau L)^{-1}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The effectiveness of any regularization method hinges on the proper tuning of its parameters, which invariably involves a trade-off between fidelity to the data and adherence to the prior. This exercise explores the fundamental bias-variance trade-off within a simplified PnP context, using a Gaussian signal and noise model . Analyzing how under- and over-regularization affect the reconstruction error will provide you with crucial intuition for calibrating denoisers in practical applications.",
            "id": "3466508",
            "problem": "Consider a simplified linear Gaussian imaging model consistent with Plug-and-Play priors (PnP) and Regularization by Denoising in compressed sensing. Let the unknown signal be $x_{0} \\in \\mathbb{R}^{n}$ drawn from a zero-mean white Gaussian prior $x_{0} \\sim \\mathcal{N}(0, s^{2} I)$, and let the measurement model be the identity forward operator contaminated by independent Gaussian noise: $y = x_{0} + w$, where $w \\sim \\mathcal{N}(0, \\tau^{2} I)$. Assume the PnP reconstruction in this simplified setting applies a denoiser to the observed data: $\\hat{x}_{\\sigma} = D_{\\sigma}(y)$, where $D_{\\sigma}$ is the minimum mean-square error (MMSE) Gaussian denoiser calibrated to an assumed noise level $\\sigma^{2}$, i.e., $z = x + e$ with $x \\sim \\mathcal{N}(0, s^{2} I)$, $e \\sim \\mathcal{N}(0, \\sigma^{2} I)$, and $D_{\\sigma}(z) = \\mathbb{E}[x \\mid z]$. Here, Plug-and-Play priors (PnP) refer to the use of a denoiser within an optimization scheme such as the Alternating Direction Method of Multipliers (ADMM).\n\nStarting from first principles—namely, the Gaussian conditioning formulas for MMSE estimation and the definitions of bias, variance, and mean-square error (MSE)—carry out the following:\n\n1. Derive the explicit linear form of the MMSE Gaussian denoiser $D_{\\sigma}(z)$ under the given prior and noise assumptions, and write $\\hat{x}_{\\sigma}$ explicitly as a linear shrinkage of $y$.\n2. Using the standard definitions of bias and variance with respect to the measurement noise $w$ conditioned on a fixed realization of $x_{0}$, derive the per-coordinate bias and variance of $\\hat{x}_{\\sigma}$ relative to $x_{0}$, and express the per-coordinate expected mean-square error as a function of $\\sigma$, $s^{2}$, and $\\tau^{2}$.\n3. Analyze the limits of the bias and variance as $\\sigma \\to 0$ (under-regularization) and as $\\sigma \\to \\infty$ (over-regularization), explaining how the regularization strength $\\sigma$ trades bias against variance.\n4. Compute the value of $\\sigma$ that minimizes the per-coordinate expected mean-square error. Express your final answer as a single closed-form analytical expression in terms of $\\tau$ and $s$ only. No numerical approximation is required; do not round.\n\nYour final answer must be a single analytical expression.",
            "solution": "The problem requires a four-part analysis of a simplified Plug-and-Play (PnP) reconstruction scheme. We will proceed by first deriving the explicit form of the estimator, then analyzing its statistical properties (bias, variance, and mean-square error), and finally optimizing the regularization parameter.\n\n**Part 1: Derivation of the MMSE Gaussian Denoiser**\n\nThe PnP reconstruction is given by $\\hat{x}_{\\sigma} = D_{\\sigma}(y)$, where $D_{\\sigma}$ is the Minimum Mean-Square Error (MMSE) denoiser for a specific statistical model. This model assumes an observation $z = x + e$, where the signal $x$ and noise $e$ are independent and drawn from Gaussian distributions: $x \\sim \\mathcal{N}(0, s^{2} I)$ and $e \\sim \\mathcal{N}(0, \\sigma^{2} I)$. The denoiser is defined as the conditional expectation $D_{\\sigma}(z) = \\mathbb{E}[x \\mid z]$.\n\nTo find this conditional expectation, we first characterize the joint distribution of $x$ and $z$. Since they are linear combinations of Gaussian random vectors, they are jointly Gaussian.\nThe means are $\\mathbb{E}[x] = 0$ and $\\mathbb{E}[z] = \\mathbb{E}[x+e] = \\mathbb{E}[x] + \\mathbb{E}[e] = 0$.\n\nThe covariance matrices are:\n- $\\Sigma_{xx} = \\mathbb{E}[xx^T] = s^2 I$\n- The cross-covariance is $\\Sigma_{xz} = \\mathbb{E}[x z^T] = \\mathbb{E}[x(x+e)^T] = \\mathbb{E}[xx^T] + \\mathbb{E}[xe^T]$. Since $x$ and $e$ are independent, $\\mathbb{E}[xe^T] = \\mathbb{E}[x]\\mathbb{E}[e]^T = 0$. Thus, $\\Sigma_{xz} = s^2 I$.\n- The covariance of $z$ is $\\Sigma_{zz} = \\mathbb{E}[zz^T] = \\mathbb{E}[(x+e)(x+e)^T] = \\mathbb{E}[xx^T] + \\mathbb{E}[xe^T] + \\mathbb{E}[ex^T] + \\mathbb{E}[ee^T] = \\Sigma_{xx} + \\Sigma_{ee} = s^2 I + \\sigma^2 I = (s^2 + \\sigma^2)I$.\n\nFor jointly Gaussian vectors $(X_1, X_2)$ with means $(\\mu_1, \\mu_2)$ and block covariance matrix $\\begin{pmatrix} \\Sigma_{11}  \\Sigma_{12} \\\\ \\Sigma_{21}  \\Sigma_{22} \\end{pmatrix}$, the conditional expectation is given by the formula $\\mathbb{E}[X_1 \\mid X_2=x_2] = \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1} (x_2 - \\mu_2)$.\n\nApplying this formula with $X_1 = x$ and $X_2 = z$, we have:\n$$D_{\\sigma}(z) = \\mathbb{E}[x \\mid z] = 0 + (s^2 I) ((s^2 + \\sigma^2)I)^{-1} (z - 0)$$\n$$D_{\\sigma}(z) = s^2 (s^2 + \\sigma^2)^{-1} I z = \\frac{s^2}{s^2 + \\sigma^2} z$$\nThis is the explicit linear form of the MMSE Gaussian denoiser. It is a simple shrinkage operator, also known as a Wiener filter.\n\nThe PnP reconstruction $\\hat{x}_{\\sigma}$ is obtained by applying this denoiser to the measurement $y$:\n$$\\hat{x}_{\\sigma} = D_{\\sigma}(y) = \\frac{s^2}{s^2 + \\sigma^2} y$$\n\n**Part 2: Bias, Variance, and Expected Mean-Square Error**\n\nWe analyze the statistical properties of the estimator $\\hat{x}_{\\sigma}$ with respect to the measurement noise $w$, conditioned on a fixed realization of the true signal $x_0$. The measurement is $y = x_0 + w$, where $w \\sim \\mathcal{N}(0, \\tau^2 I)$.\n\nThe estimator is $\\hat{x}_{\\sigma} = \\frac{s^2}{s^2 + \\sigma^2} (x_0 + w)$. Let us define the shrinkage coefficient $c_{\\sigma} = \\frac{s^2}{s^2 + \\sigma^2}$. Then $\\hat{x}_{\\sigma} = c_{\\sigma}(x_0 + w)$.\n\nThe expected value of the estimator, conditioned on $x_0$, is:\n$$\\mathbb{E}_w[\\hat{x}_{\\sigma} \\mid x_0] = \\mathbb{E}_w[c_{\\sigma}(x_0 + w) \\mid x_0] = c_{\\sigma}(x_0 + \\mathbb{E}_w[w]) = c_{\\sigma}x_0$$\n\nThe bias of the estimator is defined as $\\text{Bias}(\\hat{x}_{\\sigma} \\mid x_0) = \\mathbb{E}_w[\\hat{x}_{\\sigma} \\mid x_0] - x_0$.\n$$\\text{Bias}(\\hat{x}_{\\sigma} \\mid x_0) = c_{\\sigma}x_0 - x_0 = (c_{\\sigma} - 1)x_0 = \\left(\\frac{s^2}{s^2 + \\sigma^2} - 1\\right)x_0 = -\\frac{\\sigma^2}{s^2 + \\sigma^2}x_0$$\nThe per-coordinate bias is $-\\frac{\\sigma^2}{s^2 + \\sigma^2}x_{0,i}$ for the $i$-th coordinate.\n\nThe covariance matrix of the estimator, conditioned on $x_0$, is:\n$$\\text{Cov}_w(\\hat{x}_{\\sigma} \\mid x_0) = \\mathbb{E}_w\\left[ (\\hat{x}_{\\sigma} - \\mathbb{E}_w[\\hat{x}_{\\sigma} \\mid x_0]) (\\hat{x}_{\\sigma} - \\mathbb{E}_w[\\hat{x}_{\\sigma} \\mid x_0])^T \\right]$$\nThe term inside the expectation is $(c_{\\sigma}(x_0+w) - c_{\\sigma}x_0) = c_{\\sigma}w$.\n$$\\text{Cov}_w(\\hatx_{\\sigma} \\mid x_0) = \\mathbb{E}_w[(c_{\\sigma}w)(c_{\\sigma}w)^T] = c_{\\sigma}^2 \\mathbb{E}_w[ww^T] = c_{\\sigma}^2 (\\tau^2 I) = \\left(\\frac{s^2}{s^2 + \\sigma^2}\\right)^2 \\tau^2 I$$\nThe per-coordinate variance is the diagonal entry of this matrix, which is $\\text{Var}_i = \\left(\\frac{s^2}{s^2 + \\sigma^2}\\right)^2 \\tau^2$.\n\nThe per-coordinate Mean-Square Error (MSE), conditioned on $x_{0,i}$, is given by the sum of the squared bias and the variance:\n$$\\text{MSE}_i(\\sigma \\mid x_{0,i}) = \\left(-\\frac{\\sigma^2}{s^2 + \\sigma^2}x_{0,i}\\right)^2 + \\left(\\frac{s^2}{s^2 + \\sigma^2}\\right)^2 \\tau^2 = \\frac{\\sigma^4}{(s^2 + \\sigma^2)^2}x_{0,i}^2 + \\frac{s^4 \\tau^2}{(s^2 + \\sigma^2)^2}$$\nThe problem asks for the per-coordinate *expected* MSE, which requires taking the expectation over the prior distribution of $x_0 \\sim \\mathcal{N}(0, s^2 I)$. For each coordinate $x_{0,i}$, we have $\\mathbb{E}[x_{0,i}^2] = s^2$.\n\nLet $E(\\sigma)$ be the per-coordinate expected MSE:\n$$E(\\sigma) = \\mathbb{E}_{x_{0,i}}[\\text{MSE}_i(\\sigma \\mid x_{0,i})] = \\frac{\\sigma^4}{(s^2 + \\sigma^2)^2}\\mathbb{E}[x_{0,i}^2] + \\frac{s^4 \\tau^2}{(s^2 + \\sigma^2)^2}$$\n$$E(\\sigma) = \\frac{\\sigma^4 s^2}{(s^2 + \\sigma^2)^2} + \\frac{s^4 \\tau^2}{(s^2 + \\sigma^2)^2} = \\frac{s^2(\\sigma^4 + s^2 \\tau^2)}{(s^2 + \\sigma^2)^2}$$\n\n**Part 3: Bias-Variance Trade-off Analysis**\n\nWe analyze the limits of the squared bias and variance as a function of the regularization parameter $\\sigma$.\nPer-coordinate squared bias: $\\text{Bias}_i^2 = \\frac{\\sigma^4}{(s^2 + \\sigma^2)^2}x_{0,i}^2$.\nPer-coordinate variance: $\\text{Var}_i = \\frac{s^4 \\tau^2}{(s^2 + \\sigma^2)^2}$.\n\nCase 1: Under-regularization ($\\sigma \\to 0$)\n- $\\lim_{\\sigma \\to 0} \\text{Bias}_i^2 = \\lim_{\\sigma \\to 0} \\frac{\\sigma^4}{(s^2 + \\sigma^2)^2}x_{0,i}^2 = \\frac{0}{(s^2)^2}x_{0,i}^2 = 0$. The bias vanishes.\n- $\\lim_{\\sigma \\to 0} \\text{Var}_i = \\lim_{\\sigma \\to 0} \\frac{s^4 \\tau^2}{(s^2 + \\sigma^2)^2} = \\frac{s^4 \\tau^2}{(s^2)^2} = \\tau^2$. The variance equals the measurement noise variance.\nWhen $\\sigma$ is small, the denoiser trusts its input too much ($D_0(y)=y$). The estimate $\\hat{x}_{\\sigma} \\approx y$ is unbiased but noisy.\n\nCase 2: Over-regularization ($\\sigma \\to \\infty$)\n- $\\lim_{\\sigma \\to \\infty} \\text{Bias}_i^2 = \\lim_{\\sigma \\to \\infty} \\frac{\\sigma^4}{(s^2 + \\sigma^2)^2}x_{0,i}^2 = \\lim_{\\sigma \\to \\infty} \\frac{1}{(s^2/\\sigma^2 + 1)^2}x_{0,i}^2 = x_{0,i}^2$. The bias is maximal, equal to $-x_{0,i}$.\n- $\\lim_{\\sigma \\to \\infty} \\text{Var}_i = \\lim_{\\sigma \\to \\infty} \\frac{s^4 \\tau^2}{(s^2 + \\sigma^2)^2} = 0$. The variance vanishes.\nWhen $\\sigma$ is large, the denoiser assumes the input is pure noise and outputs an estimate close to the prior mean, which is zero ($D_{\\infty}(y)=0$). The estimate $\\hat{x}_{\\sigma} \\approx 0$ has zero variance but is heavily biased towards zero, effectively erasing the signal.\n\nThis demonstrates the classic bias-variance trade-off controlled by the regularization parameter $\\sigma$.\n\n**Part 4: Optimal Regularization Parameter**\n\nTo find the value of $\\sigma$ that minimizes the per-coordinate expected MSE, we must minimize $E(\\sigma)$. It is equivalent and simpler to minimize with respect to $u = \\sigma^2$, assuming $\\sigma \\ge 0$.\n$$E(u) = \\frac{s^2(u^2 + s^2 \\tau^2)}{(s^2 + u)^2}$$\nWe compute the derivative with respect to $u$ and set it to $0$. Using the quotient rule:\n$$\\frac{dE}{du} = \\frac{ \\left[ \\frac{d}{du} (s^2 u^2 + s^4 \\tau^2) \\right] (s^2+u)^2 - (s^2 u^2 + s^4 \\tau^2) \\left[ \\frac{d}{du} (s^2+u)^2 \\right] }{ (s^2+u)^4 }$$\n$$\\frac{dE}{du} = \\frac{ (2s^2 u) (s^2+u)^2 - (s^2 u^2 + s^4 \\tau^2) (2(s^2+u)) }{ (s^2+u)^4 }$$\nSetting the numerator to zero (assuming $s^2+u \\neq 0$ and $s^2  0$):\n$$(2s^2 u) (s^2+u) - 2(s^2 u^2 + s^4 \\tau^2) = 0$$\n$$s^2 u (s^2+u) - (s^2 u^2 + s^4 \\tau^2) = 0$$\n$$s^4 u + s^2 u^2 - s^2 u^2 - s^4 \\tau^2 = 0$$\n$$s^4 u - s^4 \\tau^2 = 0$$\nSince the problem assumes a non-trivial prior ($s^2  0$), we can divide by $s^4$:\n$$u - \\tau^2 = 0 \\implies u = \\tau^2$$\nSo, the optimal value is $\\sigma^2 = \\tau^2$. Since $\\sigma$ represents a noise standard deviation and must be non-negative, the unique minimizer is $\\sigma = \\tau$. This result signifies that the optimal PnP regularization, in this simplified linear Gaussian case, is achieved when the denoiser is calibrated to the true noise level of the measurements. This choice makes the PnP estimator $\\hat{x}_{\\sigma=\\tau}$ equal to the true MMSE estimator $\\mathbb{E}[x_0 \\mid y]$, which by definition minimizes the mean-squared error.\nThe optimal value of $\\sigma$ is therefore $\\tau$.",
            "answer": "$$\\boxed{\\tau}$$"
        },
        {
            "introduction": "The power of the \"plug-and-play\" approach comes with a critical caveat: not every denoiser will lead to a convergent algorithm. This exercise provides a concrete counterexample to illustrate one of the most important conditions for the stability of PnP schemes . By analyzing and simulating a one-dimensional case with an \"expansive\" linear denoiser, you will observe firsthand how and why the iterative process can diverge, reinforcing the necessity of using non-expansive operators to ensure reliable reconstructions.",
            "id": "3466533",
            "problem": "Consider the Alternating Direction Method of Multipliers (ADMM) for solving problems of the form $\\min_{\\mathbf{x}} \\, g(\\mathbf{x}) + h(\\mathbf{x})$ by splitting variables and introducing the constraint $\\mathbf{x} = \\mathbf{v}$. The scaled ADMM iterates over three variables: the primal variable $\\mathbf{x}$, the auxiliary variable $\\mathbf{v}$, and the scaled dual variable $\\mathbf{u}$. Plug-and-Play (PnP) methods replace the proximal operator of $h$ with a denoiser $D$, producing PnP-ADMM. In one dimension, assume the data-fidelity term is quadratic, the forward operator is identity, and the denoiser is linear.\n\nStarting from the following fundamental base:\n\n- The variable-splitting form of ADMM for $\\min_{\\mathbf{x}} \\, g(\\mathbf{x}) + h(\\mathbf{v})$ subject to $\\mathbf{x} = \\mathbf{v}$ uses the augmented Lagrangian with penalty parameter $\\rho  0$ and the scaled dual variable $\\mathbf{u}$.\n- In PnP-ADMM, the update corresponding to the proximal operator of $h$ is replaced by a denoising operation $D$ evaluated at the appropriate argument.\n- For a one-dimensional quadratic data fidelity $g(x) = \\frac{\\gamma}{2} (x - y)^2$ with $\\gamma  0$, identity forward operator, and linear denoiser $D(z) = \\alpha z$ with gain $\\alpha  0$, each iteration is well-defined and yields a deterministic linear-affine recurrence.\n\nYour task is to construct a counterexample showing that an expansive denoiser (i.e., a denoiser with gain $\\alpha  1$) can cause divergence in PnP-ADMM and, moreover, may produce oscillatory behavior. Proceed as follows, strictly deriving from the base above:\n\n1. Derive, from first principles, the one-dimensional PnP-ADMM iteration for the triple $(x^{k+1}, v^{k+1}, u^{k+1})$ using the quadratic $g(x)$, linear denoiser $D(z)$, and penalty parameter $\\rho$. Express the iteration in closed form in terms of $x^k$, $v^k$, $u^k$, $\\gamma$, $\\rho$, $y$, and $\\alpha$.\n2. Show that the iteration can be written as a linear-affine dynamical system on an appropriate state, and derive the matrix that governs the linear part. Compute the spectral radius of this matrix and provide the precise divergence condition in terms of $\\alpha$, $\\gamma$, and $\\rho$ using only this derivation.\n3. Explain, in terms of the sign of the dominant eigenvalue, when oscillatory behavior (alternating signs across iterations) occurs.\n4. Implement a complete, runnable program that:\n   - Simulates the PnP-ADMM iteration in one dimension using the derived closed-form updates.\n   - For each test case, computes the spectral radius of the linear part, determines if the sequence diverges (based on the derived necessary and sufficient condition), and detects oscillatory behavior by counting sign alternations in the scaled dual variable over the simulated iterations.\n   - Returns, for each test case, a list of three items: the spectral radius as a float rounded to six decimal places, a boolean indicating divergence, and a boolean indicating oscillatory behavior.\n\nUse the following test suite with one-dimensional parameters $(\\alpha, \\gamma, \\rho, y, x^0, v^0, u^0, K)$, where $K$ is the number of iterations:\n\n- Test case $1$ (expansive, monotone divergence): $(\\alpha, \\gamma, \\rho, y, x^0, v^0, u^0, K) = (1.2, 0.1, 1.0, 1.0, 0.0, 0.0, 0.0, 50)$.\n- Test case $2$ (expansive, oscillatory divergence): $(\\alpha, \\gamma, \\rho, y, x^0, v^0, u^0, K) = (5.0, 1.0, 0.1, 1.0, 0.0, 0.0, 0.0, 50)$.\n- Test case $3$ (borderline expansive, linear-growth divergence): $(\\alpha, \\gamma, \\rho, y, x^0, v^0, u^0, K) = \\left(\\frac{1}{1 - 0.1}, 0.1, 1.0, 1.0, 0.0, 0.0, 0.0, 50\\right)$.\n- Test case $4$ (contractive, convergence): $(\\alpha, \\gamma, \\rho, y, x^0, v^0, u^0, K) = (0.8, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 50)$.\n- Test case $5$ (nonexpansive, convergence): $(\\alpha, \\gamma, \\rho, y, x^0, v^0, u^0, K) = (1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 50)$.",
            "solution": "**1. Derivation of the PnP-ADMM Iteration**\n\nThe scaled ADMM algorithm addresses the problem $\\min_{x,v} g(x) + h(v)$ subject to $x = v$ by iterating on the augmented Lagrangian:\n$$ \\mathcal{L}_{\\rho}(x, v, u) = g(x) + h(v) + \\frac{\\rho}{2} \\|x - v + u\\|_2^2 - \\frac{\\rho}{2} \\|u\\|_2^2 $$\nThe one-dimensional ADMM iteration consists of three steps:\n\n**x-update:** The variable $x$ is updated by minimizing $\\mathcal{L}_{\\rho}$ with respect to $x$:\n$$ x^{k+1} = \\arg\\min_x \\left( g(x) + \\frac{\\rho}{2} (x - v^k + u^k)^2 \\right) $$\nSubstituting $g(x) = \\frac{\\gamma}{2} (x - y)^2$:\n$$ x^{k+1} = \\arg\\min_x \\left( \\frac{\\gamma}{2} (x - y)^2 + \\frac{\\rho}{2} (x - v^k + u^k)^2 \\right) $$\nTo find the minimum, we set the derivative with respect to $x$ to zero:\n$$ \\frac{\\partial}{\\partial x} \\left[ \\dots \\right] = \\gamma(x-y) + \\rho(x - v^k + u^k) = 0 $$\n$$ (\\gamma + \\rho)x = \\gamma y + \\rho(v^k - u^k) $$\nSolving for $x$ gives the closed-form update:\n$$ x^{k+1} = \\frac{\\gamma y + \\rho(v^k - u^k)}{\\gamma + \\rho} $$\n\n**v-update:** The standard $v$-update is $v^{k+1} = \\text{prox}_{h/\\rho}(x^{k+1} + u^k)$. In PnP-ADMM, this step is replaced by applying a denoiser $D$ to the argument of the proximal operator:\n$$ v^{k+1} = D(x^{k+1} + u^k) $$\nUsing the specified linear denoiser $D(z) = \\alpha z$, the update becomes:\n$$ v^{k+1} = \\alpha (x^{k+1} + u^k) $$\n\n**u-update:** The scaled dual variable is updated as:\n$$ u^{k+1} = u^k + x^{k+1} - v^{k+1} $$\n\nThese three equations define the PnP-ADMM iteration in closed form.\n\n**2. Linear-Affine System and Stability Analysis**\n\nTo analyze the convergence, we formulate the iteration as a linear-affine dynamical system. The state of the system can be defined by the variables that are carried over between iterations, which are $v^k$ and $u^k$. We express $(v^{k+1}, u^{k+1})$ in terms of $(v^k, u^k)$.\n\nFirst, substitute the expression for $v^{k+1}$ into the $u$-update:\n$$ u^{k+1} = u^k + x^{k+1} - \\alpha (x^{k+1} + u^k) = (1 - \\alpha)u^k + (1 - \\alpha)x^{k+1} $$\nNow, substitute the expression for $x^{k+1}$ into the updates for $v^{k+1}$ and $u^{k+1}$. Let $c_x = \\frac{\\gamma y}{\\gamma + \\rho}$.\n$$ x^{k+1} = \\frac{\\rho}{\\gamma + \\rho}v^k - \\frac{\\rho}{\\gamma + \\rho}u^k + c_x $$\nThe update for $v^{k+1}$ becomes:\n$$ v^{k+1} = \\alpha \\left( \\left[ \\frac{\\rho}{\\gamma + \\rho}v^k - \\frac{\\rho}{\\gamma + \\rho}u^k + c_x \\right] + u^k \\right) $$\n$$ v^{k+1} = \\alpha \\left( \\frac{\\rho}{\\gamma + \\rho}v^k + \\left(1 - \\frac{\\rho}{\\gamma + \\rho}\\right)u^k + c_x \\right) $$\n$$ v^{k+1} = \\frac{\\alpha\\rho}{\\gamma + \\rho}v^k + \\frac{\\alpha\\gamma}{\\gamma + \\rho}u^k + \\alpha c_x $$\nThe update for $u^{k+1}$ becomes:\n$$ u^{k+1} = (1 - \\alpha)u^k + (1 - \\alpha)\\left( \\frac{\\rho}{\\gamma + \\rho}v^k - \\frac{\\rho}{\\gamma + \\rho}u^k + c_x \\right) $$\n$$ u^{k+1} = \\frac{\\rho(1 - \\alpha)}{\\gamma + \\rho}v^k + \\left( (1 - \\alpha) - \\frac{\\rho(1 - \\alpha)}{\\gamma + \\rho} \\right)u^k + (1 - \\alpha)c_x $$\n$$ u^{k+1} = \\frac{\\rho(1 - \\alpha)}{\\gamma + \\rho}v^k + (1 - \\alpha)\\left( \\frac{\\gamma + \\rho - \\rho}{\\gamma + \\rho} \\right)u^k + (1 - \\alpha)c_x $$\n$$ u^{k+1} = \\frac{\\rho(1 - \\alpha)}{\\gamma + \\rho}v^k + \\frac{\\gamma(1 - \\alpha)}{\\gamma + \\rho}u^k + (1 - \\alpha)c_x $$\nLet the state vector be $z^k = [v^k, u^k]^T$. The iteration is a linear-affine system $z^{k+1} = M z^k + b$, where the matrix $M$ and vector $b$ are:\n$$ M = \\frac{1}{\\gamma + \\rho} \\begin{pmatrix} \\alpha\\rho  \\alpha\\gamma \\\\ \\rho(1-\\alpha)  \\gamma(1-\\alpha) \\end{pmatrix}, \\quad b = c_x \\begin{pmatrix} \\alpha \\\\ 1-\\alpha \\end{pmatrix} = \\frac{\\gamma y}{\\gamma + \\rho} \\begin{pmatrix} \\alpha \\\\ 1-\\alpha \\end{pmatrix} $$\nThe stability of this system is determined by the spectral radius of $M$, $\\rho(M)$, which is the maximum absolute value of its eigenvalues. We find the eigenvalues $\\lambda$ by solving $\\det(M - \\lambda I) = 0$. Let $\\lambda' = (\\gamma+\\rho)\\lambda$ be the eigenvalues of $(\\gamma+\\rho)M$.\n$$ \\det \\begin{pmatrix} \\alpha\\rho - \\lambda'  \\alpha\\gamma \\\\ \\rho(1-\\alpha)  \\gamma(1-\\alpha) - \\lambda' \\end{pmatrix} = 0 $$\n$$ (\\alpha\\rho - \\lambda')(\\gamma(1-\\alpha) - \\lambda') - \\alpha\\gamma\\rho(1-\\alpha) = 0 $$\n$$ \\lambda'^2 - (\\alpha\\rho + \\gamma - \\alpha\\gamma)\\lambda' = 0 $$\nThe eigenvalues of $(\\gamma+\\rho)M$ are $\\lambda'_1 = 0$ and $\\lambda'_2 = \\alpha(\\rho - \\gamma) + \\gamma$. The eigenvalues of $M$ are therefore:\n$$ \\lambda_1 = 0, \\quad \\lambda_2 = \\frac{\\alpha(\\rho - \\gamma) + \\gamma}{\\gamma + \\rho} $$\nThe spectral radius is $\\rho(M) = \\max(|\\lambda_1|, |\\lambda_2|) = |\\lambda_2|$.\n$$ \\rho(M) = \\left| \\frac{\\alpha(\\rho - \\gamma) + \\gamma}{\\gamma + \\rho} \\right| $$\nThe PnP-ADMM iteration converges if $\\rho(M)  1$ and diverges if $\\rho(M) \\ge 1$.\n\n**3. Condition for Oscillatory Behavior**\n\nOscillatory behavior, characterized by the alternating signs of an error term across iterations, occurs when the dominant eigenvalue is real and negative. Since $\\lambda_1 = 0$, the dynamics are governed by $\\lambda_2$. Oscillation occurs when $\\lambda_2  0$.\n$$ \\frac{\\alpha(\\rho - \\gamma) + \\gamma}{\\gamma + \\rho}  0 $$\nAs $\\gamma  0$ and $\\rho  0$, the denominator $\\gamma + \\rho$ is positive. The condition simplifies to:\n$$ \\alpha(\\rho - \\gamma) + \\gamma  0 \\implies \\alpha(\\rho - \\gamma)  -\\gamma $$\nThis inequality can only be satisfied if $\\rho - \\gamma  0$ (i.e., $\\gamma  \\rho$), because if $\\rho - \\gamma \\ge 0$, the left side is non-negative for $\\alpha  0$ and cannot be less than the negative value $-\\gamma$. If $\\gamma  \\rho$, the condition for oscillation becomes:\n$$ \\alpha  \\frac{-\\gamma}{\\rho - \\gamma} \\implies \\alpha  \\frac{\\gamma}{\\gamma - \\rho} $$\nThus, oscillatory behavior is expected when $\\gamma  \\rho$ and the denoiser gain $\\alpha$ is sufficiently large, specifically $\\alpha  \\gamma / (\\gamma - \\rho)$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and simulates PnP-ADMM for several test cases.\n\n    For each case, it computes the spectral radius of the iteration matrix,\n    determines divergence based on the spectral radius, and detects oscillatory\n    behavior by simulating the sequence and counting sign changes.\n    \"\"\"\n    # Test cases: (alpha, gamma, rho, y, x0, v0, u0, K)\n    test_cases = [\n        (1.2, 0.1, 1.0, 1.0, 0.0, 0.0, 0.0, 50),\n        (5.0, 1.0, 0.1, 1.0, 0.0, 0.0, 0.0, 50),\n        (1.0 / (1.0 - 0.1), 0.1, 1.0, 1.0, 0.0, 0.0, 0.0, 50),\n        (0.8, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 50),\n        (1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 50),\n    ]\n\n    results = []\n    for alpha, gamma, rho, y, x0, v0, u0, K in test_cases:\n        # 1. Theoretical Analysis\n        # Calculate the non-zero eigenvalue of the iteration matrix.\n        lambda2 = (alpha * (rho - gamma) + gamma) / (gamma + rho)\n        \n        # The spectral radius is the absolute value of the dominant eigenvalue.\n        spectral_radius = abs(lambda2)\n        \n        # Divergence occurs if the spectral radius is greater than or equal to 1.\n        diverges = spectral_radius = 1.0\n\n        # 2. Simulation\n        x, v, u = float(x0), float(v0), float(u0)\n        u_history = [u]\n        \n        for _ in range(K):\n            # One-dimensional PnP-ADMM updates\n            x_next = (gamma * y + rho * (v - u)) / (gamma + rho)\n            v_next = alpha * (x_next + u)\n            u_next = u + x_next - v_next\n            \n            x, v, u = x_next, v_next, u_next\n            u_history.append(u)\n        \n        # 3. Oscillation Check\n        # Oscillation is detected by counting sign alternations in the dual variable u.\n        sign_changes = 0\n        for i in range(1, len(u_history)):\n            # A sign change occurs if u_k and u_{k-1} have opposite signs.\n            # np.sign handles the zero case correctly.\n            if np.sign(u_history[i]) * np.sign(u_history[i-1]) == -1:\n                sign_changes += 1\n        \n        # A reasonable threshold for oscillation: sign changes in more than a quarter of steps.\n        oscillatory = sign_changes  K / 4\n\n        results.append([spectral_radius, diverges, oscillatory])\n\n    # 4. Format Output\n    # The output must be a single line, formatted as a list of lists with no spaces.\n    formatted_results = []\n    for r, b, o in results:\n        r_str = f\"{r:.6f}\"\n        b_str = str(b).lower()\n        o_str = str(o).lower()\n        formatted_results.append(f\"[{r_str},{b_str},{o_str}]\")\n    \n    # Final print statement must be in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}