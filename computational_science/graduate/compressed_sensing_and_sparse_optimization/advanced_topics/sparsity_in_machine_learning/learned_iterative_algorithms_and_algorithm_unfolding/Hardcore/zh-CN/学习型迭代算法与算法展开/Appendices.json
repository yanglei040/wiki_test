{
    "hands_on_practices": [
        {
            "introduction": "算法展开的核心思想是将经典的迭代优化算法的每一步映射为深度网络的一层。本练习将通过一个具体计算，帮助你掌握这一过程的机制。我们将从用于稀疏回归的交替方向乘子法（ADMM）出发，将其中的软阈值收缩步骤替换为一个具有可学习参数的函数，从而构建一个学习迭代算法层。通过手动计算两层更新后的状态，你将深入理解参数化、前向传播以及变量在学习算法中的演化过程。",
            "id": "3456561",
            "problem": "考虑压缩感知中的标准稀疏回归模型，其中潜在信号 $x \\in \\mathbb{R}^{n}$ 是通过矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 从线性测量 $y \\in \\mathbb{R}^{m}$ 在 $\\ell_{1}$ 稀疏惩罚下估计得到的。其经典形式引入了一个分裂变量 $z \\in \\mathbb{R}^{n}$ 并施加约束 $z = x$，从而得到如下约束优化问题\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|z\\|_{1}\n\\quad \\text{subject to} \\quad z = x,\n$$\n其中惩罚权重 $\\lambda > 0$。交替方向乘子法 (ADMM) 应用于与上述约束相关的增广拉格朗日量，使用缩放对偶变量 $u \\in \\mathbb{R}^{n}$ 和惩罚参数 $\\rho > 0$。在一个学习型迭代算法（算法展开）中，$z$-子问题被一个学习到的收缩函数逐元素地替代，该函数对标量输入 $v \\in \\mathbb{R}$ 定义为\n$$\nS_{\\theta}(v) = \\alpha \\cdot \\mathrm{sign}(v) \\cdot \\max\\!\\big(|v| - \\tau,\\, 0\\big),\n$$\n其中 $\\theta = (\\alpha, \\tau)$ 是学习到的参数，且 $\\alpha > 0$ 和 $\\tau \\geq 0$。\n\n设 $n = m = 2$，并考虑具有以下参数的具体实例\n$$\nA = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}, \\qquad y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\qquad \\lambda = 1, \\qquad \\rho = 1.\n$$\n在以下初始值处初始化展开的 ADMM\n$$\nx^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\qquad z^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\qquad u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\n执行两个展开的 ADMM 层（迭代）。在第 $k$ 层（$k \\in \\{1,2\\}$）中，使用学习到的收缩参数\n$$\n\\theta_{1} = \\big(\\alpha_{1}, \\tau_{1}\\big) = \\left(\\frac{9}{10}, \\frac{1}{4}\\right), \\qquad\n\\theta_{2} = \\big(\\alpha_{2}, \\tau_{2}\\big) = \\left(\\frac{11}{10}, \\frac{2}{5}\\right),\n$$\n并按元素应用学习到的收缩更新 $z^{(k)} = S_{\\theta_{k}}\\!\\big(x^{(k)} + u^{(k-1)}\\big)$。$x$-更新和 $u$-更新是由 ADMM 方法针对上述约束形式和惩罚参数 $\\rho$ 使用缩放对偶变量所隐含的更新。\n\n从指定的初始值开始，完成这两层计算后，计算欧几里得范数平方 $\\|x^{(2)}\\|_{2}^{2}$ 的值。请以精确形式的单个实数表示最终答案。无需四舍五入。",
            "solution": "用户希望在一个学习型交替方向乘子法 (ADMM) 算法迭代两次后，计算状态变量 $x^{(2)}$ 的欧几里得范数平方。\n\n首先，我将验证问题陈述。\n\n### 第 1 步：提取已知条件\n- **优化问题**：$\\min_{x, z} \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|z\\|_{1}$，约束条件为 $z = x$。\n- **维度**：$n=2$, $m=2$。\n- **数据**：\n  - $A = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$\n  - $y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- **参数**：\n  - $\\lambda = 1$\n  - $\\rho = 1$\n- **初始条件**：\n  - $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n  - $z^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n  - $u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n- **算法**：使用缩放对偶变量的展开 ADMM，其中 $k \\in \\{1, 2\\}$。\n- **更新规则**：\n  - $x$-更新和 $u$-更新是标准的 ADMM 更新。\n  - $z$-更新是一个学习到的收缩：$z^{(k)} = S_{\\theta_{k}}(x^{(k)} + u^{(k-1)})$，按元素应用。\n  - 学习到的收缩函数是 $S_{\\theta}(v) = \\alpha \\cdot \\mathrm{sign}(v) \\cdot \\max(|v| - \\tau, 0)$。\n- **学习到的参数**：\n  - 第 1 层 ($k=1$): $\\theta_{1} = (\\alpha_{1}, \\tau_{1}) = \\left(\\frac{9}{10}, \\frac{1}{4}\\right)$。\n  - 第 2 层 ($k=2$): $\\theta_{2} = (\\alpha_{2}, \\tau_{2}) = \\left(\\frac{11}{10}, \\frac{2}{5}\\right)$。\n- **目标**：计算 $\\|x^{(2)}\\|_{2}^{2}$。\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题在稀疏优化、压缩感知和机器学习（特别是算法展开）领域有科学依据。ADMM 的形式和学习到的收缩函数在此背景下是标准的。问题是适定的，提供了所有必要的数据、初始条件和更新规则以唯一确定结果。语言精确客观。没有矛盾或含糊不清之处。问题有效。\n\n### 第 3 步：开始求解\n\n使用缩放对偶变量 $u$ 的 ADMM 更新步骤如下，其中 $k$ 是迭代索引：\n1.  **$x$-更新**：$x^{(k)} = \\arg\\min_{x} \\left( \\frac{1}{2}\\|Ax - y\\|_{2}^{2} + \\frac{\\rho}{2}\\|x - z^{(k-1)} + u^{(k-1)}\\|_{2}^{2} \\right)$。\n    一阶最优性条件产生一个线性系统：\n    $A^T(Ax - y) + \\rho(x - z^{(k-1)} + u^{(k-1)}) = 0$\n    $(A^T A + \\rho I)x = A^T y + \\rho(z^{(k-1)} - u^{(k-1)})$\n    $x^{(k)} = (A^T A + \\rho I)^{-1} (A^T y + \\rho(z^{(k-1)} - u^{(k-1)}))$\n\n2.  **$z$-更新**：问题指定了一个学习到的更新规则：\n    $z^{(k)} = S_{\\theta_k}(x^{(k)} + u^{(k-1)})$\n\n3.  **$u$-更新**：$u^{(k)} = u^{(k-1)} + x^{(k)} - z^{(k)}$。\n\n首先，我预先计算 $x$-更新中的常数部分。\n给定 $A = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$ 和 $\\rho = 1$：\n$A^T A = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} = \\begin{pmatrix} 5  4 \\\\ 4  5 \\end{pmatrix}$。\n$A^T A + \\rho I = \\begin{pmatrix} 5  4 \\\\ 4  5 \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 6  4 \\\\ 4  6 \\end{pmatrix}$。\n其逆矩阵是：\n$(A^T A + \\rho I)^{-1} = \\frac{1}{6 \\cdot 6 - 4 \\cdot 4} \\begin{pmatrix} 6  -4 \\\\ -4  6 \\end{pmatrix} = \\frac{1}{20} \\begin{pmatrix} 6  -4 \\\\ -4  6 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 3  -2 \\\\ -2  3 \\end{pmatrix}$。\n\n项 $A^T y$ 是：\n$A^T y = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$。\n\n初始条件为 $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，$z^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 和 $u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n\n**第 1 层 ($k=1$)：**\n\n1.  **计算 $x^{(1)}$**：\n    $x^{(1)} = (A^T A + \\rho I)^{-1} (A^T y + \\rho(z^{(0)} - u^{(0)}))$\n    因为 $z^{(0)} = u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，表达式简化为：\n    $x^{(1)} = (A^T A + \\rho I)^{-1} A^T y = \\frac{1}{10} \\begin{pmatrix} 3  -2 \\\\ -2  3 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 3(2) - 2(1) \\\\ -2(2) + 3(1) \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} \\\\ -\\frac{1}{10} \\end{pmatrix}$。\n\n2.  **计算 $z^{(1)}$**：\n    学习到的更新使用 $\\theta_1 = (\\alpha_1, \\tau_1) = (\\frac{9}{10}, \\frac{1}{4})$。\n    收缩函数的输入是 $v^{(1)} = x^{(1)} + u^{(0)} = \\begin{pmatrix} \\frac{2}{5} \\\\ -\\frac{1}{10} \\end{pmatrix}$。\n    我们按元素应用 $S_{\\theta_1}$：\n    对于第一个分量，$v_1 = \\frac{2}{5} = 0.4$。阈值是 $\\tau_1 = \\frac{1}{4} = 0.25$。\n    因为 $|v_1| > \\tau_1$，$z^{(1)}$ 的第一个分量是：\n    $z_1^{(1)} = \\alpha_1 \\cdot \\mathrm{sign}(v_1) \\cdot (|v_1| - \\tau_1) = \\frac{9}{10} \\cdot 1 \\cdot \\left(\\frac{2}{5} - \\frac{1}{4}\\right) = \\frac{9}{10} \\cdot \\left(\\frac{8-5}{20}\\right) = \\frac{9}{10} \\cdot \\frac{3}{20} = \\frac{27}{200}$。\n    对于第二个分量，$v_2 = -\\frac{1}{10} = -0.1$。\n    因为 $|v_2| = 0.1  \\tau_1 = 0.25$，$z^{(1)}$ 的第二个分量是：\n    $z_2^{(1)} = \\alpha_1 \\cdot \\mathrm{sign}(v_2) \\cdot \\max(|v_2| - \\tau_1, 0) = \\frac{9}{10} \\cdot (-1) \\cdot 0 = 0$。\n    所以，$z^{(1)} = \\begin{pmatrix} \\frac{27}{200} \\\\ 0 \\end{pmatrix}$。\n\n3.  **计算 $u^{(1)}$**：\n    $u^{(1)} = u^{(0)} + x^{(1)} - z^{(1)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} \\frac{2}{5} \\\\ -\\frac{1}{10} \\end{pmatrix} - \\begin{pmatrix} \\frac{27}{200} \\\\ 0 \\end{pmatrix}$。\n    为了进行减法，我们使用公分母 $200$：$\\frac{2}{5} = \\frac{80}{200}$ 且 $-\\frac{1}{10} = -\\frac{20}{200}$。\n    $u^{(1)} = \\begin{pmatrix} \\frac{80}{200} - \\frac{27}{200} \\\\ -\\frac{20}{200} - 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{53}{200} \\\\ -\\frac{20}{200} \\end{pmatrix} = \\begin{pmatrix} \\frac{53}{200} \\\\ -\\frac{1}{10} \\end{pmatrix}$。\n\n**第 2 层 ($k=2$)：**\n\n1.  **计算 $x^{(2)}$**：\n    $x^{(2)} = (A^T A + \\rho I)^{-1} (A^T y + \\rho(z^{(1)} - u^{(1)}))$。\n    首先，我们计算项 $z^{(1)} - u^{(1)}$：\n    $z^{(1)} - u^{(1)} = \\begin{pmatrix} \\frac{27}{200} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} \\frac{53}{200} \\\\ -\\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{27-53}{200} \\\\ 0 - (-\\frac{1}{10}) \\end{pmatrix} = \\begin{pmatrix} -\\frac{26}{200} \\\\ \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} -\\frac{13}{100} \\\\ \\frac{1}{10} \\end{pmatrix}$。\n    接下来，我们计算要与矩阵相乘的向量：\n    $A^T y + \\rho(z^{(1)} - u^{(1)}) = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} -\\frac{13}{100} \\\\ \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} 2 - \\frac{13}{100} \\\\ 1 + \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{200-13}{100} \\\\ \\frac{10+1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{187}{100} \\\\ \\frac{11}{10} \\end{pmatrix}$。\n    现在，我们计算 $x^{(2)}$：\n    $x^{(2)} = \\frac{1}{10} \\begin{pmatrix} 3  -2 \\\\ -2  3 \\end{pmatrix} \\begin{pmatrix} \\frac{187}{100} \\\\ \\frac{11}{10} \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 3(\\frac{187}{100}) - 2(\\frac{11}{10}) \\\\ -2(\\frac{187}{100}) + 3(\\frac{11}{10}) \\end{pmatrix}$。\n    我们在向量内部使用公分母 $100$：$\\frac{11}{10} = \\frac{110}{100}$。\n    $x^{(2)} = \\frac{1}{10} \\begin{pmatrix} \\frac{3(187) - 2(110)}{100} \\\\ \\frac{-2(187) + 3(110)}{100} \\end{pmatrix} = \\frac{1}{1000} \\begin{pmatrix} 561 - 220 \\\\ -374 + 330 \\end{pmatrix} = \\frac{1}{1000} \\begin{pmatrix} 341 \\\\ -44 \\end{pmatrix} = \\begin{pmatrix} \\frac{341}{1000} \\\\ -\\frac{44}{1000} \\end{pmatrix}$。\n\n**最终计算：**\n\n目标是计算 $\\|x^{(2)}\\|_{2}^{2}$。\n$\\|x^{(2)}\\|_{2}^{2} = \\left(\\frac{341}{1000}\\right)^2 + \\left(-\\frac{44}{1000}\\right)^2 = \\frac{341^2 + (-44)^2}{1000^2}$。\n$341^2 = 116281$。\n$(-44)^2 = 44^2 = 1936$。\n$341^2 + 44^2 = 116281 + 1936 = 118217$。\n$1000^2 = 1000000$。\n因此，$\\|x^{(2)}\\|_{2}^{2} = \\frac{118217}{1000000}$。\n这也可以写作 $0.118217$。",
            "answer": "$$ \\boxed{\\frac{118217}{1000000}} $$"
        },
        {
            "introduction": "在学习迭代算法的大家族中，近似消息传递（AMP）算法因其精确的理论特性而独树一帜。本练习将带你探索学习化的AMP（LAMP）算法，重点关注其一个关键组成部分——Onsager修正项。这个修正项是保证AMP算法性能和状态演化（state evolution）理论分析精度的核心。通过对一个具体实例执行单层LAMP更新，你将亲手计算该修正项，并体会它如何与其他步骤（如线性估计和去噪）协同工作，从而揭示AMP与其他近端梯度方法的结构差异。",
            "id": "3456609",
            "problem": "考虑压缩感知中的线性逆问题，其测量模型为 $y = A x_{\\star} + w$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 的元素是独立同分布的 $\\mathcal{N}(0,1/m)$，$x_{\\star} \\in \\mathbb{R}^{n}$ 是 $k$-稀疏的，而 $w \\in \\mathbb{R}^{m}$ 是加性噪声。您将执行一层学习型近似消息传递 (Learned Approximate Message Passing, LAMP) 算法（即带有学习参数的近似消息传递 (Approximate Message Passing, AMP)），并从零向量开始。请使用以下具体实例：\n- $m = 3$, $n = 4$。\n- \n$$\nA = \n\\begin{pmatrix}\n0.62  -0.17  0.35  0.08 \\\\\n-0.44  0.59  -0.28  0.49 \\\\\n0.22  0.31  0.07  -0.53\n\\end{pmatrix}.\n$$\n- 真实信号为 $x_{\\star} = (1.5,\\, 0,\\, 0,\\, -0.8)^{\\top}$，噪声为 $w = (0.05,\\,-0.02,\\,0.03)^{\\top}$，因此 $y = A x_{\\star} + w$。\n- 初始化 $x^{0} = 0$ 和 $r^{0} = y$。\n- 使用一个学习得到的标量步长矩阵 $B^{0} = \\alpha I$，其中 $\\alpha = 0.9$。\n- 使用一个学习得到的阈值缩放因子 $\\lambda = 1.0$ 和基于残差的噪声估计 $\\sigma_{0} = \\|r^{0}\\|_{2}/\\sqrt{m}$，因此阈值为 $\\theta_{0} = \\lambda \\sigma_{0}$。\n- 使用逐元素的软阈值函数 $\\eta(u;\\theta) = \\mathrm{sign}(u)\\,\\max(|u|-\\theta,\\,0)$，其弱导数为 $\\eta'(u;\\theta) = \\mathbf{1}\\{|u| > \\theta\\}$。\n\n单层 LAMP ($t=0 \\to 1$) 的定义如下：\n- 线性步骤：$u^{0} = x^{0} + B^{0} A^{\\top} r^{0}$。\n- 去噪步骤：$x^{1} = \\eta(u^{0}; \\theta_{0})$。\n- Onsager系数：$b_{0} = \\frac{1}{m} \\sum_{i=1}^{n} \\eta'(u^{0}_{i}; \\theta_{0})$。\n- 残差更新：$r^{1} = y - A x^{1} + b_{0} r^{0}$。\n\n请针对指定的实例，数值计算更新后的残差 $r^{1}$ 的第一个分量，包括 Onsager 项。将您的答案四舍五入至六位有效数字。无需单位。",
            "solution": "用户希望计算经过一轮学习型近似消息传递 (LAMP) 算法迭代后，更新后残差的第一个分量 $r_1^1$。状态变量从迭代 $t=0$ 到 $t=1$ 的更新方程已给出。我们将按照这些步骤，使用给定的数值进行顺序计算。\n\n问题指定了以下量：\n- 维度：$m = 3$, $n = 4$。\n- 传感矩阵：$A = \\begin{pmatrix} 0.62  -0.17  0.35  0.08 \\\\ -0.44  0.59  -0.28  0.49 \\\\ 0.22  0.31  0.07  -0.53 \\end{pmatrix}$。\n- 真实信号：$x_{\\star} = (1.5, 0, 0, -0.8)^{\\top}$。\n- 加性噪声：$w = (0.05, -0.02, 0.03)^{\\top}$。\n- 初始信号估计：$x^{0} = (0, 0, 0, 0)^{\\top}$。\n- 学习得到的参数：步长 $\\alpha = 0.9$（因此 $B^0 = 0.9I$）和阈值缩放因子 $\\lambda = 1.0$。\n\n目标是计算 $r^1$ 的第一个分量，更新后的残差向量 $r^1$ 由下式给出：\n$$r^{1} = y - A x^{1} + b_{0} r^{0}$$\n我们将计算此表达式中的每一项。\n\n**步骤1：计算测量向量 $y$ 和初始残差 $r^0$。**\n测量模型为 $y = A x_{\\star} + w$。\n首先，我们计算乘积 $A x_{\\star}$：\n$$\nA x_{\\star} = \\begin{pmatrix} 0.62  -0.17  0.35  0.08 \\\\ -0.44  0.59  -0.28  0.49 \\\\ 0.22  0.31  0.07  -0.53 \\end{pmatrix} \\begin{pmatrix} 1.5 \\\\ 0 \\\\ 0 \\\\ -0.8 \\end{pmatrix} = \\begin{pmatrix} (0.62)(1.5) + (-0.17)(0) + (0.35)(0) + (0.08)(-0.8) \\\\ (-0.44)(1.5) + (0.59)(0) + (-0.28)(0) + (0.49)(-0.8) \\\\ (0.22)(1.5) + (0.31)(0) + (0.07)(0) + (-0.53)(-0.8) \\end{pmatrix}\n$$\n$$\nA x_{\\star} = \\begin{pmatrix} 0.93 - 0.064 \\\\ -0.66 - 0.392 \\\\ 0.33 + 0.424 \\end{pmatrix} = \\begin{pmatrix} 0.866 \\\\ -1.052 \\\\ 0.754 \\end{pmatrix}\n$$\n现在，我们加上噪声向量 $w$：\n$$\ny = A x_{\\star} + w = \\begin{pmatrix} 0.866 \\\\ -1.052 \\\\ 0.754 \\end{pmatrix} + \\begin{pmatrix} 0.05 \\\\ -0.02 \\\\ 0.03 \\end{pmatrix} = \\begin{pmatrix} 0.916 \\\\ -1.072 \\\\ 0.784 \\end{pmatrix}\n$$\n初始残差为 $r^{0} = y$，因此 $r^0 = (0.916, -1.072, 0.784)^{\\top}$。\n\n**步骤2：计算阈值 $\\theta_0$。**\n阈值定义为 $\\theta_{0} = \\lambda \\sigma_{0}$，其中 $\\sigma_{0} = \\|r^{0}\\|_{2}/\\sqrt{m}$。\n首先，计算 $r^0$ 的欧几里得范数的平方：\n$$\n\\|r^{0}\\|_{2}^2 = (0.916)^2 + (-1.072)^2 + (0.784)^2 = 0.839056 + 1.149184 + 0.614656 = 2.602896\n$$\n噪声估计 $\\sigma_0$ 为：\n$$\n\\sigma_{0} = \\frac{\\sqrt{2.602896}}{\\sqrt{3}} = \\sqrt{\\frac{2.602896}{3}} = \\sqrt{0.867632} \\approx 0.93146766\n$$\n由于 $\\lambda = 1.0$，阈值为 $\\theta_{0} = \\lambda \\sigma_{0} = \\sigma_{0} \\approx 0.93146766$。\n\n**步骤3：计算线性步骤 $u^0$。**\n线性步骤为 $u^{0} = x^{0} + B^{0} A^{\\top} r^{0}$。给定 $x^0 = 0$ 和 $B^0 = \\alpha I = 0.9I$，该式简化为 $u^{0} = 0.9 A^{\\top} r^{0}$。\n首先，我们计算乘积 $A^{\\top} r^{0}$：\n$$\nA^{\\top} r^{0} = \\begin{pmatrix} 0.62  -0.44  0.22 \\\\ -0.17  0.59  0.31 \\\\ 0.35  -0.28  0.07 \\\\ 0.08  0.49  -0.53 \\end{pmatrix} \\begin{pmatrix} 0.916 \\\\ -1.072 \\\\ 0.784 \\end{pmatrix}\n$$\n$$\nA^{\\top} r^{0} = \\begin{pmatrix} (0.62)(0.916) + (-0.44)(-1.072) + (0.22)(0.784) \\\\ (-0.17)(0.916) + (0.59)(-1.072) + (0.31)(0.784) \\\\ (0.35)(0.916) + (-0.28)(-1.072) + (0.07)(0.784) \\\\ (0.08)(0.916) + (0.49)(-1.072) + (-0.53)(0.784) \\end{pmatrix} = \\begin{pmatrix} 0.56792 + 0.47168 + 0.17248 \\\\ -0.15572 - 0.63248 + 0.24304 \\\\ 0.3206 + 0.30016 + 0.05488 \\\\ 0.07328 - 0.52528 - 0.41552 \\end{pmatrix} = \\begin{pmatrix} 1.21208 \\\\ -0.54516 \\\\ 0.67564 \\\\ -0.86752 \\end{pmatrix}\n$$\n现在，我们乘以 $\\alpha=0.9$：\n$$\nu^{0} = 0.9 \\begin{pmatrix} 1.21208 \\\\ -0.54516 \\\\ 0.67564 \\\\ -0.86752 \\end{pmatrix} = \\begin{pmatrix} 1.090872 \\\\ -0.490644 \\\\ 0.608076 \\\\ -0.780768 \\end{pmatrix}\n$$\n\n**步骤4：计算去噪后的信号估计 $x^1$。**\n去噪步骤为 $x^{1} = \\eta(u^{0}; \\theta_{0})$，其中 $\\eta$ 是逐元素的软阈值函数 $\\eta(u;\\theta) = \\mathrm{sign}(u)\\,\\max(|u|-\\theta,\\,0)$。我们将此函数应用于 $u^0$ 的每个分量，使用的阈值为 $\\theta_0 \\approx 0.93146766$。\n- 对于 $u^0_1 = 1.090872$：$|u^0_1| > \\theta_0$。因此，$x^1_1 = u^0_1 - \\theta_0 = 1.090872 - 0.93146766 = 0.15940434$。\n- 对于 $u^0_2 = -0.490644$：$|u^0_2|  \\theta_0$。因此，$x^1_2 = 0$。\n- 对于 $u^0_3 = 0.608076$：$|u^0_3|  \\theta_0$。因此，$x^1_3 = 0$。\n- 对于 $u^0_4 = -0.780768$：$|u^0_4|  \\theta_0$。因此，$x^1_4 = 0$。\n因此，更新后的信号估计为 $x^1 = (0.15940434, 0, 0, 0)^{\\top}$。\n\n**步骤5：计算 Onsager 系数 $b_0$。**\nOnsager 系数为 $b_{0} = \\frac{1}{m} \\sum_{i=1}^{n} \\eta'(u^{0}_{i}; \\theta_{0})$，其中 $\\eta'(u;\\theta) = \\mathbf{1}\\{|u| > \\theta\\}$。\n我们对每个分量检查条件 $|u_i^0| > \\theta_0$：\n- $|u^0_1| = 1.090872 > \\theta_0 \\implies \\eta'(u^0_1; \\theta_0) = 1$。\n- $|u^0_2| = 0.490644  \\theta_0 \\implies \\eta'(u^0_2; \\theta_0) = 0$。\n- $|u^0_3| = 0.608076  \\theta_0 \\implies \\eta'(u^0_3; \\theta_0) = 0$。\n- $|u^0_4| = 0.780768  \\theta_0 \\implies \\eta'(u^0_4; \\theta_0) = 0$。\n总和为 $\\sum_{i=1}^4 \\eta'(u^0_i; \\theta_0) = 1 + 0 + 0 + 0 = 1$。由于 $m=3$，该系数为：\n$$\nb_0 = \\frac{1}{3} \\times 1 = \\frac{1}{3}\n$$\n\n**步骤6：计算更新后残差的第一个分量 $r^1_1$。**\n第一个分量的公式为 $r^{1}_1 = y_1 - (A x^{1})_1 + b_{0} r^{0}_1$。\n我们有：\n- $y_1 = 0.916$。\n- $r^0_1 = 0.916$。\n- $b_0 = 1/3$。\n我们需要计算 $(A x^{1})_1$。由于 $x^1_2 = x^1_3 = x^1_4 = 0$，这大大简化了计算：\n$$\n(A x^{1})_1 = A_{11} x^1_1 + A_{12} x^1_2 + A_{13} x^1_3 + A_{14} x^1_4 = (0.62)(x^1_1) + 0 + 0 + 0\n$$\n$$\n(A x^{1})_1 = 0.62 \\times 0.15940434 \\approx 0.09883069\n$$\n现在，我们将所有值代入 $r^1_1$ 的方程中：\n$$\nr^1_1 = 0.916 - 0.09883069 + \\frac{1}{3} (0.916)\n$$\n我们可以将含有 $0.916$ 的项组合在一起：\n$$\nr^1_1 = 0.916 \\left(1 + \\frac{1}{3}\\right) - 0.09883069 = 0.916 \\left(\\frac{4}{3}\\right) - 0.09883069\n$$\n$$\nr^1_1 \\approx 1.22133333 - 0.09883069 = 1.12250264\n$$\n问题要求将答案四舍五入到六位有效数字。该值为 $1.12250264...$。前六位有效数字是 $1$、$1$、$2$、$2$、$5$、$0$。第七位数字是 $2$，所以我们向下舍入。\n\n最终的数值为 $1.12250$。",
            "answer": "$$\n\\boxed{1.12250}\n$$"
        },
        {
            "introduction": "构建一个展开的算法模型只是第一步，如何有效地训练它则是一个更深层次的挑战。本练习将引导你从理论上分析学习迭代算法的可训练性问题，特别是梯度消失现象。你将发现，当一个迭代算法被展开成深度网络时，其固有的收缩性质可能导致在反向传播过程中梯度信号急剧衰减。此练习不仅要求你推导出这种衰减的上界，还将指导你设计一种结构性的解决方案——残差连接（skip-connection），以确保梯度能够稳定地在层间流动，从而将经典优化理论与现代深度学习的工程实践联系起来。",
            "id": "3456587",
            "problem": "考虑压缩感知中的稀疏优化目标，其函数定义为 $F(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是感知矩阵，$y \\in \\mathbb{R}^{m}$ 是测量向量，$\\lambda > 0$ 是一个正则化参数。一种最小化 $F$ 的经典基于模型的迭代方法是近端梯度法，其更新形式为 $x^{k+1} = \\mathrm{S}_{\\alpha \\lambda}\\!\\left(x^{k} - \\alpha A^{\\top}(A x^{k} - y)\\right)$，其中 $\\alpha > 0$ 是一个步长，$\\mathrm{S}_{\\tau}(\\cdot)$ 表示在水平 $\\tau$ 上的逐元素软阈值算子。在学习型迭代算法和算法展开中，通常用学习到的线性映射代替手工设计的线性映射，并在保持近端骨架的同时在各层之间解耦参数，从而得到以下形式的更新：\n$$\nx^{k+1} = \\mathrm{S}_{\\lambda}\\!\\left(W x^{k} + B y\\right), \\quad k = 0,1,\\ldots,K-1,\n$$\n其中 $W \\in \\mathbb{R}^{n \\times n}$ 和 $B \\in \\mathbb{R}^{n \\times m}$ 是学习到的矩阵，其受约束以反映基于模型的结构（例如，$W \\approx I - \\alpha A^{\\top}A$ 和 $B \\approx \\alpha A^{\\top}$）。假设我们工作的机制是：软阈值算子的活跃支撑集在各层之间是固定的，并且其在活跃坐标上的局部导数是恒等（identity）的。在这种局部线性化下，关于 $x^{k}$ 的每层雅可比矩阵近似为限制在活跃子空间上的矩阵 $W$。设 $W$ 在该子空间上的谱范数是一个收缩因子 $\\rho \\in (0,1)$，即 $\\|W\\|_{2} = \\rho$。\n\n设训练损失为 $L(x^{K}) = \\frac{1}{2}\\|A x^{K} - y\\|_{2}^{2}$，并考虑其通过 $K$ 个展开层的梯度反向传播。仅使用收缩映射、谱范数和链式法则的核心定义，完成以下任务：\n\n1. 在所述的线性化假设下，推导从第 $K$ 层到第 $0$ 层的梯度范数衰减的上界，用 $K$ 和 $\\rho$ 表示。您的答案必须是关于 $K$ 和 $\\rho$ 的单个解析表达式。\n\n2. 提出一种基于跳跃连接的松弛方法，通过在当前迭代点和基于模型的近端更新之间形成凸组合来保持基于模型的定点，\n$$\nx^{k+1} = (1 - \\eta)\\, x^{k} + \\eta\\, \\mathrm{S}_{\\lambda}\\!\\left(W x^{k} + B y\\right),\n$$\n其中标量松弛参数 $\\eta > 0$ 在各层之间共享。在相同的局部线性化假设下，并假定 $W$ 在活跃子空间上的谱位于 $[0,\\rho]$ 内，确定一个闭式解 $\\eta^{\\star}(\\rho)$，该解通过在保持前向映射为收缩映射的约束下，最大化谱区间端点处的最坏情况每层反向传播增益，从而改善梯度流。将 $\\eta^{\\star}(\\rho)$ 表示为仅依赖于 $\\rho$ 的单个闭式解析表达式。\n\n您的最终答案必须包含由第 1 部分的梯度衰减界和第 2 部分的松弛参数组成的对，格式化为单个行矩阵。不需要进行数值计算。如果在任何时候进行了近似，请在推理中明确说明理由。最终答案必须以无单位的形式表示。",
            "solution": "该问题陈述清晰、科学基础扎实且内部一致。它提出了一个在学习型迭代算法用于稀疏优化领域（信号处理和机器学习的一个子领域）内的标准但非平凡的问题。所有推导所需量所必需的假设和定义均已提供。因此，我们可以着手求解。\n\n**第1部分：梯度衰减界**\n\n问题的核心在于分析梯度在展开的迭代算法中的流动。该算法由以下针对 $k = 0, 1, \\ldots, K-1$ 的递推关系定义：\n$$\nx^{k+1} = f_k(x^k) = \\mathrm{S}_{\\lambda}\\!\\left(W x^{k} + B y\\right)\n$$\n从初始状态 $x^0$ 到最终状态 $x^K$ 的总变换是这 $K$ 个函数的复合：$x^K = f_{K-1} \\circ \\cdots \\circ f_0 (x^0)$。损失函数为 $L(x^K)$。我们关心的是损失函数关于初始状态的梯度 $\\nabla_{x^0} L = \\frac{\\partial L}{\\partial x^0}$ 与关于最终状态的梯度 $\\nabla_{x^K} L = \\frac{\\partial L}{\\partial x^K}$ 之间的关系。\n\n使用链式法则，第 $k$ 层的梯度可以与第 $k+1$ 层的梯度关联如下：\n$$\n\\nabla_{x^k} L = \\left(\\frac{\\partial x^{k+1}}{\\partial x^k}\\right)^{\\top} \\nabla_{x^{k+1}} L\n$$\n令 $J_k = \\frac{\\partial x^{k+1}}{\\partial x^k}$ 为第 $k$ 层变换的雅可比矩阵。问题陈述了一个关键的线性化假设：软阈值算子 $\\mathrm{S}_{\\lambda}(\\cdot)$ 的活跃支撑集是固定的，并且其在该支撑集上的导数是恒等（identity）的。在此假设下，雅可比矩阵 $J_k$ 局部等价于限制在活跃子空间上的学习矩阵 $W$。问题进一步指明，这个有效算子的谱范数是一个收缩因子 $\\rho \\in (0,1)$，即 $\\|J_k\\|_2 = \\rho$。\n\n将矩阵范数的次乘性应用于反向传播方程，我们得到梯度范数的一个界：\n$$\n\\|\\nabla_{x^k} L\\|_2 \\le \\left\\|J_k^{\\top}\\right\\|_2 \\|\\nabla_{x^{k+1}} L\\|_2\n$$\n谱范数的一个基本性质是 $\\|A^{\\top}\\|_2 = \\|A\\|_2$。因此，$\\|J_k^{\\top}\\|_2 = \\|J_k\\|_2 = \\rho$。这给了我们每层的梯度范数衰减关系：\n$$\n\\|\\nabla_{x^k} L\\|_2 \\le \\rho \\|\\nabla_{x^{k+1}} L\\|_2\n$$\n为了找到从第 $K$ 层到第 $0$ 层的总衰减，我们递归地展开这个不等式：\n$$\n\\|\\nabla_{x^{K-1}} L\\|_2 \\le \\rho \\|\\nabla_{x^K} L\\|_2\n$$\n$$\n\\|\\nabla_{x^{K-2}} L\\|_2 \\le \\rho \\|\\nabla_{x^{K-1}} L\\|_2 \\le \\rho (\\rho \\|\\nabla_{x^K} L\\|_2) = \\rho^2 \\|\\nabla_{x^K} L\\|_2\n$$\n对所有 $K$ 层继续这个过程，我们得到第 $0$ 层和第 $K$ 层梯度之间的关系：\n$$\n\\|\\nabla_{x^0} L\\|_2 \\le \\rho^K \\|\\nabla_{x^K} L\\|_2\n$$\n问题要求梯度范数衰减的上界，即比率 $\\frac{\\|\\nabla_{x^0} L\\|_2}{\\|\\nabla_{x^K} L\\|_2}$。从上面的不等式，这个比率的界为：\n$$\n\\frac{\\|\\nabla_{x^0} L\\|_2}{\\|\\nabla_{x^K} L\\|_2} \\le \\rho^K\n$$\n因此，在 $K$ 层上的梯度范数衰减的上界是 $\\rho^K$。\n\n**第2部分：最优松弛参数**\n\n我们现在考虑包含一个带有松弛参数 $\\eta > 0$ 的跳跃连接的修正更新规则：\n$$\nx^{k+1} = (1 - \\eta)\\, x^{k} + \\eta\\, \\mathrm{S}_{\\lambda}\\!\\left(W x^{k} + B y\\right)\n$$\n为了分析其性质，我们首先在与第1部分相同的局部线性化假设下计算其雅可比矩阵 $J_{\\eta} = \\frac{\\partial x^{k+1}}{\\partial x^k}$。$\\mathrm{S}_{\\lambda}(\\cdot)$ 在活跃支撑集上的导数是恒等（identity）的，所以我们有：\n$$\nJ_{\\eta} = (1 - \\eta)I + \\eta W\n$$\n这里，$I$ 和 $W$ 是活跃子空间上的算子。问题陈述了 $W$ 在该子空间上的谱，我们称之为 $\\sigma(W)$，位于实数区间 $[0, \\rho]$ 内。这意味着 $W$ 在此子空间上充当对称半正定算子。因此，$J_{\\eta}$ 也是对称的，其特征值由 $\\lambda_i(J_{\\eta}) = (1 - \\eta) + \\eta \\sigma_i(W)$ 给出，其中每个 $\\sigma_i(W) \\in \\sigma(W)$。$J_{\\eta}$ 的谱因此包含在区间 $[1-\\eta, 1-\\eta+\\eta\\rho]$ 内。\n\n第一个约束是前向映射保持为收缩映射，即 $\\|J_{\\eta}\\|_2 \\le 1$。对于对称矩阵，谱范数是最大绝对值特征值。因此，我们需要：\n$$\n\\|J_{\\eta}\\|_2 = \\max\\left(|1-\\eta|, |1-\\eta+\\eta\\rho|\\right) \\le 1\n$$\n给定 $\\eta > 0$ 和 $\\rho \\in (0,1)$，我们有 $1-\\rho > 0$，所以 $1-\\eta+\\eta\\rho = 1-\\eta(1-\\rho) \\le 1$。条件 $|1-\\eta+\\eta\\rho|\\le 1$ 变为 $-1 \\le 1-\\eta+\\eta\\rho$，简化为 $2 \\ge \\eta(1-\\rho)$，或 $\\eta \\le \\frac{2}{1-\\rho}$。另一个条件 $|1-\\eta|\\le 1$ 意味着 $-1 \\le 1-\\eta \\le 1$，这给出 $0 \\le \\eta \\le 2$。由于 $\\frac{2}{1-\\rho} > 2$，对 $\\eta$ 的有效约束是 $0  \\eta \\le 2$。\n\n目标是最大化“谱区间端点处的最坏情况每层反向传播增益”。反向传播算子是 $J_{\\eta}^{\\top} = J_{\\eta}$。其特征值决定了不同梯度分量的增益。最坏情况增益对应于具有最小幅值的特征值。$J_{\\eta}$ 谱区间端点处的特征值是 $1-\\eta$ 和 $1-\\eta+\\eta\\rho$。因此，我们的目标是解决以下优化问题：\n$$\n\\eta^{\\star} = \\arg\\max_{0  \\eta \\le 2} \\min\\left(|1-\\eta|, |1-\\eta+\\eta\\rho|\\right)\n$$\n两个函数最小值的最大值通常在它们的交点处找到，即它们的值相等的地方。我们令两个端点特征值的幅值相等：\n$$\n|1-\\eta| = |1-\\eta+\\eta\\rho|\n$$\n这导致两种可能性：\n1. $1-\\eta = 1-\\eta+\\eta\\rho \\implies \\eta\\rho=0 \\implies \\eta=0$，这超出了我们的域 $(0,2]$。\n2. $1-\\eta = -(1-\\eta+\\eta\\rho) \\implies 1-\\eta = -1+\\eta-\\eta\\rho \\implies 2 = 2\\eta-\\eta\\rho = \\eta(2-\\rho)$。\n\n在第二种情况下求解 $\\eta$ 得到：\n$$\n\\eta^{\\star} = \\frac{2}{2-\\rho}\n$$\n我们必须验证该解位于有效域 $\\eta \\in (0,2]$ 内。由于 $\\rho \\in (0,1)$，分母 $2-\\rho$ 在 $(1,2)$ 内。因此，$\\eta^{\\star} = \\frac{2}{2-\\rho}$ 在区间 $(\\frac{2}{2}, \\frac{2}{1}) = (1, 2)$ 内。这符合我们的约束 $0  \\eta \\le 2$。在这个最优值下，端点特征值的幅值是平衡的，这最大化了最小增益，从而改善了反向传播步骤的条件。\n\n最终答案由第1部分的界和第2部分的最优参数组成。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\rho^{K}  \\frac{2}{2-\\rho}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}