{
    "hands_on_practices": [
        {
            "introduction": "为了具体理解学习型迭代算法的运作机制，我们从展开一个经典的优化算法——交替方向乘子法 (ADMM)——入手。本练习将指导您完成一个用于稀疏回归的展开 ADMM 的前向计算过程，其中的关键步骤是将标准的软阈值算子替换为一个可学习的收缩函数。通过这个循序渐进的计算 ，您将亲身体验这些混合了模型驱动和数据驱动思想的架构是如何处理数据的。",
            "id": "3456561",
            "problem": "考虑压缩感知中的标准稀疏回归模型，其中潜在信号 $x \\in \\mathbb{R}^{n}$ 是通过矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 从线性测量 $y \\in \\mathbb{R}^{m}$ 在 $\\ell_{1}$ 稀疏性惩罚下估计得到的。其经典范式引入一个分裂变量 $z \\in \\mathbb{R}^{n}$ 并施加约束 $z = x$，从而得到以下约束优化问题\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|z\\|_{1}\n\\quad \\text{subject to} \\quad z = x,\n$$\n其中惩罚权重为 $\\lambda > 0$。使用缩放对偶变量 $u \\in \\mathbb{R}^{n}$ 和惩罚参数 $\\rho > 0$，将交替方向乘子法 (ADMM) 应用于与上述约束相关的增广拉格朗日量。在一个学习型迭代算法（算法展开）中，$z$-子问题被一个按元素施加的学习型收缩函数所替代，该函数对标量输入 $v \\in \\mathbb{R}$ 定义为\n$$\nS_{\\theta}(v) = \\alpha \\cdot \\mathrm{sign}(v) \\cdot \\max\\!\\big(|v| - \\tau,\\, 0\\big),\n$$\n其中学习型参数为 $\\theta = (\\alpha, \\tau)$，且 $\\alpha > 0$ 和 $\\tau \\geq 0$。\n\n设 $n = m = 2$，并考虑具有以下参数的具体实例\n$$\nA = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}, \\qquad y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\qquad \\lambda = 1, \\qquad \\rho = 1。\n$$\n将展开的 ADMM 初始化为\n$$\nx^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\qquad z^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\qquad u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}。\n$$\n执行两个展开的 ADMM 层（迭代）。在第 $k \\in \\{1,2\\}$ 层，使用学习型收缩参数\n$$\n\\theta_{1} = \\big(\\alpha_{1}, \\tau_{1}\\big) = \\left(\\frac{9}{10}, \\frac{1}{4}\\right), \\qquad\n\\theta_{2} = \\big(\\alpha_{2}, \\tau_{2}\\big) = \\left(\\frac{11}{10}, \\frac{2}{5}\\right),\n$$\n并按元素应用学习型收缩为 $z^{(k)} = S_{\\theta_{k}}\\!\\big(x^{(k)} + u^{(k-1)}\\big)$。$x$-更新和 $u$-更新是由 ADMM 方法针对上述约束范式和惩罚参数 $\\rho$ 使用缩放对偶变量所蕴含的更新。\n\n从指定的初始值开始，完成这两层后，计算欧几里得范数的平方 $\\|x^{(2)}\\|_{2}^{2}$ 的值。请以精确形式的单个实数表示您的最终答案。无需四舍五入。",
            "solution": "用户希望在一个学习型交替方向乘子法 (ADMM) 算法经过两次迭代后，计算状态变量 $x^{(2)}$ 的欧几里得范数的平方。\n\n首先，我将验证问题陈述。\n\n### 步骤1：提取已知条件\n- **优化问题**：$\\min_{x, z} \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|z\\|_{1}$ subject to $z = x$。\n- **维度**：$n=2, m=2$。\n- **数据**：\n  - $A = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$\n  - $y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- **参数**：\n  - $\\lambda = 1$\n  - $\\rho = 1$\n- **初始条件**：\n  - $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n  - $z^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n  - $u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n- **算法**：使用缩放对偶变量的展开 ADMM，共 $k \\in \\{1, 2\\}$ 两层。\n- **更新规则**：\n  - $x$-更新和 $u$-更新是标准的 ADMM 更新。\n  - $z$-更新是一个学习型收缩：$z^{(k)} = S_{\\theta_{k}}(x^{(k)} + u^{(k-1)})$，按元素应用。\n  - 学习型收缩函数是 $S_{\\theta}(v) = \\alpha \\cdot \\mathrm{sign}(v) \\cdot \\max(|v| - \\tau, 0)$。\n- **学习型参数**：\n  - 第1层 ($k=1$): $\\theta_{1} = (\\alpha_{1}, \\tau_{1}) = \\left(\\frac{9}{10}, \\frac{1}{4}\\right)$。\n  - 第2层 ($k=2$): $\\theta_{2} = (\\alpha_{2}, \\tau_{2}) = \\left(\\frac{11}{10}, \\frac{2}{5}\\right)$。\n- **目标**：计算 $\\|x^{(2)}\\|_{2}^{2}$。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题在稀疏优化、压缩感知和机器学习（特别是算法展开）领域具有科学依据。ADMM 的范式和学习型收缩函数在此背景下是标准的。这是一个适定问题，所有必要的数据、初始条件和更新规则都已提供，足以唯一确定结果。其语言精确客观。没有矛盾或歧义。问题是有效的。\n\n### 步骤3：开始求解\n\n针对缩放对偶变量 $u$ 的 ADMM 更新如下，其中 $k$ 是迭代索引：\n1.  **$x$-更新**：$x^{(k)} = \\arg\\min_{x} \\left( \\frac{1}{2}\\|Ax - y\\|_{2}^{2} + \\frac{\\rho}{2}\\|x - z^{(k-1)} + u^{(k-1)}\\|_{2}^{2} \\right)$。\n    一阶最优性条件得到一个线性系统：\n    $A^T(Ax - y) + \\rho(x - z^{(k-1)} + u^{(k-1)}) = 0$\n    $(A^T A + \\rho I)x = A^T y + \\rho(z^{(k-1)} - u^{(k-1)})$\n    $x^{(k)} = (A^T A + \\rho I)^{-1} (A^T y + \\rho(z^{(k-1)} - u^{(k-1)}))$\n\n2.  **$z$-更新**：问题指定了一个学习型更新规则：\n    $z^{(k)} = S_{\\theta_k}(x^{(k)} + u^{(k-1)})$\n\n3.  **$u$-更新**：$u^{(k)} = u^{(k-1)} + x^{(k)} - z^{(k)}$。\n\n首先，我预计算 $x$-更新中的常数部分。\n给定 $A = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$ 和 $\\rho = 1$：\n$A^T A = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} = \\begin{pmatrix} 5  4 \\\\ 4  5 \\end{pmatrix}$。\n$A^T A + \\rho I = \\begin{pmatrix} 5  4 \\\\ 4  5 \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 6  4 \\\\ 4  6 \\end{pmatrix}$。\n其逆矩阵为：\n$(A^T A + \\rho I)^{-1} = \\frac{1}{6 \\cdot 6 - 4 \\cdot 4} \\begin{pmatrix} 6  -4 \\\\ -4  6 \\end{pmatrix} = \\frac{1}{20} \\begin{pmatrix} 6  -4 \\\\ -4  6 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 3  -2 \\\\ -2  3 \\end{pmatrix}$。\n\n$A^T y$ 项为：\n$A^T y = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$。\n\n初始条件为 $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，$z^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 和 $u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n\n**第 1 层 ($k=1$)：**\n\n1.  **计算 $x^{(1)}$**：\n    $x^{(1)} = (A^T A + \\rho I)^{-1} (A^T y + \\rho(z^{(0)} - u^{(0)}))$\n    由于 $z^{(0)} = u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，表达式简化为：\n    $x^{(1)} = (A^T A + \\rho I)^{-1} A^T y = \\frac{1}{10} \\begin{pmatrix} 3  -2 \\\\ -2  3 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 3(2) - 2(1) \\\\ -2(2) + 3(1) \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} \\\\ -\\frac{1}{10} \\end{pmatrix}$。\n\n2.  **计算 $z^{(1)}$**：\n    学习型更新使用 $\\theta_1 = (\\alpha_1, \\tau_1) = (\\frac{9}{10}, \\frac{1}{4})$。\n    收缩函数的输入为 $v^{(1)} = x^{(1)} + u^{(0)} = \\begin{pmatrix} \\frac{2}{5} \\\\ -\\frac{1}{10} \\end{pmatrix}$。\n    我们按元素应用 $S_{\\theta_1}$：\n    对于第一个分量，$v_1 = \\frac{2}{5} = 0.4$。阈值为 $\\tau_1 = \\frac{1}{4} = 0.25$。\n    由于 $|v_1| > \\tau_1$， $z^{(1)}$ 的第一个分量是：\n    $z_1^{(1)} = \\alpha_1 \\cdot \\mathrm{sign}(v_1) \\cdot (|v_1| - \\tau_1) = \\frac{9}{10} \\cdot 1 \\cdot \\left(\\frac{2}{5} - \\frac{1}{4}\\right) = \\frac{9}{10} \\cdot \\left(\\frac{8-5}{20}\\right) = \\frac{9}{10} \\cdot \\frac{3}{20} = \\frac{27}{200}$。\n    对于第二个分量，$v_2 = -\\frac{1}{10} = -0.1$。\n    由于 $|v_2| = 0.1  \\tau_1 = 0.25$，$z^{(1)}$ 的第二个分量是：\n    $z_2^{(1)} = \\alpha_1 \\cdot \\mathrm{sign}(v_2) \\cdot \\max(|v_2| - \\tau_1, 0) = \\frac{9}{10} \\cdot (-1) \\cdot 0 = 0$。\n    所以，$z^{(1)} = \\begin{pmatrix} \\frac{27}{200} \\\\ 0 \\end{pmatrix}$。\n\n3.  **计算 $u^{(1)}$**：\n    $u^{(1)} = u^{(0)} + x^{(1)} - z^{(1)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} \\frac{2}{5} \\\\ -\\frac{1}{10} \\end{pmatrix} - \\begin{pmatrix} \\frac{27}{200} \\\\ 0 \\end{pmatrix}$。\n    为了进行减法，我们使用公分母 $200$：$\\frac{2}{5} = \\frac{80}{200}$ 且 $-\\frac{1}{10} = -\\frac{20}{200}$。\n    $u^{(1)} = \\begin{pmatrix} \\frac{80}{200} - \\frac{27}{200} \\\\ -\\frac{20}{200} - 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{53}{200} \\\\ -\\frac{20}{200} \\end{pmatrix} = \\begin{pmatrix} \\frac{53}{200} \\\\ -\\frac{1}{10} \\end{pmatrix}$。\n\n**第 2 层 ($k=2$)：**\n\n1.  **计算 $x^{(2)}$**：\n    $x^{(2)} = (A^T A + \\rho I)^{-1} (A^T y + \\rho(z^{(1)} - u^{(1)}))$。\n    首先，我们计算 $z^{(1)} - u^{(1)}$ 项：\n    $z^{(1)} - u^{(1)} = \\begin{pmatrix} \\frac{27}{200} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} \\frac{53}{200} \\\\ -\\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{27-53}{200} \\\\ 0 - (-\\frac{1}{10}) \\end{pmatrix} = \\begin{pmatrix} -\\frac{26}{200} \\\\ \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} -\\frac{13}{100} \\\\ \\frac{1}{10} \\end{pmatrix}$。\n    接下来，我们计算将要与矩阵相乘的向量：\n    $A^T y + \\rho(z^{(1)} - u^{(1)}) = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} -\\frac{13}{100} \\\\ \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} 2 - \\frac{13}{100} \\\\ 1 + \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{200-13}{100} \\\\ \\frac{10+1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{187}{100} \\\\ \\frac{11}{10} \\end{pmatrix}$。\n    现在，我们计算 $x^{(2)}$：\n    $x^{(2)} = \\frac{1}{10} \\begin{pmatrix} 3  -2 \\\\ -2  3 \\end{pmatrix} \\begin{pmatrix} \\frac{187}{100} \\\\ \\frac{11}{10} \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 3(\\frac{187}{100}) - 2(\\frac{11}{10}) \\\\ -2(\\frac{187}{100}) + 3(\\frac{11}{10}) \\end{pmatrix}$。\n    让我们在向量内部使用公分母 $100$：$\\frac{11}{10} = \\frac{110}{100}$。\n    $x^{(2)} = \\frac{1}{10} \\begin{pmatrix} \\frac{3(187) - 2(110)}{100} \\\\ \\frac{-2(187) + 3(110)}{100} \\end{pmatrix} = \\frac{1}{1000} \\begin{pmatrix} 561 - 220 \\\\ -374 + 330 \\end{pmatrix} = \\frac{1}{1000} \\begin{pmatrix} 341 \\\\ -44 \\end{pmatrix} = \\begin{pmatrix} \\frac{341}{1000} \\\\ -\\frac{44}{1000} \\end{pmatrix}$。\n\n**最终计算：**\n\n目标是计算 $\\|x^{(2)}\\|_{2}^{2}$。\n$\\|x^{(2)}\\|_{2}^{2} = \\left(\\frac{341}{1000}\\right)^2 + \\left(-\\frac{44}{1000}\\right)^2 = \\frac{341^2 + (-44)^2}{1000^2}$。\n$341^2 = 116281$。\n$(-44)^2 = 44^2 = 1936$。\n$341^2 + 44^2 = 116281 + 1936 = 118217$。\n$1000^2 = 1000000$。\n因此，$\\|x^{(2)}\\|_{2}^{2} = \\frac{118217}{1000000}$。\n这也可以写作 $0.118217$。",
            "answer": "$$ \\boxed{\\frac{118217}{1000000}} $$"
        },
        {
            "introduction": "从算法的执行机制转向架构的设计原理，我们来探讨一个在训练深度展开网络时遇到的核心挑战：梯度消失问题。简单地堆叠许多算法层可能会导致训练不稳定，这在深度学习中是一个普遍现象。本练习  将深入分析此问题的数学根源，通过考察梯度在网络中的反向传播过程，并要求您设计一个基于“跳跃连接”的解决方案来改善梯度流，这是一种同时受到深度学习和经典数值方法启发的强大技术。",
            "id": "3456587",
            "problem": "考虑压缩感知中的稀疏优化目标，该目标由函数 $F(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ 定义，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是感知矩阵，$y \\in \\mathbb{R}^{m}$ 是测量向量，$\\lambda  0$ 是正则化参数。一种用于最小化 $F$ 的经典基于模型的迭代方法是近端梯度方案，其更新形式为 $x^{k+1} = \\mathrm{S}_{\\alpha \\lambda}\\!\\left(x^{k} - \\alpha A^{\\top}(A x^{k} - y)\\right)$，其中 $\\alpha  0$ 是步长，$\\mathrm{S}_{\\tau}(\\cdot)$ 表示水平为 $\\tau$ 的逐元素软阈值算子。在学习型迭代算法和算法展开中，通常会用学习得到的线性映射替代手工设计的线性映射，并在各层之间解耦参数，同时保留近端骨架，从而得到以下形式的更新：\n$$\nx^{k+1} = \\mathrm{S}_{\\lambda}\\!\\left(W x^{k} + B y\\right), \\quad k = 0,1,\\ldots,K-1,\n$$\n其中 $W \\in \\mathbb{R}^{n \\times n}$ 和 $B \\in \\mathbb{R}^{n \\times m}$ 是学习得到的矩阵，其受约束以反映基于模型的结构（例如，$W \\approx I - \\alpha A^{\\top}A$ 和 $B \\approx \\alpha A^{\\top}$）。假设我们在这样一种机制下工作：软阈值算子的激活支撑集在各层之间是固定的，并且其在激活坐标上的局部导数是单位阵。在这种局部线性化下，关于 $x^{k}$ 的每层雅可比矩阵近似为限制在激活子空间上的矩阵 $W$。设 $W$ 在该子空间上的谱范数是一个收缩因子 $\\rho \\in (0,1)$，即 $\\|W\\|_{2} = \\rho$。\n\n设训练损失为 $L(x^{K}) = \\frac{1}{2}\\|A x^{K} - y\\|_{2}^{2}$，并考虑其梯度通过 $K$ 个展开层的反向传播。仅使用收缩映射、谱范数和链式法则的核心定义，执行以下操作：\n\n1. 在所述的线性化假设下，推导从第 $K$ 层到第 $0$ 层的梯度范数衰减的上界，用 $K$ 和 $\\rho$ 表示。您的答案必须是该上界作为 $K$ 和 $\\rho$ 的函数的单个解析表达式。\n\n2. 提出一种基于跳跃连接的松弛方法，通过在当前迭代值和基于模型的近端更新之间形成凸组合来保留基于模型的定点，\n$$\nx^{k+1} = (1 - \\eta)\\, x^{k} + \\eta\\, \\mathrm{S}_{\\lambda}\\!\\left(W x^{k} + B y\\right),\n$$\n其中标量松弛参数 $\\eta  0$ 在各层之间共享。在相同的局部线性化假设下，并将 $W$ 在激活子空间上的谱视为位于 $[0,\\rho]$ 内，确定一个闭式选择 $\\eta^{\\star}(\\rho)$，该选择通过最大化谱区间端点处的最坏情况每层反向传播增益来改善梯度流，同时受限于前向映射保持为收缩映射。将 $\\eta^{\\star}(\\rho)$ 表示为仅依赖于 $\\rho$ 的单个闭式解析表达式。\n\n您的最终答案必须包含由第1部分的梯度衰减上界和第2部分的松弛参数组成的对，并格式化为单行矩阵。无需进行数值计算。如果在任何时候进行了近似，请在您的推理中明确说明理由。最终答案必须不带单位表示。",
            "solution": "该问题是适定的、有科学依据且内部一致的。它提出了一个在学习型迭代算法用于稀疏优化领域内的标准但非平凡的问题，该领域是信号处理和机器学习的一个子领域。所有必要的假设和定义都已提供，以推导所要求的量。因此，我们可以着手求解。\n\n**第1部分：梯度衰减上界**\n\n问题的核心在于分析梯度在展开的迭代算法中的流动。该算法由 $k = 0, 1, \\ldots, K-1$ 的递推关系定义：\n$$\nx^{k+1} = f_k(x^k) = \\mathrm{S}_{\\lambda}\\!\\left(W x^{k} + B y\\right)\n$$\n从初始状态 $x^0$ 到最终状态 $x^K$ 的总变换是这 $K$ 个函数的复合：$x^K = f_{K-1} \\circ \\cdots \\circ f_0 (x^0)$。损失函数为 $L(x^K)$。我们关心的是损失函数关于初始状态的梯度 $\\nabla_{x^0} L = \\frac{\\partial L}{\\partial x^0}$ 与关于最终状态的梯度 $\\nabla_{x^K} L = \\frac{\\partial L}{\\partial x^K}$ 之间的关系。\n\n使用链式法则，第 $k$ 层的梯度可以与第 $k+1$ 层的梯度关联如下：\n$$\n\\nabla_{x^k} L = \\left(\\frac{\\partial x^{k+1}}{\\partial x^k}\\right)^{\\top} \\nabla_{x^{k+1}} L\n$$\n设 $J_k = \\frac{\\partial x^{k+1}}{\\partial x^k}$ 是第 $k$ 层变换的雅可比矩阵。问题陈述了一个关键的线性化假设：软阈值算子 $\\mathrm{S}_{\\lambda}(\\cdot)$ 的激活支撑集是固定的，并且其在该支撑集上的导数是单位阵。在此假设下，雅可比矩阵 $J_k$ 局部等价于限制在激活子空间上的学习矩阵 $W$。问题进一步规定，这个有效算子的谱范数是一个收缩因子 $\\rho \\in (0,1)$，即 $\\|J_k\\|_2 = \\rho$。\n\n将矩阵范数的次乘性应用于反向传播方程，我们得到梯度范数的一个界：\n$$\n\\|\\nabla_{x^k} L\\|_2 \\le \\left\\|J_k^{\\top}\\right\\|_2 \\|\\nabla_{x^{k+1}} L\\|_2\n$$\n谱范数的一个基本性质是 $\\|A^{\\top}\\|_2 = \\|A\\|_2$。因此，$\\|J_k^{\\top}\\|_2 = \\|J_k\\|_2 = \\rho$。这给了我们每层的梯度范数衰减关系：\n$$\n\\|\\nabla_{x^k} L\\|_2 \\le \\rho \\|\\nabla_{x^{k+1}} L\\|_2\n$$\n为了求出从第 $K$ 层到第 $0$ 层的总衰减，我们递归地展开这个不等式：\n$$\n\\|\\nabla_{x^{K-1}} L\\|_2 \\le \\rho \\|\\nabla_{x^K} L\\|_2\n$$\n$$\n\\|\\nabla_{x^{K-2}} L\\|_2 \\le \\rho \\|\\nabla_{x^{K-1}} L\\|_2 \\le \\rho (\\rho \\|\\nabla_{x^K} L\\|_2) = \\rho^2 \\|\\nabla_{x^K} L\\|_2\n$$\n对所有 $K$ 层继续这个过程，我们得到第 $0$ 层和第 $K$ 层梯度之间的关系：\n$$\n\\|\\nabla_{x^0} L\\|_2 \\le \\rho^K \\|\\nabla_{x^K} L\\|_2\n$$\n问题要求梯度范数衰减的上界，即比率 $\\frac{\\|\\nabla_{x^0} L\\|_2}{\\|\\nabla_{x^K} L\\|_2}$。从上面的不等式，这个比率的界为：\n$$\n\\frac{\\|\\nabla_{x^0} L\\|_2}{\\|\\nabla_{x^K} L\\|_2} \\le \\rho^K\n$$\n因此，在 $K$ 层上的梯度范数衰减的上界是 $\\rho^K$。\n\n**第2部分：最优松弛参数**\n\n我们现在考虑包含一个带有松弛参数 $\\eta > 0$ 的跳跃连接的修正更新规则：\n$$\nx^{k+1} = (1 - \\eta)\\, x^{k} + \\eta\\, \\mathrm{S}_{\\lambda}\\!\\left(W x^{k} + B y\\right)\n$$\n为了分析其性质，我们首先在与第1部分相同的局部线性化假设下计算其雅可比矩阵 $J_{\\eta} = \\frac{\\partial x^{k+1}}{\\partial x^k}$。在激活支撑集上 $\\mathrm{S}_{\\lambda}(\\cdot)$ 的导数是单位阵，所以我们有：\n$$\nJ_{\\eta} = (1 - \\eta)I + \\eta W\n$$\n这里，$I$ 和 $W$ 是激活子空间上的算子。问题陈述了 $W$ 在该子空间上的谱，我们称之为 $\\sigma(W)$，位于实数区间 $[0, \\rho]$ 内。这意味着 $W$ 在该子空间上作为一个对称半正定算子。因此，$J_{\\eta}$ 也是对称的，其特征值由 $\\lambda_i(J_{\\eta}) = (1 - \\eta) + \\eta \\sigma_i(W)$ 给出，其中每个 $\\sigma_i(W) \\in \\sigma(W)$。因此，$J_{\\eta}$ 的谱包含在区间 $[1-\\eta, 1-\\eta+\\eta\\rho]$ 中。\n\n第一个约束是前向映射保持为收缩映射，即 $\\|J_{\\eta}\\|_2  1$。对于对称矩阵，谱范数是最大绝对值特征值。因此，我们需要：\n$$\n\\|J_{\\eta}\\|_2 = \\max\\left(|1-\\eta|, |1-\\eta+\\eta\\rho|\\right)  1\n$$\n给定 $\\eta > 0$ 和 $\\rho \\in (0,1)$，我们有 $1-\\rho > 0$，所以 $1-\\eta+\\eta\\rho = 1-\\eta(1-\\rho)  1$。条件 $|1-\\eta+\\eta\\rho|1$ 变为 $-1  1-\\eta+\\eta\\rho$，简化为 $2 > \\eta(1-\\rho)$，或 $\\eta  \\frac{2}{1-\\rho}$。另一个条件 $|1-\\eta|1$ 意味着 $-1  1-\\eta  1$，这给出 $0  \\eta  2$。由于 $\\frac{2}{1-\\rho} > 2$，对 $\\eta$ 的有效约束是 $0  \\eta  2$。\n\n目标是最大化“谱区间端点处的最坏情况每层反向传播增益”。反向传播算子是 $J_{\\eta}^{\\top} = J_{\\eta}$。其特征值决定了不同梯度分量的增益。最坏情况增益对应于具有最小模的特征值。$J_{\\eta}$ 的谱区间端点处的特征值是 $1-\\eta$ 和 $1-\\eta+\\eta\\rho$。因此，我们旨在解决以下优化问题：\n$$\n\\eta^{\\star} = \\arg\\max_{0  \\eta  2} \\min\\left(|1-\\eta|, |1-\\eta+\\eta\\rho|\\right)\n$$\n两个函数的最小值的最大值通常在它们的交点处找到，即它们的值相等的地方。我们令两个端点特征值的模相等：\n$$\n|1-\\eta| = |1-\\eta+\\eta\\rho|\n$$\n这导致两种可能性：\n1. $1-\\eta = 1-\\eta+\\eta\\rho \\implies \\eta\\rho=0 \\implies \\eta=0$，这超出了我们的定义域 $(0,2)$。\n2. $1-\\eta = -(1-\\eta+\\eta\\rho) \\implies 1-\\eta = -1+\\eta-\\eta\\rho \\implies 2 = 2\\eta-\\eta\\rho = \\eta(2-\\rho)$。\n\n在第二种情况下求解 $\\eta$ 得：\n$$\n\\eta^{\\star} = \\frac{2}{2-\\rho}\n$$\n我们必须验证该解位于有效域 $\\eta \\in (0,2)$ 内。由于 $\\rho \\in (0,1)$，分母 $2-\\rho$ 位于 $(1,2)$ 内。因此，$\\eta^{\\star} = \\frac{2}{2-\\rho}$ 位于区间 $(\\frac{2}{2}, \\frac{2}{1}) = (1, 2)$ 内。这符合我们的约束 $0  \\eta  2$。在这个最优值下，端点特征值的模是平衡的，这最大化了最小增益，从而改善了反向传播步骤的条件数。\n\n最终答案由第1部分的界和第2部分的最优参数组成。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\rho^{K}  \\frac{2}{2-\\rho}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "学习型算法的另一个核心目标是提升性能，尤其是收敛速度。除了替换非线性函数，我们还可以学习线性算子，例如预条件子，来加速算法的收敛。这个高级练习  深入探讨了预条件近端点法的理论，要求您在给定资源预算（预条件子矩阵的迹）的约束下，推导出一个能最大化收敛速度的最优对角预条件子。这展示了如何运用基于模型的设计原则来指导和优化学习过程。",
            "id": "3456590",
            "problem": "考虑一个稀疏恢复问题，其中感知矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，目标信号 $x^{\\star} \\in \\mathbb{R}^{n}$ 是 $s$-稀疏的（即，最多有 $s$ 个非零项），观测值为 $y = A x^{\\star} + w$，其中 $w \\in \\mathbb{R}^{m}$ 是噪声。定义目标函数 $f : \\mathbb{R}^{n} \\to \\mathbb{R}$ 为 $f(x) = \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\tau \\|x\\|_{1}$，其中 $\\tau > 0$ 是一个正则化参数。假设以下基本前提：\n- 次微分 $\\partial f$ 是一个最大单调算子，因为 $f$ 是正常的、闭的且凸的。\n- 在受限支撑集上的强凸性：存在一个常数 $\\mu_{s} > 0$，使得对于所有支撑在固定索引集 $S \\subset \\{1, \\dots, n\\}$（其中 $|S| \\leq s$）上的任意一对向量 $x, y \\in \\mathbb{R}^{n}$，以及对于任意 $u \\in \\partial f(x)$ 和 $v \\in \\partial f(y)$，不等式 $\\langle u - v, x - y \\rangle \\geq \\mu_{s} \\|x - y\\|_{2}^{2}$ 成立。这是当矩阵 $A$ 具有适当的 $s$-受限奇异值界时，由二次项 $\\frac{1}{2}\\|A x - y\\|_{2}^{2}$ 导出的受限强凸性条件，并且该性质在加上凸的 $\\ell_{1}$ 项后仍然保持。\n\n一个学习型预条件近端点方法 (PPPM) 展开为一个 $K$ 层的架构，其中每一层使用如下更新规则：\n$$\nx^{k+1} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ f(x) + \\frac{1}{2} \\|x - x^{k}\\|_{P}^{2} \\right\\}, \\quad \\|z\\|_{P}^{2} := \\langle z, P z \\rangle,\n$$\n其中 $P \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵，被视为一个学习得到的预处理器。等价地，每一层应用预解算子\n$$\nT_{P} := (I + P^{-1} \\partial f)^{-1},\n$$\n使得 $x^{k+1} = T_{P}(x^{k})$。预处理器 $P$ 被约束为对角且正定的，并具有固定的迹预算 $\\operatorname{Tr}(P) = c$（对于某个 $c > 0$），这表示学习模型中的资源约束。\n\n在由标准内积诱导的欧几里得范数下进行分析。仅使用上述基本事实，通过将 $\\partial f$ 的强单调性与矩阵缩放 $P^{-1}$ 下的预解算子利普希茨常数相关联，推导出 $T_{P}$ 在 $s$-稀疏支撑集上的逐层线性收敛收缩界，并用 $\\mu_{s}$ 和 $P$ 的特征值表示。然后，解决以下设计问题：在所有满足 $\\operatorname{Tr}(P) = c$ 的对角正定矩阵 $P$ 中，选择一个 $P$ 以最小化在所有 $|S| \\leq s$ 的支撑集 $S$ 上的最坏情况收缩因子。将最小化的最坏情况收缩因子表示为关于 $\\mu_{s}$、$n$ 和 $c$ 的闭式解析表达式。\n\n你的最终答案必须是单个闭式解析表达式。最终答案中不允许出现不等式或方程。无需进行数值代入。如果你引入任何首字母缩略词，请在首次使用时写出其全称并将缩写词放在括号内。如果你使用任何角度，请指明是弧度还是度。最终表达式中无需单位。",
            "solution": "该问题要求我们首先为一个学习型预条件近端点方法 (PPPM) 在 $s$-稀疏支撑集上推导一个逐层的线性收敛收縮界，然后通过在迹约束下选择预处理器矩阵 $P$ 来优化这个界。\n\n我们首先来推导收缩因子。PPPM的更新规则由下式给出\n$$\nx^{k+1} = T_{P}(x^{k}) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ f(z) + \\frac{1}{2} \\|z - x^{k}\\|_{P}^{2} \\right\\}\n$$\n其中 $f(x) = \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\tau \\|x\\|_{1}$，且 $P$ 是一个对称正定矩阵。该最小化问题的一阶最优性条件是：\n$$\n0 \\in \\partial f(x^{k+1}) + P(x^{k+1} - x^{k})\n$$\n其中 $\\partial f$ 是 $f$ 的次微分。\n\n为求算子 $T_P$ 的收缩因子，我们考虑两个任意输入点 $x_a, x_b \\in \\mathbb{R}^{n}$，及其在该算子下的相应输出 $y_a = T_P(x_a)$ 和 $y_b = T_P(x_b)$。根据最优性条件，我们有：\n$$\nP(x_a - y_a) \\in \\partial f(y_a)\n$$\n$$\nP(x_b - y_b) \\in \\partial f(y_b)\n$$\n令 $u_a = P(x_a - y_a)$ 且 $u_b = P(x_b - y_b)$。因此，我们有 $u_a \\in \\partial f(y_a)$ 和 $u_b \\in \\partial f(y_b)$。\n\n问题陈述，对于任何满足 $|S| \\leq s$ 的索引集 $S \\subset \\{1, \\dots, n\\}$，函数 $f$ 表现出受限强凸性。这意味着对于任意两个都支撑在 $S$ 上的向量 $y_a, y_b$，以及任意次梯度 $u_a \\in \\partial f(y_a)$ 和 $u_b \\in \\partial f(y_b)$，以下不等式成立：\n$$\n\\langle u_a - u_b, y_a - y_b \\rangle \\geq \\mu_{s} \\|y_a - y_b\\|_{2}^{2}\n$$\n其中 $\\mu_s > 0$ 是受限强凸性常数。问题引导我们在“$s$-稀疏支撑集上”寻找一个收敛界。我们将其解释为分析当算子的输入和输出都被限制在 $|S| \\leq s$ 的一个固定稀疏支撑集 $S$ 上时的行为。因此，我们假设 $x_a, x_b, y_a, y_b$ 都支撑在 $S$ 上。\n\n将 $u_a$ 和 $u_b$ 的表达式代入不等式，我们得到：\n$$\n\\langle P(x_a - y_a) - P(x_b - y_b), y_a - y_b \\rangle \\geq \\mu_{s} \\|y_a - y_b\\|_{2}^{2}\n$$\n令 $\\Delta x = x_a - x_b$ 且 $\\Delta y = y_a - y_b$。不等式变为：\n$$\n\\langle P(\\Delta x - \\Delta y), \\Delta y \\rangle \\geq \\mu_{s} \\|\\Delta y\\|_{2}^{2}\n$$\n利用内积的线性性质以及 $P$ 是对称的这一事实：\n$$\n\\langle P \\Delta x, \\Delta y \\rangle - \\langle P \\Delta y, \\Delta y \\rangle \\geq \\mu_{s} \\|\\Delta y\\|_{2}^{2}\n$$\n整理各项，我们得到：\n$$\n\\langle \\Delta x, P \\Delta y \\rangle \\geq \\langle \\Delta y, P \\Delta y \\rangle + \\mu_{s} \\|\\Delta y\\|_{2}^{2}\n$$\n对左侧应用柯西-施瓦茨不等式，得到：\n$$\n\\|\\Delta x\\|_{2} \\|P \\Delta y\\|_{2} \\geq \\langle \\Delta x, P \\Delta y \\rangle \\geq \\langle P \\Delta y, \\Delta y \\rangle + \\mu_{s} \\|\\Delta y\\|_{2}^{2}\n$$\n预处理器 $P$ 是对角矩阵，因此 $P = \\operatorname{diag}(p_1, \\dots, p_n)$，其中 $p_i > 0$。由于我们假设 $\\Delta x$ 和 $\\Delta y$ 都支撑在 $S$ 上：\n$$\n\\langle P \\Delta y, \\Delta y \\rangle = \\sum_{i \\in S} p_i (\\Delta y_i)^2 \\geq (\\min_{i \\in S} p_i) \\sum_{i \\in S} (\\Delta y_i)^2 = (\\min_{i \\in S} p_i) \\|\\Delta y\\|_{2}^{2}\n$$\n令 $p_{\\min, S} = \\min_{i \\in S} p_i$。不等式变为：\n$$\n\\|P\\Delta x\\|_2 \\|\\Delta y\\|_2 \\geq (p_{\\min, S} + \\mu_{s}) \\|\\Delta y\\|_{2}^{2}\n$$\n假设 $\\Delta y \\neq 0$，我们可以两边同除以 $\\|\\Delta y\\|_{2}$：\n$$\n\\|P \\Delta x\\|_{2} \\geq (p_{\\min, S} + \\mu_{s}) \\|\\Delta y\\|_{2}\n$$\n这给出了 $\\|\\Delta y\\|_{2}$ 的一个界：\n$$\n\\|\\Delta y\\|_{2} \\leq \\frac{1}{p_{\\min, S} + \\mu_{s}} \\|P \\Delta x\\|_{2}\n$$\n使用界 $\\|P \\Delta x\\|_{2} \\leq p_{\\max, S} \\|\\Delta x\\|_{2}$ (因为 $\\Delta x$ 也支撑在 $S$上):\n$$\n\\|\\Delta y\\|_{2} \\leq \\frac{p_{\\max, S}}{p_{\\min, S} + \\mu_{s}} \\|\\Delta x\\|_{2}\n$$\n因此，算子 $T_P$ 在支撑集 $S$ 上受限的收缩因子为 $\\kappa_S = \\frac{p_{\\max, S}}{p_{\\min, S} + \\mu_{s}}$。\n\n现在，我们进入问题的第二部分：最优预处理器 $P$ 的设计。我们需要选择对角项 $p_1, \\dots, p_n$ 来最小化在所有大小至多为 $s$ 的可能支撑集 $S$ 上的最坏情况收缩因子。该优化问题是：\n$$\n\\min_{p_1, \\dots, p_n} \\left( \\max_{S \\subset \\{1, \\dots, n\\}, |S| \\leq s} \\kappa_S \\right)\n$$\n约束条件为 $p_i > 0$ 对所有 $i=1, \\dots, n$ 成立且 $\\operatorname{Tr}(P) = \\sum_{i=1}^n p_i = c$。\n\n对于给定的 $P$，最坏情况下的收缩因子是：\n$$\n\\kappa_{\\text{wc}} = \\max_{S, |S|\\leq s} \\frac{\\max_{i \\in S} p_i}{\\min_{i \\in S} p_i + \\mu_{s}}\n$$\n为了最大化这个分数，对于一组固定的 $\\{p_i\\}$，我们应该选择一个支撑集 $S$，使其包含对应于全局最大值 $p_{\\max} = \\max_{i=1, \\dots, n} p_i$ 的索引，以及对应于全局最小值 $p_{\\min} = \\min_{i=1, \\dots, n} p_i$ 的索引。只要 $s \\geq 2$，这样的集合 $S$ 就可以被构建，这对于非平凡的稀疏恢复问题是典型情况。如果 $s=1$，最坏情况的 $\\kappa_S$ 是在具有最大 $p_i$ 的单个索引上取得的，得到 $\\frac{p_{\\max}}{p_{\\max}+\\mu_s}$。在这两种情况下，最坏情况因子都是 $p_{\\max}$ 和 $p_{\\min}$ 的函数。我们先来分析 $s \\geq 2$ 的情况。最坏情况因子为 $\\frac{p_{\\max}}{p_{\\min} + \\mu_{s}}$。\n我们的优化问题简化为：\n$$\n\\min_{p_1, \\dots, p_n} \\frac{\\max_i p_i}{\\min_i p_i + \\mu_{s}} \\quad \\text{s.t.} \\quad \\sum_{i=1}^n p_i = c, \\quad p_i > 0\n$$\n令 $p_{\\max} = \\max_i p_i$ 和 $p_{\\min} = \\min_i p_i$。目标函数相对于 $p_{\\max}$ 是递增的，相对于 $p_{\\min}$ 是递减的。为了最小化这个目标函数，我们必须选择 $p_i$ 的值，使得 $p_{\\max}$ 尽可能小，而 $p_{\\min}$ 尽可能大。这可以通过使所有的 $p_i$ 值相等来实现。\n\n我们来形式化地说明这一点。根据约束 $\\sum_{i=1}^n p_i = c$，我们可以推导出 $p_{\\min}$ 和 $p_{\\max}$ 的界。我们有 $c = \\sum p_i \\geq n \\cdot p_{\\min}$，这意味着 $p_{\\min} \\leq \\frac{c}{n}$。类似地，$c = \\sum p_i \\leq n \\cdot p_{\\max}$，这意味着 $p_{\\max} \\geq \\frac{c}{n}$。$p_{\\max}$ 的最小可能值为 $\\frac{c}{n}$，$p_{\\min}$ 的最大可能值为 $\\frac{c}{n}$。这两个界当且仅当 $p_{\\min} = p_{\\max} = \\frac{c}{n}$ 时同时达到，这要求对于所有的 $i=1, \\dots, n$都有 $p_i = \\frac{c}{n}$。\n\n这个选择，$P = \\frac{c}{n}I$，是可行的，因为 $p_i = c/n > 0$ 且 $\\sum p_i = n(\\frac{c}{n}) = c$。对于这个 $P$ 的选择，任何支撑集 $S$ 都有 $p_{\\max, S} = p_{\\min, S} = \\frac{c}{n}$。收缩因子变得与支撑集 $S$ 无关：\n$$\n\\kappa_S = \\frac{c/n}{c/n + \\mu_{s}}\n$$\n这是最坏情况收缩因子的最小可能值。如果我们选择任何其他 $p_i$ 值集合，我们都会有 $p_{\\min}  \\frac{c}{n}$ 和 $p_{\\max} > \\frac{c}{n}$，这将导致比值 $\\frac{p_{\\max}}{p_{\\min} + \\mu_{s}}$ 的值更大。\n同样的逻辑也适用于 $s=1$ 的情况，此时我们需要最小化 $\\frac{p_{\\max}}{p_{\\max}+\\mu_s}$。这个函数随 $p_{\\max}$ 的增大而增大，因此最小化 $p_{\\max}$（通过设置所有 $p_i=c/n$）仍然是最优策略。\n\n因此，最小化的最坏情况收縮因子为：\n$$\n\\kappa^* = \\frac{\\frac{c}{n}}{\\frac{c}{n} + \\mu_{s}} = \\frac{\\frac{c}{n}}{\\frac{c + n \\mu_{s}}{n}} = \\frac{c}{c + n \\mu_{s}}\n$$\n这是一个关于给定参数 $\\mu_s$、$n$ 和 $c$ 的闭式解析表达式。",
            "answer": "$$\\boxed{\\frac{c}{c + n \\mu_{s}}}$$"
        }
    ]
}