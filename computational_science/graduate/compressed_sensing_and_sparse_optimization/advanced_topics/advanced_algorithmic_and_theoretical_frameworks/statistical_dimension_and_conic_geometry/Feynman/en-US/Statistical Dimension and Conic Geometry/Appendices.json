{
    "hands_on_practices": [
        {
            "introduction": "Before exploring the probabilistic aspects of conic geometry, it is essential to have a firm grasp of the fundamental geometric objects themselves. This first practice provides a concrete exercise in calculating Euclidean projections onto a basic convex cone—the non-negative orthant—and its corresponding polar cone. By working through these derivations from first principles, you will solidify your understanding of these core concepts and verify a key orthogonality property that underpins more advanced results.",
            "id": "3481874",
            "problem": "Let $C \\subset \\mathbb{R}^{n}$ denote the nonnegative orthant $C := \\{x \\in \\mathbb{R}^{n} : x_{i} \\geq 0 \\text{ for all } i\\}$. For a given vector $g \\in \\mathbb{R}^{n}$, the Euclidean projection of $g$ onto a nonempty closed convex set $S \\subset \\mathbb{R}^{n}$ is defined by $\\Pi_{S}(g) := \\arg\\min_{x \\in S} \\frac{1}{2}\\|x - g\\|_{2}^{2}$. The polar cone of a cone $K \\subset \\mathbb{R}^{n}$ is defined by $K^{\\circ} := \\{y \\in \\mathbb{R}^{n} : \\langle y, x \\rangle \\leq 0 \\text{ for all } x \\in K\\}$. Starting only from these definitions and standard optimality conditions for convex optimization problems, carry out the following tasks:\n\n1. Derive an explicit formula for $\\Pi_{C}(g)$ in terms of the coordinates of $g$.\n2. Derive an explicit formula for $\\Pi_{C^{\\circ}}(g)$ in terms of the coordinates of $g$.\n3. Using your formulas from parts 1 and 2, verify the orthogonality condition that arises in conic geometry by computing $\\langle \\Pi_{C}(g), \\Pi_{C^{\\circ}}(g) \\rangle$ explicitly.\n\nExpress your final answer as a single analytic expression containing the two projections and the inner product value in that order. No numerical rounding is required. If you introduce any auxiliary notation such as the positive or negative part of a scalar, define it clearly within your reasoning. Your final answer must not contain any equality symbol; it should be a single expression as specified above.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary definitions to derive the requested quantities. We shall proceed with the solution by addressing each of the three tasks in order.\n\nThroughout this derivation, let $g = (g_1, g_2, \\dots, g_n)^T$ be the given vector in $\\mathbb{R}^n$. The inner product of two vectors $u, v \\in \\mathbb{R}^n$ is denoted by $\\langle u, v \\rangle = \\sum_{i=1}^n u_i v_i$. The squared Euclidean norm is $\\|v\\|_2^2 = \\langle v, v \\rangle$.\n\n**Part 1: Derivation of the projection onto the nonnegative orthant $C$**\n\nThe nonnegative orthant is defined as $C := \\{x \\in \\mathbb{R}^{n} : x_i \\geq 0 \\text{ for all } i=1, \\dots, n\\}$. The projection of $g$ onto $C$, denoted $\\Pi_C(g)$, is the solution to the convex optimization problem:\n$$\n\\Pi_C(g) = \\arg\\min_{x \\in C} \\frac{1}{2}\\|x - g\\|_{2}^{2}\n$$\nThe objective function can be written as $f(x) = \\frac{1}{2} \\sum_{i=1}^{n} (x_i - g_i)^2$. The constraints are $x_i \\geq 0$ for each $i$. Both the objective function and the constraints are separable with respect to the coordinates $x_i$. This means we can solve the $n$-dimensional problem by solving $n$ independent one-dimensional problems:\n$$\n(\\Pi_C(g))_i = \\arg\\min_{x_i \\in \\mathbb{R}} \\frac{1}{2}(x_i - g_i)^2 \\quad \\text{subject to} \\quad x_i \\geq 0\n$$\nfor each $i \\in \\{1, \\dots, n\\}$. Let $x_i^*$ be the optimal value for the $i$-th subproblem. We use the Karush-Kuhn-Tucker (KKT) conditions for this constrained problem. The constraint is $h(x_i) = -x_i \\leq 0$. The Lagrangian is:\n$$\nL(x_i, \\lambda_i) = \\frac{1}{2}(x_i - g_i)^2 - \\lambda_i x_i\n$$\nwhere $\\lambda_i$ is the Lagrange multiplier. The KKT conditions for optimality are:\n1. Stationarity: $\\frac{\\partial L}{\\partial x_i} = x_i - g_i - \\lambda_i = 0 \\implies x_i^* = g_i + \\lambda_i$.\n2. Primal feasibility: $x_i^* \\geq 0$.\n3. Dual feasibility: $\\lambda_i \\geq 0$.\n4. Complementary slackness: $\\lambda_i x_i^* = 0$.\n\nFrom the complementary slackness condition, we have two cases for each $i$:\nCase A: $\\lambda_i = 0$. From the stationarity condition, $x_i^* = g_i$. For this to be a valid solution, it must satisfy primal feasibility, i.e., $x_i^* \\geq 0$, which implies $g_i \\geq 0$.\nCase B: $x_i^* = 0$. From the stationarity condition, $0 = g_i + \\lambda_i$, so $\\lambda_i = -g_i$. For this to be a valid solution, it must satisfy dual feasibility, i.e., $\\lambda_i \\geq 0$, which implies $-g_i \\geq 0$ or $g_i \\leq 0$.\n\nCombining these cases for a given $g_i$:\n- If $g_i  0$, only Case A is possible, so $x_i^* = g_i$.\n- If $g_i  0$, only Case B is possible, so $x_i^* = 0$.\n- If $g_i = 0$, both cases yield $x_i^* = 0$.\n\nThus, the optimal solution for each coordinate is $x_i^* = \\max(g_i, 0)$.\nLet us define the scalar-valued positive-part function $(a)_+ := \\max(a, 0)$ for any $a \\in \\mathbb{R}$. We can extend this to a vector-valued function on $\\mathbb{R}^n$, denoted $(g)_+$, which applies the function component-wise: $((g)_+)_i := (g_i)_+$.\nThe explicit formula for the projection is:\n$$\n\\Pi_C(g) = (g)_+ = (\\max(g_1, 0), \\max(g_2, 0), \\dots, \\max(g_n, 0))^T\n$$\n\n**Part 2: Derivation of the projection onto the polar cone $C^{\\circ}$**\n\nFirst, we must characterize the polar cone $C^{\\circ}$. By definition, $C^{\\circ} := \\{y \\in \\mathbb{R}^{n} : \\langle y, x \\rangle \\leq 0 \\text{ for all } x \\in C\\}$.\nLet $y \\in C^{\\circ}$. For any $j \\in \\{1, \\dots, n\\}$, the standard basis vector $e_j$ (with a $1$ in the $j$-th position and $0$s elsewhere) is in $C$. Therefore, we must have $\\langle y, e_j \\rangle \\leq 0$. Since $\\langle y, e_j \\rangle = y_j$, this implies $y_j \\leq 0$ for all $j=1, \\dots, n$.\nConversely, suppose $y \\in \\mathbb{R}^n$ has $y_i \\leq 0$ for all $i$. For any $x \\in C$, we have $x_i \\geq 0$ for all $i$. Then the inner product is $\\langle y, x \\rangle = \\sum_{i=1}^n y_i x_i$. Each term in the sum, $y_i x_i$, is a product of a non-positive number and a non-negative number, so $y_i x_i \\leq 0$. The sum of non-positive terms is non-positive, so $\\langle y, x \\rangle \\leq 0$. This holds for all $x \\in C$, so $y \\in C^{\\circ}$.\nTherefore, the polar cone of the nonnegative orthant is the nonpositive orthant:\n$$\nC^{\\circ} = \\{y \\in \\mathbb{R}^{n} : y_i \\leq 0 \\text{ for all } i=1, \\dots, n\\}\n$$\nThe projection of $g$ onto $C^{\\circ}$, denoted $\\Pi_{C^{\\circ}}(g)$, is the solution to the problem:\n$$\n\\Pi_{C^{\\circ}}(g) = \\arg\\min_{y \\in C^{\\circ}} \\frac{1}{2}\\|y - g\\|_{2}^{2}\n$$\nThis problem is also separable. For each $i \\in \\{1, \\dots, n\\}$, we solve:\n$$\n(\\Pi_{C^{\\circ}}(g))_i = \\arg\\min_{y_i \\in \\mathbb{R}} \\frac{1}{2}(y_i - g_i)^2 \\quad \\text{subject to} \\quad y_i \\leq 0\n$$\nLet's use KKT conditions again. The constraint is $h(y_i) = y_i \\leq 0$. The Lagrangian is $L(y_i, \\mu_i) = \\frac{1}{2}(y_i - g_i)^2 + \\mu_i y_i$, with $\\mu_i \\geq 0$.\nThe KKT conditions for optimality are:\n1. Stationarity: $\\frac{\\partial L}{\\partial y_i} = y_i - g_i + \\mu_i = 0 \\implies y_i^* = g_i - \\mu_i$.\n2. Primal feasibility: $y_i^* \\leq 0$.\n3. Dual feasibility: $\\mu_i \\geq 0$.\n4. Complementary slackness: $\\mu_i y_i^* = 0$.\n\nFrom complementary slackness:\nCase A: $\\mu_i = 0$. From stationarity, $y_i^* = g_i$. Primal feasibility requires $g_i \\leq 0$.\nCase B: $y_i^* = 0$. From stationarity, $0 = g_i - \\mu_i$, so $\\mu_i = g_i$. Dual feasibility requires $g_i \\geq 0$.\n\nCombining these cases for a given $g_i$:\n- If $g_i  0$, only Case A is possible, so $y_i^* = g_i$.\n- If $g_i  0$, only Case B is possible, so $y_i^* = 0$.\n- If $g_i = 0$, both cases yield $y_i^* = 0$.\n\nThus, the optimal solution for each coordinate is $y_i^* = \\min(g_i, 0)$.\nLet us define the scalar-valued function $(a)_{\\text{neg}} := \\min(a, 0)$ for any $a \\in \\mathbb{R}$. We extend this to a vector-valued function on $\\mathbb{R}^n$, denoted $(g)_{\\text{neg}}$, which applies the function component-wise: $((g)_{\\text{neg}})_i := (g_i)_{\\text{neg}}$.\nThe explicit formula for the projection is:\n$$\n\\Pi_{C^{\\circ}}(g) = (g)_{\\text{neg}} = (\\min(g_1, 0), \\min(g_2, 0), \\dots, \\min(g_n, 0))^T\n$$\n\n**Part 3: Verification of the Orthogonality Condition**\n\nWe are asked to compute the inner product $\\langle \\Pi_{C}(g), \\Pi_{C^{\\circ}}(g) \\rangle$. Using the formulas derived above:\n$$\n\\langle \\Pi_{C}(g), \\Pi_{C^{\\circ}}(g) \\rangle = \\sum_{i=1}^{n} (\\Pi_C(g))_i (\\Pi_{C^{\\circ}}(g))_i = \\sum_{i=1}^{n} \\max(g_i, 0) \\min(g_i, 0)\n$$\nLet's analyze the product term for each coordinate $i$:\n- If $g_i \\ge 0$, then $\\max(g_i, 0) = g_i$ and $\\min(g_i, 0) = 0$. The product is $g_i \\cdot 0 = 0$.\n- If $g_i  0$, then $\\max(g_i, 0) = 0$ and $\\min(g_i, 0) = g_i$. The product is $0 \\cdot g_i = 0$.\n\nIn every case, the product $\\max(g_i, 0) \\min(g_i, 0)$ is identically zero for any $g_i \\in \\mathbb{R}$.\nTherefore, the inner product is a sum of zeros:\n$$\n\\langle \\Pi_{C}(g), \\Pi_{C^{\\circ}}(g) \\rangle = \\sum_{i=1}^{n} 0 = 0\n$$\nThis explicitly verifies the orthogonality condition, which is a key result in conic geometry and a special case of Moreau's decomposition theorem, which states that for any closed convex cone $K$, any vector $g$ can be uniquely decomposed as $g = \\Pi_K(g) + \\Pi_{K^{\\circ}}(g)$, where $\\langle \\Pi_K(g), \\Pi_{K^{\\circ}}(g) \\rangle = 0$. In our case, $g_i = \\max(g_i, 0) + \\min(g_i, 0)$, and we have verified the orthogonality.\n\nThe three requested quantities are $\\Pi_C(g) = (g)_+$, $\\Pi_{C^{\\circ}}(g) = (g)_{\\text{neg}}$, and $\\langle \\Pi_C(g), \\Pi_{C^{\\circ}}(g) \\rangle = 0$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n(g)_+  (g)_{\\text{neg}}  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "With the geometric fundamentals in place, we now introduce the central concept of statistical dimension, which quantifies the \"size\" of a cone in a probabilistic sense. Rather than using volume, this metric measures the expected portion of a random vector's squared norm that is captured by the cone upon projection. This exercise guides you through the first calculation of a statistical dimension for the familiar non-negative orthant , revealing an elegant and intuitive result that connects high-dimensional geometry to the properties of Gaussian distributions.",
            "id": "3481891",
            "problem": "Let $\\mathbb{R}_{+}^{n} \\subset \\mathbb{R}^{n}$ denote the nonnegative orthant. The statistical dimension $\\delta(C)$ of a closed convex cone $C \\subset \\mathbb{R}^{n}$ is defined as the expected squared Euclidean norm of the Euclidean projection of a standard normal vector onto $C$, namely $\\delta(C) = \\mathbb{E}\\!\\left[\\|\\Pi_{C}(\\boldsymbol{g})\\|_2^{2}\\right]$, where $\\boldsymbol{g} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I}_{n})$ is a random vector with independent standard normal entries and $\\Pi_{C}$ is the Euclidean projection onto $C$. Starting from this definition and no other specialized formulas, derive a closed-form expression in terms of $n$ for the statistical dimension $\\delta(\\mathbb{R}_{+}^{n})$. In addition, explain how the dependence of $\\delta(\\mathbb{R}_{+}^{n})$ on $n$ arises from the distribution of signs of a Gaussian vector. Express your final answer as a single closed-form expression in $n$. No rounding is required.",
            "solution": "The problem is valid. It is a well-posed mathematical question grounded in the established theories of conic geometry and probability. All terms are defined, and the premises are self-consistent and scientifically sound.\n\nThe statistical dimension $\\delta(C)$ of a closed convex cone $C \\subset \\mathbb{R}^{n}$ is defined as $\\delta(C) = \\mathbb{E}\\!\\left[\\|\\Pi_{C}(\\boldsymbol{g})\\|_2^{2}\\right]$, where $\\boldsymbol{g} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I}_{n})$ is a standard normal vector in $\\mathbb{R}^{n}$, and $\\Pi_{C}$ is the Euclidean projection onto $C$. We are tasked with finding the statistical dimension of the nonnegative orthant, $C = \\mathbb{R}_{+}^{n}$.\n\nLet $\\boldsymbol{g} = (g_1, g_2, \\dots, g_n)^T$, where each component $g_i$ is an independent and identically distributed (i.i.d.) random variable following the standard normal distribution, $g_i \\sim \\mathcal{N}(0, 1)$.\n\nThe nonnegative orthant is defined as $\\mathbb{R}_{+}^{n} = \\{ \\boldsymbol{x} \\in \\mathbb{R}^n \\mid x_i \\ge 0 \\text{ for } i=1, \\dots, n \\}$. Since this set is a Cartesian product of $n$ identical closed convex intervals $[0, \\infty)$, the Euclidean projection of a vector $\\boldsymbol{g}$ onto $\\mathbb{R}_{+}^{n}$ can be computed component-wise.\nLet $\\boldsymbol{p} = \\Pi_{\\mathbb{R}_{+}^{n}}(\\boldsymbol{g})$. Then the $i$-th component of $\\boldsymbol{p}$ is given by the projection of $g_i$ onto the interval $[0, \\infty)$.\nThis projection is:\n$$\np_i = \\Pi_{[0, \\infty)}(g_i) = \\begin{cases} g_i  \\text{if } g_i \\ge 0 \\\\ 0  \\text{if } g_i  0 \\end{cases}\n$$\nThis can be expressed compactly as $p_i = \\max(0, g_i)$.\n\nThe squared Euclidean norm of the projected vector is $\\|\\Pi_{\\mathbb{R}_{+}^{n}}(\\boldsymbol{g})\\|_2^2 = \\|\\boldsymbol{p}\\|_2^2 = \\sum_{i=1}^{n} p_i^2$.\nSubstituting the expression for $p_i$:\n$$\n\\|\\Pi_{\\mathbb{R}_{+}^{n}}(\\boldsymbol{g})\\|_2^2 = \\sum_{i=1}^{n} (\\max(0, g_i))^2\n$$\nWe can rewrite $(\\max(0, g_i))^2$ using the indicator function $\\mathbb{I}(\\cdot)$. The term is $g_i^2$ if $g_i  0$ and $0$ otherwise. Since the probability of $g_i=0$ is zero for a continuous distribution, we can neglect this point.\n$$\n\\|\\Pi_{\\mathbb{R}_{+}^{n}}(\\boldsymbol{g})\\|_2^2 = \\sum_{i=1}^{n} g_i^2 \\mathbb{I}(g_i  0)\n$$\nNow, we compute the expectation as required by the definition of the statistical dimension:\n$$\n\\delta(\\mathbb{R}_{+}^{n}) = \\mathbb{E}\\left[ \\sum_{i=1}^{n} g_i^2 \\mathbb{I}(g_i  0) \\right]\n$$\nBy linearity of expectation, we can move the expectation inside the sum:\n$$\n\\delta(\\mathbb{R}_{+}^{n}) = \\sum_{i=1}^{n} \\mathbb{E}\\left[ g_i^2 \\mathbb{I}(g_i  0) \\right]\n$$\nSince all components $g_i$ are i.i.d., the expectation term $\\mathbb{E}\\left[ g_i^2 \\mathbb{I}(g_i  0) \\right]$ is the same for all $i=1, \\dots, n$. Let's compute this value for a generic standard normal random variable $g \\sim \\mathcal{N}(0, 1)$ with probability density function (PDF) $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{x^2}{2})$.\n$$\n\\mathbb{E}\\left[ g^2 \\mathbb{I}(g  0) \\right] = \\int_{-\\infty}^{\\infty} x^2 \\mathbb{I}(x  0) \\phi(x) dx = \\int_{0}^{\\infty} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx\n$$\nTo evaluate this integral, we recall the variance of a standard normal variable, which is $1$. The variance is also equal to the second moment since the mean is $0$:\n$$\n\\text{Var}(g) = \\mathbb{E}[g^2] - (\\mathbb{E}[g])^2 = \\mathbb{E}[g^2] - 0^2 = \\mathbb{E}[g^2] = 1\n$$\nThe second moment is given by the integral over the entire real line:\n$$\n\\mathbb{E}[g^2] = \\int_{-\\infty}^{\\infty} x^2 \\phi(x) dx = \\int_{-\\infty}^{\\infty} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx = 1\n$$\nThe integrand $f(x) = x^2 \\phi(x)$ is an even function, since $f(-x) = (-x)^2 \\phi(-x) = x^2 \\phi(x) = f(x)$. For any even function, the integral from $0$ to $\\infty$ is exactly half the integral from $-\\infty$ to $\\infty$.\n$$\n\\int_{0}^{\\infty} x^2 \\phi(x) dx = \\frac{1}{2} \\int_{-\\infty}^{\\infty} x^2 \\phi(x) dx = \\frac{1}{2} \\mathbb{E}[g^2] = \\frac{1}{2} \\cdot 1 = \\frac{1}{2}\n$$\nThus, we have found that $\\mathbb{E}\\left[ g_i^2 \\mathbb{I}(g_i  0) \\right] = \\frac{1}{2}$ for each $i$.\nSubstituting this back into the sum for $\\delta(\\mathbb{R}_{+}^{n})$:\n$$\n\\delta(\\mathbb{R}_{+}^{n}) = \\sum_{i=1}^{n} \\frac{1}{2} = \\frac{n}{2}\n$$\nThe dependence of $\\delta(\\mathbb{R}_{+}^{n})$ on $n$ arises from the distribution of signs of the Gaussian vector $\\boldsymbol{g}$. The total expected squared norm of $\\boldsymbol{g}$ is $\\mathbb{E}[\\|\\boldsymbol{g}\\|_2^2] = \\mathbb{E}[\\sum_{i=1}^n g_i^2] = \\sum_{i=1}^n \\mathbb{E}[g_i^2] = \\sum_{i=1}^n 1 = n$. This total \"expected energy\" can be decomposed based on the signs of the components $g_i$.\n$$\n\\mathbb{E}[\\|\\boldsymbol{g}\\|_2^2] = \\mathbb{E}\\left[\\sum_{i=1}^n g_i^2\\right] = \\mathbb{E}\\left[\\sum_{i=1}^n g_i^2 (\\mathbb{I}(g_i  0) + \\mathbb{I}(g_i \\le 0))\\right] = \\mathbb{E}\\left[\\sum_{i=1}^n g_i^2 \\mathbb{I}(g_i  0)\\right] + \\mathbb{E}\\left[\\sum_{i=1}^n g_i^2 \\mathbb{I}(g_i \\le 0)\\right]\n$$\nThe first term is precisely $\\delta(\\mathbb{R}_{+}^{n})$. For the second term, we consider the expectation $\\mathbb{E}[g_i^2 \\mathbb{I}(g_i \\le 0)]$. Due to the symmetry of the standard normal distribution, the random variable $-g_i$ has the same distribution as $g_i$. Therefore, $\\mathbb{E}[g_i^2 \\mathbb{I}(g_i \\le 0)] = \\mathbb{E}[(-g_i)^2 \\mathbb{I}(-g_i \\ge 0)]$. Let $h_i = -g_i$, then this is $\\mathbb{E}[h_i^2 \\mathbb{I}(h_i \\ge 0)]$. Since $h_i$ follows the same $\\mathcal{N}(0,1)$ distribution as $g_i$, and since $P(g_i=0)=0$, we have $\\mathbb{E}[g_i^2 \\mathbb{I}(g_i \\le 0)] = \\mathbb{E}[g_i^2 \\mathbb{I}(g_i  0)] = \\frac{1}{2}$.\nThis implies that the total expected energy $n$ is split equally between the components corresponding to positive signs and those corresponding to non-positive signs. The projection $\\Pi_{\\mathbb{R}_{+}^{n}}$ preserves the former and annihilates the latter. The sign of each component $g_i$ is positive with probability $\\frac{1}{2}$ and negative with probability $\\frac{1}{2}$. Thus, on average, half of the components are projected to non-zero values. This statistical equipartition of energy based on sign, a direct consequence of the symmetry of the Gaussian distribution, is why the statistical dimension is exactly half of the ambient dimension $n$.\n$$\nn = \\delta(\\mathbb{R}_{+}^{n}) + \\sum_{i=1}^{n} \\frac{1}{2} = \\delta(\\mathbb{R}_{+}^{n}) + \\frac{n}{2} \\implies \\delta(\\mathbb{R}_{+}^{n}) = \\frac{n}{2}\n$$",
            "answer": "$$\\boxed{\\frac{n}{2}}$$"
        },
        {
            "introduction": "This final practice demonstrates the true predictive power of the conic geometry framework, bridging theory with practical application. The abstract concept of statistical dimension becomes a powerful tool for analyzing the performance of sparse recovery algorithms like the LASSO or Basis Pursuit. You will implement a numerical procedure to calculate the statistical dimension of the $\\ell_1$ norm's descent cone and use it to predict the algorithm's sensitivity to measurement noise, thereby connecting the geometry of the problem directly to a critical engineering performance metric .",
            "id": "3481924",
            "problem": "Consider the feasibility set defined by the intersection of a linear model and an $\\ell_1$ ball, namely $\\{x \\in \\mathbb{R}^n : A x = y, \\|x\\|_1 \\le \\tau\\}$. Let $x_\\star \\in \\mathbb{R}^n$ be a point on the boundary of the $\\ell_1$ ball with $\\|x_\\star\\|_1 = \\tau$ and support $S \\subset \\{1,\\dots,n\\}$ of cardinality $k = |S|$, and assume the measurement matrix $A \\in \\mathbb{R}^{m \\times n}$ has independent rows drawn from the standard normal distribution $\\mathcal{N}(0, I_n)$. The conic geometry of the feasibility set near $x_\\star$ is governed by the descent cone of the $\\ell_1$ norm at $x_\\star$, and recovery properties in compressed sensing are controlled by the statistical dimension and conic restricted singular values associated with this cone.\n\nStarting from the following fundamental definitions:\n\n- The descent cone of a proper convex function $f$ at a point $x$ is $D(f, x) := \\{h \\in \\mathbb{R}^n : \\exists t  0 \\text{ with } f(x + t h) \\le f(x)\\}$.\n\n- The subdifferential of the $\\ell_1$ norm at $x$ is $\\partial \\|\\cdot\\|_1(x)$, given by the set of $u \\in \\mathbb{R}^n$ such that $u_i = \\operatorname{sign}(x_i)$ for $i \\in \\operatorname{supp}(x)$ and $|u_j| \\le 1$ for $j \\notin \\operatorname{supp}(x)$.\n\n- The statistical dimension of a closed convex cone $C$ is $\\delta(C) := \\mathbb{E}\\left[\\|\\Pi_C(g)\\|_2^2\\right]$, where $g \\sim \\mathcal{N}(0, I_n)$ and $\\Pi_C$ denotes the Euclidean projection onto $C$.\n\n- The conic restricted minimal singular value of $A$ with respect to a cone $C$ is $\\sigma_{\\min}(A; C) := \\inf\\{\\|A h\\|_2 : h \\in C \\cap \\mathbb{S}^{n-1}\\}$, where $\\mathbb{S}^{n-1} := \\{h \\in \\mathbb{R}^n : \\|h\\|_2 = 1\\}$.\n\n- The conic condition number is $\\kappa(A; C) := 1 / \\sigma_{\\min}(A; C)$, with the convention $\\kappa(A; C) = +\\infty$ if $\\sigma_{\\min}(A; C) = 0$.\n\nUse these bases to derive, implement, and numerically evaluate the following quantities for the descent cone of the $\\ell_1$ norm at a $k$-sparse point $x_\\star$ in $\\mathbb{R}^n$:\n\n1. A computation of the statistical dimension $\\delta$ of the descent cone $D(\\|\\cdot\\|_1, x_\\star)$ in terms of $(n, k)$ via an expectation that depends on a scalar parameter, minimized over $t \\ge 0$. The derivation should start from the characterization of the subdifferential and the definition of statistical dimension and reduce the expectation to an integral with respect to the standard normal distribution, which is then minimized over $t$.\n\n2. An approximation of the conic restricted minimal singular value $\\sigma_{\\min}(A; D)$ using the Gaussian comparison principle, relating it to $\\sqrt{m}$ and the Gaussian width of $D \\cap \\mathbb{S}^{n-1}$. Use the relationship between Gaussian width and statistical dimension to produce a first-order estimate $\\sigma_{\\min} \\approx \\max\\{\\sqrt{m} - \\sqrt{\\delta}, 0\\}$.\n\n3. A bound on noise amplification in the feasibility problem near the threshold regime $m \\approx \\delta$, where a measurement perturbation $e$ with energy $\\varepsilon = \\|e\\|_2$ yields an error $\\|\\Delta x\\|_2 \\gtrsim \\varepsilon / \\sigma_{\\min}$. Define the predicted amplification factor $\\alpha := \\varepsilon / \\sigma_{\\min}$ with the convention $\\alpha = +\\infty$ if $\\sigma_{\\min} = 0$.\n\nYou must implement a complete, runnable program that carries out the numerical evaluation of $\\delta$ (by minimizing the expectation over $t \\ge 0$) and then computes $\\sigma_{\\min}$ and $\\alpha$ for the test suite specified below. Your program should not rely on randomness and must use numerically stable computations of the expectations and their integrals.\n\nTest Suite:\n- Case 1 (general regime above threshold): $(n, k, m, \\varepsilon) = (500, 25, 220, 0.01)$.\n- Case 2 (near-threshold): $(n, k, m, \\varepsilon) = (500, 25, 180, 0.01)$.\n- Case 3 (moderate dimension, small noise): $(n, k, m, \\varepsilon) = (50, 5, 40, 0.001)$.\n- Case 4 (extreme sparsity): $(n, k, m, \\varepsilon) = (1000, 1, 30, 0.05)$.\n\nYour program should produce a single line of output containing the predicted amplification factors for the test cases as a comma-separated list enclosed in square brackets (for example, \"[result1,result2,result3,result4]\"). All outputs must be floats, using the representation \"+inf\" if the amplification is unbounded. No physical units or angle units are involved in this problem; numerical outputs are dimensionless floats.",
            "solution": "The problem is valid. It is a well-posed and scientifically grounded problem in the field of compressed sensing and high-dimensional statistics, based on established principles of conic geometry. We will proceed with the derivation and solution.\n\nThe solution requires a three-step analytical derivation followed by numerical implementation. First, we compute the statistical dimension $\\delta$ of the descent cone. Second, we approximate the conic restricted minimal singular value $\\sigma_{\\min}$. Third, we compute the noise amplification factor $\\alpha$.\n\n**Step 1: Characterization of the Descent Cone**\n\nThe problem concerns the descent cone of the $\\ell_1$ norm, $f(x) = \\|x\\|_1$, at a $k$-sparse point $x_\\star \\in \\mathbb{R}^n$ with support $S$ of size $k$. The descent cone $D(f, x_\\star)$ is the set of directions $h \\in \\mathbb{R}^n$ such that the directional derivative $f'(x_\\star; h)$ is non-positive. The directional derivative of the $\\ell_1$ norm is given by:\n$$\nf'(x_\\star; h) = \\lim_{t \\downarrow 0} \\frac{\\|x_\\star + th\\|_1 - \\|x_\\star\\|_1}{t} = \\sum_{i \\in S} \\operatorname{sign}((x_\\star)_i) h_i + \\sum_{j \\notin S} |h_j|\n$$\nLet $s \\in \\mathbb{R}^n$ be the sign vector of $x_\\star$ on its support $S$, i.e., $s_S = \\operatorname{sign}((x_\\star)_S)$ and $s_{S^c} = 0$. The directional derivative can be expressed as $\\langle s, h \\rangle + \\|h_{S^c}\\|_1$. The descent cone, which we denote as $C$, is therefore:\n$$\nC := D(\\|\\cdot\\|_1, x_\\star) = \\{ h \\in \\mathbb{R}^n : \\langle s_S, h_S \\rangle + \\|h_{S^c}\\|_1 \\le 0 \\}\n$$\nNote that since $s_{S^c}=0$, $\\langle s, h \\rangle = \\langle s_S, h_S \\rangle$.\n\n**Step 2: Derivation of the Statistical Dimension $\\delta(C)$**\n\nThe statistical dimension of a closed convex cone $C$ is defined as $\\delta(C) = \\mathbb{E}_g[\\|\\Pi_C(g)\\|_2^2]$ for a standard Gaussian vector $g \\sim \\mathcal{N}(0, I_n)$. By Moreau's decomposition theorem, any vector $g$ can be uniquely decomposed as $g = \\Pi_C(g) + \\Pi_{C^\\circ}(g)$, where $\\Pi_C(g)$ and $\\Pi_{C^\\circ}(g)$ are orthogonal. Therefore, $\\|\\Pi_C(g)\\|_2^2 = \\|g - \\Pi_{C^\\circ}(g)\\|_2^2 = \\operatorname{dist}(g, C^\\circ)^2$. The statistical dimension is thus given by the expected squared Euclidean distance from a standard Gaussian vector to the polar cone $C^\\circ$:\n$$\n\\delta(C) = \\mathbb{E}_g[\\operatorname{dist}(g, C^\\circ)^2]\n$$\nThe polar cone $C^\\circ$ is defined as $C^\\circ = \\{ v \\in \\mathbb{R}^n : \\langle v, h \\rangle \\le 0 \\text{ for all } h \\in C \\}$. For the descent cone $C$ defined above, its polar is the conic hull of the set $\\{ (s_S, v_{S^c}) : \\|v_{S^c}\\|_\\infty \\le 1 \\}$. This can be written as:\n$$\nC^\\circ = \\{ v \\in \\mathbb{R}^n : v_S = t s_S, \\|v_{S^c}\\|_\\infty \\le t \\text{ for some } t \\ge 0 \\}\n$$\nThe squared distance from $g$ to $C^\\circ$ is found by solving a minimization problem:\n$$\n\\operatorname{dist}(g, C^\\circ)^2 = \\min_{v \\in C^\\circ} \\|g - v\\|_2^2 = \\min_{t \\ge 0, \\|w\\|_\\infty \\le 1} \\|g_S - t s_S\\|_2^2 + \\|g_{S^c} - t w\\|_2^2\n$$\nFor a fixed $t \\ge 0$, the minimization over $w$ in the second term is equivalent to finding the projection of $g_{S^c}$ onto the $\\ell_\\infty$-ball of radius $t$, which is $\\operatorname{dist}(g_{S^c}, t B_\\infty^{n-k})^2$. This distance is $\\sum_{j \\in S^c} (\\max(|g_j| - t, 0))^2$. The squared distance to $C^\\circ$ is then:\n$$\n\\operatorname{dist}(g, C^\\circ)^2 = \\min_{t \\ge 0} \\left( \\|g_S - t s_S\\|_2^2 + \\sum_{j \\in S^c} (\\max(|g_j| - t, 0))^2 \\right)\n$$\nA key result in this theory states that a minimum-of-expectations is equal to the expectation-of-the-minimum in this specific setting. This allows us to interchange the expectation and minimization operators:\n$$\n\\delta(C) = \\min_{t \\ge 0} \\mathbb{E}_g\\left[ \\|g_S - t s_S\\|_2^2 + \\sum_{j \\in S^c} (\\max(|g_j| - t, 0))^2 \\right]\n$$\nLet's define the function $\\psi(t)$ as the expectation inside the minimum. We evaluate the expectation term by term.\nThe first term is:\n$$\n\\mathbb{E}[\\|g_S - t s_S\\|_2^2] = \\mathbb{E}[\\|g_S\\|_2^2] - 2t\\mathbb{E}[\\langle g_S, s_S \\rangle] + t^2\\|s_S\\|_2^2 = k - 0 + t^2 k = k(1+t^2)\n$$\nThe expectation of the second term is $(n-k)$ times the expectation for a single standard normal variable $z \\sim \\mathcal{N}(0, 1)$:\n$$\n(n-k) \\mathbb{E}_z [(\\max(|z| - t, 0))^2]\n$$\nThis expectation can be computed via integration. Let $\\phi(z)$ and $\\Phi(z)$ be the PDF and CDF of the standard normal distribution, respectively.\n\\begin{align*}\n\\mathbb{E}_z [(\\max(|z| - t, 0))^2] = \\int_{-\\infty}^{\\infty} (\\max(|z|-t,0))^2 \\phi(z) dz \\\\\n= 2 \\int_t^{\\infty} (z-t)^2 \\phi(z) dz \\\\\n= 2 \\left[ \\int_t^{\\infty} z^2\\phi(z)dz - 2t\\int_t^{\\infty} z\\phi(z)dz + t^2\\int_t^{\\infty} \\phi(z)dz \\right] \\\\\n= 2 \\left[ (t\\phi(t) + 1-\\Phi(t)) - 2t\\phi(t) + t^2(1-\\Phi(t)) \\right] \\\\\n= (1+t^2) \\cdot 2(1-\\Phi(t)) - 2t\\phi(t)\n\\end{align*}\nCombining these results, the function to be minimized is:\n$$\n\\psi(t) = k(1+t^2) + (n-k) \\left[ (1+t^2) \\cdot 2(1-\\Phi(t)) - 2t\\phi(t) \\right]\n$$\nThe statistical dimension is then $\\delta = \\min_{t \\ge 0} \\psi(t)$. This minimization must be performed numerically.\n\n**Step 3: Conic Minimal Singular Value and Noise Amplification**\n\nThe problem provides an approximation for the conic restricted minimal singular value $\\sigma_{\\min}(A; C)$ based on the statistical dimension $\\delta$. This approximation arises from the concentration of measure phenomena for Gaussian matrices and Gordon's comparison inequality.\n$$\n\\sigma_{\\min} \\approx \\max\\{\\sqrt{m} - \\sqrt{\\delta}, 0\\}\n$$\nThis formula captures the phase transition phenomenon in compressed sensing: if the number of measurements $m$ is less than the statistical dimension $\\delta$, the matrix $A$ is likely to have a non-trivial null space when restricted to the cone $C$, leading to $\\sigma_{\\min} = 0$.\n\nThe predicted noise amplification factor, $\\alpha$, quantifies the worst-case error amplification for a measurement perturbation of energy $\\varepsilon = \\|e\\|_2$. It is defined as:\n$$\n\\alpha := \\frac{\\varepsilon}{\\sigma_{\\min}}\n$$\nIf $\\sigma_{\\min} = 0$, the amplification is unbounded, so $\\alpha = +\\infty$.\n\nThe numerical implementation will consist of minimizing $\\psi(t)$ to find $\\delta$, then substituting $\\delta$ into the expressions for $\\sigma_{\\min}$ and $\\alpha$ for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize_scalar\n\ndef calculate_psi(t, n, k):\n    \"\"\"\n    Calculates the function psi(t) whose minimum over t=0 is the \n    statistical dimension delta.\n    \"\"\"\n    if t  0:\n        return np.inf\n\n    # Standard normal PDF phi(t) and CDF Phi(t)\n    phi_t = norm.pdf(t)\n    Phi_t = norm.cdf(t)\n    \n    # E_t = E[(max(|z|-t,0))^2] for z ~ N(0,1)\n    # This is calculated using the pre-derived formula:\n    # (1+t^2) * 2*(1-Phi(t)) - 2*t*phi(t)\n    E_t = (1 + t**2) * 2 * (1 - Phi_t) - 2 * t * phi_t\n    \n    # Full expression for psi(t)\n    psi_val = k * (1 + t**2) + (n - k) * E_t\n    return psi_val\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, k, m, epsilon)\n        (500, 25, 220, 0.01),\n        (500, 25, 180, 0.01),\n        (50, 5, 40, 0.001),\n        (1000, 1, 30, 0.05),\n    ]\n\n    results = []\n    for n, k, m, eps in test_cases:\n        # Create a lambda function for the specific (n, k) to pass to the optimizer.\n        objective_func = lambda t: calculate_psi(t, n, k)\n        \n        # Numerically minimize psi(t) for t = 0 to find the statistical dimension.\n        # The minimum of psi(t) is convex. A search bound of [0, 50] is very safe.\n        opt_result = minimize_scalar(objective_func, bounds=(0, 50), method='bounded')\n        delta = opt_result.fun\n        \n        # Approximate the conic restricted minimal singular value, sigma_min.\n        # This is based on the provided formula relating it to m and delta.\n        sqrt_m = np.sqrt(m)\n        sqrt_delta = np.sqrt(delta)\n        sigma_min = max(0.0, sqrt_m - sqrt_delta)\n        \n        # Calculate the noise amplification factor, alpha.\n        # A small tolerance is used to handle floating point inaccuracies near zero.\n        if sigma_min  1e-12:\n            alpha = eps / sigma_min\n        else:\n            alpha = float('inf')\n        \n        results.append(alpha)\n\n    # Format the final output string as per the problem specification.\n    # Infinity should be represented as '+inf'.\n    formatted_results = []\n    for r in results:\n        if r == float('inf'):\n            formatted_results.append('+inf')\n        else:\n            formatted_results.append(str(r))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}