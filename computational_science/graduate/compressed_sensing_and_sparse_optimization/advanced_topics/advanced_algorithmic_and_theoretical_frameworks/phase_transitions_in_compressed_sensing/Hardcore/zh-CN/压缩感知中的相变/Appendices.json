{
    "hands_on_practices": [
        {
            "introduction": "理解压缩感知中的相变现象，关键在于从几何视角入手。$\\ell_1$最小化恢复成功的条件，本质上可以描述为测量矩阵的零空间与稀疏信号点的下降锥之间的关系。这个练习将引导你推导$\\ell_1$范数在稀疏点处的两个核心几何对象：次微分和下降锥。精确刻画这些集合是后续定量分析相变边界的第一步，也是深入理解稀疏恢复理论的基石。",
            "id": "3466252",
            "problem": "考虑由 $f(x)=\\|x\\|_{1}=\\sum_{i=1}^{n}|x_{i}|$ 定义的凸函数 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$。设 $x_{0}\\in\\mathbb{R}^{n}$ 是一个 k-稀疏向量，其支撑集为 $S\\subseteq\\{1,\\dots,n\\}$，且 $|S|=k$。定义符号向量 $s\\in\\mathbb{R}^{n}$，对于 $i\\in S$，其分量为 $s_{i}=\\operatorname{sign}(x_{0,i})$；对于 $j\\in S^{c}$ 且 $x_{0,j}=0$ 的情况，分量 $s_{j}$ 未定义。用 $d\\in\\mathbb{R}^{n}$ 表示一个方向，用 $d_{S}$ 和 $d_{S^{c}}$ 分别表示 $d$ 在 $S$ 及其补集 $S^{c}$ 上的限制，对任何向量 $g\\in\\mathbb{R}^{n}$ 也作类似表示。一个真·下半连续凸函数 $f$ 在点 $x$ 处的次微分定义为\n$$\n\\partial f(x)=\\left\\{g\\in\\mathbb{R}^{n}: f(y)\\geq f(x)+\\langle g,y-x\\rangle\\ \\text{for all}\\ y\\in\\mathbb{R}^{n}\\right\\}.\n$$\n函数 $f$ 在点 $x$ 处的下降锥定义为\n$$\nD(f,x)=\\left\\{d\\in\\mathbb{R}^{n}:\\exists\\,t>0\\ \\text{such that}\\ f(x+td)\\leq f(x)\\right\\}.\n$$\n从这些定义以及绝对值和凸性的基本性质出发，推导次微分 $\\partial\\|x\\|_{1}(x_{0})$ 关于支撑集 $S$ 和符号 $s_{i}$ 的完整刻画，然后将下降锥 $D(\\|\\cdot\\|_{1},x_{0})$ 刻画为满足一个用 $s_{S}$、$d_{S}$ 和 $\\|d_{S^{c}}\\|_{1}$ 写出的显式不等式的方向 $d$ 的锥包。将你的最终结果表示为单个闭式解析表达式，以紧凑形式汇总这两种刻画。不需要数值近似，也不涉及物理单位。你的最终答案必须是单个表达式。",
            "solution": "我们从凸函数的次微分和下降锥的基本定义出发，并将其应用于 $\\ell_{1}$ 范数，利用由稀疏向量 $x_{0}$ 的支撑集 $S$ 及其符号模式所诱导的结构。\n\n次微分的刻画。函数 $f(x)=\\|x\\|_{1}$ 是坐标可分的：$f(x)=\\sum_{i=1}^{n}|x_{i}|$。对于单坐标绝对值，在点 $a\\in\\mathbb{R}$ 处的次微分是众所周知的，并且可以从凸次微分的定义中得出：\n- 如果 $a\\neq 0$，则 $\\partial|a|=\\{\\operatorname{sign}(a)\\}$。\n- 如果 $a=0$，则 $\\partial|a|=[-1,1]$。\n\n这可以直接推导。对于 $a\\neq 0$ 的情况，由凸性和可微性可得 $\\partial|a|=\\{\\operatorname{sign}(a)\\}$，因为对所有 $y\\in\\mathbb{R}$，有 $|y|\\geq|a|+\\operatorname{sign}(a)(y-a)$，这是由 $|\\cdot|$ 在非零点处的支撑超平面性质得出的。对于 $a=0$ 的情况，次微分是区间 $[-1,1]$，因为对于任何 $g\\in[-1,1]$ 和任何 $y\\in\\mathbb{R}$，我们有 $|y|\\geq g\\,y$（实际上，$\\sup_{g\\in[-1,1]}g\\,y=|y|$，所以该区间中的每个 $g$ 都满足在 $0$ 处的次梯度不等式）。反之，如果 $g\\notin[-1,1]$，可以通过选择一个与 $g$ 符号相同且数值足够大的 $y$ 来违反不等式 $|y|\\geq g\\,y$，因此 $g\\notin\\partial|0|$。\n\n由于 $f$ 是绝对值的和，其在 $x_{0}$ 处的次微分是逐坐标次微分的笛卡尔积，这得出\n$$\n\\partial\\|x\\|_{1}(x_{0})=\\left\\{g\\in\\mathbb{R}^{n}: g_{i}=\\operatorname{sign}(x_{0,i})\\ \\text{for all}\\ i\\in S,\\ \\text{and}\\ g_{j}\\in[-1,1]\\ \\text{for all}\\ j\\in S^{c}\\right\\}.\n$$\n等价地，记 $s_{S}\\in\\mathbb{R}^{S}$ 为 $S$ 上的符号向量，这可以紧凑地表示为\n$$\n\\partial\\|x\\|_{1}(x_{0})=\\left\\{g\\in\\mathbb{R}^{n}: g_{S}=s_{S},\\ \\|g_{S^{c}}\\|_{\\infty}\\leq 1\\right\\}.\n$$\n为了验证必要性，假设 $g\\in\\partial\\|x\\|_{1}(x_{0})$。考虑任意 $i\\in S$。固定 $y=x_{0}+t\\,e_{i}$，其中 $e_{i}$ 是第 $i$ 个坐标向量，且 $t$ 与 $x_{0,i}$ 符号相同。次梯度不等式给出\n$$\n\\|x_{0}+t\\,e_{i}\\|_{1}\\geq\\|x_{0}\\|_{1}+g_{i}\\,t.\n$$\n但是，对于足够小的、不会使第 $i$ 个坐标穿过零的 $t$，有 $\\|x_{0}+t\\,e_{i}\\|_{1}=\\|x_{0}\\|_{1}+|x_{0,i}+t|-|x_{0,i}|=\\|x_{0}\\|_{1}+t\\,\\operatorname{sign}(x_{0,i})$。因此，对于任意小的同符号非零 $t$，有 $t\\,\\operatorname{sign}(x_{0,i})\\geq g_{i}\\,t$，这意味着 $g_{i}=\\operatorname{sign}(x_{0,i})$。对于 $j\\in S^{c}$，设 $y=t\\,e_{j}$。那么次梯度不等式得出 $|t|\\geq g_{j}\\,t$，对所有 $t\\in\\mathbb{R}$ 成立，这意味着 $|g_{j}|\\leq 1$。因此，该刻画是充分且必要的。\n\n下降锥的刻画。根据定义，\n$$\nD(\\|\\cdot\\|_{1},x_{0})=\\left\\{d\\in\\mathbb{R}^{n}:\\exists\\,t>0\\ \\text{such that}\\ \\|x_{0}+t\\,d\\|_{1}\\leq\\|x_{0}\\|_{1}\\right\\}.\n$$\n我们通过根据 $S$ 和 $S^{c}$ 分割索引来分析 $\\|x_{0}+t\\,d\\|_{1}$。写出\n$$\n\\|x_{0}+t\\,d\\|_{1}=\\sum_{i\\in S}|x_{0,i}+t\\,d_{i}|+\\sum_{j\\in S^{c}}|t\\,d_{j}|.\n$$\n对于 $i\\in S$ 且 $x_{0,i}\\neq 0$ 的情况，$|\\cdot|$ 的凸性和在 $x_{0,i}$ 处的支撑超平面不等式意味着\n$$\n|x_{0,i}+t\\,d_{i}|\\geq |x_{0,i}|+t\\,\\operatorname{sign}(x_{0,i})\\,d_{i}.\n$$\n对于 $j\\in S^{c}$，我们有精确的恒等式 $|t\\,d_{j}|=t\\,|d_{j}|$。对各坐标求和，我们得到\n$$\n\\|x_{0}+t\\,d\\|_{1}\\geq \\|x_{0}\\|_{1}+t\\left(s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\right).\n$$\n因此，只有当线性项满足 $s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\leq 0$ 时，不等式 $\\|x_{0}+t\\,d\\|_{1}\\leq\\|x_{0}\\|_{1}$ 才可能对某个 $t>0$ 成立。反之，如果 $s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}0$，那么对于足够小的 $t>0$，右侧严格小于 $\\|x_{0}\\|_{1}$，从而保证了 $\\|x_{0}+t\\,d\\|_{1}\\leq\\|x_{0}\\|_{1}$。如果 $s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}=0$，该不等式在一阶上成立，并且根据连续性和凸性，可以选择一个序列 $t\\downarrow 0$ 使得 $\\|x_{0}+t\\,d\\|_{1}\\leq\\|x_{0}\\|_{1}$。因此，充分必要条件是\n$$\ns_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\leq 0.\n$$\n这个方向集合是一个锥，因为对于任何 $\\alpha>0$，\n$$\ns_{S}^{\\top}(\\alpha d_{S})+\\|\\alpha d_{S^{c}}\\|_{1}=\\alpha\\left(s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\right)\\leq 0,\n$$\n所以 $\\alpha d$ 也满足该不等式。因此，\n$$\nD(\\|\\cdot\\|_{1},x_{0})=\\left\\{d\\in\\mathbb{R}^{n}: s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\leq 0\\right\\}=\\operatorname{cone}\\left(\\left\\{d\\in\\mathbb{R}^{n}: s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\leq 0\\right\\}\\right).\n$$\n\n汇总这两种刻画，我们已经用 $S$、$s_{S}$ 以及 $S^{c}$ 上的 $\\|\\cdot\\|_{\\infty}$ 和 $\\|\\cdot\\|_{1}$ 范数，以闭式形式表示了在 $x_{0}$ 处的次微分和下降锥。这些刻画在分析压缩感知中的相变时至关重要，因为 $D(\\|\\cdot\\|_{1},x_{0})$ 的几何性质通过统计维度等量控制着测量阈值；然而，上述推导仅依赖于凸性和绝对值的逐坐标性质。",
            "answer": "$$\\boxed{\\begin{gathered}\\partial\\|x\\|_{1}(x_{0})=\\left\\{g\\in\\mathbb{R}^{n}: g_{S}=s_{S},\\ \\|g_{S^{c}}\\|_{\\infty}\\leq 1\\right\\} \\\\ D(\\|\\cdot\\|_{1},x_{0})=\\operatorname{cone}\\left(\\left\\{d\\in\\mathbb{R}^{n}: s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\leq 0\\right\\}\\right)\\end{gathered}}$$"
        },
        {
            "introduction": "在前一个练习中，我们刻画了$\\ell_1$范数的下降锥。现在的问题是：如何“度量”这个锥的大小，从而预测成功恢复信号所需的测量次数？“统计维度”这一概念为此提供了强大的工具，它通过计算一个标准高斯随机向量投影到锥上的期望范数来量化锥的“大小”。通过这个练习，你将把前述的下降锥与相变理论的核心——统计维度——联系起来，推导出其变分表示，亲身体会如何从抽象几何推导出现象的定量预测。",
            "id": "3466268",
            "problem": "设 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ 为 $\\ell_{1}$ 范数 $f(x)=\\|x\\|_{1}$。固定一个向量 $x_{0}\\in\\mathbb{R}^{n}$，其支撑集为 $S=\\{i:x_{0,i}\\neq 0\\}$，基数为 $|S|=k$，并假设对于 $i\\in S$，$\\operatorname{sign}(x_{0,i})\\in\\{-1,+1\\}$ 是固定的。在 $x_{0}$ 处，$f$ 的下降锥定义为\n$$\nD(f,x_{0})=\\operatorname{cl}\\,\\{\\,t(u-x_{0}): t\\geq 0,\\ f(u)\\leq f(x_{0})\\,\\},\n$$\n一个闭凸锥 $C\\subset\\mathbb{R}^{n}$ 的统计维度为\n$$\n\\delta(C)=\\mathbb{E}\\big[\\|\\Pi_{C}(g)\\|_{2}^{2}\\big],\n$$\n其中 $g\\sim\\mathcal{N}(0,I_{n})$ 且 $\\Pi_{C}$ 表示到 $C$ 上的欧几里得投影。您可以不经证明地使用以下经过充分检验的事实：\n- 对于任何闭凸锥 $C$，$\\delta(C)=\\mathbb{E}[\\operatorname{dist}^{2}(g,C^{\\circ})]$，其中 $C^{\\circ}$ 是其极锥。\n- 对于任何正常凸函数 $f$，$D(f,x_{0})^{\\circ}=\\operatorname{cone}(\\partial f(x_{0}))$，其中 $\\partial f(x_{0})$ 是其次微分。\n- 对于任何不包含原点的非空紧集 $K\\subset\\mathbb{R}^{n}$，以及任何 $y\\in\\mathbb{R}^{n}$，$\\operatorname{dist}(y,\\operatorname{cone}(K))=\\inf_{\\tau\\geq 0}\\operatorname{dist}(y,\\tau K)$。\n\n从上述定义和这些事实出发，并仅使用 $\\ell_{1}$ 次微分和高斯期望的基本性质，推导统计维度 $\\delta\\big(D(\\|\\cdot\\|_{1},x_{0})\\big)$ 的一个显式一维变分表示。该表示应仅依赖于 $n$、$k$ 和一个标量参数，并包含一个关于标准正态随机变量的一维期望。您应将最终结果表示为单个闭式解析表达式，该表达式以 $n$、$k$、一个非负标量参数下的下确界，以及一个涉及应用于标准正态随机变量绝对值的正部算子的一维期望来表示。不需要进行数值计算，也无需四舍五入。请提供最终表达式作为您的答案。",
            "solution": "问题要求我们为在点 $x_{0}$ 处 $\\ell_{1}$ 范数的下降锥的统计维度 $\\delta\\big(D(\\|\\cdot\\|_{1},x_{0})\\big)$ 提供一个一维变分表示。我们已知闭凸锥 $C \\subset \\mathbb{R}^n$ 的统计维度定义为 $\\delta(C)=\\mathbb{E}\\big[\\|\\Pi_{C}(g)\\|_{2}^{2}\\big]$，其中 $g\\sim\\mathcal{N}(0,I_{n})$ 且 $\\Pi_{C}$ 是到 $C$ 上的欧几里得投影。\n\n我们的推导将从给定的定义和事实出发，分几个步骤进行。\n\n令 $f(x) = \\|x\\|_{1}$。下降锥为 $C_0 = D(f,x_{0})$。\n\n步骤 1：将统计维度与极锥相关联。\n所给的第一个事实是，对于任何闭凸锥 $C$，其统计维度可以用其极锥 $C^{\\circ}$ 表示为 $\\delta(C)=\\mathbb{E}[\\operatorname{dist}^{2}(g,C^{\\circ})]$。\n将此应用于下降锥 $C_0$，我们得到：\n$$\n\\delta(C_0) = \\mathbb{E}_{g \\sim \\mathcal{N}(0,I_n)}\\big[\\operatorname{dist}^{2}(g, C_0^{\\circ})\\big]\n$$\n其中 $\\operatorname{dist}(y, A) = \\inf_{a \\in A} \\|y-a\\|_2$ 是点 $y$ 到集合 $A$ 的欧几里得距离。\n\n步骤 2：刻画下降锥的极锥。\n第二个事实指出，对于一个正常凸函数 $f$，其下降锥的极锥是其次微分的锥包：$D(f,x_{0})^{\\circ}=\\operatorname{cone}(\\partial f(x_{0}))$。\n$\\ell_1$ 范数 $f(x) = \\|x\\|_1$ 是一个正常凸函数。因此，\n$$\nC_0^{\\circ} = \\operatorname{cone}(\\partial \\|x_{0}\\|_{1})\n$$\n\n步骤 3：确定 $\\ell_1$ 范数的次微分。\n函数为 $f(x) = \\|x\\|_{1} = \\sum_{i=1}^{n} |x_i|$。次微分 $\\partial f(x_0)$ 是所有向量 $v \\in \\mathbb{R}^n$ 的集合，满足对于所有 $x \\in \\mathbb{R}^n$，都有 $\\|x\\|_1 \\ge \\|x_0\\|_1 + v^T(x-x_0)$。在 $x_0$ 处 $\\ell_1$ 范数的次微分的一个标准刻画由下式给出：\n$$\n\\partial \\|x_{0}\\|_{1} = \\{v \\in \\mathbb{R}^n : v_i = \\operatorname{sign}(x_{0,i}) \\text{ for } i \\in S, \\text{ and } v_i \\in [-1, 1] \\text{ for } i \\notin S\\}\n$$\n其中 $S = \\{i : x_{0,i} \\neq 0\\}$ 是 $x_0$ 的支撑集，其大小为 $|S|=k$。我们将这个紧凸集记为 $K$。令对于 $i \\in S$，$s_i = \\operatorname{sign}(x_{0,i})$，其中 $s_i \\in \\{-1, +1\\}$。\n\n步骤 4：用变分形式表示到锥包的距离。\n我们现在有 $C_0^{\\circ} = \\operatorname{cone}(K)$。问题要求我们计算 $\\mathbb{E}[\\operatorname{dist}^{2}(g, \\operatorname{cone}(K))]$。\n集合 $K$ 是非空紧集。我们假设 $k = |S| \\ge 1$，这在问题设置中有强烈的暗示。如果 $k \\ge 1$，对于任何 $v \\in K$，$\\|v\\|_2^2 = \\sum_{i \\in S} v_i^2 + \\sum_{i \\notin S} v_i^2 = \\sum_{i \\in S} s_i^2 + \\sum_{i \\notin S} v_i^2 = k + \\sum_{i \\notin S} v_i^2 \\ge k \\ge 1$。因此，$K$ 不包含原点。我们因此可以应用第三个事实：\n$$\n\\operatorname{dist}(g, \\operatorname{cone}(K)) = \\inf_{\\tau \\ge 0} \\operatorname{dist}(g, \\tau K)\n$$\n其中 $\\tau K = \\{\\tau v : v \\in K\\}$。将两边平方得到：\n$$\n\\operatorname{dist}^{2}(g, \\operatorname{cone}(K)) = \\inf_{\\tau \\ge 0} \\operatorname{dist}^{2}(g, \\tau K) = \\inf_{\\tau \\ge 0} \\left( \\inf_{v \\in K} \\|g - \\tau v\\|_2^2 \\right)\n$$\n\n步骤 5：求解内部最小化问题。\n对于一个固定的 $\\tau \\ge 0$，我们需要计算 $\\inf_{v \\in K} \\|g - \\tau v\\|_2^2$。平方范数在坐标上是可分的：\n$$\n\\|g - \\tau v\\|_2^2 = \\sum_{i=1}^{n} (g_i - \\tau v_i)^2 = \\sum_{i \\in S} (g_i - \\tau v_i)^2 + \\sum_{i \\notin S} (g_i - \\tau v_i)^2\n$$\n我们逐项对 $v \\in K$ 进行最小化。\n对于 $i \\in S$，$v_i$ 固定为 $s_i$。该项为 $(g_i - \\tau s_i)^2$。\n对于 $i \\notin S$，我们必须在 $v_i \\in [-1, 1]$ 上最小化 $(g_i - \\tau v_i)^2$。这等价于在区间 $[-\\tau, \\tau]$ 中找到离 $g_i$ 最近的点。最小平方距离由 $(|g_i|-\\tau)_+^2$ 给出，其中 $(x)_+ = \\max(0,x)$ 是正部算子。\n结合这些结果，我们得到：\n$$\n\\operatorname{dist}^{2}(g, \\tau K) = \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2\n$$\n将此代回到锥距离的表达式中：\n$$\n\\operatorname{dist}^{2}(g, C_0^{\\circ}) = \\inf_{\\tau \\ge 0} \\left\\{ \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2 \\right\\}\n$$\n\n步骤 6：计算期望。\n统计维度是上述数量的期望值。\n$$\n\\delta(C_0) = \\mathbb{E}_{g} \\left[ \\inf_{\\tau \\ge 0} \\left\\{ \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2 \\right\\} \\right]\n$$\n为了获得一维变分表示，我们交换期望和下确界。这是此类推导中的一个标准步骤，其合理性由高维高斯向量的测度集中现象保证。\n$$\n\\delta(C_0) = \\inf_{\\tau \\ge 0} \\mathbb{E}_{g} \\left[ \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2 \\right]\n$$\n根据期望的线性性质，我们可以将其写为：\n$$\n\\delta(C_0) = \\inf_{\\tau \\ge 0} \\left\\{ \\sum_{i \\in S} \\mathbb{E}_{g_i}[(g_i - \\tau s_i)^2] + \\sum_{i \\notin S} \\mathbb{E}_{g_i}[(|g_i| - \\tau)_+^2] \\right\\}\n$$\n$g$ 的分量 $g_i$ 是独立同分布的，服从 $g_i \\sim \\mathcal{N}(0,1)$。\n\n我们来计算期望。对于 $i \\in S$，$s_i^2 = (\\pm 1)^2 = 1$。由于 $\\mathbb{E}[g_i] = 0$ 且 $\\mathbb{E}[g_i^2] = 1$：\n$$\n\\mathbb{E}[(g_i - \\tau s_i)^2] = \\mathbb{E}[g_i^2 - 2\\tau s_i g_i + \\tau^2 s_i^2] = \\mathbb{E}[g_i^2] - 2\\tau s_i \\mathbb{E}[g_i] + \\tau^2 s_i^2 = 1 - 0 + \\tau^2(1) = 1 + \\tau^2\n$$\n由于有 $|S|=k$ 个这样的项，它们的和是 $k(1 + \\tau^2)$。\n\n对于 $i \\notin S$，这些项是相同的。令 $Z \\sim \\mathcal{N}(0,1)$。我们有 $(n-k)$ 个形如 $\\mathbb{E}[(|Z|-\\tau)_+^2]$ 的项。问题要求用这个一维期望来表示结果。\n\n步骤 7：最终组合。\n结合计算出的期望，我们得到统计维度的最终表达式：\n$$\n\\delta\\big(D(\\|\\cdot\\|_{1},x_{0})\\big) = \\inf_{\\tau \\ge 0} \\left\\{ k(1+\\tau^2) + (n-k) \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)} [(|Z|-\\tau)_+^2] \\right\\}\n$$\n这就是所要求的一维变分表示，用 $n$、$k$、标量参数 $\\tau$ 和一个一维期望来表示。\n如题目所要求，该表达式使用了应用于标准正态随机变量绝对值的正部算子 $(x)_+ = \\max(0,x)$。",
            "answer": "$$\n\\boxed{\\inf_{\\tau \\ge 0} \\left\\{ k(1+\\tau^2) + (n-k) \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)} \\left[\\left(\\max\\left(0, |Z|-\\tau\\right)\\right)^2\\right] \\right\\}}\n$$"
        },
        {
            "introduction": "拥有了$\\ell_1$恢复的理论工具后，一个自然的问题是：这些理论预测在实践中表现如何？我们能否通过更强的稀疏性惩罚来突破$\\ell_1$的性能界限？这个练习将引导你从理论推导走向计算实践，通过编写代码来经验性地绘制恢复性能的相变图。你不仅将验证凸松弛方法的理论边界，还将探索非凸的$\\ell_p$（$p \\lt 1$）拟范数最小化，并亲手量化其在降低测量需求方面的增益与算法不稳定性带来的实际成本。",
            "id": "3466212",
            "problem": "考虑无噪声压缩感知模型，其中测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 具有独立同分布的高斯条目（经过归一化，使得每列的期望范数为单位1），用于获取测量值 $y = A x^\\star$，其中目标向量 $x^\\star \\in \\mathbb{R}^n$ 是 $k$-稀疏的。比较了两种恢复策略：一种是凸基线方法，即在 $A x = y$ 约束下最小化 $\\ell_1$ 范数；另一种是非凸替代方法，即在 $A x = y$ 约束下最小化 $\\ell_p$ 拟范数（其中 $p \\in (0,1)$）。总体目标是生成一个计算相图，并量化非凸 $\\ell_p$ 策略在相变边界上实现的经验增益是否超过了由非凸性引入的算法不稳定性成本。\n\n此问题的基础是：\n- 稀疏性的定义：如果一个向量 $x^\\star$ 恰好有 $k$ 个非零项，则称其为 $k$-稀疏的。\n- 凸正则化器相变的锥几何解释：在 $x^\\star$ 处的下降锥（或等效的锥代理）的统计维度预测了以高概率实现精确恢复所需测量次数 $m$ 的近似阈值，该阈值是 $(n,k)$ 和正则化器几何形状的函数。\n- 对于 $p \\in (0,1)$，非凸 $\\ell_p$ 拟范数比 $\\ell_1$ 范数更能促进稀疏性，但它不是凸的，这可能导致算法不稳定。对于非凸下降集的任何广义统计维度近似都必须依赖于根据 $x^\\star$ 和正则化器的局部几何构造的有原则的代理。\n\n程序要求：\n1. 实现一个经验相图生成器，对于固定的 $(n,k,p)$ 和 $x^\\star$ 非零项的指定振幅模式，估计 $\\ell_1$ 最小化和 $\\ell_p$ 最小化的经验相变边界。相变边界被近似为在一个设计的网格中，使得成功率在多次随机试验中至少达到指定阈值的最小 $m$ 值。\n2. 为非凸 $\\ell_p$ 下降集构造一个广义统计维度近似，方法是将基数 $k$ 替换为一个有效稀疏度量，该度量是利用从 $\\ell_p$ 几何导出的权重根据 $x^\\star$ 的振幅分布计算得出的。使用此近似来选择 $\\ell_1$ 和 $\\ell_p$ 预测相变点附近的 $m$-网格，确保网格探测预期边界的下方、所在位置和上方。\n3. 通过线性规划公式实现 $\\ell_1$ 最小化，并通过等式约束的迭代重加权最小二乘（IRLS）实现 $\\ell_p$ 最小化。确保使用数值稳定的线性代数，并检测 IRLS 算法中的不收敛或数值不稳定性。\n4. 将单次试验的经验成功定义为恢复向量 $\\hat{x}$ 和 $x^\\star$ 之间的相对 $\\ell_2$ 误差小于 $10^{-3}$ 的事件，即 $\\| \\hat{x} - x^\\star \\|_2 / \\| x^\\star \\|_2  10^{-3}$。\n5. 将 $\\ell_p$ 方法在测试用例中的算法不稳定性成本定义为，在所选 $m$-网格中，IRLS 运行在固定迭代预算内未能收敛到稳定解或产生数值无效迭代的平均比例。\n6. 将相变边界的经验增益定义为 $\\ell_1$ 和 $\\ell_p$ 的经验临界测量值之差，并用 $n$ 进行归一化（即，计算 $(m_{\\text{crit},\\ell_1} - m_{\\text{crit},\\ell_p})/n$，其中 $m_{\\text{crit},\\cdot}$ 是成功率超过阈值的最小 $m$）。如果此归一化增益严格大于不稳定性成本，则断定增益大于成本。\n7. 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表（例如，“[result1,result2,result3]”），列表中的每个条目都是一个测试用例的布尔值，指示经验增益是否大于算法不稳定性成本。\n\n广义统计维度近似规范：\n- 使用为非凸 $\\ell_p$ 几何定制的参与率类型代理，从 $x^\\star$ 的非零项的振幅计算有效稀疏度。令支撑集内索引的 $a_i = |x^\\star_i|^p$，否则 $a_i = 0$。将有效稀疏度定义为\n$$\ns_{\\mathrm{eff}}(p) = \\frac{\\left( \\sum_{i=1}^n a_i \\right)^2}{\\sum_{i=1}^n a_i^2}.\n$$\n- 在凸统计维度代理中用此有效稀疏度代替 $k$，以近似 $\\ell_p$ 的相变边界，作为 $(n, s_{\\mathrm{eff}}(p))$ 的函数。凸基线使用实际基数 $k$。这会产生近似的临界测量数，程序应使用它来为经验探测的 $m$-网格定中心。\n\n恢复算法：\n- 对于 $\\ell_1$ 基线，使用标准的上镜图技巧，通过线性规划求解优化问题 $\\min \\|x\\|_1$ subject to $A x = y$。确保在无噪声设置中所有约束都得到精确满足。\n- 对于 $\\ell_p$ 方法，通过求解一系列带等式约束的加权最小二乘问题，实现 IRLS 以近似最小化 $\\sum_i |x_i|^p$ subject to $A x = y$。对相关的线性求解施加一个小的 Tikhonov 正则化来稳定该过程，并且如果在指定的迭代限制内，迭代的相对变化没有降到容差以下，则检测为不收敛。\n\n测试套件：\n对于每个测试用例，程序必须在所有 $k$-子集中均匀随机地选择 $x^\\star$ 的支撑集，并根据指定的模式分配振幅。非零项的符号必须在 $\\{-1,+1\\}$ 中均匀随机。测量矩阵 $A$ 必须具有独立同分布的高斯条目，并按 $1/\\sqrt{m}$ 进行缩放。\n\n使用以下测试套件：\n- 案例1（理想情况）：$n = 64$, $k = 8$, $p = 0.5$，支撑集上振幅相等。\n- 案例2（强非凸，重尾）：$n = 64$, $k = 8$, $p = 0.25$，支撑集上振幅从拉普拉斯分布中抽取。\n- 案例3（中度非凸，结构化衰减）：$n = 64$, $k = 12$, $p = 0.5$，支撑集上振幅呈几何衰减。\n- 案例4（近凸基线）：$n = 64$, $k = 8$, $p = 0.9$，支撑集上振幅相等。\n\n对于每种情况，通过围绕从 $\\ell_p$ 的广义统计维度近似和 $\\ell_1$ 的凸代理导出的预测相变数来构建 $m$-网格，并探测至少三个不同的 $m$ 值，覆盖预测边界的下方、附近和上方。\n\n阈值和数值参数：\n- 在给定 $m$ 下跨试验的成功阈值：成功率至少为 $0.6$。\n- IRLS 最大迭代次数：$200$，迭代容差：$10^{-6}$，等式求解正则化：$10^{-12}$。\n- 每个测试用例中每个 $m$ 的试验次数：$5$。\n- 相对误差成功标准：严格小于 $10^{-3}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个测试用例的布尔结果，以方括号括起来的逗号分隔列表形式，顺序与上面列出的案例一致。示例格式：\"[true,false,true,false]\"。实际大写必须与 Python 布尔格式匹配，即 \"True\" 或 \"False\"。",
            "solution": "我们从无噪声线性模型 $y = A x^\\star$ 开始，其中 $A \\in \\mathbb{R}^{m \\times n}$ 具有独立同分布且按 $1/\\sqrt{m}$ 缩放的高斯条目，$x^\\star \\in \\mathbb{R}^n$ 是一个 $k$-稀疏的目标向量。稀疏恢复旨在从 $y$ 和 $A$ 重构 $x^\\star$。对于像 $\\ell_1$ 范数这样的凸正则化器，相变现象可以通过锥几何来解释：当测量次数 $m$ 超过一个由正则化器在 $x^\\star$ 处的下降锥的统计维度决定的阈值时，精确恢复会以高概率发生；低于该阈值时，恢复通常会失败。一个闭凸锥 $C \\subset \\mathbb{R}^n$ 的统计维度定义为\n$$\n\\delta(C) = \\mathbb{E}\\left[ \\| \\Pi_C(g) \\|_2^2 \\right],\n$$\n其中 $\\Pi_C$ 表示到 $C$ 上的欧几里得投影，而 $g \\sim \\mathcal{N}(0, I_n)$ 是一个标准高斯向量。对于凸 $\\ell_1$ 正则化，这个统计维度取决于 $x^\\star$ 的支撑集大小和 $\\ell_1$ 范数的几何形状，并得出一个近似阈值 $m_{crit} \\approx \\delta(C)$。\n\n对于 $p \\in (0,1)$ 的非凸 $\\ell_p$ 拟范数正则化，非凸下降集没有闭式解的统计维度，而且，这些下降集不是凸的，可能高度不规则。然而，为了生成一个实用的相图，可以通过构建一个尊重 $\\ell_p$ 正则化器在 $x^\\star$ 处局部几何的凸代理，来采用广义统计维度近似。出发点是由 $\\ell_p$ 几何在振幅上导出的权重序列：为支撑集内的条目定义 $a_i = |x^\\star_i|^p$，为支撑集外的条目定义 $a_i = 0$。一个量化有多少条目对 $\\ell_p$ 几何有显著贡献的有效稀疏度，由一个参与率类型的量给出\n$$\ns_{\\mathrm{eff}}(p) = \\frac{\\left( \\sum_{i=1}^n a_i \\right)^2}{\\sum_{i=1}^n a_i^2}.\n$$\n当支撑集上的振幅相等时，该量简化为 $s_{\\mathrm{eff}}(p) = k$；当振幅异质或重尾时，该量减小，这反映了非凸惩罚比凸 $\\ell_1$ 范数更强烈地将权重集中在更少的条目上。\n\n为了近似 $\\ell_p$ 最小化的相变阈值，在凸统计维度代理中用 $s_{\\mathrm{eff}}(p)$ 替换基数 $k$。一个广泛使用的、用于计算在环境维度 $n$ 中 $k$-稀疏向量上 $\\ell_1$ 下降锥统计维度的凸代理具有以下形式\n$$\n\\delta_{\\ell_1}(n,k) \\approx 2 k \\log\\left( \\frac{n}{k} \\right) + c k,\n$$\n其中 $c$ 是一个捕捉曲率效应的适中常数（在经验表征中通常取接近 $7/5$）。类似地，我们提出广义近似\n$$\n\\delta_{\\ell_p}(n,s_{\\mathrm{eff}}(p)) \\approx 2 s_{\\mathrm{eff}}(p) \\log\\left( \\frac{n}{s_{\\mathrm{eff}}(p)} \\right) + c s_{\\mathrm{eff}}(p).\n$$\n这种替换体现了一种启发式思想，即 $x^\\star$ 处的非凸下降集类似于一个具有较少有效自由度的正则化器的凸下降锥。这个近似有两个作用：在预测的相变点附近选择 $m$-网格，并提供一个用于比较经验边界的基准。\n\n经验相图生成过程如下：\n- 对于每个测试用例，我们根据指定的模式为 $x^\\star$ 随机生成支撑集和振幅，确保非零项具有随机符号。我们用高斯矩阵 $A$（其条目按 $1/\\sqrt{m}$ 缩放）来构成 $y = A x^\\star$。\n- 对于凸 $\\ell_1$ 基线，我们使用上镜图公式来求解线性规划问题 $\\min \\|x\\|_1$ subject to $A x = y$，其中我们引入辅助变量 $u \\in \\mathbb{R}^n$ 满足 $u_i \\ge |x_i|$ 并最小化 $\\sum_i u_i$。这产生一个决策变量为 $(x,u)$、等式约束为 $A x = y$、以及对所有索引的不等式约束为 $x_i \\le u_i$ 和 $-x_i \\le u_i$ 的线性规划问题。现代求解器在无噪声等式约束设置下能可靠地处理此公式。\n- 对于非凸 $\\ell_p$ 方法，我们使用等式约束下的迭代重加权最小二乘（IRLS）来近似最小化 $\\sum_i |x_i|^p$ subject to $A x = y$。在第 $t$ 次迭代，给定当前迭代值 $x^{(t)}$，定义权重\n$$\nw_i^{(t)} = \\left( |x_i^{(t)}|^2 + \\varepsilon \\right)^{\\frac{p}{2} - 1},\n$$\n其中 $\\varepsilon > 0$ 是一个小的平滑参数，以避免奇异性。下一个迭代值通过最小化 $\\sum_i w_i^{(t)} x_i^2$ subject to $A x = y$ 获得，这是一个加权最小范数解：\n$$\nx^{(t+1)} = W^{-1} A^\\top \\left( A W^{-1} A^\\top \\right)^{-1} y,\n$$\n其中 $W = \\operatorname{diag}(w^{(t)})$。为了稳定 $A W^{-1} A^\\top$ 的求逆，我们添加一个小的 Tikhonov 正则化项 $\\gamma I_m$。当相对变化 $\\|x^{(t+1)} - x^{(t)}\\|_2 / \\|x^{(t+1)}\\|_2$ 降至容差以下时，我们宣布收敛。\n- 对于网格中的每个 $m$，我们运行多次试验并计算每种方法的成功率。我们将经验临界测量数 $m_{\\text{crit},\\cdot}$ 定义为在试验中成功率至少为 $0.6$ 的最小 $m$。如果没有 $m$ 达到此标准，我们设置 $m_{\\text{crit},\\cdot} = +\\infty$。\n- 每个测试用例中 $\\ell_p$ 的算法不稳定性成本，是根据 $m$-网格中IRLS运行在迭代预算内未能收敛或产生数值无效迭代的平均比例来计算的。这直接从观察到的算法行为中量化了不稳定性。\n\n决策标准：\n- 相变边界的经验增益是\n$$\ng = \\frac{m_{\\text{crit},\\ell_1} - m_{\\text{crit},\\ell_p}}{n},\n$$\n如果 $m_{\\text{crit},\\ell_p} \\ge m_{\\text{crit},\\ell_1}$ 或者任一临界数为无穷大，则 $g$ 的下限被截断为 $0$。算法不稳定性成本是 $\\ell_p$ 在整个网格上的平均未收敛率。如果 $g$ 严格大于不稳定性成本，我们断定增益大于成本。\n\n测试套件覆盖范围：\n- 案例1探测了一个典型的中等稀疏度情况，具有相等的振幅和 $p = 0.5$；预计非凸方法将在可控的不稳定性下带来增益。\n- 案例2专注于一个强非凸设置，具有 $p = 0.25$ 和重尾振幅（拉普拉斯分布），这应该会加强有效稀疏度的降低，同时可能增加不稳定性。\n- 案例3在 $p = 0.5$ 和更大的 $k$ 值下引入了结构化的几何衰减振幅，测试由振幅偏斜带来的有效稀疏度降低是否能抵消增加的稀疏度。\n- 案例4检查了近凸情况，具有 $p = 0.9$ 和相等的振幅，预计增益很小且不稳定性最低。\n\n算法和数值细节：\n- 测量矩阵按 $1/\\sqrt{m}$ 缩放可以稳定格拉姆矩阵并将列范数保持在合理范围内。\n- IRLS 的权重指数 $\\frac{p}{2} - 1$ 对于 $p \\in (0,1)$ 是负的，因此大系数的权重较小，受到的惩罚也较少，通过更多地抑制小系数来鼓励稀疏性。平滑参数 $\\varepsilon$ 避免了分量接近零时的奇异权重，而 Tikhonov 正则化 $\\gamma$ 稳定了加权正规方程的等式约束线性求解。\n- $\\ell_1$ 的线性规划公式使用了一个上镜图技巧，变量为 $(x,u)$，等式约束为 $A x = y$，不等式约束为 $x - u \\le 0$ 和 $-x - u \\le 0$，这在没有噪声的情况下是数值可靠的。\n\n该程序实现了上述所有组件，执行四个测试用例，并按顺序打印一个布尔值列表，指示每个案例的经验增益是否大于算法不稳定性成本。",
            "answer": "```python\nimport numpy as np\nfrom numpy.random import default_rng\nfrom scipy.optimize import linprog\nfrom scipy.linalg import solve\n\n# Seeded random generator for reproducibility\nrng = default_rng(12345)\n\ndef generate_support(n, k):\n    return rng.choice(n, size=k, replace=False)\n\ndef generate_amplitudes(k, pattern):\n    if pattern == \"equal\":\n        amps = np.ones(k)\n    elif pattern == \"geo\":\n        # Geometric decay amplitudes\n        rho = 0.7\n        amps = rho ** np.arange(k)\n        # Shuffle to avoid ordered alignment\n        rng.shuffle(amps)\n        # Normalize to maintain comparable scale\n        amps = amps / np.linalg.norm(amps) * np.sqrt(k)\n    elif pattern == \"laplace\":\n        # Laplace distribution |Laplace(0,1)|, with floor to avoid zeros\n        raw = rng.laplace(loc=0.0, scale=1.0, size=k)\n        amps = np.abs(raw) + 0.1\n        # Normalize\n        amps = amps / np.linalg.norm(amps) * np.sqrt(k)\n    else:\n        # Default equal\n        amps = np.ones(k)\n    return amps\n\ndef generate_sparse_vector(n, k, pattern):\n    x = np.zeros(n)\n    S = generate_support(n, k)\n    amps = generate_amplitudes(k, pattern)\n    signs = rng.choice([-1.0, 1.0], size=k)\n    x[S] = signs * amps\n    return x\n\ndef gaussian_matrix(m, n):\n    # Scale by 1/sqrt(m) to stabilize columns\n    return rng.normal(loc=0.0, scale=1.0/np.sqrt(m), size=(m, n))\n\ndef basis_pursuit_lp(A, y):\n    m, n = A.shape\n    # Variables: [x (n), u (n)]\n    c = np.hstack([np.zeros(n), np.ones(n)])\n    A_eq = np.hstack([A, np.zeros((m, n))])\n    b_eq = y.copy()\n    # Inequalities:\n    # x - u = 0 -> [I, -I]\n    # -x - u = 0 -> [-I, -I]\n    I = np.eye(n)\n    A_ub_top = np.hstack([ I, -I ])\n    A_ub_bottom = np.hstack([ -I, -I ])\n    A_ub = np.vstack([A_ub_top, A_ub_bottom])\n    b_ub = np.zeros(2*n)\n\n    bounds = []\n    # x free: (None, None), u >= 0: (0, None)\n    for _ in range(n):\n        bounds.append((None, None))\n    for _ in range(n):\n        bounds.append((0.0, None))\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n    if not res.success:\n        return None\n    x_rec = res.x[:n]\n    return x_rec\n\ndef irls_lp_equality(A, y, p, max_iter=200, tol=1e-6, eps0=1e-2, gamma=1e-12):\n    m, n = A.shape\n    # Initialize with weighted least squares solution (unweighted)\n    try:\n        G = A @ A.T\n        lam0 = solve(G + gamma*np.eye(m), y, assume_a='sym')\n        x = A.T @ lam0\n    except Exception:\n        # Fallback to least squares via pseudo-inverse\n        x = A.T @ y\n\n    eps = eps0\n    converged = True\n    for it in range(max_iter):\n        # Compute weights\n        w = (np.abs(x)**2 + eps)**(p/2.0 - 1.0)\n        # Avoid singular weights\n        w = np.maximum(w, 1e-12)\n        W_inv = 1.0 / w\n        # Build B = A W^{-1} A^T efficiently by column scaling\n        # A_scaled = A * W_inv along columns\n        A_scaled = A * W_inv\n        B = A_scaled @ A.T\n        try:\n            lam = solve(B + gamma*np.eye(m), y, assume_a='sym')\n        except Exception:\n            converged = False\n            break\n        x_new = W_inv * (A.T @ lam)\n        if not np.all(np.isfinite(x_new)):\n            converged = False\n            break\n        # Check convergence\n        norm_new = np.linalg.norm(x_new)\n        if norm_new == 0:\n            # Degenerate; treat as nonconvergent\n            converged = False\n            break\n        rel_change = np.linalg.norm(x_new - x) / norm_new\n        x = x_new\n        if rel_change  tol:\n            break\n        # Slow decrease of epsilon to allow sharper weights\n        if (it+1) % 10 == 0:\n            eps *= 0.9\n    else:\n        # Max iterations reached\n        converged = False\n    return x, converged\n\ndef relative_error(x, x_true):\n    denom = np.linalg.norm(x_true)\n    if denom == 0:\n        return np.inf\n    return np.linalg.norm(x - x_true) / denom\n\ndef effective_sparsity(x_true, p):\n    a = np.abs(x_true)**p\n    s_num = np.sum(a)**2\n    s_den = np.sum(a**2)\n    if s_den == 0:\n        return 0.0\n    return s_num / s_den\n\ndef delta_surrogate(n, s_eff):\n    # Convex-like surrogate: delta ≈ 2 s log(n/s) + c s, with c ≈ 7/5.\n    s = max(s_eff, 1e-9)\n    return 2.0 * s * np.log(n / s) + (7.0/5.0) * s\n\ndef run_trials(n, k, m, p, amp_pattern, trials=5, tol=1e-3):\n    succ_l1 = 0\n    succ_lp = 0\n    conv_lp = 0\n    for _ in range(trials):\n        x_true = generate_sparse_vector(n, k, amp_pattern)\n        A = gaussian_matrix(m, n)\n        y = A @ x_true\n\n        # l1 recovery\n        x_l1 = basis_pursuit_lp(A, y)\n        if x_l1 is not None and np.all(np.isfinite(x_l1)):\n            err1 = relative_error(x_l1, x_true)\n            if err1  tol:\n                succ_l1 += 1\n\n        # lp recovery via IRLS\n        x_lp, converged = irls_lp_equality(A, y, p)\n        if converged:\n            conv_lp += 1\n        if x_lp is not None and np.all(np.isfinite(x_lp)):\n            errp = relative_error(x_lp, x_true)\n            if errp  tol:\n                succ_lp += 1\n\n    rate_l1 = succ_l1 / trials\n    rate_lp = succ_lp / trials\n    conv_rate_lp = conv_lp / trials\n    return rate_l1, rate_lp, conv_rate_lp\n\ndef empirical_critical_m(n, k, p, amp_pattern, m_grid, trials=5, tol=1e-3, threshold=0.6):\n    crit_l1 = np.inf\n    crit_lp = np.inf\n    conv_rates = []\n    for m in sorted(m_grid):\n        rate_l1, rate_lp, conv_rate_lp = run_trials(n, k, m, p, amp_pattern, trials=trials, tol=tol)\n        conv_rates.append(conv_rate_lp)\n        if np.isinf(crit_l1) and rate_l1 >= threshold:\n            crit_l1 = m\n        if np.isinf(crit_lp) and rate_lp >= threshold:\n            crit_lp = m\n    # Instability cost: average failure rate across grid\n    if len(conv_rates) == 0:\n        inst_cost = 1.0\n    else:\n        inst_cost = 1.0 - (np.mean(conv_rates))\n    return crit_l1, crit_lp, inst_cost\n\ndef build_m_grid(n, k, p, amp_pattern):\n    # Use a representative x_true to compute effective sparsity and predicted deltas\n    x_true = generate_sparse_vector(n, k, amp_pattern)\n    s_eff = effective_sparsity(x_true, p)\n    # Surrogates for both methods\n    delta_lp = delta_surrogate(n, s_eff)\n    delta_l1 = delta_surrogate(n, float(k))\n    # Center grid near both predictions\n    m_candidates = set()\n    # Ensure m is within [2, n-1] bounds to be meaningful\n    for delta in [delta_lp, delta_l1]:\n        m0 = int(np.clip(np.round(delta), 2, n-1))\n        m_candidates.update([\n            int(np.clip(np.round(0.9 * m0), 2, n-1)),\n            int(np.clip(m0, 2, n-1)),\n            int(np.clip(np.round(1.1 * m0), 2, n-1)),\n        ])\n    # Also include a slightly larger point to probe above boundary\n    m_candidates.add(int(np.clip(np.round(1.25 * delta_lp), 2, n-1)))\n    return sorted(m_candidates)\n\ndef evaluate_case(n, k, p, amp_pattern):\n    m_grid = build_m_grid(n, k, p, amp_pattern)\n    crit_l1, crit_lp, inst_cost = empirical_critical_m(n, k, p, amp_pattern, m_grid)\n    if np.isinf(crit_l1) or np.isinf(crit_lp):\n        gain_frac = 0.0\n    else:\n        gain = crit_l1 - crit_lp\n        gain_frac = max(gain / n, 0.0)\n    # Gains outweigh costs if normalized gain > instability cost\n    decision = gain_frac > inst_cost\n    return decision\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, k, p, amplitude_pattern)\n        (64, 8, 0.5, \"equal\"),    # Case 1\n        (64, 8, 0.25, \"laplace\"), # Case 2\n        (64, 12, 0.5, \"geo\"),     # Case 3\n        (64, 8, 0.9, \"equal\"),    # Case 4\n    ]\n\n    results = []\n    for n, k, p, pattern in test_cases:\n        decision = evaluate_case(n, k, p, pattern)\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}