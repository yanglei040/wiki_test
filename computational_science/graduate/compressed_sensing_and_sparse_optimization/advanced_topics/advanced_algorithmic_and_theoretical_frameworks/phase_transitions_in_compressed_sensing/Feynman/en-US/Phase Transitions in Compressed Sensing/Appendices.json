{
    "hands_on_practices": [
        {
            "introduction": "The remarkable success of $\\ell_1$ minimization in finding sparse solutions is not magic; it is a direct consequence of the geometry of the $\\ell_1$ norm. This practice provides a foundational understanding by guiding you to derive the precise mathematical description of the subdifferential and the descent cone at a sparse vector . Mastering the characterization of these geometric objects is the first step toward rigorously analyzing why and when sparse recovery works.",
            "id": "3466252",
            "problem": "Consider the convex function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ defined by $f(x)=\\|x\\|_{1}=\\sum_{i=1}^{n}|x_{i}|$. Let $x_{0}\\in\\mathbb{R}^{n}$ be $k$-sparse with support $S\\subseteq\\{1,\\dots,n\\}$, $|S|=k$, and define the sign vector $s\\in\\mathbb{R}^{n}$ by $s_{i}=\\operatorname{sign}(x_{0,i})$ for $i\\in S$ and leaving $s_{j}$ undefined for $j\\in S^{c}$ where $x_{0,j}=0$. Denote by $d\\in\\mathbb{R}^{n}$ a direction, by $d_{S}$ and $d_{S^{c}}$ the restrictions of $d$ to $S$ and its complement $S^{c}$, respectively, and similarly for any vector $g\\in\\mathbb{R}^{n}$. The subdifferential of a proper lower semicontinuous convex function $f$ at a point $x$ is defined by\n$$\n\\partial f(x)=\\left\\{g\\in\\mathbb{R}^{n}: f(y)\\geq f(x)+\\langle g,y-x\\rangle\\ \\text{for all}\\ y\\in\\mathbb{R}^{n}\\right\\}.\n$$\nThe descent cone of $f$ at $x$ is defined by\n$$\nD(f,x)=\\left\\{d\\in\\mathbb{R}^{n}:\\exists\\,t>0\\ \\text{such that}\\ f(x+td)\\leq f(x)\\right\\}.\n$$\nStarting from these definitions and elementary properties of the absolute value and convexity, derive a complete characterization of the subdifferential $\\partial\\|x\\|_{1}(x_{0})$ in terms of the support $S$ and signs $s_{i}$, and then characterize the descent cone $D(\\|\\cdot\\|_{1},x_{0})$ as the conic hull of directions $d$ that satisfy a single explicit inequality written in terms of $s_{S}$, $d_{S}$, and $\\|d_{S^{c}}\\|_{1}$. Express your final result as a single closed-form analytic expression collecting both characterizations in a compact form. No numerical approximation is required, and no physical units are involved. Your final answer must be a single expression.",
            "solution": "We begin from the fundamental definitions of subdifferential and descent cone for convex functions and apply them to the $\\ell_{1}$ norm, using the structure induced by the support $S$ of the sparse vector $x_{0}$ and its sign pattern.\n\nCharacterization of the subdifferential. The function $f(x)=\\|x\\|_{1}$ is separable across coordinates: $f(x)=\\sum_{i=1}^{n}|x_{i}|$. For a single-coordinate absolute value, the subdifferential at a point $a\\in\\mathbb{R}$ is well known and follows from the convex subdifferential definition:\n- If $a\\neq 0$, then $\\partial|a|=\\{\\operatorname{sign}(a)\\}$.\n- If $a=0$, then $\\partial|a|=[-1,1]$.\n\nThis can be derived directly. For $a\\neq 0$, convexity and differentiability give $\\partial|a|=\\{\\operatorname{sign}(a)\\}$, because $|y|\\geq|a|+\\operatorname{sign}(a)(y-a)$ for all $y\\in\\mathbb{R}$, which follows from the supporting hyperplane at a nonzero point of $|\\cdot|$. For $a=0$, the subdifferential is the interval $[-1,1]$, because for any $g\\in[-1,1]$ and any $y\\in\\mathbb{R}$ we have $|y|\\geq g\\,y$ (indeed, $\\sup_{g\\in[-1,1]}g\\,y=|y|$, so every $g$ in the interval satisfies the subgradient inequality at $0$). Conversely, if $g\\notin[-1,1]$, one can violate $|y|\\geq g\\,y$ by choosing $y$ with the same sign as $g$ and sufficiently large magnitude, so $g\\notin\\partial|0|$.\n\nSince $f$ is a sum of absolute values, its subdifferential at $x_{0}$ is the Cartesian product of the coordinatewise subdifferentials, which yields\n$$\n\\partial\\|x\\|_{1}(x_{0})=\\left\\{g\\in\\mathbb{R}^{n}: g_{i}=\\operatorname{sign}(x_{0,i})\\ \\text{for all}\\ i\\in S,\\ \\text{and}\\ g_{j}\\in[-1,1]\\ \\text{for all}\\ j\\in S^{c}\\right\\}.\n$$\nEquivalently, writing $s_{S}\\in\\mathbb{R}^{S}$ for the sign vector on $S$, this can be expressed compactly as\n$$\n\\partial\\|x\\|_{1}(x_{0})=\\left\\{g\\in\\mathbb{R}^{n}: g_{S}=s_{S},\\ \\|g_{S^{c}}\\|_{\\infty}\\leq 1\\right\\}.\n$$\nTo verify necessity, assume $g\\in\\partial\\|x\\|_{1}(x_{0})$. Consider any $i\\in S$. Fix $y=x_{0}+t\\,e_{i}$ where $e_{i}$ is the $i$th coordinate vector and $t$ has the same sign as $x_{0,i}$. The subgradient inequality gives\n$$\n\\|x_{0}+t\\,e_{i}\\|_{1}\\geq\\|x_{0}\\|_{1}+g_{i}\\,t.\n$$\nBut $\\|x_{0}+t\\,e_{i}\\|_{1}=\\|x_{0}\\|_{1}+|x_{0,i}+t|-|x_{0,i}|=\\|x_{0}\\|_{1}+t\\,\\operatorname{sign}(x_{0,i})$ for sufficiently small $t$ not crossing zero in the $i$th coordinate. Hence $t\\,\\operatorname{sign}(x_{0,i})\\geq g_{i}\\,t$ for arbitrarily small nonzero $t$ of the same sign, which implies $g_{i}=\\operatorname{sign}(x_{0,i})$. For $j\\in S^{c}$, set $y=t\\,e_{j}$. Then the subgradient inequality yields $|t|\\geq g_{j}\\,t$ for all $t\\in\\mathbb{R}$, implying $|g_{j}|\\leq 1$. Thus the characterization is both sufficient and necessary.\n\nCharacterization of the descent cone. By definition,\n$$\nD(\\|\\cdot\\|_{1},x_{0})=\\left\\{d\\in\\mathbb{R}^{n}:\\exists\\,t>0\\ \\text{such that}\\ \\|x_{0}+t\\,d\\|_{1}\\leq\\|x_{0}\\|_{1}\\right\\}.\n$$\nWe analyze $\\|x_{0}+t\\,d\\|_{1}$ by splitting indices according to $S$ and $S^{c}$. Write\n$$\n\\|x_{0}+t\\,d\\|_{1}=\\sum_{i\\in S}|x_{0,i}+t\\,d_{i}|+\\sum_{j\\in S^{c}}|t\\,d_{j}|.\n$$\nFor $i\\in S$ with $x_{0,i}\\neq 0$, convexity of $|\\cdot|$ and the supporting hyperplane inequality at $x_{0,i}$ imply\n$$\n|x_{0,i}+t\\,d_{i}|\\geq |x_{0,i}|+t\\,\\operatorname{sign}(x_{0,i})\\,d_{i}.\n$$\nFor $j\\in S^{c}$, we have the exact identity $|t\\,d_{j}|=t\\,|d_{j}|$. Summing over coordinates, we obtain\n$$\n\\|x_{0}+t\\,d\\|_{1}\\geq \\|x_{0}\\|_{1}+t\\left(s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\right).\n$$\nTherefore, the inequality $\\|x_{0}+t\\,d\\|_{1}\\leq\\|x_{0}\\|_{1}$ can hold for some $t>0$ only if the linear term satisfies $s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\leq 0$. Conversely, if $s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}<0$, then the right-hand side is strictly less than $\\|x_{0}\\|_{1}$ for sufficiently small $t>0$, ensuring $\\|x_{0}+t\\,d\\|_{1}\\leq\\|x_{0}\\|_{1}$. If $s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}=0$, the inequality holds to first order, and by continuity and convexity, one can choose a sequence $t\\downarrow 0$ such that $\\|x_{0}+t\\,d\\|_{1}\\leq\\|x_{0}\\|_{1}$. Hence the necessary and sufficient condition is\n$$\ns_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\leq 0.\n$$\nThis set of directions is a cone, because for any $\\alpha>0$,\n$$\ns_{S}^{\\top}(\\alpha d_{S})+\\|\\alpha d_{S^{c}}\\|_{1}=\\alpha\\left(s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\right)\\leq 0,\n$$\nso $\\alpha d$ also satisfies the inequality. Therefore,\n$$\nD(\\|\\cdot\\|_{1},x_{0})=\\left\\{d\\in\\mathbb{R}^{n}: s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\leq 0\\right\\}.\n$$\n\nCollecting the two characterizations, we have expressed both the subdifferential at $x_{0}$ and the descent cone at $x_{0}$ in closed form in terms of $S$, $s_{S}$, and the norms $\\|\\cdot\\|_{\\infty}$ and $\\|\\cdot\\|_{1}$ on $S^{c}$. These characterizations are central in analyzing phase transitions in compressed sensing, because the geometry of $D(\\|\\cdot\\|_{1},x_{0})$ controls measurement thresholds via quantities such as the statistical dimension; however, the derivation above relies only on convexity and coordinatewise properties of the absolute value.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\partial\\|x\\|_{1}(x_{0})=\\left\\{g\\in\\mathbb{R}^{n}: g_{S}=s_{S},\\ \\|g_{S^{c}}\\|_{\\infty}\\leq 1\\right\\} & D(\\|\\cdot\\|_{1},x_{0})=\\left\\{d\\in\\mathbb{R}^{n}: s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\leq 0\\right\\}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Knowing the geometry of the descent cone is essential, but to predict a phase transition, we must quantify its \"size\" in a probabilistic sense. The statistical dimension provides the correct notion of size, capturing how a cone interacts with random subspaces and thereby predicting the threshold number of measurements needed for recovery. This exercise bridges the gap between qualitative geometry and quantitative prediction by tasking you with the derivation of the famous variational formula for the statistical dimension of the $\\ell_1$ descent cone .",
            "id": "3466268",
            "problem": "Let $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ be the $\\ell_{1}$ norm $f(x)=\\|x\\|_{1}$. Fix a vector $x_{0}\\in\\mathbb{R}^{n}$ with support set $S=\\{i:x_{0,i}\\neq 0\\}$ of cardinality $|S|=k$, and assume that $\\operatorname{sign}(x_{0,i})\\in\\{-1,+1\\}$ is fixed for $i\\in S$. The descent cone of $f$ at $x_{0}$ is defined by\n$$\nD(f,x_{0})=\\operatorname{cl}\\,\\{\\,t(u-x_{0}): t\\geq 0,\\ f(u)\\leq f(x_{0})\\,\\},\n$$\nand the statistical dimension of a closed convex cone $C\\subset\\mathbb{R}^{n}$ is\n$$\n\\delta(C)=\\mathbb{E}\\big[\\|\\Pi_{C}(g)\\|_{2}^{2}\\big],\n$$\nwhere $g\\sim\\mathcal{N}(0,I_{n})$ and $\\Pi_{C}$ denotes the Euclidean projection onto $C$. You may use the following well-tested facts without proof:\n- For any closed convex cone $C$, $\\delta(C)=\\mathbb{E}[\\operatorname{dist}^{2}(g,C^{\\circ})]$, where $C^{\\circ}$ is the polar cone.\n- For any proper convex function $f$, $D(f,x_{0})^{\\circ}=\\operatorname{cone}(\\partial f(x_{0}))$, where $\\partial f(x_{0})$ is the subdifferential.\n- For any nonempty compact set $K\\subset\\mathbb{R}^{n}$ that does not contain the origin, and any $y\\in\\mathbb{R}^{n}$, $\\operatorname{dist}(y,\\operatorname{cone}(K))=\\inf_{\\tau\\geq 0}\\operatorname{dist}(y,\\tau K)$.\n\nStarting from the definitions above and these facts, and using only fundamental properties of the $\\ell_{1}$ subdifferential and Gaussian expectation, derive an explicit one-dimensional variational representation for the statistical dimension $\\delta\\big(D(\\|\\cdot\\|_{1},x_{0})\\big)$ that depends only on $n$, $k$, and a scalar parameter, together with a one-dimensional expectation with respect to a standard normal random variable. You should express your final result as a single closed-form analytic expression in terms of $n$, $k$, an infimum over a nonnegative scalar parameter, and a one-dimensional expectation involving the positive part operator applied to the absolute value of a standard normal random variable. No numerical evaluation is required, and no rounding is needed. Provide the final expression as your answer.",
            "solution": "The problem asks for a one-dimensional variational representation for the statistical dimension $\\delta\\big(D(\\|\\cdot\\|_{1},x_{0})\\big)$ of the descent cone of the $\\ell_{1}$ norm at a point $x_{0}$. We are given the definition of the statistical dimension of a closed convex cone $C \\subset \\mathbb{R}^n$ as $\\delta(C)=\\mathbb{E}\\big[\\|\\Pi_{C}(g)\\|_{2}^{2}\\big]$, where $g\\sim\\mathcal{N}(0,I_{n})$ and $\\Pi_{C}$ is the Euclidean projection onto $C$.\n\nOur derivation will proceed in several steps, starting from the given definitions and facts.\n\nLet $f(x) = \\|x\\|_{1}$. The descent cone is $C_0 = D(f,x_{0})$.\n\nStep 1: Relate the statistical dimension to the polar cone.\nThe first fact provided is that for any closed convex cone $C$, its statistical dimension can be expressed in terms of its polar cone $C^{\\circ}$ as $\\delta(C)=\\mathbb{E}[\\operatorname{dist}^{2}(g,C^{\\circ})]$.\nApplying this to the descent cone $C_0$, we have:\n$$\n\\delta(C_0) = \\mathbb{E}_{g \\sim \\mathcal{N}(0,I_n)}\\big[\\operatorname{dist}^{2}(g, C_0^{\\circ})\\big]\n$$\nwhere $\\operatorname{dist}(y, A) = \\inf_{a \\in A} \\|y-a\\|_2$ is the Euclidean distance from a point $y$ to a set $A$.\n\nStep 2: Characterize the polar cone of the descent cone.\nThe second fact states that for a proper convex function $f$, the polar of the descent cone is the conic hull of the subdifferential: $D(f,x_{0})^{\\circ}=\\operatorname{cone}(\\partial f(x_{0}))$.\nThe $\\ell_1$ norm $f(x) = \\|x\\|_1$ is a proper convex function. Therefore,\n$$\nC_0^{\\circ} = \\operatorname{cone}(\\partial \\|x_{0}\\|_{1})\n$$\n\nStep 3: Determine the subdifferential of the $\\ell_1$ norm.\nThe function is $f(x) = \\|x\\|_{1} = \\sum_{i=1}^{n} |x_i|$. The subdifferential $\\partial f(x_0)$ is the set of vectors $v \\in \\mathbb{R}^n$ such that $\\|x\\|_1 \\ge \\|x_0\\|_1 + v^T(x-x_0)$ for all $x \\in \\mathbb{R}^n$. A standard characterization of the subdifferential for the $\\ell_1$ norm at $x_0$ is given by:\n$$\n\\partial \\|x_{0}\\|_{1} = \\{v \\in \\mathbb{R}^n : v_i = \\operatorname{sign}(x_{0,i}) \\text{ for } i \\in S, \\text{ and } v_i \\in [-1, 1] \\text{ for } i \\notin S\\}\n$$\nwhere $S = \\{i : x_{0,i} \\neq 0\\}$ is the support of $x_0$ with size $|S|=k$. Let's denote this compact convex set by $K$. Let $s_i = \\operatorname{sign}(x_{0,i})$ for $i \\in S$, where $s_i \\in \\{-1, +1\\}$.\n\nStep 4: Express the distance to the conic hull variationally.\nWe now have $C_0^{\\circ} = \\operatorname{cone}(K)$. The problem asks us to compute $\\mathbb{E}[\\operatorname{dist}^{2}(g, \\operatorname{cone}(K))]$.\nThe set $K$ is nonempty and compact. We assume $k = |S| \\ge 1$, which is strongly implied by the problem setup. If $k \\ge 1$, for any $v \\in K$, $\\|v\\|_2^2 = \\sum_{i \\in S} v_i^2 + \\sum_{i \\notin S} v_i^2 = \\sum_{i \\in S} s_i^2 + \\sum_{i \\notin S} v_i^2 = k + \\sum_{i \\notin S} v_i^2 \\ge k \\ge 1$. Thus, $K$ does not contain the origin. We can therefore apply the third fact:\n$$\n\\operatorname{dist}(g, \\operatorname{cone}(K)) = \\inf_{\\tau \\ge 0} \\operatorname{dist}(g, \\tau K)\n$$\nwhere $\\tau K = \\{\\tau v : v \\in K\\}$. Squaring both sides gives:\n$$\n\\operatorname{dist}^{2}(g, \\operatorname{cone}(K)) = \\inf_{\\tau \\ge 0} \\operatorname{dist}^{2}(g, \\tau K) = \\inf_{\\tau \\ge 0} \\left( \\inf_{v \\in K} \\|g - \\tau v\\|_2^2 \\right)\n$$\n\nStep 5: Solve the inner minimization problem.\nFor a fixed $\\tau \\ge 0$, we need to compute $\\inf_{v \\in K} \\|g - \\tau v\\|_2^2$. The squared norm is separable over the coordinates:\n$$\n\\|g - \\tau v\\|_2^2 = \\sum_{i=1}^{n} (g_i - \\tau v_i)^2 = \\sum_{i \\in S} (g_i - \\tau v_i)^2 + \\sum_{i \\notin S} (g_i - \\tau v_i)^2\n$$\nWe minimize this term by term with respect to $v \\in K$.\nFor $i \\in S$, $v_i$ is fixed to $s_i$. The term is $(g_i - \\tau s_i)^2$.\nFor $i \\notin S$, we must minimize $(g_i - \\tau v_i)^2$ over $v_i \\in [-1, 1]$. This is equivalent to finding the point in the interval $[-\\tau, \\tau]$ that is closest to $g_i$. The minimum squared distance is given by $(|g_i|-\\tau)_+^2$, where $(x)_+ = \\max(0,x)$ is the positive part operator.\nCombining these results, we get:\n$$\n\\operatorname{dist}^{2}(g, \\tau K) = \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2\n$$\nPlugging this back into the expression for the distance to the cone:\n$$\n\\operatorname{dist}^{2}(g, C_0^{\\circ}) = \\inf_{\\tau \\ge 0} \\left\\{ \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2 \\right\\}\n$$\n\nStep 6: Compute the expectation.\nThe statistical dimension is the expectation of the above quantity.\n$$\n\\delta(C_0) = \\mathbb{E}_{g} \\left[ \\inf_{\\tau \\ge 0} \\left\\{ \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2 \\right\\} \\right]\n$$\nTo obtain a one-dimensional variational representation, we exchange the expectation and the infimum. This is a standard step in such derivations, justified by concentration of measure phenomena for high-dimensional Gaussian vectors.\n$$\n\\delta(C_0) = \\inf_{\\tau \\ge 0} \\mathbb{E}_{g} \\left[ \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2 \\right]\n$$\nBy linearity of expectation, we can write this as:\n$$\n\\delta(C_0) = \\inf_{\\tau \\ge 0} \\left\\{ \\sum_{i \\in S} \\mathbb{E}_{g_i}[(g_i - \\tau s_i)^2] + \\sum_{i \\notin S} \\mathbb{E}_{g_i}[(|g_i| - \\tau)_+^2] \\right\\}\n$$\nThe components $g_i$ of $g$ are independent and identically distributed as $g_i \\sim \\mathcal{N}(0,1)$.\n\nLet's evaluate the expectations. For $i \\in S$, $s_i^2 = (\\pm 1)^2 = 1$. Since $\\mathbb{E}[g_i] = 0$ and $\\mathbb{E}[g_i^2] = 1$:\n$$\n\\mathbb{E}[(g_i - \\tau s_i)^2] = \\mathbb{E}[g_i^2 - 2\\tau s_i g_i + \\tau^2 s_i^2] = \\mathbb{E}[g_i^2] - 2\\tau s_i \\mathbb{E}[g_i] + \\tau^2 s_i^2 = 1 - 0 + \\tau^2(1) = 1 + \\tau^2\n$$\nSince there are $|S|=k$ such terms, their sum is $k(1 + \\tau^2)$.\n\nFor $i \\notin S$, the terms are identical. Let $Z \\sim \\mathcal{N}(0,1)$. We have $(n-k)$ terms of the form $\\mathbb{E}[(|Z|-\\tau)_+^2]$. The problem asks for the result in terms of this one-dimensional expectation.\n\nStep 7: Final Assembly.\nCombining the computed expectations, we obtain the final expression for the statistical dimension:\n$$\n\\delta\\big(D(\\|\\cdot\\|_{1},x_{0})\\big) = \\inf_{\\tau \\ge 0} \\left\\{ k(1+\\tau^2) + (n-k) \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)} [(|Z|-\\tau)_+^2] \\right\\}\n$$\nThis is the required one-dimensional variational representation, expressed in terms of $n$, $k$, a scalar parameter $\\tau$, and a one-dimensional expectation.\nThe expression uses the positive part operator $(x)_+ = \\max(0,x)$ applied to the absolute value of a standard normal random variable, as requested.",
            "answer": "$$\n\\boxed{\\inf_{\\tau \\ge 0} \\left\\{ k(1+\\tau^2) + (n-k) \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)} \\left[\\left(\\max\\left(0, |Z|-\\tau\\right)\\right)^2\\right] \\right\\}}\n$$"
        },
        {
            "introduction": "Theoretical models of phase transitions provide powerful predictions, but their true utility is revealed through empirical validation and exploration of more complex scenarios. This capstone computational practice brings the theory to life by having you implement a simulation to generate and compare the empirical phase diagrams for both convex $\\ell_1$ minimization and its non-convex $\\ell_p$ counterpart . Through this exercise, you will not only visualize the sharp transition from failure to success but also gain practical insight into the trade-offs between the enhanced performance of non-convex methods and their inherent algorithmic challenges.",
            "id": "3466212",
            "problem": "Consider the noiseless compressed sensing model where a measurement matrix $A \\in \\mathbb{R}^{m \\times n}$ with independent and identically distributed Gaussian entries (normalized so that each column has expected unit norm) is used to acquire measurements $y = A x^\\star$, where the target vector $x^\\star \\in \\mathbb{R}^n$ is $k$-sparse. Two recovery strategies are compared: the convex baseline that minimizes the $\\ell_1$ norm subject to $A x = y$, and a nonconvex alternative that minimizes the $\\ell_p$ quasi-norm with $p \\in (0,1)$ subject to $A x = y$. The overarching goal is to generate a computational phase diagram and to quantify whether the empirical gains in the transition boundary achieved by the nonconvex $\\ell_p$ strategy outweigh the algorithmic instability costs introduced by nonconvexity.\n\nThe foundational base for this problem is:\n- The definition of sparsity: a vector $x^\\star$ is $k$-sparse if it has exactly $k$ nonzero entries.\n- The conic geometry interpretation of phase transitions for convex regularizers: the statistical dimension of the descent cone (or an equivalent conic surrogate) at $x^\\star$ predicts an approximate threshold on the number of measurements $m$ required for exact recovery with high probability, as a function of $(n,k)$ and the geometry of the regularizer.\n- The nonconvex $\\ell_p$ quasi-norm for $p \\in (0,1)$ promotes sparsity more aggressively than the $\\ell_1$ norm but is not convex, which can cause algorithmic instability. Any generalized statistical dimension approximation for nonconvex descent sets must rely on principled surrogates constructed from $x^\\star$ and the local geometry of the regularizer.\n\nProgram requirements:\n1. Implement an empirical phase-diagram generator that, for a fixed $(n,k,p)$ and a specified amplitude pattern for the nonzero entries of $x^\\star$, estimates the empirical transition boundary for both $\\ell_1$ minimization and $\\ell_p$ minimization. The transition boundary is approximated as the smallest $m$ in a designed grid for which the success rate is at least a specified threshold across multiple random trials.\n2. Construct a generalized statistical dimension approximation for the nonconvex $\\ell_p$ descent set by replacing the cardinality $k$ with an effective sparsity quantity computed from the amplitude distribution of $x^\\star$ using weights derived from the $\\ell_p$ geometry. Use this approximation to select the $m$-grid near the predicted transition for both $\\ell_1$ and $\\ell_p$, ensuring the grid probes below, at, and above the anticipated boundary.\n3. Implement $\\ell_1$ minimization via a linear programming formulation and $\\ell_p$ minimization via Iteratively Reweighted Least Squares (IRLS) for equality constraints. Ensure numerically stable linear algebra is used and detect nonconvergence or numerical instability in the IRLS algorithm.\n4. Define empirical success for a single trial as the event that the relative $\\ell_2$ error between the recovered vector $\\hat{x}$ and $x^\\star$ is less than $10^{-3}$, namely $\\| \\hat{x} - x^\\star \\|_2 / \\| x^\\star \\|_2 < 10^{-3}$.\n5. Define the algorithmic instability cost for the $\\ell_p$ method in a test case as the average fraction of IRLS runs within the chosen $m$-grid that fail to converge to a stable solution within a fixed iteration budget or produce numerically invalid iterates.\n6. Define the empirical gain in the transition boundary as the difference between the empirical critical measurements for $\\ell_1$ and $\\ell_p$, normalized by $n$ (i.e., compute $(m_{\\text{crit},\\ell_1} - m_{\\text{crit},\\ell_p})/n$, where $m_{\\text{crit},\\cdot}$ is the smallest $m$ whose success rate exceeds the threshold). Conclude that the gains outweigh the costs if this normalized gain is strictly larger than the instability cost.\n7. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each entry is a boolean for one test case indicating whether the empirical gains outweigh the algorithmic instability costs.\n\nGeneralized statistical dimension approximation specification:\n- Compute an effective sparsity from the amplitudes of the nonzero entries of $x^\\star$ using a participation-ratio type proxy tailored to the nonconvex $\\ell_p$ geometry. Let $a_i = |x^\\star_i|^p$ for indices in the support, and $a_i = 0$ otherwise. Define the effective sparsity as\n$$\ns_{\\mathrm{eff}}(p) = \\frac{\\left( \\sum_{i=1}^n a_i \\right)^2}{\\sum_{i=1}^n a_i^2}.\n$$\n- Use this effective sparsity in place of $k$ in a convex statistical dimension surrogate to approximate the transition boundary for $\\ell_p$ as a function of $(n, s_{\\mathrm{eff}}(p))$. The convex baseline uses the actual cardinality $k$. This produces approximate critical measurement counts that the program should use to center the $m$-grid for empirical probing.\n\nRecovery algorithms:\n- For the $\\ell_1$ baseline, solve the optimization problem $\\min \\|x\\|_1$ subject to $A x = y$ via a linear program using a standard epigraph trick. Ensure all constraints are satisfied exactly in the noiseless setting.\n- For the $\\ell_p$ method, implement IRLS to approximately minimize $\\sum_i |x_i|^p$ subject to $A x = y$ by solving a sequence of weighted least-squares problems with equality constraints. Stabilize the procedure with a small Tikhonov regularization on the relevant linear solves, and detect nonconvergence if the relative change in iterates does not fall below a tolerance within a specified iteration limit.\n\nTest suite:\nFor each test case, the program must randomly select the support of $x^\\star$ uniformly among all $k$-subsets and assign amplitudes according to the specified pattern. The signs of the nonzero entries must be uniformly random in $\\{-1,+1\\}$. The measurement matrix $A$ must have independent and identically distributed Gaussian entries scaled by $1/\\sqrt{m}$.\n\nUse the following test suite:\n- Case 1 (happy path): $n = 64$, $k = 8$, $p = 0.5$, amplitudes equal on the support.\n- Case 2 (strongly nonconvex, heavy-tail): $n = 64$, $k = 8$, $p = 0.25$, amplitudes drawn from a Laplace distribution on the support.\n- Case 3 (moderate nonconvex, structured decay): $n = 64$, $k = 12$, $p = 0.5$, amplitudes follow geometric decay on the support.\n- Case 4 (near-convex baseline): $n = 64$, $k = 8$, $p = 0.9$, amplitudes equal on the support.\n\nFor each case, construct the $m$-grid by centering around the predicted transition counts derived from the generalized statistical dimension approximation for $\\ell_p$ and the convex surrogate for $\\ell_1$, and probe at least three distinct $m$-values that cover below, near, and above the predicted boundary.\n\nThresholds and numerical parameters:\n- Success threshold across trials at a given $m$: at least $0.6$ fraction successful.\n- IRLS maximum iterations: $200$, iteration tolerance: $10^{-6}$, equality solve regularization: $10^{-12}$.\n- Number of trials per $m$ in each test case: $5$.\n- Relative error success criterion: strictly less than $10^{-3}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the boolean results for the four test cases as a comma-separated list enclosed in square brackets, in the order of the cases listed above. Example format: \"[true,false,true,false]\". Actual capitalization must match Python boolean formatting, i.e., \"True\" or \"False\".",
            "solution": "We begin from the noiseless linear model $y = A x^\\star$ with $A \\in \\mathbb{R}^{m \\times n}$ having independent and identically distributed Gaussian entries scaled by $1/\\sqrt{m}$ and a $k$-sparse target $x^\\star \\in \\mathbb{R}^n$. Sparse recovery seeks to reconstruct $x^\\star$ from $y$ and $A$. The phase transition phenomenon for convex regularizers such as the $\\ell_1$ norm can be interpreted via conic geometry: when the number of measurements $m$ exceeds a threshold determined by the statistical dimension of the descent cone of the regularizer at $x^\\star$, exact recovery occurs with high probability; below that threshold, recovery typically fails. The statistical dimension of a closed convex cone $C \\subset \\mathbb{R}^n$ is defined as\n$$\n\\delta(C) = \\mathbb{E}\\left[ \\| \\Pi_C(g) \\|_2^2 \\right],\n$$\nwhere $\\Pi_C$ denotes the Euclidean projection onto $C$ and $g \\sim \\mathcal{N}(0, I_n)$ is a standard Gaussian vector. For convex $\\ell_1$ regularization, this statistical dimension depends on the support size of $x^\\star$ and the geometry of the $\\ell_1$ norm, and yields an approximate threshold $m \\approx \\delta(C)/n$.\n\nFor nonconvex $\\ell_p$ quasi-norm regularization with $p \\in (0,1)$, there is no closed-form statistical dimension for the nonconvex descent set, and further, the descent sets are not convex and may be highly irregular. Nevertheless, to generate a practical phase diagram, one may employ a generalized statistical dimension approximation by constructing a convex surrogate that respects the local geometry of the $\\ell_p$ regularizer at $x^\\star$. The starting point is the weight sequence induced by the $\\ell_p$ geometry on amplitudes: define $a_i = |x^\\star_i|^p$ for the entries in the support and $a_i = 0$ for entries outside the support. An effective sparsity that quantifies how many entries significantly contribute to the $\\ell_p$ geometry is given by a participation-ratio type quantity\n$$\ns_{\\mathrm{eff}}(p) = \\frac{\\left( \\sum_{i=1}^n a_i \\right)^2}{\\sum_{i=1}^n a_i^2}.\n$$\nThis quantity reduces to $s_{\\mathrm{eff}}(p) = k$ when amplitudes are equal on the support and decreases when amplitudes are heterogeneous or heavy-tailed, reflecting that the nonconvex penalty concentrates weight on fewer entries more strongly than the convex $\\ell_1$ norm.\n\nTo approximate the transition threshold for $\\ell_p$ minimization, replace the cardinality $k$ in a convex statistical dimension surrogate with $s_{\\mathrm{eff}}(p)$. A widely used convex surrogate for the statistical dimension of the $\\ell_1$ descent cone at a $k$-sparse vector in ambient dimension $n$ has the form\n$$\n\\delta_{\\ell_1}(n,k) \\approx 2 k \\log\\left( \\frac{n}{k} \\right) + c k,\n$$\nwhere $c$ is a modest constant capturing curvature effects (often taken near $7/5$ in empirical characterizations). Analogously, we propose the generalized approximation\n$$\n\\delta_{\\ell_p}(n,s_{\\mathrm{eff}}(p)) \\approx 2 s_{\\mathrm{eff}}(p) \\log\\left( \\frac{n}{s_{\\mathrm{eff}}(p)} \\right) + c s_{\\mathrm{eff}}(p).\n$$\nThis replacement embodies the heuristic that the nonconvex descent set at $x^\\star$ is akin to a convex descent cone of a regularizer with fewer effective degrees of freedom. This approximation serves two roles: selecting the $m$-grid near the predicted transition and providing a benchmark to compare the empirical boundary.\n\nEmpirical phase diagram generation proceeds as follows:\n- For each test case, we randomly generate supports and amplitudes for $x^\\star$ according to a specified pattern, ensuring the nonzero entries have random signs. We form $y = A x^\\star$ with $A$ being Gaussian with entries scaled by $1/\\sqrt{m}$.\n- For the convex $\\ell_1$ baseline, we solve the linear program $\\min \\|x\\|_1$ subject to $A x = y$ using an epigraph formulation in which we introduce auxiliary variables $u \\in \\mathbb{R}^n$ with $u_i \\ge |x_i|$ and minimize $\\sum_i u_i$. This yields a linear program with decision variables $(x,u)$, equality constraints $A x = y$, and inequalities $x_i \\le u_i$ and $-x_i \\le u_i$ for all indices. Modern solvers reliably handle this formulation in the noiseless equality-constrained setting.\n- For the nonconvex $\\ell_p$ method, we use Iteratively Reweighted Least Squares (IRLS) under equality constraints to approximately minimize $\\sum_i |x_i|^p$ subject to $A x = y$. At iteration $t$, given a current iterate $x^{(t)}$, define weights\n$$\nw_i^{(t)} = \\left( |x_i^{(t)}|^2 + \\varepsilon \\right)^{\\frac{p}{2} - 1},\n$$\nwith a small smoothing parameter $\\varepsilon > 0$ to avoid singularities. The next iterate is obtained as the minimizer of $\\sum_i w_i^{(t)} x_i^2$ subject to $A x = y$, which is a weighted minimum-norm solution:\n$$\nx^{(t+1)} = W^{-1} A^\\top \\left( A W^{-1} A^\\top \\right)^{-1} y,\n$$\nwith $W = \\operatorname{diag}(w^{(t)})$. To stabilize the inversion of $A W^{-1} A^\\top$, we add a small Tikhonov regularization $\\gamma I_m$. We declare convergence when the relative change $\\|x^{(t+1)} - x^{(t)}\\|_2 / \\|x^{(t+1)}\\|_2$ falls below a tolerance.\n- For each $m$ in the grid, we run multiple trials and compute the success rate for each method. We define the empirical critical measurement count $m_{\\text{crit},\\cdot}$ as the smallest $m$ for which the success rate is at least $0.6$ across the trials. If no $m$ achieves this, we set $m_{\\text{crit},\\cdot} = +\\infty$.\n- The algorithmic instability cost for $\\ell_p$ in each test case is computed as the average fraction of IRLS runs in the $m$-grid that fail to converge within the iteration budget or produce numerically invalid iterates. This quantifies instability directly from observed algorithmic behavior.\n\nDecision criterion:\n- The empirical gain in the transition boundary is\n$$\ng = \\frac{m_{\\text{crit},\\ell_1} - m_{\\text{crit},\\ell_p}}{n},\n$$\nwith $g$ clipped below by $0$ if $m_{\\text{crit},\\ell_p} \\ge m_{\\text{crit},\\ell_1}$ or if either critical count is infinite. The algorithmic instability cost is the average failure-to-converge rate for $\\ell_p$ across the grid. We conclude that gains outweigh costs if $g$ is strictly larger than the instability cost.\n\nTest suite coverage:\n- Case 1 probes a typical moderate-sparsity regime with equal amplitudes and $p = 0.5$; the nonconvex method is expected to deliver a gain with manageable instability.\n- Case 2 focuses on a strongly nonconvex setting with $p = 0.25$ and heavy-tailed amplitudes (Laplace), which should strengthen the effective sparsity reduction while potentially increasing instability.\n- Case 3 introduces structured geometric decay in amplitudes at $p = 0.5$ and larger $k$, testing whether effective sparsity reduction from amplitude skew offsets the increased sparsity.\n- Case 4 examines the near-convex regime with $p = 0.9$ and equal amplitudes, where gains are expected to be small and instability minimal.\n\nAlgorithmic and numerical details:\n- The measurement matrix scaling by $1/\\sqrt{m}$ stabilizes the gram matrix and keeps column norms in a reasonable range.\n- The IRLS weight exponent $\\frac{p}{2} - 1$ is negative for $p \\in (0,1)$, so large coefficients have smaller weights and are penalized less, encouraging sparsity by suppressing small coefficients more. The smoothing parameter $\\varepsilon$ avoids singular weights when components are near zero, and the Tikhonov regularization $\\gamma$ stabilizes the equality-constrained linear solve for the weighted normal equations.\n- The linear programming formulation for $\\ell_1$ uses an epigraph trick with variables $(x,u)$, equality $A x = y$, and inequality constraints $x - u \\le 0$ and $-x - u \\le 0$, which is numerically reliable without noise.\n\nThe program implements all above components, executes the four test cases, and prints the list of booleans indicating whether empirical gains outweigh algorithmic instability costs for each case in order.",
            "answer": "```python\nimport numpy as np\nfrom numpy.random import default_rng\nfrom scipy.optimize import linprog\nfrom scipy.linalg import solve\n\n# Seeded random generator for reproducibility\nrng = default_rng(12345)\n\ndef generate_support(n, k):\n    return rng.choice(n, size=k, replace=False)\n\ndef generate_amplitudes(k, pattern):\n    if pattern == \"equal\":\n        amps = np.ones(k)\n    elif pattern == \"geo\":\n        # Geometric decay amplitudes\n        rho = 0.7\n        amps = rho ** np.arange(k)\n        # Shuffle to avoid ordered alignment\n        rng.shuffle(amps)\n        # Normalize to maintain comparable scale\n        amps = amps / np.linalg.norm(amps) * np.sqrt(k)\n    elif pattern == \"laplace\":\n        # Laplace distribution |Laplace(0,1)|, with floor to avoid zeros\n        raw = rng.laplace(loc=0.0, scale=1.0, size=k)\n        amps = np.abs(raw) + 0.1\n        # Normalize\n        amps = amps / np.linalg.norm(amps) * np.sqrt(k)\n    else:\n        # Default equal\n        amps = np.ones(k)\n    return amps\n\ndef generate_sparse_vector(n, k, pattern):\n    x = np.zeros(n)\n    S = generate_support(n, k)\n    amps = generate_amplitudes(k, pattern)\n    signs = rng.choice([-1.0, 1.0], size=k)\n    x[S] = signs * amps\n    return x\n\ndef gaussian_matrix(m, n):\n    # Scale by 1/sqrt(m) to stabilize columns\n    return rng.normal(loc=0.0, scale=1.0/np.sqrt(m), size=(m, n))\n\ndef basis_pursuit_lp(A, y):\n    m, n = A.shape\n    # Variables: [x (n), u (n)]\n    c = np.hstack([np.zeros(n), np.ones(n)])\n    A_eq = np.hstack([A, np.zeros((m, n))])\n    b_eq = y.copy()\n    # Inequalities:\n    # x - u <= 0 -> [I, -I]\n    # -x - u <= 0 -> [-I, -I]\n    I = np.eye(n)\n    A_ub_top = np.hstack([ I, -I ])\n    A_ub_bottom = np.hstack([ -I, -I ])\n    A_ub = np.vstack([A_ub_top, A_ub_bottom])\n    b_ub = np.zeros(2*n)\n\n    bounds = []\n    # x free: (None, None), u >= 0: (0, None)\n    for _ in range(n):\n        bounds.append((None, None))\n    for _ in range(n):\n        bounds.append((0.0, None))\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n    if not res.success:\n        return None\n    x_rec = res.x[:n]\n    return x_rec\n\ndef irls_lp_equality(A, y, p, max_iter=200, tol=1e-6, eps0=1e-2, gamma=1e-12):\n    m, n = A.shape\n    # Initialize with weighted least squares solution (unweighted)\n    try:\n        G = A @ A.T\n        lam0 = solve(G + gamma*np.eye(m), y, assume_a='sym')\n        x = A.T @ lam0\n    except Exception:\n        # Fallback to least squares via pseudo-inverse\n        x = A.T @ y\n\n    eps = eps0\n    converged = True\n    for it in range(max_iter):\n        # Compute weights\n        w = (np.abs(x)**2 + eps)**(p/2.0 - 1.0)\n        # Avoid singular weights\n        w = np.maximum(w, 1e-12)\n        W_inv = 1.0 / w\n        # Build B = A W^{-1} A^T efficiently by column scaling\n        # A_scaled = A * W_inv along columns\n        A_scaled = A * W_inv\n        B = A_scaled @ A.T\n        try:\n            lam = solve(B + gamma*np.eye(m), y, assume_a='sym')\n        except Exception:\n            converged = False\n            break\n        x_new = W_inv * (A.T @ lam)\n        if not np.all(np.isfinite(x_new)):\n            converged = False\n            break\n        # Check convergence\n        norm_new = np.linalg.norm(x_new)\n        if norm_new == 0:\n            # Degenerate; treat as nonconvergent\n            converged = False\n            break\n        rel_change = np.linalg.norm(x_new - x) / norm_new\n        x = x_new\n        if rel_change < tol:\n            break\n        # Slow decrease of epsilon to allow sharper weights\n        if (it+1) % 10 == 0:\n            eps *= 0.9\n    else:\n        # Max iterations reached\n        converged = False\n    return x, converged\n\ndef relative_error(x, x_true):\n    denom = np.linalg.norm(x_true)\n    if denom == 0:\n        return np.inf\n    return np.linalg.norm(x - x_true) / denom\n\ndef effective_sparsity(x_true, p):\n    a = np.abs(x_true)**p\n    s_num = np.sum(a)**2\n    s_den = np.sum(a**2)\n    if s_den <= 0:\n        return 0.0\n    return s_num / s_den\n\ndef delta_surrogate(n, s_eff):\n    # Convex-like surrogate: delta ≈ 2 s log(n/s) + c s, with c ≈ 7/5.\n    s = max(s_eff, 1e-9)\n    return 2.0 * s * np.log(n / s) + (7.0/5.0) * s\n\ndef run_trials(n, k, m, p, amp_pattern, trials=5, tol=1e-3):\n    succ_l1 = 0\n    succ_lp = 0\n    conv_lp = 0\n    for _ in range(trials):\n        x_true = generate_sparse_vector(n, k, amp_pattern)\n        A = gaussian_matrix(m, n)\n        y = A @ x_true\n\n        # l1 recovery\n        x_l1 = basis_pursuit_lp(A, y)\n        if x_l1 is not None and np.all(np.isfinite(x_l1)):\n            err1 = relative_error(x_l1, x_true)\n            if err1 < tol:\n                succ_l1 += 1\n\n        # lp recovery via IRLS\n        x_lp, converged = irls_lp_equality(A, y, p)\n        if converged:\n            conv_lp += 1\n        if x_lp is not None and np.all(np.isfinite(x_lp)):\n            errp = relative_error(x_lp, x_true)\n            if errp < tol:\n                succ_lp += 1\n\n    rate_l1 = succ_l1 / trials\n    rate_lp = succ_lp / trials\n    conv_rate_lp = conv_lp / trials\n    return rate_l1, rate_lp, conv_rate_lp\n\ndef empirical_critical_m(n, k, p, amp_pattern, m_grid, trials=5, tol=1e-3, threshold=0.6):\n    crit_l1 = np.inf\n    crit_lp = np.inf\n    conv_rates = []\n    for m in sorted(m_grid):\n        rate_l1, rate_lp, conv_rate_lp = run_trials(n, k, m, p, amp_pattern, trials=trials, tol=tol)\n        conv_rates.append(conv_rate_lp)\n        if np.isinf(crit_l1) and rate_l1 >= threshold:\n            crit_l1 = m\n        if np.isinf(crit_lp) and rate_lp >= threshold:\n            crit_lp = m\n    # Instability cost: average failure rate across grid\n    if len(conv_rates) == 0:\n        inst_cost = 1.0\n    else:\n        inst_cost = 1.0 - (np.mean(conv_rates))\n    return crit_l1, crit_lp, inst_cost\n\ndef build_m_grid(n, k, p, amp_pattern):\n    # Use a representative x_true to compute effective sparsity and predicted deltas\n    x_true = generate_sparse_vector(n, k, amp_pattern)\n    s_eff = effective_sparsity(x_true, p)\n    # Surrogates for both methods\n    delta_lp = delta_surrogate(n, s_eff)\n    delta_l1 = delta_surrogate(n, float(k))\n    # Center grid near both predictions\n    m_candidates = set()\n    # Ensure m is within [2, n-1] bounds to be meaningful\n    for delta in [delta_lp, delta_l1]:\n        m0 = int(np.clip(np.round(delta), 2, n-1))\n        m_candidates.update([\n            int(np.clip(np.round(0.9 * m0), 2, n-1)),\n            int(np.clip(m0, 2, n-1)),\n            int(np.clip(np.round(1.1 * m0), 2, n-1)),\n        ])\n    # Also include a slightly larger point to probe above boundary\n    m_candidates.add(int(np.clip(np.round(1.25 * delta_lp), 2, n-1)))\n    return sorted(m_candidates)\n\ndef evaluate_case(n, k, p, amp_pattern):\n    m_grid = build_m_grid(n, k, p, amp_pattern)\n    crit_l1, crit_lp, inst_cost = empirical_critical_m(n, k, p, amp_pattern, m_grid)\n    if np.isinf(crit_l1) or np.isinf(crit_lp):\n        gain_frac = 0.0\n    else:\n        gain = crit_l1 - crit_lp\n        gain_frac = max(gain / n, 0.0)\n    # Gains outweigh costs if normalized gain > instability cost\n    decision = gain_frac > inst_cost\n    return decision\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, k, p, amplitude_pattern)\n        (64, 8, 0.5, \"equal\"),    # Case 1\n        (64, 8, 0.25, \"laplace\"), # Case 2\n        (64, 12, 0.5, \"geo\"),     # Case 3\n        (64, 8, 0.9, \"equal\"),    # Case 4\n    ]\n\n    results = []\n    for n, k, p, pattern in test_cases:\n        decision = evaluate_case(n, k, p, pattern)\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}