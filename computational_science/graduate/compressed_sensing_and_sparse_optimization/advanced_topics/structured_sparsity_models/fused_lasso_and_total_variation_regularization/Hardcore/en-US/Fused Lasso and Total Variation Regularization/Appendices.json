{
    "hands_on_practices": [
        {
            "introduction": "The power of the fused LASSO lies in its ability to simultaneously enforce sparsity and piecewise-constant structure. This practice delves into the mathematical heart of this behavior by examining the problem's optimality conditions . By deriving the solution structure from the principles of subdifferential calculus, you will gain a rigorous understanding of how the interplay between the data fidelity term, the $\\ell_{1}$ penalty, and the total variation penalty determines which coefficients are forced to be exactly zero.",
            "id": "3447208",
            "problem": "Consider the fused Least Absolute Shrinkage and Selection Operator (LASSO) signal approximator, also known as one-dimensional Total Variation (TV) denoising with an additional coordinate-wise sparsity penalty. Let $y \\in \\mathbb{R}^{n}$ be given, and define the optimization problem\n$$\n\\min_{\\beta \\in \\mathbb{R}^{n}} \\; \\frac{1}{2}\\|\\beta - y\\|_{2}^{2} + \\lambda_{1}\\|\\beta\\|_{1} + \\lambda_{2}\\|D\\beta\\|_{1},\n$$\nwhere $\\lambda_{1} \\ge 0$, $\\lambda_{2} \\ge 0$, and $D \\in \\mathbb{R}^{(n-1)\\times n}$ is the first-order forward difference operator whose rows satisfy $(D\\beta)_{i} = \\beta_{i+1} - \\beta_{i}$ for $i \\in \\{1,\\dots,n-1\\}$. Throughout, use the definition of the subdifferential of a proper closed convex function $g:\\mathbb{R}^{p}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ at a point $x \\in \\mathbb{R}^{p}$,\n$$\n\\partial g(x) := \\left\\{\\, v \\in \\mathbb{R}^{p} \\; : \\; g(z) \\ge g(x) + \\langle v, z - x\\rangle \\;\\; \\text{for all } z \\in \\mathbb{R}^{p} \\,\\right\\},\n$$\nand the necessary and sufficient optimality condition for unconstrained convex minimization, namely $0 \\in \\partial f(x^{\\star})$ where $f$ is convex and $x^{\\star}$ is a minimizer.\n\n1) Starting from the subdifferential definition above, write the subdifferential of the $\\ell_{1}$ norm $\\|\\beta\\|_{1}$ at an arbitrary point $\\beta \\in \\mathbb{R}^{n}$, in a coordinate-wise form.\n\n2) Using the optimality condition for the convex objective and properties of subdifferentials of sums, derive a coordinate-wise inclusion that must hold at any minimizer $\\beta^{\\star}$:\n$$\n0 \\in \\beta_{i}^{\\star} - y_{i} + \\lambda_{1} u_{i}^{\\star} + \\lambda_{2}(D^{\\top}v^{\\star})_{i} \\quad \\text{for each } i \\in \\{1,\\dots,n\\},\n$$\nfor some $u^{\\star} \\in \\partial \\|\\beta^{\\star}\\|_{1}$ and some $v^{\\star} \\in \\partial \\|D\\beta^{\\star}\\|_{1}$. Use this to show that, for any fixed $v \\in \\mathbb{R}^{n-1}$, the coordinate $\\beta_{i}^{\\star}$ can be written as the solution of a one-dimensional strongly convex problem with a unique closed form depending on $y_{i}$, $\\lambda_{1}$, and $(D^{\\top}v)_{i}$, and conclude a necessary and sufficient condition under which $\\beta_{i}^{\\star} = 0$ holds exactly.\n\n3) Specialize to $n=4$ and suppose that at an optimal solution the TV subgradient components satisfy $v_{1}^{\\star} = \\frac{1}{2}$, $v_{2}^{\\star} = -\\frac{1}{3}$, and $v_{0}^{\\star} = 0$, $v_{4}^{\\star} = 0$ as boundary conditions for $D^{\\top}$. Provide a single closed-form analytic expression for $\\beta_{2}^{\\star}$ in terms of $y_{2}$, $\\lambda_{1}$, and $\\lambda_{2}$, using only elementary functions such as $\\operatorname{sign}$, $\\max$, and absolute value. Your final answer must be a single analytic expression depending on $y_{2}$, $\\lambda_{1}$, and $\\lambda_{2}$, with no inequalities and no piecewise cases. No rounding is required.",
            "solution": "We begin from foundational principles in convex analysis. The subdifferential of a proper closed convex function $g:\\mathbb{R}^{p}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ at a point $x$ is defined by\n$$\n\\partial g(x) := \\{\\, v \\in \\mathbb{R}^{p} : g(z) \\ge g(x) + \\langle v, z-x\\rangle \\text{ for all } z \\in \\mathbb{R}^{p} \\,\\}.\n$$\nFor the sum $f = f_{1}+f_{2}$ of proper closed convex functions, a standard well-tested subdifferential sum rule states that\n$$\n\\partial f(x) \\subseteq \\partial f_{1}(x) + \\partial f_{2}(x),\n$$\nwith equality holding under mild regularity such as continuity of one summand at $x$. For unconstrained convex minimization, a point $x^{\\star}$ minimizes $f$ if and only if $0 \\in \\partial f(x^{\\star})$.\n\nStep 1: Subdifferential of the $\\ell_{1}$ norm. Consider $g(\\beta) = \\|\\beta\\|_{1} = \\sum_{i=1}^{n} |\\beta_{i}|$. The subdifferential is separable across coordinates:\n$$\n\\partial \\|\\beta\\|_{1} = \\prod_{i=1}^{n} \\partial |\\beta_{i}|,\n$$\nwhere, for a scalar $t \\in \\mathbb{R}$, the subdifferential of the absolute value is\n$$\n\\partial |t| = \n\\begin{cases}\n\\{ \\operatorname{sign}(t) \\},  \\text{if } t \\neq 0,\\\\\n[-1,1],  \\text{if } t = 0.\n\\end{cases}\n$$\nTherefore, in coordinate-wise form,\n$$\n\\partial \\|\\beta\\|_{1} = \\left\\{\\, u \\in \\mathbb{R}^{n} : u_{i} =\n\\begin{cases}\n\\operatorname{sign}(\\beta_{i}),  \\text{if } \\beta_{i} \\neq 0,\\\\\n\\xi \\text{ with } \\xi \\in [-1,1],  \\text{if } \\beta_{i} = 0,\n\\end{cases}\n\\text{ for all } i \\in \\{1,\\dots,n\\}\\,\\right\\}.\n$$\n\nStep 2: Optimality for the fused Least Absolute Shrinkage and Selection Operator (LASSO) objective. Define the objective\n$$\nF(\\beta) := \\frac{1}{2}\\|\\beta - y\\|_{2}^{2} + \\lambda_{1}\\|\\beta\\|_{1} + \\lambda_{2}\\|D\\beta\\|_{1}.\n$$\nThe first term is smooth with gradient $\\nabla \\left(\\frac{1}{2}\\|\\beta - y\\|_{2}^{2}\\right) = \\beta - y$. The second and third terms are convex but generally nonsmooth. By the subdifferential sum rule,\n$$\n\\partial F(\\beta) = (\\beta - y) + \\lambda_{1}\\,\\partial \\|\\beta\\|_{1} + \\lambda_{2}\\, D^{\\top} \\partial \\|D\\beta\\|_{1},\n$$\nwhere we have used the chain rule for subdifferentials of linear mappings: $\\partial \\|D\\beta\\|_{1} = D^{\\top} \\partial \\|z\\|_{1}\\big|_{z = D\\beta}$. Thus, at a minimizer $\\beta^{\\star}$, there exist $u^{\\star} \\in \\partial \\|\\beta^{\\star}\\|_{1}$ and $v^{\\star} \\in \\partial \\|D\\beta^{\\star}\\|_{1}$ such that the necessary and sufficient optimality condition holds:\n$$\n0 \\in \\beta^{\\star} - y + \\lambda_{1} u^{\\star} + \\lambda_{2} D^{\\top} v^{\\star}.\n$$\nEquivalently, in coordinates, for each $i \\in \\{1,\\dots,n\\}$,\n$$\n0 \\in \\beta_{i}^{\\star} - y_{i} + \\lambda_{1} u_{i}^{\\star} + \\lambda_{2} (D^{\\top} v^{\\star})_{i}.\n$$\nBecause $D$ is the first-order forward difference, $(D\\beta)_{i} = \\beta_{i+1} - \\beta_{i}$ for $i \\in \\{1,\\dots,n-1\\}$. A direct calculation shows that\n$$\nD^{\\top} v = \\begin{bmatrix}\n- v_{1} \\\\\nv_{1} - v_{2} \\\\\n\\vdots \\\\\nv_{n-2} - v_{n-1} \\\\\nv_{n-1}\n\\end{bmatrix},\n$$\nso that for interior indices $i \\in \\{2,\\dots,n-1\\}$ one has $(D^{\\top} v)_{i} = v_{i-1} - v_{i}$, and at boundaries $(D^{\\top} v)_{1} = -v_{1}$, $(D^{\\top} v)_{n} = v_{n-1}$.\n\nFix any $v \\in \\mathbb{R}^{n-1}$ and define the shifted data\n$$\nz_{i} := y_{i} - \\lambda_{2} (D^{\\top} v)_{i}, \\quad \\text{for } i \\in \\{1,\\dots,n\\}.\n$$\nThen the coordinate-wise optimality inclusion becomes\n$$\n0 \\in \\beta_{i}^{\\star} - z_{i} + \\lambda_{1} u_{i}^{\\star}, \\quad \\text{with } u_{i}^{\\star} \\in \\partial |\\beta_{i}^{\\star}|.\n$$\nThis is exactly the first-order optimality condition for the scalar problem in the single variable $b$,\n$$\n\\min_{b \\in \\mathbb{R}} \\; \\frac{1}{2}(b - z_{i})^{2} + \\lambda_{1} |b|.\n$$\nThis problem has a unique closed-form solution given by the soft-thresholding operation,\n$$\nb^{\\star} = \\operatorname{sign}(z_{i}) \\max\\left(|z_{i}| - \\lambda_{1}, 0\\right).\n$$\nTherefore, for any fixed $v$,\n$$\n\\beta_{i}^{\\star} = \\operatorname{sign}\\!\\left(y_{i} - \\lambda_{2}(D^{\\top} v)_{i}\\right) \\max\\!\\left(\\left|y_{i} - \\lambda_{2}(D^{\\top} v)_{i}\\right| - \\lambda_{1}, 0\\right).\n$$\nFrom this representation, $\\beta_{i}^{\\star} = 0$ if and only if the thresholding condition\n$$\n\\left|y_{i} - \\lambda_{2}(D^{\\top} v^{\\star})_{i}\\right| \\le \\lambda_{1}\n$$\nholds with $v^{\\star} \\in \\partial \\|D\\beta^{\\star}\\|_{1}$. This precisely exhibits how exact zeros can occur even in the presence of the TV coupling: the neighboring-difference subgradient $(D^{\\top} v^{\\star})_{i}$ shifts the effective datum $y_{i}$ before the $\\ell_{1}$ shrinkage, and if the shifted magnitude is at most $\\lambda_{1}$, the coordinate is zero.\n\nStep 3: Specialization to $n=4$ and the requested expression. For $n=4$, suppose the optimal TV subgradient satisfies $v_{1}^{\\star} = \\frac{1}{2}$, $v_{2}^{\\star} = -\\frac{1}{3}$, and boundary conditions $v_{0}^{\\star} = 0$, $v_{4}^{\\star} = 0$. For $i=2$ (an interior index), we have\n$$\n(D^{\\top} v^{\\star})_{2} = v_{1}^{\\star} - v_{2}^{\\star} = \\frac{1}{2} - \\left(-\\frac{1}{3}\\right) = \\frac{5}{6}.\n$$\nTherefore,\n$$\n\\beta_{2}^{\\star} = \\operatorname{sign}\\!\\left(y_{2} - \\lambda_{2}\\cdot \\frac{5}{6}\\right) \\max\\!\\left(\\left|y_{2} - \\lambda_{2}\\cdot \\frac{5}{6}\\right| - \\lambda_{1}, 0\\right).\n$$\nThis is a single closed-form analytic expression in the variables $y_{2}$, $\\lambda_{1}$, and $\\lambda_{2}$, and it encodes the exact-zero condition implicitly via the outer $\\max$.",
            "answer": "$$\\boxed{\\operatorname{sign}\\left(y_{2} - \\frac{5}{6}\\lambda_{2}\\right)\\,\\max\\left(\\left|y_{2} - \\frac{5}{6}\\lambda_{2}\\right| - \\lambda_{1},\\, 0\\right)}$$"
        },
        {
            "introduction": "While the general fused LASSO problem requires iterative algorithms, the special case with an identity design matrix offers a path to a solution that provides invaluable intuition. This exercise guides you through a powerful two-step solution technique that first addresses the total variation term to find a piecewise-constant signal, and then applies the soft-thresholding operator to enforce sparsity . Working through this concrete example solidifies the theoretical concepts of penalty decoupling and demonstrates how to calculate an exact solution from first principles.",
            "id": "3447151",
            "problem": "Consider the fused Least Absolute Shrinkage and Selection Operator (LASSO) signal approximator in one dimension with identity design matrix. Let the observation vector be $y \\in \\mathbb{R}^{6}$ with components $y_{1}=3$, $y_{2}=3$, $y_{3}=3$, $y_{4}=0$, $y_{5}=0$, $y_{6}=2$. The estimator $\\hat{\\beta} \\in \\mathbb{R}^{6}$ is defined as the unique minimizer of the strictly convex objective\n$$\n\\frac{1}{2}\\| \\beta - y \\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\lambda_{2} \\sum_{i=1}^{5} |\\beta_{i+1} - \\beta_{i}|,\n$$\nwhere $\\lambda_{1} = \\frac{1}{2}$ and $\\lambda_{2} = \\frac{1}{5}$. The total variation operator is the first-order discrete difference, and the fused LASSO penalty is the sum of absolute differences of adjacent coefficients.\n\nStarting from foundational optimality conditions for convex optimization (specifically, subgradient Karush–Kuhn–Tucker (KKT) conditions), derive from first principles why, in the identity-design setting, one can first solve the Total Variation (TV) denoising subproblem\n$$\n\\min_{u \\in \\mathbb{R}^{6}} \\;\\frac{1}{2}\\|u - y\\|_{2}^{2} + \\lambda_{2} \\sum_{i=1}^{5} |u_{i+1} - u_{i}|\n$$\nto obtain a piecewise-constant preliminary signal $u$, and then obtain $\\hat{\\beta}$ by applying the soft-thresholding operator pointwise to $u$, defined for each coordinate by $S_{\\lambda_{1}}(x) = \\mathrm{sign}(x)\\max\\{|x| - \\lambda_{1}, 0\\}$.\n\nProceed as follows:\n- Identify the constant segments in the TV solution $u$ by verifying consistency of jump signs through subgradient optimality. For each identified segment, denote its constant plateau value by $c$.\n- Using subgradient balance over each segment (derived by summing the KKT stationarity conditions across the segment), solve for the plateau values $c$ as exact rational numbers.\n- Apply the soft-thresholding operator $S_{\\lambda_{1}}$ to the components of $u$ to obtain the fused LASSO estimator $\\hat{\\beta}$.\n\nExpress the final answer as a single row vector containing the six components of $\\hat{\\beta}$, in exact rational form. No rounding is required, and no units are involved.",
            "solution": "The problem asks for the solution to a fused LASSO optimization problem with an identity design matrix. The objective function is\n$$\nL(\\beta) = \\frac{1}{2}\\| \\beta - y \\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\lambda_{2} \\|D\\beta\\|_{1}\n$$\nwhere $\\beta \\in \\mathbb{R}^{6}$, $y = (3, 3, 3, 0, 0, 2)^T$, $\\lambda_{1} = \\frac{1}{2}$, $\\lambda_{2} = \\frac{1}{5}$, and $D$ is the first-difference operator.\n\nThe key insight for this problem structure is that the solution can be found via a two-step procedure. This is a property of the proximal operators associated with the $\\ell_1$ and TV norms. First, solve the TV denoising subproblem to get an intermediate solution $u$:\n$$\nu = \\arg\\min_{u' \\in \\mathbb{R}^6} \\left\\{ \\frac{1}{2}\\|u' - y\\|_{2}^{2} + \\lambda_{2}\\|Du'\\|_1 \\right\\}\n$$\nSecond, obtain the final solution $\\hat{\\beta}$ by applying the coordinate-wise soft-thresholding operator to $u$:\n$$\n\\hat{\\beta} = S_{\\lambda_1}(u) \\quad \\text{where } (S_{\\lambda_1}(u))_i = \\operatorname{sign}(u_i)\\max\\{|u_i| - \\lambda_1, 0\\}\n$$\nThe validity of this decomposition rests on the fact that the subgradient of the TV norm at the final solution $\\hat{\\beta}$ can be related to the subgradient at the intermediate solution $u$. Because the soft-thresholding operator is non-decreasing, the sign of any difference $u_{i+1}-u_i$ is either preserved in $\\hat{\\beta}_{i+1}-\\hat{\\beta}_i$ or the difference becomes zero. In either case, a valid subgradient for the TV norm at $u$ remains valid at $\\hat{\\beta}$.\n\n**Step 1: Solve the TV denoising subproblem**\nWe solve for $u$ with $\\lambda_2 = \\frac{1}{5}$. The optimality (KKT) conditions for $u$ are:\n$$\nu_i - y_i + \\lambda_2 (v_{i-1} - v_i) = 0, \\quad \\text{for } i=1, \\dots, 6\n$$\nwhere $v_i \\in \\partial|u_{i+1}-u_i|$ for $i=1,\\dots,5$, and by convention $v_0 = v_6 = 0$. The solution $u$ will be piecewise constant. Inspecting $y$, we hypothesize jumps after indices 3 and 5, leading to three constant segments: $u_1=u_2=u_3=c_1$, $u_4=u_5=c_2$, and $u_6=c_3$. Based on $y$, we assume $c_1 > c_2$ and $c_3 > c_2$, so we set the subgradients at the jumps to $v_3 = \\operatorname{sign}(c_2-c_1) = -1$ and $v_5=\\operatorname{sign}(c_3-c_2)=1$.\n\nWe find the plateau values by summing the KKT conditions over each segment:\nFor segment 1 ($i=1, 2, 3$): $\\sum_{i=1}^3 (u_i - y_i) + \\lambda_2 \\sum_{i=1}^3 (v_{i-1}-v_i) = 0$.\nThis simplifies to $(3c_1 - (y_1+y_2+y_3)) + \\lambda_2 (v_0-v_3) = 0$.\n$3c_1 - 9 + \\frac{1}{5}(0 - (-1)) = 0 \\implies 3c_1 = 9 - \\frac{1}{5} = \\frac{44}{5} \\implies c_1 = \\frac{44}{15}$.\n\nFor segment 2 ($i=4, 5$): $\\sum_{i=4}^5 (u_i - y_i) + \\lambda_2 \\sum_{i=4}^5 (v_{i-1}-v_i) = 0$.\nThis simplifies to $(2c_2 - (y_4+y_5)) + \\lambda_2 (v_3-v_5) = 0$.\n$2c_2 - 0 + \\frac{1}{5}(-1 - 1) = 0 \\implies 2c_2 = \\frac{2}{5} \\implies c_2 = \\frac{1}{5}$.\n\nFor segment 3 ($i=6$): $(u_6 - y_6) + \\lambda_2 (v_5-v_6) = 0$.\n$c_3 - 2 + \\frac{1}{5}(1 - 0) = 0 \\implies c_3 = 2 - \\frac{1}{5} = \\frac{9}{5}$.\n\nThe results $c_1 = \\frac{44}{15} \\approx 2.93$, $c_2 = \\frac{1}{5}=0.2$, $c_3=\\frac{9}{5}=1.8$ are consistent with our assumptions $c_1>c_2$ and $c_3>c_2$. We must also verify that the subgradients $v_1, v_2, v_4$ for the flat segments are in $[-1,1]$.\n$i=1: u_1-y_1 + \\lambda_2(v_0-v_1)=0 \\implies \\frac{44}{15}-3 + \\frac{1}{5}(0-v_1)=0 \\implies -\\frac{1}{15} = \\frac{1}{5}v_1 \\implies v_1 = -\\frac{1}{3}$. Valid since $|v_1| \\le 1$.\n$i=2: u_2-y_2 + \\lambda_2(v_1-v_2)=0 \\implies \\frac{44}{15}-3 + \\frac{1}{5}(-\\frac{1}{3}-v_2)=0 \\implies -\\frac{1}{15} = -\\frac{1}{5}(\\frac{1}{3}+v_2) \\implies \\frac{1}{3}=\\frac{1}{3}+v_2 \\implies v_2 = 0$. Valid since $|v_2| \\le 1$.\n$i=4: u_4-y_4 + \\lambda_2(v_3-v_4)=0 \\implies \\frac{1}{5}-0 + \\frac{1}{5}(-1-v_4)=0 \\implies 1+(-1-v_4)=0 \\implies v_4 = 0$. Valid since $|v_4|\\le 1$.\nAll conditions are satisfied. The TV denoising solution is $u = (\\frac{44}{15}, \\frac{44}{15}, \\frac{44}{15}, \\frac{1}{5}, \\frac{1}{5}, \\frac{9}{5})^T$.\n\n**Step 2: Apply soft-thresholding**\nWe obtain $\\hat{\\beta}$ by applying $S_{\\lambda_1}(x)$ to each component of $u$, with $\\lambda_1 = \\frac{1}{2}$. Since all components of $u$ are positive, $\\hat{\\beta}_i = \\max(u_i - \\lambda_1, 0)$.\n\nFor $i=1, 2, 3$:\n$\\hat{\\beta}_i = \\max(\\frac{44}{15} - \\frac{1}{2}, 0) = \\max(\\frac{88 - 15}{30}, 0) = \\frac{73}{30}$.\n\nFor $i=4, 5$:\n$\\hat{\\beta}_i = \\max(\\frac{1}{5} - \\frac{1}{2}, 0) = \\max(\\frac{2-5}{10}, 0) = \\max(-\\frac{3}{10}, 0) = 0$.\n\nFor $i=6$:\n$\\hat{\\beta}_6 = \\max(\\frac{9}{5} - \\frac{1}{2}, 0) = \\max(\\frac{18-5}{10}, 0) = \\frac{13}{10}$.\n\nThe fused LASSO estimator is $\\hat{\\beta} = (\\frac{73}{30}, \\frac{73}{30}, \\frac{73}{30}, 0, 0, \\frac{13}{10})^T$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{73}{30}  \\frac{73}{30}  \\frac{73}{30}  0  0  \\frac{13}{10}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Total variation regularization finds one of its most compelling applications in image processing, where it is used to remove noise while preserving sharp edges. This practice extends the concept from one-dimensional signals to two-dimensional images, introducing the critical distinction between anisotropic and isotropic TV norms . By calculating both for a small sample image, you will develop a clear intuition for how each regularizer penalizes changes differently and why this choice has a significant impact on the structure of the recovered image.",
            "id": "3447189",
            "problem": "Consider a two-dimensional discrete image represented on a $2 \\times 2$ grid with homogeneous Neumann boundary conditions (zero normal gradient at the boundary). Let the grayscale intensity array $u$ be\n$$\nu \\;=\\;\n\\begin{pmatrix}\n0  2 \\\\\n3  1\n\\end{pmatrix}.\n$$\nUse the forward-difference discrete gradient: for each pixel $(i,j)$, the horizontal component is $D_{x}u_{i,j} = u_{i,j+1} - u_{i,j}$ if $j2$ and $D_{x}u_{i,2} = 0$, and the vertical component is $D_{y}u_{i,j} = u_{i+1,j} - u_{i,j}$ if $i2$ and $D_{y}u_{2,j} = 0$. The anisotropic total variation $\\mathrm{TV}_{\\mathrm{aniso}}$ uses the one-norm of the discrete gradient components at each pixel, whereas the isotropic total variation $\\mathrm{TV}_{\\mathrm{iso}}$ uses the two-norm (Euclidean norm) of the discrete gradient at each pixel. Starting from these core definitions of discrete gradients and norms, compute $\\mathrm{TV}_{\\mathrm{aniso}}$ and $\\mathrm{TV}_{\\mathrm{iso}}$ for the given image. Then, based on first principles of norm inequalities, determine which directional differences (horizontal-only, vertical-only, or simultaneous horizontal and vertical at a pixel) are penalized more strongly by $\\mathrm{TV}_{\\mathrm{aniso}}$ versus $\\mathrm{TV}_{\\mathrm{iso}}$, and explain why this is consistent with the structure of the fused Least Absolute Shrinkage and Selection Operator (LASSO) penalty in compressed sensing. Provide the values of $\\mathrm{TV}_{\\mathrm{aniso}}$ and $\\mathrm{TV}_{\\mathrm{iso}}$ in exact form without rounding.",
            "solution": "The problem is well-posed and scientifically grounded, providing all necessary definitions and data for a unique solution. We proceed with the solution.\n\nThe image is represented by a $2 \\times 2$ array of intensities $u$, where the indices $(i,j)$ refer to the row and column, respectively, starting from $1$. The given image is:\n$$\nu = \\begin{pmatrix} u_{1,1}  u_{1,2} \\\\ u_{2,1}  u_{2,2} \\end{pmatrix} = \\begin{pmatrix} 0  2 \\\\ 3  1 \\end{pmatrix}\n$$\nThe discrete gradient at each pixel $(i,j)$ is a vector $(\\nabla u)_{i,j} = (D_x u_{i,j}, D_y u_{i,j})$. The components are defined by the forward-difference scheme with homogeneous Neumann boundary conditions.\nFor a $2 \\times 2$ grid (where the maximum index is $2$), the definitions are:\n$D_x u_{i,j} = u_{i,j+1} - u_{i,j}$ for $j  2$, and $D_x u_{i,2} = 0$.\n$D_y u_{i,j} = u_{i+1,j} - u_{i,j}$ for $i  2$, and $D_y u_{2,j} = 0$.\n\nWe compute the discrete gradient vector at each of the four pixels:\n1.  For pixel $(1,1)$:\n    $D_x u_{1,1} = u_{1,2} - u_{1,1} = 2 - 0 = 2$.\n    $D_y u_{1,1} = u_{2,1} - u_{1,1} = 3 - 0 = 3$.\n    So, $(\\nabla u)_{1,1} = (2, 3)$.\n\n2.  For pixel $(1,2)$:\n    $D_x u_{1,2} = 0$ (boundary condition as $j=2$).\n    $D_y u_{1,2} = u_{2,2} - u_{1,2} = 1 - 2 = -1$.\n    So, $(\\nabla u)_{1,2} = (0, -1)$.\n\n3.  For pixel $(2,1)$:\n    $D_x u_{2,1} = u_{2,2} - u_{2,1} = 1 - 3 = -2$.\n    $D_y u_{2,1} = 0$ (boundary condition as $i=2$).\n    So, $(\\nabla u)_{2,1} = (-2, 0)$.\n\n4.  For pixel $(2,2)$:\n    $D_x u_{2,2} = 0$ (boundary condition as $j=2$).\n    $D_y u_{2,2} = 0$ (boundary condition as $i=2$).\n    So, $(\\nabla u)_{2,2} = (0, 0)$.\n\nThe anisotropic total variation, $\\mathrm{TV}_{\\mathrm{aniso}}$, is the sum of the $\\ell_1$-norms of the gradient vectors at each pixel:\n$$\n\\mathrm{TV}_{\\mathrm{aniso}}(u) = \\sum_{i=1}^2 \\sum_{j=1}^2 \\|(\\nabla u)_{i,j}\\|_1 = \\sum_{i=1}^2 \\sum_{j=1}^2 \\left( |D_x u_{i,j}| + |D_y u_{i,j}| \\right)\n$$\nSubstituting the computed values:\n$\\mathrm{TV}_{\\mathrm{aniso}}(u) = \\|(2, 3)\\|_1 + \\|(0, -1)\\|_1 + \\|(-2, 0)\\|_1 + \\|(0, 0)\\|_1$\n$\\mathrm{TV}_{\\mathrm{aniso}}(u) = (|2| + |3|) + (|0| + |-1|) + (|-2| + |0|) + (|0| + |0|)$\n$\\mathrm{TV}_{\\mathrm{aniso}}(u) = (2 + 3) + (0 + 1) + (2 + 0) + 0 = 5 + 1 + 2 = 8$.\n\nThe isotropic total variation, $\\mathrm{TV}_{\\mathrm{iso}}$, is the sum of the $\\ell_2$-norms (Euclidean norms) of the gradient vectors at each pixel:\n$$\n\\mathrm{TV}_{\\mathrm{iso}}(u) = \\sum_{i=1}^2 \\sum_{j=1}^2 \\|(\\nabla u)_{i,j}\\|_2 = \\sum_{i=1}^2 \\sum_{j=1}^2 \\sqrt{(D_x u_{i,j})^2 + (D_y u_{i,j})^2}\n$$\nSubstituting the computed values:\n$\\mathrm{TV}_{\\mathrm{iso}}(u) = \\|(2, 3)\\|_2 + \\|(0, -1)\\|_2 + \\|(-2, 0)\\|_2 + \\|(0, 0)\\|_2$\n$\\mathrm{TV}_{\\mathrm{iso}}(u) = \\sqrt{2^2 + 3^2} + \\sqrt{0^2 + (-1)^2} + \\sqrt{(-2)^2 + 0^2} + \\sqrt{0^2 + 0^2}$\n$\\mathrm{TV}_{\\mathrm{iso}}(u) = \\sqrt{4 + 9} + \\sqrt{1} + \\sqrt{4} + 0 = \\sqrt{13} + 1 + 2 = 3 + \\sqrt{13}$.\n\nNext, we analyze which directional differences are penalized more strongly by each TV variant. This comparison is rooted in the properties of the $\\ell_1$ and $\\ell_2$ norms in $\\mathbb{R}^2$. Let the gradient vector at a pixel be $\\mathbf{g} = (g_x, g_y)$. The anisotropic penalty is $\\|\\mathbf{g}\\|_1 = |g_x| + |g_y|$ and the isotropic penalty is $\\|\\mathbf{g}\\|_2 = \\sqrt{g_x^2 + g_y^2}$.\nThe fundamental relationship between these two norms is $\\|\\mathbf{g}\\|_2 \\le \\|\\mathbf{g}\\|_1$. Equality holds if and only if one of the components of $\\mathbf{g}$ is zero.\n\nCase 1: The gradient is aligned with a coordinate axis (horizontal-only or vertical-only difference). For example, $\\mathbf{g} = (g_x, 0)$ with $g_x \\neq 0$.\nThe anisotropic penalty is $|g_x| + |0| = |g_x|$.\nThe isotropic penalty is $\\sqrt{g_x^2 + 0^2} = |g_x|$.\nIn this case, the penalties are identical.\n\nCase 2: The gradient has both horizontal and vertical components (e.g., a \"diagonal\" difference), so $g_x \\neq 0$ and $g_y \\neq 0$.\nThe anisotropic penalty is $|g_x| + |g_y|$.\nThe isotropic penalty is $\\sqrt{g_x^2 + g_y^2}$.\nHere, the strict inequality $\\|\\mathbf{g}\\|_2  \\|\\mathbf{g}\\|_1$ holds. For example, if $\\mathbf{g} = (c, c)$ for some constant $c \\neq 0$, then $\\|\\mathbf{g}\\|_1 = 2|c|$ while $\\|\\mathbf{g}\\|_2 = \\sqrt{c^2+c^2} = \\sqrt{2}|c|$. Clearly, $2|c|  \\sqrt{2}|c|$.\n\nThis demonstrates that for a gradient of a given Euclidean magnitude $\\|\\mathbf{g}\\|_2$, the $\\ell_1$ norm (anisotropic penalty) is maximized when the gradient is equally distributed between components (e.g., along the diagonal) and minimized when it is aligned with an axis. Therefore, $\\mathrm{TV}_{\\mathrm{aniso}}$ penalizes simultaneous horizontal and vertical differences more harshly relative to its penalization of axis-aligned differences. This property encourages solutions where gradients are sparse and aligned with the coordinate axes, which can lead to \"blocky\" or \"staircase\" artifacts in image reconstruction problems. In contrast, $\\mathrm{TV}_{\\mathrm{iso}}$ is rotationally invariant, penalizing only the magnitude of the gradient, not its direction.\n\nThe connection to the fused LASSO is direct. The fused LASSO penalty for a one-dimensional signal $\\mathbf{x} = (x_1, \\ldots, x_n)$ includes a term of the form $\\lambda \\sum_{i=2}^n |x_i - x_{i-1}|$. This term is precisely the one-dimensional anisotropic total variation of the signal $\\mathbf{x}$. Its purpose is to promote piecewise-constant solutions by penalizing differences between adjacent coefficients.\n\nA direct generalization of this penalty to a two-dimensional image $u$ is to penalize differences along both dimensions separately:\n$$\nP(u) = \\lambda_v \\sum_{i,j} |u_{i+1,j} - u_{i,j}| + \\lambda_h \\sum_{i,j} |u_{i,j+1} - u_{i,j}|\n$$\nIf $\\lambda_v = \\lambda_h = \\lambda$, this becomes $\\lambda \\sum_{i,j} (|u_{i+1,j} - u_{i,j}| + |u_{i,j+1} - u_{i,j}|)$, which is exactly $\\lambda \\cdot \\mathrm{TV}_{\\mathrm{aniso}}(u)$. The structure of this penalty is a sum of absolute values of individual difference components, which is characteristic of the LASSO's $\\ell_1$-norm penalty. This separable structure is precisely what leads to the anisotropic behavior analyzed above: it penalizes diagonal gradients more strongly than axis-aligned gradients, a behavior inherited from the properties of the $\\ell_1$ norm. Thus, the fused LASSO is structurally an application of anisotropic total variation regularization.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 8  3 + \\sqrt{13} \\end{pmatrix}}\n$$"
        }
    ]
}