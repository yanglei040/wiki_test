{
    "hands_on_practices": [
        {
            "introduction": "许多用于重叠组 LASSO 的高效算法都依赖于一种“潜变量”或“变量分裂”的技巧，它将复杂的非光滑惩罚项分解为更容易处理的部分。本练习将通过一个具体的例子，指导您手动构建此公式的核心组件，例如复制矩阵和共识约束。掌握这些基本构造是理解和实现相关优化算法（如交替方向乘子法 ADMM）的关键第一步。",
            "id": "3465480",
            "problem": "考虑在压缩感知和稀疏优化中使用的潜变量表示法中的重叠组最小绝对收缩和选择算子 (LASSO) 惩罚项。设参数向量为 $\\beta \\in \\mathbb{R}^{p}$，其中 $p=5$，重叠组为 $g_1=\\{1,2,3\\}$，$g_2=\\{3,4\\}$ 和 $g_3=\\{4,5\\}$。在潜变量重构中，对每个组 $g$，引入一个组限制的潜变量 $\\beta^{(g)} \\in \\mathbb{R}^{|g|}$，它存储由 $g$ 索引的 $\\beta$ 分量的副本。然后通过按 $(g_1, g_2, g_3)$ 的顺序拼接 $\\beta^{(g)}$ 来定义堆叠的潜变量向量 $z \\in \\mathbb{R}^{\\sum_{g} |g|}$。重复映射 $D \\in \\mathbb{R}^{(\\sum_{g} |g|)\\times p}$ 由线性关系 $z = D \\beta$ 定义，其中 $D$ 的每一行选择与该行索引的潜变量副本相对应的 $\\beta$ 的适当分量。\n\n从上述重复映射的基本定义和潜变量的构建出发，完成以下任务：\n- 显式地用 $\\beta$ 的元素写出潜变量向量 $\\beta^{(g_1)}$、$\\beta^{(g_2)}$ 和 $\\beta^{(g_3)}$。\n- 显式地构造矩阵 $D$。\n- 以矩阵形式写出连接 $z$ 和 $\\beta$ 的共识约束。\n\n然后，仅使用线性代数运算，不借助任何预先给定的简化公式，推导出 $D^{\\top} D$ 并计算其行列式。将该行列式作为您的最终答案。如果最终结果是数字，请给出精确值，无需四舍五入。",
            "solution": "我们从重叠组的潜变量构造开始：对每个组 $g \\subset \\{1,\\dots,p\\}$，我们引入一个潜变量向量 $\\beta^{(g)} \\in \\mathbb{R}^{|g|}$，它存储由 $g$ 索引的 $\\beta$ 分量的副本。堆叠的潜变量向量形成为 $z = \\big(\\beta^{(g_1)};\\beta^{(g_2)};\\beta^{(g_3)}\\big) \\in \\mathbb{R}^{|g_1|+|g_2|+|g_3|}$，其中我们用分号表示垂直拼接。\n\n给定 $p=5$ 和组 $g_1=\\{1,2,3\\}$，$g_2=\\{3,4\\}$ 和 $g_3=\\{4,5\\}$，潜变量是 $\\beta = (\\beta_1,\\beta_2,\\beta_3,\\beta_4,\\beta_5)^{\\top}$ 的组限制副本：\n- 对于 $g_1=\\{1,2,3\\}$，潜变量向量为\n$$\n\\beta^{(g_1)} = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix}.\n$$\n- 对于 $g_2=\\{3,4\\}$，潜变量向量为\n$$\n\\beta^{(g_2)} = \\begin{pmatrix} \\beta_3 \\\\ \\beta_4 \\end{pmatrix}.\n$$\n- 对于 $g_3=\\{4,5\\}$，潜变量向量为\n$$\n\\beta^{(g_3)} = \\begin{pmatrix} \\beta_4 \\\\ \\beta_5 \\end{pmatrix}.\n$$\n\n按 $(g_1,g_2,g_3)$ 的顺序拼接得到堆叠的潜变量向量 $z \\in \\mathbb{R}^{7}$：\n$$\nz \\;=\\; \\begin{pmatrix}\n\\beta^{(g_1)} \\\\ \\beta^{(g_2)} \\\\ \\beta^{(g_3)}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_4 \\\\ \\beta_5\n\\end{pmatrix}.\n$$\n\n根据重复映射的定义，存在一个矩阵 $D \\in \\mathbb{R}^{7 \\times 5}$ 使得 $z = D \\beta$。$D$ 的每一行在与被复制的 $\\beta$ 索引相对应的列中放置一个 $1$，在其他位置放置零。按照上面写出的 $z$ 中元素的顺序，$D$ 的七行是：\n- 第 1 行对应于 $\\beta_1$，\n- 第 2 行对应于 $\\beta_2$，\n- 第 3 行和第 4 行都对应于 $\\beta_3$（由于 $g_1$ 和 $g_2$ 之间的重叠），\n- 第 5 行和第 6 行都对应于 $\\beta_4$（由于 $g_2$ 和 $g_3$ 之间的重叠），\n- 第 7 行对应于 $\\beta_5$。\n\n因此，\n$$\nD \\;=\\;\n\\begin{pmatrix}\n1  0  0  0  0 \\\\\n0  1  0  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  0  1  0 \\\\\n0  0  0  1  0 \\\\\n0  0  0  0  1\n\\end{pmatrix}.\n$$\n\n将潜变量副本与原始变量联系起来的共识约束正是线性关系\n$$\nz - D \\beta \\;=\\; 0,\n$$\n等价于 $z = D \\beta$。这确保了共享坐标的所有组级别副本都等于相应的原始坐标。以坐标形式，这强制了诸如索引 3 的两个副本等于 $\\beta_3$，以及索引 4 的两个副本等于 $\\beta_4$ 这样的等式。\n\n接下来，我们计算 $D^{\\top} D$。根据构造，$D$ 的每一行只有一个元素等于 $1$，所有其他元素都为 $0$，并且当一个原始索引出现在多个组中时，就会出现重复。因此，乘积 $D^{\\top} D$ 是一个对角矩阵，其 $(i,i)$ 项等于索引 $i$ 在所有组中出现的次数。我们显式地验证这一点。\n\n令 $e_i \\in \\mathbb{R}^{5}$ 为第 $i$ 个标准基向量。$D$ 的每一行都是某个 $e_i^{\\top}$。如果一个索引 $i$ 在堆叠的潜变量向量 $z$ 中（即在各组中）出现 $c_i$ 次，那么 $D$ 就包含 $c_i$ 个 $e_i^{\\top}$ 行的副本。因此，\n$$\nD^{\\top} D \\;=\\; \\sum_{k=1}^{7} r_k^{\\top} r_k \\;=\\; \\sum_{i=1}^{5} c_i \\, e_i e_i^{\\top} \\;=\\; \\operatorname{diag}(c_1, c_2, c_3, c_4, c_5),\n$$\n其中 $r_k$ 表示 $D$ 的第 $k$ 行。\n\n从组结构中计数出现次数：\n- 索引 $1$ 仅出现在 $g_1$ 中，所以 $c_1 = 1$。\n- 索引 $2$ 仅出现在 $g_1$ 中，所以 $c_2 = 1$。\n- 索引 $3$ 出现在 $g_1$ 和 $g_2$ 中，所以 $c_3 = 2$。\n- 索引 $4$ 出现在 $g_2$ 和 $g_3$ 中，所以 $c_4 = 2$。\n- 索引 $5$ 仅出现在 $g_3$ 中，所以 $c_5 = 1$。\n\n因此，\n$$\nD^{\\top} D \\;=\\; \\operatorname{diag}(1,\\,1,\\,2,\\,2,\\,1).\n$$\n\n最后，对角矩阵的行列式是其对角元素的乘积。因此，\n$$\n\\det(D^{\\top} D) \\;=\\; 1 \\times 1 \\times 2 \\times 2 \\times 1 \\;=\\; 4.\n$$\n\n这就是所要求的标量值。",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "在理解了重叠组 LASSO 的代数结构之后，我们自然会问：它究竟能产生什么样的解？本练习将引导您运用次梯度和 Karush-Kuhn-Tucker (KKT) 最优性条件来分析一个精心设计的实例。通过求解，您将亲眼见证该方法的一个独特优势——选择不严格局限于预定义分组的特征组合，从而揭示其在建模中的高度灵活性。",
            "id": "3465473",
            "problem": "考虑一个维度为 $p=4$ 的重叠组最小绝对收缩和选择算子 (LASSO) 问题，该问题采用平方误差数据拟合项和两个重叠的组。设决策变量为 $x \\in \\mathbb{R}^{4}$，并考虑以下凸优化问题\n$$\n\\min_{x \\in \\mathbb{R}^{4}} \\;\\; \\frac{1}{2}\\|x - b\\|_{2}^{2} + \\lambda\\Big(\\|x_{G_{1}}\\|_{2} + \\|x_{G_{2}}\\|_{2}\\Big),\n$$\n其中组为 $G_{1}=\\{1,2\\}$ 和 $G_{2}=\\{2,3,4\\}$，且 $\\lambda>0$。向量 $x_{G}$ 表示 $x$ 中仅限于索引集合 $G$ 内分量的子向量。取 $\\lambda=1$，数据向量 $b \\in \\mathbb{R}^{4}$ 为\n$$\nb_{1}=0, \\quad b_{2}=3+\\frac{2}{\\sqrt{5}}, \\quad b_{3}=1+\\frac{1}{\\sqrt{5}}, \\quad b_{4}=0.\n$$\n从凸优化最优性和重叠组惩罚项的次梯度微积分的基本原理出发，分析此实例。证明唯一最小化子 $x^{\\star}$ 的支撑集等于特征的并集 $\\{2,3\\}$，该支撑集不能表示为同一组系统 $\\{G_{1},G_{2}\\}$ 中不相交的完整组的并集，并确定在交集特征 2 处的精确最优次梯度坐标，即在 $x^{\\star}$ 处参与最优性条件的惩罚项次梯度的第二个分量的值。\n\n报告在优化器处特征 2 的该次梯度坐标的精确值作为最终答案。不要四舍五入；提供一个精确的封闭形式表达式。",
            "solution": "该优化问题由下式给出\n$$\n\\min_{x \\in \\mathbb{R}^{4}} \\;\\; f(x) := \\frac{1}{2}\\|x - b\\|_{2}^{2} + \\lambda\\Omega(x),\n$$\n其中惩罚项为 $\\Omega(x) = \\|x_{G_{1}}\\|_{2} + \\|x_{G_{2}}\\|_{2}$。问题参数指定如下：\n- 维度：$p=4$。\n- 组：$G_{1}=\\{1,2\\}$ 和 $G_{2}=\\{2,3,4\\}$。\n- 正则化参数：$\\lambda=1$。\n- 数据向量 $b \\in \\mathbb{R}^{4}$：$b_{1}=0$, $b_{2}=3+\\frac{2}{\\sqrt{5}}$, $b_{3}=1+\\frac{1}{\\sqrt{5}}$, $b_{4}=0$。\n\n目标函数 $f(x)$ 是严格凸的，因为它是一个严格凸函数（平方欧几里得范数）和一个凸函数（组 LASSO 惩罚项）的和。因此，存在唯一的最小化子 $x^{\\star}$。最优性的一阶充要条件是，零向量必须是 $f(x)$ 在 $x = x^{\\star}$ 处的次微分的元素：\n$$\n0 \\in \\partial f(x^{\\star}) = (x^{\\star} - b) + \\lambda \\partial\\Omega(x^{\\star}).\n$$\n这个条件可以重写为 $b - x^{\\star} \\in \\lambda \\partial\\Omega(x^{\\star})$。当 $\\lambda=1$ 时，这简化为 $b - x^{\\star} \\in \\partial\\Omega(x^{\\star})$。这意味着向量 $b - x^{\\star}$ 必须是惩罚项 $\\Omega(x)$ 在最优点 $x^{\\star}$ 处的一个次梯度。\n\n惩罚项 $\\Omega(x)$ 的次微分由其组成范数项的次微分之和给出：$\\partial\\Omega(x) = \\partial_x (\\|x_{G_1}\\|_2) + \\partial_x (\\|x_{G_2}\\|_2)$。$\\partial\\Omega(x)$ 中的一个元素 $g$ 是一个形式为 $g = u + v$ 的向量 $g \\in \\mathbb{R}^4$，其中 $u \\in \\partial_x (\\|x_{G_1}\\|_2)$ 且 $v \\in \\partial_x (\\|x_{G_2}\\|_2)$。向量 $u$ 和 $v$ 的支撑集分别限制在 $G_1$ 和 $G_2$ 内。\n子向量 $x_G$ 的欧几里得范数的次梯度为：\n- 如果 $x_G \\neq 0$，$\\partial_x(\\|x_G\\|_2)$ 是一个单点集，其中包含向量 $u$，满足 $u_G = x_G/\\|x_G\\|_2$ 且 $u_{G^c}=0$。\n- 如果 $x_G = 0$，$\\partial_x(\\|x_G\\|_2)$ 是向量 $u$ 的集合，其中 $\\|u_G\\|_2 \\le 1$ 且 $u_{G^c}=0$。\n\n问题要求我们证明最小化子的支撑集是 $\\{2, 3\\}$。我们假设这是真的，即 $x^{\\star}_1=0, x^{\\star}_4=0$，并且 $x^{\\star}_2 \\neq 0, x^{\\star}_3 \\neq 0$。\n在此假设下，对应于组的子向量为 $x^{\\star}_{G_1} = (0, x^{\\star}_2)$ 和 $x^{\\star}_{G_2} = (x^{\\star}_2, x^{\\star}_3, 0)$。两者都是非零向量。因此，惩罚项 $\\Omega(x)$ 在 $x^{\\star}$ 处是可微的，其次微分 $\\partial\\Omega(x^{\\star})$ 是一个只包含梯度 $\\nabla\\Omega(x^{\\star})$ 的单点集。\n\n让我们计算唯一的次梯度 $g^{\\star} \\in \\partial\\Omega(x^{\\star})$。\n对于 $G_1 = \\{1,2\\}$，我们有 $x^{\\star}_{G_1}=(0, x^{\\star}_2)$。相应的次梯度分量向量 $u^{\\star}$ 在 $G_1$ 上的分量由 $x^{\\star}_{G_1} / \\|x^{\\star}_{G_1}\\|_2 = (0, x^{\\star}_2) / |x^{\\star}_2| = (0, \\text{sgn}(x^{\\star}_2))$ 给出。所以 $u^{\\star}_1=0$ 且 $u^{\\star}_2=\\text{sgn}(x^{\\star}_2)$。\n对于 $G_2 = \\{2,3,4\\}$，我们有 $x^{\\star}_{G_2}=(x^{\\star}_2, x^{\\star}_3, 0)$。相应的次梯度分量向量 $v^{\\star}$ 在 $G_2$ 上的分量由 $x^{\\star}_{G_2} / \\|x^{\\star}_{G_2}\\|_2 = (x^{\\star}_2, x^{\\star}_3, 0) / \\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}$ 给出。所以，$v^{\\star}_2 = x^{\\star}_2 / \\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}$，$v^{\\star}_3 = x^{\\star}_3 / \\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}$，并且 $v^{\\star}_4=0$。\n\n总次梯度 $g^{\\star} = u^{\\star} + v^{\\star}$ 的分量是：\n$g^{\\star}_1 = u^{\\star}_1 = 0$\n$g^{\\star}_2 = u^{\\star}_2 + v^{\\star}_2 = \\text{sgn}(x^{\\star}_2) + \\frac{x^{\\star}_2}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}}$\n$g^{\\star}_3 = v^{\\star}_3 = \\frac{x^{\\star}_3}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}}$\n$g^{\\star}_4 = v^{\\star}_4 = 0$\n\n最优性条件 $b - x^{\\star} = g^{\\star}$ 提供了一个方程组。由于 $b_2 > 0$ 且 $b_3 > 0$，我们可以合理地假设 $x^{\\star}_2 > 0$ 且 $x^{\\star}_3 > 0$。那么 $\\text{sgn}(x^{\\star}_2)=1$。\n对于活跃索引 $j \\in \\{2,3\\}$：\n1) $b_2 - x^{\\star}_2 = 1 + \\frac{x^{\\star}_2}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}}$\n2) $b_3 - x^{\\star}_3 = \\frac{x^{\\star}_3}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}}$\n\n从方程 (2) 中，我们可以表示 $\\frac{1}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}} = \\frac{b_3 - x^{\\star}_3}{x^{\\star}_3}$。将此代入 (1)：\n$b_2 - x^{\\star}_2 = 1 + x^{\\star}_2 \\left( \\frac{b_3 - x^{\\star}_3}{x^{\\star}_3} \\right) = 1 + \\frac{b_3 x^{\\star}_2}{x^{\\star}_3} - x^{\\star}_2$\n$b_2 - 1 = \\frac{b_3 x^{\\star}_2}{x^{\\star}_3} \\implies x^{\\star}_3 = \\frac{b_3}{b_2 - 1} x^{\\star}_2$。\n\n让我们代入 $b_2$ 和 $b_3$ 的给定值：\n$b_2 - 1 = \\left(3 + \\frac{2}{\\sqrt{5}}\\right) - 1 = 2 + \\frac{2}{\\sqrt{5}} = \\frac{2\\sqrt{5}+2}{\\sqrt{5}}$。\n$b_3 = 1 + \\frac{1}{\\sqrt{5}} = \\frac{\\sqrt{5}+1}{\\sqrt{5}}$。\n该比率为 $\\frac{b_3}{b_2 - 1} = \\frac{(\\sqrt{5}+1)/\\sqrt{5}}{2(\\sqrt{5}+1)/\\sqrt{5}} = \\frac{1}{2}$。\n因此，我们找到了线性关系 $x^{\\star}_3 = \\frac{1}{2} x^{\\star}_2$。\n\n现在，将此关系代回方程 (2)：\n$\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2} = \\sqrt{(2x^{\\star}_3)^2 + (x^{\\star}_3)^2} = \\sqrt{5(x^{\\star}_3)^2} = \\sqrt{5}x^{\\star}_3$ (因为我们假设了 $x^{\\star}_3>0$)。\n方程 (2) 变为 $b_3 - x^{\\star}_3 = \\frac{x^{\\star}_3}{\\sqrt{5}x^{\\star}_3} = \\frac{1}{\\sqrt{5}}$。\n求解 $x^{\\star}_3$：\n$x^{\\star}_3 = b_3 - \\frac{1}{\\sqrt{5}} = \\left(1 + \\frac{1}{\\sqrt{5}}\\right) - \\frac{1}{\\sqrt{5}} = 1$。\n由此，我们得到 $x^{\\star}_2 = 2x^{\\star}_3 = 2(1)=2$。\n我们的候选解是 $x^{\\star} = (0, 2, 1, 0)$。$x^{\\star}_2$ 和 $x^{\\star}_3$ 均为正，这与我们之前的假设一致。\n\n我们必须验证该解是否满足非活跃索引 $j \\in \\{1,4\\}$ 的最优性条件。\n对于 $j=1$：$b_1 - x^{\\star}_1 = 0 - 0 = 0$。次梯度分量为 $g^{\\star}_1 = 0$，如前计算。条件 $b_1 - x^{\\star}_1 = g^{\\star}_1$ 得到满足。\n对于 $j=4$：$b_4 - x^{\\star}_4 = 0 - 0 = 0$。次梯度分量为 $g^{\\star}_4 = 0$，如前计算。条件 $b_4 - x^{\\star}_4 = g^{\\star}_4$ 得到满足。\n由于所有最优性条件都得到满足，且最小化子是唯一的，我们已经证实 $x^{\\star} = (0, 2, 1, 0)$ 是唯一的解。其支撑集确实是 $\\{2, 3\\}$。\n\n问题陈述该支撑集不能表示为组系统 $\\{G_1, G_2\\}$ 中组的并集。可能的组的并集有 $\\emptyset$、$G_1=\\{1,2\\}$、$G_2=\\{2,3,4\\}$ 和 $G_1 \\cup G_2 = \\{1,2,3,4\\}$。$x^{\\star}$ 的支撑集是 $\\{2,3\\}$，与这些集合中的任何一个都不匹配。这表明重叠组 LASSO 如何能够选择不符合预定义组结构的特征集，而是选择参与多个重要组的特征。\n\n最后，我们需要找到在 $x^{\\star}$ 处参与最优性条件的惩罚项 $\\Omega(x)$ 的次梯度的第二个分量的值。这是向量 $g^{\\star} = b - x^{\\star}$ 的分量 $g^{\\star}_2$（因为 $\\lambda=1$）。\n这个值可以通过两种方式计算。\n首先，使用我们为次梯度分量推导出的公式：\n$g^{\\star}_2 = \\text{sgn}(x^{\\star}_2) + \\frac{x^{\\star}_2}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}} = 1 + \\frac{2}{\\sqrt{2^2 + 1^2}} = 1 + \\frac{2}{\\sqrt{5}}$。\n其次，直接使用最优性条件：\n$g^{\\star}_2 = b_2 - x^{\\star}_2 = \\left(3 + \\frac{2}{\\sqrt{5}}\\right) - 2 = 1 + \\frac{2}{\\sqrt{5}}$。\n两种方法得出了相同的结果。该值为所要求的精确表达式。",
            "answer": "$$\n\\boxed{1 + \\frac{2}{\\sqrt{5}}}\n$$"
        },
        {
            "introduction": "一个优化模型的构建离不开其统计目标。在重叠组 LASSO 中，如何为不同大小的组别设置惩罚权重是一个核心问题。本练习将带您从统计学的角度探讨权重选择的原理，通过推导零假设下得分统计量的分布，说明如何校准权重以在不同组之间公平地控制伪发现率，从而确保模型的统计稳健性。",
            "id": "3465461",
            "problem": "考虑一个线性模型，其观测值 $y\\in\\mathbb{R}^{n}$ 由 $y=X\\beta^{\\star}+\\varepsilon$ 给出，其中 $X\\in\\mathbb{R}^{n\\times p}$ 是一个固定设计矩阵，$\\beta^{\\star}\\in\\mathbb{R}^{p}$ 是真实系数向量，$\\varepsilon\\sim\\mathcal{N}(0,\\sigma^{2}I_{n})$。$X$ 的列是标准化的，即在每个组 $g$ 内，子矩阵 $X_{g}\\in\\mathbb{R}^{n\\times d_{g}}$ 具有标准正交列，并且每一列都具有单位欧几里得范数。\n\n您拟合一个重叠组最小绝对收缩和选择算子 (LASSO)，即凸估计量\n$$\n\\widehat{\\beta}(\\lambda)\\in\\arg\\min_{\\beta\\in\\mathbb{R}^{p}}\\left\\{\\frac{1}{2}\\|y-X\\beta\\|_{2}^{2}+\\lambda\\sum_{g\\in\\mathcal{G}}w_{g}\\|\\beta_{g}\\|_{2}\\right\\},\n$$\n其中 $\\mathcal{G}$ 是（可能重叠的）索引集 $g\\subseteq\\{1,\\dots,p\\}$ 的集合，$\\beta_{g}\\in\\mathbb{R}^{d_{g}}$ 是 $\\beta$ 限制在组 $g$ 上的子向量，$\\lambda>0$ 是正则化参数，$(w_{g})_{g\\in\\mathcal{G}}$ 是正的组权重。\n\n假设单个组 $g$ 的零假设成立，即 $\\beta^{\\star}_{g}=0$，并考虑得分 $X_{g}^{\\top}y$ 的零分布。仅使用关于多元高斯分布和凸正则化的基本事实来完成以下任务：\n\n1. 从上述假设出发，推导在零假设下 $X_{g}^{\\top}y$ 的分布以及 $\\|X_{g}^{\\top}y\\|_{2}$ 的相应分布。\n\n2. 使用不依赖于特定捷径的第一性原理进行论证，证明对于给定的 $\\lambda$，组 $g$ 被排除的一个充分条件是 $\\|X_{g}^{\\top}y\\|_{2}\\leq\\lambda w_{g}$，并解释选择与 $\\|X_{g}^{\\top}y\\|_{2}$ 的典型零分布下的大小成比例的权重如何能够以统一的方式控制各组的错误发现。\n\n3. 计算与期望的零分布下的大小 $\\mathbb{E}\\|X_{g}^{\\top}y\\|_{2}$ 成比例的 $w_{g}$ 的闭式解析表达式，用组大小 $d_{g}$ 和噪声水平 $\\sigma$ 表示。\n\n请以 $w_{g}$ 的精确解析表达式形式提供您的最终答案（相差一个不依赖于 $g$ 的通用比例常数）。不需要进行数值舍入，也不涉及单位。",
            "solution": "该问题分为三个部分。我们将按顺序进行解答。\n\n### 第 1 部分：得分的零分布\n\n我们被要求在组 $g$ 的零假设下，推导得分向量 $X_{g}^{\\top}y$ 及其欧几里得范数 $\\|X_{g}^{\\top}y\\|_{2}$ 的分布。零假设表述为 $\\beta^{\\star}_{g}=0$。线性模型为 $y=X\\beta^{\\star}+\\varepsilon$，其中 $\\varepsilon\\sim\\mathcal{N}(0,\\sigma^{2}I_{n})$。\n\n在组 $g$ 的零假设下，我们可以将 $\\beta^\\star$ 写成一个向量，其中对应于组 $g$ 中索引的分量为零。令 $\\beta^{\\star}_{\\setminus g}$ 为不在组 $g$ 中的索引对应的系数子向量，令 $X_{\\setminus g}$ 为 $X$ 的相应列。模型为 $y = X_{\\setminus g}\\beta^{\\star}_{\\setminus g} + \\varepsilon$。\n\n组 $g$ 的得分向量是 $X_{g}^{\\top}y$。代入零假设下的模型，我们得到：\n$$X_{g}^{\\top}y = X_{g}^{\\top}(X_{\\setminus g}\\beta^{\\star}_{\\setminus g} + \\varepsilon) = X_{g}^{\\top}X_{\\setminus g}\\beta^{\\star}_{\\setminus g} + X_{g}^{\\top}\\varepsilon$$\n这个表达式表明，得分是一个常数项（取决于其他系数的真实值 $\\beta^\\star_{\\setminus g}$）和一个随机项的和。随机向量 $X_{g}^{\\top}\\varepsilon$ 是高斯向量 $\\varepsilon$ 的一个线性变换。因此，它的分布是多元高斯分布。\n\n$X_g^\\top \\varepsilon$ 的均值是 $\\mathbb{E}[X_{g}^{\\top}\\varepsilon] = X_{g}^{\\top}\\mathbb{E}[\\varepsilon] = X_{g}^{\\top}0 = 0$。\n协方差矩阵是 $\\text{Cov}(X_{g}^{\\top}\\varepsilon) = X_{g}^{\\top}\\text{Cov}(\\varepsilon)(X_{g}^{\\top})^{\\top} = X_{g}^{\\top}(\\sigma^{2}I_{n})X_{g} = \\sigma^{2}X_{g}^{\\top}X_{g}$。\n\n我们已知子矩阵 $X_{g}$ 具有标准正交列。这意味着 $X_{g}^{\\top}X_{g} = I_{d_g}$，其中 $d_g = |g|$ 是组 $g$ 中的变量数。\n因此，$X_{g}^{\\top}\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2}I_{d_g})$。\n\n因此，得分 $X_g^\\top y$ 的分布是 $\\mathcal{N}(X_g^\\top X_{\\setminus g} \\beta^\\star_{\\setminus g}, \\sigma^2 I_{d_g})$。这个分布依赖于未知的滋扰参数 $\\beta^\\star_{\\setminus g}$。\n\n然而，在校准正则化参数或统计检验的语境中，“零分布”一词通常指在全局零假设 $\\beta^\\star = 0$ 下的分布。这为仅由噪声引起的统计量变异性提供了一个基线，用于检测信号。假设采用这种标准解释，我们设 $\\beta^\\star = 0$。模型简化为 $y = \\varepsilon$。\n\n在全局零假设 $\\beta^\\star = 0$ 下：\n得分向量为 $X_{g}^{\\top}y = X_{g}^{\\top}\\varepsilon$。\n如上所推导，其分布为：\n$$X_{g}^{\\top}y \\sim \\mathcal{N}(0, \\sigma^{2}I_{d_g})$$\n这是一个 $d_g$ 维多元正态分布，均值为 $0$，协方差矩阵为对角矩阵，对角线元素为 $\\sigma^2$。\n\n接下来，我们求其欧几里得范数 $\\|X_{g}^{\\top}y\\|_{2}$ 的分布。令 $Z = \\frac{1}{\\sigma}X_{g}^{\\top}y$。根据以上推导，$Z \\sim \\mathcal{N}(0, I_{d_g})$。$Z$ 的分量，比如 $Z_1, \\dots, Z_{d_g}$，是独立同分布的标准正态随机变量，$Z_i \\sim \\mathcal{N}(0, 1)$。\n其范数的平方为 $\\|Z\\|_{2}^{2} = \\sum_{i=1}^{d_g} Z_{i}^{2}$。根据定义，$d_g$ 个独立标准正态变量的平方和服从自由度为 $d_g$ 的卡方分布。\n$$\\|Z\\|_{2}^{2} \\sim \\chi_{d_g}^{2}$$\n范数本身 $\\|Z\\|_{2} = \\sqrt{\\sum_{i=1}^{d_g} Z_{i}^{2}}$ 服从自由度为 $d_g$ 的$\\chi$分布，记为 $\\chi_{d_g}$。\n由于 $\\|X_{g}^{\\top}y\\|_{2} = \\sigma\\|Z\\|_{2}$，$\\|X_{g}^{\\top}y\\|_{2}$ 的分布是 $\\sigma$ 乘以一个服从 $\\chi_{d_g}$ 分布的随机变量的分布。\n\n### 第 2 部分：组排除和加权的理由\n\n我们被要求证明为什么 $\\|X_{g}^{\\top}y\\|_{2} \\leq \\lambda w_{g}$ 是排除组 $g$（即 $\\widehat{\\beta}_g = 0$）的一个充分条件，并解释特定权重选择 $w_g$ 的作用。\n\n一种“第一性原理”的证明可以从凸优化问题的 Karush-Kuhn-Tucker (KKT) 最优性条件推导出来。目标函数为 $L(\\beta) = \\frac{1}{2}\\|y-X\\beta\\|_{2}^{2}+\\lambda\\sum_{h\\in\\mathcal{G}}w_{h}\\|\\beta_{h}\\|_{2}$。一个向量 $\\widehat{\\beta}$ 是最小化子当且仅当 $0$ 属于 $L(\\beta)$ 在 $\\widehat{\\beta}$ 处的次梯度。\n次梯度为 $\\partial L(\\beta) = -X^{\\top}(y-X\\beta) + \\lambda \\sum_{h\\in\\mathcal{G}}w_{h}\\partial\\|\\beta_{h}\\|_{2}$。这里，$\\partial\\|\\beta_h\\|_2$ 是子向量 $\\beta_h$ 的欧几里得范数的次梯度。\n- 如果 $\\beta_h \\neq 0$，$\\partial\\|\\beta_h\\|_2 = \\{ \\beta_h / \\|\\beta_h\\|_2 \\}$。\n- 如果 $\\beta_h = 0$，$\\partial\\|\\beta_h\\|_2 = \\{u \\in \\mathbb{R}^{d_h} : \\|u\\|_2 \\le 1\\}$，即闭单位球。\n\n考虑最简单的情况，即解为 $\\widehat{\\beta} = 0$。为此，KKT 条件必须在 $\\beta=0$ 时成立。对于每个组 $h \\in \\mathcal{G}$，我们必须有 $0 \\in -X_{h}^{\\top}y + \\lambda w_{h}\\partial\\|\\beta_{h}\\|_{2}|_{\\beta_h=0}$。这意味着必须存在一个向量 $u_h$ 满足 $\\|u_h\\|_2 \\le 1$，使得 $-X_{h}^{\\top}y + \\lambda w_h u_h = 0$，这等价于 $X_{h}^{\\top}y = \\lambda w_h u_h$。对两边取范数，我们得到 $\\|X_{h}^{\\top}y\\|_{2} = \\lambda w_h \\|u_h\\|_2$。由于 $\\|u_h\\|_2 \\le 1$，这意味着要使其可能成立，一个必要条件是 $\\|X_{h}^{\\top}y\\|_{2} \\le \\lambda w_h$。\n因此，使*整个*向量 $\\widehat{\\beta}$ 为零的一个充分条件是对于所有 $h \\in \\mathcal{G}$ 都有 $\\|X_{h}^{\\top}y\\|_{2} \\le \\lambda w_h$。\n\n这提供了一个强烈的直觉。量 $\\|X_g^\\top y\\|_2 / w_g$ 充当组 $g$ 的得分。如果该得分低于阈值 $\\lambda$，则该组不会被选中。虽然在一般的重叠情况下，$\\|X_g^\\top y\\|_2 \\le \\lambda w_g$ 并不是 $\\widehat{\\beta}_g = 0$ 的严格充分条件（因为其他相关组的选择会影响组 $g$ 的 KKT 条件），但它是当所有其他组都处于非活动状态时，一个组保持非活动状态的基本条件。这种启发式方法是路径算法和筛选规则的基础，并可作为“第一性原理”的证明。如果组 $g$ 的得分统计量“太小”，则该组不会被选中。\n\n现在，我们来解释加权。目标是控制错误发现，即在组 $g$ 的真实系数 $\\beta_g^\\star$ 为 $0$ 时选中了该组。根据上述逻辑，当 $\\beta_g^\\star=0$ 时，如果 $\\|X_g^\\top y\\|_2 > \\lambda w_g$，则可能发生对组 $g$ 的错误发现。为了使选择过程对不同大小的组都公平，我们希望在零假设下，此事件的概率对所有组 $g$ 都大致相同。\n\n$P(\\text{对 } g \\text{ 的错误发现}) = P(\\|X_g^\\top y\\|_2 > \\lambda w_g \\mid \\beta_g^\\star=0)$\n\n从第 1 部分（在全局零假设 $\\beta^\\star=0$ 下），我们知道 $\\|X_g^\\top y\\|_2$ 的分布取决于组大小 $d_g$。具体来说，其期望值 $\\mathbb{E}\\|X_g^\\top y\\|_2$（代表其“典型的零分布下的大小”）是 $d_g$ 和 $\\sigma$ 的函数。如果我们选择权重 $w_g$ 与这个典型大小成正比，即 $w_g \\propto \\mathbb{E}\\|X_g^\\top y\\|_2$，比如对于某个常数 $C$，有 $w_g = C \\cdot \\mathbb{E}\\|X_g^\\top y\\|_2$，那么错误发现的条件就变成：\n$$\\|X_g^\\top y\\|_2 > \\lambda C \\cdot \\mathbb{E}\\|X_g^\\top y\\|_2 \\iff \\frac{\\|X_g^\\top y\\|_2}{\\mathbb{E}\\|X_g^\\top y\\|_2} > \\lambda C$$\n通过这样选择 $w_g$，阈值处理被应用于一个“标准化”的统计量 $\\|X_g^\\top y\\|_2 / \\mathbb{E}\\|X_g^\\top y\\|_2$，该统计量已被其在零假设下的期望大小归一化。这些标准化统计量的分布在不同组大小 $d_g$ 之间比原始得分 $\\|X_g^\\top y\\|_2$ 的分布更具可比性。因此，对于固定的 $\\lambda$，这种权重的选择有助于使所有组的错误发现概率更加均匀。\n\n### 第 3 部分：权重 $w_g$ 的表达式\n\n我们需要计算与 $\\mathbb{E}\\|X_{g}^{\\top}y\\|_{2}$ 成比例的 $w_g$ 的闭式表达式。正如在第 1 部分所确定的，在零假设 $\\beta^\\star=0$ 下，我们有 $\\|X_{g}^{\\top}y\\|_{2} = \\sigma R_g$，其中 $R_g$ 是一个服从自由度为 $d_g$ 的 $\\chi$ 分布的随机变量。我们需要计算其期望。\n\n$R_g \\sim \\chi_{d_g}$ 的概率密度函数由下式给出：\n$$f(x; d_g) = \\frac{2^{1-d_g/2}}{\\Gamma(d_g/2)} x^{d_g-1} \\exp(-x^2/2) \\quad \\text{for } x \\ge 0$$\n其中 $\\Gamma(\\cdot)$ 是伽马函数。\n\n期望为 $\\mathbb{E}[R_g] = \\int_0^\\infty x f(x; d_g) dx$。\n$$ \\mathbb{E}[R_g] = \\frac{2^{1-d_g/2}}{\\Gamma(d_g/2)} \\int_0^\\infty x^{d_g} \\exp(-x^2/2) dx $$\n为了计算积分 $I = \\int_0^\\infty x^{d_g} \\exp(-x^2/2) dx$，我们使用换元法，令 $t = x^2/2$，则 $x=\\sqrt{2t}$ 且 $dx = \\frac{1}{\\sqrt{2t}}dt$。\n$$ I = \\int_0^\\infty (\\sqrt{2t})^{d_g} e^{-t} \\frac{1}{\\sqrt{2t}} dt = \\int_0^\\infty 2^{(d_g-1)/2} t^{(d_g-1)/2} e^{-t} dt $$\n$$ I = 2^{(d_g-1)/2} \\int_0^\\infty t^{\\frac{d_g+1}{2}-1} e^{-t} dt $$\n根据伽马函数 $\\Gamma(z) = \\int_0^\\infty t^{z-1}e^{-t}dt$ 的定义，上述积分等于 $\\Gamma\\left(\\frac{d_g+1}{2}\\right)$。\n因此，$I = 2^{(d_g-1)/2} \\Gamma\\left(\\frac{d_g+1}{2}\\right)$。\n\n现在，将此结果代回 $\\mathbb{E}[R_g]$ 的表达式中：\n$$\\mathbb{E}[R_g] = \\frac{2^{1-d_g/2}}{\\Gamma(d_g/2)} \\cdot \\left( 2^{(d_g-1)/2} \\Gamma\\left(\\frac{d_g+1}{2}\\right) \\right)$$\n2的幂合并：$2^{1-d_g/2 + (d_g-1)/2} = 2^{1/2} = \\sqrt{2}$。\n$$\\mathbb{E}[R_g] = \\sqrt{2} \\frac{\\Gamma\\left(\\frac{d_g+1}{2}\\right)}{\\Gamma\\left(\\frac{d_g}{2}\\right)}$$\n最后，我们有 $\\mathbb{E}\\|X_{g}^{\\top}y\\|_{2} = \\sigma \\mathbb{E}[R_g]$。\n$$\\mathbb{E}\\|X_{g}^{\\top}y\\|_{2} = \\sigma\\sqrt{2} \\frac{\\Gamma\\left(\\frac{d_g+1}{2}\\right)}{\\Gamma\\left(\\frac{d_g}{2}\\right)}$$\n权重 $w_g$ 被选择为与此量成比例。问题要求给出 $w_g$ 的表达式，用 $d_g$ 和 $\\sigma$ 表示，并可相差一个通用比例常数。因此，我们可以将该常数设为1，并将此表达式作为 $w_g$ 的选择。",
            "answer": "$$\\boxed{\\sigma\\sqrt{2} \\frac{\\Gamma\\left(\\frac{d_g+1}{2}\\right)}{\\Gamma\\left(\\frac{d_g}{2}\\right)}}$$"
        }
    ]
}