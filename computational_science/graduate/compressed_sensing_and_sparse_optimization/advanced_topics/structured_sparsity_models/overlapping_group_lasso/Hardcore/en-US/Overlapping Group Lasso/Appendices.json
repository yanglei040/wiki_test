{
    "hands_on_practices": [
        {
            "introduction": "The overlapping group LASSO penalty, while conceptually clear, can be challenging to implement and analyze directly. A powerful technique to manage the complexity of overlapping groups is to introduce latent variables, creating an equivalent but more structured optimization problem. This first practice problem provides a concrete, hands-on opportunity to master this reformulation by explicitly constructing the key components—the latent variables and the duplication matrix—for a simple case (). Working through this exercise builds a foundational understanding of the algebraic machinery that underpins many theoretical analyses and algorithms.",
            "id": "3465480",
            "problem": "Consider the overlapping group least absolute shrinkage and selection operator (LASSO) penalty in a latent-variable formulation used in compressed sensing and sparse optimization. Let there be a parameter vector $\\beta \\in \\mathbb{R}^{p}$ with $p=5$ and overlapping groups $g_1=\\{1,2,3\\}$, $g_2=\\{3,4\\}$, and $g_3=\\{4,5\\}$. In the latent-variable reformulation, for each group $g$, introduce a group-restricted latent variable $\\beta^{(g)} \\in \\mathbb{R}^{|g|}$ that stores a copy of the components of $\\beta$ indexed by $g$, and define the stacked latent vector $z \\in \\mathbb{R}^{\\sum_{g} |g|}$ by concatenating the $\\beta^{(g)}$ in the order $(g_1,g_2,g_3)$. The duplication mapping $D \\in \\mathbb{R}^{(\\sum_{g} |g|)\\times p}$ is defined by the linear relation $z = D \\beta$, where each row of $D$ selects the appropriate component of $\\beta$ that corresponds to the latent copy indexed by that row.\n\nStarting from the basic definition of the duplication mapping and the latent variable construction described above, do the following:\n- Explicitly write the latent vectors $\\beta^{(g_1)}$, $\\beta^{(g_2)}$, and $\\beta^{(g_3)}$ in terms of the entries of $\\beta$.\n- Explicitly construct the matrix $D$.\n- Write the consensus constraint in matrix form linking $z$ and $\\beta$.\n\nThen, using only linear algebra operations and without appealing to any pre-stated shortcut formulas, derive $D^{\\top} D$ and compute its determinant. Provide the determinant as your final answer. If your final result is a number, give it exactly; no rounding is required.",
            "solution": "We begin from the latent-variable construction for overlapping groups: for each group $g \\subset \\{1,\\dots,p\\}$, we introduce a latent vector $\\beta^{(g)} \\in \\mathbb{R}^{|g|}$ that stores a copy of the components of $\\beta$ indexed by $g$. The stacked latent vector is formed as $z = \\big(\\beta^{(g_1)};\\beta^{(g_2)};\\beta^{(g_3)}\\big) \\in \\mathbb{R}^{|g_1|+|g_2|+|g_3|}$, where we use the semicolon to denote vertical concatenation.\n\nGiven $p=5$ and the groups $g_1=\\{1,2,3\\}$, $g_2=\\{3,4\\}$, and $g_3=\\{4,5\\}$, the latent variables are the group-restricted copies of $\\beta = (\\beta_1,\\beta_2,\\beta_3,\\beta_4,\\beta_5)^{\\top}$:\n- For $g_1=\\{1,2,3\\}$, the latent vector is\n$$\n\\beta^{(g_1)} = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix}.\n$$\n- For $g_2=\\{3,4\\}$, the latent vector is\n$$\n\\beta^{(g_2)} = \\begin{pmatrix} \\beta_3 \\\\ \\beta_4 \\end{pmatrix}.\n$$\n- For $g_3=\\{4,5\\}$, the latent vector is\n$$\n\\beta^{(g_3)} = \\begin{pmatrix} \\beta_4 \\\\ \\beta_5 \\end{pmatrix}.\n$$\n\nConcatenating in the order $(g_1,g_2,g_3)$ gives the stacked latent vector $z \\in \\mathbb{R}^{7}$:\n$$\nz \\;=\\; \\begin{pmatrix}\n\\beta^{(g_1)} \\\\ \\beta^{(g_2)} \\\\ \\beta^{(g_3)}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_4 \\\\ \\beta_5\n\\end{pmatrix}.\n$$\n\nBy definition of the duplication mapping, there exists a matrix $D \\in \\mathbb{R}^{7 \\times 5}$ such that $z = D \\beta$. Each row of $D$ places a $1$ in the column corresponding to the index of $\\beta$ that is being copied and zeros elsewhere. Following the order of entries in $z$ written above, the seven rows of $D$ are:\n- Row $1$ corresponds to $\\beta_1$,\n- Row $2$ corresponds to $\\beta_2$,\n- Rows $3$ and $4$ both correspond to $\\beta_3$ (due to the overlap between $g_1$ and $g_2$),\n- Rows $5$ and $6$ both correspond to $\\beta_4$ (due to the overlap between $g_2$ and $g_3$),\n- Row $7$ corresponds to $\\beta_5$.\n\nThus,\n$$\nD \\;=\\;\n\\begin{pmatrix}\n1  0  0  0  0 \\\\\n0  1  0  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  0  1  0 \\\\\n0  0  0  1  0 \\\\\n0  0  0  0  1\n\\end{pmatrix}.\n$$\n\nThe consensus constraint that links the latent copies to the original variable is precisely the linear relation\n$$\nz - D \\beta \\;=\\; 0,\n$$\nequivalently $z = D \\beta$. This ensures that all group-wise copies of a shared coordinate are equal to the corresponding original coordinate. In coordinate form, this enforces equalities such as the two copies for index $3$ being equal to $\\beta_3$, and the two copies for index $4$ being equal to $\\beta_4$.\n\nNext, we compute $D^{\\top} D$. By construction, each row of $D$ has exactly one entry equal to $1$ and all other entries $0$, and duplicates occur exactly when an original index appears in multiple groups. Consequently, the product $D^{\\top} D$ is diagonal, with the $(i,i)$ entry equal to the number of times index $i$ appears across all groups. We verify this explicitly.\n\nLet $e_i \\in \\mathbb{R}^{5}$ be the $i$-th standard basis vector. Each row of $D$ is one of the $e_i^{\\top}$. If an index $i$ appears $c_i$ times in the stacked latent vector $z$ (i.e., in the groups), then $D$ contains $c_i$ copies of the row $e_i^{\\top}$. Therefore,\n$$\nD^{\\top} D \\;=\\; \\sum_{k=1}^{7} r_k^{\\top} r_k \\;=\\; \\sum_{i=1}^{5} c_i \\, e_i e_i^{\\top} \\;=\\; \\operatorname{diag}(c_1, c_2, c_3, c_4, c_5),\n$$\nwhere $r_k$ denotes the $k$-th row of $D$.\n\nCounting occurrences from the group structure:\n- Index $1$ appears only in $g_1$, so $c_1 = 1$.\n- Index $2$ appears only in $g_1$, so $c_2 = 1$.\n- Index $3$ appears in $g_1$ and $g_2$, so $c_3 = 2$.\n- Index $4$ appears in $g_2$ and $g_3$, so $c_4 = 2$.\n- Index $5$ appears only in $g_3$, so $c_5 = 1$.\n\nHence,\n$$\nD^{\\top} D \\;=\\; \\operatorname{diag}(1,\\,1,\\,2,\\,2,\\,1).\n$$\n\nFinally, the determinant of a diagonal matrix is the product of its diagonal entries. Therefore,\n$$\n\\det(D^{\\top} D) \\;=\\; 1 \\times 1 \\times 2 \\times 2 \\times 1 \\;=\\; 4.\n$$\n\nThis is the requested scalar quantity.",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "With the latent variable formulation in hand, we can now explore the unique modeling power of the overlapping group LASSO. Unlike standard group LASSO, the overlapping version can identify sparse solutions whose support does not correspond to a simple union of the predefined groups. This exercise demonstrates this key feature through a carefully designed example, requiring you to apply first-order optimality conditions to find the unique solution and analyze the behavior of the subgradient at the overlapping feature (). This practice illuminates how the penalty encourages the selection of features that are important across multiple, interacting contexts.",
            "id": "3465473",
            "problem": "Consider an overlapping group Least Absolute Shrinkage and Selection Operator (LASSO) problem in dimension $p=4$ with squared-error data fit and two overlapping groups. Let the decision variable be $x \\in \\mathbb{R}^{4}$, and consider the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{4}} \\;\\; \\frac{1}{2}\\|x - b\\|_{2}^{2} + \\lambda\\Big(\\|x_{G_{1}}\\|_{2} + \\|x_{G_{2}}\\|_{2}\\Big),\n$$\nwhere the groups are $G_{1}=\\{1,2\\}$ and $G_{2}=\\{2,3,4\\}$, and $\\lambda0$. The vector $x_{G}$ denotes the subvector of $x$ restricted to indices in $G$. Take $\\lambda=1$ and the data vector $b \\in \\mathbb{R}^{4}$ to be\n$$\nb_{1}=0, \\quad b_{2}=3+\\frac{2}{\\sqrt{5}}, \\quad b_{3}=1+\\frac{1}{\\sqrt{5}}, \\quad b_{4}=0.\n$$\nStart from first principles of convex optimality and subgradient calculus for the overlapping group penalty to analyze this instance. Establish that the unique minimizer $x^{\\star}$ has support equal to the union of features $\\{2,3\\}$, which is not expressible as a union of disjoint whole groups with the same group system $\\{G_{1},G_{2}\\}$, and determine the exact optimal subgradient coordinate at the intersection feature $2$, that is, the value of the second component of a subgradient of the penalty at $x^{\\star}$ that participates in the optimality condition.\n\nReport as your final answer the exact value of this subgradient coordinate at feature $2$ at the optimizer. Do not round; provide an exact closed-form expression.",
            "solution": "The optimization problem is given by\n$$\n\\min_{x \\in \\mathbb{R}^{4}} \\;\\; f(x) := \\frac{1}{2}\\|x - b\\|_{2}^{2} + \\lambda\\Omega(x),\n$$\nwhere the penalty term is $\\Omega(x) = \\|x_{G_{1}}\\|_{2} + \\|x_{G_{2}}\\|_{2}$. The problem parameters are specified as:\n- Dimension: $p=4$.\n- Groups: $G_{1}=\\{1,2\\}$ and $G_{2}=\\{2,3,4\\}$.\n- Regularization parameter: $\\lambda=1$.\n- Data vector $b \\in \\mathbb{R}^{4}$: $b_{1}=0$, $b_{2}=3+\\frac{2}{\\sqrt{5}}$, $b_{3}=1+\\frac{1}{\\sqrt{5}}$, $b_{4}=0$.\n\nThe objective function $f(x)$ is strictly convex, being the sum of a strictly convex function (the squared Euclidean norm) and a convex function (the group LASSO penalty). Therefore, a unique minimizer $x^{\\star}$ exists. The first-order necessary and sufficient condition for optimality is that the zero vector must be an element of the subdifferential of $f(x)$ at $x = x^{\\star}$:\n$$\n0 \\in \\partial f(x^{\\star}) = (x^{\\star} - b) + \\lambda \\partial\\Omega(x^{\\star}).\n$$\nThis condition can be rewritten as $b - x^{\\star} \\in \\lambda \\partial\\Omega(x^{\\star})$. With $\\lambda=1$, this simplifies to $b - x^{\\star} \\in \\partial\\Omega(x^{\\star})$. This means that the vector $b - x^{\\star}$ must be a subgradient of the penalty $\\Omega(x)$ at the optimal point $x^{\\star}$.\n\nThe subdifferential of the penalty term $\\Omega(x)$ is given by the sum of the subdifferentials of its constituent norm terms: $\\partial\\Omega(x) = \\partial_x (\\|x_{G_1}\\|_2) + \\partial_x (\\|x_{G_2}\\|_2)$. An element $g \\in \\partial\\Omega(x)$ is a vector $g \\in \\mathbb{R}^4$ of the form $g = u + v$, where $u \\in \\partial_x (\\|x_{G_1}\\|_2)$ and $v \\in \\partial_x (\\|x_{G_2}\\|_2)$. The vectors $u$ and $v$ have support restricted to $G_1$ and $G_2$, respectively.\nThe subgradient of the Euclidean norm of a subvector $x_G$ is:\n- If $x_G \\neq 0$, $\\partial_x(\\|x_G\\|_2)$ is the singleton set containing the vector $u$ where $u_G = x_G/\\|x_G\\|_2$ and $u_{G^c}=0$.\n- If $x_G = 0$, $\\partial_x(\\|x_G\\|_2)$ is the set of vectors $u$ where $\\|u_G\\|_2 \\le 1$ and $u_{G^c}=0$.\n\nThe problem requires us to establish that the support of the minimizer is $\\{2, 3\\}$. Let us hypothesize that this is true, i.e., $x^{\\star}_1=0$, $x^{\\star}_4=0$, and $x^{\\star}_2 \\neq 0, x^{\\star}_3 \\neq 0$.\nUnder this hypothesis, the subvectors corresponding to the groups are $x^{\\star}_{G_1} = (0, x^{\\star}_2)$ and $x^{\\star}_{G_2} = (x^{\\star}_2, x^{\\star}_3, 0)$. Both are non-zero vectors. Therefore, the penalty $\\Omega(x)$ is differentiable at $x^{\\star}$, and its subdifferential $\\partial\\Omega(x^{\\star})$ is a singleton set containing only the gradient $\\nabla\\Omega(x^{\\star})$.\n\nLet's compute the unique subgradient $g^{\\star} \\in \\partial\\Omega(x^{\\star})$.\nFor $G_1 = \\{1,2\\}$, we have $x^{\\star}_{G_1}=(0, x^{\\star}_2)$. The corresponding subgradient component vector $u^{\\star}$ has its components over $G_1$ given by $x^{\\star}_{G_1} / \\|x^{\\star}_{G_1}\\|_2 = (0, x^{\\star}_2) / |x^{\\star}_2| = (0, \\text{sgn}(x^{\\star}_2))$. So $u^{\\star}_1=0$ and $u^{\\star}_2=\\text{sgn}(x^{\\star}_2)$.\nFor $G_2 = \\{2,3,4\\}$, we have $x^{\\star}_{G_2}=(x^{\\star}_2, x^{\\star}_3, 0)$. The corresponding subgradient component vector $v^{\\star}$ has its components over $G_2$ given by $x^{\\star}_{G_2} / \\|x^{\\star}_{G_2}\\|_2 = (x^{\\star}_2, x^{\\star}_3, 0) / \\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}$. So, $v^{\\star}_2 = x^{\\star}_2 / \\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}$, $v^{\\star}_3 = x^{\\star}_3 / \\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}$, and $v^{\\star}_4=0$.\n\nThe components of the total subgradient $g^{\\star} = u^{\\star} + v^{\\star}$ are:\n$g^{\\star}_1 = u^{\\star}_1 = 0$\n$g^{\\star}_2 = u^{\\star}_2 + v^{\\star}_2 = \\text{sgn}(x^{\\star}_2) + \\frac{x^{\\star}_2}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}}$\n$g^{\\star}_3 = v^{\\star}_3 = \\frac{x^{\\star}_3}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}}$\n$g^{\\star}_4 = v^{\\star}_4 = 0$\n\nThe optimality condition $b - x^{\\star} = g^{\\star}$ provides a system of equations. Since $b_2  0$ and $b_3  0$, we can reasonably assume $x^{\\star}_2  0$ and $x^{\\star}_3  0$. Then $\\text{sgn}(x^{\\star}_2)=1$.\nFor the active indices $j \\in \\{2,3\\}$:\n1) $b_2 - x^{\\star}_2 = 1 + \\frac{x^{\\star}_2}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}}$\n2) $b_3 - x^{\\star}_3 = \\frac{x^{\\star}_3}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}}$\n\nFrom equation (2), we can express $\\frac{1}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}} = \\frac{b_3 - x^{\\star}_3}{x^{\\star}_3}$. Substituting this into (1):\n$b_2 - x^{\\star}_2 = 1 + x^{\\star}_2 \\left( \\frac{b_3 - x^{\\star}_3}{x^{\\star}_3} \\right) = 1 + \\frac{b_3 x^{\\star}_2}{x^{\\star}_3} - x^{\\star}_2$\n$b_2 - 1 = \\frac{b_3 x^{\\star}_2}{x^{\\star}_3} \\implies x^{\\star}_3 = \\frac{b_3}{b_2 - 1} x^{\\star}_2$.\n\nLet's substitute the given values for $b_2$ and $b_3$:\n$b_2 - 1 = \\left(3 + \\frac{2}{\\sqrt{5}}\\right) - 1 = 2 + \\frac{2}{\\sqrt{5}} = \\frac{2\\sqrt{5}+2}{\\sqrt{5}}$.\n$b_3 = 1 + \\frac{1}{\\sqrt{5}} = \\frac{\\sqrt{5}+1}{\\sqrt{5}}$.\nThe ratio is $\\frac{b_3}{b_2 - 1} = \\frac{(\\sqrt{5}+1)/\\sqrt{5}}{2(\\sqrt{5}+1)/\\sqrt{5}} = \\frac{1}{2}$.\nThus, we find the linear relationship $x^{\\star}_3 = \\frac{1}{2} x^{\\star}_2$.\n\nNow, substitute this relation back into equation (2):\n$\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2} = \\sqrt{(2x^{\\star}_3)^2 + (x^{\\star}_3)^2} = \\sqrt{5(x^{\\star}_3)^2} = \\sqrt{5}x^{\\star}_3$ (since we assumed $x^{\\star}_30$).\nEquation (2) becomes $b_3 - x^{\\star}_3 = \\frac{x^{\\star}_3}{\\sqrt{5}x^{\\star}_3} = \\frac{1}{\\sqrt{5}}$.\nSolving for $x^{\\star}_3$:\n$x^{\\star}_3 = b_3 - \\frac{1}{\\sqrt{5}} = \\left(1 + \\frac{1}{\\sqrt{5}}\\right) - \\frac{1}{\\sqrt{5}} = 1$.\nFrom this, we find $x^{\\star}_2 = 2x^{\\star}_3 = 2(1)=2$.\nOur candidate solution is $x^{\\star} = (0, 2, 1, 0)$. Both $x^{\\star}_2$ and $x^{\\star}_3$ are positive, which is consistent with our earlier assumption.\n\nWe must verify that this solution satisfies the optimality conditions for the inactive indices $j \\in \\{1,4\\}$.\nFor $j=1$: $b_1 - x^{\\star}_1 = 0 - 0 = 0$. The subgradient component is $g^{\\star}_1 = 0$, as calculated before. The condition $b_1 - x^{\\star}_1 = g^{\\star}_1$ is satisfied.\nFor $j=4$: $b_4 - x^{\\star}_4 = 0 - 0 = 0$. The subgradient component is $g^{\\star}_4 = 0$, as calculated before. The condition $b_4 - x^{\\star}_4 = g^{\\star}_4$ is satisfied.\nSince all optimality conditions are met, and the minimizer is unique, we have confirmed that $x^{\\star} = (0, 2, 1, 0)$ is the unique solution. Its support is indeed $\\{2, 3\\}$.\n\nThe problem states that this support is not expressible as a union of groups from the system $\\{G_1, G_2\\}$. The possible unions of groups are $\\emptyset$, $G_1=\\{1,2\\}$, $G_2=\\{2,3,4\\}$, and $G_1 \\cup G_2 = \\{1,2,3,4\\}$. The support of $x^{\\star}$, which is $\\{2,3\\}$, does not match any of these sets. This demonstrates how overlapping group LASSO can select feature sets that do not conform to the predefined group structure, instead selecting features that participate in multiple important groups.\n\nFinally, we need to find the value of the second component of the subgradient of the penalty $\\Omega(x)$ at $x^{\\star}$ that participates in the optimality condition. This is the component $g^{\\star}_2$ of the vector $g^{\\star} = b - x^{\\star}$ (since $\\lambda=1$).\nThis value can be calculated in two ways.\nFirst, using the formula we derived for the subgradient components:\n$g^{\\star}_2 = \\text{sgn}(x^{\\star}_2) + \\frac{x^{\\star}_2}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}} = 1 + \\frac{2}{\\sqrt{2^2 + 1^2}} = 1 + \\frac{2}{\\sqrt{5}}$.\nSecond, using the optimality condition directly:\n$g^{\\star}_2 = b_2 - x^{\\star}_2 = \\left(3 + \\frac{2}{\\sqrt{5}}\\right) - 2 = 1 + \\frac{2}{\\sqrt{5}}$.\nBoth methods yield the same result. The value is an exact expression as required.",
            "answer": "$$\n\\boxed{1 + \\frac{2}{\\sqrt{5}}}\n$$"
        },
        {
            "introduction": "The performance of the group LASSO estimator depends critically on the choice of group-specific weights, $w_g$. An effective weighting scheme ensures that the penalty is applied equitably across groups of varying sizes, preventing larger groups from being unfairly penalized. This practice problem delves into the statistical principles for choosing these weights by examining the behavior of the model under the null hypothesis where no features are active (). By deriving a weight proportional to the expected size of a group's score statistic under null noise, you will understand how to calibrate the penalty to control the rate of false discoveries, a crucial step in any practical application.",
            "id": "3465461",
            "problem": "Consider the linear model with observations $y\\in\\mathbb{R}^{n}$ given by $y=X\\beta^{\\star}+\\varepsilon$, where $X\\in\\mathbb{R}^{n\\times p}$ is a fixed design matrix, $\\beta^{\\star}\\in\\mathbb{R}^{p}$ is the true coefficient vector, and $\\varepsilon\\sim\\mathcal{N}(0,\\sigma^{2}I_{n})$. The columns of $X$ are standardized in the sense that within each group $g$ the submatrix $X_{g}\\in\\mathbb{R}^{n\\times d_{g}}$ has orthonormal columns, and each column has unit Euclidean norm.\n\nYou fit an overlapping group Least Absolute Shrinkage and Selection Operator (LASSO), i.e., the convex estimator\n$$\n\\widehat{\\beta}(\\lambda)\\in\\arg\\min_{\\beta\\in\\mathbb{R}^{p}}\\left\\{\\frac{1}{2}\\|y-X\\beta\\|_{2}^{2}+\\lambda\\sum_{g\\in\\mathcal{G}}w_{g}\\|\\beta_{g}\\|_{2}\\right\\},\n$$\nwhere $\\mathcal{G}$ is a collection of (possibly overlapping) index sets $g\\subseteq\\{1,\\dots,p\\}$, $\\beta_{g}\\in\\mathbb{R}^{d_{g}}$ is the subvector of $\\beta$ restricted to group $g$, $\\lambda0$ is the regularization parameter, and $(w_{g})_{g\\in\\mathcal{G}}$ are positive group weights.\n\nAssume a global null for a single group $g$, meaning $\\beta^{\\star}_{g}=0$, and consider the null distribution of the score $X_{g}^{\\top}y$. Use only foundational facts about multivariate Gaussian distributions and convex regularization to do the following:\n\n1. Starting from the assumptions above, derive the distribution of $X_{g}^{\\top}y$ under the null and the corresponding distribution of $\\|X_{g}^{\\top}y\\|_{2}$.\n\n2. Using a first-principles argument that does not rely on specialized shortcuts, justify why a sufficient condition for the group $g$ to be excluded at a given $\\lambda$ is that $\\|X_{g}^{\\top}y\\|_{2}\\leq\\lambda w_{g}$, and explain how choosing weights proportional to the typical null size of $\\|X_{g}^{\\top}y\\|_{2}$ can control false discoveries across groups in a uniform manner.\n\n3. Compute the closed-form analytic expression for the choice $w_{g}$ proportional to the expected null magnitude $\\mathbb{E}\\|X_{g}^{\\top}y\\|_{2}$, in terms of the group size $d_{g}$ and the noise level $\\sigma$.\n\nProvide your final answer as the exact analytical expression for $w_{g}$ (up to a universal proportionality constant that does not depend on $g$). No numerical rounding is required and no units are involved.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the theory of high-dimensional statistics and convex optimization, is well-posed, and is stated in objective, unambiguous mathematical language. All necessary components for a solution are provided.\n\nThe problem is divided into three parts. We shall address them in order.\n\n### Part 1: Null Distribution of the Score\n\nWe are asked to derive the distribution of the score vector $X_{g}^{\\top}y$ and its Euclidean norm $\\|X_{g}^{\\top}y\\|_{2}$ under the null hypothesis for group $g$. The null hypothesis is stated as $\\beta^{\\star}_{g}=0$. The linear model is $y=X\\beta^{\\star}+\\varepsilon$, where $\\varepsilon\\sim\\mathcal{N}(0,\\sigma^{2}I_{n})$.\n\nUnder the null hypothesis for group $g$, we can write $\\beta^\\star$ as a vector where the components corresponding to indices in $g$ are zero. Let $\\beta^{\\star}_{\\setminus g}$ be the subvector of coefficients with indices not in group $g$, and $X_{\\setminus g}$ be the corresponding columns of $X$. The model is $y = X_{\\setminus g}\\beta^{\\star}_{\\setminus g} + \\varepsilon$.\n\nThe score vector for group $g$ is $X_{g}^{\\top}y$. Substituting the model under the null, we get:\n$$X_{g}^{\\top}y = X_{g}^{\\top}(X_{\\setminus g}\\beta^{\\star}_{\\setminus g} + \\varepsilon) = X_{g}^{\\top}X_{\\setminus g}\\beta^{\\star}_{\\setminus g} + X_{g}^{\\top}\\varepsilon$$\nThis expression shows that the score is a sum of a constant term (which depends on the true values of the other coefficients, $\\beta^\\star_{\\setminus g}$) and a random term. The random vector $X_{g}^{\\top}\\varepsilon$ is a linear transformation of the Gaussian vector $\\varepsilon$. Its distribution is therefore multivariate Gaussian.\n\nThe mean of $X_g^\\top \\varepsilon$ is $\\mathbb{E}[X_{g}^{\\top}\\varepsilon] = X_{g}^{\\top}\\mathbb{E}[\\varepsilon] = X_{g}^{\\top}0 = 0$.\nThe covariance matrix is $\\text{Cov}(X_{g}^{\\top}\\varepsilon) = X_{g}^{\\top}\\text{Cov}(\\varepsilon)(X_{g}^{\\top})^{\\top} = X_{g}^{\\top}(\\sigma^{2}I_{n})X_{g} = \\sigma^{2}X_{g}^{\\top}X_{g}$.\n\nWe are given that the submatrix $X_{g}$ has orthonormal columns. This means $X_{g}^{\\top}X_{g} = I_{d_g}$, where $d_g = |g|$ is the number of variables in group $g$.\nThus, $X_{g}^{\\top}\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2}I_{d_g})$.\n\nThe distribution of the score $X_g^\\top y$ is therefore $\\mathcal{N}(X_g^\\top X_{\\setminus g} \\beta^\\star_{\\setminus g}, \\sigma^2 I_{d_g})$. This distribution depends on the unknown nuisance parameters $\\beta^\\star_{\\setminus g}$.\n\nHowever, the term \"null distribution\" in the context of calibrating regularization parameters or statistical tests typically refers to the distribution under a global null hypothesis, $\\beta^\\star = 0$. This provides a baseline for the variability of the statistic due to noise alone, against which a signal is to be detected. Assuming this standard interpretation, we set $\\beta^\\star = 0$. The model simplifies to $y = \\varepsilon$.\n\nUnder the global null hypothesis $\\beta^\\star = 0$:\nThe score vector is $X_{g}^{\\top}y = X_{g}^{\\top}\\varepsilon$.\nAs derived above, its distribution is:\n$$X_{g}^{\\top}y \\sim \\mathcal{N}(0, \\sigma^{2}I_{d_g})$$\nThis is a $d_g$-dimensional multivariate normal distribution with mean $0$ and a diagonal covariance matrix with entries $\\sigma^2$.\n\nNext, we find the distribution of its Euclidean norm, $\\|X_{g}^{\\top}y\\|_{2}$. Let $Z = \\frac{1}{\\sigma}X_{g}^{\\top}y$. From the above, $Z \\sim \\mathcal{N}(0, I_{d_g})$. The components of $Z$, say $Z_1, \\dots, Z_{d_g}$, are independent and identically distributed as standard normal random variables, $Z_i \\sim \\mathcal{N}(0, 1)$.\nThe squared norm is $\\|Z\\|_{2}^{2} = \\sum_{i=1}^{d_g} Z_{i}^{2}$. By definition, the sum of squares of $d_g$ independent standard normal variables follows a chi-squared distribution with $d_g$ degrees of freedom.\n$$\\|Z\\|_{2}^{2} \\sim \\chi_{d_g}^{2}$$\nThe norm itself, $\\|Z\\|_{2} = \\sqrt{\\sum_{i=1}^{d_g} Z_{i}^{2}}$, follows a chi-distribution with $d_g$ degrees of freedom, denoted $\\chi_{d_g}$.\nSince $\\|X_{g}^{\\top}y\\|_{2} = \\sigma\\|Z\\|_{2}$, the distribution of $\\|X_{g}^{\\top}y\\|_{2}$ is that of $\\sigma$ times a random variable that follows a $\\chi_{d_g}$ distribution.\n\n### Part 2: Justification for Group Exclusion and Weighting\n\nWe are asked to justify why $\\|X_{g}^{\\top}y\\|_{2} \\leq \\lambda w_{g}$ is a sufficient condition for excluding group $g$ (i.e., for $\\widehat{\\beta}_g = 0$), and explain the role of a particular choice of weights $w_g$.\n\nA \"first-principles\" justification can be derived from the Karush-Kuhn-Tucker (KKT) optimality conditions for the convex optimization problem. The objective function is $L(\\beta) = \\frac{1}{2}\\|y-X\\beta\\|_{2}^{2}+\\lambda\\sum_{h\\in\\mathcal{G}}w_{h}\\|\\beta_{h}\\|_{2}$.\nA vector $\\widehat{\\beta}$ is a minimizer if and only if $0$ is in the subgradient of $L(\\beta)$ at $\\widehat{\\beta}$.\nThe subgradient is $\\partial L(\\beta) = -X^{\\top}(y-X\\beta) + \\lambda \\sum_{h\\in\\mathcal{G}}w_{h}\\partial\\|\\beta_{h}\\|_{2}$.\nHere, $\\partial\\|\\beta_h\\|_2$ is the subgradient of the Euclidean norm for the subvector $\\beta_h$.\n- If $\\beta_h \\neq 0$, $\\partial\\|\\beta_h\\|_2 = \\{ \\beta_h / \\|\\beta_h\\|_2 \\}$.\n- If $\\beta_h = 0$, $\\partial\\|\\beta_h\\|_2 = \\{u \\in \\mathbb{R}^{d_h} : \\|u\\|_2 \\le 1\\}$, which is the closed unit ball.\n\nConsider the simplest case, where the solution is $\\widehat{\\beta} = 0$. For this to be true, the KKT conditions must hold at $\\beta=0$. For each group $h \\in \\mathcal{G}$, we must have $0 \\in -X_{h}^{\\top}y + \\lambda w_{h}\\partial\\|\\beta_{h}\\|_{2}|_{\\beta_h=0}$. This means there must exist a vector $u_h$ with $\\|u_h\\|_2 \\le 1$ such that $-X_{h}^{\\top}y + \\lambda w_h u_h = 0$, which is equivalent to $X_{h}^{\\top}y = \\lambda w_h u_h$. Taking the norm of both sides, we get $\\|X_{h}^{\\top}y\\|_{2} = \\lambda w_h \\|u_h\\|_2$. Since $\\|u_h\\|_2 \\le 1$, this implies a necessary condition for this to be possible is $\\|X_{h}^{\\top}y\\|_{2} \\le \\lambda w_h$.\nThus, a sufficient condition for the *entire* vector $\\widehat{\\beta}$ to be zero is $\\|X_{h}^{\\top}y\\|_{2} \\le \\lambda w_h$ for all $h \\in \\mathcal{G}$.\n\nThis gives a strong intuition. The quantity $\\|X_g^\\top y\\|_2 / w_g$ acts as a score for group $g$. A group is not selected if this score is below the threshold $\\lambda$. While $\\|X_g^\\top y\\|_2 \\le \\lambda w_g$ is not, in the general overlapping case, strictly sufficient for $\\widehat{\\beta}_g = 0$ (as selection of other correlated groups can influence group $g$'s KKT condition), it is the foundational condition for a group to remain inactive when all other groups are inactive. This heuristic is the basis for pathwise algorithms and screening rules, and serves as a \"first-principles\" justification. A group $g$ is not selected if its score statistic is \"too small\".\n\nNow, we explain the weighting. The goal is to control false discoveries, i.e., selecting a group $g$ when its true coefficient $\\beta_g^\\star$ is $0$. Following the logic above, a false discovery for group $g$ may occur if $\\|X_g^\\top y\\|_2  \\lambda w_g$ when $\\beta_g^\\star=0$. To make the selection process fair across groups of different sizes, we want the probability of this event, under the null, to be roughly the same for all groups $g$.\n\n$P(\\text{false discovery for } g) = P(\\|X_g^\\top y\\|_2  \\lambda w_g \\mid \\beta_g^\\star=0)$\n\nFrom Part 1 (under the global null $\\beta^\\star=0$), we know the distribution of $\\|X_g^\\top y\\|_2$ depends on the group size $d_g$. Specifically, its expected value, $\\mathbb{E}\\|X_g^\\top y\\|_2$, which represents its \"typical null size\", is a function of $d_g$ and $\\sigma$. If we choose weights $w_g$ to be proportional to this typical size, $w_g \\propto \\mathbb{E}\\|X_g^\\top y\\|_2$, say $w_g = C \\cdot \\mathbb{E}\\|X_g^\\top y\\|_2$ for some constant $C$, the condition for a false discovery becomes:\n$$\\|X_g^\\top y\\|_2  \\lambda C \\cdot \\mathbb{E}\\|X_g^\\top y\\|_2 \\iff \\frac{\\|X_g^\\top y\\|_2}{\\mathbb{E}\\|X_g^\\top y\\|_2}  \\lambda C$$\nBy choosing $w_g$ this way, the thresholding is applied to a \"standardized\" statistic $\\|X_g^\\top y\\|_2 / \\mathbb{E}\\|X_g^\\top y\\|_2$, which has been normalized by its expected size under the null. The distributions of these standardized statistics are more comparable across different group sizes $d_g$ than the distributions of the raw scores $\\|X_g^\\top y\\|_2$. This choice of weights thus helps to make the false discovery probability more uniform across all groups for a fixed $\\lambda$.\n\n### Part 3: Expression for the Weight $w_g$\n\nWe need to compute the closed-form expression for $w_g$ proportional to $\\mathbb{E}\\|X_{g}^{\\top}y\\|_{2}$. As determined in Part 1, under the null hypothesis $\\beta^\\star=0$, we have $\\|X_{g}^{\\top}y\\|_{2} = \\sigma R_g$, where $R_g$ is a random variable following a chi-distribution with $d_g$ degrees of freedom. We need to compute its expectation.\n\nThe probability density function of $R_g \\sim \\chi_{d_g}$ is given by:\n$$f(x; d_g) = \\frac{2^{1-d_g/2}}{\\Gamma(d_g/2)} x^{d_g-1} \\exp(-x^2/2) \\quad \\text{for } x \\ge 0$$\nwhere $\\Gamma(\\cdot)$ is the gamma function.\n\nThe expectation is $\\mathbb{E}[R_g] = \\int_0^\\infty x f(x; d_g) dx$.\n$$\\mathbb{E}[R_g] = \\int_0^\\infty x \\left( \\frac{2^{1-d_g/2}}{\\Gamma(d_g/2)} x^{d_g-1} \\exp(-x^2/2) \\right) dx = \\frac{2^{1-d_g/2}}{\\Gamma(d_g/2)} \\int_0^\\infty x^{d_g} \\exp(-x^2/2) dx$$\nLet's evaluate the integral $I = \\int_0^\\infty x^{d_g} \\exp(-x^2/2) dx$. We use the substitution $u = x^2/2$, which implies $x = \\sqrt{2u}$ and $du = x dx$.\nThen $I = \\int_0^\\infty x^{d_g-1} \\exp(-x^2/2) (x dx)$.\n$$I = \\int_0^\\infty (\\sqrt{2u})^{d_g-1} \\exp(-u) du = (\\sqrt{2})^{d_g-1} \\int_0^\\infty u^{(d_g-1)/2} \\exp(-u) du$$\n$$I = 2^{(d_g-1)/2} \\int_0^\\infty u^{\\frac{d_g+1}{2} - 1} \\exp(-u) du$$\nThe integral is the definition of the gamma function $\\Gamma\\left(\\frac{d_g+1}{2}\\right)$.\nSo, $I = 2^{(d_g-1)/2} \\Gamma\\left(\\frac{d_g+1}{2}\\right)$.\n\nNow, substitute this back into the expression for $\\mathbb{E}[R_g]$:\n$$\\mathbb{E}[R_g] = \\frac{2^{1-d_g/2}}{\\Gamma(d_g/2)} \\cdot \\left( 2^{(d_g-1)/2} \\Gamma\\left(\\frac{d_g+1}{2}\\right) \\right)$$\nThe powers of $2$ combine: $2^{1-d_g/2 + (d_g-1)/2} = 2^{1-d_g/2+d_g/2-1/2} = 2^{1/2} = \\sqrt{2}$.\n$$\\mathbb{E}[R_g] = \\sqrt{2} \\frac{\\Gamma\\left(\\frac{d_g+1}{2}\\right)}{\\Gamma\\left(\\frac{d_g}{2}\\right)}$$\nFinally, we have $\\mathbb{E}\\|X_{g}^{\\top}y\\|_{2} = \\sigma \\mathbb{E}[R_g]$.\n$$\\mathbb{E}\\|X_{g}^{\\top}y\\|_{2} = \\sigma\\sqrt{2} \\frac{\\Gamma\\left(\\frac{d_g+1}{2}\\right)}{\\Gamma\\left(\\frac{d_g}{2}\\right)}$$\nThe weight $w_g$ is chosen to be proportional to this quantity. The problem asks for the expression for $w_g$ up to a universal proportionality constant, in terms of $d_g$ and $\\sigma$. We can thus set the constant to $1$ and provide this expression as the choice for $w_g$.",
            "answer": "$$\\boxed{\\sigma\\sqrt{2} \\frac{\\Gamma\\left(\\frac{d_g+1}{2}\\right)}{\\Gamma\\left(\\frac{d_g}{2}\\right)}}$$"
        }
    ]
}