{
    "hands_on_practices": [
        {
            "introduction": "Before applying Total Variation (TV) in complex optimization models, it is crucial to build a concrete intuition for what it measures. This first exercise takes you back to basics, asking you to compute the anisotropic and isotropic TV for a small image patch directly from their definitions . By engaging with the underlying vector norms at the pixel level, you will gain a tangible understanding of how TV quantifies image structure and why the choice of norm matters.",
            "id": "3491268",
            "problem": "Consider a discrete image model on a Cartesian grid with unit spacing. Let the discrete gradient at pixel $(i,j)$ be defined by the forward-difference operator with homogeneous Neumann boundary conditions (i.e., any forward difference that would access an out-of-bounds pixel is set to zero). Specifically, the discrete gradient at $(i,j)$ is the two-vector consisting of the forward horizontal and vertical differences at $(i,j)$. The anisotropic total variation (TV) of a discrete image is the sum, over all pixels, of the $\\ell_{1}$ norm of the discrete gradient vector at each pixel. The isotropic total variation is the sum, over all pixels, of the $\\ell_{2}$ norm of the discrete gradient vector at each pixel.\n\nYou are given the $3\\times 3$ image patch with pixel values\n$$\nU \\;=\\;\n\\begin{pmatrix}\n1  2  2 \\\\\n1  3  5 \\\\\n2  3  6\n\\end{pmatrix}.\n$$\nUsing only the above fundamental definitions and the stated boundary model, do the following:\n- Compute explicitly the anisotropic total variation and the isotropic total variation of $U$.\n- Explain concisely, from first principles of vector norms, why these two values differ numerically for this patch and identify the structural source of the difference at the pixel level.\n\nLet $D$ denote the exact difference between the anisotropic and isotropic total variations,\n$$\nD \\;=\\; \\mathrm{TV}_{\\mathrm{anisotropic}}(U) \\;-\\; \\mathrm{TV}_{\\mathrm{isotropic}}(U).\n$$\nReport $D$ as a single closed-form analytic expression. Do not round your answer.",
            "solution": "The problem statement is internally consistent, scientifically grounded in the principles of numerical image analysis and vector norms, and provides all necessary information to compute a unique solution. Therefore, the problem is deemed valid. We proceed with the solution.\n\nLet the discrete image be represented by the matrix $U$, where $U_{i,j}$ is the pixel value at row $i$ and column $j$. The grid is a $3 \\times 3$ Cartesian grid, so the indices $(i,j)$ range from $(1,1)$ to $(3,3)$. The given image is:\n$$\nU =\n\\begin{pmatrix}\n1  2  2 \\\\\n1  3  5 \\\\\n2  3  6\n\\end{pmatrix}\n$$\nThe discrete gradient at a pixel $(i,j)$ is a vector $\\nabla U_{i,j} = \\begin{pmatrix} (\\nabla_x U)_{i,j} \\\\ (\\nabla_y U)_{i,j} \\end{pmatrix}$. The components are defined by forward differences:\n$$\n(\\nabla_x U)_{i,j} = U_{i,j+1} - U_{i,j} \\quad \\text{(horizontal)}\n$$\n$$\n(\\nabla_y U)_{i,j} = U_{i+1,j} - U_{i,j} \\quad \\text{(vertical)}\n$$\nThe homogeneous Neumann boundary conditions imply that any difference that requires an out-of-bounds pixel is set to zero. For a $3 \\times 3$ image, this means $(\\nabla_x U)_{i,3} = 0$ for $i \\in \\{1,2,3\\}$ and $(\\nabla_y U)_{3,j} = 0$ for $j \\in \\{1,2,3\\}$.\n\nFirst, we compute the discrete gradient vector for each of the $9$ pixels:\n- At $(1,1)$: $\\nabla U_{1,1} = \\begin{pmatrix} U_{1,2} - U_{1,1} \\\\ U_{2,1} - U_{1,1} \\end{pmatrix} = \\begin{pmatrix} 2 - 1 \\\\ 1 - 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- At $(1,2)$: $\\nabla U_{1,2} = \\begin{pmatrix} U_{1,3} - U_{1,2} \\\\ U_{2,2} - U_{1,2} \\end{pmatrix} = \\begin{pmatrix} 2 - 2 \\\\ 3 - 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n- At $(1,3)$: $\\nabla U_{1,3} = \\begin{pmatrix} 0 \\\\ U_{2,3} - U_{1,3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 5 - 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$\n- At $(2,1)$: $\\nabla U_{2,1} = \\begin{pmatrix} U_{2,2} - U_{2,1} \\\\ U_{3,1} - U_{2,1} \\end{pmatrix} = \\begin{pmatrix} 3 - 1 \\\\ 2 - 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$\n- At $(2,2)$: $\\nabla U_{2,2} = \\begin{pmatrix} U_{2,3} - U_{2,2} \\\\ U_{3,2} - U_{2,2} \\end{pmatrix} = \\begin{pmatrix} 5 - 3 \\\\ 3 - 3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$\n- At $(2,3)$: $\\nabla U_{2,3} = \\begin{pmatrix} 0 \\\\ U_{3,3} - U_{2,3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 6 - 5 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n- At $(3,1)$: $\\nabla U_{3,1} = \\begin{pmatrix} U_{3,2} - U_{3,1} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 - 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- At $(3,2)$: $\\nabla U_{3,2} = \\begin{pmatrix} U_{3,3} - U_{3,2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 6 - 3 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$\n- At $(3,3)$: $\\nabla U_{3,3} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n\nNext, we compute the anisotropic total variation, $\\mathrm{TV}_{\\mathrm{anisotropic}}(U)$, which is the sum of the $\\ell_1$ norms of these gradient vectors. For a vector $v = \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix}$, its $\\ell_1$ norm is $\\|v\\|_1 = |v_x| + |v_y|$.\n$$\n\\mathrm{TV}_{\\mathrm{anisotropic}}(U) = \\sum_{i=1}^{3} \\sum_{j=1}^{3} \\|\\nabla U_{i,j}\\|_1\n$$\n$$\n\\mathrm{TV}_{\\mathrm{anisotropic}}(U) = (|1|+|0|) + (|0|+|1|) + (|0|+|3|) + (|2|+|1|) + (|2|+|0|) + (|0|+|1|) + (|1|+|0|) + (|3|+|0|) + (|0|+|0|)\n$$\n$$\n\\mathrm{TV}_{\\mathrm{anisotropic}}(U) = 1 + 1 + 3 + 3 + 2 + 1 + 1 + 3 + 0 = 15\n$$\n\nThen, we compute the isotropic total variation, $\\mathrm{TV}_{\\mathrm{isotropic}}(U)$, which is the sum of the $\\ell_2$ norms. For a vector $v = \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix}$, its $\\ell_2$ norm is $\\|v\\|_2 = \\sqrt{v_x^2 + v_y^2}$.\n$$\n\\mathrm{TV}_{\\mathrm{isotropic}}(U) = \\sum_{i=1}^{3} \\sum_{j=1}^{3} \\|\\nabla U_{i,j}\\|_2\n$$\n$$\n\\mathrm{TV}_{\\mathrm{isotropic}}(U) = \\sqrt{1^2+0^2} + \\sqrt{0^2+1^2} + \\sqrt{0^2+3^2} + \\sqrt{2^2+1^2} + \\sqrt{2^2+0^2} + \\sqrt{0^2+1^2} + \\sqrt{1^2+0^2} + \\sqrt{3^2+0^2} + \\sqrt{0^2+0^2}\n$$\n$$\n\\mathrm{TV}_{\\mathrm{isotropic}}(U) = 1 + 1 + 3 + \\sqrt{5} + 2 + 1 + 1 + 3 + 0 = 12 + \\sqrt{5}\n$$\n\nThe two values differ because of the fundamental properties of the $\\ell_1$ and $\\ell_2$ norms. For any vector $v$ in $\\mathbb{R}^n$, the inequality $\\|v\\|_1 \\ge \\|v\\|_2$ holds. In our case, for a $2$-dimensional gradient vector $v = \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix}$, we have $\\|v\\|_1 = |v_x|+|v_y|$ and $\\|v\\|_2 = \\sqrt{v_x^2+v_y^2}$. Equality, $|v_x|+|v_y| = \\sqrt{v_x^2+v_y^2}$, holds if and only if at least one component, $v_x$ or $v_y$, is zero. This corresponds to a gradient that is purely horizontal or purely vertical.\n\nThe numerical difference between $\\mathrm{TV}_{\\mathrm{anisotropic}}(U)$ and $\\mathrm{TV}_{\\mathrm{isotropic}}(U)$ arises from the sum of the differences $\\|\\nabla U_{i,j}\\|_1 - \\|\\nabla U_{i,j}\\|_2$ over all pixels. This difference is non-zero only at pixels where both components of the gradient vector are non-zero.\nExamining our computed gradients, only the pixel at $(2,1)$ has a gradient vector with two non-zero components: $\\nabla U_{2,1} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$. For all other pixels, the gradient vector has at least one zero component, meaning $\\|\\nabla U_{i,j}\\|_1 = \\|\\nabla U_{i,j}\\|_2$ for all $(i,j) \\neq (2,1)$.\nThe structural source of the difference is therefore located entirely at pixel $(2,1)$, which represents a corner or a point where the image intensity changes in both the horizontal and vertical directions simultaneously.\n\nFinally, we compute the exact difference $D$:\n$$\nD = \\mathrm{TV}_{\\mathrm{anisotropic}}(U) - \\mathrm{TV}_{\\mathrm{isotropic}}(U)\n$$\nThis difference is the sum of the term-by-term differences, which simplifies to the difference at the single contributing pixel:\n$$\nD = (\\|\\nabla U_{2,1}\\|_1 - \\|\\nabla U_{2,1}\\|_2) = (|2|+|1|) - \\sqrt{2^2+1^2} = 3 - \\sqrt{5}\n$$\nAlternatively, using the total computed values:\n$$\nD = 15 - (12 + \\sqrt{5}) = 3 - \\sqrt{5}\n$$\nThe result is consistent.",
            "answer": "$$\n\\boxed{3 - \\sqrt{5}}\n$$"
        },
        {
            "introduction": "With a grasp of the primal definition of Total Variation, we now turn to a more powerful and abstract viewpoint: its dual formulation. This practice guides you through deriving the dual characterization of the TV-norm and constructing a dual-feasible field for a simple piecewise-constant image . This exercise is fundamental to understanding the theoretical underpinnings of many state-of-the-art optimization schemes and reveals how the primal and dual perspectives are intimately connected.",
            "id": "3491283",
            "problem": "Consider the discrete anisotropic total variation (TV) for image denoising and deblurring on a rectangular grid, a widely used regularizer in compressed sensing and sparse optimization. Let the image be defined on a $4 \\times 4$ grid with pixel values $u_{i,j} \\in \\mathbb{R}$ for $i \\in \\{1,2,3,4\\}$ and $j \\in \\{1,2,3,4\\}$. Define the forward difference operators by\n$$\n(D_x u)_{i,j} = u_{i+1,j} - u_{i,j}, \\quad 1 \\leq i \\leq 3, \\ 1 \\leq j \\leq 4,\n$$\n$$\n(D_y u)_{i,j} = u_{i,j+1} - u_{i,j}, \\quad 1 \\leq j \\leq 3, \\ 1 \\leq i \\leq 4,\n$$\nand the anisotropic total variation of $u$ by\n$$\n\\mathrm{TV}(u) = \\sum_{i=1}^{3} \\sum_{j=1}^{4} \\left| (D_x u)_{i,j} \\right| + \\sum_{i=1}^{4} \\sum_{j=1}^{3} \\left| (D_y u)_{i,j} \\right|.\n$$\nStarting from the fundamental property of the $\\ell_{1}$ norm that $|a| = \\sup_{|s| \\leq 1} s a$ for any $a \\in \\mathbb{R}$, derive the dual support-function characterization of $\\mathrm{TV}(u)$ in terms of a dual field $p = (p^x, p^y)$ with components satisfying $\\|p\\|_{\\infty} \\leq 1$, where\n$$\n\\|p\\|_{\\infty} = \\max\\left\\{ \\max_{1 \\leq i \\leq 3, \\ 1 \\leq j \\leq 4} |p^x_{i,j}|, \\ \\max_{1 \\leq i \\leq 4, \\ 1 \\leq j \\leq 3} |p^y_{i,j}| \\right\\},\n$$\nand the dual objective\n$$\n\\langle D u, p \\rangle = \\sum_{i=1}^{3} \\sum_{j=1}^{4} p^x_{i,j} (D_x u)_{i,j} + \\sum_{i=1}^{4} \\sum_{j=1}^{3} p^y_{i,j} (D_y u)_{i,j}.\n$$\nNow consider the simple piecewise constant image\n$$\nu_{i,j} = \\begin{cases}\n1,  \\text{if } i \\in \\{2,3\\} \\text{ and } j \\in \\{2,3\\}, \\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\nTasks:\n1. Derive the dual support-function characterization of $\\mathrm{TV}(u)$ as a supremum of $\\langle D u, p \\rangle$ over dual-feasible fields with $\\|p\\|_{\\infty} \\leq 1$, using only the property $|a| = \\sup_{|s| \\leq 1} s a$ and linearity of summation.\n2. Construct an explicit dual-feasible field $p = (p^x, p^y)$ for the given $u$ by choosing $p^x_{i,j}$ and $p^y_{i,j}$ on the grid so that each component lies in $[-1,1]$ and yields a nontrivial dual objective.\n3. Evaluate the dual objective $\\langle D u, p \\rangle$ for your constructed $p$.\n4. Compute directly the primal $\\mathrm{TV}(u)$ by summing the absolute forward differences for the given $u$.\n5. Use the dual objective to bound $\\mathrm{TV}(u)$ from below and the primal summation to bound $\\mathrm{TV}(u)$ from above. Report your final lower and upper bounds as a two-entry row vector. Provide the bounds exactly; no rounding is needed and no physical units are involved.",
            "solution": "The problem asks for a multi-step analysis of the discrete anisotropic total variation (TV) of a specific image. This involves deriving the dual formulation of the TV semi-norm, computing the primal TV value, constructing a dual-feasible field, evaluating the corresponding dual objective, and finally reporting the resulting lower and upper bounds on the TV value.\n\n### Task 1: Derivation of the Dual Support-Function Characterization\n\nThe anisotropic total variation is defined as the $\\ell_1$ norm of the discrete gradient of the image $u$. Let $Du = ((D_x u), (D_y u))$ represent the discrete gradient. The definition is:\n$$\n\\mathrm{TV}(u) = \\sum_{i=1}^{3} \\sum_{j=1}^{4} \\left| (D_x u)_{i,j} \\right| + \\sum_{i=1}^{4} \\sum_{j=1}^{3} \\left| (D_y u)_{i,j} \\right|\n$$\nWe are given the fundamental property of the absolute value (which is the $\\ell_1$ norm in $\\mathbb{R}$) in terms of its support function: for any scalar $a \\in \\mathbb{R}$, $|a| = \\sup_{|s| \\leq 1} s a$. We apply this property to each term in the sum defining $\\mathrm{TV}(u)$.\n\nFor each pair of indices $(i,j)$ corresponding to the domain of $D_x u$, we can write:\n$$\n\\left| (D_x u)_{i,j} \\right| = \\sup_{|p^x_{i,j}| \\leq 1} p^x_{i,j} (D_x u)_{i,j}\n$$\nSimilarly, for each pair of indices $(i,j)$ corresponding to the domain of $D_y u$, we have:\n$$\n\\left| (D_y u)_{i,j} \\right| = \\sup_{|p^y_{i,j}| \\leq 1} p^y_{i,j} (D_y u)_{i,j}\n$$\nHere, $p^x_{i,j}$ and $p^y_{i,j}$ are scalar dual variables associated with each difference term.\n\nSubstituting these into the definition of $\\mathrm{TV}(u)$:\n$$\n\\mathrm{TV}(u) = \\sum_{i=1}^{3} \\sum_{j=1}^{4} \\left( \\sup_{|p^x_{i,j}| \\leq 1} p^x_{i,j} (D_x u)_{i,j} \\right) + \\sum_{i=1}^{4} \\sum_{j=1}^{3} \\left( \\sup_{|p^y_{i,j}| \\leq 1} p^y_{i,j} (D_y u)_{i,j} \\right)\n$$\nSince the supremum for each term is taken independently over a separate variable, the sum of suprema is equal to the supremum of the sum:\n$$\n\\mathrm{TV}(u) = \\sup_{\\substack{|p^x_{i,j}| \\leq 1 \\ \\forall i,j \\\\ |p^y_{i,j}| \\leq 1 \\ \\forall i,j}} \\left( \\sum_{i=1}^{3} \\sum_{j=1}^{4} p^x_{i,j} (D_x u)_{i,j} + \\sum_{i=1}^{4} \\sum_{j=1}^{3} p^y_{i,j} (D_y u)_{i,j} \\right)\n$$\nThe set of constraints on all components of the dual field $p = (p^x, p^y)$, i.e., $|p^x_{i,j}| \\leq 1$ for all relevant $(i,j)$ and $|p^y_{i,j}| \\leq 1$ for all relevant $(i,j)$, is precisely the condition that the infinity norm of the dual field is bounded by $1$:\n$$\n\\|p\\|_{\\infty} = \\max\\left\\{ \\max_{i,j} |p^x_{i,j}|, \\ \\max_{i,j} |p^y_{i,j}| \\right\\} \\leq 1\n$$\nThe expression inside the supremum is the definition of the dual objective, $\\langle D u, p \\rangle$. Therefore, we have derived the dual support-function characterization of the total variation:\n$$\n\\mathrm{TV}(u) = \\sup_{\\|p\\|_{\\infty} \\leq 1} \\langle D u, p \\rangle\n$$\n\n### Application to the Specific Image\n\nWe are given a specific $4 \\times 4$ image $u$:\n$$\nu_{i,j} = \\begin{cases}\n1,  \\text{if } i \\in \\{2,3\\} \\text{ and } j \\in \\{2,3\\}, \\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\nIn matrix form, with row index $i$ and column index $j$:\n$$\nu = \\begin{pmatrix}\n0  0  0  0 \\\\\n0  1  1  0 \\\\\n0  1  1  0 \\\\\n0  0  0  0\n\\end{pmatrix}\n$$\n\n### Task 4: Computation of the Primal $\\mathrm{TV}(u)$\n\nFirst, we compute the horizontal differences $(D_x u)_{i,j} = u_{i,j+1} - u_{i,j}$ for $i \\in \\{1,2,3,4\\}, j \\in \\{1,2,3\\}$. Note the problem defines $D_x$ slightly differently from convention, as a difference in columns. We will adhere to the problem's definitions: $(D_x u)_{i,j} = u_{i+1,j} - u_{i,j}$ (vertical differences), and $(D_y u)_{i,j} = u_{i,j+1} - u_{i,j}$ (horizontal differences).\n\nFor $i=1$: $(D_x u)_{1,j} = u_{2,j} - u_{1,j} \\Rightarrow (0-0, 1-0, 1-0, 0-0) = (0, 1, 1, 0)$.\nFor $i=2$: $(D_x u)_{2,j} = u_{3,j} - u_{2,j} \\Rightarrow (0-0, 1-1, 1-1, 0-0) = (0, 0, 0, 0)$.\nFor $i=3$: $(D_x u)_{3,j} = u_{4,j} - u_{3,j} \\Rightarrow (0-0, 0-1, 0-1, 0-0) = (0, -1, -1, 0)$.\nThe matrix of vertical differences is:\n$$\nD_x u = \\begin{pmatrix}\n0  1  1  0 \\\\\n0  0  0  0 \\\\\n0  -1  -1  0\n\\end{pmatrix}\n$$\nThe sum of absolute values is $\\sum_{i,j} |(D_x u)_{i,j}| = |1| + |1| + |-1| + |-1| = 4$.\n\nNext, we compute the horizontal differences $(D_y u)_{i,j} = u_{i,j+1} - u_{i,j}$ for $i \\in \\{1,2,3,4\\}, j \\in \\{1,2,3\\}$.\nFor $j=1$: $(D_y u)_{i,1} = u_{i,2} - u_{i,1} \\Rightarrow (0-0, 1-0, 1-0, 0-0) = (0, 1, 1, 0)$.\nFor $j=2$: $(D_y u)_{i,2} = u_{i,3} - u_{i,2} \\Rightarrow (0-0, 1-1, 1-1, 0-0) = (0, 0, 0, 0)$.\nFor $j=3$: $(D_y u)_{i,3} = u_{i,4} - u_{i,3} \\Rightarrow (0-0, 0-1, 0-1, 0-0) = (0, -1, -1, 0)$.\nThe matrix of horizontal differences is:\n$$\nD_y u = \\begin{pmatrix}\n0  0  0 \\\\\n1  0  -1 \\\\\n1  0  -1 \\\\\n0  0  0\n\\end{pmatrix}\n$$\nThe sum of absolute values is $\\sum_{i,j} |(D_y u)_{i,j}| = |1| + |1| + |-1| + |-1| = 4$.\n\nThe total variation is the sum of these two quantities:\n$$\n\\mathrm{TV}(u) = 4 + 4 = 8\n$$\nThis value provides an upper bound on $\\mathrm{TV}(u)$ (trivially, it is the least upper bound).\n\n### Task 2: Construction of a Dual-Feasible Field $p$\n\nTo maximize the dual objective $\\langle D u, p \\rangle = \\sum p^k (Du)_k$ subject to $|p^k| \\leq 1$, we should choose $p^k$ to have the same sign as $(Du)_k$. The optimal choice that attains the supremum is $p^k = \\mathrm{sgn}((Du)_k)$, where $\\mathrm{sgn}(0)$ can be any value in $[-1,1]$. We will set $\\mathrm{sgn}(0)=0$.\n\nBased on the computed $D_x u$ and $D_y u$:\n$$\np^x_{i,j} = \\mathrm{sgn}((D_x u)_{i,j}) \\quad \\Rightarrow \\quad p^x = \\begin{pmatrix}\n0  1  1  0 \\\\\n0  0  0  0 \\\\\n0  -1  -1  0\n\\end{pmatrix}\n$$\n$$\np^y_{i,j} = \\mathrm{sgn}((D_y u)_{i,j}) \\quad \\Rightarrow \\quad p^y = \\begin{pmatrix}\n0  0  0 \\\\\n1  0  -1 \\\\\n1  0  -1 \\\\\n0  0  0\n\\end{pmatrix}\n$$\nThis dual field $p=(p^x, p^y)$ is dual-feasible because all its components are in $\\{-1, 0, 1\\}$, which implies that $\\|p\\|_{\\infty} = 1 \\leq 1$.\n\n### Task 3: Evaluation of the Dual Objective\n\nWe evaluate $\\langle D u, p \\rangle$ for the constructed field $p$.\n$$\n\\langle D u, p \\rangle = \\sum_{i=1}^{3} \\sum_{j=1}^{4} p^x_{i,j} (D_x u)_{i,j} + \\sum_{i=1}^{4} \\sum_{j=1}^{3} p^y_{i,j} (D_y u)_{i,j}\n$$\nSince we chose $p^x_{i,j} = \\mathrm{sgn}((D_x u)_{i,j})$, each term in the first sum is $p^x_{i,j} (D_x u)_{i,j} = \\mathrm{sgn}((D_x u)_{i,j}) (D_x u)_{i,j} = |(D_x u)_{i,j}|$.\nThe first sum is therefore $\\sum_{i,j} |(D_x u)_{i,j}| = 4$.\nSimilarly, the second sum is $\\sum_{i,j} |(D_y u)_{i,j}| = 4$.\nThe dual objective is:\n$$\n\\langle D u, p \\rangle = 4 + 4 = 8\n$$\n\n### Task 5: Reporting Lower and Upper Bounds\n\nThe principle of weak duality states that for any dual-feasible field $p$, the dual objective provides a lower bound for the primal objective. Thus, $\\langle D u, p \\rangle \\leq \\mathrm{TV}(u)$.\nFrom Task 3, we computed $\\langle D u, p \\rangle = 8$ for a specific feasible $p$. This gives a lower bound:\n$$\n\\mathrm{TV}(u) \\geq 8\n$$\nFrom Task 4, we computed the primal value directly as $\\mathrm{TV}(u) = 8$. This value is itself an upper bound.\n$$\n\\mathrm{TV}(u) \\leq 8\n$$\nCombining these, we have a lower bound of $8$ and an upper bound of $8$. The problem requests these bounds as a two-entry row vector. The fact that the lower and upper bounds coincide demonstrates strong duality for this problem.\n\nFinal Bounds: [Lower Bound, Upper Bound] = [$8$, $8$].",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n8  8\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Theory becomes practice in our final exercise, where we use a modern primal-dual algorithm to solve a TV denoising problem. You will perform two complete iterations of the Chambolle-Pock method, a powerful algorithm that explicitly leverages the dual formulation we explored previously . By manually updating the primal and dual variables and calculating the resulting residuals, you will gain firsthand experience with the mechanics of a state-of-the-art solver used in computational imaging.",
            "id": "3491269",
            "problem": "Consider the Rudin–Osher–Fatemi total variation model for image denoising, augmented here with the identity forward operator to emphasize the denoising case, namely the convex program\n$$\n\\min_{x \\in \\mathbb{R}^{4}} \\;\\; \\frac{1}{2}\\|x - y\\|_{2}^{2} \\;+\\; \\lambda \\sum_{i=1}^{2}\\sum_{j=1}^{2} \\left\\|(\\nabla x)_{i,j}\\right\\|_{2},\n$$\nwhere the image patch $x$ is a $2 \\times 2$ array flattened in row-major order, the observed data $y$ is given, $\\lambda  0$ is the total variation weight, and $\\nabla$ denotes the discrete forward-difference gradient with homogeneous Neumann boundary conditions. The discrete gradient at pixel $(i,j)$ is defined componentwise by $(\\nabla x)_{i,j} = \\big(x_{i+1,j} - x_{i,j}, \\; x_{i,j+1} - x_{i,j}\\big)$ with the convention that forward differences that point outside the domain are zero. Let $K = \\nabla$ and $K^{\\top}$ be its adjoint, which coincides with the negative divergence under the stated boundary conditions. The problem can be cast in the standard form $\\min_{x} G(x) + F(Kx)$ with $G(x) = \\frac{1}{2}\\|x - y\\|_{2}^{2}$ and $F(z) = \\lambda \\sum_{i,j} \\|z_{i,j}\\|_{2}$.\n\nUsing the Primal–Dual Hybrid Gradient (PDHG), also known as the Chambolle–Pock (CP) method, perform exactly two iterations starting from $x^{0} = y$, $p^{0} = 0$, with step sizes $\\tau = 0.25$, $\\sigma = 0.25$, relaxation parameter $\\theta = 0$, and regularization weight $\\lambda = 0.8$, on the following $2 \\times 2$ observed patch (flattened in row-major order):\n$$\ny = \\begin{pmatrix} 1.0  0.6 \\\\ 0.4  0.2 \\end{pmatrix}.\n$$\nUse the isotropic total variation defined above, and for the data fidelity the identity forward operator. Assume the operator norm bound $\\|K\\|^{2} \\leq 8$ so that the step-size condition $\\sigma \\tau \\|K\\|^{2}  1$ is satisfied. Define the primal and dual residuals (for $\\theta = 0$) at iteration $k+1$ by\n$$\nr_{\\mathrm{p}}^{k+1} \\;=\\; \\left\\| \\frac{1}{\\tau}\\big(x^{k} - x^{k+1}\\big) \\;-\\; K^{\\top}\\big(p^{k+1} - p^{k}\\big) \\right\\|_{2}, \n\\qquad\nr_{\\mathrm{d}}^{k+1} \\;=\\; \\left\\| \\frac{1}{\\sigma}\\big(p^{k} - p^{k+1}\\big) \\;-\\; K\\big(x^{k} - x^{k+1}\\big) \\right\\|_{2}.\n$$\n\nCompute the two CP iterations under these settings and report the pair consisting of the primal residual norm and the dual residual norm after the second iteration, that is $\\big(r_{\\mathrm{p}}^{2}, r_{\\mathrm{d}}^{2}\\big)$. Express the final pair as dimensionless numbers and round your answer to four significant figures.",
            "solution": "The problem describes a standard application of the Chambolle-Pock algorithm to the Rudin-Osher-Fatemi (ROF) model for image denoising. All definitions, parameters, and initial conditions are provided and are consistent with the established literature in convex optimization and computational imaging. The problem is mathematically well-defined and self-contained. We proceed with the solution.\n\nThe Chambolle–Pock algorithm for the problem $\\min_{x} G(x) + F(Kx)$ with relaxation $\\theta=0$ generates a sequence $(x^k, p^k)$ via the updates:\n$$\n\\begin{align*}\np^{k+1} = \\mathrm{prox}_{\\sigma F^*}(p^k + \\sigma K x^k) \\\\\nx^{k+1} = \\mathrm{prox}_{\\tau G}(x^k - \\tau K^\\top p^{k+1})\n\\end{align*}\n$$\n\n**Proximal Operators**\n1.  For $G(x) = \\frac{1}{2}\\|x-y\\|_2^2$, the proximal operator is:\n    $\\mathrm{prox}_{\\tau G}(v) = \\arg\\min_x \\left( \\frac{1}{2}\\|x-y\\|_2^2 + \\frac{1}{2\\tau}\\|x-v\\|_2^2 \\right) = \\frac{v + \\tau y}{1+\\tau}$.\n2.  For $F(z) = \\lambda \\sum_{i,j} \\|z_{i,j}\\|_2$, its convex conjugate $F^*(p)$ is the indicator function of the set where $\\|p_{i,j}\\|_2 \\leq \\lambda$ for all $(i,j)$. The proximal operator $\\mathrm{prox}_{\\sigma F^*}(v)$ is the Euclidean projection onto this set, which acts component-wise on the vector groups $v_{i,j}$:\n    $(\\mathrm{prox}_{\\sigma F^*}(v))_{i,j} = \\frac{v_{i,j}}{\\max(1, \\|v_{i,j}\\|_2 / \\lambda)}$.\n\n**Operators $K$ and $K^\\top$**\nThe image is represented by $x = (x_1, x_2, x_3, x_4)^\\top$, corresponding to pixels $(x_{11}, x_{12}, x_{21}, x_{22})$. The gradient operator $K=\\nabla$ is defined as $(\\nabla x)_{i,j} = \\big(x_{i+1,j} - x_{i,j}, \\; x_{i,j+1} - x_{i,j}\\big)$, i.e., (vertical, horizontal). With Neumann boundary conditions, this gives:\n$(\\nabla x)_{11} = (x_{21}-x_{11}, x_{12}-x_{11}) = (x_3-x_1, x_2-x_1)$.\n$(\\nabla x)_{12} = (x_{22}-x_{12}, 0) = (x_4-x_2, 0)$.\n$(\\nabla x)_{21} = (0, x_{22}-x_{21}) = (0, x_4-x_3)$.\n$(\\nabla x)_{22} = (0, 0)$.\nThe operator $K$ and its adjoint $K^\\top$ are thus represented by the matrices:\n$$\nK = \\begin{pmatrix} -1  0  1  0 \\\\ -1  1  0  0 \\\\ 0  -1  0  1 \\\\ 0  0  0  0 \\\\ 0  0  0  0 \\\\ 0  0  -1  1 \\\\ 0  0  0  0 \\\\ 0  0  0  0 \\end{pmatrix}, \\quad K^\\top = \\begin{pmatrix} -1  -1  0  0  0  0  0  0 \\\\ 0  1  -1  0  0  0  0  0 \\\\ 1  0  0  0  0  -1  0  0 \\\\ 0  0  1  0  0  1  0  0 \\end{pmatrix}.\n$$\n\n**Iteration 1** ($k=0$)\nInitial state: $x^{0} = y = (1.0, 0.6, 0.4, 0.2)^{\\top}$ and $p^{0} = 0$.\nParameters: $\\tau=0.25$, $\\sigma=0.25$, $\\lambda=0.8$.\n\n1.  **Dual update for $p^1$**:\n    $p^1 = \\mathrm{prox}_{\\sigma F^*}(p^0 + \\sigma K x^0) = \\mathrm{prox}_{\\sigma F^*}(\\sigma K x^0)$.\n    $Kx^0 = (0.4-1.0, 0.6-1.0, 0.2-0.6, 0, 0, 0.2-0.4, 0, 0)^\\top = (-0.6, -0.4, -0.4, 0, 0, -0.2, 0, 0)^\\top$.\n    Let $v^0 = \\sigma K x^0 = 0.25 \\cdot Kx^0 = (-0.15, -0.1, -0.1, 0, 0, -0.05, 0, 0)^\\top$.\n    We must project each 2D sub-vector of $v^0$ onto the ball of radius $\\lambda=0.8$.\n    $v^0_{1,1} = (-0.15, -0.1)^\\top$, $\\|v^0_{1,1}\\|_2 = \\sqrt{(-0.15)^2 + (-0.1)^2} = \\sqrt{0.0325} \\approx 0.1803  0.8$.\n    $v^0_{1,2} = (-0.1, 0)^\\top$, $\\|v^0_{1,2}\\|_2 = 0.1  0.8$.\n    $v^0_{2,1} = (0, -0.05)^\\top$, $\\|v^0_{2,1}\\|_2 = 0.05  0.8$.\n    Since all norms are less than $\\lambda$, no projection occurs. Thus, $p^1 = v^0$:\n    $p^1 = (-0.15, -0.1, -0.1, 0, 0, -0.05, 0, 0)^\\top$.\n\n2.  **Primal update for $x^1$**:\n    $x^1 = \\mathrm{prox}_{\\tau G}(x^0 - \\tau K^\\top p^1) = \\frac{(x^0 - \\tau K^\\top p^1) + \\tau y}{1+\\tau}$. Since $x^0 = y$, this simplifies to $x^1 = y - \\frac{\\tau}{1+\\tau} K^\\top p^1$.\n    $\\frac{\\tau}{1+\\tau} = \\frac{0.25}{1.25} = 0.2$.\n    $K^\\top p^1 = (-(-0.15)-(-0.1), (-0.1)-(-0.1), (-0.15)-(-0.05), -0.1+(-0.05))^\\top = (0.25, 0, -0.1, -0.15)^\\top$.\n    $x^1 = (1.0, 0.6, 0.4, 0.2)^\\top - 0.2 \\cdot (0.25, 0, -0.1, -0.15)^\\top = (1.0, 0.6, 0.4, 0.2)^\\top - (0.05, 0, -0.02, -0.03)^\\top$.\n    $x^1 = (0.95, 0.6, 0.42, 0.23)^\\top$.\n\n**Iteration 2** ($k=1$)\nCurrent state: $x^1 = (0.95, 0.6, 0.42, 0.23)^\\top$, $p^1 = (-0.15, -0.1, -0.1, 0, 0, -0.05, 0, 0)^\\top$.\n\n1.  **Dual update for $p^2$**:\n    $p^2 = \\mathrm{prox}_{\\sigma F^*}(p^1 + \\sigma K x^1)$.\n    $Kx^1 = (0.42-0.95, 0.6-0.95, 0.23-0.6, 0, 0, 0.23-0.42, 0, 0)^\\top = (-0.53, -0.35, -0.37, 0, 0, -0.19, 0, 0)^\\top$.\n    $\\sigma Kx^1 = 0.25 \\cdot Kx^1 = (-0.1325, -0.0875, -0.0925, 0, 0, -0.0475, 0, 0)^\\top$.\n    Let $v^1 = p^1 + \\sigma K x^1 = (-0.2825, -0.1875, -0.1925, 0, 0, -0.0975, 0, 0)^\\top$.\n    Check norms for projection:\n    $v^1_{1,1} = (-0.2825, -0.1875)^\\top$, $\\|v^1_{1,1}\\|_2 = \\sqrt{(-0.2825)^2 + (-0.1875)^2} = \\sqrt{0.1149625} \\approx 0.3391  0.8$.\n    $v^1_{1,2} = (-0.1925, 0)^\\top$, $\\|v^1_{1,2}\\|_2 = 0.1925  0.8$.\n    $v^1_{2,1} = (0, -0.0975)^\\top$, $\\|v^1_{2,1}\\|_2 = 0.0975  0.8$.\n    Again, no projection occurs. Thus, $p^2 = v^1$.\n\n2.  **Primal update for $x^2$**:\n    $x^2 = \\mathrm{prox}_{\\tau G}(x^1 - \\tau K^\\top p^2) = \\frac{(x^1 - \\tau K^\\top p^2) + \\tau y}{1+\\tau}$.\n    $K^\\top p^2 = (-(-0.2825)-(-0.1875), -0.1875-(-0.1925), -0.2825-(-0.0975), -0.1925+(-0.0975))^\\top = (0.47, 0.005, -0.185, -0.29)^\\top$.\n    $\\tau K^\\top p^2 = 0.25 \\cdot (0.47, 0.005, -0.185, -0.29)^\\top = (0.1175, 0.00125, -0.04625, -0.0725)^\\top$.\n    $x^1 - \\tau K^\\top p^2 = (0.95, 0.6, 0.42, 0.23)^\\top - (0.1175, 0.00125, -0.04625, -0.0725)^\\top = (0.8325, 0.59875, 0.46625, 0.3025)^\\top$.\n    $\\tau y = 0.25 \\cdot (1.0, 0.6, 0.4, 0.2)^\\top = (0.25, 0.15, 0.1, 0.05)^\\top$.\n    $x^2 = \\frac{1}{1.25} ((0.8325, 0.59875, 0.46625, 0.3025)^\\top + (0.25, 0.15, 0.1, 0.05)^\\top) = 0.8 \\cdot (1.0825, 0.74875, 0.56625, 0.3525)^\\top$.\n    $x^2 = (0.866, 0.599, 0.453, 0.282)^\\top$.\n\n**Residual Calculation for the Second Iteration** ($k=1$)\nWe compute the residuals $r_{\\mathrm{p}}^{2}$ and $r_{\\mathrm{d}}^{2}$.\n\n1.  **Primal Residual $r_{\\mathrm{p}}^{2}$**:\n    $r_{\\mathrm{p}}^{2} = \\left\\| \\frac{1}{\\tau}(x^{1} - x^{2}) - K^{\\top}(p^{2} - p^{1}) \\right\\|_{2}$.\n    $x^1 - x^2 = (0.95-0.866, 0.6-0.599, 0.42-0.453, 0.23-0.282)^\\top = (0.084, 0.001, -0.033, -0.052)^\\top$.\n    $\\frac{1}{\\tau}(x^1 - x^2) = 4 \\cdot (0.084, 0.001, -0.033, -0.052)^\\top = (0.336, 0.004, -0.132, -0.208)^\\top$.\n    $p^2 - p^1 = (-0.1325, -0.0875, -0.0925, 0, 0, -0.0475, 0, 0)^\\top$.\n    $K^\\top(p^2 - p^1) = (0.22, 0.005, -0.085, -0.14)^\\top$.\n    Residual vector: $(0.336-0.22, 0.004-0.005, -0.132-(-0.085), -0.208-(-0.14))^\\top = (0.116, -0.001, -0.047, -0.068)^\\top$.\n    $r_{\\mathrm{p}}^{2} = \\sqrt{0.116^2 + (-0.001)^2 + (-0.047)^2 + (-0.068)^2} = \\sqrt{0.013456 + 0.000001 + 0.002209 + 0.004624} = \\sqrt{0.02029} \\approx 0.142443$.\n\n2.  **Dual Residual $r_{\\mathrm{d}}^{2}$**:\n    $r_{\\mathrm{d}}^{2} = \\left\\| \\frac{1}{\\sigma}(p^{1} - p^{2}) - K(x^{1} - x^{2}) \\right\\|_{2}$.\n    $\\frac{1}{\\sigma}(p^1 - p^2) = -4 \\cdot (p^2 - p^1) = (0.53, 0.35, 0.37, 0, 0, 0.19, 0, 0)^\\top$.\n    $K(x^1 - x^2) = K \\cdot (0.084, 0.001, -0.033, -0.052)^\\top = (-0.033-0.084, 0.001-0.084, -0.052-0.001, 0, 0, -0.052-(-0.033), 0, 0)^\\top = (-0.117, -0.083, -0.053, 0, 0, -0.019, 0, 0)^\\top$.\n    Residual vector: $(0.53-(-0.117), 0.35-(-0.083), 0.37-(-0.053), 0, 0, 0.19-(-0.019), 0, 0)^\\top = (0.647, 0.433, 0.423, 0, 0, 0.209, 0, 0)^\\top$.\n    $r_{\\mathrm{d}}^{2} = \\sqrt{0.647^2 + 0.433^2 + 0.423^2 + 0.209^2} = \\sqrt{0.418609 + 0.187489 + 0.178929 + 0.043681} = \\sqrt{0.828708} \\approx 0.910334$.\n\nRounding the results to four significant figures yields:\n$r_{\\mathrm{p}}^{2} \\approx 0.1424$.\n$r_{\\mathrm{d}}^{2} \\approx 0.9103$.\n\nThe requested pair is $(r_{\\mathrm{p}}^{2}, r_{\\mathrm{d}}^{2})$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.1424  0.9103 \\end{pmatrix}}\n$$"
        }
    ]
}