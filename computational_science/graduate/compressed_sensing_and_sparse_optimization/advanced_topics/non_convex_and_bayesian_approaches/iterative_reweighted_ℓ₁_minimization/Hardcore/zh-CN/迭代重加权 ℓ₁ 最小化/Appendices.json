{
    "hands_on_practices": [
        {
            "introduction": "为了掌握迭代重加权ℓ₁最小化算法，我们首先需要理解其核心的求解器是如何工作的。这个练习将带你亲手完成一次近端梯度下降迭代，它是许多加权ℓ₁问题求解的基础。通过计算梯度、执行前向步骤和应用加权软阈值算子，你将对算法的内部机制建立起具体而深入的理解。",
            "id": "3454424",
            "problem": "考虑在压缩感知和稀疏优化中用于迭代重加权 $\\ell_{1}$ 最小化的复合优化模型：\n$$F(x) \\equiv f(x) + g(x), \\quad f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}, \\quad g(x) = \\lambda \\sum_{i=1}^{n} w_{i} |x_{i}|,$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，$\\lambda  0$ 和 $w \\in \\mathbb{R}^{n}_{+}$ 是给定数据。邻近梯度法（Proximal Gradient Method, PGM）执行以下形式的迭代\n$$x^{(k+1)} = \\operatorname{prox}_{t g}\\Big(x^{(k)} - t \\nabla f\\big(x^{(k)}\\big)\\Big),$$\n其中 $t  0$ 是步长，$\\operatorname{prox}_{t g}$ 是 $t g$ 的邻近算子。\n\n仅使用梯度作为将方向映射到方向导数的线性泛函的定义、A 的线性性质以及邻近算子作为强凸可分函数的唯一最小化子的定义，计算在点\n$$x^{(0)} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix},$$\n处的一个显式邻近梯度步骤，数据如下\n$$A = \\begin{pmatrix} 1  -1  2 \\\\ 0  2  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad w = \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix}, \\quad \\lambda = \\frac{1}{2}, \\quad t = \\frac{1}{5}.$$\n\n你的任务是通过以下步骤执行这单次迭代：\n1. 从第一性原理计算梯度 $\\nabla f\\big(x^{(0)}\\big)$。\n2. 形成前向（梯度）步 $v = x^{(0)} - t \\nabla f\\big(x^{(0)}\\big)$。\n3. 应用由 $t g$ 的邻近算子导出的加权软阈值，该算子以阈值 $t \\lambda w_{i}$ 逐分量作用。\n\n将最终迭代结果 $x^{(1)}$ 表示为精确形式的单个行向量。不需要四舍五入。",
            "solution": "首先根据指定标准验证问题。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n- 优化模型：$F(x) \\equiv f(x) + g(x)$\n- 可微部分：$f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$\n- 非光滑部分：$g(x) = \\lambda \\sum_{i=1}^{n} w_{i} |x_{i}|$\n- 数据类型：$A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，$\\lambda  0$，$w \\in \\mathbb{R}^{n}_{+}$\n- 迭代公式：$x^{(k+1)} = \\operatorname{prox}_{t g}\\Big(x^{(k)} - t \\nabla f\\big(x^{(k)}\\big)\\Big)$\n- 步长：$t  0$\n- 初始点：$x^{(0)} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix}$\n- 矩阵 $A$：$A = \\begin{pmatrix} 1  -1  2 \\\\ 0  2  1 \\end{pmatrix}$\n- 向量 $y$：$y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$\n- 权重向量 $w$：$w = \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix}$\n- 正则化参数：$\\lambda = \\frac{1}{2}$\n- 步长：$t = \\frac{1}{5}$\n\n问题要求执行单个邻近梯度步骤，包括三个子任务：计算梯度、执行前向步骤和应用邻近算子。\n\n**第 2 步：使用提取的已知条件进行验证**\n- **科学基础：** 该问题建立在凸优化和稀疏信号恢复的明确数学框架中。复合模型、邻近梯度法以及具体函数（$f(x)$作为最小二乘损失，$g(x)$作为加权$\\ell_1$范数惩罚）在该领域都是经典内容。问题严格遵循既定的数学原理。\n- **适定性：** 该问题要求计算一次确定性的迭代。函数 $f(x)$ 是凸且连续可微的，函数 $g(x)$ 是凸的。缩放$\\ell_1$范数的邻近算子是唯一定义的，并且有众所周知的前定解（软阈值算子）。因此，存在唯一解。\n- **客观性：** 问题使用精确的数学定义和数值进行表述，没有任何主观或模糊的语言。\n- **完备性与一致性：** 执行一次迭代所需的所有数值数据（$A$、$y$、$x^{(0)}$、$w$、$\\lambda$、$t$）均已提供。矩阵和向量的维度是一致的：$A$ 是 $2 \\times 3$，$x^{(0)}$ 是 $3 \\times 1$，因此 $Ax^{(0)}$ 是 $2 \\times 1$。这与 $y$ 的维度（$2 \\times 1$）兼容。$w$ 的维度（$3 \\times 1$）也与 $x^{(0)}$ 的维度匹配。\n- **无其他缺陷：** 该问题是应用一种基本优化算法的标准、非平凡练习。它不是比喻性的、循环论证的，也不是基于错误前提的。\n\n**第 3 步：结论与行动**\n问题有效。将提供完整解答。\n\n### 解答\n\n任务是从 $x^{(0)}$ 开始，计算一个明确的邻近梯度步骤 $x^{(1)}$。该过程涉及三个顺序计算。\n\n**1. 从第一性原理计算梯度 $\\nabla f\\big(x^{(0)}\\big)$。**\n\n函数为 $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$。梯度 $\\nabla f(x)$ 是满足任意方向 $h \\in \\mathbb{R}^n$ 上方向导数 $D f(x)[h]$ 关系的唯一向量：\n$$D f(x)[h] = \\lim_{\\alpha \\to 0} \\frac{f(x+\\alpha h) - f(x)}{\\alpha} = \\langle \\nabla f(x), h \\rangle = (\\nabla f(x))^{T} h.$$\n我们展开 $f(x+\\alpha h)$：\n\\begin{align*}\nf(x+\\alpha h) = \\frac{1}{2} \\|A(x+\\alpha h) - y\\|_{2}^{2} \\\\\n= \\frac{1}{2} \\|(Ax - y) + \\alpha Ah\\|_{2}^{2} \\\\\n= \\frac{1}{2} \\langle (Ax - y) + \\alpha Ah, (Ax - y) + \\alpha Ah \\rangle \\\\\n= \\frac{1}{2} \\left( \\|Ax - y\\|_{2}^{2} + 2\\alpha \\langle Ax - y, Ah \\rangle + \\alpha^2 \\|Ah\\|_{2}^{2} \\right) \\\\\n= f(x) + \\alpha (Ax - y)^{T} (Ah) + \\frac{\\alpha^2}{2} \\|Ah\\|_{2}^{2}.\n\\end{align*}\n利用转置的性质，$(Ax - y)^{T} (Ah) = (A^{T}(Ax-y))^{T} h$。\n因此，方向导数为：\n$$D f(x)[h] = \\lim_{\\alpha \\to 0} \\frac{f(x+\\alpha h) - f(x)}{\\alpha} = \\lim_{\\alpha \\to 0} \\left( (A^{T}(Ax-y))^{T} h + \\frac{\\alpha}{2} \\|Ah\\|_{2}^{2} \\right) = (A^{T}(Ax-y))^{T} h.$$\n通过辨识，梯度为 $\\nabla f(x) = A^{T}(Ax-y)$。\n\n现在我们根据给定数据在 $x^{(0)}$ 处计算该值：\n首先，计算残差 $r = Ax^{(0)} - y$：\n$$Ax^{(0)} = \\begin{pmatrix} 1  -1  2 \\\\ 0  2  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (-1)(-1) + (2)(1) \\\\ (0)(0) + (2)(-1) + (1)(1) \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}.$$\n$$r = Ax^{(0)} - y = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix}.$$\n接下来，计算梯度 $\\nabla f(x^{(0)}) = A^{T}r$：\n$$A^{T} = \\begin{pmatrix} 1  0 \\\\ -1  2 \\\\ 2  1 \\end{pmatrix}.$$\n$$\\nabla f(x^{(0)}) = \\begin{pmatrix} 1  0 \\\\ -1  2 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (0)(-2) \\\\ (-1)(0) + (2)(-2) \\\\ (2)(0) + (1)(-2) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -4 \\\\ -2 \\end{pmatrix}.$$\n\n**2. 形成前向（梯度）步 $v = x^{(0)} - t \\nabla f\\big(x^{(0)}\\big)$。**\n\n利用 $t = \\frac{1}{5}$ 和已计算出的梯度，我们求得中间向量 $v$：\n$$v = x^{(0)} - t \\nabla f(x^{(0)}) = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 0 \\\\ -4 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ -\\frac{4}{5} \\\\ -\\frac{2}{5} \\end{pmatrix} = \\begin{pmatrix} 0 - 0 \\\\ -1 - (-\\frac{4}{5}) \\\\ 1 - (-\\frac{2}{5}) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{5} \\\\ \\frac{7}{5} \\end{pmatrix}.$$\n\n**3. 应用加权软阈值计算 $x^{(1)} = \\operatorname{prox}_{t g}(v)$。**\n\n邻近算子的定义为：\n$$x^{(1)} = \\operatorname{prox}_{t g}(v) = \\arg\\min_{z \\in \\mathbb{R}^n} \\left( t g(z) + \\frac{1}{2}\\|z - v\\|_{2}^{2} \\right).$$\n代入 $g(z) = \\lambda \\sum_{i=1}^{n} w_{i} |z_{i}|$，目标函数变为：\n$$\\arg\\min_{z \\in \\mathbb{R}^n} \\left( t \\lambda \\sum_{i=1}^{n} w_{i} |z_{i}| + \\frac{1}{2}\\sum_{i=1}^{n} (z_i - v_i)^{2} \\right).$$\n这个优化问题是可分的，意味着我们可以独立求解每个分量 $z_i$：\n$$x^{(1)}_{i} = \\arg\\min_{z_i \\in \\mathbb{R}} \\left( (t \\lambda w_{i}) |z_{i}| + \\frac{1}{2}(z_{i} - v_{i})^{2} \\right).$$\n解由软阈值算子 $S_{\\tau}(u) = \\operatorname{sign}(u) \\max(|u| - \\tau, 0)$ 给出，其中分量阈值为 $\\tau_i = t \\lambda w_{i}$。\n\n我们来计算阈值 $\\tau_i$：\n乘积 $t \\lambda = \\frac{1}{5} \\times \\frac{1}{2} = \\frac{1}{10}$。\n权重向量为 $w = (2, 1, 3)^T$。\n$$\\tau_1 = t \\lambda w_1 = \\frac{1}{10} \\times 2 = \\frac{2}{10} = \\frac{1}{5}.$$\n$$\\tau_2 = t \\lambda w_2 = \\frac{1}{10} \\times 1 = \\frac{1}{10}.$$\n$$\\tau_3 = t \\lambda w_3 = \\frac{1}{10} \\times 3 = \\frac{3}{10}.$$\n\n现在我们将软阈值算子应用于 $v = (0, -1/5, 7/5)^T$ 的每个分量：\n对于 $i=1$：$v_1 = 0$，$\\tau_1 = \\frac{1}{5}$。\n$x^{(1)}_{1} = S_{1/5}(0) = \\operatorname{sign}(0) \\max(|0| - \\frac{1}{5}, 0) = 0$。\n\n对于 $i=2$：$v_2 = -\\frac{1}{5}$，$\\tau_2 = \\frac{1}{10}$。\n$|v_2| = \\frac{1}{5} = \\frac{2}{10}$，大于 $\\tau_2 = \\frac{1}{10}$。\n$x^{(1)}_{2} = S_{1/10}(-\\frac{1}{5}) = \\operatorname{sign}(-\\frac{1}{5}) \\left( |-\\frac{1}{5}| - \\frac{1}{10} \\right) = (-1) \\left( \\frac{1}{5} - \\frac{1}{10} \\right) = (-1) \\left( \\frac{2}{10} - \\frac{1}{10} \\right) = -\\frac{1}{10}$。\n\n对于 $i=3$：$v_3 = \\frac{7}{5}$，$\\tau_3 = \\frac{3}{10}$。\n$|v_3| = \\frac{7}{5} = \\frac{14}{10}$，大于 $\\tau_3 = \\frac{3}{10}$。\n$x^{(1)}_{3} = S_{3/10}(\\frac{7}{5}) = \\operatorname{sign}(\\frac{7}{5}) \\left( |\\frac{7}{5}| - \\frac{3}{10} \\right) = (+1) \\left( \\frac{7}{5} - \\frac{3}{10} \\right) = \\frac{14}{10} - \\frac{3}{10} = \\frac{11}{10}$。\n\n组合各分量，下一个迭代结果 $x^{(1)}$ 为：\n$$x^{(1)} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{10} \\\\ \\frac{11}{10} \\end{pmatrix}.$$\n问题要求以单个行向量的形式给出答案。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} 0  -\\frac{1}{10}  \\frac{11}{10} \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "现实世界中的稀疏信号常常表现出结构化的特性，例如非零系数以组（块）的形式出现。这个练习将迭代重加权的思想从标量推广到分组稀疏的情境。你将首先从第一性原理出发，推导分组软阈值算子——这是块稀疏恢复的关键更新规则，然后通过一个数值算例来应用它，从而加深对主化-最小化（MM）算法及其在结构化稀疏中应用的理解。",
            "id": "3454450",
            "problem": "考虑一个块稀疏信号模型，其索引被固定地划分为不相交的组 $\\mathcal{G} = \\{g_{1}, g_{2}, \\dots, g_{M}\\}$，其中每个组 $g \\in \\mathcal{G}$ 都是 $\\{1,2,\\dots,N\\}$ 的一个子集。对于一个向量 $\\mathbf{x} \\in \\mathbb{R}^{N}$ 和一个子集 $g$，用 $\\mathbf{x}_{g}$ 表示 $\\mathbf{x}$ 中限制在索引 $g$ 内的子向量。$\\mathbf{x}$ 的组 $\\ell_{1,2}$ 范数定义为 $\\sum_{g \\in \\mathcal{G}} \\|\\mathbf{x}_{g}\\|_{2}$。为了促进块稀疏性，考虑将一个凹、非减函数 $\\psi:[0,\\infty)\\to\\mathbb{R}$（例如，对数代理函数）应用于组范数，从而得到目标函数 $\\sum_{g \\in \\mathcal{G}} \\psi(\\|\\mathbf{x}_{g}\\|_{2})$。\n\n从凸分析和主要化-最小化（MM）算法的标准原理出发，执行以下操作：\n\n1. 通过在当前迭代点对 $\\sum_{g \\in \\mathcal{G}} \\psi(\\|\\mathbf{x}_{g}\\|_{2})$ 进行上界化，将一个迭代重加权组 $\\ell_{1,2}$ 方案表述为一系列凸子问题。您的表述必须明确显示代理函数如何变为加权组 $\\ell_{1,2}$ 惩罚项，并根据当前迭代点和 $\\psi$ 确定组权重。\n\n2. 对于形如\n$$\n\\min_{\\mathbf{x}\\in\\mathbb{R}^{N}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{v}\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}\\|_{2},\n$$\n的单个凸子问题，使用最优性条件从基本原理出发，推导出将 $\\mathbf{v}$ 映射到最小化子的组加权软阈值算子的显式闭式表达式。您的推导必须从欧几里得范数的次微分的定义和跨组的可分离性开始，并逻辑地推导出最终的算子，而无需援引预先记忆的公式。\n\n3. 为一个包含两组 $g_{1}=\\{1,2,3\\}$ 和 $g_{2}=\\{4,5,6\\}$ 的六维信号，提供一次 MM 迭代的完整数值示例。使用数据保真模型\n$$\n\\min_{\\mathbf{x}\\in\\mathbb{R}^{6}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{b}\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}^{(k)}\\|\\mathbf{x}_{g}\\|_{2},\n$$\n其中第 $k$ 次迭代的权重是通过 MM 线性化从 $\\mathbf{x}^{(k)}$ 获得的，其中 $\\psi(s)=\\ln(s+\\epsilon)$ 且 $\\epsilon0$ 是一个固定的常数。设初始迭代点为 $\\mathbf{x}^{(0)}=\\mathbf{b}$，其中 $\\mathbf{b}=(3,4,0,0,2,0)$，参数 $\\lambda=1$，稳定化常数 $\\epsilon=\\tfrac{1}{2}$。计算权重 $w_{g}^{(0)}$，然后通过将推导出的组加权软阈值算子应用于 $\\mathbf{b}$ 来计算下一个迭代点 $\\mathbf{x}^{(1)}$。$\\mathbf{x}^{(1)}$ 的最终答案需以精确形式表示，无需四舍五入。\n\n您的最终答案必须是单个行向量 $\\mathbf{x}^{(1)}$。",
            "solution": "该问题经核实具有科学依据，是适定的、客观且完整的。我们接下来进行详细解答。\n\n该问题分为三个部分。我们将依次进行解答。\n\n**第 1 部分：迭代重加权方案的构建**\n\n目标是最小化形如 $\\sum_{g \\in \\mathcal{G}} \\psi(\\|\\mathbf{x}_{g}\\|_{2})$ 的目标函数，其中 $\\psi$ 是一个凹、非减函数。我们采用主要化-最小化（MM）算法。MM 算法的核心思想是用一系列更简单的问题来替代一个困难的优化问题。在每次迭代 $k$ 中，我们在当前迭代点 $\\mathbf{x}^{(k)}$ 处最小化一个代理函数 $Q(\\mathbf{x} | \\mathbf{x}^{(k)})$，该函数是原始目标函数 $\\Phi(\\mathbf{x}) = \\sum_{g \\in \\mathcal{G}} \\psi(\\|\\mathbf{x}_{g}\\|_{2})$ 的一个上界。该代理函数必须满足：\n1. 对所有 $\\mathbf{x}$，$Q(\\mathbf{x} | \\mathbf{x}^{(k)}) \\ge \\Phi(\\mathbf{x})$。\n2. $Q(\\mathbf{x}^{(k)} | \\mathbf{x}^{(k)}) = \\Phi(\\mathbf{x})$。\n然后通过 $\\mathbf{x}^{(k+1)} = \\arg\\min_{\\mathbf{x}} Q(\\mathbf{x} | \\mathbf{x}^{(k)})$ 找到下一个迭代点。\n\n由于 $\\psi: [0,\\infty)\\to\\mathbb{R}$ 是一个凹且可微的函数，我们可以利用它的一阶泰勒近似来构造一个上界函数。对于任何凹函数 $f$，我们有性质 $f(z) \\le f(z_0) + f'(z_0)(z - z_0)$，适用于其定义域中的所有 $z, z_0$。这个不等式在 $z=z_0$ 处提供了一个紧上界。\n\n我们将此性质应用于和式中的每一项 $\\psi(\\|\\mathbf{x}_{g}\\|_{2})$。令 $z_g = \\|\\mathbf{x}_g\\|_2$ 且 $z_g^{(k)} = \\|\\mathbf{x}_g^{(k)}\\|_2$。第 $g$ 项的上界化为：\n$$\n\\psi(\\|\\mathbf{x}_{g}\\|_{2}) \\le \\psi(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) + \\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) \\cdot (\\|\\mathbf{x}_{g}\\|_{2} - \\|\\mathbf{x}_{g}^{(k)}\\|_{2})\n$$\n对所有组 $g \\in \\mathcal{G}$ 求和，我们得到整个目标函数的代理函数：\n$$\nQ(\\mathbf{x} | \\mathbf{x}^{(k)}) = \\sum_{g \\in \\mathcal{G}} \\left[ \\psi(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) + \\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) \\cdot (\\|\\mathbf{x}_{g}\\|_{2} - \\|\\mathbf{x}_{g}^{(k)}\\|_{2}) \\right]\n$$\n为了找到下一个迭代点 $\\mathbf{x}^{(k+1)}$，我们最小化这个代理函数。在对 $\\mathbf{x}$ 进行最小化时，我们可以舍弃相对于 $\\mathbf{x}$ 为常数的项，即 $\\psi(\\|\\mathbf{x}_{g}^{(k)}\\|_{2})$ 和 $-\\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2})\\|\\mathbf{x}_{g}^{(k)}\\|_{2}$。第 $(k+1)$ 次迭代的最小化问题变为：\n$$\n\\mathbf{x}^{(k+1)} = \\arg\\min_{\\mathbf{x}} \\sum_{g \\in \\mathcal{G}} \\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) \\|\\mathbf{x}_{g}\\|_{2}\n$$\n这是一个加权组 $\\ell_{1,2}$ 最小化问题。在第 $k$ 次迭代中，每个组 $g$ 的权重由 $\\psi$ 的导数在组的当前迭代点 $\\mathbf{x}_g^{(k)}$ 的范数处的值给出：\n$$\nw_g^{(k)} = \\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2})\n$$\n当与像 $\\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2$ 这样的数据保真项结合时，完整的迭代重加权方案涉及求解一系列形如以下形式的凸子问题：\n$$\n\\mathbf{x}^{(k+1)} = \\arg\\min_{\\mathbf{x}} \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 + \\lambda \\sum_{g \\in \\mathcal{G}} w_g^{(k)} \\|\\mathbf{x}_g\\|_2\n$$\n\n**第 2 部分：组加权软阈值算子的推导**\n\n我们被要求解出以下凸子问题的闭式解：\n$$\n\\min_{\\mathbf{x}\\in\\mathbb{R}^{N}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{v}\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}\\|_{2}\n$$\n设目标函数为 $F(\\mathbf{x}) = f(\\mathbf{x}) + h(\\mathbf{x})$，其中 $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{v}\\|_{2}^{2}$ 是一个光滑、强凸函数，而 $h(\\mathbf{x}) = \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}\\|_{2}$ 是一个凸、非光滑的正则化项。一个向量 $\\mathbf{x}^{*}$ 是唯一的最小化子，当且仅当它满足凸分析中的一阶最优性条件：\n$$\n\\mathbf{0} \\in \\partial F(\\mathbf{x}^{*}) = \\nabla f(\\mathbf{x}^{*}) + \\partial h(\\mathbf{x}^{*})\n$$\n$f(\\mathbf{x})$ 的梯度是 $\\nabla f(\\mathbf{x}) = \\mathbf{x} - \\mathbf{v}$。条件变为：\n$$\n\\mathbf{0} \\in (\\mathbf{x}^{*} - \\mathbf{v}) + \\partial \\left(\\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}^{*}\\|_{2}\\right) \\iff \\mathbf{v} - \\mathbf{x}^{*} \\in \\partial \\left(\\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}^{*}\\|_{2}\\right)\n$$\n由于组 $\\{g\\}$ 构成了索引的一个划分，正则化项 $h(\\mathbf{x})$ 相对于子向量 $\\mathbf{x}_g$ 是可分离的。也就是说，$h(\\mathbf{x}) = \\sum_{g \\in \\mathcal{G}} h_g(\\mathbf{x}_g)$，其中 $h_g(\\mathbf{x}_g) = \\lambda w_g \\|\\mathbf{x}_g\\|_2$。次微分 $\\partial h(\\mathbf{x})$ 是各次微分 $\\partial h_g(\\mathbf{x}_g)$ 的笛卡尔积。这种可分离性意味着优化问题可以解耦为 $M$ 个独立的问题，每个组 $g \\in \\mathcal{G}$ 对应一个：\n$$\n\\min_{\\mathbf{x}_g} \\frac{1}{2}\\|\\mathbf{x}_g - \\mathbf{v}_g\\|_2^2 + \\lambda w_g \\|\\mathbf{x}_g\\|_2\n$$\n让我们求解单个组 $g$。令 $\\mathbf{y} = \\mathbf{x}_g$，$\\mathbf{u} = \\mathbf{v}_g$，以及 $\\mu = \\lambda w_g \\ge 0$。问题是 $\\min_{\\mathbf{y}} \\frac{1}{2} \\|\\mathbf{y}-\\mathbf{u}\\|_2^2 + \\mu\\|\\mathbf{y}\\|_2$。最小化子 $\\mathbf{y}^{*}$ 的最优性条件是：\n$$\n\\mathbf{u} - \\mathbf{y}^{*} \\in \\partial(\\mu \\|\\mathbf{y}^{*}\\|_2) = \\mu \\partial\\|\\mathbf{y}^{*}\\|_2\n$$\n欧几里得范数的次微分是：\n$$\n\\partial\\|\\mathbf{y}\\|_2 = \n\\begin{cases} \n\\{\\frac{\\mathbf{y}}{\\|\\mathbf{y}\\|_2}\\}   \\text{if } \\mathbf{y} \\neq \\mathbf{0} \\\\\n\\{\\mathbf{z} \\mid \\|\\mathbf{z}\\|_2 \\le 1\\}  \\text{if } \\mathbf{y} = \\mathbf{0} \n\\end{cases}\n$$\n我们分析解 $\\mathbf{y}^{*}$ 的两种情况：\n情况 1：$\\mathbf{y}^{*} \\neq \\mathbf{0}$。最优性条件变为 $\\mathbf{u} - \\mathbf{y}^{*} = \\mu \\frac{\\mathbf{y}^{*}}{\\|\\mathbf{y}^{*}\\|_2}$。对 $\\mathbf{u}$ 重新整理，我们得到 $\\mathbf{u} = \\mathbf{y}^{*} + \\mu \\frac{\\mathbf{y}^{*}}{\\|\\mathbf{y}^{*}\\|_2} = \\mathbf{y}^{*} \\left(1 + \\frac{\\mu}{\\|\\mathbf{y}^{*}\\|_2}\\right)$。这意味着 $\\mathbf{u}$ 和 $\\mathbf{y}^{*}$ 是共线的，因此对于某个标量 $c  0$，有 $\\mathbf{y}^{*} = c \\mathbf{u}$。取范数，我们有 $\\|\\mathbf{y}^{*}\\|_2 = c\\|\\mathbf{u}\\|_2$。代回到 $\\mathbf{u}$ 的主要表达式中：\n$\\|\\mathbf{u}\\|_2 = \\|\\mathbf{y}^{*}\\|_2 \\left(1 + \\frac{\\mu}{\\|\\mathbf{y}^{*}\\|_2}\\right) = \\|\\mathbf{y}^{*}\\|_2 + \\mu$。\n这得到 $\\|\\mathbf{y}^{*}\\|_2 = \\|\\mathbf{u}\\|_2 - \\mu$。由于在本例中 $\\|\\mathbf{y}^{*}\\|_2  0$，我们必须有 $\\|\\mathbf{u}\\|_2  \\mu$。那么解就是：\n$$\n\\mathbf{y}^{*} = \\frac{\\|\\mathbf{y}^{*}\\|_2}{\\|\\mathbf{u}\\|_2}\\mathbf{u} = \\frac{\\|\\mathbf{u}\\|_2 - \\mu}{\\|\\mathbf{u}\\|_2}\\mathbf{u} = \\left(1 - \\frac{\\mu}{\\|\\mathbf{u}\\|_2}\\right)\\mathbf{u}\n$$\n情况 2：$\\mathbf{y}^{*} = \\mathbf{0}$。最优性条件变为 $\\mathbf{u} - \\mathbf{0} \\in \\mu \\partial\\|\\mathbf{0}\\|_2$，即 $\\mathbf{u} \\in \\{\\mathbf{z} \\mid \\|\\mathbf{z}\\|_2 \\le \\mu\\}$。这等价于条件 $\\|\\mathbf{u}\\|_2 \\le \\mu$。\n\n结合两种情况，我们可以使用正部函数 $(a)_+ = \\max(a, 0)$ 紧凑地写出 $\\mathbf{y}^{*}$ 的解：\n$$\n\\mathbf{y}^{*} = \\left(1 - \\frac{\\mu}{\\|\\mathbf{u}\\|_2}\\right)_+ \\mathbf{u}\n$$\n这个表达式就是组软阈值算子。如果 $\\|\\mathbf{u}\\|_2=0$，分数是未定义的，但条件 $\\|\\mathbf{u}\\|_2 \\le \\mu$ 成立，所以 $\\mathbf{y}^*=\\mathbf{0}$，这是正确的极限。\n回到组 $g$ 的原始记法，最小化子 $\\mathbf{x}_g^*$ 是：\n$$\n\\mathbf{x}_g^{*} = \\left(1 - \\frac{\\lambda w_g}{\\|\\mathbf{v}_g\\|_2}\\right)_+ \\mathbf{v}_g\n$$\n最终解 $\\mathbf{x}^*$ 是通过拼接所有组的解 $\\mathbf{x}_g^*$ 得到的。\n\n**第 3 部分：数值示例**\n\n我们执行一次 MM 迭代。问题是通过求解\n$$\n\\min_{\\mathbf{x}\\in\\mathbb{R}^{6}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{b}\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}^{(0)}\\|\\mathbf{x}_{g}\\|_{2}\n$$\n来找到 $\\mathbf{x}^{(1)}$。\n已知：\n- 信号维度 $N=6$。\n- 组：$g_1 = \\{1, 2, 3\\}$，$g_2 = \\{4, 5, 6\\}$。\n- 初始迭代点 $\\mathbf{x}^{(0)} = \\mathbf{b} = (3, 4, 0, 0, 2, 0)^T$。\n- 凹函数：$\\psi(s) = \\ln(s+\\epsilon)$，其中 $\\epsilon=\\frac{1}{2}$。\n- 参数：$\\lambda = 1$。\n\n首先，我们计算权重 $w_g^{(0)} = \\psi'(\\|\\mathbf{x}_g^{(0)}\\|_2)$。\n$\\psi(s)$ 的导数是 $\\psi'(s) = \\frac{1}{s+\\epsilon}$。\n初始迭代点的子向量是：\n$\\mathbf{x}_{g_1}^{(0)} = (3, 4, 0)^T$\n$\\mathbf{x}_{g_2}^{(0)} = (0, 2, 0)^T$\n接下来，我们计算它们的欧几里得范数：\n$\\|\\mathbf{x}_{g_1}^{(0)}\\|_2 = \\sqrt{3^2 + 4^2 + 0^2} = \\sqrt{9+16} = \\sqrt{25} = 5$。\n$\\|\\mathbf{x}_{g_2}^{(0)}\\|_2 = \\sqrt{0^2 + 2^2 + 0^2} = \\sqrt{4} = 2$。\n现在我们可以计算第 $k=0$ 次迭代的权重：\n$w_{g_1}^{(0)} = \\psi'(\\|\\mathbf{x}_{g_1}^{(0)}\\|_2) = \\frac{1}{5 + \\epsilon} = \\frac{1}{5 + \\frac{1}{2}} = \\frac{1}{\\frac{11}{2}} = \\frac{2}{11}$。\n$w_{g_2}^{(0)} = \\psi'(\\|\\mathbf{x}_{g_2}^{(0)}\\|_2) = \\frac{1}{2 + \\epsilon} = \\frac{1}{2 + \\frac{1}{2}} = \\frac{1}{\\frac{5}{2}} = \\frac{2}{5}$。\n\n其次，我们通过应用第 2 部分中推导出的组加权软阈值算子来计算下一个迭代点 $\\mathbf{x}^{(1)}$，其中 $\\mathbf{v}=\\mathbf{b}$。每个组 $\\mathbf{x}_g^{(1)}$ 的解是：\n$$\n\\mathbf{x}_g^{(1)} = \\left(1 - \\frac{\\lambda w_g^{(0)}}{\\|\\mathbf{b}_g\\|_2}\\right)_+ \\mathbf{b}_g\n$$\n$\\mathbf{b}$ 的子向量是 $\\mathbf{b}_{g_1} = (3, 4, 0)^T$ 和 $\\mathbf{b}_{g_2} = (0, 2, 0)^T$。它们的范数是 $\\|\\mathbf{b}_{g_1}\\|_2 = 5$ 和 $\\|\\mathbf{b}_{g_2}\\|_2 = 2$。\n\n对于组 $g_1$：\n阈值是 $\\mu_1 = \\lambda w_{g_1}^{(0)} = 1 \\cdot \\frac{2}{11} = \\frac{2}{11}$。\n由于 $\\|\\mathbf{b}_{g_1}\\|_2 = 5  \\frac{2}{11}$，该组不被置为零。缩放因子是：\n$$\n\\left(1 - \\frac{\\mu_1}{\\|\\mathbf{b}_{g_1}\\|_2}\\right) = 1 - \\frac{\\frac{2}{11}}{5} = 1 - \\frac{2}{55} = \\frac{53}{55}\n$$\n所以，更新后的子向量是：\n$$\n\\mathbf{x}_{g_1}^{(1)} = \\frac{53}{55} \\mathbf{b}_{g_1} = \\frac{53}{55} \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{159}{55} \\\\ \\frac{212}{55} \\\\ 0 \\end{pmatrix}\n$$\n\n对于组 $g_2$：\n阈值是 $\\mu_2 = \\lambda w_{g_2}^{(0)} = 1 \\cdot \\frac{2}{5} = \\frac{2}{5}$。\n由于 $\\|\\mathbf{b}_{g_2}\\|_2 = 2  \\frac{2}{5}$，该组也不被置为零。缩放因子是：\n$$\n\\left(1 - \\frac{\\mu_2}{\\|\\mathbf{b}_{g_2}\\|_2}\\right) = 1 - \\frac{\\frac{2}{5}}{2} = 1 - \\frac{1}{5} = \\frac{4}{5}\n$$\n所以，更新后的子向量是：\n$$\n\\mathbf{x}_{g_2}^{(1)} = \\frac{4}{5} \\mathbf{b}_{g_2} = \\frac{4}{5} \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{8}{5} \\\\ 0 \\end{pmatrix}\n$$\n\n最后，我们通过拼接 $\\mathbf{x}_{g_1}^{(1)}$ 和 $\\mathbf{x}_{g_2}^{(1)}$ 来组装完整的向量 $\\mathbf{x}^{(1)}$：\n$$\n\\mathbf{x}^{(1)} = \\left( \\frac{159}{55}, \\frac{212}{55}, 0, 0, \\frac{8}{5}, 0 \\right)^T\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{159}{55}  \\frac{212}{55}  0  0  \\frac{8}{5}  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "选择正确的稀疏模型至关重要，但如果模型与信号的真实结构不匹配会发生什么？这个高级练习探讨了模型失配的后果，特别是当对一个块稀疏信号使用标准的标量重加权方案时。通过推导和比较精确恢复的充分条件，你将能够量化地分析采用结构感知（块信息）的加权策略所带来的性能增益，从而从理论上理解选择合适正则化项的重要性。",
            "id": "3454474",
            "problem": "考虑在压缩感知和稀疏优化的背景下，迭代重加权 $\\ell_{1}$ 最小化 (IRL1) 的单次迭代。设 $A \\in \\mathbb{R}^{m \\times n}$ 是一个传感矩阵，其列是单位范数的，且其互相关性受 $\\mu$ 限制，其中互相关性定义为 $\\mu \\triangleq \\max_{i \\neq j} |a_{i}^{\\top} a_{j}|$，$a_{i}$ 表示 $A$ 的第 $i$ 列。将坐标 $\\{1,\\dots,n\\}$ 划分为 $G$ 个大小为 $d$ 的不相交且大小相等的块，并假设真实信号 $x^{\\star} \\in \\mathbb{R}^{n}$ 是块稀疏的：恰好有 $K$ 个块是活跃的，并且在每个活跃块内，每个条目都非零且具有相等的大小 $a0$。测量是无噪声的，$y = A x^{\\star}$，且 $m \\geq K d$，因此对于支撑集 $S \\subset \\{1,\\dots,n\\}$（其中 $|S| = K d$），$A_{S}$ 具有满列秩。\n\n假设以下最坏情况但相干且自洽的几何条件成立：$A_{S}^{\\top} A_{S} = I_{Kd}$，并且对于任何 $i \\in S^{c}$ 和 $j \\in S$，都有 $|a_{i}^{\\top} a_{j}| = \\mu$。考虑一次性加权 $\\ell_{1}$ 恢复步骤\n$\\min_{x \\in \\mathbb{R}^{n}} \\|W x\\|_{1} \\ \\text{subject to} \\ A x = y$，\n其中 $W = \\mathrm{diag}(w_{1},\\dots,w_{n}) \\succ 0$ 是根据前一次迭代 $x^{\\mathrm{prev}}$ 计算出的权重。将 $x^{\\mathrm{prev}}$ 建模如下：对于 $i \\in S$，$x^{\\mathrm{prev}}_{i} = a$；对于 $i \\in S^{c}$，$x^{\\mathrm{prev}}_{i} = 0$。为避免退化，权重被截断：对于所有 $i$，$w_{i} = \\min\\{w_{\\max}, 1/(u_{i}+\\varepsilon)\\}$，其中 $\\varepsilon  0$ 且 $u_{i}$ 是一个非负代理。\n\n考虑两种权重设计：\n1) 标量 IRL1 (模型失配)：$u_{i} = |x^{\\mathrm{prev}}_{i}|$，因此对于 $i \\in S$，$w^{(s)}_{\\mathrm{in}} = 1/(a+\\varepsilon)$，对于 $i \\in S^{c}$，$w^{(s)}_{0} = \\min\\{w_{\\max}, 1/\\varepsilon\\}$。假设对所有 $i \\in S^{c}$ 使用相同的权重，并记为 $w_{0} \\triangleq w^{(s)}_{0}$。\n2) 块信息重加权：对于块 $g$ 中的 $i$，$u_{i} = \\|x^{\\mathrm{prev}}_{g}\\|_{2}$。那么对于 $i \\in S$，$w^{(b)}_{\\mathrm{in}} = 1/(\\sqrt{d}\\, a + \\varepsilon)$，对于 $i \\in S^{c}$，同样的截断产生 $w^{(b)}_{0} = \\min\\{w_{\\max}, 1/\\varepsilon\\}$。假设对所有 $i \\in S^{c}$ 使用相同的 $w_{0}$。\n\n从次梯度和加权 $\\ell_{1}$ 最小化的 Karush–Kuhn–Tucker (KKT) 最优性条件的定义出发，结合互相关性框架，推导一个关于 $A$、$W$ 和支撑集 $S$ 的加权 $\\ell_{1}$ 规划精确支撑集恢复的充分条件。然后，在上述结构简化的前提下，将此条件特化，以获得分别保证标量权重和块信息权重精确支撑集恢复的最小振幅 $a^{(s)}_{\\min}$ 和 $a^{(b)}_{\\min}$，并用 $\\mu$、$K$、$d$、$\\varepsilon$ 和 $w_{0}$ 表示。假设参数满足 $\\mu K d / w_{0}  \\varepsilon$，从而使得两个阈值都是正的且非平凡的。\n\n最后，通过推导闭式比率 $R \\triangleq a^{(s)}_{\\min} / a^{(b)}_{\\min}$ 来量化模型失配的代价。请以 $R$ 的单个简化解析表达式的形式提供最终答案。不包括单位。不要四舍五入。",
            "solution": "问题要求计算在两种不同迭代重加权 $\\ell_{1}$ (IRL1) 方案的单次迭代中，实现精确支撑集恢复所需的最小信号振幅之比。我们首先为加权 $\\ell_{1}$ 最小化问题建立精确支撑集恢复的一般条件。\n\n优化问题是\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\|W x\\|_{1} \\ \\text{subject to} \\ A x = y $$\n其中 $W = \\mathrm{diag}(w_{1},\\dots,w_{n})$ 是一个正权重对角矩阵，$A \\in \\mathbb{R}^{m \\times n}$ 是传感矩阵，而 $y = A x^{\\star}$ 是真实信号 $x^{\\star}$ 的无噪声测量值。\n\n真实信号 $x^{\\star}$ 是一个可行点。要使其成为唯一解，它必须满足 Karush-Kuhn-Tucker (KKT) 条件。该问题的拉格朗日函数是 $L(x, \\nu) = \\|W x\\|_{1} + \\nu^{\\top}(A x - y)$。KKT 条件指出，必须存在一个对偶向量 $\\nu \\in \\mathbb{R}^{m}$，使得拉格朗日函数在 $x=x^{\\star}$ 处关于 $x$ 的次梯度包含零向量。\n$$ 0 \\in \\partial \\left( \\|W x\\|_{1} \\right)|_{x=x^{\\star}} + A^{\\top}\\nu $$\n这等价于 $-A^{\\top}\\nu \\in \\partial \\left( \\|W x\\|_{1} \\right)|_{x=x^{\\star}}$。加权 $\\ell_1$ 范数的次梯度由 $(\\partial \\|Wx\\|_1)_i = w_i \\partial |x_i|$ 给出。设 $S$ 是 $x^{\\star}$ 的支撑集，即 $S = \\{i \\mid x^{\\star}_i \\neq 0\\}$。KKT 条件可以分量形式写成：\n1. 对于 $i \\in S$，$x^{\\star}_i \\neq 0$，所以 $\\partial |x_i||_{x=x^{\\star}_i} = \\{\\mathrm{sgn}(x^{\\star}_i)\\}$。条件是 $(A^{\\top}\\nu)_i = -w_i \\mathrm{sgn}(x^{\\star}_i)$。\n2. 对于 $i \\in S^c$，$x^{\\star}_i = 0$，所以 $\\partial |x_i||_{x=x^{\\star}_i} = [-1, 1]$。条件是 $|(A^{\\top}\\nu)_i| \\leq w_i$。\n\n为使支撑集为 $S$ 的解是唯一的，对非支撑集 $S^c$ 的条件必须是严格的：对于所有 $i \\in S^c$，都有 $|(A^{\\top}\\nu)_i|  w_i$。\n\n设 $A_S$ 表示由 $S$ 索引的 $A$ 的列组成的矩阵。第一个条件可以写成矩阵形式 $A_S^{\\top}\\nu = -W_S \\mathrm{sgn}(x^{\\star}_S)$，其中 $W_S$ 是 $S$ 中索引的权重对角矩阵。由于问题陈述 $A_S$ 具有满列秩，因此 $A_S^{\\top}A_S$ 是可逆的。我们可以找到一个满足此条件的对偶向量 $\\nu$。$\\nu$ 的一个特解由 $\\nu = -A_S(A_S^{\\top}A_S)^{-1}W_S \\mathrm{sgn}(x^{\\star}_S)$ 给出。\n\n将此代入 $i \\in S^c$ 的严格不等式中：\n$$ |a_i^{\\top} \\left( -A_S(A_S^{\\top}A_S)^{-1}W_S \\mathrm{sgn}(x^{\\star}_S) \\right) |  w_i \\quad \\forall i \\in S^c $$\n这就是精确支撑集恢复的一般充分条件，通常称为对偶凭证条件。\n\n现在，我们应用问题中给出的结构简化。给定 $A_S^{\\top}A_S = I_{Kd}$，其中 $|S| = Kd$。该条件简化为：\n$$ |a_i^{\\top} A_S W_S \\mathrm{sgn}(x^{\\star}_S)|  w_i \\quad \\forall i \\in S^c $$\n绝对值内的项是一个标量积：$a_i^{\\top} \\sum_{j \\in S} a_j (W_S \\mathrm{sgn}(x^{\\star}_S))_j = \\sum_{j \\in S} (a_i^{\\top} a_j) w_j \\mathrm{sgn}(x^{\\star}_j)$。\n所以，对于每个 $i \\in S^c$，我们需要：\n$$ \\left| \\sum_{j \\in S} (a_i^{\\top} a_j) w_j \\mathrm{sgn}(x^{\\star}_j) \\right|  w_i $$\n为了获得一个独立于 $x^\\star_j$ 符号和特定内积的充分条件，我们使用三角不等式和最坏情况相干性假设。对于任何 $i \\in S^c$ 和 $j \\in S$，给定 $|a_i^{\\top} a_j| = \\mu$。\n$$ \\left| \\sum_{j \\in S} (a_i^{\\top} a_j) w_j \\mathrm{sgn}(x^{\\star}_j) \\right| \\leq \\sum_{j \\in S} |a_i^{\\top} a_j| w_j |\\mathrm{sgn}(x^{\\star}_j)| = \\sum_{j \\in S} \\mu w_j = \\mu \\sum_{j \\in S} w_j $$\n因此，恢复的一个充分条件是对所有 $i \\in S^c$，都有 $\\mu \\sum_{j \\in S} w_j  w_i$。\n\n接下来，我们将此条件针对两种加权方案进行特化。\n\n情况 1：标量 IRL1 (模型失配)\n权重由 $w_j = w^{(s)}_{\\mathrm{in}} = 1/(a+\\varepsilon)$（对于 $j \\in S$）和 $w_i = w_0$（对于 $i \\in S^c$）给出。支撑集大小为 $|S|=Kd$。\n支撑集上的权重之和为 $\\sum_{j \\in S} w_j = Kd \\cdot w^{(s)}_{\\mathrm{in}} = \\frac{Kd}{a+\\varepsilon}$。\n恢复条件变为：\n$$ \\mu \\frac{Kd}{a+\\varepsilon}  w_0 $$\n我们求解信号振幅 $a$：\n$$ \\frac{\\mu Kd}{w_0}  a + \\varepsilon \\implies a  \\frac{\\mu Kd}{w_0} - \\varepsilon $$\n在此条件下保证恢复的最小振幅是阈值：\n$$ a^{(s)}_{\\min} = \\frac{\\mu Kd}{w_0} - \\varepsilon $$\n\n情况 2：块信息重加权\n权重为 $w_j = w^{(b)}_{\\mathrm{in}} = 1/(\\sqrt{d} a + \\varepsilon)$（对于 $j \\in S$）和 $w_i = w_0$（对于 $i \\in S^c$）。\n支撑集上的权重之和为 $\\sum_{j \\in S} w_j = Kd \\cdot w^{(b)}_{\\mathrm{in}} = \\frac{Kd}{\\sqrt{d} a + \\varepsilon}$。\n恢复条件变为：\n$$ \\mu \\frac{Kd}{\\sqrt{d} a + \\varepsilon}  w_0 $$\n求解 $a$：\n$$ \\frac{\\mu Kd}{w_0}  \\sqrt{d} a + \\varepsilon \\implies \\sqrt{d} a  \\frac{\\mu Kd}{w_0} - \\varepsilon \\implies a  \\frac{1}{\\sqrt{d}}\\left(\\frac{\\mu Kd}{w_0} - \\varepsilon\\right) $$\n最小振幅为：\n$$ a^{(b)}_{\\min} = \\frac{1}{\\sqrt{d}}\\left(\\frac{\\mu Kd}{w_0} - \\varepsilon\\right) $$\n问题陈述 $\\mu K d / w_{0}  \\varepsilon$，这确保了 $a^{(s)}_{\\min}$ 和 $a^{(b)}_{\\min}$ 均为正。\n\n最后，我们计算模型失配惩罚比 $R = a^{(s)}_{\\min} / a^{(b)}_{\\min}$。\n$$ R = \\frac{\\frac{\\mu Kd}{w_0} - \\varepsilon}{\\frac{1}{\\sqrt{d}}\\left(\\frac{\\mu Kd}{w_0} - \\varepsilon\\right)} $$\n项 $\\left(\\frac{\\mu Kd}{w_0} - \\varepsilon\\right)$ 是分子和分母中的一个正常数公因子，因此可以消去。\n$$ R = \\frac{1}{1/\\sqrt{d}} = \\sqrt{d} $$\n这个结果量化了使用块信息加权策略的好处。与标准标量重加权方法相比，保证恢复所需的信号幅度减少了 $\\sqrt{d}$ 倍，这表明当块结构已知并被利用时，性能有显著提升。",
            "answer": "$$\n\\boxed{\\sqrt{d}}\n$$"
        }
    ]
}