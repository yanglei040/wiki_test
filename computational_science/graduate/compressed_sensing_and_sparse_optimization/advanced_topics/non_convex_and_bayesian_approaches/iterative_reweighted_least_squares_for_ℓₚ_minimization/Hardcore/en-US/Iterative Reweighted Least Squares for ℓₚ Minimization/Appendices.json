{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the mechanics of the Iterative Reweighted Least Squares (IRLS) algorithm, it is essential to work through a concrete iteration by hand. This exercise grounds the abstract Majorization-Minimization framework by having you derive the specific IRLS update for a smoothed $\\ell_1$ penalty. By performing a single numerical iteration, you will gain direct experience with how the weights are formed and how they influence the subsequent solution update .",
            "id": "3454747",
            "problem": "Consider the penalized least-squares formulation of sparse recovery that approximates $\\ell_{1}$ minimization by smoothing the absolute value with a differentiable surrogate. Let the objective be\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\|A x - y\\|_{2}^{2} \\;+\\; \\lambda \\sum_{i=1}^{n} \\phi_{\\varepsilon}(x_{i}),\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, $\\lambda0$, and $\\phi_{\\varepsilon}(t)$ is a smooth, strictly convex function that approximates $|t|$ as $\\varepsilon \\to 0^{+}$, specifically $\\phi_{\\varepsilon}(t) = \\sqrt{t^{2} + \\varepsilon^{2}}$. The Iterative Reweighted Least Squares (IRLS) method constructs at iteration $k$ a quadratic upper-bound (majorizer) of the penalty term around $x^{(k)}$ and updates $x^{(k+1)}$ by minimizing that quadratic surrogate plus the data fidelity term. Starting from the above fundamental definitions and the inequality properties of concave square-root functions, derive the weighted normal equations for $x^{(k+1)}$ in terms of a diagonal weight matrix $W^{(k)}$ whose diagonal entries depend on $x^{(k)}$ and $\\varepsilon$, and show that the weights correspond to the curvature of the surrogate at $x^{(k)}$.\n\nThen, with the specific data\n$$\nA \\;=\\; \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix}, \\qquad y \\;=\\; \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\qquad p \\;=\\; 1, \\qquad \\lambda \\;=\\; 0.1, \\qquad \\varepsilon \\;=\\; 10^{-3},\n$$\nand an initialization $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$, compute one IRLS iteration as follows:\n- Evaluate $w^{(0)}$ to form $W^{(0)}$, the diagonal of the weight matrix at iteration $k=0$.\n- Solve the linear system $(A^{\\top} A + \\lambda W^{(0)})\\, x^{(1)} = A^{\\top} y$ for $x^{(1)}$.\n- Evaluate $w^{(1)}$ from $x^{(1)}$.\n\nExpress your final answer as a single row matrix containing, in order, the three entries of $x^{(1)}$ followed by the three entries of $w^{(1)}$. Provide exact values; no rounding is required. Do not include any units in your final answer.",
            "solution": "The problem is well-defined and requires the derivation of the Iterative Reweighted Least Squares (IRLS) update for a specific smoothed $\\ell_1$ penalty, a discussion of the interpretation of the weights, and a single-iteration numerical computation.\n\n### Step 1: Extract Givens\nThe givens are as follows:\n- The objective function: $J(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\phi_{\\varepsilon}(x_{i})$.\n- Matrix and vector dimensions: $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, $x \\in \\mathbb{R}^{n}$.\n- Regularization parameter: $\\lambda  0$.\n- Smoothing function: $\\phi_{\\varepsilon}(t) = \\sqrt{t^{2} + \\varepsilon^{2}}$, which approximates $|t|$ for small $\\varepsilon  0$.\n- Numerical data:\n  - $A = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n  - $p = 1$ (context for the choice of penalty)\n  - $\\lambda = 0.1$\n  - $\\varepsilon = 10^{-3}$\n- Initialization: $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n- Task: Derive the weighted normal equations for $x^{(k+1)}$, show the connection between weights and curvature, and compute one IRLS iteration to find $x^{(1)}$ and the subsequent weights $w^{(1)}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is validated against the specified criteria:\n- **Scientifically Grounded**: The problem is set within the established mathematical framework of convex optimization, specifically Majorization-Minimization and IRLS for sparse signal recovery. The choice of $\\phi_{\\varepsilon}(t)$ as a smooth, strictly convex surrogate for the absolute value function is a standard technique.\n- **Well-Posed**: The objective function $J(x)$ is strictly convex because it is the sum of a convex function (the least-squares term) and a sum of strictly convex functions ($\\phi_{\\varepsilon}(t)$ is strictly convex for $\\varepsilon  0$). Therefore, a unique minimizer exists. The IRLS algorithm described is a standard procedure for solving such problems.\n- **Objective**: The problem is stated using precise mathematical language and definitions.\n- **Completeness and Consistency**: All necessary data and definitions for both the derivation and the numerical calculation are provided. There are no contradictions.\n- **Other flaws**: The problem is not trivial, metaphorical, or pseudo-profound. It addresses a core concept in computational optimization for sparse problems.\n\n### Step 3: Verdict and Action\nThe problem is VALID. I will proceed with the solution.\n\n### Derivation of the IRLS Update\nThe objective function to minimize is:\n$$\nJ(x) = \\frac{1}{2}\\|Ax - y\\|_2^2 + \\lambda \\sum_{i=1}^n \\phi_\\varepsilon(x_i)\n$$\nwhere $\\phi_\\varepsilon(t) = \\sqrt{t^2 + \\varepsilon^2}$. The IRLS method is an instance of a Majorization-Minimization (MM) algorithm. We need to find a quadratic majorizer for the non-quadratic penalty term, $\\psi(x) = \\sum_{i=1}^n \\phi_\\varepsilon(x_i)$.\n\nLet's focus on a single component $\\phi_\\varepsilon(t)$. Let $g(u) = \\sqrt{u}$ for $u \\ge 0$. The function $g(u)$ is concave. For any concave function, the tangent line at any point lies above the function graph. This gives the inequality:\n$$\ng(u) \\le g(u_k) + g'(u_k)(u - u_k)\n$$\nwhere $u_k$ is the point of tangency. The derivative is $g'(u) = \\frac{1}{2\\sqrt{u}}$.\n\nWe apply this inequality to $\\phi_\\varepsilon(x_i) = \\sqrt{x_i^2 + \\varepsilon^2}$. We set $u = x_i^2 + \\varepsilon^2$. At the current iterate $x^{(k)}$, we define $u_k = (x_i^{(k)})^2 + \\varepsilon^2$.\nSubstituting into the inequality, we get a majorizer for $\\phi_\\varepsilon(x_i)$:\n\\begin{align*}\n\\phi_\\varepsilon(x_i) = \\sqrt{x_i^2 + \\varepsilon^2} \\le \\sqrt{(x_i^{(k)})^2 + \\varepsilon^2} + \\frac{1}{2\\sqrt{(x_i^{(k)})^2 + \\varepsilon^2}} \\left( (x_i^2 + \\varepsilon^2) - ((x_i^{(k)})^2 + \\varepsilon^2) \\right) \\\\\n\\le \\phi_\\varepsilon(x_i^{(k)}) + \\frac{1}{2\\phi_\\varepsilon(x_i^{(k)})} (x_i^2 - (x_i^{(k)})^2)\n\\end{align*}\nThis inequality holds for all $x_i$, with equality at $x_i = x_i^{(k)}$. The right-hand side is a quadratic function of $x_i$ and serves as a majorizer.\n\nSumming over all components $i=1, \\dots, n$ and multiplying by $\\lambda$, we obtain a majorizer for the entire penalty term:\n$$\n\\lambda \\sum_{i=1}^n \\phi_\\varepsilon(x_i) \\le \\lambda \\sum_{i=1}^n \\left( \\phi_\\varepsilon(x_i^{(k)}) + \\frac{1}{2\\phi_\\varepsilon(x_i^{(k)})} (x_i^2 - (x_i^{(k)})^2) \\right)\n$$\nThe full objective function $J(x)$ is thus majorized by a quadratic surrogate function $Q(x, x^{(k)})$:\n$$\nJ(x) \\le Q(x, x^{(k)}) = \\frac{1}{2}\\|Ax - y\\|_2^2 + \\frac{\\lambda}{2} \\sum_{i=1}^n \\frac{1}{\\phi_\\varepsilon(x_i^{(k)})} x_i^2 + C(x^{(k)})\n$$\nwhere $C(x^{(k)})$ groups all terms that are constant with respect to $x$.\nThe next iterate $x^{(k+1)}$ is found by minimizing this surrogate function:\n$$\nx^{(k+1)} = \\arg\\min_x Q(x, x^{(k)}) = \\arg\\min_x \\left( \\frac{1}{2}\\|Ax - y\\|_2^2 + \\frac{\\lambda}{2} \\sum_{i=1}^n w_i^{(k)} x_i^2 \\right)\n$$\nwhere we define the weights $w_i^{(k)}$ as:\n$$\nw_i^{(k)} = \\frac{1}{\\phi_\\varepsilon(x_i^{(k)})} = \\frac{1}{\\sqrt{(x_i^{(k)})^2 + \\varepsilon^2}}\n$$\nLet $W^{(k)}$ be the diagonal matrix with diagonal entries $w_i^{(k)}$. The term $\\sum_{i=1}^n w_i^{(k)} x_i^2$ can be written as $x^T W^{(k)} x$. The minimization problem becomes:\n$$\nx^{(k+1)} = \\arg\\min_x \\left( \\frac{1}{2}(Ax - y)^T(Ax-y) + \\frac{\\lambda}{2} x^T W^{(k)} x \\right)\n$$\nThis is a quadratic function of $x$. To find the minimum, we set its gradient with respect to $x$ to zero:\n$$\n\\nabla_x \\left( \\frac{1}{2}(x^T A^T A x - 2y^T A x + y^T y) + \\frac{\\lambda}{2} x^T W^{(k)} x \\right) = 0\n$$\n$$\nA^T A x - A^T y + \\lambda W^{(k)} x = 0\n$$\nRearranging gives the weighted normal equations for $x = x^{(k+1)}$:\n$$\n(A^T A + \\lambda W^{(k)}) x^{(k+1)} = A^T y\n$$\n\n### Interpretation of Weights as Curvature\nThe problem asks to show that the weights correspond to the curvature of the surrogate. The term \"surrogate\" in this context refers to the majorizing function.\nFor the $i$-th component of the penalty, $\\lambda \\phi_\\varepsilon(x_i)$, we constructed the quadratic majorizer (surrogate):\n$$\nM_i(x_i; x_i^{(k)}) = \\text{const}(x_i^{(k)}) + \\frac{\\lambda}{2} \\frac{1}{\\phi_\\varepsilon(x_i^{(k)})} x_i^2 = \\text{const}(x_i^{(k)}) + \\frac{\\lambda}{2} w_i^{(k)} x_i^2\n$$\nThe curvature of a one-dimensional function is related to its second derivative. The second derivative of this quadratic surrogate with respect to $x_i$ is:\n$$\n\\frac{d^2}{dx_i^2} M_i(x_i; x_i^{(k)}) = \\lambda w_i^{(k)}\n$$\nThus, the weight $w_i^{(k)}$ is directly proportional (with constant $\\lambda$) to the curvature (second derivative) of the quadratic surrogate used for the $i$-th penalty term at iteration $k$.\n\n### Numerical Computation: One IRLS Iteration\nWe are given:\n$A = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix}$, $y = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, $\\lambda = 0.1$, $\\varepsilon = 10^{-3}$, and $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n\n**1. Compute weights $w^{(0)}$ and matrix $W^{(0)}$:**\nThe weights are $w_i^{(k)} = 1/\\sqrt{(x_i^{(k)})^2 + \\varepsilon^2}$. For $k=0$ and $x^{(0)} = \\begin{bmatrix} 0  0  0 \\end{bmatrix}^T$:\n$$\nw_1^{(0)} = w_2^{(0)} = w_3^{(0)} = \\frac{1}{\\sqrt{0^2 + (10^{-3})^2}} = \\frac{1}{10^{-3}} = 1000\n$$\nSo, $w^{(0)} = \\begin{bmatrix} 1000 \\\\ 1000 \\\\ 1000 \\end{bmatrix}$, and $W^{(0)} = \\text{diag}(1000, 1000, 1000) = 1000 I_3$.\n\n**2. Solve for $x^{(1)}$:**\nWe must solve the system $(A^T A + \\lambda W^{(0)}) x^{(1)} = A^T y$.\nFirst, compute the components:\n$$\nA^T = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}\n$$\n$$\nA^T A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix} = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\end{bmatrix}\n$$\n$$\nA^T y = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix}\n$$\n$$\n\\lambda W^{(0)} = 0.1 \\times 1000 I_3 = 100 I_3 = \\begin{bmatrix} 100  0  0 \\\\ 0  100  0 \\\\ 0  0  100 \\end{bmatrix}\n$$\nThe matrix of the system is:\n$$\nA^T A + \\lambda W^{(0)} = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\end{bmatrix} + \\begin{bmatrix} 100  0  0 \\\\ 0  100  0 \\\\ 0  0  100 \\end{bmatrix} = \\begin{bmatrix} 101  0  1 \\\\ 0  101  1 \\\\ 1  1  102 \\end{bmatrix}\n$$\nThe linear system is:\n$$\n\\begin{bmatrix} 101  0  1 \\\\ 0  101  1 \\\\ 1  1  102 \\end{bmatrix} \\begin{bmatrix} x_1^{(1)} \\\\ x_2^{(1)} \\\\ x_3^{(1)} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix}\n$$\nFrom the first two rows:\n$101 x_1^{(1)} + x_3^{(1)} = 1 \\implies x_1^{(1)} = (1 - x_3^{(1)}) / 101$\n$101 x_2^{(1)} + x_3^{(1)} = 1 \\implies x_2^{(1)} = (1 - x_3^{(1)}) / 101$\nThis shows $x_1^{(1)} = x_2^{(1)}$.\nSubstitute these into the third equation:\n$x_1^{(1)} + x_2^{(1)} + 102 x_3^{(1)} = 2$\n$2 \\left( \\frac{1 - x_3^{(1)}}{101} \\right) + 102 x_3^{(1)} = 2$\n$2(1 - x_3^{(1)}) + 101 \\times 102 x_3^{(1)} = 2 \\times 101$\n$2 - 2x_3^{(1)} + 10302 x_3^{(1)} = 202$\n$10300 x_3^{(1)} = 200 \\implies x_3^{(1)} = \\frac{200}{10300} = \\frac{2}{103}$\nNow, we find $x_1^{(1)}$ and $x_2^{(1)}$:\n$x_1^{(1)} = x_2^{(1)} = \\frac{1 - 2/103}{101} = \\frac{(103-2)/103}{101} = \\frac{101/103}{101} = \\frac{1}{103}$\nSo, $x^{(1)} = \\begin{bmatrix} 1/103 \\\\ 1/103 \\\\ 2/103 \\end{bmatrix}$.\n\n**3. Evaluate $w^{(1)}$ from $x^{(1)}$:**\nUsing $x^{(1)}$ and $\\varepsilon = 10^{-3} = 1/1000$:\n$w_1^{(1)} = \\frac{1}{\\sqrt{(x_1^{(1)})^2 + \\varepsilon^2}} = \\frac{1}{\\sqrt{(1/103)^2 + (1/1000)^2}} = \\frac{1}{\\sqrt{\\frac{1}{103^2} + \\frac{1}{1000^2}}} = \\frac{1}{\\sqrt{\\frac{1000^2 + 103^2}{103^2 \\cdot 1000^2}}} = \\frac{103 \\cdot 1000}{\\sqrt{1000^2 + 103^2}}$\n$103^2 = 10609$, $1000^2 = 1000000$.\n$w_1^{(1)} = \\frac{103000}{\\sqrt{1000000 + 10609}} = \\frac{103000}{\\sqrt{1010609}}$.\n\nSince $x_2^{(1)} = x_1^{(1)}$, $w_2^{(1)} = w_1^{(1)} = \\frac{103000}{\\sqrt{1010609}}$.\n\nFor $w_3^{(1)}$:\n$w_3^{(1)} = \\frac{1}{\\sqrt{(x_3^{(1)})^2 + \\varepsilon^2}} = \\frac{1}{\\sqrt{(2/103)^2 + (1/1000)^2}} = \\frac{1}{\\sqrt{\\frac{4}{103^2} + \\frac{1}{1000^2}}} = \\frac{103 \\cdot 1000}{\\sqrt{4 \\cdot 1000^2 + 103^2}}$\n$w_3^{(1)} = \\frac{103000}{\\sqrt{4000000 + 10609}} = \\frac{103000}{\\sqrt{4010609}}$.\n\nThe required components for the final answer are:\n$x_1^{(1)} = \\frac{1}{103}$, $x_2^{(1)} = \\frac{1}{103}$, $x_3^{(1)} = \\frac{2}{103}$\n$w_1^{(1)} = \\frac{103000}{\\sqrt{1010609}}$, $w_2^{(1)} = \\frac{103000}{\\sqrt{1010609}}$, $w_3^{(1)} = \\frac{103000}{\\sqrt{4010609}}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{103}  \\frac{1}{103}  \\frac{2}{103}  \\frac{103000}{\\sqrt{1010609}}  \\frac{103000}{\\sqrt{1010609}}  \\frac{103000}{\\sqrt{4010609}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While powerful, IRLS for non-convex $\\ell_p$ minimization with $p \\lt 1$ does not guarantee convergence to the globally optimal sparse solution. This practice demonstrates a crucial limitation by constructing a simple yet insightful counterexample where the algorithm, due to a symmetric initialization, becomes trapped in a suboptimal, non-sparse stationary point. Analyzing this failure case is key to understanding the local nature of the algorithm and the importance of initialization strategies .",
            "id": "3454768",
            "problem": "Consider a linear sensing model in compressed sensing with measurement matrix $A \\in \\mathbb{R}^{1 \\times 2}$, unknown signal $x \\in \\mathbb{R}^{2}$, and measurements $y \\in \\mathbb{R}$ given by $y = A x$. Let $A = [\\,1 \\;\\; 1\\,]$ and suppose the data are noise-free with ground-truth $x^{\\mathrm{true}} = (1,0)^{\\top}$, so that $y = A x^{\\mathrm{true}} = 1$. The Restricted Isometry Property (RIP), defined for order $k$ by the existence of a constant $\\delta_{k} \\in [0,1)$ such that for every $k$-sparse $x$,\n$$(1 - \\delta_{k}) \\|x\\|_{2}^{2} \\le \\|A x\\|_{2}^{2} \\le (1 + \\delta_{k}) \\|x\\|_{2}^{2},$$\nis violated for $k = 2$ by this $A$. We consider Iterative Reweighted Least Squares (IRLS) for $\\ell_{p}$ minimization with $p \\in (0,1)$ and a smoothing parameter $\\epsilon  0$. Define the smoothed $\\ell_{p}$ objective\n$$J_{p,\\epsilon}(x) = \\sum_{i=1}^{2} \\big(x_{i}^{2} + \\epsilon^{2}\\big)^{\\frac{p}{2}}.$$\nThe IRLS scheme at iteration $k$ chooses weights $w_{i}^{(k)}$ based on $x^{(k)}$, and then computes $x^{(k+1)}$ as a solution of the constrained weighted least squares subproblem,\n$$\\min_{x \\in \\mathbb{R}^{2}} \\sum_{i=1}^{2} w_{i}^{(k)} x_{i}^{2} \\quad \\text{subject to} \\quad A x = y,$$\nwith positive weights obeying the standard monotonicity with respect to $|x_{i}^{(k)}|$, for instance $w_{i}^{(k)} = \\big(x_{i}^{(k)}{}^{2} + \\epsilon^{2}\\big)^{\\frac{p}{2} - 1}$. Initialize the IRLS algorithm at the symmetric point $x^{(0)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$.\n\nUsing only fundamental definitions and first-order optimality of constrained smooth optimization, show that the IRLS iterates remain at the symmetric point and hence converge to a stationary point $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$ for the smoothed objective $J_{p,\\epsilon}$, despite the existence of a sparser feasible point $x^{\\mathrm{true}} = (1,0)^{\\top}$. Construct the explicit counterexample described above and, to quantify the suboptimality, compute the exact analytic expression for the cost gap\n$$\\Delta(p,\\epsilon) = J_{p,\\epsilon}\\!\\big(x^{\\star}\\big) - J_{p,\\epsilon}\\!\\big(x^{\\mathrm{true}}\\big),$$\nas a closed-form function of $p \\in (0,1)$ and $\\epsilon  0$. Your final answer must be this single expression. No rounding is required, and there are no physical units.",
            "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. All necessary information is provided, and there are no internal contradictions. We proceed with the solution.\n\nThe problem asks us to analyze the behavior of the Iterative Reweighted Least Squares (IRLS) algorithm for a specific instance of $\\ell_{p}$ minimization. We are given a linear system $y = Ax$ with $A = [\\,1 \\;\\; 1\\,]$, $x \\in \\mathbb{R}^{2}$, and $y = 1$. The IRLS algorithm attempts to find a sparse solution by minimizing the smoothed $\\ell_{p}$ objective function $J_{p,\\epsilon}(x) = \\sum_{i=1}^{2} \\big(x_{i}^{2} + \\epsilon^{2}\\big)^{\\frac{p}{2}}$ for $p \\in (0,1)$ and a smoothing parameter $\\epsilon  0$, subject to the constraint $Ax = y$.\n\nThe algorithm is initialized at the feasible point $x^{(0)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$, since $A x^{(0)} = 1 \\cdot \\tfrac{1}{2} + 1 \\cdot \\tfrac{1}{2} = 1 = y$. At each iteration $k$, the next iterate $x^{(k+1)}$ is found by solving the weighted least squares problem:\n$$x^{(k+1)} = \\arg\\min_{x \\in \\mathbb{R}^{2}} \\sum_{i=1}^{2} w_{i}^{(k)} x_{i}^{2} \\quad \\text{subject to} \\quad A x = y$$\nwhere the weights are given by $w_{i}^{(k)} = \\big((x_{i}^{(k)})^{2} + \\epsilon^{2}\\big)^{\\frac{p}{2} - 1}$.\n\nFirst, we show by induction that the iterates remain at the symmetric point $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$.\nThe base case is given: $x^{(0)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$.\n\nFor the inductive step, assume that for some $k \\ge 0$, we have $x^{(k)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$. We must compute $x^{(k+1)}$.\nFirst, we calculate the weights $w_i^{(k)}$ based on $x^{(k)}$:\n$$x_{1}^{(k)} = x_{2}^{(k)} = \\frac{1}{2}$$\nThe weights are therefore identical:\n$$w_{1}^{(k)} = \\left(\\left(\\frac{1}{2}\\right)^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2} - 1} = \\left(\\frac{1}{4} + \\epsilon^{2}\\right)^{\\frac{p}{2} - 1}$$\n$$w_{2}^{(k)} = \\left(\\left(\\frac{1}{2}\\right)^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2} - 1} = \\left(\\frac{1}{4} + \\epsilon^{2}\\right)^{\\frac{p}{2} - 1}$$\nLet $w^{(k)} = w_{1}^{(k)} = w_{2}^{(k)}$. Since $p \\in (0,1)$ and $\\epsilon  0$, the base of the power is positive, so the weight $w^{(k)}$ is well-defined and positive.\n\nNow, we solve the subproblem for $x^{(k+1)} = (x_1, x_2)^\\top$:\n$$\\min_{x_1, x_2} w^{(k)} x_{1}^{2} + w^{(k)} x_{2}^{2} \\quad \\text{subject to} \\quad x_{1} + x_{2} = 1$$\nSince $w^{(k)}  0$, this is equivalent to minimizing $x_{1}^{2} + x_{2}^{2}$. This is a classic problem of finding the point on the line $x_1+x_2=1$ with the minimum Euclidean norm. We can use the method of Lagrange multipliers. The Lagrangian is:\n$$\\mathcal{L}(x_1, x_2, \\nu) = x_{1}^{2} + x_{2}^{2} + \\nu(x_{1} + x_{2} - 1)$$\nTaking partial derivatives and setting them to zero gives the first-order optimality conditions:\n$$\\frac{\\partial \\mathcal{L}}{\\partial x_1} = 2x_{1} + \\nu = 0 \\implies x_1 = -\\frac{\\nu}{2}$$\n$$\\frac{\\partial \\mathcal{L}}{\\partial x_2} = 2x_{2} + \\nu = 0 \\implies x_2 = -\\frac{\\nu}{2}$$\nThis implies $x_1 = x_2$. Substituting this into the constraint $x_1 + x_2 = 1$, we get:\n$$x_1 + x_1 = 2x_1 = 1 \\implies x_1 = \\frac{1}{2}$$\nTherefore, the unique solution is $x_1 = \\frac{1}{2}$ and $x_2 = \\frac{1}{2}$.\nSo, $x^{(k+1)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$.\nBy induction, $x^{(k)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$ for all $k \\ge 0$. The sequence of iterates is constant and thus converges to $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$.\n\nNext, we must show that $x^{\\star}$ is a stationary point of the constrained smoothed objective $J_{p,\\epsilon}(x)$. A point is stationary if it satisfies the Karush-Kuhn-Tucker (KKT) conditions. For an equality-constrained problem $\\min f(x)$ s.t. $g(x)=0$, this means there exists a Lagrange multiplier $\\nu$ such that $\\nabla f(x^{\\star}) + \\nu \\nabla g(x^{\\star}) = 0$.\nHere, $f(x) = J_{p,\\epsilon}(x)$ and $g(x) = Ax-y = x_1+x_2-1$.\nThe gradient of the objective function is $\\nabla J_{p,\\epsilon}(x) = \\begin{pmatrix} \\frac{\\partial J_{p,\\epsilon}}{\\partial x_1} \\\\ \\frac{\\partial J_{p,\\epsilon}}{\\partial x_2} \\end{pmatrix}$, where\n$$\\frac{\\partial J_{p,\\epsilon}}{\\partial x_i} = p x_i \\left(x_i^2 + \\epsilon^2\\right)^{\\frac{p}{2}-1}$$\nAt $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$, both components of the gradient are equal:\n$$\\frac{\\partial J_{p,\\epsilon}}{\\partial x_1}\\bigg|_{x^{\\star}} = \\frac{\\partial J_{p,\\epsilon}}{\\partial x_2}\\bigg|_{x^{\\star}} = p \\left(\\frac{1}{2}\\right) \\left(\\left(\\frac{1}{2}\\right)^2 + \\epsilon^2\\right)^{\\frac{p}{2}-1} = \\frac{p}{2} \\left(\\frac{1}{4} + \\epsilon^2\\right)^{\\frac{p}{2}-1}$$\nLet this common value be $C$. So, $\\nabla J_{p,\\epsilon}(x^{\\star}) = (C, C)^{\\top}$.\nThe gradient of the constraint is $\\nabla g(x) = A^{\\top} = (1, 1)^{\\top}$.\nThe KKT condition is $\\nabla J_{p,\\epsilon}(x^{\\star}) + \\nu A^{\\top} = 0$:\n$$\\begin{pmatrix} C \\\\ C \\end{pmatrix} + \\nu \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThis system is satisfied by choosing $\\nu = -C$. Since such a Lagrange multiplier exists, $x^{\\star}$ is a stationary point of the constrained problem. This confirms that the IRLS algorithm converges to a stationary point, which in this case is not the sparsest possible solution $x^{\\mathrm{true}} = (1,0)^{\\top}$.\n\nFinally, we compute the cost gap $\\Delta(p,\\epsilon) = J_{p,\\epsilon}(x^{\\star}) - J_{p,\\epsilon}(x^{\\mathrm{true}})$.\nWe evaluate the objective function at the stationary point $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$:\n$$J_{p,\\epsilon}(x^{\\star}) = \\left(\\left(\\frac{1}{2}\\right)^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\left(\\left(\\frac{1}{2}\\right)^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2}} = 2\\left(\\frac{1}{4} + \\epsilon^{2}\\right)^{\\frac{p}{2}}$$\nWe can simplify this term:\n$$2\\left(\\frac{1}{4} + \\epsilon^{2}\\right)^{\\frac{p}{2}} = 2\\left(\\frac{1+4\\epsilon^{2}}{4}\\right)^{\\frac{p}{2}} = 2 \\frac{\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}}}{4^{\\frac{p}{2}}} = 2 \\frac{\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}}}{2^{p}} = 2^{1-p}\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}}$$\nNext, we evaluate the objective function at the sparse ground-truth point $x^{\\mathrm{true}} = (1,0)^{\\top}$:\n$$J_{p,\\epsilon}(x^{\\mathrm{true}}) = \\left(1^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\left(0^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2}} = \\left(1 + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\left(\\epsilon^{2}\\right)^{\\frac{p}{2}}$$\nSince $\\epsilon  0$, $(\\epsilon^2)^{p/2} = \\epsilon^p$.\nSo, $J_{p,\\epsilon}(x^{\\mathrm{true}}) = \\left(1 + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\epsilon^{p}$.\n\nThe cost gap $\\Delta(p,\\epsilon)$ is the difference:\n$$\\Delta(p,\\epsilon) = J_{p,\\epsilon}(x^{\\star}) - J_{p,\\epsilon}(x^{\\mathrm{true}}) = 2^{1-p}\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}} - \\left( \\left(1 + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\epsilon^{p} \\right)$$\n$$\\Delta(p,\\epsilon) = 2^{1-p}\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}} - \\left(1 + \\epsilon^{2}\\right)^{\\frac{p}{2}} - \\epsilon^{p}$$\nThis is the final analytical expression for the suboptimality gap.",
            "answer": "$$\\boxed{2^{1-p}(1+4\\epsilon^2)^{\\frac{p}{2}} - (1+\\epsilon^2)^{\\frac{p}{2}} - \\epsilon^{p}}$$"
        },
        {
            "introduction": "The regularization term in the $\\ell_p$ objective, essential for inducing sparsity, simultaneously introduces a systematic bias by shrinking the magnitudes of the estimated coefficients. This exercise explores the nature of this shrinkage bias and introduces a powerful and widely used technique for correcting it. You will first identify the signal's support using IRLS and then perform a debiasing step by solving an unregularized least-squares problem on that identified support .",
            "id": "3454744",
            "problem": "Consider the nonconvex sparsity-regularized least-squares problem that seeks to minimize the objective\n$$\n\\frac{1}{2}\\,\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{p}^{p},\n$$\nwhere $x \\in \\mathbb{R}^{n}$, $y \\in \\mathbb{R}^{m}$, $A \\in \\mathbb{R}^{m \\times n}$, $0  p  1$, and $\\lambda  0$. The Iteratively Reweighted Least Squares (IRLS) method constructs at iteration $t$ a quadratic surrogate for the nonconvex penalty via diagonal weights $w_{i}^{(t)}$ and then minimizes the surrogate subproblem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\;\\frac{1}{2}\\,\\|A x - y\\|_{2}^{2} + \\frac{\\lambda}{2}\\sum_{i=1}^{n} w_{i}^{(t)} x_{i}^{2},\n$$\nwith $w_{i}^{(t)}$ chosen from the current iterate $x^{(t)}$ by\n$$\nw_{i}^{(t)} \\;=\\; \\left(\\,|x_{i}^{(t)}|^{2} + \\varepsilon^{2}\\,\\right)^{\\frac{p}{2} - 1},\n$$\nwhere $\\varepsilon  0$ is a small parameter that ensures differentiability and bounds the weights. Assume throughout this problem that $A$ has orthonormal columns so that $A^{\\top}A = I$.\n\nYou will analyze, in a single IRLS surrogate step, the bias introduced by the quadratic surrogate and by the choice $\\lambda  0$, and then propose a debiasing step using restricted least squares on the support identified by IRLS.\n\nStart from the following fundamental definitions:\n- The least-squares normal equations for minimizing a strictly convex quadratic in $x$.\n- The diagonal structure of the surrogate Hessian implied by the IRLS weights.\n- The definition of support of a vector as the set of indices where its entries are nonzero.\n\nThen carry out the following tasks:\n\n1) Derive, from first principles and the assumptions above, the closed-form solution of the IRLS surrogate subproblem with fixed weights, expressed coordinate-wise as a function of $y$, $\\lambda$, and the weights.\n\n2) Consider a single surrogate step initialized at $x^{(0)} = y$ with parameters $p = 0.5$, $\\varepsilon = 0.1$, $\\lambda = 0.4$, $A = I$, and\n$$\ny \\;=\\; \\begin{pmatrix} 3 \\\\ 1 \\\\ 0.2 \\end{pmatrix}.\n$$\nUsing the weight formula at $x^{(0)}$, construct two sets of weights:\n- The $\\varepsilon$-regularized weights $w_{\\varepsilon,i} = \\left(|y_{i}|^{2} + \\varepsilon^{2}\\right)^{\\frac{p}{2}-1}$.\n- The idealized weights without $\\varepsilon$, $w_{0,i} = |y_{i}|^{p-2}$ (use the standard subgradient convention at zero; all given $y_{i}$ are nonzero).\n\nUsing your result from part (1), compute the two surrogate solutions\n$$\n\\widehat{x}_{\\varepsilon} \\quad \\text{and} \\quad \\widehat{x}_{0},\n$$\nobtained by using $w_{\\varepsilon}$ and $w_{0}$, respectively.\n\n3) Define and decompose the total bias of the IRLS estimate relative to data-consistent least squares (here, $A = I$ implies least squares returns $y$) into\n$$\nb_{\\lambda} \\;=\\; \\widehat{x}_{0} - y, \n\\qquad\nb_{\\mathrm{sur}} \\;=\\; \\widehat{x}_{\\varepsilon} - \\widehat{x}_{0},\n\\qquad\nb_{\\mathrm{tot}} \\;=\\; \\widehat{x}_{\\varepsilon} - y \\;=\\; b_{\\lambda} + b_{\\mathrm{sur}}.\n$$\nCompute the Euclidean norm $\\|b_{\\mathrm{tot}}\\|_{2}$.\n\n4) Propose a debiasing step that uses the support identified by the IRLS estimate to eliminate the shrinkage bias on that support by solving a restricted least-squares problem. Let the IRLS-identified support be\n$$\nS \\;=\\; \\{\\, i \\in \\{1,2,3\\} : |\\widehat{x}_{\\varepsilon,i}| \\geq 0.5 \\,\\}.\n$$\nWrite down the restricted least-squares estimator on $S$ and explain why, for $A = I$, this removes the shrinkage bias on $S$.\n\nAnswer specification: Report only the single numerical value of $\\|b_{\\mathrm{tot}}\\|_{2}$, rounded to four significant figures. No units are required.",
            "solution": "The problem is validated as self-contained, scientifically grounded in the field of sparse optimization, and well-posed. All necessary data and definitions are provided, and there are no internal contradictions or ambiguities. I will proceed with a full solution.\n\nThe analysis is structured into four parts as requested by the problem statement.\n\n1) Derivation of the closed-form solution for the IRLS surrogate subproblem.\n\nThe objective function for the IRLS surrogate subproblem is given by:\n$$\nL(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{\\lambda}{2}\\sum_{i=1}^{n} w_{i} x_{i}^{2}\n$$\nwhere the weights $w_i$ are considered fixed for this minimization step. This is a strictly convex quadratic function of $x$, as the Hessian is positive definite. The first term's Hessian is $A^{\\top}A$, and the second term's Hessian is $\\lambda W$, where $W=\\text{diag}(w_1, \\dots, w_n)$. With $w_i  0$ (guaranteed by $\\varepsilon  0$) and $\\lambda0$, $\\lambda W$ is positive definite. $A^{\\top}A$ is positive semi-definite. Thus, their sum is positive definite, ensuring a unique minimum.\n\nTo find the minimum, we compute the gradient of $L(x)$ with respect to $x$ and set it to the zero vector.\n$$\n\\nabla_x L(x) = \\frac{\\partial}{\\partial x} \\left( \\frac{1}{2}(Ax-y)^{\\top}(Ax-y) + \\frac{\\lambda}{2}x^{\\top}Wx \\right)\n$$\n$$\n\\nabla_x L(x) = A^{\\top}(Ax - y) + \\lambda Wx\n$$\nSetting the gradient to zero to find the optimal $x$:\n$$\nA^{\\top}Ax - A^{\\top}y + \\lambda Wx = 0\n$$\nRearranging the terms, we get the normal equations for this regularized least-squares problem:\n$$\n(A^{\\top}A + \\lambda W)x = A^{\\top}y\n$$\nThe problem states the assumption that $A$ has orthonormal columns, which means $A^{\\top}A = I$, where $I$ is the $n \\times n$ identity matrix. Substituting this into the equation gives:\n$$\n(I + \\lambda W)x = A^{\\top}y\n$$\nThe matrix $(I + \\lambda W)$ is a diagonal matrix with entries $(I + \\lambda W)_{ii} = 1 + \\lambda w_i$. Its inverse is also a diagonal matrix with entries $(I + \\lambda W)^{-1}_{ii} = \\frac{1}{1 + \\lambda w_i}$. Therefore, the solution for $x$ can be found by simple element-wise division. The $i$-th component of the solution vector $x$ is:\n$$\nx_i = \\frac{(A^{\\top}y)_i}{1 + \\lambda w_i}\n$$\nThis is the closed-form coordinate-wise solution for the IRLS surrogate subproblem under the given assumptions.\n\n2) Computation of surrogate solutions $\\widehat{x}_{\\varepsilon}$ and $\\widehat{x}_{0}$.\n\nWe are given the specific parameters: $p = 0.5$, $\\varepsilon = 0.1$, $\\lambda = 0.4$, $A = I$, and $y = \\begin{pmatrix} 3 \\\\ 1 \\\\ 0.2 \\end{pmatrix}$. The initialization for the IRLS iteration is $x^{(0)} = y$. The weights are computed based on this initial iterate. Since $A=I$, we have $A^{\\top}y=y$, and the solution from part (1) simplifies to:\n$$\nx_i = \\frac{y_i}{1 + \\lambda w_i}\n$$\n\nFirst, we compute the $\\varepsilon$-regularized weights $w_{\\varepsilon,i}$ using the formula $w_{i} = (|x_i^{(0)}|^2 + \\varepsilon^2)^{\\frac{p}{2} - 1}$. With $x^{(0)} = y$, $p=0.5$, and $\\varepsilon=0.1$, the exponent is $\\frac{p}{2}-1 = \\frac{0.5}{2}-1 = 0.25 - 1 = -0.75$.\n$$\nw_{\\varepsilon,1} = (|y_1|^2 + \\varepsilon^2)^{-0.75} = (3^2 + 0.1^2)^{-0.75} = (9.01)^{-0.75}\n$$\n$$\nw_{\\varepsilon,2} = (|y_2|^2 + \\varepsilon^2)^{-0.75} = (1^2 + 0.1^2)^{-0.75} = (1.01)^{-0.75}\n$$\n$$\nw_{\\varepsilon,3} = (|y_3|^2 + \\varepsilon^2)^{-0.75} = (0.2^2 + 0.1^2)^{-0.75} = (0.04 + 0.01)^{-0.75} = (0.05)^{-0.75}\n$$\nNow we compute the solution $\\widehat{x}_{\\varepsilon}$:\n$$\n\\widehat{x}_{\\varepsilon,1} = \\frac{y_1}{1 + \\lambda w_{\\varepsilon,1}} = \\frac{3}{1 + 0.4 \\cdot (9.01)^{-0.75}} \\approx \\frac{3}{1 + 0.4 \\cdot 0.192348} = \\frac{3}{1.076939} \\approx 2.78566\n$$\n$$\n\\widehat{x}_{\\varepsilon,2} = \\frac{y_2}{1 + \\lambda w_{\\varepsilon,2}} = \\frac{1}{1 + 0.4 \\cdot (1.01)^{-0.75}} \\approx \\frac{1}{1 + 0.4 \\cdot 0.992550} = \\frac{1}{1.397020} \\approx 0.71581\n$$\n$$\n\\widehat{x}_{\\varepsilon,3} = \\frac{y_3}{1 + \\lambda w_{\\varepsilon,3}} = \\frac{0.2}{1 + 0.4 \\cdot (0.05)^{-0.75}} \\approx \\frac{0.2}{1 + 0.4 \\cdot 9.457416} = \\frac{0.2}{4.782966} \\approx 0.04182\n$$\nSo, $\\widehat{x}_{\\varepsilon} \\approx \\begin{pmatrix} 2.78566 \\\\ 0.71581 \\\\ 0.04182 \\end{pmatrix}$.\n\nNext, we compute the idealized weights $w_{0,i}$ using the formula $w_{0,i} = |y_i|^{p-2}$. The exponent is $p-2 = 0.5-2 = -1.5$.\n$$\nw_{0,1} = |3|^{-1.5} = 3^{-1.5}\n$$\n$$\nw_{0,2} = |1|^{-1.5} = 1\n$$\n$$\nw_{0,3} = |0.2|^{-1.5}\n$$\nNow we compute the solution $\\widehat{x}_{0}$:\n$$\n\\widehat{x}_{0,1} = \\frac{y_1}{1 + \\lambda w_{0,1}} = \\frac{3}{1 + 0.4 \\cdot 3^{-1.5}} \\approx \\frac{3}{1 + 0.4 \\cdot 0.192450} = \\frac{3}{1.076980} \\approx 2.78556\n$$\n$$\n\\widehat{x}_{0,2} = \\frac{y_2}{1 + \\lambda w_{0,2}} = \\frac{1}{1 + 0.4 \\cdot 1} = \\frac{1}{1.4} = \\frac{5}{7} \\approx 0.71429\n$$\n$$\n\\widehat{x}_{0,3} = \\frac{y_3}{1 + \\lambda w_{0,3}} = \\frac{0.2}{1 + 0.4 \\cdot (0.2)^{-1.5}} \\approx \\frac{0.2}{1 + 0.4 \\cdot 11.18034} = \\frac{0.2}{5.472136} \\approx 0.03655\n$$\nSo, $\\widehat{x}_{0} \\approx \\begin{pmatrix} 2.78556 \\\\ 0.71429 \\\\ 0.03655 \\end{pmatrix}$.\n\n3) Computation of the total bias Euclidean norm $\\|b_{\\mathrm{tot}}\\|_{2}$.\n\nThe total bias is defined as $b_{\\mathrm{tot}} = \\widehat{x}_{\\varepsilon} - y$. Using the high-precision value of $\\widehat{x}_{\\varepsilon}$ and the given $y$:\n$\n\\widehat{x}_{\\varepsilon} \\approx \\begin{pmatrix} 2.7856636 \\\\ 0.7158092 \\\\ 0.0418158 \\end{pmatrix}\n$, \n$\ny = \\begin{pmatrix} 3 \\\\ 1 \\\\ 0.2 \\end{pmatrix}\n$.\n$$\nb_{\\mathrm{tot}} = \\widehat{x}_{\\varepsilon} - y \\approx \\begin{pmatrix} 2.7856636 - 3 \\\\ 0.7158092 - 1 \\\\ 0.0418158 - 0.2 \\end{pmatrix} = \\begin{pmatrix} -0.2143364 \\\\ -0.2841908 \\\\ -0.1581842 \\end{pmatrix}\n$$\nThe Euclidean norm of the total bias is $\\|b_{\\mathrm{tot}}\\|_{2} = \\sqrt{\\sum_{i=1}^3 b_{\\mathrm{tot},i}^2}$.\n$$\n\\|b_{\\mathrm{tot}}\\|_{2}^2 \\approx (-0.2143364)^2 + (-0.2841908)^2 + (-0.1581842)^2\n$$\n$$\n\\|b_{\\mathrm{tot}}\\|_{2}^2 \\approx 0.0459400 + 0.0807640 + 0.0250222 = 0.1517262\n$$\n$$\n\\|b_{\\mathrm{tot}}\\|_{2} = \\sqrt{0.1517262} \\approx 0.3895204\n$$\nRounding to four significant figures, we get $0.3895$.\n\n4) Proposal and explanation of a debiasing step.\n\nThe IRLS estimate $\\widehat{x}_{\\varepsilon}$ is biased towards zero due to the regularization term. This is known as shrinkage. A common technique to mitigate this bias is to perform a debiasing step. This involves first identifying the set of indices corresponding to the nonzero components of the sparse signal, known as the support, and then solving a least-squares problem restricted to this support.\n\nFirst, we identify the support $S$ from the IRLS estimate $\\widehat{x}_{\\varepsilon}$ using the given threshold:\n$$\nS = \\{\\, i \\in \\{1,2,3\\} : |\\widehat{x}_{\\varepsilon,i}| \\geq 0.5 \\,\\}\n$$\nUsing our computed values for $\\widehat{x}_{\\varepsilon}$:\n$|\\widehat{x}_{\\varepsilon,1}| \\approx 2.786 \\geq 0.5$\n$|\\widehat{x}_{\\varepsilon,2}| \\approx 0.716 \\geq 0.5$\n$|\\widehat{x}_{\\varepsilon,3}| \\approx 0.042  0.5$\nThe identified support is therefore $S = \\{1, 2\\}$.\n\nThe proposed debiasing step is to solve the following restricted least-squares problem:\n$$\n\\min_{x \\in \\mathbb{R}^3} \\|Ax - y\\|_{2}^{2} \\quad \\text{subject to} \\quad x_i = 0 \\text{ for all } i \\notin S\n$$\nThis is equivalent to solving for the non-zero components $x_S = (x_i)_{i \\in S}$:\n$$\n\\min_{x_S \\in \\mathbb{R}^{|S|}} \\|A_S x_S - y\\|_{2}^{2}\n$$\nwhere $A_S$ is the submatrix of $A$ containing columns with indices in $S$. The solution is given by the normal equations: $x_S = (A_S^{\\top}A_S)^{-1}A_S^{\\top}y$.\n\nIn our specific case, $A = I$. Thus, $A_S$ consists of the first two standard basis vectors, $e_1$ and $e_2$.\n$$\nA_S = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix}\n$$\nSince the columns of $A_S$ are orthonormal, $A_S^{\\top}A_S = I_{2 \\times 2}$. The solution simplifies to $x_S = A_S^{\\top}y$.\n$$\nx_S = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\\\ 0.2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n$$\nThe debiased estimator, $x_{\\text{debiased}}$, is constructed by setting components on $S$ to $x_S$ and components outside $S$ to zero:\n$$\nx_{\\text{debiased}} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThis procedure removes the shrinkage bias on the identified support $S$. The original IRLS estimate components on $S$, $\\widehat{x}_{\\varepsilon,1} \\approx 2.786$ and $\\widehat{x}_{\\varepsilon,2} \\approx 0.716$, were shrunken versions of the corresponding data components $y_1=3$ and $y_2=1$. The debiasing step restores these values to the original data values, $x_{\\text{debiased},1} = y_1 = 3$ and $x_{\\text{debiased},2} = y_2 = 1$. For the special case $A=I$, the data-consistent least-squares solution is $y$ itself. By setting $x_S = y_S$, the debiased estimator exactly matches the ideal data on the estimated support, thus completely eliminating the bias on $S$ under the assumption that the support was correctly identified.",
            "answer": "$$\n\\boxed{0.3895}\n$$"
        }
    ]
}