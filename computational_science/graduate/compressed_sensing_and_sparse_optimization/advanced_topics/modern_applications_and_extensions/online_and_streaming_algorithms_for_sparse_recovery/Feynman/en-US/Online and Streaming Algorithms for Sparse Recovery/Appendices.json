{
    "hands_on_practices": [
        {
            "introduction": "To begin, we address the fundamental challenge of tracking a sparse signal that evolves over time. This exercise guides you through the construction of an online estimator from first principles, using a proximal gradient update that balances fidelity to new measurements with temporal smoothness via an $\\ell_1$ penalty on successive estimates. By analyzing the tracker's response to both abrupt and gradual changes in a simplified setting, you will develop a concrete understanding of how algorithm parameters influence dynamic tracking performance .",
            "id": "3463870",
            "problem": "Consider a streaming linear measurement model for a time-varying sparse signal, where at each time index $t$ you observe $y_t \\in \\mathbb{R}^{m}$ given by $y_t = A x_t^{\\star} + w_t$, with $A \\in \\mathbb{R}^{m \\times n}$ fixed, $x_t^{\\star} \\in \\mathbb{R}^{n}$ the ground-truth signal that evolves over time, and $w_t \\in \\mathbb{R}^{m}$ additive noise. You are tasked with designing an online estimator $x_t \\in \\mathbb{R}^{n}$ that enforces temporal sparsity of the increments $x_t - x_{t-1}$ via an $\\ell_1$ penalty and can be updated upon arrival of $y_t$ without revisiting past data.\n\nStarting from the fundamental base of convex composite optimization and the definition of the proximal operator, construct a one-step online update at time $t$ by minimizing a composite objective comprised of a least-squares data fit and a temporal regularizer that is the sum of an $\\ell_1$ penalty and an $\\ell_2$ (quadratic) penalty on the increment $x - x_{t-1}$. The data-fit term should correspond to the current sample $(A, y_t)$ only. Your construction must yield an explicit componentwise update formula that can be implemented in streaming form, derived from first principles, and should indicate precisely how the $\\ell_1$ increment penalty enters via a proximal mapping.\n\nThen, analyze the estimator’s ability to track abrupt versus gradual changes in the following simplified and noise-free scenario: let $n = 1$, $m = 1$, $A = 1$, $w_t = 0$, and suppose the previous time-step estimate is exact, so $x_{t-1} = x_{t-1}^{\\star}$. The ground-truth changes according to $x_t^{\\star} = x_{t-1}^{\\star} + D$, where $D \\in \\mathbb{R}$ models the magnitude of the change. Consider two cases: an abrupt change $D_{\\mathrm{a}} = 3$ and a gradual change $D_{\\mathrm{g}} = 0.8$. Use a proximal-gradient step size $\\tau = 0.6$, a quadratic increment penalty coefficient $\\gamma = 0.4$, and an $\\ell_1$ increment penalty coefficient $\\mu = 1$. Compute, after a single update at time $t$, the absolute one-step tracking error magnitudes $|e_t^{\\mathrm{a}}|$ and $|e_t^{\\mathrm{g}}|$ for the abrupt and gradual cases, respectively, and then compute the ratio $R = |e_t^{\\mathrm{a}}| / |e_t^{\\mathrm{g}}|$. Round your final numeric answer $R$ to four significant figures. No physical units are involved; report a pure number.",
            "solution": "We begin with the streaming linear model $y_t = A x_t^{\\star} + w_t$ and the goal of producing an online estimate $x_t$ from $y_t$, given the previous estimate $x_{t-1}$. To enforce temporal sparsity in the increments, we consider the convex composite objective at time $t$:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2} \\|A x - y_t\\|_2^2 \\;+\\; \\mu \\|x - x_{t-1}\\|_1 \\;+\\; \\frac{\\gamma}{2} \\|x - x_{t-1}\\|_2^2,\n$$\nwhere $\\mu  0$ controls the $\\ell_1$ penalty on the increment $x - x_{t-1}$ and $\\gamma \\ge 0$ adds a quadratic stabilization. This is a convex composite problem: a smooth data-fit term $\\frac{1}{2}\\|A x - y_t\\|_2^2$ plus a nonsmooth, separable regularizer in the increment $x - x_{t-1}$.\n\nTo update online in streaming form, we employ the proximal-gradient method (also known as the Iterative Shrinkage-Thresholding Algorithm (ISTA)). The proximal-gradient iteration at time $t$ starting from $x_{t-1}$ with step size $\\tau  0$ is\n$$\nz_t \\;=\\; x_{t-1} \\;-\\; \\tau \\nabla \\left( \\frac{1}{2}\\|A x - y_t\\|_2^2 \\right)\\bigg|_{x = x_{t-1}} \\;=\\; x_{t-1} \\;-\\; \\tau A^{\\top} \\left( A x_{t-1} - y_t \\right).\n$$\nIntroduce the increment variable $v = x - x_{t-1}$. The nonsmooth regularizer can be written as $g(v) = \\mu \\|v\\|_1 + \\frac{\\gamma}{2}\\|v\\|_2^2$, which is separable across coordinates. The proximal update applies the proximal operator of $g$ to the gradient step displacement $v_0 = z_t - x_{t-1}$:\n$$\nx_t \\;=\\; x_{t-1} \\;+\\; \\operatorname{prox}_{\\tau g}(v_0),\n$$\nwhere by definition of the proximal operator, for any $v_0 \\in \\mathbb{R}^{n}$ and $\\tau  0$,\n$$\n\\operatorname{prox}_{\\tau g}(v_0) \\;=\\; \\arg\\min_{v \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|v - v_0\\|_2^2 \\;+\\; \\tau \\mu \\|v\\|_1 \\;+\\; \\frac{\\tau \\gamma}{2} \\|v\\|_2^2 \\right\\}.\n$$\nWe now derive the closed-form proximal mapping for $g(v) = \\mu \\|v\\|_1 + \\frac{\\gamma}{2}\\|v\\|_2^2$. The optimization problem is separable and strictly convex. Completing the square yields, for each coordinate,\n$$\n\\min_{v} \\;\\; \\frac{1 + \\tau \\gamma}{2} \\|v\\|_2^2 \\;-\\; v_0^{\\top} v \\;+\\; \\tau \\mu \\|v\\|_1 \\;+\\; \\text{const}.\n$$\nThis is equivalent to\n$$\n\\min_{v} \\;\\; \\frac{1 + \\tau \\gamma}{2} \\left\\| v - \\frac{v_0}{1 + \\tau \\gamma} \\right\\|_2^2 \\;+\\; \\tau \\mu \\|v\\|_1 \\;+\\; \\text{const},\n$$\nwhose unique minimizer is the coordinatewise soft-threshold of the scaled input with a scaled threshold:\n$$\n\\operatorname{prox}_{\\tau g}(v_0) \\;=\\; S_{\\frac{\\tau \\mu}{1 + \\tau \\gamma}} \\!\\left( \\frac{v_0}{1 + \\tau \\gamma} \\right),\n$$\nwhere $S_{\\theta}(u)$ is the soft-threshold operator defined componentwise by $S_{\\theta}(u)_i = \\operatorname{sign}(u_i)\\,\\max\\{|u_i| - \\theta,\\,0\\}$.\n\nTherefore, the explicit streaming update is\n$$\nx_t \\;=\\; x_{t-1} \\;+\\; S_{\\frac{\\tau \\mu}{1 + \\tau \\gamma}} \\!\\left( \\frac{z_t - x_{t-1}}{1 + \\tau \\gamma} \\right),\n\\quad \\text{with} \\quad\nz_t \\;=\\; x_{t-1} \\;-\\; \\tau A^{\\top}(A x_{t-1} - y_t).\n$$\nThis implements temporal sparsity through the soft-thresholding of the predicted increment $z_t - x_{t-1}$.\n\nWe now analyze tracking in the simplified scalar, noise-free case. Let $n = 1$, $m = 1$, $A = 1$, $w_t = 0$, previous estimate exact so $x_{t-1} = x_{t-1}^{\\star}$, and ground-truth changes as $x_t^{\\star} = x_{t-1}^{\\star} + D$. Then $y_t = x_t^{\\star} = x_{t-1}^{\\star} + D$. The gradient step becomes\n$$\nz_t \\;=\\; x_{t-1} \\;-\\; \\tau \\left( x_{t-1} - y_t \\right) \\;=\\; x_{t-1} \\;+\\; \\tau \\left( y_t - x_{t-1} \\right)\n\\;=\\; x_{t-1} \\;+\\; \\tau D,\n$$\nso the predicted increment is $v_0 = z_t - x_{t-1} = \\tau D$. The soft-threshold input and threshold are\n$$\nu \\;=\\; \\frac{v_0}{1 + \\tau \\gamma} \\;=\\; \\frac{\\tau D}{1 + \\tau \\gamma},\n\\qquad\n\\theta \\;=\\; \\frac{\\tau \\mu}{1 + \\tau \\gamma}.\n$$\nThe updated increment is $v = S_{\\theta}(u)$. The one-step estimation error is\n$$\ne_t \\;=\\; x_t - x_t^{\\star} \\;=\\; \\big(x_{t-1} + v\\big) - \\big(x_{t-1}^{\\star} + D\\big) \\;=\\; v - D,\n$$\nbecause $x_{t-1} = x_{t-1}^{\\star}$.\n\nWe evaluate two cases with parameters $\\tau = 0.6$, $\\gamma = 0.4$, $\\mu = 1$ and changes $D_{\\mathrm{a}} = 3$ (abrupt) and $D_{\\mathrm{g}} = 0.8$ (gradual). First compute $1 + \\tau \\gamma = 1 + 0.6 \\cdot 0.4 = 1.24$ and $\\theta = \\frac{0.6 \\cdot 1}{1.24} = \\frac{0.6}{1.24} = 0.4838709677419355$.\n\nAbrupt case $D = D_{\\mathrm{a}} = 3$:\n- $u_{\\mathrm{a}} = \\frac{\\tau D_{\\mathrm{a}}}{1 + \\tau \\gamma} = \\frac{0.6 \\cdot 3}{1.24} = \\frac{1.8}{1.24} = 1.4516129032258065$.\n- Since $u_{\\mathrm{a}}  \\theta$, $v_{\\mathrm{a}} = u_{\\mathrm{a}} - \\theta = 1.4516129032258065 - 0.4838709677419355 = 0.967741935483871$.\n- $e_t^{\\mathrm{a}} = v_{\\mathrm{a}} - D_{\\mathrm{a}} = 0.967741935483871 - 3 = -2.032258064516129$, so $|e_t^{\\mathrm{a}}| = 2.032258064516129$.\n\nGradual case $D = D_{\\mathrm{g}} = 0.8$:\n- $u_{\\mathrm{g}} = \\frac{\\tau D_{\\mathrm{g}}}{1 + \\tau \\gamma} = \\frac{0.6 \\cdot 0.8}{1.24} = \\frac{0.48}{1.24} = 0.3870967741935484$.\n- Since $u_{\\mathrm{g}}  \\theta$, $v_{\\mathrm{g}} = 0$.\n- $e_t^{\\mathrm{g}} = v_{\\mathrm{g}} - D_{\\mathrm{g}} = 0 - 0.8 = -0.8$, so $|e_t^{\\mathrm{g}}| = 0.8$.\n\nThe requested ratio is\n$$\nR \\;=\\; \\frac{|e_t^{\\mathrm{a}}|}{|e_t^{\\mathrm{g}}|}\n\\;=\\; \\frac{2.032258064516129}{0.8}\n\\;=\\; 2.540322580645161.\n$$\nRounded to four significant figures, $R = 2.540$.",
            "answer": "$$\\boxed{2.540}$$"
        },
        {
            "introduction": "While $\\ell_1$-regularized estimators are powerful tools for sparse recovery, they are known to introduce a systematic bias that shrinks the magnitude of non-zero coefficients. A common strategy to mitigate this is online debiasing, where an unpenalized least-squares estimate is computed on the active support set. This practice provides a sharp, probabilistic analysis of how errors in support selection—a persistent challenge in noisy, streaming environments—propagate and affect the steady-state bias of the final debiased estimate .",
            "id": "3463824",
            "problem": "Consider an online linear measurement model for sparse recovery with a fixed dictionary in which, at each discrete time $t \\in \\mathbb{N}$, an observation $y_t \\in \\mathbb{R}^{m}$ is acquired as $y_t = A x^{\\star} + w_t$. The unknown signal $x^{\\star} \\in \\mathbb{R}^{2}$ is time-invariant and exactly sparse with support $\\{1,2\\}$ and equal amplitudes $x^{\\star} = [\\theta,\\theta]^{\\top}$ for some $\\theta \\in \\mathbb{R}$. The matrix $A = [a_1 \\ a_2] \\in \\mathbb{R}^{m \\times 2}$ has unit-norm columns satisfying $\\|a_1\\|_{2} = \\|a_2\\|_{2} = 1$ and mutual inner product $a_1^{\\top} a_2 = \\rho$ with $|\\rho|  1$. The noise process $w_t \\sim \\mathcal{N}(0,\\sigma^{2} I_{m})$ is independent and identically distributed across time and independent of all other randomness. At each time $t$, a least absolute shrinkage and selection operator (LASSO) stage produces an estimated support $\\widehat{S}_t \\subseteq \\{1,2\\}$, with the following probabilistic model: index $1$ is always selected, while index $2$ is included with probability $1-p$ and missed with probability $p \\in [0,1)$, independently across time and independent of $w_t$. No false discoveries outside $\\{1,2\\}$ occur.\n\nTo correct the amplitude shrinkage induced by the $\\ell_{1}$ penalty, an online debiasing step is performed after support selection as follows. At each time $t$, given $\\widehat{S}_t$, compute the instantaneous debiased coefficient for index $1$ via ordinary least squares (OLS) restricted to $\\widehat{S}_t$: if $1 \\in \\widehat{S}_t$ then\n$$\n\\tilde{x}_{1,t} \\triangleq e_1^{\\top} \\arg\\min_{z \\in \\mathbb{R}^{|\\widehat{S}_t|}} \\|y_t - A_{\\widehat{S}_t} z\\|_{2}^{2},\n$$\nwhere $A_{\\widehat{S}_t}$ is the submatrix of $A$ with columns indexed by $\\widehat{S}_t$, and $e_1$ selects the coefficient of index $1$ in the solution when present. The online estimate of index $1$ is then updated by exponential smoothing with forgetting factor $\\alpha \\in (0,1]$:\n$$\n\\widehat{x}_{1,t} = (1-\\alpha)\\,\\widehat{x}_{1,t-1} + \\alpha\\,\\tilde{x}_{1,t}, \\quad \\widehat{x}_{1,0} = 0.\n$$\n\nStarting only from the linear model definition, the properties of ordinary least squares, and basic probability, derive a closed-form analytic expression for the steady-state expected bias of the online debiased estimate of index $1$:\n$$\n\\lim_{t \\to \\infty} \\mathbb{E}\\big[\\widehat{x}_{1,t} - \\theta\\big],\n$$\nas a function of $\\theta$, $\\rho$, $p$, and $\\alpha$. Express your final answer as a single simplified analytic expression. No rounding is required and no units are involved.",
            "solution": "The problem asks for the steady-state expected bias of an online debiased estimate for a sparse signal. Let us denote the true signal value for the first component as $x_1^{\\star} = \\theta$. The quantity to be determined is $\\lim_{t \\to \\infty} \\mathbb{E}[\\widehat{x}_{1,t} - \\theta]$.\n\nThe online estimate $\\widehat{x}_{1,t}$ is updated via the exponential smoothing rule:\n$$\n\\widehat{x}_{1,t} = (1-\\alpha)\\,\\widehat{x}_{1,t-1} + \\alpha\\,\\tilde{x}_{1,t}\n$$\nwith initial condition $\\widehat{x}_{1,0} = 0$. Taking the expectation of this equation, we obtain a recurrence relation for the mean estimate $\\mu_t \\triangleq \\mathbb{E}[\\widehat{x}_{1,t}]$:\n$$\n\\mu_t = (1-\\alpha)\\,\\mu_{t-1} + \\alpha\\,\\mathbb{E}[\\tilde{x}_{1,t}]\n$$\nThe initial condition is $\\mu_0 = \\mathbb{E}[\\widehat{x}_{1,0}] = 0$.\n\nThe probabilistic model for support selection and the noise process are independent and identically distributed over time $t$. Consequently, the statistical properties of the instantaneous estimate $\\tilde{x}_{1,t}$ are time-invariant. Let us denote its constant expectation as $\\nu \\triangleq \\mathbb{E}[\\tilde{x}_{1,t}]$. The recurrence for $\\mu_t$ becomes:\n$$\n\\mu_t = (1-\\alpha)\\,\\mu_{t-1} + \\alpha\\,\\nu\n$$\nThis is a standard linear first-order recurrence relation. Its solution can be found by unrolling the recursion:\n$$\n\\mu_t = (1-\\alpha)^t \\mu_0 + \\alpha \\nu \\sum_{k=0}^{t-1} (1-\\alpha)^k = \\alpha \\nu \\frac{1 - (1-\\alpha)^t}{1 - (1-\\alpha)} = (1 - (1-\\alpha)^t)\\nu\n$$\nWe are interested in the steady-state limit as $t \\to \\infty$. Since $\\alpha \\in (0,1]$, the term $(1-\\alpha)$ lies in the interval $[0,1)$. Therefore, $\\lim_{t \\to \\infty} (1-\\alpha)^t = 0$. The steady-state expected value is:\n$$\n\\mu_{\\infty} \\triangleq \\lim_{t \\to \\infty} \\mu_t = \\nu = \\mathbb{E}[\\tilde{x}_{1,t}]\n$$\nThus, the problem reduces to calculating the expected value of the instantaneous OLS estimate $\\tilde{x}_{1,t}$. The forgetting factor $\\alpha$ affects the convergence rate to the steady state, but not the value of the steady-state expectation itself.\n\nTo find $\\mathbb{E}[\\tilde{x}_{1,t}]$, we use the law of total expectation, conditioning on the two possible realizations of the estimated support set $\\widehat{S}_t$:\n\\begin{enumerate}\n    \\item $\\widehat{S}_t = \\{1\\}$, which occurs with probability $p$.\n    \\item $\\widehat{S}_t = \\{1,2\\}$, which occurs with probability $1-p$.\n\\end{enumerate}\nSo, we can write:\n$$\n\\mathbb{E}[\\tilde{x}_{1,t}] = P(\\widehat{S}_t=\\{1\\}) \\cdot \\mathbb{E}[\\tilde{x}_{1,t} \\,|\\, \\widehat{S}_t=\\{1\\}] + P(\\widehat{S}_t=\\{1,2\\}) \\cdot \\mathbb{E}[\\tilde{x}_{1,t} \\,|\\, \\widehat{S}_t=\\{1,2\\}]\n$$\n$$\n\\mathbb{E}[\\tilde{x}_{1,t}] = p \\cdot \\mathbb{E}[\\tilde{x}_{1,t} \\,|\\, \\widehat{S}_t=\\{1\\}] + (1-p) \\cdot \\mathbb{E}[\\tilde{x}_{1,t} \\,|\\, \\widehat{S}_t=\\{1,2\\}]\n$$\nThe problem states that the support selection process is independent of the noise $w_t$. This means we can compute the expectations of the OLS estimates separately for each case. The observation model is $y_t = A x^{\\star} + w_t = a_1 \\theta + a_2 \\theta + w_t$. The expected value of the observation is $\\mathbb{E}[y_t] = a_1 \\theta + a_2 \\theta$, since $\\mathbb{E}[w_t]=0$.\n\nCase 1: $\\widehat{S}_t = \\{1\\}$.\nThe OLS problem is $\\min_{z_1 \\in \\mathbb{R}} \\|y_t - a_1 z_1\\|_2^2$. The solution is given by the normal equations:\n$$\n\\tilde{x}_{1,t} = (a_1^{\\top} a_1)^{-1} a_1^{\\top} y_t\n$$\nGiven $\\|a_1\\|_2 = 1$, we have $a_1^{\\top} a_1 = 1$. The estimate becomes $\\tilde{x}_{1,t} = a_1^{\\top} y_t$.\nIts conditional expectation is:\n$$\n\\mathbb{E}[\\tilde{x}_{1,t} \\,|\\, \\widehat{S}_t=\\{1\\}] = \\mathbb{E}[a_1^{\\top} y_t] = a_1^{\\top} \\mathbb{E}[y_t] = a_1^{\\top} (a_1 \\theta + a_2 \\theta)\n$$\n$$\n= (a_1^{\\top} a_1)\\theta + (a_1^{\\top} a_2)\\theta = 1 \\cdot \\theta + \\rho \\cdot \\theta = \\theta(1+\\rho)\n$$\nThis is the classic omitted-variable bias: the estimate for coefficient $1$ is biased by the effect of the omitted variable $2$, which is proportional to its true coefficient $\\theta$ and the correlation $\\rho$ between the regressors.\n\nCase 2: $\\widehat{S}_t = \\{1,2\\}$.\nThe OLS problem is $\\min_{z \\in \\mathbb{R}^2} \\|y_t - A z\\|_2^2$. The solution for the vector $z$ is:\n$$\n\\tilde{z}_t = (A^{\\top} A)^{-1} A^{\\top} y_t\n$$\nThe instantaneous estimate $\\tilde{x}_{1,t}$ is the first component of $\\tilde{z}_t$, i.e., $\\tilde{x}_{1,t} = e_1^{\\top} \\tilde{z}_t$.\nFirst, we compute the matrix $A^{\\top} A$ and its inverse:\n$$\nA^{\\top}A = \\begin{pmatrix} a_1^{\\top}a_1  a_1^{\\top}a_2 \\\\ a_2^{\\top}a_1  a_2^{\\top}a_2 \\end{pmatrix} = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}\n$$\n$$\n(A^{\\top}A)^{-1} = \\frac{1}{\\det(A^{\\top}A)} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} = \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix}\n$$\nThe conditional expectation of the estimator vector is:\n$$\n\\mathbb{E}[\\tilde{z}_t \\,|\\, \\widehat{S}_t=\\{1,2\\}] = \\mathbb{E}[(A^{\\top}A)^{-1} A^{\\top} y_t] = (A^{\\top}A)^{-1} A^{\\top} \\mathbb{E}[y_t]\n$$\n$$\n= (A^{\\top}A)^{-1} A^{\\top} (A x^{\\star}) = (A^{\\top}A)^{-1} (A^{\\top}A) x^{\\star} = x^{\\star} = \\begin{pmatrix} \\theta \\\\ \\theta \\end{pmatrix}\n$$\nThis shows that when the support is correctly identified, the OLS estimator is unbiased. The expectation of the first component is:\n$$\n\\mathbb{E}[\\tilde{x}_{1,t} \\,|\\, \\widehat{S}_t=\\{1,2\\}] = e_1^{\\top} x^{\\star} = \\theta\n$$\nNow, we substitute the results from both cases into the law of total expectation:\n$$\n\\mathbb{E}[\\tilde{x}_{1,t}] = p \\cdot \\theta(1+\\rho) + (1-p) \\cdot \\theta\n$$\n$$\n= p\\theta + p\\theta\\rho + \\theta - p\\theta = \\theta + p\\theta\\rho = \\theta(1+p\\rho)\n$$\nThis is the steady-state expected value of the estimate $\\widehat{x}_{1,t}$:\n$$\n\\lim_{t \\to \\infty} \\mathbb{E}[\\widehat{x}_{1,t}] = \\theta(1+p\\rho)\n$$\nFinally, the steady-state expected bias is the difference between this value and the true value $\\theta$:\n$$\n\\text{Bias} = \\lim_{t \\to \\infty} \\mathbb{E}[\\widehat{x}_{1,t} - \\theta] = \\lim_{t \\to \\infty} \\mathbb{E}[\\widehat{x}_{1,t}] - \\theta\n$$\n$$\n= \\theta(1+p\\rho) - \\theta = \\theta + p\\theta\\rho - \\theta = p\\theta\\rho\n$$\nThe bias is directly proportional to the probability of missing the support element $p$, the signal amplitude $\\theta$, and the dictionary coherence $\\rho$.",
            "answer": "$$\n\\boxed{p\\theta\\rho}\n$$"
        },
        {
            "introduction": "In many modern applications, we have access to auxiliary data streams, or \"side information,\" that are correlated with the primary signal of interest. This exercise demonstrates how to formally incorporate such data into an online recovery framework using co-regularization to enhance estimation accuracy. You will derive an optimal weighting scheme that balances the information from the primary observation and the side information, yielding an explicit formula that minimizes the tracking error and highlights the benefits of data fusion .",
            "id": "3463825",
            "problem": "Consider an online streaming estimation problem for a time-varying $k$-sparse signal $x_t \\in \\mathbb{R}^n$ observed through a noisy identity sensing model. At each time $t \\in \\{1,2,\\dots\\}$ you observe $y_t \\in \\mathbb{R}^n$ and side information $z_t \\in \\mathbb{R}^n$ generated as follows. The observation model is $y_t = x_t + w_t$, where $w_t$ has independent, identically distributed entries with zero mean and variance $\\sigma_w^2$. The side information is linearly correlated with $x_t$ and obeys $z_t = \\rho x_t + \\xi_t$, where $\\rho \\in [-1,1]$, and $\\xi_t$ has independent, identically distributed entries with zero mean and variance $\\sigma_z^2$. Assume $x_t$ is zero-mean with independent, identically distributed entries of variance $\\sigma_x^2$, and $\\{x_t\\}$, $\\{w_t\\}$, and $\\{\\xi_t\\}$ are mutually independent processes across all times.\n\nAn online co-regularized estimator is used at each $t$ by minimizing the instantaneous surrogate loss\n$$\n\\mathcal{L}_t(x) = \\frac{1}{2 \\sigma_w^2} \\|x - y_t\\|_2^2 + \\frac{\\gamma}{2} \\|x - B z_t\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwhere $B \\in \\mathbb{R}^{n \\times n}$ is a known linear map, $\\gamma \\ge 0$ is a co-regularization weight, and $\\lambda \\ge 0$ is an $\\ell_1$-regularization parameter intended to promote sparsity. For the purpose of isolating the effect of side information on tracking accuracy in closed form, consider the case $B = I_n$ and $\\lambda = 0$. In this case, the one-shot streaming update at time $t$ is\n$$\n\\widehat{x}_t \\in \\arg\\min_{x \\in \\mathbb{R}^n} \\left\\{ \\frac{1}{2 \\sigma_w^2} \\|x - y_t\\|_2^2 + \\frac{\\gamma}{2} \\|x - z_t\\|_2^2 \\right\\}.\n$$\n\nStarting from the normal equations for the minimizer of a strictly convex quadratic and the independence assumptions above, derive an explicit upper bound on the expected instantaneous tracking error $\\mathbb{E}\\big[\\|\\widehat{x}_t - x_t\\|_2^2\\big]$ as a function of $\\gamma$, $\\sigma_x^2$, $\\sigma_w^2$, $\\sigma_z^2$, and $\\rho$. Then minimize this bound over $\\gamma \\ge 0$ to obtain a closed-form analytic expression for the optimal co-regularization weight $\\gamma^\\star$ that minimizes the bound. Your final answer must be this analytic expression for $\\gamma^\\star$ in terms of $\\sigma_x^2$, $\\sigma_z^2$, and $\\rho$. No numerical evaluation is required, and no rounding is needed. Express your final answer symbolically.",
            "solution": "The problem asks us to find the optimal co-regularization weight $\\gamma^\\star$ for a specific online streaming estimator. The estimator $\\widehat{x}_t$ is defined as the minimizer of an instantaneous loss function. We will first find a closed-form expression for the estimator, then derive the expected instantaneous tracking error, and finally minimize this error with respect to the parameter $\\gamma$ (denoted as $\\gamma$ in the problem for simplicity).\n\nThe loss function to be minimized at each time $t$ is given by:\n$$\n\\mathcal{L}_t(x) = \\frac{1}{2 \\sigma_w^2} \\|x - y_t\\|_2^2 + \\frac{\\gamma}{2} \\|x - z_t\\|_2^2\n$$\nThis function is strictly convex for $\\gamma \\ge 0$ as it is a sum of squared $\\ell_2$-norms with positive coefficients (assuming $\\sigma_w^2  0$). The unique minimizer $\\widehat{x}_t$ can be found by setting the gradient of $\\mathcal{L}_t(x)$ with respect to $x$ to zero.\n\nThe gradient is:\n$$\n\\nabla_x \\mathcal{L}_t(x) = \\frac{1}{\\sigma_w^2} (x - y_t) + \\gamma (x - z_t)\n$$\nSetting the gradient to zero at $x = \\widehat{x}_t$:\n$$\n\\frac{1}{\\sigma_w^2} (\\widehat{x}_t - y_t) + \\gamma (\\widehat{x}_t - z_t) = 0\n$$\nWe rearrange the terms to solve for $\\widehat{x}_t$:\n$$\n\\left(\\frac{1}{\\sigma_w^2} + \\gamma\\right) \\widehat{x}_t = \\frac{1}{\\sigma_w^2} y_t + \\gamma z_t\n$$\n$$\n\\widehat{x}_t = \\frac{\\frac{1}{\\sigma_w^2} y_t + \\gamma z_t}{\\frac{1}{\\sigma_w^2} + \\gamma} = \\frac{y_t + \\gamma \\sigma_w^2 z_t}{1 + \\gamma \\sigma_w^2}\n$$\nThis expression gives the estimator $\\widehat{x}_t$ in terms of the observation $y_t$, the side information $z_t$, and the parameters $\\gamma$ and $\\sigma_w^2$.\n\nNext, we derive the expression for the instantaneous tracking error vector, $\\widehat{x}_t - x_t$. We substitute the given observation and side information models, $y_t = x_t + w_t$ and $z_t = \\rho x_t + \\xi_t$, into the expression for $\\widehat{x}_t$:\n$$\n\\widehat{x}_t = \\frac{(x_t + w_t) + \\gamma \\sigma_w^2 (\\rho x_t + \\xi_t)}{1 + \\gamma \\sigma_w^2}\n$$\n$$\n\\widehat{x}_t = \\frac{(1 + \\gamma \\sigma_w^2 \\rho) x_t + w_t + \\gamma \\sigma_w^2 \\xi_t}{1 + \\gamma \\sigma_w^2}\n$$\nThe error vector is then:\n$$\n\\widehat{x}_t - x_t = \\frac{(1 + \\gamma \\sigma_w^2 \\rho) x_t + w_t + \\gamma \\sigma_w^2 \\xi_t}{1 + \\gamma \\sigma_w^2} - x_t\n$$\n$$\n\\widehat{x}_t - x_t = \\frac{(1 + \\gamma \\sigma_w^2 \\rho - (1 + \\gamma \\sigma_w^2)) x_t + w_t + \\gamma \\sigma_w^2 \\xi_t}{1 + \\gamma \\sigma_w^2}\n$$\n$$\n\\widehat{x}_t - x_t = \\frac{(\\gamma \\sigma_w^2 \\rho - \\gamma \\sigma_w^2) x_t + w_t + \\gamma \\sigma_w^2 \\xi_t}{1 + \\gamma \\sigma_w^2} = \\frac{\\gamma \\sigma_w^2 (\\rho - 1) x_t + w_t + \\gamma \\sigma_w^2 \\xi_t}{1 + \\gamma \\sigma_w^2}\n$$\nThe problem asks for an upper bound on the expected instantaneous tracking error, $\\mathbb{E}\\big[\\|\\widehat{x}_t - x_t\\|_2^2\\big]$. In this simplified setting, we can compute this expectation exactly. This exact expression is the tightest possible upper bound. Let $J(\\gamma) = \\mathbb{E}\\big[\\|\\widehat{x}_t - x_t\\|_2^2\\big]$.\n$$\nJ(\\gamma) = \\mathbb{E}\\left[ \\left\\| \\frac{\\gamma \\sigma_w^2 (\\rho - 1) x_t + w_t + \\gamma \\sigma_w^2 \\xi_t}{1 + \\gamma \\sigma_w^2} \\right\\|_2^2 \\right]\n$$\n$$\nJ(\\gamma) = \\frac{1}{(1 + \\gamma \\sigma_w^2)^2} \\mathbb{E}\\left[ \\|\\gamma \\sigma_w^2 (\\rho - 1) x_t + w_t + \\gamma \\sigma_w^2 \\xi_t\\|_2^2 \\right]\n$$\nThe random processes $\\{x_t\\}$, $\\{w_t\\}$, and $\\{\\xi_t\\}$ are mutually independent and have zero-mean entries. Therefore, the expectation of the cross-product terms in the expansion of the norm squared is zero. For example, $\\mathbb{E}[x_t^T w_t] = 0$. The expectation of the squared norm is:\n$$\nJ(\\gamma) = \\frac{1}{(1 + \\gamma \\sigma_w^2)^2} \\left( \\mathbb{E}\\left[\\|\\gamma \\sigma_w^2 (\\rho - 1) x_t\\|_2^2\\right] + \\mathbb{E}\\left[\\|w_t\\|_2^2\\right] + \\mathbb{E}\\left[\\|\\gamma \\sigma_w^2 \\xi_t\\|_2^2\\right] \\right)\n$$\nWe are given that the entries of $x_t$, $w_t$, and $\\xi_t$ are i.i.d. with variances $\\sigma_x^2$, $\\sigma_w^2$, and $\\sigma_z^2$ respectively. The expected squared $\\ell_2$-norm of a random vector in $\\mathbb{R}^n$ with i.i.d. zero-mean entries of variance $\\sigma^2$ is $n \\sigma^2$. Thus:\n$\\mathbb{E}[\\|x_t\\|_2^2] = n \\sigma_x^2$, $\\mathbb{E}[\\|w_t\\|_2^2] = n \\sigma_w^2$, and $\\mathbb{E}[\\|\\xi_t\\|_2^2] = n \\sigma_z^2$.\nSubstituting these into the expression for $J(\\gamma)$:\n$$\nJ(\\gamma) = \\frac{1}{(1 + \\gamma \\sigma_w^2)^2} \\left( (\\gamma \\sigma_w^2 (\\rho - 1))^2 (n \\sigma_x^2) + n \\sigma_w^2 + (\\gamma \\sigma_w^2)^2 (n \\sigma_z^2) \\right)\n$$\n$$\nJ(\\gamma) = \\frac{n}{(1 + \\gamma \\sigma_w^2)^2} \\left( \\gamma^2 \\sigma_w^4 (\\rho - 1)^2 \\sigma_x^2 + \\sigma_w^2 + \\gamma^2 \\sigma_w^4 \\sigma_z^2 \\right)\n$$\n$$\nJ(\\gamma) = \\frac{n \\sigma_w^2 + n \\gamma^2 \\sigma_w^4 \\left( (\\rho - 1)^2 \\sigma_x^2 + \\sigma_z^2 \\right)}{(1 + \\gamma \\sigma_w^2)^2}\n$$\nTo find the optimal $\\gamma^\\star$ that minimizes this bound, we can minimize the function $f(\\gamma) = J(\\gamma)/n$, as $n$ is a positive constant.\n$$\nf(\\gamma) = \\frac{\\sigma_w^2 + \\gamma^2 \\sigma_w^4 \\left( (\\rho - 1)^2 \\sigma_x^2 + \\sigma_z^2 \\right)}{(1 + \\gamma \\sigma_w^2)^2}\n$$\nWe compute the derivative of $f(\\gamma)$ with respect to $\\gamma$ and set it to zero. Let $A = \\sigma_w^2$, $B = \\sigma_w^4 \\left( (\\rho - 1)^2 \\sigma_x^2 + \\sigma_z^2 \\right)$, and $C = \\sigma_w^2$. The function is $f(\\gamma) = \\frac{A + B\\gamma^2}{(1 + C\\gamma)^2}$.\nUsing the quotient rule:\n$$\n\\frac{df}{d\\gamma} = \\frac{(2B\\gamma)(1+C\\gamma)^2 - (A+B\\gamma^2)(2C(1+C\\gamma))}{(1+C\\gamma)^4}\n$$\nSetting the derivative to zero, we can simplify by canceling the term $2(1+C\\gamma)$ from the numerator (assuming $1+C\\gamma \\neq 0$, which is true for $\\gamma \\ge 0$ and $C=\\sigma_w^2  0$):\n$$\n(B\\gamma)(1+C\\gamma) - C(A+B\\gamma^2) = 0\n$$\n$$\nB\\gamma + BC\\gamma^2 - AC - BC\\gamma^2 = 0\n$$\n$$\nB\\gamma - AC = 0\n$$\n$$\n\\gamma = \\frac{AC}{B}\n$$\nSubstituting back the expressions for $A$, $B$, and $C$:\n$$\n\\gamma = \\frac{(\\sigma_w^2)(\\sigma_w^2)}{\\sigma_w^4 \\left( (\\rho - 1)^2 \\sigma_x^2 + \\sigma_z^2 \\right)}\n$$\nThe term $\\sigma_w^4$ cancels out, provided $\\sigma_w^2 \\neq 0$.\n$$\n\\gamma^\\star = \\frac{1}{(\\rho - 1)^2 \\sigma_x^2 + \\sigma_z^2}\n$$\nThis expression is the optimal co-regularization weight $\\gamma^\\star$. We must check if it satisfies $\\gamma \\ge 0$. The variances $\\sigma_x^2$ and $\\sigma_z^2$ are non-negative. The term $(\\rho-1)^2$ is also non-negative. Thus, the denominator is non-negative. Assuming the trivial case where both $\\sigma_z^2=0$ and either $\\rho=1$ or $\\sigma_x^2=0$ is excluded (which would make the denominator zero), $\\gamma^\\star$ is positive. The second derivative test would confirm this is a minimum. As this is the only critical point for $\\gamma \\ge 0$, it is the global minimum. The final answer is this expression for $\\gamma^\\star$.",
            "answer": "$$\n\\boxed{\\frac{1}{(\\rho - 1)^{2} \\sigma_x^{2} + \\sigma_z^{2}}}\n$$"
        }
    ]
}