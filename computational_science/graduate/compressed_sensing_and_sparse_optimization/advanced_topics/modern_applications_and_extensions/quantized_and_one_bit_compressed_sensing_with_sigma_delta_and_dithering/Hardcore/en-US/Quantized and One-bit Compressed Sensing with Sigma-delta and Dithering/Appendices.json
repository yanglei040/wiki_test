{
    "hands_on_practices": [
        {
            "introduction": "A central challenge in quantized compressed sensing is managing the error introduced by the quantization process. This exercise explores subtractive dithering, a remarkably effective technique that transforms complex, signal-dependent quantization error into a simple, signal-independent additive noise source. By working from first principles, you will derive the statistical properties of this dithered quantization error and apply the result to determine a robust fidelity constraint for signal reconstruction, a foundational skill for designing and analyzing practical quantized systems .",
            "id": "3471421",
            "problem": "Consider a real-valued measurement model in compressed sensing where an unknown vector $x \\in \\mathbb{R}^{n}$ is observed through linear measurements $s = A x \\in \\mathbb{R}^{m}$, followed by uniform scalar quantization with subtractive dithering. Let the quantizer be a mid-rise uniform quantizer with step size $\\Delta = 10^{-3}$, and let $m = 10^{4}$ measurements be acquired. The subtractive dithering scheme adds an independent dither $d_{i} \\sim \\operatorname{Uniform}(-\\Delta/2, \\Delta/2)$ to each pre-quantized measurement $s_{i}$ prior to quantization and subtracts the same dither at the decoder, so the reconstructed measurement is $y_{i} = Q(s_{i} + d_{i}) - d_{i}$, where $Q(\\cdot)$ denotes the uniform quantizer. Define the quantization error $e_{i} = y_{i} - s_{i}$, and the aggregate error vector $e \\in \\mathbb{R}^{m}$ with entries $e_{i}$.\n\nStarting only from the foundational definitions of uniform quantization, subtractive dithering, and properties of independent and identically distributed random variables, and without invoking any pre-stated shortcut formulas, determine:\n- the probability distribution of each $e_{i}$,\n- the variance of each $e_{i}$,\n- and, using the tightest deterministic bound implied by the error model, set the smallest fidelity radius $R$ such that the $\\ell_{2}$-constrained decoding feasibility set $\\{z \\in \\mathbb{R}^{n} : \\|A z - y\\|_{2} \\leq R\\}$ is guaranteed to contain the true measurement vector $s$.\n\nExpress your final answer as the numerical value of $R$, rounded to four significant figures. No physical units are involved. Angles are not present in this problem.",
            "solution": "The problem requires an analysis of a uniform scalar quantization scheme that incorporates subtractive dithering. We are asked to determine the probability distribution and variance of the quantization error for a single measurement, and then to find the tightest deterministic bound on the $\\ell_2$-norm of the total error vector, which defines the fidelity radius $R$.\n\nLet's begin by formalizing the components of the model. The unknown signal is $x \\in \\mathbb{R}^{n}$, and the pre-quantized measurements are $s = Ax \\in \\mathbb{R}^{m}$, where $s = [s_1, \\dots, s_m]^T$. The quantizer is a uniform mid-rise quantizer with step size $\\Delta$. This means a value $u$ is mapped to $Q(u) = k\\Delta$ if $u$ falls into the interval $[(k - 1/2)\\Delta, (k + 1/2)\\Delta)$ for some integer $k$.\n\nThe subtractive dithering scheme involves a dither signal $d \\in \\mathbb{R}^{m}$ where each component $d_i$ is an independent random variable drawn from a uniform distribution, $d_i \\sim \\operatorname{Uniform}(-\\Delta/2, \\Delta/2)$. The reconstructed measurement at the decoder is given by $y_i = Q(s_i + d_i) - d_i$. The quantization error for the $i$-th measurement is defined as $e_i = y_i - s_i$.\n\nSubstituting the expression for $y_i$, we can write the error as:\n$$e_i = (Q(s_i + d_i) - d_i) - s_i = Q(s_i + d_i) - (s_i + d_i)$$\nLet's define a new variable $v_i = s_i + d_i$. The error is then $e_i = Q(v_i) - v_i$. This is the classic form of quantization error for the dithered signal $v_i$. Our first task is to find the probability distribution of this error $e_i$.\n\n**Part 1: Probability Distribution of the Error $e_i$**\n\nThe value $s_i$ is a fixed, deterministic quantity for a given signal $x$. The randomness in $e_i$ comes from the dither $d_i$. Since $d_i \\sim \\operatorname{Uniform}(-\\Delta/2, \\Delta/2)$, the dithered signal $v_i = s_i + d_i$ is a random variable following a uniform distribution on the interval $[s_i - \\Delta/2, s_i + \\Delta/2]$. The length of this interval is $\\Delta$.\n$$v_i \\sim \\operatorname{Uniform}(s_i - \\Delta/2, s_i + \\Delta/2)$$\nThe error $e_i$ is a function of the random variable $v_i$, specifically $e_i = g(v_i)$ where $g(u) = Q(u) - u$. This function $g(u)$ is a sawtooth wave with period $\\Delta$. For any $u$ in a quantization interval $[(k - 1/2)\\Delta, (k + 1/2)\\Delta)$, we have $g(u) = k\\Delta - u$. The range of $g(u)$ over this interval is $(-\\Delta/2, \\Delta/2]$.\n\nWe need to find the distribution of $e_i = g(v_i)$, where $v_i$ is uniformly distributed over an interval of length $\\Delta$. Let this interval be $I = [s_i - \\Delta/2, s_i + \\Delta/2]$. Let's find the cumulative distribution function (CDF) of $e_i$, denoted by $F_{e_i}(\\epsilon) = P(e_i \\le \\epsilon)$, for $\\epsilon \\in (-\\Delta/2, \\Delta/2]$.\n\n$$F_{e_i}(\\epsilon) = P(g(v_i) \\le \\epsilon) = \\int_{s_i - \\Delta/2}^{s_i + \\Delta/2} \\mathbf{1}_{g(u) \\le \\epsilon} \\frac{1}{\\Delta} du = \\frac{1}{\\Delta} \\text{Length}(\\{u \\in I : g(u) \\le \\epsilon\\})$$\nwhere $\\mathbf{1}_{\\{\\cdot\\}}$ is the indicator function. The function $g(u)$ is periodic with period $\\Delta$, and the integration is over an interval of exactly this length. For any interval of length $\\Delta$, the measure of the set $\\{u: g(u) \\le \\epsilon\\}$ is the same. Let's analyze this measure over one period, for instance, for $u \\in [-\\Delta/2, \\Delta/2)$. In this interval, $Q(u)=0$, so $g(u)=-u$. The condition $g(u) \\le \\epsilon$ becomes $-u \\le \\epsilon$, or $u \\ge -\\epsilon$. The set of points satisfying this in $[-\\Delta/2, \\Delta/2)$ is $[-\\epsilon, \\Delta/2)$. The length of this set is $\\Delta/2 - (-\\epsilon) = \\Delta/2 + \\epsilon$.\n\nThis holds regardless of the specific interval of length $\\Delta$ over which $v_i$ is distributed. Therefore, for any $s_i$, the length of the set $\\{u \\in I : g(u) \\le \\epsilon\\}$ is $\\Delta/2 + \\epsilon$.\nThe CDF of $e_i$ is:\n$$F_{e_i}(\\epsilon) = \\frac{1}{\\Delta}(\\Delta/2 + \\epsilon) = \\frac{1}{2} + \\frac{\\epsilon}{\\Delta}, \\quad \\text{for } \\epsilon \\in (-\\Delta/2, \\Delta/2]$$\nThe probability density function (PDF) is the derivative of the CDF with respect to $\\epsilon$:\n$$f_{e_i}(\\epsilon) = \\frac{d}{d\\epsilon} F_{e_i}(\\epsilon) = \\frac{1}{\\Delta}$$\nThis is the PDF of a uniform distribution. Thus, the error $e_i$ is uniformly distributed over the interval $(-\\Delta/2, \\Delta/2]$.\n$$e_i \\sim \\operatorname{Uniform}(-\\Delta/2, \\Delta/2)$$\nA key result of subtractive dithering is that the quantization error becomes independent of the original signal $s_i$. Since the dither components $d_i$ are independent, the error components $e_i$ are also independent and identically distributed (i.i.d.).\n\n**Part 2: Variance of the Error $e_i$**\n\nNow we calculate the variance of $e_i$. For a random variable $X \\sim \\operatorname{Uniform}(a,b)$, the mean is $E[X] = (a+b)/2$ and the variance is $\\operatorname{Var}(X) = (b-a)^2/12$. Here, our interval is $(-\\Delta/2, \\Delta/2]$. We derive this from first principles as requested.\nThe expected value (mean) of $e_i$ is:\n$$E[e_i] = \\int_{-\\Delta/2}^{\\Delta/2} \\epsilon \\cdot f_{e_i}(\\epsilon) d\\epsilon = \\int_{-\\Delta/2}^{\\Delta/2} \\epsilon \\frac{1}{\\Delta} d\\epsilon = \\frac{1}{\\Delta} \\left[ \\frac{\\epsilon^2}{2} \\right]_{-\\Delta/2}^{\\Delta/2} = \\frac{1}{2\\Delta} \\left( \\left(\\frac{\\Delta}{2}\\right)^2 - \\left(-\\frac{\\Delta}{2}\\right)^2 \\right) = 0$$\nThe variance is $\\operatorname{Var}(e_i) = E[e_i^2] - (E[e_i])^2 = E[e_i^2]$.\n$$E[e_i^2] = \\int_{-\\Delta/2}^{\\Delta/2} \\epsilon^2 \\cdot f_{e_i}(\\epsilon) d\\epsilon = \\int_{-\\Delta/2}^{\\Delta/2} \\epsilon^2 \\frac{1}{\\Delta} d\\epsilon = \\frac{1}{\\Delta} \\left[ \\frac{\\epsilon^3}{3} \\right]_{-\\Delta/2}^{\\Delta/2}$$\n$$E[e_i^2] = \\frac{1}{3\\Delta} \\left( \\left(\\frac{\\Delta}{2}\\right)^3 - \\left(-\\frac{\\Delta}{2}\\right)^3 \\right) = \\frac{1}{3\\Delta} \\left( \\frac{\\Delta^3}{8} - \\left(-\\frac{\\Delta^3}{8}\\right) \\right) = \\frac{1}{3\\Delta} \\left( 2 \\frac{\\Delta^3}{8} \\right) = \\frac{2\\Delta^3}{24\\Delta} = \\frac{\\Delta^2}{12}$$\nSo, the variance of each error component is $\\operatorname{Var}(e_i) = \\Delta^2/12$.\n\n**Part 3: Fidelity Radius $R$**\n\nThe problem asks for the smallest fidelity radius $R$ such that the set $\\{z \\in \\mathbb{R}^{n} : \\|A z - y\\|_{2} \\leq R\\}$ is **guaranteed** to contain the true measurement vector $s = Ax$. This means the condition $\\|s - y\\|_2 \\leq R$ must hold.\nThe vector $s-y$ is equal to $-e$, where $e = [e_1, \\dots, e_m]^T$ is the aggregate error vector. Thus, the condition is $\\| -e \\|_2 \\leq R$, which is equivalent to $\\|e\\|_2 \\leq R$.\n\nThe word \"guaranteed\" implies that the inequality must hold with probability $1$, for any realization of the dither vector $d$. We are therefore looking for a deterministic upper bound on the $\\ell_2$-norm of the error vector $e$.\n$$\\|e\\|_2 = \\sqrt{\\sum_{i=1}^{m} e_i^2}$$\nFrom Part 1, we established that each error component $e_i$ is a random variable whose values are always contained within the interval $(-\\Delta/2, \\Delta/2]$. This implies a strict, deterministic bound on the magnitude of each error component:\n$$|e_i| \\le \\frac{\\Delta}{2} \\quad \\text{for all } i=1, \\dots, m$$\nUsing this bound, we can find an upper bound for the squared $\\ell_2$-norm of the error vector:\n$$\\|e\\|_2^2 = \\sum_{i=1}^{m} e_i^2 \\le \\sum_{i=1}^{m} \\left(\\frac{\\Delta}{2}\\right)^2 = m \\left(\\frac{\\Delta}{2}\\right)^2 = \\frac{m\\Delta^2}{4}$$\nTaking the square root gives the deterministic upper bound on the $\\ell_2$-norm:\n$$\\|e\\|_2 \\le \\sqrt{\\frac{m\\Delta^2}{4}} = \\frac{\\Delta\\sqrt{m}}{2}$$\nThis bound is the tightest possible deterministic bound because it's possible to construct a dither sequence such that each $|e_i|$ is arbitrarily close to $\\Delta/2$. For example, if for each $i$, we choose $s_i$ and a realization of $d_i$ such that $s_i+d_i$ approaches a quantization boundary $(k-1/2)\\Delta$ from above, then $e_i = Q(s_i+d_i)-(s_i+d_i)$ will approach $\\Delta/2$. Consequently, the supremum of $\\|e\\|_2$ over all possible outcomes is indeed $\\frac{\\Delta\\sqrt{m}}{2}$.\n\nTherefore, the smallest radius $R$ that guarantees $\\|s - y\\|_2 \\leq R$ is $R = \\frac{\\Delta\\sqrt{m}}{2}$.\n\nWe are given $\\Delta = 10^{-3}$ and $m = 10^4$. Substituting these values:\n$$R = \\frac{10^{-3} \\sqrt{10^4}}{2} = \\frac{10^{-3} \\times 10^2}{2} = \\frac{10^{-1}}{2} = 0.05$$\nThe problem asks for this value rounded to four significant figures. This is $0.05000$, which in standard scientific notation is $5.000 \\times 10^{-2}$.",
            "answer": "$$\n\\boxed{5.000 \\times 10^{-2}}\n$$"
        },
        {
            "introduction": "Moving to the extreme of one-bit quantization, where measurements are reduced to a single sign bit, introduces unique fundamental challenges not present in multi-bit systems. The odd symmetry of the sign function creates an inherent ambiguity between a signal $x$ and its negative counterpart $-x$, making unique recovery of the signal's direction impossible in the simplest models. This practice will sharpen your conceptual understanding by guiding you to formally identify this symmetry and then critically evaluate which modifications to the measurement process, such as adding thresholds or dither, successfully break it .",
            "id": "3471448",
            "problem": "Consider the one-bit compressed sensing model with an unknown $k$-sparse vector $x \\in \\mathbb{R}^{n}$, a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ with rows $a_{i}^{\\top}$, and a sign comparator. Define the sign quantizer $Q:\\mathbb{R}\\to\\{-1,+1\\}$ by $Q(t)=\\operatorname{sign}(t)$, with the convention that $\\operatorname{sign}(t)$ is odd for all $t \\neq 0$ and ties occur on a set of measure zero under continuous models. Assume first that all thresholds are zero, so the measurement model reads\n$$\ny_{i} \\;=\\; \\operatorname{sign}\\!\\big(\\langle a_{i},x\\rangle\\big), \\quad i\\in\\{1,\\dots,m\\}.\n$$\nIn many practical comparators there is no absolute polarity reference, i.e., a global bit flip $g\\in\\{-1,+1\\}$ may be present and unknown to the decoder, so the observed bits satisfy\n$$\ny \\;=\\; g \\cdot \\operatorname{sign}(A x),\n$$\nwith $g$ unknown to the decoder. Treat the “direction” of $x$ as the equivalence class of all positive rescalings of $x$ on the unit sphere, i.e., $\\mathrm{dir}(x)= \\{ \\alpha x : \\alpha0\\}$, and suppose the decoder’s goal is to recover the unique oriented direction $x/\\|x\\|_{2}$ rather than the undirected line $\\{\\pm x\\}$.\n\nStarting only from the definitions above and the oddness of the sign function, argue first whether, when all thresholds are zero, the one-bit model is fundamentally unable to distinguish $x$ from $-x$ in the sense that $(x,g)$ and $(-x,-g)$ produce identical observations. Then evaluate which minimal changes to the measurement model break this origin-centered symmetry and enable unique direction recovery (i.e., allow a decoder to choose between $x/\\|x\\|_{2}$ and $-x/\\|x\\|_{2}$) with probability one under generic random sensing. Consider the following proposed modifications, where “known” means known to the decoder:\n\nA. Add known independent dithers $\\tau_{i}$ to the comparator thresholds with $\\tau_{i}\\overset{\\text{i.i.d.}}{\\sim} \\nu$ for a continuous distribution $\\nu$ of nonzero variance, and measure\n$$\ny_{i}\\;=\\; \\operatorname{sign}\\!\\big(\\langle a_{i},x\\rangle - \\tau_{i}\\big), \\quad i\\in\\{1,\\dots,m\\},\n$$\nwith the quantizer polarity still potentially flipped by an unknown global $g\\in\\{-1,+1\\}$.\n\nB. Replace the static quantizer with a first-order one-bit Sigma-Delta ($\\Sigma\\Delta$) loop with zero thresholds and no dither, namely\n$$\nq_{i}\\;=\\;\\operatorname{sign}\\!\\big(u_{i-1} + \\langle a_{i}, x\\rangle\\big), \\quad u_{i}\\;=\\; u_{i-1} + \\langle a_{i}, x\\rangle - q_{i}, \\quad u_{0}\\;=\\;0,\n$$\nand observe $y=q$ (up to an unknown global polarity $g$ as above).\n\nC. Multiply the sensing matrix by a known diagonal sign matrix $D=\\operatorname{diag}(d_{1},\\dots,d_{m})$ with $d_{i}\\in\\{-1,+1\\}$, and keep zero thresholds:\n$$\ny \\;=\\; \\operatorname{sign}(D A x),\n$$\nwith unknown global polarity $g$ as above.\n\nD. Keep zero thresholds for $i\\in\\{2,\\dots,m\\}$, but introduce a single known nonzero threshold $\\tau_{1}\\neq 0$ on one measurement:\n$$\ny_{1} \\;=\\; \\operatorname{sign}\\!\\big(\\langle a_{1},x\\rangle - \\tau_{1}\\big), \\quad\ny_{i} \\;=\\; \\operatorname{sign}\\!\\big(\\langle a_{i},x\\rangle\\big) \\text{ for } i\\geq 2,\n$$\nagain allowing an unknown global polarity $g$ on all bits.\n\nE. Add independent dithers $\\tau_{i}$ drawn i.i.d. from a continuous, symmetric (even) distribution, but do not reveal the $\\tau_{i}$ to the decoder:\n$$\ny_{i}\\;=\\; \\operatorname{sign}\\!\\big(\\langle a_{i},x\\rangle - \\tau_{i}\\big), \\quad i\\in\\{1,\\dots,m\\},\n$$\nwith an unknown global polarity $g$.\n\nWhich of the options A–E correctly break the $x\\mapsto -x$ symmetry and allow unique direction recovery (choose all that apply)? Justify your selection starting from the definitions and the oddness property, and make clear why the other options do or do not break the symmetry. Assume $A$ has independent and identically distributed sub-Gaussian rows in $\\mathbb{R}^{n}$ and that any continuous random choices (e.g., dithers) are in general position with probability one. Do not invoke any specialized theorems beyond the core definitions and properties stated here.",
            "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It presents a standard, non-trivial question within the field of one-bit compressed sensing.\n\nThe core of the problem is an ambiguity between a signal vector $x$ and its negative $-x$ in the one-bit compressed sensing model with an unknown global sign flip $g \\in \\{-1, +1\\}$. We are asked to first confirm this ambiguity in the basic model and then determine which of the proposed modifications resolve it.\n\nFirst, let us analyze the fundamental symmetry of the base model:\n$$\ny \\;=\\; g \\cdot \\operatorname{sign}(A x)\n$$\nwhere $y, \\operatorname{sign}(Ax) \\in \\{-1, +1\\}^m$, $x \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{m \\times n}$, and $g \\in \\{-1, +1\\}$ is an unknown scalar. The quantizer is the sign function, $Q(t) = \\operatorname{sign}(t)$, which is defined to be an odd function, meaning $\\operatorname{sign}(-t) = -\\operatorname{sign}(t)$ for $t \\neq 0$. We assume that for a random sensing matrix $A$ and a non-zero vector $x$, the components of $Ax$, namely $\\langle a_i, x \\rangle$, are non-zero with probability $1$.\n\nLet's consider the observations generated by the state $(x, g)$. The measurement vector is $y^{(1)} = g \\cdot \\operatorname{sign}(Ax)$.\nNow, let's consider the observations generated by the state $(-x, -g)$. The measurement vector is $y^{(2)} = (-g) \\cdot \\operatorname{sign}(A(-x))$.\nUsing the linearity of the inner product and the oddness of the sign function, we can simplify the expression for $y^{(2)}$:\n$$\n\\operatorname{sign}(A(-x))_i = \\operatorname{sign}(\\langle a_i, -x \\rangle) = \\operatorname{sign}(-\\langle a_i, x \\rangle) = -\\operatorname{sign}(\\langle a_i, x \\rangle)\n$$\nThis holds for each component $i \\in \\{1, \\dots, m\\}$. In vector notation, $\\operatorname{sign}(A(-x)) = -\\operatorname{sign}(Ax)$.\nSubstituting this into the expression for $y^{(2)}$:\n$$\ny^{(2)} = (-g) \\cdot (-\\operatorname{sign}(Ax)) = g \\cdot \\operatorname{sign}(Ax) = y^{(1)}\n$$\nThe measurement vectors are identical. Since both the signal $x$ and the global sign $g$ are unknown, a decoder that observes $y$ cannot distinguish between the state $(x, g)$ and the state $(-x, -g)$. This confirms that the one-bit model with zero thresholds is fundamentally unable to distinguish $x$ from $-x$. The goal is to find modifications that break this symmetry, meaning the measurements produced by $(x, g)$ and $(-x, -g)$ are no longer identical in general. This allows a decoder to distinguish $x/\\|x\\|_2$ from $-x/\\|x\\|_2$.\n\nNow, we evaluate each option.\n\nA. Add known independent dithers $\\tau_{i}$ with $\\tau_{i}\\overset{\\text{i.i.d.}}{\\sim} \\nu$ for a continuous distribution $\\nu$ of nonzero variance.\nThe model is $y_i = g \\cdot \\operatorname{sign}(\\langle a_i, x \\rangle - \\tau_i)$.\nThe observation for state $(x, g)$ is $y^{(1)}_i = g \\cdot \\operatorname{sign}(\\langle a_i, x \\rangle - \\tau_i)$.\nThe observation for state $(-x, -g)$ is $y^{(2)}_i = (-g) \\cdot \\operatorname{sign}(\\langle a_i, -x \\rangle - \\tau_i)$.\nLet's simplify $y^{(2)}_i$:\n$$\ny^{(2)}_i = (-g) \\cdot \\operatorname{sign}(-\\langle a_i, x \\rangle - \\tau_i) = (-g) \\cdot [-\\operatorname{sign}(\\langle a_i, x \\rangle + \\tau_i)] = g \\cdot \\operatorname{sign}(\\langle a_i, x \\rangle + \\tau_i)\n$$\nFor the symmetry to be broken, we need $y^{(1)} \\neq y^{(2)}$ in general. This requires that for at least one index $i$, $\\operatorname{sign}(\\langle a_i, x \\rangle - \\tau_i) \\neq \\operatorname{sign}(\\langle a_i, x \\rangle + \\tau_i)$.\nLet $z_i = \\langle a_i, x \\rangle$. The inequality becomes $\\operatorname{sign}(z_i - \\tau_i) \\neq \\operatorname{sign}(z_i + \\tau_i)$. This occurs if and only if $z_i - \\tau_i$ and $z_i + \\tau_i$ have opposite signs, which means $z_i$ must lie strictly between $-\\tau_i$ and $\\tau_i$.\nSince $\\tau_i$ are drawn from a continuous distribution with non-zero variance, there is a probability greater than zero that $\\tau_i \\neq 0$. The quantities $\\langle a_i, x \\rangle$ are also random variables with a continuous distribution (since the rows $a_i$ are). Thus, for any given $i$, there is a positive probability that $|\\langle a_i, x \\rangle|  |\\tau_i|$. With $m$ independent measurements, it is highly probable that this will occur for at least one $i$. When it occurs, the symmetry of the observations is broken. The function $z \\mapsto \\operatorname{sign}(z - \\tau_i)$ is not an odd function for $\\tau_i \\neq 0$. Since the $\\tau_i$ are known to the decoder, this asymmetry is exploitable.\nVerdict: **Correct**.\n\nB. Replace the static quantizer with a first-order Sigma-Delta loop.\nThe model is $q_{i} = \\operatorname{sign}(u_{i-1} + \\langle a_{i}, x\\rangle)$ and $u_{i} = u_{i-1} + \\langle a_{i}, x\\rangle - q_{i}$, with $u_0 = 0$. The observation is $y = g \\cdot q$.\nLet's examine the behavior for input $-x$. Let $q_i(x)$ and $u_i(x)$ be the sequences for input $x$, and $q_i(-x)$ and $u_i(-x)$ for input $-x$. We proceed by induction.\nBase case ($i=1$): $u_0(-x) = u_0(x) = 0$.\n$q_1(-x) = \\operatorname{sign}(u_0(-x) + \\langle a_1, -x \\rangle) = \\operatorname{sign}(-\\langle a_1, x \\rangle) = -\\operatorname{sign}(\\langle a_1, x \\rangle) = -q_1(x)$.\n$u_1(-x) = u_0(-x) + \\langle a_1, -x \\rangle - q_1(-x) = 0 - \\langle a_1, x \\rangle - (-q_1(x)) = -(\\langle a_1, x \\rangle - q_1(x)) = -u_1(x)$.\nInductive step: Assume $q_{i-1}(-x) = -q_{i-1}(x)$ and $u_{i-1}(-x) = -u_{i-1}(x)$.\n$q_i(-x) = \\operatorname{sign}(u_{i-1}(-x) + \\langle a_i, -x \\rangle) = \\operatorname{sign}(-u_{i-1}(x) - \\langle a_i, x \\rangle) = -\\operatorname{sign}(u_{i-1}(x) + \\langle a_i, x \\rangle) = -q_i(x)$.\n$u_i(-x) = u_{i-1}(-x) + \\langle a_i, -x \\rangle - q_i(-x) = -u_{i-1}(x) - \\langle a_i, x \\rangle - (-q_i(x)) = -(u_{i-1}(x) + \\langle a_i, x \\rangle - q_i(x)) = -u_i(x)$.\nThe induction holds. The output sequence of the quantizer is odd with respect to the input signal $x$: $q(-x) = -q(x)$.\nNow check the overall observation:\nFor $(x,g)$: $y^{(1)} = g \\cdot q(x)$.\nFor $(-x,-g)$: $y^{(2)} = (-g) \\cdot q(-x) = (-g) \\cdot (-q(x)) = g \\cdot q(x) = y^{(1)}$.\nThe observations are identical. The symmetry is not broken.\nVerdict: **Incorrect**.\n\nC. Multiply the sensing matrix by a known diagonal sign matrix $D$.\nThe model is $y = g \\cdot \\operatorname{sign}(D A x)$. The $i$-th measurement is $y_i = g \\cdot \\operatorname{sign}(d_i \\langle a_i, x \\rangle)$.\nThe function $f_i(z) = \\operatorname{sign}(d_i z)$ is odd regardless of whether $d_i = +1$ or $d_i = -1$.\n$f_i(-z) = \\operatorname{sign}(d_i (-z)) = \\operatorname{sign}(-d_i z) = -\\operatorname{sign}(d_i z) = -f_i(z)$.\nSince each component of the measurement function $\\operatorname{sign}(DAx)$ is odd with respect to $x$, the entire vector function is odd. That is, $\\operatorname{sign}(DA(-x)) = -\\operatorname{sign}(DAx)$.\nThe observation for state $(x, g)$ is $y^{(1)} = g \\cdot \\operatorname{sign}(DAx)$.\nThe observation for state $(-x, -g)$ is $y^{(2)} = (-g) \\cdot \\operatorname{sign}(DA(-x)) = (-g) \\cdot (-\\operatorname{sign}(DAx)) = g \\cdot \\operatorname{sign}(DAx) = y^{(1)}$.\nThe observations are identical, and the symmetry is not broken.\nVerdict: **Incorrect**.\n\nD. Introduce a single known nonzero threshold $\\tau_{1}\\neq 0$.\nThe model is $y_1 = g \\cdot \\operatorname{sign}(\\langle a_1, x\\rangle - \\tau_1)$ and $y_i = g \\cdot \\operatorname{sign}(\\langle a_i,x\\rangle)$ for $i \\geq 2$.\nObservation for $(x, g)$: $y^{(1)}_1 = g \\cdot \\operatorname{sign}(\\langle a_1, x\\rangle - \\tau_1)$, and for $i \\ge 2$, $y^{(1)}_i = g \\cdot \\operatorname{sign}(\\langle a_i, x\\rangle)$.\nObservation for $(-x, -g)$: $y^{(2)}_1 = (-g) \\cdot \\operatorname{sign}(\\langle a_1, -x\\rangle - \\tau_1) = g \\cdot \\operatorname{sign}(\\langle a_1, x\\rangle + \\tau_1)$. For $i \\ge 2$, $y^{(2)}_i = (-g) \\cdot \\operatorname{sign}(\\langle a_i, -x\\rangle) = g \\cdot \\operatorname{sign}(\\langle a_i, x\\rangle)$.\nComparing the two vectors, $y^{(1)}_i = y^{(2)}_i$ for all $i \\ge 2$.\nThe vectors are identical if and only if $y^{(1)}_1 = y^{(2)}_1$, which requires $\\operatorname{sign}(\\langle a_1, x\\rangle - \\tau_1) = \\operatorname{sign}(\\langle a_1, x\\rangle + \\tau_1)$.\nAs in case A, this equality fails if $|\\langle a_1, x\\rangle|  |\\tau_1|$. Since $\\tau_1 \\neq 0$ and $\\langle a_1, x\\rangle$ is a continuous random variable, this symmetry-breaking event occurs with positive probability. When it occurs, $y^{(1)} \\neq y^{(2)}$, and the ambiguity is resolved. The presence of even a single known non-zero threshold breaks the structural symmetry of the overall measurement system.\nVerdict: **Correct**.\n\nE. Add independent but unknown dithers $\\tau_{i}$ from a symmetric distribution.\nThe model is $y_i = g \\cdot \\operatorname{sign}(\\langle a_i, x\\rangle - \\tau_i)$, but the $\\tau_i$ are unknown to the decoder. The decoder only knows that the $\\tau_i$ are i.i.d. draws from a continuous, symmetric distribution $\\nu$.\nSince the decoder does not know the $\\tau_i$, it must rely on statistical properties. Let's analyze the probability of an observation.\nLet $z_i = \\langle a_i, x \\rangle$. The probability of getting $y_i = +1$ for a given $g$ is $P(\\operatorname{sign}(z_i - \\tau_i) = g) = P(g(z_i - \\tau_i)  0)$.\nLet $F_\\nu$ be the CDF of the dither distribution $\\nu$.\nIf $g = +1$, $P(y_i=1) = P(\\tau_i  z_i) = F_\\nu(z_i)$.\nIf $g = -1$, $P(y_i=1) = P(-(z_i - \\tau_i)0) = P(\\tau_i  z_i) = 1 - F_\\nu(z_i)$.\nThe distribution $\\nu$ is symmetric, which means its PDF is even and its CDF satisfies $F_\\nu(-t) = 1 - F_\\nu(t)$.\nThe decoder compares the likelihood of the observed data $y$ under hypothesis $H_x: (x,g)$ vs. $H_{-x}: (-x,-g)$. The likelihood contribution from the $i$-th measurement for hypothesis $(x,g)$ depends on quantities like $F_\\nu(\\langle a_i, x \\rangle)$.\nThe likelihood a for hypothesis $(-x,-g)$ will depend on quantities like $F_\\nu(\\langle a_i, -x \\rangle)$.\nLet's consider the average measurement function, $E[ \\operatorname{sign}(z-\\tau) ] = (+1)P(\\tau  z) + (-1)P(\\tau  z) = F_\\nu(z) - (1-F_\\nu(z)) = 2F_\\nu(z)-1$. Let this be $\\bar{f}(z)$. For a symmetric dither, $\\bar{f}(-z) = 2F_\\nu(-z)-1 = 2(1-F_\\nu(z))-1 = 1-2F_\\nu(z) = -\\bar{f}(z)$. The average response function is odd.\nBecause of this restored odd symmetry in the statistical properties, the evidence for $(x,g)$ is identical to the evidence for $(-x,-g)$, as shown in the validation section. The decoder cannot distinguish the two cases.\nVerdict: **Incorrect**.\n\nIn summary, options A and D introduce a known, fixed offset into the quantization process. This breaks the origin-centered symmetry of the sign function and makes the overall measurement mapping non-odd, which in turn allows for the disambiguation of $x$ and $-x$.",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "While the previous exercise demonstrates how to recover a signal's direction, its amplitude or scale seems irretrievably lost in one-bit sensing. This advanced practice introduces a powerful and elegant method to overcome this limitation by using randomized thresholds, effectively turning a simple comparator into an amplitude estimation tool. By deriving an estimator based on the layer-cake representation and implementing it via a Monte Carlo simulation, you will see how carefully structured randomness can be used to extract surprisingly rich information from seemingly coarse binary data .",
            "id": "3471430",
            "problem": "Consider a measurement model in which a signal vector $x \\in \\mathbb{R}^n$ is observed only through one-bit thresholded linear measurements with randomized thresholds and dithering. Let $A \\in \\mathbb{R}^{m \\times n}$ be a known sensing matrix, and define $z = A x \\in \\mathbb{R}^m$. For any scalar threshold $t \\in \\mathbb{R}$, the one-bit measurement is the elementwise sign\n$$\ny^{(t)} = \\operatorname{sign}\\!\\left( A x - t \\mathbf{1} \\right) \\in \\{ -1, +1 \\}^m,\n$$\nwhere $\\operatorname{sign}(\\cdot)$ is applied elementwise and $\\mathbf{1} \\in \\mathbb{R}^m$ is the all-ones vector.\n\nYou are asked to design an estimator for the amplitude of $x$ by aggregating randomized threshold queries. Throughout, use subtractive dithering to the thresholds: for a sequence of thresholds $\\{ t_j \\}_{j=1}^K$ drawn independently from the uniform distribution on $[0, T]$, add independent dither $\\{ d_j \\}_{j=1}^K$ with $d_j \\sim \\operatorname{Uniform}(-\\Delta/2, \\Delta/2)$, and query two measurement streams:\n$$\ny^{(+)}_j = \\operatorname{sign}\\!\\left( A x - (t_j + d_j)\\mathbf{1} \\right), \\quad\ny^{(-)}_j = \\operatorname{sign}\\!\\left( A x + (t_j + d_j)\\mathbf{1} \\right),\n$$\nfor $j = 1, \\dots, K$. The subtractive dithering is used to de-bias the thresholding around $t_j$; this is a standard technique in quantization theory.\n\nStarting from first principles, derive an estimator for the quantity $\\| A x \\|_1$ that uses the aggregation over $j$ of the number of sign flips (equivalently, the counts of positive and negative signs) in the streams $y^{(+)}_j$ and $y^{(-)}_j$. Then, derive an estimator for the scale $\\alpha$ of $x$ under the model $x = \\alpha x_{\\mathrm{dir}}$ with $\\alpha \\ge 0$ and a known direction $x_{\\mathrm{dir}}$ (for example, obtained by a separate direction estimation step from one-bit compressed sensing with zero thresholds). Use the linearity $\\| A (\\alpha x_{\\mathrm{dir}}) \\|_1 = \\alpha \\| A x_{\\mathrm{dir}} \\|_1$ to relate the amplitude $\\alpha$ to $\\| A x \\|_1$.\n\nUse the following fundamental base:\n- The definition of one-bit quantization via the sign operator.\n- The layer-cake representation for nonnegative functions, applied discretely: for $u \\in \\mathbb{R}$, the identity $u_+ = \\int_0^\\infty \\mathbf{1}\\{ u  t \\} \\, dt$, and similarly $(-u)_+ = \\int_0^\\infty \\mathbf{1}\\{ -u  t \\} \\, dt$, where $u_+ = \\max(u, 0)$.\n- Aggregation of independent and identically distributed (i.i.d.) samples to approximate integrals via Monte Carlo.\n\nYour program must implement the estimator and evaluate it on the following test suite. In all test cases, generate the sensing matrix $A$ with entries i.i.d. Gaussian $\\mathcal{N}(0, 1/m)$, and generate the $k$-sparse direction $x_{\\mathrm{dir}}$ by choosing a support of size $k$ uniformly at random, assigning i.i.d. Gaussian $\\mathcal{N}(0, 1)$ values on the support, and normalizing the result to unit $\\ell_2$-norm. Then set $x = \\alpha x_{\\mathrm{dir}}$. For each case, set $T$ to $\\eta \\cdot \\max_i |(A x)_i|$ with the specified $\\eta  1$ to avoid saturation, and set the dithering width $\\Delta$ to $T / 1000$. Use $K$ threshold queries.\n\nTest Suite:\n1. Happy path: $m = 64$, $n = 32$, $k = 6$, $\\alpha = 1.0$, $K = 4000$, $\\eta = 1.2$.\n2. Boundary small amplitude: $m = 64$, $n = 32$, $k = 6$, $\\alpha = 0.05$, $K = 4000$, $\\eta = 1.5$.\n3. Large amplitude edge case: $m = 64$, $n = 32$, $k = 6$, $\\alpha = 3.0$, $K = 6000$, $\\eta = 1.3$.\n\nYour estimator must:\n- Compute $\\| A x \\|_1$ from the one-bit streams using the aggregation of sign counts over the randomized thresholds with dithering.\n- Compute $\\alpha$ via $\\widehat{\\alpha} = \\widehat{\\| A x \\|_1} / \\| A x_{\\mathrm{dir}} \\|_1$.\n\nFor reproducibility, fix the random seed to $42$ for all randomness (matrix generation, support selection, dithering, and threshold generation). There are no physical units; all computations are dimensionless.\n\nFinal Output Format:\nYour program should produce a single line of output containing the estimated amplitudes for the three test cases as a comma-separated list enclosed in square brackets. The three results must be floats. For example, the output should look exactly like\n$$\n[\\widehat{\\alpha}_1,\\widehat{\\alpha}_2,\\widehat{\\alpha}_3].\n$$",
            "solution": "The user has provided a well-posed problem in the domain of one-bit compressed sensing. All parameters and conditions are clearly specified, and the problem is scientifically grounded in established signal processing principles. Therefore, a solution can be formulated.\n\n### Step 1: Derivation of the Estimator for $\\|Ax\\|_1$\n\nLet $z = Ax \\in \\mathbb{R}^m$. The quantity to be estimated is the $\\ell_1$-norm of $z$, defined as $\\|z\\|_1 = \\sum_{i=1}^m |z_i|$. For any scalar $u \\in \\mathbb{R}$, its absolute value can be written as the sum of its positive and negative parts, $|u| = u_+ + (-u)_+$, where $u_+ = \\max(u, 0)$ and $(-u)_+ = \\max(-u, 0)$. Applying this to each component of $z$ gives:\n$$\n\\|z\\|_1 = \\sum_{i=1}^m |z_i| = \\sum_{i=1}^m \\left( (z_i)_+ + (-z_i)_+ \\right)\n$$\n\nThe problem directs us to use the layer-cake representation for non-negative functions. For any $u \\in \\mathbb{R}$, this representation states:\n$$\nu_+ = \\int_0^\\infty \\mathbf{1}\\{u  t\\} \\, dt \\quad \\text{and} \\quad (-u)_+ = \\int_0^\\infty \\mathbf{1}\\{-u  t\\} \\, dt = \\int_0^\\infty \\mathbf{1}\\{u  -t\\} \\, dt\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nSubstituting these integral forms into the expression for $\\|z\\|_1$ and swapping the order of summation and integration, we obtain:\n$$\n\\|z\\|_1 = \\sum_{i=1}^m \\left( \\int_0^\\infty \\mathbf{1}\\{z_i  t\\} \\, dt + \\int_0^\\infty \\mathbf{1}\\{z_i  -t\\} \\, dt \\right)\n$$\n$$\n\\|z\\|_1 = \\int_0^\\infty \\left( \\sum_{i=1}^m \\mathbf{1}\\{z_i  t\\} + \\sum_{i=1}^m \\mathbf{1}\\{z_i  -t\\} \\right) dt\n$$\n\nThe problem specifies that the upper limit for the random thresholds, $T$, is chosen such that $T = \\eta \\cdot \\max_i |z_i|$ with $\\eta  1$. This ensures that for any $t  T$, we have $|z_i|  t$ for all $i=1, \\dots, m$. Consequently, for $t  T$, the integrand $\\sum_{i=1}^m \\mathbf{1}\\{z_i  t\\} + \\sum_{i=1}^m \\mathbf{1}\\{z_i  -t\\}$ is identically zero. The integral's upper limit can thus be changed from $\\infty$ to $T$ without altering its value:\n$$\n\\|z\\|_1 = \\int_0^T \\left( \\sum_{i=1}^m \\mathbf{1}\\{z_i  t\\} + \\sum_{i=1}^m \\mathbf{1}\\{z_i  -t\\} \\right) dt\n$$\n\nThis integral can be estimated using the Monte Carlo method. By drawing $K$ independent samples $\\{t_j\\}_{j=1}^K$ from the uniform distribution $\\operatorname{Uniform}(0, T)$, we can approximate the integral as:\n$$\n\\|z\\|_1 \\approx \\frac{T}{K} \\sum_{j=1}^K \\left( \\sum_{i=1}^m \\mathbf{1}\\{z_i  t_j\\} + \\sum_{i=1}^m \\mathbf{1}\\{z_i  -t_j\\} \\right)\n$$\n\nThe problem states that measurements are performed with dithered thresholds $\\tau_j = t_j + d_j$, where $t_j \\sim \\operatorname{Uniform}(0, T)$ and $d_j \\sim \\operatorname{Uniform}(-\\Delta/2, \\Delta/2)$. The available measurements are:\n$$\ny^{(+)}_j = \\operatorname{sign}(z - \\tau_j \\mathbf{1}), \\quad y^{(-)}_j = \\operatorname{sign}(z + \\tau_j \\mathbf{1})\n$$\nThe quantities inside the Monte Carlo summation must be related to these measurements. For each threshold $\\tau_j$, we can count the number of positive and negative signs.\nThe number of positive signs in $y^{(+)}_j$ is:\n$$\n\\sum_{i=1}^m \\mathbf{1}\\{(y^{(+)}_j)_i  0\\} = \\sum_{i=1}^m \\mathbf{1}\\{z_i - \\tau_j  0\\} = \\sum_{i=1}^m \\mathbf{1}\\{z_i  \\tau_j\\}\n$$\nThe number of negative signs in $y^{(-)}_j$ is:\n$$\n\\sum_{i=1}^m \\mathbf{1}\\{(y^{(-)}_j)_i  0\\} = \\sum_{i=1}^m \\mathbf{1}\\{z_i + \\tau_j  0\\} = \\sum_{i=1}^m \\mathbf{1}\\{z_i  -\\tau_j\\}\n$$\nThe dithering term $d_j$ slightly perturbs the threshold. For a sufficiently small dither width $\\Delta$, the dithered threshold $\\tau_j = t_j + d_j$ serves as a close proxy for the undithered threshold $t_j$. The randomness introduced by the dither is averaged out over the $K$ samples, and its primary purpose is to de-bias the quantization, leading to a more stable estimator in practice. We thus formulate our estimator by replacing the ideal indicator functions at $t_j$ with the observable ones at $\\tau_j$:\n$$\n\\widehat{\\|z\\|_1} = \\frac{T}{K} \\sum_{j=1}^K \\left( \\sum_{i=1}^m \\mathbf{1}\\{z_i  \\tau_j\\} + \\sum_{i=1}^m \\mathbf{1}\\{z_i  -\\tau_j\\} \\right)\n$$\nThis is the final form of the estimator for $\\|Ax\\|_1$.\n\n### Step 2: Derivation of the Amplitude Estimator $\\widehat{\\alpha}$\n\nThe signal is modeled as $x = \\alpha x_{\\mathrm{dir}}$, with $\\alpha \\ge 0$ being the unknown amplitude and $x_{\\mathrm{dir}}$ a known direction vector. The sensing process is linear, so $Ax = A(\\alpha x_{\\mathrm{dir}}) = \\alpha (A x_{\\mathrm{dir}})$. The $\\ell_1$-norm has the property of positive homogeneity, $\\|c v\\|_1 = |c| \\|v\\|_1$. Since $\\alpha \\ge 0$, we have:\n$$\n\\|Ax\\|_1 = \\|\\alpha (A x_{\\mathrm{dir}})\\|_1 = \\alpha \\|A x_{\\mathrm{dir}}\\|_1\n$$\nBy rearranging this equation, we can express $\\alpha$ in terms of $\\|Ax\\|_1$ and $\\|A x_{\\mathrm{dir}}\\|_1$:\n$$\n\\alpha = \\frac{\\|Ax\\|_1}{\\|A x_{\\mathrm{dir}}\\|_1}\n$$\nTo estimate $\\alpha$, we substitute our estimator $\\widehat{\\|Ax\\|_1}$ for the true norm in the numerator. Since $A$ and $x_{\\mathrm{dir}}$ are known, the denominator $\\|A x_{\\mathrm{dir}}\\|_1$ can be calculated exactly. This gives the estimator for the amplitude:\n$$\n\\widehat{\\alpha} = \\frac{\\widehat{\\|Ax\\|_1}}{\\|A x_{\\mathrm{dir}}\\|_1}\n$$\n\n### Summary of the Algorithm\n\nThe implementation will follow these steps for each test case:\n1.  Set the random seed to $42$ for reproducibility.\n2.  Generate the sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ with i.i.d. entries from $\\mathcal{N}(0, 1/m)$.\n3.  Generate the $k$-sparse unit-norm direction vector $x_{\\mathrm{dir}} \\in \\mathbb{R}^n$.\n4.  Construct the signal $x = \\alpha x_{\\mathrm{dir}}$ using the given amplitude $\\alpha$.\n5.  Compute the linear measurements $z = Ax$.\n6.  Determine the threshold range $T = \\eta \\cdot \\max_i |z_i|$ and the dither width $\\Delta = T/1000$.\n7.  Generate $K$ i.i.d. thresholds $\\{t_j\\}_{j=1}^K \\sim \\operatorname{Uniform}(0, T)$ and $K$ i.i.d. dithers $\\{d_j\\}_{j=1}^K \\sim \\operatorname{Uniform}(-\\Delta/2, \\Delta/2)$. Combine them to form the dithered thresholds $\\tau_j = t_j + d_j$.\n8.  Initialize a total count to zero. For each $j=1, \\dots, K$:\n    a. Calculate the number of components of $z$ greater than $\\tau_j$.\n    b. Calculate the number of components of $z$ less than $-\\tau_j$.\n    c. Add these two counts to the total.\n9.  Compute the estimate $\\widehat{\\|Ax\\|_1} = (T/K) \\times (\\text{total count})$.\n10. Compute the exact norm $\\|A x_{\\mathrm{dir}}\\|_1$.\n11. Compute the final estimate $\\widehat{\\alpha} = \\widehat{\\|Ax\\|_1} / \\|A x_{\\mathrm{dir}}\\|_1$.\nThe results for the three test cases are then collected and printed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements an estimator for signal amplitude from one-bit\n    compressed sensing measurements with randomized and dithered thresholds.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (m, n, k, alpha, K, eta)\n        (64, 32, 6, 1.0, 4000, 1.2),\n        (64, 32, 6, 0.05, 4000, 1.5),\n        (64, 32, 6, 3.0, 6000, 1.3),\n    ]\n\n    results = []\n    # Use a single random number generator for reproducibility as required.\n    rng = np.random.default_rng(42)\n\n    for case in test_cases:\n        m, n, k, alpha_true, K, eta = case\n\n        # Generate sensing matrix A with i.i.d. N(0, 1/m) entries.\n        A = rng.normal(loc=0.0, scale=1.0 / np.sqrt(m), size=(m, n))\n\n        # Generate k-sparse direction vector x_dir.\n        # Choose a support of size k uniformly at random.\n        support = rng.choice(n, k, replace=False)\n        \n        # Assign i.i.d. N(0, 1) values on the support.\n        x_dir_unnormalized = np.zeros(n)\n        x_dir_unnormalized[support] = rng.normal(loc=0.0, scale=1.0, size=k)\n\n        # Normalize to unit l2-norm.\n        norm_val = np.linalg.norm(x_dir_unnormalized)\n        x_dir = x_dir_unnormalized / norm_val if norm_val > 0 else x_dir_unnormalized\n\n        # Construct the true signal x and the measurement vector z.\n        x = alpha_true * x_dir\n        z = A @ x\n\n        # Determine thresholding parameters T and Delta.\n        # If z is a zero vector (alpha=0), max_abs_z is 0, T is 0.\n        max_abs_z = np.max(np.abs(z)) if z.size > 0 else 0.0\n        T = eta * max_abs_z\n        Delta = T / 1000.0\n\n        # If T=0, all counts will be 0, leading to a ||Ax||_1 estimate of 0.\n        # This correctly results in an alpha estimate of 0.\n        if T == 0:\n            estimated_alpha = 0.0\n            results.append(estimated_alpha)\n            continue\n\n        # Generate K randomized thresholds and dithers.\n        t_j = rng.uniform(low=0.0, high=T, size=K)\n        d_j = rng.uniform(low=-Delta / 2.0, high=Delta / 2.0, size=K)\n        tau_j = t_j + d_j\n\n        # Estimate ||Ax||_1 using Monte Carlo integration.\n        # The estimator is (T/K) * sum_j(sum_i(1{z_i > tau_j} + 1{z_i  -tau_j})).\n        # We can implement this efficiently using broadcasting.\n        z_col = z[:, np.newaxis]  # Shape (m, 1)\n        tau_row = tau_j[np.newaxis, :]  # Shape (1, K)\n\n        # Count occurrences where z_i > tau_j for all i, j.\n        # This is equivalent to counting positive signs in y_j^{(+)}.\n        counts_plus = np.sum(z_col > tau_row, axis=0)  # Shape (K,)\n\n        # Count occurrences where z_i  -tau_j for all i, j.\n        # This is equivalent to counting negative signs in y_j^{(-)}.\n        counts_minus = np.sum(z_col  -tau_row, axis=0) # Shape (K,)\n\n        # Sum counts over all K thresholds.\n        total_counts = np.sum(counts_plus + counts_minus)\n        \n        # Compute the estimate of ||Ax||_1.\n        estimated_l1_norm = (T / K) * total_counts\n\n        # Compute the amplitude estimate alpha_hat.\n        # The denominator ||A @ x_dir||_1 is known.\n        norm_A_x_dir = np.linalg.norm(A @ x_dir, ord=1)\n        \n        if norm_A_x_dir == 0:\n            # This is highly unlikely with random A and non-zero x_dir.\n            # If it occurs, the scale is indeterminate. We can output NaN or 0.\n            # The problem context implies a non-degenerate setup. An estimate of 0 is reasonable.\n            estimated_alpha = 0.0\n        else:\n            estimated_alpha = estimated_l1_norm / norm_A_x_dir\n            \n        results.append(estimated_alpha)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}