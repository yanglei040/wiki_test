{
    "hands_on_practices": [
        {
            "introduction": "在从量化数据中恢复信号时，首要步骤是将测量中包含的信息进行数学化表示。本练习将引导您把信号落入某个量化器区间的物理过程，转化为一组精确的数学不等式，并进一步构建一个凸惩罚函数。掌握这一关键步骤是后续构建可被现代优化求解器理解和处理的恢复问题的前提 。",
            "id": "3472918",
            "problem": "考虑一个无噪声的压缩感知模型，其中包含一个未知的 $k$-稀疏向量 $\\mathbf{x} \\in \\mathbb{R}^{n}$ 和一个已知的传感矩阵 $A \\in \\mathbb{R}^{m \\times n}$。令 $\\mathbf{a}_{i}^{\\top}$ 表示 $A$ 的第 $i$ 行，因此量化前的第 $i$ 个标量测量值为 $z_{i} = \\mathbf{a}_{i}^{\\top} \\mathbf{x}$。测量值由一个非均匀标量量化器 $Q$ 进行量化，该量化器由一个严格递增的阈值序列 $\\{t_{k}\\}_{k=1}^{K+1}$ 定义，其中 $t_{1}  t_{2}  \\cdots  t_{K+1}$。对于所有 $i \\in \\{1,\\dots,m\\}$，通过 $\\mathbf{a}_{i}^{\\top} \\mathbf{x} \\in [t_{1}, t_{K+1})$ 来确保其工作范围。量化器根据规则 $Q(z) = k$ 当且仅当 $t_{k} \\leq z  t_{k+1}$ 时，将标量 $z$ 映射到一个区间索引 $Q(z) \\in \\{1,\\dots,K\\}$。观测到的标签为 $y_{i} = Q(\\mathbf{a}_{i}^{\\top} \\mathbf{x})$，其中 $i = 1,\\dots,m$，这些标签集合在 $\\mathbf{y} \\in \\{1,\\dots,K\\}^{m}$ 中。\n\n从非均匀量化器的定义和区间索引的区间隶属特性出发，形式化由观测值 $\\mathbf{y}$ 所蕴含的对 $A \\mathbf{x}$ 的测量一致性约束，并推导一个凸铰链惩罚项的单一闭式解析表达式。该表达式当且仅当所有区间约束都满足时（即对于每个 $i$，条件 $t_{y_{i}} \\leq \\mathbf{a}_{i}^{\\top} \\mathbf{x}  t_{y_{i}+1}$ 成立）等于零，否则对每个边界上的违规行为进行线性的单侧惩罚。将你的最终答案表示为 $A$、$\\mathbf{x}$、$\\mathbf{y}$ 和阈值 $\\{t_{k}\\}_{k=1}^{K+1}$ 的显式函数。\n\n你的最终答案必须是单一的闭式解析表达式。不需要数值近似或四舍五入。",
            "solution": "首先确认该问题具有科学依据、是适定的、客观的且在形式上是完备的。它代表了压缩感知中从量化测量中恢复信号的优化问题构建中的一个标准任务。因此，我们可以继续进行推导。\n\n问题要求构建一个强制测量一致性的凸惩罚函数。让我们首先形式化一致性约束本身。\n\n根据问题陈述，标量量化器 $Q$ 基于一组严格递增的阈值 $\\{t_{j}\\}_{j=1}^{K+1}$，将输入 $z$ 映射到一个整数区间索引 $k \\in \\{1, \\dots, K\\}$。量化规则由下式给出：\n$$\nQ(z) = k \\quad \\text{当且仅当} \\quad t_{k} \\leq z  t_{k+1}\n$$\n未量化的测量值为 $z_{i} = \\mathbf{a}_{i}^{\\top} \\mathbf{x}$，其中 $i \\in \\{1, \\dots, m\\}$，$\\mathbf{a}_{i}^{\\top}$ 是传感矩阵 $A$ 的第 $i$ 行，$\\mathbf{x} \\in \\mathbb{R}^{n}$ 是未知的信号向量。观测数据是区间索引 $y_{i} = Q(z_{i}) = Q(\\mathbf{a}_{i}^{\\top} \\mathbf{x})$。\n\n对于每次测量 $i$，观测值 $y_{i}$ 意味着真实值 $\\mathbf{a}_{i}^{\\top} \\mathbf{x}$ 必须位于由阈值定义的特定区间内。应用量化规则，观测值 $y_i$ 等价于对 $\\mathbf{x}$ 的以下约束集：\n$$\nt_{y_{i}} \\leq \\mathbf{a}_{i}^{\\top} \\mathbf{x}  t_{y_{i}+1}\n$$\n这必须对每次测量都成立，因此我们有一个包含 $m$ 个此类区间约束的系统，其中 $i = 1, \\dots, m$。每个区间约束可以分解为两个独立的不等式约束：\n1. $\\mathbf{a}_{i}^{\\top} \\mathbf{x} \\geq t_{y_{i}}$\n2. $\\mathbf{a}_{i}^{\\top} \\mathbf{x}  t_{y_{i}+1}$\n\n问题要求一个凸铰链惩罚项，该惩罚项当且仅当这些约束被满足时为零。根据定义，铰链惩罚以单侧、线性的方式惩罚对不等式的违反。对于形如 $f(\\mathbf{x}) \\leq 0$ 的一般不等式约束，相应的铰链惩罚是 $\\max\\{0, f(\\mathbf{x})\\}$。对于形如 $g(\\mathbf{x}) \\geq 0$ 的约束，它等价于 $-g(\\mathbf{x}) \\leq 0$，其惩罚是 $\\max\\{0, -g(\\mathbf{x})\\}$。\n\n让我们将此原则应用于第 $i$ 次测量的两个不等式。\n\n对于第一个不等式 $\\mathbf{a}_{i}^{\\top} \\mathbf{x} \\geq t_{y_{i}}$，我们可以将其重写为 $t_{y_{i}} - \\mathbf{a}_{i}^{\\top} \\mathbf{x} \\leq 0$。当 $t_{y_{i}} - \\mathbf{a}_{i}^{\\top} \\mathbf{x} > 0$ 时，发生违规。违反此下界的铰链惩罚是：\n$$\nP_{\\text{lower}, i}(\\mathbf{x}) = \\max\\{0, t_{y_{i}} - \\mathbf{a}_{i}^{\\top} \\mathbf{x}\\}\n$$\n如果 $\\mathbf{a}_{i}^{\\top} \\mathbf{x} \\geq t_{y_{i}}$，该函数为零；当 $\\mathbf{a}_{i}^{\\top} \\mathbf{x}  t_{y_{i}}$ 时，该函数随违规的幅度 $t_{y_{i}} - \\mathbf{a}_{i}^{\\top} \\mathbf{x}$ 线性增长。\n\n对于第二个不等式 $\\mathbf{a}_{i}^{\\top} \\mathbf{x}  t_{y_{i}+1}$，我们首先将严格不等式松弛为 $\\mathbf{a}_{i}^{\\top} \\mathbf{x} \\leq t_{y_{i}+1}$，以便于构建凸惩罚函数。这是优化中的标准做法，因为对开集进行惩罚是有问题的。松弛后的约束是 $\\mathbf{a}_{i}^{\\top} \\mathbf{x} - t_{y_{i}+1} \\leq 0$。当 $\\mathbf{a}_{i}^{\\top} \\mathbf{x} - t_{y_{i}+1} > 0$ 时，发生违规。违反此上界的铰链惩罚是：\n$$\nP_{\\text{upper}, i}(\\mathbf{x}) = \\max\\{0, \\mathbf{a}_{i}^{\\top} \\mathbf{x} - t_{y_{i}+1}\\}\n$$\n如果 $\\mathbf{a}_{i}^{\\top} \\mathbf{x} \\leq t_{y_{i}+1}$，该函数为零；当 $\\mathbf{a}_{i}^{\\top} \\mathbf{x} > t_{y_{i}+1}$ 时，该函数随违规的幅度 $\\mathbf{a}_{i}^{\\top} \\mathbf{x} - t_{y_{i}+1}$ 线性增长。\n\n第 $i$ 次测量的总惩罚（当且仅当两个约束都满足时必须为零）是各个惩罚之和：\n$$\nP_{i}(\\mathbf{x}) = P_{\\text{lower}, i}(\\mathbf{x}) + P_{\\text{upper}, i}(\\mathbf{x}) = \\max\\{0, t_{y_{i}} - \\mathbf{a}_{i}^{\\top} \\mathbf{x}\\} + \\max\\{0, \\mathbf{a}_{i}^{\\top} \\mathbf{x} - t_{y_{i}+1}\\}\n$$\n这个组合惩罚 $P_{i}(\\mathbf{x})$ 当且仅当两项都为零时为零，这发生在 $t_{y_{i}} - \\mathbf{a}_{i}^{\\top} \\mathbf{x} \\leq 0$ 和 $\\mathbf{a}_{i}^{\\top} \\mathbf{x} - t_{y_{i}+1} \\leq 0$ 时。这恰好是条件 $t_{y_{i}} \\leq \\mathbf{a}_{i}^{\\top} \\mathbf{x} \\leq t_{y_{i}+1}$。\n\n为获得所有测量的总惩罚，我们将各个惩罚对 $i = 1, \\dots, m$求和。设此总惩罚由 $\\mathcal{L}(\\mathbf{x})$ 表示。\n$$\n\\mathcal{L}(\\mathbf{x}) = \\sum_{i=1}^{m} P_{i}(\\mathbf{x}) = \\sum_{i=1}^{m} \\left( \\max\\{0, t_{y_{i}} - \\mathbf{a}_{i}^{\\top} \\mathbf{x}\\} + \\max\\{0, \\mathbf{a}_{i}^{\\top} \\mathbf{x} - t_{y_{i}+1}\\} \\right)\n$$\n这个最终表达式满足了问题的所有要求：\n- 它是 $A$、$\\mathbf{x}$、$\\mathbf{y}$ 和阈值 $\\{t_{k}\\}$ 的函数。项 $\\mathbf{a}_{i}^{\\top} \\mathbf{x}$ 涉及 $A$ 和 $\\mathbf{x}$。阈值上的索引 $y_{i}$ 和 $y_{i}+1$ 来自观测向量 $\\mathbf{y}$。\n- 它是 $\\mathbf{x}$ 的凸函数。项 $t_{y_{i}} - \\mathbf{a}_{i}^{\\top} \\mathbf{x}$ 和 $\\mathbf{a}_{i}^{\\top} \\mathbf{x} - t_{y_{i}+1}$ 在 $\\mathbf{x}$ 中是仿射的。函数 $\\max\\{0, u\\}$ 是凸且非递减的。一个凸非递减函数与一个仿射函数的复合是凸的。凸函数的和是凸的。\n- 当且仅当所有区间约束（在其松弛形式 $t_{y_{i}} \\leq \\mathbf{a}_{i}^{\\top} \\mathbf{x} \\leq t_{y_{i}+1}$ 下）对所有 $i \\in \\{1, \\dots, m\\}$ 都满足时，它等于零。由于和中的每一项都是非负的，所以和为零当且仅当所有单项都为零。\n- 它在每个边界上对违规行为进行线性的单侧惩罚，这是铰链惩罚的本质。\n\n该表达式是所求的测量一致性惩罚的闭式解析表示。",
            "answer": "$$\n\\boxed{\\sum_{i=1}^{m} \\left( \\max\\{0, t_{y_i} - \\mathbf{a}_{i}^{\\top} \\mathbf{x}\\} + \\max\\{0, \\mathbf{a}_{i}^{\\top} \\mathbf{x} - t_{y_{i}+1}\\} \\right)}\n$$"
        },
        {
            "introduction": "将问题形式化为优化任务后，下一步自然是设计一个求解算法。本练习聚焦于单位比特压缩感知，这是一种将测量值极度粗略地量化为单个符号位的极端情况。您将推导二进制迭代硬阈值（BIHT）算法的更新规则，该练习清晰地展示了如何利用损失函数梯度来迭代地修正稀疏信号的估计值，从而为您提供一个具体而高效的恢复算法实例 。",
            "id": "3472923",
            "problem": "设 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 是一个感知矩阵，其行向量为 $\\mathbf{a}_{i}^{\\top}$，并设 $\\mathbf{x}^{\\star} \\in \\mathbb{R}^{n}$ 是一个未知的 $s$-稀疏向量。在一比特压缩感知（CS）中，仅观测线性测量的符号，因此测量模型为 $\\mathbf{y} = \\operatorname{sign}(\\mathbf{A}\\mathbf{x}^{\\star}) \\in \\{-1, +1\\}^{m}$。一种恢复与 $\\mathbf{y}$ 一致的稀疏向量 $\\mathbf{x}$ 的常用方法是使用平方铰链损失来惩罚符号不一致的测量值：\n$$\nL(\\mathbf{x}) = \\sum_{i=1}^{m} \\left(\\max\\!\\left(0, - y_{i}\\,\\mathbf{a}_{i}^{\\top}\\mathbf{x}\\right)\\right)^{2},\n$$\n其中 $\\max(0, \\cdot)$ 按元素方式应用。二进制迭代硬阈值（BIHT）算法通过重复对 $L(\\mathbf{x})$ 进行最速下降步，然后应用一个硬阈值算子（该算子保留幅度最大的 $s$ 个条目并将余下条目置零），来寻求 $L(\\mathbf{x})$ 的稀疏最小化子。\n\n从一比特量化测量、平方铰链损失、通过次梯度对非光滑目标进行的最速下降迭代，以及保留幅度最大 $s$ 个条目的硬阈值算子 $H_{s}(\\cdot)$ 的定义出发，推导将 $\\mathbf{x}^{t}$ 映射到下一个迭代值的 BIHT 更新的显式解析表达式。使用按元素的正部算子 $[\\,\\cdot\\,]_{+} := \\max(0, \\cdot)$，将您的最终更新表示为关于 $\\mathbf{A}$、$\\mathbf{y}$、$\\mathbf{x}^{t}$、一个正步长参数 $\\mu$ 和 $s$ 的单一闭式表达式。您的最终答案必须是一个不带等号的单一闭式解析表达式。无需进行数值评估。",
            "solution": "二进制迭代硬阈值（BIHT）算法从迭代 $\\mathbf{x}^{t}$ 到 $\\mathbf{x}^{t+1}$ 的更新包含两个步骤：首先是对损失函数 $L(\\mathbf{x})$ 的最速下降（梯度下降）步，然后是应用硬阈值算子 $H_{s}(\\cdot)$。\n\n第一步，梯度下降更新，用于计算一个中间向量 $\\mathbf{z}^{t+1}$：\n$$\n\\mathbf{z}^{t+1} = \\mathbf{x}^{t} - \\mu \\nabla L(\\mathbf{x}^{t})\n$$\n其中 $\\mu > 0$ 是步长，$\\nabla L(\\mathbf{x}^{t})$ 是损失函数在 $\\mathbf{x}^{t}$ 处的梯度。\n\n第二步是硬阈值操作：\n$$\n\\mathbf{x}^{t+1} = H_{s}(\\mathbf{z}^{t+1})\n$$\n我们的任务是找到梯度 $\\nabla L(\\mathbf{x})$ 的解析表达式。损失函数为：\n$$\nL(\\mathbf{x}) = \\sum_{i=1}^{m} \\left(\\max(0, -y_i \\mathbf{a}_i^\\top \\mathbf{x})\\right)^2 = \\sum_{i=1}^{m} ([-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)^2\n$$\n该函数是连续可微的（问题中提到的“次梯度”一词在这种情况下等同于标准梯度，因为平方铰链损失是光滑的）。我们可以对每一项分别求导，然后求和。设 $L_i(\\mathbf{x}) = ([-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)^2$。\n\n使用链式法则，设 $u_i(\\mathbf{x}) = -y_i \\mathbf{a}_i^\\top \\mathbf{x}$，且 $f(u) = ([u]_+)^2$。$f(u)$ 的导数是 $f'(u) = 2[u]_+$。$u_i(\\mathbf{x})$ 对 $\\mathbf{x}$ 的梯度是 $\\nabla u_i(\\mathbf{x}) = -y_i \\mathbf{a}_i$。\n\n因此，第 $i$ 项的梯度是：\n$$\n\\nabla L_i(\\mathbf{x}) = f'(u_i(\\mathbf{x})) \\cdot \\nabla u_i(\\mathbf{x}) = 2[-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i)\n$$\n总梯度是所有项的梯度之和：\n$$\n\\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} \\nabla L_i(\\mathbf{x}) = \\sum_{i=1}^{m} 2[-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i)\n$$\n我们可以将这个和式用矩阵形式表示。注意到求和的形式是 $\\sum_i (\\text{标量})_i \\cdot (\\text{向量})_i$，这可以写成矩阵与向量的乘积。\n$$\n\\nabla L(\\mathbf{x}) = -2 \\sum_{i=1}^{m} \\mathbf{a}_i y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ = -2 \\mathbf{A}^\\top \\mathbf{w}(\\mathbf{x})\n$$\n其中 $\\mathbf{w}(\\mathbf{x})$ 是一个 $m \\times 1$ 的向量，其第 $i$ 个元素为 $w_i(\\mathbf{x}) = y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+$。\n\n为了更紧凑地表示 $\\mathbf{w}(\\mathbf{x})$，我们引入对角矩阵 $\\operatorname{diag}(\\mathbf{y})$，其对角线元素为 $\\mathbf{y}$ 的元素。那么，向量 $-y_i \\mathbf{a}_i^\\top \\mathbf{x}$ 可以表示为 $-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}$。于是，向量 $\\mathbf{w}(\\mathbf{x})$ 可以写成：\n$$\n\\mathbf{w}(\\mathbf{x}) = \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}]_+\n$$\n将此代入梯度表达式中：\n$$\n\\nabla L(\\mathbf{x}) = -2 \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}]_{+} \\right)\n$$\n现在我们可以写出梯度下降步骤：\n$$\n\\mathbf{z}^{t+1} = \\mathbf{x}^{t} - \\mu \\nabla L(\\mathbf{x}^{t}) = \\mathbf{x}^{t} + 2\\mu \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^{t}]_{+} \\right)\n$$\n最后，应用硬阈值算子 $H_s(\\cdot)$，我们得到完整的 BIHT 更新规则：\n$$\n\\mathbf{x}^{t+1} = H_{s}\\left( \\mathbf{x}^{t} + 2\\mu \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^{t}]_{+} \\right) \\right)\n$$\n这便是所求的单一闭式解析表达式。",
            "answer": "$$\n\\boxed{\nH_{s}\\left( \\mathbf{x}^{t} + 2\\mu \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^{t}]_{+} \\right) \\right)\n}\n$$"
        },
        {
            "introduction": "超越单个算法的实现细节，实际的工程应用更要求我们具备系统级的设计决策能力。最后一个练习将探讨一个核心的权衡问题：在固定的总比特预算下，我们应该选择采集更多低精度（比特少）的测量值，还是更少但高精度（比特多）的测量值？通过一个动手编程练习，您将探索这一权衡关系，并确定最优的资源分配策略以最小化重建误差，从而在理论和实际系统设计之间架起一座桥梁 。",
            "id": "3472956",
            "problem": "考虑一个线性压缩感知采集模型，其测量值为 $z_{i} = \\mathbf{a}_{i}^{\\top} \\mathbf{x}$，其中 $i = 1, \\dots, m$，$\\mathbf{x} \\in \\mathbb{R}^{n}$ 是一个随机信号，$\\mathbf{a}_{i} \\in \\mathbb{R}^{n}$ 表示感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的第 $i$ 行。假设 $\\mathbf{x}$ 是均值为零、协方差矩阵为 $\\Sigma_{x}$ 的高斯分布，并且感知矩阵的设计使得所有测量方差相等，即对于已知的标量 $\\sigma  0$ 和每一个 $i$，都有 $\\mathbf{a}_{i}^{\\top} \\Sigma_{x} \\mathbf{a}_{i} = \\sigma^{2}$。每个测量值 $z_{i}$ 由一个具有 $b$ 位和步长 $\\Delta  0$ 的对称均匀中升型量化器进行量化。当量化器 $|z_{i}| \\geq R(\\Delta)$ 时，量化器会饱和，其中动态范围为 $R(\\Delta) = 2^{b-1} \\Delta$，并将所有超出动态范围的值映射到最近的可表示水平。令标准正态尾函数（也称为Q函数）定义为 $Q(u) = \\int_{u}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\exp\\!\\left( -\\frac{t^{2}}{2} \\right) \\, dt$。\n\n1. 仅使用高斯模型所蕴含的测量分布和量化器的定义，推导单个测量的饱和概率，将其表示为 $\\Delta$、$b$ 和 $\\sigma$ 的函数。\n\n2. 在从量化测量中进行稀疏恢复时，对预期重构误差的两个主要贡献者是动态范围内的量化误差和因饱和造成的信息损失。一个用于平衡这些影响的常见代理目标是\n$$\nJ(\\Delta) = \\alpha \\Delta^{2} + \\beta p_{\\mathrm{sat}}(\\Delta),\n$$\n其中 $\\alpha  0$ 和 $\\beta  0$ 是已知的权衡权重，反映了量化误差和饱和对恢复的影响，而 $p_{\\mathrm{sat}}(\\Delta)$ 是你在第1部分中推导出的饱和概率。从第一性原理出发，推导最小化 $J(\\Delta)$ 的唯一步长 $\\Delta^{\\star}$，并用 $\\alpha$、$\\beta$、$\\sigma$ 和 $b$ 以闭式形式表示。你的最终答案必须是单一的闭式解析表达式。\n\n将最终答案表示为符号表达式。不需要四舍五入。不要包含任何物理单位。方框内的最终答案必须仅为 $\\Delta^{\\star}$ 的表达式。",
            "solution": "这个问题分为两个部分。首先，我们必须推导单个量化测量的饱和概率。其次，我们必须利用这个概率来找到最小化给定目标函数的最优量化器步长 $\\Delta$。\n\n**第1部分：饱和概率的推导**\n\n问题陈述给出了线性测量模型 $z_{i} = \\mathbf{a}_{i}^{\\top} \\mathbf{x}$，其中信号 $\\mathbf{x} \\in \\mathbb{R}^{n}$ 是一个均值为零、协方差矩阵为 $\\Sigma_{x}$ 的高斯随机向量，即 $\\mathbf{x} \\sim \\mathcal{N}(0, \\Sigma_{x})$。\n\n高斯随机向量的线性变换也是高斯分布的。因此，每个测量值 $z_{i}$ 是一个高斯随机变量。其均值为：\n$$\n\\mathbb{E}[z_{i}] = \\mathbb{E}[\\mathbf{a}_{i}^{\\top} \\mathbf{x}] = \\mathbf{a}_{i}^{\\top} \\mathbb{E}[\\mathbf{x}] = \\mathbf{a}_{i}^{\\top} 0 = 0\n$$\n$z_{i}$ 的方差由下式给出：\n$$\n\\text{Var}(z_{i}) = \\mathbb{E}[(z_{i} - \\mathbb{E}[z_{i}])^{2}] = \\mathbb{E}[z_{i}^{2}] = \\mathbb{E}[(\\mathbf{a}_{i}^{\\top} \\mathbf{x})(\\mathbf{x}^{\\top} \\mathbf{a}_{i})] = \\mathbf{a}_{i}^{\\top} \\mathbb{E}[\\mathbf{x} \\mathbf{x}^{\\top}] \\mathbf{a}_{i} = \\mathbf{a}_{i}^{\\top} \\Sigma_{x} \\mathbf{a}_{i}\n$$\n问题陈述指出，感知矩阵 $A$ 的设计使得对于所有 $i=1, \\dots, m$ 都有 $\\mathbf{a}_{i}^{\\top} \\Sigma_{x} \\mathbf{a}_{i} = \\sigma^{2}$。因此，每个测量值 $z_{i}$ 具有相同的分布：均值为0，方差为 $\\sigma^{2}$ 的高斯分布，记为 $z_i \\sim \\mathcal{N}(0, \\sigma^{2})$。\n\n当量化器中测量值的幅值 $|z_i|$ 大于或等于动态范围限制 $R(\\Delta)$ 时，就会发生饱和。动态范围给定为 $R(\\Delta) = 2^{b-1} \\Delta$。因此，饱和事件为 $|z_{i}| \\geq 2^{b-1} \\Delta$。\n\n饱和概率 $p_{\\mathrm{sat}}(\\Delta)$ 是此事件的概率：\n$$\np_{\\mathrm{sat}}(\\Delta) = P(|z_{i}| \\geq 2^{b-1} \\Delta)\n$$\n由于 $z_i$ 的分布关于其均值0是对称的，我们可以将其写为：\n$$\np_{\\mathrm{sat}}(\\Delta) = P(z_{i} \\geq 2^{b-1} \\Delta) + P(z_{i} \\leq -2^{b-1} \\Delta) = 2 P(z_{i} \\geq 2^{b-1} \\Delta)\n$$\n为了使用标准正态尾函数 $Q(u)$ 表示此概率，我们对随机变量 $z_{i}$ 进行标准化。令 $U = z_{i}/\\sigma$。那么 $U$ 是一个标准正态随机变量，$U \\sim \\mathcal{N}(0, 1)$。\n现在概率可以写成 $U$ 的形式：\n$$\nP(z_{i} \\geq 2^{b-1} \\Delta) = P(\\sigma U \\geq 2^{b-1} \\Delta) = P\\left(U \\geq \\frac{2^{b-1} \\Delta}{\\sigma}\\right)\n$$\n根据 $Q$ 函数的定义，$Q(u) = P(U \\geq u)$，我们有：\n$$\nP(z_{i} \\geq 2^{b-1} \\Delta) = Q\\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)\n$$\n因此，任何单个测量的饱和概率为：\n$$\np_{\\mathrm{sat}}(\\Delta) = 2 Q\\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)\n$$\n\n**第2部分：最优步长 $\\Delta^{\\star}$ 的推导**\n\n我们被要求找到最小化目标函数 $J(\\Delta)$ 的步长 $\\Delta^{\\star}$：\n$$\nJ(\\Delta) = \\alpha \\Delta^{2} + \\beta p_{\\mathrm{sat}}(\\Delta)\n$$\n将第1部分中得到的 $p_{\\mathrm{sat}}(\\Delta)$ 的表达式代入，得到：\n$$\nJ(\\Delta) = \\alpha \\Delta^{2} + 2 \\beta Q\\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)\n$$\n为了找到最小值，我们计算 $J(\\Delta)$ 关于 $\\Delta$ 的导数并令其为零。我们必须求 $\\frac{dJ}{d\\Delta}$。第一项的导数是 $2 \\alpha \\Delta$。对于第二项，我们使用链式法则和Q函数的导数。Q函数的定义为 $Q(u) = \\int_{u}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{t^{2}}{2}) dt$。根据微积分基本定理（莱布尼茨法则），其导数为 $\\frac{dQ(u)}{du} = -\\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{u^{2}}{2})$。\n\n令 $u(\\Delta) = \\frac{2^{b-1} \\Delta}{\\sigma}$。那么 $\\frac{du}{d\\Delta} = \\frac{2^{b-1}}{\\sigma}$。第二项的导数是：\n$$\n\\frac{d}{d\\Delta}\\left[2 \\beta Q\\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)\\right] = 2 \\beta \\cdot \\frac{dQ(u)}{du} \\cdot \\frac{du}{d\\Delta} = 2 \\beta \\left( -\\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{u^{2}}{2}\\right) \\right) \\left( \\frac{2^{b-1}}{\\sigma} \\right)\n$$\n$$\n= -\\frac{2 \\beta \\cdot 2^{b-1}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2} \\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)^{2}\\right) = -\\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right)\n$$\n现在，我们将全导数 $\\frac{dJ}{d\\Delta}$ 置为零：\n$$\n\\frac{dJ}{d\\Delta} = 2 \\alpha \\Delta - \\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right) = 0\n$$\n整理各项，我们得到：\n$$\n2 \\alpha \\Delta = \\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right)\n$$\n这是一个超越方程。为了求解 $\\Delta$，我们将其变换为与朗伯W函数相容的形式，该函数定义为 $z = W(z)\\exp(W(z))$。\n让我们对方程两边进行平方：\n$$\n4 \\alpha^{2} \\Delta^{2} = \\frac{\\beta^{2} \\cdot 2^{2b}}{\\sigma^{2} \\cdot 2\\pi} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{\\sigma^{2}}\\right)\n$$\n$$\n4 \\alpha^{2} \\Delta^{2} = \\frac{\\beta^{2} \\cdot 2^{2b-1}}{\\pi\\sigma^{2}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{\\sigma^{2}}\\right)\n$$\n让我们定义一个新变量 $Y = \\frac{2^{2b-2} \\Delta^{2}}{\\sigma^{2}}$。这意味着 $\\Delta^{2} = \\frac{\\sigma^{2}}{2^{2b-2}}Y$。将此代入平方后的方程中：\n$$\n4 \\alpha^{2} \\left(\\frac{\\sigma^{2}}{2^{2b-2}}Y\\right) = \\frac{\\beta^{2} \\cdot 2^{2b-1}}{\\pi\\sigma^{2}} \\exp(-Y)\n$$\n$$\n\\frac{4 \\pi \\alpha^{2} \\sigma^{4}}{\\beta^{2} \\cdot 2^{2b-1} \\cdot 2^{2b-2}} Y = \\exp(-Y)\n$$\n$$\n\\frac{4 \\pi \\alpha^{2} \\sigma^{4}}{\\beta^{2} \\cdot 2^{4b-3}} Y = \\exp(-Y)\n$$\n$$\n\\frac{\\pi \\alpha^{2} \\sigma^{4}}{\\beta^{2} \\cdot 2^{4b-5}} Y = \\exp(-Y)\n$$\n乘以 $\\exp(Y)$ 并整理得到朗伯W函数的典范形式：\n$$\nY \\exp(Y) = \\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\n$$\n应用朗伯W函数，我们求解 $Y$：\n$$\nY = W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)\n$$\n我们将 $Y = \\frac{2^{2b-2} (\\Delta^{\\star})^{2}}{\\sigma^{2}}$ 代回，以求得最优步长 $\\Delta^{\\star}$：\n$$\n\\frac{2^{2b-2} (\\Delta^{\\star})^{2}}{\\sigma^{2}} = W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)\n$$\n$$\n(\\Delta^{\\star})^{2} = \\frac{\\sigma^{2}}{2^{2b-2}} W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)\n$$\n由于步长 $\\Delta$ 必须为正，我们取正平方根：\n$$\n\\Delta^{\\star} = \\sqrt{\\frac{\\sigma^{2}}{2^{2b-2}} W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)} = \\frac{\\sigma}{2^{b-1}} \\sqrt{W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)}\n$$\n为了确认这是一个唯一的最小值，我们检查 $J(\\Delta)$ 的二阶导数：\n$$\n\\frac{d^2 J}{d\\Delta^2} = \\frac{d}{d\\Delta}\\left[2 \\alpha \\Delta - \\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right)\\right]\n$$\n$$\n\\frac{d^2 J}{d\\Delta^2} = 2 \\alpha - \\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right) \\left(-\\frac{2 \\cdot 2^{2b-2} \\Delta}{2\\sigma^{2}}\\right)\n$$\n$$\n\\frac{d^2 J}{d\\Delta^2} = 2 \\alpha + \\frac{\\beta \\cdot 2^{b} \\cdot 2^{2b-2} \\Delta}{\\sigma^3\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right)\n$$\n对于 $\\Delta  0$，并且给定 $\\alpha  0$, $\\beta  0$, $\\sigma  0$，二阶导数表达式中的所有项都是正的。因此，对于所有 $\\Delta  0$，$\\frac{d^2 J}{d\\Delta^2}  0$，这意味着 $J(\\Delta)$ 在 $\\Delta  0$ 上是一个严格凸函数。因此，我们找到的临界点对应一个唯一的全局最小值。朗伯W函数的自变量是正的，所以其输出是一个唯一的正实数，从而确保 $\\Delta^{\\star}$ 是一个唯一的正实数。",
            "answer": "$$\\boxed{\\frac{\\sigma}{2^{b-1}} \\sqrt{W\\left(\\frac{\\beta^{2} 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)}}$$"
        }
    ]
}