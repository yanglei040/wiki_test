{
    "hands_on_practices": [
        {
            "introduction": "A fundamental skill in computational science is translating a mathematical objective into a practical algorithm. This first exercise guides you through this process for sparse phase retrieval, starting from a standard loss function. You will derive the core update rule for the Wirtinger Flow algorithm, a gradient-based method adapted for complex variables, and combine it with a projection step to enforce sparsity .",
            "id": "3477906",
            "problem": "Consider complex-valued sparse phase retrieval, where an unknown $k$-sparse signal $x_{\\star} \\in \\mathbb{C}^n$ is measured via quadratic intensities $y_i = \\lvert a_i^* x_{\\star} \\rvert^2$ for $i = 1,\\dots,m$, with known sensing vectors $a_i \\in \\mathbb{C}^n$ and $m \\ge n$. A standard approach is to minimize the intensity-based empirical loss\n$$\nf(z) = \\frac{1}{2m} \\sum_{i=1}^m \\left( \\lvert a_i^* z \\rvert^2 - y_i \\right)^2,\n$$\nwhere $z \\in \\mathbb{C}^n$ is the decision variable, using Wirtinger calculus to form gradient-based updates, while enforcing the $k$-sparsity constraint via projection. Let $\\Sigma_k \\subset \\mathbb{C}^n$ denote the set of $k$-sparse vectors, and let $\\mathcal{H}_k : \\mathbb{C}^n \\to \\Sigma_k$ denote the hard-thresholding operator that retains the $k$ entries of largest magnitude and sets all others to zero. Define the rank-one Hermitian matrices $M_i = a_i a_i^*$.\n\nStarting from first principles and well-tested facts:\n- The measurements are invariant under a global phase on $x_{\\star}$, and the intensity-based loss $f(z)$ respects this invariance.\n- For real-valued objectives depending on complex variables, the steepest descent direction is formed using the Wirtinger derivative with respect to the conjugate variable, i.e., $\\nabla f(z) := \\partial f(z)/\\partial \\bar{z}$.\n- Projection onto $\\Sigma_k$ is implemented by $\\mathcal{H}_k$.\n\nWhich option correctly formulates one iteration of projected gradient descent for sparse phase retrieval that uses the appropriate Wirtinger gradient of the intensity-based loss and the correct projection onto $\\Sigma_k$ via hard thresholding, with a constant step size $\\mu > 0$?\n\nA. $$z^{t+1} = \\mathcal{H}_k\\!\\left( z^t - \\mu \\cdot \\frac{1}{m} \\sum_{i=1}^m \\big( \\lvert a_i^* z^t \\rvert^2 - y_i \\big) \\, a_i a_i^* z^t \\right)$$\n\nB. $$z^{t+1} = \\mathcal{H}_k\\!\\left( z^t - \\mu \\cdot \\frac{2}{m} \\sum_{i=1}^m \\big( \\lvert a_i^* z^t \\rvert^2 - y_i \\big) \\, a_i a_i^* z^t \\right)$$\n\nC. $$z^{t+1} = \\mathcal{H}_k\\!\\left( z^t - \\mu \\cdot \\frac{1}{m} \\sum_{i=1}^m \\big( \\lvert a_i^\\top z^t \\rvert^2 - y_i \\big) \\, a_i a_i^\\top z^t \\right)$$\n\nD. $$z^{t+1} = \\mathcal{H}_k\\!\\left( z^t - \\mu \\cdot \\frac{1}{m} \\sum_{i=1}^m \\big( \\lvert a_i^* z^t \\rvert^2 - y_i \\big) \\, a_i z^t \\right)$$\n\nE. $$z^{t+1} = \\mathcal{S}_\\lambda\\!\\left( z^t - \\mu \\cdot \\frac{1}{m} \\sum_{i=1}^m \\big( \\lvert a_i^* z^t \\rvert - \\sqrt{y_i} \\big) \\, a_i \\right)$$\n\nHere, $\\mathcal{S}_\\lambda$ denotes the complex soft-thresholding (shrinkage) operator with parameter $\\lambda > 0$. Select the correct option.",
            "solution": "The user wants to identify the correct formulation for one iteration of projected gradient descent for sparse phase retrieval.\n\n### Problem Validation\n\nFirst, I will validate the problem statement.\n\n**Step 1: Extract Givens**\n-   Unknown signal: $x_{\\star} \\in \\mathbb{C}^n$, $k$-sparse.\n-   Measurements: $y_i = \\lvert a_i^* x_{\\star} \\rvert^2$ for $i = 1, \\dots, m$.\n-   Sensing vectors: $a_i \\in \\mathbb{C}^n$.\n-   Number of measurements: $m \\ge n$.\n-   Loss function: $f(z) = \\frac{1}{2m} \\sum_{i=1}^m \\left( \\lvert a_i^* z \\rvert^2 - y_i \\right)^2$.\n-   Decision variable: $z \\in \\mathbb{C}^n$.\n-   Sparsity constraint: The solution must be in $\\Sigma_k \\subset \\mathbb{C}^n$, the set of $k$-sparse vectors.\n-   Projection operator: $\\mathcal{H}_k : \\mathbb{C}^n \\to \\Sigma_k$ is the hard-thresholding operator.\n-   Matrix definition: $M_i = a_i a_i^*$.\n-   Algorithm: Projected gradient descent with a constant step size $\\mu > 0$.\n-   Gradient definition: The steepest descent direction is given by the Wirtinger derivative $\\nabla f(z) := \\partial f(z)/\\partial \\bar{z}$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem describes a standard formulation of sparse phase retrieval using a least-squares loss on the intensities. The use of Wirtinger calculus for optimizing real-valued functions of complex variables is the standard and correct mathematical tool. Projected gradient descent with hard thresholding is a common algorithm for this task, known as Iterative Hard Thresholding (IHT). The problem is firmly grounded in the fields of signal processing, optimization, and compressed sensing.\n-   **Well-Posed:** The question asks for the derivation of a specific algorithmic update step. Given the loss function, the gradient definition, and the projection operator, this is a well-defined mathematical task.\n-   **Objective:** The problem is stated using precise mathematical language and standard definitions. There is no ambiguity or subjective content.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, and objective, providing all necessary information to derive the solution. I will proceed with the derivation.\n\n### Derivation of the Update Rule\n\nThe algorithm is projected gradient descent. One iteration, starting from the current estimate $z^t$, is given by:\n1.  Compute the gradient descent step: $z_{grad} = z^t - \\mu \\nabla f(z^t)$.\n2.  Project the result onto the constraint set: $z^{t+1} = \\mathcal{H}_k(z_{grad})$.\n\nThe core of the task is to compute the Wirtinger gradient $\\nabla f(z) = \\partial f(z)/\\partial \\bar{z}$. The loss function is:\n$$\nf(z) = \\frac{1}{2m} \\sum_{i=1}^m \\left( \\lvert a_i^* z \\rvert^2 - y_i \\right)^2\n$$\nThe gradient with respect to $\\bar{z}$ is found by differentiating term-by-term:\n$$\n\\nabla f(z) = \\frac{\\partial f(z)}{\\partial \\bar{z}} = \\frac{1}{2m} \\sum_{i=1}^m \\frac{\\partial}{\\partial \\bar{z}} \\left( \\lvert a_i^* z \\rvert^2 - y_i \\right)^2\n$$\nWe apply the chain rule for Wirtinger derivatives. Let $u_i(z, \\bar{z}) = \\lvert a_i^* z \\rvert^2 - y_i$. Then the term in the sum is $(u_i)^2$.\n$$\n\\frac{\\partial (u_i)^2}{\\partial \\bar{z}} = 2 u_i \\frac{\\partial u_i}{\\partial \\bar{z}} = 2 \\left( \\lvert a_i^* z \\rvert^2 - y_i \\right) \\frac{\\partial}{\\partial \\bar{z}} \\left( \\lvert a_i^* z \\rvert^2 - y_i \\right)\n$$\nSince $y_i$ is a constant, its derivative is zero. We only need to find the derivative of $\\lvert a_i^* z \\rvert^2$.\nThis term can be written as a quadratic form:\n$$\n\\lvert a_i^* z \\rvert^2 = (a_i^* z)^* (a_i^* z) = (z^* a_i) (a_i^* z) = z^* (a_i a_i^*) z\n$$\nThe matrix $M_i = a_i a_i^*$ is Hermitian, since $M_i^* = (a_i a_i^*)^* = (a_i^*)^* a_i^* = a_i a_i^* = M_i$.\nFor a general Hermitian matrix $Q$, the Wirtinger derivative of the quadratic form $z^* Q z$ with respect to $\\bar{z}$ is a standard result:\n$$\n\\frac{\\partial (z^* Q z)}{\\partial \\bar{z}} = Q z\n$$\nApplying this result with $Q = M_i = a_i a_i^*$, we get:\n$$\n\\frac{\\partial}{\\partial \\bar{z}} \\left( \\lvert a_i^* z \\rvert^2 \\right) = (a_i a_i^*) z\n$$\nSubstituting this back into the chain rule expression:\n$$\n\\frac{\\partial}{\\partial \\bar{z}} \\left( \\lvert a_i^* z \\rvert^2 - y_i \\right)^2 = 2 \\left( \\lvert a_i^* z \\rvert^2 - y_i \\right) (a_i a_i^*) z\n$$\nNow, we can assemble the full gradient of $f(z)$:\n$$\n\\nabla f(z) = \\frac{1}{2m} \\sum_{i=1}^m 2 \\left( \\lvert a_i^* z \\rvert^2 - y_i \\right) (a_i a_i^*) z\n$$\nThe factors of $2$ cancel, yielding:\n$$\n\\nabla f(z) = \\frac{1}{m} \\sum_{i=1}^m \\left( \\lvert a_i^* z \\rvert^2 - y_i \\right) (a_i a_i^*) z\n$$\nFinally, we construct the projected gradient descent update rule at iteration $t$:\n$$\nz^{t+1} = \\mathcal{H}_k \\left( z^t - \\mu \\nabla f(z^t) \\right)\n$$\n$$\nz^{t+1} = \\mathcal{H}_k \\left( z^t - \\mu \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left( \\lvert a_i^* z^t \\rvert^2 - y_i \\right) (a_i a_i^*) z^t \\right)\n$$\nThis is the correct update rule based on the problem specification.\n\n### Option-by-Option Analysis\n\nNow I will evaluate each option against the derived result.\n\n**A.** $$z^{t+1} = \\mathcal{H}_k\\!\\left( z^t - \\mu \\cdot \\frac{1}{m} \\sum_{i=1}^m \\big( \\lvert a_i^* z^t \\rvert^2 - y_i \\big) \\, a_i a_i^* z^t \\right)$$\nThis expression perfectly matches the derived update rule. The Wirtinger gradient calculation is correct, including the leading factor of $\\frac{1}{m}$, and the projection operator $\\mathcal{H}_k$ is correctly applied.\n**Verdict: Correct.**\n\n**B.** $$z^{t+1} = \\mathcal{H}_k\\!\\left( z^t - \\mu \\cdot \\frac{2}{m} \\sum_{i=1}^m \\big( \\lvert a_i^* z^t \\rvert^2 - y_i \\big) \\, a_i a_i^* z^t \\right)$$\nThis option contains an erroneous factor of $2$ in the gradient term. The factor of $2$ from the chain rule of differentiating the squared term is cancelled by the factor of $\\frac{1}{2}$ in the definition of the loss function $f(z) = \\frac{1}{2m} \\sum(\\dots)^2$. The correct pre-factor for the sum is $\\frac{1}{m}$.\n**Verdict: Incorrect.**\n\n**C.** $$z^{t+1} = \\mathcal{H}_k\\!\\left( z^t - \\mu \\cdot \\frac{1}{m} \\sum_{i=1}^m \\big( \\lvert a_i^\\top z^t \\rvert^2 - y_i \\big) \\, a_i a_i^\\top z^t \\right)$$\nThis option incorrectly uses the transpose operator ($^\\top$) instead of the conjugate transpose operator ($^*$). For complex-valued vectors, the inner product and the induced norm are defined using the conjugate transpose. The problem statement itself defines the measurements using the conjugate transpose ($y_i = \\lvert a_i^* x_{\\star} \\rvert^2$). Using the transpose is mathematically incorrect in this context.\n**Verdict: Incorrect.**\n\n**D.** $$z^{t+1} = \\mathcal{H}_k\\!\\left( z^t - \\mu \\cdot \\frac{1}{m} \\sum_{i=1}^m \\big( \\lvert a_i^* z^t \\rvert^2 - y_i \\big) \\, a_i z^t \\right)$$\nThis option has an ill-formed gradient term. The expression $a_i z^t$ is not a standard vector or matrix operation. If it were element-wise multiplication, it would be denoted differently. The correct gradient involves the matrix-vector product $(a_i a_i^*) z^t$. The term $a_i z^t$ is dimensionally and operationally inconsistent with the result from Wirtinger calculus.\n**Verdict: Incorrect.**\n\n**E.** $$z^{t+1} = \\mathcal{S}_\\lambda\\!\\left( z^t - \\mu \\cdot \\frac{1}{m} \\sum_{i=1}^m \\big( \\lvert a_i^* z^t \\rvert - \\sqrt{y_i} \\big) \\, a_i \\right)$$\nThis option is incorrect for multiple reasons.\n1.  It uses the soft-thresholding operator $\\mathcal{S}_\\lambda$, which is the proximal operator for the $\\ell_1$-norm, not the hard-thresholding operator $\\mathcal{H}_k$ specified in the problem for enforcing $k$-sparsity.\n2.  The gradient term is derived from an amplitude-based loss function, $\\sum (\\lvert a_i^* z \\rvert - \\sqrt{y_i})^2$, not the intensity-based loss function, $\\sum (\\lvert a_i^* z \\rvert^2 - y_i)^2$, given in the problem.\n3.  Even for the amplitude-based loss, the gradient term shown is not correct.\nThis update rule corresponds to a different algorithm (like FISTA or ISTA) applied to a different optimization problem.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "After designing an algorithm, we must ask: will it converge to the correct solution? For non-convex problems like phase retrieval, algorithms can get trapped in 'spurious' solutions that are not the true answer. This practice provides a hands-on look at this critical issue, where you will analyze a simplified loss landscape and mathematically prove the existence of a misleading stationary point .",
            "id": "3477917",
            "problem": "Consider sparse phase retrieval under non-Gaussian, structured measurements. The goal is to recover an unknown sparse signal $x_{0} \\in \\mathbb{R}^{n}$ from intensity-only measurements $y_{i}^{2} = |a_{i}^{\\top} x_{0}|^{2}$, where $a_{i} \\in \\mathbb{R}^{n}$ are measurement vectors. A common approach (Wirtinger Flow, WF) performs gradient descent on the quartic intensity loss\n$$\nf(z) = \\frac{1}{4 m} \\sum_{i=1}^{m} \\left( |a_{i}^{\\top} z|^{2} - |a_{i}^{\\top} x_{0}|^{2} \\right)^{2},\n$$\nwith $z \\in \\mathbb{R}^{n}$ as the reconstruction variable. In the real-valued setting, the Wirtinger gradient coincides with the standard gradient, and critical points satisfy $\\nabla f(z) = 0$. In regimes where the measurements are non-Gaussian or structurally coherent, spurious stationary points can exist outside the basin of attraction of the global minimizers.\n\nConstruct a concrete counterexample that exhibits a spurious stationary point outside the basin by specifying a sparse ground truth $x_{0} \\in \\mathbb{R}^{2}$ and three structured measurement vectors $a_{1}, a_{2}, a_{3} \\in \\mathbb{R}^{2}$, and then analyzing the induced intensity loss $f$. Use the following data:\n- Choose $x_{0} = (1, 0)^{\\top}$, which is $1$-sparse.\n- Choose $m = 3$ and the structured, non-Gaussian measurement vectors $a_{1} = (1, 0)^{\\top}$, $a_{2} = (0, 1)^{\\top}$, and $a_{3} = (1, 1)^{\\top}$.\n\nTasks:\n1. Starting from the definition of $f$, write $f(z)$ explicitly as a function of $z = (z_{1}, z_{2})^{\\top} \\in \\mathbb{R}^{2}$, using only fundamental rules of differentiation and algebra.\n2. Derive the gradient $\\nabla f(z)$ for general $z$ in the real-valued setting and then evaluate it at $z = (0, 0)^{\\top}$ to demonstrate that $z = (0, 0)^{\\top}$ is a stationary point. Do not assume or use any prepackaged gradient formulas beyond the chain rule and the product rule.\n3. Compute $f(0, 0)$ and compare it to $f(x_{0})$ to show that $z = (0, 0)^{\\top}$ is not a global minimizer.\n4. Compute the Hessian matrix $\\nabla^{2} f(z)$ at $z = (0, 0)^{\\top}$ and determine its smallest eigenvalue in exact form. Provide the smallest eigenvalue as your final answer.\n\nThe final answer must be the smallest eigenvalue of $\\nabla^{2} f(0, 0)$ as a single closed-form analytic expression. No rounding is required. No units are involved.",
            "solution": "The problem asks us to analyze the loss function for a specific instance of sparse phase retrieval, demonstrate the existence of a spurious stationary point, and characterize its nature by computing the Hessian matrix and its eigenvalues.\n\nFirst, we validate the problem statement.\nThe problem provides a signal $x_0 \\in \\mathbb{R}^n$, measurement vectors $a_i \\in \\mathbb{R}^n$, and a loss function $f(z)$. The given values are $n=2$, $x_0=(1,0)^\\top$, $m=3$, $a_1=(1,0)^\\top$, $a_2=(0,1)^\\top$, and $a_3=(1,1)^\\top$. The function $f(z) = \\frac{1}{4 m} \\sum_{i=1}^{m} \\left( |a_{i}^{\\top} z|^{2} - |a_{i}^{\\top} x_{0}|^{2} \\right)^{2}$ is standard in phase retrieval literature. The problem is scientifically grounded in optimization and signal processing theory, is well-posed with all necessary information provided, and is stated objectively. The setup is a valid and common technique for illustrating the challenges of non-convex optimization in this domain. The problem is deemed valid.\n\nWe proceed with the four tasks outlined in the problem.\n\nFirst, we pre-compute the true measurement intensities, $|a_{i}^{\\top} x_{0}|^{2}$, which are constants in the loss function $f(z)$. Let $y_i^2 = |a_{i}^{\\top} x_{0}|^{2}$.\nWith $x_0 = (1, 0)^\\top$:\nFor $i=1$: $a_1^\\top x_0 = (1, 0) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$. Thus, $y_1^2 = 1^2 = 1$.\nFor $i=2$: $a_2^\\top x_0 = (0, 1) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 0$. Thus, $y_2^2 = 0^2 = 0$.\nFor $i=3$: $a_3^\\top x_0 = (1, 1) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$. Thus, $y_3^2 = 1^2 = 1$.\n\nNext, we express the terms $|a_i^\\top z|^2$ for a general variable $z = (z_1, z_2)^\\top$.\nFor $i=1$: $a_1^\\top z = (1, 0) \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix} = z_1$. Thus, $|a_1^\\top z|^2 = z_1^2$.\nFor $i=2$: $a_2^\\top z = (0, 1) \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix} = z_2$. Thus, $|a_2^\\top z|^2 = z_2^2$.\nFor $i=3$: $a_3^\\top z = (1, 1) \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix} = z_1+z_2$. Thus, $|a_3^\\top z|^2 = (z_1+z_2)^2$.\n\nTask 1: Write $f(z)$ explicitly.\nSubstituting these expressions into the definition of $f(z)$ with $m=3$:\n$$\nf(z) = \\frac{1}{4 \\cdot 3} \\sum_{i=1}^{3} \\left( |a_{i}^{\\top} z|^{2} - y_{i}^{2} \\right)^{2}\n$$\n$$\nf(z_1, z_2) = \\frac{1}{12} \\left[ \\left( |a_1^\\top z|^2 - y_1^2 \\right)^2 + \\left( |a_2^\\top z|^2 - y_2^2 \\right)^2 + \\left( |a_3^\\top z|^2 - y_3^2 \\right)^2 \\right]\n$$\n$$\nf(z_1, z_2) = \\frac{1}{12} \\left[ (z_1^2 - 1)^2 + (z_2^2 - 0)^2 + ((z_1+z_2)^2 - 1)^2 \\right]\n$$\n$$\nf(z_1, z_2) = \\frac{1}{12} \\left( (z_1^2 - 1)^2 + z_2^4 + ((z_1+z_2)^2 - 1)^2 \\right)\n$$\n\nTask 2: Derive the gradient $\\nabla f(z)$ and evaluate at $z=(0,0)^\\top$.\nWe compute the partial derivatives of $f(z_1, z_2)$ with respect to $z_1$ and $z_2$.\nFor $z_1$:\n$$\n\\frac{\\partial f}{\\partial z_1} = \\frac{1}{12} \\frac{\\partial}{\\partial z_1} \\left( (z_1^2 - 1)^2 + z_2^4 + ((z_1+z_2)^2 - 1)^2 \\right)\n$$\nUsing the chain rule:\n$$\n\\frac{\\partial f}{\\partial z_1} = \\frac{1}{12} \\left[ 2(z_1^2-1) \\cdot (2z_1) + 0 + 2((z_1+z_2)^2 - 1) \\cdot \\frac{\\partial}{\\partial z_1}((z_1+z_2)^2) \\right]\n$$\n$$\n\\frac{\\partial f}{\\partial z_1} = \\frac{1}{12} \\left[ 4z_1(z_1^2-1) + 2((z_1+z_2)^2 - 1) \\cdot 2(z_1+z_2) \\cdot 1 \\right]\n$$\n$$\n\\frac{\\partial f}{\\partial z_1} = \\frac{1}{3} \\left[ z_1(z_1^2-1) + (z_1+z_2)((z_1+z_2)^2-1) \\right]\n$$\nFor $z_2$:\n$$\n\\frac{\\partial f}{\\partial z_2} = \\frac{1}{12} \\frac{\\partial}{\\partial z_2} \\left( (z_1^2 - 1)^2 + z_2^4 + ((z_1+z_2)^2 - 1)^2 \\right)\n$$\n$$\n\\frac{\\partial f}{\\partial z_2} = \\frac{1}{12} \\left[ 0 + 4z_2^3 + 2((z_1+z_2)^2-1) \\cdot \\frac{\\partial}{\\partial z_2}((z_1+z_2)^2) \\right]\n$$\n$$\n\\frac{\\partial f}{\\partial z_2} = \\frac{1}{12} \\left[ 4z_2^3 + 2((z_1+z_2)^2-1) \\cdot 2(z_1+z_2) \\cdot 1 \\right]\n$$\n$$\n\\frac{\\partial f}{\\partial z_2} = \\frac{1}{3} \\left[ z_2^3 + (z_1+z_2)((z_1+z_2)^2-1) \\right]\n$$\nThe gradient is $\\nabla f(z) = \\begin{pmatrix} \\frac{\\partial f}{\\partial z_1} \\\\ \\frac{\\partial f}{\\partial z_2} \\end{pmatrix}$.\nNow, we evaluate the gradient at $z=(0, 0)^\\top$:\n$$\n\\frac{\\partial f}{\\partial z_1}\\bigg|_{(0,0)} = \\frac{1}{3} \\left[ 0(0^2-1) + (0+0)((0+0)^2-1) \\right] = 0\n$$\n$$\n\\frac{\\partial f}{\\partial z_2}\\bigg|_{(0,0)} = \\frac{1}{3} \\left[ 0^3 + (0+0)((0+0)^2-1) \\right] = 0\n$$\nSince $\\nabla f(0,0) = (0,0)^\\top$, the point $z=(0,0)^\\top$ is a stationary point (a critical point) of the function $f(z)$.\n\nTask 3: Compute $f(0,0)$ and $f(x_0)$.\nThe value of the loss function at the stationary point $z=(0,0)^\\top$ is:\n$$\nf(0,0) = \\frac{1}{12} \\left( (0^2 - 1)^2 + 0^4 + ((0+0)^2 - 1)^2 \\right) = \\frac{1}{12} ((-1)^2 + 0 + (-1)^2) = \\frac{1}{12}(1+1) = \\frac{2}{12} = \\frac{1}{6}\n$$\nThe global minimizers of $f(z)$ are vectors $z$ such that $|a_i^\\top z|^2 = |a_i^\\top x_0|^2$ for all $i$, which yields $f(z)=0$. The ground truth signal $x_0$ is one such minimizer. Let's evaluate $f(x_0) = f(1,0)$:\n$$\nf(1,0) = \\frac{1}{12} \\left( (1^2 - 1)^2 + 0^4 + ((1+0)^2 - 1)^2 \\right) = \\frac{1}{12} (0^2 + 0 + (1-1)^2) = 0\n$$\nSince $f(0,0) = 1/6 > f(x_0) = 0$, the stationary point $z = (0,0)^\\top$ is not a global minimizer. It is a spurious stationary point.\n\nTask 4: Compute the Hessian $\\nabla^2 f(0,0)$ and its smallest eigenvalue.\nThe Hessian matrix is $\\nabla^2 f(z) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial z_1^2} & \\frac{\\partial^2 f}{\\partial z_1 \\partial z_2} \\\\ \\frac{\\partial^2 f}{\\partial z_2 \\partial z_1} & \\frac{\\partial^2 f}{\\partial z_2^2} \\end{pmatrix}$. We compute the second partial derivatives from the components of the gradient.\n$\\frac{\\partial^2 f}{\\partial z_1^2} = \\frac{\\partial}{\\partial z_1} \\left( \\frac{1}{3} \\left[ z_1^3-z_1 + (z_1+z_2)^3-(z_1+z_2) \\right] \\right) = \\frac{1}{3} \\left( 3z_1^2-1 + 3(z_1+z_2)^2-1 \\right)$.\n$\\frac{\\partial^2 f}{\\partial z_2^2} = \\frac{\\partial}{\\partial z_2} \\left( \\frac{1}{3} \\left[ z_2^3 + (z_1+z_2)^3-(z_1+z_2) \\right] \\right) = \\frac{1}{3} \\left( 3z_2^2 + 3(z_1+z_2)^2-1 \\right)$.\n$\\frac{\\partial^2 f}{\\partial z_2 \\partial z_1} = \\frac{\\partial}{\\partial z_2} \\left( \\frac{1}{3} \\left[ z_1^3-z_1 + (z_1+z_2)^3-(z_1+z_2) \\right] \\right) = \\frac{1}{3} \\left( 3(z_1+z_2)^2-1 \\right)$.\nBy Clairaut's theorem, $\\frac{\\partial^2 f}{\\partial z_1 \\partial z_2} = \\frac{\\partial^2 f}{\\partial z_2 \\partial z_1}$.\n\nNow, we evaluate these at $z=(0,0)^\\top$:\n$\\frac{\\partial^2 f}{\\partial z_1^2}\\bigg|_{(0,0)} = \\frac{1}{3}(3 \\cdot 0^2 - 1 + 3(0+0)^2 - 1) = \\frac{1}{3}(-1-1) = -\\frac{2}{3}$.\n$\\frac{\\partial^2 f}{\\partial z_2^2}\\bigg|_{(0,0)} = \\frac{1}{3}(3 \\cdot 0^2 + 3(0+0)^2 - 1) = \\frac{1}{3}(-1) = -\\frac{1}{3}$.\n$\\frac{\\partial^2 f}{\\partial z_2 \\partial z_1}\\bigg|_{(0,0)} = \\frac{1}{3}(3(0+0)^2 - 1) = \\frac{1}{3}(-1) = -\\frac{1}{3}$.\n\nThe Hessian matrix at the origin is:\n$$\n\\nabla^2 f(0,0) = \\begin{pmatrix} -\\frac{2}{3} & -\\frac{1}{3} \\\\ -\\frac{1}{3} & -\\frac{1}{3} \\end{pmatrix}\n$$\nTo find the eigenvalues $\\lambda$, we solve the characteristic equation $\\det(\\nabla^2 f(0,0) - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} -\\frac{2}{3} - \\lambda & -\\frac{1}{3} \\\\ -\\frac{1}{3} & -\\frac{1}{3} - \\lambda \\end{pmatrix} = 0\n$$\n$$\n\\left(-\\frac{2}{3} - \\lambda\\right)\\left(-\\frac{1}{3} - \\lambda\\right) - \\left(-\\frac{1}{3}\\right)\\left(-\\frac{1}{3}\\right) = 0\n$$\n$$\n\\left(\\lambda + \\frac{2}{3}\\right)\\left(\\lambda + \\frac{1}{3}\\right) - \\frac{1}{9} = 0\n$$\n$$\n\\lambda^2 + \\frac{1}{3}\\lambda + \\frac{2}{3}\\lambda + \\frac{2}{9} - \\frac{1}{9} = 0\n$$\n$$\n\\lambda^2 + \\lambda + \\frac{1}{9} = 0\n$$\nWe use the quadratic formula $\\lambda = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$ with $a=1$, $b=1$, and $c=1/9$:\n$$\n\\lambda = \\frac{-1 \\pm \\sqrt{1^2 - 4(1)\\left(\\frac{1}{9}\\right)}}{2(1)} = \\frac{-1 \\pm \\sqrt{1 - \\frac{4}{9}}}{2} = \\frac{-1 \\pm \\sqrt{\\frac{5}{9}}}{2} = \\frac{-1 \\pm \\frac{\\sqrt{5}}{3}}{2}\n$$\nThe two eigenvalues are:\n$$\n\\lambda_1 = \\frac{-1 + \\frac{\\sqrt{5}}{3}}{2} = \\frac{-3 + \\sqrt{5}}{6} \\quad \\text{and} \\quad \\lambda_2 = \\frac{-1 - \\frac{\\sqrt{5}}{3}}{2} = \\frac{-3 - \\sqrt{5}}{6}\n$$\nTo find the smallest eigenvalue, we compare $\\lambda_1$ and $\\lambda_2$. Since $\\sqrt{5} > 0$, the term $-3-\\sqrt{5}$ is smaller than $-3+\\sqrt{5}$. Therefore, the smallest eigenvalue is $\\lambda_2$.\n$$\n\\lambda_{\\min} = \\frac{-3 - \\sqrt{5}}{6}\n$$\nBoth eigenvalues are negative, indicating that the stationary point at the origin is a local maximum, which repels gradient descent algorithms, causing them to diverge from this point. This is a common characteristic of spurious stationary points in phase retrieval problems.",
            "answer": "$$\n\\boxed{\\frac{-3 - \\sqrt{5}}{6}}\n$$"
        },
        {
            "introduction": "A theoretically sound algorithm is only useful if it runs in a reasonable amount of time. This final practice shifts our focus to computational efficiency, a cornerstone of algorithm design for large-scale blind deconvolution problems. You will analyze the per-iteration cost of a modern factorized gradient descent method and see firsthand how the Fast Fourier Transform (FFT) is leveraged to make the computation feasible .",
            "id": "3477971",
            "problem": "Consider circular blind deconvolution in the factorized parameterization for signals of length $n$, where the observation model is $y = h * x + w$, with $h \\in \\mathbb{C}^{n}$ and $x \\in \\mathbb{C}^{n}$ unknown, $w \\in \\mathbb{C}^{n}$ noise, and $*$ denoting circular convolution. Let the objective be the least-squares loss $f(h,x) = \\frac{1}{2}\\|h * x - y\\|_{2}^{2}$, possibly combined with a sparsity-promoting regularizer that is separable and incurs only elementwise operations of cost $O(n)$ per iteration. Assume complex variables and gradients are computed using Wirtinger calculus. The algorithm uses factorized gradient descent updates on $(h,x)$, and all convolutions and correlations are implemented via the Discrete Fourier Transform using the Fast Fourier Transform (FFT).\n\nYou may use the following foundational facts:\n- The Fast Fourier Transform (FFT) computes the Discrete Fourier Transform (DFT) in $O(n \\log n)$ time.\n- Circular convolution in the time domain corresponds to pointwise multiplication in the frequency domain, i.e., if $\\widehat{h} = \\mathcal{F}(h)$ and $\\widehat{x} = \\mathcal{F}(x)$, then $\\mathcal{F}(h * x) = \\widehat{h} \\odot \\widehat{x}$, where $\\odot$ denotes elementwise multiplication.\n- The gradient of $\\frac{1}{2}\\|r\\|_{2}^{2}$ with respect to its argument $r$ is $r$, and the adjoint of circular convolution by $x$ is circular correlation by $\\tilde{x}$, the conjugate-reversal of $x$.\n\nIn one iteration of factorized gradient descent, the following operations are performed:\n- Compute $\\widehat{h}$ and $\\widehat{x}$ by applying the FFT to $h$ and $x$, respectively. The Fourier transform of $y$, denoted $\\widehat{y}$, is precomputed once and reused for all iterations.\n- Form the residual in the frequency domain as $\\widehat{r} = \\widehat{h} \\odot \\widehat{x} - \\widehat{y}$.\n- Compute the gradients via frequency-domain multiplication followed by inverse FFTs:\n  - $\\widehat{\\nabla_{h} f} = \\overline{\\widehat{x}} \\odot \\widehat{r}$ and then $\\nabla_{h} f = \\mathcal{F}^{-1}(\\widehat{\\nabla_{h} f})$,\n  - $\\widehat{\\nabla_{x} f} = \\overline{\\widehat{h}} \\odot \\widehat{r}$ and then $\\nabla_{x} f = \\mathcal{F}^{-1}(\\widehat{\\nabla_{x} f})$,\n  where the overline denotes complex conjugation.\n- Perform the parameter updates and any elementwise regularization steps in the time domain.\n\nAssume all pointwise multiplications and additions in the frequency and time domains are $O(n)$, the FFT and inverse FFT each cost $O(n \\log n)$, and $\\widehat{y}$ is not recomputed inside the iteration. Under these assumptions, derive a closed-form analytic expression for the per-iteration arithmetic complexity of this factorized gradient descent scheme in terms of $n \\log n$, ignoring lower-order $n$ terms and extracting an exact leading coefficient multiplying $n \\log n$. The final answer must be a single analytic expression in $n \\log n$ (no inequalities or asymptotic notation).",
            "solution": "The problem statement has been validated and is deemed to be self-contained, scientifically grounded, and well-posed. It presents a standard algorithmic procedure for blind deconvolution and asks for a computational complexity analysis based on a clear set of assumptions. The problem is valid, and a solution will be provided.\n\nThe objective is to determine the per-iteration computational complexity of the described factorized gradient descent algorithm. The problem specifies that we should only consider the highest-order complexity term, which arises from the Fast Fourier Transform (FFT) and its inverse (IFFT), and ignore all lower-order terms, such as those of complexity $O(n)$. The complexity of a single FFT or IFFT operation on a vector of length $n$ is given as being proportional to $n \\log n$. We are asked to find the exact leading coefficient that multiplies the $n \\log n$ term.\n\nWe will analyze the computational cost of each step within a single iteration of the algorithm as described.\n\n1.  **Step 1: Fourier transform of the current estimates.**\n    The iteration begins with the current estimates of the signals $h \\in \\mathbb{C}^{n}$ and $x \\in \\mathbb{C}^{n}$ in the time domain. The first step is to compute their Discrete Fourier Transforms (DFTs) using the FFT algorithm.\n    -   Computation of $\\widehat{h} = \\mathcal{F}(h)$: This requires one FFT operation.\n    -   Computation of $\\widehat{x} = \\mathcal{F}(x)$: This requires one FFT operation.\n    The total number of transforms in this step is $2$.\n\n2.  **Step 2: Formation of the residual in the frequency domain.**\n    The residual is computed as $\\widehat{r} = \\widehat{h} \\odot \\widehat{x} - \\widehat{y}$. This involves one elementwise multiplication ($\\odot$) and one elementwise subtraction. Both operations act on vectors of length $n$. The cost of this step is $O(n)$. As per the problem's instructions, we ignore lower-order terms.\n\n3.  **Step 3: Computation of the gradients.**\n    The gradients with respect to $h$ and $x$ are calculated. The procedure is specified as follows:\n\n    a. **Gradient with respect to $h$**:\n       -   First, the frequency-domain representation of the gradient is computed: $\\widehat{\\nabla_{h} f} = \\overline{\\widehat{x}} \\odot \\widehat{r}$. This involves elementwise complex conjugation and elementwise multiplication on vectors of length $n$. The cost is $O(n)$ and is therefore ignored.\n       -   Next, the gradient is transformed back to the time domain: $\\nabla_{h} f = \\mathcal{F}^{-1}(\\widehat{\\nabla_{h} f})$. This requires one IFFT operation.\n\n    b. **Gradient with respect to $x$**:\n       -   Similarly, the frequency-domain representation is computed: $\\widehat{\\nabla_{x} f} = \\overline{\\widehat{h}} \\odot \\widehat{r}$. This has a cost of $O(n)$, which is ignored.\n       -   The gradient is then transformed back to the time domain: $\\nabla_{x} f = \\mathcal{F}^{-1}(\\widehat{\\nabla_{x} f})$. This requires one IFFT operation.\n\n    The total number of transforms in this step is $1 + 1 = 2$. Both are IFFTs.\n\n4.  **Step 4: Parameter updates and regularization.**\n    The final step in the iteration involves updating the parameters $h$ and $x$ using the computed gradients, e.g., $h \\leftarrow h - \\mu \\nabla_{h} f$. The problem also states that any sparsity-promoting regularization is applied at this stage and consists of elementwise operations. Therefore, all operations in this step are elementwise on vectors of length $n$. The cost is $O(n)$ and is ignored.\n\n**Total Complexity Calculation**\n\nTo find the total per-iteration complexity, we sum the number of dominant operations, which are the FFTs and IFFTs.\n-   From Step 1: $2$ FFTs.\n-   From Step 3: $2$ IFFTs.\n\nThe total number of FFT or IFFT operations per iteration is $2 + 2 = 4$.\n\nThe problem states that the complexity of each such transform is proportional to $n \\log n$. By asking for the \"exact leading coefficient,\" the question requires us to express the total complexity as $K \\cdot n \\log n$, where $K$ is the total count of these high-cost transforms.\n\nBased on our analysis, the coefficient $K$ is $4$. Therefore, the total per-iteration arithmetic complexity, ignoring lower-order terms, is $4n \\log n$.",
            "answer": "$$\n\\boxed{4n \\log n}\n$$"
        }
    ]
}