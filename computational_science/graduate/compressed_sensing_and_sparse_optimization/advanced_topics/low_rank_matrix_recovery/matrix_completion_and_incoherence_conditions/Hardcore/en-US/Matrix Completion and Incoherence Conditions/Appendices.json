{
    "hands_on_practices": [
        {
            "introduction": "The success of matrix completion algorithms hinges on a crucial property known as incoherence, which ensures that the information in a low-rank matrix is spread out rather than concentrated in a few entries. To build a solid understanding of this concept, we will begin by calculating the coherence parameters for a matrix that is maximally coherent. This exercise () demonstrates the upper bound of coherence and provides a clear benchmark for the most challenging structures that matrix completion methods must handle.",
            "id": "3459273",
            "problem": "Consider positive integers $m$ and $n$ and the $m \\times n$ rank-$1$ matrix $M = e^{(m)}_{1} \\left(e^{(n)}_{1}\\right)^{\\top}$, where $e^{(m)}_{1} \\in \\mathbb{R}^{m}$ and $e^{(n)}_{1} \\in \\mathbb{R}^{n}$ denote the first standard basis vectors in $\\mathbb{R}^{m}$ and $\\mathbb{R}^{n}$, respectively. Let the singular value decomposition (SVD) denote the factorization of $M$ as $M = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$ have orthonormal columns spanning the column and row subspaces of $M$, respectively, and $\\Sigma \\in \\mathbb{R}^{r \\times r}$ contains the singular values, with $r = \\mathrm{rank}(M)$. Define the orthogonal projectors onto the column and row subspaces by $P_{U} = U U^{\\top}$ and $P_{V} = V V^{\\top}$. The coherence of a subspace with respect to the standard basis is defined, for rank $r$, by\n$$\n\\mu(U) = \\frac{m}{r} \\max_{1 \\leq i \\leq m} \\left\\| P_{U} e^{(m)}_{i} \\right\\|_{2}^{2}, \n\\qquad\n\\mu(V) = \\frac{n}{r} \\max_{1 \\leq j \\leq n} \\left\\| P_{V} e^{(n)}_{j} \\right\\|_{2}^{2}.\n$$\nStarting from these definitions and the SVD of $M$, compute the coherence parameters $\\mu(U)$ and $\\mu(V)$ for the matrix $M = e^{(m)}_{1} \\left(e^{(n)}_{1}\\right)^{\\top}$, and briefly explain why these values illustrate an extreme coherence configuration in the context of matrix completion. Report your final answer as the single row matrix containing the two values $\\left(\\mu(U), \\mu(V)\\right)$. No rounding is required, and your answer must be exact.",
            "solution": "We begin with the $m \\times n$ matrix $M = e^{(m)}_{1} \\left(e^{(n)}_{1}\\right)^{\\top}$. This is a rank-$1$ matrix because it is the outer product of two nonzero vectors. The singular value decomposition (SVD) of a rank-$1$ outer product $u v^{\\top}$ with $u \\in \\mathbb{R}^{m}$ and $v \\in \\mathbb{R}^{n}$ is given by $M = U \\Sigma V^{\\top}$ where $U$ and $V$ each have one column equal to the normalized versions of $u$ and $v$, respectively, and $\\Sigma$ contains the single nonzero singular value equal to $\\|u\\|_{2} \\|v\\|_{2}$. In our specific case, $u = e^{(m)}_{1}$ and $v = e^{(n)}_{1}$ already have unit norm, so the SVD is\n$$\nM = U \\Sigma V^{\\top}, \\quad U = e^{(m)}_{1}, \\quad V = e^{(n)}_{1}, \\quad \\Sigma = [1],\n$$\nwith rank $r = 1$. The column subspace $U$ is $\\mathrm{span}\\left(e^{(m)}_{1}\\right)$ and the row subspace $V$ is $\\mathrm{span}\\left(e^{(n)}_{1}\\right)$.\n\nThe orthogonal projectors onto these one-dimensional subspaces are\n$$\nP_{U} = U U^{\\top} = e^{(m)}_{1} \\left(e^{(m)}_{1}\\right)^{\\top}, \n\\qquad \nP_{V} = V V^{\\top} = e^{(n)}_{1} \\left(e^{(n)}_{1}\\right)^{\\top}.\n$$\nTo compute the coherence parameters, we evaluate the squared norms of the projected standard basis vectors. For $1 \\leq i \\leq m$,\n$$\nP_{U} e^{(m)}_{i} = \\left(e^{(m)}_{1} \\left(e^{(m)}_{1}\\right)^{\\top}\\right) e^{(m)}_{i} \n= e^{(m)}_{1} \\left\\langle e^{(m)}_{1}, e^{(m)}_{i} \\right\\rangle \n= \n\\begin{cases}\ne^{(m)}_{1},  i = 1, \\\\\n0,  i \\neq 1,\n\\end{cases}\n$$\nhence\n$$\n\\left\\|P_{U} e^{(m)}_{i}\\right\\|_{2}^{2} = \n\\begin{cases}\n1,  i = 1, \\\\\n0,  i \\neq 1.\n\\end{cases}\n$$\nTherefore,\n$$\n\\max_{1 \\leq i \\leq m} \\left\\| P_{U} e^{(m)}_{i} \\right\\|_{2}^{2} = 1,\n$$\nand with $r = 1$,\n$$\n\\mu(U) = \\frac{m}{r} \\max_{i} \\left\\| P_{U} e^{(m)}_{i} \\right\\|_{2}^{2} = m.\n$$\n\nAn analogous calculation holds for $P_{V}$. For $1 \\leq j \\leq n$,\n$$\nP_{V} e^{(n)}_{j} = \\left(e^{(n)}_{1} \\left(e^{(n)}_{1}\\right)^{\\top}\\right) e^{(n)}_{j} \n= e^{(n)}_{1} \\left\\langle e^{(n)}_{1}, e^{(n)}_{j} \\right\\rangle \n= \n\\begin{cases}\ne^{(n)}_{1},  j = 1, \\\\\n0,  j \\neq 1,\n\\end{cases}\n$$\nand thus\n$$\n\\left\\|P_{V} e^{(n)}_{j}\\right\\|_{2}^{2} = \n\\begin{cases}\n1,  j = 1, \\\\\n0,  j \\neq 1.\n\\end{cases}\n$$\nConsequently,\n$$\n\\max_{1 \\leq j \\leq n} \\left\\| P_{V} e^{(n)}_{j} \\right\\|_{2}^{2} = 1,\n$$\nand with $r = 1$,\n$$\n\\mu(V) = \\frac{n}{r} \\max_{j} \\left\\| P_{V} e^{(n)}_{j} \\right\\|_{2}^{2} = n.\n$$\n\nThese values illustrate extreme coherence because, for rank $r = 1$, the coherence satisfies the general bounds $1 \\leq \\mu(U) \\leq m$ and $1 \\leq \\mu(V) \\leq n$. The case computed here achieves the largest possible values, $\\mu(U) = m$ and $\\mu(V) = n$, meaning the subspaces are maximally aligned with single standard basis vectors. In matrix completion, such extreme coherence signifies that the information in $M$ is highly concentrated in a single coordinate in both the column and row spaces, which is precisely the pathological case where incoherence is violated most severely. The requested final answer is the row matrix containing these two values.",
            "answer": "$$\\boxed{\\begin{pmatrix} m  n \\end{pmatrix}}$$"
        },
        {
            "introduction": "Understanding why a low-rank matrix can be recovered from a small number of samples requires delving into the optimization problem's optimality conditions. This exercise () provides a hands-on opportunity to construct a 'dual certificate,' a key object used in the theoretical analysis of nuclear norm minimization. By explicitly verifying the optimality conditions for a small example, you will gain a concrete understanding of the machinery that guarantees exact recovery.",
            "id": "3459243",
            "problem": "Consider the matrix completion problem with equality constraints and nuclear norm regularization. Let the true unknown matrix be the rank-one matrix $X_{\\star} \\in \\mathbb{R}^{2 \\times 2}$ defined by the singular value decomposition $X_{\\star} = \\sigma\\, u v^{\\top},$ where $u = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}, \\quad v = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}, \\quad \\sigma = 2.$ Hence, $X_{\\star} = \\begin{pmatrix}1  1 \\\\ 1  1\\end{pmatrix}.$ The observed index set is $\\Omega = \\{(1,1),(1,2),(2,1)\\},$ and the sampling operator $\\mathcal{P}_{\\Omega}:\\mathbb{R}^{2\\times 2}\\to\\mathbb{R}^{2\\times 2}$ acts as $(\\mathcal{P}_{\\Omega}(X))_{ij} = \\begin{cases} X_{ij},  (i,j)\\in \\Omega, \\\\ 0,  \\text{otherwise.}\\end{cases}$ Consider the convex program $\\min_{X\\in \\mathbb{R}^{2\\times 2}} \\|X\\|_{\\ast} \\quad \\text{subject to} \\quad \\mathcal{P}_{\\Omega}(X) = \\mathcal{P}_{\\Omega}(X_{\\star}),$ where $\\|X\\|_{\\ast}$ is the nuclear norm (sum of singular values). Let $T$ denote the tangent space at $X_{\\star}$ to the rank-one manifold, $T = \\{u a^{\\top} + b v^{\\top}: a\\in\\mathbb{R}^{2},\\, b\\in\\mathbb{R}^{2}\\},$ and let $\\mathcal{P}_{T}$ and $\\mathcal{P}_{T^{\\perp}}$ denote the orthogonal projections onto $T$ and its orthogonal complement, respectively, with respect to the Frobenius inner product. Using only core definitions (convex optimality via Karush–Kuhn–Tucker conditions, the subgradient of the nuclear norm at a rank-one point, and the definition of the tangent space and its orthogonal complement), perform the following steps:\n\n1. Explicitly construct a dual certificate $Y \\in \\mathbb{R}^{2\\times 2}$ such that $Y \\in \\mathrm{range}(\\mathcal{P}_{\\Omega}), \\mathcal{P}_{T}(Y) = u v^{\\top}, \\|\\mathcal{P}_{T^{\\perp}}(Y)\\|_{2} \\leq 1,$ where $\\|\\cdot\\|_{2}$ is the spectral norm. Your construction must start from the requirement that $Y$ is supported on $\\Omega$ (i.e., $Y_{22}=0$) and from the characterization of the subgradient $\\partial \\|X_{\\star}\\|_{\\ast} = \\{u v^{\\top} + W: \\mathcal{P}_{T}(W)=0, \\|W\\|_{2} \\leq 1\\}.$\n\n2. Verify the Karush–Kuhn–Tucker (KKT) conditions for optimality by identifying a Lagrange multiplier $\\Lambda \\in \\mathbb{R}^{2 \\times 2}$ such that the stationarity condition holds and by checking primal feasibility. In particular, show that your $Y$ can be written as $Y = \\mathcal{P}_{\\Omega}(\\Lambda)$ with an appropriate sign convention to satisfy stationarity, and verify $\\langle Y, X_{\\star}\\rangle = \\|X_{\\star}\\|_{\\ast}.$\n\n3. Compute the quantity $\\|\\mathcal{P}_{T^{\\perp}}(Y)\\|_{2}.$\n\nProvide your final answer as a single real number with no units. No rounding is required; give the exact value.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It is a standard exercise in the theoretical analysis of matrix completion, based on established mathematical principles. All provided data and definitions are consistent and sufficient for a rigorous solution. Therefore, the problem is deemed valid.\n\nThe task is to construct a dual certificate $Y$ for the given matrix completion problem, verify its properties in the context of the Karush-Kuhn-Tucker (KKT) conditions, and compute the spectral norm of its projection onto the orthogonal complement of the tangent space.\n\nThe true matrix is $X_{\\star} = \\sigma u v^{\\top}$ where $\\sigma=2$ and $u=v=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$. This gives\n$$X_{\\star} = 2 \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}\\right) \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1\\end{pmatrix}\\right) = \\begin{pmatrix}1  1 \\\\ 1  1\\end{pmatrix}.$$\nThe nuclear norm is $\\|X_{\\star}\\|_{\\ast} = \\sigma = 2$. The observed entries are specified by $\\Omega = \\{(1,1),(1,2),(2,1)\\}$. The convex optimization problem is\n$$\\min_{X\\in \\mathbb{R}^{2\\times 2}} \\|X\\|_{\\ast} \\quad \\text{subject to} \\quad \\mathcal{P}_{\\Omega}(X) = \\mathcal{P}_{\\Omega}(X_{\\star}).$$\n\nA sufficient condition for $X_{\\star}$ to be the unique solution to this problem is the existence of a matrix $Y \\in \\mathbb{R}^{2\\times 2}$, known as a dual certificate, that satisfies the following conditions:\n1.  $Y$ is in the range of the sampling operator, $Y \\in \\mathrm{range}(\\mathcal{P}_{\\Omega})$. This means $Y_{ij}=0$ for $(i,j) \\notin \\Omega$. In this case, $Y_{22}=0$.\n2.  $Y$ is an element of the subgradient of the nuclear norm at $X_{\\star}$, denoted $\\partial \\|X_{\\star}\\|_{\\ast}$. For a rank-1 matrix $X_{\\star}=\\sigma u v^{\\top}$, the subgradient is given by $\\partial \\|X_{\\star}\\|_{\\ast} = \\{u v^{\\top} + W \\mid \\mathcal{P}_{T}(W)=0, \\|W\\|_{2} \\leq 1\\}$.\n\nThe tangent space $T$ at $X_{\\star}$ is $T = \\{u a^{\\top} + b v^{\\top}: a,b\\in\\mathbb{R}^{2}\\}$. Its orthogonal complement, $T^{\\perp}$, consists of matrices $Z$ such that $u^{\\top}Z=0$ and $Zv=0$. Since $u=v$, these conditions become $u^{\\top}Z=0$ and $Zu=0$. Let $u_{\\perp} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$ be a vector orthogonal to $u$. Any matrix in $T^{\\perp}$ must be of the form $c \\, u_{\\perp}u_{\\perp}^{\\top}$ for some scalar $c$.\n$$u_{\\perp}u_{\\perp}^{\\top} = \\frac{1}{2} \\begin{pmatrix}1 \\\\ -1\\end{pmatrix} \\begin{pmatrix}1  -1\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix}1  -1 \\\\ -1  1\\end{pmatrix}.$$\nLet $A = u_{\\perp}u_{\\perp}^{\\top}$. A matrix $W$ is in $T^{\\perp}$ if and only if it is a multiple of $A$. By definition, for any $W \\in T^{\\perp}$, $\\mathcal{P}_{T}(W)=0$.\n\n**Step 1: Construct the dual certificate $Y$**\n\nWe need to find $Y$ such that:\n(a) $Y_{22}=0$.\n(b) $\\mathcal{P}_{T}(Y) = uv^{\\top}$.\n(c) $\\|\\mathcal{P}_{T^{\\perp}}(Y)\\|_{2} \\leq 1$.\n\nAny matrix $Y$ can be decomposed as $Y = \\mathcal{P}_{T}(Y) + \\mathcal{P}_{T^{\\perp}}(Y)$. Using condition (b), this becomes $Y = uv^{\\top} + \\mathcal{P}_{T^{\\perp}}(Y)$.\nLet $W = \\mathcal{P}_{T^{\\perp}}(Y)$. Since $W \\in T^{\\perp}$, we can write $W = c u_{\\perp}u_{\\perp}^{\\top}$ for some scalar $c$.\n$$Y = uv^{\\top} + c u_{\\perp}u_{\\perp}^{\\top}.$$\nWe have $uv^{\\top} = \\frac{1}{2}\\begin{pmatrix}1  1 \\\\ 1  1\\end{pmatrix}$.\nThus,\n$$Y = \\frac{1}{2}\\begin{pmatrix}1  1 \\\\ 1  1\\end{pmatrix} + c \\frac{1}{2}\\begin{pmatrix}1  -1 \\\\ -1  1\\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix}1+c  1-c \\\\ 1-c  1+c\\end{pmatrix}.$$\nNow we apply condition (a), $Y_{22}=0$:\n$$\\frac{1}{2}(1+c) = 0 \\implies c = -1.$$\nSubstituting $c=-1$ back into the expression for $Y$, we obtain the dual certificate:\n$$Y = \\frac{1}{2}\\begin{pmatrix}1-1  1-(-1) \\\\ 1-(-1)  1-1\\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix}0  2 \\\\ 2  0\\end{pmatrix} = \\begin{pmatrix}0  1 \\\\ 1  0\\end{pmatrix}.$$\nThis matrix $Y$ is supported on $\\Omega$ since $Y_{11}=0, Y_{12}=1, Y_{21}=1$, and $Y_{22}=0$.\n\n**Step 2: Verify the KKT conditions**\n\nThe Lagrangian for the optimization problem can be written as $\\mathcal{L}(X, \\Lambda) = \\|X\\|_{\\ast} - \\langle \\Lambda, \\mathcal{P}_{\\Omega}(X) - \\mathcal{P}_{\\Omega}(X_{\\star}) \\rangle$. The stationarity condition at $X=X_{\\star}$ is $0 \\in \\partial_X \\mathcal{L}(X_{\\star}, \\Lambda)$, which implies $0 \\in \\partial \\|X_{\\star}\\|_{\\ast} - \\mathcal{P}_{\\Omega}(\\Lambda)$. This requires $\\mathcal{P}_{\\Omega}(\\Lambda)$ to be an element of the subgradient $\\partial \\|X_{\\star}\\|_{\\ast}$.\n\nLet us identify $Y = \\mathcal{P}_{\\Omega}(\\Lambda)$. We need to show that our constructed $Y$ is in $\\partial \\|X_{\\star}\\|_{\\ast}$. This means we must show $Y = uv^{\\top} + W$ where $\\mathcal{P}_{T}(W)=0$ and $\\|W\\|_{2} \\leq 1$.\n\nFrom our construction, $Y = uv^{\\top} + (-1)u_{\\perp}u_{\\perp}^{\\top}$.\nLet $W = -u_{\\perp}u_{\\perp}^{\\top} = -\\frac{1}{2}\\begin{pmatrix}1  -1 \\\\ -1  1\\end{pmatrix}$.\n- $\\mathcal{P}_{T}(W)=0$: This is true by definition, as $W$ is in $T^{\\perp}$.\n- $\\|W\\|_{2} \\leq 1$: We compute the spectral norm of $W$.\n$$ \\|W\\|_{2} = \\|-u_{\\perp}u_{\\perp}^{\\top}\\|_{2} = |{-1}| \\cdot \\|u_{\\perp}u_{\\perp}^{\\top}\\|_{2}. $$\nSince $u_{\\perp}$ is a unit vector, $u_{\\perp}u_{\\perp}^{\\top}$ is an orthogonal projection matrix, so its spectral norm is $1$. Alternatively, as a rank-1 matrix $u_{\\perp}u_{\\perp}^{\\top} = (u_{\\perp})(u_{\\perp}^{\\top})$, its only non-zero singular value is $\\|u_{\\perp}\\|_2 \\|u_{\\perp}\\|_2 = 1 \\cdot 1 = 1$.\nTherefore, $\\|W\\|_{2} = 1$. The condition $\\|W\\|_{2} \\leq 1$ is satisfied.\n\nThis confirms $Y \\in \\partial \\|X_{\\star}\\|_{\\ast}$. To complete the KKT verification, we can choose a Lagrange multiplier $\\Lambda$ such that $\\mathcal{P}_{\\Omega}(\\Lambda) = Y$. Since $Y$ is already in the range of $\\mathcal{P}_{\\Omega}$, we can simply choose $\\Lambda = Y = \\begin{pmatrix}0  1 \\\\ 1  0\\end{pmatrix}$.\nPrimal feasibility is satisfied by $X=X_{\\star}$ by construction of the problem.\nFinally, we verify the property $\\langle Y, X_{\\star}\\rangle = \\|X_{\\star}\\|_{\\ast}$ which is part of the subgradient definition.\n$$\\langle Y, X_{\\star}\\rangle = \\mathrm{Tr}(Y^{\\top}X_{\\star}) = \\mathrm{Tr}\\left(\\begin{pmatrix}0  1 \\\\ 1  0\\end{pmatrix} \\begin{pmatrix}1  1 \\\\ 1  1\\end{pmatrix}\\right) = \\mathrm{Tr}\\left(\\begin{pmatrix}1  1 \\\\ 1  1\\end{pmatrix}\\right) = 1+1=2.$$\nThe nuclear norm of $X_{\\star}$ is $\\|X_{\\star}\\|_{\\ast} = \\sigma = 2$.\nSince $\\langle Y, X_{\\star}\\rangle = \\|X_{\\star}\\|_{\\ast}$, the conditions for $X_{\\star}$ being an optimal solution are fully verified.\n\n**Step 3: Compute $\\|\\mathcal{P}_{T^{\\perp}}(Y)\\|_{2}$**\n\nFrom the decomposition $Y = \\mathcal{P}_{T}(Y) + \\mathcal{P}_{T^{\\perp}}(Y)$ and the construction $Y = uv^{\\top} + W$ with $W = -u_{\\perp}u_{\\perp}^{\\top}$, we identify the projection onto the orthogonal complement of the tangent space as:\n$$\\mathcal{P}_{T^{\\perp}}(Y) = W = -u_{\\perp}u_{\\perp}^{\\top}.$$\nThe quantity we need to compute is the spectral norm of this matrix:\n$$\\|\\mathcal{P}_{T^{\\perp}}(Y)\\|_{2} = \\|-u_{\\perp}u_{\\perp}^{\\top}\\|_{2}.$$\nAs calculated in Step 2, the spectral norm is:\n$$\\|-u_{\\perp}u_{\\perp}^{\\top}\\|_{2} = 1.$$\nThe value is exactly $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "The standard theory of matrix completion often assumes a simple uniform random sampling model, but real-world applications frequently feature structured or deterministic patterns of missing data. This practice problem () explores such a scenario, where an entire band of entries is missing, forcing you to connect the fundamental sample complexity requirements with the realities of non-uniform observation patterns. By deriving the necessary sampling probability, you will learn how to analyze the feasibility of recovery under practical, anisotropic constraints.",
            "id": "3459292",
            "problem": "Consider an unknown square matrix $M \\in \\mathbb{R}^{n \\times n}$ of rank $r$ with singular value decomposition $M = U \\Sigma V^{\\top}$. Assume the standard incoherence conditions hold with parameter $\\mu \\geq 1$: for the column spaces $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$, the coherence constraints satisfy $\\max_{1 \\leq i \\leq n} \\|P_{U} e_{i}\\|_{2}^{2} \\leq \\mu r / n$ and $\\max_{1 \\leq j \\leq n} \\|P_{V} e_{j}\\|_{2}^{2} \\leq \\mu r / n$, where $P_{U}$ and $P_{V}$ denote the orthogonal projectors onto the column spaces of $U$ and $V$, respectively, and $\\{e_{i}\\}_{i=1}^{n}$ is the standard basis of $\\mathbb{R}^{n}$. Observations are noiseless. A deterministic anisotropic mask removes all entries within a band of half-width $w \\in \\{0,1,\\ldots,n-1\\}$ around the main diagonal, that is, the entries $(i,j)$ with $|i-j| \\leq w$ are never observed. On the complement of this band, each entry is observed independently with probability $p \\in [0,1]$ (Bernoulli sampling restricted to the available indices). Recovery is performed by nuclear norm minimization.\n\nStarting only from the fundamental definitions above and the well-tested fact that in the isotropic case of Bernoulli sampling over all $n^{2}$ entries, on the order of $m \\asymp \\mu r n \\ln n$ samples suffice for exact recovery with high probability, derive the exact expression for the minimal sampling probability $p_{\\star}$ (as a function of $n$, $r$, $\\mu$, and $w$) such that the same sample complexity criterion is met under the band-missing anisotropic mask by sampling only on the available indices. Your derivation must explicitly count the number of deterministically removed entries induced by the missing band and reduce the requirement to a closed-form expression $p_{\\star}(n,r,\\mu,w)$ in terms of an absolute constant $C  0$. Provide the final answer as a single closed-form analytic expression. No rounding is required.",
            "solution": "The problem statement is assessed to be valid as it is scientifically grounded in the theory of matrix completion, well-posed, and objective. It presents a solvable theoretical task based on standard principles and definitions within the field. We proceed with the derivation.\n\nThe problem requires the derivation of a minimal sampling probability, $p_{\\star}$, for an anisotropic sampling scheme, such that the number of acquired samples meets the criterion established for the isotropic case. The provided benchmark for the isotropic Bernoulli sampling model is that the number of samples, $m$, required for exact recovery of a rank-$r$ matrix $M \\in \\mathbb{R}^{n \\times n}$ with incoherence $\\mu$ is on the order of $m \\asymp \\mu r n \\ln n$. We formalize this by introducing an absolute constant $C  0$ and setting the required number of samples to be:\n$$m_{\\text{req}} = C \\mu r n \\ln(n)$$\n\nThe sampling process is restricted. A deterministic anisotropic mask removes all entries $(i,j)$ within a band of half-width $w$ around the main diagonal, meaning entries where $|i-j| \\leq w$ are never observed. Sampling occurs only on the complement of this band. To determine the required sampling probability on the available entries, we must first calculate the number of available entries, $N_{\\text{avail}}$. This is the total number of entries, $n^2$, minus the number of deterministically removed entries, $N_{\\text{removed}}$.\n\nWe proceed to count the number of removed entries. The condition $|i-j| \\leq w$ for $w \\in \\{0, 1, \\ldots, n-1\\}$ defines a band around the main diagonal. This band consists of the main diagonal itself ($|i-j|=0$) and the first $w$ super-diagonals ($j-i=k$ for $k=1, \\ldots, w$) and sub-diagonals ($i-j=k$ for $k=1, \\ldots, w$).\nThe number of entries on the main diagonal is $n$.\nFor any $k \\in \\{1, \\ldots, n-1\\}$, the $k$-th super-diagonal (where $j=i+k$) contains $n-k$ entries. Similarly, the $k$-th sub-diagonal (where $i=j+k$) contains $n-k$ entries.\nThe total number of removed entries is the sum of the entries on the main diagonal and the $w$ adjacent diagonals on either side:\n$$N_{\\text{removed}} = n + \\sum_{k=1}^{w} (\\text{count on } k\\text{-th super-diag} + \\text{count on } k\\text{-th sub-diag})$$\n$$N_{\\text{removed}} = n + \\sum_{k=1}^{w} ((n-k) + (n-k)) = n + 2 \\sum_{k=1}^{w} (n-k)$$\nWe evaluate the summation:\n$$N_{\\text{removed}} = n + 2 \\left( \\sum_{k=1}^{w} n - \\sum_{k=1}^{w} k \\right) = n + 2 \\left( nw - \\frac{w(w+1)}{2} \\right)$$\n$$N_{\\text{removed}} = n + 2nw - w(w+1) = n + 2nw - w^2 - w$$\n\nThe number of available entries, $N_{\\text{avail}}$, is the total number of entries, $n^2$, less the number of removed entries:\n$$N_{\\text{avail}} = n^2 - N_{\\text{removed}} = n^2 - (n + 2nw - w^2 - w) = n^2 - 2nw - n + w^2 + w$$\nThis expression can be factored. A more direct way to count $N_{\\text{avail}}$ is to count the entries $(i,j)$ for which $|i-j|  w$. This is the sum of entries for which $j  i+w$ and $i  j+w$. The number of entries with $j  i+w$ is given by the sum $\\sum_{i=1}^{n-w-1} (n-(i+w))$. By a change of index, this sum evaluates to $\\frac{(n-w-1)(n-w)}{2}$. By symmetry, the number of entries with $i  j+w$ is identical. Thus, the total number of available entries is:\n$$N_{\\text{avail}} = 2 \\times \\frac{(n-w-1)(n-w)}{2} = (n-w)(n-w-1)$$\nThis confirms the factored form of our previous calculation. This expression is valid for $w \\in \\{0, 1, \\ldots, n-2\\}$. If $w=n-1$, all entries are removed, so $N_{\\text{avail}}=0$, and no sampling is possible.\n\nEach of these $N_{\\text{avail}}$ entries is observed independently with probability $p$. The expected number of observed samples, $m_{\\text{obs}}$, is therefore:\n$$m_{\\text{obs}} = p \\cdot N_{\\text{avail}} = p (n-w)(n-w-1)$$\n\nTo satisfy the sample complexity requirement for successful recovery, the expected number of observed samples must equal the required number of samples, $m_{\\text{req}}$. The minimal probability $p_{\\star}$ is the one that satisfies this condition:\n$$p_{\\star} (n-w)(n-w-1) = m_{\\text{req}}$$\n$$p_{\\star} (n-w)(n-w-1) = C \\mu r n \\ln(n)$$\n\nSolving for $p_{\\star}$ yields the final expression:\n$$p_{\\star} = \\frac{C \\mu r n \\ln(n)}{(n-w)(n-w-1)}$$\nThis expression represents the minimal sampling probability on the available entries required to achieve the target sample complexity, as a function of $n$, $r$, $\\mu$, $w$, and an absolute constant $C$. This expression is defined for $w  n-1$, as for $w \\geq n-1$ the number of available entries is zero, making it impossible to acquire any samples.",
            "answer": "$$\n\\boxed{\\frac{C \\mu r n \\ln(n)}{(n-w)(n-w-1)}}\n$$"
        }
    ]
}