{
    "hands_on_practices": [
        {
            "introduction": "To solve the elastic net optimization problem, modern algorithms often rely on iterative methods built around a fundamental tool: the proximal operator. This exercise guides you through deriving the proximal operator for the elastic net penalty from first principles. By completing this practice , you will uncover the elegant geometric action of this operator and see how it provides a blueprint for algorithms that find sparse and stable solutions.",
            "id": "3469107",
            "problem": "Consider the elastic-net penalty defined on $\\mathbb{R}^{n}$ by the convex function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ given by $f(x)=\\lambda_{1}\\|x\\|_{1}+\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}$, where $\\lambda_{1}>0$ and $\\lambda_{2}>0$. Let $\\tau>0$ and define the proximal operator $\\operatorname{prox}_{\\tau f}(y)$ by the standard variational characterization\n$$\n\\operatorname{prox}_{\\tau f}(y)=\\arg\\min_{x\\in\\mathbb{R}^{n}}\\left\\{\\frac{1}{2}\\|x-y\\|_{2}^{2}+\\tau\\lambda_{1}\\|x\\|_{1}+\\frac{\\tau\\lambda_{2}}{2}\\|x\\|_{2}^{2}\\right\\}.\n$$\nUse the definition of subdifferentials and first-order optimality conditions for convex functions to derive a closed-form expression for $\\operatorname{prox}_{\\tau f}(y)$, expressed coordinate-wise in terms of the soft-thresholding operator $S_{\\kappa}(z)$, defined for $z\\in\\mathbb{R}$ and $\\kappa\\geq 0$ by $S_{\\kappa}(z)=\\operatorname{sign}(z)\\max\\{|z|-\\kappa,0\\}$. Then, restrict attention to the case $n=2$ and provide a geometric interpretation of the mapping $\\operatorname{prox}_{\\tau f}:\\mathbb{R}^{2}\\to\\mathbb{R}^{2}$ as a composition of a radial shrinkage (uniform scaling toward the origin) and a coordinate-wise soft-thresholding, making precise the order and the parameters of these operations.\n\nFinally, consider the fixed-point iteration $x^{k+1}=T(x^{k})$ with $T=\\operatorname{prox}_{\\tau f}$. Analyze the fixed-point set $\\{x\\in\\mathbb{R}^{n}:x=T(x)\\}$ and, using the piecewise-linear structure of $T$ and properties of $S_{\\kappa}$, determine the tight global Lipschitz modulus (optimal contraction factor) of $T$ on $\\mathbb{R}^{n}$, expressed as a closed-form analytic function of $\\tau$ and $\\lambda_{2}$.\n\nReport the tight global Lipschitz modulus of $T$ as your final answer, in exact symbolic form. No rounding is required and no units are involved.",
            "solution": "The problem asks for the derivation and analysis of the proximal operator associated with the elastic-net penalty. The solution proceeds in four steps: (1) derivation of the closed-form expression for the proximal operator, (2) geometric interpretation in $\\mathbb{R}^{2}$, (3) analysis of the fixed points of the operator, and (4) determination of its tight global Lipschitz modulus.\n\n**1. Derivation of the Proximal Operator**\n\nThe elastic-net penalty is given by the function $f(x) = \\lambda_{1}\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}$ for $x \\in \\mathbb{R}^{n}$, with $\\lambda_{1} > 0$ and $\\lambda_{2} > 0$. The proximal operator $\\operatorname{prox}_{\\tau f}(y)$ for $\\tau > 0$ is defined as the unique minimizer of the following objective function:\n$$\nx^* = \\operatorname{prox}_{\\tau f}(y) = \\arg\\min_{x\\in\\mathbb{R}^{n}}\\left\\{ G(x, y) = \\frac{1}{2}\\|x-y\\|_{2}^{2} + \\tau f(x) \\right\\}.\n$$\nSubstituting the expression for $f(x)$, we get:\n$$\nG(x, y) = \\frac{1}{2}\\|x-y\\|_{2}^{2} + \\tau\\lambda_{1}\\|x\\|_{1} + \\frac{\\tau\\lambda_{2}}{2}\\|x\\|_{2}^{2}.\n$$\nWe can express the squared norms as sums over the components:\n$$\nG(x, y) = \\frac{1}{2}\\sum_{i=1}^{n}(x_i - y_i)^2 + \\tau\\lambda_{1}\\sum_{i=1}^{n}|x_i| + \\frac{\\tau\\lambda_{2}}{2}\\sum_{i=1}^{n}x_i^2.\n$$\nThe objective function is separable with respect to the components $x_i$, meaning we can write $G(x, y) = \\sum_{i=1}^{n} g_i(x_i, y_i)$, where\n$$\ng_i(x_i, y_i) = \\frac{1}{2}(x_i - y_i)^2 + \\tau\\lambda_{1}|x_i| + \\frac{\\tau\\lambda_{2}}{2}x_i^2.\n$$\nTherefore, we can find the minimizer $x^*$ by minimizing each $g_i(x_i, y_i)$ independently:\n$$\nx_i^* = \\arg\\min_{x_i\\in\\mathbb{R}} g_i(x_i, y_i).\n$$\nTo find the minimum, we can rearrange the terms in $g_i$ by completing the square:\n\\begin{align*}\ng_i(x_i, y_i) &= \\left(\\frac{1}{2} + \\frac{\\tau\\lambda_{2}}{2}\\right)x_i^2 - y_i x_i + \\tau\\lambda_{1}|x_i| + \\frac{1}{2}y_i^2 \\\\\n&= \\frac{1+\\tau\\lambda_{2}}{2}\\left(x_i^2 - \\frac{2y_i}{1+\\tau\\lambda_{2}}x_i\\right) + \\tau\\lambda_{1}|x_i| + \\frac{1}{2}y_i^2 \\\\\n&= \\frac{1+\\tau\\lambda_{2}}{2}\\left( \\left(x_i - \\frac{y_i}{1+\\tau\\lambda_{2}}\\right)^2 - \\left(\\frac{y_i}{1+\\tau\\lambda_{2}}\\right)^2 \\right) + \\tau\\lambda_{1}|x_i| + \\frac{1}{2}y_i^2 \\\\\n&= \\frac{1+\\tau\\lambda_{2}}{2}\\left(x_i - \\frac{y_i}{1+\\tau\\lambda_{2}}\\right)^2 + \\tau\\lambda_{1}|x_i| + C(y_i),\n\\end{align*}\nwhere $C(y_i)$ contains terms that do not depend on $x_i$. Minimizing $g_i(x_i, y_i)$ with respect to $x_i$ is equivalent to minimizing\n$$\n\\tilde{g}_i(x_i) = \\frac{1+\\tau\\lambda_{2}}{2}\\left(x_i - \\frac{y_i}{1+\\tau\\lambda_{2}}\\right)^2 + \\tau\\lambda_{1}|x_i|.\n$$\nDividing by the positive constant $(1+\\tau\\lambda_{2})$, the minimization problem becomes:\n$$\nx_i^* = \\arg\\min_{x_i\\in\\mathbb{R}}\\left\\{ \\frac{1}{2}\\left(x_i - \\frac{y_i}{1+\\tau\\lambda_{2}}\\right)^2 + \\frac{\\tau\\lambda_{1}}{1+\\tau\\lambda_{2}}|x_i| \\right\\}.\n$$\nThis is the definition of the proximal operator of the function $z \\mapsto \\kappa|z|$ with $\\kappa = \\frac{\\tau\\lambda_{1}}{1+\\tau\\lambda_{2}}$, evaluated at the point $\\frac{y_i}{1+\\tau\\lambda_{2}}$. This operator is the soft-thresholding operator $S_{\\kappa}(z) = \\operatorname{sign}(z)\\max\\{|z|-\\kappa,0\\}$.\nThus, for each coordinate $i$:\n$$\nx_i^* = S_{\\frac{\\tau\\lambda_{1}}{1+\\tau\\lambda_{2}}}\\left(\\frac{y_i}{1+\\tau\\lambda_{2}}\\right).\n$$\nIn vector form, the proximal operator is given by\n$$\n\\operatorname{prox}_{\\tau f}(y) = S_{\\frac{\\tau\\lambda_{1}}{1+\\tau\\lambda_{2}}}\\left(\\frac{1}{1+\\tau\\lambda_{2}}y\\right),\n$$\nwhere the soft-thresholding operator is applied component-wise.\n\n**2. Geometric Interpretation for $n=2$**\n\nFor $n=2$, the mapping $T(y) = \\operatorname{prox}_{\\tau f}(y)$ can be interpreted as a composition of two distinct geometric operations. Let $\\alpha = \\frac{1}{1+\\tau\\lambda_{2}}$ and $\\kappa = \\frac{\\tau\\lambda_{1}}{1+\\tau\\lambda_{2}}$. The mapping is $T(y) = S_{\\kappa}(\\alpha y)$. Since $\\tau > 0$ and $\\lambda_2 > 0$, we have $0 < \\alpha < 1$.\nThe transformation from $y \\in \\mathbb{R}^2$ to $T(y) \\in \\mathbb{R}^2$ occurs in two steps:\n1.  **Uniform Radial Shrinkage:** First, the vector $y$ is scaled by the factor $\\alpha$. The operation $y \\mapsto \\alpha y$ uniformly shrinks the vector $y$ towards the origin along the line connecting the origin and $y$. The length of the vector becomes $\\alpha \\|y\\|_{2}$.\n2.  **Coordinate-wise Soft-Thresholding:** Second, the soft-thresholding operator $S_{\\kappa}$ is applied to each component of the shrunken vector $\\alpha y$. For a vector $z=(z_1, z_2)$, $S_{\\kappa}(z) = (S_{\\kappa}(z_1), S_{\\kappa}(z_2))$. This operation moves each component towards the origin by a distance $\\kappa$ and sets it to zero if its magnitude is less than or equal to $\\kappa$. This is a non-uniform transformation that can change the direction of the vector and introduce sparsity (zero components).\n\nSo, the map $\\operatorname{prox}_{\\tau f}$ is a composition of a radial shrinkage followed by a coordinate-wise soft-thresholding.\n\n**3. Fixed-Point Analysis**\n\nWe are interested in the fixed-point set $\\{x \\in \\mathbb{R}^n : x=T(x)\\}$, where $T(x) = \\operatorname{prox}_{\\tau f}(x)$.\nAccording to the first-order optimality conditions for proximal operators, a point $x^*$ is a fixed point of $\\operatorname{prox}_{\\tau f}$ if and only if it is a minimizer of $f$.\n$x^* = \\operatorname{prox}_{\\tau f}(x^*) \\iff x^* = \\arg\\min_x \\left\\{\\frac{1}{2}\\|x-x^*\\|^2_2 + \\tau f(x) \\right\\}$.\nThe optimality condition for this minimization is $0 \\in \\partial_x \\left(\\frac{1}{2}\\|x-x^*\\|^2_2 + \\tau f(x)\\right)\\Big|_{x=x^*}$, which simplifies to $0 \\in (x^*-x^*) + \\tau\\partial f(x^*)$, or $0 \\in \\partial f(x^*)$.\nThis is precisely the condition for $x^*$ being a minimizer of the convex function $f$.\n\nNow we find the minimizers of $f(x) = \\lambda_{1}\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}$. The subdifferential of $f$ is $\\partial f(x) = \\lambda_1 \\partial\\|x\\|_1 + \\lambda_2 x$. The optimality condition $0 \\in \\partial f(x^*)$ is equivalent to $-\\lambda_2 x^* \\in \\lambda_1 \\partial\\|x^*\\|_1$.\nThis is a coordinate-wise condition: for each $i \\in \\{1,...,n\\}$, $-\\lambda_2 x_i^* \\in \\lambda_1 \\partial|x_i^*|$.\n- If $x_i^* \\ne 0$, then $\\partial|x_i^*| = \\{\\operatorname{sign}(x_i^*)\\}$, so we must have $-\\lambda_2 x_i^* = \\lambda_1 \\operatorname{sign}(x_i^*)$. Taking the absolute value of both sides gives $\\lambda_2 |x_i^*| = \\lambda_1$, so $|x_i^*| = \\lambda_1 / \\lambda_2$. Substituting back, $-\\lambda_2 x_i^* = \\lambda_1 (x_i^*/|x_i^*|) = \\lambda_1 x_i^* / (\\lambda_1/\\lambda_2) = \\lambda_2 x_i^*$. This implies $2\\lambda_2 x_i^* = 0$, which means $x_i^*=0$, a contradiction to our assumption $x_i^* \\ne 0$.\n- If $x_i^* = 0$, the condition becomes $0 \\in \\lambda_1 [-1, 1]$. Since $\\lambda_1 > 0$, this is equivalent to $0 \\in [-\\lambda_1, \\lambda_1]$, which is true.\n\nThus, the only possibility is $x_i^*=0$ for all $i=1,\\dots,n$. The unique minimizer is $x^*=0$. Consequently, the unique fixed point of the operator $T = \\operatorname{prox}_{\\tau f}$ is the origin $x=0$.\n\n**4. Tight Global Lipschitz Modulus**\n\nThe operator is $T(y) = S_{\\kappa}(\\alpha y)$, where $\\alpha = \\frac{1}{1+\\tau\\lambda_{2}}$ and $\\kappa = \\frac{\\tau\\lambda_{1}}{1+\\tau\\lambda_{2}}$. We wish to find the tight global Lipschitz modulus $L$ of $T$ with respect to the Euclidean norm $\\|\\cdot\\|_2$, defined as\n$$\nL = \\sup_{y,z \\in \\mathbb{R}^n, y \\ne z} \\frac{\\|T(y)-T(z)\\|_2}{\\|y-z\\|_2}.\n$$\nThe operator $T$ is differentiable almost everywhere. Its Jacobian matrix $JT(y)$ is diagonal, as $T_i(y) = S_\\kappa(\\alpha y_i)$ depends only on $y_i$. The diagonal entries are:\n$$\n(JT(y))_{ii} = \\frac{\\partial T_i}{\\partial y_i} = \\frac{d}{dy_i} S_\\kappa(\\alpha y_i) = \\alpha \\cdot S'_\\kappa(\\alpha y_i).\n$$\nThe derivative of the scalar soft-thresholding function $S_\\kappa(u)$ is:\n$$\nS'_\\kappa(u) = \\begin{cases} 1 & \\text{if } |u| > \\kappa \\\\ 0 & \\text{if } |u| < \\kappa \\end{cases}\n$$\nand is undefined at $|u|=\\kappa$. Therefore, the diagonal entries of the Jacobian are either $\\alpha$ (if $|\\alpha y_i| > \\kappa$) or $0$ (if $|\\alpha y_i| < \\kappa$).\nThe Lipschitz modulus $L$ of a piecewise-differentiable map is the supremum of the spectral norms of its Jacobian matrices over the regions where it is defined. The spectral norm of a diagonal matrix is the maximum of the absolute values of its diagonal entries.\n$$\n\\|JT(y)\\|_2 = \\max_{i} |(JT(y))_{ii}| = \\max_{i} |\\alpha \\cdot S'_\\kappa(\\alpha y_i)| \\le \\alpha.\n$$\nThis shows that $L \\le \\alpha$. To show that this bound is tight, we must demonstrate that it can be achieved. Consider two points $y$ and $z$ such that for some component $j$, $|\\alpha y_j| > \\kappa$ and $|\\alpha z_j| > \\kappa$, and all other components are zero. For instance, let $y = (M, 0, \\dots, 0)$ and $z = (M+\\delta, 0, \\dots, 0)$ with $\\delta>0$. Choose $M$ large enough such that $\\alpha M > \\kappa$. Then $\\alpha(M+\\delta) > \\kappa$ as well.\nThe outputs are:\n$$\nT(y) = (S_\\kappa(\\alpha M), 0, \\dots, 0) = (\\alpha M - \\kappa, 0, \\dots, 0)\n$$\n$$\nT(z) = (S_\\kappa(\\alpha(M+\\delta)), 0, \\dots, 0) = (\\alpha(M+\\delta) - \\kappa, 0, \\dots, 0)\n$$\nThe distance between the outputs is:\n$$\n\\|T(y)-T(z)\\|_2 = \\|((\\alpha M - \\kappa) - (\\alpha M + \\alpha\\delta - \\kappa), 0, \\dots, 0)\\|_2 = \\|(-\\alpha\\delta, 0, \\dots, 0)\\|_2 = \\alpha\\delta.\n$$\nThe distance between the inputs is:\n$$\n\\|y-z\\|_2 = \\|(M-(M+\\delta), 0, \\dots, 0)\\|_2 = \\|(-\\delta, 0, \\dots, 0)\\|_2 = \\delta.\n$$\nThe ratio is $\\frac{\\|T(y)-T(z)\\|_2}{\\|y-z\\|_2} = \\frac{\\alpha\\delta}{\\delta} = \\alpha$.\nSince we have shown that $L \\le \\alpha$ and have found a case where the ratio is exactly $\\alpha$, the tight global Lipschitz modulus is $L=\\alpha$.\nSubstituting the expression for $\\alpha$:\n$$\nL = \\frac{1}{1+\\tau\\lambda_{2}}.\n$$\nThis expression is a function of $\\tau$ and $\\lambda_2$, as required.",
            "answer": "$$\\boxed{\\frac{1}{1+\\tau\\lambda_{2}}}$$"
        },
        {
            "introduction": "Having explored the elastic net through its penalized form, we now examine its geometry from a different angle: as a constraint set. This practice asks you to find the projection of a point onto the \"elastic net ball,\" the region where the penalty value is bounded. By applying the powerful Karush-Kuhn-Tucker (KKT) conditions, you will discover a deep connection between this geometric projection and the proximal operator, revealing the elegant duality inherent in convex optimization .",
            "id": "3469123",
            "problem": "Consider the projection problem in Euclidean space: given a vector $y \\in \\mathbb{R}^{n}$ and parameters $\\lambda_{1} > 0$, $\\lambda_{2} > 0$, and $t > 0$, define the elastic net ball $\\mathcal{C} = \\{x \\in \\mathbb{R}^{n} : \\lambda_{1}\\|x\\|_{1} + \\tfrac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} \\le t\\}$. The projection of $y$ onto $\\mathcal{C}$ is the unique solution $x^{\\star}$ of the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\tfrac{1}{2}\\|x - y\\|_{2}^{2} \\quad \\text{subject to} \\quad \\lambda_{1}\\|x\\|_{1} + \\tfrac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} \\le t.\n$$\nTasks:\n- Derive the necessary and sufficient optimality conditions for this projection using the Karush–Kuhn–Tucker conditions from convex optimization, starting from the Lagrangian construction and subdifferential calculus for the $\\ell_{1}$-norm. Show that the optimal solution can be expressed componentwise via a soft-thresholding operation parametrized by a Lagrange multiplier $\\nu \\ge 0$, followed by a uniform scaling depending on $\\nu$ and $\\lambda_{2}$.\n- Provide a geometric interpretation of the projection mapping as a composition of two steps acting on $y$: a sparsity-inducing shrinkage and a quadratic-induced scaling, each controlled by the same multiplier $\\nu$.\n- Then, specialize to the case $n = 3$, with $y = (3, \\tfrac{3}{2}, \\tfrac{1}{5})^{\\top}$, $\\lambda_{1} = 1$, $\\lambda_{2} = 1$, and $t = \\tfrac{57}{32}$. Compute the unique Lagrange multiplier $\\nu^{\\star}$ associated with the active constraint at the solution. Provide your answer as an exact value. If an approximation were needed, instructions would specify significant figures, but here an exact value is required.",
            "solution": "The problem asks for the derivation of the optimality conditions for projecting a vector $y \\in \\mathbb{R}^{n}$ onto an elastic net ball $\\mathcal{C}$, a geometric interpretation of the projection, and the computation of the associated Lagrange multiplier for a specific instance.\n\nThe optimization problem is\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\tfrac{1}{2}\\|x - y\\|_{2}^{2} \\quad \\text{subject to} \\quad \\lambda_{1}\\|x\\|_{1} + \\tfrac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} \\le t\n$$\nwhere $\\lambda_1 > 0$, $\\lambda_2 > 0$, and $t > 0$.\n\n### Part 1: Derivation of Optimality Conditions and Solution Form\n\nThe objective function $f_0(x) = \\frac{1}{2}\\|x - y\\|_{2}^{2}$ is strictly convex. The constraint is defined by the function $f_1(x) = \\lambda_{1}\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} - t$. The $\\ell_1$-norm $\\|x\\|_1$ and the squared $\\ell_2$-norm $\\|x\\|_2^2$ are convex functions. Since $\\lambda_1 > 0$ and $\\lambda_2 > 0$, their positive linear combination $f_1(x)+t$ is also convex, making the constraint set $\\mathcal{C} = \\{x \\mid f_1(x) \\le 0\\}$ a convex set. This is a convex optimization problem.\n\nFor $t>0$, the point $x=0$ is strictly feasible, since $f_1(0) = -t < 0$. Thus, Slater's condition holds, and the Karush–Kuhn–Tucker (KKT) conditions are both necessary and sufficient for optimality.\n\nThe Lagrangian is\n$$\nL(x, \\nu) = \\frac{1}{2}\\|x - y\\|_{2}^{2} + \\nu \\left( \\lambda_{1}\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} - t \\right)\n$$\nwhere $\\nu \\ge 0$ is the Lagrange multiplier for the inequality constraint.\n\nThe KKT conditions for an optimal solution $x^{\\star}$ and multiplier $\\nu^{\\star}$ are:\n1.  **Stationarity**: $0 \\in \\nabla_x L(x^{\\star}, \\nu^{\\star})$\n2.  **Primal feasibility**: $\\lambda_{1}\\|x^{\\star}\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x^{\\star}\\|_{2}^{2} - t \\le 0$\n3.  **Dual feasibility**: $\\nu^{\\star} \\ge 0$\n4.  **Complementary slackness**: $\\nu^{\\star} \\left( \\lambda_{1}\\|x^{\\star}\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x^{\\star}\\|_{2}^{2} - t \\right) = 0$\n\nThe objective function is differentiable, but the $\\ell_1$-norm is not. We use subdifferential calculus. The subdifferential of $L(x, \\nu)$ with respect to $x$ is:\n$$\n\\partial_x L(x, \\nu) = (x - y) + \\nu \\lambda_1 \\partial\\|x\\|_1 + \\nu \\lambda_2 x\n$$\nwhere $\\partial\\|x\\|_1$ is the subdifferential of the $\\ell_1$-norm. The stationarity condition $0 \\in \\partial_x L(x^{\\star}, \\nu^{\\star})$ becomes:\n$$\n0 \\in (x^{\\star} - y) + \\nu^{\\star} \\lambda_1 \\partial\\|x^{\\star}\\|_1 + \\nu^{\\star} \\lambda_2 x^{\\star}\n$$\nRearranging gives:\n$$\ny \\in (1 + \\nu^{\\star}\\lambda_2)x^{\\star} + \\nu^{\\star}\\lambda_1 \\partial\\|x^{\\star}\\|_1\n$$\nThis must hold component-wise for $i=1, \\dots, n$:\n$$\ny_i \\in (1 + \\nu^{\\star}\\lambda_2)x^{\\star}_i + \\nu^{\\star}\\lambda_1 \\partial|x^{\\star}_i|\n$$\nwhere $\\partial|z|$ is $\\text{sign}(z)$ for $z \\neq 0$ and the interval $[-1, 1]$ for $z=0$.\n\nWe analyze this component-wise relationship:\n- If $x^{\\star}_i > 0$, then $\\partial|x^{\\star}_i| = \\{1\\}$, so $y_i = (1 + \\nu^{\\star}\\lambda_2)x^{\\star}_i + \\nu^{\\star}\\lambda_1$. This implies $x^{\\star}_i = \\frac{y_i - \\nu^{\\star}\\lambda_1}{1 + \\nu^{\\star}\\lambda_2}$. For $x^{\\star}_i$ to be positive, we must have $y_i > \\nu^{\\star}\\lambda_1$.\n- If $x^{\\star}_i < 0$, then $\\partial|x^{\\star}_i| = \\{-1\\}$, so $y_i = (1 + \\nu^{\\star}\\lambda_2)x^{\\star}_i - \\nu^{\\star}\\lambda_1$. This implies $x^{\\star}_i = \\frac{y_i + \\nu^{\\star}\\lambda_1}{1 + \\nu^{\\star}\\lambda_2}$. For $x^{\\star}_i$ to be negative, we must have $y_i < -\\nu^{\\star}\\lambda_1$.\n- If $x^{\\star}_i = 0$, then $\\partial|x^{\\star}_i| = [-1, 1]$, so $y_i \\in (1 + \\nu^{\\star}\\lambda_2)(0) + \\nu^{\\star}\\lambda_1 [-1, 1]$. This implies $|y_i| \\le \\nu^{\\star}\\lambda_1$.\n\nThese three cases can be compactly written using the soft-thresholding operator $S_\\alpha(z) = \\text{sign}(z)\\max(|z|-\\alpha, 0)$. The solution for each component is:\n$$\nx^{\\star}_i = \\frac{1}{1 + \\nu^{\\star}\\lambda_2} S_{\\nu^{\\star}\\lambda_1}(y_i)\n$$\nIn vector form, the optimal solution is a function of the Lagrange multiplier $\\nu^\\star$:\n$$\nx^{\\star} = \\frac{1}{1 + \\nu^{\\star}\\lambda_2} S_{\\nu^{\\star}\\lambda_1}(y)\n$$\nThe value of $\\nu^\\star$ is determined by the KKT conditions. If $y \\in \\mathcal{C}$, then $x^\\star = y$ and $\\nu^\\star=0$. If $y \\notin \\mathcal{C}$, the constraint must be active, so $\\nu^\\star>0$ is the unique positive root of the equation $\\lambda_{1}\\|x(\\nu)\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x(\\nu)\\|_{2}^{2} = t$.\n\n### Part 2: Geometric Interpretation\n\nThe solution $x^{\\star} = \\frac{1}{1+\\nu^{\\star}\\lambda_2} S_{\\nu^{\\star}\\lambda_1}(y)$ reveals a two-step process:\n1.  **Sparsity-inducing shrinkage**: First, the vector $y$ is operated on by $S_{\\nu^{\\star}\\lambda_1}(\\cdot)$. This is the soft-thresholding function, which is central to Lasso (Least Absolute Shrinkage and Selection Operator) regression. It performs two actions: it shrinks every component of $y$ towards zero by an amount $\\nu^{\\star}\\lambda_1$, and it sets any component with magnitude less than the threshold $\\nu^{\\star}\\lambda_1$ to exactly zero. This step promotes a sparse solution.\n2.  **Quadratic-induced scaling**: Second, the resulting shrunken vector, $S_{\\nu^{\\star}\\lambda_1}(y)$, is uniformly scaled by a factor of $\\frac{1}{1+\\nu^{\\star}\\lambda_2}$. Since $\\nu^{\\star} \\ge 0$ and $\\lambda_2 > 0$, this factor is always in $(0, 1]$. This scaling is a form of shrinkage reminiscent of ridge regression, penalizing large coefficient values.\n\nIn essence, the projection onto the elastic net ball geometrically corresponds to a Lasso-type shrinkage and sparsification, followed by a Ridge-type uniform contraction. Both operations are governed by the same Lagrange multiplier $\\nu^\\star$, which balances the distance to $y$ with the elastic net constraint.\n\n### Part 3: Computation for the Specific Case\n\nWe are given $n=3$, $y = (3, \\frac{3}{2}, \\frac{1}{5})^{\\top}$, $\\lambda_1 = 1$, $\\lambda_2 = 1$, and $t = \\frac{57}{32}$.\nThe constraint is $\\|x\\|_1 + \\frac{1}{2}\\|x\\|_2^2 \\le \\frac{57}{32}$.\n\nFirst, we check if $y$ is already in the feasible set $\\mathcal{C}$.\n$\\|y\\|_1 = |3| + |\\frac{3}{2}| + |\\frac{1}{5}| = 3 + \\frac{3}{2} + \\frac{1}{5} = \\frac{30+15+2}{10} = \\frac{47}{10}$.\n$\\|y\\|_2^2 = 3^2 + (\\frac{3}{2})^2 + (\\frac{1}{5})^2 = 9 + \\frac{9}{4} + \\frac{1}{25} = \\frac{900+225+4}{100} = \\frac{1129}{100}$.\nThe constraint function evaluated at $y$ is:\n$\\|y\\|_1 + \\frac{1}{2}\\|y\\|_2^2 = \\frac{47}{10} + \\frac{1}{2}\\left(\\frac{1129}{100}\\right) = \\frac{940}{200} + \\frac{1129}{200} = \\frac{2069}{200} = 10.345$.\nThe limit is $t = \\frac{57}{32} = 1.78125$.\nSince $10.345 > 1.78125$, the point $y$ is outside $\\mathcal{C}$. Therefore, the projection $x^\\star$ must lie on the boundary of $\\mathcal{C}$, meaning the constraint is active: $\\|x^\\star\\|_1 + \\frac{1}{2}\\|x^\\star\\|_2^2 = t$. By complementary slackness, we must have $\\nu^\\star > 0$.\n\nThe solution is $x^\\star = \\frac{1}{1+\\nu^\\star} S_{\\nu^\\star}(y)$. We need to find $\\nu^\\star > 0$ that satisfies the active constraint equation $g(\\nu) = \\|\\frac{1}{1+\\nu} S_{\\nu}(y)\\|_1 + \\frac{1}{2}\\|\\frac{1}{1+\\nu} S_{\\nu}(y)\\|_2^2 = \\frac{57}{32}$.\nThe components of $y$ are $y_1=3$, $y_2=1.5$, $y_3=0.2$. The form of $S_\\nu(y)$ depends on the value of $\\nu$.\nLet's analyze $g(\\nu)$ on intervals of $\\nu$:\nThe thresholds are $0.2$, $1.5$, $3$.\nLet's test the interval $0.2 < \\nu \\le 1.5$. In this range, $y_3$ is thresholded to zero, but $y_1$ and $y_2$ are not.\n$S_\\nu(y) = (3-\\nu, \\frac{3}{2}-\\nu, 0)^\\top$.\n$\\|S_\\nu(y)\\|_1 = (3-\\nu) + (\\frac{3}{2}-\\nu) = \\frac{9}{2}-2\\nu$.\n$\\|S_\\nu(y)\\|_2^2 = (3-\\nu)^2 + (\\frac{3}{2}-\\nu)^2 = (9 - 6\\nu + \\nu^2) + (\\frac{9}{4} - 3\\nu + \\nu^2) = \\frac{45}{4} - 9\\nu + 2\\nu^2$.\n\nThe constraint equation becomes:\n$$\n\\frac{\\frac{9}{2}-2\\nu}{1+\\nu} + \\frac{1}{2(1+\\nu)^2} \\left( \\frac{45}{4} - 9\\nu + 2\\nu^2 \\right) = \\frac{57}{32}\n$$\nTo simplify, we multiply by $2(1+\\nu)^2$:\n$$\n2(1+\\nu)\\left(\\frac{9}{2}-2\\nu\\right) + \\left( \\frac{45}{4} - 9\\nu + 2\\nu^2 \\right) = \\frac{57}{16}(1+\\nu)^2\n$$\nThe left-hand side is:\n$$\n(1+\\nu)(9-4\\nu) + \\frac{45}{4} - 9\\nu + 2\\nu^2 = (9+5\\nu-4\\nu^2) + \\frac{45}{4} - 9\\nu + 2\\nu^2 = \\frac{81}{4} - 4\\nu - 2\\nu^2\n$$\nSo, the equation is:\n$$\n\\frac{81}{4} - 4\\nu - 2\\nu^2 = \\frac{57}{16}(1+2\\nu+\\nu^2)\n$$\nCross-multiply by 16:\n$$\n16 \\left( \\frac{81}{4} - 4\\nu - 2\\nu^2 \\right) = 57(1+2\\nu+\\nu^2)\n$$\n$$\n4 \\times 81 - 64\\nu - 32\\nu^2 = 57 + 114\\nu + 57\\nu^2\n$$\n$$\n324 - 64\\nu - 32\\nu^2 = 57 + 114\\nu + 57\\nu^2\n$$\nRearranging into a standard quadratic form $a\\nu^2+b\\nu+c=0$:\n$$\n(57+32)\\nu^2 + (114+64)\\nu + (57-324) = 0\n$$\n$$\n89\\nu^2 + 178\\nu - 267 = 0\n$$\nDividing the entire equation by $89$:\n$$\n\\nu^2 + 2\\nu - 3 = 0\n$$\nFactoring the quadratic equation:\n$$\n(\\nu+3)(\\nu-1) = 0\n$$\nThe roots are $\\nu = 1$ and $\\nu = -3$. Since we must have $\\nu^\\star \\ge 0$, the only valid solution is $\\nu^\\star=1$.\nWe must verify that this solution lies in the assumed interval $0.2 < \\nu \\le 1.5$. Indeed, $\\nu=1$ is in this interval.\nThus, the unique Lagrange multiplier is $\\nu^\\star=1$.\n\nTo confirm, let's compute $x^\\star$ for $\\nu^\\star=1$:\n$x^\\star = \\frac{1}{1+1}S_1( (3, \\frac{3}{2}, \\frac{1}{5})^\\top ) = \\frac{1}{2} (3-1, \\frac{3}{2}-1, 0)^\\top = \\frac{1}{2} (2, \\frac{1}{2}, 0)^\\top = (1, \\frac{1}{4}, 0)^\\top$.\nCheck the constraint:\n$\\|x^\\star\\|_1 + \\frac{1}{2}\\|x^\\star\\|_2^2 = (1+\\frac{1}{4}) + \\frac{1}{2}(1^2 + (\\frac{1}{4})^2) = \\frac{5}{4} + \\frac{1}{2}(1+\\frac{1}{16}) = \\frac{5}{4} + \\frac{1}{2}(\\frac{17}{16}) = \\frac{5}{4} + \\frac{17}{32} = \\frac{40}{32} + \\frac{17}{32} = \\frac{57}{32}$.\nThe constraint is perfectly satisfied.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Beyond its geometric and optimization properties, how does the elastic net penalty affect the statistical complexity of a model? This exercise bridges the gap between geometry and statistical learning by tasking you with calculating the \"degrees of freedom\" of an elastic net fit. This powerful concept measures a model's effective number of parameters, and through this derivation , you will obtain a clear, quantitative formula that shows precisely how the elastic net penalty controls model complexity.",
            "id": "3469093",
            "problem": "Consider an observation vector $y \\in \\mathbb{R}^{n}$ and a design matrix $X \\in \\mathbb{R}^{n \\times p}$ whose columns satisfy the orthonormality condition $X^{\\top} X = n I_{p}$. For regularization parameters $\\lambda_{1} \\geq 0$ and $\\lambda_{2} \\geq 0$, define the elastic net estimator $\\hat{\\beta}(y) \\in \\mathbb{R}^{p}$ as any minimizer of the convex objective\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2n} \\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\frac{\\lambda_{2}}{2} \\|\\beta\\|_{2}^{2}.\n$$\nLet the fitted values be $\\hat{y}(y) = X \\hat{\\beta}(y) \\in \\mathbb{R}^{n}$. Assume a generic position of $y$ such that, with $z = X^{\\top} y / n$, all coordinates satisfy $|z_{j}| \\neq \\lambda_{1}$ for all $j \\in \\{1,\\dots,p\\}$. Using the divergence-based definition of degrees of freedom for an almost-everywhere differentiable mapping $y \\mapsto \\hat{y}(y)$,\n$$\n\\mathrm{df}(y) \\equiv \\operatorname{tr}\\!\\left( \\frac{\\partial \\hat{y}(y)}{\\partial y} \\right),\n$$\nderive an explicit closed-form expression for $\\mathrm{df}(y)$ in terms of $z$ and the parameters $\\lambda_{1}, \\lambda_{2}$ under the orthonormality condition $X^{\\top} X = n I_{p}$. Your final answer must be a single closed-form analytic expression involving only $z$, $\\lambda_{1}$, and $\\lambda_{2}$, and must not involve any uneliminated derivatives or limits. No rounding is required.",
            "solution": "The problem is to derive a closed-form expression for the degrees of freedom, $\\mathrm{df}(y)$, of the elastic net estimator under specific conditions. The degrees of freedom are defined as $\\mathrm{df}(y) \\equiv \\operatorname{tr}\\!\\left( \\frac{\\partial \\hat{y}(y)}{\\partial y} \\right)$, where $\\hat{y}(y) = X \\hat{\\beta}(y)$ are the fitted values.\n\nFirst, we analyze the elastic net objective function that is to be minimized with respect to $\\beta \\in \\mathbb{R}^{p}$:\n$$\nL(\\beta) = \\frac{1}{2n} \\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\frac{\\lambda_{2}}{2} \\|\\beta\\|_{2}^{2}\n$$\nThe least-squares term can be expanded:\n$$\n\\|y - X \\beta\\|_{2}^{2} = (y - X \\beta)^{\\top}(y - X \\beta) = y^{\\top}y - 2 y^{\\top}X\\beta + \\beta^{\\top}X^{\\top}X\\beta\n$$\nWe are given the orthonormality condition $X^{\\top} X = n I_{p}$, where $I_p$ is the $p \\times p$ identity matrix. Substituting this into the objective function yields:\n$$\nL(\\beta) = \\frac{1}{2n} (y^{\\top}y - 2 y^{\\top}X\\beta + n \\beta^{\\top}\\beta) + \\lambda_{1} \\|\\beta\\|_{1} + \\frac{\\lambda_{2}}{2} \\|\\beta\\|_{2}^{2}\n$$\nRearranging and distributing the $\\frac{1}{2n}$ term:\n$$\nL(\\beta) = \\frac{1}{2n} y^{\\top}y - \\frac{1}{n} y^{\\top}X\\beta + \\frac{1}{2} \\beta^{\\top}\\beta + \\lambda_{1} \\|\\beta\\|_{1} + \\frac{\\lambda_{2}}{2} \\beta^{\\top}\\beta\n$$\nLet $z = X^{\\top} y / n$. The term $y^{\\top}X\\beta$ can be written as $(X^{\\top}y)^{\\top}\\beta = (nz)^{\\top}\\beta = n z^{\\top}\\beta$. The term $\\frac{1}{2n} y^{\\top}y$ is a constant with respect to $\\beta$ and does not affect the minimization. We can write the objective, up to an additive constant, as:\n$$\nL(\\beta) \\propto -z^{\\top}\\beta + \\frac{1}{2} \\|\\beta\\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\frac{\\lambda_{2}}{2} \\|\\beta\\|_{2}^{2}\n$$\nCombining the $\\|\\beta\\|_{2}^{2}$ terms:\n$$\nL(\\beta) \\propto \\frac{1+\\lambda_{2}}{2} \\|\\beta\\|_{2}^{2} - z^{\\top}\\beta + \\lambda_{1} \\|\\beta\\|_{1}\n$$\nThis objective function is separable with respect to the components of $\\beta$. That is, $L(\\beta) \\propto \\sum_{j=1}^{p} L_j(\\beta_j)$, where\n$$\nL_j(\\beta_j) = \\frac{1+\\lambda_{2}}{2} \\beta_{j}^{2} - z_{j}\\beta_{j} + \\lambda_{1} |\\beta_{j}|\n$$\nTo find the minimizer $\\hat{\\beta}_j$, we use subgradient calculus. The subgradient of $L_j$ with respect to $\\beta_j$ is:\n$$\n\\partial_{\\beta_j} L_j(\\beta_j) = (1+\\lambda_{2})\\beta_{j} - z_{j} + \\lambda_{1} \\partial |\\beta_{j}|\n$$\nwhere $\\partial |\\beta_{j}|$ is the subgradient of the absolute value function: $\\operatorname{sign}(\\beta_j)$ if $\\beta_j \\neq 0$, and the interval $[-1, 1]$ if $\\beta_j = 0$. At the minimum $\\hat{\\beta}_j$, the subgradient must contain $0$.\n$$\n0 \\in (1+\\lambda_{2})\\hat{\\beta}_{j} - z_{j} + \\lambda_{1} \\operatorname{sign}(\\hat{\\beta}_j)\n$$\nThis can be rewritten as $z_j \\in (1+\\lambda_{2})\\hat{\\beta}_{j} + \\lambda_{1} \\operatorname{sign}(\\hat{\\beta}_j)$.\nWe analyze the cases:\n1. If $\\hat{\\beta}_j > 0$, then $\\operatorname{sign}(\\hat{\\beta}_j) = 1$, and $z_j = (1+\\lambda_{2})\\hat{\\beta}_{j} + \\lambda_{1}$. This gives $\\hat{\\beta}_j = \\frac{z_j - \\lambda_{1}}{1+\\lambda_{2}}$. This is consistent only if $\\hat{\\beta}_j > 0$, which requires $z_j > \\lambda_{1}$.\n2. If $\\hat{\\beta}_j < 0$, then $\\operatorname{sign}(\\hat{\\beta}_j) = -1$, and $z_j = (1+\\lambda_{2})\\hat{\\beta}_{j} - \\lambda_{1}$. This gives $\\hat{\\beta}_j = \\frac{z_j + \\lambda_{1}}{1+\\lambda_{2}}$. This is consistent only if $\\hat{\\beta}_j < 0$, which requires $z_j < -\\lambda_{1}$.\n3. If $\\hat{\\beta}_j = 0$, then $z_j \\in \\lambda_{1} [-1, 1]$, which means $|z_j| \\leq \\lambda_{1}$.\n\nThese three cases can be compactly written using the soft-thresholding operator, defined as $S_{\\alpha}(x) = \\operatorname{sign}(x) \\max(|x|-\\alpha, 0)$. The solution is:\n$$\n\\hat{\\beta}_{j} = \\frac{1}{1+\\lambda_{2}} S_{\\lambda_{1}}(z_{j})\n$$\nThe problem states that for all $j$, $|z_j| \\neq \\lambda_1$. This means we are away from the non-differentiable points of the soft-thresholding function.\n\nThe degrees of freedom are $\\mathrm{df}(y) = \\operatorname{tr}\\!\\left( \\frac{\\partial \\hat{y}(y)}{\\partial y} \\right)$. With $\\hat{y} = X\\hat{\\beta}$, we have $\\frac{\\partial \\hat{y}}{\\partial y} = X \\frac{\\partial \\hat{\\beta}}{\\partial y}$. The degrees of freedom are:\n$$\n\\mathrm{df}(y) = \\operatorname{tr}\\!\\left( X \\frac{\\partial \\hat{\\beta}}{\\partial y} \\right)\n$$\nWe use the chain rule to compute $\\frac{\\partial \\hat{\\beta}}{\\partial y}$. Let's find the Jacobian of $\\hat{\\beta}$ with respect to $y$:\n$$\n\\frac{\\partial \\hat{\\beta}}{\\partial y} = \\frac{\\partial \\hat{\\beta}}{\\partial z} \\frac{\\partial z}{\\partial y}\n$$\nThe term $\\frac{\\partial z}{\\partial y}$ is obtained from the definition $z = \\frac{1}{n}X^{\\top}y$:\n$$\n\\frac{\\partial z}{\\partial y} = \\frac{1}{n}X^{\\top}\n$$\nThis is a $p \\times n$ matrix. The term $\\frac{\\partial \\hat{\\beta}}{\\partial z}$ is a $p \\times p$ Jacobian matrix. Since each component $\\hat{\\beta}_j$ depends only on the corresponding component $z_j$, this matrix is diagonal:\n$$\n\\left(\\frac{\\partial \\hat{\\beta}}{\\partial z}\\right)_{jk} = \\frac{\\partial \\hat{\\beta}_j}{\\partial z_k} = \\delta_{jk} \\frac{d\\hat{\\beta}_j}{dz_j}\n$$\nwhere $\\delta_{jk}$ is the Kronecker delta. We compute the derivative $\\frac{d\\hat{\\beta}_j}{dz_j}$:\n$$\n\\frac{d\\hat{\\beta}_j}{dz_j} = \\frac{d}{dz_j} \\left( \\frac{1}{1+\\lambda_{2}} S_{\\lambda_{1}}(z_{j}) \\right) = \\frac{1}{1+\\lambda_{2}} \\frac{d}{dz_j} S_{\\lambda_{1}}(z_{j})\n$$\nThe derivative of the soft-thresholding function $S_{\\lambda_1}(z_j)$ is $1$ if $|z_j| > \\lambda_1$ and $0$ if $|z_j| < \\lambda_1$. Since the problem assumes $|z_j| \\neq \\lambda_1$, the derivative is well-defined and is given by the indicator function $I(|z_j| > \\lambda_1)$.\n$$\n\\frac{d\\hat{\\beta}_j}{dz_j} = \\frac{1}{1+\\lambda_{2}} I(|z_j| > \\lambda_1)\n$$\nLet $D$ be the diagonal matrix with entries $D_{jj} = \\frac{1}{1+\\lambda_{2}} I(|z_j| > \\lambda_1)$. Then $\\frac{\\partial \\hat{\\beta}}{\\partial z} = D$.\nSubstituting these Jacobians back into the expression for $\\frac{\\partial \\hat{\\beta}}{\\partial y}$:\n$$\n\\frac{\\partial \\hat{\\beta}}{\\partial y} = D \\left( \\frac{1}{n}X^{\\top} \\right) = \\frac{1}{n} D X^{\\top}\n$$\nNow we compute the degrees of freedom:\n$$\n\\mathrm{df}(y) = \\operatorname{tr}\\!\\left( X \\left( \\frac{1}{n} D X^{\\top} \\right) \\right) = \\frac{1}{n} \\operatorname{tr}\\!\\left( X D X^{\\top} \\right)\n$$\nUsing the cyclic property of the trace, $\\operatorname{tr}(ABC) = \\operatorname{tr}(CAB)$, we have:\n$$\n\\operatorname{tr}\\!\\left( X D X^{\\top} \\right) = \\operatorname{tr}\\!\\left( X^{\\top} X D \\right)\n$$\nUsing the given condition $X^{\\top}X = n I_p$:\n$$\n\\operatorname{tr}\\!\\left( X^{\\top} X D \\right) = \\operatorname{tr}\\!\\left( n I_p D \\right) = n \\operatorname{tr}(D)\n$$\nThe trace of the diagonal matrix $D$ is the sum of its diagonal elements:\n$$\n\\operatorname{tr}(D) = \\sum_{j=1}^{p} D_{jj} = \\sum_{j=1}^{p} \\frac{1}{1+\\lambda_{2}} I(|z_j| > \\lambda_1) = \\frac{1}{1+\\lambda_{2}} \\sum_{j=1}^{p} I(|z_j| > \\lambda_1)\n$$\nFinally, substituting this back into the expression for $\\mathrm{df}(y)$:\n$$\n\\mathrm{df}(y) = \\frac{1}{n} (n \\operatorname{tr}(D)) = \\operatorname{tr}(D)\n$$\nTherefore, the degrees of freedom are:\n$$\n\\mathrm{df}(y) = \\frac{1}{1+\\lambda_{2}} \\sum_{j=1}^{p} I(|z_{j}| > \\lambda_{1})\n$$\nThe term $\\sum_{j=1}^{p} I(|z_{j}| > \\lambda_{1})$ represents the number of coefficients $\\hat{\\beta}_j$ that are non-zero, which is the size of the active set.",
            "answer": "$$\n\\boxed{\\frac{1}{1+\\lambda_{2}} \\sum_{j=1}^{p} I(|z_{j}| > \\lambda_{1})}\n$$"
        }
    ]
}