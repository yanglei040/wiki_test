{
    "hands_on_practices": [
        {
            "introduction": "稀疏信号恢复领域存在两种基础的数学形式：约束形式，即基追踪降噪（Basis Pursuit Denoising, BPDN），其形式为 $\\|y-Ax\\|_2 \\le \\epsilon$；以及惩罚形式，即 LASSO，其形式为 $\\frac{1}{2}\\|y - A x\\|_2^2 + \\lambda \\|x\\|_1$。尽管形式不同，但它们在本质上是紧密相连的。本练习旨在通过推导噪声容限 $\\epsilon$ 和正则化参数 $\\lambda$ 之间的显式映射关系，来揭示这两种形式的深刻联系，从而将抽象的对偶理论转化为具体的数学公式。通过完成这一推导，你将能够更深刻地理解参数选择的几何意义，并掌握在不同理论框架之间灵活转换的关键技能。",
            "id": "3441813",
            "problem": "考虑压缩感知和稀疏优化中的基追踪降噪（basis pursuit denoising）公式，该公式通过求解凸约束问题 $\\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_1$ subject to $\\|y - A x\\|_2 \\leq \\epsilon$ 来寻找一个稀疏向量，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个传感矩阵，$y \\in \\mathbb{R}^{m}$ 是一个带噪测量向量，$\\epsilon \\geq 0$ 是一个噪声容限。一个常见的替代方法是二次惩罚拉格朗日形式（也称为 $\\ell_{1}$-正则化最小二乘问题），该方法引入一个正则化参数 $\\lambda > 0$ 并最小化目标函数 $\\frac{1}{2}\\|y - A x\\|_2^2 + \\lambda \\|x\\|_1$。在实践中，$\\lambda$ 通常通过 $K$-折交叉验证来选择，这是一种数据驱动的过程，它划分测量值并通过最小化样本外预测误差来选择 $\\lambda$。\n\n从凸分析和最优性原理的基础（范数的凸性、欧几里得投影和 Karush–Kuhn–Tucker (KKT) 条件）出发，在以下科学上合理的结构假设下，即 $A$ 的列是标准正交的（$A^\\top A = I_{n}$），通过令约束问题和惩罚问题的解相等，推导出一个将约束容限 $\\epsilon$ 与正则化参数 $\\lambda$ 联系起来的显式映射 $\\epsilon(\\lambda)$。将 $\\epsilon(\\lambda)$ 完全用 $A$ 和 $y$ 表示，不涉及任何数值算法。\n\n然后，简要解释在什么条件下，当 $\\lambda$ 变化时，约束和惩罚形式会产生等价的解（例如，可行性、凸性、强对偶性以及沿权衡曲线的单调性），以及对 $\\lambda$ 的 $K$-折交叉验证如何通过您的映射导出相应的 $\\epsilon$ 选择。您的最终答案必须是 $\\epsilon(\\lambda)$ 的单一闭式解析表达式，无需单位。如果您引入任何近似，请确保对其进行严格论证；最终表达式无需四舍五入。",
            "solution": "该问题陈述被评估为有效，因为它在科学上基于凸优化和稀疏信号处理的原理，是适定的，并以客观、形式化的语言表达。矩阵 $A$ 的列是标准正交的这一简化假设是推导解析见解的标准技术，并不会使问题无效。因此，我们可以进行完整求解。\n\n目标是找到一个映射 $\\epsilon(\\lambda)$，以确保在结构假设 $A$ 的列是标准正交的（即 $A^\\top A = I_{n}$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵）下，约束基追踪降噪（BPDN）问题\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_1 \\quad \\text{subject to} \\quad \\|y - A x\\|_2 \\leq \\epsilon $$\n的解与惩罚拉格朗日（LASSO）问题\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\left\\{ F(x) = \\frac{1}{2}\\|y - A x\\|_2^2 + \\lambda \\|x\\|_1 \\right\\} $$\n的解相同。\n\n首先，我们求解 LASSO 问题。目标函数 $F(x)$ 是凸的，因为它是两个凸函数之和。一个向量 $x^*_{\\lambda}$ 是最小化子的充要条件是零向量位于 $F$ 在 $x^*_{\\lambda}$ 处的次微分中，即条件 $0 \\in \\partial F(x^*_{\\lambda})$。\n次微分由 $\\partial F(x) = \\nabla \\left( \\frac{1}{2}\\|y - Ax\\|_2^2 \\right) + \\lambda \\partial \\|x\\|_1$ 给出。\n可微二次项的梯度是：\n$$ \\nabla \\left( \\frac{1}{2}\\|y - A x\\|_2^2 \\right) = A^\\top(Ax - y) $$\n$\\ell_1$-范数 $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$ 的次微分是向量集合 $g \\in \\mathbb{R}^n$，满足如果 $x_i \\neq 0$，则 $g_i = \\text{sign}(x_i)$；如果 $x_i = 0$，则 $g_i \\in [-1, 1]$。\n\n因此，LASSO 解 $x^*_{\\lambda}$ 的最优性条件是：\n$$ 0 \\in A^\\top(Ax^*_{\\lambda} - y) + \\lambda \\partial \\|x^*_{\\lambda}\\|_1 $$\n$$ A^\\top(y - Ax^*_{\\lambda}) \\in \\lambda \\partial \\|x^*_{\\lambda}\\|_1 $$\n使用给定的假设 $A^\\top A = I_n$，该条件简化为：\n$$ A^\\top y - x^*_{\\lambda} \\in \\lambda \\partial \\|x^*_{\\lambda}\\|_1 $$\n这意味着存在一个次梯度向量 $g \\in \\partial \\|x^*_{\\lambda}\\|_1$ 使得 $A^\\top y - x^*_{\\lambda} = \\lambda g$。我们可以对 $i \\in \\{1, \\dots, n\\}$ 的每个分量进行分析：\n$$(x^*_{\\lambda})_i = (A^\\top y)_i - \\lambda g_i$$\n1.  如果 $(x^*_{\\lambda})_i > 0$，则 $g_i = 1$，这意味着 $(x^*_{\\lambda})_i = (A^\\top y)_i - \\lambda$。为使此一致，我们必须有 $(A^\\top y)_i - \\lambda > 0$，即 $(A^\\top y)_i > \\lambda$。\n2.  如果 $(x^*_{\\lambda})_i  0$，则 $g_i = -1$，这意味着 $(x^*_{\\lambda})_i = (A^\\top y)_i + \\lambda$。为使此一致，需要 $(A^\\top y)_i + \\lambda  0$，即 $(A^\\top y)_i  -\\lambda$。\n3.  如果 $(x^*_{\\lambda})_i = 0$，则 $g_i \\in [-1, 1]$。这意味着 $0 = (A^\\top y)_i - \\lambda g_i$，所以 $g_i = (A^\\top y)_i / \\lambda$。为使此一致，需要 $|(A^\\top y)_i / \\lambda| \\leq 1$，即 $|(A^\\top y)_i| \\leq \\lambda$。\n\n结合这些情况，我们得到 $x^*_{\\lambda}$ 的每个分量的解：\n$$ (x^*_{\\lambda})_i = \\begin{cases} (A^\\top y)_i - \\lambda  \\text{if } (A^\\top y)_i  \\lambda \\\\ 0  \\text{if } |(A^\\top y)_i| \\leq \\lambda \\\\ (A^\\top y)_i + \\lambda  \\text{if } (A^\\top y)_i  -\\lambda \\end{cases} $$\n这就是软阈值算子，记作 $S_{\\lambda}(\\cdot)$。解是 $x^*_{\\lambda} = S_{\\lambda}(A^\\top y)$，其中算子是逐分量作用的。\n\n为了使 BPDN 和 LASSO 问题的解等价，LASSO 的解 $x^*_{\\lambda}$ 必须以等式满足 BPDN 的约束，即 $\\|y - A x^*_{\\lambda}\\|_2 = \\epsilon$。这就定义了映射 $\\epsilon(\\lambda)$。我们现在计算这个残差范数。\n令 $P = AA^\\top$ 为到 $A$ 的列空间 $\\mathcal{R}(A)$ 上的正交投影矩阵。我们可以将向量 $y$ 分解为两个正交分量：$y = Py + (I - P)y$，其中 $Py \\in \\mathcal{R}(A)$ 且 $(I-P)y \\in \\mathcal{R}(A)^{\\perp}$。项 $Ax^*_{\\lambda}$ 也在 $\\mathcal{R}(A)$ 中。\n残差向量为 $r = y - Ax^*_{\\lambda} = (Py - Ax^*_{\\lambda}) + (I-P)y$。由于这两个分量是正交的，其范数的平方由毕达哥拉斯定理给出：\n$$ \\|y - A x^*_{\\lambda}\\|_2^2 = \\|Py - Ax^*_{\\lambda}\\|_2^2 + \\|(I-P)y\\|_2^2 $$\n代入 $P=AA^\\top$，第一项变为：\n$$ \\|AA^\\top y - A x^*_{\\lambda}\\|_2^2 = \\|A(A^\\top y - x^*_{\\lambda})\\|_2^2 $$\n使用性质 $A^\\top A = I_n$，这简化为：\n$$ (A^\\top y - x^*_{\\lambda})^\\top A^\\top A (A^\\top y - x^*_{\\lambda}) = \\|A^\\top y - x^*_{\\lambda}\\|_2^2 $$\n所以我们有：\n$$ \\epsilon(\\lambda)^2 = \\|y - A x^*_{\\lambda}\\|_2^2 = \\|A^\\top y - x^*_{\\lambda}\\|_2^2 + \\|(I - AA^\\top)y\\|_2^2 $$\n令 $z = A^\\top y$。则 $x^*_{\\lambda} = S_{\\lambda}(z)$。我们需要计算 $\\|z - S_{\\lambda}(z)\\|_2^2$。我们来考察其第 $i$ 个分量：\n- 如果 $|z_i| \\leq \\lambda$，则 $(S_{\\lambda}(z))_i = 0$，所以分量是 $z_i$。平方值为 $z_i^2 = \\min(|z_i|, \\lambda)^2$。\n- 如果 $z_i > \\lambda$，则 $(S_{\\lambda}(z))_i = z_i - \\lambda$，所以分量是 $z_i - (z_i - \\lambda) = \\lambda$。平方值为 $\\lambda^2 = \\min(|z_i|, \\lambda)^2$。\n- 如果 $z_i  -\\lambda$，则 $(S_{\\lambda}(z))_i = z_i + \\lambda$，所以分量是 $z_i - (z_i + \\lambda) = -\\lambda$。平方值为 $(-\\lambda)^2 = \\lambda^2 = \\min(|z_i|, \\lambda)^2$。\n在所有情况下，$(z - S_{\\lambda}(z))$ 的第 $i$ 个分量的平方都是 $\\min(|z_i|, \\lambda)^2$。\n因此，范数的平方是这些分量之和：\n$$ \\|A^\\top y - x^*_{\\lambda}\\|_2^2 = \\sum_{i=1}^{n} \\min\\left(|(A^\\top y)_i|, \\lambda\\right)^{2} $$\n将此代回 $\\epsilon(\\lambda)^2$ 的表达式，我们得到最终的映射：\n$$ \\epsilon(\\lambda)^2 = \\|(I - AA^\\top)y\\|_2^2 + \\sum_{i=1}^{n} \\min\\left(|(A^\\top y)_i|, \\lambda\\right)^{2} $$\n取平方根即可得到 $\\epsilon(\\lambda)$。\n\n约束（BPDN）和惩罚（LASSO）形式之间的等价性是凸优化中的一个普遍原理。对于任何 $\\lambda  0$，LASSO 问题的解 $x^*_{\\lambda}$ 也解一个对应的 BPDN 问题，其中 $\\epsilon = \\|y - Ax^*_{\\lambda}\\|_2$。反之，对于某个范围（从 $0$ 到 $\\|y\\|_2$）内的任何 $\\epsilon$，BPDN 问题的解 $x^*_{\\epsilon}$ 也解一个对应于某个 $\\lambda \\ge 0$ 的 LASSO 问题。这种对应关系源于约束优化的 Karush-Kuhn-Tucker (KKT) 条件。解的存在性和唯一性由平方 $\\ell_2$-范数的严格凸性和 $\\ell_1$-范数的凸性保证。强对偶性成立（例如，在 Slater's 条件下），确保了 KKT 条件是达到最优性的充分条件，并且最优原始值和对偶值相等，这巩固了拉格朗日乘子 $\\lambda$ 和约束边界 $\\epsilon$ 之间的联系。映射 $\\epsilon(\\lambda)$ 是连续且单调递增的：一个更强的惩罚 $\\lambda$ 会导致一个更稀疏的解，该解通常对数据的拟合较差，从而导致更大的残差范数 $\\epsilon$。\n\n$K$-折交叉验证（CV）是选择正则化参数 $\\lambda$ 的一种数据驱动方法。其工作原理是将 $m$ 个测量值（$A$ 的行和 $y$ 的相应条目）的集合划分为 $K$ 个不相交的子集。对于每一折，在 $K-1$ 个子集上训练一个 LASSO 模型，并在留出的子集上评估其预测误差。对一系列 $\\lambda$ 值重复此过程，并选择使所有折的平均预测误差最小化的那个值，记为 $\\lambda^*$。我们推导的映射为从这种数据驱动的 $\\lambda$ 选择到 BPDN 公式提供了一座直接的桥梁。一旦 CV 产生一个最优的 $\\lambda^*$，就可以使用 $\\epsilon(\\lambda^*)$ 的推导表达式计算相应的噪声容限 $\\epsilon^*$。这使得人们能够用一个经过严格选择的、有望提供良好样本外预测性能的参数 $\\epsilon^* = \\epsilon(\\lambda^*)$ 来求解（通常更直观的）BPDN 问题。",
            "answer": "$$\\boxed{\\sqrt{\\|(I - A A^\\top) y\\|_2^2 + \\sum_{i=1}^{n} \\min(|(A^\\top y)_i|, \\lambda)^{2}}}$$"
        },
        {
            "introduction": "在建立了核心理论联系之后，我们转向一个关键的实践问题：交叉验证的适用边界。虽然K折交叉验证是选择正则化参数的黄金标准，但它并非在所有情况下都万无一失。本练习通过一个精心设计的思想实验，探讨了当测量数据量 $m$ 仅略高于理论上成功恢复信号所需的最小样本数 $m_{\\min}$ 时的情景。你将发现，在这种临界情况下，标准的小 $K$ 值交叉验证可能会导致每个训练子集都样本不足，从而使得模型训练本身就无法稳定进行。这个练习将锻炼你对交叉验证内在假设的批判性思维，并强调在设计验证方案时考虑样本复杂度的重要性。",
            "id": "3441878",
            "problem": "考虑压缩感知中的线性逆模型，其中观测值 $y \\in \\mathbb{R}^m$ 是通过 $y = A x_{\\star} + w$ 得到的。这里，$A \\in \\mathbb{R}^{m \\times n}$ 的元素是均值为 $0$、方差为 $1/m$ 的独立同分布高斯随机变量， $x_{\\star} \\in \\mathbb{R}^n$ 是一个未知的 $s$-稀疏向量， $w \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ 是加性噪声。一种标准的估计量是最小绝对收缩和选择算子 (LASSO)，它对于给定的正则化水平 $\\lambda  0$，在一个测量值的训练子集 $(A_{\\text{tr}}, y_{\\text{tr}})$ 上求解 $\\min_{x \\in \\mathbb{R}^n} \\tfrac{1}{2} \\| A_{\\text{tr}} x - y_{\\text{tr}} \\|_2^2 + \\lambda \\| x \\|_1$，并在一个不相交的验证子集 $(A_{\\text{val}}, y_{\\text{val}})$ 上进行评分；正则化水平 $\\lambda$ 通过交叉验证 (CV) 来选择。众所周知且广泛应用的是，对于高斯设计，稳定恢复一个 $s$-稀疏向量所需的最小样本复杂度 $m_{\\min}$（在绝对常数和对数因子范围内）的数量级为 $m_{\\min} \\asymp C \\, s \\log(n/s)$，其中 $C  0$ 为某个绝对常数。并且，通过选择 $A$ 的行构成的子矩阵会保持相同的分布形式。在全文中，假设 $\\log$ 表示自然对数。\n\n假设 $n$、$s$ 和 $m$ 的取值使得 $m$ 仅略大于 $m_{\\min}$。具体地，取 $n = 10^5$，$s = 100$，$C = 2$，并假设 $m = 1500$。你正在考虑使用 $K$-折交叉验证，在每一折中，留出 $A$ 的一部分行（以及 $y$ 对应的条目）作为验证集，并使用剩余的行进行训练。\n\n下列哪个陈述是正确的？\n\nA. 当 $m$ 仅略高于最小样本复杂度时，使用等分的 $K$-折交叉验证且 $K$ 值较小（例如，$K = 5$）会使每个训练子问题相对于信息论阈值采样不足，从而导致恢复不稳定或不可能；一个直接的缓解措施是选择足够大的 $K$，以确保每次训练划分都至少使用稳定恢复所需的最小测量数。\n\nB. 由于验证样本不用于训练，交叉验证不会改变训练问题的样本复杂度；因此，如果完整问题高于阈值，那么无论 $K$ 取何值，$K$-折交叉验证中的所有训练子问题也都将高于阈值。\n\nC. 一种为确保满足最小样本复杂度要求的合适划分方法是，留出一个随机的列（特征）子集用于验证，同时保留所有 $m$ 个测量值用于训练；这可以在不改变感知行分布的情况下，使训练子问题保持在阈值之上。\n\nD. 一种为确保满足最小样本复杂度要求的分层划分是，构造的折其验证集大小最多为 $m - m_{\\min}$（从而确保每次训练划分至少使用 $m_{\\min}$ 个测量值），并将验证行分配到各个折中，使得每行最多用于验证一次；当 $m$ 接近 $m_{\\min}$ 时，这通常需要使用很多折。\n\nE. 在信息论阈值附近，两折交叉验证是最优的，因为它最大化了验证集的大小，从而减少了验证误差的方差；由于限制等距性质 (RIP)，训练问题保持稳定，因此在这种情况下应优先选择较小的 $K$。\n\n选择所有适用项。",
            "solution": "这个问题的核心在于，要通过 LASSO 等方法稳定地恢复一个 $s$-稀疏向量，测量数量必须达到或超过一个最小阈值 $m_{\\min}$。问题陈述了这个阈值为 $m_{\\min} \\asymp C s \\log(n/s)$。给定参数 $n = 10^5$，$s = 100$，$C = 2$，所需的最小样本数约为：\n$$m_{\\min} = 2 \\cdot 100 \\cdot \\log\\left(\\frac{10^5}{100}\\right) = 200 \\cdot \\log(1000) \\approx 200 \\cdot 6.9078 = 1381.55$$\n可用的总测量数为 $m=1500$。由于 $m  m_{\\min}$，完整问题是可解的。\n\n在 $K$-折交叉验证中，$m$ 个测量值（行）被划分为 $K$ 个大小各为 $m/K$ 的不相交的折。在每次运行中，一折用于验证，另外 $K-1$ 折用于训练。\n每个训练子问题的测量数为：\n$$m_{\\text{tr}} = m \\cdot \\frac{K-1}{K} = m \\left(1 - \\frac{1}{K}\\right)$$\n为了使 LASSO 估计量在每次训练中都有效，训练测量数 $m_{\\text{tr}}$ 本身必须足以实现稳定恢复。也就是说，我们必须有 $m_{\\text{tr}} \\geq m_{\\min}$。\n代入数值：\n$$1500 \\left(1 - \\frac{1}{K}\\right) \\geq 1381.55$$\n$$1 - \\frac{1}{K} \\geq \\frac{1381.55}{1500} \\approx 0.921$$\n$$\\frac{1}{K} \\leq 1 - 0.921 = 0.079$$\n$$K \\geq \\frac{1}{0.079} \\approx 12.66$$\n因此，为确保每个训练子问题都是良构的，我们必须使用 $K \\geq 13$。这意味着需要大量的折，这个过程接近于留一法交叉验证。\n\n现在，我们基于此分析来评估每个选项。\n\n**A. 当 $m$ 仅略高于最小样本复杂度时，使用等分的 $K$-折交叉验证且 $K$ 值较小（例如，$K = 5$）会使每个训练子问题相对于信息论阈值采样不足，从而导致恢复不稳定或不可能；一个直接的缓解措施是选择足够大的 $K$，以确保每次训练划分都至少使用稳定恢复所需的最小测量数。**\n让我们用 $K=5$ 的例子来检验这一点。\n训练样本数将是 $m_{\\text{tr}} = 1500 \\left(1 - \\frac{1}{5}\\right) = 1500 \\cdot \\frac{4}{5} = 1200$。\n由于 $1200  m_{\\min} \\approx 1382$，训练子问题确实是采样不足的。这意味着对于任何 $\\lambda$ 值，在这些训练集上的 LASSO 解都将不稳定，无法可靠地恢复真实的稀疏信号。所提出的缓解措施是选择足够大的 $K$ 以使 $m_{\\text{tr}} \\ge m_{\\min}$，我们的推导显示 $K \\geq 13$。该陈述与我们的分析完全一致。\n**结论：正确。**\n\n**B. 由于验证样本不用于训练，交叉验证不会改变训练问题的样本复杂度；因此，如果完整问题高于阈值，那么无论 $K$ 取何值，$K$-折交叉验证中的所有训练子问题也都将高于阈值。**\n这个陈述包含一个根本性的误解。“样本复杂度” $m_{\\min}$ 是问题规模（$n, s$）的内在属性，不会改变。然而，交叉验证明确地*减少*了可用于训练的样本数量（$m_{\\text{tr}} = m(1-1/K)  m$）。问题的核心恰恰在于，虽然完整问题高于阈值（$m > m_{\\min}$），但训练子问题可能不高于阈值（$m_{\\text{tr}}$ 可能小于 $m_{\\min}$）。如 $K=5$ 的例子所示，这是错误的。\n**结论：不正确。**\n\n**C. 一种为确保满足最小样本复杂度要求的合适划分方法是，留出一个随机的列（特征）子集用于验证，同时保留所有 $m$ 个测量值用于训练；这可以在不改变感知行分布的情况下，使训练子问题保持在阈值之上。**\n这提出了一种不同的交叉验证策略：划分 $A$ 的列（特征），而不是行（测量值）。在稀疏恢复的背景下，目标是识别向量 $x_{\\star} \\in \\mathbb{R}^n$ 的非零项。留出列意味着训练过程将无法估计与这些列相对应的系数。这不是一个调整正则化参数 $\\lambda$ 的有效方法，因为 $\\lambda$ 的目的是在整个特征空间上控制稀疏性和预测误差。针对此问题执行交叉验证的标准和正确方法是模拟对新测量值的泛化能力，这需要划分行。\n**结论：不正确。**\n\n**D. 一种为确保满足最小样本复杂度要求的分层划分是，构造的折其验证集大小最多为 $m - m_{\\min}$（从而确保每次训练划分至少使用 $m_{\\min}$ 个测量值），并将验证行分配到各个折中，使得每行最多用于验证一次；当 $m$ 接近 $m_{\\min}$ 时，这通常需要使用很多折。**\n这个陈述形式化了有效交叉验证划分的条件。训练集的大小是 $m_{\\text{tr}} = m - m_{\\text{val}}$，其中 $m_{\\text{val}}$ 是验证集的大小。稳定恢复的条件是 $m_{\\text{tr}} \\geq m_{\\min}$，这等价于 $m - m_{\\text{val}} \\geq m_{\\min}$，或 $m_{\\text{val}} \\leq m - m_{\\min}$。陈述的第一部分是正确的。\n第二部分指出，如果 $m$ 接近 $m_{\\min}$（即 $m - m_{\\min}$ 很小），这需要很多折。如果我们使用标准的 $K$-折交叉验证方案，其中所有 $m$ 行在 $K$ 个折中都被用作验证恰好一次，则验证实例的总数为 $m$。如果每个验证折的大小为 $m_{\\text{val}}$，那么 $K \\cdot m_{\\text{val}} \\approx m$。将此与约束 $m_{\\text{val}} \\leq m - m_{\\min}$ 结合，我们得到 $K \\geq m / m_{\\text{val}} \\geq m / (m - m_{\\min})$。当 $m$ 接近 $m_{\\min}$ 时，分母 $m - m_{\\min}$ 是一个小数，使得所需的 $K$ 很大。我们的计算得出 $m/(m-m_{\\min}) \\approx 1500 / (1500-1381.55) \\approx 12.66$，证实了需要很多折。\n**结论：正确。**\n\n**E. 在信息论阈值附近，两折交叉验证是最优的，因为它最大化了验证集的大小，从而减少了验证误差的方差；由于限制等距性质 (RIP)，训练问题保持稳定，因此在这种情况下应优先选择较小的 $K$。**\n让我们分析 $K=2$ 的情况。训练集大小变为 $m_{\\text{tr}} = m/2 = 1500/2 = 750$。这远低于所需的最小值 $m_{\\min} \\approx 1382$。因此，训练子问题是严重采样不足的，并且不会稳定。当测量数量对于给定的稀疏水平不足时，限制等距性质 (RIP) 恰恰是那个会失效的性质。在这种情况下，声称最大化验证集大小是最优的也是错误的；它会在训练方面产生严重有偏（实际上是无用的）模型。\n**结论：不正确。**",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "理论的完备性和对实践陷阱的认知是基础，但任何算法在现实世界中的应用都离不开对其计算成本的考量。交叉验证因其重复训练的特性而计算量巨大，这是一个众所周知的事实。本练习将引导你对这一成本进行精确的量化分析。你将推导在采用坐标下降和“热启动”（warm starts）等现代优化技巧时，运行K折交叉验证的总计算成本与在完整数据集上计算一次正则化路径的成本之比。通过这个过程，你将得出一个简洁而深刻的结论，这对于在实际项目中进行计算资源规划、选择合适的 $K$ 值以及评估更高效验证策略的可行性至关重要。",
            "id": "3441833",
            "problem": "考虑最小绝对收缩和选择算子 (Lasso) 回归问题，其目标函数由设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和响应向量 $y \\in \\mathbb{R}^{n}$ 定义为\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2n}\\|y - X \\beta\\|_2^2 + \\lambda \\|\\beta\\|_1,\n$$\n该目标函数在一个递减的正则化参数网格 $\\{\\lambda_{\\ell}\\}_{\\ell=1}^{L}$（其中 $\\lambda_{1}  \\lambda_{2}  \\cdots  \\lambda_{L}  0$）上进行评估。你需要使用一个在网格上实现温启动并在每折内重用数据相关因式分解的求解器，来执行 K-折交叉验证以选择正则化参数。\n\n假设以下建模选择和事实成立，这些在稀疏优化实践和分析中都是被广泛接受的：\n- K-折划分产生 K 个大小相等的、不相交的验证折，每个大小为 $n/K$；训练折的大小为 $m = n(K-1)/K$。\n- 使用一个坐标下降 Lasso 求解器，该求解器采用循环更新和跨 $\\lambda$ 的温启动。因此，在固定的训练集内，从冷启动求解第一个网格点 $\\lambda_1$ 的成本是 $t_0$ 次完整扫描，而后续的每个 $\\lambda_{\\ell}$（$\\ell \\geq 2$）从其温启动开始需要 $t$ 次扫描。整数 $t_0$ 和 $t$ 不依赖于 $n$、$p$、$K$ 或折的索引，并且为了主阶复杂性分析的目的，它们被视作常数。\n- 在大小为 $m$ 的训练集上对所有 $p$ 个坐标进行一次完整扫描的主阶成本为 $c \\, m p$ 次浮点运算，其中 $c  0$ 是一个常数，它包含了诸如缓存残差更新和稳定活性集内积维护等因式分解重用的微观优化。假设常数 $c$ 在各折之间以及在完整数据集上训练时是相同的，因为使用的是相同的实现。\n- 在一折内跨 $\\lambda$ 的因式分解重用是有效的，并已体现在减少的扫描次数 $t_0$ 和 $t$ 中；不同折之间没有重用，因为训练集不同。与拟合成本相比，计算验证损失的成本可以忽略不计，在主阶复杂性分析中可以忽略。\n\n令 $C_{\\mathrm{cv}}$ 表示在所有 $L$ 个网格点上计算 K-折交叉验证拟合的总主阶计算成本（以浮点运算次数计），令 $C_{\\mathrm{full}}$ 表示使用相同的求解器、温启动和因式分解重用，在大小为 $n$ 的完整数据集上计算一次完整正则化路径的总主阶计算成本。仅使用上述假设以及 K-折交叉验证和 Lasso 的基本定义，推导比率\n$$\nR \\;=\\; \\frac{C_{\\mathrm{cv}}}{C_{\\mathrm{full}}},\n$$\n的封闭形式表达式，并将其完全简化为仅含 $K$ 的形式。你的最终答案必须是这个表达式。不要包含任何中间量，也不要进行四舍五入或近似。",
            "solution": "用户希望找到比率 $R = \\frac{C_{\\mathrm{cv}}}{C_{\\mathrm{full}}}$，其中 $C_{\\mathrm{cv}}$ 是用于 Lasso 正则化路径选择的 K-折交叉验证的总计算成本，而 $C_{\\mathrm{full}}$ 是在完整数据集上计算相同路径的成本。推导将基于问题陈述中定义的主阶成本。\n\n首先，我们推导 $C_{\\mathrm{full}}$ 的表达式，即在大小为 $n$ 的完整数据集上针对 $L$ 个正则化参数拟合模型的成本。\n数据集包含 $n$ 个样本和 $p$ 个特征。\n在大小为 $n$ 的数据集上对所有 $p$ 个坐标进行一次完整坐标下降扫描的成本为 $c n p$ 次浮点运算。\n求解器为 $L$ 个参数的网格 $\\{\\lambda_{\\ell}\\}_{\\ell=1}^{L}$ 计算解。对于第一个参数 $\\lambda_1$，求解器从冷启动开始，需要 $t_0$ 次扫描。对于随后的 $L-1$ 个参数 $\\lambda_2, \\dots, \\lambda_L$，求解器使用温启动，每个需要 $t$ 次扫描。\n因此，计算完整正则化路径的总扫描次数为 $t_{0} + (L-1)t$。总主阶成本 $C_{\\mathrm{full}}$ 是每次扫描的成本与总扫描次数的乘积：\n$$\nC_{\\mathrm{full}} = (c n p) (t_{0} + (L-1)t)\n$$\n\n接下来，我们推导 $C_{\\mathrm{cv}}$ 的表达式，即 K-折交叉验证的总成本。\n在 K-折交叉验证中，数据集被划分为 K 折。该过程重复 K 次。在每次迭代 $k \\in \\{1, \\dots, K\\}$ 中，一折用于验证，其余 K-1 折用于训练。\n每个训练集的大小为 $m = n(K-1)/K$。在大小为 $m$ 的训练集上进行一次完整坐标下降扫描的成本为 $c m p$。代入 $m$ 的表达式：\n$$\n\\text{每折每次扫描的成本} = c \\left(n \\frac{K-1}{K}\\right) p\n$$\n对于 K 个训练集中的每一个，求解器都会在相同的 L 个参数上计算完整的正则化路径。扫描次数的逻辑与完整数据集的情况相同：$\\lambda_1$ 需要 $t_0$ 次扫描，随后的 $L-1$ 个参数中的每一个都需要 $t$ 次扫描。\n单个折的训练集的总扫描次数为 $t_{0} + (L-1)t$。为单个折训练模型的计算成本，我们称之为 $C_{\\mathrm{fold}}$，是：\n$$\nC_{\\mathrm{fold}} = \\left(c \\left(n \\frac{K-1}{K}\\right) p\\right) (t_{0} + (L-1)t)\n$$\n总交叉验证成本 $C_{\\mathrm{cv}}$ 是所有 K 折成本的总和。由于每折的训练集大小相同，因此 K 次迭代中每次的成本都相同。\n$$\nC_{\\mathrm{cv}} = K \\times C_{\\mathrm{fold}} = K \\left[ \\left(c \\left(n \\frac{K-1}{K}\\right) p\\right) (t_{0} + (L-1)t) \\right]\n$$\n我们可以通过将分子中的 K 与训练集大小项分母中的 K 相消来简化此表达式：\n$$\nC_{\\mathrm{cv}} = \\left(c n (K-1) p\\right) (t_{0} + (L-1)t)\n$$\n这可以改写为：\n$$\nC_{\\mathrm{cv}} = (K-1) \\left[ (c n p) (t_{0} + (L-1)t) \\right]\n$$\n\n最后，我们计算所求比率 $R = \\frac{C_{\\mathrm{cv}}}{C_{\\mathrm{full}}}$。代入 $C_{\\mathrm{cv}}$ 和 $C_{\\mathrm{full}}$ 的推导表达式：\n$$\nR = \\frac{(K-1) \\left[ (c n p) (t_{0} + (L-1)t) \\right]}{(c n p) (t_{0} + (L-1)t)}\n$$\n项 $(c n p) (t_{0} + (L-1)t)$ 是分子和分母的公因子。由于所有常数 $c, n, p, t_0, t$ 均为正数且 $L \\geq 1$，该因子非零，可以消去。\n这剩下：\n$$\nR = K-1\n$$\n这个结果是一个仅含 K 的简化封闭形式表达式，符合要求。它表明，在给定的假设下，K-折交叉验证的计算成本是在完整数据集上拟合模型一次成本的 K-1 倍。",
            "answer": "$$\n\\boxed{K-1}\n$$"
        }
    ]
}