{
    "hands_on_practices": [
        {
            "introduction": "One of the most powerful ways to understand the Lasso is to trace its solution path as the regularization parameter $\\lambda$ changes. This exercise provides a concrete example to compute the \"knots\" where the model's active set changes and to see firsthand how the degrees of freedom behave. By applying the Karush-Kuhn-Tucker (KKT) conditions, you will determine the sequence of models generated by the Lasso and establish the fundamental intuition that, for well-posed problems, the degrees of freedom correspond to the number of active variables in the model .",
            "id": "3443323",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nwith $n=3$ observations and $p=2$ predictors, where the design matrix $X \\in \\mathbb{R}^{3 \\times 2}$ has columns $x_{1}, x_{2}$ given by\n$$\nx_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \n\\qquad\nx_{2} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\\\ 0 \\end{pmatrix},\n$$\nand the response vector is\n$$\ny = \\begin{pmatrix} 3 \\\\ -\\frac{1}{\\sqrt{3}} \\\\ 0 \\end{pmatrix}.\n$$\nAll columns of $X$ have unit Euclidean norm and $x_{1}^{\\top} x_{2} = \\frac{1}{2}$. Let the regularization parameter $\\lambda \\ge 0$ vary from $+\\infty$ down to $0$, and consider the full LASSO solution path. Using only first principles in convex optimization (specifically the Karush–Kuhn–Tucker (KKT) conditions) and the definition of degrees of freedom as the divergence (trace of the Jacobian) of the fitted values mapping, perform the following:\n\n1. Determine the first knot $\\lambda^{(1)}$ at which the active set becomes nonempty, and the second knot $\\lambda^{(2)}$ at which the active set changes next along the LASSO path.\n\n2. On each segment of $\\lambda$ where the active set and its sign pattern remain fixed, compute the degrees of freedom (DoF) of the LASSO fit as the trace of the derivative of the fitted values with respect to $y$. Identify these values for the segments \n$$\nS_{0}: \\ \\lambda \\ge \\lambda^{(1)}, \n\\qquad\nS_{1}: \\ \\lambda^{(2)}  \\lambda  \\lambda^{(1)}, \n\\qquad\nS_{2}: \\ 0 \\le \\lambda  \\lambda^{(2)}.\n$$\n\nReport your final answer as a single row matrix \n$$\n\\big(\\lambda^{(1)}, \\ \\lambda^{(2)}, \\ d_{0}, \\ d_{1}, \\ d_{2}\\big),\n$$\nwhere $d_{i}$ denotes the degrees of freedom on segment $S_{i}$ for $i \\in \\{0,1,2\\}$. Provide exact values; no rounding is required.",
            "solution": "The user wants to find the knots and degrees of freedom for a specific LASSO problem.\n\nThe LASSO optimization problem is given by\n$$L(\\beta) = \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$$\nwhere $\\beta \\in \\mathbb{R}^{p}$. The Karush–Kuhn–Tucker (KKT) conditions for optimality state that the zero vector must be in the subgradient of $L(\\beta)$ at the solution $\\hat{\\beta}$. The gradient of the least-squares term is $\\nabla_{\\beta} \\left(\\frac{1}{2}\\|y - X \\beta\\|_{2}^{2}\\right) = -X^T(y - X\\beta)$. The subgradient of the $\\ell_1$-norm penalty term is $\\lambda \\partial \\|\\beta\\|_{1}$, where the $j$-th component of $\\partial \\|\\beta\\|_{1}$ is a vector $s$ such that $s_j = \\text{sign}(\\beta_j)$ if $\\beta_j \\neq 0$ and $s_j \\in [-1, 1]$ if $\\beta_j = 0$.\n\nThe KKT conditions are therefore:\n$$-X^T(y - X\\hat{\\beta}) + \\lambda s = 0, \\quad s \\in \\partial\\|\\hat{\\beta}\\|_1$$\nThis can be written for each component $j \\in \\{1, \\dots, p\\}$:\n$$x_j^T(y - X\\hat{\\beta}) = \\lambda s_j$$\nwhere $s_j = \\text{sign}(\\hat{\\beta}_j)$ if $\\hat{\\beta}_j \\neq 0$ and $|s_j| \\le 1$ if $\\hat{\\beta}_j=0$.\nThis is equivalent to:\n1. If $\\hat{\\beta}_j \\neq 0$, then $x_j^T(y - X\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_j)$.\n2. If $\\hat{\\beta}_j = 0$, then $|x_j^T(y - X\\hat{\\beta})| \\le \\lambda$.\n\nLet's compute the necessary matrix products with the given data:\n$X = \\begin{pmatrix} 1  1/2 \\\\ 0  \\sqrt{3}/2 \\\\ 0  0 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ -1/\\sqrt{3} \\\\ 0 \\end{pmatrix}$.\nThe correlation vector $X^T y$ is:\n$$X^T y = \\begin{pmatrix} 1  0  0 \\\\ 1/2  \\sqrt{3}/2  0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1/\\sqrt{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ \\frac{3}{2} - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$$\nSo, $x_1^T y = 3$ and $x_2^T y = 1$. The Gram matrix $X^T X$ is:\n$$X^T X = \\begin{pmatrix} x_1^T x_1  x_1^T x_2 \\\\ x_2^T x_1  x_2^T x_2 \\end{pmatrix} = \\begin{pmatrix} 1  1/2 \\\\ 1/2  1 \\end{pmatrix}$$\n\n**Part 1: Determination of the knots $\\lambda^{(1)}$ and $\\lambda^{(2)}$**\n\n**Finding the first knot $\\lambda^{(1)}$:**\nFor very large $\\lambda$, the penalty term dominates, forcing the solution to be $\\hat{\\beta}=0$. The KKT conditions for $\\hat{\\beta}=0$ simplify to $|x_j^T y| \\le \\lambda$ for all $j \\in \\{1, 2\\}$.\nA coefficient can become non-zero only when $\\lambda$ decreases to a value where this condition is violated. The first coefficient becomes non-zero at $\\lambda = \\max_j |x_j^T y|$.\n$$\\lambda^{(1)} = \\max(|x_1^T y|, |x_2^T y|) = \\max(3, 1) = 3$$\nThe first variable to enter the active set is the one corresponding to the maximum correlation, which is predictor $1$. So, for $\\lambda  \\lambda^{(1)}$, we have $\\hat{\\beta}_1 \\neq 0$.\n\n**Finding the second knot $\\lambda^{(2)}$:**\nFor the segment $\\lambda  \\lambda^{(1)}$, the active set is $\\mathcal{A}=\\{1\\}$, meaning $\\hat{\\beta}_1 \\neq 0$ and $\\hat{\\beta}_2=0$. The KKT conditions are:\n1. $x_1^T(y - x_1\\hat{\\beta}_1) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_1)$\n2. $|x_2^T(y - x_1\\hat{\\beta}_1)| \\le \\lambda$\n\nFrom the first condition: $x_1^T y - (x_1^T x_1)\\hat{\\beta}_1 = \\lambda s_1$. Since $\\lambda$ decreases from $\\lambda^{(1)}=3$, $\\hat{\\beta}_1$ will emerge from $0$. Its sign will be that of $x_1^T y = 3$, which is positive. So, $s_1 = \\text{sign}(\\hat{\\beta}_1)=1$.\n$$3 - (1)\\hat{\\beta}_1 = \\lambda \\cdot 1 \\implies \\hat{\\beta}_1 = 3 - \\lambda$$\nThis is consistent, as $\\hat{\\beta}_1  0$ for $\\lambda  3$.\n\nThe second knot $\\lambda^{(2)}$ occurs when the second KKT condition meets its boundary, i.e., when predictor $2$ enters the model. We must solve for $\\lambda$ in $|x_2^T(y - x_1\\hat{\\beta}_1)| = \\lambda$.\nLet $C_2(\\lambda) = x_2^T(y - x_1\\hat{\\beta}_1) = x_2^T y - (x_2^T x_1)\\hat{\\beta}_1$.\nUsing our pre-computed values, $x_2^T y = 1$ and $x_2^T x_1 = 1/2$:\n$$C_2(\\lambda) = 1 - \\frac{1}{2}(3-\\lambda) = 1 - \\frac{3}{2} + \\frac{\\lambda}{2} = \\frac{\\lambda}{2} - \\frac{1}{2}$$\nThe condition becomes $|\\frac{\\lambda}{2} - \\frac{1}{2}| = \\lambda$. This gives two possibilities:\na) $\\frac{\\lambda}{2} - \\frac{1}{2} = \\lambda \\implies -\\frac{\\lambda}{2} = \\frac{1}{2} \\implies \\lambda = -1$, which is not valid as $\\lambda \\ge 0$.\nb) $-(\\frac{\\lambda}{2} - \\frac{1}{2}) = \\lambda \\implies \\frac{1}{2} - \\frac{\\lambda}{2} = \\lambda \\implies \\frac{1}{2} = \\frac{3\\lambda}{2} \\implies \\lambda = \\frac{1}{3}$.\nThis value is in the interval $(0, \\lambda^{(1)})$. Thus, the second knot is $\\lambda^{(2)} = \\frac{1}{3}$.\n\n**Summary of solution path segments:**\n- For $\\lambda \\ge \\lambda^{(1)} = 3$ ($S_0$): $\\hat{\\beta}=(0, 0)^T$. Active set $\\mathcal{A}_0 = \\emptyset$.\n- For $\\lambda^{(2)}  \\lambda  \\lambda^{(1)}$, i.e., $1/3  \\lambda  3$ ($S_1$): $\\hat{\\beta}=(3-\\lambda, 0)^T$. Active set $\\mathcal{A}_1 = \\{1\\}$.\n- For $0 \\le \\lambda  \\lambda^{(2)} = 1/3$ ($S_2$): Both predictors are in the model, so $\\mathcal{A}_2=\\{1, 2\\}$. The KKT conditions form a linear system: $X^T(y - X\\hat{\\beta}) = \\lambda s$.\n$$\\hat{\\beta} = (X^T X)^{-1} (X^T y - \\lambda s)$$\nThe sign vector $s=(s_1, s_2)^T$ needs to be determined. For $\\beta_1$, $s_1=1$ as before. For $\\beta_2$, its sign upon entry is the sign of the correlation $C_2(\\lambda)$ at the knot $\\lambda^{(2)}=1/3$.\n$C_2(1/3) = \\frac{1/3}{2} - \\frac{1}{2} = \\frac{1}{6} - \\frac{1}{2} = -\\frac{2}{6} = -\\frac{1}{3}$. The sign is negative, so $s_2=-1$.\nSo, $s=(1, -1)^T$.\nLet's compute $(X^T X)^{-1}$:\n$$(X^T X)^{-1} = \\begin{pmatrix} 1  1/2 \\\\ 1/2  1 \\end{pmatrix}^{-1} = \\frac{1}{1 - 1/4} \\begin{pmatrix} 1  -1/2 \\\\ -1/2  1 \\end{pmatrix} = \\frac{4}{3} \\begin{pmatrix} 1  -1/2 \\\\ -1/2  1 \\end{pmatrix} = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix}$$\nNow we find $\\hat{\\beta}$ for this segment:\n$$\\hat{\\beta} = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix} \\left( \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\right) = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix} \\begin{pmatrix} 3-\\lambda \\\\ 1+\\lambda \\end{pmatrix}$$\n$$\\hat{\\beta}_1 = \\frac{4}{3}(3-\\lambda) - \\frac{2}{3}(1+\\lambda) = \\frac{12-4\\lambda-2-2\\lambda}{3} = \\frac{10-6\\lambda}{3} = \\frac{10}{3} - 2\\lambda$$\n$$\\hat{\\beta}_2 = -\\frac{2}{3}(3-\\lambda) + \\frac{4}{3}(1+\\lambda) = \\frac{-6+2\\lambda+4+4\\lambda}{3} = \\frac{-2+6\\lambda}{3} = 2\\lambda - \\frac{2}{3}$$\nFor $\\lambda \\in [0, 1/3)$, $\\hat{\\beta}_1  0$ and $\\hat{\\beta}_2  0$, which is consistent with our assumed signs $s=(1, -1)^T$.\n\n**Part 2: Computation of the Degrees of Freedom (DoF)**\n\nThe Degrees of Freedom (DoF) is defined as $d = \\text{Tr}\\left(\\frac{\\partial \\hat{y}}{\\partial y}\\right)$, where $\\hat{y} = X\\hat{\\beta}_\\lambda(y)$ are the fitted values. On any segment between knots, the active set $\\mathcal{A}$ and the signs $s_\\mathcal{A}$ are fixed. Let $X_\\mathcal{A}$ be the submatrix of $X$ with columns in $\\mathcal{A}$. The solution is given by:\n$$\\hat{\\beta}_\\mathcal{A} = (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} (X_\\mathcal{A}^T y - \\lambda s_\\mathcal{A})$$\nThe fitted values are $\\hat{y} = X_\\mathcal{A}\\hat{\\beta}_\\mathcal{A} = X_\\mathcal{A} (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} (X_\\mathcal{A}^T y - \\lambda s_\\mathcal{A})$.\nDifferentiating with respect to the vector $y$:\n$$\\frac{\\partial \\hat{y}}{\\partial y} = X_\\mathcal{A} (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} X_\\mathcal{A}^T$$\nThis is the projection matrix onto the column space of $X_\\mathcal{A}$. The DoF is its trace:\n$$d = \\text{Tr}\\left(X_\\mathcal{A} (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} X_\\mathcal{A}^T\\right) = \\text{Tr}\\left((X_\\mathcal{A}^T X_\\mathcal{A})^{-1} X_\\mathcal{A}^T X_\\mathcal{A}\\right) = \\text{Tr}(I_{|\\mathcal{A}|}) = |\\mathcal{A}|$$\nSo, the degrees of freedom on each segment is simply the size of the active set.\n\n-   **Segment $S_0: \\lambda \\ge 3$**: The active set is empty, $\\mathcal{A}_0 = \\emptyset$. So $|\\mathcal{A}_0|=0$.\n    $$d_0 = 0$$\n    Rigorously, $\\hat{\\beta}=0$, so $\\hat{y}=0$, and $\\frac{\\partial \\hat{y}}{\\partial y}$ is the zero matrix, whose trace is $0$.\n\n-   **Segment $S_1: 1/3  \\lambda  3$**: The active set is $\\mathcal{A}_1 = \\{1\\}$. So $|\\mathcal{A}_1|=1$.\n    $$d_1 = 1$$\n    Rigorously, $\\frac{\\partial \\hat{y}}{\\partial y} = x_1 (x_1^T x_1)^{-1} x_1^T = x_1 (1)^{-1} x_1^T = x_1 x_1^T$.\n    $d_1 = \\text{Tr}(x_1 x_1^T) = \\text{Tr}(x_1^T x_1) = \\text{Tr}(1) = 1$.\n\n-   **Segment $S_2: 0 \\le \\lambda  1/3$**: The active set is $\\mathcal{A}_2=\\{1, 2\\}$. So $|\\mathcal{A}_2|=2$.\n    $$d_2 = 2$$\n    Rigorously, $\\frac{\\partial \\hat{y}}{\\partial y} = X (X^T X)^{-1} X^T$.\n    $d_2 = \\text{Tr}(X (X^T X)^{-1} X^T) = \\text{Tr}((X^T X)^{-1} X^T X) = \\text{Tr}(I_2) = 2$.\n\nThe final results are $\\lambda^{(1)}=3$, $\\lambda^{(2)}=1/3$, $d_0=0$, $d_1=1$, and $d_2=2$. These are to be reported in a single row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3  \\frac{1}{3}  0  1  2\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The previous exercise builds the intuition that degrees of freedom simply count active variables, a heuristic that holds in many common scenarios. This practice challenges that simple view by exploring a critical edge case: when predictor variables are perfectly collinear. Working through this carefully constructed example reveals that the degrees of freedom are more fundamentally related to the rank of the active design submatrix, $\\operatorname{rank}(A_S)$, which is a more robust and accurate measure of model complexity . This distinction is crucial for correctly interpreting model complexity in the presence of degenerate designs.",
            "id": "3443335",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) estimator, formally the $\\ell_{1}$-regularized least squares estimator, in the linear model $y = \\mu + \\varepsilon$ where $\\varepsilon \\sim \\mathcal{N}(0, I_{n})$ and $I_{n}$ is the $n \\times n$ identity matrix. Let $A \\in \\mathbb{R}^{3 \\times 2}$ be a design matrix whose two columns are identical and normalized, specifically $a_{1} = a_{2} = a$ with $a = \\frac{1}{\\sqrt{3}}(1,1,1)^{\\top}$ so that $\\|a\\|_{2} = 1$. The LASSO estimator is defined by\n$$\n\\hat{x}(y) \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\},\n$$\nand the fitted values are $\\hat{y}(y) = A \\hat{x}(y)$. Assume the observed data vector is $y = 2 a$ and the regularization parameter is $\\lambda = \\frac{1}{2}$.\n\nUsing only fundamental definitions and optimality conditions, do the following:\n- Establish that there exists an optimal solution $\\hat{x}(y)$ whose support $S = \\{ j : \\hat{x}_{j}(y) \\neq 0 \\}$ satisfies $|S| = 2$ even though the two active columns are identical.\n- Using the definition of degrees of freedom via Stein’s unbiased risk framework, namely the divergence of the fitted value mapping $y \\mapsto \\hat{y}(y)$ at the observed $y$, compute the value of $\\operatorname{df}(\\hat{y})$ for this setup.\n\nExpress the final answer as a single real number. No rounding is required.",
            "solution": "The user wants me to solve a problem about the LASSO estimator with two identical, normalized columns in the design matrix.\n\n### Problem Validation\nI will first validate the problem statement.\n\n**Step 1: Extract Givens**\n-   Linear model: $y = \\mu + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, I_{n})$ and $n=3$.\n-   Design matrix: $A \\in \\mathbb{R}^{3 \\times 2}$ with columns $a_{1} = a_{2} = a = \\frac{1}{\\sqrt{3}}(1,1,1)^{\\top}$, and $\\|a\\|_{2} = 1$.\n-   LASSO estimator definition: $\\hat{x}(y) \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\}$.\n-   Fitted values definition: $\\hat{y}(y) = A \\hat{x}(y)$.\n-   Observed data vector: $y = 2 a$.\n-   Regularization parameter: $\\lambda = \\frac{1}{2}$.\n-   Degrees of freedom definition: $\\operatorname{df}(\\hat{y}) = \\nabla \\cdot \\hat{y}(y) = \\sum_{i=1}^n \\frac{\\partial \\hat{y}_i}{\\partial y_i}$.\n-   Tasks:\n    1.  Show that an optimal solution $\\hat{x}(y)$ exists with support size $|S|=2$.\n    2.  Compute $\\operatorname{df}(\\hat{y})$ for the given setup.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is well-established in the field of statistical learning and optimization theory. The LASSO estimator, its properties under collinearity, and the concept of degrees of freedom via Stein's Unbiased Risk Estimate (SURE) are standard topics. The scenario with identical columns is a legitimate, albeit degenerate, case used to probe a deeper understanding of the method.\n-   **Well-Posed**: The problem is well-posed. The LASSO objective function is convex, guaranteeing the existence of a minimum. While the minimizer $\\hat{x}$ is not unique due to the collinearity of the columns of $A$, the fitted value vector $\\hat{y} = A\\hat{x}$ is unique. The divergence of the mapping $y \\mapsto \\hat{y}(y)$ is well-defined almost everywhere, and the problem asks for its value at a specific point where it is defined.\n-   **Objective**: The problem is stated using precise mathematical language, with no subjective or ambiguous terms.\n-   **Flaw Checklist**: The problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, and well-structured.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. I will proceed with the solution.\n\n### Solution\nThe LASSO objective function to be minimized is\n$$\nL(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\nwhere $x = (x_1, x_2)^\\top$. Given that $A = [a, a]$, the term $Ax$ can be written as $Ax = x_1 a + x_2 a = (x_1+x_2)a$.\nLet $s = x_1+x_2$. The objective function becomes\n$$\nL(x) = \\frac{1}{2}\\|y - s a\\|_{2}^{2} + \\lambda (|x_1| + |x_2|)\n$$\nWe need to minimize this function with respect to $x_1$ and $x_2$. This can be done in two stages. First, for a fixed sum $s = x_1+x_2$, we minimize the penalty term $\\lambda(|x_1| + |x_2|)$. By the triangle inequality, $|x_1|+|x_2| \\ge |x_1+x_2| = |s|$, with equality holding if and only if $x_1$ and $x_2$ have the same sign (or one or both are zero). Thus, the minimum value of the penalty term for a fixed $s$ is $\\lambda|s|$.\n\nThe problem then reduces to minimizing the following function of a single variable $s$:\n$$\ng(s) = \\frac{1}{2}\\|y - s a\\|_{2}^{2} + \\lambda|s|\n$$\nExpanding the squared norm, we get:\n$$\ng(s) = \\frac{1}{2}(y - sa)^\\top(y - sa) + \\lambda|s| = \\frac{1}{2}(y^\\top y - 2s a^\\top y + s^2 a^\\top a) + \\lambda|s|\n$$\nSince $\\|a\\|_2 = 1$, we have $a^\\top a = 1$. The function to minimize is\n$$\ng(s) = \\frac{1}{2}s^2 - (a^\\top y)s + \\frac{1}{2}y^\\top y + \\lambda|s|\n$$\nThis is a one-dimensional LASSO problem. The solution $\\hat{s}$ that minimizes $g(s)$ is given by the soft-thresholding operator applied to the least-squares estimate $a^\\top y$:\n$$\n\\hat{s}(y) = S_{\\lambda}(a^\\top y) = \\operatorname{sign}(a^\\top y) \\max(|a^\\top y| - \\lambda, 0)\n$$\nFor the given data $y=2a$ and $\\lambda = 1/2$, we first compute $a^\\top y$:\n$$\na^\\top y = a^\\top (2a) = 2 a^\\top a = 2(1) = 2\n$$\nPlugging this into the soft-thresholding formula:\n$$\n\\hat{s} = S_{1/2}(2) = \\operatorname{sign}(2) \\max(|2| - 1/2, 0) = 1 \\cdot \\max(3/2, 0) = \\frac{3}{2}\n$$\nThe set of optimal solutions $\\hat{x} = (\\hat{x}_1, \\hat{x}_2)^\\top$ for the original problem must satisfy two conditions:\n1.  $\\hat{x}_1 + \\hat{x}_2 = \\hat{s} = \\frac{3}{2}$\n2.  $|\\hat{x}_1| + |\\hat{x}_2| = |\\hat{s}| = \\frac{3}{2}$\n\nThe second condition implies that $\\hat{x}_1$ and $\\hat{x}_2$ must be non-negative, since their sum is positive. Thus, the set of optimal solutions is the line segment in $\\mathbb{R}^2$ given by $\\{(\\hat{x}_1, \\hat{x}_2) \\mid \\hat{x}_1 + \\hat{x}_2 = 3/2, \\hat{x}_1 \\ge 0, \\hat{x}_2 \\ge 0\\}$.\n\n**Part 1: Existence of a solution with support size 2**\nThe problem asks to show that an optimal solution $\\hat{x}(y)$ exists with support size $|S|=2$. We can choose any point in the interior of the solution segment. For example, let $\\hat{x}_1 = \\hat{x}_2 = 3/4$. This choice satisfies the conditions: $\\hat{x}_1 + \\hat{x}_2 = 3/4 + 3/4 = 3/2$, and $\\hat{x}_1, \\hat{x}_2  0$. The support for this solution is $S=\\{1, 2\\}$, and its size is $|S|=2$. This establishes the first part of the problem.\n\n**Part 2: Computation of degrees of freedom**\nThe fitted value vector $\\hat{y}$ is unique, regardless of the choice of $\\hat{x}$ from the solution set:\n$$\n\\hat{y}(y) = A\\hat{x}(y) = (\\hat{x}_1(y) + \\hat{x}_2(y))a = \\hat{s}(y) a\n$$\nSubstituting the expression for $\\hat{s}(y)$, we have the mapping $y \\mapsto \\hat{y}(y)$:\n$$\n\\hat{y}(y) = S_{\\lambda}(a^\\top y) a\n$$\nThe degrees of freedom are defined as the divergence of this vector field:\n$$\n\\operatorname{df}(\\hat{y}) = \\nabla \\cdot \\hat{y}(y) = \\sum_{i=1}^3 \\frac{\\partial \\hat{y}_i}{\\partial y_i}\n$$\nThe $i$-th component of $\\hat{y}$ is $\\hat{y}_i(y) = S_{\\lambda}(a^\\top y) a_i$. We compute the partial derivative with respect to $y_j$:\n$$\n\\frac{\\partial \\hat{y}_i}{\\partial y_j} = \\frac{\\partial}{\\partial y_j} [S_{\\lambda}(a^\\top y) a_i] = a_i \\frac{\\partial}{\\partial y_j} [S_{\\lambda}(a^\\top y)]\n$$\nUsing the chain rule, let $u(y) = a^\\top y = \\sum_{k=1}^3 a_k y_k$. Then $\\frac{\\partial u}{\\partial y_j} = a_j$.\n$$\n\\frac{\\partial \\hat{y}_i}{\\partial y_j} = a_i \\frac{d S_{\\lambda}(u)}{du} \\frac{\\partial u}{\\partial y_j} = a_i a_j \\frac{d S_{\\lambda}(u)}{du}\n$$\nThis is the $(i,j)$-th element of the Jacobian matrix of the map $\\hat{y}$. The divergence is the trace of this Jacobian:\n$$\n\\operatorname{df}(\\hat{y}) = \\sum_{i=1}^3 \\frac{\\partial \\hat{y}_i}{\\partial y_i} = \\sum_{i=1}^3 a_i^2 \\frac{d S_{\\lambda}(u)}{du} = \\left(\\sum_{i=1}^3 a_i^2\\right) \\frac{d S_{\\lambda}(u)}{du}\n$$\nSince $\\sum_{i=1}^3 a_i^2 = \\|a\\|_2^2 = 1$, this simplifies to:\n$$\n\\operatorname{df}(\\hat{y}) = \\frac{d S_{\\lambda}(u)}{du}\n$$\nThe derivative of the soft-thresholding function $S_{\\lambda}(u)$ is $1$ if $|u|  \\lambda$ and $0$ if $|u|  \\lambda$. It is undefined at $|u|=\\lambda$. Thus, for almost all $y$, the derivative is the indicator function $\\mathbb{I}(|u|  \\lambda)$.\n$$\n\\operatorname{df}(\\hat{y}) = \\mathbb{I}(|a^\\top y|  \\lambda)\n$$\nWe now evaluate this for the specific values given in the problem: $y=2a$ and $\\lambda=1/2$.\nWe have $a^\\top y = 2$. We need to check if $|2|  1/2$. This condition is true.\nTherefore, the value of the degrees of freedom is:\n$$\n\\operatorname{df}(\\hat{y}) = \\mathbb{I}(2  1/2) = 1\n$$\nThis result is notable. While there exists a solution $\\hat{x}$ with support size $2$, the degrees of freedom of the fit $\\hat{y}$ is $1$. This illustrates that the common heuristic $\\operatorname{df} \\approx |S|$ can be misleading in degenerate cases like perfect collinearity. The degrees of freedom correspond to the dimensionality of the subspace selected by the estimator, which in this case is the one-dimensional subspace spanned by the vector $a$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Having explored the piecewise-constant nature of degrees of freedom along the Lasso path, we now adopt a more general, continuous perspective. This advanced practice investigates how the degrees of freedom behave when the design matrix itself is slightly perturbed from a simple orthonormal structure. By connecting the degrees of freedom to the Jacobian of the proximal operator, this exercise demonstrates how to analyze the estimator's properties through first-order perturbation theory, revealing its sensitivity to the geometric configuration of the predictors .",
            "id": "3443358",
            "problem": "Consider the least-squares denoising formulation with an $\\ell_{1}$-regularized estimator defined by the proximal operator. For an orthonormal transform matrix $A \\in \\mathbb{R}^{n \\times n}$, the fitted signal can be expressed as $\\hat{y} = A \\,\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}(A^{\\top} y)$, where $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}(v)$ denotes the proximal operator of the function $\\lambda \\|v\\|_{1}$, that is, the unique minimizer of $\\frac{1}{2}\\|u - v\\|_{2}^{2} + \\lambda \\|u\\|_{1}$. Let the degrees of freedom be defined via the Jacobian of the fit mapping as $df(y) = \\operatorname{tr}\\big(\\nabla \\hat{y}(y)\\big)$, consistent with Stein’s unbiased risk estimate (SURE). Assume differentiability holds at the point of evaluation (no coordinate lies exactly at a threshold).\n\nWork in dimension $n=5$. Take $\\lambda = 1$ and the observation vector $y \\in \\mathbb{R}^{5}$ given by $y = (3,\\, 0.5,\\, -2.7,\\, 0.1,\\, 5)^{\\top}$. \n\n1. In the orthonormal identity case $A = I$, identify the Jacobian of the map $y \\mapsto \\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}(y)$ at the given $y$ and express the degrees of freedom $df_{0}(y)$ as the trace of this Jacobian.\n\n2. Now consider an approximately orthonormal perturbation $A_{\\epsilon} = I + \\frac{\\epsilon}{2} H$ with a small parameter $\\epsilon \\in \\mathbb{R}$ and a fixed symmetric matrix $H \\in \\mathbb{R}^{5 \\times 5}$ satisfying $A_{\\epsilon}^{\\top} A_{\\epsilon} = I + \\epsilon H + \\mathcal{O}(\\epsilon^{2})$. Let the estimator be defined by the composition\n$$\n\\hat{y}_{\\epsilon}(y) \\;=\\; A_{\\epsilon}\\,\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}\\!\\big(A_{\\epsilon}^{\\top} y\\big).\n$$\nAssume that no entry of $y$ lies on the threshold $\\{|y_{i}| = \\lambda\\}$ and that for sufficiently small $|\\epsilon|$ the active set induced by the soft-thresholding pattern remains unchanged in a neighborhood of the given $y$. Derive, to first order in $\\epsilon$, the degrees of freedom\n$$\ndf_{\\epsilon}(y) \\;=\\; \\operatorname{tr}\\!\\big(\\nabla \\hat{y}_{\\epsilon}(y)\\big),\n$$\nexpressed in terms of the soft-thresholding active set at $\\epsilon = 0$ and the entries of $H$.\n\nEvaluate your first-order expression at the specific symmetric matrix\n$$\nH \\;=\\; \\begin{pmatrix}\n\\frac{1}{5}  \\frac{1}{10}  0  0  \\frac{1}{20} \\\\\n\\frac{1}{10}  -\\frac{1}{10}  \\frac{1}{30}  0  0 \\\\\n0  \\frac{1}{30}  \\frac{3}{10}  0  -\\frac{1}{40} \\\\\n0  0  0  0  0 \\\\\n\\frac{1}{20}  0  -\\frac{1}{40}  0  -\\frac{1}{5}\n\\end{pmatrix}.\n$$\n\nProvide your final answer as a single closed-form analytic expression in $\\epsilon$ for $df_{\\epsilon}(y)$, accurate to first order in $\\epsilon$. No rounding is required.",
            "solution": "We begin from the definition of the proximal operator of the $\\ell_{1}$ norm. For any $v \\in \\mathbb{R}^{n}$ and $\\lambda  0$, the proximal operator $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}(v)$ is given by the componentwise soft-thresholding mapping\n$$\n\\big[\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}(v)\\big]_{i} \\;=\\; \\operatorname{sign}(v_{i}) \\,\\max\\{|v_{i}| - \\lambda,\\, 0\\}.\n$$\nThe Jacobian of this mapping exists almost everywhere and is diagonal with entries\n$$\n\\frac{\\partial}{\\partial v_{i}} \\big[\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}(v)\\big]_{i} \\;=\\; \\begin{cases}\n1,  \\text{if } |v_{i}|  \\lambda, \\\\\n0,  \\text{if } |v_{i}|  \\lambda,\n\\end{cases}\n$$\nand is undefined at $|v_{i}| = \\lambda$ (a measure-zero set). Under the given assumption that differentiability holds (no coordinate lies exactly at a threshold), the Jacobian is the diagonal matrix $D(v) = \\operatorname{diag}\\big(\\mathbf{1}\\{|v_{i}|  \\lambda\\}\\big)$.\n\nPart 1 (orthonormal identity case). For $A = I$, the fit is $\\hat{y}(y) = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}(y)$. Its Jacobian at $y$ is $J_{0}(y) = D(y) = \\operatorname{diag}\\big(\\mathbf{1}\\{|y_{i}|  \\lambda\\}\\big)$. The degrees of freedom is the divergence\n$$\ndf_{0}(y) \\;=\\; \\operatorname{tr}\\big(J_{0}(y)\\big) \\;=\\; \\sum_{i=1}^{n} \\mathbf{1}\\{|y_{i}|  \\lambda\\}.\n$$\nWith $y = (3,\\, 0.5,\\, -2.7,\\, 0.1,\\, 5)^{\\top}$ and $\\lambda = 1$, the active coordinates are those with $|y_{i}|  1$, namely indices $i \\in \\{1, 3, 5\\}$. Thus $D(y) = \\operatorname{diag}(1, 0, 1, 0, 1)$ and $df_{0}(y) = 3$.\n\nPart 2 (approximately orthonormal perturbation). Consider $A_{\\epsilon} = I + \\frac{\\epsilon}{2} H$ with symmetric $H$ so that\n$$\nA_{\\epsilon}^{\\top} A_{\\epsilon} \\;=\\; \\big(I + \\tfrac{\\epsilon}{2} H\\big)^{\\top}\\big(I + \\tfrac{\\epsilon}{2} H\\big) \\;=\\; I + \\epsilon H + \\mathcal{O}(\\epsilon^{2}).\n$$\nDefine the estimator\n$$\n\\hat{y}_{\\epsilon}(y) \\;=\\; A_{\\epsilon}\\,\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}(A_{\\epsilon}^{\\top} y).\n$$\nUsing the chain rule and the fact that $A_{\\epsilon}$ is constant with respect to $y$, the Jacobian of $\\hat{y}_{\\epsilon}$ at $y$ is\n$$\nJ_{\\epsilon}(y) \\;=\\; A_{\\epsilon}\\, D\\!\\big(A_{\\epsilon}^{\\top} y\\big)\\, A_{\\epsilon}^{\\top},\n$$\nwhere $D(\\cdot)$ is the diagonal derivative of the soft-thresholding map evaluated at its argument. By the stated separation assumption (no coordinate at threshold and stability of the active set for sufficiently small $|\\epsilon|$), there exists an open neighborhood of $\\epsilon = 0$ on which the active set does not change. Therefore,\n$$\nD\\!\\big(A_{\\epsilon}^{\\top} y\\big) \\;=\\; D\\!\\big(A_{0}^{\\top} y\\big) \\;=\\; D(y) \\;\\equiv\\; D_{0},\n$$\nfor all sufficiently small $|\\epsilon|$. Hence\n$$\nJ_{\\epsilon}(y) \\;=\\; A_{\\epsilon}\\, D_{0}\\, A_{\\epsilon}^{\\top} \\;=\\; \\Big(I + \\tfrac{\\epsilon}{2} H\\Big) D_{0} \\Big(I + \\tfrac{\\epsilon}{2} H\\Big) + \\mathcal{O}(\\epsilon^{2}).\n$$\nExpanding to first order in $\\epsilon$ gives\n$$\nJ_{\\epsilon}(y) \\;=\\; D_{0} + \\tfrac{\\epsilon}{2}\\big(H D_{0} + D_{0} H\\big) + \\mathcal{O}(\\epsilon^{2}).\n$$\nTaking the trace and using the cyclic property of the trace,\n$$\ndf_{\\epsilon}(y) \\;=\\; \\operatorname{tr}\\!\\big(J_{\\epsilon}(y)\\big) \\;=\\; \\operatorname{tr}(D_{0}) + \\tfrac{\\epsilon}{2}\\,\\operatorname{tr}\\!\\big(H D_{0} + D_{0} H\\big) + \\mathcal{O}(\\epsilon^{2}) \\;=\\; \\operatorname{tr}(D_{0}) + \\epsilon\\,\\operatorname{tr}(H D_{0}) + \\mathcal{O}(\\epsilon^{2}).\n$$\nSince $D_{0}$ is the diagonal projector onto the active coordinates at $\\epsilon = 0$, $\\operatorname{tr}(H D_{0})$ equals the sum of the diagonal entries of $H$ over the active set. With the active set $\\{1, 3, 5\\}$ identified in Part 1 and the given symmetric matrix $H$, we have\n$$\nH_{11} \\;=\\; \\frac{1}{5}, \\quad H_{33} \\;=\\; \\frac{3}{10}, \\quad H_{55} \\;=\\; -\\frac{1}{5}.\n$$\nTherefore,\n$$\n\\operatorname{tr}(H D_{0}) \\;=\\; H_{11} + H_{33} + H_{55} \\;=\\; \\frac{1}{5} + \\frac{3}{10} - \\frac{1}{5} \\;=\\; \\frac{3}{10}.\n$$\nFinally,\n$$\ndf_{\\epsilon}(y) \\;=\\; \\operatorname{tr}(D_{0}) + \\epsilon\\,\\operatorname{tr}(H D_{0}) + \\mathcal{O}(\\epsilon^{2}) \\;=\\; 3 + \\frac{3}{10}\\,\\epsilon + \\mathcal{O}(\\epsilon^{2}).\n$$\nRetaining terms to first order in $\\epsilon$ yields the desired expression\n$$\ndf_{\\epsilon}(y) \\;=\\; 3 + \\frac{3}{10}\\,\\epsilon.\n$$",
            "answer": "$$\\boxed{3+\\frac{3}{10}\\,\\epsilon}$$"
        }
    ]
}