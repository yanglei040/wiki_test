{
    "hands_on_practices": [
        {
            "introduction": "这项练习将引导你亲手计算一个具体的 LASSO 问题。通过找出解路径上的“节点”并确定各区间的自由度 ，你将直观地理解 $\\ell_1$ 正则化如何进行变量选择，并看到模型复杂度是如何随正则化参数 $\\lambda$ 变化的。",
            "id": "3443323",
            "problem": "考虑最小绝对收缩和选择算子 (LASSO) 问题\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n其中有 $n=3$ 个观测值和 $p=2$ 个预测变量，设计矩阵 $X \\in \\mathbb{R}^{3 \\times 2}$ 的列 $x_{1}, x_{2}$ 由下式给出\n$$\nx_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \n\\qquad\nx_{2} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\\\ 0 \\end{pmatrix},\n$$\n响应向量为\n$$\ny = \\begin{pmatrix} 3 \\\\ -\\frac{1}{\\sqrt{3}} \\\\ 0 \\end{pmatrix}.\n$$\n矩阵 $X$ 的所有列都具有单位欧几里得范数，且 $x_{1}^{\\top} x_{2} = \\frac{1}{2}$。令正则化参数 $\\lambda \\ge 0$ 从 $+\\infty$ 变化到 $0$，并考虑完整的 LASSO 解路径。仅使用凸优化中的第一性原理（特别是 Karush–Kuhn–Tucker (KKT) 条件）以及自由度作为拟合值映射的散度（雅可比矩阵的迹）的定义，执行以下操作：\n\n1. 确定第一个节点 $\\lambda^{(1)}$（在此节点上活动集变为非空），以及沿 LASSO 路径活动集下一次发生变化的第二个节点 $\\lambda^{(2)}$。\n\n2. 在活动集及其符号模式保持固定的每个 $\\lambda$ 段上，计算 LASSO 拟合的自由度 (DoF)，即拟合值相对于 $y$ 的导数的迹。确定以下分段的这些值\n$$\nS_{0}: \\ \\lambda \\ge \\lambda^{(1)}, \n\\qquad\nS_{1}: \\ \\lambda^{(2)}  \\lambda  \\lambda^{(1)}, \n\\qquad\nS_{2}: \\ 0 \\le \\lambda \\le \\lambda^{(2)}.\n$$\n\n将您的最终答案以单行矩阵的形式报告\n$$\n\\big(\\lambda^{(1)}, \\ \\lambda^{(2)}, \\ d_{0}, \\ d_{1}, \\ d_{2}\\big),\n$$\n其中 $d_{i}$ 表示分段 $S_{i}$ 上的自由度，$i \\in \\{0,1,2\\}$。请提供精确值，无需四舍五入。",
            "solution": "用户希望找到一个特定 LASSO 问题的节点和自由度。\n\nLASSO 优化问题由下式给出\n$$L(\\beta) = \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$$\n其中 $\\beta \\in \\mathbb{R}^{p}$。最优性的 Karush–Kuhn–Tucker (KKT) 条件表明，在解 $\\hat{\\beta}$ 处，$L(\\beta)$ 的次梯度必须包含零向量。最小二乘项的梯度是 $\\nabla_{\\beta} \\left(\\frac{1}{2}\\|y - X \\beta\\|_{2}^{2}\\right) = -X^T(y - X\\beta)$。$\\ell_1$-范数惩罚项的次梯度是 $\\lambda \\partial \\|\\beta\\|_{1}$，其中 $\\partial \\|\\beta\\|_{1}$ 的第 $j$ 个分量是一个向量 $s$，满足：如果 $\\beta_j \\neq 0$，则 $s_j = \\text{sign}(\\beta_j)$；如果 $\\beta_j = 0$，则 $s_j \\in [-1, 1]$。\n\n因此，KKT 条件为：\n$$-X^T(y - X\\hat{\\beta}) + \\lambda s = 0, \\quad s \\in \\partial\\|\\hat{\\beta}\\|_1$$\n这可以为每个分量 $j \\in \\{1, \\dots, p\\}$ 写成：\n$$x_j^T(y - X\\hat{\\beta}) = \\lambda s_j$$\n其中，如果 $\\hat{\\beta}_j \\neq 0$，则 $s_j = \\text{sign}(\\hat{\\beta}_j)$；如果 $\\hat{\\beta}_j=0$，则 $|s_j| \\le 1$。\n这等价于：\n1. 如果 $\\hat{\\beta}_j \\neq 0$，则 $x_j^T(y - X\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_j)$。\n2. 如果 $\\hat{\\beta}_j = 0$，则 $|x_j^T(y - X\\hat{\\beta})| \\le \\lambda$。\n\n我们用给定的数据计算必要的矩阵乘积：\n$X = \\begin{pmatrix} 1  1/2 \\\\ 0  \\sqrt{3}/2 \\\\ 0  0 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ -1/\\sqrt{3} \\\\ 0 \\end{pmatrix}$。\n相关向量 $X^T y$ 为：\n$$X^T y = \\begin{pmatrix} 1  0  0 \\\\ 1/2  \\sqrt{3}/2  0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1/\\sqrt{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ \\frac{3}{2} - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$$\n所以，$x_1^T y = 3$ 且 $x_2^T y = 1$。格拉姆矩阵 $X^T X$ 是：\n$$X^T X = \\begin{pmatrix} x_1^T x_1  x_1^T x_2 \\\\ x_2^T x_1  x_2^T x_2 \\end{pmatrix} = \\begin{pmatrix} 1  1/2 \\\\ 1/2  1 \\end{pmatrix}$$\n\n**第1部分：确定节点 $\\lambda^{(1)}$ 和 $\\lambda^{(2)}$**\n\n**寻找第一个节点 $\\lambda^{(1)}$：**\n对于非常大的 $\\lambda$，惩罚项占主导地位，迫使解为 $\\hat{\\beta}=0$。对于 $\\hat{\\beta}=0$ 的 KKT 条件简化为对所有 $j \\in \\{1, 2\\}$ 都有 $|x_j^T y| \\le \\lambda$。\n一个系数只有在 $\\lambda$ 减小到违反此条件的值时才能变为非零。第一个系数在 $\\lambda = \\max_j |x_j^T y|$ 处变为非零。\n$$\\lambda^{(1)} = \\max(|x_1^T y|, |x_2^T y|) = \\max(3, 1) = 3$$\n第一个进入活动集的变量是对应于最大相关性的变量，即预测变量 $1$。所以，对于 $\\lambda  \\lambda^{(1)}$，我们有 $\\hat{\\beta}_1 \\neq 0$。\n\n**寻找第二个节点 $\\lambda^{(2)}$：**\n对于分段 $\\lambda  \\lambda^{(1)}$，活动集是 $\\mathcal{A}=\\{1\\}$，意味着 $\\hat{\\beta}_1 \\neq 0$ 且 $\\hat{\\beta}_2=0$。KKT 条件为：\n1. $x_1^T(y - x_1\\hat{\\beta}_1) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_1)$\n2. $|x_2^T(y - x_1\\hat{\\beta}_1)| \\le \\lambda$\n\n根据第一个条件：$x_1^T y - (x_1^T x_1)\\hat{\\beta}_1 = \\lambda s_1$。由于 $\\lambda$ 从 $\\lambda^{(1)}=3$ 开始减小，$\\hat{\\beta}_1$ 将从 $0$ 开始出现。其符号将与 $x_1^T y = 3$ 的符号相同，为正。所以，$s_1 = \\text{sign}(\\hat{\\beta}_1)=1$。\n$$3 - (1)\\hat{\\beta}_1 = \\lambda \\cdot 1 \\implies \\hat{\\beta}_1 = 3 - \\lambda$$\n这是相符的，因为对于 $\\lambda  3$ 有 $\\hat{\\beta}_1  0$。\n\n第二个节点 $\\lambda^{(2)}$ 发生在第二个 KKT 条件达到其边界时，即当预测变量 $2$ 进入模型时。我们必须在 $|x_2^T(y - x_1\\hat{\\beta}_1)| = \\lambda$ 中解出 $\\lambda$。\n令 $C_2(\\lambda) = x_2^T(y - x_1\\hat{\\beta}_1) = x_2^T y - (x_2^T x_1)\\hat{\\beta}_1$。\n使用我们预先计算的值，$x_2^T y = 1$ 和 $x_2^T x_1 = 1/2$：\n$$C_2(\\lambda) = 1 - \\frac{1}{2}(3-\\lambda) = 1 - \\frac{3}{2} + \\frac{\\lambda}{2} = \\frac{\\lambda}{2} - \\frac{1}{2}$$\n条件变为 $|\\frac{\\lambda}{2} - \\frac{1}{2}| = \\lambda$。这给出了两种可能性：\na) $\\frac{\\lambda}{2} - \\frac{1}{2} = \\lambda \\implies -\\frac{\\lambda}{2} = \\frac{1}{2} \\implies \\lambda = -1$，这是无效的，因为 $\\lambda \\ge 0$。\nb) $-(\\frac{\\lambda}{2} - \\frac{1}{2}) = \\lambda \\implies \\frac{1}{2} - \\frac{\\lambda}{2} = \\lambda \\implies \\frac{1}{2} = \\frac{3\\lambda}{2} \\implies \\lambda = \\frac{1}{3}$。\n这个值在区间 $(0, \\lambda^{(1)})$ 内。因此，第二个节点是 $\\lambda^{(2)} = \\frac{1}{3}$。\n\n**解路径分段总结：**\n- 对于 $\\lambda \\ge \\lambda^{(1)} = 3$ ($S_0$)：$\\hat{\\beta}=(0, 0)^T$。活动集 $\\mathcal{A}_0 = \\emptyset$。\n- 对于 $\\lambda^{(2)}  \\lambda  \\lambda^{(1)}$，即 $1/3  \\lambda  3$ ($S_1$)：$\\hat{\\beta}=(3-\\lambda, 0)^T$。活动集 $\\mathcal{A}_1 = \\{1\\}$。\n- 对于 $0 \\le \\lambda \\le \\lambda^{(2)} = 1/3$ ($S_2$)：两个预测变量都在模型中，所以 $\\mathcal{A}_2=\\{1, 2\\}$。KKT 条件构成一个线性系统：$X^T(y - X\\hat{\\beta}) = \\lambda s$。\n$$\\hat{\\beta} = (X^T X)^{-1} (X^T y - \\lambda s)$$\n需要确定符号向量 $s=(s_1, s_2)^T$。对于 $\\beta_1$，$s_1=1$ 如前所述。对于 $\\beta_2$，其进入时的符号是相关性 $C_2(\\lambda)$ 在节点 $\\lambda^{(2)}=1/3$ 处的符号。\n$C_2(1/3) = \\frac{1/3}{2} - \\frac{1}{2} = \\frac{1}{6} - \\frac{1}{2} = -\\frac{2}{6} = -\\frac{1}{3}$。符号是负的，所以 $s_2=-1$。\n所以，$s=(1, -1)^T$。\n我们来计算 $(X^T X)^{-1}$：\n$$(X^T X)^{-1} = \\begin{pmatrix} 1  1/2 \\\\ 1/2  1 \\end{pmatrix}^{-1} = \\frac{1}{1 - 1/4} \\begin{pmatrix} 1  -1/2 \\\\ -1/2  1 \\end{pmatrix} = \\frac{4}{3} \\begin{pmatrix} 1  -1/2 \\\\ -1/2  1 \\end{pmatrix} = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix}$$\n现在我们求这个分段的 $\\hat{\\beta}$：\n$$\\hat{\\beta} = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix} \\left( \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\right) = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix} \\begin{pmatrix} 3-\\lambda \\\\ 1+\\lambda \\end{pmatrix}$$\n$$\\hat{\\beta}_1 = \\frac{4}{3}(3-\\lambda) - \\frac{2}{3}(1+\\lambda) = \\frac{12-4\\lambda-2-2\\lambda}{3} = \\frac{10-6\\lambda}{3} = \\frac{10}{3} - 2\\lambda$$\n$$\\hat{\\beta}_2 = -\\frac{2}{3}(3-\\lambda) + \\frac{4}{3}(1+\\lambda) = \\frac{-6+2\\lambda+4+4\\lambda}{3} = \\frac{-2+6\\lambda}{3} = 2\\lambda - \\frac{2}{3}$$\n对于 $\\lambda \\in [0, 1/3]$，$\\hat{\\beta}_1  0$ 且 $\\hat{\\beta}_2 \\le 0$，这与我们假设的符号 $s=(1, -1)^T$ 一致。\n\n**第2部分：自由度 (DoF) 的计算**\n\n自由度 (DoF) 定义为 $d = \\text{Tr}\\left(\\frac{\\partial \\hat{y}}{\\partial y}\\right)$，其中 $\\hat{y} = X\\hat{\\beta}_\\lambda(y)$ 是拟合值。在节点之间的任何分段上，活动集 $\\mathcal{A}$ 和符号 $s_\\mathcal{A}$ 是固定的。令 $X_\\mathcal{A}$ 为 $X$ 中列在 $\\mathcal{A}$ 内的子矩阵。解由下式给出：\n$$\\hat{\\beta}_\\mathcal{A} = (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} (X_\\mathcal{A}^T y - \\lambda s_\\mathcal{A})$$\n拟合值为 $\\hat{y} = X_\\mathcal{A}\\hat{\\beta}_\\mathcal{A} = X_\\mathcal{A} (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} (X_\\mathcal{A}^T y - \\lambda s_\\mathcal{A})$。\n对向量 $y$ 求导：\n$$\\frac{\\partial \\hat{y}}{\\partial y} = X_\\mathcal{A} (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} X_\\mathcal{A}^T$$\n这是到 $X_\\mathcal{A}$ 的列空间上的投影矩阵。DoF 是它的迹：\n$$d = \\text{Tr}\\left(X_\\mathcal{A} (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} X_\\mathcal{A}^T\\right) = \\text{Tr}\\left((X_\\mathcal{A}^T X_\\mathcal{A})^{-1} X_\\mathcal{A}^T X_\\mathcal{A}\\right) = \\text{Tr}(I_{|\\mathcal{A}|}) = |\\mathcal{A}|$$\n在本题中，由于 $X$ 的列是线性无关的，所以子矩阵 $X_\\mathcal{A}$ 总是有满列秩，因此我们可以简单地使用活动集的大小。\n\n-   **分段 $S_0: \\lambda \\ge 3$**：活动集为空，$\\mathcal{A}_0 = \\emptyset$。所以 $|\\mathcal{A}_0|=0$。\n    $$d_0 = 0$$\n    严格来说，$\\hat{\\beta}=0$，所以 $\\hat{y}=0$，而 $\\frac{\\partial \\hat{y}}{\\partial y}$ 是零矩阵，其迹为 $0$。\n\n-   **分段 $S_1: 1/3  \\lambda  3$**：活动集为 $\\mathcal{A}_1 = \\{1\\}$。所以 $|\\mathcal{A}_1|=1$。\n    $$d_1 = 1$$\n    严格来说，$\\frac{\\partial \\hat{y}}{\\partial y} = x_1 (x_1^T x_1)^{-1} x_1^T = x_1 (1)^{-1} x_1^T = x_1 x_1^T$。\n    $d_1 = \\text{Tr}(x_1 x_1^T) = \\text{Tr}(x_1^T x_1) = \\text{Tr}(1) = 1$。\n\n-   **分段 $S_2: 0 \\le \\lambda \\le 1/3$**：活动集为 $\\mathcal{A}_2=\\{1, 2\\}$。所以 $|\\mathcal{A}_2|=2$。\n    $$d_2 = 2$$\n    严格来说，$\\frac{\\partial \\hat{y}}{\\partial y} = X (X^T X)^{-1} X^T$。\n    $d_2 = \\text{Tr}(X (X^T X)^{-1} X^T) = \\text{Tr}((X^T X)^{-1} X^T X) = \\text{Tr}(I_2) = 2$。\n\n最终结果是 $\\lambda^{(1)}=3$, $\\lambda^{(2)}=1/3$, $d_0=0$, $d_1=1$, 和 $d_2=2$。这些结果将以单行矩阵的形式报告。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3  \\frac{1}{3}  0  1  2\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "一个常见的直觉是，LASSO 拟合的自由度等于其非零系数的数量。本练习  通过一个精心设计的、具有完全共线性的例子来挑战这一看法。解决这个问题将揭示一个更深刻的原理：自由度实际上由活动设计子矩阵的秩（rank）决定，这在处理相关预测变量时至关重要。",
            "id": "3443335",
            "problem": "考虑最小绝对收缩和选择算子 (LASSO) 估计量，即在线性模型 $y = \\mu + \\varepsilon$ 中的 $\\ell_1$-正则化最小二乘估计量，其中 $\\varepsilon \\sim \\mathcal{N}(0, I_{n})$ 且 $I_{n}$ 是 $n \\times n$ 的单位矩阵。设 $A \\in \\mathbb{R}^{3 \\times 2}$ 是一个设计矩阵，其两列相同且已归一化，具体为 $a_{1} = a_{2} = a$，其中 $a = \\frac{1}{\\sqrt{3}}(1,1,1)^{\\top}$，因此 $\\|a\\|_{2} = 1$。LASSO 估计量定义为\n$$\n\\hat{x}(y) \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\},\n$$\n拟合值为 $\\hat{y}(y) = A \\hat{x}(y)$。假设观测数据向量为 $y = 2 a$，正则化参数为 $\\lambda = \\frac{1}{2}$。\n\n仅使用基本定义和最优性条件，完成以下任务：\n- 证明存在一个最优解 $\\hat{x}(y)$，其支撑集 $S = \\{ j : \\hat{x}_{j}(y) \\neq 0 \\}$ 满足 $|S| = 2$，即使两个活跃列是相同的。\n- 使用通过 Stein 无偏风险框架定义的自由度，即拟合值映射 $y \\mapsto \\hat{y}(y)$ 在观测值 $y$ 处的散度，计算此设置下 $\\operatorname{df}(\\hat{y})$ 的值。\n\n将最终答案表示为单个实数。无需四舍五入。",
            "solution": "需要最小化的 LASSO 目标函数是\n$$\nL(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\n其中 $x = (x_1, x_2)^\\top$。给定 $A = [a, a]$，项 $Ax$ 可以写为 $Ax = x_1 a + x_2 a = (x_1+x_2)a$。\n令 $s = x_1+x_2$。目标函数变为\n$$\nL(x) = \\frac{1}{2}\\|y - s a\\|_{2}^{2} + \\lambda (|x_1| + |x_2|)\n$$\n我们需要对 $x_1$ 和 $x_2$ 最小化此函数。这可以分两步完成。首先，对于一个固定的和 $s = x_1+x_2$，我们最小化惩罚项 $\\lambda(|x_1| + |x_2|)$。根据三角不等式，有 $|x_1|+|x_2| \\ge |x_1+x_2| = |s|$，等号当且仅当 $x_1$ 和 $x_2$ 同号（或其中一个或两个为零）时成立。因此，对于一个固定的 $s$，惩罚项的最小值为 $\\lambda|s|$。\n\n问题于是简化为最小化以下关于单变量 $s$ 的函数：\n$$\ng(s) = \\frac{1}{2}\\|y - s a\\|_{2}^{2} + \\lambda|s|\n$$\n展开平方范数，我们得到：\n$$\ng(s) = \\frac{1}{2}(y - sa)^\\top(y - sa) + \\lambda|s| = \\frac{1}{2}(y^\\top y - 2s a^\\top y + s^2 a^\\top a) + \\lambda|s|\n$$\n因为 $\\|a\\|_2 = 1$，我们有 $a^\\top a = 1$。需要最小化的函数是\n$$\ng(s) = \\frac{1}{2}s^2 - (a^\\top y)s + \\frac{1}{2}y^\\top y + \\lambda|s|\n$$\n这是一个一维 LASSO 问题。最小化 $g(s)$ 的解 $\\hat{s}$ 由应用于最小二乘估计 $a^\\top y$ 的软阈值算子给出：\n$$\n\\hat{s}(y) = S_{\\lambda}(a^\\top y) = \\operatorname{sign}(a^\\top y) \\max(|a^\\top y| - \\lambda, 0)\n$$\n对于给定的数据 $y=2a$ 和 $\\lambda = 1/2$，我们首先计算 $a^\\top y$：\n$$\na^\\top y = a^\\top (2a) = 2 a^\\top a = 2(1) = 2\n$$\n将此代入软阈值公式：\n$$\n\\hat{s} = S_{1/2}(2) = \\operatorname{sign}(2) \\max(|2| - 1/2, 0) = 1 \\cdot \\max(3/2, 0) = \\frac{3}{2}\n$$\n原始问题的最优解集合 $\\hat{x} = (\\hat{x}_1, \\hat{x}_2)^\\top$ 必须满足两个条件：\n1.  $\\hat{x}_1 + \\hat{x}_2 = \\hat{s} = \\frac{3}{2}$\n2.  $|\\hat{x}_1| + |\\hat{x}_2| = |\\hat{s}| = \\frac{3}{2}$\n\n第二个条件意味着 $\\hat{x}_1$ 和 $\\hat{x}_2$ 必须为非负，因为它们的和是正数。因此，最优解的集合是 $\\mathbb{R}^2$ 中的线段，由 $\\{(\\hat{x}_1, \\hat{x}_2) \\mid \\hat{x}_1 + \\hat{x}_2 = 3/2, \\hat{x}_1 \\ge 0, \\hat{x}_2 \\ge 0\\}$ 给出。\n\n**第1部分：存在支撑集大小为2的解**\n问题要求证明存在一个最优解 $\\hat{x}(y)$，其支撑集大小为 $|S|=2$。我们可以选择解线段内部的任意一点。例如，令 $\\hat{x}_1 = \\hat{x}_2 = 3/4$。这个选择满足条件：$\\hat{x}_1 + \\hat{x}_2 = 3/4 + 3/4 = 3/2$，且 $\\hat{x}_1, \\hat{x}_2  0$。该解的支撑集是 $S=\\{1, 2\\}$，其大小为 $|S|=2$。这证明了问题的第一部分。\n\n**第2部分：自由度的计算**\n拟合值向量 $\\hat{y}$ 是唯一的，无论从解集合中选择哪个 $\\hat{x}$：\n$$\n\\hat{y}(y) = A\\hat{x}(y) = (\\hat{x}_1(y) + \\hat{x}_2(y))a = \\hat{s}(y) a\n$$\n代入 $\\hat{s}(y)$ 的表达式，我们得到映射 $y \\mapsto \\hat{y}(y)$：\n$$\n\\hat{y}(y) = S_{\\lambda}(a^\\top y) a\n$$\n自由度定义为该向量场的散度：\n$$\n\\operatorname{df}(\\hat{y}) = \\nabla \\cdot \\hat{y}(y) = \\sum_{i=1}^3 \\frac{\\partial \\hat{y}_i}{\\partial y_i}\n$$\n$\\hat{y}$ 的第 $i$ 个分量是 $\\hat{y}_i(y) = S_{\\lambda}(a^\\top y) a_i$。我们计算关于 $y_j$ 的偏导数：\n$$\n\\frac{\\partial \\hat{y}_i}{\\partial y_j} = \\frac{\\partial}{\\partial y_j} [S_{\\lambda}(a^\\top y) a_i] = a_i \\frac{\\partial}{\\partial y_j} [S_{\\lambda}(a^\\top y)]\n$$\n使用链式法则，令 $u(y) = a^\\top y = \\sum_{k=1}^3 a_k y_k$。则 $\\frac{\\partial u}{\\partial y_j} = a_j$。\n$$\n\\frac{\\partial \\hat{y}_i}{\\partial y_j} = a_i \\frac{d S_{\\lambda}(u)}{du} \\frac{\\partial u}{\\partial y_j} = a_i a_j \\frac{d S_{\\lambda}(u)}{du}\n$$\n这是映射 $\\hat{y}$ 的雅可比矩阵的 $(i,j)$ 元。散度是该雅可比矩阵的迹：\n$$\n\\operatorname{df}(\\hat{y}) = \\sum_{i=1}^3 \\frac{\\partial \\hat{y}_i}{\\partial y_i} = \\sum_{i=1}^3 a_i^2 \\frac{d S_{\\lambda}(u)}{du} = \\left(\\sum_{i=1}^3 a_i^2\\right) \\frac{d S_{\\lambda}(u)}{du}\n$$\n由于 $\\sum_{i=1}^3 a_i^2 = \\|a\\|_2^2 = 1$，这可以简化为：\n$$\n\\operatorname{df}(\\hat{y}) = \\frac{d S_{\\lambda}(u)}{du}\n$$\n软阈值函数 $S_{\\lambda}(u)$ 的导数在 $|u|  \\lambda$ 时为 $1$，在 $|u|  \\lambda$ 时为 $0$。在 $|u|=\\lambda$ 处无定义。因此，对于几乎所有的 $y$，导数是指示函数 $\\mathbb{I}(|u|  \\lambda)$。\n$$\n\\operatorname{df}(\\hat{y}) = \\mathbb{I}(|a^\\top y|  \\lambda)\n$$\n现在我们针对问题中给定的特定值进行计算：$y=2a$ 和 $\\lambda=1/2$。\n我们有 $a^\\top y = 2$。我们需要检查是否 $|2|  1/2$。此条件为真。\n因此，自由度的值为：\n$$\n\\operatorname{df}(\\hat{y}) = \\mathbb{I}(2  1/2) = 1\n$$\n这个结果值得注意。虽然存在一个支撑集大小为 $2$ 的解 $\\hat{x}$，但拟合 $\\hat{y}$ 的自由度是 $1$。这说明，在像完全共线性这样的退化情况下，常用的启发式规则 $\\operatorname{df} \\approx |S|$ 可能会产生误导。自由度对应于估计量所选择的子空间的维度，在本例中是由向量 $a$ 张成的一维子空间。",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "理论上自由度等于活动子矩阵的秩，但在实际的数值计算中，我们该如何处理近乎线性的相关性呢？本练习  将理论与实践联系起来，要求你使用奇异值分解（SVD）等数值稳定的方法来计算“数值秩”。通过此练习，你将理解数值容差的选择如何影响自由度的估计，这是任何严肃的科学计算都必须面对的问题。",
            "id": "3443334",
            "problem": "给定一个设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和一个与 $\\ell_1$ 正则化最小二乘估计器（Lasso）相关的活性指标集 $A \\subset \\{0,1,\\dots,p-1\\}$。考虑拟合值映射 $f: \\mathbb{R}^n \\to \\mathbb{R}^n$，其定义为 $f(y) = X \\widehat{\\beta}(y)$，其中 $y \\in \\mathbb{R}^n$ 是数据向量，$\\widehat{\\beta}(y)$ 是下式的任意解：\n$$\n\\widehat{\\beta}(y) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\tfrac{1}{2}\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1 \\right\\},\n$$\n其中正则化水平 $\\lambda  0$ 是固定的。$f$ 在点 $y$ 处的自由度可以通过源于 Stein 无偏风险估计 (SURE) 的基于散度的概念来定义：当 $f$ 在 $y$ 处（弱）可微时，自由度由散度给出，\n$$\n\\mathrm{df}(y) \\equiv \\sum_{i=1}^n \\frac{\\partial f_i(y)}{\\partial y_i}.\n$$\n在数据空间的任何区域，如果活性集 $A$ 和活性系数的符号保持不变，并且定义 $\\widehat{\\beta}(y)$ 的代数约束是正则的，那么映射 $f$ 在该区域上是光滑的。你的任务是：\n\n1) 仅从自由度是拟合值映射的散度的定义以及 $\\ell_1$ 正则化最小二乘的基本最优性条件出发，推导为什么在活性集 $A$ 和活性系数符号固定且代数正则性成立的任何区域上，自由度等于活性设计子矩阵 $X_A \\in \\mathbb{R}^{n \\times |A|}$ 的代数秩。明确指出需要对 $X_A$ 施加的条件以及 Karush–Kuhn–Tucker (KKT) 系统的结构，以将 $f$ 的雅可比矩阵与到 $X_A$ 列空间上的投影联系起来。\n\n2) 提出并实现两种计算代数秩 $\\operatorname{rank}(X_A)$ 的数值稳定方法：\n   - 一种基于奇异值分解 (SVD) 的方法。给定一个相对阈值 $\\tau \\in (0,1)$，将数值秩定义为 $X_A$ 的奇异值中严格大于 $X_A$ 最大奇异值 $\\tau$ 倍的奇异值的数量。\n   - 一种基于列主元 QR 分解的方法。给定相同的相对阈值 $\\tau$，将数值秩定义为 R 因子中对角线元素的数量，这些元素的绝对值严格大于首个对角线元素绝对值的 $\\tau$ 倍。\n\n3) 讨论并说明数值阈值 $\\tau$ 的选择在实践中如何影响自由度估计 $\\widehat{\\mathrm{df}} = \\operatorname{rank}(X_A)$，尤其是在存在精确或近似多重共线性的情况下。然后，实现一个程序，针对下面的测试套件，为每种情况和每个阈值计算基于 SVD 和基于主元 QR 的数值秩，并以指定格式输出所有结果。\n\n测试套件规范。对于每个测试用例，给定 $X$ 和 $A$。你必须通过提取 $X$ 中由 $A$ 索引的列（使用从零开始的索引）来构成 $X_A$。对所有情况使用阈值集合 $\\mathcal{T} = \\{10^{-12}, 10^{-6}, 10^{-3}, 10^{-1}\\}$。\n\n- 情况 1（良态满列秩）：$n=5$, $p=3$,\n  $$\n  X = \\begin{bmatrix}\n  1  0  0 \\\\\n  0  1  0 \\\\\n  0  0  1 \\\\\n  0  0  0 \\\\\n  0  0  0\n  \\end{bmatrix}, \\quad A = \\{0,1,2\\}.\n  $$\n- 情况 2（近似相关的活性列）：$n=5$, $p=4$，小参数 $\\varepsilon = 10^{-7}$,\n  $$\n  X = \\begin{bmatrix}\n  1  0  0  1 \\\\\n  0  1  0  0 \\\\\n  0  0  1  \\varepsilon \\\\\n  0  0  0  0 \\\\\n  0  0  0  0\n  \\end{bmatrix}, \\quad A = \\{0,1,3\\}.\n  $$\n- 情况 3（活性集中存在精确重复列）：$n=4$, $p=3$,\n  $$\n  X = \\begin{bmatrix}\n  1  1  0 \\\\\n  0  0  1 \\\\\n  0  0  0 \\\\\n  0  0  0\n  \\end{bmatrix}, \\quad A = \\{0,1,2\\}.\n  $$\n- 情况 4（宽活性集，其固有秩受 $n$ 限制）：$n=3$, $p=5$,\n  $$\n  X = \\begin{bmatrix}\n  1  0  0  1  0 \\\\\n  0  1  0  1  0 \\\\\n  0  0  1  0  0\n  \\end{bmatrix}, \\quad A = \\{0,1,2,3\\}.\n  $$\n\n最终输出格式。对于顺序为 1, 2, 3, 4 的每个情况，以及对于按升序排列的每个阈值 $\\tau \\in \\{10^{-12}, 10^{-6}, 10^{-3}, 10^{-1}\\}$，计算：\n- 基于 SVD 的 $X_A$ 的数值秩。\n- 基于主元 QR 的 $X_A$ 的数值秩。\n\n将所有这些整数按\n$$\n\\big[\\; r_{\\mathrm{svd}}(1,10^{-12}),\\; r_{\\mathrm{qr}}(1,10^{-12}),\\; r_{\\mathrm{svd}}(1,10^{-6}),\\; r_{\\mathrm{qr}}(1,10^{-6}),\\; \\dots,\\; r_{\\mathrm{svd}}(4,10^{-1}),\\; r_{\\mathrm{qr}}(4,10^{-1}) \\;\\big]\n$$\n的确切顺序汇总到一个扁平列表中。你的程序应该生成单行输出，其中包含此列表，形式为用方括号括起来的逗号分隔序列，例如 $[3,3,3,3,\\dots]$。所有答案都是无单位的整数。不允许用户输入或使用外部文件；所有数据都如上文测试套件中所指定，固定在你的程序中。",
            "solution": "该问题要求在特定的正则性条件下推导 Lasso 估计器的自由度，实现两种计算矩阵秩的数值方法，并讨论这些方法及将其应用于给定的测试套件。\n\n### 第 1 部分：自由度的推导\n\n拟合值映射 $f(y)$ 的自由度定义为其散度，$\\mathrm{df}(y) = \\nabla \\cdot f(y) = \\mathrm{tr}(J_f(y))$，其中 $J_f(y)$ 是 $f$ 关于 $y$ 的雅可比矩阵。该映射由 $f(y) = X\\widehat{\\beta}(y)$ 给出，其中 $\\widehat{\\beta}(y)$ 是 Lasso 优化问题的一个解：\n$$\n\\widehat{\\beta}(y) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\tfrac{1}{2}\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1 \\right\\}\n$$\n对于一个固定的正则化参数 $\\lambda  0$。\n\n此问题的 Karush-Kuhn-Tucker (KKT) 最优性条件是充要条件。目标函数关于 $\\beta$ 的次梯度必须包含零向量：\n$$\n-X^T(y - X\\widehat{\\beta}) + \\lambda z = 0\n$$\n其中 $z$ 是 $\\ell_1$ 范数 $\\partial \\lVert \\widehat{\\beta} \\rVert_1$ 的次微分中的一个向量。$z$ 的分量由下式给出：\n$$\nz_j = \\begin{cases} \\mathrm{sign}(\\widehat{\\beta}_j)  \\text{若 } \\widehat{\\beta}_j \\neq 0 \\\\ v_j, \\text{ 其中 } |v_j| \\le 1  \\text{若 } \\widehat{\\beta}_j = 0 \\end{cases}\n$$\n设活性集 $A = \\{j \\in \\{0, \\dots, p-1\\} \\mid \\widehat{\\beta}_j \\neq 0\\}$ 是对应于非零系数的索引集。设 $A^c$ 是其补集，即非活性集。设 $X_A$ 是 $X$ 中包含由 $A$ 索引的列的子矩阵，$\\widehat{\\beta}_A$ 是非零系数的向量。根据定义，$\\widehat{\\beta}_{A^c} = 0$。KKT 条件可以根据活性集进行划分：\n1.  对于活性集 $A$：$X_A^T(y - X_A\\widehat{\\beta}_A) = \\lambda s_A$，其中 $s_A$ 是 $\\widehat{\\beta}_A$ 中系数的符号向量。\n2.  对于非活性集 $A^c$：$|X_{A^c}^T(y - X_A\\widehat{\\beta}_A)| \\le \\lambda$ (逐元素)。\n\n问题陈述我们在数据空间的一个区域上，其中活性集 $A$ 和符号 $s_A$ 是固定的。第二个 KKT 条件（不等式）被假定为严格成立，意味着没有非活性预测变量即将进入模型。术语“代数正则性”意味着第一个 KKT 条件（对于 $\\widehat{\\beta}_A$ 是一个线性方程组）将 $\\widehat{\\beta}_A$ 定义为 $y$ 的一个光滑函数。有了这些假设，我们可以分析第一个条件：\n$$\nX_A^T X_A \\widehat{\\beta}_A = X_A^T y - \\lambda s_A\n$$\n拟合值映射为 $f(y) = X\\widehat{\\beta}(y) = X_A\\widehat{\\beta}_A(y)$。为了求其雅可比矩阵，我们对这个表达式关于 $y^T$ 求导。$f$ 的雅可比矩阵由 $J_f(y) = \\frac{\\partial f(y)}{\\partial y^T} = X_A \\frac{\\partial \\widehat{\\beta}_A(y)}{\\partial y^T}$ 给出。\n\n为了求 $\\widehat{\\beta}_A$ 的雅可比矩阵，我们对活性集的 KKT 方程关于 $y^T$ 求导。项 $\\lambda s_A$ 在该区域是常数，所以其导数为零。\n$$\n\\frac{\\partial}{\\partial y^T} \\left( X_A^T X_A \\widehat{\\beta}_A \\right) = \\frac{\\partial}{\\partial y^T} \\left( X_A^T y - \\lambda s_A \\right)\n$$\n应用链式法则，我们得到：\n$$\n(X_A^T X_A) \\frac{\\partial \\widehat{\\beta}_A}{\\partial y^T} = X_A^T \\frac{\\partial y}{\\partial y^T} = X_A^T I_n = X_A^T\n$$\n这是一个关于雅可比矩阵 $\\frac{\\partial \\widehat{\\beta}_A}{\\partial y^T}$ 的线性方程组。形式为 $B Z = C$ 的方程的解由 $Z = B^+ C$ 给出，其中 $B^+$表示 $B$ 的 Moore-Penrose 伪逆。因此，我们可以写出：\n$$\n\\frac{\\partial \\widehat{\\beta}_A}{\\partial y^T} = (X_A^T X_A)^+ X_A^T\n$$\n这里，使用伪逆至关重要，因为它即使在矩阵 $X_A^T X_A$ 是奇异的情况下也提供了解，这种情况发生在 $X_A$ 的列是线性相关时。\n\n现在，我们将其代入拟合值映射的雅可比矩阵的表达式中：\n$$\nJ_f(y) = X_A \\frac{\\partial \\widehat{\\beta}_A}{\\partial y^T} = X_A (X_A^T X_A)^+ X_A^T\n$$\n矩阵 $P_A = X_A (X_A^T X_A)^+ X_A^T$ 是到 $X_A$ 的列空间（表示为 $\\mathcal{C}(X_A)$）上的正交投影矩阵。这是线性代数中的一个标准结果。\n\n自由度定义为该雅可比矩阵的迹：\n$$\n\\mathrm{df}(y) = \\mathrm{tr}(J_f(y)) = \\mathrm{tr}(P_A)\n$$\n任何投影矩阵的一个基本性质是它的迹等于它所投影到的子空间的维度。在这种情况下，子空间 $\\mathcal{C}(X_A)$ 的维度是矩阵 $X_A$ 的秩。因此，\n$$\n\\mathrm{df}(y) = \\operatorname{rank}(P_A) = \\dim(\\mathcal{C}(X_A)) = \\operatorname{rank}(X_A)\n$$\n该推导表明，在活性集和符号固定（意味着估计器的局部线性）的所述条件下，Lasso 拟合的自由度就是活性设计子矩阵 $X_A$ 中线性无关列的数量。所需的关键条件是 KKT 系统可微的能力，这由“代数正则性”假设保证，以及将拟合映射的雅可比矩阵与到 $\\mathcal{C}(X_A)$ 上的投影算子等同起来。\n\n### 第 2 部分：数值秩的计算\n\n代数秩在浮点运算中通常是一个不稳定的量。近似共线性可以使一个矩阵在代数上是满秩的，但在实际的数值意义上是秩亏的。我们被要求实现两种方法来计算数值秩。\n\n1.  **基于奇异值分解 (SVD) 的秩**：\n    $X_A \\in \\mathbb{R}^{n \\times |A|}$ 的 SVD 分解为 $X_A = U \\Sigma V^T$，其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{|A| \\times |A|}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{n \\times |A|}$ 是一个矩形对角矩阵，其对角线上的非负实数称为奇异值，按非增顺序排列：$\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$，其中 $k=\\min(n, |A|)$。代数秩是非零奇异值的数量。给定一个相对阈值 $\\tau \\in (0,1)$，数值秩定义为显著大于零的奇异值的数量，相对于最大奇异值而言：\n    $$\n    \\operatorname{rank}_{\\mathrm{svd}}(X_A, \\tau) = \\left| \\left\\{ i \\mid \\sigma_i  \\tau \\cdot \\sigma_1 \\right\\} \\right|\n    $$\n    这是确定数值秩最可靠的方法。\n\n2.  **基于列主元 QR 分解的秩**：\n    $X_A$ 的列主元 QR 分解为 $X_A P = Q R$，其中 $P$ 是一个置换矩阵，$Q$ 是一个正交矩阵，$R$ 是一个上梯形矩阵，其对角线元素的绝对值是递减的：$|R_{11}| \\ge |R_{22}| \\ge \\dots \\ge |R_{kk}| \\ge 0$。置换矩阵 $P$ 是通过贪心选择来最大化每一步对角线元素的绝对值。如果 $X_A$ 是秩亏的，则 $R$ 的某些对角线元素将为零。与 SVD 方法类似，我们可以通过相对于最大对角线元素对 $R$ 的对角线元素进行阈值处理来定义数值秩：\n    $$\n    \\operatorname{rank}_{\\mathrm{qr}}(X_A, \\tau) = \\left| \\left\\{ i \\mid |R_{ii}|  \\tau \\cdot |R_{11}| \\right\\} \\right|\n    $$\n    这种方法通常比 SVD 快，但对于某些病态矩阵可能不太可靠，尽管它在大多数实际情况中表现良好。\n\n### 第 3 部分：阈值的影响与应用\n\n阈值 $\\tau$ 的选择是关键且依赖于上下文的。它定义了什么被认为是“数值上为零”。小的 $\\tau$ 紧密遵循秩的代数定义，而大的 $\\tau$ 提供了一个更稳定、“有效”的秩，它忽略了近似共线性列的影响。以下程序为给定的测试套件计算这些数值秩。\n\n```python\nimport numpy as np\nfrom scipy.linalg import qr\n\ndef solve():\n    \"\"\"\n    Computes numerical ranks for a suite of test cases using SVD and pivoted QR methods.\n    \"\"\"\n    # Define the set of relative thresholds for rank determination.\n    thresholds = [1e-12, 1e-6, 1e-3, 1e-1]\n\n    # Test cases specification: (matrix X, active set A)\n    # Case 1: Well-conditioned, full column rank\n    X1 = np.array([\n        [1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.],\n        [0., 0., 0.],\n        [0., 0., 0.]\n    ])\n    A1 = [0, 1, 2]\n\n    # Case 2: Nearly dependent active columns\n    eps = 1e-7\n    X2 = np.array([\n        [1., 0., 0., 1.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., eps],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]\n    ])\n    A2 = [0, 1, 3]\n\n    # Case 3: Exact duplicate column in the active set\n    X3 = np.array([\n        [1., 1., 0.],\n        [0., 0., 1.],\n        [0., 0., 0.],\n        [0., 0., 0.]\n    ])\n    A3 = [0, 1, 2]\n\n    # Case 4: Wide active set with rank limited by n\n    X4 = np.array([\n        [1., 0., 0., 1., 0.],\n        [0., 1., 0., 1., 0.],\n        [0., 0., 1., 0., 0.]\n    ])\n    A4 = [0, 1, 2, 3]\n    \n    test_cases = [\n        (X1, A1),\n        (X2, A2),\n        (X3, A3),\n        (X4, A4)\n    ]\n\n    # List to store the final sequence of results.\n    results = []\n\n    # Iterate over each test case.\n    for X, A in test_cases:\n        # Form the active submatrix X_A.\n        # Ensure that A is a list or tuple of indices, not a set, for correct slicing.\n        X_A = X[:, list(A)]\n\n        # Handle the case where X_A has no columns.\n        if X_A.shape[1] == 0:\n            for _ in thresholds:\n                results.extend([0, 0])\n            continue\n            \n        # Iterate over each threshold.\n        for tau in thresholds:\n            # --- SVD-based numerical rank ---\n            try:\n                s = np.linalg.svd(X_A, compute_uv=False)\n                # If s is empty, rank is 0. Otherwise, compute rank.\n                if s.size == 0:\n                    rank_svd = 0\n                else:\n                    s_max = s[0]\n                    # Check for s_max == 0 to avoid division by zero if X_A is a zero matrix.\n                    if s_max > 0:\n                        rank_svd = np.sum(s > tau * s_max)\n                    else:\n                        rank_svd = 0\n            except np.linalg.LinAlgError:\n                rank_svd = 0 # Should not happen with compute_uv=False\n\n            results.append(int(rank_svd))\n\n            # --- Pivoted QR-based numerical rank ---\n            try:\n                # Use scipy.linalg.qr with column pivoting.\n                Q, R, P = qr(X_A, pivoting=True)\n                r_diag_abs = np.abs(np.diag(R))\n                \n                # If r_diag_abs is empty, rank is 0. Otherwise compute rank.\n                if r_diag_abs.size == 0:\n                    rank_qr = 0\n                else:\n                    r_max = r_diag_abs[0]\n                    # Check for r_max == 0 to avoid division by zero.\n                    if r_max > 0:\n                        rank_qr = np.sum(r_diag_abs > tau * r_max)\n                    else:\n                        rank_qr = 0\n            except np.linalg.LinAlgError:\n                rank_qr = 0\n\n            results.append(int(rank_qr))\n    return f\"[{','.join(map(str, results))}]\"\n\n# The final answer is the output of the solve() function.\n# The code is shown here as part of the solution process.\n```",
            "answer": "[3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3]"
        }
    ]
}