{
    "hands_on_practices": [
        {
            "introduction": "该实践提供了应用信息准则的基础练习。通过一个简化的正交设计案例，您将从第一性原理出发，推导并计算赤池信息准则（AIC）和贝叶斯信息准则（BIC），从而具体理解这些准则如何在模型拟合度和复杂性之间进行权衡，以选择最佳的模型大小。这项练习将巩固您对AIC和BIC计算方法的掌握。",
            "id": "3452903",
            "problem": "考虑压缩感知中的一个线性观测模型，该模型带有高斯噪声，由 $y \\in \\mathbb{R}^{n}$ 和 $X \\in \\mathbb{R}^{n \\times p}$ 给出，其中 $X$ 的列是标准正交的，因此 $X^{\\top}X = I_{p}$。假设 $y = X \\beta^{\\star} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。定义最佳k项近似为在$X$的所有$k$列子集上最小化经验平方误差的$k$项线性模型。令 $z = X^{\\top} y \\in \\mathbb{R}^{p}$，并令 $|z|_{(1)} \\geq |z|_{(2)} \\geq \\cdots \\geq |z|_{(p)}$ 表示$z$中各元素大小的顺序统计量，其对应的带符号值为 $z_{(j)}$。\n\n从高斯对数似然和$X$的标准正交性出发，推导沿着最佳k项路径的残差平方和作为$k$的函数的显式表达式。然后，使用赤池信息准则（AIC）和贝叶斯信息准则（BIC）的定义——即从最大化高斯似然导出的信息准则，其惩罚项与自由参数数量成正比——获得它们在此路径上作为$k$的函数的表达式。你可以忽略不依赖于$k$的加性常数。\n\n现在，具体考虑以下数据：\n- $n = 120$， $p = 10$，\n- $\\|y\\|^{2} = 300$，\n- $z = X^{\\top} y = [\\,5.2,\\,-3.1,\\,2.7,\\,-2.2,\\,1.9,\\,-1.5,\\,1.2,\\,0.9,\\,-0.6,\\,0.4\\,]$。\n\n计算在由给定$z$构建的最佳k项路径上，分别最小化AIC和BIC的$k$值。若值相同，则选择较小的$k$。将你的最终答案以数对 $(k_{\\mathrm{AIC}}, k_{\\mathrm{BIC}})$ 的形式报告。最终答案必须作为一个单独的实体给出，不需要四舍五入。",
            "solution": "用户想要解决一个压缩感知背景下的模型选择问题。\n第一步是验证问题陈述。\n\n### 步骤1：提取已知条件\n- **模型**：线性观测模型 $y = X \\beta^{\\star} + \\varepsilon \\in \\mathbb{R}^{n}$。\n- **传感矩阵**：$X \\in \\mathbb{R}^{n \\times p}$，其列标准正交，即 $X^{\\top}X = I_{p}$。\n- **噪声**：$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。\n- **数据变换**：$z = X^{\\top} y \\in \\mathbb{R}^{p}$。\n- **顺序统计量**：$|z|_{(1)} \\geq |z|_{(2)} \\geq \\cdots \\geq |z|_{(p)}$ 是$z$中各元素大小的顺序统计量。对应的带符号值记为 $z_{(j)}$。\n- **最佳k项近似**：在$X$的所有$k$列子集上最小化经验平方误差的$k$项线性模型。\n- **信息准则**：从最大化高斯对数似然导出的赤池信息准则（AIC）和贝叶斯信息准则（BIC）。\n- **数据**：$n = 120$， $p = 10$， $\\|y\\|^{2} = 300$。\n- **变换后的数据**：$z = [\\,5.2,\\,-3.1,\\,2.7,\\,-2.2,\\,1.9,\\,-1.5,\\,1.2,\\,0.9,\\,-0.6,\\,0.4\\,]$。\n- **任务**：找到分别最小化AIC和BIC的整数 $(k_{\\mathrm{AIC}}, k_{\\mathrm{BIC}})$。如果出现平局，选择较小的$k$。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题在科学上是合理的、适定的、客观的并且是自洽的。\n- **科学合理性**：问题设置是高维统计和压缩感知中的一个标准场景，依赖于带有高斯噪声和标准正交设计矩阵的线性模型。AIC和BIC的定义是统计模型选择中的标准定义。\n- **适定性**：目标明确：推导AIC和BIC的表达式，然后找到最小化各自准则的$k$值。提供的数据允许得到唯一解。$X$的标准正交性极大地简化了问题，这是一个常见的理论假设。\n- **客观性**：语言精确，没有主观论断。\n- **完整性**：为解决问题提供了所有必要的数据和定义。\n该问题是有效的。\n\n### 步骤3：进行求解\n该问题是有效的。我现在将提供一个完整、有理有据的解答。\n\n模型 $y = X\\beta + \\varepsilon$ 的高斯对数似然由下式给出：\n$$ \\ell(\\beta, \\sigma^2; y) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\|y - X\\beta\\|^2 $$\n对于一个固定的模型（即$\\beta$的支撑集固定），最大化关于$\\beta$的对数似然等价于最小化残差平方和（RSS），定义为$\\mathrm{RSS}(\\beta) = \\|y - X\\beta\\|^2$。\n\n我们考虑一个具有$k$个非零系数的模型，其索引由集合 $S \\subset \\{1, \\dots, p\\}$ 给出，且$|S|=k$。该模型为 $y = X_S \\beta_S + \\varepsilon$，其中$X_S$包含由$S$索引的$X$的列。$\\beta_S$的最小二乘估计是 $\\hat{\\beta}_S = (X_S^{\\top}X_S)^{-1} X_S^{\\top}y$。\n鉴于$X$的列是标准正交的，我们有$X^{\\top}X = I_p$。这意味着对于任何列子集$S$，$X_S^{\\top}X_S = I_k$。\n因此，估计量简化为 $\\hat{\\beta}_S = X_S^{\\top}y$。$\\hat{\\beta}_S$的分量是$z=X^{\\top}y$的相应分量。也就是说，对于$j \\in S$，$\\hat{\\beta}_j = z_j$。对于$j \\notin S$，我们有$\\hat{\\beta}_j = 0$。\n\n该模型的RSS为：\n$$ \\mathrm{RSS}(S) = \\|y - X_S \\hat{\\beta}_S\\|^2 = \\|y - X_S (X_S^{\\top}y)\\|^2 $$\n向量$X_S(X_S^{\\top}y)$是$y$在由$X_S$中的列所张成的子空间上的正交投影。根据勾股定理，$\\|y\\|^2 = \\|X_S(X_S^{\\top}y)\\|^2 + \\mathrm{RSS}(S)$。\n投影的范数平方为：\n$$ \\|X_S(X_S^{\\top}y)\\|^2 = (X_S^{\\top}y)^{\\top} (X_S^{\\top}X_S) (X_S^{\\top}y) = (X_S^{\\top}y)^{\\top} I_k (X_S^{\\top}y) = \\|X_S^{\\top}y\\|^2 = \\sum_{j \\in S} z_j^2 $$\n因此，RSS为：\n$$ \\mathrm{RSS}(S) = \\|y\\|^2 - \\sum_{j \\in S} z_j^2 $$\n最佳k项近似是最小化RSS的那个。为了在$k$固定的情况下最小化$\\mathrm{RSS}(S)$，我们必须选择能够最大化$\\sum_{j \\in S} z_j^2$的集合$S$。这通过选择与$|z_j|$（或$z_j^2$）的$k$个最大值相对应的$k$个索引来实现。\n令$S_{(k)}$为这组最优的$k$个索引的集合。最佳k项模型的RSS，记为$\\mathrm{RSS}_k$，是：\n$$ \\mathrm{RSS}_k = \\|y\\|^2 - \\sum_{j=1}^{k} |z|_{(j)}^2 = \\|y\\|^2 - \\sum_{j=1}^{k} z_{(j)}^2 $$\n其中 $z_{(j)}^2$ 是第j大的平方大小值。\n如果$k=0$（零模型），我们有$\\hat{\\beta}=0$，所以$\\mathrm{RSS}_0 = \\|y\\|^2$。\n\n为了找到AIC和BIC的表达式，我们首先需要最大化的对数似然。对于一个大小为$k$的模型，方差的最大似然估计是 $\\hat{\\sigma}_k^2 = \\frac{\\mathrm{RSS}_k}{n}$。将$\\hat{\\beta}_{S_{(k)}}$和$\\hat{\\sigma}_k^2$代入对数似然表达式中，得到：\n$$ \\hat{\\ell}_k = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2}\\ln(\\hat{\\sigma}_k^2) - \\frac{1}{2\\hat{\\sigma}_k^2}\\mathrm{RSS}_k = -\\frac{n}{2}\\ln(2\\pi\\frac{\\mathrm{RSS}_k}{n}) - \\frac{n}{2} = -\\frac{n}{2}\\ln(\\mathrm{RSS}_k) + C $$\n其中$C$是一个不依赖于$k$的常数。\n\nAIC定义为 $-2\\hat{\\ell} + 2 \\times (\\text{参数数量})$。一个大小为$k$的模型有$k$个非零系数和一个估计的方差$\\sigma^2$，所以有$k+1$个自由参数。\n$$ \\mathrm{AIC}(k) = -2\\hat{\\ell}_k + 2(k+1) = n\\ln(\\mathrm{RSS}_k) - 2C + 2(k+1) $$\n忽略与$k$无关的常数项，需要最小化的表达式是：\n$$ \\mathrm{AIC}(k) \\propto n\\ln(\\mathrm{RSS}_k) + 2k $$\n\nBIC定义为 $-2\\hat{\\ell} + \\ln(n) \\times (\\text{参数数量})$。\n$$ \\mathrm{BIC}(k) = -2\\hat{\\ell}_k + (k+1)\\ln(n) = n\\ln(\\mathrm{RSS}_k) - 2C + (k+1)\\ln(n) $$\n忽略与$k$无关的常数项，需要最小化的表达式是：\n$$ \\mathrm{BIC}(k) \\propto n\\ln(\\mathrm{RSS}_k) + k\\ln(n) $$\n\n现在我们使用提供的数据来找到$k_{\\mathrm{AIC}}$和$k_{\\mathrm{BIC}}$。\n$n = 120$, $p = 10$, $\\|y\\|^2 = 300$。\n$z = [\\,5.2,\\,-3.1,\\,2.7,\\,-2.2,\\,1.9,\\,-1.5,\\,1.2,\\,0.9,\\,-0.6,\\,0.4\\,]$。\n$z$的分量已经按大小降序排列。我们计算平方值：\n$z_{(1)}^2 = 5.2^2 = 27.04$\n$z_{(2)}^2 = (-3.1)^2 = 9.61$\n$z_{(3)}^2 = 2.7^2 = 7.29$\n$z_{(4)}^2 = (-2.2)^2 = 4.84$\n$z_{(5)}^2 = 1.9^2 = 3.61$\n$z_{(6)}^2 = (-1.5)^2 = 2.25$\n$z_{(7)}^2 = 1.2^2 = 1.44$\n$z_{(8)}^2 = 0.9^2 = 0.81$\n$z_{(9)}^2 = (-0.6)^2 = 0.36$\n$z_{(10)}^2 = 0.4^2 = 0.16$\n\n令$S_k = \\sum_{j=1}^{k} z_{(j)}^2$。我们计算累积和：\n$S_0 = 0$\n$S_1 = 27.04$\n$S_2 = 36.65$\n$S_3 = 43.94$\n$S_4 = 48.78$\n$S_5 = 52.39$\n$S_6 = 54.64$\n$S_7 = 56.08$\n$S_8 = 56.89$\n$S_9 = 57.25$\n$S_{10} = 57.41$\n\n现在，我们计算$\\mathrm{RSS}_k = \\|y\\|^2 - S_k = 300 - S_k$：\n$\\mathrm{RSS}_0 = 300$\n$\\mathrm{RSS}_1 = 272.96$\n$\\mathrm{RSS}_2 = 263.35$\n$\\mathrm{RSS}_3 = 256.06$\n$\\mathrm{RSS}_4 = 251.22$\n$\\mathrm{RSS}_5 = 247.61$\n$\\mathrm{RSS}_6 = 245.36$\n$\\mathrm{RSS}_7 = 243.92$\n$\\mathrm{RSS}_8 = 243.11$\n$\\mathrm{RSS}_9 = 242.75$\n$\\mathrm{RSS}_{10} = 242.59$\n\n为了找到$k_{\\mathrm{AIC}}$，我们最小化$\\mathrm{AIC}(k) \\propto 120\\ln(\\mathrm{RSS}_k) + 2k$。\n$\\mathrm{AIC}(0) \\propto 120\\ln(300) + 0 \\approx 684.46$\n$\\mathrm{AIC}(1) \\propto 120\\ln(272.96) + 2 \\approx 673.12 + 2 = 675.12$\n$\\mathrm{AIC}(2) \\propto 120\\ln(263.35) + 4 \\approx 668.82 + 4 = 672.82$\n$\\mathrm{AIC}(3) \\propto 120\\ln(256.06) + 6 \\approx 665.45 + 6 = 671.45$\n$\\mathrm{AIC}(4) \\propto 120\\ln(251.22) + 8 \\approx 663.16 + 8 = 671.16$\n$\\mathrm{AIC}(5) \\propto 120\\ln(247.61) + 10 \\approx 661.42 + 10 = 671.42$\n$\\mathrm{AIC}(6) \\propto 120\\ln(245.36) + 12 \\approx 660.32 + 12 = 672.32$\n最小值出现在$k=4$处。$k=5$时的值略大。因此，$k_{\\mathrm{AIC}} = 4$。\n\n为了找到$k_{\\mathrm{BIC}}$，我们最小化$\\mathrm{BIC}(k) \\propto 120\\ln(\\mathrm{RSS}_k) + k\\ln(120)$。\n我们有 $\\ln(120) \\approx 4.7875$。\n$\\mathrm{BIC}(0) \\propto 120\\ln(300) + 0 \\approx 684.46$\n$\\mathrm{BIC}(1) \\propto 120\\ln(272.96) + \\ln(120) \\approx 673.12 + 4.79 = 677.91$\n$\\mathrm{BIC}(2) \\propto 120\\ln(263.35) + 2\\ln(120) \\approx 668.82 + 9.58 = 678.40$\n$\\mathrm{BIC}(3) \\propto 120\\ln(256.06) + 3\\ln(120) \\approx 665.45 + 14.36 = 679.81$\n在$k=1$之后，$\\mathrm{BIC}(k)$的值增加，因为惩罚项$k\\ln(120)$的增长速度快于拟合优度项$120\\ln(\\mathrm{RSS}_k)$的减少速度。最小值在$k=1$处。因此，$k_{\\mathrm{BIC}} = 1$。\n\n最优模型大小的数对是 $(k_{\\mathrm{AIC}}, k_{\\mathrm{BIC}})$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在经典框架的基础上，本题挑战您将信息准则的概念推广到标准高斯噪声假设之外。您将为一个带有拉普拉斯噪声的线性模型推导出一个类似AIC的惩罚项，该模型与L1损失最小化相关。这项练习揭示了一个更深层次的原理：“惩罚项 = $2 \\times$ 参数数量”这一规则是最大似然理论的一个更普遍的推论，而不仅仅是高斯模型的特性。",
            "id": "3452916",
            "problem": "考虑具有由拉普拉斯分布建模的独立同分布噪声的线性回归。具体来说，假设 $y_{i} \\in \\mathbb{R}$ 和 $x_{i} \\in \\mathbb{R}^{p}$ 满足 $y_{i} = x_{i}^{\\top}\\beta + \\epsilon_{i}$ (其中 $i=1,\\dots,n$)，$\\epsilon_{i}$ 是独立同分布的拉普拉斯随机变量，其均值为 $0$，尺度参数为 $b0$，密度函数为 $f(\\epsilon) = \\frac{1}{2b}\\exp\\!\\big(-|\\epsilon|/b\\big)$。假设设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 是固定的，并考虑一个候选稀疏模型，该模型仅使用一个索引集 $S \\subset \\{1,\\dots,p\\}$ (其中 $|S| = k$)，其对应的参数向量为 $\\beta_{S} \\in \\mathbb{R}^{k}$ (其余分量被约束为零)。参数 $(\\beta_{S}, b)$ 通过最大似然估计得到，这等同于关于 $\\beta_{S}$ 最小化绝对残差之和，并通过最大化拉普拉斯似然来估计 $b$。\n\n将训练偏差定义为 $D_{\\mathrm{train}}(S) = -2\\,\\ell\\big(\\hat{\\beta}_{S}, \\hat{b}; y\\big)$，其中 $\\ell(\\beta, b; y)$ 是在由 $S$ 索引的模型下观测样本的拉普拉斯对数似然，而 $(\\hat{\\beta}_{S}, \\hat{b})$ 是在该模型下的最大似然估计。对于从相同数据生成过程中抽取的、以训练设计 $X$ 为条件的新独立样本 $y^{\\star}$，将预测偏差定义为 $D_{\\mathrm{pred}}(S) = -2\\,\\mathbb{E}_{y^{\\star}|X}\\!\\big[\\ell\\big(\\hat{\\beta}_{S}, \\hat{b}; y^{\\star}\\big)\\big]$，其中期望是关于 $y^{\\star}$ 的抽样分布计算的，并且条件是用于计算 $(\\hat{\\beta}_{S}, \\hat{b})$ 的 $X$ 和训练样本。\n\n从上述定义出发，并利用正确指定的参数模型下最大似然估计的渐近性质，构建一个形式为 $\\mathrm{IC}(S) = D_{\\mathrm{train}}(S) + \\mathrm{pen}(k,n)$ 的信息准则，该准则能够产生预测偏差的渐近无偏估计，即当 $n \\to \\infty$ 时，$\\mathbb{E}\\!\\big[\\mathrm{IC}(S)\\big] = \\mathbb{E}\\!\\big[D_{\\mathrm{pred}}(S)\\big] + o(1)$。推导出罚项 $\\mathrm{pen}(k,n)$ 关于 $k$ 和任何其他必要量的显式闭式表达式。你的最终答案必须是单一的闭式解析表达式。如果需要呈现数值常数，请精确保留它，不要进行数值近似。",
            "solution": "问题要求推导一个信息准则的罚项 $\\mathrm{pen}(k,n)$，该准则的形式为 $\\mathrm{IC}(S) = D_{\\mathrm{train}}(S) + \\mathrm{pen}(k,n)$。该准则必须提供预测偏差 $D_{\\mathrm{pred}}(S)$ 的渐近无偏估计，即当样本量 $n \\to \\infty$ 时，$\\mathbb{E}[\\mathrm{IC}(S)] = \\mathbb{E}[D_{\\mathrm{pred}}(S)] + o(1)$。期望是关于训练数据 $y$ 的分布计算的。\n\n根据条件 $\\mathbb{E}[D_{\\mathrm{train}}(S) + \\mathrm{pen}(k,n)] = \\mathbb{E}[D_{\\mathrm{pred}}(S)] + o(1)$，并假设罚项 $\\mathrm{pen}(k,n)$ 是非随机的，我们必须推导期望乐观度 (expected optimism) 的渐近值表达式，它被定义为期望预测偏差与期望训练偏差之间的差值：\n$$\n\\mathrm{pen}(k,n) \\approx \\mathbb{E}[D_{\\mathrm{pred}}(S)] - \\mathbb{E}[D_{\\mathrm{train}}(S)]\n$$\n此推导遵循了用于赤池信息准则 (Akaike's Information Criterion, AIC) 的一般逻辑，依赖于最大似然估计 (Maximum Likelihood Estimators, MLEs) 的渐近性质。问题陈述指明，一个由集合 $S$ (其中 $|S|=k$) 索引的模型涉及估计一个参数向量 $\\beta_S \\in \\mathbb{R}^k$ 和拉普拉斯分布的尺度参数 $b$。因此，该模型需要估计的总参数数量为 $d = k+1$。令 $\\theta = (\\beta_S, b)$ 为 $(k+1)$ 维参数向量。令 $\\hat{\\theta} = (\\hat{\\beta}_S, \\hat{b})$ 表示从训练数据 $y$ 中获得的 $\\theta$ 的最大似然估计，并令 $\\theta_0$ 为生成数据的真实未知参数向量。“正确指定”模型的假设意味着，在所考虑的模型族中存在这样一个 $\\theta_0$。\n\n训练偏差为 $D_{\\mathrm{train}}(S) = -2\\ell(\\hat{\\theta}; y)$，其中 $\\ell(\\theta;y)$ 是训练数据 $y$ 的对数似然函数。预测偏差为 $D_{\\mathrm{pred}}(S) = -2\\mathbb{E}_{y^\\star|X}[\\ell(\\hat{\\theta}; y^\\star)]$，其中期望是关于来自相同生成过程的新独立数据集 $y^\\star$ 计算的。\n\n让我们分析训练偏差与在真实参数 $\\theta_0$ 处求值的对数似然之间的关系。我们围绕最大似然估计 $\\hat{\\theta}$ 对 $\\ell(\\theta_0; y)$ 进行二阶泰勒级数展开：\n$$\n\\ell(\\theta_0; y) \\approx \\ell(\\hat{\\theta}; y) + (\\theta_0 - \\hat{\\theta})^{\\top} \\nabla\\ell(\\hat{\\theta}; y) + \\frac{1}{2}(\\theta_0 - \\hat{\\theta})^{\\top} \\nabla^2\\ell(\\hat{\\theta}; y) (\\theta_0 - \\hat{\\theta})\n$$\n根据最大似然估计的定义，在 $\\hat{\\theta}$ 处的得分（梯度）为零，即 $\\nabla\\ell(\\hat{\\theta}; y) = 0$。两边乘以 $-2$，我们得到：\n$$\n-2\\ell(\\theta_0; y) \\approx -2\\ell(\\hat{\\theta}; y) - (\\theta_0 - \\hat{\\theta})^{\\top} \\nabla^2\\ell(\\hat{\\theta}; y) (\\theta_0 - \\hat{\\theta}) = D_{\\mathrm{train}}(S) + (\\hat{\\theta} - \\theta_0)^{\\top} [-\\nabla^2\\ell(\\hat{\\theta}; y)] (\\hat{\\theta} - \\theta_0)\n$$\n根据最大似然估计的标准渐近理论，对于大的 $n$，最大似然估计 $\\hat{\\theta}$ 是一致的，即 $\\hat{\\theta} \\to \\theta_0$ (依概率)。此外，观测到的费雪信息 (observed Fisher information) $-\\frac{1}{n}\\nabla^2\\ell(\\hat{\\theta}; y)$ 收敛于单位观测的费雪信息矩阵 $I(\\theta_0)$。因此，$-\\nabla^2\\ell(\\hat{\\theta}; y) \\approx n I(\\theta_0)$。最大似然估计的渐近分布由 $\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\xrightarrow{d} N(0, I(\\theta_0)^{-1})$ 给出。\n对训练数据 $y$ 取期望：\n$$\n\\mathbb{E}[-2\\ell(\\theta_0; y)] \\approx \\mathbb{E}[D_{\\mathrm{train}}(S)] + \\mathbb{E}\\left[(\\hat{\\theta} - \\theta_0)^{\\top} (n I(\\theta_0)) (\\hat{\\theta} - \\theta_0)\\right]\n$$\n该二次型渐近地遵循矩阵乘积的迹：$\\mathbb{E}[Z^\\top A Z] = \\mathbb{E}[Z]^\\top A \\mathbb{E}[Z] + \\mathrm{tr}(A \\cdot \\mathrm{Cov}(Z))$。这里 $Z = (\\hat{\\theta} - \\theta_0)$ 且 $A = nI(\\theta_0)$。渐近地，$\\mathbb{E}[Z] \\approx 0$ 且 $\\mathrm{Cov}(Z) \\approx (nI(\\theta_0))^{-1}$。二次项的期望渐近地为 $\\mathrm{tr}(nI(\\theta_0) \\cdot (nI(\\theta_0))^{-1}) = \\mathrm{tr}(I_d) = d$，其中 $d=k+1$ 是 $\\theta$ 的维度。\n所以，我们得到第一个关系：\n$$\n\\mathbb{E}[-2\\ell(\\theta_0; y)] \\approx \\mathbb{E}[D_{\\mathrm{train}}(S)] + d\n$$\n\n接下来，我们分析预测偏差。我们围绕真实参数 $\\theta_0$ 展开新数据 $y^\\star$ 的对数似然 $\\ell(\\hat{\\theta}; y^\\star)$：\n$$\n\\ell(\\hat{\\theta}; y^\\star) \\approx \\ell(\\theta_0; y^\\star) + (\\hat{\\theta} - \\theta_0)^{\\top} \\nabla\\ell(\\theta_0; y^\\star) + \\frac{1}{2}(\\hat{\\theta} - \\theta_0)^{\\top} \\nabla^2\\ell(\\theta_0; y^\\star) (\\hat{\\theta} - \\theta_0)\n$$\n预测偏差涉及对 $y^\\star$ 的期望。注意到 $\\hat{\\theta}$ 相对于 $y^\\star$是固定的：\n$$\n\\mathbb{E}_{y^\\star}[\\ell(\\hat{\\theta}; y^\\star)] \\approx \\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)] + (\\hat{\\theta} - \\theta_0)^{\\top} \\mathbb{E}_{y^\\star}[\\nabla\\ell(\\theta_0; y^\\star)] + \\frac{1}{2}(\\hat{\\theta} - \\theta_0)^{\\top} \\mathbb{E}_{y^\\star}[\\nabla^2\\ell(\\theta_0; y^\\star)] (\\hat{\\theta} - \\theta_0)\n$$\n在真实参数 $\\theta_0$ 处，我们有 $\\mathbb{E}_{y^\\star}[\\nabla\\ell(\\theta_0; y^\\star)] = 0$ 和 $\\mathbb{E}_{y^\\star}[\\nabla^2\\ell(\\theta_0; y^\\star)] = -n I(\\theta_0)$。因此：\n$$\n\\mathbb{E}_{y^\\star}[\\ell(\\hat{\\theta}; y^\\star)] \\approx \\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)] - \\frac{1}{2}(\\hat{\\theta} - \\theta_0)^{\\top} (n I(\\theta_0)) (\\hat{\\theta} - \\theta_0)\n$$\n两边乘以 $-2$ 得到：\n$$\nD_{\\mathrm{pred}}(S) = -2\\mathbb{E}_{y^\\star}[\\ell(\\hat{\\theta}; y^\\star)] \\approx -2\\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)] + (\\hat{\\theta} - \\theta_0)^{\\top} (n I(\\theta_0)) (\\hat{\\theta} - \\theta_0)\n$$\n现在，我们对训练数据 $y$ 取期望：\n$$\n\\mathbb{E}_y[D_{\\mathrm{pred}}(S)] \\approx \\mathbb{E}_y[-2\\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)]] + \\mathbb{E}_y\\left[(\\hat{\\theta} - \\theta_0)^{\\top} (n I(\\theta_0)) (\\hat{\\theta} - \\theta_0)\\right]\n$$\n由于 $y$ 和 $y^\\star$ 是独立同分布的样本，$\\mathbb{E}_y[-2\\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)]] = \\mathbb{E}_y[-2\\ell(\\theta_0; y)]$。第二项，如前所计算，其期望渐近地为 $d$。\n这给出了第二个关系：\n$$\n\\mathbb{E}[D_{\\mathrm{pred}}(S)] \\approx \\mathbb{E}[-2\\ell(\\theta_0; y)] + d\n$$\n\n结合这两个渐近关系：\n$$\n\\mathbb{E}[D_{\\mathrm{train}}(S)] \\approx \\mathbb{E}[-2\\ell(\\theta_0; y)] - d\n$$\n$$\n\\mathbb{E}[D_{\\mathrm{pred}}(S)] \\approx \\mathbb{E}[-2\\ell(\\theta_0; y)] + d\n$$\n用第二个关系减去第一个关系，我们得到期望乐观度：\n$$\n\\mathbb{E}[D_{\\mathrm{pred}}(S)] - \\mathbb{E}[D_{\\mathrm{train}}(S)] \\approx (\\mathbb{E}[-2\\ell(\\theta_0; y)] + d) - (\\mathbb{E}[-2\\ell(\\theta_0; y)] - d) = 2d\n$$\n因此，纠正训练偏差的乐观偏差的罚项渐近地为 $2d$。问题指明，一个 $|S|=k$ 的候选模型估计了 $\\beta_S$ 中的 $k$ 个回归系数和尺度参数 $b$。估计的参数总数为 $d = k+1$。\n罚项是：\n$$\n\\mathrm{pen}(k,n) = 2d = 2(k+1)\n$$\n这是推广到此建模背景下的赤池信息准则 (AIC) 的罚项。尽管拉普拉斯似然函数并非处处可微，这违反了经典的正则性条件，但已知 AIC 的结果在覆盖此情况的更一般条件下仍然成立。问题中要求使用标准渐近性质的指引证实了这是预期的解题路径。得到的罚项依赖于 $k$ 但不依赖于 $n$，这与标准的 AIC 公式是一致的。",
            "answer": "$$ \\boxed{2(k+1)} $$"
        },
        {
            "introduction": "这项高级实践介绍了一种新颖而强大的模型选择范式：最小描述长度（MDL）原则。您将通过显式计算描述模型和数据所需的编码长度，从头开始构建一个选择准则。这项练习展示了如何将设计矩阵的结构特性（例如互相关性）直接整合到选择规则中，为稀疏建模中的权衡问题提供了一个丰富的信息论视角。",
            "id": "3452868",
            "problem": "考虑稀疏线性模型 $y = A x + \\varepsilon$，其中 $y \\in \\mathbb{R}^{m}$，$A \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^{n}$。假设 $A$ 的列是 $\\ell_{2}$-归一化的，噪声 $\\varepsilon$ 服从均值为零、协方差为 $\\sigma^{2} I_{m}$ 的高斯分布，且 $x$ 的支撑集 $S \\subset \\{1,\\dots,n\\}$ 的基数为 $|S| = k$。对于每个候选支撑集 $S$，一个模型由 $(S, x_{S})$ 指定，其中 $x_{S} \\in \\mathbb{R}^{k}$ 包含非零幅值。您希望设计一个最小描述长度 (MDL) 选择准则，该准则明确考虑 (i) $S$ 中索引的码长，(ii) 幅值 $x_{S}$ 的量化，以及 (iii) 在 $A$ 的不同相干性下残差的描述长度。\n\n采用以下编码假设：\n- $S$ 中的索引使用对 $\\{1,\\dots,n\\}$ 的所有 $k$-子集的枚举编码进行编码。\n- $x_{S}$ 中的幅值作为 $k$ 个独立的实数，通过一个在固定已知范围 $[-B, B]$（其中 $B  0$）上步长为 $\\Delta  0$ 的均匀标量量化器进行编码，这意味着 $x_{S}$ 的每个坐标的表示误差最多为 $\\Delta/2$。\n- 给定模型 $(S, x_{S})$ 的数据 $y$ 使用对应于方差为 $\\sigma^{2}$ 的高斯似然的理想香农码长进行编码。\n\n为明确反映 $A$ 的相干性，使用相互相干性定义 $\\mu(A) := \\max_{i \\neq j} |a_{i}^{\\top} a_{j}|$，其中 $a_{i}$ 表示 $A$ 的第 $i$ 列，并回顾对于任何大小为 $k$ 的支撑集 $S$，格拉姆矩阵 $A_{S}^{\\top} A_{S}$ 的最大特征值满足 $\\lambda_{\\max}(A_{S}^{\\top} A_{S}) \\leq 1 + (k-1) \\mu(A)$。\n\n从第一性原理出发——即最小描述长度 (MDL) 原理作为模型码长和数据码长之和，高斯负对数似然作为理想码长，以及由相互相干性导出的范数界——在对量化器步长 $\\Delta$ 的总描述长度进行优化后，推导 MDL 准则 $L_{\\mathrm{MDL}}(S)$ 的闭式表达式（以奈特为单位）。您的推导应通过相干性来界定幅值量化误差的贡献，从而构建残差描述长度，并且应使用最小二乘 (LS) 残差范数 $R_{S} := \\|y - A_{S} \\hat{x}_{S}\\|_{2}^{2}$，其中 $\\hat{x}_{S}$ 是在支撑集 $S$ 上忽略量化的最小范数最小二乘估计。仅用 $n$, $k$, $B$, $\\sigma$, $\\mu(A)$, $m$ 和 $R_{S}$ 表示您的最终选择准则。\n\n以单一闭式解析表达式的形式提供 $L_{\\mathrm{MDL}}(S)$ 的最终答案（以奈特为单位）。不需要进行数值评估，也无需四舍五入。",
            "solution": "用户提供的问题已经过评估和验证，是合理的。它在信息论和线性代数方面具有科学依据，问题阐述清晰、客观，并包含进行严格推导所需的所有必要信息。\n\n目标是推导一个用于稀疏模型选择的最小描述长度 (MDL) 准则。MDL 原理假定，最佳模型是能够以最短的总描述长度来描述模型本身以及借助该模型编码的数据的模型。以奈特 (nats) 为单位的总码长 $L_{\\mathrm{total}}$ 是模型码长 $L_{\\mathrm{model}}$ 和数据码长 $L_{\\mathrm{data}}$ 的和。\n\n$$\nL_{\\mathrm{total}}(S, x_S) = L_{\\mathrm{model}}(S, x_S) + L_{\\mathrm{data}}(y | S, x_S)\n$$\n\n对于给定的支撑集 $S$，最终的 MDL 准则是通过选择模型参数（在此例中为量化步长 $\\Delta$）来最小化此总长度而获得的。\n\n$$\nL_{\\mathrm{MDL}}(S) = \\min_{\\Delta  0} L_{\\mathrm{total}}(S, \\Delta)\n$$\n\n我们现在将根据问题的具体说明推导总描述长度的各个组成部分。\n\n**1. 模型描述长度 ($L_{\\mathrm{model}}$)**\n\n模型由支撑集 $S$ 和该支撑集上量化后的非零幅值指定。总模型码长是这两个组成部分码长的总和。\n\n$$\nL_{\\mathrm{model}}(S, \\Delta) = L(S) + L(x_S | \\Delta)\n$$\n\n- **支撑集 $S$ 的码长**：支撑集 $S$ 是从索引集 $\\{1, \\dots, n\\}$ 中选出的一个大小为 $k$ 的子集。共有 $\\binom{n}{k}$ 个这样的子集。使用枚举编码，其中每个子集都被分配一个唯一的索引，指定这样一个子集的码长为：\n$$\nL(S) = \\ln\\left(\\binom{n}{k}\\right)\n$$\n\n- **幅值 $x_S$ 的码长**：$k$ 个非零幅值是独立编码的。每个幅值定义在一个已知范围 $[-B, B]$ 内，并使用步长为 $\\Delta$ 的均匀标量量化器进行量化。单个幅值的量化级别数为 $\\frac{2B}{\\Delta}$。假设在这些级别上概率均匀分布，则一个幅值的码长为 $\\ln(\\frac{2B}{\\Delta})$。对于 $k$ 个独立的幅值，总码长为：\n$$\nL(x_S | \\Delta) = k \\ln\\left(\\frac{2B}{\\Delta}\\right)\n$$\n\n将这些结合起来，模型描述长度为：\n$$\nL_{\\mathrm{model}}(S, \\Delta) = \\ln\\left(\\binom{n}{k}\\right) + k \\ln\\left(\\frac{2B}{\\Delta}\\right)\n$$\n\n**2. 数据描述长度 ($L_{\\mathrm{data}}$)**\n\n数据 $y$ 使用从指定的高斯似然导出的理想香农码长进行编码。对于噪声模型 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$，给定模型预测 $A_S x_S$ 后观测到 $y$ 的负对数似然为：\n\n$$\nL_{\\mathrm{data}}(y | S, x_S) = -\\ln p(y | S, x_S) = \\frac{m}{2} \\ln(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\|y - A_S x_S\\|_2^2\n$$\n\n幅值 $x_S$ 是一个量化表示。我们使用支撑集 $S$ 上的最小范数最小二乘 (LS) 估计，记为 $\\hat{x}_S = (A_S^\\top A_S)^{-1} A_S^\\top y$，作为理想的未量化向量。传输的向量是其量化版本，我们称之为 $(x_S)_q$。量化误差为 $e_q = \\hat{x}_S - (x_S)_q$。问题陈述中指出，幅值向量的每个分量的表示误差在 $\\frac{\\Delta}{2}$ 以内，这意味着量化误差向量的无穷范数有一个上界：$\\|e_q\\|_\\infty \\le \\frac{\\Delta}{2}$。\n\n我们必须用 LS 残差范数 $R_S = \\|y - A_S \\hat{x}_S\\|_2^2$ 来表示残差项 $\\|y - A_S (x_S)_q\\|_2^2$。\n\n$$\n\\|y - A_S (x_S)_q\\|_2^2 = \\|y - A_S (\\hat{x}_S - e_q)\\|_2^2 = \\|(y - A_S \\hat{x}_S) + A_S e_q\\|_2^2\n$$\n\n设 LS 残差向量为 $r_S = y - A_S \\hat{x}_S$。最小二乘法的一个基本性质是残差向量与 $A_S$ 的列空间正交，即 $A_S^\\top r_S = 0$。展开范数：\n\n$$\n\\|r_S + A_S e_q\\|_2^2 = \\|r_S\\|_2^2 + 2r_S^\\top(A_S e_q) + \\|A_S e_q\\|_2^2 = R_S + 2(A_S^\\top r_S)^\\top e_q + \\|A_S e_q\\|_2^2\n$$\n\n由于 $A_S^\\top r_S = 0$，交叉项消失。表达式简化为：\n$$\n\\|y - A_S (x_S)_q\\|_2^2 = R_S + \\|A_S e_q\\|_2^2\n$$\n\n为了创建一个确定性的码长，我们必须对量化误差的贡献 $\\|A_S e_q\\|_2^2$ 进行界定。这里就需要用到相干性信息。\n\n$$\n\\|A_S e_q\\|_2^2 = e_q^\\top A_S^\\top A_S e_q \\le \\lambda_{\\max}(A_S^\\top A_S) \\|e_q\\|_2^2\n$$\n\n使用给定的界 $\\lambda_{\\max}(A_S^\\top A_S) \\le 1 + (k-1)\\mu(A)$ 和量化误差范数的界 $\\|e_q\\|_2^2 \\le \\sum_{i=1}^k (\\frac{\\Delta}{2})^2 = k \\frac{\\Delta^2}{4}$，我们得到：\n\n$$\n\\|A_S e_q\\|_2^2 \\le \\left(1 + (k-1)\\mu(A)\\right) \\frac{k\\Delta^2}{4}\n$$\n\n我们在数据描述长度中使用这个最坏情况下的界。这得到：\n$$\nL_{\\mathrm{data}}(y | S, \\Delta) = \\frac{m}{2} \\ln(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}\\left(R_S + \\frac{k\\left(1 + (k-1)\\mu(A)\\right)\\Delta^2}{4}\\right)\n$$\n\n**3. 总描述长度与优化**\n\n结合模型和数据长度，总描述长度作为 $\\Delta$ 的函数是：\n$$\nL_{\\mathrm{total}}(S, \\Delta) = \\ln\\left(\\binom{n}{k}\\right) + k \\ln\\left(\\frac{2B}{\\Delta}\\right) + \\frac{m}{2} \\ln(2\\pi\\sigma^2) + \\frac{R_S}{2\\sigma^2} + \\frac{k(1 + (k-1)\\mu(A))}{8\\sigma^2} \\Delta^2\n$$\n我们可以将 $k\\ln(\\frac{2B}{\\Delta})$ 重写为 $k\\ln(2B) - k\\ln(\\Delta)$。为了找到最优的量化器步长 $\\Delta^*$，我们对 $L_{\\mathrm{total}}(S, \\Delta)$ 关于 $\\Delta$ 求最小值。我们对依赖于 $\\Delta$ 的项求导，并令导数为零：\n\n$$\n\\frac{d}{d\\Delta} \\left( -k\\ln(\\Delta) + \\frac{k(1 + (k-1)\\mu(A))}{8\\sigma^2} \\Delta^2 \\right) = -\\frac{k}{\\Delta} + \\frac{k(1 + (k-1)\\mu(A))}{4\\sigma^2} \\Delta = 0\n$$\n\n求解 $\\Delta^2$（对于 $k0$）：\n$$\n\\frac{(1 + (k-1)\\mu(A))}{4\\sigma^2} \\Delta^2 = \\frac{1}{1} \\implies \\Delta^{*2} = \\frac{4\\sigma^2}{1 + (k-1)\\mu(A)}\n$$\n最优量化器步长为 $\\Delta^* = \\frac{2\\sigma}{\\sqrt{1 + (k-1)\\mu(A)}}$。二阶导数为正，确认这是一个最小值点。\n\n**4. 最终 MDL 准则表达式**\n\n最后，我们将 $\\Delta^*$ 代入 $L_{\\mathrm{total}}(S, \\Delta)$ 的表达式中，以获得 MDL 准则 $L_{\\mathrm{MDL}}(S)$。\n包含 $\\Delta^2$ 的项变为：\n$$\n\\frac{k(1 + (k-1)\\mu(A))}{8\\sigma^2} \\Delta^{*2} = \\frac{k(1 + (k-1)\\mu(A))}{8\\sigma^2} \\frac{4\\sigma^2}{1 + (k-1)\\mu(A)} = \\frac{k}{2}\n$$\n包含 $\\Delta$ 的对数项变为：\n$$\nk \\ln\\left(\\frac{2B}{\\Delta^*}\\right) = k \\ln\\left(\\frac{2B \\sqrt{1 + (k-1)\\mu(A)}}{2\\sigma}\\right) = k \\ln\\left(\\frac{B \\sqrt{1 + (k-1)\\mu(A)}}{\\sigma}\\right)\n$$\n这可以展开为：\n$$\nk \\ln\\left(\\frac{B}{\\sigma}\\right) + \\frac{k}{2} \\ln(1 + (k-1)\\mu(A))\n$$\n将所有部分相加，最终的 MDL 准则为：\n$$\nL_{\\mathrm{MDL}}(S) = \\frac{R_S}{2\\sigma^2} + \\frac{m}{2}\\ln(2\\pi\\sigma^2) + \\ln\\left(\\binom{n}{k}\\right) + k\\ln\\left(\\frac{B}{\\sigma}\\right) + \\frac{k}{2}\\ln(1 + (k-1)\\mu(A)) + \\frac{k}{2}\n$$\n该表达式是完整的 MDL 准则（以奈特为单位），包括数据保真度 ($R_S$)、模型结构 ($k, n, \\mu(A)$) 和实验参数 ($m, \\sigma, B$) 的项。",
            "answer": "$$\n\\boxed{\\frac{R_S}{2\\sigma^2} + \\frac{m}{2}\\ln(2\\pi\\sigma^2) + \\ln\\binom{n}{k} + k\\ln\\left(\\frac{B}{\\sigma}\\right) + \\frac{k}{2}\\ln(1 + (k-1)\\mu(A)) + \\frac{k}{2}}\n$$"
        }
    ]
}