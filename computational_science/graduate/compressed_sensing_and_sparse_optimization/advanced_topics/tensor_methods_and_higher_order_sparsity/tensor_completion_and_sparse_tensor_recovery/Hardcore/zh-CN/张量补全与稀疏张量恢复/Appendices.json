{
    "hands_on_practices": [
        {
            "introduction": "理论上的恢复保证通常依赖于某些理想条件。本练习旨在通过一个思想实验，揭示张量补全中的一个基本挑战——相干性（coherence）。通过构建一个极度相干的秩-1张量反例，您将亲身体会到为何均匀随机采样并不总是足够，并直观理解“非相干性”假设在确保唯一可恢复性中的关键作用。",
            "id": "3485381",
            "problem": "考虑非相干性在低秩张量补全中的作用。设 $d \\in \\mathbb{N}$ 且 $d \\geq 3$，并设 $X \\in \\mathbb{R}^{n_{1} \\times n_{2} \\times \\cdots \\times n_{d}}$ 是一个秩为1的典范多项分解（Canonical Polyadic, CP）张量，定义为 $X = \\alpha \\, \\mathbf{e}_{i_{1}^{\\star}} \\otimes \\mathbf{e}_{i_{2}^{\\star}} \\otimes \\cdots \\otimes \\mathbf{e}_{i_{d}^{\\star}}$，其中 $\\alpha > 0$ 是一个标量，$\\mathbf{e}_{i}$ 表示标准基向量。因此，$X$ 在索引 $(i_{1}^{\\star}, i_{2}^{\\star}, \\ldots, i_{d}^{\\star})$ 处有一个值为 $\\alpha$ 的唯一非零项，而在其他位置均为零，这是一种高度相干的结构。设 $N = \\prod_{\\ell=1}^{d} n_{\\ell}$ 为 $X$ 中项的总数。\n\n假设一个采样算子从 $N$ 个可能的索引中不放回地均匀随机选择并揭示 $X$ 的 $m$ 个不同项，并返回它们无噪声的值。要求一个补全算法在秩为1的约束下从这些揭示的项中恢复 $X$，但没有施加非相干性假设。\n\n仅使用张量代数的核心定义和初等概率论，通过论证当唯一的非零项未被采样时，观测值恒为零，因此与任何支撑在未观测索引上的秩为1的张量（特别是零张量）同样一致，从而构造一个可辨识性的反例。然后，推导这个失效事件的确切概率，用一个包含 $n_{1}, n_{2}, \\ldots, n_{d}$ 和 $m$ 的闭式解析表达式表示。\n\n你的最终答案必须是单一的解析表达式。不需要四舍五入，也不需要报告单位。",
            "solution": "问题陈述已被解析和验证。我们发现它具有科学依据、问题设定良好、客观且内部一致。它提出了一个在张量补全领域内有效的理论问题。因此，我们可以进行完整解答。\n\n该问题要求两件事：首先，构造一个反例，展示当一个高度相干张量的唯一非零项未被采样时，可辨识性会失效；其次，推导这个失效事件的确切概率。\n\n设 $\\mathcal{X} \\in \\mathbb{R}^{n_{1} \\times n_{2} \\times \\cdots \\times n_{d}}$ 是一个 $d$ 阶张量。问题定义了一个特定的秩为1的典范多项分解（CP）张量 $X$，由标准基向量的外积给出：\n$$\nX = \\alpha \\, \\mathbf{e}_{i_{1}^{\\star}} \\otimes \\mathbf{e}_{i_{2}^{\\star}} \\otimes \\cdots \\otimes \\mathbf{e}_{i_{d}^{\\star}}\n$$\n其中 $\\alpha > 0$ 是一个标量，$\\mathbf{e}_{k} \\in \\mathbb{R}^{n}$ 是一个在第 $k$ 个位置为1、其他位置为0的向量，而 $(i_{1}^{\\star}, i_{2}^{\\star}, \\ldots, i_{d}^{\\star})$ 是一个特定的多重索引。这个张量 $X$ 恰好有一个非零项 $X_{i_{1}^{\\star}, i_{2}^{\\star}, \\ldots, i_{d}^{\\star}} = \\alpha$，所有其他项均为0。这样的张量是最大相干的。\n\n设 $\\Omega$ 是所有可能的多重索引 $(j_{1}, j_{2}, \\ldots, j_{d})$ 的集合，其中对于 $\\ell = 1, \\ldots, d$，有 $1 \\leq j_{\\ell} \\leq n_{\\ell}$。张量中项的总数为 $N = |\\Omega| = \\prod_{\\ell=1}^{d} n_{\\ell}$。\n从 $\\Omega$ 中不放回地均匀随机选择一个包含 $m$ 个不同索引的集合 $\\Omega_{\\text{obs}} \\subset \\Omega$，其中 $|\\Omega_{\\text{obs}}| = m$。观测者得到这些值 $\\{X_{j} : j \\in \\Omega_{\\text{obs}}\\}$。张量补全问题是在恢复的张量必须为秩1的约束下，从这些观测值中恢复 $X$。\n\n我们将恢复的张量表示为 $\\hat{X}$。该问题可以表述为：\n$$\n\\text{寻找 } \\hat{X} \\text{ 使得 } \\text{rank}(\\hat{X}) \\leq 1 \\text{ 且 } P_{\\Omega_{\\text{obs}}}(\\hat{X}) = P_{\\Omega_{\\text{obs}}}(X)\n$$\n其中 $P_{\\Omega_{\\text{obs}}}$ 是一个投影算子，它将不在 $\\Omega_{\\text{obs}}$ 中的索引对应的所有项设为零，或者更准确地说，在观测集上强制相等。\n\n现在，我们构造这个反例。如果存在至少两个不同的张量 $\\hat{X}_{1}$ 和 $\\hat{X}_{2}$ 都满足给定约束，那么可辨识性就会失效。我们考虑的具体失效事件是当 $X$ 的唯一非零项未被采样时。设 $j^{\\star} = (i_{1}^{\\star}, i_{2}^{\\star}, \\ldots, i_{d}^{\\star})$ 是非零项的索引。失效事件为 $j^{\\star} \\notin \\Omega_{\\text{obs}}$。\n\n如果这个事件发生，那么 $X$ 的每个被采样的项都是零，因为在索引 $j^{\\star}$ 处的唯一非零项被错过了。也就是说，对于每个索引 $j \\in \\Omega_{\\text{obs}}$，我们有 $X_{j} = 0$。因此，观测数据恒为零。恢复问题变为：\n$$\n\\text{寻找 } \\hat{X} \\text{ 使得 } \\text{rank}(\\hat{X}) \\leq 1 \\text{ 且对所有 } j \\in \\Omega_{\\text{obs}} \\text{ 有 } \\hat{X}_{j} = 0\n$$\n我们可以立即确定两个满足这些条件的不同解：\n\n1. 零张量：设 $\\hat{X}_{1} = \\mathcal{O}$，即全零张量。零张量的秩为0，满足约束 $\\text{rank}(\\hat{X}_{1}) \\leq 1$。它的所有项都是0，因此它自然地与 $\\Omega_{\\text{obs}}$ 上的零观测值匹配。\n\n2. 真实张量：原始张量 $X$ 也是一个有效的候选解。我们知道 $\\text{rank}(X) = 1$。由于我们处于 $j^{\\star} \\notin \\Omega_{\\text{obs}}$ 的失效情况，索引在 $\\Omega_{\\text{obs}}$ 中的所有 $X$ 的项确实都为0。因此，$X$ 也满足这些约束。\n\n因为 $\\alpha > 0$，所以张量 $X$ 不是零张量 $\\mathcal{O}$。我们找到了两个不同的张量 $X$ 和 $\\mathcal{O}$，它们的秩都为1（或更小），并且都与观测数据完全一致。在没有额外信息或假设（例如最小化某个范数，而对于许多范数来说会偏向于零解，从而无法恢复 $X$）的情况下，算法没有理由偏好其中一个。恢复问题是不适定的，因为它没有唯一解。这就构成了可辨识性的反例。事实上，任何秩为1的张量 $\\hat{X}' = \\beta \\, \\mathbf{e}_{k_{1}} \\otimes \\cdots \\otimes \\mathbf{e}_{k_{d}}$，只要其索引 $(k_{1}, \\ldots, k_{d})$ 在未观测集 $\\Omega \\setminus \\Omega_{\\text{obs}}$ 中，也是一个有效的解，这意味着存在一个庞大的解族。\n\n接下来，我们推导这个失效事件的概率。该事件的特征是单个特殊索引 $j^{\\star}$ 未被包含在随机选择的 $m$ 个索引的集合 $\\Omega_{\\text{obs}}$ 中。\n\n从 $N$ 个索引的总池中选择一个包含 $m$ 个不同索引的集合的总方式数由二项式系数 $\\binom{N}{m}$ 给出。这是我们样本空间的大小。\n\n失效事件的有利结果数是从张量 $X$ 为零的索引集合中专门选择 $m$ 个索引的方式数。有1个非零项，所以有 $N-1$ 个零项。因此，我们必须从这 $N-1$ 个索引的集合中选择我们的 $m$ 个样本。这样做的方式数是 $\\binom{N-1}{m}$。只要 $m \\leq N-1$，这就是有效的。如果 $m=N$，失效概率为0，我们的公式将正确反映这一点。\n\n失效概率 $P(\\text{failure})$ 是有利结果数与所有可能结果数的比率：\n$$\nP(\\text{failure}) = \\frac{\\binom{N-1}{m}}{\\binom{N}{m}}\n$$\n我们展开二项式系数：\n$$\n\\binom{N-1}{m} = \\frac{(N-1)!}{m!(N-1-m)!}\n$$\n$$\n\\binom{N}{m} = \\frac{N!}{m!(N-m)!}\n$$\n因此，比率为：\n$$\nP(\\text{failure}) = \\frac{\\frac{(N-1)!}{m!(N-1-m)!}}{\\frac{N!}{m!(N-m)!}} = \\frac{(N-1)!}{N!} \\cdot \\frac{(N-m)!}{(N-1-m)!}\n$$\n我们简化这两个阶乘比：\n$$\n\\frac{(N-1)!}{N!} = \\frac{(N-1)!}{N \\cdot (N-1)!} = \\frac{1}{N}\n$$\n$$\n\\frac{(N-m)!}{(N-1-m)!} = \\frac{(N-m) \\cdot (N-m-1)!}{(N-m-1)!} = N-m\n$$\n将这两个结果相乘得到概率：\n$$\nP(\\text{failure}) = \\frac{1}{N} \\cdot (N-m) = \\frac{N-m}{N} = 1 - \\frac{m}{N}\n$$\n最后，我们将定义 $N = \\prod_{\\ell=1}^{d} n_{\\ell}$ 代入此表达式，以获得用给定问题参数表示的最终答案。\n$$\nP(\\text{failure}) = 1 - \\frac{m}{\\prod_{\\ell=1}^{d} n_{\\ell}}\n$$\n此表达式表示一个包含 $m$ 个项的随机样本未能观测到高度相干的秩为1张量 $X$ 的单个非零元素的确切概率，从而导致一个模糊的恢复问题。",
            "answer": "$$\n\\boxed{1 - \\frac{m}{\\prod_{\\ell=1}^{d} n_{\\ell}}}\n$$"
        },
        {
            "introduction": "理解了理论局限性后，我们转向构建一个实际的求解算法。本练习将指导您为重叠核范数最小化问题推导交替方向乘子法（ADMM）的更新步骤，这是张量补全中最经典和强大的算法之一。通过推导其核心更新并分析其计算成本，您将掌握将一个复杂的凸优化问题分解为一系列简单子问题的关键技巧。",
            "id": "3485380",
            "problem": "考虑一个实值$d$阶张量 $\\mathcal{X} \\in \\mathbb{R}^{N_{1} \\times \\cdots \\times N_{d}}$ 和一个观测张量 $\\mathcal{Y} \\in \\mathbb{R}^{N_{1} \\times \\cdots \\times N_{d}}$，其观测条目的索引集为 $\\Omega \\subseteq \\{1,\\ldots,N_{1}\\} \\times \\cdots \\times \\{1,\\ldots,N_{d}\\}$。令 $\\mathcal{P}_{\\Omega}$ 表示到观测索引集上的正交投影，即，如果 $(i_{1},\\ldots,i_{d}) \\in \\Omega$，则 $(\\mathcal{P}_{\\Omega}(\\mathcal{X}))_{i_{1},\\ldots,i_{d}} = \\mathcal{X}_{i_{1},\\ldots,i_{d}}$，否则为 $0$。将 $\\mathcal{X}$ 的重叠核范数定义为 $\\sum_{k=1}^{d} \\lambda_{k} \\| X_{(k)} \\|_{*}$，其中 $X_{(k)}$ 是 $\\mathcal{X}$ 的模-$k$展开，$\\| \\cdot \\|_{*}$ 是核范数，且 $\\lambda_{k}  0$。考虑张量补全目标\n$$\n\\min_{\\mathcal{X}} \\; \\frac{1}{2} \\left\\| \\mathcal{P}_{\\Omega}(\\mathcal{X} - \\mathcal{Y}) \\right\\|_{F}^{2} + \\sum_{k=1}^{d} \\lambda_{k} \\| X_{(k)} \\|_{*}.\n$$\n使用交替方向乘子法 (ADMM)，引入辅助变量 $Z_{k} \\in \\mathbb{R}^{N_{k} \\times \\prod_{j \\neq k} N_{j}}$ 和缩放对偶变量 $U_{k}$ 来强制约束 $Z_{k} = X_{(k)}$。从核范数、弗罗贝尼乌斯范数、模-$k$展开的基本定义以及凸优化中近端映射的性质出发，为一个惩罚参数 $\\rho  0$ 推导 $Z_{k}$、$\\mathcal{X}$ 和 $U_{k}$ 的 ADMM 更新式。您的推导必须明确使用弗罗贝尼乌斯范数在展开下的不变性，以及通过奇异值收缩对核范数近端算子的刻画，并从凸分析和正交不变性的第一性原理出发进行论证。\n\n假设一个计算成本模型，其中计算一个 $m \\times n$ 矩阵的薄奇异值分解 (SVD) 的成本恰好是 $\\gamma \\, \\min\\{m,n\\} \\, \\max\\{m,n\\}^{2}$ 次浮点乘法运算，其中 $\\gamma  0$ 是一个固定常数，并忽略所有非 $Z_{k}$ 更新中 SVD 产生的成本。令 $N_{-k} \\triangleq \\prod_{j \\neq k} N_{j}$ 表示 $X_{(k)}$ 第二个维度的大小。计算每次 ADMM 迭代中仅由更新变量 $Z_{k}$ 所需的 SVD 引起的浮点乘法运算总数，并用一个关于 $\\gamma$、 $d$ 和 $\\{N_{k}\\}_{k=1}^{d}$ 的单一闭式解析表达式表示。\n\n您的最终答案必须是一个单一的闭式解析表达式。最终答案中不要包含任何不等式、大$\\mathcal{O}$符号或文字解释。",
            "solution": "问题是为一个张量补全目标推导交替方向乘子法 (ADMM) 的更新式，然后计算每次迭代中奇异值分解 (SVD) 步骤的计算成本。\n\n首先，我们通过为张量 $\\mathcal{X}$ 的每个模-$k$展开 $X_{(k)}$ 引入辅助变量 $Z_{k}$ 来重新表述该优化问题。原始问题是\n$$\n\\min_{\\mathcal{X}} \\; \\frac{1}{2} \\left\\| \\mathcal{P}_{\\Omega}(\\mathcal{X} - \\mathcal{Y}) \\right\\|_{F}^{2} + \\sum_{k=1}^{d} \\lambda_{k} \\| X_{(k)} \\|_{*}\n$$\n其中 $\\mathcal{X}, \\mathcal{Y} \\in \\mathbb{R}^{N_{1} \\times \\cdots \\times N_{d}}$，$\\Omega$ 是观测索引集，$\\mathcal{P}_{\\Omega}$ 是到这些索引上的投影，$X_{(k)} \\in \\mathbb{R}^{N_k \\times \\prod_{j \\neq k} N_j}$ 是 $\\mathcal{X}$ 的模-$k$展开，$\\| \\cdot \\|_{*}$ 是核范数。常数 $\\lambda_k  0$ 是正则化参数。\n\n该问题被转化为一个约束优化问题：\n$$\n\\min_{\\mathcal{X}, \\{Z_k\\}_{k=1}^d} \\; \\frac{1}{2} \\left\\| \\mathcal{P}_{\\Omega}(\\mathcal{X} - \\mathcal{Y}) \\right\\|_{F}^{2} + \\sum_{k=1}^{d} \\lambda_{k} \\| Z_{k} \\|_{*} \\quad \\text{subject to} \\quad Z_{k} = X_{(k)} \\text{ for } k=1, \\ldots, d.\n$$\n此问题的缩放对偶形式的增广拉格朗日函数为\n$$\n\\mathcal{L}_{\\rho}(\\mathcal{X}, \\{Z_k\\}, \\{U_k\\}) = \\frac{1}{2} \\left\\| \\mathcal{P}_{\\Omega}(\\mathcal{X} - \\mathcal{Y}) \\right\\|_{F}^{2} + \\sum_{k=1}^{d} \\lambda_{k} \\| Z_{k} \\|_{*} + \\frac{\\rho}{2} \\sum_{k=1}^{d} \\left( \\| X_{(k)} - Z_{k} + U_{k} \\|_{F}^{2} - \\| U_{k} \\|_{F}^{2} \\right)\n$$\n其中 $U_{k}$ 是缩放对偶变量，$\\rho  0$ 是惩罚参数。ADMM 通过对每个变量块交替最小化 $\\mathcal{L}_{\\rho}$，然后更新对偶变量来进行。令迭代索引为 $t$。\n\n**1. $Z_{k}$ 的更新**\n\n我们相对于每个 $Z_{k}$ 最小化 $\\mathcal{L}_{\\rho}$，同时将其他变量固定在它们在第 $t$ 次迭代时的值：\n$$\nZ_{k}^{(t+1)} = \\arg\\min_{Z_{k}} \\left( \\lambda_{k} \\| Z_{k} \\|_{*} + \\frac{\\rho}{2} \\| X_{(k)}^{(t)} - Z_{k} + U_{k}^{(t)} \\|_{F}^{2} \\right)\n$$\n这可以重写为：\n$$\nZ_{k}^{(t+1)} = \\arg\\min_{Z_{k}} \\left( \\frac{\\lambda_k}{\\rho} \\| Z_{k} \\|_{*} + \\frac{1}{2} \\| Z_k - (X_{(k)}^{(t)} + U_{k}^{(t)}) \\|_{F}^{2} \\right)\n$$\n这是核范数的近端算子的定义，缩放因子为 $\\frac{\\lambda_k}{\\rho}$。\n$$\nZ_{k}^{(t+1)} = \\text{prox}_{\\frac{\\lambda_k}{\\rho} \\| \\cdot \\|_{*}} \\left( X_{(k)}^{(t)} + U_{k}^{(t)} \\right)\n$$\n核范数的近端算子是奇异值收缩算子。这可以从第一性原理证明如下。令 $A = X_{(k)}^{(t)} + U_{k}^{(t)}$。我们要求解 $\\min_{Z} \\frac{1}{2} \\|Z-A\\|_F^2 + \\tau \\|Z\\|_*$，其中 $\\tau = \\lambda_k/\\rho$。设 $A$ 的 SVD 为 $A = S \\Sigma_A V^T$。由于弗罗贝尼乌斯范数的正交不变性（$\\|S' \\Sigma_Z V'^T - S \\Sigma_A V^T\\|_F^2 = \\|\\Sigma_Z - S'^T S \\Sigma_A V^T V'\\|_F^2$）和 von Neumann 迹不等式，对于固定的 $Z$ 的奇异值集合，当 $Z$ 的奇异向量与 $A$ 的奇异向量相同时，$\\|Z-A\\|_F^2$ 达到最小值。因此，我们可以写出 $Z = S \\Sigma_Z V^T$。优化问题随后在奇异值 $\\sigma_i(Z)$ 和 $\\sigma_i(A)$ 上是可分的：\n$$\n\\min_{\\sigma_i(Z) \\ge 0} \\sum_i \\left( \\frac{1}{2} (\\sigma_i(Z) - \\sigma_i(A))^2 + \\tau \\sigma_i(Z) \\right)\n$$\n每个标量子问题的解是 $\\sigma_i(Z) = \\max(0, \\sigma_i(A) - \\tau)$，这是一种标量软阈值操作。因此，如果 $X_{(k)}^{(t)} + U_{k}^{(t)} = S_k \\Sigma_k V_k^T$ 是其 SVD，则更新式为：\n$$\nZ_{k}^{(t+1)} = S_k \\, (\\Sigma_k - \\frac{\\lambda_k}{\\rho} I)_+ \\, V_k^T\n$$\n其中 $(\\cdot)_+$ 表示取每个对角元素的正部。\n\n**2. $\\mathcal{X}$ 的更新**\n\n我们相对于 $\\mathcal{X}$ 最小化 $\\mathcal{L}_{\\rho}$，同时保持 $Z_k$ 和 $U_k$ 固定：\n$$\n\\mathcal{X}^{(t+1)} = \\arg\\min_{\\mathcal{X}} \\left( \\frac{1}{2} \\| \\mathcal{P}_{\\Omega}(\\mathcal{X} - \\mathcal{Y}) \\|_{F}^{2} + \\frac{\\rho}{2} \\sum_{k=1}^{d} \\| X_{(k)} - Z_{k}^{(t+1)} + U_{k}^{(t)} \\|_{F}^{2} \\right)\n$$\n弗罗贝尼乌斯范数的一个基本性质是它对展开的不变性：对于任何张量 $\\mathcal{A}$ 和模 $k$，都有 $\\| \\mathcal{A} \\|_F = \\| A_{(k)} \\|_F$。令 $\\mathcal{B}_k = \\text{fold}_k(Z_{k}^{(t+1)} - U_{k}^{(t)})$，其中 $\\text{fold}_k$ 是模-$k$展开算子的逆算子。目标函数变为：\n$$\n\\min_{\\mathcal{X}} \\left( \\frac{1}{2} \\| \\mathcal{P}_{\\Omega}(\\mathcal{X} - \\mathcal{Y}) \\|_{F}^{2} + \\frac{\\rho}{2} \\sum_{k=1}^{d} \\| \\mathcal{X} - \\mathcal{B}_k \\|_{F}^{2} \\right)\n$$\n这是一个关于 $\\mathcal{X}$ 的二次函数，其最小化子可以通过将关于 $\\mathcal{X}$ 的梯度设为零来找到。梯度为：\n$$\n\\nabla_{\\mathcal{X}} \\mathcal{L}_{\\rho} = \\mathcal{P}_{\\Omega}(\\mathcal{X} - \\mathcal{Y}) + \\rho \\sum_{k=1}^{d} (\\mathcal{X} - \\mathcal{B}_k) = 0\n$$\n令 $\\mathcal{A}^{(t+1)} = \\sum_{k=1}^{d} \\mathcal{B}_k = \\sum_{k=1}^{d} \\text{fold}_k(Z_{k}^{(t+1)} - U_{k}^{(t)})$。重新整理各项可得：\n$$\n\\mathcal{P}_{\\Omega}(\\mathcal{X}) + \\rho d \\mathcal{X} - \\rho \\mathcal{A}^{(t+1)} = \\mathcal{P}_{\\Omega}(\\mathcal{Y})\n$$\n这个方程可以对每个元素 $\\mathcal{X}_{\\mathbf{i}}$ 求解，其中 $\\mathbf{i} = (i_1, \\ldots, i_d)$：\n如果 $\\mathbf{i} \\in \\Omega$：\n$(\\mathcal{X}_{\\mathbf{i}} - \\mathcal{Y}_{\\mathbf{i}}) + \\rho d \\mathcal{X}_{\\mathbf{i}} - \\rho \\mathcal{A}_{\\mathbf{i}}^{(t+1)} = 0 \\implies (1 + \\rho d) \\mathcal{X}_{\\mathbf{i}} = \\mathcal{Y}_{\\mathbf{i}} + \\rho \\mathcal{A}_{\\mathbf{i}}^{(t+1)}$\n$$\n\\mathcal{X}_{\\mathbf{i}}^{(t+1)} = \\frac{1}{1 + \\rho d} \\left( \\mathcal{Y}_{\\mathbf{i}} + \\rho \\mathcal{A}_{\\mathbf{i}}^{(t+1)} \\right)\n$$\n如果 $\\mathbf{i} \\notin \\Omega$：\n$0 + \\rho d \\mathcal{X}_{\\mathbf{i}} - \\rho \\mathcal{A}_{\\mathbf{i}}^{(t+1)} = 0 \\implies d \\mathcal{X}_{\\mathbf{i}} = \\mathcal{A}_{\\mathbf{i}}^{(t+1)}$\n$$\n\\mathcal{X}_{\\mathbf{i}}^{(t+1)} = \\frac{1}{d} \\mathcal{A}_{\\mathbf{i}}^{(t+1)}\n$$\n\n**3. $U_{k}$ 的更新**\n\n对偶变量的更新是一个标准的梯度上升步骤，在缩放 ADMM 形式下为：\n$$\nU_{k}^{(t+1)} = U_{k}^{(t)} + X_{(k)}^{(t+1)} - Z_{k}^{(t+1)}\n$$\n\n**SVD 的计算成本**\n\n问题要求计算每次 ADMM 迭代中仅由 $Z_{k}$ 更新中的 SVD 引起的浮点乘法运算总数。\n计算一个 $m \\times n$ 矩阵的薄 SVD 的成本给定为 $\\gamma \\, \\min\\{m,n\\} \\, \\max\\{m,n\\}^{2}$，其中 $\\gamma  0$ 是一个常数。\n\n在每次迭代中，对每个 $k \\in \\{1, \\ldots, d\\}$ 计算一次 SVD。SVD 在矩阵 $X_{(k)}^{(t)} + U_{k}^{(t)}$ 上执行。该矩阵的维度与模-$k$展开 $X_{(k)}$ 相同，即 $N_k \\times (\\prod_{j \\neq k} N_j)$。我们将维度表示为 $m = N_k$ 和 $n = N_{-k} \\triangleq \\prod_{j \\neq k} N_j$。\n\n更新单个 $Z_k$ 中 SVD 的成本是：\n$$\n\\text{Cost}_k = \\gamma \\cdot \\min\\{N_k, N_{-k}\\} \\cdot \\max\\{N_k, N_{-k}\\}^{2}\n$$\n我们可以简化这个乘积项：\n$$\n\\min\\{a, b\\} \\cdot \\max\\{a, b\\}^{2} = (\\min\\{a, b\\} \\cdot \\max\\{a, b\\}) \\cdot \\max\\{a, b\\} = (a \\cdot b) \\cdot \\max\\{a, b\\}\n$$\n将此简化应用于 $a=N_k$ 和 $b=N_{-k}$，第 $k$ 次更新的成本变为：\n$$\n\\text{Cost}_k = \\gamma \\cdot (N_k \\cdot N_{-k}) \\cdot \\max\\{N_k, N_{-k}\\}\n$$\n维度的乘积是 $N_k \\cdot N_{-k} = N_k \\cdot \\prod_{j \\neq k} N_j = \\prod_{j=1}^{d} N_j$。对于所有的 $k$，这个乘积是常数。令 $P = \\prod_{j=1}^{d} N_j$。\n$$\n\\text{Cost}_k = \\gamma \\cdot P \\cdot \\max\\{N_k, N_{-k}\\}\n$$\n每次 ADMM 迭代的总成本是从 $1$ 到 $d$ 的每个 $k$ 的成本之和：\n$$\n\\text{Total Cost} = \\sum_{k=1}^{d} \\text{Cost}_k = \\sum_{k=1}^{d} \\left( \\gamma \\cdot P \\cdot \\max\\{N_k, N_{-k}\\} \\right)\n$$\n提出常数 $\\gamma$ 和 $P$：\n$$\n\\text{Total Cost} = \\gamma \\cdot P \\cdot \\sum_{k=1}^{d} \\max\\{N_k, N_{-k}\\}\n$$\n代回 $P$ 和 $N_{-k}$ 的定义，我们得到最终的闭式表达式：\n$$\n\\text{Total Cost} = \\gamma \\left( \\prod_{j=1}^{d} N_j \\right) \\sum_{k=1}^{d} \\max\\left\\{ N_k, \\prod_{j \\neq k} N_j \\right\\}\n$$\n该表达式表示每次 ADMM 迭代中可归因于所需 SVD 的浮点乘法运算总数。",
            "answer": "$$\n\\boxed{\\gamma \\left( \\prod_{j=1}^{d} N_{j} \\right) \\sum_{k=1}^{d} \\max\\left\\{ N_{k}, \\prod_{j \\neq k} N_{j} \\right\\}}\n$$"
        },
        {
            "introduction": "理论和算法最终需要在实践中得到检验。本编程练习要求您亲自设计“对抗性”采样模式，以实证的方式展示张量原生方法相对于朴素矩阵化方法的优势。通过编码实现并比较两种算法在不同采样条件下的表现，您将深刻理解为何利用张量的多模态结构对于处理非均匀和结构化缺失数据至关重要。",
            "id": "3485347",
            "problem": "您的任务是经验性地证明，在低多线性秩张量上精心设计的采样掩码可以在特定展开中违反矩阵非相干性，同时仍为基于张量的恢复提供足够的多模态覆盖，从而导致矩阵补全方法失败而张量补全模型成功。\n\n您必须从以下基本依据构建推导过程：\n\n- 从三阶张量、到观测条目上的投影以及通过核范数的凸松弛的定义开始。令一个三阶张量表示为 $X \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}$。令观测掩码为 $\\Omega \\subseteq \\{1,\\dots,n_1\\} \\times \\{1,\\dots,n_2\\} \\times \\{1,\\dots,n_3\\}$。令投影算子 $P_{\\Omega}$ 逐元素作用，若 $(i,j,k) \\in \\Omega$，则 $(P_{\\Omega}(X))_{i,j,k} = X_{i,j,k}$，否则为 $0$。\n\n- 使用广泛接受的凸松弛：\n  1. 对于固定展开上的矩阵补全，在观测条目的约束下最小化矩阵核范数：给定 $X$ 的一个展开 $M \\in \\mathbb{R}^{n \\times m}$，求解 $\\min_{Z} \\frac{1}{2}\\|P_{\\Omega}(Z - M)\\|_F^2 + \\lambda \\|Z\\|_*$，其中 $\\|\\cdot\\|_*$ 是矩阵核范数，$\\lambda  0$ 是一个正则化参数。\n  2. 对于张量补全，使用跨模态展开的核范数之和：$\\min_{X} \\alpha_1 \\|X_{(1)}\\|_* + \\alpha_2 \\|X_{(2)}\\|_* + \\alpha_3 \\|X_{(3)}\\|_*$，约束条件为 $P_{\\Omega}(X) = P_{\\Omega}(X^{\\star})$，其中 $X^{\\star}$ 是真实张量，$X_{(i)}$ 表示模-$i$展开。\n\n- 假设真实张量服从一个低多线性秩模型（Tucker模型）：$X^{\\star} = \\mathcal{G} \\times_1 U_1 \\times_2 U_2 \\times_3 U_3$，其中 $\\mathcal{G} \\in \\mathbb{R}^{r_1 \\times r_2 \\times r_3}$ 是核心张量，$U_i \\in \\mathbb{R}^{n_i \\times r_i}$ 的列是正交的，$\\times_i$ 表示模-$i$张量-矩阵乘积。这是一个在张量补全和针对低秩结构的稀疏优化中经过充分检验的模型。\n\n您的程序必须执行以下操作：\n\n1. 通过采样正交因子矩阵 $U_1,U_2,U_3$ 和一个小的随机核心张量 $\\mathcal{G}$（全部使用固定的随机种子），构建一个确定性的合成真实张量 $X^{\\star} \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}$，其维度为 $n_1 = 15, n_2 = 15, n_3 = 10$，低多线性秩为 $(r_1,r_2,r_3) = (3,3,3)$。不添加噪声。\n\n2. 设计四个采样掩码 $\\Omega$ 组成一个测试套件，确保科学真实性和清晰的覆盖属性：\n   - 案例A（对抗性多模态覆盖）：选择一个小集合 $S_1 \\subset \\{1,\\dots,n_1\\}$，其大小 $|S_1|=3$。对于 $i \\in S_1$，观测其模-1纤维上的所有条目（即，对于所有 $j,k$ 的 $(i,j,k)$）。对于 $i \\notin S_1$，为每个 $i$ 确定性地选择一个 $j = f(i)$，并观测其上一个完整的模-3纤维（即包含所有 $k$ 的 $(i,j,k)$）。此外，额外包含少量确定性散布的单个条目，以确保每个模-2和模-3索引都在多个 $i$ 上被覆盖。该掩码在模-1展开 $X^{\\star}_{(1)}$ 的行上具有高度不均匀的覆盖（违反了矩阵非相干性），但为张量恢复提供了跨模态的覆盖。\n   - 案例B（近似均匀采样）：使用固定的种子，以固定概率 $p = 0.2$ 独立包含每个条目，近似于均匀采样，并提供一个两种方法都应表现良好的基线。\n   - 案例C（极端对抗性）：令 $S_1 = \\{i_0\\}$，只有一个被完全观测的模-1纤维，此外在其他地方只有极小固定比例的剩余单点观测，这使得 $X^{\\star}_{(1)}$ 的许多行几乎完全未被观测。这会因信息不足而导致两种方法都失败。\n   - 案例D（温和对抗性多模态覆盖）：令 $|S_1|=5$，对于每个 $i \\notin S_1$，通过选择两个确定性索引 $j_1(i), j_2(i)$ 并包含所有 $k$ 和 $\\ell \\in \\{1,2\\}$ 的 $(i,j_{\\ell}(i),k)$，来观测两个完整的模-3纤维。这仍然违反了模-1展开的矩阵非相干性，但改善了跨模态覆盖，使得张量方法可以成功。\n\n3. 实现两种恢复算法：\n   - 在模-1展开上使用带有奇异值阈值处理的近端梯度算法（也称为Soft-Impute）进行矩阵补全。使用目标函数 $\\min_{Z} \\frac{1}{2}\\|P_{\\Omega}(Z - M)\\|_F^2 + \\lambda \\|Z\\|_*$，其中 $M = X^{\\star}_{(1)}$。使用固定的 $\\lambda = 1.0$ 和步长 $t = 1$，从 $Z_0 = 0$ 开始，迭代直至达到 $200$ 次迭代或相对变化低于 $10^{-6}$。收敛后，将解重折叠回一个张量 $\\widehat{X}^{\\text{mat}}$。\n   - 使用交替方向乘子法（ADMM）对跨模态的核范数之和进行张量补全，通常称为高精度低秩张量补全（HaLRTC）。构建问题\n     $$\\min_{X} \\sum_{i=1}^{3} \\alpha_i \\|X_{(i)}\\|_* \\quad \\text{约束条件为 } P_{\\Omega}(X) = P_{\\Omega}(X^{\\star}),$$\n     其中 $\\alpha_1 = \\alpha_2 = \\alpha_3 = \\frac{1}{3}$。引入辅助变量 $Y_i$ 来解耦核范数，并为 $i = 1,2,3$ 引入对偶变量 $\\Lambda_i$。使用 ADMM 迭代，参数为 $\\beta = 1.0$，奇异值阈值为 $\\tau_i = \\alpha_i / \\beta$，迭代次数为 $200$ 次或相对变化低于 $10^{-6}$。确保 $X$ 的更新通过投影 $P_{\\Omega}(X) = P_{\\Omega}(X^{\\star})$ 来精确地施加观测约束。\n\n4. 对于每个测试案例，计算两种方法在整个张量上的相对弗罗贝尼乌斯范数重构误差：\n   $$\\text{err}_{\\text{mat}} = \\frac{\\|\\widehat{X}^{\\text{mat}} - X^{\\star}\\|_F}{\\|X^{\\star}\\|_F}, \\quad \\text{err}_{\\text{ten}} = \\frac{\\|\\widehat{X}^{\\text{ten}} - X^{\\star}\\|_F}{\\|X^{\\star}\\|_F}.$$\n\n5. 对于每个案例，根据以下决策规则输出一个表示为整数 $1$ 或 $0$ 的布尔值：\n   - 如果 $\\text{err}_{\\text{mat}} > 0.08$ 且 $\\text{err}_{\\text{ten}}  0.04$（矩阵补全失败而张量补全成功），则输出 $1$。\n   - 否则输出 $0$。\n\n您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，\"[1,0,0,1]\"），按顺序对应于案例 A、B、C 和 D。\n\n不涉及物理单位。所有角度（如有）都应以弧度为单位，但此处未使用。\n\n测试套件规范：\n- 张量维度： $(n_1,n_2,n_3) = (15,15,10)$。\n- 多线性秩： $(r_1,r_2,r_3) = (3,3,3)$。\n- 随机种子： $42$ 用于所有随机抽取。\n- 矩阵补全参数： $\\lambda = 1.0$，步长 $t=1$，最大迭代次数 $200$，容差 $10^{-6}$。\n- 张量补全参数： $\\alpha_1=\\alpha_2=\\alpha_3 = 1/3$，$\\beta=1.0$，最大迭代次数 $200$，容差 $10^{-6}$。\n- 掩码：\n  - 案例 A: $|S_1|=3$ 个完全观测的模-1纤维；对于 $i \\notin S_1$，每个 $i$ 有一个由 $j = 1 + ((i-1) \\bmod n_2)$ 给出的完整模-3纤维；外加对所有 $i$ 在位置 $(i, 1 + ((i-1) \\bmod n_2), 1 + ((i-1) \\bmod n_3))$ 处的确定性单点包含。\n  - 案例 B: 使用种子 $42$ 进行独立伯努利采样，概率 $p=0.2$。\n  - 案例 C: $|S_1|=1$ 个完全观测的模-1纤维；外加对所有 $i$ 在位置 $(i, 1 + ((i-1) \\bmod n_2), 1 + ((2(i-1)) \\bmod n_3))$ 处的单点包含。\n  - 案例 D: $|S_1|=5$ 个完全观测的模-1纤维；对于 $i \\notin S_1$，每个 $i$ 有两个由 $j_1 = 1 + ((i-1) \\bmod n_2)$ 和 $j_2 = 1 + ((i-1)\\cdot 2 \\bmod n_2)$ 给出的完整模-3纤维。\n\n您的实现必须严格遵守最终输出格式，不得包含额外文本，并且在给定上述固定种子和参数的情况下必须是完全确定性的。",
            "solution": "该问题要求进行一次经验性演示，比较矩阵补全与张量补全在从低秩张量的一小部分条目中恢复该张量的功效。要测试的核心假设是，一个精心构造的采样掩码可以违反矩阵补全在张量展开上成功恢复所必需的非相干性假设，同时仍然为基于张量的方法提供足够的多模态信息以实现精确恢复。\n\n首先，我们建立数学框架。三阶张量是一个三维数字数组，表示为 $X \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}$。我们假设底层的真实张量 $X^{\\star}$ 具有低秩结构，特别是低多线性秩 $(r_1, r_2, r_3)$。这通过Tucker分解来形式化：\n$$\nX^{\\star} = \\mathcal{G} \\times_1 U_1 \\times_2 U_2 \\times_3 U_3\n$$\n其中 $\\mathcal{G} \\in \\mathbb{R}^{r_1 \\times r_2 \\times r_3}$ 是核心张量，$U_i \\in \\mathbb{R}^{n_i \\times r_i}$ 是具有正交列的因子矩阵，$\\times_i$ 是模-$i$张量-矩阵乘积。张量 $X$ 与矩阵 $U \\in \\mathbb{R}^{m \\times n_i}$ 的模-$i$乘积是一个大小为 $n_1 \\times \\dots \\times n_{i-1} \\times m \\times n_{i+1} \\times \\dots \\times n_3$ 的张量 $Y = X \\times_i U$，其条目为 $Y_{j_1 \\dots j_{i-1} k j_{i+1} \\dots j_3} = \\sum_{j_i=1}^{n_i} X_{j_1 \\dots j_i \\dots j_3} U_{k, j_i}$。\n\n恢复是从由掩码 $\\Omega \\subseteq \\{1, \\dots, n_1\\} \\times \\{1, \\dots, n_2\\} \\times \\{1, \\dots, n_3\\}$ 指定的条目子集中执行的。观测算子 $P_{\\Omega}$ 是一个逐元素定义的投影，即 $(P_{\\Omega}(X))_{i,j,k} = X_{i,j,k}$ 如果 $(i,j,k) \\in \\Omega$，否则为 $0$。\n\n该问题比较了两种恢复策略。\n\n第一种策略，矩阵补全，通过展开将张量视为一个大矩阵。$X$ 的模-1展开，记为 $X_{(1)}$，将张量重塑为一个大小为 $n_1 \\times (n_2 n_3)$ 的矩阵。然后，恢复问题被形式化为寻找一个低秩矩阵 $Z$，该矩阵近似于展开后的真实矩阵 $M = X^{\\star}_{(1)}$。优化问题是一个正则化的最小二乘目标：\n$$\n\\min_{Z \\in \\mathbb{R}^{n_1 \\times (n_2 n_3)}} \\frac{1}{2}\\|P_{\\Omega}(Z - M)\\|_F^2 + \\lambda \\|Z\\|_*\n$$\n其中 $\\|\\cdot\\|_*$ 是核范数（奇异值之和），它是矩阵秩的凸代理，而 $\\lambda  0$ 是一个正则化参数。投影算子 $P_{\\Omega}$ 根据原始张量掩码 $\\Omega$ 应用于矩阵 $Z$。此问题使用近端梯度算法（也称为Soft-Impute）求解。步长 $t=1$ 的迭代更新为：\n$$\nZ_{k+1} = S_{\\lambda}( P_{\\Omega}(M) + P_{\\Omega^c}(Z_k) )\n$$\n其中 $S_{\\lambda}(Y) = U \\text{diag}(\\max(\\sigma_i - \\lambda, 0))V^T$ 是 $Y=U\\Sigma V^T$ 的奇异值阈值算子，$P_{\\Omega^c}$ 投影到未观测的条目上。矩阵补全的成功通常依赖于采样掩码足够均匀，这一性质被称为非相干性。\n\n第二种策略，张量补全，直接利用多模态结构。“核范数之和”（Sum of Nuclear Norms, SNN）模型求解：\n$$\n\\min_{X \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}} \\sum_{i=1}^{3} \\alpha_i \\|X_{(i)}\\|_* \\quad \\text{约束条件为 } P_{\\Omega}(X) = P_{\\Omega}(X^{\\star})\n$$\n其中 $X_{(i)}$ 是张量 $X$ 的模-$i$展开，$\\alpha_i$ 是正常数权重。此模型同时促进所有模态的低秩性。我们使用交替方向乘子法（ADMM）来解决这个约束优化问题。我们引入辅助变量 $Y_1, Y_2, Y_3$ 和对偶变量 $\\Lambda_1, \\Lambda_2, \\Lambda_3$。ADMM迭代过程如下：\n1.  **$Y_i$-更新**：对每个模态 $i=1,2,3$，通过求解一个近端问题来更新辅助变量，这等同于对展开后的张量进行奇异值阈值处理：\n    $$ Y_{i, k+1} = \\text{fold}_i\\left( S_{\\alpha_i/\\beta}\\left( \\text{unfold}_i(X_k + \\Lambda_{i,k}/\\beta) \\right) \\right) $$\n2.  **$X$-更新**：通过平均辅助变量并施加数据保真度约束来更新主张量变量：\n    $$ X_{k+1} = P_{\\Omega}(X^{\\star}) + P_{\\Omega^c}\\left( \\frac{1}{3} \\sum_{i=1}^3 (Y_{i,k+1} - \\Lambda_{i,k}/\\beta) \\right) $$\n3.  **$\\Lambda_i$-更新**：更新对偶变量（缩放的拉格朗日乘子）：\n    $$ \\Lambda_{i,k+1} = \\Lambda_{i,k} + \\beta (X_{k+1} - Y_{i,k+1}) $$\n这里，$\\beta  0$ 是ADMM惩罚参数。即使采样模式在某一模态上高度不均匀，只要它在其他模态上提供足够的覆盖，该方法也能成功。\n\n实验构建如下：\n一个具有多线性秩 $(3,3,3)$ 的真实张量 $X^{\\star} \\in \\mathbb{R}^{15 \\times 15 \\times 10}$ 被确定性地生成。创建四个采样掩码以测试不同的恢复场景：\n-   **案例A（对抗性多模态）**：采样三个完整的模-1纤维，并为剩余的模-1索引采样稀疏、结构化的纤维。这种设计严重违反了模-1矩阵展开的非相干性条件，但保持了跨模态2和3的广泛覆盖。\n-   **案例B（近似均匀）**：标准随机采样，其中每个条目以概率 $p=0.2$ 被观测。两种方法预期都表现良好。\n-   **案例C（极端对抗性）**：仅采样一个完整的模-1纤维和一小部分非常稀疏的其他条目。总信息量不足以让任何一种方法成功。\n-   **案例D（温和对抗性多模态）**：与案例A类似，但具有更多被完全观测的模-1纤维，从而加强了多模态覆盖。\n\n对于每种情况，都运行两种恢复算法。重构质量通过相对弗罗贝尼乌斯范数误差 $\\text{err} = \\|\\widehat{X} - X^{\\star}\\|_F / \\|X^{\\star}\\|_F$ 来衡量。如果 $\\text{err}_{\\text{mat}} > 0.08$ 且 $\\text{err}_{\\text{ten}}  0.04$，则宣告张量方法成功而矩阵方法失败，这证明了在结构化非均匀采样下，基于张量的方法的优越性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef unfold(tensor, mode):\n    \"\"\"Unfolds a tensor into a matrix.\"\"\"\n    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1))\n\ndef fold(matrix, mode, shape):\n    \"\"\"Folds a matrix back into a tensor.\"\"\"\n    full_shape = list(shape)\n    mode_dim = full_shape.pop(mode)\n    full_shape.insert(0, mode_dim)\n    return np.moveaxis(np.reshape(matrix, full_shape), 0, mode)\n\ndef ttm(tensor, matrix, mode):\n    \"\"\"Tensor-times-matrix product (n-mode product).\"\"\"\n    shape = list(tensor.shape)\n    shape[mode] = matrix.shape[0]\n    unfolded_tensor = unfold(tensor, mode)\n    res = matrix @ unfolded_tensor\n    return fold(res, mode, tuple(shape))\n\ndef svt(matrix, tau):\n    \"\"\"Singular Value Thresholding operator.\"\"\"\n    U, s, Vh = np.linalg.svd(matrix, full_matrices=False)\n    s_thresh = np.maximum(s - tau, 0)\n    return (U * s_thresh) @ Vh\n\ndef generate_tensor(dims, ranks, seed):\n    \"\"\"Generates a low-rank tensor using Tucker decomposition.\"\"\"\n    np.random.seed(seed)\n    n1, n2, n3 = dims\n    r1, r2, r3 = ranks\n    \n    # Orthonormal factor matrices\n    U1, _ = np.linalg.qr(np.random.randn(n1, r1))\n    U2, _ = np.linalg.qr(np.random.randn(n2, r2))\n    U3, _ = np.linalg.qr(np.random.randn(n3, r3))\n    \n    # Core tensor\n    G = np.random.randn(r1, r2, r3)\n    \n    # Construct tensor\n    X = ttm(ttm(ttm(G, U1, 0), U2, 1), U3, 2)\n    return X\n\ndef generate_mask(case_id, dims, seed):\n    \"\"\"Generates a sampling mask for the given case.\"\"\"\n    n1, n2, n3 = dims\n    omega = np.zeros(dims, dtype=bool)\n\n    if case_id == 'A':\n        # Adversarial multimode coverage\n        s1_indices = [0, 1, 2]\n        for i in s1_indices:\n            omega[i, :, :] = True\n        for i in range(n1):\n            if i not in s1_indices:\n                j = (i - 1) % n2\n                omega[i, j, :] = True\n        # Add deterministic singletons\n        for i in range(n1):\n            j = (i - 1) % n2\n            k = (i - 1) % n3\n            omega[i, j, k] = True\n            \n    elif case_id == 'B':\n        # Near-uniform sampling\n        p = 0.2\n        np.random.seed(seed)\n        omega = np.random.rand(*dims)  p\n        \n    elif case_id == 'C':\n        # Extreme adversarial\n        omega[0, :, :] = True\n        for i in range(n1):\n            j = (i - 1) % n2\n            k = (2 * (i - 1)) % n3\n            omega[i, j, k] = True\n\n    elif case_id == 'D':\n        # Mild adversarial multimode coverage\n        s1_indices = [0, 1, 2, 3, 4]\n        for i in s1_indices:\n            omega[i, :, :] = True\n        for i in range(n1):\n            if i not in s1_indices:\n                j1 = (i - 1) % n2\n                j2 = ((i - 1) * 2) % n2\n                omega[i, j1, :] = True\n                omega[i, j2, :] = True\n    \n    return omega\n\ndef solve_matrix_completion(X_star, Omega, lambda_val, tol, max_iter):\n    \"\"\"Solves matrix completion via Soft-Impute on mode-1 unfolding.\"\"\"\n    dims = X_star.shape\n    M = unfold(X_star, 0)\n    Omega_mat = unfold(Omega, 0)\n    \n    Z = np.zeros_like(M)\n    M_obs = Omega_mat * M\n\n    for k in range(max_iter):\n        Z_prev = Z\n        \n        # Proximal gradient update (t=1)\n        Y = M_obs + (1 - Omega_mat) * Z\n        Z = svt(Y, lambda_val)\n        \n        # Convergence check\n        norm_prev = np.linalg.norm(Z_prev)\n        if norm_prev > 0:\n            rel_change = np.linalg.norm(Z - Z_prev) / norm_prev\n            if rel_change  tol:\n                break\n    \n    X_hat = fold(Z, 0, dims)\n    return X_hat\n\ndef solve_tensor_completion(X_star, Omega, alphas, beta, tol, max_iter):\n    \"\"\"Solves tensor completion via ADMM for Sum-of-Nuclear-Norms (HaLRTC).\"\"\"\n    dims = X_star.shape\n    X_obs = X_star * Omega\n    \n    # Initialization\n    X = X_obs.copy()\n    Y = [np.zeros(dims) for _ in range(3)]\n    L = [np.zeros(dims) for _ in range(3)] # Lambda (dual variables)\n    \n    taus = [alpha / beta for alpha in alphas]\n\n    for k in range(max_iter):\n        X_prev = X\n        \n        # Y-updates\n        for i in range(3):\n            unfolded_val = unfold(X + L[i] / beta, i)\n            svt_res = svt(unfolded_val, taus[i])\n            Y[i] = fold(svt_res, i, dims)\n            \n        # X-update\n        X_unconstrained = np.mean([Y[i] - L[i] / beta for i in range(3)], axis=0)\n        X = X_obs + (1 - Omega) * X_unconstrained\n\n        # L-updates\n        for i in range(3):\n            L[i] += beta * (X - Y[i])\n        \n        # Convergence check\n        norm_prev = np.linalg.norm(X_prev)\n        if norm_prev > 0:\n            rel_change = np.linalg.norm(X - X_prev) / norm_prev\n            if rel_change  tol:\n                break\n                \n    return X\n\ndef solve():\n    # --- Problem Parameters ---\n    dims = (15, 15, 10)\n    ranks = (3, 3, 3)\n    seed = 42\n    \n    # Matrix completion params\n    lambda_val = 1.0\n    mat_max_iter = 200\n    mat_tol = 1e-6\n\n    # Tensor completion params\n    alphas = [1/3.0, 1/3.0, 1/3.0]\n    beta = 1.0\n    ten_max_iter = 200\n    ten_tol = 1e-6\n\n    # --- Ground Truth Generation ---\n    X_star = generate_tensor(dims, ranks, seed)\n    norm_X_star = np.linalg.norm(X_star)\n    if norm_X_star == 0: norm_X_star = 1.0 # Avoid division by zero\n\n    # --- Main Loop for Test Cases ---\n    results = []\n    case_ids = ['A', 'B', 'C', 'D']\n\n    for case_id in case_ids:\n        # Generate mask\n        Omega = generate_mask(case_id, dims, seed)\n\n        # Run matrix completion\n        X_hat_mat = solve_matrix_completion(X_star, Omega, lambda_val, mat_tol, mat_max_iter)\n        err_mat = np.linalg.norm(X_hat_mat - X_star) / norm_X_star\n\n        # Run tensor completion\n        X_hat_ten = solve_tensor_completion(X_star, Omega, alphas, beta, ten_tol, ten_max_iter)\n        err_ten = np.linalg.norm(X_hat_ten - X_star) / norm_X_star\n\n        # Apply decision rule\n        decision = 1 if err_mat > 0.08 and err_ten  0.04 else 0\n        results.append(decision)\n    \n    # --- Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}