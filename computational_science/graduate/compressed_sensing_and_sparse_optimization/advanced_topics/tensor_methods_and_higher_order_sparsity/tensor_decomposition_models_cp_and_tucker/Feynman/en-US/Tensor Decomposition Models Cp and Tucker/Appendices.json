{
    "hands_on_practices": [
        {
            "introduction": "Understanding a model's fundamental properties is the first step toward using it effectively. For the Canonical Polyadic (CP) decomposition, a key property is its capacity, or the number of independent parameters it can represent. This exercise guides you through a first-principles calculation of the model's degrees of freedom, a concept essential for gauging model complexity and developing identifiability theory . By carefully accounting for the inherent scaling ambiguities, you will develop a deeper appreciation for the geometry of the CP model manifold.",
            "id": "3485661",
            "problem": "Consider a real third-order tensor $\\mathcal{X} \\in \\mathbb{R}^{I \\times J \\times K}$ that admits a Canonical Polyadic (CP) decomposition of rank $R$, written as\n$$\n\\mathcal{X} \\;=\\; \\sum_{r=1}^{R} \\mathbf{a}_{r} \\circ \\mathbf{b}_{r} \\circ \\mathbf{c}_{r},\n$$\nwhere $\\mathbf{a}_{r} \\in \\mathbb{R}^{I}$, $\\mathbf{b}_{r} \\in \\mathbb{R}^{J}$, $\\mathbf{c}_{r} \\in \\mathbb{R}^{K}$, and $\\circ$ denotes the vector outer product. The representation is known to be identifiable only up to component-wise rescalings and permutations: for each component $r$, one may rescale $(\\mathbf{a}_{r}, \\mathbf{b}_{r}, \\mathbf{c}_{r})$ while compensating across modes without changing $\\mathcal{X}$, and one may permute the $R$ components without changing $\\mathcal{X}$. Treat the model parameters as the entries of the factor matrices $\\mathbf{A} = [\\mathbf{a}_{1}\\ \\cdots\\ \\mathbf{a}_{R}] \\in \\mathbb{R}^{I \\times R}$, $\\mathbf{B} = [\\mathbf{b}_{1}\\ \\cdots\\ \\mathbf{b}_{R}] \\in \\mathbb{R}^{J \\times R}$, and $\\mathbf{C} = [\\mathbf{c}_{1}\\ \\cdots\\ \\mathbf{c}_{R}] \\in \\mathbb{R}^{K \\times R}$.\n\nStarting from first principles of parameter counting in smooth manifolds and the invariances implied by the model, compute the number of free (identifiable) real parameters of a generic rank-$R$ CP model on $\\mathbb{R}^{I \\times J \\times K}$ after quotienting out all continuous rescaling indeterminacies. Justify why any discrete permutation symmetry among the $R$ components does not change this parameter count. Provide your final answer as a closed-form analytic expression in terms of $I$, $J$, $K$, and $R$.",
            "solution": "The problem requires us to determine the number of free, identifiable parameters in a rank-$R$ Canonical Polyadic (CP) decomposition of a third-order tensor $\\mathcal{X} \\in \\mathbb{R}^{I \\times J \\times K}$. This number corresponds to the dimension of the manifold of generic rank-$R$ tensors. We approach this by first defining the space of all possible parameters, identifying the symmetries (indeterminacies) of the model, and then calculating the dimension of the quotient space that results from identifying all equivalent parameter sets.\n\nThe CP model is given by\n$$\n\\mathcal{X} = \\sum_{r=1}^{R} \\mathbf{a}_{r} \\circ \\mathbf{b}_{r} \\circ \\mathbf{c}_{r}\n$$\nThe parameters of this model are the entries of the factor matrices $\\mathbf{A} = [\\mathbf{a}_{1}\\ \\cdots\\ \\mathbf{a}_{R}] \\in \\mathbb{R}^{I \\times R}$, $\\mathbf{B} = [\\mathbf{b}_{1}\\ \\cdots\\ \\mathbf{b}_{R}] \\in \\mathbb{R}^{J \\times R}$, and $\\mathbf{C} = [\\mathbf{c}_{1}\\ \\cdots\\ \\mathbf{c}_{R}] \\in \\mathbb{R}^{K \\times R}$.\n\n1.  **The Parameter Space and its Dimension**\n\nThe set of all possible parameters $(\\mathbf{A}, \\mathbf{B}, \\mathbf{C})$ forms the parameter space. This space can be identified with the Cartesian product of the corresponding matrix spaces:\n$$\n\\mathcal{P} = \\mathbb{R}^{I \\times R} \\times \\mathbb{R}^{J \\times R} \\times \\mathbb{R}^{K \\times R}\n$$\nThis space is a smooth manifold, specifically a Euclidean space. Its dimension is the total number of scalar entries in the three matrices.\n$$\n\\dim(\\mathcal{P}) = \\operatorname{dim}(\\mathbb{R}^{I \\times R}) + \\operatorname{dim}(\\mathbb{R}^{J \\times R}) + \\operatorname{dim}(\\mathbb{R}^{K \\times R}) = IR + JR + KR\n$$\nThis can be factored as:\n$$\n\\dim(\\mathcal{P}) = R(I+J+K)\n$$\n\n2.  **Continuous Indeterminacies (Rescaling Symmetries)**\n\nThe CP decomposition is not unique because the individual rank-one components, $\\mathbf{a}_{r} \\circ \\mathbf{b}_{r} \\circ \\mathbf{c}_{r}$, are subject to scaling ambiguities. For any of the $R$ components, we can rescale the factor vectors without changing their outer product. Specifically, for any $r \\in \\{1, \\dots, R\\}$, the transformation\n$$\n(\\mathbf{a}_{r}, \\mathbf{b}_{r}, \\mathbf{c}_{r}) \\mapsto (\\alpha_{r}\\mathbf{a}_{r}, \\beta_{r}\\mathbf{b}_{r}, \\gamma_{r}\\mathbf{c}_{r})\n$$\nwhere $\\alpha_{r}, \\beta_{r}, \\gamma_{r}$ are arbitrary non-zero real scalars, results in the new outer product:\n$$\n(\\alpha_{r}\\mathbf{a}_{r}) \\circ (\\beta_{r}\\mathbf{b}_{r}) \\circ (\\gamma_{r}\\mathbf{c}_{r}) = (\\alpha_{r}\\beta_{r}\\gamma_{r}) (\\mathbf{a}_{r} \\circ \\mathbf{b}_{r} \\circ \\mathbf{c}_{r})\n$$\nTo leave the component $\\mathbf{a}_{r} \\circ \\mathbf{b}_{r} \\circ \\mathbf{c}_{r}$ unchanged, and thus leave the sum $\\mathcal{X}$ unchanged, the condition $\\alpha_{r}\\beta_{r}\\gamma_{r} = 1$ must hold.\n\nFor each component $r$, the set of such scaling triples $(\\alpha_{r}, \\beta_{r}, \\gamma_{r})$ forms a group under component-wise multiplication. Let this group be $G_r$:\n$$\nG_r = \\{(\\alpha_r, \\beta_r, \\gamma_r) \\in (\\mathbb{R}^*)^3 \\mid \\alpha_r \\beta_r \\gamma_r = 1\\}\n$$\nwhere $\\mathbb{R}^* = \\mathbb{R} \\setminus \\{0\\}$. This is a continuous Lie group. Its dimension can be determined by noting that two of the scalars (e.g., $\\alpha_r$ and $\\beta_r$) can be chosen freely from $\\mathbb{R}^*$, while the third is then determined ($\\gamma_r = (\\alpha_r \\beta_r)^{-1}$). Thus, for each component $r$, there are $2$ degrees of freedom in the scaling ambiguity.\n$$\n\\dim(G_r) = 2\n$$\nSince these scaling choices are independent for each of the $R$ components, the total group of continuous symmetries, $G_{cont}$, is the direct product of the individual groups:\n$$\nG_{cont} = G_1 \\times G_2 \\times \\cdots \\times G_R\n$$\nThe dimension of this total Lie group is the sum of the dimensions of its factors:\n$$\n\\dim(G_{cont}) = \\sum_{r=1}^{R} \\dim(G_r) = \\sum_{r=1}^{R} 2 = 2R\n$$\n\n3.  **Dimension of the Quotient Manifold**\n\nThe set of identifiable models corresponds to the orbits of the parameter space $\\mathcal{P}$ under the action of the group $G_{cont}$. For a generic parameter set (where no factor vectors are zero), this group action is free. The resulting space of orbits, the quotient space $\\mathcal{P}/G_{cont}$, is a smooth manifold whose dimension is given by the dimension of the original manifold minus the dimension of the Lie group.\nThe number of free parameters, $N_{free}$, is precisely this dimension.\n$$\nN_{free} = \\dim(\\mathcal{P}/G_{cont}) = \\dim(\\mathcal{P}) - \\dim(G_{cont})\n$$\nSubstituting the calculated dimensions, we get:\n$$\nN_{free} = R(I+J+K) - 2R = R(I+J+K-2)\n$$\nThis is the number of free parameters after accounting for all continuous rescaling indeterminacies.\n\n4.  **Effect of Discrete Permutation Symmetry**\n\nIn addition to continuous rescaling, the CP model is also invariant to permutations of the $R$ rank-one components. The sum $\\sum_{r=1}^{R} \\mathbf{a}_{r} \\circ \\mathbf{b}_{r} \\circ \\mathbf{c}_{r}$ is unchanged if we reorder the terms. This symmetry is described by the action of the symmetric group $S_R$ on the set of $R$ components.\nThe group $S_R$ is a finite, discrete group of order $R!$. As a Lie group, its dimension is $0$.\nThe number of free parameters is a measure of the local dimension of the space of models. When we take the quotient of a manifold $M$ by a finite group $G_{disc}$, the resulting object (an orbifold) has the same dimension as the original manifold $M$. This is because the quotient map is a local homeomorphism almost everywhere (specifically, at points that are not fixed points of any non-identity group element). The identification of a finite number of points in each orbit does not reduce the local degrees of freedom.\nIn our case, the space of models corrected for scaling, $\\mathcal{P}/G_{cont}$, is a manifold of dimension $R(I+J+K-2)$. The permutation symmetry group $S_R$ acts on this manifold. The final space of identifiable models is the orbit space $(\\mathcal{P}/G_{cont})/S_R$. The dimension of this space is given by:\n$$\n\\dim((\\mathcal{P}/G_{cont})/S_R) = \\dim(\\mathcal{P}/G_{cont}) - \\dim(S_R) = R(I+J+K-2) - 0\n$$\nThus, the discrete permutation symmetry does not change the count of free parameters. The number of identifiable parameters remains $R(I+J+K-2)$.\n\nFinal calculation:\nTotal parameters in $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$: $R(I+J+K)$.\nNumber of independent continuous scaling constraints to remove indeterminacy: $2$ for each of the $R$ components, totaling $2R$.\nNumber of free parameters = $R(I+J+K) - 2R$.",
            "answer": "$$\n\\boxed{R(I+J+K-2)}\n$$"
        },
        {
            "introduction": "While the CP decomposition suffers from inherent scaling and permutation ambiguities, its components can, under certain conditions, be uniquely recovered. Kruskal's theorem provides a powerful sufficient condition for this essential uniqueness, linking it to the linear independence properties of the factor matrices. This practice  moves from abstract theory to a concrete test, requiring you to compute the Kruskal rank for a given set of factor matrices. Mastering this calculation provides a practical tool for analyzing specific decompositions and solidifies the core concepts that ensure a model's components are well-defined.",
            "id": "3485712",
            "problem": "Consider an order-$3$ tensor $\\mathcal{X} \\in \\mathbb{R}^{I \\times J \\times K}$ that admits a Canonical Polyadic (CP) decomposition (also known as the CANDECOMP/PARAFAC decomposition) of rank $R$, with factor matrices $A \\in \\mathbb{R}^{3 \\times 4}$, $B \\in \\mathbb{R}^{4 \\times 4}$, and $C \\in \\mathbb{R}^{5 \\times 4}$. The columns of the factor matrices are given explicitly by\n$$\nA=\\begin{bmatrix}\n1 & 0 & 0 & 1\\\\\n0 & 1 & 0 & 1\\\\\n0 & 0 & 1 & 0\n\\end{bmatrix},\\quad\nB=\\begin{bmatrix}\n1 & 0 & 0 & 1\\\\\n0 & 1 & 0 & 1\\\\\n0 & 0 & 1 & 1\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix},\\quad\nC=\\begin{bmatrix}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 1\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}.\n$$\nLet $A=[a_{1}\\;a_{2}\\;a_{3}\\;a_{4}]$, $B=[b_{1}\\;b_{2}\\;b_{3}\\;b_{4}]$, and $C=[c_{1}\\;c_{2}\\;c_{3}\\;c_{4}]$ denote the column partitions. Using only the definitions of linear independence and the Kruskal rank (the maximum integer $k$ such that every subset of $k$ columns is linearly independent), compute $k_{A}$, $k_{B}$, and $k_{C}$. Then, from first principles, determine whether the CP decomposition is essentially unique under Kruskal’s condition for $R=4$ by analyzing the independence properties of the columns and the values of $k_{A}$, $k_{B}$, and $k_{C}$.\n\nYour final reported numerical answer must be the single integer equal to $k_{A}+k_{B}+k_{C}$. No rounding is required. Do not include any units.",
            "solution": "The problem requires the computation of the Kruskal ranks of three given factor matrices, $A$, $B$, and $C$, and then using these ranks to assess the essential uniqueness of the corresponding Canonical Polyadic (CP) decomposition via Kruskal's condition. The final answer is the sum of the Kruskal ranks.\n\nFirst, we recall the definition of the Kruskal rank of a matrix. The Kruskal rank of a matrix $M$, denoted as $k_M$, is the largest integer $k$ such that every set of $k$ columns of $M$ is linearly independent. The rank of the CP decomposition is given as $R=4$.\n\nLet us compute the Kruskal rank for each factor matrix.\n\n1.  **Kruskal rank of $A$ ($k_A$)**\n    The matrix $A \\in \\mathbb{R}^{3 \\times 4}$ is given by:\n    $$\n    A=\\begin{bmatrix}\n    1 & 0 & 0 & 1\\\\\n    0 & 1 & 0 & 1\\\\\n    0 & 0 & 1 & 0\n    \\end{bmatrix}\n    $$\n    The columns are $a_1 = [1, 0, 0]^T$, $a_2 = [0, 1, 0]^T$, $a_3 = [0, 0, 1]^T$, and $a_4 = [1, 1, 0]^T$. The rank of $A$ is at most $\\min(3, 4) = 3$. The columns $a_1, a_2, a_3$ are the standard basis vectors of $\\mathbb{R}^3$ and are thus linearly independent, which implies $\\text{rank}(A) = 3$. Therefore, $k_A \\le \\text{rank}(A) = 3$.\n\n    We check for linear independence of column subsets:\n    -   **For $k=1$**: All columns are non-zero vectors, so any set containing a single column is linearly independent. Thus, $k_A \\ge 1$.\n    -   **For $k=2$**: We must check all $\\binom{4}{2}=6$ pairs of columns.\n        -   $\\{a_1, a_2\\}$, $\\{a_1, a_3\\}$, $\\{a_2, a_3\\}$ are subsets of the standard basis and are linearly independent.\n        -   $\\{a_1, a_4\\}$: $\\alpha_1 a_1 + \\alpha_4 a_4 = 0 \\implies [\\alpha_1+\\alpha_4, \\alpha_4, 0]^T = [0,0,0]^T \\implies \\alpha_4=0, \\alpha_1=0$. Linearly independent.\n        -   $\\{a_2, a_4\\}$: $\\alpha_2 a_2 + \\alpha_4 a_4 = 0 \\implies [\\alpha_4, \\alpha_2+\\alpha_4, 0]^T = [0,0,0]^T \\implies \\alpha_4=0, \\alpha_2=0$. Linearly independent.\n        -   $\\{a_3, a_4\\}$: $\\alpha_3 a_3 + \\alpha_4 a_4 = 0 \\implies [\\alpha_4, \\alpha_4, \\alpha_3]^T = [0,0,0]^T \\implies \\alpha_4=0, \\alpha_3=0$. Linearly independent.\n        Since all pairs of columns are linearly independent, $k_A \\ge 2$.\n    -   **For $k=3$**: We must check all $\\binom{4}{3}=4$ triplets of columns.\n        -   Consider the set $\\{a_1, a_2, a_4\\}$. We check for linear dependence: $a_4 = [1, 1, 0]^T = 1 \\cdot [1, 0, 0]^T + 1 \\cdot [0, 1, 0]^T = a_1 + a_2$. Since $a_4 = a_1 + a_2$, this set of three columns is linearly dependent.\n    Since we found a subset of $3$ columns that is linearly dependent, the condition for $k=3$ is not met. The largest integer $k$ for which every subset of $k$ columns is linearly independent is $2$.\n    Therefore, the Kruskal rank of $A$ is $k_A = 2$.\n\n2.  **Kruskal rank of $B$ ($k_B$)**\n    The matrix $B \\in \\mathbb{R}^{4 \\times 4}$ is given by:\n    $$\n    B=\\begin{bmatrix}\n    1 & 0 & 0 & 1\\\\\n    0 & 1 & 0 & 1\\\\\n    0 & 0 & 1 & 1\\\\\n    0 & 0 & 0 & 0\n    \\end{bmatrix}\n    $$\n    Its columns are $b_1=[1,0,0,0]^T$, $b_2=[0,1,0,0]^T$, $b_3=[0,0,1,0]^T$, and $b_4=[1,1,1,0]^T$. The rank of $B$ is at most $4$. Ignoring the last zero row, we see that $b_1, b_2, b_3$ are linearly independent. We can also see that $b_4 = b_1 + b_2 + b_3$. Thus, $\\text{rank}(B)=3$, so $k_B \\le 3$.\n\n    -   **For $k=1, 2$**: All columns are non-zero and no column is a multiple of another. Thus, all sets of size $1$ and $2$ are linearly independent. $k_B \\ge 2$.\n    -   **For $k=3$**: We check all $\\binom{4}{3}=4$ triplets.\n        -   $\\{b_1, b_2, b_3\\}$: These are linearly independent as they are the first three standard basis vectors in $\\mathbb{R}^4$.\n        -   $\\{b_1, b_2, b_4\\}$: $\\alpha_1 b_1 + \\alpha_2 b_2 + \\alpha_4 b_4 = 0 \\implies [\\alpha_1+\\alpha_4, \\alpha_2+\\alpha_4, \\alpha_4, 0]^T = [0,0,0,0]^T$. This gives $\\alpha_4=0$, which implies $\\alpha_1 = 0$ and $\\alpha_2=0$. This set is linearly independent.\n        -   $\\{b_1, b_3, b_4\\}$: $\\alpha_1 b_1 + \\alpha_3 b_3 + \\alpha_4 b_4 = 0 \\implies [\\alpha_1+\\alpha_4, \\alpha_4, \\alpha_3+\\alpha_4, 0]^T = [0,0,0,0]^T$. This gives $\\alpha_4=0$, which implies $\\alpha_1=0$ and $\\alpha_3=0$. This set is linearly independent.\n        -   $\\{b_2, b_3, b_4\\}$: $\\alpha_2 b_2 + \\alpha_3 b_3 + \\alpha_4 b_4 = 0 \\implies [\\alpha_4, \\alpha_2+\\alpha_4, \\alpha_3+\\alpha_4, 0]^T = [0,0,0,0]^T$. This gives $\\alpha_4=0$, which implies $\\alpha_2=0$ and $\\alpha_3=0$. This set is linearly independent.\n        Since all subsets of size $3$ are linearly independent, $k_B \\ge 3$.\n    -   **For $k=4$**: The only subset of size $4$ is $\\{b_1, b_2, b_3, b_4\\}$. As noted earlier, $b_4 = b_1 + b_2 + b_3$. This set is linearly dependent.\n    The largest integer $k$ for which every subset of $k$ columns is linearly independent is $3$.\n    Therefore, the Kruskal rank of $B$ is $k_B = 3$.\n\n3.  **Kruskal rank of $C$ ($k_C$)**\n    The matrix $C \\in \\mathbb{R}^{5 \\times 4}$ is given by:\n    $$\n    C=\\begin{bmatrix}\n    1 & 0 & 0 & 0\\\\\n    0 & 1 & 0 & 0\\\\\n    0 & 0 & 1 & 0\\\\\n    0 & 0 & 0 & 1\\\\\n    0 & 0 & 0 & 0\n    \\end{bmatrix}\n    $$\n    Its columns are $c_1=[1,0,0,0,0]^T$, $c_2=[0,1,0,0,0]^T$, $c_3=[0,0,1,0,0]^T$, and $c_4=[0,0,0,1,0]^T$. These are the first four standard basis vectors in $\\mathbb{R}^5$. Any set of distinct standard basis vectors is linearly independent by definition. The number of columns is $4$. The largest possible subset of columns is the set of all four columns, which is linearly independent.\n    Therefore, the Kruskal rank of $C$ is equal to the number of its columns.\n    $k_C = 4$.\n\n4.  **Uniqueness Analysis via Kruskal's Condition**\n    Kruskal's theorem provides a sufficient condition for the essential uniqueness of the CP decomposition of an order-$3$ tensor. For a rank-$R$ decomposition with factor matrices $A$, $B$, and $C$, the decomposition is unique if:\n    $$k_A + k_B + k_C \\ge 2R + 2$$\n    In this problem, we have $R=4$, and we have computed the Kruskal ranks as $k_A=2$, $k_B=3$, and $k_C=4$.\n    We sum the Kruskal ranks:\n    $$k_A + k_B + k_C = 2 + 3 + 4 = 9$$\n    Next, we compute the term on the right-hand side of the inequality:\n    $$2R + 2 = 2(4) + 2 = 8 + 2 = 10$$\n    Comparing the two values, we have $9 \\ge 10$, which is false.\n    Since Kruskal's condition is not satisfied, we cannot conclude that the CP decomposition is essentially unique based on this theorem. Kruskal's theorem gives a sufficient, but not necessary, condition. The failure to meet the condition means uniqueness is not guaranteed by this criterion.\n\n5.  **Final Calculation**\n    The problem asks for the numerical value of the sum of the Kruskal ranks.\n    $$k_A + k_B + k_C = 2 + 3 + 4 = 9$$",
            "answer": "$$\n\\boxed{9}\n$$"
        },
        {
            "introduction": "In many scientific domains, underlying data components are known to be inherently non-negative. Imposing nonnegativity constraints on the CP model not only incorporates this prior knowledge but also improves model stability and interpretability by favoring an additive, parts-based representation. This problem  explores the important variant of Nonnegative CP (NCP) decomposition, examining how constraints mitigate ambiguities and how principled optimization algorithms are derived. By working through the logic of multiplicative updates and the Karush-Kuhn-Tucker (KKT) optimality conditions, you will gain crucial insights into the practical aspects of fitting constrained tensor models to real-world data.",
            "id": "3485663",
            "problem": "Consider a third-order data tensor $\\mathcal{X} \\in \\mathbb{R}_{+}^{I \\times J \\times K}$ and its Canonical Polyadic (CP) model $\\mathcal{X} \\approx \\sum_{r=1}^{R} a_{r} \\circ b_{r} \\circ c_{r}$ with factor matrices $A \\in \\mathbb{R}_{+}^{I \\times R}$, $B \\in \\mathbb{R}_{+}^{J \\times R}$, and $C \\in \\mathbb{R}_{+}^{K \\times R}$, where $\\circ$ denotes the outer product and the inequality denotes componentwise nonnegativity. The nonnegative least-squares objective is to minimize $f(A,B,C) = \\tfrac{1}{2}\\left\\|\\mathcal{X} - \\sum_{r=1}^{R} a_{r} \\circ b_{r} \\circ c_{r}\\right\\|_{F}^{2}$ subject to $A \\ge 0$, $B \\ge 0$, $C \\ge 0$. Let $X_{(n)}$ denote the mode-$n$ unfolding of $\\mathcal{X}$ and let $\\odot$ denote the Khatri–Rao product. Let $\\ast$ denote the Hadamard (elementwise) product and $\\oslash$ the elementwise division. Assume the usual CP matricization relations $X_{(1)} \\approx A (C \\odot B)^{\\top}$, $X_{(2)} \\approx B (C \\odot A)^{\\top}$, and $X_{(3)} \\approx C (B \\odot A)^{\\top}$.\n\nSelect all statements that are correct about how nonnegativity constraints mitigate ambiguities in CP decomposition and about principled multiplicative updates for minimizing $f(A,B,C)$ under nonnegativity:\n\nA. Imposing $A,B,C \\ge 0$ eliminates both permutation and scaling ambiguities and ensures uniqueness of the CP decomposition whenever $R \\le \\min\\{I,J,K\\}$.\n\nB. Imposing $A,B,C \\ge 0$ rules out sign flips and destructive cancellations, thereby shrinking the feasible equivalence class and mitigating degeneracy; however, permutation ambiguity and positive rescaling of rank-$1$ components remain unless additional normalization or regularization is enforced.\n\nC. With $M = C \\odot B$, the gradient of $g(A) = \\tfrac{1}{2}\\|X_{(1)} - A M^{\\top}\\|_{F}^{2}$ with respect to $A$ is $\\nabla_{A} g = X_{(1)} M - A (M^{\\top} M)$.\n\nD. A valid Lee–Seung–type multiplicative update that respects nonnegativity for the $A$-subproblem (holding $B,C$ fixed) is $A \\leftarrow A \\ast \\left( X_{(1)} (C \\odot B) \\right) \\oslash \\left( A \\left( (C \\odot B)^{\\top} (C \\odot B) \\right) \\right)$, with analogous updates for $B$ and $C$ using the corresponding unfoldings and Khatri–Rao products.\n\nE. Under nonnegativity constraints, first-order optimality for the $A$-subproblem requires $\\nabla_{A} g = 0$ componentwise at the solution, identical to the unconstrained least-squares case.\n\nF. For the $A$-subproblem with $M = C \\odot B$, the Karush–Kuhn–Tucker (KKT) conditions are $A \\ge 0$, $\\nabla_{A} g \\ge 0$, and $A \\ast \\nabla_{A} g = 0$ componentwise, where $\\nabla_{A} g = A (M^{\\top} M) - X_{(1)} M$.\n\nG. Using the identity $(C \\odot B)^{\\top}(C \\odot B) = (C^{\\top} C) \\ast (B^{\\top} B)$, the update in option D can be written equivalently as $A \\leftarrow A \\ast \\left( X_{(1)} (C \\odot B) \\right) \\oslash \\left( A \\left( (C^{\\top} C) \\ast (B^{\\top} B) \\right) \\right)$, and similarly for the other modes.\n\nChoose all that apply.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, and objective, representing a standard problem in the field of tensor decompositions.\n\nThe problem asks to evaluate statements concerning the nonnegative Canonical Polyadic (CP) decomposition of a third-order tensor $\\mathcal{X} \\in \\mathbb{R}_{+}^{I \\times J \\times K}$. The decomposition is approximated as $\\mathcal{X} \\approx \\sum_{r=1}^{R} a_{r} \\circ b_{r} \\circ c_{r}$, where the factor matrices $A = [a_1, \\dots, a_R] \\in \\mathbb{R}_{+}^{I \\times R}$, $B = [b_1, \\dots, b_R] \\in \\mathbb{R}_{+}^{J \\times R}$, and $C = [c_1, \\dots, c_R] \\in \\mathbb{R}_{+}^{K \\times R}$ are constrained to be componentwise nonnegative. The objective is to minimize the least-squares error $f(A,B,C) = \\tfrac{1}{2}\\left\\|\\mathcal{X} - \\llbracket A,B,C \\rrbracket \\right\\|_{F}^{2}$ subject to $A \\ge 0, B \\ge 0, C \\ge 0$. The matricized form of the approximation is given for mode-$n$ unfoldings, e.g., $X_{(1)} \\approx A (C \\odot B)^{\\top}$.\n\nWe will now evaluate each statement.\n\n**Analysis of Option A**\n\nThis statement claims that imposing nonnegativity ($A,B,C \\ge 0$) eliminates both permutation and scaling ambiguities and ensures uniqueness of the CP decomposition for a given rank $R$.\n\nThe CP decomposition is subject to two primary ambiguities:\n$1$. **Permutation ambiguity**: The order of the rank-$1$ components can be arbitrarily permuted. If $\\pi$ is a permutation of $\\{1, 2, \\ldots, R\\}$, then the model using permuted columns of factor matrices, $A_{\\pi}$, $B_{\\pi}$, $C_{\\pi}$ produces the exact same tensor:\n$$ \\sum_{r=1}^{R} a_{r} \\circ b_{r} \\circ c_{r} = \\sum_{r=1}^{R} a_{\\pi(r)} \\circ b_{\\pi(r)} \\circ c_{\\pi(r)} $$\nNonnegativity constraints do not affect this ambiguity, as permuting nonnegative columns results in nonnegative matrices.\n\n$2$. **Scaling ambiguity**: The vectors within a rank-$1$ component can be rescaled, as long as their product remains constant. For any set of scalars $\\lambda_r, \\mu_r, \\nu_r$ such that $\\lambda_r \\mu_r \\nu_r = 1$ for each $r=1, \\dots, R$:\n$$ \\sum_{r=1}^{R} a_{r} \\circ b_{r} \\circ c_{r} = \\sum_{r=1}^{R} (\\lambda_r a_{r}) \\circ (\\mu_r b_{r}) \\circ (\\nu_r c_{r}) $$\nThe nonnegativity constraint only requires that the rescaled vectors remain nonnegative. This restricts the scalars $\\lambda_r, \\mu_r, \\nu_r$ to be positive, but does not eliminate the ambiguity.\n\nSince permutation and positive scaling ambiguities remain, the CP decomposition is not unique even with nonnegativity constraints. The condition $R \\le \\min\\{I,J,K\\}$ is insufficient for uniqueness; stronger conditions like Kruskal's condition are required for essential uniqueness in the unconstrained case, and analogous (but not identical) conditions exist for the nonnegative case. Thus, the statement is false.\n\nVerdict: **Incorrect**.\n\n**Analysis of Option B**\n\nThis statement asserts that nonnegativity rules out sign flips and destructive cancellations, shrinking the equivalence class and mitigating degeneracy, but that permutation and positive rescaling ambiguities remain.\n\nThis is a correct description of the effect of nonnegativity. By forcing scaling factors to be positive, it eliminates sign flips (e.g., $(\\lambda_r, \\mu_r, \\nu_r) = (-1, -1, 1)$), which are a source of non-uniqueness. This also helps to prevent \"swamp\" or \"degeneracy\" issues, where two or more rank-$1$ components with large norms nearly cancel each other out, making the optimization landscape difficult. Nonnegativity enforces an additive, parts-based representation, which is often more interpretable and stable. As established in the analysis of option A, permutation ambiguity and positive scaling ambiguity persist. To resolve the latter, one typically enforces an additional constraint, such as normalizing the columns of one or more factor matrices (e.g., to unit norm).\n\nVerdict: **Correct**.\n\n**Analysis of Option C**\n\nThis statement gives an expression for the gradient of the $A$-subproblem's objective function, $g(A) = \\tfrac{1}{2}\\|X_{(1)} - A M^{\\top}\\|_{F}^{2}$, where $M = C \\odot B$. We compute the gradient $\\nabla_{A} g$.\nThe squared Frobenius norm is $\\|Z\\|_F^2 = \\text{tr}(Z^{\\top}Z)$.\n$$ g(A) = \\frac{1}{2} \\text{tr}\\left( \\left(X_{(1)} - A M^{\\top}\\right)^{\\top} \\left(X_{(1)} - A M^{\\top}\\right) \\right) $$\n$$ g(A) = \\frac{1}{2} \\text{tr}\\left( X_{(1)}^{\\top}X_{(1)} - X_{(1)}^{\\top} A M^{\\top} - M A^{\\top}X_{(1)} + M A^{\\top} A M^{\\top} \\right) $$\nUsing the cyclic property of the trace, $\\text{tr}(M A^{\\top}X_{(1)}) = \\text{tr}(X_{(1)}^{\\top} A M^{\\top})$.\n$$ g(A) = \\frac{1}{2} \\left[ \\text{tr}(X_{(1)}^{\\top}X_{(1)}) - 2\\text{tr}(X_{(1)}^{\\top} A M^{\\top}) + \\text{tr}(A^{\\top} A M^{\\top}M) \\right] $$\nUsing standard matrix calculus identities, $\\frac{\\partial}{\\partial A}\\text{tr}(X^{\\top}AY) = XY^{\\top}$ and $\\frac{\\partial}{\\partial A}\\text{tr}(A^{\\top}BA) = 2BA$, we find the gradient with respect to $A$:\n$$ \\nabla_{A} g = \\frac{1}{2} \\left[ 0 - 2X_{(1)}M + 2A(M^{\\top}M) \\right] = A (M^{\\top}M) - X_{(1)}M $$\nThe statement provides $\\nabla_{A} g = X_{(1)} M - A (M^{\\top} M)$, which is the negative of the correct gradient.\n\nVerdict: **Incorrect**.\n\n**Analysis of Option D**\n\nThis statement proposes a multiplicative update rule for the $A$-subproblem. The Alternating Least Squares (ALS) approach for this problem involves solving for one factor matrix while keeping the others fixed. For $A$, we solve the nonnegative least squares problem: $\\min_{A \\ge 0} g(A_k) = \\frac{1}{2} \\|X_{(1)} - A_k M^{\\top}\\|_{F}^2$, with $M=C \\odot B$.\nThe multiplicative update rules derived by Lee and Seung for Nonnegative Matrix Factorization (NMF) of $V \\approx WH$ are:\n$W \\leftarrow W \\ast (VH^\\top) \\oslash (WHH^\\top)$\n$H \\leftarrow H \\ast (W^\\top V) \\oslash (W^\\top W H)$\nMapping our problem to $V \\approx WH$, we have $V=X_{(1)}$, $W=A$, and $H=M^\\top$. We are updating $A$, which corresponds to $W$. The update for $W$ using $V=X_{(1)}$ and $H^\\top=M$ is:\n$$ A \\leftarrow A \\ast (X_{(1)} M) \\oslash (A M^{\\top}M) $$\nSubstituting $M = C \\odot B$:\n$$ A \\leftarrow A \\ast \\left( X_{(1)} (C \\odot B) \\right) \\oslash \\left( A \\left( (C \\odot B)^{\\top} (C \\odot B) \\right) \\right) $$\nThis matches the expression given in the option. This update respects the nonnegativity constraint ($A$ remains nonnegative if initialized as such, given that $X_{(1)}$, $B$, and $C$ are nonnegative).\n\nVerdict: **Correct**.\n\n**Analysis of Option E**\n\nThis statement claims that first-order optimality for the non-negatively constrained $A$-subproblem requires $\\nabla_{A} g = 0$ componentwise.\nThis is incorrect. The condition $\\nabla_{A} g = 0$ is the first-order optimality condition for an *unconstrained* optimization problem. Our subproblem is constrained: $\\min_{A \\ge 0} g(A)$. The proper optimality conditions are the Karush-Kuhn-Tucker (KKT) conditions. For a solution $A^*$, these conditions account for the boundary of the feasible set (where one or more elements of $A^*$ are zero). The gradient components corresponding to zero elements of $A^*$ do not need to be zero; they only need to be non-negative.\n\nVerdict: **Incorrect**.\n\n**Analysis of Option F**\n\nThis statement provides the Karush-Kuhn-Tucker (KKT) conditions for the $A$-subproblem, with $M = C \\odot B$. The problem is $\\min_{A} g(A)$ subject to the constraint $A \\ge 0$ (i.e., $-A_{ir} \\le 0$ for all $i,r$).\nThe KKT conditions for a solution $A^*$ are:\n$1$. Primal feasibility: $A^* \\ge 0$.\n$2$. Dual feasibility: The gradient of the objective must be non-negative. $\\nabla_{A} g(A^*) \\ge 0$. This arises from the stationarity condition $\\nabla_A g(A^*) - \\Lambda = 0$ and the non-negativity of the Lagrange multipliers $\\Lambda \\ge 0$.\n$3$. Complementary slackness: The product of the primal variables and the corresponding gradient components must be zero. This is written elementwise as $A_{ir}^* (\\nabla_{A} g(A^*))_{ir} = 0$ for all $i,r$, or in matrix form as $A^* \\ast \\nabla_{A} g(A^*) = 0$.\n\nThe option correctly states these three conditions: $A \\ge 0$, $\\nabla_{A} g \\ge 0$, and $A \\ast \\nabla_{A} g = 0$.\nIt also provides the correct expression for the gradient, $\\nabla_{A} g = A (M^{\\top} M) - X_{(1)} M$, as derived in our analysis of option C.\nTherefore, this statement is a correct and complete description of the first-order optimality conditions for the nonnegative subproblem.\n\nVerdict: **Correct**.\n\n**Analysis of Option G**\n\nThis option claims that $(C \\odot B)^{\\top}(C \\odot B) = (C^{\\top} C) \\ast (B^{\\top} B)$ and uses this identity to rewrite the update from option D.\nLet's verify the identity. The Khatri-Rao product $M = C \\odot B$ is defined by its columns, where the $r$-th column is the Kronecker product of the $r$-th columns of $C$ and $B$: $m_r = c_r \\otimes b_r$.\nThe $(r,s)$-th element of $M^{\\top}M$ is the inner product of the $r$-th and $s$-th columns of $M$:\n$$ (M^{\\top}M)_{rs} = m_r^{\\top} m_s = (c_r \\otimes b_r)^{\\top} (c_s \\otimes b_s) $$\nUsing the property of the Kronecker product $(u \\otimes v)^{\\top}(w \\otimes z) = (u^{\\top}w)(v^{\\top}z)$, we get:\n$$ (M^{\\top}M)_{rs} = (c_r^{\\top}c_s)(b_r^{\\top}b_s) $$\nNow consider the right-hand side of the identity, $(C^{\\top} C) \\ast (B^{\\top} B)$.\nThe $(r,s)$-th element of $C^{\\top}C$ is $c_r^{\\top}c_s$.\nThe $(r,s)$-th element of $B^{\\top}B$ is $b_r^{\\top}b_s$.\nThe Hadamard product $\\ast$ is the elementwise product, so the $(r,s)$-th element of the result is:\n$$ ((C^{\\top} C) \\ast (B^{\\top} B))_{rs} = (C^{\\top}C)_{rs} (B^{\\top}B)_{rs} = (c_r^{\\top}c_s) (b_r^{\\top}b_s) $$\nThe identity is correct.\nThe update in option D is:\n$$ A \\leftarrow A \\ast \\left( X_{(1)} (C \\odot B) \\right) \\oslash \\left( A \\left( (C \\odot B)^{\\top} (C \\odot B) \\right) \\right) $$\nSubstituting the proven identity into the denominator yields:\n$$ A \\leftarrow A \\ast \\left( X_{(1)} (C \\odot B) \\right) \\oslash \\left( A \\left( (C^{\\top} C) \\ast (B^{\\top} B) \\right) \\right) $$\nThis matches the expression in option G. This reformulation is computationally efficient as it avoids forming the potentially very large matrix $C \\odot B \\in \\mathbb{R}^{KJ \\times R}$ and instead works with smaller Gramian matrices $C^{\\top}C, B^{\\top}B \\in \\mathbb{R}^{R \\times R}$.\n\nVerdict: **Correct**.\n\nIn summary, the correct statements are B, D, F, and G.",
            "answer": "$$\\boxed{BDFG}$$"
        }
    ]
}