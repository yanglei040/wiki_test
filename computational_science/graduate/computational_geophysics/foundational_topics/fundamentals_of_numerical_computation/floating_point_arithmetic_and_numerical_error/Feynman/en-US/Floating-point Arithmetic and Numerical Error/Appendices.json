{
    "hands_on_practices": [
        {
            "introduction": "The summation of a long sequence of numbers is one of the most fundamental operations in scientific computing, yet it is fraught with numerical peril. In this exercise , you will explore how naive summation can lead to significant errors when calculating total wavefield energy, a common task in seismology. By implementing and comparing naive summation against compensated methods like the Kahan and Neumaier algorithms, you will gain a first-hand appreciation for how round-off errors accumulate and how they can be effectively mitigated.",
            "id": "3596759",
            "problem": "You are given a computational task grounded in floating-point arithmetic and numerical error within computational geophysics. Consider a discrete wavefield amplitude sequence $\\{A_i\\}_{i=1}^N$ and define the energy sequence $\\{E_i\\}_{i=1}^N$ by $E_i = A_i^2$, with $E_i \\ge 0$. In finite precision arithmetic conforming to Institute of Electrical and Electronics Engineers (IEEE) 754 double precision, each basic operation is subject to rounding to the nearest representable floating-point number. Let the unit roundoff $u$ denote the maximum relative error of a single rounding, where $u = 2^{-53}$ for double precision.\n\nYour task is to implement and compare three summation schemes applied to $\\sum_{i=1}^N E_i$:\n- Naive sequential summation in double precision.\n- Kahan compensated summation in double precision.\n- Neumaier compensated summation in double precision.\n\nFor each scheme, define the summation error for a realized sequence as $\\varepsilon = S_{\\text{scheme}} - S_{\\text{ref}}$, where $S_{\\text{ref}}$ is a high-accuracy reference sum computed in software-emulated high-precision for the same finite-precision inputs, and $S_{\\text{scheme}}$ is the double-precision sum produced by the specified scheme. Over multiple independent realizations with different random seeds, compute the sample bias and sample variance of $\\varepsilon$. The bias is the sample mean of $\\varepsilon$, and the variance is the sample mean of $(\\varepsilon - \\text{bias})^2$.\n\nThe goal is to quantify how both bias and variance of the summation error depend on the distribution of the magnitudes of $E_i$ and on the order in which terms are summed. You must use three scientifically plausible energy magnitude distributions relevant to wavefield energies:\n1. Gaussian amplitude model: $A_i \\sim \\mathcal{N}(0,\\sigma^2)$ with $\\sigma = 1$, so that $E_i = A_i^2$ has a chi-square-like distribution of energy magnitudes.\n2. Log-normal energy model: $E_i \\sim \\operatorname{LogNormal}(\\mu,\\sigma)$ with $(\\mu,\\sigma) = (-2,2)$ to model heavy-tailed energy magnitudes typical in heterogeneous media.\n3. Bimodal energy mixture: $E_i$ takes $E_{\\text{small}}$ with probability $p$ and $E_{\\text{large}}$ with probability $1-p$, with $(E_{\\text{small}}, E_{\\text{large}}, p) = (10^{-12}, 1, 0.999)$, to create extreme scale separation typical when small scattered energy is overwhelmed by a few strong coherent components.\n\nIn addition to the distribution, for the bimodal case you must test sensitivity to order by summing in two deterministic orders: all small terms first followed by large terms, and all large terms first followed by small terms. For the Gaussian amplitude and log-normal cases, sum in the natural generation order.\n\nYou must implement the following:\n- A robust random number generator for reproducibility via fixed seeds.\n- Deterministic implementations of naive, Kahan, and Neumaier summation, all in double precision arithmetic.\n- A high-accuracy reference sum computed from the same double-precision inputs using a software algorithm that significantly reduces rounding error compared to naive double-precision summation.\n\nUse a test suite with the following parameter sets:\n- Case 1 (Gaussian amplitude energies): $N = 500000$, seeds $\\{777, 888\\}$, distribution as in item 1.\n- Case 2 (Log-normal energies): $N = 500000$, seeds $\\{777, 888\\}$, distribution as in item 2.\n- Case 3 (Bimodal, small-first order): $N = 500000$, seeds $\\{777, 888\\}$, distribution as in item 3, deterministic order with all $E_{\\text{small}}$ first followed by all $E_{\\text{large}}$.\n- Case 4 (Bimodal, large-first order): $N = 500000$, seeds $\\{777, 888\\}$, distribution as in item 3, deterministic order with all $E_{\\text{large}}$ first followed by all $E_{\\text{small}}$.\n\nFor each case, your program must:\n- Generate the energy sequence $\\{E_i\\}$ in arbitrary units (a.u.).\n- Compute $S_{\\text{ref}}$ using a high-accuracy summation algorithm applied to the realized floating-point sequence.\n- Compute $S_{\\text{naive}}$, $S_{\\text{Kahan}}$, $S_{\\text{Neumaier}}$.\n- Compute the summation errors $\\varepsilon_{\\text{naive}}$, $\\varepsilon_{\\text{Kahan}}$, $\\varepsilon_{\\text{Neumaier}}$ for each seed.\n- Compute the sample bias and sample variance across the two seeds for each scheme.\n\nExpress all outputs in arbitrary units (a.u.) without any unit symbol and without rounding; the outputs are pure floating-point numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case contributes a sub-list of six floats in the order $[\\text{bias}_{\\text{naive}}, \\text{var}_{\\text{naive}}, \\text{bias}_{\\text{Kahan}}, \\text{var}_{\\text{Kahan}}, \\text{bias}_{\\text{Neumaier}}, \\text{var}_{\\text{Neumaier}}]$. The final output must therefore be a list of four sub-lists, one per case, for example:\n\"[[b1,v1,b2,v2,b3,v3],[b1,v1,b2,v2,b3,v3],[b1,v1,b2,v2,b3,v3],[b1,v1,b2,v2,b3,v3]]\".\n\nAll mathematical symbols that appear in this problem statement are expressed in LaTeX. No angles are involved. No percentages are involved.\n\nYour program must be a complete, runnable program that requires no user input and prints only the final single line of output in the specified format.",
            "solution": "The problem presents a valid and well-posed computational task in the domain of numerical analysis as applied to computational geophysics. It requires the implementation and comparison of three distinct summation algorithms—naive sequential sum, Kahan compensated summation, and Neumaier compensated summation—to quantify their accuracy when summing a sequence of floating-point numbers representing wavefield energy. The problem is scientifically grounded, providing plausible models for energy distributions and a clear methodology for assessing numerical error through statistical measures of bias and variance. All necessary parameters and conditions are specified, allowing for a unique and verifiable solution.\n\nThe core of the problem lies in understanding the accumulation of rounding errors in floating-point arithmetic. When summing a sequence of numbers $\\{x_i\\}_{i=1}^N$, the naive approach, $S_k = S_{k-1} + x_k$, can suffer from significant error accumulation, especially if small numbers are added to a running sum of large magnitude. This is because the low-order bits of the small number are lost during the floating-point addition.\n\nThe summation schemes are defined as follows:\n\n1.  **Naive Summation**: This is the most straightforward method. The sum $S$ is initialized to $0$ and each element $x_i$ of the sequence is added to it sequentially.\n    $$ S \\leftarrow S + x_i $$\n    In IEEE 754 double precision, a floating-point addition $a+b$ is computed as $\\text{fl}(a+b) = (a+b)(1+\\delta)$, where $|\\delta| \\le u$ and $u = 2^{-53}$ is the unit roundoff. When summing $N$ positive numbers, the forward error for naive summation can be bounded, but in the worst case, the relative error can grow proportionally to $N$.\n\n2.  **Kahan Summation**: This algorithm introduces a compensation variable, $c$, to accumulate the error lost in each addition.\n    Let $S$ be the running sum and $c$ be the running compensation, both initialized to $0$. For each element $x_i$:\n    \\begin{enumerate}\n        \\item $y \\leftarrow x_i - c$: Subtract the previous error from the current term.\n        \\item $T \\leftarrow S + y$: Add the corrected term to the sum.\n        \\item $c \\leftarrow (T - S) - y$: The new error is the part of $y$ that was lost when adding it to $S$. The term $(T - S)$ algebraically recovers the high-order part of $y$ that was successfully added to $S$. Subtracting $y$ from this isolates the negative of the low-order part of $y$ that was truncated.\n        \\item $S \\leftarrow T$: Update the sum.\n    \\end{enumerate}\n    The final sum is $S$. Kahan summation dramatically reduces the error growth, making the final error bound independent of $N$ and dependent only on the error in the input values themselves, plus a small constant factor times the sum of their magnitudes.\n\n3.  **Neumaier Summation**: This is a refinement of Kahan's algorithm that provides greater accuracy in cases where the magnitude of the next term to be added, $|x_i|$, is much larger than the magnitude of the running sum, $|S|$. In such scenarios, the Kahan step $y \\leftarrow x_i - c$ can lose precision if $|x_i| \\gg |c|$. Neumaier's algorithm avoids this.\n    Let $S$ be the running sum and $c$ be the running compensation, both initialized to $0$. For each element $x_i$:\n    \\begin{enumerate}\n        \\item $T \\leftarrow S + x_i$: Add the next term to the sum, potentially losing precision.\n        \\item If $|S| \\ge |x_i|$: $c \\leftarrow c + ((S - T) + x_i)$.\n        \\item Else: $c \\leftarrow c + ((x_i - T) + S)$.\n        \\item $S \\leftarrow T$: Update the sum.\n    \\end{enumerate}\n    The final sum is $S+c$. By checking the magnitudes of $S$ and $x_i$, Neumaier ensures that the subtraction to find the compensatory term is always done between the larger-magnitude number and the intermediate sum, preserving the most precision.\n\nThe problem requires testing these algorithms on three distributions of energy values $E_i = A_i^2 \\ge 0$.\n-   **Gaussian amplitude model**: $A_i \\sim \\mathcal{N}(0, 1^2)$ results in $E_i = A_i^2$ following a chi-squared distribution with one degree of freedom, $\\chi^2(1)$. This represents a mix of small and moderate energy values.\n-   **Log-normal energy model**: $E_i \\sim \\operatorname{LogNormal}(-2, 2)$ creates a heavy-tailed distribution with a few very large values and many small ones. This is a challenging case for naive summation.\n-   **Bimodal energy mixture**: A vast majority ($99.9\\%$) of terms are tiny ($10^{-12}$), and a few are large ($1$). This case is designed to expose a critical weakness of naive summation: summing small numbers after large ones leads to their contribution being completely lost (swamped). The two specified orderings, small-first and large-first, will highlight this dependency.\n\nFor reference, a high-accuracy sum $S_{\\text{ref}}$ is computed. An appropriate\nmethod for this is to use an algorithm that maintains precision, such as the one implemented in Python's `math.fsum`, which is based on an error-free transformation approach. The error for each scheme is $\\varepsilon = S_{\\text{scheme}} - S_{\\text{ref}}$. We analyze the sample bias, $\\mathbb{E}[\\varepsilon]$, and sample variance, $\\text{Var}[\\varepsilon]$, over two independent realizations (from seeds $777$ and $888$) to assess the statistical properties of the error. The bias measures the systematic error, while the variance measures the random component of the error.\n\nThe implementation will proceed by first setting up the random number generators, then defining Python functions for each of the three summation schemes. For each test case, we will generate the data for each seed, perform the summations, and compute the errors against the reference `math.fsum` result. Finally, the bias and variance for each scheme are calculated from the errors obtained across the two seeds. The final output will be a formatted list containing these six statistical measures for each of the four test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef naive_sum(arr: np.ndarray) -> np.float64:\n    \"\"\"Computes the sum using a standard naive sequential summation.\"\"\"\n    # Using np.sum with a specified dtype is a fast and standard\n    # way to perform naive sequential summation in double precision.\n    return np.sum(arr, dtype=np.float64)\n\ndef kahan_sum(arr: np.ndarray) -> np.float64:\n    \"\"\"Computes the sum using Kahan's compensated summation algorithm.\"\"\"\n    s = np.float64(0.0)\n    c = np.float64(0.0)  # A running compensation for the lost low-order bits.\n    for x in arr:\n        y = x - c\n        t = s + y\n        c = (t - s) - y\n        s = t\n    return s\n\ndef neumaier_sum(arr: np.ndarray) -> np.float64:\n    \"\"\"Computes the sum using Neumaier's improved summation algorithm.\"\"\"\n    s = np.float64(0.0)\n    c = np.float64(0.0)  # A running compensation.\n    for x in arr:\n        t = s + x\n        if abs(s) >= abs(x):\n            # If s is bigger, c collects the error of x.\n            c += (s - t) + x\n        else:\n            # If x is bigger, c collects the error of s.\n            c += (x - t) + s\n        s = t\n    return s + c\n\ndef solve():\n    \"\"\"Main function to run the simulation and produce the output.\"\"\"\n    test_cases = [\n        {\n            \"name\": \"Gaussian amplitude\",\n            \"N\": 500000,\n            \"seeds\": [777, 888],\n            \"distribution\": \"gaussian_amplitude\",\n            \"params\": {\"sigma\": 1.0},\n            \"order\": \"natural\",\n        },\n        {\n            \"name\": \"Log-normal\",\n            \"N\": 500000,\n            \"seeds\": [777, 888],\n            \"distribution\": \"log_normal\",\n            \"params\": {\"mu\": -2.0, \"sigma\": 2.0},\n            \"order\": \"natural\",\n        },\n        {\n            \"name\": \"Bimodal, small-first\",\n            \"N\": 500000,\n            \"seeds\": [777, 888],\n            \"distribution\": \"bimodal\",\n            \"params\": {\"E_small\": 1e-12, \"E_large\": 1.0, \"p\": 0.999},\n            \"order\": \"small_first\",\n        },\n        {\n            \"name\": \"Bimodal, large-first\",\n            \"N\": 500000,\n            \"seeds\": [777, 888],\n            \"distribution\": \"bimodal\",\n            \"params\": {\"E_small\": 1e-12, \"E_large\": 1.0, \"p\": 0.999},\n            \"order\": \"large_first\",\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        N = case[\"N\"]\n        seeds = case[\"seeds\"]\n        dist = case[\"distribution\"]\n        params = case[\"params\"]\n        order = case[\"order\"]\n\n        errors_naive = []\n        errors_kahan = []\n        errors_neumaier = []\n\n        for seed in seeds:\n            rng = np.random.default_rng(seed)\n            E = None\n\n            if dist == \"gaussian_amplitude\":\n                A = rng.normal(loc=0.0, scale=params[\"sigma\"], size=N)\n                E = np.square(A, dtype=np.float64)\n            elif dist == \"log_normal\":\n                E = rng.lognormal(mean=params[\"mu\"], sigma=params[\"sigma\"], size=N).astype(np.float64)\n            elif dist == \"bimodal\":\n                p = params[\"p\"]\n                E_small = np.float64(params[\"E_small\"])\n                E_large = np.float64(params[\"E_large\"])\n                \n                # Generate a boolean mask by comparing uniform random numbers to p\n                is_small = rng.random(size=N) < p\n                E = np.where(is_small, E_small, E_large)\n\n            if order == \"small_first\":\n                E = np.sort(E)\n            elif order == \"large_first\":\n                E = np.sort(E)[::-1]\n\n            # Ensure all values are float64 for consistent input\n            E = E.astype(np.float64)\n\n            # High-accuracy reference sum\n            s_ref = math.fsum(E)\n\n            # Compute sums for each scheme\n            s_naive = naive_sum(E)\n            s_kahan = kahan_sum(E)\n            s_neumaier = neumaier_sum(E)\n\n            # Compute and store errors\n            errors_naive.append(s_naive - s_ref)\n            errors_kahan.append(s_kahan - s_ref)\n            errors_neumaier.append(s_neumaier - s_ref)\n\n        # Compute sample bias and variance\n        bias_naive = np.mean(errors_naive)\n        var_naive = np.var(errors_naive, ddof=0)\n        \n        bias_kahan = np.mean(errors_kahan)\n        var_kahan = np.var(errors_kahan, ddof=0)\n\n        bias_neumaier = np.mean(errors_neumaier)\n        var_neumaier = np.var(errors_neumaier, ddof=0)\n\n        case_results = [\n            bias_naive, var_naive,\n            bias_kahan, var_kahan,\n            bias_neumaier, var_neumaier\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string as per requirements\n    # e.g., \"[[r1,r2,...],[r1,r2,...]]\"\n    output_str = \"[\" + \",\".join(\n        \"[\" + \",\".join(f\"{val:.17g}\" for val in res) + \"]\" for res in all_results\n    ) + \"]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "One of the most notorious sources of numerical error is catastrophic cancellation, which occurs when subtracting two nearly-equal large numbers. This practice problem  places you in the context of seismic tomography, where data residuals are computed as the small difference between large observed and predicted travel times. You will analytically derive the error bounds for two different computational formulations, demonstrating quantitatively why algebraically reformulating an expression to avoid direct subtraction is crucial for obtaining accurate results.",
            "id": "3596749",
            "problem": "In global seismic tomography, phase travel-time predictions and observations are both large positive numbers whose difference is used as the linearized data residual driving the inversion. Consider a ray path discretized into $N$ segments of lengths $\\ell_i > 0$, $i = 1,\\dots,N$. The predicted and observed travel times are $T_{\\mathrm{pred}} = \\sum_{i=1}^{N} s^{\\mathrm{pred}}_i \\,\\ell_i$ and $T_{\\mathrm{obs}} = \\sum_{i=1}^{N} s^{\\mathrm{obs}}_i \\,\\ell_i$, where $s^{\\mathrm{pred}}_i$ and $s^{\\mathrm{obs}}_i$ are segment slownesses. Suppose the residual is small in the sense that there exists a $T > 0$ and a small parameter $0 < \\rho \\ll 1$ with $T_{\\mathrm{obs}} = T \\left(1 + \\rho/2\\right)$ and $T_{\\mathrm{pred}} = T \\left(1 - \\rho/2\\right)$, so the true residual is $r = T_{\\mathrm{obs}} - T_{\\mathrm{pred}} = T \\rho$.\n\nAssume binary $64$-bit floating-point arithmetic following the Institute of Electrical and Electronics Engineers (IEEE) $754$ round-to-nearest model, with unit roundoff $u = 2^{-53}$. Use the standard model of floating-point rounding: for any real $x$, $\\mathrm{fl}(x) = x\\,(1+\\delta)$ with $|\\delta| \\le u$, and for a basic operation $\\circ \\in \\{+,-,\\times,\\div\\}$, $\\mathrm{fl}(x \\circ y) = (x \\circ y)\\,(1+\\delta)$ with $|\\delta| \\le u$. Throughout, retain only first-order terms in $u$.\n\nConsider two computational formulations for the residual:\n\n- Formulation A (difference of totals): compute $\\widehat{T}_{\\mathrm{obs}} = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(s^{\\mathrm{obs}}_i \\ell_i)\\right)$ and $\\widehat{T}_{\\mathrm{pred}} = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(s^{\\mathrm{pred}}_i \\ell_i)\\right)$, then return $\\widehat{r}_A = \\mathrm{fl}\\!\\left(\\widehat{T}_{\\mathrm{obs}} - \\widehat{T}_{\\mathrm{pred}}\\right)$.\n\n- Formulation B (residual-first accumulation): compute segment residuals $\\Delta s_i = s^{\\mathrm{obs}}_i - s^{\\mathrm{pred}}_i$ first and then accumulate $\\widehat{r}_B = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(\\Delta s_i \\ell_i)\\right)$.\n\nAssume that the only source of floating-point error is rounding at the points explicitly indicated by $\\mathrm{fl}(\\cdot)$, and that the $\\ell_i$ are exactly representable. Work in a first-order forward error model with respect to relative perturbations induced by these roundings.\n\n(a) Starting from the definitions above and the standard floating-point model, derive a first-order worst-case relative error bound of the form $E_A(\\rho,u)$ for Formulation A, where $|\\widehat{r}_A - r|/|r| \\le E_A(\\rho,u)$ to first order in $u$.\n\n(b) Under the same modeling assumptions, derive a first-order worst-case relative error bound $E_B(u)$ for Formulation B, where $|\\widehat{r}_B - r|/|r| \\le E_B(u)$ to first order in $u$.\n\n(c) Define the asymptotic improvement factor\n$$\nI(\\rho) = \\lim_{u \\to 0} \\frac{E_A(\\rho,u)}{E_B(u)} \\, .\n$$\nGiven $\\rho = 3.2 \\times 10^{-8}$, compute $I(\\rho)$. Round your answer to $4$ significant figures. The answer is dimensionless and must be reported without units.",
            "solution": "The user wants me to solve a numerical analysis problem related to floating-point error in seismic tomography. I will first validate the problem statement and then provide a detailed solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Discretized ray path: $N$ segments, lengths $\\ell_i > 0$ for $i = 1, \\dots, N$.\n- Predicted travel time: $T_{\\mathrm{pred}} = \\sum_{i=1}^{N} s^{\\mathrm{pred}}_i \\,\\ell_i$.\n- Observed travel time: $T_{\\mathrm{obs}} = \\sum_{i=1}^{N} s^{\\mathrm{obs}}_i \\,\\ell_i$.\n- Slownesses: $s^{\\mathrm{pred}}_i, s^{\\mathrm{obs}}_i$.\n- Travel time relationship: $T_{\\mathrm{obs}} = T(1 + \\rho/2)$ and $T_{\\mathrm{pred}} = T(1 - \\rho/2)$ for $T > 0$ and $0 < \\rho \\ll 1$.\n- True residual: $r = T_{\\mathrm{obs}} - T_{\\mathrm{pred}} = T \\rho$.\n- Floating-point arithmetic: IEEE 754 binary $64$-bit, round-to-nearest. Unit roundoff $u = 2^{-53}$.\n- Floating-point model: $\\mathrm{fl}(x) = x(1+\\delta)$, $|\\delta| \\le u$. $\\mathrm{fl}(x \\circ y) = (x \\circ y)(1+\\delta)$, $|\\delta| \\le u$. Analysis is first-order in $u$.\n- Formulation A: $\\widehat{T}_{\\mathrm{obs}} = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(s^{\\mathrm{obs}}_i \\ell_i)\\right)$, $\\widehat{T}_{\\mathrm{pred}} = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(s^{\\mathrm{pred}}_i \\ell_i)\\right)$, $\\widehat{r}_A = \\mathrm{fl}\\!\\left(\\widehat{T}_{\\mathrm{obs}} - \\widehat{T}_{\\mathrm{pred}}\\right)$.\n- Formulation B: $\\Delta s_i = s^{\\mathrm{obs}}_i - s^{\\mathrm{pred}}_i$, $\\widehat{r}_B = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(\\Delta s_i \\ell_i)\\right)$.\n- Error source assumption: Only from explicit $\\mathrm{fl}(\\cdot)$ calls. $\\ell_i$ are exact. $\\Delta s_i$ computed exactly.\n- Tasks:\n  (a) Derive first-order worst-case relative error bound $E_A(\\rho,u)$ for Formulation A.\n  (b) Derive first-order worst-case relative error bound $E_B(u)$ for Formulation B.\n  (c) Compute $I(\\rho) = \\lim_{u \\to 0} E_A(\\rho,u) / E_B(u)$ for $\\rho = 3.2 \\times 10^{-8}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It describes a classic numerical stability problem (catastrophic cancellation) in a realistic scientific context (seismic tomography). The floating-point model is standard. The givens are self-contained and consistent, and the tasks are well-defined. There are no violations of the validation criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with the solution.\n\n### Solution\n\nThis problem analyzes the numerical stability of two different ways to calculate the travel-time residual in seismic tomography. The core issue is the potential for catastrophic cancellation when subtracting two nearly equal large numbers.\n\n(a) Derivation of the error bound $E_A(\\rho,u)$ for Formulation A.\n\nIn Formulation A, we first compute the total travel times $\\widehat{T}_{\\mathrm{obs}}$ and $\\widehat{T}_{\\mathrm{pred}}$ and then find their difference.\n\nThe computation of $\\widehat{T}_{\\mathrm{obs}} = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(s^{\\mathrm{obs}}_i \\ell_i)\\right)$ is a sum of products, equivalent to a dot product. The standard forward error model for a dot product of two $N$-vectors, computed with $N$ multiplications and $N-1$ additions, gives a bound on the absolute error. Let $T_{\\mathrm{obs}} = \\sum_{i=1}^N s_i^{\\mathrm{obs}}\\ell_i$. The computed value $\\widehat{T}_{\\mathrm{obs}}$ satisfies:\n$$\n|\\widehat{T}_{\\mathrm{obs}} - T_{\\mathrm{obs}}| \\le \\gamma_N \\sum_{i=1}^N |s_i^{\\mathrm{obs}}\\ell_i|\n$$\nwhere $\\gamma_N = \\frac{Nu}{1-Nu}$. Since we are working to first order in $u$, we approximate $\\gamma_N \\approx Nu$. Since slownesses $s_i^{\\mathrm{obs}}$ and lengths $\\ell_i$ are positive, the sum of absolute values is the value itself: $\\sum_{i=1}^N |s_i^{\\mathrm{obs}}\\ell_i| = T_{\\mathrm{obs}}$.\nSo, we have:\n$$\n|\\widehat{T}_{\\mathrm{obs}} - T_{\\mathrm{obs}}| \\le Nu \\cdot T_{\\mathrm{obs}}\n$$\nThis means we can write the computed value as $\\widehat{T}_{\\mathrm{obs}} = T_{\\mathrm{obs}}(1 + \\eta_{\\mathrm{obs}})$ where $|\\eta_{\\mathrm{obs}}| \\le Nu$. Similarly, for the predicted travel time:\n$$\n\\widehat{T}_{\\mathrm{pred}} = T_{\\mathrm{pred}}(1 + \\eta_{\\mathrm{pred}}) \\quad \\text{with} \\quad |\\eta_{\\mathrm{pred}}| \\le Nu\n$$\nNext, the residual is computed as $\\widehat{r}_A = \\mathrm{fl}(\\widehat{T}_{\\mathrm{obs}} - \\widehat{T}_{\\mathrm{pred}})$. This subtraction introduces another rounding error:\n$$\n\\widehat{r}_A = (\\widehat{T}_{\\mathrm{obs}} - \\widehat{T}_{\\mathrm{pred}})(1 + \\delta_{sub}) \\quad \\text{with} \\quad |\\delta_{sub}| \\le u\n$$\nThe absolute error is $\\widehat{r}_A - r$. Let's analyze this term by term.\n$$\n\\widehat{r}_A - r = (\\widehat{T}_{\\mathrm{obs}} - \\widehat{T}_{\\mathrm{pred}})(1 + \\delta_{sub}) - (T_{\\mathrm{obs}} - T_{\\mathrm{pred}})\n$$\nSubstituting the expressions for $\\widehat{T}_{\\mathrm{obs}}$ and $\\widehat{T}_{\\mathrm{pred}}$:\n$$\n\\widehat{r}_A - r = (T_{\\mathrm{obs}}(1 + \\eta_{\\mathrm{obs}}) - T_{\\mathrm{pred}}(1 + \\eta_{\\mathrm{pred}}))(1 + \\delta_{sub}) - r\n$$\n$$\n= (T_{\\mathrm{obs}} - T_{\\mathrm{pred}} + T_{\\mathrm{obs}}\\eta_{\\mathrm{obs}} - T_{\\mathrm{pred}}\\eta_{\\mathrm{pred}})(1 + \\delta_{sub}) - r\n$$\n$$\n= (r + T_{\\mathrm{obs}}\\eta_{\\mathrm{obs}} - T_{\\mathrm{pred}}\\eta_{\\mathrm{pred}})(1 + \\delta_{sub}) - r\n$$\nExpanding and keeping only first-order terms in the error factors ($u, \\eta_{\\mathrm{obs}}, \\eta_{\\mathrm{pred}}, \\delta_{sub}$):\n$$\n\\widehat{r}_A - r \\approx (r + T_{\\mathrm{obs}}\\eta_{\\mathrm{obs}} - T_{\\mathrm{pred}}\\eta_{\\mathrm{pred}}) - r + r\\delta_{sub} = T_{\\mathrm{obs}}\\eta_{\\mathrm{obs}} - T_{\\mathrm{pred}}\\eta_{\\mathrm{pred}} + r\\delta_{sub}\n$$\nThe worst-case absolute error is found using the triangle inequality:\n$$\n|\\widehat{r}_A - r| \\le |T_{\\mathrm{obs}}\\eta_{\\mathrm{obs}}| + |T_{\\mathrm{pred}}\\eta_{\\mathrm{pred}}| + |r\\delta_{sub}|\n$$\nSubstituting the bounds for $|\\eta_{\\mathrm{obs}}|$, $|\\eta_{\\mathrm{pred}}|$, and $|\\delta_{sub}|$:\n$$\n|\\widehat{r}_A - r| \\le T_{\\mathrm{obs}}(Nu) + T_{\\mathrm{pred}}(Nu) + |r|u = (T_{\\mathrm{obs}} + T_{\\mathrm{pred}})Nu + |r|u\n$$\nNow, we use the problem's givens: $T_{\\mathrm{obs}} = T(1 + \\rho/2)$, $T_{\\mathrm{pred}} = T(1 - \\rho/2)$, so $T_{\\mathrm{obs}} + T_{\\mathrm{pred}} = 2T$. And $|r| = |T\\rho| = T\\rho$ since $T>0, \\rho>0$.\n$$\n|\\widehat{r}_A - r| \\le (2T)Nu + (T\\rho)u\n$$\nTo find the relative error, we divide by $|r| = T\\rho$:\n$$\n\\frac{|\\widehat{r}_A - r|}{|r|} \\le \\frac{2TNu}{T\\rho} + \\frac{T\\rho u}{T\\rho} = \\frac{2N}{\\rho}u + u = \\left(\\frac{2N}{\\rho} + 1\\right)u\n$$\nSo, the first-order worst-case relative error bound is $E_A(\\rho,u) = \\left(\\frac{2N}{\\rho} + 1\\right)u$.\n\n(b) Derivation of the error bound $E_B(u)$ for Formulation B.\n\nIn Formulation B, we first form the segment residual slownesses $\\Delta s_i = s^{\\mathrm{obs}}_i - s^{\\mathrm{pred}}_i$ and then accumulate the residual: $\\widehat{r}_B = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(\\Delta s_i \\ell_i)\\right)$. The true residual is $r = \\sum_i \\Delta s_i \\ell_i$.\n\nThis is again a sum of products. Applying the same standard error model as in part (a):\n$$\n|\\widehat{r}_B - r| \\le \\gamma_N \\sum_{i=1}^{N} |\\Delta s_i \\ell_i| \\approx Nu \\sum_{i=1}^{N} |\\Delta s_i \\ell_i|\n$$\nThe relative error is therefore:\n$$\n\\frac{|\\widehat{r}_B - r|}{|r|} \\le Nu \\frac{\\sum_{i=1}^{N} |\\Delta s_i \\ell_i|}{|\\sum_{i=1}^{N} \\Delta s_i \\ell_i|}\n$$\nThe factor $C_r = \\frac{\\sum_{i=1}^{N} |\\Delta s_i \\ell_i|}{|\\sum_{i=1}^{N} \\Delta s_i \\ell_i|}$ is the condition number of the summation. If the terms $\\Delta s_i \\ell_i$ have mixed signs, this factor can be very large, indicating cancellation in the sum.\n\nThe problem asks for an error bound $E_B(u)$ that is independent of the specific data values $\\{\\Delta s_i\\}$ and of $\\rho$. This is a strong indication that we should consider the case for which this formulation is numerically stable, i.e., the case where cancellation is not an issue. The worst-case bound is then taken over the rounding errors, under the assumption of a well-conditioned sum. The ideal case corresponds to all terms $\\Delta s_i \\ell_i$ having the same sign. In this case, $\\sum |\\Delta s_i \\ell_i| = |\\sum \\Delta s_i \\ell_i|$ and the condition number $C_r = 1$. This simplifying assumption is necessary to produce a bound with the requested functional form $E_B(u)$. Under this assumption, the relative error bound becomes:\n$$\n\\frac{|\\widehat{r}_B - r|}{|r|} \\le Nu\n$$\nThus, the bound is $E_B(u) = Nu$.\n\n(c) Computation of the asymptotic improvement factor $I(\\rho)$.\n\nThe asymptotic improvement factor is defined as:\n$$\nI(\\rho) = \\lim_{u \\to 0} \\frac{E_A(\\rho,u)}{E_B(u)}\n$$\nUsing the bounds derived in parts (a) and (b):\n$$\nI(\\rho) = \\lim_{u \\to 0} \\frac{\\left(\\frac{2N}{\\rho} + 1\\right)u}{Nu} = \\frac{\\frac{2N}{\\rho} + 1}{N} = \\frac{2}{\\rho} + \\frac{1}{N}\n$$\nThe question asks for $I(\\rho)$, which suggests the final result should not depend on $N$. The expression $\\frac{2}{\\rho} + \\frac{1}{N}$ contains $N$. However, the problem states that $\\rho$ is a small parameter. We are given $\\rho = 3.2 \\times 10^{-8}$. The term $2/\\rho$ evaluates to:\n$$\n\\frac{2}{3.2 \\times 10^{-8}} = \\frac{20}{3.2} \\times 10^7 = 6.25 \\times 10^7\n$$\nThe number of segments $N$ is a positive integer, so $N \\ge 1$, which means $0 < 1/N \\le 1$. The term $1/N$ is negligible compared to $2/\\rho$. For example, even for $N=1$, the ratio of the terms is $(1/N)/(2/\\rho) = \\rho/2 = 1.6 \\times 10^{-8}$, which is insignificant. Therefore, it is appropriate to neglect the $1/N$ term, which corresponds to recognizing that the error in Formulation A is overwhelmingly dominated by the catastrophic cancellation term.\n$$\nI(\\rho) \\approx \\frac{2}{\\rho}\n$$\nSubstituting the given value of $\\rho$:\n$$\nI(3.2 \\times 10^{-8}) = \\frac{2}{3.2 \\times 10^{-8}} = 6.25 \\times 10^7\n$$\nThe problem requires the answer to be rounded to $4$ significant figures.\n$$\nI(\\rho) = 6.250 \\times 10^7\n$$\nThis factor quantifies the massive loss of relative precision in Formulation A due to catastrophic cancellation compared to the more stable Formulation B.",
            "answer": "$$\n\\boxed{6.250 \\times 10^7}\n$$"
        },
        {
            "introduction": "While modern scientific codes rely on libraries for fundamental tasks, a computational geophysicist must understand how to build robust numerical tools from first principles. This exercise  challenges you to design and implement an algorithm for computing the Euclidean norm $\\|x\\|_2$ that is robust against both overflow and underflow, even when vector components span many orders of magnitude. By deriving a scaling-based approach, you will learn a powerful technique for creating algorithms that don't fail unexpectedly, a cornerstone of reliable scientific software.",
            "id": "3596774",
            "problem": "Consider the task of evaluating the Euclidean norm of a real-valued vector in double-precision floating-point arithmetic, which is widely used throughout computational geophysics for operations such as amplitude normalization and least-squares residual evaluation. Let a vector be denoted by $x = (x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n$ and its Euclidean norm be defined by\n$$\n\\|x\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}.\n$$\nNaively computing $\\sum_{i=1}^n x_i^2$ and then taking the square root can overflow or underflow in double-precision arithmetic when entries span many orders of magnitude.\n\nAssume arithmetic follows the Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) in base $\\beta = 2$ with precision $p = 53$ (binary64, also known as double precision) and rounding to nearest, ties-to-even. Let the unit roundoff be $u = \\tfrac{1}{2}\\beta^{1-p} = 2^{-53}$. Assume a finite exponent range with smallest positive normal number approximately $2.225\\times 10^{-308}$ and largest finite number approximately $1.798\\times 10^{308}$, so that underflow to subnormal representations is possible. Throughout, when modeling rounding, use the standard floating-point model: for a basic operation $\\mathrm{op}\\in\\{+,-,\\times,\\div\\}$,\n$$\n\\mathrm{fl}(a \\ \\mathrm{op}\\ b) = (a \\ \\mathrm{op}\\ b)\\,(1+\\delta), \\quad |\\delta| \\le u,\n$$\nand for square root,\n$$\n\\mathrm{fl}(\\sqrt{y}) = \\sqrt{y}\\,(1+\\delta), \\quad |\\delta| \\le u,\n$$\nwhenever the exact result is within the representable range. Define the quantity\n$$\n\\gamma_k = \\frac{k u}{1 - k u}\n$$\nfor nonnegative integers $k$ such that $k u < 1$.\n\nYou must devise and analyze a scaling-based algorithm to compute $\\|x\\|_2$ robustly in the presence of entries spanning many orders of magnitude. Your task comprises the following steps:\n\n1. Starting only from the definition of $\\|x\\|_2$ and the floating-point model above, derive a scaling strategy that ensures all intermediate operations stay within representable range whenever the exact $\\|x\\|_2$ is representable in binary64. The derivation must be purely from first principles, without introducing any shortcut formulas. Explain why this strategy avoids overflow and underflow during intermediate steps.\n\n2. Prove the correctness of your algorithm by establishing a loop invariant that relates the maintained scale and accumulated sum-of-squares to the exact quantity $\\|x\\|_2$. Your proof must not rely on external lemmas beyond the floating-point model specified above.\n\n3. Derive a forward error bound for the computed norm $\\widehat{\\|x\\|_2}$ of the form\n$$\n\\frac{\\left|\\widehat{\\|x\\|_2} - \\|x\\|_2\\right|}{\\|x\\|_2} \\le C(n) u + \\mathcal{O}(u^2),\n$$\nwhere $C(n)$ is an explicit, simple function of $n$. You must show how $C(n)$ arises from counting the floating-point operations in your algorithm and applying the stated model, and you must express the bound in terms of $\\gamma_k$.\n\n4. Implement your algorithm in a complete, runnable program. The implementation must follow your derivation and use double-precision arithmetic. The program must evaluate $\\|x\\|_2$ for each vector in the test suite below, using your scaling algorithm, and output the results.\n\nThe test suite is the following set of vectors, each specified explicitly to exercise different numerical regimes. All values are dimensionless real numbers:\n- Happy-path mixed magnitudes:\n  $$\n  x^{(1)} = \\left[\\,1.0,\\ 10^{-16},\\ 10^{16},\\ -10^{-3},\\ 3.141592653589793,\\ -2.718281828,\\ 10^{-200},\\ -10^{200}\\,\\right].\n  $$\n- Near-overflow regime with exact result representable:\n  $$\n  x^{(2)} = \\left[\\,10^{308},\\ 10^{308}\\,\\right].\n  $$\n- Near-underflow regime including subnormals:\n  $$\n  x^{(3)} = \\left[\\,10^{-320},\\ -10^{-320},\\ 2\\cdot 10^{-320},\\ -3\\cdot 10^{-320}\\,\\right].\n  $$\n- All zeros (boundary case):\n  $\n  x^{(4)} = \\left[\\,0.0,\\ 0.0,\\ 0.0\\,\\right].\n  $\n- Mixed extremes dominated by a very large entry:\n  $\n  x^{(5)} = \\left[\\,10^{308},\\ 10^{-320},\\ 3.0,\\ -4.0,\\ 10^{-150}\\,\\right].\n  $\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The results must be the five computed norms, in order, each as a floating-point number. For example, the output format must be exactly like\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5].\n$$",
            "solution": "The task is to devise, analyze, and implement a robust algorithm for computing the Euclidean norm $\\|x\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}$ of a vector $x \\in \\mathbb{R}^n$ in double-precision floating-point arithmetic. The analysis must proceed from first principles as laid out in the problem statement.\n\n### 1. Derivation of a Scaling-Based Algorithm\n\nThe naive computation of $\\|x\\|_2$ involves first summing the squares of the elements, $\\sum_{i=1}^n x_i^2$, and then taking the square root. In floating-point arithmetic, the intermediate squaring operation $x_i^2$ can easily lead to overflow if $|x_i|$ is large (e.g., $>10^{155}$) or harmful underflow if $|x_i|$ is small (e.g., $<10^{-155}$), even when the final norm $\\|x\\|_2$ is perfectly representable. For instance, if $x = (10^{200}, 10^{200})$, $\\|x\\|_2 = \\sqrt{2} \\cdot 10^{200}$ is representable, but the intermediate term $x_i^2 = 10^{400}$ overflows the approximate range of double-precision numbers, which is up to $1.8 \\times 10^{308}$.\n\nTo prevent this, we introduce a scaling factor. The key identity is:\n$$\n\\|x\\|_2 = s \\cdot \\sqrt{\\sum_{i=1}^n \\left(\\frac{x_i}{s}\\right)^2} \\quad \\text{for any } s > 0.\n$$\nA two-pass algorithm could first find the maximum absolute value $s = \\max_i |x_i|$, and then, in a second pass, compute the sum of squares of the scaled vector $y = x/s$. In this case, all elements of $y$ satisfy $|y_i| \\le 1$, so squaring them cannot cause overflow.\n\nA more efficient one-pass algorithm can be developed by dynamically updating the scale as we iterate through the vector elements. We maintain a running scale, let's call it $s$, and a running sum of squares, let's call it $\\sigma$. The state $(s, \\sigma)$ at any point represents the norm of the subvector processed so far.\n\nLet's formalize the one-pass algorithm. We initialize a scale variable $s \\leftarrow 0$ and a sum-of-squares variable $\\sigma \\leftarrow 0$. We iterate through the elements $x_i$ for $i=1, \\dots, n$. For each non-zero element $x_i$, we let $a = |x_i|$ and update $s$ and $\\sigma$ as follows:\n\n1.  **Initialization**: If $s=0$ (i.e., this is the first non-zero element encountered), we set the initial scale $s \\leftarrow a$ and initialize the sum of squares $\\sigma \\leftarrow 1$.\n\n2.  **Scale Update**: If $s < a$, the current element $a$ is larger in magnitude than any element seen so far. We must adopt $a$ as the new scale. The current accumulated squared norm is $s^2\\sigma$. The new squared norm will be $s^2\\sigma + a^2$. To prevent overflow, we factor out the new scale $a$:\n    $$\n    s^2\\sigma + a^2 = a^2 \\left( \\sigma \\left(\\frac{s}{a}\\right)^2 + 1 \\right).\n    $$\n    So, we update the state by setting the new sum of squares $\\sigma \\leftarrow 1 + \\sigma(s/a)^2$ and the new scale $s \\leftarrow a$. The ratio $|s/a| < 1$, preventing overflow in its squaring.\n\n3.  **Summation**: If $s \\ge a$, the current scale $s$ is sufficient. The current accumulated squared norm is $s^2\\sigma$. The new squared norm is $s^2\\sigma + a^2$. We factor out the existing scale $s$:\n    $$\n    s^2\\sigma + a^2 = s^2 \\left( \\sigma + \\left(\\frac{a}{s}\\right)^2 \\right).\n    $$\n    So, we update the sum of squares $\\sigma \\leftarrow \\sigma + (a/s)^2$, while the scale $s$ remains unchanged. The ratio $|a/s| \\le 1$, again preventing overflow.\n\nAfter iterating through all elements, the final norm is computed as $s \\sqrt{\\sigma}$. If the input vector contains only zeros, the scale $s$ will remain $0$, and the result is correctly $0$.\n\n**Why this strategy is robust:**\n-   **Overflow Avoidance**: All divisions involve a ratio of a smaller-or-equal magnitude number to a larger one. The result of the division is always in $[-1, 1]$. Squaring this value gives a result in $[0, 1]$, which cannot overflow. The sum of squares $\\sigma$ can grow, but it is bounded by $1 \\le \\sigma \\le n$. For any practical vector dimension $n$, $\\sigma$ will not overflow. The final multiplication $s \\cdot \\sqrt{\\sigma}$ will only overflow if the true norm $\\|x\\|_2$ itself is not representable, which is the correct behavior.\n-   **Underflow Management**: If $|x_i|$ is much smaller than the current scale $s$, the ratio $|x_i/s|$ will be tiny, and its square may underflow to $0$. This is known as \"benign underflow.\" It means the contribution of $x_i^2$ is negligible compared to the squared norm of the largest elements. The loss of such a small term introduces a very small relative error in the final result, which is an acceptable trade-off for robustness.\n\n### 2. Proof of Correctness\n\nWe prove the correctness of the algorithm in exact arithmetic using a loop invariant. Let $(s_k, \\sigma_k)$ be the state of the scale and sum-of-squares variables after processing the first $k$ elements $(x_1, \\dots, x_k)$. We handle the all-zero vector as a trivial case, so assume there is at least one non-zero element.\n\n**Loop Invariant**: For $k \\ge 1$ where $x_k$ is the last element processed from a non-zero subvector $(x_1, \\dots, x_k)$:\n1.  $s_k = \\max_{i=1}^k |x_i|$.\n2.  $\\|(x_1, \\dots, x_k)\\|_2^2 = s_k^2 \\sigma_k$.\n\n**Base Case**: Let $x_j$ be the first non-zero element. After processing it, the state becomes $(s_j, \\sigma_j)$. The algorithm sets $s_j = |x_j|$ and $\\sigma_j = 1$.\n1.  $s_j = |x_j| = \\max_{i=1}^j |x_i|$ holds.\n2.  $\\|(x_1, \\dots, x_j)\\|_2^2 = x_j^2 = |x_j|^2 \\cdot 1 = s_j^2 \\sigma_j$ holds.\nThe invariant holds for the base case.\n\n**Inductive Step**: Assume the invariant holds for step $k$. We process element $x_{k+1}$ and show the invariant holds for the new state $(s_{k+1}, \\sigma_{k+1})$. Let $a = |x_{k+1}|$. By the inductive hypothesis, $\\|(x_1, \\dots, x_k)\\|_2^2 = s_k^2 \\sigma_k$. The target squared norm is $\\|(x_1, \\dots, x_{k+1})\\|_2^2 = \\|(x_1, \\dots, x_k)\\|_2^2 + x_{k+1}^2 = s_k^2 \\sigma_k + a^2$.\n\n**Case 1: $s_k < a$**\nThe algorithm updates $s_{k+1} \\leftarrow a$ and $\\sigma_{k+1} \\leftarrow 1 + \\sigma_k (s_k/a)^2$.\n1.  $s_{k+1} = a = |x_{k+1}|$. Since $s_k = \\max_{i=1}^k |x_i|$ and $s_k < |x_{k+1}|$, we have $s_{k+1} = \\max_{i=1}^{k+1} |x_i|$.\n2.  The new computed squared norm is $s_{k+1}^2 \\sigma_{k+1} = a^2 \\left(1 + \\sigma_k \\left(\\frac{s_k}{a}\\right)^2\\right) = a^2 + a^2\\sigma_k\\frac{s_k^2}{a^2} = a^2 + s_k^2 \\sigma_k$. This matches the target squared norm.\n\n**Case 2: $s_k \\ge a$**\nThe algorithm updates $s_{k+1} \\leftarrow s_k$ and $\\sigma_{k+1} \\leftarrow \\sigma_k + (a/s_k)^2$.\n1.  $s_{k+1} = s_k$. Since $s_k = \\max_{i=1}^k |x_i|$ and $s_k \\ge |x_{k+1}|$, we have $s_{k+1} = \\max_{i=1}^{k+1} |x_i|$.\n2.  The new computed squared norm is $s_{k+1}^2 \\sigma_{k+1} = s_k^2 \\left(\\sigma_k + \\left(\\frac{a}{s_k}\\right)^2\\right) = s_k^2 \\sigma_k + s_k^2\\frac{a^2}{s_k^2} = s_k^2 \\sigma_k + a^2$. This also matches the target squared norm.\n\nSince the invariant holds for the base case and the inductive step, it holds for all $k$ from $1$ to $n$. At the end of the loop ($k=n$), we have $s_n = \\max_{i=1}^n |x_i|$ and $\\|x\\|_2^2 = s_n^2 \\sigma_n$. Taking the square root, we get $\\|x\\|_2 = s_n \\sqrt{\\sigma_n}$, which is precisely what the algorithm computes as its final step. This proves the algorithm's correctness in exact arithmetic.\n\n### 3. Forward Error Bound\n\nWe derive a forward error bound for the computed norm $\\widehat{\\|x\\|_2}$. The analysis of the one-pass algorithm is complex due to the state-dependent updates. A simplified but insightful analysis can be performed on the equivalent two-pass algorithm, which has similar numerical properties.\n\n1.  **Scaling**: Let $s = \\max_i |x_i|$. This value is one of the $|x_i|$ and is assumed to be represented exactly in floating point.\n2.  **Division**: For each $i$, we compute $y_i = x_i/s$. The floating-point operation gives $\\hat{y}_i = \\mathrm{fl}(x_i/s) = (x_i/s)(1+\\delta_i)$ with $|\\delta_i| \\le u$.\n3.  **Squaring**: We compute $\\hat{z}_i = \\mathrm{fl}(\\hat{y}_i^2) = \\hat{y}_i^2(1+\\epsilon_i)$. Substituting $\\hat{y}_i$, we get $\\hat{z}_i = (x_i/s)^2(1+\\delta_i)^2(1+\\epsilon_i) = z_i(1 + 2\\delta_i + \\epsilon_i + \\mathcal{O}(u^2))$, where $z_i=(x_i/s)^2$. The relative error for computing each squared term $\\hat{z}_i$ is bounded by $3u + \\mathcal{O}(u^2)$.\n4.  **Summation**: We compute the sum $\\hat{\\sigma} = \\mathrm{fl}(\\sum_{i=1}^n \\hat{z}_i)$. Using the standard model for floating-point summation, the computed sum is $\\hat{\\sigma} = \\sum_{i=1}^n \\hat{z}_i(1+\\theta_i)$, where $|\\theta_i| \\le \\gamma_{n-1}$ is a conservative bound (a tighter bound depends on the order of summation).\n5.  **Error in $\\sigma$**: Combining these errors, $\\hat{\\sigma} = \\sum_{i=1}^n z_i(1+3u+\\mathcal{O}(u^2))(1+\\gamma_{n-1})$. Let $\\sigma = \\sum z_i$. The relative error in $\\hat{\\sigma}$ is:\n    $$\n    \\frac{|\\hat{\\sigma} - \\sigma|}{\\sigma} = \\frac{|\\sum z_i(1+\\eta_i) - \\sum z_i|}{\\sum z_i} \\le \\max_i |\\eta_i| \\approx 3u + \\gamma_{n-1} \\approx (3+n-1)u = (n+2)u.\n    $$\n    So we can write $\\hat{\\sigma} = \\sigma (1 + \\Delta_\\sigma)$ where $|\\Delta_\\sigma| \\le \\gamma_{n+2} + \\mathcal{O}(u^2)$.\n6.  **Square Root**: We compute $\\hat{\\rho} = \\mathrm{fl}(\\sqrt{\\hat{\\sigma}}) = \\sqrt{\\hat{\\sigma}}(1+\\delta_\\rho)$, $|\\delta_\\rho| \\le u$.\n    $$\n    \\hat{\\rho} = \\sqrt{\\sigma(1+\\Delta_\\sigma)}(1+\\delta_\\rho) = \\sqrt{\\sigma}\\sqrt{1+\\Delta_\\sigma}(1+\\delta_\\rho) \\approx \\sqrt{\\sigma}(1+\\Delta_\\sigma/2)(1+\\delta_\\rho) \\approx \\sqrt{\\sigma}(1+\\Delta_\\sigma/2 + \\delta_\\rho).\n    $$\n7.  **Final Multiplication**: The final result is $\\widehat{\\|x\\|_2} = \\mathrm{fl}(s \\cdot \\hat{\\rho}) = s\\hat{\\rho}(1+\\delta_m)$, $|\\delta_m| \\le u$.\n    $$\n    \\widehat{\\|x\\|_2} \\approx s\\sqrt{\\sigma}(1 + \\Delta_\\sigma/2 + \\delta_\\rho + \\delta_m) = \\|x\\|_2 (1 + \\Delta_\\sigma/2 + \\delta_\\rho + \\delta_m).\n    $$\nThe total relative error is thus bounded by:\n$$\n\\frac{|\\widehat{\\|x\\|_2} - \\|x\\|_2|}{\\|x\\|_2} \\le \\frac{|\\Delta_\\sigma|}{2} + |\\delta_\\rho| + |\\delta_m| \\le \\frac{\\gamma_{n+2}}{2} + u + u.\n$$\nFor small $n u$, $\\gamma_{n+2} \\approx (n+2)u$. The bound becomes:\n$$\n\\frac{|\\widehat{\\|x\\|_2} - \\|x\\|_2|}{\\|x\\|_2} \\le \\frac{(n+2)u}{2} + 2u = \\left(\\frac{n}{2} + 1 + 2\\right)u = \\frac{n+6}{2} u + \\mathcal{O}(u^2).\n$$\nThus, we found an explicit function $C(n) = \\frac{n+6}{2}$. The derived bound demonstrates that the algorithm is numerically stable, with the error growing linearly with the vector dimension $n$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of robustly computing the Euclidean norm for a suite of test vectors.\n    \"\"\"\n\n    def robust_euclidean_norm(x: np.ndarray) -> np.float64:\n        \"\"\"\n        Computes the Euclidean norm of a vector using a one-pass, scaling-based algorithm\n        to prevent unnecessary overflow and underflow.\n\n        This algorithm is based on the principle of dynamically maintaining a scale factor\n        and a sum of squares. It is equivalent to LAPACK's 'xnrm2' routine.\n\n        Args:\n            x: A numpy array of float64 values.\n\n        Returns:\n            The computed Euclidean norm as a float64.\n        \"\"\"\n        if x.size == 0:\n            return np.float64(0.0)\n\n        # Initialize scale and sum_sq. Using the LAPACK 'dnrm2' initialization.\n        # scale is the largest absolute value encountered so far.\n        # sum_sq is the sum of squares of elements scaled by 1/scale.\n        scale = np.float64(0.0)\n        sum_sq = np.float64(1.0)\n        \n        # Another initialization strategy also works:\n        # scale = np.float64(0.0)\n        # sum_sq = np.float64(0.0)\n        # This requires an explicit check for scale == 0.0 inside the loop.\n        # The LAPACK method combines initialization and update more seamlessly.\n        \n        for val in x:\n            # We must use Python's built-in abs() or numpy's for intermediate\n            # calculations to maintain precision before casting to float64 if needed.\n            abs_val = np.abs(val)\n            \n            if abs_val > 0:\n                if scale  abs_val:\n                    # Current element is larger than the current scale.\n                    # Rescale the sum of squares and update the scale.\n                    # The ratio is guaranteed to be  1.\n                    ratio = scale / abs_val\n                    sum_sq = np.float64(1.0) + sum_sq * (ratio * ratio)\n                    scale = abs_val\n                else:\n                    # Current element is smaller than or equal to the scale.\n                    # Add its contribution to the sum of squares.\n                    # The ratio is guaranteed to be = 1.\n                    ratio = abs_val / scale\n                    sum_sq = sum_sq + (ratio * ratio)\n\n        # After the loop, the norm is scale * sqrt(sum_sq).\n        # If the vector was all zeros, scale remains 0.0.\n        # Multiplying by 0.0 gives the correct result of 0.0.\n        return scale * np.sqrt(sum_sq)\n\n    # Define the test cases from the problem statement as numpy arrays of double precision floats.\n    test_cases = [\n        np.array([1.0, 1e-16, 1e16, -1e-3, 3.141592653589793, -2.718281828, 1e-200, -1e200], dtype=np.float64),\n        np.array([1e308, 1e308], dtype=np.float64),\n        np.array([1e-320, -1e-320, 2e-320, -3e-320], dtype=np.float64),\n        np.array([0.0, 0.0, 0.0], dtype=np.float64),\n        np.array([1e308, 1e-320, 3.0, -4.0, 1e-150], dtype=np.float64),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = robust_euclidean_norm(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # We use a custom formatting to avoid scientific notation for some numbers\n    # and ensure the output matches typical floating point representations.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}