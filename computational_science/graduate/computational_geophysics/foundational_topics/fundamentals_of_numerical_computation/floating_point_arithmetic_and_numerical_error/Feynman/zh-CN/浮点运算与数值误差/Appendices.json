{
    "hands_on_practices": [
        {
            "introduction": "最具迷惑性的数值误差往往源于最简单的算术运算。本练习将探讨灾难性抵消（catastrophic cancellation）现象，即两个几乎相等的数值相减，可能导致相对精度的灾难性损失。通过分析地震层析成像中走时残差的计算，你将推导两种不同计算方法的误差界。这项实践分析将向你展示，为何代数上等价的公式可能具有截然不同的数值稳定性，并凸显出重构计算以避免此类陷阱的重要性 。",
            "id": "3596749",
            "problem": "在全球地震层析成像中，相位走时的预测值和观测值都是很大的正数，它们的差值被用作驱动反演的线性化数据残差。考虑一条被离散化为 $N$ 段的射线路径，各段长度为 $\\ell_i > 0$，其中 $i = 1,\\dots,N$。预测走时和观测走时分别为 $T_{\\mathrm{pred}} = \\sum_{i=1}^{N} s^{\\mathrm{pred}}_i \\,\\ell_i$ 和 $T_{\\mathrm{obs}} = \\sum_{i=1}^{N} s^{\\mathrm{obs}}_i \\,\\ell_i$，其中 $s^{\\mathrm{pred}}_i$ 和 $s^{\\mathrm{obs}}_i$ 是各段的慢度。假设残差很小，即存在一个 $T > 0$ 和一个小参数 $0  \\rho \\ll 1$，使得 $T_{\\mathrm{obs}} = T \\left(1 + \\rho/2\\right)$ 且 $T_{\\mathrm{pred}} = T \\left(1 - \\rho/2\\right)$，因此真实残差为 $r = T_{\\mathrm{obs}} - T_{\\mathrm{pred}} = T \\rho$。\n\n假设采用二进制64位浮点运算，遵循电气和电子工程师协会（IEEE）754标准的“舍入到最近”（round-to-nearest）模型，单位舍入误差为 $u = 2^{-53}$。使用标准浮点舍入模型：对于任意实数 $x$，$\\mathrm{fl}(x) = x\\,(1+\\delta)$，其中 $|\\delta| \\le u$；对于基本运算 $\\circ \\in \\{+,-,\\times,\\div\\}$，$\\mathrm{fl}(x \\circ y) = (x \\circ y)\\,(1+\\delta)$，其中 $|\\delta| \\le u$。在整个推导过程中，只保留 $u$ 的一阶项。\n\n考虑两种计算残差的公式：\n\n- 公式A（总和之差）：计算 $\\widehat{T}_{\\mathrm{obs}} = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(s^{\\mathrm{obs}}_i \\ell_i)\\right)$ 和 $\\widehat{T}_{\\mathrm{pred}} = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(s^{\\mathrm{pred}}_i \\ell_i)\\right)$，然后返回 $\\widehat{r}_A = \\mathrm{fl}\\!\\left(\\widehat{T}_{\\mathrm{obs}} - \\widehat{T}_{\\mathrm{pred}}\\right)$。\n\n- 公式B（残差优先累加）：先计算各段的残差 $\\Delta s_i = s^{\\mathrm{obs}}_i - s^{\\mathrm{pred}}_i$，然后累加得到 $\\widehat{r}_B = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(\\Delta s_i \\ell_i)\\right)$。\n\n假设浮点误差的唯一来源是在 $\\mathrm{fl}(\\cdot)$ 明确标示之处的舍入，并且 $\\ell_i$ 是可以精确表示的。在相对于这些舍入引起的相对扰动的一阶前向误差模型中进行分析。\n\n(a) 从以上定义和标准浮点模型出发，为公式A推导一个形式为 $E_A(\\rho,u)$ 的一阶最坏情况相对误差界，其中 $|\\widehat{r}_A - r|/|r| \\le E_A(\\rho,u)$（保留到 $u$ 的一阶）。\n\n(b) 在相同的模型假设下，为公式B推导一个一阶最坏情况相对误差界 $E_B(u)$，其中 $|\\widehat{r}_B - r|/|r| \\le E_B(u)$（保留到 $u$ 的一阶）。\n\n(c) 定义渐近改进因子\n$$\nI(\\rho) = \\lim_{u \\to 0} \\frac{E_A(\\rho,u)}{E_B(u)} \\, .\n$$\n给定 $\\rho = 3.2 \\times 10^{-8}$，计算 $I(\\rho)$。将您的答案四舍五入到4位有效数字。该答案是无量纲的，报告时必须不带单位。",
            "solution": "用户要求我解决一个与地震层析成像中浮点误差相关的数值分析问题。我将首先验证问题陈述，然后提供详细的解答。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- 离散化射线路径：$N$ 段，长度 $\\ell_i > 0$，其中 $i = 1, \\dots, N$。\n- 预测走时：$T_{\\mathrm{pred}} = \\sum_{i=1}^{N} s^{\\mathrm{pred}}_i \\,\\ell_i$。\n- 观测走时：$T_{\\mathrm{obs}} = \\sum_{i=1}^{N} s^{\\mathrm{obs}}_i \\,\\ell_i$。\n- 慢度：$s^{\\mathrm{pred}}_i, s^{\\mathrm{obs}}_i$。\n- 走时关系：$T_{\\mathrm{obs}} = T(1 + \\rho/2)$ 和 $T_{\\mathrm{pred}} = T(1 - \\rho/2)$，对于 $T > 0$ 和 $0  \\rho \\ll 1$。\n- 真实残差：$r = T_{\\mathrm{obs}} - T_{\\mathrm{pred}} = T \\rho$。\n- 浮点运算：IEEE 754 二进制64位，舍入到最近。单位舍入误差 $u = 2^{-53}$。\n- 浮点模型：$\\mathrm{fl}(x) = x(1+\\delta)$，$|\\delta| \\le u$。$\\mathrm{fl}(x \\circ y) = (x \\circ y)(1+\\delta)$，$|\\delta| \\le u$。分析保留到 $u$ 的一阶。\n- 公式A：$\\widehat{T}_{\\mathrm{obs}} = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(s^{\\mathrm{obs}}_i \\ell_i)\\right)$，$\\widehat{T}_{\\mathrm{pred}} = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(s^{\\mathrm{pred}}_i \\ell_i)\\right)$，$\\widehat{r}_A = \\mathrm{fl}\\!\\left(\\widehat{T}_{\\mathrm{obs}} - \\widehat{T}_{\\mathrm{pred}}\\right)$。\n- 公式B：$\\Delta s_i = s^{\\mathrm{obs}}_i - s^{\\mathrm{pred}}_i$，$\\widehat{r}_B = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(\\Delta s_i \\ell_i)\\right)$。\n- 误差来源假设：仅来自明确的 $\\mathrm{fl}(\\cdot)$ 调用。$\\ell_i$ 是精确的。$\\Delta s_i$ 是精确计算的。\n- 任务：\n  (a) 为公式A推导一阶最坏情况相对误差界 $E_A(\\rho,u)$。\n  (b) 为公式B推导一阶最坏情况相对误差界 $E_B(u)$。\n  (c) 对于 $\\rho = 3.2 \\times 10^{-8}$，计算 $I(\\rho) = \\lim_{u \\to 0} E_A(\\rho,u) / E_B(u)$。\n\n**步骤2：使用提取的已知条件进行验证**\n该问题具有科学依据、是适定的、客观的。它在真实的科学背景（地震层析成像）下描述了一个经典的数值稳定性问题（灾难性抵消）。浮点模型是标准的。已知条件是自洽且一致的，任务定义明确。没有违反验证标准的情况。\n\n**步骤3：结论与行动**\n问题有效。我将开始解答。\n\n### 解答\n\n本问题分析了在地震层析成像中计算走时残差的两种不同方法的数值稳定性。核心问题是在两个几乎相等的大数相减时可能发生的灾难性抵消。\n\n(a) 公式A的误差界 $E_A(\\rho,u)$ 的推导。\n\n在公式A中，我们首先计算总走时 $\\widehat{T}_{\\mathrm{obs}}$ 和 $\\widehat{T}_{\\mathrm{pred}}$，然后求它们的差。\n\n$\\widehat{T}_{\\mathrm{obs}} = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(s^{\\mathrm{obs}}_i \\ell_i)\\right)$ 的计算是乘积之和，等价于一个点积。对于一个由 $N$ 次乘法和 $N-1$ 次加法计算的两个 $N$ 维向量的点积，其标准前向误差模型给出了绝对误差的一个界。令 $T_{\\mathrm{obs}} = \\sum_{i=1}^N s_i^{\\mathrm{obs}}\\ell_i$。计算值 $\\widehat{T}_{\\mathrm{obs}}$ 满足：\n$$\n|\\widehat{T}_{\\mathrm{obs}} - T_{\\mathrm{obs}}| \\le \\gamma_N \\sum_{i=1}^N |s_i^{\\mathrm{obs}}\\ell_i|\n$$\n其中 $\\gamma_N = \\frac{Nu}{1-Nu}$。由于我们只考虑 $u$ 的一阶项，我们近似 $\\gamma_N \\approx Nu$。由于慢度 $s_i^{\\mathrm{obs}}$ 和长度 $\\ell_i$ 都是正数，绝对值之和就是其本身：$\\sum_{i=1}^N |s_i^{\\mathrm{obs}}\\ell_i| = T_{\\mathrm{obs}}$。\n所以，我们有：\n$$\n|\\widehat{T}_{\\mathrm{obs}} - T_{\\mathrm{obs}}| \\le Nu \\cdot T_{\\mathrm{obs}}\n$$\n这意味着我们可以将计算值写为 $\\widehat{T}_{\\mathrm{obs}} = T_{\\mathrm{obs}}(1 + \\eta_{\\mathrm{obs}})$，其中 $|\\eta_{\\mathrm{obs}}| \\le Nu$。类似地，对于预测走时：\n$$\n\\widehat{T}_{\\mathrm{pred}} = T_{\\mathrm{pred}}(1 + \\eta_{\\mathrm{pred}}) \\quad \\text{with} \\quad |\\eta_{\\mathrm{pred}}| \\le Nu\n$$\n接下来，残差计算为 $\\widehat{r}_A = \\mathrm{fl}(\\widehat{T}_{\\mathrm{obs}} - \\widehat{T}_{\\mathrm{pred}})$。这个减法引入了另一个舍入误差：\n$$\n\\widehat{r}_A = (\\widehat{T}_{\\mathrm{obs}} - \\widehat{T}_{\\mathrm{pred}})(1 + \\delta_{sub}) \\quad \\text{with} \\quad |\\delta_{sub}| \\le u\n$$\n绝对误差为 $\\widehat{r}_A - r$。让我们逐项分析此式。\n$$\n\\widehat{r}_A - r = (\\widehat{T}_{\\mathrm{obs}} - \\widehat{T}_{\\mathrm{pred}})(1 + \\delta_{sub}) - (T_{\\mathrm{obs}} - T_{\\mathrm{pred}})\n$$\n代入 $\\widehat{T}_{\\mathrm{obs}}$ 和 $\\widehat{T}_{\\mathrm{pred}}$ 的表达式：\n$$\n\\widehat{r}_A - r = (T_{\\mathrm{obs}}(1 + \\eta_{\\mathrm{obs}}) - T_{\\mathrm{pred}}(1 + \\eta_{\\mathrm{pred}}))(1 + \\delta_{sub}) - r\n$$\n$$\n= (T_{\\mathrm{obs}} - T_{\\mathrm{pred}} + T_{\\mathrm{obs}}\\eta_{\\mathrm{obs}} - T_{\\mathrm{pred}}\\eta_{\\mathrm{pred}})(1 + \\delta_{sub}) - r\n$$\n$$\n= (r + T_{\\mathrm{obs}}\\eta_{\\mathrm{obs}} - T_{\\mathrm{pred}}\\eta_{\\mathrm{pred}})(1 + \\delta_{sub}) - r\n$$\n展开并只保留误差因子（$u, \\eta_{\\mathrm{obs}}, \\eta_{\\mathrm{pred}}, \\delta_{sub}$）的一阶项：\n$$\n\\widehat{r}_A - r \\approx (r + T_{\\mathrm{obs}}\\eta_{\\mathrm{obs}} - T_{\\mathrm{pred}}\\eta_{\\mathrm{pred}}) - r + r\\delta_{sub} = T_{\\mathrm{obs}}\\eta_{\\mathrm{obs}} - T_{\\mathrm{pred}}\\eta_{\\mathrm{pred}} + r\\delta_{sub}\n$$\n使用三角不等式可找到最坏情况下的绝对误差：\n$$\n|\\widehat{r}_A - r| \\le |T_{\\mathrm{obs}}\\eta_{\\mathrm{obs}}| + |T_{\\mathrm{pred}}\\eta_{\\mathrm{pred}}| + |r\\delta_{sub}|\n$$\n代入 $|\\eta_{\\mathrm{obs}}|$、 $|\\eta_{\\mathrm{pred}}|$ 和 $|\\delta_{sub}|$ 的界：\n$$\n|\\widehat{r}_A - r| \\le T_{\\mathrm{obs}}(Nu) + T_{\\mathrm{pred}}(Nu) + |r|u = (T_{\\mathrm{obs}} + T_{\\mathrm{pred}})Nu + |r|u\n$$\n现在，我们使用问题的已知条件：$T_{\\mathrm{obs}} = T(1 + \\rho/2)$，$T_{\\mathrm{pred}} = T(1 - \\rho/2)$，所以 $T_{\\mathrm{obs}} + T_{\\mathrm{pred}} = 2T$。并且 $|r| = |T\\rho| = T\\rho$ 因为 $T>0, \\rho>0$。\n$$\n|\\widehat{r}_A - r| \\le (2T)Nu + (T\\rho)u\n$$\n为了求相对误差，我们除以 $|r| = T\\rho$：\n$$\n\\frac{|\\widehat{r}_A - r|}{|r|} \\le \\frac{2TNu}{T\\rho} + \\frac{T\\rho u}{T\\rho} = \\frac{2N}{\\rho}u + u = \\left(\\frac{2N}{\\rho} + 1\\right)u\n$$\n所以，一阶最坏情况相对误差界是 $E_A(\\rho,u) = \\left(\\frac{2N}{\\rho} + 1\\right)u$。\n\n(b) 公式B的误差界 $E_B(u)$ 的推导。\n\n在公式B中，我们首先形成段残差慢度 $\\Delta s_i = s^{\\mathrm{obs}}_i - s^{\\mathrm{pred}}_i$，然后累加残差：$\\widehat{r}_B = \\mathrm{fl}\\!\\left(\\sum_{i=1}^{N} \\mathrm{fl}(\\Delta s_i \\ell_i)\\right)$。真实残差是 $r = \\sum_i \\Delta s_i \\ell_i$。\n\n这同样是一个乘积之和。应用与(a)部分相同的标准误差模型：\n$$\n|\\widehat{r}_B - r| \\le \\gamma_N \\sum_{i=1}^{N} |\\Delta s_i \\ell_i| \\approx Nu \\sum_{i=1}^{N} |\\Delta s_i \\ell_i|\n$$\n因此，相对误差为：\n$$\n\\frac{|\\widehat{r}_B - r|}{|r|} \\le Nu \\frac{\\sum_{i=1}^{N} |\\Delta s_i \\ell_i|}{|\\sum_{i=1}^{N} \\Delta s_i \\ell_i|}\n$$\n因子 $C_r = \\frac{\\sum_{i=1}^{N} |\\Delta s_i \\ell_i|}{|\\sum_{i=1}^{N} \\Delta s_i \\ell_i|}$ 是求和的条件数。如果各项 $\\Delta s_i \\ell_i$ 的符号不同，这个因子可能会非常大，表明求和过程中存在抵消。\n\n问题要求一个与具体数据值 $\\{\\Delta s_i\\}$ 和 $\\rho$ 无关的误差界 $E_B(u)$。这强烈表明我们应该考虑该公式数值稳定的情况，即不存在抵消问题的情况。此时，最坏情况的界是在和是良态的假设下，对舍入误差取界。理想情况对应于所有项 $\\Delta s_i \\ell_i$ 具有相同的符号。在这种情况下，$\\sum |\\Delta s_i \\ell_i| = |\\sum \\Delta s_i \\ell_i|$ 且条件数 $C_r = 1$。为了得到所要求的函数形式 $E_B(u)$ 的界，这个简化假设是必要的。在此假设下，相对误差界变为：\n$$\n\\frac{|\\widehat{r}_B - r|}{|r|} \\le Nu\n$$\n因此，界为 $E_B(u) = Nu$。\n\n(c) 渐近改进因子 $I(\\rho)$ 的计算。\n\n渐近改进因子定义为：\n$$\nI(\\rho) = \\lim_{u \\to 0} \\frac{E_A(\\rho,u)}{E_B(u)}\n$$\n使用在(a)和(b)部分推导出的界：\n$$\nI(\\rho) = \\lim_{u \\to 0} \\frac{\\left(\\frac{2N}{\\rho} + 1\\right)u}{Nu} = \\frac{\\frac{2N}{\\rho} + 1}{N} = \\frac{2}{\\rho} + \\frac{1}{N}\n$$\n问题要求计算 $I(\\rho)$，这表明最终结果不应依赖于 $N$。表达式 $\\frac{2}{\\rho} + \\frac{1}{N}$ 包含 $N$。然而，问题陈述指出 $\\rho$ 是一个小参数。我们已知 $\\rho = 3.2 \\times 10^{-8}$。$2/\\rho$ 项的计算结果为：\n$$\n\\frac{2}{3.2 \\times 10^{-8}} = \\frac{20}{3.2} \\times 10^7 = 6.25 \\times 10^7\n$$\n段数 $N$ 是一个正整数，所以 $N \\ge 1$，这意味着 $0  1/N \\le 1$。与 $2/\\rho$ 相比，$1/N$ 项可以忽略不计。例如，即使对于 $N=1$，这两项的比率是 $(1/N)/(2/\\rho) = \\rho/2 = 1.6 \\times 10^{-8}$，这是微不足道的。因此，忽略 $1/N$ 项是合理的，这相当于认识到公式A中的误差绝大部分由灾难性抵消项主导。\n$$\nI(\\rho) \\approx \\frac{2}{\\rho}\n$$\n代入给定的 $\\rho$ 值：\n$$\nI(3.2 \\times 10^{-8}) = \\frac{2}{3.2 \\times 10^{-8}} = 6.25 \\times 10^7\n$$\n问题要求将答案四舍五入到4位有效数字。\n$$\nI(\\rho) = 6.250 \\times 10^7\n$$\n这个因子量化了由于灾难性抵消，公式A相对于更稳定的公式B所造成的相对精度的巨大损失。",
            "answer": "$$\n\\boxed{6.250 \\times 10^7}\n$$"
        },
        {
            "introduction": "求和是科学计算中的一项基本操作，但要精确地完成它却颇具挑战，尤其是当处理大量符号或量级各异的项时。在本练习中，我们将探讨在计算对数似然函数梯度（地球物理反演中的一项常见任务）时遇到的这个问题。你将实现并比较多种求和算法——从朴素方法到补偿求和与成对求和——以量化在一个已知真实总和为零的场景中，数据排序和算法选择如何影响最终精度 。",
            "id": "3596693",
            "problem": "您的任务是研究在计算标量参数贝叶斯反演的对数似然梯度时产生的浮点求和误差，该反演使用学生$t$分布噪声模型来处理重尾残差。考虑由 $i \\in \\{1,\\dots,N\\}$ 索引的观测值，其标准化残差为 $r_i \\in \\mathbb{R}$，自由度参数为 $\\nu \\in \\mathbb{R}_{>0}$。单个数据点的似然正比于\n$$p(r_i \\mid \\nu) \\propto \\left(1 + \\frac{r_i^2}{\\nu}\\right)^{-\\frac{\\nu + 1}{2}},$$\n总对数似然为\n$$\\ell(m) = \\sum_{i=1}^{N} \\log p(r_i(m) \\mid \\nu),$$\n其中 $m \\in \\mathbb{R}$ 是一个标量模型参数。假设一个单参数线性正演映射，其中残差 $r_i(m)$ 线性依赖于 $m$，并且已经经过标准化以吸收灵敏度，这意味着\n$$\\frac{\\partial r_i}{\\partial m} = -1.$$\n您必须根据上述定义和链式法则推导出单个数据点的梯度项 $\\frac{\\partial \\ell_i}{\\partial m}$，然后在不使用题目说明中给出的任何闭式表达式的情况下，为给定的 $r_i$ 和 $\\nu$ 实现其计算。\n\n为了分离出求和误差，您将通过使用反对称的残差配置来构造确定性的测试用例，在这些用例中，真实的梯度总和恰好为零。具体来说，令\n$$g(r,\\nu) \\equiv \\frac{\\partial \\ell_i}{\\partial m}\\Big|_{r_i=r},$$\n这样总梯度就是 $\\nabla \\ell(m) = \\sum_{i=1}^{N} g(r_i,\\nu)$。对于所述的学生$t$分布模型，函数 $g(r,\\nu)$ 是关于 $r$ 的奇函数。因此，如果残差的多重集在 $r \\mapsto -r$ 变换下是对称的，那么精确的梯度总和为 $0$。您的程序将利用这一点，通过计算绝对前向误差 $|\\sum_i g(r_i,\\nu) - 0|$ 来衡量纯粹的浮点求和误差。\n\n您必须实现以下求和策略，以从一个项列表 $g_i$ 中累加求和 $S = \\sum_{i=1}^{N} g(r_i,\\nu)$：\n- 双精度（IEEE $754$ binary$64$）的朴素从左到右累加。\n- 单精度（IEEE $754$ binary$32$）的朴素从左到右累加。\n- 双精度的成对递归求和，使用平衡二分分区，直到块大小最多为 $32$ 时停止，每个块内从左到右求和。\n- 双精度的分块求和，固定块大小 $B = 1024$，每个块内从左到右求和，然后块的总和再从左到右累加。\n- 双精度的Kahan补偿求和。\n\n根据一个幅值列表 $\\{u_j\\}$ 和一个正整数重复次数 $R$，按如下方式确定性地为每个案例构建残差列表。对于给定的排序规范，为每个幅值 $u_j$ 创建一个包含 $R$ 个 $+u_j$ 副本和 $R$ 个 $-u_j$ 副本的列表。将使用两种排序方式：\n- 正数聚集后接负数：按 $u_j$ 的升序列出所有 $u_j$ 的所有正值，然后按相同的升序序列出所有 $u_j$ 的所有负值。\n- 按幅值完美交错：按 $u_j$ 的升序，对每个 $u_j$ 生成长度为 $2R$ 的序列 $[+u_j, -u_j, +u_j, -u_j, \\dots]$。\n\n您的程序必须计算每个测试用例上每种求和策略的绝对前向误差，并将结果以列表的列表形式单行报告。每个内部列表必须按\n$$[\\text{naive64},\\,\\text{naive32},\\,\\text{pairwise64},\\,\\text{blocked64},\\,\\text{kahan64}],$$\n的顺序列出，并包含作为实数的绝对误差。\n\n测试套件。对于每个测试用例 $k \\in \\{1,2,3,4\\}$，您必须使用指定的 $\\nu$、幅值 $\\{u_j\\}$、重复次数 $R$ 和排序方式生成残差：\n- 测试 $1$：$\\nu = 3$, $\\{u_j\\} = [10^{-8},\\,10^{-4},\\,10^{0},\\,10^{4},\\,10^{8}]$, $R = 2000$，正数聚集后接负数。\n- 测试 $2$：$\\nu = 1.1$, $\\{u_j\\} = [10^{-12},\\,10^{-6},\\,10^{-3},\\,10^{-1},\\,10^{1},\\,10^{3},\\,10^{6}]$, $R = 1000$，正数聚集后接负数。\n- 测试 $3$：$\\nu = 300$, $\\{u_j\\} = [10^{-12},\\,10^{-9},\\,10^{-6},\\,10^{-3},\\,10^{-2},\\,10^{-1},\\,10^{0},\\,10^{1},\\,10^{3},\\,10^{6}]$, $R = 500$，正数聚集后接负数。\n- 测试 $4$：$\\nu = 3$, $\\{u_j\\} = [10^{-8},\\,10^{-4},\\,10^{0},\\,10^{4},\\,10^{8}]$, $R = 2000$，按幅值完美交错。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个无空格、逗号分隔的列表的列表形式的结果，例如\n$$[[e_{1,1},e_{1,2},e_{1,3},e_{1,4},e_{1,5}],[e_{2,1},\\dots,e_{2,5}],[e_{3,1},\\dots,e_{3,5}],[e_{4,1},\\dots,e_{4,5}]],$$\n其中 $e_{k,j}$ 是测试用例 $k$ 使用求和策略 $j$ 的绝对前向误差。不应打印任何附加文本。",
            "solution": "用户提供了一个问题，要求在贝叶斯反演问题的背景下分析浮点求和误差。解决方案涉及推导对数似然函数的梯度，生成一个旨在使真实梯度为零的特定数据集，然后实现并比较几种求和算法来计算数值梯度及其误差。\n\n### 1. 单数据点梯度的推导\n\n问题始于根据学生$t$分布定义的单数据点似然，其正比于：\n$$p(r_i \\mid \\nu) \\propto \\left(1 + \\frac{r_i^2}{\\nu}\\right)^{-\\frac{\\nu + 1}{2}}$$\n其中 $r_i$ 是第 $i$ 个数据点的标准化残差，$\\nu$ 是自由度参数。相应的单数据点对数似然 $\\ell_i(m) = \\log p(r_i(m) \\mid \\nu)$ 为：\n$$\\ell_i(m) = C - \\frac{\\nu + 1}{2} \\log\\left(1 + \\frac{r_i(m)^2}{\\nu}\\right)$$\n其中 $C$ 是一个不依赖于模型参数 $m$ 的比例常数。\n\n为了求得关于标量模型参数 $m$ 的梯度，我们应用链式法则：\n$$\\frac{\\partial \\ell_i}{\\partial m} = \\frac{\\partial \\ell_i}{\\partial r_i} \\frac{\\partial r_i}{\\partial m}$$\n首先，我们计算 $\\ell_i$ 关于 $r_i$ 的导数：\n$$\n\\begin{align*}\n\\frac{\\partial \\ell_i}{\\partial r_i} = \\frac{\\partial}{\\partial r_i} \\left[ C - \\frac{\\nu + 1}{2} \\log\\left(1 + \\frac{r_i^2}{\\nu}\\right) \\right] \\\\\n= -\\frac{\\nu + 1}{2} \\cdot \\frac{1}{1 + \\frac{r_i^2}{\\nu}} \\cdot \\frac{\\partial}{\\partial r_i}\\left(1 + \\frac{r_i^2}{\\nu}\\right) \\\\\n= -\\frac{\\nu + 1}{2} \\cdot \\frac{1}{\\frac{\\nu + r_i^2}{\\nu}} \\cdot \\frac{2r_i}{\\nu} \\\\\n= -\\frac{\\nu + 1}{2} \\cdot \\frac{\\nu}{\\nu + r_i^2} \\cdot \\frac{2r_i}{\\nu} \\\\\n= -\\frac{(\\nu + 1)r_i}{\\nu + r_i^2}\n\\end{align*}\n$$\n问题指定了一个线性正演模型，其中灵敏度已被吸收到标准化残差中，因此 $\\frac{\\partial r_i}{\\partial m} = -1$。将此式以及 $\\frac{\\partial \\ell_i}{\\partial r_i}$ 的表达式代入链式法则方程，得到：\n$$\\frac{\\partial \\ell_i}{\\partial m} = \\left( -\\frac{(\\nu + 1)r_i}{\\nu + r_i^2} \\right) \\cdot (-1) = \\frac{(\\nu + 1)r_i}{\\nu + r_i^2}$$\n因此，单数据点梯度项的函数 $g(r, \\nu)$ 是：\n$$g(r, \\nu) = \\frac{(\\nu + 1)r}{\\nu + r^2}$$\n根据问题的设计要求，该函数关于 $r$ 是奇函数，因为 $g(-r, \\nu) = \\frac{(\\nu + 1)(-r)}{\\nu + (-r)^2} = -\\frac{(\\nu + 1)r}{\\nu + r^2} = -g(r, \\nu)$。\n\n### 2. 数值测试设计\n\n数值实验的核心是构建一个梯度项序列 $\\{g_i\\}_{i=1}^N$，其真实总和恰好为零。这通过利用 $g(r, \\nu)$ 的奇对称性来实现。如果残差的多重集 $\\{r_i\\}$ 是反对称的，即对于每个出现的残差 $r$，都有一个对应的 $-r$ 出现，那么在精确算术中，梯度项的总和将为零：\n$$S_{\\text{exact}} = \\sum_{i=1}^{N} g(r_i, \\nu) = 0$$\n因此，任何使用浮点算术计算出的非零总和都纯粹是由数值误差引起的。绝对前向误差就是计算出的总和的绝对值，即 $|S_{\\text{computed}}|$。\n\n问题定义了两种残差排序方案，以突显求和顺序对误差的影响：\n- **正数聚集后接负数**：这种排序首先将所有正梯度项分组，然后是所有负梯度项。这对于朴素求和来说是一种已知的“坏”排序，因为它可能导致累积一个大的中间和，然后减去一个同样大的数，这个过程被称为灾难性抵消。\n- **按幅值完美交错**：这种排序在正负项之间交替，$[+u, -u, +u, -u, \\dots]$。这是一种“好”的排序，因为它倾向于在求和早期将相互抵消的项配对，从而保持运行和的幅值较小，减少舍入误差。\n\n### 3. 求和算法\n\n问题要求实现五种不同的求和算法以比较它们的精度。\n\n1.  **朴素从左到右求和（双精度和单精度）**：这是最基本的方法，即按各项出现的顺序逐个累加。我们为IEEE $754$双精度（`float64`）和单精度（`float32`）都实现了此方法，以显示降低精度的巨大影响。这通过一个简单的for循环实现，以保证从左到右的计算顺序。\n\n2.  **成对递归求和（双精度）**：该算法通过将数字序列递归地分成两半，对每一半求和，然后将两个结果相加来减轻误差。这种策略倾向于将数量级相似的数字相加，这比将非常小的数字加到非常大的数字上在数值上更稳定。当块较小（大小 $\\leq 32$）时，递归停止，并对这些小块进行朴素求和。\n\n3.  **分块求和（双精度）**：这是成对求和的一种更简单、非递归的替代方法。序列被分成固定大小的块（此处为 $B=1024$）。每个块进行朴素求和，然后块的总和本身也进行朴素求和。该方法相比完全朴素的求和能有所改进，但通常不如成对求和精确。\n\n4.  **Kahan补偿求和（双精度）**：这是一种非常有效的减少求和误差的算法。它维护一个运行的补偿变量 $c$，用于累积每次加法中丢失的低位比特。在每一步中，这个“丢失”的部分在下一项被加到运行和之前从中减去，从而有效地重新引入了丢失的精度。\n\n### 4. 实现细节\n\n实现过程首先按规定定义测试用例。对每个测试用例，生成残差列表。然后，计算相应的梯度项列表 $g(r_i, \\nu)$，并将其作为 `np.float64` 值的数组存储。然后用这个项数组调用五个求和函数中的每一个。每个函数结果的绝对值被记录为误差。最终的误差列表的列表随后根据问题规范进行格式化和打印。对于单精度朴素求和，需要注意确保在累加循环中，运行和与每个项都被转换为 `np.float32`，以正确模拟单精度过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef g(r: float, nu: float) -> float:\n    \"\"\"\n    Computes the per-datum gradient term.\n    g(r, nu) = (nu + 1) * r / (nu + r^2)\n    \"\"\"\n    return (nu + 1.0) * r / (nu + r**2)\n\ndef generate_residuals(u_magnitudes: list[float], R: int, ordering: str) -> list[float]:\n    \"\"\"Generates the list of residuals based on the problem specification.\"\"\"\n    residuals = []\n    sorted_u = sorted(u_magnitudes)\n    if ordering == 'clustered':\n        # Positives first, sorted by magnitude\n        for u in sorted_u:\n            residuals.extend([u] * R)\n        # Then negatives, sorted by a magnitude\n        for u in sorted_u:\n            residuals.extend([-u] * R)\n    elif ordering == 'interleaved':\n        # Interleave +u and -u for each magnitude\n        for u in sorted_u:\n            residuals.extend([val for _ in range(R) for val in (u, -u)])\n    return residuals\n\n# --- Summation Algorithms ---\n\ndef naive64_sum(terms: np.ndarray) -> float:\n    \"\"\"Naive left-to-right summation in double precision.\"\"\"\n    s = 0.0\n    for x in terms:\n        s += x\n    return s\n\ndef naive32_sum(terms: np.ndarray) -> float:\n    \"\"\"Naive left-to-right summation in single precision.\"\"\"\n    s = np.float32(0.0)\n    for x in terms:\n        # Each term is cast to float32 before addition\n        s += np.float32(x)\n    return float(s)\n\ndef pairwise64_sum(terms: np.ndarray) -> float:\n    \"\"\"Pairwise recursive summation in double precision.\"\"\"\n    def _pairwise_recursive(arr: np.ndarray, start: int, end: int) -> float:\n        n = end - start\n        if n = 32:\n            return naive64_sum(arr[start:end])\n        mid = start + n // 2\n        return _pairwise_recursive(arr, start, mid) + _pairwise_recursive(arr, mid, end)\n    \n    return _pairwise_recursive(terms, 0, len(terms))\n\ndef blocked64_sum(terms: np.ndarray) - float:\n    \"\"\"Blocked summation in double precision.\"\"\"\n    B = 1024\n    n = len(terms)\n    if n == 0:\n        return 0.0\n    \n    num_blocks = (n + B - 1) // B\n    block_sums = np.zeros(num_blocks, dtype=np.float64)\n    \n    for i in range(num_blocks):\n        start = i * B\n        end = min((i + 1) * B, n)\n        block_sums[i] = naive64_sum(terms[start:end])\n        \n    return naive64_sum(block_sums)\n\ndef kahan64_sum(terms: np.ndarray) - float:\n    \"\"\"Kahan compensated summation in double precision.\"\"\"\n    s = 0.0\n    c = 0.0\n    for x in terms:\n        y = x - c\n        t = s + y\n        c = (t - s) - y\n        s = t\n    return s\n\ndef solve():\n    \"\"\"Main function to run the test suite and print results.\"\"\"\n    test_cases = [\n        {\n            'nu': 3.0,\n            'u_mags': [1e-8, 1e-4, 1e0, 1e4, 1e8],\n            'R': 2000,\n            'ordering': 'clustered'\n        },\n        {\n            'nu': 1.1,\n            'u_mags': [1e-12, 1e-6, 1e-3, 1e-1, 1e1, 1e3, 1e6],\n            'R': 1000,\n            'ordering': 'clustered'\n        },\n        {\n            'nu': 300.0,\n            'u_mags': [1e-12, 1e-9, 1e-6, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e3, 1e6],\n            'R': 500,\n            'ordering': 'clustered'\n        },\n        {\n            'nu': 3.0,\n            'u_mags': [1e-8, 1e-4, 1e0, 1e4, 1e8],\n            'R': 2000,\n            'ordering': 'interleaved'\n        },\n    ]\n\n    all_tests_results = []\n    \n    summation_functions = [\n        naive64_sum,\n        naive32_sum,\n        pairwise64_sum,\n        blocked64_sum,\n        kahan64_sum\n    ]\n    \n    for case in test_cases:\n        nu = case['nu']\n        u_mags = case['u_mags']\n        R = case['R']\n        ordering = case['ordering']\n\n        residuals = generate_residuals(u_mags, R, ordering)\n        \n        # Use np.float64 for explicit double precision\n        gradient_terms = np.array([g(r, nu) for r in residuals], dtype=np.float64)\n        \n        current_case_errors = []\n        for func in summation_functions:\n            computed_sum = func(gradient_terms)\n            # True sum is 0, so error is abs(computed_sum)\n            error = abs(computed_sum)\n            current_case_errors.append(error)\n            \n        all_tests_results.append(current_case_errors)\n\n    # Format the final output as a comma-separated list of lists with no spaces.\n    print(repr(all_tests_results).replace(\" \", \"\"))\n\nsolve()\n\n```"
        },
        {
            "introduction": "除了精度损失，数值计算还可能因上溢（overflow）或下溢（underflow）而完全失败，此时计算结果会超出机器所能表示的最大或最小数值范围。本练习将通过检验欧几里得范数（Euclidean norm）的计算来应对这一挑战，该计算是许多地球物理算法的基石。你将推导并实现一个基于缩放的稳健算法，它能够巧妙地避免中间过程的上溢和下溢，确保即使对于分量跨越多个数量级的向量也能得到可靠的结果 。",
            "id": "3596774",
            "problem": "考虑在双精度浮点运算中评估一个实值向量的欧几里得范数的任务，该运算在计算地球物理学中广泛用于振幅归一化和最小二乘残差评估等操作。设一个向量表示为 $x = (x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n$，其欧几里得范数定义为\n$$\n\\|x\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}.\n$$\n当向量元素跨越多个数量级时，朴素地计算 $\\sum_{i=1}^n x_i^2$ 然后取平方根，在双精度运算中可能会发生上溢或下溢。\n\n假设运算遵循电气和电子工程师协会（IEEE）浮点算术标准（IEEE 754），基数 $\\beta = 2$，精度 $p = 53$（binary64，也称为双精度），并采用舍入到最近、偶数优先的规则。设单位舍入为 $u = \\tfrac{1}{2}\\beta^{1-p} = 2^{-53}$。假设指数范围有限，最小正规格化数约为 $2.225\\times 10^{-308}$，最大有限数约为 $1.798\\times 10^{308}$，因此可能发生下溢至非规格化表示。在整个过程中，当对舍入进行建模时，使用标准浮点模型：对于基本运算 $\\mathrm{op}\\in\\{+,-,\\times,\\div\\}$，\n$$\n\\mathrm{fl}(a \\ \\mathrm{op}\\ b) = (a \\ \\mathrm{op}\\ b)\\,(1+\\delta), \\quad |\\delta| \\le u,\n$$\n对于平方根运算，\n$$\n\\mathrm{fl}(\\sqrt{y}) = \\sqrt{y}\\,(1+\\delta), \\quad |\\delta| \\le u,\n$$\n只要精确结果在可表示范围内。定义量\n$$\n\\gamma_k = \\frac{k u}{1 - k u}\n$$\n对于非负整数 $k$ 且 $k u  1$。\n\n你必须设计并分析一个基于缩放的算法，以便在存在跨越多个数量级的元素时稳健地计算 $\\|x\\|_2$。你的任务包括以下步骤：\n\n1.  仅从 $\\|x\\|_2$ 的定义和上述浮点模型出发，推导出一个缩放策略，确保在精确的 $\\|x\\|_2$ 可以在binary64中表示的情况下，所有中间运算都保持在可表示范围内。推导必须纯粹从第一性原理出发，不引入任何快捷公式。解释为什么此策略可以避免中间步骤中的上溢和下溢。\n\n2.  通过建立一个将所维护的缩放因子和累积的平方和与精确量 $\\|x\\|_2$ 联系起来的循环不变量，来证明你的算法的正确性。你的证明不得依赖于除上述指定浮点模型之外的外部引理。\n\n3.  为计算出的范数 $\\widehat{\\|x\\|_2}$ 推导一个前向误差界，形式为\n    $$\n    \\frac{\\left|\\widehat{\\|x\\|_2} - \\|x\\|_2\\right|}{\\|x\\|_2} \\le C(n) u + \\mathcal{O}(u^2),\n    $$\n    其中 $C(n)$ 是一个关于 $n$ 的显式、简单的函数。你必须展示 $C(n)$ 是如何通过计算算法中的浮点运算次数并应用所述模型得出的，并且必须用 $\\gamma_k$ 来表示该界。\n\n4.  在一个完整的、可运行的程序中实现你的算法。该实现必须遵循你的推导并使用双精度算术。程序必须使用你的缩放算法为下面的测试套件中的每个向量评估 $\\|x\\|_2$，并输出结果。\n\n测试套件是以下向量集合，每个向量都经过明确指定，以测试不同的数值范围。所有值都是无量纲实数：\n-   混合数量级的理想情况：\n    $$\n    x^{(1)} = \\left[\\,1.0,\\ 10^{-16},\\ 10^{16},\\ -10^{-3},\\ 3.141592653589793,\\ -2.718281828,\\ 10^{-200},\\ -10^{200}\\,\\right].\n    $$\n-   接近上溢的情形，精确结果可表示：\n    $$\n    x^{(2)} = \\left[\\,10^{308},\\ 10^{308}\\,\\right].\n    $$\n-   接近下溢的情形，包括非规格化数：\n    $$\n    x^{(3)} = \\left[\\,10^{-320},\\ -10^{-320},\\ 2\\cdot 10^{-320},\\ -3\\cdot 10^{-320}\\,\\right].\n    $$\n-   全零（边界情况）：\n    $\n    x^{(4)} = \\left[\\,0.0,\\ 0.0,\\ 0.0\\,\\right].\n    $\n-   由极大项主导的混合极端值：\n    $\n    x^{(5)} = \\left[\\,10^{308},\\ 10^{-320},\\ 3.0,\\ -4.0,\\ 10^{-150}\\,\\right].\n    $\n\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。结果必须是按顺序排列的五个计算出的范数，每个都是一个浮点数。例如，输出格式必须完全如下：\n$$\n[\\text{结果}_1,\\text{结果}_2,\\text{结果}_3,\\text{结果}_4,\\text{结果}_5].\n$$",
            "solution": "任务是设计、分析并实现一个稳健的算法，用于在双精度浮点运算中计算向量 $x \\in \\mathbb{R}^n$ 的欧几里得范数 $\\|x\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}$。分析必须按照问题陈述中阐明的第一性原理进行。\n\n### 1. 基于缩放算法的推导\n\n朴素计算 $\\|x\\|_2$ 的方法是先求元素平方和 $\\sum_{i=1}^n x_i^2$，然后取平方根。在浮点运算中，即使最终的范数 $\\|x\\|_2$ 完全可以表示，中间的平方运算 $x_i^2$ 也很容易在 $|x_i|$ 很大（例如 $10^{155}$）时导致上溢，或在 $|x_i|$ 很小（例如 $10^{-155}$）时导致有害的下溢。例如，如果 $x = (10^{200}, 10^{200})$，$\\|x\\|_2 = \\sqrt{2} \\cdot 10^{200}$ 是可表示的，但中间项 $x_i^2 = 10^{400}$ 会上溢，超出了双精度数的近似范围（最高可达 $1.8 \\times 10^{308}$）。\n\n为防止这种情况，我们引入一个缩放因子。关键恒等式是：\n$$\n\\|x\\|_2 = s \\cdot \\sqrt{\\sum_{i=1}^n \\left(\\frac{x_i}{s}\\right)^2} \\quad \\text{对于任何 } s  0.\n$$\n一个双遍算法可以先找出最大绝对值 $s = \\max_i |x_i|$，然后在第二遍中计算缩放后向量 $y = x/s$ 的平方和。在这种情况下，$y$ 的所有元素都满足 $|y_i| \\le 1$，所以对它们进行平方不会导致上溢。\n\n通过在遍历向量元素时动态更新缩放因子，可以开发出更高效的单遍算法。我们维护一个动态缩放因子（称之为 $s$）和一个动态平方和（称之为 $\\sigma$）。在任何时刻，状态 $(s, \\sigma)$ 代表了到目前为止已处理子向量的范数。\n\n让我们形式化这个单遍算法。我们初始化一个缩放变量 $s \\leftarrow 0$ 和一个平方和变量 $\\sigma \\leftarrow 0$。我们遍历元素 $x_i$，其中 $i=1, \\dots, n$。对于每个非零元素 $x_i$，我们令 $a = |x_i|$ 并按如下方式更新 $s$ 和 $\\sigma$：\n\n1.  **初始化**：如果 $s=0$（即，这是遇到的第一个非零元素），我们将初始缩放因子设为 $s \\leftarrow a$，并将平方和初始化为 $\\sigma \\leftarrow 1$。\n\n2.  **缩放更新**：如果 $s  a$，当前元素 $a$ 的量级大于迄今为止见过的任何元素。我们必须采用 $a$ 作为新的缩放因子。当前累积的平方范数为 $s^2\\sigma$。新的平方范数将是 $s^2\\sigma + a^2$。为防止上溢，我们提出新的缩放因子 $a$：\n    $$\n    s^2\\sigma + a^2 = a^2 \\left( \\sigma \\left(\\frac{s}{a}\\right)^2 + 1 \\right).\n    $$\n    因此，我们通过设置新的平方和 $\\sigma \\leftarrow 1 + \\sigma(s/a)^2$ 和新的缩放因子 $s \\leftarrow a$ 来更新状态。比率 $|s/a|  1$，这可以防止其平方运算上溢。\n\n3.  **求和**：如果 $s \\ge a$，当前的缩放因子 $s$ 足够大。当前累积的平方范数为 $s^2\\sigma$。新的平方范数为 $s^2\\sigma + a^2$。我们提出已有的缩放因子 $s$：\n    $$\n    s^2\\sigma + a^2 = s^2 \\left( \\sigma + \\left(\\frac{a}{s}\\right)^2 \\right).\n    $$\n    因此，我们更新平方和 $\\sigma \\leftarrow \\sigma + (a/s)^2$，而缩放因子 $s$ 保持不变。比率 $|a/s| \\le 1$，同样可以防止上溢。\n\n遍历所有元素后，最终范数计算为 $s \\sqrt{\\sigma}$。如果输入向量只包含零，缩放因子 $s$ 将保持为 $0$，结果正确地为 $0$。\n\n**为什么这个策略是稳健的：**\n-   **避免上溢**：所有除法都涉及一个量级较小或相等的数除以一个量级较大的数。除法的结果总是在 $[-1, 1]$ 范围内。对该值进行平方得到的结果在 $[0, 1]$ 范围内，不会上溢。平方和 $\\sigma$ 可能增长，但它是有界的，$1 \\le \\sigma \\le n$。对于任何实际的向量维度 $n$，$\\sigma$ 都不会上溢。最终的乘法 $s \\cdot \\sqrt{\\sigma}$ 只有在真实范数 $\\|x\\|_2$ 本身不可表示时才会上溢，这是正确的行为。\n-   **下溢管理**：如果 $|x_i|$ 远小于当前缩放因子 $s$，比率 $|x_i/s|$ 将非常小，其平方可能会下溢到 $0$。这被称为“良性下溢”。这意味着 $x_i^2$ 的贡献与最大元素的平方范数相比可以忽略不计。丢失这样一个小数项对最终结果引入的相对误差非常小，这是为稳健性付出的可接受的代价。\n\n### 2. 正确性证明\n\n我们使用循环不变量在精确算术中证明算法的正确性。设 $(s_k, \\sigma_k)$ 是处理完前 $k$ 个元素 $(x_1, \\dots, x_k)$ 后的缩放因子和平方和变量的状态。我们将全零向量作为平凡情况处理，因此假设至少有一个非零元素。\n\n**循环不变量**：对于 $k \\ge 1$，其中 $x_k$ 是从一个非零子向量 $(x_1, \\dots, x_k)$ 中处理的最后一个元素：\n1.  $s_k = \\max_{i=1}^k |x_i|$.\n2.  $\\|(x_1, \\dots, x_k)\\|_2^2 = s_k^2 \\sigma_k$.\n\n**基础情况**：设 $x_j$ 是第一个非零元素。处理它之后，状态变为 $(s_j, \\sigma_j)$。算法设置 $s_j = |x_j|$ 和 $\\sigma_j = 1$。\n1.  $s_j = |x_j| = \\max_{i=1}^j |x_i|$ 成立。\n2.  $\\|(x_1, \\dots, x_j)\\|_2^2 = x_j^2 = |x_j|^2 \\cdot 1 = s_j^2 \\sigma_j$ 成立。\n不变量在基础情况成立。\n\n**归纳步骤**：假设不变量在第 $k$ 步成立。我们处理元素 $x_{k+1}$ 并证明不变量对于新状态 $(s_{k+1}, \\sigma_{k+1})$ 成立。令 $a = |x_{k+1}|$。根据归纳假设，$\\|(x_1, \\dots, x_k)\\|_2^2 = s_k^2 \\sigma_k$。目标平方范数是 $\\|(x_1, \\dots, x_{k+1})\\|_2^2 = \\|(x_1, \\dots, x_k)\\|_2^2 + x_{k+1}^2 = s_k^2 \\sigma_k + a^2$。\n\n**情况 1: $s_k  a$**\n算法更新 $s_{k+1} \\leftarrow a$ 和 $\\sigma_{k+1} \\leftarrow 1 + \\sigma_k (s_k/a)^2$。\n1.  $s_{k+1} = a = |x_{k+1}|$。由于 $s_k = \\max_{i=1}^k |x_i|$ 且 $s_k  |x_{k+1}|$，我们有 $s_{k+1} = \\max_{i=1}^{k+1} |x_i|$。\n2.  新的计算平方范数是 $s_{k+1}^2 \\sigma_{k+1} = a^2 \\left(1 + \\sigma_k \\left(\\frac{s_k}{a}\\right)^2\\right) = a^2 + a^2\\sigma_k\\frac{s_k^2}{a^2} = a^2 + s_k^2 \\sigma_k$。这与目标平方范数相匹配。\n\n**情况 2: $s_k \\ge a$**\n算法更新 $s_{k+1} \\leftarrow s_k$ 和 $\\sigma_{k+1} \\leftarrow \\sigma_k + (a/s_k)^2$。\n1.  $s_{k+1} = s_k$。由于 $s_k = \\max_{i=1}^k |x_i|$ 且 $s_k \\ge |x_{k+1}|$，我们有 $s_{k+1} = \\max_{i=1}^{k+1} |x_i|$。\n2.  新的计算平方范数是 $s_{k+1}^2 \\sigma_{k+1} = s_k^2 \\left(\\sigma_k + \\left(\\frac{a}{s_k}\\right)^2\\right) = s_k^2 \\sigma_k + s_k^2\\frac{a^2}{s_k^2} = s_k^2 \\sigma_k + a^2$。这也与目标平方范数相匹配。\n\n由于不变量在基础情况和归纳步骤中都成立，因此它对所有从 $1$ 到 $n$ 的 $k$ 都成立。在循环结束时（$k=n$），我们有 $s_n = \\max_{i=1}^n |x_i|$ 和 $\\|x\\|_2^2 = s_n^2 \\sigma_n$。取平方根，我们得到 $\\|x\\|_2 = s_n \\sqrt{\\sigma_n}$，这正是算法作为其最后一步计算的结果。这证明了算法在精确算术中的正确性。\n\n### 3. 前向误差界\n\n我们为计算出的范数 $\\widehat{\\|x\\|_2}$ 推导一个前向误差界。由于状态依赖的更新，单遍算法的分析很复杂。一个简化但有见地的分析可以在等效的双遍算法上进行，该算法具有相似的数值属性。\n\n1.  **缩放**：令 $s = \\max_i |x_i|$。该值是 $|x_i|$ 之一，并假设在浮点数中可以精确表示。\n2.  **除法**：对每个 $i$，我们计算 $y_i = x_i/s$。浮点运算给出 $\\hat{y}_i = \\mathrm{fl}(x_i/s) = (x_i/s)(1+\\delta_i)$，其中 $|\\delta_i| \\le u$。\n3.  **平方**：我们计算 $\\hat{z}_i = \\mathrm{fl}(\\hat{y}_i^2) = \\hat{y}_i^2(1+\\epsilon_i)$。代入 $\\hat{y}_i$，我们得到 $\\hat{z}_i = (x_i/s)^2(1+\\delta_i)^2(1+\\epsilon_i) = z_i(1 + 2\\delta_i + \\epsilon_i + \\mathcal{O}(u^2))$，其中 $z_i=(x_i/s)^2$。计算每个平方项 $\\hat{z}_i$ 的相对误差界为 $3u + \\mathcal{O}(u^2)$。\n4.  **求和**：我们计算和 $\\hat{\\sigma} = \\mathrm{fl}(\\sum_{i=1}^n \\hat{z}_i)$。使用浮点求和的标准模型，计算出的和是 $\\hat{\\sigma} = \\sum_{i=1}^n \\hat{z}_i(1+\\theta_i)$，其中 $|\\theta_i| \\le \\gamma_{n-1}$ 是一个保守的界（更紧的界取决于求和的顺序）。\n5.  **$\\sigma$ 中的误差**：结合这些误差，$\\hat{\\sigma} = \\sum_{i=1}^n z_i(1+3u+\\mathcal{O}(u^2))(1+\\gamma_{n-1})$。令 $\\sigma = \\sum z_i$。$\\hat{\\sigma}$ 中的相对误差为：\n    $$\n    \\frac{|\\hat{\\sigma} - \\sigma|}{\\sigma} = \\frac{|\\sum z_i(1+\\eta_i) - \\sum z_i|}{\\sum z_i} \\le \\max_i |\\eta_i| \\approx 3u + \\gamma_{n-1} \\approx (3+n-1)u = (n+2)u.\n    $$\n    所以我们可以写成 $\\hat{\\sigma} = \\sigma (1 + \\Delta_\\sigma)$，其中 $|\\Delta_\\sigma| \\le \\gamma_{n+2} + \\mathcal{O}(u^2)$。\n6.  **平方根**：我们计算 $\\hat{\\rho} = \\mathrm{fl}(\\sqrt{\\hat{\\sigma}}) = \\sqrt{\\hat{\\sigma}}(1+\\delta_\\rho)$，$|\\delta_\\rho| \\le u$。\n    $$\n    \\hat{\\rho} = \\sqrt{\\sigma(1+\\Delta_\\sigma)}(1+\\delta_\\rho) = \\sqrt{\\sigma}\\sqrt{1+\\Delta_\\sigma}(1+\\delta_\\rho) \\approx \\sqrt{\\sigma}(1+\\Delta_\\sigma/2)(1+\\delta_\\rho) \\approx \\sqrt{\\sigma}(1+\\Delta_\\sigma/2 + \\delta_\\rho).\n    $$\n7.  **最终乘法**：最终结果是 $\\widehat{\\|x\\|_2} = \\mathrm{fl}(s \\cdot \\hat{\\rho}) = s\\hat{\\rho}(1+\\delta_m)$，$|\\delta_m| \\le u$。\n    $$\n    \\widehat{\\|x\\|_2} \\approx s\\sqrt{\\sigma}(1 + \\Delta_\\sigma/2 + \\delta_\\rho + \\delta_m) = \\|x\\|_2 (1 + \\Delta_\\sigma/2 + \\delta_\\rho + \\delta_m).\n    $$\n总相对误差因此有界：\n$$\n\\frac{|\\widehat{\\|x\\|_2} - \\|x\\|_2|}{\\|x\\|_2} \\le \\frac{|\\Delta_\\sigma|}{2} + |\\delta_\\rho| + |\\delta_m| \\le \\frac{\\gamma_{n+2}}{2} + u + u.\n$$\n对于小的 $n u$，$\\gamma_{n+2} \\approx (n+2)u$。该界变为：\n$$\n\\frac{|\\widehat{\\|x\\|_2} - \\|x\\|_2|}{\\|x\\|_2} \\le \\frac{(n+2)u}{2} + 2u = \\left(\\frac{n}{2} + 1 + 2\\right)u = \\frac{n+6}{2} u + \\mathcal{O}(u^2).\n$$\n因此，我们找到了一个显式函数 $C(n) = \\frac{n+6}{2}$。推导出的界表明该算法是数值稳定的，误差随向量维度 $n$ 线性增长。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of robustly computing the Euclidean norm for a suite of test vectors.\n    \"\"\"\n\n    def robust_euclidean_norm(x: np.ndarray) - np.float64:\n        \"\"\"\n        Computes the Euclidean norm of a vector using a one-pass, scaling-based algorithm\n        to prevent unnecessary overflow and underflow.\n\n        This algorithm is based on the principle of dynamically maintaining a scale factor\n        and a sum of squares. It is equivalent to LAPACK's 'xnrm2' routine.\n\n        Args:\n            x: A numpy array of float64 values.\n\n        Returns:\n            The computed Euclidean norm as a float64.\n        \"\"\"\n        if x.size == 0:\n            return np.float64(0.0)\n\n        # Initialize scale and sum_sq. Using the LAPACK 'dnrm2' initialization.\n        # scale is the largest absolute value encountered so far.\n        # sum_sq is the sum of squares of elements scaled by 1/scale.\n        scale = np.float64(0.0)\n        sum_sq = np.float64(1.0)\n        \n        # Another initialization strategy also works:\n        # scale = np.float64(0.0)\n        # sum_sq = np.float64(0.0)\n        # This requires an explicit check for scale == 0.0 inside the loop.\n        # The LAPACK method combines initialization and update more seamlessly.\n        \n        for val in x:\n            # We must use Python's built-in abs() or numpy's for intermediate\n            # calculations to maintain precision before casting to float64 if needed.\n            abs_val = np.abs(val)\n            \n            if abs_val  0:\n                if scale  abs_val:\n                    # Current element is larger than the current scale.\n                    # Rescale the sum of squares and update the scale.\n                    # The ratio is guaranteed to be  1.\n                    ratio = scale / abs_val\n                    sum_sq = np.float64(1.0) + sum_sq * (ratio * ratio)\n                    scale = abs_val\n                else:\n                    # Current element is smaller than or equal to the scale.\n                    # Add its contribution to the sum of squares.\n                    # The ratio is guaranteed to be = 1.\n                    ratio = abs_val / scale\n                    sum_sq = sum_sq + (ratio * ratio)\n\n        # After the loop, the norm is scale * sqrt(sum_sq).\n        # If the vector was all zeros, scale remains 0.0.\n        # Multiplying by 0.0 gives the correct result of 0.0.\n        return scale * np.sqrt(sum_sq)\n\n    # Define the test cases from the problem statement as numpy arrays of double precision floats.\n    test_cases = [\n        np.array([1.0, 1e-16, 1e16, -1e-3, 3.141592653589793, -2.718281828, 1e-200, -1e200], dtype=np.float64),\n        np.array([1e308, 1e308], dtype=np.float64),\n        np.array([1e-320, -1e-320, 2e-320, -3e-320], dtype=np.float64),\n        np.array([0.0, 0.0, 0.0], dtype=np.float64),\n        np.array([1e308, 1e-320, 3.0, -4.0, 1e-150], dtype=np.float64),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = robust_euclidean_norm(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # We use a custom formatting to avoid scientific notation for some numbers\n    # and ensure the output matches typical floating point representations.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}