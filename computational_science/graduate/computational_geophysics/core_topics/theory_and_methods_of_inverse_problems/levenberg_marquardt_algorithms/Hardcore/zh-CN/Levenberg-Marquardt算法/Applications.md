## 应用与交叉学科联系

在前面的章节中，我们已经详细探讨了 Levenberg-Marquardt (LM) 算法的数学原理和核心机制。我们了解到，该算法通过一个自适应的阻尼参数 $\lambda$，巧妙地在[梯度下降法](@entry_id:637322)和[高斯-牛顿法](@entry_id:173233)之间进行插值，从而为求解[非线性](@entry_id:637147)最小二乘问题提供了一个既稳健又高效的“主力”方法。现在，我们将[超越理论](@entry_id:203777)层面，深入探索 LM 算法在广阔的科学与工程领域中的实际应用。本章的目的不是重复介绍核心概念，而是展示这些概念如何在多样化、跨学科的真实世界问题中得到应用、扩展和整合。我们将看到，无论是反演地球的内部结构，还是在机器人视觉中重建三维场景，LM 算法及其变体都扮演着不可或缺的角色。

### [地球物理反演](@entry_id:749866)中的基础应用

[地球物理反演](@entry_id:749866)旨在利用地表或钻孔中观测到的物理场数据（如地震波、重力、[电磁场](@entry_id:265881)等），推断地下介质的物理属性[分布](@entry_id:182848)。这些问题本质上都是[非线性反问题](@entry_id:752643)，因此 LM 算法成为了该领域内应用最广泛的优化工具之一。

#### [地震层析成像](@entry_id:754649)：旅行时反演

[地震层析成像](@entry_id:754649)是[地球物理学](@entry_id:147342)中的一个经典问题，其目标是通过分析[地震波](@entry_id:164985)的旅行时来重建地下的速度结构。在一个简化的模型中，我们可以将地下划分为若干个速度恒定的网格单元。对于一条穿过这些单元的地震射线，其总旅行时 $t$ 是路径长度 $L_{ij}$ 与相应单元慢度 $s_j = 1/v_j$（速度 $v_j$ 的倒数）的乘[积之和](@entry_id:266697)。这构建了一个[非线性](@entry_id:637147)正演模型 $t(\mathbf{m}) = \mathbf{g}(\mathbf{m})$，其中模型向量 $\mathbf{m}$ 包含了各单元的速度（或慢度）。反演的目标是寻找一组模型参数 $\mathbf{m}$，使得预测的旅行时与观测数据 $t^{\text{obs}}$ 之间的加权[残差平方和](@entry_id:174395)最小。

LM 算法为解决这一问题提供了迭代方案。在每次迭代中，我们首先需要计算正演模型关于模型参数的灵敏度，即[雅可比矩阵](@entry_id:264467) $\mathbf{J}$。对于旅行时反演，雅可比矩阵的元素 $J_{ij} = \frac{\partial t_i}{\partial m_j}$ 可以通过对正演公式求导直接得到，其值与射线路径和单元速度有关。例如，当模型参数为速度 $v_j$ 时，导数为 $J_{ij} = -L_{ij}/v_j^2$。有了雅可比矩阵和当前模型的残差，LM 算法便构建一个阻尼化的正规方程组 $(\mathbf{J}^\top \mathbf{J} + \lambda \mathbf{I})\delta \mathbf{m} = \mathbf{J}^\top \mathbf{r}$ 来求解模型更新量 $\delta \mathbf{m}$。阻尼参数 $\lambda$ 的大小决定了更新步长的特性：当 $\lambda$ 很小时，算法接近[高斯-牛顿法](@entry_id:173233)，利用了问题的二阶曲率信息，收敛较快；当 $\lambda$ 很大时，算法退化为[梯度下降法](@entry_id:637322)，步长较小但能保证目标函数的下降，从而提高了在强[非线性](@entry_id:637147)区域的稳定性。通过在一个简单的合成速度模型上进行单步 LM 更新，可以清晰地观察到，一个合适的 $\lambda$ 值能够有效降低[数据失配](@entry_id:748209)，而一个过大的 $\lambda$ 值则会使更新步长变得微乎其微，几乎不改变模型 。

#### [位场](@entry_id:143025)方法：[重力反演](@entry_id:750042)与正则化

与[地震学](@entry_id:203510)不同，重力勘探等[位场](@entry_id:143025)方法通常可以被线性化，其正演关系可以表示为 $\mathbf{d} = \mathbf{G}\mathbf{m}$，其中 $\mathbf{d}$ 是[重力异常](@entry_id:750038)数据，$\mathbf{m}$ 是地下密度异常模型，$\mathbf{G}$ 是由[格林函数](@entry_id:147802)和几何形状决定的正演算子（或称灵敏度矩阵）。尽管是线性问题，但[重力反演](@entry_id:750042)是典型的[不适定问题](@entry_id:182873) (ill-posed problem)，主要体现在解的非唯一性和不稳定性。例如，深部的大尺度密度异常和浅部的小尺度密度异常可能产生相似的重力响应，特别是对于长波长成分，数据对其约束非常弱。

直接求解[最小二乘问题](@entry_id:164198) $\min_{\mathbf{m}} \|\mathbf{d} - \mathbf{G}\mathbf{m}\|_2^2$ 往往会导致解被噪声严重污染，并产生不符合地质实际的剧烈[振荡](@entry_id:267781)。为了获得稳定且有意义的解，必须引入正则化。Tikhonov 正则化是一种常用方法，它在最小二乘[目标函数](@entry_id:267263)中加入一个模型惩罚项，形式为 $\min_{\mathbf{m}} \|\mathbf{d} - \mathbf{G}\mathbf{m}\|_2^2 + \mu \|\mathbf{m}\|_2^2$。有趣的是，这个正则化问题的解恰好等价于从零初始模型出发，使用 LM 算法求解[非线性](@entry_id:637147)问题进行一次迭代的解，其正规方程为 $(\mathbf{G}^\top \mathbf{G} + \mu \mathbf{I})\mathbf{m} = \mathbf{G}^\top \mathbf{d}$。

在这里，LM 算法中的阻尼参数 $\mu$ 扮演了[正则化参数](@entry_id:162917)的角色，它控制着数据拟合项和模型惩罚项之间的权衡。一个很小的 $\mu$ 值意味着我们更相信数据，但可能放大由数据噪声和模型不敏感性（即 $\mathbf{G}^\top \mathbf{G}$ 的小[特征值](@entry_id:154894)或零[特征值](@entry_id:154894)）引起的伪影。相反，一个很大的 $\mu$ 值会强制解的范数很小，从而抑制那些数据约束弱的解分量（例如，深部长波长结构），使得反演结果更加平滑和稳定，但可能牺牲对细节的刻画。通过一个合成的[重力梯度](@entry_id:181198)反演实验可以证明，选择不同的 $\mu$ 值会显著影响对浅层目标体和深部长波长背景场的恢复效果。较小的 $\mu$ 可能在恢复浅层目标的同时引入来自深部背景场的伪影，而较大的 $\mu$ 则能有效压制这些伪影，但可能导致浅层目标的振幅被低估 。

### 大规模反演中的高等技术与实践挑战

随着计算能力的提升，[地球物理反演](@entry_id:749866)的模型参数量可达数百万甚至数十亿。在这种大规模背景下，标准的 LM 算法面临着巨大的计算和存储挑战。因此，一系列高等技术应运而生，以使得 LM 算法在这些复杂问题中依然行之有效。

#### 约束条件的整合：[内点法](@entry_id:169727)

在许多地球物理问题中，模型参数具有明确的物理边界。例如，速度、密度等参数必须为正。在反演中施加这些约束，不仅符合物理现实，还能缩小解空间，改善反演的[适定性](@entry_id:148590)。将这些[不等式约束](@entry_id:176084)（如 $m_j \ge m_{\min, j}$）整合到 LM 框架中的一种有效方法是[内点法](@entry_id:169727)（或称[障碍函数](@entry_id:168066)法）。

该方法通过向原始的最小二乘[目标函数](@entry_id:267263)中添加一个[对数障碍](@entry_id:144309)项来改造目标函数，例如 $\Phi_{\mu}(\mathbf{m}) = \Phi_{\text{LS}}(\mathbf{m}) - \mu \sum_j \log(m_j - m_{\min,j})$。这个障碍项在可行域内部是良定义的，但在接近边界时会趋于无穷大，从而“阻止”迭代解穿越边界。反演过程在一个递减的障碍参数 $\mu$ 序列的控制下进行。对于每一个固定的 $\mu$，算法在[可行域](@entry_id:136622)内部求解一个无约束（但包含障碍项）的[非线性](@entry_id:637147)最小二乘问题。LM 算法在此处再次发挥作用，用于求解这个子问题。此时，LM 迭代步的计算需要考虑障碍项的梯度和（对角）Hessian 矩阵，并结合[线搜索策略](@entry_id:636391)确保每一步更新都保持在[可行域](@entry_id:136622)的严格内部。随着 $\mu$ 逐渐趋向于零，通过[内点](@entry_id:270386) LM 算法得到的一系列解会收敛到原始约束问题的解 。

#### [偏微分方程](@entry_id:141332)约束问题的计算效率：伴随状态法

在如[全波形反演](@entry_id:749622)（FWI）或频率域电磁（EM）反演等更复杂的问题中，正演模型由一个[偏微分方程](@entry_id:141332)（PDE）描述，例如[亥姆霍兹方程](@entry_id:149977)或波动方程。对于这类问题，模型参数 $\mathbf{m}$ 的数量（通常是百万级以上）远大于数据量。这意味着[雅可比矩阵](@entry_id:264467) $\mathbf{J}$ 是一个“矮胖”矩阵，其尺寸巨大，显式地计算和存储它变得完全不可行。

然而，LM 算法的迭代步求解 $(\mathbf{J}^\top \mathbf{J} + \lambda \mathbf{I})\delta \mathbf{m} = \mathbf{J}^\top \mathbf{r}$ 并不需要 $\mathbf{J}$ 本身，而只需要有能力计算 $\mathbf{J}$ 与向量的乘积（$\mathbf{J}\mathbf{v}$）以及 $\mathbf{J}^\top$ 与向量的乘积（$\mathbf{J}^\top\mathbf{w}$）。其中，计算梯度项 $\mathbf{J}^\top \mathbf{r}$ 是每一步迭代都必需的。伴随状态法（adjoint-state method）提供了一种极为高效的方式来计算 $\mathbf{J}^\top \mathbf{r}$ 这一项，其计算成本与一次正演求解相当，而与模型参数的数量无关。

该方法通过引入一个伴随场（adjoint field）$\mathbf{v}$ 来实现，该伴随场由一个伴随 PDE 定义，其[源项](@entry_id:269111)是数据残差。通过求解这个伴随 PDE 得到伴随场后，梯度 $-\mathbf{J}^\top \mathbf{r}$ 就可以通过伴随场和正演场的[内积](@entry_id:158127)来计算。例如，在频率域电[磁反演](@entry_id:751628)中，$(-\mathbf{J}^\top \mathbf{r})$ 的第 $j$ 个分量可以表示为关于正演[电场](@entry_id:194326) $\mathbf{u}$、伴随[电场](@entry_id:194326) $\mathbf{v}$ 以及算子对 $m_j$ 导数的积分。这种方法避免了对每个模型参数进行一次正演来计算雅可比矩阵的列，从而将梯度计算的复杂度从与模型参数数量成正比降低到常数级别。同样，计算 $\mathbf{J}^\top \mathbf{J} \delta \mathbf{m}$（在[共轭梯度](@entry_id:145712)等迭代求解器中需要）也可以通过两次正演和两次伴随求解来实现。因此，对于一个包含 $N_s$ 个源和 $N_f$ 个频率的 PDE 约束问题，使用伴随状态法和 $N_{cg}$ 次共轭梯度迭代的单次 LM 更新，总共需要约 $2(N_{cg}+1)N_s N_f$ 次 PDE 求解 。

#### 真实数据的处理：[稳健估计](@entry_id:261282)

真实的地球物理数据几乎总是被非高斯噪声和离群值（outliers）污染。标准的最小二乘[目标函数](@entry_id:267263)对这些离群值非常敏感，一个大的离群值会产生巨大的残差平方，从而不成比例地影响模型更新。为了提高反演的稳健性，可以使用 M-估计（M-estimation）框架，将[目标函数](@entry_id:267263)从二次型（$L_2$ 范数）惩罚替换为对大残差增长较慢的稳健惩罚函数，例如 Huber [损失函数](@entry_id:634569)。

Huber [损失函数](@entry_id:634569)在一个由阈值 $c$ 定义的区间内表现为二次型，而在区间外则表现为线性（$L_1$ 范数），从而限制了离群值的影响。将这种稳健[目标函数](@entry_id:267263)整合到 LM 算法中，可以通过迭代重加权最小二乘（Iteratively Reweighted Least Squares, IRLS）实现。在每次 LM 迭代中，根据当前残差的大小，为每个数据点计算一个权重。对于小于阈值 $c$ 的残差，权重为 1；对于大于 $c$ 的残差，权重则小于 1，且随着残差增大而减小。

关键在于如何选择阈值 $c$。一个有原则的选择是使其与残差[分布](@entry_id:182848)的稳健尺度估计（robust scale estimate）相适应。一个常用的尺度估计是[中位数绝对偏差](@entry_id:167991)（Median Absolute Deviation, MAD）。通过在每次 LM 迭代中计算[标准化残差](@entry_id:634169)的 MAD，并将其乘以一个适当的常数（例如，1.4826，使其在理想高斯分布下与[标准差](@entry_id:153618)等价），可以得到一个对离群值不敏感的尺度估计 $s$。然后，将 Huber 阈值设置为 $c = k \cdot s$，其中常数 $k$（例如，1.345）被选择用于在未污染的高斯数据上实现高[统计效率](@entry_id:164796)（如 95%），同时有效抑制离群值的影响。这种自适应地在每次迭代中更新阈值 $c$ 的策略，使得算法能够动态地识别和降权离群值，极大地提高了反演过程在面对真实、复杂噪声时的稳健性 。

### 挑战极限：强[非线性](@entry_id:637147)与[多物理场](@entry_id:164478)问题

LM 算法作为一个局部[优化方法](@entry_id:164468)，在面对强[非线性](@entry_id:637147)或[多物理场耦合](@entry_id:171389)问题时会遇到其固有的局限性。然而，通过与其他先进策略相结合，其应用范围可以被极大地扩展。

#### [全波形反演](@entry_id:749622)：[周期跳跃](@entry_id:748134)问题与[全局化策略](@entry_id:177837)

[全波形反演](@entry_id:749622)（Full Waveform Inversion, FWI）是地震勘探中分辨率最高但计算也最昂贵的技术之一。它试图通过匹配整个[地震波](@entry_id:164985)形（而不仅仅是旅行时）来反演高精度的地下介质模型。FWI 的[目标函数](@entry_id:267263)景观是高度[非线性](@entry_id:637147)的，充满了大量的[局部极小值](@entry_id:143537)。

LM 算法（以及其他局部[优化方法](@entry_id:164468)）在 FWI 中面临的最大挑战是“[周期跳跃](@entry_id:748134)”（cycle-skipping）。如果初始模型的预测波形与观测波形之间的相位差超过半个周期（约 $\pi$ [弧度](@entry_id:171693)），那么基于一阶线性化（即雅可比矩阵）计算出的梯度方向将是错误的，它会引导模型更新走向一个邻近的局部极小值，而不是[全局最优解](@entry_id:175747)。在这种情况下，LM 算法的信任域策略会检测到[线性模型](@entry_id:178302)预测的下降与实际下降不符，从而急剧增加阻尼参数 $\lambda$，导致步长变得极小，算法陷入停滞。

为了克服[周期跳跃](@entry_id:748134)，必须采用“全局化”（globalization）策略来扩大[全局最小值](@entry_id:165977)的吸引盆（basin of attraction），使得一个合理的初始模型也能落入其中。常用的策略包括：
1.  **多尺度/频率延拓法**：反演从低频数据开始，逐渐过渡到高频数据。因为相位误差与频率成正比，低频数据不易产生[周期跳跃](@entry_id:748134)，可以帮助构建一个平滑的、[运动学](@entry_id:173318)上正确的背景模型。这个模型随后作为更高频率反演的初始模型，逐级逼近高分辨率解  。
2.  **改造[目标函数](@entry_id:267263)**：设计对[相位误差](@entry_id:162993)不那么敏感的新型[目标函数](@entry_id:267263)。例如，基于瞬时相位、波形包络或最优传输理论（如二次 Wasserstein 距离）的[目标函数](@entry_id:267263)，可以在更大范围的相位失配下提供正确的梯度信息，从而“凸化”目标函数景观，引导 LM 算法走向正确的解 。

#### [联合反演](@entry_id:750950)：[多模态数据](@entry_id:635386)集成

为了降低[地球物理反演](@entry_id:749866)的非唯一性，一个强大的策略是[联合反演](@entry_id:750950)（joint inversion），即同时使用多种不同物理属性的观测数据来约束一个共享的地[球模型](@entry_id:161388)。例如，可以[联合反演](@entry_id:750950)地震旅行时数据（对速度敏感）和重力数据（对密度敏感）。

在 LM 框架下实施[联合反演](@entry_id:750950)，需要构建一个增广的[目标函数](@entry_id:267263)，它包含了所有数据类型的失配项以及可能的正则化项。例如，一个[联合反演](@entry_id:750950)地震慢度 $\mathbf{s}$ 和密度 $\rho$ 的目标函数可以写为：
$$ \Phi(\mathbf{s}, \boldsymbol{\rho}) = \|\mathbf{W}_t(\mathbf{t}(\mathbf{s}) - \mathbf{t}^{\text{obs}})\|_2^2 + \|\mathbf{W}_g(\mathbf{g}(\boldsymbol{\rho}) - \mathbf{g}^{\text{obs}})\|_2^2 + \text{regularization} $$
其中 $\mathbf{W}_t$ 和 $\mathbf{W}_g$ 是各自的[数据加权](@entry_id:635715)矩阵，用于平衡不同单位和量级的数据。[雅可比矩阵](@entry_id:264467)也相应地变为一个[分块矩阵](@entry_id:148435)，其对角块分别对应于各地震和重力数据对相应参数的灵敏度，而非对角块则为零（除非存在[交叉](@entry_id:147634)灵敏度）。此外，还可以通过正则化项引入岩石物理约束，例如假设密度和慢度之间存在某种（弱）[线性关系](@entry_id:267880) $\boldsymbol{\rho} \approx a\mathbf{s} + b$，这会在近似 Hessian 矩阵中引入非对角耦合项。为了在 LM 更新中妥善处理不同物理参数的巨大尺度差异，采用分区或[对角缩放](@entry_id:748382)的阻尼策略至关重要 。

### 交叉学科联系与普适原理

Levenberg-Marquardt 算法的威力远不止于[地球物理学](@entry_id:147342)。它作为求解[非线性](@entry_id:637147)最小二乘问题的通用框架，在众多科学和工程领域中都找到了关键应用，并且不同领域中的挑战与解决方案常常表现出惊人的结构相似性。

#### [机器人学](@entry_id:150623)与计算机视觉：同步定位与建图 (SLAM)

在机器人学和计算机视觉中，同步定位与建图（Simultaneous Localization and Mapping, SLAM）是一个核心问题：一个移动的机器人（或相机）在未知环境中，需要利用传感器读数（如相机图像、[激光雷达](@entry_id:192841)扫描）同时构建环境的地图，并确定自身在该地图中的位置。这可以被建模为一个大规模的[非线性](@entry_id:637147)[最小二乘问题](@entry_id:164198)。

在基于[图优化](@entry_id:261938)的 SLAM 中，机器人的位姿（位置和姿态）和环境中的路标点（landmarks）被表示为图中的节点。传感器测量（如两个位姿之间的[相对运动](@entry_id:169798)，或一个位姿对一个路标点的观测）则构成图的边，每条边都对应一个残差项，表示测量值与根据当前节点状态预测的值之间的差异。整个问题就是调整所有节点（位姿和路标点）的位置，以最小化所有残差的加权平方和。

这个问题被称为捆绑调整（Bundle Adjustment），其目标函数结构与[地球物理反演](@entry_id:749866)极其相似。由于每个测量仅涉及少数几个节点（通常是一个或两个位姿和一个路标点），其[雅可比矩阵](@entry_id:264467)是高度稀疏的。LM 算法是解决 BA 问题最常用和最成功的方法。为了处理其巨大的规模（成千上万的位姿和数百万的点），必须利用其[稀疏结构](@entry_id:755138)。通过舒尔补（Schur complement）技巧，可以从 LM 的[正规方程](@entry_id:142238)中解析地消去数量庞大的路标点参数，从而得到一个只涉及位姿参数的、规模大大减小但通常更稠密的“简约相机系统”（reduced camera system）。求解这个简约系统后，再反代回求解路标点更新。这种利用问题特定[稀疏结构](@entry_id:755138)的策略，是使 LM 算法能够高效求解大规模 SLAM 问题的关键 。将 SLAM 的位姿[图优化](@entry_id:261938)与基于图的旅行时层析成像进行对比，可以发现两者在数学结构上的深刻联系，并且都受益于相似的、考虑了参数敏感度差异的阻尼策略 。

#### 气象学与医学成像：从[数据同化](@entry_id:153547)到[光学层析](@entry_id:193648)

LM 算法的应用还延伸到其他看似不相关的领域。在[数值天气预报](@entry_id:191656)中，四维[变分数据同化](@entry_id:756439)（4D-Var）旨在通过融合一段时间内稀疏的观测数据，来寻找一个物理上一致的最佳初始大气状态，以便进行未来的预报。这个过程可以被构建为一个大规模的[非线性](@entry_id:637147)[最小二乘问题](@entry_id:164198)，其目标是最小化初始状态与背景预测（先验）的偏差，以及模式演化轨迹与实际观测的偏差。LM 算法可以被用来求解这个[优化问题](@entry_id:266749)，找到最优的[初始条件](@entry_id:152863) 。

在[生物医学工程](@entry_id:268134)中，[扩散光学层析成像](@entry_id:748405)（Diffuse Optical Tomography, DOT）利用近红外光穿过生物组织（如脑部或乳腺）来成像其光学特性（如吸收和散射系数）。光的传播过程可以用扩散方程来近似描述，这与地球物理中的[电磁感应](@entry_id:181154)或热传导等扩散过程在数学上是相似的。因此，DOT 的[图像重建](@entry_id:166790)问题也是一个由 PDE 约束的[非线性反问题](@entry_id:752643)，同样面临着灵敏度随深度衰减、问题不适定等挑战。从地球物理（如跨孔电磁法）中借鉴的[正则化方法](@entry_id:150559)、多尺度策略（使用不同波长的光）以及高效的 LM 实施方案，都可以在 DOT 中找到直接的应用和对应 。

#### 跨领域反问题中的结构相似性

不同科学领域中的[反问题](@entry_id:143129)，尽管其物理背景迥异，但常常在数学结构上表现出共性，从而面临相似的挑战。例如，一个简单的气候[能量平衡模型](@entry_id:195903)，试图通过观测到的全球温度时间序列来估计气候系统的有效[热容](@entry_id:137594) $C$ 和反馈参数 $\lambda$。当外部[辐射强迫](@entry_id:155289) $F(t)$ 变化缓慢时，模型响应主要由 $\lambda$ 决定，而对 $C$ 的敏感度极低。这导致了参数 $C$ 和 $\lambda$ 之间的严重权衡，使得雅可比矩阵的列向量近似[线性相关](@entry_id:185830)，其 Hessian 矩阵病态。这种情况与[重力反演](@entry_id:750042)中著名的“深度-密度模糊”现象形成了深刻的类比：长波长的[重力异常](@entry_id:750038)无法唯一地区分深部的大密度体和浅部的小密度体。在这两个看似无关的问题中，都是由于输入信号（[辐射强迫](@entry_id:155289)或重力源）缺乏高频（短波长）分量，导致无法解析系统内部控制瞬态响应（[热容](@entry_id:137594) $C$）或空间分布（深度）的参数 。同样，[地下水](@entry_id:201480)流问题和[电磁感应](@entry_id:181154)问题都由[扩散](@entry_id:141445)型方程控制，因此它们的反演都受到[扩散核](@entry_id:204628)的平滑效应影响，导致对[高分辨率结构](@entry_id:197416)不敏感，从而需要相似的正则化和[多尺度反演](@entry_id:752259)策略 。

### 阻尼的艺术：参数缩放与[步长控制](@entry_id:755439)

贯穿以上所有应用，一个共同的主题是 LM 算法的成功在很大程度上取决于如何巧妙地选择和调整阻尼项。标准的 LM 算法使用 $(\mathbf{J}^\top \mathbf{J} + \lambda \mathbf{I})$ 作为其核心矩阵，其中阻尼项是 $\lambda \mathbf{I}$。然而，对于许多真实世界的问题，特别是那些涉及不同物理单位或灵敏度差异巨大的参数的[联合反演](@entry_id:750950)问题，这种“单位矩阵阻尼”策略是次优的，甚至是错误的。

问题的关键在于，单位矩阵阻尼在参数空间中定义了一个各向同性的（球形的）信任域。如果模型参数的量级或单位相差悬殊（例如，一个参数是米，另一个是秒/米），或者数据对不同参数的敏感度差异巨大，那么一个球形的信任域就不再有意义。这会导致算法的更新步长严重偏向于那些名义上数值较小或灵敏度较高的参数，而其他参数则几乎不被更新，使得收敛过程异常缓慢或停滞。

Marquardt 本人在其 1963 年的开创性论文中就提出了一个更优越的策略：**[对角缩放](@entry_id:748382)**。其核心思想是使用一个对角的[缩放矩阵](@entry_id:188350) $\mathbf{S}$ 来代替单位矩阵 $\mathbf{I}$，使得阻尼项变为 $\lambda \mathbf{S}$。一个特别有效且广泛使用的选择是令 $\mathbf{S} = \text{diag}(\mathbf{J}^\top \mathbf{J})$，即近似 Hessian 矩阵的对角[线元](@entry_id:196833)素。LM 系统变为：
$$ (\mathbf{J}^\top \mathbf{J} + \lambda \text{diag}(\mathbf{J}^\top \mathbf{J})) \delta \mathbf{m} = \mathbf{J}^\top \mathbf{r} $$
这种策略的深刻之处在于，它使得阻尼的大小与每个参数的敏感度成正比。$\text{diag}(\mathbf{J}^\top \mathbf{J})$ 的对角元 $(\mathbf{J}^\top \mathbf{J})_{ii}$ 正是第 $i$ 个参数对数据[残差平方和](@entry_id:174395)影响的度量。因此，[对角缩放](@entry_id:748382)对敏感度高的参数施加了更强的（绝对）阻尼，而对敏感度低的参数施加了较弱的阻尼。这相当于在参数空间中定义了一个椭球形的信任域，其轴的长度与参数的敏感度成反比，从而自动地、无量纲地平衡了所有参数的更新步长。无论是在包含不同物理量的[联合反演](@entry_id:750950) ，还是在具有复杂连接性的 SLAM [图优化](@entry_id:261938)  中，这种自适应的、与灵敏度相关的[对角缩放](@entry_id:748382)策略都是实现稳健高效收敛的关键，它确保了算法的性能不受参数单位和量级的[人为选择](@entry_id:168356)的影响  。