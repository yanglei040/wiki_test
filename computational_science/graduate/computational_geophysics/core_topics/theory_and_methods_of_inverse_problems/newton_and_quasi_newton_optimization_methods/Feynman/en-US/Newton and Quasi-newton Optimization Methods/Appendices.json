{
    "hands_on_practices": [
        {
            "introduction": "To fully appreciate Newton's method, we begin with its ideal application: minimizing a quadratic function. This exercise demonstrates the method's remarkable efficiency in this setting, revealing that it can find the exact minimizer in a single step. By working through this foundational case , you will gain a core understanding of why Newton's method is considered the benchmark for optimization speed and how it relates to the local curvature of the objective function.",
            "id": "3611910",
            "problem": "In linearized seismic travel-time tomography, a common quadratic approximation to the data misfit with Tikhonov regularization is modeled as an objective function of the form $J(m)=\\tfrac{1}{2} m^{\\top} H m - b^{\\top} m$, where $m \\in \\mathbb{R}^{n}$ represents the model parameters, $H \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite matrix encoding the normal equations and regularization curvature, and $b \\in \\mathbb{R}^{n}$ encodes the projected data. Assume $H \\succ 0$. Consider applying Newton’s method to minimize $J(m)$ starting from an arbitrary initial guess $m_{0} \\in \\mathbb{R}^{n}$.\n\nUsing only the standard definitions of gradient and Hessian for a scalar field and the second-order Taylor expansion that underpins Newton’s method, perform the following:\n\n- Derive the Newton search direction $p_{0}$ at $m_{0}$ for $J(m)$ by minimizing the second-order Taylor model of $J$ around $m_{0}$.\n- From $p_{0}$, form the updated iterate $m_{1} = m_{0} + p_{0}$ and prove that $m_{1}$ equals the unique global minimizer of $J(m)$ and is independent of $m_{0}$.\n- Provide the final explicit expression for $m_{1}$ in terms of $H$ and $b$ only.\n\nYour final answer must be a single closed-form analytic expression for $m_{1}$. Do not include units. Do not provide any intermediate expressions. Do not round or approximate your answer.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- Objective function: $J(m)=\\tfrac{1}{2} m^{\\top} H m - b^{\\top} m$\n- Model parameters: $m \\in \\mathbb{R}^{n}$\n- Matrix $H$: $H \\in \\mathbb{R}^{n \\times n}$, symmetric positive definite ($H \\succ 0$)\n- Vector $b$: $b \\in \\mathbb{R}^{n}$\n- Initial guess: $m_{0} \\in \\mathbb{R}^{n}$\n- Method: Newton's method\n- Task 1: Derive the Newton search direction $p_{0}$ at $m_{0}$ by minimizing the second-order Taylor model of $J(m)$ around $m_{0}$.\n- Task 2: Form the updated iterate $m_{1} = m_{0} + p_{0}$ and prove that $m_{1}$ is the unique global minimizer of $J(m)$.\n- Task 3: Provide the final expression for $m_{1}$ in terms of $H$ and $b$ only.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard optimization problem from numerical linear algebra, commonly encountered in inverse problems like seismic tomography.\n- **Scientifically Grounded:** The problem uses a quadratic objective function, which is a cornerstone of linearized inverse theory and optimization. The concepts of gradient, Hessian, and Newton's method are fundamental and correctly applied. The premise is scientifically and mathematically sound.\n- **Well-Posed:** The objective function $J(m)$ is a strictly convex function. This is guaranteed because its Hessian, the matrix $H$, is given as symmetric positive definite. A strictly convex function defined on $\\mathbb{R}^{n}$ has a unique global minimizer. The problem asks to find this minimizer, so it is well-posed.\n- **Objective:** The problem is stated using precise mathematical language, with no ambiguity or subjective elements.\n\nThe problem is free of the listed flaws. It is complete, consistent, and formalizable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe objective function to be minimized is a quadratic function of the model parameters $m$:\n$$\nJ(m) = \\frac{1}{2} m^{\\top} H m - b^{\\top} m\n$$\nwhere $m \\in \\mathbb{R}^{n}$, $H \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite matrix, and $b \\in \\mathbb{R}^{n}$.\n\nNewton's method iteratively finds the minimum of a function by creating a quadratic model of the function at the current iterate and then minimizing this model. The step that minimizes the model is taken to find the next iterate.\n\nFirst, we compute the gradient and the Hessian of the objective function $J(m)$. Using standard rules of vector calculus, the gradient of $J(m)$ with respect to $m$ is:\n$$\n\\nabla J(m) = \\frac{1}{2} (H^{\\top}m + Hm) - b\n$$\nSince $H$ is symmetric ($H = H^{\\top}$), the gradient simplifies to:\n$$\n\\nabla J(m) = \\frac{1}{2} (2Hm) - b = Hm - b\n$$\nThe Hessian of $J(m)$ is the gradient of $\\nabla J(m)$:\n$$\n\\nabla^2 J(m) = \\nabla (\\nabla J(m)) = \\nabla (Hm - b) = H\n$$\nThe Hessian is the constant matrix $H$.\n\nNewton's method approximates $J(m)$ around an iterate $m_k$ using a second-order Taylor expansion. Let $p = m - m_k$ be the step from the current iterate $m_k$. The second-order Taylor model of $J$ at $m_k$ is:\n$$\nJ(m_k + p) \\approx J(m_k) + \\nabla J(m_k)^{\\top} p + \\frac{1}{2} p^{\\top} \\nabla^2 J(m_k) p\n$$\nFor a quadratic function like $J(m)$, this expansion is not an approximation but is exact. Let this quadratic model for the step $p$ be denoted by $Q_k(p)$. At the initial guess $m_{0}$, the model is:\n$$\nQ_{0}(p) = J(m_{0}) + \\nabla J(m_{0})^{\\top} p + \\frac{1}{2} p^{\\top} \\nabla^2 J(m_{0}) p\n$$\nSubstituting the expressions for the gradient and Hessian evaluated at $m_{0}$:\n$$\n\\nabla J(m_{0}) = Hm_{0} - b\n$$\n$$\n\\nabla^2 J(m_{0}) = H\n$$\nThe model becomes:\n$$\nQ_{0}(p) = J(m_{0}) + (Hm_{0} - b)^{\\top} p + \\frac{1}{2} p^{\\top} H p\n$$\nThe Newton search direction $p_{0}$ is the vector $p$ that minimizes this quadratic model $Q_{0}(p)$. To find this minimum, we take the gradient of $Q_{0}(p)$ with respect to $p$ and set it to the zero vector.\n$$\n\\nabla_p Q_{0}(p) = (Hm_{0} - b) + Hp\n$$\nSetting the gradient to zero to find the optimal step $p_{0}$:\n$$\nHp_{0} + (Hm_{0} - b) = 0\n$$\n$$\nHp_{0} = -(Hm_{0} - b) = b - Hm_{0}\n$$\nSince $H$ is positive definite ($H \\succ 0$), it is invertible. We can solve for $p_{0}$ by multiplying by the inverse of $H$:\n$$\np_{0} = H^{-1}(b - Hm_{0})\n$$\nThis is the Newton search direction at the initial point $m_{0}$.\n\nThe next step is to compute the new iterate $m_{1}$ and show that it is the global minimizer. The Newton update rule is $m_{1} = m_{0} + p_{0}$.\n$$\nm_{1} = m_{0} + p_{0} = m_{0} + H^{-1}(b - Hm_{0})\n$$\nDistributing $H^{-1}$:\n$$\nm_{1} = m_{0} + H^{-1}b - H^{-1}Hm_{0}\n$$\nSince $H^{-1}H = I$, where $I$ is the identity matrix:\n$$\nm_{1} = m_{0} + H^{-1}b - Im_{0} = m_{0} + H^{-1}b - m_{0}\n$$\n$$\nm_{1} = H^{-1}b\n$$\nThis expression for $m_{1}$ is independent of the initial guess $m_{0}$. To prove that $m_{1}$ is the unique global minimizer of $J(m)$, we must verify the first-order and second-order conditions for optimality.\n\nThe first-order necessary condition for a minimum is that the gradient of the objective function at that point is zero. Let's evaluate $\\nabla J(m)$ at $m = m_{1}$:\n$$\n\\nabla J(m_{1}) = H m_{1} - b\n$$\nSubstituting $m_{1} = H^{-1}b$:\n$$\n\\nabla J(m_{1}) = H(H^{-1}b) - b = (HH^{-1})b - b = Ib - b = b - b = 0\n$$\nThe gradient at $m_{1}$ is indeed the zero vector.\n\nThe second-order sufficient condition for a unique global minimum is that the Hessian of the objective function is positive definite. The Hessian of $J(m)$ is $\\nabla^2 J(m) = H$. By the problem statement, $H$ is a symmetric positive definite matrix ($H \\succ 0$).\n\nSince the gradient of $J(m)$ is zero at $m_{1}$ and its Hessian is positive definite everywhere, the point $m_{1} = H^{-1}b$ is the unique global minimizer of the strictly convex function $J(m)$. This result demonstrates that for a quadratic objective function, Newton's method finds the exact minimizer in a single iteration, regardless of the starting point $m_{0}$.\n\nThe final expression for $m_{1}$, the unique global minimizer, is solely in terms of $H$ and $b$.",
            "answer": "$$\n\\boxed{H^{-1} b}\n$$"
        },
        {
            "introduction": "While Newton's method is powerful, its direct application is often impossible for large-scale geophysical problems where forming and storing the Hessian matrix is computationally prohibitive. This practice explores the practical consequences of using a popular alternative, the Limited-memory BFGS (L-BFGS) algorithm. By calculating the memory footprint of L-BFGS , you will confront the real-world trade-off between algorithmic accuracy, which improves with more stored information, and hardware limitations, providing a crucial perspective for designing large-scale inversions.",
            "id": "3611901",
            "problem": "A large-scale Full Waveform Inversion (FWI) in computational seismology seeks to minimize a data misfit objective function by iteratively updating a model with $N$ parameters using a quasi-Newton method. In the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm, the approximation to the inverse Hessian is built from a fixed number $m$ of recent correction pairs: step vectors $s_{i} = x_{i+1} - x_{i}$ and gradient differences $y_{i} = g_{i+1} - g_{i}$, each stored in double precision. Assume the following scientifically realistic conditions:\n\n- The parameter dimension is $N = 10^{8}$.\n- Double precision arrays store $8$ bytes per entry.\n- The L-BFGS implementation stores $m$ correction pairs $\\{s_{i}, y_{i}\\}_{i=1}^{m}$, each of length $N$, the current model $x \\in \\mathbb{R}^{N}$, the current gradient $g \\in \\mathbb{R}^{N}$, and one additional working vector of length $N$ used during the two-loop recursion. It also stores two arrays of $m$ scalars used in the two-loop recursion.\n- The available memory budget is $64$ gigabytes, and gigabytes (GB) are defined as $10^{9}$ bytes.\n\nAs a first-principles model for curvature fidelity, suppose the spectrum of the Gauss-Newton Hessian in this inversion is strongly ill-conditioned with a power-law decay $\\lambda_{i} = \\lambda_{1} i^{-\\alpha}$, with $\\alpha = 1.5$, and define the curvature fidelity of an $m$-pair L-BFGS approximation by the fraction of the trace captured,\n$$\nF(m) = \\frac{\\sum_{i=1}^{m} \\lambda_{i}}{\\sum_{i=1}^{\\infty} \\lambda_{i}} = \\frac{\\sum_{i=1}^{m} i^{-\\alpha}}{\\sum_{i=1}^{\\infty} i^{-\\alpha}},\n$$\nwhich illustrates diminishing returns as $m$ increases. Use this to qualitatively discuss the trade-off between increasing $m$ (which improves curvature fidelity) and memory usage (which increases linearly with $m$).\n\nTask:\n1. From the definitions above, derive an analytical expression for the total memory $M(m)$ in GB required by L-BFGS with $m$ correction pairs, including all arrays explicitly listed.\n2. Under the $64$ GB memory budget, determine the maximal integer number of correction pairs $m^{\\star}$ such that $M(m^{\\star}) \\leq 64$ GB.\n\nYour final reported quantity must be the integer $m^{\\star}$ only. No rounding to significant figures is necessary; report the exact integer. Express any intermediate memory expressions in gigabytes (GB) using $1\\,\\mathrm{GB} = 10^{9}$ bytes.",
            "solution": "The problem will first be validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Topic:** Newton and quasi-Newton optimization methods in computational geophysics.\n- **Algorithm:** Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS).\n- **Model dimension:** $N = 10^{8}$ parameters.\n- **Data storage:** Double precision arrays store $8$ bytes per entry.\n- **L-BFGS stored items:**\n    1.  $m$ correction pairs $\\{s_{i}, y_{i}\\}_{i=1}^{m}$, where $s_i, y_i \\in \\mathbb{R}^{N}$.\n    2.  Current model vector $x \\in \\mathbb{R}^{N}$.\n    3.  Current gradient vector $g \\in \\mathbb{R}^{N}$.\n    4.  One additional working vector of length $N$.\n    5.  Two arrays of $m$ scalars.\n- **Memory budget:** $64$ gigabytes (GB), where $1\\,\\text{GB} = 10^{9}$ bytes.\n- **Curvature fidelity model (for context):** $F(m) = \\frac{\\sum_{i=1}^{m} i^{-\\alpha}}{\\sum_{i=1}^{\\infty} i^{-\\alpha}}$ with $\\alpha = 1.5$.\n- **Tasks:**\n    1.  Derive an analytical expression for the total memory $M(m)$ in GB.\n    2.  Determine the maximal integer $m^{\\star}$ such that $M(m^{\\star}) \\leq 64$ GB.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem is set in the context of Full Waveform Inversion, a standard and computationally intensive problem in geophysics. L-BFGS is a widely used algorithm for such large-scale optimization problems. The parameters given, such as model size ($N=10^8$) and memory constraints ($64$ GB), are realistic for high-performance computing scenarios. The memory accounting is a standard exercise in scientific computing. The curvature model is a plausible, albeit simplified, representation of the trade-off between memory and accuracy in quasi-Newton methods. The problem does not violate any scientific principles.\n- **Well-Posed:** The problem provides all necessary data to formulate and solve for the required quantities. The tasks are specific and lead to a unique integer solution.\n- **Objective:** The problem is stated in precise, quantitative, and unbiased language.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, objective, and self-contained. The solution process will now proceed.\n\n### Solution Derivation\n\nThe solution involves two parts as specified in the problem statement. First, we derive the expression for the total memory consumption, $M(m)$, as a function of the number of stored correction pairs, $m$. Second, we use this expression and the given memory budget to calculate the maximum allowed integer value for $m$, denoted as $m^{\\star}$.\n\n**Part 1: Analytical Expression for Memory Usage $M(m)$**\n\nThe total memory required is the sum of the memory used by all stored components. Each component is an array of double-precision numbers, where each number occupies $8$ bytes.\n\nThe components and their sizes in terms of the number of double-precision entries are:\n1.  **Correction pairs:** There are $m$ pairs of vectors, $\\{s_{i}, y_{i}\\}$. Each vector has a dimension of $N$. This amounts to $2 \\times m$ vectors, each of length $N$.\n    - Number of entries: $2 \\times m \\times N = 2mN$.\n2.  **Current model vector:** The vector $x$ has length $N$.\n    - Number of entries: $N$.\n3.  **Current gradient vector:** The vector $g$ has length $N$.\n    - Number of entries: $N$.\n4.  **Additional working vector:** One vector of length $N$ is used.\n    - Number of entries: $N$.\n5.  **Scalar arrays:** Two arrays, each storing $m$ scalars. We assume these scalars are also stored as double-precision numbers for consistency, which is standard practice.\n    - Number of entries: $2 \\times m = 2m$.\n\nThe total number of double-precision values to be stored, let's call it $D(m)$, is the sum of all these entries:\n$$D(m) = 2mN + N + N + N + 2m$$\n$$D(m) = 2mN + 3N + 2m$$\nThis can be factored in terms of $m$ and $N$:\n$$D(m) = (2N + 2)m + 3N$$\nThe total memory usage in bytes is $8 \\times D(m)$, since each double-precision value requires $8$ bytes.\n$$\\text{Memory in bytes} = 8 \\times ((2N + 2)m + 3N)$$\nThe problem requires the memory expression $M(m)$ in gigabytes (GB), with the definition $1\\,\\text{GB} = 10^{9}$ bytes.\n$$M(m) = \\frac{8 \\times ((2N + 2)m + 3N)}{10^9}$$\nThis is the analytical expression for the total memory in GB.\n\n**Part 2: Calculation of the Maximal Number of Correction Pairs $m^{\\star}$**\n\nWe are given a memory budget of $64$ GB. We must find the maximum integer $m = m^{\\star}$ that satisfies the condition $M(m^{\\star}) \\leq 64$.\n$$\\frac{8 \\times ((2N + 2)m + 3N)}{10^9} \\leq 64$$\nWe are given $N = 10^{8}$. Substituting this value into the inequality:\n$$\\frac{8 \\times ((2 \\times 10^8 + 2)m + 3 \\times 10^8)}{10^9} \\leq 64$$\nTo solve for $m$, we first simplify the inequality. Divide both sides by $8$:\n$$\\frac{(2 \\times 10^8 + 2)m + 3 \\times 10^8}{10^9} \\leq 8$$\nMultiply both sides by $10^9$:\n$$(2 \\times 10^8 + 2)m + 3 \\times 10^8 \\leq 8 \\times 10^9$$\nIsolate the term containing $m$:\n$$(2 \\times 10^8 + 2)m \\leq 8 \\times 10^9 - 3 \\times 10^8$$\nTo facilitate the subtraction on the right-hand side, we write $8 \\times 10^9$ as $80 \\times 10^8$:\n$$(2 \\times 10^8 + 2)m \\leq 80 \\times 10^8 - 3 \\times 10^8$$\n$$(2 \\times 10^8 + 2)m \\leq (80 - 3) \\times 10^8$$\n$$(2 \\times 10^8 + 2)m \\leq 77 \\times 10^8$$\nNow, solve for $m$:\n$$m \\leq \\frac{77 \\times 10^8}{2 \\times 10^8 + 2}$$\nLet's evaluate the fraction:\n$$m \\leq \\frac{7,700,000,000}{200,000,002}$$\nPerforming the division:\n$$m \\leq 38.499999615...$$\nSince $m$ must be an integer representing the number of correction pairs, we must take the floor of this value.\n$$m^{\\star} = \\lfloor 38.499999615... \\rfloor = 38$$\nTherefore, the maximal integer number of correction pairs that can be stored within the $64$ GB memory budget is $38$.\nThe qualitative context about curvature fidelity $F(m)$ illustrates that while a larger $m$ is generally better for the optimization algorithm's convergence rate, it comes at a linear cost in memory. The value $m^{\\star}=38$ represents the hard limit imposed by the available hardware resources in this specific scenario.",
            "answer": "$$\\boxed{38}$$"
        },
        {
            "introduction": "Geophysical inverse problems are notoriously ill-posed, meaning small errors in data can lead to large, unphysical artifacts in the solution. This exercise investigates how Tikhonov regularization, a standard technique to combat this issue, impacts the conditioning of the Newton system. By analyzing a simplified spectral model , you will learn how to choose the regularization parameter $\\lambda$ to balance the data misfit and the regularization term, ultimately leading to a more stable and well-conditioned optimization problem.",
            "id": "3611922",
            "problem": "Consider linear gravity inversion in which the forward operator maps a discretized subsurface density vector to surface gravity anomalies. The objective function is the Tikhonov-regularized least squares functional\n$$\nJ(m) = \\frac{1}{2}\\|G m - d\\|^{2} + \\frac{\\lambda}{2}\\|D m\\|^{2},\n$$\nwhere $m$ is the model, $G$ is the linear forward operator, $d$ is the observed data, $D$ is a linear regularization operator (e.g., a discrete gradient), and $\\lambda$ is a dimensionless regularization weight. Starting from the definition of $J(m)$ and the Newton method for unconstrained optimization, derive the Newton system and the conditioning of the corresponding curvature (Hessian) operator as a function of $\\lambda$. Assume the discretization and boundary conditions render $G^{\\top}G$ and $D^{\\top}D$ symmetric positive semidefinite and commuting, so that they share an orthonormal basis of eigenvectors. In a simplified two-mode spectral model representative of dominant curvature directions in gravity inversion, suppose $G^{\\top}G$ has eigenvalues $a_{1} = 100$ and $a_{2} = 1$, while $D^{\\top}D$ has eigenvalues $b_{1} = 0.01$ and $b_{2} = 10$ along the same two eigenvectors.\n\nUsing these assumptions, construct the condition number $\\kappa(\\lambda)$ of the Newton system as the ratio of its largest to smallest eigenvalue, and, by analytically balancing the data misfit curvature and the regularization curvature across the two principal modes, determine the value of $\\lambda$ that minimizes $\\kappa(\\lambda)$. Express the final answer as a unitless number and round your answer to four significant figures.",
            "solution": "### Problem Validation\nThe problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Objective function: $J(m) = \\frac{1}{2}\\|G m - d\\|^{2} + \\frac{\\lambda}{2}\\|D m\\|^{2}$\n- $G$ is a linear forward operator, $D$ is a linear regularization operator, $d$ is data, $\\lambda$ is a regularization weight.\n- Assumptions: $G^{\\top}G$ and $D^{\\top}D$ are symmetric positive semidefinite and commute.\n- Spectral model: Eigenvalues for $G^{\\top}G$ are $a_1=100, a_2=1$. Eigenvalues for $D^{\\top}D$ are $b_1=0.01, b_2=10$. The eigenvalue pairs $(a_1, b_1)$ and $(a_2, b_2)$ correspond to the same two eigenvectors.\n- Task: Find the value of $\\lambda \\ge 0$ that minimizes the condition number $\\kappa(\\lambda)$ of the Newton system's Hessian matrix.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem uses a standard Tikhonov-regularized least-squares objective function, which is fundamental to solving ill-posed inverse problems in geophysics and other fields. The analysis of the Hessian's condition number via its spectral properties is a core concept in numerical optimization and analysis. The assumption of commuting operators, while a simplification, allows for an analytical solution that illustrates the core principle of regularization. The setup is scientifically sound.\n- **Well-Posed:** The problem is a well-defined mathematical optimization task: find the minimizer of a function $\\kappa(\\lambda)$. All necessary information is provided.\n- **Objective:** The problem is stated with precise mathematical definitions and values, with no subjective components.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution is derived below.\n\n### Solution Derivation\nThe objective function is $J(m) = \\frac{1}{2}\\|G m - d\\|^{2} + \\frac{\\lambda}{2}\\|D m\\|^{2}$. We first derive the Hessian matrix, which is the matrix of the Newton system for this quadratic optimization problem.\n\nExpanding the norms (using $\\|v\\|^2 = v^\\top v$):\n$$\nJ(m) = \\frac{1}{2}(Gm-d)^\\top(Gm-d) + \\frac{\\lambda}{2}(Dm)^\\top(Dm)\n$$\n$$\nJ(m) = \\frac{1}{2}(m^\\top G^\\top G m - 2d^\\top G m + d^\\top d) + \\frac{\\lambda}{2} m^\\top D^\\top D m\n$$\nThe gradient with respect to $m$ is:\n$$\n\\nabla J(m) = G^\\top G m - G^\\top d + \\lambda D^\\top D m = (G^\\top G + \\lambda D^\\top D)m - G^\\top d\n$$\nThe Hessian of $J(m)$, denoted $H(\\lambda)$, is the derivative of the gradient with respect to $m$:\n$$\nH(\\lambda) = \\nabla^2 J(m) = G^\\top G + \\lambda D^\\top D\n$$\nThis Hessian matrix is the curvature operator for the Newton system. The condition number, $\\kappa(H(\\lambda))$, is the ratio of its largest eigenvalue to its smallest eigenvalue.\n\nSince $G^\\top G$ and $D^\\top D$ are symmetric and commute, they are simultaneously diagonalizable, meaning they share a common basis of eigenvectors. Let $v_i$ be a common eigenvector with corresponding eigenvalues $a_i$ for $G^\\top G$ and $b_i$ for $D^\\top D$. Then the eigenvalues of $H(\\lambda)$, denoted $\\mu_i(\\lambda)$, are given by:\n$$\nH(\\lambda)v_i = (G^\\top G + \\lambda D^\\top D)v_i = G^\\top G v_i + \\lambda D^\\top D v_i = a_i v_i + \\lambda b_i v_i = (a_i + \\lambda b_i)v_i\n$$\nSo, $\\mu_i(\\lambda) = a_i + \\lambda b_i$.\n\nUsing the given two-mode spectral model:\n- For the first mode: $a_1=100, b_1=0.01$. The eigenvalue is $\\mu_1(\\lambda) = 100 + 0.01\\lambda$.\n- For the second mode: $a_2=1, b_2=10$. The eigenvalue is $\\mu_2(\\lambda) = 1 + 10\\lambda$.\n\nThe condition number of the Hessian is the ratio of the largest to the smallest of these two eigenvalues:\n$$\n\\kappa(\\lambda) = \\frac{\\max(\\mu_1(\\lambda), \\mu_2(\\lambda))}{\\min(\\mu_1(\\lambda), \\mu_2(\\lambda))}\n$$\nSince eigenvalues $a_i, b_i$ are from positive semidefinite matrices and $\\lambda \\ge 0$, the eigenvalues $\\mu_i(\\lambda)$ are non-negative. The condition number is always $\\kappa(\\lambda) \\ge 1$. The minimum possible value is 1, which occurs if and only if all eigenvalues are equal. We seek the value of $\\lambda$ that makes the two eigenvalues equal, which will minimize the condition number.\n$$\n\\mu_1(\\lambda) = \\mu_2(\\lambda)\n$$\n$$\n100 + 0.01\\lambda = 1 + 10\\lambda\n$$\nSolving for $\\lambda$:\n$$\n100 - 1 = 10\\lambda - 0.01\\lambda\n$$\n$$\n99 = 9.99\\lambda\n$$\n$$\n\\lambda = \\frac{99}{9.99} = \\frac{9900}{999}\n$$\nDividing the numerator and denominator by their greatest common divisor, 9:\n$$\n\\lambda = \\frac{1100}{111}\n$$\nNow, we calculate the numerical value:\n$$\n\\lambda \\approx 9.9099099...\n$$\nThe problem asks to round the answer to four significant figures. The fifth significant figure is 9, so we round up the fourth.\n$$\n\\lambda \\approx 9.910\n$$\nAt this value of $\\lambda$, both eigenvalues are equal, and the condition number $\\kappa(\\lambda)$ is 1, its absolute minimum. This value of $\\lambda$ optimally balances the conditioning of the problem across these two modes.",
            "answer": "$$\n\\boxed{9.910}\n$$"
        }
    ]
}