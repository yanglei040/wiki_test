{
    "hands_on_practices": [
        {
            "introduction": "In geophysical inversion, the squared $L_2$ norm is the default choice for measuring data misfit, primarily due to its simplicity and connection to Gaussian noise statistics. However, it is notoriously sensitive to outliers—a few data points with large errors can disproportionately influence the final model. This exercise explores the family of $L_p$ norms as a more robust alternative, allowing you to derive and implement a misfit function that can systematically down-weight the influence of outliers by adjusting the parameter $p$ between $2$ and $1$ . Through this practice, you will gain direct insight into how the choice of norm impacts the sensitivity of the inversion to bad data, a crucial skill for processing real-world geophysical datasets.",
            "id": "3612245",
            "problem": "Consider a linearized travel-time tomography setting for body waves in which the observed travel-time vector is denoted by $T_{\\text{obs}} \\in \\mathbb{R}^{n}$, the model is a slowness vector $m \\in \\mathbb{R}^{p}$ with entries in seconds per kilometer, and the linear forward operator that maps slowness to travel-time is $G \\in \\mathbb{R}^{n \\times p}$ with entries that are path lengths in kilometers. The predicted travel-time is $T(m) = G m$, and the residual is $e(m) = T_{\\text{obs}} - T(m) \\in \\mathbb{R}^{n}$. Assume data are weighted by an inverse standard deviation diagonal matrix $W = \\mathrm{diag}(1/\\sigma_1,\\dots,1/\\sigma_n)$, so that $r(m) = W e(m)$ is dimensionless.\n\nDefine, for $p \\in [1,2]$, an $L_p$-type misfit objective with a smooth absolute-value surrogate (to ensure differentiability at the origin) as\n$$\n\\phi_{p,\\epsilon}(m) = \\sum_{i=1}^{n} \\left(r_i(m)^2 + \\epsilon^2\\right)^{\\frac{p}{2}},\n$$\nwhere $\\epsilon > 0$ is a fixed smoothing constant. The sensitivity of the objective to each data residual can be analyzed through the data-space gradient components\n$$\ng_i(r) = \\frac{\\partial \\phi_{p,\\epsilon}}{\\partial r_i} = \\frac{\\partial}{\\partial r_i} \\left(r_i^2+\\epsilon^2\\right)^{\\frac{p}{2}},\n$$\nwhich quantify how a small perturbation in $r_i$ affects the objective. Large, sparse residuals (outliers) along particular ray paths can be diagnosed by comparing $|g_i(r)|$ for different $i$ and $p$.\n\nStart from only the following base facts and definitions:\n- The linearized forward model for travel-time tomography is $T(m) = G m$.\n- The residual is $e(m) = T_{\\text{obs}} - T(m)$, and the weighted residual is $r(m) = W e(m)$.\n- For $a > 0$ and $\\alpha \\in \\mathbb{R}$, the derivative $\\frac{d}{dx}(x^2 + a^2)^{\\alpha} = 2 \\alpha x (x^2 + a^2)^{\\alpha - 1}$.\n- The chain rule for derivatives and standard linear algebra.\n\nTasks:\n1) Derive the expression for $g_i(r)$ for general $p \\in [1,2]$ and $\\epsilon > 0$ using only the definitions and base facts above. Then, using the chain rule, derive the model-space gradient $\\nabla_m \\phi_{p,\\epsilon}(m)$ in terms of $G$, $W$, and $r(m)$ without simplifying to any special case of $p$.\n2) Explain qualitatively, using your expression for $g_i(r)$, how the magnitude of $r_i$ affects $|g_i(r)|$ as a function of $p$ when $|r_i| \\gg \\epsilon$ and when $|r_i| \\ll \\epsilon$, and connect this to the robustness of the misfit to sparse, large residuals. Your explanation must use only asymptotic reasoning grounded in your derived expression.\n\nThen, implement a program to compute, for specified test cases, the scalar misfit value and a dimensionless influence ratio that quantifies the sensitivity to an outlier. Use the following numerically specified, physically consistent setup:\n- Number of data $n = 6$ and number of model parameters $p = 4$.\n- Forward operator $G$ in kilometers:\n  $$\n  G = \\begin{bmatrix}\n  10 & 10 & 0 & 0 \\\\\n  0 & 12 & 8 & 0 \\\\\n  0 & 0 & 9 & 11 \\\\\n  7 & 0 & 0 & 13 \\\\\n  5 & 5 & 5 & 5 \\\\\n  0 & 15 & 0 & 5\n  \\end{bmatrix}.\n  $$\n- True slowness model $m_{\\text{true}}$ in seconds per kilometer:\n  $$\n  m_{\\text{true}} = \\begin{bmatrix} 0.5 \\\\ 0.6 \\\\ 0.55 \\\\ 0.52 \\end{bmatrix}.\n  $$\n- Observed data are generated as $T_{\\text{obs}} = G m_{\\text{true}} + \\delta$, with $\\delta$ a deterministic additive travel-time error vector in seconds specified for each scenario below. The evaluation point for the objective is the true model, $m = m_{\\text{true}}$, so that modeling error is zero and only data errors contribute to residuals.\n- Data standard deviations are all identical and equal to $\\sigma_i = 0.01$ seconds for all $i$, so that $W = \\mathrm{diag}(100,100,100,100,100,100)$ makes $r$ dimensionless.\n- Smoothing parameter is $\\epsilon = 10^{-6}$ (dimensionless).\n\nDefine three scenarios (all entries in seconds):\n- Scenario $1$ (single strong outlier with small background noise):\n  $$\n  \\delta^{(1)} = \\begin{bmatrix} 0.002 \\\\ -0.001 \\\\ 1.0 \\\\ 0.0005 \\\\ -0.0015 \\\\ 0.0008 \\end{bmatrix}.\n  $$\n- Scenario $2$ (no outlier, small noise):\n  $$\n  \\delta^{(2)} = \\begin{bmatrix} 0.002 \\\\ -0.003 \\\\ 0.001 \\\\ -0.0015 \\\\ 0.0 \\\\ 0.0025 \\end{bmatrix}.\n  $$\n- Scenario $3$ (two opposite-sign outliers with tiny background noise):\n  $$\n  \\delta^{(3)} = \\begin{bmatrix} 0.6 \\\\ 0.0 \\\\ 0.0 \\\\ -0.8 \\\\ 0.0005 \\\\ -0.0005 \\end{bmatrix}.\n  $$\n\nFor each scenario $s \\in \\{1,2,3\\}$ and each norm parameter $p \\in \\{2.0, 1.5, 1.0\\}$, compute:\n- The scalar objective value $\\phi_{p,\\epsilon}(m_{\\text{true}})$, which is dimensionless due to the use of $W$.\n- The influence ratio $R_{p}^{(s)}$, defined by:\n  1. Compute $r = W\\left(T_{\\text{obs}}^{(s)} - G m_{\\text{true}}\\right)$.\n  2. Compute $g(r)$ componentwise from your $g_i(r)$ expression.\n  3. Let $i^{\\star}$ be the index that maximizes $|r_i|$.\n  4. Define $R_{p}^{(s)} = \\dfrac{|g_{i^{\\star}}(r)|}{\\mathrm{median}\\left(\\{|g_i(r)|\\}_{i=1}^{n}\\right)}$, a dimensionless ratio quantifying the relative influence of the largest-magnitude residual compared to a typical data point.\n\nAngle units are not involved. No physical units are required in the final reported values because both $\\phi_{p,\\epsilon}$ and $R_{p}^{(s)}$ are dimensionless.\n\nTest suite and output specification:\n- Use the three scenarios $s \\in \\{1,2,3\\}$ given above and the three norm parameters $p \\in \\{2.0, 1.5, 1.0\\}$.\n- For each ordered pair $(s,p)$ in lexicographic order with $s$ increasing fastest inside each $p$-block or vice versa, you must follow exactly this order: for $s=1$ list $p=2.0$, then $p=1.5$, then $p=1.0$; then for $s=2$ list $p=2.0$, then $p=1.5$, then $p=1.0$; then for $s=3$ list $p=2.0$, then $p=1.5$, then $p=1.0$.\n- For each $(s,p)$, output the two floats $\\phi_{p,\\epsilon}(m_{\\text{true}})$ and $R_{p}^{(s)}$ in that order.\n- Your program should produce a single line of output containing all results as a single comma-separated Python-style list of floats enclosed in square brackets, in the exact sequence:\n  $$\n  [\\phi_{2.0}^{(1)}, R_{2.0}^{(1)}, \\phi_{1.5}^{(1)}, R_{1.5}^{(1)}, \\phi_{1.0}^{(1)}, R_{1.0}^{(1)}, \\phi_{2.0}^{(2)}, R_{2.0}^{(2)}, \\phi_{1.5}^{(2)}, R_{1.5}^{(2)}, \\phi_{1.0}^{(2)}, R_{1.0}^{(2)}, \\phi_{2.0}^{(3)}, R_{2.0}^{(3)}, \\phi_{1.5}^{(3)}, R_{1.5}^{(3)}, \\phi_{1.0}^{(3)}, R_{1.0}^{(3)}].\n  $$\nNo other text should be printed by the program.",
            "solution": "The problem statement is a well-posed and scientifically sound exercise in computational geophysics, specifically within the domain of inverse theory and optimization. It requests the derivation of the gradient of a generalized $L_p$ objective function, a qualitative analysis of its robustness properties, and the implementation of a numerical calculation for specific scenarios. All provided data, definitions, and constraints are self-contained, consistent, and physically plausible. The problem is therefore deemed valid.\n\n### Part 1: Derivations\n\nThe first task is to derive the data-space gradient component $g_i(r)$ and the model-space gradient $\\nabla_m \\phi_{p,\\epsilon}(m)$.\n\nThe misfit objective function is defined as:\n$$\n\\phi_{p,\\epsilon}(m) = \\sum_{j=1}^{n} \\left(r_j(m)^2 + \\epsilon^2\\right)^{\\frac{p}{2}}\n$$\nwhere $r(m)$ is the weighted residual vector, $p \\in [1,2]$ is the norm parameter, and $\\epsilon > 0$ is a smoothing constant.\n\n**Derivation of the data-space gradient component $g_i(r)$**\n\nThe data-space gradient components are defined as $g_i(r) = \\frac{\\partial \\phi_{p,\\epsilon}}{\\partial r_i}$. Since the objective function is a sum over independent terms for each data component $r_j$, the partial derivative with respect to $r_i$ only affects the $j=i$ term of the summation.\n$$\ng_i(r) = \\frac{\\partial}{\\partial r_i} \\left( \\left(r_i^2 + \\epsilon^2\\right)^{\\frac{p}{2}} \\right)\n$$\nWe use the provided base fact, which is a direct application of the chain rule: for $a > 0$ and $\\alpha \\in \\mathbb{R}$, $\\frac{d}{dx}(x^2 + a^2)^{\\alpha} = 2 \\alpha x (x^2 + a^2)^{\\alpha - 1}$. We can make the substitution $x = r_i$, $a = \\epsilon$, and $\\alpha = \\frac{p}{2}$. Applying this rule yields:\n$$\ng_i(r) = 2 \\left(\\frac{p}{2}\\right) r_i \\left(r_i^2 + \\epsilon^2\\right)^{\\frac{p}{2} - 1}\n$$\nSimplifying the expression, we obtain the general form for the data-space gradient component:\n$$\ng_i(r) = p \\, r_i \\left(r_i^2 + \\epsilon^2\\right)^{\\frac{p-2}{2}}\n$$\n\n**Derivation of the model-space gradient $\\nabla_m \\phi_{p,\\epsilon}(m)$**\n\nThe model-space gradient is the vector of partial derivatives with respect to the model parameters, $\\nabla_m \\phi_{p,\\epsilon}(m) = \\left[ \\frac{\\partial \\phi}{\\partial m_1}, \\dots, \\frac{\\partial \\phi}{\\partial m_p} \\right]^T$. We apply the multivariable chain rule. The objective function $\\phi$ depends on the model parameters $m_k$ through the weighted residuals $r_i(m)$.\nThe $k$-th component of the gradient is:\n$$\n\\frac{\\partial \\phi}{\\partial m_k} = \\sum_{i=1}^{n} \\frac{\\partial \\phi}{\\partial r_i} \\frac{\\partial r_i}{\\partial m_k}\n$$\nThe first term in the product is simply $g_i(r(m))$. The second term is the partial derivative of the $i$-th weighted residual with respect to the $k$-th model parameter. The weighted residual is given by $r(m) = W (T_{\\text{obs}} - G m)$, where $W$ is a diagonal matrix with entries $W_{ii} = 1/\\sigma_i$.\nThe $i$-th component is $r_i(m) = \\frac{1}{\\sigma_i} \\left( T_{\\text{obs},i} - (Gm)_i \\right)$.\nThe term $(Gm)_i$ is the $i$-th row of $G$ times the vector $m$: $(Gm)_i = \\sum_{l=1}^{p} G_{il} m_l$.\nNow we compute the partial derivative:\n$$\n\\frac{\\partial r_i}{\\partial m_k} = \\frac{\\partial}{\\partial m_k} \\left( \\frac{1}{\\sigma_i} \\left( T_{\\text{obs},i} - \\sum_{l=1}^{p} G_{il} m_l \\right) \\right) = -\\frac{1}{\\sigma_i} G_{ik}\n$$\nThis expression is the $(i, k)$-th entry of the matrix $-W G$.\nSubstituting this back into the sum for $\\frac{\\partial \\phi}{\\partial m_k}$:\n$$\n\\frac{\\partial \\phi}{\\partial m_k} = \\sum_{i=1}^{n} g_i(r(m)) \\left( -\\frac{G_{ik}}{\\sigma_i} \\right) = -\\sum_{i=1}^{n} G_{ik} \\frac{1}{\\sigma_i} g_i(r(m))\n$$\nThis summation is recognizable as the $k$-th component of a matrix-vector product. Let $g(r(m))$ be the column vector with components $g_i(r(m))$. Let's consider the expression $-G^T W g(r(m))$. The $k$-th entry of this $p \\times 1$ vector is:\n$$\n(-G^T W g(r(m)))_k = -\\sum_{i=1}^n (G^T)_{ki} (W g(r(m)))_i = -\\sum_{i=1}^n G_{ik} (W_{ii} g_i(r(m))) = -\\sum_{i=1}^n G_{ik} \\frac{1}{\\sigma_i} g_i(r(m))\n$$\nThis perfectly matches our expression for $\\frac{\\partial \\phi}{\\partial m_k}$. Therefore, the model-space gradient in vector form is:\n$$\n\\nabla_m \\phi_{p,\\epsilon}(m) = -G^T W g(r(m))\n$$\nwhere $g(r(m))$ is the $n \\times 1$ vector whose $i$-th component is $g_i(r(m)) = p \\, r_i(m) \\left(r_i(m)^2 + \\epsilon^2\\right)^{\\frac{p-2}{2}}$. The expression is in terms of $G$, $W$, and $r(m)$ as required.\n\n### Part 2: Qualitative Analysis of Robustness\n\nThe robustness of the misfit to sparse, large residuals (outliers) can be understood by analyzing the magnitude of the data-space gradient component, $|g_i(r)| = p |r_i| (r_i^2 + \\epsilon^2)^{\\frac{p-2}{2}}$, in different asymptotic regimes. This term quantifies the sensitivity of the objective function to a change in the $i$-th residual, and thus its \"influence\" on the gradient-based updates during optimization.\n\n**Case 1: Large Residuals ($|r_i| \\gg \\epsilon$)**\nIn this regime, an outlier is present. The term $(r_i^2 + \\epsilon^2)$ can be approximated by $r_i^2$.\n$$\n|g_i(r)| \\approx p |r_i| (r_i^2)^{\\frac{p-2}{2}} = p |r_i| |r_i|^{p-2} = p |r_i|^{p-1}\n$$\nWe analyze this behavior as a function of $p \\in [1, 2]$:\n- For $p=2$ (Least-Squares): $|g_i(r)| \\approx 2 |r_i|^{2-1} = 2|r_i|$. The influence of a residual grows linearly with its magnitude. This makes the $L_2$ norm highly sensitive to outliers; a single large residual can exert a massive pull on the gradient, potentially corrupting the solution by forcing it to fit the outlier at the expense of other data. It is therefore **not robust**.\n- For $p=1$ (Least-Absolute-Deviations): $|g_i(r)| \\approx 1 |r_i|^{1-1} = |r_i|^0 = 1$. For large residuals, the influence saturates to a constant value. This means that once a residual is identified as an outlier, its influence on the gradient does not increase further, no matter how large it becomes. This makes the $L_1$ norm **robust** to outliers.\n- For $1 < p < 2$: The influence $|g_i(r)| \\approx p|r_i|^{p-1}$ grows sub-linearly with the residual magnitude, since the exponent $p-1$ is between $0$ and $1$. This provides a family of intermediate norms that are more robust than $L_2$ but less so than $L_1$. As $p$ decreases toward $1$, robustness increases.\n\n**Case 2: Small Residuals ($|r_i| \\ll \\epsilon$)**\nIn this regime, the data are well-fit or contain small noise. The term $(r_i^2 + \\epsilon^2)$ can be approximated by $\\epsilon^2$.\n$$\n|g_i(r)| \\approx p |r_i| (\\epsilon^2)^{\\frac{p-2}{2}} = p |r_i| \\epsilon^{p-2} = \\left( \\frac{p}{\\epsilon^{2-p}} \\right) |r_i|\n$$\nSince $p$ and $\\epsilon$ are constants, the influence $|g_i(r)|$ is directly proportional to $|r_i|$. For small residuals, all these $L_p$-type norms behave like the $L_2$ norm. This is a desirable property, as the $L_2$ norm is smooth and has a quadratic basin of attraction near the minimum, leading to stable and efficient convergence. The smoothing parameter $\\epsilon$ defines the threshold for a residual to be considered \"large\" or \"small\", controlling the transition from $L_2$-like behavior to $L_p$-like behavior.\n\nIn summary, decreasing $p$ from $2$ to $1$ systematically reduces the influence of large residuals on the objective function's gradient, thereby increasing the estimator's robustness to outliers.\n\n### Numerical Implementation\n\nTo compute the required values, we will follow these steps for each scenario $s \\in \\{1,2,3\\}$ and each norm parameter $p \\in \\{2.0, 1.5, 1.0\\}$:\n1.  Define the problem constants: $G$, $m_{\\text{true}}$, $\\sigma_i$, $W$, and $\\epsilon$.\n2.  For a given scenario $s$, define the error vector $\\delta^{(s)}$.\n3.  Calculate the weighted residual vector $r^{(s)}$. As the evaluation is at $m=m_{\\text{true}}$, the residual is $e^{(s)} = T_{\\text{obs}}^{(s)} - G m_{\\text{true}} = (G m_{\\text{true}} + \\delta^{(s)}) - G m_{\\text{true}} = \\delta^{(s)}$. Thus, $r^{(s)} = W \\delta^{(s)} = \\delta^{(s)} / \\sigma_i$.\n4.  For a given $p$, calculate the objective function value $\\phi_{p,\\epsilon}(m_{\\text{true}}) = \\sum_{i=1}^{n} ( (r_i^{(s)})^2 + \\epsilon^2 )^{p/2}$.\n5.  Calculate the data-space gradient vector $g(r^{(s)})$ with components $g_i(r^{(s)}) = p \\, r_i^{(s)} ( (r_i^{(s)})^2 + \\epsilon^2 )^{\\frac{p-2}{2}}$.\n6.  Determine the index $i^{\\star}$ corresponding to the maximum absolute residual, $i^{\\star} = \\mathrm{argmax}_i |r_i^{(s)}|$.\n7.  Compute the influence ratio $R_p^{(s)} = \\frac{|g_{i^{\\star}}(r^{(s)})|}{\\mathrm{median}\\left(\\{|g_i(r^{(s)})|\\}_{i=1}^{n}\\right)}$.\n8.  Store the results $(\\phi_{p,\\epsilon}, R_p^{(s)})$ in the specified order and format for the final output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes misfit objectives and influence ratios for different L_p norms in a\n    linearized travel-time tomography problem.\n    \"\"\"\n    # Define the problem setup\n    n = 6\n    p_dim = 4\n\n    # Forward operator G in kilometers\n    G = np.array([\n        [10.0, 10.0, 0.0, 0.0],\n        [0.0, 12.0, 8.0, 0.0],\n        [0.0, 0.0, 9.0, 11.0],\n        [7.0, 0.0, 0.0, 13.0],\n        [5.0, 5.0, 5.0, 5.0],\n        [0.0, 15.0, 0.0, 5.0]\n    ])\n\n    # True slowness model m_true in s/km\n    m_true = np.array([0.5, 0.6, 0.55, 0.52])\n\n    # Data standard deviation sigma_i in seconds\n    sigma = 0.01\n\n    # Weighting matrix W is sigma^-1 * I.\n    # We can apply it by scalar division.\n\n    # Smoothing parameter epsilon\n    epsilon = 1e-6\n\n    # Scenarios for additive travel-time error delta in seconds\n    deltas = [\n        # Scenario 1\n        np.array([0.002, -0.001, 1.0, 0.0005, -0.0015, 0.0008]),\n        # Scenario 2\n        np.array([0.002, -0.003, 0.001, -0.0015, 0.0, 0.0025]),\n        # Scenario 3\n        np.array([0.6, 0.0, 0.0, -0.8, 0.0005, -0.0005])\n    ]\n\n    # Norm parameters p\n    p_values = [2.0, 1.5, 1.0]\n\n    results = []\n\n    # Loop through scenarios and p-values in the specified order\n    for s_idx, delta in enumerate(deltas):\n        \n        # Since we evaluate at m_true, the residual e = T_obs - G*m_true = delta.\n        # The weighted residual r = W * e = delta / sigma.\n        r = delta / sigma\n\n        for p_val in p_values:\n            # 1. Compute the scalar objective value phi\n            phi_terms = (r**2 + epsilon**2)**(p_val / 2.0)\n            phi = np.sum(phi_terms)\n            results.append(phi)\n\n            # 2. Compute the influence ratio R\n            \n            # Compute data-space gradient vector g(r)\n            g = p_val * r * (r**2 + epsilon**2)**((p_val - 2.0) / 2.0)\n            \n            # Find the index of the largest-magnitude residual\n            i_star = np.argmax(np.abs(r))\n            \n            # Get the influence of the largest residual\n            g_i_star_abs = np.abs(g[i_star])\n            \n            # Compute the median of the absolute influences\n            median_g_abs = np.median(np.abs(g))\n            \n            # Compute the influence ratio R\n            # Handle potential division by zero, although not expected here.\n            if median_g_abs > 0:\n                R = g_i_star_abs / median_g_abs\n            else:\n                R = np.inf # Or another representation of an undefined ratio\n            \n            results.append(R)\n\n    # Format the final output as a single-line list of floats\n    print(f\"[{','.join(f'{x:.8f}' for x in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many geophysical forward problems, such as those involving gravitational or magnetic fields, are described by convolutions, which become simple multiplications in the Fourier domain. This makes frequency-domain inversion methods computationally attractive, but this transformation is not without its perils. This practice explores the critical relationship between data misfit computed in the spatial domain versus a weighted misfit in the spectral domain . By simulating scenarios involving spectral leakage and aliasing, you will investigate how these common signal processing artifacts can break the direct equivalence between the two domains, highlighting the need for careful consideration of sampling and windowing when designing spectral-domain objective functions.",
            "id": "3612211",
            "problem": "Consider a one-dimensional gravity inversion toy model where the forward operator maps a spatial model $m(x)$ to data $d(x)$ through spatial convolution with a physically plausible, depth-dependent Green's function kernel $K(x; z_0)$. The spatial data misfit is defined as the squared two-norm in the spatial domain,\n$$\n\\phi_s(m) = \\| d - G(m) \\|_2^2,\n$$\nwhere $G(m)$ denotes the forward operator applied to $m(x)$, implemented as circular convolution on a discrete grid. In the spectral domain, the weighted misfit is defined as\n$$\n\\phi_k(m) = \\| \\widehat{d} - \\widehat{G(m)} \\|_{2,w}^2 = \\sum_{k=0}^{N-1} w_k \\left| \\widehat{d}_k - \\widehat{G(m)}_k \\right|^2,\n$$\nwhere $\\widehat{(\\cdot)}$ denotes the unitary Discrete Fourier Transform (DFT), $N$ is the number of samples, and $w_k \\ge 0$ is a prescribed spectral weighting. The unitary Discrete Fourier Transform (DFT) is defined by\n$$\n\\widehat{x}_k = \\frac{1}{\\sqrt{N}} \\sum_{n=0}^{N-1} x_n \\exp\\left(- i \\frac{2\\pi k n}{N}\\right), \\quad k = 0,1,\\dots,N-1,\n$$\nwhich ensures that Parseval's theorem holds exactly:\n$$\n\\sum_{n=0}^{N-1} |x_n|^2 = \\sum_{k=0}^{N-1} |\\widehat{x}_k|^2.\n$$\nYou will investigate how spectral leakage and aliasing impact the agreement between $\\phi_s(m)$ and $\\phi_k(m)$ under different weighting choices $w_k$. Spectral leakage arises from finite-window sampling when the underlying signal does not have an integer number of cycles within the observation window, causing energy to spread across multiple spectral bins. Aliasing arises when the sampling rate is insufficient to resolve high-frequency content, causing energy at high frequencies to fold into lower frequencies after decimation.\n\nAssume the following toy forward model. Let the discrete spatial grid be $x_n$ for $n=0,1,\\dots,N-1$, with circular boundary conditions. The forward operator $G(m)$ is circular convolution with\n$$\nK_n(z_0) = \\frac{z_0}{x_n^2 + z_0^2}, \\quad \\text{normalized so that } \\sum_{n=0}^{N-1} K_n(z_0) = 1,\n$$\nwhich captures a physically plausible low-pass effect of gravity sensitivity with depth $z_0$. For a given $m(x)$ and $z_0$, define $d = K(z_0) * m$ and a predicted $G(m_{\\text{est}})$ computed with a possibly perturbed depth $z_0^{\\text{est}}$ and scaled model $m_{\\text{est}}(x) = s \\, m(x)$.\n\nYour program must compute and report three diagnostic floats across the following test suite. Use only dimensionless quantities; angles are implicit in the trigonometric definitions and do not require unit conversion.\n\nFundamental base for derivation and algorithmic design:\n- Definition of spatial misfit $\\phi_s(m)$ as a squared norm.\n- Definition of spectral misfit $\\phi_k(m)$ as a weighted squared norm using the unitary Discrete Fourier Transform (DFT).\n- Parseval's theorem linking spatial and spectral energy for the unitary DFT.\n- Definitions of spectral leakage due to finite-window sampling and aliasing due to insufficient sampling in relation to the sampling theorem.\n\nTest Suite:\n1. Happy path (Parseval consistency):\n   - Grid size $N = 512$, length $L = 51200$ so that $x_n = n \\Delta x$ with $\\Delta x = L/N$.\n   - True depth $z_0 = 1500$, estimated depth $z_0^{\\text{est}} = 1800$, scale $s = 0.9$.\n   - True model $m(x)$ is periodic with integer cycles: $m_n = \\sin\\left( 2\\pi \\cdot 3 \\cdot \\frac{n}{N} \\right) + 0.5 \\cos\\left( 2\\pi \\cdot 7 \\cdot \\frac{n}{N} \\right)$.\n   - Weighting $w_k = 1$ for all $k$.\n   - Output the relative difference $r_1 = \\frac{|\\phi_s - \\phi_k|}{\\phi_s}$.\n\n2. Spectral leakage case (non-integer cycles with band-limited weighting):\n   - Grid size $N = 512$, length $L = 51200$, $\\Delta x = L/N$.\n   - True depth $z_0 = 1500$, estimated depth $z_0^{\\text{est}} = 1600$, scale $s = 0.95$.\n   - True model with non-integer cycles: $m_n = \\sin\\left( 2\\pi \\cdot 3.5 \\cdot \\frac{n}{N} \\right) + 0.4 \\sin\\left( 2\\pi \\cdot 20.25 \\cdot \\frac{n}{N} \\right)$.\n   - Band-limited weighting $w_k$ equal to $1$ only for bins $k \\in \\{k_0-1, k_0, k_0+1, N-(k_0+1), N-k_0, N-(k_0-1)\\}$ with $k_0 = 4$, and $w_k = 0$ otherwise.\n   - Output the ratio $r_2 = \\frac{\\phi_k}{\\phi_s}$.\n\n3. Aliasing case (decimation with high-frequency content and smooth spectral weighting):\n   - High-resolution grid $N_{\\text{hi}} = 4096$, length $L = 51200$, $\\Delta x_{\\text{hi}} = L/N_{\\text{hi}}$.\n   - True depth $z_0 = 1000$, estimated depth $z_0^{\\text{est}} = 1200$, scale $s = 0.92$.\n   - High-resolution true model containing high-frequency content: $m^{\\text{hi}}_n = \\sin\\left( 2\\pi \\cdot 30 \\cdot \\frac{n}{N_{\\text{hi}}} \\right) + 0.3 \\sin\\left( 2\\pi \\cdot 1200 \\cdot \\frac{n}{N_{\\text{hi}}} \\right)$.\n   - Compute $d^{\\text{hi}} = K(z_0) * m^{\\text{hi}}$ and $G(m_{\\text{est}})^{\\text{hi}} = K(z_0^{\\text{est}}) * (s m^{\\text{hi}})$, then decimate both by a factor of $r = 8$ to obtain $d^{\\text{lo}}$ and $G(m_{\\text{est}})^{\\text{lo}}$ of size $N_{\\text{lo}} = 512$.\n   - Smooth spectral weighting on the low-resolution grid defined by\n     $$\n     w_k = \\frac{1}{1 + \\left( \\frac{\\kappa_k}{\\kappa_0} \\right)^4 }, \\quad \\kappa_k = \\min(k, N_{\\text{lo}} - k), \\quad \\kappa_0 = 64,\n     $$\n     which downweights higher wavenumbers.\n   - Output the ratio $r_3 = \\frac{\\phi_k}{\\phi_s}$ computed on the decimated data.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each result must be a float. There are no physical units required in the outputs, and angles used inside trigonometric functions are dimensionless by construction.",
            "solution": "The problem requires the computation of three diagnostic quantities, $r_1$, $r_2$, and $r_3$, which compare a spatial-domain data misfit $\\phi_s$ with a spectrally-weighted misfit $\\phi_k$ under different conditions. The core of the problem lies in implementing a toy 1D gravity forward model and then analyzing the misfits in scenarios designed to highlight the properties of the Discrete Fourier Transform (DFT), specifically Parseval's theorem, spectral leakage, and aliasing.\n\nThe spatial misfit is defined as the squared Euclidean norm of the residual vector $r = d - G(m_{\\text{est}})$:\n$$ \\phi_s(m) = \\| r \\|_2^2 = \\sum_{n=0}^{N-1} |r_n|^2 $$\nThe spectral misfit is the weighted squared Euclidean norm of the DFT of the residual, $\\widehat{r} = \\widehat{d} - \\widehat{G(m_{\\text{est}})}$:\n$$ \\phi_k(m) = \\| \\widehat{r} \\|_{2,w}^2 = \\sum_{k=0}^{N-1} w_k |\\widehat{r}_k|^2 $$\nThe problem specifies the use of the unitary DFT, for which Parseval's theorem holds: $\\sum_{n=0}^{N-1} |x_n|^2 = \\sum_{k=0}^{N-1} |\\widehat{x}_k|^2$. This implies that if the weights $w_k=1$ for all $k$, then $\\phi_s$ and $\\phi_k$ must be identical, up to numerical precision.\n\nThe forward operator $G(m)$ applies a circular convolution with a kernel $K(z_0)$. Using the convolution theorem, this operation is most efficiently performed in the frequency domain. For a unitary DFT, the theorem is $\\widehat{K*m}_k = \\sqrt{N} \\widehat{K}_k \\widehat{m}_k$.\n\nThe overall computational procedure for each test case is as follows:\n1.  Define the spatial grid and model parameters.\n2.  Construct the true model vector $m(x)$ and the estimated model $m_{\\text{est}}(x) = s \\cdot m(x)$.\n3.  Construct the convolution kernels $K(z_0)$ and $K(z_0^{\\text{est}})$. To ensure the kernel is symmetric, as is physically plausible for a gravity source, we define it on a grid centered at $x=0$ and then use a circular shift (`fftshift`) to align it with the DFT's conventions. The kernel is normalized such that its elements sum to $1$.\n4.  Compute the DFTs of the models and kernels using the unitary `ortho` norm.\n5.  Calculate the \"true\" data $d$ and the predicted data $d_{\\text{est}} = G(m_{\\text{est}})$ in the frequency domain:\n    $$ \\widehat{d}_k = \\sqrt{N} \\widehat{K}(z_0)_k \\widehat{m}_k $$\n    $$ \\widehat{d_{\\text{est}}}_k = \\sqrt{N} \\widehat{K}(z_0^{\\text{est}})_k \\widehat{m_{\\text{est}}}_k = \\sqrt{N} \\widehat{K}(z_0^{\\text{est}})_k (s \\widehat{m}_k) $$\n6.  Calculate the residual in both spatial and spectral domains. The spectral residual is $\\widehat{r}_k = \\widehat{d}_k - \\widehat{d_{\\text{est}}}_k$. The spatial residual $r_n$ is obtained by applying the inverse unitary DFT to $\\widehat{r}_k$.\n7.  Compute $\\phi_s$ and $\\phi_k$ using their definitions and the specified weighting vector $w_k$.\n8.  Calculate the final required ratio for the test case.\n\nThe three test cases are designed to probe specific phenomena:\n\n**Test Case 1: Happy Path (Parseval Consistency)**\nThis case uses a model $m(x)$ composed of sinusoids with integer frequencies, which do not exhibit spectral leakage. The spectral weighting is uniform, $w_k = 1$ for all $k$. According to Parseval's theorem for the unitary DFT, $\\phi_s = \\sum |r_n|^2$ must equal $\\phi_k = \\sum |\\widehat{r}_k|^2$. The computed relative difference $r_1 = |\\phi_s - \\phi_k|/\\phi_s$ is expected to be close to zero, limited only by floating-point arithmetic precision.\n\n**Test Case 2: Spectral Leakage Case**\nHere, the model $m(x)$ is composed of sinusoids with non-integer frequencies ($3.5$ and $20.25$ cycles over the domain). This causes spectral leakage, meaning the energy of these sinusoids spreads out over many frequency bins in the DFT, rather than being concentrated in single bins. The weighting $w_k$ is designed to be highly selective, being non-zero only for a few frequency bins around $k=4$. As a result, $\\phi_k$ will capture only a small fraction of the total spectral energy of the residual, while $\\phi_s$ captures the entire energy in the spatial domain. Consequently, the ratio $r_2 = \\phi_k / \\phi_s$ is expected to be significantly less than $1$.\n\n**Test Case 3: Aliasing Case**\nThis case demonstrates the effect of aliasing from under-sampling. A high-resolution model $m^{\\text{hi}}$ is created, containing a component with a very high frequency ($k=1200$ on a grid of size $N_{\\text{hi}}=4096$). This frequency is well above the Nyquist frequency of the low-resolution grid ($k_{\\text{Nyquist, lo}} = N_{\\text{lo}}/2 = 512/2 = 256$). The forward modeling is performed on the high-resolution grid. The resulting data $d^{\\text{hi}}$ and prediction $d^{\\text{est, hi}}$ are then decimated (sub-sampled) by a factor of $r=8$ without an anti-aliasing filter. This decimation causes the high-frequency energy to \"fold\" into the low-frequency part of the spectrum of the decimated signals $d^{\\text{lo}}$ and $d^{\\text{est, lo}}$. The misfits $\\phi_s$ and $\\phi_k$ are then computed on this aliased, low-resolution data. The spectral weighting $w_k$ is a smooth filter that downweights high wavenumbers. The final ratio $r_3 = \\phi_k / \\phi_s$ quantifies the relationship between spatial and weighted spectral energy in the presence of aliasing.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef create_kernel(N, L, z0):\n    \"\"\"\n    Creates a normalized, centered gravity kernel for circular convolution.\n\n    Args:\n        N (int): Number of grid points.\n        L (float): Length of the spatial domain.\n        z0 (float): Depth parameter for the kernel.\n\n    Returns:\n        np.ndarray: The kernel vector, ordered for FFT-based convolution.\n    \"\"\"\n    dx = L / N\n    # Create a centered spatial coordinate vector for a symmetric kernel\n    x_centered = np.arange(-N / 2, N / 2) * dx\n    \n    # Calculate the unnormalized kernel values\n    kernel_unnormalized = z0 / (x_centered**2 + z0**2)\n    \n    # Normalize the kernel to have a sum of 1\n    kernel_centered = kernel_unnormalized / np.sum(kernel_unnormalized)\n    \n    # Shift the kernel to be compatible with np.fft's ordering\n    # The zero-frequency component is at index 0\n    kernel = np.fft.fftshift(kernel_centered)\n    \n    return kernel\n\ndef compute_misfits(N, L, z0, z0_est, s, m, w):\n    \"\"\"\n    Computes spatial and spectral misfits for a given model and parameters.\n    \n    Args:\n        N (int): Number of grid points.\n        L (float): Length of the spatial domain.\n        z0 (float): True depth.\n        z0_est (float): Estimated depth.\n        s (float): Estimated model scaling factor.\n        m (np.ndarray): The true model vector.\n        w (np.ndarray): The spectral weighting vector.\n\n    Returns:\n        tuple[float, float]: A tuple containing (phi_s, phi_k).\n    \"\"\"\n    # Create kernels\n    K_true = create_kernel(N, L, z0)\n    K_est = create_kernel(N, L, z0_est)\n\n    # Unitary DFTs of model and kernels\n    m_hat = np.fft.fft(m, norm='ortho')\n    K_true_hat = np.fft.fft(K_true, norm='ortho')\n    K_est_hat = np.fft.fft(K_est, norm='ortho')\n\n    # Compute \"true\" and predicted data in the spectral domain\n    # using the convolution theorem for unitary DFT\n    sqrt_N = np.sqrt(N)\n    d_hat = sqrt_N * K_true_hat * m_hat\n    d_est_hat = sqrt_N * K_est_hat * (s * m_hat)\n\n    # Spectral residual\n    r_hat = d_hat - d_est_hat\n    \n    # Spatial residual (from inverse DFT of spectral residual)\n    r = np.fft.ifft(r_hat, norm='ortho')\n\n    # Compute misfits\n    phi_s = np.sum(np.abs(r)**2)\n    phi_k = np.sum(w * np.abs(r_hat)**2)\n    \n    return phi_s, phi_k\n\n\ndef solve():\n    \"\"\"\n    Main function to run the three test cases and print the results.\n    \"\"\"\n    # --- Test Case 1: Happy path (Parseval consistency) ---\n    N1 = 512\n    L1 = 51200.0\n    z0_1 = 1500.0\n    z0_est_1 = 1800.0\n    s1 = 0.9\n    \n    n1 = np.arange(N1)\n    m1 = np.sin(2 * np.pi * 3 * n1 / N1) + 0.5 * np.cos(2 * np.pi * 7 * n1 / N1)\n    w1 = np.ones(N1)\n    \n    phi_s1, phi_k1 = compute_misfits(N1, L1, z0_1, z0_est_1, s1, m1, w1)\n    r1 = np.abs(phi_s1 - phi_k1) / phi_s1\n\n    # --- Test Case 2: Spectral leakage case ---\n    N2 = 512\n    L2 = 51200.0\n    z0_2 = 1500.0\n    z0_est_2 = 1600.0\n    s2 = 0.95\n\n    n2 = np.arange(N2)\n    m2 = np.sin(2 * np.pi * 3.5 * n2 / N2) + 0.4 * np.sin(2 * np.pi * 20.25 * n2 / N2)\n    \n    w2 = np.zeros(N2)\n    k0_2 = 4\n    # Positive frequencies\n    w2[k0_2 - 1 : k0_2 + 2] = 1.0\n    # Negative frequencies (mirrored)\n    w2[N2 - (k0_2 + 1) : N2 - (k0_2 - 1) + 1] = 1.0\n\n    phi_s2, phi_k2 = compute_misfits(N2, L2, z0_2, z0_est_2, s2, m2, w2)\n    r2 = phi_k2 / phi_s2\n\n    # --- Test Case 3: Aliasing case ---\n    N_hi = 4096\n    L3 = 51200.0\n    N_lo = 512\n    r_dec = N_hi // N_lo\n    \n    z0_3 = 1000.0\n    z0_est_3 = 1200.0\n    s3 = 0.92\n\n    # High-resolution model\n    n_hi = np.arange(N_hi)\n    m_hi = np.sin(2 * np.pi * 30 * n_hi / N_hi) + 0.3 * np.sin(2 * np.pi * 1200 * n_hi / N_hi)\n\n    # High-resolution kernels\n    K_true_hi = create_kernel(N_hi, L3, z0_3)\n    K_est_hi = create_kernel(N_hi, L3, z0_est_3)\n    \n    # High-resolution forward modeling\n    m_hi_hat = np.fft.fft(m_hi, norm='ortho')\n    K_true_hi_hat = np.fft.fft(K_true_hi, norm='ortho')\n    K_est_hi_hat = np.fft.fft(K_est_hi, norm='ortho')\n    \n    # High-resolution data and prediction in spectral domain\n    sqrt_N_hi = np.sqrt(N_hi)\n    d_hi_hat = sqrt_N_hi * K_true_hi_hat * m_hi_hat\n    d_est_hi_hat = sqrt_N_hi * K_est_hi_hat * (s3 * m_hi_hat)\n\n    # Transform back to spatial domain\n    d_hi = np.fft.ifft(d_hi_hat, norm='ortho')\n    d_est_hi = np.fft.ifft(d_est_hi_hat, norm='ortho')\n    \n    # Decimate (sub-sample) to create low-resolution signals\n    d_lo = d_hi[::r_dec]\n    d_est_lo = d_est_hi[::r_dec]\n    \n    # Compute misfits on low-resolution, aliased data\n    r_lo = d_lo - d_est_lo\n    phi_s3 = np.sum(np.abs(r_lo)**2)\n    \n    r_lo_hat = np.fft.fft(r_lo, norm='ortho')\n    \n    # Smooth spectral weighting on low-res grid\n    kappa0_3 = 64.0\n    k_lo = np.arange(N_lo)\n    kappa_k = np.minimum(k_lo, N_lo - k_lo)\n    w3 = 1.0 / (1.0 + (kappa_k / kappa0_3)**4)\n    \n    phi_k3 = np.sum(w3 * np.abs(r_lo_hat)**2)\n    r3 = phi_k3 / phi_s3\n\n    results = [r1, r2, r3]\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A successful geophysical model must not only fit the observed data but also adhere to fundamental physical principles, such as conservation of mass or momentum. These principles can often be formulated as hard equality constraints on the model parameters. This advanced practice moves beyond simple penalty methods to introduce the Augmented Lagrangian framework, a powerful and principled approach for constrained optimization . You will derive and implement an iterative solver based on the Alternating Direction Method of Multipliers (ADMM), demonstrating its effectiveness at enforcing physical constraints and providing a robust alternative to solving the often ill-conditioned KKT systems directly.",
            "id": "3612288",
            "problem": "Consider a linearized inverse problem in computational geophysics where the predicted data are modeled by a linear forward operator. Let the model vector be $m \\in \\mathbb{R}^n$, the observed data be $d \\in \\mathbb{R}^p$, and the forward operator be represented by a matrix $F \\in \\mathbb{R}^{p \\times n}$ so that the predicted data are $F m$. The data misfit objective is defined by the squared two-norm. In many geophysical applications, conservation laws impose linear equality constraints of the form $A m = b$, where $A \\in \\mathbb{R}^{k \\times n}$ and $b \\in \\mathbb{R}^{k}$.\n\nYou are tasked to construct an augmented Lagrangian objective that enforces the equality constraint and to split the optimization into tractable subproblems. Specifically, formulate the constrained least-squares problem\n$$\n\\min_{m \\in \\mathbb{R}^n} \\ \\phi(m) = \\| d - F m \\|_2^2 \\quad \\text{subject to} \\quad A m = b,\n$$\nand use the augmented Lagrangian with a scaled dual variable $u \\in \\mathbb{R}^k$,\n$$\n\\mathcal{L}_\\rho(m,u) = \\| d - F m \\|_2^2 + \\frac{\\rho}{2} \\left\\| A m - b + u \\right\\|_2^2,\n$$\nwhere $\\rho > 0$ is a penalty parameter. Derive, from first principles and without using any shortcut formulas, the update step for $m$ that results from minimizing the augmented objective with respect to $m$ for a fixed $u$, and the update step for $u$ that enforces the equality constraint through a splitting scheme. The splitting scheme must be based on Alternating Direction Method of Multipliers (ADMM) and Method of Multipliers principles, and you must make explicit use of the Karush–Kuhn–Tucker (KKT) optimality conditions. Your derivation must begin from the definitions of least-squares misfit, equality-constrained optimization, and the augmented Lagrangian construction.\n\nImplement two solvers:\n- An augmented Lagrangian solver using Alternating Direction Method of Multipliers (ADMM), with the scaled dual variable $u$ and iterative updates that alternate between minimizing with respect to $m$ and updating $u$ to reduce the constraint violation.\n- A single-shot penalty method solver that minimizes the penalized objective $\\| d - F m \\|_2^2 + \\frac{\\rho}{2} \\| A m - b \\|_2^2$ without dual variable updates.\n\nDefine the following termination tests for the ADMM solver:\n- Primal feasibility tolerance: stop when $\\| A m - b \\|_2 \\le \\varepsilon_{\\text{pri}}$.\n- Model update tolerance: stop when $\\| m^{(k)} - m^{(k-1)} \\|_2 \\le \\varepsilon_{\\text{mod}}$,\nwhere $m^{(k)}$ denotes the model at iteration $k$, and $\\varepsilon_{\\text{pri}}$ and $\\varepsilon_{\\text{mod}}$ are fixed tolerances.\n\nFor each run, report the following quantities:\n- The number of iterations $N_{\\text{it}}$ taken by ADMM to satisfy both termination tests (or the maximum number of iterations if not satisfied).\n- The final data misfit value $\\phi(m)$ evaluated at the ADMM solution.\n- The final constraint violation $\\| A m - b \\|_2$ at the ADMM solution.\n- The constraint violation $\\| A m_{\\text{pen}} - b \\|_2$ at the penalty method solution $m_{\\text{pen}}$.\n\nYour program must implement the above and evaluate the following test suite. All matrices and vectors are in exact numeric form and must be used exactly as given. Let $n=3$, $p=4$, and $k=2$. Use initial conditions $m^{(0)} = 0$ and $u^{(0)} = 0$. Use tolerances $\\varepsilon_{\\text{pri}} = 10^{-10}$ and $\\varepsilon_{\\text{mod}} = 10^{-10}$, and a maximum iteration count of $N_{\\max} = 2000$.\n\nTest Suite:\n- Case 1 (happy path, moderate penalty, consistent constraints):\n  $$\n  F = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 1 \\\\\n  1 & 1 & 1\n  \\end{bmatrix}, \\quad\n  A = \\begin{bmatrix}\n  1 & 1 & 0 \\\\\n  0 & 1 & 1\n  \\end{bmatrix}, \\quad\n  d = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 6 \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 3 \\\\ 5 \\end{bmatrix}, \\quad\n  \\rho = 1.\n  $$\n- Case 2 (significant edge case, inconsistent data vs. constraint coupling):\n  $$\n  F = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 1 \\\\\n  1 & 1 & 1\n  \\end{bmatrix}, \\quad\n  A = \\begin{bmatrix}\n  1 & 1 & 0 \\\\\n  0 & 1 & 1\n  \\end{bmatrix}, \\quad\n  d = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 5 \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 3 \\\\ 5 \\end{bmatrix}, \\quad\n  \\rho = 1.\n  $$\n- Case 3 (boundary condition, very small penalty parameter):\n  $$\n  F = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 1 \\\\\n  1 & 1 & 1\n  \\end{bmatrix}, \\quad\n  A = \\begin{bmatrix}\n  1 & 1 & 0 \\\\\n  0 & 1 & 1\n  \\end{bmatrix}, \\quad\n  d = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 6 \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 3 \\\\ 5 \\end{bmatrix}, \\quad\n  \\rho = 10^{-2}.\n  $$\n- Case 4 (boundary condition, very large penalty parameter):\n  $$\n  F = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 1 \\\\\n  1 & 1 & 1\n  \\end{bmatrix}, \\quad\n  A = \\begin{bmatrix}\n  1 & 1 & 0 \\\\\n  0 & 1 & 1\n  \\end{bmatrix}, \\quad\n  d = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 6 \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 3 \\\\ 5 \\end{bmatrix}, \\quad\n  \\rho = 5 \\cdot 10^{2}.\n  $$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list of four numbers in the order $\\left[N_{\\text{it}}, \\phi(m), \\|A m - b\\|_2, \\|A m_{\\text{pen}} - b\\|_2\\right]$. For example, the output format must be\n$$\n\\left[ [N_{\\text{it}}^{(1)}, \\phi^{(1)}, r^{(1)}, r_{\\text{pen}}^{(1)}], [N_{\\text{it}}^{(2)}, \\phi^{(2)}, r^{(2)}, r_{\\text{pen}}^{(2)}], [N_{\\text{it}}^{(3)}, \\phi^{(3)}, r^{(3)}, r_{\\text{pen}}^{(3)}], [N_{\\text{it}}^{(4)}, \\phi^{(4)}, r^{(4)}, r_{\\text{pen}}^{(4)}] \\right].\n$$\n\nNo physical units or angle units are involved in this problem. All outputs must be real numbers or integers. The code must be self-contained, runnable as-is, and must not read any external input.",
            "solution": "The problem is to find a numerical solution to a linearly constrained least-squares problem, which is common in computational geophysics and other inverse problems. The formulation is valid, scientifically grounded in optimization theory and linear algebra, and provides a complete and consistent set of parameters for testing.\n\nThe core problem is to find a model vector $m \\in \\mathbb{R}^n$ that minimizes the data misfit, subject to linear equality constraints:\n$$\n\\min_{m \\in \\mathbb{R}^n} \\ \\phi(m) = \\| d - F m \\|_2^2 \\quad \\text{subject to} \\quad A m = b\n$$\nHere, $\\| \\cdot \\|_2$ denotes the Euclidean norm (or $L_2$-norm), $d \\in \\mathbb{R}^p$ is the vector of observed data, $F \\in \\mathbb{R}^{p \\times n}$ is the linear forward operator, $A \\in \\mathbb{R}^{k \\times n}$ is the constraint matrix, and $b \\in \\mathbb{R}^k$ is the constraint vector.\n\nThis is a convex optimization problem, as the objective function is quadratic and the constraints are linear. The Karush-Kuhn-Tucker (KKT) conditions provide necessary and sufficient conditions for the optimal solution. The Lagrangian for this problem is:\n$$\n\\mathcal{L}(m, \\lambda) = \\| d - F m \\|_2^2 + \\lambda^T (A m - b)\n$$\nwhere $\\lambda \\in \\mathbb{R}^k$ is the vector of Lagrange multipliers. The KKT conditions are:\n1.  **Stationarity:** $\\nabla_m \\mathcal{L}(m, \\lambda) = 0$\n2.  **Primal Feasibility:** $A m - b = 0$\n\nThe stationarity condition requires setting the gradient with respect to $m$ to zero:\n$$\n\\nabla_m \\mathcal{L} = \\nabla_m \\left( (d - F m)^T (d - F m) \\right) + \\nabla_m \\left( \\lambda^T (A m - b) \\right) = 0\n$$\n$$\n-2 F^T (d - F m) + A^T \\lambda = 0 \\implies 2 F^T F m + A^T \\lambda = 2 F^T d\n$$\nCombined with the feasibility condition, this gives the KKT system, a block matrix linear system for the optimal pair $(m^*, \\lambda^*)$:\n$$\n\\begin{bmatrix} 2F^TF & A^T \\\\ A & 0 \\end{bmatrix} \\begin{bmatrix} m \\\\ \\lambda \\end{bmatrix} = \\begin{bmatrix} 2F^Td \\\\ b \\end{bmatrix}\n$$\nSolving this system directly can be difficult for large-scale problems. The augmented Lagrangian method provides an iterative alternative.\n\n### Augmented Lagrangian and ADMM/Method of Multipliers\n\nThe augmented Lagrangian modifies the objective by adding a penalty term for constraint violation, leading to better convergence properties than the simple penalty method. The problem specifies the augmented Lagrangian in its scaled dual form:\n$$\n\\mathcal{L}_\\rho(m,u) = \\| d - F m \\|_2^2 + \\frac{\\rho}{2} \\left\\| A m - b + u \\right\\|_2^2\n$$\nwhere $\\rho > 0$ is a penalty parameter and $u \\in \\mathbb{R}^k$ is a scaled dual variable, related to the Lagrange multiplier $\\lambda$ by $u = \\lambda/\\rho$. The Alternating Direction Method of Multipliers (ADMM), or more specifically for this problem structure, the Method of Multipliers, is an iterative scheme that seeks a saddle point of $\\mathcal{L}_\\rho(m, u)$. It alternates between minimizing $\\mathcal{L}_\\rho$ with respect to $m$ and updating the dual variable $u$.\n\nThe iterative scheme is as follows, starting with initial guesses $m^{(0)}$ and $u^{(0)}$:\n1.  **$m$-minimization step:** Find the model $m^{(k+1)}$ that minimizes the augmented Lagrangian for the current dual variable $u^{(k)}$:\n    $$\n    m^{(k+1)} = \\arg\\min_m \\mathcal{L}_\\rho(m, u^{(k)}) = \\arg\\min_m \\left( \\| d - F m \\|_2^2 + \\frac{\\rho}{2} \\| A m - b + u^{(k)} \\|_2^2 \\right)\n    $$\n2.  **$u$-update step:** Update the dual variable to enforce the constraint:\n    $$\n    u^{(k+1)} = u^{(k)} + (A m^{(k+1)} - b)\n    $$\n\n#### Derivation of the $m$-Update from First Principles\n\nThe $m$-minimization subproblem is an unconstrained quadratic optimization. We find the minimizer by setting the gradient of $\\mathcal{L}_\\rho(m, u^{(k)})$ with respect to $m$ to zero. Let's compute this gradient:\n$$\n\\nabla_m \\mathcalL_\\rho(m, u^{(k)}) = \\nabla_m \\left( \\| d - F m \\|_2^2 \\right) + \\nabla_m \\left( \\frac{\\rho}{2} \\| A m - b + u^{(k)} \\|_2^2 \\right)\n$$\nThe gradient of the first term (the squared $L_2$-norm of the data residual) is:\n$$\n\\nabla_m \\left( (d - F m)^T (d - F m) \\right) = -2 F^T (d - F m) = 2(F^T F m - F^T d)\n$$\nThe gradient of the second term (the augmented penalty term) is:\n$$\n\\nabla_m \\left( \\frac{\\rho}{2} (A m - b + u^{(k)})^T (A m - b + u^{(k)}) \\right) = \\frac{\\rho}{2} \\cdot 2 A^T (A m - b + u^{(k)}) = \\rho A^T (A m - b + u^{(k)})\n$$\nSetting the sum of these gradients to zero to find the optimal $m^{(k+1)}$:\n$$\n2(F^T F m^{(k+1)} - F^T d) + \\rho A^T (A m^{(k+1)} - b + u^{(k)}) = 0\n$$\nRearranging the terms to solve for $m^{(k+1)}$:\n$$\n2 F^T F m^{(k+1)} + \\rho A^T A m^{(k+1)} = 2 F^T d + \\rho A^T (b - u^{(k)})\n$$\n$$\n(2 F^T F + \\rho A^T A) m^{(k+1)} = 2 F^T d + \\rho A^T (b - u^{(k)})\n$$\nThis is a linear system of the form $H m = g$, where the matrix $H = 2 F^T F + \\rho A^T A$ is constant throughout the iterations, and the vector $g^{(k)} = 2 F^T d + \\rho A^T (b - u^{(k)})$ is updated at each step. Since $F$ has full column rank, $F^T F$ is positive definite. $A^T A$ is positive semi-definite. For $\\rho > 0$, the sum $H$ is positive definite and thus invertible, guaranteeing a unique solution for $m^{(k+1)}$ at each step. We can pre-compute the inverse or a factorization of $H$ for efficiency.\n\n#### Derivation of the $u$-Update\nThe $u$-update step is a dual-ascent step. It drives the primal residual, $r_{\\text{pri}} = A m - b$, to zero. At convergence, $m^{(k+1)} \\approx m^{(k)}$, so the update rule $u^{(k+1)} = u^{(k)} + (A m^{(k+1)} - b)$ implies that $A m^{(k+1)} - b \\approx 0$. This ensures that the primal feasibility KKT condition is met in the limit.\n\n### Penalty Method Solver\n\nFor comparison, the simple penalty method solves a single unconstrained optimization problem where the constraint is incorporated as a penalty term in the objective function. The objective is:\n$$\n\\min_{m} \\mathcal{J}_{\\text{pen}}(m) = \\| d - F m \\|_2^2 + \\frac{\\rho}{2} \\| A m - b \\|_2^2\n$$\nThis corresponds to the first $m$-minimization step of the ADMM algorithm with the initial dual variable $u^{(0)}=0$. The solution, $m_{\\text{pen}}$, is found by setting the gradient of $\\mathcal{J}_{\\text{pen}}(m)$ to zero:\n$$\n\\nabla_m \\mathcal{J}_{\\text{pen}} = 2(F^T F m - F^T d) + \\rho A^T (A m - b) = 0\n$$\n$$\n(2 F^T F + \\rho A^T A) m = 2 F^T d + \\rho A^T b\n$$\nThe solution $m_{\\text{pen}}$ is obtained by solving this single linear system. Unlike ADMM, this method does not guarantee that the constraint $A m = b$ is satisfied exactly, but the violation $\\|A m_{\\text{pen}} - b\\|_2$ typically decreases as the penalty parameter $\\rho$ increases.\n\n### Implementation Logic\n\nThe implementation will consist of two primary functions:\n1.  `admm_solver`: This function implements the iterative ADMM/Method of Multipliers scheme. It initializes $m^{(0)}$ and $u^{(0)}$ to zero vectors, then iterates the $m$-update and $u$-update steps until the primal feasibility tolerance $\\| A m^{(k)} - b \\|_2 \\le \\varepsilon_{\\text{pri}}$ and the model update tolerance $\\| m^{(k)} - m^{(k-1)} \\|_2 \\le \\varepsilon_{\\text{mod}}$ are both met, or a maximum number of iterations $N_{\\max}$ is reached.\n2.  `penalty_solver`: This function computes the single-shot penalty method solution by forming and solving the corresponding linear system.\n\nFor each test case, we will run both solvers and compute the four required metrics: the number of ADMM iterations $N_{\\text{it}}$, the final data misfit $\\phi(m) = \\|d - Fm\\|_2^2$ for the ADMM solution, the final constraint violation $\\|Am - b\\|_2$ for the ADMM solution, and the constraint violation $\\|A m_{\\text{pen}} - b\\|_2$ for the penalty method solution.",
            "answer": "```python\nimport numpy as np\n\ndef admm_solver(F, A, d, b, rho, m0, u0, eps_pri, eps_mod, n_max):\n    \"\"\"\n    Solves a constrained least-squares problem using the Alternating Direction\n    Method of Multipliers (ADMM) with a scaled dual variable.\n    \"\"\"\n    m_k = np.copy(m0)\n    u_k = np.copy(u0)\n\n    H = 2 * F.T @ F + rho * A.T @ A\n    \n    # Pre-compute inverse for efficiency in the loop, as H is constant.\n    # For a small 3x3 system, this is fine. For larger systems, a\n    # factorization (e.g., LU or Cholesky) would be more stable and efficient.\n    H_inv = np.linalg.inv(H)\n    \n    FTd_x2 = 2 * F.T @ d\n    rho_AT = rho * A.T\n\n    for k in range(n_max):\n        m_prev = np.copy(m_k)\n        \n        # m-minimization step\n        g_k = FTd_x2 + rho_AT @ (b - u_k)\n        m_k = H_inv @ g_k\n        \n        # u-update step\n        primal_residual_vec = A @ m_k - b\n        u_k = u_k + primal_residual_vec\n        \n        # Check for convergence\n        primal_residual_norm = np.linalg.norm(primal_residual_vec)\n        model_update_norm = np.linalg.norm(m_k - m_prev)\n        \n        if primal_residual_norm <= eps_pri and model_update_norm <= eps_mod:\n            return m_k, k + 1\n            \n    return m_k, n_max\n\ndef penalty_solver(F, A, d, b, rho):\n    \"\"\"\n    Solves the penalized least-squares problem in a single shot.\n    \"\"\"\n    H = 2 * F.T @ F + rho * A.T @ A\n    g = 2 * F.T @ d + rho * A.T @ b\n    \n    # Solve the linear system H * m_pen = g\n    m_pen = np.linalg.solve(H, g)\n    return m_pen\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases\n    test_cases = [\n        { # Case 1\n            \"F\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]),\n            \"A\": np.array([[1, 1, 0], [0, 1, 1]]),\n            \"d\": np.array([1, 2, 3, 6]),\n            \"b\": np.array([3, 5]),\n            \"rho\": 1.0,\n        },\n        { # Case 2\n            \"F\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]),\n            \"A\": np.array([[1, 1, 0], [0, 1, 1]]),\n            \"d\": np.array([1, 2, 3, 5]),\n            \"b\": np.array([3, 5]),\n            \"rho\": 1.0,\n        },\n        { # Case 3\n            \"F\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]),\n            \"A\": np.array([[1, 1, 0], [0, 1, 1]]),\n            \"d\": np.array([1, 2, 3, 6]),\n            \"b\": np.array([3, 5]),\n            \"rho\": 1e-2,\n        },\n        { # Case 4\n            \"F\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]),\n            \"A\": np.array([[1, 1, 0], [0, 1, 1]]),\n            \"d\": np.array([1, 2, 3, 6]),\n            \"b\": np.array([3, 5]),\n            \"rho\": 5e2,\n        },\n    ]\n\n    # Solver parameters\n    n = 3  # dimension of m\n    k = 2  # number of constraints\n    m0 = np.zeros(n)\n    u0 = np.zeros(k)\n    eps_pri = 1e-10\n    eps_mod = 1e-10\n    n_max = 2000\n\n    all_results = []\n    \n    for case in test_cases:\n        F, A, d, b, rho = case[\"F\"], case[\"A\"], case[\"d\"], case[\"b\"], case[\"rho\"]\n\n        # Run ADMM solver\n        m_admm, n_it = admm_solver(F, A, d, b, rho, m0, u0, eps_pri, eps_mod, n_max)\n\n        # Run penalty method solver\n        m_pen = penalty_solver(F, A, d, b, rho)\n\n        # Calculate final metrics\n        phi_m_admm = np.linalg.norm(d - F @ m_admm)**2\n        res_admm = np.linalg.norm(A @ m_admm - b)\n        res_pen = np.linalg.norm(A @ m_pen - b)\n        \n        # Store results for this case\n        case_results = [n_it, phi_m_admm, res_admm, res_pen]\n        all_results.append(case_results)\n\n    # Format the output string as specified\n    result_strings = []\n    for res in all_results:\n        # Use a consistent, high-precision format for floats\n        formatted_res = [f\"{v}\" if isinstance(v, int) else f\"{v:.12g}\" for v in res]\n        result_strings.append(f\"[{','.join(formatted_res)}]\")\n    \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}