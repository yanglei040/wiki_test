{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of direct solvers for large-scale geophysical problems hinges on exploiting matrix sparsity. A key challenge during factorization is 'fill-in,' where zero entries become non-zero, increasing memory and computational costs. This first exercise provides a foundational, step-by-step analysis of Gaussian elimination on a tridiagonal matrix, a common structure in finite difference methods, to demonstrate precisely how the band structure is preserved and why no fill-in occurs in this ideal case .",
            "id": "3584574",
            "problem": "Consider the discrete Laplacian operator arising in computational geophysics from a central-difference approximation of steady diffusion along a one-dimensional transect extracted from a two-dimensional five-point stencil on a $5 \\times 1$ grid with homogeneous Dirichlet boundary conditions. This restriction yields the standard tridiagonal stiffness matrix $\\mathbf{A} \\in \\mathbb{R}^{5 \\times 5}$ with $2$ on the diagonal and $-1$ on the first sub- and super-diagonals:\n$$\n\\mathbf{A} \\;=\\;\n\\begin{pmatrix}\n2 & -1 & 0 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 & 0 \\\\\n0 & -1 & 2 & -1 & 0 \\\\\n0 & 0 & -1 & 2 & -1 \\\\\n0 & 0 & 0 & -1 & 2\n\\end{pmatrix}.\n$$\nUsing first principles of Gaussian elimination and the definition of Lower-Upper (LU) factorization, perform a step-by-step elimination without any form of pivoting to transform $\\mathbf{A}$ to an upper-triangular matrix. At each step, reason from the update formulas that follow from elementary row operations and explicitly track the sparsity pattern of the evolving matrix. Use the following definition of fill-in: a fill-in is any matrix entry that is structurally zero in the original matrix $\\mathbf{A}$ but becomes structurally nonzero at any stage of elimination or in the factors of the $LU$ decomposition.\n\nStarting from the core update rule for Gaussian elimination and the banded structure of $\\mathbf{A}$, demonstrate why the band structure is preserved or altered and identify all positions where fill-in could occur. Compute the total number of fill-in entries generated during the complete elimination of $\\mathbf{A}$ under these conditions. Express your final answer as a single integer.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the field of numerical linear algebra and its application to computational geophysics, specifically the finite difference method. The problem is well-posed, providing a specific matrix, a defined procedure (Gaussian elimination without pivoting), and a clear objective (compute the total fill-in). All terms are precisely defined, and the premises are factually correct.\n\nThe core of Gaussian elimination is the systematic transformation of a matrix into an upper triangular form through a sequence of elementary row operations. At step $k$ of the elimination process (for $k=1, 2, \\dots, n-1$), we eliminate the subdiagonal entries in the $k$-th column. This is achieved by updating each row $i$ (where $i > k$) using the pivot row $k$. The update rule for an element $a_{ij}$ in the matrix is given by:\n$$ a_{ij}^{(k)} \\leftarrow a_{ij}^{(k-1)} - l_{ik} a_{kj}^{(k-1)} \\quad \\text{for } i > k, j \\ge k $$\nwhere $\\mathbf{A}^{(k-1)}$ is the matrix before step $k$, $\\mathbf{A}^{(k)}$ is the matrix after step $k$ (with $\\mathbf{A}^{(0)} = \\mathbf{A}$), and $l_{ik}$ is the multiplier defined as:\n$$ l_{ik} = \\frac{a_{ik}^{(k-1)}}{a_{kk}^{(k-1)}} $$\nFill-in is defined as any entry $(i, j)$ that is structurally zero in the original matrix $\\mathbf{A}$ (i.e., $a_{ij}=0$) but becomes structurally non-zero ($a_{ij}^{(k)} \\neq 0$) at any stage $k$ of the elimination, or in the final factors $\\mathbf{L}$ and $\\mathbf{U}$ of the LU decomposition $\\mathbf{A}=\\mathbf{L}\\mathbf{U}$.\n\nThe given matrix is:\n$$ \\mathbf{A} = \\mathbf{A}^{(0)} = \\begin{pmatrix} 2 & -1 & 0 & 0 & 0 \\\\ -1 & 2 & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & -1 \\\\ 0 & 0 & 0 & -1 & 2 \\end{pmatrix} $$\nThis is a banded matrix with a lower bandwidth of $1$ and an upper bandwidth of $1$.\n\nWe now perform the elimination step-by-step.\n\n**Step 1: Elimination in column $k=1$**\nWe eliminate the entry $a_{21}^{(0)} = -1$. The pivot is $a_{11}^{(0)} = 2$.\nThe multiplier is $l_{21} = \\frac{a_{21}^{(0)}}{a_{11}^{(0)}} = \\frac{-1}{2}$.\nThe only row to be updated is row $i=2$, since $a_{i1}^{(0)} = 0$ for $i > 2$, which makes $l_{i1}=0$.\nThe update operation is $R_2 \\leftarrow R_2 - l_{21} R_1 = R_2 - (-\\frac{1}{2})R_1$.\nThe original rows are $R_1^{(0)} = (2, -1, 0, 0, 0)$ and $R_2^{(0)} = (-1, 2, -1, 0, 0)$.\nThe new row $R_2^{(1)}$ is:\n$$ R_2^{(1)} = (-1, 2, -1, 0, 0) + \\frac{1}{2}(2, -1, 0, 0, 0) = (-1+1, 2-\\frac{1}{2}, -1+0, 0+0, 0+0) = (0, \\frac{3}{2}, -1, 0, 0) $$\nThe matrix after this step is:\n$$ \\mathbf{A}^{(1)} = \\begin{pmatrix} 2 & -1 & 0 & 0 & 0 \\\\ 0 & \\frac{3}{2} & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & -1 \\\\ 0 & 0 & 0 & -1 & 2 \\end{pmatrix} $$\nThe positions $(2,4)$ and $(2,5)$ were zero in $\\mathbf{A}$ and remain zero in $\\mathbf{A}^{(1)}$. No fill-in has occurred.\n\n**Step 2: Elimination in column $k=2$**\nWe eliminate $a_{32}^{(1)} = -1$. The pivot is $a_{22}^{(1)} = \\frac{3}{2}$.\nThe multiplier is $l_{32} = \\frac{a_{32}^{(1)}}{a_{22}^{(1)}} = \\frac{-1}{3/2} = -\\frac{2}{3}$.\nThe update is $R_3 \\leftarrow R_3 - l_{32} R_2^{(1)} = R_3 + \\frac{2}{3} R_2^{(1)}$.\nThe relevant rows are $R_2^{(1)} = (0, \\frac{3}{2}, -1, 0, 0)$ and $R_3^{(1)} = (0, -1, 2, -1, 0)$.\nThe new row $R_3^{(2)}$ is:\n$$ R_3^{(2)} = (0, -1, 2, -1, 0) + \\frac{2}{3}(0, \\frac{3}{2}, -1, 0, 0) = (0, -1+1, 2-\\frac{2}{3}, -1+0, 0+0) = (0, 0, \\frac{4}{3}, -1, 0) $$\nThe matrix is now:\n$$ \\mathbf{A}^{(2)} = \\begin{pmatrix} 2 & -1 & 0 & 0 & 0 \\\\ 0 & \\frac{3}{2} & -1 & 0 & 0 \\\\ 0 & 0 & \\frac{4}{3} & -1 & 0 \\\\ 0 & 0 & -1 & 2 & -1 \\\\ 0 & 0 & 0 & -1 & 2 \\end{pmatrix} $$\nThe position $(3,5)$ was zero and remains zero. No fill-in has occurred.\n\n**Step 3: Elimination in column $k=3$**\nWe eliminate $a_{43}^{(2)} = -1$. The pivot is $a_{33}^{(2)} = \\frac{4}{3}$.\nThe multiplier is $l_{43} = \\frac{a_{43}^{(2)}}{a_{33}^{(2)}} = \\frac{-1}{4/3} = -\\frac{3}{4}$.\nThe update is $R_4 \\leftarrow R_4 - l_{43} R_3^{(2)} = R_4 + \\frac{3}{4} R_3^{(2)}$.\nThe relevant rows are $R_3^{(2)} = (0, 0, \\frac{4}{3}, -1, 0)$ and $R_4^{(2)} = (0, 0, -1, 2, -1)$.\nThe new row $R_4^{(3)}$ is:\n$$ R_4^{(3)} = (0, 0, -1, 2, -1) + \\frac{3}{4}(0, 0, \\frac{4}{3}, -1, 0) = (0, 0, -1+1, 2-\\frac{3}{4}, -1+0) = (0, 0, 0, \\frac{5}{4}, -1) $$\nThe matrix becomes:\n$$ \\mathbf{A}^{(3)} = \\begin{pmatrix} 2 & -1 & 0 & 0 & 0 \\\\ 0 & \\frac{3}{2} & -1 & 0 & 0 \\\\ 0 & 0 & \\frac{4}{3} & -1 & 0 \\\\ 0 & 0 & 0 & \\frac{5}{4} & -1 \\\\ 0 & 0 & 0 & -1 & 2 \\end{pmatrix} $$\nNo new non-zero entries have been created. No fill-in has occurred.\n\n**Step 4: Elimination in column $k=4$**\nWe eliminate $a_{54}^{(3)} = -1$. The pivot is $a_{44}^{(3)} = \\frac{5}{4}$.\nThe multiplier is $l_{54} = \\frac{a_{54}^{(3)}}{a_{44}^{(3)}} = \\frac{-1}{5/4} = -\\frac{4}{5}$.\nThe update is $R_5 \\leftarrow R_5 - l_{54} R_4^{(3)} = R_5 + \\frac{4}{5} R_4^{(3)}$.\nThe relevant rows are $R_4^{(3)} = (0, 0, 0, \\frac{5}{4}, -1)$ and $R_5^{(3)} = (0, 0, 0, -1, 2)$.\nThe new row $R_5^{(4)}$ is:\n$$ R_5^{(4)} = (0, 0, 0, -1, 2) + \\frac{4}{5}(0, 0, 0, \\frac{5}{4}, -1) = (0, 0, 0, -1+1, 2-\\frac{4}{5}) = (0, 0, 0, 0, \\frac{6}{5}) $$\nThe final upper triangular matrix, $\\mathbf{U}$, is:\n$$ \\mathbf{U} = \\mathbf{A}^{(4)} = \\begin{pmatrix} 2 & -1 & 0 & 0 & 0 \\\\ 0 & \\frac{3}{2} & -1 & 0 & 0 \\\\ 0 & 0 & \\frac{4}{3} & -1 & 0 \\\\ 0 & 0 & 0 & \\frac{5}{4} & -1 \\\\ 0 & 0 & 0 & 0 & \\frac{6}{5} \\end{pmatrix} $$\nAt no stage of the elimination did any structurally zero entry become non-zero.\n\nNow, we must also examine the factors $\\mathbf{L}$ and $\\mathbf{U}$ for fill-in.\nThe upper triangular factor $\\mathbf{U}$ is the matrix $\\mathbf{A}^{(4)}$ shown above. The original matrix $\\mathbf{A}$ had zero entries for all $(i,j)$ where $j-i > 1$. The resulting matrix $\\mathbf{U}$ also has zero entries for all $(i,j)$ where $j-i > 1$. Comparing the sparsity structure of the upper triangles of $\\mathbf{A}$ and $\\mathbf{U}$, we find no new non-zero entries have appeared. Thus, there is no fill-in in $\\mathbf{U}$.\n\nThe lower triangular factor $\\mathbf{L}$ is a unit lower triangular matrix composed of the multipliers $l_{ik}$:\n$$ \\mathbf{L} = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\\\ l_{21} & 1 & 0 & 0 & 0 \\\\ l_{31} & l_{32} & 1 & 0 & 0 \\\\ l_{41} & l_{42} & l_{43} & 1 & 0 \\\\ l_{51} & l_{52} & l_{53} & l_{54} & 1 \\end{pmatrix} $$\nFrom our step-by-step analysis, we found the only non-zero multipliers were $l_{21}$, $l_{32}$, $l_{43}$, and $l_{54}$. All other multipliers $l_{ik}$ were zero because the corresponding entry $a_{ik}^{(k-1)}$ to be eliminated was already zero. For example, $l_{31} = a_{31}^{(0)}/a_{11}^{(0)} = 0/2 = 0$.\nSubstituting the calculated values:\n$$ \\mathbf{L} = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\\\ -\\frac{1}{2} & 1 & 0 & 0 & 0 \\\\ 0 & -\\frac{2}{3} & 1 & 0 & 0 \\\\ 0 & 0 & -\\frac{3}{4} & 1 & 0 \\\\ 0 & 0 & 0 & -\\frac{4}{5} & 1 \\end{pmatrix} $$\nThe original matrix $\\mathbf{A}$ had non-zero entries in its strictly lower triangle only on the first subdiagonal, i.e., at positions $(i,j)$ where $i-j=1$. All entries where $i-j>1$ were zero. The resulting matrix $\\mathbf{L}$ also has non-zero entries in its strictly lower triangle only on the first subdiagonal. The locations of the zeros in the strictly lower triangle of $\\mathbf{A}$ are preserved in $\\mathbf{L}$. Therefore, there is no fill-in in the $\\mathbf{L}$ factor.\n\nThe reason for this behavior lies in the banded structure of $\\mathbf{A}$. At any step $k$, the update to row $i$ is $R_i \\leftarrow R_i - l_{ik} R_k$. For a tridiagonal matrix, the pivot row $R_k$ has non-zero entries only at columns $j=k$ and $j=k+1$ (assuming $j>k$). The row to be updated, $R_i$, also has a limited number of non-zero entries. An update can only create a non-zero value at $a_{ij}^{(k)}$ if $a_{ij}^{(k-1)}=0$ and both $l_{ik}\\neq 0$ and $a_{kj}^{(k-1)}\\neq 0$. For a tridiagonal matrix, $l_{ik}$ is non-zero only for $i=k+1$. The update is thus confined to $R_{k+1}$. Since $R_k$ has no non-zero entries past column $k+1$, it cannot introduce non-zeros into $R_{k+1}$ beyond column $k+1$. The band structure is therefore preserved.\n\nSince no structurally zero entry in the original matrix $\\mathbf{A}$ became non-zero during any stage of the elimination or in the final factors $\\mathbf{L}$ and $\\mathbf{U}$, the total number of fill-in entries is zero.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "While many systems in geophysics are symmetric positive definite, constrained problems often yield symmetric indefinite matrices, for which Cholesky factorization is not applicable. This practice introduces the robust $LDL^T$ factorization with Bunch-Kaufman pivoting, a standard for handling such systems. By working through a small example , you will see firsthand why and how $2 \\times 2$ pivot blocks are chosen to maintain numerical stability when diagonal entries are zero or too small.",
            "id": "3584552",
            "problem": "In a constrained normal-equations formulation of a discrete geophysical inverse problem, one often encounters symmetric indefinite systems that must be solved robustly by direct methods. Consider the symmetric indefinite matrix\n$$\nA \\;=\\; \\begin{bmatrix}\n0 & 1 & 0 & 0\\\\\n1 & 0 & 1 & 0\\\\\n0 & 1 & 0 & 1\\\\\n0 & 0 & 1 & 0\n\\end{bmatrix},\n$$\nwhich represents a simplified coupling structure arising in a saddle-point discretization. Using the definition of the Lower-Diagonal-Lower-Transpose ($LDL^T$) factorization with symmetric pivoting, and the well-tested Bunch–Kaufman (BK) strategy for symmetric indefinite matrices, carry out an exact block $LDL^T$ factorization of $A$ with symmetric permutations as needed:\n$$\nP^T A P = L D L^T,\n$$\nwhere $P$ is a permutation matrix, $L$ is unit lower-triangular, and $D$ is block-diagonal with $1 \\times 1$ or $2 \\times 2$ symmetric blocks.\n\nStarting from first principles and the BK pivot rules, explicitly identify the ordered sequence of symmetric pivots selected, the corresponding $D$ blocks, and the associated elimination steps that produce the trailing Schur complements. Then, using the multiplicativity of the determinant for matrix products and the fact that a permutation matrix has determinant of unit magnitude and a unit lower-triangular matrix has determinant $1$, compute the scalar value $\\det(D)$ for your factorization.\n\nExpress your final answer as a single exact real number (dimensionless). No rounding is required.",
            "solution": "The problem requires the block $L D L^T$ factorization of a given symmetric indefinite matrix $A$ using the Bunch-Kaufman pivoting strategy. The factorization takes the form $P^T A P = L D L^T$, where $P$ is a permutation matrix, $L$ is a unit lower-triangular matrix, and $D$ is a block-diagonal matrix with $1 \\times 1$ or $2 \\times 2$ symmetric blocks. Subsequently, we must compute the determinant of the matrix $D$.\n\nThe given matrix is:\n$$\nA = \\begin{bmatrix}\n0 & 1 & 0 & 0\\\\\n1 & 0 & 1 & 0\\\\\n0 & 1 & 0 & 1\\\\\n0 & 0 & 1 & 0\n\\end{bmatrix}\n$$\nThe Bunch-Kaufman algorithm uses a parameter $\\alpha$, typically set to $\\alpha = \\frac{1+\\sqrt{17}}{8} \\approx 0.64$. The choice of pivot at each step $k$ depends on a series of comparisons involving this parameter.\n\n**Step 1: Factorization of $A$ (k=1)**\n\nWe begin with the full matrix, which we denote as $A^{(1)} = A$. We examine the first column to select a pivot.\nThe diagonal element is $a_{11}^{(1)} = 0$.\nThe magnitude of the largest off-diagonal element in the first column is $\\lambda_1 = \\max_{i>1} |a_{i1}^{(1)}| = \\max(|a_{21}|, |a_{31}|, |a_{41}|) = \\max(1, 0, 0) = 1$. The row index of this element is $r=2$.\n\nAccording to the Bunch-Kaufman strategy, we first test if a $1 \\times 1$ pivot from the current diagonal element $a_{11}^{(1)}$ is suitable. The condition is $|a_{11}^{(1)}| \\ge \\alpha \\lambda_1$.\nSubstituting the values, we check if $|0| \\ge \\alpha \\cdot 1$, which is false. Thus, a $1 \\times 1$ pivot at position $(1,1)$ is not stable.\n\nNext, we consider either swapping rows/columns to bring a suitable $1 \\times 1$ pivot to the $(1,1)$ position or using a $2 \\times 2$ pivot. The decision is based on the element $a_{rr}^{(1)} = a_{22} = 0$. We compute the largest off-diagonal element in row $r=2$, denoted by $\\sigma_1 = \\max_{j\\ne 2} |a_{2j}| = \\max(|a_{21}|, |a_{23}|, |a_{24}|) = \\max(1,1,0)=1$.\nThe condition to swap rows/columns $1$ and $r=2$ and use a $1 \\times 1$ pivot is $|a_{rr}^{(1)}| \\ge \\alpha \\sigma_1$.\nSubstituting values, we check if $|0| \\ge \\alpha \\cdot 1$, which is also false.\n\nSince both conditions for a $1 \\times 1$ pivot fail, the Bunch-Kaufman algorithm mandates the use of a $2 \\times 2$ pivot. The pivot is formed by rows and columns $1$ and $r=2$. Per the algorithm, row/column $2$ is swapped with row/column $k+1=2$. This is an identity permutation, so no reordering is needed at this step. The initial permutation matrix is $P_1 = I$.\n\nThe first pivot block, $D_1$, is the leading $2 \\times 2$ submatrix of $A$:\n$$\nD_1 = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}\n$$\nWe partition the matrix $A$ as:\n$$\nA = \\begin{bmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix} = \\begin{bmatrix} \\begin{smallmatrix} 0 & 1 \\\\ 1 & 0 \\end{smallmatrix} & \\begin{smallmatrix} 0 & 0 \\\\ 1 & 0 \\end{smallmatrix} \\\\ \\begin{smallmatrix} 0 & 1 \\\\ 0 & 0 \\end{smallmatrix} & \\begin{smallmatrix} 0 & 1 \\\\ 1 & 0 \\end{smallmatrix} \\end{bmatrix}\n$$\nThe first step of the block factorization yields:\n$$\nA = \\begin{bmatrix} I & 0 \\\\ L_{21} & I \\end{bmatrix} \\begin{bmatrix} D_1 & 0 \\\\ 0 & S \\end{bmatrix} \\begin{bmatrix} I & L_{21}^T \\\\ 0 & I \\end{bmatrix}\n$$\nwhere $L_{21} = A_{21}D_1^{-1}$ and the Schur complement is $S = A_{22} - A_{21}D_1^{-1}A_{12} = A_{22} - L_{21}A_{12}$.\n\nFirst, we compute the inverse of $D_1$:\n$$\nD_1^{-1} = \\frac{1}{(0)(0)-(1)(1)} \\begin{bmatrix} 0 & -1 \\\\ -1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}\n$$\nNext, we compute the block $L_{21}$:\n$$\nL_{21} = A_{21}D_1^{-1} = \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}\n$$\nNow, we compute the Schur complement $S$:\n$$\nS = A_{22} - L_{21}A_{12} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} - \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} - \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}\n$$\n\n**Step 2: Factorization of the Schur Complement $S$**\n\nThe remaining matrix to factor is $A^{(2)} = S = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$. We apply the same Bunch-Kaufman logic.\nThe first diagonal element is $s_{11}=0$. The largest off-diagonal element in the first column is $|s_{21}|=1$, so $\\lambda=1$ and $r=2$.\nThe condition for a $1 \\times 1$ pivot $|s_{11}| \\ge \\alpha \\lambda$ ($|0| \\ge \\alpha \\cdot 1$) is false.\nThe largest off-diagonal element in row $r=2$ is $|s_{21}|=1$, so $\\sigma=1$.\nThe condition for swapping and using a $1 \\times 1$ pivot $|s_{rr}| \\ge \\alpha \\sigma$ ($|s_{22}| \\ge \\alpha \\sigma \\implies |0| \\ge \\alpha \\cdot 1$) is also false.\nTherefore, we must use a $2 \\times 2$ pivot, which is the entire matrix $S$. This constitutes the second pivot block, $D_2$:\n$$\nD_2 = S = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}\n$$\nThe factorization is now complete. No further permutations were necessary, so the overall permutation matrix is the identity matrix, $P=I$.\n\n**Assembling the Factorization**\n\nThe factorization $A = L D L^T$ is constructed from the computed components.\nThe permutation matrix is $P=I$.\nThe block-diagonal matrix $D$ consists of the pivot blocks $D_1$ and $D_2$:\n$$\nD = \\begin{bmatrix} D_1 & 0 \\\\ 0 & D_2 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix}\n$$\nThe unit lower-triangular matrix $L$ is formed by the identity and the computed multiplier block $L_{21}$:\n$$\nL = \\begin{bmatrix} I & 0 \\\\ L_{21} & I \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}\n$$\nTo verify the result, we compute $L D L^T$:\n$$\nL D L^T = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}\n$$\n$$\n= \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 1 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} = A\n$$\nThe factorization is correct. The sequence of pivots consisted of two $2 \\times 2$ blocks.\n\n**Computation of det(D)**\n\nThe problem requires the computation of $\\det(D)$. From the property of determinants, we know that $\\det(P^T A P) = \\det(L D L^T)$.\nThis simplifies to $\\det(A) = \\det(D)$, because $\\det(P^T)\\det(P) = (\\det(P))^2 = 1$ and $\\det(L) = \\det(L^T)=1$.\nWe can calculate $\\det(D)$ directly from its block-diagonal structure. The determinant of a block-diagonal matrix is the product of the determinants of its diagonal blocks.\n$$\n\\det(D) = \\det(D_1) \\cdot \\det(D_2)\n$$\nThe determinants of the individual blocks are:\n$$\n\\det(D_1) = \\det \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = (0)(0) - (1)(1) = -1\n$$\n$$\n\\det(D_2) = \\det \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = (0)(0) - (1)(1) = -1\n$$\nTherefore, the determinant of $D$ is:\n$$\n\\det(D) = (-1) \\cdot (-1) = 1\n$$\nThis value should be equal to $\\det(A)$. Expanding $\\det(A)$ along the first row confirms this:\n$$\n\\det(A) = -1 \\cdot \\det \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix} = -1 \\cdot (1 \\cdot \\det \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}) = -1 \\cdot (1 \\cdot (-1)) = 1\n$$\nThe calculation is consistent. The scalar value of $\\det(D)$ for the computed factorization is $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Obtaining a numerical solution is only half the battle; we must also assess its quality and reliability. This exercise bridges the gap between numerical computation and practical acceptance by analyzing the accuracy of a given approximate solution . You will compute the residual, estimate the backward and forward errors, and see how the matrix condition number, $\\kappa(A)$, governs the relationship between them, ultimately deciding if the solution is precise enough for a geophysical application.",
            "id": "3584593",
            "problem": "A linearized seismic travel-time inversion at a single Gauss–Newton iteration requires solving a small linear system arising from a finite-difference discretization of the slowness update. The system is\n$$\nA x = b,\n$$\nwhere\n$$\nA = \\begin{pmatrix}\n3 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}, \n\\quad\nb = \\begin{pmatrix}\n5 \\\\\n4 \\\\\n1\n\\end{pmatrix}.\n$$\nA direct solve using Lower–Upper (LU) factorization with partial pivoting produced the approximate model increment\n$$\n\\hat{x} = \\begin{pmatrix}\n0.999 \\\\\n2.001 \\\\\n-0.999\n\\end{pmatrix}.\n$$\nAssume all quantities are dimensionless and use the matrix and vector infinity norms as the underlying norms. Compute the residual vector $r = b - A \\hat{x}$, estimate the relative backward error using\n$$\n\\beta = \\frac{\\|r\\|_{\\infty}}{\\|A\\|_{\\infty}\\,\\|\\hat{x}\\|_{\\infty} + \\|b\\|_{\\infty}},\n$$\nand estimate the forward relative error using the condition number $\\kappa_{\\infty}(A)$ via the standard bound\n$$\n\\frac{\\|\\hat{x} - x\\|_{\\infty}}{\\|x\\|_{\\infty}} \\lesssim \\kappa_{\\infty}(A)\\,\\beta.\n$$\nFor this inversion, the geophysical acceptance tolerance for the forward relative error is\n$$\n\\tau = 5 \\times 10^{-3}.\n$$\nDefine the acceptance indicator $S$ by\n$$\nS = \\begin{cases}\n1, & \\text{if } \\kappa_{\\infty}(A)\\,\\beta \\le \\tau, \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\nCompute $S$ and provide it as the final answer. No rounding is required, and the final answer must be a single real number.",
            "solution": "The objective of this problem is to determine the value of an acceptance indicator, $S$, for an approximate solution to a linear system. The value of $S$ depends on whether an estimate for the forward relative error is within a given geophysical tolerance $\\tau$. The problem is defined by the linear system $Ax = b$, where\n$$\nA = \\begin{pmatrix}\n3 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}, \n\\quad\nb = \\begin{pmatrix}\n5 \\\\\n4 \\\\\n1\n\\end{pmatrix}.\n$$\nAn approximate solution $\\hat{x}$ is provided:\n$$\n\\hat{x} = \\begin{pmatrix}\n0.999 \\\\\n2.001 \\\\\n-0.999\n\\end{pmatrix}.\n$$\nThe acceptance indicator $S$ is defined as\n$$\nS = \\begin{cases}\n1, & \\text{if } \\kappa_{\\infty}(A)\\,\\beta \\le \\tau, \\\\\n0, & \\text{otherwise,}\n\\end{cases}\n$$\nwhere $\\tau=5 \\times 10^{-3}$, $\\kappa_{\\infty}(A)$ is the condition number of $A$ in the infinity norm, and $\\beta$ is the relative backward error.\n\nTo compute $S$, we must first calculate the quantities $\\beta$ and $\\kappa_{\\infty}(A)$.\n\nThe relative backward error $\\beta$ is given by the formula:\n$$\n\\beta = \\frac{\\|r\\|_{\\infty}}{\\|A\\|_{\\infty}\\,\\|\\hat{x}\\|_{\\infty} + \\|b\\|_{\\infty}}\n$$\nThis requires the computation of the residual vector $r = b - A\\hat{x}$ and several infinity norms.\n\nFirst, we compute the product $A\\hat{x}$:\n$$\nA\\hat{x} = \\begin{pmatrix}\n3 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n0.999 \\\\\n2.001 \\\\\n-0.999\n\\end{pmatrix}\n= \\begin{pmatrix}\n3(0.999) + 1(2.001) + 0(-0.999) \\\\\n1(0.999) + 2(2.001) + 1(-0.999) \\\\\n0(0.999) + 1(2.001) + 1(-0.999)\n\\end{pmatrix}\n= \\begin{pmatrix}\n2.997 + 2.001 \\\\\n0.999 + 4.002 - 0.999 \\\\\n2.001 - 0.999\n\\end{pmatrix}\n= \\begin{pmatrix}\n4.998 \\\\\n4.002 \\\\\n1.002\n\\end{pmatrix}.\n$$\nNext, we compute the residual vector $r$:\n$$\nr = b - A\\hat{x} = \\begin{pmatrix}\n5 \\\\\n4 \\\\\n1\n\\end{pmatrix} - \\begin{pmatrix}\n4.998 \\\\\n4.002 \\\\\n1.002\n\\end{pmatrix} = \\begin{pmatrix}\n0.002 \\\\\n-0.002 \\\\\n-0.002\n\\end{pmatrix}.\n$$\nNow, we compute the required infinity norms. The infinity norm of a vector is the maximum absolute value of its components, and for a matrix, it is the maximum absolute row sum.\n$$\n\\|r\\|_{\\infty} = \\max(|0.002|, |-0.002|, |-0.002|) = 0.002.\n$$\n$$\n\\|A\\|_{\\infty} = \\max(|3|+|1|+|0|, |1|+|2|+|1|, |0|+|1|+|1|) = \\max(4, 4, 2) = 4.\n$$\n$$\n\\|\\hat{x}\\|_{\\infty} = \\max(|0.999|, |2.001|, |-0.999|) = 2.001.\n$$\n$$\n\\|b\\|_{\\infty} = \\max(|5|, |4|, |1|) = 5.\n$$\nWe can now calculate $\\beta$:\n$$\n\\beta = \\frac{0.002}{4(2.001) + 5} = \\frac{0.002}{8.004 + 5} = \\frac{0.002}{13.004} = \\frac{2}{13004} = \\frac{1}{6502}.\n$$\nNext, we determine the condition number $\\kappa_{\\infty}(A) = \\|A\\|_{\\infty} \\|A^{-1}\\|_{\\infty}$. We already have $\\|A\\|_{\\infty} = 4$. We need to find $A^{-1}$. The inverse is given by $A^{-1} = \\frac{1}{\\det(A)}\\text{adj}(A)$.\nThe determinant of $A$ is:\n$$\n\\det(A) = 3(2 \\cdot 1 - 1 \\cdot 1) - 1(1 \\cdot 1 - 1 \\cdot 0) + 0 = 3(1) - 1(1) = 2.\n$$\nThe matrix of cofactors is:\n$$\nC = \\begin{pmatrix}\n(2\\cdot 1 - 1\\cdot 1) & -(1\\cdot 1 - 1\\cdot 0) & (1\\cdot 1 - 2\\cdot 0) \\\\\n-(1\\cdot 1 - 0\\cdot 1) & (3\\cdot 1 - 0\\cdot 0) & -(3\\cdot 1 - 1\\cdot 0) \\\\\n(1\\cdot 1 - 2\\cdot 0) & -(3\\cdot 1 - 1\\cdot 0) & (3\\cdot 2 - 1\\cdot 1)\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 & -1 & 1 \\\\\n-1 & 3 & -3 \\\\\n1 & -3 & 5\n\\end{pmatrix}.\n$$\nThe adjugate of $A$ is the transpose of the cofactor matrix, $\\text{adj}(A) = C^T$. Since $C$ is symmetric, $\\text{adj}(A) = C$.\n$$\nA^{-1} = \\frac{1}{2}\\begin{pmatrix}\n1 & -1 & 1 \\\\\n-1 & 3 & -3 \\\\\n1 & -3 & 5\n\\end{pmatrix} = \\begin{pmatrix}\n0.5 & -0.5 & 0.5 \\\\\n-0.5 & 1.5 & -1.5 \\\\\n0.5 & -1.5 & 2.5\n\\end{pmatrix}.\n$$\nThe infinity norm of $A^{-1}$ is the maximum absolute row sum:\n$$\n\\|A^{-1}\\|_{\\infty} = \\max(|0.5|+|-0.5|+|0.5|, |-0.5|+|1.5|+|-1.5|, |0.5|+|-1.5|+|2.5|)\n$$\n$$\n\\|A^{-1}\\|_{\\infty} = \\max(1.5, 3.5, 4.5) = 4.5.\n$$\nNow, we can compute the condition number:\n$$\n\\kappa_{\\infty}(A) = \\|A\\|_{\\infty} \\|A^{-1}\\|_{\\infty} = 4 \\times 4.5 = 18.\n$$\nFinally, we check the condition for the acceptance indicator $S$. We need to evaluate if $\\kappa_{\\infty}(A)\\beta \\le \\tau$.\n$$\n\\kappa_{\\infty}(A)\\beta = 18 \\times \\frac{1}{6502} = \\frac{18}{6502} = \\frac{9}{3251}.\n$$\nThe tolerance is $\\tau = 5 \\times 10^{-3} = \\frac{5}{1000} = \\frac{1}{200}$.\nWe check the inequality:\n$$\n\\frac{9}{3251} \\le \\frac{1}{200}.\n$$\nBy cross-multiplication, this is equivalent to checking if $9 \\times 200 \\le 3251 \\times 1$:\n$$\n1800 \\le 3251.\n$$\nThis inequality is true. Therefore, the condition $\\kappa_{\\infty}(A)\\beta \\le \\tau$ is satisfied.\nAccording to the definition of the acceptance indicator $S$, if the condition is met, $S=1$.\n$$\nS = 1.\n$$",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}