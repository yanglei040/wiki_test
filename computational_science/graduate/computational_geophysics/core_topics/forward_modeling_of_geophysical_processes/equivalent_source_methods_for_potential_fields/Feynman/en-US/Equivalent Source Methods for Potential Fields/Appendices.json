{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of the equivalent source method is the accurate and efficient computation of the forward model. This practice focuses on deriving the analytical expressions for the gravitational potential and its vertical derivative from a single, uniformly dense rectangular source element. Mastering this derivation  is essential, as these closed-form solutions form the building blocks of the sensitivity matrix $\\mathbf{G}$ for models that discretize the source plane into a grid of such patches.",
            "id": "3589271",
            "problem": "Consider the equivalent source method that represents a mass distribution on a plane by a tessellation of uniformly dense rectangular patches. Let a single rectangular patch lie in the plane $z=0$, with uniform areal density $\\sigma$ (mass per unit area), and be bounded by $x \\in [x_1, x_2]$ and $y \\in [y_1, y_2]$. Let the observation point be $\\mathbf{r}_0 = (x_0, y_0, z_0)$ with $z_0 > 0$ (the vertical coordinate is positive upward). Using Newtonian gravitation with gravitational constant $G$, the gravitational potential $U(\\mathbf{r}_0)$ due to the patch is defined by\n$$\nU(\\mathbf{r}_0) = G \\sigma \\iint_{\\text{patch}} \\frac{1}{\\|\\mathbf{r}_0 - \\mathbf{r}\\|} \\, \\mathrm{d}A,\n$$\nand the gravitational acceleration is $\\mathbf{g}(\\mathbf{r}_0) = \\nabla U(\\mathbf{r}_0)$, whose vertical component is $g_z(\\mathbf{r}_0) = \\frac{\\partial U}{\\partial z_0}(\\mathbf{r}_0)$.\n\nStarting from these definitions and first principles of potential theory, derive closed-form analytic expressions for $U(\\mathbf{r}_0)$ and $g_z(\\mathbf{r}_0)$ in terms of $x_1$, $x_2$, $y_1$, $y_2$, $x_0$, $y_0$, $z_0$, $\\sigma$, and $G$. Your derivation must be valid for an arbitrary observation point with $z_0>0$. Express your final result using corner-based inclusion-exclusion with four signed contributions that depend on the relative offsets to the observation point. No numerical evaluation is required. Provide the final answer as analytic expressions. Do not include units in the final boxed expressions.",
            "solution": "The problem requires the derivation of closed-form expressions for the gravitational potential $U(\\mathbf{r}_0)$ and the vertical component of gravitational acceleration $g_z(\\mathbf{r}_0)$ produced by a uniformly dense rectangular patch. The derivation must start from first principles and be expressed using a corner-based inclusion-exclusion formulation.\n\nLet the observation point be $\\mathbf{r}_0 = (x_0, y_0, z_0)$ with $z_0 > 0$. The rectangular patch is in the $z=0$ plane, bounded by $x \\in [x_1, x_2]$ and $y \\in [y_1, y_2]$, with uniform areal density $\\sigma$. A point on the patch is $\\mathbf{r} = (x, y, 0)$.\n\nThe distance between the observation point and a point on the patch is:\n$$ \\|\\mathbf{r}_0 - \\mathbf{r}\\| = \\sqrt{(x-x_0)^2 + (y-y_0)^2 + (z_0-0)^2} $$\nFor simplicity, we introduce relative coordinates: $\\xi = x - x_0$ and $\\eta = y - y_0$. The integration is performed over the shifted rectangle $[\\xi_1, \\xi_2] \\times [\\eta_1, \\eta_2]$, where $\\xi_i = x_i - x_0$ and $\\eta_j = y_j - y_0$.\n\nThe gravitational potential $U(\\mathbf{r}_0)$ is given by the integral:\n$$ U(\\mathbf{r}_0) = G \\sigma \\int_{y_1}^{y_2} \\int_{x_1}^{x_2} \\frac{1}{\\sqrt{(x-x_0)^2 + (y-y_0)^2 + z_0^2}} \\, dx \\, dy $$\nIn the shifted coordinates:\n$$ U(\\mathbf{r}_0) = G \\sigma \\int_{\\eta_1}^{\\eta_2} \\int_{\\xi_1}^{\\xi_2} \\frac{1}{\\sqrt{\\xi^2 + \\eta^2 + z_0^2}} \\, d\\xi \\, d\\eta $$\n\nThe vertical component of gravitational acceleration, $g_z(\\mathbf{r}_0)$, is the partial derivative of the potential with respect to $z_0$:\n$$ g_z(\\mathbf{r}_0) = \\frac{\\partial U}{\\partial z_0} = \\frac{\\partial}{\\partial z_0} \\left( G \\sigma \\iint \\frac{1}{\\sqrt{\\xi^2 + \\eta^2 + z_0^2}} \\, d\\xi \\, d\\eta \\right) $$\nUnder the condition $z_0 > 0$, the integrand is continuously differentiable, allowing us to interchange differentiation and integration (Leibniz integral rule):\n$$ g_z(\\mathbf{r}_0) = G \\sigma \\iint \\frac{\\partial}{\\partial z_0} \\left( (\\xi^2 + \\eta^2 + z_0^2)^{-1/2} \\right) \\, d\\xi \\, d\\eta $$\n$$ g_z(\\mathbf{r}_0) = G \\sigma \\iint -\\frac{1}{2} (\\xi^2 + \\eta^2 + z_0^2)^{-3/2} (2 z_0) \\, d\\xi \\, d\\eta $$\n$$ g_z(\\mathbf{r}_0) = -G \\sigma z_0 \\int_{\\eta_1}^{\\eta_2} \\int_{\\xi_1}^{\\xi_2} (\\xi^2 + \\eta^2 + z_0^2)^{-3/2} \\, d\\xi \\, d\\eta $$\n\n**Derivation of the Vertical Acceleration $g_z(\\mathbf{r}_0)$**\n\nWe solve the integral for $g_z$ by integrating first with respect to $\\xi$ and then $\\eta$. The order is interchangeable. Let's integrate with respect to $\\eta$ first.\nLet $A^2 = \\xi^2 + z_0^2$. The inner integral is:\n$$ I_\\eta = \\int \\frac{d\\eta}{(\\eta^2 + A^2)^{3/2}} $$\nThis is a standard integral, which evaluates to:\n$$ I_\\eta = \\frac{\\eta}{A^2 \\sqrt{\\eta^2 + A^2}} = \\frac{\\eta}{(\\xi^2+z_0^2)\\sqrt{\\xi^2+\\eta^2+z_0^2}} $$\nEvaluating this over the interval $[\\eta_1, \\eta_2]$:\n$$ \\int_{\\eta_1}^{\\eta_2} (\\xi^2 + \\eta^2 + z_0^2)^{-3/2} \\, d\\eta = \\frac{\\eta_2}{(\\xi^2+z_0^2)\\sqrt{\\xi^2+\\eta_2^2+z_0^2}} - \\frac{\\eta_1}{(\\xi^2+z_0^2)\\sqrt{\\xi^2+\\eta_1^2+z_0^2}} $$\nNow, we integrate this result with respect to $\\xi$ over $[\\xi_1, \\xi_2]$. This requires evaluating an integral of the form:\n$$ I_\\xi = \\int \\frac{\\eta}{(\\xi^2+z_0^2)\\sqrt{\\xi^2+\\eta^2+z_0^2}} \\, d\\xi $$\nThis is also a standard, though less common, integral. It evaluates to:\n$$ I_\\xi = \\frac{1}{z_0} \\arctan\\left(\\frac{\\xi\\eta}{z_0\\sqrt{\\xi^2+\\eta^2+z_0^2}}\\right) $$\nLet's define the kernel for the indefinite integral of the $g_z$ integrand as $\\Psi_{g_z}(\\xi, \\eta, z_0)$.\nThe indefinite double integral of $-z_0(\\xi^2 + \\eta^2 + z_0^2)^{-3/2}$ is:\n$$ \\Psi_{g_z}(\\xi, \\eta, z_0) = -z_0 \\left( \\frac{1}{z_0} \\arctan\\left(\\frac{\\xi\\eta}{z_0\\sqrt{\\xi^2+\\eta^2+z_0^2}}\\right) \\right) = -\\arctan\\left(\\frac{\\xi\\eta}{z_0\\sqrt{\\xi^2+\\eta^2+z_0^2}}\\right) $$\nThe definite double integral over the rectangle $[\\xi_1, \\xi_2] \\times [\\eta_1, \\eta_2]$ is found by the inclusion-exclusion principle:\n$$ \\int_{\\eta_1}^{\\eta_2} \\int_{\\xi_1}^{\\xi_2} f(\\xi, \\eta) d\\xi d\\eta = F(\\xi_2, \\eta_2) - F(\\xi_2, \\eta_1) - F(\\xi_1, \\eta_2) + F(\\xi_1, \\eta_1) $$\nwhere $F$ is the indefinite double integral of $f$.\nApplying this to $g_z$:\n$$ g_z(\\mathbf{r}_0) = G \\sigma \\left[ \\Psi_{g_z}(\\xi_2, \\eta_2) - \\Psi_{g_z}(\\xi_2, \\eta_1) - \\Psi_{g_z}(\\xi_1, \\eta_2) + \\Psi_{g_z}(\\xi_1, \\eta_1) \\right] $$\nThis can be written as a sum over the four corners $(i,j)$ where $i,j \\in \\{1,2\\}$:\n$$ g_z(\\mathbf{r}_0) = G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\Psi_{g_z}(\\xi_i, \\eta_j, z_0) $$\nSubstituting the expression for $\\Psi_{g_z}$:\n$$ g_z(\\mathbf{r}_0) = -G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\arctan\\left(\\frac{\\xi_i\\eta_j}{z_0 r_{ij}}\\right) $$\nwhere $\\xi_i = x_i - x_0$, $\\eta_j = y_j - y_0$, and $r_{ij} = \\sqrt{\\xi_i^2 + \\eta_j^2 + z_0^2}$.\n\n**Derivation of the Gravitational Potential $U(\\mathbf{r}_0)$**\n\nThe direct integration of $1/r$ is more involved.\n$$ U(\\mathbf{r}_0) = G \\sigma \\int_{\\eta_1}^{\\eta_2} \\left[ \\int_{\\xi_1}^{\\xi_2} \\frac{d\\xi}{\\sqrt{\\xi^2 + \\eta^2 + z_0^2}} \\right] d\\eta $$\nThe inner integral with respect to $\\xi$ is:\n$$ \\int \\frac{d\\xi}{\\sqrt{\\xi^2 + (\\eta^2 + z_0^2)}} = \\ln(\\xi + \\sqrt{\\xi^2 + \\eta^2 + z_0^2}) $$\nThe subsequent integration with respect to $\\eta$ of the $\\ln(\\cdot)$ term is complex. A more robust method is to identify the indefinite double integral kernel, $\\Phi_U(\\xi, \\eta, z_0)$, and verify it. The kernel must satisfy:\n$$ \\frac{\\partial^2 \\Phi_U}{\\partial\\xi \\partial\\eta} = \\frac{1}{\\sqrt{\\xi^2 + \\eta^2 + z_0^2}} $$\nThe known kernel for this integral is:\n$$ \\Phi_U(\\xi, \\eta, z_0) = \\xi \\ln(\\eta + r) + \\eta \\ln(\\xi + r) - z_0 \\arctan\\left(\\frac{\\xi\\eta}{z_0 r}\\right) $$\nwhere $r = \\sqrt{\\xi^2+\\eta^2+z_0^2}$. We verify this by differentiation. First, with respect to $\\eta$:\n$$ \\frac{\\partial \\Phi_U}{\\partial \\eta} = \\xi \\frac{1 + \\partial r/\\partial \\eta}{\\eta + r} + \\ln(\\xi+r) + \\eta \\frac{\\partial r/\\partial \\eta}{\\xi+r} - z_0 \\frac{\\partial}{\\partial \\eta}\\left[\\arctan\\left(\\frac{\\xi\\eta}{z_0 r}\\right)\\right] $$\nUsing $\\frac{\\partial r}{\\partial \\eta} = \\frac{\\eta}{r}$:\n$$ \\frac{\\partial \\Phi_U}{\\partial \\eta} = \\xi \\frac{1 + \\eta/r}{\\eta + r} + \\ln(\\xi+r) + \\frac{\\eta^2/r}{\\xi+r} - z_0 \\frac{\\partial}{\\partial \\eta}\\left[\\arctan\\left(\\frac{\\xi\\eta}{z_0 r}\\right)\\right] $$\n$$ \\frac{\\partial \\Phi_U}{\\partial \\eta} = \\frac{\\xi}{r} + \\ln(\\xi+r) + \\frac{\\eta^2}{r(\\xi+r)} - z_0 \\frac{\\xi z_0 (\\xi^2+z_0^2)}{r((\\xi^2+z_0^2)(\\eta^2+z_0^2))} = \\frac{\\xi}{r} + \\ln(\\xi+r) + \\frac{\\eta^2}{r(\\xi+r)} - \\frac{\\xi z_0^2}{r(\\eta^2+z_0^2)} $$\nNow, differentiating with respect to $\\xi$, and using $\\frac{\\partial r}{\\partial \\xi} = \\frac{\\xi}{r}$:\n$$ \\frac{\\partial^2 \\Phi_U}{\\partial\\xi \\partial\\eta} = \\frac{\\partial}{\\partial\\xi} \\left( \\frac{\\xi}{r} \\right) + \\frac{\\partial}{\\partial\\xi} \\left( \\ln(\\xi+r) \\right) + \\frac{\\partial}{\\partial\\xi} \\left( \\frac{\\eta^2}{r(\\xi+r)} \\right) - \\frac{\\partial}{\\partial\\xi} \\left( \\frac{\\xi z_0^2}{r(\\eta^2+z_0^2)} \\right) $$\nThe terms evaluate as:\n1. $\\frac{\\partial}{\\partial\\xi}(\\frac{\\xi}{r}) = \\frac{r - \\xi(\\xi/r)}{r^2} = \\frac{r^2-\\xi^2}{r^3} = \\frac{\\eta^2+z_0^2}{r^3}$\n2. $\\frac{\\partial}{\\partial\\xi}(\\ln(\\xi+r)) = \\frac{1+\\xi/r}{\\xi+r} = \\frac{1}{r}$\n3. $\\frac{\\partial}{\\partial\\xi}(\\frac{\\eta^2}{r(\\xi+r)}) = \\eta^2 \\frac{-[r(1+\\xi/r)+(\\xi/r)(\\xi+r)]}{(r(\\xi+r))^2} = -\\eta^2 \\frac{r+\\xi+\\xi+\\xi^2/r}{r^2(\\xi+r)^2} = -\\eta^2 \\frac{r^2+2\\xi r+\\xi^2}{r^3(\\xi+r)^2} = -\\frac{\\eta^2(\\xi+r)^2}{r^3(\\xi+r)^2} = -\\frac{\\eta^2}{r^3}$\n4. $-\\frac{\\partial}{\\partial\\xi}(\\frac{\\xi z_0^2}{r(\\eta^2+z_0^2)}) = -\\frac{z_0^2}{\\eta^2+z_0^2} \\frac{\\partial}{\\partial\\xi}(\\frac{\\xi}{r}) = -\\frac{z_0^2}{\\eta^2+z_0^2} \\frac{\\eta^2+z_0^2}{r^3} = -\\frac{z_0^2}{r^3}$\nSumming these four terms:\n$$ \\frac{\\partial^2 \\Phi_U}{\\partial\\xi \\partial\\eta} = \\frac{\\eta^2+z_0^2}{r^3} + \\frac{1}{r} - \\frac{\\eta^2}{r^3} - \\frac{z_0^2}{r^3} = \\frac{1}{r} $$\nThe verification is successful. The kernel $\\Phi_U(\\xi, \\eta, z_0)$ is indeed the indefinite double integral of $1/r$.\nThe potential $U(\\mathbf{r}_0)$ is obtained by applying the inclusion-exclusion principle to this kernel:\n$$ U(\\mathbf{r}_0) = G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\Phi_U(\\xi_i, \\eta_j, z_0) $$\n$$ U(\\mathbf{r}_0) = G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\left[ \\xi_i \\ln(\\eta_j+r_{ij}) + \\eta_j \\ln(\\xi_i+r_{ij}) - z_0 \\arctan\\left(\\frac{\\xi_i\\eta_j}{z_0 r_{ij}}\\right) \\right] $$\nwith $\\xi_i = x_i - x_0$, $\\eta_j = y_j - y_0$, and $r_{ij} = \\sqrt{\\xi_i^2 + \\eta_j^2 + z_0^2}$.\n\nThis completes the derivation from first principles.\n\nFinal Expressions:\nLet $\\xi_i = x_i - x_0$ and $\\eta_j = y_j - y_0$ for $i,j \\in \\{1,2\\}$. Let $r_{ij} = \\sqrt{\\xi_i^2 + \\eta_j^2 + z_0^2}$.\nThe potential is:\n$$ U(\\mathbf{r}_0) = G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\left[ \\xi_i \\ln(\\eta_j+r_{ij}) + \\eta_j \\ln(\\xi_i+r_{ij}) - z_0 \\arctan\\left(\\frac{\\xi_i\\eta_j}{z_0 r_{ij}}\\right) \\right] $$\nThe vertical gravitational acceleration is:\n$$ g_z(\\mathbf{r}_0) = -G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\arctan\\left(\\frac{\\xi_i\\eta_j}{z_0 r_{ij}}\\right) $$",
            "answer": "$$\n\\boxed{\n\\begin{aligned}\nU(\\mathbf{r}_0) = G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\left[ (x_i-x_0) \\ln(y_j-y_0+r_{ij}) + (y_j-y_0) \\ln(x_i-x_0+r_{ij}) - z_0 \\arctan\\left(\\frac{(x_i-x_0)(y_j-y_0)}{z_0 r_{ij}}\\right) \\right] \\\\\ng_z(\\mathbf{r}_0) = -G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\arctan\\left(\\frac{(x_i-x_0)(y_j-y_0)}{z_0 r_{ij}}\\right)\n\\end{aligned}\n}\n$$"
        },
        {
            "introduction": "A significant challenge in potential field inversion is the natural decay of the field's sensitivity with source depth, which can make it difficult to resolve deeper structures. This exercise  tackles this ill-conditioning by guiding you through the derivation of a depth-weighting function. Understanding how to construct and apply such a weight is a key practical skill that stabilizes the inversion process and ensures that sources at different depths are resolved with more comparable resolution.",
            "id": "3589251",
            "problem": "Consider a three-dimensional potential field measured on a horizontal observation plane at $z=0$ above a homogeneous half-space. An equivalent layer is used to represent the field by a distribution of scalar monopoles with surface density $\\sigma(x',y')$ located on a plane at depth $z=z_{0}0$. Let $U(x,y,z)$ denote the scalar potential at $(x,y,z)$ produced by the layer, and let the measured datum be the $n$th vertical derivative of the potential evaluated on the observation plane, that is $d_{n}(x,y)=\\partial^{n}U(x,y,0)/\\partial z^{n}$, where $n$ is a positive integer.\n\nYou will model the forward response using the fundamental Green’s function of the Laplace equation in three dimensions, $G(\\mathbf{r})=1/(4\\pi|\\mathbf{r}|)$, and you may assume that the sources are sufficiently localized that the on-axis behavior (the receiver directly above a source element, i.e., $(x,y)=(x',y')$) controls the leading-order depth dependence of the sensitivity for large $z_{0}$. The equivalent-layer sensitivity kernel for $d_{n}$ with respect to a unit source at $(x',y',z_{0})$ is $K_{n}(x-x',y-y';z_{0})=\\partial^{n}G(\\mathbf{r})/\\partial z^{n}$ with $\\mathbf{r}=(x-x',y-y',0-z_{0})$.\n\nDerive, from first principles and the asymptotic behavior of $G(\\mathbf{r})$ for large $z_{0}$, an explicit depth-weighting function $w(z_{0})$ that counteracts the decay of sensitivity with depth in the sense that $w(z_{0})\\,K_{n}(0,0;z_{0})$ is independent of $z_{0}$ to leading order as $z_{0}\\to\\infty$. Normalize your answer by imposing $w(1)=1$ so that the weighting is dimensionless. Provide your final expression for $w(z_{0})$ in closed form. No numerical evaluation is required, and your answer must be a single analytical expression.",
            "solution": "The problem requires the derivation of a depth-weighting function, $w(z_{0})$, for an equivalent-layer representation of a potential field. The function must counteract the decay of the on-axis sensitivity kernel with depth, $z_{0}$. The derivation will proceed from the fundamental principles laid out in the problem statement.\n\nFirst, we identify the key components. The potential field is modeled using the Green's function for the 3D Laplace equation, given as $G(\\mathbf{r}) = \\frac{1}{4\\pi|\\mathbf{r}|}$. The source layer is at a constant depth $z = z_{0}  0$, and the observation plane is at $z=0$. The vector $\\mathbf{r}$ connects a source point $(x', y', z_{0})$ to an observation point $(x, y, z)$, so $\\mathbf{r} = (x-x', y-y', z-z_{0})$. The magnitude of this vector is $|\\mathbf{r}| = \\sqrt{(x-x')^{2} + (y-y')^{2} + (z-z_{0})^{2}}$.\n\nThe sensitivity kernel for the $n$-th vertical derivative of the potential, $d_{n}(x,y)$, is given by $K_{n}(x-x',y-y';z_{0})$. This kernel represents the response at $(x,y,0)$ due to a unit source at $(x',y',z_{0})$. It is defined as the $n$-th partial derivative of the Green's function with respect to the vertical observation coordinate $z$, evaluated at the observation plane $z=0$. Formally,\n$$\nK_{n}(x-x', y-y'; z_{0}) = \\left. \\frac{\\partial^{n}}{\\partial z^{n}} G(x-x', y-y', z-z_{0}) \\right|_{z=0}\n$$\nThe problem specifies that the leading-order depth dependence is controlled by the on-axis behavior, where the receiver is directly above the source element. This corresponds to the case where $(x,y) = (x',y')$. We therefore need to find the expression for the on-axis kernel, $K_{n}(0,0;z_{0})$.\n\nFor the on-axis case, $x-x'=0$ and $y-y'=0$. The magnitude of the position vector simplifies to $|\\mathbf{r}| = \\sqrt{0^{2} + 0^{2} + (z-z_{0})^{2}} = |z-z_{0}|$. Since the observation plane is at $z=0$ and the source plane is at $z_{0}0$, we are concerned with points in space where $z \\le 0$ or at least $z  z_{0}$. In this regime, $z-z_{0}$ is always negative, so $|z-z_{0}| = -(z-z_{0}) = z_{0}-z$.\n\nThe on-axis Green's function is then:\n$$\nG_{\\text{on-axis}}(z; z_{0}) = \\frac{1}{4\\pi(z_{0}-z)} = \\frac{1}{4\\pi}(z_{0}-z)^{-1}\n$$\nNow, we compute the $n$-th partial derivative of this function with respect to $z$. We perform the first few differentiations to establish a pattern.\nFor $n=1$:\n$$\n\\frac{\\partial}{\\partial z} \\left( \\frac{1}{4\\pi}(z_{0}-z)^{-1} \\right) = \\frac{1}{4\\pi} (-1)(z_{0}-z)^{-2}(-1) = \\frac{1}{4\\pi}(z_{0}-z)^{-2}\n$$\nFor $n=2$:\n$$\n\\frac{\\partial^{2}}{\\partial z^{2}} \\left( \\frac{1}{4\\pi}(z_{0}-z)^{-1} \\right) = \\frac{\\partial}{\\partial z} \\left( \\frac{1}{4\\pi}(z_{0}-z)^{-2} \\right) = \\frac{1}{4\\pi} (-2)(z_{0}-z)^{-3}(-1) = \\frac{2}{4\\pi}(z_{0}-z)^{-3}\n$$\nFor $n=3$:\n$$\n\\frac{\\partial^{3}}{\\partial z^{3}} \\left( \\frac{1}{4\\pi}(z_{0}-z)^{-1} \\right) = \\frac{\\partial}{\\partial z} \\left( \\frac{2}{4\\pi}(z_{0}-z)^{-3} \\right) = \\frac{2}{4\\pi} (-3)(z_{0}-z)^{-4}(-1) = \\frac{6}{4\\pi}(z_{0}-z)^{-4}\n$$\nBy inspection and induction, the general formula for the $n$-th derivative is:\n$$\n\\frac{\\partial^{n}}{\\partial z^{n}} \\left( \\frac{1}{4\\pi}(z_{0}-z)^{-1} \\right) = \\frac{n!}{4\\pi}(z_{0}-z)^{-(n+1)}\n$$\nTo obtain the on-axis sensitivity kernel $K_{n}(0,0;z_{0})$, we evaluate this expression at the observation plane $z=0$:\n$$\nK_{n}(0,0;z_{0}) = \\left. \\frac{n!}{4\\pi}(z_{0}-z)^{-(n+1)} \\right|_{z=0} = \\frac{n!}{4\\pi}z_{0}^{-(n+1)}\n$$\nThis expression describes the decay of the on-axis sensitivity with source depth $z_{0}$ for the $n$-th vertical derivative of the potential.\n\nThe problem requires us to find a depth-weighting function $w(z_{0})$ such that the product $w(z_{0})K_{n}(0,0;z_{0})$ is independent of $z_{0}$ for large $z_{0}$. This means the product must be a constant. Let this constant be $C$.\n$$\nw(z_{0}) K_{n}(0,0;z_{0}) = C\n$$\nSubstituting the derived expression for $K_{n}(0,0;z_{0})$:\n$$\nw(z_{0}) \\left( \\frac{n!}{4\\pi}z_{0}^{-(n+1)} \\right) = C\n$$\nTo make the left-hand side independent of $z_{0}$, $w(z_{0})$ must be structured to cancel the $z_{0}^{-(n+1)}$ term. Therefore, $w(z_{0})$ must be proportional to the reciprocal of this term:\n$$\nw(z_{0}) \\propto \\frac{1}{z_{0}^{-(n+1)}} = z_{0}^{n+1}\n$$\nWe can express this relationship with a proportionality constant, $A$:\n$$\nw(z_{0}) = A \\, z_{0}^{n+1}\n$$\nThe problem provides a normalization condition to determine the constant $A$: $w(1)=1$. We apply this condition:\n$$\nw(1) = A \\cdot (1)^{n+1} = A\n$$\nThis implies that $A=1$.\n\nTherefore, the explicit, closed-form expression for the normalized depth-weighting function is:\n$$\nw(z_{0}) = z_{0}^{n+1}\n$$\nThis function, when multiplied by the on-axis sensitivity kernel $K_{n}(0,0;z_{0})$, yields a result $\\frac{n!}{4\\pi}$ which is indeed independent of the depth $z_{0}$.",
            "answer": "$$\\boxed{z_{0}^{n+1}}$$"
        },
        {
            "introduction": "With a well-conditioned forward problem, the next step is to solve the resulting linear system. For the large-scale systems typical in geophysical applications, iterative methods are computationally superior to direct inversion. This practice  explores the Least Squares QR (LSQR) algorithm, a robust method for solving Tikhonov-regularized problems, and introduces the Morozov Discrepancy Principle as a statistically-grounded criterion for determining when to stop the iterations, preventing the solution from fitting noise in the data.",
            "id": "3589290",
            "problem": "Consider the two-dimensional equivalent source method for modeling a potential field anomaly measured on a horizontal observation line at elevation $z = z_{0}$. Let the model consist of $n$ equivalent point sources distributed on a plane at depth $z = z_{s}$, with unknown scalar source strengths collected in the vector $\\boldsymbol{\\sigma} \\in \\mathbb{R}^{n}$. Discretization of the forward map based on the Newtonian kernel yields a linear system $\\mathbf{G} \\boldsymbol{\\sigma} \\approx \\mathbf{d}$, where $\\mathbf{G} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{d} \\in \\mathbb{R}^{m}$ is the observed anomaly. Because the system is ill-conditioned, we adopt Tikhonov regularization and seek $\\boldsymbol{\\sigma}$ by minimizing the objective \n$$\nJ(\\boldsymbol{\\sigma}) = \\frac{1}{2} \\| \\mathbf{G} \\boldsymbol{\\sigma} - \\mathbf{d} \\|_{2}^{2} + \\frac{1}{2} \\alpha^{2} \\| \\mathbf{L} \\boldsymbol{\\sigma} \\|_{2}^{2},\n$$\nwhere $\\alpha  0$ is a regularization parameter and $\\mathbf{L} \\in \\mathbb{R}^{p \\times n}$ is a discrete smoothing operator. The corresponding regularized normal equations are\n$$\n\\left( \\mathbf{G}^{\\mathsf{T}} \\mathbf{G} + \\alpha^{2} \\mathbf{L}^{\\mathsf{T}} \\mathbf{L} \\right) \\boldsymbol{\\sigma} = \\mathbf{G}^{\\mathsf{T}} \\mathbf{d}.\n$$\nIt is computationally advantageous to avoid forming the normal equations explicitly and instead solve the equivalent least-squares problem\n$$\n\\min_{\\boldsymbol{\\sigma}} \\left\\| \\begin{bmatrix} \\mathbf{G} \\\\ \\alpha \\mathbf{L} \\end{bmatrix} \\boldsymbol{\\sigma} - \\begin{bmatrix} \\mathbf{d} \\\\ \\mathbf{0} \\end{bmatrix} \\right\\|_{2},\n$$\nusing Least Squares QR (LSQR), which is based on Golub–Kahan bidiagonalization.\n\nTask A (derivation). Starting from the definitions of Golub–Kahan bidiagonalization applied to the augmented matrix \n$$\n\\mathbf{A}_{\\alpha} = \\begin{bmatrix} \\mathbf{G} \\\\ \\alpha \\mathbf{L} \\end{bmatrix}, \\quad \\mathbf{b}_{\\alpha} = \\begin{bmatrix} \\mathbf{d} \\\\ \\mathbf{0} \\end{bmatrix},\n$$\nderive the LSQR iteration that generates approximations $\\boldsymbol{\\sigma}_{k}$ to the solution. Your derivation must begin with the initializations $ \\beta_{1} = \\| \\mathbf{b}_{\\alpha} \\|_{2}$ and $ \\mathbf{u}_{1} = \\mathbf{b}_{\\alpha} / \\beta_{1}$, and proceed by constructing the sequences $\\{ \\alpha_{k} \\}$, $\\{ \\beta_{k} \\}$, $\\{ \\mathbf{u}_{k} \\}$, $\\{ \\mathbf{v}_{k} \\}$, and the scalar recurrences for $ \\rho_{k}$, $ \\bar{\\rho}_{k}$, $ \\phi_{k}$, $ \\bar{\\phi}_{k}$, $ c_{k}$, $ s_{k}$ that update $ \\boldsymbol{\\sigma}_{k}$ without explicitly forming $ \\mathbf{A}_{\\alpha}^{\\mathsf{T}} \\mathbf{A}_{\\alpha}$. Clearly show how the algorithm implicitly solves the regularized normal equations.\n\nTask B (stopping criterion tied to noise). Suppose the data $\\mathbf{d}$ are contaminated by additive, independent, zero-mean noise with known standard deviation $ \\sigma_{n} $, and the observation count is $ m $. According to the Morozov discrepancy principle (MDP), a practical stopping rule is to terminate at the smallest iteration $ k_{\\star} $ such that the data residual satisfies\n$$\n\\| \\mathbf{G} \\boldsymbol{\\sigma}_{k} - \\mathbf{d} \\|_{2} \\le \\tau \\sqrt{m} \\, \\sigma_{n},\n$$\nwith a safety factor $ \\tau \\ge 1 $. In a dimensionless, scaled setting, let $ m = 200 $, $ \\sigma_{n} = 1.0 $, and $ \\tau = 1.2 $. An LSQR run on the augmented system reported the following sequence of data residual norms $ r_{k} = \\| \\mathbf{G} \\boldsymbol{\\sigma}_{k} - \\mathbf{d} \\|_{2} $:\n$$\nr_{1} = 100, \\quad r_{2} = 60, \\quad r_{3} = 35, \\quad r_{4} = 25, \\quad r_{5} = 21, \\quad r_{6} = 19, \\quad r_{7} = 17, \\quad r_{8} = 16.\n$$\nCompute the earliest iteration index $ k_{\\star} $ at which the MDP stopping condition is satisfied. Report only the value of $ k_{\\star} $; no units are required, and no rounding is necessary beyond exact integer identification.",
            "solution": "The problem is evaluated as valid as it is scientifically grounded in computational geophysics and numerical linear algebra, well-posed, objective, and self-contained. The provided information is sufficient and consistent for both the derivation (Task A) and the numerical calculation (Task B).\n\nThe solution is partitioned into two parts as requested by the problem statement.\n\nTask A: Derivation of the LSQR Iteration for the Augmented System\n\nThe task is to derive the LSQR iteration for solving the regularized least-squares problem, which is formulated as minimizing $\\| \\mathbf{A}_{\\alpha} \\boldsymbol{\\sigma} - \\mathbf{b}_{\\alpha} \\|_{2}$ where\n$$ \\mathbf{A}_{\\alpha} = \\begin{bmatrix} \\mathbf{G} \\\\ \\alpha \\mathbf{L} \\end{bmatrix}, \\quad \\mathbf{b}_{\\alpha} = \\begin{bmatrix} \\mathbf{d} \\\\ \\mathbf{0} \\end{bmatrix}. $$\nLSQR is based on the Golub-Kahan bidiagonalization process applied to the matrix $\\mathbf{A}_{\\alpha}$ and the vector $\\mathbf{b}_{\\alpha}$. This process iteratively generates a sequence of orthonormal vectors $\\{ \\mathbf{u}_{k} \\}$ and $\\{ \\mathbf{v}_{k} \\}$ such that they form bases for Krylov subspaces, and two sequences of positive scalars $\\{ \\alpha_{k} \\}$ and $\\{ \\beta_{k} \\}$ that define a bidiagonal matrix.\n\nThe derivation begins with the initialization.\nLet $\\boldsymbol{\\sigma}_{0} = \\mathbf{0}$. The initial residual is $\\mathbf{b}_{\\alpha} - \\mathbf{A}_{\\alpha}\\boldsymbol{\\sigma}_0 = \\mathbf{b}_{\\alpha}$.\nThe first basis vector $\\mathbf{u}_{1}$ is aligned with the initial residual.\n$$ \\beta_{1} = \\| \\mathbf{b}_{\\alpha} \\|_{2} = \\left\\| \\begin{bmatrix} \\mathbf{d} \\\\ \\mathbf{0} \\end{bmatrix} \\right\\|_{2} = \\| \\mathbf{d} \\|_{2}. $$\n$$ \\mathbf{u}_{1} = \\frac{\\mathbf{b}_{\\alpha}}{\\beta_{1}} = \\frac{1}{\\beta_{1}} \\begin{bmatrix} \\mathbf{d} \\\\ \\mathbf{0} \\end{bmatrix}. $$\nWe partition $\\mathbf{u}_k$ corresponding to the structure of $\\mathbf{A}_\\alpha$ and $\\mathbf{b}_\\alpha$ as $\\mathbf{u}_{k} = \\begin{bmatrix} \\mathbf{u}_{k}^{d} \\\\ \\mathbf{u}_{k}^{L} \\end{bmatrix}$. Thus, for $k=1$, $\\mathbf{u}_{1}^{d} = \\mathbf{d} / \\beta_{1}$ and $\\mathbf{u}_{1}^{L} = \\mathbf{0}$.\n\nThe bidiagonalization process proceeds with the following Lanczos-type two-term recurrences, starting from $k=1$:\n1. Generate $\\mathbf{v}_{k}$ from $\\mathbf{u}_{k}$:\n$$ \\tilde{\\mathbf{v}}_{k} = \\mathbf{A}_{\\alpha}^{\\mathsf{T}} \\mathbf{u}_{k} - \\beta_{k} \\mathbf{v}_{k-1} \\quad (\\text{with } \\beta_1\\mathbf{v}_0 \\equiv \\mathbf{0}). $$\n$$ \\alpha_{k} = \\| \\tilde{\\mathbf{v}}_{k} \\|_{2}. $$\n$$ \\mathbf{v}_{k} = \\frac{\\tilde{\\mathbf{v}}_{k}}{\\alpha_{k}}. $$\n2. Generate $\\mathbf{u}_{k+1}$ from $\\mathbf{v}_{k}$:\n$$ \\tilde{\\mathbf{u}}_{k+1} = \\mathbf{A}_{\\alpha} \\mathbf{v}_{k} - \\alpha_{k} \\mathbf{u}_{k}. $$\n$$ \\beta_{k+1} = \\| \\tilde{\\mathbf{u}}_{k+1} \\|_{2}. $$\n$$ \\mathbf{u}_{k+1} = \\frac{\\tilde{\\mathbf{u}}_{k+1}}{\\beta_{k+1}}. $$\n\nLet's express these steps in terms of $\\mathbf{G}$, $\\mathbf{L}$, and $\\alpha$.\nFor $k=1$:\n$$ \\tilde{\\mathbf{v}}_{1} = \\mathbf{A}_{\\alpha}^{\\mathsf{T}} \\mathbf{u}_{1} = \\begin{bmatrix} \\mathbf{G}^{\\mathsf{T}}  \\alpha \\mathbf{L}^{\\mathsf{T}} \\end{bmatrix} \\begin{bmatrix} \\mathbf{u}_{1}^{d} \\\\ \\mathbf{u}_{1}^{L} \\end{bmatrix} = \\mathbf{G}^{\\mathsf{T}} \\mathbf{u}_{1}^{d} + \\alpha \\mathbf{L}^{\\mathsf{T}} \\mathbf{u}_{1}^{L} = \\frac{1}{\\beta_{1}} \\mathbf{G}^{\\mathsf{T}} \\mathbf{d}. $$\n$$ \\alpha_{1} = \\| \\tilde{\\mathbf{v}}_{1} \\|_{2} = \\frac{1}{\\beta_{1}} \\| \\mathbf{G}^{\\mathsf{T}} \\mathbf{d} \\|_{2}. $$\n$$ \\mathbf{v}_{1} = \\frac{\\tilde{\\mathbf{v}}_{1}}{\\alpha_{1}}. $$\n\nFor the general iteration step $k$:\nThe computation of $\\tilde{\\mathbf{u}}_{k+1}$ involves a matrix-vector product with $\\mathbf{A}_{\\alpha}$:\n$$ \\tilde{\\mathbf{u}}_{k+1} = \\begin{bmatrix} \\mathbf{G} \\\\ \\alpha \\mathbf{L} \\end{bmatrix} \\mathbf{v}_{k} - \\alpha_{k} \\begin{bmatrix} \\mathbf{u}_{k}^{d} \\\\ \\mathbf{u}_{k}^{L} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{G} \\mathbf{v}_{k} - \\alpha_{k} \\mathbf{u}_{k}^{d} \\\\ \\alpha \\mathbf{L} \\mathbf{v}_{k} - \\alpha_{k} \\mathbf{u}_{k}^{L} \\end{bmatrix}. $$\nThe computation of $\\tilde{\\mathbf{v}}_{k+1}$ involves a matrix-vector product with $\\mathbf{A}_{\\alpha}^{\\mathsf{T}}$:\n$$ \\tilde{\\mathbf{v}}_{k+1} = \\begin{bmatrix} \\mathbf{G}^{\\mathsf{T}}  \\alpha \\mathbf{L}^{\\mathsf{T}} \\end{bmatrix} \\begin{bmatrix} \\mathbf{u}_{k+1}^{d} \\\\ \\mathbf{u}_{k+1}^{L} \\end{bmatrix} - \\beta_{k+1} \\mathbf{v}_{k} = (\\mathbf{G}^{\\mathsf{T}} \\mathbf{u}_{k+1}^{d} + \\alpha \\mathbf{L}^{\\mathsf{T}} \\mathbf{u}_{k+1}^{L}) - \\beta_{k+1} \\mathbf{v}_{k}. $$\nThese recurrences build the sequences $\\{ \\alpha_{k} \\}$, $\\{ \\beta_{k} \\}$, $\\{ \\mathbf{u}_{k} \\}$, and $\\{ \\mathbf{v}_{k} \\}$.\n\nThe core of LSQR is to find the solution $\\boldsymbol{\\sigma}_{k}$ in the Krylov subspace $\\mathcal{K}_{k}(\\mathbf{A}_\\alpha^\\mathsf{T}\\mathbf{A}_\\alpha, \\mathbf{A}_\\alpha^\\mathsf{T}\\mathbf{b}_\\alpha) = \\mathrm{span}\\{\\mathbf{v}_{1}, \\dots, \\mathbf{v}_{k}\\}$ that minimizes the objective function. Let $\\boldsymbol{\\sigma}_{k} = \\mathbf{V}_{k} \\mathbf{y}_{k}$, where $\\mathbf{V}_{k} = [\\mathbf{v}_{1}, \\dots, \\mathbf{v}_{k}]$. The problem reduces to solving a smaller least-squares problem:\n$$ \\min_{\\mathbf{y}_{k}} \\| \\mathbf{B}_{k} \\mathbf{y}_{k} - \\beta_{1} \\mathbf{e}_{1} \\|_{2}, $$\nwhere $\\mathbf{B}_{k} \\in \\mathbb{R}^{(k+1) \\times k}$ is the lower bidiagonal matrix whose non-zero entries are $\\{ \\alpha_{i}, \\beta_{i+1} \\}_{i=1}^{k}$.\nLSQR solves this system progressively using a sequence of Givens rotations. At each iteration $k$, a new rotation $Q_{k}$ is constructed to eliminate the subdiagonal element $\\beta_{k+1}$. Let $c_{k} = \\cos(\\theta_{k})$ and $s_{k} = \\sin(\\theta_{k})$. The rotation is applied to the previous system.\n\nThe full LSQR algorithm for the augmented system is as follows:\n\n1.  **Initialization**:\n    $\\beta_{1} = \\| \\mathbf{d} \\|_{2}$, $\\mathbf{u}_{1}^{d} = \\mathbf{d} / \\beta_{1}$, $\\mathbf{u}_{1}^{L} = \\mathbf{0}$.\n    $\\tilde{\\mathbf{v}}_{1} = \\mathbf{G}^{\\mathsf{T}} \\mathbf{u}_{1}^{d}$.\n    $\\alpha_{1} = \\| \\tilde{\\mathbf{v}}_{1} \\|_{2}$, $\\mathbf{v}_{1} = \\tilde{\\mathbf{v}}_{1} / \\alpha_{1}$.\n    Initialize search direction $\\mathbf{p}_{1} = \\mathbf{v}_{1}$.\n    Initialize solution $\\boldsymbol{\\sigma}_{0} = \\mathbf{0}$.\n    Initialize scalars for the plane rotation: $\\bar{\\rho}_{1} = \\alpha_{1}$, $\\bar{\\phi}_{1} = \\beta_{1}$.\n\n2.  **Iteration**: For $k = 1, 2, 3, \\dots$\n    a. **Bidiagonalization Step 1**:\n       $\\tilde{\\mathbf{u}}_{k+1}^{d} = \\mathbf{G} \\mathbf{v}_{k} - \\alpha_{k} \\mathbf{u}_{k}^{d}$.\n       $\\tilde{\\mathbf{u}}_{k+1}^{L} = \\alpha \\mathbf{L} \\mathbf{v}_{k} - \\alpha_{k} \\mathbf{u}_{k}^{L}$.\n       $\\beta_{k+1} = \\sqrt{\\| \\tilde{\\mathbf{u}}_{k+1}^{d} \\|_{2}^{2} + \\| \\tilde{\\mathbf{u}}_{k+1}^{L} \\|_{2}^{2}}$.\n       $\\mathbf{u}_{k+1}^{d} = \\tilde{\\mathbf{u}}_{k+1}^{d} / \\beta_{k+1}$, $\\mathbf{u}_{k+1}^{L} = \\tilde{\\mathbf{u}}_{k+1}^{L} / \\beta_{k+1}$.\n\n    b. **Bidiagonalization Step 2**:\n       $\\tilde{\\mathbf{v}}_{k+1} = \\mathbf{G}^{\\mathsf{T}} \\mathbf{u}_{k+1}^{d} + \\alpha \\mathbf{L}^{\\mathsf{T}} \\mathbf{u}_{k+1}^{L} - \\beta_{k+1} \\mathbf{v}_{k}$.\n       $\\alpha_{k+1} = \\| \\tilde{\\mathbf{v}}_{k+1} \\|_{2}$.\n       $\\mathbf{v}_{k+1} = \\tilde{\\mathbf{v}}_{k+1} / \\alpha_{k+1}$.\n\n    c. **Givens Rotation**:\n       $\\rho_{k} = \\sqrt{\\bar{\\rho}_{k}^{2} + \\beta_{k+1}^{2}}$.\n       $c_{k} = \\bar{\\rho}_{k} / \\rho_{k}$.\n       $s_{k} = \\beta_{k+1} / \\rho_{k}$.\n\n    d. **Update scalar variables**:\n       $\\theta_{k+1} = s_{k} \\alpha_{k+1}$.\n       $\\bar{\\rho}_{k+1} = -c_{k} \\alpha_{k+1}$.\n       $\\phi_{k} = c_{k} \\bar{\\phi}_{k}$.\n       $\\bar{\\phi}_{k+1} = s_{k} \\bar{\\phi}_{k}$.\n\n    e. **Update solution and search direction**:\n       $\\boldsymbol{\\sigma}_{k} = \\boldsymbol{\\sigma}_{k-1} + (\\phi_{k} / \\rho_{k}) \\mathbf{p}_{k}$.\n       $\\mathbf{p}_{k+1} = \\mathbf{v}_{k+1} - (\\theta_{k+1} / \\rho_{k}) \\mathbf{p}_{k}$.\n\nThis completes the derivation of the iteration steps.\n\n**Implicit Solution of Regularized Normal Equations**:\nLSQR is an iterative method for solving the least-squares problem $\\min_{\\boldsymbol{\\sigma}} \\| \\mathbf{A}_{\\alpha} \\boldsymbol{\\sigma} - \\mathbf{b}_{\\alpha} \\|_{2}^{2}$. The solution to this problem is formally given by the normal equations:\n$$ \\mathbf{A}_{\\alpha}^{\\mathsf{T}} \\mathbf{A}_{\\alpha} \\boldsymbol{\\sigma} = \\mathbf{A}_{\\alpha}^{\\mathsf{T}} \\mathbf{b}_{\\alpha}. $$\nSubstituting the definitions of $\\mathbf{A}_{\\alpha}$ and $\\mathbf{b}_{\\alpha}$:\nThe left-hand side becomes:\n$$ \\mathbf{A}_{\\alpha}^{\\mathsf{T}} \\mathbf{A}_{\\alpha} = \\begin{bmatrix} \\mathbf{G}^{\\mathsf{T}}  \\alpha \\mathbf{L}^{\\mathsf{T}} \\end{bmatrix} \\begin{bmatrix} \\mathbf{G} \\\\ \\alpha \\mathbf{L} \\end{bmatrix} = \\mathbf{G}^{\\mathsf{T}} \\mathbf{G} + \\alpha^{2} \\mathbf{L}^{\\mathsf{T}} \\mathbf{L}. $$\nThe right-hand side becomes:\n$$ \\mathbf{A}_{\\alpha}^{\\mathsf{T}} \\mathbf{b}_{\\alpha} = \\begin{bmatrix} \\mathbf{G}^{\\mathsf{T}}  \\alpha \\mathbf{L}^{\\mathsf{T}} \\end{bmatrix} \\begin{bmatrix} \\mathbf{d} \\\\ \\mathbf{0} \\end{bmatrix} = \\mathbf{G}^{\\mathsf{T}} \\mathbf{d}. $$\nThus, the normal equations for the augmented system are:\n$$ \\left( \\mathbf{G}^{\\mathsf{T}} \\mathbf{G} + \\alpha^{2} \\mathbf{L}^{\\mathsf{T}} \\mathbf{L} \\right) \\boldsymbol{\\sigma} = \\mathbf{G}^{\\mathsf{T}} \\mathbf{d}. $$\nThese are precisely the Tikhonov regularized normal equations given in the problem statement. LSQR is mathematically equivalent to the conjugate gradient algorithm applied to these normal equations (a method known as CGNE), but it exhibits better numerical stability because it avoids the explicit formation of the product matrices $\\mathbf{G}^{\\mathsf{T}}\\mathbf{G}$ and $\\mathbf{L}^{\\mathsf{T}}\\mathbf{L}$, whose condition number is the square of that of the original matrices, potentially leading to a loss of numerical precision. Therefore, by solving the augmented least-squares problem, LSQR implicitly solves the Tikhonov regularized normal equations.\n\nTask B: Morozov Discrepancy Principle Stopping Criterion\n\nThe task is to find the smallest iteration index $k_{\\star}$ at which the Morozov Discrepancy Principle (MDP) stopping condition is met. The condition is given as:\n$$ \\| \\mathbf{G} \\boldsymbol{\\sigma}_{k} - \\mathbf{d} \\|_{2} \\le \\tau \\sqrt{m} \\, \\sigma_{n}. $$\nThe problem provides the following values:\n- Number of observations, $m = 200$.\n- Standard deviation of the noise, $\\sigma_{n} = 1.0$.\n- Safety factor, $\\tau = 1.2$.\n\nFirst, we compute the stopping threshold on the right-hand side of the inequality:\n$$ \\text{Threshold} = \\tau \\sqrt{m} \\, \\sigma_{n} = 1.2 \\times \\sqrt{200} \\times 1.0 = 1.2 \\times \\sqrt{100 \\times 2} = 1.2 \\times 10 \\sqrt{2} = 12 \\sqrt{2}. $$\nTo compare this value with the given residual norms, we can either approximate it or use squares for an exact comparison. The squared threshold is:\n$$ (12 \\sqrt{2})^{2} = 12^{2} \\times (\\sqrt{2})^{2} = 144 \\times 2 = 288. $$\nNow, we compare the given sequence of data residual norms, $r_{k} = \\| \\mathbf{G} \\boldsymbol{\\sigma}_{k} - \\mathbf{d} \\|_{2}$, with this threshold. We seek the smallest $k$ such that $r_{k} \\le 12 \\sqrt{2}$, or equivalently, $r_{k}^{2} \\le 288$.\n\nThe provided sequence of residual norms is:\n- $r_{1} = 100$: $r_{1}^{2} = 10000  288$. Condition not met.\n- $r_{2} = 60$: $r_{2}^{2} = 3600  288$. Condition not met.\n- $r_{3} = 35$: $r_{3}^{2} = 1225  288$. Condition not met.\n- $r_{4} = 25$: $r_{4}^{2} = 625  288$. Condition not met.\n- $r_{5} = 21$: $r_{5}^{2} = 441  288$. Condition not met.\n- $r_{6} = 19$: $r_{6}^{2} = 361  288$. Condition not met.\n- $r_{7} = 17$: $r_{7}^{2} = 289  288$. Condition not met.\n- $r_{8} = 16$: $r_{8}^{2} = 256  288$. Condition is met.\n\nThe first iteration at which the MDP condition is satisfied is $k = 8$. Therefore, the earliest iteration index is $k_{\\star} = 8$.",
            "answer": "$$\n\\boxed{8}\n$$"
        }
    ]
}