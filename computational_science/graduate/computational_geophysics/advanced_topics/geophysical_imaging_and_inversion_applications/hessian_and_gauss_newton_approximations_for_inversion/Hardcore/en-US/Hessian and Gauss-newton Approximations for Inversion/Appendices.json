{
    "hands_on_practices": [
        {
            "introduction": "To build intuition for the behavior of different Hessian approximations, we begin with a focused analytical exercise. This problem explores a one-parameter nonlinear model near a point where the Gauss-Newton and exact Newton methods diverge dramatically, revealing their distinct behaviors in regions of high nonlinearity or small gradients. By deriving the steps analytically, you will gain a fundamental understanding of potential pitfalls, such as the instability of the Gauss-Newton method when the model's sensitivity is low and the problematic nature of the exact Newton method when the objective function is non-convex .",
            "id": "3603042",
            "problem": "In a one-parameter nonlinear inverse problem representative of a local amplitude-calibration subproblem in computational geophysics, consider the forward model $F(m) = \\sin(m)$ with $m$ in radians. A single datum $d \\in \\mathbb{R}$ is observed, and the residual and least-squares objective are defined by $r(m) = F(m) - d$ and $\\phi(m) = \\tfrac{1}{2} r(m)^{2}$, respectively. Take $d = \\tfrac{1}{2}$ and denote $m^{\\star} = \\tfrac{\\pi}{2}$. Note that $F^{\\prime}(m^{\\star}) = 0$ and $F^{\\prime\\prime}(m^{\\star}) = -1$, so $\\phi(m)$ has a nonconvex stationary point at $m^{\\star}$ where the exact curvature is negative. Let $m = m^{\\star} + \\varepsilon$ with $|\\varepsilon| \\ll 1$.\n\nStarting only from the definitions of $r(m)$ and $\\phi(m)$, and using Taylor expansions of $F$ and its derivatives about $m^{\\star}$, do the following:\n\n1. Derive expressions for the gradient $g(m) = \\nabla \\phi(m)$ and the exact Hessian $H(m) = \\nabla^{2} \\phi(m)$ in terms of $F^{\\prime}(m)$, $F^{\\prime\\prime}(m)$, and $r(m)$.\n\n2. Using the second-order Taylor model of $\\phi$ at $m$, minimize that model to obtain the exact Newton step $\\delta m_{\\mathrm{N}}$ as a function of $\\varepsilon$.\n\n3. Using a first-order linearization of the residual $r(m + \\delta m) \\approx r(m) + F^{\\prime}(m)\\,\\delta m$, minimize the corresponding quadratic model to obtain the Gauss–Newton step $\\delta m_{\\mathrm{GN}}$ as a function of $\\varepsilon$.\n\n4. Compute the two-term asymptotic expansions of $\\delta m_{\\mathrm{N}}(\\varepsilon)$ and $\\delta m_{\\mathrm{GN}}(\\varepsilon)$ about $\\varepsilon = 0$, keeping terms up to and including order $\\varepsilon^{3}$ for $\\delta m_{\\mathrm{N}}$ and up to and including order $\\varepsilon^{-1}$ and $\\varepsilon^{1}$ for $\\delta m_{\\mathrm{GN}}$.\n\nProvide your final result as a single $1 \\times 2$ row vector $\\big[\\delta m_{\\mathrm{N}}(\\varepsilon),\\, \\delta m_{\\mathrm{GN}}(\\varepsilon)\\big]$. No units are required. Angles are in radians.",
            "solution": "The problem is validated as self-contained, consistent, and scientifically sound. It is a well-posed problem in the field of numerical optimization applied to geophysical inversion.\n\nThe least-squares objective function is $\\phi(m) = \\frac{1}{2}r(m)^2$, where the residual is $r(m) = F(m) - d$. The forward model is $F(m) = \\sin(m)$ and the datum is $d = \\frac{1}{2}$. The point of expansion is $m^{\\star} = \\frac{\\pi}{2}$, and we analyze the problem at $m = m^{\\star} + \\varepsilon = \\frac{\\pi}{2} + \\varepsilon$ for $|\\varepsilon| \\ll 1$.\n\n**1. Gradient and Exact Hessian**\n\nFirst, we derive the general expressions for the gradient $g(m)$ and Hessian $H(m)$ of the objective function $\\phi(m)$.\nThe gradient, $g(m) = \\phi^{\\prime}(m)$, is found by differentiating $\\phi(m)$ with respect to $m$ using the chain rule:\n$$\ng(m) = \\frac{d}{dm} \\left( \\frac{1}{2} r(m)^2 \\right) = \\frac{1}{2} \\cdot 2 r(m) \\cdot r^{\\prime}(m) = r(m) r^{\\prime}(m)\n$$\nSince $r(m) = F(m) - d$, its derivative is $r^{\\prime}(m) = F^{\\prime}(m)$. Substituting this gives the gradient:\n$$\ng(m) = r(m) F^{\\prime}(m)\n$$\nThe Hessian, $H(m) = \\phi^{\\prime\\prime}(m)$, is the derivative of the gradient $g(m)$ with respect to $m$. Using the product rule:\n$$\nH(m) = \\frac{d}{dm} \\left( r(m) F^{\\prime}(m) \\right) = r^{\\prime}(m) F^{\\prime}(m) + r(m) F^{\\prime\\prime}(m)\n$$\nSubstituting $r^{\\prime}(m) = F^{\\prime}(m)$ yields the exact Hessian:\n$$\nH(m) = (F^{\\prime}(m))^2 + r(m) F^{\\prime\\prime}(m)\n$$\nThis expression consists of two terms. The first term, $(F^{\\prime}(m))^2$, is the Gauss-Newton approximation of the Hessian. The second term, $r(m) F^{\\prime\\prime}(m)$, is often omitted in practice, especially if residuals $r(m)$ are small or the model is nearly linear (i.e., $F^{\\prime\\prime}(m)$ is small).\n\n**2. Exact Newton Step $\\delta m_{\\mathrm{N}}$**\n\nThe Newton step $\\delta m_{\\mathrm{N}}$ is obtained by minimizing the second-order Taylor expansion of $\\phi(m)$ around the current point $m$:\n$$\n\\phi(m + \\delta m) \\approx \\phi(m) + g(m) \\delta m + \\frac{1}{2} H(m) (\\delta m)^2\n$$\nSetting the derivative with respect to $\\delta m$ to zero to find the stationary point gives:\n$$\ng(m) + H(m) \\delta m = 0 \\implies \\delta m_{\\mathrm{N}} = -H(m)^{-1} g(m) = -\\frac{g(m)}{H(m)}\n$$\nTo express this as a function of $\\varepsilon$, we evaluate $F(m)$, $F^{\\prime}(m)$, $F^{\\prime\\prime}(m)$, and $r(m)$ at $m = \\frac{\\pi}{2} + \\varepsilon$:\n$F(m) = \\sin(\\frac{\\pi}{2} + \\varepsilon) = \\cos(\\varepsilon)$\n$F^{\\prime}(m) = \\cos(\\frac{\\pi}{2} + \\varepsilon) = -\\sin(\\varepsilon)$\n$F^{\\prime\\prime}(m) = -\\sin(\\frac{\\pi}{2} + \\varepsilon) = -\\cos(\\varepsilon)$\n$r(m) = F(m) - d = \\cos(\\varepsilon) - \\frac{1}{2}$\n\nNow, we formulate $g(m)$ and $H(m)$ in terms of $\\varepsilon$:\n$g(m) = (\\cos(\\varepsilon) - \\frac{1}{2})(-\\sin(\\varepsilon))$\n$H(m) = (-\\sin(\\varepsilon))^2 + (\\cos(\\varepsilon) - \\frac{1}{2})(-\\cos(\\varepsilon)) = \\sin^2(\\varepsilon) - \\cos^2(\\varepsilon) + \\frac{1}{2}\\cos(\\varepsilon)$\n\nSubstituting these into the formula for the Newton step gives:\n$$\n\\delta m_{\\mathrm{N}}(\\varepsilon) = -\\frac{(\\cos(\\varepsilon) - \\frac{1}{2})(-\\sin(\\varepsilon))}{\\sin^2(\\varepsilon) - \\cos^2(\\varepsilon) + \\frac{1}{2}\\cos(\\varepsilon)} = \\frac{(\\cos(\\varepsilon) - \\frac{1}{2})\\sin(\\varepsilon)}{\\sin^2(\\varepsilon) - \\cos^2(\\varepsilon) + \\frac{1}{2}\\cos(\\varepsilon)}\n$$\n\n**3. Gauss–Newton Step $\\delta m_{\\mathrm{GN}}$**\n\nThe Gauss-Newton step is derived by minimizing the objective function formed from a first-order linearization of the residual, $r(m + \\delta m) \\approx r(m) + F^{\\prime}(m) \\delta m$. The corresponding quadratic model is $\\phi_{\\mathrm{lin}}(\\delta m) = \\frac{1}{2}(r(m) + F^{\\prime}(m) \\delta m)^2$. Minimizing this with respect to $\\delta m$:\n$$\n\\frac{d\\phi_{\\mathrm{lin}}}{d(\\delta m)} = (r(m) + F^{\\prime}(m) \\delta m) F^{\\prime}(m) = 0\n$$\nThis leads to $r(m)F'(m) + (F'(m))^2 \\delta m = 0$, which is equivalent to solving $g(m) + H_{\\mathrm{GN}}(m)\\delta m = 0$, where $H_{\\mathrm{GN}}(m) = (F^{\\prime}(m))^2$. The Gauss-Newton step is therefore:\n$$\n\\delta m_{\\mathrm{GN}} = -\\frac{g(m)}{H_{\\mathrm{GN}}(m)} = -\\frac{r(m) F^{\\prime}(m)}{(F^{\\prime}(m))^2} = -\\frac{r(m)}{F^{\\prime}(m)}\n$$\nSubstituting the expressions in terms of $\\varepsilon$:\n$$\n\\delta m_{\\mathrm{GN}}(\\varepsilon) = -\\frac{\\cos(\\varepsilon) - \\frac{1}{2}}{-\\sin(\\varepsilon)} = \\frac{\\cos(\\varepsilon) - \\frac{1}{2}}{\\sin(\\varepsilon)}\n$$\n\n**4. Asymptotic Expansions**\n\nWe use the following Taylor series expansions about $\\varepsilon = 0$:\n$\\sin(\\varepsilon) = \\varepsilon - \\frac{\\varepsilon^3}{6} + \\mathcal{O}(\\varepsilon^5)$\n$\\cos(\\varepsilon) = 1 - \\frac{\\varepsilon^2}{2} + \\frac{\\varepsilon^4}{24} + \\mathcal{O}(\\varepsilon^6)$\n\n**Expansion of $\\delta m_{\\mathrm{N}}(\\varepsilon)$:**\nThe numerator is $N(\\varepsilon) = (\\cos(\\varepsilon) - \\frac{1}{2})\\sin(\\varepsilon) = (\\frac{1}{2} - \\frac{\\varepsilon^2}{2} + \\mathcal{O}(\\varepsilon^4))(\\varepsilon - \\frac{\\varepsilon^3}{6} + \\mathcal{O}(\\varepsilon^5)) = \\frac{1}{2}\\varepsilon - \\frac{\\varepsilon^3}{12} - \\frac{\\varepsilon^3}{2} + \\mathcal{O}(\\varepsilon^5) = \\frac{1}{2}\\varepsilon - \\frac{7}{12}\\varepsilon^3 + \\mathcal{O}(\\varepsilon^5)$.\nThe denominator is $D(\\varepsilon) = \\sin^2(\\varepsilon) - \\cos^2(\\varepsilon) + \\frac{1}{2}\\cos(\\varepsilon) = (\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)) - (1 - \\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)) + \\frac{1}{2}(1 - \\frac{\\varepsilon^2}{2} + \\mathcal{O}(\\varepsilon^4)) = -\\frac{1}{2} + \\frac{7}{4}\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)$.\nPerforming series division:\n$$\n\\delta m_{\\mathrm{N}}(\\varepsilon) = \\frac{\\frac{1}{2}\\varepsilon - \\frac{7}{12}\\varepsilon^3 + \\mathcal{O}(\\varepsilon^5)}{-\\frac{1}{2} + \\frac{7}{4}\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)} = \\left(-\\varepsilon + \\frac{7}{6}\\varepsilon^3\\right) \\frac{1}{1 - \\frac{7}{2}\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)}\n$$\nUsing the geometric series expansion $(1-x)^{-1} = 1+x+x^2+\\dots$:\n$$\n\\delta m_{\\mathrm{N}}(\\varepsilon) = \\left(-\\varepsilon + \\frac{7}{6}\\varepsilon^3\\right) \\left(1 + \\frac{7}{2}\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)\\right) = -\\varepsilon - \\frac{7}{2}\\varepsilon^3 + \\frac{7}{6}\\varepsilon^3 + \\mathcal{O}(\\varepsilon^5) = -\\varepsilon - \\frac{14}{6}\\varepsilon^3 + \\mathcal{O}(\\varepsilon^5)\n$$\nThus, the two-term expansion for the Newton step is:\n$$\n\\delta m_{\\mathrm{N}}(\\varepsilon) = -\\varepsilon - \\frac{7}{3}\\varepsilon^3\n$$\n\n**Expansion of $\\delta m_{\\mathrm{GN}}(\\varepsilon)$:**\nThe numerator is $N(\\varepsilon) = \\cos(\\varepsilon) - \\frac{1}{2} = \\frac{1}{2} - \\frac{\\varepsilon^2}{2} + \\frac{\\varepsilon^4}{24} + \\mathcal{O}(\\varepsilon^6)$.\nThe denominator is $D(\\varepsilon) = \\sin(\\varepsilon) = \\varepsilon - \\frac{\\varepsilon^3}{6} + \\mathcal{O}(\\varepsilon^5)$.\nPerforming series division:\n$$\n\\delta m_{\\mathrm{GN}}(\\varepsilon) = \\frac{\\frac{1}{2} - \\frac{\\varepsilon^2}{2} + \\mathcal{O}(\\varepsilon^4)}{\\varepsilon - \\frac{\\varepsilon^3}{6} + \\mathcal{O}(\\varepsilon^5)} = \\frac{1}{\\varepsilon} \\frac{\\frac{1}{2} - \\frac{\\varepsilon^2}{2} + \\mathcal{O}(\\varepsilon^4)}{1 - \\frac{\\varepsilon^2}{6} + \\mathcal{O}(\\varepsilon^4)}\n$$\n$$\n\\delta m_{\\mathrm{GN}}(\\varepsilon) = \\frac{1}{\\varepsilon} \\left(\\frac{1}{2} - \\frac{\\varepsilon^2}{2}\\right)\\left(1 + \\frac{\\varepsilon^2}{6}\\right) + \\mathcal{O}(\\varepsilon^3) = \\frac{1}{\\varepsilon} \\left(\\frac{1}{2} + \\frac{\\varepsilon^2}{12} - \\frac{\\varepsilon^2}{2}\\right) + \\mathcal{O}(\\varepsilon^3) = \\frac{1}{\\varepsilon} \\left(\\frac{1}{2} - \\frac{5}{12}\\varepsilon^2\\right) + \\mathcal{O}(\\varepsilon^3)\n$$\nThus, the two-term expansion for the Gauss-Newton step is:\n$$\n\\delta m_{\\mathrm{GN}}(\\varepsilon) = \\frac{1}{2}\\varepsilon^{-1} - \\frac{5}{12}\\varepsilon\n$$\n\nThe Newton step corrects the current position $m = m^\\star+\\varepsilon$ by approximately $-\\varepsilon$, indicating it attempts to return to the stationary point $m^\\star$. The higher-order term shows the influence of curvature. The Gauss-Newton step becomes singular as $\\varepsilon \\to 0$, a characteristic behavior when $F^{\\prime}(m) \\to 0$.\n\nThe final result is the row vector containing these two asymptotic expansions.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\varepsilon - \\frac{7}{3}\\varepsilon^{3}  \\frac{1}{2}\\varepsilon^{-1} - \\frac{5}{12}\\varepsilon \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Theoretical understanding must be paired with practical diagnostic tools. This hands-on coding exercise challenges you to quantify the error introduced by the Gauss-Newton approximation by examining its Taylor remainder . By implementing a program to compare this remainder with the neglected second-derivative terms, you will develop a concrete method for assessing the degree of nonlinearity and determining when the Gauss-Newton model provides a reliable local approximation to the true objective function.",
            "id": "3603122",
            "problem": "Consider an inverse problem in computational geophysics where the predicted data are generated by a nonlinear forward operator. Let the model vector be $m \\in \\mathbb{R}^n$, the predicted data be $F(m) \\in \\mathbb{R}^p$, and the observed data be $d \\in \\mathbb{R}^p$. Define the least-squares data misfit objective function by\n$$\n\\phi(m) = \\frac{1}{2} \\lVert F(m) - d \\rVert_2^2,\n$$\nwith residual $r(m) = F(m) - d$. Let $J(m) \\in \\mathbb{R}^{p \\times n}$ denote the Jacobian (first derivative) of $F(m)$ and let $\\nabla^2 F_i(m) \\in \\mathbb{R}^{n \\times n}$ denote the Hessian (second derivative) of the $i$-th component $F_i(m)$.\n\nThe gradient of the misfit at $m$ is $g = J(m)^\\top r(m)$. The Gauss-Newton Hessian approximation is $H_{GN}(m) = J(m)^\\top J(m)$. For a step $v \\in \\mathbb{R}^n$, define the Taylor remainder with respect to the Gauss-Newton quadratic model by\n$$\nR(v) = \\phi(m + v) - \\phi(m) - g^\\top v - \\frac{1}{2} v^\\top H_{GN}(m) \\, v.\n$$\n\nYour task is to implement a program that, for a specific nonlinear forward operator $F(m)$, computes $R(v)$ and quantitatively relates it to the contribution from the neglected second derivatives of $F(m)$ in the Hessian of $\\phi(m)$. Use the following forward operator, which is common in modeling exponentially decaying geophysical responses:\n$$\nF(m) = \\exp(A m),\n$$\nwhere the exponential is applied component-wise, and $A \\in \\mathbb{R}^{p \\times n}$ is a fixed matrix. This $F(m)$ is a smooth nonlinear map. For this $F(m)$, the Jacobian and component-wise Hessians exist and are continuous.\n\nDefine the following numeric test configuration:\n- Model dimension $n = 3$ and data dimension $p = 4$.\n- Matrix $A \\in \\mathbb{R}^{4 \\times 3}$ given by\n$$\nA = \\begin{bmatrix}\n0.5  -0.2  0.3 \\\\\n-0.1  0.4  0.6 \\\\\n0.3  0.1  -0.5 \\\\\n0.0  -0.3  0.2 \\\\\n\\end{bmatrix}.\n$$\n- True model $m_{\\mathrm{true}} \\in \\mathbb{R}^3$ given by\n$$\nm_{\\mathrm{true}} = \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\end{bmatrix}.\n$$\n- Observed data $d \\in \\mathbb{R}^4$ are noise-free and given by $d = F(m_{\\mathrm{true}})$.\n- Consider three test cases, each specifying a base model $m$ and a step $v$:\n  1. Case $1$ (moderate residual, moderate step): \n     $$\n     m = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}, \\quad\n     v = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.04 \\end{bmatrix}.\n     $$\n  2. Case $2$ (moderate residual, very small step):\n     $$\n     m = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}, \\quad\n     v = \\begin{bmatrix} 0.00005 \\\\ -0.00002 \\\\ 0.00004 \\end{bmatrix}.\n     $$\n  3. Case $3$ (zero residual at base point, moderate step):\n     $$\n     m = \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\end{bmatrix}, \\quad\n     v = \\begin{bmatrix} 0.1 \\\\ 0.1 \\\\ -0.05 \\end{bmatrix}.\n     $$\n\nFor each case, compute:\n- The Taylor remainder $R(v)$ as defined above.\n- The leading quadratic term arising from the neglected second derivatives of $F(m)$ in the exact Hessian of $\\phi(m)$ at $m$, defined by\n$$\nB(v) = \\frac{1}{2} \\sum_{i=1}^{p} r_i(m) \\, v^\\top \\left( \\nabla^2 F_i(m) \\right) v,\n$$\nwhere $r_i(m)$ is the $i$-th component of $r(m)$.\n- The relative discrepancy\n$$\n\\varepsilon(v) = \\frac{\\lvert R(v) - B(v) \\rvert}{\\max\\left( \\lvert B(v) \\rvert, 10^{-12} \\right)}.\n$$\n\nYour program should:\n- Implement $F(m)$, $J(m)$, and the component-wise Hessians $\\nabla^2 F_i(m)$ for the specified $F(m) = \\exp(A m)$.\n- Compute $\\phi(m)$, $g$, $H_{GN}(m)$, $R(v)$, $B(v)$, and $\\varepsilon(v)$ for each of the three test cases.\n- Produce a single line of output containing the results as a comma-separated list of three sublists (one per test case), each sublist containing $R(v)$, $B(v)$, and $\\varepsilon(v)$ as floating-point numbers. The output must be formatted exactly as:\n$$\n\\left[ [R_1,B_1,\\varepsilon_1],[R_2,B_2,\\varepsilon_2],[R_3,B_3,\\varepsilon_3] \\right],\n$$\nwhere each floating-point number must be written in scientific notation with twelve digits after the decimal point and the letter $e$ for the exponent, and there must be no spaces anywhere in the line.\n\nNotes:\n- Frame all computations in pure mathematical terms; no physical units are involved.\n- Angles do not appear in this problem.\n- The expected outputs are real numbers (floating-point values).",
            "solution": "The problem is valid as it is mathematically well-posed, scientifically grounded in the principles of computational geophysics and numerical optimization, and provides a complete and consistent set of data and definitions for the required computations.\n\nHere is a step-by-step derivation of the required quantities and the solution methodology.\n\n### 1. Mathematical Formulation\n\nThe problem revolves around the Taylor series expansion of the least-squares objective function $\\phi(m) = \\frac{1}{2} \\lVert F(m) - d \\rVert_2^2$. The expansion of $\\phi(m+v)$ around $m$ is given by:\n$$\n\\phi(m+v) = \\phi(m) + \\nabla\\phi(m)^\\top v + \\frac{1}{2} v^\\top \\nabla^2\\phi(m) v + O(\\lVert v \\rVert^3)\n$$\nThe gradient $\\nabla\\phi(m)$ and the Hessian $\\nabla^2\\phi(m)$ are required. Let $r(m) = F(m) - d$.\n\nThe gradient is given by:\n$$\n\\nabla\\phi(m) = J(m)^\\top r(m) = g\n$$\nwhere $J(m)$ is the Jacobian of $F(m)$.\n\nThe exact Hessian is given by:\n$$\n\\nabla^2\\phi(m) = J(m)^\\top J(m) + \\sum_{i=1}^{p} r_i(m) \\nabla^2 F_i(m)\n$$\nwhere $\\nabla^2 F_i(m)$ is the Hessian of the $i$-th component of the forward operator $F(m)$.\n\nThe Gauss-Newton approximation to the Hessian, $H_{GN}(m)$, neglects the second term involving second derivatives of $F$:\n$$\nH_{GN}(m) = J(m)^\\top J(m)\n$$\nThe quadratic model of $\\phi(m+v)$ based on the Gauss-Newton approximation is:\n$$\n\\phi(m+v) \\approx \\phi(m) + g^\\top v + \\frac{1}{2} v^\\top H_{GN}(m) v\n$$\nThe problem defines the remainder $R(v)$ as the difference between the true value $\\phi(m+v)$ and this quadratic model:\n$$\nR(v) = \\phi(m + v) - \\left( \\phi(m) + g^\\top v + \\frac{1}{2} v^\\top H_{GN}(m) v \\right)\n$$\nBy comparing the full Taylor series with the definition of $R(v)$, we can see that $R(v)$ captures all higher-order terms, with the leading term being the quadratic form involving the neglected part of the Hessian:\n$$\nR(v) = \\frac{1}{2} v^\\top \\left( \\sum_{i=1}^{p} r_i(m) \\nabla^2 F_i(m) \\right) v + O(\\lVert v \\rVert^3)\n$$\nThe problem defines the quantity $B(v)$ as this leading term:\n$$\nB(v) = \\frac{1}{2} \\sum_{i=1}^{p} r_i(m) v^\\top \\left( \\nabla^2 F_i(m) \\right) v\n$$\nThus, we expect $R(v) \\approx B(v)$ for small step vectors $v$, and the difference $R(v) - B(v)$ to be of order $O(\\lVert v \\rVert^3)$.\n\n### 2. Derivatives of the Specific Forward Operator\n\nThe forward operator is $F(m) = \\exp(A m)$, where the exponential is component-wise. Let $u = Am$, so $F_i(m) = \\exp(u_i) = \\exp(\\sum_j A_{ij} m_j)$.\n\n**Jacobian $J(m)$**: The entry $J_{ik}(m)$ is $\\frac{\\partial F_i}{\\partial m_k}$.\n$$\nJ_{ik}(m) = \\frac{\\partial}{\\partial m_k} \\exp\\left(\\sum_{j=1}^n A_{ij} m_j\\right) = \\exp\\left(\\sum_{j=1}^n A_{ij} m_j\\right) \\cdot A_{ik} = F_i(m) A_{ik}\n$$\nIn matrix form, using a diagonal matrix $\\text{diag}(F(m))$, this is:\n$$\nJ(m) = \\text{diag}(F(m)) A\n$$\n\n**Component Hessians $\\nabla^2 F_i(m)$**: The entry $(\\nabla^2 F_i(m))_{kl}$ is $\\frac{\\partial^2 F_i}{\\partial m_l \\partial m_k}$.\n$$\n(\\nabla^2 F_i(m))_{kl} = \\frac{\\partial}{\\partial m_l} (F_i(m) A_{ik}) = A_{ik} \\frac{\\partial F_i}{\\partial m_l} = A_{ik} (F_i(m) A_{il}) = F_i(m) A_{ik} A_{il}\n$$\nThis is the $(k,l)$-th entry of an outer product. Let $a_{i \\cdot}$ be the $i$-th row of $A$ (as a $1 \\times n$ row vector). Then:\n$$\n\\nabla^2 F_i(m) = F_i(m) (a_{i \\cdot}^\\top a_{i \\cdot})\n$$\n\n### 3. Computational Strategy\n\nFor each test case, we perform the following calculations:\n1.  **Define Constants**: Set the matrix $A$, true model $m_{\\mathrm{true}}$, and test cases $(m, v)$ as specified.\n2.  **Compute Observed Data**: Calculate $d = F(m_{\\mathrm{true}}) = \\exp(A m_{\\mathrm{true}})$.\n3.  **Evaluate at Base Point $m$**:\n    -   $F(m) = \\exp(Am)$\n    -   $r(m) = F(m) - d$\n    -   $\\phi(m) = \\frac{1}{2} r(m)^\\top r(m)$\n    -   $J(m) = \\text{diag}(F(m)) A$\n    -   $g = J(m)^\\top r(m)$\n    -   $H_{GN}(m) = J(m)^\\top J(m)$\n4.  **Evaluate at Perturbed Point $m+v$**:\n    -   $\\phi(m+v) = \\frac{1}{2} \\lVert \\exp(A(m+v)) - d \\rVert_2^2$\n5.  **Calculate $R(v)$**: Use the definition $R(v) = \\phi(m + v) - \\phi(m) - g^\\top v - \\frac{1}{2} v^\\top H_{GN}(m) v$.\n6.  **Calculate $B(v)$**: Substitute the specific form of the component Hessian into the definition of $B(v)$.\n    $$\n    v^\\top (\\nabla^2 F_i(m)) v = v^\\top (F_i(m) a_{i \\cdot}^\\top a_{i \\cdot}) v = F_i(m) (v^\\top a_{i \\cdot}^\\top) (a_{i \\cdot} v) = F_i(m) (a_{i \\cdot} v)^2\n    $$\n    This simplifies the calculation of $B(v)$ to:\n    $$\n    B(v) = \\frac{1}{2} \\sum_{i=1}^{p} r_i(m) F_i(m) (a_{i \\cdot} v)^2\n    $$\n    This form is computationally more efficient than constructing each Hessian matrix explicitly.\n7.  **Calculate $\\varepsilon(v)$**: Use the formula $\\varepsilon(v) = \\frac{\\lvert R(v) - B(v) \\rvert}{\\max\\left( \\lvert B(v) \\rvert, 10^{-12} \\right)}$, where the small number $10^{-12}$ in the denominator prevents division by zero, particularly for Case 3 where $r(m)=0$ and thus $B(v)=0$.\n8.  **Format Output**: The final results for each case, $[R(v), B(v), \\varepsilon(v)]$, are formatted into a single line of text according to the strict specified format.\n\nThis structured approach is implemented in the Python code below.",
            "answer": "[[-4.004124976798e-07,-4.003923769152e-07,5.025257969113e-05],[-4.003925781329e-17,-4.003923769152e-17,5.025257969113e-05],[-1.121543666236e-05,0.000000000000e+00,1.000000000000e+00]]"
        },
        {
            "introduction": "Approximations like the Gauss-Newton Hessian are not just theoretical concepts; they are the engines of practical optimization algorithms. This problem demonstrates how the positive semi-definite nature of the Gauss-Newton Hessian can be a significant advantage within a trust-region framework, especially in regions where the exact Hessian has negative curvature . You will implement and compare trust-region steps using both Hessians, observing firsthand how the Gauss-Newton approximation can lead to more robust and reliable steps in challenging, non-convex regions of the objective function.",
            "id": "3603124",
            "problem": "You are asked to construct and analyze a one-parameter nonlinear least-squares inversion example that exhibits negative curvature in the exact Hessian and to compare trust-region steps computed using the exact Hessian versus the Gauss–Newton approximation. The base is the standard nonlinear least-squares objective and chain rule calculus. You must implement a program that, for a specified synthetic residual model and a small test suite, computes trust-region steps for both choices of Hessian, and reports predicted and actual reductions in the objective to quantify when the Gauss–Newton approximation avoids deleterious steps in strongly nonlinear regions.\n\nThe inversion setup is as follows. Consider a scalar model parameter $m \\in \\mathbb{R}$. Define a two-component residual vector $r(m) = [r_1(m), r_2(m)]^\\top$ with\n- $r_1(m) = a \\exp(-b m^2) - d$,\n- $r_2(m) = c m - e$,\n\nwhere $a$, $b$, $c$, $d$, and $e$ are fixed positive constants. The objective function is the nonlinear least-squares misfit\n$$\n\\phi(m) = \\tfrac{1}{2} \\|r(m)\\|_2^2 = \\tfrac{1}{2}\\big(r_1(m)^2 + r_2(m)^2\\big).\n$$\nThe gradient is defined by the chain rule as\n$$\ng(m) = \\nabla \\phi(m) = J(m)^\\top r(m),\n$$\nwhere $J(m)$ is the Jacobian of $r(m)$. The exact Hessian is\n$$\nH(m) = \\nabla^2 \\phi(m) = J(m)^\\top J(m) + \\sum_{i=1}^2 r_i(m)\\,\\nabla^2 r_i(m),\n$$\nand the Gauss–Newton (GN) approximation uses\n$$\nH_{\\mathrm{GN}}(m) = J(m)^\\top J(m),\n$$\nthat is, it neglects the second-derivative term $\\sum_{i=1}^2 r_i(m)\\,\\nabla^2 r_i(m)$.\n\nA trust-region (TR) step $p$ at iterate $m$ is defined as the solution of the one-dimensional quadratic subproblem\n$$\n\\min_{p \\in \\mathbb{R}} \\; q(p) = g(m)\\,p + \\tfrac{1}{2} H_\\star(m)\\,p^2 \\quad \\text{subject to} \\quad |p| \\le \\Delta,\n$$\nwhere $H_\\star(m)$ denotes either the exact Hessian $H(m)$ or the Gauss–Newton Hessian $H_{\\mathrm{GN}}(m)$, and $\\Delta  0$ is the trust-region radius. For one dimension, this subproblem has an elementary closed-form solution: if the unconstrained minimizer $p^\\mathrm{N} = -g/H_\\star$ lies within the interval $[-\\Delta,\\Delta]$ and $H_\\star  0$, then $p = p^\\mathrm{N}$; otherwise, $p$ is the boundary point in $\\{-\\Delta, +\\Delta\\}$ that minimizes $q(p)$.\n\nFor any computed step $p$, define:\n- the predicted reduction using the model $q(p)$ as\n$$\n\\mathrm{pred} = -\\big(g(m)\\,p + \\tfrac{1}{2} H_\\star(m)\\,p^2\\big),\n$$\n- the actual reduction using the true objective as\n$$\n\\mathrm{ared} = \\phi(m) - \\phi(m + p),\n$$\n- the ratio\n$$\n\\rho = \\frac{\\mathrm{ared}}{\\mathrm{pred}},\n$$\nwith the convention that if the denominator is $0$, treat the ratio as $0$.\n\nYour program must implement the following using only standard calculus from the definitions above:\n- compute $r_1(m)$ and $r_2(m)$,\n- compute $J(m)$, $g(m)$, $H(m)$, and $H_{\\mathrm{GN}}(m)$,\n- solve the one-dimensional trust-region subproblem for both choices $H_\\star \\in \\{H, H_{\\mathrm{GN}}\\}$,\n- compute $\\mathrm{pred}$, $\\mathrm{ared}$, and $\\rho$ for both choices.\n\nUse the following fixed parameters and test suite:\n- constants: $a = 1.0$, $b = 5.0$, $c = 1.0$, $d = 0.1$, $e = 0.0$,\n- test cases are ordered pairs $(m, \\Delta)$:\n  - case $1$: $(m, \\Delta) = (0.1, 0.05)$,\n  - case $2$: $(m, \\Delta) = (0.1, 0.4)$,\n  - case $3$: $(m, \\Delta) = (0.1, 1.0)$,\n  - case $4$: $(m, \\Delta) = (0.3, 0.2)$.\n\nAll quantities are unitless. Your program must, for each case, output a list of six real numbers in the order\n$$\n[p_H, \\; p_{\\mathrm{GN}}, \\; \\mathrm{ared}_H, \\; \\mathrm{ared}_{\\mathrm{GN}}, \\; \\rho_H, \\; \\rho_{\\mathrm{GN}}],\n$$\nwhere $p_H$ and $p_{\\mathrm{GN}}$ are the trust-region steps using $H$ and $H_{\\mathrm{GN}}$, respectively. The final output must be a single line containing a list of these lists, one per test case, in the same order as above. Each real number must be rounded to six decimal places. For example, a syntactic template is\n$$\n\\big[ [\\cdots], [\\cdots], [\\cdots], [\\cdots] \\big].\n$$",
            "solution": "The user has provided a valid problem statement in computational geophysics concerning nonlinear least-squares inversion. The task is to analyze a one-parameter model, comparing trust-region steps computed using the exact Hessian versus its Gauss–Newton approximation. This analysis requires deriving the necessary mathematical quantities, implementing the trust-region step calculation, and evaluating the quality of the steps through predicted and actual reductions in the objective function.\n\nThe core of the problem lies in the nonlinear least-squares objective function $\\phi(m)$ for a scalar parameter $m \\in \\mathbb{R}$:\n$$\n\\phi(m) = \\frac{1}{2} \\|r(m)\\|_2^2 = \\frac{1}{2}\\left(r_1(m)^2 + r_2(m)^2\\right)\n$$\nwhere the residual components are given by:\n$$\n\\begin{aligned}\nr_1(m) = a \\exp(-b m^2) - d \\\\\nr_2(m) = c m - e\n\\end{aligned}\n$$\nwith fixed positive constants $a$, $b$, $c$, $d$, and $e$.\n\nTo implement the trust-region method, we must first compute the gradient $g(m)$ and the Hessian $H(m)$ of the objective function $\\phi(m)$. The gradient is given by the chain rule:\n$$\ng(m) = \\nabla \\phi(m) = J(m)^\\top r(m)\n$$\nwhere $J(m)$ is the Jacobian of the residual vector $r(m)$. For a scalar parameter $m$, the Jacobian is a $2 \\times 1$ matrix whose entries are the derivatives of the residual components with respect to $m$:\n$$\nJ(m) = \\begin{pmatrix} \\frac{\\partial r_1}{\\partial m} \\\\ \\frac{\\partial r_2}{\\partial m} \\end{pmatrix} = \\begin{pmatrix} -2abm \\exp(-b m^2) \\\\ c \\end{pmatrix}\n$$\nLet's denote the components of the Jacobian as $J_1(m) = -2abm \\exp(-b m^2)$ and $J_2(m) = c$. The gradient is then a scalar:\n$$\ng(m) = J_1(m) r_1(m) + J_2(m) r_2(m)\n$$\n\nThe exact Hessian $H(m)$ is given by:\n$$\nH(m) = \\nabla^2 \\phi(m) = J(m)^\\top J(m) + \\sum_{i=1}^2 r_i(m)\\,\\nabla^2 r_i(m)\n$$\nThe first term, $J(m)^\\top J(m)$, is the Gauss–Newton (GN) approximation to the Hessian:\n$$\nH_{\\mathrm{GN}}(m) = J_1(m)^2 + J_2(m)^2 = \\left(-2abm \\exp(-b m^2)\\right)^2 + c^2 = 4a^2b^2m^2 \\exp(-2bm^2) + c^2\n$$\nNote that $H_{\\mathrm{GN}}(m)$ is always non-negative, and strictly positive if $c \\neq 0$.\n\nThe second term in the exact Hessian requires the second derivatives of the residuals:\n$$\n\\frac{d^2 r_1}{dm^2} = \\frac{d}{dm}\\left(-2abm \\exp(-b m^2)\\right) = -2ab \\exp(-b m^2) (1 - 2bm^2)\n$$\n$$\n\\frac{d^2 r_2}{dm^2} = \\frac{d}{dm}(c) = 0\n$$\nThe exact Hessian is therefore:\n$$\nH(m) = H_{\\mathrm{GN}}(m) + r_1(m) \\frac{d^2 r_1}{dm^2}\n$$\nThe second term, $r_1(m) \\frac{d^2 r_1}{dm^2}$, can be negative. If this term is sufficiently large and negative, the entire Hessian $H(m)$ can become negative, a condition known as negative curvature. This signifies that the objective function is locally concave, and the Gauss-Newton approximation, being positive definite, provides a poor local model.\n\nThe trust-region (TR) step $p$ is the solution to the one-dimensional quadratic subproblem:\n$$\n\\min_{p \\in \\mathbb{R}} \\; q(p) = g(m)\\,p + \\frac{1}{2} H_\\star(m)\\,p^2 \\quad \\text{subject to} \\quad |p| \\le \\Delta\n$$\nwhere $H_\\star$ is either $H(m)$ or $H_{\\mathrm{GN}}(m)$, and $\\Delta  0$ is the trust-region radius. The solution strategy for this subproblem depends on the sign of $H_\\star$.\n1.  If $H_\\star  0$, the quadratic model $q(p)$ is convex. The unconstrained minimizer is $p^\\mathrm{N} = -g(m)/H_\\star$. If $|p^\\mathrm{N}| \\le \\Delta$, the optimal step is $p=p^\\mathrm{N}$. If $|p^\\mathrm{N}|  \\Delta$, the step is on the boundary of the trust region, $p = \\Delta \\cdot \\mathrm{sign}(p^\\mathrm{N}) = \\mathrm{copysign}(\\Delta, p^\\mathrm{N})$.\n2.  If $H_\\star \\le 0$, the quadratic model is linear or concave. The minimum over the interval $[-\\Delta, \\Delta]$ must occur at a boundary. To minimize $q(p)$, we choose the boundary point that makes the dominant linear term $g(m)p$ most negative. The step is thus $p = -\\Delta \\cdot \\mathrm{sign}(g(m)) = \\mathrm{copysign}(\\Delta, -g(m))$.\n\nA unified algorithm for finding the step $p$ is:\n- If $H_\\star  0$, calculate the unconstrained step $p^\\mathrm{N} = -g/H_\\star$. If $|p^\\mathrm{N}| \\le \\Delta$, then $p = p^\\mathrm{N}$.\n- Otherwise (if $H_\\star \\le 0$ or $|p^\\mathrm{N}|  \\Delta$), the step lies on the boundary: $p = \\mathrm{copysign}(\\Delta, -g)$.\n\nOnce a step $p$ is computed, we evaluate its quality using two metrics:\n- The predicted reduction, based on the quadratic model: $\\mathrm{pred} = -\\left(g(m)\\,p + \\frac{1}{2} H_\\star(m)\\,p^2\\right)$.\n- The actual reduction, based on the true objective function: $\\mathrm{ared} = \\phi(m) - \\phi(m + p)$.\n\nThe ratio $\\rho = \\mathrm{ared}/\\mathrm{pred}$ measures the agreement between the model and the actual function. A value of $\\rho$ close to $1$ indicates an excellent local approximation.\n\nThe program will execute the following steps for each test case $(m, \\Delta)$:\n1.  Compute the numerical values of $g(m)$, $H(m)$, and $H_{\\mathrm{GN}}(m)$ using the derived formulas and provided constants.\n2.  Solve the TR subproblem for $p_H$ using $H_\\star = H(m)$.\n3.  Solve the TR subproblem for $p_{\\mathrm{GN}}$ using $H_\\star = H_{\\mathrm{GN}}(m)$.\n4.  For each step ($p_H$ and $p_{\\mathrm{GN}}$), calculate the corresponding $\\mathrm{ared}$ and $\\rho$ values.\n5.  Collect the six resulting values: $p_H, p_{\\mathrm{GN}}, \\mathrm{ared}_H, \\mathrm{ared}_{\\mathrm{GN}}, \\rho_H, \\rho_{\\mathrm{GN}}$.\n6.  Format the results as required.",
            "answer": "[[-0.050000, -0.050000, 0.040183, 0.040183, 0.902996, 0.902996], [-0.400000, -0.400000, 0.141697, 0.141697, 0.207909, 0.207909], [-1.000000, -1.000000, -1.258169, -1.258169, 0.401925, 0.401925], [-0.200000, -0.191632, 0.125134, 0.130637, 0.771960, 1.000109]]"
        }
    ]
}