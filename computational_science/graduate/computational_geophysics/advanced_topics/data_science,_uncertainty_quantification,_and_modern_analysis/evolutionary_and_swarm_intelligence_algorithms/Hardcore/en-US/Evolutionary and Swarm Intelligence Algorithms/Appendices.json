{
    "hands_on_practices": [
        {
            "introduction": "Particle Swarm Optimization (PSO) is a powerful swarm intelligence method inspired by the collective behavior of bird flocks or fish schools. This exercise provides a concrete, step-by-step walkthrough of a single PSO iteration, grounding the algorithm in a geophysical inversion context. By manually computing the velocity and position updates for a small swarm, you will gain a practical understanding of key mechanics like inertia, cognitive and social influences, and the important role of parameter normalization and velocity clamping in real-world applications. ",
            "id": "3589764",
            "problem": "A team is using Particle Swarm Optimization (PSO) in a ring neighborhood topology to invert a one-dimensional two-parameter subsurface model for Rayleigh-wave dispersion in a computational geophysics study. The model parameter vector is $x = [V_{s}, H]^{\\top}$, where $V_{s}$ is the shear-wave velocity in $\\mathrm{km/s}$ and $H$ is the layer thickness in $\\mathrm{km}$. To mitigate parameter scale disparity, the algorithm internally uses a normalized parameter $z$ defined by\n$z = D^{-1}(x - x_{\\mathrm{ref}})$,\nwith reference $x_{\\mathrm{ref}} = [2.5, 1.5]^{\\top}$ and $D = \\mathrm{diag}(0.5, 0.5)$, both expressed in the physical units of $x$. The PSO update and velocity clamping are performed entirely in the normalized space and then mapped back to the physical space.\n\nConsider a ring of three particles with the following current positions $x_{i}$, velocities $v_{i}$, personal bests $p_{i}$, and neighborhood bests $n_{i}$, all in the physical space (units as above), where the ring order is $1 \\to 2 \\to 3 \\to 1$:\n- Particle $1$: $x_{1} = [2.2, 1.8]^{\\top}$, $v_{1} = [0.05, -0.02]^{\\top}$, $p_{1} = [2.15, 1.85]^{\\top}$, $n_{1} = [2.95, 1.05]^{\\top}$.\n- Particle $2$: $x_{2} = [2.9, 1.2]^{\\top}$, $v_{2} = [-0.08, 0.04]^{\\top}$, $p_{2} = [2.95, 1.05]^{\\top}$, $n_{2} = [2.35, 1.10]^{\\top}$.\n- Particle $3$: $x_{3} = [2.4, 1.0]^{\\top}$, $v_{3} = [0.02, 0.00]^{\\top}$, $p_{3} = [2.35, 1.10]^{\\top}$, $n_{3} = [2.15, 1.85]^{\\top}$.\n\nThe PSO hyperparameters are inertia weight $w = 0.7$, cognitive acceleration $c_{1} = 1.5$, and social acceleration $c_{2} = 1.2$. Velocity clamping in the normalized space is elementwise with $v_{\\max} = [0.9, 0.9]^{\\top}$. The stochastic multipliers $r_{1}$ and $r_{2}$ are independently drawn for each particle and dimension and are fixed by the following random seeds for this iteration:\n- For particle $1$: $r_{1} = [0.62, 0.15]^{\\top}$, $r_{2} = [0.35, 0.78]^{\\top}$.\n- For particle $2$: $r_{1} = [0.27, 0.91]^{\\top}$, $r_{2} = [0.58, 0.22]^{\\top}$.\n- For particle $3$: $r_{1} = [0.83, 0.40]^{\\top}$, $r_{2} = [0.11, 0.66]^{\\top}$.\n\nCompute the next-iteration velocities $v_{i}^{+}$ and positions $x_{i}^{+}$ in the physical space for all three particles, given that the canonical PSO velocity-position update is applied in the normalized space with the above ring-neighborhood bests and velocity clamping, followed by mapping back to the physical space.\n\nProvide your final result as a single row matrix in the order\n$\\big[v_{1,1}^{+}, v_{1,2}^{+}, x_{1,1}^{+}, x_{1,2}^{+}, v_{2,1}^{+}, v_{2,2}^{+}, x_{2,1}^{+}, x_{2,2}^{+}, v_{3,1}^{+}, v_{3,2}^{+}, x_{3,1}^{+}, x_{3,2}^{+}\\big]$.\nRound all numerical entries to four significant figures. Do not include units in your final boxed answer, but interpret $V_{s}$ in $\\mathrm{km/s}$ and $H$ in $\\mathrm{km}$ throughout the computation.",
            "solution": "### Step 1: Extract Givens\n- **Model Parameters**: $x = [V_{s}, H]^{\\top}$, where $V_s$ is shear-wave velocity in $\\mathrm{km/s}$ and $H$ is layer thickness in $\\mathrm{km}$.\n- **Normalization Transformation**: $z = D^{-1}(x - x_{\\mathrm{ref}})$.\n- **Reference Vector**: $x_{\\mathrm{ref}} = [2.5, 1.5]^{\\top}$ in physical units.\n- **Scaling Matrix**: $D = \\mathrm{diag}(0.5, 0.5)$ in physical units.\n- **Particle Data (Physical Space)**:\n  - Particle $1$: $x_{1} = [2.2, 1.8]^{\\top}$, $v_{1} = [0.05, -0.02]^{\\top}$, $p_{1} = [2.15, 1.85]^{\\top}$, $n_{1} = [2.95, 1.05]^{\\top}$.\n  - Particle $2$: $x_{2} = [2.9, 1.2]^{\\top}$, $v_{2} = [-0.08, 0.04]^{\\top}$, $p_{2} = [2.95, 1.05]^{\\top}$, $n_{2} = [2.35, 1.10]^{\\top}$.\n  - Particle $3$: $x_{3} = [2.4, 1.0]^{\\top}$, $v_{3} = [0.02, 0.00]^{\\top}$, $p_{3} = [2.35, 1.10]^{\\top}$, $n_{3} = [2.15, 1.85]^{\\top}$.\n- **PSO Hyperparameters**:\n  - Inertia weight: $w = 0.7$.\n  - Cognitive acceleration: $c_{1} = 1.5$.\n  - Social acceleration: $c_{2} = 1.2$.\n- **Velocity Clamping (Normalized Space)**: $v_{\\max} = [0.9, 0.9]^{\\top}$.\n- **Stochastic Multipliers**:\n  - For particle $1$: $r_{1} = [0.62, 0.15]^{\\top}$, $r_{2} = [0.35, 0.78]^{\\top}$.\n  - For particle $2$: $r_{1} = [0.27, 0.91]^{\\top}$, $r_{2} = [0.58, 0.22]^{\\top}$.\n  - For particle $3$: $r_{1} = [0.83, 0.40]^{\\top}$, $r_{2} = [0.11, 0.66]^{\\top}$.\n- **Task**: Compute the next-iteration velocities $v_{i}^{+}$ and positions $x_{i}^{+}$ in the physical space for all three particles. Round the final answer to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard application of the Particle Swarm Optimization (PSO) algorithm to an inverse problem in computational geophysics. All necessary parameters, initial conditions, and algorithmic rules are provided. The values are numerically consistent and do not violate any mathematical or physical principles. The problem specifies that the PSO updates are performed in a normalized space, which is a common technique. The provision of specific neighborhood bests ($n_i$) for each particle is a given condition of the algorithm's state at a particular iteration, and while their pattern might be specific, it does not constitute a contradiction. The problem is self-contained and algorithmically executable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe core of the problem is to apply the PSO update equations in a normalized parameter space and then transform the results back into the physical space.\n\nFirst, we define the transformation equations. Let a vector in the physical space be denoted by a subscript $x$ (e.g., $x_i$, $v_x$) and in the normalized space by a subscript $z$ (e.g., $z_i$, $v_z$).\nThe transformation from physical to normalized space is given by:\n$z = D^{-1}(x - x_{\\mathrm{ref}})$\nThe velocity transformation is found by differentiating the position transformation: $v_z = D^{-1}v_x$.\nGiven $D = \\mathrm{diag}(0.5, 0.5)$, its inverse is $D^{-1} = \\mathrm{diag}(2, 2)$.\nSo, $z = 2(x - x_{\\mathrm{ref}})$ and $v_z = 2v_x$.\n\nThe transformation from normalized to physical space is:\n$x = Dz + x_{\\mathrm{ref}} = 0.5z + x_{\\mathrm{ref}}$\n$v_x = Dv_z = 0.5v_z$\n\nThe canonical PSO update equations in the normalized space are:\n1. Velocity update: $v_{z,i}^{+} = w v_{z,i} + c_{1} (r_{1,i} \\odot (p_{z,i} - z_i)) + c_{2} (r_{2,i} \\odot (n_{z,i} - z_i))$\n2. Velocity clamping: Each component of $v_{z,i}^{+}$ is clamped to the range $[-v_{\\max,j}, v_{\\max,j}]$.\n3. Position update: $z_i^{+} = z_i + v_{z,i}^{+}$\nHere, $\\odot$ denotes element-wise vector multiplication.\n\nWe will now process each particle individually.\n\n**Particle 1**\n\n1.  **Transform to Normalized Space**:\n    $x_{\\mathrm{ref}} = [2.5, 1.5]^{\\top}$.\n    $z_1 = 2(x_1 - x_{\\mathrm{ref}}) = 2([2.2, 1.8]^{\\top} - [2.5, 1.5]^{\\top}) = 2([-0.3, 0.3]^{\\top}) = [-0.6, 0.6]^{\\top}$.\n    $v_{z,1} = 2v_1 = 2([0.05, -0.02]^{\\top}) = [0.1, -0.04]^{\\top}$.\n    $p_{z,1} = 2(p_1 - x_{\\mathrm{ref}}) = 2([2.15, 1.85]^{\\top} - [2.5, 1.5]^{\\top}) = 2([-0.35, 0.35]^{\\top}) = [-0.7, 0.7]^{\\top}$.\n    $n_{z,1} = 2(n_1 - x_{\\mathrm{ref}}) = 2([2.95, 1.05]^{\\top} - [2.5, 1.5]^{\\top}) = 2([0.45, -0.45]^{\\top}) = [0.9, -0.9]^{\\top}$.\n\n2.  **Velocity Update (Normalized Space)**:\n    The cognitive term is $p_{z,1} - z_1 = [-0.7, 0.7]^{\\top} - [-0.6, 0.6]^{\\top} = [-0.1, 0.1]^{\\top}$.\n    The social term is $n_{z,1} - z_1 = [0.9, -0.9]^{\\top} - [-0.6, 0.6]^{\\top} = [1.5, -1.5]^{\\top}$.\n    The un-clamped velocity $v'_{z,1}$ is:\n    $v'_{z,1} = 0.7 [0.1, -0.04]^{\\top} + 1.5 ([0.62, 0.15]^{\\top} \\odot [-0.1, 0.1]^{\\top}) + 1.2 ([0.35, 0.78]^{\\top} \\odot [1.5, -1.5]^{\\top})$\n    $v'_{z,1} = [0.07, -0.028]^{\\top} + 1.5 [-0.062, 0.015]^{\\top} + 1.2 [0.525, -1.17]^{\\top}$\n    $v'_{z,1} = [0.07, -0.028]^{\\top} + [-0.093, 0.0225]^{\\top} + [0.63, -1.404]^{\\top}$\n    $v'_{z,1} = [0.07-0.093+0.63, -0.028+0.0225-1.404]^{\\top} = [0.607, -1.4095]^{\\top}$.\n\n3.  **Velocity Clamping**:\n    $v_{\\max} = [0.9, 0.9]^{\\top}$.\n    $v_{z,1,1}^{+} = 0.607$ (since $|0.607| \\le 0.9$).\n    $v_{z,1,2}^{+} = -0.9$ (since $|-1.4095| > 0.9$).\n    So, the clamped velocity is $v_{z,1}^{+} = [0.607, -0.9]^{\\top}$.\n\n4.  **Position Update (Normalized Space)**:\n    $z_1^{+} = z_1 + v_{z,1}^{+} = [-0.6, 0.6]^{\\top} + [0.607, -0.9]^{\\top} = [0.007, -0.3]^{\\top}$.\n\n5.  **Transform back to Physical Space**:\n    $v_{1}^{+} = 0.5 v_{z,1}^{+} = 0.5 [0.607, -0.9]^{\\top} = [0.3035, -0.45]^{\\top}$.\n    $x_{1}^{+} = 0.5 z_1^{+} + x_{\\mathrm{ref}} = 0.5 [0.007, -0.3]^{\\top} + [2.5, 1.5]^{\\top} = [0.0035, -0.15]^{\\top} + [2.5, 1.5]^{\\top} = [2.5035, 1.35]^{\\top}$.\n\n**Particle 2**\n\n1.  **Transform to Normalized Space**:\n    $z_2 = 2(x_2 - x_{\\mathrm{ref}}) = 2([2.9, 1.2]^{\\top} - [2.5, 1.5]^{\\top}) = 2([0.4, -0.3]^{\\top}) = [0.8, -0.6]^{\\top}$.\n    $v_{z,2} = 2v_2 = 2([-0.08, 0.04]^{\\top}) = [-0.16, 0.08]^{\\top}$.\n    $p_{z,2} = 2(p_2 - x_{\\mathrm{ref}}) = 2([2.95, 1.05]^{\\top} - [2.5, 1.5]^{\\top}) = 2([0.45, -0.45]^{\\top}) = [0.9, -0.9]^{\\top}$.\n    $n_{z,2} = 2(n_2 - x_{\\mathrm{ref}}) = 2([2.35, 1.10]^{\\top} - [2.5, 1.5]^{\\top}) = 2([-0.15, -0.4]^{\\top}) = [-0.3, -0.8]^{\\top}$.\n\n2.  **Velocity Update (Normalized Space)**:\n    The cognitive term is $p_{z,2} - z_2 = [0.9, -0.9]^{\\top} - [0.8, -0.6]^{\\top} = [0.1, -0.3]^{\\top}$.\n    The social term is $n_{z,2} - z_2 = [-0.3, -0.8]^{\\top} - [0.8, -0.6]^{\\top} = [-1.1, -0.2]^{\\top}$.\n    $v'_{z,2} = 0.7 [-0.16, 0.08]^{\\top} + 1.5 ([0.27, 0.91]^{\\top} \\odot [0.1, -0.3]^{\\top}) + 1.2 ([0.58, 0.22]^{\\top} \\odot [-1.1, -0.2]^{\\top})$\n    $v'_{z,2} = [-0.112, 0.056]^{\\top} + 1.5 [0.027, -0.273]^{\\top} + 1.2 [-0.638, -0.044]^{\\top}$\n    $v'_{z,2} = [-0.112, 0.056]^{\\top} + [0.0405, -0.4095]^{\\top} + [-0.7656, -0.0528]^{\\top}$\n    $v'_{z,2} = [-0.112+0.0405-0.7656, 0.056-0.4095-0.0528]^{\\top} = [-0.8371, -0.4063]^{\\top}$.\n\n3.  **Velocity Clamping**:\n    $|v'_{z,2,1}| = 0.8371 \\le 0.9$ and $|v'_{z,2,2}| = 0.4063 \\le 0.9$. No clamping is needed.\n    $v_{z,2}^{+} = [-0.8371, -0.4063]^{\\top}$.\n\n4.  **Position Update (Normalized Space)**:\n    $z_2^{+} = z_2 + v_{z,2}^{+} = [0.8, -0.6]^{\\top} + [-0.8371, -0.4063]^{\\top} = [-0.0371, -1.0063]^{\\top}$.\n\n5.  **Transform back to Physical Space**:\n    $v_{2}^{+} = 0.5 v_{z,2}^{+} = 0.5 [-0.8371, -0.4063]^{\\top} = [-0.41855, -0.20315]^{\\top}$.\n    $x_{2}^{+} = 0.5 z_2^{+} + x_{\\mathrm{ref}} = 0.5 [-0.0371, -1.0063]^{\\top} + [2.5, 1.5]^{\\top} = [-0.01855, -0.50315]^{\\top} + [2.5, 1.5]^{\\top} = [2.48145, 0.99685]^{\\top}$.\n\n**Particle 3**\n\n1.  **Transform to Normalized Space**:\n    $z_3 = 2(x_3 - x_{\\mathrm{ref}}) = 2([2.4, 1.0]^{\\top} - [2.5, 1.5]^{\\top}) = 2([-0.1, -0.5]^{\\top}) = [-0.2, -1.0]^{\\top}$.\n    $v_{z,3} = 2v_3 = 2([0.02, 0.00]^{\\top}) = [0.04, 0.0]^{\\top}$.\n    $p_{z,3} = 2(p_3 - x_{\\mathrm{ref}}) = 2([2.35, 1.10]^{\\top} - [2.5, 1.5]^{\\top}) = 2([-0.15, -0.4]^{\\top}) = [-0.3, -0.8]^{\\top}$.\n    $n_{z,3} = 2(n_3 - x_{\\mathrm{ref}}) = 2([2.15, 1.85]^{\\top} - [2.5, 1.5]^{\\top}) = 2([-0.35, 0.35]^{\\top}) = [-0.7, 0.7]^{\\top}$.\n\n2.  **Velocity Update (Normalized Space)**:\n    The cognitive term is $p_{z,3} - z_3 = [-0.3, -0.8]^{\\top} - [-0.2, -1.0]^{\\top} = [-0.1, 0.2]^{\\top}$.\n    The social term is $n_{z,3} - z_3 = [-0.7, 0.7]^{\\top} - [-0.2, -1.0]^{\\top} = [-0.5, 1.7]^{\\top}$.\n    $v'_{z,3} = 0.7 [0.04, 0.0]^{\\top} + 1.5 ([0.83, 0.40]^{\\top} \\odot [-0.1, 0.2]^{\\top}) + 1.2 ([0.11, 0.66]^{\\top} \\odot [-0.5, 1.7]^{\\top})$\n    $v'_{z,3} = [0.028, 0.0]^{\\top} + 1.5 [-0.083, 0.08]^{\\top} + 1.2 [-0.055, 1.122]^{\\top}$\n    $v'_{z,3} = [0.028, 0.0]^{\\top} + [-0.1245, 0.12]^{\\top} + [-0.066, 1.3464]^{\\top}$\n    $v'_{z,3} = [0.028-0.1245-0.066, 0.0+0.12+1.3464]^{\\top} = [-0.1625, 1.4664]^{\\top}$.\n\n3.  **Velocity Clamping**:\n    $|v'_{z,3,1}| = 0.1625 \\le 0.9$.\n    $|v'_{z,3,2}| = 1.4664 > 0.9$, so it is clamped to $0.9$.\n    $v_{z,3}^{+} = [-0.1625, 0.9]^{\\top}$.\n\n4.  **Position Update (Normalized Space)**:\n    $z_3^{+} = z_3 + v_{z,3}^{+} = [-0.2, -1.0]^{\\top} + [-0.1625, 0.9]^{\\top} = [-0.3625, -0.1]^{\\top}$.\n\n5.  **Transform back to Physical Space**:\n    $v_{3}^{+} = 0.5 v_{z,3}^{+} = 0.5 [-0.1625, 0.9]^{\\top} = [-0.08125, 0.45]^{\\top}$.\n    $x_{3}^{+} = 0.5 z_3^{+} + x_{\\mathrm{ref}} = 0.5 [-0.3625, -0.1]^{\\top} + [2.5, 1.5]^{\\top} = [-0.18125, -0.05]^{\\top} + [2.5, 1.5]^{\\top} = [2.31875, 1.45]^{\\top}$.\n\n**Final Assembly and Rounding**\nWe compile the computed vectors and round each entry to four significant figures.\n\n-   $v_{1}^{+} = [0.3035, -0.45]^{\\top} \\to [0.3035, -0.4500]^{\\top}$\n-   $x_{1}^{+} = [2.5035, 1.35]^{\\top} \\to [2.504, 1.350]^{\\top}$\n-   $v_{2}^{+} = [-0.41855, -0.20315]^{\\top} \\to [-0.4186, -0.2032]^{\\top}$\n-   $x_{2}^{+} = [2.48145, 0.99685]^{\\top} \\to [2.481, 0.9969]^{\\top}$\n-   $v_{3}^{+} = [-0.08125, 0.45]^{\\top} \\to [-0.08125, 0.4500]^{\\top}$\n-   $x_{3}^{+} = [2.31875, 1.45]^{\\top} \\to [2.319, 1.450]^{\\top}$\n\nThe final result is the flattened row matrix of these components.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.3035 & -0.4500 & 2.504 & 1.350 & -0.4186 & -0.2032 & 2.481 & 0.9969 & -0.08125 & 0.4500 & 2.319 & 1.450\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Differential Evolution (DE) is a remarkably effective and elegant evolutionary algorithm for global optimization. This practice focuses on the core mechanism of DE, where new candidate solutions are generated by creating vector differences between existing population members. By executing a single generation of the \"DE/best/1/bin\" variant on a simple problem, you will see exactly how the mutation and crossover operators work together to explore the search space, providing a clear and foundational understanding of this popular algorithm. ",
            "id": "3589833",
            "problem": "Consider a toy global inversion subproblem arising in computational geophysics for recovering a two-parameter model vector $x \\in \\mathbb{R}^{2}$ that encodes a simplified pair of normalized subsurface properties (for example, two-layer log-impedances). Assume that at this early optimization stage, the data misfit is already minimized within tolerance so that the regularization dominates the objective. Use the quadratic Tikhonov model norm $J(x) = \\|x\\|_{2}^{2}$ as the scalar objective to be minimized.\n\nYou will perform one generation of the Differential Evolution (DE) algorithm, specifically the \"DE/best/1/bin\" variant. The current population is \n$$\\mathcal{P}^{0} = \\{(0,0), (1,0), (0,1)\\},$$\nwith differential weight $F = 0.8$ and binomial crossover rate $CR = 0.9$. The greedy selection rule is: for each target $x_{i}$, replace $x_{i}$ by its trial vector $u_{i}$ if and only if $J(u_{i}) < J(x_{i})$; otherwise retain $x_{i}$.\n\nFor mutation (DE/best/1), let $x_{\\mathrm{best}}$ be the population member with the smallest objective value under $J$. For each target $x_{i}$, construct the mutant\n$$v_{i} = x_{\\mathrm{best}} + F\\left(x_{r_{1}} - x_{r_{2}}\\right),$$\nwhere $x_{r_{1}}$ and $x_{r_{2}}$ are the two distinct donors drawn deterministically as the two members of the current population that are not equal to the target $x_{i}$, ordered lexicographically so that $x_{r_{1}}$ is the lexicographically smaller of the pair and $x_{r_{2}}$ the larger. This donor choice may include $x_{\\mathrm{best}}$ if it is not the target.\n\nFor binomial crossover, form the trial vector $u_{i}$ componentwise as\n$$u_{i,j} = \n\\begin{cases}\nv_{i,j}, & \\text{if } r_{i,j} \\leq CR \\text{ or } j=j_{\\mathrm{rand}}(i),\\\\\nx_{i,j}, & \\text{otherwise},\n\\end{cases}$$\nwhere $r_{i,j} \\sim \\mathrm{Uniform}(0,1)$ are fixed draws and $j_{\\mathrm{rand}}(i)$ enforces at least one mutant component per trial.\n\nUse the following deterministic random draws and forced indices:\n- For target $x_{i} = (0,0)$, take $j_{\\mathrm{rand}}(i) = 1$ and $(r_{i,1}, r_{i,2}) = (0.10, 0.20)$.\n- For target $x_{i} = (1,0)$, take $j_{\\mathrm{rand}}(i) = 1$ and $(r_{i,1}, r_{i,2}) = (0.10, 0.95)$.\n- For target $x_{i} = (0,1)$, take $j_{\\mathrm{rand}}(i) = 2$ and $(r_{i,1}, r_{i,2}) = (0.95, 0.10)$.\n\nCarry out this single DE generation step explicitly: identify $x_{\\mathrm{best}}$, compute mutants $v_{i}$, compute trial vectors $u_{i}$ via binomial crossover, apply greedy selection against $J(x)=\\|x\\|_{2}^{2}$, and obtain the selected population $\\mathcal{P}^{1}$. Finally, report the single scalar quantity\n$$S = \\sum_{x \\in \\mathcal{P}^{1}} J(x)$$\nas an exact decimal (unitless). No rounding is required, and no units should be included in the reported value.",
            "solution": "The problem is first validated against the established criteria.\n\n### Step 1: Extract Givens\n-   Objective function to minimize: $J(x) = \\|x\\|_{2}^{2}$ for a model vector $x \\in \\mathbb{R}^{2}$.\n-   Algorithm: One generation of Differential Evolution (DE), \"DE/best/1/bin\" variant.\n-   Initial population: $\\mathcal{P}^{0} = \\{(0,0), (1,0), (0,1)\\}$.\n-   Differential weight: $F = 0.8$.\n-   Binomial crossover rate: $CR = 0.9$.\n-   Selection rule: For a target vector $x_{i}$, it is replaced by its trial vector $u_{i}$ if and only if $J(u_{i}) < J(x_{i})$.\n-   Mutation rule (\"DE/best/1\"): $v_{i} = x_{\\mathrm{best}} + F\\left(x_{r_{1}} - x_{r_{2}}\\right)$, where $x_{\\mathrm{best}}$ is the population member with the lowest objective value.\n-   Donor selection: For a target $x_{i}$, the donors $x_{r_{1}}$ and $x_{r_{2}}$ are the two other distinct members of the population, ordered lexicographically such that $x_{r_{1}}$ is smaller than $x_{r_{2}}$.\n-   Crossover rule (binomial):\n    $$u_{i,j} = \\begin{cases} v_{i,j}, & \\text{if } r_{i,j} \\leq CR \\text{ or } j=j_{\\mathrm{rand}}(i),\\\\ x_{i,j}, & \\text{otherwise}, \\end{cases}$$\n    where $r_{i,j}$ are random draws and $j_{\\mathrm{rand}}(i)$ is a forced index.\n-   Deterministic draws and indices:\n    -   Target $x_{i} = (0,0)$: $j_{\\mathrm{rand}}(i) = 1$, $(r_{i,1}, r_{i,2}) = (0.10, 0.20)$.\n    -   Target $x_{i} = (1,0)$: $j_{\\mathrm{rand}}(i) = 1$, $(r_{i,1}, r_{i,2}) = (0.10, 0.95)$.\n    -   Target $x_{i} = (0,1)$: $j_{\\mathrm{rand}}(i) = 2$, $(r_{i,1}, r_{i,2}) = (0.95, 0.10)$.\n-   Required output: The scalar sum $S = \\sum_{x \\in \\mathcal{P}^{1}} J(x)$, where $\\mathcal{P}^{1}$ is the population after one generation.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as it describes a standard application of a well-known metaheuristic optimization algorithm (Differential Evolution) to a common problem in computational science (Tikhonov-regularized inversion). The objective function is a standard quadratic norm. The problem is well-posed; it provides all necessary parameters, initial conditions, and deterministic rules (including fixed \"random\" numbers and a deterministic donor selection protocol) to compute a unique result. The language is precise and objective. The problem is self-contained and free from internal contradictions. The population size of $3$ is sufficient for the \"DE/best/1\" scheme, which requires a target vector and two distinct donor vectors for mutation.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe solution proceeds by performing one generation of the DE algorithm. Let the initial population be $\\mathcal{P}^{0} = \\{x_1, x_2, x_3\\}$, where $x_1 = (0,0)$, $x_2 = (1,0)$, and $x_3 = (0,1)$. The objective function is $J(x_1, x_2) = x_1^2 + x_2^2$.\n\nFirst, we evaluate the objective function for each member of the initial population:\n$J(x_1) = J(0,0) = 0^2 + 0^2 = 0$.\n$J(x_2) = J(1,0) = 1^2 + 0^2 = 1$.\n$J(x_3) = J(0,1) = 0^2 + 1^2 = 1$.\n\nThe best vector in the population, $x_{\\mathrm{best}}$, is the one with the minimum objective function value. Here, $x_{\\mathrm{best}} = x_1 = (0,0)$, with $J(x_{\\mathrm{best}}) = 0$.\n\nNow, we iterate through each of the three target vectors in $\\mathcal{P}^{0}$ to generate the new population $\\mathcal{P}^{1}$.\n\n**1. Target vector $x_1 = (0,0)$**\n-   **Mutation:** The other two population members are $(0,1)$ and $(1,0)$. Ordered lexicographically, they are $x_{r_1} = (0,1)$ and $x_{r_2} = (1,0)$. The mutant vector $v_1$ is:\n    $$v_1 = x_{\\mathrm{best}} + F(x_{r_1} - x_{r_2}) = (0,0) + 0.8((0,1) - (1,0)) = 0.8(-1, 1) = (-0.8, 0.8)$$\n-   **Crossover:** We form the trial vector $u_1 = (u_{1,1}, u_{1,2})$ from target $x_1=(0,0)$ and mutant $v_1=(-0.8, 0.8)$. The given parameters are $j_{\\mathrm{rand}}(1) = 1$, $(r_{1,1}, r_{1,2})=(0.10, 0.20)$, and $CR=0.9$.\n    -   For component $j=1$: $j=j_{\\mathrm{rand}}(1)$, so we must take the mutant's component: $u_{1,1} = v_{1,1} = -0.8$.\n    -   For component $j=2$: $j \\neq j_{\\mathrm{rand}}(1)$. We check if $r_{1,2} \\leq CR$. Since $0.20 \\leq 0.9$, the condition is met, so we take the mutant's component: $u_{1,2} = v_{1,2} = 0.8$.\n    -   The trial vector is $u_1 = (-0.8, 0.8)$.\n-   **Selection:** We compare $J(u_1)$ with $J(x_1)$.\n    $J(u_1) = (-0.8)^2 + (0.8)^2 = 0.64 + 0.64 = 1.28$.\n    $J(x_1) = 0$.\n    Since $J(u_1) \\not< J(x_1)$ (i.e., $1.28 \\not< 0$), the trial vector $u_1$ is discarded, and $x_1$ is retained. The first member of $\\mathcal{P}^{1}$ is $(0,0)$.\n\n**2. Target vector $x_2 = (1,0)$**\n-   **Mutation:** The other two population members are $(0,0)$ and $(0,1)$. Ordered lexicographically, they are $x_{r_1} = (0,0)$ and $x_{r_2} = (0,1)$. The mutant vector $v_2$ is:\n    $$v_2 = x_{\\mathrm{best}} + F(x_{r_1} - x_{r_2}) = (0,0) + 0.8((0,0) - (0,1)) = 0.8(0, -1) = (0, -0.8)$$\n-   **Crossover:** We form the trial vector $u_2 = (u_{2,1}, u_{2,2})$ from target $x_2=(1,0)$ and mutant $v_2=(0, -0.8)$. The given parameters are $j_{\\mathrm{rand}}(2) = 1$, $(r_{2,1}, r_{2,2})=(0.10, 0.95)$, and $CR=0.9$.\n    -   For component $j=1$: $j=j_{\\mathrm{rand}}(2)$, so we must take the mutant's component: $u_{2,1} = v_{2,1} = 0$.\n    -   For component $j=2$: $j \\neq j_{\\mathrm{rand}}(2)$. We check if $r_{2,2} \\leq CR$. Since $0.95 \\not\\leq 0.9$, the condition is not met, so we take the target's component: $u_{2,2} = x_{2,2} = 0$.\n    -   The trial vector is $u_2 = (0, 0)$.\n-   **Selection:** We compare $J(u_2)$ with $J(x_2)$.\n    $J(u_2) = 0^2 + 0^2 = 0$.\n    $J(x_2) = 1$.\n    Since $J(u_2) < J(x_2)$ (i.e., $0 < 1$), the trial vector $u_2$ replaces $x_2$. The second member of $\\mathcal{P}^{1}$ is $(0,0)$.\n\n**3. Target vector $x_3 = (0,1)$**\n-   **Mutation:** The other two population members are $(0,0)$ and $(1,0)$. Ordered lexicographically, they are $x_{r_1} = (0,0)$ and $x_{r_2} = (1,0)$. The mutant vector $v_3$ is:\n    $$v_3 = x_{\\mathrm{best}} + F(x_{r_1} - x_{r_2}) = (0,0) + 0.8((0,0) - (1,0)) = 0.8(-1, 0) = (-0.8, 0)$$\n-   **Crossover:** We form the trial vector $u_3 = (u_{3,1}, u_{3,2})$ from target $x_3=(0,1)$ and mutant $v_3=(-0.8, 0)$. The given parameters are $j_{\\mathrm{rand}}(3) = 2$, $(r_{3,1}, r_{3,2})=(0.95, 0.10)$, and $CR=0.9$.\n    -   For component $j=1$: $j \\neq j_{\\mathrm{rand}}(3)$. We check if $r_{3,1} \\leq CR$. Since $0.95 \\not\\leq 0.9$, the condition is not met, so we take the target's component: $u_{3,1} = x_{3,1} = 0$.\n    -   For component $j=2$: $j=j_{\\mathrm{rand}}(3)$, so we must take the mutant's component: $u_{3,2} = v_{3,2} = 0$.\n    -   The trial vector is $u_3 = (0, 0)$.\n-   **Selection:** We compare $J(u_3)$ with $J(x_3)$.\n    $J(u_3) = 0^2 + 0^2 = 0$.\n    $J(x_3) = 1$.\n    Since $J(u_3) < J(x_3)$ (i.e., $0 < 1$), the trial vector $u_3$ replaces $x_3$. The third member of $\\mathcal{P}^{1}$ is $(0,0)$.\n\nThe population after one generation is $\\mathcal{P}^{1} = \\{(0,0), (0,0), (0,0)\\}$.\nThe final step is to calculate the sum $S$ of the objective function values for all members of $\\mathcal{P}^{1}$.\n$$S = \\sum_{x \\in \\mathcal{P}^{1}} J(x) = J(0,0) + J(0,0) + J(0,0) = 0 + 0 + 0 = 0$$\nThe final result is required as an exact decimal.",
            "answer": "$$\\boxed{0.0}$$"
        },
        {
            "introduction": "The balance between exploration and exploitation is critical to the success of any evolutionary algorithm, and this balance is heavily influenced by the selection operator. This problem moves from numerical execution to theoretical analysis, asking you to derive the selection probability for the globally best individual under tournament selection, a widely used mechanism in Genetic Algorithms (GAs). This exercise will strengthen your analytical skills and provide deep insight into the concept of selection pressure and its implications for premature convergence and population diversity in complex geophysical inverse problems. ",
            "id": "3589798",
            "problem": "In a population-based global optimization for geophysical inverse modeling (for example, resistivity inversion from magnetotelluric data), a Genetic Algorithm (GA) maintains a population of candidate Earth models. Each model has a distinct rank according to a strictly ordered fitness measure (lower misfit implies better rank). Consider the following selection mechanism used to build a mating pool: in each independent tournament, draw $k$ distinct individuals uniformly at random without replacement from the current population of size $N$, and select the best-ranked individual among the $k$ as the tournament winner. Assume that the current globally best individual (rank $1$) is unique.\n\nLet $T$ denote the number of independent tournaments conducted to assemble the mating pool in one generation.\n\nStarting only from core probability principles (counting via combinations, the complement rule, and independence of trials), derive a closed-form expression for the probability that the globally best individual is selected at least once among the $T$ tournaments. Then, briefly discuss the qualitative implication of this expression for the expected takeover dynamics in GA-driven geophysical inversion as $k$ varies, assuming all other operators are neutral.\n\nProvide your final answer as a single closed-form analytic expression in terms of $N$, $k$, and $T$. Do not include units. Do not provide any numerical evaluation. The discussion should not appear in the final answer.",
            "solution": "The problem asks for a closed-form expression for the probability that the globally best individual in a population is selected at least once in a series of $T$ independent tournaments.\n\nFirst, we must validate the problem statement.\nThe givens are:\n- Population size: $N$.\n- Tournament size: $k$. Individuals for a tournament are drawn uniformly at random without replacement from the population.\n- The winner of a tournament is the individual with the best rank among the $k$ participants.\n- The population contains a unique, globally best individual (rank $1$).\n- The number of independent tournaments is $T$.\nThe problem is scientifically grounded, as tournament selection is a standard operator in Genetic Algorithms, which are frequently used in computational geophysics. The problem is well-posed, with all necessary parameters ($N$, $k$, $T$) defined symbolically to derive a closed-form expression. It is stated objectively and contains no internal contradictions or missing information, assuming the physically necessary condition $k \\leq N$. The problem is therefore deemed valid.\n\nLet $E$ be the event that the globally best individual (rank $1$) is selected at least once among the $T$ tournaments. A direct calculation of this probability is complex. It is more straightforward to use the complement rule.\n\nLet $E^c$ be the complement event: the globally best individual is *never* selected in any of the $T$ tournaments. The desired probability is $P(E) = 1 - P(E^c)$.\n\nThe problem states that the $T$ tournaments are independent trials. Let $A_i$ be the event that the globally best individual is *not* selected in the $i$-th tournament. Because the tournaments are independent, the probability of the complement event is the product of the probabilities for each trial:\n$$P(E^c) = P(A_1 \\cap A_2 \\cap \\dots \\cap A_T) = \\prod_{i=1}^{T} P(A_i)$$\nSince each tournament is identical, the probability $P(A_i)$ is the same for all $i$. Let this common probability be $P(A)$. Then:\n$$P(E^c) = [P(A)]^T$$\nOur task reduces to finding $P(A)$, the probability that the globally best individual is not selected in a single tournament.\n\nFor the globally best individual to be selected as the winner, it must first be drawn into the sample of $k$ participants. If it is drawn, it is guaranteed to win, as it has the best rank by definition. Therefore, the event \"the globally best individual wins the tournament\" is equivalent to \"the globally best individual is included in the random draw of $k$ participants.\"\n\nConsequently, the event $A$—that the globally best individual is *not* selected—is equivalent to the event that this individual is *not* included in the random draw of $k$ individuals.\n\nWe calculate the probability of this event using combinations. The total number of ways to draw $k$ distinct individuals from a population of $N$ is given by the binomial coefficient $\\binom{N}{k}$.\n$$|\\Omega| = \\binom{N}{k} = \\frac{N!}{k!(N-k)!}$$\nFor event $A$ to occur, all $k$ individuals must be chosen from the subset of the population that does *not* include the globally best individual. This subset has $N-1$ members. The number of ways to choose $k$ individuals from this subset is $\\binom{N-1}{k}$.\n$$|A| = \\binom{N-1}{k} = \\frac{(N-1)!}{k!(N-1-k)!}$$\nThe probability of event $A$ is the ratio of favorable outcomes to total outcomes:\n$$P(A) = \\frac{|A|}{|\\Omega|} = \\frac{\\binom{N-1}{k}}{\\binom{N}{k}}$$\nSimplifying this expression:\n$$P(A) = \\frac{\\frac{(N-1)!}{k!(N-1-k)!}}{\\frac{N!}{k!(N-k)!}} = \\frac{(N-1)!}{N!} \\cdot \\frac{(N-k)!}{(N-1-k)!} = \\frac{1}{N} \\cdot (N-k) = \\frac{N-k}{N} = 1 - \\frac{k}{N}$$\nNow we can find the probability of the complement event over $T$ tournaments:\n$$P(E^c) = [P(A)]^T = \\left(1 - \\frac{k}{N}\\right)^T$$\nFinally, the probability that the globally best individual is selected at least once is:\n$$P(E) = 1 - P(E^c) = 1 - \\left(1 - \\frac{k}{N}\\right)^T$$\n\n**Discussion of Implications**\n\nThe derived expression, $P(E) = 1 - \\left(1 - \\frac{k}{N}\\right)^{T}$, quantifies the selection pressure exerted by the tournament size, $k$. As $k$ increases, the ratio $k/N$ increases, causing the base term $\\left(1 - \\frac{k}{N}\\right)$ to decrease. For any fixed number of tournaments $T > 0$, the probability of the best individual *not* being selected, $\\left(1 - \\frac{k}{N}\\right)^{T}$, decreases, and thus the probability of it being selected at least once, $P(E)$, increases.\n\nThis has direct implications for the balance between exploration and exploitation in a GA used for geophysical inversion.\n-   **High Selection Pressure (large $k$):** A large tournament size (e.g., $k$ approaching $N$) leads to a very high probability of selecting the best individual. This creates strong selection pressure, rapidly promoting elite solutions. In the context of a GA, this can lead to a quick takeover of the population by the current best individuals, causing fast convergence. While seemingly desirable, for complex, multi-modal geophysical misfit landscapes, this can result in **premature convergence**, where the algorithm gets trapped in a local minimum and fails to find the globally optimal Earth model.\n-   **Low Selection Pressure (small $k$):** A small tournament size (e.g., $k=2$) gives less-fit individuals a greater chance to be selected and reproduce. This reduces selection pressure, helps maintain population diversity, and enhances the algorithm's ability to explore the search space more broadly. This improved exploration increases the chance of escaping local minima and finding the global optimum, but it typically slows down the overall convergence rate.\n\nTherefore, the choice of $k$ represents a critical tuning parameter to manage the trade-off between exploiting promising regions and exploring new ones in the search for a geophysically meaningful inversion result.",
            "answer": "$$\\boxed{1 - \\left(1 - \\frac{k}{N}\\right)^{T}}$$"
        }
    ]
}