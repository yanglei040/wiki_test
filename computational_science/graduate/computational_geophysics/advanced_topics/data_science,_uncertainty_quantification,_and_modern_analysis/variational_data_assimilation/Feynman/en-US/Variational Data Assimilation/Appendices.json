{
    "hands_on_practices": [
        {
            "introduction": "The engine of variational data assimilation is gradient-based optimization, which requires accurate and efficient computation of the cost function's gradient. The adjoint model is the workhorse that provides this gradient, but its correctness hinges on a valid tangent-linear approximation of the forward model. This exercise guides you through the implementation of a \"Taylor test,\" the standard and indispensable procedure for verifying that your tangent-linear and adjoint code pair is mathematically sound .",
            "id": "3618466",
            "problem": "You are given a nonlinear observation operator $H:\\mathbb{R}^n \\to \\mathbb{R}^m$ defined by\n$$\nH(x) \\;=\\; M\\,\\tanh(Ax)\\;+\\;\\big(s \\odot \\exp(Cx)\\big)\\;+\\;D\\,(x\\odot x),\n$$\nwhere $x\\in \\mathbb{R}^n$, $A\\in \\mathbb{R}^{m\\times n}$, $C\\in \\mathbb{R}^{m\\times n}$, $D\\in \\mathbb{R}^{m\\times n}$, $M\\in \\mathbb{R}^{m\\times m}$, $s\\in \\mathbb{R}^m$, the symbol $\\odot$ denotes componentwise (Hadamard) multiplication, and $\\tanh(\\cdot)$, $\\exp(\\cdot)$ act componentwise. Define the scalar-valued cost function $J:\\mathbb{R}^n\\to\\mathbb{R}$ by\n$$\nJ(x)\\;=\\;\\tfrac{1}{2}\\,\\|H(x)-y\\|_2^2,\n$$\nwith $y\\in\\mathbb{R}^m$ equal to the zero vector for all test cases.\n\nYou must verify a tangent-linear and adjoint pair for $H$ via Taylor remainder tests and report the observed order-of-accuracy as $\\epsilon\\to 0$. Use the following fundamental definitions. The Fréchet derivative $H'(x)$ satisfies\n$$\nH(x+\\epsilon p) \\;=\\; H(x) + \\epsilon\\,H'(x)\\,p \\;+\\; r_H(x,p,\\epsilon),\n$$\nwhere $\\|r_H(x,p,\\epsilon)\\|_2 = o(\\epsilon)$ as $\\epsilon\\to 0$. The gradient of $J$ is given by\n$$\n\\nabla J(x) \\;=\\; H'(x)^\\top\\big(H(x)-y\\big),\n$$\nwith transpose $H'(x)^\\top$ defined by the Euclidean inner product. The Taylor expansion of $J$ along direction $p$ is\n$$\nJ(x+\\epsilon p) \\;=\\; J(x) + \\epsilon\\,\\nabla J(x)^\\top p \\;+\\; r_J(x,p,\\epsilon),\n$$\nwhere $|r_J(x,p,\\epsilon)| = o(\\epsilon)$ as $\\epsilon\\to 0$.\n\nYour tasks are:\n- Derive the explicit tangent-linear operator $H'(x)$ and its adjoint $H'(x)^\\top$ from first principles for the given $H(x)$.\n- Implement both Taylor remainder tests:\n  1. The observation-operator test\n     $$\n     R_H(\\epsilon)\\;=\\;\\big\\|H(x+\\epsilon p) - H(x) - \\epsilon\\,H'(x)\\,p\\big\\|_2,\n     $$\n  2. The cost-function test\n     $$\n     R_J(\\epsilon)\\;=\\;\\big|J(x+\\epsilon p) - J(x) - \\epsilon\\,\\nabla J(x)^\\top p\\big|.\n     $$\n- For each test, estimate the observed order-of-accuracy by the slope of a linear least-squares fit of $\\log R(\\epsilon)$ versus $\\log \\epsilon$ over a prescribed grid of $\\epsilon$ values.\n\nAll trigonometric and hyperbolic function arguments are in radians. All computations are dimensionless.\n\nThe matrices and vector defining $H$ are deterministic functions of the sizes $m$ and $n$:\n- For $i\\in\\{0,\\dots,m-1\\}$ and $j\\in\\{0,\\dots,n-1\\}$,\n  $$\n  A_{ij} \\;=\\; 0.2\\,\\sin\\!\\Big(\\frac{(i+1)(j+1)}{3}\\Big),\\quad\n  C_{ij} \\;=\\; 0.15\\,\\cos\\!\\Big(\\frac{(i+1)+(j+1)}{2}\\Big),\\quad\n  D_{ij} \\;=\\; \\frac{0.05}{i+j+2}.\n  $$\n- For $i,j\\in\\{0,\\dots,m-1\\}$,\n  $$\n  M_{ij} \\;=\\; \n  \\begin{cases}\n  0.1, & i=j, \\\\[4pt]\n  \\dfrac{0.02}{i+j+2}, & i\\neq j.\n  \\end{cases}\n  $$\n- For $i\\in\\{0,\\dots,m-1\\}$,\n  $$\n  s_i \\;=\\; 0.25 + 0.05\\,(i+1).\n  $$\n\nUse the following $\\epsilon$ grid for both tests:\n$$\n\\epsilon \\in \\{10^{-1},\\,10^{-2},\\,10^{-3},\\,10^{-4},\\,10^{-5}\\}.\n$$\n\nTest suite. For each case below, construct $A$, $C$, $D$, $M$, $s$ as specified, set $y=0$, and compute the two observed orders-of-accuracy (one for $R_H$, one for $R_J$):\n- Case $1$: $m=3$, $n=4$, $x=[0.3,\\,-0.2,\\,0.1,\\,-0.4]^\\top$, $p=[-0.5,\\,0.7,\\,-0.1,\\,0.2]^\\top$.\n- Case $2$: $m=5$, $n=3$, $x=[0,\\,0,\\,0]^\\top$, $p=[1,\\,1,\\,1]^\\top$.\n- Case $3$: $m=2$, $n=5$, $x=[0.2,\\,-0.1,\\,0.05,\\,0.3,\\,-0.25]^\\top$, $p=x$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as\n\n`[order_H,1, order_J,1, order_H,2, order_J,2, order_H,3, order_J,3]`\n\nwhere `order_H,k` is the observed order for $R_H$ in case $k$, and `order_J,k` is the observed order for $R_J$ in case $k$. Each entry must be a floating-point number.",
            "solution": "We start from the fundamental definitions of the Fréchet derivative and the Euclidean adjoint. The Fréchet derivative $H'(x)$ is defined by the limit\n$$\n\\lim_{\\epsilon\\to 0}\\frac{\\|H(x+\\epsilon p)-H(x)-\\epsilon H'(x)p\\|_2}{\\epsilon} \\;=\\; 0\n$$\nfor every direction $p\\in\\mathbb{R}^n$. The Euclidean adjoint $H'(x)^\\top:\\mathbb{R}^m\\to\\mathbb{R}^n$ is defined by\n$$\n\\langle H'(x)p,\\,q\\rangle_{\\mathbb{R}^m} \\;=\\; \\langle p,\\,H'(x)^\\top q\\rangle_{\\mathbb{R}^n}\n$$\nfor all $p\\in\\mathbb{R}^n$, $q\\in\\mathbb{R}^m$, where $\\langle\\cdot,\\cdot\\rangle$ denotes the standard Euclidean inner product. The cost function $J(x)=\\tfrac{1}{2}\\|H(x)-y\\|_2^2$ has gradient $\\nabla J(x)=H'(x)^\\top(H(x)-y)$ by the chain rule and symmetry of the Euclidean inner product.\n\nWe derive $H'(x)$ by applying the chain rule and product rules term by term to\n$$\nH(x) \\;=\\; M\\,\\tanh(Ax)\\;+\\;\\big(s \\odot \\exp(Cx)\\big)\\;+\\;D\\,(x\\odot x).\n$$\n\nFirst term. Let $u(x)=Ax\\in\\mathbb{R}^m$ and $v(u)=\\tanh(u)$ componentwise, so $v'(u)=\\operatorname{sech}^2(u)$ componentwise. For a direction $p$, $du=Ap$ and thus\n$$\nd\\big[M\\,\\tanh(Ax)\\big] \\;=\\; M\\,\\Big(\\operatorname{sech}^2(Ax)\\odot (Ap)\\Big).\n$$\nEquivalently, if we use a diagonal operator notation, this is $M\\,\\operatorname{Diag}(\\operatorname{sech}^2(Ax))\\,A\\,p$.\n\nSecond term. Let $w(x)=Cx\\in\\mathbb{R}^m$ and $z(w)=\\exp(w)$ componentwise, so $dz=\\exp(w)\\odot dw$. Then\n$$\nd\\big[s\\odot \\exp(Cx)\\big] \\;=\\; \\big(s\\odot \\exp(Cx)\\big)\\odot (Cp)\\;=\\;\\operatorname{Diag}\\big(s\\odot \\exp(Cx)\\big)\\,C\\,p.\n$$\n\nThird term. The map $x\\mapsto x\\odot x$ has derivative $p\\mapsto 2x\\odot p$ componentwise, so\n$$\nd\\big[D\\,(x\\odot x)\\big] \\;=\\; D\\,\\big(2x\\odot p\\big)\\;=\\;D\\,\\operatorname{Diag}(2x)\\,p.\n$$\n\nBy linearity of differentiation, we combine these to obtain the tangent-linear operator\n$$\nH'(x)\\,p \\;=\\; M\\,\\big(\\operatorname{sech}^2(Ax)\\odot (Ap)\\big)\\;+\\;\\big(s\\odot \\exp(Cx)\\big)\\odot (Cp)\\;+\\;D\\,\\big(2x\\odot p\\big).\n$$\n\nNext, we derive the adjoint $H'(x)^\\top$. For any $q\\in\\mathbb{R}^m$, consider the Euclidean inner product with the first term:\n$$\n\\langle M(\\operatorname{sech}^2(Ax)\\odot (Ap)),\\,q\\rangle \\;=\\; \\langle \\operatorname{sech}^2(Ax)\\odot (Ap),\\,M^\\top q\\rangle \\\\\n=\\; \\langle Ap,\\,\\operatorname{sech}^2(Ax)\\odot (M^\\top q)\\rangle \\;=\\; \\langle p,\\,A^\\top\\big(\\operatorname{sech}^2(Ax)\\odot (M^\\top q)\\big)\\rangle.\n$$\nTherefore the contribution of this term to $H'(x)^\\top q$ is $A^\\top\\big(\\operatorname{sech}^2(Ax)\\odot (M^\\top q)\\big)$.\n\nFor the second term,\n$$\n\\langle \\big(s\\odot \\exp(Cx)\\big)\\odot (Cp),\\,q\\rangle \\;=\\; \\langle Cp,\\,\\big(s\\odot \\exp(Cx)\\big)\\odot q\\rangle \\;=\\; \\langle p,\\,C^\\top\\big(\\big(s\\odot \\exp(Cx)\\big)\\odot q\\big)\\rangle,\n$$\nso the contribution is $C^\\top\\big(\\big(s\\odot \\exp(Cx)\\big)\\odot q\\big)$.\n\nFor the third term,\n$$\n\\langle D(2x\\odot p),\\,q\\rangle \\;=\\; \\langle 2x\\odot p,\\,D^\\top q\\rangle \\;=\\; \\langle p,\\,\\operatorname{Diag}(2x)\\,(D^\\top q)\\rangle \\;=\\; \\langle p,\\, (2x)\\odot (D^\\top q)\\rangle,\n$$\nso the contribution is $(2x)\\odot (D^\\top q)$.\n\nCollecting these, we obtain\n$$\nH'(x)^\\top q \\;=\\; A^\\top\\big(\\operatorname{sech}^2(Ax)\\odot (M^\\top q)\\big)\\;+\\;C^\\top\\big(\\big(s\\odot \\exp(Cx)\\big)\\odot q\\big)\\;+\\;(2x)\\odot (D^\\top q).\n$$\n\nFor the cost function $J(x)=\\tfrac{1}{2}\\|H(x)-y\\|_2^2$, the gradient follows directly from the chain rule as\n$$\n\\nabla J(x) \\;=\\; H'(x)^\\top\\big(H(x)-y\\big).\n$$\nAssuming $H$ is twice continuously differentiable, the Taylor remainder for $H$ obeys $\\|H(x+\\epsilon p)-H(x)-\\epsilon H'(x)p\\|_2 = \\mathcal{O}(\\epsilon^2)$ as $\\epsilon\\to 0$, and similarly the scalar Taylor remainder for $J$ satisfies $|J(x+\\epsilon p)-J(x)-\\epsilon \\nabla J(x)^\\top p| = \\mathcal{O}(\\epsilon^2)$. Therefore, plotting $\\log R(\\epsilon)$ versus $\\log \\epsilon$ should yield an asymptotic slope of $2$ for both tests when the tangent-linear and adjoint implementations are correct.\n\nAlgorithmic procedure:\n- Construct $A$, $C$, $D$, $M$, $s$ as specified from $m$ and $n$.\n- Implement $H(x)$ using matrix-vector multiplications and componentwise operations.\n- Implement the tangent-linear action $p\\mapsto H'(x)p$ and adjoint action $q\\mapsto H'(x)^\\top q$ using the derived formulas.\n- Implement $J(x)=\\tfrac{1}{2}\\|H(x)\\|_2^2$ with $y=0$ and its gradient $\\nabla J(x)=H'(x)^\\top H(x)$.\n- For $\\epsilon\\in\\{10^{-1},10^{-2},10^{-3},10^{-4},10^{-5}\\}$, compute $R_H(\\epsilon)$ and $R_J(\\epsilon)$ for each case, then estimate the observed orders by fitting a line to $(\\log \\epsilon,\\log R(\\epsilon))$ and taking the slope.\n- Aggregate the six floating-point slopes in the required order.\n\nNumerical considerations:\n- Very small $\\epsilon$ can yield round-off dominated remainders, flattening the slope. Using $\\epsilon$ in $\\{10^{-1},\\dots,10^{-5}\\}$ balances truncation and round-off for smooth $H$ with moderate scales. If any remainder becomes numerically zero, exclude such points from the fit; otherwise, a least-squares fit over the grid yields stable slope estimates close to $2$ when the implementations are correct.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_matrices(m, n):\n    # Deterministic construction per the problem statement\n    A = np.zeros((m, n), dtype=float)\n    C = np.zeros((m, n), dtype=float)\n    D = np.zeros((m, n), dtype=float)\n    M = np.zeros((m, m), dtype=float)\n    s = np.zeros(m, dtype=float)\n\n    for i in range(m):\n        for j in range(n):\n            A[i, j] = 0.2 * np.sin(((i + 1) * (j + 1)) / 3.0)\n            C[i, j] = 0.15 * np.cos(((i + 1) + (j + 1)) / 2.0)\n            D[i, j] = 0.05 / (i + j + 2.0)\n    for i in range(m):\n        for j in range(m):\n            if i == j:\n                M[i, j] = 0.1\n            else:\n                M[i, j] = 0.02 / (i + j + 2.0)\n    for i in range(m):\n        s[i] = 0.25 + 0.05 * (i + 1)\n    return A, C, D, M, s\n\ndef H_op(x, A, C, D, M, s):\n    # H(x) = M tanh(Ax) + (s ⊙ exp(Cx)) + D (x ⊙ x)\n    ax = A @ x\n    cx = C @ x\n    term1 = M @ np.tanh(ax)\n    term2 = s * np.exp(cx)\n    term3 = D @ (x * x)\n    return term1 + term2 + term3\n\ndef TL_op(x, p, A, C, D, M, s):\n    # Tangent-linear action H'(x) p\n    ax = A @ x\n    cx = C @ x\n    Ap = A @ p\n    Cp = C @ p\n    sech2 = 1.0 / np.cosh(ax)\n    sech2 = sech2 * sech2\n    term1 = M @ (sech2 * Ap)\n    term2 = (s * np.exp(cx)) * Cp\n    term3 = D @ (2.0 * x * p)\n    return term1 + term2 + term3\n\ndef ADJ_op(x, q, A, C, D, M, s):\n    # Adjoint action H'(x)^T q\n    ax = A @ x\n    cx = C @ x\n    sech2 = 1.0 / np.cosh(ax)\n    sech2 = sech2 * sech2\n    v1 = A.T @ (sech2 * (M.T @ q))\n    v2 = C.T @ ((s * np.exp(cx)) * q)\n    v3 = (2.0 * x) * (D.T @ q)\n    return v1 + v2 + v3\n\ndef J_cost(x, A, C, D, M, s):\n    hx = H_op(x, A, C, D, M, s)\n    return 0.5 * float(hx @ hx)\n\ndef grad_J(x, A, C, D, M, s):\n    hx = H_op(x, A, C, D, M, s)\n    return ADJ_op(x, hx, A, C, D, M, s)\n\ndef observed_order(eps_vals, residuals):\n    eps_vals = np.asarray(eps_vals, dtype=float)\n    residuals = np.asarray(residuals, dtype=float)\n    mask = residuals > 0.0\n    x = np.log(eps_vals[mask])\n    y = np.log(residuals[mask])\n    # If insufficient points, return nan\n    if x.size < 2:\n        return float('nan')\n    coeffs = np.polyfit(x, y, 1)\n    slope = coeffs[0]\n    return float(slope)\n\ndef run_case(m, n, x, p, eps_list):\n    A, C, D, M, s = build_matrices(m, n)\n    # Precompute base values\n    Hx = H_op(x, A, C, D, M, s)\n    TLxp = TL_op(x, p, A, C, D, M, s)\n    Jx = 0.5 * float(Hx @ Hx)\n    gx = ADJ_op(x, Hx, A, C, D, M, s)\n\n    RH = []\n    RJ = []\n    for eps in eps_list:\n        x_eps = x + eps * p\n        Hx_eps = H_op(x_eps, A, C, D, M, s)\n        # Remainder for H\n        rem_H = Hx_eps - Hx - eps * TLxp\n        RH.append(float(np.linalg.norm(rem_H)))\n        # Remainder for J\n        Jx_eps = J_cost(x_eps, A, C, D, M, s)\n        rem_J = Jx_eps - Jx - eps * float(gx @ p)\n        RJ.append(abs(float(rem_J)))\n    order_H = observed_order(eps_list, RH)\n    order_J = observed_order(eps_list, RJ)\n    return order_H, order_J\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (m, n, x, p)\n        (3, 4, np.array([0.3, -0.2, 0.1, -0.4], dtype=float),\n             np.array([-0.5, 0.7, -0.1, 0.2], dtype=float)),\n        (5, 3, np.array([0.0, 0.0, 0.0], dtype=float),\n             np.array([1.0, 1.0, 1.0], dtype=float)),\n        (2, 5, np.array([0.2, -0.1, 0.05, 0.3, -0.25], dtype=float),\n             np.array([0.2, -0.1, 0.05, 0.3, -0.25], dtype=float)),\n    ]\n\n    eps_list = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n\n    results = []\n    for (m, n, x, p) in test_cases:\n        order_H, order_J = run_case(m, n, x, p, eps_list)\n        results.append(order_H)\n        results.append(order_J)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Variational assimilation is fundamentally a statistical method, and its success depends on the validity of its underlying assumptions about error statistics. This practice focuses on diagnosing the specified background error covariance $B$ and observation error covariance $R$ by examining data generated by the system itself. You will implement diagnostics based on the innovation vector $d=y-Hx_b$ and analysis residuals $r=y-Hx_a$, whose statistical properties, when properly normalized, should be consistent with a chi-squared ($\\chi^2$) distribution .",
            "id": "3618559",
            "problem": "You will design and implement a quantitative diagnostic for a linear Gaussian variational data assimilation system and then compute it for a set of test cases. The setting is as follows. Let the model state be a vector $x \\in \\mathbb{R}^n$, the background (prior) state be $x_b \\in \\mathbb{R}^n$ with background error covariance $B \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite, and let the observations be $y \\in \\mathbb{R}^m$ with observation error covariance $R \\in \\mathbb{R}^{m \\times m}$ that is symmetric positive definite. The observation operator is linear and represented by $H \\in \\mathbb{R}^{m \\times n}$. The variational analysis $x_a$ is defined as the unique minimizer of the quadratic cost\n$$\nJ(x) = \\tfrac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b) + \\tfrac{1}{2}(y - H x)^\\top R^{-1} (y - H x).\n$$\nDefine the innovation (also called observation-minus-background) vector\n$$\nd = y - H x_b,\n$$\nand the analysis residual (also called observation-minus-analysis)\n$$\nr = y - H x_a.\n$$\nYour task is to construct a chi-squared diagnostic for both $d$ and $r$, and to state the precise conditions under which each diagnostic follows a known reference distribution when the covariances $R$ and $B$ are correct. Then, implement a program that:\n- Solves for $x_a$ by minimizing $J(x)$.\n- Computes the innovation $d$ and the residual $r$.\n- Forms the innovation error covariance\n$$\nS = H B H^\\top + R,\n$$\nand computes the innovation chi-square diagnostic\n$$\nz_d = d^\\top S^{-1} d.\n$$\n- Computes the residual chi-square diagnostic whitened by $R$,\n$$\nz_a = r^\\top R^{-1} r.\n$$\n- Computes the observation-space influence matrix (also called the observation hat matrix)\n$$\nA = H K \\quad \\text{with} \\quad K = B H^\\top S^{-1},\n$$\nand reports the scalar\n$$\np = \\operatorname{trace}(A),\n$$\ntogether with the baseline expectation\n$$\n\\mathbb{E}[z_a] = m - p.\n$$\nYou must provide the scientific conditions under which $z_d$ follows a chi-square distribution with $m$ degrees of freedom and under which $z_a$ follows a chi-square distribution with $m - p$ degrees of freedom, assuming $R$ and $B$ are correct.\n\nYour program must:\n- Implement the analysis $x_a$ by solving the first-order optimality condition for $J(x)$, using linear algebra primitives, without relying on any black-box optimizers or stochastic sampling.\n- Compute for each test case the tuple $[z_d, z_a, m, p, m - p]$ as floating-point numbers.\n- Produce a single line of output containing the list of results for all test cases as a comma-separated list enclosed in square brackets, where each element is the list $[z_d, z_a, m, p, m - p]$ for one test.\n\nNo physical units are required because the diagnostics are dimensionless. Angles do not appear. Percentages are not used.\n\nUse the following test suite, chosen to probe a general case with correlated errors, a case with highly informative observations, and a case with weak observations relative to the prior:\n- Test case $1$ (general, correlated $R$): $n = 2$, $m = 3$,\n  $$\n  H = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix},\\;\n  B = \\begin{bmatrix} 1.0 & 0.3 \\\\ 0.3 & 1.5 \\end{bmatrix},\\;\n  R = \\begin{bmatrix} 0.7 & 0.1 & 0.0 \\\\ 0.1 & 0.6 & 0.05 \\\\ 0.0 & 0.05 & 0.9 \\end{bmatrix},\n  $$\n  $$\n  x_b = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix},\\;\n  y = \\begin{bmatrix} 0.9 \\\\ -0.2 \\\\ 0.4 \\end{bmatrix}.\n  $$\n- Test case $2$ (highly informative observations): $n = 2$, $m = 3$,\n  $$\n  H = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix},\\;\n  B = \\begin{bmatrix} 1.0 & 0.3 \\\\ 0.3 & 1.5 \\end{bmatrix},\\;\n  R = \\operatorname{diag}(0.05, 0.05, 0.05),\n  $$\n  $$\n  x_b = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix},\\;\n  y = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.\n  $$\n- Test case $3$ (weak observations relative to prior): $n = 2$, $m = 2$,\n  $$\n  H = I_2,\\;\n  B = \\begin{bmatrix} 0.8 & 0.2 \\\\ 0.2 & 0.8 \\end{bmatrix},\\;\n  R = \\begin{bmatrix} 5.0 & 0.5 \\\\ 0.5 & 4.0 \\end{bmatrix},\n  $$\n  $$\n  x_b = \\begin{bmatrix} -0.5 \\\\ 0.3 \\end{bmatrix},\\;\n  y = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}.\n  $$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element is the list $[z_d, z_a, m, p, m - p]$ for one test case, in the order of the test cases above. For example, the output format must look like:\n\n`[[z_{d,1}, z_{a,1}, m_1, p_1, m_1 - p_1],[z_{d,2}, z_{a,2}, m_2, p_2, m_2 - p_2],[z_{d,3}, z_{a,3}, m_3, p_3, m_3 - p_3]]`\n\nIn addition to implementing the computation, in your solution you must clearly state the mathematical conditions that guarantee the reference distributions for $z_d$ and $z_a$ under correct $R$ and $B$.",
            "solution": "The problem requires the design and implementation of a quantitative diagnostic for a linear Gaussian variational data assimilation system. This involves deriving the analysis state, defining two chi-squared diagnostics, stating the precise conditions for their validity, and computing them for specific test cases.\n\nFirst, we validate the problem statement.\nAll givens, including the cost function $J(x)$, matrices $H, B, R$, and vectors $x_b, y$, as well as the definitions of $d, r, S, z_d, z_a, A, K, p$ are explicitly provided. The numerical values for three test cases are also supplied. The matrices $B$ and $R$ are defined as symmetric positive definite, which is standard. A check of the provided matrices confirms they satisfy this property. The problem is scientifically grounded in the well-established theory of statistical data assimilation. It is objective, well-posed, completely specified, and its components are internally consistent. The problem is therefore deemed valid.\n\nWe proceed with the solution.\n\n**1. Derivation of the Analysis State**\n\nThe analysis state $x_a$ is the vector $x$ that minimizes the cost function:\n$$\nJ(x) = \\tfrac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b) + \\tfrac{1}{2}(y - H x)^\\top R^{-1} (y - H x)\n$$\nSince $J(x)$ is a quadratic and convex function (its Hessian, $\\nabla^2 J = B^{-1} + H^\\top R^{-1} H$, is positive definite as $B^{-1}$ is positive definite and $H^\\top R^{-1} H$ is positive semi-definite), it has a unique minimum. We find this minimum by setting the gradient of $J(x)$ with respect to $x$ to zero:\n$$\n\\nabla_x J(x) = B^{-1} (x - x_b) - H^\\top R^{-1} (y - H x) = 0\n$$\nSetting $x = x_a$ and rearranging the terms gives:\n$$\n(B^{-1} + H^\\top R^{-1} H) x_a = B^{-1} x_b + H^\\top R^{-1} y\n$$\nSolving for $x_a$ directly requires inverting the $n \\times n$ matrix $(B^{-1} + H^\\top R^{-1} H)$. A more common and often computationally advantageous formulation, especially when the number of observations $m$ is smaller than the state dimension $n$, is the gain-form solution. Using the Woodbury matrix identity, one can show that this is equivalent to:\n$$\nx_a = x_b + B H^\\top (H B H^\\top + R)^{-1} (y - H x_b)\n$$\nUsing the problem's definitions, the innovation is $d = y - H x_b$ and the innovation covariance is $S = H B H^\\top + R$. The optimal gain matrix is $K = B H^\\top S^{-1}$. The analysis state is then expressed elegantly as:\n$$\nx_a = x_b + K d\n$$\nThis is the equation we will use for computation.\n\n**2. Statistical Diagnostics and Their Underlying Conditions**\n\nThe diagnostics $z_d$ and $z_a$ are used to check the consistency of the assumed error statistics ($B$ and $R$) with the observed data. Their statistical distributions rely on specific assumptions about the nature of the errors.\n\nLet the unknown true state be $x_t$. The background state $x_b$ and observations $y$ are modeled as random variables related to $x_t$ by:\n$$\nx_b = x_t + \\epsilon_b, \\quad \\text{where } \\epsilon_b \\sim \\mathcal{N}(0, B)\n$$\n$$\ny = H x_t + \\epsilon_o, \\quad \\text{where } \\epsilon_o \\sim \\mathcal{N}(0, R)\n$$\nHere, $\\epsilon_b$ is the background error and $\\epsilon_o$ is the observation error.\n\n**Innovation Diagnostic ($z_d$)**\n\nThe innovation vector is $d = y - H x_b$. Substituting the error models:\n$$\nd = (H x_t + \\epsilon_o) - H(x_t + \\epsilon_b) = \\epsilon_o - H \\epsilon_b\n$$\nThe distribution of $d$ is derived from the distributions of $\\epsilon_o$ and $\\epsilon_b$.\nThe mean of $d$ is $\\mathbb{E}[d] = \\mathbb{E}[\\epsilon_o] - H\\mathbb{E}[\\epsilon_b] = 0$.\nThe covariance of $d$, assuming the background and observation errors are uncorrelated ($\\mathbb{E}[\\epsilon_b \\epsilon_o^\\top] = 0$), is:\n$$\n\\operatorname{Cov}(d) = \\mathbb{E}[dd^\\top] = \\mathbb{E}[(\\epsilon_o - H \\epsilon_b)(\\epsilon_o - H \\epsilon_b)^\\top] = \\mathbb{E}[\\epsilon_o \\epsilon_o^\\top] + H \\mathbb{E}[\\epsilon_b \\epsilon_b^\\top] H^\\top = R + H B H^\\top = S\n$$\nThus, the innovation vector $d$ is a Gaussian random variable with zero mean and covariance $S$. The statistic $z_d = d^\\top S^{-1} d$ is a quadratic form of this Gaussian vector. If $d \\sim \\mathcal{N}(0, S)$, then the whitened vector $S^{-1/2} d \\sim \\mathcal{N}(0, I_m)$, where $I_m$ is the $m \\times m$ identity matrix. The statistic $z_d$ can be written as $(S^{-1/2} d)^\\top (S^{-1/2} d)$, which is the sum of squares of $m$ independent standard normal random variables.\n\nTherefore, the **precise conditions** under which $z_d$ follows a chi-square distribution with $m$ degrees of freedom ($z_d \\sim \\chi^2_m$) are:\n1.  The background error $\\epsilon_b$ and observation error $\\epsilon_o$ are drawn from Gaussian distributions with zero mean (i.e., $x_b$ and $y$ are unbiased).\n2.  The background and observation errors are mutually uncorrelated.\n3.  The covariance matrices $B$ and $R$ used to compute $S$ are the true covariance matrices of the respective errors.\n\n**Analysis Residual Diagnostic ($z_a$)**\n\nThe analysis residual is $r = y - H x_a$. Substituting the gain-form of $x_a$:\n$$\nr = y - H(x_b + K d) = (y - H x_b) - H K d = d - A d = (I_m - A)d\n$$\nwhere $A = H K = H B H^\\top S^{-1}$ is the observation-space influence matrix.\nOne can show that $I_m - A = I_m - HBH^\\top S^{-1} = (S - HBH^\\top)S^{-1} = R S^{-1}$. Thus, $r = R S^{-1} d$.\nThe statistic is $z_a = r^\\top R^{-1} r$. Substituting the expression for $r$:\n$$\nz_a = (R S^{-1} d)^\\top R^{-1} (R S^{-1} d) = d^\\top (S^{-1})^\\top R^\\top R^{-1} R S^{-1} d = d^\\top S^{-1} R S^{-1} d\n$$\nThe expected value of $z_a$ is $\\mathbb{E}[z_a] = \\mathbb{E}[\\operatorname{trace}(z_a)] = \\operatorname{trace}(\\mathbb{E}[d^\\top S^{-1} R S^{-1} d]) = \\operatorname{trace}(S^{-1} R S^{-1} \\mathbb{E}[dd^\\top]) = \\operatorname{trace}(S^{-1} R S^{-1} S) = \\operatorname{trace}(S^{-1} R)$.\nWe can relate this to $p = \\operatorname{trace}(A)$:\n$$\np = \\operatorname{trace}(H B H^\\top S^{-1}) = \\operatorname{trace}((S-R)S^{-1}) = \\operatorname{trace}(I_m - R S^{-1}) = m - \\operatorname{trace}(R S^{-1})\n$$\nTherefore, $\\mathbb{E}[z_a] = \\operatorname{trace}(R S^{-1}) = m - p$. This result holds under the same conditions as for the innovation diagnostic.\n\nThe distribution of $z_a$, however, is more complex. Let $v = S^{-1/2} d \\sim \\mathcal{N}(0, I_m)$. Then $z_a$ can be written as a quadratic form in $v$:\n$$\nz_a = v^\\top (S^{1/2} S^{-1} R S^{-1} S^{1/2}) v = v^\\top (S^{-1/2} R S^{-1/2}) v\n$$\nFor $z_a$ to follow a chi-square distribution, the matrix $Q = S^{-1/2} R S^{-1/2}$ must be an idempotent projection matrix ($Q^2=Q$). The degrees of freedom would be its rank, which equals its trace. $\\operatorname{trace}(Q) = \\operatorname{trace}(S^{-1}R) = m-p$.\nThe condition $Q^2=Q$ is equivalent to $R S^{-1} R = R$. Since $R$ is invertible, this implies $R S^{-1} = I_m$, which means $R=S$. This would require $H B H^\\top = 0$, a trivial condition meaning that either there is no background error ($B=0$) or the state is not observed ($H=0$).\n\nTherefore, the **precise condition** under which $z_a$ follows a chi-square distribution with $m-p$ degrees of freedom ($z_a \\sim \\chi^2_{m-p}$) is that the matrix $R S^{-1}$ must be an idempotent matrix of rank $m-p$. This condition is met only in trivial, non-physical scenarios (e.g., $HBH^\\top=0$). In general, $z_a$ follows a generalized chi-square distribution (a weighted sum of $\\chi^2_1$ variables), but its expectation remains $m-p$.\n\n**3. Computational Procedure**\n\nFor each test case, we will execute the following steps:\n1.  Initialize the matrices $H, B, R$ and vectors $x_b, y$. Define $n$ and $m$.\n2.  Compute the innovation: $d = y - H x_b$.\n3.  Compute the innovation covariance: $S = H B H^\\top + R$.\n4.  Compute the innovation chi-square: $z_d = d^\\top S^{-1} d$.\n5.  Compute the analysis state: $x_a = x_b + B H^\\top S^{-1} d$.\n6.  Compute the analysis residual: $r = y - H x_a$.\n7.  Compute the residual chi-square: $z_a = r^\\top R^{-1} r$.\n8.  Compute the observation influence matrix: $A = H B H^\\top S^{-1}$.\n9.  Compute its trace: $p = \\operatorname{trace}(A)$.\n10. The output for each case is the tuple $[z_d, z_a, m, p, m-p]$.\n\nThis procedure is implemented below for the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the variational data assimilation problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"n\": 2, \"m\": 3,\n            \"H\": np.array([[1, 0], [0, 1], [1, 1]]),\n            \"B\": np.array([[1.0, 0.3], [0.3, 1.5]]),\n            \"R\": np.array([[0.7, 0.1, 0.0], [0.1, 0.6, 0.05], [0.0, 0.05, 0.9]]),\n            \"x_b\": np.array([0.2, -0.1]),\n            \"y\": np.array([0.9, -0.2, 0.4])\n        },\n        {\n            \"n\": 2, \"m\": 3,\n            \"H\": np.array([[1, 0], [0, 1], [1, 1]]),\n            \"B\": np.array([[1.0, 0.3], [0.3, 1.5]]),\n            \"R\": np.diag([0.05, 0.05, 0.05]),\n            \"x_b\": np.array([1.0, 1.0]),\n            \"y\": np.array([0.0, 0.0, 0.0])\n        },\n        {\n            \"n\": 2, \"m\": 2,\n            \"H\": np.eye(2),\n            \"B\": np.array([[0.8, 0.2], [0.2, 0.8]]),\n            \"R\": np.array([[5.0, 0.5], [0.5, 4.0]]),\n            \"x_b\": np.array([-0.5, 0.3]),\n            \"y\": np.array([0.1, 0.2])\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        H = params[\"H\"]\n        B = params[\"B\"]\n        R = params[\"R\"]\n        x_b = params[\"x_b\"]\n        y = params[\"y\"]\n        m = float(params[\"m\"])\n\n        # Compute innovation (observation-minus-background)\n        # d = y - H @ x_b\n        d = y - H @ x_b\n\n        # Compute innovation error covariance\n        # S = H @ B @ H.T + R\n        S = H @ B @ H.T + R\n        \n        # Compute inverse of S\n        S_inv = np.linalg.inv(S)\n\n        # Compute innovation chi-square diagnostic\n        # z_d = d.T @ S_inv @ d\n        z_d = d.T @ S_inv @ d\n\n        # Compute the Kalman gain matrix K\n        # K = B @ H.T @ S_inv\n        K = B @ H.T @ S_inv\n        \n        # Solve for the analysis state x_a\n        # x_a = x_b + K @ d\n        x_a = x_b + K @ d\n\n        # Compute analysis residual (observation-minus-analysis)\n        # r = y - H @ x_a\n        r = y - H @ x_a\n\n        # Compute inverse of R\n        R_inv = np.linalg.inv(R)\n\n        # Compute the residual chi-square diagnostic\n        # z_a = r.T @ R_inv @ r\n        z_a = r.T @ R_inv @ r\n        \n        # Compute the observation-space influence matrix (hat matrix) A\n        # A = H @ K\n        A = H @ K\n\n        # Compute p, the trace of A (degrees of freedom for signal)\n        p = np.trace(A)\n\n        # Baseline expectation for z_a is m - p\n        expected_za = m - p\n        \n        # Store results for this case\n        results.append([z_d, z_a, m, p, expected_za])\n\n    # Final print statement in the exact required format.\n    # The output format is a list of lists.\n    print(f\"[{','.join(str(res) for res in results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Real-world geophysical models are imperfect and often nonlinear, posing significant challenges for data assimilation. This advanced practice moves beyond idealized scenarios by implementing a weak-constraint 4D-Var system, which accounts for model deficiencies by explicitly solving for a model error term $w_k$ as part of the control vector. You will apply this framework to a nonlinear flow model and use the robust Levenberg-Marquardt algorithm to solve the resulting nonlinear least-squares problem, gaining essential skills for building operational-grade systems .",
            "id": "3618507",
            "problem": "You are tasked with implementing a weak-constraint four-dimensional variational data assimilation method with Levenberg–Marquardt regularization for a simplified porous media flow model that includes nonlinear relative permeability. The goal is to estimate the initial state and a sequence of additive model-error terms over a short assimilation window by minimizing a sum-of-squares cost function. All quantities in the model are nondimensional.\n\nThe forward model evolves a two-component state $x_k = [p_k, s_k]^\\top$, where $p_k$ is pressure and $s_k$ is water saturation at discrete time index $k$, according to a nonlinear discrete-time update with additive model error,\n$$\nx_{k+1} = \\mathcal{M}(x_k) + w_k,\n$$\nwhere $w_k = [w^p_k, w^s_k]^\\top$ is the model error at time step $k$. The model $\\mathcal{M}$ is defined by Darcy-flow-inspired lumped dynamics with Corey-type relative permeability curves:\n$$\n\\begin{aligned}\n\\mathrm{kr}_w(s) &= s^2, \\\\\n\\mathrm{kr}_o(s) &= (1 - s)^2, \\\\\n\\mu_w &= 1,\\quad \\mu_o = 5, \\\\\nm_w(s) &= \\frac{\\mathrm{kr}_w(s)}{\\mu_w},\\quad m_o(s) = \\frac{\\mathrm{kr}_o(s)}{\\mu_o}, \\\\\nm_{\\mathrm{tot}}(s) &= m_w(s) + m_o(s), \\\\\nf_w(s) &= \\frac{m_w(s)}{m_{\\mathrm{tot}}(s)}, \\\\\np_b &= 1.\n\\end{aligned}\n$$\nThe time-discrete update with time step $\\Delta t$ and constants $\\alpha$ and $\\beta$ is\n$$\n\\begin{aligned}\np_{k+1} &= p_k + \\Delta t\\, \\alpha\\, m_{\\mathrm{tot}}(s_k)\\, (p_b - p_k) + w^p_k, \\\\\ns_{k+1} &= s_k + \\Delta t\\, \\beta\\, f_w(s_k)\\, (p_b - p_k) + w^s_k.\n\\end{aligned}\n$$\n\nThe observation operator is the identity, so observations are\n$$\ny_k = H x_k + \\varepsilon_k,\\quad H = I_2,\n$$\nwhere $\\varepsilon_k$ denotes observation noise. In weak-constraint four-dimensional variational assimilation, the cost function to be minimized over the control vector $z = [x_0, w_0, w_1, \\ldots, w_{N-1}]$ is\n$$\nJ(z) = \\frac{1}{2} \\lVert B^{-1/2}(x_0 - x_b) \\rVert^2 + \\frac{1}{2} \\sum_{k=0}^{N-1} \\lVert Q^{-1/2} w_k \\rVert^2 + \\frac{1}{2} \\sum_{k=0}^{N} \\lVert R^{-1/2}(H x_k - y_k) \\rVert^2,\n$$\nwhere $x_b$ is the prior (background) estimate of $x_0$, $B$ is the background error covariance, $Q$ is the model error covariance, and $R$ is the observation error covariance. The notation $A^{-1/2}$ denotes a matrix square root of $A^{-1}$ (for diagonal matrices, this is the component-wise reciprocal of the standard deviations).\n\nYou must implement a Gauss–Newton method with Levenberg–Marquardt regularization to minimize $J(z)$, using the predicted versus actual reduction to adapt the damping parameter. Specifically, at an iteration with current control $z$, residual vector $r(z)$ formed by stacking all weighted residuals, and Jacobian $J_r(z)$ of $r(z)$ with respect to $z$, compute the step $\\delta$ by solving\n$$\n\\left(J_r(z)^\\top J_r(z) + \\lambda I\\right)\\, \\delta = - J_r(z)^\\top r(z),\n$$\nfor a damping parameter $\\lambda > 0$. The predicted reduction is\n$$\n\\mathrm{PR} = \\frac{1}{2} \\left(\\lVert r(z) \\rVert^2 - \\lVert r(z) + J_r(z)\\, \\delta \\rVert^2\\right),\n$$\nand the actual reduction is\n$$\n\\mathrm{AR} = J(z) - J(z + \\delta) = \\frac{1}{2}\\left(\\lVert r(z) \\rVert^2 - \\lVert r(z + \\delta) \\rVert^2\\right).\n$$\nUse the ratio $\\rho = \\mathrm{AR} / \\mathrm{PR}$ to tune $\\lambda$ adaptively: increase $\\lambda$ if $\\rho$ is small, decrease $\\lambda$ if $\\rho$ is large, and accept or reject the trial step accordingly. Convergence can be declared when the step norm is sufficiently small or when the reduction in $J$ is negligible. All computations must be performed in nondimensional units.\n\nThe Jacobian $J_r(z)$ must be derived from first principles using the chain rule and the tangent linear model. Let $A_k = \\frac{\\partial \\mathcal{M}}{\\partial x}\\big|_{x_k}$ denote the $2 \\times 2$ Jacobian of the model at time $k$. Then the tangent sensitivities obey\n$$\n\\begin{aligned}\n\\frac{\\partial x_{k+1}}{\\partial x_0} &= A_k\\, \\frac{\\partial x_k}{\\partial x_0}, \\quad \\frac{\\partial x_0}{\\partial x_0} = I_2, \\\\\n\\frac{\\partial x_{k+1}}{\\partial w_j} &= \n\\begin{cases}\nA_k\\, \\frac{\\partial x_k}{\\partial w_j}, & j < k, \\\\\nI_2, & j = k, \\\\\n0, & j > k,\n\\end{cases}\n\\end{aligned}\n$$\nwhich propagate forward in time. The observation residual for time $k$ is $R^{-1/2}(x_k - y_k)$, whose Jacobian with respect to $z$ uses the sensitivities above. The background residual is $B^{-1/2}(x_0 - x_b)$, whose Jacobian is $B^{-1/2}$ with respect to $x_0$, and the model-error residuals are $Q^{-1/2} w_k$, whose Jacobians are $Q^{-1/2}$ with respect to the corresponding $w_k$.\n\nYou must implement the algorithm and apply it to the following three test cases, each with $N = 5$ and $\\Delta t = 0.2$, $\\alpha = 0.8$, $\\beta = 0.5$, $\\mu_w = 1$, $\\mu_o = 5$, $p_b = 1$. The observation noise is deterministic and identical at all times: $\\varepsilon_k = [0.02, -0.01]^\\top$ for $k = 0,1,2,3,4,5$.\n\nFor each test case, the truth used to generate observations is defined by a true initial state and true model errors. The prior, covariances, and true values are:\n\n- Test Case 1 (happy path):\n  - Prior $x_b = [0.4, 0.4]^\\top$.\n  - Background standard deviations: $\\sigma_{B,p} = 0.3$, $\\sigma_{B,s} = 0.3$, so $B = \\mathrm{diag}([\\sigma_{B,p}^2, \\sigma_{B,s}^2])$.\n  - Model-error standard deviations: $\\sigma_{Q,p} = 0.05$, $\\sigma_{Q,s} = 0.05$, so $Q = \\mathrm{diag}([\\sigma_{Q,p}^2, \\sigma_{Q,s}^2])$.\n  - Observation standard deviations: $\\sigma_{R,p} = 0.05$, $\\sigma_{R,s} = 0.05$, so $R = \\mathrm{diag}([\\sigma_{R,p}^2, \\sigma_{R,s}^2])$.\n  - True initial state $x_0^{\\mathrm{true}} = [0.2, 0.6]^\\top$.\n  - True model errors at each step $w_k^{\\mathrm{true}} = [0.01(-1)^k, -0.005]^\\top$ for $k = 0,1,2,3,4$.\n\n- Test Case 2 (boundary saturation):\n  - Prior $x_b = [0.5, 0.1]^\\top$.\n  - Background standard deviations: $\\sigma_{B,p} = 0.25$, $\\sigma_{B,s} = 0.2$, so $B = \\mathrm{diag}([\\sigma_{B,p}^2, \\sigma_{B,s}^2])$.\n  - Model-error standard deviations: $\\sigma_{Q,p} = 0.03$, $\\sigma_{Q,s} = 0.03$, so $Q = \\mathrm{diag}([\\sigma_{Q,p}^2, \\sigma_{Q,s}^2])$.\n  - Observation standard deviations: $\\sigma_{R,p} = 0.06$, $\\sigma_{R,s} = 0.08$, so $R = \\mathrm{diag}([\\sigma_{R,p}^2, \\sigma_{R,s}^2])$.\n  - True initial state $x_0^{\\mathrm{true}} = [0.3, 0.05]^\\top$.\n  - True model errors at each step $w_k^{\\mathrm{true}} = [0.008, -0.004(-1)^k]^\\top$ for $k = 0,1,2,3,4$.\n\n- Test Case 3 (weak observations):\n  - Prior $x_b = [0.25, 0.7]^\\top$.\n  - Background standard deviations: $\\sigma_{B,p} = 0.2$, $\\sigma_{B,s} = 0.2$, so $B = \\mathrm{diag}([\\sigma_{B,p}^2, \\sigma_{B,s}^2])$.\n  - Model-error standard deviations: $\\sigma_{Q,p} = 0.04$, $\\sigma_{Q,s} = 0.04$, so $Q = \\mathrm{diag}([\\sigma_{Q,p}^2, \\sigma_{Q,s}^2])$.\n  - Observation standard deviations: $\\sigma_{R,p} = 0.5$, $\\sigma_{R,s} = 0.5$, so $R = \\mathrm{diag}([\\sigma_{R,p}^2, \\sigma_{R,s}^2])$.\n  - True initial state $x_0^{\\mathrm{true}} = [0.25, 0.7]^\\top$.\n  - True model errors at each step $w_k^{\\mathrm{true}} = [0.005, 0.0]^\\top$ for $k = 0,1,2,3,4$.\n\nFor each test case, generate the synthetic observation sequence by propagating the true state using the forward model with the true model errors, and then adding the deterministic observation noise $\\varepsilon_k$ at each time.\n\nImplement the Levenberg–Marquardt weak-constraint four-dimensional variational assimilation to estimate $x_0$ and the sequence $\\{w_k\\}_{k=0}^{N-1}$. Use an initial guess $x_0 = x_b$ and $w_k = [0, 0]^\\top$ for all $k$, and choose an initial damping parameter $\\lambda$ based on the scale of $J_r(z)^\\top J_r(z)$ (for example, proportional to its largest diagonal element). The final required outputs for the three test cases are the estimated initial states $[p_0^{\\mathrm{est}}, s_0^{\\mathrm{est}}]$ rounded to six decimal places.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list corresponding to a test case in order. For example, in the correct format: \"[[p1,s1],[p2,s2],[p3,s3]]\". All quantities are nondimensional, so no physical units are required. Angles do not appear in this problem, and there is no use of percentages.",
            "solution": "The user has provided a well-defined problem in computational geophysics, specifically in the domain of variational data assimilation. The task is to implement a weak-constraint 4D-Var algorithm using a Levenberg-Marquardt (LM) optimization scheme to estimate the initial state and model errors for a simplified two-phase porous media flow model.\n\n### Step 1: Extract Givens\n\n- **State Vector**: $x_k = [p_k, s_k]^\\top$ at discrete time $k$, where $p$ is pressure and $s$ is saturation.\n- **Forward Model**: $x_{k+1} = \\mathcal{M}(x_k) + w_k$, where $w_k$ is the additive model error.\n- **Model Operator $\\mathcal{M}(x)$**:\n    - Relative permeabilities: $\\mathrm{kr}_w(s) = s^2$, $\\mathrm{kr}_o(s) = (1 - s)^2$.\n    - Viscosities: $\\mu_w = 1$, $\\mu_o = 5$.\n    - Mobilities: $m_w(s) = \\mathrm{kr}_w(s)/\\mu_w$, $m_o(s) = \\mathrm{kr}_o(s)/\\mu_o$.\n    - Total mobility: $m_{\\mathrm{tot}}(s) = m_w(s) + m_o(s)$.\n    - Water fractional flow: $f_w(s) = m_w(s) / m_{\\mathrm{tot}}(s)$.\n    - Boundary pressure: $p_b = 1$.\n    - Discrete update equations with time step $\\Delta t$ and constants $\\alpha, \\beta$:\n        $p_{k+1} = p_k + \\Delta t\\, \\alpha\\, m_{\\mathrm{tot}}(s_k)\\, (p_b - p_k) + w^p_k$\n        $s_{k+1} = s_k + \\Delta t\\, \\beta\\, f_w(s_k)\\, (p_b - p_k) + w^s_k$\n- **Observation Model**: $y_k = H x_k + \\varepsilon_k$ with observation operator $H = I_2$.\n- **Cost Function**:\n    - Control vector: $z = [x_0, w_0, w_1, \\ldots, w_{N-1}]$.\n    - Cost: $J(z) = \\frac{1}{2} \\lVert B^{-1/2}(x_0 - x_b) \\rVert^2 + \\frac{1}{2} \\sum_{k=0}^{N-1} \\lVert Q^{-1/2} w_k \\rVert^2 + \\frac{1}{2} \\sum_{k=0}^{N} \\lVert R^{-1/2}(H x_k - y_k) \\rVert^2$.\n    - $x_b$: prior estimate of $x_0$.\n    - $B, Q, R$: background, model, and observation error covariance matrices, respectively. They are diagonal.\n- **Optimization Method**: Gauss-Newton with Levenberg-Marquardt regularization.\n    - Step equation: $\\left(J_r(z)^\\top J_r(z) + \\lambda I\\right)\\, \\delta = - J_r(z)^\\top r(z)$, where $r(z)$ is the stacked vector of weighted residuals.\n    - Adaptive $\\lambda$ based on the ratio $\\rho = \\mathrm{AR} / \\mathrm{PR}$.\n- **Jacobian Propagation**: The problem specifies the tangent linear model for propagating sensitivities of the state with respect to the control variables.\n- **Problem Parameters**: Assimilation window steps $N = 5$, time step $\\Delta t = 0.2$, model constants $\\alpha = 0.8, \\beta = 0.5$.\n- **Observation Noise**: $\\varepsilon_k = [0.02, -0.01]^\\top$ for $k = 0, \\ldots, 5$.\n- **Test Cases**: Three cases are provided, each with specific values for $x_b$, covariance matrices (via standard deviations $\\sigma$), true initial state $x_0^{\\mathrm{true}}$, and true model errors $w_k^{\\mathrm{true}}$.\n- **Initial Guess**: Start optimization with $x_0 = x_b$ and $w_k = [0, 0]^\\top$.\n- **Required Output**: For each test case, the estimated initial state $[p_0^{\\mathrm{est}}, s_0^{\\mathrm{est}}]$ rounded to six decimal places, formatted as a list of lists.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is well-grounded. The model is a simplification of established physics (two-phase Darcy flow). 4D-Var is a cornerstone technique in data assimilation for geosciences. The Levenberg-Marquardt algorithm is a standard, robust method for nonlinear least-squares optimization. All components are scientifically sound.\n- **Well-Posed**: The problem is well-posed. All required data, initial conditions, constants, and governing equations are provided. The use of a cost function with background and model error regularization terms ensures the optimization problem is not underdetermined. The LM algorithm is designed to produce stable steps even if the problem is ill-conditioned. A unique and stable solution is expected.\n- **Objective**: The problem statement is entirely objective, quantitative, and free of ambiguity or subjective claims.\n\nThe problem does not exhibit any of the invalidity flaws. It is a complete, consistent, and formalizable scientific computing task.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be developed and implemented.\n\n---\n\n### Algorithmic Design and Principles\n\nThe core of the solution lies in implementing the Levenberg-Marquardt algorithm to minimize the 4D-Var cost function $J(z)$. The algorithm iteratively refines an estimate of the control vector $z = [x_0, w_0, \\ldots, w_{N-1}]$. Each iteration involves linearizing the problem around the current estimate, solving for a step, and then updating the estimate.\n\n**1. Control and State Variables**\nThe control vector $z$ has dimension $2(N+1)$. For $N=5$, its size is $12$. It concatenates the initial state $x_0$ and the model errors $w_k$ for $k=0, \\ldots, 4$. The state vectors $x_k$ for $k > 0$ are dependent variables, determined by $z$ through the forward model: $x_{k+1} = \\mathcal{M}(x_k) + w_k$.\n\n**2. Residual Vector and Cost Function**\nThe cost function $J(z) = \\frac{1}{2}\\lVert r(z) \\rVert^2$ is a sum of squared, weighted residuals. The total residual vector $r(z)$ is formed by stacking the following components:\n- Background residual: $r_b = B^{-1/2}(x_0 - x_b)$ (size $2 \\times 1$).\n- Model error residuals: $r_{q,k} = Q^{-1/2}w_k$ for $k=0, \\ldots, N-1$ (total size $2N \\times 1$).\n- Observation residuals: $r_{o,k} = R^{-1/2}(x_k - y_k)$ for $k=0, \\ldots, N$ (total size $2(N+1) \\times 1$).\nFor $N=5$, the total size of $r(z)$ is $2 + 10 + 12 = 24$.\nThe weighting matrices ($B^{-1/2}$, $Q^{-1/2}$, $R^{-1/2}$) are diagonal matrices whose entries are the reciprocals of the corresponding error standard deviations.\n\n**3. Jacobian of the Residuals ($J_r$)**\nThe LM algorithm requires the Jacobian of the residual vector with respect to the control vector, $J_r = \\partial r / \\partial z$. This matrix has dimensions $(4N+4) \\times (2N+2)$, which is $24 \\times 12$ for $N=5$. We derive its components using the chain rule.\n\nFirst, we need the Jacobian of the nonlinear model operator, $A_k = \\frac{\\partial \\mathcal{M}}{\\partial x}|_{x_k}$. Given $\\mathcal{M}(x) = [\\mathcal{M}_p(p,s), \\mathcal{M}_s(p,s)]^\\top$, its Jacobian is a $2 \\times 2$ matrix:\n$$\nA_k = \\begin{pmatrix}\n\\frac{\\partial \\mathcal{M}_p}{\\partial p} & \\frac{\\partial \\mathcal{M}_p}{\\partial s} \\\\\n\\frac{\\partial \\mathcal{M}_s}{\\partial p} & \\frac{\\partial \\mathcal{M}_s}{\\partial s}\n\\end{pmatrix}_{x=x_k}\n$$\nThe partial derivatives are:\n- $\\frac{\\partial \\mathcal{M}_p}{\\partial p} = 1 - \\Delta t\\, \\alpha\\, m_{\\mathrm{tot}}(s)$\n- $\\frac{\\partial \\mathcal{M}_p}{\\partial s} = \\Delta t\\, \\alpha\\, (p_b - p) \\frac{d m_{\\mathrm{tot}}}{ds}$\n- $\\frac{\\partial \\mathcal{M}_s}{\\partial p} = - \\Delta t\\, \\beta\\, f_w(s)$\n- $\\frac{\\partial \\mathcal{M}_s}{\\partial s} = 1 + \\Delta t\\, \\beta\\, (p_b - p) \\frac{d f_w}{ds}$\n\nWith these, we compute the sensitivities of the state $x_k$ with respect to the entire control vector $z$. Let $S_k = \\partial x_k / \\partial z$ be the $2 \\times (2N+2)$ sensitivity matrix. These are propagated forward in time:\n- $S_0 = \\frac{\\partial x_0}{\\partial z} = [I_2, 0, \\ldots, 0]$\n- $S_{k+1} = \\frac{\\partial x_{k+1}}{\\partial z} = A_k \\frac{\\partial x_k}{\\partial z} + \\frac{\\partial w_k}{\\partial z} = A_k S_k + E_k$, where $E_k$ is a matrix that is zero everywhere except for an identity matrix $I_2$ in the columns corresponding to $w_k$.\n\nThe full Jacobian $J_r$ is constructed by stacking the Jacobians of each residual component:\n- $\\frac{\\partial r_b}{\\partial z} = [B^{-1/2}, 0, \\ldots, 0]$\n- $\\frac{\\partial r_{q,k}}{\\partial z}$ has a $Q^{-1/2}$ block in the columns corresponding to $w_k$ and is zero elsewhere.\n- $\\frac{\\partial r_{o,k}}{\\partial z} = R^{-1/2} \\frac{\\partial x_k}{\\partial z} = R^{-1/2} S_k$\n\n**4. Levenberg-Marquardt Iteration**\nStarting with an initial guess $z^{(0)}$, each iteration $i$ performs the following steps:\n1.  **System Construction**: Given the current estimate $z^{(i)}$, compute the state trajectory $\\{x_k\\}$, the residual vector $r(z^{(i)})$, and its Jacobian $J_r(z^{(i)})$. Calculate the approximate Hessian $G = J_r^\\top J_r$ and the gradient of the cost function $\\nabla J = J_r^\\top r$.\n2.  **Solve for Step**: Solve the damped normal equations for the step $\\delta$: $(G + \\lambda I)\\delta = -\\nabla J$. The damping parameter $\\lambda$ balances between a Gauss-Newton step ($\\lambda \\to 0$) and a steepest descent step ($\\lambda \\to \\infty$).\n3.  **Evaluate Step**: Calculate a trial point $z_{\\text{trial}} = z^{(i)} + \\delta$. Compute the actual reduction in cost, $\\mathrm{AR} = J(z^{(i)}) - J(z_{\\text{trial}})$, and the predicted reduction based on the linear model, $\\mathrm{PR} = J(z^{(i)}) - \\frac{1}{2}\\lVert r(z^{(i)}) + J_r \\delta \\rVert^2$.\n4.  **Update**: Compute the ratio $\\rho = \\mathrm{AR} / \\mathrm{PR}$.\n    - If $\\rho$ is sufficiently positive (e.g., $\\rho > 0.1$), the step is accepted: $z^{(i+1)} = z_{\\text{trial}}$. The damping $\\lambda$ may be decreased if $\\rho$ is large, indicating a good linear approximation.\n    - If $\\rho$ is small or negative, the step is rejected: $z^{(i+1)} = z^{(i)}$. The damping $\\lambda$ is increased to make the next step smaller and closer to the steepest descent direction.\n5.  **Termination**: The loop terminates when the norm of the step $\\delta$, the norm of the gradient $\\nabla J$, or the change in cost function value falls below a predefined tolerance, or a maximum number of iterations is reached.\n\nThe final estimated initial state $[p_0^{\\mathrm{est}}, s_0^{\\mathrm{est}}]$ is extracted from the first two components of the converged control vector $z$. The overall process begins by generating synthetic observations for each test case using the provided true states and model errors.",
            "answer": "```python\nimport numpy as np\n\nclass PorousMediaModel:\n    \"\"\"Implements the simplified porous media forward model and its Jacobian.\"\"\"\n    def __init__(self, dt, alpha, beta, mu_w=1.0, mu_o=5.0, pb=1.0):\n        self.dt = dt\n        self.alpha = alpha\n        self.beta = beta\n        self.mu_w = mu_w\n        self.mu_o = mu_o\n        self.pb = pb\n\n    def kr_w(self, s): return s**2\n    def kr_o(self, s): return (1.0 - s)**2\n    def m_w(self, s): return self.kr_w(s) / self.mu_w\n    def m_o(self, s): return self.kr_o(s) / self.mu_o\n    def m_tot(self, s): return self.m_w(s) + self.m_o(s)\n    def f_w(self, s):\n        mt = self.m_tot(s)\n        # Avoid division by zero, although not expected for s in [0,1]\n        return self.m_w(s) / mt if mt > 1e-12 else 0.0\n\n    def d_mtot_ds(self, s):\n        return 2.0 * s / self.mu_w - 2.0 * (1.0 - s) / self.mu_o\n\n    def d_fw_ds(self, s):\n        mt = self.m_tot(s)\n        if mt < 1e-12: return 0.0\n        # Simplified derivative: d/ds (m_w / (m_w + m_o))\n        # -> (m_w' m_o - m_w m_o') / (m_w+m_o)^2\n        # m_w' = 2s/mu_w, m_o' = -2(1-s)/mu_o\n        # -> ( (2s/mu_w)(1-s)^2/mu_o - (s^2/mu_w)(-2(1-s)/mu_o) ) / mt^2\n        # -> ( 2s(1-s)/mu_w/mu_o * ( (1-s) + s ) ) / mt^2\n        return (2.0 * s * (1.0 - s)) / (self.mu_w * self.mu_o * mt**2)\n\n    def M(self, x):\n        \"\"\"Nonlinear model operator M(x_k).\"\"\"\n        p, s = x\n        p_next = p + self.dt * self.alpha * self.m_tot(s) * (self.pb - p)\n        s_next = s + self.dt * self.beta * self.f_w(s) * (self.pb - p)\n        return np.array([p_next, s_next])\n\n    def jacobian_M(self, x):\n        \"\"\"Jacobian of the model operator dM/dx.\"\"\"\n        p, s = x\n        A = np.zeros((2, 2))\n        A[0, 0] = 1.0 - self.dt * self.alpha * self.m_tot(s)\n        A[0, 1] = self.dt * self.alpha * (self.pb - p) * self.d_mtot_ds(s)\n        A[1, 0] = -self.dt * self.beta * self.f_w(s)\n        A[1, 1] = 1.0 + self.dt * self.beta * (self.pb - p) * self.d_fw_ds(s)\n        return A\n\ndef generate_observations(case, model, N):\n    \"\"\"Generates synthetic truth and observations.\"\"\"\n    x_true = np.zeros((N + 1, 2))\n    x_true[0] = case['x0_true']\n    \n    for k in range(N):\n        w_true_k = case['w_true_func'](k)\n        x_true[k+1] = model.M(x_true[k]) + w_true_k\n\n    y_obs = x_true + case['epsilon']\n    return y_obs\n\ndef perform_lm_4dvar(y_obs, case, model, N):\n    \"\"\"Performs weak-constraint 4D-Var using Levenberg-Marquardt.\"\"\"\n    # Control vector z = [x0, w0, ..., w_{N-1}]\n    dim_z = 2 * (N + 1)\n    \n    # Initial guess for z\n    z = np.zeros(dim_z)\n    z[0:2] = case['xb']\n    \n    # Problem parameters\n    xb = case['xb']\n    B_inv_sqrt = np.diag(1.0 / case['sig_B'])\n    Q_inv_sqrt = np.diag(1.0 / case['sig_Q'])\n    R_inv_sqrt = np.diag(1.0 / case['sig_R'])\n\n    # LM algorithm parameters\n    max_iter = 50\n    lambda_val = 1e-3\n    lambda_up_factor = 4\n    lambda_down_factor = 2\n    accept_ratio = 0.1\n    grad_tol = 1e-6\n    step_tol = 1e-8\n\n    for it in range(max_iter):\n        # 1. Compute residuals and Jacobian\n        x0 = z[0:2]\n        w = z[2:].reshape((N, 2))\n        \n        dim_r = 2 + 2 * N + 2 * (N + 1)\n        r = np.zeros(dim_r)\n        J_r = np.zeros((dim_r, dim_z))\n\n        # Background term\n        r[0:2] = B_inv_sqrt @ (x0 - xb)\n        J_r[0:2, 0:2] = B_inv_sqrt\n        \n        # Model error terms\n        for k in range(N):\n            r[2+2*k : 4+2*k] = Q_inv_sqrt @ w[k]\n            J_r[2+2*k : 4+2*k, 2+2*k : 4+2*k] = Q_inv_sqrt\n        \n        # Observation terms (requires forward integration)\n        x_traj = np.zeros((N+1, 2))\n        x_traj[0] = x0\n        \n        # Sensitivities S_k = dx_k/dz\n        S = np.zeros((N + 1, 2, dim_z))\n        S[0, :, 0:2] = np.eye(2)\n        \n        # k=0 observation\n        obs_idx_start = 2 + 2 * N\n        r[obs_idx_start : obs_idx_start+2] = R_inv_sqrt @ (x_traj[0] - y_obs[0])\n        J_r[obs_idx_start : obs_idx_start+2, :] = R_inv_sqrt @ S[0]\n\n        # Propagate state and sensitivities\n        for k in range(N):\n            A_k = model.jacobian_M(x_traj[k])\n            x_traj[k+1] = model.M(x_traj[k]) + w[k]\n            \n            # Sensitivity update: S_{k+1} = A_k S_k + E_k\n            E_k = np.zeros((2, dim_z))\n            E_k[:, 2+2*k:4+2*k] = np.eye(2)\n            S[k+1] = A_k @ S[k] + E_k\n            \n            # k+1 observation\n            r[obs_idx_start+2*(k+1) : obs_idx_start+2*(k+2)] = R_inv_sqrt @ (x_traj[k+1] - y_obs[k+1])\n            J_r[obs_idx_start+2*(k+1) : obs_idx_start+2*(k+2), :] = R_inv_sqrt @ S[k+1]\n        \n        # 2. Solve LM system\n        cost = 0.5 * np.dot(r, r)\n        G = J_r.T @ J_r\n        grad_J = J_r.T @ r\n\n        if it == 0:\n            lambda_val *= np.max(np.diag(G))\n\n        try:\n            delta = np.linalg.solve(G + lambda_val * np.eye(dim_z), -grad_J)\n        except np.linalg.LinAlgError:\n            # If matrix is singular, increase damping and retry\n            lambda_val *= lambda_up_factor\n            continue\n\n        # Check for convergence\n        if np.linalg.norm(grad_J) < grad_tol: break\n        if np.linalg.norm(delta) < step_tol * (np.linalg.norm(z) + step_tol): break\n            \n        # 3. Evaluate step and update\n        z_trial = z + delta\n        r_trial_est = r + J_r @ delta\n        \n        # Predicted reduction\n        pr = 0.5 * (np.dot(r, r) - np.dot(r_trial_est, r_trial_est))\n\n        # To get actual reduction, re_calculate cost with z_trial\n        x0_trial = z_trial[0:2]\n        w_trial = z_trial[2:].reshape((N, 2))\n        x_traj_trial = np.zeros((N+1, 2)); x_traj_trial[0] = x0_trial\n        for k in range(N): x_traj_trial[k+1] = model.M(x_traj_trial[k]) + w_trial[k]\n        \n        cost_trial = 0.5 * np.linalg.norm(B_inv_sqrt @ (x0_trial-xb))**2\n        for k in range(N): cost_trial += 0.5 * np.linalg.norm(Q_inv_sqrt @ w_trial[k])**2\n        for k in range(N+1): cost_trial += 0.5 * np.linalg.norm(R_inv_sqrt @ (x_traj_trial[k] - y_obs[k]))**2\n            \n        ar = cost - cost_trial\n        \n        if pr > 0:\n            rho = ar / pr\n        else: # Avoid division by zero/negative\n            rho = -1.0 # Or some other indicator of poor prediction\n\n        if rho > accept_ratio:\n            z = z_trial\n            # Decrease lambda if linear model is very good\n            if rho > 0.75:\n                lambda_val = max(1e-9, lambda_val / lambda_down_factor)\n        else:\n            lambda_val = min(1e9, lambda_val * lambda_up_factor)\n\n    return z[0:2]\n\ndef solve():\n    \"\"\"Sets up and solves the three test cases.\"\"\"\n    \n    # Common parameters\n    N = 5\n    dt = 0.2\n    alpha = 0.8\n    beta = 0.5\n    epsilon = np.array([0.02, -0.01])\n    \n    model = PorousMediaModel(dt=dt, alpha=alpha, beta=beta)\n    \n    # Test Cases\n    test_cases = [\n        {\n            'xb': np.array([0.4, 0.4]),\n            'sig_B': np.array([0.3, 0.3]),\n            'sig_Q': np.array([0.05, 0.05]),\n            'sig_R': np.array([0.05, 0.05]),\n            'x0_true': np.array([0.2, 0.6]),\n            'w_true_func': lambda k: np.array([0.01 * (-1)**k, -0.005]),\n            'epsilon': epsilon\n        },\n        {\n            'xb': np.array([0.5, 0.1]),\n            'sig_B': np.array([0.25, 0.2]),\n            'sig_Q': np.array([0.03, 0.03]),\n            'sig_R': np.array([0.06, 0.08]),\n            'x0_true': np.array([0.3, 0.05]),\n            'w_true_func': lambda k: np.array([0.008, -0.004 * (-1)**k]),\n            'epsilon': epsilon\n        },\n        {\n            'xb': np.array([0.25, 0.7]),\n            'sig_B': np.array([0.2, 0.2]),\n            'sig_Q': np.array([0.04, 0.04]),\n            'sig_R': np.array([0.5, 0.5]),\n            'x0_true': np.array([0.25, 0.7]),\n            'w_true_func': lambda k: np.array([0.005, 0.0]),\n            'epsilon': epsilon\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        y_obs = generate_observations(case, model, N)\n        estimated_x0 = perform_lm_4dvar(y_obs, case, model, N)\n        results.append(estimated_x0)\n        \n    # Format output string\n    inner_parts = [f\"[{r[0]:.6f},{r[1]:.6f}]\" for r in results]\n    final_output = f\"[{','.join(inner_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}