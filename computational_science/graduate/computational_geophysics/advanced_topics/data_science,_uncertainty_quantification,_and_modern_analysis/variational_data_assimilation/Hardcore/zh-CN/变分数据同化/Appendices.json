{
    "hands_on_practices": [
        {
            "introduction": "变分资料同化依赖于基于梯度的优化算法，而伴随模型是高效计算代价函数梯度的关键。确保伴随模型及其对应的切线性模型的正确性，是开发任何非线性同化系统不可或缺的第一步。本练习  将指导你完成泰勒检验（Taylor test），这是验证切线性和伴随代码对正确性的黄金标准。",
            "id": "3618466",
            "problem": "给定一个非线性观测算子 $H:\\mathbb{R}^n \\to \\mathbb{R}^m$，其定义为\n$$\nH(x) \\;=\\; M\\,\\tanh(Ax)\\;+\\;\\big(s \\odot \\exp(Cx)\\big)\\;+\\;D\\,(x\\odot x),\n$$\n其中 $x\\in \\mathbb{R}^n$，$A\\in \\mathbb{R}^{m\\times n}$，$C\\in \\mathbb{R}^{m\\times n}$，$D\\in \\mathbb{R}^{m\\times n}$，$M\\in \\mathbb{R}^{m\\times m}$，$s\\in \\mathbb{R}^m$，符号 $\\odot$ 表示逐分量（Hadamard）乘法，$\\tanh(\\cdot)$ 和 $\\exp(\\cdot)$ 作用于每个分量。定义标量值代价函数 $J:\\mathbb{R}^n\\to\\mathbb{R}$ 为\n$$\nJ(x)\\;=\\;\\tfrac{1}{2}\\,\\|H(x)-y\\|_2^2,\n$$\n对于所有测试用例，$y\\in\\mathbb{R}^m$ 均等于零向量。\n\n您必须通过 Taylor 余项检验来验证 $H$ 的一个切线性-伴随对，并报告当 $\\epsilon\\to 0$ 时观测到的精度阶。使用以下基本定义。Fréchet 导数 $H'(x)$ 满足\n$$\nH(x+\\epsilon p) \\;=\\; H(x) + \\epsilon\\,H'(x)\\,p \\;+\\; r_H(x,p,\\epsilon),\n$$\n其中，当 $\\epsilon\\to 0$ 时，$\\|r_H(x,p,\\epsilon)\\|_2 = o(\\epsilon)$。$J$ 的梯度由下式给出\n$$\n\\nabla J(x) \\;=\\; H'(x)^\\top\\big(H(x)-y\\big),\n$$\n其中转置 $H'(x)^\\top$ 由欧几里得内积定义。$J$ 沿方向 $p$ 的 Taylor 展开为\n$$\nJ(x+\\epsilon p) \\;=\\; J(x) + \\epsilon\\,\\nabla J(x)^\\top p \\;+\\; r_J(x,p,\\epsilon),\n$$\n其中，当 $\\epsilon\\to 0$ 时，$|r_J(x,p,\\epsilon)| = o(\\epsilon)$。\n\n您的任务是：\n- 对于给定的 $H(x)$，从第一性原理出发，推导显式的切线性算子 $H'(x)$ 及其伴随算子 $H'(x)^\\top$。\n- 实现两种 Taylor 余项检验：\n  1. 观测算子检验\n     $$\n     R_H(\\epsilon)\\;=\\;\\big\\|H(x+\\epsilon p) - H(x) - \\epsilon\\,H'(x)\\,p\\big\\|_2,\n     $$\n  2. 代价函数检验\n     $$\n     R_J(\\epsilon)\\;=\\;\\big|J(x+\\epsilon p) - J(x) - \\epsilon\\,\\nabla J(x)^\\top p\\big|.\n     $$\n- 对每项检验，通过对预设的 $\\epsilon$ 值网格上的 $\\log R(\\epsilon)$ 与 $\\log \\epsilon$ 进行线性最小二乘拟合，以其斜率来估计观测到的精度阶。\n\n所有三角函数和双曲函数的参数都以弧度为单位。所有计算都是无量纲的。\n\n定义 $H$ 的矩阵和向量是关于大小 $m$ 和 $n$ 的确定性函数：\n- 对于 $i\\in\\{0,\\dots,m-1\\}$ 和 $j\\in\\{0,\\dots,n-1\\}$，\n  $$\n  A_{ij} \\;=\\; 0.2\\,\\sin\\!\\Big(\\frac{(i+1)(j+1)}{3}\\Big),\\quad\n  C_{ij} \\;=\\; 0.15\\,\\cos\\!\\Big(\\frac{(i+1)+(j+1)}{2}\\Big),\\quad\n  D_{ij} \\;=\\; \\frac{0.05}{i+j+2}.\n  $$\n- 对于 $i,j\\in\\{0,\\dots,m-1\\}$，\n  $$ M_{ij} \\;=\\; \\begin{cases} 0.1,  i=j, \\\\[4pt] \\dfrac{0.02}{i+j+2},  i\\neq j. \\end{cases} $$\n- 对于 $i\\in\\{0,\\dots,m-1\\}$，\n  $$\n  s_i \\;=\\; 0.25 + 0.05\\,(i+1).\n  $$\n\n对两项检验使用以下 $\\epsilon$ 网格：\n$$\n\\epsilon \\in \\{10^{-1},\\,10^{-2},\\,10^{-3},\\,10^{-4},\\,10^{-5}\\}.\n$$\n\n测试套件。对于以下每个用例，按照规定构造 $A$、$C$、$D$、$M$、$s$，设置 $y=0$，并计算两个观测精度阶（一个用于 $R_H$，一个用于 $R_J$）：\n- 用例 1：$m=3$, $n=4$, $x=[0.3,\\,-0.2,\\,0.1,\\,-0.4]^\\top$, $p=[-0.5,\\,0.7,\\,-0.1,\\,0.2]^\\top$。\n- 用例 2：$m=5$, $n=3$, $x=[0,\\,0,\\,0]^\\top$, $p=[1,\\,1,\\,1]^\\top$。\n- 用例 3：$m=2$, $n=5$, $x=[0.2,\\,-0.1,\\,0.05,\\,0.3,\\,-0.25]^\\top$, $p=x$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，其顺序为\n$$\n[\\text{order}_{H,1},\\;\\text{order}_{J,1},\\;\\text{order}_{H,2},\\;\\text{order}_{J,2},\\;\\text{order}_{H,3},\\;\\text{order}_{J,3}],\n$$\n其中 $\\text{order}_{H,k}$ 是用例 $k$ 中 $R_H$ 的观测阶，$\\text{order}_{J,k}$ 是用例 $k$ 中 $R_J$ 的观测阶。每个条目必须是浮点数。",
            "solution": "我们从 Fréchet 导数和欧几里得伴随的基本定义开始。Fréchet 导数 $H'(x)$ 由极限定义\n$$\n\\lim_{\\epsilon\\to 0}\\frac{\\|H(x+\\epsilon p)-H(x)-\\epsilon H'(x)p\\|_2}{\\epsilon} \\;=\\; 0\n$$\n对于每个方向 $p\\in\\mathbb{R}^n$。欧几里得伴随 $H'(x)^\\top:\\mathbb{R}^m\\to\\mathbb{R}^n$ 定义为\n$$\n\\langle H'(x)p,\\,q\\rangle_{\\mathbb{R}^m} \\;=\\; \\langle p,\\,H'(x)^\\top q\\rangle_{\\mathbb{R}^n}\n$$\n对于所有 $p\\in\\mathbb{R}^n$，$q\\in\\mathbb{R}^m$，其中 $\\langle\\cdot,\\cdot\\rangle$ 表示标准欧几里得内积。根据链式法则和欧几里得内积的对称性，代价函数 $J(x)=\\tfrac{1}{2}\\|H(x)-y\\|_2^2$ 的梯度为 $\\nabla J(x)=H'(x)^\\top(H(x)-y)$。\n\n我们通过对下式逐项应用链式法则和乘积法则来推导 $H'(x)$\n$$\nH(x) \\;=\\; M\\,\\tanh(Ax)\\;+\\;\\big(s \\odot \\exp(Cx)\\big)\\;+\\;D\\,(x\\odot x).\n$$\n\n第一项。令 $u(x)=Ax\\in\\mathbb{R}^m$ 且 $v(u)=\\tanh(u)$ 逐分量作用，因此 $v'(u)=\\operatorname{sech}^2(u)$ 也是逐分量作用。对于方向 $p$，$du=Ap$，因此\n$$\nd\\big[M\\,\\tanh(Ax)\\big] \\;=\\; M\\,\\Big(\\operatorname{sech}^2(Ax)\\odot (Ap)\\Big).\n$$\n等价地，如果我们使用对角算子表示法，这便是 $M\\,\\operatorname{Diag}(\\operatorname{sech}^2(Ax))\\,A\\,p$。\n\n第二项。令 $w(x)=Cx\\in\\mathbb{R}^m$ 且 $z(w)=\\exp(w)$ 逐分量作用，因此 $dz=\\exp(w)\\odot dw$。那么\n$$\nd\\big[s\\odot \\exp(Cx)\\big] \\;=\\; \\big(s\\odot \\exp(Cx)\\big)\\odot (Cp)\\;=\\;\\operatorname{Diag}\\big(s\\odot \\exp(Cx)\\big)\\,C\\,p.\n$$\n\n第三项。映射 $x\\mapsto x\\odot x$ 的导数是逐分量的 $p\\mapsto 2x\\odot p$，所以\n$$\nd\\big[D\\,(x\\odot x)\\big] \\;=\\; D\\,\\big(2x\\odot p\\big)\\;=\\;D\\,\\operatorname{Diag}(2x)\\,p.\n$$\n\n根据微分的线性性质，我们将这些项组合起来得到切线性算子\n$$\nH'(x)\\,p \\;=\\; M\\,\\big(\\operatorname{sech}^2(Ax)\\odot (Ap)\\big)\\;+\\;\\big(s\\odot \\exp(Cx)\\big)\\odot (Cp)\\;+\\;D\\,\\big(2x\\odot p\\big).\n$$\n\n接下来，我们推导伴随算子 $H'(x)^\\top$。对于任意 $q\\in\\mathbb{R}^m$，考虑与第一项的欧几里得内积：\n$$\n\\langle M(\\operatorname{sech}^2(Ax)\\odot (Ap)),\\,q\\rangle \\;=\\; \\langle \\operatorname{sech}^2(Ax)\\odot (Ap),\\,M^\\top q\\rangle \\\\\n=\\; \\langle Ap,\\,\\operatorname{sech}^2(Ax)\\odot (M^\\top q)\\rangle \\;=\\; \\langle p,\\,A^\\top\\big(\\operatorname{sech}^2(Ax)\\odot (M^\\top q)\\big)\\rangle.\n$$\n因此该项对 $H'(x)^\\top q$ 的贡献是 $A^\\top\\big(\\operatorname{sech}^2(Ax)\\odot (M^\\top q)\\big)$。\n\n对于第二项，\n$$\n\\langle \\big(s\\odot \\exp(Cx)\\big)\\odot (Cp),\\,q\\rangle \\;=\\; \\langle Cp,\\,\\big(s\\odot \\exp(Cx)\\big)\\odot q\\rangle \\;=\\; \\langle p,\\,C^\\top\\big(\\big(s\\odot \\exp(Cx)\\big)\\odot q\\big)\\rangle,\n$$\n所以贡献是 $C^\\top\\big(\\big(s\\odot \\exp(Cx)\\big)\\odot q\\big)$。\n\n对于第三项，\n$$\n\\langle D(2x\\odot p),\\,q\\rangle \\;=\\; \\langle 2x\\odot p,\\,D^\\top q\\rangle \\;=\\; \\langle p,\\,\\operatorname{Diag}(2x)\\,(D^\\top q)\\rangle \\;=\\; \\langle p,\\, (2x)\\odot (D^\\top q)\\rangle,\n$$\n所以贡献是 $(2x)\\odot (D^\\top q)$。\n\n将这些项汇总，我们得到\n$$\nH'(x)^\\top q \\;=\\; A^\\top\\big(\\operatorname{sech}^2(Ax)\\odot (M^\\top q)\\big)\\;+\\;C^\\top\\big(\\big(s\\odot \\exp(Cx)\\big)\\odot q\\big)\\;+\\;(2x)\\odot (D^\\top q).\n$$\n\n对于代价函数 $J(x)=\\tfrac{1}{2}\\|H(x)-y\\|_2^2$，其梯度可直接由链式法则得出\n$$\n\\nabla J(x) \\;=\\; H'(x)^\\top\\big(H(x)-y\\big).\n$$\n假设 $H$ 是二次连续可微的，当 $\\epsilon\\to 0$ 时，$H$ 的 Taylor 余项满足 $\\|H(x+\\epsilon p)-H(x)-\\epsilon H'(x)p\\|_2 = \\mathcal{O}(\\epsilon^2)$，类似地，$J$ 的标量 Taylor 余项满足 $|J(x+\\epsilon p)-J(x)-\\epsilon \\nabla J(x)^\\top p| = \\mathcal{O}(\\epsilon^2)$。因此，当切线性和伴随实现正确时，绘制 $\\log R(\\epsilon)$ 相对于 $\\log \\epsilon$ 的图像，对于两项检验都应得到一个渐近斜率为 2 的结果。\n\n算法步骤：\n- 根据 $m$ 和 $n$ 构造指定的 $A$、$C$、$D$、$M$、$s$。\n- 使用矩阵-向量乘法和逐分量运算实现 $H(x)$。\n- 使用推导出的公式实现切线性作用 $p\\mapsto H'(x)p$ 和伴随作用 $q\\mapsto H'(x)^\\top q$。\n- 实现 $J(x)=\\tfrac{1}{2}\\|H(x)\\|_2^2$（$y=0$）及其梯度 $\\nabla J(x)=H'(x)^\\top H(x)$。\n- 对于 $\\epsilon\\in\\{10^{-1},10^{-2},10^{-3},10^{-4},10^{-5}\\}$，计算每个用例的 $R_H(\\epsilon)$ 和 $R_J(\\epsilon)$，然后通过对 $(\\log \\epsilon,\\log R(\\epsilon))$ 进行线性拟合并取其斜率来估计观测阶。\n- 按要求的顺序汇总六个浮点斜率。\n\n数值考量：\n- 非常小的 $\\epsilon$ 会产生由舍入误差主导的余项，从而使斜率变平。对于具有中等尺度的平滑 $H$，使用 $\\{10^{-1},\\dots,10^{-5}\\}$ 范围内的 $\\epsilon$ 可以平衡截断误差和舍入误差。如果任何余项在数值上变为零，则应在拟合中排除这些点；否则，在网格上进行最小二乘拟合，当实现正确时，会得到接近 2 的稳定斜率估计。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_matrices(m, n):\n    # Deterministic construction per the problem statement\n    A = np.zeros((m, n), dtype=float)\n    C = np.zeros((m, n), dtype=float)\n    D = np.zeros((m, n), dtype=float)\n    M = np.zeros((m, m), dtype=float)\n    s = np.zeros(m, dtype=float)\n\n    for i in range(m):\n        for j in range(n):\n            A[i, j] = 0.2 * np.sin(((i + 1) * (j + 1)) / 3.0)\n            C[i, j] = 0.15 * np.cos(((i + 1) + (j + 1)) / 2.0)\n            D[i, j] = 0.05 / (i + j + 2.0)\n    for i in range(m):\n        for j in range(m):\n            if i == j:\n                M[i, j] = 0.1\n            else:\n                M[i, j] = 0.02 / (i + j + 2.0)\n    for i in range(m):\n        s[i] = 0.25 + 0.05 * (i + 1)\n    return A, C, D, M, s\n\ndef H_op(x, A, C, D, M, s):\n    # H(x) = M tanh(Ax) + (s ⊙ exp(Cx)) + D (x ⊙ x)\n    ax = A @ x\n    cx = C @ x\n    term1 = M @ np.tanh(ax)\n    term2 = s * np.exp(cx)\n    term3 = D @ (x * x)\n    return term1 + term2 + term3\n\ndef TL_op(x, p, A, C, D, M, s):\n    # Tangent-linear action H'(x) p\n    ax = A @ x\n    cx = C @ x\n    Ap = A @ p\n    Cp = C @ p\n    sech2 = 1.0 / np.cosh(ax)\n    sech2 = sech2 * sech2\n    term1 = M @ (sech2 * Ap)\n    term2 = (s * np.exp(cx)) * Cp\n    term3 = D @ (2.0 * x * p)\n    return term1 + term2 + term3\n\ndef ADJ_op(x, q, A, C, D, M, s):\n    # Adjoint action H'(x)^T q\n    ax = A @ x\n    cx = C @ x\n    sech2 = 1.0 / np.cosh(ax)\n    sech2 = sech2 * sech2\n    v1 = A.T @ (sech2 * (M.T @ q))\n    v2 = C.T @ ((s * np.exp(cx)) * q)\n    v3 = (2.0 * x) * (D.T @ q)\n    return v1 + v2 + v3\n\ndef J_cost(x, A, C, D, M, s):\n    hx = H_op(x, A, C, D, M, s)\n    return 0.5 * float(hx @ hx)\n\ndef grad_J(x, A, C, D, M, s):\n    hx = H_op(x, A, C, D, M, s)\n    return ADJ_op(x, hx, A, C, D, M, s)\n\ndef observed_order(eps_vals, residuals):\n    eps_vals = np.asarray(eps_vals, dtype=float)\n    residuals = np.asarray(residuals, dtype=float)\n    mask = residuals > 0.0\n    x = np.log(eps_vals[mask])\n    y = np.log(residuals[mask])\n    # If insufficient points, return nan\n    if x.size  2:\n        return float('nan')\n    coeffs = np.polyfit(x, y, 1)\n    slope = coeffs[0]\n    return float(slope)\n\ndef run_case(m, n, x, p, eps_list):\n    A, C, D, M, s = build_matrices(m, n)\n    # Precompute base values\n    Hx = H_op(x, A, C, D, M, s)\n    TLxp = TL_op(x, p, A, C, D, M, s)\n    Jx = 0.5 * float(Hx @ Hx)\n    gx = ADJ_op(x, Hx, A, C, D, M, s)\n\n    RH = []\n    RJ = []\n    for eps in eps_list:\n        x_eps = x + eps * p\n        Hx_eps = H_op(x_eps, A, C, D, M, s)\n        # Remainder for H\n        rem_H = Hx_eps - Hx - eps * TLxp\n        RH.append(float(np.linalg.norm(rem_H)))\n        # Remainder for J\n        Jx_eps = J_cost(x_eps, A, C, D, M, s)\n        rem_J = Jx_eps - Jx - eps * float(gx @ p)\n        RJ.append(abs(float(rem_J)))\n    order_H = observed_order(eps_list, RH)\n    order_J = observed_order(eps_list, RJ)\n    return order_H, order_J\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (m, n, x, p)\n        (3, 4, np.array([0.3, -0.2, 0.1, -0.4], dtype=float),\n             np.array([-0.5, 0.7, -0.1, 0.2], dtype=float)),\n        (5, 3, np.array([0.0, 0.0, 0.0], dtype=float),\n             np.array([1.0, 1.0, 1.0], dtype=float)),\n        (2, 5, np.array([0.2, -0.1, 0.05, 0.3, -0.25], dtype=float),\n             np.array([0.2, -0.1, 0.05, 0.3, -0.25], dtype=float)),\n    ]\n\n    eps_list = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n\n    results = []\n    for (m, n, x, p) in test_cases:\n        order_H, order_J = run_case(m, n, x, p, eps_list)\n        results.append(order_H)\n        results.append(order_J)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "运行同化系统后，评估其性能至关重要，这通常通过检查结果是否与关于误差（背景误差、观测误差）的统计假设相符来实现。本练习  探讨了如何在新息（观测减背景）和分析残差（观测减分析）上应用卡方（$\\chi^2$）诊断。这是一种强有力的工具，用以检验同化系统的统计一致性并发现误差协方差中可能存在的错误设定。",
            "id": "3618559",
            "problem": "你将为一个线性高斯变分资料同化系统设计并实现一个定量诊断方法，然后在一组测试案例上进行计算。设定如下。设模式状态为向量 $x \\in \\mathbb{R}^n$，背景（先验）状态为 $x_b \\in \\mathbb{R}^n$，其背景误差协方差 $B \\in \\mathbb{R}^{n \\times n}$ 为对称正定矩阵，观测为 $y \\in \\mathbb{R}^m$，其观测误差协方差 $R \\in \\mathbb{R}^{m \\times m}$ 为对称正定矩阵。观测算子是线性的，由 $H \\in \\mathbb{R}^{m \\times n}$ 表示。变分分析 $x_a$ 定义为二次代价函数\n$$\nJ(x) = \\tfrac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b) + \\tfrac{1}{2}(y - H x)^\\top R^{-1} (y - H x)\n$$\n的唯一极小值点。\n定义新息（也称为观测减背景）向量\n$$\nd = y - H x_b,\n$$\n和分析残差（也称为观测减分析）\n$$\nr = y - H x_a.\n$$\n你的任务是为 $d$ 和 $r$ 构建一个卡方诊断，并陈述在协方差 $R$ 和 $B$ 正确的情况下，每种诊断遵循已知参考分布的精确条件。然后，实现一个程序，该程序：\n- 通过最小化 $J(x)$ 求解 $x_a$。\n- 计算新息 $d$ 和残差 $r$。\n- 构建新息误差协方差\n$$\nS = H B H^\\top + R,\n$$\n并计算新息卡方诊断\n$$\nz_d = d^\\top S^{-1} d.\n$$\n- 计算由 $R$ 白化的残差卡方诊断，\n$$\nz_a = r^\\top R^{-1} r.\n$$\n- 计算观测空间影响矩阵（也称为观测帽子矩阵）\n$$\nA = H K \\quad \\text{with} \\quad K = B H^\\top S^{-1},\n$$\n并报告标量\n$$\np = \\operatorname{trace}(A),\n$$\n以及基线期望\n$$\n\\mathbb{E}[z_a] = m - p.\n$$\n你必须提供在 $R$ 和 $B$ 正确的假设下，$z_d$ 遵循自由度为 $m$ 的卡方分布以及 $z_a$ 遵循自由度为 $m - p$ 的卡方分布的科学条件。\n\n你的程序必须：\n- 使用线性代数基元，通过求解 $J(x)$ 的一阶最优性条件来实现分析 $x_a$，不依赖任何黑箱优化器或随机抽样。\n- 为每个测试案例计算元组 $[z_d, z_a, m, p, m - p]$ 作为浮点数。\n- 生成单行输出，其中包含所有测试案例结果的列表，该列表用方括号括起，以逗号分隔，其中每个元素是针对一个测试的列表 $[z_d, z_a, m, p, m - p]$。\n\n不需要物理单位，因为诊断是无量纲的。不出现角度。不使用百分比。\n\n使用以下测试套件，其选择旨在探究包含相关误差的一般情况、包含高信息量观测的情况以及相对于先验观测较弱的情况：\n- 测试案例 1（一般情况，相关的 $R$）：$n = 2$, $m = 3$,\n  $$\n  H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix},\\;\n  B = \\begin{bmatrix} 1.0  0.3 \\\\ 0.3  1.5 \\end{bmatrix},\\;\n  R = \\begin{bmatrix} 0.7  0.1  0.0 \\\\ 0.1  0.6  0.05 \\\\ 0.0  0.05  0.9 \\end{bmatrix},\n  $$\n  $$\n  x_b = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix},\\;\n  y = \\begin{bmatrix} 0.9 \\\\ -0.2 \\\\ 0.4 \\end{bmatrix}.\n  $$\n- 测试案例 2（高信息量观测）：$n = 2$, $m = 3$,\n  $$\n  H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix},\\;\n  B = \\begin{bmatrix} 1.0  0.3 \\\\ 0.3  1.5 \\end{bmatrix},\\;\n  R = \\operatorname{diag}(0.05, 0.05, 0.05),\n  $$\n  $$\n  x_b = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix},\\;\n  y = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.\n  $$\n- 测试案例 3（相对于先验的弱观测）：$n = 2$, $m = 2$,\n  $$\n  H = I_2,\\;\n  B = \\begin{bmatrix} 0.8  0.2 \\\\ 0.2  0.8 \\end{bmatrix},\\;\n  R = \\begin{bmatrix} 5.0  0.5 \\\\ 0.5  4.0 \\end{bmatrix},\n  $$\n  $$\n  x_b = \\begin{bmatrix} -0.5 \\\\ 0.3 \\end{bmatrix},\\;\n  y = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}.\n  $$\n\n你的程序应该生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。每个元素是对应一个测试案例的列表 $[z_d, z_a, m, p, m - p]$，顺序与上述测试案例的顺序一致。例如，输出格式必须如下所示：\n$$\n[[z_{d,1}, z_{a,1}, m_1, p_1, m_1 - p_1],[z_{d,2}, z_{a,2}, m_2, p_2, m_2 - p_2],[z_{d,3}, z_{a,3}, m_3, p_3, m_3 - p_3]].\n$$\n\n除了实现计算之外，在你的解决方案中，你必须清楚地说明在 $R$ 和 $B$ 正确的情况下，保证 $z_d$ 和 $z_a$ 的参考分布的数学条件。",
            "solution": "该问题要求为一个线性高斯变分资料同化系统设计和实现一种定量诊断方法。这涉及推导分析状态、定义两种卡方诊断、阐述其有效性的精确条件，并为特定的测试案例计算这些诊断值。\n\n首先，我们验证问题陈述。\n所有给定信息，包括代价函数 $J(x)$、矩阵 $H, B, R$、向量 $x_b, y$，以及 $d, r, S, z_d, z_a, A, K, p$ 的定义都已明确提供。三个测试案例的数值也已给出。矩阵 $B$ 和 $R$ 被定义为对称正定，这是标准设定。检查所提供的矩阵可以确认它们满足此属性。该问题在科学上根植于成熟的统计资料同化理论。它是客观、适定、完全指定且其各组成部分内部一致的。因此，该问题被认定为有效。\n\n我们继续进行求解。\n\n**1. 分析状态的推导**\n\n分析状态 $x_a$ 是使代价函数最小化的向量 $x$：\n$$\nJ(x) = \\tfrac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b) + \\tfrac{1}{2}(y - H x)^\\top R^{-1} (y - H x)\n$$\n由于 $J(x)$ 是一个二次凸函数（其 Hessian 矩阵 $\\nabla^2 J = B^{-1} + H^\\top R^{-1} H$ 是正定的，因为 $B^{-1}$ 是正定的，而 $H^\\top R^{-1} H$ 是半正定的），它有唯一的极小值。我们通过将 $J(x)$ 对 $x$ 的梯度设为零来找到这个极小值：\n$$\n\\nabla_x J(x) = B^{-1} (x - x_b) - H^\\top R^{-1} (y - H x) = 0\n$$\n设 $x = x_a$ 并整理各项得：\n$$\n(B^{-1} + H^\\top R^{-1} H) x_a = B^{-1} x_b + H^\\top R^{-1} y\n$$\n直接求解 $x_a$ 需要对 $n \\times n$ 矩阵 $(B^{-1} + H^\\top R^{-1} H)$ 求逆。一种更常用且通常在计算上更有优势的公式是增益形式的解，特别是当观测数量 $m$ 小于状态维度 $n$ 时。使用 Woodbury 矩阵恒等式，可以证明这等价于：\n$$\nx_a = x_b + B H^\\top (H B H^\\top + R)^{-1} (y - H x_b)\n$$\n使用问题中的定义，新息为 $d = y - H x_b$，新息协方差为 $S = H B H^\\top + R$。最优增益矩阵为 $K = B H^\\top S^{-1}$。分析状态可以优雅地表示为：\n$$\nx_a = x_b + K d\n$$\n这是我们将用于计算的方程。\n\n**2. 统计诊断及其基本条件**\n\n诊断量 $z_d$ 和 $z_a$ 用于检查假设的误差统计量（$B$ 和 $R$）与观测数据的一致性。它们的统计分布依赖于关于误差性质的特定假设。\n\n设未知的真实状态为 $x_t$。背景状态 $x_b$ 和观测 $y$ 被建模为与 $x_t$ 相关的随机变量：\n$$\nx_b = x_t + \\epsilon_b, \\quad \\text{其中 } \\epsilon_b \\sim \\mathcal{N}(0, B)\n$$\n$$\ny = H x_t + \\epsilon_o, \\quad \\text{其中 } \\epsilon_o \\sim \\mathcal{N}(0, R)\n$$\n这里，$\\epsilon_b$ 是背景误差，$\\epsilon_o$ 是观测误差。\n\n**新息诊断 ($z_d$)**\n\n新息向量为 $d = y - H x_b$。代入误差模型：\n$$\nd = (H x_t + \\epsilon_o) - H(x_t + \\epsilon_b) = \\epsilon_o - H \\epsilon_b\n$$\n$d$ 的分布由 $\\epsilon_o$ 和 $\\epsilon_b$ 的分布导出。\n$d$ 的均值为 $\\mathbb{E}[d] = \\mathbb{E}[\\epsilon_o] - H\\mathbb{E}[\\epsilon_b] = 0$。\n$d$ 的协方差，假设背景误差和观测误差不相关（$\\mathbb{E}[\\epsilon_b \\epsilon_o^\\top] = 0$），为：\n$$\n\\operatorname{Cov}(d) = \\mathbb{E}[dd^\\top] = \\mathbb{E}[(\\epsilon_o - H \\epsilon_b)(\\epsilon_o - H \\epsilon_b)^\\top] = \\mathbb{E}[\\epsilon_o \\epsilon_o^\\top] + H \\mathbb{E}[\\epsilon_b \\epsilon_b^\\top] H^\\top = R + H B H^\\top = S\n$$\n因此，新息向量 $d$ 是一个均值为零、协方差为 $S$ 的高斯随机变量。统计量 $z_d = d^\\top S^{-1} d$ 是该高斯向量的二次型。如果 $d \\sim \\mathcal{N}(0, S)$，则白化向量 $S^{-1/2} d \\sim \\mathcal{N}(0, I_m)$，其中 $I_m$ 是 $m \\times m$ 的单位矩阵。统计量 $z_d$ 可以写成 $(S^{-1/2} d)^\\top (S^{-1/2} d)$，这是 $m$ 个独立标准正态随机变量的平方和。\n\n因此，使 $z_d$ 遵循自由度为 $m$ 的卡方分布 ($z_d \\sim \\chi^2_m$) 的**精确条件**是：\n1.  背景误差 $\\epsilon_b$ 和观测误差 $\\epsilon_o$ 来自均值为零的高斯分布（即，$x_b$ 和 $y$ 是无偏的）。\n2.  背景误差和观测误差相互不相关。\n3.  用于计算 $S$ 的协方差矩阵 $B$ 和 $R$ 是相应误差的真实协方差矩阵。\n\n**分析残差诊断 ($z_a$)**\n\n分析残差为 $r = y - H x_a$。代入 $x_a$ 的增益形式：\n$$\nr = y - H(x_b + K d) = (y - H x_b) - H K d = d - A d = (I_m - A)d\n$$\n其中 $A = H K = H B H^\\top S^{-1}$ 是观测空间影响矩阵。\n可以证明 $I_m - A = I_m - HBH^\\top S^{-1} = (S - HBH^\\top)S^{-1} = R S^{-1}$。因此，$r = R S^{-1} d$。\n统计量为 $z_a = r^\\top R^{-1} r$。代入 $r$ 的表达式：\n$$\nz_a = (R S^{-1} d)^\\top R^{-1} (R S^{-1} d) = d^\\top (S^{-1})^\\top R^\\top R^{-1} R S^{-1} d = d^\\top S^{-1} R S^{-1} d\n$$\n$z_a$ 的期望值为 $\\mathbb{E}[z_a] = \\mathbb{E}[\\operatorname{trace}(z_a)] = \\operatorname{trace}(\\mathbb{E}[d^\\top S^{-1} R S^{-1} d]) = \\operatorname{trace}(S^{-1} R S^{-1} \\mathbb{E}[dd^\\top]) = \\operatorname{trace}(S^{-1} R S^{-1} S) = \\operatorname{trace}(S^{-1} R)$。\n我们可以将其与 $p = \\operatorname{trace}(A)$ 联系起来：\n$$\np = \\operatorname{trace}(H B H^\\top S^{-1}) = \\operatorname{trace}((S-R)S^{-1}) = \\operatorname{trace}(I_m - R S^{-1}) = m - \\operatorname{trace}(R S^{-1})\n$$\n因此，$\\mathbb{E}[z_a] = \\operatorname{trace}(R S^{-1}) = m - p$。此结果在与新息诊断相同的条件下成立。\n\n然而，$z_a$ 的分布更为复杂。令 $v = S^{-1/2} d \\sim \\mathcal{N}(0, I_m)$。那么 $z_a$ 可以写成 $v$ 的二次型：\n$$\nz_a = v^\\top (S^{1/2} S^{-1} R S^{-1} S^{1/2}) v = v^\\top (S^{-1/2} R S^{-1/2}) v\n$$\n为了使 $z_a$ 遵循卡方分布，矩阵 $Q = S^{-1/2} R S^{-1/2}$ 必须是幂等投影矩阵 ($Q^2=Q$)。其自由度将是它的秩，等于它的迹。$\\operatorname{trace}(Q) = \\operatorname{trace}(S^{-1}R) = m-p$。\n条件 $Q^2=Q$ 等价于 $R S^{-1} R = R$。由于 $R$ 是可逆的，这意味着 $R S^{-1} = I_m$，即 $R=S$。这将要求 $H B H^\\top = 0$，这是一个平凡的条件，意味着要么没有背景误差 ($B=0$)，要么状态未被观测 ($H=0$)。\n\n因此，使 $z_a$ 遵循自由度为 $m-p$ 的卡方分布 ($z_a \\sim \\chi^2_{m-p}$) 的**精确条件**是矩阵 $R S^{-1}$ 必须是一个秩为 $m-p$ 的幂等矩阵。这个条件只有在平凡的、非物理的场景中（例如，$HBH^\\top=0$）才能满足。在一般情况下，$z_a$ 遵循一个广义卡方分布（$\\chi^2_1$ 变量的加权和），但其期望值仍然是 $m-p$。\n\n**3. 计算流程**\n\n对于每个测试案例，我们将执行以下步骤：\n1.  初始化矩阵 $H, B, R$ 和向量 $x_b, y$。定义 $n$ 和 $m$。\n2.  计算新息：$d = y - H x_b$。\n3.  计算新息协方差：$S = H B H^\\top + R$。\n4.  计算新息卡方值：$z_d = d^\\top S^{-1} d$。\n5.  计算分析状态：$x_a = x_b + B H^\\top S^{-1} d$。\n6.  计算分析残差：$r = y - H x_a$。\n7.  计算残差卡方值：$z_a = r^\\top R^{-1} r$。\n8.  计算观测影响矩阵：$A = H B H^\\top S^{-1}$。\n9.  计算其迹：$p = \\operatorname{trace}(A)$。\n10. 每个案例的输出是元组 $[z_d, z_a, m, p, m-p]$。\n\n下面对所提供的测试案例实施此流程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the variational data assimilation problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"n\": 2, \"m\": 3,\n            \"H\": np.array([[1, 0], [0, 1], [1, 1]]),\n            \"B\": np.array([[1.0, 0.3], [0.3, 1.5]]),\n            \"R\": np.array([[0.7, 0.1, 0.0], [0.1, 0.6, 0.05], [0.0, 0.05, 0.9]]),\n            \"x_b\": np.array([0.2, -0.1]),\n            \"y\": np.array([0.9, -0.2, 0.4])\n        },\n        {\n            \"n\": 2, \"m\": 3,\n            \"H\": np.array([[1, 0], [0, 1], [1, 1]]),\n            \"B\": np.array([[1.0, 0.3], [0.3, 1.5]]),\n            \"R\": np.diag([0.05, 0.05, 0.05]),\n            \"x_b\": np.array([1.0, 1.0]),\n            \"y\": np.array([0.0, 0.0, 0.0])\n        },\n        {\n            \"n\": 2, \"m\": 2,\n            \"H\": np.eye(2),\n            \"B\": np.array([[0.8, 0.2], [0.2, 0.8]]),\n            \"R\": np.array([[5.0, 0.5], [0.5, 4.0]]),\n            \"x_b\": np.array([-0.5, 0.3]),\n            \"y\": np.array([0.1, 0.2])\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        H = params[\"H\"]\n        B = params[\"B\"]\n        R = params[\"R\"]\n        x_b = params[\"x_b\"]\n        y = params[\"y\"]\n        m = float(params[\"m\"])\n\n        # Compute innovation (observation-minus-background)\n        # d = y - H @ x_b\n        d = y - H @ x_b\n\n        # Compute innovation error covariance\n        # S = H @ B @ H.T + R\n        S = H @ B @ H.T + R\n        \n        # Compute inverse of S\n        S_inv = np.linalg.inv(S)\n\n        # Compute innovation chi-square diagnostic\n        # z_d = d.T @ S_inv @ d\n        z_d = d.T @ S_inv @ d\n\n        # Compute the Kalman gain matrix K\n        # K = B @ H.T @ S_inv\n        K = B @ H.T @ S_inv\n        \n        # Solve for the analysis state x_a\n        # x_a = x_b + K @ d\n        x_a = x_b + K @ d\n\n        # Compute analysis residual (observation-minus-analysis)\n        # r = y - H @ x_a\n        r = y - H @ x_a\n\n        # Compute inverse of R\n        R_inv = np.linalg.inv(R)\n\n        # Compute the residual chi-square diagnostic\n        # z_a = r.T @ R_inv @ r\n        z_a = r.T @ R_inv @ r\n        \n        # Compute the observation-space influence matrix (hat matrix) A\n        # A = H @ K\n        A = H @ K\n\n        # Compute p, the trace of A (degrees of freedom for signal)\n        p = np.trace(A)\n\n        # Baseline expectation for z_a is m - p\n        expected_za = m - p\n        \n        # Store results for this case\n        results.append([z_d, z_a, m, p, expected_za])\n\n    # Final print statement in the exact required format.\n    # The output format is a list of lists.\n    print(f\"[{','.join(str(res) for res in results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "实际的资料同化系统总是在有限的计算资源下运行，这迫使系统设计者做出权衡，例如通过“稀疏化”来减少使用的观测数量。本练习  引入了信号自由度（Degrees of Freedom for Signal, DFS）这一概念来量化从观测中提取的信息量。你将通过本练习分析在节约计算成本与因观测稀疏化导致的信息损失之间的基本权衡关系，这是实用系统设计的核心问题之一。",
            "id": "3618450",
            "problem": "给定一个适用于计算地球物理学的线性、高斯、一维变分数据同化设置。状态向量的长度为 $n$，索引在间距为 $\\Delta x = 1$（无量纲）的均匀网格上。线性观测算子 $H$ 选择网格点的一个子集（逐点采样），背景误差协方差 $B$ 是平稳且高斯的，其元素为 $B_{ij} = \\sigma_b^2 \\exp\\left(-\\frac{(|i-j|\\Delta x)^2}{2 L^2}\\right)$。观测误差是不相关的，其协方差为 $R = \\sigma_o^2 I$。考虑一个稀疏化过程，该过程通过从原始有序观测集中（从第一个开始）每隔 $q$ 个观测保留一个来减少观测数量，其中 $q \\in \\mathbb{N}$。\n\n基本基础：\n- 先验（背景）和观测的线性高斯模型：$x \\sim \\mathcal{N}(x_b, B)$，$y = H x + \\varepsilon$，$\\varepsilon \\sim \\mathcal{N}(0, R)$。\n- 变分分析（在此线性高斯设置中等效于最佳线性无偏估计）的特点是通过卡尔曼增益矩阵 $K$ 对观测具有线性敏感性。\n- 信号自由度（DFS）定义为分析敏感度算子相对于观测的迹，即 $\\mathrm{DFS} = \\mathrm{tr}(H K)$，它量化了分析中从观测提取的信息。\n\n任务：\n1. 从上述定义和线性高斯假设出发，推导一个信号自由度（DFS）的可实现线性代数表达式。该表达式应在观测空间中可计算，并且不需要对大小为 $n \\times n$ 的矩阵进行求逆。推导过程应仅使用上面陈述的基本定义和对称正定矩阵的性质；您不得依赖任何直接包含最终表达式的预先给定的简化公式或恒等式。\n2. 为在观测空间中执行的三维变分（3D-Var）分析中的主要线性代数运算提出一个计算成本模型，其中主要成本是对大小为 $m \\times m$（$m$ 等于观测数量）的对称正定矩阵进行分解。使用成本代理 $C(m) = \\frac{1}{3} m^3$（任意单位）。在本练习中，此成本是无量纲的。\n3. 实现一个程序，对于下面的每个测试用例，该程序从高斯核构造 $B$，为给定的步长 $s$（意味着从索引 0 开始每隔 $s$ 个网格点进行一次观测）构造观测索引集，通过从原始观测集中保留每 $q$ 个观测来进行稀疏化，使用您的可实现表达式计算稀疏化前后的 DFS，使用 $C(m)$ 评估前后的计算成本，并报告一个定义为如下的量化权衡指标\n$$\n\\rho \\;=\\; \\frac{\\mathrm{DFS}_{\\text{before}} - \\mathrm{DFS}_{\\text{after}}}{C(m_{\\text{before}}) - C(m_{\\text{after}})}.\n$$\n所有量 $\\mathrm{DFS}_{\\text{before}}$、$\\mathrm{DFS}_{\\text{after}}$ 和 $\\rho$ 都是无量纲的。\n4. 数值规格和实现细节：\n   - 使用 $\\Delta x = 1$（无量纲）。\n   - 完全按照规定，从均匀网格上的高斯核构造 $B$。\n   - 观测是逐点的；观测算子 $H$ 是对观测网格索引的限制。\n   - 稀疏化从原始有序观测索引列表的第一个开始，保留每第 $q$ 个观测。\n   - 程序必须使用数值稳定的线性代数方法，利用您的观测空间表达式来计算 DFS，并尽可能避免显式矩阵求逆。\n   - 最终的数值答案必须四舍五入到恰好 $6$ 位小数。\n\n测试套件：\n每个测试用例提供 $(n, s, q, \\sigma_b, L, \\sigma_o)$，所有量均为无量纲。\n- 案例 $1$：$(n, s, q, \\sigma_b, L, \\sigma_o) = (100, 1, 2, 1.0, 5.0, 0.5)$。\n- 案例 $2$：$(n, s, q, \\sigma_b, L, \\sigma_o) = (100, 1, 3, 1.0, 5.0, 2.0)$。\n- 案例 $3$：$(n, s, q, \\sigma_b, L, \\sigma_o) = (100, 2, 2, 1.0, 20.0, 1.0)$。\n- 案例 $4$：$(n, s, q, \\sigma_b, L, \\sigma_o) = (120, 4, 2, 1.0, 0.5, 0.2)$。\n- 案例 $5$：$(n, s, q, \\sigma_b, L, \\sigma_o) = (60, 1, 5, 1.0, 10.0, 0.7)$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个类 JSON 的列表的列表，每个内部列表对应一个测试用例，按 $[\\mathrm{DFS}_{\\text{before}}, \\mathrm{DFS}_{\\text{after}}, \\rho]$ 的顺序排列。\n- 所有数字必须打印为十进制浮点数，并四舍五入到恰好 $6$ 位小数。\n- 示例格式：$[[d_1,a_1,r_1],[d_2,a_2,r_2],\\dots]$，其中每个 $d_i$、$a_i$ 和 $r_i$ 都是小数点后有 $6$ 位数字的浮点数。\n- 此问题中没有物理单位；所有输出都是无量纲的。",
            "solution": "该问题是有效的，因为它科学地基于线性高斯数据同化的原理，是数学上适定的、客观的和自洽的。所有定义、参数和目标都已明确说明，从而能够得出一个唯一且有意义的解。\n\n该解答分三部分展开：首先，推导信号自由度（DFS）的可计算表达式；其次，建立计算成本模型和权衡指标；第三，针对指定的测试用例进行算法实现。\n\n**1. 在观测空间中推导信号自由度（DFS）**\n\n分析状态 $x_a$ 是使三维变分（3D-Var）代价函数 $J(x)$ 最小化的状态向量 $x$：\n$$\nJ(x) = \\frac{1}{2}(x - x_b)^T B^{-1}(x - x_b) + \\frac{1}{2}(y - Hx)^T R^{-1}(y - Hx)\n$$\n这里，$x_b$ 是背景状态，$B$ 是背景误差协方差矩阵，$y$ 是观测向量，$H$ 是（线性）观测算子，$R$ 是观测误差协方差矩阵。为求最小值，我们将 $J(x)$ 对 $x$ 的梯度设为零：\n$$\n\\nabla_x J(x) = B^{-1}(x - x_b) - H^T R^{-1}(y - Hx) = 0\n$$\n求解分析状态 $x_a$ 可得：\n$$\n(B^{-1} + H^T R^{-1} H) x_a = B^{-1} x_b + H^T R^{-1} y\n$$\n$$\nx_a = (B^{-1} + H^T R^{-1} H)^{-1} (B^{-1} x_b + H^T R^{-1} y)\n$$\n这个表达式给出了分析状态，但涉及到对一个 $n \\times n$ 矩阵的求逆，其中 $n$ 是状态空间的维度。对于大规模地球物理模型，这在计算上是不可行的。\n\n分析的一个等效且更常见的表示是更新形式：\n$$\nx_a = x_b + K(y - Hx_b)\n$$\n其中 $K$ 是卡尔曼增益矩阵。为了找到 $K$ 的表达式，我们可以令 $x_a$ 的两种形式相等。通过将更新形式代入梯度方程，我们发现为了使方程对任何新息 $(y - Hx_b)$ 都成立，增益矩阵必须满足：\n$$\n(B^{-1} + H^T R^{-1} H) K = H^T R^{-1}\n$$\n这给出了状态空间中 $K$ 的表达式：\n$$\nK = (B^{-1} + H^T R^{-1} H)^{-1} H^T R^{-1}\n$$\n为了将其转换为在观测空间中可计算的表达式，我们应用 Sherman-Morrison-Woodbury 矩阵恒等式，该恒等式表述为 $(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}$。令 $A = B^{-1}$，$U = H^T$，$C = R^{-1}$ 和 $V = H$。应用该恒等式可得：\n$$\n(B^{-1} + H^T R^{-1} H)^{-1} = B - B H^T (R + H B H^T)^{-1} H B\n$$\n将此代回 $K$ 的表达式中：\n$$\nK = [B - B H^T (R + H B H^T)^{-1} H B] H^T R^{-1}\n$$\n$$\nK = B H^T R^{-1} - B H^T (R + H B H^T)^{-1} H B H^T R^{-1}\n$$\n提出公因子 $B H^T R^{-1}$：\n$$\nK = B H^T [I - (R + H B H^T)^{-1} H B H^T] R^{-1}\n$$\n让我们重写括号中的项：\n$$\nI - (R + H B H^T)^{-1} H B H^T = (R + H B H^T)^{-1} [(R + H B H^T) - H B H^T] = (R + H B H^T)^{-1} R\n$$\n将此代回，得到卡尔曼增益矩阵的所需形式，该形式在观测空间（维度 $m \\times m$）中是可计算的：\n$$\nK = B H^T [(R + H B H^T)^{-1} R] R^{-1} = B H^T (R + H B H^T)^{-1}\n$$\n信号自由度（DFS）定义为分析对观测的敏感度的迹，$\\mathrm{tr}(HK)$：\n$$\n\\mathrm{DFS} = \\mathrm{tr}(HK) = \\mathrm{tr}(H [B H^T (R + H B H^T)^{-1}])\n$$\n利用迹的循环性质 $\\mathrm{tr}(AB) = \\mathrm{tr}(BA)$，我们可以重新排列这些项：\n$$\n\\mathrm{DFS} = \\mathrm{tr}((H B H^T) (R + H B H^T)^{-1})\n$$\n这就是所需的 DFS 可实现表达式。矩阵 $H B H^T$ 是一个 $m \\times m$ 矩阵，表示投影到观测空间中的背景误差协方差。矩阵 $R$ 也是 $m \\times m$。现在的计算依赖于在通常小得多的观测空间中进行矩阵求逆。\n\n对于数值实现，应避免显式矩阵求逆。令 $M = HBH^T$。给定 $R = \\sigma_o^2 I_m$，表达式变为 $\\mathrm{DFS} = \\mathrm{tr}(M (M + \\sigma_o^2 I_m)^{-1})$。设 $M = U \\Lambda U^T$ 是对称矩阵 $M$ 的特征分解，其中 $U$ 是正交的，$\\Lambda = \\mathrm{diag}(\\mu_1, \\dots, \\mu_m)$ 包含 $M$ 的特征值 $\\mu_i$。那么：\n$$\nM + \\sigma_o^2 I_m = U \\Lambda U^T + \\sigma_o^2 U U^T = U (\\Lambda + \\sigma_o^2 I_m) U^T\n$$\n$$\n(M + \\sigma_o^2 I_m)^{-1} = U (\\Lambda + \\sigma_o^2 I_m)^{-1} U^T\n$$\n乘积为：\n$$\nM(M + \\sigma_o^2 I_m)^{-1} = (U \\Lambda U^T) (U (\\Lambda + \\sigma_o^2 I_m)^{-1} U^T) = U \\Lambda (\\Lambda + \\sigma_o^2 I_m)^{-1} U^T\n$$\n迹在相似变换下是不变的，所以：\n$$\n\\mathrm{DFS} = \\mathrm{tr}(\\Lambda (\\Lambda + \\sigma_o^2 I_m)^{-1}) = \\sum_{i=1}^{m} \\frac{\\mu_i}{\\mu_i + \\sigma_o^2}\n$$\n这个最终表达式是数值稳定且计算高效的，因为它依赖于计算 $m \\times m$ 对称矩阵 $HBH^T$ 的特征值。\n\n**2. 计算成本模型和权衡指标**\n\n在观测空间公式中，主要的计算成本与 $m \\times m$ 矩阵运算相关，其中 $m$ 是观测数量。对于一个稠密的对称正定矩阵，求解线性系统或执行特征分解通常通过 Cholesky 分解或类似方法实现，其成本与 $m^3$ 成正比。问题提供了成本代理：\n$$\nC(m) = \\frac{1}{3} m^3\n$$\n稀疏化将观测数量从 $m_{\\text{before}}$ 减少到 $m_{\\text{after}}$，从而将计算成本从 $C(m_{\\text{before}})$ 降低到 $C(m_{\\text{after}})$。这种成本降低是以信息损失为代价的，信息损失由 DFS 从 $\\mathrm{DFS}_{\\text{before}}$ 减少到 $\\mathrm{DFS}_{\\text{after}}$ 来量化。这种权衡由指标 $\\rho$ 来衡量：\n$$\n\\rho = \\frac{\\mathrm{DFS}_{\\text{before}} - \\mathrm{DFS}_{\\text{after}}}{C(m_{\\text{before}}) - C(m_{\\text{after}})}\n$$\n该指标表示每节省一个单位计算成本所损失的信息量。\n\n**3. 算法实现**\n\n对于每个测试用例 $(n, s, q, \\sigma_b, L, \\sigma_o)$：\n1.  使用公式 $B_{ij} = \\sigma_b^2 \\exp\\left(-\\frac{(i-j)^2}{2L^2}\\right)$（对于 $i, j \\in \\{0, \\dots, n-1\\}$），构造 $n \\times n$ 背景误差协方差矩阵 $B$，注意 $\\Delta x = 1$。\n2.  确定稀疏化前的观测索引集：从索引 0 开始，间隔为 $s$ 的网格点。设这些观测的数量为 $m_{\\text{before}}$。\n3.  确定稀疏化后的观测索引集：从原始集中每隔 $q$ 个观测保留一个，从第一个开始。设这些观测的数量为 $m_{\\text{after}}$。\n4.  对于“稀疏化前”的情况：\n    a. 通过选择与“稀疏化前”索引对应的 $B$ 的行和列，形成观测空间矩阵 $M_{\\text{before}} = H_{\\text{before}} B H_{\\text{before}}^T$。\n    b. 计算 $m_{\\text{before}} \\times m_{\\text{before}}$ 矩阵 $M_{\\text{before}}$ 的特征值 $\\mu_i$。\n    c. 计算 $\\mathrm{DFS}_{\\text{before}} = \\sum_{i=1}^{m_{\\text{before}}} \\frac{\\mu_i}{\\mu_i + \\sigma_o^2}$。\n    d. 计算 $C(m_{\\text{before}}) = \\frac{1}{3} m_{\\text{before}}^3$。\n5.  对于“稀疏化后”的情况：\n    a. 使用“稀疏化后”的索引形成矩阵 $M_{\\text{after}} = H_{\\text{after}} B H_{\\text{after}}^T$。\n    b. 计算 $m_{\\text{after}} \\times m_{\\text{after}}$ 矩阵 $M_{\\text{after}}$ 的特征值 $\\lambda_i$。\n    c. 计算 $\\mathrm{DFS}_{\\text{after}} = \\sum_{i=1}^{m_{\\text{after}}} \\frac{\\lambda_i}{\\lambda_i + \\sigma_o^2}$。\n    d. 计算 $C(m_{\\text{after}}) = \\frac{1}{3} m_{\\text{after}}^3$。\n6.  计算权衡指标 $\\rho = \\frac{\\mathrm{DFS}_{\\text{before}} - \\mathrm{DFS}_{\\text{after}}}{C(m_{\\text{before}}) - C(m_{\\text{after}})}$。\n7.  存储结果 $[\\mathrm{DFS}_{\\text{before}}, \\mathrm{DFS}_{\\text{after}}, \\rho]$，四舍五入到六位小数，并将最终输出格式化为类 JSON 的列表的列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the variational data assimilation problem for a suite of test cases.\n    For each case, it computes the Degrees of Freedom for Signal (DFS) and computational\n    cost before and after observation thinning, then calculates a trade-off metric.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (n, s, q, sigma_b, L, sigma_o)\n    test_cases = [\n        (100, 1, 2, 1.0, 5.0, 0.5),\n        (100, 1, 3, 1.0, 5.0, 2.0),\n        (100, 2, 2, 1.0, 20.0, 1.0),\n        (120, 4, 2, 1.0, 0.5, 0.2),\n        (60, 1, 5, 1.0, 10.0, 0.7),\n    ]\n\n    results = []\n\n    def cost_model(m):\n        \"\"\"Computes the computational cost proxy C(m) = m^3 / 3.\"\"\"\n        return (1/3) * m**3\n\n    def compute_dfs(hbh_t, sigma_o):\n        \"\"\"\n        Computes the Degrees of Freedom for Signal (DFS) using an eigenvalue-based formula.\n        DFS = sum(eig_i / (eig_i + sigma_o^2))\n        \"\"\"\n        if hbh_t.shape[0] == 0:\n            return 0.0\n        \n        # Use eigvalsh for symmetric matrices for stability and performance.\n        eigenvalues = np.linalg.eigvalsh(hbh_t)\n        \n        # The eigenvalues of a positive semi-definite matrix should be non-negative.\n        # A small epsilon can be used to avoid division by zero in pathological cases,\n        # though not expected here since sigma_o > 0.\n        dfs = np.sum(eigenvalues / (eigenvalues + sigma_o**2))\n        return dfs\n\n    for case in test_cases:\n        n, s, q, sigma_b, L, sigma_o = case\n\n        # 1. Construct the background error covariance matrix B\n        # Delta x = 1 is implicitly used.\n        indices = np.arange(n).reshape(-1, 1)\n        dist_sq = (indices - indices.T)**2\n        B = sigma_b**2 * np.exp(-dist_sq / (2 * L**2))\n\n        # 2. Define observation sets before and after thinning\n        obs_indices_before = np.arange(0, n, s)\n        obs_indices_after = obs_indices_before[::q]\n\n        m_before = len(obs_indices_before)\n        m_after = len(obs_indices_after)\n\n        # 3. Calculations for the 'before thinning' case\n        HBHt_before = B[np.ix_(obs_indices_before, obs_indices_before)]\n        dfs_before = compute_dfs(HBHt_before, sigma_o)\n        cost_before = cost_model(m_before)\n        \n        # 4. Calculations for the 'after thinning' case\n        HBHt_after = B[np.ix_(obs_indices_after, obs_indices_after)]\n        dfs_after = compute_dfs(HBHt_after, sigma_o)\n        cost_after = cost_model(m_after)\n\n        # 5. Calculate the trade-off metric rho\n        delta_dfs = dfs_before - dfs_after\n        delta_cost = cost_before - cost_after\n        \n        # The denominator delta_cost will not be zero as q > 1 for all test cases,\n        # ensuring m_before > m_after.\n        rho = delta_dfs / delta_cost\n\n        results.append([dfs_before, dfs_after, rho])\n\n    # Final print statement in the exact required format.\n    # Format each number to exactly 6 decimal places.\n    formatted_results = []\n    for res_tuple in results:\n        formatted_tuple = f\"[{res_tuple[0]:.6f},{res_tuple[1]:.6f},{res_tuple[2]:.6f}]\"\n        formatted_results.append(formatted_tuple)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}