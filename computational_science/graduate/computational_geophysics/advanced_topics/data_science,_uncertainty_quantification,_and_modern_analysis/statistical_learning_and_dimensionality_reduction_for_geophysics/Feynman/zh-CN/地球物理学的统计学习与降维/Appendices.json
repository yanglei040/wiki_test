{
    "hands_on_practices": [
        {
            "introduction": "主成分分析（PCA）是将高维地球物理数据集（如多属性地震或电磁数据）压缩到低维表示的一种基础技术。应用PCA时，一个关键步骤是确定应保留多少个重要的主成分。本练习  将超越碎石图“肘部”等主观方法，要求您基于“断棍”模型实现一个统计上严谨的准则，帮助您从随机噪声中分辨出有意义的地球物理结构。",
            "id": "3615523",
            "problem": "一项海洋可控源电磁勘探产生一个数据矩阵，其列是沿大陆架剖面计算的八个频率归一化属性通道。设均值中心化的数据矩阵为 $X \\in \\mathbb{R}^{n \\times r}$，其中有 $r=8$ 个通道和 $n \\gg r$ 个观测值。对 $X$ 应用主成分分析（PCA），以识别嵌入在电磁响应中的低维海洋学结构。\n\n从样本协方差矩阵 $C = \\frac{1}{n-1} X^{\\top} X$ 和奇异值分解 (SVD) $X = U \\Sigma V^{\\top}$（其中奇异值为 $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r} \\geq 0$）的定义出发，推导前 $k$ 个主成分的解释方差比率的表达式，该表达式应以 $\\{\\sigma_{i}\\}_{i=1}^{r}$ 表示。\n\n接下来，假设属性通道的潜在零结构是它们是可交换的且没有优先方向，这可以通过断棍模型来建模：将一根单位长度的棍子在 $r-1$ 个独立的 Uniform$(0,1)$ 位置上折断，产生 $r$ 个片段，其长度按降序排列。在此零假设下，设计一个准则，利用预期的有序片段长度来选择 $k$ 个数据驱动的主成分。你的准则必须用观测到的各成分方差比例和断棍模型下预期的有序片段长度来明确表述；并从 PCA 的第一性原理和断棍零模型的定义来论证其构造的合理性。\n\n最后，将你的准则应用于从 $X$ 获得的以下奇异值（单位任意）：\n$\\{\\sigma_{i}\\}_{i=1}^{8} = \\{12.3,\\ 7.8,\\ 5.1,\\ 3.3,\\ 2.6,\\ 2.2,\\ 2.1,\\ 2.0\\}$。\n以整数形式报告所选的 $k$。如果需要任何中间数值，请保留足够的精度以确保选择是明确的。将你最终选择的 $k$ 表示为无单位的整数。",
            "solution": "该问题被评估为有效，因为它具有科学依据、问题明确且客观。它包括一个标准的理论推导，一个基于已知零模型的统计准则的制定，及其在给定数据集上的应用。所有必要的信息都已提供。\n\n按照要求，该问题分三部分解决：推导解释方差比率，设计断棍选择准则，并将其应用于所提供的数据。\n\n首先，我们推导前 $k$ 个主成分的解释方差比率的表达式。\n样本协方差矩阵定义为 $C = \\frac{1}{n-1} X^{\\top} X$，其中 $X \\in \\mathbb{R}^{n \\times r}$ 是均值中心化的数据矩阵，有 $n$ 个观测值和 $r$ 个通道。主成分分析（PCA）旨在寻找该协方差矩阵的特征向量和特征值。设 $X$ 的奇异值分解（SVD）为 $X = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{n \\times r}$ 的列是标准正交的（$U^{\\top}U=I_r$），$\\Sigma \\in \\mathbb{R}^{r \\times r}$ 是一个对角矩阵，其对角线元素为奇异值 $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r \\geq 0$，而 $V \\in \\mathbb{R}^{r \\times r}$ 是一个正交矩阵（$V^{\\top}V = VV^{\\top} = I_r$）。\n\n我们可以将 $X$ 的 SVD 代入 $C$ 的表达式中：\n$$ C = \\frac{1}{n-1} (U \\Sigma V^{\\top})^{\\top} (U \\Sigma V^{\\top}) $$\n使用性质 $(ABC)^{\\top} = C^{\\top}B^{\\top}A^{\\top}$，我们得到：\n$$ C = \\frac{1}{n-1} (V \\Sigma^{\\top} U^{\\top}) (U \\Sigma V^{\\top}) $$\n因为 $\\Sigma$ 是对角矩阵，所以 $\\Sigma^{\\top} = \\Sigma$。$U$ 的列是标准正交的，所以 $U^{\\top}U = I_r$。\n$$ C = \\frac{1}{n-1} V \\Sigma (U^{\\top}U) \\Sigma V^{\\top} = \\frac{1}{n-1} V \\Sigma I_r \\Sigma V^{\\top} = \\frac{1}{n-1} V \\Sigma^2 V^{\\top} $$\n这个方程是 $C$ 的特征分解。$V$ 的列是 $C$ 的特征向量，即主成分方向。$C$ 的特征值（记为 $\\lambda_i$）是对角矩阵 $\\frac{1}{n-1}\\Sigma^2$ 的对角线元素。因此，与第 $i$ 个主成分相对应的特征值为：\n$$ \\lambda_i = \\frac{\\sigma_i^2}{n-1} $$\n数据中的总方差是协方差矩阵的迹，$\\text{Tr}(C)$。\n$$ \\text{Tr}(C) = \\text{Tr}\\left(\\frac{1}{n-1} V \\Sigma^2 V^{\\top}\\right) $$\n使用迹的循环性质 $\\text{Tr}(ABC) = \\text{Tr}(CAB)$：\n$$ \\text{Tr}(C) = \\frac{1}{n-1} \\text{Tr}(\\Sigma^2 V^{\\top}V) = \\frac{1}{n-1} \\text{Tr}(\\Sigma^2 I_r) = \\frac{1}{n-1} \\text{Tr}(\\Sigma^2) $$\n对角矩阵的迹是其对角元素的和，所以：\n$$ \\text{Tr}(C) = \\frac{1}{n-1} \\sum_{j=1}^{r} \\sigma_j^2 $$\n第 $i$ 个主成分解释的方差比例是其特征值 $\\lambda_i$ 与总方差 $\\text{Tr}(C)$ 的比值：\n$$ p_i = \\frac{\\lambda_i}{\\text{Tr}(C)} = \\frac{\\frac{\\sigma_i^2}{n-1}}{\\frac{1}{n-1} \\sum_{j=1}^{r} \\sigma_j^2} = \\frac{\\sigma_i^2}{\\sum_{j=1}^{r} \\sigma_j^2} $$\n前 $k$ 个主成分的解释方差比率是各个比例的总和：\n$$ R(k) = \\sum_{i=1}^{k} p_i = \\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{j=1}^{r} \\sigma_j^2} $$\n\n其次，我们基于断棍模型设计一个选择主成分数量 $k$ 的准则。\n零假设 ($H_0$) 是数据中没有主导结构，意味着总方差在 $r$ 个成分之间是随机分配的。断棍模型为这个零假设提供了一种形式化描述。在这个模型中，一根单位长度的棍子在从 Uniform$(0,1)$ 分布中抽取的 $r-1$ 个独立的随机位置上被折断，产生 $r$ 个片段。当这些片段的长度按降序排列时，它们代表了在 $H_0$ 下有序主成分所解释的方差的期望比例。\n\n设 $b_i$ 为一根棍子被折成 $r$ 段后第 $i$ 长片段的期望长度。$b_i$ 的解析表达式是：\n$$ b_i = \\frac{1}{r} \\sum_{j=i}^{r} \\frac{1}{j} $$\n这些 $b_i$ 值可作为没有优先结构的数据集的基准比例。选择准则是通过将每个成分的观测方差比例 $p_i$ 与断棍模型得出的期望比例 $b_i$ 进行比较来构建的。如果一个成分解释的方差比例大于随机情况下的期望值，那么它就被认为捕捉到了显著的、非随机的结构。\n\n准则如下：只要主成分的观测方差比例超过断棍零模型预测的比例，就保留该主成分。选择主成分数量 $k$ 为满足以下条件的最大整数：对于所有成分 $j=1, \\dots, k$，条件 $p_j > b_j$ 均成立。鉴于 $p_j$ 和 $b_j$ 都是单调递减序列，这等价于找到第一个使得 $p_i \\le b_i$ 的成分 $i$，然后选择 $k=i-1$。\n\n第三，我们将此准则应用于给定的 $r=8$ 个通道的奇异值：\n$\\{\\sigma_{i}\\}_{i=1}^{8} = \\{12.3,\\ 7.8,\\ 5.1,\\ 3.3,\\ 2.6,\\ 2.2,\\ 2.1,\\ 2.0\\}$。\n\n步骤1：计算观测到的方差比例 $p_i$。\n首先，我们计算奇异值的平方 $\\sigma_i^2$：\n$\\sigma_1^2 = 12.3^2 = 151.29$\n$\\sigma_2^2 = 7.8^2 = 60.84$\n$\\sigma_3^2 = 5.1^2 = 26.01$\n$\\sigma_4^2 = 3.3^2 = 10.89$\n$\\sigma_5^2 = 2.6^2 = 6.76$\n$\\sigma_6^2 = 2.2^2 = 4.84$\n$\\sigma_7^2 = 2.1^2 = 4.41$\n$\\sigma_8^2 = 2.0^2 = 4.00$\n\n平方和的总值为 $\\sum_{j=1}^{8} \\sigma_j^2 = 151.29 + 60.84 + 26.01 + 10.89 + 6.76 + 4.84 + 4.41 + 4.00 = 269.04$。\n观测到的比例 $p_i = \\sigma_i^2 / 269.04$ 为：\n$p_1 \\approx 0.5623$\n$p_2 \\approx 0.2261$\n$p_3 \\approx 0.0967$\n$p_4 \\approx 0.0405$\n$p_5 \\approx 0.0251$\n$p_6 \\approx 0.0180$\n$p_7 \\approx 0.0164$\n$p_8 \\approx 0.0149$\n\n步骤2：计算 $r=8$ 时预期的断棍比例 $b_i$。\n使用公式 $b_i = \\frac{1}{8} \\sum_{j=i}^{8} \\frac{1}{j}$：\n$b_1 = \\frac{1}{8} (\\frac{1}{1} + \\frac{1}{2} + \\dots + \\frac{1}{8}) = \\frac{1}{8}(\\frac{761}{280}) \\approx 0.3397$\n$b_2 = \\frac{1}{8} (\\frac{1}{2} + \\dots + \\frac{1}{8}) \\approx 0.2147$\n$b_3 = \\frac{1}{8} (\\frac{1}{3} + \\dots + \\frac{1}{8}) \\approx 0.1522$\n$b_4 = \\frac{1}{8} (\\frac{1}{4} + \\dots + \\frac{1}{8}) \\approx 0.1106$\n$b_5 = \\frac{1}{8} (\\frac{1}{5} + \\dots + \\frac{1}{8}) \\approx 0.0793$\n$b_6 = \\frac{1}{8} (\\frac{1}{6} + \\frac{1}{7} + \\frac{1}{8}) \\approx 0.0543$\n$b_7 = \\frac{1}{8} (\\frac{1}{7} + \\frac{1}{8}) \\approx 0.0335$\n$b_8 = \\frac{1}{8} (\\frac{1}{8}) \\approx 0.0156$\n\n步骤3：比较 $p_i$ 和 $b_i$ 以确定 $k$。\n我们逐个成分进行比较：\n- 对于 $i=1$：$p_1 \\approx 0.5623 > b_1 \\approx 0.3397$。条件成立。\n- 对于 $i=2$：$p_2 \\approx 0.2261 > b_2 \\approx 0.2147$。条件成立。\n- 对于 $i=3$：$p_3 \\approx 0.0967  b_3 \\approx 0.1522$。条件不成立。\n\n由于条件 $p_i  b_i$ 在 $i=3$ 处首次不成立，我们只保留条件成立的那些成分。条件对 $j=1$ 和 $j=2$ 成立，但对 $j=3$ 不成立。因此，使得对于所有 $j=1, \\dots, k$ 都有 $p_j  b_j$ 的最大 $k$ 值为 $2$。\n\n所选的主成分数量为 $k=2$。",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "奇异值分解（SVD）不仅是一种数据分析工具，也是解决地球物理学中普遍存在的不适定线性反问题的基石。本练习  探讨了截断奇异值分解（TSVD）如何作为一种正则化方法，通过控制基本的偏差-方差权衡来稳定解。通过推导期望均方误差并找到最优截断水平，您将对正则化如何以牺牲解的平滑度为代价来抑制噪声放大获得定量的理解。",
            "id": "3615522",
            "problem": "考虑一个线性化的地球物理反演问题，其正演算子为 $G \\in \\mathbb{R}^{m \\times n}$，数据模型为 $d = G m_{\\text{true}} + \\varepsilon$，其中 $\\varepsilon$ 是均值为零、协方差为 $\\sigma^{2} I_{m}$ 的噪声，$I_{m}$ 是 $m \\times m$ 的单位矩阵。设 $G$ 的奇异值分解（SVD）为 $G = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots, \\sigma_{n})$ 包含奇异值 $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{n}  0$。在许多离散化的地球物理积分算子中，奇异值可以用幂律很好地近似。假设一个 $n = 60$ 的离散化，其奇异值遵循\n$$\n\\sigma_{i} = 2.5 \\, i^{-1.3}, \\quad i = 1, 2, \\dots, 60.\n$$\n截断指数为 $k$ 的截断奇异值分解估计量定义为\n$$\n\\hat{m}_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} d}{\\sigma_{i}} \\, v_{i},\n$$\n其中 $u_{i}$ 和 $v_{i}$ 分别是左、右奇异向量。为了分析偏差-方差权衡，将 $m_{\\text{true}}$ 在右奇异向量基中展开为 $m_{\\text{true}} = \\sum_{i=1}^{n} c_{i} v_{i}$。假设一个阶数为 $p = 1$ 的平滑先验，使得系数的先验二阶矩满足\n$$\n\\mathbb{E}[c_{i}^{2}] = C^{2} \\, i^{-2p} = C^{2} \\, i^{-2}, \\quad \\text{其中 } C = 0.8.\n$$\n设噪声水平为 $\\sigma = 2.0 \\times 10^{-2}$。从以上定义以及 $U$ 和 $V$ 的正交性出发，推导期望均方重构误差 $\\mathbb{E}\\!\\left[\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2}\\right]$ 作为 $k$ 的函数，并将其在奇异向量基中分解为方差项和偏差项，然后确定最优截断指数 $k^{\\star}$ 的选择准则，以在期望意义上最小化此误差。最后，根据给定参数计算最优截断指数 $k^{\\star}$ 的数值。以单个整数 $k^{\\star}$ 的形式提供最终答案。",
            "solution": "该问题陈述是地球物理学中正则化线性反演问题的一种标准表述。它具有科学依据、是适定的、客观的，并包含了唯一解所需的所有信息。因此，该问题被认为是有效的。我们继续进行推导。\n\n目标是找到最优截断指数 $k^{\\star}$，以最小化期望均方重构误差 $\\mathbb{E}\\!\\left[\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2}\\right]$。\n\n首先，我们在右奇异向量基 $\\{v_i\\}_{i=1}^n$ 中表示重构误差向量 $\\hat{m}_{k} - m_{\\text{true}}$。\n真实模型由其在该基下的展开式给出：\n$$\nm_{\\text{true}} = \\sum_{i=1}^{n} c_{i} v_{i}\n$$\n截断奇异值分解（TSVD）估计量定义为：\n$$\n\\hat{m}_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} d}{\\sigma_{i}} v_{i}\n$$\n我们将数据模型 $d = G m_{\\text{true}} + \\varepsilon$ 代入 $\\hat{m}_{k}$ 的表达式中。项 $u_{i}^{\\top} d$ 变为：\n$$\nu_{i}^{\\top} d = u_{i}^{\\top} (G m_{\\text{true}} + \\varepsilon) = u_{i}^{\\top} G m_{\\text{true}} + u_{i}^{\\top}\\varepsilon\n$$\n利用 $G$ 的 SVD $G = U \\Sigma V^{\\top} = \\sum_{j=1}^{n} \\sigma_{j} u_{j} v_{j}^{\\top}$，我们可以计算第一项：\n$$\nu_{i}^{\\top} G m_{\\text{true}} = u_{i}^{\\top} \\left( \\sum_{j=1}^{n} \\sigma_{j} u_{j} v_{j}^{\\top} \\right) \\left( \\sum_{l=1}^{n} c_{l} v_{l} \\right)\n$$\n由于奇异向量的正交性（$u_i^{\\top}u_j = \\delta_{ij}$ 和 $v_j^{\\top}v_l = \\delta_{jl}$），上式可以简化为：\n$$\nu_{i}^{\\top} G m_{\\text{true}} = \\sum_{j=1}^{n} \\sum_{l=1}^{n} \\sigma_{j} c_{l} (u_{i}^{\\top} u_{j}) (v_{j}^{\\top} v_{l}) = \\sum_{j=1}^{n} \\sum_{l=1}^{n} \\sigma_{j} c_{l} \\delta_{ij} \\delta_{jl} = \\sigma_{i} c_{i}\n$$\n因此，$u_{i}^{\\top} d = \\sigma_{i} c_{i} + u_{i}^{\\top}\\varepsilon$。将此结果代回估计量 $\\hat{m}_{k}$：\n$$\n\\hat{m}_{k} = \\sum_{i=1}^{k} \\frac{\\sigma_{i} c_{i} + u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} v_{i} = \\sum_{i=1}^{k} \\left( c_{i} + \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} \\right) v_{i}\n$$\n现在，我们可以写出误差向量：\n$$\n\\hat{m}_{k} - m_{\\text{true}} = \\sum_{i=1}^{k} \\left( c_{i} + \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} \\right) v_{i} - \\sum_{i=1}^{n} c_{i} v_{i} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} v_{i} - \\sum_{i=k+1}^{n} c_{i} v_{i}\n$$\n误差向量由两个正交分量组成：一个在由 $\\{v_1, \\dots, v_k\\}$ 张成的子空间中，另一个在由 $\\{v_{k+1}, \\dots, v_n\\}$ 张成的子空间中。第一个分量是由于噪声传播，第二个分量是由于解展开式的截断。\n\n接下来，我们计算误差向量的 $L_2$ 范数的平方。根据这两个和的正交性（函数空间中的勾股定理）：\n$$\n\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2} = \\left\\| \\sum_{i=1}^{k} \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} v_{i} \\right\\|_{2}^{2} + \\left\\| \\sum_{i=k+1}^{n} c_{i} v_{i} \\right\\|_{2}^{2}\n$$\n由于 $\\{v_i\\}$ 是标准正交基，范数简化为系数的平方和：\n$$\n\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2} = \\sum_{i=1}^{k} \\left( \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} \\right)^{2} + \\sum_{i=k+1}^{n} c_{i}^{2}\n$$\n现在我们对噪声分布 $\\varepsilon$ 和真实模型系数 $c_i$ 的先验分布取期望。总期望均方误差 $E(k)$ 为：\n$$\nE(k) = \\mathbb{E}\\!\\left[\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2}\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{k} \\frac{(u_{i}^{\\top}\\varepsilon)^{2}}{\\sigma_{i}^{2}}\\right] + \\mathbb{E}\\left[\\sum_{i=k+1}^{n} c_{i}^{2}\\right]\n$$\n期望算子是线性的，因此可以将其移到求和符号内部。假设项 $c_i$ 和 $\\varepsilon$ 是独立的。\n对于第一项（方差），我们需要 $\\mathbb{E}[(u_{i}^{\\top}\\varepsilon)^{2}]$。由于 $\\varepsilon$ 的均值为零，$\\mathbb{E}[u_{i}^{\\top}\\varepsilon] = u_i^{\\top}\\mathbb{E}[\\varepsilon] = 0$。该表达式是随机变量 $u_{i}^{\\top}\\varepsilon$ 的方差。\n$$\n\\mathbb{E}[(u_{i}^{\\top}\\varepsilon)^{2}] = \\text{Var}(u_{i}^{\\top}\\varepsilon) = u_{i}^{\\top} \\text{Cov}(\\varepsilon) u_{i} = u_{i}^{\\top} (\\sigma^{2} I_{m}) u_{i} = \\sigma^{2} (u_{i}^{\\top} u_{i}) = \\sigma^{2}\n$$\n$E(k)$ 的第一项是估计量的方差：\n$$\n\\text{Var}(k) = \\sum_{i=1}^{k} \\frac{\\mathbb{E}[(u_{i}^{\\top}\\varepsilon)^{2}]}{\\sigma_{i}^{2}} = \\sum_{i=1}^{k} \\frac{\\sigma^{2}}{\\sigma_{i}^{2}}\n$$\n$E(k)$ 的第二项是期望偏差的平方：\n$$\n\\text{Bias}^{2}(k) = \\sum_{i=k+1}^{n} \\mathbb{E}[c_{i}^{2}]\n$$\n因此，总期望均方误差分解为：\n$$\nE(k) = \\underbrace{\\sum_{i=1}^{k} \\frac{\\sigma^{2}}{\\sigma_{i}^{2}}}_{\\text{方差}} + \\underbrace{\\sum_{i=k+1}^{n} \\mathbb{E}[c_{i}^{2}]}_{\\text{期望偏差平方}}\n$$\n我们给定了模型 $\\sigma_{i} = 2.5 \\, i^{-1.3}$ 和 $\\mathbb{E}[c_{i}^{2}] = C^{2} i^{-2p}$，其中 $C=0.8$ 且 $p=1$。我们记 $A=2.5$ 和 $a=1.3$。\n$$\nE(k) = \\sum_{i=1}^{k} \\frac{\\sigma^{2}}{(A i^{-a})^{2}} + \\sum_{i=k+1}^{n} C^{2} i^{-2p} = \\frac{\\sigma^{2}}{A^{2}}\\sum_{i=1}^{k} i^{2a} + C^{2}\\sum_{i=k+1}^{n} i^{-2p}\n$$\n为了找到最小化 $E(k)$ 的最优整数 $k^{\\star}$，我们考察差值 $E(k+1) - E(k)$：\n$$\nE(k+1) - E(k) = \\left( \\frac{\\sigma^{2}}{A^{2}}\\sum_{i=1}^{k+1} i^{2a} + C^{2}\\sum_{i=k+2}^{n} i^{-2p} \\right) - \\left( \\frac{\\sigma^{2}}{A^{2}}\\sum_{i=1}^{k} i^{2a} + C^{2}\\sum_{i=k+1}^{n} i^{-2p} \\right)\n$$\n$$\nE(k+1) - E(k) = \\frac{\\sigma^{2}}{A^{2}}(k+1)^{2a} - C^{2}(k+1)^{-2p}\n$$\n最小值出现在 $k=k^{\\star}$ 处，在该点误差经过下降后开始增加。这对应于差值 $E(k+1)-E(k)$ 的符号从负变为正的点。我们寻找整数 $k$ 使得 $E(k+1)-E(k)  0$ 且 $E(k)-E(k-1)  0$。最优 $k^{\\star}$ 将接近于该差值的连续近似为零时的值。\n$$\n\\frac{\\sigma^{2}}{A^{2}}(k^{\\star}+1)^{2a} - C^{2}(k^{\\star}+1)^{-2p} \\approx 0 \\implies (k^{\\star}+1)^{2a+2p} \\approx \\frac{C^{2}A^{2}}{\\sigma^{2}}\n$$\n这给出了最优截断指数的选择准则：\n$$\nk^{\\star}+1 \\approx \\left( \\frac{CA}{\\sigma} \\right)^{\\frac{2}{2a+2p}} = \\left( \\frac{CA}{\\sigma} \\right)^{\\frac{1}{a+p}}\n$$\n现在，我们代入给定的数值：\n$C = 0.8$，$A = 2.5$，$\\sigma = 2.0 \\times 10^{-2} = 0.02$，$a = 1.3$，$p = 1$。\n$$\n\\frac{CA}{\\sigma} = \\frac{0.8 \\times 2.5}{0.02} = \\frac{2}{0.02} = 100\n$$\n$$\na+p = 1.3 + 1 = 2.3\n$$\n$$\nk^{\\star}+1 \\approx (100)^{\\frac{1}{2.3}} \\approx 10^{\\frac{2}{2.3}} \\approx 10^{0.869565} \\approx 7.4056\n$$\n$$\nk^{\\star} \\approx 6.4056\n$$\n由于 $k^{\\star}$ 必须是整数，它可能是 6 或 7。我们必须对这些值检查离散差 $\\Delta(k) = E(k+1)-E(k)$ 的符号。\n$$\n\\Delta(k) = \\frac{\\sigma^2}{A^2}(k+1)^{2.6} - C^2(k+1)^{-2} = \\frac{(0.02)^2}{(2.5)^2}(k+1)^{2.6} - (0.8)^2(k+1)^{-2}\n$$\n$$\n\\Delta(k) = (6.4 \\times 10^{-5})(k+1)^{2.6} - 0.64(k+1)^{-2}\n$$\n对于 $k=6$：\n$$\n\\Delta(6) = E(7)-E(6) = (6.4 \\times 10^{-5})(7)^{2.6} - 0.64(7)^{-2} \\approx (6.4 \\times 10^{-5})(157.486) - 0.64(0.020408)\n$$\n$$\n\\Delta(6) \\approx 0.010079 - 0.013061 = -0.002982\n$$\n由于 $\\Delta(6)  0$，我们有 $E(7)  E(6)$。误差仍在减小，因此最小值在 $k^{\\star} \\geq 7$ 处。\n\n对于 $k=7$：\n$$\n\\Delta(7) = E(8)-E(7) = (6.4 \\times 10^{-5})(8)^{2.6} - 0.64(8)^{-2} \\approx (6.4 \\times 10^{-5})(222.848) - 0.64(0.015625)\n$$\n$$\n\\Delta(7) \\approx 0.014262 - 0.01 = 0.004262\n$$\n由于 $\\Delta(7)  0$，我们有 $E(8)  E(7)$。误差在 $k=7$ 后开始增加。\n误差序列满足 $E(6)  E(7)$ 且 $E(7)  E(8)$。因此，最小期望均方误差出现在整数截断指数 $k^{\\star}=7$ 处。",
            "answer": "$$\\boxed{7}$$"
        },
        {
            "introduction": "地球物理数据集（如测井曲线）通常表现出复杂的、多峰值的分布，这些分布对应于不同的物理单元，如岩性或流体类型。高斯混合模型（GMM）通过无监督学习，为捕捉此类结构提供了一个灵活的概率框架。本练习  将引导您从第一性原理出发，推导期望最大化（EM）算法，这是拟合GMM及其他潜变量模型的关键算法，为您提供一个用于岩性自动分类的强大工具。",
            "id": "3615487",
            "problem": "考虑一个具有按深度索引的多道地球物理测井的钻孔，其中每个深度样本表示为一个向量 $y_{i} \\in \\mathbb{R}^{d}$（$i=1,\\dots,N$），该向量收集了 $d$ 种具有物理意义的响应（例如，自然伽马、中子孔隙度、体积密度、声波时差）。假设地层由 $K$ 个岩性类别组成，并且任何深度的测井响应都源于一个潜在的岩性类别。将观测数据建模为 $d$ 维高斯分量的有限混合，\n$$\np(y_{i} \\mid \\Theta) \\;=\\; \\sum_{k=1}^{K} \\pi_{k}\\,\\mathcal{N}\\!\\big(y_{i} \\mid \\mu_{k}, \\Sigma_{k}\\big),\n$$\n其中 $\\Theta = \\big\\{ \\pi_{1:K}, \\mu_{1:K}, \\Sigma_{1:K} \\big\\}$ 集合了混合权重 $\\pi_{k}$（非负且总和为1）、均值 $\\mu_{k} \\in \\mathbb{R}^{d}$ 和对称正定的全协方差矩阵 $\\Sigma_{k} \\in \\mathbb{R}^{d \\times d}$。令 $z_{i} \\in \\{1,\\dots,K\\}$ 表示指定深度 $i$ 处岩性类别的未观测到的类别潜在变量。\n\n从似然原理和高斯密度的定义出发，从第一性原理推导用于 $\\Theta$ 的最大似然估计的期望最大化（EM）算法。具体而言：\n- 从联合模型 $p(y_{i}, z_{i} \\mid \\Theta)$ 引入完全数据似然及其对数。\n- 基于琴生不等式，使用一个有效的变分下界构造来阐明期望最大化过程的动机。\n- 在期望步骤（E-步）中，使用贝叶斯法则为每个样本获得每个分量的后验责任。\n- 在最大化步骤（M-步）中，在约束 $\\sum_{k=1}^{K} \\pi_{k} = 1$ 下，相对于 $\\pi_{k}$ 最大化期望完全数据对数似然，并相对于 $\\mu_{k}$ 和 $\\Sigma_{k}$ 最大化，从而获得闭式更新。\n\n你的最终答案必须是 $\\pi_{k}$、$\\mu_{k}$ 和 $\\Sigma_{k}$（$k=1,\\dots,K$）的EM更新的闭式解析表达式，表示为当前参数值和数据 $\\{y_{i}\\}_{i=1}^{N}$ 的函数。不要提供数值。最终答案必须是单个解析表达式；不要包含任何单位。",
            "solution": "问题是推导用于估计一个 $d$ 维高斯混合模型（GMM）参数 $\\Theta = \\{ \\pi_{1:K}, \\mu_{1:K}, \\Sigma_{1:K} \\}$ 的期望最大化（EM）算法。数据由 $N$ 个观测值 $\\{y_{i}\\}_{i=1}^{N}$ 组成，其中 $y_i \\in \\mathbb{R}^d$。\n\n在GMM下，观测数据的对数似然由下式给出：\n$$\n\\mathcal{L}(\\Theta) = \\ln p(Y \\mid \\Theta) = \\sum_{i=1}^{N} \\ln p(y_{i} \\mid \\Theta) = \\sum_{i=1}^{N} \\ln \\left( \\sum_{k=1}^{K} \\pi_{k}\\,\\mathcal{N}(y_{i} \\mid \\mu_{k}, \\Sigma_{k}) \\right)\n$$\n由于对数内部存在求和，直接对 $\\mathcal{L}(\\Theta)$ 关于参数 $\\Theta$ 进行最大化在解析上是难以处理的。EM算法提供了一种迭代方法来找到该对数似然的一个局部最大值。\n\n**潜在变量和完全数据似然的引入**\n\n核心思想是引入一组潜在变量 $Z = \\{z_1, \\dots, z_N\\}$，其中每个 $z_i$ 是一个 $K$ 维二元随机向量。我们使用K中取一（1-of-K）的表示法，因此对于生成 $y_i$ 的分量 $k$，向量 $z_i$ 中有一个元素 $z_{ik}=1$，而所有其他元素 $z_{ij}=0$（$j \\neq k$）。$z_{ik}=1$ 的先验概率是混合权重 $\\pi_k$，所以 $p(z_{ik}=1) = \\pi_k$。\n\n观测值 $y_i$ 及其潜在变量 $z_i$ 的联合分布可以写为：\n$$\np(y_i, z_i \\mid \\Theta) = p(y_i \\mid z_i, \\Theta) p(z_i \\mid \\Theta) = \\prod_{k=1}^{K} \\left[ \\pi_k \\mathcal{N}(y_i \\mid \\mu_k, \\Sigma_k) \\right]^{z_{ik}}\n$$\n集合 $(Y,Z)$ 被称为“完全数据”。完全数据对数似然为：\n$$\n\\ln p(Y, Z \\mid \\Theta) = \\sum_{i=1}^{N} \\ln p(y_i, z_i \\mid \\Theta) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} z_{ik} \\left( \\ln \\pi_k + \\ln \\mathcal{N}(y_i \\mid \\mu_k, \\Sigma_k) \\right)\n$$\n这个表达式关于潜在变量 $z_{ik}$ 是线性的，这使得处理起来容易得多。\n\n**变分下界与EM算法**\n\n由于潜在变量 $Z$ 是未观测到的，我们不能直接最大化完全数据对数似然。EM算法通过迭代地最大化完全数据对数似然的条件期望来解决这个问题。这可以通过构建观测数据对数似然的一个下界来更形式化地阐明其动机。\n\n对于潜在变量上的任意概率分布 $q(Z)$，我们可以写出：\n$$\n\\mathcal{L}(\\Theta) = \\ln p(Y \\mid \\Theta) = \\sum_{i=1}^{N} \\ln \\sum_{z_i} p(y_i, z_i \\mid \\Theta) = \\sum_{i=1}^{N} \\ln \\left( \\sum_{z_i} q(z_i) \\frac{p(y_i, z_i \\mid \\Theta)}{q(z_i)} \\right)\n$$\n其中 $\\sum_{z_i}$ 表示对向量 $z_i$ 所有可能状态的求和。根据琴生不等式，由于对数是凹函数，我们有 $\\ln(\\mathbb{E}[X]) \\ge \\mathbb{E}[\\ln(X)]$。这给出了对数似然的一个下界：\n$$\n\\mathcal{L}(\\Theta) \\ge \\sum_{i=1}^{N} \\sum_{z_i} q(z_i) \\ln \\frac{p(y_i, z_i \\mid \\Theta)}{q(z_i)} \\equiv \\mathcal{J}(q, \\Theta)\n$$\nEM算法包含两个步骤，迭代地最大化这个下界 $\\mathcal{J}(q, \\Theta)$。令 $\\Theta^{(t)}$ 表示在第 $t$ 次迭代时的参数值。\n\n**期望步骤 (E-步)**\n\n在E-步中，我们固定参数 $\\Theta^{(t)}$，并关于变分分布 $q(Z)$ 最大化下界 $\\mathcal{J}(q, \\Theta^{(t)})$。$\\mathcal{L}(\\Theta^{(t)})$ 与 $\\mathcal{J}(q, \\Theta^{(t)})$ 之间的差距是 $q(Z)$ 与真实后验 $p(Z \\mid Y, \\Theta^{(t)})$ 之间的 Kullback-Leibler (KL) 散度。当此 KL 散度为零时，下界最紧（即 $\\mathcal{J}$ 被最大化），这发生在我们将 $q(Z)$ 精确地设置为后验分布时：\n$$\nq(z_i) = p(z_i \\mid y_i, \\Theta^{(t)})\n$$\n数据点 $y_i$ 属于分量 $k$ 的后验概率，记为“责任” $r_{ik}$，使用贝叶斯法则计算：\n$$\nr_{ik} \\equiv p(z_{ik}=1 \\mid y_i, \\Theta^{(t)}) = \\frac{p(y_i \\mid z_{ik}=1, \\Theta^{(t)}) p(z_{ik}=1 \\mid \\Theta^{(t)})}{\\sum_{j=1}^{K} p(y_i \\mid z_{ij}=1, \\Theta^{(t)}) p(z_{ij}=1 \\mid \\Theta^{(t)})}\n$$\n代入模型定义，我们得到：\n$$\nr_{ik} = \\frac{\\pi_k^{(t)} \\mathcal{N}(y_i \\mid \\mu_k^{(t)}, \\Sigma_k^{(t)})}{\\sum_{j=1}^{K} \\pi_j^{(t)} \\mathcal{N}(y_i \\mid \\mu_j^{(t)}, \\Sigma_j^{(t)})}\n$$\n这一步在给定当前参数的情况下，提供了潜在变量的期望值，即 $\\mathbb{E}[z_{ik}] = r_{ik}$。\n\n**最大化步骤 (M-步)**\n\n在M-步中，我们固定责任 $r_{ik}$（即固定 $q(Z)$），并关于模型参数 $\\Theta$ 最大化下界 $\\mathcal{J}(q, \\Theta)$。这等价于最大化完全数据对数似然的期望值，该期望值被称为 $Q$ 函数：\n$$\nQ(\\Theta, \\Theta^{(t)}) = \\mathbb{E}_{Z \\mid Y, \\Theta^{(t)}}[\\ln p(Y, Z \\mid \\Theta)] = \\sum_{i=1}^{N} \\sum_{k=1}^{K} r_{ik} \\left( \\ln \\pi_k + \\ln \\mathcal{N}(y_i \\mid \\mu_k, \\Sigma_k) \\right)\n$$\n我们通过关于 $\\pi_k$、$\\mu_k$ 和 $\\Sigma_k$ 最大化 $Q(\\Theta, \\Theta^{(t)})$ 来找到新的参数 $\\Theta^{(t+1)}$。\n\n1.  **关于 $\\pi_k$ 的最大化**：我们在约束 $\\sum_{k=1}^{K} \\pi_k = 1$ 下最大化 $\\sum_{i=1}^{N} \\sum_{k=1}^{K} r_{ik} \\ln \\pi_k$。使用一个拉格朗日乘子 $\\lambda$，我们构造拉格朗日函数 $L = \\sum_{i,k} r_{ik} \\ln \\pi_k + \\lambda(\\sum_{k} \\pi_k - 1)$。令 $\\frac{\\partial L}{\\partial \\pi_k}$ 为零，得到 $\\pi_k = -(\\sum_i r_{ik})/\\lambda$。令 $N_k = \\sum_{i=1}^N r_{ik}$，即簇 $k$ 中的有效点数。则 $\\pi_k = -N_k/\\lambda$。使用约束条件 $\\sum_k \\pi_k = 1 = -(\\sum_k N_k)/\\lambda = -N/\\lambda$，所以 $\\lambda = -N$。这给出了更新公式：\n    $$\n    \\pi_k^{(t+1)} = \\frac{N_k}{N} = \\frac{1}{N} \\sum_{i=1}^{N} r_{ik}\n    $$\n2.  **关于 $\\mu_k$ 的最大化**：$Q$ 函数中依赖于 $\\mu_k$ 的项是 $\\sum_i r_{ik} \\ln \\mathcal{N}(y_i \\mid \\mu_k, \\Sigma_k)$。最大化此项等价于最小化 $\\sum_i r_{ik} (y_i - \\mu_k)^T \\Sigma_k^{-1} (y_i - \\mu_k)$。对其关于 $\\mu_k$ 求导并令其为零，得出：\n    $$\n    \\sum_{i=1}^{N} r_{ik} \\Sigma_k^{-1} (y_i - \\mu_k) = 0 \\quad\\implies\\quad \\sum_{i=1}^{N} r_{ik} y_i = \\left(\\sum_{i=1}^{N} r_{ik}\\right) \\mu_k\n    $$\n    这给出了均值的更新公式，即数据点的加权平均：\n    $$\n    \\mu_k^{(t+1)} = \\frac{\\sum_{i=1}^{N} r_{ik} y_i}{\\sum_{i=1}^{N} r_{ik}} = \\frac{1}{N_k} \\sum_{i=1}^{N} r_{ik} y_i\n    $$\n3.  **关于 $\\Sigma_k$ 的最大化**：我们最大化 $Q$ 函数中与 $\\Sigma_k$ 相关的项：$\\sum_i r_{ik} [-\\frac{1}{2} \\ln |\\Sigma_k| - \\frac{1}{2}(y_i - \\mu_k)^T \\Sigma_k^{-1} (y_i - \\mu_k)]$。标准的M-步过程在同一步中更新 $\\mu_k$ 和 $\\Sigma_k$，因此我们在这里使用新的均值 $\\mu_k^{(t+1)}$。将此表达式对 $\\Sigma_k^{-1}$ 求导并令其为零，我们得到高斯协方差的最大似然估计：\n    $$\n    \\Sigma_k^{(t+1)} = \\frac{\\sum_{i=1}^{N} r_{ik} (y_i - \\mu_k^{(t+1)})(y_i - \\mu_k^{(t+1)})^T}{\\sum_{i=1}^{N} r_{ik}} = \\frac{1}{N_k} \\sum_{i=1}^{N} r_{ik} (y_i - \\mu_k^{(t+1)})(y_i - \\mu_k^{(t+1)})^T\n    $$\n这组新参数 $\\Theta^{(t+1)} = \\{\\pi_k^{(t+1)}, \\mu_k^{(t+1)}, \\Sigma_k^{(t+1)}\\}$ 保证满足 $\\mathcal{L}(\\Theta^{(t+1)}) \\ge \\mathcal{L}(\\Theta^{(t)})$。E-步和M-步被重复执行，直到对数似然或参数收敛。最终的更新方程是在M-步中找到的表达式。",
            "answer": "$$\n\\boxed{\n\\pmatrix{\n\\frac{1}{N}\\sum_{i=1}^{N} r_{ik}  \\frac{\\sum_{i=1}^{N} r_{ik} y_i}{\\sum_{i=1}^{N} r_{ik}}  \\frac{\\sum_{i=1}^{N} r_{ik} (y_i - \\mu_k^{\\text{new}})(y_i - \\mu_k^{\\text{new}})^T}{\\sum_{i=1}^{N} r_{ik}}\n}\n}\n$$"
        }
    ]
}