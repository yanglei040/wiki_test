{
    "hands_on_practices": [
        {
            "introduction": "Many dimensionality reduction techniques, including Principal Component Analysis (PCA), are built upon the Singular Value Decomposition (SVD). For the massive data matrices encountered in geophysics, such as those from seismic surveys, computing the full SVD is often computationally prohibitive. This practice introduces subspace iteration, an efficient algorithm for finding the dominant singular vectors that capture the most significant patterns in the data . By deriving the method from the spectral properties of the covariance matrix $A A^{\\top}$ and applying it to a small example, you will understand the computational engine that makes large-scale dimensionality reduction feasible.",
            "id": "3615480",
            "problem": "A seismic survey can be represented by a data matrix $A \\in \\mathbb{R}^{m \\times n}$ whose rows index receiver locations and whose columns index shot gathers. In many computational geophysics workflows, the dominant left singular vectors of $A$ reveal coherent wavefield subspaces (for example, surface-wave modes or strong reflectors). Consider computing these vectors by subspace iteration, a method grounded in the spectral properties of symmetric positive semidefinite matrices.\n\nStarting only from the following fundamental base:\n- The definition of the Singular Value Decomposition (SVD): $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative entries.\n- The observation that $A A^{\\top}$ is symmetric positive semidefinite and has eigen-decomposition $A A^{\\top} = U (\\Sigma \\Sigma^{\\top}) U^{\\top}$.\n- The amplification property of repeated multiplication by a matrix with a spectral gap: if $A A^{\\top}$ has a dominant eigenvalue separated from the rest, repeated multiplication by $A A^{\\top}$ amplifies components along the corresponding eigenvector(s).\n- The Rayleigh quotient for a symmetric matrix $M$ and a nonzero vector $y$, defined as $R(y; M) = \\dfrac{y^{\\top} M y}{y^{\\top} y}$.\n\nTask (theoretical derivation):\n1. Derive the subspace iteration mapping to update a $p$-dimensional subspace that approximates the span of the top $p$ left singular vectors of $A$. Your derivation must explicitly connect the amplification mechanism of $A A^{\\top}$ to the subspace update and explain the necessity of an orthonormalization step to maintain numerical stability and a well-conditioned basis.\n\nTask (explicit computation):\n2. Let $A \\in \\mathbb{R}^{3 \\times 2}$ be\n$$\nA = \\begin{pmatrix}\n2 & 0 \\\\\n1 & 1 \\\\\n0 & 2\n\\end{pmatrix}.\n$$\nConsider rank-$p = 1$ subspace iteration with initial unit vector\n$$\ny_{0} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\nPerform one iteration of subspace iteration for the dominant left singular vector as follows: compute the unnormalized update $\\tilde{y}_{1} = (A A^{\\top}) y_{0}$, then normalize it to obtain $y_{1} = \\dfrac{\\tilde{y}_{1}}{\\|\\tilde{y}_{1}\\|_{2}}$. Finally, compute the Rayleigh quotient\n$$\n\\lambda^{(1)} = y_{1}^{\\top} (A A^{\\top}) y_{1},\n$$\nwhich serves as a one-step approximation to the square of the dominant singular value of $A$.\n\nAnswer specification:\n- Your final answer must be the single scalar value of $\\lambda^{(1)}$.\n- Express the number exactly; do not round.",
            "solution": "The problem presents a theoretical derivation task followed by an explicit computational task, both concerning the subspace iteration method for finding dominant left singular vectors of a data matrix $A$.\n\n### Task 1: Theoretical Derivation\n\nThe goal is to derive the subspace iteration mapping for approximating the subspace spanned by the top $p$ left singular vectors of a matrix $A \\in \\mathbb{R}^{m \\times n}$.\n\nThe fundamental connection is established through the matrix product $A A^{\\top}$. Given the Singular Value Decomposition (SVD) of $A$ as $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix with non-negative singular values $\\sigma_i$ on its diagonal, we can form the product:\n$$A A^{\\top} = (U \\Sigma V^{\\top})(U \\Sigma V^{\\top})^{\\top} = U \\Sigma V^{\\top} V \\Sigma^{\\top} U^{\\top}$$\nSince $V$ is orthogonal, $V^{\\top}V = I$, where $I$ is the identity matrix. The expression simplifies to:\n$$A A^{\\top} = U (\\Sigma \\Sigma^{\\top}) U^{\\top}$$\nThis is the spectral decomposition of the matrix $A A^{\\top}$. The matrix $\\Sigma \\Sigma^{\\top} \\in \\mathbb{R}^{m \\times m}$ is a diagonal matrix whose leading diagonal entries are the squared singular values, $\\sigma_i^2$, of $A$. This decomposition shows that the columns of $U$, which are the left singular vectors of $A$, are the eigenvectors of $A A^{\\top}$. The corresponding eigenvalues of $A A^{\\top}$ are the squared singular values $\\sigma_i^2$. Therefore, finding the $p$ left singular vectors of $A$ corresponding to the $p$ largest singular values is equivalent to finding the $p$ eigenvectors of $A A^{\\top}$ corresponding to the $p$ largest eigenvalues.\n\nSubspace iteration is an algorithm for finding such a subspace of dominant eigenvectors. It generalizes the power iteration method from a single vector to a $p$-dimensional subspace. Let the evolving approximation of the desired subspace at step $k$ be represented by the columns of a matrix $Y_k \\in \\mathbb{R}^{m \\times p}$ which form an orthonormal basis.\n\nThe mapping from the basis $Y_k$ to the next, $Y_{k+1}$, consists of two main steps:\n\n1.  **Amplification Step**: This step leverages the amplification property of repeated matrix multiplication. We apply the matrix $M = A A^{\\top}$ to the current basis $Y_k$.\n    $$Z_{k+1} = M Y_k = (A A^{\\top}) Y_k$$\n    Each column of $Y_k$ is a vector in $\\mathbb{R}^m$. When multiplied by $M$, its components in the directions of eigenvectors with larger eigenvalues are amplified more than components in directions of eigenvectors with smaller eigenvalues. After this operation, the subspace spanned by the columns of the resulting matrix $Z_{k+1}$ is a better approximation of the targeted dominant eigenspace than the subspace spanned by the columns of $Y_k$.\n\n2.  **Orthonormalization Step**: After the amplification step, the column vectors of $Z_{k+1}$ are no longer orthonormal. More critically, as the iteration proceeds, all column vectors tend to align with the direction of the dominant eigenvector, making them nearly linearly dependent. This leads to a poorly conditioned basis and numerical instability, eventually causing the collapse of the subspace to a single dimension. To prevent this and to extract a new, stable basis for the enriched subspace, an orthonormalization procedure is required. A robust method for this is the QR decomposition of $Z_{k+1}$:\n    $$Z_{k+1} = Y_{k+1} R_{k+1}$$\n    where $Y_{k+1} \\in \\mathbb{R}^{m \\times p}$ is a matrix with orthonormal columns and $R_{k+1} \\in \\mathbb{R}^{p \\times p}$ is an upper triangular matrix. The columns of $Y_{k+1}$ provide the new orthonormal basis for the subspace at step $k+1$.\n\nIn summary, the subspace iteration mapping combines amplification via matrix multiplication with numerically stabilizing orthonormalization. This iterative process causes the subspace spanned by the columns of $Y_k$ to converge to the subspace spanned by the top $p$ eigenvectors of $A A^{\\top}$, which are the top $p$ left singular vectors of $A$.\n\n### Task 2: Explicit Computation\n\nWe are given the matrix $A \\in \\mathbb{R}^{3 \\times 2}$, the subspace dimension $p=1$, and the initial unit vector $y_0$.\n$$A = \\begin{pmatrix} 2 & 0 \\\\ 1 & 1 \\\\ 0 & 2 \\end{pmatrix}, \\quad y_{0} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$$\nThe task is to perform one iteration of subspace iteration ($p=1$ reduces this to power iteration) and compute the corresponding Rayleigh quotient.\n\nFirst, we compute the matrix $M = A A^{\\top}$:\n$$M = A A^{\\top} = \\begin{pmatrix} 2 & 0 \\\\ 1 & 1 \\\\ 0 & 2 \\end{pmatrix} \\begin{pmatrix} 2 & 1 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} = \\begin{pmatrix} 4 & 2 & 0 \\\\ 2 & 2 & 2 \\\\ 0 & 2 & 4 \\end{pmatrix}$$\n\nNext, we perform the amplification step by computing the unnormalized update $\\tilde{y}_{1} = M y_{0}$:\n$$\\tilde{y}_{1} = (A A^{\\top}) y_{0} = \\begin{pmatrix} 4 & 2 & 0 \\\\ 2 & 2 & 2 \\\\ 0 & 2 & 4 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 4(1) + 2(0) + 0(1) \\\\ 2(1) + 2(0) + 2(1) \\\\ 0(1) + 2(0) + 4(1) \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 4 \\\\ 4 \\\\ 4 \\end{pmatrix} = 2\\sqrt{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\n\nThen, we perform the normalization step to obtain the new unit vector $y_{1}$:\n$$y_{1} = \\frac{\\tilde{y}_{1}}{\\|\\tilde{y}_{1}\\|_{2}}$$\nThe direction of $\\tilde{y}_{1}$ is given by the vector $\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$. The norm of this direction vector is $\\sqrt{1^2 + 1^2 + 1^2} = \\sqrt{3}$. Normalizing gives:\n$$y_{1} = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\n\nFinally, we compute the Rayleigh quotient $\\lambda^{(1)} = y_{1}^{\\top} (A A^{\\top}) y_{1}$ to approximate the dominant eigenvalue of $A A^{\\top}$:\n$$\\lambda^{(1)} = y_{1}^{\\top} M y_{1} = \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\right) \\begin{pmatrix} 4 & 2 & 0 \\\\ 2 & 2 & 2 \\\\ 0 & 2 & 4 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right)$$\n$$\\lambda^{(1)} = \\frac{1}{3} \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 4 & 2 & 0 \\\\ 2 & 2 & 2 \\\\ 0 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\nFirst, we compute the matrix-vector product:\n$$\\begin{pmatrix} 4 & 2 & 0 \\\\ 2 & 2 & 2 \\\\ 0 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4+2+0 \\\\ 2+2+2 \\\\ 0+2+4 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 6 \\\\ 6 \\end{pmatrix}$$\nNow, we complete the calculation for $\\lambda^{(1)}$:\n$$\\lambda^{(1)} = \\frac{1}{3} \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 6 \\\\ 6 \\end{pmatrix} = \\frac{1}{3} (1 \\cdot 6 + 1 \\cdot 6 + 1 \\cdot 6) = \\frac{1}{3} (18) = 6$$\nThe one-step approximation to the square of the dominant singular value is $6$.",
            "answer": "$$\\boxed{6}$$"
        },
        {
            "introduction": "After extracting the principal components from a geophysical dataset, a critical question remains: how many of these components represent meaningful geological or physical structure, and how many are simply capturing noise? This exercise guides you through quantifying the proportion of variance explained by each component using its corresponding singular value . You will then implement a selection criterion based on the \"broken-stick\" model, a statistical null hypothesis that provides a baseline for distinguishing significant components from random partitions of variance, enabling you to make a principled decision on the intrinsic dimensionality of the data.",
            "id": "3615523",
            "problem": "A marine controlled-source electromagnetic survey produces a data matrix whose columns are eight frequency-normalized attribute channels computed along a continental shelf transect. Let the mean-centered data matrix be $X \\in \\mathbb{R}^{n \\times r}$ with $r=8$ channels and $n \\gg r$ observations. Principal Component Analysis (PCA) is applied to $X$ to identify low-dimensional oceanographic structures embedded in the electromagnetic response.\n\nStarting from the definition of the sample covariance matrix $C = \\frac{1}{n-1} X^{\\top} X$ and the singular value decomposition (SVD) $X = U \\Sigma V^{\\top}$ with singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r} \\geq 0$, derive an expression for the explained variance ratio of the first $k$ principal components in terms of $\\{\\sigma_{i}\\}_{i=1}^{r}$.\n\nNext, suppose the underlying null structure of the attribute channels is that they are exchangeable and devoid of preferential directions, modeled by a broken-stick process: a unit-length stick is broken at $r-1$ independent Uniform$(0,1)$ locations, producing $r$ fragments whose lengths are sorted in descending order. Under this null, design a criterion that uses the expected ordered fragment lengths to select $k$ data-driven principal components. Your criterion must be explicitly stated in terms of the observed component-wise variance proportions and the expected ordered fragment lengths under the broken-stick model; justify its construction from first principles of PCA and the definition of the broken-stick null.\n\nFinally, apply your criterion to the following singular values (in arbitrary units) obtained from $X$:\n$\\{\\sigma_{i}\\}_{i=1}^{8} = \\{12.3,\\ 7.8,\\ 5.1,\\ 3.3,\\ 2.6,\\ 2.2,\\ 2.1,\\ 2.0\\}$.\nReport the selected $k$ as an integer. If any intermediate numerical values are required, carry sufficient precision to ensure the selection is unambiguous. Express your final selected $k$ as an integer with no units.",
            "solution": "The problem is evaluated as valid, as it is scientifically grounded, well-posed, and objective. It consists of a standard theoretical derivation, the formulation of a statistical criterion based on a known null model, and its application to a given dataset. All necessary information is provided.\n\nThe problem is addressed in three parts as requested: deriving the explained variance ratio, designing the broken-stick selection criterion, and applying it to the provided data.\n\nFirst, we derive an expression for the explained variance ratio of the first $k$ principal components.\nThe sample covariance matrix is defined as $C = \\frac{1}{n-1} X^{\\top} X$, where $X \\in \\mathbb{R}^{n \\times r}$ is the mean-centered data matrix, with $n$ observations and $r$ channels. Principal Component Analysis (PCA) seeks the eigenvectors and eigenvalues of this covariance matrix. Let the Singular Value Decomposition (SVD) of $X$ be $X = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{n \\times r}$ has orthonormal columns ($U^{\\top}U=I_r$), $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is a diagonal matrix of singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r \\geq 0$, and $V \\in \\mathbb{R}^{r \\times r}$ is an orthogonal matrix ($V^{\\top}V = VV^{\\top} = I_r$).\n\nWe can substitute the SVD of $X$ into the expression for $C$:\n$$ C = \\frac{1}{n-1} (U \\Sigma V^{\\top})^{\\top} (U \\Sigma V^{\\top}) $$\nUsing the property $(ABC)^{\\top} = C^{\\top}B^{\\top}A^{\\top}$, we get:\n$$ C = \\frac{1}{n-1} (V \\Sigma^{\\top} U^{\\top}) (U \\Sigma V^{\\top}) $$\nSince $\\Sigma$ is a diagonal matrix, $\\Sigma^{\\top} = \\Sigma$. The columns of $U$ are orthonormal, so $U^{\\top}U = I_r$.\n$$ C = \\frac{1}{n-1} V \\Sigma (U^{\\top}U) \\Sigma V^{\\top} = \\frac{1}{n-1} V \\Sigma I_r \\Sigma V^{\\top} = \\frac{1}{n-1} V \\Sigma^2 V^{\\top} $$\nThis equation is the eigendecomposition of $C$. The columns of $V$ are the eigenvectors of $C$, which are the principal component directions. The eigenvalues of $C$, denoted $\\lambda_i$, are the diagonal entries of the diagonal matrix $\\frac{1}{n-1}\\Sigma^2$. Thus, the eigenvalue corresponding to the $i$-th principal component is:\n$$ \\lambda_i = \\frac{\\sigma_i^2}{n-1} $$\nThe total variance in the data is the trace of the covariance matrix, $\\text{Tr}(C)$.\n$$ \\text{Tr}(C) = \\text{Tr}\\left(\\frac{1}{n-1} V \\Sigma^2 V^{\\top}\\right) $$\nUsing the cyclic property of the trace, $\\text{Tr}(ABC) = \\text{Tr}(CAB)$:\n$$ \\text{Tr}(C) = \\frac{1}{n-1} \\text{Tr}(\\Sigma^2 V^{\\top}V) = \\frac{1}{n-1} \\text{Tr}(\\Sigma^2 I_r) = \\frac{1}{n-1} \\text{Tr}(\\Sigma^2) $$\nThe trace of a diagonal matrix is the sum of its diagonal elements, so:\n$$ \\text{Tr}(C) = \\frac{1}{n-1} \\sum_{j=1}^{r} \\sigma_j^2 $$\nThe proportion of variance explained by the $i$-th principal component is the ratio of its eigenvalue $\\lambda_i$ to the total variance $\\text{Tr}(C)$:\n$$ p_i = \\frac{\\lambda_i}{\\text{Tr}(C)} = \\frac{\\frac{\\sigma_i^2}{n-1}}{\\frac{1}{n-1} \\sum_{j=1}^{r} \\sigma_j^2} = \\frac{\\sigma_i^2}{\\sum_{j=1}^{r} \\sigma_j^2} $$\nThe explained variance ratio for the first $k$ principal components is the sum of the individual proportions:\n$$ R(k) = \\sum_{i=1}^{k} p_i = \\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{j=1}^{r} \\sigma_j^2} $$\n\nSecond, we design a criterion for selecting the number of components $k$ based on the broken-stick model.\nThe null hypothesis ($H_0$) is that there is no dominant structure in the data, meaning the total variance is partitioned randomly among the $r$ components. The broken-stick model provides a formalization of this null hypothesis. In this model, a stick of unit length is broken at $r-1$ independent random locations drawn from a Uniform$(0,1)$ distribution, creating $r$ fragments. The lengths of these fragments, when sorted in descending order, represent the expected proportions of variance explained by the ordered principal components under $H_0$.\n\nLet $b_i$ be the expected length of the $i$-th longest fragment for a stick broken into $r$ pieces. The analytical expression for $b_i$ is:\n$$ b_i = \\frac{1}{r} \\sum_{j=i}^{r} \\frac{1}{j} $$\nThese $b_i$ values serve as the benchmark proportions for a dataset with no preferential structure. The selection criterion is constructed by comparing the observed proportion of variance $p_i$ for each component with the expected proportion $b_i$ from the broken-stick model. A component is deemed to capture a significant, non-random structure if its explained variance proportion is greater than what would be expected by chance.\n\nThe criterion is as follows: Retain principal components as long as their observed variance proportion exceeds the proportion predicted by the broken-stick null model. Select the number of components $k$ to be the largest integer such that for all components $j=1, \\dots, k$, the condition $p_j > b_j$ is satisfied. Given that both $p_j$ and $b_j$ are monotonically decreasing sequences, this is equivalent to finding the first component $i$ for which $p_i \\le b_i$, and then selecting $k=i-1$.\n\nThird, we apply this criterion to the given singular values for $r=8$ channels:\n$\\{\\sigma_{i}\\}_{i=1}^{8} = \\{12.3,\\ 7.8,\\ 5.1,\\ 3.3,\\ 2.6,\\ 2.2,\\ 2.1,\\ 2.0\\}$.\n\nStep 1: Calculate the observed variance proportions, $p_i$.\nFirst, we compute the squared singular values, $\\sigma_i^2$:\n$\\sigma_1^2 = 12.3^2 = 151.29$\n$\\sigma_2^2 = 7.8^2 = 60.84$\n$\\sigma_3^2 = 5.1^2 = 26.01$\n$\\sigma_4^2 = 3.3^2 = 10.89$\n$\\sigma_5^2 = 2.6^2 = 6.76$\n$\\sigma_6^2 = 2.2^2 = 4.84$\n$\\sigma_7^2 = 2.1^2 = 4.41$\n$\\sigma_8^2 = 2.0^2 = 4.00$\n\nThe total sum of squares is $\\sum_{j=1}^{8} \\sigma_j^2 = 151.29 + 60.84 + 26.01 + 10.89 + 6.76 + 4.84 + 4.41 + 4.00 = 269.04$.\nThe observed proportions $p_i = \\sigma_i^2 / 269.04$ are:\n$p_1 \\approx 0.5623$\n$p_2 \\approx 0.2261$\n$p_3 \\approx 0.0967$\n$p_4 \\approx 0.0405$\n$p_5 \\approx 0.0251$\n$p_6 \\approx 0.0180$\n$p_7 \\approx 0.0164$\n$p_8 \\approx 0.0149$\n\nStep 2: Calculate the expected broken-stick proportions, $b_i$, for $r=8$.\nUsing the formula $b_i = \\frac{1}{8} \\sum_{j=i}^{8} \\frac{1}{j}$:\n$b_1 = \\frac{1}{8} (\\frac{1}{1} + \\frac{1}{2} + \\dots + \\frac{1}{8}) = \\frac{1}{8}(\\frac{761}{280}) \\approx 0.3397$\n$b_2 = \\frac{1}{8} (\\frac{1}{2} + \\dots + \\frac{1}{8}) \\approx 0.2147$\n$b_3 = \\frac{1}{8} (\\frac{1}{3} + \\dots + \\frac{1}{8}) \\approx 0.1522$\n$b_4 = \\frac{1}{8} (\\frac{1}{4} + \\dots + \\frac{1}{8}) \\approx 0.1106$\n$b_5 = \\frac{1}{8} (\\frac{1}{5} + \\dots + \\frac{1}{8}) \\approx 0.0793$\n$b_6 = \\frac{1}{8} (\\frac{1}{6} + \\frac{1}{7} + \\frac{1}{8}) \\approx 0.0543$\n$b_7 = \\frac{1}{8} (\\frac{1}{7} + \\frac{1}{8}) \\approx 0.0335$\n$b_8 = \\frac{1}{8} (\\frac{1}{8}) \\approx 0.0156$\n\nStep 3: Compare $p_i$ and $b_i$ to determine $k$.\nWe compare the values component by component:\n- For $i=1$: $p_1 \\approx 0.5623 > b_1 \\approx 0.3397$. The condition holds.\n- For $i=2$: $p_2 \\approx 0.2261 > b_2 \\approx 0.2147$. The condition holds.\n- For $i=3$: $p_3 \\approx 0.0967  b_3 \\approx 0.1522$. The condition fails.\n\nSince the condition $p_i > b_i$ fails for the first time at $i=3$, we only retain the components for which the condition held. The condition holds for $j=1$ and $j=2$, but not for $j=3$. Thus, the largest value of $k$ such that $p_j > b_j$ for all $j=1, \\dots, k$ is $2$.\n\nThe selected number of principal components is $k=2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Beyond data analysis, dimensionality reduction is a cornerstone of solving geophysical inverse problems, where we aim to recover a model of the Earth's subsurface from indirect measurements. This practice demonstrates how Truncated Singular Value Decomposition (TSVD) acts as a form of regularization, stabilizing the solution by filtering out components associated with small singular values that amplify noise . By deriving and analyzing the bias-variance tradeoff, you will gain a deep understanding of how to select an optimal truncation level to balance solution stability against fidelity to the true model, a central challenge in geophysical inversion.",
            "id": "3615522",
            "problem": "Consider the linearized geophysical inverse problem with forward operator $G \\in \\mathbb{R}^{m \\times n}$ and data model $d = G m_{\\text{true}} + \\varepsilon$, where $\\varepsilon$ is mean-zero noise with covariance $\\sigma^{2} I_{m}$, and $I_{m}$ is the $m \\times m$ identity matrix. Let the singular value decomposition (SVD) of $G$ be $G = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots, \\sigma_{n})$ has singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{n}  0$. In many discretized geophysical integral operators, the singular values are well-approximated by a power law. Assume a discretization with $n = 60$ and the singular values follow\n$$\n\\sigma_{i} = 2.5 \\, i^{-1.3}, \\quad i = 1, 2, \\dots, 60.\n$$\nThe truncated singular value decomposition estimator with truncation index $k$ is defined by\n$$\n\\hat{m}_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} d}{\\sigma_{i}} \\, v_{i},\n$$\nwhere $u_{i}$ and $v_{i}$ are the left and right singular vectors. To analyze the bias-variance tradeoff, expand $m_{\\text{true}}$ in the right singular vector basis as $m_{\\text{true}} = \\sum_{i=1}^{n} c_{i} v_{i}$. Assume a smoothness prior of order $p = 1$ such that the prior second moments of the coefficients satisfy\n$$\n\\mathbb{E}[c_{i}^{2}] = C^{2} \\, i^{-2p} = C^{2} \\, i^{-2}, \\quad \\text{with } C = 0.8.\n$$\nLet the noise level be $\\sigma = 2.0 \\times 10^{-2}$. Starting from the definitions above and the orthogonality of $U$ and $V$, derive the expected mean-squared reconstruction error $\\mathbb{E}\\!\\left[\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2}\\right]$ as a function of $k$, decompose it into variance and bias terms in the singular vector basis, and determine the selection rule for the optimal truncation index $k^{\\star}$ that minimizes this error in expectation. Finally, compute the numerical value of the optimal truncation index $k^{\\star}$ for the given parameters. Provide the final answer as the single integer $k^{\\star}$.",
            "solution": "The problem statement is a standard formulation of a regularized linear inverse problem in geophysics. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. Therefore, the problem is deemed valid. We proceed with the derivation.\n\nThe objective is to find the optimal truncation index $k^{\\star}$ that minimizes the expected mean-squared reconstruction error, $\\mathbb{E}\\!\\left[\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2}\\right]$.\n\nFirst, we express the reconstruction error vector, $\\hat{m}_{k} - m_{\\text{true}}$, in the basis of the right singular vectors, $\\{v_i\\}_{i=1}^n$.\nThe true model is given by its expansion in this basis:\n$$\nm_{\\text{true}} = \\sum_{i=1}^{n} c_{i} v_{i}\n$$\nThe truncated singular value decomposition (TSVD) estimator is defined as:\n$$\n\\hat{m}_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} d}{\\sigma_{i}} v_{i}\n$$\nWe substitute the data model $d = G m_{\\text{true}} + \\varepsilon$ into the expression for $\\hat{m}_{k}$. The term $u_{i}^{\\top} d$ becomes:\n$$\nu_{i}^{\\top} d = u_{i}^{\\top} (G m_{\\text{true}} + \\varepsilon) = u_{i}^{\\top} G m_{\\text{true}} + u_{i}^{\\top}\\varepsilon\n$$\nUsing the SVD of $G = U \\Sigma V^{\\top} = \\sum_{j=1}^{n} \\sigma_{j} u_{j} v_{j}^{\\top}$, we can evaluate the first term:\n$$\nu_{i}^{\\top} G m_{\\text{true}} = u_{i}^{\\top} \\left( \\sum_{j=1}^{n} \\sigma_{j} u_{j} v_{j}^{\\top} \\right) \\left( \\sum_{l=1}^{n} c_{l} v_{l} \\right)\n$$\nDue to the orthogonality of the singular vectors ($u_i^{\\top}u_j = \\delta_{ij}$ and $v_j^{\\top}v_l = \\delta_{jl}$), this simplifies to:\n$$\nu_{i}^{\\top} G m_{\\text{true}} = \\sum_{j=1}^{n} \\sum_{l=1}^{n} \\sigma_{j} c_{l} (u_{i}^{\\top} u_{j}) (v_{j}^{\\top} v_{l}) = \\sum_{j=1}^{n} \\sum_{l=1}^{n} \\sigma_{j} c_{l} \\delta_{ij} \\delta_{jl} = \\sigma_{i} c_{i}\n$$\nThus, $u_{i}^{\\top} d = \\sigma_{i} c_{i} + u_{i}^{\\top}\\varepsilon$. Substituting this back into the estimator $\\hat{m}_{k}$:\n$$\n\\hat{m}_{k} = \\sum_{i=1}^{k} \\frac{\\sigma_{i} c_{i} + u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} v_{i} = \\sum_{i=1}^{k} \\left( c_{i} + \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} \\right) v_{i}\n$$\nNow, we can write the error vector:\n$$\n\\hat{m}_{k} - m_{\\text{true}} = \\sum_{i=1}^{k} \\left( c_{i} + \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} \\right) v_{i} - \\sum_{i=1}^{n} c_{i} v_{i} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} v_{i} - \\sum_{i=k+1}^{n} c_{i} v_{i}\n$$\nThe error vector is composed of two orthogonal components: one in the subspace spanned by $\\{v_1, \\dots, v_k\\}$ and the other in the subspace spanned by $\\{v_{k+1}, \\dots, v_n\\}$. The first component is due to noise propagation, and the second is due to the truncation of the solution expansion.\n\nNext, we compute the squared $L_2$-norm of the error vector. Due to the orthogonality of the two sums (Pythagorean theorem in function space):\n$$\n\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2} = \\left\\| \\sum_{i=1}^{k} \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} v_{i} \\right\\|_{2}^{2} + \\left\\| \\sum_{i=k+1}^{n} c_{i} v_{i} \\right\\|_{2}^{2}\n$$\nSince $\\{v_i\\}$ is an orthonormal basis, the norm simplifies to the sum of squared coefficients:\n$$\n\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2} = \\sum_{i=1}^{k} \\left( \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} \\right)^{2} + \\sum_{i=k+1}^{n} c_{i}^{2}\n$$\nWe now take the expectation over the noise distribution $\\varepsilon$ and the prior distribution of the true model coefficients $c_i$. The total expected mean-squared error, $E(k)$, is:\n$$\nE(k) = \\mathbb{E}\\!\\left[\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2}\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{k} \\frac{(u_{i}^{\\top}\\varepsilon)^{2}}{\\sigma_{i}^{2}}\\right] + \\mathbb{E}\\left[\\sum_{i=k+1}^{n} c_{i}^{2}\\right]\n$$\nThe expectation operator is linear, so we can move it inside the sums. The terms $c_i$ and $\\varepsilon$ are assumed to be independent.\nFor the first term (variance), we need $\\mathbb{E}[(u_{i}^{\\top}\\varepsilon)^{2}]$. Since $\\varepsilon$ has zero mean, $\\mathbb{E}[u_{i}^{\\top}\\varepsilon] = u_i^{\\top}\\mathbb{E}[\\varepsilon] = 0$. The expression is the variance of the random variable $u_{i}^{\\top}\\varepsilon$.\n$$\n\\mathbb{E}[(u_{i}^{\\top}\\varepsilon)^{2}] = \\text{Var}(u_{i}^{\\top}\\varepsilon) = u_{i}^{\\top} \\text{Cov}(\\varepsilon) u_{i} = u_{i}^{\\top} (\\sigma^{2} I_{m}) u_{i} = \\sigma^{2} (u_{i}^{\\top} u_{i}) = \\sigma^{2}\n$$\nThe first term of $E(k)$ is the variance of the estimator:\n$$\n\\text{Var}(k) = \\sum_{i=1}^{k} \\frac{\\mathbb{E}[(u_{i}^{\\top}\\varepsilon)^{2}]}{\\sigma_{i}^{2}} = \\sum_{i=1}^{k} \\frac{\\sigma^{2}}{\\sigma_{i}^{2}}\n$$\nThe second term of $E(k)$ is the expected squared bias:\n$$\n\\text{Bias}^{2}(k) = \\sum_{i=k+1}^{n} \\mathbb{E}[c_{i}^{2}]\n$$\nThus, the total expected mean-squared error is decomposed as:\n$$\nE(k) = \\underbrace{\\sum_{i=1}^{k} \\frac{\\sigma^{2}}{\\sigma_{i}^{2}}}_{\\text{Variance}} + \\underbrace{\\sum_{i=k+1}^{n} \\mathbb{E}[c_{i}^{2}]}_{\\text{Expected Squared Bias}}\n$$\nWe are given the models $\\sigma_{i} = 2.5 \\, i^{-1.3}$ and $\\mathbb{E}[c_{i}^{2}] = C^{2} i^{-2p}$ with $C=0.8$ and $p=1$. Let's denote $A=2.5$ and $a=1.3$.\n$$\nE(k) = \\sum_{i=1}^{k} \\frac{\\sigma^{2}}{(A i^{-a})^{2}} + \\sum_{i=k+1}^{n} C^{2} i^{-2p} = \\frac{\\sigma^{2}}{A^{2}}\\sum_{i=1}^{k} i^{2a} + C^{2}\\sum_{i=k+1}^{n} i^{-2p}\n$$\nTo find the optimal integer $k^{\\star}$ that minimizes $E(k)$, we examine the difference $E(k+1) - E(k)$:\n$$\nE(k+1) - E(k) = \\left( \\frac{\\sigma^{2}}{A^{2}}\\sum_{i=1}^{k+1} i^{2a} + C^{2}\\sum_{i=k+2}^{n} i^{-2p} \\right) - \\left( \\frac{\\sigma^{2}}{A^{2}}\\sum_{i=1}^{k} i^{2a} + C^{2}\\sum_{i=k+1}^{n} i^{-2p} \\right)\n$$\n$$\nE(k+1) - E(k) = \\frac{\\sigma^{2}}{A^{2}}(k+1)^{2a} - C^{2}(k+1)^{-2p}\n$$\nThe minimum occurs at $k=k^{\\star}$ where the error, after decreasing, begins to increase. This corresponds to the point where the difference $E(k+1)-E(k)$ changes sign from negative to positive. We look for the integer $k$ such that $E(k+1)-E(k) > 0$ and $E(k)-E(k-1)  0$. The optimal $k^{\\star}$ will be close to the value where the continuous approximation of this difference is zero.\n$$\n\\frac{\\sigma^{2}}{A^{2}}(k^{\\star}+1)^{2a} - C^{2}(k^{\\star}+1)^{-2p} \\approx 0 \\implies (k^{\\star}+1)^{2a+2p} \\approx \\frac{C^{2}A^{2}}{\\sigma^{2}}\n$$\nThis gives the selection rule for the optimal truncation index:\n$$\nk^{\\star}+1 \\approx \\left( \\frac{CA}{\\sigma} \\right)^{\\frac{2}{2a+2p}} = \\left( \\frac{CA}{\\sigma} \\right)^{\\frac{1}{a+p}}\n$$\nNow, we substitute the given numerical values:\n$C = 0.8$, $A = 2.5$, $\\sigma = 2.0 \\times 10^{-2} = 0.02$, $a = 1.3$, and $p = 1$.\n$$\n\\frac{CA}{\\sigma} = \\frac{0.8 \\times 2.5}{0.02} = \\frac{2}{0.02} = 100\n$$\n$$\na+p = 1.3 + 1 = 2.3\n$$\n$$\nk^{\\star}+1 \\approx (100)^{\\frac{1}{2.3}} \\approx 10^{\\frac{2}{2.3}} \\approx 10^{0.869565} \\approx 7.4056\n$$\n$$\nk^{\\star} \\approx 6.4056\n$$\nSince $k^{\\star}$ must be an integer, it is likely $6$ or $7$. We must check the sign of the discrete difference $\\Delta(k) = E(k+1)-E(k)$ for these values.\n$$\n\\Delta(k) = \\frac{\\sigma^2}{A^2}(k+1)^{2.6} - C^2(k+1)^{-2} = \\frac{(0.02)^2}{(2.5)^2}(k+1)^{2.6} - (0.8)^2(k+1)^{-2}\n$$\n$$\n\\Delta(k) = (6.4 \\times 10^{-5})(k+1)^{2.6} - 0.64(k+1)^{-2}\n$$\nFor $k=6$:\n$$\n\\Delta(6) = E(7)-E(6) = (6.4 \\times 10^{-5})(7)^{2.6} - 0.64(7)^{-2} \\approx (6.4 \\times 10^{-5})(157.486) - 0.64(0.020408)\n$$\n$$\n\\Delta(6) \\approx 0.010079 - 0.013061 = -0.002982\n$$\nSince $\\Delta(6)  0$, we have $E(7)  E(6)$. The error is still decreasing, so the minimum is at $k^{\\star} \\geq 7$.\n\nFor $k=7$:\n$$\n\\Delta(7) = E(8)-E(7) = (6.4 \\times 10^{-5})(8)^{2.6} - 0.64(8)^{-2} \\approx (6.4 \\times 10^{-5})(222.848) - 0.64(0.015625)\n$$\n$$\n\\Delta(7) \\approx 0.014262 - 0.01 = 0.004262\n$$\nSince $\\Delta(7) > 0$, we have $E(8) > E(7)$. The error starts to increase after $k=7$.\nThe sequence of errors satisfies $E(6) > E(7)$ and $E(7)  E(8)$. Therefore, the minimum expected mean-squared error occurs at the integer truncation index $k^{\\star}=7$.",
            "answer": "$$\\boxed{7}$$"
        }
    ]
}