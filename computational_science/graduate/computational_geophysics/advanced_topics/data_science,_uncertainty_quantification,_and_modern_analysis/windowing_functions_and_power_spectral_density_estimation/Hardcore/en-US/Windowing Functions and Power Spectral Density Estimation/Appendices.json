{
    "hands_on_practices": [
        {
            "introduction": "The first step in robust spectral estimation is selecting an appropriate window function to mitigate spectral leakage. This exercise focuses on the versatile Kaiser window, allowing you to explore the fundamental trade-off between resolving closely spaced spectral features (mainlobe width) and detecting weak signals in the presence of strong ones (sidelobe attenuation). By deriving the Equivalent Noise Bandwidth ($ENBW$) and applying standard design rules, you will gain practical skills in tailoring a window to specific analysis requirements. ",
            "id": "3618894",
            "problem": "A discrete-time geophysical signal, sampled at uniform spacing, is analyzed with a Discrete Fourier Transform (DFT) of length $N$. To mitigate spectral leakage in the estimation of the Power Spectral Density (PSD), the time series is tapered with a window $w[n]$ prior to computing the periodogram. Consider the Kaiser window, defined on the index set $n = 0,1,\\dots,N-1$ by\n$$\nw[n] = \\frac{I_0\\left(\\beta \\sqrt{1 - \\left(\\frac{n - \\alpha}{\\alpha}\\right)^2}\\right)}{I_0(\\beta)}, \\quad \\alpha = \\frac{N-1}{2},\n$$\nwhere $I_0(\\cdot)$ is the modified Bessel function of the first kind and order zero, and $\\beta \\ge 0$ is a tunable shape parameter controlling the trade-off between mainlobe width and sidelobe attenuation. For a given target sidelobe attenuation (specified as a negative decibel level, e.g., $-60$ $\\mathrm{dB}$), a widely accepted design rule relates the magnitude of attenuation to a corresponding choice of $\\beta$.\n\nThe Equivalent Noise Bandwidth (ENBW) of a window quantifies the bandwidth over which white noise is averaged by the window’s spectral response. In continuous frequency, ENBW is defined by\n$$\n\\mathrm{ENBW} = \\frac{\\int_{-\\infty}^{\\infty} \\left|W(f)\\right|^2 \\, df}{\\left|W(0)\\right|^2},\n$$\nwhere $W(f)$ is the Fourier transform of $w[n]$. For DFT-based analysis with bin spacing $\\Delta f$ and a rectangular window baseline for comparison, it is standard to express ENBW in units of DFT bins. Using first principles that relate Parseval’s theorem in the discrete-time setting to the integrated squared window spectrum, derive a discrete-sum expression for the ENBW in bins as a function of $N$ and $w[n]$. Then, using that expression, compute ENBW for the Kaiser window instances in the test suite defined below.\n\nDefine the resolution loss relative to the rectangular window as the ratio of the ENBW (in bins) of the chosen window to that of the rectangular window. For the rectangular window $w[n]=1$, use the discrete-sum expression to show that its ENBW equals $1$ bin, making the resolution loss equal to the ENBW of the chosen window.\n\nYour tasks are:\n1. Given a target sidelobe attenuation in decibels (negative number, e.g., $-60$ $\\mathrm{dB}$), choose the Kaiser parameter $\\beta$ that meets the target using a well-tested design rule that maps attenuation magnitude to $\\beta$.\n2. Using your derived discrete-sum expression, compute the ENBW (in bins) for the Kaiser window of length $N$.\n3. Estimate the resolution loss relative to the rectangular window as the ENBW ratio defined above.\n\nAnswer all numerical quantities as dimensionless floats. Angles do not appear. No physical units are required because ENBW is expressed in bins and resolution loss is a ratio. Round all reported floats to six decimal places.\n\nImplement a program that evaluates the following test suite, where each case is a pair $(S_{\\mathrm{dB}}, N)$:\n- Case 1 (happy path): $(-60.0, 1024)$.\n- Case 2 (boundary of design rule): $(-50.0, 1024)$.\n- Case 3 (edge case yielding $\\beta=0$): $(-10.0, 1024)$.\n- Case 4 (higher attenuation, smaller $N$): $(-80.0, 256)$.\n- Case 5 (very high attenuation, larger $N$): $(-100.0, 4096)$.\n\nFor each case, produce a triple $[\\beta, \\mathrm{ENBW\\_bins}, \\mathrm{resolution\\_loss}]$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each case represented as a nested list of three floats, for example:\n$[[\\beta_1,\\mathrm{ENBW}_1,\\mathrm{loss}_1],[\\beta_2,\\mathrm{ENBW}_2,\\mathrm{loss}_2],\\dots]$.",
            "solution": "The problem requires the derivation and application of a discrete-sum formula for the Equivalent Noise Bandwidth (ENBW) of a window function, specifically the Kaiser window, in the context of spectral analysis. The solution proceeds in three principal stages: first, the derivation of the ENBW formula from fundamental principles; second, the specification of the design rules for the Kaiser window; and third, the application of these results to compute the required quantities for the provided test cases.\n\n### Derivation of the Discrete-Sum Expression for ENBW\n\nThe Equivalent Noise Bandwidth (ENBW) is a measure of a filter's or window's effective bandwidth for noise. The continuous-frequency definition is given as:\n$$\n\\mathrm{ENBW_{Hz}} = \\frac{\\int_{-\\infty}^{\\infty} \\left|W(f)\\right|^2 \\, df}{\\left|W(0)\\right|^2}\n$$\nwhere $W(f)$ is the Fourier Transform of the continuous-time window function $w(t)$. In our discrete-time setting, we are given a window sequence $w[n]$ of length $N$, defined for $n = 0, 1, \\dots, N-1$. The relevant Fourier transform is the Discrete-Time Fourier Transform (DTFT), $W(e^{j\\omega})$, which is periodic in frequency with period $f_s$, the sampling rate. The integral is thus performed over one period, for instance, from $-f_s/2$ to $f_s/2$.\n$$\n\\mathrm{ENBW_{Hz}} = \\frac{\\int_{-f_s/2}^{f_s/2} \\left|W(e^{j2\\pi f T_s})\\right|^2 \\, df}{\\left|W(e^{j0})\\right|^2}\n$$\nwhere $T_s = 1/f_s$ is the sampling period.\n\nThe numerator involves the integral of the squared magnitude of the DTFT. We can relate this integral to the energy of the time-domain sequence using Parseval's theorem for discrete-time signals:\n$$\n\\sum_{n=-\\infty}^{\\infty} \\left|w[n]\\right|^2 = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\left|W(e^{j\\omega})\\right|^2 \\, d\\omega\n$$\nLet's perform a change of variables in the numerator's integral from frequency $f$ to normalized angular frequency $\\omega = 2\\pi f T_s$. This gives $df = d\\omega / (2\\pi T_s)$, and the integration limits change from $[-f_s/2, f_s/2]$ to $[-\\pi, \\pi]$.\n$$\n\\int_{-f_s/2}^{f_s/2} \\left|W(e^{j2\\pi f T_s})\\right|^2 \\, df = \\int_{-\\pi}^{\\pi} \\left|W(e^{j\\omega})\\right|^2 \\frac{d\\omega}{2\\pi T_s} = \\frac{1}{T_s} \\left( \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\left|W(e^{j\\omega})\\right|^2 \\, d\\omega \\right)\n$$\nApplying Parseval's theorem, and noting that $w[n]$ is zero outside $n = 0, \\dots, N-1$, the expression becomes:\n$$\n\\frac{1}{T_s} \\sum_{n=0}^{N-1} \\left|w[n]\\right|^2\n$$\nSince window functions are typically real-valued, this simplifies to $\\frac{1}{T_s} \\sum_{n=0}^{N-1} w[n]^2$.\n\nThe denominator is the squared magnitude of the DTFT at zero frequency ($f=0$, $\\omega=0$), which is the DC component:\n$$\nW(e^{j0}) = \\sum_{n=0}^{N-1} w[n] e^{-j0 \\cdot n} = \\sum_{n=0}^{N-1} w[n]\n$$\nThus, the denominator is $\\left(\\sum_{n=0}^{N-1} w[n]\\right)^2$.\n\nCombining the numerator and denominator, we obtain the ENBW in units of Hz:\n$$\n\\mathrm{ENBW_{Hz}} = \\frac{\\frac{1}{T_s} \\sum_{n=0}^{N-1} w[n]^2}{\\left(\\sum_{n=0}^{N-1} w[n]\\right)^2}\n$$\nThe problem requires the ENBW in units of DFT bins. For an $N$-point DFT, the frequency resolution, or bin spacing, is $\\Delta f = f_s/N$. To convert from Hz to bins, we divide by $\\Delta f$:\n$$\n\\mathrm{ENBW_{bins}} = \\frac{\\mathrm{ENBW_{Hz}}}{\\Delta f} = \\frac{\\mathrm{ENBW_{Hz}}}{f_s/N} = \\frac{N}{f_s} \\mathrm{ENBW_{Hz}}\n$$\nSubstituting the expression for $\\mathrm{ENBW_{Hz}}$:\n$$\n\\mathrm{ENBW_{bins}} = \\frac{N}{f_s} \\frac{\\frac{1}{T_s} \\sum_{n=0}^{N-1} w[n]^2}{\\left(\\sum_{n=0}^{N-1} w[n]\\right)^2}\n$$\nSince $T_s = 1/f_s$, the terms cancel, yielding the final discrete-sum expression for ENBW in bins:\n$$\n\\mathrm{ENBW_{bins}} = N \\frac{\\sum_{n=0}^{N-1} w[n]^2}{\\left(\\sum_{n=0}^{N-1} w[n]\\right)^2}\n$$\nThis is the discrete-sum expression required.\n\n### Validation with the Rectangular Window and Definition of Resolution Loss\n\nFor the rectangular window, $w[n] = 1$ for $n = 0, \\dots, N-1$. We apply the derived formula:\nThe sum is $\\sum_{n=0}^{N-1} w[n] = \\sum_{n=0}^{N-1} 1 = N$.\nThe sum of squares is $\\sum_{n=0}^{N-1} w[n]^2 = \\sum_{n=0}^{N-1} 1^2 = N$.\nSubstituting these into the ENBW formula:\n$$\n\\mathrm{ENBW_{bins, rect}} = N \\frac{N}{N^2} = 1\n$$\nThe ENBW of the rectangular window is exactly $1$ bin. The problem defines resolution loss as the ratio of the ENBW of the chosen window to that of the rectangular window. Therefore:\n$$\n\\mathrm{Resolution\\;Loss} = \\frac{\\mathrm{ENBW_{bins}}(w)}{\\mathrm{ENBW_{bins, rect}}} = \\frac{\\mathrm{ENBW_{bins}}(w)}{1} = \\mathrm{ENBW_{bins}}(w)\n$$\nThus, for any window, the resolution loss is numerically identical to its ENBW expressed in bins.\n\n### Kaiser Window Design and Computational Procedure\n\nThe Kaiser window is defined as\n$$\nw[n] = \\frac{I_0\\left(\\beta \\sqrt{1 - \\left(\\frac{n - \\alpha}{\\alpha}\\right)^2}\\right)}{I_0(\\beta)}, \\quad \\text{for } n = 0, \\dots, N-1\n$$\nwhere $\\alpha = (N-1)/2$, $I_0(\\cdot)$ is the modified Bessel function of the first kind of order zero, and $\\beta$ is the shape parameter. A key property is that if $\\beta=0$, the argument of $I_0$ in the numerator is $0$, and since $I_0(0)=1$, the window simplifies to $w[n]=1$, the rectangular window.\n\nThe parameter $\\beta$ is chosen based on the desired sidelobe attenuation, $A = -S_{\\mathrm{dB}}$, where $S_{\\mathrm{dB}}$ is the attenuation in decibels (a negative value). A standard set of empirical formulas is:\n$$ \\beta = \\begin{cases} 0,  A \\le 21 \\\\ 0.5842(A - 21)^{0.4} + 0.07886(A - 21),  21  A \\le 50 \\\\ 0.1102(A - 8.7),  A  50 \\end{cases} $$\n\nThe computational procedure for each test case $(S_{\\mathrm{dB}}, N)$ is as follows:\n1.  Calculate the positive attenuation $A = -S_{\\mathrm{dB}}$.\n2.  Determine the Kaiser parameter $\\beta$ using the piecewise formula above.\n3.  Generate the $N$-point Kaiser window sequence $w[n]$ using the calculated $\\beta$ and the given $N$. The function $I_0(x)$ is evaluated using standard numerical libraries.\n4.  Compute the sum of window samples, $\\sum_{n=0}^{N-1} w[n]$, and the sum of squared window samples, $\\sum_{n=0}^{N-1} w[n]^2$.\n5.  Calculate the ENBW in bins using the derived formula: $\\mathrm{ENBW_{bins}} = N \\frac{\\sum w[n]^2}{(\\sum w[n])^2}$.\n6.  The resolution loss is equal to the computed $\\mathrm{ENBW_{bins}}$.\n7.  The final results $[\\beta, \\mathrm{ENBW_{bins}}, \\mathrm{Resolution\\;Loss}]$ are rounded to six decimal places.\n\nThis procedure is implemented to solve for the specified test suite.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import i0\n\ndef solve():\n    \"\"\"\n    Computes Kaiser window parameters, ENBW, and resolution loss for a given\n    test suite of sidelobe attenuations and window lengths.\n    \"\"\"\n    # Test suite format: (S_dB, N)\n    # S_dB: Target sidelobe attenuation in decibels\n    # N: Window length\n    test_cases = [\n        (-60.0, 1024),\n        (-50.0, 1024),\n        (-10.0, 1024),\n        (-80.0, 256),\n        (-100.0, 4096),\n    ]\n\n    results = []\n    \n    for s_db, n in test_cases:\n        # 1. Calculate positive attenuation A\n        A = -s_db\n\n        # 2. Determine Kaiser parameter beta from attenuation A\n        if A = 21:\n            beta = 0.0\n        elif A = 50:\n            beta = 0.5842 * (A - 21)**0.4 + 0.07886 * (A - 21)\n        else: # A  50\n            beta = 0.1102 * (A - 8.7)\n        \n        # 3. Handle the special case of beta=0 (rectangular window)\n        if beta == 0.0:\n            enbw_bins = 1.0\n            resolution_loss = 1.0\n        else:\n            # 4. Generate the N-point Kaiser window sequence\n            # Create the index array for the window\n            indices = np.arange(n)\n            alpha = (n - 1) / 2.0\n            \n            # Argument for the Bessel function in the numerator\n            # The argument inside the sqrt is 1 - ((n-alpha)/alpha)^2\n            # which is mathematically guaranteed to be non-negative.\n            # Using floating point arithmetic, it should remain so.\n            arg = beta * np.sqrt(1 - ((indices - alpha) / alpha)**2)\n            \n            window = i0(arg) / i0(beta)\n            \n            # 5. Compute sums needed for ENBW calculation\n            sum_w = np.sum(window)\n            sum_w_sq = np.sum(window**2)\n            \n            # 6. Calculate ENBW in bins using the derived discrete-sum formula\n            enbw_bins = n * sum_w_sq / (sum_w**2)\n            \n            # 7. Resolution loss equals ENBW in bins\n            resolution_loss = enbw_bins\n\n        # 8. Round results to six decimal places and store them\n        beta_rounded = round(beta, 6)\n        enbw_rounded = round(enbw_bins, 6)\n        loss_rounded = round(resolution_loss, 6)\n        \n        results.append([beta_rounded, enbw_rounded, loss_rounded])\n\n    # Format the final output string as specified\n    case_strings = []\n    for r in results:\n        # Creates a string like \"[val1,val2,val3]\" for each case\n        case_strings.append(f\"[{r[0]},{r[1]},{r[2]}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A Power Spectral Density ($PSD$) estimate is not just a qualitative picture; it is a quantitative tool. For a $PSD$ to be physically meaningful, it must be correctly normalized to reflect the signal's power or variance, a property often called Parseval consistency. This practice guides you through demonstrating this consistency for a properly scaled estimator and quantifies the significant errors that arise from common but incorrect normalization schemes. ",
            "id": "3618920",
            "problem": "Consider a discrete-time, zero-mean, wide-sense stationary geophysical signal $x[n]$ with variance $\\sigma_{x}^{2}$, sampled at frequency $f_{s}$, and observed over $N=1001$ samples indexed by $n=0,1,\\dots,N-1$. A generic taper (window) $w[n]$ is applied to the record to form $x_{w}[n]=w[n]x[n]$. Define the discrete-time Fourier transform of the windowed record as\n$$\nX_{w}(\\omega)=\\sum_{n=0}^{N-1} w[n]\\,x[n]\\,\\exp(-\\mathrm{i}\\,\\omega n),\n$$\nwith angular frequency $\\omega\\in[-\\pi,\\pi]$. By Parseval’s theorem,\n$$\n\\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi}\\left|X_{w}(\\omega)\\right|^{2}\\,\\mathrm{d}\\omega=\\sum_{n=0}^{N-1}\\left|w[n]x[n]\\right|^{2}.\n$$\nTwo distinct power spectral density (PSD) estimators are considered.\n\n1. An energy-normalized estimator,\n$$\n\\widehat{S}_{U}(\\omega)=\\frac{1}{2\\pi\\,N\\,U}\\,\\left|X_{w}(\\omega)\\right|^{2},\\quad U=\\frac{1}{N}\\sum_{n=0}^{N-1}w[n]^{2},\n$$\nwhose integral over $\\omega\\in[-\\pi,\\pi]$ is used to recover the time-domain variance.\n\n2. A practitioner’s estimator that attempts to correct the window’s amplitude response via the coherent gain $G_{c}$ but (incorrectly) sets the equivalent noise bandwidth to unity,\n$$\n\\widehat{S}_{B}(\\omega)=\\frac{1}{2\\pi\\,N\\,G_{c}^{2}}\\,\\left|X_{w}(\\omega)\\right|^{2},\\quad G_{c}=\\frac{1}{N}\\sum_{n=0}^{N-1}w[n],\n$$\ni.e., this choice implicitly takes $B_{e}=1$ in discrete-frequency bins.\n\nTasks:\n- Starting from the above definitions and Parseval’s theorem, demonstrate that $\\int_{-\\pi}^{\\pi}\\widehat{S}_{U}(\\omega)\\,\\mathrm{d}\\omega$ equals the time-domain variance in expectation for any window $w[n]$ when $x[n]$ is zero-mean and wide-sense stationary.\n- For the “$G_{c}$-only” practitioner’s estimator $\\widehat{S}_{B}(\\omega)$, derive the multiplicative deviation factor\n$$\nD_{w}=\\frac{\\mathbb{E}\\left[\\int_{-\\pi}^{\\pi}\\widehat{S}_{B}(\\omega)\\,\\mathrm{d}\\omega\\right]}{\\sigma_{x}^{2}},\n$$\nin terms of $w[n]$ and $N$. Then, using $N=1001$ and the following windows defined on $n=0,1,\\dots,N-1$ with $\\theta_{n}=\\frac{2\\pi n}{N-1}$,\n- Rectangular: $w[n]=1$,\n- Hann: $w[n]=\\frac{1}{2}\\left(1-\\cos\\theta_{n}\\right)$,\n- Hamming: $w[n]=0.54-0.46\\cos\\theta_{n}$,\n- Blackman: $w[n]=0.42-0.50\\cos\\theta_{n}+0.08\\cos(2\\theta_{n})$,\ncompute $D_{w}$ for each window.\n\nReport the four dimensionless values $\\left(D_{\\text{rect}},D_{\\text{hann}},D_{\\text{hamming}},D_{\\text{blackman}}\\right)$, rounded to four significant figures. No units are required. Angles, where relevant, are in radians.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard exercise in digital signal processing, specifically concerning the properties of power spectral density estimators under different windowing functions. All required definitions and data are provided.\n\nFirst, we demonstrate that the total power obtained by integrating the energy-normalized estimator $\\widehat{S}_{U}(\\omega)$ is, in expectation, equal to the signal variance $\\sigma_{x}^{2}$. We begin with the integral of $\\widehat{S}_{U}(\\omega)$ over its frequency domain $\\omega \\in [-\\pi, \\pi]$:\n$$\n\\int_{-\\pi}^{\\pi}\\widehat{S}_{U}(\\omega)\\,\\mathrm{d}\\omega = \\int_{-\\pi}^{\\pi}\\frac{1}{2\\pi\\,N\\,U}\\,\\left|X_{w}(\\omega)\\right|^{2}\\,\\mathrm{d}\\omega\n$$\nThe terms $\\frac{1}{N\\,U}$ are constant with respect to $\\omega$ and can be factored out of the integral:\n$$\n\\int_{-\\pi}^{\\pi}\\widehat{S}_{U}(\\omega)\\,\\mathrm{d}\\omega = \\frac{1}{N\\,U} \\left( \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi}\\left|X_{w}(\\omega)\\right|^{2}\\,\\mathrm{d}\\omega \\right)\n$$\nUsing the provided Parseval’s theorem, $\\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi}\\left|X_{w}(\\omega)\\right|^{2}\\,\\mathrm{d}\\omega=\\sum_{n=0}^{N-1}\\left|w[n]x[n]\\right|^{2}$, we substitute the sum for the integral:\n$$\n\\int_{-\\pi}^{\\pi}\\widehat{S}_{U}(\\omega)\\,\\mathrm{d}\\omega = \\frac{1}{N\\,U} \\sum_{n=0}^{N-1}\\left(w[n]x[n]\\right)^{2}\n$$\nNext, we take the expectation $\\mathbb{E}[\\cdot]$ of this quantity. Since the window $w[n]$ and the normalization factor $U$ are deterministic, the expectation operator applies only to the signal term $x[n]$:\n$$\n\\mathbb{E}\\left[\\int_{-\\pi}^{\\pi}\\widehat{S}_{U}(\\omega)\\,\\mathrm{d}\\omega\\right] = \\frac{1}{N\\,U} \\sum_{n=0}^{N-1}w[n]^{2}\\mathbb{E}\\left[x[n]^{2}\\right]\n$$\nThe signal $x[n]$ is defined as zero-mean and wide-sense stationary (WSS). For a zero-mean process, the variance $\\sigma_{x}^{2}$ is equal to the mean square value: $\\sigma_{x}^{2} = \\mathbb{E}[x[n]^2] - (\\mathbb{E}[x[n]])^2 = \\mathbb{E}[x[n]^2] - 0^2 = \\mathbb{E}[x[n]^2]$. The WSS property ensures that this variance is constant for all $n$. Substituting $\\mathbb{E}[x[n]^2] = \\sigma_{x}^{2}$:\n$$\n\\mathbb{E}\\left[\\int_{-\\pi}^{\\pi}\\widehat{S}_{U}(\\omega)\\,\\mathrm{d}\\omega\\right] = \\frac{1}{N\\,U} \\sum_{n=0}^{N-1}w[n]^{2}\\sigma_{x}^{2} = \\frac{\\sigma_{x}^{2}}{N\\,U} \\sum_{n=0}^{N-1}w[n]^{2}\n$$\nNow, we substitute the definition of the energy normalization factor, $U=\\frac{1}{N}\\sum_{n=0}^{N-1}w[n]^{2}$:\n$$\n\\mathbb{E}\\left[\\int_{-\\pi}^{\\pi}\\widehat{S}_{U}(\\omega)\\,\\mathrm{d}\\omega\\right] = \\frac{\\sigma_{x}^{2}}{N\\left(\\frac{1}{N}\\sum_{n=0}^{N-1}w[n]^{2}\\right)} \\sum_{n=0}^{N-1}w[n]^{2} = \\frac{\\sigma_{x}^{2}}{\\sum_{n=0}^{N-1}w[n]^{2}} \\sum_{n=0}^{N-1}w[n]^{2} = \\sigma_{x}^{2}\n$$\nThis completes the demonstration.\n\nNext, we derive the multiplicative deviation factor $D_{w}$ for the practitioner's estimator $\\widehat{S}_{B}(\\omega)$. The procedure is analogous. We start with the expected value of the integrated spectrum:\n$$\n\\mathbb{E}\\left[\\int_{-\\pi}^{\\pi}\\widehat{S}_{B}(\\omega)\\,\\mathrm{d}\\omega\\right] = \\mathbb{E}\\left[\\int_{-\\pi}^{\\pi}\\frac{1}{2\\pi\\,N\\,G_{c}^{2}}\\,\\left|X_{w}(\\omega)\\right|^{2}\\,\\mathrm{d}\\omega\\right]\n$$\nFollowing the same steps of applying Parseval's theorem and taking the expectation:\n$$\n\\mathbb{E}\\left[\\int_{-\\pi}^{\\pi}\\widehat{S}_{B}(\\omega)\\,\\mathrm{d}\\omega\\right] = \\frac{1}{N\\,G_{c}^{2}} \\mathbb{E}\\left[\\sum_{n=0}^{N-1}\\left(w[n]x[n]\\right)^{2}\\right] = \\frac{1}{N\\,G_{c}^{2}} \\sum_{n=0}^{N-1}w[n]^{2}\\mathbb{E}\\left[x[n]^{2}\\right] = \\frac{\\sigma_{x}^{2}}{N\\,G_{c}^{2}} \\sum_{n=0}^{N-1}w[n]^{2}\n$$\nThe deviation factor $D_{w}$ is defined as the ratio of this expected total power to the true variance $\\sigma_{x}^{2}$:\n$$\nD_{w} = \\frac{\\mathbb{E}\\left[\\int_{-\\pi}^{\\pi}\\widehat{S}_{B}(\\omega)\\,\\mathrm{d}\\omega\\right]}{\\sigma_{x}^{2}} = \\frac{\\frac{\\sigma_{x}^{2}}{N\\,G_{c}^{2}} \\sum_{n=0}^{N-1}w[n]^{2}}{\\sigma_{x}^{2}} = \\frac{1}{N\\,G_{c}^{2}} \\sum_{n=0}^{N-1}w[n]^{2}\n$$\nSubstituting the definition of the coherent gain, $G_{c}=\\frac{1}{N}\\sum_{n=0}^{N-1}w[n]$:\n$$\nD_{w} = \\frac{\\sum_{n=0}^{N-1}w[n]^{2}}{N \\left( \\frac{1}{N}\\sum_{n=0}^{N-1}w[n] \\right)^{2}} = \\frac{N \\sum_{n=0}^{N-1}w[n]^{2}}{\\left(\\sum_{n=0}^{N-1}w[n]\\right)^{2}}\n$$\nThis is the general expression for $D_w$. We now compute this for each window with $N=1001$. Let $M=N-1=1000$ and $\\theta_n = \\frac{2\\pi n}{M}$. We utilize the summation identity $\\sum_{n=0}^{M} \\cos(k\\theta_n) = 1$ for integer $k$ not divisible by $M$.\n\n1.  **Rectangular window**: $w[n]=1$ for $n=0, \\dots, N-1$.\n    $\\sum_{n=0}^{N-1} w[n] = \\sum_{n=0}^{N-1} 1 = N$.\n    $\\sum_{n=0}^{N-1} w[n]^2 = \\sum_{n=0}^{N-1} 1^2 = N$.\n    $D_{\\text{rect}} = \\frac{N \\cdot N}{N^2} = 1$.\n\n2.  **Hann window**: $w[n]=\\frac{1}{2}\\left(1-\\cos\\theta_{n}\\right)$.\n    $\\sum_{n=0}^{N-1} w[n] = \\frac{1}{2}\\sum_{n=0}^{M} (1-\\cos\\theta_n) = \\frac{1}{2}\\left(\\sum_0^M 1 - \\sum_0^M \\cos\\theta_n\\right) = \\frac{1}{2}(N-1) = \\frac{M}{2}$.\n    $\\sum_{n=0}^{N-1} w[n]^2 = \\frac{1}{4}\\sum_{n=0}^{M} (1-\\cos\\theta_n)^2 = \\frac{1}{4}\\sum (1 - 2\\cos\\theta_n + \\cos^2\\theta_n)$.\n    Using $\\cos^2\\theta_n = \\frac{1}{2}(1+\\cos(2\\theta_n))$, the sum becomes $\\frac{1}{4}\\sum( \\frac{3}{2} - 2\\cos\\theta_n + \\frac{1}{2}\\cos(2\\theta_n) ) = \\frac{1}{4}(\\frac{3}{2}N - 2(1) + \\frac{1}{2}(1)) = \\frac{1}{4}(\\frac{3N-3}{2}) = \\frac{3(N-1)}{8}$.\n    $D_{\\text{hann}} = \\frac{N \\cdot \\frac{3(N-1)}{8}}{(\\frac{N-1}{2})^2} = \\frac{3N(N-1)/8}{(N-1)^2/4} = \\frac{3N}{2(N-1)}$.\n    For $N=1001$, $D_{\\text{hann}} = \\frac{3 \\cdot 1001}{2 \\cdot 1000} = \\frac{3003}{2000} = 1.5015$.\n\n3.  **Hamming window**: $w[n]=0.54-0.46\\cos\\theta_{n}$. Let $a=0.54, b=0.46$.\n    $\\sum w[n] = \\sum(a-b\\cos\\theta_n) = aN - b(1) = 0.54(1001) - 0.46 = 540.08$.\n    $\\sum w[n]^2 = \\sum(a^2 - 2ab\\cos\\theta_n + b^2\\cos^2\\theta_n) = a^2N - 2ab(1) + b^2\\sum \\frac{1+\\cos(2\\theta_n)}{2} = a^2N - 2ab + b^2(\\frac{N+1}{2})$.\n    $= (0.54)^2(1001) - 2(0.54)(0.46) + (0.46)^2(\\frac{1002}{2}) = 0.2916(1001) - 0.4968 + 0.2116(501) = 291.8916 - 0.4968 + 106.0116 = 397.4064$.\n    $D_{\\text{hamming}} = \\frac{1001 \\cdot 397.4064}{(540.08)^2} = \\frac{397803.8064}{291686.4064} \\approx 1.363806$.\n\n4.  **Blackman window**: $w[n]=0.42-0.50\\cos\\theta_{n}+0.08\\cos(2\\theta_{n})$. Let $a_0=0.42, a_1=0.50, a_2=0.08$.\n    $\\sum w[n] = \\sum (a_0 - a_1\\cos\\theta_n + a_2\\cos(2\\theta_n)) = a_0N - a_1(1) + a_2(1) = 0.42(1001) - 0.50 + 0.08 = 420.0$.\n    $\\sum w[n]^2 = \\sum (a_0 - a_1\\cos\\theta_n + a_2\\cos(2\\theta_n))^2$. Expanding gives terms with sums of $1$, $\\cos(k\\theta_n)$, $\\cos^2(k\\theta_n)$ and cross products $\\cos(j\\theta_n)\\cos(k\\theta_n)$. Using the summation identities:\n    $\\sum w[n]^2 = a_0^2N + (a_1^2+a_2^2)\\frac{N+1}{2} - 2(a_0a_1 - a_0a_2 + a_1a_2)$.\n    $= (0.42)^2(1001) + (0.50^2+0.08^2)\\frac{1002}{2} - 2(0.42 \\cdot 0.50 - 0.42 \\cdot 0.08 + 0.50 \\cdot 0.08)$.\n    $= 0.1764(1001) + (0.25+0.0064)(501) - 2(0.21 - 0.0336 + 0.04)$.\n    $= 176.5764 + (0.2564)(501) - 2(0.2164) = 176.5764 + 128.4564 - 0.4328 = 304.6$.\n    $D_{\\text{blackman}} = \\frac{1001 \\cdot 304.6}{(420.0)^2} = \\frac{304904.6}{176400} \\approx 1.728484$.\n\nRounding the values to four significant figures:\n- $D_{\\text{rect}} = 1.000$\n- $D_{\\text{hann}} = 1.5015 \\to 1.502$\n- $D_{\\text{hamming}} = 1.363806... \\to 1.364$\n- $D_{\\text{blackman}} = 1.728484... \\to 1.728$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.000  1.502  1.364  1.728\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While averaged periodograms provide a consistent estimate of the Power Spectral Density ($PSD$), their statistical properties can be subtle, especially when viewed on a logarithmic scale common in geophysics. This exercise delves into the statistical nature of $PSD$ estimators, revealing the inherent bias introduced by the logarithm operation on chi-square distributed periodogram values. You will derive this bias from first principles and formulate a correction term, a crucial step towards creating statistically unbiased log-$PSD$ estimates. ",
            "id": "3618912",
            "problem": "Consider a zero-mean, wide-sense stationary Gaussian seismic trace $x[n]$ sampled at rate $f_{s}$, with continuous Power Spectral Density (PSD) $S(f)$. Let $w[n]$, $n=0,\\ldots,N-1$, be a deterministic taper (window) normalized so that $\\sum_{n=0}^{N-1} w[n]^{2} = 1$. Define the windowed Discrete Fourier Transform (DFT) at frequency $f$ by $X_{w}(f) = \\sum_{n=0}^{N-1} w[n]\\,x[n]\\,\\exp(-\\mathrm{i}\\,2\\pi f n/f_{s})$, and the corresponding tapered periodogram by $I_{w}(f) = |X_{w}(f)|^{2}$. Assume the large-sample regime where, at a fixed frequency $f$ away from spectral lines and under negligible leakage, the real and imaginary parts of $X_{w}(f)$ are independent zero-mean Gaussian random variables with variance proportional to $S(f)$, so that $I_{w}(f)/S(f)$ has the same distribution as a scaled chi-square with two degrees of freedom.\n\nYou form $K$ independent tapered periodograms $\\{I_{w,k}(f)\\}_{k=1}^{K}$ by segmenting the trace into $K$ blocks, each of length $N$, applying the same window $w[n]$ on each block, and evaluating the periodogram at the same frequency $f$. Assume across $k$, the $I_{w,k}(f)$ are independent, and that for each $k$, $I_{w,k}(f)/S(f)$ has the same distribution as $\\chi^{2}_{2}/2$. Consider two log-based PSD estimators at frequency $f$:\n\n1. The average of log-periodograms: $L_{\\mathrm{avglog}}(f) = \\frac{1}{K}\\sum_{k=1}^{K} \\ln I_{w,k}(f)$.\n\n2. The log of the averaged periodogram: $L_{\\log\\mathrm{avg}}(f) = \\ln\\!\\left(\\frac{1}{K}\\sum_{k=1}^{K} I_{w,k}(f)\\right)$.\n\nBegin from the definitions above, and the well-tested fact that a sum of independent chi-square random variables is chi-square with degrees of freedom equal to the sum, and that a chi-square distribution with $\\nu$ degrees of freedom is a Gamma distribution with shape $\\nu/2$ and scale $2$. Do not use any pre-compiled formulas for expectations of log-chi-square; instead, derive them from first principles using the Gamma representation.\n\n(a) For each estimator, express the bias $b(f)$ defined by $b(f) = \\mathbb{E}[L(f)] - \\ln S(f)$ explicitly in terms of an expectation of $\\ln(\\chi^{2}_{\\nu}/\\nu)$ with an appropriate $\\nu$.\n\n(b) Derive a closed-form analytic expression for $\\mathbb{E}[\\ln(\\chi^{2}_{\\nu}/\\nu)]$ in terms of the digamma function $\\psi(\\cdot)$ and natural logarithms.\n\n(c) Using your result from part (b), propose a debiased log-PSD estimator $\\widetilde{L}(f)$ of the form $\\widetilde{L}(f) = L(f) - c_{\\nu}$, where $c_{\\nu}$ is a correction term depending only on the degrees of freedom $\\nu$ of the chi-square ratio appearing in part (a). State $c_{\\nu}$ explicitly.\n\nYour final answer must be the single closed-form analytic expression for $\\mathbb{E}[\\ln(\\chi^{2}_{\\nu}/\\nu)]$ that you derive in part (b). No numerical evaluation is required; express the answer exactly using standard special functions. If you introduce any acronyms, spell them out upon first use. All mathematical entities must be written in LaTeX. The final boxed answer must be dimensionless and contain no units.",
            "solution": "We begin with the setting of a zero-mean, wide-sense stationary Gaussian time series $x[n]$. Under the large-$N$ assumptions and appropriate taper normalization, the windowed Discrete Fourier Transform (DFT) $X_{w}(f)$ at a fixed frequency $f$ has real and imaginary parts that are independent Gaussian random variables with mean zero and variance proportional to the Power Spectral Density (PSD) $S(f)$. Consequently, the tapered periodogram $I_{w}(f) = |X_{w}(f)|^{2}$ satisfies\n$$\n\\frac{I_{w}(f)}{S(f)} \\stackrel{d}{=} \\frac{1}{2}\\,\\chi^{2}_{2},\n$$\nwhere $\\chi^{2}_{2}$ denotes a chi-square random variable with two degrees of freedom, and $\\stackrel{d}{=}$ denotes equality in distribution. This scaled chi-square characterization follows from the fact that the sum of squares of two independent zero-mean Gaussian variables yields a chi-square with two degrees of freedom, and the scale factor $1/2$ arises from the particular variance normalization inherited from the taper energy $\\sum_{n} w[n]^{2} = 1$.\n\nWe construct $K$ independent tapered periodograms $I_{w,k}(f)$ by segmenting the trace into $K$ blocks and applying the same taper to each block. The independence across $k$ follows from disjoint blocking and the Gaussian setting; hence each $I_{w,k}(f)$ obeys the same scaled chi-square ratio with $S(f)$.\n\nWe consider two log-based estimators:\n$$\nL_{\\mathrm{avglog}}(f) = \\frac{1}{K}\\sum_{k=1}^{K} \\ln I_{w,k}(f),\n\\qquad\nL_{\\log\\mathrm{avg}}(f) = \\ln\\!\\left(\\frac{1}{K}\\sum_{k=1}^{K} I_{w,k}(f)\\right).\n$$\n\nPart (a): Express the bias in terms of $\\mathbb{E}[\\ln(\\chi^{2}_{\\nu}/\\nu)]$.\n\nWe compute the bias $b(f) = \\mathbb{E}[L(f)] - \\ln S(f)$ for each estimator.\n\nFor $L_{\\mathrm{avglog}}(f)$, linearity of expectation yields\n$$\n\\mathbb{E}[L_{\\mathrm{avglog}}(f)] = \\frac{1}{K}\\sum_{k=1}^{K}\\mathbb{E}[\\ln I_{w,k}(f)] = \\mathbb{E}[\\ln I_{w,1}(f)].\n$$\nUsing the distributional identity, for any $k$,\n$$\n\\ln I_{w,k}(f) = \\ln S(f) + \\ln\\!\\left(\\frac{1}{2}\\,\\chi^{2}_{2}\\right).\n$$\nHence\n$$\n\\mathbb{E}[L_{\\mathrm{avglog}}(f)] = \\ln S(f) + \\mathbb{E}\\!\\left[\\ln\\!\\left(\\frac{\\chi^{2}_{2}}{2}\\right)\\right],\n$$\nso the bias is\n$$\nb_{\\mathrm{avglog}}(f) = \\mathbb{E}\\!\\left[\\ln\\!\\left(\\frac{\\chi^{2}_{2}}{2}\\right)\\right].\n$$\nThis is of the form $\\mathbb{E}[\\ln(\\chi^{2}_{\\nu}/\\nu)]$ with $\\nu=2$.\n\nFor $L_{\\log\\mathrm{avg}}(f)$, define the averaged periodogram\n$$\n\\overline{I}_{w}(f) = \\frac{1}{K}\\sum_{k=1}^{K} I_{w,k}(f).\n$$\nBecause a sum of independent chi-square random variables is chi-square with degrees of freedom equal to the sum, we have\n$$\n\\sum_{k=1}^{K} \\frac{I_{w,k}(f)}{S(f)} \\stackrel{d}{=} \\sum_{k=1}^{K} \\frac{1}{2}\\,\\chi^{2}_{2} \\stackrel{d}{=} \\frac{1}{2}\\,\\chi^{2}_{2K}.\n$$\nTherefore,\n$$\n\\frac{\\overline{I}_{w}(f)}{S(f)} = \\frac{1}{K}\\sum_{k=1}^{K} \\frac{I_{w,k}(f)}{S(f)} \\stackrel{d}{=} \\frac{1}{K}\\cdot \\frac{1}{2}\\,\\chi^{2}_{2K} = \\frac{1}{2K}\\,\\chi^{2}_{2K}.\n$$\nTaking logarithms,\n$$\nL_{\\log\\mathrm{avg}}(f) = \\ln \\overline{I}_{w}(f) = \\ln S(f) + \\ln\\!\\left(\\frac{1}{2K}\\,\\chi^{2}_{2K}\\right),\n$$\nso\n$$\n\\mathbb{E}[L_{\\log\\mathrm{avg}}(f)] = \\ln S(f) + \\mathbb{E}\\!\\left[\\ln\\!\\left(\\frac{\\chi^{2}_{2K}}{2K}\\right)\\right],\n$$\nand the bias is\n$$\nb_{\\log\\mathrm{avg}}(f) = \\mathbb{E}\\!\\left[\\ln\\!\\left(\\frac{\\chi^{2}_{2K}}{2K}\\right)\\right].\n$$\nThis is of the general form $\\mathbb{E}[\\ln(\\chi^{2}_{\\nu}/\\nu)]$ with $\\nu = 2K$.\n\nThus, both estimators admit a bias expressed by $\\mathbb{E}[\\ln(\\chi^{2}_{\\nu}/\\nu)]$ for appropriate $\\nu$.\n\nPart (b): Derive $\\mathbb{E}[\\ln(\\chi^{2}_{\\nu}/\\nu)]$ in closed form.\n\nA chi-square random variable with $\\nu$ degrees of freedom can be represented in terms of a Gamma random variable. Specifically,\n$$\n\\chi^{2}_{\\nu} \\stackrel{d}{=} 2\\,G,\n$$\nwhere $G \\sim \\mathrm{Gamma}(k=\\nu/2,\\ \\theta=1)$ has probability density function\n$$\nf_{G}(g) = \\frac{1}{\\Gamma(\\nu/2)}\\, g^{\\frac{\\nu}{2}-1}\\,\\exp(-g), \\quad g > 0,\n$$\nwith shape parameter $k=\\nu/2$ and scale parameter $\\theta=1$. Using this representation,\n$$\n\\ln\\!\\left(\\frac{\\chi^{2}_{\\nu}}{\\nu}\\right) = \\ln(2G) - \\ln \\nu = \\ln G + \\ln 2 - \\ln \\nu.\n$$\nTherefore,\n$$\n\\mathbb{E}\\!\\left[\\ln\\!\\left(\\frac{\\chi^{2}_{\\nu}}{\\nu}\\right)\\right] = \\mathbb{E}[\\ln G] + \\ln 2 - \\ln \\nu.\n$$\nIt is a standard consequence of the definition of the digamma function $\\psi(\\cdot)$ that for $G \\sim \\mathrm{Gamma}(k,\\theta)$,\n$$\n\\mathbb{E}[\\ln G] = \\psi(k) + \\ln \\theta.\n$$\nHere, $k = \\nu/2$ and $\\theta = 1$, so\n$$\n\\mathbb{E}[\\ln G] = \\psi\\!\\left(\\frac{\\nu}{2}\\right) + \\ln 1 = \\psi\\!\\left(\\frac{\\nu}{2}\\right).\n$$\nSubstituting back,\n$$\n\\mathbb{E}\\!\\left[\\ln\\!\\left(\\frac{\\chi^{2}_{\\nu}}{\\nu}\\right)\\right] = \\psi\\!\\left(\\frac{\\nu}{2}\\right) + \\ln 2 - \\ln \\nu = \\psi\\!\\left(\\frac{\\nu}{2}\\right) - \\ln\\!\\left(\\frac{\\nu}{2}\\right).\n$$\nThis is the desired closed-form analytic expression in terms of the digamma function and natural logarithms.\n\nPart (c): Propose debiased log-PSD estimators.\n\nFrom part (a), for either log-based estimator $L(f)$, the bias is $b(f) = \\mathbb{E}[\\ln(\\chi^{2}_{\\nu}/\\nu)]$ with the appropriate $\\nu$. Using the result of part (b), the correction term is\n$$\nc_{\\nu} = \\psi\\!\\left(\\frac{\\nu}{2}\\right) - \\ln\\!\\left(\\frac{\\nu}{2}\\right).\n$$\nThus, a debiased log-PSD estimator can be formed as\n$$\n\\widetilde{L}(f) = L(f) - c_{\\nu}.\n$$\nFor the average of log-periodograms, each term involves $\\nu=2$, so $c_{2} = \\psi(1) - \\ln(1) = \\psi(1)$; since $\\psi(1) = -\\gamma$ where $\\gamma$ is the Euler–Mascheroni constant, the debiasing adds $\\gamma$. For the log of the averaged periodogram, $\\nu = 2K$, so $c_{2K} = \\psi(K) - \\ln K$. In both cases, subtracting $c_{\\nu}$ yields an estimator whose expectation equals $\\ln S(f)$ under the stated assumptions, thereby removing the bias introduced by the logarithm.\n\nThe final requested expression is the general correction term identified in part (b).",
            "answer": "$$\\boxed{\\psi\\!\\left(\\frac{\\nu}{2}\\right)\\;-\\;\\ln\\!\\left(\\frac{\\nu}{2}\\right)}$$"
        }
    ]
}