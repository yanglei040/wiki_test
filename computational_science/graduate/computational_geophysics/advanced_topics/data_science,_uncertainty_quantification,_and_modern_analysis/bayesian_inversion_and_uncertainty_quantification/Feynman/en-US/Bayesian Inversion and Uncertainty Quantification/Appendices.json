{
    "hands_on_practices": [
        {
            "introduction": "Before deploying sophisticated gradient-based algorithms like Hamiltonian Monte Carlo or variational inference, we must ensure our gradient computations are correct. This exercise grounds your understanding in the adjoint method, the computational workhorse for efficiently calculating gradients in large-scale inversion, and verifies its implementation using a Taylor test. By confirming that your analytical gradient matches a finite-difference approximation, you build the foundational trust required for all subsequent Bayesian computations .",
            "id": "3577517",
            "problem": "You are given a linearized one-dimensional travel-time tomography setup that serves as a simplified forward model in computational geophysics. The unknown subsurface slowness model is represented by a vector $m \\in \\mathbb{R}^N$, where each component parameterizes a cell with unit length. Each data point is a travel time measurement, which is modeled as a sum over a contiguous block of cells. This gives a linear forward operator $A \\in \\mathbb{R}^{M \\times N}$, where each row selects a contiguous block of $m$ and sums its entries with weight $1$. Observations are generated as $d = A m_{\\mathrm{true}} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_d^2 I_M)$.\n\nAssume a Gaussian prior $m \\sim \\mathcal{N}(m_0, C_m)$ with $m_0 \\in \\mathbb{R}^N$ and $C_m = \\sigma_m^2 I_N$. With the Gaussian likelihood and prior, the negative log-posterior (up to an additive constant independent of $m$) is\n$$\n\\Phi(m) = \\frac{1}{2} \\left\\| \\frac{A m - d}{\\sigma_d} \\right\\|_2^2 + \\frac{1}{2} \\left\\| \\frac{m - m_0}{\\sigma_m} \\right\\|_2^2.\n$$\nYour task is to implement and verify adjoint methods and gradient consistency for this objective $\\Phi(m)$ via first principles.\n\nFundamental base to be used:\n- Bayes’ theorem relating posterior, likelihood, and prior under Gaussian assumptions.\n- Definition of the negative log-posterior as an objective function for inference.\n- Definition of the gradient of a scalar functional via the Fréchet derivative.\n- Definition of the adjoint operator $A^\\ast$ with respect to the Euclidean inner product, characterized by $\\langle A x, y \\rangle = \\langle x, A^\\ast y \\rangle$ for all $x, y$.\n- Central difference approximation for a directional derivative:\n$$\n\\frac{\\partial \\Phi(m)}{\\partial s} \\approx \\frac{\\Phi(m + h s) - \\Phi(m - h s)}{2 h},\n$$\nfor a direction $s$ and small step $h$.\n\nImplement the following in a complete, runnable program:\n1. Construction of the forward operator $A \\in \\mathbb{R}^{M \\times N}$ from a prescribed random seed by generating $M$ rays; each ray corresponds to a contiguous block of indices. For each row $i$, draw a block length $\\ell_i$ uniformly from $\\{1, 2, \\dots, \\lfloor N/2 \\rfloor\\}$, then a start index $b_i$ uniformly from $\\{0, 1, \\dots, N - \\ell_i\\}$, and set $A_{i,j} = 1$ if $j \\in \\{b_i, b_i+1, \\dots, b_i+\\ell_i-1\\}$ and $A_{i,j} = 0$ otherwise.\n2. A function to evaluate $\\Phi(m)$ for any $m$ given $A$, $d$, $m_0$, $\\sigma_d$, and $\\sigma_m$.\n3. A function to evaluate the gradient $\\nabla \\Phi(m)$ using only the definitions above and the adjoint operator characterization. You must not hard-code a pre-derived closed-form gradient; instead, use the chain rule and the adjoint notion to map residuals back to the parameter space.\n4. An adjoint test that verifies the property $\\langle A x, y \\rangle = \\langle x, A^\\ast y \\rangle$ for random vectors $x \\in \\mathbb{R}^N$ and $y \\in \\mathbb{R}^M$, using the Euclidean inner product $\\langle u, v \\rangle = u^\\top v$. Report the maximum relative discrepancy over several random draws:\n$$\n\\mathrm{err}_{\\mathrm{adj}} = \\max_{k} \\frac{|\\langle A x_k, y_k \\rangle - \\langle x_k, A^\\ast y_k \\rangle|}{\\max\\{1, |\\langle A x_k, y_k \\rangle|, |\\langle x_k, A^\\ast y_k \\rangle|\\}}.\n$$\n5. A gradient consistency test that checks, for a random unit-norm direction $s \\in \\mathbb{R}^N$ and a set of step sizes $\\{h_j\\}$, whether the central-difference directional derivative matches the inner product $s^\\top \\nabla \\Phi(m)$. For each $h_j$, compute\n$$\n\\mathrm{err}_j = \\frac{\\left| \\frac{\\Phi(m + h_j s) - \\Phi(m - h_j s)}{2 h_j} - s^\\top \\nabla \\Phi(m) \\right|}{\\max\\{1, \\left| \\frac{\\Phi(m + h_j s) - \\Phi(m - h_j s)}{2 h_j} \\right|, |s^\\top \\nabla \\Phi(m)|\\}},\n$$\nand report $\\min_j \\mathrm{err}_j$ as the gradient consistency error.\n\nTest suite and required behavior:\nImplement three independent test cases. For each case, independently construct $A$, draw a ground truth $m_{\\mathrm{true}}$ and noise to generate $d$, and choose a test point $m$ and a direction $s$. Unless otherwise stated, set $m_0 = 0$. Use the following parameter sets:\n\n- Case $1$ (general case):\n  - $N = 64$, $M = 48$.\n  - Random seeds: $A$: $1$, $m_{\\mathrm{true}}$: $2$, noise: $3$, gradient direction and test point: $4$.\n  - Noise standard deviation: $\\sigma_d = 0.05$.\n  - Prior standard deviation: $\\sigma_m = 0.5$.\n  - Central-difference step sizes: $[10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}]$.\n  - Tolerances: $\\mathrm{tol}_{\\mathrm{adj}} = 10^{-12}$, $\\mathrm{tol}_{\\mathrm{grad}} = 10^{-6}$.\n\n- Case $2$ (larger, stronger regularization):\n  - $N = 128$, $M = 96$.\n  - Random seeds: $A$: $10$, $m_{\\mathrm{true}}$: $20$, noise: $30$, gradient direction and test point: $40$.\n  - Noise standard deviation: $\\sigma_d = 0.10$.\n  - Prior standard deviation: $\\sigma_m = 0.2$.\n  - Central-difference step sizes: $[10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}]$.\n  - Tolerances: $\\mathrm{tol}_{\\mathrm{adj}} = 10^{-11}$, $\\mathrm{tol}_{\\mathrm{grad}} = 5 \\cdot 10^{-6}$.\n\n- Case $3$ (edge case with few data and weaker regularization):\n  - $N = 32$, $M = 8$.\n  - Random seeds: $A$: $100$, $m_{\\mathrm{true}}$: $200$, noise: $300$, gradient direction and test point: $400$.\n  - Noise standard deviation: $\\sigma_d = 0.20$.\n  - Prior standard deviation: $\\sigma_m = 1.0$.\n  - Central-difference step sizes: $[10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}, 10^{-7}]$.\n  - Tolerances: $\\mathrm{tol}_{\\mathrm{adj}} = 10^{-12}$, $\\mathrm{tol}_{\\mathrm{grad}} = 10^{-5}$.\n\nFor each case:\n- Perform the adjoint test using $K = 5$ independent draws of $x$ and $y$ (e.g., standard normal entries).\n- For the gradient test, draw $m$ and $s$ independently with standard normal entries, then set $s \\leftarrow s / \\|s\\|_2$.\n\nAcceptance criteria per case:\n- Adjoint test passes if $\\mathrm{err}_{\\mathrm{adj}} \\le \\mathrm{tol}_{\\mathrm{adj}}$.\n- Gradient test passes if $\\min_j \\mathrm{err}_j \\le \\mathrm{tol}_{\\mathrm{grad}}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain six Boolean values in the following order: $[\\text{adj\\_pass\\_1}, \\text{grad\\_pass\\_1}, \\text{adj\\_pass\\_2}, \\text{grad\\_pass\\_2}, \\text{adj\\_pass\\_3}, \\text{grad\\_pass\\_3}]$.",
            "solution": "The problem statement has been meticulously reviewed and is deemed valid. It is scientifically grounded in the principles of Bayesian inverse problems and numerical linear algebra, is well-posed with a complete and consistent set of definitions and parameters, and is formalizable into a concrete computational task. The problem asks for the implementation and verification of fundamental numerical methods—specifically, the adjoint method for gradient computation and its verification via a Taylor test—which are central to the field of computational geophysics.\n\nThe solution proceeds by first deriving the necessary mathematical expressions from first principles and then implementing them in a structured program that executes the specified test cases.\n\n**1. Theoretical Formulation**\n\nThe core of the task is to compute the gradient of the negative log-posterior objective function, $\\Phi(m)$, and to verify its correctness. The objective function is given by:\n$$\n\\Phi(m) = \\frac{1}{2} \\left\\| \\frac{A m - d}{\\sigma_d} \\right\\|_2^2 + \\frac{1}{2} \\left\\| \\frac{m - m_0}{\\sigma_m} \\right\\|_2^2\n$$\nThis function can be decomposed into two parts: a likelihood term $\\Phi_{\\text{like}}(m)$ and a prior term $\\Phi_{\\text{prior}}(m)$.\n\n$\\Phi_{\\text{like}}(m) = \\frac{1}{2\\sigma_d^2} \\| A m - d \\|_2^2$\n$\\Phi_{\\text{prior}}(m) = \\frac{1}{2\\sigma_m^2} \\| m - m_0 \\|_2^2$\n\nThe gradient $\\nabla \\Phi(m)$ is the sum of the gradients of these two terms: $\\nabla \\Phi(m) = \\nabla \\Phi_{\\text{like}}(m) + \\nabla \\Phi_{\\text{prior}}(m)$.\n\n**Gradient of the Prior Term**\n\nThe prior term is a simple quadratic function of $m$. Its gradient is found directly:\n$\\nabla \\Phi_{\\text{prior}}(m) = \\nabla_m \\left( \\frac{1}{2\\sigma_m^2} (m - m_0)^\\top(m - m_0) \\right) = \\frac{1}{2\\sigma_m^2} \\cdot 2(m-m_0) = \\frac{1}{\\sigma_m^2} (m - m_0)$.\n\n**Gradient of the Likelihood Term via the Adjoint Method**\n\nThe gradient of the likelihood term is derived using the chain rule and the definition of the adjoint operator. The Fréchet derivative of $\\Phi_{\\text{like}}(m)$ in a direction $s \\in \\mathbb{R}^N$ is given by the directional derivative:\n$$\nD\\Phi_{\\text{like}}(m)[s] = \\lim_{h \\to 0} \\frac{\\Phi_{\\text{like}}(m + hs) - \\Phi_{\\text{like}}(m)}{h}\n$$\nBy definition, this directional derivative is equal to the inner product $\\langle \\nabla \\Phi_{\\text{like}}(m), s \\rangle$. Let the data residual be $r(m) = Am - d$. Then $\\Phi_{\\text{like}}(m) = \\frac{1}{2\\sigma_d^2} \\langle r(m), r(m) \\rangle$.\nUsing the chain rule, the derivative of $\\Phi_{\\text{like}}$ is composed of the derivative of the norm-squared function and the derivative of the residual function $r(m)$. The derivative of $r(m)$ with respect to $m$ in direction $s$ is $A s$.\n$$\nD\\Phi_{\\text{like}}(m)[s] = \\frac{1}{\\sigma_d^2} \\langle r(m), A s \\rangle = \\frac{1}{\\sigma_d^2} \\langle Am - d, A s \\rangle\n$$\nThe \"adjoint method\" consists of using the property of the adjoint operator $A^\\ast$ to move the operator $A$ from the second argument of the inner product to the first. For the standard Euclidean inner product, the adjoint $A^\\ast$ is the transpose $A^\\top$.\n$$\n\\langle Am - d, A s \\rangle = \\langle A^\\ast (Am - d), s \\rangle = \\langle A^\\top (Am - d), s \\rangle\n$$\nThus, we have:\n$$\n\\langle \\nabla \\Phi_{\\text{like}}(m), s \\rangle = D\\Phi_{\\text{like}}(m)[s] = \\frac{1}{\\sigma_d^2} \\langle A^\\top (Am - d), s \\rangle\n$$\nSince this equality must hold for all directions $s$, we can identify the gradient as:\n$$\n\\nabla \\Phi_{\\text{like}}(m) = \\frac{1}{\\sigma_d^2} A^\\top (Am - d)\n$$\nThis derivation illustrates the principle: to find the gradient's effect on the parameter space ($m$), we first compute the residual in the data space ($Am - d$) and then map it back to the parameter space using the adjoint operator ($A^\\top$).\n\n**Total Gradient**\n\nCombining the two components, the full gradient is:\n$$\n\\nabla \\Phi(m) = \\frac{1}{\\sigma_d^2} A^\\top (Am - d) + \\frac{1}{\\sigma_m^2} (m - m_0)\n$$\nThe implementation will use functions `apply_A(m)` for the forward operation $Am$ and `apply_A_adjoint(y)` for the adjoint operation $A^\\top y$, adhering to the \"adjoint-state\" methodology.\n\n**2. Numerical Verification Protocol**\n\n**Adjoint Test**\n\nThis test verifies that the implemented adjoint operator, `apply_A_adjoint`, is indeed the adjoint of the forward operator, `apply_A`. For real matrices and the Euclidean inner product ($\\langle u, v \\rangle = u^\\top v$), the adjoint is the transpose, so we must verify $\\langle Ax, y \\rangle = \\langle x, A^\\top y \\rangle$. The test is performed for $K=5$ pairs of random vectors $x_k \\in \\mathbb{R}^N$ and $y_k \\in \\mathbb{R}^M$. The test passes if the maximum relative discrepancy is below a given tolerance $\\mathrm{tol}_{\\mathrm{adj}}$.\n$$\n\\mathrm{err}_{\\mathrm{adj}} = \\max_{k} \\frac{|\\langle A x_k, y_k \\rangle - \\langle x_k, A^\\top y_k \\rangle|}{\\max\\{1, |\\langle A x_k, y_k \\rangle|, |\\langle x_k, A^\\top y_k \\rangle|\\}} \\le \\mathrm{tol}_{\\mathrm{adj}}\n$$\nA value near machine precision for $\\mathrm{err}_{\\mathrm{adj}}$ confirms the correctness of the adjoint implementation.\n\n**Gradient Consistency Test (Taylor Test)**\n\nThis test verifies that the implemented gradient function, `evaluate_grad_Phi`, is correct. It compares the analytical directional derivative, $g_{\\text{analytic}} = s^\\top \\nabla \\Phi(m)$, with a numerical approximation obtained via a central finite difference, $g_{\\text{fd}}$.\n$$\ng_{\\text{fd}}(h) = \\frac{\\Phi(m + h s) - \\Phi(m - h s)}{2 h}\n$$\nThe central difference scheme has a truncation error of order $O(h^2)$. For very small $h$, round-off error from floating-point arithmetic becomes dominant. The test calculates the relative error for a series of step sizes $h_j$ and reports the minimum error found.\n$$\n\\mathrm{err}_{\\text{grad}} = \\min_{j} \\frac{| g_{\\text{fd}}(h_j) - g_{\\text{analytic}} |}{\\max\\{1, |g_{\\text{fd}}(h_j)|, |g_{\\text{analytic}}|\\}}\n$$\nThe test passes if this minimum error is below a specified tolerance $\\mathrm{tol}_{\\mathrm{grad}}$. A small error confirms that the derived gradient expression is correctly implemented.\n\nThe implementation below follows this protocol for the three specified test cases.",
            "answer": "```python\nimport numpy as np\n\ndef construct_A(N, M, seed):\n    \"\"\"\n    Constructs the forward operator matrix A.\n    \n    Args:\n        N (int): Number of model parameters (columns).\n        M (int): Number of data points (rows).\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        np.ndarray: The MxN forward operator matrix A.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    A = np.zeros((M, N))\n    max_len = N // 2\n    for i in range(M):\n        length = rng.integers(1, max_len + 1)\n        start = rng.integers(0, N - length + 1)\n        A[i, start : start + length] = 1.0\n    return A\n\ndef apply_A(A, m):\n    \"\"\"Applies the forward operator A to a model vector m.\"\"\"\n    return A @ m\n\ndef apply_A_adjoint(A, y):\n    \"\"\"Applies the adjoint of the operator A to a data vector y.\"\"\"\n    return A.T @ y\n\ndef test_adjoint(A, K, seed):\n    \"\"\"\n    Performs the adjoint test for the operator A and its adjoint.\n\n    Args:\n        A (np.ndarray): The MxN forward operator matrix.\n        K (int): Number of random trials.\n        seed (int): Random seed for generating vectors.\n\n    Returns:\n        float: The maximum relative discrepancy found.\n    \"\"\"\n    M, N = A.shape\n    rng = np.random.default_rng(seed)\n    max_rel_err = 0.0\n    \n    for _ in range(K):\n        x = rng.standard_normal(size=N)\n        y = rng.standard_normal(size=M)\n\n        lhs = np.dot(y, apply_A(A, x))\n        rhs = np.dot(x, apply_A_adjoint(A, y))\n\n        diff = np.abs(lhs - rhs)\n        denom = max(1.0, np.abs(lhs), np.abs(rhs))\n        rel_err = diff / denom\n        \n        if rel_err > max_rel_err:\n            max_rel_err = rel_err\n            \n    return max_rel_err\n\ndef evaluate_Phi(m, A, d, m0, sigma_d, sigma_m):\n    \"\"\"\n    Evaluates the negative log-posterior objective function Phi(m).\n    \"\"\"\n    residual_data = apply_A(A, m) - d\n    term_like = 0.5 * np.sum((residual_data / sigma_d)**2)\n    \n    residual_prior = m - m0\n    term_prior = 0.5 * np.sum((residual_prior / sigma_m)**2)\n    \n    return term_like + term_prior\n\ndef evaluate_grad_Phi(m, A, d, m0, sigma_d, sigma_m):\n    \"\"\"\n    Evaluates the gradient of Phi(m) using the adjoint method.\n    \"\"\"\n    # Gradient of likelihood term\n    residual_data = apply_A(A, m) - d\n    grad_like = apply_A_adjoint(A, residual_data) / (sigma_d**2)\n    \n    # Gradient of prior term\n    residual_prior = m - m0\n    grad_prior = residual_prior / (sigma_m**2)\n    \n    return grad_like + grad_prior\n\ndef test_gradient(A, d, m0, sigma_d, sigma_m, m_test, s, h_values):\n    \"\"\"\n    Performs the gradient consistency test (Taylor test).\n\n    Returns:\n        float: The minimum relative error found.\n    \"\"\"\n    # Analytical directional derivative\n    grad_val = evaluate_grad_Phi(m_test, A, d, m0, sigma_d, sigma_m)\n    grad_proj = np.dot(s, grad_val)\n\n    min_rel_err = np.inf\n    \n    for h in h_values:\n        phi_plus = evaluate_Phi(m_test + h * s, A, d, m0, sigma_d, sigma_m)\n        phi_minus = evaluate_Phi(m_test - h * s, A, d, m0, sigma_d, sigma_m)\n        \n        fd_approx = (phi_plus - phi_minus) / (2 * h)\n        \n        diff = np.abs(fd_approx - grad_proj)\n        denom = max(1.0, np.abs(fd_approx), np.abs(grad_proj))\n        rel_err = diff / denom\n        \n        if rel_err  min_rel_err:\n            min_rel_err = rel_err\n            \n    return min_rel_err\n\ndef run_case(params):\n    \"\"\"\n    Runs a single test case with given parameters.\n    \"\"\"\n    N, M = params[\"N\"], params[\"M\"]\n    seeds = params[\"seeds\"]\n    sigma_d, sigma_m = params[\"sigma_d\"], params[\"sigma_m\"]\n    h_values = params[\"h_values\"]\n    tol_adj, tol_grad = params[\"tol_adj\"], params[\"tol_grad\"]\n\n    # 1. Construct A\n    A = construct_A(N, M, seeds[\"A\"])\n\n    # 2. Adjoint test\n    # The seed for adjoint test vectors is part of the grad/test point seed dict key\n    err_adj = test_adjoint(A, K=5, seed=seeds[\"grad\"])\n    adj_pass = err_adj = tol_adj\n\n    # 3. Generate data\n    rng_m_true = np.random.default_rng(seeds[\"m_true\"])\n    m_true = rng_m_true.standard_normal(size=N)\n    \n    rng_noise = np.random.default_rng(seeds[\"noise\"])\n    noise = rng_noise.standard_normal(size=M) * sigma_d\n\n    d = apply_A(A, m_true) + noise\n    m0 = np.zeros(N)\n\n    # 4. Gradient test\n    rng_grad = np.random.default_rng(seeds[\"grad\"])\n    m_test = rng_grad.standard_normal(size=N)\n    s = rng_grad.standard_normal(size=N)\n    s /= np.linalg.norm(s)\n\n    err_grad = test_gradient(A, d, m0, sigma_d, sigma_m, m_test, s, h_values)\n    grad_pass = err_grad = tol_grad\n    \n    return adj_pass, grad_pass\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 64, \"M\": 48,\n            \"seeds\": {\"A\": 1, \"m_true\": 2, \"noise\": 3, \"grad\": 4},\n            \"sigma_d\": 0.05, \"sigma_m\": 0.5,\n            \"h_values\": [1e-1, 1e-2, 1e-3, 1e-4, 1e-5],\n            \"tol_adj\": 1e-12, \"tol_grad\": 1e-6\n        },\n        {\n            \"N\": 128, \"M\": 96,\n            \"seeds\": {\"A\": 10, \"m_true\": 20, \"noise\": 30, \"grad\": 40},\n            \"sigma_d\": 0.10, \"sigma_m\": 0.2,\n            \"h_values\": [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n            \"tol_adj\": 1e-11, \"tol_grad\": 5e-6\n        },\n        {\n            \"N\": 32, \"M\": 8,\n            \"seeds\": {\"A\": 100, \"m_true\": 200, \"noise\": 300, \"grad\": 400},\n            \"sigma_d\": 0.20, \"sigma_m\": 1.0,\n            \"h_values\": [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n            \"tol_adj\": 1e-12, \"tol_grad\": 1e-5\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        adj_pass, grad_pass = run_case(case_params)\n        results.extend([adj_pass, grad_pass])\n    \n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A key strength of the Bayesian framework is its ability to characterize all plausible solutions, not just a single \"best fit.\" This practice explores the concept of multimodality, which arises naturally in geophysics when observed data are consistent with multiple, distinct underlying models, such as competing lithologies. Using a simple 1D travel-time problem with a bimodal prior, you will investigate how the posterior distribution reflects this ambiguity and how tempering can be used to navigate and quantify the probability mass of each mode .",
            "id": "3577524",
            "problem": "Consider a one-dimensional Bayesian inversion problem for straight-ray travel time in a homogeneous medium, a canonical setting in computational geophysics. The unknown is the seismic velocity $v$ of a homogeneous layer. A ray of length $L$ induces a noisily measured travel time $t_{\\text{obs}}$. The forward operator follows basic kinematics: the travel time $t$ satisfies $t = L / v$. Assume independent and identically distributed Gaussian measurement noise with known standard deviation $\\sigma_t$, so that the likelihood for a given $v$ is $p(t_{\\text{obs}} \\mid v) = \\mathcal{N}\\left(t_{\\text{obs}}; L/v, \\sigma_t^2\\right)$. The geological prior encodes mutually exclusive lithologies as a mixture of two Gaussian components in $v$, with component means $v_1$ and $v_2$, standard deviations $s_1$ and $s_2$, and mixture weights $w_1$ and $w_2 = 1 - w_1$. Explicitly, the prior density is\n$$\np(v) = w_1 \\, \\mathcal{N}\\left(v; v_1, s_1^2\\right) + w_2 \\, \\mathcal{N}\\left(v; v_2, s_2^2\\right),\n$$\nwhich produces multimodality in the posterior under certain configurations of $(t_{\\text{obs}}, \\sigma_t)$.\n\nTo explore tempering strategies that address multimodality, define the tempered posterior for a power temperature parameter $\\beta \\in (0,1]$ by\n$$\np_{\\beta}(v \\mid t_{\\text{obs}}) \\propto p(v) \\, \\left[p(t_{\\text{obs}} \\mid v)\\right]^{\\beta}.\n$$\nFor $\\beta = 1$ this coincides with the usual posterior, while smaller $\\beta$ values flatten the likelihood and reduce posterior concentration, which can aid exploration across modes.\n\nYour task is to implement a complete program that, for the given test suite of cases, computes the fraction of the tempered posterior probability mass that lies in the lower-velocity mode region, defined as $v \\leq v_{\\text{mid}}$ with $v_{\\text{mid}} = (v_1 + v_2)/2$. For each case and for three temperatures $\\beta \\in \\{1.0, 0.5, 0.1\\}$, the program must discretize the velocity domain on a uniform grid, evaluate the unnormalized tempered posterior $p(v)\\left[p(t_{\\text{obs}} \\mid v)\\right]^{\\beta}$, and then compute the mass fraction\n$$\nf_{1}(\\beta) = \\frac{\\int_{v_{\\min}}^{v_{\\text{mid}}} p(v)\\left[p(t_{\\text{obs}} \\mid v)\\right]^{\\beta} \\, \\mathrm{d}v}{\\int_{v_{\\min}}^{v_{\\max}} p(v)\\left[p(t_{\\text{obs}} \\mid v)\\right]^{\\beta} \\, \\mathrm{d}v},\n$$\nwhere $[v_{\\min}, v_{\\max}]$ is a fixed velocity range chosen to contain the bulk of the prior probability. Report these fractions as unitless decimals rounded to six digits after the decimal point. Use the following physical units and constants: $L$ in meters (m), $v$ in meters per second (m/s), $t_{\\text{obs}}$ and $\\sigma_t$ in seconds (s).\n\nUse a uniform velocity grid with $v_{\\min} = 1000$ m/s, $v_{\\max} = 5000$ m/s, and grid spacing $\\Delta v = 1$ m/s for numerical integration. The Gaussian normal density must be evaluated with its full normalizing constant to ensure scientific consistency.\n\nTest suite:\n1. Ambiguous case with significant likelihood width enabling both lithologies to be plausible:\n   - $L = 1000$ m, $t_{\\text{obs}} = 0.33$ s, $\\sigma_t = 0.08$ s,\n   - $w_1 = 0.5$, $v_1 = 2000$ m/s, $s_1 = 200$ m/s,\n   - $w_2 = 0.5$, $v_2 = 3500$ m/s, $s_2 = 300$ m/s.\n2. Strongly informative case favoring the higher-velocity lithology:\n   - $L = 1000$ m, $t_{\\text{obs}} = 0.286$ s, $\\sigma_t = 0.01$ s,\n   - $w_1 = 0.5$, $v_1 = 2000$ m/s, $s_1 = 200$ m/s,\n   - $w_2 = 0.5$, $v_2 = 3500$ m/s, $s_2 = 300$ m/s.\n3. Diffuse case where the likelihood is wide and the posterior approaches the prior mixture:\n   - $L = 1000$ m, $t_{\\text{obs}} = 0.40$ s, $\\sigma_t = 0.20$ s,\n   - $w_1 = 0.5$, $v_1 = 2000$ m/s, $s_1 = 200$ m/s,\n   - $w_2 = 0.5$, $v_2 = 3500$ m/s, $s_2 = 300$ m/s.\n4. Skewed prior case where the prior favors the lower-velocity lithology but data are more consistent with higher velocity:\n   - $L = 1000$ m, $t_{\\text{obs}} = 0.32$ s, $\\sigma_t = 0.03$ s,\n   - $w_1 = 0.7$, $v_1 = 2200$ m/s, $s_1 = 150$ m/s,\n   - $w_2 = 0.3$, $v_2 = 3800$ m/s, $s_2 = 400$ m/s.\n\nFinal output format specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case, output exactly three numbers: $f_1(1.0)$, $f_1(0.5)$, and $f_1(0.1)$, in that order and rounded to six decimals.\n- Aggregate the results for all test cases in sequence. For example, the output must look like\n$[f_{1}^{(1)}(1.0),f_{1}^{(1)}(0.5),f_{1}^{(1)}(0.1),f_{1}^{(2)}(1.0),\\dots,f_{1}^{(4)}(0.1)]$,\nwhere superscripts index the test cases $1$ to $4$.\n\nAll answers are unitless decimals. No percentage signs are permitted anywhere in the output.",
            "solution": "The design begins from basic principles relevant to Bayesian inversion and uncertainty quantification in computational geophysics. The forward operator is defined by the kinematic relation $t = L/v$ for straight-ray travel time in a homogeneous medium, where $L$ is a known path length and $v$ is the unknown velocity. The data model assumes Gaussian noise with standard deviation $\\sigma_t$, hence the likelihood for observing $t_{\\text{obs}}$ given $v$ is\n$$\np(t_{\\text{obs}} \\mid v) = \\frac{1}{\\sqrt{2\\pi}\\sigma_t} \\exp\\left( -\\frac{\\left(t_{\\text{obs}} - L/v\\right)^2}{2\\sigma_t^2} \\right).\n$$\nWe posit a bimodal prior for $v$ reflecting two competing geological hypotheses. This is modeled as a mixture of Gaussian densities,\n$$\np(v) = w_1 \\frac{1}{\\sqrt{2\\pi}s_1} \\exp\\left( -\\frac{(v - v_1)^2}{2 s_1^2} \\right) + w_2 \\frac{1}{\\sqrt{2\\pi}s_2} \\exp\\left( -\\frac{(v - v_2)^2}{2 s_2^2} \\right),\n$$\nwith $w_2 = 1 - w_1$, $v_1, v_2$ the modal velocities, and $s_1, s_2$ their standard deviations. Under Bayes' theorem, the posterior density is proportional to the product $p(v) p(t_{\\text{obs}} \\mid v)$. Multimodality arises because the prior itself has two separated modes, and the likelihood does not necessarily eliminate either mode if it is sufficiently broad relative to the separation of $v_1$ and $v_2$ or if $t_{\\text{obs}}$ is consistent with both hypotheses.\n\nA tempering strategy modifies the posterior by a power parameter $\\beta \\in (0,1]$ applied to the likelihood, yielding the tempered posterior density\n$$\np_{\\beta}(v \\mid t_{\\text{obs}}) \\propto p(v) \\left[p(t_{\\text{obs}} \\mid v)\\right]^{\\beta}.\n$$\nThis formulation is known as the power posterior or tempered posterior. For $\\beta = 1$, $p_{\\beta}$ coincides with the usual posterior; for smaller $\\beta$, the effect of the likelihood is diminished, producing a flatter distribution that allows the exploration of multiple modes. Such tempering is common in Markov Chain Monte Carlo (MCMC) methods like parallel tempering to facilitate transitions between modes.\n\nTo compute the posterior mass fraction associated with the lower-velocity mode region, we partition the velocity space at the midpoint $v_{\\text{mid}} = (v_1 + v_2)/2$. Define the fraction\n$$\nf_1(\\beta) = \\frac{\\int_{v_{\\min}}^{v_{\\text{mid}}} p(v)\\left[p(t_{\\text{obs}} \\mid v)\\right]^{\\beta} \\, \\mathrm{d}v}{\\int_{v_{\\min}}^{v_{\\max}} p(v)\\left[p(t_{\\text{obs}} \\mid v)\\right]^{\\beta} \\, \\mathrm{d}v}\n$$\nfor a fixed domain $[v_{\\min}, v_{\\max}]$ that contains the bulk of the prior mass. Because all densities are nonnegative, $f_1(\\beta) \\in [0,1]$, and the complement $1 - f_1(\\beta)$ represents the mass in the upper-velocity mode region.\n\nAlgorithmic approach:\n1. Choose a uniform velocity grid $v_k = v_{\\min} + k \\Delta v$ for $k = 0, 1, \\dots, K$ with $v_{\\min} = 1000$ m/s, $v_{\\max} = 5000$ m/s, and $\\Delta v = 1$ m/s. This grid is sufficiently fine for smooth Gaussian densities.\n2. For each grid point $v_k$, compute the prior density $p(v_k)$ as the sum of weighted Gaussian components with their proper normalizing constants.\n3. Compute the likelihood $p(t_{\\text{obs}} \\mid v_k)$ using the Gaussian formula centered at $L/v_k$ with standard deviation $\\sigma_t$.\n4. For each temperature $\\beta \\in \\{1.0, 0.5, 0.1\\}$, compute the unnormalized tempered posterior $u_{\\beta}(v_k) = p(v_k)\\left[p(t_{\\text{obs}} \\mid v_k)\\right]^{\\beta}$.\n5. Compute the normalization constant $Z_{\\beta} = \\sum_k u_{\\beta}(v_k) \\Delta v$ and the lower-mode mass $M_{\\beta}^{\\text{low}} = \\sum_{v_k \\leq v_{\\text{mid}}} u_{\\beta}(v_k) \\Delta v$.\n6. The fraction is $f_1(\\beta) = M_{\\beta}^{\\text{low}} / Z_{\\beta}$. Because the grid is uniform, including $\\Delta v$ ensures dimensional consistency; note that it cancels in the ratio, but its presence is scientifically correct when expressing numerical integrals.\n\nNumerical and physical consistency:\n- The Gaussian densities are evaluated with their full normalizing constants, which is important for scientific realism, even though ratios will not depend on these constants.\n- The forward model $t = L/v$ stems from basic kinematics and is valid for a homogeneous medium and straight rays, which is a common reference case in geophysics.\n- The chosen test cases ensure coverage of multimodality handling:\n  - An ambiguous scenario where both modes can carry significant mass at $\\beta = 1$ or under tempering.\n  - A strongly informative scenario where the likelihood selects the higher-velocity mode.\n  - A diffuse scenario where the posterior approaches the prior mixture due to broad likelihood.\n  - A skewed prior scenario testing the interplay between prior weights and data consistency.\n\nImplementation details:\n- The program defines the test cases as a list, evaluates $f_1(\\beta)$ for each, and prints a single line list of floats rounded to six decimals, as required.\n- All outputs are unitless decimals; grid integration uses $\\Delta v = 1$ m/s to produce consistent ratios.\n\nThis procedure from first principles demonstrates how multimodality emerges from mixture priors in Bayesian inversion and how tempering modifies the posterior to facilitate uncertainty quantification across competing geological hypotheses.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef gaussian_pdf(x, mean, std):\n    \"\"\"Compute the Gaussian density with full normalizing constant.\"\"\"\n    coef = 1.0 / (np.sqrt(2.0 * np.pi) * std)\n    return coef * np.exp(-0.5 * ((x - mean) / std) ** 2)\n\ndef prior_mixture_pdf(v, w1, v1, s1, w2, v2, s2):\n    \"\"\"Mixture of two Gaussian components in velocity.\"\"\"\n    return w1 * gaussian_pdf(v, v1, s1) + w2 * gaussian_pdf(v, v2, s2)\n\ndef likelihood_t_given_v(t_obs, L, v, sigma_t):\n    \"\"\"Gaussian likelihood in travel time given velocity via t = L / v.\"\"\"\n    mean_t = L / v\n    return gaussian_pdf(t_obs, mean_t, sigma_t)\n\ndef tempered_mass_fraction_lower_mode(L, t_obs, sigma_t, w1, v1, s1, w2, v2, s2, betas, vmin=1000.0, vmax=5000.0, dv=1.0):\n    \"\"\"\n    Compute f1(beta): fraction of tempered posterior mass in lower-velocity region v = (v1+v2)/2.\n    \"\"\"\n    # Velocity grid\n    v_grid = np.arange(vmin, vmax + dv, dv)\n    # Prior on grid\n    p_prior = prior_mixture_pdf(v_grid, w1, v1, s1, w2, v2, s2)\n    # Likelihood on grid\n    p_like = likelihood_t_given_v(t_obs, L, v_grid, sigma_t)\n    # Midpoint between modes\n    v_mid = 0.5 * (v1 + v2)\n    # Indicator for lower-mode region\n    lower_mask = v_grid = v_mid\n\n    fractions = []\n    for beta in betas:\n        # Tempered unnormalized posterior\n        u_beta = p_prior * (p_like ** beta)\n        # Integrals via Riemann sum (uniform grid)\n        Z_beta = np.sum(u_beta) * dv\n        M_beta_low = np.sum(u_beta[lower_mask]) * dv\n        f1 = M_beta_low / Z_beta if Z_beta > 0.0 else 0.0\n        fractions.append(f1)\n    return fractions\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (L, t_obs, sigma_t, w1, v1, s1, w2, v2, s2)\n    test_cases = [\n        # 1. Ambiguous case\n        (1000.0, 0.33, 0.08, 0.5, 2000.0, 200.0, 0.5, 3500.0, 300.0),\n        # 2. Strongly informative case favoring higher velocity\n        (1000.0, 0.286, 0.01, 0.5, 2000.0, 200.0, 0.5, 3500.0, 300.0),\n        # 3. Diffuse case\n        (1000.0, 0.40, 0.20, 0.5, 2000.0, 200.0, 0.5, 3500.0, 300.0),\n        # 4. Skewed prior case\n        (1000.0, 0.32, 0.03, 0.7, 2200.0, 150.0, 0.3, 3800.0, 400.0),\n    ]\n\n    betas = [1.0, 0.5, 0.1]\n\n    results = []\n    for case in test_cases:\n        L, t_obs, sigma_t, w1, v1, s1, w2, v2, s2 = case\n        fractions = tempered_mass_fraction_lower_mode(\n            L=L,\n            t_obs=t_obs,\n            sigma_t=sigma_t,\n            w1=w1,\n            v1=v1,\n            s1=s1,\n            w2=w2,\n            v2=v2,\n            s2=s2,\n            betas=betas,\n            vmin=1000.0,\n            vmax=5000.0,\n            dv=1.0\n        )\n        results.extend([f\"{val:.6f}\" for val in fractions])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world geophysical projects often involve a trade-off between model fidelity and computational cost. This exercise tackles this challenge head-on by developing a multi-fidelity Bayesian inversion framework that formally combines data from an expensive, high-fidelity solver with data from a cheap, low-fidelity one. You will implement a principled approach that introduces a learnable discrepancy term to model the bias of the coarse solver, preventing overconfident conclusions and demonstrating a powerful technique for accelerating uncertainty quantification in practical settings .",
            "id": "3577515",
            "problem": "You are asked to formalize and implement a mathematically principled multi-fidelity Bayesian inversion in a linearized setting representative of computational geophysics, with an explicit fidelity indicator and a discrepancy prior to avoid overconfident posteriors. All quantities in this problem are dimensionless. You must derive the posterior and predictive distributions from first principles and implement a program that computes specified scalar summaries for a provided test suite. Your final program must output a single line exactly as specified at the end of this problem.\n\nThe scenario is as follows. We consider a parameter vector $\\mathbf{m} \\in \\mathbb{R}^2$ and two forward solvers, a fine solver $G_f$ and a coarse solver $G_c$, each linear in $\\mathbf{m}$. For $i \\in \\{1,\\dots,n\\}$ and a fidelity indicator $z_i \\in \\{0,1\\}$, define the observation model\n$$\ny_i = \n\\begin{cases}\n\\mathbf{a}_{f,i}^\\top \\mathbf{m} + \\varepsilon_i,  \\text{if } z_i = 1 \\,\\,(\\text{fine}),\\\\\n\\mathbf{a}_{c,i}^\\top \\mathbf{m} + b + \\varepsilon_i,  \\text{if } z_i = 0 \\,\\,(\\text{coarse}),\n\\end{cases}\n$$\nwhere $b \\in \\mathbb{R}$ is a scalar discrepancy term modeling fidelity bias of the coarse solver. The random noise $\\varepsilon_i$ is independent across $i$ with\n$$\n\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_f^2) \\text{ if } z_i=1,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_c^2) \\text{ if } z_i=0,\n$$\nwhere $\\sigma_f^2 > 0$ and $\\sigma_c^2 > 0$ are known hyperparameters.\n\nPriors. Assume independent Gaussian priors\n$$\n\\mathbf{m} \\sim \\mathcal{N}(\\mathbf{m}_0,\\mathbf{C}_0), \\qquad b \\sim \\mathcal{N}(0,s_b^2).\n$$\n\nData and operators. Use $n=4$ observations with the following forward operators:\n- Fine forward operator matrix $A_f \\in \\mathbb{R}^{4\\times 2}$ with rows $\\mathbf{a}_{f,i}^\\top$ given by\n$$\nA_f = \\begin{bmatrix}\n1.0  0.5\\\\\n0.5  1.0\\\\\n1.0  -0.2\\\\\n0.3  0.8\n\\end{bmatrix}.\n$$\n- Coarse forward operator matrix $A_c \\in \\mathbb{R}^{4\\times 2}$ with rows $\\mathbf{a}_{c,i}^\\top$ given by\n$$\nA_c = \\begin{bmatrix}\n0.9  0.5\\\\\n0.5  0.9\\\\\n0.95  -0.15\\\\\n0.32  0.78\n\\end{bmatrix}.\n$$\n\nUse the following observed data vector $\\mathbf{y} \\in \\mathbb{R}^4$:\n$$\n\\mathbf{y} = \\begin{bmatrix} 1.57 \\\\ 1.29 \\\\ 1.06 \\\\ 0.935 \\end{bmatrix}.\n$$\n\nUse the following Gaussian priors:\n$$\n\\mathbf{m}_0 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}, \\quad \\mathbf{C}_0 = \\mathrm{diag}(0.25, 0.25), \\quad s_b^2 = 0.09.\n$$\n\nDefine the new coarse-design vector for prediction as\n$$\n\\mathbf{a}_{\\text{new},c} = \\begin{bmatrix} 0.6 \\\\ 0.4 \\end{bmatrix}.\n$$\n\nTasks.\n1) Derivation. Starting from Bayes’ rule and the stated Gaussian assumptions, derive the posterior distribution for the augmented parameter $\\boldsymbol{\\theta} = [\\mathbf{m}^\\top, b]^\\top \\in \\mathbb{R}^3$ given data $\\mathbf{y}$, an arbitrary fidelity indicator vector $\\mathbf{z} \\in \\{0,1\\}^n$, and known $\\sigma_f^2,\\sigma_c^2$. Your derivation must express the posterior in closed form as a multivariate Gaussian with mean and covariance written in terms of:\n- The block-diagonal prior mean and covariance for $\\boldsymbol{\\theta}$,\n- The design matrix that combines $A_f$, $A_c$, and the bias column implied by $\\mathbf{z}$,\n- The diagonal noise covariance matrix that encodes $\\sigma_f^2$ and $\\sigma_c^2$ row-wise.\n\n2) Posterior predictive. Derive the posterior predictive distribution for a new coarse-fidelity observation $y_{\\text{new}}$ at design vector $\\mathbf{a}_{\\text{new},c}$ with fidelity indicator $z_{\\text{new}} = 0$. Express the predictive mean and variance explicitly in terms of the posterior mean and covariance of $\\boldsymbol{\\theta}$ and the known noise variance $\\sigma_c^2$.\n\n3) Implementation. Implement a program that uses your derived formulas to compute the following outputs for each test case in the test suite below:\n- The posterior mean of the first component $m_1$ (a float),\n- The posterior predictive variance for $y_{\\text{new}}$ at $\\mathbf{a}_{\\text{new},c}$ with $z_{\\text{new}}=0$ (a float).\n\nYour program must implement all linear algebra using standard dense operations and must not rely on any random number generation.\n\nTest suite. Use the fixed $\\mathbf{y}$, $A_f$, $A_c$, $\\mathbf{m}_0$, $\\mathbf{C}_0$, $s_b^2$, and $\\mathbf{a}_{\\text{new},c}$ given above. Evaluate the two outputs for each of the following three cases:\n- Case $1$ (mixed fidelity): \n  - $\\mathbf{z} = [1,0,1,0]$,\n  - $\\sigma_f^2 = 0.01$, $\\sigma_c^2 = 0.04$.\n- Case $2$ (all coarse, larger coarse variance):\n  - $\\mathbf{z} = [0,0,0,0]$,\n  - $\\sigma_f^2 = 0.01$, $\\sigma_c^2 = 0.09$.\n- Case $3$ (all fine, small fine variance):\n  - $\\mathbf{z} = [1,1,1,1]$,\n  - $\\sigma_f^2 = 0.0025$, $\\sigma_c^2 = 0.04$.\n\nFinal output format. Your program must produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a two-element list in the order specified above. Each float must be rounded to six decimal places. Concretely, the program must print a single line of the form\n$$ [[m1\\_{case1},vpred\\_{case1}],[m1\\_{case2},vpred\\_{case2}],[m1\\_{case3},vpred\\_{case3}]] $$\nfor example, \n$$ [[0.123456,0.654321],[\\dots],[\\dots]] $$\nwith no extra whitespace beyond the commas and brackets shown here. All quantities are dimensionless and must be reported as plain decimal numbers rounded to six digits after the decimal point.",
            "solution": "The user-provided problem is valid. It is a well-posed problem in computational geophysics and uncertainty quantification, free of scientific or mathematical inconsistencies. All required data and definitions are provided.\n\n### 1. Problem Formalization and Bayesian Model Setup\n\nThe problem requires a Bayesian inversion for an augmented parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^3$ that includes both the physical parameters $\\mathbf{m} \\in \\mathbb{R}^2$ and the scalar bias term $b \\in \\mathbb{R}$. We define this augmented vector as $\\boldsymbol{\\theta} = [\\mathbf{m}^\\top, b]^\\top$.\n\n**Prior Distribution:** The problem states independent Gaussian priors for $\\mathbf{m}$ and $b$:\n$$\n\\mathbf{m} \\sim \\mathcal{N}(\\mathbf{m}_0, \\mathbf{C}_0) \\quad \\text{and} \\quad b \\sim \\mathcal{N}(0, s_b^2).\n$$\nThis implies a joint Gaussian prior on the augmented parameter vector $\\boldsymbol{\\theta}$:\n$$\np(\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}_0, \\mathbf{C}_{\\theta,0}),\n$$\nwhere the prior mean $\\boldsymbol{\\theta}_0$ and prior covariance $\\mathbf{C}_{\\theta,0}$ are given by:\n$$\n\\boldsymbol{\\theta}_0 = \\begin{bmatrix} \\mathbf{m}_0 \\\\ 0 \\end{bmatrix}, \\qquad\n\\mathbf{C}_{\\theta,0} = \\begin{bmatrix} \\mathbf{C}_0  \\mathbf{0} \\\\ \\mathbf{0}^\\top  s_b^2 \\end{bmatrix}.\n$$\n\n**Likelihood Function:** The observation model depends on the fidelity indicator $z_i \\in \\{0, 1\\}$. We can express this in a unified linear form for all $n$ observations.\nLet $\\mathbf{y} = [y_1, \\dots, y_n]^\\top$ be the vector of observations. The model for the $i$-th observation is:\n$$\ny_i =\n\\begin{cases}\n[\\mathbf{a}_{f,i}^\\top, 0] \\boldsymbol{\\theta} + \\varepsilon_i,  \\text{if } z_i=1 \\\\\n[\\mathbf{a}_{c,i}^\\top, 1] \\boldsymbol{\\theta} + \\varepsilon_i,  \\text{if } z_i=0\n\\end{cases}\n$$\nThis can be written compactly as a single linear model for the entire data vector $\\mathbf{y}$:\n$$\n\\mathbf{y} = \\mathbf{G} \\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon},\n$$\nwhere $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{C}_\\varepsilon)$ is the noise vector, and $\\mathbf{G}$ is an $n \\times 3$ design matrix.\n\nThe $i$-th row of $\\mathbf{G}$, denoted $\\mathbf{g}_i^\\top$, is constructed based on the fidelity indicator $z_i$:\n$$\n\\mathbf{g}_i^\\top = z_i [\\mathbf{a}_{f,i}^\\top, 0] + (1-z_i) [\\mathbf{a}_{c,i}^\\top, 1].\n$$\nThe noise covariance matrix $\\mathbf{C}_\\varepsilon$ is diagonal, as the noise terms $\\varepsilon_i$ are independent. Its diagonal entries are determined by $z_i$:\n$$\n(\\mathbf{C}_\\varepsilon)_{ii} = z_i \\sigma_f^2 + (1-z_i) \\sigma_c^2.\n$$\nThe likelihood function is therefore given by the probability density of a multivariate Gaussian:\n$$\np(\\mathbf{y}|\\boldsymbol{\\theta}) = \\mathcal{N}(\\mathbf{y} | \\mathbf{G}\\boldsymbol{\\theta}, \\mathbf{C}_\\varepsilon).\n$$\n\n### 2. Derivation of the Posterior Distribution\n\nWe apply Bayes' rule to find the posterior distribution $p(\\boldsymbol{\\theta}|\\mathbf{y}) \\propto p(\\mathbf{y}|\\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})$. Since both the prior and the likelihood are Gaussian, the posterior distribution for $\\boldsymbol{\\theta}$ is also a multivariate Gaussian, which we denote as:\n$$\np(\\boldsymbol{\\theta}|\\mathbf{y}) = \\mathcal{N}(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}_{\\text{post}}, \\mathbf{C}_{\\theta,\\text{post}}).\n$$\nThe posterior covariance matrix $\\mathbf{C}_{\\theta,\\text{post}}$ and posterior mean vector $\\boldsymbol{\\theta}_{\\text{post}}$ are given by the standard formulas for Bayesian linear regression:\nThe posterior precision matrix is the sum of the prior precision and the data precision (from the likelihood):\n$$\n\\mathbf{C}_{\\theta,\\text{post}}^{-1} = \\mathbf{C}_{\\theta,0}^{-1} + \\mathbf{G}^\\top \\mathbf{C}_{\\varepsilon}^{-1} \\mathbf{G}.\n$$\nThe posterior covariance is the inverse of the posterior precision:\n$$\n\\mathbf{C}_{\\theta,\\text{post}} = (\\mathbf{C}_{\\theta,0}^{-1} + \\mathbf{G}^\\top \\mathbf{C}_{\\varepsilon}^{-1} \\mathbf{G})^{-1}.\n$$\nThe posterior mean is a precision-weighted average of the prior mean and the data:\n$$\n\\boldsymbol{\\theta}_{\\text{post}} = \\mathbf{C}_{\\theta,\\text{post}} (\\mathbf{C}_{\\theta,0}^{-1} \\boldsymbol{\\theta}_0 + \\mathbf{G}^\\top \\mathbf{C}_{\\varepsilon}^{-1} \\mathbf{y}).\n$$\nThese formulas provide a closed-form expression for the posterior distribution in terms of the quantities defined in the problem statement. The first component of the vector $\\boldsymbol{\\theta}_{\\text{post}}$ is the posterior mean of $m_1$, one of the required outputs.\n\n### 3. Derivation of the Posterior Predictive Distribution\n\nWe need to derive the distribution for a new coarse-fidelity observation $y_{\\text{new}}$ at design vector $\\mathbf{a}_{\\text{new},c}$ (i.e., $z_{\\text{new}}=0$). The model for this new observation is:\n$$\ny_{\\text{new}} = \\mathbf{a}_{\\text{new},c}^\\top \\mathbf{m} + b + \\varepsilon_{\\text{new}},\n$$\nwhere $\\varepsilon_{\\text{new}} \\sim \\mathcal{N}(0, \\sigma_c^2)$. In terms of the augmented parameter $\\boldsymbol{\\theta}$, this is:\n$$\ny_{\\text{new}} = \\mathbf{g}_{\\text{new}}^\\top \\boldsymbol{\\theta} + \\varepsilon_{\\text{new}}, \\quad \\text{where} \\quad \\mathbf{g}_{\\text{new}}^\\top = [\\mathbf{a}_{\\text{new},c}^\\top, 1].\n$$\nThe posterior predictive distribution $p(y_{\\text{new}}|\\mathbf{y})$ is found by marginalizing over the posterior of $\\boldsymbol{\\theta}$:\n$$\np(y_{\\text{new}}|\\mathbf{y}) = \\int p(y_{\\text{new}}|\\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}|\\mathbf{y}) d\\boldsymbol{\\theta}.\n$$\nThis is a convolution of two Gaussian distributions, which results in another Gaussian distribution. We can find its mean and variance.\n\n**Predictive Mean:** By the law of total expectation:\n$$\n\\mathbb{E}[y_{\\text{new}}|\\mathbf{y}] = \\mathbb{E}_{\\boldsymbol{\\theta}|\\mathbf{y}}[\\mathbb{E}[y_{\\text{new}}|\\boldsymbol{\\theta}]] = \\mathbb{E}_{\\boldsymbol{\\theta}|\\mathbf{y}}[\\mathbf{g}_{\\text{new}}^\\top \\boldsymbol{\\theta}] = \\mathbf{g}_{\\text{new}}^\\top \\mathbb{E}[\\boldsymbol{\\theta}|\\mathbf{y}] = \\mathbf{g}_{\\text{new}}^\\top \\boldsymbol{\\theta}_{\\text{post}}.\n$$\n\n**Predictive Variance:** By the law of total variance:\n$$\n\\text{Var}(y_{\\text{new}}|\\mathbf{y}) = \\mathbb{E}_{\\boldsymbol{\\theta}|\\mathbf{y}}[\\text{Var}(y_{\\text{new}}|\\boldsymbol{\\theta})] + \\text{Var}_{\\boldsymbol{\\theta}|\\mathbf{y}}[\\mathbb{E}(y_{\\text{new}}|\\boldsymbol{\\theta})].\n$$\nThe first term is the expected observation variance: $\\mathbb{E}_{\\boldsymbol{\\theta}|\\mathbf{y}}[\\sigma_c^2] = \\sigma_c^2$.\nThe second term is the variance of the predicted mean due to uncertainty in $\\boldsymbol{\\theta}$:\n$$\n\\text{Var}_{\\boldsymbol{\\theta}|\\mathbf{y}}[\\mathbf{g}_{\\text{new}}^\\top \\boldsymbol{\\theta}] = \\mathbf{g}_{\\text{new}}^\\top \\text{Var}(\\boldsymbol{\\theta}|\\mathbf{y}) \\mathbf{g}_{\\text{new}} = \\mathbf{g}_{\\text{new}}^\\top \\mathbf{C}_{\\theta,\\text{post}} \\mathbf{g}_{\\text{new}}.\n$$\nCombining these, the posterior predictive variance, $V_{\\text{pred}}$, is:\n$$\nV_{\\text{pred}} = \\mathbf{g}_{\\text{new}}^\\top \\mathbf{C}_{\\theta,\\text{post}} \\mathbf{g}_{\\text{new}} + \\sigma_c^2.\n$$\nThis is the second required output.\n\n### 4. Implementation Strategy\n\nThe implementation will proceed as follows for each of the three test cases:\n1.  Initialize the given constant matrices and vectors: $A_f, A_c, \\mathbf{y}, \\mathbf{m}_0, \\mathbf{C}_0, s_b^2, \\mathbf{a}_{\\text{new},c}$.\n2.  Construct the augmented prior mean $\\boldsymbol{\\theta}_0$ and prior covariance $\\mathbf{C}_{\\theta,0}$.\n3.  For a given test case specified by $(\\mathbf{z}, \\sigma_f^2, \\sigma_c^2)$:\n    a. Construct the $4 \\times 3$ design matrix $\\mathbf{G}$ by selecting and augmenting rows from $A_f$ and $A_c$ according to the fidelity vector $\\mathbf{z}$.\n    b. Construct the $4 \\times 4$ diagonal noise covariance matrix $\\mathbf{C}_\\varepsilon$ using $\\sigma_f^2$ and $\\sigma_c^2$ as specified by $\\mathbf{z}$.\n    c. Compute the inverses $\\mathbf{C}_{\\theta,0}^{-1}$ and $\\mathbf{C}_\\varepsilon^{-1}$.\n    d. Calculate the posterior covariance $\\mathbf{C}_{\\theta,\\text{post}}$ using the derived formula.\n    e. Calculate the posterior mean $\\boldsymbol{\\theta}_{\\text{post}}$ using its derived formula.\n    f. Extract the first element of $\\boldsymbol{\\theta}_{\\text{post}}$ as the posterior mean of $m_1$.\n    g. Construct the new design vector $\\mathbf{g}_{\\text{new}} = [\\mathbf{a}_{\\text{new},c}^\\top, 1]^\\top$.\n    h. Calculate the posterior predictive variance $V_{\\text{pred}}$ using its derived formula, where the added noise variance corresponds to the coarse model, $\\sigma_c^2$.\n4.  Collect and format the results as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes posterior and predictive quantities for a multi-fidelity Bayesian inversion problem.\n    \"\"\"\n    \n    # --- 1. Define constants from the problem statement ---\n    \n    # Forward operator matrices\n    A_f = np.array([\n        [1.0, 0.5],\n        [0.5, 1.0],\n        [1.0, -0.2],\n        [0.3, 0.8]\n    ])\n    A_c = np.array([\n        [0.9, 0.5],\n        [0.5, 0.9],\n        [0.95, -0.15],\n        [0.32, 0.78]\n    ])\n    \n    # Observed data vector\n    y_obs = np.array([1.57, 1.29, 1.06, 0.935])\n    \n    # Prior on m\n    m0 = np.array([1.0, 1.0])\n    C0 = np.diag([0.25, 0.25])\n    \n    # Prior on b\n    s_b_sq = 0.09\n    \n    # New design vector for prediction\n    a_new_c = np.array([0.6, 0.4])\n\n    # --- 2. Construct augmented prior for theta = [m, b] ---\n    \n    # Augmented prior mean theta_0 = [m0, 0]\n    theta_0 = np.append(m0, 0.0)\n    \n    # Augmented prior covariance C_theta_0\n    C_theta_0 = np.block([\n        [C0, np.zeros((2, 1))],\n        [np.zeros((1, 2)), s_b_sq]\n    ])\n    \n    # Pre-compute prior precision and related term for posterior mean calculation\n    C_theta_0_inv = np.linalg.inv(C_theta_0)\n    C_theta_0_inv_theta_0 = C_theta_0_inv @ theta_0\n    \n    # Define test cases\n    test_cases = [\n        # Case 1: Mixed fidelity\n        {'z': np.array([1, 0, 1, 0]), 'sigma_f_sq': 0.01, 'sigma_c_sq': 0.04},\n        # Case 2: All coarse\n        {'z': np.array([0, 0, 0, 0]), 'sigma_f_sq': 0.01, 'sigma_c_sq': 0.09},\n        # Case 3: All fine\n        {'z': np.array([1, 1, 1, 1]), 'sigma_f_sq': 0.0025, 'sigma_c_sq': 0.04},\n    ]\n\n    all_results = []\n    \n    # --- 3. Process each test case ---\n    \n    for case in test_cases:\n        z = case['z']\n        sigma_f_sq = case['sigma_f_sq']\n        sigma_c_sq = case['sigma_c_sq']\n        \n        # --- a. Construct case-specific matrices G and C_epsilon ---\n        \n        # Design matrix G (4x3)\n        G = np.zeros((4, 3))\n        for i in range(4):\n            if z[i] == 1:\n                G[i, :2] = A_f[i, :]\n                G[i, 2] = 0.0\n            else: # z[i] == 0\n                G[i, :2] = A_c[i, :]\n                G[i, 2] = 1.0\n        \n        # Noise covariance C_epsilon (diagonal)\n        diag_C_eps = z * sigma_f_sq + (1 - z) * sigma_c_sq\n        C_eps_inv = np.diag(1.0 / diag_C_eps)\n\n        # --- b. Calculate posterior distribution for theta ---\n        \n        # Posterior precision matrix for theta\n        G_T_C_eps_inv_G = G.T @ C_eps_inv @ G\n        C_theta_post_inv = C_theta_0_inv + G_T_C_eps_inv_G\n        \n        # Posterior covariance matrix for theta\n        C_theta_post = np.linalg.inv(C_theta_post_inv)\n        \n        # Posterior mean for theta\n        G_T_C_eps_inv_y = G.T @ C_eps_inv @ y_obs\n        theta_post = C_theta_post @ (C_theta_0_inv_theta_0 + G_T_C_eps_inv_y)\n        \n        # Extract posterior mean of m1\n        m1_post_mean = theta_post[0]\n        \n        # --- c. Calculate posterior predictive variance ---\n        \n        # New design vector g_new for coarse prediction\n        g_new = np.append(a_new_c, 1.0)\n        \n        # Predictive variance V_pred = g_new^T * C_theta_post * g_new + sigma_c^2\n        V_pred = g_new.T @ C_theta_post @ g_new + sigma_c_sq\n        \n        all_results.append([m1_post_mean, V_pred])\n\n    # --- 4. Format and print the final output ---\n    \n    # Format each result pair into \"[val1,val2]\"\n    formatted_results = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in all_results]\n    \n    # Join the pairs and enclose in outer brackets\n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}