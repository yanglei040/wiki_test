{
    "hands_on_practices": [
        {
            "introduction": "Many classical geophysical inversion methods rely on iterative optimization schemes like gradient descent. This exercise bridges these traditional techniques with deep learning by \"unrolling\" an optimization algorithm into a sequence of network layers. By analyzing the conditions for stability and descent for a learned preconditioned gradient step , you will gain fundamental insights into how to design and stabilize learned optimization algorithms for solving complex inverse problems.",
            "id": "3583447",
            "problem": "Design a mathematically precise, unrolled gradient-like architecture for full-waveform inversion where each layer performs one step $$m^{k+1} = m^{k} - \\alpha_k P_k \\nabla J(m^k),$$ with objective $$J(m) = \\lVert F(m) - d \\rVert_2^2 + \\lambda R(m),$$ assuming the forward operator and regularizer are such that the gradient of $J$ is globally Lipschitz continuous with constant $L$ in the Euclidean norm. The matrices $P_k$ represent learned linear preconditioners and the scalars $\\alpha_k$ are learned step sizes. Use the fundamental smoothness inequality for functions with $L$-Lipschitz gradients, the properties of symmetric positive semidefinite matrices, and operator norm bounds as the base of your derivation.\n\nYou must perform the following.\n\n1) Starting from the smoothness inequality for an $L$-smooth function $$J(y) \\le J(x) + \\nabla J(x)^\\top (y-x) + \\frac{L}{2} \\lVert y-x \\rVert_2^2,$$ derive a sufficient descent condition for the update $$m^{+} = m - \\alpha P \\nabla J(m),$$ expressed as an explicit inequality coupling the step size $\\alpha$, the learned matrix $P$, and the Lipschitz constant $L$. Your derivation must treat the general case where $P$ is not necessarily symmetric by working with its symmetric part $$S \\equiv \\frac{P + P^\\top}{2},$$ and must end in an implementable bound of the form\n$$\\text{if } \\mu \\equiv \\lambda_{\\min}(S) > 0 \\text{ and } M \\equiv \\lVert P \\rVert_2, \\text{ then } J(m^{+}) < J(m) \\text{ whenever } 0 < \\alpha < \\frac{2\\mu}{L M^2}.$$\n\n2) Provide a Lipschitz stability bound for the single-layer mapping $$T(m) \\equiv m - \\alpha P \\nabla J(m).$$ Specifically, derive a sufficient upper bound on the Lipschitz constant of $T$ in the Euclidean norm of the form $$\\lVert T(x) - T(y) \\rVert_2 \\le K \\lVert x - y \\rVert_2,$$ and express $K$ explicitly in terms of $\\alpha$, $L$, and $\\lVert P \\rVert_2$. State clearly any additional sufficient conditions that make this bound finite and, if available, any conservative parameter range that makes $T$ nonexpansive or contractive in an appropriate norm.\n\n3) Implement and evaluate the theory on a quadratic surrogate that is standard in linearized full-waveform inversion with Tikhonov regularization, namely $$J(m) = \\lVert A m - b \\rVert_2^2 + \\lambda \\lVert L m \\rVert_2^2,$$ where $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^n$, $L \\in \\mathbb{R}^{n \\times n}$, and $\\lambda > 0$ are fixed. For this quadratic, the Hessian is constant, $$H = \\nabla^2 J(m) = 2\\left(A^\\top A + \\lambda L^\\top L\\right),$$ so the global Lipschitz constant of $\\nabla J$ equals $$L = \\lVert H \\rVert_2.$$\n\nFor numerical evaluation, use dimension $n = 3$ with the following test suite (these choices are dimensionless, so no physical units are required):\n\n- Shared data for all cases:\n  - $$A = \\begin{bmatrix} 1.0 & 0.2 & 0.0 \\\\ 0.2 & 1.0 & 0.3 \\\\ 0.0 & 0.3 & 1.5 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.3 \\end{bmatrix}, \\quad L = \\begin{bmatrix} 1.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & -1.0 \\\\ 0.0 & 0.0 & 1.0 \\end{bmatrix}, \\quad \\lambda = 0.1,$$\n  - initial model $$m^{0} = \\begin{bmatrix} -0.2 \\\\ 0.5 \\\\ 1.0 \\end{bmatrix}.$$\n- Four learned layer configurations:\n  - Case $1$: $$P = \\operatorname{diag}(0.5, 1.0, 2.0), \\quad \\alpha = \\frac{1}{4} \\cdot \\frac{2 \\mu}{L M^2}.$$\n  - Case $2$: same $P$ as Case $1$, $$\\alpha = \\frac{2 \\mu}{L M^2}.$$\n  - Case $3$: same $P$ as Case $1$, $$\\alpha = 1.1 \\cdot \\frac{2 \\mu}{L M^2}.$$\n  - Case $4$: $$P = \\begin{bmatrix} 1.0 & 0.5 & 0.0 \\\\ 0.0 & 1.0 & 0.2 \\\\ 0.0 & 0.0 & 0.8 \\end{bmatrix}, \\quad \\alpha = \\frac{1}{2} \\cdot \\frac{2 \\mu}{L M^2},$$ where for nonsymmetric $P$, $\\mu = \\lambda_{\\min}\\!\\left(\\frac{P + P^\\top}{2}\\right)$ and $M = \\lVert P \\rVert_2.$\n\nFor each case:\n  - Compute the theoretical threshold $$\\alpha_\\star = \\frac{2 \\mu}{L M^2}.$$\n  - Report a boolean for the theoretical strict descent condition, i.e., whether $$\\alpha < \\alpha_\\star \\text{ and } \\mu > 0.$$\n  - Perform one update $$m^{1} = m^{0} - \\alpha P \\nabla J(m^{0})$$ and report a boolean for actual strict decrease, i.e., $$J(m^{1}) < J(m^{0}).$$\n  - Report the conservative Lipschitz bound $$K_{\\mathrm{bound}} = 1 + \\alpha L \\lVert P \\rVert_2.$$\n  - For this quadratic $J$, the single-layer map is linear: $$T(m) = \\left(I - \\alpha P H\\right)m + \\alpha P \\cdot \\mathrm{const},$$ so its exact Euclidean Lipschitz constant equals $$\\lVert I - \\alpha P H \\rVert_2.$$ Report this value.\n\nYour program must output a single line containing all per-case results concatenated into one list, in the exact order\n$$[\\text{theory\\_descent}_1, \\text{actual\\_descent}_1, K_{\\mathrm{bound},1}, K_{\\mathrm{actual},1}, \\; \\ldots \\;, \\text{theory\\_descent}_4, \\text{actual\\_descent}_4, K_{\\mathrm{bound},4}, K_{\\mathrm{actual},4}],$$\nwhere booleans are in lower case and floats are decimal numbers rounded to $6$ decimal places. The output must be a single line, exactly a comma-separated list enclosed in square brackets, with no extra text.",
            "solution": "The problem is valid as it is scientifically grounded in optimization theory, well-posed with all necessary data and conditions, and objectively stated. It presents a standard yet rigorous exercise in the analysis of learned optimization algorithms.\n\nThe solution proceeds in three parts as requested.\n\n### Part 1: Derivation of the Sufficient Descent Condition\n\nWe begin with the fundamental inequality for a function $J(m)$ with an $L$-Lipschitz continuous gradient $\\nabla J$, which holds for any two points $x$ and $y$ in its domain:\n$$J(y) \\le J(x) + \\nabla J(x)^\\top (y-x) + \\frac{L}{2} \\lVert y-x \\rVert_2^2.$$\nWe are interested in the single update step of a preconditioned gradient-like method, given by\n$$m^{k+1} = m^{k} - \\alpha_k P_k \\nabla J(m^k).$$\nTo simplify notation for a single step, let $m = m^k$, $m^{+} = m^{k+1}$, $\\alpha = \\alpha_k$, and $P = P_k$. The update is $m^{+} = m - \\alpha P \\nabla J(m)$.\n\nLet us substitute $x = m$ and $y = m^{+}$ into the smoothness inequality:\n$$J(m^{+}) \\le J(m) + \\nabla J(m)^\\top (m^{+} - m) + \\frac{L}{2} \\lVert m^{+} - m \\rVert_2^2.$$\nThe displacement vector is $m^{+} - m = -\\alpha P \\nabla J(m)$. Substituting this expression into the inequality yields:\n$$J(m^{+}) \\le J(m) + \\nabla J(m)^\\top (-\\alpha P \\nabla J(m)) + \\frac{L}{2} \\lVert -\\alpha P \\nabla J(m) \\rVert_2^2.$$\nLet $g \\equiv \\nabla J(m)$ for brevity. The inequality becomes:\n$$J(m^{+}) \\le J(m) - \\alpha g^\\top P g + \\frac{L \\alpha^2}{2} \\lVert P g \\rVert_2^2.$$\nFor strict descent, we require $J(m^{+}) < J(m)$. This is guaranteed if the sum of the last two terms on the right-hand side is strictly negative:\n$$- \\alpha g^\\top P g + \\frac{L \\alpha^2}{2} \\lVert P g \\rVert_2^2 < 0.$$\nAssuming the step size $\\alpha > 0$ and that we are not at a stationary point (i.e., $g \\neq 0$), we can divide by $\\alpha$:\n$$- g^\\top P g + \\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2 < 0,$$\nwhich can be rearranged to:\n$$\\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2 < g^\\top P g.$$\nThe term $g^\\top P g$ is a quadratic form. Since the preconditioner $P$ is not guaranteed to be symmetric, this form is not directly related to the eigenvalues of $P$. However, for any real matrix $P$ and vector $g$, the quadratic form is determined by the symmetric part of $P$. Let $S \\equiv \\frac{P + P^\\top}{2}$. Then:\n$$g^\\top S g = g^\\top \\left(\\frac{P + P^\\top}{2}\\right) g = \\frac{1}{2} (g^\\top P g + g^\\top P^\\top g) = \\frac{1}{2} (g^\\top P g + (P g)^\\top g).$$\nSince $(P g)^\\top g$ is a scalar, it equals its transpose, $(P g)^\\top g = g^\\top P g$. Thus, $g^\\top S g = g^\\top P g$.\n\nThe descent inequality can be rewritten using $S$:\n$$\\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2 < g^\\top S g.$$\nTo establish a sufficient condition that depends only on properties of $P$ and not on the specific gradient $g$, we use the provided bounds. We are given $\\mu \\equiv \\lambda_{\\min}(S) > 0$, where $\\lambda_{\\min}(S)$ is the smallest eigenvalue of the symmetric part of $P$. From the Rayleigh quotient theorem for the symmetric matrix $S$, we have:\n$$g^\\top S g \\ge \\lambda_{\\min}(S) \\lVert g \\rVert_2^2 = \\mu \\lVert g \\rVert_2^2.$$\nWe are also given $M \\equiv \\lVert P \\rVert_2$, which is the spectral norm of $P$. By the definition of an induced matrix norm:\n$$\\lVert P g \\rVert_2 \\le \\lVert P \\rVert_2 \\lVert g \\rVert_2 = M \\lVert g \\rVert_2 \\implies \\lVert P g \\rVert_2^2 \\le M^2 \\lVert g \\rVert_2^2.$$\nTo guarantee the inequality $\\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2 < g^\\top S g$ holds for any $g \\neq 0$, it is sufficient to satisfy the more conservative inequality obtained by replacing the left side with an upper bound and the right side with a lower bound:\n$$\\frac{L \\alpha}{2} (M^2 \\lVert g \\rVert_2^2) < \\mu \\lVert g \\rVert_2^2.$$\nSince we assume $g \\neq 0$, we can divide by the positive scalar $\\lVert g \\rVert_2^2$:\n$$\\frac{L \\alpha M^2}{2} < \\mu.$$\nSolving for $\\alpha$, we obtain the sufficient condition on the step size:\n$$\\alpha < \\frac{2\\mu}{L M^2}.$$\nCombining this with the requirements that $\\alpha > 0$ and that the lower bound on the quadratic form is positive (i.e., $\\mu > 0$), we arrive at the final condition for guaranteed strict descent:\n$$J(m^{+}) < J(m) \\text{ whenever } \\mu > 0 \\text{ and } 0 < \\alpha < \\frac{2\\mu}{L M^2}.$$\n\n### Part 2: Lipschitz Stability Bound\n\nWe aim to find a Lipschitz constant $K$ for the single-layer mapping $T(m) \\equiv m - \\alpha P \\nabla J(m)$ in the Euclidean norm. That is, we seek a bound $K$ such that for any two points $x$ and $y$:\n$$\\lVert T(x) - T(y) \\rVert_2 \\le K \\lVert x - y \\rVert_2.$$\nLet's analyze the difference $T(x) - T(y)$:\n$$T(x) - T(y) = (x - \\alpha P \\nabla J(x)) - (y - \\alpha P \\nabla J(y)) = (x - y) - \\alpha P (\\nabla J(x) - \\nabla J(y)).$$\nTaking the $L_2$-norm and applying the triangle inequality:\n$$\\lVert T(x) - T(y) \\rVert_2 = \\lVert (x - y) - \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 \\le \\lVert x - y \\rVert_2 + \\lVert \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2.$$\nWe analyze the second term using properties of matrix and vector norms:\n$$\\lVert \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 = |\\alpha| \\cdot \\lVert P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 \\le |\\alpha| \\cdot \\lVert P \\rVert_2 \\cdot \\lVert \\nabla J(x) - \\nabla J(y) \\rVert_2.$$\nThe gradient $\\nabla J$ is globally Lipschitz continuous with constant $L$, meaning $\\lVert \\nabla J(x) - \\nabla J(y) \\rVert_2 \\le L \\lVert x - y \\rVert_2$. Substituting this into our expression:\n$$\\lVert \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 \\le |\\alpha| \\cdot \\lVert P \\rVert_2 \\cdot (L \\lVert x - y \\rVert_2).$$\nAssuming a positive step size $\\alpha > 0$, we substitute this back into the main inequality:\n$$\\lVert T(x) - T(y) \\rVert_2 \\le \\lVert x - y \\rVert_2 + \\alpha L \\lVert P \\rVert_2 \\lVert x - y \\rVert_2.$$\nFactoring out $\\lVert x-y \\rVert_2$:\n$$\\lVert T(x) - T(y) \\rVert_2 \\le (1 + \\alpha L \\lVert P \\rVert_2) \\lVert x - y \\rVert_2.$$\nThis establishes a sufficient upper bound on the Lipschitz constant of $T$, given by\n$$K = 1 + \\alpha L \\lVert P \\rVert_2.$$\nThis bound is finite provided that the step size $\\alpha$, the gradient's Lipschitz constant $L$, and the preconditioner's norm $\\lVert P \\rVert_2$ are all finite.\nThis particular bound, being derived from the triangle inequality, is conservative. Since $\\alpha > 0$, $L > 0$, and $\\lVert P \\rVert_2 \\ge 0$, this bound $K$ is always greater than or equal to $1$. Consequently, it cannot be used to demonstrate that the map $T$ is a contraction ($K < 1$). To show contractivity, a tighter analysis is needed, often specific to the structure of the problem. For the special case where $J$ is a quadratic function, as in Part 3, the map $T$ becomes affine, and its exact Lipschitz constant can be computed, which may indeed be less than $1$.\n\n### Part 3: Numerical Evaluation Setup\n\nFor the numerical evaluation, we are given the quadratic objective function:\n$$J(m) = \\lVert A m - b \\rVert_2^2 + \\lambda \\lVert L m \\rVert_2^2.$$\nThis can be expanded as $J(m) = (A m - b)^\\top(A m - b) + \\lambda (L m)^\\top(L m)$, which simplifies to a standard quadratic form:\n$$J(m) = m^\\top A^\\top A m - 2b^\\top A m + b^\\top b + \\lambda m^\\top L^\\top L m = m^\\top(A^\\top A + \\lambda L^\\top L) m - 2 (A^\\top b)^\\top m + b^\\top b.$$\nThe gradient $\\nabla J(m)$ is found by differentiating with respect to $m$:\n$$\\nabla J(m) = 2(A^\\top A + \\lambda L^\\top L)m - 2A^\\top b.$$\nThe Hessian $\\nabla^2 J(m)$ is the derivative of the gradient:\n$$H \\equiv \\nabla^2 J(m) = 2(A^\\top A + \\lambda L^\\top L).$$\nSince the Hessian $H$ is a constant matrix, the gradient $\\nabla J(m)$ is a linear map of $m$ plus a constant, making it globally Lipschitz. The Lipschitz constant of $\\nabla J(m)$ is the operator norm of its derivative matrix, which is $H$. Thus, the global Lipschitz constant $L$ is given by:\n$$L = \\lVert H \\rVert_2 = \\lVert 2(A^\\top A + \\lambda L^\\top L) \\rVert_2.$$\nThe single-layer map $T(m)$ becomes an affine transformation:\n$$T(m) = m - \\alpha P \\nabla J(m) = m - \\alpha P (H m - 2A^\\top b) = (I - \\alpha P H) m + 2\\alpha P A^\\top b.$$\nThe Lipschitz constant of an affine map $f(x) = M x + c$ is $\\lVert M \\rVert_2$. Therefore, the exact Lipschitz constant for our map $T(m)$ is:\n$$K_{\\mathrm{actual}} = \\lVert I - \\alpha P H \\rVert_2.$$\nThe following implementation will compute the quantities for each specified case using these formulas.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the numerical evaluation for the unrolled gradient descent architecture.\n    \"\"\"\n    # Shared data for all cases\n    A = np.array([\n        [1.0, 0.2, 0.0],\n        [0.2, 1.0, 0.3],\n        [0.0, 0.3, 1.5]\n    ])\n    b = np.array([1.0, -0.5, 0.3])\n    L_mat = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, -1.0],\n        [0.0, 0.0, 1.0]\n    ])\n    lambda_reg = 0.1\n    m0 = np.array([-0.2, 0.5, 1.0])\n\n    # Objective function and its gradient\n    def J(m, A_mat, b_vec, L_mat, lam):\n        res_norm_sq = np.linalg.norm(A_mat @ m - b_vec)**2\n        reg_norm_sq = np.linalg.norm(L_mat @ m)**2\n        return res_norm_sq + lam * reg_norm_sq\n\n    def grad_J(m, A_mat, b_vec, L_mat, lam):\n        return 2 * A_mat.T @ (A_mat @ m - b_vec) + 2 * lam * L_mat.T @ L_mat @ m\n\n    # Constant Hessian and its norm (Lipschitz constant of the gradient)\n    H = 2 * (A.T @ A + lambda_reg * L_mat.T @ L_mat)\n    L_const = np.linalg.norm(H, 2)\n\n    # Define the four learned layer configurations\n    P1 = np.diag([0.5, 1.0, 2.0])\n    P4 = np.array([\n        [1.0, 0.5, 0.0],\n        [0.0, 1.0, 0.2],\n        [0.0, 0.0, 0.8]\n    ])\n    \n    test_configs = [\n        {'P': P1, 'alpha_factor': 0.25, 'id': 1},\n        {'P': P1, 'alpha_factor': 1.0,  'id': 2},\n        {'P': P1, 'alpha_factor': 1.1,  'id': 3},\n        {'P': P4, 'alpha_factor': 0.5,  'id': 4},\n    ]\n\n    results = []\n\n    for config in test_configs:\n        P = config['P']\n        alpha_factor = config['alpha_factor']\n\n        # Determine if P is symmetric to find its symmetric part S\n        if np.allclose(P, P.T):\n            S = P\n        else:\n            S = 0.5 * (P + P.T)\n\n        # Compute mu and M\n        mu = np.min(np.linalg.eigvalsh(S))\n        M = np.linalg.norm(P, 2)\n        \n        # Compute theoretical alpha threshold\n        if M > 1e-12 and L_const > 1e-12:\n            alpha_star = (2 * mu) / (L_const * M**2)\n        else:\n            alpha_star = float('inf') # Avoid division by zero\n        \n        alpha = alpha_factor * alpha_star\n\n        # 1. Theoretical strict descent condition\n        theory_descent = (mu > 0) and (alpha < alpha_star)\n        \n        # 2. Actual strict decrease\n        grad_m0 = grad_J(m0, A, b, L_mat, lambda_reg)\n        m1 = m0 - alpha * (P @ grad_m0)\n        \n        J_m0 = J(m0, A, b, L_mat, lambda_reg)\n        J_m1 = J(m1, A, b, L_mat, lambda_reg)\n        actual_descent = J_m1 < J_m0\n\n        # 3. Conservative Lipschitz bound K_bound\n        K_bound = 1 + alpha * L_const * M\n        \n        # 4. Exact Lipschitz constant K_actual for the quadratic problem\n        I = np.identity(A.shape[0])\n        K_actual_mat = I - alpha * P @ H\n        K_actual = np.linalg.norm(K_actual_mat, 2)\n        \n        # Collect results for this case\n        results.extend([theory_descent, actual_descent, K_bound, K_actual])\n\n    # Format the final output string\n    formatted_results = []\n    for res in results:\n        if isinstance(res, (bool, np.bool_)):\n            formatted_results.append(str(res).lower())\n        else:\n            formatted_results.append(f\"{res:.6f}\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The U-Net architecture is a powerful tool for mapping data to model space, but its effectiveness depends on correctly matching its structure to the physics of the problem. This practice focuses on a critical architectural property: the theoretical receptive field. By deriving how the receptive field grows with network depth and relating it to the maximum recoverable wavelength in a seismic model , you will learn how to design architectures that are purpose-built to resolve features at the scales of interest.",
            "id": "3583499",
            "problem": "Consider a one-dimensional variant of a U-Net-style Convolutional Neural Network (CNN), applied along the seismic time axis to map a shot-gather input to a one-dimensional vertical profile of seismic velocity. Assume the following architecture and conventions, which are standard in practice and scientifically plausible for vertical profiling when offsets are not the limiting factor:\n\n- The network is strictly one-dimensional along time, with input sampling interval $\\Delta t$ (in seconds).\n- The encoder has $L$ resolution-reduction stages. At each encoder stage $l \\in \\{1,\\dots,L\\}$, there are two convolution layers with kernel size $k$ (odd, stride $1$, dilation $1$, and zero-padding chosen to preserve length), followed by max-pooling with kernel size $2$ and stride $2$.\n- At the bottleneck (after $L$ poolings), there are two convolution layers with kernel size $k$ (stride $1$, dilation $1$, same-padding).\n- The decoder has $L$ resolution-increase stages. Each decoder stage upsamples by a factor of $2$ using nearest-neighbor upsampling (no kernel), concatenates with the corresponding encoder feature map (skip connection), and then applies two convolution layers with kernel size $k$ (stride $1$, dilation $1$, same-padding).\n- A final $1 \\times 1$ convolution maps features to the output (kernel size $1$).\n- All convolutions are linear and time-invariant; there are no dilations beyond $1$.\n\nYou may use the following well-tested facts:\n\n- For a sequence of layers indexed in forward order by $i$, each with kernel size $k_i$ and stride $s_i$, the length of the theoretical receptive field (in input samples) of an output sample is given by the base $1$ plus, for each layer, the incremental span contributed by its kernel, scaled by the product of the strides of all layers before it.\n- A max-pooling layer with kernel size $2$ and stride $2$ contributes a receptive-field increment equal to the current effective input stride and doubles that stride thereafter.\n- Nearest-neighbor upsampling by a factor of $2$ does not itself expand the receptive field, but it halves the effective input stride that propagates to subsequent layers.\n\nAssume a constant background velocity $v_0$ (in meters per second) and near-normal-incidence single scattering (Single-Scattering (Born) approximation), so that two-way travel time $t$ and depth $z$ are related by $t = \\frac{2 z}{v_0}$. Under this mapping, a sinusoidal velocity perturbation of depth-wavelength $\\lambda$ corresponds to a sinusoid in two-way time with period $T = \\frac{2 \\lambda}{v_0}$. For the network to capture at least one full cycle of such a component from the shot-gather input, the theoretical receptive field along time must span at least one period, i.e., $R_t \\, \\Delta t \\geq T$, where $R_t$ is the receptive field in input samples along time.\n\nTasks:\n\n- Derive from first principles an explicit, closed-form expression for the theoretical receptive field $R_t(L,k)$ (in input samples) of a single output sample with respect to the input, given the above U-Net architecture.\n- Using the time-depth relation $t = \\frac{2 z}{v_0}$ and the one-cycle criterion, express the maximum recoverable depth-wavelength $\\lambda_{\\max}$ (in meters) as a closed-form analytic function of $L$, $k$, $\\Delta t$, and $v_0$.\n\nState any additional minimal assumptions you require. Express the final wavelength in meters. The final answer must be a single closed-form expression. No rounding is required.",
            "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded, well-posed, internally consistent, and contains all necessary information to derive a unique solution. The language is precise, and the physical and computational models are standard simplifications used in the field.\n\nA minimal assumption is required regarding the receptive field calculation in a U-Net architecture with skip connections: The theoretical receptive field of an output neuron is determined by the longest path of influence from the input. In the specified U-Net architecture, this corresponds to the path traversing the full depth of the encoder, the bottleneck, and the full depth of the decoder. At each decoder stage, an upsampled feature map is concatenated with a feature map from a skip connection. The receptive field of a neuron in the subsequent convolutional layer is the union of its receptive fields with respect to both concatenated inputs. Since the upsampled path is deeper, its receptive field is strictly larger and encompasses that of the skip-connection path. Therefore, the overall receptive field is determined by tracing this deepest path, and this is the standard interpretation.\n\nThe first task is to derive an expression for the theoretical receptive field, $R_t(L,k)$, in input samples. We use the provided formula for receptive field growth: the total receptive field is $1$ plus the sum of incremental contributions from all layers. The contribution of a layer $i$ with kernel size $k_i$ is $(k_i-1)$ scaled by the product of strides of all preceding layers, $S_{i-1}$.\n$R_t = 1 + \\sum_{i} (k_i - 1) S_{i-1}$.\n\nLet's trace the receptive field growth and cumulative stride through the network's longest path.\n\n$1$. **Encoder**: The encoder has $L$ stages. For each stage $l \\in \\{1, \\dots, L\\}$:\nThe cumulative stride at the input of stage $l$ is $S_{\\text{in},l} = 2^{l-1}$.\nEach stage has two convolution layers with kernel size $k$ and stride $1$, and one max-pooling layer with kernel size $2$ and stride $2$.\n- The first convolution layer (stride $1$) contributes $(k-1)S_{\\text{in},l} = (k-1)2^{l-1}$.\n- The second convolution layer (stride $1$) contributes $(k-1)S_{\\text{in},l} = (k-1)2^{l-1}$.\n- The max-pooling layer (stride $2$) contributes $(2-1)S_{\\text{in},l} = 2^{l-1}$.\nThe total contribution from encoder stage $l$ is $\\Delta R_l^{\\text{enc}} = (k-1)2^{l-1} + (k-1)2^{l-1} + 2^{l-1} = (2k-1)2^{l-1}$.\nThe total contribution from all $L$ encoder stages is the sum over $l$:\n$$ \\Delta R_{\\text{enc}} = \\sum_{l=1}^{L} (2k-1)2^{l-1} = (2k-1) \\sum_{l=1}^{L} 2^{l-1} $$\nThis is a geometric series $\\sum_{j=0}^{L-1} 2^j = \\frac{2^L-1}{2-1} = 2^L - 1$.\n$$ \\Delta R_{\\text{enc}} = (2k-1)(2^L - 1) $$\nAfter $L$ encoder stages, the cumulative stride is $2^L$.\n\n$2$. **Bottleneck**: The bottleneck follows the encoder. The cumulative stride at its input is $S_{\\text{bottle}} = 2^L$.\nIt consists of two convolution layers, each with kernel size $k$ and stride $1$.\n- The first convolution contributes $(k-1)S_{\\text{bottle}} = (k-1)2^L$.\n- The second convolution contributes $(k-1)S_{\\text{bottle}} = (k-1)2^L$.\nThe total contribution from the bottleneck is:\n$$ \\Delta R_{\\text{bottle}} = 2(k-1)2^L $$\nThe stride does not change through the bottleneck.\n\n$3$. **Decoder**: The decoder has $L$ stages. We index them as $l \\in \\{1, \\dots, L\\}$ from the one closest to the bottleneck to the one closest to the output.\nFor each decoder stage $l$, there is an upsampling step followed by two convolution layers.\nThe cumulative stride entering decoder stage $l$ is $2^{L-l+1}$. The upsampling by a factor of $2$ reduces this stride to $S_{\\text{dec},l} = \\frac{1}{2} \\cdot 2^{L-l+1} = 2^{L-l}$. This stride applies to the subsequent convolutions.\nEach of the two convolution layers has kernel size $k$ and stride $1$.\nThe total contribution from decoder stage $l$ is:\n$\\Delta R_l^{\\text{dec}} = (k-1)S_{\\text{dec},l} + (k-1)S_{\\text{dec},l} = 2(k-1)2^{L-l}$.\nThe total contribution from all $L$ decoder stages is:\n$$ \\Delta R_{\\text{dec}} = \\sum_{l=1}^{L} 2(k-1)2^{L-l} = 2(k-1) \\sum_{l=1}^{L} 2^{L-l} $$\nLet $j=L-l$. The sum becomes $\\sum_{j=0}^{L-1} 2^j = 2^L - 1$.\n$$ \\Delta R_{\\text{dec}} = 2(k-1)(2^L - 1) $$\n\n$4$. **Final Convolution**: A final $1 \\times 1$ convolution (kernel size $1$, stride $1$) is applied. The stride entering this layer is $1$. Its contribution to the receptive field is $(1-1) \\cdot 1 = 0$.\n\n$5$. **Total Receptive Field**: The total receptive field $R_t(L,k)$ is the sum of the initial $1$ and all contributions:\n$$ R_t(L,k) = 1 + \\Delta R_{\\text{enc}} + \\Delta R_{\\text{bottle}} + \\Delta R_{\\text{dec}} $$\n$$ R_t(L,k) = 1 + (2k-1)(2^L-1) + 2(k-1)2^L + 2(k-1)(2^L-1) $$\nWe can group terms with $(2^L-1)$:\n$$ R_t(L,k) = 1 + \\left( (2k-1) + 2(k-1) \\right)(2^L-1) + 2(k-1)2^L $$\n$$ R_t(L,k) = 1 + (2k-1+2k-2)(2^L-1) + (2k-2)2^L $$\n$$ R_t(L,k) = 1 + (4k-3)(2^L-1) + (2k-2)2^L $$\nExpanding the terms:\n$$ R_t(L,k) = 1 + (4k-3)2^L - (4k-3) + (2k-2)2^L $$\n$$ R_t(L,k) = 1 - 4k + 3 + (4k-3 + 2k-2)2^L $$\n$$ R_t(L,k) = 4 - 4k + (6k-5)2^L $$\n$$ R_t(L,k) = (6k-5)2^L - 4(k-1) $$\nThis is the closed-form expression for the theoretical receptive field.\n\nThe second task is to find the maximum recoverable depth-wavelength $\\lambda_{\\max}$.\nThe condition for capturing a feature of temporal period $T$ is that the receptive field in time, $R_t \\cdot \\Delta t$, must be at least one period: $R_t \\Delta t \\ge T$.\nThe maximum period $T_{\\max}$ the network can resolve is limited by its receptive field:\n$$ T_{\\max} = R_t(L,k) \\Delta t $$\nThe problem provides the physical relationship between the two-way travel time period $T$ and the depth-wavelength $\\lambda$ of a velocity perturbation: $T = \\frac{2\\lambda}{v_0}$.\nTherefore, the maximum recoverable wavelength $\\lambda_{\\max}$ corresponds to $T_{\\max}$:\n$$ T_{\\max} = \\frac{2\\lambda_{\\max}}{v_0} $$\nEquating the two expressions for $T_{\\max}$:\n$$ \\frac{2\\lambda_{\\max}}{v_0} = R_t(L,k) \\Delta t $$\nSolving for $\\lambda_{\\max}$:\n$$ \\lambda_{\\max} = \\frac{v_0 \\Delta t}{2} R_t(L,k) $$\nSubstituting the derived expression for $R_t(L,k)$:\n$$ \\lambda_{\\max} = \\frac{v_0 \\Delta t}{2} \\left[ (6k-5)2^L - 4(k-1) \\right] $$\nThis expression can also be written as:\n$$ \\lambda_{\\max} = v_0 \\Delta t \\left[ (6k-5)2^{L-1} - 2(k-1) \\right] $$\nThis is the final closed-form analytic function for the maximum recoverable depth-wavelength.",
            "answer": "$$\\boxed{\\frac{v_0 \\Delta t}{2} \\left( (6k-5)2^L - 4(k-1) \\right)}$$"
        },
        {
            "introduction": "A complete solution to an inverse problem includes not just a single best-fit model, but also a characterization of its uncertainty. This exercise introduces a state-of-the-art approach to this challenge using conditional invertible neural networks, or normalizing flows. By implementing a model that learns the conditional posterior distribution $p(m|d)$ , you will gain hands-on experience with generative modeling and its application to quantifying the ambiguity and reliability of geophysical inversions.",
            "id": "3583443",
            "problem": "Consider modeling the conditional posterior density $p(m \\mid d)$ of a geophysical model vector $m \\in \\mathbb{R}^n$ given survey data $d \\in \\mathbb{R}^q$ using an invertible neural network $g_\\theta$ and the change-of-variables principle. Let the latent variable $z \\in \\mathbb{R}^n$ be distributed as a standard multivariate normal $z \\sim \\mathcal{N}(0, I_n)$, and define an invertible, data-conditioned transformation $m = g_\\theta(z, d)$. Under mild regularity conditions for $g_\\theta$, the conditional density transforms according to the change-of-variables formula and the conditional differential entropy transforms according to the entropy-of-transform formula.\n\nFundamental base:\n- Bayes' theorem and the change-of-variables formula: for an invertible mapping $m = g_\\theta(z, d)$ with Jacobian $J(z, d) = \\partial g_\\theta(z, d) / \\partial z$, the conditional density satisfies $p(m \\mid d) = p(z) \\,\\big|\\det J(z, d)\\big|^{-1}$ where $z = g_\\theta^{-1}(m, d)$ and $p(z)$ is the standard normal density.\n- Differential entropy of a standard multivariate normal: $h(z) = \\frac{1}{2} n \\ln(2\\pi e)$.\n- Entropy under an invertible transformation: $h(m \\mid d) = h(z) + \\mathbb{E}_{z \\sim \\mathcal{N}(0, I_n)}\\big[\\ln\\big|\\det J(z, d)\\big|\\big]$.\n\nIn this problem, implement a specific, scientifically plausible conditional normalizing flow $g_\\theta$ using two conditional affine coupling layers (a widely used invertible architecture known as Real Non-Volume Preserving). Work in dimension $n = q = 4$ and define two layers with binary masks that partition $z$ into two halves:\n- Layer $1$ mask selects the first two components as the conditioning part and transforms the last two components.\n- Layer $2$ mask selects the last two components as the conditioning part and transforms the first two components.\n\nLet $z = [z_1, z_2, z_3, z_4]^\\top$ and $d = [d_1, d_2, d_3, d_4]^\\top$. Define the two affine coupling layers:\n- Layer $1$ ($A = [z_1, z_2]^\\top$, $B = [z_3, z_4]^\\top$):\n  - Scale $s_1(A, d) = \\alpha_1 \\tanh(W_{1,s} A + U_{1,s} d + b_{1,s})$,\n  - Shift $t_1(A, d) = W_{1,t} A + U_{1,t} d + b_{1,t}$,\n  - Transform $B' = B \\odot \\exp(s_1) + t_1$, leaving $A' = A$ unchanged.\n- Layer $2$ ($A = [m^{(1)}_3, m^{(1)}_4]^\\top$, $B = [m^{(1)}_1, m^{(1)}_2]^\\top$):\n  - Scale $s_2(A, d) = \\alpha_2 \\tanh(W_{2,s} A + U_{2,s} d + b_{2,s})$,\n  - Shift $t_2(A, d) = W_{2,t} A + U_{2,t} d + b_{2,t}$,\n  - Transform $B'' = B \\odot \\exp(s_2) + t_2$, leaving $A'' = A$ unchanged.\n\nHere $\\odot$ denotes element-wise multiplication. The overall mapping is $m = g_\\theta(z, d)$ obtained by applying Layer $1$ followed by Layer $2$. The Jacobian of each affine coupling layer is block-triangular, so $\\ln|\\det J(z, d)|$ equals the sum of the scale components across the transformed subset in each layer: $\\ln|\\det J(z, d)| = \\sum s_1 + \\sum s_2$, evaluated at the same $(z, d)$.\n\nUse the following fixed parameters (all numeric entries are constants to ensure reproducibility):\n- $\\alpha_1 = 2.0$, $\\alpha_2 = 2.0$.\n- $W_{1,s} = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}$, $U_{1,s} = \\begin{bmatrix} -0.5 & -0.4 & -0.3 & -0.2 \\\\ -0.2 & -0.3 & -0.4 & -0.5 \\end{bmatrix}$, $b_{1,s} = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$.\n- $W_{1,t} = \\begin{bmatrix} 0.1 & -0.05 \\\\ 0.03 & 0.08 \\end{bmatrix}$, $U_{1,t} = \\begin{bmatrix} 0.05 & 0.02 & -0.01 & 0.03 \\\\ -0.02 & 0.04 & 0.01 & -0.03 \\end{bmatrix}$, $b_{1,t} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n- $W_{2,s} = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}$, $U_{2,s} = \\begin{bmatrix} -0.6 & -0.2 & -0.5 & -0.1 \\\\ -0.1 & -0.5 & -0.2 & -0.6 \\end{bmatrix}$, $b_{2,s} = \\begin{bmatrix} 0.05 \\\\ -0.05 \\end{bmatrix}$.\n- $W_{2,t} = \\begin{bmatrix} 0.07 & -0.02 \\\\ -0.01 & 0.09 \\end{bmatrix}$, $U_{2,t} = \\begin{bmatrix} 0.02 & -0.03 & 0.05 & 0.01 \\\\ -0.04 & 0.02 & -0.01 & 0.03 \\end{bmatrix}$, $b_{2,t} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n\nStudy identifiability across survey geometries by computing:\n- The Monte Carlo estimate of the conditional differential entropy $h(m \\mid d)$ in nats via $h(m \\mid d) \\approx \\frac{1}{2} n \\ln(2\\pi e) + \\frac{1}{N} \\sum_{i=1}^N \\ln|\\det J(z_i, d)|$, for $z_i \\sim \\mathcal{N}(0, I_n)$ and a specified sample size $N$.\n- A robustness proxy: the empirical minimum singular value across the Jacobian matrices $J(z_i, d)$ evaluated at a set of samples $\\{z_i\\}$, computed numerically by finite differences with step size $\\varepsilon$ to construct $J(z_i, d)$, followed by singular value decomposition. Define a boolean \"local identifiability\" flag that is true if the empirical minimum singular value is at least a threshold $\\tau$.\n\nImplement the above with:\n- Dimension $n = 4$ and $q = 4$.\n- Entropy Monte Carlo sample count $N = 2000$.\n- Singular value sample count $K = 128$.\n- Finite-difference step $\\varepsilon = 10^{-5}$.\n- Threshold $\\tau = 10^{-2}$.\n\nUse the following test suite of survey geometries:\n- Case $1$ (\"baseline geometry\"): $d^{(1)} = [0.0, 0.0, 0.0, 0.0]^\\top$.\n- Case $2$ (\"moderate contrast geometry\"): $d^{(2)} = [1.0, -1.5, 2.0, -0.5]^\\top$.\n- Case $3$ (\"strongly biased geometry\"): $d^{(3)} = [6.0, -6.0, 6.0, -6.0]^\\top$.\n\nFor each case $j \\in \\{1, 2, 3\\}$, produce a result triple:\n- The estimated $h(m \\mid d^{(j)})$ as a float in nats.\n- The empirical minimum singular value across the $K$ Jacobian samples for $d^{(j)}$.\n- The local identifiability boolean indicating whether the minimum singular value is at least $\\tau$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a list of the three values for one test case, for example, $[[h_1, s_1, b_1],[h_2, s_2, b_2],[h_3, s_3, b_3]]$. Floats should be provided directly as computed; no units are involved in this problem (dimensionless quantities). Angles are not used. Percentages are not used.",
            "solution": "The problem requires the implementation and analysis of a conditional invertible neural network, specifically a two-layer conditional normalizing flow based on affine coupling layers (RealNVP architecture), for modeling a conditional posterior density $p(m \\mid d)$ in a geophysical context. The analysis involves computing the conditional differential entropy $h(m \\mid d)$ and a robustness proxy based on the singular values of the network's Jacobian for three different data conditions $d$.\n\nThe process can be systematically broken down into several stages: defining the forward transformation, deriving the Jacobian determinant, estimating the conditional entropy, and evaluating the robustness proxy.\n\n**1. Forward Transformation $m = g_\\theta(z, d)$**\n\nThe transformation from the latent variable $z \\in \\mathbb{R}^4$ to the model parameters $m \\in \\mathbb{R}^4$, conditioned on the data $d \\in \\mathbb{R}^4$, is defined by the composition of two affine coupling layers, $g_\\theta = g_2 \\circ g_1$. The latent variable $z$ is assumed to follow a standard multivariate normal distribution, $z \\sim \\mathcal{N}(0, I_4)$.\n\n**Layer 1 Transformation ($g_1$)**:\nThe input $z$ is partitioned into two halves: $z_A = [z_1, z_2]^\\top$ and $z_B = [z_3, z_4]^\\top$. $z_A$ remains unchanged (the identity part) while $z_B$ is transformed using an affine transformation whose parameters are functions of $z_A$ and the conditioning data $d$. The problem specifies a conditional form where the scale and shift functions $s_1$ and $t_1$ depend on $z_A$ and $d$. However, the provided weight matrices for the latent variable part, $W_{1,s}$ and $W_{2,s}$, are zero matrices. This simplifies the scale functions to be dependent only on the data $d$. The transformation is:\n- Let $z_A = [z_1, z_2]^\\top$ and $z_B = [z_3, z_4]^\\top$.\n- Scale: $s_1(d) = \\alpha_1 \\tanh(U_{1,s} d + b_{1,s})$. Note the model simplification, as $W_{1,s}$ is the zero matrix.\n- Shift: $t_1(z_A, d) = W_{1,t} z_A + U_{1,t} d + b_{1,t}$.\n- The output of Layer $1$ is $m^{(1)} = [m^{(1)}_1, m^{(1)}_2, m^{(1)}_3, m^{(1)}_4]^\\top$, where the first two components are unchanged and the last two are transformed:\n$$\nm^{(1)} = \\begin{bmatrix} z_A \\\\ z_B \\odot \\exp(s_1(d)) + t_1(z_A, d) \\end{bmatrix}\n$$\nHere, $\\odot$ denotes the element-wise (Hadamard) product.\n\n**Layer 2 Transformation ($g_2$)**:\nThe second layer takes the output of the first layer, $m^{(1)}$, as its input. It swaps the roles of the two halves: the part that was transformed by Layer $1$ now conditions the transformation, and the part that was unchanged is now transformed.\n- The input $m^{(1)}$ is partitioned into $m^{(1)}_A = [m^{(1)}_3, m^{(1)}_4]^\\top$ and $m^{(1)}_B = [m^{(1)}_1, m^{(1)}_2]^\\top$.\n- Scale: $s_2(m^{(1)}_A, d) = \\alpha_2 \\tanh(W_{2,s} m^{(1)}_A + U_{2,s} d + b_{2,s})$. Again, with $W_{2,s} = 0$, this simplifies to $s_2(d) = \\alpha_2 \\tanh(U_{2,s} d + b_{2,s})$.\n- Shift: $t_2(m^{(1)}_A, d) = W_{2,t} m^{(1)}_A + U_{2,t} d + b_{2,t}$.\n- The final output $m$ is formed by transforming $m^{(1)}_B$ and keeping $m^{(1)}_A$ unchanged, then concatenating them in the order of the transformation:\n$$\nm = g_\\theta(z,d) = g_2(m^{(1)}, d) = \\begin{bmatrix} m^{(1)}_B \\odot \\exp(s_2(m^{(1)}_A, d)) + t_2(m^{(1)}_A, d) \\\\ m^{(1)}_A \\end{bmatrix}\n$$\n\n**2. Jacobian Log-Determinant**\n\nA key property of an affine coupling layer is that its Jacobian matrix is block-triangular. For Layer $1$, the Jacobian $J_1 = \\partial m^{(1)} / \\partial z$ has the form:\n$$\nJ_1 = \\begin{bmatrix} I & 0 \\\\ \\frac{\\partial m^{(1)}_{3,4}}{\\partial z_{1,2}} & \\text{diag}(\\exp(s_1)) \\end{bmatrix}\n$$\nThe determinant is the product of its diagonal entries, which is $\\det(J_1) = \\prod_i \\exp(s_{1,i}) = \\exp(\\sum_i s_{1,i})$. Consequently, the log-determinant is simply the sum of the scale components: $\\ln|\\det(J_1)| = \\sum_i s_{1,i}$.\n\nSimilarly, for Layer $2$, $\\ln|\\det(J_2)| = \\sum_j s_{2,j}$. The Jacobian of the composite map $g_\\theta = g_2 \\circ g_1$ is $J = J_2 \\cdot J_1$. By the property of determinants, $\\det(J) = \\det(J_2) \\det(J_1)$, which leads to:\n$$\n\\ln|\\det J(z, d)| = \\ln|\\det(J_1)| + \\ln|\\det(J_2)| = \\sum_{i=1}^2 s_{1,i} + \\sum_{j=1}^2 s_{2,j}\n$$\nThis quantity is crucial for both the change-of-variables formula and the entropy calculation. Note that the scale factors $s_1$ and $s_2$ depend on the inputs to their respective layers.\n\n**3. Conditional Differential Entropy Estimation**\n\nThe differential entropy of a random vector $z$ under an invertible transformation $m = g(z)$ is given by $h(m) = h(z) + \\mathbb{E}[\\ln|\\det J(z)|]$. For our conditional setting, this becomes:\n$$\nh(m \\mid d) = h(z) + \\mathbb{E}_{z \\sim \\mathcal{N}(0, I_n)}\\big[\\ln\\big|\\det J(z, d)\\big|\\big]\n$$\nThe base entropy for a standard $n$-dimensional normal distribution is $h(z) = \\frac{1}{2} n \\ln(2\\pi e)$. The expectation term is estimated using a Monte Carlo approach. By drawing $N$ samples $\\{z_i\\}_{i=1}^N$ from $\\mathcal{N}(0, I_n)$, the entropy is approximated as:\n$$\nh(m \\mid d) \\approx \\frac{1}{2} n \\ln(2\\pi e) + \\frac{1}{N} \\sum_{i=1}^N \\ln|\\det J(z_i, d)|\n$$\nWe will implement this with $n=4$ and $N=2000$.\n\n**4. Robustness Proxy and Local Identifiability**\n\nThe robustness of the mapping $g_\\theta$ can be related to how it distorts the latent space. A large variation in the singular values of the Jacobian $J(z,d)$ indicates that the transformation can excessively stretch or compress volumes, which can lead to numerical instability or poor model conditioning. The minimum singular value, $\\sigma_{min}(J)$, provides a measure of the most extreme volume compression. A value close to zero suggests that the mapping is nearly singular, implying a loss of information and poor local identifiability of the model parameters.\n\nWe are tasked to compute an empirical minimum singular value by first sampling $K=128$ latent vectors $\\{z_i\\}_{i=1}^K$ from $\\mathcal{N}(0, I_n)$. For each sample $z_i$, the Jacobian matrix $J(z_i, d)$ is constructed numerically using the forward-difference method with a step size $\\varepsilon = 10^{-5}$. The $j$-th column of the Jacobian is approximated by:\n$$\nJ_{:,j} \\approx \\frac{g_\\theta(z_i + \\varepsilon e_j, d) - g_\\theta(z_i, d)}{\\varepsilon}\n$$\nwhere $e_j$ is the $j$-th standard basis vector. After constructing each Jacobian $J(z_i, d)$, its singular values are computed. The empirical minimum singular value for a given $d$ is the minimum of these minimums across all $K$ samples.\n$$\ns_{min} = \\min_{i \\in \\{1, \\dots, K\\}} \\sigma_{min}(J(z_i, d))\n$$\nA \"local identifiability\" flag is then set to true if this value is greater than or equal to a threshold $\\tau = 10^{-2}$.\n\nThese computations will be performed for the three specified data vectors $d^{(1)}$, $d^{(2)}$, and $d^{(3)}$, and the results will be reported as a list of triples.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes a conditional normalizing flow model.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    n = 4\n    q = 4\n    N_entropy = 2000\n    K_svd = 128\n    epsilon = 1e-5\n    tau = 1e-2\n\n    params = {\n        'alpha1': 2.0,\n        'alpha2': 2.0,\n        'W1s': np.array([[0.0, 0.0], [0.0, 0.0]]),\n        'U1s': np.array([[-0.5, -0.4, -0.3, -0.2], [-0.2, -0.3, -0.4, -0.5]]),\n        'b1s': np.array([0.1, -0.1]),\n        'W1t': np.array([[0.1, -0.05], [0.03, 0.08]]),\n        'U1t': np.array([[0.05, 0.02, -0.01, 0.03], [-0.02, 0.04, 0.01, -0.03]]),\n        'b1t': np.array([0.0, 0.0]),\n        'W2s': np.array([[0.0, 0.0], [0.0, 0.0]]),\n        'U2s': np.array([[-0.6, -0.2, -0.5, -0.1], [-0.1, -0.5, -0.2, -0.6]]),\n        'b2s': np.array([0.05, -0.05]),\n        'W2t': np.array([[0.07, -0.02], [-0.01, 0.09]]),\n        'U2t': np.array([[0.02, -0.03, 0.05, 0.01], [-0.04, 0.02, -0.01, 0.03]]),\n        'b2t': np.array([0.0, 0.0]),\n    }\n\n    test_cases = [\n        np.array([0.0, 0.0, 0.0, 0.0]),       # Case 1\n        np.array([1.0, -1.5, 2.0, -0.5]),    # Case 2\n        np.array([6.0, -6.0, 6.0, -6.0]),    # Case 3\n    ]\n\n    # --- Core Functions ---\n\n    def g_theta(z, d, p):\n        \"\"\" The forward mapping m = g_theta(z, d). \"\"\"\n        # Layer 1\n        z_A = z[:2]\n        z_B = z[2:]\n        s1 = p['alpha1'] * np.tanh(p['W1s'] @ z_A + p['U1s'] @ d + p['b1s'])\n        t1 = p['W1t'] @ z_A + p['U1t'] @ d + p['b1t']\n        m1_B = z_B * np.exp(s1) + t1\n        m1_A = z_A\n        m1 = np.concatenate([m1_A, m1_B])\n\n        # Layer 2\n        m1_A_swapped = m1[2:] # These are the transformed components from Layer 1\n        m1_B_swapped = m1[:2] # These are the identity components from Layer 1\n        s2 = p['alpha2'] * np.tanh(p['W2s'] @ m1_A_swapped + p['U2s'] @ d + p['b2s'])\n        t2 = p['W2t'] @ m1_A_swapped + p['U2t'] @ d + p['b2t']\n        m2_B = m1_B_swapped * np.exp(s2) + t2\n        m2_A = m1_A_swapped\n        m = np.concatenate([m2_B, m2_A])\n        return m\n\n    def log_det_J(z, d, p):\n        \"\"\" Computes the log-determinant of the Jacobian analytically. \"\"\"\n        # Layer 1 scale\n        z_A = z[:2]\n        z_B = z[2:]\n        s1 = p['alpha1'] * np.tanh(p['W1s'] @ z_A + p['U1s'] @ d + p['b1s'])\n        \n        # Intermediate state for Layer 2 input\n        t1 = p['W1t'] @ z_A + p['U1t'] @ d + p['b1t']\n        m1_B = z_B * np.exp(s1) + t1\n        \n        # Layer 2 scale\n        m1_A_swapped = m1_B # The components that were transformed\n        s2 = p['alpha2'] * np.tanh(p['W2s'] @ m1_A_swapped + p['U2s'] @ d + p['b2s'])\n        \n        return np.sum(s1) + np.sum(s2)\n\n    all_results = []\n\n    # Use fixed seeds for reproducibility\n    rng_entropy = np.random.default_rng(seed=0)\n    rng_svd = np.random.default_rng(seed=1)\n    \n    z_samples_entropy = rng_entropy.standard_normal(size=(N_entropy, n))\n    z_samples_svd = rng_svd.standard_normal(size=(K_svd, n))\n\n    for d in test_cases:\n        # --- 1. Conditional Differential Entropy ---\n        log_dets = [log_det_J(z, d, params) for z in z_samples_entropy]\n        mean_log_det = np.mean(log_dets)\n        h_z = 0.5 * n * np.log(2 * np.pi * np.e)\n        h_m_d = h_z + mean_log_det\n\n        # --- 2. Robustness Proxy (Min Singular Value) ---\n        min_singular_values = []\n        for z in z_samples_svd:\n            J = np.zeros((n, n))\n            m_base = g_theta(z, d, params)\n            for j in range(n):\n                z_perturbed = z.copy()\n                z_perturbed[j] += epsilon\n                m_perturbed = g_theta(z_perturbed, d, params)\n                J[:, j] = (m_perturbed - m_base) / epsilon\n            \n            singular_values = np.linalg.svd(J, compute_uv=False)\n            min_singular_values.append(np.min(singular_values))\n        \n        empirical_min_sv = np.min(min_singular_values)\n\n        # --- 3. Local Identifiability Flag ---\n        identifiability_flag = empirical_min_sv >= tau\n\n        all_results.append([h_m_d, empirical_min_sv, identifiability_flag])\n\n    # Format the final output string\n    # Using str() on the list of lists produces the desired format with spaces\n    # To match the example format without spaces, we need to build the string manually.\n    # However, the problem stub's print statement suggests str() is sufficient.\n    # Let's follow the stub's logic.\n    result_strings = [str(res) for res in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}