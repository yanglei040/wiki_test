{
    "hands_on_practices": [
        {
            "introduction": "Understanding the 'why' behind the adjoint method is the first step toward mastering it. This exercise guides you through a foundational, first-principles derivation of the adjoint equations for a general frequency-domain electromagnetic problem, starting from the definition of a complex inner product. By working through the calculus, you will see exactly how the adjoint system arises to efficiently compute the gradient of a real-valued objective function .",
            "id": "3312413",
            "problem": "Consider the frequency-domain Maxwell curl-curl equation for the electric field $\\mathbf{E}(\\mathbf{r};\\alpha)$ in a source-driven, nonmagnetic medium,\n$$\n\\nabla \\times \\mu_0^{-1} \\nabla \\times \\mathbf{E} - \\omega^2 \\epsilon(\\mathbf{r};\\alpha)\\,\\mathbf{E} = i \\omega \\mathbf{J},\n$$\nwhere $\\mu_0$ is the permeability of free space, $\\omega$ is the angular frequency, $\\epsilon(\\mathbf{r};\\alpha) = \\epsilon_0(\\mathbf{r}) + \\alpha\\,p(\\mathbf{r})$ is a scalar permittivity distribution parametrized by a real scalar design variable $\\alpha$, and $\\mathbf{J}$ is a given impressed current density. Let the complex inner product on fields be defined by $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\int \\mathbf{u}^*(\\mathbf{r}) \\cdot \\mathbf{v}(\\mathbf{r})\\, d\\mathbf{r}$, which is conjugate-linear in its first argument. Assume perfectly electric conductor boundary conditions on the computational boundary so that boundary terms vanish upon integration by parts.\n\nA standard edge-element Galerkin finite element method (FEM) discretization yields a linear system for the complex degrees of freedom $\\mathbf{x}(\\alpha) \\in \\mathbb{C}^n$,\n$$\n\\mathbf{A}(\\alpha)\\,\\mathbf{x}(\\alpha) = \\mathbf{b},\n$$\nwhere $\\mathbf{A}(\\alpha) = \\mathbf{K} - \\omega^2 \\operatorname{diag}(\\boldsymbol{\\epsilon}_0 + \\alpha \\mathbf{p})\\,\\mathbf{M}$, with stiffness matrix $\\mathbf{K} \\in \\mathbb{C}^{n \\times n}$, mass matrix $\\mathbf{M} \\in \\mathbb{C}^{n \\times n}$, and vectors $\\boldsymbol{\\epsilon}_0, \\mathbf{p} \\in \\mathbb{R}^n$. Let the complex inner product on $\\mathbb{C}^n$ be $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\mathbf{u}^* \\mathbf{v}$. Consider the real-valued objective functional\n$$\nJ(\\alpha) = \\frac{1}{2}\\,\\| \\mathbf{C}\\,\\mathbf{x}(\\alpha) - \\mathbf{d} \\|_2^2,\n$$\nwhere $\\mathbf{C} \\in \\mathbb{C}^{m \\times n}$ and $\\mathbf{d} \\in \\mathbb{C}^m$ are given, and the norm is induced by the complex inner product.\n\n1) Starting from the frequency-domain Maxwell equation and the above definition of the complex inner product, and using only first principles of variational calculus in complex vector spaces (for example, the Lagrangian method with the convention that the inner product is conjugate-linear in its first slot, or the equivalent Wirtinger calculus), derive a first-order sensitivity expression for $\\frac{dJ}{d\\alpha}$ in terms of the state vector $\\mathbf{x}(\\alpha)$, a suitably defined adjoint vector $\\mathbf{y}(\\alpha)$ that enforces the field constraint, and the derivative $\\frac{\\partial \\mathbf{A}}{\\partial \\alpha}$. Your derivation must explicitly show where the real part operator arises from the complex inner product and why the adjoint linear system has the form it does.\n\n2) Evaluate your derived expression for the specific $n = 2$ instance with\n$$\n\\omega = 1,\\quad\n\\mathbf{K} = \\begin{pmatrix} 3  1 + i \\\\ 1 - i  4 \\end{pmatrix},\\quad\n\\mathbf{M} = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix},\n$$\n$$\n\\boldsymbol{\\epsilon}_0 = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix},\\quad\n\\mathbf{p} = \\begin{pmatrix} \\tfrac{1}{2} \\\\ -\\tfrac{1}{4} \\end{pmatrix},\\quad\n\\mathbf{b} = \\begin{pmatrix} 1 \\\\ i \\end{pmatrix},\\quad\n\\mathbf{C} = \\begin{pmatrix} 1  0 \\end{pmatrix},\\quad\n\\mathbf{d} = 0,\n$$\nso that $\\mathbf{A}(\\alpha) = \\mathbf{K} - \\operatorname{diag}(\\boldsymbol{\\epsilon}_0 + \\alpha \\mathbf{p})\\,\\mathbf{M}$. Compute the numerical value of $\\frac{dJ}{d\\alpha}$ at $\\alpha = 0.2$ by:\n- solving the forward system for $\\mathbf{x}(\\alpha)$,\n- solving the adjoint system you derived for $\\mathbf{y}(\\alpha)$,\n- evaluating the sensitivity in terms of $\\mathbf{x}(\\alpha)$, $\\mathbf{y}(\\alpha)$, and $\\frac{\\partial \\mathbf{A}}{\\partial \\alpha}$.\n\nRound your final numerical answer for $\\frac{dJ}{d\\alpha}$ at $\\alpha = 0.2$ to four significant figures. Express the final answer as a pure number with no units.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard problem in computational electromagnetics sensitivity analysis. We proceed with the solution.\n\nThe problem is divided into two parts: first, to derive a general expression for the sensitivity $\\frac{dJ}{d\\alpha}$ using the adjoint method, and second, to apply this expression to a specific numerical example.\n\n**Part 1: Derivation of the Adjoint Sensitivity Expression**\n\nWe are given the objective functional $J(\\alpha)$, which is a real-valued function of the design parameter $\\alpha$:\n$$\nJ(\\alpha) = \\frac{1}{2}\\,\\| \\mathbf{C}\\,\\mathbf{x}(\\alpha) - \\mathbf{d} \\|_2^2\n$$\nHere, $\\mathbf{x}(\\alpha) \\in \\mathbb{C}^n$ is the state vector, which depends implicitly on $\\alpha$ through the linear system (the state equation):\n$$\n\\mathbf{A}(\\alpha)\\,\\mathbf{x}(\\alpha) = \\mathbf{b}\n$$\nThe norm is induced by the complex inner product $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\mathbf{u}^* \\mathbf{v} = \\mathbf{u}^H \\mathbf{v}$, where $\\mathbf{u}^H$ is the conjugate transpose of $\\mathbf{u}$. We can write the objective function using this inner product:\n$$\nJ(\\alpha) = \\frac{1}{2} \\langle \\mathbf{C}\\mathbf{x}(\\alpha) - \\mathbf{d}, \\mathbf{C}\\mathbf{x}(\\alpha) - \\mathbf{d} \\rangle\n$$\nTo find the sensitivity $\\frac{dJ}{d\\alpha}$, we differentiate $J$ with respect to $\\alpha$ using the chain rule. Let $\\mathbf{x}'(\\alpha)$ denote $\\frac{d\\mathbf{x}}{d\\alpha}$.\n$$\n\\frac{dJ}{d\\alpha} = \\frac{d}{d\\alpha} \\left[ \\frac{1}{2} (\\mathbf{C}\\mathbf{x} - \\mathbf{d})^H (\\mathbf{C}\\mathbf{x} - \\mathbf{d}) \\right]\n$$\nUsing the product rule for differentiation, we get:\n$$\n\\frac{dJ}{d\\alpha} = \\frac{1}{2} \\left[ (\\mathbf{C}\\mathbf{x}')^H (\\mathbf{C}\\mathbf{x} - \\mathbf{d}) + (\\mathbf{C}\\mathbf{x} - \\mathbf{d})^H (\\mathbf{C}\\mathbf{x}') \\right]\n$$\nThe two terms in the brackets are complex conjugates of each other. The sum of a complex number $z$ and its conjugate $z^*$ is $2 \\operatorname{Re}(z)$. This is the origin of the real-part operator. Letting $z = (\\mathbf{C}\\mathbf{x} - \\mathbf{d})^H (\\mathbf{C}\\mathbf{x}')$, we have:\n$$\n\\frac{dJ}{d\\alpha} = \\operatorname{Re}\\left[ (\\mathbf{C}\\mathbf{x} - \\mathbf{d})^H (\\mathbf{C}\\mathbf{x}') \\right]\n$$\nUsing the definition of the complex inner product, this can be written as:\n$$\n\\frac{dJ}{d\\alpha} = \\operatorname{Re}\\left[ \\langle \\mathbf{C}\\mathbf{x} - \\mathbf{d}, \\mathbf{C}\\mathbf{x}' \\rangle \\right]\n$$\nWe use the property of the adjoint operator (in this case, the conjugate transpose $\\mathbf{C}^H$) with respect to the inner product: $\\langle \\mathbf{u}, \\mathbf{T}\\mathbf{v} \\rangle = \\langle \\mathbf{T}^H\\mathbf{u}, \\mathbf{v} \\rangle$.\n$$\n\\frac{dJ}{d\\alpha} = \\operatorname{Re}\\left[ \\langle \\mathbf{C}^H(\\mathbf{C}\\mathbf{x} - \\mathbf{d}), \\mathbf{x}' \\rangle \\right]\n$$\nThis expression for the gradient depends on $\\mathbf{x}' = \\frac{d\\mathbf{x}}{d\\alpha}$, which is computationally expensive to find directly. The adjoint method circumvents this. We find an expression for $\\mathbf{x}'$ by differentiating the state equation $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ with respect to $\\alpha$:\n$$\n\\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} + \\mathbf{A} \\frac{d\\mathbf{x}}{d\\alpha} = 0 \\implies \\mathbf{x}' = -\\mathbf{A}^{-1} \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x}\n$$\nSubstituting this into the expression for $\\frac{dJ}{d\\alpha}$:\n$$\n\\frac{dJ}{d\\alpha} = \\operatorname{Re}\\left[ \\langle \\mathbf{C}^H(\\mathbf{C}\\mathbf{x} - \\mathbf{d}), -\\mathbf{A}^{-1} \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} \\rangle \\right]\n$$\nAgain, we use the adjoint property of the inner product, this time for the operator $T = -\\mathbf{A}^{-1}$:\n$$\n\\frac{dJ}{d\\alpha} = \\operatorname{Re}\\left[ \\langle -(\\mathbf{A}^{-1})^H \\mathbf{C}^H(\\mathbf{C}\\mathbf{x} - \\mathbf{d}), \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} \\rangle \\right]\n$$\nWe introduce an adjoint vector $\\mathbf{y}(\\alpha) \\in \\mathbb{C}^n$ defined as the solution to the adjoint linear system, which is chosen to eliminate the inverse of $\\mathbf{A}$:\n$$\n\\mathbf{A}^H \\mathbf{y} = \\mathbf{C}^H(\\mathbf{C}\\mathbf{x} - \\mathbf{d})\n$$\nThis is the adjoint equation. The right-hand side is the derivative of the objective function $J$ with respect to $\\mathbf{x}^*$ (or proportional to it, depending on the convention for complex gradients). Solving this system gives $\\mathbf{y} = (\\mathbf{A}^H)^{-1} \\mathbf{C}^H(\\mathbf{C}\\mathbf{x} - \\mathbf{d})$. We can then substitute $\\mathbf{y}$ into the sensitivity expression:\n$$\n\\frac{dJ}{d\\alpha} = \\operatorname{Re}\\left[ \\langle -\\mathbf{y}, \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} \\rangle \\right]\n$$\nDue to the conjugate-linearity of the inner product in the first argument, $\\langle - \\mathbf{y}, \\mathbf{v} \\rangle = -\\langle \\mathbf{y}, \\mathbf{v} \\rangle$. Therefore:\n$$\n\\frac{dJ}{d\\alpha} = -\\operatorname{Re}\\left[ \\langle \\mathbf{y}, \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} \\rangle \\right] = -\\operatorname{Re}\\left[ \\mathbf{y}^H \\left(\\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x}\\right) \\right]\n$$\nThis is the final first-order sensitivity expression. It requires solving one forward linear system for $\\mathbf{x}$ and one adjoint linear system for $\\mathbf{y}$.\n\n**Part 2: Numerical Evaluation**\n\nWe now apply this formula to the given numerical instance at $\\alpha=0.2$.\n\n1. **Assemble the matrix $\\mathbf{A}(\\alpha)$ at $\\alpha=0.2$:**\nThe parameterized permittivity vector is $\\boldsymbol{\\epsilon}(\\alpha) = \\boldsymbol{\\epsilon}_0 + \\alpha \\mathbf{p}$:\n$$\n\\boldsymbol{\\epsilon}(0.2) = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix} + 0.2 \\begin{pmatrix} 0.5 \\\\ -0.25 \\end{pmatrix} = \\begin{pmatrix} 2.1 \\\\ 2.95 \\end{pmatrix}\n$$\nWith $\\omega = 1$, the matrix $\\mathbf{A}(\\alpha)$ is $\\mathbf{A}(\\alpha) = \\mathbf{K} - \\operatorname{diag}(\\boldsymbol{\\epsilon}(\\alpha))\\,\\mathbf{M}$:\n$$\n\\mathbf{A}(0.2) = \\begin{pmatrix} 3  1 + i \\\\ 1 - i  4 \\end{pmatrix} - \\begin{pmatrix} 2.1  0 \\\\ 0  2.95 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 3  1 + i \\\\ 1 - i  4 \\end{pmatrix} - \\begin{pmatrix} 4.2  0 \\\\ 0  2.95 \\end{pmatrix}\n$$\n$$\n\\mathbf{A}(0.2) = \\begin{pmatrix} -1.2  1 + i \\\\ 1 - i  1.05 \\end{pmatrix}\n$$\n\n2. **Solve the forward system for $\\mathbf{x}(0.2)$:**\nWe solve $\\mathbf{A}(0.2)\\mathbf{x} = \\mathbf{b}$:\n$$\n\\begin{pmatrix} -1.2  1 + i \\\\ 1 - i  1.05 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ i \\end{pmatrix}\n$$\nThe determinant is $\\det(\\mathbf{A}) = (-1.2)(1.05) - (1+i)(1-i) = -1.26 - 2 = -3.26$.\nThe inverse is $\\mathbf{A}^{-1} = \\frac{1}{-3.26}\\begin{pmatrix} 1.05  -1-i \\\\ -1+i  -1.2 \\end{pmatrix}$.\nThe solution is $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$:\n$$\n\\mathbf{x} = \\frac{1}{-3.26} \\begin{pmatrix} 1.05 - i(1+i) \\\\ -1+i - 1.2i \\end{pmatrix} = \\frac{1}{-3.26} \\begin{pmatrix} 1.05 - i + 1 \\\\ -1 - 0.2i \\end{pmatrix} = \\frac{1}{-3.26} \\begin{pmatrix} 2.05 - i \\\\ -1 - 0.2i \\end{pmatrix}\n$$\n\n3. **Solve the adjoint system for $\\mathbf{y}(0.2)$:**\nThe matrix $\\mathbf{K}$ is Hermitian, and $\\operatorname{diag}(\\boldsymbol{\\epsilon}) \\mathbf{M}$ is real and symmetric (thus Hermitian). Therefore, $\\mathbf{A}(\\alpha)$ is Hermitian, so $\\mathbf{A}^H = \\mathbf{A}$. The adjoint system is $\\mathbf{A} \\mathbf{y} = \\mathbf{C}^H(\\mathbf{C}\\mathbf{x} - \\mathbf{d})$.\nGiven $\\mathbf{C} = \\begin{pmatrix} 1  0 \\end{pmatrix}$ and $\\mathbf{d} = 0$, the right-hand side is:\n$$\n\\mathbf{C}^H(\\mathbf{C}\\mathbf{x}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\left( \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\right) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} x_1 = \\begin{pmatrix} x_1 \\\\ 0 \\end{pmatrix}\n$$\nSo we solve $\\mathbf{A}\\mathbf{y} = \\begin{pmatrix} x_1 \\\\ 0 \\end{pmatrix}$. The solution is $\\mathbf{y} = \\mathbf{A}^{-1} \\begin{pmatrix} x_1 \\\\ 0 \\end{pmatrix}$:\n$$\n\\mathbf{y} = \\frac{1}{-3.26} \\begin{pmatrix} 1.05  -1-i \\\\ -1+i  -1.2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ 0 \\end{pmatrix} = \\frac{x_1}{-3.26} \\begin{pmatrix} 1.05 \\\\ -1+i \\end{pmatrix}\n$$\nSubstituting $x_1 = \\frac{2.05-i}{-3.26}$:\n$$\n\\mathbf{y} = \\frac{2.05-i}{(-3.26)^2} \\begin{pmatrix} 1.05 \\\\ -1+i \\end{pmatrix} = \\frac{2.05-i}{10.6276} \\begin{pmatrix} 1.05 \\\\ -1+i \\end{pmatrix}\n$$\n\n4. **Evaluate the sensitivity $\\frac{dJ}{d\\alpha}$:**\nFirst, we find the derivative of the matrix $\\mathbf{A}$ with respect to $\\alpha$:\n$$\n\\frac{\\partial \\mathbf{A}}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left( \\mathbf{K} - \\omega^2 \\operatorname{diag}(\\boldsymbol{\\epsilon}_0 + \\alpha \\mathbf{p})\\,\\mathbf{M} \\right) = -\\omega^2 \\operatorname{diag}(\\mathbf{p})\\,\\mathbf{M}\n$$\nPlugging in the values with $\\omega=1$:\n$$\n\\frac{\\partial \\mathbf{A}}{\\partial \\alpha} = -1^2 \\begin{pmatrix} 0.5  0 \\\\ 0  -0.25 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix} = -\\begin{pmatrix} 1  0 \\\\ 0  -0.25 \\end{pmatrix} = \\begin{pmatrix} -1  0 \\\\ 0  0.25 \\end{pmatrix}\n$$\nThe sensitivity is $\\frac{dJ}{d\\alpha} = -\\operatorname{Re}\\left[ \\mathbf{y}^H \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} \\right]$. Let's compute the term in brackets:\n$$\n\\mathbf{y}^H = \\frac{2.05+i}{10.6276} \\begin{pmatrix} 1.05  -1-i \\end{pmatrix}\n$$\n$$\n\\frac{\\partial \\mathbf{A}}{\\partial \\alpha}\\mathbf{x} = \\begin{pmatrix} -1  0 \\\\ 0  0.25 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} -x_1 \\\\ 0.25 x_2 \\end{pmatrix}\n$$\n$$\n\\mathbf{y}^H \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} = \\frac{2.05+i}{10.6276} \\left( 1.05(-x_1) + (-1-i)(0.25 x_2) \\right) = \\frac{- (2.05+i)}{10.6276} \\left( 1.05 x_1 + 0.25(1+i)x_2 \\right)\n$$\nLet $c=-3.26$. We have $x_1 = \\frac{2.05-i}{c}$ and $x_2 = \\frac{-1-0.2i}{c}$.\nThe term in parentheses is:\n$$\n1.05 \\frac{2.05-i}{c} + 0.25(1+i) \\frac{-1-0.2i}{c} = \\frac{1}{c} \\left[ (2.1525 - 1.05i) + (0.25)(-1-0.2i-i+0.2) \\right]\n$$\n$$\n= \\frac{1}{c} \\left[ 2.1525 - 1.05i - 0.2 - 0.3i \\right] = \\frac{1.9525 - 1.35i}{c}\n$$\nSo, we have:\n$$\n\\mathbf{y}^H \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} = \\frac{- (2.05+i)}{10.6276} \\frac{1.9525 - 1.35i}{-3.26} = \\frac{- (2.05+i)(1.9525 - 1.35i)}{-34.645976}\n$$\nNumerator product: $(2.05+i)(1.9525 - 1.35i) = 4.002625 - 2.7675i + 1.9525i + 1.35 = 5.352625 - 0.815i$.\n$$\n\\mathbf{y}^H \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} = \\frac{-(5.352625 - 0.815i)}{-34.645976} = \\frac{5.352625 - 0.815i}{34.645976} \\approx 0.154497 - 0.023523i\n$$\nFinally, the sensitivity is:\n$$\n\\frac{dJ}{d\\alpha} = -\\operatorname{Re}\\left[ 0.154497 - 0.023523i \\right] = -0.154497\n$$\nRounding to four significant figures, we get $-0.1545$.",
            "answer": "$$ \\boxed{-0.1545} $$"
        },
        {
            "introduction": "A correct theoretical derivation is necessary, but not sufficient; a correct implementation is the goal. This practice addresses the crucial step of code verification, as adjoint implementations are notoriously prone to subtle bugs. You will implement an adjoint-based gradient for a 2D scalar wave problem and validate it against the 'gold standard' of a finite-difference gradient check, a technique essential for any serious practitioner .",
            "id": "3312449",
            "problem": "Construct a complete, runnable program that validates an adjoint-based gradient for a frequency-domain scalar electromagnetics model by comparing it against finite-difference checks. The program must implement a two-dimensional discrete scalar Helmholtz problem with zero Dirichlet boundary conditions, an objective function that measures mismatch at selected probe locations with optional quadratic regularization on the material parameter, and both the analytical adjoint gradient and finite-difference gradient approximations. The final output must be a single line containing a list of floating-point numbers that quantify gradient-validation errors for a given test suite.\n\nThe starting point is a frequency-domain scalar Helmholtz equation derived from Maxwell’s equations under a single-polarization, homogeneous magnetic permeability assumption. On a rectangular grid, with nondimensionalized units, the discrete forward problem is\n$$\nA(\\boldsymbol{\\varepsilon}) \\, u = \\mathbf{b},\n$$\nwhere $u \\in \\mathbb{C}^{N}$ is the vector of field values at interior nodes, $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{N}$ is the vector of relative permittivity values (one per interior node), and $\\mathbf{b} \\in \\mathbb{C}^{N}$ is a fixed source. The operator is\n$$\nA(\\boldsymbol{\\varepsilon}) = L - c \\,\\mathrm{diag}(\\boldsymbol{\\varepsilon}),\n$$\nwhere $L \\in \\mathbb{R}^{N \\times N}$ is the standard $5$-point finite-difference Laplacian with zero Dirichlet boundaries and unit grid spacing, and $c = \\omega^{2} + \\mathrm{i}\\,\\eta$ for angular frequency $\\omega$ and loss parameter $\\eta \\ge 0$. The interior unknowns are the nodes excluding the boundary; for a grid with $N_{x}$ and $N_{y}$ total nodes including boundary layers, the number of unknowns is $N = (N_{x}-2)(N_{y}-2)$.\n\nThe objective function to be minimized is\n$$\nJ(\\boldsymbol{\\varepsilon}) = \\tfrac{1}{2}\\,\\lVert S u(\\boldsymbol{\\varepsilon}) - u_{\\mathrm{t}}\\rVert_{2}^{2} + \\tfrac{\\alpha}{2}\\,\\lVert \\boldsymbol{\\varepsilon} - \\varepsilon_{\\mathrm{ref}} \\rVert_{2}^{2},\n$$\nwhere $S$ is a linear selection operator that extracts the components of $u$ at specified measurement nodes (probes), $u_{\\mathrm{t}}$ is the target field at those probes, $\\alpha \\ge 0$ is a real regularization weight, and $\\varepsilon_{\\mathrm{ref}}$ is a reference permittivity vector. All quantities are nondimensional; no physical units are required.\n\nYour program must:\n- Assemble the interior-grid Laplacian $L$ for each test case using a $5$-point stencil with unit grid spacing and zero Dirichlet boundary conditions.\n- Solve the forward problem $A(\\boldsymbol{\\varepsilon}) u = \\mathbf{b}$ for $u$.\n- Compute the objective $J(\\boldsymbol{\\varepsilon})$.\n- Derive and implement the analytical adjoint gradient $\\nabla_{\\boldsymbol{\\varepsilon}} J$ using a principled first-variation and adjoint approach based on the discrete equations and the complex-valued inner products appropriate for frequency-domain electromagnetics.\n- Independently compute a central finite-difference approximation of $\\nabla_{\\boldsymbol{\\varepsilon}} J$ with step size $h$ specified below.\n- Perform a directional-derivative check by comparing $\\nabla_{\\boldsymbol{\\varepsilon}} J \\cdot p$ against a central-difference approximation of $\\tfrac{\\mathrm{d}}{\\mathrm{d}t} J(\\boldsymbol{\\varepsilon} + t p)$ at a small scalar step $t$ along a specified random direction $p$.\n- Report, for each test case, a single floating-point number equal to the maximum of two relative errors: the relative $\\ell_{2}$-error between the analytical and finite-difference gradient vectors, and the relative error between the analytical and finite-difference directional derivatives.\n\nDefinitions and implementation details:\n- Grid and indexing: Let the total grid have $N_{x}$ and $N_{y}$ nodes including boundary layers. The interior index set is $\\{(i,j)\\,|\\, i=0,\\dots,N_{x}-3;\\, j=0,\\dots,N_{y}-3\\}$, with column-major flattening $k = i + (N_{x}-2)\\,j$, yielding $N=(N_{x}-2)(N_{y}-2)$.\n- Laplacian $L$: For each interior node $(i,j)$ with flattened index $k$, set\n  $$\n  L_{k,k} = -4,\\quad\n  L_{k,k\\pm 1} = 1 \\text{ for valid horizontal neighbors},\\quad\n  L_{k,k\\pm (N_{x}-2)} = 1 \\text{ for valid vertical neighbors}.\n  $$\n- Selection operator $S$: Implement $S$ implicitly by indexing the appropriate entries in $u$; its Hermitian transpose $S^{H}$ corresponds to scattering measurement residuals back into the full interior-sized vector at the selected indices.\n- Objective $J$: Use the standard complex least-squares form with Hermitian inner product on the residual, i.e., $\\tfrac{1}{2}\\,\\lVert r \\rVert_{2}^{2}$ where $r = S u - u_{\\mathrm{t}}$, which equals $\\tfrac{1}{2}\\, r^{H} r$.\n- Finite-difference gradient: Use central differences,\n  $$\n  \\left[\\nabla_{\\boldsymbol{\\varepsilon}} J\\right]_{i} \\approx \\frac{J(\\boldsymbol{\\varepsilon} + h e_{i}) - J(\\boldsymbol{\\varepsilon} - h e_{i})}{2h},\n  $$\n  with step size $h = 10^{-6}$, where $e_{i}$ is the $i$-th canonical basis vector.\n- Directional-derivative check: For a given direction $p \\in \\mathbb{R}^{N}$ and step $t = 10^{-6}$, compare $\\nabla_{\\boldsymbol{\\varepsilon}} J(\\boldsymbol{\\varepsilon})^{\\top} p$ against\n  $$\n  \\frac{J(\\boldsymbol{\\varepsilon} + t p) - J(\\boldsymbol{\\varepsilon} - t p)}{2 t}.\n  $$\n\nTest suite:\n- Case A (general case):\n  - Grid: $N_{x} = 10$, $N_{y} = 12$.\n  - Frequency and loss: $\\omega = 5.0$, $\\eta = 0.2$ so $c = \\omega^{2} + \\mathrm{i}\\,\\eta$.\n  - Regularization: $\\alpha = 0.01$, $\\varepsilon_{\\mathrm{ref}}$ is the constant vector with entries $1.5$.\n  - Source: single point source of unit amplitude at interior index $(i_{s}, j_{s})$ with $i_{s} = \\lfloor (N_{x}-2)/3 \\rfloor$, $j_{s} = \\lfloor (N_{y}-2)/2 \\rfloor$.\n  - Initial permittivity: for interior coordinates $(i,j)$, set\n    $$\n    \\varepsilon_{0}(i,j) = 2.0 + 0.1 \\sin\\!\\left( \\frac{2\\pi i}{N_{x}-2} \\right) \\sin\\!\\left( \\frac{3\\pi j}{N_{y}-2} \\right).\n    $$\n  - Measurements: all interior nodes where $i \\bmod 3 = 1$ and $j \\bmod 4 = 2$.\n  - Target field: $u_{\\mathrm{t}}$ is the zero vector of appropriate length.\n- Case B (boundary regularization case):\n  - Grid: $N_{x} = 8$, $N_{y} = 8$.\n  - Frequency and loss: $\\omega = 3.0$, $\\eta = 0.05$, so $c = \\omega^{2} + \\mathrm{i}\\,\\eta$.\n  - Regularization: $\\alpha = 0$, $\\varepsilon_{\\mathrm{ref}}$ is the constant vector with entries $1.0$.\n  - Source: single point source of unit amplitude at the interior center $(i_{c}, j_{c})$ with $i_{c} = \\lfloor (N_{x}-2)/2 \\rfloor$, $j_{c} = \\lfloor (N_{y}-2)/2 \\rfloor$.\n  - Initial permittivity: Gaussian bump,\n    $$\n    \\varepsilon_{0}(i,j) = 1.0 + 0.2 \\exp\\!\\left( -\\frac{(i-i_{c})^{2} + (j-j_{c})^{2}}{2 \\sigma^{2}} \\right), \\quad \\sigma = 2.0.\n    $$\n  - Measurements: single interior probe at $(i_{c}, j_{c})$.\n  - Target field: $u_{\\mathrm{t}}$ is a scalar with value $0.1$ at the single probe.\n- Case C (nearer-to-resonance but stabilized by loss):\n  - Grid: $N_{x} = 12$, $N_{y} = 12$.\n  - Frequency and loss: $\\omega = 10.0$, $\\eta = 0.8$, so $c = \\omega^{2} + \\mathrm{i}\\,\\eta$.\n  - Regularization: $\\alpha = 0.001$, $\\varepsilon_{\\mathrm{ref}}$ is the constant vector with entries $1.0$.\n  - Source: two unit-amplitude point sources at interior indices $(i_{c}, j_{c})$ and $(i_{c}-2, j_{c}+1)$ where $i_{c} = \\lfloor (N_{x}-2)/2 \\rfloor$, $j_{c} = \\lfloor (N_{y}-2)/2 \\rfloor$.\n  - Initial permittivity: uniform,\n    $$\n    \\varepsilon_{0}(i,j) = 1.5.\n    $$\n  - Measurements: cross through the interior center, i.e., all interior nodes with $i = i_{c}$ and $j$ even, together with all interior nodes with $j = j_{c}$ and $i$ even; duplicates are included only once.\n  - Target field: $u_{\\mathrm{t}}$ is the zero vector of appropriate length.\n\nValidation metrics and output:\n- For each case, compute:\n  - The relative $\\ell_{2}$-error between the analytical gradient and the central finite-difference gradient,\n    $$\n    E_{\\mathrm{grad}} = \\frac{\\lVert \\nabla_{\\boldsymbol{\\varepsilon}} J - \\nabla_{\\boldsymbol{\\varepsilon}}^{\\mathrm{FD}} J \\rVert_{2}}{\\max\\{\\lVert \\nabla_{\\boldsymbol{\\varepsilon}}^{\\mathrm{FD}} J \\rVert_{2},\\, 10^{-12}\\}}.\n    $$\n  - The relative error in the directional derivative along a deterministic random direction $p$,\n    $$\n    E_{\\mathrm{dir}} = \\frac{\\left| \\nabla_{\\boldsymbol{\\varepsilon}} J^{\\top} p - \\frac{J(\\boldsymbol{\\varepsilon} + t p) - J(\\boldsymbol{\\varepsilon} - t p)}{2 t} \\right|}{\\max\\left\\{\\left|\\frac{J(\\boldsymbol{\\varepsilon} + t p) - J(\\boldsymbol{\\varepsilon} - t p)}{2 t}\\right|,\\, 10^{-12}\\right\\}},\n    $$\n    using $t = 10^{-6}$ and a fixed seed for the random number generator unique to each case.\n- For each case, report a single float\n  $$\n  E = \\max\\{E_{\\mathrm{grad}},\\, E_{\\mathrm{dir}}\\}.\n  $$\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[e_A,e_B,e_C]\"), where $e_{\\mathrm{A}}$, $e_{\\mathrm{B}}$, and $e_{\\mathrm{C}}$ are the values of $E$ for Case A, Case B, and Case C, respectively.\n\nNo user input or external files are permitted. All computations must be performed using the specified test suite and parameters. Angles do not appear; there is no angle-unit requirement. All quantities are nondimensional; there is no physical-unit requirement.",
            "solution": "The user requests a Python program to validate an adjoint-based gradient for a 2D scalar Helmholtz problem against finite-difference approximations. The problem is well-defined, scientifically sound, and provides all necessary parameters and equations for a complete implementation. The physics is based on the standard scalar Helmholtz equation, and the validation methodology (gradient checking via finite differences and directional derivatives) is a cornerstone of gradient-based optimization. The problem is valid.\n\nThe solution proceeds in several steps for each test case:\n1.  **Discretization Setup**: The continuous 2D domain is discretized onto a grid. The interior nodes, where the field is unknown, are flattened into a 1D vector of size $N = (N_x-2)(N_y-2)$. The 5-point finite-difference Laplacian operator $L$ for zero Dirichlet boundary conditions is constructed as a sparse matrix.\n\n2.  **Forward Problem**: The discrete Helmholtz equation is a linear system $A(\\boldsymbol{\\varepsilon})u = \\mathbf{b}$, where $A(\\boldsymbol{\\varepsilon}) = L - c \\cdot \\mathrm{diag}(\\boldsymbol{\\varepsilon})$ with $c = \\omega^2 + i\\eta$. This system is solved for the complex-valued field vector $u$ using a sparse linear solver.\n\n3.  **Objective Function**: The objective function $J(\\boldsymbol{\\varepsilon})$ is defined as the sum of a field mismatch term and a regularization term:\n    $$\n    J(\\boldsymbol{\\varepsilon}) = \\tfrac{1}{2}\\,\\lVert S u(\\boldsymbol{\\varepsilon}) - u_{\\mathrm{t}}\\rVert_{2}^{2} + \\tfrac{\\alpha}{2}\\,\\lVert \\boldsymbol{\\varepsilon} - \\varepsilon_{\\mathrm{ref}} \\rVert_{2}^{2}\n    $$\n    Here, $S$ is a selection operator extracting field values at probe locations. The first term, involving complex vectors, is computed as $\\tfrac{1}{2} r^H r$, where $r = S u(\\boldsymbol{\\varepsilon}) - u_{\\mathrm{t}}$ is the residual.\n\n4.  **Adjoint Gradient Derivation**: To compute the gradient $\\nabla_{\\boldsymbol{\\varepsilon}} J$ efficiently, the adjoint method is used. We start with the first variation of $J$:\n    $$\n    \\delta J = \\mathrm{Re}\\left[ (Su - u_{\\mathrm{t}})^H S (\\delta u) \\right] + \\alpha (\\boldsymbol{\\varepsilon} - \\varepsilon_{\\mathrm{ref}})^{\\top} \\delta\\boldsymbol{\\varepsilon}\n    $$\n    The variation of the field, $\\delta u$, is related to the variation of the permittivity, $\\delta\\boldsymbol{\\varepsilon}$, through the forward equation:\n    $$\n    \\delta(A u) = 0 \\implies (\\delta A) u + A (\\delta u) = 0 \\implies \\delta u = -A^{-1} (\\delta A) u\n    $$\n    Since $A = L - c \\cdot \\mathrm{diag}(\\boldsymbol{\\varepsilon})$, its variation is $\\delta A = -c \\cdot \\mathrm{diag}(\\delta\\boldsymbol{\\varepsilon})$. This yields:\n    $$\n    \\delta u = c A^{-1} \\mathrm{diag}(u) \\delta\\boldsymbol{\\varepsilon}\n    $$\n    Substituting this into the expression for $\\delta J$:\n    $$\n    \\delta J = \\mathrm{Re}\\left[ c \\, (Su - u_{\\mathrm{t}})^H S A^{-1} \\mathrm{diag}(u) \\delta\\boldsymbol{\\varepsilon} \\right] + \\alpha (\\boldsymbol{\\varepsilon} - \\varepsilon_{\\mathrm{ref}})^{\\top} \\delta\\boldsymbol{\\varepsilon}\n    $$\n    To avoid the computationally expensive matrix inverse $A^{-1}$, we introduce an adjoint problem. Let the adjoint source be $r_{\\text{adj}} = S^H(Su - u_{\\mathrm{t}})$. The expression can be rewritten using the property $(X A^{-1} Y) = ((A^{-1})^H X)^H Y = ((A^H)^{-1} X)^H Y$. Let the adjoint field $\\lambda$ be the solution to the adjoint equation:\n    $$\n    A^H \\lambda = r_{\\text{adj}}\n    $$\n    where $A^H = (L - c \\cdot \\mathrm{diag}(\\boldsymbol{\\varepsilon}))^H = L^T - c^* \\cdot \\mathrm{diag}(\\boldsymbol{\\varepsilon})^T$. Since $L$ is real and symmetric ($L^T=L$) and $\\boldsymbol{\\varepsilon}$ is real, $A^H = L - c^* \\cdot \\mathrm{diag}(\\boldsymbol{\\varepsilon})$.\n    With $\\lambda = (A^H)^{-1} r_{\\text{adj}}$, the variation becomes:\n    $$\n    \\delta J = \\mathrm{Re}\\left[ c \\, \\lambda^H \\mathrm{diag}(u) \\delta\\boldsymbol{\\varepsilon} \\right] + \\alpha (\\boldsymbol{\\varepsilon} - \\varepsilon_{\\mathrm{ref}})^{\\top} \\delta\\boldsymbol{\\varepsilon}\n    $$\n    The term $\\lambda^H \\mathrm{diag}(u) \\delta\\boldsymbol{\\varepsilon}$ is equivalent to the inner product of the element-wise product $(\\overline{\\lambda} \\odot u)$ with $\\delta\\boldsymbol{\\varepsilon}$, where $\\overline{\\lambda}$ is the complex conjugate of $\\lambda$. Thus, we can identify the gradient:\n    $$\n    \\nabla_{\\boldsymbol{\\varepsilon}} J = \\mathrm{Re}[c \\cdot (\\overline{\\lambda} \\odot u)] + \\alpha (\\boldsymbol{\\varepsilon} - \\varepsilon_{\\mathrm{ref}})\n    $$\n    where `Re[...]` and the element-wise product `$\\odot$` are applied component-wise. This expression is computationally efficient as it requires only one forward solve (for $u$) and one adjoint solve (for $\\lambda$).\n\n5.  **Finite-Difference Gradient**: For validation, the gradient is also approximated using the central finite-difference formula for each component $i$:\n    $$\n    \\left[\\nabla_{\\boldsymbol{\\varepsilon}}^{\\mathrm{FD}} J\\right]_{i} = \\frac{J(\\boldsymbol{\\varepsilon} + h e_{i}) - J(\\boldsymbol{\\varepsilon} - h e_{i})}{2h}\n    $$\n    This requires $2N$ evaluations of the objective function, making it much slower than the adjoint method but useful for verification.\n\n6.  **Directional Derivative Check**: A further check is performed by comparing the analytical directional derivative, $\\nabla_{\\boldsymbol{\\varepsilon}} J^{\\top} p$, with a finite-difference approximation along a random direction $p$:\n    $$\n    \\frac{\\mathrm{d}J(\\boldsymbol{\\varepsilon} + t p)}{\\mathrm{d}t}\\bigg|_{t=0} \\approx \\frac{J(\\boldsymbol{\\varepsilon} + t p) - J(\\boldsymbol{\\varepsilon} - t p)}{2 t}\n    $$\n    This test is computationally cheap (two objective function evaluations) and provides a robust check of the overall gradient implementation.\n\n7.  **Error Calculation**: The validation is quantified by two relative errors: $E_{\\text{grad}}$, the normalized $\\ell_2$ distance between the adjoint and finite-difference gradients, and $E_{\\text{dir}}$, the relative error between the analytical and finite-difference directional derivatives. The final reported error for each test case is $E = \\max\\{E_{\\text{grad}}, E_{\\text{dir}}\\}$.\n\nThe implementation will encapsulate this logic, processing each test case from the provided suite and producing the specified list of error values.",
            "answer": "```python\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient validation program for a set of test cases.\n    \"\"\"\n\n    def run_case(params):\n        \"\"\"\n        Processes a single test case for gradient validation.\n        \"\"\"\n        # Unpack parameters\n        Nx, Ny = params['grid']\n        omega, eta = params['freq_loss']\n        alpha, eps_ref_val = params['reg']\n        source_locs = params['source_locs']\n        eps0_func = params['eps0_func']\n        probe_cond = params['probe_cond']\n        ut_def = params['ut_def']\n        seed = params['seed']\n\n        # h for full gradient FD, t for directional derivative FD\n        h = 1e-6\n        t = 1e-6\n\n        # Grid and indexing setup\n        nx_int, ny_int = Nx - 2, Ny - 2\n        N = nx_int * ny_int\n        \n        # Create 1D grid coord arrays for vectorized eps0 calculation\n        i_coords = np.arange(nx_int)\n        j_coords = np.arange(ny_int)\n        i_grid, j_grid = np.meshgrid(i_coords, j_coords, indexing='ij')\n\n        # Build sparse 5-point Laplacian L with zero Dirichlet BCs\n        def build_laplacian(ni, nj):\n            num_nodes = ni * nj\n            diagonals = [\n                np.ones(num_nodes),    # Main diagonal fill\n                np.ones(num_nodes-1),  # Off-diagonal for i neighbors\n                np.ones(num_nodes-1),  # Off-diagonal for i neighbors\n                np.ones(num_nodes-ni), # Off-diagonal for j neighbors\n                np.ones(num_nodes-ni)  # Off-diagonal for j neighbors\n            ]\n            diagonals[0] = -4.0 * diagonals[0]\n            offsets = [0, -1, 1, -ni, ni]\n            L = sp.diags(diagonals, offsets, shape=(num_nodes, num_nodes), format='csr')\n\n            # Remove connections between rows for the +/- 1 diagonals\n            for j in range(1, nj):\n                k = j * ni - 1\n                if k  num_nodes - 1:\n                    L[k, k + 1] = 0\n                    L[k + 1, k] = 0\n            return L\n\n        L = build_laplacian(nx_int, ny_int)\n\n        # Setup problem constants and vectors\n        c = omega**2 + 1j * eta\n        # Flatten with 'F' for Fortran/column-major order, matching k = i + nx_int*j\n        eps0 = eps0_func(i_grid, j_grid).flatten('F')\n        eps_ref = np.full(N, eps_ref_val)\n\n        b = np.zeros(N, dtype=complex)\n        for isrc, jsrc in source_locs:\n            k_src = isrc + nx_int * jsrc\n            b[k_src] = 1.0\n\n        probe_indices = [i + nx_int * j for j in j_coords for i in i_coords if probe_cond(i, j)]\n        u_t = ut_def(len(probe_indices))\n\n        # Helper function to compute the objective J(eps)\n        def compute_objective(epsilon_vec):\n            A = L - c * sp.diags(epsilon_vec)\n            u = spsolve(A, b)\n            \n            residual = u[probe_indices] - u_t\n            J_field = 0.5 * np.real(np.vdot(residual, residual))\n            J_reg = 0.5 * alpha * np.linalg.norm(epsilon_vec - eps_ref)**2\n            return J_field + J_reg\n\n        # 1. Analytical Adjoint Gradient\n        A0 = L - c * sp.diags(eps0)\n        u0 = spsolve(A0, b)\n        \n        residual = u0[probe_indices] - u_t\n        adj_source = np.zeros(N, dtype=complex)\n        adj_source[probe_indices] = residual\n        \n        AH = L - np.conj(c) * sp.diags(eps0) # L is real-symmetric, eps0 is real\n        lambda_vec = spsolve(AH, adj_source)\n        \n        grad_adj_field = np.real(c * np.conj(lambda_vec) * u0)\n        grad_adj_reg = alpha * (eps0 - eps_ref)\n        grad_adj = grad_adj_field + grad_adj_reg\n\n        # 2. Finite-Difference Gradient\n        grad_fd = np.zeros(N)\n        for i in range(N):\n            eps_plus = eps0.copy()\n            eps_minus = eps0.copy()\n            eps_plus[i] += h\n            eps_minus[i] -= h\n            J_plus = compute_objective(eps_plus)\n            J_minus = compute_objective(eps_minus)\n            grad_fd[i] = (J_plus - J_minus) / (2.0 * h)\n\n        # 3. Directional Derivative Check\n        rng = np.random.default_rng(seed)\n        p = rng.random(N)\n        \n        dir_deriv_adj = np.dot(grad_adj, p)\n        \n        J_plus_p = compute_objective(eps0 + t * p)\n        J_minus_p = compute_objective(eps0 - t * p)\n        dir_deriv_fd = (J_plus_p - J_minus_p) / (2.0 * t)\n\n        # 4. Compute Errors\n        E_grad_num = np.linalg.norm(grad_adj - grad_fd)\n        E_grad_den = max(np.linalg.norm(grad_fd), 1e-12)\n        E_grad = E_grad_num / E_grad_den\n\n        E_dir_num = np.abs(dir_deriv_adj - dir_deriv_fd)\n        E_dir_den = max(np.abs(dir_deriv_fd), 1e-12)\n        E_dir = E_dir_num / E_dir_den\n        \n        return max(E_grad, E_dir)\n\n    # Test Suite Definition\n    test_cases = [\n        # Case A\n        {\n            'grid': (10, 12), 'freq_loss': (5.0, 0.2), 'reg': (0.01, 1.5),\n            'source_locs': [(int((10-2)/3), int((12-2)/2))],\n            'eps0_func': lambda i, j: 2.0 + 0.1 * np.sin(2*np.pi*i/(10-2)) * np.sin(3*np.pi*j/(12-2)),\n            'probe_cond': lambda i, j: i % 3 == 1 and j % 4 == 2,\n            'ut_def': lambda n_probes: np.zeros(n_probes),\n            'seed': 1\n        },\n        # Case B\n        {\n            'grid': (8, 8), 'freq_loss': (3.0, 0.05), 'reg': (0.0, 1.0),\n            'source_locs': [(int((8-2)/2), int((8-2)/2))],\n            'eps0_func': lambda i, j: (lambda ic, jc, s: 1.0 + 0.2 * np.exp(-((i-ic)**2 + (j-jc)**2) / (2*s**2)))(int((8-2)/2), int((8-2)/2), 2.0),\n            'probe_cond': lambda i, j: i == int((8-2)/2) and j == int((8-2)/2),\n            'ut_def': lambda n_probes: np.array([0.1]),\n            'seed': 2\n        },\n        # Case C\n        {\n            'grid': (12, 12), 'freq_loss': (10.0, 0.8), 'reg': (0.001, 1.0),\n            'source_locs': [(int((12-2)/2), int((12-2)/2)), (int((12-2)/2)-2, int((12-2)/2)+1)],\n            'eps0_func': lambda i, j: np.full_like(i, 1.5, dtype=float),\n            'probe_cond': lambda i, j: (i == int((12-2)/2) and j % 2 == 0) or (j == int((12-2)/2) and i % 2 == 0),\n            'ut_def': lambda n_probes: np.zeros(n_probes),\n            'seed': 3\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        error = run_case(case)\n        results.append(error)\n\n    # Print results in the required format\n    print(f\"[{','.join(f'{r:.8e}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With the fundamentals of derivation and verification established, we now apply these skills to a practical design problem using a different numerical framework. This exercise involves minimizing the radar cross-section (RCS) of a dielectric object modeled with the Method of Moments (MoM), which leads to dense system matrices. You will implement a complete cycle: solve the forward and adjoint problems, compute the gradient, and perform a design update to improve the scattering performance .",
            "id": "3312420",
            "problem": "You are given a two-dimensional transverse-magnetic scattering problem formulated via a volume integral equation and discretized using the Method of Moments (MoM). The total electric field $E_z(\\mathbf{r})$ in free space at angular frequency $\\omega$ satisfies the Lippmann–Schwinger equation for an inhomogeneous dielectric distribution:\n$$\nE_z(\\mathbf{r}) = E_z^{\\text{inc}}(\\mathbf{r}) + k_0^2 \\int_{\\Omega} G(\\mathbf{r},\\mathbf{r}') \\chi(\\mathbf{r}') E_z(\\mathbf{r}') \\, dA',\n$$\nwhere $k_0 = \\omega \\sqrt{\\mu_0 \\epsilon_0}$ is the free-space wavenumber, $\\chi(\\mathbf{r}) = \\epsilon_r(\\mathbf{r}) - 1$ is the dielectric contrast where $\\epsilon_r$ is the relative permittivity, $\\Omega$ is the support of the scatterer, and $G(\\mathbf{r},\\mathbf{r}') = \\frac{j}{4} H_0^{(1)}(k_0 \\lvert \\mathbf{r} - \\mathbf{r}' \\rvert)$ is the two-dimensional scalar Green's function with $H_0^{(1)}$ the Hankel function of the first kind of order zero. The incident field $E_z^{\\text{inc}}(\\mathbf{r})$ is a unit-amplitude plane wave.\n\nUsing pulse (constant) basis and point-collocation testing at $N$ point-cells with areas $\\{A_i\\}_{i=1}^N$ and centers $\\{\\mathbf{r}_i\\}_{i=1}^N$, the integral equation is discretized into the dense linear system\n$$\nZ(\\boldsymbol{\\epsilon}_r) \\, \\mathbf{x} = \\mathbf{b}, \\quad \\text{with} \\quad Z(\\boldsymbol{\\epsilon}_r) = I - k_0^2 G \\, D(\\boldsymbol{\\epsilon}_r),\n$$\nwhere $I$ is the $N \\times N$ identity matrix, $G \\in \\mathbb{C}^{N \\times N}$ has entries $G_{ij} = \\frac{j}{4} H_0^{(1)}(k_0 \\lvert \\mathbf{r}_i - \\mathbf{r}_j \\rvert)$ for $i \\neq j$ and $G_{ii} = 0$ under a low-contrast collocation approximation, $D(\\boldsymbol{\\epsilon}_r) = \\operatorname{diag}(\\chi_1 A_1, \\dots, \\chi_N A_N)$, $\\chi_i = \\epsilon_{r,i} - 1$, $\\mathbf{x} \\in \\mathbb{C}^N$ is the vector of total field samples $E_z(\\mathbf{r}_i)$ inside $\\Omega$, and $\\mathbf{b} \\in \\mathbb{C}^N$ has entries $b_i = E_z^{\\text{inc}}(\\mathbf{r}_i)$. The impedance matrix $Z(\\boldsymbol{\\epsilon}_r)$ is dense due to the long-range nature of $G$.\n\nDefine the far-field scattering amplitude in direction $\\hat{\\mathbf{s}}$ by the discrete approximation\n$$\nq(\\boldsymbol{\\epsilon}_r) = \\sum_{i=1}^N \\left( \\chi_i A_i \\, E_z(\\mathbf{r}_i) \\, e^{-j k_0 \\hat{\\mathbf{s}} \\cdot \\mathbf{r}_i} \\right) = \\mathbf{s}^T D(\\boldsymbol{\\epsilon}_r) \\mathbf{x},\n$$\nwith $\\mathbf{s} \\in \\mathbb{C}^N$ having entries $s_i = e^{-j k_0 \\hat{\\mathbf{s}} \\cdot \\mathbf{r}_i}$. As a scalar objective, use the squared magnitude\n$$\nJ(\\boldsymbol{\\epsilon}_r) = \\lvert q(\\boldsymbol{\\epsilon}_r) \\rvert^2,\n$$\nwhich is proportional to the two-dimensional radar scattering width (the two-dimensional analogue of Radar Cross Section (RCS)). The optimization variable is the piecewise-constant relative permittivity vector $\\boldsymbol{\\epsilon}_r = (\\epsilon_{r,1}, \\dots, \\epsilon_{r,N})$.\n\nYour task is to implement a discrete adjoint-based gradient computation for $J(\\boldsymbol{\\epsilon}_r)$ with respect to $\\boldsymbol{\\epsilon}_r$, and perform a single gradient-descent update for specified test cases. Derive the expression for the gradient from first principles based on electromagnetic integral equations and linear algebraic adjoint sensitivity. In the discrete adjoint method, let $\\mathbf{x}$ solve $Z(\\boldsymbol{\\epsilon}_r) \\mathbf{x} = \\mathbf{b}$. The derivative of $Z(\\boldsymbol{\\epsilon}_r)$ with respect to $\\epsilon_{r,i}$ is\n$$\n\\frac{\\partial Z}{\\partial \\epsilon_{r,i}} = - k_0^2 \\, \\mathbf{g}_i \\, A_i \\, \\mathbf{e}_i^T, \\quad \\text{with } \\mathbf{g}_i = G \\mathbf{e}_i, \\text{ and } \\mathbf{e}_i \\text{ the } i\\text{-th standard basis vector}.\n$$\nThe discrete adjoint $\\boldsymbol{\\lambda}$ is defined by the conjugate transpose system\n$$\nZ(\\boldsymbol{\\epsilon}_r)^H \\boldsymbol{\\lambda} = \\frac{\\partial J}{\\partial \\mathbf{x}^*},\n$$\nwhere $^H$ denotes the conjugate transpose and $\\frac{\\partial J}{\\partial \\mathbf{x}^*}$ is the Wirtinger derivative with respect to the conjugate of $\\mathbf{x}$. For the objective $J(\\boldsymbol{\\epsilon}_r) = \\lvert \\mathbf{s}^T D(\\boldsymbol{\\epsilon}_r) \\mathbf{x} \\rvert^2$, this right-hand side equals\n$$\n\\frac{\\partial J}{\\partial \\mathbf{x}^*} = D(\\boldsymbol{\\epsilon}_r)^H \\mathbf{s} \\, q(\\boldsymbol{\\epsilon}_r).\n$$\nThe total gradient component for $\\epsilon_{r,i}$ is\n$$\n\\frac{\\partial J}{\\partial \\epsilon_{r,i}} = -2 \\operatorname{Re} \\left\\{ \\boldsymbol{\\lambda}^H \\left( \\frac{\\partial Z}{\\partial \\epsilon_{r,i}} \\right) \\mathbf{x} \\right\\} + 2 \\operatorname{Re} \\left\\{ q(\\boldsymbol{\\epsilon}_r)^* \\left( s_i \\, A_i \\, x_i \\right) \\right\\}.\n$$\nAssume the incident and observation directions are the same (monostatic, backscattering), $\\hat{\\mathbf{p}} = \\hat{\\mathbf{s}} = (1,0)$, and $E_z^{\\text{inc}}(\\mathbf{r}) = e^{j k_0 \\hat{\\mathbf{p}} \\cdot \\mathbf{r}}$. Use the following physically meaningful parameters in International System of Units (SI):\n- Free-space speed of light $c = 3.0 \\times 10^8$ meters per second, free-space permittivity $\\epsilon_0 = 8.854187817 \\times 10^{-12}$ farads per meter, and absolute permeability $\\mu_0 = 4\\pi \\times 10^{-7}$ henries per meter.\n- Frequency $f = 1.0 \\times 10^9$ hertz (so $k_0 = \\frac{2\\pi f}{c}$ radians per meter).\n- Four point-cells at positions $\\mathbf{r}_1 = (-d/2,-d/2)$, $\\mathbf{r}_2 = (d/2,-d/2)$, $\\mathbf{r}_3 = (-d/2,d/2)$, $\\mathbf{r}_4 = (d/2,d/2)$ with $d = 0.05$ meters and areas $A_i = d^2$ meters squared for $i = 1,\\dots,4$.\n\nPerform a single gradient-descent update\n$$\n\\epsilon_{r,i}^{\\text{new}} = \\max\\{ 1.0, \\, \\epsilon_{r,i} - \\alpha \\, \\frac{\\partial J}{\\partial \\epsilon_{r,i}} \\},\n$$\nwith step size $\\alpha = 0.2$ (dimensionless), and constrain updates via the $\\max$ to keep $\\epsilon_{r,i}^{\\text{new}} \\ge 1.0$ to ensure nonnegative dielectric contrast. For any index $i$ with a \"frozen\" mask indicating a perfectly electrically conducting (PEC)-like segment under an impedance boundary idealization, disable the update by setting $\\epsilon_{r,i}^{\\text{new}} = \\epsilon_{r,i}$, while still including the segment in the scattering calculation if its $\\epsilon_{r,i}$ contributes to $D(\\boldsymbol{\\epsilon}_r)$.\n\nYour program should implement:\n- Construction of the dense impedance matrix $Z(\\boldsymbol{\\epsilon}_r)$ and the incident vector $\\mathbf{b}$.\n- Solution of the forward system for $\\mathbf{x}$, computation of $q(\\boldsymbol{\\epsilon}_r)$, and $J(\\boldsymbol{\\epsilon}_r)$.\n- Solution of the adjoint system for $\\boldsymbol{\\lambda}$ and computation of the gradient vector $\\nabla_{\\boldsymbol{\\epsilon}_r} J$ using the formula above.\n- A single masked and clamped gradient-descent update of $\\boldsymbol{\\epsilon}_r$ with step size $\\alpha$.\n- Recompute $J(\\boldsymbol{\\epsilon}_r^{\\text{new}})$ after the update.\n\nTest Suite:\n- Case $1$: Dielectric-only, initial relative permittivity vector $\\boldsymbol{\\epsilon}_r = [2.5, 1.8, 2.2, 1.5]$, mask $\\mathbf{m} = [1, 1, 1, 1]$ (all entries updatable).\n- Case $2$: Boundary condition edge case, initial relative permittivity vector $\\boldsymbol{\\epsilon}_r = [1.0, 1.0, 1.0, 1.0]$, mask $\\mathbf{m} = [1, 1, 1, 1]$.\n- Case $3$: Composite with PEC-like frozen segments, initial relative permittivity vector $\\boldsymbol{\\epsilon}_r = [10.0, 2.0, 10.0, 2.0]$, mask $\\mathbf{m} = [0, 1, 0, 1]$ (only entries $2$ and $4$ are updated).\n\nAnswer Specification:\n- All geometry variables are in meters, angular quantities in radians, and frequency in hertz. The objective $J(\\boldsymbol{\\epsilon}_r)$ is dimensionless and should be reported as a float.\n- Your program should produce a single line of output containing the post-update objective values for all test cases as a comma-separated list enclosed in square brackets, in the order of the cases specified, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$.",
            "solution": "The problem requires the implementation of a single gradient-descent update step for an inverse electromagnetic scattering problem. The goal is to optimize the dielectric permittivity distribution of a scatterer to manipulate its far-field scattering pattern. The method specified is the discrete adjoint method, which is a computationally efficient technique for calculating the gradient of an objective function with respect to a large number of design parameters.\n\nFirst, we must validate the problem statement. All physical constants, parameters, material properties, and geometric configurations are provided. The governing equations—the Lippmann-Schwinger integral equation, its Method of Moments (MoM) discretization, the definition of the far-field scattering amplitude, and the form of the objective function—are standard in computational electromagnetics. The core of the problem lies in the provided formulas for the adjoint method.\n\nA critical ambiguity exists in the problem statement regarding the optimization variable. The text specifies the variable as the absolute permittivity vector $\\boldsymbol{\\epsilon} = (\\epsilon_1, \\dots, \\epsilon_N)$, with units of farads per meter. However, the provided derivative formulas, such as $\\frac{\\partial Z}{\\partial \\epsilon_i} = - k_0^2 \\, \\mathbf{g}_i \\, V_i \\, \\mathbf{e}_i^T$ with $V_i=A_i$, are only mathematically consistent if the optimization variable is the dimensionless relative permittivity vector $\\boldsymbol{\\epsilon}_r = (\\epsilon_{r,1}, \\dots, \\epsilon_{r,N})$ where $\\epsilon_{r,i} = \\epsilon_i/\\epsilon_0$. This is because the dielectric contrast is $\\chi_i = \\epsilon_{r,i} - 1$, and its derivative with respect to $\\epsilon_{r,i}$ is simply $1$, whereas its derivative with respect to $\\epsilon_i$ is $1/\\epsilon_0$. The provided formulas omit this $1/\\epsilon_0$ factor. Furthermore, the test cases supply permittivity values as dimensionless numbers, described as \"scaled by $\\epsilon_0$\", and the gradient update clamp is at the dimensionless value $1.0$, which corresponds to the relative permittivity of vacuum. Based on this overwhelming evidence, we must conclude that the intended optimization variable is the relative permittivity $\\boldsymbol{\\epsilon}_r$. The solution will proceed under this necessary interpretation.\n\nAnother minor point is the formula for the adjoint source, $\\frac{\\partial J}{\\partial \\mathbf{x}^*} = D(\\boldsymbol{\\epsilon})^H \\mathbf{s} \\, q(\\boldsymbol{\\epsilon})$. Standard derivations often yield a complex conjugate on the vector $\\mathbf{s}$. However, as the problem statement is otherwise self-consistent under the $\\boldsymbol{\\epsilon}_r$ interpretation, we will adhere strictly to the provided formulas as given. The problem is thus deemed valid and solvable under this interpretation.\n\nThe solution proceeds as follows:\n\n**1. System Setup and Constant Definition**\nWe begin by defining the physical constants and problem parameters.\n- Free-space constants: speed of light $c = 3.0 \\times 10^8 \\, \\text{m/s}$, permittivity $\\epsilon_0 = 8.854187817 \\times 10^{-12} \\, \\text{F/m}$.\n- Operating frequency $f = 1.0 \\times 10^9 \\, \\text{Hz}$.\n- From these, we calculate the angular frequency $\\omega = 2\\pi f$ and the free-space wavenumber $k_0 = \\omega/c$.\n- The geometry consists of $N=4$ square cells of side length $d = 0.05 \\, \\text{m}$ and area $A_i = d^2 = 0.0025 \\, \\text{m}^2$. The cell centers are $\\{\\mathbf{r}_i\\}_{i=1}^4$ at $(\\pm d/2, \\pm d/2)$.\n- The incident plane wave propagates along $\\hat{\\mathbf{p}} = (1,0)$, and the backscattering is observed in the same direction, $\\hat{\\mathbf{s}} = (1,0)$.\n- The gradient descent step size is $\\alpha = 0.2$.\n\nFor each test case, we are given an initial relative permittivity vector $\\boldsymbol{\\epsilon}_r$ and an update mask $\\mathbf{m}$.\n\n**2. Forward Problem Solution**\nThe core of the analysis is solving the discretized Lippmann-Schwinger equation $Z(\\boldsymbol{\\epsilon}_r) \\mathbf{x} = \\mathbf{b}$ for the total electric field vector $\\mathbf{x}$.\n\n- **Green's Function Matrix $G$**: An $N \\times N$ matrix $G$ is constructed. Its entries are given by $G_{ij} = \\frac{j}{4} H_0^{(1)}(k_0 |\\mathbf{r}_i - \\mathbf{r}_j|)$ for $i \\neq j$, and $G_{ii} = 0$. $H_0^{(1)}$ is the Hankel function of the first kind and order zero.\n- **Contrast Matrix $D(\\boldsymbol{\\epsilon}_r)$**: The dielectric contrast vector is $\\boldsymbol{\\chi} = \\boldsymbol{\\epsilon}_r - 1$. The matrix $D$ is a diagonal matrix with entries $D_{ii} = \\chi_i A_i$.\n- **Impedance Matrix $Z(\\boldsymbol{\\epsilon}_r)$**: The system matrix is $Z = I - k_0^2 G D$, where $I$ is the $N \\times N$ identity matrix.\n- **Incident Field Vector $\\mathbf{b}$**: The right-hand side vector $\\mathbf{b}$ contains the incident field values at the cell centers, $b_i = E_z^{\\text{inc}}(\\mathbf{r}_i) = e^{j k_0 \\hat{\\mathbf{p}} \\cdot \\mathbf{r}_i}$.\n- **Field Solution $\\mathbf{x}$**: With $Z$ and $\\mathbf{b}$ constructed, the dense complex linear system is solved for $\\mathbf{x} = Z^{-1} \\mathbf{b}$.\n\n**3. Objective Function Calculation**\nThe objective function is the squared magnitude of the far-field scattering amplitude, $J(\\boldsymbol{\\epsilon}_r) = |q(\\boldsymbol{\\epsilon}_r)|^2$.\n\n- **Observation Vector $\\mathbf{s}$**: This vector is defined by the phase factors for the far-field calculation, $s_i = e^{-j k_0 \\hat{\\mathbf{s}} \\cdot \\mathbf{r}_i}$.\n- **Scattering Amplitude $q(\\boldsymbol{\\epsilon}_r)$**: This scalar value is computed as $q = \\mathbf{s}^T D(\\boldsymbol{\\epsilon}_r) \\mathbf{x}$.\n- **Objective $J(\\boldsymbol{\\epsilon}_r)$**: The objective is then simply $J = |q|^2 = q^*q$.\n\n**4. Adjoint Problem Solution**\nThe discrete adjoint method avoids the costly computation of $\\frac{\\partial \\mathbf{x}}{\\partial \\epsilon_{r,i}}$ by introducing an adjoint state vector $\\boldsymbol{\\lambda}$.\n\n- **Adjoint Source**: The right-hand side of the adjoint equation is given by $\\frac{\\partial J}{\\partial \\mathbf{x}^*} = D^H \\mathbf{s} q$. Since $D$ is a real diagonal matrix, $D^H = D$.\n- **Adjoint Equation**: The adjoint vector $\\boldsymbol{\\lambda} \\in \\mathbb{C}^N$ is found by solving the linear system $Z^H \\boldsymbol{\\lambda} = \\frac{\\partial J}{\\partial \\mathbf{x}^*}$, where $Z^H$ is the conjugate transpose of $Z$.\n\n**5. Gradient Calculation**\nThe gradient of the objective function with respect to each component of the relative permittivity vector, $\\frac{\\partial J}{\\partial \\epsilon_{r,i}}$, is computed using the forward field $\\mathbf{x}$ and the adjoint field $\\boldsymbol{\\lambda}$. The formula provided is:\n$$\n\\frac{\\partial J}{\\partial \\epsilon_{r,i}} = -2 \\operatorname{Re} \\left\\{ \\boldsymbol{\\lambda}^H \\left( \\frac{\\partial Z}{\\partial \\epsilon_{r,i}} \\right) \\mathbf{x} \\right\\} + 2 \\operatorname{Re} \\left\\{ q^* \\left( s_i A_i x_i \\right) \\right\\}\n$$\nThe derivative of the impedance matrix is $\\frac{\\partial Z}{\\partial \\epsilon_{r,i}} = -k_0^2 G \\frac{\\partial D}{\\partial \\epsilon_{r,i}} = -k_0^2 G (A_i \\mathbf{e}_i \\mathbf{e}_i^T) = -k_0^2 A_i \\mathbf{g}_i \\mathbf{e}_i^T$, where $\\mathbf{g}_i = G \\mathbf{e}_i$ is the $i$-th column of $G$.\nSubstituting this into the gradient formula gives:\n$$\n\\frac{\\partial J}{\\partial \\epsilon_{r,i}} = -2 \\operatorname{Re} \\left\\{ \\boldsymbol{\\lambda}^H (-k_0^2 A_i x_i \\mathbf{g}_i) \\right\\} + 2 \\operatorname{Re} \\left\\{ q^* s_i A_i x_i \\right\\}\n= 2 A_i \\operatorname{Re} \\left\\{ x_i \\left( k_0^2 (\\boldsymbol{\\lambda}^H \\mathbf{g}_i) + q^* s_i \\right) \\right\\}\n$$\nThis expression is computed for each $i=1, \\dots, N$ to form the gradient vector $\\nabla_{\\boldsymbol{\\epsilon}_r} J$.\n\n**6. Gradient Descent Update**\nA single step of gradient descent is performed.\n\n- **Masking**: The computed gradient vector is multiplied element-wise by the mask vector $\\mathbf{m}$ to freeze specified permittivity values.\n- **Update**: The new permittivity vector is calculated as $\\boldsymbol{\\epsilon}_r^{\\text{new}} = \\boldsymbol{\\epsilon}_r - \\alpha \\, (\\mathbf{m} \\odot \\nabla_{\\boldsymbol{\\epsilon}_r} J)$, where $\\odot$ denotes element-wise multiplication.\n- **Clamping**: To ensure physical plausibility (non-negative contrast), the updated permittivities are constrained: $\\epsilon_{r,i}^{\\text{new}} = \\max\\{1.0, \\epsilon_{r,i}^{\\text{new}}\\}$.\n\nFinally, the objective function $J(\\boldsymbol{\\epsilon}_r^{\\text{new}})$ is re-computed using the updated permittivity vector $\\boldsymbol{\\epsilon}_r^{\\text{new}}$ by repeating steps 2 and 3. The resulting values for each test case are collected and printed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import hankel1\n\n# from scipy import ...\n\ndef solve():\n    # Define physical constants\n    C0 = 3.0e8  # Speed of light in vacuum (m/s)\n    EPS0 = 8.854187817e-12  # Permittivity of free space (F/m)\n    MU0 = 4 * np.pi * 1e-7  # Permeability of free space (H/m)\n\n    # Problem parameters\n    FREQ = 1.0e9  # Frequency (Hz)\n    D_GEOM = 0.05  # Side length of the square geometry (m)\n    ALPHA = 0.2  # Gradient descent step size\n\n    # Derived constants\n    OMEGA = 2 * np.pi * FREQ\n    K0 = OMEGA / C0\n\n    # Geometry setup\n    N = 4 # Number of cells\n    r_coords = (D_GEOM / 2.0) * np.array([\n        [-1.0, -1.0],\n        [1.0, -1.0],\n        [-1.0, 1.0],\n        [1.0, 1.0]\n    ])\n    A_cell = D_GEOM**2 # Area of each cell\n\n    # Incident and observation directions\n    p_hat = np.array([1.0, 0.0])\n    s_hat = np.array([1.0, 0.0])\n\n    # Pre-compute distance matrix and Green's function matrix structure\n    dist_matrix = np.zeros((N, N))\n    for i in range(N):\n        for j in range(N):\n            dist_matrix[i, j] = np.linalg.norm(r_coords[i, :] - r_coords[j, :])\n\n    G = np.zeros((N, N), dtype=complex)\n    for i in range(N):\n        for j in range(N):\n            if i != j:\n                G[i, j] = 1j / 4 * hankel1(0, K0 * dist_matrix[i, j])\n    \n    # Pre-compute incident field vector b and observation vector s\n    b = np.exp(1j * K0 * (r_coords @ p_hat))\n    s_vec = np.exp(-1j * K0 * (r_coords @ s_hat))\n\n    def compute_objective_and_gradient(eps_r):\n        \"\"\"\n        Computes the objective function J and its gradient wrt eps_r.\n        \"\"\"\n        chi = eps_r - 1.0\n        D = np.diag(chi * A_cell)\n        \n        Z = np.eye(N) - K0**2 * G @ D\n        \n        # Forward solve\n        try:\n            x = np.linalg.solve(Z, b)\n        except np.linalg.LinAlgError:\n            return np.inf, np.zeros_like(eps_r)\n        \n        # Objective calculation\n        q = s_vec.T @ D @ x\n        J = np.abs(q)**2\n        \n        # Adjoint solve\n        # problem statement gives D^H s q, D is real so D^H = D\n        adj_rhs = D @ s_vec * q\n        \n        try:\n            lambda_ = np.linalg.solve(Z.conj().T, adj_rhs)\n        except np.linalg.LinAlgError:\n             return J, np.zeros_like(eps_r)\n        \n        # Gradient calculation\n        grad_J = np.zeros(N)\n        # Vectorized calculation for efficiency\n        # term1_vec = k0^2 * (lambda_H * G)\n        # problem: term is lambda_H * g_i = lambda_.conj().T @ G[:, i]\n        # This is the i-th element of lambda_.conj().T @ G\n        w_row = K0**2 * (lambda_.conj().T @ G)\n        \n        grad_terms = x * (w_row.T + np.conj(q) * s_vec)\n        grad_J = 2 * A_cell * np.real(grad_terms)\n        \n        return J, grad_J\n\n    def compute_objective(eps_r):\n        \"\"\"\n        Computes only the objective function value J.\n        \"\"\"\n        chi = eps_r - 1.0\n        D = np.diag(chi * A_cell)\n        \n        Z = np.eye(N) - K0**2 * G @ D\n        \n        try:\n            x = np.linalg.solve(Z, b)\n        except np.linalg.LinAlgError:\n            return np.inf\n\n        q = s_vec.T @ D @ x\n        J = np.abs(q)**2\n        return J\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Dielectric-only\n        {'eps_r_initial': np.array([2.5, 1.8, 2.2, 1.5]), 'mask': np.array([1, 1, 1, 1])},\n        # Case 2: Boundary condition edge case\n        {'eps_r_initial': np.array([1.0, 1.0, 1.0, 1.0]), 'mask': np.array([1, 1, 1, 1])},\n        # Case 3: Composite with frozen segments\n        {'eps_r_initial': np.array([10.0, 2.0, 10.0, 2.0]), 'mask': np.array([0, 1, 0, 1])},\n    ]\n\n    results = []\n    for case in test_cases:\n        eps_r = case['eps_r_initial'].copy()\n        mask = case['mask']\n\n        # 1. Compute gradient at initial point\n        J_initial, grad_J = compute_objective_and_gradient(eps_r)\n\n        # 2. Apply mask to gradient\n        masked_grad = grad_J * mask\n        \n        # 3. Perform a single gradient descent update\n        eps_r_new = eps_r - ALPHA * masked_grad\n        \n        # 4. Apply clamp to ensure eps_r >= 1.0\n        eps_r_new = np.maximum(1.0, eps_r_new)\n        \n        # 5. Recompute the objective function at the new point\n        J_new = compute_objective(eps_r_new)\n        \n        results.append(J_new)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}