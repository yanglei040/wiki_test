{
    "hands_on_practices": [
        {
            "introduction": "In evolutionary optimization, the fitness function serves as the sole guide, translating complex physical behavior into a single scalar objective. However, numerical simulations, such as the Finite-Difference Time-Domain (FDTD) method, are imperfect and can introduce artifacts like spurious reflections from Perfectly Matched Layer (PML) boundaries. This exercise  challenges you to think like a practitioner by designing an objective function that not only measures the desired physical performance but also penalizes these numerical errors in a way that is robust and fair, ensuring the optimization process is not misled by simulation-dependent noise.",
            "id": "3306079",
            "problem": "In a Finite-Difference Time-Domain (FDTD) simulation of a scattering structure embedded in a rectangular computational domain, the outer boundary is terminated by a Perfectly Matched Layer (PML). A parametric geometry is encoded by a vector of design variables $\\,\\boldsymbol{\\theta}\\,$ that modulates a spatially varying permittivity $\\,\\varepsilon(\\mathbf{x};\\boldsymbol{\\theta})\\,$ within a compact region strictly separated from the PML. A compactly supported current source $\\,\\mathbf{J}_{\\mathrm{s}}(\\mathbf{x},t)\\,$ drives the system from time $\\,t=0\\,$ to time $\\,t=\\tau_{\\mathrm{src}}\\,$, and the simulation runs until a fixed horizon $\\,t=T_{\\mathrm{end}}\\,$ using a common spatial grid and time step for all candidates $\\,\\boldsymbol{\\theta}\\,$. You wish to use an evolutionary algorithm to minimize a physical objective $\\,J_{\\mathrm{phys}}(\\boldsymbol{\\theta})\\,$, but you observe that late-time spurious reflections originating at the PML boundaries can contaminate the time-domain fields and thereby distort $\\,J_{\\mathrm{phys}}(\\boldsymbol{\\theta})\\,$ in a design-dependent manner. To robustify the fitness, you decide to augment $\\,J_{\\mathrm{phys}}(\\boldsymbol{\\theta})\\,$ with a penalty that measures late-time field energy near the PML. Let $\\,\\Omega_{\\mathrm{PML}}\\,$ denote a thin monitoring shell co-located with and entirely within the PML region, and let $\\,\\mathbf{E}_{\\mathrm{PML}}(t;\\mathbf{x})\\,$ denote the electric field restricted to $\\,\\Omega_{\\mathrm{PML}}\\,$. The window $[t_{1},t_{2}]$ is to be chosen to capture late-time artifacts while excluding direct-source transients. From first principles of electromagnetics and numerical stability considerations, select the most appropriate augmented fitness and the associated justification for how its construction yields consistent evaluation across different $\\,\\boldsymbol{\\theta}\\,$ under an evolutionary algorithm.\n\nWhich option is the most appropriate?\n\nA. Define\n$$\nJ_{\\mathrm{aug}}(\\boldsymbol{\\theta}) \\;=\\; J_{\\mathrm{phys}}(\\boldsymbol{\\theta}) \\;+\\; \\alpha \\,\\frac{\\displaystyle \\int_{t_{1}}^{t_{2}} \\int_{\\Omega_{\\mathrm{PML}}} \\|\\mathbf{E}_{\\mathrm{PML}}(t;\\mathbf{x})\\|^{2}\\,\\mathrm{d}\\mathbf{x}\\,\\mathrm{d}t}{\\displaystyle \\int_{0}^{T_{\\mathrm{end}}} \\int_{\\Omega_{\\mathrm{src}}} \\left|\\mathbf{J}_{\\mathrm{s}}(\\mathbf{x},t)\\cdot \\mathbf{E}(\\mathbf{x},t)\\right|\\,\\mathrm{d}\\mathbf{x}\\,\\mathrm{d}t},\n$$\nwith $\\,\\alpha>0\\,$ fixed, where $\\,\\Omega_{\\mathrm{src}}\\,$ is the compact source support. Choose $\\,t_{1}=\\tau_{\\mathrm{src}}+t_{\\mathrm{exit},\\max}+\\delta\\,$ and $\\,t_{2}=T_{\\mathrm{end}}\\,$, where $\\,t_{\\mathrm{exit},\\max}=D_{\\max}/v_{\\min}\\,$ is a conservative upper bound on the time for any wave packet to traverse from the source region to $\\,\\Omega_{\\mathrm{PML}}\\,$ given the known domain diameter $\\,D_{\\max}\\,$ and a lower bound $\\,v_{\\min}\\,$ on the phase velocity implied by material bounds across all feasible $\\,\\boldsymbol{\\theta}\\,$, and $\\,\\delta>0\\,$ is a small safety margin; these choices and the normalization are held fixed across all candidates $\\,\\boldsymbol{\\theta}\\,$.\n\nB. Define\n$$\nJ_{\\mathrm{aug}}(\\boldsymbol{\\theta}) \\;=\\; J_{\\mathrm{phys}}(\\boldsymbol{\\theta}) \\;+\\; \\alpha \\,\\int_{t_{1}(\\boldsymbol{\\theta})}^{t_{2}(\\boldsymbol{\\theta})} \\int_{\\Omega_{\\mathrm{PML}}} \\|\\mathbf{E}_{\\mathrm{PML}}(t;\\mathbf{x})\\|^{2}\\,\\mathrm{d}\\mathbf{x}\\,\\mathrm{d}t,\n$$\nwhere $\\,t_{1}(\\boldsymbol{\\theta})\\,$ is defined for each $\\,\\boldsymbol{\\theta}\\,$ as the earliest time at which the field magnitude at a single interior probe $\\,\\mathbf{x}_{0}\\,$ falls below a fixed threshold, and $\\,t_{2}(\\boldsymbol{\\theta})\\,$ is the first subsequent time the probe crosses the same threshold again. No normalization is applied.\n\nC. Define\n$$\nJ_{\\mathrm{aug}}(\\boldsymbol{\\theta}) \\;=\\; J_{\\mathrm{phys}}(\\boldsymbol{\\theta}) \\;+\\; \\alpha \\,\\int_{0}^{T_{\\mathrm{end}}} \\int_{\\Omega} \\|\\mathbf{E}(t;\\mathbf{x})\\|^{2}\\,\\mathrm{d}\\mathbf{x}\\,\\mathrm{d}t,\n$$\nwhere $\\,\\Omega\\,$ is the entire computational domain (including but not restricted to the PML). The window is the full simulation $[0,T_{\\mathrm{end}}]$ to capture all energy, since spurious reflections are part of the total.\n\nD. Define\n$$\nJ_{\\mathrm{aug}}(\\boldsymbol{\\theta}) \\;=\\; J_{\\mathrm{phys}}(\\boldsymbol{\\theta}) \\;+\\; \\alpha \\,\\sup_{t\\in[t_{1},t_{2}]} \\|\\mathbf{E}_{\\mathrm{PML}}(t;\\mathbf{x}_{\\mathrm{b}})\\|^{2},\n$$\nwith a fixed probe point $\\,\\mathbf{x}_{\\mathrm{b}}\\,$ on the PML interface and a fixed late-time window $[t_{1},t_{2}]$ common to all $\\,\\boldsymbol{\\theta}\\,$. No normalization is applied because the supremum is scale-invariant up to a constant factor.",
            "solution": "The problem asks for the most appropriate formulation of an augmented fitness function $J_{\\mathrm{aug}}(\\boldsymbol{\\theta})$ for an evolutionary algorithm. The goal is to minimize a primary physical objective $J_{\\mathrm{phys}}(\\boldsymbol{\\theta})$, while simultaneously penalizing spurious late-time reflections from the Perfectly Matched Layer (PML) boundary of a Finite-Difference Time-Domain (FDTD) simulation. The key challenge is that the magnitude of these reflections can be design-dependent, and the penalty must provide a consistent and fair basis for comparing different designs $\\boldsymbol{\\theta}$.\n\nA valid penalty function must satisfy several criteria based on first principles of physics and numerical optimization:\n1.  **Specificity:** The penalty must selectively target the artifact of interest, namely the spurious, late-time reflections from the PML. It should not be sensitive to the legitimate, desired physical behavior, such as the initial wave propagation or scattering.\n2.  **Robustness:** The measurement should be insensitive to arbitrary choices like single probe locations or noisy instantaneous field values. It should capture the overall magnitude of the artifact, for instance, by integrating over the relevant spatial region and time interval.\n3.  **Consistency and Fairness:** This is the most critical requirement for an evolutionary algorithm. The algorithm compares fitness values of different candidate designs. If the penalty term is not properly normalized, it may unfairly punish designs that are, for example, stronger scatterers, even if the PML is performing equally well in a relative sense. The penalty should reflect the *performance* of the PML for a given design, not the absolute magnitude of the fields.\n\nLet's evaluate the proposed options against these criteria. The artifact to be measured is the energy of the spuriously reflected fields. The energy density of the electric field is proportional to $\\|\\mathbf{E}\\|^2$. Therefore, a sound penalty term will be based on the integral of $\\|\\mathbf{E}\\|^2$.\n\n**Option A Analysis**\nThe proposed augmented fitness is:\n$$\nJ_{\\mathrm{aug}}(\\boldsymbol{\\theta}) \\;=\\; J_{\\mathrm{phys}}(\\boldsymbol{\\theta}) \\;+\\; \\alpha \\,\\frac{\\displaystyle \\int_{t_{1}}^{t_{2}} \\int_{\\Omega_{\\mathrm{PML}}} \\|\\mathbf{E}_{\\mathrm{PML}}(t;\\mathbf{x})\\|^{2}\\,\\mathrm{d}\\mathbf{x}\\,\\mathrm{d}t}{\\displaystyle \\int_{0}^{T_{\\mathrm{end}}} \\int_{\\Omega_{\\mathrm{src}}} \\left|\\mathbf{J}_{\\mathrm{s}}(\\mathbf{x},t)\\cdot \\mathbf{E}(\\mathbf{x},t)\\right|\\,\\mathrm{d}\\mathbf{x}\\,\\mathrm{d}t}\n$$\n- **Specificity and Robustness:** The penalty term (the fraction) is constructed by integrating the electric field energy density ($\\|\\mathbf{E}\\|^2$) over a monitoring shell $\\Omega_{\\mathrm{PML}}$ within the PML. This spatial integration makes the measure robust against local field variations, unlike a single-point probe. The time integration is performed over a late-time window $[t_1, t_2]$. The start time $t_1 = \\tau_{\\mathrm{src}} + t_{\\mathrm{exit},\\max} + \\delta$ is rigorously defined to begin after any legitimate wave (from the source or scatterer) could have traversed the domain and exited through the PML. This timing isolates the late-time artifacts. Using $t_2 = T_{\\mathrm{end}}$ ensures all late-time effects within the simulation horizon are captured. The use of a fixed time window for all $\\boldsymbol{\\theta}$ is crucial for consistent comparison.\n- **Consistency and Fairness:** The numerator measures the total spurious reflected energy in the late-time window. Crucially, this quantity is normalized by the denominator, which is the integral of $|\\mathbf{J}_{\\mathrm{s}}\\cdot\\mathbf{E}|$. From Poynting's theorem, $\\mathbf{J}_{\\mathrm{s}}\\cdot\\mathbf{E}$ represents the power density delivered by the source to the electromagnetic field. The integral of its magnitude over the source volume and the relevant time represents the total energy coupled into the simulation for a particular design $\\boldsymbol{\\theta}$. By normalizing the reflected energy (numerator) by the total injected energy (denominator), the penalty becomes a dimensionless quantity analogous to a power reflection coefficient. This ratio measures the *relative* inefficiency of the PML for each specific design. It ensures that a design that strongly interacts with the source (large denominator) is not unfairly penalized for having a proportionally larger, but not relatively worse, reflection. This normalization is essential for a fair comparison across the design space.\n\nThis formulation meets all the required criteria.\n\n**Verdict:** **Correct**.\n\n**Option B Analysis**\nThe proposed augmented fitness is:\n$$\nJ_{\\mathrm{aug}}(\\boldsymbol{\\theta}) \\;=\\; J_{\\mathrm{phys}}(\\boldsymbol{\\theta}) \\;+\\; \\alpha \\,\\int_{t_{1}(\\boldsymbol{\\theta})}^{t_{2}(\\boldsymbol{\\theta})} \\int_{\\Omega_{\\mathrm{PML}}} \\|\\mathbf{E}_{\\mathrm{PML}}(t;\\mathbf{x})\\|^{2}\\,\\mathrm{d}\\mathbf{x}\\,\\mathrm{d}t\n$$\nThis option has three major deficiencies:\n1.  **No Normalization:** The penalty is an absolute measure of energy. A design $\\boldsymbol{\\theta}_A$ that scatters twice as much energy towards the PML as a design $\\boldsymbol{\\theta}_B$ would have a much larger penalty, even if the PML's relative reflection performance is identical for both. This biases the optimizer against strongly interacting designs.\n2.  **Inconsistent Time Window:** The time window $[t_1(\\boldsymbol{\\theta}), t_2(\\boldsymbol{\\theta})]$ is design-dependent. This means the fitness function is evaluating integrals over different domains for different candidates, which violates the principle of consistent comparison. The EA's selection process would be compromised.\n3.  **Fragile Window Definition:** Defining the time window based on a threshold crossing at a single probe point $\\mathbf{x}_0$ is not robust. The field behavior at a single point may not be representative of the global field decay, as it can be affected by local nulls or interferences. The choice of $\\mathbf{x}_0$ and the threshold are arbitrary and can lead to erratic behavior.\n\n**Verdict:** **Incorrect**.\n\n**Option C Analysis**\nThe proposed augmented fitness is:\n$$\nJ_{\\mathrm{aug}}(\\boldsymbol{\\theta}) \\;=\\; J_{\\mathrm{phys}}(\\boldsymbol{\\theta}) \\;+\\; \\alpha \\,\\int_{0}^{T_{\\mathrm{end}}} \\int_{\\Omega} \\|\\mathbf{E}(t;\\mathbf{x})\\|^{2}\\,\\mathrm{d}\\mathbf{x}\\,\\mathrm{d}t\n$$\nThe penalty term here is the total electric field energy in the entire domain $\\Omega$ over the entire simulation time.\n- **Lack of Specificity:** This measure is not specific to the PML reflection artifact. It is dominated by the energy of the source pulse and the primary scattered fields. The spurious reflected energy is typically a very small fraction of this total energy. Minimizing this term would not effectively target the reduction of PML reflections; instead, it would likely penalize any design that stores or radiates energy efficiently, which is contrary to the likely goal of the primary objective $J_{\\mathrm{phys}}$. The argument that \"spurious reflections are part of the total\" is true but misleading, as their contribution is masked.\n\n**Verdict:** **Incorrect**.\n\n**Option D Analysis**\nThe proposed augmented fitness is:\n$$\nJ_{\\mathrm{aug}}(\\boldsymbol{\\theta}) \\;=\\; J_{\\mathrm{phys}}(\\boldsymbol{\\theta}) \\;+\\; \\alpha \\,\\sup_{t\\in[t_{1},t_{2}]} \\|\\mathbf{E}_{\\mathrm{PML}}(t;\\mathbf{x}_{\\mathrm{b}})\\|^{2}\n$$\nThis option has several flaws:\n1.  **Lack of Spatial Robustness:** The penalty is based on the field at a single, fixed point $\\mathbf{x}_{\\mathrm{b}}$. The spatial distribution of reflected fields can be complex. An optimizer could find a trivial solution that creates a field null at $\\mathbf{x}_{\\mathrm{b}}$ while reflections are large elsewhere. A spatially integrated measure is far more robust.\n2.  **Lack of Temporal Robustness:** Using the supremum (maximum value) is less robust than integrating over time. The penalty can be dominated by a single transient spike, while ignoring a persistent, lower-amplitude reflection that contains more total energy. Time integration provides a more stable measure of the total reflected energy.\n3.  **No Normalization:** The claim that the supremum is scale-invariant is incorrect in this physical context. If a design doubles the incident field amplitude at the boundary, the reflected field amplitude will also double (for a linear system), and the penalty term $\\|\\mathbf{E}\\|^2$ will quadruple. This term is not scale-invariant and, like in option B, will unfairly penalize designs that produce stronger fields, irrespective of the PML's relative performance.\n\n**Verdict:** **Incorrect**.\n\nBased on this detailed analysis, Option A provides the only scientifically sound, robust, and consistent formulation for the augmented fitness function, making it the most appropriate choice for the stated problem.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "While evolutionary algorithms excel at exploring vast design spaces, their inherent randomness can produce solutions that violate fundamental physical laws. A crucial skill is to integrate physical constraints directly into the optimization loop to guide the search toward valid designs. This practice  focuses on enforcing the principle of reciprocity—a cornerstone of linear electromagnetics—by formulating a feasibility rule based on scattering matrix symmetry and implementing a mathematically optimal repair operator, providing a template for handling physics-based constraints in engineering optimization.",
            "id": "3306120",
            "problem": "Consider a linear, time-invariant, isotropic, source-free electromagnetic medium governed by Maxwell's equations. For a multiport structure excited by time-harmonic sources with angular frequency $\\omega$ expressed in radians per second, define the incident wave amplitudes vector $\\mathbf{a}(\\omega)$ and the scattered (outgoing) wave amplitudes vector $\\mathbf{b}(\\omega)$. The scattering matrix $\\mathbf{S}(\\omega; x)$ is the linear operator relating these quantities via $\\mathbf{b}(\\omega) = \\mathbf{S}(\\omega; x)\\,\\mathbf{a}(\\omega)$, where $x$ denotes the design parameters of the structure (e.g., geometric features or material property distributions).\n\nA foundational result derived from the Lorentz reciprocity theorem, applied to Maxwell's equations for linear time-invariant isotropic media with symmetric permittivity and permeability tensors, implies that for reciprocal systems the scattering matrix must be symmetric, i.e., $\\mathbf{S}(\\omega; x) = \\mathbf{S}^{\\top}(\\omega; x)$ for all $\\omega$.\n\nYou are tasked with formulating a feasibility rule suitable for use within an evolutionary optimization algorithm to enforce reciprocity. The feasibility rule must evaluate a candidate design $x$ at a finite set of sampled angular frequencies and declare the design feasible only if the simulated scattering matrices obey the reciprocity constraint within a specified numerical tolerance. Additionally, you must implement a repair operator that minimally modifies a given (possibly infeasible) $\\mathbf{S}(\\omega; x)$ to produce a symmetric matrix, thereby restoring feasibility without altering the underlying design excessively.\n\nYour solution must satisfy the following:\n\n- Starting from Maxwell's equations and the Lorentz reciprocity theorem, logically justify why reciprocity requires $\\mathbf{S}(\\omega; x) = \\mathbf{S}^{\\top}(\\omega; x)$.\n- Define a reciprocity violation metric using the Frobenius norm and construct a feasibility rule and a repair operator grounded in first principles. The feasibility rule must use a tolerance parameter $\\tau$.\n- The repair operator must be the symmetric projection $\\mathbf{R}(\\mathbf{S}) = \\tfrac{1}{2}\\left(\\mathbf{S} + \\mathbf{S}^{\\top}\\right)$ and you must justify, from a mathematical optimization perspective, that it minimally perturbs $\\mathbf{S}$ in the Frobenius norm among all symmetric matrices.\n- Implement a program that, for each test case below, computes the following four quantities:\n  1. The maximum reciprocity violation before repair, defined as $v_{\\text{before}} = \\max_{\\omega \\in \\Omega} \\left\\|\\mathbf{S}(\\omega; x) - \\mathbf{S}^{\\top}(\\omega; x)\\right\\|_{F}$.\n  2. The maximum reciprocity violation after repair, defined as $v_{\\text{after}} = \\max_{\\omega \\in \\Omega} \\left\\|\\mathbf{R}(\\mathbf{S}(\\omega; x)) - \\mathbf{R}(\\mathbf{S}(\\omega; x))^{\\top}\\right\\|_{F}$.\n  3. The feasibility after repair, defined as the boolean $b_{\\text{feasible}} = \\left[v_{\\text{after}} \\le \\tau\\right]$.\n  4. The maximum perturbation introduced by the repair, defined as $d_{\\text{max}} = \\max_{\\omega \\in \\Omega} \\left\\|\\mathbf{R}(\\mathbf{S}(\\omega; x)) - \\mathbf{S}(\\omega; x)\\right\\|_{F}$.\n- Use the numerical tolerance $\\tau = 10^{-8}$.\n- Express angular frequencies in radians per second; outputs are dimensionless real numbers and booleans. No percentage units are to be used.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case's result represented as a list in the form $[v_{\\text{before}}, v_{\\text{after}}, b_{\\text{feasible}}, d_{\\text{max}}]$.\n\nTest Suite:\nFor each test case, $\\Omega$ contains two angular frequencies, and $\\mathbf{S}(\\omega; x)$ is a $3\\times 3$ complex matrix specified below.\n\n- Test Case A (already reciprocal across sampled frequencies):\n  Frequencies: $\\Omega = \\{2\\pi \\cdot 10^{9},\\, 2\\pi \\cdot 2 \\cdot 10^{9}\\}$.\n  Matrices:\n  $$\n  \\mathbf{S}(2\\pi \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.10 & 0.02 & 0.03\\\\\n  0.02 & 0.20 & 0.01\\\\\n  0.03 & 0.01 & 0.15\n  \\end{bmatrix}\n  $$\n  $$\n  \\mathbf{S}(2\\pi \\cdot 2 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.08 + 0.02\\,\\mathrm{i} & 0.01 - 0.005\\,\\mathrm{i} & 0.00 + 0.01\\,\\mathrm{i}\\\\\n  0.01 - 0.005\\,\\mathrm{i} & 0.12 + 0.00\\,\\mathrm{i} & 0.02 + 0.003\\,\\mathrm{i}\\\\\n  0.00 + 0.01\\,\\mathrm{i} & 0.02 + 0.003\\,\\mathrm{i} & 0.09 - 0.01\\,\\mathrm{i}\n  \\end{bmatrix}\n  $$\n\n- Test Case B (slightly asymmetric due to small noise):\n  Frequencies: $\\Omega = \\{2\\pi \\cdot 1.2 \\cdot 10^{9},\\, 2\\pi \\cdot 1.8 \\cdot 10^{9}\\}$.\n  Matrices:\n  $$\n  \\mathbf{S}(2\\pi \\cdot 1.2 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.20 & 0.05 + 0.001 & 0.00 - 0.002\\\\\n  0.05 - 0.001 & 0.15 & 0.01 + 0.0005\\\\\n  0.00 + 0.002 & 0.01 - 0.0005 & 0.10\n  \\end{bmatrix}\n  $$\n  $$\n  \\mathbf{S}(2\\pi \\cdot 1.8 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.18 + 0.01\\,\\mathrm{i} & 0.02 + 0.00\\,\\mathrm{i} + 0.001 + 0.0005\\,\\mathrm{i} & 0.03 - 0.01\\,\\mathrm{i} - 0.001 - 0.0003\\,\\mathrm{i}\\\\\n  0.02 + 0.00\\,\\mathrm{i} - 0.001 - 0.0005\\,\\mathrm{i} & 0.16 - 0.005\\,\\mathrm{i} & 0.04 + 0.002\\,\\mathrm{i} + 0.0004 - 0.0002\\,\\mathrm{i}\\\\\n  0.03 - 0.01\\,\\mathrm{i} + 0.001 + 0.0003\\,\\mathrm{i} & 0.04 + 0.002\\,\\mathrm{i} - 0.0004 + 0.0002\\,\\mathrm{i} & 0.11 + 0.00\\,\\mathrm{i}\n  \\end{bmatrix}\n  $$\n\n- Test Case C (strongly asymmetric):\n  Frequencies: $\\Omega = \\{2\\pi \\cdot 0.8 \\cdot 10^{9},\\, 2\\pi \\cdot 1.6 \\cdot 10^{9}\\}$.\n  Matrices:\n  $$\n  \\mathbf{S}(2\\pi \\cdot 0.8 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.30 + 0.00\\,\\mathrm{i} & 0.10 + 0.05\\,\\mathrm{i} & -0.20 + 0.10\\,\\mathrm{i}\\\\\n  0.05 - 0.10\\,\\mathrm{i} & -0.10 + 0.00\\,\\mathrm{i} & 0.25 + 0.00\\,\\mathrm{i}\\\\\n  0.15 - 0.05\\,\\mathrm{i} & 0.40 + 0.20\\,\\mathrm{i} & 0.05 + 0.30\\,\\mathrm{i}\n  \\end{bmatrix}\n  $$\n  $$\n  \\mathbf{S}(2\\pi \\cdot 1.6 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.25 - 0.05\\,\\mathrm{i} & -0.35 + 0.10\\,\\mathrm{i} & 0.20 + 0.00\\,\\mathrm{i}\\\\\n  0.20 - 0.15\\,\\mathrm{i} & 0.30 + 0.05\\,\\mathrm{i} & -0.10 + 0.25\\,\\mathrm{i}\\\\\n  0.05 + 0.20\\,\\mathrm{i} & 0.12 - 0.30\\,\\mathrm{i} & -0.20 + 0.10\\,\\mathrm{i}\n  \\end{bmatrix}\n  $$\n\n- Test Case D (boundary case: zero scattering):\n  Frequencies: $\\Omega = \\{2\\pi \\cdot 10^{9},\\, 2\\pi \\cdot 1.5 \\cdot 10^{9}\\}$.\n  Matrices:\n  $$\n  \\mathbf{S}(2\\pi \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0 & 0 & 0\\\\\n  0 & 0 & 0\\\\\n  0 & 0 & 0\n  \\end{bmatrix}\n  \\quad,\\quad\n  \\mathbf{S}(2\\pi \\cdot 1.5 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0 & 0 & 0\\\\\n  0 & 0 & 0\\\\\n  0 & 0 & 0\n  \\end{bmatrix}\n  $$\n\n- Test Case E (frequency-dependent with moderate asymmetry):\n  Frequencies: $\\Omega = \\{2\\pi \\cdot 1.1 \\cdot 10^{9},\\, 2\\pi \\cdot 1.7 \\cdot 10^{9}\\}$.\n  Matrices:\n  $$\n  \\mathbf{S}(2\\pi \\cdot 1.1 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.12 & 0.03 + 0.005 & 0.025 - 0.004\\\\\n  0.03 - 0.005 & 0.18 & 0.02 + 0.003\\\\\n  0.025 + 0.004 & 0.02 - 0.003 & 0.14\n  \\end{bmatrix}\n  $$\n  $$\n  \\mathbf{S}(2\\pi \\cdot 1.7 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.10 + 0.02\\,\\mathrm{i} & 0.04 - 0.01\\,\\mathrm{i} - 0.01 + 0.005\\,\\mathrm{i} & 0.00 + 0.03\\,\\mathrm{i} + 0.006 - 0.004\\,\\mathrm{i}\\\\\n  0.04 - 0.01\\,\\mathrm{i} + 0.01 - 0.005\\,\\mathrm{i} & 0.20 + 0.00\\,\\mathrm{i} & 0.01 + 0.02\\,\\mathrm{i} - 0.005 + 0.002\\,\\mathrm{i}\\\\\n  0.00 + 0.03\\,\\mathrm{i} - 0.006 + 0.004\\,\\mathrm{i} & 0.01 + 0.02\\,\\mathrm{i} + 0.005 - 0.002\\,\\mathrm{i} & 0.13 - 0.01\\,\\mathrm{i}\n  \\end{bmatrix}\n  $$\n\nImplementation details and output specification:\n- Use $\\tau = 10^{-8}$.\n- Compute $v_{\\text{before}}$, $v_{\\text{after}}$, $b_{\\text{feasible}}$, and $d_{\\text{max}}$ for each test case exactly as defined above.\n- Your program should produce a single line of output containing a comma-separated list of the five test case results, each represented as a list $[v_{\\text{before}}, v_{\\text{after}}, b_{\\text{feasible}}, d_{\\text{max}}]$, enclosed in square brackets, for example:\n  $$\n  [\\,[v_{\\text{before}}^{(A)}, v_{\\text{after}}^{(A)}, b_{\\text{feasible}}^{(A)}, d_{\\text{max}}^{(A)}],\\,\\ldots,\\, [v_{\\text{before}}^{(E)}, v_{\\text{after}}^{(E)}, b_{\\text{feasible}}^{(E)}, d_{\\text{max}}^{(E)}]\\,]\n  $$\nNo input from the user is required; all data must be embedded in the program.",
            "solution": "The problem requires the formulation and implementation of a reciprocity constraint for use in evolutionary optimization algorithms applied to computational electromagnetics. This involves deriving the theoretical basis for the constraint, defining a quantitative measure of its violation, and constructing a repair operator that enforces the constraint while minimally perturbing the original system.\n\n### 1. Theoretical Justification for S-Matrix Symmetry from Lorentz Reciprocity\n\nThe symmetry of the scattering matrix, $\\mathbf{S} = \\mathbf{S}^{\\top}$, for a reciprocal multiport network is a direct consequence of the Lorentz reciprocity theorem. The derivation begins with Maxwell's equations for time-harmonic electromagnetic fields with an angular frequency $\\omega$, denoted by phasors $(\\vec{E}, \\vec{H})$.\n\nIn a source-free volume $V$ occupied by a linear, time-invariant, and isotropic medium, Maxwell's curl equations are:\n$$ \\nabla \\times \\vec{E} = -j\\omega\\mu\\vec{H} $$\n$$ \\nabla \\times \\vec{H} = j\\omega\\epsilon\\vec{E} $$\nThe material properties are described by the permittivity $\\epsilon$ and permeability $\\mu$. For an isotropic medium, these are scalars. More generally, for a reciprocal medium, they are symmetric tensors, i.e., $\\epsilon = \\epsilon^{\\top}$ and $\\mu = \\mu^{\\top}$.\n\nConsider two independent sets of sources producing two distinct sets of fields, $(\\vec{E}_1, \\vec{H}_1)$ and $(\\vec{E}_2, \\vec{H}_2)$, at the same frequency $\\omega$. By vector identity, we have:\n$$ \\nabla \\cdot (\\vec{E}_1 \\times \\vec{H}_2) = \\vec{H}_2 \\cdot (\\nabla \\times \\vec{E}_1) - \\vec{E}_1 \\cdot (\\nabla \\times \\vec{H}_2) = -j\\omega\\mu (\\vec{H}_1 \\cdot \\vec{H}_2) - j\\omega\\epsilon (\\vec{E}_1 \\cdot \\vec{E}_2) $$\n$$ \\nabla \\cdot (\\vec{E}_2 \\times \\vec{H}_1) = \\vec{H}_1 \\cdot (\\nabla \\times \\vec{E}_2) - \\vec{E}_2 \\cdot (\\nabla \\times \\vec{H}_1) = -j\\omega\\mu (\\vec{H}_2 \\cdot \\vec{H}_1) - j\\omega\\epsilon (\\vec{E}_2 \\cdot \\vec{E}_1) $$\nSubtracting these two equations yields $\\nabla \\cdot (\\vec{E}_1 \\times \\vec{H}_2 - \\vec{E}_2 \\times \\vec{H}_1) = 0$. Applying the divergence theorem over the volume $V$ leads to the integral form of the Lorentz reciprocity theorem:\n$$ \\oint_{\\partial V} (\\vec{E}_1 \\times \\vec{H}_2 - \\vec{E}_2 \\times \\vec{H}_1) \\cdot d\\vec{A} = 0 $$\nwhere $\\partial V$ is the boundary surface of $V$.\n\nFor a multiport device, this surface $\\partial V$ consists of the port apertures and the enclosing metallic or radiation boundaries. Assuming perfect conductors or a surface at infinity where fields vanish, the integral is non-zero only over the ports. The fields at each port $k$ can be decomposed into a superposition of guided modes. For a single-mode port, the field is a combination of an incident wave with amplitude $a_k$ and a scattered wave with amplitude $b_k$. The total tangential fields at port $k$ are related to these amplitudes and the port's normalized modal fields $(\\vec{e}_k, \\vec{h}_k)$. The surface integral reduces to a sum over all $N$ ports:\n$$ \\sum_{k=1}^{N} \\int_{A_k} (\\vec{E}_{1,t} \\times \\vec{H}_{2,t} - \\vec{E}_{2,t} \\times \\vec{H}_{1,t}) \\cdot d\\vec{A}_k = 0 $$\nWith proper normalization of the modal fields, this algebraic sum becomes a relationship between the incident and scattered wave amplitudes for the two states:\n$$ \\sum_{k=1}^{N} (a_{1k} b_{2k} - a_{2k} b_{1k}) = 0 $$\nIn vector form, this is $\\mathbf{a}_1^{\\top} \\mathbf{b}_2 - \\mathbf{a}_2^{\\top} \\mathbf{b}_1 = 0$. Using the definition of the scattering matrix, $\\mathbf{b} = \\mathbf{S}\\mathbf{a}$, we can substitute $\\mathbf{b}_1 = \\mathbf{S}\\mathbf{a}_1$ and $\\mathbf{b}_2 = \\mathbf{S}\\mathbf{a}_2$:\n$$ \\mathbf{a}_1^{\\top} (\\mathbf{S}\\mathbf{a}_2) - \\mathbf{a}_2^{\\top} (\\mathbf{S}\\mathbf{a}_1) = 0 $$\nSince $(\\mathbf{X}\\mathbf{Y})^\\top = \\mathbf{Y}^\\top\\mathbf{X}^\\top$ and a scalar is its own transpose, we can write $\\mathbf{a}_2^{\\top}\\mathbf{S}\\mathbf{a}_1 = (\\mathbf{a}_1^\\top \\mathbf{S}^\\top \\mathbf{a}_2)$. The equation becomes:\n$$ \\mathbf{a}_1^{\\top} \\mathbf{S} \\mathbf{a}_2 - \\mathbf{a}_1^{\\top} \\mathbf{S}^{\\top} \\mathbf{a}_2 = \\mathbf{a}_1^{\\top} (\\mathbf{S} - \\mathbf{S}^{\\top}) \\mathbf{a}_2 = 0 $$\nThis relationship must hold for any arbitrary choice of excitation vectors $\\mathbf{a}_1$ and $\\mathbf{a}_2$. This is only possible if the matrix in the middle is the zero matrix, i.e., $\\mathbf{S} - \\mathbf{S}^{\\top} = \\mathbf{0}$, which implies $\\mathbf{S} = \\mathbf{S}^{\\top}$. This proves that reciprocity requires the scattering matrix to be symmetric.\n\n### 2. Feasibility Rule and Reciprocity Violation Metric\n\nIn computational electromagnetics, numerical errors in simulation can lead to a scattering matrix $\\mathbf{S}$ that is not perfectly symmetric, even if the modeled physical system is reciprocal. For an optimization algorithm, we need a rule to decide if a candidate design is \"feasible\" with respect to the reciprocity constraint.\n\nA natural way to quantify the violation is to measure the \"size\" of the skew-symmetric component of the matrix, $\\mathbf{S} - \\mathbf{S}^{\\top}$. We use the Frobenius norm for this purpose. The Frobenius norm of a complex matrix $\\mathbf{A} \\in \\mathbb{C}^{m \\times n}$ is defined as:\n$$ \\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n} |A_{ij}|^2} $$\nThe reciprocity violation metric for a given scattering matrix $\\mathbf{S}(\\omega; x)$ is defined as:\n$$ v(\\mathbf{S}) = \\|\\mathbf{S}(\\omega; x) - \\mathbf{S}^{\\top}(\\omega; x)\\|_F $$\nFor an optimization problem evaluated over a set of frequencies $\\Omega$, the feasibility rule is constructed using a numerical tolerance $\\tau > 0$. A design $x$ is declared feasible if the maximum violation over all sampled frequencies is within this tolerance:\n$$ \\text{Design } x \\text{ is feasible if } \\max_{\\omega \\in \\Omega} v(\\mathbf{S}(\\omega; x)) \\le \\tau $$\n\n### 3. Repair Operator and its Optimality\n\nWhen a candidate design is found to be infeasible (i.e., its $\\mathbf{S}$ matrix is not sufficiently symmetric), a repair operator can be used to project it back into the feasible set. The proposed operator is the symmetric projection:\n$$ \\mathbf{R}(\\mathbf{S}) = \\frac{1}{2}(\\mathbf{S} + \\mathbf{S}^{\\top}) $$\nThis operator generates a symmetric matrix, as $(\\mathbf{R}(\\mathbf{S}))^{\\top} = \\frac{1}{2}(\\mathbf{S}^{\\top} + (\\mathbf{S}^{\\top})^{\\top}) = \\frac{1}{2}(\\mathbf{S}^{\\top} + \\mathbf{S}) = \\mathbf{R}(\\mathbf{S})$.\n\nWe must justify that this operator minimally perturbs $\\mathbf{S}$, which mathematically means that $\\mathbf{R}(\\mathbf{S})$ is the solution to the following optimization problem:\n$$ \\min_{\\mathbf{X}} \\|\\mathbf{S} - \\mathbf{X}\\|_F \\quad \\text{subject to} \\quad \\mathbf{X} = \\mathbf{X}^{\\top} $$\nWe seek the symmetric matrix $\\mathbf{X}$ that is closest to $\\mathbf{S}$ in the sense of the Frobenius norm. Let's minimize the squared norm, $f(\\mathbf{X}) = \\|\\mathbf{S} - \\mathbf{X}\\|_F^2$:\n$$ f(\\mathbf{X}) = \\sum_{i,j} |S_{ij} - X_{ij}|^2 $$\nWe can minimize this function by taking derivatives with respect to the independent elements of $\\mathbf{X}$ and setting them to zero. Since $\\mathbf{X}$ is symmetric ($X_{ij} = X_{ji}$), the independent variables are $X_{ii}$ for all $i$, and $X_{ij}$ for $i < j$.\n\nThe objective function can be written as:\n$$ f(\\mathbf{X}) = \\sum_{i} |S_{ii} - X_{ii}|^2 + \\sum_{i < j} (|S_{ij} - X_{ij}|^2 + |S_{ji} - X_{ji}|^2) $$\nSince $X_{ij} = X_{ji}$, this simplifies to:\n$$ f(\\mathbf{X}) = \\sum_{i} |S_{ii} - X_{ii}|^2 + \\sum_{i < j} (|S_{ij} - X_{ij}|^2 + |S_{ji} - X_{ij}|^2) $$\nMinimizing each term separately:\nFor the diagonal elements ($i=j$): The term $|S_{ii} - X_{ii}|^2$ is minimized when $X_{ii} = S_{ii}$.\nFor the off-diagonal elements ($i \\neq j$): Consider the pair of terms for $(i, j)$ and $(j, i)$ with $i < j$. Let $g(X_{ij}) = |S_{ij} - X_{ij}|^2 + |S_{ji} - X_{ij}|^2$. Let $X_{ij} = u + iv$, $S_{ij} = a+ib$, and $S_{ji} = c+id$.\n$$ g(u, v) = (a-u)^2 + (b-v)^2 + (c-u)^2 + (d-v)^2 $$\nTaking partial derivatives and setting to zero:\n$$ \\frac{\\partial g}{\\partial u} = -2(a-u) - 2(c-u) = 0 \\implies 4u = 2(a+c) \\implies u = \\frac{a+c}{2} $$\n$$ \\frac{\\partial g}{\\partial v} = -2(b-v) - 2(d-v) = 0 \\implies 4v = 2(b+d) \\implies v = \\frac{b+d}{2} $$\nThe optimal $X_{ij}$ is therefore $u+iv = \\frac{a+c}{2} + i\\frac{b+d}{2} = \\frac{1}{2}((a+ib) + (c+id)) = \\frac{1}{2}(S_{ij} + S_{ji})$.\nCombining the results for diagonal and off-diagonal elements, the optimal symmetric matrix $\\mathbf{X}$ has elements $X_{ij} = \\frac{1}{2}(S_{ij} + S_{ji})$. This is precisely the matrix $\\mathbf{R}(\\mathbf{S}) = \\frac{1}{2}(\\mathbf{S} + \\mathbf{S}^{\\top})$.\nThus, $\\mathbf{R}(\\mathbf{S})$ is the unique closest symmetric matrix to $\\mathbf{S}$ in the Frobenius norm, justifying its use as a minimal perturbation repair operator.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the reciprocity validation and repair problem for a series of test cases.\n    \"\"\"\n    \n    # Define the numerical tolerance for the feasibility check.\n    tau = 1e-8\n\n    # Define the test cases from the problem statement.\n    # Each test case is a list of S-matrices given as numpy arrays.\n    test_cases = [\n        # Test Case A (already reciprocal)\n        [\n            np.array([\n                [0.10, 0.02, 0.03],\n                [0.02, 0.20, 0.01],\n                [0.03, 0.01, 0.15]\n            ], dtype=complex),\n            np.array([\n                [0.08 + 0.02j, 0.01 - 0.005j, 0.00 + 0.01j],\n                [0.01 - 0.005j, 0.12 + 0.00j,  0.02 + 0.003j],\n                [0.00 + 0.01j,  0.02 + 0.003j, 0.09 - 0.01j]\n            ], dtype=complex)\n        ],\n        \n        # Test Case B (slightly asymmetric)\n        [\n            np.array([\n                [0.20, 0.05 + 0.001, 0.00 - 0.002],\n                [0.05 - 0.001, 0.15, 0.01 + 0.0005],\n                [0.00 + 0.002, 0.01 - 0.0005, 0.10]\n            ], dtype=complex),\n            np.array([\n                [0.18 + 0.01j, 0.02 + 0.00j + 0.001 + 0.0005j, 0.03 - 0.01j - 0.001 - 0.0003j],\n                [0.02 + 0.00j - 0.001 - 0.0005j, 0.16 - 0.005j, 0.04 + 0.002j + 0.0004 - 0.0002j],\n                [0.03 - 0.01j + 0.001 + 0.0003j, 0.04 + 0.002j - 0.0004 + 0.0002j, 0.11 + 0.00j]\n            ], dtype=complex)\n        ],\n        \n        # Test Case C (strongly asymmetric)\n        [\n            np.array([\n                [0.30 + 0.00j, 0.10 + 0.05j, -0.20 + 0.10j],\n                [0.05 - 0.10j, -0.10 + 0.00j, 0.25 + 0.00j],\n                [0.15 - 0.05j, 0.40 + 0.20j, 0.05 + 0.30j]\n            ], dtype=complex),\n            np.array([\n                [0.25 - 0.05j, -0.35 + 0.10j, 0.20 + 0.00j],\n                [0.20 - 0.15j, 0.30 + 0.05j, -0.10 + 0.25j],\n                [0.05 + 0.20j, 0.12 - 0.30j, -0.20 + 0.10j]\n            ], dtype=complex)\n        ],\n\n        # Test Case D (boundary case: zero scattering)\n        [\n            np.array([\n                [0, 0, 0],\n                [0, 0, 0],\n                [0, 0, 0]\n            ], dtype=complex),\n            np.array([\n                [0, 0, 0],\n                [0, 0, 0],\n                [0, 0, 0]\n            ], dtype=complex)\n        ],\n\n        # Test Case E (frequency-dependent with moderate asymmetry)\n        [\n            np.array([\n                [0.12, 0.03 + 0.005, 0.025 - 0.004],\n                [0.03 - 0.005, 0.18, 0.02 + 0.003],\n                [0.025 + 0.004, 0.02 - 0.003, 0.14]\n            ], dtype=complex),\n            np.array([\n                [0.10 + 0.02j, 0.04 - 0.01j - 0.01 + 0.005j, 0.00 + 0.03j + 0.006 - 0.004j],\n                [0.04 - 0.01j + 0.01 - 0.005j, 0.20 + 0.00j, 0.01 + 0.02j - 0.005 + 0.002j],\n                [0.00 + 0.03j - 0.006 + 0.004j, 0.01 + 0.02j + 0.005 - 0.002j, 0.13 - 0.01j]\n            ], dtype=complex)\n        ]\n    ]\n\n    all_results = []\n    \n    for case_matrices in test_cases:\n        violations_before = []\n        violations_after = []\n        perturbations = []\n\n        for S in case_matrices:\n            # Calculate violation before repair\n            diff_before = S - S.T\n            v_before_single = np.linalg.norm(diff_before, 'fro')\n            violations_before.append(v_before_single)\n\n            # Apply the repair operator\n            S_repaired = 0.5 * (S + S.T)\n\n            # Calculate violation after repair\n            # This should be zero or very close to it due to floating-point arithmetic\n            diff_after = S_repaired - S_repaired.T\n            v_after_single = np.linalg.norm(diff_after, 'fro')\n            violations_after.append(v_after_single)\n\n            # Calculate the perturbation introduced by the repair\n            perturbation_matrix = S_repaired - S\n            d_single = np.linalg.norm(perturbation_matrix, 'fro')\n            perturbations.append(d_single)\n\n        # Compute the maximums over the frequencies for the current test case\n        v_before = max(violations_before)\n        v_after = max(violations_after)\n        d_max = max(perturbations)\n        \n        # Determine feasibility after repair\n        b_feasible = v_after <= tau\n        \n        all_results.append([v_before, v_after, b_feasible, d_max])\n\n    # Format the final output string to be a list of lists with no spaces\n    # Example: [[v1,v2,True,v3],[v4,v5,False,v6]]\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The practical application of evolutionary algorithms in computational electromagnetics is often limited by the prohibitive cost of high-fidelity simulations. This capstone practice  introduces a powerful strategy to overcome this bottleneck: multifidelity optimization. You will construct and combine a fast, low-fidelity model with a slow, high-fidelity one, using a learned surrogate model to bridge the gap and intelligently guide the search, ultimately achieving superior designs within a fixed computational budget.",
            "id": "3306133",
            "problem": "You are tasked with designing a multifidelity evolutionary optimization algorithm for reflectarray phase tuning across angle and frequency in computational electromagnetics, combining a low-fidelity Method of Moments (MoM) model and a high-fidelity Finite Element Method (FEM) model via a learned transfer mapping. The design variable is a phase vector assigned to a one-dimensional reflectarray. The goal is to maximize the mean normalized mainlobe power at a specified target angle over a set of frequencies, subject to realistic electromagnetic modeling assumptions. Your program must implement both the low-fidelity and high-fidelity models, learn the transfer mapping, and compare the performance of a multifidelity evolutionary algorithm to a purely high-fidelity baseline algorithm under the same expensive evaluation budget.\n\nFundamental base, definitions, and modeling assumptions:\n1. Start from the time-harmonic Maxwell equations in source-free homogeneous space, with angular frequency $\\omega = 2 \\pi f$, permittivity $\\epsilon$, permeability $\\mu$, and speed of light $c = 1/\\sqrt{\\epsilon \\mu}$. The wave number is $k(f) = \\omega \\sqrt{\\epsilon \\mu} = 2 \\pi f / c$. The reflectarray elements are modeled as re-radiating with controllable phase shifts, and the far-field pattern in the principal plane can be described by the array factor under standard approximations when element sizes are small compared to the wavelength.\n2. A one-dimensional reflectarray of $M$ elements has positions $x_i$ uniformly spaced by $s$ meters and centered so that $\\sum_i x_i = 0$. The beamforming objective at angle $\\theta$ and frequency $f$ is driven by the constructive superposition of contributions $\\exp(j \\phi_i(\\theta,f))$, where $j$ is the imaginary unit and $\\phi_i(\\theta,f)$ denotes the total phase of the $i$-th element contribution.\n3. Low-fidelity model based on the Method of Moments (MoM): Neglect mutual coupling and element dispersion. For a phase vector $\\mathbf{p} = (p_0,\\dots,p_{M-1})$ with $p_i \\in [0,2\\pi)$, and frequency set $\\mathcal{F}$, the low-fidelity array factor magnitude-squared at angle $\\theta$ and frequency $f$ is\n$$\nP_L(\\mathbf{p}; \\theta, f) = \\frac{1}{M^2} \\left| \\sum_{i=0}^{M-1} \\exp\\left( j \\left( k(f) x_i \\sin \\theta + p_i \\right) \\right) \\right|^2.\n$$\nDefine the low-fidelity performance averaged over frequencies as\n$$\n\\bar{P}_L(\\mathbf{p}; \\theta, \\mathcal{F}) = \\frac{1}{|\\mathcal{F}|} \\sum_{f \\in \\mathcal{F}} P_L(\\mathbf{p}; \\theta, f).\n$$\n4. High-fidelity model based on the Finite Element Method (FEM): Incorporate simple, deterministic parametric corrections for mutual coupling and frequency-dependent element amplitude dispersion. Let $f_0$ be a reference (center) frequency. For each element $i$, define an amplitude factor\n$$\nt_i(f) = 1 + \\gamma \\left( \\frac{f}{f_0} - 1 \\right) \\cos\\left( \\frac{2 \\pi i}{M} \\right),\n$$\nand a phase correction\n$$\n\\Delta_i(\\mathbf{p}; \\theta, f) = \\alpha \\sum_{j \\in \\mathcal{N}(i)} \\sin(p_i - p_j) + \\beta \\left( \\frac{f}{f_0} - 1 \\right) \\sin \\theta \\cdot \\frac{i}{M-1},\n$$\nwhere $\\mathcal{N}(i)$ denotes nearest neighbors (indices $i-1$ and $i+1$ when they exist). The high-fidelity magnitude-squared is defined by\n$$\nP_H(\\mathbf{p}; \\theta, f) = \\frac{1}{M^2} \\left| \\sum_{i=0}^{M-1} t_i(f) \\exp\\left( j \\left( k(f) x_i \\sin \\theta + p_i + \\Delta_i(\\mathbf{p}; \\theta, f) \\right) \\right) \\right|^2,\n$$\nand the high-fidelity performance averaged over frequencies is\n$$\n\\bar{P}_H(\\mathbf{p}; \\theta, \\mathcal{F}) = \\frac{1}{|\\mathcal{F}|} \\sum_{f \\in \\mathcal{F}} P_H(\\mathbf{p}; \\theta, f).\n$$\nThese choices represent a scientifically plausible parametric correction consistent with the qualitative effects of mutual coupling and dispersion while remaining computationally tractable.\n\nLearned transfer mapping:\nDefine the residual (model discrepancy) for a design $\\mathbf{p}$ as\n$$\nr(\\mathbf{p}) = \\bar{P}_H(\\mathbf{p}; \\theta, \\mathcal{F}) - \\bar{P}_L(\\mathbf{p}; \\theta, \\mathcal{F}).\n$$\nConstruct a feature vector $\\boldsymbol{\\phi}(\\mathbf{p})$ for learning a transfer mapping via linear regression:\n- Bias term: $1$.\n- Low-fidelity performance: $\\bar{P}_L(\\mathbf{p}; \\theta, \\mathcal{F})$.\n- Mean nearest-neighbor cosine: $\\frac{1}{M-1} \\sum_{i=0}^{M-2} \\cos(p_i - p_{i+1})$.\n- Phase variance: $\\frac{1}{M} \\sum_{i=0}^{M-1} (p_i - \\bar{p})^2$ with $\\bar{p} = \\frac{1}{M} \\sum_i p_i$.\n- First-harmonic coherence: $\\frac{1}{M} \\left| \\sum_{i=0}^{M-1} \\exp(j p_i) \\right|$.\nUse least squares to fit weights $\\mathbf{w}$ such that\n$$\n\\Delta(\\mathbf{p}) \\approx \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{p}) \\approx r(\\mathbf{p}),\n$$\nand define a surrogate as\n$$\n\\bar{P}_S(\\mathbf{p}; \\theta, \\mathcal{F}) = \\bar{P}_L(\\mathbf{p}; \\theta, \\mathcal{F}) + \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{p}).\n$$\n\nOptimization objective:\nMaximize $\\bar{P}(\\mathbf{p})$ over $\\mathbf{p}$, where $\\bar{P}$ denotes either the high-fidelity performance $\\bar{P}_H$ (for evaluation) or the surrogate $\\bar{P}_S$ (for cheap guidance). Phases must satisfy $p_i \\in [0,2\\pi)$ and are wrapped modulo $2\\pi$ after mutation.\n\nAlgorithms to implement:\n1. Baseline algorithm (high-fidelity only): A simple expensive local evolutionary search that starts from a physically motivated initial guess and iteratively mutates the best design using Gaussian perturbations, accepting improvements according to $\\bar{P}_H$ and consuming the expensive evaluation budget.\n2. Multifidelity algorithm (MoM + FEM + learned transfer): Use cheap evaluations of $\\bar{P}_L$ and the learned transfer $\\Delta(\\cdot)$ to guide exploration. Periodically evaluate the top surrogate-ranked candidates with $\\bar{P}_H$, update the transfer mapping via least squares on accumulating data, and consume the same expensive evaluation budget as the baseline.\n\nFundamental initialization:\nDefine the center frequency $f_0$ and the physically motivated initial phase vector as\n$$\np_i^{(0)} = \\mathrm{wrap}\\left( - k(f_0) x_i \\sin \\theta \\right)\n$$\nto approximately align contributions at $(\\theta, f_0)$, where $\\mathrm{wrap}(\\cdot)$ maps phases to $[0,2\\pi)$.\n\nUnits:\n- Angle $\\theta$ must be specified in radians.\n- Frequency $f$ must be specified in Hertz (Hz).\n- Spacing $s$ must be specified in meters (m).\n- Speed of light $c$ must be specified in meters per second (m/s).\n- The objective $\\bar{P}$ is dimensionless and lies in $[0,1]$ under the given normalization.\n\nTest suite:\nImplement the algorithms for the following three test cases. In each case, report the improvement as a float:\n$$\n\\Delta_{\\mathrm{imp}} = \\left( \\max_{\\text{budget}} \\bar{P}_H \\text{ under multifidelity} \\right) - \\left( \\max_{\\text{budget}} \\bar{P}_H \\text{ under baseline} \\right).\n$$\nThe budget refers to the maximum number of high-fidelity (FEM) evaluations; cheap low-fidelity (MoM) evaluations are allowed to be more numerous.\n\n- Case 1 (happy path):\n  - $M = 16$, $s = 0.015$ m, $\\theta = 0.523599$ rad.\n  - $\\mathcal{F} = \\{9.5 \\times 10^9, 1.0 \\times 10^{10}, 1.05 \\times 10^{10}\\}$ Hz, $f_0 = 1.0 \\times 10^{10}$ Hz.\n  - High-fidelity parameters: $\\alpha = 0.08$, $\\beta = 0.15$, $\\gamma = 0.10$.\n  - Budget: maximum $60$ high-fidelity evaluations.\n\n- Case 2 (boundary with small array and near-broadside):\n  - $M = 4$, $s = 0.030$ m, $\\theta = 0.0$ rad.\n  - $\\mathcal{F} = \\{9.8 \\times 10^9, 1.0 \\times 10^{10}, 1.02 \\times 10^{10}\\}$ Hz, $f_0 = 1.0 \\times 10^{10}$ Hz.\n  - High-fidelity parameters: $\\alpha = 0.02$, $\\beta = 0.05$, $\\gamma = 0.05$.\n  - Budget: maximum $20$ high-fidelity evaluations.\n\n- Case 3 (challenging with stronger coupling and wider bandwidth):\n  - $M = 32$, $s = 0.015$ m, $\\theta = 0.785398$ rad.\n  - $\\mathcal{F} = \\{9.0 \\times 10^9, 1.0 \\times 10^{10}, 1.10 \\times 10^{10}\\}$ Hz, $f_0 = 1.0 \\times 10^{10}$ Hz.\n  - High-fidelity parameters: $\\alpha = 0.15$, $\\beta = 0.25$, $\\gamma = 0.15$.\n  - Budget: maximum $80$ high-fidelity evaluations.\n\nAlgorithmic constraints:\n- Use a fixed random seed per test case for reproducibility.\n- The baseline and multifidelity algorithms must consume no more than their respective high-fidelity evaluation budget for each case.\n- The transfer mapping must be learned by least squares on an initial set of paired evaluations, updated as new pairs are collected.\n- Phases must be wrapped modulo $2\\pi$ after every mutation.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the three test cases in order, with each improvement formatted as a decimal float (not a percentage), for example, \"[0.012345,-0.000123,0.054321]\". No other text should be printed.",
            "solution": "The user has provided a problem that requires the design, implementation, and comparison of two evolutionary optimization algorithms for a computational electromagnetics application: reflectarray phase tuning. The problem is to be validated for scientific soundness, completeness, and clarity before a solution is attempted.\n\n### **Step 1: Problem Validation**\n\n#### **Extraction of Givens**\n\n- **Physical Model**: The problem is based on time-harmonic Maxwell's equations. Key entities include angular frequency $\\omega = 2 \\pi f$, speed of light $c = 1/\\sqrt{\\epsilon \\mu}$, and wavenumber $k(f) = 2 \\pi f / c$. The system is a 1D reflectarray of $M$ elements at positions $x_i$ with uniform spacing $s$ and $\\sum_i x_i = 0$.\n- **Low-Fidelity Model ($\\bar{P}_L$)**: This model neglects mutual coupling and element dispersion. Its performance is given by the frequency-averaged array factor:\n$$\n\\bar{P}_L(\\mathbf{p}; \\theta, \\mathcal{F}) = \\frac{1}{|\\mathcal{F}|} \\sum_{f \\in \\mathcal{F}} \\frac{1}{M^2} \\left| \\sum_{i=0}^{M-1} \\exp\\left( j \\left( k(f) x_i \\sin \\theta + p_i \\right) \\right) \\right|^2\n$$\n- **High-Fidelity Model ($\\bar{P}_H$)**: This model includes parametric corrections for mutual coupling and dispersion. The frequency-averaged performance is:\n$$\n\\bar{P}_H(\\mathbf{p}; \\theta, \\mathcal{F}) = \\frac{1}{|\\mathcal{F}|} \\sum_{f \\in \\mathcal{F}} \\frac{1}{M^2} \\left| \\sum_{i=0}^{M-1} t_i(f) \\exp\\left( j \\left( k(f) x_i \\sin \\theta + p_i + \\Delta_i(\\mathbf{p}; \\theta, f) \\right) \\right) \\right|^2\n$$\nwhere $t_i(f)$ is an amplitude factor and $\\Delta_i$ is a phase correction term defined as:\n$$\nt_i(f) = 1 + \\gamma \\left( \\frac{f}{f_0} - 1 \\right) \\cos\\left( \\frac{2 \\pi i}{M} \\right)\n$$\n$$\n\\Delta_i(\\mathbf{p}; \\theta, f) = \\alpha \\sum_{j \\in \\mathcal{N}(i)} \\sin(p_i - p_j) + \\beta \\left( \\frac{f}{f_0} - 1 \\right) \\sin \\theta \\cdot \\frac{i}{M-1}\n$$\n- **Learned Transfer Mapping**: A surrogate model $\\bar{P}_S$ is defined to approximate $\\bar{P}_H$ by correcting $\\bar{P}_L$ with a learned residual $\\Delta(\\mathbf{p})$.\n$$\n\\bar{P}_S(\\mathbf{p}; \\theta, \\mathcal{F}) = \\bar{P}_L(\\mathbf{p}; \\theta, \\mathcal{F}) + \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{p})\n$$\nThe weights $\\mathbf{w}$ are found via least squares regression on the residual $r(\\mathbf{p}) = \\bar{P}_H(\\mathbf{p}) - \\bar{P}_L(\\mathbf{p})$. The feature vector $\\boldsymbol{\\phi}(\\mathbf{p})$ consists of a bias term, $\\bar{P}_L$, mean nearest-neighbor phase cosine, phase variance, and first-harmonic coherence of the phase vector $\\mathbf{p}$.\n- **Optimization**: The objective is to maximize the high-fidelity performance. The design variables are the phases $p_i \\in [0, 2\\pi)$.\n- **Algorithms**: A baseline evolutionary search using only the high-fidelity model is to be compared against a multifidelity algorithm that leverages the low-fidelity and surrogate models to guide the search. Both operate under a fixed budget of high-fidelity evaluations.\n- **Initialization**: A physically motivated initial phase vector is defined as $p_i^{(0)} = \\mathrm{wrap}\\left( -k(f_0) x_i \\sin \\theta \\right)$.\n- **Test Suite**: Three specific test cases are provided with all necessary parameters ($M, s, \\theta, \\mathcal{F}, f_0, \\alpha, \\beta, \\gamma$) and evaluation budgets.\n- **Output**: The required output is the performance improvement $\\Delta_{\\mathrm{imp}}$ of the multifidelity algorithm over the baseline for each test case.\n\n#### **Validation Verdict**\n\n- **Scientifically Grounded**: The problem is valid. The formulation is well-grounded in the principles of antenna theory and computational electromagnetics. The low-fidelity model represents the ideal array factor, a standard analytical tool. The high-fidelity model introduces scientifically plausible parametric corrections that qualitatively model real-world effects like mutual coupling and dispersion in a computationally tractable manner.\n- **Well-Posed and Objective**: The problem is well-posed. The objective functions, constraints, and performance metrics are defined with mathematical precision. The task, which is to implement and compare two specified algorithmic strategies under a fixed computational budget and random seed, is a well-defined computational experiment.\n- **Completeness and Consistency**: The problem is self-contained and provides all necessary definitions, formulas, and parameters for each test case. The only missing constant is the speed of light, $c$, which is a universal physical constant, and its standard value can be assumed. The problem statement is internally consistent.\n\nThe problem satisfies all criteria for validity. It is a rigorous and challenging task in a specialized area of computational science and engineering.\n\n### **Step 2: Solution Design**\n\nThe solution requires implementing the physical models, the learning framework, and the two optimization algorithms as specified.\n\n#### **Modeling and Framework**\n\n1.  **Physical Models**: Functions will be implemented for both $\\bar{P}_L(\\mathbf{p})$ and $\\bar{P}_H(\\mathbf{p})$. These functions will take a phase vector $\\mathbf{p}$ and the case-specific physical parameters as input. The calculations will involve complex number arithmetic, as is standard in phasor-based analysis. The position vector $x_i$ for the centered array is calculated as $x_i = s \\cdot (i - (M-1)/2)$ for $i=0, \\dots, M-1$.\n2.  **Surrogate Framework**: A set of functions will manage the surrogate model.\n    - `calculate_features`: Computes the 5-dimensional feature vector $\\boldsymbol{\\phi}(\\mathbf{p})$ from a given phase vector $\\mathbf{p}$ and its corresponding $\\bar{P}_L$ value.\n    - `update_transfer_mapping`: Takes a dataset of high-fidelity evaluations `(p, p_l, p_h)` and uses `numpy.linalg.lstsq` to compute the optimal weight vector $\\mathbf{w}$ for the linear surrogate model.\n    - `surrogate_model`: Predicts the high-fidelity performance for a new phase vector $\\mathbf{p}$ using the current weights $\\mathbf{w}$ via $\\bar{P}_S(\\mathbf{p}) = \\bar{P}_L(\\mathbf{p}) + \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{p})$.\n\n#### **Optimization Algorithms**\n\nA fixed random seed will be used for each test case to ensure reproducibility of the stochastic algorithms.\n\n1.  **Baseline Algorithm**: This is a simple evolutionary search.\n    - It starts with the physically motivated initial phase vector $p^{(0)}$.\n    - It maintains the best solution found so far, $\\mathbf{p}_{\\text{best}}$.\n    - In each step, it generates a new candidate by applying a Gaussian mutation to $\\mathbf{p}_{\\text{best}}$: $\\mathbf{p}_{\\text{mut}} = \\mathrm{wrap}(\\mathbf{p}_{\\text{best}} + \\mathcal{N}(0, \\sigma^2))$.\n    - The new candidate is evaluated using the expensive high-fidelity model $\\bar{P}_H$.\n    - If the new candidate is an improvement, it replaces $\\mathbf{p}_{\\text{best}}$.\n    - This process repeats until the budget of high-fidelity evaluations is exhausted. The final result is the maximum $\\bar{P}_H$ value observed during the entire search.\n\n2.  **Multifidelity Algorithm**: This algorithm intelligently allocates the expensive evaluation budget.\n    - **Initialization**: It begins by performing a small number of high-fidelity evaluations ($N_{\\text{initial}}$) on random or structured initial points to build an initial surrogate model.\n    - **Iterative Refinement**: The main loop consists of cycles that repeat until the budget is spent:\n        a. **Cheap Search**: An inner loop generates a pool of candidate solutions (`N_cheap_per_cycle`) by mutating the current best-known design. These candidates are evaluated using the fast surrogate model $\\bar{P}_S$.\n        b. **Infill Selection**: The most promising candidate from the pool (the one with the highest predicted $\\bar{P}_S$) is selected for expensive evaluation.\n        c. **Expensive Evaluation**: The selected candidate is evaluated using the true high-fidelity model $\\bar{P}_H$.\n        d. **Model Update**: The new data point $(\\mathbf{p}, \\bar{P}_L, \\bar{P}_H)$ is added to the training set, and the surrogate model weights $\\mathbf{w}$ are re-computed.\n    - The final result is the maximum $\\bar{P}_H$ value observed across all high-fidelity evaluations.\n\nFinally, for each test case, the performance improvement is calculated as $\\Delta_{\\mathrm{imp}} = (\\max \\bar{P}_{H, \\text{multi}}) - (\\max \\bar{P}_{H, \\text{base}})$ and reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ... # No scipy needed for this implementation\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization problems and print the final result.\n    \"\"\"\n    C_LIGHT = 299792458.0  # Speed of light in m/s\n\n    # -------------------------------------------------------------------------\n    # Helper and Model Functions\n    # -------------------------------------------------------------------------\n\n    def get_element_positions(M, s):\n        \"\"\"Computes positions for a centered 1D array.\"\"\"\n        return s * (np.arange(M) - (M - 1) / 2.0)\n\n    def wrap_phases(phases):\n        \"\"\"Wraps phases to the interval [0, 2*pi).\"\"\"\n        return np.mod(phases, 2 * np.pi)\n\n    def low_fidelity_model(p, M, x, theta, freqs, c):\n        \"\"\"Computes the frequency-averaged low-fidelity performance.\"\"\"\n        p_l_vals = []\n        for f in freqs:\n            k = 2 * np.pi * f / c\n            total_phases = k * x * np.sin(theta) + p\n            complex_sum = np.sum(np.exp(1j * total_phases))\n            p_l_at_f = (1.0 / M**2) * np.abs(complex_sum)**2\n            p_l_vals.append(p_l_at_f)\n        return np.mean(p_l_vals)\n\n    def high_fidelity_model(p, M, x, theta, freqs, f0, alpha, beta, gamma, c):\n        \"\"\"Computes the frequency-averaged high-fidelity performance.\"\"\"\n        p_h_vals = []\n        for f in freqs:\n            k = 2 * np.pi * f / c\n            f_ratio_term = (f / f0) - 1.0\n            complex_sum = 0.0\n            for i in range(M):\n                t_i = 1.0 + gamma * f_ratio_term * np.cos(2 * np.pi * i / M)\n                \n                coupling_term = 0.0\n                if M > 1:\n                    if i > 0:\n                        coupling_term += np.sin(p[i] - p[i-1])\n                    if i < M - 1:\n                        coupling_term += np.sin(p[i] - p[i+1])\n                coupling_term *= alpha\n                \n                dispersion_term = beta * f_ratio_term * np.sin(theta) * (i / (M - 1.0)) if M > 1 else 0.0\n                delta_i = coupling_term + dispersion_term\n                \n                total_phase = k * x[i] * np.sin(theta) + p[i] + delta_i\n                complex_sum += t_i * np.exp(1j * total_phase)\n            \n            p_h_at_f = (1.0 / M**2) * np.abs(complex_sum)**2\n            p_h_vals.append(p_h_at_f)\n        return np.mean(p_h_vals)\n\n    def calculate_features(p, p_l_val, M):\n        \"\"\"Computes the feature vector for the surrogate model.\"\"\"\n        features = np.zeros(5)\n        features[0] = 1.0  # Bias\n        features[1] = p_l_val  # Low-fidelity performance\n        if M > 1:\n            features[2] = np.mean(np.cos(p[:-1] - p[1:]))  # Mean nearest-neighbor cosine\n        else: # Case not in tests, but handle defensively.\n            features[2] = 1.0\n        features[3] = np.var(p)  # Phase variance\n        features[4] = np.abs(np.mean(np.exp(1j * p)))  # First-harmonic coherence\n        return features\n\n    def update_transfer_mapping(hf_data, M):\n        \"\"\"Learns the transfer mapping weights via least squares.\"\"\"\n        if not hf_data:\n            return np.zeros(5)\n        \n        num_points = len(hf_data)\n        Phi = np.zeros((num_points, 5))\n        r = np.zeros(num_points)\n        \n        for i, (p_vec, p_l, p_h) in enumerate(hf_data):\n            Phi[i, :] = calculate_features(p_vec, p_l, M)\n            r[i] = p_h - p_l\n        \n        weights, _, _, _ = np.linalg.lstsq(Phi, r, rcond=None)\n        return weights\n\n    def surrogate_model(p, p_l_val, w, M):\n        \"\"\"Predicts high-fidelity performance using the surrogate.\"\"\"\n        features = calculate_features(p, p_l_val, M)\n        correction = np.dot(w, features)\n        return p_l_val + correction\n\n    # -------------------------------------------------------------------------\n    # Optimization Algorithms\n    # -------------------------------------------------------------------------\n\n    def run_baseline_ea(case_params, seed, c):\n        \"\"\"Runs the high-fidelity-only baseline evolutionary algorithm.\"\"\"\n        M, s, theta, freqs, f0, alpha, beta, gamma, budget = (\n            case_params['M'], case_params['s'], case_params['theta'], case_params['freqs'],\n            case_params['f0'], case_params['alpha'], case_params['beta'], case_params['gamma'],\n            case_params['budget'])\n        \n        rng = np.random.default_rng(seed)\n        x = get_element_positions(M, s)\n        mutation_std = 0.2\n\n        k0 = 2 * np.pi * f0 / c\n        p_initial = wrap_phases(-k0 * x * np.sin(theta))\n        \n        p_h_initial = high_fidelity_model(p_initial, M, x, theta, freqs, f0, alpha, beta, gamma, c)\n        eval_count = 1\n        \n        best_p = p_initial\n        best_p_h = p_h_initial\n        all_p_h_values = [p_h_initial]\n\n        while eval_count < budget:\n            p_mutant = wrap_phases(best_p + rng.normal(loc=0.0, scale=mutation_std, size=M))\n            \n            p_h_mutant = high_fidelity_model(p_mutant, M, x, theta, freqs, f0, alpha, beta, gamma, c)\n            eval_count += 1\n            all_p_h_values.append(p_h_mutant)\n            \n            if p_h_mutant > best_p_h:\n                best_p_h = p_h_mutant\n                best_p = p_mutant\n                \n        return np.max(all_p_h_values)\n\n    def run_multifidelity_ea(case_params, seed, c):\n        \"\"\"Runs the multifidelity evolutionary algorithm.\"\"\"\n        M, s, theta, freqs, f0, alpha, beta, gamma, budget = (\n            case_params['M'], case_params['s'], case_params['theta'], case_params['freqs'],\n            case_params['f0'], case_params['alpha'], case_params['beta'], case_params['gamma'],\n            case_params['budget'])\n\n        rng = np.random.default_rng(seed)\n        x = get_element_positions(M, s)\n        mutation_std = 0.2\n        N_initial = 5\n        N_cheap_per_cycle = 50\n\n        hf_data = []  # List of (p, p_l, p_h) tuples\n        eval_count = 0\n        \n        k0 = 2 * np.pi * f0 / c\n        p_initial = wrap_phases(-k0 * x * np.sin(theta))\n        initial_designs = [p_initial] + [wrap_phases(rng.uniform(0, 2 * np.pi, M)) for _ in range(N_initial - 1)]\n\n        for p in initial_designs:\n            if eval_count >= budget: break\n            p_l = low_fidelity_model(p, M, x, theta, freqs, c)\n            p_h = high_fidelity_model(p, M, x, theta, freqs, f0, alpha, beta, gamma, c)\n            eval_count += 1\n            hf_data.append((p, p_l, p_h))\n\n        weights = update_transfer_mapping(hf_data, M)\n\n        while eval_count < budget:\n            best_hf_point = max(hf_data, key=lambda item: item[2])\n            p_base_for_mutation = best_hf_point[0]\n            \n            cheap_candidates = []\n            for _ in range(N_cheap_per_cycle):\n                p_mutant = wrap_phases(p_base_for_mutation + rng.normal(0.0, scale=mutation_std, size=M))\n                p_l_mutant = low_fidelity_model(p_mutant, M, x, theta, freqs, c)\n                p_s_mutant = surrogate_model(p_mutant, p_l_mutant, weights, M)\n                cheap_candidates.append((p_mutant, p_s_mutant))\n                \n            best_cheap_p, _ = max(cheap_candidates, key=lambda item: item[1])\n            \n            p_l_new = low_fidelity_model(best_cheap_p, M, x, theta, freqs, c)\n            p_h_new = high_fidelity_model(best_cheap_p, M, x, theta, freqs, f0, alpha, beta, gamma, c)\n            eval_count += 1\n\n            hf_data.append((best_cheap_p, p_l_new, p_h_new))\n            weights = update_transfer_mapping(hf_data, M)\n            \n        return np.max([item[2] for item in hf_data])\n\n    # -------------------------------------------------------------------------\n    # Main Execution\n    # -------------------------------------------------------------------------\n\n    test_cases = [\n        # Case 1\n        {\n            \"M\": 16, \"s\": 0.015, \"theta\": 0.523599,\n            \"freqs\": np.array([9.5e9, 1.0e10, 1.05e10]), \"f0\": 1.0e10,\n            \"alpha\": 0.08, \"beta\": 0.15, \"gamma\": 0.10, \"budget\": 60\n        },\n        # Case 2\n        {\n            \"M\": 4, \"s\": 0.030, \"theta\": 0.0,\n            \"freqs\": np.array([9.8e9, 1.0e10, 1.02e10]), \"f0\": 1.0e10,\n            \"alpha\": 0.02, \"beta\": 0.05, \"gamma\": 0.05, \"budget\": 20\n        },\n        # Case 3\n        {\n            \"M\": 32, \"s\": 0.015, \"theta\": 0.785398,\n            \"freqs\": np.array([9.0e9, 1.0e10, 1.10e10]), \"f0\": 1.0e10,\n            \"alpha\": 0.15, \"beta\": 0.25, \"gamma\": 0.15, \"budget\": 80\n        }\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        seed = i\n        \n        max_ph_baseline = run_baseline_ea(case, seed, C_LIGHT)\n        max_ph_multi = run_multifidelity_ea(case, seed, C_LIGHT)\n        \n        improvement = max_ph_multi - max_ph_baseline\n        results.append(improvement)\n\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}