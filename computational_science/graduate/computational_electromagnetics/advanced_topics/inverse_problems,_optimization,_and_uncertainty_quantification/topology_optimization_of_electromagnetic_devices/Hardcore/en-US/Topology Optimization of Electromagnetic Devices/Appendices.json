{
    "hands_on_practices": [
        {
            "introduction": "Gradient-based topology optimization hinges on the accurate calculation of sensitivities, which dictate the direction of design improvement. This first exercise establishes a critical validation workflow by deriving adjoint sensitivities for a frequency-domain electromagnetic model from first principles. You will then implement the complex-step method—a powerful numerical technique that provides machine-precision derivative estimates—to verify the correctness of your adjoint-based gradient implementation . Mastering this practice is essential for building reliable and effective optimization tools.",
            "id": "3356381",
            "problem": "You are tasked with designing and implementing a complex-step gradient check for the sensitivity of a scalar objective with respect to a spatially distributed material density in a frequency-domain electromagnetic model used for topology optimization. The problem must be framed within computational electromagnetics and must start from first principles.\n\nConsider two-dimensional transverse electric (TE) polarization under harmonic time dependence. Starting from Maxwell’s equations and their frequency-domain form, the scalar TE field satisfies a Helmholtz-type equation. In normalized units (and assuming uniform magnetic permeability), the strong form of the governing equation for the out-of-plane field is\n$$\n\\nabla^2 u(\\mathbf{x}) + \\omega^2 \\mu \\epsilon(\\rho(\\mathbf{x}))\\, u(\\mathbf{x}) = s(\\mathbf{x}),\n$$\nwhere $u$ is the out-of-plane electric field component, $\\omega$ is the angular frequency, $\\mu$ is the magnetic permeability (constant), $\\epsilon(\\rho)$ is the spatially varying permittivity as a function of the material density variable $\\rho(\\mathbf{x}) \\in [0,1]$, and $s$ is a prescribed source distribution. Assume homogeneous Dirichlet boundary conditions on a square domain to guarantee well-posedness.\n\nDiscretize the governing equation using Finite Difference Frequency Domain (FDFD), yielding a linear system\n$$\nA(\\rho)\\, u = b,\n$$\nwhere $A(\\rho) = K + \\omega^2 \\mu\\, M(\\rho)$, $K$ is the discrete Laplacian with homogeneous Dirichlet boundary conditions over an $N \\times N$ interior grid, and $M(\\rho)$ is a diagonal matrix with entries $\\epsilon(\\rho_i)$ at the corresponding grid points $i$. Adopt a physically plausible, monotonically increasing material interpolation for the permittivity\n$$\n\\epsilon(\\rho) = \\epsilon_{\\min} + (\\epsilon_{\\max} - \\epsilon_{\\min})\\, \\rho,\n$$\nwith real and positive $\\epsilon_{\\min}$ and $\\epsilon_{\\max}$.\n\nDefine a complex-valued objective\n$$\nf(u) = c^\\top u,\n$$\nwhere $c$ is a fixed real weighting vector that selects a linear functional of the field (for example, the field amplitude at a target location or a weighted sum over a region). This objective is suitable for adjoint sensitivity analysis and complex-step validation because it is linear in $u$ and holomorphic in the parameters through $u(\\rho)$.\n\nYour tasks are:\n- Derive, from the discrete governing equation and the definition of the objective, an adjoint-based expression for the sensitivity $df/d\\rho$, starting only from the first principles above. Your derivation must explicitly apply the chain rule, the linearization of the state equation, and the introduction of an adjoint field defined via an appropriate transpose system. You must ensure scientific realism by clearly stating any assumptions (e.g., constant $\\mu$, real $\\epsilon(\\rho)$).\n- Design and implement a complex-step gradient check. That is, for a chosen direction vector $d$ in the design space, perturb the design variable as $\\rho \\mapsto \\rho + i\\, \\Delta\\, d$ with a small real increment $\\Delta$, solve the perturbed complex system, evaluate $f$, and compute the directional derivative estimate\n$$\n\\frac{d f}{d \\rho}\\cdot d \\approx \\frac{\\operatorname{Im}\\left[f\\left(\\rho + i\\, \\Delta\\, d\\right)\\right]}{\\Delta}.\n$$\nExplain why this procedure is valid when $\\epsilon(\\rho)$ is real and the mapping from $\\rho$ to $A(\\rho)$ is analytic, and how it avoids subtractive cancellation.\n\nYour program must:\n- Construct $K$ for the $N \\times N$ interior grid using the standard five-point stencil under homogeneous Dirichlet boundary conditions, and assemble $A(\\rho) = K + \\omega^2 \\mu\\, \\operatorname{diag}(\\epsilon(\\rho))$.\n- Form $b$ as a single-point source located near the left side of the interior grid and $c$ as a selector near the right side (precise indices specified below).\n- Compute the adjoint sensitivity vector $g = df/d\\rho$ and the complex-step directional derivative for specified directions $d$ and increments $\\Delta$.\n- Report, for each test case, the relative error between the adjoint directional derivative $g \\cdot d$ and the complex-step estimate, defined as\n$$\n\\mathrm{err} = \\frac{\\left|\\, (g \\cdot d) - \\frac{\\operatorname{Im}(f(\\rho + i\\, \\Delta\\, d))}{\\Delta} \\,\\right|}{\\max\\left(\\left|g \\cdot d\\right|, 10^{-12}\\right)}.\n$$\n\nUse the following fixed parameters and test suite:\n- Grid size: $N = 12$ interior points per dimension. Grid spacing is normalized to $1$.\n- Angular frequency: $\\omega = 10$ (dimensionless normalized units).\n- Magnetic permeability: $\\mu = 1$ (normalized units).\n- Permittivity bounds: $\\epsilon_{\\min} = 1$, $\\epsilon_{\\max} = 12$.\n- Source vector $b$: a unit amplitude at the grid index $(i,j) = (N//2, 1)$ mapped to a single index using row-major ordering.\n- Objective selector $c$: a unit selector at the grid index $(i,j) = (N//2, N-2)$ in the same ordering.\n- Design variable $\\rho \\in \\mathbb{R}^{N^2}$ with entries in $[0,1]$.\n\nTest cases:\n1. Happy path: random $\\rho$ drawn uniformly from $[0,1]$ with seed $42$, direction $d$ a random vector normalized to unit Euclidean norm (same seed), and complex-step increment $\\Delta = 10^{-20}$.\n2. Step-size robustness: same as case 1, but with $\\Delta = 10^{-10}$.\n3. Boundary design extreme: uniform design $\\rho = \\mathbf{0}$, direction $d$ a standard basis vector selecting the center grid index $(i,j) = (N//2, N//2)$, and $\\Delta = 10^{-20}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is a float equal to the relative error defined above for the corresponding test case. No physical unit is required in the output since the quantities are dimensionless numbers.",
            "solution": "The problem requires the derivation and validation of an adjoint sensitivity analysis for a frequency-domain electromagnetic model, using the complex-step method as a high-precision benchmark. The derivation and implementation will proceed from first principles as laid out in the problem statement.\n\n### 1. Adjoint Sensitivity Analysis Derivation\n\nThe analysis begins with the discrete system of equations governing the electric field $u$ and the scalar objective function $f$.\n\n**Given:**\n- State Equation: $A(\\rho)u = b$, where $A(\\rho) = K + \\omega^2 \\mu M(\\rho)$\n- Objective Function: $f(u) = c^\\top u$\n\nHere, $\\rho$ is the vector of design variables (material densities), $u$ is the state vector (discretized electric field), $b$ is the source vector, and $c$ is the objective weighting vector. We assume that $A$ is invertible for the given parameters, ensuring a unique solution for $u$. The goal is to compute the gradient of the objective function with respect to the design variables, $g = \\frac{df}{d\\rho}$.\n\nThe components of the gradient are $g_j = \\frac{df}{d\\rho_j}$ for each design variable $\\rho_j$. We apply the chain rule to the objective function $f(u(\\rho))$:\n$$\n\\frac{df}{d\\rho_j} = \\frac{\\partial f}{\\partial u} \\frac{du}{d\\rho_j}\n$$\nThe term $\\frac{\\partial f}{\\partial \\rho_j}$ is zero because $f$ depends on $\\rho_j$ only implicitly through $u$. From the definition $f(u)=c^\\top u$, we have $\\frac{\\partial f}{\\partial u} = c^\\top$. Thus,\n$$\n\\frac{df}{d\\rho_j} = c^\\top \\frac{du}{d\\rho_j}\n$$\nTo find the state sensitivity vector $\\frac{du}{d\\rho_j}$, we differentiate the state equation $A u = b$ with respect to $\\rho_j$. The source vector $b$ is assumed to be independent of $\\rho$.\n$$\n\\frac{d}{d\\rho_j} (A(\\rho) u(\\rho)) = \\frac{d b}{d\\rho_j} = 0\n$$\nApplying the product rule:\n$$\n\\frac{\\partial A}{\\partial \\rho_j} u + A \\frac{du}{d\\rho_j} = 0\n$$\nSolving for $\\frac{du}{d\\rho_j}$:\n$$\n\\frac{du}{d\\rho_j} = -A^{-1} \\frac{\\partial A}{\\partial \\rho_j} u\n$$\nSubstituting this back into the expression for the sensitivity:\n$$\n\\frac{df}{d\\rho_j} = -c^\\top A^{-1} \\frac{\\partial A}{\\partial \\rho_j} u\n$$\nThis expression constitutes the \"direct method\" for sensitivity analysis. It is computationally expensive if the number of design variables ($N^2$) is large, as it would require solving for $A^{-1}$ multiplied by a vector for each variable.\n\nTo derive the more efficient adjoint method, we regroup the terms. Let us define the adjoint vector $\\lambda$ as the solution to the following linear system:\n$$\nA^\\top \\lambda = c\n$$\nThis allows us to write $\\lambda^\\top = c^\\top (A^\\top)^{-1} = c^\\top (A^{-1})^\\top$. If $A$ is symmetric, as is the case here ($K$ is real symmetric, $M(\\rho)$ is real diagonal, so $A$ is real symmetric), then $A^\\top = A$ and $\\lambda^\\top = c^\\top A^{-1}$. The sensitivity expression then becomes:\n$$\n\\frac{df}{d\\rho_j} = -\\lambda^\\top \\frac{\\partial A}{\\partial \\rho_j} u\n$$\nThe adjoint method requires only one forward solve ($Au=b$) to find $u$, and one adjoint solve ($A^\\top \\lambda=c$) to find $\\lambda$. Once $u$ and $\\lambda$ are known, the gradient component for each $\\rho_j$ can be computed efficiently.\n\nNow, we find the specific form of $\\frac{\\partial A}{\\partial \\rho_j}$. The matrix $A$ is defined as $A(\\rho) = K + \\omega^2 \\mu M(\\rho)$. The discrete Laplacian $K$, angular frequency $\\omega$, and permeability $\\mu$ are constant with respect to $\\rho$. The mass matrix $M(\\rho)$ is diagonal with entries given by the material interpolation rule:\n$M(\\rho) = \\operatorname{diag}(\\epsilon(\\rho_i))$, where $\\epsilon(\\rho_i) = \\epsilon_{\\min} + (\\epsilon_{\\max} - \\epsilon_{\\min})\\rho_i$.\n\nThe partial derivative of $A$ with respect to $\\rho_j$ is:\n$$\n\\frac{\\partial A}{\\partial \\rho_j} = \\omega^2 \\mu \\frac{\\partial M(\\rho)}{\\partial \\rho_j}\n$$\nThe matrix $\\frac{\\partial M(\\rho)}{\\partial \\rho_j}$ is a diagonal matrix where only the $j$-th diagonal entry is non-zero:\n$$\n\\left(\\frac{\\partial M(\\rho)}{\\partial \\rho_j}\\right)_{ii} = \\frac{d\\epsilon(\\rho_i)}{d\\rho_j} = (\\epsilon_{\\max} - \\epsilon_{\\min}) \\delta_{ij}\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. Thus, $\\frac{\\partial A}{\\partial \\rho_j}$ is a matrix with only one non-zero entry at position $(j,j)$, equal to $\\omega^2 \\mu (\\epsilon_{\\max} - \\epsilon_{\\min})$.\nThe sensitivity is then:\n$$\n\\frac{df}{d\\rho_j} = -\\lambda^\\top \\left( \\omega^2 \\mu (\\epsilon_{\\max} - \\epsilon_{\\min}) E_{jj} \\right) u\n$$\nwhere $E_{jj}$ is a matrix with a $1$ at $(j,j)$ and zeros elsewhere. This simplifies to:\n$$\n\\frac{df}{d\\rho_j} = - \\omega^2 \\mu (\\epsilon_{\\max} - \\epsilon_{\\min}) \\lambda_j u_j\n$$\nThe full gradient vector $g = df/d\\rho$ can be expressed compactly using the Hadamard (element-wise) product $\\odot$:\n$$\ng = - \\omega^2 \\mu (\\epsilon_{\\max} - \\epsilon_{\\min}) (\\lambda \\odot u)\n$$\n\n### 2. Complex-Step Gradient Check\n\nThe complex-step method is a numerical differentiation technique that provides high-accuracy derivatives while avoiding the subtractive cancellation errors inherent in finite difference methods.\n\nConsider a scalar function $g(\\alpha) = f(\\rho + \\alpha d)$, where $\\alpha$ is a scalar and $d$ is a direction vector. The directional derivative of $f$ at $\\rho$ in the direction $d$ is $g'(0)$. To compute this, we use a complex step. If the function $f$ (and thus $g$) is analytic, it can be expanded in a Taylor series around $\\alpha=0$. Evaluating at a purely imaginary step $\\alpha = i\\Delta$:\n$$\ng(i\\Delta) = g(0) + g'(0)(i\\Delta) + \\frac{g''(0)}{2!}(i\\Delta)^2 + \\frac{g'''(0)}{3!}(i\\Delta)^3 + \\mathcal{O}(\\Delta^4)\n$$\n$$\ng(i\\Delta) = g(0) + i g'(0)\\Delta - \\frac{g''(0)}{2}\\Delta^2 - i \\frac{g'''(0)}{6}\\Delta^3 + \\mathcal{O}(\\Delta^4)\n$$\nAssuming $g(\\alpha)$ is real for real $\\alpha$, then $g(0) = f(\\rho)$ is real, and the derivatives $g^{(k)}(0)$ are also real. Taking the imaginary part of the equation above:\n$$\n\\operatorname{Im}[g(i\\Delta)] = g'(0)\\Delta - \\frac{g'''(0)}{6}\\Delta^3 + \\mathcal{O}(\\Delta^5)\n$$\nSolving for $g'(0)$:\n$$\ng'(0) = \\frac{\\operatorname{Im}[g(i\\Delta)]}{\\Delta} + \\frac{g'''(0)}{6}\\Delta^2 + \\mathcal{O}(\\Delta^4)\n$$\nThe complex-step approximation for the directional derivative is therefore:\n$$\n\\frac{df}{d\\rho} \\cdot d = g'(0) \\approx \\frac{\\operatorname{Im}[f(\\rho + i\\Delta d)]}{\\Delta}\n$$\nThe leading error term is of order $\\mathcal{O}(\\Delta^2)$, which allows for high accuracy by choosing a very small $\\Delta$ (e.g., $\\Delta=10^{-20}$). Crucially, this formula does not involve the subtraction of two nearly equal numbers, thus bypassing the round-off errors that plague finite difference schemes for small step sizes.\n\nThis method is valid for the present problem because the mapping $\\rho \\mapsto f(\\rho)$ is analytic. This can be seen by examining the chain of dependencies:\n1.  $\\rho \\mapsto \\epsilon(\\rho) = \\epsilon_{\\min} + (\\epsilon_{\\max} - \\epsilon_{\\min})\\rho$. This is a linear, hence analytic, function.\n2.  $\\epsilon(\\rho) \\mapsto A(\\rho) = K + \\omega^2 \\mu \\operatorname{diag}(\\epsilon(\\rho))$. This is also linear in $\\epsilon$ and hence analytic in $\\rho$.\n3.  $A(\\rho) \\mapsto u(\\rho) = A(\\rho)^{-1}b$. Matrix inversion is an analytic operation, so $u(\\rho)$ is analytic as long as $A(\\rho)$ is non-singular. The problem is assumed to be non-resonant.\n4.  $u(\\rho) \\mapsto f(u) = c^\\top u$. This is a linear functional, which is analytic.\n\nSince the composition of analytic functions is analytic, $f(\\rho)$ is analytic, justifying the use of the complex-step method.\n\n### 3. Implementation and Verification\n\nThe implementation will proceed as follows:\n1.  Construct the discrete Laplacian $K$ for an $N \\times N$ interior grid with homogeneous Dirichlet boundary conditions.\n2.  For each test case, define the design vector $\\rho$ and direction vector $d$.\n3.  **Adjoint Method**:\n    a. Compute $\\epsilon(\\rho)$ and assemble the real, symmetric matrix $A(\\rho)$.\n    b. Solve the forward system $A u = b$ for the field $u$.\n    c. Solve the adjoint system $A^{\\top} \\lambda = c$ (which is $A \\lambda = c$ due to symmetry) for the adjoint field $\\lambda$.\n    d. Compute the gradient vector $g$ using the derived formula.\n    e. Calculate the directional derivative $g \\cdot d$.\n4.  **Complex-Step Method**:\n    a. Form the complex design vector $\\rho_{\\text{cs}} = \\rho + i\\Delta d$.\n    b. Compute the complex permittivity $\\epsilon(\\rho_{\\text{cs}})$ and assemble the complex, symmetric matrix $A(\\rho_{\\text{cs}})$.\n    c. Solve the complex system $A_{\\text{cs}} u_{\\text{cs}} = b$ for the complex field $u_{\\text{cs}}$.\n    d. Evaluate the complex objective $f_{\\text{cs}} = c^\\top u_{\\text{cs}}$.\n    e. Compute the directional derivative approximation $\\operatorname{Im}(f_{\\text{cs}})/\\Delta$.\n5.  Calculate the relative error between the adjoint and complex-step results as specified. This serves as a validation of the adjoint implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import diags, kron, eye\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Solves the problem of validating adjoint sensitivities for a 2D FDFD\n    electromagnetic model using the complex-step method.\n    \"\"\"\n    # Define fixed parameters from the problem statement.\n    N = 12\n    omega = 10.0\n    mu = 1.0\n    eps_min = 1.0\n    eps_max = 12.0\n    grid_spacing = 1.0\n\n    # Total number of degrees of freedom (interior grid points)\n    N_dof = N * N\n\n    # Construct the 2D discrete Laplacian K for an N x N interior grid\n    # with homogeneous Dirichlet boundary conditions.\n    # The five-point stencil for del^2 u is (u_i-1 + u_i+1 + ... - 4*u_i) / h^2.\n    # With h=1, the matrix entries are 1 for neighbors and -4 for the center.\n    # We use the kronecker product of 1D Laplacian matrices to build the 2D operator.\n    T_N = diags([1, -2, 1], [-1, 0, 1], shape=(N, N), format='csc') / (grid_spacing**2)\n    I_N = eye(N, format='csc')\n    K = kron(I_N, T_N) + kron(T_N, I_N)\n\n    # Define source vector b and objective selector c.\n    # Indices are based on row-major ordering: idx = row * N + col.\n    src_idx = (N // 2) * N + 1\n    obj_idx = (N // 2) * N + (N - 2)\n\n    b = np.zeros(N_dof, dtype=np.float64)\n    b[src_idx] = 1.0\n\n    c = np.zeros(N_dof, dtype=np.float64)\n    c[obj_idx] = 1.0\n\n    # Define the test cases.\n    test_cases = [\n        {'seed': 42, 'rho_type': 'random', 'd_type': 'random_normalized', 'delta': 1e-20},\n        {'seed': 42, 'rho_type': 'random', 'd_type': 'random_normalized', 'delta': 1e-10},\n        {'seed': None, 'rho_type': 'zeros', 'd_type': 'basis', 'delta': 1e-20},\n    ]\n\n    results = []\n    \n    # Process each test case\n    for case in test_cases:\n        # --- 1. Set up design (rho) and direction (d) vectors for the case ---\n        if case['rho_type'] == 'random':\n            rng = np.random.default_rng(case['seed'])\n            rho = rng.random(N_dof)\n        elif case['rho_type'] == 'zeros':\n            rho = np.zeros(N_dof)\n\n        if case['d_type'] == 'random_normalized':\n            # Use the same seed for d to ensure reproducibility with rho\n            rng = np.random.default_rng(case['seed'])\n            d = rng.random(N_dof)\n            d /= np.linalg.norm(d)\n        elif case['d_type'] == 'basis':\n            d_idx = (N // 2) * N + (N // 2)\n            d = np.zeros(N_dof)\n            d[d_idx] = 1.0\n        \n        delta = case['delta']\n\n        # --- 2. Adjoint Sensitivity Calculation ---\n        # Material interpolation: eps(rho)\n        eps_rho = eps_min + (eps_max - eps_min) * rho\n        M_rho = diags(eps_rho, 0, shape=(N_dof, N_dof), format='csc')\n        \n        # System matrix: A(rho) = K + omega^2 * mu * M(rho)\n        A = K + (omega**2 * mu) * M_rho\n\n        # Solve forward problem: A * u = b\n        u = spsolve(A, b)\n        \n        # Solve adjoint problem: A.T * lambda = c\n        # Since A is real and symmetric, A.T = A.\n        lambda_adj = spsolve(A.T, c)\n\n        # Compute gradient vector g = df/d(rho)\n        # g_j = -omega^2 * mu * (eps_max - eps_min) * lambda_j * u_j\n        g = -(omega**2 * mu * (eps_max - eps_min)) * (lambda_adj * u)\n        \n        # Compute directional derivative: g . d\n        adjoint_deriv = np.dot(g, d)\n\n        # --- 3. Complex-Step Derivative Calculation ---\n        # Perturb design variable: rho_cs = rho + i*delta*d\n        rho_cs = rho + 1j * delta * d\n        \n        # Complex material properties and system matrix\n        eps_rho_cs = eps_min + (eps_max - eps_min) * rho_cs\n        M_rho_cs = diags(eps_rho_cs, 0, shape=(N_dof, N_dof), format='csc')\n        A_cs = K + (omega**2 * mu) * M_rho_cs\n        \n        # Solve complex forward system: A_cs * u_cs = b\n        u_cs = spsolve(A_cs, b)\n        \n        # Evaluate complex objective function: f_cs = c.T * u_cs\n        f_cs = np.dot(c, u_cs)\n        \n        # Compute complex-step derivative estimate\n        cs_deriv = np.imag(f_cs) / delta\n\n        # --- 4. Calculate Relative Error ---\n        numerator = np.abs(adjoint_deriv - cs_deriv)\n        # Use a floor for the denominator to handle cases where the derivative is near zero\n        denominator = np.maximum(np.abs(adjoint_deriv), 1e-12)\n        relative_error = numerator / denominator\n        \n        results.append(relative_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A common challenge in topology optimization is the emergence of non-physical, grid-scale artifacts like checkerboard patterns and intermediate \"gray\" material regions. This hands-on exercise introduces the standard arsenal of regularization techniques used to ensure manufacturable and well-posed designs. You will implement and analyze the interplay between spatial density filtering, which imposes a minimum feature size, and a smooth Heaviside projection, which encourages a binary material distribution . Through a continuation strategy, you will explore how to gradually sharpen the design to achieve clear, high-performance layouts.",
            "id": "3356441",
            "problem": "Consider time-harmonic Maxwell's equations with angular frequency $\\omega$, electric field $\\mathbf{E}$, magnetic permeability $\\mu$, electric permittivity $\\varepsilon$, and impressed current density $\\mathbf{J}$ in a bounded domain discretized on a uniform Cartesian grid. The governing equation can be written as the curl-curl form\n$$\n\\nabla \\times \\left( \\mu^{-1} \\nabla \\times \\mathbf{E} \\right) - \\omega^2 \\varepsilon(\\rho) \\mathbf{E} = i \\omega \\mathbf{J},\n$$\nwhere $\\varepsilon(\\rho)$ denotes the material interpolation that depends on an element-wise design variable density $\\rho \\in [0,1]$. In density-based topology optimization, enforcing a minimum feature size and promoting near-binary materials are essential to suppress non-physical checkerboarding and gray regions that degrade electromagnetic performance and manufacturability. A standard approach composes a spatial density filter and a smooth Heaviside projection:\n$$\n\\tilde{\\rho} = \\mathrm{proj}\\!\\left( \\mathrm{filter}(\\rho) \\right),\n$$\nwhere $\\mathrm{filter}$ enforces a minimum length scale and $\\mathrm{proj}$ drives densities toward $0$ or $1$ while allowing differentiability for gradient-based optimization.\n\nYou will implement the following operators and analyze continuation strategies on the projection sharpness parameter to reduce checkerboarding:\n\n1. Density filter. Let the grid be of size $N \\times N$ with unit grid spacing. For a given filter radius $r > 0$, define weights\n$$\nw_{ij} = \\max\\!\\left( 0,\\, r - d_{ij} \\right),\n$$\nwhere $d_{ij}$ is the Euclidean distance (in grid-spacing units) between pixel $i$ and pixel $j$ (with $i$ and $j$ indexing grid locations). The filtered density at pixel $i$ is\n$$\n\\hat{\\rho}_i = \\frac{\\sum_j w_{ij} \\rho_j}{\\sum_j w_{ij}}.\n$$\nOn a uniform grid, $w_{ij}$ depends only on the relative offset between pixels, so the filtering can be implemented via a convolution with a radially symmetric kernel followed by pointwise normalization:\n$$\n\\hat{\\rho} = \\frac{K * \\rho}{K * \\mathbf{1}},\n$$\nwhere $K$ is the kernel of weights $w_{ij}$, $*$ denotes discrete convolution with zero padding at the boundary, and $\\mathbf{1}$ is the all-ones image.\n\n2. Heaviside projection. For sharpness parameter $\\beta > 0$ and threshold $\\eta \\in (0,1)$, use the smooth projection\n$$\n\\tilde{\\rho}_i = \\frac{\\tanh\\!\\left( \\beta \\eta \\right) + \\tanh\\!\\left( \\beta (\\hat{\\rho}_i - \\eta) \\right)}{\\tanh\\!\\left( \\beta \\eta \\right) + \\tanh\\!\\left( \\beta (1 - \\eta) \\right)}.\n$$\nThis map is differentiable with respect to $\\hat{\\rho}$ and the derivative needed for gradient-based updates is\n$$\n\\frac{\\partial \\tilde{\\rho}_i}{\\partial \\hat{\\rho}_i} = \\frac{\\beta \\left( 1 - \\tanh^2\\!\\left( \\beta (\\hat{\\rho}_i - \\eta) \\right) \\right)}{\\tanh\\!\\left( \\beta \\eta \\right) + \\tanh\\!\\left( \\beta (1 - \\eta) \\right)}.\n$$\n\n3. Surrogate objective for regularization and binarization. To study the effect of filtering and projection without solving the full electromagnetic field, consider the surrogate functional on the projected density\n$$\n\\Phi(\\tilde{\\rho}) = \\lambda_{\\mathrm{smooth}} \\sum_{i} \\left\\| \\nabla \\tilde{\\rho}_i \\right\\|^2 \\;+\\; \\lambda_{\\mathrm{binary}} \\sum_i \\tilde{\\rho}_i (1 - \\tilde{\\rho}_i) \\;+\\; \\lambda_{\\mathrm{target}} \\sum_i T_i \\tilde{\\rho}_i,\n$$\nwhere $\\nabla$ is the discrete gradient with unit spacing, $\\left\\| \\nabla \\tilde{\\rho}_i \\right\\|^2$ is implemented using the standard $5$-point discrete Laplacian in the gradient of the smoothness term, and $T$ is a fixed target mask that encourages material presence in a designated subregion. The derivative with respect to $\\tilde{\\rho}$ reads\n$$\n\\frac{\\partial \\Phi}{\\partial \\tilde{\\rho}} = -2 \\lambda_{\\mathrm{smooth}} \\Delta \\tilde{\\rho} + \\lambda_{\\mathrm{binary}} (1 - 2 \\tilde{\\rho}) + \\lambda_{\\mathrm{target}} T,\n$$\nwhere $\\Delta$ denotes the discrete Laplacian operator. The chain rule gives the gradient with respect to the original design variable $\\rho$:\n$$\n\\frac{\\partial \\Phi}{\\partial \\rho_j} = \\sum_i \\left( \\frac{\\partial \\Phi}{\\partial \\tilde{\\rho}_i} \\cdot \\frac{\\partial \\tilde{\\rho}_i}{\\partial \\hat{\\rho}_i} \\cdot \\frac{\\partial \\hat{\\rho}_i}{\\partial \\rho_j} \\right) \\;=\\; \\sum_i \\left( g^{\\mathrm{tilde}}_i \\cdot s_i \\cdot \\frac{w_{ij}}{\\sum_k w_{ik}} \\right),\n$$\nwhere $g^{\\mathrm{tilde}} = \\partial \\Phi / \\partial \\tilde{\\rho}$ and $s_i = \\partial \\tilde{\\rho}_i / \\partial \\hat{\\rho}_i$. Computationally, this can be evaluated by convolving the field $g^{\\mathrm{tilde}} \\cdot s / (K * \\mathbf{1})$ with the kernel $K$.\n\n4. Continuation strategies on $\\beta$. A continuation strategy chooses a sequence of increasing $\\beta$ values to gradually sharpen the projection while avoiding premature non-differentiability that can lock in checkerboard patterns. You will simulate gradient descent updates on $\\rho$ with step size $\\alpha > 0$ over a fixed number of iterations per $\\beta$ in the schedule:\n$$\n\\rho^{(k+1)} = \\mathrm{clip}_{[0,1]}\\!\\left( \\rho^{(k)} - \\alpha \\frac{\\partial \\Phi}{\\partial \\rho}\\bigg|_{\\rho^{(k)}} \\right),\n$$\nwhere at each iteration, $\\hat{\\rho} = \\mathrm{filter}(\\rho)$, $\\tilde{\\rho} = \\mathrm{proj}_{\\beta,\\eta}(\\hat{\\rho})$, and $\\partial \\Phi / \\partial \\rho$ is computed via the chain rule above.\n\n5. Metrics. After completing the continuation schedule, compute the following two metrics on the final $\\tilde{\\rho}$:\n- Checkerboard index\n$$\nI_{\\mathrm{cb}} = \\frac{\\left| \\sum_{i,j} \\tilde{\\rho}_{ij} (-1)^{i + j} \\right|}{\\sum_{i,j} \\left| \\tilde{\\rho}_{ij} \\right|}.\n$$\nThis quantifies the normalized correlation with the alternating high-frequency pattern.\n- Grayness (binarity penalty)\n$$\nG = \\frac{1}{N^2} \\sum_{i,j} 4 \\tilde{\\rho}_{ij} (1 - \\tilde{\\rho}_{ij}),\n$$\nwhich is $0$ for perfectly binary designs and $1$ when all entries equal $0.5$.\n\nImplementation details:\n- Use unit grid spacing; the filter radius $r$ is measured in grid-spacing units.\n- Use zero padding for convolution at boundaries.\n- Use the standard $5$-point stencil for the discrete Laplacian with Neumann-like treatment by replicating edges.\n- Use $\\lambda_{\\mathrm{smooth}} = 0.1$, $\\lambda_{\\mathrm{binary}} = 1.0$, $\\lambda_{\\mathrm{target}} = 0.05$, and step size $\\alpha = 0.2$.\n- The target mask $T$ is defined as $T_{ij} = -1$ inside the central box of size $6 \\times 6$ centered in the grid and $T_{ij} = 0$ elsewhere; this encourages material presence in the central region.\n\nTest suite:\nFor each test case, run the continuation schedule with a fixed number of iterations per $\\beta$ equal to $10$.\n\n- Test case $1$ (happy path): $N = 16$, $r = 3.0$, $\\eta = 0.5$, schedule $\\beta = [1.0, 2.0, 4.0, 8.0]$, initial $\\rho$ is independent identically distributed uniform on $[0,1]$ with random seed $0$.\n- Test case $2$ (boundary condition: uniform initial design): $N = 16$, $r = 3.0$, $\\eta = 0.5$, schedule $\\beta = [1.0, 2.0, 4.0, 8.0]$, initial $\\rho_{ij} = 0.5$ for all $i,j$.\n- Test case $3$ (edge case: near-absent filtering with aggressive projection): $N = 16$, $r = 1.0$, $\\eta = 0.5$, schedule $\\beta = [16.0, 16.0, 16.0, 16.0]$, initial $\\rho_{ij} = 1$ if $(i + j)$ is even and $\\rho_{ij} = 0$ otherwise (perfect checkerboard).\n- Test case $4$ (large filter radius with non-monotone continuation): $N = 16$, $r = 7.9$, $\\eta = 0.5$, schedule $\\beta = [1.0, 1.0, 2.0, 1.0, 4.0]$, initial $\\rho$ is independent identically distributed uniform on $[0,1]$ with random seed $42$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result is a two-element list $[I_{\\mathrm{cb}}, G]$ with both entries as floats. The overall output is a list of these per-test-case lists, for example, $\\left[ [I_{\\mathrm{cb},1}, G_1], [I_{\\mathrm{cb},2}, G_2], [I_{\\mathrm{cb},3}, G_3], [I_{\\mathrm{cb},4}, G_4] \\right]$.",
            "solution": "We begin from time-harmonic Maxwell's equations, which in discrete form lead to a curl-curl operator and a material interpolation $\\varepsilon(\\rho)$ that connects the electromagnetic response to the design variable density field $\\rho \\in [0,1]$. In density-based topology optimization, numerical oscillations at the scale of the grid can arise due to the discrete nature of the curl-curl operator and the stiffness interpolation, which manifests as checkerboarding: alternating high-frequency patterns that minimize discrete energies but do not correspond to physically realizable structures. Enforcing a minimum feature size and promoting near-binary designs are standard remedies.\n\nEnforcing a minimum feature size is accomplished by a spatial filter on the design variable. On a grid with unit spacing, a distance-based filter with radius $r$ uses weights $w_{ij} = \\max(0, r - d_{ij})$ to form\n$$\n\\hat{\\rho}_i = \\frac{\\sum_j w_{ij} \\rho_j}{\\sum_j w_{ij}}.\n$$\nBecause $w_{ij}$ depends only on relative offsets, we can implement this as a convolution with a kernel $K$, followed by pointwise normalization using $K * \\mathbf{1}$. This suppresses features smaller than $r$ and attenuates high spatial frequencies that contribute to checkerboarding.\n\nTo encourage near-binary materials, we apply a smooth Heaviside projection with threshold $\\eta$ and sharpness $\\beta$:\n$$\n\\tilde{\\rho}_i = \\frac{\\tanh\\!\\left( \\beta \\eta \\right) + \\tanh\\!\\left( \\beta (\\hat{\\rho}_i - \\eta) \\right)}{\\tanh\\!\\left( \\beta \\eta \\right) + \\tanh\\!\\left( \\beta (1 - \\eta) \\right)}.\n$$\nThis map approaches a step function as $\\beta \\to \\infty$, but for finite $\\beta$ it remains differentiable. The derivative is\n$$\n\\frac{\\partial \\tilde{\\rho}_i}{\\partial \\hat{\\rho}_i} = \\frac{\\beta \\left( 1 - \\tanh^2\\!\\left( \\beta (\\hat{\\rho}_i - \\eta) \\right) \\right)}{\\tanh\\!\\left( \\beta \\eta \\right) + \\tanh\\!\\left( \\beta (1 - \\eta) \\right)}.\n$$\nThis derivative scales the sensitivity, controlling how sharp the projection influences gradients.\n\nTo study the effect of these operators without solving the electromagnetic field, we define a surrogate objective that encodes two regularization mechanisms and a mild target preference:\n$$\n\\Phi(\\tilde{\\rho}) = \\lambda_{\\mathrm{smooth}} \\sum_i \\left\\| \\nabla \\tilde{\\rho}_i \\right\\|^2 + \\lambda_{\\mathrm{binary}} \\sum_i \\tilde{\\rho}_i (1 - \\tilde{\\rho}_i) + \\lambda_{\\mathrm{target}} \\sum_i T_i \\tilde{\\rho}_i.\n$$\nThe first term penalizes high gradients in $\\tilde{\\rho}$, analogous to discouraging rapid material variation that can cause numerical artifacts. The second term penalizes gray regions by making $\\tilde{\\rho}_i$ close to either $0$ or $1$. The third term introduces a simple target mask $T$ to make the problem nontrivial and to mimic a preference for material in a region where fields might concentrate. To compute the gradient, we use the discrete $5$-point Laplacian $\\Delta$ for $\\left\\| \\nabla \\tilde{\\rho} \\right\\|^2$, yielding\n$$\n\\frac{\\partial \\Phi}{\\partial \\tilde{\\rho}} = -2 \\lambda_{\\mathrm{smooth}} \\Delta \\tilde{\\rho} + \\lambda_{\\mathrm{binary}} (1 - 2 \\tilde{\\rho}) + \\lambda_{\\mathrm{target}} T.\n$$\n\nThe chain rule propagates sensitivities back to $\\rho$:\n$$\n\\frac{\\partial \\Phi}{\\partial \\rho_j} = \\sum_i \\left( \\frac{\\partial \\Phi}{\\partial \\tilde{\\rho}_i} \\cdot \\frac{\\partial \\tilde{\\rho}_i}{\\partial \\hat{\\rho}_i} \\cdot \\frac{\\partial \\hat{\\rho}_i}{\\partial \\rho_j} \\right).\n$$\nGiven\n$$\n\\frac{\\partial \\hat{\\rho}_i}{\\partial \\rho_j} = \\frac{w_{ij}}{\\sum_k w_{ik}},\n$$\nthe computation reduces to forming the field\n$$\ng^{\\mathrm{hat}}_i = \\frac{\\partial \\Phi}{\\partial \\tilde{\\rho}_i} \\cdot \\frac{\\partial \\tilde{\\rho}_i}{\\partial \\hat{\\rho}_i}, \\quad f_i = \\frac{g^{\\mathrm{hat}}_i}{\\sum_k w_{ik}},\n$$\nand convolving $f$ with $K$:\n$$\n\\frac{\\partial \\Phi}{\\partial \\rho} = K * f.\n$$\nThis maintains consistency with the filter normalization and correctly accounts for boundary effects via zero-padding and the normalization map $K * \\mathbf{1}$.\n\nWe perform gradient descent with step size $\\alpha$ and clipping to $[0,1]$ to enforce box constraints on $\\rho$:\n$$\n\\rho^{(k+1)} = \\mathrm{clip}_{[0,1]}\\!\\left( \\rho^{(k)} - \\alpha \\frac{\\partial \\Phi}{\\partial \\rho} \\right).\n$$\nAt each iteration, we recompute $\\hat{\\rho} = \\mathrm{filter}(\\rho)$, $\\tilde{\\rho} = \\mathrm{proj}(\\hat{\\rho})$, and the gradient via the chain rule.\n\nContinuation on $\\beta$ is carried out by running several iterations with moderate $\\beta$ values first, then increasing $\\beta$ to sharpen the projection. The rationale is that small $\\beta$ keeps the sensitivity smooth, allowing the filter and smoothness penalty to suppress high-frequency content before the projection pushes toward binarity. An overly aggressive $\\beta$ early on can lock in high-frequency checkerboards, particularly when the filter radius $r$ is small.\n\nWe evaluate two metrics on the final $\\tilde{\\rho}$:\n- The checkerboard index\n$$\nI_{\\mathrm{cb}} = \\frac{\\left| \\sum_{i,j} \\tilde{\\rho}_{ij} (-1)^{i+j} \\right|}{\\sum_{i,j} \\left| \\tilde{\\rho}_{ij} \\right|}.\n$$\nIf $\\tilde{\\rho}$ closely resembles a checkerboard, the numerator becomes large relative to the denominator, approaching values near $1$ in extreme cases. For uniform or low-frequency patterns, the numerator cancels and $I_{\\mathrm{cb}}$ becomes small.\n- The grayness\n$$\nG = \\frac{1}{N^2} \\sum_{i,j} 4 \\tilde{\\rho}_{ij} (1 - \\tilde{\\rho}_{ij}),\n$$\nwhich attains $0$ for binary designs and $1$ for uniform $\\tilde{\\rho} = 0.5$.\n\nWe use the fixed constants $\\lambda_{\\mathrm{smooth}} = 0.1$, $\\lambda_{\\mathrm{binary}} = 1.0$, $\\lambda_{\\mathrm{target}} = 0.05$, and $\\alpha = 0.2$. The target mask $T$ is $-1$ inside the central $6 \\times 6$ box and $0$ elsewhere, which effectively incentivizes the presence of material in the middle when minimizing $\\Phi$.\n\nThe test suite explores:\n- A typical case ($N = 16$, $r = 3.0$, $\\eta = 0.5$, schedule $\\beta = [1.0, 2.0, 4.0, 8.0]$) with random initial $\\rho$ to validate expected behavior.\n- A uniform initial $\\rho = 0.5$ to test how binarization and target preference interact under the same schedule.\n- A near-absent filter ($r = 1.0$) with aggressive projection ($\\beta$ fixed at $16.0$), beginning from a perfect checkerboard to expose failure modes with insufficient filtering.\n- A large filter radius ($r = 7.9$) and a non-monotone schedule $\\beta = [1.0, 1.0, 2.0, 1.0, 4.0]$ to test robustness under strong smoothing.\n\nAlgorithmic steps per test case:\n1. Initialize $\\rho$ according to the case definition.\n2. For each $\\beta$ in the schedule, perform $10$ gradient descent iterations:\n   - Build kernel $K$ of radius $r$ with $w_{ij} = \\max(0, r - \\sqrt{\\Delta x^2 + \\Delta y^2})$.\n   - Compute $\\hat{\\rho} = (K * \\rho) / (K * \\mathbf{1})$.\n   - Compute $\\tilde{\\rho}$ via the projection formula.\n   - Compute $\\partial \\Phi / \\partial \\tilde{\\rho} = -2 \\lambda_{\\mathrm{smooth}} \\Delta \\tilde{\\rho} + \\lambda_{\\mathrm{binary}} (1 - 2 \\tilde{\\rho}) + \\lambda_{\\mathrm{target}} T$.\n   - Compute $s = \\partial \\tilde{\\rho} / \\partial \\hat{\\rho}$ and $g^{\\mathrm{hat}} = (\\partial \\Phi / \\partial \\tilde{\\rho}) \\cdot s$.\n   - Compute $f = g^{\\mathrm{hat}} / (K * \\mathbf{1})$ and then $\\partial \\Phi / \\partial \\rho = K * f$.\n   - Update $\\rho \\leftarrow \\mathrm{clip}_{[0,1]}( \\rho - \\alpha \\, \\partial \\Phi / \\partial \\rho )$.\n3. After the schedule, compute the final $\\tilde{\\rho}$ with the last $\\beta$ and filter radius $r$, then evaluate $I_{\\mathrm{cb}}$ and $G$.\n4. Return the pair $[I_{\\mathrm{cb}}, G]$.\n\nThe single-line output aggregates the four test case results as a list of two-element lists. This experiment demonstrates, from first principles and consistent with electromagnetic regularization needs, how filtering and smooth projection coupled with continuation affect high-frequency artifacts and binarity in the design variable, offering a computationally sound strategy to suppress checkerboarding before sharpening the material distribution.",
            "answer": "```python\nimport numpy as np\n\n# Implement density filter and Heaviside projection with continuation on beta.\n# Environment: Python 3.12, numpy 1.23.5, scipy 1.11.4 allowed but not needed.\n\ndef build_kernel(radius: float):\n    \"\"\"\n    Build a radially symmetric distance-weighted kernel K with weights\n    w(dx,dy) = max(0, radius - sqrt(dx^2 + dy^2)),\n    over offsets in the square [-R, R] x [-R, R], where R = ceil(radius).\n    \"\"\"\n    R = int(np.ceil(radius))\n    size = 2 * R + 1\n    K = np.zeros((size, size), dtype=np.float64)\n    for i in range(size):\n        for j in range(size):\n            dx = i - R\n            dy = j - R\n            dist = np.sqrt(dx * dx + dy * dy)\n            w = max(0.0, radius - dist)\n            K[i, j] = w\n    return K\n\ndef convolve2d_same(image: np.ndarray, kernel: np.ndarray):\n    \"\"\"\n    Convolve a 2D image with kernel using zero padding, output same size as image.\n    \"\"\"\n    H, W = image.shape\n    kH, kW = kernel.shape\n    pad_h = kH // 2\n    pad_w = kW // 2\n    # Pad image with zeros\n    padded = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w)), mode='constant', constant_values=0.0)\n    result = np.zeros_like(image, dtype=np.float64)\n    # Flip kernel for convolution\n    kflip = kernel[::-1, ::-1]\n    for i in range(H):\n        for j in range(W):\n            window = padded[i:i + kH, j:j + kW]\n            result[i, j] = np.sum(window * kflip)\n    return result\n\ndef density_filter(rho: np.ndarray, kernel: np.ndarray):\n    \"\"\"\n    Apply density filter: hat_rho = (K * rho) / (K * 1).\n    \"\"\"\n    ones = np.ones_like(rho, dtype=np.float64)\n    num = convolve2d_same(rho, kernel)\n    den = convolve2d_same(ones, kernel)\n    # Avoid division by zero by adding tiny epsilon\n    eps = 1e-12\n    hat_rho = num / (den + eps)\n    return hat_rho, den\n\ndef heaviside_projection(hat_rho: np.ndarray, beta: float, eta: float):\n    \"\"\"\n    Smooth Heaviside projection with sharpness beta and threshold eta.\n    \"\"\"\n    th_eta = np.tanh(beta * eta)\n    th_one = np.tanh(beta * (1.0 - eta))\n    denom = th_eta + th_one\n    proj = (th_eta + np.tanh(beta * (hat_rho - eta))) / denom\n    return proj\n\ndef dproj_dhat(hat_rho: np.ndarray, beta: float, eta: float):\n    \"\"\"\n    Derivative of the smooth Heaviside projection with respect to hat_rho.\n    \"\"\"\n    th_eta = np.tanh(beta * eta)\n    th_one = np.tanh(beta * (1.0 - eta))\n    denom = th_eta + th_one\n    tanh_arg = np.tanh(beta * (hat_rho - eta))\n    deriv = beta * (1.0 - tanh_arg ** 2) / denom\n    return deriv\n\ndef laplacian_5pt(u: np.ndarray):\n    \"\"\"\n    Discrete 5-point Laplacian with Neumann-like edge handling by replication.\n    \"\"\"\n    H, W = u.shape\n    # Pad by replication\n    up = np.pad(u, ((1,1),(1,1)), mode='edge')\n    center = up[1:-1, 1:-1]\n    left = up[1:-1, 0:-2]\n    right = up[1:-1, 2:]\n    upv = up[0:-2, 1:-1]\n    down = up[2:, 1:-1]\n    lap = (left + right + upv + down - 4.0 * center)\n    return lap\n\ndef surrogate_gradient_tilde(tilde_rho: np.ndarray, lamb_smooth: float, lamb_binary: float, lamb_target: float, T: np.ndarray):\n    \"\"\"\n    Compute dPhi/d(tilde_rho) for the surrogate objective.\n    \"\"\"\n    grad = -2.0 * lamb_smooth * laplacian_5pt(tilde_rho) + lamb_binary * (1.0 - 2.0 * tilde_rho) + lamb_target * T\n    return grad\n\ndef checkerboard_index(tilde_rho: np.ndarray):\n    \"\"\"\n    Compute checkerboard index I_cb = |sum tilde_rho * (-1)^(i+j)| / sum |tilde_rho|.\n    \"\"\"\n    H, W = tilde_rho.shape\n    # Build alternating sign pattern\n    ii, jj = np.indices((H, W))\n    sign = ((ii + jj) % 2) * 2 - 1  # values in {-1, +1}\n    num = np.abs(np.sum(tilde_rho * sign))\n    den = np.sum(np.abs(tilde_rho)) + 1e-12\n    return float(num / den)\n\ndef grayness(tilde_rho: np.ndarray):\n    \"\"\"\n    Compute grayness G = mean(4 * x * (1 - x)).\n    \"\"\"\n    g = 4.0 * tilde_rho * (1.0 - tilde_rho)\n    return float(np.mean(g))\n\ndef run_case(N: int, r: float, eta: float, beta_schedule: list, rho_init: np.ndarray, iterations_per_beta: int = 10,\n             lamb_smooth: float = 0.1, lamb_binary: float = 1.0, lamb_target: float = 0.05, alpha: float = 0.2):\n    \"\"\"\n    Run continuation schedule on a test case and return [I_cb, G] on final projected density.\n    \"\"\"\n    # Build target mask T: central 6x6 box has -1, else 0\n    T = np.zeros((N, N), dtype=np.float64)\n    c0 = N // 2 - 3\n    c1 = N // 2 + 3\n    T[c0:c1, c0:c1] = -1.0\n\n    # Build kernel once per case (radius fixed)\n    K = build_kernel(r)\n    rho = rho_init.copy()\n\n    for beta in beta_schedule:\n        for _ in range(iterations_per_beta):\n            hat_rho, den_map = density_filter(rho, K)\n            tilde = heaviside_projection(hat_rho, beta, eta)\n            # Gradient on tilde_rho\n            g_tilde = surrogate_gradient_tilde(tilde, lamb_smooth, lamb_binary, lamb_target, T)\n            # Chain rule through projection\n            s = dproj_dhat(hat_rho, beta, eta)\n            g_hat = g_tilde * s\n            # Divide by local filter normalization\n            f = g_hat / (den_map + 1e-12)\n            # Gradient wrt rho is convolution of f with K\n            g_rho = convolve2d_same(f, K)\n            # Update and clip\n            rho = np.clip(rho - alpha * g_rho, 0.0, 1.0)\n\n    # Final metrics computed on final projected density with last beta\n    hat_rho, _ = density_filter(rho, K)\n    tilde = heaviside_projection(hat_rho, beta_schedule[-1], eta)\n    Icb = checkerboard_index(tilde)\n    G = grayness(tilde)\n    return [Icb, G]\n\ndef solve():\n    # Define test cases from the problem statement.\n    # Test case 1: happy path\n    N1 = 16\n    r1 = 3.0\n    eta1 = 0.5\n    beta_sched1 = [1.0, 2.0, 4.0, 8.0]\n    rng1 = np.random.default_rng(0)\n    rho_init1 = rng1.random((N1, N1))\n\n    # Test case 2: uniform initial design\n    N2 = 16\n    r2 = 3.0\n    eta2 = 0.5\n    beta_sched2 = [1.0, 2.0, 4.0, 8.0]\n    rho_init2 = np.full((N2, N2), 0.5, dtype=np.float64)\n\n    # Test case 3: near-absent filtering with aggressive projection, starting from perfect checkerboard\n    N3 = 16\n    r3 = 1.0\n    eta3 = 0.5\n    beta_sched3 = [16.0, 16.0, 16.0, 16.0]\n    ii3, jj3 = np.indices((N3, N3))\n    rho_init3 = (((ii3 + jj3) % 2) == 0).astype(np.float64)\n\n    # Test case 4: large filter radius with non-monotone continuation, random seed 42\n    N4 = 16\n    r4 = 7.9\n    eta4 = 0.5\n    beta_sched4 = [1.0, 1.0, 2.0, 1.0, 4.0]\n    rng4 = np.random.default_rng(42)\n    rho_init4 = rng4.random((N4, N4))\n\n    results = []\n    results.append(run_case(N1, r1, eta1, beta_sched1, rho_init1))\n    results.append(run_case(N2, r2, eta2, beta_sched2, rho_init2))\n    results.append(run_case(N3, r3, eta3, beta_sched3, rho_init3))\n    results.append(run_case(N4, r4, eta4, beta_sched4, rho_init4))\n\n    # Final print statement in the exact required format: single line, comma-separated list enclosed in brackets.\n    # Each result is a two-element list [I_cb, G].\n    def fmt_pair(pair):\n        # Format floats with adequate precision\n        return \"[\" + \",\".join(f\"{x:.6f}\" for x in pair) + \"]\"\n    print(\"[\" + \",\".join(fmt_pair(res) for res in results) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "The performance of many advanced electromagnetic devices, such as rectennas, is determined by the tight coupling between linear wave physics and nonlinear circuit dynamics. This capstone exercise moves beyond linear device optimization into the realm of system-level co-design. You will implement a full optimization loop that maximizes the DC power from a rectenna by simultaneously designing its electromagnetic structure and accounting for the nonlinear harmonic generation in its rectifier circuit . This practice requires developing a composite adjoint method to propagate sensitivities through both a frequency-domain electromagnetic model and a nonlinear harmonic-balance solver, showcasing a powerful technique for holistic system optimization.",
            "id": "3356353",
            "problem": "You are given a simplified, but physically consistent, co-design model of a radio-frequency rectifying antenna (rectenna) that couples a linear electromagnetic ladder network to a nonlinear diode-based rectifier. Your task is to implement a complete, runnable program that, for each test case, performs a few iterations of gradient-based topology optimization of the ladder’s material distribution to maximize the direct-current output voltage $V_{\\mathrm{dc}}$, using a composite adjoint that differentiates through both a harmonically balanced nonlinear circuit solve and the linear electromagnetic network.\n\nThe electromagnetic subproblem is modeled as a one-dimensional ladder network, representing a finite-difference discretization of Maxwell’s equations via a transmission-line analog. There are $N$ nodes indexed $i \\in \\{0,\\dots,N-1\\}$; node $i=0$ is the input (port) node. Each node $i$ is connected to ground by a shunt capacitor with capacitance $C_i(\\rho_i)$, where $\\rho_i \\in [0,1]$ is a design variable (material density) and the material interpolation uses a standard Solid Isotropic Material with Penalization (SIMP) form\n$$\nC_i(\\rho_i) = C_{\\min} + \\rho_i^p \\left(C_{\\max} - C_{\\min}\\right),\n$$\nwith penalization exponent $p \\ge 1$. Adjacent nodes $i$ and $i+1$ are connected by a series inductor of inductance $L$ that is independent of the design variables. The last node $i = N-1$ is terminated to ground by a real resistance $R_{\\mathrm{term}}$ to emulate absorption (radiation) and avoid trivial resonances. At angular frequency $\\omega$, the complex nodal admittance matrix $\\mathbf{G}(\\omega,\\boldsymbol{\\rho}) \\in \\mathbb{C}^{N \\times N}$ is assembled by Kirchhoff’s Current Law (KCL). For each series inductive branch, the series admittance is $Y_L(\\omega) = 1/(j\\omega L)$; for each shunt capacitance, the shunt admittance is $Y_{Ci}(\\omega) = j\\omega C_i(\\rho_i)$; and the termination contributes a conductance $G_{\\mathrm{term}} = 1/R_{\\mathrm{term}}$ at node $N-1$. The driving-point (port) admittance at node $i=0$, denoted $Y_{\\mathrm{em}}(\\omega,\\boldsymbol{\\rho})$, is computed by solving\n$$\n\\mathbf{G}(\\omega,\\boldsymbol{\\rho}) \\mathbf{v} = \\mathbf{e}_0,\n$$\nwhere $\\mathbf{e}_0$ is the unit vector with $1$ at index $0$ and $0$ elsewhere, giving the port voltage $V_0 = \\left(\\mathbf{G}^{-1}\\mathbf{e}_0\\right)_0$. The driving-point admittance is\n$$\nY_{\\mathrm{em}}(\\omega,\\boldsymbol{\\rho}) = \\frac{1}{V_0}.\n$$\n\nThe rectifier is modeled as a single nonlinear diode connected at the port node to ground, with current–voltage law\n$$\ni_{\\mathrm{d}}(t) = I_{\\mathrm{s}} \\left( \\exp\\!\\left(\\frac{v(t)}{V_{\\mathrm{t}}}\\right) - 1 \\right),\n$$\nwhere $I_{\\mathrm{s}}$ is the reverse-saturation current and $V_{\\mathrm{t}}$ is the thermal voltage. The alternating-current (AC) source is a Thevenin source at fundamental angular frequency $\\omega$, with series impedance $Z_{\\mathrm{s}}$ and sinusoidal voltage of complex phasor amplitude $V_{\\mathrm{s}}$ at the fundamental only. In Norton form, this is an AC Norton current $I_{\\mathrm{N},1} = V_{\\mathrm{s}}/Z_{\\mathrm{s}}$ in parallel with source admittance $Y_{\\mathrm{s}} = 1/Z_{\\mathrm{s}}$ at all harmonics $k \\ge 1$. The rectified direct-current (DC) is extracted through an ideal low-pass path into a DC load of resistance $R_{\\mathrm{L}}$ attached at the port node. We assume the electromagnetic network does not conduct at $k=0$, so $Y_{\\mathrm{em}}(0,\\boldsymbol{\\rho}) = 0$.\n\nTo incorporate nonlinear rectification and harmonic generation, use harmonic balance with truncated harmonics $k \\in \\{0,1,2\\}$, where the port voltage is represented as a real waveform\n$$\nv(t) = V_0 + \\operatorname{Re}\\!\\left( V_1 e^{j\\omega t} + V_2 e^{j 2\\omega t} \\right),\n$$\nwith $V_0 \\in \\mathbb{R}$ and $V_1,V_2 \\in \\mathbb{C}$. The diode current $i_{\\mathrm{d}}(t)$ is sampled uniformly over one period and transformed to Fourier coefficients $\\{I_0, I_1, I_2\\}$ via the discrete-time Fourier transform. The harmonic-balance KCL equations at the port, for $k \\in \\{0,1,2\\}$, are\n$$\n\\begin{aligned}\n&k=0: && G_{\\mathrm{L}} V_0 + I_0(V_0,V_1,V_2) = 0, \\quad G_{\\mathrm{L}} = \\frac{1}{R_{\\mathrm{L}}}, \\\\\n&k=1: && \\left( Y_{\\mathrm{s}} + Y_{\\mathrm{em}}(\\omega,\\boldsymbol{\\rho}) \\right) V_1 + I_1(V_0,V_1,V_2) - I_{\\mathrm{N},1} = 0, \\\\\n&k=2: && \\left( Y_{\\mathrm{s}} + Y_{\\mathrm{em}}(2\\omega,\\boldsymbol{\\rho}) \\right) V_2 + I_2(V_0,V_1,V_2) = 0.\n\\end{aligned}\n$$\nThese define a nonlinear residual $\\mathbf{F}(\\mathbf{x},\\boldsymbol{\\rho}) = \\mathbf{0}$, where $\\mathbf{x} = [V_0, \\operatorname{Re}V_1, \\operatorname{Im}V_1, \\operatorname{Re}V_2, \\operatorname{Im}V_2]^T \\in \\mathbb{R}^5$. You must solve this system using Newton’s method with line search, forming the Jacobian $\\partial \\mathbf{F}/\\partial \\mathbf{x}$ analytically by differentiating through the discrete Fourier transform and the diode nonlinearity.\n\nThe co-design objective is the DC output voltage at the port,\n$$\nJ(\\mathbf{x}) = V_0,\n$$\nwhich you must maximize with respect to $\\boldsymbol{\\rho} \\in [0,1]^N$ using a small number of gradient-ascent iterations. Compute the total derivative $\\mathrm{d}J/\\mathrm{d}\\boldsymbol{\\rho}$ via a composite adjoint that differentiates through the harmonic-balance system and the electromagnetic admittance. Specifically, let $\\mathbf{J}_x = \\partial \\mathbf{F}/\\partial \\mathbf{x}$ and $\\mathbf{J}_\\rho = \\partial \\mathbf{F}/\\partial \\boldsymbol{\\rho}$. The adjoint vector $\\boldsymbol{\\lambda} \\in \\mathbb{R}^5$ solves\n$$\n\\mathbf{J}_x^T \\boldsymbol{\\lambda} = \\left( \\frac{\\partial J}{\\partial \\mathbf{x}} \\right)^T,\n$$\nand the gradient is\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}\\boldsymbol{\\rho}} = - \\boldsymbol{\\lambda}^T \\mathbf{J}_\\rho.\n$$\nThe only $\\boldsymbol{\\rho}$-dependence in $\\mathbf{F}$ is through $Y_{\\mathrm{em}}(k\\omega,\\boldsymbol{\\rho})$ for $k \\in \\{1,2\\}$. Using the identity for the derivative of a driving-point admittance with respect to real parameters (obtained by differentiating $\\mathbf{G}^{-1}$),\n$$\n\\frac{\\partial Y_{\\mathrm{em}}}{\\partial \\rho_i}(k\\omega) = \\frac{\\mathbf{a}^T \\left( \\frac{\\partial \\mathbf{G}}{\\partial \\rho_i} \\right) \\mathbf{a}}{\\left( \\mathbf{a}_0 \\right)^2}, \\quad \\mathbf{a} = \\mathbf{G}(k\\omega,\\boldsymbol{\\rho})^{-1} \\mathbf{e}_0, \\quad \\mathbf{a}_0 = \\left( \\mathbf{G}^{-1} \\mathbf{e}_0 \\right)_0,\n$$\nand the fact that $\\partial \\mathbf{G}/\\partial \\rho_i$ arises only from the shunt capacitance at node $i$, with\n$$\n\\frac{\\partial \\mathbf{G}}{\\partial \\rho_i} = j k \\omega \\frac{\\partial C_i}{\\partial \\rho_i} \\mathbf{E}_{ii}, \\quad \\frac{\\partial C_i}{\\partial \\rho_i} = p \\,\\rho_i^{p-1} \\left( C_{\\max} - C_{\\min} \\right),\n$$\nwhere $\\mathbf{E}_{ii}$ is the matrix with a $1$ at position $(i,i)$ and $0$ elsewhere.\n\nTo assemble the scalar gradient efficiently, work directly in the harmonic-balance complex form. Let the adjoint components for harmonics $k=1,2$ be $\\lambda_k = \\lambda_{k,\\mathrm{re}} + j \\lambda_{k,\\mathrm{im}}$, collected from the corresponding rows of $\\boldsymbol{\\lambda}$, and let $V_k$ be the complex port voltage at harmonic $k$. The contribution from the electromagnetic admittance to the scalar product $-\\boldsymbol{\\lambda}^T \\mathbf{J}_\\rho$ equals\n$$\n-\\sum_{k=1}^2 \\operatorname{Re}\\!\\left\\{ \\overline{\\lambda_k} \\, V_k \\, \\frac{\\partial Y_{\\mathrm{em}}(k\\omega,\\boldsymbol{\\rho})}{\\partial \\rho_i} \\right\\},\n$$\nfor each design component $i$, where the overbar denotes complex conjugation. Substituting the expression for $\\partial Y_{\\mathrm{em}}/\\partial \\rho_i$ yields an explicit, efficiently computable gradient.\n\nImplementation requirements:\n- Solve the harmonic-balance equations using Newton’s method with a backtracking line search. Use at least $M$ uniform time samples per period, with phases $\\phi_m = 2\\pi m/M$ for $m \\in \\{0,\\dots,M-1\\}$, to evaluate the diode nonlinearity and its discrete Fourier transform. Angles must be in radians.\n- Use the exact Jacobian with respect to the real variables $(V_0, \\operatorname{Re}V_1, \\operatorname{Im}V_1, \\operatorname{Re}V_2, \\operatorname{Im}V_2)$, formed by combining the linear complex-admittance mapping with the Fourier-linearized diode contributions.\n- Use the composite adjoint to compute $\\mathrm{d}J/\\mathrm{d}\\boldsymbol{\\rho}$, and perform projected gradient ascent on $\\boldsymbol{\\rho}$ with simple bound constraints $\\rho_i \\in [0,1]$. Normalize the ascent direction by its infinity norm before stepping, and clip the updated $\\boldsymbol{\\rho}$ to the feasible interval.\n- For each test case, initialize $\\rho_i = 0.5$ for all $i$, perform the specified number of design iterations, and report the final optimized direct-current voltage $V_{\\mathrm{dc}} = V_0$.\n\nPhysical units:\n- Inductance in henry (H), capacitance in farad (F), resistances in ohm ($\\Omega$), frequency in hertz (Hz), voltages in volt (V), currents in ampere (A). Express the final $V_{\\mathrm{dc}}$ in volt (V), rounded to six decimal places.\n\nTest suite:\nEach test case is specified as $(N, f, V_{\\mathrm{s}}, Z_{\\mathrm{s}}, R_{\\mathrm{L}}, R_{\\mathrm{term}}, L, C_{\\min}, C_{\\max}, p, M, \\text{iters})$.\n\nProvide results for the following three cases:\n- Case 1 (happy path): $N=6$, $f=2.0\\times 10^9\\,\\mathrm{Hz}$, $V_{\\mathrm{s}}=1.0\\,\\mathrm{V}$, $Z_{\\mathrm{s}}=50\\,\\Omega$, $R_{\\mathrm{L}}=10{,}000\\,\\Omega$, $R_{\\mathrm{term}}=50\\,\\Omega$, $L=1.5\\times 10^{-9}\\,\\mathrm{H}$, $C_{\\min}=0.05\\times 10^{-12}\\,\\mathrm{F}$, $C_{\\max}=2.0\\times 10^{-12}\\,\\mathrm{F}$, $p=3$, $M=64$, $\\text{iters}=12$.\n- Case 2 (near-linear excitation): $N=5$, $f=1.5\\times 10^9\\,\\mathrm{Hz}$, $V_{\\mathrm{s}}=0.5\\,\\mathrm{V}$, $Z_{\\mathrm{s}}=50\\,\\Omega$, $R_{\\mathrm{L}}=10{,}000\\,\\Omega$, $R_{\\mathrm{term}}=75\\,\\Omega$, $L=1.0\\times 10^{-9}\\,\\mathrm{H}$, $C_{\\min}=0.05\\times 10^{-12}\\,\\mathrm{F}$, $C_{\\max}=2.0\\times 10^{-12}\\,\\mathrm{F}$, $p=3$, $M=64$, $\\text{iters}=12$.\n- Case 3 (higher frequency, different source impedance): $N=8$, $f=3.0\\times 10^9\\,\\mathrm{Hz}$, $V_{\\mathrm{s}}=0.8\\,\\mathrm{V}$, $Z_{\\mathrm{s}}=75\\,\\Omega$, $R_{\\mathrm{L}}=10{,}000\\,\\Omega$, $R_{\\mathrm{term}}=50\\,\\Omega$, $L=1.2\\times 10^{-9}\\,\\mathrm{H}$, $C_{\\min}=0.05\\times 10^{-12}\\,\\mathrm{F}$, $C_{\\max}=2.0\\times 10^{-12}\\,\\mathrm{F}$, $p=3$, $M=64$, $\\text{iters}=12$.\n\nNonlinear diode parameters are fixed across cases: $I_{\\mathrm{s}}=1.0\\times 10^{-12}\\,\\mathrm{A}$, $V_{\\mathrm{t}}=0.02585\\,\\mathrm{V}$.\n\nFinal output format:\nYour program should produce a single line of output containing the final optimized direct-current voltages as a comma-separated list of floats rounded to six decimal places, enclosed in square brackets, in the order of the test cases. For example: \"[0.123456,0.234567,0.345678]\".",
            "solution": "The user has provided a well-posed and scientifically grounded problem in the field of computational electromagnetics and nonlinear circuit co-design. The problem asks for the gradient-based topology optimization of a simplified radio-frequency rectifying antenna (rectenna). The system consists of a linear one-dimensional electromagnetic ladder network coupled to a nonlinear diode rectifier. The objective is to maximize the direct-current (DC) output voltage. The problem is valid as it is self-contained, physically realistic, and mathematically well-defined. All required parameters, equations, and algorithmic steps are specified.\n\nThe solution proceeds by implementing a projected gradient ascent algorithm. For each test case, the design variables $\\boldsymbol{\\rho}$, representing material densities, are initialized to $0.5$. The optimization consists of a specified number of iterations, where each iteration involves three main steps:\n\n1.  **Forward Analysis**: The core of the problem is a nonlinear system of equations derived from Harmonic Balance (HB) theory, coupling the electromagnetic network's response with the diode's nonlinear behavior. The state vector $\\mathbf{x} = [V_0, \\operatorname{Re}V_1, \\operatorname{Im}V_1, \\operatorname{Re}V_2, \\operatorname{Im}V_2]^T$ represents the DC voltage ($V_0$) and the complex phasors of the first two harmonics ($V_1, V_2$) at the device port. This $5 \\times 5$ nonlinear system, denoted $\\mathbf{F}(\\mathbf{x}, \\boldsymbol{\\rho})=\\mathbf{0}$, is solved for $\\mathbf{x}$ using Newton's method with a backtracking line search for robustness.\n    -   The electromagnetic network's frequency-dependent driving-point admittance, $Y_{\\mathrm{em}}(k\\omega, \\boldsymbol{\\rho})$, is required at harmonics $k=1,2$. This is computed by constructing the $N \\times N$ nodal admittance matrix $\\mathbf{G}(k\\omega, \\boldsymbol{\\rho})$ and solving a linear system $\\mathbf{G}\\mathbf{v}=\\mathbf{e}_0$.\n    -   The nonlinear diode current is handled in the time domain. The port voltage waveform $v(t)$ is constructed from $\\mathbf{x}$, and the corresponding diode current $i_{\\mathrm{d}}(t)$ is computed using the provided exponential law. The discrete Fourier transform (DFT) is used to find the harmonic components of $i_{\\mathrm{d}}(t)$ needed for the HB residual equations.\n    -   The Jacobian of the HB system, $\\mathbf{J}_x = \\partial\\mathbf{F}/\\partial\\mathbf{x}$, is computed analytically. This involves the derivatives of the linear network admittances and the derivatives of the diode's harmonic currents with respect to the voltage harmonics. The latter is a convolution in the frequency domain, which is equivalent to the Fourier transform of the product of the time-domain diode transconductance and the basis functions of the voltage harmonics.\n\n2.  **Adjoint Sensitivity Analysis**: The objective function is the DC voltage, $J(\\mathbf{x}) = V_0$. To compute its gradient with respect to the design variables, $\\mathrm{d}J/\\mathrm{d}\\boldsymbol{\\rho}$, a composite adjoint method is employed. This avoids the costly direct computation of $\\mathrm{d}\\mathbf{x}/\\mathrm{d}\\boldsymbol{\\rho}$. First, a real-valued adjoint vector $\\boldsymbol{\\lambda} \\in \\mathbb{R}^5$ is found by solving the linear system $\\mathbf{J}_x^T \\boldsymbol{\\lambda} = (\\partial J/\\partial \\mathbf{x})^T = [1, 0, 0, 0, 0]^T$. The gradient is then given by the chain rule expression $\\mathrm{d}J/\\mathrm{d}\\boldsymbol{\\rho} = -\\boldsymbol{\\lambda}^T (\\partial \\mathbf{F}/\\partial \\boldsymbol{\\rho})$.\n    -   The term $\\partial \\mathbf{F}/\\partial \\boldsymbol{\\rho}$ represents the change in the HB residual due to a change in the design variables. Since $\\boldsymbol{\\rho}$ only affects the shunt capacitances, this derivative is non-zero only through the $Y_{\\mathrm{em}}(k\\omega, \\boldsymbol{\\rho})$ terms.\n    -   The derivative of the driving-point admittance, $\\partial Y_{\\mathrm{em}}/\\partial \\rho_i$, is computed using a standard adjoint identity for linear networks. This requires the voltage vector from the forward EM solve.\n    -   The scalar gradient components $\\mathrm{d}J/\\mathrm{d}\\rho_i$ are then assembled efficiently using the complex-valued adjoint variables $\\lambda_k$ (reconstructed from $\\boldsymbol{\\lambda}$) and the solved harmonic voltages $V_k$, as specified in the problem statement.\n\n3.  **Gradient Ascent Update**: With the gradient $\\mathrm{d}J/\\mathrm{d}\\boldsymbol{\\rho}$ computed, the design variables are updated. The gradient is normalized by its infinity norm to create a uniform ascent direction. The updated design variables are then projected (clipped) back into the feasible range $[0, 1]$.\n\nAfter the specified number of iterations, a final forward analysis is performed on the optimized design $\\boldsymbol{\\rho}$ to determine the final DC output voltage, which is reported as the result. The entire process is encapsulated in a Python program adhering to the specified library constraints.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom numpy.linalg import solve, norm\nimport sys\n\n# Suppress potential overflow warnings from the exponential in the diode model,\n# which are handled by clipping.\nnp.seterr(over='ignore', invalid='ignore')\n\ndef solve_problem():\n    \"\"\"\n    Main function to run the optimization for all test cases and print results.\n    \"\"\"\n    # Diode parameters (fixed)\n    IS = 1.0e-12  # A\n    VT = 0.02585 # V\n\n    test_cases = [\n        # Case 1: (N, f, Vs, Zs, RL, Rterm, L, Cmin, Cmax, p, M, iters)\n        (6, 2.0e9, 1.0, 50.0, 10000.0, 50.0, 1.5e-9, 0.05e-12, 2.0e-12, 3.0, 64, 12),\n        # Case 2\n        (5, 1.5e9, 0.5, 50.0, 10000.0, 75.0, 1.0e-9, 0.05e-12, 2.0e-12, 3.0, 64, 12),\n        # Case 3\n        (8, 3.0e9, 0.8, 75.0, 10000.0, 50.0, 1.2e-9, 0.05e-12, 2.0e-12, 3.0, 64, 12),\n    ]\n\n    results = []\n    for params in test_cases:\n        optimizer = RectennaOptimizer(params, IS, VT)\n        final_vdc = optimizer.optimize()\n        results.append(f\"{final_vdc:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nclass RectennaOptimizer:\n    \"\"\"\n    Implements the co-design optimization of a rectenna.\n    \"\"\"\n    def __init__(self, params, Is, Vt):\n        \"\"\"Initializes the optimizer with problem parameters.\"\"\"\n        (self.N, self.f, self.Vs, self.Zs, self.RL, self.Rterm, self.L,\n         self.Cmin, self.Cmax, self.p, self.M, self.iters) = params\n        self.Is = Is\n        self.Vt = Vt\n\n        self.omega = 2 * np.pi * self.f\n        self.Ys = 1.0 / self.Zs\n        self.IN1 = self.Vs / self.Zs\n        self.GL = 1.0 / self.RL\n\n    def _C(self, rho):\n        \"\"\"Computes capacitance using the SIMP model.\"\"\"\n        return self.Cmin + rho**self.p * (self.Cmax - self.Cmin)\n\n    def _dC_drho(self, rho):\n        \"\"\"Computes the derivative of capacitance w.r.t. rho.\"\"\"\n        # Handle rho=0 case for p<1, though p>=1 here.\n        rho_safe = np.maximum(rho, 1e-12)\n        return self.p * rho_safe**(self.p - 1) * (self.Cmax - self.Cmin)\n\n    def _build_G(self, freq, rho):\n        \"\"\"Assembles the N x N complex nodal admittance matrix G.\"\"\"\n        w = 2 * np.pi * freq\n        if w < 1e-9:\n             return np.diag(np.full(self.N, 1e-12)) # Avoid singular matrix for DC\n\n        yc = 1j * w * self._C(rho)\n        diag_terms = np.copy(yc)\n        \n        if self.N == 1:\n            diag_terms[0] += 1.0 / self.Rterm\n            G = np.diag(diag_terms)\n        else:\n            yl = 1.0 / (1j * w * self.L)\n            diag_terms[0] += yl\n            diag_terms[-1] += yl + (1.0 / self.Rterm)\n            diag_terms[1:-1] += 2 * yl\n            G = np.diag(diag_terms)\n            off_diag = np.full(self.N - 1, -yl)\n            G += np.diag(off_diag, k=1)\n            G += np.diag(off_diag, k=-1)\n        return G\n\n    def _solve_em(self, freq, rho):\n        \"\"\"Computes driving-point admittance and EM voltage vector 'a'.\"\"\"\n        G = self._build_G(freq, rho)\n        e0 = np.zeros(self.N, dtype=complex)\n        e0[0] = 1.0\n        try:\n            # Add small regularization to handle potential resonances\n            G_reg = G + np.eye(self.N) * 1e-12\n            a = solve(G_reg, e0)\n        except np.linalg.LinAlgError:\n            return np.inf, None\n        if abs(a[0]) < 1e-12: return np.inf, None\n        Yem = 1.0 / a[0]\n        return Yem, a\n\n    def _hb_system(self, x, rho, Yem1, Yem2):\n        \"\"\"Computes the HB residual F and Jacobian Jx.\"\"\"\n        V0, V1r, V1i, V2r, V2i = x\n        V1 = V1r + 1j * V1i\n        V2 = V2r + 1j * V2i\n\n        phi = 2 * np.pi * np.arange(self.M) / self.M\n        vt = V0 + np.real(V1 * np.exp(1j * phi) + V2 * np.exp(2j * phi))\n        \n        vt_clipped = np.clip(vt, -np.inf, 20 * self.Vt)\n        exp_term = np.exp(vt_clipped / self.Vt)\n        it = self.Is * (exp_term - 1)\n        gt = (self.Is / self.Vt) * exp_term\n\n        I_fft = np.fft.fft(it) / self.M\n        I0 = I_fft[0].real\n        I1 = 2 * I_fft[1] if self.M > 1 else 0.0\n        I2 = 2 * I_fft[2] if self.M > 2 else 0.0\n\n        F = np.zeros(5)\n        F[0] = self.GL * V0 + I0\n        Res1 = (self.Ys + Yem1) * V1 + I1 - self.IN1\n        F[1], F[2] = Res1.real, Res1.imag\n        Res2 = (self.Ys + Yem2) * V2 + I2\n        F[3], F[4] = Res2.real, Res2.imag\n        \n        J = np.zeros((5, 5))\n        J[0, 0] = self.GL\n        Y1_eff = self.Ys + Yem1\n        J[1:3, 1:3] = [[Y1_eff.real, -Y1_eff.imag], [Y1_eff.imag, Y1_eff.real]]\n        Y2_eff = self.Ys + Yem2\n        J[3:5, 3:5] = [[Y2_eff.real, -Y2_eff.imag], [Y2_eff.imag, Y2_eff.real]]\n\n        s_basis = [\n            np.ones(self.M), np.cos(phi), -np.sin(phi),\n            np.cos(2*phi), -np.sin(2*phi)\n        ]\n\n        for j in range(5):\n            d_it = gt * s_basis[j]\n            dI_fft = np.fft.fft(d_it) / self.M\n            J[0, j] += dI_fft[0].real\n            if self.M > 1:\n                dI1 = 2 * dI_fft[1]\n                J[1, j] += dI1.real\n                J[2, j] += dI1.imag\n            if self.M > 2:\n                dI2 = 2 * dI_fft[2]\n                J[3, j] += dI2.real\n                J[4, j] += dI2.imag\n        \n        return F, J\n\n    def solve_hb(self, rho, x_init=None):\n        \"\"\"Solves the HB nonlinear system using Newton's method.\"\"\"\n        x = np.zeros(5) if x_init is None else np.copy(x_init)\n        \n        Yem1, _ = self._solve_em(self.f, rho)\n        Yem2, _ = self._solve_em(2 * self.f, rho)\n        if np.isinf(Yem1) or np.isinf(Yem2):\n            return None, False\n\n        for _ in range(25):\n            F, J = self._hb_system(x, rho, Yem1, Yem2)\n            if norm(F) < 1e-10:\n                return x, True\n            try:\n                delta_x = solve(J, -F)\n            except np.linalg.LinAlgError:\n                return x, False\n\n            alpha = 1.0\n            norm_F = norm(F)\n            for _ in range(10):\n                x_new = x + alpha * delta_x\n                F_new, _ = self._hb_system(x_new, rho, Yem1, Yem2)\n                if norm(F_new) < norm_F:\n                    break\n                alpha /= 2.0\n            else:\n                return x, norm(F) < 1e-8\n\n            x = x_new\n            if norm(alpha * delta_x) < 1e-10:\n                return x, True\n\n        return x, norm(F) < 1e-8\n\n    def _calculate_gradient(self, x, rho, Yem1, a1, Yem2, a2):\n        \"\"\"Computes the gradient dJ/d(rho) using the adjoint method.\"\"\"\n        V0, V1r, V1i, V2r, V2i = x\n        V1 = V1r + 1j * V1i\n        V2 = V2r + 1j * V2i\n        \n        _, Jx = self._hb_system(x, rho, Yem1, Yem2)\n        rhs_adj = np.array([1.0, 0, 0, 0, 0])\n        try:\n            lmbda = solve(Jx.T, rhs_adj)\n        except np.linalg.LinAlgError:\n            return np.zeros_like(rho)\n\n        lmbda_cplx = [lmbda[1] + 1j * lmbda[2], lmbda[3] + 1j * lmbda[4]]\n        dC_drho_vals = self._dC_drho(rho)\n        grad = np.zeros(self.N)\n\n        for i in range(self.N):\n            dC_dr_i = dC_drho_vals[i]\n            if abs(dC_dr_i) < 1e-20: continue\n\n            w1 = 2 * np.pi * self.f\n            dG_drho_i_1 = 1j * w1 * dC_dr_i\n            dYem_drho_i_1 = (a1[i]**2 / a1[0]**2) * dG_drho_i_1\n            grad_term1 = np.real(np.conj(lmbda_cplx[0]) * V1 * dYem_drho_i_1)\n            \n            w2 = 2 * np.pi * 2 * self.f\n            dG_drho_i_2 = 1j * w2 * dC_dr_i\n            dYem_drho_i_2 = (a2[i]**2 / a2[0]**2) * dG_drho_i_2\n            grad_term2 = np.real(np.conj(lmbda_cplx[1]) * V2 * dYem_drho_i_2)\n            \n            grad[i] = -(grad_term1 + grad_term2)\n\n        return grad\n\n    def optimize(self):\n        \"\"\"Runs the gradient ascent optimization loop.\"\"\"\n        rho = np.full(self.N, 0.5)\n        x_sol = None\n        step_size = 0.1\n        direction = np.zeros(self.N)\n\n        for _ in range(self.iters):\n            Yem1, a1 = self._solve_em(self.f, rho)\n            Yem2, a2 = self._solve_em(2 * self.f, rho)\n\n            if np.isinf(Yem1) or np.isinf(Yem2) or a1 is None or a2 is None:\n                rho = np.clip(rho - step_size * direction, 0.0, 1.0)\n                continue\n                \n            x_sol, success = self.solve_hb(rho, x_init=x_sol)\n            if not success:\n                break\n\n            grad = self._calculate_gradient(x_sol, rho, Yem1, a1, Yem2, a2)\n            \n            grad_norm_inf = norm(grad, ord=np.inf)\n            if grad_norm_inf > 1e-12:\n                direction = grad / grad_norm_inf\n            else:\n                break\n\n            rho = rho + step_size * direction\n            rho = np.clip(rho, 0.0, 1.0)\n\n        final_x, success = self.solve_hb(rho, x_init=x_sol)\n        return final_x[0] if success and final_x is not None else (x_sol[0] if x_sol is not None else 0.0)\n\nif __name__ == '__main__':\n    solve_problem()\n```"
        }
    ]
}