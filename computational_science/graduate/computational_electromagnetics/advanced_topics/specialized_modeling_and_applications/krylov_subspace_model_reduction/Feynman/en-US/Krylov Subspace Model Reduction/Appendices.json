{
    "hands_on_practices": [
        {
            "introduction": "A critical question in any approximation method is how to quantify the accuracy of the result without knowing the exact answer. This practice provides the theoretical foundation for answering that question in model reduction. You will derive a fundamental *a posteriori* error estimator, establishing the crucial link between the computable Petrov-Galerkin residual and the actual error in the transfer function, a concept that underpins many advanced adaptive algorithms. ",
            "id": "3322125",
            "problem": "Consider a semi-discrete formulation of Maxwell’s equations obtained, for example, by a curl-conforming finite element method or finite integration technique, which yields a linear time-invariant (LTI) descriptor system of the form $E \\dot{x}(t) = A x(t) + B u(t)$ with output $y(t) = C x(t)$, where $E \\in \\mathbb{C}^{n \\times n}$ is typically Hermitian positive semidefinite, $A \\in \\mathbb{C}^{n \\times n}$ arises from material and topological operators, and $B \\in \\mathbb{C}^{n \\times m}$, $C \\in \\mathbb{C}^{p \\times n}$ are input and output matrices consistent with the chosen ports or probes. The frequency-domain transfer function is $H(s) = C (s E - A)^{-1} B$, with $s \\in \\mathbb{C}$ in the open right half-plane ensuring well-posedness. A Petrov–Galerkin reduced-order model is constructed by projection with trial basis $V \\in \\mathbb{C}^{n \\times r}$ and test basis $W \\in \\mathbb{C}^{n \\times r}$ such that $W^{*} E V$ is nonsingular, leading to the reduced transfer function $\\hat{H}(s) = C V \\left(s W^{*} E V - W^{*} A V\\right)^{-1} W^{*} B$.\n\nFix a frequency $s \\in \\mathbb{C}$ such that $(s E - A)$ is invertible. For a given input $u \\in \\mathbb{C}^{m}$, define $x(s) \\in \\mathbb{C}^{n}$ by $(s E - A) x(s) = B u$ and the reduced state $\\hat{x}(s) = V \\hat{z}(s)$ where $\\hat{z}(s) \\in \\mathbb{C}^{r}$ satisfies the Petrov–Galerkin condition $W^{*} \\left((s E - A) V \\hat{z}(s) - B u\\right) = 0$. Define the Petrov–Galerkin residual at frequency $s$ associated with $u$ by $r_{u}(s) = B u - (s E - A) \\hat{x}(s)$. Assume the Euclidean vector norm and the induced spectral operator $2$-norm throughout.\n\nStarting only from the definitions above and standard norm inequalities, derive an a posteriori error estimator at the fixed frequency $s$ for the induced $2$-norm of the transfer function error $\\|H(s) - \\hat{H}(s)\\|_{2}$ that depends explicitly on the Petrov–Galerkin residual and the operator norm of $(s E - A)^{-1}$. Your derivation must begin from the error in the state and output implied by the residual and arrive at a bound of the form 'norm of a resolvent-like factor times a computable residual magnitude.' Then, discuss how to obtain computable bounds on the resolvent factor by local solves at the frequency $s$ without forming $(s E - A)^{-1}$ explicitly, and how to bound the worst-case residual over inputs using only projected quantities. Conclude by stating your final estimator as a single closed-form expression in terms of the Petrov–Galerkin residual and the resolvent factor. No numerical evaluation is required; provide the final estimator expression symbolically as your answer.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of model order reduction for linear time-invariant systems, well-posed, objective, and self-contained. We proceed with the derivation.\n\nOur goal is to derive an a posteriori error estimator for the induced $2$-norm of the transfer function error, $\\|H(s) - \\hat{H}(s)\\|_{2}$, at a fixed frequency $s \\in \\mathbb{C}$. The derivation will commence from the error in the state variable.\n\nLet $x(s) \\in \\mathbb{C}^{n}$ be the state of the full system and $\\hat{x}(s) = V \\hat{z}(s) \\in \\mathbb{C}^{n}$ be the approximate state from the reduced-order model for a given input $u \\in \\mathbb{C}^{m}$ at frequency $s$. The state error is defined as $e_x(s) = x(s) - \\hat{x}(s)$.\n\nFrom the problem statement, the full system state satisfies the frequency-domain equation:\n$$ (s E - A) x(s) = B u $$\nThe Petrov-Galerkin residual $r_{u}(s)$ is defined by:\n$$ r_{u}(s) = B u - (s E - A) \\hat{x}(s) $$\nThis can be rearranged to express the action of the system matrix on the approximate state:\n$$ (s E - A) \\hat{x}(s) = B u - r_{u}(s) $$\nSubtracting this equation from the full system equation, we obtain an expression for the state error:\n$$ (s E - A) x(s) - (s E - A) \\hat{x}(s) = B u - (B u - r_{u}(s)) $$\n$$ (s E - A) (x(s) - \\hat{x}(s)) = r_{u}(s) $$\n$$ (s E - A) e_x(s) = r_{u}(s) $$\nSince the problem states that $(s E - A)$ is invertible for the chosen $s$, we can formally write the state error in terms of the residual:\n$$ e_x(s) = (s E - A)^{-1} r_{u}(s) $$\nThis equation provides a direct link between the state estimation error and the computable residual.\n\nNext, we relate the state error to the error in the output. The output of the full system is $y(s) = C x(s)$, and the output corresponding to the approximate state is $\\hat{y}(s) = C \\hat{x}(s)$. The output error is $e_y(s) = y(s) - \\hat{y}(s)$.\n$$ e_y(s) = C x(s) - C \\hat{x}(s) = C(x(s) - \\hat{x}(s)) = C e_x(s) $$\nSubstituting the expression for $e_x(s)$, we get:\n$$ e_y(s) = C (s E - A)^{-1} r_{u}(s) $$\nThe output error can also be expressed in terms of the transfer function error. By definition, $y(s) = H(s) u$ and $\\hat{y}(s) = \\hat{H}(s) u$. Therefore:\n$$ e_y(s) = (H(s) - \\hat{H}(s)) u $$\nCombining these two expressions for the output error gives the exact error representation:\n$$ (H(s) - \\hat{H}(s)) u = C (s E - A)^{-1} r_{u}(s) $$\nTo find a bound on the norm of the transfer function error, $\\|H(s) - \\hat{H}(s)\\|_{2}$, we take the Euclidean vector norm ($2$-norm) of both sides.\n$$ \\|(H(s) - \\hat{H}(s)) u\\|_{2} = \\|C (s E - A)^{-1} r_{u}(s)\\|_{2} $$\nUsing the property of induced operator norms, $\\|M v\\|_{2} \\le \\|M\\|_{2} \\|v\\|_{2}$, we can establish an inequality:\n$$ \\|(H(s) - \\hat{H}(s)) u\\|_{2} \\le \\|C (s E - A)^{-1}\\|_{2} \\|r_{u}(s)\\|_{2} $$\nA further application of the submultiplicative property of norms, $\\|MN\\|_{2} \\le \\|M\\|_{2} \\|N\\|_{2}$, yields:\n$$ \\|(H(s) - \\hat{H}(s)) u\\|_{2} \\le \\|C\\|_{2} \\|(s E - A)^{-1}\\|_{2} \\|r_{u}(s)\\|_{2} $$\nThe induced $2$-norm of the transfer function error matrix is defined as the maximum amplification of the output error norm over all unit-norm inputs:\n$$ \\|H(s) - \\hat{H}(s)\\|_{2} = \\sup_{\\|u\\|_{2}=1} \\|(H(s) - \\hat{H}(s)) u\\|_{2} $$\nApplying this definition to our inequality, we obtain the error bound:\n$$ \\|H(s) - \\hat{H}(s)\\|_{2} \\le \\sup_{\\|u\\|_{2}=1} \\left( \\|C\\|_{2} \\|(s E - A)^{-1}\\|_{2} \\|r_{u}(s)\\|_{2} \\right) $$\nSince $\\|C\\|_{2}$ and $\\|(s E - A)^{-1}\\|_{2}$ are independent of the input $u$, we can write:\n$$ \\|H(s) - \\hat{H}(s)\\|_{2} \\le \\|C\\|_{2} \\|(s E - A)^{-1}\\|_{2} \\sup_{\\|u\\|_{2}=1} \\|r_{u}(s)\\|_{2} $$\nThis inequality provides an a posteriori error bound of the desired form: a \"resolvent-like factor,\" which is $\\|C\\|_{2} \\|(s E - A)^{-1}\\|_{2}$, multiplied by a \"computable residual magnitude,\" $\\sup_{\\|u\\|_{2}=1} \\|r_{u}(s)\\|_{2}$.\n\nThe problem then asks for a discussion on computing the factors in this bound.\n$1$. The resolvent factor, $\\|(s E - A)^{-1}\\|_{2}$, represents the largest amplification factor of the operator $(s E - A)^{-1}$. For large-scale systems, forming the inverse explicitly is computationally prohibitive. Instead, its norm can be estimated. The norm is equal to the reciprocal of the smallest singular value of $(s E - A)$, i.e., $\\|(s E - A)^{-1}\\|_{2} = 1/\\sigma_{\\min}(s E - A)$. This singular value can be estimated using iterative methods, such as the inverse power method or Lanczos/Arnoldi algorithms, applied to $(s E - A)^{*}(s E - A)$ or $(s E - A)(s E - A)^{*}$. Each iteration of these methods requires solving a linear system of the form $(s E - A)x=b$ or $(s E - A)^{*}x=b$, which are the \"local solves at the frequency $s$\" mentioned in the problem. These solves are performed with iterative methods like GMRES or BiCGSTAB, which only require matrix-vector products with the large, sparse matrices $E$ and $A$.\n\n$2$. The worst-case residual magnitude, $\\sup_{\\|u\\|_{2}=1} \\|r_{u}(s)\\|_{2}$, is the induced $2$-norm of the residual operator. To see this, we express the residual $r_u(s)$ as a linear operator acting on $u$. From the definitions, $\\hat{x}(s) = V \\hat{z}(s)$ and $\\hat{z}(s) = (s W^{*} E V - W^{*} A V)^{-1} W^{*} B u$. Substituting this into the definition of the residual:\n$$ r_{u}(s) = B u - (s E - A) V \\left(s W^{*} E V - W^{*} A V\\right)^{-1} W^{*} B u $$\n$$ r_{u}(s) = \\left(I - (s E - A) V \\left(s W^{*} E V - W^{*} A V\\right)^{-1} W^{*} B\\right) u $$\nThe worst-case residual is the norm of the $n \\times m$ matrix operator:\n$$ \\sup_{\\|u\\|_{2}=1} \\|r_{u}(s)\\|_{2} = \\left\\| I - (s E - A) V \\left(s W^{*} E V - W^{*} A V\\right)^{-1} W^{*} B \\right\\|_{2} $$\nComputing this norm requires forming this matrix. This involves operating with the large matrices $A$ and $E$ (specifically, forming $(sE-A)V = s(EV)-(AV)$) but also utilizes the \"projected quantities\" $W^*AV$, $W^*EV$, and $W^*B$ to form and invert the small $r \\times r$ matrix $(s W^{*} E V - W^{*} A V)$. Once the $n \\times m$ residual operator matrix is formed, its $2$-norm can be computed (e.g., via SVD), which is computationally expensive but provides a rigorous bound. For certain choices of projection bases $V$ and $W$ (e.g., from Krylov methods), this expression can be simplified significantly.\n\nFinally, the problem asks for the estimator as a single closed-form expression. The derived upper bound serves as the a posteriori error estimator. Denoting the worst-case residual norm over all unit-norm inputs as $\\mathcal{R}(s) = \\sup_{\\|u\\|_{2}=1} \\|r_u(s)\\|_2$, the estimator $\\Delta(s)$ is:\n$$ \\Delta(s) = \\|C\\|_{2} \\|(s E - A)^{-1}\\|_{2} \\mathcal{R}(s) $$\nThis expression clearly separates the estimator into a factor related to the full system's properties (resolvent) and a factor related to the quality of the approximation (residual).",
            "answer": "$$ \\boxed{\\|C\\|_{2} \\|(s E - A)^{-1}\\|_{2} \\sup_{\\|u\\|_{2}=1} \\|r_u(s)\\|_{2}} $$"
        },
        {
            "introduction": "While estimating the error at a single frequency is foundational, real-world electromagnetic systems must perform accurately over a wide band. This practice moves from theory to implementation by tasking you with designing and computing a wideband error indicator. By aggregating the norms of residuals at multiple frequencies using a numerical quadrature, you will create a practical tool for assessing the overall fidelity of a reduced model across a frequency range of interest. ",
            "id": "3322065",
            "problem": "Consider a frequency-domain formulation of Maxwell's equations discretized by the finite element method, producing a generalized linear time-invariant system with matrices $E \\in \\mathbb{R}^{n \\times n}$ and $A \\in \\mathbb{R}^{n \\times n}$, an input matrix $B \\in \\mathbb{R}^{n \\times m}$, and an output matrix $C \\in \\mathbb{R}^{p \\times n}$. For a complex frequency shift $s \\in \\mathbb{C}$, the full-order state $X(s) \\in \\mathbb{C}^{n \\times m}$ satisfies the linear system $$(sE - A) X(s) = B,$$ and the associated transfer function is $$H(s) = C X(s) = C (sE - A)^{-1} B.$$ In projection-based model reduction using a Krylov subspace, a column-orthonormal projection matrix $V \\in \\mathbb{R}^{n \\times r}$ defines reduced operators $$E_r = V^\\top E V, \\quad A_r = V^\\top A V, \\quad B_r = V^\\top B, \\quad C_r = C V.$$ The reduced-order approximation of the state is then $$\\widetilde{X}(s) = V (s E_r - A_r)^{-1} B_r,$$ and the corresponding reduced transfer function is $$H_r(s) = C_r (s E_r - A_r)^{-1} B_r.$$\n\nWideband accuracy over a frequency interval can be assessed by aggregating information at multiple frequency shifts $\\{ s_i \\}_{i=1}^q$ with nonnegative quadrature weights $\\{ w_i \\}_{i=1}^q$ chosen to approximate an integral over the band, in the spirit of the Hardy space $\\mathcal{H}_2$ norm on the imaginary axis. Your task is to design an error indicator that combines the residuals produced by the reduced approximation at these shifts, with the aggregation controlled by the quadrature weights, so that it acts as a physically meaningful and computationally tractable surrogate for an $\\mathcal{H}_2$-like norm over the band. The residual at a shift should be defined in terms of the original full-order operators and the reduced-order approximate state.\n\nBased on first principles, derive such an indicator and implement a program to compute it for the following test suite. Each test case provides $E$, $A$, $B$, $V$, a list of imaginary-axis shifts $s_i = \\jmath \\omega_i$ with $\\omega_i \\in \\mathbb{R}_{\\ge 0}$, and quadrature weights $w_i \\in \\mathbb{R}_{\\ge 0}$. The residual norm to be combined across shifts must be computed in a way that is consistent for multiple right-hand sides (multiple columns of $B$), and the aggregation across shifts must incorporate the given weights.\n\nYou must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets. Express each result as a floating-point number. No physical units are required in the output.\n\nUse the following test suite:\n\n- Test case 1 (happy path, multiple inputs):\n  - $n=4$, $m=2$, $r=2$.\n  - $$E = \\mathrm{diag}(2.0,\\,1.5,\\,1.0,\\,0.8).$$\n  - $$A = \\begin{bmatrix}\n  6.0 & -1.0 & 0.0 & 0.0 \\\\\n  -1.0 & 5.0 & -1.0 & 0.0 \\\\\n  0.0 & -1.0 & 4.5 & -0.8 \\\\\n  0.0 & 0.0 & -0.8 & 3.5\n  \\end{bmatrix}.$$\n  - $$B = \\begin{bmatrix}\n  1.0 & 0.5 \\\\\n  0.2 & -0.3 \\\\\n  0.0 & 0.1 \\\\\n  -0.1 & 0.2\n  \\end{bmatrix}.$$\n  - $$V = \\begin{bmatrix}\n  1.0 & 0.0 \\\\\n  0.0 & 1.0 \\\\\n  0.0 & 0.0 \\\\\n  0.0 & 0.0\n  \\end{bmatrix}.$$\n  - Shifts on the imaginary axis: $\\omega_1 = 1.0\\times 10^8$, $\\omega_2 = 2.0\\times 10^8$, $\\omega_3 = 3.0\\times 10^8$.\n  - Simpson's rule weights over the band $[1.0\\times 10^8,\\,3.0\\times 10^8]$: $w_1 = \\dfrac{2.0\\times 10^8}{6}$, $w_2 = \\dfrac{4\\cdot 2.0\\times 10^8}{6}$, $w_3 = \\dfrac{2.0\\times 10^8}{6}$.\n\n- Test case 2 (identity mass, zero weight edge):\n  - $n=4$, $m=1$, $r=1$.\n  - $E = I_4$.\n  - $$A = \\begin{bmatrix}\n  4.0 & -1.0 & 0.0 & 0.0 \\\\\n  -1.0 & 4.0 & -1.0 & 0.0 \\\\\n  0.0 & -1.0 & 3.5 & -1.0 \\\\\n  0.0 & 0.0 & -1.0 & 3.0\n  \\end{bmatrix}.$$\n  - $$B = \\begin{bmatrix}\n  1.0 \\\\\n  0.0 \\\\\n  0.5 \\\\\n  -0.2\n  \\end{bmatrix}.$$\n  - $$V = \\frac{1}{2} \\begin{bmatrix}\n  1.0 \\\\\n  1.0 \\\\\n  1.0 \\\\\n  1.0\n  \\end{bmatrix}.$$\n  - Shifts: $\\omega_1 = 5.0\\times 10^7$, $\\omega_2 = 1.5\\times 10^8$.\n  - Weights: $w_1 = 1.0$, $w_2 = 0.0$.\n\n- Test case 3 (near-resonant band, single input):\n  - $n=3$, $m=1$, $r=1$.\n  - $$E = \\mathrm{diag}(1.0,\\,0.8,\\,0.6).$$\n  - $$A = \\begin{bmatrix}\n  2.0 & -0.9 & 0.0 \\\\\n  -0.9 & 2.2 & -0.8 \\\\\n  0.0 & -0.8 & 1.8\n  \\end{bmatrix}.$$\n  - $$B = \\begin{bmatrix}\n  0.3 \\\\\n  0.1 \\\\\n  0.2\n  \\end{bmatrix}.$$\n  - $$V = \\begin{bmatrix}\n  1.0 \\\\\n  0.0 \\\\\n  0.0\n  \\end{bmatrix}.$$\n  - Shifts: $\\omega_1 = 1.0\\times 10^7$, $\\omega_2 = 7.0\\times 10^7$, $\\omega_3 = 1.3\\times 10^8$, $\\omega_4 = 1.9\\times 10^8$.\n  - Trapezoidal weights for uniform spacing $h = 6.0\\times 10^7$: $w_1 = \\dfrac{h}{2}$, $w_2 = h$, $w_3 = h$, $w_4 = \\dfrac{h}{2}$.\n\n- Test case 4 (consistency check, full-order subspace should give zero residual):\n  - Use the same $E$, $A$, $B$, shifts, and weights as in Test case 3.\n  - Choose $V = I_3$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3,result4]$). Each `resultk` must be a floating-point number computed by aggregating the residuals at the given shifts using the provided weights, as defined by your derived indicator.",
            "solution": "The problem requires the derivation and implementation of a wideband error indicator for a projection-based model reduction of a linear time-invariant system arising from discretized Maxwell's equations. The indicator must aggregate residuals at multiple frequency points, weighted by quadrature coefficients, to serve as a surrogate for an $\\mathcal{H}_2$-like norm over a frequency band.\n\n### Step 1: Problem Validation\n\nThe first step is a critical validation of the problem statement.\n\n#### Step 1.1: Extracted Givens\n- **Full-Order System**:\n  - Equation: $(sE - A) X(s) = B$\n  - Matrices: $E \\in \\mathbb{R}^{n \\times n}$, $A \\in \\mathbb{R}^{n \\times n}$, $B \\in \\mathbb{R}^{n \\times m}$, $C \\in \\mathbb{R}^{p \\times n}$\n  - State: $X(s) \\in \\mathbb{C}^{n \\times m}$\n  - Transfer Function: $H(s) = C (sE - A)^{-1} B$\n- **Reduced-Order System**:\n  - Projection Matrix: $V \\in \\mathbb{R}^{n \\times r}$, column-orthonormal ($V^\\top V = I_r$)\n  - Reduced Operators: $E_r = V^\\top E V$, $A_r = V^\\top A V$, $B_r = V^\\top B$, $C_r = C V$\n  - Approximate State: $\\widetilde{X}(s) = V (s E_r - A_r)^{-1} B_r$\n  - Approximate Transfer Function: $H_r(s) = C_r (s E_r - A_r)^{-1} B_r$\n- **Error Indicator Requirements**:\n  - Sample points: A set of complex frequency shifts $\\{ s_i \\}_{i=1}^q$ where $s_i = \\jmath \\omega_i$ and $\\omega_i \\in \\mathbb{R}_{\\ge 0}$.\n  - Quadrature weights: A set of nonnegative weights $\\{ w_i \\}_{i=1}^q$.\n  - The indicator must combine residuals defined in terms of the full-order operators ($E, A$) and the reduced-order approximate state ($\\widetilde{X}(s)$).\n  - The aggregation must use the weights $w_i$.\n  - The norm used for the residual must be consistent for multiple right-hand sides (i.e., multiple columns of $B$).\n- **Test Suite**: Four test cases are provided, each specifying the numerical values for $E$, $A$, $B$, $V$, shifts $\\{\\omega_i\\}$, and weights $\\{w_i\\}$. The output format for the computed indicator values is specified. Note that the matrix $C$ is introduced in the preamble but is not provided in the test cases and is not required for a state-based residual calculation.\n\n#### Step 1.2: Validation Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the established theory of projection-based model order reduction for linear time-invariant systems, a standard technique in computational electromagnetics and control theory. All equations and definitions are standard.\n- **Well-Posed**: The problem is well-posed. It asks for the derivation of a specific quantity (an error indicator) based on clear constraints and provides all necessary data for its computation in the test suite. The absence of the matrix $C$ is not a flaw, as the task is explicitly to define the residual in terms of the state approximation $\\widetilde{X}(s)$, not the output approximation $H_r(s)$.\n- **Objective**: The problem is stated using precise and objective mathematical language.\n- **Flaw Analysis**:\n  1.  **Scientific Unsoundness**: None. The framework is standard.\n  2.  **Non-Formalizable/Irrelevant**: None. The problem is highly specific and formalizable.\n  3.  **Incomplete/Contradictory Setup**: None. All required data for calculating the state-based residual are provided.\n  4.  **Unrealistic/Infeasible**: None. The provided matrices are small, well-conditioned, and suitable for a computational test.\n  5.  **Ill-Posed/Poorly Structured**: None. The derivation leads to a unique and meaningful numerical result.\n  6.  **Pseudo-Profound/Trivial**: None. The task requires a correct derivation and implementation, which is non-trivial. Test case 4, where the projection space is the full space, provides a non-trivial consistency check.\n  7.  **Outside Scientific Verifiability**: None. The result is numerically verifiable.\n\n#### Step 1.3: Verdict and Action\nThe problem is **valid**. A solution will be derived and implemented.\n\n### Step 2: Derivation of the Error Indicator\n\nThe goal is to construct a scalar error indicator $\\mathcal{E}$ that quantifies the accuracy of the reduced-order model over a frequency band. The derivation proceeds from first principles as requested.\n\n1.  **The State Residual**: The fundamental measure of error is the residual, which quantifies how well the approximate solution satisfies the original governing equation. The full-order system is defined by $(sE - A) X(s) = B$. We substitute the approximate state from the reduced model, $\\widetilde{X}(s)$, into this equation. The resulting imbalance is the state residual matrix $R(s) \\in \\mathbb{C}^{n \\times m}$:\n    $$ R(s) = (sE - A) \\widetilde{X}(s) - B $$\n    A smaller residual implies a better approximation of the system's internal dynamics.\n\n2.  **Norm of the Residual**: The residual $R(s)$ is a matrix. To quantify its magnitude at a single frequency $s_i$, we require a matrix norm. For problems with multiple inputs (i.e., $m > 1$), a norm that treats all columns of the residual matrix equitably is needed. The Frobenius norm is the natural choice, as it is the matrix extension of the Euclidean vector norm and is defined as the square root of the sum of the squared magnitudes of all its elements. The squared Frobenius norm is given by:\n    $$ \\| R(s_i) \\|_F^2 = \\sum_{j=1}^n \\sum_{k=1}^m |(R(s_i))_{jk}|^2 = \\mathrm{Tr}(R(s_i)^* R(s_i)) $$\n    where $*$ denotes the conjugate transpose. This norm is computationally convenient and properly aggregates the error across all states and all inputs.\n\n3.  **Aggregation Across Frequencies**: The problem asks for a wideband error indicator that approximates an $\\mathcal{H}_2$-like norm, which is defined by an integral over the frequency axis. We are given quadrature points $s_i = \\jmath \\omega_i$ and corresponding weights $w_i$. This setup directly corresponds to a numerical quadrature scheme for approximating an integral. The integral of the squared Frobenius norm of the residual over a frequency band can be approximated by a weighted sum:\n    $$ \\int_{\\text{band}} \\|R(\\jmath \\omega)\\|_F^2 \\, d\\omega \\approx \\sum_{i=1}^q w_i \\|R(s_i)\\|_F^2 $$\n    This weighted sum, let us call it $\\mathcal{E}^2$, represents the total squared error over the band, as approximated by the given quadrature rule.\n\n4.  **Final Error Indicator**: The final error indicator, $\\mathcal{E}$, should have the same units as the residual norm itself, not its square. Therefore, we define the indicator as the square root of the aggregated sum:\n    $$ \\mathcal{E} = \\sqrt{\\sum_{i=1}^q w_i \\|R(s_i)\\|_F^2} $$\n    This is our physically meaningful and computationally tractable surrogate for the integrated residual norm.\n\n### Step 3: Computational Algorithm\n\nBased on the derived indicator, the algorithm for its computation is as follows:\n\n1.  Given the system matrices $E, A, B$, the projection matrix $V$, the frequency sample points $\\{\\omega_i\\}_{i=1}^q$, and the quadrature weights $\\{w_i\\}_{i=1}^q$.\n2.  Pre-compute the reduced-order matrices, which are independent of frequency:\n    $$ E_r = V^\\top E V \\quad \\in \\mathbb{R}^{r \\times r} $$\n    $$ A_r = V^\\top A V \\quad \\in \\mathbb{R}^{r \\times r} $$\n    $$ B_r = V^\\top B \\quad \\in \\mathbb{R}^{n \\times m} $$\n3.  Initialize a variable for the total weighted squared error, $\\mathcal{E}^2_{\\text{total}} = 0$.\n4.  For each sample index $i = 1, \\dots, q$:\n    a.  Set the complex frequency $s_i = \\jmath \\omega_i$.\n    b.  Form the reduced system matrix for this frequency: $M_{r,i} = s_i E_r - A_r$.\n    c.  Solve the $r \\times r$ complex linear system for the reduced state coefficient matrix $Y(s_i) \\in \\mathbb{C}^{r \\times m}$:\n        $$ M_{r,i} Y(s_i) = B_r \\implies Y(s_i) = M_{r,i}^{-1} B_r $$\n    d.  Project the solution back to the full $n$-dimensional space to obtain the approximate state $\\widetilde{X}(s_i) \\in \\mathbb{C}^{n \\times m}$:\n        $$ \\widetilde{X}(s_i) = V Y(s_i) $$\n    e.  Compute the full-order residual matrix $R(s_i) \\in \\mathbb{C}^{n \\times m}$:\n        $$ R(s_i) = (s_i E - A) \\widetilde{X}(s_i) - B $$\n    f.  Calculate the squared Frobenius norm of the residual, $\\epsilon_i^2 = \\|R(s_i)\\|_F^2$.\n    g.  Add the weighted contribution to the total: $\\mathcal{E}^2_{\\text{total}} = \\mathcal{E}^2_{\\text{total}} + w_i \\epsilon_i^2$.\n5.  The final error indicator is $\\mathcal{E} = \\sqrt{\\mathcal{E}^2_{\\text{total}}}$.\n\nThis algorithm will be implemented to solve the provided test cases. For Test Case 4, where $V = I_n$, we have $r=n$. The reduced system is identical to the full system, leading to $\\widetilde{X}(s) = X(s)$. Consequently, the residual $R(s)$ must be zero for all $s$, and the final indicator $\\mathcal{E}$ must be zero. This serves as a critical check for the correctness of the implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and computes a wideband error indicator for Krylov subspace model reduction.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (happy path, multiple inputs)\n        {\n            \"E\": np.diag([2.0, 1.5, 1.0, 0.8]),\n            \"A\": np.array([\n                [6.0, -1.0, 0.0, 0.0],\n                [-1.0, 5.0, -1.0, 0.0],\n                [0.0, -1.0, 4.5, -0.8],\n                [0.0, 0.0, -0.8, 3.5]\n            ]),\n            \"B\": np.array([\n                [1.0, 0.5],\n                [0.2, -0.3],\n                [0.0, 0.1],\n                [-0.1, 0.2]\n            ]),\n            \"V\": np.array([\n                [1.0, 0.0],\n                [0.0, 1.0],\n                [0.0, 0.0],\n                [0.0, 0.0]\n            ]),\n            \"omegas\": np.array([1.0e8, 2.0e8, 3.0e8]),\n            \"weights\": np.array([2.0e8 / 6, 4 * 2.0e8 / 6, 2.0e8 / 6])\n        },\n        # Test case 2 (identity mass, zero weight edge)\n        {\n            \"E\": np.identity(4),\n            \"A\": np.array([\n                [4.0, -1.0, 0.0, 0.0],\n                [-1.0, 4.0, -1.0, 0.0],\n                [0.0, -1.0, 3.5, -1.0],\n                [0.0, 0.0, -1.0, 3.0]\n            ]),\n            \"B\": np.array([\n                [1.0],\n                [0.0],\n                [0.5],\n                [-0.2]\n            ]),\n            \"V\": 0.5 * np.array([\n                [1.0],\n                [1.0],\n                [1.0],\n                [1.0]\n            ]),\n            \"omegas\": np.array([5.0e7, 1.5e8]),\n            \"weights\": np.array([1.0, 0.0])\n        },\n        # Test case 3 (near-resonant band, single input)\n        {\n            \"E\": np.diag([1.0, 0.8, 0.6]),\n            \"A\": np.array([\n                [2.0, -0.9, 0.0],\n                [-0.9, 2.2, -0.8],\n                [0.0, -0.8, 1.8]\n            ]),\n            \"B\": np.array([\n                [0.3],\n                [0.1],\n                [0.2]\n            ]),\n            \"V\": np.array([\n                [1.0],\n                [0.0],\n                [0.0]\n            ]),\n            \"omegas\": np.array([1.0e7, 7.0e7, 1.3e8, 1.9e8]),\n            \"weights\": np.array([6e7/2, 6e7, 6e7, 6e7/2])\n        },\n        # Test case 4 (consistency check, full-order subspace should give zero residual)\n        {\n            \"E\": np.diag([1.0, 0.8, 0.6]),\n            \"A\": np.array([\n                [2.0, -0.9, 0.0],\n                [-0.9, 2.2, -0.8],\n                [0.0, -0.8, 1.8]\n            ]),\n            \"B\": np.array([\n                [0.3],\n                [0.1],\n                [0.2]\n            ]),\n            \"V\": np.identity(3),\n            \"omegas\": np.array([1.0e7, 7.0e7, 1.3e8, 1.9e8]),\n            \"weights\": np.array([6e7/2, 6e7, 6e7, 6e7/2])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        E, A, B, V = case[\"E\"], case[\"A\"], case[\"B\"], case[\"V\"]\n        omegas, weights = case[\"omegas\"], case[\"weights\"]\n        \n        # 1. Compute reduced system matrices\n        E_r = V.T @ E @ V\n        A_r = V.T @ A @ V\n        B_r = V.T @ B\n\n        total_weighted_sq_err = 0.0\n\n        for omega, w in zip(omegas, weights):\n            if w == 0.0:\n                continue\n\n            # 2. Set complex frequency\n            s = 1j * omega\n\n            # 3. Solve the reduced system\n            # M_r * Y = B_r\n            M_r = s * E_r - A_r\n            try:\n                Y = np.linalg.solve(M_r, B_r)\n            except np.linalg.LinAlgError:\n                # Handle potential singularity, though not expected in these test cases\n                # If singular, the residual norm would be infinite.\n                Y = np.linalg.pinv(M_r) @ B_r\n            \n            # 4. Reconstruct approximate full-order state\n            X_tilde = V @ Y\n\n            # 5. Compute the residual\n            R = (s * E - A) @ X_tilde - B\n\n            # 6. Compute squared Frobenius norm of the residual\n            sq_frob_norm = np.linalg.norm(R, 'fro')**2\n\n            # 7. Add weighted contribution to total error\n            total_weighted_sq_err += w * sq_frob_norm\n\n        # 8. Compute the final indicator\n        indicator = np.sqrt(total_weighted_sq_err)\n        results.append(indicator)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}