{
    "hands_on_practices": [
        {
            "introduction": "Before tackling complex partial differential equations, we begin with the fundamental algebraic structure of a coupled system. This exercise  introduces the Jacobian matrix in its simplest form, showing how it linearizes a nonlinear residual vector. By assembling the Jacobian for a simple two-variable system, you will gain a concrete understanding of how its off-diagonal entries mathematically represent the coupling that is the hallmark of monolithic multiphysics solvers.",
            "id": "3515394",
            "problem": "In a monolithic (fully coupled) implicit solution of a two-field multiphysics problem, the algebraic residual vector is defined by two components $R_{u}(u,v)$ and $R_{v}(u,v)$ that must vanish simultaneously. Consider the coupled algebraic system with residuals\n$$\nR_{u}(u,v) = u + 2v - 1, \\qquad R_{v}(u,v) = 3u + v^{2} - 2.\n$$\nIn a Newton linearization of the monolithic system, the block Jacobian $J(u,v)$ is the Fréchet derivative of the residual vector with respect to the state vector, equivalently the matrix of partial derivatives $J_{ij}(u,v) = \\frac{\\partial R_{i}}{\\partial x_{j}}(u,v)$ for the state vector components $x_{1}=u$ and $x_{2}=v$. Using this fundamental definition, assemble the full $2\\times 2$ block Jacobian $J(u,v)$ and evaluate it at the state $(u,v)=(1,1)$. Then, based on first principles of the Newton method for nonlinear systems, explain how the off-diagonal entries of this Jacobian affect the Newton update direction when solving the linearized system for the increment $(\\Delta u,\\Delta v)$ at $(u,v)=(1,1)$.\n\nProvide the final answer as the evaluated Jacobian matrix at $(u,v)=(1,1)$ written as a single $2\\times 2$ matrix. No rounding is required and no units apply. The interpretive discussion about off-diagonal entries is part of your solution reasoning and does not need to appear in the final answer.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It represents a standard application of the Newton-Raphson method for a system of nonlinear algebraic equations, a fundamental technique in the monolithic solution of coupled multiphysics problems. All necessary information is provided, and the task is unambiguous.\n\nThe problem requires the assembly and evaluation of the Jacobian matrix for a given system of nonlinear algebraic residuals, followed by an interpretation of its structure within the context of the Newton method.\n\nThe coupled algebraic system is defined by a residual vector $R(u,v)$ which must be driven to zero. The components of the residual vector are given as:\n$$\nR(u,v) = \\begin{pmatrix} R_{u}(u,v) \\\\ R_{v}(u,v) \\end{pmatrix} = \\begin{pmatrix} u + 2v - 1 \\\\ 3u + v^{2} - 2 \\end{pmatrix}\n$$\nThe state vector is $x = \\begin{pmatrix} u \\\\ v \\end{pmatrix}$. In a monolithic Newton-Raphson scheme, the system is linearized at the current state $(u^k, v^k)$ to find an incremental update $(\\Delta u, \\Delta v)$ that moves closer to the solution. This linearization is given by the equation:\n$$\nJ(u^k, v^k) \\begin{pmatrix} \\Delta u \\\\ \\Delta v \\end{pmatrix} = -R(u^k, v^k)\n$$\nwhere $J(u^k, v^k)$ is the Jacobian matrix evaluated at the state $(u^k, v^k)$.\n\nThe Jacobian matrix $J(u,v)$ is defined as the matrix of all first-order partial derivatives of the residual vector components with respect to the state vector components. For our $2 \\times 2$ system, the Jacobian is:\n$$\nJ(u,v) = \\begin{pmatrix} \\frac{\\partial R_{u}}{\\partial u}  \\frac{\\partial R_{u}}{\\partial v} \\\\ \\frac{\\partial R_{v}}{\\partial u}  \\frac{\\partial R_{v}}{\\partial v} \\end{pmatrix}\n$$\nWe compute each entry of the Jacobian by differentiating the given residual functions:\n\nThe top-left entry, $J_{11}$, is the derivative of $R_u$ with respect to $u$:\n$$\nJ_{11}(u,v) = \\frac{\\partial R_{u}}{\\partial u} = \\frac{\\partial}{\\partial u}(u + 2v - 1) = 1\n$$\n\nThe top-right entry, $J_{12}$, is the derivative of $R_u$ with respect to $v$. This is an off-diagonal term representing the coupling from variable $v$ to the first equation:\n$$\nJ_{12}(u,v) = \\frac{\\partial R_{u}}{\\partial v} = \\frac{\\partial}{\\partial v}(u + 2v - 1) = 2\n$$\n\nThe bottom-left entry, $J_{21}$, is the derivative of $R_v$ with respect to $u$. This is the other off-diagonal term, representing the coupling from variable $u$ to the second equation:\n$$\nJ_{21}(u,v) = \\frac{\\partial R_{v}}{\\partial u} = \\frac{\\partial}{\\partial u}(3u + v^{2} - 2) = 3\n$$\n\nThe bottom-right entry, $J_{22}$, is the derivative of $R_v$ with respect to $v$:\n$$\nJ_{22}(u,v) = \\frac{\\partial R_{v}}{\\partial v} = \\frac{\\partial}{\\partial v}(3u + v^{2} - 2) = 2v\n$$\n\nAssembling these components yields the full symbolic Jacobian matrix:\n$$\nJ(u,v) = \\begin{pmatrix} 1  2 \\\\ 3  2v \\end{pmatrix}\n$$\nThe problem requires evaluating this Jacobian at the specific state $(u,v)=(1,1)$:\n$$\nJ(1,1) = \\begin{pmatrix} 1  2 \\\\ 3  2(1) \\end{pmatrix} = \\begin{pmatrix} 1  2 \\\\ 3  2 \\end{pmatrix}\n$$\nThis is the matrix requested in the final answer.\n\nTo explain the role of the off-diagonal entries, we first evaluate the residual vector at the point $(u,v)=(1,1)$:\n$$\nR(1,1) = \\begin{pmatrix} 1 + 2(1) - 1 \\\\ 3(1) + (1)^{2} - 2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\nThe Newton update system at this point is therefore:\n$$\n\\begin{pmatrix} 1  2 \\\\ 3  2 \\end{pmatrix} \\begin{pmatrix} \\Delta u \\\\ \\Delta v \\end{pmatrix} = -\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix}\n$$\nThis matrix equation corresponds to the system of linear equations:\n$$\n\\begin{cases} 1\\Delta u + 2\\Delta v = -2 \\\\ 3\\Delta u + 2\\Delta v = -2 \\end{cases}\n$$\nThe off-diagonal entries of the Jacobian, $J_{12} = \\frac{\\partial R_u}{\\partial v} = 2$ and $J_{21} = \\frac{\\partial R_v}{\\partial u} = 3$, are central to the \"fully coupled\" nature of the problem.\nThe term $J_{12} = 2$ indicates that the first residual, $R_u$, is sensitive to changes in the variable $v$. Its presence in the linearized system means that the equation to determine the update $\\Delta u$ is coupled to the update $\\Delta v$. Specifically, the first equation $1\\Delta u + 2\\Delta v = -2$ shows that the choice of $\\Delta u$ depends on the value of $\\Delta v$.\nSimilarly, the term $J_{21} = 3$ indicates that the second residual, $R_v$, is sensitive to changes in the variable $u$. This couples the determination of the update $\\Delta v$ to the update $\\Delta u$ via the second equation, $3\\Delta u + 2\\Delta v = -2$.\n\nIf the system were uncoupled, the off-diagonal terms would be zero. In that hypothetical case, the update equations would be $J_{11}\\Delta u = -R_u$ and $J_{22}\\Delta v = -R_v$, allowing $\\Delta u$ and $\\Delta v$ to be calculated independently. Because the off-diagonal entries are non-zero, the system of equations for the increments must be solved simultaneously. This mathematical coupling, embodied by the non-zero off-diagonal blocks of the Jacobian, is the defining characteristic of a monolithic solution approach for multiphysics problems, where the behavior of one physical field (represented by $u$) directly influences another (represented by $v$), and vice versa.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  2 \\\\ 3  2 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having grasped the algebraic essence of the Jacobian, we now apply this concept within the Finite Element Method (FEM). This practice  requires deriving the Jacobian, or \"tangent stiffness matrix,\" from the weak form of a nonlinear heat conduction equation. You will see firsthand how a physical nonlinearity—in this case, temperature-dependent conductivity—translates into a state-dependent Jacobian, a foundational skill for analyzing continuum mechanics problems.",
            "id": "3515324",
            "problem": "Consider a nondimensional steady one-dimensional nonlinear heat conduction problem on a single two-node linear finite element spanning a bar of nondimensional length $h=3$ with unit cross-sectional area. The temperature field $T(x)$ is approximated by standard linear shape functions $N_1(x)=1-x/h$ and $N_2(x)=x/h$ as $T(x)=N_1(x)\\,T_1+N_2(x)\\,T_2$, where $T_1$ and $T_2$ are the nodal temperatures. The conductivity is temperature-dependent and given by $k(T)=1+T$, where $T$ is nondimensional. Assume no internal sources and focus on the interior operator in the weak form.\n\nStarting from the strong form $-\\frac{d}{dx}\\left(k(T)\\frac{dT}{dx}\\right)=0$, use the standard Galerkin construction of the residual and the consistent linearization required by the monolithic Newton-Raphson (NR) method within the Finite Element Method (FEM) to derive, from first principles, the element-level Jacobian entries $J_{ij}=\\frac{\\partial R_i}{\\partial T_j}$ for $i,j\\in\\{1,2\\}$, where $R_i$ is the element residual associated with the test function $N_i$. Then, at the current NR iterate given by the nodal values $T_1=0.2$ and $T_2=0.8$, compute the numerical values of the four Jacobian entries $J_{11}$, $J_{12}$, $J_{21}$, and $J_{22}$.\n\nExpress the four entries as dimensionless numbers and round each to four significant figures. Report your final answer as a single row vector in the order $\\left(J_{11},\\,J_{12},\\,J_{21},\\,J_{22}\\right)$.",
            "solution": "The problem is to derive and compute the element-level Jacobian matrix for a one-dimensional nonlinear heat conduction problem using the Finite Element Method (FEM).\n\nThe strong form of the steady-state heat conduction equation with no internal sources is given as:\n$$-\\frac{d}{dx}\\left(k(T)\\frac{dT}{dx}\\right) = 0$$\nwhere $x$ is the spatial coordinate, $T(x)$ is the temperature field, and $k(T)$ is the temperature-dependent thermal conductivity.\n\nTo derive the weak form, we multiply the strong form by a test function $w(x)$ and integrate over the element domain, which spans from $x=0$ to $x=h$.\n$$\\int_0^h w(x) \\left[-\\frac{d}{dx}\\left(k(T(x))\\frac{dT}{dx}\\right)\\right] dx = 0$$\nApplying integration by parts to reduce the order of the derivatives and distribute them between the trial and test functions, we get:\n$$\\int_0^h \\frac{dw}{dx} k(T(x)) \\frac{dT}{dx} dx - \\left[ w(x) k(T(x)) \\frac{dT}{dx} \\right]_0^h = 0$$\nThe second term represents the heat flux at the element boundaries. In assembling the element-level operator, we only consider the integral part, which defines the internal residual.\n\nAccording to the Galerkin method, the test functions $w(x)$ are chosen from the same set as the basis functions (shape functions) used to approximate the temperature field, i.e., $w(x) = N_i(x)$ for $i=1,2$. The temperature field $T(x)$ is approximated as a linear combination of the nodal temperatures $T_j$ and shape functions $N_j(x)$:\n$$T(x) = \\sum_{j=1}^{2} N_j(x) T_j = N_1(x)T_1 + N_2(x)T_2$$\nThe element residual vector $\\mathbf{R}$ has components $R_i$ corresponding to each test function $N_i(x)$:\n$$R_i = \\int_0^h \\frac{dN_i}{dx} k(T(x)) \\frac{dT}{dx} dx$$\nThe shape functions for a two-node linear element of length $h$ are:\n$$N_1(x) = 1 - \\frac{x}{h}, \\quad N_2(x) = \\frac{x}{h}$$\nTheir derivatives, which are constant over the element, are:\n$$B_1 = \\frac{dN_1}{dx} = -\\frac{1}{h}, \\quad B_2 = \\frac{dN_2}{dx} = \\frac{1}{h}$$\nThe temperature gradient is also constant over the element:\n$$\\frac{dT}{dx} = \\frac{d}{dx}(N_1T_1+N_2T_2) = B_1T_1+B_2T_2 = \\frac{T_2-T_1}{h}$$\nThe conductivity is given by $k(T) = 1+T$.\n\nThe Jacobian matrix, or tangent stiffness matrix, is required for the Newton-Raphson iterative scheme. Its entries are defined by the partial derivative of the residual components $R_i$ with respect to the nodal temperature unknowns $T_j$:\n$$J_{ij} = \\frac{\\partial R_i}{\\partial T_j} = \\frac{\\partial}{\\partial T_j} \\left( \\int_0^h \\frac{dN_i}{dx} k(T(x)) \\frac{dT}{dx} dx \\right)$$\nSince the integration is over $x$, we can move the differentiation inside the integral:\n$$J_{ij} = \\int_0^h \\frac{dN_i}{dx} \\frac{\\partial}{\\partial T_j} \\left( k(T(x)) \\frac{dT}{dx} \\right) dx$$\nUsing the product rule for differentiation:\n$$J_{ij} = \\int_0^h \\frac{dN_i}{dx} \\left( \\frac{\\partial k(T)}{\\partial T_j} \\frac{dT}{dx} + k(T) \\frac{\\partial}{\\partial T_j}\\left(\\frac{dT}{dx}\\right) \\right) dx$$\nWe evaluate the partial derivative terms:\n$$\\frac{\\partial}{\\partial T_j}\\left(\\frac{dT}{dx}\\right) = \\frac{\\partial}{\\partial T_j}\\left(\\sum_{l=1}^{2} B_l T_l\\right) = B_j = \\frac{dN_j}{dx}$$\n$$\\frac{\\partial k(T)}{\\partial T_j} = \\frac{dk}{dT} \\frac{\\partial T}{\\partial T_j} = \\frac{dk}{dT} N_j(x)$$\nGiven $k(T)=1+T$, we have $\\frac{dk}{dT}=1$.\nSubstituting these into the expression for $J_{ij}$:\n$$J_{ij} = \\int_0^h B_i \\left( (1) N_j(x) \\frac{dT}{dx} + k(T(x)) B_j \\right) dx$$\n$$J_{ij} = \\int_0^h B_i B_j k(T(x)) dx + \\int_0^h B_i N_j(x) \\frac{dT}{dx} dx$$\nSince $B_i$, $B_j$, and $\\frac{dT}{dx}$ are constant within the element, we can take them out of the integrals:\n$$J_{ij} = B_i B_j \\int_0^h k(T(x)) dx + B_i \\frac{dT}{dx} \\int_0^h N_j(x) dx$$\nWe need to evaluate the integrals:\n$$\\int_0^h N_1(x) dx = \\int_0^h (1-\\frac{x}{h}) dx = \\left[x - \\frac{x^2}{2h}\\right]_0^h = h - \\frac{h^2}{2h} = \\frac{h}{2}$$\n$$\\int_0^h N_2(x) dx = \\int_0^h \\frac{x}{h} dx = \\left[\\frac{x^2}{2h}\\right]_0^h = \\frac{h^2}{2h} = \\frac{h}{2}$$\n$$\\int_0^h k(T(x)) dx = \\int_0^h (1+T(x)) dx = h + \\int_0^h (N_1(x)T_1 + N_2(x)T_2) dx = h + T_1\\frac{h}{2} + T_2\\frac{h}{2} = h\\left(1 + \\frac{T_1+T_2}{2}\\right)$$\nSubstituting these integrals back into the expression for $J_{ij}$:\n$$J_{ij} = B_i B_j h\\left(1 + \\frac{T_1+T_2}{2}\\right) + B_i \\left(\\frac{T_2-T_1}{h}\\right) \\frac{h}{2}$$\n$$J_{ij} = B_i B_j h\\left(1 + \\frac{T_1+T_2}{2}\\right) + B_i \\frac{T_2-T_1}{2}$$\n\nNow we compute the four entries of the Jacobian matrix:\nFor $J_{11}$ ($i=1, j=1$): $B_1 = -1/h$\n$$J_{11} = \\left(-\\frac{1}{h}\\right)\\left(-\\frac{1}{h}\\right)h\\left(1 + \\frac{T_1+T_2}{2}\\right) + \\left(-\\frac{1}{h}\\right)\\frac{T_2-T_1}{2} = \\frac{1}{h}\\left(1 + \\frac{T_1+T_2}{2}\\right) - \\frac{T_2-T_1}{2h} = \\frac{1}{h}\\left(1 + \\frac{T_1+T_2 - T_2+T_1}{2}\\right) = \\frac{1}{h}(1+T_1)$$\nFor $J_{12}$ ($i=1, j=2$): $B_1 = -1/h, B_2 = 1/h$\n$$J_{12} = \\left(-\\frac{1}{h}\\right)\\left(\\frac{1}{h}\\right)h\\left(1 + \\frac{T_1+T_2}{2}\\right) + \\left(-\\frac{1}{h}\\right)\\frac{T_2-T_1}{2} = -\\frac{1}{h}\\left(1 + \\frac{T_1+T_2}{2}\\right) - \\frac{T_2-T_1}{2h} = -\\frac{1}{h}\\left(1 + \\frac{T_1+T_2 + T_2-T_1}{2}\\right) = -\\frac{1}{h}(1+T_2)$$\nFor $J_{21}$ ($i=2, j=1$): $B_2 = 1/h, B_1 = -1/h$\n$$J_{21} = \\left(\\frac{1}{h}\\right)\\left(-\\frac{1}{h}\\right)h\\left(1 + \\frac{T_1+T_2}{2}\\right) + \\left(\\frac{1}{h}\\right)\\frac{T_2-T_1}{2} = -\\frac{1}{h}\\left(1 + \\frac{T_1+T_2}{2}\\right) + \\frac{T_2-T_1}{2h} = -\\frac{1}{h}\\left(1 + \\frac{T_1+T_2 - (T_2-T_1)}{2}\\right) = -\\frac{1}{h}(1+T_1)$$\nFor $J_{22}$ ($i=2, j=2$): $B_2 = 1/h$\n$$J_{22} = \\left(\\frac{1}{h}\\right)\\left(\\frac{1}{h}\\right)h\\left(1 + \\frac{T_1+T_2}{2}\\right) + \\left(\\frac{1}{h}\\right)\\frac{T_2-T_1}{2} = \\frac{1}{h}\\left(1 + \\frac{T_1+T_2}{2}\\right) + \\frac{T_2-T_1}{2h} = \\frac{1}{h}\\left(1 + \\frac{T_1+T_2 + T_2-T_1}{2}\\right) = \\frac{1}{h}(1+T_2)$$\nThe symbolic Jacobian matrix is:\n$$\\mathbf{J} = \\frac{1}{h} \\begin{pmatrix} 1+T_1  -(1+T_2) \\\\ -(1+T_1)  1+T_2 \\end{pmatrix}$$\nNow, we substitute the given numerical values: $h=3$, $T_1=0.2$, and $T_2=0.8$.\n$$J_{11} = \\frac{1}{3}(1+0.2) = \\frac{1.2}{3} = 0.4$$\n$$J_{12} = -\\frac{1}{3}(1+0.8) = -\\frac{1.8}{3} = -0.6$$\n$$J_{21} = -\\frac{1}{3}(1+0.2) = -\\frac{1.2}{3} = -0.4$$\n$$J_{22} = \\frac{1}{3}(1+0.8) = \\frac{1.8}{3} = 0.6$$\nThe problem requires the answers to be rounded to four significant figures.\n$J_{11} = 0.4000$\n$J_{12} = -0.6000$\n$J_{21} = -0.4000$\n$J_{22} = 0.6000$\nThe final answer is a row vector of these four entries.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.4000  -0.6000  -0.4000  0.6000\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The final practice moves from paper-and-pencil derivation to computational implementation, addressing a crucial aspect of practical multiphysics simulation. This coding exercise  directs you to compare the performance of a Newton solver using an exact analytical Jacobian versus a simpler finite-difference approximation. By analyzing the results, you will discover the profound impact of Jacobian accuracy on both the solver's convergence rate and, critically, the physical validity of a subsequent stability analysis.",
            "id": "3515360",
            "problem": "Consider a reduced, dimensionless, three-amplitude model of resistive Magnetohydrodynamics (MHD) stability obtained by a Galerkin truncation of the steady-state momentum balance, induction equation, and closure for pressure. Let the unknowns be the dimensionless amplitudes $U$, $B$, and $P$ representing, respectively, fluid velocity, magnetic perturbation, and pressure perturbation. The fully coupled residual vector $\\mathbf{F}(U,B,P)$ is defined as\n$$\n\\mathbf{F}(U,B,P) =\n\\begin{bmatrix}\nF_u(U,B,P) \\\\\nF_b(U,B,P) \\\\\nF_p(U,B,P)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nf - \\alpha P + \\beta B^2 - \\gamma U - \\delta U^3 \\\\\n\\theta U B - \\eta B - \\zeta B^3 \\\\\nP - \\chi U^2 - \\xi B^2\n\\end{bmatrix},\n$$\nwhere all quantities are dimensionless and the parameters $f$, $\\alpha$, $\\beta$, $\\gamma$, $\\delta$, $\\theta$, $\\eta$, $\\zeta$, $\\chi$, and $\\xi$ are positive constants except $\\gamma$ which may be either positive or negative depending on the net linear drive or damping.\n\nA monolithic (fully coupled) Newton method for solving the nonlinear system $\\mathbf{F}(\\mathbf{x})=\\mathbf{0}$ with $\\mathbf{x}=[U,B,P]^T$ requires assembling the Jacobian matrix\n$$\n\\mathbf{J}(\\mathbf{x}) = \\frac{\\partial \\mathbf{F}}{\\partial \\mathbf{x}}(\\mathbf{x}) =\n\\begin{bmatrix}\n\\frac{\\partial F_u}{\\partial U}  \\frac{\\partial F_u}{\\partial B}  \\frac{\\partial F_u}{\\partial P} \\\\\n\\frac{\\partial F_b}{\\partial U}  \\frac{\\partial F_b}{\\partial B}  \\frac{\\partial F_b}{\\partial P} \\\\\n\\frac{\\partial F_p}{\\partial U}  \\frac{\\partial F_p}{\\partial B}  \\frac{\\partial F_p}{\\partial P}\n\\end{bmatrix}.\n$$\nTwo assembly strategies are considered:\n- Exact Jacobian: each partial derivative is evaluated analytically from the residual definitions.\n- Finite-difference Jacobian: each column $j$ of $\\mathbf{J}$ is approximated by the forward-difference formula\n$$\n\\mathbf{J}_{:,j}(\\mathbf{x}) \\approx \\frac{\\mathbf{F}(\\mathbf{x}+h\\,\\mathbf{e}_j) - \\mathbf{F}(\\mathbf{x})}{h},\n$$\nwhere $\\mathbf{e}_j$ is the $j$-th unit vector and $h0$ is the chosen finite-difference step.\n\nFor stability characterization at the converged state $\\mathbf{x}^\\star$, consider the eigenvalues of the Jacobian $\\lambda_i(\\mathbf{J}(\\mathbf{x}^\\star))$. In this reduced model, spurious unstable modes arising from Jacobian approximation errors are defined as additional eigenvalues with strictly positive real part present in the finite-difference Jacobian spectrum but not in the exact Jacobian spectrum, quantified by the difference in counts\n$$\nN_+^{\\text{FD}}(\\tau) - N_+^{\\text{EX}}(\\tau),\n$$\nwhere\n$$\nN_+^{\\text{FD}}(\\tau) = \\#\\left\\{i \\,\\middle|\\, \\operatorname{Re}(\\lambda_i(\\mathbf{J}_{\\text{FD}}(\\mathbf{x}^\\star)))  \\tau \\right\\},\\quad\nN_+^{\\text{EX}}(\\tau) = \\#\\left\\{i \\,\\middle|\\, \\operatorname{Re}(\\lambda_i(\\mathbf{J}_{\\text{EX}}(\\mathbf{x}^\\star)))  \\tau \\right\\},\n$$\nand $\\tau$ is a small positive threshold.\n\nYour task is to implement a monolithic Newton solver with backtracking line search for this system using both exact and finite-difference Jacobian assemblies, starting from a specified initial guess $\\mathbf{x}_0$. For each test case, perform the following steps:\n1. Solve $\\mathbf{F}(\\mathbf{x})=\\mathbf{0}$ using the exact Jacobian to obtain $\\mathbf{x}_{\\text{EX}}^\\star$ and record a boolean $C_{\\text{EX}}$ indicating whether the solver converged within a given tolerance.\n2. Assemble both $\\mathbf{J}_{\\text{EX}}(\\mathbf{x}_{\\text{EX}}^\\star)$ and $\\mathbf{J}_{\\text{FD}}(\\mathbf{x}_{\\text{EX}}^\\star)$ using the specified $h$, then compute:\n   - The count of spurious eigenvalues $S = \\max\\left(0, N_+^{\\text{FD}}(\\tau) - N_+^{\\text{EX}}(\\tau)\\right)$.\n   - The difference in spectral radii $\\Delta \\rho = \\left|\\max_i \\left|\\lambda_i(\\mathbf{J}_{\\text{FD}}(\\mathbf{x}_{\\text{EX}}^\\star))\\right| - \\max_i \\left|\\lambda_i(\\mathbf{J}_{\\text{EX}}(\\mathbf{x}_{\\text{EX}}^\\star))\\right| \\right|$.\n3. Solve $\\mathbf{F}(\\mathbf{x})=\\mathbf{0}$ using the finite-difference Jacobian with the same initial guess to obtain $\\mathbf{x}_{\\text{FD}}^\\star$ and record a boolean $C_{\\text{FD}}$ indicating whether the solver converged within tolerance.\n\nAdopt the following concrete parameter values (dimensionless) for all test cases:\n- $f = 0.05$, $\\alpha = 1.5$, $\\beta = 0.7$, $\\gamma = 0.3$, $\\delta = 1.2$, $\\theta = 0.9$, $\\eta = 0.25$, $\\zeta = 0.5$, $\\chi = 0.8$, $\\xi = 0.6$.\n\nUse the following test suite, which varies the finite-difference step $h$ and the initial guess $\\mathbf{x}_0$ to probe accuracy and robustness:\n- Test case A (happy path, accurate finite-difference Jacobian):\n  - $h = 10^{-8}$.\n  - $\\mathbf{x}_0 = [0.2, 0.05, 0.01]$.\n- Test case B (moderate finite-difference step, potential mild degradation):\n  - $h = 10^{-3}$.\n  - $\\mathbf{x}_0 = [0.5, 0.3, 0.1]$.\n- Test case C (large finite-difference step, likely significant approximation error):\n  - $h = 10^{-1}$.\n  - $\\mathbf{x}_0 = [0.7, 0.5, 0.2]$.\n\nAll quantities are dimensionless; no physical units are required. Use a convergence tolerance of $\\|\\mathbf{F}(\\mathbf{x})\\|_2  10^{-12}$, a maximum of $50$ Newton iterations, and a backtracking line search that reduces the step length by a factor of $1/2$ up to $10$ times whenever the residual norm does not decrease.\n\nSet the instability threshold to $\\tau = 10^{-6}$.\n\nYour program should produce a single line of output containing the results for all three test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list of four values in the order $[C_{\\text{EX}}, C_{\\text{FD}}, S, \\Delta \\rho]$. For example, the output format must be\n$$\n[\\ [\\text{bool},\\text{bool},\\text{int},\\text{float}],\\ [\\text{bool},\\text{bool},\\text{int},\\text{float}],\\ [\\text{bool},\\text{bool},\\text{int},\\text{float}]\\ ].\n$$",
            "solution": "The problem requires the implementation and comparison of a monolithic Newton solver for a system of nonlinear equations using two different Jacobian assembly strategies: an exact analytical Jacobian and a finite-difference approximation.\n\n### 1. Mathematical Formulation\n\nThe system to be solved is a set of three coupled, nonlinear algebraic equations, represented by the residual vector $\\mathbf{F}(\\mathbf{x}) = \\mathbf{0}$, where $\\mathbf{x} = [U, B, P]^T$ is the state vector of dimensionless amplitudes. The residual vector is given by:\n$$\n\\mathbf{F}(U,B,P) =\n\\begin{bmatrix}\nF_u(U,B,P) \\\\\nF_b(U,B,P) \\\\\nF_p(U,B,P)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nf - \\alpha P + \\beta B^2 - \\gamma U - \\delta U^3 \\\\\n\\theta U B - \\eta B - \\zeta B^3 \\\\\nP - \\chi U^2 - \\xi B^2\n\\end{bmatrix} = \\mathbf{0}\n$$\nThe constants $f, \\alpha, \\beta, \\gamma, \\delta, \\theta, \\eta, \\zeta, \\chi, \\xi$ are given fixed values.\n\n### 2. Monolithic Newton-Raphson Method\n\nThe Newton-Raphson method is an iterative algorithm for finding the roots of a system of nonlinear equations. Starting from an initial guess $\\mathbf{x}_0$, it generates a sequence of approximations $\\mathbf{x}_k$ that ideally converge to a solution $\\mathbf{x}^\\star$. The core of the method is the update rule derived from a first-order Taylor expansion of $\\mathbf{F}$ around the current iterate $\\mathbf{x}_k$:\n$$\n\\mathbf{F}(\\mathbf{x}_{k+1}) \\approx \\mathbf{F}(\\mathbf{x}_k) + \\mathbf{J}(\\mathbf{x}_k)(\\mathbf{x}_{k+1} - \\mathbf{x}_k)\n$$\nSetting $\\mathbf{F}(\\mathbf{x}_{k+1}) = \\mathbf{0}$ and defining the update step as $\\Delta\\mathbf{x}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$, we obtain a linear system for the update:\n$$\n\\mathbf{J}(\\mathbf{x}_k) \\Delta\\mathbf{x}_k = -\\mathbf{F}(\\mathbf{x}_k)\n$$\nHere, $\\mathbf{J}(\\mathbf{x}_k)$ is the Jacobian matrix of $\\mathbf{F}$ evaluated at $\\mathbf{x}_k$. After solving for $\\Delta\\mathbf{x}_k$, the next iterate is computed as $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\Delta\\mathbf{x}_k$.\n\n### 3. Jacobian Assembly Strategies\n\nThe Jacobian matrix $\\mathbf{J}$ contains the partial derivatives of the residual components with respect to the state variables. Two assembly methods are specified.\n\n**a) Exact Analytical Jacobian ($\\mathbf{J}_{\\text{EX}}$)**\n\nThis approach involves analytically differentiating each component of $\\mathbf{F}$ with respect to each variable in $\\mathbf{x} = [U, B, P]^T$. The resulting derivatives are:\n- $\\frac{\\partial F_u}{\\partial U} = -\\gamma - 3\\delta U^2$\n- $\\frac{\\partial F_u}{\\partial B} = 2\\beta B$\n- $\\frac{\\partial F_u}{\\partial P} = -\\alpha$\n\n- $\\frac{\\partial F_b}{\\partial U} = \\theta B$\n- $\\frac{\\partial F_b}{\\partial B} = \\theta U - \\eta - 3\\zeta B^2$\n- $\\frac{\\partial F_b}{\\partial P} = 0$\n\n- $\\frac{\\partial F_p}{\\partial U} = -2\\chi U$\n- $\\frac{\\partial F_p}{\\partial B} = -2\\xi B$\n- $\\frac{\\partial F_p}{\\partial P} = 1$\n\nAssembling these into matrix form gives the exact Jacobian:\n$$\n\\mathbf{J}_{\\text{EX}}(U,B,P) =\n\\begin{bmatrix}\n-\\gamma - 3\\delta U^2    2\\beta B        -\\alpha \\\\\n\\theta B                 \\theta U - \\eta - 3\\zeta B^2  0 \\\\\n-2\\chi U                 -2\\xi B         1\n\\end{bmatrix}\n$$\n\n**b) Finite-Difference Jacobian ($\\mathbf{J}_{\\text{FD}}$)**\n\nThis method approximates the partial derivatives numerically. The problem specifies a forward-difference scheme. The $j$-th column of the Jacobian is approximated by perturbing the $j$-th component of the state vector $\\mathbf{x}$:\n$$\n\\mathbf{J}_{:,j}(\\mathbf{x}) \\approx \\frac{\\mathbf{F}(\\mathbf{x}+h\\,\\mathbf{e}_j) - \\mathbf{F}(\\mathbf{x})}{h}\n$$\nwhere $\\mathbf{e}_j$ is the $j$-th standard basis vector (e.g., $\\mathbf{e}_1 = [1, 0, 0]^T$) and $h$ is a small, positive step size. This approach avoids the need for manual differentiation but introduces a truncation error that scales with $h$.\n\n### 4. Globalization: Backtracking Line Search\n\nThe standard Newton step ($\\Delta\\mathbf{x}_k$) does not guarantee a reduction in the residual norm $\\|\\mathbf{F}(\\mathbf{x})\\|_2$, especially far from the solution. To improve robustness, a backtracking line search is employed. The update is modified to $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\Delta\\mathbf{x}_k$, where $\\alpha_k \\in (0, 1]$ is a step length. We start with $\\alpha_k=1$ (the full Newton step) and check if $\\|\\mathbf{F}(\\mathbf{x}_k + \\alpha_k \\Delta\\mathbf{x}_k)\\|_2  \\|\\mathbf{F}(\\mathbf{x}_k)\\|_2$. If the condition is not met, $\\alpha_k$ is successively reduced (e.g., by a factor of $1/2$) until a decrease in the residual norm is achieved or a maximum number of reductions is performed. If no suitable $\\alpha_k$ is found, the solver terminates, indicating non-convergence.\n\n### 5. Stability and Spectral Analysis\n\nAt a converged solution $\\mathbf{x}^\\star$, the stability of this steady state is governed by the eigenvalues $\\lambda_i$ of the Jacobian $\\mathbf{J}(\\mathbf{x}^\\star)$. If any eigenvalue has a strictly positive real part ($\\operatorname{Re}(\\lambda_i)  0$), the system is unstable. The task requires quantifying the spurious unstable modes introduced by the finite-difference approximation. This is done by computing $S = \\max(0, N_+^{\\text{FD}}(\\tau) - N_+^{\\text{EX}}(\\tau))$, where $N_+$ is the count of eigenvalues with a real part greater than a small threshold $\\tau = 10^{-6}$. Additionally, the difference in the spectral radii, $\\Delta \\rho$, between the two Jacobians is calculated as a measure of the overall spectral distortion. The spectral radius $\\rho(\\mathbf{J})$ is defined as $\\max_i |\\lambda_i(\\mathbf{J})|$.\n\n### 6. Implementation Strategy\n\nThe solution is implemented in Python using the `numpy` library.\n1.  A function `residual` computes $\\mathbf{F}(\\mathbf{x})$ for a given state vector $\\mathbf{x}$.\n2.  Functions `exact_jacobian` and `fd_jacobian` implement the two assembly strategies described above.\n3.  A generic `newton_solver` function is created. It takes an initial guess $\\mathbf{x}_0$ and a handle to a Jacobian assembly function. It iteratively solves the linear system and applies the backtracking line search until the L2-norm of the residual is below the tolerance of $10^{-12}$ or the maximum iteration count of $50$ is reached.\n4.  For each test case, the `newton_solver` is first called with the exact Jacobian. The resulting solution $\\mathbf{x}_{\\text{EX}}^\\star$ is used to compute $\\mathbf{J}_{\\text{EX}}$ and $\\mathbf{J}_{\\text{FD}}$.\n5.  `numpy.linalg.eigvals` is used to find the eigenvalues of both matrices. These are then used to calculate the number of spurious modes $S$ and the spectral radius difference $\\Delta\\rho$.\n6.  The `newton_solver` is called a second time with the finite-difference Jacobian and the same initial guess $\\mathbf{x}_0$ to determine its convergence behavior.\n7.  The results ($C_{\\text{EX}}$, $C_{\\text{FD}}$, $S$, $\\Delta \\rho$) for each test case are collected and printed in the specified format.",
            "answer": "```python\nimport numpy as np\n\n# A meticulous and exacting professor in the STEM fields.\n\ndef solve():\n    \"\"\"\n    Solves the MHD amplitude model using a monolithic Newton method\n    with both exact and finite-difference Jacobians and analyzes the results.\n    \"\"\"\n    \n    # ------------------ Parameters and Configuration ------------------\n    # Dimensionless physical parameters\n    PARAMS = {\n        'f': 0.05, 'alpha': 1.5, 'beta': 0.7, 'gamma': 0.3, 'delta': 1.2,\n        'theta': 0.9, 'eta': 0.25, 'zeta': 0.5, 'chi': 0.8, 'xi': 0.6\n    }\n    \n    # Newton solver settings\n    TOL = 1e-12\n    MAX_ITER = 50\n    MAX_LS_STEPS = 10\n    \n    # Stability analysis threshold\n    TAU = 1e-6\n    \n    # Test cases {h_value, initial_guess}\n    test_cases = [\n        {'h': 1e-8, 'x0': [0.2, 0.05, 0.01]},  # Test case A\n        {'h': 1e-3, 'x0': [0.5, 0.3, 0.1]},    # Test case B\n        {'h': 1e-1, 'x0': [0.7, 0.5, 0.2]}     # Test case C\n    ]\n    \n    # ------------------ Core Mathematical Functions ------------------\n    def residual(x, params):\n        \"\"\"Computes the residual vector F(x).\"\"\"\n        U, B, P = x\n        p = params\n        F_u = p['f'] - p['alpha']*P + p['beta']*B**2 - p['gamma']*U - p['delta']*U**3\n        F_b = p['theta']*U*B - p['eta']*B - p['zeta']*B**3\n        F_p = P - p['chi']*U**2 - p['xi']*B**2\n        return np.array([F_u, F_b, F_p])\n\n    def exact_jacobian(x, params):\n        \"\"\"Computes the exact analytical Jacobian matrix J_EX(x).\"\"\"\n        U, B, P = x\n        p = params\n        J = np.zeros((3, 3))\n        # dF_u/dx\n        J[0, 0] = -p['gamma'] - 3*p['delta']*U**2\n        J[0, 1] = 2*p['beta']*B\n        J[0, 2] = -p['alpha']\n        # dF_b/dx\n        J[1, 0] = p['theta']*B\n        J[1, 1] = p['theta']*U - p['eta'] - 3*p['zeta']*B**2\n        J[1, 2] = 0\n        # dF_p/dx\n        J[2, 0] = -2*p['chi']*U\n        J[2, 1] = -2*p['xi']*B\n        J[2, 2] = 1\n        return J\n\n    def fd_jacobian(x, h, params):\n        \"\"\"Computes the finite-difference Jacobian matrix J_FD(x).\"\"\"\n        n = len(x)\n        J = np.zeros((n, n))\n        F0 = residual(x, params)\n        for j in range(n):\n            x_plus_h = x.copy()\n            x_plus_h[j] += h\n            F_plus_h = residual(x_plus_h, params)\n            J[:, j] = (F_plus_h - F0) / h\n        return J\n\n    # ------------------ Newton Solver Implementation ------------------\n    def newton_solver(x0, jacobian_func, params, tol, max_iter, max_ls_steps):\n        \"\"\"Monolithic Newton solver with backtracking line search.\"\"\"\n        x_k = np.array(x0, dtype=float)\n\n        for _ in range(max_iter):\n            F_k = residual(x_k, params)\n            norm_Fk = np.linalg.norm(F_k)\n\n            if norm_Fk  tol:\n                return x_k, True\n\n            try:\n                J_k = jacobian_func(x_k)\n                dx = np.linalg.solve(J_k, -F_k)\n            except np.linalg.LinAlgError:\n                return x_k, False  # Singular matrix\n\n            # Backtracking line search\n            alpha = 1.0\n            for _ in range(max_ls_steps):\n                x_new = x_k + alpha * dx\n                norm_F_new = np.linalg.norm(residual(x_new, params))\n                if norm_F_new  norm_Fk:\n                    x_k = x_new\n                    break\n                alpha /= 2.0\n            else:\n                # Line search failed to find a decreasing step\n                return x_k, False\n        \n        # Max iterations reached\n        return x_k, False\n\n    # ------------------ Main Execution Logic ------------------\n    all_results = []\n    for case in test_cases:\n        h, x0_list = case['h'], case['x0']\n        x0 = np.array(x0_list, dtype=float)\n\n        # 1. Solve with Exact Jacobian\n        jac_ex_func = lambda x: exact_jacobian(x, PARAMS)\n        x_ex_star, C_ex = newton_solver(x0, jac_ex_func, PARAMS, TOL, MAX_ITER, MAX_LS_STEPS)\n        \n        # 2. Assemble Jacobians at the exact solution and analyze\n        J_ex_at_sol = exact_jacobian(x_ex_star, PARAMS)\n        J_fd_at_sol = fd_jacobian(x_ex_star, h, PARAMS)\n        \n        eigs_ex = np.linalg.eigvals(J_ex_at_sol)\n        eigs_fd = np.linalg.eigvals(J_fd_at_sol)\n        \n        # Count spurious unstable modes\n        N_plus_ex = np.sum(np.real(eigs_ex)  TAU)\n        N_plus_fd = np.sum(np.real(eigs_fd)  TAU)\n        S = max(0, N_plus_fd - N_plus_ex)\n        \n        # Calculate difference in spectral radii\n        rho_ex = np.max(np.abs(eigs_ex))\n        rho_fd = np.max(np.abs(eigs_fd))\n        delta_rho = np.abs(rho_fd - rho_ex)\n        \n        # 3. Solve with Finite-Difference Jacobian\n        jac_fd_func = lambda x: fd_jacobian(x, h, PARAMS)\n        x_fd_star, C_fd = newton_solver(x0, jac_fd_func, PARAMS, TOL, MAX_ITER, MAX_LS_STEPS)\n        \n        all_results.append([C_ex, C_fd, int(S), delta_rho])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}