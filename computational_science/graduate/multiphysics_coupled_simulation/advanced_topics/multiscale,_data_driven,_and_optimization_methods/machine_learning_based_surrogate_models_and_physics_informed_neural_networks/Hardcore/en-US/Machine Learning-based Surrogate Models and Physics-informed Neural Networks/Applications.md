## Applications and Interdisciplinary Connections

The preceding chapters have established the foundational principles and mechanisms of machine learning-based [surrogate models](@entry_id:145436), with a particular focus on Physics-Informed Neural Networks (PINNs). We now shift our perspective from the theoretical underpinnings to the practical utility of these methods. This chapter explores the remarkable versatility of PINNs and related techniques by demonstrating their application to a wide array of complex, real-world, and interdisciplinary problems. Our objective is not to reiterate the core concepts, but to illustrate how they are extended, combined, and adapted to tackle challenges in advanced forward and inverse modeling, [data fusion](@entry_id:141454), [parametric analysis](@entry_id:634671), and model-based design. Through these examples, we will see that PINNs represent more than a mere function approximator; they constitute a powerful computational paradigm that merges physical laws, observational data, and the expressive power of [deep learning](@entry_id:142022).

### Advanced Forward Modeling: Coupled Systems and Non-Standard Physics

Most systems of scientific and engineering interest are characterized by the interplay of multiple physical phenomena. PINNs provide a naturally unified framework for solving such [coupled multiphysics](@entry_id:747969) problems, adeptly handling complex geometries, [material interfaces](@entry_id:751731), and even non-standard physical laws.

A primary architectural choice when modeling coupled systems on composite domains is whether to use a monolithic or a partitioned approach. In a **monolithic PINN**, a single, often multi-headed, neural network is used to represent all unknown fields across the entire domain. The training is performed jointly by minimizing a single [loss function](@entry_id:136784) that aggregates the residuals of all governing equations, boundary conditions, and [interface conditions](@entry_id:750725). This enforces all physical constraints simultaneously. In contrast, a **partitioned PINN** employs separate neural networks for the fields in different subdomains. These sub-networks must then communicate, either by directly enforcing interface continuity in the [loss function](@entry_id:136784) or through more sophisticated schemes inspired by classical [domain decomposition methods](@entry_id:165176). For instance, an auxiliary network can be introduced to represent a quantity on the interface, such as a flux, which then serves as a boundary condition for the adjacent sub-domain networks. This latter approach, analogous to a Dirichlet-Neumann decomposition, can offer greater flexibility and modularity, especially for problems with very disparate physics or scales across subdomains .

Regardless of the high-level architecture, the explicit enforcement of [interface physics](@entry_id:143998) is non-negotiable. For a coupled system defined on a domain $\Omega = \Omega_1 \cup \Omega_2$ with an interface $\Gamma$, the PINN loss function must include terms that penalize violations of the conditions on $\Gamma$, such as continuity of [primary fields](@entry_id:153633) ($u_1 - u_2 = 0$) and the balance of fluxes ($k_1 \nabla u_1 \cdot \mathbf{n}_1 + k_2 \nabla u_2 \cdot \mathbf{n}_2 = 0$). Simply minimizing the interior PDE residuals in $\Omega_1$ and $\Omega_2$ is insufficient, as the [interface conditions](@entry_id:750725) are independent constraints that are essential for defining a unique, physically correct solution . This principle is critical in applications like modeling [reaction-diffusion systems](@entry_id:136900) across [material interfaces](@entry_id:751731), where species concentrations and their diffusive fluxes must be consistently coupled .

The flexibility of the PINN framework extends beyond standard partial [differential operators](@entry_id:275037). Its reliance on [automatic differentiation](@entry_id:144512) to compute residuals from a symbolic representation of the governing equations allows it to tackle non-standard physics, including those described by integral or [non-local operators](@entry_id:752581). A prime example arises in the field of continuum mechanics with [peridynamics](@entry_id:191791), a non-local theory that models [material failure](@entry_id:160997) by replacing spatial derivatives with integral equations. A coupled peridynamic-heat system might involve a non-local momentum balance where the force on a point depends on the displacements of all points within a finite horizon, $\delta$. Directly incorporating the resulting integral into a PINN loss would require costly numerical quadrature at every collocation point and training iteration. However, for certain classes of problems, this computational bottleneck can be circumvented through a synergy of classical [applied mathematics](@entry_id:170283) and machine learning. By choosing a surrogate with a spectral basis (e.g., a Fourier series ansatz), the non-local [integral operator](@entry_id:147512) can be analytically converted into a multiplication in the [spectral domain](@entry_id:755169). The computationally expensive integral is replaced by an analytic "kernel transform" that depends only on the wavenumber and the peridynamic [kernel function](@entry_id:145324). This elegant approach makes the training of PINNs for certain non-local problems computationally feasible, demonstrating how domain knowledge can dramatically enhance the efficiency of ML-based methods .

### Inverse Problems and Parameter Identification

One of the most impactful applications of PINNs is in solving inverse problems, where the goal is to infer unknown parameters, coefficients, or source terms within a physical model from observational data. By incorporating the unknown quantities as trainable variables—either as global parameters or as the outputs of auxiliary neural networks—the PINN framework can learn the physics from data.

A canonical [inverse problem](@entry_id:634767) is the identification of a spatially varying material property, such as the diffusion coefficient $a(\mathbf{x})$ in the elliptic PDE $-\nabla \cdot (a(\mathbf{x})\nabla u) = f$. Two primary strategies exist for representing the unknown function $a(\mathbf{x})$ within a PINN. The first is a direct, high-dimensional [parameterization](@entry_id:265163), such as representing $a(\mathbf{x})$ as a piecewise constant field on a predefined mesh. This offers great flexibility and can capture sharp discontinuities, but it may require significant regularization and is prone to high variance or non-uniqueness in data-scarce regimes. The second strategy is to represent the unknown coefficient with another neural network, $a_\phi(\mathbf{x})$, whose parameters $\phi$ are trained alongside the parameters of the primary solution network $u_\theta(\mathbf{x})$. This approach implicitly regularizes the inverse problem by introducing an [inductive bias](@entry_id:137419) towards smooth coefficients, a characteristic property of neural networks. This often reduces variance and improves stability, at the cost of potential modeling bias if the true coefficient is non-smooth. Positivity constraints, essential for physical admissibility (e.g., positive diffusivity), can be easily enforced by applying a transformation like a softplus or [exponential function](@entry_id:161417) to the output of the network for $a_\phi(\mathbf{x})$ .

The success of such inverse problems hinges on identifiability. Jointly learning both the state $u(\mathbf{x})$ and the coefficient $a(\mathbf{x})$ from only the PDE residual and boundary conditions is often an underdetermined problem, as compensatory errors in $u_\theta$ and $a_\phi$ can conspire to minimize the residual. The availability of even sparse measurements of the state field $u$ can significantly constrain the problem and improve identifiability. In the absence of such data, leveraging multiple, distinct physical experiments can be a powerful strategy. By subjecting the system to different forcing terms or boundary conditions while assuming the underlying coefficient $a(\mathbf{x})$ remains the same, one generates a system of constraints that more robustly determines the unknown coefficient .

The optimization process itself can also be tailored for [inverse problems](@entry_id:143129). While first-order methods like Adam are common, the structure of the PINN loss lends itself to more advanced, [second-order optimization](@entry_id:175310) schemes. For instance, in inferring the thermal conductivity in the heat equation, a Gauss-Newton method can be formulated. This requires computing the Jacobian of the PDE [residual vector](@entry_id:165091) with respect to the unknown parameters. Thanks to [automatic differentiation](@entry_id:144512), these sensitivities can be obtained efficiently, allowing for the construction and solution of the Gauss-Newton system to obtain more effective update steps, which can accelerate convergence, especially as the optimization nears a solution .

### Data Fusion and Multi-Fidelity Modeling

In many engineering applications, simulations exist at varying levels of fidelity and cost. High-fidelity models are accurate but computationally expensive, while low-fidelity models are cheap but may neglect important physical couplings. Multi-fidelity [surrogate modeling](@entry_id:145866) aims to fuse information from these different sources to build a surrogate that is more accurate than one built on low-fidelity data alone, yet requires fewer high-fidelity samples than a purely high-fidelity approach.

Several philosophies exist for this fusion. A powerful statistical approach is **[co-kriging](@entry_id:747413)**, a form of multi-output Gaussian Process regression. In its common auto-regressive form, the high-fidelity model $f_H$ is represented as a scaled version of the low-fidelity model $f_L$ plus a discrepancy function $\delta$: $f_H(\mathbf{x}) = \rho f_L(\mathbf{x}) + \delta(\mathbf{x})$. The scalar $\rho$ and the functions $f_L$ and $\delta$ are modeled with Gaussian Process priors. Given abundant low-fidelity data and sparse high-fidelity data, this model efficiently learns the low-fidelity function and the small discrepancy, transferring information from the low-fidelity data to the high-fidelity prediction through the learned cross-covariance structure. This approach is particularly sample-efficient when the two fidelities are strongly linearly correlated and the discrepancy is a smooth, simple function . The [optimal scaling](@entry_id:752981) factor $\rho$ that minimizes the [mean-square error](@entry_id:194940) can be derived from first principles of linear [estimation theory](@entry_id:268624) as the ratio of the covariance between the fields to the variance of the low-fidelity field, i.e.,
$$\rho^\star = \operatorname{Cov}[f_L, f_H] / \operatorname{Var}[f_L]$$
.

This statistical fusion should be carefully distinguished from **[transfer learning](@entry_id:178540)**, a common technique in deep learning. In [transfer learning](@entry_id:178540), a neural network is first pre-trained on the large low-fidelity dataset and then fine-tuned on the small high-fidelity dataset. Here, the low-fidelity information is only used to provide a better initialization for the network's parameters; the low-fidelity model is not an active component of the final surrogate during inference . Other hybrid approaches like **Deep Kernel Learning (DKL)** use a neural network to learn a feature mapping from the input space to a latent space, where a Gaussian Process is then applied. This combines the [expressive power](@entry_id:149863) of [deep learning](@entry_id:142022) for [feature extraction](@entry_id:164394) with the [uncertainty quantification](@entry_id:138597) capabilities of GPs.

The choice of method depends on the relationship between the fidelities. If the low-fidelity model is qualitatively wrong (e.g., by omitting a key physical coupling like piezoelectricity in a thermo-electro-mechanical system), its features may be misaligned with the high-fidelity physics. In such cases, a DKL model that shares features across fidelities risks "[negative transfer](@entry_id:634593)," where the low-fidelity data harms rather than helps. A hierarchical [co-kriging](@entry_id:747413) model can be more robust, as its additive discrepancy term $\delta(\mathbf{x})$ is flexible enough to absorb the complex, non-linear error, effectively learning to ignore the unhelpful low-fidelity information . A physics-informed deep surrogate, on the other hand, relies on the correctness of the embedded physical laws. It can be made robust to mis-specified boundary priors by enforcing boundary conditions as hard constraints in the [network architecture](@entry_id:268981), but it will inevitably produce biased results if the constitutive coefficients (e.g., elastic stiffness, thermal expansion) used in its physics-based loss are incorrect .

### Operator Learning for Parametric Systems

Beyond solving single problem instances, a grand challenge in computational science is to learn the entire solution operator—the mapping from input parameters or functions to the solution functions. Operator learning models, such as the Deep Operator Network (DeepONet), are designed for this task. When combined with a physics-informed loss, they become a powerful tool for creating parametric surrogates that can be queried in near real-time.

A DeepONet consists of two main components: a *branch network* that takes the input parameters or functions (e.g., a material property field, a boundary condition function) and produces a set of latent coefficients, and a *trunk network* that takes the coordinates of the output function's domain (e.g., position $\mathbf{x}$ and time $t$) and produces a set of basis functions. The final output is the dot product of these two components. By training this architecture over a diverse ensemble of input parameters while enforcing the governing physical laws via a PINN-style loss, the network learns an approximation of the solution operator itself .

This approach is particularly powerful for complex, coupled problems like elastoplastic-thermal systems. The operator network can learn to predict the displacement and temperature fields for new, unseen material property fields. The primary advantage is the amortization of computational cost: after a very expensive, one-time offline training phase, the evaluation of the surrogate for a new parameter set is a single, rapid forward pass. This is a dramatic speed-up compared to re-training a standard PINN or re-running a traditional numerical solver for every new query, making [operator learning](@entry_id:752958) ideal for many-query applications like [uncertainty quantification](@entry_id:138597), optimization, and [inverse problems](@entry_id:143129). Crucially, for highly [nonlinear physics](@entry_id:187625) such as plasticity, the physics-informed loss must include terms that enforce the associated constraints, like the yield [admissibility condition](@entry_id:200767) and the [flow rule](@entry_id:177163), to ensure the learned operator produces physically realistic solutions .

### Advanced Model Analysis and Design

The utility of PINNs extends beyond simply generating solutions. The framework's core components—in particular, its differentiability—can be leveraged for deeper model analysis and even to guide the experimental process itself.

One of the most exciting frontiers is **Optimal Experimental Design (OED)**. The goal of OED is to determine which measurements should be taken to gain the most information about a system or its parameters. In a model-based setting, this can be quantified using the Fisher Information Matrix (FIM), which measures the sensitivity of the model's predictions to its parameters. A key ingredient for the FIM is the vector of sensitivities of the [observables](@entry_id:267133) with respect to the parameters, $\partial u / \partial \theta$. In a PINN, these sensitivities are readily available through [automatic differentiation](@entry_id:144512). By computing these sensitivities at various candidate measurement locations, one can construct the FIM for any proposed set of experiments. A common objective is D-optimality, which seeks to maximize the determinant of the FIM, as this corresponds to minimizing the volume of the confidence [ellipsoid](@entry_id:165811) for the parameter estimates. A PINN-based framework thus allows for the computational search for an [optimal experimental design](@entry_id:165340)—a selection of measurements that maximizes [parameter identifiability](@entry_id:197485) under practical constraints like a limited budget .

PINNs can also be situated within the broader landscape of data-driven methods for **dynamical [systems analysis](@entry_id:275423)**. For instance, their properties can be compared with those of Koopman [operator theory](@entry_id:139990), which seeks to find a nonlinear transformation of a system's [state variables](@entry_id:138790) into a latent space where the dynamics evolve linearly. An interpretable [modal decomposition](@entry_id:637725) of the dynamics can be obtained from the [eigenvalues and eigenfunctions](@entry_id:167697) of the Koopman operator, providing information about frequencies and growth/decay rates of physical modes. This offers a level of interpretability that the internal weights of a standard PINN typically lack. However, the promise of an exact finite-dimensional [linearization](@entry_id:267670) is only realized for a limited class of systems. For strongly nonlinear or [chaotic dynamics](@entry_id:142566), the Koopman spectrum becomes continuous, and a finite-dimensional linear model can only be an approximation. Both PINNs and Koopman models face challenges when the observed [state variables](@entry_id:138790) do not form a complete Markovian state, which can occur when a system is a reduction of a higher-dimensional one. In such cases, the dynamics exhibit memory effects that cannot be captured without augmenting the state, for instance, by using time-delay coordinates .

### Interdisciplinary Transfer and Hybrid Modeling

The principles underlying PINNs are not restricted to classical mechanics or engineering. Their fundamental nature as a tool for solving differential equations makes them broadly applicable across scientific disciplines and enables the construction of novel hybrid models.

One powerful paradigm is the creation of **hybrid physical-data models**, where a learned component is embedded within a larger, physics-based framework. This is particularly relevant for phenomena that are too complex to model from first principles, such as turbulence. For example, in a simulation of [reacting flow](@entry_id:754105), a machine learning surrogate can be trained to act as a subgrid-scale closure model, predicting the effects of unresolved turbulent fluctuations on the resolved flow and chemistry. This learned closure can then be embedded within a PINN that solves the governing equations for the resolved scales. A critical aspect of this approach is ensuring that the learned component respects fundamental physical laws. The loss function for the subgrid surrogate should include terms that enforce constraints like [conservation of energy](@entry_id:140514). Failing to do so can lead to an unphysical model that introduces spurious sources or sinks, corrupting the [global solution](@entry_id:180992) and yielding incorrect predictions for quantities of interest like the [laminar flame speed](@entry_id:202145) .

Furthermore, the numerical strategies developed for PINNs often exhibit remarkable **cross-domain transferability**. A technique refined for balancing loss terms in a fluid-thermal problem, for instance, may prove effective in a completely different context. An example is the application of PINNs to [epidemiology](@entry_id:141409), where a system of [reaction-diffusion equations](@entry_id:170319) can model the spatial spread of a disease through susceptible and infected populations. The equations for the susceptible and infected fractions can have very different [characteristic scales](@entry_id:144643), leading to an imbalance during training. An adaptive residual weighting scheme, such as one that weights each equation's loss term by the inverse of its root-mean-square residual, can effectively balance the gradients and stabilize training. The successful transfer of such a numerical "trick" from computational fluid dynamics to epidemiology highlights the abstract and general power of the underlying mathematical framework .

In summary, the applications discussed in this chapter demonstrate that machine learning-based surrogates and PINNs are a profoundly versatile toolset. They provide novel pathways for solving complex coupled systems, discovering physical laws from data, fusing information from disparate sources, enabling rapid parametric design, and analyzing system behavior. Their true potential is realized when they are not treated as black-box replacements for traditional methods, but as a flexible computational framework that is deeply integrated with physical principles, domain knowledge, and the rich history of [applied mathematics](@entry_id:170283).