{
    "hands_on_practices": [
        {
            "introduction": "在求解偏微分方程时，施加边界条件是至关重要的一步。本练习将引导你通过构造一个在边界上为零的距离函数，来探索一种在物理信息神经网络（PINN）中“硬性”施加狄利克雷边界条件的方法。通过解析验证该方法的有效性，你将深入理解如何通过网络结构设计来确保解自动满足物理约束，这是提升PINN模型稳定性和准确性的关键技巧。",
            "id": "3513307",
            "problem": "考虑一个圆形域 $ \\Omega \\subset \\mathbb{R}^{2} $ 上的双场多物理场模型，该圆形域半径为 $R > 0$ 且以原点为中心，其中一个场 $ u $ 必须满足齐次狄利克雷边界条件 $ u|_{\\partial \\Omega} = 0 $。在物理信息神经网络 (PINN) 框架中，通常通过将未知解重新参数化为一个边界消失距离函数与一个自由神经网络拟设的乘积，来强制施加狄利克雷边界条件。令 $ \\hat{u}_{\\theta} : \\mathbb{R}^{2} \\to \\mathbb{R} $ 表示一个在闭包 $ \\overline{\\Omega} $ 上连续的神经网络，并对 $ x \\in \\Omega $ 定义 $ u(x) = d(x) \\, \\hat{u}_{\\theta}(x) $，其中 $ d(x) $ 是一个待从第一性原理确定的标量函数。\n\n从欧几里得范数、圆形域以及 $ H^{1}(\\Omega) $ 函数的迹的基本定义出发，完成以下任务：\n\n1. 推导一个利普希茨连续函数 $ d : \\overline{\\Omega} \\to \\mathbb{R} $ 的显式公式，该函数恰好在边界 $ \\partial \\Omega $ 上为零，并在内部非负。仅使用 $ \\Omega = \\{ x \\in \\mathbb{R}^{2} : \\| x \\|_{2}  R \\} $ 和欧几里得范数 $ \\| x \\|_{2} $ 的定义。\n\n2. 使用迹算子的定义以及 $ d $ 和 $ \\hat{u}_{\\theta} $ 在 $ \\overline{\\Omega} $ 上的连续性，解析地验证重新参数化的 $ u(x) = d(x) \\, \\hat{u}_{\\theta}(x) $ 满足狄利克雷边界条件 $ u|_{\\partial \\Omega} = 0 $。将边界值 $ u|_{\\partial \\Omega} $ 显式地计算为单个实数或单个闭式解析表达式。\n\n以精确形式给出最终的边界值。无需四舍五入，最终答案中也无需物理单位。",
            "solution": "所述问题具有科学依据、是适定的、客观且自洽的。在物理信息神经网络 (PINN) 的背景下，这是一个验证狄利克雷边界条件施加的有效练习。我们将进行严谨的、分步的解答。\n\n问题分为两个部分。首先，我们必须推导一个在圆形域边界上为零的函数 $d(x)$。其次，我们必须使用这个函数来验证重新参数化的场 $u(x)$ 满足齐次狄利克雷边界条件。\n\n### 第1部分：边界消失函数 $d(x)$ 的推导\n\n目标是找到一个函数 $d : \\overline{\\Omega} \\to \\mathbb{R}$ 的显式公式，该函数具有以下性质：\n1.  它在闭域 $\\overline{\\Omega}$ 上是利普希茨连续的。\n2.  当且仅当一个点位于边界上时，它才为零，即 $d(x) = 0 \\iff x \\in \\partial\\Omega$。\n3.  对于内部的所有点，它都是非负的，即对于 $x \\in \\Omega$，有 $d(x) \\ge 0$。\n\n该域是一个以原点为中心、半径为 $R > 0$ 的圆，定义为 $\\Omega = \\{ x \\in \\mathbb{R}^{2} : \\| x \\|_{2}  R \\}$。对应的闭域是 $\\overline{\\Omega} = \\{ x \\in \\mathbb{R}^{2} : \\| x \\|_{2} \\leq R \\}$，其边界是 $\\partial\\Omega = \\{ x \\in \\mathbb{R}^{2} : \\| x \\|_{2} = R \\}$。问题要求只使用这些定义和欧几里得范数 $\\|x\\|_2$。\n\n构造这样一个函数的一种自然方法是考虑点 $x$ 到边界的距离。对于以原点为中心的圆形域这一特定情况，点 $x \\in \\overline{\\Omega}$ 沿径向到边界的距离由圆的半径 $R$ 与该点到原点的距离 $\\|x\\|_2$ 之差给出。\n\n让我们提出以下函数：\n$$\nd(x) = R - \\|x\\|_{2}\n$$\n我们现在验证该函数是否满足所需准则。\n\n1.  **利普希茨连续性**：我们需要证明存在一个常数 $L \\geq 0$，使得对于所有 $x_1, x_2 \\in \\overline{\\Omega}$，不等式 $|d(x_1) - d(x_2)| \\leq L \\|x_1 - x_2\\|_{2}$ 成立。\n    让我们计算绝对差：\n    $$\n    |d(x_1) - d(x_2)| = |(R - \\|x_1\\|_{2}) - (R - \\|x_2\\|_{2})| = |\\|x_2\\|_{2} - \\|x_1\\|_{2}|\n    $$\n    根据范数的反三角不等式，我们有 $|\\|x_1\\|_{2} - \\|x_2\\|_{2}| \\leq \\|x_1 - x_2\\|_{2}$。\n    因此，我们可以写出：\n    $$\n    |d(x_1) - d(x_2)| \\leq \\|x_1 - x_2\\|_{2}\n    $$\n    这是利普希茨连续性的定义，其中利普希茨常数为 $L=1$。因此，$d(x)$ 在 $\\mathbb{R}^2$ 上是利普希茨连续的，因此在其子域 $\\overline{\\Omega}$ 上也是利普希茨连续的。\n\n2.  **在边界上为零**：\n    - 如果 $x \\in \\partial\\Omega$，那么根据定义 $\\|x\\|_{2} = R$。在这种情况下，$d(x) = R - R = 0$。\n    - 反之，如果对于某个 $x \\in \\overline{\\Omega}$ 有 $d(x) = 0$，那么 $R - \\|x\\|_{2} = 0$，这意味着 $\\|x\\|_{2} = R$。根据定义，这表示 $x \\in \\partial\\Omega$。\n    因此，$d(x) = 0$ 当且仅当 $x \\in \\partial\\Omega$。\n\n3.  **在内部的非负性**：\n    如果 $x \\in \\Omega$，那么根据定义 $\\|x\\|_{2}  R$。这意味着 $R - \\|x\\|_{2} > 0$。问题要求非负性，而我们的函数在内部是严格为正的，这满足了条件。对于 $x \\in \\partial\\Omega$，$d(x)=0$，所以 $d(x)$ 在整个闭包 $\\overline{\\Omega}$ 上都是非负的。\n\n函数 $d(x) = R - \\|x\\|_{2}$ 满足所有要求的性质。\n\n### 第2部分：狄利克雷边界条件的验证\n\n我们已知重新参数化 $u(x) = d(x) \\, \\hat{u}_{\\theta}(x)$，其中 $d(x)$ 是在第1部分中推导的函数，而 $\\hat{u}_{\\theta}(x)$ 是一个假设在 $\\overline{\\Omega}$ 上连续的神经网络。我们必须证明这个函数 $u(x)$ 满足齐次狄利克雷边界条件 $u|_{\\partial \\Omega} = 0$。\n\n该函数显式为：\n$$\nu(x) = (R - \\|x\\|_{2}) \\, \\hat{u}_{\\theta}(x)\n$$\n问题陈述中提到了 $H^1(\\Omega)$ 函数的迹算子。对于一个在具有足够正则边界（如我们的圆形）的域的闭包上连续的函数 $v$（即 $v \\in C^0(\\overline{\\Omega})$），迹算子 $\\gamma(v) = v|_{\\partial\\Omega}$ 就是该函数在边界上的限制。也就是说，对于边界上的任意点 $x_b \\in \\partial\\Omega$，迹的值由 $\\lim_{x \\to x_b, x \\in \\Omega} v(x)$ 给出。由于 $v$ 在 $\\overline{\\Omega}$ 上是连续的，这个极限就是 $v(x_b)$。\n\n在我们的例子中，函数 $u(x)$ 是紧集 $\\overline{\\Omega}$ 上两个连续函数的乘积：\n- $d(x) = R - \\|x\\|_2$，它已被证明是利普希茨连续的，因此是连续的。\n- $\\hat{u}_{\\theta}(x)$，已知是连续的。\n\n两个连续函数的乘积是连续的。因此，$u(x) \\in C^0(\\overline{\\Omega})$。为了计算边界值 $u|_{\\partial\\Omega}$，我们可以对边界 $\\partial\\Omega$ 上的任意点 $x_b$ 计算 $u(x)$ 的值。\n\n设 $x_b$ 是满足 $x_b \\in \\partial\\Omega$ 的任意点。根据定义，$\\|x_b\\|_{2} = R$。\n$u$ 在此点的值为：\n$$\nu(x_b) = d(x_b) \\, \\hat{u}_{\\theta}(x_b)\n$$\n从第1部分我们知道 $d(x_b) = R - \\|x_b\\|_{2} = R - R = 0$。\n将此代入 $u(x_b)$ 的表达式中：\n$$\nu(x_b) = 0 \\cdot \\hat{u}_{\\theta}(x_b)\n$$\n由于 $\\hat{u}_{\\theta}$ 在紧集 $\\overline{\\Omega}$ 上是连续的，所以它是有界的。这意味着 $\\hat{u}_{\\theta}(x_b)$ 是一个有限实数。0 与任何有限实数的乘积都是 0。\n$$\nu(x_b) = 0\n$$\n由于 $x_b$ 是边界 $\\partial\\Omega$ 上的一个任意点，这个结果对边界上的所有点都成立。因此，函数 $u(x)$ 在 $\\partial\\Omega$ 上恒等于零。我们将其写为：\n$$\nu|_{\\partial\\Omega} = 0\n$$\n问题要求将此边界值表示为单个实数。函数 $u(x)$ 对于所有 $x \\in \\partial\\Omega$ 都取常数值 $0$。因此，边界值为 $0$。",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "物理信息神经网络（PINN）的核心思想在于最小化物理残差，即网络输出在多大程度上违反了控制偏微分方程（PDE）。此过程需要利用自动微分技术计算网络输出对其输入的各阶导数，并将其代入PDE。本练习将让你亲手实现这一核心计算过程，你将为一个给定的神经网络结构计算伯格斯方程的物理残差，从而深入理解PINN是如何“感知”和学习物理定律的。",
            "id": "3513283",
            "problem": "考虑一维粘性伯格斯方程（viscous Burgers equation），这是一个流体力学中的典型模型，由动量守恒和扩散输运推导得出。该方程由标量场 $u(x,t)$ 在一个时空域上给出：$u_t + u u_x - \\nu u_{xx} = 0$。设运动粘度固定为 $\\nu = 0.01/\\pi$。目标是，对于一个给定的网络拟设（network ansatz）和一组配置点（collocation points），使用与计算图一致的程序化微分（即自动微分）来评估物理残差 $r(x,t)$，并总结该配置点集上的残差统计数据。\n\n你必须从控制偏微分方程和微积分的链式法则出发，并将网络拟设视为一个可微代理模型。残差定义为 $r(x,t) = u_t(x,t) + u(x,t)\\, u_x(x,t) - \\nu\\, u_{xx}(x,t)$。网络拟设指定为带有双曲正切激活函数的单隐藏层形式：\n$$\nu(x,t) = \\sum_{j=1}^{m} a_j\\, \\phi(z_j(x,t)), \\quad \\phi(z) = \\tanh(z), \\quad z_j(x,t) = b_j x + c_j t + d_j,\n$$\n其中 $m$ 是神经元的数量，$a_j$、$b_j$、$c_j$、$d_j$ 是固定参数。\n\n你必须通过拟设定义的计算图，利用链式法则（即概念上使用自动微分，但在代码中显式表达）来计算 $u(x,t)$ 及其导数 $u_x(x,t)$、$u_t(x,t)$ 和 $u_{xx}(x,t)$，从而实现残差计算。某一点的残差范数是其绝对值 $|r(x,t)|$。\n\n定义三个测试用例，它们具有不同的配置点集和网络参数化，全部定义在域 $x \\in [0,1]$ 和 $t \\in [0,1]$ 上：\n\n- 测试用例 1（通用内部采样，“理想路径”）：\n  - 神经元数量 $m = 5$，参数如下：\n    - $a = [1.0, -0.5, 0.3, 0.7, -0.2]$,\n    - $b = [2.0, -1.0, 0.5, 3.0, -2.5]$,\n    - $c = [-1.5, 0.7, 1.2, -0.8, 0.9]$,\n    - $d = [0.1, -0.2, 0.3, -0.4, 0.5]$.\n  - 配置点：$N=1024$ 个点 $(x_i,t_i)$，使用固定的伪随机种子 $123$ 以确保可复现性，从 $[0,1]\\times[0,1]$ 中独立均匀采样。即，对 $i=1,\\dots,1024$，独立地抽取 $x_i \\sim \\mathcal{U}(0,1)$ 和 $t_i \\sim \\mathcal{U}(0,1)$。\n\n- 测试用例 2（边界集中采样）：\n  - 神经元数量 $m = 3$，参数如下：\n    - $a = [0.8, -0.6, 0.4]$,\n    - $b = [5.0, -4.0, 3.0]$,\n    - $c = [2.0, -1.0, 0.5]$,\n    - $d = [-0.1, 0.2, -0.3]$.\n  - 配置点：沿每个边界（包括端点）选取 $K=64$ 个等距点。具体来说，通过连接四个边界集合来组装点集：\n    - 下边界：$(x,t)=(x_k, 0)$，其中 $x_k$ 来自 $\\text{linspace}(0,1,K)$，\n    - 上边界：$(x,t)=(x_k, 1)$，其中 $x_k$ 来自 $\\text{linspace}(0,1,K)$，\n    - 左边界：$(x,t)=(0, t_k)$，其中 $t_k$ 来自 $\\text{linspace}(0,1,K)$，\n    - 右边界：$(x,t)=(1, t_k)$，其中 $t_k$ 来自 $\\text{linspace}(0,1,K)$。\n    这将产生 $N=4K=256$ 个配置点（角点会出现在多个边界集中，这是可接受的）。\n\n- 测试用例 3（分层内部采样与小振幅网络，边缘情况）：\n  - 神经元数量 $m = 4$，参数如下：\n    - $a = [0.05, -0.03, 0.02, -0.04]$,\n    - $b = [8.0, -7.0, 6.0, -5.0]$,\n    - $c = [4.0, -3.0, 2.0, -1.0]$,\n    - $d = [0.0, 0.1, -0.1, 0.2]$.\n  - 配置点：构建一个每轴有 $Q=16$ 个子区间的分层网格；使用子单元的中心。具体来说，对于 $i=0,\\dots,15$ 和 $j=0,\\dots,15$，定义点 $(x_{ij}, t_{ij})$ 为 $x_{ij} = (i+0.5)/Q$ 和 $t_{ij} = (j+0.5)/Q$。这将产生 $N=Q^2=256$ 个点。\n\n对于每个测试用例：\n- 计算所有配置点上的残差 $r(x,t)$。\n- 计算这些点上的残差范数 $|r(x,t)|$。\n- 报告两个浮点数：配置点集上 $|r|$ 的平均值和 $|r|$ 的最大值。\n\n所有中间计算必须遵循上述定义，最终结果必须汇总三个测试用例的产出。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。要求的格式为 $[m_1,M_1,m_2,M_2,m_3,M_3]$，其中 $m_i$ 是测试用例 $i$ 的平均残差范数，$M_i$ 是测试用例 $i$ 的最大残差范数。不涉及任何物理单位；所有量均为无量纲实数。",
            "solution": "该问题要求针对一维粘性伯格斯方程，在给定一个用于解场 $u(x,t)$ 的特定神经网络拟设的情况下，评估其物理残差。评估必须针对三个不同的测试用例进行，每个用例都有其自身的网络参数化和配置点集。任务的核心是通过显式应用微积分的链式法则，来计算网络输出关于其输入 $x$ 和 $t$ 的必要偏导数，而链式法则是自动微分背后的基本原理。\n\n控制偏微分方程（PDE）是粘性伯格斯方程：\n$$\nu_t + u u_x - \\nu u_{xx} = 0\n$$\n其中 $u_t = \\frac{\\partial u}{\\partial t}$，$u_x = \\frac{\\partial u}{\\partial x}$，以及 $u_{xx} = \\frac{\\partial^2 u}{\\partial x^2}$。运动粘度为给定常数 $\\nu = 0.01/\\pi$。\n\n物理残差（记为 $r(x,t)$）是将网络拟设代入PDE后得到的值。它量化了在任意给定点 $(x,t)$，该拟设满足控制方程的程度：\n$$\nr(x,t) = u_t(x,t) + u(x,t)\\, u_x(x,t) - \\nu\\, u_{xx}(x,t)\n$$\n\n网络拟设是一个带有双曲正切激活函数 $\\phi(z) = \\tanh(z)$ 的单隐藏层神经网络。输出 $u(x,t)$ 是 $m$ 个神经元激活值的线性组合：\n$$\nu(x,t) = \\sum_{j=1}^{m} a_j\\, \\phi(z_j(x,t))\n$$\n每个神经元的输入 $z_j$ 是空间和时间坐标的仿射变换：\n$$\nz_j(x,t) = b_j x + c_j t + d_j\n$$\n对于每个测试用例，参数 $a_j, b_j, c_j, d_j$（其中 $j=1, \\dots, m$）均已提供并固定。\n\n为了计算残差 $r(x,t)$，我们必须求出网络拟设的偏导数 $u_t$、$u_x$ 和 $u_{xx}$。这通过在拟设定义的计算图上应用链式法则来实现。\n\n首先，我们确定激活函数 $\\phi(z) = \\tanh(z)$ 关于其自变量 $z$ 的导数：\n- 一阶导数：$\\phi'(z) = \\frac{d}{dz} \\tanh(z) = \\text{sech}^2(z) = 1 - \\tanh^2(z)$。用我们的记法，即为 $\\phi'(z) = 1 - \\phi(z)^2$。\n- 二阶导数：$\\phi''(z) = \\frac{d}{dz} (1 - \\tanh^2(z)) = -2 \\tanh(z) \\cdot \\text{sech}^2(z) = -2\\phi(z)(1-\\phi(z)^2)$。用我们的记法，即为 $\\phi''(z) = -2\\phi(z)\\phi'(z)$。\n\n接下来，我们计算网络输出 $u(x,t)$ 的偏导数。由于微分是线性算子，我们可以对和的每一项逐项求导。\n\n- 关于时间的偏导数 $u_t$：\n$$\nu_t(x,t) = \\frac{\\partial}{\\partial t} \\sum_{j=1}^{m} a_j \\phi(z_j) = \\sum_{j=1}^{m} a_j \\frac{d\\phi}{dz_j} \\frac{\\partial z_j}{\\partial t} = \\sum_{j=1}^{m} a_j \\phi'(z_j) c_j\n$$\n\n- 关于空间的偏导数 $u_x$：\n$$\nu_x(x,t) = \\frac{\\partial}{\\partial x} \\sum_{j=1}^{m} a_j \\phi(z_j) = \\sum_{j=1}^{m} a_j \\frac{d\\phi}{dz_j} \\frac{\\partial z_j}{\\partial x} = \\sum_{j=1}^{m} a_j \\phi'(z_j) b_j\n$$\n\n- 关于空间的二阶偏导数 $u_{xx}$：\n$$\nu_{xx}(x,t) = \\frac{\\partial}{\\partial x} u_x(x,t) = \\frac{\\partial}{\\partial x} \\sum_{j=1}^{m} a_j b_j \\phi'(z_j) = \\sum_{j=1}^{m} a_j b_j \\frac{d\\phi'}{dz_j} \\frac{\\partial z_j}{\\partial x} = \\sum_{j=1}^{m} a_j b_j \\phi''(z_j) b_j = \\sum_{j=1}^{m} a_j b_j^2 \\phi''(z_j)\n$$\n\n将这些表达式代入残差定义，得到在任意点 $(x,t)$ 处残差的完整公式：\n$$\nr(x,t) = \\left(\\sum_{j=1}^{m} a_j c_j \\phi'(z_j)\\right) + \\left(\\sum_{j=1}^{m} a_j \\phi(z_j)\\right)\\left(\\sum_{j=1}^{m} a_j b_j \\phi'(z_j)\\right) - \\nu \\left(\\sum_{j=1}^{m} a_j b_j^2 \\phi''(z_j)\\right)\n$$\n其中 $z_j = z_j(x,t)$。\n\n每个测试用例的计算流程如下：\n1.  定义运动粘度 $\\nu$ 和网络参数 $a_j, b_j, c_j, d_j$。\n2.  生成指定的 $N$ 个配置点 $(x_i, t_i)$ 集合。\n3.  对所有配置点执行向量化计算。对于每个点 $(x_i, t_i)$ 和每个神经元 $j \\in \\{1, \\dots, m\\}$：\n    a. 计算 $z_{ij} = b_j x_i + c_j t_i + d_j$。\n    b. 评估激活值 $\\phi(z_{ij})$ 及其导数 $\\phi'(z_{ij})$ 和 $\\phi''(z_{ij})$。\n4.  对神经元求和，以找出每个配置点上 $u, u_t, u_x, u_{xx}$ 的值。\n5.  计算每个点的残差 $r_i$。\n6.  计算残差范数 $|r_i|$。\n7.  最后，计算所有配置点上残差范数的平均值和最大值：\n$$\n\\text{mean}(|r|) = \\frac{1}{N} \\sum_{i=1}^{N} |r(x_i, t_i)|, \\quad \\text{max}(|r|) = \\max_{i=1, \\dots, N} |r(x_i, t_i)|\n$$\n该流程针对三个指定的测试用例实现，并报告得到的六个浮点数（每个用例的平均值和最大值）。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the mean and maximum of the physics residual norm for the 1D viscous\n    Burgers' equation for three different test cases involving a neural network ansatz.\n    \"\"\"\n    \n    # Define the constant kinematic viscosity\n    nu = 0.01 / np.pi\n\n    # Define the parameters for the three test cases\n    test_cases = [\n        # Test Case 1: General interior sampling\n        {\n            \"m\": 5,\n            \"params\": {\n                \"a\": np.array([[1.0, -0.5, 0.3, 0.7, -0.2]]),\n                \"b\": np.array([[2.0, -1.0, 0.5, 3.0, -2.5]]),\n                \"c\": np.array([[-1.5, 0.7, 1.2, -0.8, 0.9]]),\n                \"d\": np.array([[0.1, -0.2, 0.3, -0.4, 0.5]]),\n            },\n            \"sampling\": {\n                \"type\": \"uniform\",\n                \"N\": 1024,\n                \"seed\": 123\n            }\n        },\n        # Test Case 2: Boundary-focused sampling\n        {\n            \"m\": 3,\n            \"params\": {\n                \"a\": np.array([[0.8, -0.6, 0.4]]),\n                \"b\": np.array([[5.0, -4.0, 3.0]]),\n                \"c\": np.array([[2.0, -1.0, 0.5]]),\n                \"d\": np.array([[-0.1, 0.2, -0.3]]),\n            },\n            \"sampling\": {\n                \"type\": \"boundary\",\n                \"K\": 64\n            }\n        },\n        # Test Case 3: Stratified interior sampling\n        {\n            \"m\": 4,\n            \"params\": {\n                \"a\": np.array([[0.05, -0.03, 0.02, -0.04]]),\n                \"b\": np.array([[8.0, -7.0, 6.0, -5.0]]),\n                \"c\": np.array([[4.0, -3.0, 2.0, -1.0]]),\n                \"d\": np.array([[0.0, 0.1, -0.1, 0.2]]),\n            },\n            \"sampling\": {\n                \"type\": \"stratified\",\n                \"Q\": 16\n            }\n        }\n    ]\n\n    results = []\n    \n    # Process each test case\n    for case in test_cases:\n        # 1. Unpack parameters\n        params = case[\"params\"]\n        a, b, c, d = params[\"a\"], params[\"b\"], params[\"c\"], params[\"d\"]\n        sampling_config = case[\"sampling\"]\n\n        # 2. Generate collocation points\n        if sampling_config[\"type\"] == \"uniform\":\n            N = sampling_config[\"N\"]\n            seed = sampling_config[\"seed\"]\n            rng = np.random.default_rng(seed)\n            x = rng.uniform(0.0, 1.0, N)\n            t = rng.uniform(0.0, 1.0, N)\n        elif sampling_config[\"type\"] == \"boundary\":\n            K = sampling_config[\"K\"]\n            x_k = np.linspace(0, 1, K)\n            t_k = np.linspace(0, 1, K)\n            \n            x_bottom = x_k\n            t_bottom = np.zeros(K)\n\n            x_top = x_k\n            t_top = np.ones(K)\n\n            x_left = np.zeros(K)\n            t_left = t_k\n            \n            x_right = np.ones(K)\n            t_right = t_k\n            \n            x = np.concatenate((x_bottom, x_top, x_left, x_right))\n            t = np.concatenate((t_bottom, t_top, t_left, t_right))\n        elif sampling_config[\"type\"] == \"stratified\":\n            Q = sampling_config[\"Q\"]\n            ticks = (np.arange(Q) + 0.5) / Q\n            x_grid, t_grid = np.meshgrid(ticks, ticks)\n            x = x_grid.flatten()\n            t = t_grid.flatten()\n        \n        # Reshape inputs for broadcasting\n        x_col = x.reshape(-1, 1)\n        t_col = t.reshape(-1, 1)\n\n        # 3. Compute ansatz and its derivatives\n        # z_j(x,t) = b_j*x + c_j*t + d_j\n        z = x_col * b + t_col * c + d\n\n        # Activation function and its derivatives\n        phi_z = np.tanh(z)\n        phi_prime_z = 1.0 - phi_z**2\n        phi_double_prime_z = -2.0 * phi_z * phi_prime_z\n\n        # Compute u and its partial derivatives via chain rule\n        u = np.sum(a * phi_z, axis=1)\n        u_t = np.sum(a * c * phi_prime_z, axis=1)\n        u_x = np.sum(a * b * phi_prime_z, axis=1)\n        u_xx = np.sum(a * b**2 * phi_double_prime_z, axis=1)\n\n        # 4. Calculate the physics residual\n        # r = u_t + u*u_x - nu*u_xx\n        r = u_t + u * u_x - nu * u_xx\n\n        # 5. Compute residual norm statistics\n        r_norms = np.abs(r)\n        mean_norm = np.mean(r_norms)\n        max_norm = np.max(r_norms)\n        \n        results.extend([mean_norm, max_norm])\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "这个练习将你从PINN引向另一种强大的代理模型——高斯过程（GP），并介绍主动学习中的一个核心概念。你将学习如何利用GP的预测不确定性，通过最大化“预期提升”采集函数来智能地选择下一个采样点，这对于在昂贵的仿真预算下高效构建精确的代理模型至关重要。",
            "id": "3513339",
            "problem": "考虑一个一维、稳态、耦合多物理场模型，该模型在一个无量纲域中通过标量耦合参数 $\\theta \\in [0,1]$ 将热传导和物质扩散联系起来。温度场 $T(x)$ 和浓度场 $C(x)$ 通过平衡定律 $-k\\, T''(x) + \\beta\\, \\theta\\, C(x) = 0$（在 $x \\in [0,1]$ 上）相关联，其中 $k0$ 是热导率，$\\beta0$ 是耦合系数。假设物质浓度场 $C(x) = \\sin(\\pi x)$ 已知，且 $T(x)$ 满足齐次边界条件，当温度场用谐波模式 $T(x) = \\sin(2\\pi x)$ 近似时，这导出了一个解析的温度场族和一个由耦合平衡的平方残差（在 $x \\in [0,1]$ 上积分）定义的目标量 $J(\\theta)$。根据正弦函数在 $[0,1]$ 上的正交性，可得到其闭式解 $J(\\theta) = A + B\\, \\theta^2$，其中 $A = \\frac{k^2 (2\\pi)^4}{2}$ 且 $B = \\frac{\\beta^2}{2}$。\n\n您将为 $J(\\theta)$ 设计一个使用平方指数核的高斯过程（GP）代理模型，并选择下一个采样点位置，以最大化在降低代理模型当前最大绝对误差方面的期望提升。其目的是在采样预算下，从第一性原理出发，形式化并实现一个有原则的采集策略，该策略旨在平衡预测不确定性与代理模型和真实 $J(\\theta)$ 之间的最坏情况差异。\n\n需要使用的基本原理和定义：\n- 高斯过程（GP）先验，具有零均值和平方指数核 $k(\\theta,\\theta') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(\\theta-\\theta')^2}{2\\ell^2}\\right)$，其中 $\\sigma_f0$ 是信号振幅，$\\ell0$ 是长度尺度。\n- 观测噪声建模为独立的、零均值的高斯分布，其方差 $\\sigma_n^2$ 仅加在训练对角线上。\n- 对于给定训练输入 $\\Theta = \\{\\theta_i\\}_{i=1}^n$ 和观测值 $\\mathbf{y} = \\{y_i\\}_{i=1}^n$ 的测试点 $\\theta$ 的 GP 后验为\n$$\nm(\\theta) = \\mathbf{k}(\\theta)^\\top \\left(\\mathbf{K} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{y},\n\\qquad\ns^2(\\theta) = k(\\theta,\\theta) - \\mathbf{k}(\\theta)^\\top \\left(\\mathbf{K} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{k}(\\theta),\n$$\n其中 $\\mathbf{K}_{ij} = k(\\theta_i,\\theta_j)$ 且 $\\mathbf{k}(\\theta)_i = k(\\theta,\\theta_i)$。\n- 物理信息神经网络（PINNs）通过在损失函数中强制执行控制方程的残差来约束学习；在实践中，可以通过在参数 $\\theta_i$ 下运行基于 PINN 的求解器并评估导出的 $J(\\theta_i)$ 来获得 $y_i$。对于本任务，请使用上面定义的解析 $J(\\theta)$ 来生成一致的、无噪声的训练数据，这模拟了一个没有训练误差的高保真 PINN 解。\n- 为量化当前最坏情况下的代理模型差异，将阈值 $t$ 定义为代理模型在固定评估网格 $\\mathcal{G}=\\{\\vartheta_j\\}_{j=1}^{M}$ 上的最大绝对误差：\n$$\nt = \\max_{\\vartheta \\in \\mathcal{G}} \\left| m(\\vartheta) - J(\\vartheta) \\right|.\n$$\n- 为选择下一个采样点位置，采用相对于当前阈值 $t$ 的候选点 $\\theta$ 处的绝对误差期望提升。在 GP 后验下，未知的 $J(\\theta)$ 被建模为正态随机变量 $\\mathcal{N}\\!\\left(m(\\theta), s^2(\\theta)\\right)$，因此绝对误差 $\\left|J(\\theta)-m(\\theta)\\right|$ 是一个尺度为 $s(\\theta)$ 的折叠正态随机变量 $A$。在 $\\theta$ 处的期望提升为\n$$\n\\operatorname{EI}_{\\mathrm{abs}}(\\theta) = \\mathbb{E}\\left[\\left(A - t\\right)_+\\right]\n= \\int_{t}^{\\infty} (a-t)\\, f_A(a)\\, da,\n$$\n其中 $f_A(a)$ 是零均值、尺度为 $s(\\theta)$ 的折叠正态分布的概率密度函数。该积分可以以闭式形式求值，得到\n$$\n\\operatorname{EI}_{\\mathrm{abs}}(\\theta) =\n\\sqrt{\\frac{2}{\\pi}}\\, s(\\theta)\\, \\exp\\!\\left(-\\frac{t^2}{2 s^2(\\theta)}\\right)\n- t \\, \\operatorname{erfc}\\!\\left(\\frac{t}{\\sqrt{2}\\, s(\\theta)}\\right),\n$$\n并约定当 $s(\\theta)=0$ 时 $\\operatorname{EI}_{\\mathrm{abs}}(\\theta)=0$。\n\n算法要求：\n- 使用零均值 GP、平方指数核以及解析函数 $J(\\theta)$ 来构建训练数据 $\\{(\\theta_i, y_i)\\}_{i=1}^{n}$，其中 $y_i = J(\\theta_i)$。\n- 训练位置必须确定性地设置为内部等距点 $\\theta_i = \\frac{i+1}{n+1}$（对于 $i=0,1,\\dots,n-1$），确保初始时不采样端点。\n- 用于选择下一个采样点位置的候选集和评估网格都必须是 $[0,1]$ 上包含 $M$ 个点的均匀网格 $\\mathcal{G}$，其中 $M = 401$ 且 $\\vartheta_j = \\frac{j}{M-1}$（对于 $j=0,1,\\dots,M-1$）。\n- 计算所有 $\\vartheta \\in \\mathcal{G}$ 的后验均值 $m(\\vartheta)$ 和方差 $s^2(\\vartheta)$。\n- 在 $\\mathcal{G}$ 上计算当前误差阈值 $t$。\n- 对于每个候选点 $\\vartheta \\in \\mathcal{G}$，使用上述闭式表达式计算 $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$，并遵循当 $s(\\vartheta)=0$ 时 $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)=0$ 的约定。\n- 选择下一个采样点位置为在采样预算约束下最大化 $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$ 的 $\\vartheta \\in \\mathcal{G}$。如果当前样本数 $n$ 等于最大允许样本数 $n_{\\max}$，则不允许进一步采样：返回哨兵值 $-1.0$。\n\n物理和数值单位：\n- 在本任务中，所有量都是无量纲的。\n- 不涉及角度。\n- 不涉及百分比。\n\n程序输入规范（嵌入程序中；无外部输入）：\n- 在构建 $J(\\theta) = A + B\\, \\theta^2$ 中的 $A$ 和 $B$ 时，使用固定的物理常数 $k = 0.1$ 和 $\\beta = 1.0$。\n- 使用具有指定超参数 $\\ell$ 和 $\\sigma_f$ 的平方指数核 $k(\\theta,\\theta')$，以及仅在训练对角线上使用的观测噪声方差 $\\sigma_n^2$。\n\n测试套件：\n对于每个测试用例，程序必须在内部点上从 $J(\\theta)$ 构建训练数据，然后如上所述计算下一个采样点位置。测试套件包含以下参数集 $(n, n_{\\max}, \\ell, \\sigma_f, \\sigma_n)$：\n- 案例 1 (正常路径)：$(n = 3, n_{\\max} = 5, \\ell = 0.2, \\sigma_f = 1.5, \\sigma_n = 10^{-6})$。\n- 案例 2 (预算耗尽)：$(n = 8, n_{\\max} = 8, \\ell = 0.1, \\sigma_f = 1.0, \\sigma_n = 10^{-6})$。\n- 案例 3 (短长度尺度)：$(n = 4, n_{\\max} = 6, \\ell = 0.05, \\sigma_f = 0.7, \\sigma_n = 10^{-6})$。\n- 案例 4 (非常稀疏的训练)：$(n = 2, n_{\\max} = 4, \\ell = 0.3, \\sigma_f = 2.0, \\sigma_n = 10^{-6})$。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含结果（每个测试用例的下一个采样点位置，如果预算耗尽则为哨兵值 $-1.0$），形式为用方括号括起来的逗号分隔列表。例如，输出必须为 $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4\\right]$ 的形式，其中每个条目都是一个浮点数。",
            "solution": "该问题要求设计并实现一个基于高斯过程（GP）代理模型的​​主动学习算法。目标是为一个从简化的一维多物理场模型中导出的目标量 $J(\\theta)$ 智能地选择下一个采样点位置。选择标准是最大化绝对误差的期望提升（$\\operatorname{EI}_{\\mathrm{abs}}$），该标准平衡了对模型预测的利用和对高不确定性区域的探索。\n\n### 步骤 1：问题陈述验证\n\n我将首先按照要求的程序验证问题陈述。\n\n**1.1. 提取给定信息**\n\n- **控制方程**: $-k\\, T''(x) + \\beta\\, \\theta\\, C(x) = 0$ for $x \\in [0,1]$。\n- **参数**: 热导率 $k0$，耦合系数 $\\beta0$，标量耦合参数 $\\theta \\in [0,1]$。\n- **已知场**: 浓度场 $C(x) = \\sin(\\pi x)$，近似温度场 $T(x) = \\sin(2\\pi x)$。\n- **边界条件**: 对 $T(x)$ 为齐次边界条件，即 $T(0)=0$ 和 $T(1)=0$。\n- **目标量**: $J(\\theta) = A + B\\, \\theta^2$，从积分平方残差导出。\n- **$J(\\theta)$ 的系数**: $A = \\frac{k^2 (2\\pi)^4}{2}$ 和 $B = \\frac{\\beta^2}{2}$。\n- **GP 先验**: 零均值，平方指数核 $k(\\theta,\\theta') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(\\theta-\\theta')^2}{2\\ell^2}\\right)$，其中信号振幅 $\\sigma_f0$ 且长度尺度 $\\ell0$。\n- **观测噪声**: 独立的、零均值的高斯分布，其方差为 $\\sigma_n^2$。\n- **GP 后验均值**: $m(\\theta) = \\mathbf{k}(\\theta)^\\top \\left(\\mathbf{K} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{y}$。\n- **GP 后验方差**: $s^2(\\theta) = k(\\theta,\\theta) - \\mathbf{k}(\\theta)^\\top \\left(\\mathbf{K} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{k}(\\theta)$。\n- **后验的定义**: $\\mathbf{K}_{ij} = k(\\theta_i,\\theta_j)$，$\\mathbf{k}(\\theta)_i = k(\\theta,\\theta_i)$，$\\mathbf{y}$ 是观测值。\n- **数据生成**: 使用解析的 $J(\\theta)$ 生成无噪声的训练数据 ($y_i=J(\\theta_i)$)。\n- **误差阈值**: $t = \\max_{\\vartheta \\in \\mathcal{G}} \\left| m(\\vartheta) - J(\\vartheta) \\right|$，在固定的评估网格 $\\mathcal{G}$ 上。\n- **采集函数**: 绝对误差的期望提升, $\\operatorname{EI}_{\\mathrm{abs}}(\\theta) = \\sqrt{\\frac{2}{\\pi}}\\, s(\\theta)\\, \\exp\\!\\left(-\\frac{t^2}{2 s^2(\\theta)}\\right) - t \\, \\operatorname{erfc}\\!\\left(\\frac{t}{\\sqrt{2}\\, s(\\theta)}\\right)$。约定：如果 $s(\\theta)=0$，则 $\\operatorname{EI}_{\\mathrm{abs}}(\\theta)=0$。\n- **训练位置**: $\\theta_i = \\frac{i+1}{n+1}$ for $i=0,1,\\dots,n-1$。\n- **评估/候选网格**: $\\mathcal{G}$ 是一个在 $[0,1]$ 上的包含 $M=401$ 个点的均匀网格，$\\vartheta_j = \\frac{j}{M-1}$。\n- **采样预算**: 如果当前样本数 $n$ 等于 $n_{\\max}$，返回哨兵值 $-1.0$。\n- **物理常数**: $k = 0.1$, $\\beta = 1.0$。\n- **测试用例**: $(n, n_{\\max}, \\ell, \\sigma_f, \\sigma_n)$，针对四个指定案例。\n\n**1.2. 使用清单进行验证**\n\n- **科学基础**: 该问题基于数值模拟和机器学习等成熟领域。多物理场模型虽然简化，但代表了一个有效的物理概念。使用高斯过程作为代理模型和期望提升作为采集函数是贝叶斯优化和不确定性量化中的标准、有据可查的技术。对于给定的 $T(x)$ 和 $C(x)$ 函数形式，$J(\\theta)$ 的推导是合理的。\n- **适定性**: 该问题是适定的。所有必要的函数、参数和算法步骤都已明确定义。任务是在一个有限点集上找到一个确定性计算函数（$\\operatorname{EI}_{\\mathrm{abs}}$）的最大值，这保证了解的存在性和唯一性（除了可能出现的平局，这可以通过确定性方式解决，例如选择第一个出现的位置）。\n- **客观性**: 问题陈述以精确、客观和数学化的语言编写。没有主观或基于意见的成分。\n- **不完整或矛盾的设置**: 该问题是自包含的。它提供了所有必要的常数 ($k, \\beta$)、GP 超参数 ($\\ell, \\sigma_f, \\sigma_n$)、网格定义 ($M$) 和训练数据生成规则。没有明显的矛盾。\n- **不切实际或不可行**: 该问题在计算上是可行的。GP 的矩阵求逆是在一个 $n \\times n$ 的小矩阵上进行的，其中 $n$ 很小。使用解析函数代替复杂模拟的前提是算法开发和测试中的标准且现实的做法。\n- **不适定或结构不良**: 问题结构清晰。逻辑从物理模型流向代理模型，最终到主动学习算法。\n- **伪深刻、琐碎或同义反复**: 该问题并非无足轻重。它需要正确实现高斯过程回归和一个非标准的采集函数，涉及线性代数和特殊的数学函数。其概念深度对于计算科学中的一个问题是合适的。\n- **超出科学可验证性**: 通过实现所述算法，结果是完全可验证的。\n\n**1.3. 结论**\n\n该问题是**有效的**。它是在科学计算的代理建模领域中一个定义明确、科学上合理且计算上可行的 tractable 问题。我现在将继续进行解答。\n\n### 步骤 2：原则性解决方案设计\n\n任务的核心是实现主动学习循环中的一个步骤。给定一个包含 $n$ 个样本的现有集合，我们构建一个 GP 代理模型，并用它来决定要采样的下一个信息最丰富的点，旨在减少模型的最大误差。\n\n**2.1. 解析目标量 $J(\\theta)$**\n\n我们旨在建模的真实函数是 $J(\\theta) = A + B\\, \\theta^2$。常数 $A$ 和 $B$ 由问题的物理特性决定。\n给定物理常数 $k = 0.1$ 和 $\\beta = 1.0$，我们可以计算出 $A$ 和 $B$：\n$$\nA = \\frac{k^2 (2\\pi)^4}{2} = \\frac{(0.1)^2 \\cdot 16 \\pi^4}{2} = 0.08 \\pi^4\n$$\n$$\nB = \\frac{\\beta^2}{2} = \\frac{1.0^2}{2} = 0.5\n$$\n所以，要逼近的函数是 $J(\\theta) = 0.08 \\pi^4 + 0.5 \\theta^2$。这个函数作为“真实情况”或高保真模拟的输出。\n\n**2.2. 高斯过程代理模型**\n\n我们使用高斯过程对 $J(\\theta)$ 进行建模。GP 是一个随机过程，其中任何有限的随机变量集合都服从多元高斯分布。它由一个均值函数 $m_0(\\theta)$ 和一个协方差（或核）函数 $k(\\theta, \\theta')$ 完全指定。\n\n- **先验**: 我们假设一个零均值先验，$m_0(\\theta) = 0$。核函数是平方指数核：\n$$\nk(\\theta, \\theta') = \\sigma_f^2 \\exp\\left(-\\frac{(\\theta - \\theta')^2}{2\\ell^2}\\right)\n$$\n这里，$\\sigma_f$ 是信号振幅，控制总方差；$\\ell$ 是长度尺度，控制“平滑度”或相关距离。\n\n- **训练数据**: 我们生成 $n$ 个训练点。输入位置是确定性选择的 $\\Theta_{\\text{train}} = \\{\\theta_i\\}_{i=0}^{n-1}$，其中 $\\theta_i = \\frac{i+1}{n+1}$。相应的输出是来自真实函数的无噪声观测值 $\\mathbf{y}_{\\text{train}} = \\{J(\\theta_i)\\}_{i=0}^{n-1}$。\n\n- **后验分布**: 给定训练数据 $(\\Theta_{\\text{train}}, \\mathbf{y}_{\\text{train}})$，任何测试点 $\\theta_*$ 的 GP 后验也是一个高斯分布，其均值为 $m(\\theta_*)$，方差为 $s^2(\\theta_*)$。它们的计算公式如下：\n$$\nm(\\theta_*) = \\mathbf{k}_*^\\top (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_{\\text{train}}\n$$\n$$\ns^2(\\theta_*) = k(\\theta_*, \\theta_*) - \\mathbf{k}_*^\\top (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\n$$\n其中：\n- $\\mathbf{K}$ 是训练点的 $n \\times n$ 核矩阵，其元素为 $K_{ij} = k(\\theta_i, \\theta_j)$。\n- $\\mathbf{k}_*$ 是测试点 $\\theta_*$ 与训练点之间协方差的 $n \\times 1$ 向量，其元素为 $(\\mathbf{k}_*)_i = k(\\theta_*, \\theta_i)$。\n- $k(\\theta_*, \\theta_*)$ 是测试点处的先验方差。\n- $\\sigma_n^2$ 是观测噪声的方差，加到 $\\mathbf{K}$ 的对角线上，以确保矩阵稳定性和对观测误差建模。尽管我们的数据是无噪声的，但这一项对于数值条件处理至关重要。\n\n**2.3. 主动学习标准：$\\operatorname{EI}_{\\mathrm{abs}}$**\n\n目标是选择最有可能减少我们代理模型当前最大绝对误差的下一个点。\n\n- **候选与评估网格**: 我们使用一个精细的均匀网格 $\\mathcal{G} = \\{\\vartheta_j\\}_{j=0}^{M-1}$（在 $[0,1]$ 上有 $M=401$ 个点），既用于评估模型性能，也用于选择下一个样本。\n\n- **误差阈值 $t$**: 首先，我们为所有 $\\vartheta \\in \\mathcal{G}$ 计算 GP 后验均值 $m(\\vartheta)$。然后，我们找到这个代理模型相对于真实函数 $J(\\vartheta)$ 的当前最大绝对误差：\n$$\nt = \\max_{\\vartheta \\in \\mathcal{G}} |m(\\vartheta) - J(\\vartheta)|\n$$\n这个值 $t$ 代表了我们模型在网格 $\\mathcal{G}$ 上的当前最差性能。\n\n- **绝对误差的期望提升**: 对于任何候选点 $\\vartheta \\in \\mathcal{G}$，从模型的角度看，真实值 $J(\\vartheta)$ 是未知的。GP 后验将其建模为一个随机变量 $J(\\vartheta) \\sim \\mathcal{N}(m(\\vartheta), s^2(\\vartheta))$。因此，该点的绝对误差 $|J(\\vartheta) - m(\\vartheta)|$ 是一个折叠正态随机变量。我们感兴趣的是相对于当前阈值 $t$ 的期望提升。提升定义为 $\\max(0, |J(\\vartheta) - m(\\vartheta)| - t)$。这个量的期望就是采集函数 $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$：\n$$\n\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta) = \\mathbb{E}\\left[ \\max(0, |J(\\vartheta) - m(\\vartheta)| - t) \\right]\n$$\n问题提供了该积分的闭式解：\n$$\n\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta) = \\sqrt{\\frac{2}{\\pi}}\\, s(\\vartheta) \\exp\\left(-\\frac{t^2}{2 s^2(\\vartheta)}\\right) - t \\, \\operatorname{erfc}\\left(\\frac{t}{\\sqrt{2}\\, s(\\vartheta)}\\right)\n$$\n其中 $s(\\vartheta)$ 是在 $\\vartheta$ 处的后验标准差，$\\operatorname{erfc}$ 是互补误差函数。如果后验方差 $s^2(\\vartheta)$ 为零（或数值上可忽略），则不可能有提升，因此我们设置 $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta) = 0$。这通常发生在没有不确定性的训练点上。\n\n**2.4. 算法实现步骤**\n\n对于每个具有参数 $(n, n_{\\max}, \\ell, \\sigma_f, \\sigma_n)$ 的测试用例：\n1.  **检查预算**: 如果 $n \\ge n_{\\max}$，采样完成。返回哨兵值 $-1.0$。\n2.  **定义模型**: 设置常数 $k=0.1, \\beta=1.0$ 并计算 $A, B$。定义真实函数 $J(\\theta) = A + B \\theta^2$。\n3.  **生成数据**: 创建训练输入 $\\Theta_{\\text{train}} = \\{\\frac{i+1}{n+1}\\}_{i=0}^{n-1}$ 和输出 $\\mathbf{y}_{\\text{train}} = \\{J(\\theta_i)\\}_{i=0}^{n-1}$。\n4.  **定义网格**: 创建从 $0$ 到 $1$ 的包含 $M=401$ 个点的评估/候选网格 $\\mathcal{G}$。\n5.  **计算后验**:\n    a. 根据 $\\Theta_{\\text{train}}$ 构建核矩阵 $\\mathbf{K}$。\n    b. 计算矩阵 $\\mathbf{L} = \\mathbf{K} + \\sigma_n^2 \\mathbf{I}$。\n    c. 计算其逆矩阵 $\\mathbf{L}^{-1}$。一个更稳定的方法是解线性方程组 $\\mathbf{L}\\mathbf{\\alpha} = \\mathbf{y}_{\\text{train}}$ 来求 $\\mathbf{\\alpha}$。然后 $m(\\theta_*) = \\mathbf{k}_*^\\top \\mathbf{\\alpha}$。\n    d. 对于每个 $\\vartheta \\in \\mathcal{G}$，计算 $\\mathbf{k}_*(\\vartheta)$，然后计算后验均值 $m(\\vartheta)$ 和方差 $s^2(\\vartheta)$。\n6.  **计算阈值 $t$**: 为所有 $\\vartheta \\in \\mathcal{G}$ 评估 $J(\\vartheta)$。计算 $t = \\max_{\\vartheta \\in \\mathcal{G}} |m(\\vartheta) - J(\\vartheta)|$。\n7.  **计算采集函数**: 对于每个 $\\vartheta \\in \\mathcal{G}$，使用后验标准差 $s(\\vartheta)$ 和阈值 $t$ 计算 $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$。\n8.  **选择下一点**: 找到网格中使采集函数最大化的点：$\\theta_{\\text{next}} = \\arg\\max_{\\vartheta \\in \\mathcal{G}} \\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$。这就是所求的输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erfc\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, n_max, l, sigma_f, sigma_n)\n        (3, 5, 0.2, 1.5, 1e-6),  # Case 1\n        (8, 8, 0.1, 1.0, 1e-6),  # Case 2\n        (4, 6, 0.05, 0.7, 1e-6), # Case 3\n        (2, 4, 0.3, 2.0, 1e-6),  # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        n, n_max, ell, sigma_f, sigma_n_var = case\n        # The problem gives sigma_n, but the formula uses sigma_n^2.\n        # It's more common to specify noise std dev, but problem says variance.\n        # I'll assume sigma_n in the tuple is the std dev, so I square it.\n        # But the problem states \"观测噪声方差 σn^2\". Let's stick to the text. sigma_n in the tuple is sigma_n stddev.\n        # The formula uses sigma_n^2, and the tuple provides sigma_n. \n        # Rereading: \"观测噪声方差 σn^2\". \"测试套件包含以下参数集 (n, nmax, ℓ, σf, σn)\".\n        # This is ambiguous. But σ_n = 10^-6 is very small for a variance. It's almost certainly std dev.\n        # I will treat it as std dev and square it.\n        sigma_n_sq = sigma_n_var**2\n        next_location = find_next_sample_location(n, n_max, ell, sigma_f, sigma_n_sq)\n        results.append(next_location)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef find_next_sample_location(n, n_max, ell, sigma_f, sigma_n_sq):\n    \"\"\"\n    Implements the GP-based active learning algorithm to find the next sample location.\n    \n    Args:\n        n (int): Current number of samples.\n        n_max (int): Maximum number of samples allowed.\n        ell (float): Length scale for the kernel.\n        sigma_f (float): Signal amplitude for the kernel.\n        sigma_n_sq (float): Variance of the observation noise.\n    \n    Returns:\n        float: The next sampling location theta, or -1.0 if budget is exhausted.\n    \"\"\"\n    \n    # Step 1: Check the sampling budget\n    if n >= n_max:\n        return -1.0\n        \n    # Step 2: Define physical model constants and the true function J(theta)\n    k_phys = 0.1\n    beta_phys = 1.0\n    A = k_phys**2 * (2 * np.pi)**4 / 2.0\n    B = beta_phys**2 / 2.0\n    \n    def J_true(theta):\n        return A + B * theta**2\n\n    # Step 3: Generate training data\n    if n > 0:\n        theta_train = np.array([(i + 1) / (n + 1) for i in range(n)])\n        y_train = J_true(theta_train)\n    else: # Handle case where n=0, though not in test suite.\n        theta_train = np.array([])\n        y_train = np.array([])\n        \n    # Step 4: Define evaluation and candidate grid\n    M = 401\n    grid = np.linspace(0.0, 1.0, M)\n\n    # Step 5: Compute GP posterior mean and variance on the grid\n    \n    # Define the squared-exponential kernel\n    def se_kernel(theta1, theta2, ell, sigma_f):\n        # Using np.subtract.outer for vectorization\n        sqdist = np.subtract.outer(theta1, theta2)**2\n        return sigma_f**2 * np.exp(-0.5 * sqdist / ell**2)\n\n    # Handle n=0 case (prior is zero mean, variance is kernel_diag)\n    if n == 0:\n        m_post = np.zeros(M)\n        s2_post = np.full(M, sigma_f**2)\n    else:\n        # Build kernel matrices\n        K = se_kernel(theta_train, theta_train, ell, sigma_f)\n        K_noisy = K + sigma_n_sq * np.eye(n)\n        K_star = se_kernel(grid, theta_train, ell, sigma_f)\n        K_star_star_diag = np.full(M, sigma_f**2) # k(theta_*, theta_*)\n        \n        # Calculate posterior using stable linear solve\n        # alpha = (K + sigma_n^2 I)^-1 * y_train\n        alpha = np.linalg.solve(K_noisy, y_train)\n        m_post = K_star @ alpha\n\n        # v = (K + sigma_n^2 I)^-1 * k_*(theta)\n        v = np.linalg.solve(K_noisy, K_star.T)\n        s2_post = K_star_star_diag - np.sum(K_star * v.T, axis=1)\n        \n        # Ensure variance is non-negative due to potential numerical issues\n        s2_post[s2_post  0] = 0\n\n    # Step 6: Compute current error threshold 't'\n    J_grid = J_true(grid)\n    abs_error = np.abs(m_post - J_grid)\n    t = np.max(abs_error)\n\n    # Step 7: Compute Expected Improvement for Absolute Error (EI_abs)\n    \n    s_post = np.sqrt(s2_post)\n    ei_abs = np.zeros(M)\n    \n    # Avoid division by zero where standard deviation is negligible\n    # The problem states EI_abs=0 if s(theta)=0.\n    # We use a small threshold to handle floating point inaccuracies.\n    nonzero_s_mask = s_post > 1e-12 \n    \n    s_nonzero = s_post[nonzero_s_mask]\n    z = t / s_nonzero\n    \n    term1 = np.sqrt(2.0 / np.pi) * s_nonzero * np.exp(-0.5 * z**2)\n    term2 = t * erfc(z / np.sqrt(2.0))\n    \n    ei_abs[nonzero_s_mask] = term1 - term2\n    \n    # Step 8: Select next point that maximizes EI_abs\n    best_idx = np.argmax(ei_abs)\n    next_location = grid[best_idx]\n    \n    return float(next_location)\n\n# Execute the solver\nsolve()\n```"
        }
    ]
}