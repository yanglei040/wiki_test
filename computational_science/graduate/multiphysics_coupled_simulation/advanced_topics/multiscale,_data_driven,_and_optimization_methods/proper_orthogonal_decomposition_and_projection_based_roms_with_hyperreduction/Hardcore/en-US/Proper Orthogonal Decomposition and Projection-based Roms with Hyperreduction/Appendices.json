{
    "hands_on_practices": [
        {
            "introduction": "Reduced-order models rely on projecting the governing equations onto a low-dimensional subspace. This exercise explores the distinction between two cornerstone projection techniques: the standard Galerkin method and the Least-Squares Petrov-Galerkin (LSPG) method. By deriving the conditions under which they are equivalent and working through a concrete example where they differ , you will gain a deeper understanding of how the choice of projection impacts the resulting reduced system and its solution.",
            "id": "3524021",
            "problem": "Consider a semi-discrete multiphysics system obtained after spatial discretization of coupled governing laws, given by $M \\frac{d y}{d t} = f(y,t)$, where $y \\in \\mathbb{R}^{N}$ is the state vector, $M \\in \\mathbb{R}^{N \\times N}$ is a symmetric positive definite mass matrix, and $f : \\mathbb{R}^{N} \\times \\mathbb{R} \\to \\mathbb{R}^{N}$ is a sufficiently smooth right-hand side representing the coupled physics. Let a reduced-order model (ROM) be constructed via Proper Orthogonal Decomposition (POD), yielding an $M$-orthonormal trial basis $\\Phi \\in \\mathbb{R}^{N \\times r}$ satisfying $\\Phi^{\\top} M \\Phi = I_{r}$, where $I_{r}$ is the $r \\times r$ identity. The ROM state is parameterized as $y \\approx y_{\\mathrm{ref}} + \\Phi a$, with $a \\in \\mathbb{R}^{r}$ the reduced coordinates and $y_{\\mathrm{ref}}$ a reference state.\n\nThe Galerkin projection enforces $M$-inner-product orthogonality of the continuous residual to the trial space, while the Least-Squares Petrov–Galerkin (LSPG) method enforces optimality of a time-discrete residual in a weighted least-squares sense, possibly with hyperreduction through a sampling-and-weighting operator $W \\in \\mathbb{R}^{s \\times N}$ (with $s \\le N$). Use the backward Euler method with time step $\\Delta t$ to define the discrete residual $r_{n}(y_{n}) = M (y_{n} - y_{n-1}) - \\Delta t\\, f(y_{n}, t_{n})$ at time $t_{n}$.\n\nTasks:\n1) Starting from the definitions above and first principles of projection, derive the algebraic condition on the weighting and Jacobian under which the first-order optimality conditions of the LSPG method (with backward Euler and hyperreduction) reduce exactly to the Galerkin orthogonality condition for the same time discretization. Express the condition as a single matrix equality involving $W$, $M$, the Jacobian $J_{f}(y_{n}) = \\frac{\\partial f}{\\partial y}(y_{n}, t_{n})$, and the trial basis $\\Phi$.\n\n2) Construct a concrete example that demonstrates a difference between Galerkin and LSPG for the same backward Euler time step. Let $N = 2$, $M = I_{2}$, $f(y) = A y$ with $A = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}$, $\\Delta t = 1$, $y_{\\mathrm{ref}} = 0$, $y_{n-1} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, and $\\Phi = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. For LSPG, take $W = I_{2}$ (no hyperreduction). Compute the one-step backward Euler update produced by Galerkin and by LSPG, and then compute the Euclidean norm of the difference between these two ROM updates at time $t_{n}$.\n\nAnswer specification:\n- Provide the final answer as the exact Euclidean norm of the difference between the LSPG and Galerkin one-step updates for the specified example.\n- No rounding is required.\n- The final answer must be a single real number.",
            "solution": "This problem consists of two parts. The first part requires the derivation of a condition under which the Least-Squares Petrov–Galerkin (LSPG) method becomes equivalent to the Galerkin projection method. The second part requires the computation of a one-step update for a specific system using both methods and finding the norm of their difference.\n\nPart 1: Derivation of the Equivalence Condition\n\nThe state of the reduced-order model (ROM) at time $t_n$ is approximated as $y_n = y_{\\mathrm{ref}} + \\Phi a_n$, where $a_n \\in \\mathbb{R}^r$ are the reduced coordinates. The backward Euler time discretization of the governing equation $M \\frac{dy}{dt} = f(y,t)$ leads to the definition of the full-order discrete residual at step $n$:\n$$\nr_n(y_n) = M (y_n - y_{n-1}) - \\Delta t\\, f(y_n, t_n)\n$$\nBoth Galerkin and LSPG methods formulate a system of equations to solve for the unknown reduced coordinates $a_n$.\n\nThe Galerkin method enforces that the residual $r_n(y_n)$ is orthogonal to the trial subspace $\\text{span}(\\Phi)$ using the Euclidean inner product. This is expressed as:\n$$\n\\Phi^{\\top} r_n(y_n(a_n)) = 0\n$$\nThis is a system of $r$ nonlinear equations for the $r$ components of $a_n$.\n\nThe LSPG method seeks to find the reduced coordinates $a_n$ that minimize the weighted squared norm of the residual, possibly with hyperreduction. The objective function is:\n$$\nJ_{\\text{LSPG}}(a_n) = \\frac{1}{2} \\| W r_n(y_n(a_n)) \\|_2^2 = \\frac{1}{2} r_n(y_n(a_n))^{\\top} W^{\\top} W r_n(y_n(a_n))\n$$\nThe first-order optimality condition is that the gradient of $J_{\\text{LSPG}}$ with respect to $a_n$ is zero:\n$$\n\\frac{\\partial J_{\\text{LSPG}}}{\\partial a_n} = 0\n$$\nUsing the chain rule, this gradient is:\n$$\n\\frac{\\partial J_{\\text{LSPG}}}{\\partial a_n} = \\left(\\frac{\\partial y_n}{\\partial a_n}\\right)^{\\top} \\left(\\frac{\\partial r_n}{\\partial y_n}\\right)^{\\top} W^{\\top} W r_n(y_n)\n$$\nWe have the following derivatives:\n$\\frac{\\partial y_n}{\\partial a_n} = \\Phi$\n$\\frac{\\partial r_n}{\\partial y_n} = M - \\Delta t \\frac{\\partial f}{\\partial y}(y_n, t_n) = M - \\Delta t J_f(y_n)$\nSubstituting these into the optimality condition yields the LSPG nonlinear system for $a_n$:\n$$\n\\Phi^{\\top} (M - \\Delta t J_f(y_n))^{\\top} W^{\\top} W r_n(y_n(a_n)) = 0\n$$\nFor the LSPG first-order optimality condition to reduce exactly to the Galerkin orthogonality condition, the two systems of equations must be equivalent. That is, for any residual vector $r_n$, the following equivalence must hold:\n$$\n\\Phi^{\\top} (M - \\Delta t J_f)^{\\top} W^{\\top} W r_n = 0 \\iff \\Phi^{\\top} r_n = 0\n$$\nThis equivalence holds if and only if the row space of the operator on the left is identical to the row space of the operator on the right. This means there must exist an invertible matrix $C \\in \\mathbb{R}^{r \\times r}$ such that:\n$$\n\\Phi^{\\top} (M - \\Delta t J_f)^{\\top} W^{\\top} W = C \\Phi^{\\top}\n$$\nTransposing both sides (and noting that $W^{\\top}W$ is symmetric) gives:\n$$\nW^{\\top} W (M - \\Delta t J_f) \\Phi = \\Phi C^{\\top}\n$$\nLet $K = C^{\\top}$, which is also an invertible matrix in $\\mathbb{R}^{r \\times r}$. The condition is that there exists an invertible matrix $K$ such that:\n$$\nW^{\\top} W (M - \\Delta t J_f) \\Phi = \\Phi K\n$$\nThis equation states that the subspace spanned by the columns of $\\Phi$, denoted $\\text{span}(\\Phi)$, must be an invariant subspace of the operator $W^{\\top} W (M - \\Delta t J_f)$. The invertibility of $K$ ensures that the mapping does not collapse the dimension of the subspace, thus preserving the equivalence of the equation systems.\n\nPart 2: Concrete Example Calculation\n\nWe are given the following parameters:\n$N=2$, $M=I_2$, $\\Delta t=1$, $y_{\\mathrm{ref}}=0$, $W=I_2$.\n$f(y) = Ay$ with $A = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}$.\n$y_{n-1} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n$\\Phi = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nSince $r=1$, the reduced coordinate $a_n$ is a scalar. The ROM state is $y_n = \\Phi a_n = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} a_n = \\begin{pmatrix} a_n \\\\ 0 \\end{pmatrix}$.\nFirst, we compute the residual $r_n(y_n)$:\n$$\nr_n(a_n) = M(y_n - y_{n-1}) - \\Delta t f(y_n) = I_2 \\left( \\begin{pmatrix} a_n \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right) - (1) \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} a_n \\\\ 0 \\end{pmatrix}\n$$\n$$\nr_n(a_n) = \\begin{pmatrix} a_n \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ a_n \\end{pmatrix} = \\begin{pmatrix} a_n \\\\ -1-a_n \\end{pmatrix}\n$$\n\nGalerkin Update:\nThe Galerkin condition is $\\Phi^{\\top} r_n = 0$:\n$$\n\\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} a_n \\\\ -1-a_n \\end{pmatrix} = 0 \\implies a_n^G = 0\n$$\nThe Galerkin state update is $y_n^G = \\Phi a_n^G = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} (0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nLSPG Update:\nThe LSPG condition is $\\Phi^{\\top} (M - \\Delta t J_f)^{\\top} W^{\\top} W r_n = 0$.\nSince $f(y)=Ay$ is linear, the Jacobian is constant: $J_f = A = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}$.\nWith $M=I_2$, $\\Delta t=1$, and $W=I_2$, the condition becomes $\\Phi^{\\top} (I_2 - A)^{\\top} r_n = 0$.\nThe operator term is:\n$$\nI_2 - A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\end{pmatrix}\n$$\n$$\n(I_2 - A)^{\\top} = \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe full LSPG operator acting on the residual is:\n$$\n\\Phi^{\\top} (I_2 - A)^{\\top} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\end{pmatrix}\n$$\nThe LSPG equation is thus:\n$$\n\\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} a_n \\\\ -1-a_n \\end{pmatrix} = 0\n$$\n$$\na_n - (-1-a_n) = 0 \\implies a_n + 1 + a_n = 0 \\implies 2a_n = -1 \\implies a_n^L = -\\frac{1}{2}\n$$\nThe LSPG state update is $y_n^L = \\Phi a_n^L = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\left(-\\frac{1}{2}\\right) = \\begin{pmatrix} -1/2 \\\\ 0 \\end{pmatrix}$.\n\nDifference between Updates:\nThe difference vector between the LSPG and Galerkin updates is:\n$$\ny_n^L - y_n^G = \\begin{pmatrix} -1/2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1/2 \\\\ 0 \\end{pmatrix}\n$$\nThe Euclidean norm of this difference is:\n$$\n\\| y_n^L - y_n^G \\|_2 = \\sqrt{\\left(-\\frac{1}{2}\\right)^2 + 0^2} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}\n$$",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "A truly efficient ROM for a nonlinear system requires not only projection but also hyperreduction to overcome the computational bottleneck of evaluating nonlinear terms. This practice guides you through the essential derivation of a complete, hyperreduced ROM from first principles. By combining a multi-step time integrator with the Discrete Empirical Interpolation Method (DEIM) , you will construct the final reduced residual, seeing precisely how state-dependent and history-dependent terms are handled in a computationally tractable manner.",
            "id": "3524024",
            "problem": "Consider a semi-discrete multiphysics system obtained by the method of lines applied to coupled partial differential equations with algebraic constraints. Let the state be $u(t) \\in \\mathbb{R}^{N}$ and define the residual\n$$\nR(u,\\dot{u},t) := M(u)\\,\\dot{u} - f(u) - A\\,u - b(t),\n$$\nwhere $M(u) \\in \\mathbb{R}^{N \\times N}$ is a possibly state-dependent mass/capacitance/inertia operator arising from the coupling, $f(u) \\in \\mathbb{R}^{N}$ is a nonlinear state-dependent term aggregating flux, reaction, and inter-field coupling effects, $A \\in \\mathbb{R}^{N \\times N}$ is a known linear operator, and $b(t) \\in \\mathbb{R}^{N}$ is a known forcing. Let the time grid be $t^{n} = t^{0} + n\\,\\Delta t$ with constant time step $\\Delta t > 0$.\n\nLet the time derivative be discretized by a $k$-step Backward Differentiation Formula (BDF), which approximates the time derivative at $t^{n+1}$ by\n$$\n\\dot{u}^{n+1} \\approx \\frac{1}{\\Delta t} \\sum_{j=0}^{k} \\alpha_{j}\\,u^{n+1-j},\n$$\nwhere the coefficients $\\alpha_{j}$ are the standard $k$-step BDF coefficients satisfying $\\sum_{j=0}^{k} \\alpha_{j} = 0$ and $\\alpha_{0} > 0$. Assume an implicit treatment of the state-dependent operators at $t^{n+1}$ to obtain a consistent fully discrete residual.\n\nIntroduce a Proper Orthogonal Decomposition (POD) trial basis $V \\in \\mathbb{R}^{N \\times r}$ with $r \\ll N$, and a Petrov–Galerkin test basis $W \\in \\mathbb{R}^{N \\times r}$ such that $W^T V = I$. The Reduced-Order Model (ROM) unknowns are the reduced coordinates $q^{n} \\in \\mathbb{R}^{r}$ so that $u^{n} \\approx V\\,q^{n}$. Let $b^{n+1} := b(t^{n+1})$.\n\nStarting from these definitions alone (without invoking any target formulas), do the following:\n- Derive the consistent fully discrete Full-Order Model residual at step $n+1$ in terms of $u^{n+1}$ and the history states $\\{u^{n},\\dots,u^{n+1-k}\\}$.\n- Apply Petrov–Galerkin projection to obtain the ROM residual $r^{n+1}(q^{n+1})$ in terms of $V$, $W$, $q^{n+1}$, the history $\\{q^{n},\\dots,q^{n+1-k}\\}$, and the operators defined above.\n- Factor the ROM residual to expose clearly which terms are state-dependent at $t^{n+1}$ and which are history-dependent, using only algebraic manipulations grounded in the BDF definition and the ROM ansatz.\n- Then, using the principle of interpolatory hyperreduction for vector- and matrix-valued nonlinear terms, construct a Discrete Empirical Interpolation Method (DEIM)-style hyperreduced ROM residual by introducing collateral bases $U_{g} \\in \\mathbb{R}^{N \\times m_{g}}$ for $f(V\\,q)$ and $U_{M} \\in \\mathbb{R}^{N \\times m_{M}}$ for $M(V\\,q)\\,V$, along with sampling (row-selection) matrices $P_{g} \\in \\mathbb{R}^{N \\times m_{g}}$ and $P_{M} \\in \\mathbb{R}^{N \\times m_{M}}$. Your construction must follow directly from enforcing interpolation at sampled rows and projecting onto the collateral bases, without assuming any pre-stated formula.\n\nProvide, as your final answer, a single closed-form analytic expression for the hyperreduced ROM residual at step $n+1$ in terms of the symbols introduced above. Do not include any explanatory text in the final answer. The final answer must be a single symbolic mathematical expression. No units are required.",
            "solution": "The derivation proceeds by sequentially applying the given definitions to construct the final hyperreduced residual.\n\n**1. Fully Discrete Full-Order Model (FOM) Residual**\nThe semi-discrete residual is $R(u, \\dot{u}, t) = M(u)\\,\\dot{u} - f(u) - A\\,u - b(t) = 0$. Using a fully implicit scheme, all terms are evaluated at $t^{n+1}$. We replace the time derivative $\\dot{u}^{n+1}$ with its $k$-step BDF approximation: $\\dot{u}^{n+1} \\approx \\frac{1}{\\Delta t} \\sum_{j=0}^{k} \\alpha_{j}\\,u^{n+1-j}$. The resulting fully discrete FOM residual is:\n$$R_{\\text{FOM}}^{n+1} := M(u^{n+1})\\left(\\frac{1}{\\Delta t} \\sum_{j=0}^{k} \\alpha_{j}\\,u^{n+1-j}\\right) - f(u^{n+1}) - A\\,u^{n+1} - b(t^{n+1})$$\n\n**2. Petrov-Galerkin Projected ROM Residual**\nWe introduce the ROM ansatz $u^{n+1-j} \\approx Vq^{n+1-j}$ into the FOM residual and apply the Petrov-Galerkin projection, which requires $W^T R_{\\text{FOM}}^{n+1} = 0$. The resulting ROM residual, $r^{n+1}(q^{n+1})$, is:\n$$r^{n+1}(q^{n+1}) = W^T \\left[ M(Vq^{n+1})\\left(\\frac{1}{\\Delta t} \\sum_{j=0}^{k} \\alpha_{j} Vq^{n+1-j}\\right) - f(Vq^{n+1}) - A Vq^{n+1} - b^{n+1} \\right]$$\nBy linearity of the projection, this becomes:\n$$r^{n+1}(q^{n+1}) = \\frac{1}{\\Delta t} W^T M(Vq^{n+1})V \\left(\\sum_{j=0}^{k} \\alpha_{j}q^{n+1-j}\\right) - W^T f(Vq^{n+1}) - W^T A Vq^{n+1} - W^T b^{n+1}$$\n\n**3. Hyperreduced ROM Residual**\nThe computationally expensive terms are those that depend on the full-order dimension $N$. We use DEIM to approximate them.\n\n-   **Approximation of the nonlinear term $f(Vq^{n+1})$:** Let $g(q) = f(Vq)$. Its DEIM approximation is $\\hat{g}(q) = U_g (P_g^T U_g)^{-1} P_g^T g(q)$. The projected term becomes:\n$$W^T f(Vq^{n+1}) \\approx W^T U_g (P_g^T U_g)^{-1} P_g^T f(Vq^{n+1})$$\n\n-   **Approximation of the matrix-vector products involving $M(Vq^{n+1})$:** Let $\\mathcal{M}(q) = M(Vq)V$. Its DEIM approximation is $\\hat{\\mathcal{M}}(q) = U_M (P_M^T U_M)^{-1} P_M^T \\mathcal{M}(q)$. The projected inertial term becomes:\n$$\\frac{1}{\\Delta t} W^T M(Vq^{n+1})V \\left(\\sum_{j=0}^{k} \\alpha_{j}q^{n+1-j}\\right) \\approx \\frac{1}{\\Delta t} W^T U_M (P_M^T U_M)^{-1} \\left(P_M^T M(Vq^{n+1})V\\right) \\left(\\sum_{j=0}^{k} \\alpha_{j}q^{n+1-j}\\right)$$\n\n**4. Final Expression**\nCombining the approximated terms with the exact linear terms gives the hyperreduced ROM residual, $r_H^{n+1}(q^{n+1})$:\n$$r_H^{n+1}(q^{n+1}) = \\frac{1}{\\Delta t} W^T U_M (P_M^T U_M)^{-1} \\left(P_M^T M(Vq^{n+1})V\\right) \\left(\\sum_{j=0}^{k} \\alpha_{j} q^{n+1-j}\\right) - W^T U_g (P_g^T U_g)^{-1} P_g^T f(Vq^{n+1}) - W^T A V q^{n+1} - W^T b^{n+1}$$\nThis is the final closed-form expression. The online computational cost is independent of $N$.",
            "answer": "$$\n\\boxed{\\frac{1}{\\Delta t} W^T U_M (P_M^T U_M)^{-1} \\left(P_M^T M(Vq^{n+1})V\\right) \\left(\\sum_{j=0}^{k} \\alpha_{j} q^{n+1-j}\\right) - W^T U_g (P_g^T U_g)^{-1} P_g^T f(Vq^{n+1}) - W^T A V q^{n+1} - W^T b^{n+1}}\n$$"
        },
        {
            "introduction": "The fidelity of a ROM is fundamentally tied to the quality of its underlying basis, which is trained on snapshot data. A critical question is what data to collect: are state snapshots alone sufficient? This exercise  delves into the principles justifying the augmentation of the snapshot set with fields like time derivatives and external sources, showing how a careful analysis of the full-order model's residual can guide the creation of a more robust and accurate ROM.",
            "id": "3524013",
            "problem": "A semi-discrete multiphysics model, obtained by method-of-lines discretization of coupled conservation laws and algebraic constraints, can be written in the form\n$$\nM(y,\\mu)\\,\\dot{y}(t;\\mu) + r\\!\\left(y(t;\\mu),t;\\mu\\right) \\;=\\; s(t;\\mu), \\quad h\\!\\left(y(t;\\mu),t;\\mu\\right)=0,\n$$\nwhere $y \\in \\mathbb{R}^{N}$ collects all physics fields, $M(y,\\mu)\\in\\mathbb{R}^{N\\times N}$ is a (possibly state- and parameter-dependent) mass operator, $r(y,t;\\mu)$ aggregates nonlinear internal and coupling terms, $s(t;\\mu)$ is an exogenous source, and $h(y,t;\\mu)=0$ are algebraic coupling constraints. Consider a fully implicit second-order backward differentiation formula (BDF$2$) time integrator applied to the differential part, and a Newton method for the resulting nonlinear solves:\n$$\nR^{n+1}(y^{n+1};\\mu) \\;=\\; \\frac{1}{\\Delta t}\\,M(y^{n+1},\\mu)\\left(\\alpha_0\\,y^{n+1}-\\alpha_1\\,y^{n}-\\alpha_2\\,y^{n-1}\\right) - r(y^{n+1},t^{n+1};\\mu) - s(t^{n+1};\\mu) \\;=\\; 0,\n$$\nwith coefficients $\\alpha_0,\\alpha_1,\\alpha_2 \\in \\mathbb{R}$ determined by BDF$2$ and $\\Delta t>0$. The Newton correction at step $n+1$ solves\n$$\n\\left(\\frac{\\alpha_0}{\\Delta t}\\,M(y^{n+1},\\mu) - \\frac{\\partial r}{\\partial y}(y^{n+1},t^{n+1};\\mu)\\right)\\delta y \\;=\\; -R^{n+1}(y^{n+1};\\mu).\n$$\nA projection-based reduced-order model is constructed with a Proper Orthogonal Decomposition (POD) trial basis $V\\in\\mathbb{R}^{N\\times r}$ from state snapshots $\\{y(t_k;\\mu_j)\\}$, and Petrov–Galerkin testing $W\\in\\mathbb{R}^{N\\times r}$. Hyperreduction is used to approximate the evaluation of the nonlinear term and its Jacobian action (e.g., by Discrete Empirical Interpolation Method (DEIM) or Energy-Conserving Sampling and Weighting (ECSW)), using training data formed from snapshot fields.\n\nFrom first principles of the discrete residual and Newton linearization given above, decide under which circumstances augmenting the snapshot set with time-derivative fields $\\{\\dot{y}(t_k;\\mu_j)\\}$ and source fields $\\{s(t_k;\\mu_j)\\}$ is justified to improve reduced-order model fidelity and robustness for the implicit time integrator. Select all options that are justified by a principled derivation.\n\nA. In fully implicit multistep or implicit Runge–Kutta schemes with a non-identity or state-dependent mass operator $M$, Newton’s linear systems couple the trial space through the images $M\\,\\dot{y}$ and $M\\,V$. Augmenting the snapshot set with time-derivative fields (or $M\\,\\dot{y}$) improves the alignment of trial and test spaces with the discrete-time residual and Jacobian, enhancing fidelity and Newton robustness.\n\nB. For explicit time integration schemes that satisfy a Courant–Friedrichs–Lewy condition, including time-derivative snapshots is essential for stability; otherwise, the reduced model tends to be unstable under projection.\n\nC. When hyperreduction is used to approximate the nonlinear term and the exogenous source $s(t;\\mu)$ is time- and parameter-dependent with spatial support or variability not coincident with that of $r(y,t;\\mu)$, including snapshots of $s$ in the hyperreduction training set reduces interpolation bias in the discrete residual and improves the conditioning and convergence of the implicit Newton solves.\n\nD. Because POD optimizes an $\\ell_2$-type error over state snapshots only, adding time-derivative or source snapshots necessarily degrades the basis and should be avoided irrespective of the integrator and hyperreduction strategy.",
            "solution": "The problem asks for a principled justification for augmenting the snapshot set used to train a ROM. The decision hinges on whether the standard state-snapshot basis can accurately represent all terms in the discrete residual equation. The residual at step $n+1$ for the BDF2 scheme is:\n$$ R^{n+1} = \\underbrace{\\frac{1}{\\Delta t}\\,M(y^{n+1},\\mu)\\left(\\alpha_0\\,y^{n+1}-\\alpha_1\\,y^{n}-\\alpha_2\\,y^{n-1}\\right)}_{\\text{Inertial Term}} - \\underbrace{r(y^{n+1},t^{n+1};\\mu)}_{\\text{Internal/Coupling Term}} - \\underbrace{s(t^{n+1};\\mu)}_{\\text{Source Term}} $$\nThe accuracy of the ROM depends on minimizing the projection error of this entire residual.\n\n**Analysis of Option A:** The inertial term approximates the momentum term $M\\dot{y}$. If the mass operator $M$ is not the identity matrix (or a scalar multiple of it), or if it is state-dependent, the spatial structure of the vector field $M\\dot{y}$ can be very different from the spatial structure of the state $y$. A POD basis trained only on state snapshots $\\{y\\}$ may be a poor choice for representing the inertial term, leading to a large projection error and an inaccurate ROM. Furthermore, the reduced Jacobian of the system, which governs the convergence of the Newton solver, contains terms like $W^T M V$. If the basis $V$ is misaligned with the action of $M$, the reduced Jacobian can become ill-conditioned, harming the robustness of the implicit solve. Augmenting the snapshot set with time-derivative fields $\\{\\dot{y}\\}$ (or directly with $\\{M\\dot{y}\\}$) enriches the basis to better capture these dynamics, thus improving both fidelity and robustness. This justifies option A.\n\n**Analysis of Option B:** This option discusses explicit time integration schemes and CFL conditions. The problem is explicitly framed around a fully implicit BDF2 scheme. Therefore, this option is not relevant to the given context.\n\n**Analysis of Option C:** Hyperreduction methods (like DEIM) approximate computationally expensive terms by evaluating them at a small number of points and reconstructing them using a specialized basis. Here, the expensive terms are the nonlinear forcing terms, which can be grouped as $f_{\\text{total}}(y, t; \\mu) = r(y, t; \\mu) + s(t; \\mu)$. For the hyperreduction approximation to be accurate, its training basis must be able to represent the spatial structures of all components it approximates. If the external source $s(t;\\mu)$ has a spatial distribution that is not well-represented by the modes of the internal term $r(y,t;\\mu)$, a hyperreduction basis trained only on snapshots of $\\{r\\}$ will lead to a large error (interpolation bias) when approximating $f_{\\text{total}}$. This error pollutes the computed residual, which is the right-hand side of the Newton update equation, degrading or destroying the convergence of the solver. Therefore, including snapshots of the source term $\\{s\\}$ in the hyperreduction training set is crucial for accuracy and robustness when its spatial support differs from that of $r$. This justifies option C.\n\n**Analysis of Option D:** This statement is incorrect. The goal of the ROM is not merely to reconstruct state snapshots but to accurately solve the governing equations. This requires the basis to represent all dynamically active terms in the residual. Augmenting the basis with other relevant fields (like time derivatives or sources) is a principled way to improve the ROM's overall accuracy, even if it means the basis is no longer strictly optimal in the $\\ell_2$ sense for reconstructing the state snapshots alone. Prioritizing state reconstruction error above all else can lead to a less accurate and less stable model.\n\nBased on this analysis, options A and C are both justified by first principles.",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}