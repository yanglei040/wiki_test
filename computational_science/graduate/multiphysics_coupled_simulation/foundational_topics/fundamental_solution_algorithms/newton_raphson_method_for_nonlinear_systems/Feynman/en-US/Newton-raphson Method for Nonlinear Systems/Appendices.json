{
    "hands_on_practices": [
        {
            "introduction": "Mastering the Newton-Raphson method begins with understanding its core computational step. This practice grounds your understanding by applying the method to a straightforward two-dimensional nonlinear system. By calculating the Jacobian matrix and solving for the initial search direction, you will solidify the foundational procedure of linearization that underpins every Newton iteration .",
            "id": "2190442",
            "problem": "A sensor is being placed in a 2D plane. Its final position, with coordinates $(x, y)$, must simultaneously satisfy two environmental constraints. The first constraint is that the sensor lies on a circle centered at the origin with a radius of 2. The second constraint, arising from a signal strength model, is given by the equation $\\exp(x) + y = 1$.\n\nTo find the coordinates of the sensor, we can use the Newton-Raphson method to solve the corresponding system of nonlinear equations:\n$f_1(x, y) = x^2 + y^2 - 4 = 0$\n$f_2(x, y) = \\exp(x) + y - 1 = 0$\n\nStarting with an initial guess of $\\mathbf{x}_0 = (x_0, y_0)^T = (1, 1)^T$, compute the components of the first search direction vector, $\\Delta \\mathbf{x}_0 = (\\Delta x_0, \\Delta y_0)^T$. Provide your answer as a pair of exact analytic expressions for $\\Delta x_0$ and $\\Delta y_0$.",
            "solution": "We have the system\n$$\nf_{1}(x,y)=x^{2}+y^{2}-4=0,\\qquad f_{2}(x,y)=\\exp(x)+y-1=0.\n$$\nThe Newton-Raphson update for a system uses the Jacobian matrix $J(x,y)$ and solves\n$$\nJ(x_{0},y_{0})\\,\\Delta \\mathbf{x}_{0}=-\\mathbf{f}(x_{0},y_{0}),\n$$\nwhere $\\Delta \\mathbf{x}_{0}=(\\Delta x_{0},\\Delta y_{0})^{T}$ and $\\mathbf{f}=(f_{1},f_{2})^{T}$.\n\nFirst compute the Jacobian:\n$$\nJ(x,y)=\\begin{pmatrix}\n\\frac{\\partial f_{1}}{\\partial x} & \\frac{\\partial f_{1}}{\\partial y} \\\\\n\\frac{\\partial f_{2}}{\\partial x} & \\frac{\\partial f_{2}}{\\partial y}\n\\end{pmatrix}\n=\\begin{pmatrix}\n2x & 2y \\\\\n\\exp(x) & 1\n\\end{pmatrix}.\n$$\nAt the initial guess $(x_{0},y_{0})=(1,1)$, we have\n$$\nJ(1,1)=\\begin{pmatrix}\n2 & 2 \\\\\n\\exp(1) & 1\n\\end{pmatrix}.\n$$\nEvaluate the function vector at $(1,1)$:\n$$\n\\mathbf{f}(1,1)=\\begin{pmatrix}\n1^{2}+1^{2}-4 \\\\\n\\exp(1)+1-1\n\\end{pmatrix}\n=\\begin{pmatrix}\n-2 \\\\\n\\exp(1)\n\\end{pmatrix}.\n$$\nThus the Newton system is\n$$\n\\begin{pmatrix}\n2 & 2 \\\\\n\\exp(1) & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\Delta x_{0} \\\\\n\\Delta y_{0}\n\\end{pmatrix}\n=\n-\\begin{pmatrix}\n-2 \\\\\n\\exp(1)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 \\\\\n-\\exp(1)\n\\end{pmatrix}.\n$$\nThis corresponds to the linear equations\n$$\n2\\Delta x_{0}+2\\Delta y_{0}=2,\\qquad \\exp(1)\\Delta x_{0}+\\Delta y_{0}=-\\exp(1).\n$$\nFrom the first equation, $\\Delta y_{0}=1-\\Delta x_{0}$. Substitute into the second:\n$$\n\\exp(1)\\Delta x_{0}+(1-\\Delta x_{0})=-\\exp(1)\n\\;\\Rightarrow\\;\n\\left(\\exp(1)-1\\right)\\Delta x_{0}=-\\exp(1)-1,\n$$\nso\n$$\n\\Delta x_{0}=-\\frac{\\exp(1)+1}{\\exp(1)-1}.\n$$\nThen\n$$\n\\Delta y_{0}=1-\\Delta x_{0}\n=1+\\frac{\\exp(1)+1}{\\exp(1)-1}\n=\\frac{(\\exp(1)-1)+(\\exp(1)+1)}{\\exp(1)-1}\n=\\frac{2\\exp(1)}{\\exp(1)-1}.\n$$\nTherefore,\n$$\n\\Delta x_{0}=-\\frac{\\exp(1)+1}{\\exp(1)-1},\\qquad\n\\Delta y_{0}=\\frac{2\\exp(1)}{\\exp(1)-1}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}-\\frac{\\exp(1)+1}{\\exp(1)-1} \\\\ \\frac{2\\exp(1)}{\\exp(1)-1}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Real-world multiphysics problems often involve solving nonlinear Partial Differential Equations (PDEs), which, after discretization, lead to large systems of nonlinear algebraic equations. This exercise demonstrates how the Newton-Raphson method is applied in this context, specifically for a system derived from a nonlinear diffusion-reaction equation using the implicit $\\theta$-method for time integration. You will practice the critical skill of deriving the Jacobian for a residual that combines temporal and spatial discretization terms, a common task in computational science and engineering .",
            "id": "3455013",
            "problem": "Consider the semi-discrete system obtained from the one-dimensional nonlinear diffusion-reaction Partial Differential Equation (PDE) $u_{t} = \\nu u_{xx} + \\alpha u - \\beta u^{3}$ on the interval $x \\in (0,L)$ with homogeneous Dirichlet boundary conditions $u(0,t) = u(L,t) = 0$. Using a uniform spatial grid with two interior points (so the number of unknowns is $m=2$), the central-difference Laplacian with homogeneous Dirichlet boundary conditions yields the matrix $A \\in \\mathbb{R}^{2 \\times 2}$ given by $A = \\begin{pmatrix} -2 & 1 \\\\ 1 & -2 \\end{pmatrix}$, and grid spacing $h = L/3$. The resulting semi-discrete Ordinary Differential Equation (ODE) system is $u'(t) = f(u(t))$, where $f: \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$ is defined by $f(u) = \\frac{\\nu}{h^{2}} A u + \\alpha u - \\beta u^{\\odot 3}$, with $u^{\\odot 3}$ denoting the componentwise cube of $u$.\n\nTime integration is performed with the general $\\theta$-method, which, for a time step from $t^{n}$ to $t^{n+1} = t^{n} + \\Delta t$, imposes the nonlinear system\n$$\nG(u^{n+1}) = u^{n+1} - u^{n} - \\Delta t \\Big[(1-\\theta) f(u^{n}) + \\theta f(u^{n+1})\\Big] = 0,\n$$\nto be solved for $u^{n+1} \\in \\mathbb{R}^{2}$.\n\nStarting from standard definitions of Newton’s method for nonlinear systems and basic rules of matrix calculus, do the following:\n\n1. Formulate the Newton iteration to solve $G(u^{n+1}) = 0$, clearly identifying the Newton update and the linear system to be solved at each iteration.\n2. Derive the Jacobian matrix $\\frac{\\partial G}{\\partial u}(u)$ explicitly for the given $f(u)$, expressing it in terms of $\\nu$, $\\alpha$, $\\beta$, $h$, $\\Delta t$, $\\theta$, $A$, and $u$.\n3. For the parameter values $L = 3$, $\\nu = 0.5$, $\\alpha = 1$, $\\beta = 2$, $\\Delta t = 0.1$, $\\theta = 0.7$, and the evaluation point $u^{n+1} = \\begin{pmatrix} 0.1 \\\\ -0.2 \\end{pmatrix}$, compute the determinant of the Jacobian matrix $\\frac{\\partial G}{\\partial u}(u^{n+1})$.\n\nRound your final determinant to four significant figures. Express the final answer as a pure number (unitless).",
            "solution": "### Part 1: Formulation of the Newton Iteration\n\nThe task is to solve the nonlinear system of equations $G(u^{n+1}) = 0$ for the unknown vector $u^{n+1} \\in \\mathbb{R}^{2}$. The system is given by\n$$\nG(u^{n+1}) = u^{n+1} - u^{n} - \\Delta t \\Big[(1-\\theta) f(u^{n}) + \\theta f(u^{n+1})\\Big] = 0\n$$\nNewton's method is an iterative procedure for finding the root of a vector-valued function $G(x)=0$. Starting from an initial guess $x_{(0)}$, it generates a sequence of approximations $x_{(k)}$ via the update formula\n$$\nx_{(k+1)} = x_{(k)} - \\left[ J_G(x_{(k)}) \\right]^{-1} G(x_{(k)})\n$$\nwhere $J_G(x) = \\frac{\\partial G}{\\partial x}(x)$ is the Jacobian matrix of $G$ evaluated at $x$.\n\nIn practice, the inverse of the Jacobian is not computed directly. Instead, at each iteration $k$, we solve a linear system for the update vector $\\delta x = x_{(k+1)} - x_{(k)}$. The linear system is\n$$\nJ_G(x_{(k)}) \\delta x = -G(x_{(k)})\n$$\nOnce $\\delta x$ is found, the next approximation is computed as $x_{(k+1)} = x_{(k)} + \\delta x$.\n\nFor the specific problem at hand, the unknown is $u^{n+1}$. Let us denote the $k$-th Newton iterate for $u^{n+1}$ by the vector $u_{(k)}$. A common initial guess is $u_{(0)} = u^{n}$. The iteration proceeds as follows:\nFor $k = 0, 1, 2, \\dots$ until convergence:\n1.  **Compute the Newton update** $\\delta u \\in \\mathbb{R}^{2}$. This is the core of the iteration and involves solving the following $2 \\times 2$ linear system:\n    $$\n    \\frac{\\partial G}{\\partial u}(u_{(k)}) \\delta u = -G(u_{(k)})\n    $$\n    Here, $\\frac{\\partial G}{\\partial u}(u_{(k)})$ is the Jacobian matrix of $G$ evaluated at the current iterate $u_{(k)}$, and $G(u_{(k)})$ is the residual vector:\n    $$\n    G(u_{(k)}) = u_{(k)} - u^{n} - \\Delta t \\Big[(1-\\theta) f(u^{n}) + \\theta f(u_{(k)})\\Big]\n    $$\n\n2.  **Update the solution**:\n    $$\n    u_{(k+1)} = u_{(k)} + \\delta u\n    $$\nThis iterative process is repeated until the norm of the update, $\\|\\delta u\\|$, or the norm of the residual, $\\|G(u_{(k+1)})\\|$, falls below a prescribed tolerance.\n\n### Part 2: Derivation of the Jacobian Matrix\n\nThe next task is to derive the analytical expression for the Jacobian matrix $\\frac{\\partial G}{\\partial u}(u)$, where for notational simplicity, $u$ stands for the variable $u^{n+1}$. The function $G$ is\n$$\nG(u) = u - u^{n} - \\Delta t \\Big[(1-\\theta) f(u^{n}) + \\theta f(u)\\Big]\n$$\nTo find the Jacobian, we differentiate $G(u)$ with respect to the vector $u$. The terms $u^{n}$ and $f(u^{n})$ are constants with respect to this differentiation.\n$$\n\\frac{\\partial G}{\\partial u}(u) = \\frac{\\partial}{\\partial u}(u) - \\frac{\\partial}{\\partial u}(u^{n}) - \\Delta t (1-\\theta) \\frac{\\partial}{\\partial u}(f(u^{n})) - \\Delta t \\theta \\frac{\\partial}{\\partial u}(f(u))\n$$\nThe derivatives of the constant terms are zero. The derivative of $u$ with respect to itself is the identity matrix $I \\in \\mathbb{R}^{2 \\times 2}$.\n$$\n\\frac{\\partial G}{\\partial u}(u) = I - 0 - 0 - \\Delta t \\theta \\frac{\\partial f}{\\partial u}(u) = I - \\Delta t \\theta \\frac{\\partial f}{\\partial u}(u)\n$$\nNow we need to find the Jacobian of $f(u)$, where $f(u) = \\frac{\\nu}{h^{2}} A u + \\alpha u - \\beta u^{\\odot 3}$.\nThe function $f(u)$ consists of a linear part and a nonlinear part. The Jacobian of the linear part, $\\frac{\\nu}{h^{2}} A u + \\alpha u = (\\frac{\\nu}{h^{2}} A + \\alpha I)u$, is simply the matrix coefficient:\n$$\n\\frac{\\partial}{\\partial u} \\left(\\frac{\\nu}{h^{2}} A u + \\alpha u\\right) = \\frac{\\nu}{h^{2}} A + \\alpha I\n$$\nThe nonlinear part is $-\\beta u^{\\odot 3}$. Let $u = \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix}$. Then $-\\beta u^{\\odot 3} = -\\beta \\begin{pmatrix} u_1^3 \\\\ u_2^3 \\end{pmatrix}$. The Jacobian of this term is:\n$$\n\\frac{\\partial}{\\partial u} (-\\beta u^{\\odot 3}) = -\\beta \\begin{pmatrix} \\frac{\\partial(u_1^3)}{\\partial u_1} & \\frac{\\partial(u_1^3)}{\\partial u_2} \\\\ \\frac{\\partial(u_2^3)}{\\partial u_1} & \\frac{\\partial(u_2^3)}{\\partial u_2} \\end{pmatrix} = -\\beta \\begin{pmatrix} 3u_1^2 & 0 \\\\ 0 & 3u_2^2 \\end{pmatrix} = -3\\beta \\cdot \\text{diag}(u_1^2, u_2^2)\n$$\nThis can be written compactly as $-3\\beta \\cdot \\text{diag}(u^{\\odot 2})$.\nCombining the Jacobians of the parts of $f(u)$:\n$$\n\\frac{\\partial f}{\\partial u}(u) = \\frac{\\nu}{h^{2}} A + \\alpha I - 3\\beta \\cdot \\text{diag}(u^{\\odot 2})\n$$\nFinally, substituting this into the expression for $\\frac{\\partial G}{\\partial u}(u)$:\n$$\n\\frac{\\partial G}{\\partial u}(u) = I - \\Delta t \\theta \\left[ \\frac{\\nu}{h^{2}} A + \\alpha I - 3\\beta \\cdot \\text{diag}(u^{\\odot 2}) \\right]\n$$\nDistributing the scalar factors yields the final expression for the Jacobian matrix:\n$$\n\\frac{\\partial G}{\\partial u}(u) = (1 - \\Delta t \\theta \\alpha)I - \\frac{\\Delta t \\theta \\nu}{h^2} A + 3 \\Delta t \\theta \\beta \\cdot \\text{diag}(u^{\\odot 2})\n$$\n\n### Part 3: Computation of the Determinant\n\nWe are asked to compute the determinant of the Jacobian $\\frac{\\partial G}{\\partial u}(u^{n+1})$ for the given parameter values.\nThe parameters are:\n$L = 3$, $\\nu = 0.5$, $\\alpha = 1$, $\\beta = 2$, $\\Delta t = 0.1$, $\\theta = 0.7$.\nThe evaluation point is $u^{n+1} = \\begin{pmatrix} 0.1 \\\\ -0.2 \\end{pmatrix}$.\nFirst, we calculate the grid spacing $h$:\n$h = L/3 = 3/3 = 1$.\n\nNext, we compute the scalar coefficients in the Jacobian expression:\nThe coefficient of the identity matrix $I$:\n$$\n1 - \\Delta t \\theta \\alpha = 1 - (0.1)(0.7)(1) = 1 - 0.07 = 0.93\n$$\nThe coefficient of the matrix $A$:\n$$\n-\\frac{\\Delta t \\theta \\nu}{h^2} = -\\frac{(0.1)(0.7)(0.5)}{1^2} = -0.035\n$$\nThe coefficient of the diagonal matrix:\n$$\n3 \\Delta t \\theta \\beta = 3(0.1)(0.7)(2) = 0.42\n$$\nNow, we construct the diagonal matrix term from the evaluation point $u = u^{n+1}$:\n$$\nu^{\\odot 2} = \\begin{pmatrix} (0.1)^2 \\\\ (-0.2)^2 \\end{pmatrix} = \\begin{pmatrix} 0.01 \\\\ 0.04 \\end{pmatrix}\n$$\nSo, the full diagonal term is:\n$$\n3 \\Delta t \\theta \\beta \\cdot \\text{diag}(u^{\\odot 2}) = 0.42 \\begin{pmatrix} 0.01 & 0 \\\\ 0 & 0.04 \\end{pmatrix} = \\begin{pmatrix} 0.0042 & 0 \\\\ 0 & 0.0168 \\end{pmatrix}\n$$\nWe can now assemble the Jacobian matrix, which we denote as $J_G$:\n$$\nJ_G = 0.93 \\cdot I - 0.035 \\cdot A + \\begin{pmatrix} 0.0042 & 0 \\\\ 0 & 0.0168 \\end{pmatrix}\n$$\nSubstituting $I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$ and $A = \\begin{pmatrix} -2 & 1 \\\\ 1 & -2 \\end{pmatrix}$:\n$$\nJ_G = 0.93 \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - 0.035 \\begin{pmatrix} -2 & 1 \\\\ 1 & -2 \\end{pmatrix} + \\begin{pmatrix} 0.0042 & 0 \\\\ 0 & 0.0168 \\end{pmatrix}\n$$\n$$\nJ_G = \\begin{pmatrix} 0.93 & 0 \\\\ 0 & 0.93 \\end{pmatrix} + \\begin{pmatrix} 0.07 & -0.035 \\\\ -0.035 & 0.07 \\end{pmatrix} + \\begin{pmatrix} 0.0042 & 0 \\\\ 0 & 0.0168 \\end{pmatrix}\n$$\nAdding the matrices component-wise:\n$$\nJ_G = \\begin{pmatrix} 0.93 + 0.07 + 0.0042 & -0.035 \\\\ -0.035 & 0.93 + 0.07 + 0.0168 \\end{pmatrix}\n$$\n$$\nJ_G = \\begin{pmatrix} 1.0042 & -0.035 \\\\ -0.035 & 1.0168 \\end{pmatrix}\n$$\nFinally, we compute the determinant of this $2 \\times 2$ matrix:\n$$\n\\det(J_G) = (1.0042)(1.0168) - (-0.035)(-0.035)\n$$\n$$\n\\det(J_G) = 1.02107496 - (0.035)^2\n$$\n$$\n\\det(J_G) = 1.02107496 - 0.001225\n$$\n$$\n\\det(J_G) = 1.01984996\n$$\nThe problem requires rounding the result to four significant figures. The first four significant digits are $1, 0, 1, 9$. The fifth digit is $8$, so we round up the fourth digit.\n$$\n\\det(J_G) \\approx 1.020\n$$\nThe trailing zero is significant in this context.",
            "answer": "$$\\boxed{1.020}$$"
        },
        {
            "introduction": "For large-scale coupled multiphysics systems, the cost of forming and inverting the full Jacobian matrix at every step can be prohibitive, leading to the use of inexact Newton methods. This advanced practice explores the analysis of such a method, which employs an approximate Jacobian to reduce computational expense. By analyzing a block Gauss-Seidel approximation, you will learn to derive the iteration's convergence operator and use its spectral radius to predict the performance of practical, efficient solution schemes .",
            "id": "3518059",
            "problem": "Consider a coupled nonlinear system arising from a multiphysics model, written abstractly as solving for the root of a vector-valued function $F(x) = 0$, with $x \\in \\mathbb{R}^{n}$ and Jacobian $J(x) = \\partial F / \\partial x$. In a block-partitioned formulation, suppose the unknown vector is split as $x = \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix}$ with corresponding residual partition $F(x) = \\begin{bmatrix} F_{1}(x_{1}, x_{2}) \\\\ F_{2}(x_{1}, x_{2}) \\end{bmatrix}$ and Jacobian at a current iterate denoted by\n$$\nJ = \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix},\n$$\nwhere $A \\in \\mathbb{R}^{n_{1} \\times n_{1}}$, $B \\in \\mathbb{R}^{n_{1} \\times n_{2}}$, $C \\in \\mathbb{R}^{n_{2} \\times n_{1}}$, and $D \\in \\mathbb{R}^{n_{2} \\times n_{2}}$. An inexact Newton method replaces the exact Jacobian $J$ with an approximate Jacobian $M$ to form a linear model for the update. In a block Gauss–Seidel (GS) approximation, one uses the lower block-triangular matrix\n$$\nM = \\begin{bmatrix} A & 0 \\\\ C & D \\end{bmatrix}.\n$$\nStarting from the fundamental Newton linearization definition for a smooth $F$: $F(x + \\delta x) \\approx F(x) + J \\, \\delta x$, the inexact Newton update at each iteration solves $M \\, \\delta x = -F(x)$ instead of $J \\, \\delta x = -F(x)$, and applies $x \\leftarrow x + \\delta x$. The local linear convergence of this scheme near a solution $x^\\star$ can be predicted by analyzing the fixed-point error propagation operator derived from the model error between $J$ and $M$.\n\nTasks:\n1) Derive, from first principles of the Newton method and linearization, the exact linear operator that maps the current linearized error to the next one for the inexact Newton method with the above block Gauss–Seidel (GS) approximate Jacobian $M$. Express this operator purely in terms of $J$ and $M$.\n2) Specialize your derivation to the given block structure and simplify the operator to an explicit block form in terms of $A$, $B$, $C$, and $D$. Then identify how its spectrum depends on these blocks.\n3) Implement a program that, for a given set of test cases specifying blocks $A$, $B$, $C$, and $D$, computes the spectral radius (the maximum absolute value among eigenvalues) of the iteration matrix you derived in Part 2. The spectral radius must be reported as a real number. You should not assume symmetry or commutativity beyond what is explicitly given.\n4) Use the following test suite. All matrices are to be treated as dimensionally consistent real matrices with no physical units. For each case, compute the spectral radius of the iteration matrix and round it to eight decimal places.\n- Test Case 1 (happy path, weak coupling, square blocks): $A = I_{2}$, $D = I_{2}$, $B = 0.2 \\, I_{2}$, $C = 0.3 \\, I_{2}$.\n- Test Case 2 (moderate coupling, nonsymmetric blocks): \n  $A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}$, \n  $D = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$,\n  $B = \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.2 & 0.3 \\end{bmatrix}$,\n  $C = \\begin{bmatrix} 0.4 & 0.0 \\\\ 0.1 & 0.6 \\end{bmatrix}$.\n- Test Case 3 (uncoupled upper block, exact solve): \n  $A = \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix}$,\n  $D = \\begin{bmatrix} 2 & 0.5 \\\\ 0.5 & 1.5 \\end{bmatrix}$,\n  $B = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}$,\n  $C = \\begin{bmatrix} 0.7 & -0.2 \\\\ 0.1 & 0.9 \\end{bmatrix}$.\n- Test Case 4 (scalar blocks, near-convergent): $A = [2]$, $D = [5]$, $B = [3.3]$, $C = [3.0]$.\n- Test Case 5 (scalar blocks, boundary): $A = [2]$, $D = [5]$, $B = [2.0]$, $C = [5.0]$.\n- Test Case 6 (scalar blocks, divergent): $A = [2]$, $D = [5]$, $B = [3.0]$, $C = [3.5]$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the spectral radii for the six test cases in order as a comma-separated list enclosed in square brackets. Each entry must be rounded to eight decimal places. For example, an output with two hypothetical results would look like: $\\texttt{[0.12345678,0.87654321]}$.\n- The answer for each test case must be a real number (float). No other text should be printed.",
            "solution": "The problem requires the derivation of the iteration matrix for a block Gauss-Seidel inexact Newton method and the computation of its spectral radius for several test cases.\n\n### Part 1: Derivation of the General Error Propagation Operator\n\nLet $x^{(k)}$ be the $k$-th iterate in approximating the solution $x^{\\star}$ to the nonlinear system $F(x) = 0$. The error at this iteration is defined as $e^{(k)} = x^{(k)} - x^{\\star}$. By definition, $F(x^{\\star}) = 0$.\n\nThe Taylor expansion of $F(x)$ around the solution $x^{\\star}$ is:\n$$\nF(x^{(k)}) = F(x^{\\star} + e^{(k)}) \\approx F(x^{\\star}) + J(x^{\\star}) e^{(k)}\n$$\nwhere $J(x^{\\star})$ is the Jacobian of $F$ evaluated at the solution. Since $F(x^{\\star}) = 0$, for an iterate $x^{(k)}$ sufficiently close to $x^{\\star}$, we have the approximation:\n$$\nF(x^{(k)}) \\approx J e^{(k)}\n$$\nHere, we denote the Jacobian at the solution as $J = J(x^{\\star})$, and for local convergence analysis, we assume the Jacobian at the current iterate $J(x^{(k)})$ is approximately equal to $J(x^{\\star})$.\n\nThe inexact Newton method defines the update step $\\delta x^{(k)}$ by solving the linear system $M \\, \\delta x^{(k)} = -F(x^{(k)})$, where $M$ is an approximation to the true Jacobian $J$. The update is then applied as $x^{(k+1)} = x^{(k)} + \\delta x^{(k)}$.\n\nSolving for $\\delta x^{(k)}$ gives $\\delta x^{(k)} = -M^{-1} F(x^{(k)})$.\nThe next iterate is $x^{(k+1)} = x^{(k)} - M^{-1} F(x^{(k)})$.\n\nWe can now write the expression for the error at the next iteration, $e^{(k+1)} = x^{(k+1)} - x^{\\star}$:\n$$\ne^{(k+1)} = (x^{(k)} - M^{-1} F(x^{(k)})) - x^{\\star} = (x^{(k)} - x^{\\star}) - M^{-1} F(x^{(k)}) = e^{(k)} - M^{-1} F(x^{(k)})\n$$\nSubstituting the linearization $F(x^{(k)}) \\approx J e^{(k)}$ into this equation yields the relationship between successive errors:\n$$\ne^{(k+1)} \\approx e^{(k)} - M^{-1} (J e^{(k)}) = (I - M^{-1} J) e^{(k)}\n$$\nThe linear operator that maps the current linearized error to the next is the matrix $G = I - M^{-1} J$, known as the iteration matrix. This completes the first task.\n\n### Part 2: Specialization to Block Structure\n\nWe are given the block-partitioned structure for the true Jacobian $J$ and the approximate Jacobian $M$:\n$$\nJ = \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix}, \\quad M = \\begin{bmatrix} A & 0 \\\\ C & D \\end{bmatrix}\n$$\nTo find the iteration matrix $G = I - M^{-1} J$, we first need to compute the inverse of the block lower-triangular matrix $M$. We assume that the diagonal blocks $A$ and $D$ are invertible, which is a standard requirement for this method's applicability. The inverse $M^{-1}$ will also be block lower-triangular:\n$$\nM^{-1} = \\begin{bmatrix} A^{-1} & 0 \\\\ -D^{-1}CA^{-1} & D^{-1} \\end{bmatrix}\n$$\nThis can be verified by checking that $M M^{-1} = I$.\n\nNext, we compute the product $M^{-1}J$:\n$$\nM^{-1}J = \\begin{bmatrix} A^{-1} & 0 \\\\ -D^{-1}CA^{-1} & D^{-1} \\end{bmatrix} \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix}\n$$\n$$\nM^{-1}J = \\begin{bmatrix} (A^{-1}A + 0 \\cdot C) & (A^{-1}B + 0 \\cdot D) \\\\ (-D^{-1}CA^{-1}A + D^{-1}C) & (-D^{-1}CA^{-1}B + D^{-1}D) \\end{bmatrix}\n$$\n$$\nM^{-1}J = \\begin{bmatrix} I & A^{-1}B \\\\ (-D^{-1}C + D^{-1}C) & (I - D^{-1}CA^{-1}B) \\end{bmatrix}\n$$\n$$\nM^{-1}J = \\begin{bmatrix} I & A^{-1}B \\\\ 0 & I - D^{-1}CA^{-1}B \\end{bmatrix}\n$$\nNow, we can find the iteration matrix $G$:\n$$\nG = I - M^{-1}J = \\begin{bmatrix} I & 0 \\\\ 0 & I \\end{bmatrix} - \\begin{bmatrix} I & A^{-1}B \\\\ 0 & I - D^{-1}CA^{-1}B \\end{bmatrix}\n$$\n$$\nG = \\begin{bmatrix} 0 & -A^{-1}B \\\\ 0 & D^{-1}CA^{-1}B \\end{bmatrix}\n$$\nThis is the explicit block form of the iteration matrix.\n\nThe eigenvalues of a block (upper or lower) triangular matrix are the union of the eigenvalues of its diagonal blocks. In this case, the diagonal blocks of $G$ are the zero matrix $0 \\in \\mathbb{R}^{n_1 \\times n_1}$ and the matrix $K = D^{-1}CA^{-1}B \\in \\mathbb{R}^{n_2 \\times n_2}$.\n\nThe eigenvalues of the zero matrix are all $0$. Therefore, the spectrum of $G$ is given by:\n$$\n\\text{spec}(G) = \\text{spec}(0) \\cup \\text{spec}(D^{-1}CA^{-1}B) = \\{0\\} \\cup \\text{spec}(D^{-1}CA^{-1}B)\n$$\nThe convergence of the inexact Newton method is determined by the spectral radius $\\rho(G) = \\max_{\\lambda \\in \\text{spec}(G)} |\\lambda|$. From the spectrum derived above, the spectral radius of the iteration matrix is:\n$$\n\\rho(G) = \\max(\\{0\\} \\cup \\{|\\lambda| : \\lambda \\in \\text{spec}(D^{-1}CA^{-1}B)\\}) = \\rho(D^{-1}CA^{-1}B)\n$$\nThus, the local convergence rate is governed by the spectral radius of the matrix product $D^{-1}CA^{-1}B$.\n\n### Part 3 & 4: Implementation and Calculation\n\nThe final task is to implement a program to compute the spectral radius $\\rho(K)$ where $K = D^{-1}CA^{-1}B$ for the provided test cases. This involves constructing the matrices, performing the matrix multiplications and inversions, finding the eigenvalues of the resultant matrix $K$, and taking the maximum of their absolute values.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the spectral radius of the iteration matrix for a block Gauss-Seidel\n    inexact Newton method for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1: happy path, weak coupling, square blocks\n        {\n            \"A\": np.identity(2, dtype=float),\n            \"B\": 0.2 * np.identity(2, dtype=float),\n            \"C\": 0.3 * np.identity(2, dtype=float),\n            \"D\": np.identity(2, dtype=float),\n        },\n        # Test Case 2: moderate coupling, nonsymmetric blocks\n        {\n            \"A\": np.array([[2., 0.], [0., 1.]]),\n            \"B\": np.array([[0.5, 0.1], [0.2, 0.3]]),\n            \"C\": np.array([[0.4, 0.0], [0.1, 0.6]]),\n            \"D\": np.array([[3., 1.], [1., 2.]]),\n        },\n        # Test Case 3: uncoupled upper block, exact solve\n        {\n            \"A\": np.array([[4., 1.], [1., 3.]]),\n            \"B\": np.zeros((2, 2), dtype=float),\n            \"C\": np.array([[0.7, -0.2], [0.1, 0.9]]),\n            \"D\": np.array([[2., 0.5], [0.5, 1.5]]),\n        },\n        # Test Case 4: scalar blocks, near-convergent\n        {\n            \"A\": np.array([[2.]]),\n            \"B\": np.array([[3.3]]),\n            \"C\": np.array([[3.0]]),\n            \"D\": np.array([[5.]]),\n        },\n        # Test Case 5: scalar blocks, boundary\n        {\n            \"A\": np.array([[2.]]),\n            \"B\": np.array([[2.0]]),\n            \"C\": np.array([[5.0]]),\n            \"D\": np.array([[5.]]),\n        },\n        # Test Case 6: scalar blocks, divergent\n        {\n            \"A\": np.array([[2.]]),\n            \"B\": np.array([[3.0]]),\n            \"C\": np.array([[3.5]]),\n            \"D\": np.array([[5.]]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        A, B, C, D = case[\"A\"], case[\"B\"], case[\"C\"], case[\"D\"]\n\n        # The iteration matrix for the error is G, with spectral radius\n        # rho(G) = rho(K), where K = D^{-1} * C * A^{-1} * B.\n        \n        # Calculate matrix K\n        inv_A = np.linalg.inv(A)\n        inv_D = np.linalg.inv(D)\n        \n        K = inv_D @ C @ inv_A @ B\n        \n        # Compute eigenvalues of K.\n        # If K is a 0x0 matrix (empty), its spectrum is empty, and spectral radius is 0.\n        if K.size == 0:\n            spec_rad = 0.0\n        else:\n            eigenvalues = np.linalg.eigvals(K)\n            # Spectral radius is the maximum absolute value of the eigenvalues.\n            spec_rad = np.max(np.abs(eigenvalues))\n        \n        results.append(spec_rad)\n\n    # Format results to eight decimal places and print in the specified format.\n    formatted_results = [f\"{r:.8f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}