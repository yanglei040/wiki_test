{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of any finite element method is the ability to represent differential operators like the gradient. This exercise explores the elegant structure of the gradient operator when using a Bernstein polynomial basis on a simplex . By working through this example, you will gain hands-on experience with how the gradient of a basis function of degree $p$ can be expressed exactly in terms of basis functions of degree $p-1$, revealing the inherent sparsity and efficiency of this approach.",
            "id": "3366732",
            "problem": "Consider an affine triangle in the plane with vertices $\\boldsymbol{v}_{1}=(2,1)$, $\\boldsymbol{v}_{2}=(5,2)$, and $\\boldsymbol{v}_{3}=(-1,4)$. Let $(\\lambda_{1},\\lambda_{2},\\lambda_{3})$ denote the barycentric coordinates on this triangle so that any point $\\boldsymbol{x}$ in the triangle satisfies $\\boldsymbol{x}=\\lambda_{1}\\boldsymbol{v}_{1}+\\lambda_{2}\\boldsymbol{v}_{2}+\\lambda_{3}\\boldsymbol{v}_{3}$ with $\\lambda_{1}+\\lambda_{2}+\\lambda_{3}=1$. In a Discontinuous Galerkin (DG) method using Bernstein polynomial bases on simplices, the degree-$p$ Bernstein basis associated to the multi-index $\\alpha=(\\alpha_{1},\\alpha_{2},\\alpha_{3})$ with $\\alpha_{1}+\\alpha_{2}+\\alpha_{3}=p$ is defined by $B_{\\alpha}^{p}(\\lambda)=\\dfrac{p!}{\\alpha_{1}!\\,\\alpha_{2}!\\,\\alpha_{3}!}\\,\\lambda_{1}^{\\alpha_{1}}\\lambda_{2}^{\\alpha_{2}}\\lambda_{3}^{\\alpha_{3}}$. Starting from these definitions and first principles for barycentric coordinates and the chain rule, you will:\n- Compute the constant gradients $\\nabla \\lambda_{j}$ for $j\\in\\{1,2,3\\}$ on this triangle.\n- Using the canonical degree-$2$ Bernstein basis ordering $\\big\\{(2,0,0),(1,1,0),(1,0,1),(0,2,0),(0,1,1),(0,0,2)\\big\\}$, assemble explicitly the sparse gradient operator mapping the single degree-$3$ interior coefficient $c_{(1,1,1)}$ (associated with $B_{(1,1,1)}^{3}$) to the degree-$2$ coefficient vectors of the $x$- and $y$-components of the gradient field. That is, write the two $6$-entry coefficient vectors that expand $(\\nabla B_{(1,1,1)}^{3})_{x}$ and $(\\nabla B_{(1,1,1)}^{3})_{y}$ in the degree-$2$ Bernstein basis, in the stated ordering.\n\nExpress your final answer as a single row vector whose first six entries are the $x$-component weights and whose last six entries are the $y$-component weights, in exact rational form. No rounding is required, and no units are needed. The final answer must be a single closed-form analytic expression.",
            "solution": "The user wants to solve a problem in the context of the Discontinuous Galerkin (DG) method, specifically concerning Bernstein polynomial bases on triangular elements. The problem consists of two parts: first, to compute the gradients of the barycentric coordinates for a given triangle, and second, to determine the coefficients of the gradient of a specific degree-$3$ Bernstein basis function when expressed in the degree-$2$ Bernstein basis.\n\nThe problem is determined to be valid as it is scientifically grounded in computational mathematics, is well-posed with all necessary information provided, and is formulated objectively without any ambiguities.\n\nThe first step is to compute the constant gradients of the barycentric coordinates $\\lambda_j$ for $j \\in \\{1,2,3\\}$. The vertices of the triangle are given as $\\boldsymbol{v}_{1}=(2,1)$, $\\boldsymbol{v}_{2}=(5,2)$, and $\\boldsymbol{v}_{3}=(-1,4)$. A point $\\boldsymbol{x}=(x,y)$ in the triangle can be expressed as a linear combination of the vertices using barycentric coordinates $(\\lambda_1, \\lambda_2, \\lambda_3)$:\n$$\n\\boldsymbol{x} = \\lambda_{1}\\boldsymbol{v}_{1}+\\lambda_{2}\\boldsymbol{v}_{2}+\\lambda_{3}\\boldsymbol{v}_{3}\n$$\nwith the constraint $\\lambda_{1}+\\lambda_{2}+\\lambda_{3}=1$. This can be written in matrix form as:\n$$\n\\begin{pmatrix} x \\\\ y \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} x_1  x_2  x_3 \\\\ y_1  y_2  y_3 \\\\ 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\\\ \\lambda_3 \\end{pmatrix}\n$$\nSubstituting the vertex coordinates $\\boldsymbol{v}_1=(2,1)$, $\\boldsymbol{v}_2=(5,2)$, $\\boldsymbol{v}_3=(-1,4)$:\n$$\n\\begin{pmatrix} x \\\\ y \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2  5  -1 \\\\ 1  2  4 \\\\ 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\\\ \\lambda_3 \\end{pmatrix}\n$$\nLet $\\mathbf{M}$ be the $3 \\times 3$ matrix of vertex coordinates. To find $\\lambda_j$ as functions of $x$ and $y$, we must invert $\\mathbf{M}$. The determinant of $\\mathbf{M}$ is:\n$$\n\\det(\\mathbf{M}) = 2(2 \\cdot 1 - 4 \\cdot 1) - 5(1 \\cdot 1 - 4 \\cdot 1) + (-1)(1 \\cdot 1 - 2 \\cdot 1) = 2(-2) - 5(-3) - 1(-1) = -4 + 15 + 1 = 12\n$$\nThe inverse matrix $\\mathbf{M}^{-1}$ is given by $\\frac{1}{\\det(\\mathbf{M})}\\text{adj}(\\mathbf{M})$. The adjugate matrix is the transpose of the cofactor matrix.\n$$\n\\mathbf{M}^{-1} = \\frac{1}{12} \\begin{pmatrix} y_2-y_3  x_3-x_2  x_2 y_3 - x_3 y_2 \\\\ y_3-y_1  x_1-x_3  x_3 y_1 - x_1 y_3 \\\\ y_1-y_2  x_2-x_1  x_1 y_2 - x_2 y_1 \\end{pmatrix} = \\frac{1}{12} \\begin{pmatrix} 2-4  -1-5  5(4)-(-1)(2) \\\\ 4-1  2-(-1)  (-1)(1)-2(4) \\\\ 1-2  5-2  2(2)-5(1) \\end{pmatrix} = \\frac{1}{12} \\begin{pmatrix} -2  -6  22 \\\\ 3  3  -9 \\\\ -1  3  -1 \\end{pmatrix}\n$$\nThe barycentric coordinates are found by $\\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\\\ \\lambda_3 \\end{pmatrix} = \\mathbf{M}^{-1} \\begin{pmatrix} x \\\\ y \\\\ 1 \\end{pmatrix}$:\n$$\n\\lambda_1 = \\frac{1}{12}(-2x - 6y + 22) \\\\\n\\lambda_2 = \\frac{1}{12}(3x + 3y - 9) \\\\\n\\lambda_3 = \\frac{1}{12}(-x + 3y - 1)\n$$\nThe gradient of $\\lambda_j$ is $\\nabla \\lambda_j = (\\frac{\\partial \\lambda_j}{\\partial x}, \\frac{\\partial \\lambda_j}{\\partial y})$. These are constant vectors:\n$$\n\\nabla \\lambda_1 = \\frac{1}{12}(-2, -6) = \\left(-\\frac{1}{6}, -\\frac{1}{2}\\right) \\\\\n\\nabla \\lambda_2 = \\frac{1}{12}(3, 3) = \\left(\\frac{1}{4}, \\frac{1}{4}\\right) \\\\\n\\nabla \\lambda_3 = \\frac{1}{12}(-1, 3) = \\left(-\\frac{1}{12}, \\frac{1}{4}\\right)\n$$\nAs a consistency check, $\\nabla\\lambda_1 + \\nabla\\lambda_2 + \\nabla\\lambda_3 = (-\\frac{1}{6}+\\frac{1}{4}-\\frac{1}{12}, -\\frac{1}{2}+\\frac{1}{4}+\\frac{1}{4}) = (\\frac{-2+3-1}{12}, \\frac{-2+1+1}{4}) = (0,0)$, as expected.\n\nThe second part of the problem is to compute the gradient of the degree-$p=3$ Bernstein basis function $B_{\\alpha}^{3}$ for $\\alpha=(1,1,1)$, and express its components in the degree-$p-1=2$ basis. The formula for the gradient of a Bernstein basis function $B_{\\alpha}^{p}$ is:\n$$\n\\nabla B_{\\alpha}^{p}(\\lambda) = p \\sum_{j=1}^{3} B_{\\alpha-e_j}^{p-1}(\\lambda) \\nabla\\lambda_j\n$$\nwhere $e_j$ are the standard multi-index unit vectors, and the sum is over $j$ for which $\\alpha_j0$. For $\\alpha=(1,1,1)$ and $p=3$:\n$$\n\\nabla B_{(1,1,1)}^{3}(\\lambda) = 3 \\left( B_{(1,1,1)-e_1}^{2} \\nabla\\lambda_1 + B_{(1,1,1)-e_2}^{2} \\nabla\\lambda_2 + B_{(1,1,1)-e_3}^{2} \\nabla\\lambda_3 \\right) \\\\\n= 3 B_{(0,1,1)}^{2} \\nabla\\lambda_1 + 3 B_{(1,0,1)}^{2} \\nabla\\lambda_2 + 3 B_{(1,1,0)}^{2} \\nabla\\lambda_3\n$$\nThis expression represents the vector field $\\nabla B_{(1,1,1)}^{3}$ as a linear combination of degree-$2$ Bernstein basis functions, where the coefficients are constant vectors. We need to find the scalar coefficients for the $x$- and $y$-components of this gradient field, expanded in the ordered degree-$2$ basis $\\beta \\in \\big\\{(2,0,0),(1,1,0),(1,0,1),(0,2,0),(0,1,1),(0,0,2)\\big\\}$.\n\nLet the expansion be $\\nabla B_{(1,1,1)}^{3} = \\sum_{|\\beta|=2} \\boldsymbol{c}_{\\beta} B_{\\beta}^{2}$. From the formula above, the non-zero vector coefficients $\\boldsymbol{c}_{\\beta}$ are:\n$$\n\\boldsymbol{c}_{(0,1,1)} = 3\\nabla\\lambda_1 = 3\\left(-\\frac{1}{6}, -\\frac{1}{2}\\right) = \\left(-\\frac{1}{2}, -\\frac{3}{2}\\right) \\\\\n\\boldsymbol{c}_{(1,0,1)} = 3\\nabla\\lambda_2 = 3\\left(\\frac{1}{4}, \\frac{1}{4}\\right) = \\left(\\frac{3}{4}, \\frac{3}{4}\\right) \\\\\n\\boldsymbol{c}_{(1,1,0)} = 3\\nabla\\lambda_3 = 3\\left(-\\frac{1}{12}, \\frac{1}{4}\\right) = \\left(-\\frac{1}{4}, \\frac{3}{4}\\right)\n$$\nAll other coefficients $\\boldsymbol{c}_{(2,0,0)}, \\boldsymbol{c}_{(0,2,0)}, \\boldsymbol{c}_{(0,0,2)}$ are zero vectors.\n\nThe problem asks for two $6$-entry coefficient vectors, corresponding to the $x$- and $y$-components of the gradient, ordered according to the given basis. Let the ordered vector of coefficients for the $x$-component be $\\mathbf{g}_x$ and for the $y$-component be $\\mathbf{g}_y$.\n\nThe basis ordering is:\n1. $\\beta = (2,0,0)$: Coefficient is $\\boldsymbol{c}_{(2,0,0)} = (0,0)$.\n2. $\\beta = (1,1,0)$: Coefficient is $\\boldsymbol{c}_{(1,1,0)} = (-\\frac{1}{4}, \\frac{3}{4})$.\n3. $\\beta = (1,0,1)$: Coefficient is $\\boldsymbol{c}_{(1,0,1)} = (\\frac{3}{4}, \\frac{3}{4})$.\n4. $\\beta = (0,2,0)$: Coefficient is $\\boldsymbol{c}_{(0,2,0)} = (0,0)$.\n5. $\\beta = (0,1,1)$: Coefficient is $\\boldsymbol{c}_{(0,1,1)} = (-\\frac{1}{2}, -\\frac{3}{2})$.\n6. $\\beta = (0,0,2)$: Coefficient is $\\boldsymbol{c}_{(0,0,2)} = (0,0)$.\n\nCollecting the $x$-components gives the vector $\\mathbf{g}_x$:\n$$\n\\mathbf{g}_x = \\left(0, -\\frac{1}{4}, \\frac{3}{4}, 0, -\\frac{1}{2}, 0\\right)\n$$\nCollecting the $y$-components gives the vector $\\mathbf{g}_y$:\n$$\n\\mathbf{g}_y = \\left(0, \\frac{3}{4}, \\frac{3}{4}, 0, -\\frac{3}{2}, 0\\right)\n$$\nThe final answer is a single row vector containing the entries of $\\mathbf{g}_x$ followed by the entries of $\\mathbf{g}_y$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  -\\frac{1}{4}  \\frac{3}{4}  0  -\\frac{1}{2}  0  0  \\frac{3}{4}  \\frac{3}{4}  0  -\\frac{3}{2}  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Discontinuous Galerkin methods are built upon weak formulations that involve both volume and surface integrals. This practice problem focuses on the 'lifting operator', a crucial tool that translates information from an element's face—such as a numerical flux—into a representation within the element's volume . By computing the integral of a lifted function, you will verify a key consistency property and gain insight into how DG methods enforce inter-element communication.",
            "id": "3366699",
            "problem": "Consider the Discontinuous Galerkin (DG) method on the reference triangle $K$ with vertices $(0,0)$, $(1,0)$, and $(0,1)$. Let $\\lambda_{1}=1-x-y$, $\\lambda_{2}=x$, and $\\lambda_{3}=y$ denote the barycentric coordinates on $K$. For total degree $n=2$, the Bernstein basis on $K$ is given by $B^{2}_{i j k} = \\dfrac{2!}{i! j! k!} \\lambda_{1}^{i} \\lambda_{2}^{j} \\lambda_{3}^{k}$ with $i+j+k=2$. Let $f$ be the edge opposite to the vertex associated with $\\lambda_{1}$, so that $f=\\{\\lambda_{1}=0\\}$ is the segment between $(1,0)$ and $(0,1)$. Parameterize $f$ by $t \\in [0,1]$ via $(x(t),y(t))=(t,1-t)$.\n\nOn the face $f$, consider the one-dimensional degree-$2$ Bernstein basis functions $b_{0}(t)=(1-t)^{2}$, $b_{1}(t)=2 t (1-t)$, and $b_{2}(t)=t^{2}$. Define the face lifting operator $L_{f}:\\mathbb{P}^{2}(f)\\to \\mathbb{P}^{2}(K)$ through the Riesz representation with respect to the $L^{2}(K)$ inner product: for any $r \\in \\mathbb{P}^{2}(f)$ and any $v \\in \\mathbb{P}^{2}(K)$,\n$$\n\\int_{K} L_{f}(r)\\, v \\, \\mathrm{d}x = \\int_{f} r\\, v \\, \\mathrm{d}s.\n$$\nThis lifting operator is the canonical volume representation of numerical flux contributions supported on $f$.\n\nTake the specific face function $r=b_{1}(t)=2 t (1-t)$. Using only the above fundamental definitions and standard geometric facts about $K$ and $f$, compute the scalar quantity\n$$\n\\int_{K} L_{f}(r)\\, \\mathrm{d}x,\n$$\nand verify the corresponding consistency relation obtained by choosing the constant test function $v \\equiv 1$ in the defining identity of $L_{f}$, interpreted as a discrete counterpart of the divergence theorem. Your final answer must be the exact value of the integral, expressed in simplest closed form with no numerical rounding.",
            "solution": "The problem requires the computation of the scalar quantity $\\int_{K} L_{f}(r)\\, \\mathrm{d}x$ and the verification of a consistency relation. The object $L_{f}(r)$ is the image of a face function $r \\in \\mathbb{P}^{2}(f)$ under the face lifting operator $L_{f}:\\mathbb{P}^{2}(f)\\to \\mathbb{P}^{2}(K)$. The operator is defined by its action in a weak formulation, through the Riesz representation with respect to the $L^{2}(K)$ inner product. Specifically, for any test function $v \\in \\mathbb{P}^{2}(K)$, the following identity holds:\n$$\n\\int_{K} L_{f}(r)\\, v \\, \\mathrm{d}x = \\int_{f} r\\, v \\, \\mathrm{d}s\n$$\nwhere $\\mathbb{P}^{2}(K)$ is the space of polynomials on the reference triangle $K$ with total degree at most $2$.\n\nThe primary task is to compute the value of $\\int_{K} L_{f}(r)\\, \\mathrm{d}x$. We are also asked to verify a consistency relation by choosing the constant test function $v \\equiv 1$. These two tasks are directly related. We observe that the integral we need to compute, $\\int_{K} L_{f}(r)\\, \\mathrm{d}x$, is precisely the left-hand side of the defining identity for $L_{f}$ when the test function $v$ is chosen to be the constant function $v(x,y) = 1$.\n\nThe function $v \\equiv 1$ is a polynomial of degree $0$. Since $0 \\le 2$, it is a valid member of the test space $\\mathbb{P}^{2}(K)$. Therefore, we are permitted to substitute $v=1$ into the defining equation. This substitution yields:\n$$\n\\int_{K} L_{f}(r) \\cdot 1 \\, \\mathrm{d}x = \\int_{f} r \\cdot 1 \\, \\mathrm{d}s\n$$\nThis simplifies to:\n$$\n\\int_{K} L_{f}(r) \\, \\mathrm{d}x = \\int_{f} r \\, \\mathrm{d}s\n$$\nThis equation is the consistency relation mentioned in the problem. It demonstrates that the volume integral of the lifted function is equal to the face integral of the original face function. By establishing this equality, we fulfill the verification part of the task. To complete the problem, we only need to evaluate the face integral on the right-hand side.\n\nThe face $f$ is the segment connecting the vertices $(1,0)$ and $(0,1)$. It is parameterized by $t \\in [0,1]$ via the path $\\vec{\\gamma}(t) = (x(t), y(t)) = (t, 1-t)$. The differential arc length element $\\mathrm{d}s$ is given by $||\\vec{\\gamma}'(t)|| \\, \\mathrm{d}t$. First, we compute the derivative of the path vector:\n$$\n\\vec{\\gamma}'(t) = \\frac{\\mathrm{d}}{\\mathrm{d}t}(t, 1-t) = (1, -1)\n$$\nThe magnitude of this vector, which is the speed, is constant along the path:\n$$\n||\\vec{\\gamma}'(t)|| = \\sqrt{1^{2} + (-1)^{2}} = \\sqrt{1+1} = \\sqrt{2}\n$$\nThus, the arc length element is $\\mathrm{d}s = \\sqrt{2} \\, \\mathrm{d}t$.\n\nThe function to be integrated over the face is specified as $r = b_{1}(t) = 2t(1-t)$. We can now set up and compute the definite integral over the face $f$:\n$$\n\\int_{f} r \\, \\mathrm{d}s = \\int_{0}^{1} b_{1}(t) \\sqrt{2} \\, \\mathrm{d}t = \\int_{0}^{1} 2t(1-t) \\sqrt{2} \\, \\mathrm{d}t\n$$\nWe move the constant factor $2\\sqrt{2}$ outside the integral:\n$$\n\\int_{f} r \\, \\mathrm{d}s = 2\\sqrt{2} \\int_{0}^{1} (t-t^{2}) \\, \\mathrm{d}t\n$$\nWe evaluate the integral of the polynomial $t-t^{2}$:\n$$\n\\int_{0}^{1} (t-t^{2}) \\, \\mathrm{d}t = \\left[ \\frac{t^{2}}{2} - \\frac{t^{3}}{3} \\right]_{0}^{1} = \\left(\\frac{1^{2}}{2} - \\frac{1^{3}}{3}\\right) - \\left(\\frac{0^{2}}{2} - \\frac{0^{3}}{3}\\right) = \\frac{1}{2} - \\frac{1}{3} = \\frac{3-2}{6} = \\frac{1}{6}\n$$\nSubstituting this result back, we find the value of the face integral:\n$$\n\\int_{f} r \\, \\mathrm{d}s = 2\\sqrt{2} \\left(\\frac{1}{6}\\right) = \\frac{2\\sqrt{2}}{6} = \\frac{\\sqrt{2}}{3}\n$$\nBased on the consistency relation derived earlier, the value of the required volume integral is equal to this result.\n$$\n\\int_{K} L_{f}(r)\\, \\mathrm{d}x = \\frac{\\sqrt{2}}{3}\n$$\nThis concludes the computation. The result is an exact value in simplest closed form.",
            "answer": "$$\\boxed{\\frac{\\sqrt{2}}{3}}$$"
        },
        {
            "introduction": "One of the most celebrated advantages of the Bernstein basis is its convex hull property, which provides a direct link between the values of the polynomial and its coefficients. This hands-on coding exercise  guides you through implementing a 'positivity-preserving' limiter, a vital component for robustly simulating problems with sharp gradients or discontinuities. You will see firsthand how to enforce physical bounds on the solution while preserving fundamental quantities like the cell average, a task made significantly more transparent by the Bernstein representation.",
            "id": "3366720",
            "problem": "Consider one-dimensional Discontinuous Galerkin (DG) discretizations using the Bernstein polynomial basis on each cell. On the reference interval, the degree-$p$ Bernstein polynomials are defined by $B_i^{p}(x) = \\binom{p}{i} x^i (1-x)^{p-i}$ for $i = 0,1,\\dots,p$ and $x \\in [0,1]$. A local polynomial $u(x)$ represented in the Bernstein basis has the form $u(x) = \\sum_{i=0}^{p} c_i B_i^p(x)$, where $c_i$ are the Bernstein coefficients. Two fundamental properties of the Bernstein basis are: (1) the convex hull property, which states that $u(x) \\in [\\min_i c_i, \\max_i c_i]$ for all $x \\in [0,1]$, and (2) the integral identity $\\int_0^1 B_i^p(x)\\,dx = \\frac{1}{p+1}$, implying the cell mean $\\bar{u} = \\int_0^1 u(x)\\,dx = \\frac{1}{p+1}\\sum_{i=0}^{p} c_i$.\n\nYou are to implement a simple slope limiter in Bernstein coefficient space that acts independently on each cell. The limiter must satisfy the following constraints for each cell:\n- It must preserve the cell mean $\\bar{u}$ exactly.\n- It must enforce coefficient bounds determined solely by the means of the immediate neighboring cells. For a cell index $j$ with neighbors $j-1$ and $j+1$, define $L_j = \\min(\\bar{u}_{j-1}, \\bar{u}_{j+1})$ and $U_j = \\max(\\bar{u}_{j-1}, \\bar{u}_{j+1})$. At the domain boundaries where a neighbor is missing, use the cell’s own mean in place of the missing neighbor. The limited coefficients $c_{j,i}^{\\text{lim}}$ must satisfy $L_j \\le c_{j,i}^{\\text{lim}} \\le U_j$ for all $i$ and $\\frac{1}{p+1}\\sum_{i=0}^{p} c_{j,i}^{\\text{lim}} = \\bar{u}_j$.\n\nThe slope limiter should be constructed using only these constraints and fundamental facts; do not assume any additional shortcut formulas. The numerical study must analyze a test discontinuity in the data by quantifying pre- and post-limiter overshoot and mean preservation.\n\nImplement a program that:\n1. For each test case, computes the neighbor-based bounds $L_j$ and $U_j$ from the original (unlimited) cell means.\n2. Applies the limiter cellwise to produce limited Bernstein coefficients that preserve each cell’s original mean and satisfy the bounds.\n3. Computes, for the entire mesh of the test case:\n   - The maximum coefficient overshoot magnitude before limiting: $\\max_{j,i}\\left(\\max(c_{j,i}-U_j, 0), \\max(L_j - c_{j,i}, 0)\\right)$.\n   - The maximum coefficient overshoot magnitude after limiting, defined in the same way but using $c_{j,i}^{\\text{lim}}$.\n   - The maximum absolute mean error after limiting: $\\max_j \\left|\\frac{1}{p_j+1}\\sum_{i=0}^{p_j} c_{j,i}^{\\text{lim}} - \\frac{1}{p_j+1}\\sum_{i=0}^{p_j} c_{j,i}\\right|$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, aggregating the results for all test cases in order, where each test case result is the list `[overshoot_before, overshoot_after, mean_error]`.\n\nUse the following test suite. Each test case specifies the polynomial degree `p`, the number of cells `N`, and the Bernstein coefficient arrays per cell. All numbers below are real scalars.\n\n- Test Case A (happy path near a discontinuity): `p` = 3, `N` = 6, with per-cell Bernstein coefficient arrays `[c_{j,0}, c_{j,1}, c_{j,2}, c_{j,3}]` for `j` = 0,1,\\dots,5:\n  - Cell $0$: $[0.0, 0.0, 0.0, 0.0]$.\n  - Cell $1$: $[0.0, -0.05, 0.05, 0.0]$.\n  - Cell $2$: $[-0.2, 0.1, 0.1, 0.0]$.\n  - Cell $3$: $[1.3, 1.1, 0.7, 0.9]$.\n  - Cell $4$: $[1.0, 1.0, 1.0, 1.0]$.\n  - Cell $5$: $[1.0, 1.05, 0.95, 1.0]$.\n\n- Test Case B (higher degree with stronger oscillations): `p` = 5, `N` = 6, with per-cell arrays `[c_{j,0}, c_{j,1}, c_{j,2}, c_{j,3}, c_{j,4}, c_{j,5}]`:\n  - Cell $0$: $[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$.\n  - Cell $1$: $[0.0, -0.1, 0.0, 0.05, -0.05, 0.1]$.\n  - Cell $2$: $[-0.3, 0.05, 0.2, 0.1, -0.05, 0.0]$.\n  - Cell $3$: $[1.2, 1.1, 0.6, 0.9, 1.0, 1.2]$.\n  - Cell $4$: $[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]$.\n  - Cell $5$: $[1.0, 0.95, 1.05, 1.0, 1.0, 1.0]$.\n\n- Test Case C (already within bounds): `p` = 3, `N` = 3:\n  - Cell $0$: $[0.0, 0.0, 0.0, 0.0]$.\n  - Cell $1$: $[0.4, 0.5, 0.6, 0.5]$.\n  - Cell $2$: $[1.0, 1.0, 1.0, 1.0]$.\n\n- Test Case D (boundary handling): `p` = 4, `N` = 2:\n  - Cell $0$: $[-0.1, 0.0, 0.2, -0.05, -0.05]$.\n  - Cell $1$: $[1.1, 0.8, 1.2, 1.0, 0.9]$.\n\nYour implementation should be general for any `p`, `N`, and coefficient arrays, but must run on these specified tests without external input. The final output must be a single line printed as described above.",
            "solution": "The user has provided a problem in the field of numerical methods for partial differential equations, specifically concerning the implementation of a slope limiter for a Discontinuous Galerkin (DG) method using a Bernstein polynomial basis. The task requires validating the problem statement, developing a suitable limiting algorithm based on prescribed constraints, implementing it, and evaluating its performance on a given set of test cases.\n\n### Problem Validation\n\nThe problem is evaluated against the criteria of being scientifically grounded, well-posed, and objective.\n\n-   **Givens Extraction**:\n    -   **Basis**: 1D Bernstein polynomials on $[0,1]$, $B_i^{p}(x) = \\binom{p}{i} x^i (1-x)^{p-i}$.\n    -   **Representation**: A polynomial in cell $j$ is $u_j(x) = \\sum_{i=0}^{p} c_{j,i} B_i^p(x)$.\n    -   **Cell Mean**: $\\bar{u}_j = \\frac{1}{p+1}\\sum_{i=0}^{p} c_{j,i}$.\n    -   **Limiter Constraints**: For limited coefficients $c_{j,i}^{\\text{lim}}$:\n        1.  **Mean Preservation**: $\\frac{1}{p+1}\\sum_{i=0}^{p} c_{j,i}^{\\text{lim}} = \\bar{u}_j$.\n        2.  **Coefficient Bounds**: $L_j \\le c_{j,i}^{\\text{lim}} \\le U_j$.\n    -   **Bound Definition**: For cell $j$, $L_j = \\min(\\bar{u}_{j-1}, \\bar{u}_{j+1})$ and $U_j = \\max(\\bar{u}_{j-1}, \\bar{u}_{j+1})$. At boundaries, the cell's own mean $\\bar{u}_j$ is used for the missing neighbor.\n    -   **Metrics**: Pre- and post-limiter maximum coefficient overshoot, and post-limiter maximum absolute mean error.\n    -   **Test Data**: Four test cases (A, B, C, D) with specified polynomial degrees $p$, cell counts $N$, and initial Bernstein coefficients $c_{j,i}$.\n\n-   **Validation Verdict**:\n    1.  **Scientifically Grounded**: The problem is well-grounded in the theory of high-order numerical methods for conservation laws. Bernstein polynomials, DG methods, and slope limiting are standard topics. The provided properties are correct.\n    2.  **Well-Posed**: The problem defines a set of constraints for the limited solution. A necessary condition for a solution to exist is that the cell mean $\\bar{u}_j$ must lie within the bounds $[L_j, U_j]$. A check on all provided test cases confirms that this condition, $L_j \\le \\bar{u}_j \\le U_j$, holds for every cell. Thus, the constraints are not contradictory. The problem asks for the construction of a limiter, and a standard, principled algorithm can be derived from the constraints, making the problem well-posed.\n    3.  **Objective**: All definitions, constraints, and required outputs are specified with mathematical precision, leaving no room for subjectivity.\n\nThe problem is deemed **valid**. A reasoned solution follows.\n\n### Solution Derivation\n\nThe objective is to find a set of limited Bernstein coefficients $\\{c_{j,i}^{\\text{lim}}\\}_{i=0}^p$ for each cell $j$ that satisfies two constraints: exact preservation of the cell mean and adherence to local bounds defined by neighboring cell means.\n\nLet us focus on a single cell $j$. We have the original coefficients $c_{j,i}$, the original cell mean $\\bar{u}_j$, and the prescribed bounds $L_j$ and $U_j$. The constraints on the new coefficients $c_{j,i}^{\\text{lim}}$ are:\n1.  $\\sum_{i=0}^p c_{j,i}^{\\text{lim}} = \\sum_{i=0}^p c_{j,i}$ (which implies $\\bar{u}_j^{\\text{lim}} = \\bar{u}_j$).\n2.  $L_j \\le c_{j,i}^{\\text{lim}} \\le U_j$ for all $i \\in \\{0, \\dots, p\\}$.\n\nA principled way to construct such a limiter is to scale the fluctuations of the coefficients around the cell mean. We propose a transformation of the form:\n$$\nc_{j,i}^{\\text{lim}} = \\bar{u}_j + \\theta_j (c_{j,i} - \\bar{u}_j)\n$$\nwhere $\\theta_j \\in [0, 1]$ is a scaling factor for cell $j$. A value of $\\theta_j = 1$ means no limiting is applied, while $\\theta_j = 0$ flattens the solution to the cell mean.\n\nLet's verify that this transformation preserves the cell mean for any $\\theta_j$:\n$$\n\\sum_{i=0}^p c_{j,i}^{\\text{lim}} = \\sum_{i=0}^p \\left( \\bar{u}_j + \\theta_j (c_{j,i} - \\bar{u}_j) \\right) = \\sum_{i=0}^p \\bar{u}_j + \\theta_j \\sum_{i=0}^p (c_{j,i} - \\bar{u}_j)\n$$\n$$\n= (p+1)\\bar{u}_j + \\theta_j \\left( \\left(\\sum_{i=0}^p c_{j,i}\\right) - \\sum_{i=0}^p \\bar{u}_j \\right) = (p+1)\\bar{u}_j + \\theta_j \\left( (p+1)\\bar{u}_j - (p+1)\\bar{u}_j \\right) = (p+1)\\bar{u}_j\n$$\nDividing by $(p+1)$ confirms that $\\bar{u}_j^{\\text{lim}} = \\bar{u}_j$. The mean is preserved.\n\nNext, we must choose $\\theta_j$ to satisfy the bounds $L_j \\le c_{j,i}^{\\text{lim}} \\le U_j$. We need to find the most restrictive scaling required.\n\n-   **Upper Bound Enforcement**: For any coefficient $c_{j,i}$, we need $c_{j,i}^{\\text{lim}} \\le U_j$.\n    $$\n    \\bar{u}_j + \\theta_j (c_{j,i} - \\bar{u}_j) \\le U_j\n    $$\n    If $c_{j,i}  \\bar{u}_j$, the term $(c_{j,i} - \\bar{u}_j)$ is positive. The inequality for $\\theta_j$ is $\\theta_j \\le \\frac{U_j - \\bar{u}_j}{c_{j,i} - \\bar{u}_j}$. To satisfy this for all such coefficients, we must choose $\\theta_j$ based on the maximum coefficient, $c_{j,\\max} = \\max_i c_{j,i}$.\n    Let $\\theta_j^+ = \\frac{U_j - \\bar{u}_j}{c_{j,\\max} - \\bar{u}_j}$ if $c_{j,\\max}  \\bar{u}_j$, and $\\theta_j^+ = \\infty$ (or a value $\\ge 1$) otherwise (as no limiting is needed for the upper bound).\n-   **Lower Bound Enforcement**: For any coefficient $c_{j,i}$, we need $c_{j,i}^{\\text{lim}} \\ge L_j$.\n    $$\n    \\bar{u}_j + \\theta_j (c_{j,i} - \\bar{u}_j) \\ge L_j\n    $$\n    If $c_{j,i}  \\bar{u}_j$, the term $(c_{j,i} - \\bar{u}_j)$ is negative. Dividing by it reverses the inequality: $\\theta_j \\le \\frac{L_j - \\bar{u}_j}{c_{j,i} - \\bar{u}_j} = \\frac{\\bar{u}_j - L_j}{\\bar{u}_j - c_{j,i}}$. To satisfy this for all such coefficients, we use the minimum coefficient, $c_{j,\\min} = \\min_i c_{j,i}$.\n    Let $\\theta_j^- = \\frac{\\bar{u}_j - L_j}{\\bar{u}_j - c_{j,\\min}}$ if $c_{j,\\min}  \\bar{u}_j$, and $\\theta_j^- = \\infty$ (or a value $\\ge 1$) otherwise.\n\nTo satisfy all constraints simultaneously, we must choose the smallest $\\theta_j$ that meets all requirements, while also ensuring $\\theta_j \\le 1$ (as we only want to reduce oscillations, not amplify them). Thus, the final scaling factor for cell $j$ is:\n$$\n\\theta_j = \\min(1, \\theta_j^+, \\theta_j^-)\n$$\nIf the original coefficients are already within the bounds $[L_j, U_j]$, then no limiting is needed. In this case, $c_{j,\\max} \\le U_j$ and $c_{j,\\min} \\ge L_j$. This implies $\\theta_j^+ \\ge 1$ and $\\theta_j^- \\ge 1$, so $\\theta_j=1$, correctly yielding $c_{j,i}^{\\text{lim}} = c_{j,i}$.\n\n### Metrics Calculation\nThe performance of the limiter is quantified using three metrics over the entire mesh:\n1.  **Maximum Coefficient Overshoot Before Limiting**: This measures the largest violation of the bounds by the original coefficients.\n    $$\n    \\text{overshoot}_{\\text{before}} = \\max_{j,i} \\left\\{ \\max(c_{j,i} - U_j, 0), \\max(L_j - c_{j,i}, 0) \\right\\} = \\max_{j,i} \\left\\{ \\max(c_{j,i} - U_j, L_j - c_{j,i}, 0) \\right\\}\n    $$\n2.  **Maximum Coefficient Overshoot After Limiting**: This is defined identically, but for the limited coefficients $c_{j,i}^{\\text{lim}}$. By construction, this value should be zero (or within machine precision).\n    $$\n    \\text{overshoot}_{\\text{after}} = \\max_{j,i} \\left\\{ \\max(c_{j,i}^{\\text{lim}} - U_j, L_j - c_{j,i}^{\\text{lim}}, 0) \\right\\}\n    $$\n3.  **Maximum Absolute Mean Error**: This verifies the mean-preservation property. It should also be zero (or within machine precision).\n    $$\n    \\text{mean\\_error} = \\max_j \\left| \\bar{u}_j^{\\text{lim}} - \\bar{u}_j \\right| = \\max_j \\left| \\frac{1}{p_j+1}\\sum_{i=0}^{p_j} c_{j,i}^{\\text{lim}} - \\frac{1}{p_j+1}\\sum_{i=0}^{p_j} c_{j,i} \\right|\n    $$\n\nThe implementation will apply this cell-wise limiting procedure and then compute these three global metrics for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the slope limiter problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        # Test Case A\n        {\n            \"p\": 3, \"N\": 6, \"coeffs\": np.array([\n                [0.0, 0.0, 0.0, 0.0],\n                [0.0, -0.05, 0.05, 0.0],\n                [-0.2, 0.1, 0.1, 0.0],\n                [1.3, 1.1, 0.7, 0.9],\n                [1.0, 1.0, 1.0, 1.0],\n                [1.0, 1.05, 0.95, 1.0],\n            ], dtype=float)\n        },\n        # Test Case B\n        {\n            \"p\": 5, \"N\": 6, \"coeffs\": np.array([\n                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n                [0.0, -0.1, 0.0, 0.05, -0.05, 0.1],\n                [-0.3, 0.05, 0.2, 0.1, -0.05, 0.0],\n                [1.2, 1.1, 0.6, 0.9, 1.0, 1.2],\n                [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n                [1.0, 0.95, 1.05, 1.0, 1.0, 1.0],\n            ], dtype=float)\n        },\n        # Test Case C\n        {\n            \"p\": 3, \"N\": 3, \"coeffs\": np.array([\n                [0.0, 0.0, 0.0, 0.0],\n                [0.4, 0.5, 0.6, 0.5],\n                [1.0, 1.0, 1.0, 1.0],\n            ], dtype=float)\n        },\n        # Test Case D\n        {\n            \"p\": 4, \"N\": 2, \"coeffs\": np.array([\n                [-0.1, 0.0, 0.2, -0.05, -0.05],\n                [1.1, 0.8, 1.2, 1.0, 0.9],\n            ], dtype=float)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        p = case[\"p\"]\n        N = case[\"N\"]\n        coeffs = case[\"coeffs\"]\n        p_plus_1 = p + 1\n\n        # Step 1: Compute cell means and neighbor-based bounds\n        original_means = np.sum(coeffs, axis=1) / p_plus_1\n        \n        L_bounds = np.zeros(N)\n        U_bounds = np.zeros(N)\n        for j in range(N):\n            mean_prev = original_means[j-1] if j  0 else original_means[j]\n            mean_next = original_means[j+1] if j  N - 1 else original_means[j]\n            L_bounds[j] = min(mean_prev, mean_next)\n            U_bounds[j] = max(mean_prev, mean_next)\n\n        # Step 2: Compute pre-limiter overshoot\n        overshoot_before = 0.0\n        for j in range(N):\n            # max(c-U, L-c, 0)\n            overshoots_cell = np.maximum(coeffs[j] - U_bounds[j], L_bounds[j] - coeffs[j])\n            max_overshoot_in_cell = np.max(np.maximum(overshoots_cell, 0))\n            overshoot_before = max(overshoot_before, max_overshoot_in_cell)\n\n        # Step 3: Apply cell-wise limiter\n        coeffs_lim = np.copy(coeffs)\n        for j in range(N):\n            c_j = coeffs[j]\n            mean_j = original_means[j]\n            L_j, U_j = L_bounds[j], U_bounds[j]\n\n            c_min, c_max = np.min(c_j), np.max(c_j)\n\n            # Only apply limiter if bounds are violated\n            if c_min  L_j or c_max  U_j:\n                theta_plus = float('inf')\n                if c_max  mean_j:\n                    theta_plus = (U_j - mean_j) / (c_max - mean_j)\n                \n                theta_minus = float('inf')\n                if c_min  mean_j:\n                    theta_minus = (mean_j - L_j) / (mean_j - c_min)\n                \n                theta = min(1.0, theta_plus, theta_minus)\n                \n                coeffs_lim[j] = mean_j + theta * (c_j - mean_j)\n\n        # Step 4: Compute post-limiter metrics\n        overshoot_after = 0.0\n        for j in range(N):\n            overshoots_cell = np.maximum(coeffs_lim[j] - U_bounds[j], L_bounds[j] - coeffs_lim[j])\n            max_overshoot_in_cell = np.max(np.maximum(overshoots_cell, 0))\n            overshoot_after = max(overshoot_after, max_overshoot_in_cell)\n\n        limited_means = np.sum(coeffs_lim, axis=1) / p_plus_1\n        mean_error = np.max(np.abs(limited_means - original_means))\n        \n        results.append([overshoot_before, overshoot_after, mean_error])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}