{
    "hands_on_practices": [
        {
            "introduction": "The power of Chebyshev polynomials in spectral methods stems from their role as eigenfunctions of a specific Sturm-Liouville problem. This exercise will guide you through a fundamental derivation, starting from the simple trigonometric definition of the Chebyshev polynomials of the second kind, $U_n(x)$, to uncover the second-order differential equation they satisfy . Mastering this connection is key to understanding why this basis is so effective for solving a wide range of differential equations.",
            "id": "3370040",
            "problem": "In high-order spectral and Discontinuous Galerkin (DG) methods on bounded intervals, orthogonal polynomial bases on $\\left[-1,1\\right]$ are a central tool. The Chebyshev polynomials of the second kind, denoted $U_{n}$, are commonly defined via the trigonometric mapping $x=\\cos\\theta$ with $\\theta$ measured in radians, by the identity $U_{n}(\\cos\\theta)=\\dfrac{\\sin\\!\\big((n+1)\\theta\\big)}{\\sin\\theta}$. Starting from this trigonometric definition and using only fundamental calculus identities (such as the chain rule) together with $x=\\cos\\theta$ on $(-1,1)$, derive the linear second-order ordinary differential equation with polynomial coefficients in $x$ that is satisfied by $U_{n}$ on $(-1,1)$. Express the resulting equation in the canonical form\n$$(1-x^{2})\\,y''-3x\\,y'+\\lambda_{n}\\,y=0,$$\nwhere $y(x)=U_{n}(x)$ and $\\lambda_{n}$ depends only on $n$. Your final task is to determine the exact analytic expression for $\\lambda_{n}$ in terms of $n$. Provide your final answer for $\\lambda_{n}$ only. No rounding is required.",
            "solution": "The problem requires the derivation of the second-order linear ordinary differential equation for the Chebyshev polynomials of the second kind, $U_n(x)$, and the identification of the parameter $\\lambda_n$. The derivation begins from the trigonometric definition of $U_n(x)$.\n\nLet $y(x) = U_n(x)$. The defining relation is given by the identity $U_n(\\cos\\theta) = \\frac{\\sin((n+1)\\theta)}{\\sin\\theta}$ for $x = \\cos\\theta$ on the interval $x \\in (-1, 1)$, which corresponds to $\\theta \\in (0, \\pi)$. Let $y(\\cos\\theta)$ represent $U_n(\\cos\\theta)$. We can write the identity as:\n$$y(\\cos\\theta) \\sin\\theta = \\sin((n+1)\\theta)$$\n\nTo find the differential equation satisfied by $y(x)$, we must compute its derivatives with respect to $x$. We can achieve this by repeatedly differentiating the identity with respect to $\\theta$ and using the chain rule to relate derivatives with respect to $\\theta$ to derivatives with respect to $x$.\n\nFirst, we differentiate the identity with respect to $\\theta$:\n$$\\frac{d}{d\\theta} [y(\\cos\\theta) \\sin\\theta] = \\frac{d}{d\\theta} [\\sin((n+1)\\theta)]$$\nUsing the product rule on the left-hand side, we get:\n$$\\left(\\frac{d}{d\\theta} y(\\cos\\theta)\\right) \\sin\\theta + y(\\cos\\theta) \\cos\\theta = (n+1)\\cos((n+1)\\theta)$$\nThe term $\\frac{d}{d\\theta} y(\\cos\\theta)$ is evaluated using the chain rule:\n$$\\frac{d}{d\\theta} y(\\cos\\theta) = \\frac{dy}{dx} \\frac{dx}{d\\theta} = y'(x) (-\\sin\\theta)$$\nSubstituting this back into the differentiated identity yields:\n$$(y'(x)(-\\sin\\theta)) \\sin\\theta + y(x) \\cos\\theta = (n+1)\\cos((n+1)\\theta)$$\nUsing $x = \\cos\\theta$ and $\\sin^2\\theta = 1 - \\cos^2\\theta = 1 - x^2$, the equation becomes:\n$$-y'(x) (1-x^2) + y(x) x = (n+1)\\cos((n+1)\\theta)$$\nThis is a first-order differential relation for $y(x)$, but it still contains the variable $\\theta$ on the right-hand side. To eliminate $\\theta$, we differentiate this equation with respect to $x$:\n$$\\frac{d}{dx}[xy - (1-x^2)y'] = \\frac{d}{dx}[(n+1)\\cos((n+1)\\theta)]$$\n\nWe evaluate the derivatives of the left-hand side (LHS) and right-hand side (RHS) separately.\nFor the LHS, using the product and chain rules:\n$$\\frac{d}{dx}(xy) = 1 \\cdot y + x \\cdot y' = y + xy'$$\n$$\\frac{d}{dx}[-(1-x^2)y'] = -[(-2x)y' + (1-x^2)y''] = 2xy' - (1-x^2)y''$$\nCombining these terms, the derivative of the LHS is:\n$$\\text{LHS}' = (y + xy') + (2xy' - (1-x^2)y'') = -(1-x^2)y'' + 3xy' + y$$\n\nFor the RHS, we again apply the chain rule, noting that $\\theta$ is a function of $x$:\n$$\\text{RHS}' = (n+1) \\frac{d}{dx}[\\cos((n+1)\\theta)] = (n+1) \\left(\\frac{d}{d\\theta}[\\cos((n+1)\\theta)]\\right) \\frac{d\\theta}{dx}$$\nThe derivatives are:\n$$\\frac{d}{d\\theta}[\\cos((n+1)\\theta)] = -(n+1)\\sin((n+1)\\theta)$$\n$$\\frac{d\\theta}{dx} = \\frac{1}{dx/d\\theta} = \\frac{1}{-\\sin\\theta}$$\nSubstituting these into the expression for the RHS derivative:\n$$\\text{RHS}' = (n+1) [-(n+1)\\sin((n+1)\\theta)] \\left(\\frac{1}{-\\sin\\theta}\\right) = (n+1)^2 \\frac{\\sin((n+1)\\theta)}{\\sin\\theta}$$\nFrom the original definition, we recognize that $\\frac{\\sin((n+1)\\theta)}{\\sin\\theta} = U_n(\\cos\\theta) = U_n(x) = y(x)$. Thus, the derivative of the RHS simplifies to:\n$$\\text{RHS}' = (n+1)^2 y$$\n\nEquating the derivatives of the LHS and RHS:\n$$-(1-x^2)y'' + 3xy' + y = (n+1)^2 y$$\nWe rearrange this equation to match the canonical form given in the problem statement, $(1-x^2)y''-3xy'+\\lambda_n y=0$.\n$$-(1-x^2)y'' + 3xy' + (1 - (n+1)^2)y = 0$$\nMultiplying by $-1$ to make the $y''$ term's coefficient positive:\n$$(1-x^2)y'' - 3xy' - (1 - (n+1)^2)y = 0$$\n$$(1-x^2)y'' - 3xy' + ((n+1)^2 - 1)y = 0$$\nThe coefficient of the $y$ term is $\\lambda_n$. Let's simplify it:\n$$\\lambda_n = (n+1)^2 - 1 = (n^2 + 2n + 1) - 1 = n^2 + 2n = n(n+2)$$\nThe resulting differential equation is:\n$$(1-x^2)y'' - 3xy' + n(n+2)y = 0$$\nBy comparing this to the target form $(1-x^2)y''-3xy'+\\lambda_n y=0$, we directly identify the expression for $\\lambda_n$.",
            "answer": "$$\\boxed{n(n+2)}$$"
        },
        {
            "introduction": "Having established the analytical properties of Chebyshev polynomials, we now turn to a crucial practical task: their numerical evaluation. A naive summation of a Chebyshev series can be prone to numerical instability, but this can be overcome by exploiting the three-term recurrence relation that the polynomials satisfy . This hands-on coding exercise challenges you to implement the Clenshaw algorithm, a fast and stable method for evaluating Chebyshev series, and to witness its superior stability compared to a direct approach.",
            "id": "3370007",
            "problem": "Given a bounded interval and a polynomial basis, a crucial task in spectral and Discontinuous Galerkin (DG) methods is numerically stable evaluation of truncated series at specified points. Consider the Chebyshev polynomials of the first kind, defined for $x \\in [-1,1]$ by $T_n(x) := \\cos(n \\arccos x)$, where angles are in radians. These polynomials satisfy the three-term recurrence $T_0(x) = 1$, $T_1(x) = x$, and $T_{n+1}(x) = 2 x T_n(x) - T_{n-1}(x)$ for all integers $n \\ge 1$, and are orthogonal on $[-1,1]$ with respect to the weight $w(x) = (1-x^2)^{-1/2}$. For a general physical interval $[a,b]$, the canonical affine mapping from $x \\in [a,b]$ to the Chebyshev domain is $z(x) = \\frac{2x-(a+b)}{b-a}$.\n\nYour task is to implement a numerically stable backward recurrence (Clenshaw algorithm) to evaluate the truncated Chebyshev series $S_N(x) = \\sum_{n=0}^N a_n T_n(x)$ at given points $x \\in [-1,1]$, and also under the affine mapping when $x \\in [a,b]$. You must additionally compute a baseline “naive” evaluation of the same series using the defining identity $T_n(x) = \\cos(n \\arccos x)$, and report the absolute error between the Clenshaw result and the naive result for each test case.\n\nStarting from the core definitions above, derive the necessary backward recurrence structure that makes the Clenshaw algorithm stable for Chebyshev series on $[-1,1]$, and incorporate this structure in your implementation. Use double-precision floating-point arithmetic.\n\nAngles must be treated in radians.\n\nTest Suite:\n- Case $1$ (general interior point): Let $N = 20$ and coefficients $a_n = \\frac{(-1)^n}{n+1}$ for $n = 0,1,\\dots,20$. Evaluate at $x = 0.3$.\n- Case $2$ (right endpoint): Same $N$ and $a_n$ as Case $1$, evaluate at $x = 1$.\n- Case $3$ (left endpoint): Same $N$ and $a_n$ as Case $1$, evaluate at $x = -1$.\n- Case $4$ (near-endpoint stress test): Same $N$ and $a_n$ as Case $1$, evaluate at $x = 1 - 10^{-12}$.\n- Case $5$ (mapped interval evaluation): Consider the physical interval $[a,b] = [2,5]$. Let $N = 7$ with coefficients $b_n$ satisfying $b_7 = 1$ and $b_n = 0$ for $n \\ne 7$. Evaluate the series at $x = 3.5$ by first mapping to $z(x) = \\frac{2x-(a+b)}{b-a}$ and then using the Clenshaw algorithm for $z(x) \\in [-1,1]$. Compare the Clenshaw result to the naive value $T_7(z(x)) = \\cos(7 \\arccos(z(x)))$.\n\nFor each case, compute the absolute error as a float $E = |S_N^{\\mathrm{Clenshaw}}(x) - S_N^{\\mathrm{naive}}(x)|$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the five errors as a comma-separated list enclosed in square brackets, in the order of the cases described above, for example, $[e_1,e_2,e_3,e_4,e_5]$ where each $e_i$ is a float representing the absolute error of Case $i$.",
            "solution": "The problem requires the implementation and comparison of two methods for evaluating a truncated Chebyshev series, $S_N(x) = \\sum_{n=0}^N a_n T_n(x)$. The first method is the Clenshaw algorithm, a numerically stable backward recurrence. The second is a \"naive\" evaluation based on the trigonometric definition $T_n(x) = \\cos(n \\arccos x)$. We must compute the absolute error between the results of these two methods for several test cases.\n\n### Principle-Based Design and Derivation\n\nThe evaluation of a sum of functions that satisfy a three-term recurrence relation can often be performed efficiently and stably using the Clenshaw algorithm. This algorithm is a generalization of Horner's method for polynomial evaluation and avoids the potential numerical instability of directly summing terms, which may be prone to subtractive cancellation and error accumulation.\n\n#### 1. Derivation of the Clenshaw Algorithm for Chebyshev Series\n\nThe Chebyshev polynomials of the first kind, $T_n(x)$, satisfy the three-term recurrence relation:\n$$ T_{n+1}(x) = 2x T_n(x) - T_{n-1}(x) \\quad \\text{for } n \\ge 1 $$\nThis holds with $T_0(x) = 1$ and $T_1(x) = x$.\n\nWe wish to evaluate the sum $S_N(x) = \\sum_{n=0}^N a_n T_n(x)$. We can rewrite the recurrence as $T_{n-1}(x) = 2x T_n(x) - T_{n+1}(x)$.\n\nThe Clenshaw algorithm defines an auxiliary sequence, let's call it $y_k$, using a backward recurrence. Let us separate the $n=0$ term from the sum and focus on the remaining part, $S'_N(x) = \\sum_{n=1}^N a_n T_n(x)$.\n\nWe define the sequence $y_k$ for $k = N, N-1, \\dots, 1$ as follows:\nInitialize with $y_{N+2} = 0$ and $y_{N+1} = 0$.\nThe recurrence is:\n$$ y_k = a_k + 2x y_{k+1} - y_{k+2} \\quad \\text{for } k = N, N-1, \\dots, 1 $$\nThis allows us to write $a_k = y_k - 2x y_{k+1} + y_{k+2}$. Substituting this into the sum $S'_N(x)$:\n$$ S'_N(x) = \\sum_{n=1}^N (y_n - 2x y_{n+1} + y_{n+2}) T_n(x) $$\nWe can split this into three sums:\n$$ S'_N(x) = \\sum_{n=1}^N y_n T_n(x) - 2x \\sum_{n=1}^N y_{n+1} T_n(x) + \\sum_{n=1}^N y_{n+2} T_n(x) $$\nBy re-indexing the second and third sums and collecting terms for each $y_k$:\n-   The coefficient of $y_k$ for $k \\in [3, N]$ is $T_k(x) - 2x T_{k-1}(x) + T_{k-2}(x)$. From the recurrence relation, this is exactly $0$.\n-   We need to examine the boundary terms for $k=1, 2$ and the terms involving $y_{N+1}, y_{N+2}$ which are defined to be zero.\n\nLet's expand the sums and collect terms for $y_k$:\n\\begin{align*}\nS'_N(x) &= (y_1 T_1 + y_2 T_2 + y_3 T_3 + \\dots) \\\\\n         &\\quad - 2x (y_2 T_1 + y_3 T_2 + y_4 T_3 + \\dots) \\\\\n         &\\quad + (y_3 T_1 + y_4 T_2 + y_5 T_3 + \\dots)\n\\end{align*}\nWait, the re-indexing was not done correctly. Let's be more formal.\n$ \\sum_{n=1}^N y_n T_n(x) = y_1 T_1 + y_2 T_2 + \\sum_{n=3}^N y_n T_n(x) $\n$ -2x\\sum_{n=1}^N y_{n+1} T_n(x) = -2x y_2 T_1 -2x \\sum_{n=2}^{N-1} y_{n+1} T_n(x) -2x y_{N+1}T_N(x) $. Change index $k=n+1$: $-2x \\sum_{k=3}^N y_k T_{k-1}(x)$.\n$ \\sum_{n=1}^N y_{n+2} T_n(x) = \\sum_{k=3}^{N+2} y_k T_{k-2}(x) = \\sum_{k=3}^N y_k T_{k-2}(x) $ since $y_{N+1}=y_{N+2}=0$.\n\nCombining all terms:\n$$ S'_N(x) = y_1 T_1(x) + y_2 T_2(x) - 2x y_2 T_1(x) + \\sum_{k=3}^N y_k (T_k(x) - 2x T_{k-1}(x) + T_{k-2}(x)) $$\nThe sum from $k=3$ to $N$ vanishes due to the recurrence relation. The remaining part is:\n$$ S'_N(x) = y_1 T_1(x) + y_2 (T_2(x) - 2x T_1(x)) $$\nUsing $T_1(x)=x$ and $T_2(x)=2x^2-1$, we get $T_2(x) - 2x T_1(x) = (2x^2-1) - 2x(x) = -1 = -T_0(x)$.\nSo, $S'_N(x) = y_1 T_1(x) - y_2 T_0(x) = x y_1 - y_2$.\n\nThe total sum is $S_N(x) = a_0 T_0(x) + S'_N(x)$. Since $T_0(x)=1$, we have:\n$$ S_N(x) = a_0 + x y_1 - y_2 $$\nThis is the final formula for the Clenshaw evaluation. The algorithm is:\n1.  Initialize $y_{N+1} = 0$, $y_{N+2} = 0$.\n2.  For $k = N, N-1, \\dots, 1$, compute $y_k = a_k + (2x) y_{k+1} - y_{k+2}$.\n3.  The sum is $S_N(x) = a_0 + x y_1 - y_2$.\n\n#### 2. Naive Evaluation Method\n\nThe naive method directly uses the definition $T_n(x) = \\cos(n \\arccos x)$. The sum is computed as:\n$$ S_N(x) = \\sum_{n=0}^N a_n \\cos(n \\arccos x) $$\nThe algorithm is:\n1.  Compute $\\theta = \\arccos(x)$.\n2.  Initialize a sum $S = 0$.\n3.  For $n = 0, 1, \\dots, N$, compute $T_n(x) = \\cos(n\\theta)$ and add $a_n T_n(x)$ to $S$.\n\nThis method can suffer from numerical inaccuracies. For $x$ close to $\\pm 1$, the function $\\arccos(x)$ is ill-conditioned, and small errors in $x$ can lead to larger errors in $\\theta$. Subsequent summation of terms, especially if they are of alternating sign and similar magnitude (as in Cases 1-4), can lead to loss of precision.\n\n### Implementation for Test Cases\n\nThe implementation will consist of two functions, `eval_clenshaw` and `eval_naive`, corresponding to the algorithms derived above. Both functions take the coefficient array and the evaluation point $x$ as input.\n\n-   For Cases 1-4, we use $N=20$ and coefficients $a_n = \\frac{(-1)^n}{n+1}$, evaluated at the given points $x \\in [-1,1]$.\n-   For Case 5, the physical interval is $[a,b] = [2,5]$. The evaluation point is $x_{\\text{phys}} = 3.5$. We first map this to the canonical interval $[-1,1]$ using the given affine map:\n    $$ z(x) = \\frac{2x-(a+b)}{b-a} \\implies z(3.5) = \\frac{2(3.5)-(2+5)}{5-2} = \\frac{7-7}{3} = 0 $$\n    The series has coefficients $b_n$ such that $b_7=1$ and all other $b_n=0$ for $n \\in [0,7]$. So we evaluate the sum for $N=7$ with these coefficients at the point $z=0$.\n\nThe absolute error is computed as $E = |S_N^{\\mathrm{Clenshaw}}(x) - S_N^{\\mathrm{naive}}(x)|$ for each case. We use double-precision floating-point numbers for all calculations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing and comparing Clenshaw's algorithm and a naive\n    evaluation for Chebyshev series, then reporting the absolute errors.\n    \"\"\"\n\n    def eval_clenshaw(coeffs: np.ndarray, x: float) -> float:\n        \"\"\"\n        Evaluates a Chebyshev series using Clenshaw's algorithm.\n        S_N(x) = sum_{n=0 to N} coeffs[n] * T_n(x)\n        \"\"\"\n        N = len(coeffs) - 1\n        if N < 0:\n            return 0.0\n        if N == 0:\n            return coeffs[0]\n\n        # In correspondence with the derivation: y_{N+1} and y_{N+2} are 0.\n        # d1 holds y_{k+1} and d2 holds y_{k+2} in the loop.\n        d1 = 0.0\n        d2 = 0.0\n        x2 = 2.0 * x\n        \n        # Backward recurrence for k = N, N-1, ..., 1\n        for k in range(N, 0, -1):\n            d_new = coeffs[k] + x2 * d1 - d2\n            d2 = d1\n            d1 = d_new\n        \n        # After the loop, d1 corresponds to y_1 and d2 to y_2.\n        # The sum is a_0 + x*y_1 - y_2\n        return coeffs[0] + x * d1 - d2\n\n    def eval_naive(coeffs: np.ndarray, x: float) -> float:\n        \"\"\"\n        Evaluates a Chebyshev series using the direct definition T_n(x) = cos(n*arccos(x)).\n        \"\"\"\n        N = len(coeffs) - 1\n        if N < 0:\n            return 0.0\n\n        # Clip x to avoid domain errors in arccos due to floating point inaccuracies\n        x_clipped = np.clip(x, -1.0, 1.0)\n        theta = np.arccos(x_clipped)\n        \n        s = 0.0\n        for n in range(N + 1):\n            T_n = np.cos(n * theta)\n            s += coeffs[n] * T_n\n        \n        return s\n\n    test_cases = [\n        # (N, coeffs_generator, x, mapping_params)\n        (20, lambda n: (-1)**n / (n + 1), 0.3, None),  # Case 1\n        (20, lambda n: (-1)**n / (n + 1), 1.0, None),  # Case 2\n        (20, lambda n: (-1)**n / (n + 1), -1.0, None), # Case 3\n        (20, lambda n: (-1)**n / (n + 1), 1.0 - 1e-12, None), # Case 4\n        (7, lambda n: 1.0 if n == 7 else 0.0, 3.5, (2.0, 5.0)), # Case 5\n    ]\n\n    results = []\n    for N, coeff_gen, x_val, mapping in test_cases:\n        coeffs = np.array([coeff_gen(n) for n in range(N + 1)], dtype=np.float64)\n        \n        x_eval = x_val\n        if mapping:\n            a, b = mapping\n            x_eval = (2.0 * x_val - (a + b)) / (b - a)\n\n        val_clenshaw = eval_clenshaw(coeffs, x_eval)\n        val_naive = eval_naive(coeffs, x_eval)\n        \n        error = np.abs(val_clenshaw - val_naive)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "We now apply our knowledge to construct a full spectral method for a boundary value problem. This practice involves using the Chebyshev basis to discretize a differential equation via the Tau method, resulting in a linear system for the unknown spectral coefficients . More importantly, it demonstrates how to leverage the intrinsic parity of the Chebyshev polynomials to decouple the system, significantly reducing computational cost and improving the numerical conditioning of the problem.",
            "id": "3370039",
            "problem": "Consider the interval $[-1,1]$ and the Chebyshev polynomials of the first kind $T_{n}(x)$, defined by $T_{n}(\\cos\\theta)=\\cos(n\\theta)$ for $\\theta\\in[0,\\pi]$. They satisfy the parity relation $T_{n}(-x)=(-1)^{n}T_{n}(x)$ and the weighted orthogonality relation\n$$\n\\int_{-1}^{1}\\frac{T_{m}(x)T_{n}(x)}{\\sqrt{1-x^{2}}}\\,\\mathrm{d}x = \\begin{cases}\n\\pi, & n=m=0, \\\\[4pt]\n\\frac{\\pi}{2}, & n=m\\ge 1, \\\\[4pt]\n0, & n\\ne m.\n\\end{cases}\n$$\nDefine the orthonormal Chebyshev basis $\\widehat{T}_{0}(x)=T_{0}(x)/\\sqrt{\\pi}$ and $\\widehat{T}_{n}(x)=T_{n}(x)/\\sqrt{\\pi/2}$ for $n\\ge 1$ with respect to the inner product $\\langle u,v\\rangle=\\int_{-1}^{1}u(x)v(x)\\,(1-x^{2})^{-1/2}\\,\\mathrm{d}x$. Consider the symmetric boundary value problem on $[-1,1]$ with Dirichlet boundary conditions $u(-1)=0$ and $u(1)=0$, and an even right-hand side so that the exact solution is even. Approximate $u$ in the $(N+1)$-dimensional trial space $\\mathcal{V}_{N}=\\operatorname{span}\\{\\widehat{T}_{0},\\widehat{T}_{1},\\ldots,\\widehat{T}_{N}\\}$ with $N=3$ by the Tau method: enforce the interior equation in a weighted $L^{2}$ sense using the identity operator as the model bilinear form $\\langle u,v\\rangle$ on the first $N-1$ test modes, and impose the two Dirichlet boundary conditions as Tau constraints in the last two equations. This produces a square linear system $A_{\\text{full}}\\in\\mathbb{R}^{4\\times 4}$ for the coefficients of $u$ in the basis $\\{\\widehat{T}_{n}\\}_{n=0}^{3}$.\n\nUsing only the fundamental definitions above, perform the following:\n\n1. Derive the explicit entries of $A_{\\text{full}}$ for $N=3$ by writing down the two interior equations corresponding to testing with $\\widehat{T}_{0}$ and $\\widehat{T}_{1}$ and the two boundary equations $u(1)=0$ and $u(-1)=0$ in terms of the coefficient vector of $u$ in the basis $\\{\\widehat{T}_{n}\\}_{n=0}^{3}$. Then show how parity decouples the system into even and odd subspaces.\n\n2. Exploit the even symmetry of the exact solution to reduce the approximation space to the even subspace $\\mathcal{V}_{N}^{\\text{even}}=\\operatorname{span}\\{\\widehat{T}_{0},\\widehat{T}_{2}\\}$ and the corresponding test space, replacing the two boundary equations by the single constraint $u(1)=0$ appropriate to even functions. Derive the reduced $2\\times 2$ system matrix $A_{\\text{even}}$.\n\n3. Compute the spectral two-norm condition numbers $\\kappa_{2}(A_{\\text{full}})$ and $\\kappa_{2}(A_{\\text{even}})$ directly from the singular values of $A_{\\text{full}}$ and $A_{\\text{even}}$, by forming $A_{\\text{full}}^{\\mathsf{T}}A_{\\text{full}}$ and $A_{\\text{even}}^{\\mathsf{T}}A_{\\text{even}}$, finding their eigenvalues in closed form, and taking square roots. Conclude with the exact analytic expression for the ratio\n$$\nR \\equiv \\frac{\\kappa_{2}(A_{\\text{full}})}{\\kappa_{2}(A_{\\text{even}})}.\n$$\n\nYour final answer must be the closed-form expression for $R$ as a function of $\\pi$. No numerical rounding is permitted, and no units are required. Clearly state any auxiliary algebraic quantities you introduce in your derivations within your solution, but the final answer must be a single analytic expression for $R$.",
            "solution": "The solution is presented in three parts as requested. The unknown vector of coefficients is $\\hat{\\mathbf{u}} = [\\hat{u}_0, \\hat{u}_1, \\hat{u}_2, \\hat{u}_3]^{\\mathsf{T}}$ for the full system and $\\hat{\\mathbf{u}}_{\\text{even}} = [\\hat{u}_0, \\hat{u}_2]^{\\mathsf{T}}$ for the reduced system.\n\n#### 1. Derivation of $A_{\\text{full}}$ and Parity Decoupling\n\nThe approximation is $u_3(x) = \\sum_{n=0}^{3} \\hat{u}_n \\widehat{T}_n(x)$. The $4 \\times 4$ linear system for the coefficients $\\hat{u}_n$ is constructed as follows:\n\nThe first two equations come from the interior problem, tested against $\\widehat{T}_0$ and $\\widehat{T}_1$:\n- For $k=0$: $\\langle u_3, \\widehat{T}_0 \\rangle = \\sum_{n=0}^{3} \\hat{u}_n \\langle \\widehat{T}_n, \\widehat{T}_0 \\rangle = \\hat{u}_0$. The equation is $\\hat{u}_0 = \\hat{f}_0$.\n- For $k=1$: $\\langle u_3, \\widehat{T}_1 \\rangle = \\sum_{n=0}^{3} \\hat{u}_n \\langle \\widehat{T}_n, \\widehat{T}_1 \\rangle = \\hat{u}_1$. The equation is $\\hat{u}_1 = \\hat{f}_1$.\nThese give the first two rows of $A_{\\text{full}}$ as $[1, 0, 0, 0]$ and $[0, 1, 0, 0]$.\n\nThe next two equations are the boundary constraints. We use the properties $T_n(1)=1$ for all $n \\ge 0$ and $T_n(-1)=(-1)^n$. The orthonormal basis functions at the boundaries are:\n$\\widehat{T}_0(1) = 1/\\sqrt{\\pi}$, $\\widehat{T}_n(1) = \\sqrt{2/\\pi}$ for $n \\ge 1$.\n$\\widehat{T}_0(-1) = 1/\\sqrt{\\pi}$, $\\widehat{T}_n(-1) = (-1)^n \\sqrt{2/\\pi}$ for $n \\ge 1$.\n\n- Boundary condition at $x=1$: $u_3(1) = \\sum_{n=0}^{3} \\hat{u}_n \\widehat{T}_n(1) = \\hat{u}_0 \\frac{1}{\\sqrt{\\pi}} + \\hat{u}_1 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} + \\hat{u}_2 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} + \\hat{u}_3 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} = 0$.\n- Boundary condition at $x=-1$: $u_3(-1) = \\sum_{n=0}^{3} \\hat{u}_n \\widehat{T}_n(-1) = \\hat{u}_0 \\frac{1}{\\sqrt{\\pi}} - \\hat{u}_1 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} + \\hat{u}_2 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} - \\hat{u}_3 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} = 0$.\n\nThe full system matrix $A_{\\text{full}}$ is:\n$$\nA_{\\text{full}} = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\frac{1}{\\sqrt{\\pi}} & \\frac{\\sqrt{2}}{\\sqrt{\\pi}} & \\frac{\\sqrt{2}}{\\sqrt{\\pi}} & \\frac{\\sqrt{2}}{\\sqrt{\\pi}} \\\\\n\\frac{1}{\\sqrt{\\pi}} & -\\frac{\\sqrt{2}}{\\sqrt{\\pi}} & \\frac{\\sqrt{2}}{\\sqrt{\\pi}} & -\\frac{\\sqrt{2}}{\\sqrt{\\pi}}\n\\end{pmatrix}.\n$$\n\nTo show parity decoupling, we consider a new set of boundary equations by summing and differencing the original two:\n1. $u_3(1) + u_3(-1) = 2 \\left( \\hat{u}_0 \\frac{1}{\\sqrt{\\pi}} + \\hat{u}_2 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} \\right) = 0$. This equation involves only coefficients of even basis functions ($\\widehat{T}_0, \\widehat{T}_2$).\n2. $u_3(1) - u_3(-1) = 2 \\left( \\hat{u}_1 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} + \\hat{u}_3 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} \\right) = 0$. This equation involves only coefficients of odd basis functions ($\\widehat{T}_1, \\widehat{T}_3$).\n\nThe full system decouples into an even-mode system for $(\\hat{u}_0, \\hat{u}_2)$ and an odd-mode system for $(\\hat{u}_1, \\hat{u}_3)$. Since the exact solution is even, we expect $\\hat{u}_1=0$ and $\\hat{u}_3=0$. Indeed, since the RHS $f(x)$ is even, its projection on the odd mode $\\widehat{T}_1$ is zero, so $\\hat{f}_1=0$, and the second equation of the system gives $\\hat{u}_1=0$. The second decoupled boundary equation then gives $\\hat{u}_3=0$.\n\n#### 2. Derivation of $A_{\\text{even}}$\n\nWe exploit the even symmetry by restricting the trial space to $\\mathcal{V}_{3}^{\\text{even}}=\\operatorname{span}\\{\\widehat{T}_{0},\\widehat{T}_{2}\\}$. The approximation is $u_{\\text{even}}(x) = \\hat{u}_0 \\widehat{T}_0(x) + \\hat{u}_2 \\widehat{T}_2(x)$. We need two equations for the two unknowns $\\hat{u}_0, \\hat{u}_2$.\nFor an even function, the two boundary conditions $u(1)=0$ and $u(-1)=0$ are not independent, as $u(-1)=u(1)$. We use one interior equation and one boundary constraint.\n- The interior equation is obtained by testing against the first even basis function, $\\widehat{T}_0$: $\\langle u_{\\text{even}}, \\widehat{T}_0 \\rangle = \\hat{u}_0$. This gives the row $[1, 0]$.\n- The boundary constraint is $u_{\\text{even}}(1) = \\hat{u}_0 \\widehat{T}_0(1) + \\hat{u}_2 \\widehat{T}_2(1) = \\hat{u}_0 \\frac{1}{\\sqrt{\\pi}} + \\hat{u}_2 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} = 0$. This gives the row $[\\frac{1}{\\sqrt{\\pi}}, \\frac{\\sqrt{2}}{\\sqrt{\\pi}}]$.\n\nThe reduced system matrix $A_{\\text{even}}$ is:\n$$\nA_{\\text{even}} = \\begin{pmatrix}\n1 & 0 \\\\\n\\frac{1}{\\sqrt{\\pi}} & \\frac{\\sqrt{2}}{\\sqrt{\\pi}}\n\\end{pmatrix}.\n$$\n\n#### 3. Condition Numbers and Ratio $R$\n\nThe spectral condition number is $\\kappa_2(A) = \\sigma_{\\max}(A)/\\sigma_{\\min}(A)$, where the singular values $\\sigma_i(A)$ are the square roots of the eigenvalues of $A^{\\mathsf{T}}A$.\n\n**Condition number of $A_{\\text{full}}$**:\nWe compute $A_{\\text{full}}^{\\mathsf{T}}A_{\\text{full}}$:\n$$\nA_{\\text{full}}^{\\mathsf{T}}A_{\\text{full}} = \\frac{1}{\\pi} \\begin{pmatrix}\n\\pi+2 & 0 & 2\\sqrt{2} & 0 \\\\\n0 & \\pi+4 & 0 & 4 \\\\\n2\\sqrt{2} & 0 & 4 & 0 \\\\\n0 & 4 & 0 & 4\n\\end{pmatrix}.\n$$\nThis matrix is block-diagonal under permutation of indices $(1,2)$. The eigenvalues are those of the two $2 \\times 2$ blocks.\nEven block: $M_e = \\frac{1}{\\pi}\\begin{pmatrix} \\pi+2 & 2\\sqrt{2} \\\\ 2\\sqrt{2} & 4 \\end{pmatrix}$. Its characteristic equation for eigenvalues $\\lambda$ is $\\lambda^2 - \\frac{\\pi+6}{\\pi}\\lambda + \\frac{4}{\\pi} = 0$.\nThe eigenvalues are $\\lambda_{e, \\pm} = \\frac{1}{2\\pi} (\\pi+6 \\pm \\sqrt{\\pi^2 - 4\\pi + 36})$.\nOdd block: $M_o = \\frac{1}{\\pi}\\begin{pmatrix} \\pi+4 & 4 \\\\ 4 & 4 \\end{pmatrix}$. Its characteristic equation is $\\lambda^2 - \\frac{\\pi+8}{\\pi}\\lambda + \\frac{4}{\\pi} = 0$.\nThe eigenvalues are $\\lambda_{o, \\pm} = \\frac{1}{2\\pi} (\\pi+8 \\pm \\sqrt{\\pi^2 + 64})$.\n\nTo find $\\sigma_{\\max}$ and $\\sigma_{\\min}$, we must find the largest and smallest of these four eigenvalues. By inspection and analysis, for $\\pi > 0$, we have $\\pi+8+\\sqrt{\\pi^2+64} > \\pi+6+\\sqrt{\\pi^2-4\\pi+36}$ and $\\pi+8-\\sqrt{\\pi^2+64} < \\pi+6-\\sqrt{\\pi^2-4\\pi+36}$, the latter being equivalent to $16\\pi > 0$. Thus, the maximum and minimum eigenvalues both come from the odd block.\n$\\sigma_{\\max}^2(A_{\\text{full}}) = \\lambda_{o,+} = \\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{2\\pi}$.\n$\\sigma_{\\min}^2(A_{\\text{full}}) = \\lambda_{o,-} = \\frac{\\pi+8 - \\sqrt{\\pi^2+64}}{2\\pi}$.\n\nThe squared condition number is the ratio of these eigenvalues:\n$$\n\\kappa_2(A_{\\text{full}})^2 = \\frac{\\lambda_{o,+}}{\\lambda_{o,-}} = \\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{\\pi+8 - \\sqrt{\\pi^2+64}} = \\frac{(\\pi+8 + \\sqrt{\\pi^2+64})^2}{(\\pi+8)^2 - (\\pi^2+64)} = \\frac{(\\pi+8 + \\sqrt{\\pi^2+64})^2}{16\\pi}.\n$$\nTaking the square root gives:\n$$\n\\kappa_2(A_{\\text{full}}) = \\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{4\\sqrt{\\pi}}.\n$$\n\n**Condition number of $A_{\\text{even}}$**:\nWe compute $A_{\\text{even}}^{\\mathsf{T}}A_{\\text{even}}$:\n$$\nA_{\\text{even}}^{\\mathsf{T}}A_{\\text{even}} = \\begin{pmatrix} 1 & \\frac{1}{\\sqrt{\\pi}} \\\\ 0 & \\frac{\\sqrt{2}}{\\sqrt{\\pi}} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ \\frac{1}{\\sqrt{\\pi}} & \\frac{\\sqrt{2}}{\\sqrt{\\pi}} \\end{pmatrix} = \\begin{pmatrix} 1+\\frac{1}{\\pi} & \\frac{\\sqrt{2}}{\\pi} \\\\ \\frac{\\sqrt{2}}{\\pi} & \\frac{2}{\\pi} \\end{pmatrix} = \\frac{1}{\\pi} \\begin{pmatrix} \\pi+1 & \\sqrt{2} \\\\ \\sqrt{2} & 2 \\end{pmatrix}.\n$$\nThe eigenvalues $\\lambda$ of this matrix satisfy $\\lambda^2 - \\frac{\\pi+3}{\\pi}\\lambda + \\frac{2}{\\pi}=0$.\nThe eigenvalues are $\\lambda_{\\pm} = \\frac{1}{2\\pi} (\\pi+3 \\pm \\sqrt{(\\pi+3)^2 - 8\\pi}) = \\frac{1}{2\\pi} (\\pi+3 \\pm \\sqrt{\\pi^2 - 2\\pi + 9})$.\nThese are $\\sigma_{\\max}^2(A_{\\text{even}})$ and $\\sigma_{\\min}^2(A_{\\text{even}})$. The squared condition number is:\n$$\n\\kappa_2(A_{\\text{even}})^2 = \\frac{\\lambda_{+}}{\\lambda_{-}} = \\frac{\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9}}{\\pi+3 - \\sqrt{\\pi^2 - 2\\pi + 9}} = \\frac{(\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9})^2}{(\\pi+3)^2 - (\\pi^2 - 2\\pi + 9)} = \\frac{(\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9})^2}{8\\pi}.\n$$\nTaking the square root gives:\n$$\n\\kappa_2(A_{\\text{even}}) = \\frac{\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9}}{2\\sqrt{2\\pi}}.\n$$\n\n**Ratio $R$**:\nFinally, we compute the ratio $R = \\kappa_{2}(A_{\\text{full}}) / \\kappa_{2}(A_{\\text{even}})$:\n$$\nR = \\frac{\\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{4\\sqrt{\\pi}}}{\\frac{\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9}}{2\\sqrt{2\\pi}}} = \\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{4\\sqrt{\\pi}} \\cdot \\frac{2\\sqrt{2}\\sqrt{\\pi}}{\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9}}.\n$$\nSimplifying the expression yields:\n$$\nR = \\frac{2\\sqrt{2}}{4} \\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9}} = \\frac{\\sqrt{2}}{2} \\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9}}.\n$$",
            "answer": "$$ \\boxed{ \\frac{\\sqrt{2}}{2} \\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9}} } $$"
        }
    ]
}