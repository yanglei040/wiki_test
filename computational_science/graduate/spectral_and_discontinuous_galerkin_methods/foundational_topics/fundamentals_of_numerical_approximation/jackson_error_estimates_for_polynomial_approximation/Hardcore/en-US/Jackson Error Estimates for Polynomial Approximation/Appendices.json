{
    "hands_on_practices": [
        {
            "introduction": "The relationship between a function's smoothness and the convergence rate of its polynomial approximation is a cornerstone of approximation theory. This practice explores this principle using the canonical example of $f(x)=|x|$, a function that is continuous but not differentiable at the origin. By explicitly calculating the asymptotic decay of its Chebyshev series coefficients, you will directly observe how this interior singularity limits the approximation to an algebraic rate of convergence and investigate how this rate manifests differently in the weighted $L^2$ and uniform $L^\\infty$ norms .",
            "id": "3393562",
            "problem": "Consider the polynomial approximation of the function $f(x)=|x|$ on the interval $[-1,1]$ within the setting of spectral and Discontinuous Galerkin (DG) methods. Let $\\{T_{k}(x)\\}_{k=0}^{\\infty}$ denote the Chebyshev polynomials of the first kind, which form an orthogonal system on $[-1,1]$ with respect to the weight $w(x)=(1-x^{2})^{-1/2}$, and satisfy the orthogonality relation $\\int_{-1}^{1} T_{j}(x)\\,T_{k}(x)\\,w(x)\\,dx=0$ for $j\\neq k$. The Chebyshev polynomial expansion of $f$ is $f(x)=\\frac{a_{0}}{2}+\\sum_{k=1}^{\\infty} a_{k}\\,T_{k}(x)$, where the Chebyshev coefficients $a_{k}$ are defined by the weighted projection formula consistent with the orthogonality of $\\{T_{k}\\}$.\n\nStarting from the orthogonality of Chebyshev polynomials, the change of variables $x=\\cos\\theta$, and fundamental Fourier-cosine integral identities, derive the large-$k$ asymptotics of the Chebyshev coefficients $a_{k}$ for $f(x)=|x|$, and use these asymptotics to:\n- justify the convergence rate of the truncated Chebyshev series in the weighted $L^{2}$ norm associated with $w(x)$;\n- explain, using the Nikolskii inequality (a standard result bounding the uniform norm of polynomials by a multiple of their weighted $L^{2}$ norm), why the uniform norm ($L^{\\infty}$) convergence is slower than the weighted $L^{2}$ convergence for this function, and connect this with the Jackson error estimate for Lipschitz functions.\n\nYour final answer must be the leading-order asymptotic expression for $a_{k}$ as a single closed-form analytic expression in $k$. No numerical rounding is required, and no units are involved.",
            "solution": "The problem is valid as it is scientifically grounded in the established theory of polynomial approximation, well-posed, objective, and self-contained.\n\nThe problem asks for an analysis of the Chebyshev polynomial approximation of the function $f(x)=|x|$ on the interval $[-1,1]$. This involves deriving the asymptotic form of the Chebyshev coefficients, and using them to analyze the convergence rates in the weighted $L^2$ and uniform norms.\n\nFirst, we derive the Chebyshev coefficients $a_k$ for $f(x)=|x|$. The function is expanded as $f(x) = \\frac{a_0}{2} + \\sum_{k=1}^{\\infty} a_k T_k(x)$. The coefficients are given by the projection formula, which, using the orthogonality of Chebyshev polynomials $\\{T_k(x)\\}$ with weight $w(x)=(1-x^2)^{-1/2}$, is:\n$$a_0 = \\frac{2}{\\pi} \\int_{-1}^{1} f(x) w(x) \\, dx$$\n$$a_k = \\frac{2}{\\pi} \\int_{-1}^{1} f(x) T_k(x) w(x) \\, dx \\quad \\text{for } k \\ge 1$$\n\nWe use the change of variables $x=\\cos\\theta$. For $x \\in [-1,1]$, $\\theta$ goes from $\\pi$ to $0$. So, $dx=-\\sin\\theta\\,d\\theta$. The weight function becomes $w(x)=(1-\\cos^2\\theta)^{-1/2} = 1/\\sin\\theta$ for $\\theta \\in (0,\\pi)$. The Chebyshev polynomials become $T_k(x) = T_k(\\cos\\theta) = \\cos(k\\theta)$. The function to be approximated becomes $g(\\theta) = f(\\cos\\theta) = |\\cos\\theta|$.\n\nThe integral for $a_k$ transforms to:\n$$a_k = \\frac{2}{\\pi} \\int_{\\pi}^{0} |\\cos\\theta| \\cos(k\\theta) \\frac{1}{\\sin\\theta} (-\\sin\\theta\\,d\\theta) = \\frac{2}{\\pi} \\int_{0}^{\\pi} |\\cos\\theta| \\cos(k\\theta) \\,d\\theta$$\n\nThe function $f(x)=|x|$ is an even function on $[-1,1]$. The Chebyshev polynomial $T_k(x)$ is an even function for even $k$ and an odd function for odd $k$. The product $f(x)T_k(x)$ is therefore an odd function for odd $k$. The integral of an odd function over the symmetric interval $[-1,1]$ is zero. Thus, $a_k=0$ for all odd $k$.\n\nWe only need to calculate $a_k$ for even $k$. Let $k=2m$ for an integer $m \\ge 0$. The integrand $|\\cos\\theta|\\cos(2m\\theta)$ is an even function about $\\theta=\\pi/2$. We can simplify the integral:\n$$a_{2m} = \\frac{2}{\\pi} \\int_{0}^{\\pi} |\\cos\\theta| \\cos(2m\\theta) \\,d\\theta = \\frac{4}{\\pi} \\int_{0}^{\\pi/2} |\\cos\\theta| \\cos(2m\\theta) \\,d\\theta$$\nOn the interval $[0, \\pi/2]$, $\\cos\\theta \\ge 0$, so $|\\cos\\theta|=\\cos\\theta$.\n$$a_{2m} = \\frac{4}{\\pi} \\int_{0}^{\\pi/2} \\cos\\theta \\cos(2m\\theta) \\,d\\theta$$\nUsing the product-to-sum identity $\\cos A \\cos B = \\frac{1}{2}[\\cos(A+B) + \\cos(A-B)]$:\n$$a_{2m} = \\frac{4}{\\pi} \\int_{0}^{\\pi/2} \\frac{1}{2}[\\cos((2m+1)\\theta) + \\cos((2m-1)\\theta)] \\,d\\theta$$\nFor $m=0$, we calculate $a_0$ separately:\n$$a_0 = \\frac{4}{\\pi} \\int_{0}^{\\pi/2} \\cos\\theta \\,d\\theta = \\frac{4}{\\pi} [\\sin\\theta]_0^{\\pi/2} = \\frac{4}{\\pi}$$\nFor $m \\ge 1$ (so $k \\ge 2$):\n$$a_{2m} = \\frac{2}{\\pi} \\left[ \\frac{\\sin((2m+1)\\theta)}{2m+1} + \\frac{\\sin((2m-1)\\theta)}{2m-1} \\right]_0^{\\pi/2}$$\n$$a_{2m} = \\frac{2}{\\pi} \\left( \\frac{\\sin((2m+1)\\pi/2)}{2m+1} + \\frac{\\sin((2m-1)\\pi/2)}{2m-1} \\right)$$\nWe use $\\sin((2m+1)\\pi/2) = \\sin(m\\pi+\\pi/2) = (-1)^m$ and $\\sin((2m-1)\\pi/2) = \\sin(m\\pi-\\pi/2) = -(-1)^m$.\n$$a_{2m} = \\frac{2}{\\pi} \\left( \\frac{(-1)^m}{2m+1} - \\frac{(-1)^m}{2m-1} \\right) = \\frac{2(-1)^m}{\\pi} \\left( \\frac{(2m-1) - (2m+1)}{(2m+1)(2m-1)} \\right)$$\n$$a_{2m} = \\frac{2(-1)^m}{\\pi} \\left( \\frac{-2}{4m^2-1} \\right) = \\frac{4(-1)^{m+1}}{\\pi(4m^2-1)}$$\nSubstituting $k=2m$, we get the exact expression for even $k \\ge 2$:\n$$a_k = \\frac{4(-1)^{k/2+1}}{\\pi(k^2-1)}$$\nFor large $k$, we have $k^2-1 \\sim k^2$. The leading-order asymptotic expression for even $k$ is therefore:\n$$a_k \\sim \\frac{4(-1)^{k/2+1}}{\\pi k^2}$$\nTo write a single expression valid for all large $k$, we note that for even $k$, $\\cos(k\\pi/2) = (-1)^{k/2}$, and for odd $k$, $\\cos(k\\pi/2)=0$. Also, $(-1)^{k/2+1} = -\\cos(k\\pi/2)$. Since $a_k=0$ for odd $k$, we can combine these results into a single asymptotic formula:\n$$a_k \\sim -\\frac{4\\cos(k\\pi/2)}{\\pi k^2}$$\nThis expression is the required leading-order asymptotic form for $a_k$.\n\nNow we analyze the convergence rates. Let $P_N f = \\frac{a_0}{2} + \\sum_{k=1}^N a_k T_k(x)$ be the truncated Chebyshev series (the degree-$N$ projection).\n\nThe error in the weighted $L^2$ norm, denoted $\\| \\cdot \\|_w$, is given by Parseval's identity:\n$$\\|f - P_N f\\|_w^2 = \\int_{-1}^{1} (f(x) - P_N f(x))^2 w(x) \\, dx = \\sum_{k=N+1}^{\\infty} a_k^2 \\int_{-1}^{1} T_k(x)^2 w(x) \\, dx$$\nThe normalization integrals are $\\int_{-1}^1 T_k^2 w dx = \\pi/2$ for $k \\ge 1$. Since $a_k=0$ for odd $k$, the sum contains only even indices. For large $N$, we assume $N$ is even, $N=2M$. The first non-zero term in the sum is for $k=N+2 = 2M+2$.\n$$\\|f - P_N f\\|_w^2 = \\frac{\\pi}{2} \\sum_{k=N+1, k \\text{ even}}^{\\infty} a_k^2 = \\frac{\\pi}{2} \\sum_{m=M+1}^{\\infty} a_{2m}^2$$\nUsing the asymptotic form $a_{2m} \\sim \\frac{C}{m^2}$ (where $C = (-1)^{m+1}/\\pi$ is irrelevant for the rate), we get:\n$$\\|f - P_N f\\|_w^2 \\sim \\frac{\\pi}{2} \\sum_{m=M+1}^{\\infty} \\left(\\frac{4(-1)^{m+1}}{\\pi(4m^2-1)}\\right)^2 \\approx \\frac{8}{\\pi} \\sum_{m=M+1}^{\\infty} \\frac{1}{(4m^2)^2} = \\frac{1}{2\\pi} \\sum_{m=M+1}^{\\infty} \\frac{1}{m^4}$$\nThis sum can be bounded by an integral: $\\sum_{m=M+1}^{\\infty} m^{-4} \\approx \\int_{M+1}^{\\infty} x^{-4} dx = \\frac{1}{3(M+1)^3} = O(M^{-3}) = O(N^{-3})$.\nThus, $\\|f - P_N f\\|_w^2 = O(N^{-3})$, which implies a weighted $L^2$ convergence rate of:\n$$\\|f - P_N f\\|_w = O(N^{-3/2})$$\n\nNext, we explain the slower convergence in the uniform norm ($L^\\infty$).\nThe Nikolskii inequality for a polynomial $p_N$ of degree $N$ relates its uniform and weighted $L^2$ norms: $\\|p_N\\|_{L^\\infty} \\le C N \\|p_N\\|_{L_w^2}$. The key feature is the factor of $N$, which implies that high-degree polynomials can exhibit large peaks (high $L^\\infty$ norm) even if their weighted average size ($L_w^2$ norm) is small.\n\nThe error function $E_N(x) = f(x) - P_N f(x) = \\sum_{k=N+1}^{\\infty} a_k T_k(x)$ is not a polynomial, but it is composed of high-frequency polynomials. The principle of the Nikolskii inequality applies. The pointwise error tends to be largest near the singularity of the function, which for $f(x)=|x|$ occurs at $x=0$. At this point, the high-frequency Chebyshev polynomials $T_k(x)$ for even $k$ all contribute with the same sign pattern, leading to constructive interference and a slower decay of the error peak compared to the average error.\nOur calculation showed that $\\|f-P_N f\\|_w = O(N^{-3/2})$. If the $L^\\infty$ norm simply followed the $L_w^2$ norm, we would expect a similar rate. However, the Nikolskii inequality warns that converting an $L_w^2$ bound to an $L^\\infty$ bound for the high-degree polynomial components of the error introduces a penalty factor related to the degree. This penalty degrades the convergence rate. A detailed analysis shows the error at $x=0$ to be $\\sum_{k=N+1, k \\text{ even}}^\\infty |a_k T_k(0)| \\sim \\sum_{m \\sim N/2}^\\infty m^{-2} \\sim \\int_{N/2}^\\infty x^{-2}dx = O(N^{-1})$. Therefore, $\\|f - P_N f\\|_{L^\\infty} = O(N^{-1})$.\nThis rate of $O(N^{-1})$ is clearly slower than the weighted $L^2$ rate of $O(N^{-3/2})$.\n\nFinally, we connect this result to the Jackson error estimate. The function $f(x)=|x|$ is Lipschitz continuous on $[-1,1]$ (it belongs to the class $C^{0,1}$). Jackson's theorem for such functions states that the error of the *best* uniform approximation by a polynomial of degree $N$, denoted $E_N(f)$, satisfies $E_N(f) = \\inf_{p \\in \\mathcal{P}_N} \\|f-p\\|_{L^\\infty} = O(N^{-1})$. Our finding that the Chebyshev projection error converges as $\\|f-P_Nf\\|_{L^\\infty}=O(N^{-1})$ demonstrates that for this specific function, the projection method achieves the optimal rate of convergence predicted by Jackson's theorem. The slower $L^\\infty$ convergence is not an artifact of the method but is the best possible rate for any polynomial approximation due to the limited smoothness of $f(x)=|x|$.",
            "answer": "$$\\boxed{-\\frac{4\\cos(\\frac{k\\pi}{2})}{\\pi k^{2}}}$$"
        },
        {
            "introduction": "While interior singularities are instructive, many applications involve functions with singularities at the boundaries of the domain. This raises a critical question: does the rapid decay of polynomial approximation error imply that the function's derivatives are well-behaved in the classical sense? This exercise guides you through an analysis that reveals a surprising answer, demonstrating that for functions with endpoint singularities, the natural measure of smoothness is not a classical Sobolev space but a *weighted* one . This insight is fundamental to understanding the structure of Jackson-type theorems on an interval.",
            "id": "3393508",
            "problem": "Let $\\Omega=[-1,1]$ and let $\\|\\cdot\\|_{2}$ denote the $L^{2}(\\Omega)$ norm. In spectral and Discontinuous Galerkin (DG) methods based on algebraic polynomials, the behavior of approximation near endpoints is governed by endpoint singularities and the endpoint weight $\\varphi(x)=\\sqrt{1-x^{2}}$. Consider best polynomial approximation in $L^{2}(\\Omega)$ of degree at most $n$, whose error is denoted by $E_{n}(f)_{2}=\\inf_{p\\in\\mathbb{P}_{n}}\\|f-p\\|_{2}$. A central mechanism behind Jackson-type direct and inverse estimates uses the $r$-th order Ditzian–Totik weighted modulus of smoothness with step scaled by $\\varphi(x)$, and the corresponding weighted Sobolev-type quantity $\\varphi^{r} f^{(r)}\\in L^{p}(\\Omega)$. Your task is to exhibit, analyze, and justify an example showing why the endpoint weight is necessary in inverse estimates.\n\nWork with $r=2$ and $p=2$. Consider the family of functions $f_{\\beta}(x)=(1-x)^{\\beta}$ on $\\Omega$ with parameter $\\beta0$. Using only the following ingredients as foundational starting points:\n- The standard calculus rule for differentiating power functions, which implies $f_{\\beta}^{(2)}(x)=\\beta(\\beta-1)(1-x)^{\\beta-2}$ for non-integer $\\beta1$ and gives the same local asymptotic order near $x=1$ for general $\\beta0$,\n- The local equivalence $\\varphi(x)\\sim \\sqrt{2(1-x)}$ as $x\\to 1^{-}$,\n- The integrability criterion near an endpoint: for $\\alpha\\in\\mathbb{R}$, $\\int_{0}^{\\varepsilon} t^{\\alpha}\\,\\mathrm{d}t$ is finite if and only if $\\alpha-1$,\n\nperform the following steps:\n1. Determine the range of $\\beta$ for which the weighted second derivative $\\varphi^{2} f_{\\beta}''\\in L^{2}(\\Omega)$ holds, by reducing the question to checking the integrability of a power of $(1-x)$ near $x=1$.\n2. Determine the range of $\\beta$ for which the unweighted second derivative $f_{\\beta}''\\in L^{2}(\\Omega)$ holds.\n3. Using your answers in parts 1 and 2, specify a non-integer $\\beta$ for which $\\varphi^{2} f_{\\beta}''\\in L^{2}(\\Omega)$ but $f_{\\beta}''\\notin L^{2}(\\Omega)$. Explain, in the context of inverse theorems for polynomial approximation, why this demonstrates the necessity of the endpoint weight $\\varphi$ when quantifying smoothness from decay of $E_{n}(f)_{2}$.\n4. Fix your concrete example to $\\beta=\\frac{6}{5}$. Define the weighted integrability exponent $e_{w}$ by the identity\n$$\n\\left|\\varphi^{2}(x)\\,f_{\\beta}''(x)\\right|^{2}\\ \\asymp\\ (1-x)^{e_{w}}\\quad\\text{as }x\\to 1^{-},\n$$\nwhere $\\asymp$ denotes equivalence up to positive constants independent of $x$. Compute the exact value of $e_{w}$ for $\\beta=\\frac{6}{5}$.\n\nYour final answer must be the exact value of $e_{w}$, given as a single simplified expression with no units. No rounding is required.",
            "solution": "The problem requires an analysis of the function family $f_{\\beta}(x)=(1-x)^{\\beta}$ on the interval $\\Omega=[-1,1]$ to demonstrate the necessity of the endpoint weight $\\varphi(x)=\\sqrt{1-x^2}$ in inverse theorems of polynomial approximation. We will proceed by analyzing the integrability of the second derivative of $f_{\\beta}$, both with and without the weight, following the prescribed steps. The analysis will focus on the behavior near the endpoint $x=1$, as this is the source of the singularity for $f_{\\beta}$ and its derivatives.\n\nThe first step is to determine the range of the parameter $\\beta0$ for which the weighted second derivative $\\varphi^{2} f_{\\beta}''$ is in the space $L^{2}(\\Omega)$. This condition is met if the integral $\\int_{-1}^{1} |\\varphi^{2}(x) f_{\\beta}''(x)|^{2} \\,\\mathrm{d}x$ is finite. The function $f_{\\beta}(x) = (1-x)^{\\beta}$, along with its derivatives, is smooth on any closed subinterval of $[-1,1)$ that excludes the endpoint $x=1$. Therefore, the convergence of the integral depends solely on the local behavior of the integrand near $x=1$.\n\nWe use the asymptotic information provided. As $x\\to 1^{-}$, the second derivative behaves as $f_{\\beta}''(x) \\asymp (1-x)^{\\beta-2}$, where the symbol $\\asymp$ denotes equivalence up to positive constants. The sign of the constant of proportionality, $\\beta(\\beta-1)$, is irrelevant for integrability since the term is squared. The endpoint weight has the local behavior $\\varphi(x) \\sim \\sqrt{2(1-x)}$, which implies $\\varphi^{2}(x) \\asymp 1-x$ as $x\\to 1^{-}$. Combining these, the integrand is locally equivalent to:\n$$\n|\\varphi^{2}(x) f_{\\beta}''(x)|^{2} \\asymp |(1-x) \\cdot (1-x)^{\\beta-2}|^{2} = |(1-x)^{\\beta-1}|^{2} = (1-x)^{2(\\beta-1)}\n$$\nTo assess integrability, we analyze the integral $\\int_{1-\\varepsilon}^{1} (1-x)^{2(\\beta-1)} \\,\\mathrm{d}x$ for a small $\\varepsilon  0$. By substituting $t = 1-x$ (so $\\mathrm{d}t = -\\mathrm{d}x$), the integral transforms to $\\int_{0}^{\\varepsilon} t^{2(\\beta-1)} \\,\\mathrm{d}t$. Based on the provided criterion, this integral converges if and only if the exponent is greater than $-1$.\n$$\n2(\\beta-1)  -1\n$$\n$$\n2\\beta - 2  -1 \\implies 2\\beta  1 \\implies \\beta  \\frac{1}{2}\n$$\nTherefore, the condition $\\varphi^{2} f_{\\beta}''\\in L^{2}(\\Omega)$ holds if and only if $\\beta  1/2$.\n\nThe second step is to determine the range of $\\beta0$ for which the unweighted second derivative $f_{\\beta}''$ is in $L^{2}(\\Omega)$. This requires the integral $\\int_{-1}^{1} |f_{\\beta}''(x)|^{2} \\,\\mathrm{d}x$ to be finite. Again, the analysis reduces to the behavior near $x=1$. The integrand is locally equivalent to:\n$$\n|f_{\\beta}''(x)|^{2} \\asymp |(1-x)^{\\beta-2}|^{2} = (1-x)^{2(\\beta-2)}\n$$\nThe corresponding integral near $x=1$ is $\\int_{1-\\varepsilon}^{1} (1-x)^{2(\\beta-2)} \\,\\mathrm{d}x$, which is equivalent to $\\int_{0}^{\\varepsilon} t^{2(\\beta-2)} \\,\\mathrm{d}t$ under the substitution $t=1-x$. Applying the integrability criterion:\n$$\n2(\\beta-2)  -1\n$$\n$$\n2\\beta - 4  -1 \\implies 2\\beta  3 \\implies \\beta  \\frac{3}{2}\n$$\nThus, the condition $f_{\\beta}''\\in L^{2}(\\Omega)$ holds if and only if $\\beta  3/2$.\n\nThe third step is to use these results to exhibit an example demonstrating the necessity of the weight $\\varphi$. We need to find a non-integer $\\beta$ such that $\\varphi^{2} f_{\\beta}'' \\in L^{2}(\\Omega)$ but $f_{\\beta}'' \\notin L^{2}(\\Omega)$. From the previous steps, the first condition requires $\\beta  1/2$, and the second condition requires that $\\beta$ is not greater than $3/2$, i.e., $\\beta \\le 3/2$. Combining these, we need to choose a non-integer $\\beta$ from the interval $(1/2, 3/2]$. The value $\\beta = 6/5 = 1.2$ proposed in the problem statement is a perfect example, as $0.5  1.2 \\le 1.5$.\n\nThis example is significant for inverse theorems in approximation theory. Such theorems deduce a function's smoothness from the decay rate of its approximation error $E_{n}(f)_{2}$. For a function like $f_{\\beta}$ with $\\beta \\in (1/2, 3/2]$, the error $E_{n}(f_{\\beta})_{2}$ decays rapidly (known to be of order $O(n^{-2\\beta})$). A classical, unweighted inverse theorem might incorrectly suggest from this rapid decay that $f_{\\beta}$ belongs to the Sobolev space $H^2(\\Omega)$, which means $f_{\\beta}'' \\in L^2(\\Omega)$. However, our analysis shows this is false. The Ditzian-Totik theory clarifies this by linking the error decay rate to smoothness in a *weighted* Sobolev space. For $\\beta \\in (1/2, 3/2]$, our analysis confirms that $\\varphi^{2} f_{\\beta}'' \\in L^2(\\Omega)$, which is the correct characterization of smoothness indicated by the approximation rate. The function $f_{\\beta}$ thus illustrates that the space of functions with a certain rate of polynomial approximation is a weighted space. The endpoint weight $\\varphi$ is indispensable for correctly formulating inverse theorems that are sharp for functions with endpoint singularities.\n\nThe fourth and final step is to compute the specific exponent $e_w$ for the case $\\beta=6/5$, defined by the relation $|\\varphi^{2}(x)\\,f_{\\beta}''(x)|^{2}\\ \\asymp\\ (1-x)^{e_{w}}$ as $x\\to 1^{-}$.\nFrom our analysis in the first step, we established the local equivalence:\n$$\n|\\varphi^{2}(x) f_{\\beta}''(x)|^{2} \\asymp (1-x)^{2(\\beta-1)}\n$$\nBy comparing this with the given definition, we identify the exponent as:\n$$\ne_{w} = 2(\\beta-1)\n$$\nSubstituting the value $\\beta = 6/5$:\n$$\ne_{w} = 2\\left(\\frac{6}{5} - 1\\right) = 2\\left(\\frac{6}{5} - \\frac{5}{5}\\right) = 2\\left(\\frac{1}{5}\\right) = \\frac{2}{5}\n$$\nThis is the required value for the exponent $e_w$.",
            "answer": "$$\\boxed{\\frac{2}{5}}$$"
        },
        {
            "introduction": "Having established the necessity of measuring smoothness in a weighted sense, we now introduce the modern tool designed for this task: the Ditzian-Totik (DT) modulus of smoothness. This modulus, through its variable step size $h\\varphi(x)$, intrinsically captures the notion that a function can be less smooth near the endpoints. In this final practice, you will apply this powerful framework to determine the precise, sharp rate of uniform approximation for the function $f(x)=(1-x)^{\\gamma}$, providing a quintessential example of the predictive power of Jackson's theorems within the Ditzian-Totik theory .",
            "id": "3393513",
            "problem": "Consider algebraic polynomial approximation on the interval $[-1,1]$ in the uniform norm $L^{\\infty}$. Let $\\varphi(x) = \\sqrt{1 - x^{2}}$ and, for an integer $r \\geq 1$, define the $r$-th Ditzian–Totik (DT) modulus of smoothness in $L^{\\infty}$ by\n$$\n\\omega_{r}^{\\varphi}(f,t)_{L^{\\infty}} := \\sup_{0  h \\leq t} \\left\\| \\Delta_{h \\varphi(\\cdot)}^{r} f \\right\\|_{L^{\\infty}(-1,1)},\n$$\nwhere the $r$-th forward difference with variable step $\\delta(x) := h \\varphi(x)$ is\n$$\n\\Delta_{\\delta}^{r} f(x) := \\sum_{k=0}^{r} (-1)^{r-k} \\binom{r}{k} f\\big(x + k \\delta(x)\\big),\n$$\nwith the convention that the sum is taken over those $x$ for which all arguments $x + k \\delta(x)$ lie in $[-1,1]$. Consider the endpoint-singular function\n$$\nf(x) = (1 - x)^{\\gamma}, \\quad \\gamma \\in (0,1),\n$$\nand assume the approximation space consists of algebraic polynomials of degree at most $n$. Starting strictly from the above definitions and foundational Jackson-type direct and inverse principles for polynomial approximation in the Ditzian–Totik framework (without invoking any specialized shortcut formulas), determine:\n\n1. The leading-order asymptotic behavior in $t$ of the DT modulus $\\omega_{r}^{\\varphi}(f,t)_{L^{\\infty}}$ for a fixed integer $r$ satisfying $r  2 \\gamma$.\n\n2. The resulting leading-order asymptotic rate, in terms of $n$, of the best uniform-approximation error of $f$ by degree-$n$ polynomials,\n$$\nE_{n}(f)_{L^{\\infty}} := \\inf_{p \\in \\mathbb{P}_{n}} \\| f - p \\|_{L^{\\infty}(-1,1)},\n$$\nwhere $\\mathbb{P}_{n}$ denotes the set of algebraic polynomials of degree at most $n$.\n\nExpress your final answer as two closed-form analytic expressions, one in $t$ and one in $n$, respectively. No numerical rounding is required.",
            "solution": "The problem is valid as it is a well-posed question in the mathematical field of approximation theory, built upon standard definitions and concepts such as Ditzian-Totik moduli of smoothness and best polynomial approximation. All provided information is self-contained and scientifically sound.\n\n### Part 1: Asymptotic Behavior of the Ditzian–Totik Modulus\n\nThe first task is to determine the leading-order asymptotic behavior of the Ditzian–Totik (DT) modulus of smoothness $\\omega_{r}^{\\varphi}(f,t)_{L^{\\infty}}$ for the function $f(x) = (1 - x)^{\\gamma}$, where $\\gamma \\in (0,1)$, and the integer $r$ satisfies $r  2 \\gamma$.\n\nThe key to analyzing the DT modulus with the weight $\\varphi(x) = \\sqrt{1 - x^2}$ is to employ the change of variables $x = \\cos(\\theta)$, which maps the interval $x \\in [-1,1]$ to $\\theta \\in [0,\\pi]$. This transformation is fundamental in the theory of orthogonal polynomials and approximation on an interval, as it \"unfolds\" the endpoint singularities.\n\nLet $g(\\theta) = f(\\cos \\theta)$. The function $f(x)$ becomes:\n$$ g(\\theta) = (1 - \\cos \\theta)^{\\gamma} = \\left(2 \\sin^2\\left(\\frac{\\theta}{2}\\right)\\right)^{\\gamma} $$\nThe singularity of $f(x)$ at $x=1$ corresponds to a singularity of $g(\\theta)$ at $\\theta=0$. For small $\\theta  0$, we have $\\sin(\\theta/2) \\approx \\theta/2$, so the behavior of $g(\\theta)$ near $\\theta=0$ is:\n$$ g(\\theta) \\asymp \\theta^{2\\gamma} $$\nwhere $\\asymp$ denotes that the ratio of the two sides is bounded above and below by positive constants.\n\nThe variable step in the definition of the DT modulus is $\\delta(x) = h \\varphi(x) = h\\sqrt{1-x^2}$. Under the substitution $x=\\cos\\theta$, this step becomes $\\delta(\\cos\\theta) = h \\sin\\theta$.\nThe points $x_k = x + k \\delta(x)$ used in the forward difference operator become $x_k = \\cos\\theta + k h \\sin\\theta$. For small $h$, these points can be related to a uniform step in the $\\theta$ variable:\n$$ \\cos(\\theta - kh) = \\cos\\theta \\cos(kh) + \\sin\\theta \\sin(kh) \\approx \\cos\\theta (1) + \\sin\\theta (kh) = \\cos\\theta + kh \\sin\\theta = x_k $$\nThis implies that the point $x_k$ corresponds approximately to the angle $\\theta_k \\approx \\theta - kh$. Therefore, the forward difference of $f$ with a variable step in $x$ is approximately equivalent to a forward difference of $g$ with a constant step in $\\theta$:\n$$ \\Delta_{h\\varphi(x)}^{r} f(x) = \\sum_{k=0}^{r} (-1)^{r-k} \\binom{r}{k} f(x_k) \\approx \\sum_{k=0}^{r} (-1)^{r-k} \\binom{r}{k} g(\\theta - kh) = (-1)^r \\Delta_{-h}^r g(\\theta) $$\nwhere $\\Delta_{-h}^r$ is the standard $r$-th forward difference operator with step $-h$.\n\nThe norm is taken over $L^{\\infty}(-1,1)$, which corresponds to $L^{\\infty}[0,\\pi]$ for the variable $\\theta$. Thus, the DT modulus $\\omega_{r}^{\\varphi}(f,t)_{L^{\\infty}}$ is asymptotically equivalent to the classical modulus of smoothness $\\omega_r(g,t)_{L^{\\infty}[0,\\pi]}$:\n$$ \\omega_{r}^{\\varphi}(f,t)_{L^{\\infty}} \\asymp \\omega_r(g,t)_{L^{\\infty}[0,\\pi]} := \\sup_{0  h \\leq t} \\sup_{\\theta} |\\Delta_h^r g(\\theta)| $$\nThe behavior of $\\omega_r(g,t)$ is determined by the smoothness of $g(\\theta)$. The function $g(\\theta)$ is smooth everywhere on $[0,\\pi]$ except at $\\theta=0$, where $g(\\theta) \\asymp \\theta^{2\\gamma}$.\nFor a function behaving like $y^{\\alpha}$ near the origin, its $r$-th modulus of smoothness $\\omega_r(y^\\alpha, t)_{L^\\infty}$ has a well-known behavior. Given the condition $r  \\alpha$, we have:\n$$ \\omega_r(y^\\alpha, t)_{L^\\infty} \\asymp t^\\alpha $$\nIn our case, the exponent is $\\alpha = 2\\gamma$, and the problem states that $r  2\\gamma$. Therefore, the modulus of smoothness for $g(\\theta)$ behaves as:\n$$ \\omega_r(g,t)_{L^{\\infty}[0,\\pi]} \\asymp t^{2\\gamma} $$\nThis behavior stems from taking the difference operator across the singularity at $\\theta=0$. For any $\\theta_0  0$, $g$ is $C^\\infty$ on $[\\theta_0, \\pi]$, so $|\\Delta_h^r g(\\theta)| \\le C h^r$ for $\\theta \\ge \\theta_0$. Since $r  2\\gamma$, this term is dominated by the $t^{2\\gamma}$ behavior for small $t$.\n\nFrom the equivalence, we conclude that the leading-order asymptotic behavior of the DT modulus for $f(x)$ is:\n$$ \\omega_{r}^{\\varphi}(f,t)_{L^{\\infty}} \\asymp t^{2\\gamma} $$\n\n### Part 2: Asymptotic Rate of Best Approximation Error\n\nThe second task is to find the leading-order asymptotic rate of the best uniform approximation error $E_n(f)_{L^\\infty}$. This is achieved by using the foundational direct and inverse theorems of approximation theory in the Ditzian-Totik framework.\n\nA **direct theorem**, or Jackson-type theorem, provides an upper bound for the approximation error in terms of the modulus of smoothness. For any function $f \\in C([-1,1])$ and for $n \\ge r$, there is a constant $C$ such that:\n$$ E_n(f)_{L^\\infty} \\le C\\, \\omega_r^\\varphi\\left(f, \\frac{1}{n}\\right)_{L^\\infty} $$\nSubstituting the result from Part 1, where $\\omega_{r}^{\\varphi}(f,t)_{L^{\\infty}} \\asymp t^{2\\gamma}$:\n$$ E_n(f)_{L^\\infty} \\le C \\left(\\frac{1}{n}\\right)^{2\\gamma} = C n^{-2\\gamma} $$\nThis establishes an upper bound on the rate of convergence: $E_n(f)_{L^\\infty} = O(n^{-2\\gamma})$.\n\nAn **inverse theorem**, or Bernstein-Stechkin-type theorem, provides a lower bound on the modulus of smoothness in terms of the approximation errors. It states that for $t \\in (0, 1/r)$:\n$$ \\omega_r^\\varphi(f, t)_{L^\\infty} \\le C t^r \\sum_{k=0}^{\\lfloor 1/t \\rfloor} (k+1)^{r-1} E_k(f)_{L^\\infty} $$\nWe use this theorem to establish a lower bound on $E_n(f)_{L^\\infty}$. Let $t = 1/n$. Using the lower bound on the modulus from Part 1, $\\omega_{r}^{\\varphi}(f, 1/n)_{L^{\\infty}} \\ge C_1 n^{-2\\gamma}$, we get:\n$$ C_1 n^{-2\\gamma} \\le C n^{-r} \\sum_{k=0}^{n} (k+1)^{r-1} E_k(f)_{L^\\infty} $$\nThe sequence of best approximation errors $E_k(f)_{L^\\infty}$ is non-increasing. Thus, for $k \\le n$, we have $E_k(f)_{L^\\infty} \\ge E_n(f)_{L^\\infty}$. We can therefore write:\n$$ C_1 n^{-2\\gamma} \\le C n^{-r} \\sum_{k=0}^{n} (k+1)^{r-1} E_n(f)_{L^\\infty} = C E_n(f)_{L^\\infty} n^{-r} \\sum_{j=1}^{n+1} j^{r-1} $$\nThe sum can be bounded by an integral:\n$$ \\sum_{j=1}^{n+1} j^{r-1} \\asymp \\int_1^{n+1} x^{r-1} dx = \\frac{(n+1)^r - 1}{r} \\asymp n^r $$\nSubstituting this back into the inequality:\n$$ C_1 n^{-2\\gamma} \\le C E_n(f)_{L^\\infty} n^{-r} (C_2 n^r) = (C C_2) E_n(f)_{L^\\infty} $$\nThis implies a lower bound on the error:\n$$ E_n(f)_{L^\\infty} \\ge \\frac{C_1}{C C_2} n^{-2\\gamma} $$\nThis establishes that $E_n(f)_{L^\\infty} = \\Omega(n^{-2\\gamma})$.\n\nCombining the upper bound $E_n(f)_{L^\\infty} = O(n^{-2\\gamma})$ and the lower bound $E_n(f)_{L^\\infty} = \\Omega(n^{-2\\gamma})$, we conclude that the asymptotic rate of convergence is:\n$$ E_n(f)_{L^\\infty} \\asymp n^{-2\\gamma} $$\nThe leading-order asymptotic rate is $n^{-2\\gamma}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nt^{2\\gamma}  n^{-2\\gamma}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}