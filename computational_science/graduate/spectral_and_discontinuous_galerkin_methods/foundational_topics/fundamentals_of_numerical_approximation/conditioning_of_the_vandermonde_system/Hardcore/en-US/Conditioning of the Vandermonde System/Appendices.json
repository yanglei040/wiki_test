{
    "hands_on_practices": [
        {
            "introduction": "The Vandermonde matrix is a cornerstone of polynomial interpolation, but its numerical properties can be treacherous. This exercise provides a direct, hands-on demonstration of the severe ill-conditioning that arises when using the intuitive choice of equispaced nodes. By computationally exploring the condition number's explosive growth, you will gain a tangible appreciation for why the monomial basis on equispaced points is avoided in high-order methods. ",
            "id": "3256284",
            "problem": "Consider the derivation of Newton–Cotes quadrature rules, where the weights are determined by enforcing exactness on a polynomial basis. Let there be $n$ equally spaced nodes on an interval, denoted by $x_0, x_1, \\dots, x_{n-1}$. To construct weights that integrate polynomials exactly up to degree $n-1$, one uses the linear system that matches moments of monomials. The coefficient matrix in this linear system is the Vandermonde matrix $V \\in \\mathbb{R}^{n \\times n}$ with entries $V_{i,j} = x_i^j$ for $i = 0,1,\\dots,n-1$ and $j = 0,1,\\dots,n-1$. The numerical sensitivity of solving such a linear system is governed by the condition number of $V$ in the matrix $2$-norm, defined by $\\kappa_2(V) = \\|V\\|_2 \\|V^{-1}\\|_2$, which can be computed as the ratio of the largest singular value to the smallest singular value of $V$.\n\nStarting from the fundamental fact that Newton–Cotes weights are obtained by solving a linear system whose coefficient matrix is a Vandermonde matrix constructed from equispaced nodes, and the definition of the matrix $2$-norm condition number via singular values, investigate how the conditioning of this Vandermonde matrix depends on the number of nodes $n$ and on the interval scaling. Specifically, compare two intervals:\n(1) the unit interval $[0,1]$ with nodes $x_i = \\frac{i}{n-1}$ for $n > 1$ and $x_0 = 0$ for $n = 1$, and\n(2) the symmetric interval $[-1,1]$ with nodes $x_i = -1 + \\frac{2 i}{n-1}$ for $n > 1$ and $x_0 = 0$ for $n = 1$.\nFor each $n$, construct the corresponding Vandermonde matrix $V$ using the monomial basis $1, x, x^2, \\dots, x^{n-1}$, compute $\\kappa_2(V)$ via singular value decomposition, and report the base-$10$ logarithm $\\log_{10}(\\kappa_2(V))$ for both intervals. Additionally, report the difference $\\log_{10}(\\kappa_2(V \\text{ on } [0,1])) - \\log_{10}(\\kappa_2(V \\text{ on } [-1,1]))$ to quantify the effect of interval scaling on conditioning.\n\nYour task is to write a complete, runnable program that performs these computations in double precision for the following test suite of node counts $n$: $n \\in \\{1,2,3,5,10,15,20\\}$. This test suite includes a boundary case $n=1$, small cases, and larger cases where conditioning becomes severe. For each $n$ in the test suite, your program must output a triple of floats $[\\log_{10}(\\kappa_2(V_{[0,1]})), \\log_{10}(\\kappa_2(V_{[-1,1]})), \\log_{10}(\\kappa_2(V_{[0,1]})) - \\log_{10}(\\kappa_2(V_{[-1,1]}))]$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one $n$ in the test suite and is itself a comma-separated triple enclosed in square brackets. For example, the output should look like $[[a,b,c],[d,e,f],\\dots]$ with no spaces.\n\nNo physical units or angles are involved. All answers must be floats as specified, and the numerical values should be computed using the matrix $2$-norm condition number via singular values as described.",
            "solution": "The problem requires an investigation into the numerical conditioning of Vandermonde matrices constructed from equispaced nodes on two distinct intervals: the unit interval $[0,1]$ and the symmetric interval $[-1,1]$. This analysis is fundamental to understanding the numerical stability of methods for deriving Newton-Cotes quadrature weights, which rely on solving a linear system involving such a matrix.\n\nThe derivation of weights for an $n$-point quadrature rule that is exact for all polynomials up to degree $n-1$ requires solving the linear system $Vw = \\mathbf{m}$. In this system, $V$ is an $n \\times n$ Vandermonde matrix, $w$ is the vector of unknown weights, and $\\mathbf{m}$ is the vector of moments of the monomial basis functions, with entries $m_j = \\int_a^b x^j dx$ for $j=0, 1, \\dots, n-1$. The Vandermonde matrix $V$ is constructed from the $n$ quadrature nodes $x_0, x_1, \\dots, x_{n-1}$, with entries defined as $V_{i,j} = x_i^j$ for $i,j \\in \\{0, \\dots, n-1\\}$.\n\nThe numerical stability of solving $Vw = \\mathbf{m}$ is dictated by the condition number of the matrix $V$. A large condition number signifies that small errors in the input data (e.g., the moment vector $\\mathbf{m}$) can lead to large errors in the computed solution (the weights $w$). The matrix $2$-norm condition number, denoted $\\kappa_2(V)$, is defined as $\\kappa_2(V) = \\|V\\|_2 \\|V^{-1}\\|_2$. It is calculated as the ratio of the largest singular value ($\\sigma_{\\max}$) to the smallest singular value ($\\sigma_{\\min}$) of the matrix:\n$$ \\kappa_2(V) = \\frac{\\sigma_{\\max}(V)}{\\sigma_{\\min}(V)} $$\nSingular values are computed via Singular Value Decomposition (SVD). For Vandermonde matrices constructed from equispaced points, $\\kappa_2(V)$ is known to grow exponentially with the number of nodes $n$, leading to extreme ill-conditioning. To manage the large magnitude of these numbers, we analyze their base-$10$ logarithm, $\\log_{10}(\\kappa_2(V))$.\n\nThe procedure to address the problem is as follows:\nFor each number of nodes $n$ in the test suite $\\{1, 2, 3, 5, 10, 15, 20\\}$:\n$1$. The special case $n=1$ is handled first. The problem specifies the node $x_0 = 0$ for both intervals. This results in a $1 \\times 1$ Vandermonde matrix $V = [x_0^0] = [1]$. The singular value is $1$, so $\\kappa_2(V) = 1$ and $\\log_{10}(\\kappa_2(V)) = 0$ for both intervals.\n\n$2$. For cases where $n > 1$:\n   a. **Interval $[0,1]$**: A set of $n$ equispaced nodes $\\{x_i\\}$ is generated using the formula $x_i = \\frac{i}{n-1}$ for $i=0, 1, \\dots, n-1$. The corresponding Vandermonde matrix $V_{[0,1]}$ is constructed.\n   b. **Interval $[-1,1]$**: A second set of $n$ equispaced nodes $\\{x_i\\}$ is generated using the formula $x_i = -1 + \\frac{2i}{n-1}$ for $i=0, 1, \\dots, n-1$. The corresponding Vandermonde matrix $V_{[-1,1]}$ is constructed.\n   c. For both matrices, the $2$-norm condition number is computed using SVD-based methods.\n   d. The base-$10$ logarithm of each condition number is calculated.\n   e. The difference, $\\log_{10}(\\kappa_2(V_{[0,1]})) - \\log_{10}(\\kappa_2(V_{[-1,1]}))$, is computed to quantify the impact of interval choice on conditioning.\n\nThe choice of interval has a profound effect on conditioning. On $[0,1]$, all nodes are non-negative. The basis functions $x^j$ and $x^{j+1}$ behave similarly over this interval, resulting in column vectors of the Vandermonde matrix that are nearly linearly dependent. This near-collinearity is the source of the severe ill-conditioning. In contrast, the interval $[-1,1]$ is symmetric about the origin. This symmetry induces a degree of orthogonality between the column vectors of the Vandermonde matrix (e.g., for odd and even powers $j$), which significantly mitigates the ill-conditioning. The following program implements this analysis.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the log10 of the 2-norm condition number for Vandermonde matrices\n    on two different intervals for a given set of node counts.\n    \"\"\"\n    \n    # Test suite of node counts as specified in the problem statement.\n    test_cases_n = [1, 2, 3, 5, 10, 15, 20]\n    \n    results = []\n    \n    for n in test_cases_n:\n        # The problem statement defines V_{i,j} = x_i^j, which corresponds to\n        # increasing powers in the numpy.vander function.\n        # This is handled by setting the 'increasing' parameter to True.\n        \n        # We perform calculations in double precision as requested.\n        # numpy's default float is float64 (double precision).\n        dtype = np.float64\n        \n        # Handle the special case n=1 as per the problem description.\n        if n == 1:\n            # For n=1, x0=0 for both intervals. V=[[1]].\n            # The condition number of a 1x1 non-zero matrix is 1.\n            # log10(1) = 0.\n            log_kappa_01 = 0.0\n            log_kappa_m11 = 0.0\n        else:\n            # --- Interval [0, 1] ---\n            # Generate n equispaced nodes from 0 to 1.\n            nodes_01 = np.linspace(0.0, 1.0, n, dtype=dtype)\n            # Construct the Vandermonde matrix.\n            V_01 = np.vander(nodes_01, N=n, increasing=True)\n            # Compute the 2-norm condition number.\n            kappa_01 = np.linalg.cond(V_01, p=2)\n            # Compute the base-10 logarithm.\n            log_kappa_01 = np.log10(kappa_01)\n            \n            # --- Interval [-1, 1] ---\n            # Generate n equispaced nodes from -1 to 1.\n            nodes_m11 = np.linspace(-1.0, 1.0, n, dtype=dtype)\n            # Construct the Vandermonde matrix.\n            V_m11 = np.vander(nodes_m11, N=n, increasing=True)\n            # Compute the 2-norm condition number.\n            kappa_m11 = np.linalg.cond(V_m11, p=2)\n            # Compute the base-10 logarithm.\n            log_kappa_m11 = np.log10(kappa_m11)\n\n        # Calculate the difference in log-condition numbers.\n        diff = log_kappa_01 - log_kappa_m11\n        \n        # Store the triple of results for this n.\n        results.append([log_kappa_01, log_kappa_m11, diff])\n        \n    # Format the final output string exactly as required: [[a,b,c],[d,e,f],...].\n    # No spaces are permitted in the final output string.\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]) + \"]\"\n    \n    # Print the single-line result to standard output.\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "In practical spectral and Discontinuous Galerkin (DG) methods, computations are typically performed on a reference element and then mapped to physical elements of various sizes and locations. This practice delves into the crucial question of how this mapping affects the stability of the underlying linear systems. Through an analytical derivation, you will uncover the direct relationship between the element's size, given by a scaling factor $\\alpha$, and the Vandermonde matrix's condition number, revealing a fundamental scaling law for anyone implementing these methods. ",
            "id": "3372915",
            "problem": "Consider a polynomial interpolation on a one-dimensional physical element obtained from the reference element by the affine map $x=\\alpha\\,\\xi+\\beta$, where $\\xi\\in[-1,1]$, $\\alpha\\in\\mathbb{R}\\setminus\\{0\\}$ is the element half-size scaling, and $\\beta\\in\\mathbb{R}$ is the shift to the element center. Let $\\{\\xi_{m}\\}_{m=0}^{p}$ be any fixed set of $p+1$ distinct reference nodes in $[-1,1]$ (e.g., Gauss–Legendre or Gauss–Lobatto nodes), and define the corresponding physical nodes $x_{m}=\\alpha\\,\\xi_{m}+\\beta$. For the monomial basis in the physical coordinate $x$, define the square Vandermonde matrix $V(\\alpha,\\beta)\\in\\mathbb{R}^{(p+1)\\times(p+1)}$ by\n$$\nV(\\alpha,\\beta)_{m,j} \\;=\\; x_{m}^{\\,j-1}\\,,\\qquad m=0,\\dots,p\\,,\\quad j=1,\\dots,p+1\\,.\n$$\nThis Vandermonde arises when solving for modal coefficients in spectral and Discontinuous Galerkin (DG) methods with a monomial basis on the physical element.\n\nStarting from fundamental definitions (the affine map, monomial basis, and the binomial theorem), do the following:\n1. Derive an explicit factorization of $V(\\alpha,\\beta)$ in terms of the reference-coordinate Vandermonde $V_{\\mathrm{ref}}\\in\\mathbb{R}^{(p+1)\\times(p+1)}$, defined by $V_{\\mathrm{ref}}{}_{m,k+1}=\\xi_{m}^{\\,k}$ for $k=0,\\dots,p$, and an upper-triangular column transformation $T(\\alpha,\\beta)\\in\\mathbb{R}^{(p+1)\\times(p+1)}$ depending only on $\\alpha$ and $\\beta$.\n2. Using well-tested properties of singular values under matrix multiplication, determine the leading-order multiplicative scaling factor $f(\\alpha)$ such that the two-norm condition number $\\kappa_{2}(V(\\alpha,\\beta))$ satisfies\n$$\n\\kappa_{2}\\!\\big(V(\\alpha,\\beta)\\big)\\;\\sim\\;\\kappa_{2}\\!\\big(V(1,\\beta)\\big)\\,f(\\alpha)\n$$\nas $|\\alpha|\\to\\infty$ with fixed $\\beta$ and fixed reference nodes $\\{\\xi_{m}\\}$.\n\nProvide the final answer as the closed-form expression for $f(\\alpha)$ in terms of $p$ and $\\alpha$. No numerical rounding is required.",
            "solution": "The analysis proceeds in two parts as requested. First, we derive the factorization of the Vandermonde matrix. Second, we analyze the asymptotic behavior of its condition number.\n\n### Part 1: Factorization of the Vandermonde Matrix\n\nLet the physical and reference coordinates be related by the affine map $x = \\alpha\\xi + \\beta$. The physical nodes are $x_m = \\alpha\\xi_m + \\beta$. The Vandermonde matrix $V(\\alpha, \\beta)$ in the physical coordinate $x$ is defined by its entries for $m=0, \\dots, p$ and $j=1, \\dots, p+1$:\n$$\nV(\\alpha, \\beta)_{m,j} = x_m^{j-1} = (\\alpha\\xi_m + \\beta)^{j-1}\n$$\nWe apply the binomial theorem to expand this expression:\n$$\n(\\alpha\\xi_m + \\beta)^{j-1} = \\sum_{k=0}^{j-1} \\binom{j-1}{k} (\\alpha\\xi_m)^k \\beta^{j-1-k} = \\sum_{k=0}^{j-1} \\left[ \\binom{j-1}{k} \\alpha^k \\beta^{j-1-k} \\right] \\xi_m^k\n$$\nThe reference Vandermonde matrix $V_{\\mathrm{ref}}$ is defined by its entries $V_{\\mathrm{ref}, m, k+1} = \\xi_m^k$ for $m=0, \\dots, p$ and $k=0, \\dots, p$. We can rewrite the expansion for $V(\\alpha, \\beta)_{m,j}$ using the entries of $V_{\\mathrm{ref}}$:\n$$\nV(\\alpha, \\beta)_{m,j} = \\sum_{k=0}^{j-1} \\left[ \\binom{j-1}{k} \\alpha^k \\beta^{j-1-k} \\right] V_{\\mathrm{ref}, m, k+1}\n$$\nSince $\\binom{j-1}{k} = 0$ for $k > j-1$, we can extend the sum up to $k=p$ without changing the result:\n$$\nV(\\alpha, \\beta)_{m,j} = \\sum_{k=0}^{p} \\left[ \\binom{j-1}{k} \\alpha^k \\beta^{j-1-k} \\right] V_{\\mathrm{ref}, m, k+1}\n$$\nTo match the standard matrix multiplication rule $(AB)_{ij} = \\sum_l A_{il}B_{lj}$, we must re-index the sum. Let the column index for $V_{\\mathrm{ref}}$ be $l=k+1$. Then $k=l-1$, and the sum runs from $l=1$ to $p+1$.\n$$\nV(\\alpha, \\beta)_{m,j} = \\sum_{l=1}^{p+1} V_{\\mathrm{ref}, m, l} \\left[ \\binom{j-1}{l-1} \\alpha^{l-1} \\beta^{j-1-(l-1)} \\right] = \\sum_{l=1}^{p+1} V_{\\mathrm{ref}, m, l} \\left[ \\binom{j-1}{l-1} \\alpha^{l-1} \\beta^{j-l} \\right]\n$$\nThis expression is the $(m,j)$-th entry of the matrix product $V_{\\mathrm{ref}} T(\\alpha, \\beta)$, where $T(\\alpha, \\beta)$ is a $(p+1)\\times(p+1)$ matrix with entries:\n$$\nT(\\alpha, \\beta)_{l,j} = \\binom{j-1}{l-1} \\alpha^{l-1} \\beta^{j-l}\n$$\nThe binomial coefficient $\\binom{j-1}{l-1}$ is zero if $l-1 > j-1$, which is equivalent to $l > j$. Therefore, $T(\\alpha, \\beta)_{l,j} = 0$ for $l > j$, and the matrix $T(\\alpha, \\beta)$ is upper-triangular. This completes the first part of the derivation, yielding the factorization $V(\\alpha, \\beta) = V_{\\mathrm{ref}} T(\\alpha, \\beta)$.\n\n### Part 2: Asymptotic Scaling of the Condition Number\n\nThe 2-norm condition number is defined as $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. We analyze the asymptotic behavior of $\\kappa_2(V(\\alpha, \\beta))$ as $|\\alpha| \\to \\infty$.\n\nFirst, we further factor the transformation matrix $T(\\alpha, \\beta)$. Let $D_\\alpha$ be the diagonal matrix $D_\\alpha = \\mathrm{diag}(1, \\alpha, \\alpha^2, \\dots, \\alpha^p)$. Let $C(\\beta)$ be the matrix with entries $C(\\beta)_{l,j} = \\binom{j-1}{l-1} \\beta^{j-l}$. Notice that $T(\\alpha, \\beta)_{l,j} = \\alpha^{l-1} C(\\beta)_{l,j}$. This corresponds to the matrix product $T(\\alpha, \\beta) = D_\\alpha C(\\beta)$, where we use $1$-based indexing for $D_\\alpha$'s diagonal entries $\\alpha^{l-1}$ for $l=1, \\dots, p+1$.\nThe full factorization is thus $V(\\alpha, \\beta) = V_{\\mathrm{ref}} D_\\alpha C(\\beta)$.\nThe matrix $C(\\beta)$ depends only on $\\beta$. Its diagonal entries are $C(\\beta)_{j,j} = \\binom{j-1}{j-1}\\beta^{j-j} = 1$, so it is a unit upper-triangular matrix, which is always invertible.\n\nWe analyze the norms $\\|V(\\alpha, \\beta)\\|_2$ and $\\|V(\\alpha, \\beta)^{-1}\\|_2$ in the limit $|\\alpha| \\to \\infty$.\nThe operator norm is $\\|A\\|_2 = \\sigma_{\\max}(A)$, where $\\sigma_{\\max}$ is the largest singular value.\n\nFor the forward norm, we analyze the limit of $\\|V(\\alpha, \\beta)\\|_2 / |\\alpha|^p$:\n$$\n\\lim_{|\\alpha|\\to\\infty} \\frac{\\|V(\\alpha, \\beta)\\|_2}{|\\alpha|^p} = \\lim_{|\\alpha|\\to\\infty} \\left\\| V_{\\mathrm{ref}} \\frac{D_\\alpha}{|\\alpha|^p} C(\\beta) \\right\\|_2\n$$\nThe matrix $D_\\alpha / |\\alpha|^p = \\mathrm{diag}(|\\alpha|^{-p}, \\alpha|\\alpha|^{-p}, \\dots, \\alpha^p|\\alpha|^{-p})$. As $|\\alpha|\\to\\infty$, this matrix converges to $E_{p+1,p+1} \\exp(i p \\arg(\\alpha))$, where $E_{p+1,p+1}$ is the matrix with a $1$ in the bottom-right entry and zeros elsewhere. Since the norm is insensitive to the phase factor, we consider the limit matrix $V_{\\mathrm{ref}} E_{p+1,p+1} C(\\beta)$.\nThe matrix $E_{p+1,p+1}C(\\beta)$ is zero everywhere except for its last row, which is the last row of $C(\\beta)$. The last row of the unit upper-triangular matrix $C(\\beta)$ is $(0, \\dots, 0, 1)$, i.e., $e_{p+1}^T$. So $E_{p+1,p+1}C(\\beta) = E_{p+1,p+1}$.\nThe limit becomes $\\|V_{\\mathrm{ref}} E_{p+1,p+1}\\|_2$. This matrix consists of all zero columns except for the last, which is the last column of $V_{\\mathrm{ref}}$. Let this column be $c_{p+1}$. The norm of this matrix is simply the vector norm $\\|c_{p+1}\\|_2$.\nThus, for large $|\\alpha|$, $\\|V(\\alpha, \\beta)\\|_2 \\sim |\\alpha|^p \\|c_{p+1}\\|_2$.\n\nFor the inverse norm, we have $V(\\alpha, \\beta)^{-1} = C(\\beta)^{-1} D_\\alpha^{-1} V_{\\mathrm{ref}}^{-1}$.\n$$\n\\|V(\\alpha, \\beta)^{-1}\\|_2 = \\|C(\\beta)^{-1} D_\\alpha^{-1} V_{\\mathrm{ref}}^{-1}\\|_2\n$$\nAs $|\\alpha|\\to\\infty$, the matrix $D_\\alpha^{-1} = \\mathrm{diag}(1, \\alpha^{-1}, \\dots, \\alpha^{-p})$ converges to $E_{1,1}$, which has a $1$ in the top-left entry and zeros elsewhere.\nThe limit of the norm is $\\|C(\\beta)^{-1} E_{1,1} V_{\\mathrm{ref}}^{-1}\\|_2$. The matrix $E_{1,1}V_{\\mathrm{ref}}^{-1}$ is zero except for its first row, which is the first row of $V_{\\mathrm{ref}}^{-1}$. Let this row be $R_1$. The matrix is $e_1 R_1$. The product is $(C(\\beta)^{-1}e_1) R_1$, a rank-1 matrix. Its norm is $\\|C(\\beta)^{-1}e_1\\|_2 \\|R_1\\|_2$.\nSince $C(\\beta)$ is unit upper-triangular, its inverse $C(\\beta)^{-1}$ is also unit upper-triangular. The first column of any unit upper-triangular matrix is $e_1$. Thus, $\\|C(\\beta)^{-1}e_1\\|_2 = \\|e_1\\|_2 = 1$.\nThe asymptotic inverse norm is $\\|V(\\alpha, \\beta)^{-1}\\|_2 \\sim \\|R_1\\|_2$.\n\nCombining these results, the asymptotic behavior of the condition number is:\n$$\n\\kappa_2(V(\\alpha, \\beta)) \\sim (|\\alpha|^p \\|c_{p+1}\\|_2) \\cdot (\\|R_1\\|_2) = |\\alpha|^p (\\|c_{p+1}\\|_2 \\|R_1\\|_2)\n$$\nThe constant term $\\|c_{p+1}\\|_2 \\|R_1\\|_2$ depends only on the fixed reference nodes $\\{\\xi_m\\}$ and is independent of both $\\alpha$ and $\\beta$.\nThe problem asks for a scaling factor $f(\\alpha)$ such that $\\kappa_2(V(\\alpha, \\beta)) \\sim \\kappa_2(V(1, \\beta)) f(\\alpha)$.\nSubstituting our asymptotic result:\n$$\n|\\alpha|^p (\\|c_{p+1}\\|_2 \\|R_1\\|_2) \\sim \\kappa_2(V(1, \\beta)) f(\\alpha)\n$$\nHere, $\\kappa_2(V(1, \\beta))$ is a constant with respect to $\\alpha$ (but depends on $\\beta$), and the constant $(\\|c_{p+1}\\|_2 \\|R_1\\|_2)$ is independent of $\\beta$. The term \"leading-order multiplicative scaling factor\" refers to the part of the expression that dictates the growth rate with respect to the variable of interest, $\\alpha$. This requires isolating the functional dependence on $\\alpha$. The various constants, both dependent and independent of $\\beta$, are absorbed into the proportionality of the asymptotic relation `~`.\nThe function that captures the growth is clearly $|\\alpha|^p$.\n\nFinal Answer Derivation: From the asymptotic relation, we have\n$$\nf(\\alpha) \\sim \\frac{\\|c_{p+1}\\|_2 \\|R_1\\|_2}{\\kappa_2(V(1, \\beta))} |\\alpha|^p\n$$\nThe prefactor to $|\\alpha|^p$ is a constant for a fixed $\\beta$. The scaling law is therefore governed by $|\\alpha|^p$. Thus, we identify $f(\\alpha)$ as $|\\alpha|^p$.",
            "answer": "$$\\boxed{|\\alpha|^p}$$"
        },
        {
            "introduction": "Having established the ill-conditioning of the monomial Vandermonde system, we now turn to a powerful and elegant solution. This exercise demonstrates that the problem lies not with polynomial interpolation itself, but with the choice of a non-orthogonal basis. You will derive a preconditioning strategy that transforms the problem into a basis of orthonormal Legendre polynomials, resulting in a system with a perfect condition number of $1$ and revealing the principle that underpins the stability of modern spectral methods. ",
            "id": "3372834",
            "problem": "Consider a one-dimensional element on the interval $[-1,1]$ in a nodal spectral method or a Discontinuous Galerkin (DG) discretization. Let $N \\in \\mathbb{N}$ be fixed and let $\\{x_{i}\\}_{i=1}^{N}$ denote the $N$ Gauss–Legendre nodes, that is, the $N$ distinct roots of the Legendre polynomial of degree $N$. Let $\\{w_{i}\\}_{i=1}^{N}$ be the associated Gauss–Legendre quadrature weights. Define the monomial Vandermonde matrix $V \\in \\mathbb{R}^{N \\times N}$ by\n$$\nV_{i,k} = x_{i}^{k}, \\quad 1 \\leq i \\leq N, \\quad 0 \\leq k \\leq N-1.\n$$\nLet $\\{p_{j}\\}_{j=0}^{N-1}$ be the Legendre polynomials on $[-1,1]$ normalized to be orthonormal with respect to the standard $L^{2}([-1,1])$ inner product with unit weight, that is,\n$$\n\\int_{-1}^{1} p_{j}(x)\\,p_{k}(x)\\,\\mathrm{d}x = \\delta_{jk}, \\quad 0 \\leq j,k \\leq N-1.\n$$\nDefine the evaluation matrix $\\Phi \\in \\mathbb{R}^{N \\times N}$ by\n$$\n\\Phi_{i,j} = p_{j}(x_{i}), \\quad 1 \\leq i \\leq N, \\quad 0 \\leq j \\leq N-1.\n$$\nThere exists a change-of-basis matrix $C \\in \\mathbb{R}^{N \\times N}$ from the monomial basis to the orthonormal polynomial basis such that $\\Phi = V C$. Also consider the diagonal matrix of quadrature weights $W = \\mathrm{diag}(w_{1},\\dots,w_{N})$ and its positive definite square root $L = W^{1/2} = \\mathrm{diag}(\\sqrt{w_{1}},\\dots,\\sqrt{w_{N}})$.\n\nStarting only from the definitions above and the basic properties of orthonormal polynomials and Gauss–Legendre quadrature, derive a preconditioner based on projection to the orthonormal polynomial basis that transforms the monomial Vandermonde matrix into a near-orthogonal evaluation matrix. In particular, construct left and right preconditioners built from $L$ and $C$ such that the preconditioned matrix has orthonormal columns in the Euclidean inner product, and use this to rigorously analyze the effectiveness of this preconditioning on the $2$-norm condition number $\\kappa_{2}$.\n\nYour task is to compute the exact value of the $2$-norm condition number $\\kappa_{2}$ of the fully preconditioned matrix when the polynomial degree is $N-1$ and the nodes are the $N$ Gauss–Legendre points with the associated quadrature weights. Express your final answer as an exact number. No rounding is required.",
            "solution": "The objective is to derive a preconditioning strategy for the monomial Vandermonde matrix $V$ and to compute the $2$-norm condition number of the resulting preconditioned matrix. The preconditioning should be constructed using the provided matrices $L$ and $C$.\n\nLet the preconditioned matrix be denoted by $A$. A general two-sided preconditioning of a matrix $V$ takes the form $A = P_L V P_R$, where $P_L$ and $P_R$ are the left and right preconditioners, respectively. The problem specifies that these preconditioners are to be constructed from the change-of-basis matrix $C$ and the scaled quadrature weight matrix $L = W^{1/2}$.\n\nThe problem states that there exists a change-of-basis matrix $C \\in \\mathbb{R}^{N \\times N}$ from the monomial basis $\\{x^k\\}_{k=0}^{N-1}$ to the orthonormal polynomial basis $\\{p_j\\}_{j=0}^{N-1}$ such that $\\Phi = V C$. This relation connects the ill-conditioned monomial Vandermonde matrix $V$ to the better-conditioned evaluation matrix $\\Phi$ of the orthonormal polynomials. A natural choice for the right preconditioner is $P_R = C$, as this effectively changes the basis of the problem from monomials to the orthonormal Legendre polynomials.\n\nApplying this right preconditioner, we get an intermediate matrix $V C = \\Phi$. The columns of $\\Phi$ are the values of the orthonormal polynomials $\\{p_j\\}_{j=0}^{N-1}$ evaluated at the Gauss-Legendre nodes $\\{x_i\\}_{i=1}^{N}$. The problem requires that the final preconditioned matrix has columns that are orthonormal in the Euclidean inner product. Let's examine the columns of $\\Phi$. The inner product of the $j$-th and $k$-th columns of $\\Phi$ is given by:\n$$\n(\\Phi_{:,j})^T (\\Phi_{:,k}) = \\sum_{i=1}^{N} \\Phi_{ij} \\Phi_{ik} = \\sum_{i=1}^{N} p_j(x_i) p_k(x_i)\n$$\nThis expression is not generally equal to the Kronecker delta $\\delta_{jk}$. However, we can use the fundamental property of Gauss-Legendre quadrature. The quadrature rule, using $N$ nodes $\\{x_i\\}$ and weights $\\{w_i\\}$, is exact for any polynomial $f(x)$ of degree at most $2N-1$. That is,\n$$\n\\int_{-1}^{1} f(x)\\,\\mathrm{d}x = \\sum_{i=1}^{N} w_i f(x_i)\n$$\nThe polynomials $\\{p_j(x)\\}_{j=0}^{N-1}$ are given to be orthonormal on $[-1,1]$ with a unit weight function:\n$$\n\\int_{-1}^{1} p_j(x) p_k(x)\\,\\mathrm{d}x = \\delta_{jk}\n$$\nConsider the function $f(x) = p_j(x) p_k(x)$. The degree of this polynomial is $j+k$. Since $0 \\le j, k \\le N-1$, the maximum degree is $(N-1) + (N-1) = 2N-2$. As $2N-2 < 2N-1$, the Gauss-Legendre quadrature rule is exact for $p_j(x) p_k(x)$. Therefore, we can write:\n$$\n\\delta_{jk} = \\int_{-1}^{1} p_j(x) p_k(x)\\,\\mathrm{d}x = \\sum_{i=1}^{N} w_i p_j(x_i) p_k(x_i)\n$$\nLet us express this result in matrix form. The sum on the right side is the $(j,k)$-th entry of the matrix product $\\Phi^T W \\Phi$, where $W = \\mathrm{diag}(w_1, \\dots, w_N)$. Specifically:\n$$\n(\\Phi^T W \\Phi)_{jk} = \\sum_{l=1}^{N} (\\Phi^T)_{jl} (W \\Phi)_{lk} = \\sum_{l=1}^{N} \\Phi_{lj} \\sum_{m=1}^{N} W_{lm} \\Phi_{mk} = \\sum_{l=1}^{N} \\Phi_{lj} (w_l \\Phi_{lk}) = \\sum_{l=1}^{N} w_l p_j(x_l) p_k(x_l)\n$$\nComparing with the quadrature result, we find:\n$$\n\\Phi^T W \\Phi = I\n$$\nwhere $I$ is the $N \\times N$ identity matrix. This shows that the columns of $\\Phi$ are orthogonal with respect to the inner product weighted by the matrix $W$.\n\nThe problem demands that the columns of the preconditioned matrix $A$ be orthonormal in the standard Euclidean inner product, which means we require $A^T A = I$. Let the left preconditioner be $P_L$. The preconditioned matrix is $A = P_L V C = P_L \\Phi$. Let's compute $A^T A$:\n$$\nA^T A = (P_L \\Phi)^T (P_L \\Phi) = \\Phi^T P_L^T P_L \\Phi\n$$\nWe want this to be equal to the identity matrix $I$. Comparing with our derived property $\\Phi^T W \\Phi = I$, an obvious choice is to set $P_L^T P_L = W$.\nThe problem provides the matrix $L = W^{1/2} = \\mathrm{diag}(\\sqrt{w_1},\\dots,\\sqrt{w_N})$. As the quadrature weights $w_i$ are positive, $L$ is a real, symmetric, positive-definite matrix. Let's choose $P_L=L$. Then:\n$$\nP_L^T P_L = L^T L = L L = W^{1/2} W^{1/2} = W\n$$\nThis choice satisfies the condition. Thus, the fully preconditioned matrix is $A = L V C$.\n\nWe can now verify the orthonormality of the columns of $A$:\n$$\nA = L(VC) = L\\Phi\n$$\n$$\nA^T A = (L\\Phi)^T (L\\Phi) = \\Phi^T L^T L \\Phi = \\Phi^T W \\Phi = I\n$$\nSince $A$ is an $N \\times N$ square matrix and satisfies $A^T A = I$, $A$ is an orthogonal matrix.\n\nThe final task is to compute the $2$-norm condition number, $\\kappa_2(A)$, of this matrix $A$. The condition number is defined as $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. Alternatively, for a non-singular matrix, it can be expressed as the ratio of the largest to the smallest singular value: $\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}$.\n\nThe singular values of a matrix $A$ are the square roots of the eigenvalues of the matrix $A^T A$. In our case, we have established that $A^T A = I$. The eigenvalues of the identity matrix $I$ are all equal to $1$. Let $\\lambda_j$ be the eigenvalues of $A^T A$. Then $\\lambda_j = 1$ for all $j=1, \\dots, N$.\nThe singular values $\\sigma_j(A)$ are given by:\n$$\n\\sigma_j(A) = \\sqrt{\\lambda_j(A^T A)} = \\sqrt{1} = 1 \\quad \\text{for } j=1, \\dots, N\n$$\nSince all singular values of $A$ are equal to $1$, the largest singular value is $\\sigma_{\\max}(A) = 1$ and the smallest singular value is $\\sigma_{\\min}(A) = 1$.\nThe $2$-norm condition number of $A$ is therefore:\n$$\n\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)} = \\frac{1}{1} = 1\n$$\nThis result demonstrates the profound effectiveness of the preconditioning. The original monomial Vandermonde matrix $V$ is notoriously ill-conditioned, with its condition number growing exponentially with $N$. The preconditioned matrix $A=LVC$ is an orthogonal matrix, possessing the optimal condition number of $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        }
    ]
}