{
    "hands_on_practices": [
        {
            "introduction": "This problem asks to derive the scaling of an inverse inequality with respect to polynomial degree $p$ and element size $h$. This is a foundational skill, demonstrating how properties on a single reference element translate to an entire mesh. You will then apply this result to a practical scenario involving the stability of a post-processing scheme, learning how inverse inequalities govern the design and limitations of numerical methods. ",
            "id": "3392877",
            "problem": "Consider a single one-dimensional mesh element $K$ of length $h$ supporting a polynomial finite-dimensional space $\\mathbb{P}_{p}(K)$ of degree at most $p$, as used in Spectral methods and Discontinuous Galerkin (DG) methods. Let $v_{h} \\in \\mathbb{P}_{p}(K)$ be a polynomial field on $K$. A local post-processing operator based on the $k$-th derivative, denoted $\\mathcal{P}_{k}$, is employed to enhance the maximum-norm (i.e., $L^{\\infty}$-norm) accuracy of $v_{h}$ by canceling leading-order error terms through a polynomial reconstruction that uses derivatives up to order $k$. The $L^{\\infty}$ improvement of such a reconstruction is monotone in $k$, with larger $k$ yielding a stronger reduction in the $L^{\\infty}$-error.\n\nStarting from the affine mapping from the reference element $[-1,1]$ to $K$, the scaling of derivatives under this mapping, and norm equivalence properties of $\\mathbb{P}_{p}([-1,1])$, derive an inverse inequality of the form that bounds $\\|D^{k} v_{h}\\|_{L^{2}(K)}$ by a constant times a power of $p$ and $h$ multiplied by $\\|v_{h}\\|_{L^{2}(K)}$. Use this inverse inequality to justify that the stability amplification of the post-processor can be bounded above by an expression of the form $\\alpha \\, C_{\\mathrm{inv}} \\, p^{2k} \\, h^{-k}$, for a constant $C_{\\mathrm{inv}}$ that depends only on the choice of reference norms and the polynomial basis but not on $h$ or $p$. Assume that the $L^{\\infty}$ improvement grows monotonically with $k$ (so a larger $k$ is preferable), but must be chosen to keep the amplification bounded by unity, and that $k$ must satisfy $0 \\le k \\le p$.\n\nGiven the numerical values $p=8$, $h=20$, $\\alpha=0.25$, and $C_{\\mathrm{inv}}=0.125$, choose the largest integer $k$ that optimizes the $L^{\\infty}$ improvement without violating the stability constraint $\\alpha \\, C_{\\mathrm{inv}} \\, p^{2k} \\, h^{-k} \\le 1$, subject to $k \\le p$. Report the chosen integer $k$ as your final answer.",
            "solution": "The problem requires us to first derive an inverse inequality relating the $L^2$-norm of the $k$-th derivative of a polynomial to the $L^2$-norm of the polynomial itself. Subsequently, we must justify the form of a given stability amplification bound and then determine the optimal integer order $k$ for a post-processing operator subject to a stability constraint.\n\nFirst, we derive the inverse inequality. Let $\\hat{K} = [-1, 1]$ be the reference element and $K$ be a physical element of length $h$. The affine mapping $F: \\hat{K} \\to K$ can be written as $x = F(\\hat{x}) = x_c + \\frac{h}{2}\\hat{x}$, where $x_c$ is the center of the element $K$. The Jacobian of this mapping is $J = \\frac{dx}{d\\hat{x}} = \\frac{h}{2}$.\n\nLet $v_h \\in \\mathbb{P}_p(K)$ be a polynomial of degree at most $p$ on the physical element $K$. We can define a corresponding polynomial $\\hat{v} \\in \\mathbb{P}_p(\\hat{K})$ on the reference element via the composition $\\hat{v}(\\hat{x}) = v_h(x(\\hat{x}))$. Using the chain rule for differentiation, we relate the derivatives on $K$ and $\\hat{K}$:\n$$ \\frac{dv_h}{dx} = \\frac{d\\hat{v}}{d\\hat{x}} \\frac{d\\hat{x}}{dx} = \\frac{d\\hat{v}}{d\\hat{x}} \\left(\\frac{dx}{d\\hat{x}}\\right)^{-1} = \\frac{2}{h} \\frac{d\\hat{v}}{d\\hat{x}} $$\nBy repeated application of this rule, the $k$-th derivative on the physical element is related to the $k$-th derivative on the reference element by:\n$$ D^k v_h \\equiv \\frac{d^k v_h}{dx^k} = \\left(\\frac{2}{h}\\right)^k \\frac{d^k \\hat{v}}{d\\hat{x}^k} \\equiv \\left(\\frac{2}{h}\\right)^k D^k \\hat{v} $$\nNext, we relate the $L^2$-norms on the two elements. The $L^2$-norm of a function $w$ on an element $I$ is defined as $\\|w\\|_{L^2(I)} = \\left(\\int_I |w|^2 dx\\right)^{1/2}$. For the function $v_h$, we perform a change of variables $x \\to \\hat{x}$, for which the differential element transforms as $dx = J d\\hat{x} = \\frac{h}{2} d\\hat{x}$:\n$$ \\|v_h\\|_{L^2(K)}^2 = \\int_K |v_h(x)|^2 dx = \\int_{-1}^1 |\\hat{v}(\\hat{x})|^2 \\frac{h}{2} d\\hat{x} = \\frac{h}{2} \\|\\hat{v}\\|_{L^2(\\hat{K})}^2 $$\nThis gives the relation $\\|\\hat{v}\\|_{L^2(\\hat{K})}^2 = \\frac{2}{h} \\|v_h\\|_{L^2(K)}^2$.\n\nWe apply the same transformation to the norm of the $k$-th derivative:\n$$ \\|D^k v_h\\|_{L^2(K)}^2 = \\int_K |D^k v_h(x)|^2 dx = \\int_{-1}^1 \\left|\\left(\\frac{2}{h}\\right)^k D^k \\hat{v}(\\hat{x})\\right|^2 \\frac{h}{2} d\\hat{x} = \\left(\\frac{2}{h}\\right)^{2k} \\frac{h}{2} \\|D^k \\hat{v}\\|_{L^2(\\hat{K})}^2 $$\nA standard inverse inequality for polynomials $\\hat{v} \\in \\mathbb{P}_p(\\hat{K})$ on the reference element states that there exists a constant $\\hat{C}_k$, independent of $p$, such that:\n$$ \\|D^k \\hat{v}\\|_{L^2(\\hat{K})} \\le \\hat{C}_k p^{2k} \\|\\hat{v}\\|_{L^2(\\hat{K})} $$\nThe scaling $p^{2k}$ is a known sharp estimate for large polynomial degrees. Substituting this inequality into our expression for $\\|D^k v_h\\|_{L^2(K)}^2$:\n$$ \\|D^k v_h\\|_{L^2(K)}^2 \\le \\left(\\frac{2}{h}\\right)^{2k} \\frac{h}{2} \\left( \\hat{C}_k p^{2k} \\|\\hat{v}\\|_{L^2(\\hat{K})} \\right)^2 = \\left(\\frac{2}{h}\\right)^{2k} \\frac{h}{2} \\hat{C}_k^2 p^{4k} \\|\\hat{v}\\|_{L^2(\\hat{K})}^2 $$\nNow we substitute $\\|\\hat{v}\\|_{L^2(\\hat{K})}^2 = \\frac{2}{h} \\|v_h\\|_{L^2(K)}^2$ into the right-hand side:\n$$ \\|D^k v_h\\|_{L^2(K)}^2 \\le \\left(\\frac{2}{h}\\right)^{2k} \\frac{h}{2} \\hat{C}_k^2 p^{4k} \\left(\\frac{2}{h} \\|v_h\\|_{L^2(K)}^2\\right) = \\hat{C}_k^2 \\frac{2^{2k}}{h^{2k}} p^{4k} \\|v_h\\|_{L^2(K)}^2 $$\nTaking the square root of both sides, we obtain the final form of the inverse inequality:\n$$ \\|D^k v_h\\|_{L^2(K)} \\le \\left(\\hat{C}_k 2^k\\right) p^{2k} h^{-k} \\|v_h\\|_{L^2(K)} $$\nThis expression bounds the norm of the $k$-th derivative by a constant times powers of $p$ and $h$ and the norm of the function itself, as requested.\n\nThe problem states that the stability amplification of the post-processor $\\mathcal{P}_k$ is bounded by an expression of the form $\\alpha \\, C_{\\mathrm{inv}} \\, p^{2k} \\, h^{-k}$. The post-processor $\\mathcal{P}_k$ uses derivatives up to order $k$. The stability of such an operator is fundamentally linked to the amplification of the input function's norm. Since the operator involves derivatives, its stability properties must depend on the norms of these derivatives. The derived inverse inequality shows that the relative size of the $k$-th derivative, i.e., the ratio $\\|D^k v_h\\|_{L^2(K)}/\\|v_h\\|_{L^2(K)}$, is bounded by a term proportional to $p^{2k}h^{-k}$. It is therefore entirely consistent that a measure of stability amplification for an operator involving $D^k$ would be governed by this bound. The constants $\\alpha$ and $C_{\\mathrm{inv}}$ encapsulate the particulars of the operator's construction and the constants from the underlying inequalities.\n\nFinally, we must find the largest integer $k$ that optimizes the $L^{\\infty}$-error improvement, which is given to be monotonic in $k$. This requires finding the largest integer $k$ that satisfies the stability constraint $\\alpha \\, C_{\\mathrm{inv}} \\, p^{2k} \\, h^{-k} \\le 1$ and the problem-space constraint $0 \\le k \\le p$.\nWe are given the numerical values $p=8$, $h=20$, $\\alpha=0.25$, and $C_{\\mathrm{inv}}=0.125$.\nSubstituting these values into the stability inequality yields:\n$$ (0.25) \\cdot (0.125) \\cdot (8)^{2k} \\cdot (20)^{-k} \\le 1 $$\nWe can rewrite the constants as fractions: $\\alpha = \\frac{1}{4}$ and $C_{\\mathrm{inv}} = \\frac{1}{8}$.\n$$ \\frac{1}{4} \\cdot \\frac{1}{8} \\cdot (8^2)^k \\cdot \\frac{1}{20^k} \\le 1 $$\n$$ \\frac{1}{32} \\cdot \\left(\\frac{64}{20}\\right)^k \\le 1 $$\nSimplifying the base of the power:\n$$ \\left(\\frac{64}{20}\\right)^k = \\left(\\frac{16}{5}\\right)^k = (3.2)^k $$\nSo the inequality becomes:\n$$ (3.2)^k \\le 32 $$\nWe must find the largest integer $k$ satisfying this inequality, subject to $k \\le p=8$. We can test integer values for $k$:\nFor $k=0$: $(3.2)^0 = 1 \\le 32$. The condition is satisfied.\nFor $k=1$: $(3.2)^1 = 3.2 \\le 32$. The condition is satisfied.\nFor $k=2$: $(3.2)^2 = 10.24 \\le 32$. The condition is satisfied.\nFor $k=3$: $(3.2)^3 = 10.24 \\times 3.2 = 32.768$. Since $32.768  32$, the condition is violated.\nAs $(3.2)^k$ is a strictly increasing function for $k0$, any integer $k \\ge 3$ will also violate the inequality.\nThe largest integer value of $k$ that satisfies the stability constraint is $2$. This also satisfies $k \\le 8$. Since a larger $k$ gives better $L^\\infty$ improvement, we select the largest permissible integer value.\nThus, the optimal choice for $k$ is $2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "This exercise explores the deeper structure of inverse inequalities by connecting them to the spectral theory of differential operators. You will determine an optimal inverse constant by reformulating the problem as finding the largest eigenvalue of a Sturm-Liouville operator on a specific high-pass polynomial subspace. This perspective is crucial for designing advanced numerical techniques like spectral filters, as you will explore in the second part of the problem. ",
            "id": "3392892",
            "problem": "Let $I=[-1,1]$ and let $\\mathbb{P}_p$ denote the space of all real polynomials on $I$ of degree at most $p$. Equip $\\mathbb{P}_p$ with the standard $L^2$ inner product $(u,v)=\\int_{-1}^{1} u(x)\\,v(x)\\,dx$. Let $\\{P_k\\}_{k\\ge 0}$ be the Legendre polynomials, characterized as the unique polynomial solutions of the second-order Sturm–Liouville problem $-\\frac{d}{dx}\\big((1-x^2)\\,P_k'(x)\\big)=k(k+1)\\,P_k(x)$ on $I$ with $P_k(1)=1$ for each nonnegative integer $k$, and known to be mutually orthogonal in $L^2(I)$.\n\nDefine the high-pass subspace $\\mathcal{H}_{p,m}=\\{v\\in\\mathbb{P}_p:\\,(v,q)=0\\text{ for all }q\\in\\mathbb{P}_m\\}$, where $m$ is a fixed integer with $0\\le mp$. Consider the bilinear forms $a(v,w)=\\int_{-1}^{1}(1-x^2)\\,v'(x)\\,w'(x)\\,dx$ and $b(v,w)=\\int_{-1}^{1}v(x)\\,w(x)\\,dx$ on $\\mathbb{P}_p$. The optimal inverse constant on $\\mathcal{H}_{p,m}$ associated with the weighted gradient is defined by\n$$\nC_{p,m}=\\sup_{v\\in\\mathcal{H}_{p,m}\\setminus\\{0\\}}\\frac{\\big(a(v,v)\\big)^{1/2}}{\\big(b(v,v)\\big)^{1/2}}.\n$$\nStarting from fundamental properties of the Legendre polynomials and the above bilinear forms, determine a closed-form analytic expression for $C_{p,m}$ in terms of $p$ and $m$.\n\nNext, consider a Large Eddy Simulation (LES)-like polynomial filter defined spectrally by solving for $u\\in\\mathbb{P}_p$ the variational problem\n$$\nb(u,w)+\\kappa^{-2}\\,a(u,w)=b(v,w)\\quad\\text{for all }w\\in\\mathbb{P}_p,\n$$\nwhere $\\kappa0$ is a tunable parameter and $v\\in\\mathbb{P}_p$ is the input polynomial. This filter acts diagonally on the Legendre basis due to the Sturm–Liouville structure of the operator $-\\frac{d}{dx}\\big((1-x^2)\\frac{d}{dx}\\big)$. Choose $\\kappa$ so that the filter’s amplitude gain on the lowest high-pass mode of degree $k=m+1$ equals $1/2$. Derive a closed-form analytic expression for this choice of $\\kappa$ in terms of $m$.\n\nYour final answer should consist of two expressions, written in a single row, in the order $C_{p,m}$ and $\\kappa$. No numerical approximation is required.",
            "solution": "The problem as stated is mathematically well-posed, scientifically grounded in the theory of numerical analysis and orthogonal polynomials, and contains all necessary information for a unique solution. We can therefore proceed with the derivation.\n\nThe problem is divided into two parts. First, we determine the optimal inverse constant $C_{p,m}$. Second, we derive an expression for the filter parameter $\\kappa$.\n\nPart 1: Determination of the inverse constant $C_{p,m}$.\n\nThe constant $C_{p,m}$ is defined as the supremum of a Rayleigh quotient:\n$$\nC_{p,m}^2 = \\sup_{v\\in\\mathcal{H}_{p,m}\\setminus\\{0\\}} \\frac{a(v,v)}{b(v,v)} = \\sup_{v\\in\\mathcal{H}_{p,m}\\setminus\\{0\\}} \\frac{\\int_{-1}^{1}(1-x^2)\\,(v'(x))^2\\,dx}{\\int_{-1}^{1}(v(x))^2\\,dx}\n$$\nThe space $\\mathcal{H}_{p,m}$ consists of polynomials in $\\mathbb{P}_p$ that are $L^2$-orthogonal to all polynomials in $\\mathbb{P}_m$. The Legendre polynomials $\\{P_k\\}_{k=0}^p$ form an orthogonal basis for $\\mathbb{P}_p$. The subspace $\\mathbb{P}_m$ is spanned by $\\{P_k\\}_{k=0}^m$. Therefore, the orthogonality condition $(v,q)=0$ for all $q\\in\\mathbb{P}_m$ implies that any $v \\in \\mathcal{H}_{p,m}$ can be expressed as a linear combination of Legendre polynomials of degrees higher than $m$:\n$$\nv(x) = \\sum_{k=m+1}^{p} c_k P_k(x)\n$$\nfor some real coefficients $c_k$.\n\nWe now express the numerator and denominator of the Rayleigh quotient in terms of these coefficients. The denominator is $b(v,v) = (v,v)$. Using the expansion of $v$ and the orthogonality of the Legendre polynomials, $(P_k, P_j) = \\delta_{kj} \\|P_k\\|_{L^2}^2$, we have:\n$$\nb(v,v) = \\left(\\sum_{k=m+1}^{p} c_k P_k, \\sum_{j=m+1}^{p} c_j P_j\\right) = \\sum_{k=m+1}^{p} \\sum_{j=m+1}^{p} c_k c_j (P_k, P_j) = \\sum_{k=m+1}^{p} c_k^2 \\|P_k\\|_{L^2}^2\n$$\nThe squared $L^2$-norm of the $k$-th Legendre polynomial is a standard result: $\\|P_k\\|_{L^2}^2 = \\int_{-1}^{1} P_k(x)^2 dx = \\frac{2}{2k+1}$. Thus,\n$$\nb(v,v) = \\sum_{k=m+1}^{p} c_k^2 \\frac{2}{2k+1}\n$$\nFor the numerator, $a(v,v) = \\int_{-1}^{1}(1-x^2)\\,(v'(x))^2\\,dx$, we analyze the bilinear form $a(P_k, P_j)$. Using integration by parts:\n$$\na(P_k, P_j) = \\int_{-1}^{1} (1-x^2) P_k'(x) P_j'(x) dx = \\left[(1-x^2)P_k'(x) P_j(x)\\right]_{-1}^{1} - \\int_{-1}^{1} P_j(x) \\frac{d}{dx}\\left((1-x^2)P_k'(x)\\right) dx\n$$\nThe boundary term vanishes since $(1-x^2)=0$ at $x=\\pm 1$. We are left with the integral term. By substituting the Sturm-Liouville equation for $P_k$, $-\\frac{d}{dx}\\big((1-x^2)P_k'(x)\\big)=k(k+1)P_k(x)$, we get:\n$$\na(P_k, P_j) = \\int_{-1}^{1} P_j(x) \\left(-\\frac{d}{dx}\\big((1-x^2)P_k'(x)\\big)\\right) dx = \\int_{-1}^{1} P_j(x) [k(k+1)P_k(x)] dx = k(k+1) (P_k, P_j)\n$$\nThis shows that the Legendre polynomials are also orthogonal with respect to the inner product $a(\\cdot,\\cdot)$. Now we can compute $a(v,v)$:\n$$\na(v,v) = a\\left(\\sum_{k=m+1}^{p} c_k P_k, \\sum_{j=m+1}^{p} c_j P_j\\right) = \\sum_{k,j=m+1}^{p} c_k c_j a(P_k, P_j) = \\sum_{k=m+1}^{p} c_k^2 a(P_k, P_k)\n$$\nUsing the relation $a(P_k,P_k) = k(k+1)(P_k,P_k) = k(k+1)\\|P_k\\|_{L^2}^2$:\n$$\na(v,v) = \\sum_{k=m+1}^{p} c_k^2 k(k+1) \\|P_k\\|_{L^2}^2 = \\sum_{k=m+1}^{p} c_k^2 k(k+1) \\frac{2}{2k+1}\n$$\nThe Rayleigh quotient for $C_{p,m}^2$ becomes:\n$$\n\\frac{a(v,v)}{b(v,v)} = \\frac{\\sum_{k=m+1}^{p} c_k^2 k(k+1) \\frac{2}{2k+1}}{\\sum_{k=m+1}^{p} c_k^2 \\frac{2}{2k+1}}\n$$\nThis is a weighted average of the values $\\lambda_k = k(k+1)$ for $k \\in \\{m+1, \\dots, p\\}$, with non-negative weights $w_k = c_k^2 \\frac{2}{2k+1}$. The supremum of such a weighted average is the maximum of the values being averaged. The function $f(k) = k(k+1)$ is strictly increasing for $k \\ge 0$. Therefore, the maximum value of $\\lambda_k$ for $k$ in the range from $m+1$ to $p$ occurs at $k=p$.\n$$\nC_{p,m}^2 = \\sup_{v\\in\\mathcal{H}_{p,m}\\setminus\\{0\\}} \\frac{a(v,v)}{b(v,v)} = \\max_{k \\in \\{m+1, \\dots, p\\}} k(k+1) = p(p+1)\n$$\nTaking the square root gives the final expression for the inverse constant:\n$$\nC_{p,m} = \\sqrt{p(p+1)}\n$$\n\nPart 2: Determination of the filter parameter $\\kappa$.\n\nThe filter is defined by the variational problem: for a given $v \\in \\mathbb{P}_p$, find $u \\in \\mathbb{P}_p$ such that\n$$\nb(u,w) + \\kappa^{-2}a(u,w) = b(v,w) \\quad \\text{for all } w \\in \\mathbb{P}_p\n$$\nThe problem states that this filter acts diagonally on the Legendre basis. Let's verify this and find the gain. Let the input be a single mode $v = P_k$ for some $k \\in \\{0, \\dots, p\\}$. We seek a solution of the form $u = g_k P_k$, where $g_k$ is the amplitude gain for that mode.\nSubstituting $v=P_k$ and $u=g_k P_k$ into the variational form:\n$$\nb(g_k P_k, w) + \\kappa^{-2}a(g_k P_k, w) = b(P_k, w)\n$$\n$$\ng_k b(P_k, w) + g_k \\kappa^{-2}a(P_k, w) = b(P_k, w)\n$$\nThis must hold for all test functions $w \\in \\mathbb{P}_p$. It is sufficient to test against the basis functions $w=P_j$ for $j \\in \\{0, \\dots, p\\}$.\n$$\ng_k (P_k, P_j) + g_k \\kappa^{-2} a(P_k, P_j) = (P_k, P_j)\n$$\nUsing the identities $(P_k, P_j) = \\delta_{kj} \\|P_k\\|_{L^2}^2$ and $a(P_k, P_j) = k(k+1) \\delta_{kj} \\|P_k\\|_{L^2}^2$:\n$$\ng_k \\delta_{kj} \\|P_k\\|_{L^2}^2 + g_k \\kappa^{-2} k(k+1) \\delta_{kj} \\|P_k\\|_{L^2}^2 = \\delta_{kj} \\|P_k\\|_{L^2}^2\n$$\nIf $j \\neq k$, the equation is trivially satisfied as $0=0$. For $j=k$, we have:\n$$\ng_k \\|P_k\\|_{L^2}^2 + g_k \\kappa^{-2} k(k+1) \\|P_k\\|_{L^2}^2 = \\|P_k\\|_{L^2}^2\n$$\nSince $\\|P_k\\|_{L^2}^2 \\neq 0$, we can divide by it:\n$$\ng_k (1 + \\kappa^{-2} k(k+1)) = 1\n$$\nThe amplitude gain for the $k$-th mode is:\n$$\ng_k = \\frac{1}{1 + \\kappa^{-2} k(k+1)}\n$$\nThe problem requires setting $\\kappa$ such that the gain on the lowest high-pass mode, which is $P_{m+1}$ (of degree $k=m+1$), is equal to $1/2$.\nSo we set $g_{m+1} = \\frac{1}{2}$:\n$$\n\\frac{1}{1 + \\kappa^{-2} (m+1)((m+1)+1)} = \\frac{1}{2}\n$$\n$$\n1 + \\kappa^{-2} (m+1)(m+2) = 2\n$$\n$$\n\\kappa^{-2} (m+1)(m+2) = 1\n$$\n$$\n\\kappa^2 = (m+1)(m+2)\n$$\nSince $\\kappa0$, we take the positive square root:\n$$\n\\kappa = \\sqrt{(m+1)(m+2)}\n$$\n\nThe two derived expressions are $C_{p,m} = \\sqrt{p(p+1)}$ and $\\kappa = \\sqrt{(m+1)(m+2)}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{p(p+1)}  \\sqrt{(m+1)(m+2)} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Theory meets computation in this final practice problem, where you will implement a numerical method to calculate sharp inverse constants. By formulating the problem as a generalized eigenvalue problem, you will write a program to compute the constant for various Jacobi polynomial weight functions. This exercise not only demystifies the origin of these constants but also provides hands-on experience in a common task in numerical analysis: tuning parameters to optimize the stability properties of a scheme. ",
            "id": "3392897",
            "problem": "Consider the reference interval $[-1,1]$ and the weighted polynomial space associated with Jacobi polynomials. Let the weight be $w_{\\alpha,\\beta}(x) = (1-x)^{\\alpha}(1+x)^{\\beta}$ with parameters $\\alpha-1$ and $\\beta-1$. For a non-negative integer polynomial degree $p$, define the finite-dimensional polynomial space $\\mathcal{P}_p$ of all real polynomials of degree at most $p$ on $[-1,1]$. For a function $u \\in \\mathcal{P}_p$, define the weighted norm\n$$\n\\|u\\|_{L^2(w_{\\alpha,\\beta})} = \\left( \\int_{-1}^{1} u(x)^2 w_{\\alpha,\\beta}(x)\\, dx \\right)^{1/2},\n$$\nand similarly for the derivative norm $\\|u'\\|_{L^2(w_{\\alpha,\\beta})}$.\n\nIn spectral methods and Discontinuous Galerkin (DG) methods, an inverse inequality for polynomial spaces asserts the existence of a constant $C(\\alpha,\\beta,p)$ such that for all $u \\in \\mathcal{P}_p$,\n$$\n\\|u'\\|_{L^2(w_{\\alpha,\\beta})} \\le C(\\alpha,\\beta,p)\\, p^2 \\, \\|u\\|_{L^2(w_{\\alpha,\\beta})}.\n$$\nFor fixed $\\alpha$, $\\beta$, and $p$, the sharp (best) constant $C(\\alpha,\\beta,p)$ is defined as the smallest value for which this inequality holds for all non-zero $u \\in \\mathcal{P}_p$.\n\nYour task is to write a complete program that, starting from these definitions, computes $C(\\alpha,\\beta,p)$ by evaluating the exact inner products induced by $w_{\\alpha,\\beta}$ over $\\mathcal{P}_p$, and then searches over prescribed grids of $(\\alpha,\\beta)$ to identify the pair that minimizes $C(\\alpha,\\beta,p)$ for each test case. The exploration targets regimes relevant to boundary-layer approximations near singular boundaries, that is, choices of $\\alpha$ or $\\beta$ in $(-1,0)$, where $w_{\\alpha,\\beta}$ has integrable endpoint singularities. All inner products must be computed exactly for polynomials via Gaussian quadrature that is exact for the weighted setting $w_{\\alpha,\\beta}$.\n\nUse the following test suite, where each case specifies the degree $p$, the search ranges for $\\alpha$ and $\\beta$, and the uniform step for the grid:\n\n- Case $1$ (happy path, symmetric weights): $p=8$, $\\alpha \\in [-0.5,0.5]$ with step $0.25$, $\\beta \\in [-0.5,0.5]$ with step $0.25$.\n- Case $2$ (left endpoint singular emphasis): $p=8$, $\\alpha \\in [0.0,1.0]$ with step $0.25$, $\\beta \\in [-0.9,-0.1]$ with step $0.2$.\n- Case $3$ (right endpoint singular emphasis): $p=8$, $\\alpha \\in [-0.9,-0.1]$ with step $0.2$, $\\beta \\in [0.0,1.0]$ with step $0.25$.\n- Case $4$ (low-degree boundary case): $p=2$, $\\alpha \\in [-0.9,0.9]$ with step $0.6$, $\\beta \\in [-0.9,0.9]$ with step $0.6$.\n- Case $5$ (higher-degree, moderate weights): $p=14$, $\\alpha \\in [-0.5,0.5]$ with step $0.5$, $\\beta \\in [-0.5,0.5]$ with step $0.5$.\n\nFor each test case, search the specified grid of $(\\alpha,\\beta)$ pairs and compute the corresponding $C(\\alpha,\\beta,p)$ exactly in the sense described above. Among all grid points, identify $(\\alpha^\\star,\\beta^\\star)$ that minimizes $C(\\alpha,\\beta,p)$ for that case.\n\nFinal Output Format:\n- Your program should produce a single line of output containing a list of results, one per test case, in order from Case $1$ to Case $5$.\n- Each test case result must be a list of three floating-point numbers: $[\\alpha^\\star,\\beta^\\star,C_{\\min}]$, where $C_{\\min} = C(\\alpha^\\star,\\beta^\\star,p)$.\n- All floating-point numbers must be rounded to $6$ decimal places.\n- The single line must be formatted as a comma-separated list enclosed in square brackets, for example: $[[\\alpha_1^\\star,\\beta_1^\\star,C_1],[\\alpha_2^\\star,\\beta_2^\\star,C_2],\\dots]$.\n\nNo physical units or angles are involved, so none are required. All answers must be pure numbers. Your program must be self-contained, require no input, and follow the exact output format specified above.",
            "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and complete.\n\n### Step 1: Extract Givens\n- **Domain**: Reference interval $I = [-1,1]$.\n- **Weight Function**: $w_{\\alpha,\\beta}(x) = (1-x)^{\\alpha}(1+x)^{\\beta}$ for $\\alpha, \\beta  -1$.\n- **Polynomial Space**: $\\mathcal{P}_p$, the space of real polynomials of degree at most $p$ on $I$.\n- **Weighted Norms**: For $u \\in \\mathcal{P}_p$, $\\|u\\|_{L^2(w_{\\alpha,\\beta})} = \\left( \\int_{-1}^{1} u(x)^2 w_{\\alpha,\\beta}(x)\\, dx \\right)^{1/2}$ and $\\|u'\\|_{L^2(w_{\\alpha,\\beta})} = \\left( \\int_{-1}^{1} (u'(x))^2 w_{\\alpha,\\beta}(x)\\, dx \\right)^{1/2}$.\n- **Inverse Inequality**: $\\|u'\\|_{L^2(w_{\\alpha,\\beta})} \\le C(\\alpha,\\beta,p)\\, p^2 \\, \\|u\\|_{L^2(w_{\\alpha,\\beta})}$.\n- **Objective**: Compute the sharp constant $C(\\alpha,\\beta,p)$ and find the pair $(\\alpha, \\beta)$ from a specified grid that minimizes this constant for given values of $p$.\n- **Computational Method**: Inner products must be computed exactly using Gauss-Jacobi quadrature.\n- **Test Cases**:\n    - Case $1$: $p=8$, $\\alpha \\in [-0.5,0.5]$ (step $0.25$), $\\beta \\in [-0.5,0.5]$ (step $0.25$).\n    - Case $2$: $p=8$, $\\alpha \\in [0.0,1.0]$ (step $0.25$), $\\beta \\in [-0.9,-0.1]$ (step $0.2$).\n    - Case $3$: $p=8$, $\\alpha \\in [-0.9,-0.1]$ (step $0.2$), $\\beta \\in [0.0,1.0]$ (step $0.25$).\n    - Case $4$: $p=2$, $\\alpha \\in [-0.9,0.9]$ (step $0.6$), $\\beta \\in [-0.9,0.9]$ (step $0.6$).\n    - Case $5$: $p=14$, $\\alpha \\in [-0.5,0.5]$ (step $0.5$), $\\beta \\in [-0.5,0.5]$ (step $0.5$).\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the mathematical field of numerical analysis and approximation theory. Inverse inequalities for polynomial spaces are a standard and important topic in the analysis of spectral and discontinuous Galerkin methods. The formulation of the sharp constant as a supremum is mathematically rigorous. The use of Jacobi polynomials, weighted norms, and Gaussian quadrature are all standard techniques. The parameters for all test cases satisfy the condition $\\alpha, \\beta  -1$. The polynomial degrees are moderate, making the problem computationally feasible. The problem is self-contained, objective, and scientifically sound.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be developed.\n\n### Principle-Based Solution\n\nThe core of the problem is to compute the sharp constant $C(\\alpha,\\beta,p)$ for the given inverse inequality. The sharp constant is defined as the maximum of the ratio of the norms over all non-zero polynomials in the space $\\mathcal{P}_p$:\n$$\nC(\\alpha,\\beta,p) = \\frac{1}{p^2} \\sup_{u \\in \\mathcal{P}_p, u \\ne 0} \\frac{\\|u'\\|_{L^2(w_{\\alpha,\\beta})}}{\\|u\\|_{L^2(w_{\\alpha,\\beta})}}\n$$\nTo find this supremum, it is convenient to work with the square of this expression, which leads to maximizing a Rayleigh quotient:\n$$\n(p^2 C(\\alpha,\\beta,p))^2 = \\sup_{u \\in \\mathcal{P}_p, u \\ne 0} \\frac{\\|u'\\|_{L^2(w_{\\alpha,\\beta})}^2}{\\|u\\|_{L^2(w_{\\alpha,\\beta})}^2} = \\sup_{u \\in \\mathcal{P}_p, u \\ne 0} \\frac{\\int_{-1}^{1} (u'(x))^2 w_{\\alpha,\\beta}(x)\\, dx}{\\int_{-1}^{1} u(x)^2 w_{\\alpha,\\beta}(x)\\, dx}\n$$\nTo transform this into a computation, we represent the polynomial $u(x)$ in a basis. We choose the monomial basis $\\{\\phi_j(x) = x^j\\}_{j=0}^p$ for the space $\\mathcal{P}_p$, which has dimension $p+1$. Any polynomial $u \\in \\mathcal{P}_p$ can be written as $u(x) = \\sum_{j=0}^{p} c_j \\phi_j(x) = \\sum_{j=0}^{p} c_j x^j$. Its derivative is $u'(x) = \\sum_{j=1}^{p} c_j j x^{j-1}$. Let $\\mathbf{c} = [c_0, c_1, \\dots, c_p]^T$ be the vector of coefficients.\n\nThe squared norms can be expressed as quadratic forms involving the coefficient vector $\\mathbf{c}$:\n$$\n\\|u\\|_{L^2(w_{\\alpha,\\beta})}^2 = \\int_{-1}^{1} \\left(\\sum_{i=0}^{p} c_i x^i\\right) \\left(\\sum_{j=0}^{p} c_j x^j\\right) w_{\\alpha,\\beta}(x)\\, dx = \\sum_{i=0}^{p} \\sum_{j=0}^{p} c_i c_j \\int_{-1}^{1} x^{i+j} w_{\\alpha,\\beta}(x)\\, dx = \\mathbf{c}^T M \\mathbf{c}\n$$\n$$\n\\|u'\\|_{L^2(w_{\\alpha,\\beta})}^2 = \\int_{-1}^{1} \\left(\\sum_{i=1}^{p} c_i i x^{i-1}\\right) \\left(\\sum_{j=1}^{p} c_j j x^{j-1}\\right) w_{\\alpha,\\beta}(x)\\, dx = \\sum_{i=1}^{p} \\sum_{j=1}^{p} c_i c_j (ij) \\int_{-1}^{1} x^{i+j-2} w_{\\alpha,\\beta}(x)\\, dx = \\mathbf{c}^T K \\mathbf{c}\n$$\nThe matrices $M$ and $K$ are the mass and stiffness matrices, respectively, in the monomial basis. Their entries are given by:\n$$\nM_{ij} = \\int_{-1}^{1} x^{i+j} w_{\\alpha,\\beta}(x)\\, dx, \\quad \\text{for } i,j \\in \\{0, \\dots, p\\}\n$$\n$$\nK_{ij} = ij \\int_{-1}^{1} x^{i+j-2} w_{\\alpha,\\beta}(x)\\, dx, \\quad \\text{for } i,j \\in \\{1, \\dots, p\\}, \\text{ and } K_{ij}=0 \\text{ if } i=0 \\text{ or } j=0\n$$\nThe problem of maximizing the ratio of norms is now equivalent to maximizing the Rayleigh quotient for the matrix pencil $(K, M)$:\n$$\n(p^2 C(\\alpha,\\beta,p))^2 = \\sup_{\\mathbf{c} \\ne \\mathbf{0}} \\frac{\\mathbf{c}^T K \\mathbf{c}}{\\mathbf{c}^T M \\mathbf{c}}\n$$\nThis is the definition of the largest eigenvalue, $\\lambda_{\\max}$, of the generalized eigenvalue problem $K\\mathbf{c} = \\lambda M\\mathbf{c}$. The matrices $M$ and $K$ are real and symmetric, and $M$ is positive definite for non-zero polynomials.\n\nThe problem states that the integrals in the matrix entries must be computed exactly. This is achieved using Gauss-Jacobi quadrature. An $N_q$-point Gauss-Jacobi quadrature rule for the weight $w_{\\alpha,\\beta}(x)$ is exact for any polynomial integrand of degree up to $2N_q-1$.\n- To compute entries of $M$, the integrand is $x^{i+j}$, a polynomial of degree at most $2p$. We require $2N_q - 1 \\ge 2p$, which implies $N_q \\ge p+1/2$. Thus, $N_q = p+1$ points are sufficient.\n- To compute entries of $K$, the integrand is $x^{i+j-2}$, a polynomial of degree at most $2p-2$. We require $2N_q - 1 \\ge 2p-2$, which implies $N_q \\ge p-1/2$. Thus, $N_q=p$ points are sufficient.\nBy choosing $N_q = p+1$, we ensure exactness for both matrices. Let $\\{x_k, w_k\\}_{k=1}^{N_q}$ be the Gauss-Jacobi quadrature nodes and weights for a given $(\\alpha, \\beta)$. The matrix entries are computed as:\n$$\nM_{ij} \\approx \\sum_{k=1}^{N_q} x_k^{i+j} w_k \\quad \\text{and} \\quad K_{ij} \\approx ij \\sum_{k=1}^{N_q} x_k^{i+j-2} w_k\n$$\nThese summations are not approximations but exact evaluations due to the properties of Gaussian quadrature.\n\nOnce $M$ and $K$ are constructed, the largest eigenvalue $\\lambda_{\\max}$ is found numerically. The sharp constant is then calculated as:\n$$\nC(\\alpha,\\beta,p) = \\frac{\\sqrt{\\lambda_{\\max}}}{p^2}\n$$\nThis procedure is repeated for every pair $(\\alpha, \\beta)$ in the prescribed grid for each test case. The pair that yields the minimum value of $C(\\alpha,\\beta,p)$ is identified and reported along with the minimum constant. This systematic search constitutes the solution to the problem.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import roots_jacobi\nfrom scipy.linalg import eigh\n\ndef compute_C(p, alpha, beta):\n    \"\"\"\n    Computes the sharp inverse inequality constant C(alpha, beta, p).\n    \"\"\"\n    if p == 0:\n        return 0.0\n\n    dim = p + 1\n    # Number of quadrature points for exact integration.\n    # Nq-point Gauss-Jacobi quadrature is exact for polynomials of degree up to 2*Nq - 1.\n    # For the mass matrix M, max degree is p+p=2p. Need 2*Nq-1 = 2p = Nq = p + 0.5.\n    # So, Nq = p + 1.\n    Nq = p + 1\n\n    # Get Gauss-Jacobi quadrature nodes and weights.\n    nodes, weights = roots_jacobi(Nq, alpha, beta)\n\n    # Build the mass matrix M and stiffness matrix K using the monomial basis {x^j}.\n    # A more efficient way to build M and K is using Vandermonde-like matrices.\n    \n    # Vandermonde matrix for basis functions {x^j} at quadrature nodes.\n    # V[k, j] = nodes[k]**j\n    V = np.vander(nodes, N=dim, increasing=True)\n    \n    # Vandermonde matrix for derivatives of basis functions {j*x^(j-1)} at nodes.\n    V_prime = np.zeros((Nq, dim))\n    for j in range(1, dim):\n        V_prime[:, j] = j * nodes**(j - 1)\n\n    # M_ij = sum_k w_k * (x_k^i) * (x_k^j)\n    # M = V.T @ diag(w) @ V\n    M = V.T @ (weights[:, np.newaxis] * V)\n\n    # K_ij = sum_k w_k * (i*x_k^(i-1)) * (j*x_k^(j-1))\n    # K = V_prime.T @ diag(w) @ V_prime\n    K = V_prime.T @ (weights[:, np.newaxis] * V_prime)\n\n    # Solve the generalized eigenvalue problem K*c = lambda*M*c.\n    # We need the largest eigenvalue.\n    # eigh is for symmetric/Hermitian matrices. M, K are real symmetric.\n    try:\n        eigenvalues = eigh(K, M, eigvals_only=True)\n        lambda_max = np.max(eigenvalues)\n    except np.linalg.LinAlgError:\n        # This may happen if M is singular, though unlikely for valid alpha, beta\n        return float('inf')\n\n    # The constant C is sqrt(lambda_max) / p^2\n    # Ensure lambda_max is non-negative before taking the square root\n    if lambda_max  0:\n        # This can happen due to floating point errors, should be close to zero.\n        lambda_max = 0\n\n    C = np.sqrt(lambda_max) / (p**2)\n    return C\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and find the minimum C.\n    \"\"\"\n    test_cases = [\n        # p, (alpha_start, alpha_end, alpha_step), (beta_start, beta_end, beta_step)\n        (8, (-0.5, 0.5, 0.25), (-0.5, 0.5, 0.25)),  # Case 1\n        (8, (0.0, 1.0, 0.25), (-0.9, -0.1, 0.2)),   # Case 2\n        (8, (-0.9, -0.1, 0.2), (0.0, 1.0, 0.25)),   # Case 3\n        (2, (-0.9, 0.9, 0.6), (-0.9, 0.9, 0.6)),   # Case 4\n        (14, (-0.5, 0.5, 0.5), (-0.5, 0.5, 0.5)),   # Case 5\n    ]\n\n    all_results = []\n\n    for p, alpha_params, beta_params in test_cases:\n        a_start, a_end, a_step = alpha_params\n        b_start, b_end, b_step = beta_params\n        \n        # Use np.linspace for robust grid generation with floating point steps.\n        num_a = int(round((a_end - a_start) / a_step)) + 1\n        alpha_grid = np.linspace(a_start, a_end, num_a)\n\n        num_b = int(round((b_end - b_start) / b_step)) + 1\n        beta_grid = np.linspace(b_start, b_end, num_b)\n\n        min_C = float('inf')\n        best_alpha = None\n        best_beta = None\n\n        for alpha in alpha_grid:\n            for beta in beta_grid:\n                C = compute_C(p, alpha, beta)\n                if C  min_C:\n                    min_C = C\n                    best_alpha = alpha\n                    best_beta = beta\n        \n        all_results.append([best_alpha, best_beta, min_C])\n    \n    # Format the final output string as specified.\n    formatted_results = []\n    for res in all_results:\n        alpha_star, beta_star, c_min = res\n        formatted_results.append(f\"[{alpha_star:.6f},{beta_star:.6f},{c_min:.6f}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}