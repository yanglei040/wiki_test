{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with the core principle of Hybridizable Discontinuous Galerkin (HDG) post-processing. This exercise explores the mechanism in its simplest form: a single element with a low-order polynomial approximation. By working through this analytical example , you will directly verify the remarkable \"superconvergence\" property, where the post-processed solution $u_h^{\\star}$ can achieve a higher degree of accuracy and, in this idealized case, becomes identical to the exact solution.",
            "id": "3410138",
            "problem": "Consider the steady diffusion model problem on a single convex polygonal element $K = [0,1] \\times [0,1]$ with constant diffusivity, governed by the partial differential equation $-\\Delta u = f$ and Dirichlet boundary condition $u|_{\\partial K} = g$. Let the exact (manufactured) solution be $u(x,y) = x^2 + y^2$ on $K$. Then $f(x,y) = -\\Delta u(x,y)$ and $g(x,y) = u(x,y)$ on $\\partial K$.\n\nLet $\\mathcal{P}_{k}(K)$ denote the space of polynomials of total degree at most $k$ on $K$, and let the Hybridizable Discontinuous Galerkin (HDG) method be posed with local spaces $\\boldsymbol{q}_{h} \\in [\\mathcal{P}_{k}(K)]^{2}$, $u_{h} \\in \\mathcal{P}_{k}(K)$, and numerical trace $\\widehat{u}_{h}$ on $\\partial K$ (single element setting). Assume $k=1$ and that the computed HDG flux $\\boldsymbol{q}_{h}$ equals the $L^2(K)$-projection of the exact flux $\\nabla u$ into $[\\mathcal{P}_{1}(K)]^{2}$. Since $\\nabla u(x,y) = (2x, 2y)$ already belongs to $[\\mathcal{P}_{1}(K)]^{2}$, this assumption implies $\\boldsymbol{q}_h = \\nabla u$ on $K$.\n\nDefine the standard HDG post-processing $u_{h}^{\\star} \\in \\mathcal{P}_{k+1}(K)$ as the unique polynomial satisfying, for all $w \\in \\mathcal{P}_{k+1}(K)$,\n$$(\\nabla u_{h}^{\\star}, \\nabla w)_{K} = (\\boldsymbol{q}_{h}, \\nabla w)_{K},$$\ntogether with the mean-value constraint \n$$\\int_{K} u_{h}^{\\star} \\,\\mathrm{d}\\boldsymbol{x} = \\int_{K} u \\,\\mathrm{d}\\boldsymbol{x}.$$\nHere, $(\\cdot,\\cdot)_{K}$ denotes the $L^2(K)$ inner product, and $\\mathrm{d}\\boldsymbol{x}$ is the area measure on $K$.\n\nStarting from the above definitions and the manufactured data, derive $u_{h}^{\\star}(x,y)$ explicitly and verify the achieved degree and exactness properties implied by the post-processing on this single-element problem. Your final answer must be a single closed-form analytical expression in terms of $x$ and $y$ for $u_{h}^{\\star}(x,y)$. No numerical rounding is required.",
            "solution": "The problem is to derive the explicit form of the post-processed Hybridizable Discontinuous Galerkin (HDG) solution, denoted by $u_h^\\star(x,y)$, on a single square element $K = [0,1] \\times [0,1]$. The polynomial degree for the local HDG spaces is $k=1$.\n\nThe defining properties of the post-processed solution $u_h^\\star \\in \\mathcal{P}_{k+1}(K) = \\mathcal{P}_{2}(K)$ are given by two conditions:\n1.  A variational equation:\n    $$(\\nabla u_{h}^{\\star}, \\nabla w)_{K} = (\\boldsymbol{q}_{h}, \\nabla w)_{K}, \\quad \\forall w \\in \\mathcal{P}_{2}(K)$$\n2.  A mean-value constraint:\n    $$\\int_{K} u_{h}^{\\star} \\,\\mathrm{d}\\boldsymbol{x} = \\int_{K} u \\,\\mathrm{d}\\boldsymbol{x}$$\nHere, $(\\cdot, \\cdot)_{K}$ denotes the standard $L^2$ inner product on the domain $K$.\n\nFirst, we must determine the HDG flux approximation $\\boldsymbol{q}_h$. The problem states that $\\boldsymbol{q}_h$ is the $L^2(K)$-projection of the exact flux $\\nabla u$ onto the polynomial space $[\\mathcal{P}_{k}(K)]^2$, which with $k=1$ is $[\\mathcal{P}_{1}(K)]^2$.\n\nThe exact solution is given as $u(x,y) = x^2 + y^2$.\nThe exact flux is the gradient of $u$:\n$$\\nabla u(x,y) = \\left( \\frac{\\partial u}{\\partial x}, \\frac{\\partial u}{\\partial y} \\right) = (2x, 2y)$$\nEach component of the vector field $\\nabla u$ is a polynomial of degree $1$. Therefore, the exact flux $\\nabla u$ is already an element of the target space for the projection, $[\\mathcal{P}_{1}(K)]^2$.\nThe $L^2$-projection of an element onto a space that already contains that element is simply the element itself.\nThus, we have $\\boldsymbol{q}_h = \\nabla u = (2x, 2y)$.\n\nNow, we substitute this result for $\\boldsymbol{q}_h$ into the variational equation defining $u_h^\\star$:\n$$(\\nabla u_{h}^{\\star}, \\nabla w)_{K} = (\\nabla u, \\nabla w)_{K}, \\quad \\forall w \\in \\mathcal{P}_{2}(K)$$\nBy the linearity of the inner product, this equation can be rewritten as:\n$$(\\nabla u_{h}^{\\star} - \\nabla u, \\nabla w)_{K} = 0$$\nwhich simplifies to:\n$$(\\nabla (u_{h}^{\\star} - u), \\nabla w)_{K} = 0, \\quad \\forall w \\in \\mathcal{P}_{2}(K)$$\nLet us define the error of the post-processed solution as $e_u^\\star = u_h^\\star - u$. The equation becomes:\n$$(\\nabla e_u^\\star, \\nabla w)_{K} = 0, \\quad \\forall w \\in \\mathcal{P}_{2}(K)$$\nWe know that the exact solution $u(x,y) = x^2+y^2$ is a polynomial of degree $2$, so $u \\in \\mathcal{P}_{2}(K)$. The post-processed solution $u_h^\\star$ is also sought in the space $\\mathcal{P}_{2}(K)$. Consequently, their difference, the error $e_u^\\star$, must also be a polynomial in $\\mathcal{P}_{2}(K)$.\n\nSince $e_u^\\star \\in \\mathcal{P}_{2}(K)$, we are free to choose the test function $w$ to be $e_u^\\star$ itself. Making this substitution, we obtain:\n$$(\\nabla e_u^\\star, \\nabla e_u^\\star)_{K} = 0$$\nThis inner product is the definition of the squared $L^2$-norm of the gradient of $e_u^\\star$:\n$$\\int_{K} |\\nabla e_u^\\star|^2 \\,\\mathrm{d}\\boldsymbol{x} = 0$$\nThe integrand, $|\\nabla e_u^\\star|^2$, is a non-negative continuous function. For its integral over the domain $K$, which has a positive measure (Area$(K)=1$), to be zero, the integrand must be identically zero throughout $K$.\n$$|\\nabla e_u^\\star(\\boldsymbol{x})|^2 = 0 \\quad \\forall \\boldsymbol{x} \\in K$$\nThis implies that the gradient itself must be the zero vector everywhere in $K$:\n$$\\nabla e_u^\\star(\\boldsymbol{x}) = \\boldsymbol{0} \\quad \\forall \\boldsymbol{x} \\in K$$\nA function whose gradient is zero throughout a connected domain must be constant on that domain. Therefore, $e_u^\\star(\\boldsymbol{x}) = C$ for some real constant $C$.\nRecalling the definition of $e_u^\\star$, we have $u_h^\\star(\\boldsymbol{x}) - u(\\boldsymbol{x}) = C$, or $u_h^\\star(\\boldsymbol{x}) = u(\\boldsymbol{x}) + C$.\n\nTo determine the value of the constant $C$, we now apply the second condition, the mean-value constraint:\n$$\\int_{K} u_{h}^{\\star} \\,\\mathrm{d}\\boldsymbol{x} = \\int_{K} u \\,\\mathrm{d}\\boldsymbol{x}$$\nSubstituting $u_h^\\star = u+C$ into this equation yields:\n$$\\int_{K} (u(\\boldsymbol{x}) + C) \\,\\mathrm{d}\\boldsymbol{x} = \\int_{K} u(\\boldsymbol{x}) \\,\\mathrm{d}\\boldsymbol{x}$$\nThe integral on the left side can be split:\n$$\\int_{K} u(\\boldsymbol{x}) \\,\\mathrm{d}\\boldsymbol{x} + \\int_{K} C \\,\\mathrm{d}\\boldsymbol{x} = \\int_{K} u(\\boldsymbol{x}) \\,\\mathrm{d}\\boldsymbol{x}$$\nSubtracting $\\int_K u(\\boldsymbol{x}) d\\boldsymbol{x}$ from both sides leaves:\n$$\\int_{K} C \\,\\mathrm{d}\\boldsymbol{x} = 0$$\n$$C \\int_{K} \\,\\mathrm{d}\\boldsymbol{x} = 0$$\nThe integral $\\int_K d\\boldsymbol{x}$ represents the area of the domain $K = [0,1] \\times [0,1]$, which is $1$.\n$$C \\cdot 1 = 0 \\implies C=0$$\nSince the constant $C$ is zero, the error $e_u^\\star$ is identically zero. Therefore, the post-processed solution is identical to the exact solution:\n$$u_h^\\star(\\boldsymbol{x}) = u(\\boldsymbol{x})$$\nSubstituting the given expression for $u(x,y)$:\n$$u_h^\\star(x,y) = x^2 + y^2$$\n\nThe derived solution $u_h^\\star(x,y) = x^2 + y^2$ is a polynomial of total degree $2$, which is consistent with the requirement that $u_h^\\star \\in \\mathcal{P}_{k+1}(K) = \\mathcal{P}_{2}(K)$. The result $u_h^\\star=u$ demonstrates the exactness property of the HDG post-processing for this specific case. This is a known theoretical result: for a single element, if the exact solution $u$ is a polynomial of degree at most $k+1$ (here, $u \\in \\mathcal{P}_2$ and $k=1$), the post-processed solution $u_h^\\star$ is exact. Our derivation confirms this superconvergence property.",
            "answer": "$$\n\\boxed{x^{2} + y^{2}}\n$$"
        },
        {
            "introduction": "Building on the standard isotropic post-processing, we can tailor the technique for more complex physical phenomena. This problem  introduces an anisotropic, weighted projection designed for advection-diffusion problems, where errors are often not uniform in all directions. Deriving the modified operator will deepen your understanding of the underlying projection mechanics and highlight how post-processing can be adapted to the specific characteristics of the governing partial differential equation.",
            "id": "3410083",
            "problem": "Consider the scalar advection-diffusion model problem on a single element $K \\subset \\mathbb{R}^{2}$ with constant advection field $\\boldsymbol{\\beta} \\in \\mathbb{R}^{2}$ and diffusion coefficient $\\epsilon > 0$. Let the Hybridizable Discontinuous Galerkin (HDG) method produce an element-local approximation of the gradient, denoted $\\boldsymbol{q}_{h}$, of polynomial degree $k$ on $K$. Classical superconvergent post-processing lifts the solution to degree $k+1$ by defining $u^{\\star} \\in \\mathcal{P}_{k+1}(K)$ as the unique mean-zero polynomial satisfying a gradient-matching variational condition against all mean-zero test functions in $\\mathcal{P}_{k+1}(K)$.\n\nIn anisotropic advection-diffusion, consider a modified post-processing operator defined by a weighted projection aligned with $\\boldsymbol{\\beta}$. Introduce the symmetric positive-definite weight matrix\n$$\n\\mathbf{M} \\;=\\; \\mathbf{I} \\;+\\; \\gamma \\,\\frac{\\boldsymbol{\\beta}\\,\\boldsymbol{\\beta}^{\\top}}{|\\boldsymbol{\\beta}|^{2}},\n$$\nwith $\\gamma \\geq 0$, and define $u^{\\star} \\in \\mathcal{P}_{k+1}(K)$ with zero mean to satisfy, for every $v \\in \\mathcal{P}_{k+1}(K)$ with zero mean,\n$$\n\\int_{K} \\big( \\mathbf{M}\\,\\nabla u^{\\star} \\big)\\cdot \\nabla v \\, \\mathrm{d}\\boldsymbol{x} \\;=\\; \\int_{K} \\big( \\mathbf{M}\\,\\boldsymbol{q}_{h} \\big)\\cdot \\nabla v \\, \\mathrm{d}\\boldsymbol{x}.\n$$\nStarting from the definitions of $\\mathcal{P}_{k+1}(K)$, orthogonality, and weighted $L^{2}$ projections, derive the operator above and specialize it on the reference element $K = [0,1]^{2}$ with constant $\\boldsymbol{\\beta} = (1,0)^{\\top}$ so that\n$$\n\\mathbf{M} \\;=\\; \\begin{pmatrix} 1+\\gamma & 0 \\\\ 0 & 1 \\end{pmatrix}.\n$$\nLet $k=1$ so that $u^{\\star} \\in \\mathcal{P}_{2}(K)$ and assume the HDG gradient approximation is the linear field\n$$\n\\boldsymbol{q}_{h}(x,y) \\;=\\; \\begin{pmatrix} \\mu\\,x + \\nu\\,y \\\\ \\rho\\,x + \\sigma\\,y \\end{pmatrix},\n$$\nfor given real coefficients $\\mu, \\nu, \\rho, \\sigma$. Parameterize $u^{\\star}$ by\n$$\nu^{\\star}(x,y) \\;=\\; \\tfrac{1}{2}\\,a\\,x^{2} \\;+\\; b\\,x\\,y \\;+\\; \\tfrac{1}{2}\\,c\\,y^{2} \\;+\\; d\\,x \\;+\\; e\\,y \\;+\\; g,\n$$\nand impose the mean-zero constraint $\\int_{K} u^{\\star}\\,\\mathrm{d}\\boldsymbol{x} = 0$ to fix $g$. Using only fundamental properties of polynomial spaces, weighted projections, and symmetry of mixed partial derivatives, derive the normal equations for the coefficients $a,b,c,d,e$ by minimizing the weighted residual functional\n$$\nJ(a,b,c,d,e) \\;=\\; \\int_{K} \\Big( \\nabla u^{\\star}(x,y) \\;-\\; \\boldsymbol{q}_{h}(x,y) \\Big)^{\\top} \\mathbf{M} \\Big( \\nabla u^{\\star}(x,y) \\;-\\; \\boldsymbol{q}_{h}(x,y) \\Big)\\,\\mathrm{d}\\boldsymbol{x}.\n$$\nSolve these equations explicitly and provide the closed-form analytic expression for the cross-term coefficient $b$ in terms of $\\gamma$, $\\nu$, and $\\rho$. Conclude by hypothesizing, based on your derivation, how increasing $\\gamma$ affects the anisotropy of the post-processing error along and across the direction of $\\boldsymbol{\\beta}$. Your final answer must be the requested expression for $b$ as a function of $\\gamma$, $\\nu$, and $\\rho$.",
            "solution": "The problem asks for the derivation of the normal equations for the coefficients of the post-processed solution $u^{\\star}$ by minimizing the functional $J$, and then to solve for the coefficient $b$. The variational formulation provided is the Euler-Lagrange equation associated with the minimization of the quadratic functional $J$. Minimizing $J$ with respect to its parameters $(a,b,c,d,e)$ is equivalent to finding the stationary point where the partial derivatives of $J$ with respect to each parameter are zero.\n\nFirst, we express the gradient of $u^{\\star}$:\n$$ \\nabla u^{\\star}(x,y) = \\begin{pmatrix} \\partial_x u^{\\star} \\\\ \\partial_y u^{\\star} \\end{pmatrix} = \\begin{pmatrix} a\\,x + b\\,y + d \\\\ b\\,x + c\\,y + e \\end{pmatrix} $$\nThe error vector, $\\boldsymbol{\\delta} = \\nabla u^{\\star} - \\boldsymbol{q}_h$, is:\n$$ \\boldsymbol{\\delta}(x,y) = \\begin{pmatrix} (a-\\mu)x + (b-\\nu)y + d \\\\ (b-\\rho)x + (c-\\sigma)y + e \\end{pmatrix} $$\nThe functional $J$ to be minimized is the integral of the weighted squared norm of this error over the domain $K = [0,1]^2$:\n$$ J = \\int_0^1 \\int_0^1 \\boldsymbol{\\delta}^{\\top} \\mathbf{M} \\boldsymbol{\\delta} \\,\\mathrm{d}x\\,\\mathrm{d}y $$\nSubstituting the expressions for $\\boldsymbol{\\delta}$ and $\\mathbf{M}$:\n$$ J = \\int_K \\left[ (1+\\gamma)\\big((a-\\mu)x + (b-\\nu)y + d\\big)^2 + \\big((b-\\rho)x + (c-\\sigma)y + e\\big)^2 \\right] \\mathrm{d}\\boldsymbol{x} $$\nThe normal equations are obtained by setting the partial derivatives of $J$ with respect to each of the five coefficients $a, b, c, d, e$ to zero. We utilize the identity $\\int_0^1 \\int_0^1 x^i y^j \\,\\mathrm{d}x\\,\\mathrm{d}y = \\frac{1}{(i+1)(j+1)}$.\n\n1.  $\\frac{\\partial J}{\\partial a} = 0$ leads to:\n    $$ a\\frac{1}{3} + b\\frac{1}{4} + d\\frac{1}{2} = \\mu\\frac{1}{3} + \\nu\\frac{1}{4} \\quad (1) $$\n\n2.  $\\frac{\\partial J}{\\partial c} = 0$ leads to:\n    $$ b\\frac{1}{4} + c\\frac{1}{3} + e\\frac{1}{2} = \\rho\\frac{1}{4} + \\sigma\\frac{1}{3} \\quad (2) $$\n\n3.  $\\frac{\\partial J}{\\partial d} = 0$ leads to:\n    $$ a\\frac{1}{2} + b\\frac{1}{2} + d = \\mu\\frac{1}{2} + \\nu\\frac{1}{2} \\quad (3) $$\n\n4.  $\\frac{\\partial J}{\\partial e} = 0$ leads to:\n    $$ b\\frac{1}{2} + c\\frac{1}{2} + e = \\rho\\frac{1}{2} + \\sigma\\frac{1}{2} \\quad (4) $$\nWe can simplify this system. From (3), we solve for $d$ and substitute into (1), which yields $a = \\mu$. Similarly, from (4), we solve for $e$ and substitute into (2), which yields $c = \\sigma$. The post-processing exactly recovers the coefficients of the second-order pure derivative terms from the corresponding terms in the HDG gradient approximation.\n\nNow we derive the equation for $b$ by computing $\\frac{\\partial J}{\\partial b} = 0$:\n$$ \\frac{\\partial J}{\\partial b} = \\int_K \\left[ 2(1+\\gamma)\\big((a-\\mu)x + (b-\\nu)y + d\\big)y + 2\\big((b-\\rho)x + (c-\\sigma)y + e\\big)x \\right] \\mathrm{d}\\boldsymbol{x} = 0 $$\nSubstituting $a=\\mu$ and $c=\\sigma$, the equation simplifies:\n$$ \\int_K \\left[ (1+\\gamma)\\big((b-\\nu)y + d\\big)y + \\big((b-\\rho)x + e\\big)x \\right] \\mathrm{d}\\boldsymbol{x} = 0 $$\n$$ (1+\\gamma)\\int_K \\big((b-\\nu)y^2 + dy\\big)\\mathrm{d}\\boldsymbol{x} + \\int_K \\big((b-\\rho)x^2 + ex\\big)\\mathrm{d}\\boldsymbol{x} = 0 $$\n$$ (1+\\gamma)\\left( (b-\\nu)\\frac{1}{3} + d\\frac{1}{2} \\right) + \\left( (b-\\rho)\\frac{1}{3} + e\\frac{1}{2} \\right) = 0 \\quad (5) $$\nWe have explicit forms for $d$ and $e$ in terms of $b$. With $a=\\mu$, equation (3) gives $d = \\frac{1}{2}(\\nu-b)$. With $c=\\sigma$, equation (4) gives $e = \\frac{1}{2}(\\rho-b)$. Substituting these into (5):\n$$ (1+\\gamma)\\left( (b-\\nu)\\frac{1}{3} + \\frac{1}{2}\\frac{\\nu-b}{2} \\right) + \\left( (b-\\rho)\\frac{1}{3} + \\frac{1}{2}\\frac{\\rho-b}{2} \\right) = 0 $$\n$$ (1+\\gamma)\\left( \\frac{b}{3} - \\frac{\\nu}{3} + \\frac{\\nu}{4} - \\frac{b}{4} \\right) + \\left( \\frac{b}{3} - \\frac{\\rho}{3} + \\frac{\\rho}{4} - \\frac{b}{4} \\right) = 0 $$\nCombine terms inside the parentheses:\n$$ (1+\\gamma)\\left( \\frac{b}{12} - \\frac{\\nu}{12} \\right) + \\left( \\frac{b}{12} - \\frac{\\rho}{12} \\right) = 0 $$\nMultiply by $12$:\n$$ (1+\\gamma)(b-\\nu) + (b-\\rho) = 0 $$\n$$ b + \\gamma b - \\nu - \\gamma\\nu + b - \\rho = 0 $$\n$$ b(2+\\gamma) = \\nu(1+\\gamma) + \\rho $$\nSolving for $b$:\n$$ b = \\frac{(1+\\gamma)\\nu + \\rho}{2+\\gamma} $$\n\nThe coefficient $b$ represents the mixed partial derivative $\\partial_{xy}^2 u^{\\star}$. The terms $\\nu = \\partial_y q_x$ and $\\rho = \\partial_x q_y$ are the corresponding (and generally unequal) mixed derivatives from the HDG gradient $\\boldsymbol{q}_h$. The expression for $b$ can be written as a weighted average:\n$$ b = \\left(\\frac{1+\\gamma}{2+\\gamma}\\right)\\nu + \\left(\\frac{1}{2+\\gamma}\\right)\\rho $$\nWhen $\\gamma=0$, $\\mathbf{M}=\\mathbf{I}$ (isotropic weighting), and $b = \\frac{\\nu+\\rho}{2}$, which is the arithmetic mean. As $\\gamma \\to \\infty$, the weight on $\\nu$ approaches $1$ while the weight on $\\rho$ approaches $0$, so $b \\to \\nu$.\nHypothesis: Increasing $\\gamma$ introduces anisotropy into the post-processing by prioritizing the reduction of the error component along the advection direction $\\boldsymbol{\\beta}$ over the error component in the transverse direction. This forces the post-processed solution's gradient to more closely match the component of the HDG gradient aligned with $\\boldsymbol{\\beta}$, potentially at the cost of a larger discrepancy with the transverse component of the HDG gradient. This makes the post-processing error itself anisotropic, with its magnitude likely being smaller along $\\boldsymbol{\\beta}$ and larger in the direction perpendicular to it.",
            "answer": "$$\\boxed{\\frac{(1+\\gamma)\\nu + \\rho}{2+\\gamma}}$$"
        },
        {
            "introduction": "Moving from pure theory to computational practice, this final exercise applies post-processing in a more realistic and sophisticated context. This task  involves implementing a full HDG solver and a goal-oriented post-processing scheme, where the objective is to improve the accuracy of a specific quantity of interest, not just the global solution error. This hands-on coding challenge bridges the gap between local theoretical properties and global numerical performance, demonstrating how superconvergence can be harnessed for practical engineering and scientific computations.",
            "id": "3410066",
            "problem": "Consider the one-dimensional model problem on the unit interval $[0,1]$ governed by the elliptic partial differential equation $-u''(x) = f(x)$ with homogeneous Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$. Let the exact solution be $u(x) = \\sin(\\pi x)$, so that the source term is $f(x) = \\pi^2 \\sin(\\pi x)$. Define the goal functional $J(u)$ by $J(u) = \\int_0^1 u(x)\\,x\\,dx$, which has the exact value $J(u) = 1/\\pi$. The dual or adjoint problem associated with $J(u)$ is defined by $-z''(x) = x$ with $z(0)=0$ and $z(1)=0$, and its exact solution is $z(x) = -\\frac{1}{6}x^3 + \\frac{1}{6}x$.\n\nYou must implement a Hybridizable Discontinuous Galerkin (HDG) discretization with polynomial degree $k=0$ on a uniform partition of $[0,1]$ into $N$ elements. Use a stabilization parameter $\\tau>0$. The HDG formulation is based on the first-order system $q = u'$ and $-q' = f$ with hybrid trace unknowns $\\widehat{u}$ defined at mesh nodes. On each element $K = [x_L,x_R]$, seek constants $u_h|_K = U_0^K$ and $q_h|_K = Q_0^K$ and traces $\\widehat{u}(x_L)$, $\\widehat{u}(x_R)$ such that the local weak forms of the equations $q - u' = 0$ and $-q' = f$ hold for constant test functions, and the numerical fluxes $\\hat{q}n = q n + \\tau(u - \\hat{u})$ are conservative across interior nodes. The boundary traces satisfy $\\widehat{u}(0)=0$ and $\\widehat{u}(1)=0$. Assemble the global system for the interior trace unknowns by enforcing flux conservation at each interior node.\n\nConstruct the postprocessed solution $u^\\star$ on each element $K$ by imposing the superconvergent HDG postprocessing constraint that $u^\\star$ is piecewise linear on each element with derivative $d u^\\star/dx = q_h|_K$, and by selecting the elementwise constant $c_K$ so that the dual-weighted moment matches: $\\int_K z(x)\\,u^\\star(x)\\,dx = \\int_K z(x)\\,u_h(x)\\,dx$. If the weight integral $\\int_K z(x)\\,dx$ is numerically negligible, fall back to the mean-preserving constraint $\\int_K u^\\star(x)\\,dx = \\int_K u_h(x)\\,dx$.\n\nCompute the following quantities:\n- The goal functional evaluations $J(u_h) = \\int_0^1 u_h(x)\\,x\\,dx$ and $J(u^\\star) = \\int_0^1 u^\\star(x)\\,x\\,dx$, and the exact $J(u) = 1/\\pi$.\n- The global $L^2$ errors $\\|u_h - u\\|_{L^2(0,1)}$ and $\\|u^\\star - u\\|_{L^2(0,1)}$.\n\nUse numerical quadrature with sufficiently high-order Gauss-Legendre rules for all non-polynomial integrals. Angles do not appear in this problem, so there is no angle unit specification required. No physical units are required.\n\nYour program must, for each test case with parameters $(N,\\tau)$, return a list containing:\n- A boolean indicating whether $|J(u^\\star) - J(u)| < |J(u_h) - J(u)|$.\n- A boolean indicating whether $\\|u^\\star - u\\|_{L^2(0,1)} < \\|u_h - u\\|_{L^2(0,1)}$.\n- A float equal to the relative reduction in goal error, defined as $\\left(|J(u_h) - J(u)| - |J(u^\\star) - J(u)|\\right)/|J(u_h) - J(u)|$.\n- A float equal to the relative reduction in the global $L^2$ error, defined as $\\left(\\|u_h - u\\|_{L^2} - \\|u^\\star - u\\|_{L^2}\\right)/\\|u_h - u\\|_{L^2}$.\n\nTest Suite:\n- Case 1: $(N,\\tau) = (10, 1.0)$.\n- Case 2: $(N,\\tau) = (4, 5.0)$.\n- Case 3: $(N,\\tau) = (40, 0.5)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a nested list in the order given above. For example: \"[[True,True,0.123456,0.234567],[...],[...]]\".",
            "solution": "The problem requires implementing a Hybridizable Discontinuous Galerkin (HDG) method with polynomial degree $k=0$ for the 1D Poisson equation, followed by a superconvergent post-processing step. The accuracy of the original HDG solution and the post-processed solution is then compared.\n\nThe governing equation $-u''(x) = f(x)$ on $\\Omega = [0,1]$ is rewritten as a first-order system by introducing the flux $q(x) = u'(x)$:\n$$\nq(x) - u'(x) = 0 \\\\\n-q'(x) = f(x)\n$$\nThe domain is partitioned into $N$ uniform elements $K_j = [x_j, x_{j+1}]$ of length $h = 1/N$. The HDG method seeks piecewise constant approximations $u_h|_{K_j} = U_0^j$ and $q_h|_{K_j} = Q_0^j$. The coupling is via the hybrid trace $\\widehat{u}_h$ at element interfaces, with $\\widehat{u}_h(x_j) = \\widehat{u}_j$.\n\nThe local HDG weak formulation on an element $K_j$, after applying integration by parts and using constant test functions, yields a system to express the local unknowns $(U_0^j, Q_0^j)$ in terms of the trace variables $\\widehat{u}_j, \\widehat{u}_{j+1}$:\n$$\nQ_0^j = \\frac{\\widehat{u}_{j+1} - \\widehat{u}_j}{h} \\\\\nU_0^j = \\frac{\\widehat{u}_j + \\widehat{u}_{j+1}}{2} - \\frac{1}{2\\tau} \\int_{K_j} f(x) \\,dx\n$$\n\nThe global system is formed by enforcing flux conservation, $\\widehat{q}_n(x_j)|_{K_{j-1}} + \\widehat{q}_n(x_j)|_{K_j}=0$, at each interior node $x_j$ for $j=1, \\dots, N-1$. Substituting the expressions for $U_0$ and $Q_0$ yields a three-point stencil for the unknown trace values $\\widehat{u}_j$:\n$$\n\\left(-\\frac{1}{h} + \\frac{\\tau}{2}\\right) \\widehat{u}_{j-1} + \\left(\\frac{2}{h} - \\tau\\right) \\widehat{u}_j + \\left(-\\frac{1}{h} + \\frac{\\tau}{2}\\right) \\widehat{u}_{j+1} = \\frac{1}{2} \\left( \\int_{K_{j-1}} f(x) \\,dx + \\int_{K_j} f(x) \\,dx \\right)\n$$\nThis equation for $j=1, \\dots, N-1$, along with boundary conditions $\\widehat{u}_0=0$ and $\\widehat{u}_N=0$, forms an $(N-1) \\times (N-1)$ symmetric tridiagonal linear system, which is solved for the interior trace unknowns $\\{\\widehat{u}_j\\}_{j=1}^{N-1}$. Once the $\\widehat{u}_j$ are known, the element-wise constant solutions $U_0^j$ and $Q_0^j$ are computed for all elements, completing the HDG solution $(u_h, q_h)$.\n\nNext, the post-processed solution $u^\\star$ is constructed. On each element $K_j$, $u^\\star$ is a linear function whose derivative is the constant HDG flux $Q_0^j$: $u^\\star(x) = Q_0^j x + C_j$. The constant of integration $C_j$ is determined by matching the dual-weighted moment of $u^\\star$ to that of $u_h$:\n$$\n\\int_{K_j} z(x) u^\\star(x) \\,dx = \\int_{K_j} z(x) u_h(x) \\,dx \\implies C_j = \\frac{\\int_{K_j} (U_0^j - Q_0^j x) z(x) \\,dx}{\\int_{K_j} z(x) \\,dx}\n$$\nwhere $z(x) = -\\frac{1}{6}x^3 + \\frac{1}{6}x$ is the dual solution. If the denominator is numerically negligible, a mean-preserving fallback is used: $\\int_{K_j} u^\\star(x) \\,dx = \\int_{K_j} u_h(x) \\,dx$, which yields $C_j = U_0^j - Q_0^j (x_j + x_{j+1})/2$.\n\nFinally, the required quantities are calculated. The goal functional is computed for $u_h$ and $u^\\star$:\n$J(u_h) = \\sum_j \\int_{K_j} U_0^j x \\,dx$ and $J(u^\\star) = \\sum_j \\int_{K_j} (Q_0^j x + C_j) x \\,dx$.\nThe $L^2$ errors are computed as:\n$\\|u_h-u\\|_{L^2}^2 = \\sum_j \\int_{K_j} (U_0^j - \\sin(\\pi x))^2 \\,dx$ and $\\|u^\\star-u\\|_{L^2}^2 = \\sum_j \\int_{K_j} (Q_0^j x + C_j - \\sin(\\pi x))^2 \\,dx$.\nIntegrals involving the non-polynomial functions are evaluated using high-order Gauss-Legendre quadrature. The resulting solution metrics are then computed and reported for each test case.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve():\n    \"\"\"\n    Solves the 1D Poisson problem using an HDG method with k=0 polynomials,\n    applies a superconvergent post-processing step, and compares the results.\n    \"\"\"\n\n    test_cases = [\n        (10, 1.0),\n        (4, 5.0),\n        (40, 0.5),\n    ]\n\n    all_results = []\n    \n    # Use 10-point Gauss-Legendre quadrature, sufficient for smooth integrands\n    q_points, q_weights = np.polynomial.legendre.leggauss(10)\n\n    def integrate(func, a, b):\n        \"\"\"Numerically integrates a function from a to b using Gauss-Legendre.\"\"\"\n        if a == b:\n            return 0.0\n        x_mapped = 0.5 * (b - a) * q_points + 0.5 * (a + b)\n        return 0.5 * (b - a) * np.sum(q_weights * func(x_mapped))\n\n    # Define problem functions\n    pi = np.pi\n    u_exact = lambda x: np.sin(pi * x)\n    f_source = lambda x: pi**2 * np.sin(pi * x)\n    z_dual = lambda x: -x**3/6.0 + x/6.0\n    J_exact = 1.0 / pi\n\n    for N, tau in test_cases:\n        h = 1.0 / N\n        nodes = np.linspace(0, 1, N + 1)\n\n        # 1. Assemble the global system for interior trace unknowns u_hat\n        num_unknowns = N - 1\n        if num_unknowns == 0:\n            all_results.append([False, False, 0.0, 0.0])\n            continue\n\n        # Tridiagonal matrix A\n        ab = np.zeros((3, num_unknowns))\n        diag_val = 2.0 / h - tau\n        off_diag_val = -1.0 / h + tau / 2.0\n        ab[0, 1:] = off_diag_val\n        ab[1, :] = diag_val\n        ab[2, :-1] = off_diag_val\n\n        # RHS vector b\n        b = np.zeros(num_unknowns)\n        for j in range(1, N):\n            b[j-1] = 0.5 * integrate(f_source, nodes[j-1], nodes[j+1])\n\n        # 2. Solve for interior u_hat values\n        u_hat_interior = solve_banded((1, 1), ab, b)\n        u_hat = np.zeros(N + 1)\n        u_hat[1:N] = u_hat_interior\n\n        # 3. Reconstruct local solutions U_0 and Q_0 on each element\n        U0 = np.zeros(N)\n        Q0 = np.zeros(N)\n        for j in range(N):\n            x_j, x_j1 = nodes[j], nodes[j+1]\n            f_integral_Kj = integrate(f_source, x_j, x_j1)\n            \n            Q0[j] = (u_hat[j+1] - u_hat[j]) / h\n            U0[j] = 0.5 * (u_hat[j] + u_hat[j+1]) - f_integral_Kj / (2.0 * tau)\n\n        # 4. Post-process to get u_star\n        C = np.zeros(N)\n        for j in range(N):\n            x_j, x_j1 = nodes[j], nodes[j+1]\n            z_integral = integrate(z_dual, x_j, x_j1)\n            \n            if np.abs(z_integral) < 1e-14:\n                # Fallback to mean-preserving constraint\n                C[j] = U0[j] - Q0[j] * (x_j + x_j1) / 2.0\n            else:\n                # Dual-weighted moment matching\n                xz_integral = integrate(lambda x: x * z_dual(x), x_j, x_j1)\n                num = U0[j] * z_integral - Q0[j] * xz_integral\n                C[j] = num / z_integral\n        \n        # 5. Compute goal functionals and L2 errors\n        J_uh = 0.0\n        J_ustar = 0.0\n        L2_err_uh_sq = 0.0\n        L2_err_ustar_sq = 0.0\n\n        for j in range(N):\n            x_j, x_j1 = nodes[j], nodes[j+1]\n            # Goal Functional J(u) = integral(u*x, dx)\n            J_uh += U0[j] * (x_j1**2 - x_j**2) / 2.0\n            J_ustar += Q0[j] * (x_j1**3 - x_j**3) / 3.0 + C[j] * (x_j1**2 - x_j**2) / 2.0\n\n            # L2 errors\n            uh_integrand = lambda x: (U0[j] - u_exact(x))**2\n            ustar_integrand = lambda x: (Q0[j] * x + C[j] - u_exact(x))**2\n            L2_err_uh_sq += integrate(uh_integrand, x_j, x_j1)\n            L2_err_ustar_sq += integrate(ustar_integrand, x_j, x_j1)\n\n        L2_err_uh = np.sqrt(L2_err_uh_sq)\n        L2_err_ustar = np.sqrt(L2_err_ustar_sq)\n\n        # 6. Calculate final metrics\n        goal_err_uh = abs(J_uh - J_exact)\n        goal_err_ustar = abs(J_ustar - J_exact)\n        \n        is_goal_improved = goal_err_ustar < goal_err_uh\n        is_L2_improved = L2_err_ustar < L2_err_uh\n        \n        rel_redux_goal = (goal_err_uh - goal_err_ustar) / goal_err_uh if goal_err_uh > 1e-15 else 0.0\n        rel_redux_L2 = (L2_err_uh - L2_err_ustar) / L2_err_uh if L2_err_uh > 1e-15 else 0.0\n\n        all_results.append([is_goal_improved, is_L2_improved, rel_redux_goal, rel_redux_L2])\n\n    case_str_list = []\n    for r in all_results:\n        case_str_list.append(f\"[{str(r[0])},{str(r[1])},{r[2]:.6f},{r[3]:.6f}]\")\n    final_str = f\"[{','.join(case_str_list)}]\"\n    print(final_str)\n\nsolve()\n```"
        }
    ]
}