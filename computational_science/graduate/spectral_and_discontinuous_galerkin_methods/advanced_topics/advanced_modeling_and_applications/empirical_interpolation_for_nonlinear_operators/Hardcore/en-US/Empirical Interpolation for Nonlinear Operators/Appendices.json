{
    "hands_on_practices": [
        {
            "introduction": "In numerical schemes for conservation laws, nonlinear components like flux limiters are essential for stability but can be computationally intensive, especially when parameterized. This exercise  provides a practical starting point by guiding you to build an EIM surrogate for a generalized minmod limiter. You will then test a critical aspect of your surrogate: whether it preserves the crucial $L^1$-contraction property of the underlying numerical scheme, offering a clear, stability-focused metric for success.",
            "id": "3383561",
            "problem": "Consider a one-dimensional scalar conservation law on the periodic domain $[0,1]$, given by $u_t + f(u)_x = 0$. For the linear advection case, let $f(u) = a u$ with constant advection speed $a > 0$. Discretize the domain into $N$ uniform cells with centers $x_i$, spacing $\\Delta x = 1/N$, and use a single explicit forward-Euler step of a finite-volume Monotonic Upstream-centered Schemes for Conservation Laws (MUSCL) reconstruction at the left interfaces with a slope limiter from the minmod family parameterized by $\\theta \\in [1,2]$. The generalized minmod-based flux limiter $\\phi_\\theta(r)$ acts on the slope ratio $r$ and is defined elementwise by\n$$\n\\phi_\\theta(r) = \\max\\left(0,\\min(\\theta r,1),\\min(r,\\theta)\\right).\n$$\nAssume $a=1$ and choose a Courant–Friedrichs–Lewy number $\\nu = a \\Delta t / \\Delta x \\in (0,1)$ for the time step $\\Delta t$.\n\nDefine the first, second, and third discrete difference operators for a cell-average vector $u \\in \\mathbb{R}^N$ with periodic boundary conditions by\n$$\n\\delta^- u_i = u_i - u_{i-1}, \\quad \\delta^+ u_i = u_{i+1} - u_i,\n$$\n$$\n\\Delta^{(2,-)} u_i = u_i - 2u_{i-1} + u_{i-2}, \\quad \\Delta^{(2,+)} u_i = u_{i+2} - 2u_{i+1} + u_i,\n$$\n$$\n\\Delta^{(3,-)} u_i = u_i - 3u_{i-1} + 3u_{i-2} - u_{i-3}, \\quad \\Delta^{(3,+)} u_i = u_{i+3} - 3u_{i+2} + 3u_{i+1} - u_i,\n$$\nwhere indices are modulo $N$. The corresponding slope ratios are\n$$\nr^{(1)}_i = \\frac{\\delta^- u_i}{\\delta^+ u_i + \\varepsilon}, \\quad r^{(2)}_i = \\frac{\\Delta^{(2,-)} u_i}{\\Delta^{(2,+)} u_i + \\varepsilon}, \\quad r^{(3)}_i = \\frac{\\Delta^{(3,-)} u_i}{\\Delta^{(3,+)} u_i + \\varepsilon},\n$$\nwith a small $\\varepsilon > 0$ for numerical robustness. Consider a hierarchical reconstruction of degree $p \\in \\{1,2,3\\}$ by defining the left state at the interface $x_{i+\\frac{1}{2}}$ as\n$$\nu^-_{i+\\frac{1}{2}} = u_i + \\frac{1}{2}\\,\\phi_\\theta\\!\\left(r^{(1)}_i\\right)\\,\\delta^- u_i + \\mathbf{1}_{\\{p\\ge 2\\}}\\,\\frac{1}{8}\\,\\phi_{\\alpha_2\\theta}\\!\\left(r^{(2)}_i\\right)\\,\\Delta^{(2,-)} u_i + \\mathbf{1}_{\\{p\\ge 3\\}}\\,\\frac{1}{48}\\,\\phi_{\\alpha_3\\theta}\\!\\left(r^{(3)}_i\\right)\\,\\Delta^{(3,-)} u_i,\n$$\nwhere $\\alpha_2 = \\tfrac{1}{2}$ and $\\alpha_3 = \\tfrac{1}{4}$, and $\\mathbf{1}_{\\{\\cdot\\}}$ is the indicator function. The upwind numerical flux at $x_{i+\\frac{1}{2}}$ is $F_{i+\\frac{1}{2}} = a\\,u^-_{i+\\frac{1}{2}}$, and the cell update is\n$$\nu_i^{(1)} = u_i^{(0)} - \\nu \\left(F_{i+\\frac{1}{2}} - F_{i-\\frac{1}{2}}\\right),\n$$\nwith periodic indexing.\n\nYou will implement an Empirical Interpolation Method (EIM) surrogate for the nonlinear operator $g(r;\\theta) = \\phi_\\theta(r)$ acting on the scalar variable $r$. The EIM construction must proceed from first principles:\n- Choose a uniform training grid in $r \\in [-R,R]$ with $n_r$ points, and a training set of parameters $\\theta_j \\in [\\theta_{\\min},\\theta_{\\max}]$.\n- Construct the snapshot matrix $S \\in \\mathbb{R}^{n_r \\times m}$ with columns $S(:,j) = g(r_{\\ell};\\theta_j)$ evaluated on the $r$-grid.\n- Compute the singular value decomposition and select a rank-$k$ basis $U_k \\in \\mathbb{R}^{n_r \\times k}$.\n- Select $k$ empirical interpolation nodes $\\{r_{p_j}\\}_{j=1}^k$ via the classical greedy residual maximization algorithm, forming the interpolation matrix $P^T U_k = U_k[p_1,\\dots,p_k,:] \\in \\mathbb{R}^{k \\times k}$.\n- For a given $\\theta$, form the online approximation\n$$\ng_{\\mathrm{EIM}}(\\cdot;\\theta) \\approx U_k \\left(P^T U_k\\right)^{-1} P^T g(\\cdot;\\theta),\n$$\nwhere $P^T g(\\cdot;\\theta)$ are the exact values of $g(r_{p_j};\\theta)$ at the selected nodes.\n\nUse this surrogate to approximate $\\phi_\\theta(r)$ for the limiter in the MUSCL reconstruction by evaluating $g_{\\mathrm{EIM}}(\\cdot;\\theta)$ on a dense $r$-grid and interpolating linearly to the required $r$ values. When $p \\ge 2$, reuse the same surrogate for the scaled parameters $\\alpha_2 \\theta$ and $\\alpha_3 \\theta$.\n\nDefine three pairs of initial data $(u^{(0)}, v^{(0)})$ to probe the $L^1$ contraction property:\n1. A pair of discontinuous step functions with a slight shift: $u^{(0)}(x) = \\mathbf{1}_{[0,0.5)}(x)$ and $v^{(0)}(x) = \\mathbf{1}_{[0,0.52)}(x)$.\n2. A pair of smooth sinusoidal waves with a small phase shift: $u^{(0)}(x) = 0.5 + 0.45 \\sin(2\\pi x)$ and $v^{(0)}(x) = 0.5 + 0.45 \\sin(2\\pi (x+\\delta))$ with $\\delta = 0.02$.\n3. A pair of smoothed random fields: draw $w \\in \\mathbb{R}^N$ with independent standard normal entries, smooth by local averaging over a window of width $9$ cells to obtain $u^{(0)}$, and set $v^{(0)}(x) = u^{(0)}(x) + 0.01\\sin(4\\pi x)$, clipping both to $[0,1]$.\n\nFor each pair, compute the discrete $L^1$ distance $\\|u^{(0)} - v^{(0)}\\|_1 = \\Delta x \\sum_i |u_i^{(0)} - v_i^{(0)}|$ and after one update step compute $\\|u^{(1)} - v^{(1)}\\|_1$ using:\n- the exact limiter $\\phi_\\theta$ (baseline), and\n- the EIM surrogate $g_{\\mathrm{EIM}}$ in place of $\\phi_\\theta$ (surrogate).\n\nDeclare that the surrogate degrades $L^1$ contraction for a given $(p,\\theta)$ if there exists at least one of the three pairs for which\n$$\n\\|u^{(1)}_{\\mathrm{sur}} - v^{(1)}_{\\mathrm{sur}}\\|_1 > \\|u^{(0)} - v^{(0)}\\|_1 + \\varepsilon \\quad \\text{and} \\quad \\|u^{(1)}_{\\mathrm{exact}} - v^{(1)}_{\\mathrm{exact}}\\|_1 \\le \\|u^{(0)} - v^{(0)}\\|_1 + \\varepsilon,\n$$\nwith tolerance $\\varepsilon = 10^{-12}$.\n\nImplement the above with $a = 1$, $N = 200$, $\\nu = 0.5$, $\\varepsilon = 10^{-12}$, $R = 6$, $n_r = 401$, EIM training parameters $\\theta_{\\min} = 1.2$, $\\theta_{\\max} = 1.8$, $m = 5$ training parameters evenly spaced, and rank $k = 4$.\n\nTest Suite:\nUse the following eight parameter sets $(p,\\theta)$:\n- $(1, 1.0)$,\n- $(1, 1.8)$,\n- $(1, 2.0)$,\n- $(2, 1.0)$,\n- $(2, 1.8)$,\n- $(2, 2.0)$,\n- $(3, 1.3)$,\n- $(3, 2.0)$.\n\nRequired final output format:\nYour program should produce a single line of output containing the boolean degradation results for the eight test cases in the given order as a comma-separated list enclosed in square brackets (e.g., \"[True,False,False,True,False,False,True,False]\"). No other text should be printed. Angles are not used; there are no physical units in the output. All computations must be performed in purely mathematical terms as described above.",
            "solution": "The problem requires the implementation of a high-order MUSCL-type finite volume scheme to solve the one-dimensional linear advection equation, $u_t + a u_x = 0$, on a periodic domain. The core of the problem is to construct an Empirical Interpolation Method (EIM) surrogate for the nonlinear generalized minmod flux limiter function, $\\phi_\\theta(r)$, and to assess whether this surrogate model degrades the $L^1$-contraction property of the numerical scheme.\n\nThe methodology involves several distinct steps:\n$1$. The definition and implementation of the hierarchical MUSCL finite volume scheme up to order $p \\in \\{1,2,3\\}$.\n$2$. The offline construction of the EIM surrogate for the limiter function $\\phi_\\theta(r)$.\n$3$. The online evaluation of the scheme using both the exact limiter and the EIM surrogate.\n$4$. A quantitative comparison based on the discrete $L^1$-norm to check for degradation of the contraction property for a suite of test cases.\n\nFirst, we detail the numerical scheme. The domain $[0,1]$ is discretized into $N$ cells of width $\\Delta x = 1/N$. The cell-averaged quantity at time step $n$ is denoted by $u_i^{(n)}$. A single forward Euler time step updates the solution via:\n$$\nu_i^{(n+1)} = u_i^{(n)} - \\frac{\\Delta t}{\\Delta x} \\left(F_{i+\\frac{1}{2}} - F_{i-\\frac{1}{2}}\\right)\n$$\nwhere $\\nu = a \\Delta t / \\Delta x$ is the Courant number. The numerical flux $F_{i+\\frac{1}{2}}$ is an upwind flux, $F_{i+\\frac{1}{2}} = a u^-_{i+\\frac{1}{2}}$, since the advection speed $a=1$ is positive. The value $u^-_{i+\\frac{1}{2}}$ is a high-order reconstruction of the solution at the left side of the interface $x_{i+\\frac{1}{2}}$. The hierarchical reconstruction of degree $p$ is given by:\n$$\nu^-_{i+\\frac{1}{2}} = u_i + \\frac{1}{2}\\phi_\\theta(r^{(1)}_i)\\delta^- u_i + \\mathbf{1}_{\\{p\\ge 2\\}}\\frac{1}{8}\\phi_{\\alpha_2\\theta}(r^{(2)}_i)\\Delta^{(2,-)} u_i + \\mathbf{1}_{\\{p\\ge 3\\}}\\frac{1}{48}\\phi_{\\alpha_3\\theta}(r^{(3)}_i)\\Delta^{(3,-)} u_i\n$$\nThe terms $\\delta^- u_i$, $\\Delta^{(2,-)} u_i$, and $\\Delta^{(3,-)} u_i$ are backward finite difference operators of first, second, and third order, respectively. The corresponding slope ratios $r^{(k)}_i$ are formed by dividing the backward difference by the corresponding forward difference, e.g., $r^{(1)}_i = (\\delta^- u_i) / (\\delta^+ u_i + \\varepsilon)$. The function $\\phi_\\theta(r)$ is the generalized minmod limiter:\n$$\n\\phi_\\theta(r) = \\max\\left(0, \\min(\\theta r, 1), \\min(r, \\theta)\\right)\n$$\nThis limiter is applied to each order of the reconstruction, with a scaled parameter $\\alpha_k \\theta$, to control spurious oscillations. The constants are given as $\\alpha_2 = 1/2$ and $\\alpha_3 = 1/4$. All indices are handled periodically modulo $N$.\n\nSecond, we construct the EIM surrogate for the parametric function $g(r; \\theta) = \\phi_\\theta(r)$. This is a model order reduction technique and begins with an offline training stage.\n$1$. A set of $m=5$ training parameters $\\{\\theta_j\\}_{j=1}^5$ is chosen uniformly in the interval $[\\theta_{\\min}, \\theta_{\\max}] = [1.2, 1.8]$.\n$2$. A training grid of $n_r = 401$ points for the variable $r$ is created uniformly in $[-R, R] = [-6, 6]$.\n$3$. A snapshot matrix $S \\in \\mathbb{R}^{n_r \\times m}$ is assembled, where each column $S_{:,j}$ is the evaluation of the function $g(r; \\theta_j)$ on the $r$-grid.\n$4$. The Singular Value Decomposition (SVD) of the snapshot matrix, $S = U \\Sigma V^T$, provides an orthonormal basis $U$ for the space spanned by the snapshots. We truncate this basis to the first $k=4$ dominant modes, yielding the reduced basis $U_k \\in \\mathbb{R}^{n_r \\times k}$.\n$5$. A set of $k=4$ empirical interpolation nodes $\\{r_{p_j}\\}_{j=1}^k$ is selected from the $r$-grid using a greedy algorithm. This algorithm iteratively selects the point that maximizes the residual error when approximating the next basis vector with an interpolant constructed from the previously selected points and basis vectors. This process ensures the interpolation matrix is well-conditioned.\nLet $I = \\{p_1, \\dots, p_k\\}$ be the set of indices for the interpolation nodes. We define a matrix $M = U_k[I, :]$ which represents the basis vectors evaluated at the interpolation nodes. The inverse of this matrix, $M^{-1}$, is pre-computed. The offline stage concludes by storing $U_k$, $M^{-1}$, the interpolation node indices $I$, and the $r$-grid.\n\nThird, in the online stage, for any new parameter $\\theta$, the EIM surrogate $g_{\\mathrm{EIM}}(r;\\theta)$ is evaluated. This is done by first evaluating the exact function $g(r;\\theta)$ only at the $k$ interpolation nodes, $g_{\\text{nodes}} = g(r_I; \\theta)$. The coefficients of the approximation in the reduced basis are then computed as $c = M^{-1} g_{\\text{nodes}}$. The EIM approximation is then given by $g_{\\mathrm{EIM}}(\\cdot ; \\theta) = U_k c$. The problem specifies that this EIM evaluation is to be performed on the dense $r$-grid from the training stage. When the MUSCL scheme requires a value $\\phi_\\theta(r_i)$ for a specific slope ratio $r_i$ computed from the solution data, this value is obtained by linear interpolation from the pre-computed vector $g_{\\mathrm{EIM}}(\\cdot ; \\theta)$ on the dense $r$-grid.\n\nFinally, we test the surrogate's performance. The $L^1$-contraction property states that for a valid scheme, the $L^1$-distance between two different solutions should not increase in time, i.e., $\\|u^{(n+1)} - v^{(n+1)}\\|_1 \\le \\|u^{(n)} - v^{(n)}\\|_1$. We test this property for one time step using three pairs of initial conditions $(u^{(0)}, v^{(0)})$. For each test case $(p, \\theta)$, we compare the evolution of the $L^1$ distance using the exact limiter and the EIM surrogate. The surrogate is declared to cause \"degradation\" if, for any of the three initial condition pairs, it leads to a significant increase in the $L^1$ distance while the exact scheme does not. Formally, degradation occurs if:\n$$\n\\|u^{(1)}_{\\mathrm{sur}} - v^{(1)}_{\\mathrm{sur}}\\|_1 > \\|u^{(0)} - v^{(0)}\\|_1 + \\varepsilon \\quad \\text{and} \\quad \\|u^{(1)}_{\\mathrm{exact}} - v^{(1)}_{\\mathrm{exact}}\\|_1 \\le \\|u^{(0)} - v^{(0)}\\|_1 + \\varepsilon\n$$\nwith a tolerance of $\\varepsilon = 10^{-12}$. The implementation will systematically execute this procedure for all specified $(p, \\theta)$ pairs and report a boolean value indicating whether degradation was observed.\n\nAll numerical parameters are specified in the problem: $a=1$, $N=200$, $\\nu=0.5$, $\\varepsilon=10^{-12}$, $R=6$, $n_r=401$, $\\theta_{\\min}=1.2$, $\\theta_{\\max}=1.8$, $m=5$, $k=4$. For the third initial condition involving a random field, a fixed seed is used for reproducibility.\n\nThe final Python code implements these steps. Helper functions are defined for the limiter $\\phi_\\theta$, the EIM construction, the EIM evaluation, the MUSCL update step, and the generation of initial conditions. The main function orchestrates the testing procedure for all cases and prints the final boolean list.\nThe implementation uses `numpy` for array operations and `numpy.linalg.svd` for the SVD. `numpy.roll` is used for implementing the finite differences with periodic boundary conditions efficiently. Linear interpolation is performed with `numpy.interp`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not used as per the allowed library list.\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and produce the final output.\n    \"\"\"\n    # Global parameters as specified in the problem\n    a = 1.0\n    N = 200\n    nu = 0.5\n    eps = 1e-12\n    R = 6.0\n    n_r = 401\n    theta_min = 1.2\n    theta_max = 1.8\n    m = 5\n    k = 4\n    dx = 1.0 / N\n    \n    # EIM surrogate parameters\n    eim_params = {\n        'R': R,\n        'n_r': n_r,\n        'theta_min': theta_min,\n        'theta_max': theta_max,\n        'm': m,\n        'k': k\n    }\n\n    # Test suite from the problem statement\n    test_suite = [\n        (1, 1.0),\n        (1, 1.8),\n        (1, 2.0),\n        (2, 1.0),\n        (2, 1.8),\n        (2, 2.0),\n        (3, 1.3),\n        (3, 2.0)\n    ]\n\n    # Build the EIM surrogate (offline stage)\n    eim_data = build_eim_surrogate(eim_params)\n\n    # Get initial conditions\n    ic_pairs = get_initial_conditions(N, dx)\n\n    results = []\n    for p, theta in test_suite:\n        degradation_found = False\n        for u0, v0 in ic_pairs:\n            l1_dist_0 = dx * np.sum(np.abs(u0 - v0))\n\n            # Run with exact limiter\n            u1_exact = get_update(u0, p, theta, a, nu, dx, N, eps, use_eim=False, eim_data=None)\n            v1_exact = get_update(v0, p, theta, a, nu, dx, N, eps, use_eim=False, eim_data=None)\n            l1_dist_1_exact = dx * np.sum(np.abs(u1_exact - v1_exact))\n\n            # Run with EIM surrogate\n            u1_sur = get_update(u0, p, theta, a, nu, dx, N, eps, use_eim=True, eim_data=eim_data)\n            v1_sur = get_update(v0, p, theta, a, nu, dx, N, eps, use_eim=True, eim_data=eim_data)\n            l1_dist_1_sur = dx * np.sum(np.abs(u1_sur - v1_sur))\n\n            # Check degradation condition\n            if l1_dist_1_sur > l1_dist_0 + eps and l1_dist_1_exact = l1_dist_0 + eps:\n                degradation_found = True\n                break  # One instance of degradation is sufficient for this test case\n        \n        results.append(degradation_found)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef phi_theta(r, theta):\n    \"\"\"\n    Computes the generalized minmod flux limiter.\n    phi_theta(r) = max(0, min(theta*r, 1), min(r, theta))\n    \"\"\"\n    zero = np.zeros_like(r)\n    term1 = np.minimum(theta * r, 1.0)\n    term2 = np.minimum(r, theta)\n    return np.maximum.reduce([zero, term1, term2])\n\ndef build_eim_surrogate(params):\n    \"\"\"\n    Builds the EIM surrogate (offline phase).\n    \"\"\"\n    k = params['k']\n    r_grid = np.linspace(-params['R'], params['R'], params['n_r'])\n    theta_train = np.linspace(params['theta_min'], params['theta_max'], params['m'])\n\n    # 1. Snapshot matrix\n    S = np.zeros((params['n_r'], params['m']))\n    for j, th in enumerate(theta_train):\n        S[:, j] = phi_theta(r_grid, th)\n\n    # 2. SVD and basis\n    U, s, Vh = np.linalg.svd(S, full_matrices=False)\n    U_k = U[:, :k]\n\n    # 3. Greedy selection of interpolation points\n    indices = []\n    # First point\n    p1_idx = np.argmax(np.abs(U_k[:, 0]))\n    indices.append(p1_idx)\n    \n    for j in range(1, k):\n        U_sub = U_k[:, :j]\n        target_vec = U_k[:, j]\n        \n        P_U_sub = U_k[indices, :j]\n        P_target_vec = U_k[indices, j]\n        \n        coeffs = np.linalg.solve(P_U_sub, P_target_vec)\n        \n        residual = target_vec - U_sub @ coeffs\n        p_new_idx = np.argmax(np.abs(residual))\n        indices.append(p_new_idx)\n\n    # 4. Pre-compute inverse of interpolation matrix\n    M = U_k[indices, :]\n    M_inv = np.linalg.inv(M)\n\n    return {'U_k': U_k, 'M_inv': M_inv, 'indices': indices, 'r_grid': r_grid}\n\ndef evaluate_eim_surrogate(eim_data, theta):\n    \"\"\"\n    Evaluates the EIM surrogate for a given theta (online phase).\n    \"\"\"\n    U_k = eim_data['U_k']\n    M_inv = eim_data['M_inv']\n    indices = eim_data['indices']\n    r_grid = eim_data['r_grid']\n\n    r_nodes = r_grid[indices]\n    g_at_nodes = phi_theta(r_nodes, theta)\n    \n    coeffs = M_inv @ g_at_nodes\n    g_eim = U_k @ coeffs\n\n    return g_eim\n\ndef get_update(u, p, theta, a, nu, dx, N, eps, use_eim, eim_data):\n    \"\"\"\n    Computes one time step of the MUSCL scheme.\n    \"\"\"\n    # Difference operators\n    dm1_u = u - np.roll(u, 1)\n    dp1_u = np.roll(u, -1) - u\n    \n    # Ratios\n    r1 = dm1_u / (dp1_u + eps)\n\n    # Limiter application\n    if use_eim:\n        # Evaluate EIM on dense grid\n        g_eim_1 = evaluate_eim_surrogate(eim_data, theta)\n        # Interpolate to find values at required ratios\n        phi1 = np.interp(r1, eim_data['r_grid'], g_eim_1)\n    else:\n        phi1 = phi_theta(r1, theta)\n\n    # Hierarchical Reconstruction\n    u_left = u + 0.5 * phi1 * dm1_u\n\n    alpha2 = 0.5\n    if p >= 2:\n        dm2_u = u - 2 * np.roll(u, 1) + np.roll(u, 2)\n        dp2_u = np.roll(u, -2) - 2 * np.roll(u, -1) + u\n        r2 = dm2_u / (dp2_u + eps)\n        \n        if use_eim:\n            g_eim_2 = evaluate_eim_surrogate(eim_data, alpha2 * theta)\n            phi2 = np.interp(r2, eim_data['r_grid'], g_eim_2)\n        else:\n            phi2 = phi_theta(r2, alpha2 * theta)\n            \n        u_left += (1.0 / 8.0) * phi2 * dm2_u\n\n    alpha3 = 0.25\n    if p >= 3:\n        dm3_u = u - 3 * np.roll(u, 1) + 3 * np.roll(u, 2) - np.roll(u, 3)\n        dp3_u = np.roll(u, -3) - 3 * np.roll(u, -2) + 3 * np.roll(u, -1) - u\n        r3 = dm3_u / (dp3_u + eps)\n\n        if use_eim:\n            g_eim_3 = evaluate_eim_surrogate(eim_data, alpha3 * theta)\n            phi3 = np.interp(r3, eim_data['r_grid'], g_eim_3)\n        else:\n            phi3 = phi_theta(r3, alpha3 * theta)\n            \n        u_left += (1.0 / 48.0) * phi3 * dm3_u\n\n    # Numerical Flux\n    F = a * u_left\n    F_imhalf = np.roll(F, 1)\n\n    # Update\n    u_new = u - nu * (F - F_imhalf)\n    return u_new\n\ndef get_initial_conditions(N, dx):\n    \"\"\"\n    Generates the three pairs of initial condition data.\n    \"\"\"\n    x = (np.arange(N) + 0.5) * dx\n    ic_pairs = []\n\n    # 1. Discontinuous step functions\n    u0_1 = (x  0.5).astype(float)\n    v0_1 = (x  0.52).astype(float)\n    ic_pairs.append((u0_1, v0_1))\n\n    # 2. Smooth sinusoidal waves\n    delta = 0.02\n    u0_2 = 0.5 + 0.45 * np.sin(2 * np.pi * x)\n    v0_2 = 0.5 + 0.45 * np.sin(2 * np.pi * (x + delta))\n    ic_pairs.append((u0_2, v0_2))\n\n    # 3. Smoothed random fields\n    np.random.seed(42) # For reproducibility\n    w = np.random.randn(N)\n    window_width = 9\n    pad_width = window_width // 2\n    w_padded = np.concatenate((w[-pad_width:], w, w[:pad_width]))\n    kernel = np.ones(window_width) / window_width\n    u0_3_raw = np.convolve(w_padded, kernel, mode='valid')\n    \n    v0_3_raw = u0_3_raw + 0.01 * np.sin(4 * np.pi * x)\n    \n    u0_3 = np.clip(u0_3_raw, 0, 1)\n    v0_3 = np.clip(v0_3_raw, 0, 1)\n    ic_pairs.append((u0_3, v0_3))\n\n    return ic_pairs\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "After building a basic EIM, it's natural to ask how the interpolation points are chosen, as this choice is critical to the accuracy and stability of the approximation. This practice  delves into this core algorithmic question by comparing two powerful point-selection strategies. You will implement both a classic global method based on pivoted QR factorization and a localized greedy approach tailored for Discontinuous Galerkin methods, allowing you to directly compare their performance and understand their respective strengths.",
            "id": "3383616",
            "problem": "Consider the one-dimensional domain $[0,1]$ partitioned into $E$ non-overlapping elements $\\{K\\}_{K=1}^{E}$, each equipped with $n_p$ local collocation nodes. The global set of nodes is indexed by $i=1,\\dots,M$, where $M=E \\cdot n_p$, and every index $i$ maps to a unique element $K(i) \\in \\{1,\\dots,E\\}$. Let $x_i \\in [0,1]$ denote the coordinate corresponding to the global index $i$.\n\nLet the parametric state be given by the deterministic function $u(x;\\mu)$, where $\\mu \\in [0,1]$ is a scalar parameter, and define a nonlinear operator $\\mathcal{N}(u)$. You are tasked with constructing an Empirical Interpolation Method (EIM) for $\\mathcal{N}(u)$ suitable for Discontinuous Galerkin (DG) discretizations, and comparing two point-selection strategies:\n- A localized greedy strategy driven by elementwise DG residual norms.\n- A global pivoted QR strategy.\n\nUse the following precise mathematical specifications.\n\n- Spatial discretization and parameterized state:\n  - Use $E=10$ elements and $n_p=6$ nodes per element. For each element $K$, place the $n_p$ nodes uniformly within the element interior, so that all global nodes are distinct across elements.\n  - Define\n    $$u(x;\\mu) = \\sin\\!\\big(2\\pi(x+0.3\\mu)\\big) + \\tfrac{1}{2}\\cos\\!\\big(5\\pi(x-0.2\\mu)\\big) + 0.2\\exp\\!\\big(-50(x-0.3-0.4\\mu)^2\\big).$$\n  - Define the nonlinear operator\n    $$\\mathcal{N}(u) = u^2 + \\exp(u).$$\n\n- Snapshot matrix and reduced basis:\n  - Let the training parameter set be $\\{\\mu_k\\}_{k=1}^{N_\\mathrm{train}}$ with $N_\\mathrm{train}=24$ equidistant values on $[0,1]$.\n  - Construct the snapshot matrix $S \\in \\mathbb{R}^{M \\times N_\\mathrm{train}}$ with entries\n    $$S_{i,k} = \\mathcal{N}(u(x_i;\\mu_k)).$$\n  - Compute the thin singular value decomposition $S = U \\Sigma V^\\top$ and define the reduced spatial basis as the first $r_{\\max}$ columns of $U$, denoted $\\Phi = [\\phi_1,\\dots,\\phi_{r_{\\max}}] \\in \\mathbb{R}^{M \\times r_{\\max}}$, with $r_{\\max}=12$.\n\n- Discrete Empirical Interpolation Method (EIM) approximation operator:\n  - For a set of interpolation indices $\\mathcal{I}_r = \\{i_1,\\dots,i_r\\} \\subset \\{1,\\dots,M\\}$ of cardinality $r$, let $P_{\\mathcal{I}_r} \\in \\mathbb{R}^{M \\times r}$ be the sampling matrix that extracts entries at indices $\\mathcal{I}_r$.\n  - The rank-$r$ EIM approximation of any vector $f \\in \\mathbb{R}^M$ is given by\n    $$\\mathcal{P}_r(f) = \\Phi_r \\left(P_{\\mathcal{I}_r}^\\top \\Phi_r\\right)^{-1} P_{\\mathcal{I}_r}^\\top f,$$\n    where $\\Phi_r = [\\phi_1,\\dots,\\phi_r]$.\n\n- Point-selection strategies to be implemented:\n  1. Localized greedy driven by elementwise DG residual norms:\n     - Initialize with an empty index set. For $k=1,\\dots,r_{\\max}$:\n       - Define the current approximation of $\\phi_k$ using already selected indices $\\mathcal{I}_{k-1}$ as\n         $$\\widehat{\\phi}_k = \\begin{cases}\n         0,  k=1,\\\\\n         \\Phi_{k-1} \\left(P_{\\mathcal{I}_{k-1}}^\\top \\Phi_{k-1}\\right)^{-1} P_{\\mathcal{I}_{k-1}}^\\top \\phi_k,  k \\ge 2,\n         \\end{cases}$$\n         and the residual $r_k = \\phi_k - \\widehat{\\phi}_k \\in \\mathbb{R}^M$.\n       - For each element $K$, compute the elementwise DG residual norm\n         $$\\|r_k\\|_{K} = \\left(\\sum_{i:K(i)=K} r_k(i)^2\\right)^{1/2}.$$\n       - Let $K^\\star$ be the element maximizing $\\|r_k\\|_K$. Choose the new interpolation index $i_k$ as the index within element $K^\\star$ that maximizes $|r_k(i)|$. Set $\\mathcal{I}_k = \\mathcal{I}_{k-1} \\cup \\{i_k\\}$.\n  2. Global pivoted QR:\n     - Compute a column-pivoted QR factorization of $\\Phi^\\top$, and define the pivot order as a permutation of $\\{1,\\dots,M\\}$. For rank $r$, take the first $r$ pivot indices as $\\mathcal{I}_r$.\n\n- Evaluation protocol:\n  - Define the test parameter set $\\{\\mu^\\mathrm{test}_j\\}_{j=1}^{N_\\mathrm{test}}$ with $N_\\mathrm{test}=4$ values $\\{0.07, 0.31, 0.58, 0.83\\}$.\n  - For each rank $r \\in \\{1,4,8,12\\}$ and for each test parameter $\\mu^\\mathrm{test}_j$:\n    - Form $f = \\mathcal{N}(u(\\cdot;\\mu^\\mathrm{test}_j)) \\in \\mathbb{R}^M$.\n    - Compute the EIM approximation $\\widehat{f}_\\mathrm{loc}$ using the localized greedy indices $\\mathcal{I}^{\\mathrm{loc}}_r$ and $\\widehat{f}_\\mathrm{qr}$ using the pivoted QR indices $\\mathcal{I}^{\\mathrm{qr}}_r$.\n    - Compute the relative error\n      $$e_\\mathrm{loc}(r,\\mu^\\mathrm{test}_j) = \\frac{\\|f - \\widehat{f}_\\mathrm{loc}\\|_2}{\\|f\\|_2}, \\quad e_\\mathrm{qr}(r,\\mu^\\mathrm{test}_j) = \\frac{\\|f - \\widehat{f}_\\mathrm{qr}\\|_2}{\\|f\\|_2}.$$\n  - For each rank $r$, compute the mean relative error over the test set for both methods:\n    $$\\overline{e}_\\mathrm{loc}(r) = \\frac{1}{N_\\mathrm{test}} \\sum_{j=1}^{N_\\mathrm{test}} e_\\mathrm{loc}(r,\\mu^\\mathrm{test}_j), \\quad \\overline{e}_\\mathrm{qr}(r) = \\frac{1}{N_\\mathrm{test}} \\sum_{j=1}^{N_\\mathrm{test}} e_\\mathrm{qr}(r,\\mu^\\mathrm{test}_j).$$\n\nYour tasks:\n- Implement the above pipeline fully and deterministically.\n- Use only linear algebra operations that are well-defined for the specified matrices and vectors. If a linear solve is ill-conditioned or singular at any step, use a least-squares solution instead of failing.\n- You must structure your program to produce the following final outputs for the ranks $r \\in \\{1,4,8,12\\}$, in this exact order:\n  $$\\left[\\overline{e}_\\mathrm{loc}(1), \\overline{e}_\\mathrm{qr}(1), \\overline{e}_\\mathrm{loc}(4), \\overline{e}_\\mathrm{qr}(4), \\overline{e}_\\mathrm{loc}(8), \\overline{e}_\\mathrm{qr}(8), \\overline{e}_\\mathrm{loc}(12), \\overline{e}_\\mathrm{qr}(12)\\right].$$\n\nTest suite and answer specification:\n- You must use the exact parameter values, spatial discretization, and ranks as specified above.\n- The output for each test case must be a float. The final output format must be a single line containing a Python-style list of eight floats in the exact order described, with no units and no additional text. The program must run without any user input and must not rely on any external files or network resources.",
            "solution": "The user-provided problem is a well-defined task in numerical analysis, specifically concerning model order reduction and the empirical interpolation method (EIM). It is scientifically grounded, self-contained, and algorithmically specified. The problem is deemed valid.\n\nThe solution is constructed by following the specified pipeline step-by-step.\n\n### 1. Discretization and Problem Setup\n\nFirst, we establish the computational domain and discretization. The spatial domain is $[0,1]$. It is partitioned into $E=10$ elements, denoted $K_e = [(e-1)/E, e/E]$ for $e=1, \\dots, 10$. Within each element, we place $n_p=6$ collocation nodes uniformly in the element's interior. The coordinate of the $j$-th node (with $j=1, \\dots, n_p$) in element $K_e$ is given by $x_{e,j} = \\frac{e-1}{E} + j \\frac{1/E}{n_p+1}$. This results in a set of $M = E \\cdot n_p = 60$ distinct global nodes, whose coordinates are denoted by $x_i$ for $i=1, \\dots, M$. A mapping $K(i)$ associates each global node index $i$ with its corresponding element index.\n\nThe parametric state function $u(x;\\mu)$ and the nonlinear operator $\\mathcal{N}(u)$ are defined as:\n$$u(x;\\mu) = \\sin\\!\\big(2\\pi(x+0.3\\mu)\\big) + \\tfrac{1}{2}\\cos\\!\\big(5\\pi(x-0.2\\mu)\\big) + 0.2\\exp\\!\\big(-50(x-0.3-0.4\\mu)^2\\big)$$\n$$\\mathcal{N}(u) = u^2 + \\exp(u)$$\nwhere $\\mu \\in [0,1]$ is a scalar parameter.\n\n### 2. Reduced Basis Generation\n\nA reduced basis for the nonlinear output is generated using the method of snapshots.\nThe training set for the parameter $\\mu$ is an equidistant grid of $N_{\\mathrm{train}}=24$ points in $[0,1]$, denoted $\\{\\mu_k\\}_{k=1}^{24}$.\nFor each $\\mu_k$, we evaluate the nonlinear operator at all global nodes $x_i$ to form a snapshot vector $f_k \\in \\mathbb{R}^M$, where $(f_k)_i = \\mathcal{N}(u(x_i; \\mu_k))$.\nThese snapshots are collected as columns of the snapshot matrix $S = [f_1 | f_2 | \\dots | f_{N_{\\mathrm{train}}}] \\in \\mathbb{R}^{M \\times N_{\\mathrm{train}}}$.\n\nWe then compute the thin Singular Value Decomposition (SVD) of the snapshot matrix: $S = U \\Sigma V^\\top$. The left singular vectors, the columns of $U \\in \\mathbb{R}^{M \\times N_{\\mathrm{train}}}$, form an optimal orthonormal basis for the space spanned by the snapshots. The reduced basis $\\Phi \\in \\mathbb{R}^{M \\times r_{\\max}}$ is constructed by taking the first $r_{\\max}=12$ columns of $U$, i.e., $\\Phi = [\\phi_1, \\dots, \\phi_{r_{\\max}}]$. A rank-$r$ basis is denoted $\\Phi_r = [\\phi_1, \\dots, \\phi_r]$.\n\n### 3. Point Selection Strategies\n\nThe core of the task is to select a set of $r$ interpolation points (or global node indices) $\\mathcal{I}_r = \\{i_1, \\dots, i_r\\}$ from the $M$ available nodes. We implement and compare two strategies.\n\n#### 3.1. Localized Greedy Strategy\n\nThis is an iterative procedure. For $k=1, \\dots, r_{\\max}$, we select the $k$-th index $i_k$. The selection is based on the residual vector $r_k = \\phi_k - \\widehat{\\phi}_k$, where $\\widehat{\\phi}_k$ is the EIM approximation of the $k$-th basis vector $\\phi_k$ using the previously selected $k-1$ indices.\nFor $k=1$, no indices have been selected, so the residual is simply $r_1 = \\phi_1$.\nFor $k>1$, given the set of indices $\\mathcal{I}_{k-1}=\\{i_1, \\dots, i_{k-1}\\}$, the approximation is:\n$$\\widehat{\\phi}_k = \\Phi_{k-1} \\left(P_{\\mathcal{I}_{k-1}}^\\top \\Phi_{k-1}\\right)^{-1} P_{\\mathcal{I}_{k-1}}^\\top \\phi_k$$\nwhere $P_{\\mathcal{I}_{k-1}}$ is the operator that samples a vector at the indices in $\\mathcal{I}_{k-1}$. The linear system for the coefficients is solved using a least-squares approach (`numpy.linalg.lstsq`) for numerical stability.\n\nOnce the residual $r_k$ is computed, we calculate its element-wise $L_2$ norm for each element $K$: $\\|r_k\\|_K = (\\sum_{i: K(i)=K} r_k(i)^2)^{1/2}$. We identify the element $K^\\star$ that maximizes this norm. The new index $i_k$ is then chosen as the node within $K^\\star$ where the absolute value of the residual, $|r_k(i)|$, is maximized. The index set is updated: $\\mathcal{I}_k = \\mathcal{I}_{k-1} \\cup \\{i_k\\}$. This process is repeated for $k=1, \\dots, r_{\\max}$ to generate the ordered list of indices $\\mathcal{I}^{\\mathrm{loc}}_{r_{\\max}}$.\n\n#### 3.2. Global Pivoted QR Strategy\n\nThis strategy leverages the properties of a column-pivoted QR factorization. It is a non-iterative, direct method. We compute the column-pivoted QR factorization of the transpose of the basis matrix, $\\Phi^\\top$. The factorization is $\\Phi^\\top P = QR$, where $P$ is a permutation matrix that reorders the columns of $\\Phi^\\top$ to ensure a well-conditioned, upper-triangular $R$. The columns of $\\Phi^\\top$ correspond to the global nodes $\\{1, \\dots, M\\}$. The permutation $P$ thus defines a greedy selection of the most linearly independent rows of $\\Phi$. The pivot indices, obtained from the permutation, are taken as the interpolation points. For a rank-$r$ approximation, we use the first $r$ pivot indices. This procedure generates the ordered list of indices $\\mathcal{I}^{\\mathrm{qr}}_{r_{\\max}}$.\n\n### 4. Evaluation of EIM Approximations\n\nWe evaluate the accuracy of the EIM for both sets of selected points. The rank-$r$ EIM approximation of a vector $f \\in \\mathbb{R}^M$ is given by:\n$$\\mathcal{P}_r(f) = \\Phi_r \\left(P_{\\mathcal{I}_r}^\\top \\Phi_r\\right)^{-1} P_{\\mathcal{I}_r}^\\top f$$\nAgain, the inverse is computed via a robust least-squares solve of the system $(P_{\\mathcal{I}_r}^\\top \\Phi_r)c = P_{\\mathcal{I}_r}^\\top f$ for the coefficients $c$, followed by reconstruction $\\mathcal{P}_r(f) = \\Phi_r c$.\n\nThe evaluation uses a test set of parameters $\\mu^\\mathrm{test} = \\{0.07, 0.31, 0.58, 0.83\\}$. For each rank $r \\in \\{1, 4, 8, 12\\}$ and each $\\mu^\\mathrm{test}_j$, we perform the following:\n1.  Compute the \"true\" output vector $f = \\mathcal{N}(u(\\cdot; \\mu^\\mathrm{test}_j))$ at all global nodes.\n2.  Compute the EIM approximation $\\widehat{f}_{\\mathrm{loc}}$ using the localized greedy indices $\\mathcal{I}^{\\mathrm{loc}}_r$.\n3.  Compute the EIM approximation $\\widehat{f}_{\\mathrm{qr}}$ using the pivoted QR indices $\\mathcal{I}^{\\mathrm{qr}}_r$.\n4.  Calculate the relative errors for both methods:\n    $$e_{\\mathrm{loc}}(r,\\mu^\\mathrm{test}_j) = \\frac{\\|f - \\widehat{f}_{\\mathrm{loc}}\\|_2}{\\|f\\|_2}, \\quad e_{\\mathrm{qr}}(r,\\mu^\\mathrm{test}_j) = \\frac{\\|f - \\widehat{f}_{\\mathrm{qr}}\\|_2}{\\|f\\|_2}$$\n\nFinally, for each rank $r$, we compute the mean relative error across the test set for each method, $\\overline{e}_\\mathrm{loc}(r)$ and $\\overline{e}_\\mathrm{qr}(r)$. The final output is an ordered list of these eight values.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import qr\n\ndef solve():\n    \"\"\"\n    Implements and compares two EIM point-selection strategies:\n    1. Localized greedy based on elementwise DG residual norms.\n    2. Global pivoted QR decomposition.\n    \"\"\"\n    # 1. Define constants and parameters\n    E = 10\n    n_p = 6\n    M = E * n_p\n    N_train = 24\n    r_max = 12\n    \n    mu_train = np.linspace(0, 1, N_train)\n    mu_test = np.array([0.07, 0.31, 0.58, 0.83])\n    ranks_eval = [1, 4, 8, 12]\n\n    # 2. Generate global node coordinates\n    x_nodes = np.zeros(M)\n    h = 1.0 / E\n    for e in range(E):\n        x_min = e * h\n        for j in range(n_p):\n            x_nodes[e * n_p + j] = x_min + (j + 1) * h / (n_p + 1)\n\n    # 3. Define the state function and nonlinear operator\n    def u_func(x, mu):\n        term1 = np.sin(2 * np.pi * (x + 0.3 * mu))\n        term2 = 0.5 * np.cos(5 * np.pi * (x - 0.2 * mu))\n        term3 = 0.2 * np.exp(-50 * (x - 0.3 - 0.4 * mu)**2)\n        return term1 + term2 + term3\n\n    def N_op(u_val):\n        return u_val**2 + np.exp(u_val)\n\n    # 4. Construct the snapshot matrix\n    S = np.zeros((M, N_train))\n    for k, mu in enumerate(mu_train):\n        u_vals = u_func(x_nodes, mu)\n        S[:, k] = N_op(u_vals)\n\n    # 5. Compute SVD and extract the reduced basis Phi\n    U, _, _ = np.linalg.svd(S, full_matrices=False)\n    Phi = U[:, :r_max]\n\n    # 6. Point Selection Strategy 1: Localized Greedy\n    indices_loc = []\n    for k_idx in range(r_max):\n        phi_k = Phi[:, k_idx]\n        \n        if k_idx == 0:\n            residual = phi_k\n        else:\n            Phi_prev = Phi[:, :k_idx]\n            # Solve (P^T Phi_prev) c = P^T phi_k for coefficients c\n            A = Phi_prev[indices_loc, :]\n            b = phi_k[indices_loc]\n            # Use least-squares for robustness as per problem spec\n            coeffs, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\n            phi_hat = Phi_prev @ coeffs\n            residual = phi_k - phi_hat\n        \n        # Compute element-wise residual L2 norms\n        # Reshape residual into (E, n_p) to calculate norm per element\n        res_reshaped = residual.reshape((E, n_p))\n        element_norms = np.linalg.norm(res_reshaped, axis=1)\n        \n        # Find element with the maximum residual norm\n        best_element_idx = np.argmax(element_norms)\n        \n        # Find index within that element that maximizes |residual|\n        start_idx = best_element_idx * n_p\n        end_idx = start_idx + n_p\n        local_max_idx = np.argmax(np.abs(residual[start_idx:end_idx]))\n        new_index = start_idx + local_max_idx\n        \n        indices_loc.append(new_index)\n\n    # 7. Point Selection Strategy 2: Global Pivoted QR\n    # Column-pivoted QR of Phi^T gives a row pivot selection for Phi\n    _, _, p = qr(Phi.T, pivoting=True)\n    indices_qr = p[:r_max]\n\n    # 8. Evaluation Protocol\n    final_results = []\n    for r in ranks_eval:\n        errors_loc = []\n        errors_qr = []\n\n        # Get the basis and index sets for the current rank r\n        Phi_r = Phi[:, :r]\n        I_loc_r = indices_loc[:r]\n        I_qr_r = indices_qr[:r]\n\n        for mu_j in mu_test:\n            # Generate the full-order model output for the test parameter\n            f = N_op(u_func(x_nodes, mu_j))\n            f_norm = np.linalg.norm(f)\n\n            # --- Localized Greedy Method Evaluation ---\n            A_loc = Phi_r[I_loc_r, :]\n            b_loc = f[I_loc_r]\n            coeffs_loc, _, _, _ = np.linalg.lstsq(A_loc, b_loc, rcond=None)\n            f_hat_loc = Phi_r @ coeffs_loc\n            err_loc = np.linalg.norm(f - f_hat_loc) / f_norm\n            errors_loc.append(err_loc)\n\n            # --- Pivoted QR Method Evaluation ---\n            A_qr = Phi_r[I_qr_r, :]\n            b_qr = f[I_qr_r]\n            coeffs_qr, _, _, _ = np.linalg.lstsq(A_qr, b_qr, rcond=None)\n            f_hat_qr = Phi_r @ coeffs_qr\n            err_qr = np.linalg.norm(f - f_hat_qr) / f_norm\n            errors_qr.append(err_qr)\n        \n        # Compute mean errors for rank r and append to results\n        avg_err_loc = np.mean(errors_loc)\n        avg_err_qr = np.mean(errors_qr)\n        final_results.extend([avg_err_loc, avg_err_qr])\n\n    # 9. Print the final results in the specified format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The performance of any data-driven model, including EIM, is fundamentally tied to the quality of its training data. This final exercise  explores this critical link by investigating how the choice of training \"snapshots\" affects the generalization ability of an EIM surrogate. By comparing models trained on steady-state versus transient data for physically-inspired problems, you will gain insight into how to build more robust and accurate reduced-order models that perform well on unseen parameters.",
            "id": "3383626",
            "problem": "Consider the discrete Empirical Interpolation Method (EIM) for approximating nonlinear operators appearing in spectral and Discontinuous Galerkin (DG) methods. The goal is to construct an interpolation-based surrogate for a nonlinear operator that maps discrete fields to discrete residuals, using a set of training snapshots. You will investigate the effect of including transient snapshots on generalization error to untrained Mach numbers, using two prototypical flow surrogates labeled to reflect canonical compressible Euler test problems: an entropy-stable shock-tube surrogate and an isentropic vortex surrogate.\n\nThe fundamental base consists of the following widely accepted definitions and facts:\n- The Discontinuous Galerkin (DG) method is a finite element method using discontinuous polynomial spaces, which for nonlinear conservation laws often employs split-form discretizations and entropy-stable numerical fluxes to control aliasing and ensure stability. In the one-dimensional case, for a scalar field $u(x)$ with smooth test functions, a nonlinear flux derivative is represented on a spectral grid by a discrete differentiation matrix applied to a nonlinear function of $u$.\n- Spectral methods use global basis functions and collocation at special nodes. Chebyshev-Gauss-Lobatto nodes are defined for an integer $N \\geq 1$ by $x_j = \\cos(\\pi j / N)$ for $j = 0, 1, \\dots, N$, which lie in $[-1,1]$. The spectral differentiation matrix $D \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ associated with these nodes has entries\n$$\nD_{ij} = \\begin{cases}\n\\frac{c_i}{c_j} \\frac{(-1)^{i+j}}{x_i - x_j},  i \\neq j,\\\\\n-\\frac{x_i}{2(1-x_i^2)},  1 \\leq i = j \\leq N-1,\\\\\n\\frac{2N^2+1}{6},  i=j=0,\\\\\n-\\frac{2N^2+1}{6},  i=j=N,\n\\end{cases}\n$$\nwhere $c_0 = c_N = 2$ and $c_j = 1$ for $1 \\leq j \\leq N-1$.\n- The Empirical Interpolation Method (EIM) approximates a nonlinear operator $\\mathcal{N} : \\mathbb{R}^Q \\rightarrow \\mathbb{R}^Q$ by projecting onto a subspace spanned by empirical basis vectors and determining coefficients by enforcing interpolation at selected indices. A standard practical procedure uses Proper Orthogonal Decomposition (POD) to extract leading modes from snapshots and the Discrete Empirical Interpolation Method (DEIM) to greedily select interpolation indices. Given a snapshot matrix $F \\in \\mathbb{R}^{Q \\times S}$ and its singular value decomposition $F = U \\Sigma V^\\top$, the first $r$ left singular vectors $U_r \\in \\mathbb{R}^{Q \\times r}$ serve as a basis. DEIM selects indices $\\{p_1, \\dots, p_r\\}$ by a greedy algorithm based on maximizing absolute residuals of successive projections of $U_r$ onto previously selected interpolation constraints. The EIM approximation of a new vector $f \\in \\mathbb{R}^Q$ is $\\hat{f} = U_r c$, where the coefficient vector $c \\in \\mathbb{R}^r$ solves the square linear system $U_r[P,:] c = f[P]$ with $P = [p_1,\\dots,p_r]$ and $U_r[P,:]$ the submatrix at those rows.\n\nIn this problem, define two surrogate nonlinear operators on Chebyshev-Gauss-Lobatto grids:\n\n1. Entropy-stable shock-tube surrogate in one dimension:\n   - Domain: $x \\in [-1,1]$, with $N_x = 64$ giving $65$ nodes.\n   - Mach number parameter: $M \\in \\mathbb{R}_{0}$.\n   - Time parameter: $t \\in \\mathbb{R}_{\\geq 0}$.\n   - Surrogate field:\n     $$\n     \\phi_{\\mathrm{shock}}(x; M, t) = \\frac{M}{2} \\left[1 + \\tanh\\!\\left(k(M)\\,(x - s(M)\\,t)\\right)\\right] + 0.05\\,M\\,\\sin(8\\pi x)\\,e^{-t},\n     $$\n     with $k(M) = 6M$ and $s(M) = \\frac{M}{2}$. The added decaying high-frequency term models transient content.\n   - Nonlinear operator:\n     $$\n     \\mathcal{N}_{\\mathrm{shock}}(M,t) = D \\left(\\frac{\\phi_{\\mathrm{shock}}(\\cdot;M,t)^2}{2}\\right),\n     $$\n     where $D$ is the Chebyshev-Gauss-Lobatto differentiation matrix.\n\n2. Isentropic vortex surrogate in two dimensions:\n   - Domain: $(x,y) \\in [-1,1] \\times [-1,1]$, with $N_y = 32$ giving $33$ nodes per coordinate and a tensor-product collocation grid.\n   - Mach number parameter: $M \\in \\mathbb{R}_{0}$.\n   - Time parameter: $t \\in \\mathbb{R}_{\\geq 0}$.\n   - Surrogate field:\n     $$\n     \\phi_{\\mathrm{vortex}}(x,y; M,t) = M\\,\\exp\\!\\left(-\\alpha(M)\\left[(x - v_x(M)\\,t)^2 + (y - v_y(M)\\,t)^2\\right]\\right)\\,\\sin(\\pi x)\\,\\cos(\\pi y) + 0.05\\,M\\,\\sin(6\\pi(x+y))\\,e^{-t},\n     $$\n     with $\\alpha(M) = 1 + M$, $v_x(M) = v_y(M) = \\frac{M}{2}$. The Gaussian envelope and trigonometric factors model a moving swirling structure with transient high-frequency content.\n   - Nonlinear operator:\n     $$\n     \\mathcal{N}_{\\mathrm{vortex}}(M,t) = \\partial_x\\!\\left(\\phi_{\\mathrm{vortex}}^2\\right) + \\partial_y\\!\\left(\\phi_{\\mathrm{vortex}}^2\\right),\n     $$\n     computed by applying the one-dimensional differentiation matrix $D$ in each coordinate: if $G = \\phi_{\\mathrm{vortex}}^2$ arranged as a matrix over the tensor grid, then $\\partial_x(G) = D\\,G$ and $\\partial_y(G) = G\\,D^\\top$, flattened into a vector.\n\nTraining strategies:\n- Strategy $\\mathrm{S0}$ (steady-dominated): use training times $t \\in \\{0.3,\\,0.4,\\,0.5\\}$.\n- Strategy $\\mathrm{S1}$ (transient-inclusive): use training times $t \\in \\{0.05,\\,0.2,\\,0.5\\}$.\n\nFor each surrogate scenario, use the following training Mach numbers:\n- Shock-tube training Machs: $M \\in \\{1.2,\\,2.0\\}$.\n- Vortex training Machs: $M \\in \\{0.4,\\,0.8\\}$.\n\nConstruct the snapshot matrix for each strategy by concatenating columns $\\mathcal{N}(M,t)$ for all specified $(M,t)$ pairs. From this matrix, build a rank-$r$ POD basis with $r=6$ (if fewer than $6$ singular values exist, take $r$ equal to the number of available singular values) and then construct DEIM interpolation indices. Use these to define two EIM models per scenario: one for $\\mathrm{S0}$ and one for $\\mathrm{S1}$.\n\nDefine the generalization error for an untrained Mach number $M^\\star$ and a time $t^\\star$ as the relative Euclidean norm error\n$$\n\\epsilon(M^\\star, t^\\star) = \\frac{\\left\\| \\mathcal{N}(M^\\star,t^\\star) - \\widehat{\\mathcal{N}}(M^\\star,t^\\star)\\right\\|_2}{\\left\\|\\mathcal{N}(M^\\star,t^\\star)\\right\\|_2},\n$$\nwhere $\\widehat{\\mathcal{N}}$ is the EIM reconstruction with the chosen strategy.\n\nTest suite:\n- Shock-tube tests (one-dimensional):\n  1. Untrained Mach $M^\\star = 1.6$ at $t^\\star = 0.15$.\n  2. Untrained Mach $M^\\star = 3.0$ at $t^\\star = 0.15$ (edge case at higher Mach).\n- Isentropic vortex tests (two-dimensional):\n  3. Untrained Mach $M^\\star = 0.6$ at $t^\\star = 0.15$.\n  4. Untrained Mach $M^\\star = 0.2$ at $t^\\star = 0.15$ (edge case at lower Mach).\n\nFor each test case, compute the improvement\n$$\n\\Delta = \\epsilon_{\\mathrm{S0}}(M^\\star, t^\\star) - \\epsilon_{\\mathrm{S1}}(M^\\star, t^\\star),\n$$\nwhich is a real number. Report all four improvements in the specified order. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Angles do not appear and no physical units are required since all quantities are dimensionless.\n\nImplement the above setup exactly:\n- Use Chebyshev-Gauss-Lobatto nodes and the differentiation matrix as specified.\n- Use $N_x = 64$ in one dimension and $N_y = 32$ per coordinate in two dimensions.\n- Use the training times and Mach numbers as described.\n- Use a POD rank $r=6$ for both strategies, truncated to the number of snapshots if fewer than $6$ snapshots are available.\n\nThe final output must be the list of four floating-point improvements $\\Delta$, in the order of the test suite: shock-tube at $M^\\star=1.6$, shock-tube at $M^\\star=3.0$, vortex at $M^\\star=0.6$, vortex at $M^\\star=0.2$.",
            "solution": "The user has provided a problem that requires the implementation and comparison of two training strategies for the Empirical Interpolation Method (EIM) applied to nonlinear operators arising in fluid dynamics simulations. The problem is well-defined, scientifically sound, and numerically tractable. All necessary definitions, parameters, and procedures are specified. Therefore, the problem is deemed valid.\n\nThe solution proceeds by first implementing the necessary mathematical constructs, then building the specified EIM models, and finally evaluating their performance on the given test cases to compute the requested improvement metric.\n\n### Step 1: Mathematical and Algorithmic Preliminaries\n\nThe core components required are the Chebyshev-Gauss-Lobatto (CGL) nodes and the associated spectral differentiation matrix, the surrogate nonlinear operators, and the Proper Orthogonal Decomposition-Discrete Empirical Interpolation Method (POD-DEIM) algorithm.\n\n**Chebyshev-Gauss-Lobatto Grid and Differentiation Matrix**\n\nFor a polynomial degree $N$, the $N+1$ CGL nodes on the interval $[-1, 1]$ are given by:\n$$\nx_j = \\cos\\left(\\frac{\\pi j}{N}\\right), \\quad j = 0, 1, \\dots, N\n$$\nThe spectral differentiation matrix $D \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ that exactly differentiates polynomials of degree up to $N$ at these nodes is defined by its entries $D_{ij}$. For $i \\neq j$:\n$$\nD_{ij} = \\frac{c_i}{c_j} \\frac{(-1)^{i+j}}{x_i - x_j}\n$$\nwhere $c_0 = c_N = 2$ and $c_j = 1$ for $1 \\leq j \\leq N-1$. The diagonal entries are:\n$$\nD_{jj} = \\begin{cases}\n\\frac{2N^2+1}{6},  j=0, \\\\\n-\\frac{x_j}{2(1-x_j^2)},  1 \\leq j \\leq N-1, \\\\\n-\\frac{2N^2+1}{6},  j=N.\n\\end{cases}\n$$\nThis matrix will be constructed for $N=N_x=64$ for the one-dimensional shock-tube problem and for $N=N_y=32$ for the two-dimensional a vortex problem.\n\n### Step 2: Surrogate Nonlinear Operators\n\nTwo prototypical nonlinear operators are defined on these spectral grids.\n\n**1. One-Dimensional Shock-Tube Surrogate ($\\mathcal{N}_{\\mathrm{shock}}$)**\nThe surrogate field $\\phi_{\\mathrm{shock}}$ is evaluated on the 1D CGL grid with $N_x=64$, yielding a vector of length $Q = N_x+1 = 65$. The field is given by:\n$$\n\\phi_{\\mathrm{shock}}(x; M, t) = \\frac{M}{2} \\left[1 + \\tanh\\left(k(M)(x - s(M)t)\\right)\\right] + 0.05 M \\sin(8\\pi x) e^{-t}\n$$\nwith $k(M) = 6M$ and $s(M) = M/2$. The nonlinear operator applies the differentiation matrix to the vector representing the flux $f(\\phi) = \\phi^2/2$:\n$$\n\\mathcal{N}_{\\mathrm{shock}}(M,t) = D \\left(\\frac{\\phi_{\\mathrm{shock}}(\\cdot; M,t)^2}{2}\\right)\n$$\nHere, the squaring and division are performed element-wise on the vector of field values.\n\n**2. Two-Dimensional Isentropic Vortex Surrogate ($\\mathcal{N}_{\\mathrm{vortex}}$)**\nThe surrogate field $\\phi_{\\mathrm{vortex}}$ is evaluated on a $2D$ tensor-product CGL grid with $N_y=32$ in each dimension, resulting in a $(N_y+1) \\times (N_y+1) = 33 \\times 33$ grid of points. The total number of degrees of freedom is $Q = 33^2 = 1089$. The field is:\n$$\n\\phi_{\\mathrm{vortex}}(x,y; M,t) = M e^{-\\alpha(M)((x - v_x(M)t)^2 + (y - v_y(M)t)^2)}\\sin(\\pi x)\\cos(\\pi y) + 0.05 M \\sin(6\\pi(x+y))e^{-t}\n$$\nwith $\\alpha(M) = 1+M$ and $v_x(M) = v_y(M) = M/2$. The nonlinear operator is a discrete divergence of a quadratic flux:\n$$\n\\mathcal{N}_{\\mathrm{vortex}}(M,t) = \\partial_x(\\phi_{\\mathrm{vortex}}^2) + \\partial_y(\\phi_{\\mathrm{vortex}}^2)\n$$\nLetting $G$ be the matrix of $\\phi_{\\mathrm{vortex}}^2$ values on the grid, the partial derivatives are computed via matrix multiplication with the 1D differentiation matrix $D$ (of size $33 \\times 33$). Following the problem's explicit instruction, we compute $\\partial_x G = D G$ and $\\partial_y G = G D^\\top$. The results are summed and flattened into a single vector of length $Q=1089$.\n\n### Step 3: POD-DEIM Model Construction\n\nFor a given scenario (shock or vortex) and a training strategy (S0 or S1), an EIM model is constructed as follows:\n\n1.  **Snapshot Collection**: The nonlinear operator $\\mathcal{N}(M, t)$ is evaluated for each pair of training Mach number $M$ and time $t$ specified by the strategy. The resulting vectors are collected as columns of a snapshot matrix $F \\in \\mathbb{R}^{Q \\times S}$, where $S$ is the number of snapshots ($S=6$ in this problem).\n2.  **POD Basis Generation**: The Singular Value Decomposition (SVD) of the snapshot matrix is computed: $F = U \\Sigma V^\\top$. The first $r=6$ columns of the left singular vector matrix $U$ are selected as the POD basis, denoted $U_r \\in \\mathbb{R}^{Q \\times r}$.\n3.  **DEIM Index Selection**: The Discrete Empirical Interpolation Method (DEIM) is used to select $r$ interpolation indices $P = \\{p_1, \\dots, p_r\\}$. This is a greedy algorithm:\n    *   The first index $p_1$ is chosen to be the location of the maximum absolute value of the first basis vector $u_1$.\n    *   For $k=2, \\dots, r$, the $k$-th basis vector $u_k$ is approximated by its projection onto the span of the previous $k-1$ basis vectors, with coefficients determined by interpolation at the previously selected indices $\\{p_1, \\dots, p_{k-1}\\}$. The residual of this approximation is computed, and the new index $p_k$ is chosen as the location of the maximum absolute value of this residual.\n    This procedure yields a set of $r$ indices $P$.\n\nThe EIM model is defined by the pair $(U_r, P)$.\n\n### Step 4: EIM Approximation and Error Evaluation\n\nGiven a new state vector $f_{\\mathrm{new}} = \\mathcal{N}(M^\\star, t^\\star)$ for an untrained parameter set, the EIM approximation $\\hat{f}_{\\mathrm{new}}$ is computed.\n\n1.  **Approximation**: The approximation is $\\hat{f}_{\\mathrm{new}} = U_r c$, where the coefficient vector $c \\in \\mathbb{R}^r$ is found by solving the small square linear system that enforces interpolation at the DEIM indices:\n    $$\n    (U_r)_{P,:} c = (f_{\\mathrm{new}})_P\n    $$\n    Here, $(U_r)_{P,:}$ is the $r \\times r$ matrix formed by selecting the rows of $U_r$ corresponding to indices in $P$, and $(f_{\\mathrm{new}})_P$ is the vector formed by selecting the corresponding entries of $f_{\\mathrm{new}}$.\n2.  **Error Calculation**: The generalization error for a given strategy is the relative Euclidean norm of the difference between the true operator output and the EIM approximation:\n    $$\n    \\epsilon(M^\\star, t^\\star) = \\frac{\\| \\mathcal{N}(M^\\star, t^\\star) - \\widehat{\\mathcal{N}}(M^\\star, t^\\star) \\|_2}{\\| \\mathcal{N}(M^\\star, t^\\star) \\|_2}\n    $$\n3.  **Improvement Metric**: The final quantity of interest is the improvement of the transient-inclusive strategy (S1) over the steady-dominated one (S0), measured by the difference in their errors:\n    $$\n    \\Delta = \\epsilon_{\\mathrm{S0}}(M^\\star, t^\\star) - \\epsilon_{\\mathrm{S1}}(M^\\star, t^\\star)\n    $$\n\nThis procedure is carried out for each of the four test cases specified in the problem statement. The expected outcome is that $\\Delta  0$, indicating that strategy S1, which was exposed to transient dynamics during training, provides a better approximation for the transient test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\n# This solution was executed with numpy==1.23.5 and scipy==1.11.4\n\ndef get_chebyshev_diff_matrix(N: int) -> np.ndarray:\n    \"\"\"\n    Computes the Chebyshev spectral differentiation matrix for N+1\n    Chebyshev-Gauss-Lobatto nodes.\n    \"\"\"\n    if N == 0:\n        return np.zeros((1, 1))\n    \n    x = np.cos(np.pi * np.arange(N + 1) / N)\n    D = np.zeros((N + 1, N + 1))\n    \n    c = np.ones(N + 1)\n    c[0] = 2.0\n    c[N] = 2.0\n\n    for i in range(N + 1):\n        for j in range(N + 1):\n            if i == j:\n                if i == 0:\n                    D[i, j] = (2 * N**2 + 1) / 6.0\n                elif i == N:\n                    D[i, j] = -(2 * N**2 + 1) / 6.0\n                else:\n                    D[i, j] = -x[j] / (2 * (1 - x[j]**2))\n            else:\n                D[i, j] = (c[i] / c[j]) * ((-1)**(i + j)) / (x[i] - x[j])\n    return D\n\ndef deim(U: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs the Discrete Empirical Interpolation Method (DEIM) to select\n    interpolation indices given a basis U.\n    \"\"\"\n    Q, r = U.shape\n    P = np.zeros(r, dtype=int)\n    \n    # First point\n    p1_idx = np.argmax(np.abs(U[:, 0]))\n    P[0] = p1_idx\n    \n    for k in range(1, r):\n        u_k = U[:, k]\n        \n        # Solve for coefficients\n        U_P_k_minus_1 = U[P[:k], :k]\n        u_k_P = u_k[P[:k]]\n        \n        try:\n            coeffs = linalg.solve(U_P_k_minus_1, u_k_P)\n        except linalg.LinAlgError:\n            # Fallback to pseudo-inverse if matrix is singular.\n            # This can happen if basis vectors are nearly linearly dependent at selected points.\n            coeffs = linalg.lstsq(U_P_k_minus_1, u_k_P)[0]\n\n        # Compute residual\n        residual = u_k - U[:, :k] @ coeffs\n        \n        # Select next point\n        p_k_idx = np.argmax(np.abs(residual))\n        P[k] = p_k_idx\n        \n    return P\n\ndef shock_phi(x: np.ndarray, M: float, t: float) -> np.ndarray:\n    k = 6.0 * M\n    s = M / 2.0\n    phi = M / 2.0 * (1.0 + np.tanh(k * (x - s * t))) + 0.05 * M * np.sin(8.0 * np.pi * x) * np.exp(-t)\n    return phi\n\ndef shock_op(M: float, t: float, D: np.ndarray, x: np.ndarray) -> np.ndarray:\n    phi = shock_phi(x, M, t)\n    flux = 0.5 * phi**2\n    return D @ flux\n\ndef vortex_phi(X: np.ndarray, Y: np.ndarray, M: float, t: float) -> np.ndarray:\n    alpha = 1.0 + M\n    v_x = v_y = M / 2.0\n    term1 = M * np.exp(-alpha * ((X - v_x * t)**2 + (Y - v_y * t)**2)) * np.sin(np.pi * X) * np.cos(np.pi * Y)\n    term2 = 0.05 * M * np.sin(6.0 * np.pi * (X + Y)) * np.exp(-t)\n    return term1 + term2\n\ndef vortex_op(M: float, t: float, D: np.ndarray, X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    phi_grid = vortex_phi(X, Y, M, t)\n    G = phi_grid**2\n    # Per problem statement: d_x(G) = D G, d_y(G) = G D^T\n    dG_dx = D @ G\n    dG_dy = G @ D.T\n    op_grid = dG_dx + dG_dy\n    return op_grid.ravel()\n\ndef build_eim_model(scenario: str, strategy: str, params: dict):\n    \"\"\"\n    Builds the POD-DEIM model for a given scenario and strategy.\n    \"\"\"\n    r = params['r']\n    train_times = params['train_times'][strategy]\n    train_machs = params['train_machs'][scenario]\n\n    snapshots = []\n    if scenario == 'shock':\n        N = params['N_shock']\n        D = params['D_shock']\n        x = params['x_shock']\n        for M_train in train_machs:\n            for t_train in train_times:\n                snapshot = shock_op(M_train, t_train, D, x)\n                snapshots.append(snapshot)\n    elif scenario == 'vortex':\n        N = params['N_vortex']\n        D = params['D_vortex']\n        X, Y = params['grid_vortex']\n        for M_train in train_machs:\n            for t_train in train_times:\n                snapshot = vortex_op(M_train, t_train, D, X, Y)\n                snapshots.append(snapshot)\n    \n    F = np.array(snapshots).T\n    \n    # POD basis\n    U, s, Vt = linalg.svd(F, full_matrices=False)\n    num_snaps = F.shape[1]\n    rank = min(r, num_snaps) # Use at most r basis vectors\n    U_r = U[:, :rank]\n    \n    # DEIM indices\n    P = deim(U_r)\n    \n    return U_r, P\n\ndef apply_eim_model(f: np.ndarray, U_r: np.ndarray, P: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Applies the EIM model to approximate a vector f.\n    \"\"\"\n    U_r_P = U_r[P, :]\n    f_P = f[P]\n    \n    try:\n        c = linalg.solve(U_r_P, f_P)\n    except linalg.LinAlgError:\n        c = linalg.lstsq(U_r_P, f_P)[0]\n    \n    f_hat = U_r @ c\n    return f_hat\n\ndef calculate_error(f, f_hat):\n    \"\"\"Computes the relative L2 error.\"\"\"\n    norm_f = linalg.norm(f)\n    if norm_f == 0:\n        return 0.0 if linalg.norm(f_hat) == 0 else 1.0\n    return linalg.norm(f - f_hat) / norm_f\n\ndef solve():\n    params = {\n        'N_shock': 64,\n        'N_vortex': 32,\n        'r': 6,\n        'train_times': {\n            'S0': [0.3, 0.4, 0.5],\n            'S1': [0.05, 0.2, 0.5]\n        },\n        'train_machs': {\n            'shock': [1.2, 2.0],\n            'vortex': [0.4, 0.8]\n        },\n        'test_cases': [\n            {'scenario': 'shock', 'M_star': 1.6, 't_star': 0.15},\n            {'scenario': 'shock', 'M_star': 3.0, 't_star': 0.15},\n            {'scenario': 'vortex', 'M_star': 0.6, 't_star': 0.15},\n            {'scenario': 'vortex', 'M_star': 0.2, 't_star': 0.15},\n        ]\n    }\n\n    # Pre-compute shared data\n    N_shock = params['N_shock']\n    params['D_shock'] = get_chebyshev_diff_matrix(N_shock)\n    params['x_shock'] = np.cos(np.pi * np.arange(N_shock + 1) / N_shock)\n    \n    N_vortex = params['N_vortex']\n    params['D_vortex'] = get_chebyshev_diff_matrix(N_vortex)\n    vortex_nodes = np.cos(np.pi * np.arange(N_vortex + 1) / N_vortex)\n    params['grid_vortex'] = np.meshgrid(vortex_nodes, vortex_nodes)\n\n    results = []\n    \n    # Group tests by scenario to avoid re-building models\n    scenarios = ['shock', 'vortex']\n    \n    for scenario in scenarios:\n        # Build models for the scenario\n        U_r_s0, P_s0 = build_eim_model(scenario, 'S0', params)\n        U_r_s1, P_s1 = build_eim_model(scenario, 'S1', params)\n\n        # Run tests for this scenario\n        tests_for_scenario = [tc for tc in params['test_cases'] if tc['scenario'] == scenario]\n        \n        for test in tests_for_scenario:\n            M_star, t_star = test['M_star'], test['t_star']\n            \n            # Generate true solution\n            if scenario == 'shock':\n                f_true = shock_op(M_star, t_star, params['D_shock'], params['x_shock'])\n            else: # vortex\n                f_true = vortex_op(M_star, t_star, params['D_vortex'], *params['grid_vortex'])\n\n            # Apply S0 model and get error\n            f_hat_s0 = apply_eim_model(f_true, U_r_s0, P_s0)\n            eps_s0 = calculate_error(f_true, f_hat_s0)\n            \n            # Apply S1 model and get error\n            f_hat_s1 = apply_eim_model(f_true, U_r_s1, P_s1)\n            eps_s1 = calculate_error(f_true, f_hat_s1)\n            \n            # Calculate improvement\n            delta = eps_s0 - eps_s1\n            results.append(delta)\n\n    # Reorder results to match problem statement order\n    # The loop processes tests in the order: shock 1.6, shock 3.0, vortex 0.6, vortex 0.2\n    # This already matches the required output order.\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}