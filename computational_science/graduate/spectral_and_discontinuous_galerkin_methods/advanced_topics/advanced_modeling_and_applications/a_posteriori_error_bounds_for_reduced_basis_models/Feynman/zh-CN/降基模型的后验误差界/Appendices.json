{
    "hands_on_practices": [
        {
            "introduction": "对于强制性问题，后验误差界通常与强制性常数 $\\alpha(\\mu)$ 的倒数成正比。因此，为了确保误差估计的可靠性，计算 $\\alpha(\\mu)$ 的一个严格且可计算的下界至关重要。本练习  将指导您使用逐次约束法 (Successive Constraint Method, SCM) 来解决这一挑战，这是一种针对参数仿射问题的强大技术。通过在一个各向异性扩散模型上应用该方法，您将学习如何将连续的无穷维问题离散化，并将其转化为一个可解的线性规划问题，从而为更复杂的降阶模型误差分析奠定基础。",
            "id": "3361086",
            "problem": "考虑一个在单位正方形域上具有齐次狄利克雷边界条件的参数化各向异性扩散问题。设函数 $u$ 和 $v$ 的双线性形式定义为\n$$\na(u,v;\\mu) = \\int_{\\Omega} \\nabla u(x)^{\\top} A(\\mu) \\nabla v(x) \\, dx,\n$$\n其中 $\\Omega = (0,1)\\times(0,1)$ 且 $A(\\mu)$ 是一个与参数向量 $\\mu$ 相关的常对称正定张量。假设 $A(\\mu)$ 是对角的，其对角元为 $a_x(\\mu)$ 和 $a_y(\\mu)$，因此\n$$\nA(\\mu) = \\begin{pmatrix} a_x(\\mu)  0 \\\\ 0  a_y(\\mu) \\end{pmatrix}, \\quad a_x(\\mu) > 0, \\quad a_y(\\mu) > 0.\n$$\n定义一个由带有单位张量的各向同性双线性形式导出的参考范数，\n$$\n\\|v\\|_V^2 = a_{\\mathrm{ref}}(v,v) = \\int_{\\Omega} \\left( \\left|\\frac{\\partial v}{\\partial x}\\right|^2 + \\left|\\frac{\\partial v}{\\partial y}\\right|^2 \\right) dx,\n$$\n以及矫顽常数\n$$\n\\alpha(\\mu) = \\inf_{v \\in V \\setminus \\{0\\}} \\frac{a(v,v;\\mu)}{\\|v\\|_V^2}.\n$$\n该双线性形式的参数仿射分解为\n$$\na(u,v;\\mu) = \\Theta_1(\\mu) a_1(u,v) + \\Theta_2(\\mu) a_2(u,v),\n$$\n其中 $\\Theta_1(\\mu) = a_x(\\mu)$, $\\Theta_2(\\mu) = a_y(\\mu)$, $a_1(u,v) = \\int_{\\Omega} \\frac{\\partial u}{\\partial x} \\frac{\\partial v}{\\partial x} \\, dx$，以及 $a_2(u,v) = \\int_{\\Omega} \\frac{\\partial u}{\\partial y} \\frac{\\partial v}{\\partial y} \\, dx$。对于任何非零的 $v$，定义分量比\n$$\nr_q(v) = \\frac{a_q(v,v)}{\\|v\\|_V^2}, \\quad q \\in \\{1,2\\}.\n$$\n则 $r_1(v) \\ge 0$, $r_2(v) \\ge 0$，且 $r_1(v) + r_2(v) = 1$。\n\n为获得可计算的离散近似，使用一个由满足齐次狄利克雷边界条件的张量积正弦函数张成的谱伽辽金子空间，\n$$\n\\phi_{k_x,k_y}(x,y) = \\sin(k_x \\pi x)\\sin(k_y \\pi y),\n$$\n其中整数波数 $k_x \\in \\{1,\\dots,N_x\\}$ 和 $k_y \\in \\{1,\\dots,N_y\\}$。在此基中，对于纯模 $\\phi_{k_x,k_y}$，瑞利商为\n$$\n\\mathcal{R}(k_x,k_y;\\mu) = \\frac{a(\\phi_{k_x,k_y},\\phi_{k_x,k_y};\\mu)}{\\|\\phi_{k_x,k_y}\\|_V^2} = \\frac{a_x(\\mu) k_x^2 + a_y(\\mu) k_y^2}{k_x^2 + k_y^2}.\n$$\n因此，在所选谱子空间上的离散“真实”矫顽常数为\n$$\n\\alpha_{\\mathrm{true}}(\\mu) = \\min_{1 \\le k_x \\le N_x,\\; 1 \\le k_y \\le N_y} \\mathcal{R}(k_x,k_y;\\mu).\n$$\n\n使用一组已知离散 $\\alpha_{\\mathrm{true}}(\\mu_i)$ 的参数训练集 $\\{\\mu_i\\}$，为 $\\alpha(\\mu)$ 构建一个逐次约束法 (SCM) 下界。定义一个向量 $y = (y_1,y_2)$ 来近似 $(r_1(v),r_2(v))$ 并建立线性规划\n$$\n\\alpha_{\\mathrm{LB}}(\\mu) = \\min_{y \\in \\mathbb{R}^2} \\;\\Theta_1(\\mu) y_1 + \\Theta_2(\\mu) y_2\n$$\n约束条件为\n$$\ny_1 \\ge 0, \\quad y_2 \\ge 0, \\quad y_1 + y_2 = 1, \\quad \\Theta_1(\\mu_i) y_1 + \\Theta_2(\\mu_i) y_2 \\ge \\alpha_{\\mathrm{true}}(\\mu_i) \\quad \\text{对于所有训练参数 } \\mu_i.\n$$\n此优化产生一个下界 $\\alpha_{\\mathrm{LB}}(\\mu)$，满足 $\\alpha_{\\mathrm{LB}}(\\mu) \\le \\alpha_{\\mathrm{true}}(\\mu)$。\n\n您的任务是使用谱伽辽金离散公式实现 $\\alpha_{\\mathrm{true}}(\\mu)$ 的计算，并使用上述 SCM 线性规划实现 $\\alpha_{\\mathrm{LB}}(\\mu)$ 的计算。然后，通过计算比率来研究当各向异性比增长时 SCM 界的紧致性\n$$\n\\rho(\\mu) = \\frac{\\alpha_{\\mathrm{LB}}(\\mu)}{\\alpha_{\\mathrm{true}}(\\mu)}.\n$$\n\n使用以下有限维谱参数，$N_x = 8$ 和 $N_y = 8$，并定义训练参数集为\n$$\n\\{(a_x,a_y)\\} = \\{(1,1), (1,10), (10,1), (1,100), (100,1), (1,1000), (1000,1)\\}.\n$$\n为以下测试套件评估紧致性比率\n$$\n\\text{测试 }1:\\; (a_x,a_y)=(1,1), \\quad \\text{测试 }2:\\; (a_x,a_y)=(1,10), \\quad \\text{测试 }3:\\; (a_x,a_y)=(1,100), \\quad \\text{测试 }4:\\; (a_x,a_y)=(1,1000).\n$$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，按四个测试的顺序排列，每个条目等于对应测试的 $\\rho(\\mu)$ 的浮点值，以完整机器精度计算，不进行四舍五入。不涉及物理单位。未使用角度。\n\n问题要求您：\n- 从矫顽常数的基本变分定义和参数仿射分解出发。\n- 推导并实现关于波数的离散谱伽辽金公式 $\\alpha_{\\mathrm{true}}(\\mu)$。\n- 建立并求解 SCM 线性规划，以使用指定的约束条件获得 $\\alpha_{\\mathrm{LB}}(\\mu)$。\n- 汇总所提供测试套件的紧致性比率 $\\rho(\\mu)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果 (例如, $[r_1,r_2,r_3,r_4]$)。",
            "solution": "用户希望计算一个参数化各向异性扩散问题的矫顽常数的逐次约束法 (SCM) 下界的紧致性比率。\n\n### 步骤 1：问题验证\n\n问题陈述已根据所有验证标准进行了仔细审查。\n\n- **给定条件**：所有数据，包括双线性形式、参数仿射分解、谱基函数、离散化参数（$N_x=8$, $N_y=8$）、训练参数集和测试参数集，都已明确提供。\n- **科学依据**：该问题在偏微分方程数值分析理论中有坚实的基础，特别是降阶基方法和矫顽常数的后验误差估计。所有数学公式和概念都是该领域的标准内容。\n- **适定性**：该问题是适定的。离散矫顽常数 $\\alpha_{\\mathrm{true}}(\\mu)$ 的计算是在一个有限集上的最小化。SCM 下界 $\\alpha_{\\mathrm{LB}}(\\mu)$ 由一个具有非空可行集的二维线性规划确定，这保证了解的存在性。\n- **客观性**：问题以精确、客观的数学语言陈述，没有歧义或主观论断。\n- **完整性**：为解决所述问题提供了所有必要的信息。\n- **一致性**：给定条件内部一致。瑞利商的推导在数学上是合理的且可验证的。\n- **特殊情况**：测试参数是训练参数的一个子集。虽然这会导致一个特定的数值结果（$\\rho=1$），但这是一个有效的设定，而不是一个缺陷。该问题测试了对 SCM 方法性质的理解，特别是其在训练点本身的行为。\n\n该问题被判定为**有效**。我们继续进行求解。\n\n### 步骤 2：基于原理的求解推导\n\n任务是为给定的一组测试参数 $\\mu = (a_x, a_y)$ 计算紧致性比率 $\\rho(\\mu) = \\alpha_{\\mathrm{LB}}(\\mu) / \\alpha_{\\mathrm{true}}(\\mu)$。这需要推导并实现 $\\alpha_{\\mathrm{true}}(\\mu)$ 和 $\\alpha_{\\mathrm{LB}}(\\mu)$ 的表达式。\n\n#### 2.1. 真实离散矫顽常数 $\\alpha_{\\mathrm{true}}(\\mu)$ 的计算\n\n离散矫顽常数定义为谱伽辽金子空间中所有基函数的瑞利商的最小值：\n$$\n\\alpha_{\\mathrm{true}}(\\mu) = \\min_{1 \\le k_x \\le N_x,\\; 1 \\le k_y \\le N_y} \\mathcal{R}(k_x,k_y;\\mu) = \\min_{1 \\le k_x \\le N_x,\\; 1 \\le k_y \\le N_y} \\frac{a_x(\\mu) k_x^2 + a_y(\\mu) k_y^2}{k_x^2 + k_y^2}\n$$\n瑞利商可以重写为 $a_x(\\mu)$ 和 $a_y(\\mu)$ 的加权平均值：\n$$\n\\mathcal{R}(k_x,k_y;\\mu) = a_x(\\mu) \\frac{k_x^2}{k_x^2+k_y^2} + a_y(\\mu) \\frac{k_y^2}{k_x^2+k_y^2}\n$$\n此表达式的最小值总是受 $\\min(a_x(\\mu), a_y(\\mu))$ 的限制。为了找到最小值，我们必须调整权重（通过选择 $k_x$ 和 $k_y$）以倾向于两个系数中较小的一个。\n\n- 如果 $a_x(\\mu)  a_y(\\mu)$，我们必须最大化 $a_x(\\mu)$ 上的权重，即 $\\frac{k_x^2}{k_x^2+k_y^2}$。这通过最大化 $k_x$ 和最小化 $k_y$ 来实现，即设置 $k_x = N_x$ 和 $k_y = 1$。\n- 如果 $a_y(\\mu)  a_x(\\mu)$，我们必须最大化 $a_y(\\mu)$ 上的权重，即 $\\frac{k_y^2}{k_x^2+k_y^2}$。这通过最大化 $k_y$ 和最小化 $k_x$ 来实现，即设置 $k_x = 1$ 和 $k_y = N_y$。\n- 如果 $a_x(\\mu) = a_y(\\mu)$，则该表达式对于所有 $k_x, k_y$ 都是常数，等于 $a_x(\\mu)$。\n\n给定 $N_x=8$ 和 $N_y=8$，$\\alpha_{\\mathrm{true}}(\\mu)$ 的解析公式为：\n$$\n\\alpha_{\\mathrm{true}}(a_x, a_y) = \\begin{cases}\n\\frac{a_x \\cdot 8^2 + a_y \\cdot 1^2}{8^2+1^2} = \\frac{64 a_x + a_y}{65}   \\text{若 } a_x  a_y \\\\\n\\frac{a_x \\cdot 1^2 + a_y \\cdot 8^2}{1^2+8^2} = \\frac{a_x + 64 a_y}{65}   \\text{若 } a_y  a_x \\\\\na_x   \\text{若 } a_x = a_y\n\\end{cases}\n$$\n\n#### 2.2. SCM 下界 $\\alpha_{\\mathrm{LB}}(\\mu)$ 的计算\n\nSCM 下界 $\\alpha_{\\mathrm{LB}}(\\mu)$ 是以下线性规划 (LP) 的解：\n$$\n\\alpha_{\\mathrm{LB}}(\\mu) = \\min_{y_1, y_2} \\left( a_x(\\mu) y_1 + a_y(\\mu) y_2 \\right)\n$$\n约束条件为：\n1. $y_1 \\ge 0, y_2 \\ge 0$\n2. $y_1 + y_2 = 1$\n3. 对于所有训练参数 $\\mu_i$，有 $a_x(\\mu_i) y_1 + a_y(\\mu_i) y_2 \\ge \\alpha_{\\mathrm{true}}(\\mu_i)$。\n\n我们可以简化这个 LP。使用 $y_2 = 1 - y_1$，问题在 $y_1$ 中变为一维问题：\n$$\n\\min_{y_1} \\left( (a_x(\\mu) - a_y(\\mu)) y_1 + a_y(\\mu) \\right)\n$$\n受限于 $y_1$ 的一个可行区间。约束变为：\n1. $y_1 \\ge 0$ 和 $1 - y_1 \\ge 0 \\implies 0 \\le y_1 \\le 1$。\n2. 对于每个训练参数 $\\mu_i$：$(a_x(\\mu_i) - a_y(\\mu_i)) y_1 \\ge \\alpha_{\\mathrm{true}}(\\mu_i) - a_y(\\mu_i)$。\n\n第二组约束定义了可行区间 $[y_{1,\\min}, y_{1,\\max}]$。对于一个训练参数 $\\mu_i$：\n- 如果 $a_x(\\mu_i)  a_y(\\mu_i)$：$y_1 \\ge \\frac{\\alpha_{\\mathrm{true}}(\\mu_i) - a_y(\\mu_i)}{a_x(\\mu_i) - a_y(\\mu_i)} = \\frac{\\frac{a_x(\\mu_i) + 64 a_y(\\mu_i)}{65} - a_y(\\mu_i)}{a_x(\\mu_i) - a_y(\\mu_i)} = \\frac{a_x(\\mu_i) - a_y(\\mu_i)}{65(a_x(\\mu_i) - a_y(\\mu_i))} = \\frac{1}{65}$。这提供了 $y_1$ 的一个下界。\n- 如果 $a_x(\\mu_i)  a_y(\\mu_i)$：$y_1 \\le \\frac{\\alpha_{\\mathrm{true}}(\\mu_i) - a_y(\\mu_i)}{a_x(\\mu_i) - a_y(\\mu_i)} = \\frac{\\frac{64 a_x(\\mu_i) + a_y(\\mu_i)}{65} - a_y(\\mu_i)}{a_x(\\mu_i) - a_y(\\mu_i)} = \\frac{64(a_x(\\mu_i) - a_y(\\mu_i))}{65(a_x(\\mu_i) - a_y(\\mu_i))} = \\frac{64}{65}$。这提供了 $y_1$ 的一个上界。\n\n训练集包含两种类型的参数。结合所有约束， $y_1$ 的可行区间是 $[\\frac{1}{65}, \\frac{64}{65}]$。\nLP 问题是在这个区间上最小化一个关于 $y_1$ 的线性函数。最小值将出现在其中一个端点：\n- 如果斜率 $(a_x(\\mu) - a_y(\\mu))  0$，最小值在 $y_1 = \\frac{1}{65}$。\n- 如果斜率 $(a_x(\\mu) - a_y(\\mu))  0$，最小值在 $y_1 = \\frac{64}{65}$。\n- 如果斜率为 $0$，则目标函数是常数。\n\n#### 2.3. 在测试参数上评估紧致性比率 $\\rho(\\mu)$\n\n测试参数为 $\\mu_1=(1,1)$, $\\mu_2=(1,10)$, $\\mu_3=(1,100)$, $\\mu_4=(1,1000)$。所有这些参数也都在训练集中。对于训练集中的任何参数 $\\mu_k$，SCM 约束包括 $a_x(\\mu_k) y_1 + a_y(\\mu_k) y_2 \\ge \\alpha_{\\mathrm{true}}(\\mu_k)$。$\\alpha_{\\mathrm{LB}}(\\mu_k)$ 的目标函数是最小化 $a_x(\\mu_k) y_1 + a_y(\\mu_k) y_2$。根据构造，最小值不能小于 $\\alpha_{\\mathrm{true}}(\\mu_k)$。因此，$\\alpha_{\\mathrm{LB}}(\\mu_k) \\ge \\alpha_{\\mathrm{true}}(\\mu_k)$。因为它也是一个下界，我们有 $\\alpha_{\\mathrm{LB}}(\\mu_k) \\le \\alpha_{\\mathrm{true}}(\\mu_k)$。这强制形成一个等式：$\\alpha_{\\mathrm{LB}}(\\mu_k) = \\alpha_{\\mathrm{true}}(\\mu_k)$。\n因此，对于任何同时也是训练参数的测试参数，紧致性比率 $\\rho(\\mu)$ 必须恰好为 $1$。实现将通过直接计算来证实这一点。\n我们现在将实现推导出的 $\\alpha_{\\mathrm{true}}(\\mu)$ 和 $\\alpha_{\\mathrm{LB}}(\\mu)$ 的解析公式，以计算测试套件的比率。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes the SCM tightness ratio for a parametrized anisotropic diffusion problem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 1.0),\n        (1.0, 10.0),\n        (1.0, 100.0),\n        (1.0, 1000.0),\n    ]\n\n    # Define the training parameter set for the SCM.\n    training_params = [\n        (1.0, 1.0), (1.0, 10.0), (10.0, 1.0), (1.0, 100.0), \n        (100.0, 1.0), (1.0, 1000.0), (1000.0, 1.0)\n    ]\n    \n    # Define the spectral Galerkin subspace parameters.\n    Nx = 8.0\n    Ny = 8.0\n\n    def calculate_alpha_true(mu, Nx_val, Ny_val):\n        \"\"\"\n        Calculates the discrete coercivity constant alpha_true based on the\n        analytical minimum of the Rayleigh quotient.\n        \n        Args:\n            mu (tuple): Parameter vector (ax, ay).\n            Nx_val (float): Number of basis functions in x-direction.\n            Ny_val (float): Number of basis functions in y-direction.\n            \n        Returns:\n            float: The value of alpha_true(mu).\n        \"\"\"\n        ax, ay = mu\n        if ax  ay:\n            # Minimum is at (kx, ky) = (Nx, 1) to maximize weight on smaller coefficient ax.\n            return (ax * Nx_val**2 + ay) / (Nx_val**2 + 1.0)\n        elif ay  ax:\n            # Minimum is at (kx, ky) = (1, Ny) to maximize weight on smaller coefficient ay.\n            return (ax + ay * Ny_val**2) / (1.0 + Ny_val**2)\n        else:  # ax == ay\n            # Rayleigh quotient is constant and equal to ax.\n            return ax\n\n    # Determine the feasible region for y1 from the training set.\n    # The linear program is to minimize (ax-ay)*y1 + ay subject to constraints.\n    # The constraints on y1 are derived from the training set.\n    y1_min_constraints = [0.0]\n    y1_max_constraints = [1.0]\n\n    for mu_i in training_params:\n        ax_i, ay_i = mu_i\n        alpha_true_i = calculate_alpha_true(mu_i, Nx, Ny)\n        \n        # Rewrite constraint a_x*y1 + a_y*y2 >= alpha_true as (a_x - a_y)*y1 >= alpha_true - a_y\n        diff = ax_i - ay_i\n        if diff > 0:\n            # y1 >= (alpha_true_i - ay_i) / (ax_i - ay_i)\n            lower_bound = (alpha_true_i - ay_i) / diff\n            y1_min_constraints.append(lower_bound)\n        elif diff  0:\n            # y1 = (alpha_true_i - ay_i) / (ax_i - ay_i) (inequality flips)\n            upper_bound = (alpha_true_i - ay_i) / diff\n            y1_max_constraints.append(upper_bound)\n        # If diff is 0, the constraint is 0 >= 0, which is always true and adds no information.\n\n    # The feasible interval for y1 is [y1_min_feasible, y1_max_feasible].\n    y1_min_feasible = max(y1_min_constraints)\n    y1_max_feasible = min(y1_max_constraints)\n\n    results = []\n    for case in test_cases:\n        ax_test, ay_test = case\n        \n        # 1. Calculate the \"true\" discrete coercivity constant for the test case.\n        alpha_t = calculate_alpha_true(case, Nx, Ny)\n        \n        # 2. Calculate the SCM lower bound alpha_LB.\n        # This involves minimizing the linear objective function f(y1) = (ax-ay)*y1 + ay\n        # over the feasible interval for y1.\n        \n        y1_opt = 0.0\n        slope = ax_test - ay_test\n        \n        if slope > 0: # Increasing function, minimum is at the left endpoint.\n            y1_opt = y1_min_feasible\n        elif slope  0: # Decreasing function, minimum is at the right endpoint.\n            y1_opt = y1_max_feasible\n        else: # slope is 0, function is constant, any point is a minimum.\n            y1_opt = y1_min_feasible\n\n        alpha_lb = slope * y1_opt + ay_test\n        \n        # 3. Compute the tightness ratio.\n        # As reasoned in the solution, since all test cases are in the training set,\n        # alpha_lb should be equal to alpha_t, making the ratio 1.0.\n        rho = alpha_lb / alpha_t\n        results.append(rho)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "后验误差估计理论的真正威力在于其能够处理在工程与科学领域中普遍存在的复杂非线性问题。本练习  将带您从标准的线性椭圆问题进入计算流体力学的前沿领域。您将为一个采用熵粘性方法进行激波捕捉的非线性间断 Galerkin (DG) 模型构建并验证一个后验误差估计器。这个实践将展示如何将基于残差的误差估计思想应用于包含解依赖的人工粘性项的复杂系统中，并验证误差估计器随基底规模增加而单调递减的关键性质，这对于保证降阶模型的认证至关重要。",
            "id": "3361071",
            "problem": "考虑一维域 $[0,1]$ 上的标量稳态边值问题，其狄利克雷边界条件为\n$$\n- \\frac{\\mathrm{d}}{\\mathrm{d}x} \\left( \\left( \\nu_0(\\mu) + \\nu_{\\mathrm{art}}(u; \\mu) \\right) \\frac{\\mathrm{d}u}{\\mathrm{d}x} \\right) = s(x;\\mu), \\quad x \\in (0,1), \\quad u(0)=1,\\quad u(1)=0,\n$$\n其中 $\\mu$ 表示参数向量，$\\nu_0(\\mu) \\gt 0$ 是一个常数物理粘度，$\\nu_{\\mathrm{art}}(u; \\mu) \\ge 0$ 是通过熵-粘度激波捕捉机制选择的人工粘度，用于稳定陡峭梯度。源项取为\n$$\ns(x;\\mu) = A(\\mu)\\,\\sin\\left(5\\pi x\\right),\n$$\n其中 $A(\\mu)\\gt 0$ 是源幅值。我们使用对称内部罚分不连续 Galerkin 方法（SIPG）在具有 $K$ 个单元和多项式次数 $p=0$ 的均匀网格上对此问题进行离散化。令 $h = 1/K$ 表示单元尺寸，并令 $u_h \\in V_h$ 为分片常数离散解，每个单元有一个自由度。\n\n我们使用二次熵 $\\eta(u) = \\tfrac{1}{2} u^2$逐单元地定义熵-粘度系数。对于任意 $u_h \\in V_h$，我们为单元索引 $e=0,1,\\dots,K-1$ 定义：\n- 单元梯度幅值的指示器\n$$\nG_e(u_h) = \\frac{1}{2}\\left( \\frac{|u_e - u_{e-1}|}{h} + \\frac{|u_{e+1} - u_e|}{h} \\right),\n$$\n其中约定 $u_{-1} := u(0)=1$ 和 $u_{K} := u(1)=0$，\n- 归一化\n$$\n\\Delta\\eta(u_h) = \\max_{0\\le j \\le K-1} \\left\\{ \\eta(u_j) \\right\\} - \\min_{0\\le j \\le K-1} \\left\\{ \\eta(u_j) \\right\\} + \\varepsilon_{\\eta},\n$$\n其中 $\\varepsilon_{\\eta} \\gt 0$ 是一个小的安全参数，\n- 以及熵粘度\n$$\n\\nu_{\\mathrm{art},e}(u_h;\\mu) = C_{\\mathrm{ev}}\\, h^2 \\,\\frac{|u_e|^2\\, G_e(u_h)}{\\Delta\\eta(u_h)} \\quad \\text{裁剪到 } [0, \\nu_{\\max}],\n$$\n其中 $C_{\\mathrm{ev}} \\gt 0$ 是一个可调系数，$\\nu_{\\max} \\gt 0$ 是一个粘度上限。\n\n对于分片常数粘度的扩散问题，其 SIPG 双线性形式为，对于试验函数 $u_h$ 和检验函数 $v_h$：\n$$\na(u_h,v_h;\\mu) = \\sum_{\\text{faces }F}\\lambda_F(\\mu)\\,[u_h]_F\\,[v_h]_F + \\lambda_{\\mathrm{L}}(\\mu)\\,u_0 v_0 + \\lambda_{\\mathrm{R}}(\\mu)\\,u_{K-1} v_{K-1},\n$$\n其中 $w_h$ 跨越单元 $e$ 和 $e+1$ 之间内部面 $F$ 的跳跃为 $[w_h]_F = w_e - w_{e+1}$，罚分项为 $\\lambda_F(\\mu) = C_{\\mathrm{pen}} \\, \\overline{\\nu}_F(\\mu) / h$，其中 $\\overline{\\nu}_F(\\mu) = \\tfrac{1}{2} \\left( \\nu_{e}(\\mu) + \\nu_{e+1}(\\mu) \\right)$ 且 $\\nu_e(\\mu) = \\nu_0(\\mu) + \\nu_{\\mathrm{art},e}(u_h;\\mu)$。边界罚分项满足 $\\lambda_{\\mathrm{L}}(\\mu) = C_{\\mathrm{pen}} \\, \\nu_0(\\mu)/h$ 和 $\\lambda_{\\mathrm{R}}(\\mu) = C_{\\mathrm{pen}} \\, \\nu_0(\\mu)/h$。线性泛函包含了源项和狄利克雷边界条件的弱施加，\n$$\n\\ell(v_h;\\mu) = \\sum_{e=0}^{K-1} \\left( \\int_{x_e}^{x_{e+1}} s(x;\\mu)\\,\\mathrm{d}x \\right) v_e + \\lambda_{\\mathrm{L}}(\\mu)\\, g_{\\mathrm{L}}\\, v_0 + \\lambda_{\\mathrm{R}}(\\mu)\\, g_{\\mathrm{R}}\\, v_{K-1},\n$$\n其中 $g_{\\mathrm{L}}=1$ 且 $g_{\\mathrm{R}}=0$。离散问题是：求解 $u_h \\in V_h$ 使得对于所有 $v_h \\in V_h$ 都有 $a(u_h,v_h;\\mu) = \\ell(v_h;\\mu)$。$\\nu_{\\mathrm{art}}$ 对 $u_h$ 的依赖性导致了一个非线性不动点问题，我们通过不动点迭代来求解。\n\n通过对一组高保真快照在欧几里得内积下进行正交归一化，构建一个维度为 $N$ 的降阶基（RB）空间 $V_N(\\mu)$。降阶解 $u_N(\\mu) \\in V_N(\\mu)$ 是通过求解投影到 $V_N(\\mu)$ 上的非线性不动点问题得到的 Galerkin 解。\n\n我们考虑一个基于残差的后验误差估计器，用于由算子的对称正定（SPD）部分诱导的能量范数。具体而言，定义与当前降阶解 $u_N(\\mu)$ 处的 $a(\\cdot,\\cdot;\\mu)$ 相关联的 SPD 矩阵 $S(u_N;\\mu)$，以及全阶残差向量\n$$\nr_N(\\mu) = \\ell(\\cdot;\\mu) - A(u_N(\\mu);\\mu) u_N(\\mu),\n$$\n其中 $A(u_N(\\mu);\\mu)$ 是使用 $\\nu_e(\\mu) = \\nu_0(\\mu) + \\nu_{\\mathrm{art},e}(u_N;\\mu)$ 组装的全阶刚度矩阵。那么估计器就是残差在 $S(u_N;\\mu)$ 内积下的对偶范数，\n$$\n\\eta_N(\\mu) = \\left\\| r_N(\\mu) \\right\\|_{S(u_N;\\mu)^{-1}} = \\sqrt{ r_N(\\mu)^{\\top} S(u_N;\\mu)^{-1} r_N(\\mu) }.\n$$\n对于具有强制对称双线性形式的线性问题，此量是由 $S$ 诱导的能量范数下误差的一个上界，即\n$$\n\\| u_h(\\mu) - u_N(\\mu) \\|_{S(u_N;\\mu)} \\le \\eta_N(\\mu),\n$$\n其中 $\\| v \\|_{S}^2 = v^{\\top} S v$。人工粘度增加了对称部分，从而增强了强制性，因此通过 $S$ 直接进入估计器。\n\n将认证衰减比定义为序列\n$$\nq_N(\\mu) = \\frac{\\eta_{N+1}(\\mu)}{\\eta_N(\\mu)}, \\quad N=1,2,\\dots,\n$$\n对于固定的参数 $\\mu$。我们将对于一组选定的测试参数，数值验证认证衰减的单调性属性：序列 $\\{ \\eta_N(\\mu) \\}$ 随 $N$ 非增，且满足 $0 \\le q_N(\\mu) \\le 1$。\n\n任务：\n1. 使用所述的对称内部罚分不连续 Galerkin 方法，在具有 $K=80$ 个单元和罚分系数 $C_{\\mathrm{pen}}=10$ 的均匀网格上，以分片常数基（$p=0$）离散化问题。使用不动点迭代计算高保真解 $u_h(\\mu)$，从线性初始猜测开始，当连续迭代的欧几里得范数变化小于 $10^{-10}$ 或达到 50 次迭代后宣布收敛。使用 $C_{\\mathrm{ev}}=0.02$，$\\nu_{\\max}=0.05$ 和 $\\varepsilon_{\\eta}=10^{-12}$。\n2. 通过收集高保真快照并执行正交归一化，从 $M=5$ 个训练参数构建一个降阶基空间。使用以下训练集：\n   - $\\mu_1^{\\mathrm{train}}$: $\\nu_0 = 0.015$, $A = 0.8$,\n   - $\\mu_2^{\\mathrm{train}}$: $\\nu_0 = 0.008$, $A = 1.2$,\n   - $\\mu_3^{\\mathrm{train}}$: $\\nu_0 = 0.012$, $A = 1.5$,\n   - $\\mu_4^{\\mathrm{train}}$: $\\nu_0 = 0.020$, $A = 0.6$,\n   - $\\mu_5^{\\mathrm{train}}$: $\\nu_0 = 0.010$, $A = 1.1$.\n3. 对于以下每个测试参数 $\\mu$，在 RB 空间中使用不动点迭代计算 $N=1,2,3,4,5$ 的降阶解 $u_N(\\mu)$，组装全残差 $r_N(\\mu)$ 和 SPD 矩阵 $S(u_N;\\mu)$，并评估估计器 $\\eta_N(\\mu)$。然后计算衰减比 $q_N(\\mu)$ 并验证对于所有 $N=1,\\dots,4$，单调性条件 $\\eta_{N+1}(\\mu) \\le \\eta_{N}(\\mu)$ 和 $0 \\le q_N(\\mu) \\le 1$ 是否成立。为每个测试参数报告一个布尔值，指示两个条件是否对所有 $N$ 都成立。\n   - 测试用例 1: $\\nu_0 = 0.010$, $A = 1.0$,\n   - 测试用例 2: $\\nu_0 = 0.007$, $A = 1.5$,\n   - 测试用例 3: $\\nu_0 = 0.020$, $A = 0.5$.\n\n此问题不涉及角度单位，除了提供的无量纲参数设置外，不需要其他物理单位。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[result1,result2,result3]”），其中每个结果是对应于上面列出的三个测试用例的布尔值。",
            "solution": "该问题是有效的。它提出了数值分析领域一个明确定义的任务，具体涉及非线性偏微分方程的模型降阶。所有必要的参数、方程和程序都已指定，且该问题在科学上基于已建立的方法，如不连续 Galerkin 方法、熵粘度稳定化以及带后验误差估计的降阶基方法。\n\n我们的方法包括以下步骤，实现为一系列计算过程：\n1.  **高保真（HF）求解器：** 我们首先为全阶离散问题构建一个求解器。\n2.  **降阶基（RB）生成：** 我们使用 HF 求解器为一组训练参数生成解“快照”，并从这些快照中形成一个正交归一基。\n3.  **降阶模型（ROM）求解器：** 我们实现一个用于求解投影到降阶基空间上的问题的求解器。\n4.  **后验误差估计与验证：** 对于每个测试参数，我们计算随基尺寸增加的降阶解，评估基于残差的误差估计器，并验证其单调性。\n\n### 1. 高保真离散化与求解\n\n该问题是一个一维、稳态、非线性扩散方程。我们在一个包含 $K=80$ 个尺寸为 $h=1/K$ 的单元的均匀网格上对其进行离散化。我们使用对称内部罚分不连续 Galerkin (SIPG) 方法，基函数为分片常数（$p=0$）函数 $\\{\\phi_e\\}_{e=0}^{K-1}$。离散解 $u_h$ 是 $\\mathbb{R}^K$ 中的一个向量，其中分量 $u_e$ 表示解在单元 $e$ 上的常数值。\n\n离散弱形式为：求 $u_h \\in V_h \\cong \\mathbb{R}^K$ 使得\n$$A(u_h;\\mu) u_h = \\ell(\\mu)$$\n这是一个包含 $K$ 个非线性代数方程的系统。矩阵 $A$ 和向量 $\\ell$ 来自双线性形式 $a(u_h, v_h; \\mu)$ 和线性形式 $\\ell(v_h; \\mu)$。\n\n**刚度矩阵和载荷向量的组装**\n刚度矩阵 $A(u_h;\\mu)$ 的元素为 $A_{ij} = a(\\phi_j, \\phi_i; \\mu)$。由于基函数的局部支撑性，该矩阵是三对角的。对于 $v_h=\\phi_i$，$a(u_h, v_h; \\mu)$ 的非零贡献为：\n-   **对角线元素 ($i=j$):**\n    -   $i \\in \\{1, \\dots, K-2\\}$: $A_{i,i} = \\lambda_{i-1,i}(\\mu) + \\lambda_{i,i+1}(\\mu)$\n    -   $i=0$: $A_{0,0} = \\lambda_L(\\mu) + \\lambda_{0,1}(\\mu)$\n    -   $i=K-1$: $A_{K-1,K-1} = \\lambda_R(\\mu) + \\lambda_{K-2,K-1}(\\mu)$\n-   **非对角线元素 ($j=i \\pm 1$):**\n    -   $A_{i,i+1} = A_{i+1,i} = -\\lambda_{i,i+1}(\\mu)$\n\n罚分参数定义为 $\\lambda_F(\\mu) = C_{\\mathrm{pen}} \\overline{\\nu}_F(\\mu) / h$，用于单元 $e$ 和 $e+1$ 之间的内部面 $F$，其中 $\\overline{\\nu}_F(\\mu) = \\frac{1}{2}(\\nu_e(\\mu) + \\nu_{e+1}(\\mu))$ 且 $\\nu_e(\\mu) = \\nu_0(\\mu) + \\nu_{\\mathrm{art},e}(u_h;\\mu)$。边界罚分项为 $\\lambda_L(\\mu) = C_{\\mathrm{pen}}\\nu_0(\\mu)/h$ 和 $\\lambda_R(\\mu) = C_{\\mathrm{pen}}\\nu_0(\\mu)/h$，其中 $C_{\\mathrm{pen}}=10$。\n\n载荷向量 $\\ell(\\mu)$ 的元素为 $\\ell_i = \\ell(\\phi_i; \\mu)$：\n$$ \\ell_i = \\int_{x_i}^{x_{i+1}} s(x;\\mu) dx + \\delta_{i0} \\lambda_L(\\mu) g_L + \\delta_{i,K-1} \\lambda_R(\\mu) g_R $$\n其中 $g_L=1$，$g_R=0$，$\\delta_{ij}$ 是克罗内克 delta。该积分可解析计算：\n$$ \\int_{x_i}^{x_{i+1}} A(\\mu) \\sin(5\\pi x) dx = \\frac{A(\\mu)}{5\\pi} (\\cos(5\\pi x_i) - \\cos(5\\pi x_{i+1})) $$\n\n**非线性求解器**\n矩阵 $A$ 对解 $u_h$ 的依赖性（通过人工粘度 $\\nu_{\\mathrm{art}}$）使得该系统成为非线性系统。我们通过不动点迭代来求解。给定一个迭代 $u_h^{(k)}$，我们通过求解以下线性系统来计算下一个迭代 $u_h^{(k+1)}$：\n$$ A(u_h^{(k)}; \\mu) u_h^{(k+1)} = \\ell(\\mu) $$\n迭代从初始猜测 $u_h^{(0)}$ 开始，该猜测通过在线性函数 $u(x)=1-x$ 的单元中心采样得到，即 $u_e^{(0)}=1-(e+0.5)h$。当 $\\|u_h^{(k+1)} - u_h^{(k)}\\|_2  10^{-10}$ 或达到最大 50 次迭代时，迭代终止。\n\n人工粘度 $\\nu_{\\mathrm{art}, e}(u_h; \\mu)$ 在每次迭代中更新。这包括计算梯度指示器 $G_e(u_h)$、熵范围 $\\Delta\\eta(u_h)$，然后计算粘度值本身，并将其裁剪到 $[0, \\nu_{\\max}]$。具体来说，对于解向量 $u_h = (u_0, \\dots, u_{K-1})$，我们构建一个扩展向量 $(u(0), u_0, \\dots, u_{K-1}, u(1)) = (1, u_0, \\dots, u_{K-1}, 0)$ 来计算 $G_e(u_h)$ 所需的跳跃。\n\n### 2. 降阶基生成\n\n我们从 $M=5$ 个快照生成降阶基。每个快照是 $M$ 个训练参数之一的高保真解 $u_h(\\mu_j^{\\mathrm{train}})$。将这 $M$ 个大小为 $K$ 的向量作为快照矩阵 $S_{\\text{snap}} \\in \\mathbb{R}^{K \\times M}$ 的列收集起来。然后我们对该矩阵进行（简约）QR 分解，$S_{\\text{snap}} = VR$。矩阵 $V \\in \\mathbb{R}^{K \\times N}$ 的列构成了快照空间的一个正交归一基（在欧几里得内积下）。生成的基 $V$ 用于所有后续的测试用例。\n\n### 3. 降阶模型求解\n\n对于给定的测试参数 $\\mu$ 和基尺寸 $N \\le M$，RB 空间为 $V_N = \\text{span}\\{v_1, \\dots, v_N\\}$，其中 $v_i$ 是 $V$ 的前 $N$ 列。降阶解的形式为 $u_N(\\mu) = V_N \\tilde{u}_N(\\mu)$，其中 $\\tilde{u}_N \\in \\mathbb{R}^N$ 是降阶坐标向量。\n\n将 Galerkin 投影应用于弱形式，得到关于 $\\tilde{u}_N$ 的 $N \\times N$ 非线性系统：\n$$ V_N^T A(V_N \\tilde{u}_N; \\mu) V_N \\tilde{u}_N = V_N^T \\ell(\\mu) $$\n这个较小的系统使用与高保真模型相同的不动点迭代策略求解。在每一步中计算全阶量 $A(u_N; \\mu)$ 和 $\\ell(\\mu)$（此问题不需要离线/在线分解），然后投影到降阶空间。\n\n### 4. 后验误差估计\n\n在计算出给定 $N$ 的收敛降阶解 $u_N(\\mu) = V_N \\tilde{u}_N(\\mu)$ 后，我们评估后验误差估计器 $\\eta_N(\\mu)$。这包括：\n1.  组装全阶刚度矩阵 $S(u_N;\\mu) = A(u_N(\\mu);\\mu)$ 和载荷向量 $\\ell(\\mu)$。由于双线性形式是对称正定（SPD）的，该矩阵即为用于误差范数的矩阵。\n2.  计算全阶残差向量 $r_N(\\mu) = \\ell(\\mu) - S(u_N;\\mu) u_N(\\mu)$。\n3.  计算估计器作为残差的对偶范数：\n    $$ \\eta_N(\\mu) = \\sqrt{ r_N(\\mu)^T S(u_N;\\mu)^{-1} r_N(\\mu) } $$\n    这可以通过首先求解线性系统 $S(u_N;\\mu) z = r_N(\\mu)$ 得到 $z$，然后取 $\\eta_N(\\mu) = \\sqrt{r_N(\\mu)^T z}$ 来高效计算。\n\n对于每个测试用例，我们计算 $N=1, 2, 3, 4, 5$ 的估计器 $\\eta_N(\\mu)$。然后我们计算 $N=1, 2, 3, 4$ 的衰减比 $q_N(\\mu) = \\eta_{N+1}(\\mu) / \\eta_N(\\mu)$。最后，我们验证条件 $\\eta_{N+1}(\\mu) \\le \\eta_N(\\mu)$ 和 $0 \\le q_N(\\mu) \\le 1$ 对所有 $N \\in \\{1, 2, 3, 4\\}$ 都成立。最终输出是每个测试用例的布尔值，指示这些属性是否得到满足。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    # --- Problem Constants ---\n    K = 80\n    P_DEG = 0 # Not used explicitly as p=0 logic is hard-coded\n    H = 1.0 / K\n    C_PEN = 10.0\n    HF_TOL = 1e-10\n    ROM_TOL = 1e-10\n    MAX_ITER = 50\n    C_EV = 0.02\n    NU_MAX = 0.05\n    EPS_ETA = 1e-12\n    U_L, U_R = 1.0, 0.0\n    M = 5\n\n    # --- Training and Test Cases ---\n    train_cases = [\n        (0.015, 0.8), # (nu0, A)\n        (0.008, 1.2),\n        (0.012, 1.5),\n        (0.020, 0.6),\n        (0.010, 1.1),\n    ]\n    test_cases = [\n        (0.010, 1.0),\n        (0.007, 1.5),\n        (0.020, 0.5),\n    ]\n\n    # Shared pre-computations\n    x_nodes = np.linspace(0, 1, K + 1)\n    x_e_left = x_nodes[:-1]\n    x_e_right = x_nodes[1:]\n    \n    def compute_nu_art(u, nu0):\n        if not isinstance(u, np.ndarray):\n            u = np.array(u)\n\n        u_ext = np.concatenate(([U_L], u, [U_R]))\n        \n        # Gradient indicator G_e\n        jumps_abs = np.abs(u_ext[1:] - u_ext[:-1])\n        g_e = 0.5 * (jumps_abs[:-1] + jumps_abs[1:]) / H\n        \n        # Entropy normalization Delta_eta\n        eta = 0.5 * u**2\n        delta_eta = np.max(eta) - np.min(eta) + EPS_ETA\n        \n        # Artificial viscosity\n        nu_art = C_EV * H**2 * (u**2 * g_e) / delta_eta\n        np.clip(nu_art, 0, NU_MAX, out=nu_art)\n        \n        return nu_art\n\n    def assemble_stiffness(nu_art, nu0):\n        nu_total = nu0 + nu_art\n        \n        # Penalties\n        lambda_L = C_PEN * nu0 / H\n        lambda_R = C_PEN * nu0 / H\n        \n        nu_face_avg = 0.5 * (nu_total[:-1] + nu_total[1:])\n        lambda_F = C_PEN * nu_face_avg / H\n        \n        # Matrix assembly\n        diag = np.zeros(K)\n        diag[0] = lambda_L + lambda_F[0]\n        diag[K-1] = lambda_R + lambda_F[K-2]\n        diag[1:K-1] = lambda_F[:-1] + lambda_F[1:]\n        \n        off_diag = -lambda_F\n        \n        A = np.diag(diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n        return A\n\n    def assemble_load(A_param, nu0):\n        # Source term integral\n        integral_s = (A_param / (5 * np.pi)) * (np.cos(5 * np.pi * x_e_left) - np.cos(5 * np.pi * x_e_right))\n        \n        l = np.copy(integral_s)\n        \n        # Boundary terms\n        lambda_L = C_PEN * nu0 / H\n        lambda_R = C_PEN * nu0 / H\n        l[0] += lambda_L * U_L\n        l[K-1] += lambda_R * U_R\n        \n        return l\n\n    def solve_nonlinear_system(mu, u_initial, is_rom=False, V_N=None):\n        nu0, A_param = mu\n        u = u_initial\n        \n        for _ in range(MAX_ITER):\n            u_old = u.copy()\n            \n            if is_rom:\n                u_full = V_N @ u\n                nu_art = compute_nu_art(u_full, nu0)\n                A_full = assemble_stiffness(nu_art, nu0)\n                l_full = assemble_load(A_param, nu0)\n                \n                A_rom = V_N.T @ A_full @ V_N\n                l_rom = V_N.T @ l_full\n                \n                u = np.linalg.solve(A_rom, l_rom)\n                if np.linalg.norm(u - u_old)  ROM_TOL:\n                    break\n            else: # HF\n                nu_art = compute_nu_art(u, nu0)\n                A = assemble_stiffness(nu_art, nu0)\n                l = assemble_load(A_param, nu0)\n                u = np.linalg.solve(A, l)\n                if np.linalg.norm(u - u_old)  HF_TOL:\n                    break\n        return u\n\n    # --- Step 1  2: Build Reduced Basis ---\n    snapshots = []\n    u_h_initial = 1.0 - (x_e_left + 0.5 * H)\n    for mu_train in train_cases:\n        u_h = solve_nonlinear_system(mu_train, u_h_initial)\n        snapshots.append(u_h)\n    \n    S_matrix = np.stack(snapshots, axis=1)\n    V, _ = np.linalg.qr(S_matrix, mode='reduced')\n\n    # --- Step 3: Evaluate Test Cases ---\n    final_results = []\n    for mu_test in test_cases:\n        nu0_test, A_test = mu_test\n        estimators = []\n        \n        for N in range(1, M + 1):\n            V_N = V[:, :N]\n            u_rom_initial = np.zeros(N)\n            \n            # Solve ROM\n            u_N_coeffs = solve_nonlinear_system(mu_test, u_rom_initial, is_rom=True, V_N=V_N)\n            u_N = V_N @ u_N_coeffs\n            \n            # Compute estimator\n            nu_art_N = compute_nu_art(u_N, nu0_test)\n            S_N = assemble_stiffness(nu_art_N, nu0_test)\n            l_N = assemble_load(A_test, nu0_test)\n            r_N = l_N - S_N @ u_N\n            \n            # eta_N^2 = r_N^T S_N^{-1} r_N\n            try:\n                z = np.linalg.solve(S_N, r_N)\n                eta_N_sq = r_N.T @ z\n                eta_N = np.sqrt(max(0, eta_N_sq))\n            except np.linalg.LinAlgError:\n                eta_N = np.inf # Should not happen with this SPD system\n\n            estimators.append(eta_N)\n\n        # Verify monotonicity\n        monotonic = True\n        for i in range(M - 1): # For N = 1, 2, 3, 4\n            eta_N = estimators[i]\n            eta_Np1 = estimators[i+1]\n            \n            if eta_Np1 > eta_N:\n                monotonic = False\n                break\n            \n            if eta_N > 0:\n                q_N = eta_Np1 / eta_N\n            else: # If eta_N is zero, eta_Np1 must also be zero\n                q_N = 0.0 if eta_Np1 == 0.0 else np.inf\n\n            if not (0.0 = q_N = 1.0):\n                monotonic = False\n                break\n        \n        final_results.append(monotonic)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\".lower())\n\nsolve()\n```"
        },
        {
            "introduction": "传统的后验误差界虽然可靠，但有时可能过于保守。其精确度由效用指数 $\\eta_N(\\mu) = \\Delta_N(\\mu) / e_N(\\mu)$ 来衡量，即估计误差与真实误差之比。本练习  介绍了一种更前沿的概率性方法，旨在通过学习来预测效用指数的行为。您将构建一个贝叶斯线性模型，利用离线计算的精确数据来训练一个关于效用指数的预测器。通过这种方法，您可以得到真实误差的一个更紧致的“可信上界”，并将其用作一个更智能的贪心算法终止准则，从而在满足精度要求的同时，构建出更高效的降阶基底。",
            "id": "3361052",
            "problem": "考虑一个强制的仿射参数化椭圆模型问题，该问题通过高阶谱间断 Galerkin (DG) 方法进行离散化，其高保真真实解用于构建降阶基 (RB) 近似。对于每个参数 $\\mu \\in [0,1]$，令能量范数下的 RB 近似误差表示为 $e_N(\\mu) = \\lVert u(\\mu) - u_N(\\mu) \\rVert_X$，基于残差的后验误差估计子表示为 $\\Delta_N(\\mu)$，有效性指数表示为 $\\eta_N(\\mu) = \\Delta_N(\\mu) / e_N(\\mu)$。在典型的强制性设置中，我们预期 $\\eta_N(\\mu) \\ge 1$。假设对于固定的 RB 空间大小 $N$，估计子的衰减遵循与谱 DG 行为一致的指数规律：$\\Delta_N(\\mu) = A(\\mu) \\exp(-\\rho N)$，其中 $A(\\mu)$ 是一个平滑振幅函数，$\\rho  0$ 表示谱衰减率。\n\n我们寻求设计一种算法，当预测的真实误差的最坏情况可信上限低于目标容差 $\\tau$ 时，终止标准的贪婪 RB 增量过程。为此，我们使用线性高斯贝叶斯模型来建模有效性指数的对数。令 $\\phi(\\mu) \\in \\mathbb{R}^d$ 为一个特征映射，$\\theta \\in \\mathbb{R}^d$ 为一个未知参数向量，其高斯先验为 $\\theta \\sim \\mathcal{N}(\\theta_0, \\Sigma_0)$。假设可观测量 $y(\\mu)$ 满足 $y(\\mu) = \\log(\\eta_N(\\mu)) = \\phi(\\mu)^\\top \\theta + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$，且在样本间独立。$\\theta$ 的后验分布和 $y(\\mu)$ 的后验预测分布必须通过线性高斯模型的贝叶斯法则计算得出。使用 $y(\\mu)$ 的后验预测分布，为每个 $\\mu$ 获取 $\\eta_N(\\mu)$ 在可信度水平 $1-\\beta$ 下的可信下限，该下限被解释为模型下 $\\eta_N(\\mu)$ 的较低 $(\\beta)$-分位数。然后，将预测的真实误差的可信上限定义为 $\\widehat{B}_N(\\mu) = \\Delta_N(\\mu) / \\underline{\\eta}_{\\beta}(\\mu)$，其中 $\\underline{\\eta}_{\\beta}(\\mu)$ 是 $\\eta_N(\\mu)$ 的 $(\\beta)$-分位数下限。在 RB 大小为 $N$ 时的贪婪过程在有限训练网格 $\\mathcal{P}_{\\mathrm{train}}$ 上计算 $\\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu)$，如果 $\\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu) \\le \\tau$，则终止。\n\n构建一个程序，在如下的合成但科学一致的数据集上实现此终止逻辑。\n\n- 使用参数域 $\\mathcal{P} = [0,1]$，将其离散化为一个均匀网格 $\\mathcal{P}_{\\mathrm{train}} = \\{\\mu_j\\}_{j=0}^{M-1}$，其中 $M = 21$ 且 $\\mu_j = j/(M-1)$。\n- 选择一个特征映射 $\\phi(\\mu) = [1, \\mu, \\mu^2]^\\top$，使得 $d = 3$。\n- 使用一个固定的未知向量 $\\theta^\\star \\in \\mathbb{R}^3$ 和零观测噪声，为离线 oracle 评估合成一个真实有效性模型：$y^\\star(\\mu) = \\phi(\\mu)^\\top \\theta^\\star$，其中 $\\theta^\\star = [0.25, 0.35, 0.10]^\\top$。那么，对于所有 $\\mu \\in [0,1]$，$\\eta^\\star(\\mu) = \\exp(y^\\star(\\mu))$ 严格大于 $1$。\n- 使用基于残差的估计子模型 $\\Delta_N(\\mu) = A(\\mu) \\exp(-\\rho N)$，其中 $A(\\mu) = 0.5 + \\mu$。将 $\\rho$ 解释为一个抽象化的谱 DG 衰减率，与多项式次数控制的指数收敛一致。\n- 在 $\\mu \\in \\{0, 0.5, 1\\}$ 处使用 $S_0 = 3$ 个精确 oracle 观测来初始化离线数据集：在这些点观测 $y(\\mu) = y^\\star(\\mu)$ 为贝叶斯模型提供初始观测值。\n- 对于贝叶斯预测器，使用高斯先验 $\\theta \\sim \\mathcal{N}(\\theta_0, \\Sigma_0)$，其中 $\\theta_0 = [0, 0, 0]^\\top$ 且 $\\Sigma_0 = s_0^2 I_3$，对于指定的 $s_0  0$。在预测器中使用指定的观测噪声标准差 $\\sigma  0$，它反映的是模型不确定性，而不是 oracle 噪声。\n- 当 RB 大小 $N$ 等于数据集中离线样本的数量时，执行以下循环：\n  - 在每个 $\\mu \\in \\mathcal{P}_{\\mathrm{train}}$ 上，计算 $\\theta$ 的后验分布和 $y(\\mu)$ 的后验预测分布。\n  - 使用可信度参数 $\\beta \\in (0, 0.5)$，根据 $y(\\mu)$ 的高斯预测模型，构建 $\\eta_N(\\mu)$ 的可信下限 $\\underline{\\eta}_{\\beta}(\\mu)$。然后计算 $\\widehat{B}_N(\\mu) = \\Delta_N(\\mu)/\\underline{\\eta}_{\\beta}(\\mu)$ 及其在 $\\mathcal{P}_{\\mathrm{train}}$ 上的最大值，记为 $\\widehat{B}^{\\max}_N$。\n  - 如果 $\\widehat{B}^{\\max}_N \\le \\tau$，则终止并输出当前的 RB 大小 $N$。\n  - 否则，选择 $\\mu^\\star = \\arg\\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu)$，用一个新的精确 oracle 观测 $y(\\mu^\\star) = y^\\star(\\mu^\\star)$ 扩充离线数据集，递增 $N \\leftarrow N+1$，然后重复。不要多次选择相同的 $\\mu$。如果达到预设的最大迭代次数 $N_{\\max}$，则终止并输出达到的 $N$。\n\n从高斯先验、线性观测模型以及高斯分布的贝叶斯法则的定义中推导出必要的贝叶斯更新和预测公式。然后实现该算法，并在以下测试套件上运行它，其中每个测试用例是一个元组 $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max})$：\n\n- 测试 $1$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.08, 0.10, 0.80, 0.10, 0.50, 20)$。\n- 测试 $2$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.15, 0.10, 0.60, 0.10, 0.50, 20)$。\n- 测试 $3$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.03, 0.05, 0.80, 0.15, 0.70, 20)$。\n- 测试 $4$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.02, 0.01, 0.60, 0.25, 1.00, 12)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，其中每个条目是相应测试用例终止时的最终 RB 大小 $N$，并按顺序排列。例如，包含四个整数的输出应类似于 $[n_1,n_2,n_3,n_4]$，不含多余空格或文本。此问题不涉及物理单位。所有角度（如有）必须以弧度为单位，但此处不存在角度。所有小数必须在程序输出中表示为十进制数。",
            "solution": "基本要素包括强制性问题的基于残差的后验界、有效性指数以及线性高斯贝叶斯模型。对于每个参数 $\\mu$，RB 能量范数误差 $e_N(\\mu)$ 和基于残差的界 $\\Delta_N(\\mu)$ 满足不等式 $e_N(\\mu) \\le \\Delta_N(\\mu)$。有效性指数为 $\\eta_N(\\mu) = \\Delta_N(\\mu)/e_N(\\mu)$，由于使用了强制性常数的安全下限，对于强制性问题，该值通常 $\\ge 1$。目标是利用从离线样本训练的贝叶斯预测器获得的 $\\eta_N(\\mu)$ 的可信下限，来为 $e_N(\\mu)$ 构建一个保守的可信上限。\n\n我们采用以下反映谱间断 Galerkin 行为的合成但科学一致的模型。估计子随 $N$ 呈指数衰减，形式为 $\\Delta_N(\\mu) = A(\\mu) \\exp(-\\rho N)$，这捕捉了速率参数为 $\\rho  0$ 的谱收敛性，以及平滑振幅 $A(\\mu) = 0.5 + \\mu$。对于有效性，我们将 $\\log(\\eta_N(\\mu))$ 建模为 $\\phi(\\mu) \\in \\mathbb{R}^3$ 的线性函数，其中 $\\phi(\\mu) = [1, \\mu, \\mu^2]^\\top$。我们假设\n$$\ny(\\mu) = \\log(\\eta_N(\\mu)) = \\phi(\\mu)^\\top \\theta + \\varepsilon,\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2),\n$$\n并带有高斯先验 $\\theta \\sim \\mathcal{N}(\\theta_0,\\Sigma_0)$。离线 oracle 提供来自真实值 $\\theta^\\star = [0.25, 0.35, 0.10]^\\top$ 的无噪声观测，因此 $y^\\star(\\mu) = \\phi(\\mu)^\\top \\theta^\\star$ 且 $\\eta^\\star(\\mu) = \\exp(y^\\star(\\mu))$。因为 $\\phi(\\mu)$ 的分量非负，且 $\\theta^\\star$ 的条目非负，所以对于所有 $\\mu \\in [0,1]$，我们有 $y^\\star(\\mu) \\ge 0$，因此 $\\eta^\\star(\\mu) \\ge 1$。\n\n我们从 $\\mu \\in \\{0,0.5,1\\}$ 处的 $S_0 = 3$ 个离线观测开始，设置 $N = S_0$，并通过贪婪选择进行迭代增量。在线性高斯模型下，$\\theta$ 的贝叶斯后验是高斯的。令 $\\Phi \\in \\mathbb{R}^{S \\times d}$ 为设计矩阵，其行为 $S$ 个观测参数 $\\{\\mu_i\\}_{i=1}^S$ 对应的 $\\phi(\\mu_i)^\\top$，令 $y \\in \\mathbb{R}^S$ 为观测到的 $y(\\mu_i)$ 向量。高斯先验为 $\\theta \\sim \\mathcal{N}(\\theta_0,\\Sigma_0)$，其中 $\\theta_0 \\in \\mathbb{R}^d$，$\\Sigma_0 \\in \\mathbb{R}^{d \\times d}$ 是正定的。似然函数为 $y \\mid \\theta \\sim \\mathcal{N}(\\Phi \\theta, \\sigma^2 I_S)$。根据高斯分布的贝叶斯法则，后验分布为\n$$\n\\theta \\mid y \\sim \\mathcal{N}(\\theta_{\\mathrm{post}}, \\Sigma_{\\mathrm{post}}),\n$$\n其中\n$$\n\\Sigma_{\\mathrm{post}} = \\left(\\Sigma_0^{-1} + \\sigma^{-2} \\Phi^\\top \\Phi\\right)^{-1}, \\quad \\theta_{\\mathrm{post}} = \\Sigma_{\\mathrm{post}} \\left(\\Sigma_0^{-1}\\theta_0 + \\sigma^{-2} \\Phi^\\top y \\right).\n$$\n对于任何新的 $\\mu$， $y(\\mu) = \\phi(\\mu)^\\top \\theta + \\varepsilon$ 的后验预测分布是高斯分布，其均值和方差为\n$$\nm(\\mu) = \\phi(\\mu)^\\top \\theta_{\\mathrm{post}}, \\quad s^2(\\mu) = \\phi(\\mu)^\\top \\Sigma_{\\mathrm{post}} \\phi(\\mu) + \\sigma^2.\n$$\n由于 $\\eta_N(\\mu) = \\exp(y(\\mu))$ 且 $y(\\mu) \\sim \\mathcal{N}(m(\\mu), s^2(\\mu))$，我们得到了一个 $\\eta_N(\\mu)$ 的对数正态预测模型。对于可信度参数 $\\beta \\in (0,0.5)$，$\\eta_N(\\mu)$ 的一个 $(1-\\beta)$ 可信下限是其 $\\beta$-分位数\n$$\n\\underline{\\eta}_{\\beta}(\\mu) = \\exp\\left(m(\\mu) + s(\\mu) z_{\\beta}\\right), \\quad z_{\\beta} = \\Phi^{-1}(\\beta),\n$$\n其中 $\\Phi^{-1}$ 是标准正态分布的逆累积分布函数。对应的真实误差的预测可信上限则为\n$$\n\\widehat{B}_N(\\mu) = \\frac{\\Delta_N(\\mu)}{\\underline{\\eta}_{\\beta}(\\mu)} = \\frac{A(\\mu)\\exp(-\\rho N)}{\\exp\\left(m(\\mu) + s(\\mu) z_{\\beta}\\right)}.\n$$\n我们将最坏情况下的预测界计算为 $\\widehat{B}^{\\max}_N = \\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu)$。如果 $\\widehat{B}^{\\max}_N \\le \\tau$，我们终止贪婪过程；否则，我们选择 $\\mu^\\star = \\arg\\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu)$，向 oracle 查询 $y^\\star(\\mu^\\star)$（在此合成设置中是无噪声的），扩充数据集，并设置 $N \\leftarrow N+1$。$\\Delta_N(\\mu)$ 的指数衰减是由谱 DG 的多项式次数收敛和稳定的强制性下限所驱动的；贝叶斯预测器会适应从样本中学到的有效性形态。\n\n在算法上，我们为每个测试用例 $(\\tau,\\beta,\\rho,\\sigma,s_0,N_{\\max})$ 实现以下步骤：\n- 初始化 $\\mathcal{P}_{\\mathrm{train}} = \\{\\mu_j\\}_{j=0}^{M-1}$，包含 $[0,1]$ 上的 $M = 21$ 个均匀点。\n- 设置 $\\theta_0 = [0,0,0]^\\top$ 和 $\\Sigma_0 = s_0^2 I_3$。\n- 在 $\\{0,0.5,1\\}$ 处使用 $S_0 = 3$ 个样本初始化数据集，其中 $y(\\mu) = \\phi(\\mu)^\\top \\theta^\\star$，并设置 $N = S_0$。\n- 循环直到终止：计算 $(\\theta_{\\mathrm{post}},\\Sigma_{\\mathrm{post}})$，然后对于每个 $\\mu \\in \\mathcal{P}_{\\mathrm{train}}$ 计算 $m(\\mu)$ 和 $s(\\mu)$，接着计算 $\\underline{\\eta}_{\\beta}(\\mu)$ 和 $\\widehat{B}_N(\\mu)$，并确定 $\\widehat{B}^{\\max}_N$ 及其最大化点 $\\mu^\\star$。如果 $\\widehat{B}^{\\max}_N \\le \\tau$ 或 $N \\ge N_{\\max}$，则停止并返回 $N$。否则，将 $(\\mu^\\star, y^\\star(\\mu^\\star))$ 添加到数据集并递增 $N$。\n- 确保没有参数被选择超过一次。\n\n测试套件使用：\n- 测试 $1$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.08, 0.10, 0.80, 0.10, 0.50, 20)$。\n- 测试 $2$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.15, 0.10, 0.60, 0.10, 0.50, 20)$。\n- 测试 $3$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.03, 0.05, 0.80, 0.15, 0.70, 20)$。\n- 测试 $4$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.02, 0.01, 0.60, 0.25, 1.00, 12)$。\n\n程序会计算每个测试的最终 RB 大小 $N$，并将它们打印为单个列表 $[n_1,n_2,n_3,n_4]$，不含附加文本。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\n# Synthetic ground truth for log-effectivity: y*(mu) = phi(mu)^T theta_true\ntheta_true = np.array([0.25, 0.35, 0.10])\n\ndef phi(mu: float) -> np.ndarray:\n    \"\"\"Feature map phi(mu) = [1, mu, mu^2].\"\"\"\n    return np.array([1.0, mu, mu * mu])\n\ndef log_eta_true(mu: float) -> float:\n    \"\"\"Noise-free ground-truth log-effectivity.\"\"\"\n    return float(phi(mu).dot(theta_true))\n\ndef posterior_update(theta0: np.ndarray, Sigma0: np.ndarray,\n                     mus: np.ndarray, y: np.ndarray, sigma: float):\n    \"\"\"\n    Compute Gaussian posterior parameters for linear model:\n    y = Phi theta + eps, eps ~ N(0, sigma^2 I).\n    \"\"\"\n    Phi = np.vstack([phi(mu) for mu in mus])  # S x d\n    Sigma0_inv = np.linalg.inv(Sigma0)\n    precision = Sigma0_inv + (1.0 / (sigma ** 2)) * (Phi.T @ Phi)\n    Sigma_post = np.linalg.inv(precision)\n    b = Sigma0_inv @ theta0 + (1.0 / (sigma ** 2)) * (Phi.T @ y)\n    theta_post = Sigma_post @ b\n    return theta_post, Sigma_post\n\ndef predictive_log_stats(mu: float, theta_post: np.ndarray,\n                         Sigma_post: np.ndarray, sigma: float):\n    \"\"\"Posterior predictive mean and std for log-effectivity at mu.\"\"\"\n    ph = phi(mu)\n    m = float(ph.dot(theta_post))\n    s2 = float(ph.dot(Sigma_post @ ph) + sigma ** 2)\n    s = np.sqrt(max(s2, 0.0))\n    return m, s\n\ndef eta_lower_quantile(mu: float, theta_post: np.ndarray,\n                       Sigma_post: np.ndarray, sigma: float, beta: float) -> float:\n    \"\"\"\n    Lower credible bound (beta-quantile) for eta = exp(log-eta),\n    where log-eta ~ N(m, s^2). Quantile is exp(m + s * z_beta).\n    \"\"\"\n    m, s = predictive_log_stats(mu, theta_post, Sigma_post, sigma)\n    z_beta = norm.ppf(beta)\n    log_q = m + s * z_beta\n    q = float(np.exp(log_q))\n    # Guard: effectivity should be >= 1 typically; for conservatism clamp to min 1e-6\n    return max(q, 1e-12)\n\ndef amplitude(mu: float) -> float:\n    \"\"\"Amplitude A(mu) = 0.5 + mu.\"\"\"\n    return 0.5 + mu\n\ndef delta_N(mu: float, N: int, rho: float) -> float:\n    \"\"\"Residual-based estimator model: Delta_N(mu) = A(mu) * exp(-rho * N).\"\"\"\n    return amplitude(mu) * np.exp(-rho * N)\n\ndef greedy_terminate_once(tau: float, beta: float, rho: float,\n                          sigma: float, s0: float, N_max: int) -> int:\n    \"\"\"\n    Run the Bayesian-predicted greedy termination.\n    Return final basis size N at termination (either by criterion or reaching N_max).\n    \"\"\"\n    # Training grid\n    M = 21\n    mus_grid = np.linspace(0.0, 1.0, M).tolist()\n\n    # Prior\n    theta0 = np.zeros(3)\n    Sigma0 = (s0 ** 2) * np.eye(3)\n\n    # Initial offline samples (noise-free oracle)\n    init_mus = [0.0, 0.5, 1.0]\n    train_mus = list(init_mus)\n    y_vals = np.array([log_eta_true(mu) for mu in train_mus], dtype=float)\n    N = len(train_mus)\n\n    # Ensure we do not reselect already used mus\n    used = set(train_mus)\n\n    while True:\n        # Posterior given current data\n        theta_post, Sigma_post = posterior_update(theta0, Sigma0,\n                                                  np.array(train_mus, dtype=float),\n                                                  y_vals, sigma)\n\n        # Compute predicted credible upper bound on true error over grid\n        worst_B = -np.inf\n        worst_mu = None\n        for mu in mus_grid:\n            eta_low = eta_lower_quantile(mu, theta_post, Sigma_post, sigma, beta)\n            B = delta_N(mu, N, rho) / eta_low\n            if B > worst_B:\n                worst_B = B\n                worst_mu = mu\n\n        # Termination checks\n        if worst_B = tau or N >= N_max:\n            return N\n\n        # Enrich at worst_mu if not already used; otherwise, pick next worst\n        # Build a sorted list by decreasing B to find next unused if needed\n        Bs = []\n        for mu in mus_grid:\n            eta_low = eta_lower_quantile(mu, theta_post, Sigma_post, sigma, beta)\n            B = delta_N(mu, N, rho) / eta_low\n            Bs.append((B, mu))\n        Bs.sort(reverse=True, key=lambda t: t[0])\n        selected_mu = None\n        for _, mu in Bs:\n            if mu not in used:\n                selected_mu = mu\n                break\n\n        if selected_mu is None:\n            # No new parameter available; terminate to avoid infinite loop\n            return N\n\n        # Add new sample (noise-free oracle)\n        train_mus.append(selected_mu)\n        used.add(selected_mu)\n        y_vals = np.append(y_vals, log_eta_true(selected_mu))\n        N += 1\n\ndef solve():\n    # Define the test cases: (tau, beta, rho, sigma, s0, N_max)\n    test_cases = [\n        (0.08, 0.10, 0.80, 0.10, 0.50, 20),  # Test 1\n        (0.15, 0.10, 0.60, 0.10, 0.50, 20),  # Test 2\n        (0.03, 0.05, 0.80, 0.15, 0.70, 20),  # Test 3\n        (0.02, 0.01, 0.60, 0.25, 1.00, 12),  # Test 4\n    ]\n\n    results = []\n    for tau, beta, rho, sigma, s0, N_max in test_cases:\n        N_final = greedy_terminate_once(tau, beta, rho, sigma, s0, N_max)\n        # Ensure integer output\n        results.append(int(N_final))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}