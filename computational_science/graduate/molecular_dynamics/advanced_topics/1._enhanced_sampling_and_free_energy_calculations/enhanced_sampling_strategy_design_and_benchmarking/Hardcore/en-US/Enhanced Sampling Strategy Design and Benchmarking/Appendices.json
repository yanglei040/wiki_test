{
    "hands_on_practices": [
        {
            "introduction": "Designing an effective enhanced sampling simulation begins with a careful plan. Before committing extensive computational resources, it is crucial to estimate the required simulation parameters. This first practice provides a quantitative framework for designing a one-dimensional umbrella sampling simulation, a cornerstone technique for free energy calculations. By working through this problem, you will learn how to determine the minimum number of simulation windows needed to achieve a target statistical precision, revealing the fundamental trade-offs between the biasing potential stiffness, per-window simulation time, and overall computational cost .",
            "id": "3410749",
            "problem": "Consider umbrella sampling in a Molecular Dynamics (MD) simulation for estimating the one-dimensional potential of mean force $F(s)$ along a Collective Variable (CV) $s$. The CV domain to be covered is the closed interval $[s_{\\min}, s_{\\max}]$ with $s_{\\min} = 0\\,\\mathrm{nm}$ and $s_{\\max} = 2\\,\\mathrm{nm}$. Each umbrella window employs a harmonic bias $U_{i}(s) = \\frac{1}{2} k (s - s_{i})^{2}$ with a common force constant $k$ and equally spaced window centers $\\{ s_{i} \\}$ with spacing $\\Delta s$. The simulation is conducted at absolute temperature $T$.\n\nAssume the following physically motivated simplifications that form a consistent design baseline:\n- Near any window center $s_{i}$, the unbiased free energy $F(s)$ varies slowly compared to the imposed harmonic curvature, so the biased distribution of $s$ in window $i$ is approximately Gaussian with variance $\\sigma^{2} = k_{\\mathrm{B}} T / k$, where $k_{\\mathrm{B}}$ is the Boltzmann constant.\n- The time series $s(t)$ in each window is stationary with Integrated Autocorrelation Time (IAT) $\\tau_{\\mathrm{int}}$. For a trajectory of duration $t_{w}$ in a given window, the effective number of statistically independent samples is $N_{\\mathrm{eff}} \\approx t_{w}/(2 \\tau_{\\mathrm{int}})$.\n- The design requires stable overlap between adjacent windows. Use the Bhattacharyya coefficient $O$ between two equal-variance Gaussian biased densities as a quantitative overlap metric. For two adjacent windows separated by $\\Delta s$ and having variance $\\sigma^{2}$, the overlap is $O(\\Delta s) = \\exp\\!\\left(-\\Delta s^{2}/(8 \\sigma^{2})\\right)$.\n- Treat the statistical uncertainty in the unbiased free energy at an overlap location by the delta-method approximation, using $F(s) = -k_{\\mathrm{B}} T \\ln P(s)$ and an effective sample count in the overlap region scaling as $N_{\\mathrm{ov}} \\approx 2 N_{\\mathrm{eff}} O(\\Delta s)$. Target a worst-case free energy uncertainty $\\delta F$ in the overlap region by enforcing $\\mathrm{Var}(F) \\approx (k_{\\mathrm{B}} T)^{2}/N_{\\mathrm{ov}} \\leq \\delta F^{2}$.\n\nGiven the parameter values $T = 300\\,\\mathrm{K}$, $k = 1200\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1}\\,\\mathrm{nm}^{-2}$, $t_{w} = 60\\,\\mathrm{ns}$, and $\\tau_{\\mathrm{int}} = 0.15\\,\\mathrm{ns}$, and setting the target free energy uncertainty to $\\delta F = 0.25\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1}$, determine the minimal integer number of windows $M$ required to cover the interval $[s_{\\min}, s_{\\max}]$ under the above design model. Use $k_{\\mathrm{B}} = 8.314462618 \\times 10^{-3}\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$ and assume equally spaced window centers that start at $s_{\\min}$ and end at $s_{\\max}$ with spacing $\\Delta s$, so that $M = \\left\\lceil \\frac{s_{\\max} - s_{\\min}}{\\Delta s} \\right\\rceil + 1$. Report the final answer as a dimensionless integer count. If rounding is needed in intermediate steps, carry sufficient precision to ensure the final integer is correct, and report the integer without any units.\n\nAfter computing $M$, briefly explain, based on the derived expressions and assumptions, the trade-offs between the number of windows $M$, the stiffness $k$, and the per-window sampling time $t_{w}$, without changing your computed $M$.",
            "solution": "The user wants to determine the minimal integer number of umbrella windows, $M$, required to span a collective variable (CV) range $[s_{\\min}, s_{\\max}]$ while ensuring a maximum statistical uncertainty of $\\delta F$ in the free energy estimate at the overlap points between adjacent windows.\n\nThe problem provides a set of physically motivated assumptions that link the simulation parameters to this uncertainty target. The core of the problem is to solve these equations for the maximum allowable spacing between window centers, $\\Delta s$, and then use this value to calculate the minimum number of windows $M$.\n\nFirst, let's list the given parameters and relationships:\n- CV range: $s_{\\min} = 0\\,\\mathrm{nm}$ to $s_{\\max} = 2\\,\\mathrm{nm}$. The total length is $L = s_{\\max} - s_{\\min} = 2\\,\\mathrm{nm}$.\n- Temperature: $T = 300\\,\\mathrm{K}$.\n- Umbrella force constant: $k = 1200\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1}\\,\\mathrm{nm}^{-2}$.\n- Trajectory duration per window: $t_{w} = 60\\,\\mathrm{ns}$.\n- Integrated Autocorrelation Time: $\\tau_{\\mathrm{int}} = 0.15\\,\\mathrm{ns}$.\n- Target free energy uncertainty: $\\delta F = 0.25\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1}$.\n- Boltzmann constant: $k_{\\mathrm{B}} = 8.314462618 \\times 10^{-3}\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$.\n\nThe solution proceeds in the following steps:\n1.  Use the uncertainty constraint to find the minimum required overlap, $O_{\\min}$, between adjacent biased distributions.\n2.  Use the formula for the overlap, $O(\\Delta s)$, to find the maximum allowed window spacing, $\\Delta s_{\\max}$, corresponding to $O_{\\min}$.\n3.  Calculate the minimum number of windows, $M$, needed to cover the total range $L$ with spacing $\\Delta s_{\\max}$.\n\nStep 1: Determine the minimum required overlap $O_{\\min}$.\nThe constraint on the variance of the free energy is given as:\n$$\\mathrm{Var}(F) \\leq \\delta F^{2}$$\nThe problem provides the approximation for the variance in the overlap region:\n$$\\mathrm{Var}(F) \\approx \\frac{(k_{\\mathrm{B}} T)^{2}}{N_{\\mathrm{ov}}}$$\nwhere $N_{\\mathrm{ov}} \\approx 2 N_{\\mathrm{eff}} O(\\Delta s)$ is the effective number of samples in the overlap region.\nCombining these, we get:\n$$\\frac{(k_{\\mathrm{B}} T)^{2}}{2 N_{\\mathrm{eff}} O(\\Delta s)} \\leq \\delta F^{2}$$\nTo minimize the number of windows, we need to maximize the spacing $\\Delta s$. Maximizing $\\Delta s$ corresponds to minimizing the overlap $O(\\Delta s)$, since $O(\\Delta s)$ is a decreasing function of $\\Delta s$. Therefore, we should satisfy this inequality at its boundary, which defines the minimum required overlap, $O_{\\min}$:\n$$O_{\\min} = \\frac{(k_{\\mathrm{B}} T)^{2}}{2 N_{\\mathrm{eff}} \\delta F^{2}}$$\nWe first calculate the necessary intermediate quantities.\nThe thermal energy is:\n$$k_{\\mathrm{B}} T = (8.314462618 \\times 10^{-3}\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}) \\times (300\\,\\mathrm{K}) = 2.4943387854\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1}$$\nThe effective number of independent samples per window is:\n$$N_{\\mathrm{eff}} \\approx \\frac{t_{w}}{2 \\tau_{\\mathrm{int}}} = \\frac{60\\,\\mathrm{ns}}{2 \\times 0.15\\,\\mathrm{ns}} = \\frac{60}{0.3} = 200$$\nNow we can compute $O_{\\min}$:\n$$O_{\\min} = \\frac{(2.4943387854\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1})^{2}}{2 \\times 200 \\times (0.25\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1})^{2}} = \\frac{6.221727...}{2 \\times 200 \\times 0.0625} = \\frac{6.221727...}{25} \\approx 0.248869$$\n\nStep 2: Determine the maximum allowed spacing $\\Delta s_{\\max}$.\nThe overlap is given by the Bhattacharyya coefficient for two Gaussians:\n$$O(\\Delta s) = \\exp\\left(-\\frac{\\Delta s^{2}}{8 \\sigma^{2}}\\right)$$\nwhere $\\sigma^{2} = k_{\\mathrm{B}} T / k$ is the variance of the biased distribution in each window. To find the maximum spacing $\\Delta s_{\\max}$, we set $O(\\Delta s_{\\max}) = O_{\\min}$:\n$$\\exp\\left(-\\frac{\\Delta s_{\\max}^{2}}{8 \\sigma^{2}}\\right) = O_{\\min}$$\nSolving for $\\Delta s_{\\max}$:\n$$-\\frac{\\Delta s_{\\max}^{2}}{8 \\sigma^{2}} = \\ln(O_{\\min})$$\n$$\\Delta s_{\\max}^{2} = -8 \\sigma^{2} \\ln(O_{\\min}) = 8 \\sigma^{2} \\ln\\left(\\frac{1}{O_{\\min}}\\right)$$\n$$\\Delta s_{\\max} = \\sqrt{8 \\sigma^{2} \\ln\\left(\\frac{1}{O_{\\min}}\\right)}$$\nFirst, we calculate the variance $\\sigma^2$:\n$$\\sigma^{2} = \\frac{k_{\\mathrm{B}} T}{k} = \\frac{2.4943387854\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1}}{1200\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1}\\,\\mathrm{nm}^{-2}} \\approx 0.0020786\\,\\mathrm{nm}^{2}$$\nNow, we can calculate $\\Delta s_{\\max}$:\n$$\\Delta s_{\\max} = \\sqrt{8 \\times (0.0020786\\,\\mathrm{nm}^{2}) \\times \\ln\\left(\\frac{1}{0.248869}\\right)} = \\sqrt{0.016629\\,\\mathrm{nm}^{2} \\times \\ln(4.018177...)}$$\n$$\\Delta s_{\\max} = \\sqrt{0.016629\\,\\mathrm{nm}^{2} \\times 1.390812} = \\sqrt{0.0231365\\,\\mathrm{nm}^{2}} \\approx 0.152107\\,\\mathrm{nm}$$\n\nStep 3: Calculate the minimum number of windows $M$.\nThe problem specifies the formula for the number of windows $M$ required to cover the range $L = s_{\\max} - s_{\\min} = 2\\,\\mathrm{nm}$ with spacing $\\Delta s$:\n$$M = \\left\\lceil \\frac{s_{\\max} - s_{\\min}}{\\Delta s} \\right\\rceil + 1$$\nTo find the minimal integer $M$, we must use the maximal possible spacing, $\\Delta s_{\\max}$:\n$$M = \\left\\lceil \\frac{2\\,\\mathrm{nm}}{0.152107\\,\\mathrm{nm}} \\right\\rceil + 1$$\n$$M = \\lceil 13.1486 \\rceil + 1 = 14 + 1 = 15$$\nThe minimal integer number of windows required is $15$.\n\nBrief explanation of trade-offs:\nThe relationships derived above illuminate the key trade-offs in designing an umbrella sampling simulation. The number of windows $M$ is primarily determined by the maximum allowed spacing $\\Delta s_{\\max}$, with $M$ being inversely related to $\\Delta s_{\\max}$.\n- Trade-off with force constant $k$: The spacing is given by $\\Delta s_{\\max} = \\sqrt{8(k_{\\mathrm{B}} T/k) \\ln(1/O_{\\min})}$, so $\\Delta s_{\\max} \\propto 1/\\sqrt{k}$. A stiffer harmonic restraint (larger $k$) leads to narrower sampling distributions (smaller $\\sigma^2$). To maintain the necessary overlap between windows, they must be placed closer together (smaller $\\Delta s_{\\max}$), thus increasing the total number of windows $M$.\n- Trade-off with sampling time $t_{w}$: The spacing depends on sampling time through the minimum overlap, $O_{\\min} = \\frac{(k_{\\mathrm{B}} T)^2}{2(t_w/2\\tau_{\\mathrm{int}}) \\delta F^2} \\propto 1/t_w$. A longer simulation time per window, $t_w$, increases the number of effective samples $N_{\\mathrm{eff}}$, which means a smaller degree of overlap $O_{\\min}$ is sufficient to achieve the target precision $\\delta F$. A smaller $O_{\\min}$ allows for a larger spacing $\\Delta s_{\\max}$ (since $\\Delta s_{\\max} \\propto \\sqrt{\\ln(1/O_{\\min})}$). Therefore, increasing $t_w$ reduces the required number of windows $M$. This represents a fundamental trade-off: one can either run a larger number of windows for a shorter time or a smaller number of windows for a longer time to achieve the same overall quality in the final PMF, for a fixed $k$ and $\\delta F$.",
            "answer": "$$\\boxed{15}$$"
        },
        {
            "introduction": "Many enhanced sampling methods, including umbrella sampling and metadynamics, rely on reweighting techniques to recover the unbiased free energy landscape from biased data. The success of this procedure hinges on the quality of the statistical overlap between the sampled (biased) and target (unbiased) distributions. This exercise delves into the theoretical foundations of reweighting efficiency, asking you to derive the variance of an importance sampling estimator. By connecting sampling variance to the effective sample size $N_{\\mathrm{eff}}$ and the Rényi divergence, you will develop a rigorous understanding of how to quantify and benchmark the reliability of an enhanced sampling strategy .",
            "id": "3410765",
            "problem": "Consider a Molecular Dynamics (MD) enhanced sampling protocol that biases sampling in a low-dimensional collective variable $z$ using a bias potential $V(z)$ at inverse temperature $\\beta$. Let the unbiased target density be $p(z) \\propto \\exp(-\\beta U(z))$ and the biased sampling density be $q(z) \\propto \\exp(-\\beta U(z) - \\beta V(z))$. Assume that samples $\\{z_i\\}_{i=1}^{N}$ are collected at a stride larger than the decorrelation time so that they are independent and identically distributed under $q(z)$. Define the importance weights $w(z) = p(z)/q(z)$, and use the self-normalized importance sampling estimator to estimate the target expectation of an observable $A(z)$,\n$$\n\\hat{\\mu} = \\frac{\\sum_{i=1}^{N} w(z_i) A(z_i)}{\\sum_{i=1}^{N} w(z_i)}.\n$$\nAssume that $A(z)$ has finite variance under the target distribution $p(z)$, denoted by $\\sigma_A^2$, and that fluctuations of $A(z)$ are uncorrelated with the weights $w(z)$ in the sense that conditional on the realized weights, $A(z_i)$ are independent with variance $\\sigma_A^2$ and independent of $\\{w(z_i)\\}$.\n\n1. Starting only from the definitions above and standard probability facts, derive an asymptotic expression (large $N$) for the variance of $\\hat{\\mu}$ as a function of $N$ and the second moment of the weights under $q$, $\\mathbb{E}_q[w^2]$. Introduce the effective sample size $N_{\\mathrm{eff}}$ and express the variance in terms of $N_{\\mathrm{eff}}$ as well.\n\n2. Define the order-$2$ Rényi divergence from $p$ to $q$ by $D_{2}(p\\|q) = \\ln \\int \\frac{p(z)^{2}}{q(z)} \\, dz$. Express the ratio $N_{\\mathrm{eff}}/N$ in terms of $D_{2}(p\\|q)$. Now, impose the quantitative reliability criterion that the coefficient of variation squared of the weights, $\\mathrm{CV}^{2}(w) = \\frac{\\mathrm{Var}_q(w)}{(\\mathbb{E}_q[w])^{2}}$, does not exceed $1$. Under this criterion, compute the corresponding upper bound on $D_{2}(p\\|q)$ and give its numerical value rounded to four significant figures.\n\nReport your final answer as a two-entry row matrix where the first entry is the closed-form expression for the asymptotic variance derived in part 1, and the second entry is the numerical threshold for $D_{2}(p\\|q)$ from part 2. Round the second entry to four significant figures. No units are required.",
            "solution": "This problem is valid as it is scientifically grounded in statistical mechanics and importance sampling theory, is well-posed, and uses objective, formal language. We can proceed with the solution.\n\nThe problem is divided into two parts. First, we derive the asymptotic variance of the self-normalized importance sampling estimator. Second, we relate the sampling efficiency to the Rényi divergence and evaluate a reliability criterion.\n\nPart 1: Asymptotic Variance of $\\hat{\\mu}$\n\nThe self-normalized importance sampling estimator for the expectation $\\mu = \\mathbb{E}_p[A(z)]$ is given by\n$$ \\hat{\\mu} = \\frac{\\sum_{i=1}^{N} w(z_i) A(z_i)}{\\sum_{i=1}^{N} w(z_i)} = \\frac{\\frac{1}{N} \\sum_{i=1}^{N} w_i A_i}{\\frac{1}{N} \\sum_{i=1}^{N} w_i} $$\nwhere $w_i = w(z_i)$ and $A_i = A(z_i)$, and the samples $\\{z_i\\}$ are drawn i.i.d. from the biased distribution $q(z)$. This is a ratio of two sample means, $\\hat{\\mu} = \\bar{X}/\\bar{Y}$, with $X_i = w_i A_i$ and $Y_i = w_i$.\n\nTo find the asymptotic variance of $\\hat{\\mu}$ for large $N$, we use the delta method, which involves a first-order Taylor expansion of the ratio around the expectations of the numerator and denominator. First, we compute these expectations under the sampling distribution $q(z)$:\nThe expectation of $Y_i = w_i$ is\n$$ \\mathbb{E}_q[w] = \\int w(z) q(z) \\, dz = \\int \\frac{p(z)}{q(z)} q(z) \\, dz = \\int p(z) \\, dz = 1 $$\nas $p(z)$ is a probability density.\nThe expectation of $X_i = w_i A_i$ is\n$$ \\mathbb{E}_q[wA] = \\int w(z) A(z) q(z) \\, dz = \\int \\frac{p(z)}{q(z)} A(z) q(z) \\, dz = \\int A(z) p(z) \\, dz = \\mathbb{E}_p[A(z)] = \\mu $$\nThe delta method approximation for the variance of the ratio $\\hat{\\mu} = \\bar{X}/\\bar{Y}$ is given by:\n$$ \\mathrm{Var}_q(\\hat{\\mu}) \\approx \\frac{1}{N} \\frac{1}{(\\mathbb{E}_q[w])^2} \\mathrm{Var}_q(wA - \\mu w) $$\nSubstituting $\\mathbb{E}_q[w]=1$:\n$$ \\mathrm{Var}_q(\\hat{\\mu}) \\approx \\frac{1}{N} \\mathrm{Var}_q(w(A - \\mu)) $$\nThe variance term is $\\mathrm{Var}_q(w(A - \\mu)) = \\mathbb{E}_q[(w(A - \\mu))^2] - (\\mathbb{E}_q[w(A - \\mu)])^2$. The second term is zero, since $\\mathbb{E}_q[w(A - \\mu)] = \\mathbb{E}_q[wA] - \\mu \\mathbb{E}_q[w] = \\mu - \\mu \\cdot 1 = 0$.\nThus, we need to evaluate $\\mathbb{E}_q[w^2(A - \\mu)^2]$:\n$$ \\mathbb{E}_q[w^2(A - \\mu)^2] = \\int w(z)^2 (A(z) - \\mu)^2 q(z) \\, dz = \\int \\frac{p(z)}{q(z)} (A(z) - \\mu)^2 p(z) \\, dz = \\mathbb{E}_p[w(A - \\mu)^2] $$\nThe problem states that \"fluctuations of $A(z)$ are uncorrelated with the weights $w(z)$\". This is a simplifying assumption. In this context, it is standard to interpret this as the covariance between the weight function and functions of the observable being negligible, i.e., $\\mathrm{Cov}_p(w, g(A)) \\approx 0$ for relevant functions $g$. Here, we assume $\\mathrm{Cov}_p(w, (A-\\mu)^2) = 0$. This implies:\n$$ \\mathbb{E}_p[w(A - \\mu)^2] \\approx \\mathbb{E}_p[w] \\mathbb{E}_p[(A - \\mu)^2] $$\nThe second term is the variance of $A$ under $p$, which is given as $\\sigma_A^2$. The first term is:\n$$ \\mathbb{E}_p[w] = \\int w(z) p(z) \\, dz = \\int \\frac{p(z)}{q(z)} p(z) \\, dz = \\int \\frac{p(z)^2}{q(z)} \\, dz $$\nThis can be expressed as an expectation over $q$:\n$$ \\int \\frac{p(z)^2}{q(z)^2} q(z) \\, dz = \\int w(z)^2 q(z) \\, dz = \\mathbb{E}_q[w^2] $$\nCombining these results, we get:\n$$ \\mathbb{E}_q[w^2(A - \\mu)^2] \\approx \\mathbb{E}_q[w^2] \\sigma_A^2 $$\nSubstituting this back into the expression for the variance of $\\hat{\\mu}$:\n$$ \\mathrm{Var}_q(\\hat{\\mu}) \\approx \\frac{\\sigma_A^2 \\mathbb{E}_q[w^2]}{N} $$\nThis expression gives the asymptotic variance as a function of $N$ and $\\mathbb{E}_q[w^2]$.\n\nThe effective sample size, $N_{\\mathrm{eff}}$, is defined by equating the variance of the estimator to that of a simple Monte Carlo estimate with $N_{\\mathrm{eff}}$ samples, i.e., $\\mathrm{Var}_q(\\hat{\\mu}) = \\sigma_A^2 / N_{\\mathrm{eff}}$. Comparing this with our derived expression gives:\n$$ \\frac{\\sigma_A^2}{N_{\\mathrm{eff}}} = \\frac{\\sigma_A^2 \\mathbb{E}_q[w^2]}{N} \\implies N_{\\mathrm{eff}} = \\frac{N}{\\mathbb{E}_q[w^2]} $$\nThus, the variance can also be written as $\\mathrm{Var}_q(\\hat{\\mu}) \\approx \\sigma_A^2/N_{\\mathrm{eff}}$.\n\nPart 2: Rényi Divergence and Reliability Criterion\n\nThe order-$2$ Rényi divergence is given by $D_{2}(p\\|q) = \\ln \\int \\frac{p(z)^{2}}{q(z)} \\, dz$. From the derivation in Part 1, we recognize the integral as $\\mathbb{E}_q[w^2]$.\n$$ D_{2}(p\\|q) = \\ln(\\mathbb{E}_q[w^2]) $$\nThis implies $\\mathbb{E}_q[w^2] = \\exp(D_{2}(p\\|q))$.\nThe ratio of the effective sample size to the total number of samples is:\n$$ \\frac{N_{\\mathrm{eff}}}{N} = \\frac{1}{\\mathbb{E}_q[w^2]} = \\frac{1}{\\exp(D_{2}(p\\|q))} = \\exp(-D_{2}(p\\|q)) $$\nNext, we analyze the reliability criterion. The coefficient of variation squared of the weights, $\\mathrm{CV}^{2}(w)$, is defined as:\n$$ \\mathrm{CV}^{2}(w) = \\frac{\\mathrm{Var}_q(w)}{(\\mathbb{E}_q[w])^{2}} $$\nWe have already calculated the mean and can express the variance of the weights:\n$$ \\mathbb{E}_q[w] = 1 $$\n$$ \\mathrm{Var}_q(w) = \\mathbb{E}_q[w^2] - (\\mathbb{E}_q[w])^2 = \\mathbb{E}_q[w^2] - 1 $$\nSubstituting these into the expression for $\\mathrm{CV}^{2}(w)$:\n$$ \\mathrm{CV}^{2}(w) = \\frac{\\mathbb{E}_q[w^2] - 1}{1^2} = \\mathbb{E}_q[w^2] - 1 $$\nThe reliability criterion is $\\mathrm{CV}^{2}(w) \\le 1$. Applying our expression:\n$$ \\mathbb{E}_q[w^2] - 1 \\le 1 \\implies \\mathbb{E}_q[w^2] \\le 2 $$\nTo find the corresponding upper bound on the Rényi divergence, we use the relationship $D_{2}(p\\|q) = \\ln(\\mathbb{E}_q[w^2])$. Since the natural logarithm is a monotonically increasing function, the inequality on its argument translates directly to the function value:\n$$ D_{2}(p\\|q) \\le \\ln(2) $$\nThe numerical value of this upper bound, rounded to four significant figures, is:\n$$ \\ln(2) \\approx 0.693147... \\approx 0.6931 $$\n\nThe final answer requires a two-entry row matrix containing the asymptotic variance from Part 1 and the numerical threshold for $D_2(p\\|q)$ from Part 2.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sigma_A^2 \\mathbb{E}_q[w^2]}{N} & 0.6931\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Real-world molecular processes often involve multiple, anisotropically coupled collective variables, where the free energy landscape is steep in some directions and flat in others. A naive, uniform sampling grid is highly inefficient in such cases. This advanced, hands-on coding practice challenges you to design and benchmark an adaptive, anisotropic grid for multidimensional umbrella sampling. You will implement a protocol that equalizes the statistical overlap between adjacent windows, measured by the Kullback-Leibler divergence, and demonstrate the significant computational savings of this intelligent design compared to a uniform grid .",
            "id": "3410778",
            "problem": "Design a complete, runnable program that constructs an anisotropic grid of umbrella sampling windows in a multidimensional quadratic free energy landscape such that the overlap between adjacent windows, measured by the Kullback–Leibler (KL) divergence, is equalized along each coordinate axis to a specified target. Then benchmark the grid efficiency against a uniform (isotropic) grid that uses a single spacing chosen to satisfy the same KL constraint along the worst axis. All calculations must be posed and solved in dimensionless reduced units.\n\nAssume a canonical ensemble at fixed temperature and consider an underlying quadratic potential energy\n$$\nU(\\mathbf{x})=\\tfrac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{C}\\,\\mathbf{x},\n$$\nwhere $\\mathbf{C}$ is a diagonal, positive semi-definite curvature matrix with diagonal entries $c_i \\ge 0$ for $i \\in \\{1,\\dots,d\\}$ and $\\mathbf{x} \\in \\mathbb{R}^d$. An umbrella sampling window centered at $\\boldsymbol{\\mu}\\in\\mathbb{R}^d$ applies a harmonic bias\n$$\nW_{\\boldsymbol{\\mu}}(\\mathbf{x})=\\tfrac{1}{2}\\,(\\mathbf{x}-\\boldsymbol{\\mu})^{\\top}\\mathbf{K}\\,(\\mathbf{x}-\\boldsymbol{\\mu}),\n$$\nwhere $\\mathbf{K}$ is diagonal with entries $k_i>0$. Let $\\beta=1$ in reduced units so that all quantities are dimensionless.\n\nFundamental base and modeling assumptions:\n- The biased distribution in window $\\boldsymbol{\\mu}$ is Gaussian, because the sum of quadratic forms is quadratic. Its covariance is $\\boldsymbol{\\Sigma}=(\\mathbf{C}+\\mathbf{K})^{-1}$ and its mean is $\\mathbf{m}=(\\mathbf{C}+\\mathbf{K})^{-1}\\mathbf{K}\\,\\boldsymbol{\\mu}$.\n- The Kullback–Leibler (KL) divergence between two $d$-dimensional Gaussian distributions with identical covariance $\\boldsymbol{\\Sigma}$ and means $\\mathbf{m}_a$ and $\\mathbf{m}_b$ is\n$$\nD_{\\mathrm{KL}}=\\tfrac{1}{2}(\\mathbf{m}_a-\\mathbf{m}_b)^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{m}_a-\\mathbf{m}_b).\n$$\n\nGrid design objective:\n- Consider a tensor-product grid of window centers with spacing only along coordinate axes. Denote the side half-extent along axis $i$ as $L_i>0$, so that the domain to cover is $\\prod_{i=1}^d[-L_i,L_i]$.\n- For adjacent windows that differ only by a shift $\\delta_i$ along axis $i$, equalize the KL divergence to a specified target $D>0$. Because $\\mathbf{C}$ and $\\mathbf{K}$ are diagonal, the one-axis neighbor KL condition reduces to\n$$\nD=\\tfrac{1}{2}\\,\\delta_i^2\\,\\frac{k_i^2}{c_i+k_i}.\n$$\n- Solve this constraint to obtain the anisotropic per-axis grid spacing $\\delta_i$ that equalizes the overlap:\n$$\n\\delta_i=\\sqrt{2D\\,\\frac{(c_i+k_i)}{k_i^2}}.\n$$\n\nEfficiency benchmark:\n- For the anisotropic grid, the number of windows along axis $i$ is\n$$\nn_i=\\max\\Big(1,\\,\\left\\lfloor \\frac{2L_i}{\\delta_i}\\right\\rfloor + 1\\Big),\n$$\nand the total number of windows is $N_{\\mathrm{aniso}}=\\prod_{i=1}^d n_i$.\n- For a uniform (isotropic) grid that uses one spacing $\\delta_{\\mathrm{uni}}$ across all axes, choose\n$$\n\\delta_{\\mathrm{uni}}=\\min_{i} \\delta_i\n$$\nto satisfy the KL constraint on the most restrictive axis, and compute\n$$\nn^{\\mathrm{uni}}_i=\\max\\Big(1,\\,\\left\\lfloor \\frac{2L_i}{\\delta_{\\mathrm{uni}}}\\right\\rfloor + 1\\Big),\\quad N_{\\mathrm{uni}}=\\prod_{i=1}^d n^{\\mathrm{uni}}_i.\n$$\n- Define the efficiency ratio as\n$$\n\\mathcal{R}=\\frac{N_{\\mathrm{uni}}}{N_{\\mathrm{aniso}}}.\n$$\n\nProgramming task:\n- Implement the formulas above to compute $\\delta_i$, $n_i$, $N_{\\mathrm{aniso}}$, $\\delta_{\\mathrm{uni}}$, $n^{\\mathrm{uni}}_i$, $N_{\\mathrm{uni}}$, and $\\mathcal{R}$ for each test case in the suite below.\n- For each test case, return the triple $[N_{\\mathrm{aniso}},N_{\\mathrm{uni}},\\mathcal{R}]$ with $\\mathcal{R}$ rounded to six decimal places.\n\nTest suite:\n- Case $1$ (one-dimensional happy path): $d=1$, $\\mathbf{C}=\\operatorname{diag}([1.0])$, $\\mathbf{K}=\\operatorname{diag}([9.0])$, $\\mathbf{L}=[3.0]$, $D=0.5$.\n- Case $2$ (two-dimensional anisotropy): $d=2$, $\\mathbf{C}=\\operatorname{diag}([1.0,0.25])$, $\\mathbf{K}=\\operatorname{diag}([4.0,1.0])$, $\\mathbf{L}=[2.0,3.0]$, $D=0.5$.\n- Case $3$ (edge case, extremely weak stiffness on one axis): $d=2$, $\\mathbf{C}=\\operatorname{diag}([0.0,0.0])$, $\\mathbf{K}=\\operatorname{diag}([1\\times 10^{-6},100.0])$, $\\mathbf{L}=[1.0,1.0]$, $D=0.5$.\n- Case $4$ (three-dimensional mixed curvatures): $d=3$, $\\mathbf{C}=\\operatorname{diag}([2.0,0.5,0.5])$, $\\mathbf{K}=\\operatorname{diag}([3.0,3.0,0.5])$, $\\mathbf{L}=[2.0,2.0,1.0]$, $D=0.2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the four test cases as a list of lists with no spaces, where each inner list is $[N_{\\mathrm{aniso}},N_{\\mathrm{uni}},\\mathcal{R}]$ and $\\mathcal{R}$ is rounded to six decimal places. For example, a valid output line has the form\n$$\n[[a_1,b_1,r_1],[a_2,b_2,r_2],[a_3,b_3,r_3],[a_4,b_4,r_4]]\n$$\nwith each $a_j$ and $b_j$ an integer and each $r_j$ a decimal rounded to six places.",
            "solution": "The problem statement has been critically reviewed and determined to be valid. It is scientifically grounded in the principles of statistical mechanics and computational chemistry, well-posed with all necessary information provided, and objective in its formulation. The underlying model, based on quadratic potentials and Gaussian distributions, is a standard and analytically tractable framework for analyzing enhanced sampling methods like umbrella sampling. All provided formulas are consistent with this model. We may therefore proceed with the solution.\n\nThe objective is to design and benchmark an anisotropic grid of umbrella sampling windows for a multidimensional system described by a quadratic potential energy surface. The design criterion is to equalize the overlap, quantified by the Kullback–Leibler (KL) divergence, between adjacent windows along each coordinate axis. This is compared to a naive uniform grid design.\n\nThe system's underlying potential energy is given by a quadratic form:\n$$\nU(\\mathbf{x})=\\tfrac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{C}\\,\\mathbf{x}\n$$\nwhere $\\mathbf{x} \\in \\mathbb{R}^d$ is the state vector and $\\mathbf{C}$ is a diagonal positive semi-definite matrix with entries $c_i \\ge 0$. Umbrella sampling introduces a harmonic bias potential for each window centered at $\\boldsymbol{\\mu}$:\n$$\nW_{\\boldsymbol{\\mu}}(\\mathbf{x})=\\tfrac{1}{2}\\,(\\mathbf{x}-\\boldsymbol{\\mu})^{\\top}\\mathbf{K}\\,(\\mathbf{x}-\\boldsymbol{\\mu})\n$$\nwhere $\\mathbf{K}$ is a diagonal matrix with positive entries $k_i > 0$. In reduced units where the inverse temperature $\\beta=1$, the total potential energy in a window is $U_{\\text{biased}}(\\mathbf{x}) = U(\\mathbf{x}) + W_{\\boldsymbol{\\mu}}(\\mathbf{x})$. Since this is a sum of quadratic forms, the resulting equilibrium probability distribution, $p(\\mathbf{x}) \\propto \\exp(-U_{\\text{biased}}(\\mathbf{x}))$, is a Gaussian distribution.\n\nAs given in the problem statement, the biased distribution for a window centered at $\\boldsymbol{\\mu}$ is a Gaussian with covariance $\\boldsymbol{\\Sigma}=(\\mathbf{C}+\\mathbf{K})^{-1}$ and mean $\\mathbf{m}=(\\mathbf{C}+\\mathbf{K})^{-1}\\mathbf{K}\\,\\boldsymbol{\\mu}$. The KL divergence between two such distributions at centers $\\boldsymbol{\\mu}_a$ and $\\boldsymbol{\\mu}_b$ simplifies to:\n$$\nD_{\\mathrm{KL}}=\\tfrac{1}{2}(\\mathbf{m}_a-\\mathbf{m}_b)^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{m}_a-\\mathbf{m}_b)\n$$\nFor two windows adjacent along axis $i$, separated by a displacement $\\delta_i$, this yields the per-axis constraint:\n$$\nD=\\tfrac{1}{2}\\,\\delta_i^2\\,\\frac{k_i^2}{c_i+k_i}\n$$\nwhere $D$ is the target KL divergence.\n\nThe core of the problem involves implementing the following computational steps for each test case.\n\n**1. Anisotropic Grid Construction**\n\nThe spacing $\\delta_i$ along each axis $i$ is determined by solving the KL-divergence constraint for $\\delta_i$. This ensures that the statistical overlap between adjacent windows is uniform along each respective axis, adapting the grid density to the local stiffness of the potential.\n$$\n\\delta_i=\\sqrt{2D\\,\\frac{(c_i+k_i)}{k_i^2}}\n$$\nThe domain to be sampled along axis $i$ is $[-L_i, L_i]$, a total length of $2L_i$. The number of windows $n_i$ required to cover this domain with spacing $\\delta_i$ is given by the number of intervals plus one, with a minimum of one window.\n$$\nn_i=\\max\\Big(1,\\,\\left\\lfloor \\frac{2L_i}{\\delta_i}\\right\\rfloor + 1\\Big)\n$$\nThe total number of windows for the anisotropic grid, being a tensor product of the one-dimensional grids, is the product of the per-axis window counts.\n$$\nN_{\\mathrm{aniso}}=\\prod_{i=1}^d n_i\n$$\n\n**2. Uniform (Isotropic) Grid Construction**\n\nFor benchmarking, a uniform grid is constructed. To ensure that the KL divergence constraint is met or exceeded along all axes (i.e., the overlap is at least as good as the target), the spacing for the uniform grid must be chosen as the minimum of all the calculated anisotropic spacings. This is because a smaller spacing corresponds to a smaller KL divergence (greater overlap).\n$$\n\\delta_{\\mathrm{uni}}=\\min_{i} \\delta_i\n$$\nThis single spacing is then applied to all axes. The number of windows $n^{\\mathrm{uni}}_i$ along each axis $i$ is calculated using this uniform spacing.\n$$\nn^{\\mathrm{uni}}_i=\\max\\Big(1,\\,\\left\\lfloor \\frac{2L_i}{\\delta_{\\mathrm{uni}}}\\right\\rfloor + 1\\Big)\n$$\nThe total number of windows for the uniform grid is the product of these counts.\n$$\nN_{\\mathrm{uni}}=\\prod_{i=1}^d n^{\\mathrm{uni}}_i\n$$\n\n**3. Efficiency Benchmark**\n\nThe efficiency of the anisotropic grid relative to the uniform grid is defined as the ratio of the total number of windows. A ratio greater than $1$ indicates that the anisotropic grid requires fewer windows, and is therefore more computationally efficient.\n$$\n\\mathcal{R}=\\frac{N_{\\mathrm{uni}}}{N_{\\mathrm{aniso}}}\n$$\n\nThese formulas will be systematically applied to each provided test case. The results, comprising the triplet $[N_{\\mathrm{aniso}}, N_{\\mathrm{uni}}, \\mathcal{R}]$, are then compiled. For instance, for Case $2$ ($d=2$, $\\mathbf{C}=\\operatorname{diag}([1.0, 0.25])$, $\\mathbf{K}=\\operatorname{diag}([4.0, 1.0])$, $\\mathbf{L}=[2.0, 3.0]$, $D=0.5$), the calculations proceed as follows:\n- Axis $1$: $c_1=1.0, k_1=4.0, L_1=2.0$. Spacing $\\delta_1 = \\sqrt{2(0.5)(1.0+4.0)/4.0^2} = \\sqrt{5/16} \\approx 0.559017$. Windows $n_1 = \\lfloor 2(2.0)/0.559017\\rfloor + 1 = 8$.\n- Axis $2$: $c_2=0.25, k_2=1.0, L_2=3.0$. Spacing $\\delta_2 = \\sqrt{2(0.5)(0.25+1.0)/1.0^2} = \\sqrt{1.25} \\approx 1.118034$. Windows $n_2 = \\lfloor 2(3.0)/1.118034\\rfloor + 1 = 6$.\n- Anisotropic total windows: $N_{\\mathrm{aniso}} = n_1 \\times n_2 = 8 \\times 6 = 48$.\n- Uniform grid spacing: $\\delta_{\\mathrm{uni}} = \\min(\\delta_1, \\delta_2) = \\delta_1 \\approx 0.559017$.\n- Uniform grid windows: $n^{\\mathrm{uni}}_1 = \\lfloor 2(2.0)/0.559017\\rfloor + 1 = 8$. $n^{\\mathrm{uni}}_2 = \\lfloor 2(3.0)/0.559017\\rfloor + 1 = 11$.\n- Uniform total windows: $N_{\\mathrm{uni}} = 8 \\times 11 = 88$.\n- Efficiency ratio: $\\mathcal{R} = 88 / 48 \\approx 1.833333$.\nThis procedure is general and will be encoded to solve for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and benchmarks anisotropic vs. uniform umbrella sampling grids.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (d=1, C=[1.0], K=[9.0], L=[3.0], D=0.5)\n        (np.array([1.0]), np.array([9.0]), np.array([3.0]), 0.5),\n        # Case 2 (d=2, C=[1.0, 0.25], K=[4.0, 1.0], L=[2.0, 3.0], D=0.5)\n        (np.array([1.0, 0.25]), np.array([4.0, 1.0]), np.array([2.0, 3.0]), 0.5),\n        # Case 3 (d=2, C=[0.0, 0.0], K=[1e-6, 100.0], L=[1.0, 1.0], D=0.5)\n        (np.array([0.0, 0.0]), np.array([1.0e-6, 100.0]), np.array([1.0, 1.0]), 0.5),\n        # Case 4 (d=3, C=[2.0, 0.5, 0.5], K=[3.0, 3.0, 0.5], L=[2.0, 2.0, 1.0], D=0.2)\n        (np.array([2.0, 0.5, 0.5]), np.array([3.0, 3.0, 0.5]), np.array([2.0, 2.0, 1.0]), 0.2),\n    ]\n\n    results = []\n    for case in test_cases:\n        c_diag, k_diag, L, D = case\n\n        # Anisotropic Grid Calculation\n        # delta_i = sqrt(2*D * (c_i + k_i) / k_i^2)\n        delta_aniso = np.sqrt(2 * D * (c_diag + k_diag) / (k_diag**2))\n        \n        # n_i = max(1, floor(2*L_i / delta_i) + 1)\n        n_aniso_per_axis = np.maximum(1, np.floor(2 * L / delta_aniso) + 1).astype(int)\n        \n        # N_aniso = product(n_i)\n        N_aniso = np.prod(n_aniso_per_axis)\n\n        # Uniform Grid Calculation\n        # delta_uni = min(delta_i)\n        delta_uni = np.min(delta_aniso)\n        \n        # n_uni_i = max(1, floor(2*L_i / delta_uni) + 1)\n        n_uni_per_axis = np.maximum(1, np.floor(2 * L / delta_uni) + 1).astype(int)\n        \n        # N_uni = product(n_uni_i)\n        N_uni = np.prod(n_uni_per_axis)\n\n        # Efficiency Ratio Calculation\n        # R = N_uni / N_aniso\n        if N_aniso == 0:\n            # Avoid division by zero, although not expected in this problem\n            R = 0.0\n        else:\n            R = N_uni / N_aniso\n        \n        # Round R to 6 decimal places as required.\n        R_rounded = round(R, 6)\n\n        results.append([int(N_aniso), int(N_uni), R_rounded])\n\n    # Format the final output string according to the specification.\n    # e.g., [[a1,b1,r1],[a2,b2,r2],...]\n    output_str_parts = []\n    for res in results:\n        # Format each inner list into \"[a,b,r]\"\n        output_str_parts.append(f\"[{res[0]},{res[1]},{res[2]}]\")\n    \n    # Join the parts into the final string \"[part1,part2,...]\"\n    final_output = f\"[{','.join(output_str_parts)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}