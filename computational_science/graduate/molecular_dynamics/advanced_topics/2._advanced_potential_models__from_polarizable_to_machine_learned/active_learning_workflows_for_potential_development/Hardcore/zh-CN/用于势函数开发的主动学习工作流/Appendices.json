{
    "hands_on_practices": [
        {
            "introduction": "在主动学习中，模型识别其预测何时不可靠的能力至关重要。高斯过程（GP）为此提供了一个有原则的框架来量化这种不确定性，使其成为开发机器学习势函数的热门选择。本练习将深入探讨GP后验方差的数学基础，通过推导和计算，你将理解模型不确定性是如何由训练数据的几何分布和核函数决定的。",
            "id": "3394179",
            "problem": "在通过主动学习进行分子动力学的原子间势开发中，考虑将力场的单个笛卡尔分量（表示为标量函数 $f(\\mathbf{R})$）建模为一个零均值且具有正定核 $k(\\mathbf{R},\\mathbf{R}')$ 的高斯过程（GP）。假设我们构建一个描述符映射 $s: \\mathbf{R} \\mapsto u \\in \\mathbb{R}$，并假定核函数仅通过具有幅度和长度尺度参数的平方指数形式依赖于这些描述符，因此 $k(\\mathbf{R},\\mathbf{R}') \\equiv k(u,u') = \\sigma_{f}^{2} \\exp\\!\\big(-\\frac{(u-u')^{2}}{2\\ell^{2}}\\big)$。在训练构型上对 $f$ 的测量受到方差为 $\\sigma_{n}^{2}$ 的独立高斯噪声的干扰。\n\n从在训练输入和一个新测试输入上评估的 $f$ 的联合高斯先验出发，并使用多元高斯分布的条件化法则，推导在新构型 $\\mathbf{R}_{*}$（其描述符为 $u_{*}$）处 $f$ 的预测后验方差，用训练集上的核格兰姆矩阵以及 $u_{*}$ 与训练描述符之间的核向量来表示。然后，利用正定核的特征映射对此方差进行几何解释：说明它如何等于特征向量 $\\varphi(u_{*})$ 在训练特征向量的生成空间上进行正交投影后的残差的平方范数，其中观测噪声作为格兰姆矩阵的 Tikhonov 正则化项进入。明确地将其与特征空间中训练集的几何结构联系起来。\n\n最后，针对以下与主动学习查询相关的具体自洽场景，计算预测后验方差：\n- 使用平方指数核 $k(u,u') = \\sigma_{f}^{2}\\exp\\!\\big(-\\frac{(u-u')^{2}}{2\\ell^{2}}\\big)$，超参数为 $\\sigma_{f}^{2} = 2$ 和 $\\ell = 1$。\n- 训练描述符为 $u_{1} = 0$ 和 $u_{2} = 2$。\n- 观测噪声方差为 $\\sigma_{n}^{2} = 0.1$。\n- 测试描述符为 $u_{*} = 1$。\n\n将 $u_{*}$ 处的预测后验方差报告为单个实数。将您的答案四舍五入到四位有效数字。该量是无量纲的；报告时无需单位。",
            "solution": "该问题要求推导和计算一个以高斯过程（GP）建模的函数的预测后验方差，并给出几何解释。问题陈述是有效的，因为它具有科学依据，是良定的，并为唯一解提供了所有必要信息。\n\n设训练输入集由其标量描述符 $U = \\{u_1, \\dots, u_N\\}$ 表示，函数 $f$ 相应的含噪观测值收集在向量 $\\mathbf{y} = (y_1, \\dots, y_N)^T$ 中。模型假设每个观测值 $y_i$ 与潜函数值 $f_i = f(u_i)$ 通过 $y_i = f_i + \\epsilon_i$ 相关联，其中 $\\epsilon_i$ 是独立同分布的高斯噪声变量，均值为0，方差为 $\\sigma_n^2$，即 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$。\n\n函数 $f$ 被建模为一个零均值且协方差函数（核）为 $k(u, u')$ 的高斯过程。令 $\\mathbf{f} = (f(u_1), \\dots, f(u_N))^T$ 为训练点处的潜函数值向量，而 $f_* = f(u_*)$ 为新测试点 $u_*$ 处的潜函数值。\n\n根据 GP 先验，任意函数值集合的联合分布是多元高斯分布。因此，训练观测值 $\\mathbf{y}$ 和测试函数值 $f_*$ 的联合分布是零均值的高斯分布。我们来推导其协方差矩阵。两个观测值 $y_i$ 和 $y_j$ 之间的协方差为：\n$$ \\text{cov}(y_i, y_j) = \\text{cov}(f_i + \\epsilon_i, f_j + \\epsilon_j) = \\text{cov}(f_i, f_j) + \\text{cov}(\\epsilon_i, \\epsilon_j) = k(u_i, u_j) + \\sigma_n^2 \\delta_{ij} $$\n其中 $\\delta_{ij}$ 是克罗内克 δ。这可以写成矩阵形式 $K_U + \\sigma_n^2 I$，其中 $(K_U)_{ij} = k(u_i, u_j)$ 是训练输入的格拉姆矩阵。\n一个观测值 $y_i$ 和测试值 $f_*$ 之间的协方差是：\n$$ \\text{cov}(y_i, f_*) = \\text{cov}(f_i + \\epsilon_i, f_*) = \\text{cov}(f_i, f_*) = k(u_i, u_*) $$\n令 $\\mathbf{k}_*$ 为测试点与每个训练点之间的协方差向量，即 $(\\mathbf{k}_*)_i = k(u_i, u_*)$。\n测试值的方差为 $\\text{var}(f_*) = \\text{cov}(f_*, f_*) = k(u_*, u_*)$。\n\n因此，联合分布为：\n$$ \\begin{pmatrix} \\mathbf{y} \\\\ f_* \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{0} \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} K_U + \\sigma_n^2 I  \\mathbf{k}_* \\\\ \\mathbf{k}_*^T  k(u_*, u_*) \\end{pmatrix} \\right) $$\n对于由\n$$ \\begin{pmatrix} \\mathbf{x}_a \\\\ \\mathbf{x}_b \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{aa}  \\Sigma_{ab} \\\\ \\Sigma_{ba}  \\Sigma_{bb} \\end{pmatrix} \\right) $$\n给出的变量 $\\mathbf{x}_a$ 和 $\\mathbf{x}_b$ 的一般多元高斯分布，其条件分布 $p(\\mathbf{x}_b | \\mathbf{x}_a)$ 也是高斯分布，其均值为 $\\boldsymbol{\\mu}_{b|a} = \\boldsymbol{\\mu}_b + \\Sigma_{ba}\\Sigma_{aa}^{-1}(\\mathbf{x}_a - \\boldsymbol{\\mu}_a)$，协方差为 $\\Sigma_{b|a} = \\Sigma_{bb} - \\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}$。\n\n将此条件化法则应用于我们的情况，我们确定 $\\mathbf{x}_a = \\mathbf{y}$，$\\mathbf{x}_b = f_*$，$\\boldsymbol{\\mu}_a = \\mathbf{0}$，$\\boldsymbol{\\mu}_b = 0$，$\\Sigma_{aa} = K_U + \\sigma_n^2 I$，$\\Sigma_{ab} = \\mathbf{k}_*$，$\\Sigma_{ba} = \\mathbf{k}_*^T$，以及 $\\Sigma_{bb} = k(u_*, u_*)$。\n后验预测分布 $p(f_* | \\mathbf{y})$ 是高斯分布。后验均值为 $\\bar{f}_* = \\mathbf{k}_*^T (K_U + \\sigma_n^2 I)^{-1} \\mathbf{y}$。后验方差，也就是问题所求的，是：\n$$ \\text{var}(f_* | \\mathbf{y}) = k(u_*, u_*) - \\mathbf{k}_*^T (K_U + \\sigma_n^2 I)^{-1} \\mathbf{k}_* $$\n\n对于几何解释，我们引入与正定核 $k$ 相关联的再生核希尔伯特空间（RKHS）的概念，记为 $\\mathcal{H}$。存在一个特征映射 $\\varphi: u \\mapsto \\varphi(u) \\in \\mathcal{H}$，使得核函数是该空间中的内积：$k(u, u') = \\langle \\varphi(u), \\varphi(u') \\rangle_{\\mathcal{H}}$。\n利用这个性质，预测方差可以重写为：\n$$ \\text{var}(f_*) = \\|\\varphi(u_*)\\|^2_{\\mathcal{H}} - \\langle \\varphi(u_*), \\Phi \\rangle ( \\langle \\Phi, \\Phi \\rangle + \\sigma_n^2 I )^{-1} \\langle \\Phi, \\varphi(u_*) \\rangle $$\n其中 $\\Phi$ 是一个算子，其对向量 $\\boldsymbol{\\alpha} \\in \\mathbb{R}^N$ 的作用定义为 $\\Phi\\boldsymbol{\\alpha} = \\sum_{i=1}^N \\alpha_i \\varphi(u_i)$。项 $\\langle \\varphi(u_*), \\Phi \\rangle$ 是一个行向量，其元素为 $\\langle \\varphi(u_*), \\varphi(u_i) \\rangle = k(u_*, u_i)$，即 $\\mathbf{k}_*^T$。项 $\\langle \\Phi, \\Phi \\rangle$ 是一个矩阵，其元素为 $\\langle \\varphi(u_i), \\varphi(u_j) \\rangle = k(u_i, u_j)$，即格拉姆矩阵 $K_U$。\n第二项 $\\mathbf{k}_*^T (K_U + \\sigma_n^2 I)^{-1} \\mathbf{k}_*$ 代表了特征向量 $\\varphi(u_*)$ 在由训练特征向量 $\\{\\varphi(u_i)\\}_{i=1}^N$ 生成的子空间上的正则化正交投影的平方范数。先验方差 $k(u_*, u_*) = \\|\\varphi(u_*)\\|^2_{\\mathcal{H}}$ 是测试点特征向量的总平方“长度”。因此，预测方差是 $\\varphi(u_*)$ 中与此正则化投影正交的分量的平方范数。这是在特征空间中将 $\\varphi(u_*)$ 投影到训练数据生成空间后得到的残差向量的平方范数。观测噪声方差 $\\sigma_n^2$ 作为 Tikhonov 正则化项，加到格拉姆矩阵 $K_U$ 的对角线上。这种正则化对于数值稳定性至关重要，因为如果一些训练点彼此非常接近（即高度相关），$K_U$ 可能会是病态的或奇异的。从几何上看，这意味着我们不是投影到训练特征的精确生成空间上，而是解决一个惩罚大系数的正则化问题，从而有效地收缩了投影。\n\n最后，我们为给定场景计算预测后验方差。\n核函数为 $k(u,u') = \\sigma_{f}^{2}\\exp(-\\frac{(u-u')^{2}}{2\\ell^{2}})$，其中 $\\sigma_{f}^{2} = 2$ 且 $\\ell = 1$。所以，$k(u,u') = 2\\exp(-\\frac{(u-u')^{2}}{2})$。\n训练描述符为 $u_{1} = 0$ 和 $u_{2} = 2$。\n测试描述符为 $u_{*} = 1$。\n噪声方差为 $\\sigma_{n}^{2} = 0.1$。\n\n首先，我们计算必要的核函数值：\n测试点处的先验方差是 $k(u_*, u_*) = k(1, 1) = 2\\exp(0) = 2$。\n测试点与训练点之间的核向量是：\n$$ \\mathbf{k}_* = \\begin{pmatrix} k(1, 0) \\\\ k(1, 2) \\end{pmatrix} = \\begin{pmatrix} 2\\exp(-\\frac{(1-0)^2}{2}) \\\\ 2\\exp(-\\frac{(1-2)^2}{2}) \\end{pmatrix} = \\begin{pmatrix} 2\\exp(-0.5) \\\\ 2\\exp(-0.5) \\end{pmatrix} $$\n训练点的格拉姆矩阵是：\n$$ K_U = \\begin{pmatrix} k(0, 0)  k(0, 2) \\\\ k(2, 0)  k(2, 2) \\end{pmatrix} = \\begin{pmatrix} 2\\exp(0)  2\\exp(-\\frac{(0-2)^2}{2}) \\\\ 2\\exp(-\\frac{(2-0)^2}{2})  2\\exp(0) \\end{pmatrix} = \\begin{pmatrix} 2  2\\exp(-2) \\\\ 2\\exp(-2)  2 \\end{pmatrix} $$\n需要求逆的矩阵是 $M = K_U + \\sigma_n^2 I$：\n$$ M = \\begin{pmatrix} 2  2\\exp(-2) \\\\ 2\\exp(-2)  2 \\end{pmatrix} + \\begin{pmatrix} 0.1  0 \\\\ 0  0.1 \\end{pmatrix} = \\begin{pmatrix} 2.1  2\\exp(-2) \\\\ 2\\exp(-2)  2.1 \\end{pmatrix} $$\n方差的减少量由二次型 $\\mathbf{k}_*^T M^{-1} \\mathbf{k}_*$ 给出。\n设 $k_s = 2\\exp(-0.5)$ 且 $k_d = 2\\exp(-2)$。那么 $\\mathbf{k}_* = \\begin{pmatrix} k_s \\\\ k_s \\end{pmatrix}$ 且 $M = \\begin{pmatrix} 2.1  k_d \\\\ k_d  2.1 \\end{pmatrix}$。\n该项为 $k_s^2 \\begin{pmatrix} 1  1 \\end{pmatrix} M^{-1} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n$M$ 的逆矩阵是 $M^{-1} = \\frac{1}{(2.1)^2 - k_d^2} \\begin{pmatrix} 2.1  -k_d \\\\ -k_d  2.1 \\end{pmatrix}$。\n乘积 $\\begin{pmatrix} 1  1 \\end{pmatrix} M^{-1} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 是 $M^{-1}$ 所有元素之和，即：\n$$ \\frac{2.1 - k_d - k_d + 2.1}{(2.1)^2 - k_d^2} = \\frac{2(2.1 - k_d)}{(2.1 - k_d)(2.1 + k_d)} = \\frac{2}{2.1 + k_d} $$\n因此，方差的减少量为 $k_s^2 \\frac{2}{2.1 + k_d} = (2\\exp(-0.5))^2 \\frac{2}{2.1 + 2\\exp(-2)} = \\frac{8\\exp(-1)}{2.1 + 2\\exp(-2)}$。\n现在，我们计算数值：\n$$ \\frac{8\\exp(-1)}{2.1 + 2\\exp(-2)} \\approx \\frac{8 \\times 0.36787944}{2.1 + 2 \\times 0.13533528} = \\frac{2.9430355}{2.1 + 0.27067056} = \\frac{2.9430355}{2.37067056} \\approx 1.241434 $$\n预测后验方差是：\n$$ \\text{var}(f_*) = k(u_*, u_*) - \\frac{8\\exp(-1)}{2.1 + 2\\exp(-2)} \\approx 2 - 1.241434 = 0.758566 $$\n四舍五入到四位有效数字，预测后验方差为 $0.7586$。",
            "answer": "$$\\boxed{0.7586}$$"
        },
        {
            "introduction": "一旦我们有了量化不确定性的方法，下一步就是利用它来智能地选择数据。本练习将指导你实现一种核心的主动学习策略——贪婪选择算法，用于从大型未标记数据池中挑选信息量最丰富的构型进行标记。 通过这个实践，你不仅将掌握一种关键的数据选择技术，还将发现不同选择标准（如D-最优性和方差贪婪）之间的深刻联系，从而加深对实验设计原理的理解。",
            "id": "3394147",
            "problem": "您正在为分子动力学中的原子间势开发设计一个主动学习工作流。未标记的训练池由原子环境描述符组成，您将通过一个正定核来建模环境之间的相似性。您的目标是通过成对相似度函数来形式化池中的冗余，推导出能够保持下游代理模型性能的、有原则的剪枝标准，并实现一个程序来应用这些标准，并在一个测试套件上量化其结果。\n\n假设存在以下基础设定。\n\n- 每个原子环境由一个描述符向量 $x \\in \\mathbb{R}^d$ 表示。\n- 两个环境 $x_i, x_j$ 之间的相似度由径向基函数核定义：$s(x_i,x_j) = k(x_i,x_j) = \\exp\\!\\left(-\\frac{\\lVert x_i - x_j \\rVert_2^2}{2 \\ell^2}\\right)$，其中长度尺度 $\\ell  0$ 且振幅为单位1。\n- 一个用于能量或力分量 $f(x)$ 的高斯过程（GP）代理模型使用核函数 $k(\\cdot, \\cdot)$，并带有方差为 $\\sigma^2$ 的独立同分布观测噪声。\n- 在选择并标记索引子集 $S$ 后，池中某点 $x$ 的后验预测方差为\n$$\n\\mathrm{Var}_S(x) = k(x,x) - k(x,X_S)\\left(K_{SS} + \\sigma^2 I\\right)^{-1} k(X_S,x),\n$$\n其中 $X_S$ 表示所选的描述符，$K_{SS}$ 是 $S$ 上的格拉姆矩阵，$I$ 是单位矩阵，$k(x,X_S)$ 是 $x$ 与 $X_S$ 中每个元素之间核函数求值的向量。\n- 作为性能代理指标，使用在整个池 $T$（取 $T$ 为大小为 $N$ 的完整池）上的积分后验方差，定义为\n$$\n\\mathcal{V}(S) = \\sum_{i=1}^N \\mathrm{Var}_S(x_i).\n$$\n在保持代理模型精度的同时，通过剪枝来最小化该量。\n\n定义池相对于所选子集 $S$ 的冗余度分数为\n$$\n\\mathcal{R}(S) = \\frac{1}{N} \\sum_{i=1}^N \\max_{j \\in S} s(x_i, x_j),\n$$\n如果池中许多点与至少一个保留的代表点高度相似，则该值会很大。\n\n您必须实现两个从有原则的目标推导出的剪枝标准：\n\n- 通过贪心对数行列式最大化实现的 D-最优选择：选择一个大小为指定预算 $b$ 的子集 $S$，该子集贪心地最大化对数行列式\n$$\n\\Phi_D(S) = \\log \\det\\!\\left(K_{SS} + \\sigma^2 I\\right),\n$$\n使用分块行列式恒等式计算边际增益。此标准可作为信息增益和多样性的代理。\n- 通过贪心后验方差减小实现的方差贪心选择（等效于对核矩阵进行带主元的 Cholesky 分解）：迭代选择当前具有最大 $\\mathrm{Var}_S(x_j)$ 的点 $j$，直到预算 $b$ 用尽，并使用 Cholesky 更新高效地更新后验方差。此标准直接旨在减小 $\\mathcal{V}(S)$。\n\n从上述定义出发，并且除了核函数之外不假设任何特定的势函数形式，您必须：\n\n- 从给定的基本公式和恒等式中推导出每种贪心选择的边际增益。\n- 仅使用与所述观测噪声模型兼容的线性代数原语来实现两种贪心选择器，并确保数值稳定性。\n- 在一组指定的测试用例上，为每个选择器计算最终的积分后验方差 $\\mathcal{V}(S)$。\n\n您的程序必须硬编码以下测试套件，包括数据池、参数和预算。所有坐标都是无量纲的描述符值，相似度也是无量纲的。不需要物理单位。\n\n- 测试用例 $1$（聚类池，理想情况）：\n  - 描述符 $X \\subset \\mathbb{R}^2$，包含 $8$ 个点：\n    - 靠近 $(0,0)$ 的簇：$(0,0)$, $(0.1,-0.05)$, $(-0.1,0.05)$, $(0.05,0.1)$。\n    - 靠近 $(3,3)$ 的簇：$(3,3)$, $(3.1,2.9)$, $(2.9,3.1)$, $(3.05,3.0)$。\n  - 核函数长度尺度 $\\ell = 0.5$。\n  - 噪声标准差 $\\sigma = 0.05$。\n  - 预算 $b = 2$。\n\n- 测试用例 $2$（边界情况，零预算）：\n  - 描述符 $X \\subset \\mathbb{R}^2$，包含 $5$ 个点：$(0,0)$, $(1,0)$, $(0,1)$, $(1,1)$, $(0.5,0.5)$。\n  - 核函数长度尺度 $\\ell = 1.0$。\n  - 噪声标准差 $\\sigma = 0.1$。\n  - 预算 $b = 0$。\n\n- 测试用例 $3$（边界情况，全预算）：\n  - 描述符 $X \\subset \\mathbb{R}^2$，包含 $5$ 个点：$(0,0)$, $(2,0)$, $(0,2)$, $(2,2)$, $(1,1)$。\n  - 核函数长度尺度 $\\ell = 0.7$。\n  - 噪声标准差 $\\sigma = 0.01$。\n  - 预算 $b = 5$。\n\n- 测试用例 $4$（高冗余度，在 $\\mathbb{R}$ 中有重复点）：\n  - 描述符 $X \\subset \\mathbb{R}^1$，包含 $10$ 个点：$0$, $0$, $1$, $1$, $2$, $2$, $3$, $3$, $4$, $4$。\n  - 核函数长度尺度 $\\ell = 0.3$。\n  - 噪声标准差 $\\sigma = 0.05$。\n  - 预算 $b = 5$。\n\n对于每个测试用例，您的程序必须计算两个浮点数：\n\n- 积分后验方差 $\\mathcal{V}(S_D)$，其中 $S_D$ 是通过预算为 $b$ 的 D-最优贪心选择获得的。\n- 积分后验方差 $\\mathcal{V}(S_A)$，其中 $S_A$ 是通过预算为 $b$ 的方差贪心选择获得的。\n\n您的程序应生成单行输出，其中包含所有结果，按测试用例和方法 $\\left(\\mathcal{V}(S_D), \\mathcal{V}(S_A)\\right)$ 的顺序排列，并展开成一个列表。例如，输出格式应为\n$[$result$_{1,D}$, result$_{1,A}$, result$_{2,D}$, result$_{2,A}$, result$_{3,D}$, result$_{3,A}$, result$_{4,D}$, result$_{4,A}]$,\n每个浮点数四舍五入到 $6$ 位小数。不应打印任何其他文本。不涉及角度。不涉及百分比。所有量都是无量纲的。程序不得读取任何输入，也不得访问任何文件或网络。",
            "solution": "用户提供的问题经评估有效。它在科学上基于高斯过程回归和主动学习的原理，采用了诸如径向基函数核和后验预测方差等标准数学公式。该问题是适定 (well-posed) 的，为所定义的任务提供了所有必要的数据和参数，从而导出一个唯一、可计算的解。其语言客观、无歧义。因此，有必要提供一个完整的解。\n\n### 基于原则的设计与推导\n\n问题的核心是从一个更大的池中选择原子环境（由描述符向量表示）的子集，以训练代理模型。目标是智能地执行此操作，以最大化所选集合的信息内容，此过程称为主动学习或实验设计。问题指定了两种贪心选择标准来构建一个大小为 $b$ 的选定集 $S$。我们将为每个标准推导边际增益，它决定了贪心算法每一步的选择。\n\n设 $k$ 步后选择的索引集为 $S_k$。我们希望从可用索引集 $U$ 中选择下一个索引 $j$ 添加到 $S_k$ 中，形成 $S_{k+1} = S_k \\cup \\{j\\}$。\n\n#### 1. D-最优贪心选择\n\n此标准旨在通过贪心最大化数据协方差矩阵的行列式的对数 $\\Phi_D(S) = \\log \\det(K_{SS} + \\sigma^2 I)$ 来最大化所选集的信息内容。将点 $x_j$ 添加到集合 $S_k$ 的边际增益是：\n$$\n\\Delta \\Phi_D(j | S_k) = \\Phi_D(S_k \\cup \\{j\\}) - \\Phi_D(S_k)\n$$\n为了计算它，我们使用分块矩阵行列式公式。$S_{k+1}$ 的矩阵可以分块为：\n$$\nK_{S_{k+1}S_{k+1}} + \\sigma^2 I =\n\\begin{pmatrix}\nK_{S_kS_k} + \\sigma^2 I  k(X_{S_k}, x_j) \\\\\nk(x_j, X_{S_k})  k(x_j, x_j) + \\sigma^2\n\\end{pmatrix}\n$$\n使用恒等式 $\\det \\begin{pmatrix} A  B \\\\ C  D \\end{pmatrix} = \\det(A) \\det(D - CA^{-1}B)$，我们得到：\n$$\n\\det(K_{S_{k+1}S_{k+1}} + \\sigma^2 I) = \\det(K_{S_kS_k} + \\sigma^2 I) \\left( (k(x_j, x_j) + \\sigma^2) - k(x_j, X_{S_k}) (K_{S_kS_k} + \\sigma^2 I)^{-1} k(X_{S_k}, x_j) \\right)\n$$\n对两边取对数，得到：\n$$\n\\Phi_D(S_{k+1}) = \\Phi_D(S_k) + \\log \\left( (k(x_j, x_j) + \\sigma^2) - k(x_j, X_{S_k}) (K_{S_kS_k} + \\sigma^2 I)^{-1} k(X_{S_k}, x_j) \\right)\n$$\n边际增益是第二项。我们可以通过将其与后验预测方差关联来简化对数内的表达式。给定选择 $S_k$ 时，点 $x_j$ 的后验方差为：\n$$\n\\mathrm{Var}_{S_k}(x_j) = k(x_j, x_j) - k(x_j, X_{S_k}) (K_{S_kS_k} + \\sigma^2 I)^{-1} k(X_{S_k}, x_j)\n$$\n将此代入边际增益表达式，得到：\n$$\n\\Delta \\Phi_D(j | S_k) = \\log(\\mathrm{Var}_{S_k}(x_j) + \\sigma^2)\n$$\n为了最大化边际增益 $\\Delta \\Phi_D(j | S_k)$，我们必须选择使其参数 $\\mathrm{Var}_{S_k}(x_j) + \\sigma^2$ 最大化的索引 $j$。由于 $\\sigma^2$ 是一个常数，这等价于选择当前具有最大后验方差的点 $x_j$。\n\n#### 2. 方差贪心选择\n\n此标准在问题中明确定义为“迭代选择当前具有最大 $\\mathrm{Var}_S(x_j)$ 的点 $j$”。这与从 D-最优标准推导出的选择策略完全相同。\n\n因此，对于给定的问题设置（带噪声的 GP），D-最优贪心选择和方差贪心选择是等效的过程。它们将产生完全相同的选定点序列，从而得到相同的最终集 $S$。因此，对于每个测试用例，得到的积分后验方差 $\\mathcal{V}(S_D)$ 和 $\\mathcal{V}(S_A)$ 将是相同的。\n\n### 算法实现\n\n该实现将包括三个主要部分：\n1.  一个用于计算两组描述符向量之间的 RBF 核矩阵的函数。\n2.  一个实现方差最大化策略的贪心选择函数。该函数将迭代至指定的预算 $b$。在每次迭代中，它会为所有未选择的点计算后验方差，并将方差最高的点添加到选定集中。为了数值稳定性，会求解形式为 $(K_{SS} + \\sigma^2 I)z = k(X_S, x_j)$ 的线性方程组，而不是在可能的情况下显式计算矩阵的逆。\n3.  一个用于为给定选定集 $S$ 计算最终积分后验方差 $\\mathcal{V}(S) = \\sum_{i=1}^N \\mathrm{Var}_S(x_i)$ 的函数。该函数计算整个池中每个点 $x_i$ 的后验方差 $\\mathrm{Var}_S(x_i)$ 并将它们相加。为边界情况实现了特殊处理：\n    - 如果 $S$ 为空（$b=0$），则 $\\mathrm{Var}_{\\emptyset}(x_i) = k(x_i,x_i) = 1$。因此，$\\mathcal{V}(\\emptyset) = N$。\n    - 如果 $S$ 包含整个池（$b=N$），则使用一个更高效的公式：$\\mathcal{V}(X) = \\sigma^2 (N - \\sigma^2 \\mathrm{Tr}((K_{XX}+\\sigma^2 I)^{-1}))$。\n\n程序将对提供的四个测试用例中的每一个执行此逻辑，为 $\\mathcal{V}(S_D)$ 和 $\\mathcal{V}(S_A)$ 生成成对的相同值，然后将其格式化为所需的输出字符串。",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef rbf_kernel(X1, X2, length_scale):\n    \"\"\"\n    Computes the Radial Basis Function (RBF) kernel between two sets of vectors.\n\n    Args:\n        X1 (np.ndarray): A (N1, d) array of N1 vectors.\n        X2 (np.ndarray): A (N2, d) array of N2 vectors.\n        length_scale (float): The lengthscale parameter l of the kernel.\n\n    Returns:\n        np.ndarray: The (N1, N2) kernel matrix.\n    \"\"\"\n    # Using scipy.spatial.distance.cdist for a robust and efficient calculation\n    # of squared Euclidean distances.\n    sq_dists = cdist(X1, X2, 'sqeuclidean')\n    return np.exp(-sq_dists / (2 * length_scale**2))\n\ndef greedy_selector(X, b, ell, sigma):\n    \"\"\"\n    Selects a subset of indices of size b from X using a greedy strategy\n    that maximizes the posterior variance at each step.\n\n    Args:\n        X (np.ndarray): The full (N, d) data pool.\n        b (int): The selection budget.\n        ell (float): The kernel lengthscale.\n        sigma (float): The observation noise standard deviation.\n\n    Returns:\n        list: A list of b selected indices.\n    \"\"\"\n    N = X.shape[0]\n\n    if b == 0:\n        return []\n    if b >= N:\n        return list(range(N))\n\n    selected_indices = []\n    available_indices = list(range(N))\n\n    # Initial Step: All points have variance 1.0. Tie-break by picking the smallest index.\n    first_pick = available_indices[0]\n    selected_indices.append(first_pick)\n    available_indices.remove(first_pick)\n\n    for _ in range(1, b):\n        if not available_indices:\n            break\n\n        S_arr = np.array(selected_indices)\n        X_S = X[S_arr]\n\n        K_SS = rbf_kernel(X_S, X_S, ell)\n        M = K_SS + sigma**2 * np.eye(len(selected_indices))\n\n        max_var = -1.0\n        best_candidate_idx = -1\n\n        for idx in available_indices:\n            x_j = X[idx:idx + 1]\n            k_S_j = rbf_kernel(X_S, x_j, ell)\n\n            # Var = k(j,j) - k(S,j)^T * (K_SS + sigma^2*I)^-1 * k(S,j)\n            # We solve the linear system Mz = k(S,j) for z to avoid explicit inversion.\n            z = np.linalg.solve(M, k_S_j)\n            # k(j,j) = 1 for RBF kernel with unit amplitude\n            var = 1.0 - k_S_j.T @ z\n            \n            if var.item() > max_var:\n                max_var = var.item()\n                best_candidate_idx = idx\n        \n        selected_indices.append(best_candidate_idx)\n        available_indices.remove(best_candidate_idx)\n\n    return selected_indices\n\ndef calculate_integrated_variance(X, S, ell, sigma):\n    \"\"\"\n    Calculates the integrated posterior variance over the whole pool X,\n    given a selected subset of indices S.\n\n    Args:\n        X (np.ndarray): The full (N, d) data pool.\n        S (list): The list of selected indices.\n        ell (float): The kernel lengthscale.\n        sigma (float): The observation noise standard deviation.\n\n    Returns:\n        float: The integrated posterior variance V(S).\n    \"\"\"\n    N = X.shape[0]\n    \n    if not S:\n        # For an empty selection, posterior variance is prior variance at each point.\n        # Var(x_i) = k(x_i, x_i) = 1. Sum over N points gives N.\n        return float(N)\n\n    S_arr = np.array(S)\n    X_S = X[S_arr]\n\n    if len(S) == N: # Full budget case\n        K_XX = rbf_kernel(X, X, ell)\n        M = K_XX + sigma**2 * np.eye(N)\n        M_inv = np.linalg.inv(M)\n        # Efficient trace-based formula for V(X)\n        trace_M_inv = np.trace(M_inv)\n        return sigma**2 * (N - sigma**2 * trace_M_inv)\n\n    # General case for b  N\n    K_SS = rbf_kernel(X_S, X_S, ell)\n    M = K_SS + sigma**2 * np.eye(len(S))\n\n    total_variance = 0.0\n    for i in range(N):\n        x_i = X[i:i + 1]\n        k_S_i = rbf_kernel(X_S, x_i, ell)\n        z = np.linalg.solve(M, k_S_i)\n        var_i = 1.0 - k_S_i.T @ z\n        total_variance += var_i.item()\n        \n    return total_variance\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"X\": [(0,0), (0.1,-0.05), (-0.1,0.05), (0.05,0.1), (3,3), (3.1,2.9), (2.9,3.1), (3.05,3.0)],\n            \"ell\": 0.5, \"sigma\": 0.05, \"b\": 2\n        },\n        {\n            \"X\": [(0,0), (1,0), (0,1), (1,1), (0.5,0.5)],\n            \"ell\": 1.0, \"sigma\": 0.1, \"b\": 0\n        },\n        {\n            \"X\": [(0,0), (2,0), (0,2), (2,2), (1,1)],\n            \"ell\": 0.7, \"sigma\": 0.01, \"b\": 5\n        },\n        {\n            \"X\": [0, 0, 1, 1, 2, 2, 3, 3, 4, 4],\n            \"ell\": 0.3, \"sigma\": 0.05, \"b\": 5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X_data = np.array(case[\"X\"])\n        # Ensure X is a 2D array for cdist\n        if X_data.ndim == 1:\n            X_data = X_data.reshape(-1, 1)\n\n        b = case[\"b\"]\n        ell = case[\"ell\"]\n        sigma = case[\"sigma\"]\n\n        # As derived, both D-optimal and variance-greedy selections are equivalent.\n        # We run the selector once and use the result for both.\n        S_selected = greedy_selector(X_data, b, ell, sigma)\n        \n        # Calculate the integrated variance for the selected set.\n        V_S = calculate_integrated_variance(X_data, S_selected, ell, sigma)\n\n        # Since S_D = S_A, the resulting variances are identical.\n        results.append(V_S)\n        results.append(V_S)\n    \n    # Format the final output string exactly as required.\n    print(f\"[{','.join([f'{res:.6f}' for res in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "虽然仅最大化不确定性是构建势函数的有效策略，但这并非故事的全部。一个稳健的势函数还需要在多样化的数据集上进行训练，以确保其泛化能力。这个高级练习引入了一个多目标采集框架，旨在平衡降低模型不确定性（$\\Delta \\mathrm{RMSE}_F$）和增加数据多样性（$D_k$）这两个相互竞争的目标。 你将通过构建帕累托前沿来可视化这些权衡，并使用 $\\epsilon$-约束方法做出有原则的选择，这模拟了开发高质量势函数时需要考虑多重标准的真实场景。",
            "id": "3394183",
            "problem": "您的任务是为分子动力学势函数开发中的主动学习，形式化并实现一个双目标采集策略。考虑一个由定长描述符表示的未标记候选原子环境池，一个当前的已标记训练集，以及一个用于估计泛化能力的验证集。假设力的预测误差过程通过描述符空间上的一个零均值高斯过程 (GP) 进行建模，该过程使用各向同性的平方指数核函数。您的目标是：(i) 基于将单个候选点添加到训练集所带来的预测方差减少，计算一个力的均方根误差 (RMSE) 下降目标；(ii) 基于描述符空间中的核化距离，计算一个多样性目标；(iii) 在这两个目标下，推导出候选集上的帕累托前沿；以及 (iv) 通过 epsilon 约束方法进行选择。\n\n基本原理和定义：\n- 令平方指数核函数定义为\n$$\nk(\\mathbf{x},\\mathbf{y}) \\;=\\; \\sigma_k^2 \\exp\\!\\left(-\\dfrac{\\|\\mathbf{x}-\\mathbf{y}\\|_2^2}{2\\ell^2}\\right),\n$$\n其中 $\\sigma_k^2 \\!\\! 0$ 是信号方差，$\\ell \\!\\! 0$ 是长度尺度。令 $\\sigma_n^2 \\!\\ge\\! 0$ 表示观测噪声方差。\n- 给定一个训练描述符矩阵 $X_T \\in \\mathbb{R}^{n_T \\times d}$，在添加新点之前，一个验证点 $\\mathbf{z} \\in \\mathbb{R}^d$ 处的后验预测方差为\n$$\nv_{\\mathrm{old}}(\\mathbf{z}) \\;=\\; k(\\mathbf{z},\\mathbf{z}) \\;-\\; \\mathbf{k}_{T\\mathbf{z}}^\\top \\left(K_{TT} + \\sigma_n^2 I\\right)^{-1} \\mathbf{k}_{T\\mathbf{z}},\n$$\n其中 $K_{TT} \\in \\mathbb{R}^{n_T \\times n_T}$ 的元素为 $[K_{TT}]_{ij} = k(\\mathbf{x}_i,\\mathbf{x}_j)$（对于 $\\mathbf{x}_i,\\mathbf{x}_j \\in X_T$），$\\mathbf{k}_{T\\mathbf{z}} \\in \\mathbb{R}^{n_T}$ 的元素为 $[\\mathbf{k}_{T\\mathbf{z}}]_i = k(\\mathbf{x}_i,\\mathbf{z})$。\n- 对于一个候选点 $\\mathbf{x}$，定义 $\\mathbf{z}$ 和 $\\mathbf{x}$ 之间的当前后验协方差为\n$$\nc_{\\mathrm{old}}(\\mathbf{z},\\mathbf{x}) \\;=\\; k(\\mathbf{z},\\mathbf{x}) \\;-\\; \\mathbf{k}_{T\\mathbf{z}}^\\top \\left(K_{TT} + \\sigma_n^2 I\\right)^{-1} \\mathbf{k}_{T\\mathbf{x}} ,\n$$\n其中 $\\mathbf{k}_{T\\mathbf{x}} \\in \\mathbb{R}^{n_T}$，$[\\mathbf{k}_{T\\mathbf{x}}]_i = k(\\mathbf{x}_i,\\mathbf{x})$。\n- 在假设性地于 $\\mathbf{x}$ 处添加一个带噪声的观测后，$\\mathbf{z}$ 处的后验方差通过一个秩-1 修正（依据高斯条件恒等式和 Sherman–Morrison–Woodbury 公式）更新为\n$$\nv_{\\mathrm{new}}(\\mathbf{z}\\,;\\mathbf{x}) \\;=\\; v_{\\mathrm{old}}(\\mathbf{z}) \\;-\\; \\dfrac{c_{\\mathrm{old}}(\\mathbf{z},\\mathbf{x})^2}{v_{\\mathrm{old}}(\\mathbf{x}) + \\sigma_n^2} .\n$$\n- 令一个有限验证集为 $X_V = \\{\\mathbf{z}_m\\}_{m=1}^{n_V}$。定义候选点 $\\mathbf{x}$ 的力 RMSE 下降目标为\n$$\n\\Delta \\mathrm{RMSE}_F(\\mathbf{x}) \\;=\\; \\sqrt{ \\dfrac{1}{n_V} \\sum_{m=1}^{n_V} v_{\\mathrm{old}}(\\mathbf{z}_m)} \\;-\\; \\sqrt{ \\dfrac{1}{n_V} \\sum_{m=1}^{n_V} v_{\\mathrm{new}}(\\mathbf{z}_m\\,;\\mathbf{x}) } .\n$$\n- 使用核诱导的到经验训练分布的距离来定义多样性目标（即单元素集 $\\{\\mathbf{x}\\}$ 和 $X_T$ 上的经验测度之间的平方最大均值差异）：\n$$\nD_k(\\mathbf{x}) \\;=\\; \\sqrt{\\, k(\\mathbf{x},\\mathbf{x}) \\;-\\; \\dfrac{2}{n_T} \\sum_{i=1}^{n_T} k(\\mathbf{x},\\mathbf{x}_i) \\;+\\; \\dfrac{1}{n_T^2} \\sum_{i=1}^{n_T} \\sum_{j=1}^{n_T} k(\\mathbf{x}_i,\\mathbf{x}_j) \\,} .\n$$\n- 对于两个候选点 $\\mathbf{x}$ 和 $\\mathbf{y}$，如果 $\\Delta \\mathrm{RMSE}_F(\\mathbf{x}) \\ge \\Delta \\mathrm{RMSE}_F(\\mathbf{y})$ 且 $D_k(\\mathbf{x}) \\ge D_k(\\mathbf{y})$，并且至少有一个不等式是严格的，则称 $\\mathbf{x}$ 帕累托支配 $\\mathbf{y}$。帕累托前沿是所有非被支配候选点的集合。\n- 给定一个关于多样性的 $\\epsilon$ 约束 $\\epsilon \\ge 0$，通过在满足 $D_k(\\mathbf{x}) \\ge \\epsilon$ 的条件下最大化 $\\Delta \\mathrm{RMSE}_F(\\mathbf{x})$ 来选择一个候选点。如果出现平局，则优先选择具有更大 $D_k(\\mathbf{x})$ 的候选点；如果仍然平局，则选择最小的从零开始的索引。如果没有候选点满足该约束，则返回整数 $-1$。\n\n您的程序必须：\n- 为每个测试用例实现上述目标函数。\n- 计算帕累托前沿的索引，并为每个指定的 $\\epsilon$ 执行 $\\epsilon$ 约束选择。\n- 对候选点使用从零开始的索引。\n- 生成单行输出，按顺序包含每个测试用例的结果，每个结果是一个包含两个元素的列表：第一个是按升序排序的帕累托前沿索引列表；第二个是针对该测试用例中给定的每个 $\\epsilon$ 值，按顺序排列的所选索引列表。将所有测试用例的列表聚合到一个外部列表中，并以类似 Python 列表的格式打印，不含空格，例如，`[[[a,b],[c,d]],[[e],[f,g]]]` 但不含空格。\n\n使用以下测试套件。\n\n测试用例 1（一维描述符）：\n- 核参数：$\\sigma_k^2 = 1.0$，$\\ell = 0.5$，$\\sigma_n^2 = 10^{-6}$。\n- 训练描述符 $X_T \\in \\mathbb{R}^{2 \\times 1}$：$\\big[[-1.0],[1.0]\\big]$。\n- 候选池 $X_P \\in \\mathbb{R}^{5 \\times 1}$：$\\big[[-0.8],[-0.2],[0.0],[0.5],[1.2]\\big]$。\n- 验证集 $X_V \\in \\mathbb{R}^{31 \\times 1}$：从 $-1.5$ 到 $1.5$（含）的 $31$ 个等间距点。\n- $\\epsilon$ 值（多样性阈值）：$[0.02, 0.06, 0.12]$。\n\n测试用例 2（二维描述符）：\n- 核参数：$\\sigma_k^2 = 0.8$，$\\ell = 0.7$，$\\sigma_n^2 = 0.01$。\n- 训练描述符 $X_T \\in \\mathbb{R}^{3 \\times 2}$：$\\big[[0.0,0.0],[1.0,0.0],[0.0,1.0]\\big]$。\n- 候选池 $X_P \\in \\mathbb{R}^{6 \\times 2}$：$\\big[[0.5,0.5],[1.0,1.0],[-0.5,0.5],[0.2,-0.8],[0.8,0.2],[1.5,-0.5]\\big]$。\n- 验证集 $X_V \\in \\mathbb{R}^{25 \\times 2}$：每个坐标取值为 $\\{-0.5, 0.0, 0.5, 1.0, 1.5\\}$ 形成的 $5 \\times 5$ 笛卡尔网格。\n- $\\epsilon$ 值：$[0.05, 0.12, 0.20]$。\n\n测试用例 3（可行性边界情况）：\n- 核参数：$\\sigma_k^2 = 0.5$，$\\ell = 2.0$，$\\sigma_n^2 = 10^{-6}$。\n- 训练描述符 $X_T \\in \\mathbb{R}^{1 \\times 1}$：$\\big[[0.0]\\big]$。\n- 候选池 $X_P \\in \\mathbb{R}^{3 \\times 1}$：$\\big[[0.0],[0.1],[0.2]\\big]$。\n- 验证集 $X_V \\in \\mathbb{R}^{11 \\times 1}$：从 $0.0$ 到 $0.2$（含）的 $11$ 个等间距点。\n- $\\epsilon$ 值：$[0.20, 0.40]$。\n\n最终输出格式要求：\n- 您的程序应生成单行输出，其中包含一个类似 Python 列表的结果，不带空格。对于每个测试用例，输出一个包含两个元素的列表：\n  $[ \\text{pareto\\_indices\\_sorted}, \\text{selected\\_indices\\_per\\_epsilon} ]$，\n  其中两个元素都是整数列表。外部列表按顺序聚合所有测试用例的这些列表。例如，格式为\n  $[[[i_1,i_2],[j_1,j_2,j_3]],[[i_3,i_4,i_5],[j_4,j_5]]]$，\n  其中每个 $i_\\cdot$ 和 $j_\\cdot$ 都是整数。",
            "solution": "所提出的问题要求为分子动力学势函数开发并实现一种双目标主动学习选择策略。这是一个明确定义的计算问题，其根源在于高斯过程 (GP) 回归和多目标优化理论。解决方案将通过首先实现两个目标——力 RMSE 下降和多样性——然后应用标准技术进行帕累托前沿识别和基于 $\\epsilon$ 约束的选择来得出。\n\n我们的算法方法对每个测试用例包括以下步骤：\n1.  **预处理**：一次性计算那些不依赖于特定候选点的基本矩阵和常数，以优化计算。其核心是训练集正则化核矩阵的逆，即 $M = (K_{TT} + \\sigma_n^2 I)^{-1}$，其中 $[K_{TT}]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$，适用于所有训练描述符对 $\\mathbf{x}_i, \\mathbf{x}_j \\in X_T$。\n\n2.  **目标函数计算**：对于来自候选池 $X_P$ 的每个候选描述符 $\\mathbf{x}$，我们计算两个我们旨在最大化的目标值。\n\n    a. **力 RMSE 下降, $\\Delta \\mathrm{RMSE}_F(\\mathbf{x})$**：该目标估计了将 $\\mathbf{x}$ 添加到训练集后，模型在验证集 $X_V$ 上的泛化误差的减少量。它被定义为假设性更新前后预测方差均方根的差值：\n    $$\n    \\Delta \\mathrm{RMSE}_F(\\mathbf{x}) = \\sqrt{ \\frac{1}{n_V} \\sum_{m=1}^{n_V} v_{\\mathrm{old}}(\\mathbf{z}_m)} - \\sqrt{ \\frac{1}{n_V} \\sum_{m=1}^{n_V} v_{\\mathrm{new}}(\\mathbf{z}_m; \\mathbf{x}) }\n    $$\n    项 $\\mathrm{RMSE}_{\\mathrm{old}} = \\sqrt{\\frac{1}{n_V} \\sum_{m=1}^{n_V} v_{\\mathrm{old}}(\\mathbf{z}_m)}$ 只计算一次。对于每个候选点 $\\mathbf{x}$，新的方差 $v_{\\mathrm{new}}(\\mathbf{z}_m; \\mathbf{x})$ 使用高效的秩-1 更新公式确定：\n    $$\n    v_{\\mathrm{new}}(\\mathbf{z}_m; \\mathbf{x}) = v_{\\mathrm{old}}(\\mathbf{z}_m) - \\frac{c_{\\mathrm{old}}(\\mathbf{z}_m, \\mathbf{x})^2}{v_{\\mathrm{old}}(\\mathbf{x}) + \\sigma_n^2}\n    $$\n    其中 $v_{\\mathrm{old}}$ 和 $c_{\\mathrm{old}}$ 分别表示基于当前训练集 $X_T$ 的 GP 后验方差和协方差。这些量是使用预先计算的矩阵 $M$ 来计算的。\n\n    b. **多样性, $D_k(\\mathbf{x})$**：此目标量化了候选点 $\\mathbf{x}$ 在核诱导的特征空间中与现有训练点的不同程度。它被定义为在 $\\mathbf{x}$ 处的单点测度与训练集 $X_T$ 的经验测度之间的最大均值差异 (MMD) 的平方根：\n    $$\n    D_k(\\mathbf{x}) = \\sqrt{k(\\mathbf{x},\\mathbf{x}) - \\frac{2}{n_T} \\sum_{i=1}^{n_T} k(\\mathbf{x},\\mathbf{x}_i) + \\frac{1}{n_T^2} \\sum_{i,j=1}^{n_T} k(\\mathbf{x}_i,\\mathbf{x}_j)}\n    $$\n    项 $k(\\mathbf{x},\\mathbf{x})$ 等于核方差 $\\sigma_k^2$。双重求和项对所有候选点都是常数，代表训练核矩阵 $K_{TT}$ 的均值。\n\n3.  **帕累托前沿识别**：在为每个候选点计算了目标向量 $(\\Delta \\mathrm{RMSE}_F(\\mathbf{x}), D_k(\\mathbf{x}))$ 之后，我们识别出帕累托前沿。如果一个候选点没有被任何其他候选点帕累托支配，那么它就在帕累托前沿上。如果 $\\Delta \\mathrm{RMSE}_F(\\mathbf{x}_a) \\ge \\Delta \\mathrm{RMSE}_F(\\mathbf{x}_b)$ 且 $D_k(\\mathbf{x}_a) \\ge D_k(\\mathbf{x}_b)$，并且至少有一个不等式是严格的，则候选点 $\\mathbf{x}_a$ 支配 $\\mathbf{x}_b$。收集所有非被支配候选点的索引并按升序排序。\n\n4.  **$\\epsilon$ 约束选择**：对于每个给定的多样性阈值 $\\epsilon$，选择一个候选点。首先，我们构建一个可行候选集，其中 $D_k(\\mathbf{x}) \\ge \\epsilon$。如果该集合为空，则无法选择，结果为 $-1$。否则，从此可行集中，我们选择使 $\\Delta \\mathrm{RMSE}_F(\\mathbf{x})$ 最大化的候选点。平局通过选择具有更高 $D_k(\\mathbf{x})$ 的候选点来打破。如果仍然存在平局，则选择具有最小从零开始索引的候选点。\n\n对每个测试用例实施这一结构化程序，以产生所需的结果。",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        {\n            \"kernel_params\": {\"sigma_k_sq\": 1.0, \"l\": 0.5, \"sigma_n_sq\": 1e-6},\n            \"X_T\": np.array([[-1.0], [1.0]]),\n            \"X_P\": np.array([[-0.8], [-0.2], [0.0], [0.5], [1.2]]),\n            \"X_V\": np.linspace(-1.5, 1.5, 31).reshape(-1, 1),\n            \"epsilons\": [0.02, 0.06, 0.12],\n        },\n        {\n            \"kernel_params\": {\"sigma_k_sq\": 0.8, \"l\": 0.7, \"sigma_n_sq\": 0.01},\n            \"X_T\": np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0]]),\n            \"X_P\": np.array([[0.5, 0.5], [1.0, 1.0], [-0.5, 0.5], [0.2, -0.8], [0.8, 0.2], [1.5, -0.5]]),\n            \"X_V\": np.array(np.meshgrid([-0.5, 0.0, 0.5, 1.0, 1.5], [-0.5, 0.0, 0.5, 1.0, 1.5])).T.reshape(-1, 2),\n            \"epsilons\": [0.05, 0.12, 0.20],\n        },\n        {\n            \"kernel_params\": {\"sigma_k_sq\": 0.5, \"l\": 2.0, \"sigma_n_sq\": 1e-6},\n            \"X_T\": np.array([[0.0]]),\n            \"X_P\": np.array([[0.0], [0.1], [0.2]]),\n            \"X_V\": np.linspace(0.0, 0.2, 11).reshape(-1, 1),\n            \"epsilons\": [0.20, 0.40],\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(\n            case[\"kernel_params\"],\n            case[\"X_T\"],\n            case[\"X_P\"],\n            case[\"X_V\"],\n            case[\"epsilons\"]\n        )\n        all_results.append(result)\n\n    # Format output string without spaces as per requirement\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\ndef kernel(X1, X2, sigma_k_sq, l_sq):\n    \"\"\"\n    Computes the squared-exponential kernel matrix between two sets of vectors.\n    \"\"\"\n    sq_dists = cdist(X1, X2, 'sqeuclidean')\n    return sigma_k_sq * np.exp(-sq_dists / (2 * l_sq))\n\ndef process_case(kernel_params, X_T, X_P, X_V, epsilons):\n    \"\"\"\n    Processes a single test case to find the Pareto front and epsilon-constraint selections.\n    \"\"\"\n    sigma_k_sq = kernel_params[\"sigma_k_sq\"]\n    l = kernel_params[\"l\"]\n    sigma_n_sq = kernel_params[\"sigma_n_sq\"]\n    l_sq = l**2\n    \n    n_T = X_T.shape[0]\n    n_P = X_P.shape[0]\n    n_V = X_V.shape[0]\n\n    # Pre-computation\n    K_TT = kernel(X_T, X_T, sigma_k_sq, l_sq)\n    K_TT_inv_reg = np.linalg.inv(K_TT + sigma_n_sq * np.eye(n_T))\n    \n    # --- Objective 1: Delta RMSE ---\n    # Calculate old RMSE (constant for all candidates)\n    K_TV = kernel(X_T, X_V, sigma_k_sq, l_sq)\n    v_old_at_V = sigma_k_sq - np.sum((K_TT_inv_reg @ K_TV) * K_TV, axis=0)\n    rmse_old = np.sqrt(np.mean(v_old_at_V))\n\n    delta_rmses = []\n    for i in range(n_P):\n        x_p = X_P[i:i+1] # Keep it 2D\n        k_Tx = kernel(X_T, x_p, sigma_k_sq, l_sq)\n        k_Vx = kernel(X_V, x_p, sigma_k_sq, l_sq)\n        \n        v_old_at_x = sigma_k_sq - k_Tx.T @ K_TT_inv_reg @ k_Tx\n        \n        c_old_at_V_for_x = k_Vx - (K_TV.T @ K_TT_inv_reg @ k_Tx)\n        \n        var_update_term = (c_old_at_V_for_x**2) / (v_old_at_x.item() + sigma_n_sq)\n        v_new_at_V = v_old_at_V.reshape(-1, 1) - var_update_term\n        \n        rmse_new_for_x = np.sqrt(np.mean(v_new_at_V))\n        delta_rmses.append(rmse_old - rmse_new_for_x)\n\n    # --- Objective 2: Diversity ---\n    d_ks = []\n    const_term_d = np.mean(K_TT)\n    for i in range(n_P):\n        x_p = X_P[i:i+1]\n        k_pT = kernel(x_p, X_T, sigma_k_sq, l_sq)\n        term2_d = 2.0 * np.mean(k_pT)\n        d_k_sq = sigma_k_sq - term2_d + const_term_d\n        d_ks.append(np.sqrt(np.maximum(0, d_k_sq)))\n\n    objectives = list(zip(delta_rmses, d_ks))\n\n    # --- Pareto Front Identification ---\n    pareto_indices = []\n    for i in range(n_P):\n        is_dominated = False\n        for j in range(n_P):\n            if i == j:\n                continue\n            # Check if j dominates i\n            if (objectives[j][0] >= objectives[i][0] and objectives[j][1] >= objectives[i][1]) and \\\n               (objectives[j][0] > objectives[i][0] or objectives[j][1] > objectives[i][1]):\n                is_dominated = True\n                break\n        if not is_dominated:\n            pareto_indices.append(i)\n    pareto_indices.sort()\n\n    # --- Epsilon-Constraint Selection ---\n    selected_indices = []\n    for eps in epsilons:\n        feasible_candidates = []\n        for i in range(n_P):\n            if d_ks[i] >= eps:\n                feasible_candidates.append({'delta': delta_rmses[i], 'd': d_ks[i], 'idx': i})\n\n        if not feasible_candidates:\n            selected_indices.append(-1)\n        else:\n            # Sort by delta (desc), d (desc), then index (asc)\n            feasible_candidates.sort(key=lambda c: (c['delta'], c['d'], -c['idx']), reverse=True)\n            selected_indices.append(feasible_candidates[0]['idx'])\n            \n    return [pareto_indices, selected_indices]\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}