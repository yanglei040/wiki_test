{
    "hands_on_practices": [
        {
            "introduction": "A molecular dynamics trajectory is not a sequence of independent snapshots; the state of the system at one time step is highly correlated with the state at the next. This temporal correlation means that simply counting the number of recorded frames, $N$, overestimates the amount of truly independent information we have gathered. This exercise provides a hands-on path to understanding this critical concept by deriving the effective sample size, $N_{\\mathrm{eff}}$, a metric that quantifies the number of independent samples an autocorrelated time series is equivalent to. Mastering this concept  is the first step toward robustly estimating statistical errors and planning simulation lengths.",
            "id": "3405213",
            "problem": "A Molecular Dynamics (MD) equilibration run generates a correlated time series of an observable (for example, the potential energy) sampled at uniform interval $\\Delta t$. Let $X_{i}$ denote the sample at time $t_{i} = i \\Delta t$, and assume the process is stationary after discarding an initial transient. Define the normalized autocorrelation function $\\rho(t)$ and the Integrated Autocorrelation Time (IAT) $\\tau_{\\mathrm{int}}$ via the standard time-domain definition that uses the sum (or integral) of $\\rho(t)$ over nonnegative lags. Starting from the definition of the sample mean $\\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_{i}$ and the variance of $\\bar{X}$ for correlated data expressed in terms of the autocovariance function, derive an expression for the effective sample size $N_{\\mathrm{eff}}$ in terms of $N$, $\\Delta t$, and $\\tau_{\\mathrm{int}}$ by equating $\\mathrm{Var}(\\bar{X})$ of the correlated series to $\\sigma^{2}/N_{\\mathrm{eff}}$, where $\\sigma^{2}$ is the variance of the underlying stationary process. Then, consider an MD run where the normalized autocorrelation function is well described by a single-exponential model $\\rho(t) = \\exp\\!\\left(-t/\\tau_{c}\\right)$, for which the IAT equals the correlation time, $\\tau_{\\mathrm{int}} = \\tau_{c}$. Suppose the sampling interval is $\\Delta t = 2\\,\\mathrm{fs}$, the current production segment has length $T_{\\mathrm{cur}} = 3.000\\,\\mathrm{ns}$, and a reliable analysis of the equilibrated portion yields $\\tau_{c} = 18.7\\,\\mathrm{ps}$. You wish to monitor convergence during equilibration by using a stopping rule that requires achieving a target effective sample size $N_{\\mathrm{eff}}^{\\star} = 200$. Based on your derived expression and the exponential model, compute the minimal additional production time $\\Delta T_{\\mathrm{add}}$ needed beyond $T_{\\mathrm{cur}}$ to reach $N_{\\mathrm{eff}}^{\\star}$. Round your final numerical answer to four significant figures and express it in nanoseconds (ns). In your derivation, start from the fundamental definitions of autocorrelation and the variance of the sample mean for correlated observations; do not assume any shortcut formulas.",
            "solution": "The problem asks for two main tasks: first, to derive an expression for the effective sample size $N_{\\mathrm{eff}}$ from fundamental principles, and second, to apply this expression to calculate the additional simulation time required to reach a target effective sample size.\n\n**Part 1: Derivation of the Effective Sample Size, $N_{\\mathrm{eff}}$**\n\nLet $X_i$ be the value of a stationary observable sampled at time $t_i = i \\Delta t$ for $i=1, 2, \\dots, N$. The sample mean, $\\bar{X}$, is defined as:\n$$\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_{i}\n$$\nThe variance of the sample mean is given by:\n$$\n\\mathrm{Var}(\\bar{X}) = \\mathrm{Var}\\left(\\frac{1}{N} \\sum_{i=1}^{N} X_{i}\\right) = \\frac{1}{N^2} \\mathrm{Var}\\left(\\sum_{i=1}^{N} X_{i}\\right)\n$$\nUsing the property of variance for a sum of random variables, we have:\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{N} X_{i}\\right) = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\mathrm{Cov}(X_i, X_j)\n$$\nFor a stationary process, the covariance $\\mathrm{Cov}(X_i, X_j)$ depends only on the time lag $|t_i - t_j| = |i-j|\\Delta t$. We define the autocovariance function at a discrete lag of $k$ steps as $\\gamma_k = \\mathrm{Cov}(X_i, X_{i+k})$. The variance of the process is $\\sigma^2 = \\mathrm{Var}(X_i) = \\gamma_0$. Due to stationarity, $\\gamma_k = \\gamma_{-k}$. We can rewrite the double summation by grouping terms with the same lag $k = |i-j|$:\n$$\n\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\mathrm{Cov}(X_i, X_j) = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\gamma_{|i-j|} = \\sum_{k=-(N-1)}^{N-1} (N-|k|) \\gamma_k\n$$\nThis can be split into terms for $k=0$, $k>0$, and $k<0$:\n$$\n\\sum_{k=-(N-1)}^{N-1} (N-|k|) \\gamma_k = (N-0)\\gamma_0 + \\sum_{k=1}^{N-1} (N-k)\\gamma_k + \\sum_{k=-(N-1)}^{-1} (N-|k|)\\gamma_k\n$$\nUsing $\\gamma_k = \\gamma_{-k}$, the expression becomes:\n$$\nN\\gamma_0 + 2\\sum_{k=1}^{N-1} (N-k)\\gamma_k\n$$\nSubstituting this back into the expression for $\\mathrm{Var}(\\bar{X})$:\n$$\n\\mathrm{Var}(\\bar{X}) = \\frac{1}{N^2} \\left[ N\\gamma_0 + 2\\sum_{k=1}^{N-1} (N-k)\\gamma_k \\right] = \\frac{\\gamma_0}{N} \\left[ 1 + \\frac{2}{N}\\sum_{k=1}^{N-1} (N-k)\\frac{\\gamma_k}{\\gamma_0} \\right]\n$$\nThe normalized autocorrelation function (ACF) at lag $k$ is $\\rho_k = \\gamma_k / \\gamma_0$. Substituting this and $\\sigma^2=\\gamma_0$:\n$$\n\\mathrm{Var}(\\bar{X}) = \\frac{\\sigma^2}{N} \\left[ 1 + 2\\sum_{k=1}^{N-1} \\left(1-\\frac{k}{N}\\right)\\rho_k \\right]\n$$\nThis expression is exact. For a typical MD simulation where the total time is much longer than the correlation time, the ACF $\\rho_k$ decays to zero for $k \\ll N$. In this limit, we can make two approximations:\n1.  For values of $k$ where $\\rho_k$ is non-negligible, $k/N \\ll 1$, so $(1-k/N) \\approx 1$.\n2.  The upper limit of the summation can be extended from $N-1$ to $\\infty$ since $\\rho_k \\approx 0$ for large $k$.\n\nWith these approximations, the variance becomes:\n$$\n\\mathrm{Var}(\\bar{X}) \\approx \\frac{\\sigma^2}{N} \\left( 1 + 2\\sum_{k=1}^{\\infty} \\rho_k \\right)\n$$\nThe problem defines the Integrated Autocorrelation Time (IAT), $\\tau_{\\mathrm{int}}$, using the continuous-time ACF, $\\rho(t)$:\n$$\n\\tau_{\\mathrm{int}} = \\int_0^\\infty \\rho(t) dt\n$$\nWe can relate the discrete sum $\\sum \\rho_k$ to this integral. The integral can be approximated by a sum using the trapezoidal rule with a step size of $\\Delta t$:\n$$\n\\tau_{\\mathrm{int}} = \\int_0^\\infty \\rho(t) dt \\approx \\Delta t \\left[ \\frac{1}{2}\\rho(0) + \\sum_{k=1}^{\\infty} \\rho(k \\Delta t) \\right]\n$$\nSince $\\rho(0)=1$ by definition and $\\rho_k = \\rho(k \\Delta t)$, we get:\n$$\n\\tau_{\\mathrm{int}} \\approx \\Delta t \\left( \\frac{1}{2} + \\sum_{k=1}^{\\infty} \\rho_k \\right)\n$$\nSolving for the summation term:\n$$\n\\sum_{k=1}^{\\infty} \\rho_k \\approx \\frac{\\tau_{\\mathrm{int}}}{\\Delta t} - \\frac{1}{2}\n$$\nNow, substitute this back into our approximate expression for $\\mathrm{Var}(\\bar{X})$:\n$$\n\\mathrm{Var}(\\bar{X}) \\approx \\frac{\\sigma^2}{N} \\left( 1 + 2 \\left( \\frac{\\tau_{\\mathrm{int}}}{\\Delta t} - \\frac{1}{2} \\right) \\right) = \\frac{\\sigma^2}{N} \\left( 1 + \\frac{2\\tau_{\\mathrm{int}}}{\\Delta t} - 1 \\right) = \\frac{\\sigma^2}{N} \\left( \\frac{2\\tau_{\\mathrm{int}}}{\\Delta t} \\right)\n$$\nThe problem defines the effective sample size, $N_{\\mathrm{eff}}$, by equating the variance of the correlated mean to the variance of a mean of $N_{\\mathrm{eff}}$ independent samples:\n$$\n\\mathrm{Var}(\\bar{X}) = \\frac{\\sigma^2}{N_{\\mathrm{eff}}}\n$$\nBy comparing the two expressions for $\\mathrm{Var}(\\bar{X})$:\n$$\n\\frac{\\sigma^2}{N_{\\mathrm{eff}}} = \\frac{\\sigma^2}{N} \\frac{2\\tau_{\\mathrm{int}}}{\\Delta t}\n$$\nSolving for $N_{\\mathrm{eff}}$ gives:\n$$\nN_{\\mathrm{eff}} = \\frac{N \\Delta t}{2\\tau_{\\mathrm{int}}}\n$$\nSince $T = N \\Delta t$ is the total simulation time, the final expression is:\n$$\nN_{\\mathrm{eff}} = \\frac{T}{2\\tau_{\\mathrm{int}}}\n$$\nThis is the desired expression for the effective sample size in terms of $N$, $\\Delta t$, and $\\tau_{\\mathrm{int}}$ (or $T$ and $\\tau_{\\mathrm{int}}$).\n\n**Part 2: Calculation of the Additional Production Time, $\\Delta T_{\\mathrm{add}}$**\n\nWe are given a single-exponential model for the ACF, $\\rho(t) = \\exp(-t/\\tau_c)$, for which the IAT is equal to the correlation time, $\\tau_{\\mathrm{int}} = \\tau_c$. Our derived formula for $N_{\\mathrm{eff}}$ becomes:\n$$\nN_{\\mathrm{eff}} = \\frac{T}{2\\tau_c}\n$$\nThe goal is to reach a target effective sample size of $N_{\\mathrm{eff}}^{\\star} = 200$. Let $T_{\\mathrm{total}}$ be the total simulation time required to achieve this.\n$$\nN_{\\mathrm{eff}}^{\\star} = \\frac{T_{\\mathrm{total}}}{2\\tau_c}\n$$\nWe can solve for $T_{\\mathrm{total}}$:\n$$\nT_{\\mathrm{total}} = 2 N_{\\mathrm{eff}}^{\\star} \\tau_c\n$$\nThe given values are:\n-   $N_{\\mathrm{eff}}^{\\star} = 200$\n-   $\\tau_c = 18.7\\,\\mathrm{ps}$\n\nSubstituting these values:\n$$\nT_{\\mathrm{total}} = 2 \\times 200 \\times 18.7\\,\\mathrm{ps} = 400 \\times 18.7\\,\\mathrm{ps} = 7480\\,\\mathrm{ps}\n$$\nTo express this in nanoseconds, we use the conversion $1\\,\\mathrm{ns} = 1000\\,\\mathrm{ps}$:\n$$\nT_{\\mathrm{total}} = 7480\\,\\mathrm{ps} \\times \\frac{1\\,\\mathrm{ns}}{1000\\,\\mathrm{ps}} = 7.480\\,\\mathrm{ns}\n$$\nThe current production segment has a length of $T_{\\mathrm{cur}} = 3.000\\,\\mathrm{ns}$. The minimal additional production time, $\\Delta T_{\\mathrm{add}}$, is the difference between the required total time and the current time:\n$$\n\\Delta T_{\\mathrm{add}} = T_{\\mathrm{total}} - T_{\\mathrm{cur}}\n$$\n$$\n\\Delta T_{\\mathrm{add}} = 7.480\\,\\mathrm{ns} - 3.000\\,\\mathrm{ns} = 4.480\\,\\mathrm{ns}\n$$\nThe problem requires the final answer to be rounded to four significant figures. Our result, $4.480\\,\\mathrm{ns}$, is already in this form. The sampling interval $\\Delta t = 2\\,\\mathrm{fs}$ was necessary for the theoretical derivation but not for the final numerical calculation once the formula $N_{\\mathrm{eff}} = T/(2\\tau_{\\mathrm{int}})$ was established.",
            "answer": "$$\n\\boxed{4.480}\n$$"
        },
        {
            "introduction": "While analyzing a single trajectory is essential, it carries the risk of being misled by apparent convergence if the simulation becomes trapped in a metastable state. A more rigorous approach is to run multiple, independent simulations and verify that they all converge to the same statistical distribution. This practice introduces the Potential Scale Reduction Factor ($\\hat{R}$), a powerful diagnostic that formalizes this comparison by analyzing the variance within and between chains. By calculating both the univariate and multivariate $\\hat{R}$ values , you will learn one of the gold-standard methods for declaring convergence in complex systems.",
            "id": "3405203",
            "problem": "A molecular dynamics equilibration is monitored using $M$ independent replicate trajectories (started from the same configuration with independently randomized velocities) to assess convergence of multiple observables. Suppose two observables are recorded every fixed time interval over the last segment of each trajectory of length $n$ samples, producing $M=3$ chains with $n=20$ samples per chain for each observable. Denote by $\\boldsymbol{\\mu}_m \\in \\mathbb{R}^2$ the sample mean vector of chain $m$ for the two observables and by $\\mathbf{S}_m \\in \\mathbb{R}^{2 \\times 2}$ the unbiased within-chain sample covariance matrix of chain $m$. The three replicate chains yield the following summaries over the final analysis window:\n- Chain means:\n$$\n\\boldsymbol{\\mu}_1=\\begin{pmatrix}0.50 \\\\ 1.00\\end{pmatrix},\\quad\n\\boldsymbol{\\mu}_2=\\begin{pmatrix}0.62 \\\\ 0.96\\end{pmatrix},\\quad\n\\boldsymbol{\\mu}_3=\\begin{pmatrix}0.54 \\\\ 1.06\\end{pmatrix}.\n$$\n- Within-chain covariance matrices:\n$$\n\\mathbf{S}_1=\\begin{pmatrix}0.050 & 0.012 \\\\ 0.012 & 0.042\\end{pmatrix},\\quad\n\\mathbf{S}_2=\\begin{pmatrix}0.048 & 0.009 \\\\ 0.009 & 0.039\\end{pmatrix},\\quad\n\\mathbf{S}_3=\\begin{pmatrix}0.052 & 0.011 \\\\ 0.011 & 0.041\\end{pmatrix}.\n$$\nAssume all chains are targeting the same stationary distribution of the observables in the equilibrated regime, and that the replicates are mutually independent. Starting from first principles of variance decomposition across and within independent replicates and the law of large numbers, derive expressions for:\n1) The univariate Potential Scale Reduction Factor (PSRF, $\\hat{R}$) for each observable separately, using the ratio of a pooled estimator of the marginal variance to the average within-chain variance.\n2) The multivariate PSRF (denoted $\\hat{R}_{\\mathrm{multi}}$) for the two-dimensional observable, defined via the largest generalized eigenvalue of the pooled variance estimator relative to the pooled within-chain covariance.\n\nThen compute the numerical values of the univariate and multivariate $\\hat{R}$ for the given data. Based on asymptotic consistency arguments, specify principled stopping thresholds and trend requirements over time (e.g., across successive, non-overlapping windows of the last $n$ samples) that would justify declaring convergence during equilibration monitoring in molecular dynamics. Your threshold specifications must be dimensionless and justified from the variance ratio interpretation.\n\nFinally, report the multivariate $\\hat{R}_{\\mathrm{multi}}$ computed for the data above as your answer. Express the final value as a dimensionless number and round your answer to four significant figures.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the statistical analysis of molecular simulations, is well-posed with sufficient and consistent data, and is expressed in objective, formal language. We proceed with the solution.\n\nThe problem requires the derivation and computation of the univariate and multivariate Potential Scale Reduction Factor (PSRF), denoted $\\hat{R}$, as a diagnostic for convergence in molecular dynamics simulations.\n\nLet $M$ be the number of independent replicate trajectories and $n$ be the number of samples in the analysis window for each trajectory. We are given $M=3$ and $n=20$. For each trajectory $m \\in \\{1, \\dots, M\\}$, we have the sample mean vector $\\boldsymbol{\\mu}_m$ and the sample covariance matrix $\\mathbf{S}_m$ for two observables.\n\n**1. Univariate Potential Scale Reduction Factor ($\\hat{R}$)**\n\nLet us consider a single observable, indexed by $k \\in \\{1, 2\\}$. The corresponding component of the mean vector $\\boldsymbol{\\mu}_m$ is $\\mu_{m,k}$, and the corresponding diagonal element of the covariance matrix $\\mathbf{S}_m$ is $S_{m,kk}$, which is the unbiased sample variance of the observable in chain $m$.\n\nThe analysis is based on the decomposition of the total variance into within-chain and between-chain components.\n\nThe average within-chain variance, $W_k$, is the mean of the individual chain variances:\n$$W_k = \\frac{1}{M} \\sum_{m=1}^{M} S_{m,kk}$$\n\nThe between-chain variance, $B_k$, measures the variance of the sample means across the chains, scaled by the sample size $n$:\n$$B_k = \\frac{n}{M-1} \\sum_{m=1}^{M} (\\mu_{m,k} - \\bar{\\mu}_k)^2$$\nwhere $\\bar{\\mu}_k = \\frac{1}{M} \\sum_{m=1}^{M} \\mu_{m,k}$ is the grand mean for observable $k$ across all chains.\n\nA pooled estimator for the marginal variance of the observable, $\\hat{V}_k$, combines these two variance components. It is a weighted average of the within-chain variance and the variance of the chain means:\n$$\\hat{V}_k = \\frac{n-1}{n} W_k + \\frac{1}{n} B_k$$\nThe factor $\\frac{n-1}{n}$ corrects for the fact that $W_k$ is an estimate from finite samples. When the chains have converged to the stationary distribution, both $W_k$ and $B_k/n$ estimate components of the total variance. If there is a discrepancy (e.g., chains are sampling different regions), the between-chain variance will be inflated.\n\nThe Potential Scale Reduction Factor, $\\hat{R}_k$, is the square root of the ratio of the pooled variance estimate to the average within-chain variance:\n$$\\hat{R}_k = \\sqrt{\\frac{\\hat{V}_k}{W_k}} = \\sqrt{\\frac{\\frac{n-1}{n} W_k + \\frac{1}{n} B_k}{W_k}} = \\sqrt{\\frac{n-1}{n} + \\frac{B_k}{nW_k}}$$\nIf the chains have converged, this ratio should be close to $1$.\n\n**Calculations for Univariate $\\hat{R}$:**\n\nFor observable $1$:\n$\\mu_{1,1}=0.50$, $\\mu_{2,1}=0.62$, $\\mu_{3,1}=0.54$.\n$\\bar{\\mu}_1 = \\frac{0.50+0.62+0.54}{3} = \\frac{1.66}{3}$.\n$B_1 = \\frac{20}{3-1} \\left[ \\left(0.50 - \\frac{1.66}{3}\\right)^2 + \\left(0.62 - \\frac{1.66}{3}\\right)^2 + \\left(0.54 - \\frac{1.66}{3}\\right)^2 \\right] = 10 \\left[ \\left(-\\frac{0.16}{3}\\right)^2 + \\left(\\frac{0.20}{3}\\right)^2 + \\left(-\\frac{0.04}{3}\\right)^2 \\right] = \\frac{10}{9}(0.0256 + 0.0400 + 0.0016) = \\frac{0.672}{9} \\approx 0.07467$.\n$S_{1,11}=0.050$, $S_{2,11}=0.048$, $S_{3,11}=0.052$.\n$W_1 = \\frac{0.050+0.048+0.052}{3} = \\frac{0.150}{3} = 0.050$.\n$\\hat{R}_1 = \\sqrt{\\frac{19}{20} + \\frac{0.07467}{20 \\times 0.050}} = \\sqrt{0.95 + 0.07467} = \\sqrt{1.02467} \\approx 1.0122$.\n\nFor observable $2$:\n$\\mu_{1,2}=1.00$, $\\mu_{2,2}=0.96$, $\\mu_{3,2}=1.06$.\n$\\bar{\\mu}_2 = \\frac{1.00+0.96+1.06}{3} = \\frac{3.02}{3}$.\n$B_2 = \\frac{20}{3-1} \\left[ \\left(1.00 - \\frac{3.02}{3}\\right)^2 + \\left(0.96 - \\frac{3.02}{3}\\right)^2 + \\left(1.06 - \\frac{3.02}{3}\\right)^2 \\right] = 10 \\left[ \\left(-\\frac{0.02}{3}\\right)^2 + \\left(-\\frac{0.14}{3}\\right)^2 + \\left(\\frac{0.16}{3}\\right)^2 \\right] = \\frac{10}{9}(0.0004 + 0.0196 + 0.0256) = \\frac{0.456}{9} \\approx 0.05067$.\n$S_{1,22}=0.042$, $S_{2,22}=0.039$, $S_{3,22}=0.041$.\n$W_2 = \\frac{0.042+0.039+0.041}{3} = \\frac{0.122}{3}$.\n$\\hat{R}_2 = \\sqrt{\\frac{19}{20} + \\frac{0.05067}{20 \\times (0.122/3)}} = \\sqrt{0.95 + \\frac{0.15201}{2.44}} \\approx \\sqrt{0.95 + 0.0623} = \\sqrt{1.0123} \\approx 1.0061$.\n\n**2. Multivariate Potential Scale Reduction Factor ($\\hat{R}_{\\mathrm{multi}}$)**\n\nThe univariate analysis is extended to the $p=2$ dimensional case by replacing scalar variances with covariance matrices.\n\nThe pooled within-chain covariance matrix, $\\mathbf{W}$, is the average of the chain covariance matrices:\n$$\\mathbf{W} = \\frac{1}{M} \\sum_{m=1}^{M} \\mathbf{S}_m$$\n\nThe between-chain covariance matrix, $\\mathbf{B}$, is:\n$$\\mathbf{B} = \\frac{n}{M-1} \\sum_{m=1}^{M} (\\boldsymbol{\\mu}_m - \\bar{\\boldsymbol{\\mu}})(\\boldsymbol{\\mu}_m - \\bar{\\boldsymbol{\\mu}})^T$$\nwhere $\\bar{\\boldsymbol{\\mu}} = \\frac{1}{M} \\sum_{m=1}^{M} \\boldsymbol{\\mu}_m$ is the grand mean vector.\n\nThe pooled estimator for the marginal covariance matrix, $\\hat{\\mathbf{V}}$, is:\n$$\\hat{\\mathbf{V}} = \\frac{n-1}{n} \\mathbf{W} + \\frac{1}{n} \\mathbf{B}$$\n\nThe multivariate PSRF, $\\hat{R}_{\\mathrm{multi}}$, is defined via the largest generalized eigenvalue, $\\lambda_{\\max}$, of $\\hat{\\mathbf{V}}$ with respect to $\\mathbf{W}$. This eigenvalue is the solution to the generalized eigenvalue problem $\\hat{\\mathbf{V}}\\mathbf{v} = \\lambda\\mathbf{W}\\mathbf{v}$.\nSubstituting the expression for $\\hat{\\mathbf{V}}$:\n$$\\left(\\frac{n-1}{n} \\mathbf{W} + \\frac{1}{n} \\mathbf{B}\\right)\\mathbf{v} = \\lambda\\mathbf{W}\\mathbf{v}$$\nAssuming $\\mathbf{W}$ is invertible, we can multiply by $\\mathbf{W}^{-1}$:\n$$\\left(\\frac{n-1}{n} \\mathbf{I} + \\frac{1}{n} \\mathbf{W}^{-1}\\mathbf{B}\\right)\\mathbf{v} = \\lambda\\mathbf{v}$$\nThis is a standard eigenvalue problem. The eigenvalues $\\lambda$ are related to the eigenvalues $\\eta$ of the matrix $\\mathbf{W}^{-1}\\mathbf{B}$ by $\\lambda = \\frac{n-1}{n} + \\frac{\\eta}{n}$.\nThe multivariate PSRF is the square root of the largest eigenvalue, $\\lambda_{\\max}$:\n$$\\hat{R}_{\\mathrm{multi}} = \\sqrt{\\lambda_{\\max}} = \\sqrt{\\frac{n-1}{n} + \\frac{\\eta_{\\max}}{n}}$$\nwhere $\\eta_{\\max}$ is the largest eigenvalue of $\\mathbf{W}^{-1}\\mathbf{B}$.\n\n**Calculations for Multivariate $\\hat{R}_{\\mathrm{multi}}$:**\n\nGrand mean vector:\n$\\bar{\\boldsymbol{\\mu}} = \\frac{1}{3} \\left( \\begin{pmatrix}0.50 \\\\ 1.00\\end{pmatrix} + \\begin{pmatrix}0.62 \\\\ 0.96\\end{pmatrix} + \\begin{pmatrix}0.54 \\\\ 1.06\\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix}1.66 \\\\ 3.02\\end{pmatrix}$.\n\nPooled within-chain covariance matrix $\\mathbf{W}$:\n$\\mathbf{W} = \\frac{1}{3} \\left( \\begin{pmatrix}0.050 & 0.012 \\\\ 0.012 & 0.042\\end{pmatrix} + \\begin{pmatrix}0.048 & 0.009 \\\\ 0.009 & 0.039\\end{pmatrix} + \\begin{pmatrix}0.052 & 0.011 \\\\ 0.011 & 0.041\\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix}0.150 & 0.032 \\\\ 0.032 & 0.122\\end{pmatrix}$.\n\nBetween-chain covariance matrix $\\mathbf{B}$:\nThe deviation vectors are $\\boldsymbol{\\mu}_1 - \\bar{\\boldsymbol{\\mu}} = \\frac{1}{3}\\begin{pmatrix}-0.16 \\\\ -0.02\\end{pmatrix}$, $\\boldsymbol{\\mu}_2 - \\bar{\\boldsymbol{\\mu}} = \\frac{1}{3}\\begin{pmatrix}0.20 \\\\ -0.14\\end{pmatrix}$, $\\boldsymbol{\\mu}_3 - \\bar{\\boldsymbol{\\mu}} = \\frac{1}{3}\\begin{pmatrix}-0.04 \\\\ 0.16\\end{pmatrix}$.\n$\\mathbf{B} = \\frac{20}{3-1} \\sum_{m=1}^{3} (\\boldsymbol{\\mu}_m - \\bar{\\boldsymbol{\\mu}})(\\boldsymbol{\\mu}_m - \\bar{\\boldsymbol{\\mu}})^T = \\frac{10}{9} \\left( \\begin{pmatrix}-0.16 \\\\ -0.02\\end{pmatrix}\\begin{pmatrix}-0.16 & -0.02\\end{pmatrix} + \\dots \\right)$\n$\\mathbf{B} = \\frac{10}{9} \\begin{pmatrix} 0.0672 & -0.0312 \\\\ -0.0312 & 0.0456 \\end{pmatrix} \\approx \\begin{pmatrix} 0.07467 & -0.03467 \\\\ -0.03467 & 0.05067 \\end{pmatrix}$.\n\nNext, we compute $\\mathbf{W}^{-1}\\mathbf{B}$.\n$\\det(\\mathbf{W}) = \\frac{1}{9}(0.150 \\times 0.122 - 0.032^2) = \\frac{1}{9}(0.0183 - 0.001024) = \\frac{0.017276}{9}$.\n$\\mathbf{W}^{-1} = \\frac{9}{0.017276} \\frac{1}{3} \\begin{pmatrix} 0.122 & -0.032 \\\\ -0.032 & 0.150 \\end{pmatrix} = \\frac{3}{0.017276} \\begin{pmatrix} 0.122 & -0.032 \\\\ -0.032 & 0.150 \\end{pmatrix}$.\n$\\mathbf{W}^{-1}\\mathbf{B} = \\frac{3}{0.017276} \\begin{pmatrix} 0.122 & -0.032 \\\\ -0.032 & 0.150 \\end{pmatrix} \\frac{10}{9} \\begin{pmatrix} 0.0672 & -0.0312 \\\\ -0.0312 & 0.0456 \\end{pmatrix} = \\frac{10}{3 \\times 0.017276} \\begin{pmatrix} 0.0091968 & -0.0052656 \\\\ -0.0068304 & 0.0078384 \\end{pmatrix}$.\nThe pre-factor is $\\frac{10}{0.051828} \\approx 192.943$.\n$\\mathbf{W}^{-1}\\mathbf{B} \\approx 192.943 \\begin{pmatrix} 0.0091968 & -0.0052656 \\\\ -0.0068304 & 0.0078384 \\end{pmatrix} \\approx \\begin{pmatrix} 1.7745 & -1.0159 \\\\ -1.3178 & 1.5123 \\end{pmatrix}$.\nThe eigenvalues $\\eta$ of this matrix are the roots of the characteristic equation $\\eta^2 - \\text{tr}(\\mathbf{W}^{-1}\\mathbf{B})\\eta + \\det(\\mathbf{W}^{-1}\\mathbf{B}) = 0$.\n$\\text{tr}(\\mathbf{W}^{-1}\\mathbf{B}) \\approx 1.7745 + 1.5123 = 3.2868$.\n$\\det(\\mathbf{W}^{-1}\\mathbf{B}) \\approx (1.7745)(1.5123) - (-1.0159)(-1.3178) \\approx 2.6835 - 1.3388 = 1.3447$.\n$\\eta^2 - 3.2868\\eta + 1.3447 = 0$.\n$\\eta = \\frac{3.2868 \\pm \\sqrt{3.2868^2 - 4(1.3447)}}{2} = \\frac{3.2868 \\pm \\sqrt{10.803 - 5.3788}}{2} = \\frac{3.2868 \\pm \\sqrt{5.4242}}{2} = \\frac{3.2868 \\pm 2.3290}{2}$.\nThe eigenvalues are $\\eta_1 \\approx 2.8079$ and $\\eta_2 \\approx 0.4789$. The largest is $\\eta_{\\max} \\approx 2.8079$.\nFinally, we compute $\\hat{R}_{\\mathrm{multi}}$:\n$\\hat{R}_{\\mathrm{multi}} = \\sqrt{\\frac{20-1}{20} + \\frac{\\eta_{\\max}}{20}} = \\sqrt{0.95 + \\frac{2.8079}{20}} = \\sqrt{0.95 + 0.140395} = \\sqrt{1.090395} \\approx 1.04422$.\n\n**3. Convergence Thresholds and Trend Requirements**\n\nThe PSRF, both univariate and multivariate, provides a dimensionless measure of convergence. The square of the PSRF, $\\hat{R}^2$, estimates the factor by which the marginal variance is overestimated relative to the within-chain variance. An ideal value is $\\hat{R}=1.0$, indicating that between-chain and within-chain variations are consistent.\n\n**Stopping Thresholds**: A commonly accepted, though problem-dependent, threshold for declaring convergence is when the PSRF for all monitored observables falls below a value such as $1.1$. More stringent applications might require a threshold of $1.05$ or $1.01$. For the multivariate case, $\\hat{R}_{\\mathrm{multi}} < 1.1$ is the corresponding criterion. A value of $\\hat{R}=1.1$ implies a potential scale reduction of $10\\%$, as the estimated variance of the observable is inflated by a factor of $1.1^2 \\approx 1.21$ due to incomplete convergence.\n\n**Trend Requirements**: Achieving the threshold at a single point in time is insufficient. True convergence requires stability. The PSRF should be monitored over successive, non-overlapping time windows of the simulation. A robust criterion for stopping the equilibration phase is that the PSRF, computed over these windows, must not only consistently remain below the chosen threshold but also exhibit no significant increasing trend. A stable or decreasing PSRF value over a substantial period suggests that the chains are reliably sampling from the same stationary distribution. Visual inspection of a plot of $\\hat{R}$ versus simulation time is standard practice to confirm this stability.\n\nRounding the requested final answer to four significant figures gives $1.044$.",
            "answer": "$$\n\\boxed{1.044}\n$$"
        },
        {
            "introduction": "Determining the precise moment a simulation transitions from the initial, non-equilibrium transient to the stable, equilibrated state is a ubiquitous challenge. While visual inspection of property plots is common, it is subjective and not easily automated. This exercise guides you through the process of developing a principled and automated procedure for this task using change-point detection. By implementing a multivariate statistical test to identify the last significant shift in the mean of key observables , you will build a practical tool that brings quantitative rigor to the critical task of identifying the start of the production phase.",
            "id": "3405260",
            "problem": "Consider a Molecular Dynamics (MD) trajectory undergoing equilibration, producing time series for total energy $E(t)$, pressure $P(t)$, and volume $V(t)$ at discrete time steps indexed by an integer $i \\in \\{0,1,\\dots,N-1\\}$. The purpose is to monitor convergence during equilibration by detecting the last significant transient in the joint observable $(E,P,V)$, so that the production stage can begin only after this change point. The target is to design and implement a programmatic change-point detection (CPD) procedure that, under physically realistic assumptions, locates the final statistically significant shift in the stationary distribution of the vector observable.\n\nUse the following context-appropriate base. MD follows Newton’s second law $m \\, d^2 \\mathbf{r}/dt^2 = \\mathbf{F}$ with forces arising from an interaction potential $U(\\mathbf{r})$. In equilibrium at fixed thermodynamic parameters, time averages over a stationary and ergodic trajectory estimate ensemble averages; numerically, this manifests as approximate stationarity of thermodynamic observables. Practical equilibration often produces piecewise-stationary behavior: early transients with distinct mean levels followed by a stable mean. This motivates modeling $(E,P,V)$ as a multivariate time series with piecewise-constant mean and finite covariance over segments. Under this model, the last significant transient is the last index at which the mean vector of $(E,P,V)$ undergoes a statistically significant shift relative to its covariance.\n\nDesign a principled CPD rule for a single abrupt mean shift in a multivariate Gaussian model with unknown covariance and then extend it to multiple shifts via a recursive procedure that isolates later changes. Your CPD must:\n- Treat the joint observable $\\mathbf{X}_i = [E_i, P_i, V_i]^\\top$ as multivariate data.\n- Base significance on a valid multivariate two-sample comparison of the pre- and post-split segments with unknown covariance, deriving its test statistic and its null reference distribution from first principles.\n- Impose a minimal window size $w_{\\min}$ so that both pre- and post-split samples have at least $w_{\\min}$ observations, ensuring the test’s degrees of freedom are valid.\n- For multiple changes, apply a logically sound recursive segmentation that yields candidate change points from left to right and returns the last detected change point.\n- If no statistically significant change point exists, return $0$ to indicate that production may start at the beginning.\n\nYour program must implement the above logic and apply it to the test suite below. For all test cases, the data are fully determined and require no randomness. Each test case provides the construction of $\\mathbf{X}_i$ for $i \\in \\{0,\\dots,N-1\\}$ via a piecewise-constant mean vector and a deterministic, bounded oscillatory perturbation. Specifically, the observable is constructed as\n$$\n\\mathbf{X}_i = \\boldsymbol{\\mu}(i) + \\boldsymbol{\\eta}(i),\n$$\nwhere $\\boldsymbol{\\mu}(i)$ is either piecewise constant over segments or a linear drift, and $\\boldsymbol{\\eta}(i)$ is a deterministic oscillatory term for each component. Let the oscillatory perturbation be\n$$\n\\eta_d(i) = A_d \\sin\\!\\left( \\frac{2\\pi i}{p_d} \\right) + B_d \\cos\\!\\left( \\frac{2\\pi i}{q_d} \\right),\n$$\nwith known amplitudes $A_d$, $B_d$ and periods $p_d$, $q_d$ for each component $d \\in \\{E,P,V\\}$.\n\nImplement the following five test cases. In each case, construct $\\mathbf{X}_i$ exactly as specified, apply your CPD, and return the last change point as an integer index. If no change is detected at significance level $\\alpha$, return $0$.\n\nTest case $1$ (multiple transients; happy path):\n- $N = 1000$, $w_{\\min} = 50$, $\\alpha = 0.01$.\n- Change points at indices $200$ and $600$.\n- Segment means:\n  - $i \\in [0,199]$: $\\boldsymbol{\\mu} = [-5000.0, 100.0, 10.0]$.\n  - $i \\in [200,599]$: $\\boldsymbol{\\mu} = [-4500.0, 92.0, 10.4]$.\n  - $i \\in [600,999]$: $\\boldsymbol{\\mu} = [-4400.0, 95.0, 10.2]$.\n- Oscillatory perturbation parameters: $A = [30.0, 1.5, 0.08]$, $B = [15.0, 0.7, 0.04]$, $p = [37, 43, 71]$, $q = [41, 47, 73]$.\n\nTest case $2$ (already equilibrated; no change):\n- $N = 500$, $w_{\\min} = 50$, $\\alpha = 0.01$.\n- No change points.\n- Constant mean: $\\boldsymbol{\\mu} = [-4400.0, 95.0, 10.2]$ for all $i$.\n- Oscillatory perturbation parameters: $A = [28.0, 1.4, 0.08]$, $B = [14.0, 0.6, 0.04]$, $p = [29, 41, 67]$, $q = [31, 43, 69]$.\n\nTest case $3$ (single late transient; boundary condition):\n- $N = 400$, $w_{\\min} = 50$, $\\alpha = 0.01$.\n- Change point at index $350$.\n- Segment means:\n  - $i \\in [0,349]$: $\\boldsymbol{\\mu} = [-4700.0, 110.0, 9.8]$.\n  - $i \\in [350,399]$: $\\boldsymbol{\\mu} = [-4600.0, 105.0, 10.0]$.\n- Oscillatory perturbation parameters: $A = [25.0, 2.0, 0.10]$, $B = [12.0, 0.8, 0.05]$, $p = [31, 47, 59]$, $q = [33, 49, 61]$.\n\nTest case $4$ (slow drift; edge case):\n- $N = 600$, $w_{\\min} = 50$, $\\alpha = 0.01$.\n- Linear drift in mean from start to end:\n  - $\\boldsymbol{\\mu}(0) = [-4600.0, 100.0, 10.0]$,\n  - $\\boldsymbol{\\mu}(N-1) = [-4590.0, 102.0, 10.02]$,\n  - for general $i$, $\\boldsymbol{\\mu}(i) = \\boldsymbol{\\mu}(0) + \\frac{i}{N-1}\\left(\\boldsymbol{\\mu}(N-1) - \\boldsymbol{\\mu}(0)\\right)$.\n- Oscillatory perturbation parameters: $A = [30.0, 5.0, 0.20]$, $B = [15.0, 2.0, 0.10]$, $p = [35, 39, 63]$, $q = [37, 41, 65]$.\n\nTest case $5$ (heterogeneous changes across components):\n- $N = 800$, $w_{\\min} = 50$, $\\alpha = 0.01$.\n- Change points at indices $150$ and $300$.\n- Segment means:\n  - $i \\in [0,149]$: $\\boldsymbol{\\mu} = [-4800.0, 99.0, 10.1]$.\n  - $i \\in [150,299]$: $\\boldsymbol{\\mu} = [-4800.0, 104.0, 10.1]$.\n  - $i \\in [300,799]$: $\\boldsymbol{\\mu} = [-4700.0, 104.0, 10.1]$.\n- Oscillatory perturbation parameters: $A = [29.0, 1.2, 0.08]$, $B = [14.5, 0.5, 0.04]$, $p = [33, 45, 69]$, $q = [35, 47, 71]$.\n\nRequired output:\n- For each test case, compute the integer index at which production should start, meaning one plus the detected last change point, or equivalently the detected last change point if the convention is the first index of the post-change segment. For this problem, report the detected last change point index itself as an integer, and use $0$ if no change is detected.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, in the order of test cases $1$ through $5$, for example $[r_1,r_2,r_3,r_4,r_5]$, where each $r_j$ is an integer.\n\nThe program must be self-contained, require no user input, and must rely only on the Python standard library, Numerical Python (NumPy), and Scientific Python (SciPy). The answers for each test case are integers.",
            "solution": "The problem requires the design of a change-point detection (CPD) algorithm to identify the last significant transient in a multivariate time series of molecular dynamics (MD) observables. The observables are the total energy $E$, pressure $P$, and volume $V$, forming a $3$-dimensional vector $\\mathbf{X}_i = [E_i, P_i, V_i]^\\top$ at each discrete time step $i$. The underlying physical model assumes that during equilibration, the system transitions between states characterized by different mean values of these observables, eventually settling into a stationary state with a constant mean vector. This behavior is modeled as a piecewise-stationary process with abrupt shifts in the mean vector. The task is to implement a principled statistical procedure to find the index of the last such shift.\n\nThe solution is developed in three main stages: first, establishing the statistical test for a single change point; second, defining a search procedure to locate the most probable change point within a data segment; and third, constructing a recursive algorithm to identify the final change point among multiple potential shifts.\n\n1.  **Statistical Test for a Single Change Point**\n\nWe address the core task of comparing two segments of the multivariate time series to determine if their underlying mean vectors are different. Let us consider a potential change point at index $k$ in a data series of length $N$. This splits the data into a pre-split segment $\\mathcal{S}_1 = \\{\\mathbf{X}_0, \\dots, \\mathbf{X}_{k-1}\\}$ of size $n_1 = k$ and a post-split segment $\\mathcal{S}_2 = \\{\\mathbf{X}_k, \\dots, \\mathbf{X}_{N-1}\\}$ of size $n_2 = N-k$.\n\nThe problem states that we have unknown covariance and models the data as multivariate Gaussian. The appropriate statistical tool for this scenario is **Hotelling's two-sample $T^2$ test**. This test evaluates the null hypothesis $H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2$ against the alternative $H_1: \\boldsymbol{\\mu}_1 \\neq \\boldsymbol{\\mu}_2$, where $\\boldsymbol{\\mu}_1$ and $\\boldsymbol{\\mu}_2$ are the true mean vectors of the populations from which $\\mathcal{S}_1$ and $\\mathcal{S}_2$ are sampled. The test assumes that the covariance matrices are equal ($\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\boldsymbol{\\Sigma}$) but unknown.\n\nThe test procedure is as follows:\n- Compute the sample mean vectors for each segment:\n$$ \\overline{\\mathbf{X}}_1 = \\frac{1}{n_1} \\sum_{i \\in \\mathcal{S}_1} \\mathbf{X}_i \\quad \\text{and} \\quad \\overline{\\mathbf{X}}_2 = \\frac{1}{n_2} \\sum_{i \\in \\mathcal{S}_2} \\mathbf{X}_i $$\n- Compute the unbiased sample covariance matrices:\n$$ \\mathbf{S}_1 = \\frac{1}{n_1-1} \\sum_{i \\in \\mathcal{S}_1} (\\mathbf{X}_i - \\overline{\\mathbf{X}}_1)(\\mathbf{X}_i - \\overline{\\mathbf{X}}_1)^\\top \\quad \\text{and} \\quad \\mathbf{S}_2 = \\frac{1}{n_2-1} \\sum_{i \\in \\mathcal{S}_2} (\\mathbf{X}_i - \\overline{\\mathbf{X}}_2)(\\mathbf{X}_i - \\overline{\\mathbf{X}}_2)^\\top $$\n- Calculate the pooled covariance matrix, which provides a better estimate of the common covariance $\\boldsymbol{\\Sigma}$:\n$$ \\mathbf{S}_{\\text{pool}} = \\frac{(n_1-1)\\mathbf{S}_1 + (n_2-1)\\mathbf{S}_2}{n_1 + n_2 - 2} $$\n- The Hotelling's $T^2$ statistic, which measures the squared Mahalanobis distance between the sample means, is given by:\n$$ T^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\overline{\\mathbf{X}}_1 - \\overline{\\mathbf{X}}_2)^\\top \\mathbf{S}_{\\text{pool}}^{-1} (\\overline{\\mathbf{X}}_1 - \\overline{\\mathbf{X}}_2) $$\n- For hypothesis testing, the $T^2$ statistic is transformed into an $F$-statistic, which follows a known distribution under $H_0$. Letting $p=3$ be the dimensionality of the data:\n$$ F = \\frac{n_1 + n_2 - p - 1}{(n_1 + n_2 - 2) p} T^2 $$\nUnder $H_0$, this statistic follows an $F$-distribution with $p$ and $n_1 + n_2 - p - 1$ degrees of freedom, i.e., $F \\sim F_{p, n_1+n_2-p-1}$. A large value of $F$ indicates a significant difference between the means.\n\n2.  **Locating the Most Likely Change Point in a Segment**\n\nTo find a single change point within a data segment spanning indices from `start` to `end`, we must identify the split that shows the strongest evidence of a change. We iterate through all possible split points $k$ from `start`+$w_{\\min}$ to `end`-$w_{\\min}$, ensuring both resulting sub-segments have at least the minimum window size $w_{\\min}$. For each $k$, we compute the $F$-statistic as described above. The candidate change point, $k^*$, is the index $k$ that maximizes the $F$-statistic. The change is considered statistically significant if this maximum statistic, $F(k^*)$, exceeds the critical value $F_{\\text{crit}}$ from the $F_{p, n_1+n_2-p-1}$ distribution at the specified significance level $\\alpha$.\n\n3.  **Recursive Segmentation for the Last Change Point**\n\nTo find the *last* significant change, we employ a recursive segmentation strategy. The algorithm operates on a segment of data defined by `start` and `end` indices.\n\n- **Base Case:** If a segment is too small to be split into two valid sub-segments (i.e., its length is less than $2w_{\\min}$), the recursion terminates, returning $0$ (no change point).\n\n- **Recursive Step:**\n    1.  The procedure first finds the most likely change point $k^*$ within the current segment `[start, end]` by maximizing the $F$-statistic, as described in the previous section.\n    2.  It then tests this candidate for significance. If $F(k^*) \\le F_{\\text{crit}}$, no significant change exists in the current segment, and the function returns $0$.\n    3.  If $F(k^*) > F_{\\text{crit}}$, a significant change is confirmed at $k^*$. However, this may not be the *last* one. To check for subsequent changes, the algorithm makes a recursive call on the right-hand sub-segment, spanning from $k^*$ to `end`.\n    4.  Let the result of this recursive call be `last_cp_after`. If `last_cp_after` is greater than $0$, it means a later change point was found, and this later index is propagated up as the result. If `last_cp_after` is $0$, it signifies no further changes were found, making $k^*$ the last change point in this branch of the search. The value $k^*$ is then returned.\n\nThis \"dive-right\" recursive strategy ensures that the search prioritizes and reports the change point with the largest index. The initial call to this procedure on the full dataset, `search(0, N)`, will yield the last significant change point in the entire time series, or $0$ if the series is found to be stationary.\n\nFor the provided test cases, data is first generated according to the specified piecewise-constant or linear-drift mean functions and the deterministic oscillatory perturbation. The recursive CPD algorithm is then applied with the given parameters ($N$, $w_{\\min}$, $\\alpha$) to compute the final result.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import f\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            \"N\": 1000, \"w_min\": 50, \"alpha\": 0.01,\n            \"mu_spec\": [\n                (0, np.array([-5000.0, 100.0, 10.0])),\n                (200, np.array([-4500.0, 92.0, 10.4])),\n                (600, np.array([-4400.0, 95.0, 10.2])),\n            ],\n            \"A\": [30.0, 1.5, 0.08], \"B\": [15.0, 0.7, 0.04],\n            \"p\": [37, 43, 71], \"q\": [41, 47, 73],\n        },\n        # Test case 2\n        {\n            \"N\": 500, \"w_min\": 50, \"alpha\": 0.01,\n            \"mu_spec\": [\n                (0, np.array([-4400.0, 95.0, 10.2])),\n            ],\n            \"A\": [28.0, 1.4, 0.08], \"B\": [14.0, 0.6, 0.04],\n            \"p\": [29, 41, 67], \"q\": [31, 43, 69],\n        },\n        # Test case 3\n        {\n            \"N\": 400, \"w_min\": 50, \"alpha\": 0.01,\n            \"mu_spec\": [\n                (0, np.array([-4700.0, 110.0, 9.8])),\n                (350, np.array([-4600.0, 105.0, 10.0])),\n            ],\n            \"A\": [25.0, 2.0, 0.10], \"B\": [12.0, 0.8, 0.05],\n            \"p\": [31, 47, 59], \"q\": [33, 49, 61],\n        },\n        # Test case 4\n        {\n            \"N\": 600, \"w_min\": 50, \"alpha\": 0.01,\n            \"mu_spec\": (\n                np.array([-4600.0, 100.0, 10.0]),\n                np.array([-4590.0, 102.0, 10.02]),\n            ),\n            \"A\": [30.0, 5.0, 0.20], \"B\": [15.0, 2.0, 0.10],\n            \"p\": [35, 39, 63], \"q\": [37, 41, 65],\n        },\n        # Test case 5\n        {\n            \"N\": 800, \"w_min\": 50, \"alpha\": 0.01,\n            \"mu_spec\": [\n                (0, np.array([-4800.0, 99.0, 10.1])),\n                (150, np.array([-4800.0, 104.0, 10.1])),\n                (300, np.array([-4700.0, 104.0, 10.1])),\n            ],\n            \"A\": [29.0, 1.2, 0.08], \"B\": [14.5, 0.5, 0.04],\n            \"p\": [33, 45, 69], \"q\": [35, 47, 71],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X = _generate_data(case[\"N\"], case[\"mu_spec\"], case[\"A\"], case[\"B\"], case[\"p\"], case[\"q\"])\n        last_cp = _find_last_changepoint(X, case[\"w_min\"], case[\"alpha\"])\n        results.append(last_cp)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef _generate_data(N, mu_spec, A, B, p_periods, q_periods):\n    \"\"\"\n    Generates deterministic time series data based on problem specification.\n    \"\"\"\n    dim = len(A)\n    X = np.zeros((N, dim))\n    t = np.arange(N)\n\n    # Generate oscillatory part eta\n    eta = np.zeros_like(X)\n    for d in range(dim):\n        eta[:, d] = A[d] * np.sin(2 * np.pi * t / p_periods[d]) + \\\n                    B[d] * np.cos(2 * np.pi * t / q_periods[d])\n\n    # Generate mean part mu\n    mu = np.zeros_like(X)\n    if isinstance(mu_spec, list):  # Piecewise constant\n        mu_points = sorted(mu_spec, key=lambda x: x[0])\n        mu_idx_map = np.zeros(N, dtype=int)\n        for i, (start_idx, _) in enumerate(mu_points[1:], 1):\n            mu_idx_map[start_idx:] = i\n        \n        for i in range(N):\n            mu[i, :] = mu_points[mu_idx_map[i]][1]\n\n    else:  # Linear drift\n        mu0, muN_1 = mu_spec\n        for i in range(N):\n             mu[i, :] = mu0 + (i / (N - 1)) * (muN_1 - mu0)\n\n    X = mu + eta\n    return X\n\n\ndef _find_last_changepoint(X, w_min, alpha):\n    \"\"\"\n    Finds the last significant change point in the multivariate time series X.\n    This function wraps the main recursive search logic.\n    \"\"\"\n    N, p = X.shape\n    memo = {}\n\n    def _recursive_search(start, end):\n        \"\"\"\n        Recursively searches for the last change point in the segment [start, end).\n        \"\"\"\n        if (start, end) in memo:\n            return memo[(start, end)]\n\n        # Base case: segment is too small to split\n        if end - start < 2 * w_min:\n            return 0\n\n        # Define search range for the split point k\n        k_range = range(start + w_min, end - w_min + 1)\n        if not k_range:\n            return 0\n\n        best_k = -1\n        max_f_stat = -1.0\n        \n        # Find the split point k that maximizes the F-statistic\n        for k in k_range:\n            n1 = k - start\n            n2 = end - k\n            \n            X1 = X[start:k]\n            X2 = X[k:end]\n\n            mean1 = np.mean(X1, axis=0)\n            mean2 = np.mean(X2, axis=0)\n\n            # Using ddof=1 for unbiased sample covariance\n            S1 = np.cov(X1, rowvar=False, ddof=1)\n            S2 = np.cov(X2, rowvar=False, ddof=1)\n            \n            # Pooled covariance matrix\n            Spool = ((n1 - 1) * S1 + (n2 - 1) * S2) / (n1 + n2 - 2)\n\n            try:\n                Spool_inv = np.linalg.inv(Spool)\n            except np.linalg.LinAlgError:\n                # This should not happen with the given data and w_min.\n                continue\n            \n            delta_mean = mean1 - mean2\n            \n            # Hotelling's T-squared statistic\n            t2_stat = (n1 * n2) / (n1 + n2) * (delta_mean.T @ Spool_inv @ delta_mean)\n            \n            # Corresponding F-statistic\n            f_stat = t2_stat * (n1 + n2 - p - 1) / ((n1 + n2 - 2) * p)\n\n            if f_stat > max_f_stat:\n                max_f_stat = f_stat\n                best_k = k\n        \n        if best_k == -1: # No valid split was found\n             memo[(start, end)] = 0\n             return 0\n\n        # Check significance of the most likely change point\n        dfn = p\n        dfd = (end - start) - p - 1 # n1 + n2 - p - 1\n        f_crit = f.ppf(1 - alpha, dfn=dfn, dfd=dfd)\n\n        if max_f_stat > f_crit:\n            # Change is significant. Search for another one after it.\n            later_cp = _recursive_search(best_k, end)\n            if later_cp > 0:\n                result = later_cp\n            else:\n                result = best_k\n        else:\n            # No significant change found in this segment\n            result = 0\n        \n        memo[(start, end)] = result\n        return result\n\n    return _recursive_search(0, N)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}