{
    "hands_on_practices": [
        {
            "introduction": "A crucial aspect of planning an efficient molecular dynamics simulation is the careful parameterization of the algorithms used, especially for computationally intensive tasks like calculating long-range electrostatic interactions. The Particle-Mesh Ewald (PME) method offers a powerful solution, but its accuracy and computational cost are sensitively controlled by a trio of parameters: the real-space cutoff $r_c$, the Ewald splitting parameter $\\alpha$, and the FFT grid spacing. This exercise  guides you through a practical optimization problem, where you must balance the trade-off between computational cost and the accuracy of the forces, a fundamental challenge in computational science. By searching for the set of parameters that minimizes cost for a target accuracy, you will develop a quantitative feel for how these choices impact simulation fidelity and performance.",
            "id": "3438102",
            "problem": "You are planning the production-phase electrostatics settings for a classical Molecular Dynamics simulation that will use Particle-Mesh Ewald (PME). You must construct a program that, for each system in a given test suite, selects PME parameters to minimize a cost model subject to a fixed accuracy constraint. Your selection variables are the Ewald splitting parameter $\\,\\alpha\\,$, the real-space cutoff $\\,r_c\\,$, and an isotropic three-dimensional grid size $\\,\\mathbf{n}_{\\mathrm{grid}} = (n,n,n)\\,$ constrained to be Fast Fourier Transform (FFT) friendly. The task is to produce a single-line output aggregating the optimal choices for all test cases.\n\nFundamental base and definitions:\n- The Ewald summation splits Coulomb interactions into real-space and reciprocal-space parts controlled by the Ewald splitting parameter $\\,\\alpha\\,$, such that complementary error functions weigh the short-range part and Gaussian screening weighs the long-range part.\n- The Root-Mean-Square (RMS) force error is modeled as the quadrature sum of a real-space truncation error and a reciprocal-space discretization error. We use a widely employed and conservative pair of error models that scale with density and characteristic spectral gaps:\n  1. Real-space RMS force error model:\n     $$\\mathcal{E}_{\\mathrm{real}}(\\alpha, r_c; \\rho, q_{\\mathrm{rms}}) \\;=\\; k_{\\mathrm{real}}\\; q_{\\mathrm{rms}}^2 \\, \\sqrt{\\rho}\\, \\exp\\!\\left(-(\\alpha r_c)^2\\right).$$\n  2. Reciprocal-space RMS force error model for smooth Particle-Mesh Ewald with cardinal B-spline assignment order $\\,p\\,$:\n     $$\\mathcal{E}_{\\mathrm{recip}}(\\alpha, n; L, \\rho, q_{\\mathrm{rms}}, p) \\;=\\; k_{\\mathrm{recip}}\\; q_{\\mathrm{rms}}^2 \\, \\sqrt{\\rho}\\; n^{-p}\\, \\exp\\!\\left(-\\left(\\frac{\\pi n}{\\alpha L}\\right)^2\\right).$$\n  The total RMS force error is:\n     $$\\mathcal{E}_{\\mathrm{tot}}(\\alpha, r_c, n) \\;=\\; \\sqrt{\\mathcal{E}_{\\mathrm{real}}(\\alpha, r_c)^2 + \\mathcal{E}_{\\mathrm{recip}}(\\alpha, n)^2}.$$\n- The grid size vector is constrained to $\\,\\mathbf{n}_{\\mathrm{grid}} = (n,n,n)\\,$ with $\\,n\\,$ restricted to be a $\\,5$-smooth integer (its prime factors are only $\\,2,3,5\\,$), chosen from a bounded interval $[n_{\\min}, n_{\\max}]$.\n- The simulation box is cubic of side length $\\,L\\,$, with $\\,N\\,$ particles and number density $\\,\\rho = N/L^3\\,$. All quantities are treated as dimensionless for this planning problem. The error tolerance $\\,\\varepsilon\\,$ is a dimensionless fraction of a characteristic force scale. The cost is in arbitrary time units per step.\n- The computational cost per time step is modeled as the sum of three components:\n  - Real-space pairwise computations with a Verlet list within cutoff $\\,r_c\\,$:\n    $$C_{\\mathrm{real}}(N, \\rho, r_c) \\;=\\; c_{\\mathrm{pair}}\\; N\\, \\rho\\, r_c^3.$$\n  - Charge spreading and force gathering on the mesh with B-spline of order $\\,p\\,$:\n    $$C_{\\mathrm{spread}}(n, p) \\;=\\; c_{\\mathrm{spread}}\\, p^3\\, n^3.$$\n  - Three-dimensional Fast Fourier Transform (FFT):\n    $$C_{\\mathrm{FFT}}(n) \\;=\\; c_{\\mathrm{FFT}}\\, n^3 \\log_2(n^3).$$\n  The total cost is:\n    $$C_{\\mathrm{tot}} \\;=\\; C_{\\mathrm{real}} + C_{\\mathrm{spread}} + C_{\\mathrm{FFT}}.$$\n\nOptimization problem:\n- Given $\\,N, L, q_{\\mathrm{rms}}, p, \\varepsilon\\,$, constants $\\,k_{\\mathrm{real}}, k_{\\mathrm{recip}}, c_{\\mathrm{pair}}, c_{\\mathrm{spread}}, c_{\\mathrm{FFT}}\\,$, and bounds on $\\,r_c, \\alpha, n\\,$, select $\\,(\\alpha, r_c, n)\\,$ to minimize $\\,C_{\\mathrm{tot}}\\,$ subject to:\n  $$\\mathcal{E}_{\\mathrm{tot}}(\\alpha, r_c, n) \\;\\le\\; \\varepsilon.$$\n- The search domains are constrained for practical production runs:\n  - Cutoff range: $\\,r_c \\in [r_{c,\\min}, r_{c,\\max}]$ with $\\,r_{c,\\min} = 0.5\\,$ and $\\,r_{c,\\max} = 0.49 L\\,$.\n  - Ewald splitting range tied to $\\,r_c\\,$: $\\,\\alpha \\in [\\alpha_{\\min}(r_c), \\alpha_{\\max}(r_c)]\\,$ with $\\,\\alpha_{\\min}(r_c) = 0.5/r_c\\,$ and $\\,\\alpha_{\\max}(r_c) = 3.5/r_c\\,$.\n  - Grid sizes: $\\,n \\in \\mathcal{N}\\,$, the set of $\\,5$-smooth integers in $[n_{\\min}, n_{\\max}]$ with $\\,n_{\\min} = 32\\,$ and $\\,n_{\\max} = 128\\,$.\n- If no feasible triple $\\,(\\alpha, r_c, n)\\,$ satisfies the error constraint, report an infeasible result.\n\nConstants to use:\n- Error model constants: $\\,k_{\\mathrm{real}} = 1.0\\,$ and $\\,k_{\\mathrm{recip}} = 1.0\\,$.\n- Cost model constants: $\\,c_{\\mathrm{pair}} = 10^{-6}\\,$, $\\,c_{\\mathrm{spread}} = 2\\times 10^{-7}\\,$, and $\\,c_{\\mathrm{FFT}} = 5\\times 10^{-6}\\,$.\n\nRequired outputs:\n- For each test case, your program must output the chosen parameters and metrics as a list:\n  $$[\\alpha^\\star, r_c^\\star, n^\\star, C_{\\mathrm{tot}}^\\star, \\mathcal{E}_{\\mathrm{tot}}^\\star, \\text{feasible}],$$\n  where $\\,\\text{feasible}\\,$ is a boolean that is true when a feasible solution exists and false otherwise. All floating-point outputs must be rounded to $\\,6\\,$ decimal places. If infeasible, return $\\,[-1,-1,-1,-1,-1,\\text{False}]\\,$ for that test case.\n- Your program should produce a single line of output containing the results as a comma-separated list of the per-test-case lists, enclosed in a single pair of square brackets. For example: $\\,\\texttt{[[...],[...],[...]]}\\,$.\n\nTest suite:\nUse the following four test cases, each described by the tuple $\\, (N, L, q_{\\mathrm{rms}}, p, \\varepsilon)\\,$:\n1. $\\, (100000,\\, 10.0,\\, 0.30,\\, 4,\\, 10^{-3})\\,$.\n2. $\\, (100000,\\, 10.0,\\, 0.30,\\, 4,\\, 3\\times 10^{-5})\\,$.\n3. $\\, (5000,\\, 6.0,\\, 0.50,\\, 6,\\, 5\\times 10^{-4})\\,$.\n4. $\\, (200000,\\, 8.0,\\, 0.25,\\, 6,\\, 10^{-5})\\,$.\n\nAlgorithmic requirements:\n- Your program must search the discrete set $\\,\\mathcal{N}\\,$ for $\\,n\\,$ and sample $\\,r_c\\,$ and $\\,\\alpha\\,$ on uniform grids within the specified ranges. Use at least $\\,40\\,$ linearly spaced samples for $\\,r_c\\,$ within $[r_{c,\\min}, r_{c,\\max}]$ and at least $\\,40\\,$ linearly spaced samples for $\\,\\alpha\\,$ within $[\\alpha_{\\min}(r_c), \\alpha_{\\max}(r_c)]$ for each $\\,r_c\\,$.\n- For each feasible combination, compute the total cost and select the one with minimal cost. Break ties by preferring smaller $\\,n\\,$, then smaller $\\,r_c\\,$, then smaller $\\,\\alpha\\,$.\n\nFinal output format:\n- Your program must produce a single line containing a single list with four sublists corresponding to the four test cases, each sublist having the format and rounding specified above, with no other text. For example:\n  $$\\texttt{[[a1,rc1,n1,c1,e1,True],[a2,rc2,n2,c2,e2,True],[...],[...]]}.$$",
            "solution": "The problem presented is a constrained optimization task central to the planning of molecular dynamics simulations employing the Particle-Mesh Ewald (PME) method for long-range electrostatics. The objective is to select a set of PME parameters—the Ewald splitting parameter $\\alpha$, the real-space cutoff distance $r_c$, and the FFT grid dimension $n$—that minimizes the computational cost per timestep, $C_{\\mathrm{tot}}$, while satisfying a user-defined tolerance $\\varepsilon$ on the root-mean-square (RMS) force error, $\\mathcal{E}_{\\mathrm{tot}}$.\n\nFirst, we restate the governing mathematical models provided. The total RMS force error, $\\mathcal{E}_{\\mathrm{tot}}$, is the quadrature sum of contributions from the real-space and reciprocal-space summations:\n$$\n\\mathcal{E}_{\\mathrm{tot}}(\\alpha, r_c, n) = \\sqrt{\\mathcal{E}_{\\mathrm{real}}(\\alpha, r_c)^2 + \\mathcal{E}_{\\mathrm{recip}}(\\alpha, n)^2}\n$$\nThe real-space error, $\\mathcal{E}_{\\mathrm{real}}$, arises from truncating the short-range interactions at the cutoff $r_c$:\n$$\n\\mathcal{E}_{\\mathrm{real}}(\\alpha, r_c; \\rho, q_{\\mathrm{rms}}) = k_{\\mathrm{real}} q_{\\mathrm{rms}}^2 \\sqrt{\\rho} \\exp(-(\\alpha r_c)^2)\n$$\nThe reciprocal-space error, $\\mathcal{E}_{\\mathrm{recip}}$, arises from the discretization of reciprocal space onto a grid of size $\\mathbf{n}_{\\mathrm{grid}} = (n,n,n)$:\n$$\n\\mathcal{E}_{\\mathrm{recip}}(\\alpha, n; L, \\rho, q_{\\mathrm{rms}}, p) = k_{\\mathrm{recip}} q_{\\mathrm{rms}}^2 \\sqrt{\\rho} n^{-p} \\exp\\left(-\\left(\\frac{\\pi n}{\\alpha L}\\right)^2\\right)\n$$\nHere, $\\rho = N/L^3$ is the particle number density, $q_{\\mathrm{rms}}$ is the root-mean-square particle charge, $L$ is the cubic box side length, and $p$ is the B-spline interpolation order. The constants $k_{\\mathrm{real}}$ and $k_{\\mathrm{recip}}$ are given as $1.0$.\n\nThe total computational cost, $C_{\\mathrm{tot}}$, is modeled as the sum of three primary components:\n$$\nC_{\\mathrm{tot}}(r_c, n; N, \\rho, p) = C_{\\mathrm{real}}(r_c) + C_{\\mathrm{spread}}(n) + C_{\\mathrm{FFT}}(n)\n$$\nwhere:\n- $C_{\\mathrm{real}}(N, \\rho, r_c) = c_{\\mathrm{pair}} N \\rho r_c^3$ is the cost of real-space pair computations.\n- $C_{\\mathrm{spread}}(n, p) = c_{\\mathrm{spread}} p^3 n^3$ is the cost of spreading charges to the mesh and gathering forces.\n- $C_{\\mathrm{FFT}}(n) = c_{\\mathrm{FFT}} n^3 \\log_2(n^3)$ is the cost of the $3$D Fast Fourier Transform.\nThe cost constants are specified as $c_{\\mathrm{pair}} = 10^{-6}$, $c_{\\mathrm{spread}} = 2\\times 10^{-7}$, and $c_{\\mathrm{FFT}} = 5\\times 10^{-6}$.\n\nThe optimization problem is to find $(\\alpha^\\star, r_c^\\star, n^\\star)$ that minimizes $C_{\\mathrm{tot}}$ subject to the constraint $\\mathcal{E}_{\\mathrm{tot}} \\le \\varepsilon$. The search space is defined by the parameters:\n- $n \\in \\mathcal{N}$, the set of $5$-smooth integers (prime factors $\\in \\{2,3,5\\}$) in the interval $[32, 128]$.\n- $r_c \\in [r_{c,\\min}, r_{c,\\max}] = [0.5, 0.49 L]$.\n- $\\alpha \\in [\\alpha_{\\min}(r_c), \\alpha_{\\max}(r_c)] = [0.5/r_c, 3.5/r_c]$.\n\nGiven the non-linear and coupled nature of the error and cost functions, an analytical solution is intractable. The prescribed solution method is a systematic grid search over the discretized parameter space. The discrete nature of $n$ forms the basis of the search. For each valid integer $n$, the continuous domains for $r_c$ and $\\alpha$ are sampled uniformly. Specifically, we use $40$ sample points for $r_c$ across its range and, for each $r_c$, $40$ sample points for $\\alpha$ across its corresponding range.\n\nThe algorithm proceeds as follows for each test case:\n1.  Generate the sorted list of $5$-smooth integers $\\mathcal{N}$ in $[32, 128]$.\n2.  Initialize a tracking tuple for the optimal parameters, $(C_{\\mathrm{best}}, n_{\\mathrm{best}}, r_{c, \\mathrm{best}}, \\alpha_{\\mathrm{best}})$, with $C_{\\mathrm{best}}$ set to infinity. This structure is chosen to facilitate lexicographical comparison, which naturally implements the specified tie-breaking rule: prefer lower cost, then smaller $n$, then smaller $r_c$, then smaller $\\alpha$.\n3.  Iterate through each $n \\in \\mathcal{N}$. For each $n$, iterate through the $40$ samples of $r_c$. For each $(n, r_c)$ pair, iterate through the $40$ samples of $\\alpha$.\n4.  At each point $(\\alpha, r_c, n)$ in the search grid, calculate the total error $\\mathcal{E}_{\\mathrm{tot}}$.\n5.  Check for feasibility by comparing the calculated error to the tolerance: $\\mathcal{E}_{\\mathrm{tot}} \\le \\varepsilon$.\n6.  If the parameter set is feasible, calculate the total cost $C_{\\mathrm{tot}}$.\n7.  Compare the current feasible solution's lexicographical tuple $(C_{\\mathrm{tot}}, n, r_c, \\alpha)$ with the best-so-far tuple. If the current one is smaller, it becomes the new optimum, and its parameters and error are stored.\n8.  After searching the entire grid, if a feasible solution was found, the optimal parameters $(\\alpha^\\star, r_c^\\star, n^\\star)$, the minimal cost $C_{\\mathrm{tot}}^\\star$, and the corresponding error $\\mathcal{E}_{\\mathrm{tot}}^\\star$ are recorded. Otherwise, the case is marked as infeasible.\n9.  The final results for each test case are formatted and aggregated into a single list structure as required. Floating-point values are formatted to $6$ decimal places.\n\nThis brute-force search guarantees finding the optimal parameters within the discretized space. The output is a collection of these optimal settings for the provided suite of test systems.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Finds optimal PME parameters by performing a grid search for a series of test cases.\n    \"\"\"\n\n    # Define constants from the problem statement.\n    K_REAL = 1.0\n    K_RECIP = 1.0\n    C_PAIR = 1.0e-6\n    C_SPREAD = 2.0e-7\n    C_FFT = 5.0e-6\n    N_MIN = 32\n    N_MAX = 128\n    RC_MIN_VAL = 0.5\n    NUM_SAMPLES = 40\n\n    test_cases = [\n        (100000, 10.0, 0.30, 4, 1e-3),\n        (100000, 10.0, 0.30, 4, 3e-5),\n        (5000, 6.0, 0.50, 6, 5e-4),\n        (200000, 8.0, 0.25, 6, 1e-5),\n    ]\n\n    def generate_smooth_numbers(n_min, n_max):\n        \"\"\"Generates 5-smooth integers within a given range.\"\"\"\n        nums = {1}\n        q = [1]\n        head = 0\n        # Generate all 5-smooth numbers up to n_max\n        while head  len(q):\n            curr = q[head]\n            head += 1\n            for factor in [2, 3, 5]:\n                next_val = curr * factor\n                if next_val = n_max and next_val not in nums:\n                    nums.add(next_val)\n                    q.append(next_val)\n        # Filter by n_min and sort\n        return sorted([x for x in nums if x >= n_min])\n\n    n_smooth_list = generate_smooth_numbers(N_MIN, N_MAX)\n    \n    all_results = []\n    \n    for case in test_cases:\n        N, L, q_rms, p, epsilon = case\n        rho = N / (L**3)\n\n        best_params_tuple = (float('inf'), -1, -1.0, -1.0)  # (cost, n, rc, alpha)\n        best_error = -1.0\n        found_feasible = False\n\n        # Pre-calculate reciprocal space cost components which only depend on n and p\n        cost_recip_map = {}\n        for n_val in n_smooth_list:\n            c_spread = C_SPREAD * (p**3) * (n_val**3)\n            c_fft = C_FFT * (n_val**3) * np.log2(n_val**3)\n            cost_recip_map[n_val] = c_spread + c_fft\n\n        for n in n_smooth_list:\n            rc_max = 0.49 * L\n            rc_samples = np.linspace(RC_MIN_VAL, rc_max, NUM_SAMPLES)\n            \n            cost_recip = cost_recip_map[n]\n\n            for r_c in rc_samples:\n                alpha_min = 0.5 / r_c\n                alpha_max = 3.5 / r_c\n                alpha_samples = np.linspace(alpha_min, alpha_max, NUM_SAMPLES)\n\n                cost_real = C_PAIR * N * rho * r_c**3\n\n                common_err_factor = (q_rms**2) * np.sqrt(rho)\n                e_real_base = K_REAL * common_err_factor\n                e_recip_base = K_RECIP * common_err_factor * (n**(-p))\n                \n                for alpha in alpha_samples:\n                    # Error Calculation\n                    e_real_sq_arg = -((alpha * r_c)**2)\n                    e_real = e_real_base * np.exp(e_real_sq_arg)\n\n                    e_recip_sq_arg = -((np.pi * n) / (alpha * L))**2\n                    e_recip = e_recip_base * np.exp(e_recip_sq_arg)\n\n                    e_tot = np.sqrt(e_real**2 + e_recip**2)\n\n                    if e_tot = epsilon:\n                        found_feasible = True\n                        \n                        # Cost Calculation\n                        cost_tot = cost_real + cost_recip\n                        \n                        current_params_tuple = (cost_tot, n, r_c, alpha)\n                        if current_params_tuple  best_params_tuple:\n                            best_params_tuple = current_params_tuple\n                            best_error = e_tot\n        \n        if found_feasible:\n            opt_cost, opt_n, opt_rc, opt_alpha = best_params_tuple\n            result = [opt_alpha, opt_rc, opt_n, opt_cost, best_error, True]\n        else:\n            result = [-1, -1, -1, -1, -1, False]\n        \n        all_results.append(result)\n\n    # Format the final output string exactly as specified.\n    output_parts = []\n    for res in all_results:\n        if res[-1] is False:\n            output_parts.append(\"[-1,-1,-1,-1,-1,False]\")\n        else:\n            alpha, rc, n, cost, error, _ = res\n            part = (f\"[{alpha:.6f},{rc:.6f},{n},\"\n                    f\"{cost:.6f},{error:.6f},True]\")\n            output_parts.append(part)\n    \n    final_output_string = f\"[{','.join(output_parts)}]\"\n    print(final_output_string)\n\nsolve()\n\n```"
        },
        {
            "introduction": "Before committing computational resources to a long production run, it is essential to verify that the simulation is numerically stable. A common and powerful diagnostic is to run a short simulation in the microcanonical (NVE) ensemble, where total energy should be conserved. This practice  confronts a frequent issue: observing a systematic drift in the total energy, which signals underlying problems in the simulation setup. You will explore the theoretical reasons for this drift, rooted in errors from force calculations, constraint algorithms, and finite numerical precision, and then develop a systematic strategy for diagnosing and correcting the problem. This exercise hones the critical troubleshooting skills required to ensure the physical validity and reliability of your simulation results.",
            "id": "3438091",
            "problem": "A solvated peptide is simulated in the microcanonical ensemble (NVE) using a symplectic velocity-Verlet integrator with time step $2\\,\\mathrm{fs}$, holonomic constraints on bonds to hydrogen solved with the Linear Constraint Solver (LINCS), electrostatics via Particle-Mesh Ewald (PME), and a Verlet neighbor list on a Graphics Processing Unit (GPU). The total energy $E(t) = K(t) + U(t)$ is observed to increase approximately linearly at a rate of $+0.002\\,\\mathrm{kJ\\,mol^{-1}\\,ps^{-1}}$ over $100\\,\\mathrm{ps}$. No thermostat or barostat is active. Starting from Newton’s second law $m_i \\ddot{\\mathbf{r}}_i = \\mathbf{F}_i$ and the definition of a conservative force $\\mathbf{F}_i = -\\nabla_{\\mathbf{r}_i} U(\\mathbf{r})$, and recalling that for holonomic constraints $g_j(\\mathbf{r})=0$ enforced exactly the associated reaction forces do no work, justify how systematic errors in the computed forces and constraints can lead to a monotonic drift in $E(t)$ under NVE. Then, among the following options, select the one that most appropriately prioritizes diagnostic checks across constraints, PME tolerance, precision mode, and neighbor list settings, and proposes corrective parameter changes that are scientifically consistent with minimizing energy drift while preserving physical fidelity and computational stability.\n\nOptions:\n\nA. Prioritize neighbor list safety first: verify symmetric pair lists on the GPU, increase the neighbor list buffer $\\Delta r$ from $0.1\\,\\mathrm{nm}$ to $0.3\\,\\mathrm{nm}$, and reduce the neighbor list update period from every $10$ steps to every $1$ step to avoid missing interactions near the cutoff. Next, tighten PME accuracy by reducing the relative force error tolerance from $10^{-3}$ to $10^{-5}$ and increasing the Fast Fourier Transform (FFT) grid to maintain the target tolerance. Then, strengthen constraint enforcement by reducing the LINCS tolerance from $10^{-4}$ to $10^{-6}$ and increasing the maximum iterations (e.g., from $50$ to $100$); if constraint iterations saturate, consider reducing the time step to $1\\,\\mathrm{fs}$. Finally, increase arithmetic accuracy by switching to mixed precision with double-precision accumulations or full double precision for force and energy summations.\n\nB. Apply a weak thermostat to remove the drift while keeping NVE labels, loosen PME tolerance from $10^{-3}$ to $10^{-2}$ to improve performance, switch all calculations to single precision to maximize GPU throughput, and keep the neighbor list buffer and update period unchanged at $\\Delta r=0.1\\,\\mathrm{nm}$ and $20$ steps.\n\nC. Reduce the time step aggressively to $0.5\\,\\mathrm{fs}$, disable all constraints to avoid LINCS iterations, coarsen the PME grid to decrease computational cost, and leave the neighbor list parameters unchanged. Rely on smaller time steps to suppress the observed energy drift.\n\nD. Begin with constraint enforcement: reduce the LINCS tolerance from $10^{-4}$ to $10^{-6}$, increase maximum iterations, and consider switching from LINCS to the SHAKE algorithm. Next, adjust PME tolerance from $10^{-3}$ to $10^{-5}$. Then increase the neighbor list buffer $\\Delta r$ and update frequency to avoid pair omissions. Finally, improve numerical precision by using double precision for force accumulation while keeping the time step at $2\\,\\mathrm{fs}$.",
            "solution": "The problem requires a justification for the observed energy drift in a microcanonical (NVE) molecular dynamics simulation and the selection of the most appropriate strategy to diagnose and correct it.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- Ensemble: Microcanonical (NVE), no thermostat or barostat.\n- System: Solvated peptide.\n- Integrator: Symplectic velocity-Verlet.\n- Time step ($\\Delta t$): $2\\,\\mathrm{fs}$.\n- Constraints: Holonomic constraints on bonds involving hydrogen atoms, solved using the Linear Constraint Solver (LINCS).\n- Electrostatics: Particle-Mesh Ewald (PME).\n- Neighbor searching: Verlet neighbor list.\n- Hardware: Graphics Processing Unit (GPU).\n- Observation: Total energy $E(t) = K(t) + U(t)$ exhibits an approximately linear increase.\n- Rate of energy drift: $+0.002\\,\\mathrm{kJ\\,mol^{-1}\\,ps^{-1}}$.\n- Duration of observation: $100\\,\\mathrm{ps}$.\n- Foundational Equations: $m_i \\ddot{\\mathbf{r}}_i = \\mathbf{F}_i$, $\\mathbf{F}_i = -\\nabla_{\\mathbf{r}_i} U(\\mathbf{r})$, and the principle that reaction forces from exact holonomic constraints $g_j(\\mathbf{r})=0$ perform no work.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically grounded, well-posed, and objective. It describes a standard molecular dynamics simulation setup and a common numerical artifact (energy drift). The components mentioned (NVE, velocity-Verlet, LINCS, PME) are standard methods. The observed positive energy drift, or \"numerical heating,\" at the given rate is a realistic scenario for a simulation with a moderately large time step of $2\\,\\mathrm{fs}$. The question asks for a justification of this phenomenon based on first principles and an evaluation of corrective strategies, which is a standard task in computational science. The problem is self-contained and provides sufficient information to assess the proposed options based on established best practices in the field. No scientific, logical, or factual flaws are present.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n**Justification of Energy Drift**\n\nIn an ideal microcanonical ensemble simulation, the total energy $E(t)$ must be conserved. This conservation stems from integrating Newton's equations of motion, $m_i \\ddot{\\mathbf{r}}_i = \\mathbf{F}_i$, where the forces $\\mathbf{F}_i$ are conservative, i.e., they are the negative gradient of a potential energy function $U(\\mathbf{r})$, $\\mathbf{F}_i = -\\nabla_{\\mathbf{r}_i} U$. The time derivative of the total energy is\n$$ \\frac{dE}{dt} = \\frac{d}{dt} \\left( \\sum_i \\frac{1}{2} m_i \\dot{\\mathbf{r}}_i^2 + U(\\mathbf{r}) \\right) = \\sum_i (m_i \\dot{\\mathbf{r}}_i \\cdot \\ddot{\\mathbf{r}}_i + \\nabla_{\\mathbf{r}_i} U \\cdot \\dot{\\mathbf{r}}_i) $$\nSubstituting $m_i \\ddot{\\mathbf{r}}_i = \\mathbf{F}_i$ and $\\nabla_{\\mathbf{r}_i} U = -\\mathbf{F}_i$, we get:\n$$ \\frac{dE}{dt} = \\sum_i (\\dot{\\mathbf{r}}_i \\cdot \\mathbf{F}_i - \\mathbf{F}_i \\cdot \\dot{\\mathbf{r}}_i) = 0 $$\nNumerical simulations, however, introduce several approximations that break this exact conservation, leading to energy drift.\n\n$1$. **Discretization Error**: The velocity-Verlet integrator discretizes time with a finite step $\\Delta t$. Although it is symplectic, meaning it conserves a \"shadow\" Hamiltonian close to the true Hamiltonian, this property is broken by other sources of error, which can turn bounded energy oscillations into a systematic drift.\n\n$2$. **Errors in Force Calculation**: The computed forces are not perfectly conservative.\n    - **Neighbor Lists**: Short-range forces are typically calculated for pairs of particles within a cutoff radius. To avoid $\\mathcal{O}(N^2)$ complexity, a neighbor list is built and updated periodically. If the buffer zone ($\\Delta r$) is too small or the update frequency is too low, a particle may travel beyond the cutoff plus the buffer before the next update. This results in the sudden disappearance of an existing interaction or the sudden appearance of a new one. Such discontinuous changes in force are non-conservative and inject energy into the system. GPU implementations can sometimes have subtle bugs, like constructing non-symmetric pair lists (where the interaction of particle $i$ on $j$ is accounted for, but not $j$ on $i$), which violates Newton's third law and directly breaks energy and momentum conservation.\n    - **PME Electrostatics**: The PME method splits the calculation into a real-space part (subject to cutoffs, like neighbor lists) and a reciprocal-space part calculated via FFT on a grid. The accuracy is governed by a tolerance parameter that balances the errors between the two parts. Discretization of charge onto the grid and the finite Fourier series used in reciprocal space introduce errors, meaning the resulting electrostatic force is not the exact gradient of the Ewald potential. A loose tolerance leads to larger non-conservative force components and consequently, energy drift.\n\n$3$. **Errors in Constraint Enforcement**: Holonomic constraints, such as fixing bond lengths with LINCS, are solved iteratively to a finite tolerance $\\epsilon_{LINCS}$. The constraint conditions $g_j(\\mathbf{r})=0$ are therefore not satisfied exactly. The constraint forces applied are those required to satisfy the constraints approximately. While ideal constraint forces do no work, these approximate forces can perform a small amount of net work over many time steps, leading to a systematic change in the total energy. With a relatively large time step of $2\\,\\mathrm{fs}$, the constraint algorithm is put under significant stress, and failure to converge within the maximum number of iterations or a loose tolerance are common sources of energy drift.\n\n$4$. **Finite Precision Arithmetic**: All calculations are performed with finite-precision floating-point numbers (e.g., single or double). Round-off errors accumulate, especially in large summations like force and energy calculations. This progressively degrades the time-reversibility and symplecticity of the integration, contributing to long-term energy drift.\n\nThe observed linear increase in energy is characteristic of these systematic errors combining to act as a source of \"numerical heating.\"\n\n**Option-by-Option Analysis**\n\nThe goal is to find the most appropriate prioritization of diagnostic checks and fixes. An effective strategy proceeds from the most likely and impactful sources of error to less common ones, and favors less computationally expensive fixes over more expensive ones.\n\n**A. Prioritize neighbor list safety first: verify symmetric pair lists on the GPU, increase the neighbor list buffer $\\Delta r$ from $0.1\\,\\mathrm{nm}$ to $0.3\\,\\mathrm{nm}$, and reduce the neighbor list update period from every $10$ steps to every $1$ step to avoid missing interactions near the cutoff. Next, tighten PME accuracy by reducing the relative force error tolerance from $10^{-3}$ to $10^{-5}$ and increasing the Fast Fourier Transform (FFT) grid to maintain the target tolerance. Then, strengthen constraint enforcement by reducing the LINCS tolerance from $10^{-4}$ to $10^{-6}$ and increasing the maximum iterations (e.g., from $50$ to $100$); if constraint iterations saturate, consider reducing the time step to $1\\,\\mathrm{fs}$. Finally, increase arithmetic accuracy by switching to mixed precision with double-precision accumulations or full double precision for force and energy summations.**\n\nThis option presents a highly logical and systematic diagnostic hierarchy.\n$1$. **Neighbor List:** It correctly prioritizes ensuring that the force calculation itself is not fundamentally flawed by missing interactions. This is a primary cause of instability and energy drift. Checking for GPU-specific implementation details like symmetric pair lists is an expert-level diagnostic step. The proposed parameter changes, while aggressive, are guaranteed to make the neighbor searching safe, thus isolating this as a potential problem.\n$2$. **PME:** The next step is to improve the accuracy of the long-range force calculation, another major source of non-conservative forces.\n$3$. **Constraints:** It then addresses the accuracy of the constraint algorithm, which is crucial for stability with a $2\\,\\mathrm{fs}$ time step. It correctly provides a contingent action (reduce $\\Delta t$) if the constraint algorithm fails to converge.\n$4$. **Precision:** Finally, it addresses the more subtle issue of numerical precision, which is typically the last resort.\nThe prioritization (correctness of force calculation - accuracy of forces - accuracy of integration - numerical precision) is scientifically sound and reflects best practices. The proposed changes are appropriate for reducing energy drift.\n\n**Verdict: Correct**\n\n**B. Apply a weak thermostat to remove the drift while keeping NVE labels, loosen PME tolerance from $10^{-3}$ to $10^{-2}$ to improve performance, switch all calculations to single precision to maximize GPU throughput, and keep the neighbor list buffer and update period unchanged at $\\Delta r=0.1\\,\\mathrm{nm}$ and $20$ steps.**\n\nThis option is fundamentally flawed.\n$1$. Applying a thermostat masks the numerical instability instead of fixing it. It is deceptive to label the simulation as NVE while using a thermostat; the goal is to fix the underlying issues causing the NVE simulation to be non-conservative.\n$2$. Loosening PME tolerance from $10^{-3}$ to $10^{-2}$ would *increase* the error in the forces and worsen the energy drift.\n$3$. Switching all calculations to single precision, if not already used, is more likely to increase round-off errors and exacerbate drift, not reduce it.\n$4$. Ignoring the neighbor list, a primary suspect, is poor practice.\n\n**Verdict: Incorrect**\n\n**C. Reduce the time step aggressively to $0.5\\,\\mathrm{fs}$, disable all constraints to avoid LINCS iterations, coarsen the PME grid to decrease computational cost, and leave the neighbor list parameters unchanged. Rely on smaller time steps to suppress the observed energy drift.**\n\nThis option is also deeply flawed.\n$1$. Reducing the time step to $0.5\\,\\mathrm{fs}$ is a brute-force method that should be a last resort due to its high computational cost ($4 \\times$ increase). While it would likely reduce the drift, it doesn't diagnose the actual problem.\n$2$. Disabling constraints fundamentally alters the physical model being simulated. While this necessitates a smaller time step, it is not a \"fix\" but rather a change of the research question.\n$3$. Coarsening the PME grid would decrease accuracy and *increase* energy drift, directly contradicting the goal.\n$4$. Leaving the neighbor list parameters unchanged ignores a key potential source of error.\n\n**Verdict: Incorrect**\n\n**D. Begin with constraint enforcement: reduce the LINCS tolerance from $10^{-4}$ to $10^{-6}$, increase maximum iterations, and consider switching from LINCS to the SHAKE algorithm. Next, adjust PME tolerance from $10^{-3}$ to $10^{-5}$. Then increase the neighbor list buffer $\\Delta r$ and update frequency to avoid pair omissions. Finally, improve numerical precision by using double precision for force accumulation while keeping the time step at $2\\,\\mathrm{fs}$.**\n\nThis option proposes a scientifically sound set of actions, similar to option A. However, its prioritization is less optimal. It prioritizes constraints over neighbor lists. While constraint errors are a very likely culprit with a $2\\,\\mathrm{fs}$ time step, a flawed neighbor list means the forces being fed to the integrator and constraint algorithm are incorrect to begin with. It is more logical to first ensure that all interactions are being captured correctly (neighbor list safety) before fine-tuning the accuracy of the force components (PME) and the algorithms that use those forces (integrator/constraints). Therefore, the hierarchy in option A, which starts with neighbor list safety, is superior.\n\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Once your simulation is properly parameterized and verified to be stable, the final planning question is: how long does the production run need to be? The answer depends on the scientific question, specifically the desired precision for a calculated observable. This exercise  delves into the statistical foundation of this question, revealing that the error in a time-averaged property depends not just on simulation length, but critically on the observable's intrinsic variance $\\sigma_A^2$ and its integrated autocorrelation time $\\tau_{\\mathrm{int}}$. By working through the derivation, you will understand why successive configurations from an MD trajectory are not statistically independent and learn how to use this insight to estimate the required simulation time to achieve a target precision, ensuring your computational effort yields statistically meaningful results.",
            "id": "3438067",
            "problem": "A production Molecular Dynamics (MD) run is planned to estimate the time average of an observable $A(t)$ (for example, instantaneous potential energy per mole) with a prescribed precision. A short pilot simulation has been performed and, after discarding equilibration, the time series $A(t)$ appears stationary and ergodic. From the pilot data, the following estimates were obtained: the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ and the variance $\\sigma_{A}^{2} \\equiv \\langle (A - \\langle A \\rangle)^{2} \\rangle$. The production trajectory will be run long enough that the total production time $T$ satisfies $T \\gg \\tau_{\\mathrm{int}}$, and the sampling is uniform in time.\n\nStarting from the definition of the time average $\\bar{A}_{T} \\equiv \\frac{1}{T} \\int_{0}^{T} A(t) \\, dt$ and the autocovariance function $C_{A}(t) \\equiv \\langle (A(0) - \\langle A \\rangle)(A(t) - \\langle A \\rangle) \\rangle$ for a stationary process, derive an expression for the asymptotic variance of $\\bar{A}_{T}$ in terms of $\\sigma_{A}^{2}$, $\\tau_{\\mathrm{int}}$, and $T$. Then use this expression to determine the required production time $T$ to achieve a target standard error $\\sigma_{\\bar{A}}$ on the estimator $\\bar{A}_{T}$.\n\nFor planning purposes, use the pilot estimates $\\tau_{\\mathrm{int}} = 25\\,\\mathrm{ps}$ and $\\sigma_{A}^{2} = 400\\,(\\mathrm{kJ}\\,\\mathrm{mol}^{-1})^{2}$, and require a target standard error $\\sigma_{\\bar{A}} = 1\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1}$. Express the final required production time $T$ in nanoseconds, and round your answer to three significant figures.",
            "solution": "The problem asks for a derivation of the variance of a time-averaged observable from a Molecular Dynamics (MD) simulation and subsequently a calculation of the required simulation length to achieve a target precision. The problem is scientifically grounded, well-posed, and contains all necessary information. We may proceed with the solution.\n\nFirst, we derive the expression for the variance of the time average, $\\bar{A}_{T}$. The time average of an observable $A(t)$ over a total time $T$ is defined as:\n$$\n\\bar{A}_{T} \\equiv \\frac{1}{T} \\int_{0}^{T} A(t) \\, dt\n$$\nThe variance of this estimator, denoted as $\\sigma_{\\bar{A}_{T}}^{2}$, is given by $\\sigma_{\\bar{A}_{T}}^{2} = \\langle (\\bar{A}_{T} - \\langle \\bar{A}_{T} \\rangle)^{2} \\rangle$, where $\\langle \\cdot \\rangle$ denotes an ensemble average. For a stationary process, the ensemble average $\\langle A(t) \\rangle$ is independent of time and equals the true mean, which we denote as $\\langle A \\rangle$. The ensemble average of the estimator $\\bar{A}_{T}$ is:\n$$\n\\langle \\bar{A}_{T} \\rangle = \\left\\langle \\frac{1}{T} \\int_{0}^{T} A(t) \\, dt \\right\\rangle = \\frac{1}{T} \\int_{0}^{T} \\langle A(t) \\rangle \\, dt = \\frac{1}{T} \\int_{0}^{T} \\langle A \\rangle \\, dt = \\frac{1}{T} \\langle A \\rangle [t]_{0}^{T} = \\langle A \\rangle\n$$\nThis shows that $\\bar{A}_{T}$ is an unbiased estimator of $\\langle A \\rangle$. The variance can therefore be written as:\n$$\n\\sigma_{\\bar{A}_{T}}^{2} = \\langle (\\bar{A}_{T} - \\langle A \\rangle)^{2} \\rangle\n$$\nSubstituting the definition of $\\bar{A}_{T}$ and introducing a fluctuation term $\\delta A(t) = A(t) - \\langle A \\rangle$:\n$$\n\\sigma_{\\bar{A}_{T}}^{2} = \\left\\langle \\left( \\frac{1}{T} \\int_{0}^{T} A(t) \\, dt - \\langle A \\rangle \\right)^{2} \\right\\rangle = \\left\\langle \\left( \\frac{1}{T} \\int_{0}^{T} (A(t) - \\langle A \\rangle) \\, dt \\right)^{2} \\right\\rangle = \\frac{1}{T^{2}} \\left\\langle \\left( \\int_{0}^{T} \\delta A(t) \\, dt \\right)^{2} \\right\\rangle\n$$\nWe can write the squared integral as a double integral:\n$$\n\\sigma_{\\bar{A}_{T}}^{2} = \\frac{1}{T^{2}} \\left\\langle \\int_{0}^{T} \\delta A(t) \\, dt \\int_{0}^{T} \\delta A(t') \\, dt' \\right\\rangle\n$$\nBy linearity of the expectation operator, we can move it inside the integrals:\n$$\n\\sigma_{\\bar{A}_{T}}^{2} = \\frac{1}{T^{2}} \\int_{0}^{T} dt \\int_{0}^{T} dt' \\, \\langle \\delta A(t) \\delta A(t') \\rangle\n$$\nFor a stationary process, the correlation $\\langle \\delta A(t) \\delta A(t') \\rangle$ depends only on the time difference, $|t' - t|$. This is precisely the definition of the autocovariance function, $C_{A}(\\tau) = \\langle \\delta A(0) \\delta A(\\tau) \\rangle$. Thus, $\\langle \\delta A(t) \\delta A(t') \\rangle = C_{A}(t' - t)$.\n$$\n\\sigma_{\\bar{A}_{T}}^{2} = \\frac{1}{T^{2}} \\int_{0}^{T} dt \\int_{0}^{T} dt' \\, C_{A}(t' - t)\n$$\nThis double integral over a square domain can be simplified. Using the identity $\\int_{0}^{L} \\int_{0}^{L} f(|x-y|) \\, dy \\, dx = 2 \\int_{0}^{L} (L-z) f(z) \\, dz$ and noting that the autocovariance function is even, $C_{A}(\\tau) = C_{A}(-\\tau)$, we have:\n$$\n\\sigma_{\\bar{A}_{T}}^{2} = \\frac{2}{T^{2}} \\int_{0}^{T} (T - \\tau) C_{A}(\\tau) \\, d\\tau = \\frac{2}{T} \\int_{0}^{T} \\left(1 - \\frac{\\tau}{T}\\right) C_{A}(\\tau) \\, d\\tau\n$$\nThe problem states that the total production time $T$ is much larger than the integrated autocorrelation time, $T \\gg \\tau_{\\mathrm{int}}$. The autocovariance $C_{A}(\\tau)$ decays to zero on a time scale characterized by $\\tau_{\\mathrm{int}}$. Therefore, for the significant part of the integration range where $C_{A}(\\tau)$ is non-zero, we have $\\tau \\ll T$. This allows two approximations:\n1.  The term $(1 - \\frac{\\tau}{T}) \\approx 1$.\n2.  The upper integration limit can be extended from $T$ to $\\infty$ with negligible error.\nApplying these approximations, we obtain the asymptotic variance:\n$$\n\\sigma_{\\bar{A}_{T}}^{2} \\approx \\frac{2}{T} \\int_{0}^{\\infty} C_{A}(\\tau) \\, d\\tau\n$$\nThe problem provides definitions for the variance $\\sigma_{A}^{2}$ and the integrated autocorrelation time $\\tau_{\\mathrm{int}}$. The variance of the observable $A$ is $\\sigma_{A}^{2} = C_{A}(0)$. The standard definition of the integrated autocorrelation time is the integral of the *normalized* autocorrelation function:\n$$\n\\tau_{\\mathrm{int}} = \\int_{0}^{\\infty} \\frac{C_{A}(\\tau)}{C_{A}(0)} \\, d\\tau = \\frac{1}{\\sigma_{A}^{2}} \\int_{0}^{\\infty} C_{A}(\\tau) \\, d\\tau\n$$\nFrom this, we find $\\int_{0}^{\\infty} C_{A}(\\tau) \\, d\\tau = \\sigma_{A}^{2} \\tau_{\\mathrm{int}}$. Substituting this into our expression for the variance of the mean gives the desired relationship:\n$$\n\\sigma_{\\bar{A}_{T}}^{2} \\approx \\frac{2 \\sigma_{A}^{2} \\tau_{\\mathrm{int}}}{T}\n$$\nNext, we use this expression to determine the required production time $T$. The target standard error on the estimator is $\\sigma_{\\bar{A}}$, which is the square root of the variance, $\\sigma_{\\bar{A}} = \\sqrt{\\sigma_{\\bar{A}_{T}}^{2}}$. Squaring both sides gives $\\sigma_{\\bar{A}}^{2} = \\sigma_{\\bar{A}_{T}}^{2}$. We can now solve for $T$:\n$$\nT \\approx \\frac{2 \\sigma_{A}^{2} \\tau_{\\mathrm{int}}}{\\sigma_{\\bar{A}}^{2}}\n$$\nWe are given the following pilot estimates and target precision:\n- $\\tau_{\\mathrm{int}} = 25\\,\\mathrm{ps}$\n- $\\sigma_{A}^{2} = 400\\,(\\mathrm{kJ}\\,\\mathrm{mol}^{-1})^{2}$\n- $\\sigma_{\\bar{A}} = 1\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1}$, which means $\\sigma_{\\bar{A}}^{2} = (1\\,\\mathrm{kJ}\\,\\mathrm{mol}^{-1})^{2} = 1\\,(\\mathrm{kJ}\\,\\mathrm{mol}^{-1})^{2}$.\n\nSubstituting these values into the expression for $T$:\n$$\nT \\approx \\frac{2 \\times 400\\,(\\mathrm{kJ}\\,\\mathrm{mol}^{-1})^{2} \\times 25\\,\\mathrm{ps}}{1\\,(\\mathrm{kJ}\\,\\mathrm{mol}^{-1})^{2}}\n$$\nThe units of $(\\mathrm{kJ}\\,\\mathrm{mol}^{-1})^{2}$ cancel, leaving the time in picoseconds:\n$$\nT \\approx (2 \\times 400 \\times 25)\\,\\mathrm{ps} = (800 \\times 25)\\,\\mathrm{ps} = 20000\\,\\mathrm{ps}\n$$\nThe problem requires the answer in nanoseconds (ns). Since $1\\,\\mathrm{ns} = 1000\\,\\mathrm{ps}$:\n$$\nT \\approx 20000\\,\\mathrm{ps} \\times \\frac{1\\,\\mathrm{ns}}{1000\\,\\mathrm{ps}} = 20\\,\\mathrm{ns}\n$$\nFinally, we must round the answer to three significant figures.\n$$\nT = 20.0\\,\\mathrm{ns}\n$$\nAs a consistency check, our result $T = 20000\\,\\mathrm{ps}$ is indeed much larger than $\\tau_{\\mathrm{int}} = 25\\,\\mathrm{ps}$ (by a factor of $800$), which validates the asymptotic approximation used in the derivation.",
            "answer": "$$\\boxed{20.0}$$"
        }
    ]
}