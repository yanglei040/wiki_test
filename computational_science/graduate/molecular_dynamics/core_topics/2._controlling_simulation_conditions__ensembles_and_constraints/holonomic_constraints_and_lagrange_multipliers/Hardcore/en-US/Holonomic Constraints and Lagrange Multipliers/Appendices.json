{
    "hands_on_practices": [
        {
            "introduction": "Algorithms like SHAKE are powerful tools for enforcing holonomic constraints, but it's crucial to connect their operations to the underlying physics. This exercise challenges you to reverse-engineer the process: starting from the position corrections applied by the algorithm, you will deduce the instantaneous constraint force that was implicitly active. This practice illuminates the direct relationship between algorithmic steps and the physical forces governed by the Lagrange multiplier formalism .",
            "id": "2453554",
            "problem": "You have run a classical molecular dynamics simulation using the velocity Verlet integrator with the SHAKE constraint algorithm to enforce a holonomic bond-length constraint between atoms $i$ and $j$, expressed as $\\sigma(\\mathbf{r}) = 0$ with $\\sigma(\\mathbf{r}) = \\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert^2 - d^2$. At each time step, the integrator first computes unconstrained positions and then applies SHAKE position corrections $\\Delta \\mathbf{r}_i$ and $\\Delta \\mathbf{r}_j$ so that the constraint is satisfied at the end of the step. The simulation saved the atom masses $m_i$ and $m_j$, the time step $\\Delta t$, and the SHAKE position corrections $\\Delta \\mathbf{r}_i$ and $\\Delta \\mathbf{r}_j$ for each step, but did not store any Lagrange multipliers. Without rerunning the simulation, how can you reconstruct the instantaneous constraint force vector exerted on atom $i$ by the constraint at a given step?\n\nChoose the best expression or procedure below. Assume the standard formulation of holonomic constraints with Lagrange multipliers and that forces are approximated as constant over a single time step in the velocity Verlet position update.\n\nA. Use the recorded SHAKE position correction and compute $\\mathbf{F}_i^{\\mathrm{c}}$ directly as $\\mathbf{F}_i^{\\mathrm{c}} = \\dfrac{2 m_i}{\\Delta t^2}\\,\\Delta \\mathbf{r}_i$.\n\nB. Infer an effective spring constant $k$ by matching the distribution of bond-length deviations to a harmonic model, then compute $\\mathbf{F}_i^{\\mathrm{c}} = -k\\left(\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert - d\\right)\\,\\dfrac{\\mathbf{r}_i - \\mathbf{r}_j}{\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert}$.\n\nC. It is impossible to reconstruct $\\mathbf{F}_i^{\\mathrm{c}}$ without rerunning the trajectory while explicitly outputting Lagrange multipliers.\n\nD. Project the unconstrained interatomic force onto the bond direction and negate it: $\\mathbf{F}_i^{\\mathrm{c}} = -\\left[\\mathbf{F}_i \\cdot \\dfrac{\\mathbf{r}_i - \\mathbf{r}_j}{\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert}\\right]\\dfrac{\\mathbf{r}_i - \\mathbf{r}_j}{\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert}$, because the constraint cancels the component that would change the bond length.\n\nOnly one option is correct. Provide your reasoning from fundamental principles to justify your choice.",
            "solution": "The problem statement has been validated as scientifically grounded, well-posed, and objective. It provides a solvable scenario rooted in the standard principles of computational chemistry.\n\nThe objective is to find an expression for the instantaneous constraint force, $\\mathbf{F}_i^{\\mathrm{c}}$, on atom $i$ at a given time step, using the provided data: atom mass $m_i$, time step $\\Delta t$, and the SHAKE position correction $\\Delta \\mathbf{r}_i$.\n\nWe begin with the Lagrangian equations of motion for a constrained system. For atom $i$, the total force is the sum of the unconstrained force $\\mathbf{F}_i^{\\text{un}}$ (derived from the potential energy function) and the constraint force $\\mathbf{F}_i^{\\mathrm{c}}$:\n$$ m_i \\ddot{\\mathbf{r}}_i = \\mathbf{F}_i(t) = \\mathbf{F}_i^{\\text{un}}(t) + \\mathbf{F}_i^{\\mathrm{c}}(t) $$\nThe velocity Verlet algorithm, which is a second-order accurate integrator, approximates the position of atom $i$ at time $t+\\Delta t$ based on its state at time $t$. The position update formula is derived from a Taylor series expansion of the position $\\mathbf{r}_i(t)$:\n$$ \\mathbf{r}_i(t+\\Delta t) = \\mathbf{r}_i(t) + \\mathbf{v}_i(t)\\Delta t + \\frac{1}{2}\\mathbf{a}_i(t)(\\Delta t)^2 + \\mathcal{O}((\\Delta t)^3) $$\nwhere $\\mathbf{v}_i(t)$ is the velocity and $\\mathbf{a}_i(t) = \\ddot{\\mathbf{r}}_i(t)$ is the acceleration at time $t$. Substituting the equation of motion, we have $\\mathbf{a}_i(t) = \\mathbf{F}_i(t)/m_i$. The Verlet algorithm truncates the series, using the force at time $t$ as constant over the step $\\Delta t$ for the position update. This is consistent with the assumption provided in the problem statement.\n$$ \\mathbf{r}_i(t+\\Delta t) = \\mathbf{r}_i(t) + \\mathbf{v}_i(t)\\Delta t + \\frac{(\\Delta t)^2}{2m_i}\\mathbf{F}_i(t) $$\nSubstituting the partitioned force $\\mathbf{F}_i(t) = \\mathbf{F}_i^{\\text{un}}(t) + \\mathbf{F}_i^{\\mathrm{c}}(t)$:\n$$ \\mathbf{r}_i(t+\\Delta t) = \\mathbf{r}_i(t) + \\mathbf{v}_i(t)\\Delta t + \\frac{(\\Delta t)^2}{2m_i} \\left( \\mathbf{F}_i^{\\text{un}}(t) + \\mathbf{F}_i^{\\mathrm{c}}(t) \\right) $$\nThis equation can be separated into two parts, which reflects the procedure of the SHAKE algorithm:\n$$ \\mathbf{r}_i(t+\\Delta t) = \\left[ \\mathbf{r}_i(t) + \\mathbf{v}_i(t)\\Delta t + \\frac{(\\Delta t)^2}{2m_i} \\mathbf{F}_i^{\\text{un}}(t) \\right] + \\left[ \\frac{(\\Delta t)^2}{2m_i} \\mathbf{F}_i^{\\mathrm{c}}(t) \\right] $$\nThe first bracketed term represents the updated position of atom $i$ considering only the unconstrained forces. This is precisely what the problem describes as the \"unconstrained positions\" computed by the integrator, let us call them $\\mathbf{r}'_i(t+\\Delta t)$.\n$$ \\mathbf{r}'_i(t+\\Delta t) = \\mathbf{r}_i(t) + \\mathbf{v}_i(t)\\Delta t + \\frac{(\\Delta t)^2}{2m_i} \\mathbf{F}_i^{\\text{un}}(t) $$\nThe second bracketed term represents the additional displacement due to the constraint force, which is applied to restore the bond length to its constrained value $d$. The problem defines this displacement as the \"SHAKE position correction\", $\\Delta \\mathbf{r}_i$.\n$$ \\Delta \\mathbf{r}_i = \\mathbf{r}_i(t+\\Delta t) - \\mathbf{r}'_i(t+\\Delta t) = \\frac{(\\Delta t)^2}{2m_i} \\mathbf{F}_i^{\\mathrm{c}}(t) $$\nThis equation establishes a direct relationship between the constraint force at the beginning of the step, $\\mathbf{F}_i^{\\mathrm{c}}(t)$, and the position correction $\\Delta \\mathbf{r}_i$ applied during that step. We can rearrange this equation to solve for the constraint force:\n$$ \\mathbf{F}_i^{\\mathrm{c}}(t) = \\frac{2 m_i}{(\\Delta t)^2} \\Delta \\mathbf{r}_i $$\nSince the values for $m_i$, $\\Delta t$, and $\\Delta \\mathbf{r}_i$ are saved from the simulation, it is possible to reconstruct the instantaneous constraint force $\\mathbf{F}_i^{\\mathrm{c}}$ at each time step.\n\nNow, we evaluate each of the given options.\n\nA. Use the recorded SHAKE position correction and compute $\\mathbf{F}_i^{\\mathrm{c}}$ directly as $\\mathbf{F}_i^{\\mathrm{c}} = \\dfrac{2 m_i}{\\Delta t^2}\\,\\Delta \\mathbf{r}_i$.\nThis expression is identical to the one derived from the fundamental principles of the velocity Verlet integrator and the SHAKE algorithm. The derivation shows that the position correction $\\Delta \\mathbf{r}_i$ is the integrated effect of the constraint force $\\mathbf{F}_i^{\\mathrm{c}}$ over the time step $\\Delta t$, within the second-order approximation of the integrator.\nVerdict: **Correct**.\n\nB. Infer an effective spring constant $k$ by matching the distribution of bond-length deviations to a harmonic model, then compute $\\mathbf{F}_i^{\\mathrm{c}} = -k\\left(\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert - d\\right)\\,\\dfrac{\\mathbf{r}_i - \\mathbf{r}_j}{\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert}$.\nThis option is fundamentally incorrect. The SHAKE algorithm enforces a *rigid* constraint, meaning the bond length is fixed (ideally, $\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert = d$). This option proposes to replace the rigid constraint model with a flexible harmonic spring model. The force in a harmonic model is proportional to the deviation from the equilibrium distance. In a perfectly constrained system, this deviation is zero, which would incorrectly imply zero force. The constraint force is not a function of bond length deviation; it is the force required to counteract all other influences (unconstrained forces and inertial effects) that would alter the bond length. This method describes an approximation, not a reconstruction of the original simulation's constraint force.\nVerdict: **Incorrect**.\n\nC. It is impossible to reconstruct $\\mathbf{F}_i^{\\mathrm{c}}$ without rerunning the trajectory while explicitly outputting Lagrange multipliers.\nThis statement is proven false by the derivation for option A. The SHAKE position correction $\\Delta \\mathbf{r}_i$ is a direct consequence of the constraint force (which is determined by the Lagrange multiplier $\\lambda$). Having $\\Delta \\mathbf{r}_i$ allows us to calculate $\\mathbf{F}_i^{\\mathrm{c}}$ directly, without needing to know the intermediate value of $\\lambda$. The necessary information is encoded in the position corrections.\nVerdict: **Incorrect**.\n\nD. Project the unconstrained interatomic force onto the bond direction and negate it: $\\mathbf{F}_i^{\\mathrm{c}} = -\\left[\\mathbf{F}_i \\cdot \\dfrac{\\mathbf{r}_i - \\mathbf{r}_j}{\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert}\\right]\\dfrac{\\mathbf{r}_i - \\mathbf{r}_j}{\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert}$, because the constraint cancels the component that would change the bond length.\nThis is a common but incorrect simplification. The constraint force must counteract not only the component of the unconstrained force along the bond, but also inertial effects. The correct Lagrange multiplier expression, derived from the condition $\\ddot\\sigma=0$, shows that the constraint force depends on the unconstrained forces on *both* atoms ($i$ and $j$), their masses ($m_i, m_j$), and a velocity-dependent term $\\left\\lVert \\mathbf{v}_i - \\mathbf{v}_j \\right\\rVert^2$ which acts like a centrifugal force. Option D neglects the contribution from atom $j$, the masses, and the velocity term entirely.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "At the heart of enforcing constraints via Lagrange multipliers lies the solution of a linear system for the multipliers, $\\lambda$. This practice guides you through the derivation of this core system, known as the Schur complement form $(C M^{-1} C^\\top) \\lambda = \\dot{\\eta}$, from first principles. By implementing and comparing both a direct solver and the iterative Conjugate Gradient method, you will gain practical experience with the numerical engine that powers large-scale constrained simulations .",
            "id": "3416388",
            "problem": "In a system of constrained molecular dynamics with holonomic constraints, let there be $N$ particles with positions $q \\in \\mathbb{R}^{3N}$, velocities $v \\in \\mathbb{R}^{3N}$, and a diagonal mass matrix $M \\in \\mathbb{R}^{3N \\times 3N}$, with $M = \\mathrm{diag}(m_1 I_3, m_2 I_3, \\dots, m_N I_3)$, where $m_i > 0$ and $I_3$ is the $3 \\times 3$ identity matrix. Consider a set of $m$ holonomic constraints of the form $g_k(q) = 0$ with Jacobian rows $C_k \\in \\mathbb{R}^{1 \\times 3N}$, assembled into the constraint Jacobian $C \\in \\mathbb{R}^{m \\times 3N}$. To enforce or project velocity-level constraints of the form $C \\Delta v = \\dot{\\eta}$ using Lagrange multipliers, a velocity correction of the form $\\Delta v = M^{-1} C^\\top \\lambda$ is applied, where $\\lambda \\in \\mathbb{R}^{m}$ are the Lagrange multipliers. Eliminating $\\Delta v$ leads to the Schur complement linear system\n$$\n(C M^{-1} C^\\top) \\lambda = \\dot{\\eta}.\n$$\nStarting from Newton’s second law and the definition of holonomic constraints with Lagrange multipliers, derive the above Schur complement system at the velocity level. Then implement a program that, for a given test suite, constructs $C$ from pairwise distance constraints, assembles $S = C M^{-1} C^\\top$, and solves for $\\lambda$ using both a direct dense linear solver and an iterative Conjugate Gradient method for symmetric positive definite (SPD) matrices. For nearly singular cases, use Tikhonov regularization by solving $(S + \\varepsilon I)\\lambda = \\dot{\\eta}$ with a given $\\varepsilon \\ge 0$.\n\nAll variables are dimensionless in this problem. Angles are not used. No physical units are required.\n\nConstraint construction for pairwise distance constraints: for a constraint between particles $i$ and $j$ with positions $r_i, r_j \\in \\mathbb{R}^3$, define $d_{ij} = r_j - r_i \\in \\mathbb{R}^3$. The corresponding Jacobian row $C_k$ is zero everywhere except the $3$-block for particle $i$ and particle $j$, where it is $-d_{ij}^\\top$ for particle $i$ and $+d_{ij}^\\top$ for particle $j$. This choice corresponds to the time derivative of the squared distance constraint and yields velocity-level constraints proportional to $d_{ij} \\cdot (v_j - v_i)$.\n\nYou must:\n- Derive the Schur complement system $(C M^{-1} C^\\top) \\lambda = \\dot{\\eta}$ from first principles.\n- Implement two solvers for $\\lambda$: a direct dense solver and an iterative Conjugate Gradient solver with stopping criterion $\\lVert r_k \\rVert_2 \\le \\max(\\tau_{\\mathrm{rel}} \\lVert \\dot{\\eta} \\rVert_2, \\tau_{\\mathrm{abs}})$, where $r_k$ is the residual at iteration $k$. Use $\\tau_{\\mathrm{rel}} = 10^{-12}$ and $\\tau_{\\mathrm{abs}} = 10^{-14}$, and a maximum of $10^4$ iterations.\n- For each test case below, construct $C$ from the positions and constraint pairs, assemble $S = C M^{-1} C^\\top$, form $A = S + \\varepsilon I$, solve $A \\lambda = \\dot{\\eta}$ by both methods, and report:\n  - The infinity norm of the difference between the two solutions, $\\lVert \\lambda_{\\mathrm{direct}} - \\lambda_{\\mathrm{CG}} \\rVert_{\\infty}$.\n  - The $2$-norm residuals $\\lVert A \\lambda_{\\mathrm{direct}} - \\dot{\\eta} \\rVert_2$ and $\\lVert A \\lambda_{\\mathrm{CG}} - \\dot{\\eta} \\rVert_2$.\n\nTest suite (all indices are $1$-based in the description below; implement with $0$-based indices as needed):\n- Case $1$ (happy path, single constraint):\n  - $N = 2$, $m = 1$.\n  - Masses: $[12, 16]$.\n  - Positions: $r_1 = (0, 0, 0)$, $r_2 = (1, 0, 0)$.\n  - Constraints: $(1, 2)$.\n  - Right-hand side $\\dot{\\eta} = [0.1]$.\n  - Regularization $\\varepsilon = 0$.\n- Case $2$ (multiple constraints, chain):\n  - $N = 3$, $m = 2$.\n  - Masses: $[12, 14, 12]$.\n  - Positions: $r_1 = (0, 0, 0)$, $r_2 = (1, 0, 0)$, $r_3 = (2, 0, 0)$.\n  - Constraints: $(1, 2)$ and $(2, 3)$.\n  - Right-hand side $\\dot{\\eta} = [0, 0.05]$.\n  - Regularization $\\varepsilon = 0$.\n- Case $3$ (nearly redundant constraints, regularized):\n  - $N = 3$, $m = 2$.\n  - Masses: $[12, 12, 12]$.\n  - Positions: $r_1 = (0, 0, 0)$, $r_2 = (1, 0, 0)$, $r_3 = (1, 10^{-6}, 0)$.\n  - Constraints: $(1, 2)$ and $(1, 3)$.\n  - Right-hand side $\\dot{\\eta} = [0.01, 0.01]$.\n  - Regularization $\\varepsilon = 10^{-12}$.\n- Case $4$ (boundary case, zero right-hand side, extreme mass):\n  - $N = 2$, $m = 1$.\n  - Masses: $[10^{-6}, 10]$.\n  - Positions: $r_1 = (0, 0, 0)$, $r_2 = (0.5, 0, 0)$.\n  - Constraints: $(1, 2)$.\n  - Right-hand side $\\dot{\\eta} = [0]$.\n  - Regularization $\\varepsilon = 0$.\n- Case $5$ (extreme mass ratios, chain of constraints):\n  - $N = 4$, $m = 3$.\n  - Masses: $[100, 0.1, 0.1, 100]$.\n  - Positions: $r_1 = (0, 0, 0)$, $r_2 = (1, 0, 0)$, $r_3 = (2, 0, 0)$, $r_4 = (3, 0, 0)$.\n  - Constraints: $(1, 2)$, $(2, 3)$, $(3, 4)$.\n  - Right-hand side $\\dot{\\eta} = [0.02, -0.01, 0]$.\n  - Regularization $\\varepsilon = 0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output three floating-point numbers in the following order: $[\\lVert \\lambda_{\\mathrm{direct}} - \\lambda_{\\mathrm{CG}} \\rVert_{\\infty}, \\lVert A \\lambda_{\\mathrm{direct}} - \\dot{\\eta} \\rVert_2, \\lVert A \\lambda_{\\mathrm{CG}} - \\dot{\\eta} \\rVert_2]$. Aggregate all cases in sequence into a single flat list. For example, if there were two cases the output could look like $[d_1, r_{d,1}, r_{i,1}, d_2, r_{d,2}, r_{i,2}]$.\n- No other text should be printed.",
            "solution": "We start from Newton’s second law with holonomic constraints enforced via Lagrange multipliers. Let $q \\in \\mathbb{R}^{3N}$ be the positions and $v = \\dot{q} \\in \\mathbb{R}^{3N}$ the velocities. The unconstrained equations of motion are $M \\dot{v} = f(q, v, t)$, where $M \\in \\mathbb{R}^{3N \\times 3N}$ is symmetric positive definite and diagonal for point masses. For holonomic constraints $g_k(q) = 0$ with $k \\in \\{1, \\dots, m\\}$, the constraint Jacobian $C(q) \\in \\mathbb{R}^{m \\times 3N}$ has rows $C_k = \\partial g_k / \\partial q$. Introducing Lagrange multipliers $\\lambda \\in \\mathbb{R}^m$, the constrained equations are\n$$\nM \\dot{v} = f + C^\\top \\lambda,\n$$\nwith the kinematic constraint at velocity level\n$$\nC v = 0,\n$$\nfor exact enforcement of holonomic constraints in time.\n\nTo project velocities or apply a corrective impulse that achieves a desired change $\\Delta v$ satisfying\n$$\nC \\Delta v = \\dot{\\eta},\n$$\nwe consider an impulsive correction that neglects other forces during the instantaneous correction. The impulse associated with multipliers $\\lambda$ induces a velocity change\n$$\n\\Delta v = M^{-1} C^\\top \\lambda.\n$$\nSubstituting into the velocity-level constraint yields\n$$\nC \\Delta v = C M^{-1} C^\\top \\lambda = \\dot{\\eta},\n$$\nwhich is the Schur complement system\n$$\n(C M^{-1} C^\\top) \\lambda = \\dot{\\eta}.\n$$\nFor independent constraints, $S := C M^{-1} C^\\top$ is symmetric positive definite (SPD), hence invertible, and the system has a unique solution. When constraints are nearly redundant, $S$ can be ill-conditioned; Tikhonov regularization replaces $S$ by $A := S + \\varepsilon I$ with $\\varepsilon \\ge 0$ to stabilize the solve.\n\nConstraint construction for pairwise distances: Consider holonomic constraints of the form $g_{ij}(q) = \\|r_j - r_i\\|^2 - \\ell_{ij}^2 = 0$. The Jacobian row is\n$$\nC_{ij} = \\left[ \\dots, \\frac{\\partial g_{ij}}{\\partial r_i}, \\dots, \\frac{\\partial g_{ij}}{\\partial r_j}, \\dots \\right] = \\left[ \\dots, -2 (r_j - r_i)^\\top, \\dots, 2 (r_j - r_i)^\\top, \\dots \\right].\n$$\nAt the velocity level, $\\dot{g}_{ij} = C_{ij} v = 2 (r_j - r_i) \\cdot (v_j - v_i)$. The scalar factor $2$ can be absorbed into the multipliers without loss of generality; thus we equivalently use a Jacobian row proportional to the direction vector,\n$$\n\\tilde{C}_{ij} = \\left[ \\dots, -(r_j - r_i)^\\top, \\dots, (r_j - r_i)^\\top, \\dots \\right],\n$$\nwhich generates the same nullspace and leads to an equivalent Schur system scaled by a constant. Using this linearized form, we assemble $C$ by stacking such rows for all constraints.\n\nAlgorithmic steps per test case:\n- Build $M^{-1}$ as a diagonal matrix with each mass $m_i$ repeated for its three spatial components, so that $M^{-1} = \\mathrm{diag}(m_1^{-1} I_3, \\dots, m_N^{-1} I_3)$.\n- Assemble $C \\in \\mathbb{R}^{m \\times 3N}$ using the rule above with $d_{ij} = r_j - r_i$.\n- Compute $S = C M^{-1} C^\\top$ efficiently by scaling the columns of $C$ by the appropriate inverse masses and then forming $S$ as a dense product.\n- Form $A = S + \\varepsilon I$.\n- Solve $A \\lambda = \\dot{\\eta}$ by:\n  - Direct dense solve using Gaussian elimination (for example, a general dense linear solver).\n  - Conjugate Gradient (CG), which is applicable because $A$ is SPD for $\\varepsilon \\ge 0$ and independent constraints. Use the stopping condition $\\lVert r_k \\rVert_2 \\le \\max(\\tau_{\\mathrm{rel}} \\lVert \\dot{\\eta} \\rVert_2, \\tau_{\\mathrm{abs}})$ with $\\tau_{\\mathrm{rel}} = 10^{-12}$, $\\tau_{\\mathrm{abs}} = 10^{-14}$, and a maximum of $10^4$ iterations.\n- Report diagnostics:\n  - The infinity norm of the difference between the solutions, $\\lVert \\lambda_{\\mathrm{direct}} - \\lambda_{\\mathrm{CG}} \\rVert_{\\infty}$.\n  - The residual norms $\\lVert A \\lambda_{\\mathrm{direct}} - \\dot{\\eta} \\rVert_2$ and $\\lVert A \\lambda_{\\mathrm{CG}} - \\dot{\\eta} \\rVert_2$.\n\nWhy Conjugate Gradient applies: For independent constraints, $S = C M^{-1} C^\\top$ is SPD because for any nonzero $x \\in \\mathbb{R}^m$,\n$$\nx^\\top S x = x^\\top C M^{-1} C^\\top x = (C^\\top x)^\\top M^{-1} (C^\\top x) > 0,\n$$\nsince $M^{-1}$ is SPD and $C^\\top x \\ne 0$ for independent constraints. Adding $\\varepsilon I$ preserves symmetry and positive definiteness for $\\varepsilon \\ge 0$, with strict positivity even if $C$ is rank-deficient when $\\varepsilon > 0$. Thus CG converges.\n\nTest suite coverage rationale:\n- Case $1$ validates the basic single-constraint scenario.\n- Case $2$ exercises multiple constraints and coupling through a chain.\n- Case $3$ probes near-redundancy and the need for regularization with $\\varepsilon = 10^{-12}$.\n- Case $4$ enforces a zero right-hand side with extreme mass scaling, where the exact solution is $\\lambda = 0$ and residuals should be at machine precision.\n- Case $5$ stresses conditioning with extreme mass ratios and multiple coupled constraints.\n\nExpected outcomes: In all well-posed cases, direct and CG solutions should agree to within numerical precision, so $\\lVert \\lambda_{\\mathrm{direct}} - \\lambda_{\\mathrm{CG}} \\rVert_{\\infty}$ should be near machine epsilon, and both residual norms should be small (on the order of the stopping tolerances or smaller). Regularized Case $3$ should similarly yield small discrepancies due to the SPD regularized system.\n\nThe final program implements these steps and prints a single flat list of the three diagnostics per case as required.",
            "answer": "```python\nimport numpy as np\n\ndef build_C(positions, constraints):\n    \"\"\"\n    Build the constraint Jacobian C for pairwise distance constraints.\n    positions: array of shape (N,3)\n    constraints: list of (i,j) with 0-based indices\n    Returns C of shape (m, 3N)\n    \"\"\"\n    N = positions.shape[0]\n    m = len(constraints)\n    C = np.zeros((m, 3 * N), dtype=float)\n    for k, (i, j) in enumerate(constraints):\n        d = positions[j] - positions[i]  # vector r_j - r_i\n        # Place -d^T in block for i, +d^T in block for j\n        C[k, 3 * i:3 * i + 3] = -d\n        C[k, 3 * j:3 * j + 3] = +d\n    return C\n\ndef assemble_S(C, masses):\n    \"\"\"\n    Assemble S = C M^{-1} C^T given C and masses.\n    masses: array of length N\n    \"\"\"\n    N = len(masses)\n    inv_masses = 1.0 / np.asarray(masses, dtype=float)\n    # Scale columns of C by inverse masses for each 3-block\n    C_scaled = C.copy()\n    for i in range(N):\n        C_scaled[:, 3 * i:3 * i + 3] *= inv_masses[i]\n    S = C_scaled @ C.T\n    return S\n\ndef conjugate_gradient(A, b, tol_rel=1e-12, tol_abs=1e-14, maxiter=10_000):\n    \"\"\"\n    Conjugate Gradient for SPD dense matrix A and vector b.\n    Stops when ||r||_2 <= max(tol_rel * ||b||_2, tol_abs)\n    Returns x, iters, final_res_norm\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n, dtype=float)\n    r = b - A @ x\n    p = r.copy()\n    rsold = float(r @ r)\n    bnorm = np.linalg.norm(b)\n    tol = max(tol_rel * bnorm, tol_abs)\n    if np.sqrt(rsold) <= tol:\n        return x, 0, np.sqrt(rsold)\n    for k in range(1, maxiter + 1):\n        Ap = A @ p\n        pAp = float(p @ Ap)\n        if pAp <= 0.0:\n            # Breakdown; return current iterate\n            resn = np.sqrt(rsold)\n            return x, k - 1, resn\n        alpha = rsold / pAp\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = float(r @ r)\n        if np.sqrt(rsnew) <= tol:\n            return x, k, np.sqrt(rsnew)\n        beta = rsnew / rsold\n        p = r + beta * p\n        rsold = rsnew\n    return x, maxiter, np.sqrt(rsold)\n\ndef solve_case(masses, positions, constraint_pairs, dot_eta, epsilon):\n    \"\"\"\n    Solve one test case:\n    - Build C from positions and constraints\n    - Assemble S = C M^{-1} C^T\n    - Form A = S + epsilon * I\n    - Solve A lambda = dot_eta by direct solve and CG\n    - Return diagnostics: inf-norm difference, residual norms for direct and CG\n    \"\"\"\n    masses = np.array(masses, dtype=float)\n    positions = np.array(positions, dtype=float)\n    C = build_C(positions, constraint_pairs)\n    S = assemble_S(C, masses)\n    m = S.shape[0]\n    if epsilon != 0.0:\n        A = S + epsilon * np.eye(m)\n    else:\n        A = S.copy()\n    b = np.array(dot_eta, dtype=float)\n\n    # Direct solve\n    try:\n        lam_direct = np.linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        # Fallback to least squares if singular (should not happen with epsilon > 0)\n        lam_direct, *_ = np.linalg.lstsq(A, b, rcond=None)\n\n    # CG solve\n    lam_cg, iters, res_cg = conjugate_gradient(A, b, tol_rel=1e-12, tol_abs=1e-14, maxiter=10000)\n\n    # Diagnostics\n    diff_inf = float(np.max(np.abs(lam_direct - lam_cg))) if lam_direct.size > 0 else 0.0\n    resid_direct = float(np.linalg.norm(A @ lam_direct - b))\n    resid_cg = float(np.linalg.norm(A @ lam_cg - b))\n\n    return diff_inf, resid_direct, resid_cg\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"masses\": [12.0, 16.0],\n            \"positions\": [\n                [0.0, 0.0, 0.0],\n                [1.0, 0.0, 0.0],\n            ],\n            \"constraints\": [(0, 1)],\n            \"dot_eta\": [0.1],\n            \"epsilon\": 0.0,\n        },\n        # Case 2\n        {\n            \"masses\": [12.0, 14.0, 12.0],\n            \"positions\": [\n                [0.0, 0.0, 0.0],\n                [1.0, 0.0, 0.0],\n                [2.0, 0.0, 0.0],\n            ],\n            \"constraints\": [(0, 1), (1, 2)],\n            \"dot_eta\": [0.0, 0.05],\n            \"epsilon\": 0.0,\n        },\n        # Case 3\n        {\n            \"masses\": [12.0, 12.0, 12.0],\n            \"positions\": [\n                [0.0, 0.0, 0.0],\n                [1.0, 0.0, 0.0],\n                [1.0, 1e-6, 0.0],\n            ],\n            \"constraints\": [(0, 1), (0, 2)],\n            \"dot_eta\": [0.01, 0.01],\n            \"epsilon\": 1e-12,\n        },\n        # Case 4\n        {\n            \"masses\": [1e-6, 10.0],\n            \"positions\": [\n                [0.0, 0.0, 0.0],\n                [0.5, 0.0, 0.0],\n            ],\n            \"constraints\": [(0, 1)],\n            \"dot_eta\": [0.0],\n            \"epsilon\": 0.0,\n        },\n        # Case 5\n        {\n            \"masses\": [100.0, 0.1, 0.1, 100.0],\n            \"positions\": [\n                [0.0, 0.0, 0.0],\n                [1.0, 0.0, 0.0],\n                [2.0, 0.0, 0.0],\n                [3.0, 0.0, 0.0],\n            ],\n            \"constraints\": [(0, 1), (1, 2), (2, 3)],\n            \"dot_eta\": [0.02, -0.01, 0.0],\n            \"epsilon\": 0.0,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        diff_inf, resid_direct, resid_cg = solve_case(\n            case[\"masses\"],\n            case[\"positions\"],\n            case[\"constraints\"],\n            case[\"dot_eta\"],\n            case[\"epsilon\"],\n        )\n        results.extend([diff_inf, resid_direct, resid_cg])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "The Lagrange multiplier formalism requires a set of linearly independent constraints to yield a unique, stable solution. In practice, defining constraints based on molecular topology, especially in systems with rings, can inadvertently lead to redundancies and an ill-posed or \"overconstrained\" problem. This exercise will teach you how to diagnose this issue by combining graph theory with linear algebra and how to apply corrective measures like constraint pruning or regularization to ensure your system is well-defined .",
            "id": "3416369",
            "problem": "Consider a molecular dynamics system of $N$ point particles with positions $q = (r_{1,x}, r_{1,y}, r_{1,z}, \\dots, r_{N,x}, r_{N,y}, r_{N,z}) \\in \\mathbb{R}^{3N}$ subject to holonomic constraints, enforced via Lagrange multipliers, that fix relative displacements along edges of an undirected constraint graph. Each constraint is associated with an undirected edge $(i,j)$ of the graph and a Cartesian component $\\alpha \\in \\{x,y,z\\}$, and is given by $g_{(i,j),\\alpha}(q) = r_{i,\\alpha} - r_{j,\\alpha} - d_{(i,j),\\alpha} = 0$, where $d_{(i,j),\\alpha} \\in \\mathbb{R}$ is a specified target displacement component for the edge $(i,j)$. The constraint Jacobian $C(q) = \\partial g / \\partial q$ has one row per scalar constraint and $3N$ columns; for the constraint $g_{(i,j),\\alpha}$, the corresponding row has entries $+1$ at the coordinate $(i,\\alpha)$ and $-1$ at $(j,\\alpha)$, with zeros elsewhere. Let $m$ denote the total number of scalar constraints, that is $m = 3|E|$ where $|E|$ is the number of edges in the constraint graph.\n\nThe system is consistent if the constraints are independent and the Lagrange multiplier system associated with $C$ is solvable. Overconstraint scenarios arise when constraints are algebraically dependent, typically due to cycles in the constraint graph. The rank $\\operatorname{rank} C$ detects the number of independent constraints, and if $\\operatorname{rank} C < m$ then some constraints are redundant. The matrix governing Lagrange multipliers in mass-weighted form is $G = C M^{-1} C^{\\top}$, where $M$ is the diagonal mass matrix with particle masses along each coordinate. In overconstraint scenarios, $G$ becomes singular or ill-conditioned. Two remedies are: (i) Tikhonov regularization adding $\\lambda I$ to $G$ to raise its smallest eigenvalue above a numerical threshold, and (ii) constraint pruning, removing a minimal set of constraints along cycles to restore independence.\n\nYour task is to derive, implement, and test an algorithm that:\n\n1. Constructs the constraint Jacobian $C$ for the specified set of displacement constraints.\n2. Computes a numerically robust estimate of $\\operatorname{rank} C$ using singular value decomposition with a tolerance.\n3. Detects cycles in the undirected constraint graph and computes the cyclomatic number $\\mu = |E| - |V| + c$, where $|V|$ is the number of vertices and $c$ is the number of connected components.\n4. Determines whether an overconstraint exists specifically due to cycles by checking whether $\\operatorname{rank} C < m$ and $\\mu > 0$.\n5. Proposes a regularization parameter $\\lambda \\ge 0$ such that the smallest eigenvalue of $G + \\lambda I$ is at least a target threshold $\\tau$, where $\\tau$ is chosen based on the largest eigenvalue of $G$ and machine precision. Use $M^{-1}$ formed from particle masses; the mass matrix $M$ is diagonal with blocks $(m_i, m_i, m_i)$ for each particle $i$.\n6. Proposes a constraint pruning strategy that removes exactly one undirected edge per independent cycle (thus removing $3$ scalar constraints per removed edge) by dropping non-tree edges discovered during a spanning forest construction, then recomputes $\\operatorname{rank} C$ on the pruned set to verify independence.\n\nStart from the following base principles: Newton’s second law $\\mathbf{F} = m \\mathbf{a}$; holonomic constraints $g(q) = 0$ enforced via Lagrange multipliers; the definition of the constraint Jacobian $C = \\partial g / \\partial q$; and the mass-weighted Lagrange multiplier system $G \\boldsymbol{\\mu} = -C M^{-1} f$ that must be solvable for the multipliers $\\boldsymbol{\\mu}$, where $G = C M^{-1} C^{\\top}$. Use singular value decomposition for rank determination and cyclomatic number for cycle detection. Do not assume shortcut formulas beyond these foundations.\n\nImplement the algorithm and evaluate it on the following test suite. Each test case provides $N$, positions $r_i$ (used to construct consistent $d_{(i,j)}$ values via $d_{(i,j)} = r_i - r_j$), an undirected edge list $E$, and a mass array $(m_1, \\dots, m_N)$.\n\n- Test Case 1 (happy path, no cycles):\n  - $N = 4$\n  - Positions: $r_0 = (0,0,0)$, $r_1 = (1,0,0)$, $r_2 = (2,0,0)$, $r_3 = (3,0,0)$\n  - Edges: $(0,1), (1,2), (2,3)$\n  - Masses: $(1.0, 1.0, 1.0, 1.0)$\n\n- Test Case 2 (single cycle, square ring):\n  - $N = 4$\n  - Positions: $r_0 = (0,0,0)$, $r_1 = (1,0,0)$, $r_2 = (1,1,0)$, $r_3 = (0,1,0)$\n  - Edges: $(0,1), (1,2), (2,3), (3,0)$\n  - Masses: $(1.0, 2.0, 1.5, 1.0)$\n\n- Test Case 3 (two cycles, figure-eight):\n  - $N = 5$\n  - Positions: $r_0 = (0,0,0)$, $r_1 = (1,0,0)$, $r_2 = (1,1,0)$, $r_3 = (2,1,0)$, $r_4 = (2,2,0)$\n  - Edges: $(0,1), (1,2), (2,0), (2,3), (3,4), (4,2)$\n  - Masses: $(1.0, 1.2, 1.0, 0.8, 1.5)$\n\n- Test Case 4 (boundary case, broken ring):\n  - $N = 4$\n  - Positions: $r_0 = (0,0,0)$, $r_1 = (1,0,0)$, $r_2 = (1,1,0)$, $r_3 = (0,1,0)$\n  - Edges: $(0,1), (1,2), (2,3)$\n  - Masses: $(1.0, 3.0, 1.0, 2.0)$\n\nFor each test case, your program must compute the following quantities:\n\n- $\\operatorname{rank} C$ (as an integer),\n- $m$ (the total number of scalar constraints, as an integer),\n- $\\mu$ (the cyclomatic number, as an integer),\n- the number of scalar constraints removed by pruning (as an integer),\n- the $\\operatorname{rank} C$ after pruning (as an integer),\n- the regularization parameter $\\lambda$ (as a float),\n- a boolean indicating whether the overconstraint is due to cycles (true if $\\operatorname{rank} C < m$ and $\\mu > 0$, otherwise false).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element in the list should itself be a list in the order described above. For example, the output format must be of the form $[\\,[r_1, m_1, \\mu_1, \\Delta m_1, r^{\\text{pruned}}_1, \\lambda_1, b_1], \\dots, [r_4, m_4, \\mu_4, \\Delta m_4, r^{\\text{pruned}}_4, \\lambda_4, b_4]\\,]$. No physical units are required, and angles are not involved. All boolean values should be printed in their programming language canonical form (for example, true or false).",
            "solution": "The problem requires an analysis of a molecular system with holonomic constraints, focusing on the detection and resolution of overconstraints arising from cycles in the constraint graph. The solution involves a multi-step algorithm that integrates concepts from linear algebra, graph theory, and numerical analysis, grounded in the principles of classical mechanics.\n\nThe foundational principle is the enforcement of holonomic constraints $g(q) = 0$ on a system of $N$ particles with generalized coordinates $q \\in \\mathbb{R}^{3N}$. These constraints are incorporated into the equations of motion using Lagrange multipliers. The system of equations for the Lagrange multipliers $\\lambda$ involves the constraint Jacobian, $C(q) = \\partial g / \\partial q$, and the mass matrix $M$. Specifically, the matrix $G = C M^{-1} C^{\\top}$ must be inverted. If the constraints are not linearly independent, this matrix becomes singular, leading to an overconstrained system.\n\nOur algorithm systematically addresses the tasks specified in the problem statement for each test case.\n\n**1. Constraint Graph and Jacobian Construction**\n\nThe constraints are defined on a graph where vertices represent particles and edges $(i,j)$ represent a fixed relative displacement. Each edge $(i,j)$ corresponds to three scalar constraints, one for each Cartesian component $\\alpha \\in \\{x,y,z\\}$:\n$$g_{(i,j),\\alpha}(q) = r_{i,\\alpha} - r_{j,\\alpha} - d_{(i,j),\\alpha} = 0$$\nwhere $r_{i,\\alpha}$ is the $\\alpha$-coordinate of particle $i$ and $d_{(i,j),\\alpha}$ is a constant target displacement. The total number of scalar constraints is $m = 3|E|$, where $|E|$ is the number of edges in the constraint graph.\n\nThe constraint Jacobian $C$ is an $m \\times 3N$ matrix. Each row of $C$ corresponds to the gradient of one scalar constraint function. For the constraint $g_{(i,j),\\alpha}$, the corresponding row vector has a $+1$ at the column index for coordinate $r_{i,\\alpha}$ and a $-1$ at the column index for $r_{j,\\alpha}$. All other entries are zero. The specific values of the positions $r_i$ and target displacements $d_{(i,j),\\alpha}$ do not affect the Jacobian matrix $C$.\n\n**2. Rank Analysis via Singular Value Decomposition (SVD)**\n\nThe number of linearly independent constraints is given by the rank of the Jacobian matrix, $\\operatorname{rank} C$. A numerically robust method to compute the rank is through SVD. The number of singular values of $C$ that are greater than a small tolerance determines the rank. If $\\operatorname{rank} C < m$, the set of constraints is linearly dependent, indicating redundancy and an overconstrained system.\n\n**3. Graph Cycle Detection and Cyclomatic Number**\n\nRedundancies in this type of constraint system are directly related to cycles in the underlying constraint graph. The sum of relative displacement vectors around any closed loop of particles must be zero, e.g., $(r_i-r_j) + (r_j-r_k) + (r_k-r_i) = 0$. This geometric identity implies a linear dependency among the corresponding constraint functions for each Cartesian component.\n\nTo quantify the number of cycles, we compute the cyclomatic number of the constraint graph G=($V$, $E$):\n$$\\mu = |E| - |V| + c$$\nwhere $|V|=N$ is the number of vertices (particles), $|E|$ is the number of edges (displacement constraints), and $c$ is the number of connected components in the graph. A value $\\mu > 0$ indicates the presence of one or more independent cycles. We determine $c$ by performing a graph traversal, such as a Breadth-First Search (BFS), to count the distinct subgraphs that are internally connected.\n\n**4. Identification of Cycle-Induced Overconstraint**\n\nAn overconstraint is specifically identified as being caused by cycles if both conditions are met: the system is mathematically overconstrained ($\\operatorname{rank} C < m$) and the constraint graph contains cycles ($\\mu > 0$).\n\n**5. Tikhonov Regularization**\n\nOne method to handle the singular matrix $G = C M^{-1} C^{\\top}$ is Tikhonov regularization. We seek a parameter $\\lambda \\ge 0$ to form a regularized matrix $G' = G + \\lambda I$ that is non-singular and well-conditioned. The eigenvalues of $G'$ are $\\lambda'_k = \\lambda_k + \\lambda$, where $\\lambda_k$ are the eigenvalues of $G$. Since $G$ is symmetric positive semi-definite, its eigenvalues are real and non-negative. If $G$ is singular, its smallest eigenvalue $\\lambda_{\\text{min}}$ is $0$.\n\nWe aim to ensure the smallest eigenvalue of $G'$, $\\lambda_{\\text{min}} + \\lambda$, is above a numerical stability threshold $\\tau$. A reasonable choice for this threshold is $\\tau = \\lambda_{\\text{max}} \\cdot \\epsilon_{\\text{machine}}$, where $\\lambda_{\\text{max}}$ is the largest eigenvalue of $G$ and $\\epsilon_{\\text{machine}}$ is machine precision. The required regularization parameter is then:\n$$\\lambda = \\max(0, \\tau - \\lambda_{\\text{min}})$$\nThe mass matrix $M$ is a $3N \\times 3N$ diagonal matrix where the diagonal entries corresponding to particle $i$ are all equal to its mass $m_i$. Its inverse $M^{-1}$ is also diagonal, with entries $1/m_i$.\n\n**6. Constraint Pruning**\n\nA direct approach to eliminate redundancies is to remove constraints until they are linearly independent. For cycle-based redundancies, this corresponds to making the constraint graph acyclic. This is achieved by finding a spanning forest of the graph. A spanning forest contains the maximum number of edges possible without creating a cycle. The edges from the original graph that are not in the spanning forest are the ones that complete the cycles.\n\nWe construct a spanning forest using a graph traversal (e.g., BFS), adding edges to the forest as we discover unvisited vertices. The number of edges in a spanning forest of a graph with $N$ vertices and $c$ components is $N-c$. The number of edges to remove is $|E| - (N-c) = \\mu$. For each edge removed, we remove its three associated scalar constraints. The total number of removed scalar constraints is $3\\mu$. We then construct a new, pruned Jacobian matrix $C_{\\text{pruned}}$ using only the edges in the spanning forest and verify that its rank equals its number of rows, confirming that all redundancies have been removed.\n\nBy implementing these steps, we can fully characterize each test case as requested.",
            "answer": "```python\nimport numpy as np\nfrom collections import deque\n\ndef process_case(N, edges, masses):\n    \"\"\"\n    Processes a single test case according to the problem description.\n    \"\"\"\n    \n    # Task 3 (Part 1): Graph Analysis to find connected components and spanning forest\n    adj = {i: [] for i in range(N)}\n    unique_edges = set()\n    for u, v in edges:\n        adj[u].append(v)\n        adj[v].append(u)\n        unique_edges.add(tuple(sorted((u, v))))\n    \n    num_edges = len(unique_edges)\n    visited = [False] * N\n    num_components = 0\n    spanning_forest_edges = []\n    \n    for i in range(N):\n        if not visited[i]:\n            num_components += 1\n            q = deque([i])\n            visited[i] = True\n            while q:\n                u = q.popleft()\n                for v in adj[u]:\n                    if not visited[v]:\n                        visited[v] = True\n                        q.append(v)\n                        spanning_forest_edges.append(tuple(sorted((u, v))))\n\n    # Task 3 (Part 2): Compute cyclomatic number\n    mu = num_edges - N + num_components\n\n    # Helper function to construct the Jacobian\n    def construct_jacobian(n_particles, edge_list):\n        n_constraints = 3 * len(edge_list)\n        if n_constraints == 0:\n            return np.zeros((0, 3 * n_particles)), 0\n        \n        C = np.zeros((n_constraints, 3 * n_particles))\n        constraint_idx = 0\n        for u, v in edge_list:\n            for alpha in range(3):\n                col_i = 3 * u + alpha\n                col_j = 3 * v + alpha\n                C[constraint_idx, col_i] = 1\n                C[constraint_idx, col_j] = -1\n                constraint_idx += 1\n        return C, n_constraints\n\n    # Task 1: Construct the full constraint Jacobian\n    C, m = construct_jacobian(N, unique_edges)\n\n    # Task 2: Compute rank of the full Jacobian\n    if m > 0:\n        rank_C = np.linalg.matrix_rank(C)\n    else:\n        rank_C = 0\n\n    # Task 4: Detect overconstraint due to cycles\n    is_overconstrained_by_cycles = (rank_C < m) and (mu > 0)\n\n    # Task 6: Propose and evaluate pruning strategy\n    num_removed_scalar_constraints = 3 * mu\n    C_pruned, m_pruned = construct_jacobian(N, spanning_forest_edges)\n    if m_pruned > 0:\n        rank_C_pruned = np.linalg.matrix_rank(C_pruned)\n    else:\n        rank_C_pruned = 0\n\n    # Task 5: Propose regularization parameter lambda\n    if m == 0:\n        lambda_reg = 0.0\n    else:\n        masses_array = np.array(masses, dtype=float)\n        inv_masses_rep = np.repeat(1.0 / masses_array, 3)\n        \n        # G = C M^-1 C^T, computed efficiently\n        # C_scaled_by_mass = C * inv_masses_rep (broadcasting)\n        # G = C_scaled_by_mass @ C.T\n        temp = C * inv_masses_rep\n        G = temp @ C.T\n        \n        eigvals_G = np.linalg.eigvalsh(G)\n        lambda_min = eigvals_G[0]\n        lambda_max = eigvals_G[-1]\n        \n        # Set threshold tau\n        tau = lambda_max * np.finfo(float).eps\n        \n        # Compute lambda\n        lambda_reg = max(0.0, tau - lambda_min)\n\n    return [\n        int(rank_C),\n        int(m),\n        int(mu),\n        int(num_removed_scalar_constraints),\n        int(rank_C_pruned),\n        float(lambda_reg),\n        is_overconstrained_by_cycles\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 4,\n            \"positions\": {0:(0,0,0), 1:(1,0,0), 2:(2,0,0), 3:(3,0,0)},\n            \"edges\": [(0,1), (1,2), (2,3)],\n            \"masses\": [1.0, 1.0, 1.0, 1.0]\n        },\n        {\n            \"N\": 4,\n            \"positions\": {0:(0,0,0), 1:(1,0,0), 2:(1,1,0), 3:(0,1,0)},\n            \"edges\": [(0,1), (1,2), (2,3), (3,0)],\n            \"masses\": [1.0, 2.0, 1.5, 1.0]\n        },\n        {\n            \"N\": 5,\n            \"positions\": {0:(0,0,0), 1:(1,0,0), 2:(1,1,0), 3:(2,1,0), 4:(2,2,0)},\n            \"edges\": [(0,1), (1,2), (2,0), (2,3), (3,4), (4,2)],\n            \"masses\": [1.0, 1.2, 1.0, 0.8, 1.5]\n        },\n        {\n            \"N\": 4,\n            \"positions\": {0:(0,0,0), 1:(1,0,0), 2:(1,1,0), 3:(0,1,0)},\n            \"edges\": [(0,1), (1,2), (2,3)],\n            \"masses\": [1.0, 3.0, 1.0, 2.0]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case['N'], case['edges'], case['masses'])\n        all_results.append(result)\n\n    # Format the output as specified\n    formatted_results = []\n    for res in all_results:\n        res[-1] = str(res[-1]).lower()  # Convert boolean to \"true\"/\"false\"\n        str_res = [str(x) for x in res]\n        formatted_results.append(f\"[{','.join(str_res)}]\")\n    \n    final_output = f\"[{','.join(formatted_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}