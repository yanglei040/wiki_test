{
    "hands_on_practices": [
        {
            "introduction": "In many advanced simulations, such as *ab initio* molecular dynamics, the potential energy $U$ is not given by a simple analytic function but is calculated on-the-fly, often with some numerical noise. To obtain the forces, we must differentiate this noisy potential numerically. This exercise  challenges you to confront the core trade-off of this process: balancing the systematic truncation error, which shrinks with smaller step sizes $h$, against the error from noise propagation, which is amplified by small $h$. By deriving the optimal step size that minimizes the total mean-squared error, you will gain a deep, analytical understanding of a fundamental challenge encountered throughout computational science.",
            "id": "3436116",
            "problem": "Consider a single reaction coordinate $x$ embedded in a high-dimensional potential energy surface $U(\\mathbf{r})$ for a molecular system studied via Molecular Dynamics (MD). Along a one-dimensional path parameterized by $x$, define the restricted potential energy function $U(x)$ and the corresponding force component $F(x) = -\\frac{dU}{dx}$. You wish to estimate $F(x_0)$ at a fixed point $x_0$ by numerically differentiating $U(x)$ using a central finite difference with step size $h > 0$. However, each energy evaluation is subject to unbiased, independent additive noise due to incomplete electronic structure convergence, modeled as $\\tilde{U}(x) = U(x) + \\varepsilon(x)$, where $\\mathbb{E}[\\varepsilon(x)] = 0$ and $\\operatorname{Var}(\\varepsilon(x)) = \\sigma^{2}$, with $\\varepsilon(x+h)$ and $\\varepsilon(x-h)$ independent.\n\nDefine the central-difference force estimator\n$$\n\\widehat{F}(x_0; h) = -\\frac{\\tilde{U}(x_0 + h) - \\tilde{U}(x_0 - h)}{2h}.\n$$\nAssume $U(x)$ is five times continuously differentiable in a neighborhood of $x_0$, and that $U^{(3)}(x_0)$ is finite and nonzero. Work to leading order in $h$ and ignore terms beyond the third derivative in the truncation error.\n\nStarting from the fundamental definitions of force as a negative gradient and the Taylor expansion of $U(x)$ around $x_0$, derive the leading-order mean-squared error for $\\widehat{F}(x_0; h)$ as a function of $h$, and then determine the step size $h$ that minimizes this mean-squared error. Your final answer must be a single closed-form analytic expression for the optimal $h$ in terms of $\\sigma$ and $U^{(3)}(x_0)$. No numerical values are required. If you find that the optimal expression involves the sign of $U^{(3)}(x_0)$, use its absolute value where appropriate to ensure $h > 0$. Do not include units in your final boxed expression.",
            "solution": "The problem requires the derivation of the optimal step size $h$ for a central-difference force estimator that minimizes the mean-squared error (MSE). The MSE of an estimator $\\widehat{\\theta}$ for a parameter $\\theta$ is defined as the expected value of the squared error, which can be decomposed into the square of the bias and the variance:\n$$\n\\operatorname{MSE}(\\widehat{\\theta}) = \\mathbb{E}[(\\widehat{\\theta} - \\theta)^2] = (\\operatorname{Bias}(\\widehat{\\theta}))^2 + \\operatorname{Var}(\\widehat{\\theta})\n$$\nwhere $\\operatorname{Bias}(\\widehat{\\theta}) = \\mathbb{E}[\\widehat{\\theta}] - \\theta$.\n\nIn this problem, the estimator is the force $\\widehat{F}(x_0; h)$ and the true value is $F(x_0) = -U'(x_0)$, where $U'(x_0) = \\frac{dU}{dx}|_{x=x_0}$. The estimator is given by:\n$$\n\\widehat{F}(x_0; h) = -\\frac{\\tilde{U}(x_0 + h) - \\tilde{U}(x_0 - h)}{2h}\n$$\nThe noisy potential is $\\tilde{U}(x) = U(x) + \\varepsilon(x)$, with the noise $\\varepsilon(x)$ having properties $\\mathbb{E}[\\varepsilon(x)] = 0$ and $\\operatorname{Var}(\\varepsilon(x)) = \\sigma^{2}$. The noise at different points is independent.\n\nFirst, we calculate the bias of the estimator. The bias is the difference between the expected value of the estimator and the true value. We begin by computing the expectation of $\\widehat{F}(x_0; h)$:\n$$\n\\mathbb{E}[\\widehat{F}(x_0; h)] = \\mathbb{E}\\left[-\\frac{\\tilde{U}(x_0 + h) - \\tilde{U}(x_0 - h)}{2h}\\right]\n$$\nSubstituting $\\tilde{U}(x) = U(x) + \\varepsilon(x)$:\n$$\n\\mathbb{E}[\\widehat{F}(x_0; h)] = -\\frac{1}{2h}\\mathbb{E}[(U(x_0 + h) + \\varepsilon(x_0 + h)) - (U(x_0 - h) + \\varepsilon(x_0 - h))]\n$$\nBy linearity of expectation, and since $U(x)$ is a deterministic function:\n$$\n\\mathbb{E}[\\widehat{F}(x_0; h)] = -\\frac{1}{2h}(U(x_0 + h) - U(x_0 - h) + \\mathbb{E}[\\varepsilon(x_0 + h)] - \\mathbb{E}[\\varepsilon(x_0 - h)])\n$$\nGiven that $\\mathbb{E}[\\varepsilon(x)] = 0$ for any $x$, this simplifies to:\n$$\n\\mathbb{E}[\\widehat{F}(x_0; h)] = -\\frac{U(x_0 + h) - U(x_0 - h)}{2h}\n$$\nThis is the standard central difference formula for the derivative of $U(x)$. To find the bias, we compare this to the true force $F(x_0) = -U'(x_0)$. The bias is the systematic error, or truncation error, of the finite difference approximation. We use the Taylor series for $U(x)$ around $x_0$. Since $U(x)$ is assumed to be at least $C^5$, we can write:\n$$\nU(x_0 + h) = U(x_0) + h U'(x_0) + \\frac{h^2}{2!}U''(x_0) + \\frac{h^3}{3!}U^{(3)}(x_0) + \\frac{h^4}{4!}U^{(4)}(x_0) + \\frac{h^5}{5!}U^{(5)}(x_0) + \\dots\n$$\n$$\nU(x_0 - h) = U(x_0) - h U'(x_0) + \\frac{h^2}{2!}U''(x_0) - \\frac{h^3}{3!}U^{(3)}(x_0) + \\frac{h^4}{4!}U^{(4)}(x_0) - \\frac{h^5}{5!}U^{(5)}(x_0) + \\dots\n$$\nSubtracting the second expansion from the first, the even-powered terms cancel:\n$$\nU(x_0 + h) - U(x_0 - h) = 2h U'(x_0) + 2\\frac{h^3}{3!}U^{(3)}(x_0) + 2\\frac{h^5}{5!}U^{(5)}(x_0) + \\dots\n$$\nDividing by $2h$:\n$$\n\\frac{U(x_0 + h) - U(x_0 - h)}{2h} = U'(x_0) + \\frac{h^2}{6}U^{(3)}(x_0) + \\frac{h^4}{120}U^{(5)}(x_0) + \\dots\n$$\nSubstituting this into the expression for $\\mathbb{E}[\\widehat{F}(x_0; h)]$:\n$$\n\\mathbb{E}[\\widehat{F}(x_0; h)] = -\\left( U'(x_0) + \\frac{h^2}{6}U^{(3)}(x_0) + O(h^4) \\right) = F(x_0) - \\frac{h^2}{6}U^{(3)}(x_0) + O(h^4)\n$$\nThe bias is then:\n$$\n\\operatorname{Bias}(\\widehat{F}(x_0; h)) = \\mathbb{E}[\\widehat{F}(x_0; h)] - F(x_0) = - \\frac{h^2}{6}U^{(3)}(x_0) + O(h^4)\n$$\nThe leading-order squared bias is:\n$$\n(\\operatorname{Bias}(\\widehat{F}(x_0; h)))^2 \\approx \\left( - \\frac{h^2}{6}U^{(3)}(x_0) \\right)^2 = \\frac{h^4}{36}(U^{(3)}(x_0))^2\n$$\n\nNext, we calculate the variance of the estimator. This represents the error due to the random noise $\\varepsilon(x)$.\n$$\n\\operatorname{Var}(\\widehat{F}(x_0; h)) = \\operatorname{Var}\\left[-\\frac{\\tilde{U}(x_0 + h) - \\tilde{U}(x_0 - h)}{2h}\\right]\n$$\nUsing the property $\\operatorname{Var}(cZ) = c^2\\operatorname{Var}(Z)$:\n$$\n\\operatorname{Var}(\\widehat{F}(x_0; h)) = \\left(-\\frac{1}{2h}\\right)^2 \\operatorname{Var}[\\tilde{U}(x_0 + h) - \\tilde{U}(x_0 - h)] = \\frac{1}{4h^2} \\operatorname{Var}[\\tilde{U}(x_0 + h) - \\tilde{U}(x_0 - h)]\n$$\nSince $\\tilde{U}(x) = U(x) + \\varepsilon(x)$ and $U(x)$ is deterministic, $\\operatorname{Var}(\\tilde{U}(x)) = \\operatorname{Var}(\\varepsilon(x)) = \\sigma^2$. The noise terms $\\varepsilon(x_0+h)$ and $\\varepsilon(x_0-h)$ are independent, so their covariance is zero. Using the property $\\operatorname{Var}(X-Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y)$ for independent variables:\n$$\n\\operatorname{Var}[\\tilde{U}(x_0 + h) - \\tilde{U}(x_0 - h)] = \\operatorname{Var}(\\tilde{U}(x_0 + h)) + \\operatorname{Var}(\\tilde{U}(x_0 - h)) = \\sigma^2 + \\sigma^2 = 2\\sigma^2\n$$\nSubstituting this back into the variance expression for the estimator:\n$$\n\\operatorname{Var}(\\widehat{F}(x_0; h)) = \\frac{1}{4h^2} (2\\sigma^2) = \\frac{\\sigma^2}{2h^2}\n$$\n\nNow we combine the squared bias and the variance to obtain the leading-order mean-squared error, $\\operatorname{MSE}(h)$:\n$$\n\\operatorname{MSE}(h) = (\\operatorname{Bias})^2 + \\operatorname{Var} = \\frac{h^4}{36}(U^{(3)}(x_0))^2 + \\frac{\\sigma^2}{2h^2}\n$$\nTo find the step size $h$ that minimizes this error, we differentiate $\\operatorname{MSE}(h)$ with respect to $h$ and set the derivative to zero. Let $C_1 = \\frac{(U^{(3)}(x_0))^2}{36}$ and $C_2 = \\frac{\\sigma^2}{2}$. Then $\\operatorname{MSE}(h) = C_1 h^4 + C_2 h^{-2}$.\n$$\n\\frac{d}{dh}\\operatorname{MSE}(h) = \\frac{d}{dh}\\left( \\frac{h^4}{36}(U^{(3)}(x_0))^2 + \\frac{\\sigma^2}{2}h^{-2} \\right) = \\frac{4h^3}{36}(U^{(3)}(x_0))^2 - 2\\frac{\\sigma^2}{2}h^{-3}\n$$\n$$\n\\frac{d}{dh}\\operatorname{MSE}(h) = \\frac{h^3}{9}(U^{(3)}(x_0))^2 - \\frac{\\sigma^2}{h^3}\n$$\nSetting the derivative to zero to find the optimal step size, $h_{opt}$:\n$$\n\\frac{h_{opt}^3}{9}(U^{(3)}(x_0))^2 - \\frac{\\sigma^2}{h_{opt}^3} = 0\n$$\n$$\n\\frac{h_{opt}^6}{9}(U^{(3)}(x_0))^2 = \\sigma^2\n$$\n$$\nh_{opt}^6 = \\frac{9\\sigma^2}{(U^{(3)}(x_0))^2}\n$$\nTaking the positive real sixth root to find $h_{opt}$:\n$$\nh_{opt} = \\left( \\frac{9\\sigma^2}{(U^{(3)}(x_0))^2} \\right)^{\\frac{1}{6}}\n$$\nWe can simplify this expression:\n$$\nh_{opt} = \\frac{(9)^{\\frac{1}{6}} (\\sigma^2)^{\\frac{1}{6}}}{((U^{(3)}(x_0))^2)^{\\frac{1}{6}}} = \\frac{(3^2)^{\\frac{1}{6}} \\sigma^{\\frac{2}{6}}}{|U^{(3)}(x_0)|^{\\frac{2}{6}}} = \\frac{3^{\\frac{1}{3}} \\sigma^{\\frac{1}{3}}}{|U^{(3)}(x_0)|^{\\frac{1}{3}}}\n$$\nThis gives the final closed-form expression for the optimal step size:\n$$\nh_{opt} = \\left( \\frac{3\\sigma}{|U^{(3)}(x_0)|} \\right)^{\\frac{1}{3}}\n$$\nThis result encapsulates the trade-off: a larger step size is required to average out higher noise (larger $\\sigma$), whereas a smaller step size is needed to accurately capture the local curvature of the potential (larger $|U^{(3)}(x_0)|$). The assumption that $U^{(3)}(x_0)$ is nonzero ensures the denominator is well-defined.",
            "answer": "$$\n\\boxed{\\left( \\frac{3\\sigma}{|U^{(3)}(x_0)|} \\right)^{\\frac{1}{3}}}\n$$"
        },
        {
            "introduction": "Having explored the theory of numerical error, we now move to its practical implementation and validation. This coding practice  guides you to implement both first-order (forward-difference) and second-order (central-difference) gradient estimators and compare their accuracy against an exact analytical gradient. You will numerically confirm the theoretical error scaling with step size $h$ and see how these approximation errors can impact a key task in studying chemical reactions: the identification of stationary points on the potential energy surface. This exercise solidifies theoretical concepts by translating them into functional code, providing tangible evidence for the superior accuracy of higher-order methods.",
            "id": "3436103",
            "problem": "Let a potential energy surface (PES) be defined over a $d$-dimensional configuration space with coordinate vector $\\mathbf x = (x_1,\\dots,x_d)^\\top$, where $d \\in \\mathbb N$. In molecular dynamics, the force $\\mathbf F(\\mathbf x)$ acting on the system is given by the negative gradient of the potential energy, $\\mathbf F(\\mathbf x) = -\\nabla V(\\mathbf x)$. Consider the following dimensionless PES:\n$$\nV(\\mathbf x) = \\sum_{i=1}^d \\left( \\frac{1}{4} k_i x_i^4 - \\frac{1}{2} a_i x_i^2 \\right) + \\frac{1}{2} c \\sum_{i=1}^d m_i x_i^2,\n$$\nwhere $k_i > 0$, $a_i > 0$, $c > 0$, and $m_i > 0$ are parameters. For this $V(\\mathbf x)$, the analytic gradient components are defined by the fundamental rule $\\nabla V(\\mathbf x) = \\left( \\frac{\\partial V}{\\partial x_1}, \\dots, \\frac{\\partial V}{\\partial x_d} \\right)^\\top$.\n\nAssume $d=20$, choose $k_i$ linearly spaced in the interval $[0.8, 1.2]$, choose $a_i$ linearly spaced in the interval $[0.5, 1.5]$, choose $c=1$, and choose $m_i$ such that $m_i=1.8$ for $i \\le 10$ and $m_i=0.2$ for $i \\ge 11$. These choices make the Hessian $\\nabla^2 V(\\mathbf 0)$ indefinite, so that $\\mathbf x=\\mathbf 0$ is a saddle point of $V$.\n\nTo study the relationship between the PES and the force, and to assess numerical effects of gradient approximation on saddle identification, consider two finite-difference estimators of the gradient at a point $\\mathbf x$:\n- The forward-difference gradient $\\mathbf g^{\\mathrm{FD}}(\\mathbf x; h)$ whose $i$-th component is\n$$\ng^{\\mathrm{FD}}_i(\\mathbf x; h) = \\frac{V(\\mathbf x + h \\mathbf e_i) - V(\\mathbf x)}{h},\n$$\nwhere $\\mathbf e_i$ is the $i$-th standard basis vector and $h>0$ is the step size.\n- The central-difference gradient $\\mathbf g^{\\mathrm{CD}}(\\mathbf x; h)$ whose $i$-th component is\n$$\ng^{\\mathrm{CD}}_i(\\mathbf x; h) = \\frac{V(\\mathbf x + h \\mathbf e_i) - V(\\mathbf x - h \\mathbf e_i)}{2h}.\n$$\n\nYour tasks are as follows:\n1. Derive the analytic gradient $\\nabla V(\\mathbf x)$ for the given $V(\\mathbf x)$ and implement it.\n2. Implement the forward-difference and central-difference gradient estimators as defined above.\n3. At the test point $\\mathbf x^\\star \\in \\mathbb R^d$ with components $x^\\star_i = 0.1 \\cdot (-1)^i \\cdot \\frac{i}{d}$, evaluate the error norms\n$$\nE_{\\mathrm{FD}}(h) = \\left\\| \\mathbf g^{\\mathrm{FD}}(\\mathbf x^\\star; h) - \\nabla V(\\mathbf x^\\star) \\right\\|_2, \\quad\nE_{\\mathrm{CD}}(h) = \\left\\| \\mathbf g^{\\mathrm{CD}}(\\mathbf x^\\star; h) - \\nabla V(\\mathbf x^\\star) \\right\\|_2,\n$$\nfor the step-size list $h \\in \\{ 10^{-1}, 5\\cdot10^{-2}, 2.5\\cdot10^{-2}, 1.25\\cdot10^{-2}, 6.25\\cdot10^{-3} \\}$, and estimate the empirical scaling exponents $p_{\\mathrm{FD}}$ and $p_{\\mathrm{CD}}$ by linear least squares on the pairs $\\left(\\log h, \\log E_{\\mathrm{FD}}(h)\\right)$ and $\\left(\\log h, \\log E_{\\mathrm{CD}}(h)\\right)$, respectively.\n4. Assess saddle identification at $\\mathbf x=\\mathbf 0$ using the criterion $\\|\\widehat{\\nabla V}(\\mathbf 0; h)\\|_2  \\tau$, where $\\widehat{\\nabla V}$ is either $\\mathbf g^{\\mathrm{FD}}$ or $\\mathbf g^{\\mathrm{CD}}$. Evaluate this boolean criterion for three cases:\n   - Case A: forward-difference with $h = 10^{-1}$ and threshold $\\tau = 10^{-6}$.\n   - Case B: forward-difference with $h = 10^{-8}$ and threshold $\\tau = 10^{-6}$.\n   - Case C: central-difference with $h = 10^{-1}$ and threshold $\\tau = 10^{-12}$.\nAll quantities in this problem are dimensionless; no physical units are required.\n\nTest Suite and Final Output:\n- You must produce five results in total:\n  1. The float $p_{\\mathrm{FD}}$.\n  2. The float $p_{\\mathrm{CD}}$.\n  3. A boolean for Case A.\n  4. A boolean for Case B.\n  5. A boolean for Case C.\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order specified above. For example, your output must look like $[p_{\\mathrm{FD}},p_{\\mathrm{CD}},\\mathrm{A},\\mathrm{B},\\mathrm{C}]$ with no spaces.\n\nNo input should be read; all parameters and test cases are fixed as specified. The algorithmic derivations must be grounded in first principles: the gradient definition, finite difference definitions, and the properties of Taylor expansions used to infer error behavior. Avoid using or referencing shortcut formulas not derived from these bases.",
            "solution": "The user-provided problem is assessed to be **valid**. It is scientifically grounded in the principles of classical mechanics and numerical analysis, specifically concerning potential energy surfaces and finite difference methods. All parameters, variables, and tasks are defined unambiguously, rendering the problem well-posed and self-contained. The premises and objectives are objective and free of any factual or logical contradictions.\n\n### 1. Theoretical Framework\n\nThe problem is centered on the relationship between a potential energy surface $V(\\mathbf{x})$ and the corresponding force field $\\mathbf{F}(\\mathbf{x}) = -\\nabla V(\\mathbf{x})$ in a $d$-dimensional space. We are asked to compare the analytical gradient with its numerical approximations and analyze the consequences for identifying critical points.\n\n#### 1.1. Analytic Gradient\nThe potential energy surface is given by:\n$$\nV(\\mathbf x) = \\sum_{i=1}^d \\left( \\frac{1}{4} k_i x_i^4 - \\frac{1}{2} a_i x_i^2 \\right) + \\frac{1}{2} c \\sum_{i=1}^d m_i x_i^2\n$$\nThis can be written more compactly by combining the quadratic terms:\n$$\nV(\\mathbf x) = \\sum_{i=1}^d \\left( \\frac{1}{4} k_i x_i^4 + \\frac{1}{2} (c m_i - a_i) x_i^2 \\right)\n$$\nThe gradient $\\nabla V(\\mathbf x)$ is a vector whose components are the partial derivatives $\\frac{\\partial V}{\\partial x_j}$. Since the potential is a sum of functions of individual coordinates (i.e., $V(\\mathbf{x}) = \\sum_i V_i(x_i)$), the partial derivative with respect to $x_j$ only affects the $j$-th term of the sum.\n$$\n\\frac{\\partial V}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\frac{1}{4} k_j x_j^4 + \\frac{1}{2} (c m_j - a_j) x_j^2 \\right)\n$$\nApplying basic rules of differentiation:\n$$\n\\frac{\\partial V}{\\partial x_j} = \\frac{1}{4} k_j (4 x_j^3) + \\frac{1}{2} (c m_j - a_j) (2 x_j) = k_j x_j^3 + (c m_j - a_j) x_j\n$$\nThus, the $j$-th component of the analytic gradient vector $\\nabla V(\\mathbf{x})$ is $(\\nabla V(\\mathbf{x}))_j = k_j x_j^3 + (c m_j - a_j) x_j$.\n\n#### 1.2. Finite-Difference Approximations and Error Scaling\n\nThe accuracy of finite-difference methods is determined by their truncation error, which can be analyzed using Taylor series expansions.\n\n**Forward Difference (FD):**\nThe Taylor expansion of $V(\\mathbf{x} + h \\mathbf{e}_i)$ around $\\mathbf{x}$ in the $i$-th direction is:\n$$\nV(\\mathbf{x} + h \\mathbf{e}_i) = V(\\mathbf{x}) + h \\frac{\\partial V}{\\partial x_i}\\bigg|_{\\mathbf{x}} + \\frac{h^2}{2!} \\frac{\\partial^2 V}{\\partial x_i^2}\\bigg|_{\\mathbf{x}} + O(h^3)\n$$\nRearranging for the derivative gives:\n$$\n\\frac{\\partial V}{\\partial x_i}\\bigg|_{\\mathbf{x}} = \\frac{V(\\mathbf{x} + h \\mathbf{e}_i) - V(\\mathbf{x})}{h} - \\frac{h}{2} \\frac{\\partial^2 V}{\\partial x_i^2}\\bigg|_{\\mathbf{x}} - O(h^2)\n$$\nThe forward-difference estimator $g^{\\mathrm{FD}}_i(\\mathbf{x}; h)$ matches the first term. The leading error term is $-\\frac{h}{2} \\frac{\\partial^2 V}{\\partial x_i^2}$, which is of order $O(h)$. Therefore, the error norm $E_{\\mathrm{FD}}(h) = \\|\\mathbf{g}^{\\mathrm{FD}} - \\nabla V\\|_2$ is expected to be proportional to $h$. On a log-log plot of error versus step size, this corresponds to a line with slope $p_{\\mathrm{FD}} \\approx 1$.\n\n**Central Difference (CD):**\nThe Taylor expansions for $V(\\mathbf{x} \\pm h \\mathbf{e}_i)$ are:\n$$\nV(\\mathbf{x} + h \\mathbf{e}_i) = V(\\mathbf{x}) + h \\frac{\\partial V}{\\partial x_i} + \\frac{h^2}{2} \\frac{\\partial^2 V}{\\partial x_i^2} + \\frac{h^3}{6} \\frac{\\partial^3 V}{\\partial x_i^3} + O(h^4)\n$$\n$$\nV(\\mathbf{x} - h \\mathbf{e}_i) = V(\\mathbf{x}) - h \\frac{\\partial V}{\\partial x_i} + \\frac{h^2}{2} \\frac{\\partial^2 V}{\\partial x_i^2} - \\frac{h^3}{6} \\frac{\\partial^3 V}{\\partial x_i^3} + O(h^4)\n$$\nSubtracting the second from the first cancels the even-order derivative terms:\n$$\nV(\\mathbf{x} + h \\mathbf{e}_i) - V(\\mathbf{x} - h \\mathbf{e}_i) = 2h \\frac{\\partial V}{\\partial x_i} + \\frac{2h^3}{6} \\frac{\\partial^3 V}{\\partial x_i^3} + O(h^5)\n$$\nRearranging for the derivative gives:\n$$\n\\frac{\\partial V}{\\partial x_i} = \\frac{V(\\mathbf{x} + h \\mathbf{e}_i) - V(\\mathbf{x} - h \\mathbf{e}_i)}{2h} - \\frac{h^2}{6} \\frac{\\partial^3 V}{\\partial x_i^3} - O(h^4)\n$$\nThe central-difference estimator $g^{\\mathrm{CD}}_i(\\mathbf{x}; h)$ matches the first term. The leading error term is $-\\frac{h^2}{6} \\frac{\\partial^3 V}{\\partial x_i^3}$, which is of order $O(h^2)$. Therefore, the error norm $E_{\\mathrm{CD}}(h) = \\|\\mathbf{g}^{\\mathrm{CD}} - \\nabla V\\|_2$ is expected to be proportional to $h^2$. This corresponds to a slope $p_{\\mathrm{CD}} \\approx 2$ on a log-log plot.\n\n### 2. Algorithmic Implementation\n\nThe solution will be implemented in Python using the `numpy` library, following the sequence of tasks.\n\n1.  **Parameter Setup**: The constants $d=20$ and $c=1.0$ are defined. The parameter vectors $\\mathbf{k}$, $\\mathbf{a}$, and $\\mathbf{m}$ are generated according to the specified rules. The test point $\\mathbf{x}^\\star$ is constructed.\n\n2.  **Function Implementation**: Four functions are implemented:\n    -   `potential_V(x, ...)`: Computes the scalar potential energy $V(\\mathbf{x})$.\n    -   `analytic_grad(x, ...)`: Computes the vector $\\nabla V(\\mathbf{x})$ using the derived analytical formula.\n    -   `forward_diff_grad(x, h, ...)`: Computes the vector $\\mathbf{g}^{\\mathrm{FD}}(\\mathbf{x}; h)$ by applying its definition for each component.\n    -   `central_diff_grad(x, h, ...)`: Computes the vector $\\mathbf{g}^{\\mathrm{CD}}(\\mathbf{x}; h)$ similarly.\n\n3.  **Error Scaling Analysis (Task 3)**:\n    -   The analytic gradient $\\nabla V(\\mathbf{x}^\\star)$ is computed once.\n    -   A loop iterates through the given list of step sizes $h$. In each iteration, $\\mathbf{g}^{\\mathrm{FD}}(\\mathbf{x}^\\star; h)$ and $\\mathbf{g}^{\\mathrm{CD}}(\\mathbf{x}^\\star; h)$ are calculated.\n    -   The Euclidean norms of the error vectors, $E_{\\mathrm{FD}}(h)$ and $E_{\\mathrm{CD}}(h)$, are computed and stored.\n    -   The logarithms of the step sizes and error norms are calculated.\n    -   `numpy.polyfit` is used to perform a linear least-squares regression on the $(\\log h, \\log E)$ data for both FD and CD cases. The slope of the resulting line provides the estimated scaling exponents $p_{\\mathrm{FD}}$ and $p_{\\mathrm{CD}}$.\n\n4.  **Saddle Point Analysis (Task 4)**:\n    -   This task is performed at the origin, $\\mathbf{x} = \\mathbf{0}$. The true gradient is $\\nabla V(\\mathbf{0}) = \\mathbf{0}$.\n    -   The analysis for $\\mathbf{g}^{\\mathrm{CD}}$ at $\\mathbf{x}=\\mathbf{0}$ shows a special case. The potential $V(\\mathbf{x})$ is an even function with respect to each coordinate $x_i$ (it only contains $x_i^4$ and $x_i^2$ terms). Therefore, $V(h\\mathbf{e}_i) = V(-h\\mathbf{e}_i)$. This causes the numerator in the central difference formula to be exactly zero:\n        $$\n        g^{\\mathrm{CD}}_i(\\mathbf{0}; h) = \\frac{V(h\\mathbf{e}_i) - V(-h\\mathbf{e}_i)}{2h} = 0\n        $$\n        This holds true in exact arithmetic and also in standard floating-point arithmetic, as the computations for $V(h\\mathbf{e}_i)$ and $V(-h\\mathbf{e}_i)$ are identical. Thus, $\\|\\mathbf{g}^{\\mathrm{CD}}(\\mathbf{0}; h)\\|_2 = 0$.\n    -   The forward difference gradient at the origin is non-zero:\n        $$\n        g^{\\mathrm{FD}}_i(\\mathbf{0}; h) = \\frac{V(h\\mathbf{e}_i) - V(\\mathbf{0})}{h} = \\frac{1}{h} \\left( \\frac{1}{4}k_i h^4 + \\frac{1}{2}(c m_i - a_i)h^2 \\right) = \\frac{1}{2}(c m_i - a_i)h + \\frac{1}{4}k_i h^3\n        $$\n        For small $h$, this is approximately linear in $h$.\n    -   The three boolean criteria are evaluated:\n        -   **Case A**: The norm of $\\mathbf{g}^{\\mathrm{FD}}(\\mathbf{0}; h=10^{-1})$ is calculated and compared to $\\tau = 10^{-6}$. As the norm will be $O(h)$, it is expected to be much larger than $\\tau$.\n        -   **Case B**: The norm of $\\mathbf{g}^{\\mathrm{FD}}(\\mathbf{0}; h=10^{-8})$ is calculated and compared to $\\tau = 10^{-6}$. As the norm is $O(h)$, it is expected to be smaller than $\\tau$.\n        -   **Case C**: The norm of $\\mathbf{g}^{\\mathrm{CD}}(\\mathbf{0}; h=10^{-1})$ is calculated and compared to $\\tau = 10^{-12}$. As argued, this norm will be numerically zero, thus satisfying the condition.\n\nThe final results are collected and formatted into the required string output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the numerical analysis problem by deriving and comparing analytic\n    and finite-difference gradients of a potential energy surface.\n    \"\"\"\n    # 1. Define constants and parameters\n    d = 20\n    c = 1.0\n    k = np.linspace(0.8, 1.2, d)\n    a = np.linspace(0.5, 1.5, d)\n    m = np.zeros(d)\n    m[:10] = 1.8  # for i = 10 (1-based), which is index 0-9 (0-based)\n    m[10:] = 0.2 # for i >= 11 (1-based), which is index 10-19 (0-based)\n\n    # 2. Define the potential and gradient functions\n\n    def potential_v(x_vec, k_vec, a_vec, c_val, m_vec):\n        \"\"\"Computes the potential energy V(x).\"\"\"\n        term1 = 0.25 * k_vec * x_vec**4\n        term2 = -0.5 * a_vec * x_vec**2\n        term3 = 0.5 * c_val * m_vec * x_vec**2\n        return np.sum(term1 + term2 + term3)\n\n    def analytic_grad(x_vec, k_vec, a_vec, c_val, m_vec):\n        \"\"\"Computes the analytic gradient of V(x).\"\"\"\n        return k_vec * x_vec**3 + (c_val * m_vec - a_vec) * x_vec\n\n    def forward_diff_grad(x_vec, h, k_vec, a_vec, c_val, m_vec):\n        \"\"\"Computes the forward-difference gradient.\"\"\"\n        grad = np.zeros_like(x_vec)\n        v_at_x = potential_v(x_vec, k_vec, a_vec, c_val, m_vec)\n        for i in range(len(x_vec)):\n            x_plus_h = x_vec.copy()\n            x_plus_h[i] += h\n            v_at_x_plus_h = potential_v(x_plus_h, k_vec, a_vec, c_val, m_vec)\n            grad[i] = (v_at_x_plus_h - v_at_x) / h\n        return grad\n    \n    def central_diff_grad(x_vec, h, k_vec, a_vec, c_val, m_vec):\n        \"\"\"Computes the central-difference gradient.\"\"\"\n        grad = np.zeros_like(x_vec)\n        for i in range(len(x_vec)):\n            x_plus_h = x_vec.copy()\n            x_plus_h[i] += h\n            v_at_x_plus_h = potential_v(x_plus_h, k_vec, a_vec, c_val, m_vec)\n            \n            x_minus_h = x_vec.copy()\n            x_minus_h[i] -= h\n            v_at_x_minus_h = potential_v(x_minus_h, k_vec, a_vec, c_val, m_vec)\n            \n            grad[i] = (v_at_x_plus_h - v_at_x_minus_h) / (2.0 * h)\n        return grad\n    \n    # 3. Task 3: Error norms and scaling exponents\n    i_vals = np.arange(1, d + 1)\n    x_star = 0.1 * ((-1)**i_vals) * i_vals / d\n    \n    h_values = np.array([1e-1, 5e-2, 2.5e-2, 1.25e-2, 6.25e-3])\n    \n    E_fd = []\n    E_cd = []\n    \n    grad_analytic_at_x_star = analytic_grad(x_star, k, a, c, m)\n    \n    for h in h_values:\n        # Forward difference error\n        grad_fd = forward_diff_grad(x_star, h, k, a, c, m)\n        error_fd = np.linalg.norm(grad_fd - grad_analytic_at_x_star)\n        E_fd.append(error_fd)\n        \n        # Central difference error\n        grad_cd = central_diff_grad(x_star, h, k, a, c, m)\n        error_cd = np.linalg.norm(grad_cd - grad_analytic_at_x_star)\n        E_cd.append(error_cd)\n        \n    # Perform linear regression on log-log data to find scaling exponents\n    log_h = np.log(h_values)\n    log_E_fd = np.log(np.array(E_fd))\n    log_E_cd = np.log(np.array(E_cd))\n    \n    # polyfit returns [slope, intercept]\n    p_fd = np.polyfit(log_h, log_E_fd, 1)[0]\n    p_cd = np.polyfit(log_h, log_E_cd, 1)[0]\n    \n    # 4. Task 4: Saddle point identification at x = 0\n    x_zero = np.zeros(d)\n    \n    # Case A\n    h_A = 1e-1\n    tau_A = 1e-6\n    grad_fd_A = forward_diff_grad(x_zero, h_A, k, a, c, m)\n    norm_A = np.linalg.norm(grad_fd_A)\n    case_A = norm_A  tau_A\n    \n    # Case B\n    h_B = 1e-8\n    tau_B = 1e-6\n    grad_fd_B = forward_diff_grad(x_zero, h_B, k, a, c, m)\n    norm_B = np.linalg.norm(grad_fd_B)\n    case_B = norm_B  tau_B\n\n    # Case C\n    h_C = 1e-1\n    tau_C = 1e-12\n    grad_cd_C = central_diff_grad(x_zero, h_C, k, a, c, m)\n    norm_C = np.linalg.norm(grad_cd_C)\n    case_C = norm_C  tau_C\n    \n    results = [p_fd, p_cd, case_A, case_B, case_C]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "The relationship $\\mathbf{F} = -\\nabla V$ is the bedrock of energy conservation in classical simulations. But what happens if the forces used by the integrator are not perfectly conservative, a situation that can arise from approximations in complex force fields or multi-scale models? This practice  explores this critical question by running a dynamical simulation where the force field contains a non-conservative component. By tracking the system's total mechanical energy, you will directly observe and quantify the resulting energy drift, a direct violation of the work-energy theorem for conservative systems, and learn why this drift serves as a crucial diagnostic for simulation quality.",
            "id": "3436106",
            "problem": "Consider a single particle of mass $m$ moving in two spatial dimensions under a smooth scalar potential $V(\\mathbf r)$, where $\\mathbf r \\in \\mathbb{R}^2$. The fundamental base is Newton's Second Law, the Work-Energy theorem, and the definition of potential forces. The physically implemented force field in a molecular dynamics code is often an approximation to a conservative force, and in this problem it is modeled as\n$$\n\\tilde{\\mathbf F}(\\mathbf r) = -\\nabla V(\\mathbf r) + \\boldsymbol\\delta(\\mathbf r),\n$$\nwhere $\\boldsymbol\\delta(\\mathbf r)$ is a non-conservative force field that does not derive from any scalar potential energy surface. You will study the energy drift that results from $\\boldsymbol\\delta(\\mathbf r)\\neq \\mathbf 0$, its scaling with the magnitude of $\\boldsymbol\\delta(\\mathbf r)$ and the timestep $\\,\\Delta t\\,$, and implement a correction based on non-conservative work accounting.\n\nYou must write a complete, runnable program that carries out the following, entirely in $2$ dimensions:\n1. Define the potential energy surface\n$$\nV(\\mathbf r) = \\frac{k}{2}\\left(x^2 + y^2\\right),\n$$\nwith parameters $m = 1$ and $k = 1$ (reduced units). Define the non-conservative perturbation to the force as\n$$\n\\boldsymbol\\delta(\\mathbf r) = \\alpha\\,\\mathbf R\\,\\mathbf r,\n$$\nwhere $\\alpha$ is a scalar amplitude, $\\mathbf r = (x,y)$, and $\\mathbf R$ is the constant $2\\times 2$ matrix\n$$\n\\mathbf R = \\begin{pmatrix}0  -1\\\\ 1  0\\end{pmatrix}.\n$$\nThis $\\boldsymbol\\delta(\\mathbf r)$ has nonzero curl and cannot be expressed as $-\\nabla U(\\mathbf r)$ for any scalar $U$, making it a non-conservative error representative of many realistic force-approximation artifacts.\n\n2. Propagate trajectories using the velocity-Verlet time discretization for the total force $\\tilde{\\mathbf F}(\\mathbf r)$ with time step $\\Delta t$ for a total physical time $T$. Use fixed initial conditions $\\,\\mathbf r(0) = (1,0)\\,$ and $\\,\\mathbf v(0) = (0,1)\\,$, and fixed parameters $\\,m=1\\,$ and $\\,k=1\\,$. All quantities are in reduced units. Record at every discrete step $n$ the total mechanical energy\n$$\nE_n = \\frac{m}{2}\\|\\mathbf v_n\\|^2 + V(\\mathbf r_n),\n$$\nexpressed in reduced energy units.\n\n3. For a given trajectory, estimate the energy drift rate (in reduced energy per reduced time) by performing an ordinary least-squares linear fit of $E_n$ versus $t_n = n\\,\\Delta t$ and report the fitted slope. This estimates the systematic drift of the energy under the given integration and force model.\n\n4. Implement a work-accounting correction for non-conservative forces by defining a corrected energy\n$$\nE_n^{\\mathrm{corr}} = E_n - W_n^{\\mathrm{nc}},\n$$\nwhere $W_n^{\\mathrm{nc}}$ is a discrete approximation to the accumulated non-conservative work\n$$\nW^{\\mathrm{nc}}(t) = \\int_0^t \\mathbf v(\\tau)\\cdot \\boldsymbol\\delta(\\mathbf r(\\tau))\\,d\\tau.\n$$\nYou must approximate this integral numerically using a midpoint rule at each step,\n$$\nW_{n+1}^{\\mathrm{nc}} \\approx W_n^{\\mathrm{nc}} + \\Delta t\\,\\mathbf v_{n+\\frac{1}{2}}\\cdot \\boldsymbol\\delta(\\mathbf r_{n+\\frac{1}{2}}),\n$$\nwith midpoints $\\mathbf v_{n+\\frac{1}{2}} = \\frac{1}{2}(\\mathbf v_n + \\mathbf v_{n+1})$ and $\\mathbf r_{n+\\frac{1}{2}} = \\frac{1}{2}(\\mathbf r_n + \\mathbf r_{n+1})$. Estimate the drift rate of $E_n^{\\mathrm{corr}}$ by a linear fit versus $t_n$ as in step $3$.\n\n5. To study scaling with the magnitude of the non-conservative perturbation and with the timestep, use the following test suite. For all tests use $T=50$ (reduced time units):\n   - Test A (baseline conservative): $\\alpha = 0$, $\\Delta t = 0.02$.\n   - Test B (non-conservative, coarse step): $\\alpha = 1\\times 10^{-3}$, $\\Delta t = 0.02$.\n   - Test C (non-conservative, fine step): $\\alpha = 1\\times 10^{-3}$, $\\Delta t = 0.01$.\n   - Test D (linearity in $\\alpha$): $\\alpha = 2\\times 10^{-3}$, $\\Delta t = 0.02$.\n\nFor each of Tests Aâ€“D, compute the energy drift slope as in step $3$. Additionally, for Tests B and C compute the corrected energy drift slope as in step $4$.\n\n6. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n   - The energy drift slope for Test A (a float, in reduced energy per reduced time).\n   - The energy drift slope for Test B (a float).\n   - The energy drift slope for Test C (a float).\n   - The energy drift slope for Test D (a float).\n   - The ratio of slopes $s_D/s_B$ to assess linear scaling in $\\,\\alpha\\,$ (a float without units).\n   - The finite-difference estimate of the timestep-squared bias coefficient\n     $$\n     c_{\\Delta t^2} = \\frac{s_B - s_C}{(0.02)^2 - (0.01)^2}\n     $$\n     (a float in reduced energy per reduced time cubed).\n   - The corrected energy drift slope for Test B (a float).\n   - The corrected energy drift slope for Test C (a float).\nAll slopes must be expressed in reduced energy per reduced time units. The final print should be exactly a single line in the format $[r_1,r_2,r_3,r_4,r_5,r_6,r_7,r_8]$ with no extra characters or whitespace.\n\nYour derivations and code must start from first principles and well-tested facts only, specifically Newton's Second Law, definitions of kinetic and potential energy, and the Work-Energy theorem. You must not introduce or assume any final target formulas in the problem statement. Ensure all numerical choices are scientifically consistent and use only the parameter values specified above. The program must be fully self-contained and require no external input.",
            "solution": "The problem requires a numerical study of a particle's dynamics in two dimensions under a composite force field, consisting of a conservative harmonic potential and a non-conservative rotational perturbation. The objective is to implement a numerical integration scheme, analyze the resulting energy drift, and evaluate a work-accounting correction method. We will use the velocity-Verlet algorithm for time propagation and ordinary least-squares regression to quantify energy drift rates.\n\n### Theoretical Framework\n\n**1. Equations of Motion**\n\nThe particle has a mass $m=1$ (in reduced units). Its motion is governed by Newton's Second Law:\n$$\nm \\frac{d^2\\mathbf{r}}{dt^2} = \\tilde{\\mathbf{F}}(\\mathbf{r})\n$$\nwhere $\\mathbf{r}(t) = (x(t), y(t))$ is the position vector and $\\tilde{\\mathbf{F}}(\\mathbf{r})$ is the total force. The force is given as the sum of a conservative force derived from a potential $V(\\mathbf{r})$ and a non-conservative perturbation $\\boldsymbol{\\delta}(\\mathbf{r})$:\n$$\n\\tilde{\\mathbf{F}}(\\mathbf{r}) = -\\nabla V(\\mathbf{r}) + \\boldsymbol{\\delta}(\\mathbf{r})\n$$\nThe potential energy is a two-dimensional isotropic harmonic oscillator:\n$$\nV(\\mathbf{r}) = \\frac{k}{2}(x^2 + y^2)\n$$\nWith the parameter $k=1$, the conservative force component is:\n$$\n-\\nabla V(\\mathbf{r}) = -\\left(\\frac{\\partial V}{\\partial x}, \\frac{\\partial V}{\\partial y}\\right) = -(kx, ky) = (-x, -y) = -\\mathbf{r}\n$$\nThe non-conservative component is defined as:\n$$\n\\boldsymbol{\\delta}(\\mathbf{r}) = \\alpha \\mathbf{R} \\mathbf{r} = \\alpha \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\alpha \\begin{pmatrix} -y \\\\ x \\end{pmatrix}\n$$\nwhere $\\alpha$ is a scalar amplitude. This force field has a non-zero curl, $\\nabla \\times \\boldsymbol{\\delta} = 2\\alpha \\, \\hat{\\mathbf{z}}$, confirming it is non-conservative for $\\alpha \\neq 0$.\n\nCombining these, the total force on the particle is:\n$$\n\\tilde{\\mathbf{F}}(x, y) = (-x - \\alpha y, -y + \\alpha x)\n$$\n\n**2. Work and Energy**\n\nThe total mechanical energy $E$ is the sum of kinetic energy $K$ and potential energy $V$:\n$$\nE = K + V = \\frac{1}{2} m \\|\\mathbf{v}\\|^2 + V(\\mathbf{r})\n$$\nwhere $\\mathbf{v} = d\\mathbf{r}/dt$ is the velocity. The time rate of change of energy is given by the power exerted by the total force:\n$$\n\\frac{dE}{dt} = \\frac{d}{dt}\\left(\\frac{1}{2}m\\mathbf{v}\\cdot\\mathbf{v} + V(\\mathbf{r})\\right) = m\\mathbf{v}\\cdot\\frac{d\\mathbf{v}}{dt} + \\nabla V \\cdot \\frac{d\\mathbf{r}}{dt} = \\mathbf{v} \\cdot (m\\ddot{\\mathbf{r}}) + \\mathbf{v}\\cdot\\nabla V\n$$\nSubstituting $m\\ddot{\\mathbf{r}} = \\tilde{\\mathbf{F}} = -\\nabla V + \\boldsymbol{\\delta}$:\n$$\n\\frac{dE}{dt} = \\mathbf{v} \\cdot (-\\nabla V + \\boldsymbol{\\delta}) + \\mathbf{v}\\cdot\\nabla V = -\\mathbf{v}\\cdot\\nabla V + \\mathbf{v}\\cdot\\boldsymbol{\\delta} + \\mathbf{v}\\cdot\\nabla V = \\mathbf{v}\\cdot\\boldsymbol{\\delta}\n$$\nThe rate of change of total mechanical energy is equal to the power delivered by the non-conservative force. Integrating over time from $0$ to $t$ yields the Work-Energy theorem for non-conservative systems:\n$$\nE(t) - E(0) = \\int_0^t \\mathbf{v}(\\tau) \\cdot \\boldsymbol{\\delta}(\\mathbf{r}(\\tau)) d\\tau = W^{\\mathrm{nc}}(t)\n$$\nwhere $W^{\\mathrm{nc}}(t)$ is the work done by the non-conservative force. This equation shows that if $\\boldsymbol{\\delta} \\neq \\boldsymbol{0}$, the mechanical energy $E$ will generally not be conserved.\n\n**3. Numerical Integration: Velocity-Verlet Algorithm**\n\nTo solve the equations of motion numerically, we use the velocity-Verlet algorithm, a time-reversible and symplectic integrator known for good long-term energy stability in conservative systems. Given the state $(\\mathbf{r}_n, \\mathbf{v}_n)$ at time $t_n = n\\Delta t$, the state at $t_{n+1}$ is computed as follows:\n1. Compute the force at the current position: $\\tilde{\\mathbf{F}}_n = \\tilde{\\mathbf{F}}(\\mathbf{r}_n)$.\n2. Update velocity to a half-step: $\\mathbf{v}_{n+1/2} = \\mathbf{v}_n + \\frac{\\Delta t}{2m} \\tilde{\\mathbf{F}}_n$.\n3. Update position to the full-step: $\\mathbf{r}_{n+1} = \\mathbf{r}_n + \\Delta t \\, \\mathbf{v}_{n+1/2}$.\n4. Compute the force at the new position: $\\tilde{\\mathbf{F}}_{n+1} = \\tilde{\\mathbf{F}}(\\mathbf{r}_{n+1})$.\n5. Update velocity to the full-step: $\\mathbf{v}_{n+1} = \\mathbf{v}_{n+1/2} + \\frac{\\Delta t}{2m} \\tilde{\\mathbf{F}}_{n+1}$.\n\n**4. Energy Drift Analysis and Correction**\n\nThe simulation will produce a time series of energy values $E_n = \\frac{m}{2}\\|\\mathbf{v}_n\\|^2 + V(\\mathbf{r}_n)$ at times $t_n = n\\Delta t$. We estimate the systematic energy drift rate, $s$, by performing an ordinary least-squares linear fit of the data points $(t_n, E_n)$, finding the slope of the best-fit line.\n\nTo compensate for the physical energy drift, we define a corrected energy, $E^{\\mathrm{corr}}$. Based on the work-energy theorem, we have $E(t) - W^{\\mathrm{nc}}(t) = E(0)$, which implies that the quantity $E^{\\mathrm{corr}}(t) = E(t) - W^{\\mathrm{nc}}(t)$ should be conserved. We compute a discrete analogue, $E_n^{\\mathrm{corr}} = E_n - W_n^{\\mathrm{nc}}$, where $W_n^{\\mathrm{nc}}$ is the accumulated non-conservative work up to step $n$. This work is calculated by summing incremental contributions using the trapezoidal rule for the integral over each time step $[t_n, t_{n+1}]$:\n$$\nW_{n+1}^{\\mathrm{nc}} = W_n^{\\mathrm{nc}} + \\Delta t \\, \\mathbf{v}_{n+1/2}^{\\mathrm{mid}} \\cdot \\boldsymbol{\\delta}(\\mathbf{r}_{n+1/2}^{\\mathrm{mid}})\n$$\nwhere the mid-point values are defined as the averages of the interval endpoints:\n$$\n\\mathbf{r}_{n+1/2}^{\\mathrm{mid}} = \\frac{1}{2}(\\mathbf{r}_n + \\mathbf{r}_{n+1}), \\quad \\mathbf{v}_{n+1/2}^{\\mathrm{mid}} = \\frac{1}{2}(\\mathbf{v}_n + \\mathbf{v}_{n+1})\n$$\nThe drift rate of this corrected energy, $s^{\\mathrm{corr}}$, is also computed via linear regression. Any residual drift in $E^{\\mathrm{corr}}$ is attributable to numerical errors in the Verlet algorithm and the trapezoidal approximation of the work integral.\n\n### Computational Procedure\n\nFor each test case (A, B, C, D) defined by a specific $(\\alpha, \\Delta t)$ pair, we will perform the following steps:\n1. Set initial conditions $\\mathbf{r}(0)=(1,0)$, $\\mathbf{v}(0)=(0,1)$, and parameters $m=1, k=1$.\n2. Simulate the trajectory for a total time $T=50$ using the velocity-Verlet algorithm.\n3. At each step $n$, calculate and store the time $t_n$, the total energy $E_n$, and the corrected energy $E_n^{\\mathrm{corr}}$ (if applicable).\n4. After the simulation, perform a linear regression on the time series $(t_n, E_n)$ to find the uncorrected energy drift slope $s$.\n5. For tests B and C, also perform a linear regression on $(t_n, E_n^{\\mathrm{corr}})$ to find the corrected energy drift slope $s^{\\mathrm{corr}}$.\n6. Finally, calculate the derived quantities: the ratio $s_D/s_B$ to check for linearity in $\\alpha$, and the coefficient $c_{\\Delta t^2}$ to analyze the timestep-dependent error. The results will be reported in the specified order.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(alpha, dt, T, r0, v0, m, k, compute_corrected_energy):\n    \"\"\"\n    Runs a single particle simulation using the velocity-Verlet algorithm.\n\n    Args:\n        alpha (float): Amplitude of the non-conservative force.\n        dt (float): Time step.\n        T (float): Total simulation time.\n        r0 (np.ndarray): Initial position vector.\n        v0 (np.ndarray): Initial velocity vector.\n        m (float): Mass of the particle.\n        k (float): Spring constant of the potential.\n        compute_corrected_energy (bool): Flag to compute corrected energy.\n\n    Returns:\n        tuple: A tuple containing:\n            - slope (float): The drift slope of the uncorrected energy.\n            - corrected_slope (float): The drift slope of the corrected energy,\n                                      or 0.0 if not computed.\n    \"\"\"\n    num_steps = int(T / dt)\n    \n    # Define force functions\n    R_matrix = np.array([[0., -1.], [1., 0.]])\n    \n    def conservative_force(r):\n        return -k * r\n        \n    def non_conservative_force(r):\n        return alpha * np.dot(R_matrix, r)\n\n    def total_force(r):\n        return conservative_force(r) + non_conservative_force(r)\n        \n    def potential_energy(r):\n        return 0.5 * k * np.dot(r, r)\n        \n    # History arrays\n    t_hist = np.linspace(0, T, num_steps + 1)\n    E_hist = np.zeros(num_steps + 1)\n    E_corr_hist = np.zeros(num_steps + 1)\n\n    # Initial state\n    r = r0.copy()\n    v = v0.copy()\n    W_nc = 0.0\n\n    # Store initial energy\n    E_hist[0] = 0.5 * m * np.dot(v, v) + potential_energy(r)\n    E_corr_hist[0] = E_hist[0]\n\n    # Main integration loop\n    for i in range(num_steps):\n        # Velocity-Verlet Integration\n        F = total_force(r)\n        v_half = v + (0.5 * dt / m) * F\n        r_next = r + dt * v_half\n        F_next = total_force(r_next)\n        v_next = v_half + (0.5 * dt / m) * F_next\n        \n        # Calculate uncorrected energy at step i+1\n        E_hist[i+1] = 0.5 * m * np.dot(v_next, v_next) + potential_energy(r_next)\n        \n        # Calculate work correction and corrected energy\n        if compute_corrected_energy:\n            r_mid = 0.5 * (r + r_next)\n            v_mid = 0.5 * (v + v_next)\n            delta_F_mid = non_conservative_force(r_mid)\n            dW_nc = dt * np.dot(v_mid, delta_F_mid)\n            W_nc += dW_nc\n        \n        E_corr_hist[i+1] = E_hist[i+1] - W_nc\n        \n        # Update state for next iteration\n        r, v = r_next, v_next\n\n    # Perform linear regression to find slopes\n    slope = np.polyfit(t_hist, E_hist, 1)[0]\n    \n    corrected_slope = 0.0\n    if compute_corrected_energy:\n        corrected_slope = np.polyfit(t_hist, E_corr_hist, 1)[0]\n        \n    return slope, corrected_slope\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Fixed parameters\n    m = 1.0\n    k = 1.0\n    T = 50.0\n    r0 = np.array([1.0, 0.0])\n    v0 = np.array([0.0, 1.0])\n\n    # Test cases parameters: (alpha, dt)\n    params = {\n        'A': {'alpha': 0.0, 'dt': 0.02, 'correct': False},\n        'B': {'alpha': 1e-3, 'dt': 0.02, 'correct': True},\n        'C': {'alpha': 1e-3, 'dt': 0.01, 'correct': True},\n        'D': {'alpha': 2e-3, 'dt': 0.02, 'correct': False},\n    }\n\n    # Run simulations and store results\n    slopes = {}\n    corrected_slopes = {}\n    for test_name, p in params.items():\n        s, s_corr = run_simulation(p['alpha'], p['dt'], T, r0, v0, m, k, p['correct'])\n        slopes[test_name] = s\n        if p['correct']:\n            corrected_slopes[test_name] = s_corr\n\n    s_A = slopes['A']\n    s_B = slopes['B']\n    s_C = slopes['C']\n    s_D = slopes['D']\n    \n    # Calculate derived quantities\n    # 5. Ratio of slopes s_D/s_B\n    ratio_sD_sB = s_D / s_B if s_B != 0 else 0.0\n    \n    # 6. Finite-difference estimate of the timestep-squared bias coefficient\n    dt_B = params['B']['dt']\n    dt_C = params['C']['dt']\n    c_dt2 = (s_B - s_C) / (dt_B**2 - dt_C**2)\n    \n    # 7  8. Corrected energy drift slopes\n    s_B_corr = corrected_slopes['B']\n    s_C_corr = corrected_slopes['C']\n\n    # Assemble final results list\n    results = [\n        s_A,\n        s_B,\n        s_C,\n        s_D,\n        ratio_sD_sB,\n        c_dt2,\n        s_B_corr,\n        s_C_corr,\n    ]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}