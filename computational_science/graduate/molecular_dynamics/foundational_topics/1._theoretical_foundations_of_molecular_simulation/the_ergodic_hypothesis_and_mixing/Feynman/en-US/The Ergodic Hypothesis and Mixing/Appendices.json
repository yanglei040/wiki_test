{
    "hands_on_practices": [
        {
            "introduction": "This first practice takes you to the historical roots of ergodicity studies in computational physics. You will recreate the famous Fermi–Pasta–Ulam–Tsingou (FPUT) experiment, a seemingly simple chain of masses and springs that produced unexpectedly complex, non-ergodic behavior . By implementing a microcanonical molecular dynamics simulation, you will measure the equipartition time of energy among the system's normal modes, providing a direct, hands-on observation of how a system transitions from near-integrable, non-ergodic dynamics at low energy to chaotic, ergodic behavior at higher energies.",
            "id": "3452493",
            "problem": "Consider a one-dimensional Fermi–Pasta–Ulam–Tsingou (FPUT) chain of $N$ identical point masses connected by nearest-neighbor springs, with fixed ends at both boundaries. Let the mass of each particle be $m=1$ and the spring constants be chosen such that the quadratic stiffness is unity and a small quartic nonlinearity of strength $\\beta$ modifies the potential. The Hamiltonian of the $\\beta$-FPUT chain is\n$$\nH(\\mathbf{x},\\mathbf{v}) \\;=\\; \\sum_{i=1}^{N} \\frac{v_i^2}{2} \\;+\\; \\sum_{i=0}^{N} \\left[ \\frac{1}{2} \\Delta_i^2 \\;+\\; \\frac{\\beta}{4} \\Delta_i^4 \\right],\n$$\nwhere $\\Delta_i = x_{i+1} - x_i$, $x_0 = 0$, $x_{N+1} = 0$, $\\mathbf{x} = (x_1,\\dots,x_N)$ are the displacements, and $\\mathbf{v} = (v_1,\\dots,v_N)$ are the velocities.\n\nUsing Newton's second law of motion, the microcanonical molecular dynamics (constant energy ensemble) follows from\n$$\n\\dot{x}_i = v_i, \\qquad \\dot{v}_i = f_i(\\mathbf{x}),\n$$\nwhere the force $f_i$ is derived from the negative gradient of the total potential energy:\n$$\nf_i(\\mathbf{x}) \\;=\\; \\left( \\Delta_i + \\beta \\Delta_i^3 \\right) \\;-\\; \\left( \\Delta_{i-1} + \\beta \\Delta_{i-1}^3 \\right), \\quad i=1,\\dots,N.\n$$\nYou must simulate these equations using the velocity-Verlet integrator, which for a time step $\\Delta t$ updates $(\\mathbf{x},\\mathbf{v})$ as\n$$\n\\mathbf{v}\\left(t+\\frac{\\Delta t}{2}\\right) = \\mathbf{v}(t) + \\frac{\\Delta t}{2}\\,\\mathbf{f}\\left(\\mathbf{x}(t)\\right),\n$$\n$$\n\\mathbf{x}(t+\\Delta t) = \\mathbf{x}(t) + \\Delta t\\,\\mathbf{v}\\left(t+\\frac{\\Delta t}{2}\\right),\n$$\n$$\n\\mathbf{v}(t+\\Delta t) = \\mathbf{v}\\left(t+\\frac{\\Delta t}{2}\\right) + \\frac{\\Delta t}{2}\\,\\mathbf{f}\\left(\\mathbf{x}(t+\\Delta t)\\right).\n$$\n\nTo quantify equipartition over linear normal modes, use the linearized normal modes of the fixed-end chain. Define the orthonormal sine transform\n$$\nQ_k(t) \\;=\\; \\sqrt{\\frac{2}{N+1}} \\sum_{i=1}^{N} x_i(t) \\,\\sin\\!\\left(\\frac{\\pi k i}{N+1}\\right), \\qquad P_k(t) \\;=\\; \\sqrt{\\frac{2}{N+1}} \\sum_{i=1}^{N} v_i(t) \\,\\sin\\!\\left(\\frac{\\pi k i}{N+1}\\right),\n$$\nwith harmonic frequencies\n$$\n\\omega_k^2 \\;=\\; 4\\,\\sin^2\\!\\left(\\frac{\\pi k}{2(N+1)}\\right), \\qquad k = 1,\\dots,N.\n$$\nDefine the instantaneous linear-mode energy diagnostic as\n$$\nE_k^{\\text{lin}}(t) \\;=\\; \\frac{1}{2}\\left[ P_k(t)^2 \\;+\\; \\omega_k^2\\,Q_k(t)^2 \\right],\n$$\nand the normalized energy fractions\n$$\nf_k(t) \\;=\\; \\frac{E_k^{\\text{lin}}(t)}{\\sum_{j=1}^{N} E_j^{\\text{lin}}(t)}.\n$$\nDefine the spectral entropy of the mode energy distribution\n$$\nH(t) \\;=\\; -\\sum_{k=1}^{N} f_k(t)\\,\\ln f_k(t), \\qquad S(t) \\;=\\; \\frac{H(t)}{\\ln N},\n$$\nwhere $S(t)\\in[0,1]$ and $S(t)=1$ corresponds to a perfectly uniform distribution over modes.\n\nInitialization protocol (single-mode excitation): For each total energy $E_{\\text{tot}}$ (determined by energy density $e$ as $E_{\\text{tot}} = e\\,N$), set all velocities to zero and displacements to excite only the fundamental mode $k=1$,\n$$\nx_i(0) \\;=\\; A\\,\\sqrt{\\frac{2}{N+1}}\\,\\sin\\!\\left(\\frac{\\pi i}{N+1}\\right),\n$$\nwhere the amplitude $A$ is chosen so that the initial total energy equals $E_{\\text{tot}}$ when evaluated with the full nonlinear Hamiltonian. This choice ensures a microcanonical initial condition consistent with the specified energy density.\n\nEquipartition time criterion: Sample the spectral entropy $S(t)$ at regular intervals. Define the equipartition threshold by a tolerance $\\delta\\in(0,1)$ and a persistence window length $L\\in\\mathbb{N}$. Declare the equipartition time $T_{\\text{eq}}$ as the earliest time $t$ at which $S(t') \\ge 1 - \\delta$ for $L$ consecutive sampled times $t'$ including $t$. If the threshold is never met within the total simulated time $T_{\\max}$, return $T_{\\text{eq}} = T_{\\max}$.\n\nYour task is to write a complete program that:\n- Implements the velocity-Verlet microcanonical molecular dynamics of the $\\beta$-FPUT chain with fixed ends.\n- Initializes the chain for each specified energy density by exciting only mode $k=1$ with amplitude $A$ chosen to meet the target energy.\n- Computes the linear-mode energies and the spectral entropy over time.\n- Detects and reports the equipartition time $T_{\\text{eq}}$ for each energy density.\n\nAll times must be expressed in dimensionless time units consistent with the chosen time step $\\Delta t$. The final output must be a single line containing a list of floats.\n\nUse the following parameter values as a test suite to probe distinct dynamical regimes:\n- Number of particles: $N = 32$.\n- Nonlinearity strength: $\\beta = 0.1$.\n- Time step: $\\Delta t = 0.02$.\n- Maximum simulated time: $T_{\\max} = 800$ (i.e., $40000$ integration steps).\n- Spectral entropy sampling stride: every $20$ integration steps.\n- Equipartition tolerance: $\\delta = 0.2$.\n- Persistence window length: $L = 8$ consecutive samples.\n- Energy densities: $e \\in \\{0.005,\\;0.05,\\;0.3\\}$, with total energy $E_{\\text{tot}} = e\\,N$ for each case.\n\nYour program should produce a single line of output containing the equipartition times for the three energy densities in the same order, as a comma-separated list enclosed in square brackets (e.g., \"[t_low,t_mid,t_high]\"), where each $t$ is a float in dimensionless time units.",
            "solution": "The user has provided a computational problem from the field of molecular dynamics, specifically concerning the Fermi–Pasta–Ulam–Tsingou (FPUT) chain. The task is to simulate the system's dynamics and determine the equipartition time for different initial energy densities.\n\n### Problem Validation\n\n**Step 1: Extraction of Givens**\n- **System**: A one-dimensional chain of $N$ particles of mass $m=1$ with fixed ends ($x_0 = 0$, $x_{N+1} = 0$).\n- **Hamiltonian**: $H(\\mathbf{x},\\mathbf{v}) = \\sum_{i=1}^{N} \\frac{v_i^2}{2} + \\sum_{i=0}^{N} \\left[ \\frac{1}{2} (x_{i+1} - x_i)^2 + \\frac{\\beta}{4} (x_{i+1} - x_i)^4 \\right]$.\n- **Equations of Motion**: $\\dot{x}_i = v_i$, $\\dot{v}_i = f_i(\\mathbf{x}) = (\\Delta_i + \\beta \\Delta_i^3) - (\\Delta_{i-1} + \\beta \\Delta_{i-1}^3)$, where $\\Delta_i = x_{i+1} - x_i$.\n- **Integrator**: Velocity-Verlet with time step $\\Delta t$.\n- **Diagnostics**:\n    - Normal mode coordinates: $Q_k(t) = \\sqrt{\\frac{2}{N+1}} \\sum_{i=1}^{N} x_i(t) \\sin(\\frac{\\pi k i}{N+1})$ and $P_k(t)$.\n    - Mode frequencies: $\\omega_k^2 = 4\\sin^2(\\frac{\\pi k}{2(N+1)})$.\n    - Linear-mode energy: $E_k^{\\text{lin}}(t) = \\frac{1}{2}[P_k(t)^2 + \\omega_k^2 Q_k(t)^2]$.\n    - Spectral entropy: $S(t) = -\\frac{1}{\\ln N} \\sum_{k=1}^{N} f_k(t)\\ln f_k(t)$, with $f_k(t) = E_k^{\\text{lin}}(t) / \\sum_j E_j^{\\text{lin}}(t)$.\n- **Initialization**: Single-mode excitation of mode $k=1$, $v_i(0)=0$, $x_i(0) = A\\sqrt{\\frac{2}{N+1}}\\sin(\\frac{\\pi i}{N+1})$. The amplitude $A$ is set to match the total energy $E_{\\text{tot}} = eN$.\n- **Equipartition Criterion**: $T_{\\text{eq}}$ is the first sampled time $t$ for which $S(t') \\ge 1-\\delta$ holds for $L$ consecutive samples. If never met, $T_{\\text{eq}} = T_{\\max}$.\n- **Numerical Parameters**: $N=32$, $\\beta=0.1$, $\\Delta t=0.02$, $T_{\\max}=800$, sampling stride $= 20$ steps, $\\delta=0.2$, $L=8$.\n- **Test Cases**: Energy densities $e \\in \\{0.005, 0.05, 0.3\\}$.\n\n**Step 2: Validation Assessment**\nThe problem is a standard exercise in computational statistical mechanics.\n- **Scientific Soundness**: The FPUT model is a cornerstone of nonlinear dynamics. The provided Hamiltonian, equations of motion, and diagnostic tools (normal modes, spectral entropy) are all standard and correctly defined.\n- **Well-Posedness**: The problem is completely specified with all necessary parameters, initial conditions, a deterministic evolution algorithm (velocity-Verlet), and an unambiguous criterion for the solution. A unique numerical solution exists.\n- **Objectivity**: The problem is stated in precise mathematical and algorithmic terms, free from any subjective elements.\n- **Completeness and Consistency**: The problem is self-contained and all provided information is internally consistent.\n\n**Step 3: Verdict**\nThe problem is **valid**.\n\n### Principle-Based Solution Design\n\nThe solution requires implementing a molecular dynamics simulation of the $\\beta$-FPUT chain. The overall approach is to loop through the specified energy densities, and for each, run a full simulation to determine the equipartition time $T_{\\text{eq}}$.\n\n#### 1. Initialization: Determining the Initial Amplitude $A$\n\nThe system is initialized by exciting only the fundamental mode ($k=1$), with all initial velocities set to zero ($\\mathbf{v}(0)=\\mathbf{0}$). The initial positions are given by $x_i(0) = A\\sqrt{\\frac{2}{N+1}}\\sin(\\frac{\\pi i}{N+1})$ for $i=1, \\dots, N$. The amplitude $A$ must be chosen so that the total energy of the system, evaluated using the full nonlinear Hamiltonian, matches the target energy $E_{\\text{tot}} = eN$.\n\nSince $\\mathbf{v}(0)=\\mathbf{0}$, the initial total energy is purely potential: $E_{\\text{tot}} = V(\\mathbf{x}(0))$.\n$E_{\\text{tot}} = \\sum_{i=0}^{N} \\left[ \\frac{1}{2} \\Delta_i(0)^2 + \\frac{\\beta}{4} \\Delta_i(0)^4 \\right]$.\nThe initial displace- ment differences are $\\Delta_i(0) = x_{i+1}(0) - x_i(0)$. By substituting the form of $x_i(0)$ and performing the summation over $i$, the total energy can be expressed as a polynomial in $A$. The derivation involves trigonometric identities and summation formulas for sine and cosine series. The resulting equation for $Y=A^2$ is a quadratic equation:\n$$\n\\left( \\frac{6\\beta}{N+1} \\sin^4\\left(\\frac{\\pi}{2(N+1)}\\right) \\right) Y^2 + \\left( 2 \\sin^2\\left(\\frac{\\pi}{2(N+1)}\\right) \\right) Y - E_{\\text{tot}} = 0\n$$\nThis equation, of the form $aY^2+bY+c=0$, is solved for the single positive root $Y$, from which the amplitude is found as $A = \\sqrt{Y}$.\n\n#### 2. Numerical Integration: The Velocity-Verlet Algorithm\n\nThe system's time evolution is simulated by numerically integrating Newton's equations of motion, $\\ddot{\\mathbf{x}} = \\mathbf{f}(\\mathbf{x})$, using the velocity-Verlet algorithm. This is a symplectic integrator, well-suited for Hamiltonian systems as it exhibits good long-term energy conservation properties.\nFor each time step $\\Delta t$, the positions $\\mathbf{x}$ and velocities $\\mathbf{v}$ are updated as follows:\n1. Update velocities to the midpoint of the time step: $\\mathbf{v}(t+\\frac{\\Delta t}{2}) = \\mathbf{v}(t) + \\frac{\\Delta t}{2}\\mathbf{f}(\\mathbf{x}(t))$.\n2. Update positions to the end of the time step: $\\mathbf{x}(t+\\Delta t) = \\mathbf{x}(t) + \\Delta t\\,\\mathbf{v}(t+\\frac{\\Delta t}{2})$.\n3. Compute the new forces $\\mathbf{f}(\\mathbf{x}(t+\\Delta t))$ at the new positions.\n4. Update velocities to the end of the time step: $\\mathbf{v}(t+\\Delta t) = \\mathbf{v}(t+\\frac{\\Delta t}{2}) + \\frac{\\Delta t}{2}\\mathbf{f}(\\mathbf{x}(t+\\Delta t))$.\n\nThe force vector $\\mathbf{f}(\\mathbf{x})$ is calculated at each step. The force on the $i$-th particle is $f_i = T_i - T_{i-1}$, where $T_j = \\Delta_j + \\beta \\Delta_j^3$ is the tension in the $j$-th spring. This can be implemented efficiently using vectorized operations in `numpy` by first computing an array of all spring extensions $\\Delta_i$ and then the tensions $T_i$.\n\n#### 3. Diagnostics: Spectral Entropy Calculation\n\nAt specified intervals (every $20$ steps), we diagnose the state of the system by calculating the spectral entropy $S(t)$. This multi-step process quantifies the degree of energy equipartition among the linear normal modes.\n1. **Normal Mode Transformation**: The particle positions $\\mathbf{x}(t)$ and velocities $\\mathbf{v}(t)$ are projected onto the normal mode basis to get the mode coordinates $Q_k(t)$ and $P_k(t)$. This is achieved via a Discrete Sine Transform. A transform matrix $\\mathbf{M}$ with elements $M_{ki} = \\sqrt{\\frac{2}{N+1}} \\sin(\\frac{\\pi ki}{N+1})$ is pre-computed, so that $\\mathbf{Q} = \\mathbf{M}\\mathbf{x}$ and $\\mathbf{P} = \\mathbf{M}\\mathbf{v}$.\n2. **Mode Energies**: The instantaneous energy in each linear mode, $E_k^{\\text{lin}}(t)$, is calculated using its definition. The required squared normal mode frequencies $\\omega_k^2$ are also pre-computed.\n3. **Energy Fractions**: The energies are normalized to obtain fractions $f_k(t) = E_k^{\\text{lin}}(t) / \\sum_j E_j^{\\text{lin}}(t)$.\n4. **Entropy**: The normalized spectral entropy $S(t)$ is computed from these fractions using the Shannon entropy formula. Care is taken to handle terms where $f_k(t)=0$, as $0\\ln 0 \\equiv 0$.\n\n#### 4. Equipartition Time Detection\n\nThe simulation runs up to a maximum time $T_{\\max}$. The calculated entropy values $S(t)$ are stored at each sampling point. After the simulation is complete, these samples are analyzed to find the equipartition time $T_{\\text{eq}}$. We search for the first instance where the entropy $S(t)$ remains at or above a threshold ($1-\\delta$) for a sequence of $L$ consecutive samples. The time corresponding to the start of this first qualifying sequence is reported as $T_{\\text{eq}}$. If no such sequence is found, $T_{\\text{eq}}$ is set to $T_{\\max}$. This logic is implemented by iterating through the stored entropy samples with a sliding window of length $L$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run the FPUT simulation for the specified test cases\n    and print the results in the required format.\n    \"\"\"\n    \n    # ----------------------------------------------------------------------\n    # Define problem parameters from the statement\n    # ----------------------------------------------------------------------\n    N = 32\n    BETA = 0.1\n    DT = 0.02\n    T_MAX = 800.0\n    SAMPLE_STRIDE = 20\n    ENTROPY_TOLERANCE_DELTA = 0.2\n    PERSISTENCE_WINDOW_L = 8\n    ENERGY_DENSITIES = [0.005, 0.05, 0.3]\n\n    def calculate_force(x, beta, n_particles):\n        \"\"\"\n        Calculates the force on each particle in the FPUT chain.\n        - x: array of particle displacements (shape: (n_particles,))\n        - beta: nonlinearity parameter\n        - n_particles: number of particles\n        \"\"\"\n        # Create a padded array for positions to handle fixed boundaries\n        # x_padded corresponds to (x_0, x_1, ..., x_N, x_{N+1})\n        x_padded = np.concatenate(([0.0], x, [0.0]))\n        \n        # Calculate displacements between adjacent particles, delta_i = x_{i+1} - x_i\n        delta = np.diff(x_padded)\n        \n        # Calculate tension in each spring, T_i = delta_i + beta * delta_i^3\n        tension = delta + beta * delta**3\n        \n        # Force on particle i is T_i - T_{i-1}\n        force = np.diff(tension)\n        \n        return force\n\n    def run_fput_simulation(n, beta, dt, t_max, sample_stride, delta, L, e):\n        \"\"\"\n        Runs a single FPUT simulation for a given energy density `e`.\n        Returns the calculated equipartition time.\n        \"\"\"\n        # 1. INITIALIZATION\n        total_energy = e * n\n\n        # Find initial amplitude A by solving aY^2 + bY + c = 0 for Y = A^2\n        sin_term = np.sin(np.pi / (2 * (n + 1)))\n        a_quad = (6 * beta / (n + 1)) * sin_term**4\n        b_quad = 2 * sin_term**2\n        c_quad = -total_energy\n        \n        # Since a_quad > 0, b_quad > 0, c_quad < 0, the discriminant is positive.\n        # We take the positive root for Y = A^2.\n        discriminant = b_quad**2 - 4 * a_quad * c_quad\n        Y = (-b_quad + np.sqrt(discriminant)) / (2 * a_quad)\n        A = np.sqrt(Y)\n\n        # Set initial positions and velocities for k=1 mode excitation\n        i_vals = np.arange(1, n + 1)\n        x = A * np.sqrt(2 / (n + 1)) * np.sin(np.pi * i_vals / (n + 1))\n        v = np.zeros(n)\n        \n        # 2. PRE-COMPUTATION FOR DIAGNOSTICS\n        k_vals = np.arange(1, n + 1)\n        \n        # Sine transform matrix M_ki = sqrt(2/(N+1)) * sin(pi*k*i / (N+1))\n        M = np.sqrt(2 / (n + 1)) * np.sin(np.outer(k_vals, np.pi * i_vals / (n + 1)))\n\n        # Squared harmonic frequencies omega_k^2\n        omega2_k = 4 * np.sin(np.pi * k_vals / (2 * (n + 1)))**2\n\n        # 3. TIME INTEGRATION (VELOCITY-VERLET)\n        num_steps = int(t_max / dt)\n        entropy_samples = []\n        sample_times = []\n\n        # Initial force calculation\n        f = calculate_force(x, beta, n)\n\n        for step in range(num_steps):\n            # Velocity-Verlet integration step\n            v_half = v + 0.5 * dt * f\n            x = x + dt * v_half\n            f = calculate_force(x, beta, n)\n            v = v_half + 0.5 * dt * f\n\n            # Sample diagnostics at specified stride\n            if (step + 1) % sample_stride == 0:\n                current_time = (step + 1) * dt\n                \n                # Project onto normal modes\n                Q_k = M @ x\n                P_k = M @ v\n                \n                # Calculate linear mode energies\n                E_k_lin = 0.5 * (P_k**2 + omega2_k * Q_k**2)\n                \n                total_E_lin = np.sum(E_k_lin)\n                if total_E_lin > 1e-15:\n                    f_k = E_k_lin / total_E_lin\n                    # Filter out zero fractions to avoid log(0)\n                    f_k_positive = f_k[f_k > 1e-15]\n                    H_t = -np.sum(f_k_positive * np.log(f_k_positive))\n                    S_t = H_t / np.log(n)\n                else:\n                    S_t = 0.0\n\n                entropy_samples.append(S_t)\n                sample_times.append(current_time)\n                \n        # 4. DETERMINE EQUIPARTITION TIME\n        threshold = 1.0 - delta\n        num_samples = len(entropy_samples)\n        \n        if num_samples < L:\n            return t_max\n\n        # Check for persistence window\n        for i in range(num_samples - L + 1):\n            window = entropy_samples[i : i + L]\n            if all(s >= threshold for s in window):\n                return sample_times[i] # Return time of first sample in the window\n            \n        return t_max\n\n    # ----------------------------------------------------------------------\n    # Run simulations for all specified test cases\n    # ----------------------------------------------------------------------\n    results = []\n    for energy_density in ENERGY_DENSITIES:\n        t_eq = run_fput_simulation(\n            N, BETA, DT, T_MAX, SAMPLE_STRIDE,\n            ENTROPY_TOLERANCE_DELTA, PERSISTENCE_WINDOW_L, energy_density\n        )\n        results.append(t_eq)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having observed that deterministic physical systems can fail to be ergodic, we now turn our attention to the algorithms designed to enforce thermal equilibrium in simulations. This analytical practice investigates why some thermostats might fail to guarantee ergodicity, focusing on the classic example of a single harmonic oscillator . By deriving the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ for systems coupled to Nosé–Hoover and Langevin thermostats, you will gain a first-principles understanding of the crucial role that chaotic dynamics or stochasticity plays in ensuring robust mixing and canonical sampling.",
            "id": "3452506",
            "problem": "Consider a one-dimensional harmonic oscillator with mass $m$, spring constant $k$, and angular frequency $\\omega$, where $\\omega = \\sqrt{k/m}$. The oscillator is coupled to a thermal reservoir at temperature $T$, modeled in three different ways that are standard in molecular dynamics: the deterministic Nosé–Hoover thermostat, Nosé–Hoover chains, and the stochastic Langevin thermostat. The goal is to compare these thermostats in terms of their ergodic and mixing properties for the harmonic oscillator via a calculable metric derived from first principles.\n\nDefine the normalized coordinate autocorrelation function $C_{q}(t)$ by\n$$\nC_{q}(t) = \\frac{\\langle q(0)\\, q(t)\\rangle_{\\text{stat}}}{\\langle q^{2}\\rangle_{\\text{stat}}},\n$$\nwhere $\\langle \\cdot \\rangle_{\\text{stat}}$ denotes the steady-state ensemble average. Define the integrated autocorrelation time of the coordinate by\n$$\n\\tau_{\\mathrm{int}} = \\int_{0}^{\\infty} C_{q}(t)\\, dt.\n$$\nYou will compute $\\tau_{\\mathrm{int}}$ for the three thermostat models below and express your final answer as a single row matrix containing the three values in the order: Nosé–Hoover, Nosé–Hoover chains, Langevin.\n\n1. Nosé–Hoover thermostat. The extended deterministic dynamics are\n$$\n\\dot{q} = \\frac{p}{m}, \\quad \\dot{p} = -k\\, q - \\xi\\, p, \\quad \\dot{\\xi} = \\frac{1}{Q}\\left(\\frac{p^{2}}{m} - k_{B}T\\right),\n$$\nwhere $p$ is the momentum, $\\xi$ is the thermostat variable, $Q$ is the thermostat mass parameter, and $k_{B}$ is Boltzmann's constant. Using only these equations and basic definitions, demonstrate that there exist trajectories consistent with the thermostat that yield non-decaying $C_{q}(t)$ for the single harmonic oscillator, and compute the implied value of $\\tau_{\\mathrm{int}}$ for the Nosé–Hoover case.\n\n2. Nosé–Hoover chains in a Markovian white-noise limit. Consider an infinite Nosé–Hoover chain coupled to the harmonic oscillator. In the limit where the chain acts as an effectively Markovian thermal reservoir on the physical momentum (a standard coarse-graining justified by the Mori–Zwanzig projection formalism), the oscillator’s momentum $p$ is modeled as obeying an effective Ornstein–Uhlenbeck (OU) equation with damping coefficient $\\nu$ and Gaussian white noise consistent with the Fluctuation–Dissipation Theorem (FDT). Under this Markovian white-noise limit, the effective dynamics reduce to\n$$\nm\\, \\ddot{q} + \\nu\\, m\\, \\dot{q} + k\\, q = \\sqrt{2\\, \\nu\\, m\\, k_{B}T}\\, \\eta(t),\n$$\nwhere $\\eta(t)$ is unit-variance Gaussian white noise. Compute $\\tau_{\\mathrm{int}}$ in terms of $\\nu$ and $\\omega$.\n\n3. Langevin thermostat. The standard Langevin dynamics for the harmonic oscillator are\n$$\nm\\, \\ddot{q} + \\gamma\\, m\\, \\dot{q} + k\\, q = \\sqrt{2\\, \\gamma\\, m\\, k_{B}T}\\, \\eta(t),\n$$\nwith friction coefficient $\\gamma > 0$ and unit-variance Gaussian white noise $\\eta(t)$. Assume the underdamped regime $\\gamma < 2\\, \\omega$. Starting from these equations and equilibrium statistical mechanics for the harmonic oscillator, derive $C_{q}(t)$ and compute $\\tau_{\\mathrm{int}}$ in terms of $\\gamma$ and $\\omega$.\n\nYour final answer must be a single analytical expression containing the three values of $\\tau_{\\mathrm{int}}$, ordered as (Nosé–Hoover, Nosé–Hoover chains, Langevin), expressed as a row matrix using the `\\begin{pmatrix}` environment. No numerical approximation is required. Do not include units in the final answer.",
            "solution": "The problem asks for the computation of the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, for the position of a one-dimensional harmonic oscillator under three different thermostatting schemes. The integrated autocorrelation time is defined as\n$$\n\\tau_{\\mathrm{int}} = \\int_{0}^{\\infty} C_{q}(t)\\, dt,\n$$\nwhere $C_{q}(t)$ is the normalized position autocorrelation function,\n$$\nC_{q}(t) = \\frac{\\langle q(0)\\, q(t)\\rangle_{\\text{stat}}}{\\langle q^{2}\\rangle_{\\text{stat}}}.\n$$\nThe averaging $\\langle \\cdot \\rangle_{\\text{stat}}$ is over the steady-state ensemble generated by the dynamics. For an ergodic system at temperature $T$, this corresponds to the canonical ensemble average. A key property of the canonical ensemble for a harmonic oscillator with potential $V(q) = \\frac{1}{2}kq^2$ is the equipartition theorem, which states that $\\frac{1}{2}k\\langle q^2 \\rangle_{\\text{stat}} = \\frac{1}{2}k_B T$. This gives the normalization factor:\n$$\n\\langle q^{2}\\rangle_{\\text{stat}} = \\frac{k_B T}{k}.\n$$\nWe will now analyze each of the three cases.\n\n1. Nosé–Hoover thermostat\nThe equations of motion for the oscillator coordinate $q$, its momentum $p$, and the thermostat variable $\\xi$ are given by the deterministic set:\n$$\n\\dot{q} = \\frac{p}{m}, \\quad \\dot{p} = -k\\, q - \\xi\\, p, \\quad \\dot{\\xi} = \\frac{1}{Q}\\left(\\frac{p^{2}}{m} - k_{B}T\\right).\n$$\nThese equations can be combined to give a second-order equation for $q$:\n$$\nm\\ddot{q} = \\dot{p} = -kq - \\xi p = -kq - \\xi (m\\dot{q}) \\implies m\\ddot{q} + m\\xi\\dot{q} + kq = 0.\n$$\nThis is an oscillator equation with a time-varying damping coefficient $\\xi(t)$. The system of three ordinary differential equations for $(q, p, \\xi)$ is known to be non-ergodic for the single harmonic oscillator. The dynamics are not chaotic; instead, for almost all initial conditions, the trajectory in the three-dimensional phase space is confined to a two-dimensional invariant torus.\nA trajectory on such a torus corresponds to quasi-periodic motion. A quasi-periodic function $q(t)$ can be expressed as a Fourier-like series with multiple incommensurate frequencies. Such a function does not decay to zero as $t \\to \\infty$.\nConsequently, the position autocorrelation function, $C_q(t)$, which is constructed from this quasi-periodic $q(t)$, will also be a quasi-periodic function and will not decay to zero as $t \\to \\infty$. For example, a purely periodic motion $q(t) = A\\cos(\\omega' t)$ would lead to a time-averaged autocorrelation function that behaves as $\\cos(\\omega' t)$.\nSince $C_q(t)$ does not decay to zero, its integral from $t=0$ to $t=\\infty$ does not converge to a finite value.\n$$\n\\tau_{\\mathrm{int}} = \\int_{0}^{\\infty} C_{q}(t)\\, dt \\to \\infty.\n$$\nThis divergence signifies that the Nosé–Hoover thermostat fails to provide efficient statistical sampling for the single harmonic oscillator, a classic result demonstrating the importance of chaotic dynamics for ergodicity. Thus, for this case, the integrated autocorrelation time is infinite.\n\n2. Nosé–Hoover chains (Markovian white-noise limit)\nIn this model, the dynamics of the oscillator are described by the stochastic differential equation:\n$$\nm\\, \\ddot{q} + \\nu\\, m\\, \\dot{q} + k\\, q = F(t),\n$$\nwhere $F(t) = \\sqrt{2\\, \\nu\\, m\\, k_{B}T}\\, \\eta(t)$ and $\\eta(t)$ is Gaussian white noise with $\\langle \\eta(t)\\eta(t') \\rangle = \\delta(t-t')$. This is a form of the Langevin equation, which is known to be ergodic and produces a canonical distribution at temperature $T$.\nLet $R(t) = \\langle q(0)q(t) \\rangle_{\\text{stat}}$. Due to the time-translation invariance of the steady state, we can write\n$$\n\\ddot{R}(t) = \\langle q(0)\\ddot{q}(t) \\rangle, \\quad \\dot{R}(t) = \\langle q(0)\\dot{q}(t) \\rangle.\n$$\nMultiplying the equation of motion at time $t$ by $q(0)$ and taking the ensemble average gives:\n$$\nm \\langle q(0)\\ddot{q}(t) \\rangle + \\nu m \\langle q(0)\\dot{q}(t) \\rangle + k \\langle q(0)q(t) \\rangle = \\langle q(0)F(t) \\rangle.\n$$\nFor $t > 0$, the random force $F(t)$ is uncorrelated with the position $q(0)$, since $q(0)$ depends only on the history of the noise for times prior to $0$. Thus, $\\langle q(0)F(t) \\rangle = 0$. This yields a homogeneous ordinary differential equation for $R(t)$:\n$$\nm\\ddot{R}(t) + \\nu m\\dot{R}(t) + k R(t) = 0 \\quad (\\text{for } t > 0).\n$$\nTo compute $\\tau_{\\mathrm{int}}$, we can integrate this equation from $t=0$ to $\\infty$:\n$$\n\\int_{0}^{\\infty} \\left( m\\ddot{R}(t) + \\nu m\\dot{R}(t) + k R(t) \\right) dt = 0.\n$$\n$$\nm[\\dot{R}(t)]_{0}^{\\infty} + \\nu m[R(t)]_{0}^{\\infty} + k\\int_{0}^{\\infty} R(t) dt = 0.\n$$\nThe system thermalizes, so $R(t) \\to 0$ and $\\dot{R}(t) \\to 0$ as $t \\to \\infty$. This gives:\n$$\nm(0 - \\dot{R}(0)) + \\nu m(0 - R(0)) + k\\int_{0}^{\\infty} R(t) dt = 0.\n$$\nThe initial conditions are $R(0) = \\langle q^2 \\rangle$ and $\\dot{R}(0) = \\langle q(0)\\dot{q}(0) \\rangle$. In thermal equilibrium, position and velocity are uncorrelated, i.e., $\\langle q\\dot{q} \\rangle = (1/m)\\langle qp \\rangle = 0$, as the phase space distribution function is separable in $q$ and $p$. So, $\\dot{R}(0) = 0$.\nThe equation simplifies to:\n$$\n- \\nu m R(0) + k\\int_{0}^{\\infty} R(t) dt = 0.\n$$\n$$\n\\int_{0}^{\\infty} R(t) dt = \\frac{\\nu m}{k} R(0).\n$$\nThe integrated autocorrelation time is then\n$$\n\\tau_{\\mathrm{int}} = \\int_{0}^{\\infty} \\frac{R(t)}{R(0)} dt = \\frac{1}{R(0)} \\int_{0}^{\\infty} R(t) dt = \\frac{1}{R(0)} \\left( \\frac{\\nu m}{k} R(0) \\right) = \\frac{\\nu m}{k}.\n$$\nUsing the definition $\\omega = \\sqrt{k/m}$, we have $m/k = 1/\\omega^2$. Therefore,\n$$\n\\tau_{\\mathrm{int}} = \\frac{\\nu}{\\omega^2}.\n$$\n\n3. Langevin thermostat\nThe equation of motion is given as:\n$$\nm\\, \\ddot{q} + \\gamma\\, m\\, \\dot{q} + k\\, q = \\sqrt{2\\, \\gamma\\, m\\, k_{B}T}\\, \\eta(t).\n$$\nThis equation has the exact same mathematical form as the one in Part 2, with the friction coefficient $\\gamma$ replacing the damping coefficient $\\nu$. The procedure for finding $\\tau_{\\mathrm{int}}$ is identical.\nFollowing the same steps as above, we arrive at the equation for the unnormalized autocorrelation function $R(t) = \\langle q(0)q(t) \\rangle$:\n$$\nm\\ddot{R}(t) + \\gamma m\\dot{R}(t) + k R(t) = 0 \\quad (\\text{for } t > 0).\n$$\nIntegrating this equation from $t=0$ to $\\infty$ yields:\n$$\n- \\gamma m R(0) + k\\int_{0}^{\\infty} R(t) dt = 0.\n$$\nThis gives $\\int_{0}^{\\infty} R(t) dt = \\frac{\\gamma m}{k} R(0)$.\nThe integrated autocorrelation time is therefore:\n$$\n\\tau_{\\mathrm{int}} = \\frac{\\gamma m}{k} = \\frac{\\gamma}{\\omega^2}.\n$$\nFor completeness, we derive $C_q(t)$ as requested for this case, under the specified underdamped condition $\\gamma < 2\\omega$. The characteristic equation for the ODE for $R(t)$ is $\\lambda^2 + \\gamma\\lambda + \\omega^2 = 0$, with roots $\\lambda = -\\frac{\\gamma}{2} \\pm i\\omega'_d$, where $\\omega'_d = \\sqrt{\\omega^2 - (\\gamma/2)^2}$.\nThe general solution for $R(t)$ is $R(t) = e^{-\\gamma t/2} (A \\cos(\\omega'_d t) + B \\sin(\\omega'_d t))$.\nThe initial conditions are $R(0) = \\langle q^2 \\rangle = k_B T/k$ and $\\dot{R}(0) = \\langle q\\dot{q} \\rangle = 0$.\n$R(0) = A = k_B T/k$.\n$\\dot{R}(0) = -\\frac{\\gamma}{2}A + B\\omega'_d = 0$, which implies $B = \\frac{\\gamma A}{2\\omega'_d}$.\nThus, $R(t) = \\frac{k_B T}{k} e^{-\\gamma t/2} \\left( \\cos(\\omega'_d t) + \\frac{\\gamma}{2\\omega'_d}\\sin(\\omega'_d t) \\right)$.\nThe normalized autocorrelation function is:\n$$\nC_q(t) = \\frac{R(t)}{R(0)} = e^{-\\gamma t/2} \\left( \\cos(\\omega'_d t) + \\frac{\\gamma}{2\\omega'_d}\\sin(\\omega'_d t) \\right).\n$$\nIntegrating this expression from $0$ to $\\infty$ confirms the previous result for $\\tau_{\\mathrm{int}}$.\n\nFinal Answer Assembly\nThe three computed values for $\\tau_{\\mathrm{int}}$ in the specified order (Nosé–Hoover, Nosé–Hoover chains, Langevin) are:\n1. $\\tau_{\\mathrm{int,NH}} = \\infty$\n2. $\\tau_{\\mathrm{int,NHC}} = \\frac{\\nu}{\\omega^2}$\n3. $\\tau_{\\mathrm{int,Lang}} = \\frac{\\gamma}{\\omega^2}$\nThese are combined into a single row matrix.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\infty & \\frac{\\nu}{\\omega^{2}} & \\frac{\\gamma}{\\omega^{2}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Our final practice synthesizes the preceding concepts into a comprehensive, practical workflow for simulation validation. Your task is to design and implement an automated diagnostic suite to rigorously test whether a simulation is correctly sampling the canonical ensemble and, crucially, to distinguish between true lack of ergodicity and mere slow convergence . This capstone project will equip you with a powerful and generalizable framework for assessing the reliability of your own molecular dynamics simulations, moving from theoretical understanding to robust practical application.",
            "id": "3452525",
            "problem": "You are given the task of designing a fully automated diagnostic to test whether a thermostatted molecular dynamics trajectory samples the canonical ensemble and to algorithmically separate lack of ergodicity from finite-time bias. The diagnostics must be based on first principles and should not rely on any unspecified external heuristics.\n\nYou must implement a single program that, for each member of a provided test suite of one-dimensional systems, performs the following tasks:\n\n1. Generate two independent replicas of a trajectory by integrating the one-dimensional Langevin dynamics in reduced units, where the Boltzmann constant is set to $k_{\\mathrm{B}} = 1$ and the mass is set to $m = 1$. The equation of motion is\n$$\n\\frac{dx}{dt} = v, \\qquad \\frac{dv}{dt} = -\\frac{dU(x)}{dx} - \\gamma v + \\sqrt{2 \\gamma T}\\, \\eta(t),\n$$\nwhere $x$ is the position, $v$ is the velocity, $U(x)$ is a given potential energy function, $\\gamma$ is the friction coefficient, $T$ is the temperature, and $\\eta(t)$ is unit-variance Gaussian white noise. Use a time-discretization scheme that respects these dynamics and is stable for small $dt$ with the specified parameters.\n\n2. After discarding an initial burn-in, compute diagnostics from the simulated data that test canonical sampling and mixing:\n   - Canonical velocity marginal test: In the canonical ensemble, velocity components are independent Gaussian random variables with mean $0$ and variance $T$. Using a hypothesis test such as the Kolmogorov–Smirnov (KS) test against the normal distribution with mean $0$ and standard deviation $\\sqrt{T}$, assess whether the pooled velocity samples from the two replicas are consistent with this distribution. Also compute the sample variance of velocities and compare it to $T$ within a relative tolerance.\n   - Replica consistency test: Let $A(t)$ be the observable $x(t)$. For replica $i \\in \\{1, 2\\}$, compute the time average $\\bar{A}_i$ over the post-burn-in trajectory, its sample variance $s_i^2$, and its integrated autocorrelation time $\\tau_{\\mathrm{int}, i}$ as\n     $$\n     \\tau_{\\mathrm{int}} = \\frac{1}{2} + \\sum_{t=1}^{t^\\star} \\rho(t), \\quad \\rho(t) = \\frac{C(t)}{C(0)},\n     $$\n     where $C(t)$ is the autocovariance function and the sum is truncated at the first non-positive $\\rho(t)$ or at the maximum available lag. Estimate the standard error of $\\bar{A}_i$ via\n     $$\n     \\sigma_i \\approx \\sqrt{\\frac{s_i^2 \\, 2 \\tau_{\\mathrm{int}, i}}{N_i}},\n     $$\n     where $N_i$ is the number of post-burn-in samples in replica $i$. Using the two replicas, compute the $z$-score\n     $$\n     z_{\\mathrm{rep}} = \\frac{|\\bar{A}_1 - \\bar{A}_2|}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}},\n     $$\n     to assess whether the two independent time-averages agree within estimated uncertainty.\n   - Consistency with the canonical expectation: For each system, compute the exact canonical expectation $\\langle x \\rangle$ using numerical quadrature of the weight $e^{-U(x)/T}$ over $x \\in (-\\infty, \\infty)$,\n     $$\n     \\langle x \\rangle = \\frac{\\int_{-\\infty}^{\\infty} x \\, e^{-U(x)/T} \\, dx}{\\int_{-\\infty}^{\\infty} e^{-U(x)/T} \\, dx}.\n     $$\n     Compare the pooled time average $\\bar{A}$ from both replicas to this theoretical value using a $z$-score computed from a pooled standard error that uses the integrated autocorrelation time.\n   - Finite-time trend test: Compute the cumulative running average $\\bar{A}(t)$ over the second half of the pooled trajectory and fit a straight line to $\\bar{A}(t)$ versus time $t$ using least squares. Using the standard error of the slope from linear regression, compute a $t$-statistic for the slope. A statistically significant non-zero slope indicates that the time average is still drifting, consistent with finite-time bias rather than broken ergodicity.\n\nYour program must return, for each test case, a single integer classification code based on the following rule:\n- Output $0$ if the canonical velocity marginal test fails (either the KS test rejects at significance level $\\alpha = 10^{-3}$ or the sample variance of velocities differs from $T$ by more than $10\\%$).\n- Otherwise, output $3$ if the replicas are mutually inconsistent (i.e., $z_{\\mathrm{rep}} > 3$), indicating lack of ergodicity or broken mixing on the sampling timescale.\n- Otherwise, output $2$ if the pooled time average significantly differs from the theoretical canonical expectation (i.e., the corresponding $z$-score exceeds $3$) and the finite-time trend test indicates a statistically significant slope (absolute $t$-statistic greater than $3$), indicating finite-time bias.\n- Otherwise, output $1$, indicating that the trajectory is consistent with canonical sampling and no evidence of broken mixing is detected.\n\nNumerical and physical units: Use reduced units where $k_{\\mathrm{B}} = 1$ and $m = 1$. Time is measured in arbitrary units consistent with the discretization parameter $dt$. No output must include explicit units.\n\nTest suite: Apply your program to the following three cases. For each case, simulate two independent replicas with the specified parameters. Discard the specified number of initial steps as burn-in. Use independent random number seeds for the replicas.\n\n- Case A (well-mixed harmonic):\n  - Potential: $U(x) = \\tfrac{1}{2} k x^2$ with $k = 1$.\n  - Temperature: $T = 1$.\n  - Friction: $\\gamma = 1$.\n  - Time step: $dt = 0.005$.\n  - Total steps per replica: $30000$.\n  - Burn-in steps per replica: $5000$.\n  - Initial conditions: $x(0) = 0$, $v(0) = 0$ for both replicas.\n  - Use two distinct random seeds.\n\n- Case B (non-ergodic double well, trapped on timescale):\n  - Potential: $U(x) = a(x^2 - b^2)^2 + c x$ with $a = 5$, $b = 1.5$, $c = 0.3$.\n  - Temperature: $T = 0.2$.\n  - Friction: $\\gamma = 1$.\n  - Time step: $dt = 0.002$.\n  - Total steps per replica: $20000$.\n  - Burn-in steps per replica: $2000$.\n  - Initial conditions: Replica $1$ starts at $x(0) = -b$, $v(0) = 0$; Replica $2$ starts at $x(0) = +b$, $v(0) = 0$.\n  - Use two distinct random seeds.\n\n- Case C (ergodic but with finite-time bias on the available timescale):\n  - Potential: $U(x) = a(x^2 - b^2)^2 + c x$ with $a = 1$, $b = 1$, $c = 0.2$.\n  - Temperature: $T = 0.5$.\n  - Friction: $\\gamma = 1$.\n  - Time step: $dt = 0.002$.\n  - Total steps per replica: $6000$.\n  - Burn-in steps per replica: $1000$.\n  - Initial conditions: Both replicas start at $x(0) = -b$, $v(0) = 0$.\n  - Use two distinct random seeds.\n\nImplementation constraints:\n- The integration scheme, statistical estimators, and hypothesis tests must be implemented in a numerically stable manner. For numerical quadrature of $\\langle x \\rangle$, use a stable approach to avoid overflow by shifting the potential with a constant reference energy; the shift cancels between numerator and denominator.\n- All thresholds must be implemented exactly as specified: $\\alpha = 10^{-3}$ for the Kolmogorov–Smirnov velocity test, $10\\%$ relative tolerance for the velocity variance check, and critical values of $3$ for both $z$-scores and the slope $t$-statistic.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_A, r_B, r_C]$), where $r_A$, $r_B$, and $r_C$ are the integer classification codes for Cases A, B, and C, respectively.\n\nYour program must be self-contained and must not accept any input. It must use only the libraries specified in the execution environment. All numerical answers must be represented as integers in the final printed list.",
            "solution": "The provided problem statement is valid. It is scientifically grounded in the principles of statistical mechanics and molecular dynamics, well-posed with a clear objective and constraints, and free of contradictions or ambiguities. The task is to design and implement a numerical diagnostic suite to assess the convergence and ergodicity of molecular dynamics trajectories. We will proceed with a full solution.\n\nThe solution is structured around a series of statistical tests applied to trajectories generated by Langevin dynamics. Each test targets a specific aspect of canonical sampling, as dictated by the ergodic hypothesis. The ergodic hypothesis posits that for a sufficiently long trajectory, the time average of an observable converges to its ensemble average in the corresponding statistical ensemble—in this case, the canonical (NVT) ensemble. Our diagnostic framework is designed to detect violations of this principle and to distinguish between two primary failure modes: broken ergodicity (or poor mixing) and finite-time bias (slow convergence).\n\nFirst, we must generate the dynamical data. The system evolves according to one-dimensional Langevin dynamics, whose equation of motion is given by:\n$$\nm \\frac{d^2x}{dt^2} = -\\frac{dU(x)}{dx} - \\gamma m v + \\sqrt{2 \\gamma m k_{\\mathrm{B}} T} \\, \\eta(t)\n$$\nIn the specified reduced units ($m=1$, $k_{\\mathrm{B}}=1$), this simplifies to:\n$$\n\\frac{dv}{dt} = -\\frac{dU(x)}{dx} - \\gamma v + \\sqrt{2 \\gamma T} \\, \\eta(t)\n$$\nTo integrate these equations numerically, we employ the BAOAB splitting scheme, a popular and robust algorithm for Langevin dynamics. This method is time-reversible and preserves the correct canonical distribution. The scheme discretizes a time step $dt$ into a sequence of updates:\n1.  **B-step:** Propagate position by half a time step: $x \\leftarrow x + v \\frac{dt}{2}$.\n2.  **A-step:** Propagate velocity due to the conservative force: $v \\leftarrow v - \\frac{dU(x)}{dx} \\frac{dt}{2}$.\n3.  **O-step:** Propagate velocity due to the thermostat (friction and random force). This corresponds to an exact solution of the Ornstein-Uhlenbeck process for velocity: $v \\leftarrow v e^{-\\gamma dt} + \\sqrt{T(1 - e^{-2\\gamma dt})} \\mathcal{N}(0,1)$, where $\\mathcal{N}(0,1)$ is a standard normal random number.\n4.  **A-step:** Propagate velocity due to conservative force again: $v \\leftarrow v - \\frac{dU(x)}{dx} \\frac{dt}{2}$.\n5.  **B-step:** Propagate position for the final half step: $x \\leftarrow x + v \\frac{dt}{2}$.\n\nFor each test case, we generate two independent trajectories (replicas) starting from specified initial conditions but with different random number seeds for the stochastic force term. An initial portion of each trajectory is discarded as burn-in to allow the system to equilibrate.\n\nThe analysis follows a strict hierarchical procedure:\n\n**1. Canonical Velocity Marginal Test (Code $0$):**\nThe foundational check is whether the thermostat correctly maintains the system's temperature. In the canonical ensemble, the velocity distribution is a Maxwell-Boltzmann distribution, which for a one-dimensional system is a Gaussian with mean $0$ and variance $T$ (since $m=1, k_{\\mathrm{B}}=1$). We test this in two ways using the pooled velocity samples from both post-burn-in replicas:\n-   **Distribution Shape:** A Kolmogorov-Smirnov (KS) test is performed to compare the empirical cumulative distribution function (CDF) of the sampled velocities against the theoretical CDF of $\\mathcal{N}(0, T)$. If the p-value is below the significance level $\\alpha = 10^{-3}$, we reject the null hypothesis that the velocities are drawn from the target distribution.\n-   **Distribution Variance:** We compute the sample variance of the velocities. The equipartition theorem dictates that the average kinetic energy $\\frac{1}{2}m \\langle v^2 \\rangle$ should equal $\\frac{1}{2}k_{\\mathrm{B}}T$. For $m=1, k_{\\mathrm{B}}=1$, this implies $\\langle v^2 \\rangle = T$. We check if the sample variance deviates from $T$ by more than a $10\\%$ relative tolerance.\nA failure in either of these tests is a fundamental failure of the simulation to sample the canonical ensemble, resulting in classification code $0$.\n\n**2. Replica Consistency Test (Code $3$):**\nIf the velocity distribution is correct, we next test for ergodicity. We run two independent replicas to see if they sample the same statistical properties. If the system is ergodic, the time-averages of an observable from both replicas should converge to the same ensemble average and thus be statistically indistinguishable. We use the position, $A(t) = x(t)$, as our observable.\nFor each replica $i \\in \\{1, 2\\}$, we compute the mean $\\bar{A}_i$ and its standard error $\\sigma_i$. Because successive samples in a trajectory are correlated, the standard error of the mean is not simply the sample standard deviation divided by $\\sqrt{N_i}$. Instead, it is corrected by the integrated autocorrelation time, $\\tau_{\\mathrm{int}, i}$:\n$$\n\\sigma_i^2 \\approx \\frac{s_i^2 \\, 2 \\tau_{\\mathrm{int}, i}}{N_i}\n$$\nwhere $s_i^2$ is the sample variance of $A$ in replica $i$, and $N_i$ is the number of samples. The integrated autocorrelation time is computed from the normalized autocovariance function $\\rho(t)$:\n$$\n\\tau_{\\mathrm{int}} = \\frac{1}{2} + \\sum_{t=1}^{t^\\star} \\rho(t)\n$$\nThe sum is truncated at the first lag $t$ where $\\rho(t)$ becomes non-positive, a standard heuristic to reduce noise from the tail of the ACF.\nWe then compute a $z$-score to quantify the difference between the two replica means:\n$$\nz_{\\mathrm{rep}} = \\frac{|\\bar{A}_1 - \\bar{A}_2|}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}}\n$$\nA large $z$-score (specifically, $z_{\\mathrm{rep}} > 3$) implies that the two replicas have produced statistically different averages, a strong sign that they are trapped in different regions of phase space and the system is not mixing on the simulation timescale. This indicates broken ergodicity and results in classification code $3$.\n\n**3. Finite-Time Bias Test (Code $2$):**\nIf the replicas are consistent, it suggests they are sampling the same region of phase space. However, the simulation may still be too short to have fully converged to the true canonical average. This is known as finite-time bias. We test for this by checking for two simultaneous conditions:\n-   **Inconsistency with a known theoretical value:** The pooled average from both replicas, $\\bar{A}$, is compared against the exact canonical expectation, $\\langle A \\rangle = \\langle x \\rangle$. The theoretical average is computed by numerical quadrature:\n    $$\n    \\langle x \\rangle = \\frac{\\int_{-\\infty}^{\\infty} x \\, e^{-U(x)/T} \\, dx}{\\int_{-\\infty}^{\\infty} e^{-U(x)/T} \\, dx}\n    $$\n    A $z$-score, $z_{\\text{canon}} = \\frac{|\\bar{A} - \\langle x \\rangle|}{\\sigma_{\\text{pool}}}$, is computed, where $\\sigma_{\\text{pool}}$ is the standard error of the pooled mean. A significant deviation ($z_{\\text{canon}} > 3$) suggests the time average has not converged to the ensemble average.\n-   **Persistent drift:** We check for a remaining trend in the data. The cumulative running average of the observable, $\\bar{A}(t)$, is computed over the second half of the pooled trajectory. A linear regression of $\\bar{A}(t)$ against time $t$ is performed. A statistically significant non-zero slope indicates the average is still systematically drifting. We use the $t$-statistic of the slope (slope divided by its standard error) for this test. A value $|t_{\\text{slope}}| > 3$ is considered significant.\n\nIf both conditions are met—the pooled average differs from the theoretical one AND there is a significant drift—we classify the run as exhibiting finite-time bias, with code $2$. This signature distinguishes slow convergence from broken ergodicity, where replicas would be inconsistent.\n\n**4. Consistent Canonical Sampling (Code $1$):**\nIf a trajectory passes all the preceding tests, we conclude that there is no evidence of incorrect thermostating, broken ergodicity, or significant finite-time bias. The simulation is deemed consistent with proper canonical sampling, and is assigned classification code $1$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import kstest, linregress\nfrom scipy.integrate import quad\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Main function to run the diagnostic suite on all test cases and print results.\n    \"\"\"\n\n    # Test cases parameter definitions\n    test_cases = [\n        {\n            \"name\": \"Case A (Harmonic)\",\n            \"potential\": lambda x, k=1: 0.5 * k * x**2,\n            \"force\": lambda x, k=1: -k * x,\n            \"potential_params\": {\"k\": 1},\n            \"T\": 1.0,\n            \"gamma\": 1.0,\n            \"dt\": 0.005,\n            \"total_steps\": 30000,\n            \"burn_in_steps\": 5000,\n            \"initial_conditions\": [(0.0, 0.0), (0.0, 0.0)],\n            \"seed\": 101,\n            \"theory_mean_x\": 0.0,\n        },\n        {\n            \"name\": \"Case B (Non-ergodic)\",\n            \"potential\": lambda x, a=5, b=1.5, c=0.3: a * (x**2 - b**2)**2 + c * x,\n            \"force\": lambda x, a=5, b=1.5, c=0.3: -4 * a * x * (x**2 - b**2) - c,\n            \"potential_params\": {\"a\": 5, \"b\": 1.5, \"c\": 0.3},\n            \"T\": 0.2,\n            \"gamma\": 1.0,\n            \"dt\": 0.002,\n            \"total_steps\": 20000,\n            \"burn_in_steps\": 2000,\n            \"initial_conditions\": [(-1.5, 0.0), (1.5, 0.0)],\n            \"seed\": 202,\n            \"theory_mean_x\": \"compute\",\n        },\n        {\n            \"name\": \"Case C (Finite-time bias)\",\n            \"potential\": lambda x, a=1, b=1, c=0.2: a * (x**2 - b**2)**2 + c * x,\n            \"force\": lambda x, a=1, b=1, c=0.2: -4 * a * x * (x**2 - b**2) - c,\n            \"potential_params\": {\"a\": 1, \"b\": 1, \"c\": 0.2},\n            \"T\": 0.5,\n            \"gamma\": 1.0,\n            \"dt\": 0.002,\n            \"total_steps\": 6000,\n            \"burn_in_steps\": 1000,\n            \"initial_conditions\": [(-1.0, 0.0), (-1.0, 0.0)],\n            \"seed\": 303,\n            \"theory_mean_x\": \"compute\",\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result_code = run_diagnostic_case(case)\n        results.append(result_code)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef run_diagnostic_case(case_params):\n    \"\"\"\n    Runs the full diagnostic suite for a single test case.\n    \"\"\"\n    # --- 1. Generate Trajectories ---\n    trajectories = []\n    for i in range(2):  # Two replicas\n        seed = case_params[\"seed\"] + i\n        rng = np.random.default_rng(seed)\n        x0, v0 = case_params[\"initial_conditions\"][i]\n        \n        traj_x, traj_v = langevin_integrator(\n            x0, v0, case_params[\"force\"], case_params[\"potential_params\"],\n            case_params[\"T\"], case_params[\"gamma\"], case_params[\"dt\"],\n            case_params[\"total_steps\"], rng\n        )\n        # Discard burn-in\n        burn_in = case_params[\"burn_in_steps\"]\n        trajectories.append((traj_x[burn_in:], traj_v[burn_in:]))\n    \n    x1, v1 = trajectories[0]\n    x2, v2 = trajectories[1]\n\n    # --- 2. Perform Diagnostics ---\n    \n    # Code 0: Canonical Velocity Marginal Test\n    v_pooled = np.concatenate((v1, v2))\n    ks_stat, ks_pvalue = kstest(v_pooled, 'norm', args=(0, np.sqrt(case_params[\"T\"])))\n    \n    v_var = np.var(v_pooled)\n    v_var_rel_diff = np.abs(v_var - case_params[\"T\"]) / case_params[\"T\"]\n\n    if ks_pvalue < 1e-3 or v_var_rel_diff > 0.1:\n        return 0\n\n    # Code 3: Replica Consistency Test\n    stats1 = compute_observable_stats(x1)\n    stats2 = compute_observable_stats(x2)\n    \n    mean_diff = np.abs(stats1['mean'] - stats2['mean'])\n    pooled_se = np.sqrt(stats1['se']**2 + stats2['se']**2)\n    \n    z_rep = mean_diff / pooled_se if pooled_se > 0 else np.inf\n    \n    if z_rep > 3.0:\n        return 3\n\n    # Code 2: Finite-Time Bias Test\n    # Condition 1: Inconsistency with canonical expectation\n    if case_params[\"theory_mean_x\"] == \"compute\":\n        theory_mean_x = compute_canonical_average(\n            case_params[\"potential\"], case_params[\"potential_params\"], case_params[\"T\"]\n        )\n    else:\n        theory_mean_x = case_params[\"theory_mean_x\"]\n\n    x_pooled = np.concatenate((x1, x2))\n    pooled_stats = compute_observable_stats(x_pooled)\n    \n    z_canon = np.abs(pooled_stats['mean'] - theory_mean_x) / pooled_stats['se'] if pooled_stats['se'] > 0 else np.inf\n    \n    inconsistent_with_theory = z_canon > 3.0\n\n    # Condition 2: Finite-time trend\n    second_half_start = len(x_pooled) // 2\n    x_trend_data = x_pooled[second_half_start:]\n    \n    if len(x_trend_data) > 2:\n        running_avg = np.cumsum(x_trend_data) / (np.arange(len(x_trend_data)) + 1)\n        time_points = np.arange(len(x_trend_data))\n        \n        # Linear regression can fail with constant data\n        if np.ptp(running_avg) > 1e-9:\n            lin_reg = linregress(time_points, running_avg)\n            t_stat_slope = np.abs(lin_reg.slope / lin_reg.stderr) if lin_reg.stderr > 0 else 0.0\n        else:\n            t_stat_slope = 0.0\n        \n        has_significant_trend = t_stat_slope > 3.0\n    else:\n        has_significant_trend = False\n\n    if inconsistent_with_theory and has_significant_trend:\n        return 2\n\n    # Code 1: Consistent Canonical Sampling\n    return 1\n\n\ndef langevin_integrator(x0, v0, force_func, force_params, T, gamma, dt, n_steps, rng):\n    \"\"\"\n    BAOAB Langevin integrator.\n    \"\"\"\n    x_traj, v_traj = np.zeros(n_steps), np.zeros(n_steps)\n    x, v = x0, v0\n    \n    c1 = np.exp(-gamma * dt)\n    c2 = np.sqrt(T * (1 - c1**2))\n\n    for i in range(n_steps):\n        # B\n        x = x + v * dt / 2.0\n        # A\n        f = force_func(x, **force_params)\n        v = v + f * dt / 2.0\n        # O\n        v = c1 * v + c2 * rng.normal()\n        # A\n        f = force_func(x, **force_params)\n        v = v + f * dt / 2.0\n        # B\n        x = x + v * dt / 2.0\n        \n        x_traj[i], v_traj[i] = x, v\n    \n    return x_traj, v_traj\n\n\ndef compute_acf(series):\n    \"\"\"\n    Computes the autocorrelation function using FFT.\n    \"\"\"\n    n = len(series)\n    x = series - np.mean(series)\n    \n    # Pad to next power of 2 for performance\n    fft_len = 2**int(np.ceil(np.log2(2 * n - 1)))\n    \n    f = np.fft.fft(x, n=fft_len)\n    acf_full = np.fft.ifft(f * np.conj(f)).real\n    \n    # Normalize\n    acf_normalized = acf_full[:n] / acf_full[0]\n    return acf_normalized\n\n\ndef compute_observable_stats(series):\n    \"\"\"\n    Computes mean, variance, integrated autocorrelation time, and standard error.\n    \"\"\"\n    n = len(series)\n    if n < 2:\n        return {'mean': np.mean(series), 'var': 0, 'tau': 0, 'se': np.inf}\n\n    mean = np.mean(series)\n    var = np.var(series, ddof=1)\n    \n    if var < 1e-12: # Constant series\n        return {'mean': mean, 'var': var, 'tau': 0, 'se': 0}\n\n    acf = compute_acf(series)\n    \n    # Truncate sum for tau_int at first non-positive value\n    positive_acf = np.where(acf > 0)[0]\n    # Find first non-consecutive index, indicating end of initial positive part\n    if len(positive_acf) > 1:\n        first_zero_crossing_idx = np.where(np.diff(positive_acf) > 1.5)[0]\n        if len(first_zero_crossing_idx) > 0:\n            t_star = positive_acf[first_zero_crossing_idx[0]]\n        else:\n            t_star = len(acf) -1\n    else:\n        t_star = 0\n            \n    # Per problem spec: tau_int = 1/2 + sum_{t=1}^{t*} rho(t)\n    tau_int = 0.5 + np.sum(acf[1:t_star + 1])\n    \n    # Guard against negative tau due to noise\n    if tau_int < 0.5:\n        tau_int = 0.5\n\n    # Standard error of the mean for correlated data\n    se = np.sqrt(2 * tau_int * var / n) if n > 0 else np.inf\n    \n    return {'mean': mean, 'var': var, 'tau': tau_int, 'se': se}\n\n\ndef compute_canonical_average(potential_func, potential_params, T):\n    \"\"\"\n    Computes the exact canonical average <x> using numerical quadrature.\n    \"\"\"\n    # Find potential minimum for stable integration\n    res = minimize_scalar(potential_func, args=tuple(potential_params.values()), bounds=(-10, 10), method='bounded')\n    U_min = res.fun\n    \n    # Define integrands, subtracting U_min to prevent overflow/underflow\n    integrand_num = lambda x: x * np.exp(-(potential_func(x, **potential_params) - U_min) / T)\n    integrand_den = lambda x: np.exp(-(potential_func(x, **potential_params) - U_min) / T)\n    \n    # Use scipy.integrate.quad for integration\n    # The integration range [-20, 20] is sufficient for these potentials\n    num, _ = quad(integrand_num, -20, 20)\n    den, _ = quad(integrand_den, -20, 20)\n\n    return num / den if den != 0 else 0.0\n\n# Execute the main function\nsolve()\n\n```"
        }
    ]
}