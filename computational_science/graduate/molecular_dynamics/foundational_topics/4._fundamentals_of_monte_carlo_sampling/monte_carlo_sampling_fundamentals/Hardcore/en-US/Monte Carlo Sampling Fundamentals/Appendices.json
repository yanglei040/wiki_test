{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of any valid Markov Chain Monte Carlo (MCMC) simulation is ergodicity—the guarantee that the sampler can, in principle, explore the entire relevant configuration space. In molecular systems with high energy barriers or disconnected basins, standard local moves often fail, trapping the simulation in a single region and yielding non-converged results. This exercise presents a stark, idealized system with two disconnected energy basins to demonstrate how simple single-particle moves can break ergodicity, and challenges you to design and analyze a collective move to restore it . By working through this problem, you will gain a practical understanding of a fundamental challenge in molecular simulation and a common strategy for overcoming it.",
            "id": "3427345",
            "problem": "Consider a two-particle one-dimensional system intended to highlight core properties of Markov Chain Monte Carlo (MCMC) sampling in molecular dynamics. Let the configuration space be $\\Omega \\subset \\mathbb{R}^2$ with particle positions $(x_1,x_2)$. Define two basins and a hard-wall potential energy function $U:\\Omega \\rightarrow \\{0,+\\infty\\}$ as follows. The admissible basins are\n$$\n\\mathcal{A} = \\{(x_1,x_2) \\in \\mathbb{R}^2 \\mid -1 \\le x_1 \\le 0,\\,-1 \\le x_2 \\le 0,\\, x_1 \\le x_2\\}\n$$\nand\n$$\n\\mathcal{B} = \\{(x_1,x_2) \\in \\mathbb{R}^2 \\mid 1 \\le x_1 \\le 2,\\,1 \\le x_2 \\le 2,\\, x_1 \\le x_2\\}.\n$$\nSet the potential to be $U(x_1,x_2) = 0$ if $(x_1,x_2) \\in \\mathcal{A} \\cup \\mathcal{B}$ and $U(x_1,x_2) = +\\infty$ otherwise. The Metropolis acceptance rule at inverse temperature $\\beta$ is applied to proposals $(x_1,x_2) \\mapsto (x_1',x_2')$: accept with probability $\\min\\{1, \\exp(-\\beta[U(x_1',x_2') - U(x_1,x_2)])\\}$.\n\nTwo proposal move sets are considered:\n\n- Single-particle displacement moves: at each step, choose $i \\in \\{1,2\\}$ uniformly and propose $x_i' = x_i + \\delta$ with $\\delta$ drawn uniformly from $[-d,d]$, leaving the other coordinate unchanged. The proposal is rejected if $(x_1',x_2') \\notin \\mathcal{A} \\cup \\mathcal{B}$ due to the hard-wall constraint.\n\n- Collective translation moves: at each step, with probability $p_c$ select a collective move that proposes $(x_1',x_2') = (x_1 + s, x_2 + s)$ with $s \\in \\{+2,-2\\}$ chosen uniformly, and with probability $1 - p_c$ select a single-particle displacement move as above.\n\nTasks:\n\n1. Using only first principles of MCMC and the above definitions, demonstrate that with single-particle displacement moves alone, the Markov chain on $\\Omega$ restricted by $U$ is non-ergodic. Specifically, show that no admissible path comprised solely of single-particle displacements can transition from basin $\\mathcal{A}$ to basin $\\mathcal{B}$.\n\n2. Identify a necessary form of collective move to restore ergodicity in the presence of the hard-wall potential and justify that it maintains detailed balance. Then, at the level of macrostates $\\{\\mathcal{A}, \\mathcal{B}\\}$, derive the mixing time $t_{\\mathrm{mix}}(\\varepsilon)$ in terms of $p_c$ and a total variation threshold $\\varepsilon$ for the resulting two-state Markov chain under the symmetric choice $s \\in \\{+2,-2\\}$.\n\n3. Implement a program that performs two computations:\n   - Construct a discretized state graph using step size $h = 0.5$ by enumerating admissible grid points in $\\mathcal{A}$ and $\\mathcal{B}$ and connecting states by edges if they differ by a single-particle displacement of magnitude $h$ on one coordinate and remain admissible. Return a boolean indicating non-ergodicity, defined here as the graph having more than one connected component under these single-particle moves. The boolean must be computed from the graph connectivity on this discretization; it must not be hard-coded.\n   - For four specified values of the collective-move probability $p_c \\in \\{1.0, 0.2, 0.02, 0.0\\}$ and a fixed total variation threshold $\\varepsilon = 10^{-3}$, compute the mixing time $t_{\\mathrm{mix}}(\\varepsilon)$ for the macro two-state chain $\\{\\mathcal{A},\\mathcal{B}\\}$ induced by the collective move. Express $t_{\\mathrm{mix}}(\\varepsilon)$ as an integer number of steps using the ceiling of your analytical expression, except when it is infinite (for $p_c = 0.0$). In the special case where the macro transition probability equals $0.5$, take $t_{\\mathrm{mix}}(\\varepsilon)$ to be $1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the exact order\n$$\n[\\text{non\\_ergodic\\_boolean},\\, t_{\\mathrm{mix}}(p_c=1.0),\\, t_{\\mathrm{mix}}(p_c=0.2),\\, t_{\\mathrm{mix}}(p_c=0.02),\\, t_{\\mathrm{mix}}(p_c=0.0)].\n$$\nThe first entry is a boolean, and the remaining entries are integers, except the last which is a float $+\\infty$ when appropriate. No physical units are involved; all times are dimensionless counts of steps. Angles do not appear. Percentages must not be used; probabilities are to be treated as real numbers in $[0,1]$.\n\nTest suite for coverage:\n- Graph construction with $h = 0.5$ to detect non-ergodicity under single-particle moves.\n- Collective-move probabilities $p_c \\in \\{1.0, 0.2, 0.02, 0.0\\}$ at threshold $\\varepsilon = 10^{-3}$ to probe fast mixing, moderate mixing, slow mixing, and the boundary case of no mixing across basins.",
            "solution": "### Part 1: Non-Ergodicity of Single-Particle Displacement Moves\n\nAn MCMC sampler is ergodic if it is possible to transition from any state in the configuration space to any other state in a finite number of steps. The configuration space with finite potential energy is the union of two disjoint basins, $\\mathcal{A}$ and $\\mathcal{B}$.\n$$\n\\mathcal{A} = \\{(x_1,x_2) \\in \\mathbb{R}^2 \\mid -1 \\le x_1 \\le 0,\\,-1 \\le x_2 \\le 0,\\, x_1 \\le x_2\\}\n$$\n$$\n\\mathcal{B} = \\{(x_1,x_2) \\in \\mathbb{R}^2 \\mid 1 \\le x_1 \\le 2,\\,1 \\le x_2 \\le 2,\\, x_1 \\le x_2\\}\n$$\nThe potential $U(x_1,x_2)$ is $0$ for $(x_1,x_2) \\in \\mathcal{A} \\cup \\mathcal{B}$ and $+\\infty$ otherwise.\n\nLet the initial state of the system be $(x_1, x_2) \\in \\mathcal{A}$. By definition of $\\mathcal{A}$, this implies $-1 \\le x_1 \\le 0$ and $-1 \\le x_2 \\le 0$. We consider a single-particle displacement move, which proposes a new state $(x_1', x_2')$. The Metropolis acceptance probability for a transition from a state $x$ to $x'$ is $\\min\\{1, \\exp(-\\beta[U(x') - U(x)])\\}$. Since $U(x)=0$ within $\\mathcal{A}$, any accepted move to a new state $x'$ must have $U(x')=0$, meaning $x' \\in \\mathcal{A} \\cup \\mathcal{B}$.\n\nThere are two cases for a single-particle move from $(x_1, x_2) \\in \\mathcal{A}$:\n\n1.  **Displace particle $1$**: The proposed new state is $(x_1', x_2') = (x_1 + \\delta, x_2)$, where $\\delta$ is a random displacement. The coordinate $x_2$ remains unchanged, so $x_2' = x_2 \\in [-1, 0]$. A state $(y_1, y_2)$ in basin $\\mathcal{B}$ must satisfy $1 \\le y_2 \\le 2$. Since $x_2' \\notin [1, 2]$, the proposed state $(x_1', x_2')$ cannot be in $\\mathcal{B}$. Therefore, for the move to be accepted (i.e., for the potential to remain finite), the new state must lie in $\\mathcal{A}$.\n\n2.  **Displace particle $2$**: The proposed new state is $(x_1', x_2') = (x_1, x_2 + \\delta)$. The coordinate $x_1$ remains unchanged, so $x_1' = x_1 \\in [-1, 0]$. A state $(y_1, y_2)$ in basin $\\mathcal{B}$ must satisfy $1 \\le y_1 \\le 2$. Since $x_1' \\notin [1, 2]$, the proposed state $(x_1', x_2')$ cannot be in $\\mathcal{B}$. Therefore, for the move to be accepted, the new state must lie in $\\mathcal{A}$.\n\nIn both cases, any accepted single-particle move originating from a state in $\\mathcal{A}$ results in a state that is also in $\\mathcal{A}$. It is impossible to generate a state in $\\mathcal{B}$ from a state in $\\mathcal{A}$ using only single-particle displacements. An analogous argument shows that one cannot transition from $\\mathcal{B}$ to $\\mathcal{A}$. The state space is partitioned into two disconnected regions. Consequently, the Markov chain is non-ergodic.\n\n### Part 2: Restoring Ergodicity and Derivation of Mixing Time\n\nThe problem proposes a collective translation move to restore ergodicity: $(x_1', x_2') = (x_1 + s, x_2 + s)$, where $s$ is chosen uniformly from $\\{+2, -2\\}$.\n\n**Restoring Ergodicity**:\n- If $(x_1, x_2) \\in \\mathcal{A}$, then $-1 \\le x_1, x_2 \\le 0$ and $x_1 \\le x_2$. Applying a translation with $s = +2$ gives $(x_1', x_2') = (x_1+2, x_2+2)$. The new coordinates satisfy $1 \\le x_1', x_2' \\le 2$. The ordering is preserved: $x_1 \\le x_2 \\implies x_1+2 \\le x_2+2 \\implies x_1' \\le x_2'$. Thus, the new state $(x_1', x_2')$ is guaranteed to be in $\\mathcal{B}$.\n- If $(x_1, x_2) \\in \\mathcal{B}$, then $1 \\le x_1, x_2 \\le 2$ and $x_1 \\le x_2$. Applying a translation with $s = -2$ gives $(x_1', x_2') = (x_1-2, x_2-2)$. The new coordinates satisfy $-1 \\le x_1', x_2' \\le 0$ and $x_1' \\le x_2'$. Thus, the new state $(x_1', x_2')$ is guaranteed to be in $\\mathcal{A}$.\nThis move successfully connects the two basins. When combined with the single-particle moves (which ensure ergodicity within each basin), the entire Markov chain becomes ergodic.\n\n**Detailed Balance**:\nThe stationary distribution $\\pi(x)$ for this system is uniform over the allowed region $\\mathcal{A} \\cup \\mathcal{B}$, because the potential $U(x)$ is constant (zero) there. Detailed balance requires $\\pi(x)P(x \\to x') = \\pi(x')P(x' \\to x)$, where $P(x \\to x') = T(x \\to x')A(x \\to x')$ is the transition probability, composed of the proposal probability $T$ and the acceptance probability $A$.\nFor the collective move between $x \\in \\mathcal{A}$ and $x' \\in \\mathcal{B}$, we have $\\pi(x) = \\pi(x')$ since the distribution is uniform. The potential energy is $U(x) = U(x') = 0$, so the acceptance probability is $A(x \\to x') = \\min\\{1, \\exp(-0)\\} = 1$. The proposal from $x$ to $x' = x+(s,s)$ and from $x'$ to $x = x'-(s,s)$ is symmetric because $s$ is chosen uniformly from $\\{+2, -2\\}$. Thus, $T(x \\to x') = T(x' \\to x)$. With $\\pi(x)=\\pi(x')$, $T(x \\to x')=T(x' \\to x)$, and $A(x \\to x')=A(x' \\to x)=1$, the detailed balance condition is satisfied.\n\n**Mixing Time for the Macrostate Chain**:\nWe analyze the two-state Markov chain on the macrostates $\\{\\mathcal{A}, \\mathcal{B}\\}$. The area of $\\mathcal{A}$ is $\\frac{1}{2}(1 \\times 1) = 1/2$ and the area of $\\mathcal{B}$ is $\\frac{1}{2}(1 \\times 1) = 1/2$. Thus, the stationary probabilities are equal: $\\pi(\\mathcal{A}) = \\pi(\\mathcal{B}) = 1/2$.\nA transition between macrostates occurs only via a collective move.\n- The probability of transitioning from $\\mathcal{A}$ to $\\mathcal{B}$ in one step is $P(\\mathcal{A} \\to \\mathcal{B}) = P(\\text{select collective}) \\times P(\\text{select } s=+2) = p_c \\times (1/2) = p_c/2$.\n- The probability of transitioning from $\\mathcal{B}$ to $\\mathcal{A}$ is $P(\\mathcal{B} \\to \\mathcal{A}) = p_c \\times (1/2) = p_c/2$.\nLet $\\alpha = p_c/2$. The transition matrix for the macrostates $(\\mathcal{A}, \\mathcal{B})$ is:\n$$\nM = \\begin{pmatrix} 1-\\alpha & \\alpha \\\\ \\alpha & 1-\\alpha \\end{pmatrix}\n$$\nThe eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = 1 - 2\\alpha = 1 - p_c$. The rate of convergence to the stationary distribution is governed by the second largest eigenvalue modulus, $|\\lambda_2| = |1 - p_c|$. For $p_c \\in [0, 1]$, this is $1-p_c$.\nThe total variation distance $d_{TV}$ from the stationary distribution $\\pi_{macro} = [1/2, 1/2]^T$ after $t$ steps, starting from a pure state (worst case), is $d_{TV}(t) = \\frac{1}{2}|\\lambda_2|^t = \\frac{1}{2}(1-p_c)^t$.\nWe seek the mixing time $t_{\\mathrm{mix}}(\\varepsilon)$, the smallest integer $t$ such that $d_{TV}(t) \\le \\varepsilon$.\n$$\n\\frac{1}{2}(1-p_c)^t \\le \\varepsilon \\implies (1-p_c)^t \\le 2\\varepsilon\n$$\nTaking the natural logarithm:\n$$\nt \\ln(1-p_c) \\le \\ln(2\\varepsilon)\n$$\nFor $p_c \\in (0, 1)$, $\\ln(1-p_c)$ is negative, so we reverse the inequality:\n$$\nt \\ge \\frac{\\ln(2\\varepsilon)}{\\ln(1-p_c)}\n$$\nThe mixing time is the ceiling of this value:\n$$\nt_{\\mathrm{mix}}(\\varepsilon) = \\left\\lceil \\frac{\\ln(2\\varepsilon)}{\\ln(1-p_c)} \\right\\rceil\n$$\nSpecial cases:\n- If $p_c=0$, the denominator is $\\ln(1)=0$, so $t_{\\mathrm{mix}} = \\infty$.\n- If $p_c=1$, the macro transition probability $\\alpha = 1/2$. As per the problem, $t_{\\mathrm{mix}}=1$. This aligns with the fact that $\\lambda_2=0$, indicating immediate convergence.\n\n### Part 3: Implementation Plan\n\n1.  **Non-Ergodicity Test**:\n    - Discretize the state space with step size $h=0.5$. The valid coordinates for basin $\\mathcal{A}$ are $\\{-1.0, -0.5, 0.0\\}$ and for basin $\\mathcal{B}$ are $\\{1.0, 1.5, 2.0\\}$.\n    - Enumerate all states $(x_1, x_2)$ in $\\mathcal{A} \\cup \\mathcal{B}$ satisfying the constraints (e.g., $x_1 \\le x_2$). This results in $6$ states in $\\mathcal{A}$ and $6$ states in $\\mathcal{B}$, for a total of $12$ states.\n    - Build a graph where states are nodes. An edge exists between two nodes if one can be reached from the other via a single-particle displacement of magnitude $h=0.5$.\n    - Perform a graph traversal (e.g., Breadth-First or Depth-First Search) to count the number of connected components. If the count is greater than $1$, the system is non-ergodic on this discretization, and the boolean is `True`.\n\n2.  **Mixing Time Calculation**:\n    - Use the derived formula $t_{\\mathrm{mix}}(\\varepsilon) = \\lceil \\frac{\\ln(2\\varepsilon)}{\\ln(1-p_c)} \\rceil$ with $\\varepsilon = 10^{-3}$.\n    - For $p_c = 1.0$, use the special case rule $t_{\\mathrm{mix}}=1$.\n    - For $p_c \\in \\{0.2, 0.02\\}$, compute the formula. With $2\\varepsilon = 0.002$:\n      - $p_c=0.2$: $t = \\ln(0.002)/\\ln(0.8) \\approx 27.85 \\implies \\lceil 27.85 \\rceil = 28$.\n      - $p_c=0.02$: $t = \\ln(0.002)/\\ln(0.98) \\approx 307.65 \\implies \\lceil 307.65 \\rceil = 308$.\n    - For $p_c=0.0$, the mixing time is infinite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the two-part problem:\n    1. Determines ergodicity of a discretized system via graph connectivity.\n    2. Calculates MCMC mixing times for a macrostate model.\n    \"\"\"\n\n    # --- Task 1: Check for non-ergodicity on a discretized grid ---\n\n    # Define the step size and coordinate grids for the two basins.\n    h = 0.5\n    coords_A = [-1.0, -0.5, 0.0]\n    coords_B = [1.0, 1.5, 2.0]\n\n    # Enumerate all valid states in basin A satisfying x1 <= x2.\n    states_A = []\n    for x1 in coords_A:\n        for x2 in coords_A:\n            if x1 <= x2:\n                states_A.append((x1, x2))\n    \n    # Enumerate all valid states in basin B satisfying x1 <= x2.\n    states_B = []\n    for x1 in coords_B:\n        for x2 in coords_B:\n            if x1 <= x2:\n                states_B.append((x1, x2))\n                \n    # Combine states from both basins and create a mapping for efficient lookup.\n    all_states = states_A + states_B\n    num_states = len(all_states)\n    state_to_idx = {state: i for i, state in enumerate(all_states)}\n\n    # Build an adjacency list for the graph.\n    adj = [[] for _ in range(num_states)]\n    for i, state in enumerate(all_states):\n        x1, x2 = state\n        # Define the four possible single-particle moves of magnitude h.\n        potential_moves = [(h, 0), (-h, 0), (0, h), (0, -h)]\n        for dx, dy in potential_moves:\n            # Calculate the neighbor coordinate.\n            # a direct key lookup will work as coordinates are exact multiples of h=0.5\n            neighbor_coord = (x1 + dx, x2 + dy)\n            if neighbor_coord in state_to_idx:\n                neighbor_idx = state_to_idx[neighbor_coord]\n                adj[i].append(neighbor_idx)\n\n    # Count connected components using Breadth-First Search (BFS) to determine ergodicity.\n    # Non-ergodicity is defined as having more than one connected component.\n    visited = [False] * num_states\n    num_components = 0\n    for i in range(num_states):\n        if not visited[i]:\n            num_components += 1\n            q = [i]\n            visited[i] = True\n            head = 0\n            while head < len(q):\n                u = q[head]\n                head += 1\n                for v in adj[u]:\n                    if not visited[v]:\n                        visited[v] = True\n                        q.append(v)\n    \n    non_ergodic_boolean = (num_components > 1)\n\n    # --- Task 2: Calculate mixing times for the two-state macrochain ---\n\n    pc_values = [1.0, 0.2, 0.02, 0.0]\n    epsilon = 1e-3\n    mixing_times = []\n\n    for pc in pc_values:\n        # The macrostate transition probability p(A->B) is alpha = pc/2.\n        # The problem defines a special case when this probability is 0.5.\n        if pc == 1.0: # Corresponds to macro transition prob = 0.5\n            mixing_times.append(1)\n        elif pc == 0.0: # No collective moves, so basins are disconnected.\n            mixing_times.append(float('inf'))\n        else:\n            # For 0 < pc < 1, use the derived analytical formula:\n            # t_mix = ceil(ln(2*eps) / ln(1-pc))\n            t_mix_float = np.log(2 * epsilon) / np.log(1 - pc)\n            mixing_times.append(int(np.ceil(t_mix_float)))\n\n    # --- Assemble and print the final output in the required format ---\n    \n    # Combine the boolean result with the list of mixing times.\n    # The boolean is formatted to lowercase as per common data standards.\n    results = [str(non_ergodic_boolean).lower()] + mixing_times\n    \n    # The final print statement must produce a single line in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "At the heart of the Metropolis-Hastings algorithm lies the detailed balance condition, which ensures that our simulation correctly samples from the desired target probability distribution. While straightforward for simple moves in Cartesian space, its application becomes more subtle when dealing with complex degrees of freedom, such as molecular rotations. This practice problem focuses on deriving the essential Hastings correction factor for a rotational move proposed in Euler angle coordinates, a common scenario in rigid-body simulation . Successfully completing this derivation is a key step toward mastering the construction of custom, efficient, and rigorously correct MCMC samplers.",
            "id": "3427343",
            "problem": "In a Monte Carlo sampling of rotational degrees of freedom for a rigid asymmetric molecule immersed in an external field, the configuration space of orientations is the Special Orthogonal Group $\\mathrm{SO}(3)$ parameterized by $\\mathrm{ZYZ}$ Euler angles $(\\phi,\\theta,\\psi)$ with ranges $\\phi \\in [0,2\\pi)$, $\\theta \\in [0,\\pi]$, and $\\psi \\in [0,2\\pi)$. The invariant volume element on $\\mathrm{SO}(3)$, known as the Haar measure, has the local form $d\\mu(\\phi,\\theta,\\psi) = \\sin\\theta\\, d\\phi\\, d\\theta\\, d\\psi$ in these coordinates. The physical target distribution over orientations has a density with respect to the Haar measure given by $\\pi(R) \\propto \\exp(-\\beta U(R))$, where $R \\in \\mathrm{SO}(3)$ encodes the orientation and $U(R)$ is the potential energy; $\\beta$ is the inverse temperature defined as $\\beta = 1/(k_{\\mathrm{B}}T)$ with $k_{\\mathrm{B}}$ the Boltzmann constant and $T$ the absolute temperature.\n\nA Metropolis-Hastings (MH) update is constructed by proposing a new orientation in angle coordinates via an independence proposal that draws $(\\phi',\\theta',\\psi')$ from the product measure with $\\phi'$ and $\\psi'$ uniform on $[0,2\\pi)$ and $\\theta'$ uniform on $[0,\\pi]$, independently of the current $(\\phi,\\theta,\\psi)$. This proposal is non-uniform with respect to the Haar measure on $\\mathrm{SO}(3)$ because it is uniform in the Euler angles rather than in the invariant measure.\n\nStarting from the definition of detailed balance on $\\mathrm{SO}(3)$ and using change-of-variables and absolute continuity of densities under the coordinate map $(\\phi,\\theta,\\psi) \\mapsto R(\\phi,\\theta,\\psi)$, derive the Hastings correction factor that arises solely from the Jacobian $\\sin\\theta$ of the angular measure when expressing the target density with respect to Lebesgue measure in angle coordinates. Provide the final Hastings factor as a closed-form expression depending only on the current polar angle $\\theta$ and the proposed polar angle $\\theta'$. Do not include energy, temperature, or proposal density terms; isolate only the factor that corrects for the Jacobian. The final answer must be a single analytical expression.",
            "solution": "The problem requires the derivation of the Hastings correction factor for a Metropolis-Hastings (MH) Monte Carlo update on the orientation space $\\mathrm{SO}(3)$ of a rigid molecule. The correction arises because the proposal mechanism is uniform in the ZYZ Euler angle coordinates $(\\phi, \\theta, \\psi)$, while the invariant measure on the group $\\mathrm{SO}(3)$ is not.\n\nThe core of the Metropolis-Hastings algorithm is the detailed balance condition, which ensures that the desired target probability distribution, $\\pi$, is the stationary distribution of the generated Markov chain. For any two states $x$ and $x'$, detailed balance requires:\n$$\n\\pi(x) q(x'|x) A(x'|x) = \\pi(x') q(x|x') A(x|x')\n$$\nwhere $q(x'|x)$ is the density for proposing a move from state $x$ to $x'$, and $A(x'|x)$ is the probability of accepting that move. The standard solution for the acceptance probability is:\n$$\nA(x'|x) = \\min\\left(1, \\frac{\\pi(x') q(x|x')}{\\pi(x) q(x'|x)}\\right)\n$$\nThe term $\\frac{q(x|x')}{q(x'|x)}$ is known as the Hastings correction factor. A critical requirement for this formalism is that all probability densities, $\\pi$ and $q$, must be expressed with respect to the same underlying reference measure.\n\nLet the state of the system be an orientation $R \\in \\mathrm{SO}(3)$. The natural and most elegant choice for the reference measure on this space is the invariant Haar measure, $d\\mu(R)$. In terms of the ZYZ Euler angles $(\\phi, \\theta, \\psi)$, this measure has the local expression $d\\mu(R) = \\sin\\theta \\, d\\phi \\, d\\theta \\, d\\psi$.\n\nFirst, we define the target density with respect to this Haar measure. The problem states that the physical target distribution has a density $\\pi(R)$ with respect to the Haar measure given by $\\pi(R) \\propto \\exp(-\\beta U(R))$. We can write this as $\\pi(R) = C_{\\pi} \\exp(-\\beta U(R))$, where $C_{\\pi}$ is a normalization constant.\n\nNext, we must determine the proposal density $q(R'|R)$ with respect to the same Haar measure $d\\mu$. The problem describes an independence proposal where the new Euler angles $(\\phi', \\theta', \\psi')$ are drawn independently of the current state. The angles $\\phi'$ and $\\psi'$ are drawn uniformly from $[0, 2\\pi)$, and $\\theta'$ is drawn uniformly from $[0, \\pi]$. The probability density of this proposal, with respect to the Lebesgue measure on the coordinate space, $d\\lambda = d\\phi' d\\theta' d\\psi'$, is:\n$$\ng_{\\lambda}(\\phi', \\theta', \\psi') = \\frac{1}{2\\pi} \\cdot \\frac{1}{\\pi} \\cdot \\frac{1}{2\\pi} = \\frac{1}{4\\pi^2}\n$$\nThis is the proposal density in the coordinate space. To find the proposal density $q_{\\mu}(R'|R)$ with respect to the Haar measure $d\\mu(R')$, we use the principle of conservation of probability. The probability of proposing a state in an infinitesimal volume must be independent of the coordinate system used to describe that volume.\n$$\nq_{\\mu}(R'|R) \\, d\\mu(R') = g_{\\lambda}(\\phi', \\theta', \\psi') \\, d\\lambda\n$$\nWe know the relationship between the two volume elements is $d\\mu(R') = \\sin\\theta' \\, d\\lambda$. Substituting $d\\lambda = d\\mu(R') / \\sin\\theta'$ into the equation gives:\n$$\nq_{\\mu}(R'|R) \\, d\\mu(R') = g_{\\lambda}(\\phi', \\theta', \\psi') \\, \\frac{d\\mu(R')}{\\sin\\theta'}\n$$\nThus, the proposal density with respect to the Haar measure is:\n$$\nq_{\\mu}(R'|R) = \\frac{g_{\\lambda}(\\phi', \\theta', \\psi')}{\\sin\\theta'} = \\frac{1}{4\\pi^2 \\sin\\theta'}\n$$\nThis is an independence sampler, so the proposal density depends only on the proposed state $R'$, which corresponds to angles $(\\phi', \\theta', \\psi')$, and not on the current state $R$. So, $q_{\\mu}(R'|R) = q_{\\mu}(R')$. The density for the reverse proposal, from $R'$ to $R$, is found by replacing the primed variables with unprimed ones:\n$$\nq_{\\mu}(R|R') = \\frac{1}{4\\pi^2 \\sin\\theta}\n$$\nNow we can compute the Hastings correction factor, which is the ratio of the reverse to the forward proposal densities:\n$$\n\\frac{q_{\\mu}(R|R')}{q_{\\mu}(R'|R)} = \\frac{\\frac{1}{4\\pi^2 \\sin\\theta}}{\\frac{1}{4\\pi^2 \\sin\\theta'}} = \\frac{4\\pi^2 \\sin\\theta'}{4\\pi^2 \\sin\\theta} = \\frac{\\sin\\theta'}{\\sin\\theta}\n$$\nThis factor accounts for the fact that proposing angles uniformly is not equivalent to proposing orientations uniformly on the sphere of directions associated with the polar angle. Specifically, a uniform proposal in $\\theta$ is more likely to generate orientations near the poles ($\\theta=0$ or $\\theta=\\pi$) where the invariant volume element $\\sin\\theta \\, d\\theta$ is small. The Hastings factor correctly compensates for this bias.\n\nThe full acceptance ratio would be $\\frac{\\pi(R')}{\\pi(R)} \\frac{q_{\\mu}(R|R')}{q_{\\mu}(R'|R)} = \\exp(-\\beta(U(R')-U(R))) \\frac{\\sin\\theta'}{\\sin\\theta}$. The problem asks to isolate only the Hastings correction factor itself, which depends only on the current and proposed polar angles, $\\theta$ and $\\theta'$.",
            "answer": "$$\\boxed{\\frac{\\sin\\theta'}{\\sin\\theta}}$$"
        },
        {
            "introduction": "Generating a long simulation trajectory is only the first step; extracting statistically meaningful results requires a careful analysis of the output data. A key feature of data from MCMC or Molecular Dynamics is its inherent time-correlation—successive samples are not independent, which invalidates simple statistical error formulas. This exercise guides you through the logic of the block bootstrap, a powerful and widely used non-parametric technique for estimating confidence intervals from correlated time series . Understanding this method is essential for moving from raw simulation output to scientifically reliable conclusions with robust error bars.",
            "id": "3427318",
            "problem": "Consider a time series $\\{A_t\\}_{t=1}^N$ of a scalar observable $A$ recorded from a single long trajectory of an ergodic Markov chain generated by Molecular Dynamics (MD) or Markov Chain Monte Carlo (MCMC). The goal is to estimate a confidence interval (CI) for the sample mean $\\hat{A}_N = \\frac{1}{N}\\sum_{t=1}^N A_t$. The chain is stationary with equilibrium mean $\\mathbb{E}[A] = \\mu$ and autocovariance function $C(k) = \\mathbb{E}\\big[(A_t-\\mu)(A_{t+k}-\\mu)\\big]$, and has normalized autocorrelation function $\\rho(k) = C(k)/C(0)$ that decays over a characteristic correlation scale. The Integrated Autocorrelation Time (IAT), defined as $\\tau_{\\text{int}} = 1 + 2\\sum_{k=1}^{\\infty} \\rho(k)$, quantifies the resulting statistical inefficiency. The Central Limit Theorem for geometrically ergodic Markov chains states that under mild conditions, $\\sqrt{N}\\big(\\hat{A}_N - \\mu\\big)$ converges in distribution to a normal random variable with variance inflation determined by time correlations.\n\nYou seek a nonparametric resampling method that respects the serial dependence to approximate the sampling distribution of $\\hat{A}_N$ and to produce a scientifically justified CI. Which option below most correctly outlines a block bootstrap procedure for correlated Markov chain data and justifies the choice of block length relative to $\\tau_{\\mathrm{int}}$?\n\nA. Estimate $\\tau_{\\mathrm{int}}$ from the time series, choose a block length $L_b$ on the order of a small multiple of $\\tau_{\\mathrm{int}}$, e.g., $L_b \\approx \\alpha\\,\\tau_{\\mathrm{int}}$ with $\\alpha$ between $2$ and $10$, construct overlapping contiguous blocks of length $L_b$ from $\\{A_t\\}_{t=1}^N$ (using a circular wrap to allow blocks that start near $t=N$), draw with replacement $K=\\lceil N/L_b \\rceil$ blocks and concatenate them to form a bootstrap series of length at least $N$, truncate to exactly $N$, compute $\\hat{A}_N^{\\ast}$ for each of $B$ bootstrap replicates, and use the empirical quantiles of $\\{\\hat{A}_N^{\\ast}\\}_{b=1}^B$ to form a CI. Justification: choosing $L_b$ comparable to a few $\\tau_{\\mathrm{int}}$ preserves most of the within-block dependence while allowing many blocks, balancing variance and bias in the bootstrap distribution.\n\nB. Because the Markov chain is ergodic, resample individual points $A_t$ independently with replacement ($L_b=1$) to form $B$ bootstrap series of length $N$, compute $\\hat{A}_N^{\\ast}$ for each, and use their quantiles for the CI. Justification: ergodicity guarantees independence in the long run, so single-point resampling is adequate.\n\nC. To avoid bias from broken correlations, set the block length equal to the full trajectory, $L_b=N$, and resample one whole-block per replicate. Compute $\\hat{A}_N^{\\ast}$ and use its empirical distribution for the CI. Justification: using entire-trajectory blocks perfectly preserves dependence and thus produces the most faithful bootstrap distribution.\n\nD. Partition the series into nonoverlapping contiguous blocks of length $L_b \\ll \\tau_{\\mathrm{int}}$ to maximize the number of available blocks, sample these blocks with replacement to form bootstrap series, and compute $\\hat{A}_N^{\\ast}$. Justification: shorter blocks reduce the variance of the bootstrap estimator by increasing the number of blocks and therefore improve CI tightness.\n\nE. Use the stationary bootstrap with geometrically distributed block lengths having mean $L_b$ set well below $\\tau_{\\mathrm{int}}$ to avoid over-smoothing, generate $B$ bootstrap series, compute $\\hat{A}_N^{\\ast}$ and derive a CI from their empirical distribution. Justification: random short blocks prevent artificially long-range correlations in the bootstrap series and thus yield reliable CIs.",
            "solution": "The problem asks to identify the correct procedure for applying a block bootstrap to estimate confidence intervals from a correlated time series, a common task in analyzing data from MD or MCMC simulations. The key challenge is to perform resampling in a way that preserves the correlation structure of the original data.\n\n- **Option A** correctly describes the standard (overlapping) block bootstrap procedure. The core idea is to choose a block length $L_b$ that is significantly larger than the integrated autocorrelation time $\\tau_{\\mathrm{int}}$. This ensures that the data within each block captures the essential correlation structure of the time series. By resampling these blocks, the bootstrap replicates maintain a similar correlation pattern. The blocks themselves, if $L_b$ is large enough, are approximately independent, which is a key assumption. Choosing $L_b$ involves a bias-variance trade-off: if $L_b$ is too small, the correlation is not captured (leading to biased, underestimated variance); if $L_b$ is too large, the number of blocks becomes small, leading to a high-variance estimate of the sampling distribution. A rule of thumb is to choose $L_b$ to be several times $\\tau_{\\mathrm{int}}$. Therefore, this option is correct.\n\n- **Option B** suggests resampling individual data points. This is the standard bootstrap for independent and identically distributed (i.i.d.) data. Applying it to a correlated time series is fundamentally flawed because it destroys the time dependence structure. The resulting bootstrap replicates will have the same marginal distribution but will be uncorrelated, leading to a severe underestimation of the true variance of the sample mean.\n\n- **Option C** suggests using a single block of length $N$. While this perfectly preserves the correlations within the block (as it's the entire series), the resampling process is meaningless. When you resample this single block, you get the original time series back every single time. The \"bootstrap distribution\" of the mean would consist of a single point, $\\hat{A}_N$, making it impossible to estimate a variance or a confidence interval.\n\n- **Option D** suggests using a block length $L_b \\ll \\tau_{\\mathrm{int}}$. This is incorrect. Such short blocks fail to capture the full correlation structure of the data. The bootstrap series constructed from these blocks will exhibit much shorter correlations than the original series, leading to a biased bootstrap distribution that underestimates the true variance.\n\n- **Option E** describes the stationary bootstrap but recommends a mean block length $L_b$ well below $\\tau_{\\mathrm{int}}$. This is the same fundamental error as in option D. For any block-based bootstrap method (stationary or non-stationary) to be effective, the block length must be large enough to capture the essential temporal dependencies.\n\nTherefore, option A provides the most scientifically sound description of the block bootstrap methodology for correlated data.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}