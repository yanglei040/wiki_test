{
    "hands_on_practices": [
        {
            "introduction": "The theoretical guarantee that all norms on a finite-dimensional space are equivalent is a cornerstone of functional analysis. However, for practical applications in numerical analysis and optimization, knowing the *exact* equivalence constants is crucial. This first practice moves beyond the abstract statement of equivalence by tasking you with deriving these constants from first principles. By framing the problem as a constrained optimization and applying the method of Lagrange multipliers, you will not only find the general constant $C$ in the inequality $\\|x\\|_p \\le C \\|x\\|_q but also discover the beautifully simple structure of the vectors that make this bound tight .",
            "id": "3544577",
            "problem": "Given integers $n \\ge 1$ and real exponents $p,q \\in [1,+\\infty]$, consider the family of vector norms on $\\mathbb{R}^n$ defined by $ \\|x\\|_r = \\left(\\sum_{i=1}^n |x_i|^r\\right)^{1/r}$ for $r \\in [1,+\\infty)$ and $\\|x\\|_{+\\infty} = \\max_{1\\le i \\le n} |x_i|$. The smallest constant $C = C(n,p,q)$ such that $ \\|x\\|_p \\le C \\|x\\|_q$ holds for all $x \\in \\mathbb{R}^n$ and $x \\ne 0$ is given by the supremum $C = \\sup_{x \\ne 0} \\frac{\\|x\\|_p}{\\|x\\|_q}$. Your task is to design and implement a procedure that, for given $n$, $p$, and $q$, estimates this smallest $C$ and identifies vectors that make the bound tight (extremal vectors).\n\nBase your derivation and algorithmic design only on fundamental definitions of norms and standard optimization principles for finite-dimensional spaces. In particular, formulate the problem as a constrained maximization of $\\|x\\|_p$ subject to $\\|x\\|_q = 1$ and reason from first principles to characterize the structure of extremal vectors. Do not assume or quote the final equivalence constant formula; instead, derive it from the structure of the maximizers.\n\nYour program must:\n- Implement a general routine that, given $n$, $p$, and $q$, estimates the optimal constant by searching over candidate extremal structures justified by your derivation, and reports:\n  - the theoretically derived constant $C(n,p,q)$,\n  - the estimated constant from your search procedure,\n  - the absolute discrepancy between these two values,\n  - an integer $k^\\star$ representing the number of nonzero components in an extremal vector identified by your procedure.\n- Treat the cases $p=+\\infty$ or $q=+\\infty$ carefully using valid limiting arguments consistent with the norm definitions.\n- Use only pure mathematical quantities; there are no physical units involved in this problem.\n- For angles, trigonometric functions, or percentages: these do not apply here and must not be used.\n\nTest Suite:\nProvide results for the following parameter sets $(n,p,q)$:\n- Test $1$: $n=7$, $p=1.5$, $q=3.2$.\n- Test $2$: $n=10$, $p=4$, $q=2$.\n- Test $3$: $n=8$, $p=+\\infty$, $q=2$.\n- Test $4$: $n=5$, $p=1$, $q=+\\infty$.\n- Test $5$: $n=6$, $p=2$, $q=2$.\n\nAnswer Format:\n- For each test, return a list $[C_{\\mathrm{theory}}, C_{\\mathrm{est}}, \\mathrm{err}, k^\\star]$, where $C_{\\mathrm{theory}}$ is the theoretically derived constant, $C_{\\mathrm{est}}$ is the estimated constant produced by your search routine, $\\mathrm{err}$ is the absolute difference $|C_{\\mathrm{theory}} - C_{\\mathrm{est}}|$, and $k^\\star$ is the number of nonzeros in an extremal vector selected by your method.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test’s result represented as a sublist and no spaces anywhere. For example: $[ [a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],\\dots ]$ printed without spaces.\n\nYour implementation must be deterministic and require no user input. The final printed line must follow the exact format described above.",
            "solution": "The problem requires the derivation of the optimal constant $C = C(n,p,q)$ in the vector norm equivalence inequality $\\|x\\|_p \\le C \\|x\\|_q$ for $x \\in \\mathbb{R}^n$, and the characterization of the extremal vectors that achieve this bound. The constant $C$ is defined as the supremum of the ratio of norms:\n$$\nC = \\sup_{x \\in \\mathbb{R}^n, x \\ne 0} \\frac{\\|x\\|_p}{\\|x\\|_q}\n$$\nDue to the homogeneity property of norms, i.e., $\\|\\alpha x\\| = |\\alpha| \\|x\\|$ for any scalar $\\alpha$, the ratio is independent of the magnitude of $x$. We can therefore rephrase the problem as a constrained optimization problem: finding the maximum of $\\|x\\|_p$ over the set of all vectors $x$ for which $\\|x\\|_q = 1$.\n$$\nC = \\max_{\\|x\\|_q=1} \\|x\\|_p\n$$\nThe functions $\\|x\\|_p$ and $\\|x\\|_q$ depend only on the absolute values of the components of $x$, $|x_i|$. Thus, we can restrict our search for a maximizing vector $x^*$ to the non-negative orthant, where $x_i \\ge 0$ for all $i=1, \\dots, n$.\n\nFor the moment, let us assume $p, q \\in [1, \\infty)$. The functions $z \\mapsto z^p$ and $z \\mapsto z^q$ are monotonically increasing for $z \\ge 0$. Therefore, maximizing $\\|x\\|_p = (\\sum x_i^p)^{1/p}$ is equivalent to maximizing its $p$-th power, $F(x) = \\sum_{i=1}^n x_i^p$. The constraint becomes $G(x) = \\sum_{i=1}^n x_i^q - 1 = 0$.\n\nWe use the method of Lagrange multipliers to find the extremal points. The Lagrangian is:\n$$\n\\mathcal{L}(x, \\lambda) = F(x) - \\lambda G(x) = \\sum_{i=1}^n x_i^p - \\lambda \\left( \\sum_{i=1}^n x_i^q - 1 \\right)\n$$\nTo find critical points, we set the partial derivatives with respect to each $x_j$ (for which $x_j > 0$) to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_j} = p x_j^{p-1} - \\lambda q x_j^{q-1} = 0\n$$\nThis implies $p x_j^{p-1} = \\lambda q x_j^{q-1}$. For any non-zero component $x_j > 0$, we have:\n$$\nx_j^{p-q} = \\frac{\\lambda q}{p}\n$$\nIf $p \\ne q$, this equation dictates that any non-zero components of a candidate extremal vector must have the same value. Let this value be $a > 0$. If $p = q$, the equation becomes $p x_j^{p-1} = \\lambda p x_j^{p-1}$, implying $\\lambda=1$ but providing no constraint on the values of $x_j$, which is expected since if $p=q$, $\\|x\\|_p / \\|x\\|_q = 1$ for any non-zero vector $x$.\n\nThis observation establishes the structure of extremal vectors: they must have some number of non-zero components, say $k$ (where $1 \\le k \\le n$), all of which are equal in magnitude, and $n-k$ components which are zero. Let's denote such a vector (with positive components) as $x^{(k)}$. Without loss of generality, $x^{(k)}$ has its first $k$ components equal to $a > 0$ and the remaining $n-k$ components equal to $0$.\n\nWe determine the value of $a$ using the constraint $\\|x^{(k)}\\|_q=1$:\n$$\n\\|x^{(k)}\\|_q = \\left( \\sum_{i=1}^k a^q + \\sum_{i=k+1}^n 0^q \\right)^{1/q} = (k a^q)^{1/q} = k^{1/q} a = 1\n$$\nSolving for $a$, we get $a = k^{-1/q}$.\n\nNow, we evaluate the objective function, $\\|x^{(k)}\\|_p$, for this vector:\n$$\n\\|x^{(k)}\\|_p = \\left( \\sum_{i=1}^k a^p \\right)^{1/p} = (k a^p)^{1/p} = k^{1/p} a\n$$\nSubstituting the expression for $a$:\n$$\nC_k = \\|x^{(k)}\\|_p = k^{1/p} (k^{-1/q}) = k^{1/p - 1/q}\n$$\nThe optimal constant $C$ is the maximum value of $C_k$ over all possible numbers of non-zero components, $k \\in \\{1, 2, \\dots, n\\}$.\n$$\nC = \\max_{k \\in \\{1, \\dots, n\\}} k^{1/p - 1/q}\n$$\nThe behavior of the function $f(k)=k^\\alpha$ with $\\alpha = 1/p - 1/q$ depends on the sign of $\\alpha$.\n\nCase 1: $p < q$.\nIn this case, $1/p > 1/q$, so the exponent $\\alpha = 1/p - 1/q$ is positive. The function $f(k) = k^\\alpha$ is strictly increasing with $k$. Therefore, the maximum is achieved at the largest possible value of $k$, which is $k=n$.\nThe optimal constant is $C(n,p,q) = n^{1/p - 1/q}$.\nThe extremal vectors have all $n$ components non-zero and equal in magnitude, so $k^\\star = n$.\n\nCase 2: $p > q$.\nIn this case, $1/p < 1/q$, so the exponent $\\alpha = 1/p - 1/q$ is negative. The function $f(k) = k^\\alpha$ is strictly decreasing with $k$. The maximum is achieved at the smallest possible value of $k$, which is $k=1$.\nThe optimal constant is $C(n,p,q) = 1^{1/p - 1/q} = 1$.\nThe extremal vectors have only one non-zero component (e.g., a scaled standard basis vector), so $k^\\star = 1$.\n\nCase 3: $p = q$.\nIn this case, $\\alpha = 0$, so $C_k = k^0 = 1$ for all $k \\in \\{1, \\dots, n\\}$. The constant is $C(n,p,p)=1$. Any vector is an extremizer in the sense that the ratio is $1$. However, to provide a single value for $k^\\star$, we align with the $p \\ge q$ case for continuity. The formal argument using Jensen's inequality for $p \\ge q$ demonstrates that the supremum is $1$ and is attained for vectors with $k=1$ non-zero component. Thus, for $p \\ge q$, we consistently have $k^\\star=1$.\n\nWe now extend this to include the infinity norm, where $\\|x\\|_\\infty = \\max_i |x_i|$. This can be done by treating $\\infty$ as a limit, where $1/\\infty \\to 0$.\n\nCase 4: $p = \\infty$ (and $q < \\infty$).\nThis corresponds to the $p > q$ case. The formula gives an exponent of $1/\\infty - 1/q = -1/q < 0$. The maximum of $k^{-1/q}$ is at $k=1$, yielding $C=1^{ -1/q}=1$ and $k^\\star=1$. This is consistent. Directly, we want to maximize $\\|x\\|_\\infty$ given $\\|x\\|_q=1$. For any $x$, let $|x_j| = \\|x\\|_\\infty$. Then $\\|x\\|_q^q = \\sum_i |x_i|^q \\ge |x_j|^q = \\|x\\|_\\infty^q$. Thus, $\\|x\\|_q \\ge \\|x\\|_\\infty$, which means $\\|x\\|_\\infty/\\|x\\|_q \\le 1$. The maximum is $1$, achieved when all other components are zero.\n\nCase 5: $q = \\infty$ (and $p < \\infty$).\nThis corresponds to the $p < q$ case. The formula gives an exponent of $1/p - 1/\\infty = 1/p > 0$. The maximum of $k^{1/p}$ is at $k=n$, yielding $C = n^{1/p}$ and $k^\\star=n$. This is consistent. Directly, we want to maximize $\\|x\\|_p$ given $\\|x\\|_\\infty=1$. The constraint $\\|x\\|_\\infty=1$ implies $|x_i| \\le 1$ for all $i$. To maximize $\\left(\\sum |x_i|^p\\right)^{1/p}$, we should choose $|x_i|$ to be its maximum possible value, $|x_i|=1$, for all $i$. Such a vector satisfies the constraint and yields $\\|x\\|_p = (\\sum 1^p)^{1/p} = n^{1/p}$.\n\nSummary of theoretical results:\n- If $p < q$: $C(n,p,q) = n^{1/p - 1/q}$ and $k^\\star = n$. This includes the case $q=\\infty, p < \\infty$.\n- If $p \\ge q$: $C(n,p,q) = 1$ and $k^\\star = 1$. This includes the cases $p=q$ and $p=\\infty, q < \\infty$.\n\nThe algorithmic procedure will confirm this derivation by performing a direct search. For a given $(n,p,q)$, it will:\n1. Define the inverse exponents, with $1/\\infty=0$.\n2. Create an array of candidate values for the number of non-zero components, $k \\in \\{1, 2, \\dots, n\\}$.\n3. Compute the ratio $C_k = k^{1/p - 1/q}$ for each $k$.\n4. The estimated constant $C_{\\mathrm{est}}$ is the maximum value in this array of ratios.\n5. The number of non-zeros in the extremal vector, $k^\\star$, is the value of $k$ that produced this maximum. We take the first such $k$ in case of ties, which `numpy.argmax` does by default. This correctly resolves ties for $p \\ge q$ to $k=1$.\n6. The theoretical constant $C_{\\mathrm{theory}}$ and $k^\\star_{\\mathrm{theory}}$ are computed from the summarized formulas.\n7. The discrepancy is calculated as $\\mathrm{err} = |C_{\\mathrm{theory}} - C_{\\mathrm{est}}|$. Due to the direct correspondence between the derivation and the search, this error will be zero.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal constant C in the norm equivalence inequality ||x||_p <= C ||x||_q\n    for given n, p, and q, and identifies the structure of an extremal vector.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Format: (n, p, q), where p or q can be np.inf\n    test_cases = [\n        (7, 1.5, 3.2),\n        (10, 4.0, 2.0),\n        (8, np.inf, 2.0),\n        (5, 1.0, np.inf),\n        (6, 2.0, 2.0),\n    ]\n\n    results = []\n    for n, p, q in test_cases:\n        # 1. Theoretical Calculation\n        # The formulas are derived from first principles in the solution text.\n        # The derivation relies on analyzing the function f(k) = k^(1/p - 1/q) for k in {1, ..., n}.\n        \n        # Handle infinite values for p and q, where 1/inf = 0.\n        inv_p = 1.0 / p if p != np.inf else 0.0\n        inv_q = 1.0 / q if q != np.inf else 0.0\n\n        if p < q:\n            # Exponent (1/p - 1/q) is positive, so k^exponent is maximized at k=n.\n            C_theory = n**(inv_p - inv_q)\n            k_star_theory = n\n        else: # p >= q\n            # Exponent (1/p - 1/q) is non-positive, so k^exponent is maximized at k=1.\n            C_theory = 1.0\n            k_star_theory = 1\n\n        # 2. Estimation via Search over Candidate Structures\n        # The derivation showed that extremal vectors have k identical non-zero components.\n        # This search programmatically confirms the derivation by checking all possible k.\n        \n        # Candidate values for the number of non-zero entries\n        k_values = np.arange(1, n + 1)\n        \n        # Exponent for the ratio calculation\n        exponent = inv_p - inv_q\n        \n        # Calculate the ratio C_k = k^exponent for each k\n        ratios = k_values.astype(float) ** exponent\n        \n        # The estimated constant is the maximum of these ratios\n        C_est = np.max(ratios)\n        \n        # k_star is the number of non-zero components (k) that yields the maximum ratio.\n        # np.argmax returns the index of the first occurrence of the maximum value.\n        # For p >= q, exponent <= 0, ratios are non-increasing, so argmax is 0, k=1.\n        # For p < q, exponent > 0, ratios are increasing, so argmax is n-1, k=n.\n        k_star_est = k_values[np.argmax(ratios)]\n        \n        # 3. Discrepancy Calculation\n        err = np.abs(C_theory - C_est)\n        \n        # Ensure k_star is an integer for the output format\n        k_star_output = int(k_star_est)\n        \n        # Append the list of results for this test case\n        results.append([C_theory, C_est, err, k_star_output])\n\n    # 4. Final Output Formatting\n    # The output must be a single line, with no spaces, containing a list of lists.\n    # Example: [[r1_1,r1_2],[r2_1,r2_2]]\n    # We build this string representation piece by piece.\n    result_strings = []\n    for res_list in results:\n        # Format each sublist like [val1,val2,val3,val4]\n        sublist_str = f'[{\",\".join(map(str, res_list))}]'\n        result_strings.append(sublist_str)\n    \n    # Join all sublist strings into the final format.\n    final_output = f'[{\",\".join(result_strings)}]'\n    \n    # Print the final, exactly formatted string.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the analysis of vector norms, we now extend the concept of equivalence to the realm of matrices and their induced operator norms. While matrix norms are also equivalent in finite dimensions, this practice reveals a critical nuance: the equivalence constants often depend on the dimension $n$, and this dependency can have profound consequences. By constructing a specific matrix and explicitly calculating its induced $1$-norm, $2$-norm, and $\\infty$-norm, you will see how these measures can diverge dramatically as the dimension grows, offering a crucial lesson on why the choice of norm is so important when analyzing the stability and error propagation of numerical algorithms .",
            "id": "3544612",
            "problem": "Let $n \\geq 2$ be an integer. Consider the induced operator norms on $\\mathbb{R}^{n \\times n}$ associated with the vector $p$-norms on $\\mathbb{R}^{n}$ defined by $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$, $\\|x\\|_{2} = \\left( \\sum_{i=1}^{n} |x_{i}|^{2} \\right)^{1/2}$, and $\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |x_{i}|$, together with the definition of the induced operator norm $\\|A\\|_{p \\to p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}}$. Starting from these definitions and the fundamental equivalence of vector norms on $\\mathbb{R}^{n}$, derive a dimension-dependent upper bound on $\\|A\\|_{2 \\to 2}$ in terms of $\\|A\\|_{1 \\to 1}$ and $\\|A\\|_{\\infty \\to \\infty}$ that holds for every $A \\in \\mathbb{R}^{n \\times n}$. Then, design a specific matrix $A \\in \\mathbb{R}^{n \\times n}$ whose column/row structure makes one of the induced norms small and the others large by taking\n$$\nA = u e_{1}^{\\top}, \\quad \\text{where } u = (1,1,\\dots,1)^{\\top} \\in \\mathbb{R}^{n} \\text{ and } e_{1} = (1,0,\\dots,0)^{\\top} \\in \\mathbb{R}^{n}.\n$$\nUsing only the above definitions and first principles, compute the three quantities $\\|A\\|_{1 \\to 1}$, $\\|A\\|_{\\infty \\to \\infty}$, and $\\|A\\|_{2 \\to 2}$ exactly, and then form the ratio vector\n$$\nr(n) = \\left( \\frac{\\|A\\|_{1 \\to 1}}{\\|A\\|_{2 \\to 2}}, \\; \\frac{\\|A\\|_{\\infty \\to \\infty}}{\\|A\\|_{2 \\to 2}}, \\; \\frac{\\sqrt{\\|A\\|_{1 \\to 1} \\, \\|A\\|_{\\infty \\to \\infty}}}{\\|A\\|_{2 \\to 2}} \\right).\n$$\nExplain how this construction highlights algorithm-dependent sensitivities that arise when different methods control error or stability in terms of $\\| \\cdot \\|_{1 \\to 1}$, $\\| \\cdot \\|_{\\infty \\to \\infty}$, or $\\| \\cdot \\|_{2 \\to 2}$ for matrices with highly nonuniform column/row structure. Provide your final answer as the closed-form expression of $r(n)$; no rounding is required and no units are needed.",
            "solution": "The problem as stated is mathematically sound, well-posed, and internally consistent. It is a standard exercise in numerical linear algebra concerning the properties of matrix norms. We may therefore proceed with a formal solution.\n\nThe problem is divided into two main parts. First, we derive a general upper bound for the matrix $2$-norm in terms of the $1$-norm and $\\infty$-norm. Second, we analyze a specific matrix to illustrate the dimension-dependence of norm equivalence constants for matrix norms.\n\nPart 1: Derivation of the Upper Bound\n\nWe are asked to derive an upper bound on $\\|A\\|_{2 \\to 2}$ in terms of $\\|A\\|_{1 \\to 1}$ and $\\|A\\|_{\\infty \\to \\infty}$ for a matrix $A \\in \\mathbb{R}^{n \\times n}$, starting from the definitions of induced norms and the equivalence of vector norms.\n\nThe induced operator norm is defined as $\\|A\\|_{p \\to p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}}$. The fundamental vector norm equivalences in $\\mathbb{R}^n$ that we will use are:\n1. $\\|v\\|_{\\infty} \\leq \\|v\\|_{2}$ for any $v \\in \\mathbb{R}^n$.\n2. $\\|v\\|_{2} \\leq \\sqrt{n} \\|v\\|_{\\infty}$ for any $v \\in \\mathbb{R}^n$.\n3. $\\|v\\|_{1} \\leq \\sqrt{n} \\|v\\|_{2}$ for any $v \\in \\mathbb{R}^n$.\n\nLet's start the derivation for an arbitrary $A \\in \\mathbb{R}^{n \\times n}$.\nBy definition, $\\|A\\|_{2 \\to 2} = \\sup_{\\|x\\|_{2}=1} \\|Ax\\|_{2}$.\nWe can chain inequalities using vector norm equivalences and the definitions of induced norms.\nUsing equivalence 2, we have $\\|Ax\\|_{2} \\leq \\sqrt{n} \\|Ax\\|_{\\infty}$.\nBy the definition of the induced $\\infty$-norm, $\\|Ax\\|_{\\infty} \\leq \\|A\\|_{\\infty \\to \\infty} \\|x\\|_{\\infty}$.\nCombining these, we get $\\|Ax\\|_{2} \\leq \\sqrt{n} \\|A\\|_{\\infty \\to \\infty} \\|x\\|_{\\infty}$.\nNow, using equivalence 1, we have $\\|x\\|_{\\infty} \\leq \\|x\\|_{2}$. For a vector $x$ on the unit sphere, $\\|x\\|_{2}=1$, so $\\|x\\|_{\\infty} \\leq 1$.\nSubstituting this into our inequality yields $\\|Ax\\|_{2} \\leq \\sqrt{n} \\|A\\|_{\\infty \\to \\infty} \\cdot 1$.\nSince this holds for any $x$ with $\\|x\\|_{2}=1$, it must hold for the one that maximizes $\\|Ax\\|_{2}$. Thus, we have our first bound:\n$$\n\\|A\\|_{2 \\to 2} = \\sup_{\\|x\\|_{2}=1} \\|Ax\\|_{2} \\leq \\sqrt{n} \\|A\\|_{\\infty \\to \\infty}.\n$$\nTo involve the $1$-norm, we can use a similar argument or leverage properties of the transpose. It is a standard result that $\\|A^T\\|_{2 \\to 2} = \\|A\\|_{2 \\to 2}$, $\\|A^T\\|_{1 \\to 1} = \\|A\\|_{\\infty \\to \\infty}$, and $\\|A^T\\|_{\\infty \\to \\infty} = \\|A\\|_{1 \\to 1}$. Applying the inequality we just derived to the matrix $A^T$:\n$$\n\\|A^T\\|_{2 \\to 2} \\leq \\sqrt{n} \\|A^T\\|_{\\infty \\to \\infty}.\n$$\nSubstituting the identities, we obtain a second bound:\n$$\n\\|A\\|_{2 \\to 2} \\leq \\sqrt{n} \\|A\\|_{1 \\to 1}.\n$$\nWe now have two upper bounds on $\\|A\\|_{2 \\to 2}$. The problem asks for a single bound in terms of both $\\|A\\|_{1 \\to 1}$ and $\\|A\\|_{\\infty \\to \\infty}$. We can combine our two results. For instance, we can state that $\\|A\\|_{2 \\to 2}$ is less than or equal to the geometric mean of the two bounds:\n$$\n\\|A\\|_{2 \\to 2} \\leq \\sqrt{(\\sqrt{n} \\|A\\|_{1 \\to 1}) (\\sqrt{n} \\|A\\|_{\\infty \\to \\infty})} = \\sqrt{n \\cdot \\|A\\|_{1 \\to 1} \\|A\\|_{\\infty \\to \\infty}}.\n$$\nThis is a valid dimension-dependent upper bound derived from first principles as requested. Note that a sharper bound, $\\|A\\|_{2 \\to 2} \\leq \\sqrt{\\|A\\|_{1 \\to 1} \\|A\\|_{\\infty \\to \\infty}}$, also exists, but its derivation is more involved than what is suggested by the problem's framing.\n\nPart 2: Analysis of the Specific Matrix\n\nWe are given the matrix $A = u e_{1}^{\\top}$, where $u = (1,1,\\dots,1)^{\\top} \\in \\mathbb{R}^{n}$ and $e_{1} = (1,0,\\dots,0)^{\\top} \\in \\mathbb{R}^{n}$.\nThe matrix $A$ is the outer product of these two vectors:\n$$\nA = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & \\dots & 0 \\end{pmatrix} = \\begin{pmatrix}\n1 & 0 & \\dots & 0 \\\\\n1 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & 0 & \\dots & 0\n\\end{pmatrix}.\n$$\nThis is a rank-$1$ matrix where the first column consists of all ones and all other columns are zero vectors.\n\nWe now compute the three specified induced norms for this matrix $A$.\n\n1.  $\\|A\\|_{1 \\to 1}$: This norm is defined as the maximum absolute column sum. Let $A_j$ be the $j$-th column of $A$.\n    For the first column ($j=1$): $\\|A_1\\|_1 = \\sum_{i=1}^{n} |a_{i1}| = \\sum_{i=1}^{n} |1| = n$.\n    For any other column ($j > 1$): $\\|A_j\\|_1 = \\sum_{i=1}^{n} |a_{ij}| = \\sum_{i=1}^{n} |0| = 0$.\n    The maximum of these values is $n$. Therefore,\n    $$\n    \\|A\\|_{1 \\to 1} = n.\n    $$\n\n2.  $\\|A\\|_{\\infty \\to \\infty}$: This norm is defined as the maximum absolute row sum. Let $A_{i,:}$ be the $i$-th row of $A$.\n    For any row $i$: $\\|A_{i,:}\\|_1 = \\sum_{j=1}^{n} |a_{ij}| = |1| + |0| + \\dots + |0| = 1$.\n    Since this is true for all rows, the maximum value is $1$. Therefore,\n    $$\n    \\|A\\|_{\\infty \\to \\infty} = 1.\n    $$\n\n3.  $\\|A\\|_{2 \\to 2}$: This norm is the largest singular value of $A$, which is $\\sigma_1(A) = \\sqrt{\\lambda_{\\max}(A^T A)}$. Let's compute $A^T A$.\n    $A^T = (u e_1^T)^T = e_1 u^T$.\n    $A^T A = (e_1 u^T) (u e_1^T) = e_1 (u^T u) e_1^T$.\n    The inner product $u^T u$ is $\\sum_{i=1}^n 1^2 = n$.\n    So, $A^T A = n (e_1 e_1^T)$. The matrix $e_1 e_1^T$ is an $n \\times n$ matrix with a $1$ at the $(1,1)$ position and zeros everywhere else.\n    $$\n    A^T A = n \\begin{pmatrix}\n    1 & 0 & \\dots & 0 \\\\\n    0 & 0 & \\dots & 0 \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    0 & 0 & \\dots & 0\n    \\end{pmatrix} = \\begin{pmatrix}\n    n & 0 & \\dots & 0 \\\\\n    0 & 0 & \\dots & 0 \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    0 & 0 & \\dots & 0\n    \\end{pmatrix}.\n    $$\n    The eigenvalues of this diagonal matrix are its diagonal entries: $n$ and $0$ (with multiplicity $n-1$). The maximum eigenvalue is $\\lambda_{\\max}(A^T A) = n$.\n    The $2$-norm is the square root of this value:\n    $$\n    \\|A\\|_{2 \\to 2} = \\sqrt{n}.\n    $$\n\nPart 3: The Ratio Vector and Interpretation\n\nWe are asked to compute the ratio vector $r(n) = \\left( \\frac{\\|A\\|_{1 \\to 1}}{\\|A\\|_{2 \\to 2}}, \\; \\frac{\\|A\\|_{\\infty \\to \\infty}}{\\|A\\|_{2 \\to 2}}, \\; \\frac{\\sqrt{\\|A\\|_{1 \\to 1} \\, \\|A\\|_{\\infty \\to \\infty}}}{\\|A\\|_{2 \\to 2}} \\right)$.\nSubstituting the values we computed:\n-   First component: $\\frac{\\|A\\|_{1 \\to 1}}{\\|A\\|_{2 \\to 2}} = \\frac{n}{\\sqrt{n}} = \\sqrt{n}$.\n-   Second component: $\\frac{\\|A\\|_{\\infty \\to \\infty}}{\\|A\\|_{2 \\to 2}} = \\frac{1}{\\sqrt{n}}$.\n-   Third component: $\\frac{\\sqrt{\\|A\\|_{1 \\to 1} \\|A\\|_{\\infty \\to \\infty}}}{\\|A\\|_{2 \\to 2}} = \\frac{\\sqrt{n \\cdot 1}}{\\sqrt{n}} = \\frac{\\sqrt{n}}{\\sqrt{n}} = 1$.\n\nSo, the ratio vector is:\n$$\nr(n) = \\left( \\sqrt{n}, \\; \\frac{1}{\\sqrt{n}}, \\; 1 \\right).\n$$\n\nThe behavior of these ratios as $n$ grows highlights the disparity between the different matrix norms for this particular matrix. For large $n$, $\\|A\\|_{1 \\to 1} = n$ is much larger than $\\|A\\|_{2 \\to 2} = \\sqrt{n}$, which in turn is much larger than $\\|A\\|_{\\infty \\to \\infty} = 1$. This occurs because all \"weight\" of the matrix $A$ is concentrated in a single column. The $1$-norm, which is sensitive to column sums, registers a large value. The $\\infty$-norm, sensitive to row sums, registers a small value. The $2$-norm provides an intermediate measure.\n\nThis has significant implications for the analysis of numerical algorithms. Many error bounds and stability analyses depend on a specific choice of matrix norm.\n- An analysis using the $1$-norm would treat matrix $A$ as a \"large\" object whose norm grows linearly with dimension $n$. This might lead to pessimistic or loose bounds on error propagation or convergence rates for algorithms applied to matrices with this structure.\n- Conversely, an analysis using the $\\infty$-norm would see $A$ as a \"small,\" well-behaved object, with a norm of $1$ irrespective of dimension. This could result in overly optimistic theoretical guarantees.\n- The $2$-norm provides a geometric mean behavior, scaling with $\\sqrt{n}$. The third component of $r(n)$ being $1$ demonstrates that for this specific matrix $A$, the sharp inequality $\\|A\\|_{2 \\to 2} \\le \\sqrt{\\|A\\|_{1 \\to 1} \\|A\\|_{\\infty \\to \\infty}}$ holds with equality.\n\nThis example starkly illustrates that while all norms on finite-dimensional spaces are equivalent, the equivalence constants for matrix norms depend on the dimension $n$. For a matrix with highly nonuniform structure, the choice of norm is not a trivial detail; it can fundamentally change the conclusions drawn from a theoretical analysis of an algorithm's performance.",
            "answer": "$$\n\\boxed{(\\sqrt{n}, \\frac{1}{\\sqrt{n}}, 1)}\n$$"
        },
        {
            "introduction": "Our final practice takes an experimental approach, placing you in the role of a computational scientist interacting with a black-box system. Instead of being given a norm's formula, you are provided with an oracle that computes its value, and your task is to deduce its fundamental properties. This exercise  requires you to synthesize several key theoretical ideas—including the parallelogram law and the polarization identity—to design a procedure that first identifies the norm's class and then estimates its equivalence constants relative to the Euclidean norm. The practice culminates in a highly practical application: using your estimated constants to recalibrate a solver's stopping criteria to meet a desired target precision.",
            "id": "3544597",
            "problem": "You are given query access to a black-box residual reporter that, for any input vector in a finite-dimensional real vector space, returns the residual magnitude in an unknown norm. Formally, for a given dimension $n \\in \\mathbb{N}$, you can query an oracle that, for any $x \\in \\mathbb{R}^n$, returns a scalar $\\|x\\|_{?} \\in \\mathbb{R}_{\\ge 0}$. The unknown norm $\\|\\cdot\\|_{?}$ is guaranteed to be equivalent to the Euclidean norm $\\|\\cdot\\|_{2}$ on $\\mathbb{R}^n$, meaning that there exist constants $c_1, c_2 \\in \\mathbb{R}$ with $0 < c_1 \\le c_2 < \\infty$ such that, for all $x \\in \\mathbb{R}^n$, the inequality $c_1 \\|x\\|_2 \\le \\|x\\|_{?} \\le c_2 \\|x\\|_2$ holds. The task is to design a detection experiment and implement it as a program that, using only queries to $\\|\\cdot\\|_{?}$, estimates $c_1$ and $c_2$, and uses these estimates to recalibrate a stopping threshold so that a prescribed Euclidean residual target is met.\n\nYour design and implementation must be derived from the following fundamental base:\n- The definition of a norm, the Euclidean norm $\\|\\cdot\\|_2$, and norm equivalence on finite-dimensional spaces.\n- The characterization of norms induced by an inner product via the parallelogram law: a norm $\\|\\cdot\\|$ is induced by some inner product if and only if, for all $x,y \\in \\mathbb{R}^n$, the identity $\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2$ holds.\n- The polarization identity in real inner product spaces: for an inner-product-induced norm $\\|\\cdot\\|$, the associated inner product $\\langle x,y\\rangle$ satisfies $\\langle x,y\\rangle = \\tfrac{1}{4}(\\|x+y\\|^2 - \\|x-y\\|^2)$ for all $x,y \\in \\mathbb{R}^n$.\n- The spectral characterization of equivalence constants when the unknown norm is induced by a Symmetric Positive Definite (SPD) matrix $M \\in \\mathbb{R}^{n \\times n}$ via $\\|x\\|_{M} = \\sqrt{x^\\top M x}$: if $\\lambda_{\\min}(M)$ and $\\lambda_{\\max}(M)$ denote the smallest and largest eigenvalues of $M$, then, for all $x \\in \\mathbb{R}^n$, the inequality $\\sqrt{\\lambda_{\\min}(M)} \\|x\\|_2 \\le \\|x\\|_M \\le \\sqrt{\\lambda_{\\max}(M)} \\|x\\|_2$ holds.\n- The fundamental inequalities relating $\\ell_p$ norms and the Euclidean norm on $\\mathbb{R}^n$: for the $\\ell_1$ norm, $\\|x\\|_2 \\le \\|x\\|_1 \\le \\sqrt{n} \\|x\\|_2$ for all $x \\in \\mathbb{R}^n$; for the $\\ell_\\infty$ norm, $n^{-1/2}\\|x\\|_2 \\le \\|x\\|_\\infty \\le \\|x\\|_2$ for all $x \\in \\mathbb{R}^n$.\n\nYour program must implement the following detection experiment:\n- Given query access to $\\| \\cdot \\|_{?}$ for a fixed dimension $n$, first test whether $\\|\\cdot\\|_{?}$ is induced by an inner product by checking the parallelogram law on a set of randomly sampled pairs $(x,y)$. If this test indicates an inner-product-induced norm (within a numerical tolerance that you must justify), reconstruct the associated Gram matrix $G \\in \\mathbb{R}^{n \\times n}$ using the polarization identity restricted to the standard basis vectors. Then estimate $c_1$ and $c_2$ as $\\sqrt{\\lambda_{\\min}(G)}$ and $\\sqrt{\\lambda_{\\max}(G)}$ respectively, where $\\lambda_{\\min}(G)$ and $\\lambda_{\\max}(G)$ are the smallest and largest eigenvalues of $G$.\n- If the parallelogram law test fails (indicating a general norm not necessarily induced by an inner product), estimate $c_1$ and $c_2$ by sampling unit Euclidean vectors on the sphere $\\{x \\in \\mathbb{R}^n : \\|x\\|_2 = 1\\}$ and computing the minimum and maximum of the ratio $\\|x\\|_{?}/\\|x\\|_2$ over this sample. Your sampling set must include a deterministic covering of candidate directions that capture extremizers for classical norms, including all coordinate axes and their negatives, as well as the uniform direction $u = \\tfrac{1}{\\sqrt{n}}(1,\\dots,1)^\\top$, along with a moderate number of random directions. Justify why this set detects the exact constants for the $\\ell_1$ and $\\ell_\\infty$ norms and provides upper and lower bounds in the general case.\n\nRecalibration rule:\n- Suppose a black-box solver stops when $\\|r\\|_{?} \\le \\tau$ for a residual vector $r \\in \\mathbb{R}^n$. By norm equivalence, a safe sufficient condition for $\\|r\\|_2 \\le \\varepsilon_2$ is $\\tau \\le c_1 \\varepsilon_2$. Therefore, given a target Euclidean residual tolerance $\\varepsilon_2 > 0$, your program must compute $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$ using your estimate $\\widehat{c}_1$.\n\nTest suite:\nImplement your program to run on the following test cases. Each case defines the dimension $n$, the unknown norm $\\|\\cdot\\|_{?}$ via an oracle, and a target Euclidean tolerance $\\varepsilon_2$. Your program must, for each case, estimate $\\widehat{c}_1$ and $\\widehat{c}_2$, compute $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$, and also report the true constants $c_1$ and $c_2$ for verification.\n\n- Case A: $n = 3$. Unknown norm $\\|x\\|_{?} = \\sqrt{x^\\top M x}$ with $M = \\mathrm{diag}(1, 4, 9)$. Target $\\varepsilon_2 = 10^{-4}$.\n- Case B: $n = 4$. Unknown norm $\\|x\\|_{?} = \\|x\\|_1$. Target $\\varepsilon_2 = 10^{-3}$.\n- Case C: $n = 5$. Unknown norm $\\|x\\|_{?} = \\|x\\|_\\infty$. Target $\\varepsilon_2 = 2 \\times 10^{-6}$.\n- Case D: $n = 3$. Unknown norm $\\|x\\|_{?} = \\sqrt{x^\\top M x}$ with $M = \\mathrm{diag}(10^{-4}, 1, 100)$. Target $\\varepsilon_2 = 10^{-5}$.\n- Case E: $n = 1$. Unknown norm $\\|x\\|_{?} = \\alpha \\|x\\|_2$ with $\\alpha = 7$. Target $\\varepsilon_2 = 3 \\times 10^{-5}$.\n\nAnswer specification:\n- For each case, your program must output a list of five floating-point numbers $[\\widehat{c}_1, \\widehat{c}_2, \\widehat{\\tau}, c_1, c_2]$ in this order.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list for one case, in the same order as the cases are presented above. For example, the output should look like $[[a_1,a_2,a_3,a_4,a_5],[b_1,b_2,b_3,b_4,b_5],\\dots]$.\n- All computations are dimensionless and no physical units are involved.\n\nYour implementation must be self-contained and must not use any external input. The detection tolerance, sampling size, and any randomized components must be justified in your solution narrative and chosen to ensure that, for the specified test suite, the estimates match the true constants exactly in all cases.",
            "solution": "The problem requires the design and implementation of an experiment to estimate the equivalence constants, $c_1$ and $c_2$, between an unknown norm $\\|\\cdot\\|_{?}$ and the standard Euclidean norm $\\|\\cdot\\|_2$ on $\\mathbb{R}^n$. These constants are defined by the inequality $c_1 \\|x\\|_2 \\le \\|x\\|_{?} \\le c_2 \\|x\\|_2$ for all $x \\in \\mathbb{R}^n$. The estimation is to be performed using only query access to an oracle for $\\|\\cdot\\|_{?}$. The estimates are then used to recalibrate a solver's stopping threshold $\\tau$ to meet a prescribed Euclidean tolerance $\\varepsilon_2$.\n\nThe design is rooted in a fundamental property of norms on vector spaces: a norm is induced by an inner product if and only if it satisfies the parallelogram law. This provides a clear criterion to distinguish between two classes of norms, each admitting a different and highly effective estimation strategy.\n\nThe overall algorithm proceeds in three stages:\n1.  Test whether the unknown norm $\\|\\cdot\\|_{?}$ is induced by an inner product by verifying the parallelogram law for a sample of random vectors.\n2.  Based on the outcome of the test, apply a specialized procedure to estimate $c_1$ and $c_2$.\n    a. If it is an inner-product-induced norm, reconstruct the associated Gram matrix and compute the constants from its eigenvalues.\n    b. If it is a general norm, estimate the constants by sampling the unit sphere at a set of strategically chosen directions.\n3.  Use the estimate $\\widehat{c}_1$ to compute the recalibrated stopping threshold $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$.\n\nWe now detail each stage of this design.\n\n**Stage 1: The Parallelogram Law Test**\n\nA norm $\\|\\cdot\\|$ on a real vector space is induced by an inner product if and only if it satisfies the parallelogram law for all vectors $x, y$:\n$$\n\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2\n$$\nDue to floating-point arithmetic, an exact equality check is not feasible. We instead test this identity probabilistically for a set of randomly generated vector pairs $(x,y)$ and check if the relative error is within a small numerical tolerance $\\delta_{PL}$. For each pair, we compute the error:\n$$\nE(x,y) = \\frac{| (\\|x+y\\|_{?}^2 + \\|x-y\\|_{?}^2) - (2\\|x\\|_{?}^2 + 2\\|y\\|_{?}^2) |}{2\\|x\\|_{?}^2 + 2\\|y\\|_{?}^2}\n$$\nIf the maximum error over a sufficient number of samples (e.g., $100$) is less than $\\delta_{PL}$, we classify the norm as being induced by an inner product. A tolerance of $\\delta_{PL} = 10^{-9}$ is appropriate, as it is small enough to account for standard double-precision floating-point inaccuracies but large enough to detect structural violations from norms not derived from an inner product. For example, the $\\ell_1$ and $\\ell_\\infty$ norms exhibit large, order-one violations of this law for simple vectors like the standard basis vectors, which are easily detected by random sampling.\n\n**Stage 2a: Estimation for Inner-Product-Induced Norms**\n\nIf the parallelogram law test passes, we proceed under the assumption that $\\|\\cdot\\|_{?}$ is induced by an inner product $\\langle \\cdot, \\cdot \\rangle_?$, such that $\\|x\\|_{?}^2 = \\langle x, x \\rangle_?$. This inner product can be represented by a symmetric positive-definite (SPD) matrix $G \\in \\mathbb{R}^{n \\times n}$, called the Gram matrix, with respect to the standard basis $\\{e_i\\}_{i=1}^n$. We have $\\|x\\|_{?}^2 = x^\\top G x$. The entries of this matrix are given by $G_{ij} = \\langle e_i, e_j \\rangle_?$.\n\nTo reconstruct $G$, we utilize the polarization identity, which expresses the inner product in terms of the norm:\n$$\n\\langle u, v \\rangle_? = \\frac{1}{4} \\left( \\|u+v\\|_{?}^2 - \\|u-v\\|_{?}^2 \\right)\n$$\nBy substituting $u=e_i$ and $v=e_j$ for all $i,j \\in \\{1,\\dots,n\\}$, we can compute every entry of $G$ using queries to the oracle.\n$$\nG_{ij} = \\frac{1}{4} \\left( \\|e_i+e_j\\|_{?}^2 - \\|e_i-e_j\\|_{?}^2 \\right)\n$$\nOnce the matrix $\\widehat{G}$ is numerically constructed, the equivalence constants are given by the square roots of its extremal eigenvalues. This is a direct consequence of the Rayleigh-Ritz theorem, which states that for any non-zero $x \\in \\mathbb{R}^n$:\n$$\n\\lambda_{\\min}(G) \\le \\frac{x^\\top G x}{x^\\top x} \\le \\lambda_{\\max}(G)\n$$\nSubstituting $\\|x\\|_{?}^2 = x^\\top G x$ and $\\|x\\|_2^2 = x^\\top x$, we get $\\lambda_{\\min}(G) \\|x\\|_2^2 \\le \\|x\\|_{?}^2 \\le \\lambda_{\\max}(G) \\|x\\|_2^2$. Taking the square root gives the equivalence inequality with $c_1 = \\sqrt{\\lambda_{\\min}(G)}$ and $c_2 = \\sqrt{\\lambda_{\\max}(G)}$. Our estimates are therefore $\\widehat{c}_1 = \\sqrt{\\lambda_{\\min}(\\widehat{G})}$ and $\\widehat{c}_2 = \\sqrt{\\lambda_{\\max}(\\widehat{G})}$.\n\n**Stage 2b: Estimation for General Norms**\n\nIf the parallelogram law test fails, we must use a different approach. The constants $c_1$ and $c_2$ are formally defined as the minimum and maximum of the function $f(x) = \\|x\\|_{?}$ over the unit sphere $S^{n-1} = \\{x \\in \\mathbb{R}^n : \\|x\\|_2 = 1\\}$:\n$$\nc_1 = \\min_{x \\in S^{n-1}} \\|x\\|_{?} \\quad , \\quad c_2 = \\max_{x \\in S^{n-1}} \\|x\\|_{?}\n$$\nSince searching the entire continuous sphere is impossible, we approximate this by finding the minimum and maximum over a finite, well-chosen subset of sample vectors. Per the problem's requirements, our sampling set will include:\n1.  A deterministic set of directions known to be extremizers for common norms: $\\{ \\pm e_i \\}_{i=1}^n$ (coordinate axes) and $\\pm \\frac{1}{\\sqrt{n}}(1, \\dots, 1)^\\top$ (uniform directions).\n2.  A set of randomly generated directions, normalized to unit Euclidean length, to improve the estimate for arbitrary norms not covered by the deterministic set.\n\nThis choice of a deterministic set is critical. For the $\\ell_1$-norm, a standard result is that $\\|x\\|_2 \\le \\|x\\|_1 \\le \\sqrt{n}\\|x\\|_2$. The lower bound is achieved for $x=e_i$, giving a ratio of $\\|e_i\\|_1/\\|e_i\\|_2 = 1/1=1$. The upper bound is achieved for $x = \\frac{1}{\\sqrt{n}}(1,\\dots,1)^\\top$, giving a ratio of $\\|x\\|_1/\\|x\\|_2 = \\sqrt{n}/1 = \\sqrt{n}$. Thus, our deterministic set is guaranteed to find the exact constants $c_1=1$ and $c_2=\\sqrt{n}$ for the $\\ell_1$-norm.\n\nFor the $\\ell_\\infty$-norm, the standard result is $n^{-1/2}\\|x\\|_2 \\le \\|x\\|_\\infty \\le \\|x\\|_2$. The lower bound is achieved for $x = \\frac{1}{\\sqrt{n}}(1,\\dots,1)^\\top$, giving ratio $n^{-1/2}$. The upper bound is achieved for $x=e_i$, giving ratio $1$. Again, our deterministic set finds the exact constants $c_1=n^{-1/2}$ and $c_2=1$.\n\nFor the test cases provided, this sampling strategy is sufficient to find the exact constants. The estimates are $\\widehat{c}_1 = \\min_{x \\in S} \\|x\\|_{?}$ and $\\widehat{c}_2 = \\max_{x \\in S} \\|x\\|_{?}$ where $S$ is our combined sample set of unit vectors.\n\n**Stage 3: Threshold Recalibration**\n\nThe final step is to compute a new stopping threshold $\\widehat{\\tau}$ for a black-box solver. We are given the goal of ensuring the final residual vector $r$ satisfies $\\|r\\|_2 \\le \\varepsilon_2$. The norm equivalence inequality $\\|r\\|_{?} \\ge c_1 \\|r\\|_2$ implies $\\|r\\|_2 \\le \\frac{1}{c_1} \\|r\\|_{?}$. To satisfy $\\|r\\|_2 \\le \\varepsilon_2$, it is sufficient to enforce $\\frac{1}{c_1} \\|r\\|_{?} \\le \\varepsilon_2$, which is equivalent to $\\|r\\|_{?} \\le c_1 \\varepsilon_2$. The solver stops when $\\|r\\|_{?} \\le \\tau$, so choosing $\\tau \\le c_1 \\varepsilon_2$ provides a safe stopping condition.\n\nThe problem specifies computing the recalibrated threshold as $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$. This is a valid and safe procedure provided that our estimate $\\widehat{c}_1$ is equal to or a lower bound on the true constant $c_1$. For the inner-product norm case, our reconstruction technique is exact up to floating-point precision, so $\\widehat{c}_1 \\approx c_1$. For the general norm cases in the test suite, our deterministic sampling strategy ensures $\\widehat{c}_1 = c_1$. In a more general setting where sampling might only yield an upper bound $\\widehat{c}_1 \\ge c_1$, the rule $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$ would not be guaranteed to be safe. However, for the scope and requirements of this problem, the procedure is sound.",
            "answer": "```python\nimport numpy as np\n\ndef test_parallelogram_law(oracle, n, num_samples=100, tolerance=1e-9):\n    \"\"\"\n    Tests if a norm satisfies the parallelogram law within a given tolerance.\n\n    A norm ||.|| is induced by an inner product if and only if\n    ||x+y||^2 + ||x-y||^2 = 2(||x||^2 + ||y||^2) for all x, y.\n\n    Args:\n        oracle (callable): The black-box norm function.\n        n (int): The dimension of the vector space.\n        num_samples (int): The number of random vector pairs to test.\n        tolerance (float): The relative error tolerance for the check.\n\n    Returns:\n        bool: True if the law holds for all samples, False otherwise.\n    \"\"\"\n    # Fix the random seed for reproducibility\n    rng = np.random.default_rng(seed=42)\n    max_relative_error = 0.0\n\n    for _ in range(num_samples):\n        x = rng.standard_normal(n)\n        y = rng.standard_normal(n)\n\n        # Handle the case where x and y are zero vectors\n        norm_x_sq = oracle(x)**2\n        norm_y_sq = oracle(y)**2\n        if norm_x_sq == 0 and norm_y_sq == 0:\n            continue\n\n        lhs = oracle(x + y)**2 + oracle(x - y)**2\n        rhs = 2 * (norm_x_sq + norm_y_sq)\n\n        if rhs == 0: # This case should not be hit if x or y is non-zero\n            relative_error = 0.0 if lhs == 0 else np.inf\n        else:\n            relative_error = np.abs(lhs - rhs) / rhs\n        \n        if relative_error > max_relative_error:\n            max_relative_error = relative_error\n\n    return max_relative_error  tolerance\n\ndef estimate_from_gram_matrix(oracle, n):\n    \"\"\"\n    Estimates c1 and c2 for an inner-product-induced norm.\n\n    This is done by reconstructing the Gram matrix G using the polarization\n    identity and finding the square roots of its min and max eigenvalues.\n\n    Args:\n        oracle (callable): The black-box norm function.\n        n (int): The dimension of the vector space.\n\n    Returns:\n        tuple[float, float]: The estimated constants (c1_hat, c2_hat).\n    \"\"\"\n    G = np.zeros((n, n))\n    I = np.eye(n)\n    \n    for i in range(n):\n        for j in range(i, n):\n            ei, ej = I[:, i], I[:, j]\n            val = 0.25 * (oracle(ei + ej)**2 - oracle(ei - ej)**2)\n            G[i, j] = val\n            if i != j:\n                G[j, i] = val\n\n    # Eigenvalues of a real symmetric matrix are real.\n    # eigvalsh is efficient for symmetric matrices.\n    eigenvalues = np.linalg.eigvalsh(G)\n    \n    lambda_min = np.min(eigenvalues)\n    lambda_max = np.max(eigenvalues)\n\n    # The eigenvalues of an SPD matrix must be positive.\n    # Add a small clip for numerical stability near zero.\n    c1_hat = np.sqrt(np.maximum(0, lambda_min))\n    c2_hat = np.sqrt(np.maximum(0, lambda_max))\n    \n    return c1_hat, c2_hat\n\ndef estimate_from_sampling(oracle, n, num_random_samples=1000):\n    \"\"\"\n    Estimates c1 and c2 for a general norm by sampling the unit sphere.\n\n    The sample set includes deterministic directions that are extremizers for\n    l1 and l_inf norms, plus random directions.\n\n    Args:\n        oracle (callable): The black-box norm function.\n        n (int): The dimension of the vector space.\n        num_random_samples (int): The number of random directions to sample.\n\n    Returns:\n        tuple[float, float]: The estimated constants (c1_hat, c2_hat).\n    \"\"\"\n    # 1. Deterministic sample set\n    samples = []\n    \n    # Standard basis vectors and their negatives\n    I = np.eye(n)\n    for i in range(n):\n        samples.append(I[:, i])\n        samples.append(-I[:, i])\n\n    # Uniform direction vector and its negative\n    if n > 0:\n        uniform_vec = np.ones(n) / np.sqrt(n)\n        samples.append(uniform_vec)\n        samples.append(-uniform_vec)\n\n    # 2. Random sample set\n    rng = np.random.default_rng(seed=42)\n    random_vectors = rng.standard_normal((num_random_samples, n))\n    norms = np.linalg.norm(random_vectors, axis=1, keepdims=True)\n    # Avoid division by zero if a zero vector is somehow generated\n    non_zero_norms = np.where(norms == 0, 1, norms)\n    normalized_random_vectors = random_vectors / non_zero_norms\n    \n    for vec in normalized_random_vectors:\n        samples.append(vec)\n        \n    # All vectors in samples have ||x||_2 = 1\n    # The value of ||x||_? is the ratio ||x||_? / ||x||_2\n    norm_values = [oracle(x) for x in samples]\n    \n    c1_hat = np.min(norm_values)\n    c2_hat = np.max(norm_values)\n    \n    return c1_hat, c2_hat\n\n\ndef solve():\n    \"\"\"\n    Main solver function to run the detection experiment on all test cases.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {\n            \"n\": 3,\n            \"oracle\": lambda x: np.sqrt(x.T @ np.diag([1, 4, 9]) @ x),\n            \"eps2\": 1e-4,\n            \"c1_true\": 1.0,\n            \"c2_true\": 3.0,\n        },\n        # Case B\n        {\n            \"n\": 4,\n            \"oracle\": lambda x: np.linalg.norm(x, ord=1),\n            \"eps2\": 1e-3,\n            \"c1_true\": 1.0,\n            \"c2_true\": 2.0,\n        },\n        # Case C\n        {\n            \"n\": 5,\n            \"oracle\": lambda x: np.linalg.norm(x, ord=np.inf),\n            \"eps2\": 2e-6,\n            \"c1_true\": 1 / np.sqrt(5),\n            \"c2_true\": 1.0,\n        },\n        # Case D\n        {\n            \"n\": 3,\n            \"oracle\": lambda x: np.sqrt(x.T @ np.diag([1e-4, 1, 100]) @ x),\n            \"eps2\": 1e-5,\n            \"c1_true\": 0.01,\n            \"c2_true\": 10.0,\n        },\n        # Case E\n        {\n            \"n\": 1,\n            \"oracle\": lambda x: 7 * np.linalg.norm(x, ord=2),\n            \"eps2\": 3e-5,\n            \"c1_true\": 7.0,\n            \"c2_true\": 7.0,\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        n = case[\"n\"]\n        oracle = case[\"oracle\"]\n        eps2 = case[\"eps2\"]\n        c1_true = case[\"c1_true\"]\n        c2_true = case[\"c2_true\"]\n        \n        is_inner_product = test_parallelogram_law(oracle, n)\n        \n        if is_inner_product:\n            c1_hat, c2_hat = estimate_from_gram_matrix(oracle, n)\n        else:\n            # For general norms, the number of random samples is chosen\n            # to be moderate as per the problem. \n            # 100*n is more than sufficient.\n            c1_hat, c2_hat = estimate_from_sampling(oracle, n, num_random_samples=100*n)\n            \n        tau_hat = c1_hat * eps2\n        \n        all_results.append([c1_hat, c2_hat, tau_hat, c1_true, c2_true])\n        \n    # Format the output as a string representing a list of lists.\n    # Python's str() on a list gives a square-bracketed representation,\n    # so we just join these strings with commas.\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}