{
    "hands_on_practices": [
        {
            "introduction": "The theorem of norm equivalence in finite-dimensional spaces is a foundational result, yet its practical power lies in understanding the explicit, dimension-dependent constants that bind the norms together. This exercise moves beyond memorizing these constants by guiding you through their derivation from first principles. By framing the problem as a constrained optimization task, you will not only calculate the sharpest possible equivalence constant $C$ in the inequality $\\|x\\|_p \\le C \\|x\\|_q$ but also uncover the geometric structure of the \"extremal\" vectors that make this bound an equality .",
            "id": "3544577",
            "problem": "Given integers $n \\ge 1$ and real exponents $p,q \\in [1, \\infty]$, consider the family of vector norms on $\\mathbb{R}^n$ defined by $ \\|x\\|_r = \\left(\\sum_{i=1}^n |x_i|^r\\right)^{1/r}$ for $r \\in [1,\\infty)$ and $\\|x\\|_{\\infty} = \\max_{1\\le i \\le n} |x_i|$. The smallest constant $C = C(n,p,q)$ such that $ \\|x\\|_p \\le C \\|x\\|_q$ holds for all $x \\in \\mathbb{R}^n$ and $x \\ne 0$ is given by the supremum $C = \\sup_{x \\ne 0} \\frac{\\|x\\|_p}{\\|x\\|_q}$. Your task is to design and implement a procedure that, for given $n$, $p$, and $q$, estimates this smallest $C$ and identifies vectors that make the bound tight (extremal vectors).\n\nBase your derivation and algorithmic design only on fundamental definitions of norms and standard optimization principles for finite-dimensional spaces. In particular, formulate the problem as a constrained maximization of $\\|x\\|_p$ subject to $\\|x\\|_q = 1$ and reason from first principles to characterize the structure of extremal vectors. Do not assume or quote the final equivalence constant formula; instead, derive it from the structure of the maximizers.\n\nYour program must:\n- Implement a general routine that, given $n$, $p$, and $q$, estimates the optimal constant by searching over candidate extremal structures justified by your derivation, and reports:\n  - the theoretically derived constant $C(n,p,q)$,\n  - the estimated constant from your search procedure,\n  - the absolute discrepancy between these two values,\n  - an integer $k^\\star$ representing the number of nonzero components in an extremal vector identified by your procedure.\n- Treat the cases $p=\\infty$ or $q=\\infty$ carefully using valid limiting arguments consistent with the norm definitions.\n- Use only pure mathematical quantities; there are no physical units involved in this problem.\n- For angles, trigonometric functions, or percentages: these do not apply here and must not be used.\n\nTest Suite:\nProvide results for the following parameter sets $(n,p,q)$:\n- Test $1$: $n=7$, $p=1.5$, $q=3.2$.\n- Test $2$: $n=10$, $p=4$, $q=2$.\n- Test $3$: $n=8$, $p=\\infty$, $q=2$.\n- Test $4$: $n=5$, $p=1$, $q=\\infty$.\n- Test $5$: $n=6$, $p=2$, $q=2$.\n\nAnswer Format:\n- For each test, return a list $[C_{\\mathrm{theory}}, C_{\\mathrm{est}}, \\mathrm{err}, k^\\star]$, where $C_{\\mathrm{theory}}$ is the theoretically derived constant, $C_{\\mathrm{est}}$ is the estimated constant produced by your search routine, $\\mathrm{err}$ is the absolute difference $|C_{\\mathrm{theory}} - C_{\\mathrm{est}}|$, and $k^\\star$ is the number of nonzeros in an extremal vector selected by your method.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each testâ€™s result represented as a sublist and no spaces anywhere. For example: $[ [a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],\\dots ]$ printed without spaces.\n\nYour implementation must be deterministic and require no user input. The final printed line must follow the exact format described above.",
            "solution": "The problem requires the derivation of the optimal constant $C = C(n,p,q)$ in the vector norm equivalence inequality $\\|x\\|_p \\le C \\|x\\|_q$ for $x \\in \\mathbb{R}^n$, and the characterization of the extremal vectors that achieve this bound. The constant $C$ is defined as the supremum of the ratio of norms:\n$$\nC = \\sup_{x \\in \\mathbb{R}^n, x \\ne 0} \\frac{\\|x\\|_p}{\\|x\\|_q}\n$$\nDue to the homogeneity property of norms, i.e., $\\| \\alpha x \\| = |\\alpha| \\|x\\|$ for any scalar $\\alpha$, the ratio is independent of the magnitude of $x$. We can therefore rephrase the problem as a constrained optimization problem: finding the maximum of $\\|x\\|_p$ over the set of all vectors $x$ for which $\\|x\\|_q = 1$.\n$$\nC = \\max_{\\|x\\|_q=1} \\|x\\|_p\n$$\nThe functions $\\|x\\|_p$ and $\\|x\\|_q$ depend only on the absolute values of the components of $x$, $|x_i|$. Thus, we can restrict our search for a maximizing vector $x^*$ to the non-negative orthant, where $x_i \\ge 0$ for all $i=1, \\dots, n$.\n\nFor the moment, let us assume $p, q \\in [1, \\infty)$. The functions $z \\mapsto z^p$ and $z \\mapsto z^q$ are monotonically increasing for $z \\ge 0$. Therefore, maximizing $\\|x\\|_p = (\\sum x_i^p)^{1/p}$ is equivalent to maximizing its $p$-th power, $F(x) = \\sum_{i=1}^n x_i^p$. The constraint becomes $G(x) = \\sum_{i=1}^n x_i^q - 1 = 0$.\n\nWe use the method of Lagrange multipliers to find the extremal points. The Lagrangian is:\n$$\n\\mathcal{L}(x, \\lambda) = F(x) - \\lambda G(x) = \\sum_{i=1}^n x_i^p - \\lambda \\left( \\sum_{i=1}^n x_i^q - 1 \\right)\n$$\nTo find critical points, we set the partial derivatives with respect to each $x_j$ (for which $x_j > 0$) to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_j} = p x_j^{p-1} - \\lambda q x_j^{q-1} = 0\n$$\nThis implies $p x_j^{p-1} = \\lambda q x_j^{q-1}$. For any non-zero component $x_j > 0$, we have:\n$$\nx_j^{p-q} = \\frac{\\lambda q}{p}\n$$\nIf $p \\ne q$, this equation dictates that any non-zero components of a candidate extremal vector must have the same value. Let this value be $a > 0$. If $p = q$, the equation becomes $p x_j^{p-1} = \\lambda p x_j^{p-1}$, implying $\\lambda=1$ but providing no constraint on the values of $x_j$, which is expected since if $p=q$, $\\|x\\|_p / \\|x\\|_q = 1$ for any non-zero vector $x$.\n\nThis observation establishes the structure of extremal vectors: they must have some number of non-zero components, say $k$ (where $1 \\le k \\le n$), all of which are equal in magnitude, and $n-k$ components which are zero. Let's denote such a vector (with positive components) as $x^{(k)}$. Without loss of generality, $x^{(k)}$ has its first $k$ components equal to $a > 0$ and the remaining $n-k$ components equal to $0$.\n\nWe determine the value of $a$ using the constraint $\\|x^{(k)}\\|_q=1$:\n$$\n\\|x^{(k)}\\|_q = \\left( \\sum_{i=1}^k a^q + \\sum_{i=k+1}^n 0^q \\right)^{1/q} = (k a^q)^{1/q} = k^{1/q} a = 1\n$$\nSolving for $a$, we get $a = k^{-1/q}$.\n\nNow, we evaluate the objective function, $\\|x^{(k)}\\|_p$, for this vector:\n$$\n\\|x^{(k)}\\|_p = \\left( \\sum_{i=1}^k a^p \\right)^{1/p} = (k a^p)^{1/p} = k^{1/p} a\n$$\nSubstituting the expression for $a$:\n$$\nC_k = \\|x^{(k)}\\|_p = k^{1/p} (k^{-1/q}) = k^{1/p - 1/q}\n$$\nThe optimal constant $C$ is the maximum value of $C_k$ over all possible numbers of non-zero components, $k \\in \\{1, 2, \\dots, n\\}$.\n$$\nC = \\max_{k \\in \\{1, \\dots, n\\}} k^{1/p - 1/q}\n$$\nThe behavior of the function $f(k)=k^\\alpha$ with $\\alpha = 1/p - 1/q$ depends on the sign of $\\alpha$.\n\nCase 1: $p  q$.\nIn this case, $1/p > 1/q$, so the exponent $\\alpha = 1/p - 1/q$ is positive. The function $f(k) = k^\\alpha$ is strictly increasing with $k$. Therefore, the maximum is achieved at the largest possible value of $k$, which is $k=n$.\nThe optimal constant is $C(n,p,q) = n^{1/p - 1/q}$.\nThe extremal vectors have all $n$ components non-zero and equal in magnitude, so $k^\\star = n$.\n\nCase 2: $p > q$.\nIn this case, $1/p  1/q$, so the exponent $\\alpha = 1/p - 1/q$ is negative. The function $f(k) = k^\\alpha$ is strictly decreasing with $k$. The maximum is achieved at the smallest possible value of $k$, which is $k=1$.\nThe optimal constant is $C(n,p,q) = 1^{1/p - 1/q} = 1$.\nThe extremal vectors have only one non-zero component (e.g., a scaled standard basis vector), so $k^\\star = 1$.\n\nCase 3: $p = q$.\nIn this case, $\\alpha = 0$, so $C_k = k^0 = 1$ for all $k \\in \\{1, \\dots, n\\}$. The constant is $C(n,p,p)=1$. Any vector is an extremizer in the sense that the ratio is $1$. However, to provide a single value for $k^\\star$, we align with the $p \\ge q$ case for continuity. The formal argument using Jensen's inequality for $p \\ge q$ demonstrates that the supremum is $1$ and is attained for vectors with $k=1$ non-zero component. Thus, for $p \\ge q$, we consistently have $k^\\star=1$.\n\nWe now extend this to include the infinity norm, where $\\|x\\|_\\infty = \\max_i |x_i|$. This can be done by treating $\\infty$ as a limit, where $1/\\infty \\to 0$.\n\nCase 4: $p = \\infty$ (and $q  \\infty$).\nThis corresponds to the $p > q$ case. The formula gives an exponent of $1/\\infty - 1/q = -1/q  0$. The maximum of $k^{-1/q}$ is at $k=1$, yielding $C=1^{ -1/q}=1$ and $k^\\star=1$. This is consistent. Directly, we want to maximize $\\|x\\|_\\infty$ given $\\|x\\|_q=1$. For any $x$, let $|x_j| = \\|x\\|_\\infty$. Then $\\|x\\|_q^q = \\sum_i |x_i|^q \\ge |x_j|^q = \\|x\\|_\\infty^q$. Thus, $\\|x\\|_q \\ge \\|x\\|_\\infty$, which means $\\|x\\|_\\infty/\\|x\\|_q \\le 1$. The maximum is $1$, achieved when all other components are zero.\n\nCase 5: $q = \\infty$ (and $p  \\infty$).\nThis corresponds to the $p  q$ case. The formula gives an exponent of $1/p - 1/\\infty = 1/p > 0$. The maximum of $k^{1/p}$ is at $k=n$, yielding $C = n^{1/p}$ and $k^\\star=n$. This is consistent. Directly, we want to maximize $\\|x\\|_p$ given $\\|x\\|_\\infty=1$. The constraint $\\|x\\|_\\infty=1$ implies $|x_i| \\le 1$ for all $i$. To maximize $\\left(\\sum |x_i|^p\\right)^{1/p}$, we should choose $|x_i|$ to be its maximum possible value, $|x_i|=1$, for all $i$. Such a vector satisfies the constraint and yields $\\|x\\|_p = (\\sum 1^p)^{1/p} = n^{1/p}$.\n\nSummary of theoretical results:\n- If $p  q$: $C(n,p,q) = n^{1/p - 1/q}$ and $k^\\star = n$. This includes the case $q=\\infty, p  \\infty$.\n- If $p \\ge q$: $C(n,p,q) = 1$ and $k^\\star = 1$. This includes the cases $p=q$ and $p=\\infty, q  \\infty$.\n\nThe algorithmic procedure will confirm this derivation by performing a direct search. For a given $(n,p,q)$, it will:\n1. Define the inverse exponents, with $1/\\infty=0$.\n2. Create an array of candidate values for the number of non-zero components, $k \\in \\{1, 2, \\dots, n\\}$.\n3. Compute the ratio $C_k = k^{1/p - 1/q}$ for each $k$.\n4. The estimated constant $C_{\\mathrm{est}}$ is the maximum value in this array of ratios.\n5. The number of non-zeros in the extremal vector, $k^\\star$, is the value of $k$ that produced this maximum. We take the first such $k$ in case of ties, which `numpy.argmax` does by default. This correctly resolves ties for $p \\ge q$ to $k=1$.\n6. The theoretical constant $C_{\\mathrm{theory}}$ and $k^\\star_{\\mathrm{theory}}$ are computed from the summarized formulas.\n7. The discrepancy is calculated as $\\mathrm{err} = |C_{\\mathrm{theory}} - C_{\\mathrm{est}}|$. Due to the direct correspondence between the derivation and the search, this error will be zero.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal constant C in the norm equivalence inequality ||x||_p = C ||x||_q\n    for given n, p, and q, and identifies the structure of an extremal vector.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Format: (n, p, q), where p or q can be np.inf\n    test_cases = [\n        (7, 1.5, 3.2),\n        (10, 4.0, 2.0),\n        (8, np.inf, 2.0),\n        (5, 1.0, np.inf),\n        (6, 2.0, 2.0),\n    ]\n\n    results = []\n    for n, p, q in test_cases:\n        # 1. Theoretical Calculation\n        # The formulas are derived from first principles in the solution text.\n        # The derivation relies on analyzing the function f(k) = k^(1/p - 1/q) for k in {1, ..., n}.\n        \n        # Handle infinite values for p and q, where 1/inf = 0.\n        inv_p = 1.0 / p if p != np.inf else 0.0\n        inv_q = 1.0 / q if q != np.inf else 0.0\n\n        if p  q:\n            # Exponent (1/p - 1/q) is positive, so k^exponent is maximized at k=n.\n            C_theory = n**(inv_p - inv_q)\n            k_star_theory = n\n        else: # p = q\n            # Exponent (1/p - 1/q) is non-positive, so k^exponent is maximized at k=1.\n            C_theory = 1.0\n            k_star_theory = 1\n\n        # 2. Estimation via Search over Candidate Structures\n        # The derivation showed that extremal vectors have k identical non-zero components.\n        # This search programmatically confirms the derivation by checking all possible k.\n        \n        # Candidate values for the number of non-zero entries\n        k_values = np.arange(1, n + 1)\n        \n        # Exponent for the ratio calculation\n        exponent = inv_p - inv_q\n        \n        # Calculate the ratio C_k = k^exponent for each k\n        ratios = k_values.astype(float) ** exponent\n        \n        # The estimated constant is the maximum of these ratios\n        C_est = np.max(ratios)\n        \n        # k_star is the number of non-zero components (k) that yields the maximum ratio.\n        # np.argmax returns the index of the first occurrence of the maximum value.\n        # For p = q, exponent = 0, ratios are non-increasing, so argmax is 0, k=1.\n        # For p  q, exponent  0, ratios are increasing, so argmax is n-1, k=n.\n        k_star_est = k_values[np.argmax(ratios)]\n        \n        # 3. Discrepancy Calculation\n        err = np.abs(C_theory - C_est)\n        \n        # Ensure k_star is an integer for the output format\n        k_star_output = int(k_star_est)\n        \n        # Append the list of results for this test case\n        results.append([C_theory, C_est, err, k_star_output])\n\n    # 4. Final Output Formatting\n    # The output must be a single line, with no spaces, containing a list of lists.\n    # Example: [[r1_1,r1_2],[r2_1,r2_2]]\n    # We build this string representation piece by piece.\n    result_strings = []\n    for res_list in results:\n        # Format each sublist like [val1,val2,val3,val4]\n        sublist_str = f'[{\",\".join(map(str, res_list))}]'\n        result_strings.append(sublist_str)\n    \n    # Join all sublist strings into the final format.\n    final_output = f'[{\",\".join(result_strings)}]'\n    \n    # Print the final, exactly formatted string.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "With a foundational understanding of how to derive equivalence constants, we now explore their profound implications in modern data science. This practice examines the interplay between the $L_1$, $L_2$, and $L_\\infty$ norms within the context of penalized regression, a cornerstone of machine learning. You will construct the specific vector that maximizes the ratio between the $L_1$ and $L_\\infty$ norms, and in doing so, reveal why this \"densest\" possible vector represents a point of maximum disagreement between Lasso and Ridge regularization, highlighting the geometric origins of sparsity promotion .",
            "id": "3544607",
            "problem": "Let $n \\in \\mathbb{N}$ with $n \\ge 2$ and let $x \\in \\mathbb{R}^{n}$. For $x = (x_{1},\\dots,x_{n})$, recall the standard definitions of the vector norms: $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$, $\\|x\\|_{\\infty} = \\max_{1 \\le i \\le n} |x_{i}|$, and $\\|x\\|_{2} = \\left(\\sum_{i=1}^{n} x_{i}^{2}\\right)^{1/2}$. \n\n(a) Construct a vector $x^{\\star} \\in \\mathbb{R}^{n}$ such that $\\|x^{\\star}\\|_{1} = n \\,\\|x^{\\star}\\|_{\\infty}$, and justify from first principles (using only the definitions of the norms) that this realizes the maximal possible value of the ratio $\\|x\\|_{1}/\\|x\\|_{\\infty}$ over nonzero $x \\in \\mathbb{R}^{n}$.\n\n(b) Consider the two penalized least-squares objectives in $\\mathbb{R}^{n}$ with a common quadratic loss $f(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$, where $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^{m}$ are fixed but otherwise arbitrary. The first regularizer is the Least Absolute Shrinkage and Selection Operator (lasso) penalty $\\lambda \\|x\\|_{1}$, and the second is the ridge penalty $\\mu \\|x\\|_{2}$. To isolate the effect of norm geometry, compare the penalty magnitudes at the single point $x^{\\star}$ from part (a) under the normalization $\\|x^{\\star}\\|_{\\infty} = 1$. Choose $\\lambda  0$ and $\\mu  0$ so that the two penalties coincide at $x^{\\star}$, i.e., $\\lambda \\|x^{\\star}\\|_{1} = \\mu \\|x^{\\star}\\|_{2}$. What is the exact value of the ratio $\\mu/\\lambda$ as a function of $n$? Report your answer as a closed-form expression in terms of $n$.\n\n(c) Using only the definitions of the norms and elementary inequalities, argue how norm equivalence in finite dimension constrains the geometric distinguishability of the lasso and ridge penalties for small $n$, and explain briefly how the construction in part (a) represents a worst case for sparsity discrimination under the constraint $\\|x\\|_{\\infty}=1$. Your justification should be qualitative and rely on deriving tight bounds from first principles; no numerical answer is required for this part.\n\nThe final answer you submit must be the single closed-form expression for the ratio $\\mu/\\lambda$ from part (b).",
            "solution": "The problem is divided into three parts. We will address them sequentially. The final answer is the result from part (b).\n\n(a) Construction and justification for the maximality of the ratio $\\|x\\|_{1}/\\|x\\|_{\\infty}$.\n\nLet $x = (x_{1}, \\dots, x_{n}) \\in \\mathbb{R}^{n}$ be an arbitrary nonzero vector. The definitions of the $L_1$-norm and $L_\\infty$-norm are given as:\n$$ \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}| $$\n$$ \\|x\\|_{\\infty} = \\max_{1 \\le i \\le n} |x_{i}| $$\nBy the definition of the maximum, for any component $x_i$ of the vector $x$, we have the inequality $|x_{i}| \\le \\|x\\|_{\\infty}$. Summing this inequality over all components from $i=1$ to $n$:\n$$ \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}| \\le \\sum_{i=1}^{n} \\|x\\|_{\\infty} $$\nSince $\\|x\\|_{\\infty}$ is a constant value with respect to the summation index $i$, the sum on the right side becomes $n \\|x\\|_{\\infty}$. This establishes the general inequality:\n$$ \\|x\\|_{1} \\le n \\|x\\|_{\\infty} $$\nFor any nonzero vector $x$, we can divide by $\\|x\\|_{\\infty}$ (which must be positive) to get the upper bound on the ratio:\n$$ \\frac{\\|x\\|_{1}}{\\|x\\|_{\\infty}} \\le n $$\nTo show that $n$ is the maximal possible value of this ratio, we must demonstrate that this bound is tight. This requires constructing a specific vector $x^{\\star}$ for which the equality $\\|x^{\\star}\\|_{1} = n \\|x^{\\star}\\|_{\\infty}$ holds.\nThe equality in the derivation $\\sum_{i=1}^{n} |x_{i}| \\le \\sum_{i=1}^{n} \\|x\\|_{\\infty}$ holds if and only if $|x_{i}| = \\|x\\|_{\\infty}$ for all $i \\in \\{1, \\dots, n\\}$.\nWe can construct such a vector. Let $c$ be any nonzero real number, and define the vector $x^{\\star} \\in \\mathbb{R}^{n}$ as $x^{\\star} = (c, c, \\dots, c)$.\nLet's choose $c=1$ for simplicity. Then $x^{\\star} = (1, 1, \\dots, 1)$.\nWe calculate the norms for this vector:\n$$ \\|x^{\\star}\\|_{\\infty} = \\max_{1 \\le i \\le n} |1| = 1 $$\n$$ \\|x^{\\star}\\|_{1} = \\sum_{i=1}^{n} |1| = n $$\nNow we verify if the condition from the problem statement is met:\n$$ \\|x^{\\star}\\|_{1} = n \\quad \\text{and} \\quad n \\|x^{\\star}\\|_{\\infty} = n \\cdot 1 = n $$\nThus, $\\|x^{\\star}\\|_{1} = n \\|x^{\\star}\\|_{\\infty}$ holds. Since we have found a vector that achieves the upper bound $n$, we have proven that $n$ is the maximal value of the ratio $\\|x\\|_{1} / \\|x\\|_{\\infty}$ for any nonzero $x \\in \\mathbb{R}^{n}$.\n\n(b) Calculation of the ratio $\\mu/\\lambda$.\n\nWe are asked to compare the lasso penalty, $\\lambda \\|x\\|_{1}$, and the ridge penalty, $\\mu \\|x\\|_{2}$, at the specific vector $x^{\\star}$ from part (a). We use the vector $x^{\\star} = (1, 1, \\dots, 1)$, which satisfies the given normalization condition $\\|x^{\\star}\\|_{\\infty} = 1$.\nWe need to calculate the $L_1$ and $L_2$ norms of $x^{\\star}$.\nFrom part (a), we have $\\|x^{\\star}\\|_{1} = n$.\nThe $L_2$-norm is defined as $\\|x\\|_{2} = \\left(\\sum_{i=1}^{n} x_{i}^{2}\\right)^{1/2}$. For our vector $x^{\\star}$:\n$$ \\|x^{\\star}\\|_{2} = \\left(\\sum_{i=1}^{n} 1^{2}\\right)^{1/2} = \\left(\\sum_{i=1}^{n} 1\\right)^{1/2} = n^{1/2} = \\sqrt{n} $$\nThe parameters $\\lambda  0$ and $\\mu  0$ are chosen such that the two penalties coincide at $x^{\\star}$:\n$$ \\lambda \\|x^{\\star}\\|_{1} = \\mu \\|x^{\\star}\\|_{2} $$\nSubstituting the computed norm values into this equation:\n$$ \\lambda \\cdot n = \\mu \\cdot \\sqrt{n} $$\nTo find the exact value of the ratio $\\mu/\\lambda$, we rearrange the equation:\n$$ \\frac{\\mu}{\\lambda} = \\frac{n}{\\sqrt{n}} $$\nSimplifying this expression yields the final result:\n$$ \\frac{\\mu}{\\lambda} = \\sqrt{n} $$\n\n(c) Qualitative justification.\n\nAll norms on a finite-dimensional vector space like $\\mathbb{R}^{n}$ are equivalent. This means that for any two norms, say $\\|\\cdot\\|_a$ and $\\|\\cdot\\|_b$, there exist positive constants $c_{1}$ and $c_{2}$ (which may depend on the dimension $n$) such that for all $x \\in \\mathbb{R}^n$: $c_{1} \\|x\\|_{a} \\le \\|x\\|_{b} \\le c_{2} \\|x\\|_{a}$.\nFor the $L_1$ and $L_2$ norms, the tight equivalence inequalities are:\n$$ \\|x\\|_{2} \\le \\|x\\|_{1} \\le \\sqrt{n} \\|x\\|_{2} $$\nThe lower bound, $\\|x\\|_{2} \\le \\|x\\|_{1}$, follows from $\\|x\\|_{1}^2 = (\\sum |x_{i}|)^2 = \\sum x_{i}^2 + \\sum_{i \\neq j} |x_i||x_j| \\ge \\sum x_{i}^2 = \\|x\\|_{2}^2$. The upper bound, $\\|x\\|_{1} \\le \\sqrt{n} \\|x\\|_{2}$, follows from the Cauchy-Schwarz inequality: $\\|x\\|_{1} = \\sum |x_i| \\cdot 1 \\le (\\sum |x_i|^2)^{1/2} (\\sum 1^2)^{1/2} = \\|x\\|_2 \\sqrt{n}$.\nThe ratio of the constants $c_2/c_1$ is $\\sqrt{n}/1 = \\sqrt{n}$. For small $n$, this ratio is close to $1$, which implies the norms are geometrically similar. For example, if $n=2$, the $L_1$ unit ball is a square rotated by $45^\\circ$ and the $L_2$ unit ball is a circle; they are not drastically different. This relative similarity means the level sets of the lasso penalty ($\\lambda \\|x\\|_1 = \\text{const}$) and the ridge penalty ($\\mu \\|x\\|_2 = \\text{const}$) are geometrically similar, making their regularizing effects less distinguishable for small $n$.\n\nThe vector $x^{\\star} = (1, 1, \\dots, 1)$ with $\\|x^{\\star}\\|_\\infty = 1$ is maximally non-sparse or \"dense\" under this constraint, as all its components are non-zero and have the largest possible magnitude. The key feature of the lasso penalty is its ability to promote sparsity (solutions with many zero components), a property stemming from the sharp vertices of its polyhedral unit ball which lie on the coordinate axes. The vector $x^{\\star}$ is antithetical to this; it points towards the center of a face of the $L_1$ ball, a region maximally distant from the sparse vertices.\nThe construction in part (a) identifies $x^{\\star}$ as the vector that realizes the equality in the inequality $\\|x\\|_{1} \\le \\sqrt{n} \\|x\\|_{2}$. That is, $x^{\\star}$ is an instance where the ratio $\\|x\\|_{1}/\\|x\\|_{2}$ achieves its maximum possible value of $\\sqrt{n}$. In contrast, for a sparse vector like $x_s = (1, 0, \\dots, 0)$, we have $\\|x_s\\|_{1}=1$ and $\\|x_s\\|_{2}=1$, and the ratio is $1$. The vector $x^{\\star}$ thus represents the point of maximum geometric discrepancy between the $L_1$ and $L_2$ norms. Calibrating the penalties at this point of maximum disagreement represents a \"worst-case\" scenario for sparsity discrimination. It highlights the most extreme difference in the scaling of the two penalties, a situation that occurs on dense vectors, which are precisely the vectors for which the unique sparsity-inducing characteristic of the $L_1$ norm is least relevant.",
            "answer": "$$\\boxed{\\sqrt{n}}$$"
        },
        {
            "introduction": "This final practice synthesizes theory and computation by placing you in the role of a numerical detective. Faced with a \"black-box\" oracle that computes an unknown norm, your task is to design an experiment to uncover its fundamental properties and its relationship to the standard Euclidean norm. This investigation will require you to leverage deep theoretical tools, such as the parallelogram law and polarization identity, to build a practical estimation procedure, culminating in the crucial task of recalibrating a solver's stopping criterion to meet a desired precision .",
            "id": "3544597",
            "problem": "You are given query access to a black-box residual reporter that, for any input vector in a finite-dimensional real vector space, returns the residual magnitude in an unknown norm. Formally, for a given dimension $n \\in \\mathbb{N}$, you can query an oracle that, for any $x \\in \\mathbb{R}^n$, returns a scalar $\\|x\\|_{?} \\in \\mathbb{R}_{\\ge 0}$. The unknown norm $\\|\\cdot\\|_{?}$ is guaranteed to be equivalent to the Euclidean norm $\\|\\cdot\\|_{2}$ on $\\mathbb{R}^n$, meaning that there exist constants $c_1, c_2 \\in \\mathbb{R}$ with $0  c_1 \\le c_2  \\infty$ such that, for all $x \\in \\mathbb{R}^n$, the inequality $c_1 \\|x\\|_2 \\le \\|x\\|_{?} \\le c_2 \\|x\\|_2$ holds. The task is to design a detection experiment and implement it as a program that, using only queries to $\\|\\cdot\\|_{?}$, estimates $c_1$ and $c_2$, and uses these estimates to recalibrate a stopping threshold so that a prescribed Euclidean residual target is met.\n\nYour design and implementation must be derived from the following fundamental base:\n- The definition of a norm, the Euclidean norm $\\|\\cdot\\|_2$, and norm equivalence on finite-dimensional spaces.\n- The characterization of norms induced by an inner product via the parallelogram law: a norm $\\|\\cdot\\|$ is induced by some inner product if and only if, for all $x,y \\in \\mathbb{R}^n$, the identity $\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2$ holds.\n- The polarization identity in real inner product spaces: for an inner-product-induced norm $\\|\\cdot\\|$, the associated inner product $\\langle x,y\\rangle$ satisfies $\\langle x,y\\rangle = \\tfrac{1}{4}(\\|x+y\\|^2 - \\|x-y\\|^2)$ for all $x,y \\in \\mathbb{R}^n$.\n- The spectral characterization of equivalence constants when the unknown norm is induced by a Symmetric Positive Definite (SPD) matrix $M \\in \\mathbb{R}^{n \\times n}$ via $\\|x\\|_{M} = \\sqrt{x^\\top M x}$: if $\\lambda_{\\min}(M)$ and $\\lambda_{\\max}(M)$ denote the smallest and largest eigenvalues of $M$, then, for all $x \\in \\mathbb{R}^n$, the inequality $\\sqrt{\\lambda_{\\min}(M)} \\|x\\|_2 \\le \\|x\\|_M \\le \\sqrt{\\lambda_{\\max}(M)} \\|x\\|_2$ holds.\n- The fundamental inequalities relating $\\ell_p$ norms and the Euclidean norm on $\\mathbb{R}^n$: for the $\\ell_1$ norm, $\\|x\\|_2 \\le \\|x\\|_1 \\le \\sqrt{n} \\|x\\|_2$ for all $x \\in \\mathbb{R}^n$; for the $\\ell_\\infty$ norm, $n^{-1/2}\\|x\\|_2 \\le \\|x\\|_\\infty \\le \\|x\\|_2$ for all $x \\in \\mathbb{R}^n$.\n\nYour program must implement the following detection experiment:\n- Given query access to $\\| \\cdot \\|_{?}$ for a fixed dimension $n$, first test whether $\\|\\cdot\\|_{?}$ is induced by an inner product by checking the parallelogram law on a set of randomly sampled pairs $(x,y)$. If this test indicates an inner-product-induced norm (within a numerical tolerance that you must justify), reconstruct the associated Gram matrix $G \\in \\mathbb{R}^{n \\times n}$ using the polarization identity restricted to the standard basis vectors. Then estimate $c_1$ and $c_2$ as $\\sqrt{\\lambda_{\\min}(G)}$ and $\\sqrt{\\lambda_{\\max}(G)}$ respectively, where $\\lambda_{\\min}(G)$ and $\\lambda_{\\max}(G)$ are the smallest and largest eigenvalues of $G$.\n- If the parallelogram law test fails (indicating a general norm not necessarily induced by an inner product), estimate $c_1$ and $c_2$ by sampling unit Euclidean vectors on the sphere $\\{x \\in \\mathbb{R}^n : \\|x\\|_2 = 1\\}$ and computing the minimum and maximum of the ratio $\\|x\\|_{?}/\\|x\\|_2$ over this sample. Your sampling set must include a deterministic covering of candidate directions that capture extremizers for classical norms, including all coordinate axes and their negatives, as well as the uniform direction $u = \\tfrac{1}{\\sqrt{n}}(1,\\dots,1)^\\top$, along with a moderate number of random directions. Justify why this set detects the exact constants for the $\\ell_1$ and $\\ell_\\infty$ norms and provides upper and lower bounds in the general case.\n\nRecalibration rule:\n- Suppose a black-box solver stops when $\\|r\\|_{?} \\le \\tau$ for a residual vector $r \\in \\mathbb{R}^n$. By norm equivalence, a safe sufficient condition for $\\|r\\|_2 \\le \\varepsilon_2$ is $\\tau \\le c_1 \\varepsilon_2$. Therefore, given a target Euclidean residual tolerance $\\varepsilon_2  0$, your program must compute $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$ using your estimate $\\widehat{c}_1$.\n\nTest suite:\nImplement your program to run on the following test cases. Each case defines the dimension $n$, the unknown norm $\\|\\cdot\\|_{?}$ via an oracle, and a target Euclidean tolerance $\\varepsilon_2$. Your program must, for each case, estimate $\\widehat{c}_1$ and $\\widehat{c}_2$, compute $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$, and also report the true constants $c_1$ and $c_2$ for verification.\n\n- Case A: $n = 3$. Unknown norm $\\|x\\|_{?} = \\sqrt{x^\\top M x}$ with $M = \\mathrm{diag}(1, 4, 9)$. Target $\\varepsilon_2 = 10^{-4}$.\n- Case B: $n = 4$. Unknown norm $\\|x\\|_{?} = \\|x\\|_1$. Target $\\varepsilon_2 = 10^{-3}$.\n- Case C: $n = 5$. Unknown norm $\\|x\\|_{?} = \\|x\\|_\\infty$. Target $\\varepsilon_2 = 2 \\times 10^{-6}$.\n- Case D: $n = 3$. Unknown norm $\\|x\\|_{?} = \\sqrt{x^\\top M x}$ with $M = \\mathrm{diag}(10^{-4}, 1, 100)$. Target $\\varepsilon_2 = 10^{-5}$.\n- Case E: $n = 1$. Unknown norm $\\|x\\|_{?} = \\alpha \\|x\\|_2$ with $\\alpha = 7$. Target $\\varepsilon_2 = 3 \\times 10^{-5}$.\n\nAnswer specification:\n- For each case, your program must output a list of five floating-point numbers $[\\widehat{c}_1, \\widehat{c}_2, \\widehat{\\tau}, c_1, c_2]$ in this order.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list for one case, in the same order as the cases are presented above. For example, the output should look like $[[a_1,a_2,a_3,a_4,a_5],[b_1,b_2,b_3,b_4,b_5],\\dots]$.\n- All computations are dimensionless and no physical units are involved.\n\nYour implementation must be self-contained and must not use any external input. The detection tolerance, sampling size, and any randomized components must be justified in your solution narrative and chosen to ensure that, for the specified test suite, the estimates match the true constants exactly in all cases.",
            "solution": "The problem requires the design and implementation of an experiment to estimate the equivalence constants, $c_1$ and $c_2$, between an unknown norm $\\|\\cdot\\|_{?}$ and the standard Euclidean norm $\\|\\cdot\\|_2$ on $\\mathbb{R}^n$. These constants are defined by the inequality $c_1 \\|x\\|_2 \\le \\|x\\|_{?} \\le c_2 \\|x\\|_2$ for all $x \\in \\mathbb{R}^n$. The estimation is to be performed using only query access to an oracle for $\\|\\cdot\\|_{?}$. The estimates are then used to recalibrate a solver's stopping threshold $\\tau$ to meet a prescribed Euclidean tolerance $\\varepsilon_2$.\n\nThe design is rooted in a fundamental property of norms on vector spaces: a norm is induced by an inner product if and only if it satisfies the parallelogram law. This provides a clear criterion to distinguish between two classes of norms, each admitting a different and highly effective estimation strategy.\n\nThe overall algorithm proceeds in three stages:\n1.  Test whether the unknown norm $\\|\\cdot\\|_{?}$ is induced by an inner product by verifying the parallelogram law for a sample of random vectors.\n2.  Based on the outcome of the test, apply a specialized procedure to estimate $c_1$ and $c_2$.\n    a. If it is an inner-product-induced norm, reconstruct the associated Gram matrix and compute the constants from its eigenvalues.\n    b. If it is a general norm, estimate the constants by sampling the unit sphere at a set of strategically chosen directions.\n3.  Use the estimate $\\widehat{c}_1$ to compute the recalibrated stopping threshold $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$.\n\nWe now detail each stage of this design.\n\n**Stage 1: The Parallelogram Law Test**\n\nA norm $\\|\\cdot\\|$ on a real vector space is induced by an inner product if and only if it satisfies the parallelogram law for all vectors $x, y$:\n$$\n\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2\n$$\nDue to floating-point arithmetic, an exact equality check is not feasible. We instead test this identity probabilistically for a set of randomly generated vector pairs $(x,y)$ and check if the relative error is within a small numerical tolerance $\\delta_{PL}$. For each pair, we compute the error:\n$$\nE(x,y) = \\frac{| (\\|x+y\\|_{?}^2 + \\|x-y\\|_{?}^2) - (2\\|x\\|_{?}^2 + 2\\|y\\|_{?}^2) |}{2\\|x\\|_{?}^2 + 2\\|y\\|_{?}^2}\n$$\nIf the maximum error over a sufficient number of samples (e.g., $100$) is less than $\\delta_{PL}$, we classify the norm as being induced by an inner product. A tolerance of $\\delta_{PL} = 10^{-9}$ is appropriate, as it is small enough to account for standard double-precision floating-point inaccuracies but large enough to detect structural violations from norms not derived from an inner product. For example, the $\\ell_1$ and $\\ell_\\infty$ norms exhibit large, order-one violations of this law for simple vectors like the standard basis vectors, which are easily detected by random sampling.\n\n**Stage 2a: Estimation for Inner-Product-Induced Norms**\n\nIf the parallelogram law test passes, we proceed under the assumption that $\\|\\cdot\\|_{?}$ is induced by an inner product $\\langle \\cdot, \\cdot \\rangle_?$, such that $\\|x\\|_{?}^2 = \\langle x, x \\rangle_?$. This inner product can be represented by a symmetric positive-definite (SPD) matrix $G \\in \\mathbb{R}^{n \\times n}$, called the Gram matrix, with respect to the standard basis $\\{e_i\\}_{i=1}^n$. We have $\\|x\\|_{?}^2 = x^\\top G x$. The entries of this matrix are given by $G_{ij} = \\langle e_i, e_j \\rangle_?$.\n\nTo reconstruct $G$, we utilize the polarization identity, which expresses the inner product in terms of the norm:\n$$\n\\langle u, v \\rangle_? = \\frac{1}{4} \\left( \\|u+v\\|_{?}^2 - \\|u-v\\|_{?}^2 \\right)\n$$\nBy substituting $u=e_i$ and $v=e_j$ for all $i,j \\in \\{1,\\dots,n\\}$, we can compute every entry of $G$ using queries to the oracle.\n$$\nG_{ij} = \\frac{1}{4} \\left( \\|e_i+e_j\\|_{?}^2 - \\|e_i-e_j\\|_{?}^2 \\right)\n$$\nOnce the matrix $\\widehat{G}$ is numerically constructed, the equivalence constants are given by the square roots of its extremal eigenvalues. This is a direct consequence of the Rayleigh-Ritz theorem, which states that for any non-zero $x \\in \\mathbb{R}^n$:\n$$\n\\lambda_{\\min}(G) \\le \\frac{x^\\top G x}{x^\\top x} \\le \\lambda_{\\max}(G)\n$$\nSubstituting $\\|x\\|_{?}^2 = x^\\top G x$ and $\\|x\\|_2^2 = x^\\top x$, we get $\\lambda_{\\min}(G) \\|x\\|_2^2 \\le \\|x\\|_{?}^2 \\le \\lambda_{\\max}(G) \\|x\\|_2^2$. Taking the square root gives the equivalence inequality with $c_1 = \\sqrt{\\lambda_{\\min}(G)}$ and $c_2 = \\sqrt{\\lambda_{\\max}(G)}$. Our estimates are therefore $\\widehat{c}_1 = \\sqrt{\\lambda_{\\min}(\\widehat{G})}$ and $\\widehat{c}_2 = \\sqrt{\\lambda_{\\max}(\\widehat{G})}$.\n\n**Stage 2b: Estimation for General Norms**\n\nIf the parallelogram law test fails, we must use a different approach. The constants $c_1$ and $c_2$ are formally defined as the minimum and maximum of the function $f(x) = \\|x\\|_{?}$ over the unit sphere $S^{n-1} = \\{x \\in \\mathbb{R}^n : \\|x\\|_2 = 1\\}$:\n$$\nc_1 = \\min_{x \\in S^{n-1}} \\|x\\|_{?} \\quad , \\quad c_2 = \\max_{x \\in S^{n-1}} \\|x\\|_{?}\n$$\nSince searching the entire continuous sphere is impossible, we approximate this by finding the minimum and maximum over a finite, well-chosen subset of sample vectors. Per the problem's requirements, our sampling set will include:\n1.  A deterministic set of directions known to be extremizers for common norms: $\\{ \\pm e_i \\}_{i=1}^n$ (coordinate axes) and $\\pm \\frac{1}{\\sqrt{n}}(1, \\dots, 1)^\\top$ (uniform directions).\n2.  A set of randomly generated directions, normalized to unit Euclidean length, to improve the estimate for arbitrary norms not covered by the deterministic set.\n\nThis choice of a deterministic set is critical. For the $\\ell_1$-norm, a standard result is that $\\|x\\|_2 \\le \\|x\\|_1 \\le \\sqrt{n}\\|x\\|_2$. The lower bound is achieved for $x=e_i$, giving a ratio of $\\|e_i\\|_1/\\|e_i\\|_2 = 1/1=1$. The upper bound is achieved for $x = \\frac{1}{\\sqrt{n}}(1,\\dots,1)^\\top$, giving a ratio of $\\|x\\|_1/\\|x\\|_2 = \\sqrt{n}/1 = \\sqrt{n}$. Thus, our deterministic set is guaranteed to find the exact constants $c_1=1$ and $c_2=\\sqrt{n}$ for the $\\ell_1$-norm.\n\nFor the $\\ell_\\infty$-norm, the standard result is $n^{-1/2}\\|x\\|_2 \\le \\|x\\|_\\infty \\le \\|x\\|_2$. The lower bound is achieved for $x = \\frac{1}{\\sqrt{n}}(1,\\dots,1)^\\top$, giving ratio $n^{-1/2}$. The upper bound is achieved for $x=e_i$, giving ratio $1$. Again, our deterministic set finds the exact constants $c_1=n^{-1/2}$ and $c_2=1$.\n\nFor the test cases provided, this sampling strategy is sufficient to find the exact constants. The estimates are $\\widehat{c}_1 = \\min_{x \\in S} \\|x\\|_{?}$ and $\\widehat{c}_2 = \\max_{x \\in S} \\|x\\|_{?}$ where $S$ is our combined sample set of unit vectors.\n\n**Stage 3: Threshold Recalibration**\n\nThe final step is to compute a new stopping threshold $\\widehat{\\tau}$ for a black-box solver. We are given the goal of ensuring the final residual vector $r$ satisfies $\\|r\\|_2 \\le \\varepsilon_2$. The norm equivalence inequality $\\|r\\|_{?} \\ge c_1 \\|r\\|_2$ implies $\\|r\\|_2 \\le \\frac{1}{c_1} \\|r\\|_{?}$. To satisfy $\\|r\\|_2 \\le \\varepsilon_2$, it is sufficient to enforce $\\frac{1}{c_1} \\|r\\|_{?} \\le \\varepsilon_2$, which is equivalent to $\\|r\\|_{?} \\le c_1 \\varepsilon_2$. The solver stops when $\\|r\\|_{?} \\le \\tau$, so choosing $\\tau \\le c_1 \\varepsilon_2$ provides a safe stopping condition.\n\nThe problem specifies computing the recalibrated threshold as $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$. This is a valid and safe procedure provided that our estimate $\\widehat{c}_1$ is equal to or a lower bound on the true constant $c_1$. For the inner-product norm case, our reconstruction technique is exact up to floating-point precision, so $\\widehat{c}_1 \\approx c_1$. For the general norm cases in the test suite, our deterministic sampling strategy ensures $\\widehat{c}_1 = c_1$. In a more general setting where sampling might only yield an upper bound $\\widehat{c}_1 \\ge c_1$, the rule $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$ would not be guaranteed to be safe. However, for the scope and requirements of this problem, the procedure is sound.",
            "answer": "```python\nimport numpy as np\n\ndef test_parallelogram_law(oracle, n, num_samples=100, tolerance=1e-9):\n    \"\"\"\n    Tests if a norm satisfies the parallelogram law within a given tolerance.\n\n    A norm ||.|| is induced by an inner product if and only if\n    ||x+y||^2 + ||x-y||^2 = 2(||x||^2 + ||y||^2) for all x, y.\n\n    Args:\n        oracle (callable): The black-box norm function.\n        n (int): The dimension of the vector space.\n        num_samples (int): The number of random vector pairs to test.\n        tolerance (float): The relative error tolerance for the check.\n\n    Returns:\n        bool: True if the law holds for all samples, False otherwise.\n    \"\"\"\n    # Fix the random seed for reproducibility\n    rng = np.random.default_rng(seed=42)\n    max_relative_error = 0.0\n\n    for _ in range(num_samples):\n        x = rng.standard_normal(n)\n        y = rng.standard_normal(n)\n\n        # Handle the case where x and y are zero vectors\n        norm_x_sq = oracle(x)**2\n        norm_y_sq = oracle(y)**2\n        if norm_x_sq == 0 and norm_y_sq == 0:\n            continue\n\n        lhs = oracle(x + y)**2 + oracle(x - y)**2\n        rhs = 2 * (norm_x_sq + norm_y_sq)\n\n        if rhs == 0: # This case should not be hit if x or y is non-zero\n            relative_error = 0.0 if lhs == 0 else np.inf\n        else:\n            relative_error = np.abs(lhs - rhs) / rhs\n        \n        if relative_error > max_relative_error:\n            max_relative_error = relative_error\n\n    return max_relative_error  tolerance\n\ndef estimate_from_gram_matrix(oracle, n):\n    \"\"\"\n    Estimates c1 and c2 for an inner-product-induced norm.\n\n    This is done by reconstructing the Gram matrix G using the polarization\n    identity and finding the square roots of its min and max eigenvalues.\n\n    Args:\n        oracle (callable): The black-box norm function.\n        n (int): The dimension of the vector space.\n\n    Returns:\n        tuple[float, float]: The estimated constants (c1_hat, c2_hat).\n    \"\"\"\n    G = np.zeros((n, n))\n    I = np.eye(n)\n    \n    for i in range(n):\n        for j in range(i, n):\n            ei, ej = I[:, i], I[:, j]\n            val = 0.25 * (oracle(ei + ej)**2 - oracle(ei - ej)**2)\n            G[i, j] = val\n            if i != j:\n                G[j, i] = val\n\n    # Eigenvalues of a real symmetric matrix are real.\n    # eigvalsh is efficient for symmetric matrices.\n    eigenvalues = np.linalg.eigvalsh(G)\n    \n    lambda_min = np.min(eigenvalues)\n    lambda_max = np.max(eigenvalues)\n\n    # The eigenvalues of an SPD matrix must be positive.\n    # Add a small clip for numerical stability near zero.\n    c1_hat = np.sqrt(np.maximum(0, lambda_min))\n    c2_hat = np.sqrt(np.maximum(0, lambda_max))\n    \n    return c1_hat, c2_hat\n\ndef estimate_from_sampling(oracle, n, num_random_samples=1000):\n    \"\"\"\n    Estimates c1 and c2 for a general norm by sampling the unit sphere.\n\n    The sample set includes deterministic directions that are extremizers for\n    l1 and l_inf norms, plus random directions.\n\n    Args:\n        oracle (callable): The black-box norm function.\n        n (int): The dimension of the vector space.\n        num_random_samples (int): The number of random directions to sample.\n\n    Returns:\n        tuple[float, float]: The estimated constants (c1_hat, c2_hat).\n    \"\"\"\n    # 1. Deterministic sample set\n    samples = []\n    \n    # Standard basis vectors and their negatives\n    I = np.eye(n)\n    for i in range(n):\n        samples.append(I[:, i])\n        samples.append(-I[:, i])\n\n    # Uniform direction vector and its negative\n    if n > 0:\n        uniform_vec = np.ones(n) / np.sqrt(n)\n        samples.append(uniform_vec)\n        samples.append(-uniform_vec)\n\n    # 2. Random sample set\n    rng = np.random.default_rng(seed=42)\n    random_vectors = rng.standard_normal((num_random_samples, n))\n    norms = np.linalg.norm(random_vectors, axis=1, keepdims=True)\n    # Avoid division by zero if a zero vector is somehow generated\n    non_zero_norms = np.where(norms == 0, 1, norms)\n    normalized_random_vectors = random_vectors / non_zero_norms\n    \n    for vec in normalized_random_vectors:\n        samples.append(vec)\n        \n    # All vectors in samples have ||x||_2 = 1\n    # The value of ||x||_? is the ratio ||x||_? / ||x||_2\n    norm_values = [oracle(x) for x in samples]\n    \n    c1_hat = np.min(norm_values)\n    c2_hat = np.max(norm_values)\n    \n    return c1_hat, c2_hat\n\n\ndef solve():\n    \"\"\"\n    Main solver function to run the detection experiment on all test cases.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {\n            \"n\": 3,\n            \"oracle\": lambda x: np.sqrt(x.T @ np.diag([1, 4, 9]) @ x),\n            \"eps2\": 1e-4,\n            \"c1_true\": 1.0,\n            \"c2_true\": 3.0,\n        },\n        # Case B\n        {\n            \"n\": 4,\n            \"oracle\": lambda x: np.linalg.norm(x, ord=1),\n            \"eps2\": 1e-3,\n            \"c1_true\": 1.0,\n            \"c2_true\": 2.0, # sqrt(4)\n        },\n        # Case C\n        {\n            \"n\": 5,\n            \"oracle\": lambda x: np.linalg.norm(x, ord=np.inf),\n            \"eps2\": 2e-6,\n            \"c1_true\": 1 / np.sqrt(5),\n            \"c2_true\": 1.0,\n        },\n        # Case D\n        {\n            \"n\": 3,\n            \"oracle\": lambda x: np.sqrt(x.T @ np.diag([1e-4, 1, 100]) @ x),\n            \"eps2\": 1e-5,\n            \"c1_true\": 0.01,\n            \"c2_true\": 10.0,\n        },\n        # Case E\n        {\n            \"n\": 1,\n            \"oracle\": lambda x: 7 * np.linalg.norm(x, ord=2),\n            \"eps2\": 3e-5,\n            \"c1_true\": 7.0,\n            \"c2_true\": 7.0,\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        n = case[\"n\"]\n        oracle = case[\"oracle\"]\n        eps2 = case[\"eps2\"]\n        c1_true = case[\"c1_true\"]\n        c2_true = case[\"c2_true\"]\n        \n        is_inner_product = test_parallelogram_law(oracle, n)\n        \n        if is_inner_product:\n            c1_hat, c2_hat = estimate_from_gram_matrix(oracle, n)\n        else:\n            # For general norms, the number of random samples is chosen\n            # to be moderate as per the problem. \n            # 100*n is more than sufficient.\n            c1_hat, c2_hat = estimate_from_sampling(oracle, n, num_random_samples=100*n)\n            \n        tau_hat = c1_hat * eps2\n        \n        all_results.append([c1_hat, c2_hat, tau_hat, c1_true, c2_true])\n        \n    # Format the output as a string representing a list of lists.\n    # Python's str() on a list gives a square-bracketed representation,\n    # so we just join these strings with commas.\n    # The default str() adds spaces, which is forbidden.\n    result_strings = []\n    for res_list in all_results:\n        # Format each sublist like [val1,val2,val3,val4,val5]\n        sublist_str = f\"[{','.join(map(str, res_list))}]\"\n        result_strings.append(sublist_str)\n    \n    # Join all sublist strings into the final format.\n    final_output = f\"[{','.join(result_strings)}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}