{
    "hands_on_practices": [
        {
            "introduction": "在我们能够有效控制误差放大之前，必须首先理解其根源。条件数 $\\kappa(A)$ 是衡量线性系统问题敏感性的核心指标。本练习提供了一个分析性的实践，通过剖析一个参数化的矩阵族，您将亲手推导并观察一个矩阵的结构（特别是当它接近奇异时）如何导致其条件数急剧增大，从而为巨大的前向误差埋下伏笔。",
            "id": "3547254",
            "problem": "考虑为 $\\epsilon>0$ 定义的参数化实对称矩阵族 $A(\\epsilon) \\in \\mathbb{R}^{2 \\times 2}$：\n$$\nA(\\epsilon)=\\begin{pmatrix}1 & 1 \\\\ 1 & 1+\\epsilon\\end{pmatrix}.\n$$\n从数值线性代数的基本定义出发：\n\n- 矩阵 $2$-范数 $\\|A\\|_{2}$ 是由欧几里得向量范数导出的算子范数。\n- $2$-范数条件数是 $\\kappa_{2}(A)=\\|A\\|_{2}\\,\\|A^{-1}\\|_{2}$，前提是 $A$ 非奇异。\n\n任务：\n\n(a) 仅使用这些定义和实对称矩阵的谱性质，推导 $\\kappa_{2}\\!\\left(A(\\epsilon)\\right)$ 作为 $\\epsilon$ 函数的闭式表达式。\n\n(b) 确定极限\n$$\nL \\;=\\; \\lim_{\\epsilon\\to 0^{+}} \\,\\epsilon \\,\\kappa_{2}\\!\\left(A(\\epsilon)\\right).\n$$\n此极限 $L$ 是您必须报告为最终答案的量。\n\n(c) 为了说明小 $\\epsilon$ 的前向误差放大效应，考虑求解线性系统 $A(\\epsilon)\\,x=b(\\epsilon)$，其精确解为 $x^{\\star}=\\begin{pmatrix}1\\\\ 1\\end{pmatrix}$，对应的右端项为 $b(\\epsilon)=A(\\epsilon)\\,x^{\\star}=\\begin{pmatrix}2\\\\ 2+\\epsilon\\end{pmatrix}$。假设一个计算解 $\\widetilde{x}$ 满足 $A(\\epsilon)\\,\\widetilde{x}=b(\\epsilon)+\\delta b$，其中存在某个扰动 $\\delta b$ 使得 $\\|\\delta b\\|_{2}/\\|b(\\epsilon)\\|_{2}=\\eta$，且 $0  \\eta \\ll 1$ 独立于 $\\epsilon$。使用从第一性原理推导的扰动恒等式，证明当 $\\epsilon\\to 0^{+}$ 时，最坏情况下的相对前向误差的尺度为 $O\\!\\left(\\epsilon^{-1}\\right)$，并确定一个能渐近实现此尺度的扰动方向 $\\delta b$。本问题中任何地方都不需要数值舍入。对于(b)部分，请将 $L$ 报告为精确值。",
            "solution": "该问题陈述经证实具有科学依据、问题明确且客观。这是一个关于矩阵条件数和误差分析的数值线性代数标准问题。所有数据和定义都是自洽和一致的。\n\n(a) 条件数 $\\kappa_{2}(A(\\epsilon))$ 的推导。\n\n矩阵 $A(\\epsilon)$ 如下所示\n$$\nA(\\epsilon)=\\begin{pmatrix}1  1 \\\\ 1  1+\\epsilon\\end{pmatrix}.\n$$\n对于实对称矩阵，矩阵 $2$-范数 $\\|A\\|_{2}$ 等于其谱半径，即其特征值绝对值的最大值。令 $\\lambda$ 表示 $A(\\epsilon)$ 的一个特征值。这些特征值是特征方程 $\\det(A(\\epsilon) - \\lambda I) = 0$ 的根。\n$$\n\\det\\begin{pmatrix}1-\\lambda  1 \\\\ 1  1+\\epsilon-\\lambda\\end{pmatrix} = (1-\\lambda)(1+\\epsilon-\\lambda) - 1 = 0\n$$\n$$\n\\lambda^2 - (2+\\epsilon)\\lambda + \\epsilon = 0\n$$\n使用二次公式，特征值为：\n$$\n\\lambda = \\frac{(2+\\epsilon) \\pm \\sqrt{(2+\\epsilon)^2 - 4\\epsilon}}{2} = \\frac{2+\\epsilon \\pm \\sqrt{4+4\\epsilon+\\epsilon^2 - 4\\epsilon}}{2} = \\frac{2+\\epsilon \\pm \\sqrt{4+\\epsilon^2}}{2}\n$$\n对于 $\\epsilon  0$， $A(\\epsilon)$ 的迹为 $2+\\epsilon  0$，其行列式为 $\\det(A(\\epsilon)) = 1(1+\\epsilon) - 1(1) = \\epsilon  0$。这意味着 $A(\\epsilon)$ 是正定矩阵，因此两个特征值都是正数。我们用 $\\lambda_{max}$ 和 $\\lambda_{min}$ 表示它们：\n$$\n\\lambda_{max}(\\epsilon) = \\frac{2+\\epsilon + \\sqrt{4+\\epsilon^2}}{2}\n$$\n$$\n\\lambda_{min}(\\epsilon) = \\frac{2+\\epsilon - \\sqrt{4+\\epsilon^2}}{2}\n$$\n因此，$A(\\epsilon)$ 的 $2$-范数是 $\\|A(\\epsilon)\\|_2 = \\lambda_{max}(\\epsilon)$。\n\n当 $\\epsilon  0$ 时，矩阵 $A(\\epsilon)$ 是非奇异的。逆矩阵 $A(\\epsilon)^{-1}$ 的特征值是 $A(\\epsilon)$ 特征值的倒数，即 $1/\\lambda_{max}(\\epsilon)$ 和 $1/\\lambda_{min}(\\epsilon)$。因为两个特征值都是正数且 $\\lambda_{max}(\\epsilon)  \\lambda_{min}(\\epsilon)$，所以我们有 $1/\\lambda_{min}(\\epsilon)  1/\\lambda_{max}(\\epsilon)$。逆矩阵的 $2$-范数是\n$$\n\\|A(\\epsilon)^{-1}\\|_2 = \\frac{1}{\\lambda_{min}(\\epsilon)}.\n$$\n$2$-范数条件数 $\\kappa_2(A(\\epsilon))$ 定义为 $\\|A(\\epsilon)\\|_2 \\|A(\\epsilon)^{-1}\\|_2$。\n$$\n\\kappa_{2}(A(\\epsilon)) = \\frac{\\lambda_{max}(\\epsilon)}{\\lambda_{min}(\\epsilon)} = \\frac{\\frac{2+\\epsilon + \\sqrt{4+\\epsilon^2}}{2}}{\\frac{2+\\epsilon - \\sqrt{4+\\epsilon^2}}{2}}\n$$\n条件数作为 $\\epsilon$ 函数的闭式表达式为：\n$$\n\\kappa_{2}(A(\\epsilon)) = \\frac{2+\\epsilon + \\sqrt{4+\\epsilon^2}}{2+\\epsilon - \\sqrt{4+\\epsilon^2}}\n$$\n\n(b) 极限 $L$ 的确定。\n\n我们需要计算极限 $L = \\lim_{\\epsilon\\to 0^{+}} \\epsilon \\kappa_{2}(A(\\epsilon))$。特征值的一个有用性质是它们的乘积等于矩阵的行列式：$\\lambda_{max}(\\epsilon)\\lambda_{min}(\\epsilon) = \\det(A(\\epsilon)) = \\epsilon$。\n利用这一点，我们可以将条件数重写为：\n$$\n\\kappa_{2}(A(\\epsilon)) = \\frac{\\lambda_{max}(\\epsilon)}{\\lambda_{min}(\\epsilon)} = \\frac{\\lambda_{max}(\\epsilon)}{\\epsilon / \\lambda_{max}(\\epsilon)} = \\frac{(\\lambda_{max}(\\epsilon))^2}{\\epsilon}\n$$\n现在，我们可以计算极限 $L$：\n$$\nL = \\lim_{\\epsilon\\to 0^{+}} \\epsilon \\kappa_{2}(A(\\epsilon)) = \\lim_{\\epsilon\\to 0^{+}} \\epsilon \\left(\\frac{(\\lambda_{max}(\\epsilon))^2}{\\epsilon}\\right) = \\lim_{\\epsilon\\to 0^{+}} (\\lambda_{max}(\\epsilon))^2\n$$\n我们计算当 $\\epsilon \\to 0^{+}$ 时 $\\lambda_{max}(\\epsilon)$ 的极限：\n$$\n\\lim_{\\epsilon\\to 0^{+}} \\lambda_{max}(\\epsilon) = \\lim_{\\epsilon\\to 0^{+}} \\frac{2+\\epsilon + \\sqrt{4+\\epsilon^2}}{2} = \\frac{2+0+\\sqrt{4+0}}{2} = \\frac{2+2}{2} = 2\n$$\n因此，极限 $L$ 是：\n$$\nL = (2)^2 = 4\n$$\n\n(c) 前向误差放大分析。\n\n我们有精确系统 $A(\\epsilon)x^{\\star} = b(\\epsilon)$ 和扰动系统 $A(\\epsilon)\\widetilde{x} = b(\\epsilon) + \\delta b$。解的误差为 $\\delta x = \\widetilde{x} - x^{\\star}$。从第二个方程中减去第一个方程得到：\n$$\nA(\\epsilon)(\\widetilde{x} - x^{\\star}) = \\delta b \\implies A(\\epsilon)\\delta x = \\delta b\n$$\n因为 $A(\\epsilon)$ 是非奇异的，所以 $\\delta x = A(\\epsilon)^{-1}\\delta b$。取范数，我们得到 $\\|\\delta x\\|_2 = \\|A(\\epsilon)^{-1}\\delta b\\|_2 \\le \\|A(\\epsilon)^{-1}\\|_2 \\|\\delta b\\|_2$。\n相对前向误差的界如下：\n$$\n\\frac{\\|\\delta x\\|_2}{\\|x^{\\star}\\|_2} \\le \\frac{\\|A(\\epsilon)^{-1}\\|_2 \\|\\delta b\\|_2}{\\|x^{\\star}\\|_2}\n$$\n我们可以引入 $\\|A(\\epsilon)\\|_2$ 和 $\\|b(\\epsilon)\\|_2 = \\|A(\\epsilon)x^{\\star}\\|_2$ 来将其与条件数联系起来。注意 $\\|b(\\epsilon)\\|_2 \\le \\|A(\\epsilon)\\|_2 \\|x^{\\star}\\|_2$。\n$$\n\\frac{\\|\\delta x\\|_2}{\\|x^{\\star}\\|_2} \\le \\|A(\\epsilon)^{-1}\\|_2 \\|A(\\epsilon)\\|_2 \\frac{\\|\\delta b\\|_2}{\\|A(\\epsilon)x^{\\star}\\|_2} = \\kappa_2(A(\\epsilon)) \\frac{\\|\\delta b\\|_2}{\\|b(\\epsilon)\\|_2}\n$$\n已知 $\\|\\delta b\\|_{2}/\\|b(\\epsilon)\\|_{2}=\\eta$，界变为：\n$$\n\\frac{\\|\\delta x\\|_2}{\\|x^{\\star}\\|_2} \\le \\kappa_2(A(\\epsilon)) \\eta\n$$\n从(b)部分，我们得出 $\\lim_{\\epsilon\\to 0^{+}} \\epsilon \\kappa_{2}(A(\\epsilon)) = 4$，这意味着对于小的 $\\epsilon  0$，$\\kappa_2(A(\\epsilon)) \\approx 4/\\epsilon$。因此，相对前向误差的界的尺度为：\n$$\n\\frac{\\|\\delta x\\|_2}{\\|x^{\\star}\\|_2} \\le O(\\epsilon^{-1})\n$$\n这证明了 $O(\\epsilon^{-1})$ 尺度的合理性。\n\n为了确定一个能渐近实现此尺度的扰动方向 $\\delta b$，我们必须找到一个 $\\delta b$ 使得不等式 $\\|\\delta x\\|_2 \\le \\|A(\\epsilon)^{-1}\\|_2 \\|\\delta b\\|_2$ 变为等式。对于对称矩阵，当 $\\delta b$ 是 $A(\\epsilon)^{-1}$ 对应其最大特征值（即 $\\|A(\\epsilon)^{-1}\\|_2 = 1/\\lambda_{min}(\\epsilon)$）的特征向量时，这种情况发生。这个特征向量与 $A(\\epsilon)$ 对应于特征值 $\\lambda_{min}(\\epsilon)$ 的特征向量相同。设这个特征向量为 $v_{min}$。我们求解 $(A(\\epsilon) - \\lambda_{min}(\\epsilon)I)v_{min} = 0$。\n$$\n\\begin{pmatrix} 1-\\lambda_{min}(\\epsilon)  1 \\\\ 1  1+\\epsilon-\\lambda_{min}(\\epsilon) \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n第一行给出 $(1-\\lambda_{min}(\\epsilon))v_1 + v_2 = 0$。我们可以设 $v_1 = 1$，得到 $v_2 = \\lambda_{min}(\\epsilon) - 1$。\n所以，一个特征向量是 $v_{min}(\\epsilon) = \\begin{pmatrix} 1 \\\\ \\lambda_{min}(\\epsilon) - 1 \\end{pmatrix}$。\n我们分析当 $\\epsilon \\to 0^{+}$ 时这个向量的行为。\n$$\n\\lim_{\\epsilon\\to 0^{+}} \\lambda_{min}(\\epsilon) = \\lim_{\\epsilon\\to 0^{+}} \\frac{2+\\epsilon - \\sqrt{4+\\epsilon^2}}{2} = \\frac{2+0-\\sqrt{4}}{2} = 0\n$$\n因此，分量 $v_2$ 趋近于 $0-1 = -1$。因此，特征向量 $v_{min}(\\epsilon)$ 的渐近方向是\n$$\n\\lim_{\\epsilon\\to 0^{+}} v_{min}(\\epsilon) \\propto \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n在 $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ 方向上的扰动 $\\delta b$，对于足够小的 $\\epsilon$，将与特征向量 $v_{min}(\\epsilon)$ 紧密对齐，导致前向误差被最大程度地放大，并实现 $O(\\epsilon^{-1})$ 尺度。\n当 $\\delta b$ 选在 $v_{min}(\\epsilon)$ 的方向上时，我们有等式：$\\frac{\\|\\delta x\\|_2}{\\|x^{\\star}\\|_2} = \\eta \\frac{\\|b(\\epsilon)\\|_2}{\\|x^{\\star}\\|_2} \\frac{\\|A(\\epsilon)\\|_2}{\\|b(\\epsilon)\\|_2} \\kappa_2(A(\\epsilon))$。当 $\\epsilon \\to 0^+$ 时，$\\|x^\\star\\|_2 = \\sqrt{2}$，$b(\\epsilon) \\to \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$，所以 $\\|b(\\epsilon)\\|_2 \\to \\sqrt{8} = 2\\sqrt{2}$。此外，$\\|A(\\epsilon)\\|_2 = \\lambda_{max}(\\epsilon) \\to 2$。因子 $\\frac{\\|b(\\epsilon)\\|_2}{\\|x^{\\star}\\|_2} \\frac{\\|A(\\epsilon)\\|_2}{\\|b(\\epsilon)\\|_2}$ 变为 $\\frac{2}{\\sqrt{8}} \\frac{2\\sqrt{2}}{\\sqrt{2}} = 1$。该表达式趋近于 $\\eta \\kappa_2(A(\\epsilon))$，这证实了该尺度。所需的方向是渐近地指向向量 $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ 的方向。",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "理论的理解必须转化为实践的智慧。在求解线性系统 $Ax=b$ 时，是先计算 $A^{-1}$ 再乘以 $b$ 更好，还是直接使用求解器？本练习对这两种方法进行了清晰且定量的比较，通过在有限精度算术中处理病态系统，鲜明地展示了为何直接、后向稳定的方法在根本上更为优越。",
            "id": "3547212",
            "problem": "考虑一个线性系统 $A x = b$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 且 $b \\in \\mathbb{R}^{n}$。令 $x$ 表示在精确算术下的精确解，令 $\\hat{x}$ 表示由某种算法在浮点算术中产生的计算解。前向误差是计算解中的相对误差，定义为 $\\frac{\\lVert \\hat{x} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}$，其中 $\\lVert \\cdot \\rVert_{2}$ 是向量的欧几里得范数。后向误差是相对残差，定义为 $\\frac{\\lVert b - A \\hat{x} \\rVert_{2}}{\\lVert A \\rVert_{2} \\lVert \\hat{x} \\rVert_{2}}$，其中对于矩阵 $A$，$\\lVert A \\rVert_{2}$ 表示由向量 2-范数诱导的算子 2-范数。条件数 $\\kappa_{2}(A)$ 定义为 $A$ 的最大奇异值与最小奇异值之比，它量化了解对扰动的敏感性。\n\n浮点运算的一个基本舍入模型假设，对于 $\\circ \\in \\{+, -, \\times, \\div\\}$，计算出的基本运算 $\\operatorname{fl}(x \\circ y)$ 满足 $\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\delta)$，其中某个 $\\delta$ 满足 $\\lvert \\delta \\rvert \\leq u$，$u$ 是单位舍入误差。在此模型下，对于后向稳定的算法（如带部分主元的高斯消元法），后向误差通常很小，但前向误差仍可能被问题的条件数放大。显式构造逆矩阵 $A^{-1}$ 然后计算 $x = A^{-1} b$ 的方法，已知劣于通过后向稳定求解器直接求解 $A x = b$ 的方法，特别是对于病态矩阵，因为映射 $A \\mapsto A^{-1}$ 本身是高度敏感的，并且在与 $b$ 相乘之前就可能放大了舍入误差。\n\n你的任务是实现一个程序，在一套旨在探测良态、中度病态、近奇异和经典病态情况的测试矩阵上，对求解 $A x = b$ 的两种计算方法进行原则性比较：\n\n- 方法 $\\text{inv32}$：以单精度形式构造 $\\hat{A}^{-1}$（将 $A$ 舍入为单精度并在单精度下求逆），然后返回 $\\hat{x}_{\\text{inv32}} = \\hat{A}^{-1} b$（一旦 $\\hat{A}^{-1}$ 形成，该乘法可以在双精度下执行）。\n- 方法 $\\text{solve64}$：使用后向稳定的求解器在双精度下直接求解 $A x = b$。\n\n为了能够精确计算前向误差，你必须通过在有理数域上执行带行主元的高斯消元法来使用精确有理数算术获得 $x$，以便在转换为浮点数进行误差评估之前，$x$ 被精确计算为有理数向量。使用以下测试套件，其中 $A$ 和 $b$ 的所有元素首先必须被视为精确有理数，以获得精确解：\n\n1.  良态对称正定情况：\n    $$A_{1} = \\begin{bmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{bmatrix}, \\quad b_{1} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}.$$\n2.  节点为 $1, 2, 3, 4$ 的中度病态范德蒙情况：\n    $$A_{2}(i,j) = i^{j-1} \\text{ for } i \\in \\{1,2,3,4\\}, \\ j \\in \\{1,2,3,4\\}, \\quad b_{2} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}.$$\n3.  近奇异的 $2 \\times 2$ 情况，带有有理数扰动 $\\varepsilon = 10^{-12}$：\n    $$A_{3} = \\begin{bmatrix} 1  1 \\\\ 1  1 + \\varepsilon \\end{bmatrix}, \\quad b_{3} = \\begin{bmatrix} 2 \\\\ 2 + \\varepsilon \\end{bmatrix}, \\quad \\varepsilon = \\frac{1}{10^{12}}.$$\n    这个选择得到的精确解是 $x = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$，即使 $A_{3}$ 近奇异，该解也与一个良态方向对齐。\n4.  经典的 $6$ 维病态希尔伯特情况：\n    $$A_{4}(i,j) = \\frac{1}{i + j - 1} \\text{ for } i,j \\in \\{1,2,3,4,5,6\\}, \\quad b_{4} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}.$$\n\n对于每个测试用例，执行以下步骤：\n\n-   使用带部分主元的高斯消元法在增广系统 $[A \\mid b]$ 上计算精确解 $x$，其中所有算术运算都是在有理数上精确执行的。\n-   通过将 $A$ 转换为单精度，在单精度下构造其逆矩阵，然后乘以 $b$（结果转换为双精度进行评估）来计算 $\\hat{x}_{\\text{inv32}}$。\n-   通过在双精度下直接求解 $A x = b$ 来计算 $\\hat{x}_{\\text{solve64}}$。\n-   计算每种方法的前向误差 $\\frac{\\lVert \\hat{x}_{\\text{inv32}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}$ 和 $\\frac{\\lVert \\hat{x}_{\\text{solve64}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}$。\n-   计算后向误差 $\\frac{\\lVert b - A \\hat{x}_{\\text{inv32}} \\rVert_{2}}{\\lVert A \\rVert_{2} \\lVert \\hat{x}_{\\text{inv32}} \\rVert_{2}}$ 和 $\\frac{\\lVert b - A \\hat{x}_{\\text{solve64}} \\rVert_{2}}{\\lVert A \\rVert_{2} \\lVert \\hat{x}_{\\text{solve64}} \\rVert_{2}}$ 以供参考（这些不必包含在最终输出中，但必须计算以确保比较的科学真实性）。\n-   将基于逆矩阵的方法相对于直接求解方法的观测前向误差放大因子定义为\n    $$\\rho = \\frac{\\frac{\\lVert \\hat{x}_{\\text{inv32}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}}{\\frac{\\lVert \\hat{x}_{\\text{solve64}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}}.$$\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，按上述顺序为每个测试用例提供一个浮点数，分别表示情况 1 到 4 的 $\\rho$ 值。例如，最终的输出格式必须严格如下：\n$$[\\rho_{1},\\rho_{2},\\rho_{3},\\rho_{4}].$$\n不应打印任何额外的文本。不涉及角度。不涉及物理单位。所有计算必须使用向量 2-范数和矩阵算子 2-范数（谱范数）。程序必须是完全自包含的：它必须定义测试套件，计算精确的有理数解，执行数值计算，并打印所需的列表。",
            "solution": "任务是比较求解 $A x = b$ 的两种方法的前向误差和后向误差：显式构造 $A^{-1}$ 并乘以 $b$ 与使用后向稳定求解器直接求解系统。比较应基于原则和定义。\n\n我们从基本定义开始。对于一个计算解 $\\hat{x}$，前向误差是 $\\frac{\\lVert \\hat{x} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}$，其中 $x$ 是精确解。作为残差质量的度量，后向误差是 $\\frac{\\lVert b - A \\hat{x} \\rVert_{2}}{\\lVert A \\rVert_{2} \\lVert \\hat{x} \\rVert_{2}}$。算子 2-范数 $\\lVert A \\rVert_{2}$ 是 $A$ 的最大奇异值，条件数 $\\kappa_{2}(A)$ 是最大奇异值与最小奇异值之比。奇异值分解 (SVD) 将 $A$ 表示为 $A = U \\Sigma V^{\\top}$，其中 $U$ 和 $V$ 是正交的，$\\Sigma$ 是包含奇异值 $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{n}  0$ 的对角矩阵。那么 $\\lVert A \\rVert_{2} = \\sigma_{1}$，最小奇异值是 $\\sigma_{n}$。\n\n后向稳定的算法产生一个 $\\hat{x}$，使得 $A \\hat{x}$ 等于某个邻近矩阵 $\\tilde{A} = A + \\Delta A$ 的 $b$，其中 $\\frac{\\lVert \\Delta A \\rVert_{2}}{\\lVert A \\rVert_{2}}$ 很小（在单位舍入误差的量级上）。在这个模型下，前向误差和后向误差通过条件数相关联，通常遵循形如 $\\frac{\\lVert \\hat{x} - x \\rVert_{2}}{\\lVert x \\rVert_{2}} \\lesssim \\kappa_{2}(A) \\cdot \\text{(后向误差)}$ 的界限，对于适定问题。显式计算 $A^{-1}$ 的操作往往比直接求解 $A x = b$ 更差，因为它首先计算一个对象 $A^{-1}$，其本身的相对误差被 $\\kappa_{2}(A)$ 放大，随后再乘以 $b$，从而加剧了误差放大，并可能引入沿与小奇异值相关联方向的分量。\n\n为了进行科学上合理且可测试的比较，我们必须对照精确解 $x$ 来测量前向误差。由于浮点运算不能直接提供精确值，我们在精确有理数算术中构造 $x$。这可以通过在有理数域上执行高斯消元来实现：每个算术运算都用有理数完成，确保了对于非奇异矩阵 $A$ 的精确解。具体来说，我们用有理数项构建增广矩阵 $[A \\mid b]$，执行行主元选择以避免零主元，并应用消元法将其化为简化行阶梯形。得到的右侧列即为有理数形式的精确解 $x$，然后可以将其转换为实数以进行范数计算。\n\n每个测试用例的算法步骤：\n\n1.  将 $A$ 和 $b$ 构造为有理数。对于整数项，它们是分母为 $1$ 的分数。对于希尔伯特矩阵，项是精确可表示的有理数 $\\frac{1}{i + j - 1}$。对于近奇异情况，我们设置 $\\varepsilon = \\frac{1}{10^{12}}$。\n2.  在 $[A \\mid b]$ 上执行带行主元选择的精确高斯消元法：\n    -   对于 $i = 1, \\dots, n$，在第 $i$ 列中选择一个主元行 $p \\geq i$，使其具有最大绝对值，以避免除以小数；交换第 $i$ 行和第 $p$ 行。\n    -   通过除以主元元素来归一化第 $i$ 行。\n    -   使用精确算术将所有其他行中第 $i$ 列的项消为零：对于第 $k$ 行，减去 $\\text{因子} \\times \\text{第 } i \\text{ 行}$。\n    -   消元完成后，解向量的项就是最后一列的值。\n3.  将有理数精确解转换为浮点数以进行范数计算。\n4.  计算 $\\hat{x}_{\\text{inv32}}$：\n    -   将 $A$ 转换为单精度，即把每个元素舍入为 32 位浮点数。\n    -   在单精度下计算逆矩阵：$\\hat{A}^{-1}$。\n    -   将 $\\hat{A}^{-1}$ 乘以 $b$（在将逆矩阵转换回双精度后，乘法可以在双精度下进行）。\n5.  使用后向稳定的直接求解方法在双精度下计算 $\\hat{x}_{\\text{solve64}}$。\n6.  计算两种方法的前向误差：$\\frac{\\lVert \\hat{x}_{\\text{inv32}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}$ 和 $\\frac{\\lVert \\hat{x}_{\\text{solve64}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}$。\n7.  计算两种方法的后向误差：$\\frac{\\lVert b - A \\hat{x}_{\\text{inv32}} \\rVert_{2}}{\\lVert A \\rVert_{2} \\lVert \\hat{x}_{\\text{inv32}} \\rVert_{2}}$ 和 $\\frac{\\lVert b - A \\hat{x}_{\\text{solve64}} \\rVert_{2}}{\\lVert A \\rVert_{2} \\lVert \\hat{x}_{\\text{solve64}} \\rVert_{2}}$。这些量提供了背景信息：即使对于病态问题，前向误差被放大，直接求解的后向误差也应该很小，而显式构造逆矩阵可能会急剧增加前向误差。\n8.  报告基于逆矩阵的方法相对于直接求解方法的观测前向误差放大因子：\n    $$\\rho = \\frac{\\frac{\\lVert \\hat{x}_{\\text{inv32}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}}{\\frac{\\lVert \\hat{x}_{\\text{solve64}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}}.$$\n\n解释：\n-   对于一个良态的 $A$，两种方法都应该有小的前向误差，$\\rho$ 应该接近 $1$，尽管单精度求逆仍可能使 $\\rho$ 比 $1$ 稍大。\n-   对于中度病态的 $A$，随着求逆的敏感性增加了对舍入的脆弱性，$\\rho$ 可能会增长。\n-   对于近奇异的情况，所选的 $b$ 产生一个与稳定方向对齐的精确解，因此尽管矩阵近奇异，直接求解的前向误差可以保持很小。然而，在低精度下显式构造 $A^{-1}$ 可能会引入沿不稳定方向的巨大分量，从而导致一个大的 $\\rho$。\n-   对于经典的病态希尔伯特情况，低精度的基于逆矩阵的方法预计会显示灾难性的误差放大，产生一个非常大的 $\\rho$，即使直接求解保持了很小的后向误差。\n\n最终输出必须是 $[\\rho_{1}, \\rho_{2}, \\rho_{3}, \\rho_{4}]$ 列表，并严格以这种单行、逗号分隔、方括号括起来的格式打印。这将为四种情况（包括正常路径（良态情况）、中度病态、近奇异边界情况以及严重的病态边缘情况）分别产生可量化的浮点数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom fractions import Fraction\n\ndef gaussian_elimination_exact(A_frac, b_frac):\n    \"\"\"\n    Solve A x = b exactly using Gaussian elimination over rationals.\n    A_frac: list of lists of Fraction, shape (n, n)\n    b_frac: list of Fraction, length n\n    Returns: list of Fraction x of length n\n    \"\"\"\n    n = len(A_frac)\n    # Build augmented matrix [A | b]\n    M = [row[:] + [b_frac[i]] for i, row in enumerate(A_frac)]\n    # Perform Gaussian elimination with partial pivoting\n    for i in range(n):\n        # Pivot selection: choose row with largest absolute value in column i\n        pivot_row = i\n        pivot_val = abs(M[i][i])\n        for r in range(i+1, n):\n            if abs(M[r][i]) > pivot_val:\n                pivot_val = abs(M[r][i])\n                pivot_row = r\n        if pivot_val == 0:\n            raise ValueError(\"Matrix is singular in exact arithmetic.\")\n        # Swap to put pivot_row at i\n        if pivot_row != i:\n            M[i], M[pivot_row] = M[pivot_row], M[i]\n        # Normalize pivot row\n        pivot = M[i][i]\n        # Divide entire row i by pivot\n        for j in range(i, n+1):\n            M[i][j] = M[i][j] / pivot\n        # Eliminate column i in all other rows\n        for k in range(n):\n            if k == i:\n                continue\n            factor = M[k][i]\n            if factor != 0:\n                for j in range(i, n+1):\n                    M[k][j] = M[k][j] - factor * M[i][j]\n    # After elimination, solution is in last column\n    x_frac = [M[i][n] for i in range(n)]\n    return x_frac\n\ndef to_float_matrix(A_frac):\n    \"\"\"Convert a Fraction matrix to a float64 numpy array.\"\"\"\n    n = len(A_frac)\n    m = len(A_frac[0])\n    A = np.zeros((n, m), dtype=np.float64)\n    for i in range(n):\n        for j in range(m):\n            A[i, j] = float(A_frac[i][j])\n    return A\n\ndef to_float_vector(b_frac):\n    \"\"\"Convert a Fraction vector to a float64 numpy array.\"\"\"\n    n = len(b_frac)\n    b = np.zeros(n, dtype=np.float64)\n    for i in range(n):\n        b[i] = float(b_frac[i])\n    return b\n\ndef build_test_cases():\n    cases = []\n    # Case 1: Well-conditioned SPD\n    A1 = [\n        [Fraction(4,1), Fraction(1,1), Fraction(0,1)],\n        [Fraction(1,1), Fraction(3,1), Fraction(1,1)],\n        [Fraction(0,1), Fraction(1,1), Fraction(2,1)],\n    ]\n    b1 = [Fraction(1,1), Fraction(2,1), Fraction(3,1)]\n    cases.append((A1, b1))\n    # Case 2: Vandermonde with nodes 1,2,3,4\n    nodes = [Fraction(1,1), Fraction(2,1), Fraction(3,1), Fraction(4,1)]\n    A2 = []\n    for i in range(4):\n        row = []\n        for j in range(4):\n            # i-th node to the power j\n            row.append(nodes[i] ** j)\n        A2.append(row)\n    b2 = [Fraction(1,1), Fraction(1,1), Fraction(1,1), Fraction(1,1)]\n    cases.append((A2, b2))\n    # Case 3: Nearly singular 2x2 with epsilon = 1/10^12\n    eps = Fraction(1, 10**12)\n    A3 = [\n        [Fraction(1,1), Fraction(1,1)],\n        [Fraction(1,1), Fraction(1,1) + eps],\n    ]\n    b3 = [Fraction(2,1), Fraction(2,1) + eps]\n    cases.append((A3, b3))\n    # Case 4: Hilbert 6x6\n    A4 = []\n    for i in range(1, 7):\n        row = []\n        for j in range(1, 7):\n            row.append(Fraction(1, i + j - 1))\n        A4.append(row)\n    b4 = [Fraction(1,1) for _ in range(6)]\n    cases.append((A4, b4))\n    return cases\n\ndef relative_forward_error(x_hat, x_exact):\n    num = np.linalg.norm(x_hat - x_exact, ord=2)\n    den = np.linalg.norm(x_exact, ord=2)\n    return num / den\n\ndef relative_backward_error(A, x_hat, b):\n    # spectral norm for A\n    A_norm2 = np.linalg.norm(A, ord=2)\n    r = b - A @ x_hat\n    r_norm = np.linalg.norm(r, ord=2)\n    xhat_norm = np.linalg.norm(x_hat, ord=2)\n    return r_norm / (A_norm2 * xhat_norm)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = build_test_cases()\n\n    results = []\n    for A_frac, b_frac in test_cases:\n        # Exact solution via rational elimination\n        x_exact_frac = gaussian_elimination_exact(A_frac, b_frac)\n        x_exact = to_float_vector(x_exact_frac)\n\n        # Convert A and b to float64 for numerical computation\n        A64 = to_float_matrix(A_frac)\n        b64 = to_float_vector(b_frac)\n\n        # Approach inv32: invert A in float32 and multiply by b\n        A32 = A64.astype(np.float32)\n        try:\n            Ainv32 = np.linalg.inv(A32)\n        except np.linalg.LinAlgError:\n            # If singular in float32, set Ainv32 to nan to propagate\n            Ainv32 = np.full_like(A32, np.nan, dtype=np.float32)\n        # Multiply in float64 after casting inverse back to float64\n        x_inv32 = Ainv32.astype(np.float64) @ b64\n\n        # Approach solve64: direct solve in float64\n        try:\n            x_solve64 = np.linalg.solve(A64, b64)\n        except np.linalg.LinAlgError:\n            # If singular (should not happen for given cases), set to nan\n            x_solve64 = np.full_like(b64, np.nan, dtype=np.float64)\n\n        # Compute forward errors\n        fwd_inv32 = relative_forward_error(x_inv32, x_exact)\n        fwd_solve64 = relative_forward_error(x_solve64, x_exact)\n\n        # Compute backward errors (not printed, but computed for scientific context)\n        bwd_inv32 = relative_backward_error(A64, x_inv32, b64)\n        bwd_solve64 = relative_backward_error(A64, x_solve64, b64)\n\n        # Observed forward-error magnification factor relative to direct solve\n        if fwd_solve64 == 0:\n            rho = float('inf') if fwd_inv32 > 0 else 1.0\n        else:\n            rho = fwd_inv32 / fwd_solve64\n\n        # Optionally, one could check or log backward errors to ensure they are small,\n        # but per instructions we only output rho values.\n        results.append(rho)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "数值分析中的一项关键技能不仅在于计算误差，更在于正确地解读误差。本练习探讨了一个至关重要的细微之处：范数后向误差（normwise backward error）与分量后向误差（componentwise backward error）之间的区别。您将构建一个情景，其中一个看似很小的范数后向误差掩盖了一个严重的问题，从而学会理解误差的衡量方式有时与误差本身同样重要。",
            "id": "3547220",
            "problem": "考虑一个线性系统 $A x = b$，其计算解为 $\\hat{x}$。令 $r = b - A \\hat{x}$ 表示残差。将范数形式的后向误差定义为\n$$\n\\beta_{\\mathrm{norm}} = \\frac{\\|r\\|_2}{\\|A\\|_2 \\,\\|\\hat{x}\\|_2 + \\|b\\|_2},\n$$\n其中，对于矩阵，$\\|\\cdot\\|_2$ 表示谱范数（由欧几里得范数诱导），对于向量，表示欧几里得范数。将分量形式的后向误差定义为\n$$\n\\beta_{\\mathrm{comp}} = \\max_{i} \\frac{|r_i|}{\\left(|A|\\,|\\hat{x}| + |b|\\right)_i},\n$$\n其中 $|\\cdot|$ 表示逐元素绝对值，分母是逐分量构成的。前向误差为\n$$\n\\epsilon_{\\mathrm{fwd}} = \\frac{\\|\\hat{x} - x\\|_2}{\\|x\\|_2},\n$$\n其中 $x$ 是当 $A$ 非奇异时 $A x = b$ 的精确解。误差放大通常由条件数控制\n$$\n\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)},\n$$\n其中 $\\sigma_{\\max}(A)$ 和 $\\sigma_{\\min}(A)$ 分别是 $A$ 的最大和最小奇异值。对于小扰动，一个标准的一阶扰动界给出，在 $\\kappa_2(A)\\,\\beta_{\\mathrm{norm}}  1$ 的条件下，\n$$\n\\epsilon_{\\mathrm{fwd}} \\;\\le\\; \\frac{\\kappa_2(A)\\,\\beta_{\\mathrm{norm}}}{1 - \\kappa_2(A)\\,\\beta_{\\mathrm{norm}}}.\n$$\n\n您的任务是实现这些定义，并为下面明确指定的测试套件计算这些量，然后将结果聚合为所需的输出格式。目标是提供一个具体的例子，其中 $\\beta_{\\mathrm{norm}}$ 很小而 $\\beta_{\\mathrm{comp}}$ 很大，并从数据缩放的角度分析其原因。\n\n使用以下三个测试用例，每个用例都指定了矩阵 $A$、右端项 $b$ 和计算解 $\\hat{x}$：\n\n- 测试用例 1（理想情况，数据缩放良好）：\n  - $A_1 = \\begin{bmatrix} 2  1  0 \\\\ 0  1  1 \\\\ 1  0  1 \\end{bmatrix}$，\n  - $x_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}$，\n  - $b_1 = A_1 x_1$，\n  - $\\hat{x}_1 = x_1$。\n\n- 测试用例 2（混合缩放数据导致差异）：\n  - $A_2 = \\begin{bmatrix} 1  2  0 \\\\ 0  1  -1 \\\\ 10^{-12}  0  0 \\end{bmatrix}$，\n  - $x_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$，\n  - $b_2 = A_2 x_2 = \\begin{bmatrix} 3 \\\\ 0 \\\\ 10^{-12} \\end{bmatrix}$，\n  - $\\hat{x}_2 = \\begin{bmatrix} -1 \\\\ 2 \\\\ 2 \\end{bmatrix}$。\n\n- 测试用例 3（带有零缩放分量的边界类型变体）：\n  - $A_3 = \\begin{bmatrix} 1  2  0 \\\\ 0  1  -1 \\\\ 10^{-12}  0  0 \\end{bmatrix}$，\n  - $x_3 = \\begin{bmatrix} 0 \\\\ \\tfrac{3}{2} \\\\ \\tfrac{3}{2} \\end{bmatrix}$，\n  - $b_3 = A_3 x_3 = \\begin{bmatrix} 3 \\\\ 0 \\\\ 0 \\end{bmatrix}$，\n  - $\\hat{x}_3 = \\begin{bmatrix} -1 \\\\ 2 \\\\ 2 \\end{bmatrix}$。\n\n对于每个测试用例 $j \\in \\{1,2,3\\}$，计算：\n- 如上定义的 $\\beta_{\\mathrm{norm}}^{(j)}$，\n- 如上定义的 $\\beta_{\\mathrm{comp}}^{(j)}$，\n- 使用精确解 $x_j = A_j^{-1} b_j$ 计算的 $\\epsilon_{\\mathrm{fwd}}^{(j)}$，\n- 通过奇异值分解（SVD）计算的 $\\kappa_2(A_j)$，\n- 一个布尔值，指示不等式 $\\epsilon_{\\mathrm{fwd}}^{(j)} \\le \\dfrac{\\kappa_2(A_j)\\,\\beta_{\\mathrm{norm}}^{(j)}}{1 - \\kappa_2(A_j)\\,\\beta_{\\mathrm{norm}}^{(j)}}$ 是否成立（如果 $\\kappa_2(A_j)\\,\\beta_{\\mathrm{norm}}^{(j)} \\ge 1$，则将右侧视为 $+\\infty$）。\n\n您的程序应生成单行输出，其中包含聚合为方括号内的逗号分隔列表的结果，顺序如下：\n$$\n\\left[\n\\beta_{\\mathrm{norm}}^{(1)},\\,\\beta_{\\mathrm{comp}}^{(1)},\\,\\epsilon_{\\mathrm{fwd}}^{(1)},\\,\\kappa_2(A_1),\\,\\text{bool}_1,\\,\n\\beta_{\\mathrm{norm}}^{(2)},\\,\\beta_{\\mathrm{comp}}^{(2)},\\,\\epsilon_{\\mathrm{fwd}}^{(2)},\\,\\kappa_2(A_2),\\,\\text{bool}_2,\\,\n\\beta_{\\mathrm{norm}}^{(3)},\\,\\beta_{\\mathrm{comp}}^{(3)},\\,\\epsilon_{\\mathrm{fwd}}^{(3)},\\,\\kappa_2(A_3),\\,\\text{bool}_3\n\\right],\n$$\n其中 $\\text{bool}_j$ 是测试用例 $j$ 的布尔结果。不涉及任何单位；将所有量表示为浮点数或布尔值，不带百分号。此输出必须是程序打印的唯一内容。\n\n在您的实现和计算结果的分析中，请特别关注测试用例 2 和 3，以说明将 $A$ 的单行和 $b$ 的相应分量缩放 $10^{-12}$ 会如何使范数形式的后向误差 $\\beta_{\\mathrm{norm}}$ 变小（因为全局范数由较大尺度的分量主导），而分量形式的后向误差 $\\beta_{\\mathrm{comp}}$ 变得很大（接近 $1$），因为残差与该行数据的局部尺度一样大。",
            "solution": "该问题要求计算和分析数值线性代数中与求解线性系统 $A x = b$ 的误差相关的几个关键量。我们获得了三个不同的测试用例，每个用例都由一个矩阵 $A_j$、一个精确解 $x_j$、一个右端项 $b_j = A_j x_j$ 和一个计算出的近似解 $\\hat{x}_j$（其中 $j \\in \\{1,2,3\\}$）定义。\n\n每个测试用例需要计算的量是：\n1.  范数形式的后向误差，$\\beta_{\\mathrm{norm}} = \\frac{\\|r\\|_2}{\\|A\\|_2 \\,\\|\\hat{x}\\|_2 + \\|b\\|_2}$，其中 $r = b - A \\hat{x}$ 是残差向量。这衡量了相对于数据 $A$ 和 $b$ 的范数的后向误差。\n2.  分量形式的后向误差，$\\beta_{\\mathrm{comp}} = \\max_{i} \\frac{|r_i|}{\\left(|A|\\,|\\hat{x}| + |b|\\right)_i}$。这衡量了相对于系统每行数据局部尺度的后向误差。\n3.  前向误差，$\\epsilon_{\\mathrm{fwd}} = \\frac{\\|\\hat{x} - x\\|_2}{\\|x\\|_2}$，它衡量了计算解中的相对误差。\n4.  矩阵的 $2$-范数条件数，$\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}$，其中 $\\sigma_{\\max}(A)$ 和 $\\sigma_{\\min}(A)$ 分别是 $A$ 的最大和最小奇异值。\n5.  不等式 $\\epsilon_{\\mathrm{fwd}} \\le \\frac{\\kappa_2(A)\\,\\beta_{\\mathrm{norm}}}{1 - \\kappa_2(A)\\,\\beta_{\\mathrm{norm}}}$ 的布尔验证。\n\n对问题陈述的验证证实了其是适定的、科学上合理的，并且所有必要的数据都已提供。矩阵 $A_1$、$A_2$ 和 $A_3$ 都是非奇异的，确保每种情况下都存在唯一的精确解。我们继续对每个测试用例进行计算。\n\n**通用计算步骤：**\n对于每个测试用例 $j$：\n1.  定义矩阵 $A_j$ 和向量 $x_j$、$b_j$ 和 $\\hat{x}_j$。\n2.  计算残差向量 $r_j = b_j - A_j \\hat{x}_j$。\n3.  计算所需的范数：$\\|r_j\\|_2$、$\\|A_j\\|_2$、$\\|\\hat{x}_j\\|_2$、$\\|b_j\\|_2$、$\\|\\hat{x}_j - x_j\\|_2$ 和 $\\|x_j\\|_2$。矩阵的 $2$-范数 $\\|A_j\\|_2$ 是其最大的奇异值。\n4.  使用其定义计算 $\\beta_{\\mathrm{norm}}^{(j)}$。\n5.  为分量形式的误差计算分母向量 $(|A_j|\\,|\\hat{x}_j| + |b_j|)$。然后通过逐元素比率的最大值找到 $\\beta_{\\mathrm{comp}}^{(j)}$。\n6.  使用其定义计算 $\\epsilon_{\\mathrm{fwd}}^{(j)}$。\n7.  计算 $A_j$ 的奇异值以找到 $\\kappa_2(A_j)$。\n8.  通过首先计算 $p_j = \\kappa_2(A_j)\\,\\beta_{\\mathrm{norm}}^{(j)}$ 来评估布尔条件。如果 $p_j  1$，检查是否 $\\epsilon_{\\mathrm{fwd}}^{(j)} \\le p_j / (1-p_j)$。否则，根据定义，条件成立。\n\n**测试用例 1：缩放良好的数据**\n- $A_1 = \\begin{bmatrix} 2  1  0 \\\\ 0  1  1 \\\\ 1  0  1 \\end{bmatrix}$，$x_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}$，$b_1 = A_1 x_1 = \\begin{bmatrix} 4 \\\\ 1 \\\\ 0 \\end{bmatrix}$，$\\hat{x}_1 = x_1$。\n- 由于计算解与精确解相同，即 $\\hat{x}_1 = x_1$，因此解的误差为零。\n- 残差为 $r_1 = b_1 - A_1 \\hat{x}_1 = A_1 x_1 - A_1 x_1 = \\mathbf{0}$。\n- 因此，所有依赖于残差或解误差的误差度量均为零。\n- $\\epsilon_{\\mathrm{fwd}}^{(1)} = \\frac{\\|\\hat{x}_1 - x_1\\|_2}{\\|x_1\\|_2} = 0$。\n- $\\beta_{\\mathrm{norm}}^{(1)} = \\frac{\\|r_1\\|_2}{\\|A_1\\|_2 \\,\\|\\hat{x}_1\\|_2 + \\|b_1\\|_2} = 0$。\n- $\\beta_{\\mathrm{comp}}^{(1)} = \\max_{i} \\frac{|(r_1)_i|}{(|A_1|\\,|\\hat{x}_1| + |b_1|)_i} = 0$。\n- 条件数计算为 $\\kappa_2(A_1) \\approx 3.0646$。\n- 对于不等式检查，$p_1 = \\kappa_2(A_1) \\beta_{\\mathrm{norm}}^{(1)} = 0$。不等式变为 $0 \\le \\frac{0}{1-0}$，这是成立的。所以，$\\text{bool}_1$ 为 True。\n此用例作为一个基准，其中计算解是完美的。\n\n**测试用例 2：混合缩放数据**\n- $A_2 = \\begin{bmatrix} 1  2  0 \\\\ 0  1  -1 \\\\ 10^{-12}  0  0 \\end{bmatrix}$，$x_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$，$b_2 = \\begin{bmatrix} 3 \\\\ 0 \\\\ 10^{-12} \\end{bmatrix}$，$\\hat{x}_2 = \\begin{bmatrix} -1 \\\\ 2 \\\\ 2 \\end{bmatrix}$。\n- 残差为 $r_2 = b_2 - A_2 \\hat{x}_2 = \\begin{bmatrix} 3 \\\\ 0 \\\\ 10^{-12} \\end{bmatrix} - \\begin{pmatrix} 1(-1)+2(2) \\\\ 1(2)-1(2) \\\\ 10^{-12}(-1) \\end{pmatrix} = \\begin{bmatrix} 3 \\\\ 0 \\\\ 10^{-12} \\end{bmatrix} - \\begin{bmatrix} 3 \\\\ 0 \\\\ -10^{-12} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 2 \\cdot 10^{-12} \\end{bmatrix}$。\n- 前向误差为 $\\epsilon_{\\mathrm{fwd}}^{(2)} = \\frac{\\|\\hat{x}_2 - x_2\\|_2}{\\|x_2\\|_2} = \\frac{\\|[-2, 1, 1]^T\\|_2}{\\|[1, 1, 1]^T\\|_2} = \\frac{\\sqrt{6}}{\\sqrt{3}} = \\sqrt{2} \\approx 1.4142$。这是一个很大的前向误差，表明计算解的质量很差。\n- 残差的范数是 $\\|r_2\\|_2 = 2 \\cdot 10^{-12}$，这个值非常小。这个小范数导致了一个非常小的范数形式的后向误差：$\\beta_{\\mathrm{norm}}^{(2)} \\approx 1.948 \\times 10^{-13}$。这个值表明，在范数意义上，$\\hat{x}_2$ 是一个非常相近问题的精确解。\n- 对于分量形式的后向误差，分母为 $(|A_2||\\hat{x}_2| + |b_2|) = [8, 4, 2 \\cdot 10^{-12}]^T$。分量比率为 $\\frac{0}{8}$、$\\frac{0}{4}$ 和 $\\frac{2 \\cdot 10^{-12}}{2 \\cdot 10^{-12}}$。最大值为 $1$。因此，$\\beta_{\\mathrm{comp}}^{(2)} = 1$。\n- $\\beta_{\\mathrm{norm}}^{(2)}$（小）和 $\\beta_{\\mathrm{comp}}^{(2)}$（大）之间的差异是核心要点。系统的第三行被缩放了 $10^{-12}$。残差的唯一非零分量在这一行。在范数意义上，这个微小的分量被整个向量平均，导致范数很小。在分量意义上，第三个方程中的误差与第三个方程数据的尺度进行比较，揭示了残差分量可能达到的最大值。\n- 由于这种缩放，矩阵 $A_2$ 是病态的，其 $\\kappa_2(A_2) \\approx 2.933 \\times 10^{12}$。\n- 对于不等式，$p_2 = \\kappa_2(A_2)\\,\\beta_{\\mathrm{norm}}^{(2)} \\approx 0.5712$。要检查的不等式是 $1.4142 \\le \\frac{0.5712}{1 - 0.5712} \\approx 1.3320$。这是不成立的。所以，$\\text{bool}_2$ 为 False。这表明，即使满足前提条件 $\\kappa_2(A)\\beta_{\\mathrm{norm}}  1$，这种特定简单形式的误差界也并不成立。\n\n**测试用例 3：带有零缩放分量的变体**\n- $A_3 = A_2$，$x_3 = \\begin{bmatrix} 0 \\\\ 1.5 \\\\ 1.5 \\end{bmatrix}$，$b_3 = \\begin{bmatrix} 3 \\\\ 0 \\\\ 0 \\end{bmatrix}$，$\\hat{x}_3 = \\hat{x}_2 = \\begin{bmatrix} -1 \\\\ 2 \\\\ 2 \\end{bmatrix}$。\n- 由于 $A_3 = A_2$ 和 $\\hat{x}_3 = \\hat{x}_2$，乘积 $A_3 \\hat{x}_3$ 与情况 2 中相同。\n- 残差为 $r_3 = b_3 - A_3 \\hat{x}_3 = \\begin{bmatrix} 3 \\\\ 0 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} 3 \\\\ 0 \\\\ -10^{-12} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 10^{-12} \\end{bmatrix}$。\n- 前向误差为 $\\epsilon_{\\mathrm{fwd}}^{(3)} = \\frac{\\|\\hat{x}_3 - x_3\\|_2}{\\|x_3\\|_2} = \\frac{\\|[-1, 0.5, 0.5]^T\\|_2}{\\|[0, 1.5, 1.5]^T\\|_2} = \\frac{\\sqrt{1.5}}{\\sqrt{4.5}} = \\frac{1}{\\sqrt{3}} \\approx 0.5774$。\n- 残差范数 $\\|r_3\\|_2 = 10^{-12}$ 同样非常小，导致了一个小的范数形式的后向误差 $\\beta_{\\mathrm{norm}}^{(3)} \\approx 9.740 \\times 10^{-14}$。\n- 分量形式的分析与情况 2 非常相似。分母向量为 $(|A_3||\\hat{x}_3| + |b_3|) = [8, 4, 10^{-12}]^T$。分量比率为 $\\frac{0}{8}$、$\\frac{0}{4}$ 和 $\\frac{10^{-12}}{10^{-12}}$。最大值为 $1$，所以 $\\beta_{\\mathrm{comp}}^{(3)} = 1$。观察到同样的差异。\n- 条件数不变：$\\kappa_2(A_3) = \\kappa_2(A_2) \\approx 2.933 \\times 10^{12}$。\n- 对于不等式检查，$p_3 = \\kappa_2(A_3)\\,\\beta_{\\mathrm{norm}}^{(3)} \\approx 0.2856$。要检查的不等式是 $0.5774 \\le \\frac{0.2856}{1 - 0.2856} \\approx 0.3998$。这也是不成立的。所以，$\\text{bool}_3$ 为 False。\n\n总而言之，这些测试用例有效地说明了不良缩放（即各行条目数量级的巨大差异）会使系统变得病态，并可能导致范数形式的后向误差产生误导性的小值。分量形式的后向误差通过尊重局部数据尺度，在此类场景中提供了更稳健的度量。此外，结果表明，对简化误差界公式的简单应用可能是错误的；必须谨慎使用后向误差的特定公式和相应的已证实的界限。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes error metrics for three linear algebra test cases and\n    formats the results as specified.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"A\": np.array([[2., 1., 0.], [0., 1., 1.], [1., 0., 1.]]),\n            \"x\": np.array([1., 2., -1.]),\n            \"x_hat\": np.array([1., 2., -1.]),\n        },\n        {\n            \"A\": np.array([[1., 2., 0.], [0., 1., -1.], [1e-12, 0., 0.]]),\n            \"x\": np.array([1., 1., 1.]),\n            \"x_hat\": np.array([-1., 2., 2.]),\n        },\n        {\n            \"A\": np.array([[1., 2., 0.], [0., 1., -1.], [1e-12, 0., 0.]]),\n            \"x\": np.array([0., 1.5, 1.5]),\n            \"x_hat\": np.array([-1., 2., 2.]),\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A = case[\"A\"]\n        x = case[\"x\"]\n        x_hat = case[\"x_hat\"]\n        \n        # Calculate b from A and the exact solution x\n        b = A @ x\n        \n        # 1. Residual\n        r = b - A @ x_hat\n        \n        # Calculate norms needed for the metrics\n        norm_r = np.linalg.norm(r, 2)\n        norm_A = np.linalg.norm(A, 2)\n        norm_x_hat = np.linalg.norm(x_hat, 2)\n        norm_b = np.linalg.norm(b, 2)\n        \n        # 2. Normwise backward error (beta_norm)\n        # The denominator can't be zero unless A, x_hat, and b are all zero.\n        denominator_norm = norm_A * norm_x_hat + norm_b\n        if denominator_norm == 0:\n             beta_norm = 0.0 if norm_r == 0 else np.inf\n        else:\n             beta_norm = norm_r / denominator_norm\n        \n        # 3. Componentwise backward error (beta_comp)\n        # Denominator is a vector (|A|*|x_hat|+|b|).\n        denominator_comp = np.abs(A) @ np.abs(x_hat) + np.abs(b)\n        # If a component of the denominator is zero, it must mean the corresponding\n        # residual component is also zero for a meaningful problem. In this case,\n        # the ratio is 0/0, which we treat as 0.\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratios = np.abs(r) / denominator_comp\n        ratios[np.isnan(ratios)] = 0.0 # Handles 0/0 case\n        beta_comp = np.max(ratios)\n\n        # 4. Forward error (eps_fwd)\n        norm_x = np.linalg.norm(x, 2)\n        if norm_x == 0:\n            # If true solution is zero, relative error is 0 if x_hat is also 0, else inf.\n            eps_fwd = 0.0 if np.all(x_hat == 0) else np.inf\n        else:\n            eps_fwd = np.linalg.norm(x_hat - x, 2) / norm_x\n\n        # 5. Condition number (kappa_2)\n        # Using np.linalg.cond is equivalent to SVD ratio but more direct.\n        kappa_2 = np.linalg.cond(A, 2)\n\n        # 6. Inequality check\n        product = kappa_2 * beta_norm\n        if product >= 1:\n            bound_holds = True\n        else:\n            rhs_bound = product / (1 - product)\n            bound_holds = eps_fwd = rhs_bound\n\n        # Special handling for case 1 where x_hat=x, to avoid float precision issues.\n        if np.all(x == x_hat):\n            beta_norm = 0.0\n            beta_comp = 0.0\n            eps_fwd = 0.0\n            bound_holds = True # 0 = 0\n            \n        results.extend([beta_norm, beta_comp, eps_fwd, kappa_2, bound_holds])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}