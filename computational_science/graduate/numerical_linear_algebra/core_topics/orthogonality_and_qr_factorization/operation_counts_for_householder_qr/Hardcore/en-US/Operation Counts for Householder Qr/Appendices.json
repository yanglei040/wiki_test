{
    "hands_on_practices": [
        {
            "introduction": "Any rigorous analysis of Householder-based algorithms begins with its most fundamental operation: the application of a single Householder reflector to a matrix. This exercise guides you through the derivation of the operational cost for this key step, starting from its definition as a rank-1 update. Mastering this calculation is essential, as the resulting leading-order cost of $2ms$ flops serves as the foundational building block for analyzing the entire QR factorization process.",
            "id": "3562610",
            "problem": "Consider a single real Householder reflector $H \\in \\mathbb{R}^{m \\times m}$ of the form $H = I - \\tau v v^{T}$, where $v \\in \\mathbb{R}^{m}$ and $\\tau \\in \\mathbb{R}$, applied from the left to a dense block $B \\in \\mathbb{R}^{m \\times s}$. Starting from the identity $H B = B - \\tau v (v^{T} B)$ and the definition of a floating-point operation, derive the asymptotic operation count for computing $H B$.\n\nAdopt the following scientifically standard and widely used cost model for Basic Linear Algebra Subprograms (BLAS). In real arithmetic:\n- An inner product of length $m$ is costed as $m$ composite fused multiply-adds (abbreviated as FMA), i.e., each multiply-add pair is counted as a single floating-point operation, for a dominant cost of $m$.\n- A rank-$1$ update of size $m \\times s$ is costed as $m s$ composite FMA operations, i.e., one floating-point operation per entry for the multiply-add pair.\n- Scalar-vector and scalar-matrix scalings over $k$ entries cost $k$ floating-point operations.\n\nUnder this model, compute the total number of floating-point operations required to evaluate $H B$ using the minimal three-step algorithm implied by $H B = B - \\tau v (v^{T} B)$:\n1. Compute the row vector $w^{T} := v^{T} B \\in \\mathbb{R}^{1 \\times s}$.\n2. Scale $w^{T} := \\tau w^{T}$.\n3. Perform the rank-$1$ update $B := B - v w^{T}$.\n\nGive your answer as a single closed-form expression in terms of $m$ and $s$ that captures only the leading-order term in $m$ and $s$ (i.e., lower-order terms in $m$ and $s$ must be omitted). Your final answer must be a single analytic expression with no units. Verify that your result matches the standard leading term reported for the application of a single Householder reflector to an $m \\times s$ block.",
            "solution": "We begin from the defining action of the Householder reflector $H = I - \\tau v v^{T}$ on a matrix $B \\in \\mathbb{R}^{m \\times s}$:\n$$\nH B \\;=\\; B - \\tau v (v^{T} B).\n$$\nThis expression suggests a natural three-step algorithm: compute the intermediate row vector $w^{T} := v^{T} B \\in \\mathbb{R}^{1 \\times s}$, scale it by $\\tau$, and then perform a rank-$1$ correction $B := B - v w^{T}$.\n\nWe count floating-point operations under the stated model that treats each fused multiply-add as a single operation. This model is standard when deriving leading-term operation counts for Householder-based algorithms and aligns with the organization of computations in Level-$2$ Basic Linear Algebra Subprograms (BLAS), where the dominant costs are attributable to matrix-vector and rank-$1$ operations with multiply-add fusion.\n\nStep-by-step counts:\n1. Compute $w^{T} := v^{T} B$. This is a left matrix-vector product in block form, equivalently $w = B^{T} v$, producing a vector in $\\mathbb{R}^{s}$. It is realized as $s$ inner products of length $m$. Under the composite fused multiply-add model, each inner product of length $m$ costs $m$ floating-point operations (one per multiply-add pair). Therefore, the total dominant cost of this step is\n$$\nm s.\n$$\n\n2. Scale $w^{T} := \\tau w^{T}$. This is a scalar-vector scaling of $s$ entries, which costs\n$$\ns.\n$$\nThis is lower-order in $m$ and $s$ compared with the dominant bilinear terms.\n\n3. Perform the rank-$1$ update $B := B - v w^{T}$. This updates every entry of $B$ as $b_{ij} := b_{ij} - v_{i} w_{j}$. Under the composite fused multiply-add model, each update per entry counts as one operation, and there are $m s$ entries. Thus, the dominant cost of this step is\n$$\nm s.\n$$\n\nSumming the three contributions, the total is\n$$\nm s \\;+\\; s \\;+\\; m s \\;=\\; 2 m s \\;+\\; s.\n$$\nBy the problem’s instruction to report only the leading-order term in $m$ and $s$, we omit the lower-order $s$ term. Hence, the leading-order operation count for applying a single Householder reflector to an $m \\times s$ block under the stated model is\n$$\n2 m s.\n$$\n\nThis matches the standard leading-term count used in operation analyses of Householder-based QR factorization, where applying one reflector to a trailing $m \\times s$ block incurs a dominant cost of $2 m s$ floating-point operations (in the composite fused multiply-add sense), with lower-order terms suppressed. For completeness, we note that under the alternative model that counts a multiply and an add as separate operations, the detailed count would be $(2 m s - s) + s + 2 m s = 4 m s$, which still verifies that the dominant scaling is bilinear in $m$ and $s$, but with a different constant prefactor due to the differing operation model. Under the model specified in the problem, the leading term is $2 m s$.",
            "answer": "$$\\boxed{2 m s}$$"
        },
        {
            "introduction": "Having established the cost of a single Householder update, we can now scale up our analysis to determine the total computational cost of a full, unblocked QR factorization. This practice requires summing the costs of applying successive reflectors to the progressively shrinking trailing submatrix. By converting the iterative process into a summation and finding the dominant term, you will derive the classic cubic complexity of Householder QR and gain insight into how local costs aggregate into a global complexity estimate.",
            "id": "3562607",
            "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m < n$. The classical Householder method for the Orthogonal-Triangular factorization (QR) applies $m$ successive Householder reflections to reduce $A$ to the form $A = Q R$ where $Q \\in \\mathbb{R}^{m \\times m}$ is orthogonal and $R \\in \\mathbb{R}^{m \\times n}$ is upper trapezoidal. At step $k$, with $1 \\leq k \\leq m$, one constructs a Householder vector $v \\in \\mathbb{R}^{m - k + 1}$ and scalar $\\beta \\in \\mathbb{R}$ such that the trailing block $A_{k:m,\\,k:n}$ is updated by the rank-one transformation $A_{k:m,\\,k:n} \\leftarrow A_{k:m,\\,k:n} - v \\left( \\beta \\, v^{\\top} A_{k:m,\\,k:n} \\right)$. \n\nUsing only the fundamental definitions of a dot product and a scalar-times-vector plus vector operation, and counting each floating-point addition and multiplication as one floating-point operation (flop), while neglecting the costs of division, square root, and small constant overheads involved in forming the Householder vector, derive a closed-form expression in $m$ and $n$ for the dominant operation count of this process when it stops after $m$ steps. Your final answer must be a single closed-form analytic expression in terms of $m$ and $n$ giving the dominant flop count for the updates of the trailing blocks throughout the $m$ steps. Do not include any units in your final answer.",
            "solution": "The total operation count is the sum of the costs for the update phase at each of the $m$ steps. As per the problem statement, we count only the flops for the update of the trailing block and neglect the cost of forming the Householder vector at each step.\n\nAt step $k$ (for $k=1, \\dots, m$), the update is applied to the trailing submatrix $A_{k:m, k:n}$. This submatrix has dimensions $p \\times q$, where $p = m - k + 1$ and $q = n - k + 1$. The update operation is $A' \\leftarrow A' - v (\\beta v^T A')$. We calculate the cost by breaking it down into elementary operations:\n1.  **Matrix-vector product ($y^T = v^T A'$):** This consists of $q$ dot products between vectors of length $p$. Each dot product costs $p$ multiplications and $p-1$ additions, for a total of $2p-1$ flops. The cost for this step is $q(2p-1)$ flops.\n2.  **Scalar-vector product ($u^T = \\beta y^T$):** This requires $q$ multiplications, costing $q$ flops.\n3.  **Rank-1 update ($A' \\leftarrow A' - v u^T$):** This operation performs a multiplication and a subtraction for each of the $pq$ elements of the matrix, costing $2pq$ flops.\n\nThe total flop count for step $k$, denoted $C_k$, is the sum of these costs:\n$$C_k = q(2p-1) + q + 2pq = (2pq - q) + q + 2pq = 4pq$$\nSubstituting the expressions for $p$ and $q$:\n$$C_k = 4(m-k+1)(n-k+1)$$\nTo find the total flop count for the entire process, we sum $C_k$ over all $m$ steps:\n$$C = \\sum_{k=1}^{m} 4(m-k+1)(n-k+1)$$\nTo evaluate this sum, we perform a change of index. Let $j = m-k+1$. When $k=1$, $j=m$. When $k=m$, $j=1$. The other term becomes $n-k+1 = n-(m-j+1)+1 = n-m+j$. The sum can be rewritten as:\n$$C = \\sum_{j=1}^{m} 4j(n-m+j) = 4 \\sum_{j=1}^{m} (j(n-m) + j^2) = 4(n-m)\\sum_{j=1}^{m}j + 4\\sum_{j=1}^{m}j^2$$\nUsing the standard formulas for sums of powers, $\\sum_{j=1}^{m} j = \\frac{m(m+1)}{2}$ and $\\sum_{j=1}^{m} j^2 = \\frac{m(m+1)(2m+1)}{6}$:\n$$C = 4(n-m)\\frac{m(m+1)}{2} + 4\\frac{m(m+1)(2m+1)}{6}$$\n$$C = 2m(m+1)(n-m) + \\frac{2}{3}m(m+1)(2m+1)$$\nWe are interested in the dominant terms of this polynomial in $m$ and $n$, which are those of the highest total degree (degree 3). Expanding the expression:\n$$C = 2(nm^2 - m^3 + nm - m^2) + \\frac{2}{3}(2m^3 + 3m^2 + m)$$\n$$C = (2nm^2 - 2m^3 + \\dots) + (\\frac{4}{3}m^3 + \\dots) = 2nm^2 - \\frac{2}{3}m^3 + \\dots$$\nThe dominant terms are $2nm^2$ and $-\\frac{2}{3}m^3$. Therefore, the dominant operation count is $2nm^2 - \\frac{2}{3}m^3$.",
            "answer": "$$\\boxed{2nm^2 - \\frac{2}{3}m^3}$$"
        },
        {
            "introduction": "To achieve high performance on modern computer architectures, numerical libraries replace the unblocked (Level-2 BLAS) algorithm with a blocked (Level-3 BLAS) equivalent that maximizes data reuse. This final practice delves into the cost analysis of applying a panel of Householder reflectors represented in the compact WY form, $Q_{\\text{panel}} = I - V T V^{T}$. By dissecting this operation into a sequence of matrix-matrix multiplications, you will uncover the computational advantages of blocked algorithms and understand the principles behind high-performance scientific computing.",
            "id": "3562527",
            "problem": "Consider the blocked Householder representation of an orthogonal panel transformation, in which the panel transform is written in the compact Watson–Yamaguchi (WY) form as $Q_{\\text{panel}} = I - V T V^{T}$, with $V \\in \\mathbb{R}^{p \\times b}$ containing $b$ Householder vectors and $T \\in \\mathbb{R}^{b \\times b}$ an upper triangular matrix constructed from these vectors. Let $C \\in \\mathbb{R}^{p \\times q}$ denote a trailing matrix to be updated by a left application of the panel transform, $C \\leftarrow Q_{\\text{panel}} C$. Work in real arithmetic and use the standard floating-point operation (flop) model in which each scalar multiplication and each scalar addition counts as one flop. For dense matrix-matrix multiplication of conforming dimensions $m \\times k$ by $k \\times n$, assume a cost of $2 m n k$ flops, and for triangular (upper or lower) matrix-matrix multiplication of dimensions $b \\times b$ by $b \\times q$, assume a cost of $b^{2} q$ flops. For elementwise addition or subtraction of two $p \\times q$ matrices, assume a cost of $p q$ flops.\n\nStarting from these definitions, derive the total flop count required to apply $Q_{\\text{panel}}$ to $C$ on the left in terms of $p$, $q$, and $b$. Your final answer must be a single closed-form analytic expression in $p$, $q$, and $b$. Also, identify which Level-3 Basic Linear Algebra Subprograms (BLAS) term dominates the cost. Provide the final expression only; no rounding is required.",
            "solution": "The task is to find the total flop count for the update $C \\leftarrow Q_{\\text{panel}} C$, where $Q_{\\text{panel}} = I - V T V^{T}$. Substituting this form into the update rule gives:\n$$\nC \\leftarrow (I - V T V^{T}) C = C - V T V^{T} C\n$$\nTo compute this efficiently, we group the matrix multiplications as $C - V(T(V^T C))$. We calculate the cost of each step based on the provided models.\n\n1.  **Compute $W_1 = V^T C$**: This is a dense matrix-matrix multiplication of a $b \\times p$ matrix ($V^T$) by a $p \\times q$ matrix ($C$). Using the cost model $2mnk$ with $m=b$, $k=p$, $n=q$, the cost is:\n    $$\\text{Cost}_1 = 2bpq \\text{ flops}$$\n    This is a Level-3 BLAS `GEMM` operation.\n\n2.  **Compute $W_2 = T W_1$**: This is a multiplication of an upper triangular $b \\times b$ matrix ($T$) by a dense $b \\times q$ matrix ($W_1$). The problem specifies a cost of $b^2 q$ for this operation.\n    $$\\text{Cost}_2 = b^2q \\text{ flops}$$\n    This corresponds to a Level-3 BLAS `TRMM` operation.\n\n3.  **Compute $W_3 = V W_2$**: This is a dense matrix-matrix multiplication of a $p \\times b$ matrix ($V$) by a $b \\times q$ matrix ($W_2$). Using the cost model $2mnk$ with $m=p$, $k=b$, $n=q$, the cost is:\n    $$\\text{Cost}_3 = 2pbq \\text{ flops}$$\n    This is another `GEMM` operation.\n\n4.  **Compute $C \\leftarrow C - W_3$**: This is an elementwise subtraction of two $p \\times q$ matrices. The cost is:\n    $$\\text{Cost}_4 = pq \\text{ flops}$$\n\nThe total flop count is the sum of these four steps:\n$$\\text{Total Flops} = \\text{Cost}_1 + \\text{Cost}_2 + \\text{Cost}_3 + \\text{Cost}_4 = 2bpq + b^2q + 2pbq + pq$$\nCombining like terms gives the final expression:\n$$\\text{Total Flops} = 4pqb + b^2q + pq$$\nIn blocked algorithms, typically $p, q \\gg b$. The dominant costs come from the Level-3 BLAS operations. Specifically, the two `GEMM` operations (Steps 1 and 3) contribute $4pqb$ flops and constitute the vast majority of the computational work.",
            "answer": "$$\n\\boxed{4pqb + b^{2}q + pq}\n$$"
        }
    ]
}