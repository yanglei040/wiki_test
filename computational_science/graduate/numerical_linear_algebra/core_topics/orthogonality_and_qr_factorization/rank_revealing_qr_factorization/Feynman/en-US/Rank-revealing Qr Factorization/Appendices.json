{
    "hands_on_practices": [
        {
            "introduction": "Before tackling complex theory, it is essential to solidify understanding through direct calculation. This first exercise guides you through the mechanics of the Column-Pivoted QR (CPQR) algorithm on a simple yet illustrative matrix that depends on a small parameter $\\epsilon$ . By manually executing the greedy pivoting strategy based on column norms and deriving the resulting triangular factor $R$, you will gain procedural fluency and see firsthand how the factorization reveals the hierarchy of column importance.",
            "id": "3571796",
            "problem": "Consider the matrix $A \\in \\mathbb{R}^{3 \\times 3}$ defined by\n$$\nA=\\begin{bmatrix}\n1 & \\epsilon & 0 \\\\\n0 & 1 & \\epsilon \\\\\n0 & 0 & 1\n\\end{bmatrix},\n$$\nwhere $0<\\epsilon \\ll 1$. A Column Pivoted QR (CPQR) factorization of $A$ is a factorization $A \\Pi = Q R$, where $\\Pi$ is a permutation matrix, $Q$ is orthogonal, and $R$ is upper triangular, obtained by greedily selecting pivot columns according to the largest current column $2$-norm (with ties broken by choosing the smallest column index), and orthogonalizing subsequent columns against the selected Householder reflector or, equivalently, against the current orthonormal vector in a Gram–Schmidt view.\n\nStarting from the foundational definitions of the $2$-norm and orthogonal projections that underlie QR factorization with column pivoting, determine the pivot order and derive the exact expressions (as functions of $\\epsilon$) for the magnitudes of the diagonal entries of $R$ in the CPQR factorization of $A$. Your derivation should verify the nonincreasing diagonal magnitudes property $|r_{11}| \\ge |r_{22}| \\ge |r_{33}|$ by explicit calculation from first principles.\n\nAnswer specification: Provide, as your final answer, the single row vector $(|r_{11}|, |r_{22}|, |r_{33}|)$ in closed form in $\\epsilon$. No numerical rounding is required or permitted. Do not include any units.",
            "solution": "The problem asks for the pivot order and the magnitudes of the diagonal entries of the upper triangular factor $R$ from the Column-Pivoted QR (CPQR) factorization of the matrix $A$. The CPQR algorithm greedily selects pivot columns based on the largest 2-norm of the remaining columns at each step.\n\nLet the columns of $A$ be $a_1 = [1, 0, 0]^T$, $a_2 = [\\epsilon, 1, 0]^T$, and $a_3 = [0, \\epsilon, 1]^T$.\n\n**Step 1: Select the first pivot.**\nWe compute the initial squared Euclidean norms of the columns:\n- $\\|a_1\\|_2^2 = 1^2 + 0^2 + 0^2 = 1$\n- $\\|a_2\\|_2^2 = \\epsilon^2 + 1^2 + 0^2 = 1 + \\epsilon^2$\n- $\\|a_3\\|_2^2 = 0^2 + \\epsilon^2 + 1^2 = 1 + \\epsilon^2$\n\nThe largest norm is $\\sqrt{1+\\epsilon^2}$, shared by columns $a_2$ and $a_3$. The tie-breaking rule specifies choosing the column with the smallest index, so we select **column 2** as the first pivot. The permutation matrix $\\Pi$ will begin by moving column 2 to the first position.\nThe first diagonal entry of $R$ is the norm of the first pivot column:\n$|r_{11}| = \\|a_2\\|_2 = \\sqrt{1+\\epsilon^2}$.\n\n**Step 2: Select the second pivot.**\nNext, we find the components of the remaining columns, $a_1$ and $a_3$, that are orthogonal to the first pivot column $a_2$. The standard CPQR algorithm then chooses the pivot as the column whose orthogonal component has the largest norm.\nLet $q_1 = a_2 / \\|a_2\\|_2$. The squared norms of the orthogonal components are given by the downdate formula: $\\|a_j'\\|^2 = \\|a_j\\|^2 - (q_1^T a_j)^2$.\n- For $a_1$:\n  $q_1^T a_1 = \\frac{a_2^T a_1}{\\|a_2\\|_2} = \\frac{\\epsilon}{\\sqrt{1+\\epsilon^2}}$.\n  The squared norm of the remaining part of $a_1$ is $\\|a_1'\\|_2^2 = \\|a_1\\|_2^2 - (q_1^T a_1)^2 = 1 - \\frac{\\epsilon^2}{1+\\epsilon^2} = \\frac{1}{1+\\epsilon^2}$.\n- For $a_3$:\n  $q_1^T a_3 = \\frac{a_2^T a_3}{\\|a_2\\|_2} = \\frac{\\epsilon}{\\sqrt{1+\\epsilon^2}}$.\n  The squared norm of the remaining part of $a_3$ is $\\|a_3'\\|_2^2 = \\|a_3\\|_2^2 - (q_1^T a_3)^2 = (1+\\epsilon^2) - \\frac{\\epsilon^2}{1+\\epsilon^2} = \\frac{(1+\\epsilon^2)^2-\\epsilon^2}{1+\\epsilon^2} = \\frac{1+2\\epsilon^2+\\epsilon^4-\\epsilon^2}{1+\\epsilon^2} = \\frac{1+\\epsilon^2+\\epsilon^4}{1+\\epsilon^2}$.\n\nComparing the norms, we see that $\\frac{1+\\epsilon^2+\\epsilon^4}{1+\\epsilon^2} > \\frac{1}{1+\\epsilon^2}$ since the numerator $1+\\epsilon^2+\\epsilon^4 > 1$.\nTherefore, the algorithm selects **column 3** as the second pivot.\nThe full pivot order is $(2, 3, 1)$. The permutation matrix is $\\Pi$ such that $A\\Pi = [a_2, a_3, a_1]$.\nThe second diagonal entry of $R$ is the norm of the orthogonal component of the second pivot column:\n$|r_{22}| = \\|a_3'\\|_2 = \\sqrt{\\frac{1+\\epsilon^2+\\epsilon^4}{1+\\epsilon^2}}$.\n\n**Step 3: Determine the third diagonal entry.**\nThe final column in the permuted matrix is $a_1$. The third diagonal entry, $|r_{33}|$, is the norm of the component of $a_1$ orthogonal to the plane spanned by $a_2$ and $a_3$.\nA property of the QR factorization is that $|\\det(A\\Pi)| = |\\det(QR)| = |\\det(Q)||\\det(R)| = |\\det(R)|$. The determinant of the upper triangular matrix $R$ is the product of its diagonal entries: $\\det(R) = r_{11}r_{22}r_{33}$. Thus, $|r_{11}||r_{22}||r_{33}| = |\\det(A\\Pi)|$.\nThe permutation $\\Pi$ that produces the order $(2, 3, 1)$ from $(1, 2, 3)$ can be achieved by swapping $(1,2)$ then $(2,3)$, so $\\det(\\Pi)=1$. Thus, $\\det(A\\Pi) = \\det(A)$.\nThe matrix $A$ is upper triangular, so its determinant is the product of its diagonal entries: $\\det(A) = 1 \\cdot 1 \\cdot 1 = 1$.\nTherefore, we have $|r_{11}||r_{22}||r_{33}| = 1$.\nWe can solve for $|r_{33}|$:\n$|r_{33}| = \\frac{1}{|r_{11}||r_{22}|} = \\frac{1}{\\sqrt{1+\\epsilon^2} \\cdot \\sqrt{\\frac{1+\\epsilon^2+\\epsilon^4}{1+\\epsilon^2}}} = \\frac{1}{\\sqrt{1+\\epsilon^2+\\epsilon^4}}$.\n\n**Summary and Verification:**\nThe magnitudes of the diagonal entries are:\n- $|r_{11}| = \\sqrt{1+\\epsilon^2}$\n- $|r_{22}| = \\sqrt{\\frac{1+\\epsilon^2+\\epsilon^4}{1+\\epsilon^2}}$\n- $|r_{33}| = \\frac{1}{\\sqrt{1+\\epsilon^2+\\epsilon^4}}$\n\nTo verify the non-increasing property $|r_{11}| \\ge |r_{22}| \\ge |r_{33}|$, we compare their squares for $0  \\epsilon \\ll 1$:\n- $|r_{11}|^2 = 1+\\epsilon^2$\n- $|r_{22}|^2 = \\frac{1+\\epsilon^2+\\epsilon^4}{1+\\epsilon^2} = 1 + \\frac{\\epsilon^4}{1+\\epsilon^2}$. Since $\\epsilon^2 > \\frac{\\epsilon^4}{1+\\epsilon^2}$ (as $1+\\epsilon^2 > \\epsilon^2$), we have $|r_{11}|^2 > |r_{22}|^2$.\n- $|r_{33}|^2 = \\frac{1}{1+\\epsilon^2+\\epsilon^4}$. Since $|r_{22}|^2 = 1 + \\frac{\\epsilon^4}{1+\\epsilon^2} > 1$ and $|r_{33}|^2  1$, we clearly have $|r_{22}|^2 > |r_{33}|^2$.\nThe property holds.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{1+\\epsilon^{2}}  \\sqrt{\\frac{1+\\epsilon^{2}+\\epsilon^{4}}{1+\\epsilon^{2}}}  \\frac{1}{\\sqrt{1+\\epsilon^{2}+\\epsilon^{4}}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A robust algorithm is often defined by the pitfalls it is designed to avoid. This practice explores a classic counterexample in numerical linear algebra to demonstrate why a naive column pivoting strategy can fail, leading to an ill-conditioned factorization that masks the true rank structure . By analyzing this specific case, you will develop a deeper appreciation for the standard CPQR algorithm, which correctly measures the new information each column contributes by calculating the norm of its component orthogonal to the previously selected columns.",
            "id": "3571791",
            "problem": "Consider the following setting for Orthogonal–Triangular factorization (QR) with column pivoting in the context of Rank-Revealing QR (RRQR) factorization. Let the counterexample matrix $A \\in \\mathbb{R}^{3 \\times 3}$ have columns\n$$\na_1 = M \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}, \\quad\na_2 = M \\begin{pmatrix}\\cos(\\theta) \\\\ \\sin(\\theta) \\\\ 0\\end{pmatrix}, \\quad\na_3 = N \\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix},\n$$\nwith parameters $M0$, $N0$, and $\\theta \\in (0,\\pi/2)$ satisfying the strict inequality $M \\sin(\\theta)  N  M$. For the pivot selection rule, define “pure norm-based pivoting” to mean selecting columns solely by their original $2$-norms, with no reweighting or downdating of the norms after orthogonalization.\n\n- Using the definitions of QR, singular values, and two-norm condition number, justify that pure norm-based pivoting chooses $a_1$ and $a_2$ as the first two pivots, and derive the leading $2 \\times 2$ upper-triangular block $R_{11}$ associated with these two pivots.\n- Starting from first principles — specifically, that the singular values of $R_{11}$ equal those of the $3 \\times 2$ submatrix formed by the selected columns — compute the exact two-norm condition number $\\kappa_2(R_{11})$ as a closed-form function of $\\theta$.\n- Then, define an “augmented score” for a column $a_j$ after a set of pivots $\\mathcal{S}$ have been chosen as $s_j(\\mathcal{S}) = \\|P_{\\mathcal{S}^\\perp} a_j\\|_2$, where $P_{\\mathcal{S}^\\perp}$ is the orthogonal projector onto the complement of the span of the selected columns. Explain why, under the given inequality $M \\sin(\\theta)  N  M$, this augmented score would select $a_3$ as the second pivot after $a_1$, and why this avoids an ill-conditioned $R_{11}$.\n\nExpress your final answer as the exact analytic expression for $\\kappa_2(R_{11})$ obtained under pure norm-based pivoting, in terms of $\\theta$. No numerical rounding is required.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard example from numerical linear algebra used to demonstrate the potential failure of a naive column pivoting strategy in QR factorization and to motivate more robust rank-revealing methods.\n\nThe problem is addressed in three parts as requested.\n\nFirst, we analyze the \"pure norm-based pivoting\" strategy. This strategy selects columns for the QR factorization based on their original Euclidean ($2$-) norms in descending order. We compute the norms of the three given columns $a_1$, $a_2$, and $a_3$:\n$$\n\\|a_1\\|_2 = \\left\\| M \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} \\right\\|_2 = M \\sqrt{1^2 + 0^2 + 0^2} = M\n$$\n$$\n\\|a_2\\|_2 = \\left\\| M \\begin{pmatrix}\\cos(\\theta) \\\\ \\sin(\\theta) \\\\ 0\\end{pmatrix} \\right\\|_2 = M \\sqrt{\\cos^2(\\theta) + \\sin^2(\\theta) + 0^2} = M \\sqrt{1} = M\n$$\n$$\n\\|a_3\\|_2 = \\left\\| N \\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix} \\right\\|_2 = N \\sqrt{0^2 + 1^2 + 0^2} = N\n$$\nThe problem states the inequality $M \\sin(\\theta)  N  M$. This directly implies that $M  N$. Therefore, we have the ordering of norms: $\\|a_1\\|_2 = \\|a_2\\|_2 = M  N = \\|a_3\\|_2$.\nAccording to the pure norm-based pivoting rule, the columns with the largest norms are chosen first. Since both $a_1$ and $a_2$ have the largest norm, $M$, they will be selected as the first two pivots. The specific order between $a_1$ and $a_2$ depends on a tie-breaking rule, which is typically to choose the column with the smaller index first. Thus, the pivot order is $(a_1, a_2, a_3)$.\n\nThe leading $2 \\times 2$ upper-triangular block, $R_{11}$, is generated by the QR factorization of the submatrix formed by the first two chosen columns, $A_p = [a_1, a_2]$.\nThe problem states that the singular values of $R_{11}$ are equal to the singular values of the $3 \\times 2$ submatrix formed by the selected columns, which is $A_p$. The singular values of $A_p$, denoted by $\\sigma_i$, are the square roots of the eigenvalues of the matrix $A_p^T A_p$.\nWe compute $A_p^T A_p$:\n$$\nA_p^T A_p = \\begin{pmatrix} a_1^T \\\\ a_2^T \\end{pmatrix} \\begin{pmatrix} a_1  a_2 \\end{pmatrix} = \\begin{pmatrix} a_1^T a_1  a_1^T a_2 \\\\ a_2^T a_1  a_2^T a_2 \\end{pmatrix}\n$$\nThe entries are:\n$a_1^T a_1 = \\|a_1\\|_2^2 = M^2$.\n$a_2^T a_2 = \\|a_2\\|_2^2 = M^2$.\n$a_1^T a_2 = M \\begin{pmatrix}1  0  0\\end{pmatrix} M \\begin{pmatrix}\\cos(\\theta) \\\\ \\sin(\\theta) \\\\ 0\\end{pmatrix} = M^2 \\cos(\\theta)$.\nThus,\n$$\nA_p^T A_p = \\begin{pmatrix} M^2  M^2 \\cos(\\theta) \\\\ M^2 \\cos(\\theta)  M^2 \\end{pmatrix} = M^2 \\begin{pmatrix} 1  \\cos(\\theta) \\\\ \\cos(\\theta)  1 \\end{pmatrix}\n$$\nTo find the eigenvalues $\\lambda$ of $A_p^T A_p$, we solve the characteristic equation $\\det(A_p^T A_p - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} M^2 - \\lambda  M^2 \\cos(\\theta) \\\\ M^2 \\cos(\\theta)  M^2 - \\lambda \\end{pmatrix} = (M^2 - \\lambda)^2 - (M^2 \\cos(\\theta))^2 = 0\n$$\n$$\n(M^2 - \\lambda)^2 = M^4 \\cos^2(\\theta)\n$$\n$$\nM^2 - \\lambda = \\pm M^2 \\cos(\\theta)\n$$\n$$\n\\lambda = M^2 (1 \\mp \\cos(\\theta))\n$$\nThe two eigenvalues are $\\lambda_1 = M^2(1 + \\cos(\\theta))$ and $\\lambda_2 = M^2(1 - \\cos(\\theta))$.\nThe singular values of $A_p$ (and thus of $R_{11}$) are the square roots of these eigenvalues. Since $\\theta \\in (0, \\pi/2)$, we have $\\cos(\\theta)  0$, so $\\lambda_1  \\lambda_2$.\nThe largest singular value is $\\sigma_{\\max}(R_{11}) = \\sqrt{\\lambda_1} = M \\sqrt{1 + \\cos(\\theta)}$.\nThe smallest singular value is $\\sigma_{\\min}(R_{11}) = \\sqrt{\\lambda_2} = M \\sqrt{1 - \\cos(\\theta)}$.\n\nThe two-norm condition number of $R_{11}$ is the ratio of its largest to smallest singular value:\n$$\n\\kappa_2(R_{11}) = \\frac{\\sigma_{\\max}(R_{11})}{\\sigma_{\\min}(R_{11})} = \\frac{M \\sqrt{1 + \\cos(\\theta)}}{M \\sqrt{1 - \\cos(\\theta)}} = \\sqrt{\\frac{1 + \\cos(\\theta)}{1 - \\cos(\\theta)}}\n$$\nUsing the half-angle trigonometric identities $1 + \\cos(\\theta) = 2\\cos^2(\\theta/2)$ and $1 - \\cos(\\theta) = 2\\sin^2(\\theta/2)$:\n$$\n\\kappa_2(R_{11}) = \\sqrt{\\frac{2\\cos^2(\\theta/2)}{2\\sin^2(\\theta/2)}} = \\sqrt{\\cot^2(\\theta/2)} = |\\cot(\\theta/2)|\n$$\nSince $\\theta \\in (0, \\pi/2)$, we have $\\theta/2 \\in (0, \\pi/4)$, for which $\\cot(\\theta/2)$ is positive. Therefore,\n$$\n\\kappa_2(R_{11}) = \\cot(\\theta/2)\n$$\nAs $\\theta \\to 0^+$, the columns $a_1$ and $a_2$ become nearly linearly dependent, and $\\kappa_2(R_{11}) \\to \\infty$, indicating severe ill-conditioning.\n\nSecond, we analyze the pivoting strategy based on the \"augmented score\" $s_j(\\mathcal{S}) = \\|P_{\\mathcal{S}^\\perp} a_j\\|_2$. This is the standard pivoting strategy for QR factorization.\nThe first pivot is chosen by maximizing the score with $\\mathcal{S} = \\emptyset$. In this case, $P_{\\emptyset^\\perp} = I$, so $s_j(\\emptyset) = \\|a_j\\|_2$. As before, $\\|a_1\\|_2 = M$, $\\|a_2\\|_2 = M$, $\\|a_3\\|_2 = N$, and $MN$. We choose $a_1$ as the first pivot (by tie-breaking), so $\\mathcal{S}=\\{a_1\\}$.\n\nTo choose the second pivot, we compute the scores for the remaining columns, $a_2$ and $a_3$. The set of selected pivots is $\\mathcal{S}_1 = \\{a_1\\}$. The projector onto the orthogonal complement of the span of $\\{a_1\\}$ is $P_{\\{a_1\\}^\\perp} = I - q_1 q_1^T$, where $q_1 = a_1/\\|a_1\\|_2$.\n$q_1 = \\frac{1}{M} M \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$.\nThe score for $a_2$ is $s_2(\\{a_1\\}) = \\|a_2 - (q_1^T a_2) q_1\\|_2$.\n$q_1^T a_2 = \\begin{pmatrix}1  0  0\\end{pmatrix} M \\begin{pmatrix}\\cos(\\theta) \\\\ \\sin(\\theta) \\\\ 0\\end{pmatrix} = M\\cos(\\theta)$.\nSo, the component of $a_2$ orthogonal to $a_1$ is:\n$a_2 - (M\\cos(\\theta)) q_1 = M \\begin{pmatrix}\\cos(\\theta) \\\\ \\sin(\\theta) \\\\ 0\\end{pmatrix} - M\\cos(\\theta) \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = M \\begin{pmatrix}0 \\\\ \\sin(\\theta) \\\\ 0\\end{pmatrix}$.\nThe score is its norm:\n$s_2(\\{a_1\\}) = \\|M \\begin{pmatrix}0 \\\\ \\sin(\\theta) \\\\ 0\\end{pmatrix}\\|_2 = M\\sin(\\theta)$.\n\nThe score for $a_3$ is $s_3(\\{a_1\\}) = \\|a_3 - (q_1^T a_3) q_1\\|_2$.\n$q_1^T a_3 = \\begin{pmatrix}1  0  0\\end{pmatrix} N \\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix} = 0$.\nSo, the component of $a_3$ orthogonal to $a_1$ is simply $a_3$.\nThe score is its norm:\n$s_3(\\{a_1\\}) = \\|a_3\\|_2 = N$.\n\nTo select the second pivot, we compare $s_2(\\{a_1\\}) = M\\sin(\\theta)$ with $s_3(\\{a_1\\}) = N$. The problem provides the inequality $M\\sin(\\theta)  N$. This means $s_3(\\{a_1\\})  s_2(\\{a_1\\})$, so the augmented score strategy selects $a_3$ as the second pivot.\n\nThis choice avoids an ill-conditioned $R_{11}$ block. The new submatrix of selected columns is $A'_p = [a_1, a_3]$. The columns $a_1$ and $a_3$ are orthogonal, as shown by $a_1^T a_3 = 0$. The QR factorization of $A'_p$ is straightforward. The resulting upper triangular matrix, let's call it $R'_{11}$, will be diagonal:\n$$\nR'_{11} = \\begin{pmatrix} \\|a_1\\|_2  q_1^T a_3 \\\\ 0  \\|a_3 - (q_1^T a_3)q_1\\|_2 \\end{pmatrix} = \\begin{pmatrix} \\|a_1\\|_2  0 \\\\ 0  \\|a_3\\|_2 \\end{pmatrix} = \\begin{pmatrix} M  0 \\\\ 0  N \\end{pmatrix}\n$$\nThe singular values of this diagonal matrix are simply the absolute values of its diagonal entries, $M$ and $N$. The condition number is:\n$$\n\\kappa_2(R'_{11}) = \\frac{\\max(M, N)}{\\min(M, N)} = \\frac{M}{N}\n$$\nFrom the given inequality $M\\sin(\\theta)  N  M$, we know $1  M/N  1/\\sin(\\theta)$. For a small $\\theta$, where the pure-norm pivot choice becomes highly ill-conditioned ($\\kappa_2 \\approx 2/\\theta$), this strategy gives a condition number bounded by $1/\\sin(\\theta) \\approx 1/\\theta$. While also growing, it is significantly smaller. More importantly, this method correctly identifies that $a_2$ provides almost no new directional information after $a_1$ is chosen (when $\\theta$ is small), while $a_3$ is entirely new. The augmented score correctly measures the contribution of a new vector to the orthogonal basis being built, thus revealing the rank structure more reliably and avoiding the selection of nearly-dependent columns.",
            "answer": "$$\n\\boxed{\\cot\\left(\\frac{\\theta}{2}\\right)}\n$$"
        },
        {
            "introduction": "No numerical method is universally superior; its value is determined by its fitness for a particular task. This problem contrasts the rank-revealing capabilities of QR factorization with those of another workhorse of linear algebra, LU factorization with partial pivoting (GEPP) . You will analyze a matrix specifically constructed to show how uncontrolled element growth in the $U$ factor of GEPP can obscure a matrix's near-rank-deficiency, while the norm-preserving property of orthogonal transformations used in QR provides a clear and reliable signal of the numerical rank.",
            "id": "3571775",
            "problem": "Let $n \\geq 3$ and let $0  \\epsilon  1$ be fixed. Consider the $n \\times n$ matrix $A$ constructed as follows. Define the first $n-1$ columns so that their top $(n-1) \\times (n-1)$ block is unit lower triangular with entries $A_{ij} = 0$ for $i  j$, $A_{jj} = 1$ for $1 \\leq j \\leq n-1$, and $A_{ij} = -1$ for $j  i \\leq n-1$. Set the last row of these first $n-1$ columns to zero: $A_{n,j} = 0$ for $1 \\leq j \\leq n-1$. Let the last column be $c$ with $c_i = 1$ for $1 \\leq i \\leq n-1$ and $c_n = \\epsilon$. Thus\n$$\nA = \\begin{bmatrix}\n1  0  0  \\cdots  0  1 \\\\\n-1  1  0  \\cdots  0  1 \\\\\n-1  -1  1  \\cdots  0  1 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots  \\vdots \\\\\n-1  -1  -1  \\cdots  1  1 \\\\\n0  0  0  \\cdots  0  \\epsilon\n\\end{bmatrix}.\n$$\n\nFrom first principles:\n\n1. Use the definition of Gaussian Elimination with Partial Pivoting (GEPP) to determine the pivot growth factor, defined as $\\gamma(A) = \\dfrac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |A_{ij}|}$, where $P A = L U$ is the LU factorization with row permutation $P$ chosen by partial pivoting, $L$ is unit lower triangular, and $U$ is upper triangular.\n\n2. Use the definition of Column-Pivoted QR (CPQR) and of Rank-Revealing QR (RRQR) to argue that, under the column permutation that leaves the first $n-1$ columns in place and places the last column $c$ last, the last diagonal entry of the triangular factor $R$ in the QR factorization $A \\Pi = Q R$ equals the Euclidean norm of the component of $c$ orthogonal to the column space of the first $n-1$ columns. Determine this value explicitly.\n\nCompute, in closed form, the ratio\n$$\n\\rho(n,\\epsilon) = \\frac{\\gamma(A)}{|R_{nn}|}.\n$$\nYour final answer must be a single analytic expression in $n$ and $\\epsilon$. No rounding is required. Express any powers explicitly; angles are not involved, and there are no physical units in this problem.",
            "solution": "The user wants to find the ratio of the pivot growth factor from Gaussian Elimination with Partial Pivoting (GEPP) to the magnitude of the last diagonal entry of the triangular factor $R$ from a specific QR factorization for a given $n \\times n$ matrix $A$.\n\n### Step 1: Problem Validation\n\n**Extracted Givens:**\n- An $n \\times n$ matrix $A$ with $n \\geq 3$.\n- A fixed value $0  \\epsilon  1$.\n- The matrix $A$ is defined as:\n  - For $1 \\le j \\le n-1$:\n    - The top $(n-1) \\times (n-1)$ block is unit lower triangular:\n      - $A_{ij} = 0$ for $i  j$.\n      - $A_{jj} = 1$.\n      - $A_{ij} = -1$ for $j  i \\leq n-1$.\n    - The last row is zero: $A_{n,j} = 0$.\n  - The last column, $c$, has entries $c_i = 1$ for $1 \\leq i \\leq n-1$ and $c_n = \\epsilon$.\n- The explicit form of matrix $A$ is given as:\n$$\nA = \\begin{bmatrix}\n1  0  0  \\cdots  0  1 \\\\\n-1  1  0  \\cdots  0  1 \\\\\n-1  -1  1  \\cdots  0  1 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots  \\vdots \\\\\n-1  -1  -1  \\cdots  1  1 \\\\\n0  0  0  \\cdots  0  \\epsilon\n\\end{bmatrix}\n$$\n- The pivot growth factor is defined as $\\gamma(A) = \\dfrac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |A_{ij}|}$, where $PA=LU$ from GEPP.\n- The QR factorization is $A \\Pi = Q R$ with column permutation $\\Pi$ that leaves the first $n-1$ columns in place and the last column last, which implies $\\Pi = I$.\n- The final quantity to compute is the ratio $\\rho(n,\\epsilon) = \\frac{\\gamma(A)}{|R_{nn}|}$.\n\n**Validation using Extracted Givens:**\n- **Scientifically Grounded:** The problem is rooted in fundamental concepts of numerical linear algebra, namely LU and QR factorizations, pivot growth, and matrix conditioning. All concepts are standard and well-defined.\n- **Well-Posed:** The matrix $A$ is explicitly defined. The tasks are specific and lead to a unique, derivable result. The parameters $n$ and $\\epsilon$ are constrained.\n- **Objective:** The problem is stated in precise mathematical language, free from subjectivity or ambiguity.\n\nThe problem is self-contained, consistent, and scientifically sound. No flaws are identified. It is therefore deemed **valid**.\n\n### Step 2: Solution\n\nThe solution proceeds in three parts: first, computing the growth factor $\\gamma(A)$; second, computing the value of $|R_{nn}|$; and third, finding their ratio $\\rho(n,\\epsilon)$.\n\n**Part 1: Pivot Growth Factor $\\gamma(A)$**\n\nThe growth factor is $\\gamma(A) = \\frac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |A_{ij}|}$.\nFirst, we find the denominator. The entries of $A$ are $0, 1, -1, \\epsilon$. Since $0  \\epsilon  1$, the maximum absolute value among the entries of $A$ is $\\max_{i,j} |A_{ij}| = 1$.\n\nNext, we find the numerator by performing Gaussian Elimination with Partial Pivoting (GEPP) to find the upper triangular matrix $U$. Let $A^{(1)} = A$.\n\nAt step $k=1$: The first column is $[1, -1, \\dots, -1, 0]^T$. The largest absolute value is $1$. We can choose $A_{11}=1$ as the pivot. No row permutation is necessary. The multipliers for rows $i=2, \\dots, n-1$ are $m_{i1} = A_{i1}/A_{11} = -1/1 = -1$. For row $n$, $m_{n1} = A_{n1}/A_{11} = 0/1 = 0$.\nThe update rule is $R_i \\leftarrow R_i - m_{i1} R_1$. For $i=2, \\dots, n-1$, this is $R_i \\leftarrow R_i + R_1$. Row $n$ is unchanged.\nLet's examine the last column of the resulting matrix $A^{(2)}$. For $i=2, \\dots, n-1$, the new entry is $A_{in}^{(2)} = A_{in}^{(1)} + A_{1n}^{(1)} = 1+1=2$. The rest of the submatrix for columns $j=2, \\dots, n-1$ and rows $i=2, \\dots, n-1$ retains the same structure as the original matrix but is one size smaller.\n\nAt step $k=2$: The relevant submatrix (from row 2, col 2) has $[1, -1, \\dots, -1, 0]^T$ as its first column. The pivot is $A_{22}^{(2)}=1$. No permutation is needed. The multipliers are $m_{i2} = A_{i2}^{(2)}/A_{22}^{(2)} = -1/1 = -1$ for $i=3, \\dots, n-1$.\nThe update for the last column for $i=3, \\dots, n-1$ is $A_{in}^{(3)} = A_{in}^{(2)} + A_{2n}^{(2)} = 2+2=4$.\n\nWe can observe a pattern. At each step $k=1, \\dots, n-2$, the pivot element is $1$, so no row interchanges occur ($P=I$). The entries of the upper triangular matrix $U$ are the pivot rows at each step.\n$U_{1,n} = A_{1,n} = 1 = 2^0$.\nAt step 1, we set $U_{1,:}=A_{1,:}$. The rows $i \\ge 2$ of the working matrix are updated such that their last element becomes $2$. Thus, the pivot row at step 2 (row 2) will have $2$ as its last element. So, $U_{2,n} = 2^1$.\nAt step 2, the rows $i \\ge 3$ are updated such that their last element becomes $2+2=4$. The pivot row at step 3 will have $4$ as its last element. So, $U_{3,n} = 4 = 2^2$.\nThis continues until step $k$. The last element in the pivot row $k$ becomes $U_{k,n} = 2^{k-1}$. This holds for $k=1, \\dots, n-1$.\nThe last row of $A$ is $[0, \\dots, 0, \\epsilon]$. Since all multipliers for this row, $m_{nk}$, are zero at every step, this row is never modified.\nThe resulting upper triangular matrix $U$ is:\n$$\nU = \\begin{bmatrix}\n1  0  0  \\cdots  0  1 \\\\\n0  1  0  \\cdots  0  2 \\\\\n0  0  1  \\cdots  0  4 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots  \\vdots \\\\\n0  0  0  \\cdots  1  2^{n-2} \\\\\n0  0  0  \\cdots  0  \\epsilon\n\\end{bmatrix}\n$$\nThe entries of $U$ are $0, 1, \\epsilon$, and $2^k$ for $k \\in \\{0, 1, \\dots, n-2\\}$.\nSince $n \\geq 3$, the maximum exponent is at least $1$. The maximum absolute value is therefore $\\max_{i,j} |U_{ij}| = 2^{n-2}$.\nThe growth factor is $\\gamma(A) = \\frac{2^{n-2}}{1} = 2^{n-2}$.\n\n**Part 2: Calculation of $|R_{nn}|$**\n\nThe problem considers the QR factorization $A\\Pi = QR$ where $\\Pi=I$. Thus, $A=QR$.\nLet the columns of $A$ be $a_1, \\dots, a_{n-1}, c$. Let $W = \\text{span}\\{a_1, \\dots, a_{n-1}\\}$.\nFrom the theory of QR factorization (e.g., via Gram-Schmidt process), the diagonal entry $|R_{nn}|$ is the Euclidean norm of the component of the last column, $c$, that is orthogonal to the subspace $W$ spanned by the first $n-1$ columns.\nLet $c_{\\perp W}$ be this orthogonal component. Then $|R_{nn}| = \\|c_{\\perp W}\\|$.\n$c_{\\perp W} = c - \\text{proj}_W c$, where $\\text{proj}_W c$ is the orthogonal projection of $c$ onto $W$.\nThe projection $\\text{proj}_W c$ is the vector $y \\in W$ that minimizes $\\|y - c\\|$. Since $y \\in W$, it can be written as $y = A_{n-1}x$ for some vector $x \\in \\mathbb{R}^{n-1}$, where $A_{n-1} = [a_1, \\dots, a_{n-1}]$.\nThe vector $c - y$ must be orthogonal to $W$, meaning $(c-y)^T a_j = 0$ for all $j=1, \\dots, n-1$.\n\nLet's find the vector $y = A_{n-1}x$.\n$y_i = (A_{n-1}x)_i = x_i - \\sum_{j=1}^{i-1} x_j$ for $i=1, \\dots, n-1$.\n$y_n = 0$.\nThe vector $c$ is given by $c_i=1$ for $i=1, \\dots, n-1$ and $c_n=\\epsilon$.\nLet us propose a candidate for the projection. Consider the vector $y^* = [1, 1, \\dots, 1, 0]^T$.\nIf $y^*$ is the projection, then it must be in $W$ and $c-y^*$ must be orthogonal to $W$.\nFirst, let's check if $y^* \\in W$. This requires finding $x \\in \\mathbb{R}^{n-1}$ such that $A_{n-1}x = y^*$.\n- $x_1 = y_1^* = 1$.\n- $-x_1+x_2 = y_2^* = 1 \\implies x_2 = 1+x_1 = 2$.\n- $-x_1-x_2+x_3 = y_3^* = 1 \\implies x_3 = 1+x_1+x_2 = 1+1+2=4$.\nThe general recursion is $x_i = 1 + \\sum_{j=1}^{i-1} x_j$.\nAssuming $x_j=2^{j-1}$ for $ji$, then $\\sum_{j=1}^{i-1} x_j = \\sum_{j=0}^{i-2} 2^j = 2^{i-1}-1$.\nThis gives $x_i = 1 + (2^{i-1}-1) = 2^{i-1}$.\nBy induction, $x_i = 2^{i-1}$ for $i=1, \\dots, n-1$. Such an $x$ exists, so $y^* \\in W$.\n\nNext, we check if the residual $r = c-y^*$ is orthogonal to $W$.\n$r = c - y^* = [1, \\dots, 1, \\epsilon]^T - [1, \\dots, 1, 0]^T = [0, \\dots, 0, \\epsilon]^T = \\epsilon e_n$, where $e_n$ is the $n$-th standard basis vector.\nThe orthogonality condition is $r^T a_j = 0$ for $j=1, \\dots, n-1$.\n$r^T a_j = (\\epsilon e_n)^T a_j = \\epsilon (e_n^T a_j) = \\epsilon A_{nj}$.\nFrom the definition of matrix $A$, $A_{nj} = 0$ for all $j=1, \\dots, n-1$.\nSo, the orthogonality condition is satisfied. $y^*$ is indeed the orthogonal projection of $c$ onto $W$.\nThe component of $c$ orthogonal to $W$ is $c - y^* = \\epsilon e_n$.\nThe norm is $|R_{nn}| = \\|\\epsilon e_n\\| = \\sqrt{\\epsilon^2} = |\\epsilon|$.\nSince the problem states $0  \\epsilon  1$, we have $|R_{nn}| = \\epsilon$.\n\n**Part 3: The Ratio $\\rho(n,\\epsilon)$**\n\nWe can now compute the desired ratio:\n$$\n\\rho(n,\\epsilon) = \\frac{\\gamma(A)}{|R_{nn}|} = \\frac{2^{n-2}}{\\epsilon}\n$$\nThis is the final closed-form expression in terms of $n$ and $\\epsilon$.",
            "answer": "$$\\boxed{\\frac{2^{n-2}}{\\epsilon}}$$"
        }
    ]
}