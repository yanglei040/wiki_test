{
    "hands_on_practices": [
        {
            "introduction": "Modern computing hardware often offers different levels of floating-point precision, creating a trade-off between performance and accuracy. This practice explores how using low-precision arithmetic for the computationally intensive matrix-vector product, while retaining high precision for the sensitive orthogonalization step, impacts the accuracy of the computed Ritz values. By implementing a mixed-precision Arnoldi scheme and testing it on carefully constructed matrices, you will gain direct experience with the practical challenges of numerical eigenvalue problems, including the effects of matrix non-normality, eigenvalue clustering, and floating-point range limitations .",
            "id": "3584299",
            "problem": "You are to design and implement a mixed-precision Arnoldi iteration to construct the Arnoldi decomposition for a real square matrix $A \\in \\mathbb{R}^{n \\times n}$, using a half-precision floating-point format for the matrix-vector product and double precision for all orthogonalization steps. The mixed-precision scheme must adhere to the following principle: at iteration $j$, given a unit vector $q_j \\in \\mathbb{R}^n$, compute the new vector $w$ that enters the Arnoldi orthogonalization stage by performing the product using half precision as $w = \\operatorname{float64}\\!\\left(\\operatorname{float16}(A) \\cdot \\operatorname{float16}(q_j)\\right)$, and then complete the modified Gram–Schmidt process with reorthogonalization entirely in double precision. Here, $\\operatorname{float16}(\\cdot)$ denotes quantization to the half-precision floating-point format and $\\operatorname{float64}(\\cdot)$ denotes casting back to double precision.\n\nBase your derivation and implementation on the following foundational definitions and facts only:\n- The Arnoldi process builds an orthonormal basis $V_k = [q_1,\\dots,q_k] \\in \\mathbb{R}^{n \\times k}$ for the Krylov subspace $\\mathcal{K}_k(A,q_1) = \\operatorname{span}\\{q_1, A q_1, \\dots, A^{k-1} q_1\\}$ and a real upper-Hessenberg matrix $H_k \\in \\mathbb{R}^{k \\times k}$ such that $A V_k \\approx V_k H_k + h_{k+1,k} q_{k+1} e_k^\\top$, where $e_k \\in \\mathbb{R}^k$ is the $k$-th standard basis vector and $h_{k+1,k} \\ge 0$ is the subdiagonal entry that determines whether the process breaks down.\n- The Ritz values are the eigenvalues of $H_k$, and, in exact arithmetic with $k = n$, they coincide with the eigenvalues of $A$ because $H_n = V_n^\\top A V_n$ is orthogonally similar to $A$.\n\nYour program must:\n1. Implement the mixed-precision Arnoldi iteration exactly as specified above, using modified Gram–Schmidt with one step of reorthogonalization entirely in double precision to build $V_k$ and $H_k$.\n2. For each test case described below, run $k$ steps (or terminate early if $h_{j+1,j} = 0$), compute the Ritz values (the eigenvalues of $H_k$) in double precision, and compare them to the true eigenvalues of $A$ computed in double precision. Use the minimal one-to-one assignment of Ritz values to true eigenvalues that minimizes the sum of absolute differences, and report whether the maximum absolute difference among matched pairs is within a prescribed tolerance.\n3. If any non-finite numbers (Not-a-Number or infinity) arise during the process for a test case, report failure for that test case.\n\nYou must use a fixed initial vector $q_1$ drawn from a reproducible distribution: let its entries be independently sampled from the standard normal distribution and then normalized to unit length, with the random seed set to a fixed integer.\n\nExplain, through your implementation and your reasoning, why the scheme can succeed or fail depending on the matrix, using only the foundational facts above and sound numerical reasoning. You must not introduce any external formulas that shortcut the reasoning demanded by the above facts.\n\nMatrix families and test suite:\n- For each case below, construct the matrix $A$ exactly as prescribed, select $n$ and $k$, and use the given tolerance $\\tau$. The output for each case is a boolean indicating whether the maximum absolute discrepancy between the matched Ritz values and the true eigenvalues of $A$ is at most $\\tau$.\n- The five test cases are:\n  1. Case name: SPD_tridiag_good. Construct $A \\in \\mathbb{R}^{n \\times n}$ as the symmetric tridiagonal matrix with main diagonal entries equal to $1$ and first off-diagonals equal to $-1/2$; that is, $A = \\operatorname{tridiag}(-\\tfrac{1}{2}, 1, -\\tfrac{1}{2})$. Use $n = 30$, $k = n$, and tolerance $\\tau = 5 \\cdot 10^{-3}$.\n  2. Case name: Diagonal_fp16_exact_good. Construct $A = \\operatorname{diag}(d_1,\\dots,d_n)$ with entries cycling through the set $\\{1, \\tfrac{1}{2}, \\tfrac{1}{4}, \\tfrac{1}{8}, \\tfrac{1}{16}\\}$, multiplied by alternating signs, i.e., $d_i = s_i \\cdot 2^{-r_i}$ where $s_i \\in \\{+1,-1\\}$ alternates with $i$ and $r_i$ cycles through $\\{0,1,2,3,4\\}$. Use $n = 25$, $k = n$, and tolerance $\\tau = 5 \\cdot 10^{-3}$.\n  3. Case name: NonNormal_shift_fail. Construct $A = D + \\alpha N$ where $D = \\operatorname{diag}(d_1,\\dots,d_n)$ with $d_i$ linearly spaced in $[0.9, 1.1]$, $N \\in \\mathbb{R}^{n \\times n}$ has ones on the first superdiagonal and zeros elsewhere, and $\\alpha = 20$. Use $n = 30$, $k = n$, and tolerance $\\tau = 10^{-2}$.\n  4. Case name: Clustered_diag_fail. Construct $A = \\operatorname{diag}(1 + i \\cdot 10^{-4})$ for $i = 0,1,\\dots,n-1$. Use $n = 30$, $k = n$, and tolerance $\\tau = 3 \\cdot 10^{-4}$.\n  5. Case name: Overflow_fp16_fail. Construct $A = s I + N$ where $s = 70000$, $I$ is the identity, and $N$ has ones on the first superdiagonal and zeros elsewhere. Use $n = 20$, $k = n$, and tolerance $\\tau = 10^{-1}$.\n\nDistance and matching specification:\n- Let $\\{\\theta_1,\\dots,\\theta_m\\}$ be the Ritz values obtained from $H_m$ after performing $m \\le k$ steps (with $m = k$ if no breakdown). Let $\\{\\lambda_1,\\dots,\\lambda_n\\}$ be the eigenvalues of $A$ in double precision. Compute a minimal-cost matching between $\\{\\theta_j\\}_{j=1}^m$ and a size-$m$ subset of $\\{\\lambda_i\\}_{i=1}^n$ that minimizes $\\sum_{j=1}^m \\lvert \\theta_j - \\lambda_{\\pi(j)} \\rvert$, where $\\pi$ is an injective mapping. Report success if $m = k$, all numbers involved are finite, and $\\max_{1 \\le j \\le m} \\lvert \\theta_j - \\lambda_{\\pi(j)} \\rvert \\le \\tau$.\n\nAngle units are not involved. No physical units are involved. All answers are unitless numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the five test cases as a comma-separated Python-style list of booleans enclosed in square brackets (e.g., \"[True,False,True,False,True]\").\n\nYour submission must be a complete, runnable program that constructs the matrices as specified, runs the mixed-precision Arnoldi scheme, computes the Ritz values, performs the matching, evaluates the tolerance test, and prints the final list in the exact format described above. No user input is permitted, and no randomness other than the specified fixed-seed initialization of $q_1$ may be used.",
            "solution": "We start from the Arnoldi construction and the definition of Ritz values. Given $A \\in \\mathbb{R}^{n \\times n}$ and a unit vector $q_1 \\in \\mathbb{R}^n$, the Arnoldi process builds orthonormal vectors $q_1,\\dots,q_k$ and scalars $h_{i,j}$ forming $H_k \\in \\mathbb{R}^{k \\times k}$ with upper-Hessenberg structure so that\n$$\nA V_k = V_k H_k + h_{k+1,k} q_{k+1} e_k^\\top,\n$$\nwhere $V_k = [q_1,\\dots,q_k] \\in \\mathbb{R}^{n \\times k}$ has orthonormal columns and $e_k \\in \\mathbb{R}^k$ is the $k$-th basis vector. The eigenvalues of $H_k$ are the Ritz values and approximate eigenvalues of $A$. In exact arithmetic with $k = n$, $V_n$ is an orthonormal basis and $H_n = V_n^\\top A V_n$ is orthogonally similar to $A$, hence it has exactly the eigenvalues of $A$.\n\nWe now design a mixed-precision scheme consistent with these principles. The matrix-vector product $A q_j$ is the sole step that we perform at reduced precision to simulate hardware where application of $A$ is fast but low precision, while we preserve stability of orthogonalization in high precision. Specifically, given $q_j$ in double precision, we compute\n$$\nw = \\operatorname{float64}\\!\\left(\\operatorname{float16}(A) \\cdot \\operatorname{float16}(q_j)\\right),\n$$\nthen orthogonalize $w$ against the current basis vectors $\\{q_i\\}_{i=1}^j$ using modified Gram–Schmidt in double precision:\n$$\nh_{i,j} = q_i^\\top w, \\quad w \\leftarrow w - h_{i,j} q_i \\quad \\text{for } i = 1,\\dots,j,\n$$\nand reorthogonalize once to damp accumulation of roundoff:\n$$\n\\hat{h}_{i,j} = q_i^\\top w, \\quad h_{i,j} \\leftarrow h_{i,j} + \\hat{h}_{i,j}, \\quad w \\leftarrow w - \\hat{h}_{i,j} q_i \\quad \\text{for } i = 1,\\dots,j.\n$$\nThen we set $h_{j+1,j} = \\lVert w \\rVert_2$ in double precision. If $h_{j+1,j} = 0$, we declare breakdown and stop; otherwise, $q_{j+1} = w / h_{j+1,j}$. The resulting $H_k$ is upper-Hessenberg with entries $h_{i,j}$ on and above the subdiagonal.\n\nThe rationale for mixed precision is as follows. Using half precision for $A q_j$ introduces perturbations typical of a finite-precision linear operator application. If we denote $\\widetilde{A} = \\operatorname{float16}(A)$, and $q_j^{(16)} = \\operatorname{float16}(q_j)$, then the actual vector entering orthogonalization is $w = \\widetilde{A} q_j^{(16)}$ converted back to double. There are two sources of perturbation relative to exact arithmetic: (i) replacing $A$ by $\\widetilde{A}$ (a linear operator perturbation), and (ii) replacing $q_j$ by $q_j^{(16)}$ (introducing nonlinearity because each iteration sees $q_j$ rounded differently). However, modified Gram–Schmidt in double with reorthogonalization keeps the basis vectors close to orthonormal, controlling loss of orthogonality that would otherwise pollute $H_k$.\n\nTo evaluate accuracy, we compare the Ritz values (the eigenvalues of $H_k$) to the true eigenvalues of $A$ computed in double precision. Since the Arnoldi projection targets certain parts of the spectrum depending on the initial vector and subspace, we enforce $k = n$ in all cases so that in exact arithmetic $H_n$ is orthogonally similar to $A$. In mixed precision, deviations arise from the perturbed products, but for well-conditioned and well-scaled matrices, the eigenvalues of $H_n$ should remain close to those of $A$.\n\nWe now justify expected success and failure modes:\n- Success for symmetric, well-scaled problems with entries representable in half precision: If $A$ has entries exactly representable in half precision (for example, a symmetric tridiagonal with main diagonal $1$ and off-diagonal $-1/2$), then $\\widetilde{A} = A$. The only remaining matvec error arises from rounding $q_j$ to half precision before the product. This per-iteration perturbation is on the order of machine epsilon for half precision, approximately $\\varepsilon_{16} \\approx 2^{-10} \\approx 10^{-3}$. With $k = n$ and double-precision orthogonalization, the accumulated effect on $H_n$’s spectrum is generally modest for well-conditioned matrices, so Ritz values match true eigenvalues within a tolerance on the order of a few multiples of $\\varepsilon_{16}$, for example $5 \\cdot 10^{-3}$.\n- Failure for highly non-normal matrices: For $A = D + \\alpha N$ with $N$ strictly upper-shift and $\\alpha$ large, $A$ has eigenvalues $\\{d_i\\}$ but is highly non-normal. The eigenvalues are extremely sensitive to perturbations; small non-normal perturbations of the operator or the iteration may cause Ritz values to deviate significantly from the true eigenvalues even when $k = n$. The pseudospectrum of such $A$ is large, and the mixed-precision matvec perturbs $A$ effectively by an amount that, when measured in a non-normal basis, can yield differences larger than $10^{-2}$.\n- Failure for tightly clustered eigenvalues: If $A$ is diagonal with entries $1 + i \\cdot 10^{-4}$, then eigenvalues are spaced by $10^{-4}$. Half precision has unit roundoff near $1$ of approximately $10^{-3}$, which exceeds the clustering gap. The mixed-precision matvec cannot resolve eigenvalue differences at that scale reliably, so the matched Ritz values will not be within $3 \\cdot 10^{-4}$ of the true eigenvalues.\n- Failure due to overflow in half precision: Half precision has a largest finite magnitude approximately $6.5504 \\cdot 10^{4}$. If we scale $A$ by $s = 70000$, then $\\operatorname{float16}(A)$ contains infinities on the diagonal. The subsequent matvecs produce non-finite values, and the process fails.\n\nAlgorithmic design:\n1. Implement the mixed-precision Arnoldi iteration with the matvec computed as $w = \\operatorname{float64}\\!\\big(\\operatorname{float16}(A) \\cdot \\operatorname{float16}(q_j)\\big)$.\n2. Use modified Gram–Schmidt with reorthogonalization in double precision to compute $H_k$ and $V_k$.\n3. Compute the Ritz values as the eigenvalues of $H_k$ in double precision.\n4. Compute the true eigenvalues of $A$ in double precision.\n5. Solve a minimal assignment problem to match Ritz values to eigenvalues, minimizing the sum of absolute differences, and compute the maximum absolute difference over matched pairs.\n6. Declare success if there is no breakdown ($m = k$), all intermediate values are finite, and the maximum difference is at most the tolerance.\n\nTest suite coverage:\n- SPD_tridiag_good exercises the “happy path” with exact half-precision representability and favorable conditioning.\n- Diagonal_fp16_exact_good tests correctness when $A$ is diagonal with values exactly representable in half precision, stressing only the effect of rounding $q_j$ each iteration.\n- NonNormal_shift_fail exercises non-normal sensitivity.\n- Clustered_diag_fail is a boundary case against the half-precision unit roundoff scale.\n- Overflow_fp16_fail tests catastrophic failure due to the half-precision dynamic range limit.\n\nThe final program constructs each matrix explicitly, runs the mixed-precision Arnoldi iteration with a fixed-seed initial vector, performs the matching and tolerance check, and prints a single line with the list of boolean results in the required format.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.optimize import linear_sum_assignment\n\ndef arnoldi_mixed_precision(A, k, seed=42):\n    \"\"\"\n    Mixed-precision Arnoldi:\n    - Matvec w = float64( float16(A) @ float16(q_j) )\n    - Orthogonalization in float64 with MGS + reorthogonalization.\n    Returns V (n x m), H (m x m), m actual steps performed.\n    Stops early on breakdown or non-finite values.\n    \"\"\"\n    n = A.shape[0]\n    # Initial vector q1: random normal with fixed seed, normalized\n    rng = np.random.default_rng(seed)\n    q = rng.standard_normal(n)\n    q_norm = np.linalg.norm(q)\n    if not np.isfinite(q_norm) or q_norm == 0.0:\n        return None, None, 0\n    q = q / q_norm\n\n    # Prepare float16 version of A once (as per scheme)\n    A_fp16 = A.astype(np.float16)\n\n    V = np.zeros((n, k), dtype=np.float64)\n    H = np.zeros((k, k), dtype=np.float64)\n\n    V[:, 0] = q\n\n    for j in range(k):\n        # Compute w = A q_j using half precision for both A and q_j\n        try:\n            qj_fp16 = V[:, j].astype(np.float16)\n            w_fp16 = A_fp16 @ qj_fp16\n        except Exception:\n            # In case of incompatible sizes or others\n            return V[:, :j], H[:j, :j], j\n        w = w_fp16.astype(np.float64)\n\n        # Check finiteness\n        if not np.all(np.isfinite(w)):\n            return V[:, :j], H[:j, :j], j\n\n        # Modified Gram-Schmidt\n        for i in range(j + 1):\n            hij = np.dot(V[:, i], w)\n            H[i, j] += hij\n            w = w - hij * V[:, i]\n\n        # Reorthogonalization\n        for i in range(j + 1):\n            hij = np.dot(V[:, i], w)\n            H[i, j] += hij\n            w = w - hij * V[:, i]\n\n        h_next = np.linalg.norm(w)\n        if not np.isfinite(h_next):\n            return V[:, :j], H[:j, :j], j\n        if j + 1 < k:\n            H[j + 1, j] = h_next\n\n        if h_next == 0.0 or j + 1 == k:\n            # Breakdown or finish\n            m = j + 1\n            return V[:, :m], H[:m, :m], m\n        V[:, j + 1] = w / h_next\n\n    # Normally not reached\n    m = k\n    return V[:, :m], H[:m, :m], m\n\n\ndef construct_matrix(case_name, n):\n    if case_name == \"SPD_tridiag_good\":\n        A = np.zeros((n, n), dtype=np.float64)\n        diag = 1.0\n        off = -0.5\n        np.fill_diagonal(A, diag)\n        idx = np.arange(n - 1)\n        A[idx, idx + 1] = off\n        A[idx + 1, idx] = off\n        return A\n    elif case_name == \"Diagonal_fp16_exact_good\":\n        vals = []\n        signs = [1.0, -1.0]\n        powers = [0, 1, 2, 3, 4]  # 1, 1/2, 1/4, 1/8, 1/16\n        for i in range(n):\n            s = signs[i % 2]\n            r = powers[i % len(powers)]\n            vals.append(s * (2.0 ** (-r)))\n        return np.diag(np.array(vals, dtype=np.float64))\n    elif case_name == \"NonNormal_shift_fail\":\n        d = np.linspace(0.9, 1.1, n)\n        D = np.diag(d)\n        N = np.zeros((n, n), dtype=np.float64)\n        idx = np.arange(n - 1)\n        N[idx, idx + 1] = 1.0\n        alpha = 20.0\n        return D + alpha * N\n    elif case_name == \"Clustered_diag_fail\":\n        d = 1.0 + (np.arange(n, dtype=np.float64)) * 1e-4\n        return np.diag(d)\n    elif case_name == \"Overflow_fp16_fail\":\n        A = np.zeros((n, n), dtype=np.float64)\n        s = 70000.0\n        np.fill_diagonal(A, s)\n        idx = np.arange(n - 1)\n        A[idx, idx + 1] = 1.0\n        return A\n    else:\n        raise ValueError(\"Unknown case name\")\n\n\ndef match_and_max_diff(ritz_vals, true_eigs):\n    \"\"\"\n    Match each Ritz value to a distinct true eigenvalue minimizing total absolute difference.\n    Returns max absolute difference among matched pairs.\n    \"\"\"\n    r = np.array(ritz_vals, dtype=np.complex128)\n    lam = np.array(true_eigs, dtype=np.complex128)\n\n    # Cost matrix: absolute differences\n    cost = np.abs(r[:, None] - lam[None, :])\n    # Solve assignment: assign each Ritz to a unique eigenvalue\n    row_ind, col_ind = linear_sum_assignment(cost)\n    diffs = cost[row_ind, col_ind]\n    max_diff = np.max(diffs) if diffs.size > 0 else np.inf\n    return float(np.real(max_diff))\n\n\ndef evaluate_case(case_name, n, k, tol, seed=12345):\n    A = construct_matrix(case_name, n)\n    # Run mixed-precision Arnoldi\n    V, H, m = arnoldi_mixed_precision(A, k, seed=seed)\n\n    # Conditions for failure\n    if V is None or H is None:\n        return False\n    if m != k:\n        return False\n    if not np.all(np.isfinite(H)) or not np.all(np.isfinite(V)):\n        return False\n\n    # Compute Ritz values: eigenvalues of H\n    try:\n        ritz_vals = linalg.eigvals(H)\n    except Exception:\n        return False\n    if not np.all(np.isfinite(ritz_vals)):\n        return False\n\n    # True eigenvalues of A in double precision\n    try:\n        true_eigs = linalg.eigvals(A)\n    except Exception:\n        return False\n    if not np.all(np.isfinite(true_eigs)):\n        return False\n\n    # Match and compute maximum absolute difference\n    max_diff = match_and_max_diff(ritz_vals, true_eigs)\n    if not np.isfinite(max_diff):\n        return False\n\n    return bool(max_diff <= tol)\n\n\ndef solve():\n    # Define the test cases exactly as specified\n    test_cases = [\n        (\"SPD_tridiag_good\", 30, 30, 5e-3),\n        (\"Diagonal_fp16_exact_good\", 25, 25, 5e-3),\n        (\"NonNormal_shift_fail\", 30, 30, 1e-2),\n        (\"Clustered_diag_fail\", 30, 30, 3e-4),\n        (\"Overflow_fp16_fail\", 20, 20, 1e-1),\n    ]\n    results = []\n    for name, n, k, tol in test_cases:\n        res = evaluate_case(name, n, k, tol, seed=2024)\n        results.append(res)\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Beyond standard rounding errors, a robust numerical algorithm must handle exceptional events defined by the IEEE 754 standard, such as `Not-a-Number` (NaN) values or division by zero. This exercise challenges you to build a \"defensive\" Arnoldi iteration that uses IEEE 754 exception-handling mechanisms to detect and manage these numerical anomalies without crashing. Completing this task will equip you with practical skills in writing resilient numerical code, deepening your understanding of how floating-point semantics interact with iterative linear algebra methods like the Arnoldi iteration .",
            "id": "3589150",
            "problem": "Consider an implementation of the Arnoldi iteration within the framework of the Institute of Electrical and Electronics Engineers Standard for Floating-Point Arithmetic (IEEE 754). The goal is to build an orthonormal basis for the Krylov subspace using the matrix-vector recurrence, while investigating and controlling how quiet Not-a-Number (NaN) versus signaling NaN values propagate under IEEE 754 semantics. The Arnoldi process constructs vectors by repeated application of the matrix to a starting vector followed by Gram–Schmidt orthogonalization. Let $A \\in \\mathbb{R}^{n \\times n}$ be a real matrix, $v_1 \\in \\mathbb{R}^n$ a nonzero starting vector with $\\|v_1\\|_2 = 1$, and let $m \\in \\mathbb{N}$ be the desired number of Arnoldi steps. Define the Krylov subspace $\\mathcal{K}_m(A,v_1) = \\operatorname{span}\\{v_1, Av_1, A^2 v_1, \\dots, A^{m-1} v_1\\}$. The Arnoldi iteration proceeds by computing $y = A v_k$, orthogonalizing $y$ against the previously constructed orthonormal vectors $\\{q_j\\}_{j=1}^k$ using inner products, and then normalizing to obtain $v_{k+1}$. The Hessenberg coefficients $h_{j,k}$ satisfy $y \\leftarrow y - h_{j,k} q_j$ for $j \\in \\{1,\\dots,k\\}$, followed by $h_{k+1,k} = \\|y\\|_2$ and $v_{k+1} = y / h_{k+1,k}$. The iterative scheme is fundamentally based on linear algebra operations: matrix-vector multiplication, inner products, and vector norms.\n\nIn IEEE 754 arithmetic, a quiet NaN (qNaN) is designed to propagate through arithmetic operations without raising exceptions, while a signaling NaN (sNaN) is designed to raise an invalid-operation exception upon being used in an operation. The standard exposes exception flags for invalid operations, division by zero, overflow, and underflow. In practice, many programming environments quietly convert signaling NaNs to quiet NaNs upon load and primarily expose exception handling for invalid operations and division by zero via runtime controls. These facts are widely known and form the basis for detecting and controlling numerically hazardous events in iterative linear algebra routines.\n\nDesign and implement a robust Arnoldi iteration that adheres to the following principles:\n- Detect the first occurrence of an invalid arithmetic event according to IEEE 754 exception flags for invalid operations and division by zero, and detect any presence of nonfinite values (quiet NaNs or infinities) in operands before or after arithmetic.\n- Isolate the first offending index to prevent global derailment of the iteration. Isolation may be performed by zeroing out the affected component and sanitizing the corresponding row in the working matrix used within the iteration to limit further propagation from the first occurrence.\n- Continue the Arnoldi process for the prescribed number of steps $m$ by applying a defensive salvage strategy: if normalization yields a division by zero (for example, when $y = 0$), catch the IEEE exception, record the event, and inject a replacement direction that is orthogonalized against the current basis to proceed.\n\nYou must produce a complete, runnable program that uses the IEEE 754 exception controls provided by the programming environment to detect invalid operations and division by zero, while also explicitly checking for nonfinite values to identify quiet NaN propagation. The program must implement the Arnoldi iteration with classical Gram–Schmidt orthogonalization and the defensive strategy described above.\n\nUse the following test suite, with each case specifying $(A, v_0, m)$ and any perturbations:\n\n- Case $1$ (happy path): Let $n = 6$. Construct $D = \\operatorname{diag}(1,2,3,4,5,6)$, generate a random matrix $G \\in \\mathbb{R}^{n \\times n}$ with a fixed pseudorandom seed $42$, compute the $\\operatorname{QR}$ factorization $G = QR$ with $Q \\in \\mathbb{R}^{n \\times n}$ orthonormal, and set $A = Q^\\top D Q$. Let $v_0 = \\frac{1}{\\sqrt{n}} \\mathbf{1}$, where $\\mathbf{1} \\in \\mathbb{R}^n$ is the vector of all ones. Let $m = 4$.\n\n- Case $2$ (quiet NaN propagation): Starting from the same $A$ as Case $1$, set the matrix entry indexed in zero-based coordinates by $(2,4)$ to a NaN (quiet NaN). That is, set $A[2,4] = \\mathrm{NaN}$ and leave all other entries unchanged. Use the same $v_0$ and $m = 4$.\n\n- Case $3$ (division by zero salvage): Let $n = 6$ and $A = \\operatorname{diag}(0,1,2,3,4,5)$. Let $v_0 = e_1$, where $e_1$ is the first standard basis vector in $\\mathbb{R}^6$. Let $m = 3$.\n\nYour program must, for each test case, output a triple `[b, i, s]` where:\n- `b` is a boolean that is `true` if the first invalid event (either nonfinite detection in operands or an IEEE exception for invalid or division-by-zero) was detected by your defensive strategy, and `false` otherwise,\n- `i` is an integer that is the zero-based index of the first detected nonfinite operand in a vector (if applicable); if the first event is an IEEE exception without a specific component index, output `-1`,\n- `s` is an integer equal to the number of Arnoldi steps successfully completed by the routine (which should equal $m$ if your defensive measures succeed in continuing the process).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, `[ [b_1,i_1,s_1], [b_2,i_2,s_2], [b_3,i_3,s_3] ]`). No physical units apply. Angles are not involved. Express all booleans, integers, and lists in plain text form as shown.\n\nThe design must be universal and purely mathematical: implement the Arnoldi iteration using real matrices and vectors as described, classical Gram–Schmidt orthogonalization with the Euclidean norm, and IEEE 754 exception controls to detect and isolate the first invalid occurrence without halting the process. Do not use any external files or input; everything must be computed within the program using the specified parameters.",
            "solution": "The solution requires the implementation of a robust Arnoldi iteration using classical Gram–Schmidt (CGS) orthogonalization. The core of the task is to integrate mechanisms for detecting and handling specific numerical exceptions and non-finite values as prescribed by the IEEE 754 standard. The algorithm proceeds for a specified number of steps, $m$, and reports on the first detected numerical anomaly.\n\nLet $A \\in \\mathbb{R}^{n \\times n}$ be the input matrix, $v_{start} \\in \\mathbb{R}^n$ the starting vector, and $m \\in \\mathbb{N}$ the number of iterations. The orthonormal basis vectors are denoted by $\\{q_k\\}_{k=0}^{m-1}$ using $0$-based indexing common in programming.\n\nThe algorithm is structured as follows:\n\n1.  **Initialization**:\n    The starting vector $v_{start}$ is normalized to produce the first basis vector $q_0 = v_{start} / \\|v_{start}\\|_2$. A list of basis vectors is initialized with $q_0$. State variables are initialized to track whether an event has been detected ($b_{detected}$), the index of the first offending component ($i_{offending}$), and the number of completed steps ($s_{steps}$).\n\n2.  **Arnoldi Iteration Loop**:\n    The main loop iterates $m$ times, for $k$ from $0$ to $m-1$, to generate the subsequent basis vectors $q_1, \\dots, q_m$. Each step involves three primary stages: matrix-vector multiplication, classical Gram-Schmidt orthogonalization, and normalization.\n\n3.  **Defensive and Monitoring Strategy**:\n    A two-pronged strategy is employed to meet the problem's requirements for robustness.\n\n    a. **Proactive Detection of Non-Finite Values**: Before and after key arithmetic operations, operands and results are checked for non-finite values (quiet NaNs or infinities) using `numpy.isfinite()`. This is crucial for detecting the propagation of quiet NaNs, which do not raise IEEE 754 exceptions by default. If a non-finite value is found in a vector at index $j$ and no prior event has been recorded, the event is logged ($b_{detected} \\leftarrow \\text{true}$, $i_{offending} \\leftarrow j$). Following the problem's \"isolation\" directive, the affected vector component is set to $0.0$, and the corresponding row $j$ of matrix $A$ is also zeroed out to prevent further propagation from that source. This strategy is central to handling Case $2$, where a `NaN` is pre-injected into the matrix $A$.\n\n    b. **Reactive Handling of IEEE 754 Exceptions**: Core arithmetic operations susceptible to exceptions, particularly the normalization step, are encapsulated within a `try...except` block, governed by a `numpy.errstate` context manager configured to raise a `FloatingPointError` for `invalid` operations (e.g., $0/0$, $\\infty - \\infty$) and `divide`-by-zero errors. This directly addresses the requirement to \"catch the IEEE exception\".\n    \n    If the norm of the vector to be normalized, say $w$, is zero, the division $w / \\|w\\|_2$ will trigger an `invalid` operation exception ($0/0$). This is the expected failure mode in Case $3$. Upon catching this exception, the event is logged ($b_{detected} \\leftarrow \\text{true}$, $i_{offending} \\leftarrow -1$, as it is a scalar operation). The \"salvage strategy\" is then invoked: a new random vector is generated (using a fixed, step-dependent seed for reproducibility), orthogonalized against all previously computed basis vectors $\\{q_0, \\dots, q_k\\}$, and normalized to serve as the next basis vector, $q_{k+1}$. This allows the iteration to proceed for the full $m$ steps despite the breakdown.\n\n4.  **Classical Gram-Schmidt (CGS) Orthogonalization**:\n    At step $k$, the vector $y = A q_k$ is computed. To obtain a vector $w$ orthogonal to the existing basis $\\{q_0, \\dots, q_k\\}$, CGS is applied. A working copy $w$ is initialized to $y$. Then, for each $j$ from $0$ to $k$, the component along $q_j$ is subtracted: $w \\leftarrow w - h_{j,k} q_j$, where the coefficient $h_{j,k}$ is computed from the *original* vector $y$ as $h_{j,k} = q_j^\\top y$. This adherence to using the original vector for all projections distinguishes CGS from Modified Gram-Schmidt.\n\n5.  **Final Output**:\n    After $m$ steps are completed, the function returns a list containing the final state: $[b_{detected}, i_{offending}, s_{steps}]$. This process is repeated for each test case provided in the problem statement. The final list of results is formatted precisely as requested.\n\nThis design directly implements the specified Arnoldi process while integrating the required IEEE 754-aware monitoring and handling logic. The proactive checks using `isfinite` satisfy the requirement to detect quiet NaN propagation, and the reactive `try/except` structure satisfies the requirement to catch explicit exceptions and apply the salvage strategy.",
            "answer": "```python\nimport numpy as np\n\ndef robust_arnoldi(A, v0, m):\n    \"\"\"\n    Implements the Arnoldi iteration with robust error handling for IEEE 754 exceptions.\n\n    Args:\n        A (np.ndarray): The matrix for the iteration. A copy will be used as it may be modified.\n        v0 (np.ndarray): The starting vector.\n        m (int): The number of Arnoldi steps to perform.\n\n    Returns:\n        list: A list [b, i, s] where b is a boolean for event detection,\n              i is the index of the first offense (-1 if not applicable),\n              and s is the number of completed steps.\n    \"\"\"\n    n = A.shape[0]\n    A_work = A.copy()  # Use a working copy of the matrix\n    \n    b_detected = False\n    i_offending = -1\n    s_steps = 0\n    \n    q_vectors = []\n    \n    # Initialize with the starting vector\n    norm_v0 = np.linalg.norm(v0)\n    if norm_v0 == 0:\n        # Starting with a zero vector is an immediate failure.\n        # Although not specified in test cases, this is a sensible check.\n        return [True, -1, 0]\n        \n    q0 = v0 / norm_v0\n    q_vectors.append(q0)\n    \n    # Main Arnoldi loop\n    for k in range(m):\n        q_k = q_vectors[k]\n        \n        # 1. Matrix-vector product: y = A @ q_k\n        y = A_work @ q_k\n        \n        # Proactive check for non-finite values (e.g., from a NaN in the matrix)\n        if not b_detected and not np.all(np.isfinite(y)):\n            b_detected = True\n            # Find the first non-finite index\n            i_offending = int(np.where(~np.isfinite(y))[0][0])\n            # Apply isolation strategy as per the problem description\n            y[i_offending] = 0.0\n            A_work[i_offending, :] = 0.0\n        \n        # 2. Classical Gram-Schmidt (CGS)\n        w = y.copy()  # Vector that will be orthogonalized\n        for j in range(k + 1):\n            q_j = q_vectors[j]\n            \n            # Coefficient computation using the original vector y\n            h_jk = np.dot(q_j, y)\n            \n            # Check if the inner product produced a NaN\n            if not b_detected and not np.isfinite(h_jk):\n                b_detected = True\n                i_offending = -1  # Scalar operation\n                h_jk = 0.0  # Sanitize to prevent further propagation\n            \n            if not np.isfinite(h_jk):\n                h_jk = 0.0 # Sanitize anyway\n\n            # Subtract projection from the working vector w\n            w = w - h_jk * q_j\n\n            # Check for non-finites after subtraction\n            if not b_detected and not np.all(np.isfinite(w)):\n                b_detected = True\n                i_offending = int(np.where(~np.isfinite(w))[0][0])\n                # No specific salvage for this, but the event is recorded\n        \n        # 3. Normalization and Salvage\n        norm_w = np.linalg.norm(w)\n        next_q = np.zeros(n)\n        \n        try:\n            # Configure to raise exceptions for division by zero or invalid ops\n            with np.errstate(divide='raise', invalid='raise'):\n                # This division is the critical operation. If norm_w is 0,\n                # np.linalg.norm is safe, but this raw division will fault.\n                # 0/0 -> invalid op; x/0 -> div by zero.\n                next_q = w / norm_w\n                \n        except FloatingPointError:\n            # Catch the exception, as required. This happens on breakdown.\n            if not b_detected:\n                b_detected = True\n                i_offending = -1  # Event is a scalar operation\n                \n            # Invoke salvage strategy\n            rand_gen = np.random.default_rng(seed=k)  # Reproducible seed per step\n            while True:\n                new_vec = rand_gen.standard_normal(size=n)\n                # Orthogonalize against all existing basis vectors\n                for j in range(k + 1):\n                    new_vec -= np.dot(q_vectors[j], new_vec) * q_vectors[j]\n                \n                norm_new_vec = np.linalg.norm(new_vec)\n                # Ensure the new random vector is non-zero after projection\n                if norm_new_vec > 1e-12:\n                    next_q = new_vec / norm_new_vec\n                    break\n        \n        q_vectors.append(next_q)\n        s_steps += 1\n            \n    return [b_detected, i_offending, s_steps]\n\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test cases for the robust Arnoldi implementation.\n    \"\"\"\n    # Case 1: Happy path\n    n1 = 6\n    D1 = np.diag([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n    rng_G = np.random.default_rng(42)\n    G1 = rng_G.standard_normal(size=(n1, n1))\n    Q1, _ = np.linalg.qr(G1)\n    A1 = Q1.T @ D1 @ Q1\n    v0_1 = np.ones(n1) / np.sqrt(n1)\n    m1 = 4\n\n    # Case 2: Quiet NaN propagation\n    A2 = A1.copy()\n    A2[2, 4] = np.nan\n    v0_2 = v0_1.copy()\n    m2 = 4\n\n    # Case 3: Division by zero salvage\n    n3 = 6\n    A3 = np.diag([0.0, 1.0, 2.0, 3.0, 4.0, 5.0])\n    v0_3 = np.zeros(n3)\n    v0_3[0] = 1.0\n    m3 = 3\n    \n    test_cases = [\n        (A1, v0_1, m1),\n        (A2, v0_2, m2),\n        (A3, v0_3, m3),\n    ]\n\n    results = []\n    for A, v0, m in test_cases:\n        result = robust_arnoldi(A, v0, m)\n        results.append(result)\n        \n    # Format the output string exactly as specified.\n    # The example [ [b_1,i_1,s_1], ... ] suggests spaces after commas.\n    # Python's default list-to-string conversion provides this.\n    # We only need to convert boolean True/False to lowercase true/false.\n    final_output_str = str(results).replace('True', 'true').replace('False', 'false')\n    \n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}