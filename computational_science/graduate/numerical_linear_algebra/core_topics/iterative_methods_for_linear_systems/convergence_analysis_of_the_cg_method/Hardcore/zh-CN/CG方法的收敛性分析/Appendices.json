{
    "hands_on_practices": [
        {
            "introduction": "在运行计算成本高昂的迭代方法之前，能够预估其收敛速度是至关重要的。这个练习将理论与实践相结合，要求我们应用共轭梯度法（CG）的经典收敛界，该界基于切比雪夫多项式和矩阵的谱条件数。通过这个计算，你将亲身体验如何利用矩阵的光谱信息来提前估算达到所需精度所需的迭代次数，从而加深对CG方法收敛性理论的理解。",
            "id": "3541532",
            "problem": "考虑一个实对称正定矩阵 $A \\in \\mathbb{R}^{n \\times n}$，其谱 $\\sigma(A) \\subset [1,100]$。设 $x^{\\star}$ 是 $Ax=b$ 的唯一解，并设 $e_k := x_k - x^{\\star}$ 表示共轭梯度（CG）法第 $k$ 次迭代后的误差。利用CG法作为一种多项式方法的基于切比雪夫多项式的最优性刻画以及谱定理，推导一个关于误差的$A$-范数的先验界，其形式为 $\\|e_k\\|_{A} \\leq \\gamma_k \\|e_0\\|_{A}$，其中 $\\gamma_k$ 由在区间 $[1,100]$ 上、满足约束 $p(0)=1$ 的 $k$ 次多项式对零函数的最佳一致逼近确定。从第一性原理出发，通过将区间 $[1,100]$ 映射到 $[-1,1]$，调用切比雪夫多项式的极小化极大性质，并将结果表达式化简为谱条件数的闭式形式来构造该界。然后，使用这个基于切比雪夫的估计，确定保证 \n$$\n\\frac{\\|e_k\\|_{A}}{\\|e_0\\|_{A}} \\leq 10^{-6}.\n$$\n的最小整数迭代次数 $k \\in \\mathbb{N}$。将你的最终答案表示为单个整数。无需进行超出最小整数要求之外的舍入。",
            "solution": "该问题要求推导共轭梯度（CG）法的先验误差界，并应用它来找到一个特定的迭代次数。\n\n首先，我们验证问题陈述。\n**步骤1：提取已知条件**\n- 矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是实对称正定（SPD）的。\n- $A$ 的谱，记为 $\\sigma(A)$，是区间 $[1, 100]$ 的一个子集。\n- $x^{\\star}$ 是线性系统 $Ax=b$ 的唯一解。\n- $e_k = x_k - x^{\\star}$ 是第 $k$ 次迭代的误差向量。\n- 所用方法是共轭梯度（CG）法。\n- 任务涉及使用CG作为多项式方法的刻画以及切比雪夫多项式的性质。\n- 目标是找到最小的整数 $k$，使得$A$-范数下的相对误差 $\\|e_k\\|_{A}/\\|e_0\\|_{A}$ 以 $10^{-6}$ 为界。\n\n**步骤2：使用提取的已知条件进行验证**\n该问题在科学上植根于数值线性代数中成熟的理论，特别是克雷洛夫子空间方法的收敛性分析。所有前提，包括矩阵 $A$ 的性质和CG误差的公式，都是标准且正确的。该问题是适定的，提供了所有必要信息（谱范围和误差容限）以确定 $k$ 的唯一整数解。语言客观而精确。该问题是该领域一个标准的、非平凡的练习题。因此，该问题被认定为有效。\n\n**步骤3：结论与行动**\n问题有效。我们继续进行求解。\n\n**基于切比雪夫的误差界的推导：**\n\n用初始猜测 $x_0$ 求解 $Ax=b$ 的共轭梯度法会生成一个近似序列 $x_k$。误差向量 $e_k = x_k - x^{\\star}$ 可以用一个多项式通过初始误差 $e_0 = x_0 - x^{\\star}$ 来表示。具体来说，迭代解满足 $x_k - x_0 \\in \\mathcal{K}_k(A, r_0)$，其中 $r_0 = b - Ax_0 = -Ae_0$ 是初始残差，而 $\\mathcal{K}_k$ 是第 $k$ 个克雷洛夫子空间。这意味着误差 $e_k$ 可以写成：\n$$ e_k = p_k(A) e_0 $$\n其中 $p_k$ 是一个次数至多为 $k$ 的多项式，属于集合 $\\mathcal{P}_k$，且满足约束 $p_k(0)=1$。CG法具有最优性：它能找到那个特定的多项式 $p_k$，使得在$A$-范数下的误差最小化，其中$A$-范数定义为 $\\|v\\|_A = \\sqrt{v^T A v}$。\n$$ \\|e_k\\|_A = \\min_{q_k \\in \\mathcal{P}_k, q_k(0)=1} \\|q_k(A)e_0\\|_A $$\n为了获得一个独立于初始误差 $e_0$ 的先验界，我们使用 $A$ 的谱分解。设 $A=V\\Lambda V^T$ 是 $A$ 的特征分解，其中 $\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)$ 包含 $A$ 的特征值。误差的$A$-范数可以如下界定：\n$$ \\|e_k\\|_A = \\|p_k(A) e_0\\|_A \\le \\left( \\min_{q_k \\in \\mathcal{P}_k, q_k(0)=1} \\max_{\\lambda \\in \\sigma(A)} |q_k(\\lambda)| \\right) \\|e_0\\|_A $$\n问题陈述 $\\sigma(A) \\subset [1, 100]$。令 $\\alpha = \\lambda_{\\min}(A)$ 和 $\\beta = \\lambda_{\\max}(A)$。当谱覆盖整个给定区间时，这个界是最保守的，因此我们考虑 $\\lambda \\in [\\alpha, \\beta]$，其中我们取 $\\alpha=1$ 和 $\\beta=100$。问题简化为一个经典的多项式逼近问题：\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\min_{q_k \\in \\mathcal{P}_k, q_k(0)=1} \\max_{\\lambda \\in [\\alpha, \\beta]} |q_k(\\lambda)| $$\n为解决这个极小化极大问题，我们使用仿射变换将区间 $[\\alpha, \\beta]$ 映射到标准区间 $[-1, 1]$：\n$$ x(\\lambda) = \\frac{2\\lambda - (\\beta+\\alpha)}{\\beta-\\alpha} $$\n一个 $k$ 次多项式 $q_k(\\lambda)$ 在此映射下变成一个 $k$ 次多项式 $Q_k(x)$。约束 $q_k(0)=1$ 变成了在点 $x_0 = x(0)$ 处对 $Q_k$ 的一个约束：\n$$ x_0 = \\frac{2(0) - (\\beta+\\alpha)}{\\beta-\\alpha} = -\\frac{\\beta+\\alpha}{\\beta-\\alpha} $$\n现在问题是找到满足 $Q_k(x_0)=1$ 条件下的 $\\min \\max_{x \\in [-1, 1]} |Q_k(x)|$。解由一个缩放的第一类切比雪夫多项式 $T_k(x)$ 给出：\n$$ Q_k(x) = \\frac{T_k(x)}{T_k(x_0)} $$\n在 $[-1, 1]$ 上 $|Q_k(x)|$ 的最大值是 $\\frac{\\max_{x \\in [-1, 1]}|T_k(x)|}{|T_k(x_0)|} = \\frac{1}{|T_k(x_0)|}$。由于 $\\alpha, \\beta > 0$，我们有 $x_0 < -1$。对于 $z < -1$，$|T_k(z)| = T_k(|z|)$。因此，界变为：\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\frac{1}{T_k\\left(\\frac{\\beta+\\alpha}{\\beta-\\alpha}\\right)} $$\n这个表达式可以用谱条件数 $\\kappa = \\beta/\\alpha$ 来写：\n$$ \\frac{\\beta+\\alpha}{\\beta-\\alpha} = \\frac{(\\beta/\\alpha)+1}{(\\beta/\\alpha)-1} = \\frac{\\kappa+1}{\\kappa-1} $$\n所以，界的最终形式是：\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\frac{1}{T_k\\left(\\frac{\\kappa+1}{\\kappa-1}\\right)} $$\n\n**迭代次数 $k$ 的计算：**\n\n我们已知 $\\sigma(A) \\subset [1, 100]$，所以我们设 $\\alpha=1$ 和 $\\beta=100$。条件数是 $\\kappa = 100/1 = 100$。我们想找到最小的整数 $k$ 使得：\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le 10^{-6} $$\n使用我们推导出的界，我们必须满足：\n$$ \\frac{1}{T_k\\left(\\frac{100+1}{100-1}\\right)} \\le 10^{-6} \\implies T_k\\left(\\frac{101}{99}\\right) \\ge 10^6 $$\n对于自变量 $|z| \\ge 1$，切比雪夫多项式可以用双曲余弦函数表示：$T_k(z) = \\cosh(k \\, \\text{arccosh}(z))$。不等式变为：\n$$ \\cosh\\left(k \\, \\text{arccosh}\\left(\\frac{101}{99}\\right)\\right) \\ge 10^6 $$\n因为 $\\cosh(y)$ 是对于 $y > 0$ 的严格递增函数，我们可以对两边取反双曲余弦：\n$$ k \\, \\text{arccosh}\\left(\\frac{101}{99}\\right) \\ge \\text{arccosh}(10^6) $$\n求解 $k$：\n$$ k \\ge \\frac{\\text{arccosh}(10^6)}{\\text{arccosh}\\left(\\frac{101}{99}\\right)} $$\n我们可以使用恒等式 $\\text{arccosh}\\left(\\frac{\\kappa+1}{\\kappa-1}\\right) = \\ln\\left(\\frac{\\sqrt{\\kappa}+1}{\\sqrt{\\kappa}-1}\\right)$ 来化简分母。对于 $\\kappa=100$：\n$$ \\text{arccosh}\\left(\\frac{101}{99}\\right) = \\ln\\left(\\frac{\\sqrt{100}+1}{\\sqrt{100}-1}\\right) = \\ln\\left(\\frac{10+1}{10-1}\\right) = \\ln\\left(\\frac{11}{9}\\right) $$\n所以，关于 $k$ 的不等式是：\n$$ k \\ge \\frac{\\text{arccosh}(10^6)}{\\ln(11/9)} $$\n现在我们数值计算这个表达式。反双曲余弦的恒等式是 $\\text{arccosh}(z) = \\ln(z + \\sqrt{z^2-1})$。\n$$ \\text{arccosh}(10^6) = \\ln(10^6 + \\sqrt{(10^6)^2 - 1}) = \\ln(10^6 + \\sqrt{10^{12} - 1}) $$\n对于大的 $z$，$\\text{arccosh}(z) \\approx \\ln(2z)$。\n$$ \\text{arccosh}(10^6) \\approx \\ln(2 \\times 10^6) \\approx 14.508658 $$\n分母是：\n$$ \\ln(11/9) \\approx 0.200671 $$\n因此，\n$$ k \\ge \\frac{14.508658}{0.200671} \\approx 72.3005 $$\n由于迭代次数 $k$ 必须是整数，满足此条件的最小整数值为 $k=73$。",
            "answer": "$$\n\\boxed{73}\n$$"
        },
        {
            "introduction": "共轭梯度法的理论保证了误差的A-范数是单调递减的，这是其收敛性的基石。然而，在实际应用中我们更常监测残差的欧几里得范数，但它并不具备同样的单调性保证。这个编程练习旨在让你构建一个具体的反例，通过代码直观地观察到残差范数在迭代过程中可能出现的非单调行为，从而深刻理解CG算法的几何特性及其不同收敛度量之间的微妙差异。",
            "id": "3541518",
            "problem": "您必须编写一个完整、可运行的程序，以演示和分析共轭梯度（CG）方法对于对称正定（SPD）线性系统的单调性行为。其科学背景是数值线性代数和共轭梯度方法的收敛性分析。目标是构建一个具体的反例，其中欧几里得残差范数 $\\|r_k\\|_2$ 并非单调递减，而误差的能量范数 $\\|e_k\\|_A$ 严格递减，并对一小组测试套件中的行为进行量化。\n\n定义和核心事实作为基础：\n- 一个矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是对称正定（SPD）的，如果 $A^\\top = A$ 且对于所有非零 $x \\in \\mathbb{R}^n$ 都有 $x^\\top A x > 0$。\n- 共轭梯度（CG）方法旨在求解 $A x = b$（其中 $A$ 是SPD矩阵），通过迭代生成 $x_k \\in x_0 + \\mathcal{K}_k(A, r_0)$，其中 $r_k = b - A x_k$ 是残差，$e_k = x^\\star - x_k$ 是误差（$x^\\star$ 为精确解），$\\mathcal{K}_k(A, r_0) = \\mathrm{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$ 是第 $k$ 个Krylov子空间。\n- $A$-能量内积和范数分别为 $\\langle u, v \\rangle_A = u^\\top A v$ 和 $\\|u\\|_A = \\sqrt{u^\\top A u}$。CG方法是关于 $\\langle \\cdot, \\cdot \\rangle_A$ 在 $x_0 + \\mathcal{K}_k(A, r_0)$ 上的Galerkin投影，这意味着 $\\|e_k\\|_A$ 是单调递减的（并且是严格递减的，除非达到精确解）。\n- 欧几里得范数 $\\|\\cdot\\|_2$ 用于度量 $\\|r_k\\|_2$，并且在CG方法中不保证该量是单调的。\n\n您的程序必须：\n1. 为给定的SPD矩阵 $A$、右端项 $b$ 和初始猜测 $x_0$ 实现共轭梯度（CG）方法，生成迭代解 $x_k$、残差 $r_k$，并通过直接求解器计算精确解 $x^\\star$ 以度量 $\\|e_k\\|_A$。\n2. 对于每个测试用例，运行CG直到 $k$ 达到矩阵维度 $n$ 或 $\\|r_k\\|_2$ 低于停止容差。使用欧几里得范数下的停止容差 $10^{-12}\\|b\\|_2$。\n3. 计算序列 $\\{\\|r_k\\|_2\\}_{k=0}^{k_{\\mathrm{end}}}$ 和 $\\{\\|e_k\\|_A\\}_{k=0}^{k_{\\mathrm{end}}}$，其中 $k_{\\mathrm{end}}$ 是最后一次计算的迭代次数。\n4. 确定残差序列是否非增：对于所有连续的迭代，$\\|r_{k+1}\\|_2 \\le \\|r_k\\|_2$，并找到最小的索引 $k$（使用从零开始的索引）使得 $\\|r_{k+1}\\|_2 > \\|r_k\\|_2$。如果不存在这样的索引，则返回 $-1$。使用带有 $10^{-14}$ 数值容差的严格比较，即仅当 $\\|r_{k+1}\\|_2 > \\|r_k\\|_2 + 10^{-14}$ 时才视为增加。\n5. 确定能量范数序列是否严格递减：在终止前的所有连续迭代中，$\\|e_{k+1}\\|_A < \\|e_k\\|_A$。使用带有 $10^{-14}$ 数值容差的严格比较，即每一步都要求 $\\|e_{k+1}\\|_A \\le \\|e_k\\|_A - 10^{-14}$。\n\n几何解释要求：\n- 虽然您不需要打印几何解释，但您的程序必须包含一个展现此现象的测试用例：一个 $\\|r_k\\|_2$ 并非单调递减而 $\\|e_k\\|_A$ 严格递减的反例。其构造应基于特征值高度各向异性且偏离坐标轴旋转的SPD矩阵，以在残差和 $A$-缩放的子空间之间产生非平凡的角度，这会影响残差的欧几里得范数。\n\n矩阵构造：\n- 构造形式为 $A = R^\\top \\Lambda R$ 的SPD矩阵 $A$，其中 $\\Lambda$ 是具有正元素的对角矩阵，$R$ 是由平面旋转构建的正交旋转矩阵。使用在 $(1,2)$ 和 $(3,4)$ 坐标平面中作用的块对角旋转，角度以弧度表示。\n\n测试套件：\n提供以下四个测试用例：\n- 测试用例1（通用，各向异性，可能非单调残差）：$n=4$，$\\Lambda=\\mathrm{diag}(1, 20, 200, 5000)$，$R$ 是块对角旋转，角度为 $\\theta_{12}=0.7$ 和 $\\theta_{34}=0.5$，$b=[1,1,1,1]^\\top$，$x_0=[0,0,0,0]^\\top$。\n- 测试用例2（边界条件与特征向量对齐，单步收敛）：$n=4$，$\\Lambda=\\mathrm{diag}(2,5,7,11)$，$R=I$，$b=[1,0,0,0]^\\top$，$x_0=[0,0,0,0]^\\top$。\n- 测试用例3（良态，近各向同性）：$n=4$，$\\Lambda=\\mathrm{diag}(1,1.5,2,2.5)$，$R$ 的角度为 $\\theta_{12}=0.2$ 和 $\\theta_{34}=0.0$，$b=[1,-1,2,-2]^\\top$，$x_0=[0,0,0,0]^\\top$。\n- 测试用例4（系统性地在角度上搜索反例）：$n=4$，$\\Lambda=\\mathrm{diag}(1,50,500,10000)$，$R$ 由 $\\theta_{12}\\in\\{0.1,0.25,0.5,0.75,1.0,1.2\\}$ 和固定的 $\\theta_{34}=0.9$ 按顺序扫描构成；$b=[1,1,0.1,0.1]^\\top$，$x_0=[0,0,0,0]^\\top$；选择第一个表现出残差增加而能量范数严格递减的 $\\theta_{12}$，否则报告没有增加。\n\n角度单位规定：\n- 所有角度均以弧度为单位。\n\n您的程序必须生成单行输出，其中包含一个逗号分隔的列表形式的结果，并用方括号括起来。对于每个测试用例，输出四元组 $[\\mathrm{res\\_noninc}, \\mathrm{energy\\_strict}, k_{\\mathrm{inc}}, \\theta]$，其中：\n- $\\mathrm{res\\_noninc}$ 是一个布尔值，指示 $\\|r_k\\|_2$ 在计算出的迭代过程中是否非增。\n- $\\mathrm{energy\\_strict}$ 是一个布尔值，指示 $\\|e_k\\|_A$ 在计算出的迭代过程中是否严格递减。\n- $k_{\\mathrm{inc}}$ 是满足 $\\|r_{k+1}\\|_2 > \\|r_k\\|_2 + 10^{-14}$ 的最小整数 $k \\ge 0$，如果不存在这样的索引，则为 $-1$。\n- $\\theta$ 是用于构建 $R$ 的角度 $\\theta_{12}$；如果某个测试用例未使用 $(1,2)$-旋转，则此字段输出 $0.0$。\n\n因此，您最终打印的行必须如下所示：\n$[[\\mathrm{res\\_noninc}_1,\\mathrm{energy\\_strict}_1,k_{\\mathrm{inc},1},\\theta_1],[\\mathrm{res\\_noninc}_2,\\mathrm{energy\\_strict}_2,k_{\\mathrm{inc},2},\\theta_2],[\\mathrm{res\\_noninc}_3,\\mathrm{energy\\_strict}_3,k_{\\mathrm{inc},3},\\theta_3],[\\mathrm{res\\_noninc}_4,\\mathrm{energy\\_strict}_4,k_{\\mathrminc},4},\\theta_4]]$.",
            "solution": "该问题被评估为有效。它在科学上基于数值线性代数的原理，特别是共轭梯度（CG）方法的收敛理论。该问题定义明确，提供了所有必要的数据、定义和约束，并且没有歧义、主观性或任何事实或逻辑上的不一致之处。\n\n问题的核心是展示共轭梯度（CG）方法的一个著名性质：虽然它保证了误差的 $A$-范数单调递减，但残差的欧几里得范数不保证单调递减。\n\nCG方法是一种迭代算法，用于求解线性系统 $Ax=b$，其中矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是对称正定的（SPD）。该方法构造一系列对精确解 $x^\\star$ 的近似值 $x_k$。在每次迭代 $k$ 中，新的近似值 $x_k$ 从仿射Krylov子空间 $x_0 + \\mathcal{K}_k(A, r_0)$ 中选取，使得误差的 $A$-范数 $\\|e_k\\|_A = \\|x^\\star - x_k\\|_A$ 最小化。$A$-范数定义为 $\\|v\\|_A = \\sqrt{v^\\top A v}$。\n\n这个最小化性质确保了误差范数序列 $\\{\\|e_k\\|_A\\}$ 是严格递减的，除非找到精确解，此时误差范数变为并保持为零：\n$$ \\|e_{k+1}\\|_A < \\|e_k\\|_A \\quad \\text{if } e_k \\neq 0 $$\n这是CG方法的一个基本收敛性质。\n\n残差定义为 $r_k = b - A x_k$。它与误差的关系是 $r_k = A e_k$。残差的欧几里得范数 $\\|r_k\\|_2$ 是一个常用的终止迭代的实用度量。然而，与 $\\|e_k\\|_A$ 不同，序列 $\\{\\|r_k\\|_2\\}$ 不保证是单调的。在某些情况下，可能会发生残差范数的增加，即 $\\|r_{k+1}\\|_2 > \\|r_k\\|_2$。\n\n这种现象可以通过分析残差更新步骤来理解。在CG的标准实现中，更新由下式给出：\n$$ r_{k+1} = r_k - \\alpha_k A p_k $$\n其中 $p_k$ 是搜索方向，$\\alpha_k$ 是步长。新残差的范数平方为：\n$$ \\|r_{k+1}\\|_2^2 = \\|r_k - \\alpha_k A p_k\\|_2^2 = \\|r_k\\|_2^2 - 2\\alpha_k r_k^\\top A p_k + \\alpha_k^2 \\|A p_k\\|_2^2 $$\n要使范数减小，必须满足条件 $2\\alpha_k r_k^\\top A p_k > \\alpha_k^2 \\|A p_k\\|_2^2$，简化后为 $2 r_k^\\top A p_k > \\alpha_k \\|A p_k\\|_2^2$。使用最优步长的标准公式 $\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k}$ 以及将搜索方向 $p_k$ 与残差 $r_k$ 相关联的性质，可以证明这个不等式取决于矩阵 $A$ 的性质以及当前向量 $p_k$ 和 $r_k$。具体来说，如果矩阵 $A$ 的条件数很大（特征值分布范围很广），并且搜索方向 $p_k$ 在对应于大特征值的特征向量上有显著分量，则项 $\\alpha_k^2 \\|A p_k\\|_2^2$ 可能会大到足以导致 $\\|r_{k+1}\\|_2$ 增加。从几何角度看，算法采取的步长对于在 $A$-范数景观中减少误差是最优的，但就最小化欧几里得残差而言，这一步可能“矫枉过正”，导致暂时的增加。\n\n程序将实现CG算法并构造形式为 $A = R^\\top \\Lambda R$ 的测试矩阵，其中 $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$ 是特征值的对角矩阵，$R$ 是一个正交旋转矩阵。这种构造允许精确控制特征值（通过 $\\Lambda$）和特征向量的方向（通过 $R$）。通过选择一个大的特征值比率并将特征系统旋转偏离标准基，我们可以创建可靠地表现出非单调残差行为的测试用例。\n\n解决方案将按以下步骤进行：\n1.  创建一个函数，根据给定的特征值和旋转角度构造矩阵 $A = R^\\top \\Lambda R$。旋转矩阵 $R$ 被构建为平面（Givens）旋转的复合。对于 $n=4$，我们使用一个块对角旋转矩阵：\n    $$ R(\\theta_{12}, \\theta_{34}) = \\begin{pmatrix} \\cos\\theta_{12} & -\\sin\\theta_{12} & 0 & 0 \\\\ \\sin\\theta_{12} & \\cos\\theta_{12} & 0 & 0 \\\\ 0 & 0 & \\cos\\theta_{34} & -\\sin\\theta_{34} \\\\ 0 & 0 & \\sin\\theta_{34} & \\cos\\theta_{34} \\end{pmatrix} $$\n2.  实现标准的CG算法。算法从一个初始猜测 $x_0$ 开始，并计算：\n    - $r_0 = b - Ax_0$\n    - $p_0 = r_0$\n    - For $k = 0, 1, 2, \\ldots$:\n      - $\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k}$\n      - $x_{k+1} = x_k + \\alpha_k p_k$\n      - $r_{k+1} = r_k - \\alpha_k A p_k$\n      - 如果 $\\|r_{k+1}\\|_2$ 低于容差，则停止。\n      - $\\beta_{k+1} = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}$\n      - $p_{k+1} = r_{k+1} + \\beta_{k+1} p_k$\n3.  对于每次迭代，计算并存储范数 $\\|r_k\\|_2$ 和 $\\|e_k\\|_A$。计算 $e_k=x^\\star - x_k$ 所需的精确解 $x^\\star$ 使用直接求解器预先计算。\n4.  CG迭代终止后，分析存储的范数序列，使用指定的数值容差（$10^{-14}$）检查所需的单调性。具体来说，我们检查是否 $\\|r_{k+1}\\|_2 > \\|r_k\\|_2 + 10^{-14}$ 以找到残差增加的第一个实例，并检查是否 $\\|e_{k+1}\\|_A \\le \\|e_k\\|_A - 10^{-14}$ 对所有步骤都成立，以确认能量范数的严格递减。\n5.  将此过程应用于所有指定的测试用例，包括用例4中的搜索，并按要求格式化结果。",
            "answer": "```python\nimport numpy as np\n\ndef construct_matrix(n, lambdas, theta12, theta34):\n    \"\"\"\n    Constructs an SPD matrix A = R.T @ Lambda @ R of size n x n.\n    R is a block-diagonal rotation matrix.\n    \"\"\"\n    if n != 4:\n        raise ValueError(\"This matrix construction is defined for n=4.\")\n    \n    Lambda = np.diag(lambdas)\n    \n    R = np.identity(n)\n    \n    # Rotation in the (1,2) plane (indices 0, 1)\n    if theta12 != 0.0:\n        c12, s12 = np.cos(theta12), np.sin(theta12)\n        R12 = np.array([[c12, -s12], [s12, c12]])\n        R[0:2, 0:2] = R12\n\n    # Rotation in the (3,4) plane (indices 2, 3)\n    if theta34 != 0.0:\n        c34, s34 = np.cos(theta34), np.sin(theta34)\n        R34 = np.array([[c34, -s34], [s34, c34]])\n        R[2:4, 2:4] = R34\n\n    A = R.T @ Lambda @ R\n    return A\n\ndef run_cg_analysis(A, b, x0, theta12_val, max_iter_n):\n    \"\"\"\n    Runs the Conjugate Gradient method and analyzes norm behavior.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Tolerances\n    stop_tol = 1e-12 * np.linalg.norm(b)\n    increase_tol = 1e-14\n    decrease_tol = 1e-14\n\n    # Compute exact solution to measure error\n    x_star = np.linalg.solve(A, b)\n\n    r_norms = []\n    e_norms = []\n\n    # CG Initialization\n    x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    rs_old = np.dot(r, r)\n\n    # Store initial norms (k=0)\n    e = x_star - x\n    r_norms.append(np.linalg.norm(r))\n    e_norms.append(np.sqrt(e.T @ A @ e))\n\n    if np.linalg.norm(r) < stop_tol:\n        max_iter_n = 0\n\n    for k in range(max_iter_n):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        # Store norms for step k+1\n        e = x_star - x\n        r_norms.append(np.linalg.norm(r))\n        e_norms.append(np.sqrt(e.T @ A @ e))\n        \n        rs_new = np.dot(r, r)\n        \n        if np.sqrt(rs_new) < stop_tol:\n            break\n            \n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n\n    # Analyze norm sequences\n    # 1. Residual norm non-increase check\n    k_inc = -1\n    res_noninc = True\n    for k in range(len(r_norms) - 1):\n        if r_norms[k+1] > r_norms[k] + increase_tol:\n            k_inc = k\n            res_noninc = False\n            break\n\n    # 2. Energy norm strict decrease check\n    energy_strict = True\n    # If the method converges exactly, error norm becomes 0 and stays 0.\n    # This is not a violation of strict decrease.\n    for k in range(len(e_norms) - 1):\n        if e_norms[k] > 1e-15: # Avoid issues with floating point zero\n             # Check if e_norms[k+1] < e_norms[k] respecting tolerance\n            if e_norms[k+1] > e_norms[k] - decrease_tol:\n                energy_strict = False\n                break\n    \n    return [res_noninc, energy_strict, k_inc, theta12_val]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    results = []\n    n = 4\n\n    # Test case 1\n    lambdas1 = [1., 20., 200., 5000.]\n    theta12_1, theta34_1 = 0.7, 0.5\n    b1 = np.array([1., 1., 1., 1.])\n    x0_1 = np.zeros(n)\n    A1 = construct_matrix(n, lambdas1, theta12_1, theta34_1)\n    results.append(run_cg_analysis(A1, b1, x0_1, theta12_1, n))\n\n    # Test case 2\n    lambdas2 = [2., 5., 7., 11.]\n    theta12_2, theta34_2 = 0.0, 0.0\n    b2 = np.array([1., 0., 0., 0.])\n    x0_2 = np.zeros(n)\n    A2 = construct_matrix(n, lambdas2, theta12_2, theta34_2)\n    results.append(run_cg_analysis(A2, b2, x0_2, theta12_2, n))\n\n    # Test case 3\n    lambdas3 = [1., 1.5, 2., 2.5]\n    theta12_3, theta34_3 = 0.2, 0.0\n    b3 = np.array([1., -1., 2., -2.])\n    x0_3 = np.zeros(n)\n    A3 = construct_matrix(n, lambdas3, theta12_3, theta34_3)\n    results.append(run_cg_analysis(A3, b3, x0_3, theta12_3, n))\n\n    # Test case 4\n    lambdas4 = [1., 50., 500., 10000.]\n    theta12_vals_4 = [0.1, 0.25, 0.5, 0.75, 1.0, 1.2]\n    theta34_4 = 0.9\n    b4 = np.array([1., 1., 0.1, 0.1])\n    x0_4 = np.zeros(n)\n    \n    found_increase = False\n    last_result = None\n    for theta12 in theta12_vals_4:\n        A4 = construct_matrix(n, lambdas4, theta12, theta34_4)\n        current_result = run_cg_analysis(A4, b4, x0_4, theta12, n)\n        last_result = current_result\n        # The result's k_inc is at index 2\n        if current_result[2] != -1:\n            results.append(current_result)\n            found_increase = True\n            break\n    \n    if not found_increase:\n        results.append(last_result)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "CG方法的许多优美性质，如残差的正交性和搜索方向的A-共轭性，都建立在精确算术的假设之上。然而，计算机使用有限精度的浮点运算，这会引入舍入误差。这个高级练习将引导你探究这些微小的计算误差如何破坏算法的理论正交性，并最终导致收敛速度变慢。通过在一个受控环境中对算法的关键标量$ \\alpha_k $和$ \\beta_k $引入扰动，你将能直接观察并量化数值稳定性对CG方法实际性能的影响。",
            "id": "3541548",
            "problem": "您的任务是分析和展示浮点舍入误差对求解对称正定线性系统的共轭梯度法的影响。考虑线性系统 $A x = b$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵，$b \\in \\mathbb{R}^{n}$。共轭梯度法生成迭代序列 $\\{x_k\\}$、残差序列 $\\{r_k\\}$ 和搜索方向序列 $\\{p_k\\}$，这些序列依赖于通过内积和二次型计算出的标量系数 $\\alpha_k$ 和 $\\beta_k$。在有限精度算术中，这些计算会受到舍入误差的影响。您的任务是：\n\n1. 从标准浮点模型 $ \\mathrm{fl}(a \\,\\mathrm{op}\\, b) = (a \\,\\mathrm{op}\\, b)(1 + \\delta) $（其中对于每个基本运算都有 $|\\delta| \\le u$，$u$ 是单位舍入）出发，推导计算标量 \n   $$\\alpha_k = \\frac{ r_k^T r_k }{ p_k^T A p_k } \\quad \\text{和} \\quad \\beta_k = \\frac{ r_{k+1}^T r_{k+1} }{ r_k^T r_k }$$ \n   时的相对舍入误差的上界。假设点积是通过对 $n$ 个乘积的直接求和来计算的，矩阵向量积是通过与 $A$ 的行进行 $n$ 次点积来计算的。使用广为接受的点积和矩阵向量积中累积舍入误差的界，例如对于一个 $n$ 项和的相对误差，其界的形式为 $ \\gamma_n = \\frac{n u}{1 - n u} $，并推导出仅依赖于 $u$、$n$ 以及相关向量和矩阵的范数的界。除了 $A$ 的对称性和正定性之外，不要假设任何特殊结构。为每个标量提供能清晰区分分子和分母计算贡献的界。\n\n2. 根据您的界和共轭梯度法的结构，解释 $\\alpha_k$ 和 $\\beta_k$ 中的微小相对误差如何破坏搜索方向的 $A$-共轭性和残差的相互正交性，并说明为什么这种破坏在实践中会减慢收敛速度。您的解释应明确地将 $\\alpha_k$ 和 $\\beta_k$ 中的误差项与 $ p_{k+1}^T A p_k $ 和 $ r_{k+1}^T r_k $ 的非零值联系起来，并应论证为什么当 $A$ 的条件数很大时，即使相对于 $1$ 很小的误差也可能产生可测量的影响。\n\n3. 实现一个程序，构建一个 $n \\times n$ 的对称正定矩阵 $A$ 和一个向量 $b$，然后运行共轭梯度的两个变体：\n   - 一个未受扰动的变体，直接使用计算出的 $\\alpha_k$ 和 $\\beta_k$。\n   - 一个受扰动的变体，引入形式为 $\\tilde{\\alpha}_k = \\alpha_k (1 + \\delta_{\\alpha,k})$ 和 $\\tilde{\\beta}_k = \\beta_k (1 + \\delta_{\\beta,k})$ 的受控乘性相对误差，其中扰动 $\\delta_{\\alpha,k}$ 和 $\\delta_{\\beta,k}$ 根据下面的测试套件选择。\n\n   通过残差的欧几里得范数来衡量收敛性，并报告达到 $ \\| r_k \\|_2 \\le \\tau \\| b \\|_2 $ 所需的迭代次数，其中 $\\tau$ 是给定的容差。\n\n使用以下测试套件以确保覆盖范围：\n- 测试用例 1 (基线，理想情况)：维度 $n = 200$，容差 $\\tau = 10^{-8}$，使用固定伪随机种子生成的确定性向量 $b$，且无扰动，即对所有 $k$ 都有 $\\delta_{\\alpha,k} = 0$ 和 $\\delta_{\\beta,k} = 0$。\n- 测试用例 2 (接近单位舍入的边界情况)：与测试用例 1 相同的 $A$、$b$ 和 $\\tau$，但使用确定性的小扰动 $\\delta_{\\alpha,k} = \\eta$ 和 $\\delta_{\\beta,k} = \\eta$，其中 $\\eta = 10^{-12}$ 对所有 $k$ 都是常数。\n- 测试用例 3 (随机扰动)：相同的 $A$、$b$ 和 $\\tau$，使用随机混合符号扰动 $\\delta_{\\alpha,k}$ 和 $\\delta_{\\beta,k}$，在每次迭代中以相等的概率从 $\\{ -\\eta, +\\eta \\}$ 中独立抽取，其中 $\\eta = 10^{-8}$，并由一个固定的伪随机种子控制符号。\n- 测试用例 4 (导致减速的微小有偏扰动)：相同的 $A$、$b$ 和 $\\tau$，使用确定性正扰动 $\\delta_{\\alpha,k} = \\eta$ 和 $\\delta_{\\beta,k} = \\eta$，其中 $\\eta = 5 \\times 10^{-4}$ 对所有 $k$ 都是常数。\n\n将 $A$ 构建为大小为 $n$ 的一维狄利克雷拉普拉斯算子（one-dimensional Dirichlet Laplacian），即一个三对角矩阵，其主对角线上的元素为 $2$，第一副对角线和第一超对角线上的元素为 $-1$，已知该矩阵是对称正定的。使用固定的伪随机种子，通过抽取独立的标准正态分布项来构建 $b$。设置初始猜测值 $x_0 = 0$，并将最大迭代次数限制为 $n$。\n\n您的程序必须生成单行输出，其中包含每个测试用例收敛所需的迭代次数，按上述顺序列出，并聚合为一个用方括号括起来的逗号分隔列表，例如 $[i_1,i_2,i_3,i_4]$，其中每个 $i_j$ 是一个整数。不涉及物理单位。不出现角度。不使用百分比；所有量均为无量纲实数。程序必须是自包含的，仅使用指定的库，并且不得需要任何用户输入或文件访问。",
            "solution": "该问题陈述是数值线性代数中的一个有效练习，专注于共轭梯度（CG）方法的舍入误差分析。它具有科学依据，问题提法得当，并为可复现的计算实验提供了所有必要的参数和条件。\n\n用于求解对称正定（SPD）线性系统 $A x = b$ 的共轭梯度算法初始化为 $x_0 = 0$，$r_0 = b$ 和 $p_0 = r_0$。对于 $k = 0, 1, 2, \\dots$，其迭代如下：\n$$\n\\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k}\n$$\n$$\nx_{k+1} = x_k + \\alpha_k p_k\n$$\n$$\nr_{k+1} = r_k - \\alpha_k A p_k\n$$\n$$\n\\beta_k = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}\n$$\n$$\np_{k+1} = r_{k+1} + \\beta_k p_k\n$$\n\n### 1. $\\alpha_k$ 和 $\\beta_k$ 的舍入误差界\n\n我们分析在有限精度算术中计算 $\\alpha_k$ 和 $\\beta_k$ 时的误差。我们使用标准浮点算术模型，$\\mathrm{fl}(a \\ \\mathrm{op} \\ b) = (a \\ \\mathrm{op} \\ b)(1 + \\delta)$，其中 $|\\delta| \\le u$，$u$ 是单位舍入。对于一个 $n$ 项和，累积相对误差由 $\\gamma_n = \\frac{nu}{1-nu}$ 界定。令 $\\hat{v}$ 表示向量或标量 $v$ 的计算所得浮点表示。下面的分析考虑了从已计算的向量 $\\hat{r}_k$、$\\hat{p}_k$ 和 $\\hat{r}_{k+1}$ 计算 $\\alpha_k$ 和 $\\beta_k$ 时产生的误差。\n\n**$\\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k}$ 的分析**\n\n$\\hat{\\alpha}_k$ 的计算涉及两个点积、一个矩阵向量积和一个除法。\n\n**分子：** 分子是 $N_k = r_k^T r_k$。其计算值 $\\hat{N}_k = \\mathrm{fl}(\\hat{r}_k^T \\hat{r}_k)$ 涉及一个长度为 $n$ 的内积。点积 $x^T y$ 的舍入误差由 $|\\mathrm{fl}(x^T y) - x^T y| \\le \\gamma_n |x|^T |y|$ 界定。对于 $x=y$，这给出：\n$$\n|\\hat{N}_k - \\hat{r}_k^T \\hat{r}_k| \\le \\gamma_n |\\hat{r}_k|^T |\\hat{r}_k| = \\gamma_n \\|\\hat{r}_k\\|_2^2\n$$\n这可以写成 $\\hat{N}_k = (\\hat{r}_k^T \\hat{r}_k)(1 + \\delta_N)$，其中分子计算的相对误差界为 $|\\delta_N| \\le \\gamma_n$。\n\n**分母：** 分母是 $D_k = p_k^T A p_k$。其计算涉及一个矩阵向量积，然后是一个点积。\n步骤 1：计算矩阵向量积 $\\hat{v}_k = \\mathrm{fl}(A \\hat{p}_k)$。误差 $e_v = \\hat{v}_k - A \\hat{p}_k$ 的分量由 $|(e_v)_i| \\le \\gamma_n (|A| |\\hat{p}_k|)_i$ 界定，假设 $A$ 是稠密矩阵。\n步骤 2：计算点积 $\\hat{D}_k = \\mathrm{fl}(\\hat{p}_k^T \\hat{v}_k)$。这会引入其自身的误差。\n对于整个运算，一个标准结果给出了绝对误差的界：\n$$\n|\\hat{D}_k - \\hat{p}_k^T A \\hat{p}_k| \\le \\gamma_n |\\hat{p}_k|^T |A| |\\hat{p}_k| + \\gamma_n |\\hat{p}_k^T \\mathrm{fl}(A \\hat{p}_k)|\n$$\n简化并组合这些误差源，得到相对误差 $\\delta_D$ 使得 $\\hat{D}_k = (\\hat{p}_k^T A \\hat{p}_k)(1+\\delta_D)$，其界的形式为：\n$$\n|\\delta_D| \\le \\gamma_n \\left(1 + \\frac{|\\hat{p}_k|^T |A| |\\hat{p}_k|}{\\hat{p}_k^T A \\hat{p}_k}\\right)\n$$\n这个界将最终点积的误差（$\\gamma_n$ 项）与矩阵向量积的误差分开，后者被一个涉及矩阵和向量范数的因子所缩放。\n\n**$\\alpha_k$ 的总相对误差：** 最终的计算是一个除法，$\\hat{\\alpha}_k = \\mathrm{fl}(\\hat{N}_k / \\hat{D}_k) = (\\hat{N}_k / \\hat{D}_k)(1 + \\delta_{div})$，其中 $|\\delta_{div}| \\le u$。总相对误差为：\n$$\n\\frac{\\hat{\\alpha}_k - \\alpha_k^{\\mathrm{computed}}}{\\alpha_k^{\\mathrm{computed}}} = \\frac{(1+\\delta_N)(1+\\delta_{div})}{1+\\delta_D} - 1 \\approx \\delta_N - \\delta_D + \\delta_{div}\n$$\n总相对误差的上界约为 $|\\delta_N| + |\\delta_D| + u$。因此，分子和分母的贡献为：\n- 分子贡献界：$\\gamma_n$。\n- 分母贡献界：$\\gamma_n \\left(1 + \\frac{|p_k|^T |A| |p_k|}{p_k^T A p_k}\\right)$。\n\n**$\\beta_k = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}$ 的分析**\n\n$\\hat{\\beta}_k$ 的计算是两个点积之比。\n**分子：** $\\hat{N}_{k+1} = \\mathrm{fl}(\\hat{r}_{k+1}^T \\hat{r}_{k+1}) = (\\hat{r}_{k+1}^T \\hat{r}_{k+1})(1 + \\delta_{N,k+1})$，其中 $|\\delta_{N,k+1}| \\le \\gamma_n$。\n**分母：** $\\hat{D}_k = \\mathrm{fl}(\\hat{r}_k^T \\hat{r}_k) = (\\hat{r}_k^T \\hat{r}_k)(1 + \\delta_{D,k})$，其中 $|\\delta_{D,k}| \\le \\gamma_n$。\n\n**$\\beta_k$ 的总相对误差：** 最终的除法给出 $\\hat{\\beta}_k = (\\hat{N}_{k+1} / \\hat{D}_k)(1 + \\delta_{div})$。总相对误差为：\n$$\n\\frac{\\hat{\\beta}_k - \\beta_k^{\\mathrm{computed}}}{\\beta_k^{\\mathrm{computed}}} \\approx \\delta_{N,k+1} - \\delta_{D,k} + \\delta_{div}\n$$\n这个相对误差的模的上界约为 $2\\gamma_n + u$。贡献为：\n- 分子贡献界：$\\gamma_n$。\n- 分母贡献界：$\\gamma_n$。\n\n### 2. 正交性的破坏与收敛性\n\n在精确算术中，CG 方法生成的搜索方向 $\\{p_k\\}$ 是 A-共轭的（$p_i^T A p_j = 0$ for $i \\ne j$），残差 $\\{r_k\\}$ 是相互正交的（$r_i^T r_j = 0$ for $i \\ne j$）。这些性质保证了在至多 $n$ 次迭代内收敛。浮点误差会破坏这些性质，可能减慢收敛速度。\n\n搜索方向的更新是 $p_{k+1} = r_{k+1} + \\beta_k p_k$。选择 $\\beta_k$ 是为了确保 $p_{k+1}$ 与 $p_k$ 是 A-共轭的。我们来分析 $\\alpha_k$ 和 $\\beta_k$ 中的误差是如何破坏这一点的。在下文中，令 $\\hat{\\alpha}_k = \\alpha_k^{\\mathrm{true}}(1+\\delta_\\alpha)$ 和 $\\hat{\\beta}_k = \\beta_k^{\\mathrm{true}}(1+\\delta_\\beta)$，其中‘真’值表示将精确公式应用于计算出的向量所得的值，而 $\\delta_\\alpha, \\delta_\\beta$ 是等效相对误差。需要检查的量是 $\\hat{p}_{k+1}^T A \\hat{p}_k$：\n$$\n\\hat{p}_{k+1}^T A \\hat{p}_k = (\\hat{r}_{k+1} + \\hat{\\beta}_k \\hat{p}_k)^T A \\hat{p}_k = \\hat{r}_{k+1}^T A \\hat{p}_k + \\hat{\\beta}_k \\hat{p}_k^T A \\hat{p}_k\n$$\n从残差更新 $\\hat{r}_{k+1} \\approx \\hat{r}_k - \\hat{\\alpha}_k A \\hat{p}_k$，我们有 $A \\hat{p}_k \\approx (\\hat{r}_k - \\hat{r}_{k+1}) / \\hat{\\alpha}_k$。代入此式得到：\n$$\n\\hat{r}_{k+1}^T A \\hat{p}_k \\approx \\frac{1}{\\hat{\\alpha}_k} \\hat{r}_{k+1}^T (\\hat{r}_k - \\hat{r}_{k+1}) = \\frac{1}{\\hat{\\alpha}_k} (\\hat{r}_{k+1}^T \\hat{r}_k - \\|\\hat{r}_{k+1}\\|_2^2)\n$$\n即使在有限精度下，计算出的残差 $\\hat{r}_{k+1}$ 在构造上几乎与 $\\hat{r}_k$ 正交，因此 $\\hat{r}_{k+1}^T \\hat{r}_k \\approx 0$。于是，$\\hat{r}_{k+1}^T A \\hat{p}_k \\approx -\\|\\hat{r}_{k+1}\\|_2^2 / \\hat{\\alpha}_k$。A-共轭性检查变为：\n$$\n\\hat{p}_{k+1}^T A \\hat{p}_k \\approx -\\frac{\\|\\hat{r}_{k+1}\\|_2^2}{\\hat{\\alpha}_k} + \\hat{\\beta}_k \\hat{p}_k^T A \\hat{p}_k\n$$\n将 $\\hat{\\alpha}_k$ 和 $\\hat{\\beta}_k$ 的表达式用其‘真’值和误差项替换：\n$$\n\\hat{p}_{k+1}^T A \\hat{p}_k \\approx -\\frac{\\|\\hat{r}_{k+1}\\|_2^2}{\\alpha_k^{\\mathrm{true}}(1+\\delta_\\alpha)} + \\beta_k^{\\mathrm{true}}(1+\\delta_\\beta)\\hat{p}_k^T A \\hat{p}_k\n$$\n使用 $\\alpha_k^{\\mathrm{true}} = \\frac{\\|\\hat{r}_k\\|_2^2}{\\hat{p}_k^T A \\hat{p}_k}$ 和 $\\beta_k^{\\mathrm{true}} = \\frac{\\|\\hat{r}_{k+1}\\|_2^2}{\\|\\hat{r}_k\\|_2^2}$：\n$$\n\\hat{p}_{k+1}^T A \\hat{p}_k \\approx -\\frac{\\|\\hat{r}_{k+1}\\|_2^2 (\\hat{p}_k^T A \\hat{p}_k)}{\\|\\hat{r}_k\\|_2^2(1+\\delta_\\alpha)} + \\frac{\\|\\hat{r}_{k+1}\\|_2^2}{\\|\\hat{r}_k\\|_2^2}(1+\\delta_\\beta)\\hat{p}_k^T A \\hat{p}_k\n$$\n提出公因式得到：\n$$\n\\hat{p}_{k+1}^T A \\hat{p}_k \\approx \\frac{\\|\\hat{r}_{k+1}\\|_2^2 \\hat{p}_k^T A \\hat{p}_k}{\\|\\hat{r}_k\\|_2^2} \\left[ -\\frac{1}{1+\\delta_\\alpha} + (1+\\delta_\\beta) \\right]\n$$\n对于小的 $\\delta_\\alpha$，使用近似 $1/(1+\\delta_\\alpha) \\approx 1-\\delta_\\alpha$，这简化为：\n$$\n\\hat{p}_{k+1}^T A \\hat{p}_k \\approx \\frac{\\|\\hat{r}_{k+1}\\|_2^2}{\\alpha_k^{\\mathrm{true}}} (\\delta_\\alpha + \\delta_\\beta)\n$$\n这个结果明确地将 A-共轭性的损失与 $\\alpha_k$ 和 $\\beta_k$ 中相对误差的总和联系起来。当这些误差非零时，新的搜索方向与前一个方向不再是 A-共轭的。这种损失会累积，意味着搜索方向失去了作为克雷洛夫子空间（Krylov subspace）的 A-正交基的性质。因此，算法可能会在那些本应被消除的方向上重新引入误差分量，从而减慢收敛速度。\n\n当条件数 $\\kappa(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}$ 很大时，CG 的收敛速度可能会很慢。即使当误差 $x-x_k$ 仍然显著时，残差的范数也可能变得非常小。在这种情况下，来自机器精度的固有舍入误差相对于像 $\\|r_k\\|_2^2$ 这样的量可能会变得很大。这使得计算出的 $\\alpha_k$ 和 $\\beta_k$ 不那么准确，导致更严重的正交性损失。算法的性能从其理论上的超线性收敛退化到像最速下降法那样的慢得多的线性收敛，常常导致停滞或需要远超过 $n$ 次迭代才能达到期望的容差。",
            "answer": "```python\nimport numpy as np\n\nclass RandomPerturb:\n    \"\"\"\n    A stateful callable object to generate sequences of random perturbations.\n    \"\"\"\n    def __init__(self, seed, eta):\n        # Use a dedicated RNG for perturbations to isolate its state\n        self.rng = np.random.default_rng(seed)\n        self.eta = eta\n        self.choices = [-self.eta, self.eta]\n\n    def __call__(self, k):\n        # On each call, return a random pair of perturbations. k is unused.\n        delta_alpha = self.rng.choice(self.choices)\n        delta_beta = self.rng.choice(self.choices)\n        return delta_alpha, delta_beta\n\ndef conjugate_gradient(A, b, tau, max_iter, pert_gen_func):\n    \"\"\"\n    Implements the Conjugate Gradient method for solving Ax=b.\n\n    Args:\n        A (np.ndarray): The symmetric positive definite matrix.\n        b (np.ndarray): The right-hand side vector.\n        tau (float): The relative tolerance for convergence.\n        max_iter (int): The maximum number of iterations.\n        pert_gen_func (callable): A function that takes iteration k and returns\n                                  a tuple (delta_alpha, delta_beta) of\n                                  multiplicative relative perturbations.\n    Returns:\n        int: The number of iterations required to converge.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n)\n    \n    # Initialize CG algorithm with x_0 = 0\n    r = b.copy()  # r_0 = b - A@x_0 = b\n    p = r.copy()  # p_0 = r_0\n    \n    rs_old_sq = r.T @ r\n    \n    b_norm = np.linalg.norm(b)\n    convergence_threshold = tau * b_norm\n\n    for k in range(max_iter):\n        # Check for convergence at the beginning of the iteration\n        if np.sqrt(rs_old_sq) <= convergence_threshold:\n            return k\n\n        Ap = A @ p\n        \n        # Standard CG computation for alpha\n        # Note: p.T @ Ap can be near zero if p is close to an eigenvector\n        # associated with a zero eigenvalue, but A is SPD, so this is > 0 for p!=0\n        alpha = rs_old_sq / (p.T @ Ap)\n        \n        # Apply controlled perturbation\n        delta_alpha, delta_beta = pert_gen_func(k)\n        alpha = alpha * (1.0 + delta_alpha)\n\n        # Update solution and residual\n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        rs_new_sq = r.T @ r\n        \n        # Standard CG computation for beta\n        beta = rs_new_sq / rs_old_sq\n        beta = beta * (1.0 + delta_beta)\n        \n        # Update search direction\n        p = r + beta * p\n        \n        # Prepare for next iteration\n        rs_old_sq = rs_new_sq\n\n    # If max_iter is reached without convergence\n    return max_iter\n\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test suite for the Conjugate Gradient analysis.\n    \"\"\"\n    # Problem parameters\n    n = 200\n    tau = 1.0e-8\n    max_iter = n\n\n    # Construct the 1D Dirichlet Laplacian matrix A\n    A = np.diag(2.0 * np.ones(n)) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n\n    # Generate the vector b using a fixed seed for reproducibility\n    b_seed = 42\n    rng_b = np.random.default_rng(b_seed)\n    b = rng_b.standard_normal(n)\n\n    # Define the perturbation generators for each test case\n    test_cases = [\n        # Case 1: Unperturbed (baseline)\n        {'name': 'Unperturbed', 'pert_gen': lambda k: (0.0, 0.0)},\n        \n        # Case 2: Small constant perturbations\n        {'name': 'Small Constant Perturbation', 'pert_gen': lambda k: (1.0e-12, 1.0e-12)},\n        \n        # Case 3: Random mixed-sign perturbations\n        {'name': 'Random Perturbation', 'pert_gen': RandomPerturb(seed=123, eta=1.0e-8)},\n        \n        # Case 4: Larger biased perturbations\n        {'name': 'Biased Perturbation', 'pert_gen': lambda k: (5.0e-4, 5.0e-4)}\n    ]\n\n    iteration_counts = []\n    for case in test_cases:\n        iterations = conjugate_gradient(A, b, tau, max_iter, case['pert_gen'])\n        iteration_counts.append(iterations)\n    \n    # Print the results in the specified format\n    print(f\"[{','.join(map(str, iteration_counts))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        }
    ]
}