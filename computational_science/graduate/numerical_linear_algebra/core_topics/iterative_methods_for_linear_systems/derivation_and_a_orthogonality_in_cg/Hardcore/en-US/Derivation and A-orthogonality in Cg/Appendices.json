{
    "hands_on_practices": [
        {
            "introduction": "To bridge the gap between abstract theory and tangible insight, our first practice explores the Conjugate Gradient method in a setting where its core concepts become visualizable. By applying CG to a system defined by a graph Laplacian matrix $L$, we will discover that the crucial property of $L$-orthogonality corresponds to a physical notion of non-interfering energy flows on the graph. This exercise  will guide you through deriving this connection and verifying it computationally, transforming an algebraic condition into a powerful physical intuition.",
            "id": "3543444",
            "problem": "Design and analyze a computational experiment that demonstrates the origin and meaning of $A$-orthogonality in the Conjugate Gradient (CG) method when $A$ is a graph Laplacian $L$, and relate it to the Rayleigh quotient and to energy flows on the graph. Your experiment and explanation must proceed from first principles in numerical linear algebra and graph theory, without assuming any specific CG update formulas a priori.\n\nYou must use the following foundational bases only:\n- Definitions: For a finite, undirected, weighted graph with node set $\\{0,\\dots,n-1\\}$ and symmetric nonnegative weights $w_{uv}=w_{vu}$, the graph Laplacian is $L=D-W$, where $D$ is the diagonal degree matrix with $D_{uu}=\\sum_v w_{uv}$ and $W$ is the weighted adjacency matrix with $W_{uv}=w_{uv}$. For vectors $u,v\\in\\mathbb{R}^n$, define the energy inner product $\\langle u,v\\rangle_L:=u^\\top L v=\\sum_{(u,v)} w_{uv}\\,(u_u-u_v)(v_u-v_v)$ and the energy (Dirichlet) functional $\\mathcal{E}(x)=\\tfrac{1}{2}\\,x^\\top L x-b^\\top x$. The Rayleigh quotient of a nonzero vector $p$ is $R(p):=\\dfrac{p^\\top L p}{p^\\top p}$.\n- Facts: If a symmetric matrix $A$ is positive definite, then the energy functional $\\tfrac{1}{2}x^\\top A x-b^\\top x$ has a unique minimizer satisfying $A x=b$. For a symmetric positive definite matrix $A$, the Rayleigh quotient satisfies $\\lambda_{\\min}(A)\\le R(p)\\le \\lambda_{\\max}(A)$ for all nonzero $p$, where $\\lambda_{\\min}(A)$ and $\\lambda_{\\max}(A)$ denote the smallest and largest eigenvalues, respectively.\n\nYour tasks:\n1) Derive from first principles how the Conjugate Gradient method arises by minimizing the quadratic energy functional $\\mathcal{E}(x)$ over a sequence of nested affine Krylov subspaces, starting from an initial guess $x_0$. From the requirement that each iterate minimizes $\\mathcal{E}(x)$ over the current subspace, show that the search directions $\\{p_k\\}$ can be chosen so that they are mutually $L$-orthogonal, i.e., $p_i^\\top L p_j=0$ for $i\\ne j$, and that the residuals $\\{r_k\\}$ satisfy $r_k=b-Lx_k$ and are orthogonal in the Euclidean inner product. Present a proof that this $L$-orthogonality follows from the optimality conditions for the quadratic minimization and the symmetry of $L$.\n2) Interpret $L$-orthogonality of directions in terms of energy flows on the graph: use the identity $u^\\top L v=\\sum_{(a,b)} w_{ab}\\,(u_a-u_b)(v_a-v_b)$ to explain why $\\langle p_i,p_j\\rangle_L=0$ means that the edgewise flow patterns induced by $p_i$ and $p_j$ are orthogonal in the energy sense.\n3) Design a concrete numerical experiment that constructs three specific graphs, forms $L$, imposes a Dirichlet condition by grounding node $0$ (remove row and column $0$ from $L$ and the corresponding entry from $b$ to obtain a symmetric positive definite system), and runs CG from the zero initial guess. For each graph, record all CG search directions $\\{p_k\\}$ produced until convergence (measured by the Euclidean norm of the residual) and verify numerically:\n   - $L$-orthogonality: form the Gram matrix $G$ with $G_{ij}=p_i^\\top L p_j$. Report the maximum absolute off-diagonal entry normalized by the maximum diagonal entry, and a boolean indicating whether it is below a given tolerance.\n   - Energy-flow equivalence: verify that $p_i^\\top L p_j$ equals $\\sum_{(a,b)} w_{ab}\\,(p_i[a]-p_i[b])(p_j[a]-p_j[b])$ when the grounded node is treated as having value $0$. Report the maximum absolute discrepancy over all pairs $(i,j)$.\n   - Rayleigh quotient bounds: compute $R(p_k)$ for each search direction and the extreme eigenvalues $\\lambda_{\\min}(L_{\\text{grounded}})$ and $\\lambda_{\\max}(L_{\\text{grounded}})$. Report whether all $R(p_k)$ lie within the closed interval $[\\lambda_{\\min},\\lambda_{\\max}]$ up to tolerance, along with the minimum and maximum Rayleigh quotients observed.\n\nTest suite to implement:\n- Test $1$ (happy path): Path graph of size $n=8$ with unit weights on edges $(0,1),(1,2),\\dots,(6,7)$. Ground node $0$. Right-hand side $b$ is the standard basis vector at the last node, i.e., $b=e_7$, restricted to the ungrounded nodes.\n- Test $2$ (two-dimensional coupling): $3\\times 3$ grid graph ($n=9$) with unit weights and $4$-neighborhood connectivity. Ground node $0$. Right-hand side $b=e_4$, restricted to the ungrounded nodes.\n- Test $3$ (weighted star, conditioning edge case): Star graph with $n=6$ where node $0$ is the center connected to nodes $1,2,3,4,5$ with weights $1,3,5,7,11$. Ground node $0$. Right-hand side $b=[1,-1,2,-2,1]^\\top$ on nodes $1$ through $5$.\n\nNumerical details:\n- Use the zero initial guess $x_0=0$ for all tests.\n- Use a stopping tolerance $\\varepsilon=10^{-12}$ on the Euclidean norm of the residual or a maximum of $m$ steps where $m$ is the dimension of the grounded system.\n- Use the tolerance $10^{-10}$ to declare $L$-orthogonality and Rayleigh quotient inclusion success.\n\nRequired final output format:\nYour program should produce a single line of output containing a list of three entries, one per test, where each entry is itself a list containing:\n- a boolean indicating whether $L$-orthogonality holds within tolerance,\n- a float equal to the maximum absolute off-diagonal of $G$ normalized by the maximum diagonal of $G$,\n- a boolean indicating whether all Rayleigh quotients of the search directions lie within the eigenvalue bounds within tolerance,\n- a float equal to the maximum absolute discrepancy between $p_i^\\top L p_j$ and the edgewise energy sum over all $i,j$.\n\nConcretely, the output must be in the form:\n$[[\\text{orth\\_ok}_1,\\text{max\\_offdiag\\_norm}_1,\\text{rq\\_ok}_1,\\text{max\\_energy\\_diff}_1],[\\text{orth\\_ok}_2,\\text{max\\_offdiag\\_norm}_2,\\text{rq\\_ok}_2,\\text{max\\_energy\\_diff}_2],[\\text{orth\\_ok}_3,\\text{max\\_offdiag\\_norm}_3,\\text{rq\\_ok}_3,\\text{max\\_energy\\_diff}_3]]$\nwith booleans and decimal numbers as described.",
            "solution": "The analysis of the Conjugate Gradient (CG) method for solving linear systems involving a graph Laplacian $L$ reveals deep connections between numerical optimization, linear algebra, and graph theory. We will first derive the method from the principle of energy minimization over expanding subspaces, demonstrating how this naturally leads to the defining property of $L$-orthogonality. We will then interpret this property in the context of energy flows on the graph. Finally, a numerical experiment will be designed to verify these theoretical results.\n\n### 1. Derivation of the Conjugate Gradient Method and $L$-Orthogonality\n\nThe core problem is to solve the linear system $Lx=b$, where $L$ is a symmetric positive-definite (SPD) matrix. In our context, $L$ is a graph Laplacian of a connected graph with at least one grounded node, which guarantees it is SPD. Solving $Lx=b$ is equivalent to finding the unique minimizer of the quadratic energy functional:\n$$\n\\mathcal{E}(x) = \\frac{1}{2}x^\\top L x - b^\\top x\n$$\nThe gradient of this functional is $\\nabla\\mathcal{E}(x) = Lx - b$. The negative gradient, $r(x) = b - Lx$, is the residual of the system at a given approximation $x$. The minimum of $\\mathcal{E}(x)$ is achieved when $\\nabla\\mathcal{E}(x)=0$, i.e., when the residual $r(x)=0$.\n\nThe Conjugate Gradient method is an iterative procedure that, at step $k$, finds the exact minimizer of $\\mathcal{E}(x)$ within a specific affine subspace. Starting from an initial guess $x_0$, we define the initial residual $r_0 = b - Lx_0$. The search for the solution is restricted to the affine Krylov subspace:\n$$\nx_k \\in x_0 + \\mathcal{K}_k(L, r_0)\n$$\nwhere $\\mathcal{K}_k(L, r_0) = \\text{span}\\{r_0, Lr_0, L^2r_0, \\dots, L^{k-1}r_0\\}$ is the $k$-th Krylov subspace generated by $L$ and $r_0$.\n\nThe iterate $x_k$ is defined as the point that minimizes $\\mathcal{E}(x)$ over this subspace:\n$$\nx_k = \\arg\\min_{x \\in x_0 + \\mathcal{K}_k(L, r_0)} \\mathcal{E}(x)\n$$\nA necessary and sufficient condition for this optimality is that the gradient of $\\mathcal{E}(x)$ at $x_k$, which is $-r_k$, must be orthogonal to the subspace of search directions, $\\mathcal{K}_k(L, r_0)$. This gives the fundamental optimality condition:\n$$\nr_k^\\top v = 0 \\quad \\forall v \\in \\mathcal{K}_k(L, r_0)\n$$\nThis condition immediately implies that the residuals are mutually orthogonal. For any $j  k$, the residual $r_j$ belongs to the subspace $\\mathcal{K}_{j+1}(L, r_0)$ because $x_j - x_0 \\in \\mathcal{K}_j$, so $r_j = b - L x_j = b - L(x_0 + (x_j - x_0)) = r_0 - L(x_j-x_0)$, and since $L\\mathcal{K}_j \\subset \\mathcal{K}_{j+1}$, it follows that $r_j \\in \\mathcal{K}_{j+1}$. As $\\mathcal{K}_{j+1} \\subset \\mathcal{K}_k$, we have $r_j \\in \\mathcal{K}_k(L, r_0)$. The optimality condition $r_k \\perp \\mathcal{K}_k(L, r_0)$ thus directly implies:\n$$\nr_k^\\top r_j = 0 \\quad \\text{for } j \\neq k\n$$\nThis is the property of residual orthogonality.\n\nInstead of solving the full minimization problem in the expanding subspace $\\mathcal{K}_k$ at each step, CG builds the solution iteratively. We seek a sequence of search directions $\\{p_0, p_1, \\dots, p_{k-1}\\}$ that forms a basis for $\\mathcal{K}_k(L, r_0)$. If we cleverly choose this basis to be orthogonal with respect to the inner product induced by $L$ (i.e., $\\langle u, v \\rangle_L = u^\\top L v$), the process becomes highly efficient. We demand that the search directions be $L$-orthogonal (or \"conjugate\" with respect to $L$):\n$$\np_i^\\top L p_j = 0 \\quad \\text{for } i \\neq j\n$$\nA full Gram-Schmidt-like process could construct such a basis: $p_k = r_k - \\sum_{j=0}^{k-1} \\frac{r_k^\\top L p_j}{p_j^\\top L p_j} p_j$. A crucial simplification, however, arises from the symmetry of $L$ and the properties of the CG iterates. A key property is that the update $x_k = x_{k-1} + \\alpha_{k-1} p_{k-1}$ leads to the residual update $r_k = r_{k-1} - \\alpha_{k-1} L p_{k-1}$. From this, one can show that $r_k^\\top L p_j = 0$ for $j  k-1$. Consequently, the Gram-Schmidt summation collapses to a single term:\n$$\np_k = r_k - \\frac{r_k^\\top L p_{k-1}}{p_{k-1}^\\top L p_{k-1}} p_{k-1}\n$$\nThis yields the famous two-term recurrence for the search directions, $p_k = r_k + \\beta_{k-1} p_{k-1}$, where the coefficient $\\beta_{k-1}$ is chosen to enforce conjugacy. The derivation above shows one way to compute it: $\\beta_{k-1} = -\\frac{r_k^\\top L p_{k-1}}{p_{k-1}^\\top L p_{k-1}}$. This construction, by its nature, guarantees that $p_k^\\top L p_{k-1}=0$. The argument above shows it also ensures $p_k^\\top L p_j=0$ for all $jk-1$. Thus, the set of search directions $\\{p_k\\}$ is indeed mutually $L$-orthogonal.\n\n### 2. Interpretation of $L$-Orthogonality as Energy Flow Orthogonality\n\nThe $L$-orthogonality condition, $p_i^\\top L p_j = 0$, has a compelling physical interpretation on the graph. The problem provides the identity for the energy inner product:\n$$\n\\langle p_i, p_j \\rangle_L = p_i^\\top L p_j = \\sum_{(a,b)} w_{ab} (p_i[a] - p_i[b]) (p_j[a] - p_j[b])\n$$\nwhere the sum is over all edges $(a,b)$ in the graph, $w_{ab}$ is the weight of the edge, and $p_k[v]$ is the component of vector $p_k$ corresponding to node $v$.\n\nLet us interpret the vector $p_k$ as a potential function defined on the nodes of the graph.\n- The quantity $p_k[a] - p_k[b]$ is the potential difference across the edge $(a,b)$ induced by the potential function $p_k$.\n- The edge weight $w_{ab}$ represents the conductance of the edge, analogous to electrical conductance.\n- The term $f_{ab}^{(k)} = w_{ab}(p_k[a] - p_k[b])$ can be interpreted as the flow along the edge $(a,b)$ driven by the potential differences in $p_k$. This is a direct analogue of Ohm's Law (Current = Conductance $\\times$ Voltage Difference). The collection of these values $\\{f_{ab}^{(k)}\\}$ for all edges defines a flow pattern on the graph.\n\nWith this interpretation, the energy inner product becomes a sum over the edges of the product of flows, weighted by the edge resistivity ($1/w_{ab}$):\n$$\np_i^\\top L p_j = \\sum_{(a,b)} \\frac{1}{w_{ab}} f_{ab}^{(i)} f_{ab}^{(j)}\n$$\nThis expression is a weighted inner product of the two flow vectors corresponding to the potentials $p_i$ and $p_j$. Therefore, the condition $p_i^\\top L p_j=0$ means that the flow pattern induced by search direction $p_i$ is orthogonal to the flow pattern induced by search direction $p_j$ with respect to this energy-based inner product.\n\nIn essence, the Conjugate Gradient method systematically discovers a basis of potential functions $\\{p_k\\}$ whose corresponding flow patterns are mutually non-interfering in an energetic sense. Each step corrects the solution along a new, energy-orthogonal flow pattern, ensuring that progress made in one \"mode\" of flow is not undone by subsequent corrections.\n\n### 3. Design of the Numerical Experiment\n\nTo demonstrate these principles numerically, we will implement a computational experiment adhering to the problem's specifications.\n\n**Procedure:**\n1.  **Graph and Laplacian Construction**: For each of the three test cases (path, grid, star graph), a function will construct the weighted adjacency matrix $W$ and the degree matrix $D$, from which the graph Laplacian $L=D-W$ is formed. A list of edges and their weights will be retained for later calculations.\n2.  **Imposing Dirichlet Boundary Conditions**: Node $0$ is grounded. This is implemented by taking the submatrix of $L$ corresponding to the ungrounded nodes ($1, \\dots, n-1$), yielding an SPD matrix $L_{\\text{gnd}}$. The right-hand side vector $b$ is similarly restricted to $b_{\\text{gnd}}$.\n3.  **Conjugate Gradient Execution**: A custom CG solver will be implemented. It will start with the zero vector $x_0=0$ and iterate until the Euclidean norm of the residual is below $\\varepsilon=10^{-12}$ or the maximum number of iterations is reached. Crucially, this solver will store and return the complete sequence of search directions $\\{p_k\\}$ generated.\n4.  **Verification and Analysis**: After obtaining the search directions $\\{p_k\\}$, the following quantities will be computed for each test case:\n    - **$L$-orthogonality**: The Gram matrix $G$ with entries $G_{ij} = p_i^\\top L_{\\text{gnd}} p_j$ is computed. The maximum absolute off-diagonal entry is found and normalized by the maximum diagonal entry. This ratio is compared against a tolerance of $10^{-10}$ to declare orthogonality success.\n    - **Energy-Flow Equivalence**: For each pair $(i, j)$, we compute two values: $C_1 = p_i^\\top L_{\\text{gnd}} p_j$ and $C_2 = \\sum_{(a,b)} w_{ab}(p'_i[a]-p'_i[b])(p'_j[a]-p'_j[b])$. The vectors $p'_k$ are the full-graph potential vectors, constructed by augmenting the search directions $p_k$ (which live on the grounded subspace) with a value of $0$ at the grounded node $0$. The maximum absolute difference $|C_1 - C_2|$ over all pairs is reported.\n    - **Rayleigh Quotient Bounds**: The minimum and maximum eigenvalues of $L_{\\text{gnd}}$ are computed. For each search direction $p_k$, its Rayleigh quotient $R(p_k) = (p_k^\\top L_{\\text{gnd}} p_k) / (p_k^\\top p_k)$ is calculated. We verify if all computed Rayleigh quotients lie within the interval $[\\lambda_{\\min}(L_{\\text{gnd}}), \\lambda_{\\max}(L_{\\text{gnd}})]$ up to a tolerance of $10^{-10}$. A boolean will report the outcome of this check.\n\nThe results of these three verification steps will be collected and formatted into the required output structure, providing a quantitative demonstration of the theoretical principles derived.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to run the numerical experiments and print the results.\n    \"\"\"\n    TOL_ORTH = 1e-10\n    TOL_RQ = 1e-10\n    TOL_CG = 1e-12\n\n    def run_cg_and_collect_directions(A, b, x0, tol, max_iter):\n        \"\"\"\n        Custom Conjugate Gradient solver that returns all search directions.\n        \"\"\"\n        x = x0.copy()\n        r = b - A @ x\n        p = r.copy()\n        rs_old = r.T @ r\n        \n        if np.sqrt(rs_old)  tol:\n            return []\n\n        p_list = [p]\n        \n        for i in range(max_iter):\n            Ap = A @ p\n            alpha = rs_old / (p.T @ Ap)\n            x += alpha * p\n            r -= alpha * Ap\n            rs_new = r.T @ r\n            \n            if np.sqrt(rs_new)  tol:\n                break\n                \n            # Fletcher-Reeves update for beta\n            p = r + (rs_new / rs_old) * p\n            p_list.append(p)\n            rs_old = rs_new\n            \n        return p_list\n\n    def analyze_case(L, b_gnd, edges, n):\n        \"\"\"\n        Analyzes a single test case: grounds the system, runs CG, and performs verification.\n        \"\"\"\n        # Ground the system\n        gnd_indices = np.arange(1, n)\n        L_gnd = L[np.ix_(gnd_indices, gnd_indices)]\n        m = L_gnd.shape[0]\n\n        # Run CG\n        x0 = np.zeros(m)\n        p_dirs = run_cg_and_collect_directions(L_gnd, b_gnd, x0, TOL_CG, m)\n        k = len(p_dirs)\n\n        if k == 0: # Convergence in 0 steps\n            return [True, 0.0, True, 0.0]\n\n        # 1. L-orthogonality check\n        G = np.zeros((k, k))\n        for i in range(k):\n            for j in range(k):\n                G[i, j] = p_dirs[i].T @ L_gnd @ p_dirs[j]\n        \n        max_diag = np.max(np.abs(np.diag(G))) if k > 0 else 1.0\n        if max_diag == 0: max_diag = 1.0\n        \n        off_diag_G = G.copy()\n        np.fill_diagonal(off_diag_G, 0)\n        max_off_diag_norm = np.max(np.abs(off_diag_G)) / max_diag\n        orth_ok = max_off_diag_norm  TOL_ORTH\n\n        # 2. Energy-flow equivalence check\n        max_energy_diff = 0.0\n        for i in range(k):\n            for j in range(k):\n                c1 = G[i, j]\n                \n                # Construct full potential vectors with p[0]=0\n                p_i_full = np.zeros(n)\n                p_i_full[gnd_indices] = p_dirs[i]\n                p_j_full = np.zeros(n)\n                p_j_full[gnd_indices] = p_dirs[j]\n\n                c2 = 0.0\n                for u, v, w in edges:\n                    c2 += w * (p_i_full[u] - p_i_full[v]) * (p_j_full[u] - p_j_full[v])\n                \n                max_energy_diff = max(max_energy_diff, abs(c1 - c2))\n\n        # 3. Rayleigh quotient bounds check\n        eigvals = np.linalg.eigvalsh(L_gnd)\n        lambda_min, lambda_max = eigvals[0], eigvals[-1]\n        \n        rq_ok = True\n        for p in p_dirs:\n            p_norm_sq = p.T @ p\n            if p_norm_sq == 0: continue\n            rq = (p.T @ L_gnd @ p) / p_norm_sq\n            if not (lambda_min - TOL_RQ = rq = lambda_max + TOL_RQ):\n                rq_ok = False\n                break\n        \n        return [orth_ok, max_off_diag_norm, rq_ok, max_energy_diff]\n\n    def get_laplacian(n, edges):\n        W = np.zeros((n, n))\n        for u, v, w in edges:\n            W[u, v] = w\n            W[v, u] = w\n        D = np.diag(np.sum(W, axis=1))\n        return D - W\n\n    results = []\n\n    # Test 1: Path graph\n    n1 = 8\n    edges1 = [(i, i+1, 1.0) for i in range(n1 - 1)]\n    L1 = get_laplacian(n1, edges1)\n    b1_full = np.zeros(n1)\n    b1_full[7] = 1.0\n    b1_gnd = b1_full[1:]\n    results.append(analyze_case(L1, b1_gnd, edges1, n1))\n\n    # Test 2: 3x3 Grid graph\n    n2 = 9\n    edges2 = []\n    for r in range(3):\n        for c in range(3):\n            idx = r * 3 + c\n            # Right neighbor\n            if c  2:\n                right_idx = r * 3 + (c + 1)\n                edges2.append((idx, right_idx, 1.0))\n            # Down neighbor\n            if r  2:\n                down_idx = (r + 1) * 3 + c\n                edges2.append((idx, down_idx, 1.0))\n    L2 = get_laplacian(n2, edges2)\n    b2_full = np.zeros(n2)\n    b2_full[4] = 1.0 # Node 4 corresponding to grid pos (1,1)\n    b2_gnd = b2_full[1:]\n    results.append(analyze_case(L2, b2_gnd, edges2, n2))\n\n    # Test 3: Weighted star graph\n    n3 = 6\n    weights3 = [1, 3, 5, 7, 11]\n    edges3 = [(0, i + 1, w) for i, w in enumerate(weights3)]\n    L3 = get_laplacian(n3, edges3)\n    b3_gnd = np.array([1.0, -1.0, 2.0, -2.0, 1.0])\n    results.append(analyze_case(L3, b3_gnd, edges3, n3))\n\n    # Format output as specified\n    output_str = \"[\"\n    for i, res in enumerate(results):\n        # Format: [bool, float, bool, float]\n        output_str += f\"[{str(res[0]).lower()},{res[1]:.15f},{str(res[2]).lower()},{res[3]:.15f}]\"\n        if i  len(results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the intuition developed for $A$-orthogonality, our next practice delves into a modern application from the field of optimization and explores the practical challenges of numerical stability. We will formulate a regularized optimal transport problem as a linear system and use the Conjugate Gradient method to solve it. This exercise  focuses on how the regularization parameter $\\lambda$ impacts the conditioning of the system matrix $A_{\\lambda}$, and consequently, how well the theoretical properties of CG, such as search direction conjugacy and residual orthogonality, are maintained in finite-precision arithmetic.",
            "id": "3543447",
            "problem": "Consider a discrete one-dimensional transport setting on a path graph with $n$ nodes labeled $0,1,\\dots,n-1$ and $m=n-1$ directed edges $(i \\to i+1)$ for $i=0,\\dots,n-2$. Let $B \\in \\mathbb{R}^{n \\times m}$ be the node-edge incidence operator defined by $B_{i,i} = 1$, $B_{i+1,i} = -1$, and zeros elsewhere, so that for an edge-flow vector $f \\in \\mathbb{R}^{m}$, the node-wise net outflow is $B f \\in \\mathbb{R}^{n}$. Let $W \\in \\mathbb{R}^{m \\times m}$ be a diagonal positive matrix with entries $w_e  0$ representing edge-wise transport costs. Given two nonnegative mass distributions $s,t \\in \\mathbb{R}^{n}$ such that $\\sum_{i=0}^{n-1} s_i = \\sum_{i=0}^{n-1} t_i$ and defining the signed demand $d = s - t \\in \\mathbb{R}^{n}$, consider the regularized quadratic transport surrogate\n$$\nJ_{\\lambda}(f) \\;=\\; \\tfrac{1}{2} f^{\\top} W f \\;+\\; \\tfrac{\\lambda}{2} \\,\\| B f - d \\|_2^2,\n$$\nwhere $\\lambda \\ge 0$ is a regularization strength.\n\nTask A (derivation from first principles): Starting from the definition of the Euclidean gradient and the basic rules of matrix calculus (linearity, symmetry of $W$, and the chain rule for the squared norm), derive the necessary condition for minimizers of $J_{\\lambda}$ and show that it leads to a linear system\n$$\nA_{\\lambda} f \\;=\\; b_{\\lambda},\n$$\nwith a specific symmetric matrix $A_{\\lambda}$ and vector $b_{\\lambda}$ in terms of $W$, $B$, and $d$. Prove that for every $\\lambda \\ge 0$ and every diagonal $W$ with strictly positive entries, the matrix $A_{\\lambda}$ is Symmetric Positive Definite (SPD), and explicitly identify the inner product induced by $A_{\\lambda}$, $\\langle u,v\\rangle_{A_{\\lambda}} = u^{\\top} A_{\\lambda} v$. Interpret this inner product as an energy coupling for transport modes.\n\nTask B (Conjugate Gradient from the variational principle): Using only the facts that (i) $A_{\\lambda}$ is SPD, (ii) minimizing a strictly convex quadratic functional over affine Krylov subspaces yields unique minimizers, and (iii) orthogonality of residuals and $A_{\\lambda}$-orthogonality (conjugacy) of search directions are consequences of successive orthogonal projections, derive the Conjugate Gradient (CG) iteration for minimizing $J_{\\lambda}(f)$ over growing Krylov subspaces $\\mathcal{K}_k(A_{\\lambda},r_0)$, with $r_0 = b_{\\lambda} - A_{\\lambda} f_0$ and $f_0 = 0$. Show that in exact arithmetic:\n- residuals $\\{ r_k \\}$ are mutually orthogonal in the Euclidean inner product,\n- search directions $\\{ p_k \\}$ are mutually $A_{\\lambda}$-orthogonal (conjugate).\n\nExplain how $A_{\\lambda}$-orthogonality can be interpreted as orthogonality of transport modes in the $A_{\\lambda}$-energy, and argue how the regularization strength $\\lambda$ affects the spectral condition number of $A_{\\lambda}$ and, consequently, the numerical preservation of conjugacy in floating-point arithmetic.\n\nTask C (computational investigation): Implement a program that:\n- constructs $B$ for a path graph with $n=64$ nodes,\n- constructs $W = \\mathrm{diag}(w)$ with entries $w_e = 1 + \\alpha \\left(\\tfrac{e}{m-1}\\right)^2$ for edges $e=0,\\dots,m-1$ and $\\alpha=50$,\n- constructs $s$ and $t$ by sampling discrete Gaussian profiles with means $\\mu_s = 0.25\\,(n-1)$ and $\\mu_t = 0.75\\,(n-1)$ and common standard deviation $\\sigma = 0.12\\,(n-1)$:\n$$\n\\tilde{s}_i = \\exp\\!\\Big(-\\tfrac{(i-\\mu_s)^2}{2 \\sigma^2}\\Big),\\quad \\tilde{t}_i = \\exp\\!\\Big(-\\tfrac{(i-\\mu_t)^2}{2 \\sigma^2}\\Big),\\quad\ns = \\tfrac{\\tilde{s}}{\\sum_{i=0}^{n-1} \\tilde{s}_i},\\quad t = \\tfrac{\\tilde{t}}{\\sum_{i=0}^{n-1} \\tilde{t}_i},\n$$\n- sets $d = s - t$,\n- for each regularization strength $\\lambda \\in \\{0, 10^{-4}, 1, 10^{4}\\}$, forms $A_{\\lambda}$ and $b_{\\lambda}$ from Task A, and applies CG with initial guess $f_0 = 0$, tolerance $\\|r_k\\|_2 / \\|b_{\\lambda}\\|_2 \\le 10^{-10}$, and maximum iterations $m$ to obtain a sequence of search directions $\\{p_k\\}$ and residuals $\\{r_k\\}$.\n\nFor each $\\lambda$, compute the following quantitative diagnostics:\n- the $2$-norm condition number $\\kappa_2(A_{\\lambda})$,\n- the $A_{\\lambda}$-conjugacy loss for the computed directions\n$$\nL_A \\;=\\; \\frac{\\left\\| P^{\\top} A_{\\lambda} P - \\mathrm{diag}\\big(\\mathrm{diag}(P^{\\top} A_{\\lambda} P)\\big) \\right\\|_F}{\\left\\| \\mathrm{diag}\\big(\\mathrm{diag}(P^{\\top} A_{\\lambda} P)\\big) \\right\\|_F},\n$$\nwhere $P = [p_0,\\dots,p_{k-1}] \\in \\mathbb{R}^{m \\times k}$ is the matrix of search directions,\n- the residual orthogonality loss\n$$\nL_r \\;=\\; \\frac{\\left\\| R^{\\top} R - \\mathrm{diag}\\big(\\mathrm{diag}(R^{\\top} R)\\big) \\right\\|_F}{\\left\\| \\mathrm{diag}\\big(\\mathrm{diag}(R^{\\top} R)\\big) \\right\\|_F},\n$$\nwhere $R = [r_0,\\dots,r_k] \\in \\mathbb{R}^{m \\times (k+1)}$ collects residuals,\n- the number of iterations $k$ actually performed when the stopping criterion is met. If $b_{\\lambda} = 0$, define $k=0$ and set $L_A = 0$ and $L_r = 0$ by convention.\n\nTest suite and required output:\n- Use the fixed parameters $n=64$, $\\alpha=50$, $\\mu_s = 0.25\\,(n-1)$, $\\mu_t = 0.75\\,(n-1)$, $\\sigma = 0.12\\,(n-1)$, tolerance $10^{-10}$, and maximum iterations $m=n-1$.\n- The set of $\\lambda$ values to test is $\\{0, 10^{-4}, 1, 10^{4}\\}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the entry for each $\\lambda$ is a list $[\\kappa_2(A_{\\lambda}), L_A, L_r, k]$. The first three quantities must be decimal numbers rounded to exactly six digits after the decimal point, and $k$ must be an integer.\n\nFor example, the final line should have the structure\n$[[\\kappa_2^{(1)},L_A^{(1)},L_r^{(1)},k^{(1)}],[\\kappa_2^{(2)},L_A^{(2)},L_r^{(2)},k^{(2)}],[\\kappa_2^{(3)},L_A^{(3)},L_r^{(3)},k^{(3)}],[\\kappa_2^{(4)},L_A^{(4)},L_r^{(4)},k^{(4)}]]$.",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. All parameters and definitions are clear and consistent. The tasks involve standard derivations and computations in numerical linear algebra and optimization, forming a cohesive and substantive problem.\n\n### Task A: Derivation and Analysis of the Linear System\n\nThe objective function to be minimized is the regularized quadratic transport surrogate $J_{\\lambda}(f)$:\n$$\nJ_{\\lambda}(f) = \\tfrac{1}{2} f^{\\top} W f + \\tfrac{\\lambda}{2} \\| B f - d \\|_2^2\n$$\nwhere $f \\in \\mathbb{R}^{m}$ is the vector of edge flows, $W \\in \\mathbb{R}^{m \\times m}$ is a symmetric positive definite (SPD) matrix of transport costs, $B \\in \\mathbb{R}^{n \\times m}$ is the node-edge incidence operator, $d \\in \\mathbb{R}^{n}$ is the signed demand, and $\\lambda \\ge 0$ is a regularization parameter.\n\nTo find the minimizer of $J_{\\lambda}(f)$, we first find its gradient with respect to $f$ and set it to zero. First, we expand the squared Euclidean norm term:\n$$\n\\| B f - d \\|_2^2 = (Bf - d)^{\\top}(Bf - d) = (f^{\\top}B^{\\top} - d^{\\top})(Bf - d) = f^{\\top}B^{\\top}Bf - f^{\\top}B^{\\top}d - d^{\\top}Bf + d^{\\top}d\n$$\nSince $f^{\\top}B^{\\top}d$ is a scalar, it is equal to its transpose, $(f^{\\top}B^{\\top}d)^{\\top} = d^{\\top}Bf$. Thus, we can write:\n$$\n\\| B f - d \\|_2^2 = f^{\\top}B^{\\top}Bf - 2 f^{\\top}B^{\\top}d + d^{\\top}d\n$$\nSubstituting this back into the expression for $J_{\\lambda}(f)$:\n$$\nJ_{\\lambda}(f) = \\tfrac{1}{2} f^{\\top} W f + \\tfrac{\\lambda}{2} (f^{\\top}B^{\\top}Bf - 2 f^{\\top}B^{\\top}d + d^{\\top}d)\n$$\nThe gradient $\\nabla_f J_{\\lambda}(f)$ is found by applying standard matrix calculus rules:\n1.  $\\nabla_f (\\tfrac{1}{2} f^{\\top} W f) = Wf$, since $W$ is symmetric.\n2.  $\\nabla_f (\\tfrac{\\lambda}{2} f^{\\top}(B^{\\top}B)f) = \\lambda (B^{\\top}B)f$, since $B^{\\top}B$ is symmetric.\n3.  $\\nabla_f (-\\lambda f^{\\top}B^{\\top}d) = -\\lambda B^{\\top}d$.\n4.  $\\nabla_f (\\tfrac{\\lambda}{2}d^{\\top}d) = 0$, as this term is constant with respect to $f$.\n\nCombining these terms, we get the gradient:\n$$\n\\nabla_f J_{\\lambda}(f) = Wf + \\lambda B^{\\top}Bf - \\lambda B^{\\top}d\n$$\nThe necessary condition for a minimum is that the gradient is zero:\n$$\n\\nabla_f J_{\\lambda}(f) = (W + \\lambda B^{\\top}B)f - \\lambda B^{\\top}d = 0\n$$\nThis can be rearranged into the linear system $A_{\\lambda} f = b_{\\lambda}$, where:\n$$\nA_{\\lambda} = W + \\lambda B^{\\top}B \\quad \\text{and} \\quad b_{\\lambda} = \\lambda B^{\\top}d\n$$\n\nNext, we prove that $A_{\\lambda}$ is Symmetric Positive Definite (SPD) for all $\\lambda \\ge 0$ and any diagonal matrix $W$ with strictly positive entries $w_e  0$.\n\n**Symmetry**: We check if $A_{\\lambda}^{\\top} = A_{\\lambda}$.\n$$\nA_{\\lambda}^{\\top} = (W + \\lambda B^{\\top}B)^{\\top} = W^{\\top} + \\lambda (B^{\\top}B)^{\\top} = W + \\lambda B^{\\top}(B^{\\top})^{\\top} = W + \\lambda B^{\\top}B = A_{\\lambda}\n$$\nThe symmetry of $W$ ($W^{\\top}=W$) is given, and $B^{\\top}B$ is always symmetric. Thus, $A_{\\lambda}$ is symmetric.\n\n**Positive Definiteness**: We must show that for any non-zero vector $v \\in \\mathbb{R}^m$, the quadratic form $v^{\\top}A_{\\lambda}v$ is strictly positive.\n$$\nv^{\\top}A_{\\lambda}v = v^{\\top}(W + \\lambda B^{\\top}B)v = v^{\\top}Wv + \\lambda v^{\\top}B^{\\top}Bv\n$$\nLet's analyze the two terms:\n1.  The first term is $v^{\\top}Wv = \\sum_{e=0}^{m-1} w_e v_e^2$. Since $W$ is a diagonal matrix with all diagonal entries $w_e  0$, this sum is strictly positive if $v \\neq 0$. Thus, $W$ is positive definite.\n2.  The second term is $\\lambda v^{\\top}B^{\\top}Bv = \\lambda (Bv)^{\\top}(Bv) = \\lambda \\|Bv\\|_2^2$. Since $\\lambda \\ge 0$ and the norm is always non-negative, this term is always non-negative.\n\nCombining these observations:\n-   If $\\lambda  0$, then $v^{\\top}A_{\\lambda}v = v^{\\top}Wv + \\lambda \\|Bv\\|_2^2$. Since $v \\neq 0$, $v^{\\top}Wv  0$. The second term is $\\ge 0$. Therefore, their sum is strictly positive, $v^{\\top}A_{\\lambda}v  0$.\n-   If $\\lambda = 0$, then $A_0 = W$. As established, $W$ is positive definite.\n\nIn both cases, for any $\\lambda \\ge 0$, the matrix $A_{\\lambda}$ is positive definite. Since it is also symmetric, $A_{\\lambda}$ is SPD.\n\nThe inner product induced by the SPD matrix $A_{\\lambda}$ is, by definition, $\\langle u,v\\rangle_{A_{\\lambda}} = u^{\\top} A_{\\lambda} v$. Expanding this:\n$$\n\\langle u,v\\rangle_{A_{\\lambda}} = u^{\\top} (W + \\lambda B^{\\top}B) v = u^{\\top}Wv + \\lambda (Bu)^{\\top}(Bv)\n$$\nThis inner product can be interpreted as a combined \"energy\" functional. The term $u^{\\top}Wv$ represents the energetic coupling of two flow-fields $u$ and $v$ through the transport cost $W$. The term $\\lambda (Bu)^{\\top}(Bv)$ represents the energetic coupling of the nodal imbalances ($Bu$ and $Bv$) generated by these flows. Two flow fields are $A_{\\lambda}$-orthogonal if their combined energy coupling is zero.\n\n### Task B: Conjugate Gradient Derivation\n\nThe Conjugate Gradient (CG) method is an iterative algorithm for solving linear systems $Af=b$ where $A$ is SPD. It is derived from the equivalent optimization problem of minimizing the quadratic functional $\\phi(f) = \\frac{1}{2}f^{\\top}Af - b^{\\top}f$. Let's use $A$ for $A_\\lambda$ and $b$ for $b_\\lambda$ for simplicity.\n\nThe core idea of CG is to find the minimizer of $\\phi(f)$ by sequentially searching along a set of $A$-orthogonal (or conjugate) directions. At step $k$, the solution $f_k$ is the unique minimizer of $\\phi(f)$ over the affine Krylov subspace $f_0 + \\mathcal{K}_k(A,r_0)$, where $f_0$ is an initial guess (here, $f_0=0$), $r_0=b-Af_0$ is the initial residual, and $\\mathcal{K}_k(A,r_0) = \\mathrm{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$.\n\n**Residual Orthogonality**: The optimality condition for $f_k$ is that the gradient $\\nabla \\phi(f_k) = Af_k - b = -r_k$ must be orthogonal to the search subspace $\\mathcal{K}_k(A,r_0)$. This is the Galerkin condition:\n$$\n\\langle r_k, v \\rangle = r_k^{\\top} v = 0 \\quad \\forall v \\in \\mathcal{K}_k(A,r_0)\n$$\nThe residuals $\\{r_j\\}_{j=0}^{k-1}$ are themselves members of the Krylov subspaces. Specifically, $r_j = b - Af_j$. Since $f_j \\in \\mathcal{K}_j(A,r_0)$, it follows that $Af_j \\in A\\mathcal{K}_j(A,r_0) \\subset \\mathcal{K}_{j+1}(A,r_0)$. Thus, $r_j \\in \\mathcal{K}_{j+1}(A,r_0)$.\nFor any $j  k$, $r_j \\in \\mathcal{K}_{j+1}(A,r_0) \\subset \\mathcal{K}_k(A,r_0)$. By the Galerkin condition, $r_k$ is orthogonal to the entire subspace $\\mathcal{K}_k(A,r_0)$, which means it is orthogonal to every vector within it. Therefore,\n$$\nr_k^{\\top} r_j = 0 \\quad \\text{for all } j  k\n$$\nThis establishes the mutual orthogonality of the residuals in the Euclidean inner product.\n\n**A-Orthogonality of Search Directions**: The CG algorithm constructs a set of search directions $\\{p_k\\}$ that are $A$-orthogonal, i.e., $\\langle p_k, p_j \\rangle_A = p_k^{\\top}Ap_j = 0$ for $k \\ne j$. These directions form an $A$-orthogonal basis for the Krylov subspaces. The update rule for the search direction is a Gram-Schmidt-like process:\n$$\np_k = r_k + \\beta_{k-1} p_{k-1}\n$$\nwith $p_0=r_0$. The residuals $\\{r_k\\}$ are mutually orthogonal. The key relations from the algorithm are $r_{k+1} = r_k - \\alpha_k A p_k$ and the fact that $\\mathrm{span}\\{p_0, \\dots, p_k\\} = \\mathrm{span}\\{r_0, \\dots, r_k\\} = \\mathcal{K}_{k+1}(A,r_0)$.\nTo prove $A$-orthogonality, $p_k^{\\top}Ap_j=0$ for $jk$, we proceed by induction. The base case is vacuous. Assume $\\{p_0, \\dots, p_{k-1}\\}$ are mutually $A$-orthogonal. We need to show $p_k^{\\top}Ap_j=0$ for $j  k$.\n$$\np_k^{\\top}Ap_j = (r_k + \\beta_{k-1}p_{k-1})^{\\top}Ap_j = r_k^{\\top}Ap_j + \\beta_{k-1}p_{k-1}^{\\top}Ap_j\n$$\nFrom the residual update rule, $Ap_j = \\frac{1}{\\alpha_j}(r_j-r_{j+1})$. Substituting this:\n$$\nr_k^{\\top}Ap_j = \\frac{1}{\\alpha_j} r_k^{\\top}(r_j - r_{j+1})\n$$\nFor $j  k-1$, we have $j+1  k$. Due to residual orthogonality, $r_k^{\\top}r_j = 0$ and $r_k^{\\top}r_{j+1} = 0$. Thus, $r_k^{\\top}Ap_j=0$. Also by the induction hypothesis, $p_{k-1}^{\\top}Ap_j=0$. This makes $p_k^{\\top}Ap_j=0$ for $j  k-1$.\nFor the case $j=k-1$, the coefficient $\\beta_{k-1}$ is specifically chosen to enforce $A$-orthogonality:\n$$\n\\beta_{k-1} = -\\frac{r_k^{\\top}Ap_{k-1}}{p_{k-1}^{\\top}Ap_{k-1}}\n$$\nWith this choice, $p_k^{\\top}Ap_{k-1} = r_k^{\\top}Ap_{k-1} + \\left(-\\frac{r_k^{\\top}Ap_{k-1}}{p_{k-1}^{\\top}Ap_{k-1}}\\right) p_{k-1}^{\\top}Ap_{k-1} = 0$. Thus, by construction and induction, the search directions $\\{p_k\\}$ are mutually $A$-orthogonal.\n\n**Interpretation and Influence of $\\lambda$**: The $A_\\lambda$-orthogonality of search directions, $\\langle p_i, p_j \\rangle_{A_\\lambda} = 0$, means that each update step $f_{k+1} = f_k + \\alpha_k p_k$ adds a correction $p_k$ that is \"non-interfering\" with previous corrections in the sense of the $A_\\lambda$-energy. The full solution is built as a sum of these energetically independent flow modes.\n\nThe regularization strength $\\lambda$ significantly affects the spectral properties of $A_\\lambda = W + \\lambda B^{\\top}B$. The condition number $\\kappa_2(A_\\lambda) = \\frac{\\sigma_{\\max}(A_\\lambda)}{\\sigma_{\\min}(A_\\lambda)}$ (where $\\sigma$ are the singular values, equal to eigenvalues for SPD matrices) governs the convergence rate of CG and its numerical stability.\n- For $\\lambda=0$, $A_0=W$, and $\\kappa_2(A_0) = \\kappa_2(W)$.\n- For $\\lambda \\to \\infty$, $A_\\lambda \\approx \\lambda B^{\\top}B$, and $\\kappa_2(A_\\lambda) \\to \\kappa_2(B^{\\top}B)$. The matrix $B^\\top B$ is a discrete 1D Laplacian, which is known to have a condition number that grows with an increase in matrix size.\n- For intermediate $\\lambda$, the eigenvalues of $W$ and $\\lambda B^{\\top}B$ are mixed. A large disparity between the scales of the eigenvalues of $W$ and $\\lambda B^{\\top}B$ can lead to a very large condition number, degrading CG performance.\nIn floating-point arithmetic, a high condition number amplifies rounding errors. This leads to a loss of the theoretical orthogonality properties. The computed residuals are no longer mutually orthogonal, and search directions lose their $A_\\lambda$-conjugacy. This slows convergence and may lead to premature termination or failure to reach the desired tolerance. Larger $\\lambda$ values, by increasing the spread of eigenvalues, are expected to increase $\\kappa_2(A_\\lambda)$ and thereby increase the measured conjugacy loss $L_A$ and orthogonality loss $L_r$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the multi-part problem:\n    - Constructs the matrices for the 1D transport problem.\n    - Implements the Conjugate Gradient algorithm.\n    - Computes and reports diagnostics for different regularization strengths.\n    \"\"\"\n    # Task C: Setup\n    n = 64\n    m = n - 1\n    alpha = 50.0\n    mu_s = 0.25 * (n - 1)\n    mu_t = 0.75 * (n - 1)\n    sigma_val = 0.12 * (n - 1)\n    cg_tol = 1e-10\n    max_iter = m\n\n    lambdas = [0.0, 1e-4, 1.0, 1e4]\n    \n    # Construct B matrix\n    B = np.zeros((n, m))\n    for i in range(m):\n        B[i, i] = 1.0\n        B[i + 1, i] = -1.0\n\n    # Construct W matrix\n    e = np.arange(m)\n    w_diag = 1.0 + alpha * (e / (m - 1))**2\n    W = np.diag(w_diag)\n\n    # Construct s, t, and d vectors\n    i_nodes = np.arange(n)\n    s_tilde = np.exp(-((i_nodes - mu_s)**2) / (2 * sigma_val**2))\n    s = s_tilde / np.sum(s_tilde)\n    t_tilde = np.exp(-((i_nodes - mu_t)**2) / (2 * sigma_val**2))\n    t = t_tilde / np.sum(t_tilde)\n    d = s - t\n    \n    BT = B.T\n    BTB = BT @ B\n    BTd = BT @ d\n    \n    results = []\n\n    for lam in lambdas:\n        # Form A_lambda and b_lambda\n        A_lambda = W + lam * BTB\n        b_lambda = lam * BTd\n\n        # Calculate condition number\n        cond_A = np.linalg.cond(A_lambda)\n\n        # Handle the lambda=0 case as per problem description\n        if lam == 0.0:\n            # b_lambda is zero, so f=0 is the solution. k=0 iterations.\n            # Losses are 0 by convention.\n            k_iter = 0\n            L_A = 0.0\n            L_r = 0.0\n            results.append([cond_A, L_A, L_r, k_iter])\n            continue\n            \n        # Run custom Conjugate Gradient to collect p_k and r_k\n        f = np.zeros(m)\n        r = b_lambda.copy()\n        p = r.copy()\n        \n        P_list = []\n        R_list = [r.copy()] # r_0\n\n        rs_old = np.dot(r, r)\n        norm_b = np.linalg.norm(b_lambda)\n\n        if norm_b == 0:\n            k_iter = 0\n            L_A = 0.0\n            L_r = 0.0\n            results.append([cond_A, L_A, L_r, k_iter])\n            continue\n\n        k_iter = 0\n        for i in range(max_iter):\n            # Check stopping criterion for residual r_i\n            if np.sqrt(rs_old) / norm_b  cg_tol:\n                break\n            \n            k_iter += 1 # This is the i-th iteration\n            \n            P_list.append(p.copy())\n            \n            Ap = A_lambda @ p\n            alpha_k = rs_old / np.dot(p, Ap)\n            \n            f += alpha_k * p\n            r -= alpha_k * Ap\n            \n            R_list.append(r.copy())\n            \n            rs_new = np.dot(r, r)\n            beta_k = rs_new / rs_old\n            p = r + beta_k * p\n            rs_old = rs_new\n\n        # Compute diagnostic losses\n        if k_iter > 0:\n            P_mat = np.array(P_list).T\n            R_mat = np.array(R_list).T\n\n            # Conjugacy loss L_A\n            PAPB = P_mat.T @ A_lambda @ P_mat\n            diag_PAPB = np.diag(np.diag(PAPB))\n            num_LA = np.linalg.norm(PAPB - diag_PAPB, 'fro')\n            den_LA = np.linalg.norm(diag_PAPB, 'fro')\n            L_A = num_LA / den_LA if den_LA > 0 else 0.0\n\n            # Orthogonality loss L_r\n            RTR = R_mat.T @ R_mat\n            diag_RTR = np.diag(np.diag(RTR))\n            num_LR = np.linalg.norm(RTR - diag_RTR, 'fro')\n            den_LR = np.linalg.norm(diag_RTR, 'fro')\n            L_r = num_LR / den_LR if den_LR > 0 else 0.0\n        else: # k_iter == 0\n            L_A = 0.0\n            L_r = 0.0\n\n        results.append([cond_A, L_A, L_r, k_iter])\n\n    # Format the final output string\n    output_parts = []\n    for res_vec in results:\n        kappa, LA, Lr, k = res_vec\n        part = f\"[{kappa:.6f},{LA:.6f},{Lr:.6f},{k}]\"\n        output_parts.append(part)\n    final_string = f\"[{','.join(output_parts)}]\"\n    \n    print(final_string)\n\nsolve()\n```"
        }
    ]
}