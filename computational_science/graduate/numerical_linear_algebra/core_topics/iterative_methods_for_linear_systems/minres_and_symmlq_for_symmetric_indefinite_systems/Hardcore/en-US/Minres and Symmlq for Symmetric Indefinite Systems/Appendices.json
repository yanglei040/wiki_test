{
    "hands_on_practices": [
        {
            "introduction": "Both MINRES and SYMMLQ are powered by the same engine: the symmetric Lanczos process. This elegant algorithm iteratively builds an orthonormal basis for a Krylov subspace, simultaneously creating a compact tridiagonal representation $T_k$ of the original matrix $A$. Mastering this process is the first step to understanding how these solvers work, and this exercise  provides direct practice in computing the core Lanczos coefficients from first principles.",
            "id": "3560312",
            "problem": "Consider the linear system $A x = b$ with the symmetric, indefinite matrix\n$$A=\\begin{bmatrix}2&1&0\\\\1&-1&1\\\\0&1&0\\end{bmatrix}$$\nand the right-hand side\n$$b=\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}.$$\nBoth the Minimal Residual Method (MINRES) and the Symmetric LQ method (SYMMLQ) for symmetric indefinite systems are built upon the symmetric Lanczos process that generates an orthonormal basis $\\{v_j\\}$ for the Krylov subspaces $\\mathcal{K}_k(A,r_0)$ with $r_0=b-Ax_0$ and tridiagonalizes $A$ via three-term recurrences. Starting from the zero initial guess $x_0=\\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$, no preconditioning, the Euclidean norm, and the conventional Lanczos initialization $v_1=r_0/\\|r_0\\|$, perform the first two Lanczos steps and compute the first three Lanczos coefficients $\\alpha_1$, $\\beta_1$, and $\\alpha_2$ explicitly. Provide exact values (no rounding). Your final answer must list $\\alpha_1$, $\\beta_1$, and $\\alpha_2$ in that order.",
            "solution": "The problem is valid. It is a well-posed problem in numerical linear algebra, asking for the calculation of the first three coefficients generated by the symmetric Lanczos process for a given symmetric indefinite matrix and right-hand side. All necessary information is provided, and the premises are factually correct.\n\nThe symmetric Lanczos process generates a sequence of orthonormal vectors $\\{v_j\\}$ and a symmetric tridiagonal matrix $T_k$ whose entries are the Lanczos coefficients $\\alpha_j$ and $\\beta_j$. The relationship between the matrix $A$ and these quantities is given by the matrix equation $A V_k = V_k T_k + \\beta_k v_{k+1} e_k^T$, where $V_k = [v_1, v_2, \\dots, v_k]$ and $T_k$ is a $k \\times k$ symmetric tridiagonal matrix.\n$$T_k = \\begin{pmatrix} \\alpha_1 & \\beta_1 & & & \\\\ \\beta_1 & \\alpha_2 & \\beta_2 & & \\\\ & \\beta_2 & \\ddots & \\ddots & \\\\ & & \\ddots & \\alpha_{k-1} & \\beta_{k-1} \\\\ & & & \\beta_{k-1} & \\alpha_k \\end{pmatrix}$$\nThe algorithm proceeds as follows:\n1.  Initialize: Start with an initial guess $x_0$. Compute the initial residual $r_0 = b - A x_0$. Normalize it to get the first Lanczos vector: $v_1 = r_0 / \\|r_0\\|_2$.\n2.  Iterate: For $j = 1, 2, \\dots, k$:\n    a. Compute the matrix-vector product $w_j = A v_j$.\n    b. The diagonal coefficient is $\\alpha_j = v_j^T w_j$.\n    c. Compute the next un-normalized vector. For $j=1$, this is $\\hat{v}_{j+1} = w_j - \\alpha_j v_j$. For $j>1$, it is $\\hat{v}_{j+1} = w_j - \\alpha_j v_j - \\beta_j v_{j-1}$.\n    d. The off-diagonal coefficient is the norm of this vector: $\\beta_j = \\|\\hat{v}_{j+1}\\|_2$.\n    e. If $\\beta_j = 0$, the algorithm terminates. Otherwise, normalize to get the next Lanczos vector: $v_{j+1} = \\hat{v}_{j+1} / \\beta_j$.\n\nWe are asked to perform the first two steps to find $\\alpha_1$, $\\beta_1$, and $\\alpha_2$.\n\n**Initialization**\n\nThe given matrix is $A=\\begin{bmatrix}2&1&0\\\\1&-1&1\\\\0&1&0\\end{bmatrix}$ and the right-hand side is $b=\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}$.\nThe initial guess is the zero vector, $x_0=\\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$.\n\nThe initial residual $r_0$ is:\n$$r_0 = b - A x_0 = b - 0 = \\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}$$\nThe Euclidean norm of $r_0$ is:\n$$\\|r_0\\|_2 = \\sqrt{1^2 + 0^2 + 1^2} = \\sqrt{2}$$\nThe first Lanczos vector $v_1$ is:\n$$v_1 = \\frac{r_0}{\\|r_0\\|_2} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}$$\n\n**First Lanczos Step (to find $\\alpha_1$ and $\\beta_1$)**\n\nFirst, we compute the matrix-vector product $w_1 = A v_1$:\n$$w_1 = A v_1 = \\begin{bmatrix}2&1&0\\\\1&-1&1\\\\0&1&0\\end{bmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}2(1)+1(0)+0(1)\\\\1(1)-1(0)+1(1)\\\\0(1)+1(0)+0(1)\\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}2\\\\2\\\\0\\end{bmatrix} = \\sqrt{2} \\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix}$$\nNext, we compute the coefficient $\\alpha_1$:\n$$\\alpha_1 = v_1^T w_1 = \\left( \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1&0&1\\end{bmatrix} \\right) \\left( \\sqrt{2} \\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix} \\right) = (1)(1) + (0)(1) + (1)(0) = 1$$\nNow, we compute the un-normalized vector for the next step, $\\hat{v}_2$:\n$$\\hat{v}_2 = w_1 - \\alpha_1 v_1 = \\sqrt{2} \\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix} - (1) \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\left( 2 \\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix} - \\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}2-1\\\\2-0\\\\0-1\\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1\\\\2\\\\-1\\end{bmatrix}$$\nThe coefficient $\\beta_1$ is the norm of $\\hat{v}_2$:\n$$\\beta_1 = \\|\\hat{v}_2\\|_2 = \\left\\| \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1\\\\2\\\\-1\\end{bmatrix} \\right\\|_2 = \\frac{1}{\\sqrt{2}} \\sqrt{1^2 + 2^2 + (-1)^2} = \\frac{1}{\\sqrt{2}} \\sqrt{1+4+1} = \\frac{\\sqrt{6}}{\\sqrt{2}} = \\sqrt{3}$$\nWe find the second Lanczos vector $v_2$ by normalizing $\\hat{v}_2$:\n$$v_2 = \\frac{\\hat{v}_2}{\\beta_1} = \\frac{1}{\\sqrt{3}} \\left( \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1\\\\2\\\\-1\\end{bmatrix} \\right) = \\frac{1}{\\sqrt{6}} \\begin{bmatrix}1\\\\2\\\\-1\\end{bmatrix}$$\n\n**Second Lanczos Step (to find $\\alpha_2$)**\n\nWe proceed to the second step to find $\\alpha_2$. First, we compute $w_2 = A v_2$:\n$$w_2 = A v_2 = \\begin{bmatrix}2&1&0\\\\1&-1&1\\\\0&1&0\\end{bmatrix} \\left( \\frac{1}{\\sqrt{6}} \\begin{bmatrix}1\\\\2\\\\-1\\end{bmatrix} \\right) = \\frac{1}{\\sqrt{6}} \\begin{bmatrix}2(1)+1(2)+0(-1)\\\\1(1)-1(2)+1(-1)\\\\0(1)+1(2)+0(-1)\\end{bmatrix} = \\frac{1}{\\sqrt{6}} \\begin{bmatrix}4\\\\-2\\\\2\\end{bmatrix}$$\nThe coefficient $\\alpha_2$ is given by the inner product $\\alpha_2 = v_2^T w_2$:\n$$\\alpha_2 = v_2^T w_2 = \\left( \\frac{1}{\\sqrt{6}} \\begin{bmatrix}1&2&-1\\end{bmatrix} \\right) \\left( \\frac{1}{\\sqrt{6}} \\begin{bmatrix}4\\\\-2\\\\2\\end{bmatrix} \\right) = \\frac{1}{6} \\left( (1)(4) + (2)(-2) + (-1)(2) \\right) = \\frac{1}{6} (4 - 4 - 2) = -\\frac{2}{6} = -\\frac{1}{3}$$\nThe problem only asks for $\\alpha_1$, $\\beta_1$, and $\\alpha_2$. We have now computed all required coefficients.\n\nSummary of results:\nThe first diagonal coefficient is $\\alpha_1 = 1$.\nThe first off-diagonal coefficient is $\\beta_1 = \\sqrt{3}$.\nThe second diagonal coefficient is $\\alpha_2 = -1/3$.\nThe problem requires exact values, which have been provided.\nThe resulting $2 \\times 2$ tridiagonal matrix is $T_2 = \\begin{pmatrix} 1 & \\sqrt{3} \\\\ \\sqrt{3} & -1/3 \\end{pmatrix}$.\n\nThe requested Lanczos coefficients are:\n$\\alpha_1 = 1$\n$\\beta_1 = \\sqrt{3}$\n$\\alpha_2 = -\\frac{1}{3}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & \\sqrt{3} & -\\frac{1}{3}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The 'minimal residual' property of MINRES is achieved by solving a small least-squares problem at each step, a task typically handled by applying a sequence of Givens rotations. This exercise  peels back the curtain on this procedure, guiding you through the step-by-step construction of the MINRES solution for a simple indefinite system. By explicitly calculating the Lanczos vectors and applying the Givens rotations, you will see exactly how the residual norm is minimized.",
            "id": "3560282",
            "problem": "Let $A \\in \\mathbb{R}^{2 \\times 2}$ be symmetric indefinite with $A=\\begin{bmatrix}0 & 1 \\\\ 1 & 0\\end{bmatrix}$, and let $b=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$. Consider the Minimum Residual method for symmetric systems (MINRES) applied to the linear system $A x = b$ starting from $x_0=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$ in exact arithmetic.\n\nStarting from the fundamental definitions below, perform exactly two MINRES iterations and determine the iterate $x_2$ and the residual $2$-norm $\\|r_2\\|_2$, where $r_k=b-Ax_k$.\n\nFundamental definitions to use:\n1. The initial residual is $r_0=b-Ax_0$, with $\\beta_1=\\|r_0\\|_2$ and $v_1=r_0/\\beta_1$.\n2. The symmetric Lanczos process constructs orthonormal vectors $v_j$ and scalars $\\alpha_j, \\beta_{j+1}$ via\n   $$\\beta_{j+1} v_{j+1} = A v_j - \\alpha_j v_j - \\beta_j v_{j-1}, \\quad \\alpha_j = v_j^{\\mathsf{T}} A v_j, \\quad v_0=0,$$\n   producing the tridiagonal matrix $T_k$ with diagonal entries $\\alpha_j$ and sub/superdiagonals $\\beta_{j+1}$.\n3. The MINRES iterate $x_k$ lies in the Krylov subspace $\\mathcal{K}_k(A,r_0)=\\operatorname{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$ and minimizes the residual norm. Writing $x_k = V_k y_k$ with $V_k=[v_1,\\dots,v_k]$, the residual-minimizing coefficients $y_k$ solve the least-squares problem\n   $$\\min_{y \\in \\mathbb{R}^k} \\left\\| \\beta_1 e_1 - \\bar{T}_k y \\right\\|_2,$$\n   where $\\bar{T}_k \\in \\mathbb{R}^{(k+1)\\times k}$ augments $T_k$ with the subdiagonal $\\beta_{k+1}$.\n4. A Givens rotation $G(c,s)$ acting on two components eliminates an entry via\n   $$\\begin{bmatrix} c & s \\\\ s & -c \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = \\begin{bmatrix} \\sqrt{a^2+b^2} \\\\ 0 \\end{bmatrix}, \\quad c=\\dfrac{a}{\\sqrt{a^2+b^2}}, \\quad s=\\dfrac{b}{\\sqrt{a^2+b^2}},$$\n   and is applied to rows of $\\bar{T}_k$ and the right-hand side vector to maintain an equivalent upper-triangular least-squares problem. The residual norm after $k$ iterations equals the absolute value of the last component of the transformed right-hand side.\n\nCarry out the derivation explicitly: construct the Lanczos scalars and vectors for $k=1,2$, form the least-squares problems for $k=1$ and $k=2$, construct and apply the necessary Givens rotations with exact cosines and sines, solve for the coefficients $y_k$, and recover $x_k=V_k y_k$ together with $\\|r_k\\|_2$ for $k=2$. Report the exact $x_2$ and $\\|r_2\\|_2$. No rounding is required. Your final answer must be the triple $(x_{2,1}, x_{2,2}, \\|r_2\\|_2)$ as a single row matrix.",
            "solution": "We begin from the initial data. The initial iterate is $x_0=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, so the initial residual is\n$$\nr_0=b-Ax_0=b=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}.\n$$\nIts norm is $\\beta_1=\\|r_0\\|_2=1$, hence\n$$\nv_1=\\frac{r_0}{\\beta_1}=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}.\n$$\n\nWe proceed with the symmetric Lanczos process. Initialize $v_0=0$ and $\\beta_1$ as above.\n\nFor $j=1$, compute\n$$\nw = A v_1 - \\beta_1 v_0 = \\begin{bmatrix}0 & 1 \\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} - 1 \\cdot \\begin{bmatrix}0 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}.\n$$\nThen\n$$\n\\alpha_1 = v_1^{\\mathsf{T}} A v_1 = \\begin{bmatrix}1 & 0\\end{bmatrix} \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} = 0,\n$$\nand the orthogonalized vector is $w - \\alpha_1 v_1 = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$. Its norm gives\n$$\n\\beta_2 = \\left\\| \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} \\right\\|_2 = 1, \\quad v_2 = \\frac{1}{\\beta_2} \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}.\n$$\n\nFor $j=2$, compute\n$$\nw = A v_2 - \\beta_2 v_1 = \\begin{bmatrix}0 & 1 \\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} - 1 \\cdot \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} - \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}.\n$$\nThus\n$$\n\\alpha_2 = v_2^{\\mathsf{T}} A v_2 = \\begin{bmatrix}0 & 1\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = 0, \\quad \\beta_3 = \\| w - \\alpha_2 v_2 \\|_2 = \\| 0 \\|_2 = 0.\n$$\n\nTherefore, the $2 \\times 2$ tridiagonal matrix is\n$$\nT_2 = \\begin{bmatrix} \\alpha_1 & \\beta_2 \\\\ \\beta_2 & \\alpha_2 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix},\n$$\nand the augmented matrix for the least-squares problem is\n$$\n\\bar{T}_2 = \\begin{bmatrix} \\alpha_1 & \\beta_2 \\\\ \\beta_2 & \\alpha_2 \\\\ 0 & \\beta_3 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \\end{bmatrix}.\n$$\nWe also have $V_2 = [v_1, v_2] = \\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix}$ and $\\beta_1 = 1$.\n\nThe MINRES iterate $x_k$ is given by $x_k = V_k y_k$, where $y_k$ minimizes the residual norm\n$$\n\\left\\| \\beta_1 e_1 - \\bar{T}_k y \\right\\|_2.\n$$\nWe proceed iteration by iteration, constructing and applying Givens rotations to triangularize the least-squares system.\n\nIteration $k=1$. The matrix $\\bar{T}_1$ is the first column of $\\bar{T}_2$,\n$$\n\\bar{T}_1 = \\begin{bmatrix} \\alpha_1 \\\\ \\beta_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix},\n$$\nand the least-squares problem is\n$$\n\\min_{y_1 \\in \\mathbb{R}} \\left\\| \\beta_1 e_1 - \\bar{T}_1 y_1 \\right\\|_2 = \\min_{y_1 \\in \\mathbb{R}} \\left\\| \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} - \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} y_1 \\right\\|_2 = \\min_{y_1 \\in \\mathbb{R}} \\sqrt{ (1 - 0 \\cdot y_1)^2 + (0 - y_1)^2 } = \\min_{y_1} \\sqrt{1 + y_1^2}.\n$$\nClearly, the minimizer is $y_1=0$, yielding $x_1 = V_1 y_1 = v_1 \\cdot 0 = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$ and $\\|r_1\\|_2 = 1$.\n\nEquivalently, via Givens rotations, we eliminate the subdiagonal entry in $\\bar{T}_1$ using $G_1$ acting on rows $1$ and $2$. With $a=\\alpha_1=0$ and $b=\\beta_2=1$, we have\n$$\n\\rho_1 = \\sqrt{a^2 + b^2} = 1, \\quad c_1 = \\frac{a}{\\rho_1} = 0, \\quad s_1 = \\frac{b}{\\rho_1} = 1,\n$$\nand\n$$\nG_1 = \\begin{bmatrix} c_1 & s_1 & 0 \\\\ s_1 & -c_1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}.\n$$\nApplying $G_1$ to $\\bar{T}_1$ yields $\\begin{bmatrix} \\rho_1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, and applying $G_1$ to the right-hand side vector $g=\\begin{bmatrix}\\beta_1 \\\\ 0 \\\\ 0\\end{bmatrix}=\\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix}$ gives the transformed vector\n$$\n\\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\phi_3 \\end{bmatrix} = G_1 g = \\begin{bmatrix} c_1 \\beta_1 \\\\ s_1 \\beta_1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n$$\nThe residual norm after one iteration is $|\\phi_2|=1$, confirming the least-squares result and $x_1=0$.\n\nIteration $k=2$. We now consider $\\bar{T}_2$ and apply $G_1$ to it:\n$$\nG_1 \\bar{T}_2 = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix}.\n$$\nThe first column is already upper-triangular with leading entry $\\rho_1=1$. For the second column, to eliminate the subdiagonal entry in row $3$, we construct $G_2$ acting on rows $2$ and $3$ using the pair $(a,b)=(\\alpha_2', \\beta_3')=(1,0)$. Thus\n$$\n\\rho_2 = \\sqrt{a^2 + b^2} = 1, \\quad c_2 = \\frac{a}{\\rho_2} = 1, \\quad s_2 = \\frac{b}{\\rho_2} = 0,\n$$\nand\n$$\nG_2 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & c_2 & s_2 \\\\ 0 & s_2 & -c_2 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & -1 \\end{bmatrix}.\n$$\nApplying $G_2$ leaves $G_1 \\bar{T}_2$ unchanged:\n$$\nG_2 (G_1 \\bar{T}_2) = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix}.\n$$\nApplying $G_2$ to the previously transformed right-hand side gives\n$$\nG_2 \\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\phi_3 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & -1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} \\phi_1' \\\\ \\phi_2' \\\\ \\phi_3' \\end{bmatrix}.\n$$\nThus the fully transformed least-squares system is\n$$\n\\min_{y \\in \\mathbb{R}^2} \\left\\| \\begin{bmatrix} \\phi_1' \\\\ \\phi_2' \\\\ \\phi_3' \\end{bmatrix} - \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} y \\right\\|_2 = \\min_{y \\in \\mathbb{R}^2} \\left\\| \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} y_1 \\\\ y_2 \\\\ 0 \\end{bmatrix} \\right\\|_2 = \\min_{y} \\sqrt{ (0 - y_1)^2 + (1 - y_2)^2 }.\n$$\nThe unique minimizer is $y_1=0$, $y_2=1$. Therefore,\n$$\nx_2 = V_2 y = [v_1, v_2] \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = v_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\n$$\nThe residual norm after two iterations equals $|\\phi_3'|=0$, i.e.,\n$$\n\\| r_2 \\|_2 = 0.\n$$\n\nWe conclude that the second MINRES iterate is exact for this $2 \\times 2$ problem, with $x_2=\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$ and $\\|r_2\\|_2=0$.",
            "answer": "$$\\boxed{\\begin{pmatrix}0 & 1 & 0\\end{pmatrix}}$$"
        },
        {
            "introduction": "While MINRES minimizes the residual norm, SYMMLQ enforces a Galerkin condition by solving the projected system $T_k y_k = \\|b\\|_2 e_1$. For indefinite systems, the matrix $T_k$ can be nearly singular, leading to solutions $y_k$ with enormous norms. This hands-on problem  guides you through a carefully constructed scenario to explore and quantify this very phenomenon, revealing why the MINRES solution iterate often remains stable while the SYMMLQ iterate can become dangerously large.",
            "id": "3560323",
            "problem": "Consider a symmetric indefinite linear system with a $2 \\times 2$ symmetric matrix $A$ acting on a real $2$-dimensional space spanned by an orthonormal basis $\\{u, q\\}$, where $u^{\\top} u = 1$, $q^{\\top} q = 1$, and $u^{\\top} q = 0$. Define the matrix $A$ by its action on the basis:\n$$\nA u = \\alpha u + \\gamma q, \\quad A q = \\gamma u - \\alpha q,\n$$\nwith parameters $\\alpha \\in \\mathbb{R}$ and $\\gamma \\in \\mathbb{R}$ such that $A$ is indefinite and invertible. Let the right-hand side be $b = \\sqrt{2}\\, u$, so that $\\|b\\|_{2} = \\sqrt{2}$.\n\nYou apply two Krylov subspace methods starting from $x_{0} = 0$ and the same initial residual $r_{0} = b$:\n\n- Minimum Residual (MINRES): a Krylov method for symmetric systems that, at iteration $k$, produces $x_{k} \\in \\mathcal{K}_{k}(A, b)$ minimizing the residual norm $\\|b - A x_{k}\\|_{2}$.\n- Symmetric LQ method (SYMMLQ): a Krylov method for symmetric systems that, at iteration $k$, uses the Lanczos process to form a $k \\times k$ tridiagonal projection and computes $x_{k} = V_{k} y_{k}$ as the minimal $2$-norm solution of the projected system, where $V_{k}$ contains the Lanczos basis vectors.\n\nAssume you perform exactly one iteration ($k = 1$) of each method, both starting from the same normalized Lanczos vector $v_{1} = b / \\|b\\|_{2} = u$. In this setting, construct a case where $\\gamma / \\alpha$ is sufficiently small that the SYMMLQ residual $\\|b - A x_{1}^{\\mathrm{SYMMLQ}}\\|_{2}$ is small while $\\|x_{1}^{\\mathrm{SYMMLQ}}\\|_{2}$ is large, and MINRES produces a more stable iterate $\\|x_{1}^{\\mathrm{MINRES}}\\|_{2}$.\n\nStarting only from the above definitions and the given matrix structure, derive closed-form expressions for $\\|x_{1}^{\\mathrm{MINRES}}\\|_{2}$ and $\\|x_{1}^{\\mathrm{SYMMLQ}}\\|_{2}$ in terms of $\\alpha$ and $\\gamma$. Then, quantify the difference in solution norms by computing the ratio\n$$\n\\rho \\;=\\; \\frac{\\|x_{1}^{\\mathrm{SYMMLQ}}\\|_{2}}{\\|x_{1}^{\\mathrm{MINRES}}\\|_{2}}.\n$$\nExpress your final answer as a single simplified analytic expression in terms of $\\alpha$ and $\\gamma$. No rounding is required.",
            "solution": "The problem requires the derivation of the ratio of solution norms, $\\rho = \\|x_{1}^{\\mathrm{SYMMLQ}}\\|_{2} / \\|x_{1}^{\\mathrm{MINRES}}\\|_{2}$, after one iteration of the Minimum Residual (MINRES) and Symmetric LQ (SYMMLQ) methods for a specific $2 \\times 2$ symmetric indefinite system.\n\nFirst, we establish the matrix representation of the linear operator $A$. The space is spanned by the orthonormal basis $\\{u, q\\}$, which we can represent as the standard basis vectors $u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $q = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. The action of $A$ on this basis is given as:\n$A u = \\alpha u + \\gamma q$\n$A q = \\gamma u - \\alpha q$\nIn this basis, the matrix $A$ is:\n$$\nA = \\begin{pmatrix} \\alpha & \\gamma \\\\ \\gamma & -\\alpha \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of $A$ satisfy the characteristic equation $\\det(A - \\lambda I) = 0$, which is $(\\alpha - \\lambda)(-\\alpha - \\lambda) - \\gamma^2 = 0$, leading to $\\lambda^2 - \\alpha^2 - \\gamma^2 = 0$. The eigenvalues are $\\lambda_{1,2} = \\pm \\sqrt{\\alpha^2 + \\gamma^2}$. Since $A$ is invertible, $\\alpha$ and $\\gamma$ cannot both be zero, so the eigenvalues are real, non-zero, and have opposite signs, confirming that $A$ is indefinite.\n\nBoth MINRES and SYMMLQ are Krylov subspace methods that rely on the Lanczos process. We start with $x_0 = 0$, so the initial residual is $r_0 = b - Ax_0 = b$.\nThe given right-hand side is $b = \\sqrt{2}\\,u$. Its norm is $\\|b\\|_{2} = \\sqrt{2}\\|u\\|_{2} = \\sqrt{2}$.\nThe first step of the Lanczos process is to normalize the initial residual to get the first Lanczos vector $v_1$:\n$$\nv_1 = \\frac{b}{\\|b\\|_{2}} = \\frac{\\sqrt{2}\\,u}{\\sqrt{2}} = u\n$$\nNext, we compute the coefficients for the one-step Lanczos iteration, which generates the $1 \\times 1$ tridiagonal matrix $T_1 = (\\alpha_1)$.\nThe diagonal element $\\alpha_1$ is given by:\n$$\n\\alpha_1 = v_1^{\\top} A v_1 = u^{\\top} A u = u^{\\top}(\\alpha u + \\gamma q) = \\alpha (u^{\\top}u) + \\gamma (u^{\\top}q) = \\alpha(1) + \\gamma(0) = \\alpha\n$$\nSo, $T_1 = (\\alpha)$.\nThe off-diagonal element $\\beta_2$ is found from the next vector in the process. The unnormalized next vector is $w' = A v_1 - \\alpha_1 v_1$:\n$$\nw' = A u - \\alpha u = (\\alpha u + \\gamma q) - \\alpha u = \\gamma q\n$$\nThe norm of this vector is $\\beta_2$:\n$$\n\\beta_2 = \\|w'\\|_{2} = \\|\\gamma q\\|_{2} = |\\gamma|\\|q\\|_{2} = |\\gamma|\n$$\nThe Lanczos process constructs the relationship $A V_k = V_k T_k + \\beta_{k+1} v_{k+1} e_k^{\\top}$. For $k=1$, we have $AV_1 = V_1T_1 + \\beta_2 v_2 e_1^\\top$, with $V_1 = [v_1] = [u]$.\n\nNow we derive the first iterate $x_1$ for each method. The Krylov subspace for the first iteration is $\\mathcal{K}_1(A, b) = \\mathrm{span}\\{b\\} = \\mathrm{span}\\{v_1\\}$. Hence, both iterates will be of the form $x_1 = c\\,v_1 = c\\,u$ for some scalar $c$.\n\n**MINRES Iterate**\nMINRES finds the iterate $x_1^{\\mathrm{MINRES}} \\in \\mathcal{K}_1(A, b)$ that minimizes the Euclidean norm of the residual, $\\|r_1\\|_{2} = \\|b - A x_1\\|_{2}$.\nLet $x_1 = c\\,u$. We minimize the function $f(c) = \\|b - A(c\\,u)\\|_{2}^2$:\n$$\nf(c) = \\|\\sqrt{2}\\,u - c A u\\|_{2}^{2} = \\|\\sqrt{2}\\,u - c(\\alpha u + \\gamma q)\\|_{2}^{2} = \\|(\\sqrt{2} - c\\alpha)u - c\\gamma q\\|_{2}^{2}\n$$\nSince $u$ and $q$ are orthonormal, this simplifies to:\n$$\nf(c) = (\\sqrt{2} - c\\alpha)^{2} + (-c\\gamma)^{2} = (\\sqrt{2} - c\\alpha)^{2} + c^2\\gamma^2\n$$\nTo find the minimum, we set the derivative with respect to $c$ to zero:\n$$\n\\frac{df}{dc} = 2(\\sqrt{2} - c\\alpha)(-\\alpha) + 2c\\gamma^2 = -2\\sqrt{2}\\alpha + 2c\\alpha^2 + 2c\\gamma^2 = 0\n$$\n$$\nc(\\alpha^2 + \\gamma^2) = \\sqrt{2}\\alpha \\implies c_{\\mathrm{MINRES}} = \\frac{\\sqrt{2}\\alpha}{\\alpha^2 + \\gamma^2}\n$$\nThe MINRES iterate is $x_1^{\\mathrm{MINRES}} = c_{\\mathrm{MINRES}}\\,u = \\frac{\\sqrt{2}\\alpha}{\\alpha^2 + \\gamma^2} u$.\nThe norm of this iterate is:\n$$\n\\|x_1^{\\mathrm{MINRES}}\\|_{2} = \\left\\| \\frac{\\sqrt{2}\\alpha}{\\alpha^2 + \\gamma^2} u \\right\\|_{2} = \\left| \\frac{\\sqrt{2}\\alpha}{\\alpha^2 + \\gamma^2} \\right| \\|u\\|_{2} = \\frac{\\sqrt{2}|\\alpha|}{\\alpha^2 + \\gamma^2}\n$$\n\n**SYMMLQ Iterate**\nSYMMLQ finds the iterate $x_1^{\\mathrm{SYMMLQ}} = V_1 y_1$, where $y_1$ is the solution to the projected system $T_1 y_1 = \\|b\\|_{2} e_1$. Here, $V_1=[v_1]$, $y_1$ is a scalar, and $e_1$ is the first canonical vector in $\\mathbb{R}^1$, which is just the scalar $1$.\nThe projected system is:\n$$\n\\alpha \\cdot y_1 = \\sqrt{2}\n$$\nThe problem statement makes a reference to $\\gamma/\\alpha$, which implies $\\alpha \\neq 0$. Thus, we can solve for $y_1$:\n$$\ny_1 = \\frac{\\sqrt{2}}{\\alpha}\n$$\nThe SYMMLQ iterate is $x_1^{\\mathrm{SYMMLQ}} = y_1 v_1 = \\frac{\\sqrt{2}}{\\alpha} u$.\nThe norm of this iterate is:\n$$\n\\|x_1^{\\mathrm{SYMMLQ}}\\|_{2} = \\left\\| \\frac{\\sqrt{2}}{\\alpha} u \\right\\|_{2} = \\left| \\frac{\\sqrt{2}}{\\alpha} \\right| \\|u\\|_{2} = \\frac{\\sqrt{2}}{|\\alpha|}\n$$\n\n**Ratio of the Norms**\nFinally, we compute the ratio $\\rho$:\n$$\n\\rho = \\frac{\\|x_{1}^{\\mathrm{SYMMLQ}}\\|_{2}}{\\|x_{1}^{\\mathrm{MINRES}}\\|_{2}} = \\frac{\\frac{\\sqrt{2}}{|\\alpha|}}{\\frac{\\sqrt{2}|\\alpha|}{\\alpha^2 + \\gamma^2}}\n$$\n$$\n\\rho = \\frac{\\sqrt{2}}{|\\alpha|} \\cdot \\frac{\\alpha^2 + \\gamma^2}{\\sqrt{2}|\\alpha|} = \\frac{\\alpha^2 + \\gamma^2}{|\\alpha|^2} = \\frac{\\alpha^2 + \\gamma^2}{\\alpha^2}\n$$\nThis expression quantifies the difference in the magnitudes of the solution vectors produced by one iteration of SYMMLQ and MINRES. As $\\gamma/\\alpha \\to 0$, $\\rho \\to 1$, and the methods produce similar iterates. For small $\\alpha$ and non-small $\\gamma$, the ratio can become very large, reflecting the potential instability of the standard Galerkin approach (SYMMLQ) for indefinite systems compared to the residual-minimizing approach (MINRES).",
            "answer": "$$\\boxed{\\frac{\\alpha^{2} + \\gamma^{2}}{\\alpha^{2}}}$$"
        }
    ]
}