{
    "hands_on_practices": [
        {
            "introduction": "We begin by establishing a baseline for the computational expense of Gaussian elimination. This fundamental exercise guides you through a step-by-step derivation of the exact number of floating-point operations (flops) required to factorize a general dense matrix. By meticulously counting the multiplications and additions at each stage of the elimination process, you will arrive at the famous $(2/3)n^3$ leading-order complexity, a cornerstone result in numerical linear algebra that quantifies how the workload scales with the problem size $n$. ",
            "id": "3562232",
            "problem": "Consider the classical Gaussian elimination algorithm without pivoting applied to a dense $n \\times n$ real matrix to compute its Lower-Upper (LU) factorization. Adopt the operation counting convention where a floating-point multiplication counts as $1$ flop, a floating-point addition or subtraction counts as $1$ flop, and floating-point divisions are counted separately (do not contribute to the flop total). Starting from the algorithmic structure of elimination, derive the exact closed-form expression for the total number of flops (counting only additions and multiplications) required to complete the elimination phase, and then evaluate this expression for $n=1000$. Express your final answer as an exact integer with no rounding. You may also state the number of divisions separately in your derivation, but the requested final answer is the total flop count excluding divisions.",
            "solution": "The problem requires the derivation of the exact number of floating-point operations (flops) for the elimination phase of Gaussian elimination on a dense $n \\times n$ matrix, and the evaluation of this count for $n=1000$. A flop is defined as one multiplication or one addition/subtraction. Divisions are counted separately and excluded from the final flop total.\n\nThe Gaussian elimination algorithm transforms a matrix $A$ into an upper triangular matrix $U$ by introducing zeros below the main diagonal. This process is performed in $n-1$ stages. Let $A^{(k)}$ denote the matrix at the end of stage $k-1$, with $A^{(1)} = A$.\n\nAt stage $k$, for $k=1, 2, \\dots, n-1$, the goal is to eliminate the non-zero entries in column $k$ below the pivot element $a_{kk}^{(k)}$. This is achieved by performing row operations for each row $i$ from $k+1$ to $n$.\n\nFor each row $i$ where $i \\in \\{k+1, \\dots, n\\}$:\n1.  A multiplier $m_{ik}$ is computed. This multiplier will be stored in the $(i, k)$ entry of the lower triangular matrix $L$.\n    $$ m_{ik} = \\frac{a_{ik}^{(k)}}{a_{kk}^{(k)}} $$\n    This operation constitutes $1$ division. Since this is done for each row $i$ from $k+1$ to $n$, there are $n-k$ divisions at stage $k$.\n\n2.  Row $i$ is updated by subtracting $m_{ik}$ times row $k$. The update rule for the elements of row $i$ is:\n    $$ a_{ij}^{(k+1)} = a_{ij}^{(k)} - m_{ik} \\cdot a_{kj}^{(k)} $$\n    The element $a_{ik}^{(k+1)}$ becomes $0$ by construction and does not require computation. The actual computations are for the elements in columns $j=k+1, \\dots, n$. The number of such columns is $n - (k+1) + 1 = n-k$.\n    For each element $a_{ij}^{(k+1)}$ being updated, the operation $a_{ij}^{(k)} - m_{ik} \\cdot a_{kj}^{(k)}$ involves $1$ multiplication and $1$ subtraction, for a total of $2$ flops.\n    Therefore, updating a single row $i$ requires $2(n-k)$ flops.\n\nSince there are $n-k$ rows to update at stage $k$ (from $i=k+1$ to $n$), the total number of flops at stage $k$ is:\n$$ \\text{Flops}_k = (n-k) \\times 2(n-k) = 2(n-k)^2 $$\n\nTo find the total number of flops, $F_n$, for the entire elimination process, we sum the flops from stage $k=1$ to $k=n-1$:\n$$ F_n = \\sum_{k=1}^{n-1} \\text{Flops}_k = \\sum_{k=1}^{n-1} 2(n-k)^2 $$\nWe can simplify this summation by making a change of index. Let $j = n-k$. As $k$ goes from $1$ to $n-1$, $j$ goes from $n-1$ down to $1$.\n$$ F_n = 2 \\sum_{j=1}^{n-1} j^2 $$\nUsing the well-known formula for the sum of the first $m$ squares, $\\sum_{j=1}^{m} j^2 = \\frac{m(m+1)(2m+1)}{6}$, with $m=n-1$:\n$$ F_n = 2 \\left( \\frac{(n-1)((n-1)+1)(2(n-1)+1)}{6} \\right) $$\n$$ F_n = 2 \\left( \\frac{(n-1)n(2n-1)}{6} \\right) $$\n$$ F_n = \\frac{n(n-1)(2n-1)}{3} $$\nExpanding this expression gives the polynomial form:\n$$ F_n = \\frac{n(2n^2 - 3n + 1)}{3} = \\frac{2}{3}n^3 - n^2 + \\frac{1}{3}n $$\nThis is the exact closed-form expression for the total number of flops (multiplications and additions/subtractions).\n\nFor completeness, the total number of divisions, $D_n$, is the sum of the divisions at each stage:\n$$ D_n = \\sum_{k=1}^{n-1} (n-k) = \\sum_{j=1}^{n-1} j = \\frac{(n-1)n}{2} = \\frac{1}{2}n^2 - \\frac{1}{2}n $$\n\nThe problem asks for the evaluation of $F_n$ for $n=1000$. Substituting $n=1000$ into the derived formula:\n$$ F_{1000} = \\frac{1000(1000-1)(2 \\cdot 1000 - 1)}{3} $$\n$$ F_{1000} = \\frac{1000 \\cdot 999 \\cdot 1999}{3} $$\nSince $999 = 3 \\cdot 333$, we can simplify the expression:\n$$ F_{1000} = 1000 \\cdot \\frac{999}{3} \\cdot 1999 = 1000 \\cdot 333 \\cdot 1999 $$\nTo compute the final integer value:\n$$ F_{1000} = 333000 \\cdot 1999 $$\n$$ F_{1000} = 333000 \\cdot (2000 - 1) $$\n$$ F_{1000} = 333000 \\cdot 2000 - 333000 \\cdot 1 $$\n$$ F_{1000} = 666000000 - 333000 $$\n$$ F_{1000} = 665667000 $$\nThis is the exact integer value for the total number of flops for $n=1000$.",
            "answer": "$$\n\\boxed{665667000}\n$$"
        },
        {
            "introduction": "In practice, standard Gaussian elimination is numerically unstable and requires a pivoting strategy to ensure reliability. This exercise examines the computational cost of implementing partial pivoting, the most common stabilization technique. You will analyze the additional work required at each step to search for the optimal pivot element and determine the total number of comparisons. This practice demonstrates a critical principle: the cost of ensuring numerical stability, which is $O(n^2)$, is asymptotically negligible compared to the $O(n^3)$ cost of the factorization itself. ",
            "id": "2160709",
            "problem": "A student in a numerical analysis course is tasked with solving a dense square linear system of equations, $A\\mathbf{x} = \\mathbf{b}$, where $A$ is an $n \\times n$ invertible matrix. The standard Gaussian elimination algorithm, without any pivoting, is known to have a computational cost dominated by the forward elimination phase, which requires approximately $\\frac{2}{3}n^3$ floating-point operations (flops) for large $n$.\n\nTo improve the numerical stability of the algorithm, the student implements partial pivoting. This modification involves an extra procedure at the beginning of each of the $n-1$ steps of forward elimination. Specifically, for each step $k$ (where $k$ ranges from $1$ to $n-1$), before performing elimination on column $k$, the algorithm must identify the entry with the largest absolute value in the current pivot column from the diagonal element downwards. That is, it must find the maximum of $\\{|a_{k,k}|, |a_{k+1,k}|, \\dots, |a_{n,k}|\\}$. The row containing this maximal element is then swapped with the current pivot row, row $k$.\n\nAssuming that the computational cost of a row swap is negligible, determine the exact total number of comparison operations, $C(n)$, required to implement this partial pivoting strategy for the entire forward elimination process. Provide your answer as a closed-form expression in terms of $n$.",
            "solution": "At forward elimination step $k$, the algorithm must find the maximum of the $n-k+1$ values $\\{|a_{k,k}|,|a_{k+1,k}|,\\dots,|a_{n,k}|\\}$. To determine the maximum of $m$ numbers requires exactly $m-1$ comparisons. Therefore, at step $k$, the number of comparisons is\n$$\n(n-k+1)-1=n-k.\n$$\nSumming over all steps $k=1,2,\\dots,n-1$, the total number of comparisons is\n$$\nC(n)=\\sum_{k=1}^{n-1}(n-k)=\\sum_{j=1}^{n-1}j=\\frac{n(n-1)}{2}.\n$$\nRow swaps are assumed negligible and do not add comparisons, and evaluating absolute values is not counted as a comparison. Hence this is the exact total.",
            "answer": "$$\\boxed{\\frac{n(n-1)}{2}}$$"
        },
        {
            "introduction": "A key motivation for computing an LU factorization is to efficiently solve linear systems with multiple right-hand sides, a common task in scientific and engineering applications. This practice delves into the economic trade-off between the substantial one-time cost of factorization and the relatively inexpensive cost of repeated forward and backward substitutions. By comparing the leading-order asymptotic costs of these two phases, you will derive the \"crossover point\"â€”the number of right-hand sides for which the total solve cost matches the initial factorization cost, providing a powerful rule of thumb for algorithmic design. ",
            "id": "3562285",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a dense, nonsingular matrix, and suppose you must solve $A \\mathbf{x}^{(j)} = \\mathbf{b}^{(j)}$ for $j = 1, \\dots, k$, where $k = k(n)$ scales with $n$. You will compute a single Gaussian elimination with partial pivoting to obtain a unit-lower-triangular and upper-triangular factorization $A = P L U$ (with $P$ a permutation matrix), followed by forward and backward substitution for each right-hand side. Starting from the algorithmic definitions of Gaussian elimination and triangular solves, and using floating-point operation (flop) counts with multiplies and adds counted separately, do the following:\n\n- Derive the leading-order flop count, in terms of $n$, for the one-time factorization.\n- Derive the leading-order flop count, in terms of $n$ and $k$, for performing all the forward and backward substitutions across the $k$ right-hand sides.\n\nAssume dense data access and ignore lower-order terms. Using your derived expressions, analyze the asymptotic regime $n \\to \\infty$ with $k = k(n)$ scaling with $n$ and determine the crossover point $k^{\\star}(n)$ at which the cumulative solve cost first matches the one-time factorization cost. Express your final answer as a closed-form analytic expression for $k^{\\star}(n)$ in terms of $n$. No rounding is required, and no units are involved. Your final answer must be a single expression.",
            "solution": "The problem requires the derivation of the crossover point, denoted $k^{\\star}(n)$, where the computational cost of solving $k$ linear systems via one-time LU factorization and subsequent substitutions equals the cost of the factorization itself. We analyze the problem in the asymptotic regime where the matrix size $n \\to \\infty$. The analysis relies on counting floating-point operations (flops), where multiplications and additions are tallied separately, as specified.\n\nFirst, we derive the leading-order flop count for the LU factorization of a dense $n \\times n$ matrix $A$ using Gaussian elimination. The process consists of $n-1$ steps. At step $i$, for $i=1, 2, \\dots, n-1$, we eliminate the non-zero entries below the diagonal in column $i$. This is achieved by performing row operations on rows $j=i+1, \\dots, n$.\n\nFor each row $j$ below the pivot row $i$, the procedure is as follows:\n1.  Compute the multiplier $L_{ji} = A_{ji}^{(i-1)} / A_{ii}^{(i-1)}$. This costs $1$ division. We count divisions and multiplications together.\n2.  Update row $j$ for all columns $k = i+1, \\dots, n$. The update rule is $A_{jk}^{(i)} \\leftarrow A_{jk}^{(i-1)} - L_{ji} A_{ik}^{(i-1)}$. For each column $k$, this requires $1$ multiplication and $1$ addition (or subtraction).\n\nAt step $i$, we are working with an $(n-i+1) \\times (n-i+1)$ submatrix.\n-   Number of multipliers to compute: There are $n-i$ rows below the pivot row, so we compute $n-i$ multipliers. This contributes $n-i$ divisions.\n-   Number of row updates: For each of the $n-i$ rows, we update the elements in columns $i+1$ through $n$. There are $n-i$ such columns. The update for each element costs $1$ multiplication and $1$ addition. Thus, for each of the $n-i$ rows, the update cost is $(n-i)$ multiplications and $(n-i)$ additions.\n-   Total operations at step $i$:\n    -   Multiplications/Divisions: $(n-i) + (n-i)(n-i) = (n-i) + (n-i)^2$.\n    -   Additions/Subtractions: $(n-i)(n-i) = (n-i)^2$.\n\nTo find the total cost of factorization, we sum over all steps $i = 1, \\dots, n-1$:\n-   Total Multiplications/Divisions: $C_{\\text{mult}} = \\sum_{i=1}^{n-1} ((n-i) + (n-i)^2)$.\n-   Total Additions/Subtractions: $C_{\\text{add}} = \\sum_{i=1}^{n-1} (n-i)^2$.\n\nWe are interested in the leading-order terms as $n \\to \\infty$. The dominant term in both sums is the one involving $(n-i)^2$. Let $j = n-i$. As $i$ ranges from $1$ to $n-1$, $j$ ranges from $n-1$ to $1$. The leading-order cost for both multiplications and additions is determined by the sum $\\sum_{j=1}^{n-1} j^2$.\nUsing the formula for the sum of squares, $\\sum_{j=1}^{m} j^2 = \\frac{m(m+1)(2m+1)}{6}$:\n$$ \\sum_{j=1}^{n-1} j^2 = \\frac{(n-1)(n)(2(n-1)+1)}{6} = \\frac{(n-1)n(2n-1)}{6} = \\frac{2n^3 - 3n^2 + n}{6} $$\nAs $n \\to \\infty$, the leading-order term of this sum is $\\frac{2n^3}{6} = \\frac{n^3}{3}$.\nTherefore, the leading-order costs are:\n-   $C_{\\text{mult, fact}} \\approx \\frac{n^3}{3}$\n-   $C_{\\text{add, fact}} \\approx \\frac{n^3}{3}$\nThe total leading-order flop count for the factorization is $C_{\\text{fact}} \\approx \\frac{n^3}{3} + \\frac{n^3}{3} = \\frac{2}{3}n^3$.\n\nNext, we derive the cost of solving the system for a single right-hand side vector $\\mathbf{b}$. After obtaining the factorization $A=PLU$, solving $A\\mathbf{x} = \\mathbf{b}$ becomes $LU\\mathbf{x} = P^T \\mathbf{b}$. This is a two-step process. First, define $\\mathbf{y} = U\\mathbf{x}$ and solve the lower-triangular system $L\\mathbf{y}=P^T \\mathbf{b}$ by forward substitution. Second, solve the upper-triangular system $U\\mathbf{x}=\\mathbf{y}$ by backward substitution. The permutation $P^T \\mathbf{b}$ is a reordering of elements and costs $0$ flops.\n\n1.  **Forward Substitution:** We solve $L\\mathbf{y}=P^T \\mathbf{b}$. Since $L$ is unit-lower-triangular ($L_{ii}=1$), the formula for each component of $\\mathbf{y}$ is:\n    $$ y_i = (P^T \\mathbf{b})_i - \\sum_{j=1}^{i-1} L_{ij} y_j, \\quad \\text{for } i = 1, \\dots, n $$\n    For each $i$, computing $y_i$ requires $i-1$ multiplications and $i-1$ additions.\n    The total costs are:\n    -   $C_{\\text{mult, fwd}} = \\sum_{i=1}^{n} (i-1) = \\frac{(n-1)n}{2} = \\frac{n^2}{2} - \\frac{n}{2}$.\n    -   $C_{\\text{add, fwd}} = \\sum_{i=1}^{n} (i-1) = \\frac{(n-1)n}{2} = \\frac{n^2}{2} - \\frac{n}{2}$.\n    The leading-order cost for forward substitution is $\\frac{n^2}{2}$ multiplications and $\\frac{n^2}{2}$ additions.\n\n2.  **Backward Substitution:** We solve $U\\mathbf{x}=\\mathbf{y}$. The formula for each component of $\\mathbf{x}$ is:\n    $$ x_i = \\frac{1}{U_{ii}} \\left( y_i - \\sum_{j=i+1}^{n} U_{ij} x_j \\right), \\quad \\text{for } i = n, \\dots, 1 $$\n    For each $i$, computing $x_i$ requires $(n-i)$ multiplications, $(n-i)$ additions, and $1$ division.\n    The total costs are:\n    -   $C_{\\text{mult/div, bwd}} = \\sum_{i=1}^{n} ((n-i) + 1) = \\left(\\sum_{j=0}^{n-1} j\\right) + n = \\frac{(n-1)n}{2} + n = \\frac{n^2-n+2n}{2} = \\frac{n^2}{2} + \\frac{n}{2}$.\n    -   $C_{\\text{add, bwd}} = \\sum_{i=1}^{n} (n-i) = \\sum_{j=0}^{n-1} j = \\frac{(n-1)n}{2} = \\frac{n^2}{2} - \\frac{n}{2}$.\n    The leading-order cost for backward substitution is $\\frac{n^2}{2}$ multiplications/divisions and $\\frac{n^2}{2}$ additions.\n\nThe total cost for a single solve ($C_{\\text{solve}}$) is the sum of forward and backward substitution costs.\n-   Total multiplications/divisions per solve: $C_{\\text{mult, solve}} \\approx \\frac{n^2}{2} + \\frac{n^2}{2} = n^2$.\n-   Total additions/subtractions per solve: $C_{\\text{add, solve}} \\approx \\frac{n^2}{2} + \\frac{n^2}{2} = n^2$.\nThe total leading-order flop count for one solve is $C_{\\text{solve}} \\approx n^2 + n^2 = 2n^2$.\n\nThe total cost for solving for $k$ right-hand sides is $C_{\\text{solves}} = k \\cdot C_{\\text{solve}}$. In the asymptotic limit, this is $C_{\\text{solves}} \\approx k(2n^2)$.\n\nThe crossover point $k^{\\star}(n)$ is the value of $k$ at which the cumulative solve cost equals the one-time factorization cost. We set the leading-order expressions for these costs equal to each other:\n$$ C_{\\text{fact}} = C_{\\text{solves}} $$\n$$ \\frac{2}{3}n^3 = k^{\\star}(n) \\cdot (2n^2) $$\nSolving for $k^{\\star}(n)$:\n$$ k^{\\star}(n) = \\frac{\\frac{2}{3}n^3}{2n^2} = \\frac{2n^3}{6n^2} = \\frac{n}{3} $$\nThus, the number of solves must be approximately one-third of the matrix dimension for the cost of the solves to match the cost of the initial factorization.",
            "answer": "$$\\boxed{\\frac{n}{3}}$$"
        }
    ]
}