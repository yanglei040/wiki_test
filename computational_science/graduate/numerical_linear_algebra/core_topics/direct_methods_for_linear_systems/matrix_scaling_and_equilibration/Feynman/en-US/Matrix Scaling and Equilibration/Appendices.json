{
    "hands_on_practices": [
        {
            "introduction": "A primary motivation for matrix scaling is to improve the numerical stability of solving linear systems, especially for ill-conditioned matrices. This practice provides a direct and quantitative way to assess this benefit by calculating the normwise backward error before and after applying a simple diagonal scaling. By working through this example, you will gain a concrete understanding of how rebalancing the magnitudes of matrix columns can lead to a more well-behaved problem from a backward stability perspective.",
            "id": "3559217",
            "problem": "Consider the linear system $A x = b$ with\n$$\nA \\;=\\; \\begin{pmatrix} 1000 & 1 \\\\ 1 & 0.001 \\end{pmatrix}, \\qquad \\widehat{x} \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nSuppose the computed residual is\n$$\nr \\;=\\; b - A \\widehat{x} \\;=\\; \\begin{pmatrix} 1 \\\\ 0.001 \\end{pmatrix},\n$$\nso that $b = A \\widehat{x} + r$. You are to assess the effect of diagonal scaling on the normwise backward error, using the induced infinity norm.\n\nFor the unscaled system, define the normwise backward error (in the infinity norm) as the minimal scalar $\\eta \\ge 0$ for which there exist perturbations $\\Delta A$ and $\\Delta b$ satisfying $(A + \\Delta A)\\,\\widehat{x} = b + \\Delta b$ with $\\|\\Delta A\\|_{\\infty} \\le \\eta \\,\\|A\\|_{\\infty}$ and $\\|\\Delta b\\|_{\\infty} \\le \\eta \\,\\|b\\|_{\\infty}$. For the scaled system, apply diagonal column scaling $D_c = \\mathrm{diag}(10^{-3}, 10^{3})$, rewrite the system as $(A D_c) y = b$ with $y = D_c^{-1} \\widehat{x}$, and define the scaled normwise backward error analogously for the pair $(A D_c, b)$.\n\nUsing only the fundamental definition of induced matrix norms and of the normwise backward error, derive the standard bound for the normwise backward error in the infinity norm for both the unscaled and scaled systems, and compute the reduction factor\n$$\n\\rho \\;=\\; \\frac{\\eta_{\\mathrm{scaled}}}{\\eta_{\\mathrm{unscaled}}}.\n$$\nRound your final answer for $\\rho$ to $4$ significant figures. The answer is dimensionless and should be given as a real number.",
            "solution": "The problem asks for an assessment of the effect of diagonal column scaling on the normwise backward error for a given linear system. The analysis will be performed using the induced infinity norm. We must first derive the standard formula for the normwise backward error and then apply it to both the unscaled and scaled systems to compute the reduction factor.\n\nLet the given approximate solution be $\\widehat{x}$ for the system $A x = b$. The residual is $r = b - A\\widehat{x}$. The normwise backward error, $\\eta$, is defined as the smallest non-negative number such that there exist perturbations $\\Delta A$ and $\\Delta b$ satisfying\n$$\n(A + \\Delta A)\\,\\widehat{x} = b + \\Delta b\n$$\nwith the constraints $\\|\\Delta A\\|_{\\infty} \\le \\eta \\,\\|A\\|_{\\infty}$ and $\\|\\Delta b\\|_{\\infty} \\le \\eta \\,\\|b\\|_{\\infty}$.\n\nFirst, we derive the general formula for $\\eta$. Rearranging the perturbed system equation, we obtain an expression for the residual:\n$$\nr = b - A\\widehat{x} = \\Delta A \\widehat{x} - \\Delta b\n$$\nTaking the infinity norm of both sides and applying the triangle inequality, we get:\n$$\n\\|r\\|_{\\infty} = \\|\\Delta A \\widehat{x} - \\Delta b\\|_{\\infty} \\le \\|\\Delta A \\widehat{x}\\|_{\\infty} + \\|\\Delta b\\|_{\\infty}\n$$\nUsing the property of induced matrix norms, $\\|\\Delta A \\widehat{x}\\|_{\\infty} \\le \\|\\Delta A\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty}$, we have:\n$$\n\\|r\\|_{\\infty} \\le \\|\\Delta A\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty} + \\|\\Delta b\\|_{\\infty}\n$$\nNow, we substitute the constraints on the perturbations, $\\|\\Delta A\\|_{\\infty} \\le \\eta \\,\\|A\\|_{\\infty}$ and $\\|\\Delta b\\|_{\\infty} \\le \\eta \\,\\|b\\|_{\\infty}$:\n$$\n\\|r\\|_{\\infty} \\le (\\eta \\,\\|A\\|_{\\infty}) \\|\\widehat{x}\\|_{\\infty} + (\\eta \\,\\|b\\|_{\\infty}) = \\eta \\left( \\|A\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty} + \\|b\\|_{\\infty} \\right)\n$$\nThis inequality provides a lower bound for $\\eta$:\n$$\n\\eta \\ge \\frac{\\|r\\|_{\\infty}}{\\|A\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty} + \\|b\\|_{\\infty}}\n$$\nThis lower bound is attainable, so the normwise backward error is precisely:\n$$\n\\eta = \\frac{\\|r\\|_{\\infty}}{\\|A\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty} + \\|b\\|_{\\infty}}\n$$\n\nNow, we apply this formula to the unscaled system. The given quantities are:\n$$\nA = \\begin{pmatrix} 1000 & 1 \\\\ 1 & 0.001 \\end{pmatrix}, \\quad \\widehat{x} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad r = \\begin{pmatrix} 1 \\\\ 0.001 \\end{pmatrix}\n$$\nFrom these, we can determine the right-hand side vector $b$:\n$$\nb = A\\widehat{x} + r = \\begin{pmatrix} 1000 & 1 \\\\ 1 & 0.001 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0.001 \\end{pmatrix} = \\begin{pmatrix} 1001 \\\\ 1.001 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0.001 \\end{pmatrix} = \\begin{pmatrix} 1002 \\\\ 1.002 \\end{pmatrix}\n$$\nWe compute the required infinity norms. The induced matrix infinity norm is the maximum absolute row sum.\n$$\n\\|A\\|_{\\infty} = \\max(|1000| + |1|, |1| + |0.001|) = \\max(1001, 1.001) = 1001\n$$\n$$\n\\|\\widehat{x}\\|_{\\infty} = \\max(|1|, |1|) = 1\n$$\n$$\n\\|b\\|_{\\infty} = \\max(|1002|, |1.002|) = 1002\n$$\n$$\n\\|r\\|_{\\infty} = \\max(|1|, |0.001|) = 1\n$$\nThe backward error for the unscaled system, $\\eta_{\\mathrm{unscaled}}$, is:\n$$\n\\eta_{\\mathrm{unscaled}} = \\frac{\\|r\\|_{\\infty}}{\\|A\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty} + \\|b\\|_{\\infty}} = \\frac{1}{1001 \\cdot 1 + 1002} = \\frac{1}{2003}\n$$\n\nNext, we analyze the scaled system. The scaling is defined by $D_c = \\mathrm{diag}(10^{-3}, 10^{3})$. The system is rewritten as $(A D_c) y = b$, where the new matrix is $A' = A D_c$ and the new unknown is $y = D_c^{-1} x$. The approximate solution for the scaled system is $\\widehat{y} = D_c^{-1} \\widehat{x}$.\nLet's compute the components for the scaled system:\n$$\nA' = A D_c = \\begin{pmatrix} 1000 & 1 \\\\ 1 & 0.001 \\end{pmatrix} \\begin{pmatrix} 10^{-3} & 0 \\\\ 0 & 10^{3} \\end{pmatrix} = \\begin{pmatrix} 1 & 1000 \\\\ 0.001 & 1 \\end{pmatrix}\n$$\n$$\n\\widehat{y} = D_c^{-1} \\widehat{x} = \\begin{pmatrix} 10^{3} & 0 \\\\ 0 & 10^{-3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1000 \\\\ 0.001 \\end{pmatrix}\n$$\nThe residual for the scaled system, $r' = b - A'\\widehat{y}$, is:\n$$\nr' = b - (A D_c)(D_c^{-1} \\widehat{x}) = b - A\\widehat{x} = r = \\begin{pmatrix} 1 \\\\ 0.001 \\end{pmatrix}\n$$\nNow we compute the norms for the scaled problem:\n$$\n\\|A'\\|_{\\infty} = \\max(|1| + |1000|, |0.001| + |1|) = \\max(1001, 1.001) = 1001\n$$\n$$\n\\|\\widehat{y}\\|_{\\infty} = \\max(|1000|, |0.001|) = 1000\n$$\nThe norms of $b$ and $r'$ (which is $r$) are unchanged: $\\|b\\|_{\\infty} = 1002$ and $\\|r'\\|_{\\infty} = 1$.\nThe backward error for the scaled system, $\\eta_{\\mathrm{scaled}}$, is:\n$$\n\\eta_{\\mathrm{scaled}} = \\frac{\\|r'\\|_{\\infty}}{\\|A'\\|_{\\infty} \\|\\widehat{y}\\|_{\\infty} + \\|b\\|_{\\infty}} = \\frac{1}{1001 \\cdot 1000 + 1002} = \\frac{1}{1001000 + 1002} = \\frac{1}{1002002}\n$$\nFinally, we compute the reduction factor $\\rho$ as the ratio of the scaled backward error to the unscaled backward error:\n$$\n\\rho = \\frac{\\eta_{\\mathrm{scaled}}}{\\eta_{\\mathrm{unscaled}}} = \\frac{1/1002002}{1/2003} = \\frac{2003}{1002002}\n$$\nTo obtain the final numerical answer, we compute this value and round it to $4$ significant figures:\n$$\n\\rho \\approx 0.001998994...\n$$\nRounding to $4$ significant figures gives $0.001999$.",
            "answer": "$$\\boxed{0.001999}$$"
        },
        {
            "introduction": "While scaling can improve a matrix's condition number, its effect on the stability of algorithms like Gaussian elimination is more complex and is intimately tied to the element growth factor. This exercise demonstrates this nuance by constructing two scenarios: one where column scaling beneficially reduces the growth factor, and another where it detrimentally increases it. This powerful comparative analysis highlights that equilibration is not a universally optimal procedure and its effectiveness depends on the matrix structure.",
            "id": "3559226",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ and consider Gaussian elimination with partial pivoting, where at each step the pivot is the entry of maximum absolute value in the current column of the active submatrix, with ties broken by the smallest row index. Denote the computed factors by $P A = L U$, where $P$ is a permutation matrix, $L$ is unit lower triangular, and $U$ is upper triangular. Define the element growth factor in the infinity norm by\n$$\n\\gamma_{\\infty}(A) \\equiv \\frac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |A_{ij}|}.\n$$\nWe investigate how diagonal scaling (equilibration) can reduce or increase $\\gamma_{\\infty}(A)$ and how this propagates to a standard relative backward error bound.\n\nWork with the fixed sign-pattern matrix\n$$\nS \\equiv \\begin{pmatrix}\n1 & 1 & 1 \\\\\n-1 & 1 & 1 \\\\\n-1 & -1 & 1\n\\end{pmatrix}.\n$$\nConstruct two explicit $3 \\times 3$ matrices:\n- A reduction example $A^{(R)} \\equiv S$ together with the column diagonal scaling $D_{c}^{(\\downarrow)} \\equiv \\operatorname{diag}(1, \\tfrac{1}{2}, \\tfrac{1}{4})$.\n- An increase example $A^{(I)} \\equiv S \\operatorname{diag}(1, \\tfrac{1}{2}, \\tfrac{1}{4})$ together with the column diagonal scaling $D_{c}^{(\\uparrow)} \\equiv \\operatorname{diag}(1, 2, 4)$.\n\nFor each example, do the following:\n1. Compute the $L U$ factorization with partial pivoting (as specified above) of the unscaled matrix and of the column-scaled matrix, and from these factorizations compute the corresponding growth factors $\\gamma_{\\infty}$.\n2. Starting from the standard rounding model for floating-point arithmetic and the classical backward stability framework for Gaussian elimination with partial pivoting, derive a relative backward error bound of the form\n$$\n\\frac{\\|\\Delta A\\|_{\\infty}}{\\|A\\|_{\\infty}} \\leq c \\, u \\, p(n) \\, \\gamma_{\\infty}(A),\n$$\nwhere $u$ is the unit roundoff, $c$ is a modest constant independent of $A$ and $n$, and $p(n)$ is a polynomial in $n$ that does not depend on $A$. Explain why, when comparing pre- versus post-scaling for a fixed $n$ and the same elimination algorithm, the ratio of the relative backward error bounds is equal to the ratio of the corresponding growth factors.\n\nLet $\\rho_{R}$ denote the ratio “post-scaling bound divided by pre-scaling bound” for the reduction example $A^{(R)}$ with $D_{c}^{(\\downarrow)}$, and let $\\rho_{I}$ denote the analogous ratio for the increase example $A^{(I)}$ with $D_{c}^{(\\uparrow)}$.\n\nReport the pair $\\big(\\rho_{R}, \\rho_{I}\\big)$ as a single row vector. Give your final answer in exact form. No rounding is required and no physical units are involved.",
            "solution": "We proceed by analyzing each example, then deriving the error bound justification, and finally computing the required ratios.\n\n**1. Reduction Example: $A^{(R)}$ and $A^{(R)} D_{c}^{(\\downarrow)}$**\n\n**a) Unscaled Matrix: $A^{(R)} = S = \\begin{pmatrix} 1 & 1 & 1 \\\\ -1 & 1 & 1 \\\\ -1 & -1 & 1 \\end{pmatrix}$**\n\nThe largest element in absolute value is $\\max_{i,j} |A^{(R)}_{ij}| = 1$. We perform Gaussian elimination with partial pivoting (GEPP).\n*   **Step 1:** The first column is $(1, -1, -1)^T$. The maximum absolute value is $1$, and the tie-breaking rule selects the pivot $A_{11}^{(R)} = 1$. No row swap is needed ($P=I$). The multipliers are $l_{21} = -1$ and $l_{31} = -1$. The matrix after the first step of elimination is:\n    $$\n    \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 2 & 2 \\\\ 0 & 0 & 2 \\end{pmatrix}\n    $$\n*   **Step 2:** The matrix is now upper triangular, so the process terminates.\n\nThe factorization is $P A^{(R)} = L U$ with $P = I$,\n$$\nL = \\begin{pmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ -1 & 0 & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 2 & 2 \\\\ 0 & 0 & 2 \\end{pmatrix}\n$$\nThe maximum element in absolute value in $U$ is $\\max_{i,j} |U_{ij}| = 2$. The growth factor for $A^{(R)}$ is:\n$$\n\\gamma_{\\infty}(A^{(R)}) = \\frac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |A^{(R)}_{ij}|} = \\frac{2}{1} = 2\n$$\n\n**b) Scaled Matrix: $A_{s}^{(R)} = A^{(R)} D_{c}^{(\\downarrow)} = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ -1 & \\frac{1}{2} & \\frac{1}{4} \\\\ -1 & -\\frac{1}{2} & \\frac{1}{4} \\end{pmatrix}$**\n\nThe largest element in absolute value is $\\max_{i,j} |A_{s,ij}^{(R)}| = 1$. We perform GEPP.\n*   **Step 1:** The first column is $(1, -1, -1)^T$. The pivot is $A_{s,11}^{(R)} = 1$. Multipliers are $l_{21} = -1$ and $l_{31} = -1$. The matrix after the first step of elimination is:\n    $$\n    \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{2} \\\\ 0 & 0 & \\frac{1}{2} \\end{pmatrix}\n    $$\n*   **Step 2:** The matrix is now upper triangular.\n\nThe factorization is $P A_{s}^{(R)} = L_s U_s$ with $P = I$,\n$$\nL_s = \\begin{pmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ -1 & 0 & 1 \\end{pmatrix}, \\quad U_s = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{2} \\\\ 0 & 0 & \\frac{1}{2} \\end{pmatrix}\n$$\nThe maximum element in absolute value in $U_s$ is $\\max_{i,j} |U_{s,ij}| = 1$. The growth factor for $A_{s}^{(R)}$ is:\n$$\n\\gamma_{\\infty}(A_{s}^{(R)}) = \\frac{\\max_{i,j} |U_{s,ij}|}{\\max_{i,j} |A_{s,ij}^{(R)}|} = \\frac{1}{1} = 1\n$$\nIn this case, column scaling reduced the growth factor from $2$ to $1$.\n\n**2. Increase Example: $A^{(I)}$ and $A^{(I)} D_{c}^{(\\uparrow)}$**\n\n**a) Unscaled Matrix: $A^{(I)} = S \\operatorname{diag}(1, \\tfrac{1}{2}, \\tfrac{1}{4}) = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ -1 & \\frac{1}{2} & \\frac{1}{4} \\\\ -1 & -\\frac{1}{2} & \\frac{1}{4} \\end{pmatrix}$**\n\nThis is the same matrix as $A_{s}^{(R)}$ from the reduction example. Therefore, its growth factor is $\\gamma_{\\infty}(A^{(I)}) = 1$.\n\n**b) Scaled Matrix: $A_{s}^{(I)} = A^{(I)} D_{c}^{(\\uparrow)} = \\left(S \\operatorname{diag}(1, \\tfrac{1}{2}, \\tfrac{1}{4})\\right) \\operatorname{diag}(1, 2, 4)$**\n\nBy associativity, $A_{s}^{(I)} = S \\left(\\operatorname{diag}(1, \\tfrac{1}{2}, \\tfrac{1}{4}) \\operatorname{diag}(1, 2, 4)\\right) = S \\operatorname{diag}(1, 1, 1) = S$.\nSo, $A_{s}^{(I)}$ is the same matrix as the unscaled $A^{(R)}$. Therefore, its growth factor is $\\gamma_{\\infty}(A_{s}^{(I)}) = 2$.\nIn this case, column scaling increased the growth factor from $1$ to $2$.\n\n**3. Backward Error Bound and Ratio Justification**\n\nStandard backward error analysis for GEPP shows that the computed solution is exact for a perturbed system $(A + \\Delta A)\\hat{x} = b$, where the backward error $\\Delta A$ is bounded. A common bound is of the form:\n$$\n\\frac{\\|\\Delta A\\|_{\\infty}}{\\|A\\|_{\\infty}} \\leq C(n,u) \\, \\gamma_{\\infty}(A)\n$$\nwhere $u$ is the unit roundoff and $C(n,u)$ consolidates all factors dependent on $n$ or fundamental constants, but not on the entries of $A$. This simplified model is used to isolate the growth factor as the primary indicator of stability changes due to scaling for a fixed matrix size $n$.\n\nWhen comparing the bound for a post-scaling matrix $A_{\\text{scaled}}$ to a pre-scaling matrix $A_{\\text{unscaled}}$ of the same dimension, the ratio of their bounds is:\n$$\n\\frac{\\text{Bound}(A_{\\text{scaled}})}{\\text{Bound}(A_{\\text{unscaled}})} = \\frac{C(n, u) \\, \\gamma_{\\infty}(A_{\\text{scaled}})}{C(n, u) \\, \\gamma_{\\infty}(A_{\\text{unscaled}})} = \\frac{\\gamma_{\\infty}(A_{\\text{scaled}})}{\\gamma_{\\infty}(A_{\\text{unscaled}})}\n$$\nThis justifies why the ratio of the bounds is simply the ratio of the corresponding growth factors.\n\n**4. Final Calculation of Ratios**\n\nWe compute the ratios $\\rho_{R}$ and $\\rho_{I}$.\n\nFor the reduction example:\n$$\n\\rho_{R} = \\frac{\\gamma_{\\infty}(A_{s}^{(R)})}{\\gamma_{\\infty}(A^{(R)})} = \\frac{1}{2}\n$$\n\nFor the increase example:\n$$\n\\rho_{I} = \\frac{\\gamma_{\\infty}(A_{s}^{(I)})}{\\gamma_{\\infty}(A^{(I)})} = \\frac{2}{1} = 2\n$$\n\nThe required pair is $(\\rho_{R}, \\rho_{I}) = (\\frac{1}{2}, 2)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2} & 2\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond improving numerical conditioning, diagonal scaling can be used to enforce specific structural properties on a matrix, a task with applications in areas from statistics to economics. The Sinkhorn-Knopp algorithm addresses a classic problem of this type: scaling a non-negative matrix to be doubly stochastic. This exercise will guide you through verifying the conditions for the existence of such a scaling and computing the scaling factors, providing insight into the theory of non-negative matrices.",
            "id": "3559220",
            "problem": "Consider a nonnegative matrix $A \\in \\mathbb{R}^{n \\times n}$ and the problem of diagonal scaling to doubly stochastic form, known as Sinkhorn-Knopp scaling. A matrix $B \\in \\mathbb{R}^{n \\times n}$ is called doubly stochastic if all its entries are nonnegative and every row sum and every column sum is equal to $1$. The Sinkhorn-Knopp scaling problem asks for positive diagonal matrices $D = \\mathrm{diag}(d_{1}, \\dots, d_{n})$ and $E = \\mathrm{diag}(e_{1}, \\dots, e_{n})$ such that $B = D A E$ is doubly stochastic. A well-tested fact is that if $A$ has total support (equivalently, each positive entry of $A$ lies on a positive permutation) then the Sinkhorn-Knopp algorithm converges and such scalings exist; furthermore, if $A$ is fully indecomposable (no zero pattern admits a nontrivial block triangular form), then the scaling is unique up to a common positive scalar factor, meaning that if $D$ and $E$ yield a doubly stochastic scaling, then so do $t D$ and $t^{-1} E$ for any $t > 0$, and all solutions arise in this way.\n\nWork with the concrete matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 1 & 2 \\\\\n2 & 0 & 1 \\\\\n1 & 2 & 0\n\\end{pmatrix}.\n$$\n\nUsing only the core definitions and the above widely accepted facts, do the following:\n- Determine whether the Sinkhorn-Knopp scaling exists for the given matrix $A$ by verifying the appropriate structural condition at the level of permutation support.\n- Compute an explicit pair of positive diagonal scalings $D$ and $E$ for $A$ that yields a doubly stochastic matrix $B = D A E$, exploiting any structural invariances derivable from the definitions (do not invoke unproven shortcut formulas).\n- Analyze the uniqueness of the scaling up to a positive scalar factor for this matrix, and identify an invariant multiplicative quantity of the scalings under the transformation $(D,E) \\mapsto (t D, t^{-1} E)$.\n\nReport as your final answer the exact value of the invariant product $\\det(D)\\,\\det(E)$ associated with your computed scaling. No rounding is required. Express your answer as a single real number or a single closed-form analytic expression without units.",
            "solution": "**Part 1: Existence of the Scaling**\n\nThe problem states that a Sinkhorn-Knopp scaling exists if the matrix $A$ has total support. A matrix has total support if every positive entry lies on a positive permutation. A permutation $\\sigma$ of $\\{1, 2, \\dots, n\\}$ is positive for an $n \\times n$ matrix $A$ if the product of the corresponding entries, $\\prod_{i=1}^{n} a_{i, \\sigma(i)}$, is positive.\n\nThe given matrix is $A = \\begin{pmatrix} 0 & 1 & 2 \\\\ 2 & 0 & 1 \\\\ 1 & 2 & 0 \\end{pmatrix}$. This is a $3 \\times 3$ matrix, so we examine the $3! = 6$ permutations of $\\{1, 2, 3\\}$. The terms corresponding to permutations in the determinant expansion are:\n\\begin{enumerate}\n    \\item $\\sigma = (1, 2, 3)$ (identity): $a_{11}a_{22}a_{33} = 0 \\cdot 0 \\cdot 0 = 0$.\n    \\item $\\sigma = (1, 3, 2)$: $a_{11}a_{23}a_{32} = 0 \\cdot 1 \\cdot 2 = 0$.\n    \\item $\\sigma = (2, 1, 3)$: $a_{12}a_{21}a_{33} = 1 \\cdot 2 \\cdot 0 = 0$.\n    \\item $\\sigma = (2, 3, 1)$: $a_{12}a_{23}a_{31} = 1 \\cdot 1 \\cdot 1 = 1 > 0$. This is a positive permutation. The entries involved are $\\{a_{12}, a_{23}, a_{31}\\}$.\n    \\item $\\sigma = (3, 1, 2)$: $a_{13}a_{21}a_{32} = 2 \\cdot 2 \\cdot 2 = 8 > 0$. This is a positive permutation. The entries involved are $\\{a_{13}, a_{21}, a_{32}\\}$.\n    \\item $\\sigma = (3, 2, 1)$: $a_{13}a_{22}a_{31} = 2 \\cdot 0 \\cdot 1 = 0$.\n\\end{enumerate}\nThere are two positive permutations. The set of positive entries in $A$ is $\\{ a_{12}, a_{13}, a_{21}, a_{23}, a_{31}, a_{32} \\}$. We check if each lies on one of the two positive permutations:\n- $a_{12} = 1$: lies on the permutation $\\sigma=(2, 3, 1)$.\n- $a_{13} = 2$: lies on the permutation $\\sigma=(3, 1, 2)$.\n- $a_{21} = 2$: lies on the permutation $\\sigma=(3, 1, 2)$.\n- $a_{23} = 1$: lies on the permutation $\\sigma=(2, 3, 1)$.\n- $a_{31} = 1$: lies on the permutation $\\sigma=(2, 3, 1)$.\n- $a_{32} = 2$: lies on the permutation $\\sigma=(3, 1, 2)$.\nSince every positive entry of $A$ is on a positive permutation, $A$ has total support. Therefore, a Sinkhorn-Knopp scaling exists.\n\n**Part 2: Computation of Scaling Matrices $D$ and $E$**\n\nWe seek positive diagonal matrices $D = \\mathrm{diag}(d_1, d_2, d_3)$ and $E = \\mathrm{diag}(e_1, e_2, e_3)$ such that $B = DAE$ is doubly stochastic. The matrix $B$ has entries $b_{ij} = d_i a_{ij} e_j$.\n$$\nB = \\begin{pmatrix} d_1 & 0 & 0 \\\\ 0 & d_2 & 0 \\\\ 0 & 0 & d_3 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 2 \\\\ 2 & 0 & 1 \\\\ 1 & 2 & 0 \\end{pmatrix} \\begin{pmatrix} e_1 & 0 & 0 \\\\ 0 & e_2 & 0 \\\\ 0 & 0 & e_3 \\end{pmatrix} = \\begin{pmatrix} 0 & d_1 e_2 & 2 d_1 e_3 \\\\ 2 d_2 e_1 & 0 & d_2 e_3 \\\\ d_3 e_1 & 2 d_3 e_2 & 0 \\end{pmatrix}\n$$\nThe conditions for $B$ to be doubly stochastic are that its row sums and column sums are all equal to $1$.\nRow sums:\n1. $d_1 e_2 + 2 d_1 e_3 = 1 \\implies d_1 (e_2 + 2 e_3) = 1$\n2. $2 d_2 e_1 + d_2 e_3 = 1 \\implies d_2 (2 e_1 + e_3) = 1$\n3. $d_3 e_1 + 2 d_3 e_2 = 1 \\implies d_3 (e_1 + 2 e_2) = 1$\nColumn sums:\n4. $2 d_2 e_1 + d_3 e_1 = 1 \\implies e_1 (2 d_2 + d_3) = 1$\n5. $d_1 e_2 + 2 d_3 e_2 = 1 \\implies e_2 (d_1 + 2 d_3) = 1$\n6. $2 d_1 e_3 + d_2 e_3 = 1 \\implies e_3 (2 d_1 + d_2) = 1$\n\nFrom equations (1) and (5), we have $d_1 (e_2 + 2 e_3) = e_2 (d_1 + 2 d_3)$, which simplifies to $d_1 e_3 = d_3 e_2$.\nFrom equations (2) and (6), we have $d_2 (2 e_1 + e_3) = e_3 (2 d_1 + d_2)$, which simplifies to $d_2 e_1 = d_1 e_3$.\nFrom equations (3) and (4), we have $d_3 (e_1 + 2 e_2) = e_1 (2 d_2 + d_3)$, which simplifies to $d_3 e_2 = d_2 e_1$.\nCombining these relations, we get $d_2 e_1 = d_3 e_2 = d_1 e_3$. The symmetry of matrix A (all row and column sums are 3) suggests a symmetric solution where $d_1=d_2=d_3=d$ and $e_1=e_2=e_3=e$.\nSubstituting $d_i=d$ and $e_j=e$ into any of the six original equations, e.g., equation (1), yields $d(e+2e) = 1$, which is $3de=1$.\nThus, any solution must have $D=dI$ and $E=eI$ where $I$ is the $3 \\times 3$ identity matrix and $d,e$ are positive scalars such that $de = 1/3$.\nFor an explicit pair, we can choose $d=e$. Then $d^2 = 1/3 \\implies d = 1/\\sqrt{3}$.\nSo, one possible pair of scaling matrices is $D = \\mathrm{diag}(1/\\sqrt{3}, 1/\\sqrt{3}, 1/\\sqrt{3})$ and $E = \\mathrm{diag}(1/\\sqrt{3}, 1/\\sqrt{3}, 1/\\sqrt{3})$.\n\n**Part 3: Uniqueness and Invariant Quantity**\n\nThe matrix $A$ has total support and is nonnegative, so it is fully indecomposable. Therefore, the scaling is unique up to a scalar factor. This means if $(D, E)$ is a solution, any other solution $(D', E')$ must be of the form $(tD, t^{-1}E)$ for some scalar $t>0$. Our derivation in Part 2 showed that all solutions are of the form $D=dI, E=eI$ with $de=1/3$, which is consistent with this property.\n\nWe are asked to find the value of the invariant quantity $\\det(D)\\det(E)$ under the transformation $(D, E) \\mapsto (tD, t^{-1}E)$.\nLet $D' = tD$ and $E' = t^{-1}E$.\nThe determinant of a scaled diagonal matrix is $\\det(tD) = \\det(\\mathrm{diag}(td_1, \\dots, td_n)) = t^n \\det(D)$.\nFor our $n=3$ case:\n$\\det(D') = \\det(tD) = t^3 \\det(D)$.\n$\\det(E') = \\det(t^{-1}E) = (t^{-1})^3 \\det(E) = t^{-3} \\det(E)$.\nThe product is $\\det(D')\\det(E') = (t^3 \\det(D))(t^{-3} \\det(E)) = \\det(D)\\det(E)$.\nThis confirms that the quantity $\\det(D)\\det(E)$ is indeed an invariant.\n\nTo compute its value, we can use any valid scaling pair. From Part 2, any valid scaling has $D=dI$ and $E=eI$ with $de = 1/3$.\n$\\det(D) = \\det(dI) = d^3$.\n$\\det(E) = \\det(eI) = e^3$.\nThe invariant product is $\\det(D)\\det(E) = d^3 e^3 = (de)^3$.\nSince $de = 1/3$, the value of the invariant is $(1/3)^3$.\n$$\n\\det(D)\\det(E) = \\left(\\frac{1}{3}\\right)^3 = \\frac{1}{27}\n$$\nThis value is independent of the specific choice of $t$, as it must be for an invariant.",
            "answer": "$$\\boxed{\\frac{1}{27}}$$"
        }
    ]
}