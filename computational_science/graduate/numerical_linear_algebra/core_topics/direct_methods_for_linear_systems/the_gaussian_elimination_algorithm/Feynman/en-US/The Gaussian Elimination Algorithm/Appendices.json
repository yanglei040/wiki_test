{
    "hands_on_practices": [
        {
            "introduction": "The stability of Gaussian elimination is not guaranteed, and in the absence of a proper pivoting strategy, computed solutions can be overwhelmed by round-off errors. This first practice problem  provides a hands-on demonstration of this phenomenon by analyzing the element growth factor, a key measure of stability. By explicitly comparing Gaussian elimination with and without partial pivoting on a specially constructed matrix, you will see firsthand how a simple row-swapping rule can prevent the catastrophic growth of intermediate values and preserve the accuracy of the final result.",
            "id": "3587432",
            "problem": "Consider the $5 \\times 5$ matrix $A \\in \\mathbb{R}^{5 \\times 5}$ whose entries are defined by\n$$\na_{ij} \\;=\\; \\begin{cases}\n\\varepsilon, & i=j,\\\\\n1, & i \\neq j,\n\\end{cases}\n$$\nwhere $0<\\varepsilon<1$ is a fixed parameter. Starting from the foundational definitions of the Gaussian Elimination (GE) algorithm and the rule for row operations, perform the following tasks:\n\n1. Execute GE without pivoting on $A$. At each elimination step $k=1,2,3,4$, explicitly identify the pivot at position $(k,k)$ and write the upper-triangular matrix $U^{(k)}$ after that step. Your $U^{(k)}$ must display the full $5 \\times 5$ structure at step $k=1$, and the embedded $(5-k) \\times (5-k)$ trailing submatrix structure for steps $k=2,3,4$, in closed form as functions of $\\varepsilon$. State the sequence of pivots $\\{p_{1}^{\\mathrm{np}},p_{2}^{\\mathrm{np}},p_{3}^{\\mathrm{np}},p_{4}^{\\mathrm{np}},p_{5}^{\\mathrm{np}}\\}$, where the superscript $\\mathrm{np}$ denotes “no pivoting”.\n\n2. Execute GE with partial pivoting on $A$. At each elimination step $k=1,2,3,4$, select the pivot in column $k$ according to the partial pivoting rule (swap the current pivot row with the row having the largest absolute entry in column $k$ among rows $k,k+1,\\dots,5$), then perform the elimination. Explicitly identify the pivot at position $(k,k)$ and write the upper-triangular matrix $U_{\\mathrm{pp}}^{(k)}$ after that step, in closed form as functions of $\\varepsilon$, following the same structural requirements as in part 1. State the sequence of pivots $\\{p_{1}^{\\mathrm{pp}},p_{2}^{\\mathrm{pp}},p_{3}^{\\mathrm{pp}},p_{4}^{\\mathrm{pp}},p_{5}^{\\mathrm{pp}}\\}$, where the superscript $\\mathrm{pp}$ denotes “partial pivoting”.\n\n3. Let the element growth factor for an algorithm be defined as\n$$\n\\rho \\;=\\; \\frac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |a_{ij}|},\n$$\nwhere $U$ is the final upper-triangular matrix produced by the algorithm and $A$ is the original input. Compute the ratio\n$$\nR \\;=\\; \\frac{\\rho_{\\mathrm{np}}}{\\rho_{\\mathrm{pp}}}\n$$\nfor the particular value $\\varepsilon=10^{-6}$. Round your final numerical answer for $R$ to four significant figures. Express the final answer with no units.",
            "solution": "The user wants me to solve a numerical linear algebra problem by performing Gaussian Elimination (GE) on a specific matrix, both with and without pivoting. Finally, I need to compute the ratio of element growth factors for a given parameter value.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- The matrix is $A \\in \\mathbb{R}^{5 \\times 5}$ with entries $a_{ij} = \\varepsilon$ for $i=j$ and $a_{ij} = 1$ for $i \\neq j$.\n- The parameter $\\varepsilon$ is fixed, with $0 < \\varepsilon < 1$.\n- **Part 1**: Perform GE without pivoting on $A$.\n    - For $k=1,2,3,4$: identify the pivot $p_k^{\\mathrm{np}}$ and write the matrix $U^{(k)}$ after step $k$.\n    - The structure of $U^{(k)}$ is specified: full $5 \\times 5$ for $k=1$, and highlighting the $(5-k) \\times (5-k)$ trailing submatrix for $k=2,3,4$.\n    - State the sequence of pivots $\\{p_{1}^{\\mathrm{np}}, p_{2}^{\\mathrm{np}}, p_{3}^{\\mathrm{np}}, p_{4}^{\\mathrm{np}}, p_{5}^{\\mathrm{np}}\\}$.\n- **Part 2**: Perform GE with partial pivoting on $A$.\n    - For $k=1,2,3,4$: select the pivot, perform the swap, perform elimination, identify the pivot $p_k^{\\mathrm{pp}}$, and write the matrix $U_{\\mathrm{pp}}^{(k)}$ after step $k$.\n    - The structural requirements for $U_{\\mathrm{pp}}^{(k)}$ are the same as in Part 1.\n    - State the sequence of pivots $\\{p_{1}^{\\mathrm{pp}}, p_{2}^{\\mathrm{pp}}, p_{3}^{\\mathrm{pp}}, p_{4}^{\\mathrm{pp}}, p_{5}^{\\mathrm{pp}}\\}$.\n- **Part 3**:\n    - The element growth factor is defined as $\\rho = \\frac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |a_{ij}|}$, where $U$ is the final upper-triangular matrix.\n    - Compute the ratio $R = \\frac{\\rho_{\\mathrm{np}}}{\\rho_{\\mathrm{pp}}}$ for $\\varepsilon = 10^{-6}$.\n    - Round the final numerical answer for $R$ to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is a standard exercise in numerical linear algebra, exploring the stability of Gaussian elimination. All concepts (Gaussian elimination, pivoting, growth factor) are well-established.\n- **Well-Posed**: The matrix $A$ is clearly defined. The algorithms are standard. The matrix $A$ can be shown to be non-singular for $0 < \\varepsilon < 1$. Its eigenvalues are $6-\\varepsilon$ and $1-\\varepsilon$ (with multiplicity $4$), which are both positive, hence the matrix is invertible and GE is well-defined.\n- **Objective**: The problem is stated using precise mathematical language, free of ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with the solution.\n\n### Solution\n\nThe initial matrix $A$ is:\n$$\nA = A^{(0)} =\n\\begin{pmatrix}\n\\varepsilon & 1 & 1 & 1 & 1 \\\\\n1 & \\varepsilon & 1 & 1 & 1 \\\\\n1 & 1 & \\varepsilon & 1 & 1 \\\\\n1 & 1 & 1 & \\varepsilon & 1 \\\\\n1 & 1 & 1 & 1 & \\varepsilon\n\\end{pmatrix}\n$$\n\n#### Part 1: Gaussian Elimination without Pivoting\n\nWe denote the matrix after step $k$ as $A^{(k)}$. The final upper-triangular matrix is $U = A^{(4)}$.\n\n**Step $k=1$**:\nThe pivot is $p_1^{\\mathrm{np}} = a_{11}^{(0)} = \\varepsilon$.\nThe multipliers are $m_{i1} = a_{i1}^{(0)}/a_{11}^{(0)} = 1/\\varepsilon$ for $i \\in \\{2, 3, 4, 5\\}$.\nThe new entries for $i,j > 1$ are $a_{ij}^{(1)} = a_{ij}^{(0)} - m_{i1} a_{1j}^{(0)}$.\n- For the diagonal elements ($i=j>1$): $a_{ii}^{(1)} = \\varepsilon - (1/\\varepsilon) \\cdot 1 = \\frac{\\varepsilon^2-1}{\\varepsilon}$.\n- For the off-diagonal elements ($i \\neq j$, $i,j > 1$): $a_{ij}^{(1)} = 1 - (1/\\varepsilon) \\cdot 1 = \\frac{\\varepsilon-1}{\\varepsilon}$.\nThe matrix after step $1$ is:\n$$\nU^{(1)} = A^{(1)} =\n\\begin{pmatrix}\n\\varepsilon & 1 & 1 & 1 & 1 \\\\\n0 & \\frac{\\varepsilon^2-1}{\\varepsilon} & \\frac{\\varepsilon-1}{\\varepsilon} & \\frac{\\varepsilon-1}{\\varepsilon} & \\frac{\\varepsilon-1}{\\varepsilon} \\\\\n0 & \\frac{\\varepsilon-1}{\\varepsilon} & \\frac{\\varepsilon^2-1}{\\varepsilon} & \\frac{\\varepsilon-1}{\\varepsilon} & \\frac{\\varepsilon-1}{\\varepsilon} \\\\\n0 & \\frac{\\varepsilon-1}{\\varepsilon} & \\frac{\\varepsilon-1}{\\varepsilon} & \\frac{\\varepsilon^2-1}{\\varepsilon} & \\frac{\\varepsilon-1}{\\varepsilon} \\\\\n0 & \\frac{\\varepsilon-1}{\\varepsilon} & \\frac{\\varepsilon-1}{\\varepsilon} & \\frac{\\varepsilon-1}{\\varepsilon} & \\frac{\\varepsilon^2-1}{\\varepsilon}\n\\end{pmatrix}\n$$\n\nLet's define a recursive relation for the elements of the trailing submatrices. At step $k$, let the trailing $(5-k) \\times (5-k)$ submatrix have diagonal elements $\\varepsilon_{k-1}$ and off-diagonal elements $d_{k-1}$.\nFor $k=1$: $\\varepsilon_0 = \\varepsilon$, $d_0=1$. This gives $\\varepsilon_1 = \\varepsilon_0 - d_0^2/\\varepsilon_0 = \\frac{\\varepsilon^2-1}{\\varepsilon}$ and $d_1 = d_0 - d_0^2/\\varepsilon_0 = \\frac{\\varepsilon-1}{\\varepsilon}$. This is incorrect. The update rule is $a_{ij}' = a_{ij} - \\frac{a_{i1}a_{1j}}{a_{11}}$. For $i,j > 1$, $a_{ij}' = a_{ij} - \\frac{1 \\cdot 1}{\\varepsilon}$. This gives $a_{ii}' = \\varepsilon-1/\\varepsilon$ and $a_{ij}'=1-1/\\varepsilon$, matching the matrix above.\n\nLet the trailing submatrix after step $k-1$ have diagonal entries $\\varepsilon_{k-1}$ and off-diagonal entries $d_{k-1}$.\nThe pivot is $p_k^{\\mathrm{np}} = \\varepsilon_{k-1}$. The multiplier is $m = d_{k-1}/\\varepsilon_{k-1}$.\nThe new elements for the next trailing submatrix are:\n$\\varepsilon_k = \\varepsilon_{k-1} - m \\cdot d_{k-1} = \\varepsilon_{k-1} - d_{k-1}^2/\\varepsilon_{k-1}$\n$d_k = d_{k-1} - m \\cdot d_{k-1} = d_{k-1}(1 - d_{k-1}/\\varepsilon_{k-1})$\nFrom $k=1$: $\\varepsilon_1 = \\frac{\\varepsilon^2-1}{\\varepsilon}$, $d_1 = \\frac{\\varepsilon-1}{\\varepsilon}$.\nThe multiplier for step 2 is $m_2 = d_1/\\varepsilon_1 = \\frac{(\\varepsilon-1)/\\varepsilon}{(\\varepsilon^2-1)/\\varepsilon} = \\frac{1}{\\varepsilon+1}$.\n\n**Step $k=2$**:\nThe pivot is $p_2^{\\mathrm{np}} = a_{22}^{(1)} = \\varepsilon_1 = \\frac{\\varepsilon^2-1}{\\varepsilon}$.\nUsing the recursion:\n$\\varepsilon_2 = \\varepsilon_1 - d_1^2/\\varepsilon_1 = \\frac{(\\varepsilon-1)(\\varepsilon+2)}{\\varepsilon+1}$.\n$d_2 = d_1(1-m_2) = \\frac{\\varepsilon-1}{\\varepsilon}(1-\\frac{1}{\\varepsilon+1}) = \\frac{\\varepsilon-1}{\\varepsilon+1}$.\nThe matrix after step 2 is:\n$$\nU^{(2)} = A^{(2)} =\n\\begin{pmatrix}\n\\varepsilon & 1 & 1 & 1 & 1 \\\\\n0 & \\varepsilon_1 & d_1 & d_1 & d_1 \\\\\n0 & 0 & \\varepsilon_2 & d_2 & d_2 \\\\\n0 & 0 & d_2 & \\varepsilon_2 & d_2 \\\\\n0 & 0 & d_2 & d_2 & \\varepsilon_2\n\\end{pmatrix}\n$$\nThe embedded $3 \\times 3$ trailing submatrix has diagonal entries $\\varepsilon_2 = \\frac{(\\varepsilon-1)(\\varepsilon+2)}{\\varepsilon+1}$ and off-diagonal entries $d_2 = \\frac{\\varepsilon-1}{\\varepsilon+1}$.\n\n**Step $k=3$**:\nThe pivot is $p_3^{\\mathrm{np}} = a_{33}^{(2)} = \\varepsilon_2 = \\frac{(\\varepsilon-1)(\\varepsilon+2)}{\\varepsilon+1}$.\nThe multiplier is $m_3 = d_2/\\varepsilon_2 = \\frac{1}{\\varepsilon+2}$.\n$\\varepsilon_3 = \\varepsilon_2 - d_2^2/\\varepsilon_2 = \\frac{(\\varepsilon-1)(\\varepsilon+3)}{\\varepsilon+2}$.\n$d_3 = d_2(1-m_3) = \\frac{\\varepsilon-1}{\\varepsilon+1}(1-\\frac{1}{\\varepsilon+2}) = \\frac{\\varepsilon-1}{\\varepsilon+2}$.\nThe matrix after step 3 is:\n$$\nU^{(3)} = A^{(3)} =\n\\begin{pmatrix}\n\\varepsilon & 1 & 1 & 1 & 1 \\\\\n0 & \\varepsilon_1 & d_1 & d_1 & d_1 \\\\\n0 & 0 & \\varepsilon_2 & d_2 & d_2 \\\\\n0 & 0 & 0 & \\varepsilon_3 & d_3 \\\\\n0 & 0 & 0 & d_3 & \\varepsilon_3\n\\end{pmatrix}\n$$\nThe embedded $2 \\times 2$ trailing submatrix has diagonal entries $\\varepsilon_3 = \\frac{(\\varepsilon-1)(\\varepsilon+3)}{\\varepsilon+2}$ and off-diagonal entry $d_3 = \\frac{\\varepsilon-1}{\\varepsilon+2}$.\n\n**Step $k=4$**:\nThe pivot is $p_4^{\\mathrm{np}} = a_{44}^{(3)} = \\varepsilon_3 = \\frac{(\\varepsilon-1)(\\varepsilon+3)}{\\varepsilon+2}$.\nThe multiplier is $m_4 = d_3/\\varepsilon_3 = \\frac{1}{\\varepsilon+3}$.\n$\\varepsilon_4 = \\varepsilon_3 - d_3^2/\\varepsilon_3 = \\frac{(\\varepsilon-1)(\\varepsilon+4)}{\\varepsilon+3}$. There is no $d_4$.\nThe matrix after step 4 (the final upper-triangular matrix $U_{\\mathrm{np}}$) is:\n$$\nU^{(4)} = U_{\\mathrm{np}} =\n\\begin{pmatrix}\n\\varepsilon & 1 & 1 & 1 & 1 \\\\\n0 & \\varepsilon_1 & d_1 & d_1 & d_1 \\\\\n0 & 0 & \\varepsilon_2 & d_2 & d_2 \\\\\n0 & 0 & 0 & \\varepsilon_3 & d_3 \\\\\n0 & 0 & 0 & 0 & \\varepsilon_4\n\\end{pmatrix}\n  \\quad\n  \\begin{smallmatrix}\n    \\varepsilon_1 = \\frac{\\varepsilon^2-1}{\\varepsilon} & d_1 = \\frac{\\varepsilon-1}{\\varepsilon} \\\\\n    \\varepsilon_2 = \\frac{(\\varepsilon-1)(\\varepsilon+2)}{\\varepsilon+1} & d_2 = \\frac{\\varepsilon-1}{\\varepsilon+1} \\\\\n    \\varepsilon_3 = \\frac{(\\varepsilon-1)(\\varepsilon+3)}{\\varepsilon+2} & d_3 = \\frac{\\varepsilon-1}{\\varepsilon+2} \\\\\n    \\varepsilon_4 = \\frac{(\\varepsilon-1)(\\varepsilon+4)}{\\varepsilon+3} &\n  \\end{smallmatrix}\n$$\nThe embedded $1 \\times 1$ trailing submatrix is just the element $a_{55}^{(4)} = \\varepsilon_4 = \\frac{(\\varepsilon-1)(\\varepsilon+4)}{\\varepsilon+3}$.\n\n**Pivot Sequence (No Pivoting)**:\nThe pivots are $p_k^{\\mathrm{np}} = \\varepsilon_{k-1}$ for $k=1, \\dots, 5$.\n$p_1^{\\mathrm{np}} = \\varepsilon_0 = \\varepsilon$\n$p_2^{\\mathrm{np}} = \\varepsilon_1 = \\frac{\\varepsilon^2-1}{\\varepsilon}$\n$p_3^{\\mathrm{np}} = \\varepsilon_2 = \\frac{(\\varepsilon-1)(\\varepsilon+2)}{\\varepsilon+1}$\n$p_4^{\\mathrm{np}} = \\varepsilon_3 = \\frac{(\\varepsilon-1)(\\varepsilon+3)}{\\varepsilon+2}$\n$p_5^{\\mathrm{np}} = \\varepsilon_4 = \\frac{(\\varepsilon-1)(\\varepsilon+4)}{\\varepsilon+3}$\n\n#### Part 2: Gaussian Elimination with Partial Pivoting\n\n**Step $k=1$**:\nIn column 1 of $A$, the entries are $(\\varepsilon, 1, 1, 1, 1)^T$. Since $0 < \\varepsilon < 1$, the largest absolute value is $1$. We swap row 1 with any of rows 2, 3, 4, or 5. Let's swap $R_1 \\leftrightarrow R_2$.\n$$\nA^{(0)'} =\n\\begin{pmatrix}\n1 & \\varepsilon & 1 & 1 & 1 \\\\\n\\varepsilon & 1 & 1 & 1 & 1 \\\\\n1 & 1 & \\varepsilon & 1 & 1 \\\\\n1 & 1 & 1 & \\varepsilon & 1 \\\\\n1 & 1 & 1 & 1 & \\varepsilon\n\\end{pmatrix}\n$$\nThe pivot is $p_1^{\\mathrm{pp}} = 1$. Multipliers are $m_{21}=\\varepsilon$ and $m_{i1}=1$ for $i \\in \\{3,4,5\\}$.\nAfter elimination:\n$$\nU_{\\mathrm{pp}}^{(1)} = A_{\\mathrm{pp}}^{(1)} =\n\\begin{pmatrix}\n1 & \\varepsilon & 1 & 1 & 1 \\\\\n0 & 1-\\varepsilon^2 & 1-\\varepsilon & 1-\\varepsilon & 1-\\varepsilon \\\\\n0 & 1-\\varepsilon & \\varepsilon-1 & 0 & 0 \\\\\n0 & 1-\\varepsilon & 0 & \\varepsilon-1 & 0 \\\\\n0 & 1-\\varepsilon & 0 & 0 & \\varepsilon-1\n\\end{pmatrix}\n$$\n\n**Step $k=2$**:\nIn column 2, from row 2 down, the entries are $(1-\\varepsilon^2, 1-\\varepsilon, 1-\\varepsilon, 1-\\varepsilon)^T$.\n$|1-\\varepsilon^2| = (1-\\varepsilon)(1+\\varepsilon)$ and $|1-\\varepsilon|=1-\\varepsilon$.\nSince $1+\\varepsilon>1$, $|1-\\varepsilon^2| > |1-\\varepsilon|$. The pivot is $p_2^{\\mathrm{pp}} = 1-\\varepsilon^2$. No row swap is needed.\nMultipliers: $m_{i2} = \\frac{1-\\varepsilon}{1-\\varepsilon^2} = \\frac{1}{1+\\varepsilon}$ for $i \\in \\{3,4,5\\}$. After elimination on the trailing $4\\times 4$ block (with $R'_i \\leftarrow R'_i - m_{i2} R'_2$):\nThe a new trailing $3 \\times 3$ submatrix has diagonal entries $\\varepsilon_2' = (\\varepsilon-1) - \\frac{1}{1+\\varepsilon}(1-\\varepsilon) = -(1-\\varepsilon)(1+\\frac{1}{1+\\varepsilon}) = -(1-\\varepsilon)\\frac{2+\\varepsilon}{1+\\varepsilon}$.\nThe new off-diagonal entries are $d_2' = 0 - \\frac{1}{1+\\varepsilon}(1-\\varepsilon) = -\\frac{1-\\varepsilon}{1+\\varepsilon}$.\n$$\nU_{\\mathrm{pp}}^{(2)} = A_{\\mathrm{pp}}^{(2)} =\n\\begin{pmatrix}\n1 & \\varepsilon & 1 & 1 & 1 \\\\\n0 & 1-\\varepsilon^2 & 1-\\varepsilon & 1-\\varepsilon & 1-\\varepsilon \\\\\n0 & 0 & \\varepsilon_2' & d_2' & d_2' \\\\\n0 & 0 & d_2' & \\varepsilon_2' & d_2' \\\\\n0 & 0 & d_2' & d_2' & \\varepsilon_2'\n\\end{pmatrix}\n$$\nThe embedded $3 \\times 3$ submatrix has diagonal $\\varepsilon_2' = -(1-\\varepsilon)\\frac{2+\\varepsilon}{1+\\varepsilon}$ and off-diagonal $d_2' = -\\frac{1-\\varepsilon}{1+\\varepsilon}$.\n\n**Step $k=3$**:\nIn column 3, from row 3 down, entries are $(\\varepsilon_2', d_2', d_2')^T$.\n$|\\varepsilon_2'| = (1-\\varepsilon)\\frac{2+\\varepsilon}{1+\\varepsilon}$ and $|d_2'|=\\frac{1-\\varepsilon}{1+\\varepsilon}$. Since $2+\\varepsilon > 1$, $|\\varepsilon_2'| > |d_2'|$.\nThe pivot is $p_3^{\\mathrm{pp}} = \\varepsilon_2'$. No row swap.\nMultiplier $m = d_2'/\\varepsilon_2' = \\frac{1}{2+\\varepsilon}$.\nThe new trailing $2 \\times 2$ submatrix elements are:\n$\\varepsilon_3' = \\varepsilon_2' - m d_2' = \\varepsilon_2' (1 - m^2 (\\varepsilon_2'/d_2') )= \\dots = -(1-\\varepsilon)\\frac{3+\\varepsilon}{2+\\varepsilon}$.\n$d_3' = d_2' - m d_2' = d_2'(1-m) = \\dots = -\\frac{1-\\varepsilon}{2+\\varepsilon}$.\n$$\nU_{\\mathrm{pp}}^{(3)} = A_{\\mathrm{pp}}^{(3)} =\n\\begin{pmatrix}\n1 & \\varepsilon & 1 & 1 & 1 \\\\\n0 & 1-\\varepsilon^2 & 1-\\varepsilon & 1-\\varepsilon & 1-\\varepsilon \\\\\n0 & 0 & \\varepsilon_2' & d_2' & d_2' \\\\\n0 & 0 & 0 & \\varepsilon_3' & d_3' \\\\\n0 & 0 & 0 & d_3' & \\varepsilon_3'\n\\end{pmatrix}\n$$\nThe embedded $2 \\times 2$ submatrix has diagonal $\\varepsilon_3' = -(1-\\varepsilon)\\frac{3+\\varepsilon}{2+\\varepsilon}$ and off-diagonal $d_3' = -\\frac{1-\\varepsilon}{2+\\varepsilon}$.\n\n**Step $k=4$**:\nPivot is $p_4^{\\mathrm{pp}}=\\varepsilon_3'$. No swap. Multiplier $m=d_3'/\\varepsilon_3'=\\frac{1}{3+\\varepsilon}$.\nNew element $\\varepsilon_4' = \\varepsilon_3' - m d_3' = \\dots = -(1-\\varepsilon)\\frac{4+\\varepsilon}{3+\\varepsilon}$.\nThe final matrix is:\n$$\nU_{\\mathrm{pp}}^{(4)} = U_{\\mathrm{pp}} =\n\\begin{pmatrix}\n1 & \\varepsilon & 1 & 1 & 1 \\\\\n0 & 1-\\varepsilon^2 & 1-\\varepsilon & 1-\\varepsilon & 1-\\varepsilon \\\\\n0 & 0 & \\varepsilon_2' & d_2' & d_2' \\\\\n0 & 0 & 0 & \\varepsilon_3' & d_3' \\\\\n0 & 0 & 0 & 0 & \\varepsilon_4'\n\\end{pmatrix}\n  \\quad\n  \\begin{smallmatrix}\n    \\varepsilon_2' = - (1-\\varepsilon)\\frac{2+\\varepsilon}{1+\\varepsilon} & d_2' = - \\frac{1-\\varepsilon}{1+\\varepsilon} \\\\\n    \\varepsilon_3' = - (1-\\varepsilon)\\frac{3+\\varepsilon}{2+\\varepsilon} & d_3' = - \\frac{1-\\varepsilon}{2+\\varepsilon} \\\\\n    \\varepsilon_4' = - (1-\\varepsilon)\\frac{4+\\varepsilon}{3+\\varepsilon} &\n  \\end{smallmatrix}\n$$\nThe $1\\times 1$ trailing submatrix is $\\varepsilon_4'$.\n\n**Pivot Sequence (Partial Pivoting)**:\n$p_1^{\\mathrm{pp}} = 1$\n$p_2^{\\mathrm{pp}} = 1-\\varepsilon^2$\n$p_3^{\\mathrm{pp}} = \\varepsilon_2' = -(1-\\varepsilon)\\frac{2+\\varepsilon}{1+\\varepsilon}$\n$p_4^{\\mathrm{pp}} = \\varepsilon_3' = -(1-\\varepsilon)\\frac{3+\\varepsilon}{2+\\varepsilon}$\n$p_5^{\\mathrm{pp}} = \\varepsilon_4' = -(1-\\varepsilon)\\frac{4+\\varepsilon}{3+\\varepsilon}$\n\n#### Part 3: Growth Factor Ratio\n\nThe growth factor is $\\rho = \\frac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |a_{ij}|}$.\nFor matrix $A$, the entries are $\\varepsilon$ and $1$. Since $0 < \\varepsilon < 1$, $\\max_{i,j} |a_{ij}| = 1$.\nSo, $\\rho = \\max_{i,j} |u_{ij}|$.\n\n**Growth Factor without Pivoting ($\\rho_{\\mathrm{np}}$)**:\nWe need to find the element of largest magnitude in $U_{\\mathrm{np}}$. For small $\\varepsilon$, we inspect the terms:\n$|\\varepsilon| \\ll 1$. $|1|=1$.\n$|\\varepsilon_1| = |\\frac{\\varepsilon^2-1}{\\varepsilon}| = \\frac{1-\\varepsilon^2}{\\varepsilon} \\approx 1/\\varepsilon$.\n$|d_1| = |\\frac{\\varepsilon-1}{\\varepsilon}| = \\frac{1-\\varepsilon}{\\varepsilon} \\approx 1/\\varepsilon$.\n$|\\varepsilon_2| = | \\frac{(\\varepsilon-1)(\\varepsilon+2)}{\\varepsilon+1} | = \\frac{(1-\\varepsilon)(2+\\varepsilon)}{1+\\varepsilon} \\approx 2$.\nThe largest magnitude is clearly from the second row, specifically $|\\varepsilon_1| = \\frac{1-\\varepsilon^2}{\\varepsilon}$ since $1-\\varepsilon^2 > 1-\\varepsilon$.\n$\\rho_{\\mathrm{np}} = \\max_{i,j} |(U_{\\mathrm{np}})_{ij}| = |\\varepsilon_1| = \\frac{1-\\varepsilon^2}{\\varepsilon}$.\n\n**Growth Factor with Partial Pivoting ($\\rho_{\\mathrm{pp}}$)**:\nWe need to find the element of largest magnitude in $U_{\\mathrm{pp}}$.\nRow 1 elements have max magnitude $1$.\nRow 2 elements have max magnitude $1-\\varepsilon^2 < 1$.\nThe other elements are functions of $\\varepsilon_k'$ and $d_k'$. For small $\\varepsilon$:\n$|\\varepsilon_2'| = (1-\\varepsilon)\\frac{2+\\varepsilon}{1+\\varepsilon} \\approx 2$.\n$|d_2'| = \\frac{1-\\varepsilon}{1+\\varepsilon} \\approx 1$.\n$|\\varepsilon_3'| = (1-\\varepsilon)\\frac{3+\\varepsilon}{2+\\varepsilon} \\approx 3/2 = 1.5$.\n$|d_3'| = \\frac{1-\\varepsilon}{2+\\varepsilon} \\approx 1/2$.\n$|\\varepsilon_4'| = (1-\\varepsilon)\\frac{4+\\varepsilon}{3+\\varepsilon} \\approx 4/3 \\approx 1.33$.\nThe maximum magnitude is $|\\varepsilon_2'|$.\n$\\rho_{\\mathrm{pp}} = \\max_{i,j} |(U_{\\mathrm{pp}})_{ij}| = |\\varepsilon_2'| = (1-\\varepsilon)\\frac{2+\\varepsilon}{1+\\varepsilon}$.\n\n**Ratio $R$**:\n$$\nR = \\frac{\\rho_{\\mathrm{np}}}{\\rho_{\\mathrm{pp}}} = \\frac{\\frac{1-\\varepsilon^2}{\\varepsilon}}{(1-\\varepsilon)\\frac{2+\\varepsilon}{1+\\varepsilon}} = \\frac{\\frac{(1-\\varepsilon)(1+\\varepsilon)}{\\varepsilon}}{(1-\\varepsilon)\\frac{2+\\varepsilon}{1+\\varepsilon}} = \\frac{\\frac{1+\\varepsilon}{\\varepsilon}}{\\frac{2+\\varepsilon}{1+\\varepsilon}} = \\frac{(1+\\varepsilon)^2}{\\varepsilon(2+\\varepsilon)}\n$$\nNow, substitute $\\varepsilon=10^{-6}$:\n$$\nR = \\frac{(1+10^{-6})^2}{10^{-6}(2+10^{-6})} = \\frac{1.000001^2}{10^{-6} \\times 2.000001} = \\frac{1.000002000001}{2.000001 \\times 10^{-6}}\n$$\n$R \\approx 500000.750000125$.\nRounding to four significant figures gives $5.000 \\times 10^5$.",
            "answer": "$$\\boxed{5.000 \\times 10^{5}}$$"
        },
        {
            "introduction": "While partial pivoting is the standard for its balance of stability and efficiency, it is not foolproof. This exercise  explores a scenario where partial pivoting fails to proceed, even though the matrix is not identically zero. By contrasting this with the complete pivoting strategy, you will gain a deeper appreciation for the different levels of robustness offered by various pivoting rules and understand the specific matrix structures that can challenge the standard algorithm.",
            "id": "3587391",
            "problem": "Consider Gaussian elimination on a square matrix with pivoting. In partial pivoting, at step $k$ one searches only within rows $k$ through $n$ of column $k$ for a pivot of maximal absolute value and (if necessary) swaps rows to move it to position $(k,k)$. In complete pivoting, at step $k$ one searches within the entire trailing submatrix consisting of rows $k$ through $n$ and columns $k$ through $n$ for the entry of maximal absolute value and swaps both rows and columns to move it to $(k,k)$. Both strategies then proceed by left-multiplying by a unit lower triangular elimination matrix to zero out entries below the pivot and continue recursively on the trailing submatrix. All row and column swaps can be represented by multiplication with permutation matrices.\n\nLet\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n1 & 2 & 4 \\\\\n-1 & -2 & 0\n\\end{pmatrix}.\n$$\n\nWork in exact arithmetic over the real numbers.\n\n(a) Apply one step of Gaussian elimination with partial pivoting at $k=1$, using the tie-breaking rule that among entries of equal maximal absolute value in the search set one chooses the smallest row index as the pivot (so that at $k=1$ the pivot is selected from column $1$ without swapping rows). Derive the trailing submatrix after this first step and, using only the core definitions stated above, explain why at $k=2$ partial pivoting fails to find a nonzero pivot within column $2$ (that is, no row swap within column $2$ can expose a nonzero pivot).\n\n(b) Now repeat from the original matrix $A$ using complete pivoting. At $k=1$, select the pivot as the entry of maximal absolute value in the entire matrix, perform the required row and column swaps to bring it to position $(1,1)$, and carry out the first elimination step to form the trailing submatrix. Then, at $k=2$, identify the pivot selected by complete pivoting (the entry of maximal absolute value in the trailing submatrix) and state its numerical value.\n\nYou must provide as your final answer the scalar value of the pivot selected at step $k=2$ under complete pivoting. Express the result exactly; no rounding is required.",
            "solution": "The problem requires an analysis of Gaussian elimination with two different pivoting strategies, partial and complete, applied to a given matrix $A$. The arithmetic is exact.\n\nPart (a): Gaussian Elimination with Partial Pivoting\n\nWe are given the matrix\n$$\nA = A^{(1)} =\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n1 & 2 & 4 \\\\\n-1 & -2 & 0\n\\end{pmatrix}.\n$$\nThe first step of the algorithm corresponds to $k=1$. For partial pivoting, we must identify the pivot by searching for the entry of maximal absolute value in the first column, restricted to rows $1$ through $3$. The entries in this search set are $A_{11}^{(1)} = 1$, $A_{21}^{(1)} = 1$, and $A_{31}^{(1)} = -1$. The absolute values are $|1| = 1$, $|1| = 1$, and $|-1| = 1$. The maximal absolute value is therefore $1$.\n\nThe problem states a tie-breaking rule: \"among entries of equal maximal absolute value in the search set one chooses the smallest row index as the pivot\". The entry $A_{11}^{(1)} = 1$ in row $1$ satisfies this condition. Thus, the pivot is $1$, and no row swap is performed.\n\nWe now proceed with the elimination step to introduce zeros below the pivot $A_{11}^{(1)}$. The multipliers for row reduction are:\n$$\nm_{21} = \\frac{A_{21}^{(1)}}{A_{11}^{(1)}} = \\frac{1}{1} = 1\n$$\n$$\nm_{31} = \\frac{A_{31}^{(1)}}{A_{11}^{(1)}} = \\frac{-1}{1} = -1\n$$\nThe row operations are $R_2 \\to R_2 - m_{21} R_1$ and $R_3 \\to R_3 - m_{31} R_1$. Applying these to $A^{(1)}$ yields the matrix for the next stage, $A^{(2)}$:\n$$\nA^{(2)} =\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n1 - 1(1) & 2 - 1(2) & 4 - 1(3) \\\\\n-1 - (-1)(1) & -2 - (-1)(2) & 0 - (-1)(3)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 3\n\\end{pmatrix}.\n$$\nThe trailing submatrix, which is the subject of the next elimination step ($k=2$), is the lower-right $2 \\times 2$ block of $A^{(2)}$:\n$$\n\\begin{pmatrix}\n0 & 1 \\\\\n0 & 3\n\\end{pmatrix}.\n$$\nAt step $k=2$, the partial pivoting strategy, as defined in the problem, dictates that we \"search... only within rows $k$ through $n$ of column $k$ for a pivot of maximal absolute value\". For $k=2$ and $n=3$, this means searching column $2$ of $A^{(2)}$ from row $2$ to row $3$. The candidate entries are $A_{22}^{(2)} = 0$ and $A_{32}^{(2)} = 0$.\n\nThe set of candidate pivots is $\\{0, 0\\}$. The maximal absolute value among these is $0$. Gaussian elimination fundamentally requires a non-zero pivot to compute the multipliers for the elimination step. Since all available candidates in the search column are zero, no row swap can place a non-zero element at the pivot position $(2,2)$. Therefore, the algorithm cannot proceed and is said to fail.\n\nPart (b): Gaussian Elimination with Complete Pivoting\n\nWe begin again with the original matrix:\n$$\nA = A^{(1)} =\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n1 & 2 & 4 \\\\\n-1 & -2 & 0\n\\end{pmatrix}.\n$$\nAt step $k=1$, complete pivoting requires searching the entire $3 \\times 3$ matrix for the entry with the maximal absolute value. The matrix of absolute values of the entries of $A$ is:\n$$\n\\begin{pmatrix}\n|1| & |2| & |3| \\\\\n|1| & |2| & |4| \\\\\n|-1| & |-2| & |0|\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n1 & 2 & 4 \\\\\n1 & 2 & 0\n\\end{pmatrix}.\n$$\nThe largest value is $4$, which corresponds to the entry $A_{23} = 4$. To make this the pivot, we must move it to the $(1,1)$ position through row and column swaps. We swap row $1$ and row $2$ ($R_1 \\leftrightarrow R_2$), followed by swapping column $1$ and column $3$ ($C_1 \\leftrightarrow C_3$).\n$$\nA \\xrightarrow{R_1 \\leftrightarrow R_2}\n\\begin{pmatrix}\n1 & 2 & 4 \\\\\n1 & 2 & 3 \\\\\n-1 & -2 & 0\n\\end{pmatrix}\n\\xrightarrow{C_1 \\leftrightarrow C_3}\n\\begin{pmatrix}\n4 & 2 & 1 \\\\\n3 & 2 & 1 \\\\\n0 & -2 & -1\n\\end{pmatrix}.\n$$\nLet this transformed matrix be $A'$. The pivot is $A'_{11} = 4$. We perform elimination. The multipliers are:\n$$\nm_{21} = \\frac{A'_{21}}{A'_{11}} = \\frac{3}{4}\n$$\n$$\nm_{31} = \\frac{A'_{31}}{A'_{11}} = \\frac{0}{4} = 0\n$$\nApplying the row operations $R_2 \\to R_2 - \\frac{3}{4}R_1$ and $R_3 \\to R_3 - 0 R_1$ to $A'$ gives:\n$$\nA^{(2)} =\n\\begin{pmatrix}\n4 & 2 & 1 \\\\\n3-\\frac{3}{4}(4) & 2-\\frac{3}{4}(2) & 1-\\frac{3}{4}(1) \\\\\n0 & -2 & -1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4 & 2 & 1 \\\\\n0 & \\frac{1}{2} & \\frac{1}{4} \\\\\n0 & -2 & -1\n\\end{pmatrix}.\n$$\nFor step $k=2$, the complete pivoting strategy requires us to find the entry with the maximal absolute value in the trailing $2 \\times 2$ submatrix:\n$$\n\\begin{pmatrix}\n\\frac{1}{2} & \\frac{1}{4} \\\\\n-2 & -1\n\\end{pmatrix}.\n$$\nThe absolute values of these entries are $|\\frac{1}{2}| = \\frac{1}{2}$, $|\\frac{1}{4}| = \\frac{1}{4}$, $|-2| = 2$, and $|-1| = 1$. The maximal absolute value is $2$, which corresponds to the entry $-2$.\n\nTherefore, the pivot selected at step $k=2$ under complete pivoting is $-2$.",
            "answer": "$$\\boxed{-2}$$"
        },
        {
            "introduction": "A pivotal goal in numerical linear algebra is not just to solve a system, but to understand the intrinsic properties of its matrix, such as its numerical rank. This final practice  delves into this advanced topic, demonstrating how standard LU factorization with partial pivoting can produce a misleadingly well-conditioned triangular factor for a nearly singular matrix. By employing a column permutation to create a rank-revealing LU factorization, you will see how strategic pivoting can expose the true numerical nature of the matrix, a concept with profound implications for solving ill-conditioned problems.",
            "id": "3587374",
            "problem": "Consider the square matrix $A \\in \\mathbb{R}^{2 \\times 2}$ defined by\n$$\nA = \\begin{pmatrix}\n1 & M \\\\\n0 & 1\n\\end{pmatrix},\n$$\nwhere $M$ is a positive real parameter. Let the numerical tolerance be $\\tau = 5 \\times 10^{-9}$. For this problem, use the following three definitions of rank within the stated tolerance:\n- The gold-standard numerical rank $r_{\\mathrm{true}}$ is the number of singular values of $A$ whose magnitudes exceed $\\tau$.\n- The rank estimated by standard Lower-Upper (LU) factorization with partial pivoting (row pivoting only), denoted $r_{\\mathrm{PP}}$, is the number of diagonal entries of the upper-triangular factor $U$ whose magnitudes exceed $\\tau$.\n- The rank estimated by rank-revealing LU with column pivoting (that is, LU with both row and column permutations chosen to expose near-linear dependencies), denoted $r_{\\mathrm{RR}}$, is the number of diagonal entries of the corresponding upper-triangular factor $U$ whose magnitudes exceed $\\tau$ after performing a column permutation that maximizes exposure of small trailing pivots.\n\nStarting from fundamental definitions and well-tested facts in numerical linear algebra (the model of singular values as square roots of eigenvalues of $A^{\\top}A$, the definition and mechanics of Gaussian elimination, and the role of pivoting), do the following:\n1. For the specific choice $M = 10^{10}$, determine $r_{\\mathrm{true}}$ by analyzing the singular values of $A$.\n2. Determine $r_{\\mathrm{PP}}$ by analyzing the diagonal structure of the upper-triangular factor $U$ obtained by standard LU with partial pivoting applied to $A$.\n3. Determine $r_{\\mathrm{RR}}$ by demonstrating how a single column permutation makes $A$ rank-revealing under LU and then analyzing the resulting $U$.\n4. Compute the scalar quantity $\\Delta = r_{\\mathrm{PP}} - r_{\\mathrm{RR}}$.\n\nYour final answer must be the single real number $\\Delta$. No rounding is required for the final answer. Explain what the failure is in step $2$ and why the remedy in step $3$ succeeds, strictly deriving each conclusion from the fundamental base (definitions and algorithmic mechanics) without shortcut formulas. The matrix $A$ and the tolerance $\\tau$ are scientifically realistic for investigating the rank-revealing properties of pivoting strategies in Gaussian elimination.",
            "solution": "The user has provided a valid problem statement from the field of numerical linear algebra. The problem is scientifically grounded, well-posed, and objective. It requires an analysis of different rank estimation methods based on matrix factorization for a specific, well-chosen example matrix. I will now proceed with the solution.\n\nThe problem asks for an analysis of the numerical rank of the matrix $A = \\begin{pmatrix} 1 & M \\\\ 0 & 1 \\end{pmatrix}$ for $M=10^{10}$, using a tolerance of $\\tau = 5 \\times 10^{-9}$. We must determine the numerical rank using three different definitions and explain the discrepancies.\n\n**1. Determination of the True Numerical Rank, $r_{\\mathrm{true}}$**\n\nThe gold-standard numerical rank, $r_{\\mathrm{true}}$, is defined as the number of singular values of $A$ whose magnitudes exceed the tolerance $\\tau$. The singular values of $A$ are the square roots of the eigenvalues of the matrix $A^\\top A$.\n\nFirst, we compute $A^\\top A$:\n$$\nA^\\top = \\begin{pmatrix} 1 & 0 \\\\ M & 1 \\end{pmatrix}\n$$\n$$\nA^\\top A = \\begin{pmatrix} 1 & 0 \\\\ M & 1 \\end{pmatrix} \\begin{pmatrix} 1 & M \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 0 & 1 \\cdot M + 0 \\cdot 1 \\\\ M \\cdot 1 + 1 \\cdot 0 & M \\cdot M + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 & M \\\\ M & M^2+1 \\end{pmatrix}\n$$\nNext, we find the eigenvalues $\\lambda$ of $A^\\top A$ by solving the characteristic equation $\\det(A^\\top A - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 1-\\lambda & M \\\\ M & M^2+1-\\lambda \\end{pmatrix} = 0\n$$\n$$\n(1-\\lambda)(M^2+1-\\lambda) - M^2 = 0\n$$\n$$\nM^2+1 - \\lambda - (M^2+1)\\lambda + \\lambda^2 - M^2 = 0\n$$\n$$\n\\lambda^2 - (M^2+2)\\lambda + 1 = 0\n$$\nUsing the quadratic formula, the eigenvalues are:\n$$\n\\lambda = \\frac{(M^2+2) \\pm \\sqrt{(M^2+2)^2 - 4(1)(1)}}{2} = \\frac{M^2+2 \\pm \\sqrt{M^4+4M^2+4-4}}{2} = \\frac{M^2+2 \\pm \\sqrt{M^2(M^2+4)}}{2}\n$$\n$$\n\\lambda = \\frac{M^2+2 \\pm M\\sqrt{M^2+4}}{2}\n$$\nThe two eigenvalues are $\\lambda_1 = \\frac{M^2+2 + M\\sqrt{M^2+4}}{2}$ and $\\lambda_2 = \\frac{M^2+2 - M\\sqrt{M^2+4}}{2}$.\nThe singular values are $\\sigma_1 = \\sqrt{\\lambda_1}$ and $\\sigma_2 = \\sqrt{\\lambda_2}$.\n\nFor the given large value of $M = 10^{10}$, we can approximate the singular values. For $\\lambda_1$, the term $M^2$ dominates:\n$$\n\\lambda_1 \\approx \\frac{M^2 + M\\sqrt{M^2}}{2} = \\frac{M^2 + M^2}{2} = M^2\n$$\nSo, $\\sigma_1 \\approx \\sqrt{M^2} = M = 10^{10}$.\n\nFor $\\lambda_2$, the terms nearly cancel, so we must be more careful. We can rationalize the expression for $\\lambda_2$:\n$$\n\\lambda_2 = \\frac{(M^2+2 - M\\sqrt{M^2+4})}{2} \\times \\frac{(M^2+2 + M\\sqrt{M^2+4})}{(M^2+2 + M\\sqrt{M^2+4})} = \\frac{(M^2+2)^2 - M^2(M^2+4)}{2(M^2+2 + M\\sqrt{M^2+4})}\n$$\n$$\n\\lambda_2 = \\frac{M^4+4M^2+4 - (M^4+4M^2)}{2(M^2+2 + M\\sqrt{M^2+4})} = \\frac{4}{2(M^2+2 + M\\sqrt{M^2+4})} = \\frac{2}{M^2+2+M\\sqrt{M^2+4}}\n$$\nFor large $M$, the denominator is approximately $M^2 + M(M) = 2M^2$.\n$$\n\\lambda_2 \\approx \\frac{2}{2M^2} = \\frac{1}{M^2}\n$$\nSo, $\\sigma_2 \\approx \\sqrt{1/M^2} = 1/M = 10^{-10}$.\n\nNow we compare the magnitudes of the singular values with the tolerance $\\tau = 5 \\times 10^{-9}$:\n- $\\sigma_1 \\approx 10^{10}$. Clearly, $10^{10} > 5 \\times 10^{-9}$.\n- $\\sigma_2 \\approx 10^{-10}$. Since $1 \\times 10^{-10} < 50 \\times 10^{-10} = 5 \\times 10^{-9}$, this singular value is below the tolerance.\n\nOnly one singular value exceeds the tolerance. Therefore, the true numerical rank is $r_{\\mathrm{true}} = 1$.\n\n**2. Determination of Rank from LU with Partial Pivoting, $r_{\\mathrm{PP}}$**\n\nWe apply Gaussian elimination with partial pivoting (row swapping) to $A = \\begin{pmatrix} 1 & M \\\\ 0 & 1 \\end{pmatrix}$.\nIn the first column, we select the pivot by finding the element with the largest absolute value. The elements are $a_{11}=1$ and $a_{21}=0$. The maximum absolute value is $|1|=1$. Thus, no row swap is necessary. The pivot is $a_{11}=1$.\nThe elimination step aims to create a zero in the $a_{21}$ position. However, this element is already $0$. Therefore, no elimination operations are needed.\nThe algorithm terminates immediately, yielding the upper triangular matrix $U$ which is identical to the original matrix $A$:\n$$\nU = \\begin{pmatrix} 1 & M \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe corresponding lower triangular matrix $L$ is the identity matrix $I$.\nThe rank $r_{\\mathrm{PP}}$ is the number of diagonal entries of $U$ whose magnitudes exceed $\\tau = 5 \\times 10^{-9}$. The diagonal entries are $u_{11}=1$ and $u_{22}=1$.\n- $|u_{11}| = 1 > 5 \\times 10^{-9}$.\n- $|u_{22}| = 1 > 5 \\times 10^{-9}$.\nBoth diagonal entries are greater than the tolerance. Therefore, the rank estimated by this method is $r_{\\mathrm{PP}} = 2$.\n\nThe failure of this method arises because partial pivoting only considers elements within the current column for pivot selection. For the matrix $A$, the pivot choice is $a_{11}=1$, which is perfectly acceptable from the perspective of numerical stability in avoiding division by small numbers at this step. However, this choice fails to account for the very large element $M=10^{10}$ in the same row. The algorithm does not \"see\" the near linear dependence between the columns because the structure of matrix $A$ hides it from the partial pivoting strategy. The resulting pivots in $U$ do not reflect the true scaling of the problem, given by the singular values.\n\n**3. Determination of Rank from Rank-Revealing LU, $r_{\\mathrm{RR}}$**\n\nRank-revealing LU involves column permutations to bring the largest possible elements into pivot positions. We seek a column permutation matrix $\\Pi$ such that $A\\Pi$ leads to a more insightful LU factorization. Let's swap the two columns of $A$:\n$$\n\\Pi = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\n$$\nThe permuted matrix is $A_c = A\\Pi$:\n$$\nA_c = \\begin{pmatrix} 1 & M \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} M & 1 \\\\ 1 & 0 \\end{pmatrix}\n$$\nNow we perform LU factorization (with partial pivoting) on $A_c$.\nIn the first column, the elements are $M=10^{10}$ and $1$. The pivot is $M$, being the larger element. No row swap is needed.\nThe multiplier to eliminate the $(2,1)$ entry is $l_{21} = \\frac{1}{M}$.\nWe perform the row operation $R_2 \\leftarrow R_2 - l_{21} R_1$:\nThe new second row is $(1 - \\frac{1}{M}M, 0 - \\frac{1}{M}1) = (0, -\\frac{1}{M})$.\nThe resulting upper triangular matrix $U$ is:\n$$\nU = \\begin{pmatrix} M & 1 \\\\ 0 & -1/M \\end{pmatrix}\n$$\nThe rank $r_{\\mathrm{RR}}$ is the number of diagonal entries of this $U$ whose magnitudes exceed $\\tau = 5 \\times 10^{-9}$. The diagonal entries are $u_{11}=M=10^{10}$ and $u_{22}=-1/M = -10^{-10}$.\n- $|u_{11}| = 10^{10} > 5 \\times 10^{-9}$.\n- $|u_{22}| = |-10^{-10}| = 10^{-10}$. This is less than $\\tau = 5 \\times 10^{-9}$.\nOnly one diagonal entry exceeds the tolerance. Therefore, the rank estimated by this method is $r_{\\mathrm{RR}} = 1$.\n\nThis method succeeds because the column permutation brings the largest element of the matrix, $M$, into the initial pivot position. This is a key principle of more robust pivoting strategies like complete pivoting. The subsequent elimination step produces a second pivot, $-1/M$, whose magnitude correctly reflects the magnitude of the small singular value $\\sigma_2$. This factorization thus \"reveals\" the numerical rank of the matrix, correctly identifying it as nearly rank-deficient.\n\n**4. Computation of $\\Delta$**\n\nThe final step is to compute the scalar quantity $\\Delta = r_{\\mathrm{PP}} - r_{\\mathrm{RR}}$.\nUsing the values derived above:\n- $r_{\\mathrm{PP}} = 2$\n- $r_{\\mathrm{RR}} = 1$\n$$\n\\Delta = 2 - 1 = 1\n$$\nThe discrepancy $\\Delta=1$ highlights the failure of standard partial pivoting to reliably determine numerical rank and the success of a rank-revealing strategy for the same task.",
            "answer": "$$\n\\boxed{1}\n$$"
        }
    ]
}