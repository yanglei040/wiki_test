{
    "hands_on_practices": [
        {
            "introduction": "Many large-scale scientific simulations, particularly those arising from the discretization of partial differential equations, naturally produce linear systems with a block tridiagonal structure. This exercise challenges you to generalize the highly efficient Thomas algorithm from scalar to block matrices, a crucial skill for tackling these advanced problems. By deriving the block forward-elimination and backward-substitution steps and their precise computational cost, you will deepen your understanding of block matrix operations and complexity analysis. ",
            "id": "3578839",
            "problem": "Consider the linear system with a block tridiagonal coefficient matrix of block size $m \\times m$ and $p$ block rows,\n$$\n\\begin{pmatrix}\nA_{1}  C_{1} \\\\\nB_{2}  A_{2}  C_{2} \\\\\n \\ddots  \\ddots  \\ddots \\\\\n  B_{p}  A_{p}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{p}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf_{1} \\\\ f_{2} \\\\ \\vdots \\\\ f_{p}\n\\end{pmatrix},\n$$\nwhere for each $i$, the blocks $A_{i} \\in \\mathbb{R}^{m\\times m}$ are nonsingular, and $B_{i}, C_{i} \\in \\mathbb{R}^{m\\times m}$, while $x_{i}, f_{i} \\in \\mathbb{R}^{m}$. Assume the system is strictly block diagonally dominant so that Gaussian elimination without pivoting is well-defined at the block level and preserves nonsingularity of all intermediate Schur complements.\n\nYour tasks are:\n- Starting from the block equations and the principles of Gaussian elimination and Schur complement formation, derive the block-forward elimination that removes the subdiagonal block $B_{i}$ using only solutions with the current diagonal block (that is, without forming explicit inverses). Then derive the block-backward substitution that recovers the unknown blocks $x_{i}$.\n- Using a flop model where one floating-point addition, subtraction, multiplication, or division counts as one flop, the lower-triangular factor has unit diagonal, and the following exact costs hold:\n  1. Dense $\\mathrm{LU}$ factorization without pivoting of an $m \\times m$ matrix costs $\\left(\\frac{2}{3} m^{3} - \\frac{1}{2} m^{2} - \\frac{1}{6} m\\right)$ flops.\n  2. Solving a triangular system with one right-hand side costs $(2 m^{2} - m)$ flops, and with $r$ right-hand sides costs $r(2 m^{2} - m)$ flops.\n  3. A fused update of the form $D \\leftarrow D - E F$ with $D, E, F \\in \\mathbb{R}^{m \\times m}$ costs $2 m^{3}$ flops, and a fused update $d \\leftarrow d - E g$ with $d, g \\in \\mathbb{R}^{m}$, $E \\in \\mathbb{R}^{m\\times m}$ costs $2 m^{2}$ flops.\n- Under these assumptions, derive a closed-form analytic expression, as a function of $p$ and $m$, for the total number of flops required by the resulting block extension of the Thomas algorithm, including both the forward sweep and the backward substitution. Express your final answer as a single analytic expression in $p$ and $m$ (no big-$\\mathcal{O}$ notation).",
            "solution": "The user wants me to solve a numerical linear algebra problem.\n\nHere's my plan:\n1.  **Validate the problem statement**:\n    -   **Step 1: Extract Givens**: I'll list all the definitions, matrices, vectors, and assumptions provided in the prompt.\n    -   **Step 2: Validate**: I'll check the problem for scientific soundness, well-posedness, objectivity, and other criteria. The problem seems to be a standard numerical analysis question about block algorithms.\n        -   It describes a block tridiagonal system. This is a common structure in scientific computing, e.g., from discretizing PDEs.\n        -   It asks for the derivation of a block algorithm based on Gaussian elimination. This is a standard technique.\n        -   It gives a precise flop count model. These counts are standard approximations for the operations mentioned.\n        -   The assumption of strict block diagonal dominance ensures the existence and stability of the a block LU factorization without pivoting, which makes the problem well-posed.\n        -   The goal is to derive the algorithm and its computational cost. This is a standard exercise in numerical linear algebra.\n    -   **Step 3: Verdict**: The problem seems valid. It's a well-defined mathematical problem in numerical linear algebra.\n\n2.  **Solve the problem (if valid)**:\n    -   **Part 1: Derivation of the block algorithm**\n        -   I'll write out the block equations for the system.\n        -   The first equation is $A_1 x_1 + C_1 x_2 = f_1$.\n        -   The general equation for $i=2, \\dots, p-1$ is $B_i x_{i-1} + A_i x_i + C_i x_{i+1} = f_i$.\n        -   The last equation is $B_p x_{p-1} + A_p x_p = f_p$.\n        -   The core idea is to perform block Gaussian elimination. This is analogous to the scalar Thomas algorithm (or Tridiagonal Matrix Algorithm, TDMA).\n        -   **Forward Elimination**:\n            -   Goal: Eliminate the subdiagonal blocks $B_i$.\n            -   For the first block row `i=1`: I want to express $x_1$ in terms of $x_2$.\n                -   $A_1 x_1 = f_1 - C_1 x_2$.\n                -   Since $A_1$ is nonsingular, $x_1 = A_1^{-1}(f_1 - C_1 x_2)$. This can be written as $x_1 = A_1^{-1}f_1 - A_1^{-1}C_1 x_2$.\n                -   Let's define new matrices and vectors that are updated in the forward sweep. Let's call them $A_i'$ and $f_i'$. This is more standard.\n            -   Let's follow the elimination procedure.\n                -   The first row equation is $A_1 x_1 + C_1 x_2 = f_1$.\n                -   The second row equation is $B_2 x_1 + A_2 x_2 + C_2 x_3 = f_2$.\n                -   We want to eliminate $x_1$ from the second equation.\n                -   From the first equation, we can write $x_1 = A_1^{-1} (f_1 - C_1 x_2)$.\n                -   Substitute this into the second equation:\n                    $B_2 [A_1^{-1} (f_1 - C_1 x_2)] + A_2 x_2 + C_2 x_3 = f_2$\n                    $(A_2 - B_2 A_1^{-1} C_1) x_2 + C_2 x_3 = f_2 - B_2 A_1^{-1} f_1$.\n            -   This defines the update rule. Let's define the new matrices and vectors.\n                -   Let $\\tilde{A}_i$ and $\\tilde{f}_i$ be the modified matrices and vectors.\n                -   Start with $\\tilde{A}_1 = A_1$ and $\\tilde{f}_1 = f_1$.\n                -   The process effectively generates new matrices $C'_i$ and new vectors $f'_i$.\n                -   Let's rewrite the first equation as $x_1 = \\tilde{A}_1^{-1}( \\tilde{f}_1 - C_1 x_2)$. This is not the standard way.\n            -   Let's try a different formulation that mirrors the scalar algorithm. The scalar algorithm modifies the diagonal and the right-hand side.\n                -   Scalar algorithm: for $i=2, \\dots, p$:\n                    -   $m = b_i / a_{i-1}$\n                    -   $a_i \\leftarrow a_i - m \\cdot c_{i-1}$\n                    -   $f_i \\leftarrow f_i - m \\cdot f_{i-1}$\n            -   Let's adapt this to the block case. The multiplier will be a matrix.\n                -   For $i=2, \\dots, p$:\n                    -   Multiplier matrix: $M_i = B_i A_{i-1}^{-1}$.\n                    -   Update diagonal block: $A_i \\leftarrow A_i - M_i C_{i-1} = A_i - B_i A_{i-1}^{-1} C_{i-1}$.\n                    -   Update RHS vector: $f_i \\leftarrow f_i - M_i f_{i-1} = f_i - B_i A_{i-1}^{-1} f_{i-1}$.\n            -   The problem states \"without forming explicit inverses\". This is a crucial constraint.\n            -   So, instead of computing $A_{i-1}^{-1}$, we will solve systems with $A_{i-1}$.\n            -   Let's redefine the forward sweep for $i=2, \\dots, p$:\n                1.  Define new matrices $A'_i$ and vectors $f'_i$. Let's start with $A'_1 = A_1$ and $f'_1 = f_1$.\n                2.  For $i=2, \\dots, p$, we want to eliminate $B_i$. The new system should look like:\n                    $A'_{i-1} x_{i-1} + C_{i-1} x_i = f'_{i-1}$\n                    $A'_i x_i + C_i x_{i+1} = f'_i$\n                3.  Let's start from the original equations:\n                    $B_{i-1} x_{i-2} + A_{i-1} x_{i-1} + C_{i-1} x_i = f_{i-1}$\n                    $B_i x_{i-1} + A_i x_i + C_i x_{i+1} = f_i$ (for $i2$)\n                4.  This is a bit confusing. Let's stick to the Schur complement idea.\n                -   Let the modified system at step $i$ be:\n                    $$\n                    \\begin{pmatrix}\n                    \\tilde{A}_{i}  C_{i} \\\\\n                    B_{i+1}  A_{i+1}  C_{i+1} \\\\\n                     \\ddots  \\ddots  \\ddots\n                    \\end{pmatrix}\n                    \\begin{pmatrix}\n                    x_{i} \\\\ x_{i+1} \\\\ \\vdots\n                    \\end{pmatrix}\n                    =\n                    \\begin{pmatrix}\n                    \\tilde{f}_{i} \\\\ f_{i+1} \\\\ \\vdots\n                    \\end{pmatrix}\n                    $$\n                -   We have the relation $B_i x_{i-1} + \\tilde{A}_i x_i + C_i x_{i+1} = \\tilde{f}_i$. This is not quite right.\n                -   Let's follow the elimination process directly.\n                    -   The system matrix is $M$. Let's apply block LU factorization.\n                        $M = LU$. $L$ is block lower bidiagonal, $U$ is block upper tridiagonal.\n                        $$\n                        L = \\begin{pmatrix}\n                        I    \\\\\n                        L_2  I   \\\\\n                         \\ddots  \\ddots  \\\\\n                          L_p  I\n                        \\end{pmatrix}, \\quad\n                        U = \\begin{pmatrix}\n                        U_1  C_1   \\\\\n                         U_2  C_2  \\\\\n                          \\ddots  \\ddots \\\\\n                           U_p\n                        \\end{pmatrix}\n                        $$\n                        The diagonal blocks of $L$ are identity matrices, this is a block Doolittle factorization.\n                    -   $M_{ij} = (LU)_{ij} = \\sum_k L_{ik} U_{kj}$.\n                    -   For $i=1$: $A_1 = I \\cdot U_1 \\implies U_1 = A_1$.\n                    -   For $i=2, \\dots, p$:\n                        -   $B_i = L_i U_{i-1} \\implies L_i = B_i U_{i-1}^{-1}$.\n                        -   $A_i = L_i C_{i-1} + I \\cdot U_i \\implies U_i = A_i - L_i C_{i-1} = A_i - B_i U_{i-1}^{-1} C_{i-1}$.\n            -   This is the forward elimination step. It computes the matrices $U_i$. These $U_i$ are the Schur complements.\n            -   Let's write the algorithm \"without forming explicit inverses\":\n                **Forward elimination (computing $U_i$):**\n                1.  $U_1 = A_1$.\n                2.  For $i=2, \\dots, p$:\n                    a. Solve for a matrix $X$ in $U_{i-1}X = B_i$. This is not right, the multiplier is $B_i U_{i-1}^{-1}$. We need to solve $X U_{i-1} = B_i$. Or equivalently, solve $U_{i-1}^T X^T = B_i^T$. Let's call the solution $L_i$.\n                    b. Compute $U_i = A_i - L_i C_{i-1}$.\n            -   This is not what is usually done. The operation $B_i U_{i-1}^{-1}$ is what is needed. This is computed as $( (U_{i-1}^{-1})^T B_i^T )^T$. So one solves $U_{i-1}^T Z = B_i^T$ for $Z$, then the multiplier is $Z^T$. This is a system with $m$ right-hand sides. Let's call the multiplier $M_i = B_i U_{i-1}^{-1}$. Then $U_i = A_i - M_i C_{i-1}$.\n            -   A different, more direct way to organize the calculation is to compute $U_{i-1}^{-1}C_{i-1}$ first. Let $W_{i-1} = U_{i-1}^{-1}C_{i-1}$. This is done by solving $U_{i-1}W_{i-1} = C_{i-1}$ for $W_{i-1}$ (a matrix). Then $U_i = A_i - B_i W_{i-1}$.\n            -   Now let's apply this to the RHS. The system is $LUx = f$. Let $y = Ux$. We first solve $Ly=f$.\n                -   $y_1 = f_1$.\n                -   $L_2 y_1 + y_2 = f_2 \\implies y_2 = f_2 - L_2 y_1 = f_2 - (B_2 U_1^{-1}) y_1$.\n                -   In general, for $i=2, \\dots, p$:\n                    $y_i = f_i - L_i y_{i-1} = f_i - (B_i U_{i-1}^{-1}) y_{i-1}$.\n                **Forward substitution (computing $y_i$):**\n                1. $y_1 = f_1$.\n                2. For $i=2, \\dots, p$:\n                   a. Solve $U_{i-1} z = y_{i-1}$ for $z$. This gives $z=U_{i-1}^{-1}y_{i-1}$.\n                   b. $y_i = f_i - B_i z$.\n            -   This seems like a valid algorithm based on the prompt. It performs a block LU factorization of the original matrix $M$ into $M=LU$, where $L$ is block lower bidiagonal with identity blocks on the diagonal, and $U$ is block upper tridiagonal. Then it solves $Ly=f$ followed by $Ux=f$. The process I outlined computes the blocks of $U$ and the vector $y$ concurrently.\n\n        -   **Algorithm Derivation**\n            The block tridiagonal system can be written as a series of equations:\n            $$A_1 x_1 + C_1 x_2 = f_1$$\n            $$B_i x_{i-1} + A_i x_i + C_i x_{i+1} = f_i, \\quad \\text{for } i=2, \\ldots, p-1$$\n            $$B_p x_{p-1} + A_p x_p = f_p$$\n            The method of block Gaussian elimination (a block version of the Thomas algorithm) transforms this system into an upper block bidiagonal system. This is equivalent to performing a block LU decomposition of the coefficient matrix $M$, where $M=LU$, $L$ is block lower bidiagonal with identity matrices on its diagonal, and $U$ is block upper tridiagonal. The system $Mx=f$ becomes $LUx=f$. We first solve $Ly=f$ (forward sweep) and then $Ux=y$ (backward sweep).\n\n            The decomposition $M=LU$ yields the recurrence relations for the blocks of $U$:\n            $$U_1 = A_1$$\n            $$U_i = A_i - B_i U_{i-1}^{-1} C_{i-1}, \\quad \\text{for } i=2, \\ldots, p$$\n            The forward sweep to solve $Ly=f$ yields:\n            $$y_1 = f_1$$\n            $$y_i = f_i - B_i U_{i-1}^{-1} y_{i-1}, \\quad \\text{for } i=2, \\ldots, p$$\n            The matrices $U_i$ are the Schur complements. To avoid explicit matrix inversions, we replace expressions like $U_{i-1}^{-1}C_{i-1}$ with the solution of a linear system.\n\n            **Block-Forward Elimination:**\n            The forward sweep computes the matrices $U_i$ and vectors $y_i$.\n            1. Initialize: $U_1 = A_1$, $y_1 = f_1$.\n            2. For $i$ from $2$ to $p$:\n               a. To compute $U_i$: Solve the matrix equation $U_{i-1} W_{i-1} = C_{i-1}$ for the matrix $W_{i-1}$. Then compute $U_i = A_i - B_i W_{i-1}$.\n               b. To compute $y_i$: Solve the vector equation $U_{i-1} g_{i-1} = y_{i-1}$ for the vector $g_{i-1}$. Then compute $y_i = f_i - B_i g_{i-1}$.\n\n            After the forward sweep, we are left with the block upper bidiagonal system $Ux=y$:\n            $$\n            \\begin{pmatrix}\n            U_{1}  C_{1} \\\\\n             U_{2}  C_{2} \\\\\n              \\ddots  \\ddots \\\\\n               U_{p}\n            \\end{pmatrix}\n            \\begin{pmatrix}\n            x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{p}\n            \\end{pmatrix}\n            =\n            \\begin{pmatrix}\n            y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{p}\n            \\end{pmatrix}\n            $$\n            Note: The matrix $U$ has upper diagonal blocks $C_i$. In the LU formulation I wrote first, the upper diagonal of $U$ matched that of the original matrix. Let me re-check this.\n            $M_{i,i} = L_i C_{i-1} + U_i$. No, that's for tridiagonal.\n            For block tridiagonal:\n            $M_{i,i} = L_i U'_{i-1, i} + I U'_{i,i}$. $U'$ has blocks $U'_{i,i}$ and $U'_{i, i+1}$.\n            Let's match terms $M=LU$:\n            $A_1 = U_1$. (I'll rename $U_{ii}$ to $U_i$ and $U_{i,i+1}$ to $C'_i$)\n            $C_1 = C'_1$.\n            $B_i = L_i U_{i-1}$.\n            $A_i = L_i C'_{i-1} + U_i$.\n            $C_i = C'_i$.\n            This implies $C'_i = C_i$ for all $i$.\n            So $L_i = B_i U_{i-1}^{-1}$.\n            And $U_i = A_i - L_i C_{i-1} = A_i - B_i U_{i-1}^{-1} C_{i-1}$. This is correct.\n            The system $Ux=y$ is:\n            $$U_i x_i + C_i x_{i+1} = y_i, \\quad \\text{for } i=1, \\ldots, p-1$$\n            $$U_p x_p = y_p$$\n            This is consistent.\n\n            **Block-Backward Substitution:**\n            The backward sweep solves for the unknown vectors $x_i$ starting from $x_p$.\n            1. Solve $U_p x_p = y_p$ for $x_p$.\n            2. For $i$ from $p-1$ down to $1$:\n               a. Calculate the updated right-hand side: $z_i = y_i - C_i x_{i+1}$.\n               b. Solve $U_i x_i = z_i$ for $x_i$.\n\n            This completes the derivation of the algorithm.\n\n    -   **Part 2: Flop count derivation**\n        -   I need to analyze the flop count for the entire algorithm: forward sweep + backward substitution.\n        -   To solve linear systems of the form $U_k Z = K$, where $U_k$ is a dense $m \\times m$ matrix, we first perform an LU factorization of $U_k$ and then solve using forward and backward substitutions for each column of $K$.\n\n        -   **Cost of Elementary Operations:**\n            Let $C_{LU}(m) = \\frac{2}{3}m^3 - \\frac{1}{2}m^2 - \\frac{1}{6}m$ be the cost of an $m \\times m$ LU factorization.\n            Let $C_{Solve}(m, r) = r(2m^2 - m)$ be the cost of solving an $m \\times m$ system with $r$ right-hand sides, given its LU factors.\n            Let $C_{MM}(m) = 2m^3$ be the cost of a matrix-matrix update $D \\leftarrow D - EF$.\n            Let $C_{MV}(m) = 2m^2$ be the cost of a matrix-vector update $d \\leftarrow d - Eg$.\n\n        -   **Analysis of the Forward Sweep ($i=2, \\dots, p$):**\n            This loop runs $p-1$ times. In each iteration $i$, the algorithm operates on $U_{i-1}$. So, it involves $U_1, U_2, \\ldots, U_{p-1}$.\n            For each $i \\in \\{2, \\ldots, p\\}$:\n            1.  LU factorization of $U_{i-1}$: The matrix $U_{i-1}$ was computed in the previous step. We need its LU factors to solve with it. So, at step $i$, we factor $U_{i-1}$. Cost: $C_{LU}(m)$.\n            2.  Solve $U_{i-1}W_{i-1} = C_{i-1}$: $m$ right-hand sides. Cost: $C_{Solve}(m, m) = m(2m^2-m) = 2m^3 - m^2$.\n            3.  Solve $U_{i-1}g_{i-1} = y_{i-1}$: $1$ right-hand side. Cost: $C_{Solve}(m, 1) = 2m^2 - m$.\n            4.  Update $U_i = A_i - B_i W_{i-1}$: Cost: $C_{MM}(m) = 2m^3$.\n            5.  Update $y_i = f_i - B_i g_{i-1}$: Cost: $C_{MV}(m) = 2m^2$.\n\n            Total cost for one iteration of the forward loop:\n            $C_{fwd\\_step} = C_{LU}(m) + (2m^3 - m^2) + (2m^2 - m) + 2m^3 + 2m^2$\n            $C_{fwd\\_step} = (\\frac{2}{3}m^3 - \\frac{1}{2}m^2 - \\frac{1}{6}m) + 4m^3 + 3m^2 - m$\n            $C_{fwd\\_step} = \\frac{14}{3}m^3 + \\frac{5}{2}m^2 - \\frac{7}{6}m$.\n\n            The forward loop runs $p-1$ times (for $i=2, \\dots, p$). The LU factorizations performed are for $U_1, U_2, \\ldots, U_{p-1}$.\n            Total forward sweep cost:\n            $F_{fwd} = (p-1) \\left( \\frac{14}{3}m^3 + \\frac{5}{2}m^2 - \\frac{7}{6}m \\right)$.\n\n        -   **Analysis of the Backward Sweep:**\n            1.  Solve for $x_p$: This requires solving $U_p x_p = y_p$. The matrix $U_p$ was computed in the last step of the forward sweep, but its LU factorization was not.\n                -   LU factorization of $U_p$: $C_{LU}(m) = \\frac{2}{3}m^3 - \\frac{1}{2}m^2 - \\frac{1}{6}m$.\n                -   Solve with 1 RHS: $C_{Solve}(m, 1) = 2m^2 - m$.\n                -   Cost for $x_p$: $C_{x_p} = C_{LU}(m) + C_{Solve}(m, 1) = (\\frac{2}{3}m^3 - \\frac{1}{2}m^2 - \\frac{1}{6}m) + (2m^2 - m) = \\frac{2}{3}m^3 + \\frac{3}{2}m^2 - \\frac{7}{6}m$.\n            2.  Solve for $x_{p-1}, \\ldots, x_1$: The loop runs for $i=p-1$ down to $1$, for a total of $p-1$ iterations.\n                For each $i \\in \\{p-1, \\ldots, 1\\}$:\n                a.  Update RHS $z_i = y_i - C_i x_{i+1}$: Cost: $C_{MV}(m) = 2m^2$.\n                b.  Solve $U_i x_i = z_i$: The LU factors of $U_i$ (for $i=1, \\dots, p-1$) were computed and stored during the forward sweep. We only need to perform the substitutions. Cost: $C_{Solve}(m, 1) = 2m^2 - m$.\n                -   Cost for one iteration of the backward loop: $C_{back\\_step} = 2m^2 + (2m^2 - m) = 4m^2 - m$.\n            Total cost for the backward loop ($i=p-1, \\dots, 1$):\n            $F_{back\\_loop} = (p-1)(4m^2 - m)$.\n\n        -   **Total Flop Count:**\n            The total cost is the sum of the forward sweep cost, the cost to find $x_p$, and the cost of the rest of the backward sweep.\n            $F(p,m) = F_{fwd} + C_{x_p} + F_{back\\_loop}$\n            $F(p,m) = (p-1) \\left( \\frac{14}{3}m^3 + \\frac{5}{2}m^2 - \\frac{7}{6}m \\right) + \\left( \\frac{2}{3}m^3 + \\frac{3}{2}m^2 - \\frac{7}{6}m \\right) + (p-1)(4m^2 - m)$.\n\n            We group the terms by powers of $m$:\n            -   Term in $m^3$:\n                $(p-1)\\frac{14}{3}m^3 + \\frac{2}{3}m^3 = \\left(\\frac{14p - 14 + 2}{3}\\right)m^3 = \\left(\\frac{14p - 12}{3}\\right)m^3 = \\left(\\frac{14}{3}p - 4\\right)m^3$.\n            -   Term in $m^2$:\n                $(p-1)\\frac{5}{2}m^2 + \\frac{3}{2}m^2 + (p-1)4m^2 = \\left(\\frac{5p-5}{2} + \\frac{3}{2} + 4p - 4\\right)m^2 = \\left(\\frac{5p-2}{2} + 4p - 4\\right)m^2 = \\left(\\frac{5p-2+8p-8}{2}\\right)m^2 = \\left(\\frac{13p-10}{2}\\right)m^2 = \\left(\\frac{13}{2}p - 5\\right)m^2$.\n            -   Term in $m$:\n                $-(p-1)\\frac{7}{6}m - \\frac{7}{6}m - (p-1)m = \\left(-\\frac{7p-7}{6} - \\frac{7}{6} - p + 1\\right)m = \\left(-\\frac{7p}{6} + \\frac{7}{6} - \\frac{7}{6} - p + 1\\right)m = \\left(-\\frac{7p}{6} - p + 1\\right)m = \\left(-\\frac{13p}{6} + 1\\right)m$.\n\n            Combining these terms gives the total flop count as a function of $p$ and $m$:\n            $F(p, m) = \\left(\\frac{14}{3}p - 4\\right)m^3 + \\left(\\frac{13}{2}p - 5\\right)m^2 + \\left(1 - \\frac{13}{6}p\\right)m$.",
            "answer": "$$\n\\boxed{\\left(\\frac{14}{3}p - 4\\right)m^3 + \\left(\\frac{13}{2}p - 5\\right)m^2 + \\left(1 - \\frac{13}{6}p\\right)m}\n$$"
        },
        {
            "introduction": "While banded LU factorization is efficient, its unpivoted form is notoriously prone to numerical instability. This practice introduces restricted partial pivoting, a vital technique for creating a robust banded solver. You will first confirm the procedure for solving a system rearranged by a permutation matrix and then analyze the implementation cost of row interchanges, revealing the often-overlooked overhead that stability-enhancing techniques can introduce. ",
            "id": "3578824",
            "problem": "Let $n \\in \\mathbb{N}$, and let $A \\in \\mathbb{R}^{n \\times n}$ be a nonsingular band matrix with lower bandwidth $l$ and upper bandwidth $u$, meaning $a_{ij} = 0$ whenever $i - j  l$ or $j - i  u$, with $1 \\leq l \\leq n - 1$ and $u \\geq 0$. Suppose that Gaussian elimination with restricted partial pivoting confined to the band is performed as follows: at step $k \\in \\{1,2,\\dots,n-1\\}$, the pivot is chosen from rows $k,k+1,\\dots,\\min(n,k+l)$ and, if necessary, the pivot row is moved into position $k$ by swapping rows. Assume the factorization succeeds without breakdown and yields a factorization $P A = L U$, where $P$ is a permutation matrix encoding the sequence of row interchanges, $L$ is unit lower triangular, and $U$ is upper triangular.\n\nYour tasks are:\n\n1. Using only the definitions of band matrices, permutation matrices, and triangular matrices, and the property that $P A = L U$, derive a correct and complete procedure to recover the solution $x \\in \\mathbb{R}^{n}$ to the linear system $A x = b$ for a given right-hand side (RHS) $b \\in \\mathbb{R}^{n}$, in a way that respects the band structure during the solves. Justify why the steps produce the exact solution.\n\n2. Under the pessimistic scenario that at each elimination step $k \\in \\{1,2,\\dots,n-1\\}$ the chosen pivot lies exactly at row $\\min(n,k+l)$ within the allowed window, assume that each row interchange $k \\leftrightarrow p$ with $p  k$ is implemented physically as a sequence of adjacent transpositions of neighboring rows so as to preserve storage locality of the band. Derive a closed-form analytic expression for:\n   - the total number of logical row interchanges (i.e., the number of elimination steps at which the pivot row differs from the current row), and\n   - the total number of adjacent row swaps actually executed over the entire factorization,\nas functions of $n$ and $l$.\n\nExpress your final answer as the ordered pair consisting of these two quantities. No rounding is required. No units are required.",
            "solution": "The problem will be validated by first extracting all given information and then checking it against the established criteria for a valid scientific problem.\n\n### Step 1: Extract Givens\n- $A \\in \\mathbb{R}^{n \\times n}$ is a nonsingular band matrix.\n- $n \\in \\mathbb{N}$.\n- Lower bandwidth is $l$, where $1 \\leq l \\leq n - 1$.\n- Upper bandwidth is $u$, where $u \\geq 0$.\n- The band structure is defined by $a_{ij} = 0$ whenever $i - j  l$ or $j - i  u$.\n- Gaussian elimination with restricted partial pivoting is performed.\n- At step $k \\in \\{1, 2, \\dots, n-1\\}$, the pivot is chosen from rows $k, k+1, \\dots, \\min(n, k+l)$.\n- The factorization $P A = L U$ is successfully computed, where $P$ is a permutation matrix, $L$ is unit lower triangular, and $U$ is upper triangular.\n- For Part 2, a pessimistic scenario is assumed: at each step $k$, the pivot is chosen from row $\\min(n, k+l)$.\n- For Part 2, a row interchange $k \\leftrightarrow p$ with $p  k$ is implemented as a sequence of adjacent transpositions.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the following criteria:\n\n- **Scientifically Grounded**: The problem is set firmly within the domain of numerical linear algebra. Band matrices, LU factorization, and pivoting strategies are fundamental concepts in this field. The premises are factually sound and based on established mathematical principles.\n- **Well-Posed**: The problem is clearly structured. Part 1 asks for a standard procedure (solving a system given an LU factorization) with a justification. Part 2 asks for the calculation of specific quantities under a well-defined set of assumptions. The constraints on $n$ and $l$ ensure that the calculations are well-defined. A unique, meaningful solution exists for both parts.\n- **Objective**: The problem is stated in precise, formal mathematical language, free from ambiguity or subjective claims.\n\nThe problem does not exhibit any of the invalidity flaws:\n1.  **Scientific or Factual Unsoundness**: No violations of mathematical logic or scientific principles are present.\n2.  **Non-Formalizable or Irrelevant**: The problem is directly formalizable and central to its specified topic.\n3.  **Incomplete or Contradictory Setup**: The setup is complete and self-consistent. The condition $1 \\leq l \\leq n-1$ is crucial and correctly specified.\n4.  **Unrealistic or Infeasible**: The conditions are mathematically and algorithmically realistic.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires substantive reasoning, particularly for the derivation in Part 2.\n7.  **Outside Scientific Verifiability**: The results are mathematically derivable and verifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\n---\n\n### Part 1: Procedure to Solve $A x = b$\nGiven the linear system $A x = b$ and the factorization $P A = L U$.\n\n1.  **Initial Transformation**: Since the permutation matrix $P$ is invertible (with $P^{-1} = P^T$), the original equation $A x = b$ is equivalent to the pre-multiplied system $P A x = P b$. The vector $P b$ is obtained by applying the same sequence of row interchanges to the vector $b$ that were performed on the matrix $A$ during the factorization process. These interchanges are determined by the pivoting strategy and are encoded in $P$.\n\n2.  **Substitution**: Substitute the factorization $P A = L U$ into the transformed system:\n    $$L U x = P b$$\n    This equation can be solved in two stages by introducing an intermediate vector $y \\in \\mathbb{R}^n$.\n\n3.  **Two-Stage Solution**: Let $y = U x$. The system becomes two separate, simpler systems:\n    - Stage 1: $L y = P b$\n    - Stage 2: $U x = y$\n\n4.  **Stage 1: Forward Substitution**:\n    The first system, $L y = P b$, is solved for $y$. Let $b' = P b$. Since $L$ is a unit lower triangular matrix, this system can be solved by forward substitution. The components of $y$ are computed sequentially from $y_1$ to $y_n$.\n    $$y_i = b'_i - \\sum_{j=1}^{i-1} l_{ij} y_j$$\n    The Gaussian elimination process with pivoting, when applied to a band matrix with lower bandwidth $l$, produces a matrix $L$ that also has a lower bandwidth of $l$. That is, $l_{ij} = 0$ for $i - j  l$. This property allows the sum to be truncated, respecting the band structure:\n    $$y_i = b'_i - \\sum_{j=\\max(1, i-l)}^{i-1} l_{ij} y_j \\quad \\text{for } i = 1, 2, \\dots, n$$\n    (For $i=1$, the sum is empty, so $y_1 = b'_1$.)\n\n5.  **Stage 2: Backward Substitution**:\n    The second system, $U x = y$, is solved for the final solution $x$. Since $U$ is an upper triangular matrix, this system is solved by backward substitution. The components of $x$ are computed in reverse order, from $x_n$ to $x_1$.\n    $$x_i = \\frac{1}{u_{ii}} \\left( y_i - \\sum_{j=i+1}^{n} u_{ij} x_j \\right)$$\n    The pivoting strategy can increase the upper bandwidth of the matrix. If at step $k$, row $k$ is swapped with row $k+d$ (where $d \\leq l$), elements from row $k+d$ are brought into row $k$. The original row $k+d$ had non-zero elements up to column $(k+d)+u$. Thus, the new row $k$ can have non-zero elements up to this column. This means the upper bandwidth of $U$, let's call it $u'$, can be at most $u+l$. Taking into account this band structure, the backward substitution formula becomes:\n    $$x_i = \\frac{1}{u_{ii}} \\left( y_i - \\sum_{j=i+1}^{\\min(n, i+u')} u_{ij} x_j \\right) \\quad \\text{for } i = n, n-1, \\dots, 1$$\n    (For $i=n$, the sum is empty, so $x_n = y_n/u_{nn}$.)\n\n**Justification**: The procedure is exact because it is based on a sequence of equivalence transformations. The original system $A x = b$ is equivalent to $P A x = P b$. The substitution $P A = L U$ is an identity. The decomposition into two triangular systems is valid. Since $A$ is nonsingular and $P$ is nonsingular, $L U = P A$ must also be nonsingular. This implies $\\det(L)\\det(U) = \\det(P)\\det(A) \\neq 0$. As $L$ is unit lower triangular, $\\det(L)=1$. Thus, $\\det(U) \\neq 0$, which means $u_{ii} \\neq 0$ for all $i$, so $U$ is nonsingular. The nonsingularity of both $L$ and $U$ guarantees that unique solutions for $y$ and $x$ exist and can be found by forward and backward substitution, respectively.\n\n### Part 2: Interchange and Swap Counts\nUnder the pessimistic scenario, at each elimination step $k \\in \\{1, 2, \\dots, n-1\\}$, the pivot row is $p_k = \\min(n, k+l)$.\n\n**(a) Total number of logical row interchanges**\n\nA logical row interchange occurs at step $k$ if the pivot row is not row $k$, i.e., $p_k  k$. We need to count the number of steps $k \\in \\{1, 2, \\dots, n-1\\}$ where this condition holds.\n\nThe pivot row is $p_k = \\min(n, k+l)$. We are given $l \\geq 1$.\n- For any $k \\in \\{1, \\dots, n-1\\}$, we have $k+l  k$.\n- Also, since $k \\leq n-1$, we have $n  k$.\n- Therefore, $p_k = \\min(n, k+l)$ will always be strictly greater than $k$.\n\nSince $p_k  k$ for all $k$ from $1$ to $n-1$, a logical row interchange occurs at every elimination step. The total number of elimination steps is $n-1$.\nThus, the total number of logical row interchanges is $n-1$.\n\n**(b) Total number of adjacent row swaps**\n\nAn interchange that moves the pivot row $p_k$ to position $k$ (for $p_k  k$) by a sequence of adjacent transpositions requires $p_k - k$ such swaps. This corresponds to cyclically shifting the block of rows from $k$ to $p_k$. The total number of adjacent swaps, $S$, is the sum of these counts over all elimination steps.\n$$S = \\sum_{k=1}^{n-1} (p_k - k) = \\sum_{k=1}^{n-1} (\\min(n, k+l) - k)$$\nTo evaluate this sum, we split it based on the condition in the $\\min$ function. The switch occurs when $k+l  n$, i.e., $k  n-l$. Given $1 \\leq l \\leq n-1$, the split point $n-l$ is in the range $[1, n-1]$.\n\nThe sum can be written as:\n$$S = \\sum_{k=1}^{n-l} (\\min(n, k+l) - k) + \\sum_{k=n-l+1}^{n-1} (\\min(n, k+l) - k)$$\n\n**Case 1: $k \\leq n-l$**\nIn this range, $k+l \\leq n$, so $\\min(n, k+l) = k+l$. The term in the sum is $(k+l) - k = l$.\nThe first part of the sum is:\n$$\\sum_{k=1}^{n-l} l = l \\cdot (n-l)$$\n\n**Case 2: $k  n-l$**\nIn this range, $k+l  n$, so $\\min(n, k+l) = n$. The term in the sum is $n-k$.\nThe second part of the sum is:\n$$\\sum_{k=n-l+1}^{n-1} (n-k)$$\nLet's make a change of index. Let $j = n-k$. As $k$ goes from $n-l+1$ to $n-1$, $j$ goes from $n-(n-l+1) = l-1$ down to $n-(n-1)=1$.\nSo the sum becomes:\n$$\\sum_{j=1}^{l-1} j = \\frac{(l-1)l}{2}$$\n\n**Total Sum**\nCombining the two parts gives the total number of adjacent swaps:\n$$S = l(n-l) + \\frac{l(l-1)}{2}$$\nSimplifying this expression:\n$$S = nl - l^2 + \\frac{l^2 - l}{2} = \\frac{2nl - 2l^2 + l^2 - l}{2} = \\frac{2nl - l^2 - l}{2}$$\n$$S = \\frac{l(2n - l - 1)}{2}$$\n\nThe two quantities required are the total number of logical row interchanges and the total number of adjacent row swaps.\nNumber of logical interchanges $= n-1$.\nNumber of adjacent swaps $= \\frac{l(2n-l-1)}{2}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} n-1  \\frac{l(2n - l - 1)}{2} \\end{pmatrix}}$$"
        },
        {
            "introduction": "For problems where numerical stability is paramount, QR factorization offers a superior alternative to LU decomposition. This exercise explores the use of Givens rotations to compute the QR factorization of a band matrix, a method that elegantly preserves the sparse structure. You will derive the exact computational and memory costs of this approach and conclude by comparing its stability to that of the normal equations, reinforcing a fundamental principle in the choice of numerical algorithms. ",
            "id": "3578840",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a real square band matrix with symmetric half-bandwidth $k$ (that is, $a_{ij} = 0$ whenever $|i-j|  k$) and suppose $1 \\leq k \\leq \\left\\lfloor \\frac{n-1}{2} \\right\\rfloor$ so that $n \\geq 2k+1$. Consider computing a $QR$ factorization $A = QR$ using Givens rotations applied column-by-column to annihilate subdiagonal entries. Adopt the following operation-count model, which is standard in numerical linear algebra:\n- Each Givens rotation that annihilates an entry $a_{ij}$ with $ij$ acts on the consecutive columns $j, j+1, \\dots, \\min\\{n,\\, i+k\\}$ of the two involved rows, and the application to one column of the two-row pair costs $6$ floating-point operations (flops). Ignore the cost of computing the rotation parameters.\n- A band-stored upper-triangular factor with $p$ superdiagonals is represented in a $(p+1) \\times n$ band array, i.e., with $(p+1)n$ floating-point words.\n- A Givens rotation is stored implicitly by its cosine-sine pair $\\{c,s\\}$, requiring $2$ floating-point words.\n\nStarting from these definitions and the band pattern of $A$, and without appealing to any prepackaged formula, do the following:\n1. Derive a closed-form expression $F(n,k)$ for the exact flop count to form $R$ using the above Givens-rotation scheme, valid under $n \\geq 2k+1$.\n2. Derive a closed-form expression $M(n,k)$ for the total number of floating-point words needed to store the final factors in an implicit form suitable for least-squares solves, consisting of the band-stored upper-triangular factor $R$ and all stored Givens parameters $\\{c,s\\}$, under the assumption that $R$ has at most $2k$ superdiagonals after the factorization.\n3. For the overdetermined least-squares problem $\\min_{x} \\|Ax - b\\|_{2}$, compare the stability of using $QR$ with Givens rotations against solving the normal equations $(A^{\\mathsf{T}}A)x = A^{\\mathsf{T}}b$ via a banded factorization (such as a banded $LU$ or Cholesky factorization of $A^{\\mathsf{T}}A$). Using the standard leading-order forward error characterizations in terms of the spectral condition number $\\kappa_{2}(A)$ and unit roundoff $\\epsilon$, derive the ratio $\\rho(\\kappa_{2}(A))$ of the dominant forward error bound of the normal-equations approach to that of $QR$.\n\nExpress your final results as a single row matrix with entries in the order $\\big(F(n,k),\\, M(n,k),\\, \\rho(\\kappa_{2}(A))\\big)$. No numerical evaluation is required; provide exact closed-form expressions.",
            "solution": "The user-provided problem is a three-part question in numerical linear algebra concerning the QR factorization of a banded matrix using Givens rotations.\n\n### Step 1: Extract Givens\n- **Matrix**: $A \\in \\mathbb{R}^{n \\times n}$ is a real square matrix with symmetric half-bandwidth $k$. This means $a_{ij} = 0$ for all $i,j$ such that $|i-j|  k$.\n- **Dimensions**: $1 \\leq k \\leq \\left\\lfloor \\frac{n-1}{2} \\right\\rfloor$, which implies $n \\geq 2k+1$.\n- **Algorithm**: QR factorization $A=QR$ is computed by annihilating subdiagonal entries column-by-column using Givens rotations.\n- **Flop Count Model**:\n    - The cost of computing Givens rotation parameters $\\{c,s\\}$ is ignored.\n    - A single Givens rotation to annihilate entry $a_{ij}$ ($ij$) acts on rows, and the cost is specified by its action on columns. The affected columns are $j, j+1, \\dots, \\min\\{n, i+k\\}$.\n    - The cost for applying the rotation to a single column of the two involved rows is $6$ floating-point operations (flops).\n- **Memory Storage Model**:\n    - The upper-triangular factor $R$ has at most $p=2k$ superdiagonals and is stored in a $(p+1) \\times n$ array, requiring $(2k+1)n$ floating-point words.\n    - Each Givens rotation is stored as a $\\{c,s\\}$ pair, requiring $2$ floating-point words.\n- **Tasks**:\n    1.  Derive the exact flop count $F(n,k)$ to compute $R$.\n    2.  Derive the total memory storage $M(n,k)$ for $R$ and all Givens rotations.\n    3.  Derive the ratio $\\rho(\\kappa_{2}(A))$ of dominant forward error bounds for solving a least-squares problem via normal equations versus QR factorization.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the domain of numerical linear algebra. The concepts of band matrices, QR factorization, Givens rotations, flop counts, and stability analysis are standard. The provided models for cost and storage are typical simplifications used for algorithmic analysis. The premise that the upper-triangular factor $R$ has an upper bandwidth of $2k$ is a known result of applying Givens rotations to a matrix with lower bandwidth $k$ and upper bandwidth $k$. The question regarding stability compares two standard methods (QR vs. Normal Equations), a classic topic in numerical analysis. The problem is self-contained, objective, and well-posed. No contradictions or ambiguities that would prevent a formal solution are present.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Part 1: Flop Count $F(n,k)$\n\nThe computation proceeds column by column, from $j=1$ to $n-1$. For each column $j$, we annihilate the subdiagonal entries $a_{ij}$ where $j+1 \\leq i \\leq \\min(n, j+k)$. Let's assume the standard procedure where the element $a_{ij}$ is annihilated by a rotation involving rows $i-1$ and $i$. The problem statement gives the cost of one such rotation.\n\nA single Givens rotation designed to annihilate the entry at $(i,j)$ is applied to columns $m = j, j+1, \\dots, \\min(n, i+k)$.\nThe number of columns this rotation is applied to is $\\min(n, i+k) - j + 1$.\nThe cost of this single rotation is $6 \\times (\\min(n, i+k) - j + 1)$ flops.\n\nTo find the total flop count $F(n,k)$, we must sum this cost over all annihilations. The annihilations are indexed by column $j$ and row $i$.\n$$F(n,k) = \\sum_{j=1}^{n-1} \\sum_{i=j+1}^{\\min(n, j+k)} 6 \\left( \\min(n, i+k) - j + 1 \\right)$$\nWe can analyze this sum by splitting the range of $j$ into three regions based on the behavior of the $\\min$ functions, under the constraint $n \\geq 2k+1$.\n\n1.  **Main block ($1 \\leq j \\leq n-2k$):**\n    For this range of $j$, we have $j+k \\leq n-k  n$ and $j+2k \\leq n$.\n    The inner loop limit is $\\min(n, j+k) = j+k$.\n    For any $i$ in the inner loop, $j+1 \\leq i \\leq j+k$, we have $i+k \\leq j+2k \\leq n$, so $\\min(n, i+k)=i+k$.\n    The cost for a single column $j$ in this block is:\n    $$C_j = \\sum_{i=j+1}^{j+k} 6(i+k-j+1)$$\n    Let $s=i-j$. The sum is over $s=1, \\dots, k$.\n    $$C_j = 6 \\sum_{s=1}^{k} (s+j+k-j+1) = 6 \\sum_{s=1}^{k} (s+k+1)$$\n    This is the sum of an arithmetic series: $6 \\left( k(k+1) + \\frac{k(k+1)}{2} \\right) = 6 \\left( \\frac{3k(k+1)}{2} \\right) = 9k(k+1)$.\n    This cost is constant for all $j$ in this range. The number of such columns is $n-2k$.\n    The total flops for this block is $F_1 = (n-2k) \\cdot 9k(k+1)$.\n\n2.  **Tail block ($n-k+1 \\leq j \\leq n-1$):**\n    For this range, $j+k  n$. The inner loop limit is $\\min(n, j+k)=n$.\n    The number of elements to annihilate in column $j$ is $n-j$. So the loop is $i=j+1, \\dots, n$.\n    For any $i$ in this loop, $i \\geq j+1  n-k+1$, so $i+k  n+1$. Thus, $\\min(n,i+k)=n$.\n    The cost for column $j$ is:\n    $$C_j = \\sum_{i=j+1}^{n} 6(n - j + 1) = 6(n-j+1) \\cdot (n-(j+1)+1) = 6(n-j+1)(n-j)$$\n    To find the total flops for this block, let $m = n-j$. As $j$ goes from $n-k+1$ to $n-1$, $m$ goes from $k-1$ down to $1$.\n    $$F_3 = \\sum_{m=1}^{k-1} 6(m+1)m = 6 \\sum_{m=1}^{k-1} (m^2+m) = 6\\left[ \\frac{(k-1)k(2k-1)}{6} + \\frac{(k-1)k}{2} \\right]$$\n    $$F_3 = k(k-1)(2k-1) + 3k(k-1) = k(k-1)(2k-1+3) = 2k(k-1)(k+1) = 2k^3 - 2k$$\n\n3.  **Intermediate block ($n-2k+1 \\leq j \\leq n-k$):**\n    Calculating the sum for this block is algebraically intensive. A more direct approach is to sum the costs of the first two blocks and recognize that the total flop count must be a linear function of $n$ for $n \\gg k$, of the form $F(n,k) = (\\text{cost per column})n + \\text{lower order terms}$.\n    The cost per column in the main block is $9k(k+1)$. So $F(n,k) = 9k(k+1)n + C(k)$.\n    We can write $F(n,k) = F_1 + F_2 + F_3$, where $F_2$ is the cost for the intermediate block.\n    $F(n,k) = 9k(k+1)(n-2k) + F_2 + (2k^3-2k) = (9k^2+9k)n - (18k^3+18k^2) + F_2 + (2k^3-2k)$.\n    A detailed summation for $F_2$ shows it is a cubic polynomial in $k$. The sum of all terms not dependent on $n$ simplifies. The aggregate calculation yields:\n    $F_2 + F_3 - (18k^3+18k^2) = (10k^3+6k^2-4k) - (18k^3+18k^2) = -8k^3-12k^2-4k$.\n    So, $F(n,k) = (9k^2+9k)n - (8k^3+12k^2+4k)$.\n    Factoring this expression:\n    $F(n,k) = 9k(k+1)n - 4k(2k^2+3k+1) = 9k(k+1)n - 4k(k+1)(2k+1)$.\n    $$F(n,k) = k(k+1)(9n - 4(2k+1)) = k(k+1)(9n-8k-4)$$\n\n### Part 2: Memory Storage $M(n,k)$\n\nThe total memory $M(n,k)$ is the sum of storage for the factor $R$ and for all the Givens rotation parameters.\n\n1.  **Storage for R:**\n    The problem states that $R$ has an upper bandwidth of $p=2k$. It is stored in a $(p+1) \\times n$ array.\n    Storage for $R = (2k+1)n$ floating-point words.\n\n2.  **Storage for Givens rotations:**\n    We need to count the total number of Givens rotations applied. Each rotation costs $2$ words.\n    For each column $j$, we perform one rotation for each subdiagonal entry, which are at positions $(j+1,j), \\dots, (\\min(n,j+k),j)$.\n    Number of rotations for column $j$ is $\\min(n, j+k) - (j+1) + 1 = \\min(n-j, k)$.\n    Total number of rotations, $N_{rot}$:\n    $$N_{rot} = \\sum_{j=1}^{n-1} \\min(k, n-j)$$\n    We split the sum at $j=n-k$:\n    $$N_{rot} = \\sum_{j=1}^{n-k} k + \\sum_{j=n-k+1}^{n-1} (n-j)$$\n    The first sum has $n-k$ terms: $k(n-k)$.\n    For the second sum, let $m=n-j$. As $j$ runs from $n-k+1$ to $n-1$, $m$ runs from $k-1$ down to $1$.\n    $$\\sum_{m=1}^{k-1} m = \\frac{(k-1)k}{2}$$\n    Total rotations: $N_{rot} = k(n-k) + \\frac{k(k-1)}{2} = kn - k^2 + \\frac{k^2-k}{2} = kn - \\frac{k^2+k}{2} = k\\left(n - \\frac{k+1}{2}\\right)$.\n    Storage for rotations = $2 \\times N_{rot} = 2k\\left(n - \\frac{k+1}{2}\\right) = k(2n-k-1)$.\n\nTotal memory:\n$$M(n,k) = (2k+1)n + k(2n-k-1) = 2kn + n + 2kn - k^2 - k = 4kn + n - k^2 - k$$\n$$M(n,k) = n(4k+1) - k(k+1)$$\n\n### Part 3: Stability Ratio $\\rho(\\kappa_{2}(A))$\n\nWe compare the stability of solving the least-squares problem $\\min_x \\|Ax-b\\|_2$ using two methods for a square matrix $A$:\n1.  **Normal Equations (NE):** Solve $(A^\\mathsf{T}A)x = A^\\mathsf{T}b$.\n2.  **QR Factorization:** Compute $A=QR$ and solve $Rx = Q^\\mathsf{T}b$.\n\nStandard forward error analysis provides bounds on the relative error $\\frac{\\|\\hat{x}-x_{LS}\\|_2}{\\|x_{LS}\\|_2}$ for the computed solution $\\hat{x}$. The leading-order terms in these bounds characterize the method's stability.\n\nFor the **Normal Equations**, the problem is converted to a linear system involving the matrix $A^\\mathsf{T}A$. The condition number of this matrix is $\\kappa_2(A^\\mathsf{T}A) = \\kappa_2(A)^2$. The forward error bound for a solution computed via a backward stable method (like banded Cholesky factorization) is dominated by this quantity.\n$$ \\left(\\frac{\\|\\hat{x}_{NE} - x_{LS}\\|_2}{\\|x_{LS}\\|_2}\\right)_{\\text{bound}} \\approx C_{NE} \\cdot \\epsilon \\cdot \\kappa_2(A)^2 $$\nwhere $\\epsilon$ is the unit roundoff and $C_{NE}$ is a constant. The dominant term is proportional to $\\kappa_2(A)^2$.\n\nFor the **QR factorization** method using Givens rotations, the method is backward stable. This means the computed solution is the exact solution to a nearby problem. The forward error bound for this method is:\n$$ \\left(\\frac{\\|\\hat{x}_{QR} - x_{LS}\\|_2}{\\|x_{LS}\\|_2}\\right)_{\\text{bound}} \\approx \\epsilon \\left( C_{QR,1} \\kappa_2(A) + C_{QR,2} \\frac{\\|r_{LS}\\|_2}{\\|A\\|_2\\|x_{LS}\\|_2} \\kappa_2(A)^2 \\right) $$\nwhere $r_{LS} = b-Ax_{LS}$ is the least-squares residual.\n\nThe problem asks for a ratio $\\rho(\\kappa_2(A))$ that is a function of $\\kappa_2(A)$ only. This implies we should compare the parts of the error bounds that are independent of the right-hand side $b$ (and thus the residual $r_{LS}$). This is equivalent to considering the case of solving a square, invertible system $Ax=b$, for which the residual $r_{LS}$ is zero. In this case, the second term of the QR bound vanishes.\n\nThus, the dominant error bound for QR is taken to be proportional to $\\kappa_2(A)$:\n$$ \\left(\\frac{\\|\\hat{x}_{QR} - x_{LS}\\|_2}{\\|x_{LS}\\|_2}\\right)_{\\text{bound}} \\approx C_{QR} \\cdot \\epsilon \\cdot \\kappa_2(A) $$\nThe ratio of the dominant forward error bound of the normal equations approach to that of the QR approach is then:\n$$ \\rho(\\kappa_2(A)) = \\frac{C_{NE} \\cdot \\epsilon \\cdot \\kappa_2(A)^2}{C_{QR} \\cdot \\epsilon \\cdot \\kappa_2(A)} $$\nAssuming the constants $C_{NE}$ and $C_{QR}$ are of the same order of magnitude (a standard assumption in this type of analysis), the ratio simplifies to:\n$$ \\rho(\\kappa_2(A)) = \\kappa_2(A) $$\nThis classic result highlights that the normal equations method can be much less stable for ill-conditioned matrices, as it squares the condition number.\n\n### Final Assembly\nThe three required expressions are:\n1.  $F(n,k) = k(k+1)(9n-8k-4)$\n2.  $M(n,k) = n(4k+1) - k(k+1)$\n3.  $\\rho(\\kappa_2(A)) = \\kappa_2(A)$\n\nThese are to be presented as a single row matrix.",
            "answer": "$$ \\boxed{\\begin{pmatrix} k(k+1)(9n-8k-4)  n(4k+1) - k(k+1)  \\kappa_2(A) \\end{pmatrix}} $$"
        }
    ]
}