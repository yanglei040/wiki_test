{
    "hands_on_practices": [
        {
            "introduction": "The numerical stability of LU factorization is critically dependent on the choice of pivots. While Gaussian Elimination with Partial Pivoting (GEPP) is the standard for its robustness, specialized algorithms may use different strategies to optimize for other factors, such as communication costs in parallel computing. This exercise provides a concrete, by-hand analysis comparing GEPP with a tournament pivoting strategy, demonstrating how different valid pivot choices can dramatically alter the element growth factor, a key predictor of stability .",
            "id": "3591199",
            "problem": "Consider the $4 \\times 4$ matrix\n$$\nA \\;=\\;\n\\begin{pmatrix}\n100  10^{6}  0  0 \\\\\n10^{-1}  2  2000  0 \\\\\n90  -1  0  0 \\\\\n5 \\times 10^{-2}  3  0  0\n\\end{pmatrix}.\n$$\nYou will compare pivot selection by Gaussian Elimination with Partial Pivoting (GEPP) against Communication-Avoiding LU (CALU) with tournament pivoting, and then quantify element growth.\n\nUse the following fundamental bases and definitions.\n- The LU factorization with pivoting permutes the rows of $A$ to produce a factorization of the form $PA=LU$, where $P$ is a permutation matrix, $L$ is unit lower triangular, and $U$ is upper triangular. In GEPP, at each column $k$ the pivot row is chosen as the index $p \\geq k$ maximizing the absolute value $|a_{pk}^{(k)}|$, where $a_{ij}^{(k)}$ denotes the entry after $k-1$ elimination steps, and the pivot row is swapped into position $k$ before elimination.\n- In CALU with tournament pivoting on a panel of width $b = 2$ and two row domains $D_1 = \\{1,2\\}$, $D_2 = \\{3,4\\}$, local candidate pivots are selected by performing partial pivoting restricted to each domain on the $2$-column panel, and then a two-level tournament selects the global pivot for each column by comparing only the domain-local candidates. Concretely for this problem:\n  1. Column $1$: Each domain selects its local pivot row (the index in the domain with maximal $|a_{ik}|$ for column $k$). The global pivot for column $1$ is the candidate with the larger magnitude.\n  2. Column $2$ (within the same panel): Within each domain, perform the elimination associated with its own local column-$1$ pivot restricted to its domain rows, then select the domain-local pivot for column $2$ among the remaining row(s) in that domain. The global pivot for column $2$ is chosen by comparing the magnitudes of these two domain-local column-$2$ candidates.\n- The growth factor for a completed LU factorization is\n$$\n\\rho \\;=\\; \\frac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |a_{ij}|},\n$$\nwhere $u_{ij}$ are the entries of the final upper-triangular factor $U$ and $a_{ij}$ are the entries of the original matrix $A$.\n\nTasks:\n1. Apply GEPP to $A$ for the first two columns and state the pivot rows selected at columns $1$ and $2$, justifying from first principles why those rows are chosen.\n2. Apply the above tournament pivoting procedure for CALU to the same $2$-column panel and determine the pivot rows selected at columns $1$ and $2$, showing that they differ from GEPP’s choices.\n3. Complete the elimination implied by the CALU pivot sequence through column $2$ and propagate its effect to column $3$ so as to determine the largest $|u_{ij}|$ produced during the factorization. Compute the CALU growth factor $\\rho_{\\text{CALU}}$ for $A$. Report $\\rho_{\\text{CALU}}$ as a simplified exact fraction; do not round.\n4. Using the standard backward error framework for LU factorization, explain in terms of the growth factor and panel/tournament structure when CALU with tournament pivoting remains backward stable. Your explanation must start from the definition that backward stability means the computed solution is the exact solution of a nearby problem $(A + \\Delta A) x = b$ with $\\|\\Delta A\\|$ bounded by a modest multiple of machine precision and must explicitly relate this bound to the growth factor.\n\nYour final answer must be the single exact value of $\\rho_{\\text{CALU}}$ as a fraction, with no units and no rounding.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a well-defined numerical linear algebra problem asking for the application and analysis of two different LU factorization pivoting strategies, the computation of a growth factor, and a theoretical explanation of numerical stability. All necessary information is provided, and the tasks are unambiguous.\n\nLet the given matrix be $A$.\n$$\nA \\;=\\;\n\\begin{pmatrix}\n100  10^{6}  0  0 \\\\\n10^{-1}  2  2000  0 \\\\\n90  -1  0  0 \\\\\n5 \\times 10^{-2}  3  0  0\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n100  1000000  0  0 \\\\\n0.1  2  2000  0 \\\\\n90  -1  0  0 \\\\\n0.05  3  0  0\n\\end{pmatrix}\n$$\n\n**Task 1: Gaussian Elimination with Partial Pivoting (GEPP)**\n\nFor the first column ($k=1$), we search for the element with the largest magnitude in the first column of $A$. The elements are $a_{11} = 100$, $a_{21} = 0.1$, $a_{31} = 90$, and $a_{41} = 0.05$. The maximum absolute value is $|a_{11}| = 100$.\nTherefore, for column $1$, the pivot row is $1$. No row swap is necessary.\n\nWe proceed with elimination. The multipliers are $l_{21} = \\frac{a_{21}}{a_{11}} = \\frac{0.1}{100} = 10^{-3}$, $l_{31} = \\frac{a_{31}}{a_{11}} = \\frac{90}{100} = 0.9$, and $l_{41} = \\frac{a_{41}}{a_{11}} = \\frac{0.05}{100} = 5 \\times 10^{-4}$. The matrix after the first step of elimination, denoted $A^{(2)}$, is:\n\\begin{align*} R_2' = R_2 - l_{21}R_1 = (0.1, 2, 2000, 0) - 10^{-3}(100, 10^6, 0, 0) = (0, 2 - 1000, 2000, 0) = (0, -998, 2000, 0) \\\\ R_3' = R_3 - l_{31}R_1 = (90, -1, 0, 0) - 0.9(100, 10^6, 0, 0) = (0, -1 - 9 \\times 10^5, 0, 0) = (0, -900001, 0, 0) \\\\ R_4' = R_4 - l_{41}R_1 = (0.05, 3, 0, 0) - 5 \\times 10^{-4}(100, 10^6, 0, 0) = (0, 3 - 500, 0, 0) = (0, -497, 0, 0)\\end{align*}\n$$\nA^{(2)} \\;=\\;\n\\begin{pmatrix}\n100  10^{6}  0  0 \\\\\n0  -998  2000  0 \\\\\n0  -900001  0  0 \\\\\n0  -497  0  0\n\\end{pmatrix}\n$$\nFor the second column ($k=2$), we search for the element with the largest magnitude in the sub-column $a_{i2}^{(2)}$ for $i \\geq 2$. The elements are $a_{22}^{(2)} = -998$, $a_{32}^{(2)} = -900001$, and $a_{42}^{(2)} = -497$. The maximum absolute value is $|a_{32}^{(2)}| = 900001$.\nTherefore, for column $2$, the pivot row is $3$. GEPP would swap rows $2$ and $3$.\n\nThe pivot rows selected by GEPP for columns $1$ and $2$ are row $1$ and row $3$, respectively.\n\n**Task 2: Communication-Avoiding LU (CALU) with Tournament Pivoting**\n\nThe panel width is $b=2$, and the row domains are $D_1 = \\{1,2\\}$ and $D_2 = \\{3,4\\}$.\n\nFor column $1$:\n- In domain $D_1$, we compare $|a_{11}| = 100$ and $|a_{21}| = 0.1$. The local pivot is row $1$ with value $100$.\n- In domain $D_2$, we compare $|a_{31}| = 90$ and $|a_{41}| = 0.05$. The local pivot is row $3$ with value $90$.\n- The tournament compares the local candidates: $|100|$ versus $|90|$. The winner is the candidate from row $1$.\nThe global pivot for column $1$ is row $1$. This is the same choice as GEPP.\n\nFor column $2$:\nThe procedure requires performing local eliminations within each domain using the local column-$1$ pivots to find the column-$2$ candidates.\n- In domain $D_1=\\{1,2\\}$, the local column-$1$ pivot was row $1$ ($a_{11}=100$). We update row $2$ based on row $1$. The multiplier is $m_1 = \\frac{a_{21}}{a_{11}} = \\frac{0.1}{100} = 10^{-3}$. The new second entry of row $2$ is $a_{22} - m_1 a_{12} = 2 - 10^{-3}(10^6) = 2 - 1000 = -998$. The candidate for the column-$2$ pivot from $D_1$ is this value, $-998$, from row $2$.\n- In domain $D_2=\\{3,4\\}$, the local column-$1$ pivot was row $3$ ($a_{31}=90$). We update row $4$ based on row $3$. The multiplier is $m_2 = \\frac{a_{41}}{a_{31}} = \\frac{0.05}{90} = \\frac{5/100}{90} = \\frac{5}{9000} = \\frac{1}{1800}$. The new second entry of row $4$ is $a_{42} - m_2 a_{32} = 3 - \\frac{1}{1800}(-1) = 3 + \\frac{1}{1800} = \\frac{5401}{1800}$. The candidate for the column-$2$ pivot from $D_2$ is this value, $\\frac{5401}{1800}$, from row $4$.\n- The tournament for column $2$ compares the candidates from each domain: $|-998|$ versus $|\\frac{5401}{1800}| \\approx 3.0$. The value $|-998|=998$ is larger.\nThe global pivot for column $2$ is row $2$.\n\nThe pivot rows selected by CALU for columns $1$ and $2$ are row $1$ and row $2$. This differs from GEPP's choice for the second pivot (row $3$).\n\n**Task 3: CALU Growth Factor Calculation**\n\nThe pivot sequence for CALU is (row $1$, row $2$). This implies no permutations are performed for the first two steps. The elimination for column $1$ using pivot $a_{11}=100$ is identical to the first step of GEPP, yielding the matrix $A^{(2)}$ calculated above.\n$$\nA^{(2)} \\;=\\;\n\\begin{pmatrix}\n100  10^{6}  0  0 \\\\\n0  -998  2000  0 \\\\\n0  -900001  0  0 \\\\\n0  -497  0  0\n\\end{pmatrix}\n$$\nNext, we perform elimination for column $2$ using the CALU-selected pivot $a_{22}^{(2)}=-998$.\nThe multipliers are $l_{32} = \\frac{a_{32}^{(2)}}{a_{22}^{(2)}} = \\frac{-900001}{-998} = \\frac{900001}{998}$ and $l_{42} = \\frac{a_{42}^{(2)}}{a_{22}^{(2)}} = \\frac{-497}{-998} = \\frac{497}{998}$.\n\nWe update rows $3$ and $4$ to get the matrix $A^{(3)}$. We only need to consider columns $j \\ge 3$.\nFor row $3$: $a_{33}^{(3)} = a_{33}^{(2)} - l_{32} a_{23}^{(2)} = 0 - \\frac{900001}{998} (2000) = - \\frac{1800002000}{998} = - \\frac{900001000}{499}$.\n$a_{34}^{(3)} = a_{34}^{(2)} - l_{32} a_{24}^{(2)} = 0 - l_{32}(0) = 0$.\nFor row $4$: $a_{43}^{(3)} = a_{43}^{(2)} - l_{42} a_{23}^{(2)} = 0 - \\frac{497}{998} (2000) = - \\frac{994000}{998} = - \\frac{497000}{499}$.\n$a_{44}^{(3)} = a_{44}^{(2)} - l_{42} a_{24}^{(2)} = 0 - l_{42}(0) = 0$.\n\nThe final upper triangular matrix $U$ is obtained after one more elimination step on the $2 \\times 2$ submatrix in the bottom right, though it is clear the fourth column is zero. We can write out the matrix before the final step:\n$$\nA^{(3)} \\;=\\;\n\\begin{pmatrix}\n100  10^{6}  0  0 \\\\\n0  -998  2000  0 \\\\\n0  0  - \\frac{900001000}{499}  0 \\\\\n0  0  - \\frac{497000}{499}  0\n\\end{pmatrix}\n$$\nThe remaining elimination step for column $3$ will make $u_{43}$ zero, and all other entries shown are final entries of $U$.\nThe entries of the final matrix $U$ are the non-zero elements shown and zeros. We find the maximum magnitude entry in $U$.\n$|u_{11}| = 100$\n$|u_{12}| = 10^6$\n$|u_{22}| = 998$\n$|u_{23}| = 2000$\n$|u_{33}| = \\frac{900001000}{499} \\approx 1.803 \\times 10^6$.\nThe maximum magnitude entry in $U$ is $|u_{33}| = \\frac{900001000}{499}$.\n\nThe growth factor is $\\rho_{\\text{CALU}} = \\frac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |a_{ij}|}$.\nThe maximum magnitude entry in the original matrix $A$ is $|a_{12}| = 10^6$.\n$$\n\\rho_{\\text{CALU}} = \\frac{\\frac{900001000}{499}}{10^6} = \\frac{900001000}{499 \\times 10^6} = \\frac{900.001}{499} = \\frac{900001}{499000}\n$$\nThe number $499$ is prime. $900001 = 1803 \\times 499 + 304$, so $900001$ is not divisible by $499$. The fraction is in simplest form.\n\n**Task 4: Backward Stability of CALU**\n\nBackward stability of a numerical algorithm for solving $Ax=b$ means that the computed solution $\\hat{x}$ is the exact solution to a nearby problem, $(A + \\Delta A)\\hat{x} = b$, where the perturbation matrix $\\Delta A$ is small relative to $A$. Specifically, the norm of the perturbation is bounded, e.g., $\\|\\Delta A\\| \\le C \\cdot u \\cdot \\text{poly}(n) \\cdot \\|A\\|$, where $u$ is the machine precision, $n$ is the matrix dimension, and $C$ is a constant.\n\nFor solvers based on LU factorization, the standard backward error bound is directly proportional to the growth factor $\\rho$. A common bound is $\\|\\Delta A\\|_{\\infty} \\le n u (3\\|A\\|_{\\infty} + 5\\||L| |U|\\|_{\\infty}) + O(u^2)$. Since $\\| |L| |U| \\|_{\\infty}$ contains the entries of $U$, this bound can be related to $\\rho$. A simpler and more direct form states that the relative error is bounded by a term proportional to $\\rho$: $\\|\\Delta A\\| / \\|A\\| \\le \\gamma_n \\rho u$, where $\\gamma_n$ is a low-degree polynomial in $n$.\n\nFor an LU algorithm to be backward stable, the growth factor $\\rho$ must be small (i.e., of modest size, not growing exponentially with $n$).\n\nThe panel and tournament structure of CALU restricts the set of potential pivots at each step. Unlike GEPP, which searches the entire remaining column for the pivot, tournament pivoting only compares a small number of candidates, one from each domain. This is done to reduce communication costs in parallel computing environments. However, this restriction means that the algorithm can fail to select the globally largest pivot available.\n\nAs demonstrated in this problem, CALU's tournament for column $2$ chose row $2$ as the pivot, with value $-998$. This ignored the much larger entry of $-900001$ in row $3$ because it was outside the \"winning\" domain's local search. This suboptimal pivot choice led to a large multiplier, $l_{32} = \\frac{900001}{998} \\gg 1$. This large multiplier then caused significant element growth when it operated on the entry $a_{23}^{(2)}=2000$, creating the large entry $u_{33} \\approx -1.8 \\times 10^6$. The resulting growth factor $\\rho_{\\text{CALU}} \\approx 1.8$ is larger than the factor $\\rho_{\\text{GEPP}}=1$ that would have resulted from the GEPP pivot choice.\n\nCALU with tournament pivoting remains backward stable as long as this trade-off between communication and stability does not lead to large growth factors. The stability hinges on the hope that the local pivots selected within each domain are also reasonably good global pivots. If the matrix structure is such that large potential pivots are systematically ignored by the tournament, $\\rho$ can become large, and the term $\\gamma_n \\rho u$ can exceed a small multiple of machine precision, thus violating the condition for backward stability. In essence, CALU's stability depends on the numerical properties of the matrix not interacting poorly with the fixed domain structure.",
            "answer": "$$\\boxed{\\frac{900001}{499000}}$$"
        },
        {
            "introduction": "In scientific computing, we often work with large, sparse matrices where preserving structure is as important as maintaining numerical stability. Strict partial pivoting can destroy sparsity by inducing row swaps that cause \"fill-in,\" increasing memory and computational costs. This computational exercise explores the practical trade-off between stability and sparsity by implementing threshold pivoting, a strategy that forgoes the largest possible pivot to avoid disruptive row interchanges, thereby preserving the matrix's sparse structure .",
            "id": "3591254",
            "problem": "Consider the construction, factorization, and analysis of sparse matrices under different pivoting strategies within the Lower-Upper (LU) factorization framework. The foundational base for this problem is the definition of LU factorization of a nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$ into a unit lower-triangular factor $L$ and an upper-triangular factor $U$, together with partial pivoting, which selects at each step the largest magnitude entry in the current column as the pivot, and threshold pivoting, which accepts the diagonal entry as the pivot if it satisfies a magnitude threshold condition relative to the largest entry in the column.\n\nYou must algorithmically construct a family of sparse matrices $A$ using the following principle. Begin with a banded matrix that has $1$-entries on the main diagonal and small entries on the first sub- and super-diagonals to ensure nonsingularity and numerical stability. Then, inject large-magnitude “rogue” entries far from the diagonal in carefully chosen columns to force strict partial pivoting to select off-diagonal pivots and thereby trigger substantial fill-in by row interchanges, whereas threshold pivoting with a suitable threshold parameter $\\tau$ can preserve sparsity by favoring diagonal pivots when they are of acceptable magnitude.\n\nMatrix construction requirements:\n- For given $n \\in \\mathbb{N}$, construct $A \\in \\mathbb{R}^{n \\times n}$ with the pattern\n  - $A_{i,i} = 1$ for all $i$, which should be implemented as entries of magnitude $1$ on the diagonal,\n  - $A_{i,i+1} = \\epsilon$ and $A_{i+1,i} = \\epsilon$ for all valid $i$, which should be implemented as a tridiagonal band with small off-diagonal magnitude $\\epsilon$,\n  - For a specified set of columns $J \\subset \\{0,1,\\dots,n-1\\}$ and a mapping $r: J \\to \\{0,1,\\dots,n-1\\}$ with $r(j)$ far from $j$, set $A_{r(j),\\, j} = M$, where $M$ is a large magnitude relative to $1$ and $\\epsilon$.\n\nPivoting strategies to be executed:\n- Strict partial pivoting: This corresponds to threshold pivoting with parameter $\\tau = 1$, meaning the diagonal is chosen as pivot only if it equals the column maximum in magnitude; otherwise, the largest off-diagonal entry is chosen.\n- Threshold pivoting: Use a given $\\tau \\in [0,1]$ that accepts the diagonal as the pivot if its magnitude is at least $\\tau$ times the largest magnitude entry in the column; otherwise, use the largest magnitude entry.\n\nFor each matrix, perform LU factorization under both strategies using natural column ordering (that is, no fill-reducing preordering). From the factors, compute:\n- The fill ratio $f := \\dfrac{\\operatorname{nnz}(L) + \\operatorname{nnz}(U)}{\\operatorname{nnz}(A)}$, where $\\operatorname{nnz}(\\cdot)$ denotes the number of nonzero entries.\n- The element growth factor $\\rho := \\dfrac{\\max_{i,j} |U_{i,j}|}{\\max_{i,j} |A_{i,j}|}$, which quantifies the largest magnitude increase between the original matrix and the upper-triangular factor.\n\nYour program must, for each test case, return two boolean results:\n- Does threshold pivoting reduce fill compared to strict partial pivoting? That is, is $f_{\\text{threshold}}  f_{\\text{partial}}$?\n- Is the growth under threshold pivoting acceptable? That is, is $\\rho_{\\text{threshold}} \\le g$, for a specified bound $g  0$?\n\nImplementation constraints:\n- Use Compressed Sparse Column (CSC) storage for $A$ and perform LU factorization with natural ordering. The factorization must respect the specified pivoting threshold $\\tau$.\n\nTest suite:\n- Case $1$ (general “happy path”): $n = 50$, $\\epsilon = 0.05$, $J = \\{0,1,2,3,4,5,6,7,8,9\\}$, $r(j) = 49 - j$, $M = 100$, $\\tau = 0.01$, $g = 500$.\n- Case $2$ (boundary for strict partial pivoting): $n = 50$, $\\epsilon = 0.05$, $J = \\{0,1,2,3,4,5,6,7,8,9\\}$, $r(j) = 49 - j$, $M = 100$, $\\tau = 1.0$, $g = 500$.\n- Case $3$ (smaller dimension, moderate rogue entries): $n = 12$, $\\epsilon = 0.02$, $J = \\{0,1,2,3,4,5\\}$, $r(j) = 11 - j$, $M = 50$, $\\tau = 0.05$, $g = 250$.\n- Case $4$ (edge case of always-diagonal pivoting): $n = 20$, $\\epsilon = 0.02$, $J = \\{0,1,2,3,4,5,6,7\\}$, $r(j) = 19 - j$, $M = 500$, $\\tau = 0.0$, $g = 5000$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list of booleans in the order described above. For example, the output must look like $[\\,[b_{1}^{(1)},b_{2}^{(1)}],\\,[b_{1}^{(2)},b_{2}^{(2)}],\\,\\dots\\,]$, with no additional text.",
            "solution": "This problem explores the crucial trade-off between numerical stability and sparsity preservation in the LU factorization of sparse matrices. It contrasts two pivoting strategies: strict partial pivoting, which maximizes stability, and threshold pivoting, which attempts to balance stability with the goal of minimizing \"fill-in\"—the creation of new nonzero entries in the LU factors.\n\nThe LU factorization decomposes a matrix $A$ into $PA=LU$, where $P$ is a permutation matrix representing row swaps, $L$ is unit lower triangular, and $U$ is upper triangular. For sparse matrices, the goal is to keep $L$ and $U$ as sparse as possible.\n\n**Pivoting Strategies and Their Consequences**\n\n1.  **Strict Partial Pivoting ($\\tau=1$)**: At each elimination step $k$, this strategy selects the largest-magnitude entry in the current column (on or below the diagonal) as the pivot. This guarantees that all multipliers in the $L$ factor have a magnitude of at most 1, which helps control element growth and ensures numerical stability. However, the required row interchanges can destroy sparsity and cause significant fill-in.\n\n2.  **Threshold Pivoting ($\\tau \\in [0, 1)$)**: This is a more flexible strategy. At step $k$, it identifies the largest-magnitude entry in the current column, $|A_{p,k}^{(k)}|$. The diagonal entry $A_{k,k}^{(k)}$ is accepted as the pivot if it is \"large enough\" relative to this maximum, specifically if $|A_{k,k}^{(k)}| \\ge \\tau \\cdot |A_{p,k}^{(k)}|$. If this condition holds, no row swap is performed, thus preserving sparsity. If it fails, a row swap with row $p$ is performed, as in partial pivoting. A smaller $\\tau$ makes it easier to accept a diagonal pivot, prioritizing sparsity at the risk of choosing a smaller pivot, which can lead to numerical instability.\n\n**Matrix Construction and Analysis**\n\nThe problem uses a specially constructed sparse matrix to highlight this trade-off. The matrix has a simple tridiagonal structure with 1s on the diagonal, but it is seeded with large \"rogue\" entries $M$ placed far from the diagonal. In a column $j$ containing a rogue entry, the diagonal element is $A_{j,j}=1$, while the largest element is $A_{r(j),j}=M$.\n\n-   With **strict partial pivoting ($\\tau=1$)**, the condition to avoid a swap is $1 \\ge 1 \\cdot M$, which is false. A row swap is mandatory, leading to high fill-in.\n-   With **threshold pivoting**, the condition is $1 \\ge \\tau \\cdot M$. If $\\tau$ is small enough (e.g., Case 1: $1 \\ge 0.01 \\cdot 100 = 1$), the condition holds, the swap is avoided, and sparsity is preserved. If $\\tau$ is too large (e.g., Case 3: $1 \\ge 0.05 \\cdot 50 = 2.5$), the condition fails, and the behavior becomes identical to strict partial pivoting.\n\nThe analysis is performed using two metrics:\n1.  **Fill Ratio ($f$)**: Measures the increase in nonzero elements. We expect $f_{\\text{threshold}}  f_{\\text{partial}}$ whenever thresholding successfully avoids pivoting.\n2.  **Growth Factor ($\\rho$)**: Measures the growth of element magnitudes, a proxy for numerical stability. When a small diagonal pivot is chosen, the multiplier used for elimination is large (e.g., $M/1 = M$). This can cause element growth. In this problem's specific matrix structure, the largest new element created is of magnitude $M \\cdot \\epsilon$. The growth factor is thus $\\rho \\approx |M\\epsilon|/M = |\\epsilon|$, which remains small.\n\n**Algorithmic Implementation**\n\nThe solution involves programming the following steps for each test case:\n1.  Construct the matrix $A$ in a sparse format (CSC).\n2.  Factorize $A$ using strict partial pivoting ($\\tau=1$) and calculate the fill ratio $f_{\\text{partial}}$.\n3.  Factorize $A$ using the specified threshold $\\tau$ and calculate the fill ratio $f_{\\text{threshold}}$ and growth factor $\\rho_{\\text{threshold}}$.\n4.  Evaluate the two boolean conditions: $f_{\\text{threshold}}  f_{\\text{partial}}$ and $\\rho_{\\text{threshold}} \\le g$.\n\nThe provided Python code implements this procedure using the `scipy.sparse.linalg.splu` function, which allows fine-grained control over pivoting via the `diag_pivot_thresh` parameter, corresponding to $\\tau$. The `permc_spec='NATURAL'` option is used to disable column reordering, as required.",
            "answer": "```python\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import splu\n\n# Set the environment according to the problem statement.\n# language: Python, version: 3.12\n# libraries: numpy==1.23.5, scipy==1.11.4\n\ndef run_factorization_analysis(n, epsilon, J, r_map, M, tau_thresh, g):\n    \"\"\"\n    Constructs a sparse matrix and analyzes its LU factorization under two\n    pivoting strategies.\n\n    Args:\n        n (int): Dimension of the matrix.\n        epsilon (float): Magnitude of off-diagonal entries in the tridiagonal band.\n        J (set): Set of column indices for rogue entries.\n        r_map (callable): Function mapping a column index j to a row index r(j).\n        M (float): Magnitude of the rogue entries.\n        tau_thresh (float): Threshold for threshold pivoting.\n        g (float): Growth factor bound.\n\n    Returns:\n        list[bool, bool]: A list of two booleans:\n                           (reduces_fill, acceptable_growth)\n    \"\"\"\n    # 1. Construct the matrix A using LIL format for efficient construction.\n    A = sp.lil_matrix((n, n), dtype=np.float64)\n\n    # Add tridiagonal band\n    for i in range(n):\n        A[i, i] = 1.0\n        if i  n - 1:\n            A[i, i + 1] = epsilon\n            A[i + 1, i] = epsilon\n    \n    # Add rogue entries\n    for j in J:\n        A[r_map(j), j] = M\n\n    # Convert to CSC format for factorization\n    A_csc = A.tocsc()\n    nnz_A = A_csc.nnz\n    max_A = M  # Max magnitude in A is M by construction\n\n    # 2. Factorize with strict partial pivoting (tau = 1.0)\n    # The 'NATURAL' permutation spec ensures no column reordering.\n    lu_partial = splu(A_csc, permc_spec='NATURAL', diag_pivot_thresh=1.0)\n    nnz_partial = lu_partial.L.nnz + lu_partial.U.nnz\n    f_partial = nnz_partial / nnz_A\n\n    # 3. Factorize with threshold pivoting (given tau)\n    lu_threshold = splu(A_csc, permc_spec='NATURAL', diag_pivot_thresh=tau_thresh)\n    nnz_threshold = lu_threshold.L.nnz + lu_threshold.U.nnz\n    f_threshold = nnz_threshold / nnz_A\n    \n    # Find max element in U for growth factor calculation.\n    # U.data can contain explicit zeros, so filter them out if necessary,\n    # though max(abs(..)) handles it. If U.data is empty, max would fail.\n    max_U_thresh = 0.0\n    if lu_threshold.U.data.size > 0:\n        max_U_thresh = np.max(np.abs(lu_threshold.U.data))\n    \n    rho_threshold = max_U_thresh / max_A\n\n    # 4. Evaluate the boolean conditions\n    reduces_fill = f_threshold  f_partial\n    acceptable_growth = rho_threshold = g\n\n    return [reduces_fill, acceptable_growth]\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the LU factorization problem.\n    \"\"\"\n    test_cases = [\n        # Case 1: General \"happy path\"\n        {'n': 50, 'epsilon': 0.05, 'J': set(range(10)), 'r_map': lambda j: 49 - j, \n         'M': 100, 'tau': 0.01, 'g': 500},\n        # Case 2: Boundary for strict partial pivoting\n        {'n': 50, 'epsilon': 0.05, 'J': set(range(10)), 'r_map': lambda j: 49 - j, \n         'M': 100, 'tau': 1.0, 'g': 500},\n        # Case 3: Smaller dimension, moderate rogue entries, threshold too high\n        {'n': 12, 'epsilon': 0.02, 'J': set(range(6)), 'r_map': lambda j: 11 - j, \n         'M': 50, 'tau': 0.05, 'g': 250},\n        # Case 4: Edge case of always-diagonal pivoting (no pivoting)\n        {'n': 20, 'epsilon': 0.02, 'J': set(range(8)), 'r_map': lambda j: 19 - j, \n         'M': 500, 'tau': 0.0, 'g': 5000},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_factorization_analysis(\n            n=case['n'],\n            epsilon=case['epsilon'],\n            J=case['J'],\n            r_map=case['r_map'],\n            M=case['M'],\n            tau_thresh=case['tau'],\n            g=case['g']\n        )\n        results.append(result)\n\n    # Format the final output string exactly as required.\n    # e.g., [[true,true],[false,true],...]\n    formatted_results = []\n    for res_pair in results:\n        s = f\"[{str(res_pair[0]).lower()},{str(res_pair[1]).lower()}]\"\n        formatted_results.append(s)\n    \n    final_string = f\"[{','.join(formatted_results)}]\"\n    print(final_string)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond solving linear systems, matrix factorizations can reveal deeper properties of a matrix, such as its inertia—the counts of its positive, negative, and zero eigenvalues. A common misconception is that the signs of the pivots in the standard $PA=LU$ factorization provide this information. This conceptual practice clarifies why this is generally not true, contrasting the standard LU factorization with the symmetric indefinite factorization $P^TAP=LDL^T$, which, as a congruence transformation, correctly preserves and reveals the inertia of a symmetric matrix .",
            "id": "3591255",
            "problem": "Consider a real matrix $A\\in\\mathbb{R}^{n\\times n}$ and the following factorizations: the row-pivoted lower–upper factorization $PA=LU$, where $P$ is a permutation matrix, $L$ is unit lower triangular, and $U$ is upper triangular; and, when $A$ is symmetric, the symmetric indefinite factorization with symmetric pivoting $P^TAP=LDL^T$, where $P$ is a permutation matrix, $L$ is unit lower triangular, and $D$ is block diagonal with $1\\times 1$ and $2\\times 2$ symmetric blocks. The inertia of a real symmetric matrix $A$, denoted by $\\operatorname{In}(A)$, is the triple $(n_+(A),\\,n_-(A),\\,n_0(A))$ counting the number of positive, negative, and zero eigenvalues of $A$, respectively. Sylvester’s law of inertia states that $\\operatorname{In}(A)=\\operatorname{In}(X^TAX)$ for any nonsingular $X\\in\\mathbb{R}^{n\\times n}$, and, in particular, for any permutation matrix $P$ one has $\\operatorname{In}(A)=\\operatorname{In}(P^TAP)$. In contrast, left multiplication $PA$ is not a congruence and does not, in general, preserve inertia.\n\nInvestigate whether the signs of the diagonal pivots $U_{ii}$ in $PA=LU$ can be used to infer the inertia of $A$ for certain structured matrices, and clarify the limitations of this approach compared to $LDL^T$ with symmetric pivoting. You may use the following illustrative matrices for your reasoning:\n- $A_1=\\begin{bmatrix}2  1\\\\ 1  2\\end{bmatrix}$, which is symmetric positive definite (SPD).\n- $A_2=\\begin{bmatrix}0  1\\\\ 1  0\\end{bmatrix}$, which is symmetric indefinite with $\\operatorname{In}(A_2)=(1,1,0)$.\n\nSelect all statements that are correct:\n\nA. For any real symmetric positive definite matrix $A$, if Gaussian elimination without row pivoting succeeds so that $A=LU$ with $L$ unit lower triangular (i.e., $P=I$), then every diagonal pivot $U_{ii}$ is positive, and this suffices to infer $\\operatorname{In}(A)=(n,0,0)$.\n\nB. For a real symmetric indefinite matrix $A$ factorized with partial pivoting $PA=LU$, the number of negative diagonal entries among $\\{U_{ii}\\}_{i=1}^n$ equals $n_-(A)$.\n\nC. For a real symmetric matrix $A$ factorized via symmetric pivoting as $P^TAP=LDL^T$, the inertia of $A$ equals the inertia of $D$, which is the sum of the inertias of its $1\\times 1$ and $2\\times 2$ blocks.\n\nD. If $P\\neq I$ in a row-pivoted $PA=LU$, then the sequence of signs of $\\{U_{ii}\\}_{i=1}^n$ reflects the signs of the leading principal minors of $PA$ (the permuted matrix), not of $A$, and therefore cannot, in general, be used to infer $\\operatorname{In}(A)$.\n\nE. In an $LDL^T$ factorization that uses $2\\times 2$ pivots in $D$, it suffices to inspect only the signs of the diagonal entries of each $2\\times 2$ block of $D$ to determine that block’s contribution to the inertia of $A$.",
            "solution": "The problem statement is found to be valid. It presents standard definitions and principles from numerical linear algebra, including the $LU$ and $LDL^T$ factorizations, the concept of matrix inertia, and Sylvester's law of inertia. The problem is well-posed, objective, and scientifically grounded, asking for an evaluation of several statements based on these principles.\n\nWe proceed to evaluate each statement individually.\n\n**A. For any real symmetric positive definite matrix $A$, if Gaussian elimination without row pivoting succeeds so that $A=LU$ with $L$ unit lower triangular (i.e., $P=I$), then every diagonal pivot $U_{ii}$ is positive, and this suffices to infer $\\operatorname{In}(A)=(n,0,0)$.**\n\nThis statement consists of two claims.\n\nFirst, if $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD), then all its leading principal minors are positive. The pivots $U_{ii}$ of the $LU$ factorization (when it exists without pivoting) are given by the formula $U_{kk} = \\det(A_k) / \\det(A_{k-1})$ for $k1$, and $U_{11} = \\det(A_1)$, where $A_k$ is the $k \\times k$ leading principal submatrix of $A$. Since $A$ is SPD, $\\det(A_k)  0$ for all $k \\in \\{1, \\dots, n\\}$. Therefore, every pivot $U_{kk}$ must be positive. It is a known property of SPD matrices that Gaussian elimination can proceed without pivoting. So the first part of the statement is correct.\n\nSecond, we must assess if the positivity of the pivots is sufficient to infer that $A$ is SPD, i.e., $\\operatorname{In}(A)=(n,0,0)$. Assume $A$ is a symmetric matrix and its factorization $A=LU$ exists, where $L$ is unit lower triangular and $U$ is upper triangular with all diagonal entries $U_{ii}  0$. We can write $U = DU'$, where $D = \\operatorname{diag}(U_{11}, \\dots, U_{nn})$ and $U'$ is a unit upper triangular matrix. The factorization becomes $A=LDU'$. Since $A$ is symmetric, $A=A^T$, which implies $LDU' = (LDU')^T = (U')^T D^T L^T$. Since $D$ is diagonal, $D=D^T$. So, $LDU' = (U')^T D L^T$. The LDU factorization of a nonsingular symmetric matrix (if it exists) is unique. This uniqueness implies that $L = (U')^T$, and consequently $U' = L^T$.\nThus, the factorization is $A=LDL^T$. This expresses $A$ as a congruence transformation of $D$. By Sylvester's law of inertia, $\\operatorname{In}(A) = \\operatorname{In}(D)$. The matrix $D$ is diagonal with all entries $U_{ii}  0$. The inertia of a diagonal matrix is determined by the signs of its diagonal entries. Since all diagonal entries of $D$ are positive, $\\operatorname{In}(D)=(n,0,0)$. Therefore, $\\operatorname{In}(A)=(n,0,0)$, which means $A$ is SPD. The statement holds in both directions.\n\nTherefore, the statement is **Correct**.\n\n**B. For a real symmetric indefinite matrix $A$ factorized with partial pivoting $PA=LU$, the number of negative diagonal entries among $\\{U_{ii}\\}_{i=1}^n$ equals $n_-(A)$.**\n\nThis statement is incorrect. The factorization $PA=LU$ involves a premultiplication by a permutation matrix $P$, which is not a congruence transformation. Thus, the inertia of $A$ is not preserved in the matrix $PA$. The pivots $U_{ii}$ are related to the leading principal minors of $PA$, not of $A$.\n\nLet's use the provided counterexample $A_2=\\begin{bmatrix}0  1\\\\ 1  0\\end{bmatrix}$. The eigenvalues $\\lambda$ of $A_2$ satisfy $\\lambda^2 - 1 = 0$, so $\\lambda = \\pm 1$. The inertia is $\\operatorname{In}(A_2)=(1,1,0)$, thus $n_-(A_2)=1$.\nTo compute the $PA=LU$ factorization, we note that the top-left entry $A_{11}=0$. This requires a row interchange. The partial pivoting strategy swaps row $1$ and row $2$. The permutation matrix is $P=\\begin{bmatrix}0  1\\\\ 1  0\\end{bmatrix}$.\nThen, $PA_2 = \\begin{bmatrix}0  1\\\\ 1  0\\end{bmatrix} \\begin{bmatrix}0  1\\\\ 1  0\\end{bmatrix} = \\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix} = I$.\nThe $LU$ factorization of the identity matrix $I$ is trivial: $L=I$ and $U=I$.\nThe diagonal entries of $U$ are $\\{U_{11}, U_{22}\\} = \\{1,1\\}$. The number of negative diagonal entries is $0$. This does not match $n_-(A_2)=1$.\n\nTherefore, the statement is **Incorrect**.\n\n**C. For a real symmetric matrix $A$ factorized via symmetric pivoting as $P^TAP=LDL^T$, the inertia of $A$ equals the inertia of $D$, which is the sum of the inertias of its $1\\times 1$ and $2\\times 2$ blocks.**\n\nThis statement involves two parts.\nFirst, we relate the inertia of $A$ to the inertia of $D$. The given factorization is $P^TAP=LDL^T$. A permutation matrix $P$ is always nonsingular. According to Sylvester's law of inertia, for any nonsingular matrix $X$, $\\operatorname{In}(A) = \\operatorname{In}(X^T A X)$. Applying this with $X=P$ (note $P^T=P^{-1}$ for a permutation matrix), we get $\\operatorname{In}(A) = \\operatorname{In}(P^T A P)$.\nNext, we relate $\\operatorname{In}(P^TAP)$ to $\\operatorname{In}(D)$. From the factorization $P^TAP=LDL^T$, we can write $D=L^{-1}(P^TAP)(L^T)^{-1} = L^{-1}(P^TAP)(L^{-1})^T$. Since $L$ is unit lower triangular, it is nonsingular, and so is $L^{-1}$. This shows that $D$ is congruent to $P^TAP$. Again by Sylvester's law of inertia, $\\operatorname{In}(P^TAP) = \\operatorname{In}(D)$.\nCombining both results, we have $\\operatorname{In}(A) = \\operatorname{In}(P^TAP) = \\operatorname{In}(D)$.\n\nSecond, we consider the inertia of the block diagonal matrix $D$. Let $D = \\operatorname{diag}(D_1, D_2, \\dots, D_k)$, where each $D_j$ is a $1 \\times 1$ or $2 \\times 2$ block. The set of eigenvalues of a block diagonal matrix is the union of the sets of eigenvalues of its diagonal blocks. Consequently, the number of positive, negative, and zero eigenvalues of $D$ is the sum of the respective numbers for each block $D_j$. That is, $n_+(D) = \\sum_j n_+(D_j)$, $n_-(D) = \\sum_j n_-(D_j)$, and $n_0(D) = \\sum_j n_0(D_j)$. This is equivalent to $\\operatorname{In}(D) = \\sum_j \\operatorname{In}(D_j)$.\n\nBoth parts of the statement are correct. This principle is the foundation of the symmetric indefinite factorization method for computing inertia.\n\nTherefore, the statement is **Correct**.\n\n**D. If $P\\neq I$ in a row-pivoted $PA=LU$, then the sequence of signs of $\\{U_{ii}\\}_{i=1}^n$ reflects the signs of the leading principal minors of $PA$ (the permuted matrix), not of $A$, and therefore cannot, in general, be used to infer $\\operatorname{In}(A)$.**\n\nLet $B = PA$. The standard unpivoted $LU$ factorization of $B$ is $B=LU$. The diagonal pivots are given by $U_{11} = B_{11}$ and $U_{kk} = \\det(B_k) / \\det(B_{k-1})$ for $k1$, where $B_k$ is the $k \\times k$ leading principal submatrix of $B$. The sign of each $U_{kk}$ is therefore determined by the signs of the determinants of the leading principal minors of $B=PA$. This confirms the first part of the statement.\n\nThe second part states that this information cannot, in general, be used to infer $\\operatorname{In}(A)$. As established in the analysis of option B, the matrix $PA$ is not a congruence transformation of $A$, and thus $\\operatorname{In}(PA)$ is not necessarily equal to $\\operatorname{In}(A)$. For a symmetric matrix $A$, its inertia is related by Jacobi's criterion to the signs of the leading principal minors of $A$ itself (if they are all non-zero). The pivots of $PA=LU$ relate to the minors of $PA$. Since the properties of $PA$ (including its inertia and the signs of its minors) can be very different from those of $A$, the signs of the pivots $U_{ii}$ cannot be reliably used to find $\\operatorname{In}(A)$. The counterexample from option B, where $A_2=\\begin{bmatrix}0  1\\\\ 1  0\\end{bmatrix}$ has $\\operatorname{In}(A_2)=(1,1,0)$ but $PA_2=I$ and the pivots are $\\{1,1\\}$, perfectly illustrates this general principle.\n\nTherefore, the statement is **Correct**.\n\n**E. In an $LDL^T$ factorization that uses $2\\times 2$ pivots in $D$, it suffices to inspect only the signs of the diagonal entries of each $2\\times 2$ block of $D$ to determine that block’s contribution to the inertia of $A$.**\n\nThe contribution of a block $D_j$ to the inertia of $A$ is, by statement C, simply the inertia of that block, $\\operatorname{In}(D_j)$. The question is whether we can determine $\\operatorname{In}(D_j)$ for a $2\\times 2$ block just from the signs of its diagonal entries.\nLet the block be a symmetric matrix $D_j = \\begin{bmatrix} a  c \\\\ c  b \\end{bmatrix}$. Its inertia is given by the signs of its eigenvalues. The eigenvalues are the roots of the characteristic equation $\\lambda^2 - (a+b)\\lambda + (ab-c^2) = 0$. The signs of the eigenvalues are determined by their sum (the trace, $a+b$) and their product (the determinant, $ab-c^2$). Inspecting the signs of $a$ and $b$ alone is not sufficient.\n\nConsider the $2\\times 2$ block $D_j = \\begin{bmatrix} 1  2 \\\\ 2  1 \\end{bmatrix}$. The diagonal entries are both positive (signs are $\\{+,+\\}$). The determinant is $(1)(1) - (2)^2 = 1 - 4 = -3$. Since the determinant is negative, the eigenvalues must have opposite signs (one positive, one negative). Indeed, the eigenvalues are $\\lambda_1=3$ and $\\lambda_2=-1$. The inertia of this block is $\\operatorname{In}(D_j) = (1,1,0)$. Knowing only that the diagonal entries were positive would not allow us to deduce the existence of a negative eigenvalue. For example, the block $\\begin{bmatrix} 3  1 \\\\ 1  3 \\end{bmatrix}$ also has positive diagonal entries, but its determinant is $9-1=8 0$, and trace is $60$, so both its eigenvalues are positive and its inertia is $(2,0,0)$.\n\nThe signs of the diagonal entries are insufficient; one must consider the off-diagonal entries (via the determinant) to correctly determine the inertia of a $2 \\times 2$ block.\n\nTherefore, the statement is **Incorrect**.",
            "answer": "$$\\boxed{ACD}$$"
        }
    ]
}