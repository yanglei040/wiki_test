{
    "hands_on_practices": [
        {
            "introduction": "This first exercise takes us back to fundamentals, asking you to derive the normal equations directly from the minimization principle using multivariable calculus. By applying this to a concrete rank-deficient system , you will see how a singular matrix $A^T A$ leads to an infinite set of least squares solutions. This practice is crucial for building a solid foundation on the algebraic conditions that govern the uniqueness of minimizers.",
            "id": "3544780",
            "problem": "Consider the following explicit construction in the Euclidean space $ \\mathbb{R}^3 $. Let\n$$\nA \\in \\mathbb{R}^{3 \\times 2}, \\quad A \\;=\\; \\begin{pmatrix}\n1  2 \\\\\n2  4 \\\\\n3  6\n\\end{pmatrix}, \\qquad b \\in \\mathbb{R}^3, \\quad b \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe goal is to analyze the existence and uniqueness of least squares solutions for the linear system $ A x \\approx b $ by reasoning from first principles.\n\nTasks:\n1. Starting from the definition of a least squares solution as a minimizer of the function $ f(x) = \\| A x - b \\|_{2}^{2} $, derive the necessary and sufficient optimality condition linking stationarity of $ f $ to an algebraic system in $ x $. Do not assume any pre-stated formula; derive the condition using only multivariable calculus and basic properties of transposes.\n2. Compute $ A^T A $ and prove from first principles that it is singular. Explain the implications of this singularity for the solution set of the algebraic system derived in Task 1, including both existence and uniqueness. Give the general form of the full solution set in this explicit instance.\n3. Explain why least squares minimizers exist in this instance even though $ A^T A $ is singular. Identify the unique minimizer of minimal Euclidean norm among all least squares minimizers, justifying your choice by an explicit calculation from first principles.\n4. Compute the minimal possible value of $ \\| A x - b \\|_{2}^{2} $ over all $ x \\in \\mathbb{R}^{2} $. Express your final answer as an exact rational number. No rounding is required.\n\nYour final response should be the single numerical value requested in Task 4. Do not include units.",
            "solution": "The problem is well-posed and provides a valid framework for analyzing a rank-deficient least squares problem. All necessary data are provided, and the tasks are logically structured. We proceed with the solution.\n\nThe central task is to find a vector $x \\in \\mathbb{R}^2$ that minimizes the squared Euclidean norm of the residual, $r = Ax - b$, where\n$$\nA \\;=\\; \\begin{pmatrix}\n1  2 \\\\\n2  4 \\\\\n3  6\n\\end{pmatrix}, \\qquad b \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe function to minimize is $f(x) = \\| Ax - b \\|_{2}^{2}$.\n\n**1. Derivation of the Optimality Condition (Normal Equations)**\n\nA least squares solution $x$ is a minimizer of the function $f(x)$. We begin by expressing $f(x)$ using the definition of the Euclidean norm as a dot product:\n$$\nf(x) = \\| Ax - b \\|_{2}^{2} = (Ax - b)^T (Ax - b)\n$$\nExpanding the product, we get:\n$$\nf(x) = (x^T A^T - b^T)(Ax - b) = x^T A^T Ax - x^T A^T b - b^T Ax + b^T b\n$$\nSince $b^T Ax$ is a scalar ($1 \\times 1$ matrix), it is equal to its own transpose: $(b^T Ax)^T = x^T A^T b$. Thus, the two cross-terms are identical. The function becomes:\n$$\nf(x) = x^T(A^T A)x - 2b^T Ax + b^T b\n$$\nThis is a quadratic function of the vector variable $x$. A necessary condition for a minimum is that the gradient of $f(x)$ with respect to $x$ is the zero vector. We compute the gradient, $\\nabla_x f(x)$:\n$$\n\\nabla_x f(x) = \\nabla_x (x^T A^T Ax) - \\nabla_x (2b^T Ax) + \\nabla_x (b^T b)\n$$\nUsing standard matrix calculus identities, $\\nabla_x(x^T Mx) = (M+M^T)x$ and $\\nabla_x(c^T x) = c$, we have:\n- For the quadratic term, the matrix $M = A^T A$ is symmetric, so $M^T=M$. Thus, $\\nabla_x(x^T A^T Ax) = 2A^T Ax$.\n- For the linear term, $2b^T Ax = 2(A^T b)^T x$. Thus, $\\nabla_x(2b^T Ax) = 2A^T b$.\n- The term $b^T b$ is constant with respect to $x$, so its gradient is zero.\n\nCombining these results, the gradient is:\n$$\n\\nabla_x f(x) = 2A^T Ax - 2A^T b\n$$\nSetting the gradient to zero gives the stationary condition:\n$$\n2A^T Ax - 2A^T b = 0 \\quad \\implies \\quad A^T Ax = A^T b\n$$\nThis system of linear equations is known as the **normal equations**. To confirm that a solution to this system corresponds to a minimum, we examine the Hessian matrix of $f(x)$, which is $\\nabla_x^2 f(x) = 2A^T A$. For any vector $v \\in \\mathbb{R}^2$, the quadratic form associated with the Hessian is $v^T(2A^T A)v = 2(Av)^T(Av) = 2\\|Av\\|_2^2 \\ge 0$. This shows that the Hessian is positive semi-definite. Since $f(x)$ is a quadratic function, this guarantees that any solution to the normal equations is a global minimizer of $f(x)$.\n\n**2. Analysis of the Matrix $A^T A$ and the Solution Set**\n\nFirst, we compute the matrix $A^T A$:\n$$\nA^T = \\begin{pmatrix} 1  2  3 \\\\ 2  4  6 \\end{pmatrix}\n$$\n$$\nA^T A = \\begin{pmatrix} 1  2  3 \\\\ 2  4  6 \\end{pmatrix} \\begin{pmatrix} 1  2 \\\\ 2  4 \\\\ 3  6 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 2 \\cdot 2 + 3 \\cdot 3  1 \\cdot 2 + 2 \\cdot 4 + 3 \\cdot 6 \\\\ 2 \\cdot 1 + 4 \\cdot 2 + 6 \\cdot 3  2 \\cdot 2 + 4 \\cdot 4 + 6 \\cdot 6 \\end{pmatrix} = \\begin{pmatrix} 14  28 \\\\ 28  56 \\end{pmatrix}\n$$\nTo prove that $A^T A$ is singular, we compute its determinant:\n$$\n\\det(A^T A) = (14)(56) - (28)(28) = 784 - 784 = 0\n$$\nSince the determinant is zero, $A^T A$ is singular. This singularity arises because the columns of $A$ are linearly dependent: the second column is twice the first. This means $\\text{rank}(A) = 1$, which is less than the number of columns ($2$). Since $\\text{rank}(A^T A) = \\text{rank}(A)$, the matrix $A^T A$ is also rank-deficient and thus singular.\n\nThe singularity of $A^T A$ implies that its null space is non-trivial. If the normal equations have a solution, they must have infinitely many. A solution to $A^T Ax = A^T b$ exists if and only if $A^T b$ lies in the column space of $A^T A$. By the fundamental theorem of linear algebra, $\\text{range}(A^T A) = \\text{range}(A^T)$. Since $A^T b$ is by definition in the range of $A^T$, a solution always exists.\n\nLet's solve the system. First, compute the right-hand side, $A^T b$:\n$$\nA^T b = \\begin{pmatrix} 1  2  3 \\\\ 2  4  6 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\nThe normal equations are:\n$$\n\\begin{pmatrix} 14  28 \\\\ 28  56 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\nThe second row of this system ($28x_1 + 56x_2 = 2$) is exactly twice the first row ($14x_1 + 28x_2 = 1$), so the system is redundant and consists of the single independent equation:\n$$\n14x_1 + 28x_2 = 1 \\quad \\text{or} \\quad x_1 + 2x_2 = \\frac{1}{14}\n$$\nThis equation defines a line in $\\mathbb{R}^2$. To find the general solution, we characterize the null space of $A$. A vector $x_h = (x_1, x_2)^T$ is in $\\text{Null}(A)$ if $Ax_h = 0$, which yields $x_1 + 2x_2 = 0$. The null space is spanned by the vector $v_h = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$.\nLet's find a particular solution $x_p$. If we set $x_2=0$, then $x_1 = 1/14$. So, $x_p = (1/14, 0)^T$.\nThe general solution set is the affine line $x = x_p + k v_h$ for any scalar $k \\in \\mathbb{R}$:\n$$\nx(k) = \\begin{pmatrix} 1/14 \\\\ 0 \\end{pmatrix} + k \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1/14 + 2k \\\\ -k \\end{pmatrix}\n$$\n\n**3. Existence and the Unique Minimizer of Minimal Norm**\n\nAs shown above, a solution to the normal equations always exists because $A^T b \\in \\text{range}(A^T A)$. Any such solution is a least squares minimizer. The non-uniqueness of the solution arises from the rank deficiency of $A$.\n\nAmong the infinite set of minimizers, there is a unique solution that has the minimum Euclidean norm. This solution, denoted $x_{LS}$, is the one that lies entirely in the row space of $A$, which is the orthogonal complement of the null space of $A$. Any least squares solution $x$ can be decomposed uniquely as $x = x_{row} + x_{null}$, where $x_{row} \\in \\mathcal{R}(A^T)$ and $x_{null} \\in \\mathcal{N}(A)$. Then $\\|x\\|_2^2 = \\|x_{row}\\|_2^2 + \\|x_{null}\\|_2^2$. This norm is minimized when $x_{null}=0$, i.e., $x=x_{row}$.\n\nThe row space of $A$ is spanned by the vector $\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$. Thus, the minimum-norm solution $x_{LS}$ must be of the form:\n$$\nx_{LS} = c \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} c \\\\ 2c \\end{pmatrix}\n$$\nfor some scalar $c$. Since $x_{LS}$ must be a least squares solution, it must satisfy the normal equation $x_1 + 2x_2 = 1/14$. Substituting the components of $x_{LS}$:\n$$\n(c) + 2(2c) = \\frac{1}{14} \\implies 5c = \\frac{1}{14} \\implies c = \\frac{1}{70}\n$$\nThe unique minimizer of minimal Euclidean norm is therefore:\n$$\nx_{LS} = \\frac{1}{70} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1/70 \\\\ 2/70 \\end{pmatrix}\n$$\n\n**4. Computation of the Minimal Residual Norm**\n\nThe minimal value of $\\|Ax - b\\|_2^2$ is achieved for any least squares minimizer $x$. We can use the minimum-norm solution $x_{LS}$ for this calculation. First, we compute the projection of $b$ onto the column space of $A$, which is $p = Ax_{LS}$:\n$$\np = A x_{LS} = \\begin{pmatrix} 1  2 \\\\ 2  4 \\\\ 3  6 \\end{pmatrix} \\begin{pmatrix} 1/70 \\\\ 2/70 \\end{pmatrix} = \\frac{1}{70} \\begin{pmatrix} 1(1) + 2(2) \\\\ 2(1) + 4(2) \\\\ 3(1) + 6(2) \\end{pmatrix} = \\frac{1}{70} \\begin{pmatrix} 5 \\\\ 10 \\\\ 15 \\end{pmatrix} = \\frac{5}{70} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\frac{1}{14} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\n$$\nThe residual vector is $r = p - b = Ax_{LS} - b$:\n$$\nr = \\frac{1}{14} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/14 - 1 \\\\ 2/14 - 0 \\\\ 3/14 - 0 \\end{pmatrix} = \\begin{pmatrix} -13/14 \\\\ 2/14 \\\\ 3/14 \\end{pmatrix} = \\frac{1}{14} \\begin{pmatrix} -13 \\\\ 2 \\\\ 3 \\end{pmatrix}\n$$\nThe minimal value of the squared norm is $\\|r\\|_2^2$:\n$$\n\\|Ax - b\\|_2^2 = \\left\\| \\frac{1}{14} \\begin{pmatrix} -13 \\\\ 2 \\\\ 3 \\end{pmatrix} \\right\\|_2^2 = \\left(\\frac{1}{14}\\right)^2 \\left( (-13)^2 + 2^2 + 3^2 \\right)\n$$\n$$\n\\|Ax - b\\|_2^2 = \\frac{1}{196} (169 + 4 + 9) = \\frac{182}{196}\n$$\nSimplifying the fraction:\n$$\n\\frac{182}{196} = \\frac{91 \\times 2}{98 \\times 2} = \\frac{91}{98} = \\frac{13 \\times 7}{14 \\times 7} = \\frac{13}{14}\n$$\nThus, the minimal possible value of $\\|Ax - b\\|_2^2$ is $13/14$.",
            "answer": "$$\n\\boxed{\\frac{13}{14}}\n$$"
        },
        {
            "introduction": "Building on the algebraic insights from the previous exercise, this practice delves into the geometric heart of the least squares problem. You will analyze a system  where the right-hand side vector $b$ is first perfectly representable within the range of $A$ and then slightly perturbed out of this space. This comparison provides a powerful illustration of the least squares solution as an orthogonal projection and clarifies the distinction between the uniqueness of the solution vector versus the uniqueness of the residual.",
            "id": "3544806",
            "problem": "Let $A \\in \\mathbb{R}^{3 \\times 4}$, $b \\in \\mathbb{R}^{3}$, and $x \\in \\mathbb{R}^{4}$, with the standard Euclidean inner product on $\\mathbb{R}^{3}$. Consider the least squares (LS) problem of minimizing the map $x \\mapsto \\|A x - b\\|_{2}^{2}$. Let\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  1  0 \\\\\n0  1  0  1 \\\\\n0  0  0  0\n\\end{pmatrix}, \\qquad b_{0} \\;=\\; \\begin{pmatrix} 3 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\qquad x_{0} \\;=\\; \\begin{pmatrix} 3 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nYou may assume that the null space $\\mathcal{N}(A)$ is nontrivial and that the range $\\operatorname{Range}(A)$ is a proper subspace of $\\mathbb{R}^{3}$. Work from first principles: start from the definition of the least squares problem in the Euclidean inner product space and fundamental orthogonality concepts; do not invoke any prepackaged projection formulas.\n\n1. Establish rigorously that $b_{0} \\in \\operatorname{Range}(A)$, that the LS residual achieves zero, and that the complete solution set of LS minimizers for $b_{0}$ has the form $x_{0} + \\mathcal{N}(A)$.\n\n2. Let $\\delta  0$ and consider a perturbation of the right-hand side $b_{\\delta} = b_{0} + \\delta u$ with\n$$\nu \\;=\\; \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\nDerive, from first principles, that the LS minimizer set for $b_{\\delta}$ is non-unique while the LS residual vector is unique. Determine the Euclidean norm of this unique residual vector as an explicit function of $\\delta$.\n\nYour final answer must be a single closed-form analytical expression for the Euclidean norm of the unique residual vector for the perturbed system with $b_{\\delta}$. No rounding is required and no units apply.",
            "solution": "The problem statement has been validated and found to be self-contained, scientifically grounded in linear algebra, and well-posed. All provided data and assumptions are consistent.\n\nThe least squares (LS) problem is to find a vector $x \\in \\mathbb{R}^{4}$ that minimizes the squared Euclidean norm of the residual vector, $f(x) = \\|A x - b\\|_{2}^{2}$, for a given matrix $A \\in \\mathbb{R}^{3 \\times 4}$ and vector $b \\in \\mathbb{R}^{3}$.\n\nLet's first characterize the fundamental subspaces associated with the matrix $A$.\nThe matrix $A$ is given by\n$$\nA = \\begin{pmatrix}\n1  0  1  0 \\\\\n0  1  0  1 \\\\\n0  0  0  0\n\\end{pmatrix}\n$$\nThe range of $A$, denoted $\\operatorname{Range}(A)$, is the subspace of $\\mathbb{R}^{3}$ spanned by the columns of $A$. Let the columns be $c_1, c_2, c_3, c_4$.\n$$\nc_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\ c_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\ c_3 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\ c_4 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nWe observe that $c_3 = c_1$ and $c_4 = c_2$. Thus, $\\operatorname{Range}(A)$ is spanned by the linearly independent vectors $c_1$ and $c_2$.\n$$\n\\operatorname{Range}(A) = \\operatorname{span}\\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right\\}\n$$\nThis subspace is the $xy$-plane in $\\mathbb{R}^{3}$, which is a $2$-dimensional proper subspace of $\\mathbb{R}^{3}$, confirming the problem's assumption.\n\nThe null space of $A$, denoted $\\mathcal{N}(A)$, consists of all vectors $v \\in \\mathbb{R}^{4}$ such that $Av = 0$. Let $v = (v_1, v_2, v_3, v_4)^T$. The condition $Av=0$ translates to the system of linear equations:\n$$\nv_1 + v_3 = 0 \\\\\nv_2 + v_4 = 0\n$$\nThis implies $v_1 = -v_3$ and $v_2 = -v_4$. We can express any vector $v \\in \\mathcal{N}(A)$ as a linear combination of two basis vectors by setting the free variables $v_3$ and $v_4$ to form a basis. Let $v_3=s$ and $v_4=t$.\n$$\nv = \\begin{pmatrix} -s \\\\ -t \\\\ s \\\\ t \\end{pmatrix} = s \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} + t \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nSo, $\\mathcal{N}(A) = \\operatorname{span}\\left\\{ (-1, 0, 1, 0)^T, (0, -1, 0, 1)^T \\right\\}$. The null space has dimension $2$ and is therefore nontrivial, confirming another problem assumption.\n\n**Part 1: Analysis of the unperturbed system with $b_0$**\n\nWe are given $b_0 = (3, -1, 0)^T$. To establish that $b_0 \\in \\operatorname{Range}(A)$, we must show that $b_0$ can be written as a linear combination of the basis vectors of $\\operatorname{Range}(A)$.\n$$\nb_0 = \\begin{pmatrix} 3 \\\\ -1 \\\\ 0 \\end{pmatrix} = 3 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + (-1) \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThis is a linear combination of the basis vectors of $\\operatorname{Range}(A)$, so $b_0 \\in \\operatorname{Range}(A)$. This means the system of equations $Ax = b_0$ is consistent and has at least one solution.\n\nFor a consistent system, there exists an $x$ such that $Ax = b_0$. For any such $x$, the LS residual is $\\|Ax - b_0\\|_2^2 = \\|b_0 - b_0\\|_2^2 = 0$. Since the squared norm is always non-negative, the minimum possible value is $0$. Therefore, the LS residual for this system achieves zero.\n\nThe complete set of solutions to an inhomogeneous linear system $Ax=b$ is given by $x_p + \\mathcal{N}(A)$, where $x_p$ is any particular solution. We are given the vector $x_0 = (3, -1, 0, 0)^T$. Let's verify if it is a particular solution to $Ax=b_0$:\n$$\nA x_0 = \\begin{pmatrix} 1  0  1  0 \\\\ 0  1  0  1 \\\\ 0  0  0  0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1(3) + 0(-1) + 1(0) + 0(0) \\\\ 0(3) + 1(-1) + 0(0) + 1(0) \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\\\ 0 \\end{pmatrix} = b_0\n$$\nSince $Ax_0 = b_0$, $x_0$ is a particular solution. The set of all solutions to $Ax=b_0$ is thus $x_0 + \\mathcal{N}(A)$. As the LS residual is zero, this is precisely the set of all LS minimizers.\n\n**Part 2: Analysis of the perturbed system with $b_{\\delta}$**\n\nWe are given a perturbed vector $b_{\\delta} = b_0 + \\delta u$, where $\\delta  0$ and $u = (0, 0, 1)^T$.\n$$\nb_{\\delta} = \\begin{pmatrix} 3 \\\\ -1 \\\\ 0 \\end{pmatrix} + \\delta \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\\\ \\delta \\end{pmatrix}\n$$\nThe LS problem for $b_\\delta$ is to find $x^* \\in \\mathbb{R}^4$ that minimizes $\\|Ax - b_\\delta\\|_2^2$. According to the fundamental principle of least squares, a vector $x^*$ is a minimizer if and only if the residual vector $r = Ax^* - b_\\delta$ is orthogonal to the subspace $\\operatorname{Range}(A)$. This is equivalent to stating that $Ax^*$ is the orthogonal projection of $b_\\delta$ onto $\\operatorname{Range}(A)$.\n\nLet $w^* = Ax^*$. The condition is that $w^*$ is the vector in $W = \\operatorname{Range}(A)$ that minimizes $\\|w - b_\\delta\\|_2$. This minimum is achieved when the vector $b_\\delta - w^*$ is orthogonal to $W$. This means $b_\\delta - w^* \\in W^{\\perp}$.\n\nThe subspace $W = \\operatorname{Range}(A)$ is the $xy$-plane. Its orthogonal complement in $\\mathbb{R}^3$ with the standard inner product is the $z$-axis:\n$$\nW^{\\perp} = \\left(\\operatorname{span}\\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right\\}\\right)^{\\perp} = \\operatorname{span}\\left\\{ \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right\\}\n$$\nThe condition $b_\\delta - w^* \\in W^{\\perp}$ means there exists a scalar $k$ such that\n$$\nb_\\delta - w^* = k \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nRearranging for $w^*$, we get\n$$\nw^* = b_\\delta - k \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\\\ \\delta \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ k \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\\\ \\delta - k \\end{pmatrix}\n$$\nFor $w^*$ to be in $W = \\operatorname{Range}(A)$, its third component must be zero.\n$$\n\\delta - k = 0 \\implies k = \\delta\n$$\nThis determines the unique vector $w^*$ in $\\operatorname{Range}(A)$ closest to $b_\\delta$:\n$$\nw^* = \\begin{pmatrix} 3 \\\\ -1 \\\\ 0 \\end{pmatrix} = b_0\n$$\nThe set of LS minimizers is the set of all $x^* \\in \\mathbb{R}^4$ such that $Ax^* = w^* = b_0$. From Part 1, we know this set is $x_0 + \\mathcal{N}(A)$. Since $\\dim(\\mathcal{N}(A)) = 2$, there are infinitely many such minimizers. Thus, the LS minimizer set for $b_\\delta$ is non-unique.\n\nHowever, for any LS minimizer $x^*$, the vector $Ax^*$ is unique and equal to $w^*=b_0$. Therefore, the LS residual vector is also unique:\n$$\nr = Ax^* - b_\\delta = w^* - b_\\delta = b_0 - b_\\delta = b_0 - (b_0 + \\delta u) = -\\delta u\n$$\nSubstituting the vector $u$:\n$$\nr = -\\delta \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ -\\delta \\end{pmatrix}\n$$\nThe Euclidean norm of this unique residual vector is calculated as:\n$$\n\\|r\\|_2 = \\left\\| \\begin{pmatrix} 0 \\\\ 0 \\\\ -\\delta \\end{pmatrix} \\right\\|_2 = \\sqrt{0^2 + 0^2 + (-\\delta)^2} = \\sqrt{\\delta^2}\n$$\nSince the problem states that $\\delta  0$, we have $\\sqrt{\\delta^2} = |\\delta| = \\delta$.\n\nThus, the Euclidean norm of the unique residual vector is $\\delta$.",
            "answer": "$$\\boxed{\\delta}$$"
        },
        {
            "introduction": "When a least squares problem admits infinitely many solutions, we require a robust method for selecting a single, definitive answer. This final exercise  guides you to find the unique solution that has the minimum Euclidean norm, a crucial concept in regularization and data science. You will first identify this solution using geometric orthogonality principles and then verify your result by explicitly computing the Moore-Penrose pseudoinverse, $A^{+}$, demonstrating its role in providing the canonical solution to rank-deficient systems.",
            "id": "3544790",
            "problem": "Consider the matrix $A \\in \\mathbb{R}^{2 \\times 3}$ and vector $b \\in \\mathbb{R}^{2}$ defined by\n$$\nA = \\begin{pmatrix}\n1  0  1 \\\\\n2  0  2\n\\end{pmatrix}, \n\\qquad \nb = \\begin{pmatrix}\n3 \\\\\n1\n\\end{pmatrix}.\n$$\nThe matrix $A$ is rank-deficient.\n\nStarting from the fundamental definition of the least squares problem as the minimization of the squared Euclidean norm $f(x) = \\|A x - b\\|_{2}^{2}$ over $x \\in \\mathbb{R}^{3}$, and using only foundational facts such as the characterization of critical points by vanishing gradient and properties of orthogonal projections, do the following:\n\n- Derive the first-order optimality conditions for least squares minimizers and compute the entire set of minimizers explicitly for the given $A$ and $b$.\n- Using geometric reasoning about orthogonal projections in Euclidean space, identify the unique minimizer of minimal Euclidean norm within that set.\n- Finally, verify that this unique minimal-norm minimizer coincides with $A^{+} b$, where $A^{+}$ denotes the Mooreâ€“Penrose pseudoinverse of $A$, by computing $A^{+}$ directly from first principles for this specific rank-$1$ matrix.\n\nYour final reported answer must be the unique minimal-norm minimizer $A^{+} b$ as a single explicit column vector in $\\mathbb{R}^{3}$. No rounding is required.",
            "solution": "The objective is to find the set of all vectors $x \\in \\mathbb{R}^{3}$ that minimize the squared Euclidean norm of the residual, $f(x) = \\|A x - b\\|_{2}^{2}$, for the given matrix $A \\in \\mathbb{R}^{2 \\times 3}$ and vector $b \\in \\mathbb{R}^{2}$. We are given\n$$\nA = \\begin{pmatrix} 1  0  1 \\\\ 2  0  2 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\nThe function $f(x)$ is a quadratic function of $x$. We can express it as a dot product:\n$$\nf(x) = (A x - b)^{T} (A x - b) = (x^{T} A^{T} - b^{T})(A x - b) = x^{T} A^{T} A x - x^{T} A^{T} b - b^{T} A x + b^{T} b.\n$$\nSince $b^{T} A x$ is a scalar, it is equal to its transpose, $(b^{T} A x)^{T} = x^{T} A^{T} b$. Thus, the objective function simplifies to:\n$$\nf(x) = x^{T} A^{T} A x - 2 x^{T} A^{T} b + b^{T} b.\n$$\nThis function is a convex quadratic, so its minimizers are the critical points where the gradient with respect to $x$ vanishes. The gradient is given by:\n$$\n\\nabla_{x} f(x) = 2 A^{T} A x - 2 A^{T} b.\n$$\nSetting the gradient to the zero vector, $\\nabla_{x} f(x) = 0$, yields the first-order optimality conditions, which are known as the normal equations:\n$$\nA^{T} A x = A^{T} b.\n$$\nAny vector $x$ that satisfies the normal equations is a least squares minimizer.\n\nFirst, we compute the matrices $A^{T} A$ and $A^{T} b$:\n$$\nA^{T} = \\begin{pmatrix} 1  2 \\\\ 0  0 \\\\ 1  2 \\end{pmatrix}\n$$\n$$\nA^{T} A = \\begin{pmatrix} 1  2 \\\\ 0  0 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 1  0  1 \\\\ 2  0  2 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 2 \\cdot 2  1 \\cdot 0 + 2 \\cdot 0  1 \\cdot 1 + 2 \\cdot 2 \\\\ 0  0  0 \\\\ 1 \\cdot 1 + 2 \\cdot 2  1 \\cdot 0 + 2 \\cdot 0  1 \\cdot 1 + 2 \\cdot 2 \\end{pmatrix} = \\begin{pmatrix} 5  0  5 \\\\ 0  0  0 \\\\ 5  0  5 \\end{pmatrix}.\n$$\n$$\nA^{T} b = \\begin{pmatrix} 1  2 \\\\ 0  0 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 3 + 2 \\cdot 1 \\\\ 0 \\cdot 3 + 0 \\cdot 1 \\\\ 1 \\cdot 3 + 2 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 0 \\\\ 5 \\end{pmatrix}.\n$$\nThe normal equations $A^{T} A x = A^{T} b$ for $x = (x_1, x_2, x_3)^T$ are:\n$$\n\\begin{pmatrix} 5  0  5 \\\\ 0  0  0 \\\\ 5  0  5 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 0 \\\\ 5 \\end{pmatrix}.\n$$\nThis matrix equation corresponds to the linear system:\n$5x_1 + 5x_3 = 5$\n$0 = 0$\n$5x_1 + 5x_3 = 5$\nThis system reduces to a single equation: $x_1 + x_3 = 1$. The variable $x_2$ is unconstrained. We can parameterize the solution set. Let $x_2 = s$ and $x_3 = t$ for any $s, t \\in \\mathbb{R}$. Then $x_1 = 1 - t$. The set of all least squares minimizers is an affine subspace given by:\n$$\nx(s, t) = \\begin{pmatrix} 1 - t \\\\ s \\\\ t \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + s \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} + t \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\nThis set can be described as $x_p + \\mathcal{N}(A)$, where $x_p = (1, 0, 0)^T$ is a particular solution and $\\mathcal{N}(A)$ is the null space of $A$, spanned by the vectors $(0, 1, 0)^T$ and $(-1, 0, 1)^T$.\n\nNext, we identify the unique minimizer of minimal Euclidean norm. The set of all solutions is the affine subspace $S = \\{x_p + z \\mid z \\in \\mathcal{N}(A)\\}$. The vector in $S$ with the minimum norm is the one that is orthogonal to the subspace defining the affine translation, which is $\\mathcal{N}(A)$. In other words, the minimal-norm solution $x_{LS}$ must be in the orthogonal complement of the null space, $x_{LS} \\in \\mathcal{N}(A)^{\\perp}$. From the fundamental theorem of linear algebra, $\\mathcal{N}(A)^{\\perp} = \\mathcal{R}(A^T)$, the range of $A^T$.\nWe find the specific parameters $s$ and $t$ for a solution $x(s,t)$ that is orthogonal to the basis vectors of $\\mathcal{N}(A)$.\nLet $v_1 = (0, 1, 0)^T$ and $v_2 = (-1, 0, 1)^T$.\nThe orthogonality conditions are $x(s,t)^T v_1 = 0$ and $x(s,t)^T v_2 = 0$.\n1. $x(s,t)^T v_1 = \\begin{pmatrix} 1-t  s  t \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = s = 0$.\n2. $x(s,t)^T v_2 = \\begin{pmatrix} 1-t  s  t \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} = -(1-t) + t = -1 + 2t = 0$.\nThe second condition gives $2t = 1$, so $t = 1/2$.\nSubstituting $s=0$ and $t=1/2$ into the general solution gives the unique minimal-norm minimizer:\n$$\nx_{LS} = \\begin{pmatrix} 1 - 1/2 \\\\ 0 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1/2 \\end{pmatrix}.\n$$\n\nFinally, we verify this result by computing the Moore-Penrose pseudoinverse $A^{+}$ and the product $A^{+}b$. The matrix $A$ is a rank-$1$ matrix. It can be written as an outer product of two vectors $u \\in \\mathbb{R}^2$ and $v \\in \\mathbb{R}^3$.\n$$\nA = \\begin{pmatrix} 1  0  1 \\\\ 2  0  2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\begin{pmatrix} 1  0  1 \\end{pmatrix}.\n$$\nHere, $u = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$ and $v^T = \\begin{pmatrix} 1  0  1 \\end{pmatrix}$, so $v = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\nFor a rank-$1$ matrix $A = uv^T$, the Moore-Penrose pseudoinverse is given by the formula:\n$$\nA^{+} = \\frac{1}{\\|u\\|_{2}^2 \\|v\\|_{2}^2} v u^T.\n$$\nWe compute the squared norms:\n$\\|u\\|_{2}^2 = 1^2 + 2^2 = 1 + 4 = 5$.\n$\\|v\\|_{2}^2 = 1^2 + 0^2 + 1^2 = 1 + 1 = 2$.\nNow, we can compute $A^{+}$:\n$$\nA^{+} = \\frac{1}{5 \\cdot 2} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1  2 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 1 \\cdot 1  1 \\cdot 2 \\\\ 0 \\cdot 1  0 \\cdot 2 \\\\ 1 \\cdot 1  1 \\cdot 2 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 1  2 \\\\ 0  0 \\\\ 1  2 \\end{pmatrix}.\n$$\nThe unique minimal-norm least squares solution is given by $x_{LS} = A^{+}b$.\n$$\nx_{LS} = A^{+}b = \\frac{1}{10} \\begin{pmatrix} 1  2 \\\\ 0  0 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 1 \\cdot 3 + 2 \\cdot 1 \\\\ 0 \\\\ 1 \\cdot 3 + 2 \\cdot 1 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 5 \\\\ 0 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 5/10 \\\\ 0 \\\\ 5/10 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 1/2 \\end{pmatrix}.\n$$\nThis result matches the one obtained through geometric reasoning, confirming our calculations.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2} \\\\\n0 \\\\\n\\frac{1}{2}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}