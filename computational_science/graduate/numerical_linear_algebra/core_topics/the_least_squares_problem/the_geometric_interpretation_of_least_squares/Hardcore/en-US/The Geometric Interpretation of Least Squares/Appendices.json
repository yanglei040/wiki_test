{
    "hands_on_practices": [
        {
            "introduction": "The least-squares solution is fundamentally a projection of a data vector $b$ onto the column space of a matrix $A$, denoted $\\operatorname{col}(A)$. This exercise explores a core property of this projection: it is determined by the subspace itself, not by the specific set of vectors chosen to span it. By examining the effect of adding redundant columns—vectors that already lie within the existing column space—we can sharply distinguish between the unique, stable projection in the data space and the potentially non-unique coefficients in the parameter space .",
            "id": "3588406",
            "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$. Consider the least-squares problem of finding $x \\in \\mathbb{R}^n$ that minimizes $\\|A x - b\\|_2$. Define the augmented matrix $\\tilde{A} \\in \\mathbb{R}^{m \\times (n+k)}$ by $\\tilde{A} = [A, A C]$ where $C \\in \\mathbb{R}^{n \\times k}$ is arbitrary, so that every added column of $\\tilde{A}$ lies in $\\operatorname{col}(A)$, the column space of $A$. Using only the definition of orthogonal projection onto a subspace (the unique $p \\in S$ such that $b - p$ is orthogonal to $S$ for a subspace $S \\subseteq \\mathbb{R}^m$), the characterization of least squares as minimizing the Euclidean norm $\\|\\cdot\\|_2$, and the fact that linear dependence indicates redundancy in a generating set, determine which of the following statements are true.\n\nA. The orthogonal projection of $b$ onto $\\operatorname{col}(\\tilde{A})$ equals the orthogonal projection of $b$ onto $\\operatorname{col}(A)$, hence the fitted vector at a least-squares minimizer is unchanged by the augmentation.\n\nB. By adding columns that lie in $\\operatorname{col}(A)$, the minimum of $\\|\\tilde{A} y - b\\|_2$ over $y \\in \\mathbb{R}^{n+k}$ can strictly decrease compared to the minimum of $\\|A x - b\\|_2$ over $x \\in \\mathbb{R}^n$.\n\nC. Even though the orthogonal projection of $b$ onto the column space is unchanged, the set of least-squares minimizers in coefficient space enlarges under augmentation: there are infinitely many $y \\in \\mathbb{R}^{n+k}$ that yield the same fitted vector as a minimizer for $A$.\n\nD. The normal equations for $\\tilde{A}$, given by $\\tilde{A}^\\top \\tilde{A} y = \\tilde{A}^\\top b$, have strictly larger rank than the normal equations for $A$, given by $A^\\top A x = A^\\top b$, and therefore the fitted vector necessarily changes.\n\nE. The orthogonal projector onto $\\operatorname{col}(A)$ depends on the choice of basis for that subspace, so adding columns that are linear combinations of existing columns can change the projector and thus change the projection of $b$.\n\nSelect all statements that are correct.",
            "solution": "The present task is to evaluate the validity of several statements regarding the effect of augmenting the matrix $A$ in a least-squares problem. The augmentation consists of appending columns that are already in the column space of $A$. The analysis must be based on the geometric interpretation of least squares as an orthogonal projection problem.\n\nLet $S = \\operatorname{col}(A)$ be the column space of the matrix $A \\in \\mathbb{R}^{m \\times n}$. The least-squares problem $\\min_{x \\in \\mathbb{R}^n} \\|Ax - b\\|_2$ is geometrically equivalent to finding the vector $p \\in S$ that is closest to $b \\in \\mathbb{R}^m$. This unique vector $p$ is the orthogonal projection of $b$ onto the subspace $S$. Any vector $x^* \\in \\mathbb{R}^n$ for which $Ax^* = p$ is a least-squares solution, and is called a minimizer. The vector $p$ is the \"fitted vector\".\n\nThe augmented matrix is defined as $\\tilde{A} = [A, AC]$, where $C \\in \\mathbb{R}^{n \\times k}$. Let $\\tilde{S} = \\operatorname{col}(\\tilde{A})$ be the column space of the augmented matrix. The columns of $\\tilde{A}$ are the columns of $A$ along with the columns of the matrix product $AC$. Let the columns of $A$ be denoted by $a_1, \\dots, a_n \\in \\mathbb{R}^m$. By definition, any column of $AC$, say the $j$-th column, is given by $Ac_j$, where $c_j \\in \\mathbb{R}^n$ is the $j$-th column of $C$. This product is a linear combination of the columns of $A$: $Ac_j = \\sum_{i=1}^n (c_j)_i a_i$.\nThis shows that every column of $AC$ is an element of $\\operatorname{col}(A)$.\n\nThe column space $\\operatorname{col}(\\tilde{A})$ is the span of all columns of $\\tilde{A}$. Since the columns of $AC$ are already linear combinations of the columns of $A$, adding them to the set of generating vectors does not change the spanned subspace. That is,\n$$ \\operatorname{col}(\\tilde{A}) = \\operatorname{span}(\\text{cols of } A, \\text{cols of } AC) = \\operatorname{span}(\\text{cols of } A) = \\operatorname{col}(A) $$\nThus, the fundamental consequence of the specified augmentation is that the column space remains unchanged: $S = \\tilde{S}$.\n\nWith this established, we can evaluate each statement.\n\n**A. The orthogonal projection of $b$ onto $\\operatorname{col}(\\tilde{A})$ equals the orthogonal projection of $b$ onto $\\operatorname{col}(A)$, hence the fitted vector at a least-squares minimizer is unchanged by the augmentation.**\nThe fitted vector for the original problem is the orthogonal projection of $b$ onto $S = \\operatorname{col}(A)$. Let us call this vector $p$.\nThe fitted vector for the augmented problem is the orthogonal projection of $b$ onto $\\tilde{S} = \\operatorname{col}(\\tilde{A})$. Let us call this vector $\\tilde{p}$.\nThe orthogonal projection of a vector onto a subspace is uniquely determined by the vector and the subspace. Since we have shown that $\\operatorname{col}(A) = \\operatorname{col}(\\tilde{A})$, the subspace is the same for both problems. Consequently, the orthogonal projection of $b$ onto this subspace must be the same, i.e., $p = \\tilde{p}$.\nThe statement correctly identifies that the projection is the same and correctly concludes that the fitted vector is unchanged.\nTherefore, this statement is **Correct**.\n\n**B. By adding columns that lie in $\\operatorname{col}(A)$, the minimum of $\\|\\tilde{A} y - b\\|_2$ over $y \\in \\mathbb{R}^{n+k}$ can strictly decrease compared to the minimum of $\\|A x - b\\|_2$ over $x \\in \\mathbb{R}^n$.**\nThe minimum value sought in the least-squares problem is the length of the residual vector, which is the distance from $b$ to its projection on the column space.\nFor the original problem, the minimum value is $\\|p - b\\|_2$, where $p$ is the projection of $b$ onto $\\operatorname{col}(A)$.\nFor the augmented problem, the minimum value is $\\|\\tilde{p} - b\\|_2$, where $\\tilde{p}$ is the projection of $b$ onto $\\operatorname{col}(\\tilde{A})$.\nAs established in the analysis of option A, $p = \\tilde{p}$. Therefore, the minimum residual norm is also identical: $\\|p - b\\|_2 = \\|\\tilde{p} - b\\|_2$.\nThe minimum value cannot strictly decrease; it remains exactly the same.\nTherefore, this statement is **Incorrect**.\n\n**C. Even though the orthogonal projection of $b$ onto the column space is unchanged, the set of least-squares minimizers in coefficient space enlarges under augmentation: there are infinitely many $y \\in \\mathbb{R}^{n+k}$ that yield the same fitted vector as a minimizer for $A$.**\nThe set of minimizers for the original problem is the solution set of the consistent linear system $Ax = p$.\nThe set of minimizers for the augmented problem is the solution set of the consistent linear system $\\tilde{A}y = p$.\nThe matrix $\\tilde{A} \\in \\mathbb{R}^{m \\times (n+k)}$ has more columns than $A$ (assuming $k \\ge 1$). However, its rank is the same as $A$'s: $\\operatorname{rank}(\\tilde{A}) = \\dim(\\operatorname{col}(\\tilde{A})) = \\dim(\\operatorname{col}(A)) = \\operatorname{rank}(A)$.\nThe solution set to a consistent linear system $My=p$ is an affine subspace of the form $y_{part} + \\operatorname{null}(M)$, where $y_{part}$ is a particular solution and $\\operatorname{null}(M)$ is the null space of $M$. The dimension of this affine space is $\\dim(\\operatorname{null}(M))$.\nBy the rank-nullity theorem, $\\dim(\\operatorname{null}(\\tilde{A})) = (\\text{number of columns}) - \\operatorname{rank}(\\tilde{A}) = (n+k) - \\operatorname{rank}(A)$.\nFor the original problem, $\\dim(\\operatorname{null}(A)) = n - \\operatorname{rank}(A)$.\nSince $k \\ge 1$, we have $\\dim(\\operatorname{null}(\\tilde{A})) = \\dim(\\operatorname{null}(A)) + k  \\dim(\\operatorname{null}(A))$.\nBecause the dimension of the null space of $\\tilde{A}$ is at least $k \\ge 1$, $\\operatorname{null}(\\tilde{A})$ is a non-trivial subspace. This implies that the solution set for $\\tilde{A}y=p$ contains infinitely many vectors. The set of minimizers (in coefficient space) has indeed enlarged.\nTherefore, this statement is **Correct**.\n\n**D. The normal equations for $\\tilde{A}$, given by $\\tilde{A}^\\top \\tilde{A} y = \\tilde{A}^\\top b$, have strictly larger rank than the normal equations for $A$, given by $A^\\top A x = A^\\top b$, and therefore the fitted vector necessarily changes.**\nThe rank of the matrix in a normal equations system $M^\\top M z = M^\\top b$ is the rank of the matrix $M^\\top M$. A standard result in linear algebra is that for any real matrix $M$, $\\operatorname{rank}(M^\\top M) = \\operatorname{rank}(M)$.\nWe must compare $\\operatorname{rank}(A^\\top A)$ with $\\operatorname{rank}(\\tilde{A}^\\top \\tilde{A})$. This is equivalent to comparing $\\operatorname{rank}(A)$ with $\\operatorname{rank}(\\tilde{A})$.\nAs established, $\\operatorname{col}(A) = \\operatorname{col}(\\tilde{A})$, which means their dimensions are equal. The dimension of the column space is the rank of the matrix. Thus, $\\operatorname{rank}(A) = \\operatorname{rank}(\\tilde{A})$.\nIt follows that $\\operatorname{rank}(A^\\top A) = \\operatorname{rank}(\\tilde{A}^\\top \\tilde{A})$. The rank is not strictly larger. The first part of the statement is false. The conclusion that the fitted vector changes is also false, as shown in the analysis of A.\nTherefore, this statement is **Incorrect**.\n\n**E. The orthogonal projector onto $\\operatorname{col}(A)$ depends on the choice of basis for that subspace, so adding columns that are linear combinations of existing columns can change the projector and thus change the projection of $b$.**\nThis statement is fundamentally incorrect. The orthogonal projection operator onto a subspace $S$, denoted $P_S$, is uniquely determined by the subspace $S$ itself. It is an intrinsic property of the subspace and does not depend on the particular basis chosen to represent it. While a formula for the projector matrix, such as $P_S = Q(Q^\\top Q)^{-1}Q^\\top$, uses a matrix $Q$ whose columns form a basis for $S$, the resulting matrix $P_S$ is independent of the choice of $Q$. Any two bases for $S$ are related by an invertible matrix, and this relationship cancels out in the formula, yielding the same projector matrix. Augmenting the columns of $A$ with vectors already in $\\operatorname{col}(A)$ is equivalent to using a larger, linearly dependent generating set for the same subspace. It does not alter the subspace itself, and therefore cannot alter the unique orthogonal projector associated with that subspace.\nTherefore, this statement is **Incorrect**.",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "In many applications, the matrix $A$ may be rank-deficient, meaning its columns are not linearly independent and do not span a space of dimension equal to the number of columns. This practice delves into the geometric consequences of rank deficiency, which gives rise to a non-trivial null space, $\\operatorname{null}(A)$. Differentiating between the consistent case ($b \\in \\operatorname{col}(A)$) and the inconsistent case ($b \\notin \\operatorname{col}(A)$) is essential for correctly characterizing the entire set of least-squares minimizers, which forms an affine subspace .",
            "id": "3571399",
            "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with rank $r  \\min(m,n)$ (so $A$ is rank-deficient), and a vector $b \\in \\mathbb{R}^m$. Let $\\operatorname{col}(A)$ denote the column space (range) of $A$, and $\\operatorname{null}(A)$ denote the nullspace (kernel) of $A$. Consider the Euclidean least squares problem $\\min_{x \\in \\mathbb{R}^n} \\|A x - b\\|_2$. Assume standard inner-product and norm on $\\mathbb{R}^m$. Let $I$ denote the identity on $\\mathbb{R}^m$, and let $P$ denote the orthogonal projector onto $\\operatorname{col}(A)$.\n\nSelect all statements that are correct, contrasting the cases $b \\in \\operatorname{col}(A)$ versus $b \\notin \\operatorname{col}(A)$ and characterizing the exact solution set when $b \\in \\operatorname{col}(A)$.\n\nA. If $b \\in \\operatorname{col}(A)$, the set of minimizers of $\\min_x \\|A x - b\\|_2$ coincides with the set of exact solutions to $A x = b$, and this set is the affine subspace $x_p + \\operatorname{null}(A)$ for any particular solution $x_p$ satisfying $A x_p = b$.\n\nB. If $b \\notin \\operatorname{col}(A)$, every least-squares minimizer $x_\\star$ has residual $r_\\star = b - A x_\\star$ satisfying $r_\\star \\perp \\operatorname{col}(A)$ and $A x_\\star$ equals the orthogonal projection of $b$ onto $\\operatorname{col}(A)$; moreover, the full set of minimizers is $x_\\star + \\operatorname{null}(A)$.\n\nC. If $A$ is rank-deficient and $b \\in \\operatorname{col}(A)$, the equation $A x = b$ has a unique solution.\n\nD. In the inconsistent case $b \\notin \\operatorname{col}(A)$, two distinct least-squares minimizers can differ by a vector from $\\operatorname{null}(A^{\\top})$ but not from $\\operatorname{null}(A)$.\n\nE. In the inconsistent case $b \\notin \\operatorname{col}(A)$, the optimal residual $r_\\star$ equals the component of $b$ orthogonal to $\\operatorname{col}(A)$, i.e., $r_\\star = (I - P) b$.",
            "solution": "The problem statement has been validated and is determined to be sound. It is a well-posed problem in numerical linear algebra, free of scientific or mathematical flaws, and contains all necessary information.\n\nThe core of the problem is the characterization of the solution set for the least squares problem $\\min_{x \\in \\mathbb{R}^n} \\|A x - b\\|_2$ where the matrix $A \\in \\mathbb{R}^{m \\times n}$ is rank-deficient, i.e., its rank $r$ satisfies $r  n$ and $r  m$. The condition $r  \\min(m, n)$ is given. Since $r  n$, the rank-nullity theorem, $\\operatorname{rank}(A) + \\dim(\\operatorname{null}(A)) = n$, implies that $\\dim(\\operatorname{null}(A)) = n - r  0$. Therefore, the nullspace $\\operatorname{null}(A)$ is non-trivial, containing infinitely many vectors.\n\nThe geometric interpretation of the least squares problem is to find a vector $x_\\star \\in \\mathbb{R}^n$ such that $A x_\\star$ is the vector in the column space $\\operatorname{col}(A)$ that is closest to $b$. This vector is the orthogonal projection of $b$ onto $\\operatorname{col}(A)$. Let $P$ be the orthogonal projection matrix onto $\\operatorname{col}(A)$. Then, any least squares solution $x_\\star$ must satisfy the equation:\n$$A x_\\star = P b$$\nThis system is known as the normal equations. Since $P b$ lies in $\\operatorname{col}(A)$ by definition, this system is always consistent.\n\nLet's analyze the two cases presented in the problem.\n\nCase 1: $b \\in \\operatorname{col}(A)$ (Consistent System)\nIf $b \\in \\operatorname{col}(A)$, then the vector $b$ is already in the column space. The orthogonal projection of a vector already in a subspace is the vector itself. Thus, $P b = b$. The least squares problem then requires solving:\n$$A x = b$$\nSince $b \\in \\operatorname{col}(A)$, at least one solution exists. The minimum value of the objective function is $\\|A x - b\\|_2 = \\|b - b\\|_2 = 0$. The minimizers of the least squares problem are therefore the exact solutions to the linear system $A x = b$.\nIf $x_p$ is any particular solution to $A x = b$, the full set of solutions is given by the affine subspace $x_p + \\operatorname{null}(A)$. Since $\\dim(\\operatorname{null}(A))  0$ due to rank deficiency, there are infinitely many solutions.\n\nCase 2: $b \\notin \\operatorname{col}(A)$ (Inconsistent System)\nIf $b \\notin \\operatorname{col}(A)$, then there is no exact solution to $A x = b$. The least squares solutions $x_\\star$ are the vectors that solve the normal equations:\n$$A x_\\star = P b$$\nSince $A$ is rank-deficient, this system has infinitely many solutions. If $x_p$ is any particular solution, the full set of solutions is the affine subspace $x_p + \\operatorname{null}(A)$.\nFor any minimizer $x_\\star$, the resulting approximation is $A x_\\star = P b$. The residual vector is $r_\\star = b - A x_\\star = b - P b$. By the property of orthogonal projections, the vector $b - P b$ is orthogonal to the subspace $\\operatorname{col}(A)$. So, $r_\\star \\perp \\operatorname{col}(A)$. The squared norm of the minimal residual is $\\|r_\\star\\|_2^2 = \\|b - P b\\|_2^2  0$.\n\nNow we evaluate each option.\n\nA. If $b \\in \\operatorname{col}(A)$, the set of minimizers of $\\min_x \\|A x - b\\|_2$ coincides with the set of exact solutions to $A x = b$, and this set is the affine subspace $x_p + \\operatorname{null}(A)$ for any particular solution $x_p$ satisfying $A x_p = b$.\nThis statement accurately reflects our analysis of Case 1. The minimizers are those $x$ for which $\\|A x - b\\|_2 = 0$, which occurs if and only if $A x = b$. The structure of the solution set for a consistent linear system with a rank-deficient matrix is correctly described as $x_p + \\operatorname{null}(A)$.\nVerdict: **Correct**.\n\nB. If $b \\notin \\operatorname{col}(A)$, every least-squares minimizer $x_\\star$ has residual $r_\\star = b - A x_\\star$ satisfying $r_\\star \\perp \\operatorname{col}(A)$ and $A x_\\star$ equals the orthogonal projection of $b$ onto $\\operatorname{col}(A)$; moreover, the full set of minimizers is $x_\\star + \\operatorname{null}(A)$.\nThis statement correctly summarizes the fundamental properties of least squares solutions for the inconsistent case as derived in our analysis of Case 2. The solution $x_\\star$ makes $A x_\\star$ the projection of $b$, the residual is orthogonal to the range, and due to rank deficiency, the set of minimizers is an affine subspace formed by a particular solution plus the nullspace.\nVerdict: **Correct**.\n\nC. If $A$ is rank-deficient and $b \\in \\operatorname{col}(A)$, the equation $A x = b$ has a unique solution.\nThe problem states that $A$ is rank-deficient, so $\\operatorname{rank}(A) = r  n$. This implies $\\dim(\\operatorname{null}(A)) = n - r  0$. If the system $A x = b$ is consistent (as it is when $b \\in \\operatorname{col}(A)$), the solution set is $x_p + \\operatorname{null}(A)$. Since $\\operatorname{null}(A)$ contains non-zero vectors, there are infinitely many solutions, not a unique one. A unique solution would require $\\operatorname{null}(A) = \\{0\\}$, which means $\\operatorname{rank}(A) = n$.\nVerdict: **Incorrect**.\n\nD. In the inconsistent case $b \\notin \\operatorname{col}(A)$, two distinct least-squares minimizers can differ by a vector from $\\operatorname{null}(A^{\\top})$ but not from $\\operatorname{null}(A)$.\nAs established in our analysis of Case 2 and in the evaluation of option B, the set of all least-squares minimizers is an affine subspace $x_p + \\operatorname{null}(A)$. Let $x_1$ and $x_2$ be two distinct minimizers. Then $x_1 = x_p + z_1$ and $x_2 = x_p + z_2$ for some $z_1, z_2 \\in \\operatorname{null}(A)$ with $z_1 \\neq z_2$. Their difference is $x_1 - x_2 = z_1 - z_2$, which is a non-zero vector in $\\operatorname{null}(A)$. The statement asserts that the difference cannot be from $\\operatorname{null}(A)$, which is false. Furthermore, the minimizers $x_1, x_2$ belong to $\\mathbb{R}^n$, so their difference lies in $\\mathbb{R}^n$. The set $\\operatorname{null}(A^\\top)$ (the left nullspace of $A$) is a subspace of $\\mathbb{R}^m$. It is therefore nonsensical for the difference of vectors in $\\mathbb{R}^n$ to be an element of a subspace of $\\mathbb{R}^m$, unless $m=n$, but the statement is invalid in the general case described.\nVerdict: **Incorrect**.\n\nE. In the inconsistent case $b \\notin \\operatorname{col}(A)$, the optimal residual $r_\\star$ equals the component of $b$ orthogonal to $\\operatorname{col}(A)$, i.e., $r_\\star = (I - P) b$.\nAs derived for Case 2, any least squares minimizer $x_\\star$ satisfies $A x_\\star = P b$. The optimal residual is $r_\\star = b - A x_\\star = b - P b$. By definition, $P$ is the orthogonal projector onto $\\operatorname{col}(A)$, so $(I - P)$ is the orthogonal projector onto the orthogonal complement of the column space, $(\\operatorname{col}(A))^\\perp$. Thus, $r_\\star = (I - P) b$ represents the component of $b$ that is orthogonal to $\\operatorname{col}(A)$.\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ABE}$$"
        },
        {
            "introduction": "While rank deficiency is an absolute algebraic property, the geometric condition of near-collinearity—where column vectors are almost linearly dependent—has profound numerical consequences. This problem connects the geometry of the column space (specifically, the angle between basis vectors) to the stability of the least-squares solution. It illuminates a critical distinction in numerical linear algebra: while the orthogonal projection onto the column space is a stable operation, the coefficients of the solution can be extremely sensitive to small perturbations in the data when the columns of $A$ are nearly parallel .",
            "id": "3588414",
            "problem": "Consider a matrix $A \\in \\mathbb{R}^{m \\times 2}$ with nonzero columns $a_1, a_2 \\in \\mathbb{R}^m$. Assume the columns are normalized so that $\\|a_1\\|_2=\\|a_2\\|_2=1$, and let $\\theta \\in [0,\\pi]$ denote the principal angle between the one-dimensional subspaces $\\mathrm{span}\\{a_1\\}$ and $\\mathrm{span}\\{a_2\\}$, which for lines equals the angle between the vectors $a_1$ and $a_2$. Consider the linear least-squares problem $\\min_{x \\in \\mathbb{R}^2} \\|Ax-b\\|_2$ for a given $b \\in \\mathbb{R}^m$, and let $\\hat x$ denote the minimum-norm least-squares solution. You are to reason from first principles about how small principal angles $\\theta \\ll 1$ influence the geometry of the problem, including the behavior of singular values, projections in data space $\\mathbb{R}^m$, and stability of the coefficient mapping $b \\mapsto \\hat x$ in coefficient space $\\mathbb{R}^2$.\n\nWhich of the following statements are correct?\n\nA. If $\\theta \\to 0$ with $\\|a_1\\|_2=\\|a_2\\|_2=1$, then the smallest singular value of $A$ tends to $0$, the column space $\\operatorname{col}(A)$ tends to a one-dimensional subspace, and the orthogonal projection of $b$ onto $\\operatorname{col}(A)$ remains well defined and varies smoothly with $b$.\n\nB. For small $\\theta$, the least-squares coefficient map $b \\mapsto \\hat x$ is ill-conditioned: a perturbation $\\delta b$ can change $\\hat x$ by a factor on the order of $1/\\sigma_{\\min}(A)$, which diverges as $\\theta \\to 0$.\n\nC. Because the principal angle between $\\mathrm{span}\\{a_1\\}$ and $\\mathrm{span}\\{a_2\\}$ is small, the residual $r=b-A\\hat x$ is small for all $b$, so least-squares fits are uniformly accurate when columns are nearly collinear.\n\nD. The orthogonal projection map $b \\mapsto A\\hat x$ has operator norm $1$ and is therefore stable with respect to perturbations in $b$, regardless of $\\theta$.\n\nE. Column normalization, $\\|a_1\\|_2=\\|a_2\\|_2=1$, eliminates the instability in $b \\mapsto \\hat x$ caused by near collinearity, so both the projection and the coefficients are stable as $\\theta \\to 0$.",
            "solution": "The user wants me to solve a problem about the geometric interpretation of least squares.\n\n### Step 1: Extract Givens\n- Matrix $A \\in \\mathbb{R}^{m \\times 2}$ with columns $a_1, a_2 \\in \\mathbb{R}^m$.\n- Columns are nonzero: $a_1 \\neq 0$, $a_2 \\neq 0$.\n- Columns are normalized: $\\|a_1\\|_2=1$, $\\|a_2\\|_2=1$.\n- $\\theta \\in [0,\\pi]$ is the angle between the vectors $a_1$ and $a_2$.\n- The problem is the linear least-squares problem $\\min_{x \\in \\mathbb{R}^2} \\|Ax-b\\|_2$ for a given $b \\in \\mathbb{R}^m$.\n- $\\hat x$ is the minimum-norm least-squares solution.\n- The analysis concerns the case of small principal angles, $\\theta \\ll 1$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a standard topic in numerical linear algebra. All terms and concepts (least squares, singular values, principal angle, condition number, projection) are well-defined and fundamental to the field. The setup is mathematically sound.\n- **Well-Posed:** For any $A$ and $b$, a unique minimum-norm least-squares solution $\\hat x$ exists. The problem asks for an analysis of the behavior of this solution and related quantities as a parameter $\\theta$ changes. This is a well-posed question.\n- **Objective:** The statements to be evaluated are objective mathematical claims.\n- **Completeness and Consistency:** The problem provides all necessary information to analyze the system. The normalization condition $\\|a_1\\|_2=\\|a_2\\|_2=1$ and the definition of $\\theta$ as the angle between the vectors are consistent.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. I will proceed with the derivation and solution.\n\n### Derivation from First Principles\nThe least-squares solution is intimately related to the matrix $A^T A$. The matrix $A$ can be written as $A = [a_1, a_2]$. Let's compute $A^T A$:\n$$\nA^T A = \\begin{pmatrix} a_1^T \\\\ a_2^T \\end{pmatrix} [a_1, a_2] = \\begin{pmatrix} a_1^T a_1  a_1^T a_2 \\\\ a_2^T a_1  a_2^T a_2 \\end{pmatrix}\n$$\nUsing the given information:\n- $\\|a_1\\|_2 = 1 \\implies a_1^T a_1 = 1$.\n- $\\|a_2\\|_2 = 1 \\implies a_2^T a_2 = 1$.\n- The angle $\\theta$ between $a_1$ and $a_2$ satisfies $\\cos(\\theta) = \\frac{a_1^T a_2}{\\|a_1\\|_2 \\|a_2\\|_2} = a_1^T a_2$.\n\nSo, the matrix $A^T A$ is:\n$$\nA^T A = \\begin{pmatrix} 1  \\cos(\\theta) \\\\ \\cos(\\theta)  1 \\end{pmatrix}\n$$\nThe singular values of $A$, denoted $\\sigma_i$, are the square roots of the eigenvalues of $A^T A$. Let $\\lambda$ be an eigenvalue of $A^T A$. The characteristic equation is $\\det(A^T A - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 1-\\lambda  \\cos(\\theta) \\\\ \\cos(\\theta)  1-\\lambda \\end{pmatrix} = (1-\\lambda)^2 - \\cos^2(\\theta) = 0\n$$\nThis gives $1-\\lambda = \\pm \\cos(\\theta)$, so the eigenvalues are $\\lambda = 1 \\mp \\cos(\\theta)$.\nThe eigenvalues are $\\lambda_1 = 1 + \\cos(\\theta)$ and $\\lambda_2 = 1 - \\cos(\\theta)$. Since $\\theta \\in [0,\\pi]$, we have $-1 \\le \\cos(\\theta) \\le 1$, so both eigenvalues are non-negative, as expected.\n\nThe singular values of $A$ are the square roots of these eigenvalues:\n$$\n\\sigma_1 = \\sqrt{1 + \\cos(\\theta)} \\qquad \\text{and} \\qquad \\sigma_2 = \\sqrt{1 - \\cos(\\theta)}\n$$\nThe problem focuses on the case $\\theta \\ll 1$, which means $\\theta \\to 0$. In this limit, $\\cos(\\theta) \\to 1$.\n- The largest singular value: $\\sigma_{\\max}(A) = \\sigma_1 = \\sqrt{1 + \\cos(\\theta)} \\to \\sqrt{1+1} = \\sqrt{2}$.\n- The smallest singular value: $\\sigma_{\\min}(A) = \\sigma_2 = \\sqrt{1 - \\cos(\\theta)} \\to \\sqrt{1-1} = 0$.\n\nThe condition number of $A$ is $\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)} = \\frac{\\sqrt{1 + \\cos(\\theta)}}{\\sqrt{1 - \\cos(\\theta)}}$. As $\\theta \\to 0$, $\\kappa_2(A) \\to \\infty$. This indicates that the problem of finding the coefficients $x$ becomes ill-conditioned.\n\nThe minimum-norm least-squares solution is given by $\\hat x = A^\\dagger b$, where $A^\\dagger$ is the Moore-Penrose pseudoinverse of $A$. The projection of $b$ onto the column space of $A$, $\\operatorname{col}(A)$, is $p = A\\hat x = A A^\\dagger b$. The operator $P = A A^\\dagger$ is the orthogonal projector onto $\\operatorname{col}(A)$.\n\n### Option-by-Option Analysis\n\n**A. If $\\theta \\to 0$ with $\\|a_1\\|_2=\\|a_2\\|_2=1$, then the smallest singular value of $A$ tends to $0$, the column space $\\operatorname{col}(A)$ tends to a one-dimensional subspace, and the orthogonal projection of $b$ onto $\\operatorname{col}(A)$ remains well defined and varies smoothly with $b$.**\nThis statement consists of three claims about the limit $\\theta \\to 0$:\n1.  **\"the smallest singular value of $A$ tends to $0$\":** As derived above, $\\sigma_{\\min}(A) = \\sqrt{1 - \\cos(\\theta)}$. As $\\theta \\to 0$, $\\cos(\\theta) \\to 1$, so $\\sigma_{\\min}(A) \\to 0$. This is correct.\n2.  **\"the column space $\\operatorname{col}(A)$ tends to a one-dimensional subspace\":** The column space of $A$ is $\\operatorname{span}\\{a_1, a_2\\}$. As $\\theta \\to 0$, the vector $a_2$ becomes closer and closer to $a_1$ (since both are unit vectors). In the limit, the two vectors become identical, and the space they span collapses from a plane to the line $\\operatorname{span}\\{a_1\\}$. This is correct.\n3.  **\"the orthogonal projection of $b$ onto $\\operatorname{col}(A)$ remains well defined and varies smoothly with $b$\":** The projection is $p = P_\\theta b$, where $P_\\theta$ is the orthogonal projection operator onto $\\operatorname{col}(A_\\theta)$. While the formula $P_\\theta = A_\\theta(A_\\theta^T A_\\theta)^{-1}A_\\theta^T$ is ill-defined at $\\theta=0$ (because $A_0^T A_0$ is singular), the projection operator itself has a well-defined limit. As $\\theta \\to 0$, the subspace $\\operatorname{col}(A_\\theta)$ converges to the subspace $\\operatorname{span}\\{a_1\\}$, and the projector $P_\\theta$ converges to the projector $P_0 = a_1 a_1^T$. Thus, the projection is well-defined throughout the limit. For any fixed $\\theta$, the map $b \\mapsto p = P_\\theta b$ is a linear transformation, which is infinitely differentiable (smooth). All three parts of the statement are correct.\nVerdict: **Correct**.\n\n**B. For small $\\theta$, the least-squares coefficient map $b \\mapsto \\hat x$ is ill-conditioned: a perturbation $\\delta b$ can change $\\hat x$ by a factor on the order of $1/\\sigma_{\\min}(A)$, which diverges as $\\theta \\to 0$.**\nThe coefficient map is $\\hat x = A^\\dagger b$. A small perturbation $\\delta b$ in $b$ leads to a perturbation $\\delta \\hat x = A^\\dagger \\delta b$ in $\\hat x$. The magnitude of this effect is bounded by the operator norm of the map: $\\|\\delta \\hat x\\|_2 \\le \\|A^\\dagger\\|_2 \\|\\delta b\\|_2$. The operator norm of the pseudoinverse is the reciprocal of the smallest non-zero singular value: $\\|A^\\dagger\\|_2 = 1/\\sigma_{\\min}(A)$.\nAs shown before, $\\sigma_{\\min}(A) \\to 0$ as $\\theta \\to 0$. Therefore, $\\|A^\\dagger\\|_2 \\to \\infty$. A large operator norm signifies an ill-conditioned mapping, where small input perturbations can be magnified into large output perturbations. The factor of magnification can be as large as $1/\\sigma_{\\min}(A)$. This factor diverges as $\\theta \\to 0$. The statement accurately describes the instability of computing the coefficients $\\hat x$.\nVerdict: **Correct**.\n\n**C. Because the principal angle between $\\mathrm{span}\\{a_1\\}$ and $\\mathrm{span}\\{a_2\\}$ is small, the residual $r=b-A\\hat x$ is small for all $b$, so least-squares fits are uniformly accurate when columns are nearly collinear.**\nThe residual is $r = b - p = (I-P)b$, where $P$ is the projection onto $\\operatorname{col}(A)$. The magnitude of the residual, $\\|r\\|_2$, is the distance from $b$ to the subspace $\\operatorname{col}(A)$. This distance depends entirely on the component of $b$ that is orthogonal to $\\operatorname{col}(A)$. If we choose $b$ to be a vector in the orthogonal complement of $\\operatorname{col}(A)$ (which is possible as long as $m  2$), the projection is $p=0$ and the residual is $r=b$. The magnitude $\\|r\\|_2 = \\|b\\|_2$ can be arbitrarily large. The smallness of $\\theta$ does not guarantee a small residual for an arbitrary $b$. Therefore, least-squares fits are not uniformly accurate.\nVerdict: **Incorrect**.\n\n**D. The orthogonal projection map $b \\mapsto A\\hat x$ has operator norm $1$ and is therefore stable with respect to perturbations in $b$, regardless of $\\theta$.**\nThe map is $b \\mapsto p = A\\hat x$. This is the orthogonal projection map $b \\mapsto Pb$. An orthogonal projection operator $P$ onto a non-zero subspace has eigenvalues of $0$ and $1$. Since $P$ is self-adjoint ($P^T = P$), its operator $2$-norm is its largest eigenvalue, which is $1$. Thus, $\\|P\\|_2 = 1$.\nFor a perturbation $\\delta b$, the change in the projection is $\\delta p = P(\\delta b)$. The norm of the change is $\\|\\delta p\\|_2 = \\|P \\delta b\\|_2 \\le \\|P\\|_2 \\|\\delta b\\|_2 = 1 \\cdot \\|\\delta b\\|_2$. This means perturbations in $b$ are not amplified when computing the projection $p$. A map with an operator norm of $1$ is perfectly conditioned and thus stable. This property holds for any orthogonal projection, so it is true for any value of $\\theta$ for which $A$ is not the zero matrix (which is true here).\nVerdict: **Correct**.\n\n**E. Column normalization, $\\|a_1\\|_2=\\|a_2\\|_2=1$, eliminates the instability in $b \\mapsto \\hat x$ caused by near collinearity, so both the projection and the coefficients are stable as $\\theta \\to 0$.**\nThe entire analysis, which concluded that the coefficient map $b \\mapsto \\hat x$ is unstable as $\\theta \\to 0$, was performed *under the assumption* of column normalization. Normalization does not cure the ill-conditioning caused by near-collinearity (a small angle $\\theta$). The root cause of the instability is the near-linear-dependence of the columns, not their magnitude. The claim that normalization \"eliminates the instability\" is false. Consequently, the claim that \"the coefficients are stable\" is also false.\nVerdict: **Incorrect**.\n\nSummary of verdicts: Statements A, B, and D are correct. They describe complementary aspects of the geometry of least squares with nearly collinear regressors: the degeneration of the geometry (A), the instability of the coefficients (B), and the contrasting stability of the projection (D, also mentioned in A).",
            "answer": "$$\\boxed{ABD}$$"
        }
    ]
}