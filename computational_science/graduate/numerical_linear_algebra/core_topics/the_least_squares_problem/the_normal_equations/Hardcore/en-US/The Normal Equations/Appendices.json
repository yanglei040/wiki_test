{
    "hands_on_practices": [
        {
            "introduction": "Before we can appreciate the subtleties of numerical methods, we must have a firm grasp of the underlying theory. This first exercise guides you through deriving the normal equations, $A^TAx = A^Tb$, from the first principles of optimization . You will then see how these equations simplify dramatically in the ideal case of a system with orthonormal columns, revealing a clean and intuitive solution.",
            "id": "3592614",
            "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ with $m \\geq n$, and consider the linear least-squares problem of finding $x \\in \\mathbb{R}^{n}$ that minimizes the squared residual norm $x \\mapsto \\|A x - b\\|_{2}^{2}$ for a given $b \\in \\mathbb{R}^{m}$. \n\n(a) Starting from the definition of the least-squares objective and the fact that a minimizer must make the gradient of the objective vanish, derive the optimality condition that characterizes any minimizer.\n\n(b) Now assume that the columns of $A$ are orthonormal. Using only the optimality condition you derived in part (a) and the orthonormality assumption, obtain a closed-form expression for the unique minimizer in terms of $A$ and $b$. Justify uniqueness.\n\n(c) Consider the concrete instance with\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n0  \\tfrac{1}{\\sqrt{2}} \\\\\n0  \\tfrac{1}{\\sqrt{2}}\n\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n3 \\\\ 2 \\\\ -1\n\\end{pmatrix} \\in \\mathbb{R}^{3}.\n$$\nVerify that the columns of $A$ are orthonormal, and then compute the minimizer from part (b) explicitly. Express your final answer as a row vector. No rounding is required.",
            "solution": "The problem is valid as it is a well-posed, scientifically grounded, and objective problem in numerical linear algebra. It is free of any of the invalidating flaws listed in the problem validation guidelines. It asks for the derivation and application of fundamental results in linear least-squares theory.\n\n**(a) Derivation of the Optimality Condition**\n\nThe linear least-squares problem seeks to find a vector $x \\in \\mathbb{R}^{n}$ that minimizes the objective function $f(x)$, defined as the squared Euclidean norm of the residual vector $r = Ax - b$:\n$$f(x) = \\|Ax - b\\|_{2}^{2}$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ with $m \\geq n$, $x \\in \\mathbb{R}^{n}$, and $b \\in \\mathbb{R}^{m}$.\n\nThe squared norm can be expressed as a dot product, which in matrix notation is equivalent to a product with the transpose:\n$$f(x) = (Ax - b)^{T}(Ax - b)$$\nExpanding this expression using the properties of matrix transpose, $(P Q)^{T} = Q^{T} P^{T}$, we get:\n$$f(x) = (x^{T}A^{T} - b^{T})(Ax - b)$$\nDistributing the terms yields:\n$$f(x) = x^{T}A^{T}Ax - x^{T}A^{T}b - b^{T}Ax + b^{T}b$$\nThe terms $x^{T}A^{T}b$ and $b^{T}Ax$ are scalars. A scalar is equal to its own transpose. Let's take the transpose of the first one: $(x^{T}A^{T}b)^{T} = b^{T}(A^{T})^{T}(x^{T})^{T} = b^{T}Ax$. Thus, the two middle terms are identical. The objective function simplifies to:\n$$f(x) = x^{T}(A^{T}A)x - 2b^{T}Ax + b^{T}b$$\nThe function $f(x)$ is a quadratic form in $x$. Since the matrix $A^{T}A$ is positive semi-definite (as $z^T(A^TA)z = (Az)^T(Az) = \\|Az\\|_2^2 \\ge 0$), the function $f(x)$ is convex. A minimizer $x$ of a differentiable convex function must occur at a point where the gradient of the function with respect to $x$ is the zero vector. We compute the gradient, $\\nabla f(x)$:\n$$\\nabla f(x) = \\nabla_{x} \\left( x^{T}(A^{T}A)x - 2b^{T}Ax + b^{T}b \\right)$$\nUsing standard rules of matrix calculus:\n\\begin{itemize}\n    \\item The gradient of a quadratic form $x^{T}Mx$ is $(M+M^{T})x$. Since $A^{T}A$ is symmetric, $\\nabla_{x}(x^{T}(A^{T}A)x) = 2(A^{T}A)x$.\n    \\item The gradient of a linear form $c^{T}x$ is $c$. Here, $c^{T} = 2b^{T}A$, so $c = (2b^{T}A)^{T} = 2A^{T}b$. Thus, $\\nabla_{x}(-2b^{T}Ax) = -2A^{T}b$.\n    \\item The gradient of a constant term $b^{T}b$ is the zero vector.\n\\end{itemize}\nCombining these results gives the gradient:\n$$\\nabla f(x) = 2(A^{T}A)x - 2A^{T}b$$\nThe optimality condition is found by setting the gradient to zero:\n$$2(A^{T}A)x - 2A^{T}b = 0$$\nDividing by $2$ gives the celebrated **normal equations**:\n$$A^{T}Ax = A^{T}b$$\nThis is the optimality condition that any minimizer $x$ of $\\|Ax - b\\|_{2}^{2}$ must satisfy.\n\n**(b) Unique Minimizer for Orthonormal Columns**\n\nWe are now given that the columns of $A$ are orthonormal. Let the columns of $A$ be denoted by the vectors $a_{1}, a_{2}, \\ldots, a_{n}$, so $A = \\begin{pmatrix} a_{1}  a_{2}  \\cdots  a_{n} \\end{pmatrix}$. The orthonormality condition means that the dot product of any two distinct columns is zero, and the norm of each column is one:\n$$a_{i}^{T}a_{j} = \\delta_{ij} = \\begin{cases} 1  \\text{if } i=j \\\\ 0  \\text{if } i \\neq j \\end{cases}$$\nLet us examine the matrix product $A^{T}A$. The entry in the $i$-th row and $j$-th column of $A^{T}A$ is given by the product of the $i$-th row of $A^{T}$ and the $j$-th column of $A$. The $i$-th row of $A^{T}$ is precisely the transpose of the $i$-th column of $A$, which is $a_{i}^{T}$. Therefore:\n$$(A^{T}A)_{ij} = a_{i}^{T}a_{j} = \\delta_{ij}$$\nThis shows that the matrix $A^{T}A$ is the $n \\times n$ identity matrix, $I_{n}$.\n\nSubstituting this result into the optimality condition derived in part (a), $A^{T}Ax = A^{T}b$, we get:\n$$I_{n}x = A^{T}b$$\nThis immediately simplifies to a closed-form expression for the minimizer $x$:\n$$x = A^{T}b$$\nTo justify the uniqueness of this minimizer, we note that the solution to the linear system $(A^{T}A)x = A^{T}b$ is unique if and only if the matrix $A^{T}A$ is invertible. In this case, we have shown that $A^{T}A = I_{n}$. The identity matrix is its own inverse, $(I_{n})^{-1} = I_{n}$, and is therefore invertible. Thus, the solution $x = (A^{T}A)^{-1}A^{T}b = I_{n}^{-1}A^{T}b = A^{T}b$ is the unique minimizer.\n\n**(c) Explicit Computation**\n\nWe are given the specific instance:\n$$\nA = \\begin{pmatrix}\n1  0 \\\\\n0  \\frac{1}{\\sqrt{2}} \\\\\n0  \\frac{1}{\\sqrt{2}}\n\\end{pmatrix},\n\\qquad\nb = \\begin{pmatrix}\n3 \\\\\n2 \\\\\n-1\n\\end{pmatrix}\n$$\nFirst, we verify that the columns of $A$ are orthonormal. Let the columns be $a_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ and $a_{2} = \\begin{pmatrix} 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}$.\n\nWe compute the norms:\n$$\\|a_{1}\\|_{2}^{2} = a_{1}^{T}a_{1} = 1^{2} + 0^{2} + 0^{2} = 1 \\implies \\|a_{1}\\|_{2} = 1$$\n$$\\|a_{2}\\|_{2}^{2} = a_{2}^{T}a_{2} = 0^{2} + \\left(\\frac{1}{\\sqrt{2}}\\right)^{2} + \\left(\\frac{1}{\\sqrt{2}}\\right)^{2} = 0 + \\frac{1}{2} + \\frac{1}{2} = 1 \\implies \\|a_{2}\\|_{2} = 1$$\nThe columns have unit norm.\n\nWe compute their dot product to check for orthogonality:\n$$a_{1}^{T}a_{2} = (1)(0) + (0)\\left(\\frac{1}{\\sqrt{2}}\\right) + (0)\\left(\\frac{1}{\\sqrt{2}}\\right) = 0$$\nThe columns are orthogonal. Since they are also of unit norm, the columns of $A$ are orthonormal.\n\nSince the columns of $A$ are orthonormal, we can use the formula derived in part (b) to find the unique least-squares minimizer $x$:\n$$x = A^{T}b$$\n First, we find the transpose of $A$:\n$$A^{T} = \\begin{pmatrix}\n1  0  0 \\\\\n0  \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}$$\nNow, we perform the matrix-vector multiplication:\n$$x = \\begin{pmatrix}\n1  0  0 \\\\\n0  \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n\\begin{pmatrix}\n3 \\\\\n2 \\\\\n-1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n(1)(3) + (0)(2) + (0)(-1) \\\\\n(0)(3) + \\left(\\frac{1}{\\sqrt{2}}\\right)(2) + \\left(\\frac{1}{\\sqrt{2}}\\right)(-1)\n\\end{pmatrix}\n$$\n$$x = \\begin{pmatrix}\n3 \\\\\n\\frac{2}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\\n\\frac{1}{\\sqrt{2}}\n\\end{pmatrix}$$\nThe problem asks for the answer as a row vector. Therefore, the minimizer is represented as $[3, \\frac{1}{\\sqrt{2}}]$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3  \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The theoretical elegance of the normal equations can be dangerously misleading in the world of finite-precision arithmetic. The formation of the matrix $A^{T}A$ squares the condition number of the problem, a process that can lead to a catastrophic loss of numerical information. This computational exercise allows you to witness this failure firsthand by building and comparing a fragile normal equations solver against the robust Singular Value Decomposition (SVD) method on ill-conditioned systems .",
            "id": "3205220",
            "problem": "You are to design and implement a complete, runnable program that demonstrates algorithm robustness and stability in solving linear least-squares problems. From first principles, linear least-squares seeks an $\\ell_{2}$-norm minimizing vector $x \\in \\mathbb{R}^{n}$ that minimizes the residual $\\lVert A x - b \\rVert_{2}$ for a given matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^{m}$. Two classical approaches are: (i) forming and solving the normal equations $A^{T} A x = A^{T} b$, and (ii) using the Singular Value Decomposition (SVD) to compute a pseudoinverse solution that is numerically more stable in the presence of poor conditioning. Robustness concerns how an algorithm behaves under perturbations (for example, roundoff or small changes in data), and stability concerns the algorithm’s sensitivity to the conditioning of the input.\n\nStarting only from core definitions and well-tested facts (the linear least-squares objective, the concept of conditioning, and the existence of the Singular Value Decomposition), construct and test problems where solving the normal equations is numerically fragile. Your program must:\n\n- Implement two solvers for the linear least-squares problem:\n  1. A normal-equations solver that explicitly forms $A^{T} A$ and $A^{T} b$ and solves $A^{T} A x = A^{T} b$ using direct methods.\n  2. An SVD-based pseudoinverse solver. Given the compact SVD $A = U \\Sigma V^{T}$ with singular values $\\sigma_{1} \\ge \\cdots \\ge \\sigma_{n} \\ge 0$, define the pseudoinverse by discarding singular values below a tolerance and inverting the rest. Use the tolerance $\\tau_{\\sigma} = u \\cdot \\max(m,n) \\cdot \\sigma_{1}$, where $u$ is the unit roundoff of double precision, $u \\approx 2.22 \\times 10^{-16}$.\n\n- Define success and failure criteria:\n  - For any candidate solution $\\hat{x}$, define the relative residual $r(\\hat{x}) = \\lVert A \\hat{x} - b \\rVert_{2} / \\lVert b \\rVert_{2}$.\n  - When a ground-truth solution $x^{\\star}$ is known, define the relative solution error $e(\\hat{x}) = \\lVert \\hat{x} - x^{\\star} \\rVert_{2} / \\lVert x^{\\star} \\rVert_{2}$.\n  - Declare the SVD solver a success if $r(\\hat{x}_{\\mathrm{SVD}}) \\le \\tau_{\\mathrm{res}}$, with $\\tau_{\\mathrm{res}} = 10^{-10}$.\n  - Declare the normal-equations solver a failure if it raises a singularity error or produces a solution with either $e(\\hat{x}_{\\mathrm{NE}}) > \\tau_{\\mathrm{sol}}$ when $x^{\\star}$ is known, or $r(\\hat{x}_{\\mathrm{NE}}) > \\tau_{\\mathrm{res,NE}}$ when $x^{\\star}$ is unknown. Use $\\tau_{\\mathrm{sol}} = 10^{-2}$ and $\\tau_{\\mathrm{res,NE}} = 10^{-6}$.\n\n- For each test case, output a boolean indicating whether solving the normal equations fails while the SVD-based approach succeeds. Specifically, the boolean for a test case must be the truth value of\n  $\n  \\big(\\text{SVD success}\\big) \\land \\big(\\text{normal-equations failure}\\big).\n  $\n\nConstruct the following test suite of matrices $A$ and vectors $b$, each with specified dimensions and parameters. In all cases, use deterministic random number generation with fixed seeds to ensure reproducibility, and compute $b$ as $b = A x^{\\star} + \\eta$ with a specified noise vector $\\eta$ (if applicable).\n\n- Test case $1$ (well-conditioned benchmark, happy path):\n  - Dimensions: $m = 60$, $n = 20$.\n  - Construction: form $A = U \\Sigma V^{T}$ with $U \\in \\mathbb{R}^{60 \\times 20}$ and $V \\in \\mathbb{R}^{20 \\times 20}$ orthonormal, and singular values $\\sigma_{k}$ linearly spaced between $1$ and $0.5$ for $k = 1,\\dots,20$.\n  - Ground truth: draw $x^{\\star} \\in \\mathbb{R}^{20}$ from a fixed seed and set $\\eta = 0$.\n\n- Test case $2$ (extremely ill-conditioned, poor stability of normal equations):\n  - Dimensions: $m = 80$, $n = 20$.\n  - Construction: form $A = U \\Sigma V^{T}$ as above with $\\sigma_{k}$ logarithmically spaced from $1$ down to $10^{-8}$, so that $\\kappa_{2}(A)$ is approximately $10^{8}$.\n  - Ground truth: draw $x^{\\star} \\in \\mathbb{R}^{20}$ from a fixed seed and add small noise $\\eta$ with $\\lVert \\eta \\rVert_{2}$ scaled to approximately $10^{-12} \\cdot \\lVert A x^{\\star} \\rVert_{2}$.\n\n- Test case $3$ (classical ill-conditioned design, Hilbert-type matrix):\n  - Dimensions: $m = 40$, $n = 15$.\n  - Construction: define $A_{ij} = \\dfrac{1}{i + j - 1}$ for $i = 1,\\dots,40$ and $j = 1,\\dots,15$.\n  - Ground truth: draw $x^{\\star} \\in \\mathbb{R}^{15}$ from a fixed seed and add noise $\\eta$ with $\\lVert \\eta \\rVert_{2}$ scaled to approximately $10^{-10} \\cdot \\lVert A x^{\\star} \\rVert_{2}$.\n\n- Test case $4$ (rank-deficient, singular normal equations):\n  - Dimensions: $m = 50$, $n = 20$.\n  - Construction: draw a random $A \\in \\mathbb{R}^{50 \\times 20}$ from a fixed seed and set its second column equal to its first column exactly, making $A$ rank-deficient.\n  - Ground truth: draw $x^{\\star} \\in \\mathbb{R}^{20}$ from a fixed seed and set $\\eta = 0$.\n\nYour program must compute, for each test case, the boolean described above. The final output must be a single line containing a comma-separated list of these booleans enclosed in square brackets, for example, $\\big[ \\text{true}, \\text{false}, \\text{true}, \\text{true} \\big]$ but using the exact capitalization and formatting produced by Python booleans.\n\nNo physical units are involved. Angles are not involved. Express any threshold values numerically as decimals (do not use percentage signs).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\big[ \\text{True}, \\text{False}, \\text{True}, \\text{True} \\big]$.",
            "solution": "The tasked problem is to demonstrate the superior numerical stability of the Singular Value Decomposition (SVD) method over the normal equations method for solving linear least-squares problems, particularly for ill-conditioned or rank-deficient systems. The core of the problem is to find a vector $x \\in \\mathbb{R}^{n}$ that minimizes the Euclidean norm of the residual, $\\lVert A x - b \\rVert_{2}$, for a given matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^{m}$. We will implement and compare two classical algorithms for this task.\n\n**The Normal Equations Method**\n\nThe first principles of optimization dictate that for a solution $x$ to be a minimizer of the convex function $f(x) = \\frac{1}{2} \\lVert Ax - b \\rVert_2^2$, its gradient must be zero. The gradient is given by $\\nabla f(x) = A^{T}(Ax - b)$. Setting the gradient to zero yields the celebrated normal equations:\n$$\nA^{T} A x = A^{T} b\n$$\nThis transforms the least-squares problem into a square linear system of equations. If the matrix $A$ has full column rank, then the Gram matrix $A^{T} A \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive definite, and a unique solution $x$ can be found using standard methods like Cholesky decomposition or LU factorization.\n\nThe critical flaw of this method lies in its numerical stability. The condition number of a matrix, $\\kappa(M)$, measures the sensitivity of the solution of $Mx=y$ to perturbations in $y$. For the normal equations, the relevant matrix is $A^{T} A$. It is a fundamental result in numerical linear algebra that the condition number of this Gram matrix is the square of the condition number of the original matrix $A$:\n$$\n\\kappa_2(A^{T} A) = \\big(\\kappa_2(A)\\big)^2\n$$\nIf $A$ is ill-conditioned (i.e., $\\kappa_2(A)$ is large), $\\kappa_2(A^{T} A)$ can become enormous. For example, if $\\kappa_2(A) = 10^8$, then $\\kappa_2(A^{T} A) = 10^{16}$. In standard double-precision floating-point arithmetic, which has about $16$ decimal digits of precision, the matrix $A^{T} A$ becomes computationally indistinguishable from a singular matrix. The explicit formation of $A^{T} A$ can lead to a catastrophic loss of information.\n\n**The SVD-Based Pseudoinverse Method**\n\nA more numerically robust approach is based on the Singular Value Decomposition (SVD). Any matrix $A \\in \\mathbb{R}^{m \\times n}$ can be factored as:\n$$\nA = U \\Sigma V^{T}\n$$\nwhere $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix with non-negative real numbers on the diagonal, known as the singular values, $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{\\min(m,n)} \\ge 0$. For the compact SVD of a tall matrix ($m \\ge n$), $U$ is $m \\times n$, $\\Sigma$ is $n \\times n$, and $V$ is $n \\times n$.\n\nThe solution to the least-squares problem can be expressed using the Moore-Penrose pseudoinverse, $A^{\\dagger}$. The minimum-norm least-squares solution is $x = A^{\\dagger} b$. The SVD provides a stable way to compute the pseudoinverse:\n$$\nA^{\\dagger} = V \\Sigma^{\\dagger} U^{T}\n$$\nwhere $\\Sigma^{\\dagger}$ is the pseudoinverse of $\\Sigma$. It is obtained by taking the reciprocal of the non-zero singular values and transposing the resulting matrix. To ensure numerical stability, singular values that are very small (close to zero) are treated as if they are zero. This is achieved by thresholding. Any singular value $\\sigma_i$ below a tolerance $\\tau_{\\sigma}$ is treated as zero. The problem specifies a standard tolerance:\n$$\n\\tau_{\\sigma} = u \\cdot \\max(m,n) \\cdot \\sigma_{1}\n$$\nwhere $u$ is the machine unit roundoff (approximately $2.22 \\times 10^{-16}$ for double precision) and $\\sigma_1$ is the largest singular value. This method avoids forming $A^{T} A$ and thus circumvents the squaring of the condition number, making it highly robust.\n\n**Implementation and Evaluation Strategy**\n\nTwo solvers are implemented: `solve_normal_equations` which forms and solves the normal equations, and `solve_svd` which computes the solution via the SVD-based pseudoinverse.\n\nThe program evaluates these solvers on four test cases designed to expose the fragility of the normal equations:\n1.  **Well-conditioned Benchmark**: A matrix with a small condition number ($\\kappa_2(A)=2$). Both methods are expected to perform well.\n2.  **Extremely Ill-conditioned**: A matrix with a large condition number ($\\kappa_2(A)=10^8$), such that $\\kappa_2(A^{T}A) \\approx 10^{16}$, which is at the limit of double-precision arithmetic.\n3.  **Hilbert-type Matrix**: A classic example of an ill-conditioned matrix that arises in approximation theory.\n4.  **Rank-deficient Matrix**: A matrix whose columns are linearly dependent. Here, $A^{T}A$ is exactly singular, guaranteeing the failure of the normal equations method if a direct solver is used.\n\nFor each test case, we determine the boolean value of the expression $(\\text{SVD success}) \\land (\\text{normal-equations failure})$.\n-   SVD success is defined by the relative residual $\\lVert A \\hat{x}_{\\mathrm{SVD}} - b \\rVert_{2} / \\lVert b \\rVert_{2} \\le 10^{-10}$.\n-   Normal equations failure is defined by a singularity error during solution, or if the relative solution error $\\lVert \\hat{x}_{\\mathrm{NE}} - x^{\\star} \\rVert_{2} / \\lVert x^{\\star} \\rVert_{2} > 10^{-2}$, where $x^{\\star}$ is the known ground-truth solution.\n\nThe final output aggregates these boolean results, demonstrating the scenarios where the normal equations method fails while the SVD method remains robust and successful.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    \n    # Define constants from the problem statement.\n    U_EPS = np.finfo(float).eps\n    TAU_RES = 1e-10  # SVD success threshold for relative residual\n    TAU_SOL = 1e-2   # NE failure threshold for relative solution error\n    TAU_RES_NE = 1e-6 # NE failure threshold for relative residual (if x_star is unknown)\n\n    def solve_normal_equations(A, b):\n        \"\"\"\n        Solves min||Ax-b||_2 using the normal equations A.T*A*x = A.T*b.\n        Returns a tuple: (solution vector, singularity_flag).\n        The flag is True if a LinAlgError (singularity) is caught.\n        \"\"\"\n        AtA = A.T @ A\n        Atb = A.T @ b\n        try:\n            x_ne = np.linalg.solve(AtA, Atb)\n            return x_ne, False\n        except np.linalg.LinAlgError:\n            return None, True\n\n    def solve_svd(A, b):\n        \"\"\"\n        Solves min||Ax-b||_2 using SVD-based pseudoinverse.\n        \"\"\"\n        m, n = A.shape\n        U, s, Vt = np.linalg.svd(A, full_matrices=False)\n        \n        sigma_1 = s[0] if s.size > 0 else 0.0\n        tau_sigma = U_EPS * max(m, n) * sigma_1\n        \n        s_pinv = np.zeros_like(s)\n        if s.size > 0:\n            s_pinv[s > tau_sigma] = 1.0 / s[s > tau_sigma]\n        \n        # This computes x = V @ diag(s_pinv) @ U.T @ b efficiently\n        x_svd = Vt.T @ (s_pinv * (U.T @ b))\n        return x_svd\n\n    def evaluate_case(A, b, x_star):\n        \"\"\"\n        Performs the evaluation for a single test case.\n        Returns a boolean for (SVD_success AND NE_failure).\n        \"\"\"\n        # SVD Solver Evaluation\n        x_svd = solve_svd(A, b)\n        norm_b = np.linalg.norm(b)\n        if norm_b > 0:\n            r_svd = np.linalg.norm(A @ x_svd - b) / norm_b\n        else: # Handle b=0 case\n            r_svd = np.linalg.norm(A @ x_svd - b)\n        svd_success = r_svd = TAU_RES\n\n        # Normal Equations Solver Evaluation\n        x_ne, singularity = solve_normal_equations(A, b)\n        ne_failure = singularity\n        if not ne_failure:\n            norm_x_star = np.linalg.norm(x_star)\n            if norm_x_star > 0:\n                e_ne = np.linalg.norm(x_ne - x_star) / norm_x_star\n                if e_ne > TAU_SOL:\n                    ne_failure = True\n            elif np.linalg.norm(x_ne) > 0: # If x_star is zero, any non-zero solution is an error\n                ne_failure = True\n        \n        return svd_success and ne_failure\n\n    # --- Test Case Generation ---\n\n    def generate_case_1():\n        # Well-conditioned benchmark\n        m, n = 60, 20\n        rng = np.random.default_rng(123)\n        U, _ = np.linalg.qr(rng.standard_normal((m, n)))\n        V, _ = np.linalg.qr(rng.standard_normal((n, n)))\n        sigma = np.linspace(1.0, 0.5, n)\n        A = U @ np.diag(sigma) @ V.T\n        x_star = rng.standard_normal(n)\n        b = A @ x_star\n        return A, b, x_star\n\n    def generate_case_2():\n        # Extremely ill-conditioned\n        m, n = 80, 20\n        rng = np.random.default_rng(456)\n        U, _ = np.linalg.qr(rng.standard_normal((m, n)))\n        V, _ = np.linalg.qr(rng.standard_normal((n, n)))\n        sigma = np.logspace(0, -8, n)\n        A = U @ np.diag(sigma) @ V.T\n        x_star = rng.standard_normal(n)\n        b_noiseless = A @ x_star\n        eta_raw = rng.standard_normal(m)\n        norm_eta_raw = np.linalg.norm(eta_raw)\n        if norm_eta_raw > 0:\n            eta = eta_raw * (1e-12 * np.linalg.norm(b_noiseless)) / norm_eta_raw\n        else:\n            eta = np.zeros(m)\n        b = b_noiseless + eta\n        return A, b, x_star\n\n    def generate_case_3():\n        # Hilbert-type matrix\n        m, n = 40, 15\n        rng = np.random.default_rng(789)\n        i = np.arange(1, m + 1)[:, np.newaxis]\n        j = np.arange(1, n + 1)\n        A = 1.0 / (i + j - 1)\n        x_star = rng.standard_normal(n)\n        b_noiseless = A @ x_star\n        eta_raw = rng.standard_normal(m)\n        norm_eta_raw = np.linalg.norm(eta_raw)\n        if norm_eta_raw > 0:\n            eta = eta_raw * (1e-10 * np.linalg.norm(b_noiseless)) / norm_eta_raw\n        else:\n            eta = np.zeros(m)\n        b = b_noiseless + eta\n        return A, b, x_star\n\n    def generate_case_4():\n        # Rank-deficient\n        m, n = 50, 20\n        rng = np.random.default_rng(101)\n        A = rng.standard_normal((m, n))\n        A[:, 1] = A[:, 0]\n        x_star = rng.standard_normal(n)\n        b = A @ x_star\n        return A, b, x_star\n\n    # --- Run Suite and Print Results ---\n    \n    test_generators = [\n        generate_case_1,\n        generate_case_2,\n        generate_case_3,\n        generate_case_4,\n    ]\n\n    results = []\n    for gen_func in test_generators:\n        A, b, x_star = gen_func()\n        result = evaluate_case(A, b, x_star)\n        results.append(result)\n\n    # Format output as specified: [True,False,True,True]\n    formatted_results = [str(r).capitalize() for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having observed that the normal equations can fail, a crucial question arises for any practitioner: when do they fail? This final practice provides a quantitative answer by bridging theory with numerical experiment . You will derive and verify a powerful predictor inequality that connects the matrix condition number, $\\kappa_2(A)$, and machine precision, $\\epsilon_{\\text{mach}}$, to predict the precise point at which the normal equations become numerically unreliable.",
            "id": "3540736",
            "problem": "Consider the overdetermined linear least-squares problem in pure mathematical terms. Let $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$ and full column rank, and let $b \\in \\mathbb{R}^m$. The least-squares solution minimizes $\\|Ax - b\\|_2$ over $x \\in \\mathbb{R}^n$. A classical algorithm solves the normal equations $A^T A x = A^T b$. For floating-point arithmetic with unit roundoff (machine epsilon) $\\epsilon_{\\text{mach}}$, solving the normal equations is known to suffer amplified conditioning relative to solving the least-squares problem by more stable methods. You will construct a controlled family of matrices to study when the normal equations are acceptable in single precision and when they are acceptable in double precision, and you will connect this to a predictor inequality that separates a numerically stable regime from an unstable regime.\n\nStart from the following fundamental base:\n- The $2$-norm condition number of a full column rank matrix is defined by $\\kappa_2(A) = \\sigma_{\\max}(A)/\\sigma_{\\min}(A)$, where $\\sigma_{\\max}(A)$ and $\\sigma_{\\min}(A)$ are the largest and smallest singular values of $A$.\n- The normal equations matrix is $A^T A$, which is symmetric positive definite with $2$-norm condition number $\\kappa_2(A^T A) = \\kappa_2(A)^2$.\n- The first-order model of rounding in floating-point arithmetic with unit roundoff $\\epsilon_{\\text{mach}}$ states that each basic arithmetic operation incurs a relative error bounded (to first order) by a constant multiple of $\\epsilon_{\\text{mach}}$, and solving a well-conditioned linear system by a backward-stable method exhibits forward error proportional to the condition number times $\\epsilon_{\\text{mach}}$ in the corresponding norm.\n\nYour tasks are:\n1. Construct a parametric family of matrices $A(\\gamma) \\in \\mathbb{R}^{m \\times 2}$ with prescribed singular values $\\sigma_1 = 1$ and $\\sigma_2 = 10^{-\\gamma}$ for a parameter $\\gamma \\ge 0$. Ensure $m \\ge 2$ and full column rank. Use an orthonormal basis $U \\in \\mathbb{R}^{m \\times 2}$ to form $A(\\gamma) = U \\operatorname{diag}(\\sigma_1, \\sigma_2)$ so that $\\kappa_2(A(\\gamma)) = 10^{\\gamma}$. Choose a fixed nonzero vector $x_\\star \\in \\mathbb{R}^2$ and set $b(\\gamma) = A(\\gamma) x_\\star$.\n2. Using the floating-point model and the conditioning facts above, derive from first principles a predictor inequality that decides, based only on $\\kappa_2(A)$, $\\epsilon_{\\text{mach}}$, and a tolerance $\\tau > 0$, whether solving the normal equations should be considered numerically acceptable in a given precision. Your derivation must start with the sensitivity of solving linear systems and the relationship $\\kappa_2(A^T A) = \\kappa_2(A)^2$. Define “acceptable” precisely as producing a solution $x$ whose relative error satisfies $\\|x - x_\\star\\|_2 / \\|x_\\star\\|_2 \\le \\tau$.\n3. Implement a program that, for each test case described below, does the following:\n   - Constructs $A(\\gamma)$ and $b(\\gamma)$ deterministically.\n   - Solves the normal equations in single precision ($\\epsilon_{\\text{mach}}$ for $32$-bit floating point) and in double precision ($\\epsilon_{\\text{mach}}$ for $64$-bit floating point) to obtain $x^{(32)}(\\gamma)$ and $x^{(64)}(\\gamma)$, respectively. The construction of $A(\\gamma)$ and $b(\\gamma)$ may be carried out in higher precision, but the formation of $A^T A$, $A^T b$, and the solve must be performed in the target precision.\n   - Computes the measured relative errors $e^{(p)}(\\gamma) = \\|x^{(p)}(\\gamma) - x_\\star\\|_2/\\|x_\\star\\|_2$ for $p \\in \\{32,64\\}$.\n   - Computes a predicted stability classification in each precision using your derived inequality, which must be expressed in terms of $\\kappa_2(A(\\gamma))$, $\\epsilon_{\\text{mach}}$ for the precision, and $\\tau$.\n   - Encodes a “phase” category as an integer in $\\{0,1,2,3\\}$:\n     - $0$ if both single and double are acceptable,\n     - $1$ if single is unacceptable and double is acceptable,\n     - $2$ if single is acceptable and double is unacceptable,\n     - $3$ if both are unacceptable.\n     Produce this category twice: once from the predictor inequality (“predicted category”) and once from the measured errors (“measured category”).\n4. The test suite must use the following fixed parameters:\n   - $m = 200$, $n = 2$,\n   - $x_\\star = [1,-1]^T$,\n   - tolerance $\\tau = 10^{-3}$,\n   - parameter values $\\gamma \\in \\{0.5,\\, 2.2,\\, 6.0,\\, 6.3,\\, 6.4,\\, 8.0\\}$,\n   - a fixed random seed for constructing $U$ so that results are deterministic.\n5. Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one $\\gamma$ in the order given and is itself a two-element list of integers $[\\text{predicted\\_category},\\text{measured\\_category}]$. For example, an output with two test cases could look like: \"[[0,0],[1,1]]\". No spaces are permitted anywhere in the line.\n\nThere are no physical units in this problem. All angles, if any, are absent. All results are dimensionless. Each element of the final list must be an integer. Your code must be fully self-contained and produce the exact final output format.",
            "solution": "The problem is assessed to be valid. It is a well-posed, scientifically grounded problem in numerical linear algebra that is self-contained and free of contradictions or ambiguities. It requests a standard theoretical derivation followed by a numerical experiment to illustrate a fundamental concept: the ill-conditioning of the normal equations for least-squares problems.\n\n### 1. Derivation of the Predictor Inequality\n\nThe goal is to derive an inequality that predicts when the numerical solution to the normal equations is \"acceptable,\" defined by the relative error of the computed solution $x$ being bounded by a tolerance $\\tau$.\n$$ \\frac{\\|x - x_\\star\\|_2}{\\|x_\\star\\|_2} \\le \\tau $$\nwhere $x_\\star$ is the exact solution.\n\nThe normal equations are given by $A^T A x = A^T b$. We can express this as a standard linear system $Mx = y$, where the matrix is $M = A^T A$ and the right-hand side is $y = A^T b$. The problem states that for the constructed test cases, $b = A x_\\star$, which implies that $x_\\star$ is the exact solution to the least-squares problem and also to the normal equations, as $A^T A x_\\star = A^T (A x_\\star) = A^T b$.\n\nThe core of the analysis lies in the forward error of solving the linear system $Mx = y$ in floating-point arithmetic. Standard-error analysis for a backward-stable linear system solver (such as one based on Cholesky factorization, suitable for the symmetric positive definite matrix $M = A^T A$) provides the following bound on the relative error of the computed solution $\\hat{x}$:\n$$ \\frac{\\|\\hat{x} - x_\\star\\|_2}{\\|x_\\star\\|_2} \\le c \\cdot \\kappa_2(M) \\cdot \\epsilon_{\\text{mach}} $$\nHere, $\\epsilon_{\\text{mach}}$ is the machine epsilon (unit roundoff) of the floating-point precision used, $\\kappa_2(M)$ is the $2$-norm condition number of the matrix $M$, and $c$ is a constant of order $1$ that depends on the dimensions of the matrix and the details of the algorithm. This bound incorporates the errors from both forming the system $(M, y)$ and solving it.\n\nThe problem provides the crucial relationship between the condition number of the original matrix $A$ and the normal equations matrix $M=A^T A$:\n$$ \\kappa_2(A^T A) = \\kappa_2(A)^2 $$\nSubstituting this into the error bound, we get:\n$$ \\frac{\\|\\hat{x} - x_\\star\\|_2}{\\|x_\\star\\|_2} \\le c \\cdot \\kappa_2(A)^2 \\cdot \\epsilon_{\\text{mach}} $$\nTo create a simple, first-order predictor as requested, we can model the behavior by taking the constant $c=1$. This gives a direct estimate for the relative error:\n$$ e_{\\text{pred}} \\approx \\kappa_2(A)^2 \\cdot \\epsilon_{\\text{mach}} $$\nThe solution is deemed \"acceptable\" if the measured error is no more than $\\tau$. We can therefore predict acceptability by checking if our estimated error bound is within this tolerance:\n$$ \\kappa_2(A)^2 \\cdot \\epsilon_{\\text{mach}} \\le \\tau $$\nThis is the predictor inequality. For the specific parametric family of matrices $A(\\gamma)$ with $\\kappa_2(A(\\gamma)) = 10^{\\gamma}$, the inequality becomes:\n$$ (10^\\gamma)^2 \\cdot \\epsilon_{\\text{mach}} \\le \\tau \\quad \\implies \\quad 10^{2\\gamma} \\cdot \\epsilon_{\\text{mach}} \\le \\tau $$\nThis inequality will be used to classify the predicted stability for single precision ($\\epsilon_{\\text{mach}} \\approx 1.19 \\times 10^{-7}$) and double precision ($\\epsilon_{\\text{mach}} \\approx 2.22 \\times 10^{-16}$).\n\n### 2. Numerical Experiment Design\n\nThe implementation will systematically test this prediction. For each parameter $\\gamma$ in the test suite $\\{0.5, 2.2, 6.0, 6.3, 6.4, 8.0\\}$:\n\n1.  **Matrix Construction**: A fixed random seed is used to generate a matrix $R \\in \\mathbb{R}^{200 \\times 2}$. The QR decomposition of $R$ yields a matrix $U \\in \\mathbb{R}^{200 \\times 2}$ with orthonormal columns. This $U$ is fixed for all test cases. The matrix $A(\\gamma)$ is then constructed as $A(\\gamma) = U \\Sigma(\\gamma)$, where $\\Sigma(\\gamma) = \\operatorname{diag}(1, 10^{-\\gamma})$. This construction ensures that the singular values of $A(\\gamma)$ are indeed $1$ and $10^{-\\gamma}$, and thus $\\kappa_2(A(\\gamma)) = 10^\\gamma$. The vector $b(\\gamma)$ is set to $A(\\gamma)x_\\star$, where $x_\\star = [1, -1]^T$. This ensures the exact solution is known and the least-squares problem has zero residual.\n\n2.  **Solving and Error Measurement**: For each precision (32-bit and 64-bit), the following steps are performed:\n    a. The matrices $A(\\gamma)$ and $b(\\gamma)$ are converted to the target precision.\n    b. The normal equations matrix $A^T A$ and vector $A^T b$ are formed using arithmetic in that target precision.\n    c. The linear system $(A^T A)x = (A^T b)$ is solved to find the computed solution $x^{(p)}(\\gamma)$ for precision $p \\in \\{32, 64\\}$.\n    d. The measured relative error $e^{(p)}(\\gamma) = \\|x^{(p)}(\\gamma) - x_\\star\\|_2 / \\|x_\\star\\|_2$ is computed.\n\n3.  **Categorization**:\n    -   **Predicted Category**: For each precision $p$, the inequality $10^{2\\gamma} \\cdot \\epsilon_{\\text{mach}}^{(p)} \\le \\tau$ is evaluated. A precision is deemed acceptable if the inequality holds.\n    -   **Measured Category**: For each precision $p$, the measured error $e^{(p)}(\\gamma)$ is compared against the tolerance $\\tau$. The solution is deemed acceptable if $e^{(p)}(\\gamma) \\le \\tau$.\n    -   Based on these acceptability checks for single and double precision, a category from $\\{0, 1, 2, 3\\}$ is assigned for both the prediction and the measurement, according to the rules specified in the problem statement. Category $2$ (single acceptable, double unacceptable) is not expected to occur.\n\nThis procedure generates a pair of categories, $[\\text{predicted\\_category}, \\text{measured\\_category}]$, for each value of $\\gamma$, which are then formatted into the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical analysis problem concerning the conditioning of normal equations.\n    \"\"\"\n    # Task 4: Fixed parameters\n    m = 200\n    n = 2\n    x_star = np.array([1.0, -1.0], dtype=np.float64)\n    tau = 1e-3\n    gamma_values = [0.5, 2.2, 6.0, 6.3, 6.4, 8.0]\n    random_seed = 42\n\n    # Machine epsilon for single and double precision\n    eps_32 = np.finfo(np.float32).eps\n    eps_64 = np.finfo(np.float64).eps\n\n    # Generate a fixed orthonormal basis U\n    np.random.seed(random_seed)\n    # Generate a random m x n matrix\n    R = np.random.randn(m, n)\n    # Use QR decomposition to get an orthonormal basis for its column space\n    U, _ = np.linalg.qr(R)\n    U = U.astype(np.float64)\n\n    results = []\n\n    # Helper function for categorization\n    def get_category(single_ok, double_ok):\n        if single_ok and double_ok:\n            return 0  # Both acceptable\n        elif not single_ok and double_ok:\n            return 1  # Single unacceptable, double acceptable\n        elif single_ok and not double_ok:\n            return 2  # Single acceptable, double unacceptable (not expected)\n        else: # not single_ok and not double_ok\n            return 3  # Both unacceptable\n\n    for gamma in gamma_values:\n        # Task 1: Construct matrix A(gamma) and vector b(gamma)\n        # Use high precision (float64) for the \"true\" A and b\n        sigma1 = 1.0\n        sigma2 = 10.0**(-gamma)\n        Sigma = np.diag([sigma1, sigma2]).astype(np.float64)\n        A = U @ Sigma\n        b = A @ x_star\n\n        kappa_A = 10.0**gamma\n\n        # Task 2: Use the derived predictor inequality\n        # Predict acceptability for single precision (32-bit)\n        pred_single_ok = (kappa_A**2 * eps_32) = tau\n        # Predict acceptability for double precision (64-bit)\n        pred_double_ok = (kappa_A**2 * eps_64) = tau\n\n        predicted_category = get_category(pred_single_ok, pred_double_ok)\n\n        # Task 3: Solve normal equations and measure errors\n        # Solve for single precision (float32)\n        A_32 = A.astype(np.float32)\n        b_32 = b.astype(np.float32)\n        AtA_32 = A_32.T @ A_32\n        Atb_32 = A_32.T @ b_32\n        try:\n            x_32 = np.linalg.solve(AtA_32, Atb_32)\n            err_32 = np.linalg.norm(x_32 - x_star) / np.linalg.norm(x_star)\n            meas_single_ok = err_32 = tau\n        except np.linalg.LinAlgError:\n            meas_single_ok = False\n\n\n        # Solve for double precision (float64)\n        A_64 = A.astype(np.float64)\n        b_64 = b.astype(np.float64)\n        AtA_64 = A_64.T @ A_64\n        Atb_64 = A_64.T @ b_64\n        try:\n            x_64 = np.linalg.solve(AtA_64, Atb_64)\n            err_64 = np.linalg.norm(x_64 - x_star) / np.linalg.norm(x_star)\n            meas_double_ok = err_64 = tau\n        except np.linalg.LinAlgError:\n            meas_double_ok = False\n\n\n        measured_category = get_category(meas_single_ok, meas_double_ok)\n\n        results.append([predicted_category, measured_category])\n\n    # Task 5: Final output format\n    # The format \"[p,m]\" must be produced without spaces.\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}