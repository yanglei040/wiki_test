{
    "hands_on_practices": [
        {
            "introduction": "The stability of a least-squares solution is intimately tied to the geometry of the problem's matrix. This exercise provides a clear, analytical bridge between the geometric concept of nearly parallel vectors and the algebraic consequence of an ill-conditioned normal equations matrix. By explicitly calculating the condition number for a matrix whose columns are separated by a small angle $\\theta$, you will directly observe how near-linear dependence leads to a condition number that scales quadratically with the inverse of this angle, revealing the source of the infamous instability associated with the normal equations .",
            "id": "2162074",
            "problem": "In numerical linear algebra, the conditioning of a matrix is a critical measure of its sensitivity to errors in input data. For a linear least-squares problem $A\\mathbf{x} \\approx \\mathbf{b}$, the sensitivity is governed by the condition number of the normal equations matrix, $A^TA$. Poor conditioning often arises when the columns of the matrix $A$ are nearly linearly dependent.\n\nConsider a simple $2 \\times 2$ matrix $A$ whose columns represent two basis vectors in a plane. The first column is the vector $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and the second column is the vector $\\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix}$, where $\\theta$, in radians, is the angle between the two vectors. Assume $\\theta$ is a small positive angle. The matrix $A$ is thus given by $$ A = \\begin{pmatrix} 1  \\cos\\theta \\\\ 0  \\sin\\theta \\end{pmatrix} $$.\n\nDetermine the leading-order asymptotic expression for the 2-norm condition number of the associated normal equations matrix, $A^TA$, in the limit as $\\theta \\to 0^+$. Your answer should be a simple expression in terms of $\\theta$ that captures the dominant behavior for small angles.",
            "solution": "We are asked for the 2-norm condition number of the normal equations matrix $A^{T}A$ for $A=\\begin{pmatrix} 1  \\cos\\theta \\\\ 0  \\sin\\theta \\end{pmatrix}$, in the limit $\\theta \\to 0^{+}$. For a symmetric positive definite matrix $M$, the 2-norm condition number is $\\kappa_{2}(M)=\\frac{\\lambda_{\\max}(M)}{\\lambda_{\\min}(M)}$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues of $M$.\n\nCompute $A^{T}A$:\n$$\nA^{T}=\\begin{pmatrix} 1  0 \\\\ \\cos\\theta  \\sin\\theta \\end{pmatrix}, \\quad\nA^{T}A=\\begin{pmatrix} 1  \\cos\\theta \\\\ \\cos\\theta  1 \\end{pmatrix}.\n$$\nThe matrix $\\begin{pmatrix} 1  \\cos\\theta \\\\ \\cos\\theta  1 \\end{pmatrix}$ has eigenvalues\n$$\n\\lambda_{\\pm}=1 \\pm \\cos\\theta.\n$$\nTherefore,\n$$\n\\kappa_{2}(A^{T}A)=\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}=\\frac{1+\\cos\\theta}{1-\\cos\\theta}.\n$$\nFor small $\\theta$, use the Taylor expansion $\\cos\\theta=1-\\frac{\\theta^{2}}{2}+O(\\theta^{4})$. Hence\n$$\n1-\\cos\\theta=\\frac{\\theta^{2}}{2}+O(\\theta^{4}), \\quad 1+\\cos\\theta=2-\\frac{\\theta^{2}}{2}+O(\\theta^{4}).\n$$\nThus\n$$\n\\kappa_{2}(A^{T}A)=\\frac{2-\\frac{\\theta^{2}}{2}+O(\\theta^{4})}{\\frac{\\theta^{2}}{2}+O(\\theta^{4})}\n=\\frac{4}{\\theta^{2}}+O(1),\n$$\nso the leading-order asymptotic behavior as $\\theta \\to 0^{+}$ is\n$$\n\\kappa_{2}(A^{T}A)\\sim \\frac{4}{\\theta^{2}}.\n$$",
            "answer": "$$\\boxed{\\frac{4}{\\theta^{2}}}$$"
        },
        {
            "introduction": "While the condition number of $A^TA$ predicts sensitivity to perturbations, another danger lurks in its very formation. This practice problem constructs a scenario where catastrophic cancellation occurs during the calculation of the inner products for $A^TA$, leading to a large relative error in the computed matrix itself. By quantifying the resulting error in the final solution, you will gain insight into why forming the normal equations is numerically hazardous, even beyond the issue of condition number squaring .",
            "id": "3540706",
            "problem": "Consider a real $2 \\times 2$ matrix $A$ whose columns are $c_{1}$ and $c_{2}$ defined by $c_{1} = \\begin{pmatrix} M \\\\ M \\end{pmatrix}$ and $c_{2} = \\begin{pmatrix} M \\\\ -M + \\delta \\end{pmatrix}$, where $M  0$ and $\\delta  0$ satisfy $0  \\delta \\ll M$. Let $S = A^{T}A$ be the Gram matrix. Assume the inner products forming $S$ are computed in a standard floating-point arithmetic obeying the model $\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\varepsilon)$ with $|\\varepsilon| \\leq u$ for each primitive operation $\\circ \\in \\{+, -, \\times\\}$, and that the unit roundoff $u$ is such that $\\delta  Mu$. Under this hypothesis, the off-diagonal entry $s_{12}$ of $S$ suffers catastrophic cancellation when formed naively via inner products, while the diagonal entries $s_{11}$ and $s_{22}$ incur negligible relative error compared to $s_{12}$.\n\nNow consider solving the least squares problem $\\min_{x \\in \\mathbb{R}^{2}} \\|Ax - b\\|_{2}$ using the normal equations $Sx = A^{T}b$ for the specific right-hand side $b = c_{2}$. To isolate the impact of the off-diagonal error, suppose that $A^{T}b$ is formed in exact arithmetic and that the diagonals $s_{11}$ and $s_{22}$ of $S$ are exact, but the computed off-diagonal entry is $\\widetilde{s}_{12} = 0$ due to cancellation.\n\nStarting only from the definitions of the Gram matrix and the normal equations, and the floating-point error model stated above, determine the relative forward error in the solution,\n$$\n\\frac{\\|x_{\\text{comp}} - x_{\\star}\\|_{2}}{\\|x_{\\star}\\|_{2}},\n$$\nwhere $x_{\\star}$ is the exact least squares solution and $x_{\\text{comp}}$ is the solution obtained by solving the perturbed normal equations with the off-diagonal set to $\\widetilde{s}_{12} = 0$. Express your final answer as a closed-form analytic expression in terms of $M$ and $\\delta$ only.",
            "solution": "We begin by computing the entries of the Gram matrix $S = A^{T}A$ from the definitions. The columns are $c_{1} = \\begin{pmatrix} M \\\\ M \\end{pmatrix}$ and $c_{2} = \\begin{pmatrix} M \\\\ -M + \\delta \\end{pmatrix}$. The Gram entries are the inner products $s_{ij} = c_{i}^{T} c_{j}$:\n$$\ns_{11} = c_{1}^{T} c_{1} = M^{2} + M^{2} = 2M^{2},\n$$\n$$\ns_{22} = c_{2}^{T} c_{2} = M^{2} + (-M + \\delta)^{2} = M^{2} + (M^{2} - 2M\\delta + \\delta^{2}) = 2M^{2} - 2M\\delta + \\delta^{2},\n$$\n$$\ns_{12} = c_{1}^{T} c_{2} = M \\cdot M + M \\cdot (-M + \\delta) = M^{2} - M^{2} + M\\delta = M\\delta.\n$$\n\nThus, $s_{12}$ is small relative to $s_{11}$ and $s_{22}$ because it arises from the cancellation of $M^{2}$ terms with opposite signs, leaving a residual of scale $M\\delta$. Under the floating-point hypothesis with $\\delta  Mu$, the naive formation of $s_{12}$ by computing the two products $M \\cdot M$ and $M \\cdot (-M + \\delta)$ and summing them yields\n$$\n\\operatorname{fl}(M) \\approx M,\\quad \\operatorname{fl}(-M + \\delta) \\approx -M,\n$$\nso that\n$$\n\\operatorname{fl}(s_{12}) = \\operatorname{fl}(M \\cdot M + M \\cdot (-M + \\delta)) \\approx \\operatorname{fl}(M^{2} + M \\cdot (-M)) = \\operatorname{fl}(M^{2} - M^{2}) \\approx 0.\n$$\nTherefore, the computed off-diagonal $\\widetilde{s}_{12} = 0$, while $s_{11}$ and $s_{22}$, dominated by sums of squares, have negligible relative error under the same arithmetic model. The relative error in the off-diagonal is\n$$\n\\frac{| \\widetilde{s}_{12} - s_{12} |}{| s_{12} |} = \\frac{|0 - M\\delta|}{|M\\delta|} = 1,\n$$\nwhich is large compared to typical relative errors in well-scaled quantities.\n\nWe now quantify the impact on the normal-equations solution for the least squares problem with $b = c_{2}$. First, compute $y = A^{T} b$ exactly:\n$$\ny = \\begin{pmatrix} c_{1}^{T} b \\\\ c_{2}^{T} b \\end{pmatrix} = \\begin{pmatrix} c_{1}^{T} c_{2} \\\\ c_{2}^{T} c_{2} \\end{pmatrix} = \\begin{pmatrix} s_{12} \\\\ s_{22} \\end{pmatrix} = \\begin{pmatrix} M\\delta \\\\ 2M^{2} - 2M\\delta + \\delta^{2} \\end{pmatrix}.\n$$\n\nThe exact normal equations are $S x = y$ with\n$$\nS = \\begin{pmatrix} s_{11}  s_{12} \\\\ s_{12}  s_{22} \\end{pmatrix} = \\begin{pmatrix} 2M^{2}  M\\delta \\\\ M\\delta  2M^{2} - 2M\\delta + \\delta^{2} \\end{pmatrix}.\n$$\nObserve that $b = c_{2}$ lies in the column space of $A$; because $A$ is $2 \\times 2$ and nonsingular for $M > 0$, the least squares solution coincides with the exact solution to $Ax = b$, namely $x_{\\star} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. This also follows directly from solving the exact normal equations: since $y = \\begin{pmatrix} s_{12} \\\\ s_{22} \\end{pmatrix}$ and $S$ is the Gram matrix, the solution $x_{\\star}$ that represents $b$ as a linear combination of the columns with coefficients $(0,1)$ satisfies $S x_{\\star} = y$.\n\nNext, consider the perturbed normal equations with the off-diagonal entry set to zero while keeping the diagonals exact:\n$$\n\\widetilde{S} = \\begin{pmatrix} s_{11}  0 \\\\ 0  s_{22} \\end{pmatrix} = \\begin{pmatrix} 2M^{2}  0 \\\\ 0  2M^{2} - 2M\\delta + \\delta^{2} \\end{pmatrix}.\n$$\nWe solve $\\widetilde{S} x_{\\text{comp}} = y$ with the exact right-hand side $y$. Because $\\widetilde{S}$ is diagonal, the solution components decouple:\n$$\nx_{\\text{comp},1} = \\frac{y_{1}}{s_{11}} = \\frac{s_{12}}{s_{11}} = \\frac{M\\delta}{2M^{2}} = \\frac{\\delta}{2M}, \\quad x_{\\text{comp},2} = \\frac{y_{2}}{s_{22}} = \\frac{s_{22}}{s_{22}} = 1.\n$$\nThus,\n$$\nx_{\\text{comp}} = \\begin{pmatrix} \\frac{\\delta}{2M} \\\\ 1 \\end{pmatrix}, \\quad x_{\\star} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\n\nThe forward error is\n$$\n\\|x_{\\text{comp}} - x_{\\star}\\|_{2} = \\left\\| \\begin{pmatrix} \\frac{\\delta}{2M} \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\frac{\\delta}{2M}.\n$$\nThe exact solution has norm\n$$\n\\|x_{\\star}\\|_{2} = \\left\\| \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right\\|_{2} = 1.\n$$\nTherefore, the relative forward error is\n$$\n\\frac{\\|x_{\\text{comp}} - x_{\\star}\\|_{2}}{\\|x_{\\star}\\|_{2}} = \\frac{\\delta}{2M}.\n$$\n\nThis closed-form expression directly quantifies the impact of the large relative error in the off-diagonal entry incurred when forming $A^{T}A$ under cancellation: it injects a spurious component of magnitude $\\delta/(2M)$ into the solution, even though the true coefficient along $c_{1}$ is zero. As $M$ grows for fixed $\\delta$, the off-diagonal $s_{12} = M\\delta$ becomes increasingly vulnerable to cancellation in floating-point arithmetic, while the induced relative forward error $\\delta/(2M)$ becomes smaller; however, for fixed unit roundoff $u$ and the constraint $\\delta  Mu$, one can choose $\\delta$ proportional to $Mu$ to maintain a prescribed error level, illustrating the sensitivity of normal equations to inner-product formation under cancellation.",
            "answer": "$$\\boxed{\\frac{\\delta}{2M}}$$"
        },
        {
            "introduction": "Theoretical understanding of conditioning becomes most powerful when it can predict the success or failure of a computation in practice. This problem challenges you to derive a predictor inequality that determines when solving the normal equations is acceptable for a given floating-point precision . You will then implement a numerical experiment to test this prediction, observing firsthand the \"phase transition\" where single precision fails but double precision succeeds, thereby solidifying your understanding of the interplay between a problem's condition number, machine precision, and the attainable accuracy of a solution.",
            "id": "3540736",
            "problem": "Consider the overdetermined linear least-squares problem in pure mathematical terms. Let $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$ and full column rank, and let $b \\in \\mathbb{R}^m$. The least-squares solution minimizes $\\|Ax - b\\|_2$ over $x \\in \\mathbb{R}^n$. A classical algorithm solves the normal equations $A^\\top A x = A^\\top b$. For floating-point arithmetic with unit roundoff (machine epsilon) $\\epsilon_{\\text{mach}}$, solving the normal equations is known to suffer amplified conditioning relative to solving the least-squares problem by more stable methods. You will construct a controlled family of matrices to study when the normal equations are acceptable in single precision and when they are acceptable in double precision, and you will connect this to a predictor inequality that separates a numerically stable regime from an unstable regime.\n\nStart from the following fundamental base:\n- The $2$-norm condition number of a full column rank matrix is defined by $\\kappa_2(A) = \\sigma_{\\max}(A)/\\sigma_{\\min}(A)$, where $\\sigma_{\\max}(A)$ and $\\sigma_{\\min}(A)$ are the largest and smallest singular values of $A$.\n- The normal equations matrix is $A^\\top A$, which is symmetric positive definite with $2$-norm condition number $\\kappa_2(A^\\top A) = \\kappa_2(A)^2$.\n- The first-order model of rounding in floating-point arithmetic with unit roundoff $\\epsilon_{\\text{mach}}$ states that each basic arithmetic operation incurs a relative error bounded (to first order) by a constant multiple of $\\epsilon_{\\text{mach}}$, and solving a well-conditioned linear system by a backward-stable method exhibits forward error proportional to the condition number times $\\epsilon_{\\text{mach}}$ in the corresponding norm.\n\nYour tasks are:\n1. Construct a parametric family of matrices $A(\\gamma) \\in \\mathbb{R}^{m \\times 2}$ with prescribed singular values $\\sigma_1 = 1$ and $\\sigma_2 = 10^{-\\gamma}$ for a parameter $\\gamma \\ge 0$. Ensure $m \\ge 2$ and full column rank. Use an orthonormal basis $U \\in \\mathbb{R}^{m \\times 2}$ to form $A(\\gamma) = U \\operatorname{diag}(\\sigma_1, \\sigma_2)$ so that $\\kappa_2(A(\\gamma)) = 10^{\\gamma}$. Choose a fixed nonzero vector $x_\\star \\in \\mathbb{R}^2$ and set $b(\\gamma) = A(\\gamma) x_\\star$.\n2. Using the floating-point model and the conditioning facts above, derive from first principles a predictor inequality that decides, based only on $\\kappa_2(A)$, $\\epsilon_{\\text{mach}}$, and a tolerance $\\tau  0$, whether solving the normal equations should be considered numerically acceptable in a given precision. Your derivation must start with the sensitivity of solving linear systems and the relationship $\\kappa_2(A^\\top A) = \\kappa_2(A)^2$. Define “acceptable” precisely as producing a solution $x$ whose relative error satisfies $\\|x - x_\\star\\|_2 / \\|x_\\star\\|_2 \\le \\tau$.\n3. Implement a program that, for each test case described below, does the following:\n   - Constructs $A(\\gamma)$ and $b(\\gamma)$ deterministically.\n   - Solves the normal equations in single precision ($\\epsilon_{\\text{mach}}$ for $32$-bit floating point) and in double precision ($\\epsilon_{\\text{mach}}$ for $64$-bit floating point) to obtain $x^{(32)}(\\gamma)$ and $x^{(64)}(\\gamma)$, respectively. The construction of $A(\\gamma)$ and $b(\\gamma)$ may be carried out in higher precision, but the formation of $A^\\top A$, $A^\\top b$, and the solve must be performed in the target precision.\n   - Computes the measured relative errors $e^{(p)}(\\gamma) = \\|x^{(p)}(\\gamma) - x_\\star\\|_2/\\|x_\\star\\|_2$ for $p \\in \\{32,64\\}$.\n   - Computes a predicted stability classification in each precision using your derived inequality, which must be expressed in terms of $\\kappa_2(A(\\gamma))$, $\\epsilon_{\\text{mach}}$ for the precision, and $\\tau$.\n   - Encodes a “phase” category as an integer in $\\{0,1,2,3\\}$:\n     - $0$ if both single and double are acceptable,\n     - $1$ if single is unacceptable and double is acceptable,\n     - $2$ if single is acceptable and double is unacceptable,\n     - $3$ if both are unacceptable.\n     Produce this category twice: once from the predictor inequality (“predicted category”) and once from the measured errors (“measured category”).\n4. The test suite must use the following fixed parameters:\n   - $m = 200$, $n = 2$,\n   - $x_\\star = [1,-1]^\\top$,\n   - tolerance $\\tau = 10^{-3}$,\n   - parameter values $\\gamma \\in \\{0.5,\\, 2.2,\\, 6.0,\\, 6.3,\\, 6.4,\\, 8.0\\}$,\n   - a fixed random seed for constructing $U$ so that results are deterministic.\n5. Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one $\\gamma$ in the order given and is itself a two-element list of integers $[\\text{predicted\\_category},\\text{measured\\_category}]$. For example, an output with two test cases could look like: \"[[0,0],[1,1]]\". No spaces are permitted anywhere in the line.\n\nThere are no physical units in this problem. All angles, if any, are absent. All results are dimensionless. Each element of the final list must be an integer. Your code must be fully self-contained and produce the exact final output format.",
            "solution": "The problem is assessed to be valid. It is a well-posed, scientifically grounded problem in numerical linear algebra that is self-contained and free of contradictions or ambiguities. It requests a standard theoretical derivation followed by a numerical experiment to illustrate a fundamental concept: the ill-conditioning of the normal equations for least-squares problems.\n\n### 1. Derivation of the Predictor Inequality\n\nThe goal is to derive an inequality that predicts when the numerical solution to the normal equations is \"acceptable,\" defined by the relative error of the computed solution $x$ being bounded by a tolerance $\\tau$.\n$$ \\frac{\\|x - x_\\star\\|_2}{\\|x_\\star\\|_2} \\le \\tau $$\nwhere $x_\\star$ is the exact solution.\n\nThe normal equations are given by $A^\\top A x = A^\\top b$. We can express this as a standard linear system $Mx = y$, where the matrix is $M = A^\\top A$ and the right-hand side is $y = A^\\top b$. The problem states that for the constructed test cases, $b = A x_\\star$, which implies that $x_\\star$ is the exact solution to the least-squares problem and also to the normal equations, as $A^\\top A x_\\star = A^\\top (A x_\\star) = A^\\top b$.\n\nThe core of the analysis lies in the forward error of solving the linear system $Mx = y$ in floating-point arithmetic. Standard-error analysis for a backward-stable linear system solver (such as one based on Cholesky factorization, suitable for the symmetric positive definite matrix $M = A^\\top A$) provides the following bound on the relative error of the computed solution $\\hat{x}$:\n$$ \\frac{\\|\\hat{x} - x_\\star\\|_2}{\\|x_\\star\\|_2} \\le c \\cdot \\kappa_2(M) \\cdot \\epsilon_{\\text{mach}} $$\nHere, $\\epsilon_{\\text{mach}}$ is the machine epsilon (unit roundoff) of the floating-point precision used, $\\kappa_2(M)$ is the $2$-norm condition number of the matrix $M$, and $c$ is a constant of order $1$ that depends on the dimensions of the matrix and the details of the algorithm. This bound incorporates the errors from both forming the system $(M, y)$ and solving it.\n\nThe problem provides the crucial relationship between the condition number of the original matrix $A$ and the normal equations matrix $M=A^\\top A$:\n$$ \\kappa_2(A^\\top A) = \\kappa_2(A)^2 $$\nSubstituting this into the error bound, we get:\n$$ \\frac{\\|\\hat{x} - x_\\star\\|_2}{\\|x_\\star\\|_2} \\le c \\cdot \\kappa_2(A)^2 \\cdot \\epsilon_{\\text{mach}} $$\nTo create a simple, first-order predictor as requested, we can model the behavior by taking the constant $c=1$. This gives a direct estimate for the relative error:\n$$ e_{\\text{pred}} \\approx \\kappa_2(A)^2 \\cdot \\epsilon_{\\text{mach}} $$\nThe solution is deemed \"acceptable\" if the measured error is no more than $\\tau$. We can therefore predict acceptability by checking if our estimated error bound is within this tolerance:\n$$ \\kappa_2(A)^2 \\cdot \\epsilon_{\\text{mach}} \\le \\tau $$\nThis is the predictor inequality. For the specific parametric family of matrices $A(\\gamma)$ with $\\kappa_2(A(\\gamma)) = 10^{\\gamma}$, the inequality becomes:\n$$ (10^\\gamma)^2 \\cdot \\epsilon_{\\text{mach}} \\le \\tau \\quad \\implies \\quad 10^{2\\gamma} \\cdot \\epsilon_{\\text{mach}} \\le \\tau $$\nThis inequality will be used to classify the predicted stability for single precision ($\\epsilon_{\\text{mach}} \\approx 1.19 \\times 10^{-7}$) and double precision ($\\epsilon_{\\text{mach}} \\approx 2.22 \\times 10^{-16}$).\n\n### 2. Numerical Experiment Design\n\nThe implementation will systematically test this prediction. For each parameter $\\gamma$ in the test suite $\\{0.5, 2.2, 6.0, 6.3, 6.4, 8.0\\}$:\n\n1.  **Matrix Construction**: A fixed random seed is used to generate a matrix $R \\in \\mathbb{R}^{200 \\times 2}$. The QR decomposition of $R$ yields a matrix $U \\in \\mathbb{R}^{200 \\times 2}$ with orthonormal columns. This $U$ is fixed for all test cases. The matrix $A(\\gamma)$ is then constructed as $A(\\gamma) = U \\Sigma(\\gamma)$, where $\\Sigma(\\gamma) = \\operatorname{diag}(1, 10^{-\\gamma})$. This construction ensures that the singular values of $A(\\gamma)$ are indeed $1$ and $10^{-\\gamma}$, and thus $\\kappa_2(A(\\gamma)) = 10^\\gamma$. The vector $b(\\gamma)$ is set to $A(\\gamma)x_\\star$, where $x_\\star = [1, -1]^\\top$. This ensures the exact solution is known and the least-squares problem has zero residual.\n\n2.  **Solving and Error Measurement**: For each precision (32-bit and 64-bit), the following steps are performed:\n    a. The matrices $A(\\gamma)$ and $b(\\gamma)$ are converted to the target precision.\n    b. The normal equations matrix $A^\\top A$ and vector $A^\\top b$ are formed using arithmetic in that target precision.\n    c. The linear system $(A^\\top A)x = (A^\\top b)$ is solved to find the computed solution $x^{(p)}(\\gamma)$ for precision $p \\in \\{32, 64\\}$.\n    d. The measured relative error $e^{(p)}(\\gamma) = \\|x^{(p)}(\\gamma) - x_\\star\\|_2 / \\|x_\\star\\|_2$ is computed.\n\n3.  **Categorization**:\n    -   **Predicted Category**: For each precision $p$, the inequality $10^{2\\gamma} \\cdot \\epsilon_{\\text{mach}}^{(p)} \\le \\tau$ is evaluated. A precision is deemed acceptable if the inequality holds.\n    -   **Measured Category**: For each precision $p$, the measured error $e^{(p)}(\\gamma)$ is compared against the tolerance $\\tau$. The solution is deemed acceptable if $e^{(p)}(\\gamma) \\le \\tau$.\n    -   Based on these acceptability checks for single and double precision, a category from $\\{0, 1, 2, 3\\}$ is assigned for both the prediction and the measurement, according to the rules specified in the problem statement. Category $2$ (single acceptable, double unacceptable) is not expected to occur.\n\nThis procedure generates a pair of categories, $[\\text{predicted\\_category}, \\text{measured\\_category}]$, for each value of $\\gamma$, which are then formatted into the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical analysis problem concerning the conditioning of normal equations.\n    \"\"\"\n    # Task 4: Fixed parameters\n    m = 200\n    n = 2\n    x_star = np.array([1.0, -1.0], dtype=np.float64)\n    tau = 1e-3\n    gamma_values = [0.5, 2.2, 6.0, 6.3, 6.4, 8.0]\n    random_seed = 42\n\n    # Machine epsilon for single and double precision\n    eps_32 = np.finfo(np.float32).eps\n    eps_64 = np.finfo(np.float64).eps\n\n    # Generate a fixed orthonormal basis U\n    np.random.seed(random_seed)\n    # Generate a random m x n matrix\n    R = np.random.randn(m, n)\n    # Use QR decomposition to get an orthonormal basis for its column space\n    U, _ = np.linalg.qr(R)\n    U = U.astype(np.float64)\n\n    results = []\n\n    # Helper function for categorization\n    def get_category(single_ok, double_ok):\n        if single_ok and double_ok:\n            return 0  # Both acceptable\n        elif not single_ok and double_ok:\n            return 1  # Single unacceptable, double acceptable\n        elif single_ok and not double_ok:\n            return 2  # Single acceptable, double unacceptable (not expected)\n        else: # not single_ok and not double_ok\n            return 3  # Both unacceptable\n\n    for gamma in gamma_values:\n        # Task 1: Construct matrix A(gamma) and vector b(gamma)\n        # Use high precision (float64) for the \"true\" A and b\n        sigma1 = 1.0\n        sigma2 = 10.0**(-gamma)\n        Sigma = np.diag([sigma1, sigma2]).astype(np.float64)\n        A = U @ Sigma\n        b = A @ x_star\n\n        kappa_A = 10.0**gamma\n\n        # Task 2: Use the derived predictor inequality\n        # Predict acceptability for single precision (32-bit)\n        pred_single_ok = (kappa_A**2 * eps_32) = tau\n        # Predict acceptability for double precision (64-bit)\n        pred_double_ok = (kappa_A**2 * eps_64) = tau\n\n        predicted_category = get_category(pred_single_ok, pred_double_ok)\n\n        # Task 3: Solve normal equations and measure errors\n        # Solve for single precision (float32)\n        A_32 = A.astype(np.float32)\n        b_32 = b.astype(np.float32)\n        AtA_32 = A_32.T @ A_32\n        Atb_32 = A_32.T @ b_32\n        x_32 = np.linalg.solve(AtA_32, Atb_32)\n        err_32 = np.linalg.norm(x_32 - x_star) / np.linalg.norm(x_star)\n        meas_single_ok = err_32 = tau\n\n        # Solve for double precision (float64)\n        A_64 = A.astype(np.float64)\n        b_64 = b.astype(np.float64)\n        AtA_64 = A_64.T @ A_64\n        Atb_64 = A_64.T @ b_64\n        x_64 = np.linalg.solve(AtA_64, Atb_64)\n        err_64 = np.linalg.norm(x_64 - x_star) / np.linalg.norm(x_star)\n        meas_double_ok = err_64 = tau\n\n        measured_category = get_category(meas_single_ok, meas_double_ok)\n\n        results.append([predicted_category, measured_category])\n\n    # Task 5: Final output format\n    # The format \"[p,m]\" must be produced without spaces.\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}