{
    "hands_on_practices": [
        {
            "introduction": "The first step in understanding eigenvalue sensitivity is learning how to quantify it. This practice guides you through creating a fundamental computational tool to estimate the absolute condition number, $\\kappa_{\\text{abs}}(\\lambda)$, for any simple eigenvalue . By implementing a procedure to find both the right and left eigenvectors, you will build a robust method to measure the potential for error amplification in practical eigenvalue problems.",
            "id": "3576442",
            "problem": "Consider a square complex matrix $A \\in \\mathbb{C}^{n \\times n}$ with a simple eigenpair $(\\lambda, x)$, where $\\lambda \\in \\mathbb{C}$ and $x \\in \\mathbb{C}^{n} \\setminus \\{0\\}$ satisfy $A x = \\lambda x$. A vector $y \\in \\mathbb{C}^{n} \\setminus \\{0\\}$ is called a left eigenvector corresponding to $\\lambda$ if $y^{*} A = \\lambda y^{*}$, where $y^{*}$ denotes the Hermitian (conjugate) transpose of $y$. Starting only from the definitions of an eigenpair, a left eigenvector, and first-order perturbations of linear operators, derive a bound on the leading-order change in the eigenvalue $\\lambda$ under a small perturbation of $A$ measured in the spectral $2$-norm, and use this to define a rigorous estimator for the absolute eigenvalue condition number in the $2$-norm for a simple eigenvalue.\n\nDesign and implement a procedure that, given a matrix $A$ and a target scalar $\\lambda_{\\text{target}} \\in \\mathbb{C}$ identifying the desired eigenvalue, performs the following steps:\n\n- Compute the right eigenvalues and right eigenvectors of $A$, and select the right eigenvector $x$ associated with the eigenvalue $\\lambda$ that is closest to $\\lambda_{\\text{target}}$ in magnitude of difference.\n- Compute the left eigenvector $y$ corresponding to the same eigenvalue, using only the definition $y^{*} A = \\lambda y^{*}$ and properties of the Hermitian transpose.\n- Using your derivation, form a numerically stable estimator for the absolute eigenvalue condition number in the $2$-norm, denoted $\\kappa_{\\text{abs}}(\\lambda)$, for the selected simple eigenvalue.\n\nYour program must implement this procedure and produce the estimator $\\kappa_{\\text{abs}}(\\lambda)$ for each test case below. No physical units are involved. All angles, if any arise, are to be considered in radians, though no angle quantities are required in this task.\n\nTest suite:\n\n1. A nonnormal upper-triangular real matrix testing a straightforward simple eigenvalue selection: $A_{1} = \\begin{bmatrix} 1 & 4 \\\\ 0 & 2 \\end{bmatrix}$ with $\\lambda_{\\text{target},1} = 2$.\n2. A nearly defective (but still diagonalizable) real upper-triangular matrix with closely spaced eigenvalues, testing sensitivity amplification: $A_{2} = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 + 10^{-6} \\end{bmatrix}$ with $\\lambda_{\\text{target},2} = 1 + 10^{-6}$.\n3. A real Hermitian diagonal matrix, testing the normal case where left and right eigenvectors align: $A_{3} = \\begin{bmatrix} -2 & 0 \\\\ 0 & 3 \\end{bmatrix}$ with $\\lambda_{\\text{target},3} = 3$.\n4. A real orthogonal rotation matrix with complex eigenvalues, testing complex eigenpair handling: $A_{4} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$ with $\\lambda_{\\text{target},4} = \\mathrm{i}$.\n\nAll eigenvalues in the test suite are simple. Your program should produce a single line of output containing the four estimated condition numbers $\\kappa_{\\text{abs}}(\\lambda)$, in the order of the test cases $(1)$ through $(4)$, rounded to $8$ decimal places, as a comma-separated list enclosed in square brackets (e.g., $[r_{1},r_{2},r_{3},r_{4}]$). Each $r_{k}$ must be a real-valued float.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It presents a standard task in numerical linear algebra—the derivation and computation of an eigenvalue condition number—with a clear set of definitions, constraints, and test cases. The problem is self-contained and free of contradictions or ambiguities.\n\nHerein, we derive the estimator for the absolute eigenvalue condition number from first principles and then outline the computational procedure.\n\n### Derivation of the Eigenvalue Condition Number\n\nLet $A \\in \\mathbb{C}^{n \\times n}$ be a square matrix. Let $(\\lambda, x)$ be a simple eigenpair of $A$, meaning $\\lambda \\in \\mathbb{C}$ is a simple eigenvalue and $x \\in \\mathbb{C}^{n} \\setminus \\{0\\}$ is its corresponding right eigenvector. This is defined by the equation:\n$$A x = \\lambda x$$\nLet $y \\in \\mathbb{C}^{n} \\setminus \\{0\\}$ be the corresponding left eigenvector, defined by the equation:\n$$y^{*} A = \\lambda y^{*}$$\nwhere $y^{*}$ is the Hermitian transpose of $y$.\n\nConsider a small perturbation $\\delta A$ of the matrix $A$. The perturbed matrix is $A' = A + \\delta A$. We assume that the eigenpair $(\\lambda, x)$ is correspondingly perturbed to $(\\lambda', x') = (\\lambda + \\delta \\lambda, x + \\delta x)$. The new eigenpair satisfies the perturbed eigenvalue equation:\n$$(A + \\delta A)(x + \\delta x) = (\\lambda + \\delta \\lambda)(x + \\delta x)$$\nWe expand this equation:\n$$A x + A \\delta x + (\\delta A) x + (\\delta A) \\delta x = \\lambda x + \\lambda \\delta x + (\\delta \\lambda) x + (\\delta \\lambda) \\delta x$$\nUsing the original unperturbed equation $A x = \\lambda x$, we can cancel the first term on each side:\n$$A \\delta x + (\\delta A) x + (\\delta A) \\delta x = \\lambda \\delta x + (\\delta \\lambda) x + (\\delta \\lambda) \\delta x$$\nFor a first-order perturbation analysis, we assume the perturbations $\\delta A$, $\\delta x$, and $\\delta \\lambda$ are small. We can therefore neglect terms of second order and higher, such as $(\\delta A) \\delta x$ and $(\\delta \\lambda) \\delta x$. This yields the first-order approximation:\n$$A \\delta x + (\\delta A) x \\approx \\lambda \\delta x + (\\delta \\lambda) x$$\nRearranging the terms, we get:\n$$(\\delta A) x - (\\delta \\lambda) x \\approx (\\lambda I - A) \\delta x$$\nwhere $I$ is the $n \\times n$ identity matrix.\n\nTo isolate the term involving $\\delta \\lambda$, we pre-multiply the entire equation by the conjugate transpose of the left eigenvector, $y^{*}$:\n$$y^{*} ((\\delta A) x - (\\delta \\lambda) x) \\approx y^{*} (\\lambda I - A) \\delta x$$\n$$y^{*} (\\delta A) x - y^{*} (\\delta \\lambda) x \\approx (y^{*} \\lambda - y^{*} A) \\delta x$$\nFrom the definition of the left eigenvector, $y^{*} A = \\lambda y^{*}$, which implies $y^{*} A - \\lambda y^{*} = 0$. Consequently, the right-hand side of the approximation vanishes:\n$$(y^{*} \\lambda - y^{*} A) \\delta x = (\\lambda y^{*} - y^{*} A) \\delta x = (0) \\delta x = 0$$\nThe approximation simplifies to:\n$$y^{*} (\\delta A) x - y^{*} (\\delta \\lambda) x \\approx 0$$\nSince $\\delta \\lambda$ is a scalar, we can write $y^{*} (\\delta \\lambda) x = (\\delta \\lambda) y^{*} x$. This gives:\n$$y^{*} (\\delta A) x \\approx (\\delta \\lambda) y^{*} x$$\nBecause $\\lambda$ is a simple eigenvalue, its corresponding left and right eigenvectors are not orthogonal, i.e., $y^{*} x \\neq 0$. We can therefore solve for the first-order change in the eigenvalue, $\\delta \\lambda$:\n$$\\delta \\lambda \\approx \\frac{y^{*} (\\delta A) x}{y^{*} x}$$\nTo establish a bound, we take the absolute value of both sides:\n$$|\\delta \\lambda| \\approx \\left| \\frac{y^{*} (\\delta A) x}{y^{*} x} \\right| = \\frac{|y^{*} (\\delta A) x|}{|y^{*} x|}$$\nThe numerator can be bounded using the Cauchy-Schwarz inequality and the definition of the spectral $2$-norm, $\\|\\cdot\\|_2$. The expression $y^{*} ((\\delta A) x)$ is a scalar.\n$$|y^{*} ((\\delta A) x)| \\le \\|y\\|_2 \\|(\\delta A) x\\|_2$$\nFurther, by the definition of an induced matrix norm:\n$$\\|(\\delta A) x\\|_2 \\le \\|\\delta A\\|_2 \\|x\\|_2$$\nCombining these inequalities, we have:\n$$|y^{*} (\\delta A) x| \\le \\|y\\|_2 \\|\\delta A\\|_2 \\|x\\|_2$$\nSubstituting this bound back into the expression for $|\\delta \\lambda|$ yields the relationship for the leading-order change:\n$$|\\delta \\lambda| \\lesssim \\frac{\\|y\\|_2 \\|x\\|_2}{|y^{*} x|} \\|\\delta A\\|_2$$\nThe absolute eigenvalue condition number, $\\kappa_{\\text{abs}}(\\lambda)$, is the factor that relates the change in the eigenvalue to the perturbation in the matrix. It is defined as the supremum of the ratio $|\\delta \\lambda| / \\|\\delta A\\|_2$ as $\\|\\delta A\\|_2 \\to 0$. From our first-order analysis, this gives the estimator:\n$$\\kappa_{\\text{abs}}(\\lambda) = \\frac{\\|x\\|_2 \\|y\\|_2}{|y^{*} x|}$$\nThis expression is independent of the scaling of the eigenvectors $x$ and $y$, as any scaling factor in the numerator cancels with the corresponding factor in the denominator. If the eigenvectors are normalized such that $\\|x\\|_2 = 1$ and $\\|y\\|_2 = 1$, the formula simplifies to $\\kappa_{\\text{abs}}(\\lambda) = 1 / |y^{*} x|$. The quantity $y^{*} x$ can be related to the angle $\\theta$ between the eigenvectors, $|y^{*} x| = \\|y\\|_2 \\|x\\|_2 |\\cos(\\theta)|$, so $\\kappa_{\\text{abs}}(\\lambda) = 1/|\\cos(\\theta)|$. For a normal matrix ($A A^{*} = A^{*} A$), the left and right eigenvectors are parallel, so $|\\cos(\\theta)|=1$ and $\\kappa_{\\text{abs}}(\\lambda)=1$. For nonnormal matrices, $|\\cos(\\theta)|$ can be very small, leading to a large condition number.\n\n### Computational Procedure\n\nGiven a matrix $A$ and a target scalar $\\lambda_{\\text{target}}$, the procedure to compute $\\kappa_{\\text{abs}}(\\lambda)$ is as follows:\n\n1.  **Compute Right Eigenpairs**: Calculate the eigenvalues $w_j$ and corresponding right eigenvectors $v_j$ of the matrix $A$. This is done by solving the standard eigenvalue problem $A v = w v$.\n2.  **Select Target Eigenpair**: Identify the eigenvalue $\\lambda$ from the set $\\{w_j\\}$ that is closest to $\\lambda_{\\text{target}}$ in the complex plane, i.e., minimize $|\\lambda - \\lambda_{\\text{target}}|$. Let the corresponding right eigenvector be $x$.\n3.  **Compute Left Eigenvector**: The left eigenvector $y$ for the eigenvalue $\\lambda$ is defined by $y^{*} A = \\lambda y^{*}$. To compute $y$, we take the Hermitian transpose of this equation: $(y^{*} A)^{*} = (\\lambda y^{*})^{*}$. This yields $A^{*} (y^{*})^{*} = \\bar{\\lambda} (y^{*})^{*}$, which simplifies to $A^{*} y = \\bar{\\lambda} y$. Thus, the left eigenvector $y$ of $A$ for eigenvalue $\\lambda$ is the right eigenvector of the Hermitian transpose $A^{*}$ corresponding to the eigenvalue $\\bar{\\lambda}$. We compute the eigenpairs of $A^{*}$ and select the eigenvector corresponding to the eigenvalue closest to $\\bar{\\lambda}$.\n4.  **Calculate Condition Number**: Using the selected right eigenvector $x$ and the computed left eigenvector $y$, we calculate the condition number using the derived formula:\n    $$\\kappa_{\\text{abs}}(\\lambda) = \\frac{\\|x\\|_2 \\|y\\|_2}{|y^{*} x|}$$\n    Numerically, $\\|v\\|_2$ is the Euclidean or $L_2$ norm of a vector $v$, and $y^{*}x$ is the complex dot product, which is computed as $\\sum_{i=1}^{n} \\overline{y_i} x_i$.\n\nThis procedure is implemented for each of the given test cases to find the respective eigenvalue condition numbers.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_condition_number(A: np.ndarray, lambda_target: complex) -> float:\n    \"\"\"\n    Computes the absolute condition number for a simple eigenvalue of a matrix.\n\n    Args:\n        A (np.ndarray): A square complex matrix.\n        lambda_target (complex): A scalar value to identify the eigenvalue of interest.\n\n    Returns:\n        float: The estimated absolute eigenvalue condition number in the 2-norm.\n    \"\"\"\n    n = A.shape[0]\n    if A.shape != (n, n):\n        raise ValueError(\"Input matrix must be square.\")\n\n    # Step 1: Compute right eigenvalues and eigenvectors of A.\n    # w contains the eigenvalues, v contains the corresponding eigenvectors in columns.\n    w, v = np.linalg.eig(A)\n\n    # Step 2: Select the right eigenpair (lambda, x) closest to lambda_target.\n    idx = np.argmin(np.abs(w - lambda_target))\n    eigenvalue_lambda = w[idx]\n    right_eigenvector_x = v[:, idx]\n\n    # Step 3: Compute the left eigenvector y.\n    # The left eigenvector y of A for eigenvalue lambda is the right\n    # eigenvector of the Hermitian transpose A* for eigenvalue conj(lambda).\n    A_hermitian = A.conj().T\n    w_left, v_left = np.linalg.eig(A_hermitian)\n    \n    # Select the eigenvector of A* corresponding to the eigenvalue closest to conj(lambda).\n    idx_left = np.argmin(np.abs(w_left - np.conj(eigenvalue_lambda)))\n    left_eigenvector_y = v_left[:, idx_left]\n\n    # Step 4: Compute the absolute eigenvalue condition number.\n    # Formula: kappa_abs(lambda) = (||x||_2 * ||y||_2) / |y* x|\n    # where y* x is the dot product sum(conj(y_i) * x_i).\n    # np.linalg.norm computes the L2 norm by default.\n    norm_x = np.linalg.norm(right_eigenvector_x)\n    norm_y = np.linalg.norm(left_eigenvector_y)\n    \n    # np.vdot(a, b) computes a*.b, which is exactly y* x\n    y_star_x = np.vdot(left_eigenvector_y, right_eigenvector_x)\n    \n    # Handle the case where y_star_x is zero (defective eigenvalue), though\n    # the problem statement guarantees simple eigenvalues.\n    if abs(y_star_x) < 1e-15: # Using a tolerance for floating point comparison\n        return float('inf')\n\n    kappa = (norm_x * norm_y) / abs(y_star_x)\n    \n    return float(kappa)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([[1, 4], [0, 2]], dtype=float), 2.0),\n        (np.array([[1, 1], [0, 1 + 1e-6]], dtype=float), 1.0 + 1e-6),\n        (np.array([[-2, 0], [0, 3]], dtype=float), 3.0),\n        (np.array([[0, -1], [1, 0]], dtype=float), 1j),\n    ]\n\n    results = []\n    for A, lambda_target in test_cases:\n        kappa = compute_condition_number(A, lambda_target)\n        results.append(kappa)\n\n    # Format the output as a comma-separated list of floats rounded to 8 decimal places.\n    formatted_results = [f\"{r:.8f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "With a method to compute condition numbers, we can now investigate their practical implications. This exercise presents a classic non-normal matrix where a large condition number leads to dramatic error amplification . By analyzing this specific case, you will see firsthand how a numerically tiny residual—a small backward error—can result in a surprisingly large forward error in the computed eigenvalue, highlighting the risks of non-normality.",
            "id": "3576422",
            "problem": "Consider the family of upper triangular matrices $A(M) \\in \\mathbb{C}^{2 \\times 2}$ given by\n$$\nA(M) \\;=\\; \\begin{pmatrix} 1 & M \\\\ 0 & 2 \\end{pmatrix},\n$$\nwhere $M > 0$ is a real parameter. Let $\\lambda$ denote the simple eigenvalue $\\lambda = 1$, and let $v \\in \\mathbb{C}^{2}$ and $w \\in \\mathbb{C}^{2}$ be the associated right and left eigenvectors, respectively, for $\\lambda$. An algorithm returns a computed eigenpair $(\\widehat{\\lambda}, \\widehat{v})$ with $\\widehat{v}$ normalized to $\\|\\widehat{v}\\|_{2} = 1$ and a tiny eigenpair residual $r := A(M)\\widehat{v} - \\widehat{\\lambda}\\widehat{v}$ of norm $\\|r\\|_{2} = \\varepsilon$, where $0 < \\varepsilon \\ll 1$. By the definition of eigenpair backward error, the minimal perturbation $\\Delta A$ (in the spectral norm) satisfying $(A(M) + \\Delta A)\\widehat{v} = \\widehat{\\lambda}\\widehat{v}$ has norm equal to the backward error.\n\nStarting from the definitions of left and right eigenvectors and the first-order perturbation of simple eigenvalues, derive an explicit expression for the absolute eigenvalue condition number $\\kappa_{\\mathrm{abs}}(\\lambda)$ for the eigenvalue $\\lambda = 1$ of $A(M)$, and explain how a residual direction $r$ can yield an amplified forward eigenvalue error $|\\widehat{\\lambda} - \\lambda|$ despite a tiny backward error $\\|\\Delta A\\|_{2} = \\varepsilon$. Then, using your derivation, compute the worst-case forward eigenvalue error magnitude $|\\widehat{\\lambda} - \\lambda|$ when $M = 10^{6}$ and $\\varepsilon = 10^{-10}$. Round your final numerical answer to four significant figures.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and complete. It is a standard problem in numerical linear algebra concerning the sensitivity of eigenvalues of non-normal matrices. We may therefore proceed with a full solution.\n\nOur objective is threefold: first, to derive the absolute condition number for the eigenvalue $\\lambda = 1$ of the matrix $A(M)$; second, to explain the mechanism by which a small residual can lead to a large error in the computed eigenvalue; and third, to compute the worst-case forward error for specific values of $M$ and $\\varepsilon$.\n\nThe given matrix family is\n$$\nA(M) = \\begin{pmatrix} 1 & M \\\\ 0 & 2 \\end{pmatrix}\n$$\nwhere $M > 0$. Since $A(M)$ is an upper triangular matrix, its eigenvalues are its diagonal entries, $\\lambda_1 = 1$ and $\\lambda_2 = 2$. We are interested in the simple eigenvalue $\\lambda = 1$.\n\nFirst, we determine the right eigenvector $v$ and the left eigenvector $w$ associated with $\\lambda = 1$.\n\nThe right eigenvector $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$ is a non-zero vector satisfying the equation $A(M)v = \\lambda v$, which is equivalent to $(A(M) - \\lambda I)v = 0$. For $\\lambda = 1$, we have:\n$$\n(A(M) - 1 \\cdot I)v = \\begin{pmatrix} 1-1 & M \\\\ 0 & 2-1 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 & M \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis matrix equation yields the system of linear equations:\n$$\nM v_2 = 0\n$$\n$$\nv_2 = 0\n$$\nSince $M > 0$, both equations imply $v_2 = 0$. The first component, $v_1$, is unconstrained, so we can choose any non-zero value. A canonical choice is $v_1 = 1$. Thus, a right eigenvector for $\\lambda = 1$ is:\n$$\nv = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n\nThe left eigenvector $w = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$ is a non-zero vector satisfying $w^H A(M) = \\lambda w^H$, which is equivalent to $w^H (A(M) - \\lambda I) = 0$. For $\\lambda = 1$, and since $A(M)$ is real, we can use $w^T$:\n$$\nw^T (A(M) - 1 \\cdot I) = \\begin{pmatrix} w_1 & w_2 \\end{pmatrix} \\begin{pmatrix} 0 & M \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\end{pmatrix}\n$$\nThis gives the single equation $M w_1 + w_2 = 0$. We can choose $w_1 = 1$, which gives $w_2 = -M$. Thus, a left eigenvector for $\\lambda = 1$ is:\n$$\nw = \\begin{pmatrix} 1 \\\\ -M \\end{pmatrix}\n$$\n\nThe absolute eigenvalue condition number, $\\kappa_{\\mathrm{abs}}(\\lambda)$, for a simple eigenvalue $\\lambda$ is defined as:\n$$\n\\kappa_{\\mathrm{abs}}(\\lambda) = \\frac{\\|w\\|_2 \\|v\\|_2}{|w^H v|}\n$$\nWe compute the necessary components using our derived eigenvectors:\n$$\n\\|v\\|_2 = \\sqrt{1^2 + 0^2} = 1\n$$\n$$\n\\|w\\|_2 = \\sqrt{1^2 + (-M)^2} = \\sqrt{1 + M^2}\n$$\n$$\nw^H v = w^T v = \\begin{pmatrix} 1 & -M \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1 \\cdot 1 + (-M) \\cdot 0 = 1\n$$\nSubstituting these values into the formula for the condition number:\n$$\n\\kappa_{\\mathrm{abs}}(1) = \\frac{(\\sqrt{1+M^2})(1)}{|1|} = \\sqrt{1+M^2}\n$$\nThis is the explicit expression for the absolute condition number of the eigenvalue $\\lambda = 1$.\n\nNext, we explain how a small residual can lead to a large forward error. First-order perturbation theory states that for a small perturbation $\\Delta A$ to a matrix $A$, a simple eigenvalue $\\lambda$ is perturbed to $\\widehat{\\lambda}$ such that the forward error $|\\widehat{\\lambda} - \\lambda|$ is bounded by:\n$$\n|\\widehat{\\lambda} - \\lambda| \\lesssim \\kappa_{\\mathrm{abs}}(\\lambda) \\|\\Delta A\\|_2\n$$\nThe problem states that $(\\widehat{\\lambda}, \\widehat{v})$ is a computed eigenpair with residual $r = A(M)\\widehat{v} - \\widehat{\\lambda}\\widehat{v}$ of norm $\\|r\\|_2 = \\varepsilon$. The quantity $\\varepsilon$ is the norm of the minimal perturbation $\\Delta A$ required for $(\\widehat{\\lambda}, \\widehat{v})$ to be an exact eigenpair of $A(M) + \\Delta A$; this is the backward error. Thus, $\\|\\Delta A\\|_2 = \\varepsilon$.\n\nThe worst-case forward error is therefore approximately:\n$$\n|\\widehat{\\lambda} - \\lambda|_{\\text{worst}} \\approx \\kappa_{\\mathrm{abs}}(\\lambda) \\|\\Delta A\\|_2 = \\varepsilon \\sqrt{1+M^2}\n$$\nThis relationship shows that the condition number $\\kappa_{\\mathrm{abs}}(\\lambda)$ acts as an amplification factor. For large $M$, we have $\\kappa_{\\mathrm{abs}}(1) \\approx M$. Thus, a tiny backward error $\\varepsilon$ can be magnified into a forward error of size approximately $M\\varepsilon$.\n\nThis large condition number is a manifestation of the non-normality of the matrix $A(M)$ (since $A(M)A(M)^H \\neq A(M)^HA(M)$ for $M>0$). In non-normal matrices, the left and right eigenvectors are not necessarily orthogonal. The cosine of the angle $\\theta$ between $v$ and $w$ is given by:\n$$\n\\cos(\\theta) = \\frac{|w^H v|}{\\|w\\|_2 \\|v\\|_2} = \\frac{1}{\\sqrt{1+M^2}}\n$$\nFor large $M$, $\\cos(\\theta) \\to 0$, meaning $\\theta \\to \\pi/2$. The left and right eigenvectors become nearly orthogonal. The condition number can be expressed as $\\kappa_{\\mathrm{abs}}(\\lambda) = 1/|\\cos(\\theta)|$, which becomes very large as the eigenvectors approach orthogonality.\n\nThe amplification can also be seen from the exact relation derived by left-multiplying the residual equation by $w^H$:\n$$\nw^H r = w^H(A(M)\\widehat{v} - \\widehat{\\lambda}\\widehat{v}) = (\\lambda w^H)\\widehat{v} - \\widehat{\\lambda} w^H \\widehat{v} = (\\lambda - \\widehat{\\lambda})w^H\\widehat{v}\n$$\nThis gives an expression for the forward error:\n$$\n\\widehat{\\lambda} - \\lambda = -\\frac{w^H r}{w^H \\widehat{v}}\n$$\nThe magnitude of the error is $|\\widehat{\\lambda} - \\lambda| = \\frac{|w^H r|}{|w^H \\widehat{v}|}$. The numerator can be maximized by choosing the direction of the residual $r$. By the Cauchy-Schwarz inequality, $|w^H r| \\le \\|w\\|_2 \\|r\\|_2 = \\varepsilon \\sqrt{1+M^2}$. This maximum is achieved when the residual vector $r$ is parallel to the left eigenvector $w$. The denominator $|w^H \\widehat{v}|$ is typically close to $|w^H v|=1$ as $\\widehat{v}$ is an approximation to $v$. Therefore, the forward error is maximized when the residual $r$ points in the direction of the left eigenvector $w$, and this worst-case error has a magnitude of approximately $\\varepsilon \\sqrt{1+M^2}$.\n\nFinally, we compute the worst-case forward eigenvalue error magnitude for $M = 10^6$ and $\\varepsilon = 10^{-10}$.\n$$\n|\\widehat{\\lambda} - \\lambda|_{\\text{worst}} = \\kappa_{\\mathrm{abs}}(1) \\varepsilon = \\varepsilon \\sqrt{1+M^2}\n$$\nSubstituting the given values:\n$$\n|\\widehat{\\lambda} - \\lambda|_{\\text{worst}} = 10^{-10} \\sqrt{1 + (10^6)^2} = 10^{-10} \\sqrt{1 + 10^{12}}\n$$\nWe can factor out $10^{12}$ from the square root:\n$$\n|\\widehat{\\lambda} - \\lambda|_{\\text{worst}} = 10^{-10} \\sqrt{10^{12}(10^{-12} + 1)} = 10^{-10} \\cdot 10^6 \\sqrt{1 + 10^{-12}} = 10^{-4} \\sqrt{1 + 10^{-12}}\n$$\nFor a value $x \\ll 1$, the binomial approximation gives $\\sqrt{1+x} \\approx 1 + \\frac{1}{2}x$. Here, $x = 10^{-12}$, which is very small.\n$$\n\\sqrt{1 + 10^{-12}} \\approx 1 + \\frac{1}{2} \\cdot 10^{-12} = 1.0000000000005\n$$\nSo, the error is:\n$$\n|\\widehat{\\lambda} - \\lambda|_{\\text{worst}} \\approx 10^{-4} \\times (1 + 0.5 \\times 10^{-12}) = 10^{-4} + 0.5 \\times 10^{-16}\n$$\nThe second term is far too small to affect the first four significant figures. The value is $1.0000000000005 \\times 10^{-4}$. Rounding to four significant figures gives $1.000 \\times 10^{-4}$.",
            "answer": "$$\n\\boxed{1.000 \\times 10^{-4}}\n$$"
        },
        {
            "introduction": "This final practice explores the extreme behavior of eigenvalue sensitivity as a matrix approaches a non-diagonalizable, or defective, state. You will analyze a matrix family where the condition number diverges and relate this behavior to the geometry of the pseudospectrum . This exercise provides a deeper, more general understanding of why certain eigenvalues are sensitive by connecting the algebraic condition number to the behavior of the resolvent norm near an eigenvalue.",
            "id": "3576428",
            "problem": "Consider the $2 \\times 2$ upper-triangular, non-normal matrix $A$ parameterized by real scalars $ \\alpha $ and $ \\epsilon $,\n$$\nA = \\begin{bmatrix} 1 & \\alpha \\\\ 0 & 1+\\epsilon \\end{bmatrix}.\n$$\nThe eigenvalues of $A$ are $ \\lambda_1 = 1 $ and $ \\lambda_2 = 1+\\epsilon $. For the simple eigenvalue $ \\lambda_1 = 1 $ when $ \\epsilon \\neq 0 $, let $ x $ and $ y $ denote a corresponding right and left eigenvector, respectively, meaning $ A x = \\lambda_1 x $ and $ y^* A = \\lambda_1 y^* $. The condition number of the eigenvalue $ \\lambda_1 $ is defined by\n$$\n\\kappa(\\lambda_1) = \\frac{\\|x\\|_2 \\, \\|y\\|_2}{|y^* x|},\n$$\nwhere $ \\|\\cdot\\|_2 $ is the Euclidean vector norm. The $ \\delta $-pseudospectrum $ \\Lambda_{\\delta}(A) $ is defined as\n$$\n\\Lambda_{\\delta}(A) = \\{ z \\in \\mathbb{C} : \\sigma_{\\min}(z I - A) \\le \\delta \\},\n$$\nwhere $ \\sigma_{\\min}(\\cdot) $ denotes the smallest singular value and $ I $ is the identity matrix. It is known from first-order eigenvalue perturbation theory and the resolvent characterization of the pseudospectrum that, for a simple eigenvalue, the local boundary of $ \\Lambda_{\\delta}(A) $ around $ \\lambda_1 $ scales to first order like $ |z - \\lambda_1| \\approx \\kappa(\\lambda_1) \\, \\delta $ for small $ \\delta $.\n\nStarting from these core definitions and facts, derive the scaling behavior of $ \\kappa(\\lambda_1) $ with respect to $ \\epsilon $ for the matrix $ A $ and explain how this scaling is reflected in the local behavior of the pseudospectrum $ \\Lambda_{\\delta}(A) $ near $ \\lambda_1 $. Then implement an algorithm that, for given $ (\\alpha, \\epsilon) $ and a small real displacement $ r $, computes:\n- the condition number $ \\kappa(\\lambda_1) $ using numerically computed left and right eigenvectors;\n- the smallest singular value $ \\delta_{\\text{actual}} = \\sigma_{\\min}\\big((1+r) I - A\\big) $ using singular value decomposition (SVD);\n- the first-order prediction $ \\delta_{\\text{pred}} = \\frac{|r|}{\\kappa(\\lambda_1)} $;\n- the ratio $ \\rho = \\frac{\\delta_{\\text{actual}}}{\\delta_{\\text{pred}}} $.\n\nUse the Euclidean norm for vectors and the spectral norm induced by the singular value decomposition for matrices. Treat all quantities as dimensionless. Assume $ \\alpha $ and $ \\epsilon $ are real and use real arithmetic.\n\nYour program must compute these quantities for the following test suite, each specified as a triple $ (\\alpha, \\epsilon, r) $:\n1. $ (\\alpha, \\epsilon, r) = (1.0, 10^{-1}, 10^{-3}) $,\n2. $ (\\alpha, \\epsilon, r) = (1.0, 10^{-3}, 10^{-4}) $,\n3. $ (\\alpha, \\epsilon, r) = (10.0, 10^{-3}, 10^{-4}) $,\n4. $ (\\alpha, \\epsilon, r) = (10^{-3}, 10^{-1}, 10^{-3}) $,\n5. $ (\\alpha, \\epsilon, r) = (2.0, 2 \\cdot 10^{-2}, 10^{-3}) $,\n6. $ (\\alpha, \\epsilon, r) = (1.0, 10^{-12}, 10^{-9}) $.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where each test case’s result is itself a list of five floats in the order $ [\\kappa_{\\text{eig}}, \\kappa_{\\text{analytic}}, \\delta_{\\text{actual}}, \\delta_{\\text{pred}}, \\rho] $. Here, $ \\kappa_{\\text{eig}} $ is the condition number computed from numerically obtained left and right eigenvectors, $ \\kappa_{\\text{analytic}} $ is the condition number obtained from your derived closed-form for this $ A $, $ \\delta_{\\text{actual}} $ is the smallest singular value at $ z = 1 + r $, $ \\delta_{\\text{pred}} = |r| / \\kappa_{\\text{eig}} $, and $ \\rho = \\delta_{\\text{actual}} / \\delta_{\\text{pred}} $. For example, the overall format should look like\n$$\n[\\,[\\kappa_{\\text{eig}}, \\kappa_{\\text{analytic}}, \\delta_{\\text{actual}}, \\delta_{\\text{pred}}, \\rho],\\,\\dots\\,].\n$$\nNo other output should be produced.",
            "solution": "The problem requires a derivation of the eigenvalue condition number for a specific non-normal matrix, an analysis of its scaling behavior, and a corresponding numerical implementation to validate a first-order perturbation theory result.\n\nLet the given matrix be\n$$\nA = \\begin{bmatrix} 1 & \\alpha \\\\ 0 & 1+\\epsilon \\end{bmatrix}\n$$\nwhere $\\alpha, \\epsilon \\in \\mathbb{R}$ and we assume $\\epsilon \\neq 0$ so that the eigenvalues $\\lambda_1 = 1$ and $\\lambda_2 = 1+\\epsilon$ are simple. We focus on the eigenvalue $\\lambda_1 = 1$.\n\n**1. Derivation of Right and Left Eigenvectors**\n\nFirst, we must find a right eigenvector $x$ and a left eigenvector $y$ corresponding to $\\lambda_1 = 1$.\n\nA right eigenvector $x$ satisfies the equation $(A - \\lambda_1 I)x = 0$.\n$$\n(A - 1 \\cdot I)x = \\begin{bmatrix} 1-1 & \\alpha \\\\ 0 & (1+\\epsilon)-1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 0 & \\alpha \\\\ 0 & \\epsilon \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n$$\nThis system of equations yields $\\alpha x_2 = 0$ and $\\epsilon x_2 = 0$. Since we are interested in the non-normal case where $\\alpha \\neq 0$ and the distinct eigenvalue case where $\\epsilon \\neq 0$, these equations require that $x_2 = 0$. The component $x_1$ is unconstrained. We can choose $x_1 = 1$ for simplicity. Thus, a right eigenvector is:\n$$\nx = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n$$\n\nA left eigenvector $y$ satisfies the equation $y^* (A - \\lambda_1 I) = 0^*$. Since all parameters are real, the conjugate transpose $y^*$ is simply the transpose $y^T$.\n$$\ny^T (A - 1 \\cdot I) = \\begin{bmatrix} y_1 & y_2 \\end{bmatrix} \\begin{bmatrix} 0 & \\alpha \\\\ 0 & \\epsilon \\end{bmatrix} = \\begin{bmatrix} 0 & 0 \\end{bmatrix}\n$$\nThis matrix-vector product results in the single equation $\\alpha y_1 + \\epsilon y_2 = 0$. There is a one-dimensional space of solutions. A non-trivial solution can be found by setting $y_1 = \\epsilon$, which implies $y_2 = -\\alpha$. Thus, a left eigenvector is:\n$$\ny = \\begin{bmatrix} \\epsilon \\\\ -\\alpha \\end{bmatrix}\n$$\n\n**2. Derivation of the Eigenvalue Condition Number, $\\kappa(\\lambda_1)$**\n\nThe condition number of a simple eigenvalue $\\lambda_1$ is given by the formula\n$$\n\\kappa(\\lambda_1) = \\frac{\\|x\\|_2 \\|y\\|_2}{|y^* x|}\n$$\nUsing the eigenvectors derived above, we compute the necessary components:\n- The Euclidean norm of $x$: $\\|x\\|_2 = \\sqrt{1^2 + 0^2} = 1$.\n- The Euclidean norm of $y$: $\\|y\\|_2 = \\sqrt{\\epsilon^2 + (-\\alpha)^2} = \\sqrt{\\alpha^2 + \\epsilon^2}$.\n- The inner product $y^* x$: Since the vectors are real, $y^* x = y^T x = \\begin{bmatrix} \\epsilon & -\\alpha \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\epsilon \\cdot 1 + (-\\alpha) \\cdot 0 = \\epsilon$.\n\nSubstituting these into the definition of $\\kappa(\\lambda_1)$, we obtain the analytic expression:\n$$\n\\kappa(\\lambda_1) = \\frac{1 \\cdot \\sqrt{\\alpha^2 + \\epsilon^2}}{|\\epsilon|} = \\frac{\\sqrt{\\alpha^2 + \\epsilon^2}}{|\\epsilon|}\n$$\nThis is the closed-form expression for $\\kappa_{\\text{analytic}}$.\n\n**3. Scaling Behavior and Relationship to the Pseudospectrum**\n\nWe now analyze the scaling behavior of $\\kappa(\\lambda_1)$ as the eigenvalues become nearly coincident, i.e., as $\\epsilon \\to 0$. The expression can be rewritten as:\n$$\n\\kappa(\\lambda_1) = \\sqrt{\\frac{\\alpha^2}{\\epsilon^2} + 1}\n$$\nFor a fixed non-zero $\\alpha$ and as $|\\epsilon| \\to 0$, the term $(\\alpha/\\epsilon)^2$ dominates the $1$ under the square root. Therefore, the condition number behaves asymptotically as:\n$$\n\\kappa(\\lambda_1) \\approx \\sqrt{\\frac{\\alpha^2}{\\epsilon^2}} = \\frac{|\\alpha|}{|\\epsilon|}\n$$\nThis shows that $\\kappa(\\lambda_1)$ scales as $O(|\\epsilon|^{-1})$. The condition number grows arbitrarily large as the matrix $A$ approaches a defective matrix with a non-trivial Jordan block (which occurs at $\\epsilon=0$ for $\\alpha \\neq 0$). A large condition number signifies that the eigenvalue is highly sensitive to perturbations.\n\nThis sensitivity is captured by the pseudospectrum, $\\Lambda_{\\delta}(A)$. The quantity $\\delta_{\\text{actual}} = \\sigma_{\\min}((1+r)I - A)$ is the size of the smallest perturbation $E$ (in the spectral norm) such that $A+E$ has an eigenvalue at $z = 1+r$. Thus, $z=1+r$ lies on the boundary of $\\Lambda_{\\delta_{\\text{actual}}}(A)$.\n\nFirst-order perturbation theory predicts that a point $z$ near a simple eigenvalue $\\lambda_1$ lies on the boundary of the $\\delta$-pseudospectrum if $|z - \\lambda_1| \\approx \\kappa(\\lambda_1) \\delta$. Rearranging for $\\delta$, we have $\\delta \\approx |z - \\lambda_1| / \\kappa(\\lambda_1)$. In our case, $z = 1+r$ and $\\lambda_1 = 1$, so $|z-\\lambda_1|=|r|$. This gives the first-order prediction:\n$$\n\\delta_{\\text{pred}} = \\frac{|r|}{\\kappa(\\lambda_1)}\n$$\nThe problem asks to compute the ratio $\\rho = \\delta_{\\text{actual}} / \\delta_{\\text{pred}}$. This ratio measures the accuracy of the first-order approximation. For sufficiently small perturbations (i.e., small $|r|$), we expect $\\rho \\to 1$. The analysis confirms that the large condition number $\\kappa(\\lambda_1)$ (scaling as $O(|\\epsilon|^{-1})$) implies that a small perturbation $\\delta$ can induce a much larger change $|r|$ in the eigenvalue's position.\n\nThe algorithm to be implemented will compute $\\kappa(\\lambda_1)$ both numerically (via eigenvectors) and analytically, and then compare the actual perturbation size $\\delta_{\\text{actual}}$ required to move the eigenvalue by $r$ with the first-order prediction $\\delta_{\\text{pred}}$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes eigenvalue condition numbers and pseudospectral quantities\n    for a parameterized 2x2 non-normal matrix.\n    \"\"\"\n    test_cases = [\n        (1.0, 1e-1, 1e-3),\n        (1.0, 1e-3, 1e-4),\n        (10.0, 1e-3, 1e-4),\n        (1e-3, 1e-1, 1e-3),\n        (2.0, 2e-2, 1e-3),\n        (1.0, 1e-12, 1e-9),\n    ]\n\n    results = []\n\n    for alpha, epsilon, r in test_cases:\n        # Define the matrix A\n        A = np.array([[1.0, alpha], [0.0, 1.0 + epsilon]])\n\n        # 1. Compute numerical condition number kappa_eig\n        # Eigenvalue lambda_1 is 1.0\n        target_lambda = 1.0\n\n        # Right eigenvectors of A\n        eigvals_r, eigvecs_r = np.linalg.eig(A)\n        # Find index of eigenvalue closest to target_lambda\n        idx_r = np.argmin(np.abs(eigvals_r - target_lambda))\n        x = eigvecs_r[:, idx_r]\n\n        # Left eigenvectors of A are right eigenvectors of A.T\n        eigvals_l, eigvecs_l_T = np.linalg.eig(A.T)\n        # Find index of eigenvalue closest to target_lambda\n        idx_l = np.argmin(np.abs(eigvals_l - target_lambda))\n        y = eigvecs_l_T[:, idx_l]\n\n        # np.linalg.eig returns normalized eigenvectors, so ||x||=||y||=1\n        # kappa = (||x|| * ||y||) / |y* x| becomes 1 / |y* x|\n        kappa_eig = 1.0 / np.abs(np.dot(y.conj(), x))\n\n        # 2. Compute analytic condition number kappa_analytic\n        # kappa = sqrt(alpha^2 + epsilon^2) / |epsilon|\n        kappa_analytic = np.sqrt(alpha**2 + epsilon**2) / np.abs(epsilon)\n\n        # 3. Compute actual perturbation size delta_actual\n        # delta_actual = sigma_min((1+r)I - A)\n        M = (1.0 + r) * np.identity(2) - A\n        # svd returns singular values in descending order\n        singular_values = np.linalg.svd(M, compute_uv=False)\n        delta_actual = singular_values[-1]\n\n        # 4. Compute predicted perturbation size delta_pred\n        # delta_pred = |r| / kappa_eig\n        delta_pred = np.abs(r) / kappa_eig\n\n        # 5. Compute the ratio rho\n        rho = delta_actual / delta_pred\n        \n        # Collect the results for this case\n        case_results = [kappa_eig, kappa_analytic, delta_actual, delta_pred, rho]\n        results.append(case_results)\n\n    # Format the final output string as a list of lists, with no spaces\n    # Example: [[v1,v2],[v3,v4]]\n    # str(results) produces '[[v1, v2], [v3, v4]]', so we remove spaces.\n    output_str = str(results).replace(\" \", \"\")\n\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}