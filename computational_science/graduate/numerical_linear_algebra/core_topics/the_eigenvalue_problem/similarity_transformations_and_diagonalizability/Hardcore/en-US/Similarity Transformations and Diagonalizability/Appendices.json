{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we first reinforce a fundamental theoretical limit of diagonalizability. This exercise demonstrates that even simple real matrices cannot always be diagonalized using only real numbers, particularly when they possess complex eigenvalues. By constructing such a matrix and analyzing its structure, we motivate the need for alternative canonical forms like the real Schur decomposition, which provides a stable, upper quasi-triangular representation for any real matrix. ",
            "id": "3576903",
            "problem": "Let $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$ with $b \\neq 0$. Consider the real $2 \\times 2$ matrix\n$$\nB = \\begin{pmatrix} a & b \\\\ -b & a \\end{pmatrix},\n$$\nand let $S \\in \\mathbb{R}^{2 \\times 2}$ be any fixed real invertible matrix, for example\n$$\nS = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix}.\n$$\nDefine\n$$\nA = S B S^{-1}.\n$$\nStarting from the fundamental definitions of similarity, eigenvalues, diagonalizability over $\\mathbb{R}$ and over $\\mathbb{C}$ (the field of complex numbers), and the real version of the Schur decomposition, do the following:\n\n- Justify that $A$ has eigenvalues $a \\pm b i$, is not diagonalizable over $\\mathbb{R}$, and is diagonalizable over $\\mathbb{C}$.\n- Present the real Schur form of $A$ as a single $2 \\times 2$ rotation-scaling block, explaining why it must have the structure\n$$\n\\begin{pmatrix} a & \\gamma \\\\ -\\gamma & a \\end{pmatrix}\n$$\nfor some $\\gamma > 0$, and identify $\\gamma$ in terms of $a$ and $b$.\n\nFinally, compute the minimal polynomial $m_A(x)$ of $A$ as a single explicit polynomial in $x$ with real coefficients. Provide your final answer as a single closed-form analytic expression for $m_A(x)$. No numerical rounding is required for this problem; do not include any units.",
            "solution": "The problem statement is formally valid. It is self-contained, mathematically sound, and well-posed. We may proceed with the solution.\n\nThe problem requires an analysis of the matrix $A = S B S^{-1}$, where $a \\in \\mathbb{R}$, $b \\in \\mathbb{R}$ with $b \\neq 0$, \n$$B = \\begin{pmatrix} a & b \\\\ -b & a \\end{pmatrix},$$\nand $S$ is any real invertible $2 \\times 2$ matrix.\n\nFirst, we address the eigenvalues of $A$ and its diagonalizability over $\\mathbb{R}$ and $\\mathbb{C}$.\n\nBy definition, two matrices $A$ and $B$ are similar if there exists an invertible matrix $S$ such that $A = S B S^{-1}$. A fundamental property of similar matrices is that they share the same characteristic polynomial, and therefore the same eigenvalues. We can demonstrate this by considering the definition of an eigenvalue. Let $\\lambda$ be an eigenvalue of $B$ with a corresponding non-zero eigenvector $v$. Then, $Bv = \\lambda v$. Let us define a vector $w = Sv$. Since $S$ is invertible and $v \\neq 0$, $w$ is also a non-zero vector. We can then compute the action of $A$ on $w$:\n$$\nAw = (SBS^{-1})w = (SBS^{-1})(Sv) = SB(S^{-1}S)v = SBv\n$$\nSubstituting $Bv = \\lambda v$, we obtain:\n$$\nAw = S(\\lambda v) = \\lambda (Sv) = \\lambda w\n$$\nThis shows that $\\lambda$ is also an eigenvalue of $A$ with eigenvector $w$. Thus, the eigenvalues of $A$ are identical to the eigenvalues of $B$.\n\nTo find the eigenvalues of $B$, we compute its characteristic polynomial, $p_B(\\lambda) = \\det(B - \\lambda I)$:\n$$\np_B(\\lambda) = \\det \\begin{pmatrix} a - \\lambda & b \\\\ -b & a - \\lambda \\end{pmatrix} = (a - \\lambda)^2 - (b)(-b) = (a - \\lambda)^2 + b^2\n$$\nThe eigenvalues are the roots of the characteristic equation $p_B(\\lambda) = 0$:\n$$\n(a - \\lambda)^2 + b^2 = 0 \\implies (a - \\lambda)^2 = -b^2 \\implies a - \\lambda = \\pm \\sqrt{-b^2} = \\pm i \\sqrt{b^2} = \\pm i|b|\n$$\nSo, the eigenvalues are $\\lambda = a \\mp i|b|$. The set of eigenvalues is $\\{a + i|b|, a - i|b|\\}$. Since $b \\neq 0$, $|b| \\neq 0$. The problem statement presents the eigenvalues as $a \\pm bi$. This is the same set of eigenvalues regardless of the sign of $b$. We will proceed using this notation. The eigenvalues of $B$, and therefore of $A$, are $\\lambda_1 = a + bi$ and $\\lambda_2 = a - bi$.\n\nNext, we consider the diagonalizability of $A$. A matrix is diagonalizable over a field $\\mathbb{F}$ if it is similar to a diagonal matrix with entries from $\\mathbb{F}$. This is equivalent to stating that there exists a basis of eigenvectors for the underlying vector space.\nA necessary condition for a real matrix to be diagonalizable over the field of real numbers, $\\mathbb{R}$, is that all of its eigenvalues must be real. The eigenvalues of $A$ are $a \\pm bi$. Since we are given that $b \\in \\mathbb{R}$ and $b \\neq 0$, the imaginary part of the eigenvalues is non-zero. Therefore, the eigenvalues are not real. Consequently, the matrix $A$ is not diagonalizable over $\\mathbb{R}$.\n\nNow, we consider diagonalizability over the field of complex numbers, $\\mathbb{C}$. An $n \\times n$ matrix is diagonalizable over $\\mathbb{C}$ if it has $n$ linearly independent eigenvectors. A sufficient condition for this is that the matrix has $n$ distinct eigenvalues. The matrix $A$ is a $2 \\times 2$ matrix. Its eigenvalues are $\\lambda_1 = a + bi$ and $\\lambda_2 = a - bi$. Since $b \\neq 0$, it follows that $bi \\neq -bi$, and thus $\\lambda_1 \\neq \\lambda_2$. Since $A$ is a $2 \\times 2$ matrix with $2$ distinct eigenvalues, it is guaranteed to have $2$ linearly independent eigenvectors. Therefore, $A$ is diagonalizable over $\\mathbb{C}$.\n\nSecond, we analyze the real Schur form of $A$. The real Schur decomposition theorem states that for any real square matrix $A$, there exists a real orthogonal matrix $Q$ (i.e., $Q^T Q = I$) such that $T = Q^T A Q$ is an upper quasi-triangular matrix. This matrix $T$ is called a real Schur form of $A$. Its diagonal blocks are of size $1 \\times 1$ (corresponding to real eigenvalues) or $2 \\times 2$ (corresponding to pairs of complex conjugate eigenvalues).\nSince the matrix $A$ is a $2 \\times 2$ real matrix with a pair of complex conjugate eigenvalues $a \\pm bi$ (where $b \\neq 0$), its real Schur form $T$ must consist of a single $2 \\times 2$ block. This block $T$ is similar to $A$ (since $T = Q^T A Q = Q^{-1} A Q$) and thus must have the same eigenvalues, $a \\pm bi$.\nThe problem proposes that the real Schur form has the structure \n$$C = \\begin{pmatrix} a & \\gamma \\\\ -\\gamma & a \\end{pmatrix}$$\nfor some $\\gamma > 0$. Let's find the eigenvalues of this matrix $C$ by computing its characteristic polynomial:\n$$\n\\det(C - \\lambda I) = \\det \\begin{pmatrix} a - \\lambda & \\gamma \\\\ -\\gamma & a - \\lambda \\end{pmatrix} = (a - \\lambda)^2 + \\gamma^2\n$$\nThe eigenvalues are the roots of $(a - \\lambda)^2 + \\gamma^2 = 0$, which are $\\lambda = a \\pm i\\gamma$.\nFor these eigenvalues to match the eigenvalues of $A$, $a \\pm bi$, we must have $\\{a \\pm i\\gamma\\} = \\{a \\pm bi\\}$. This implies $\\gamma = |b|$. Since the problem specifies that $\\gamma > 0$ and we are given $b \\neq 0$, we have $\\gamma = |b|$. Thus, a real Schur form of $A$ has the specified rotation-scaling structure, with $\\gamma = |b|$.\n\nFinally, we compute the minimal polynomial $m_A(x)$ of $A$. The minimal polynomial of a matrix $A$ is the unique monic polynomial of least degree which, when evaluated at $A$, yields the zero matrix. The roots of the minimal polynomial are the eigenvalues of the matrix.\nThe eigenvalues of $A$ are $\\lambda_1 = a + bi$ and $\\lambda_2 = a - bi$. Therefore, $m_A(x)$ must be divisible by $(x - \\lambda_1)$ and $(x - \\lambda_2)$. Since we require $m_A(x)$ to have real coefficients, it must be divisible by the product:\n$$\n(x - (a + bi))(x - (a - bi)) = ((x-a) - bi)((x-a) + bi) = (x-a)^2 - (bi)^2 = (x-a)^2 + b^2\n$$\nLet this polynomial be $p(x) = x^2 - 2ax + a^2 + b^2$. This is a monic polynomial of degree $2$ with real coefficients.\nThe minimal polynomial $m_A(x)$ must divide the characteristic polynomial $p_A(x)$. We found the characteristic polynomial of $B$ to be $p_B(x) = (x-a)^2 + b^2 = x^2 - 2ax + a^2 + b^2$. Since $A$ and $B$ are similar, $p_A(x) = p_B(x)$.\nThe minimal polynomial $m_A(x)$ must have a degree of at least $2$, because a polynomial of degree $1$ with real coefficients would have a real root, which contradicts the fact that the eigenvalues of $A$ are non-real.\nSince $m_A(x)$ must divide $p_A(x)$ and both have degree $2$, and both are monic, they must be equal. This is also a direct consequence of the eigenvalues of $A$ being distinct. For any matrix with distinct eigenvalues, the minimal polynomial is identical to the characteristic polynomial.\nTherefore, the minimal polynomial of $A$ is:\n$$\nm_A(x) = x^2 - 2ax + a^2 + b^2\n$$\nThis is a single closed-form analytic expression for $m_A(x)$ with real coefficients.",
            "answer": "$$\n\\boxed{x^{2} - 2ax + a^{2} + b^{2}}\n$$"
        },
        {
            "introduction": "Having established that not all matrices are diagonalizable, we now turn to a practical question: how can we reliably diagnose diagonalizability for a given matrix? This numerical experiment moves beyond simple eigenvalue inspection to implement a robust test based on fundamental similarity invariants. You will write code to compute the dimensions of the null spaces of $(A - \\lambda I)^k$, which fully determine the Jordan structure and provide a definitive test for diagonalizability, even in the face of numerical noise. ",
            "id": "3576934",
            "problem": "You are to design and implement a program that constructs pairs of square matrices with identical spectra (multisets of eigenvalues) but with different Jordan structures, then numerically distinguishes their similarity-invariant properties without computing any Jordan normal form. Your program must demonstrate the use of invariants preserved under similarity transformations to detect non-diagonalizability and to distinguish matrices that are not similar despite having the same spectrum.\n\nFundamental base: Use the core definitions of similarity transformations, eigenvalues and eigenvectors, algebraic multiplicity, geometric multiplicity, rank, nullity, and the Jordan normal form. The Jordan normal form exists for any square matrix over the complex numbers, and similarity invariants include the spectrum with algebraic multiplicities, the minimal polynomial, and the dimension of kernels of powers of the nilpotent part associated with each eigenvalue. Explicitly, for an eigenvalue $\\lambda$ and a matrix $A$, the sequence of dimensions of kernels $\\dim \\ker (A - \\lambda I)^k$ for $k = 1, 2, \\dots, m$ (where $m$ is the algebraic multiplicity of $\\lambda$) is a similarity invariant that encodes the sizes and counts of Jordan blocks for $\\lambda$. A matrix is diagonalizable over the complex numbers if and only if for every eigenvalue $\\lambda$, the dimension of $\\ker (A - \\lambda I)$ equals the algebraic multiplicity of $\\lambda$.\n\nTasks:\n1. Construct for each test case a pair of matrices $(A,B)$ of the same size with the same spectrum but different Jordan structures. Choose real matrices so that eigenvalues are real, and construct at least one defective matrix (not diagonalizable) and its diagonalizable counterpart with the same spectrum.\n2. Implement numerical procedures to compute the following similarity-invariant quantities without forming any Jordan normal form:\n   - Cluster the eigenvalues of a matrix $A$ into groups of equal eigenvalues using a numerical tolerance $\\tau_{\\mathrm{eig}}$ to define equality (so eigenvalues within $\\tau_{\\mathrm{eig}}$ are considered the same). For each cluster representative $\\lambda$ with algebraic multiplicity $m$, compute the chain of nullities $\\nu_k = \\dim \\ker (A - \\lambda I)^k$ for $k = 1,2,\\dots,m$ using numerical rank via singular value decomposition. Use an absolute rank tolerance $\\tau_{\\mathrm{rank}}$, i.e., consider a singular value $s$ as zero if $s \\le \\tau_{\\mathrm{rank}}$. The nullity is $n - \\mathrm{rank}$, where $n$ is the matrix size.\n   - Decide whether $A$ is diagonalizable over the complex numbers by verifying the criterion $\\dim \\ker (A - \\lambda I) = m$ for every clustered eigenvalue $\\lambda$ with algebraic multiplicity $m$.\n3. Apply the above computations under two different rank tolerances to highlight numerical sensitivity:\n   - A strict tolerance $\\tau_{\\mathrm{rank}}^{\\mathrm{strict}} = 10^{-14}$.\n   - A relaxed tolerance $\\tau_{\\mathrm{rank}}^{\\mathrm{relaxed}} = 10^{-8}$.\n   Use $\\tau_{\\mathrm{eig}} = 10^{-12}$ for eigenvalue clustering in all runs.\n4. For each test case $(A,B)$, output six items:\n   - The boolean diagonalizability of $A$ under $\\tau_{\\mathrm{rank}}^{\\mathrm{strict}}$.\n   - The boolean diagonalizability of $B$ under $\\tau_{\\mathrm{rank}}^{\\mathrm{strict}}$.\n   - The boolean diagonalizability of $A$ under $\\tau_{\\mathrm{rank}}^{\\mathrm{relaxed}}$.\n   - The boolean diagonalizability of $B$ under $\\tau_{\\mathrm{rank}}^{\\mathrm{relaxed}}$.\n   - The similarity-invariant signature of $A$ under $\\tau_{\\mathrm{rank}}^{\\mathrm{strict}}$, defined as a list of pairs $[\\lambda, [\\nu_1,\\dots,\\nu_m]]$ over all eigenvalue clusters, with each $\\lambda$ represented as a real float and each $\\nu_k$ as an integer.\n   - The similarity-invariant signature of $B$ under $\\tau_{\\mathrm{rank}}^{\\mathrm{strict}}$, in the same format.\n\nTest suite:\n- Case 1 (happy path, small $3\\times 3$): \n  - $$A_1 = \\begin{bmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}$$ (defective with a size-$2$ Jordan block at $\\lambda=1$).\n  - $$B_1 = \\mathrm{diag}(1,1,2)$$ (diagonalizable).\n- Case 2 (coverage of different eigenvalue with algebraic multiplicity $2$, size $4\\times 4$):\n  - $$A_2 = \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 2 \\end{bmatrix}$$ (defective with a size-$2$ Jordan block at $\\lambda=0$).\n  - $$B_2 = \\mathrm{diag}(0,0,1,2)$$ (diagonalizable).\n- Case 3 (boundary case, near-defective, size $2\\times 2$):\n  - $$A_3 = \\begin{bmatrix} 1 & \\epsilon \\\\ 0 & 1 \\end{bmatrix}$$ with $\\epsilon = 10^{-12}$ (defective, but numerically delicate).\n  - $$B_3 = \\mathrm{diag}(1,1)$$ (diagonalizable).\n\nAngle units are not applicable, and no physical units are involved. All outputs must be booleans, integers, floats, or lists of these.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, one item per test case, where each item is itself a list of the six elements described above. Concretely, the output must be a Python-like literal of the form\n$[[d_{A_1}^{\\mathrm{strict}}, d_{B_1}^{\\mathrm{strict}}, d_{A_1}^{\\mathrm{relaxed}}, d_{B_1}^{\\mathrm{relaxed}}, \\mathrm{sig}(A_1), \\mathrm{sig}(B_1)], [d_{A_2}^{\\mathrm{strict}}, d_{B_2}^{\\mathrm{strict}}, d_{A_2}^{\\mathrm{relaxed}}, d_{B_2}^{\\mathrm{relaxed}}, \\mathrm{sig}(A_2), \\mathrm{sig}(B_2)], [d_{A_3}^{\\mathrm{strict}}, d_{B_3}^{\\mathrm{strict}}, d_{A_3}^{\\mathrm{relaxed}}, d_{B_3}^{\\mathrm{relaxed}}, \\mathrm{sig}(A_3), \\mathrm{sig}(B_3)]]$, where each $\\mathrm{sig}(A)$ is a list of $[\\lambda, [\\nu_1,\\dots,\\nu_m]]$ pairs. No additional text should be printed.",
            "solution": "The problem requires the design of a numerical procedure to distinguish between square matrices that share the same spectrum (multiset of eigenvalues) but are not similar. This distinction is to be made without explicitly computing the Jordan Normal Form (JNF). The core of the task lies in computing and comparing a more refined set of similarity invariants.\n\nA similarity transformation maps a square matrix $A$ to $P^{-1}AP$ for some invertible matrix $P$. Quantities that remain unchanged under all such transformations are called similarity invariants. The most common invariant is the spectrum, but as the problem stipulates, it is not a complete invariant. Two matrices can have identical spectra but different geometric structures, making them non-similar.\n\nThe complete classification of matrices under similarity is provided by the Jordan Normal Form. The JNF theorem states that any square matrix $A \\in \\mathbb{C}^{n \\times n}$ is similar to a block diagonal matrix $J = \\mathrm{diag}(J_1, J_2, \\ldots, J_p)$, where each $J_i$ is a Jordan block. A Jordan block for an eigenvalue $\\lambda$ of size $k$ is an upper bidiagonal matrix of the form:\n$$ J_k(\\lambda) = \\begin{bmatrix} \\lambda & 1 & & \\\\ & \\lambda & \\ddots & \\\\ & & \\ddots & 1 \\\\ & & & \\lambda \\end{bmatrix} \\in \\mathbb{C}^{k \\times k} $$\nTwo matrices are similar if and only if they have the same Jordan Normal Form, up to a permutation of the Jordan blocks. This means they must have the same eigenvalues, and for each eigenvalue $\\lambda$, the number and sizes of its corresponding Jordan blocks must be identical.\n\nA matrix is diagonalizable if and only if all its Jordan blocks are of size $1 \\times 1$. In this case, its JNF is a diagonal matrix.\n\nThe problem is to determine these structural properties—the counts and sizes of Jordan blocks—without computing $J$ or the similarity transformation $P$. This is achieved by analyzing the kernels of powers of the matrix $T_\\lambda = A - \\lambda I$. The dimensions of these kernels are similarity invariants. Let $\\lambda$ be an eigenvalue of $A$ with algebraic multiplicity $m$ (i.e., it is a root of the characteristic polynomial with multiplicity $m$). We define the sequence of nullities $\\nu_k$ as:\n$$ \\nu_k = \\dim \\ker( (A - \\lambda I)^k ) \\quad \\text{for } k = 1, 2, \\ldots, m $$\nThis sequence of integers encodes the entire Jordan structure associated with $\\lambda$. Specifically:\n- The number of Jordan blocks for $\\lambda$ is $\\nu_1$. This is the geometric multiplicity of $\\lambda$.\n- The number of Jordan blocks of size at least $k$ is given by the difference $\\nu_k - \\nu_{k-1}$ (with $\\nu_0 = 0$).\n- The sequence $(\\nu_1, \\nu_2, \\ldots, \\nu_m)$ is non-decreasing and stabilizes at $\\nu_p = \\nu_{p+1} = \\ldots = m$, where $p$ is the size of the largest Jordan block for $\\lambda$.\n\nFrom this, a matrix $A$ is diagonalizable if and only if for every eigenvalue $\\lambda$ with algebraic multiplicity $m$, its geometric multiplicity $\\nu_1$ is equal to $m$. If $\\nu_1 = m$, it implies that there are $m$ Jordan blocks, and since their sizes must sum to $m$, each block must be of size $1$.\n\nThe algorithmic procedure to solve the problem is as follows:\n\n1.  **Eigenvalue Computation and Clustering**: For a given matrix $A$, we first compute its eigenvalues using a standard numerical algorithm, such as the QR algorithm, provided by `numpy.linalg.eigvals`. Due to floating-point arithmetic, eigenvalues that are theoretically identical may be computed as a cluster of close numbers. We group these numerical eigenvalues using a tolerance $\\tau_{\\mathrm{eig}}$. For each cluster, we calculate the representative eigenvalue $\\lambda$ (e.g., the mean) and its algebraic multiplicity $m$ (the size of the cluster).\n\n2.  **Nullity Chain Computation**: For each distinct eigenvalue $\\lambda$ with multiplicity $m$, we compute the nullity chain $\\nu_1, \\nu_2, \\ldots, \\nu_m$. The nullity of a matrix $M$ is given by $\\mathrm{nullity}(M) = n - \\mathrm{rank}(M)$, where $n$ is the dimension of the matrix. The rank of the matrix $(A - \\lambda I)^k$ must be computed numerically.\n\n3.  **Numerical Rank Calculation**: Computing rank for a matrix with floating-point entries is an ill-posed problem. The most robust method is to use the Singular Value Decomposition (SVD). For any matrix $M$, its rank is the number of non-zero singular values. Numerically, we count the number of singular values $s_i$ that are greater than a specified absolute tolerance, $\\tau_{\\mathrm{rank}}$.\n    $$ \\mathrm{rank}(M) \\approx |\\{ s_i \\mid s_i > \\tau_{\\mathrm{rank}} \\}| $$\n    The choice of $\\tau_{\\mathrm{rank}}$ is critical and can affect the result, especially for \"near-defective\" matrices, as explored in the problem.\n\n4.  **Diagonalizability Check**: For each eigenvalue $\\lambda$ with algebraic multiplicity $m$, we check if $\\nu_1 = m$. The matrix $A$ is declared diagonalizable if and only if this condition holds for all its distinct eigenvalues.\n\n5.  **Signature Generation**: The similarity-invariant signature for $A$ is constructed as a list of pairs, where each pair consists of a representative eigenvalue $\\lambda$ and its corresponding nullity chain $[\\nu_1, \\ldots, \\nu_m]$. This signature uniquely characterizes the JNF of $A$. Two matrices are similar if and only if their signatures are identical (up to ordering of eigenvalues).\n\nThe problem requires performing this analysis with two different rank tolerances, $\\tau_{\\mathrm{rank}}^{\\mathrm{strict}} = 10^{-14}$ and $\\tau_{\\mathrm{rank}}^{\\mathrm{relaxed}} = 10^{-8}$, to demonstrate the sensitivity of numerical rank and, consequently, the determination of Jordan structure. For a matrix like \n$$A_3 = \\begin{bmatrix} 1 & \\epsilon \\\\ 0 & 1 \\end{bmatrix}$$\nwith a very small $\\epsilon=10^{-12}$, the matrix $A_3-I$ has a singular value of $10^{-12}$. The strict tolerance correctly identifies this as non-zero, revealing a rank of $1$ and hence defectiveness. The relaxed tolerance incorrectly classifies this singular value as zero, leading to an apparent rank of $0$ and a misleading conclusion of diagonalizability. This highlights a fundamental challenge in numerical linear algebra.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to construct and analyze matrix pairs.\n    \"\"\"\n\n    def analyze_matrix(A, tau_eig, tau_rank):\n        \"\"\"\n        Computes the diagonalizability and similarity-invariant signature of a matrix.\n\n        Args:\n            A (np.ndarray): The input square matrix.\n            tau_eig (float): Tolerance for clustering eigenvalues.\n            tau_rank (float): Tolerance for numerical rank calculation via SVD.\n\n        Returns:\n            tuple: A tuple containing:\n                - is_diagonalizable (bool): True if the matrix is numerically diagonalizable.\n                - signature (list): A list of [eigenvalue, nullity_chain] pairs.\n        \"\"\"\n        n = A.shape[0]\n        if n == 0:\n            return True, []\n        \n        try:\n            eigs = np.linalg.eigvals(A)\n        except np.linalg.LinAlgError:\n            # Handle cases where eigenvalue computation fails, though unlikely for test cases.\n            return False, []\n\n        # Sort eigenvalues to facilitate clustering\n        eigs = sorted(eigs, key=lambda x: (x.real, x.imag))\n\n        # Cluster eigenvalues\n        clusters = []\n        i = 0\n        while i < len(eigs):\n            j = i + 1\n            while j < len(eigs) and abs(eigs[j] - eigs[i]) <= tau_eig:\n                j += 1\n            \n            cluster_eigs = eigs[i:j]\n            lambda_rep = np.mean(cluster_eigs)\n            m = len(cluster_eigs)\n            clusters.append((lambda_rep, m))\n            i = j\n        \n        is_diagonalizable = True\n        signature = []\n\n        I = np.eye(n)\n\n        for lambda_val, m in clusters:\n            nullity_chain = []\n            \n            # Compute T = A - lambda*I\n            T = A - lambda_val * I\n            \n            # Use np.linalg.matrix_power for T^k\n            Tk = np.copy(T)\n            for k in range(1, m + 1):\n                if k > 1:\n                    Tk = Tk @ T\n                \n                # Compute numerical rank using SVD\n                s = np.linalg.svd(Tk, compute_uv=False)\n                rank_k = np.sum(s > tau_rank)\n                nullity_k = n - rank_k\n                nullity_chain.append(int(nullity_k))\n            \n            # Check diagonalizability condition for this eigenvalue\n            # Geometric multiplicity (nu_1) must equal algebraic multiplicity (m)\n            if nullity_chain[0] != m:\n                is_diagonalizable = False\n\n            signature.append([lambda_val.real, nullity_chain])\n\n        # Sort signature by eigenvalue for deterministic output\n        signature.sort(key=lambda x: x[0])\n\n        return is_diagonalizable, signature\n\n    # Define tolerances\n    tau_eig = 1e-12\n    tau_rank_strict = 1e-14\n    tau_rank_relaxed = 1e-8\n    \n    # Define test cases\n    A1 = np.array([[1, 1, 0], [0, 1, 0], [0, 0, 2]], dtype=float)\n    B1 = np.diag([1, 1, 2]).astype(float)\n    \n    A2 = np.array([[0, 1, 0, 0], [0, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 2]], dtype=float)\n    B2 = np.diag([0, 0, 1, 2]).astype(float)\n    \n    epsilon = 1e-12\n    A3 = np.array([[1, epsilon], [0, 1]], dtype=float)\n    B3 = np.diag([1, 1]).astype(float)\n\n    test_cases = [\n        (A1, B1),\n        (A2, B2),\n        (A3, B3)\n    ]\n\n    all_results = []\n    for A, B in test_cases:\n        # Analyze with strict tolerance\n        d_A_strict, sig_A = analyze_matrix(A, tau_eig, tau_rank_strict)\n        d_B_strict, sig_B = analyze_matrix(B, tau_eig, tau_rank_strict)\n\n        # Analyze with relaxed tolerance (only need diagonalizability)\n        d_A_relaxed, _ = analyze_matrix(A, tau_eig, tau_rank_relaxed)\n        d_B_relaxed, _ = analyze_matrix(B, tau_eig, tau_rank_relaxed)\n\n        case_result = [\n            d_A_strict, d_B_strict,\n            d_A_relaxed, d_B_relaxed,\n            sig_A, sig_B\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string to match the required Python literal format\n    # Using a direct mapping to string handles booleans, floats, and lists correctly.\n    result_str = \"[\" + \",\".join(map(str, all_results)) + \"]\"\n    \n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Finally, we confront a crucial lesson in numerical linear algebra: theoretical diagonalizability does not guarantee numerical stability. This practice explores the behavior of \"nearly defective\" matrices—those with clustered eigenvalues and nearly dependent eigenvectors—which are diagonalizable in theory but whose eigenvector bases are extremely sensitive to perturbation. Through a guided coding exercise, you will quantify this fragility by observing how tiny changes in the matrix can lead to large swings in the computed eigenvectors, a phenomenon governed by the condition number of the eigenvector matrix $S$. ",
            "id": "3576932",
            "problem": "You are to design and implement a numerical experiment that probes the stability of similarity transformations when diagonalization is theoretically possible but numerically fragile due to nearly linearly dependent eigenvectors. The investigation must be grounded in the core definitions of similarity and diagonalizability from linear algebra. A square matrix $A \\in \\mathbb{R}^{n \\times n}$ is diagonalizable if there exists an invertible matrix $S \\in \\mathbb{R}^{n \\times n}$ and a diagonal matrix $\\Lambda \\in \\mathbb{R}^{n \\times n}$ such that $A = S \\Lambda S^{-1}$. The columns of $S$ are the eigenvectors of $A$ and the diagonal entries of $\\Lambda$ are the corresponding eigenvalues. When the columns of $S$ are nearly linearly dependent, the invertibility of $S$ remains intact in exact arithmetic, but $S$ becomes ill-conditioned, making the similarity transformation numerically unstable under perturbations to $A$.\n\nConstruct a parametric family of real $2 \\times 2$ matrices \n$$\nA(\\delta) = \\begin{bmatrix}\n1 & 1 \\\\\n0 & 1+\\delta\n\\end{bmatrix},\n$$ \nwith parameter $\\delta > 0$, and consider tiny deterministic perturbations \n$$\nE(\\varepsilon) = \\begin{bmatrix}\n0 & 0 \\\\\n\\varepsilon & 0\n\\end{bmatrix},\n$$ \nwith parameter $\\varepsilon \\ge 0$, defining $B(\\delta,\\varepsilon) = A(\\delta) + E(\\varepsilon)$. For each triple $(\\delta,\\varepsilon,\\tau)$, you must perform the following computational tasks:\n\n1. Compute the eigen-decomposition of $A(\\delta)$ and $B(\\delta,\\varepsilon)$ numerically to obtain eigenvector matrices $S_A$ and $S_B$, respectively, and their eigenvalues. Order eigenvectors by ascending eigenvalues to form $S_A$ and $S_B$. Normalize each eigenvector to unit Euclidean length and fix the sign by enforcing a nonnegative first nonzero entry to eliminate arbitrary scaling and sign ambiguity.\n2. Compute the spectral condition number of $S_A$ in the matrix $2$-norm,\n$$\n\\kappa_2(S_A) = \\|S_A\\|_2 \\cdot \\|S_A^{-1}\\|_2,\n$$\nwhere $\\|\\cdot\\|_2$ denotes the operator norm induced by the vector $2$-norm.\n3. For the corresponding ordered and normalized eigenvectors $v_i$ from $S_A$ and $w_i$ from $S_B$ for $i \\in \\{1,2\\}$, compute the principal angle \n$$\n\\theta_i = \\arccos\\!\\left(\\frac{|v_i^\\top w_i|}{\\|v_i\\|_2 \\cdot \\|w_i\\|_2}\\right),\n$$\nexpressed in radians, and report the maximum angle $\\max\\{\\theta_1,\\theta_2\\}$ for each triple $(\\delta,\\varepsilon,\\tau)$.\n4. Declare a robustness decision as a boolean by comparing the maximum angle to the threshold $\\tau$: the experiment is robust if $\\max\\{\\theta_1,\\theta_2\\} \\le \\tau$ and non-robust otherwise.\n\nYour program must implement the above for the following test suite of parameter triples $(\\delta,\\varepsilon,\\tau)$, designed to exercise different regimes:\n\n- $(\\delta,\\varepsilon,\\tau) = (10^{-1}, 10^{-8}, 0.05)$,\n- $(\\delta,\\varepsilon,\\tau) = (10^{-4}, 10^{-6}, 0.05)$,\n- $(\\delta,\\varepsilon,\\tau) = (10^{-8}, 10^{-12}, 0.05)$,\n- $(\\delta,\\varepsilon,\\tau) = (10^{-8}, 10^{-8}, 0.05)$,\n- $(\\delta,\\varepsilon,\\tau) = (10^{-6}, 5 \\cdot 10^{-6}, 0.05)$.\n\nAll angles must be expressed in radians. The final output for the entire test suite must be a single line consisting of a top-level list, where each element is itself a list of the form $[\\theta_{\\max}, \\kappa_2(S_A), \\text{robust}]$, with $\\theta_{\\max}$ a floating-point number, $\\kappa_2(S_A)$ a floating-point number, and $\\text{robust}$ a boolean. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[[\\theta_1,\\kappa_1,\\text{True}],[\\theta_2,\\kappa_2,\\text{False}]]$). No external input is allowed; all parameters are fixed as given above.",
            "solution": "The problem statement is valid. It presents a well-posed numerical experiment in linear algebra, grounded in established principles of matrix theory, specifically concerning the diagonalizability and stability of eigensystems. All terms are formally defined, the parameters are complete, and the tasks are computationally feasible.\n\nThe objective is to investigate the numerical stability of the eigenvectors of a matrix that is close to being defective (i.e., not diagonalizable). A matrix is defective if it has repeated eigenvalues and an insufficient number of linearly independent eigenvectors. The provided matrix family, $A(\\delta)$, is designed to approach a defective matrix as the parameter $\\delta$ approaches $0$.\n\nThe matrix family is given by\n$$ A(\\delta) = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1+\\delta \\end{bmatrix}, \\quad \\delta > 0 $$\nSince $A(\\delta)$ is an upper triangular matrix, its eigenvalues are its diagonal entries, $\\lambda_{A,1} = 1$ and $\\lambda_{A,2} = 1+\\delta$. For $\\delta > 0$, these eigenvalues are distinct, guaranteeing that $A(\\delta)$ is diagonalizable.\n\nThe corresponding right eigenvectors are found by solving $(A(\\delta) - \\lambda I)v = 0$.\nFor $\\lambda_{A,1} = 1$:\n$$ (A(\\delta) - 1 \\cdot I)v = \\begin{bmatrix} 0 & 1 \\\\ 0 & \\delta \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\implies y=0 $$\nThe eigenvector is any multiple of \n$$v_1 = [1, 0]^\\top.$$\n\nFor $\\lambda_{A,2} = 1+\\delta$:\n$$ (A(\\delta) - (1+\\delta)I)v = \\begin{bmatrix} -\\delta & 1 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\implies -\\delta x + y = 0 $$\nThe eigenvector is any multiple of \n$$v_2 = [1, \\delta]^\\top.$$\n\nAs $\\delta \\to 0$, the two eigenvalues coalesce at $\\lambda=1$. Concurrently, the eigenvector $v_2$ approaches $v_1$, becoming nearly linearly dependent. The angle between $v_1$ and $v_2$ is approximately $\\delta$ radians.\n\nThe matrix of eigenvectors, $S_A$, is formed by the normalized and ordered eigenvectors. Following the problem's prescription (order by ascending eigenvalue, normalize to unit length, and fix sign so the first non-zero entry is non-negative), we get:\n$$ v_1^{\\text{norm}} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad v_2^{\\text{norm}} = \\frac{1}{\\sqrt{1+\\delta^2}} \\begin{bmatrix} 1 \\\\ \\delta \\end{bmatrix} $$\n$$ S_A = \\begin{bmatrix} 1 & \\frac{1}{\\sqrt{1+\\delta^2}} \\\\ 0 & \\frac{\\delta}{\\sqrt{1+\\delta^2}} \\end{bmatrix} $$\nThe matrix becomes ill-conditioned as $\\delta \\to 0$, which is evident from its determinant, $\\det(S_A) = \\frac{\\delta}{\\sqrt{1+\\delta^2}}$, approaching $0$. The condition number $\\kappa_2(S_A) = \\|S_A\\|_2 \\|S_A^{-1}\\|_2$ is expected to be large for small $\\delta$. Analytically, one can find that $\\|S_A^{-1}\\|_2$ scales as $O(1/\\delta)$, thus $\\kappa_2(S_A)$ grows as $1/\\delta$. A large condition number signifies that the eigenvector basis is sensitive to perturbations.\n\nThe experiment introduces a perturbation $E(\\varepsilon)$ to $A(\\delta)$, resulting in the matrix:\n$$ B(\\delta, \\varepsilon) = A(\\delta) + E(\\varepsilon) = \\begin{bmatrix} 1 & 1 \\\\ \\varepsilon & 1+\\delta \\end{bmatrix} $$\nThe core of the task is to numerically compute the eigensystems of $A(\\delta)$ and $B(\\delta, \\varepsilon)$ for given sets of parameters $(\\delta, \\varepsilon, \\tau)$, and to quantify the change in the eigenvectors. The change is measured by the maximum principal angle between corresponding eigenvectors.\n\nThe computational procedure for each parameter triple $(\\delta, \\varepsilon, \\tau)$ is as follows:\n1.  Construct the matrices $A(\\delta)$ and $B(\\delta, \\varepsilon)$.\n2.  Numerically compute the eigenvalues and eigenvectors for both matrices using a standard eigensolver (e.g., from the NumPy library).\n3.  For each matrix, sort the eigenvalues in ascending order and reorder the corresponding eigenvectors to form the columns of the eigenvector matrices, $S_A$ and $S_B$.\n4.  Standardize each eigenvector for uniqueness:\n    a. Normalize the vector to have a Euclidean norm of $1$.\n    b. If the first non-zero element of the normalized vector is negative, multiply the vector by $-1$.\n5.  Compute the spectral condition number of the resulting matrix $S_A$ using the matrix $2$-norm: $\\kappa_2(S_A) = \\|S_A\\|_2 \\|S_A^{-1}\\|_2$.\n6.  The columns of the processed matrices $S_A = [v_1, v_2]$ and $S_B = [w_1, w_2]$ are the standardized eigenvectors. Compute the principal angles $\\theta_1 = \\arccos(|v_1^\\top w_1|)$ and $\\theta_2 = \\arccos(|v_2^\\top w_2|)$. The absolute value of the dot product is taken, and since the vectors are of unit norm, this directly gives the angle. Care must be taken with floating-point arithmetic to ensure the argument of $\\arccos$ is within the valid range $[-1, 1]$.\n7.  Determine the maximum angle $\\theta_{\\max} = \\max\\{\\theta_1, \\theta_2\\}$.\n8.  Classify the result as robust if $\\theta_{\\max} \\le \\tau$ and non-robust otherwise.\n\nThis procedure will be systematically applied to the provided test suite to generate the required output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_standardized_eigensystem(matrix):\n    \"\"\"\n    Computes eigenvalues and eigenvectors of a matrix, then standardizes them.\n\n    1. Eigenvectors are sorted according to ascending eigenvalues.\n    2. Each eigenvector is normalized to unit L2 norm.\n    3. The sign of each eigenvector is fixed by making its first non-zero\n       element non-negative.\n    \n    Args:\n        matrix (np.ndarray): A square matrix.\n\n    Returns:\n        np.ndarray: The standardized eigenvector matrix S, where columns are\n                    the standardized eigenvectors.\n    \"\"\"\n    eigvals, eigvecs = np.linalg.eig(matrix)\n    \n    # 1. Sort eigenvectors based on ascending eigenvalues\n    sort_indices = np.argsort(eigvals)\n    sorted_eigvecs = eigvecs[:, sort_indices]\n    \n    standardized_S = np.zeros_like(sorted_eigvecs, dtype=float)\n    \n    for i in range(sorted_eigvecs.shape[1]):\n        vec = sorted_eigvecs[:, i]\n        \n        # 2. Normalize to unit Euclidean length\n        norm = np.linalg.norm(vec, 2)\n        if norm > 0:\n            vec_normalized = vec / norm\n        else:\n            # Should not happen for eigenvectors, but handle gracefully.\n            vec_normalized = vec\n        \n        # 3. Fix sign by enforcing a non-negative first non-zero entry\n        first_nonzero_indices = np.flatnonzero(np.round(vec_normalized, 15))\n        if first_nonzero_indices.size > 0:\n            first_nonzero_idx = first_nonzero_indices[0]\n            if vec_normalized[first_nonzero_idx] < 0:\n                vec_normalized *= -1\n        \n        standardized_S[:, i] = vec_normalized\n        \n    return standardized_S\n\ndef solve():\n    \"\"\"\n    Main function to run the numerical experiment for the given test suite.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (delta, epsilon, tau)\n        (1e-1, 1e-8, 0.05),\n        (1e-4, 1e-6, 0.05),\n        (1e-8, 1e-12, 0.05),\n        (1e-8, 1e-8, 0.05),\n        (1e-6, 5e-6, 0.05),\n    ]\n\n    results = []\n    for delta, epsilon, tau in test_cases:\n        # Construct matrices A(delta) and B(delta, epsilon)\n        A_delta = np.array([[1.0, 1.0], \n                             [0.0, 1.0 + delta]], dtype=float)\n        \n        B_delta_eps = np.array([[1.0, 1.0], \n                                [epsilon, 1.0 + delta]], dtype=float)\n\n        # Task 1: Compute standardized eigen-decompositions\n        S_A = compute_standardized_eigensystem(A_delta)\n        S_B = compute_standardized_eigensystem(B_delta_eps)\n        \n        # Task 2: Compute spectral condition number of S_A\n        kappa_2_SA = np.linalg.cond(S_A, 2)\n        \n        # Task 3: Compute maximum principal angle\n        v1, v2 = S_A[:, 0], S_A[:, 1]\n        w1, w2 = S_B[:, 0], S_B[:, 1]\n        \n        # Clip dot product to handle potential floating point inaccuracies\n        dot1 = np.clip(np.abs(v1.T @ w1), -1.0, 1.0)\n        dot2 = np.clip(np.abs(v2.T @ w2), -1.0, 1.0)\n        \n        theta1 = np.arccos(dot1)\n        theta2 = np.arccos(dot2)\n        \n        theta_max = max(theta1, theta2)\n        \n        # Task 4: Declare robustness decision\n        is_robust = theta_max <= tau\n        \n        results.append([theta_max, kappa_2_SA, is_robust])\n\n    # Final print statement in the exact required format.\n    # [ [theta_max1, kappa1, robust1], [theta_max2, kappa2, robust2], ... ]\n    sublist_strings = []\n    for res in results:\n        # Format each sublist as a string \"[val1,val2,val3]\"\n        sublist_str = f\"[{res[0]},{res[1]},{'True' if res[2] else 'False'}]\"\n        sublist_strings.append(sublist_str)\n    \n    final_output = f\"[{','.join(sublist_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}