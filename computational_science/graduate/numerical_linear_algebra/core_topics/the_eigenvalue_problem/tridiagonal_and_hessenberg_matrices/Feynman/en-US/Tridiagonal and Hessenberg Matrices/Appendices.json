{
    "hands_on_practices": [
        {
            "introduction": "Upper Hessenberg matrices represent a crucial intermediate structure in many numerical linear algebra algorithms, most notably in eigenvalue computations. While Gaussian elimination (GE) is a fundamental method for solving linear systems, its numerical stability is not guaranteed without proper precautions. This exercise  provides a hands-on demonstration that the standard GE algorithm, when applied without pivoting, can fail catastrophically even for a seemingly well-behaved Hessenberg matrix, underscoring the indispensable role of pivoting strategies in robust numerical software.",
            "id": "3600001",
            "problem": "Consider an $n \\times n$ upper Hessenberg matrix $A$, defined by the property that $a_{ij} = 0$ whenever $i > j + 1$. Gaussian elimination (GE) without pivoting proceeds by successively selecting the diagonal element $a_{kk}^{(k-1)}$ of the partially reduced matrix $A^{(k-1)}$ as the $k$-th pivot, eliminating entries below it in column $k$ using a unit lower triangular multiplier, and updating trailing submatrices by rank-one transformations. A breakdown occurs at step $k$ if the pivot $a_{kk}^{(k-1)}$ is exactly zero, causing division by zero in the formation of multipliers.\n\nUse these definitions to analyze the following explicit counterexample. Let\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 & 0 \\\\\n1 & 2 & 5 & 6 \\\\\n0 & 4 & 1 & 7 \\\\\n0 & 0 & 8 & 1\n\\end{pmatrix},\n$$\nwhich is upper Hessenberg since $a_{31} = 0$, $a_{41} = 0$, and $a_{42} = 0$, while entries on and above the first subdiagonal are unrestricted.\n\nApply Gaussian elimination without pivoting to $A$, beginning with the pivot $a_{11}$ at step $k=1$, and proceed in the standard way to form the partially reduced matrices $A^{(1)}$, $A^{(2)}$, and so on, provided the pivot is nonzero at each step. Determine the earliest step index $k$ at which Gaussian elimination without pivoting breaks down due to a zero pivot for this matrix. Your final answer must be the single integer value of $k$.",
            "solution": "The problem asks for the earliest step $k$ at which Gaussian elimination (GE) without pivoting breaks down for a given $4 \\times 4$ upper Hessenberg matrix $A$. A breakdown occurs at step $k$ if the pivot element, which is the diagonal element $a_{kk}^{(k-1)}$ of the matrix $A^{(k-1)}$ obtained after step $k-1$, is zero. The initial matrix is denoted as $A^{(0)}$.\n\nThe given matrix is:\n$$\nA \\;=\\; A^{(0)} \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 & 0 \\\\\n1 & 2 & 5 & 6 \\\\\n0 & 4 & 1 & 7 \\\\\n0 & 0 & 8 & 1\n\\end{pmatrix}\n$$\nThis is an upper Hessenberg matrix as all entries $a_{ij}$ with $i > j+1$ are zero. Specifically, $a_{31}=0$, $a_{41}=0$, and $a_{42}=0$.\n\nWe proceed with Gaussian elimination step-by-step.\n\n**Step $k=1$:**\nThe matrix at the beginning of this step is $A^{(0)}$. The first pivot is the element $a_{11}^{(0)}$, which is the $(1,1)$ entry of $A^{(0)}$.\n$$\n\\text{pivot}_1 = a_{11}^{(0)} = 1\n$$\nSince the pivot is non-zero, GE does not break down at this step. The goal of this step is to introduce zeros below the pivot in the first column. The only non-zero element below the pivot is $a_{21}^{(0)}=1$. The elements $a_{31}^{(0)}$ and $a_{41}^{(0)}$ are already zero.\n\nTo eliminate $a_{21}^{(0)}$, we compute the multiplier $m_{21}$:\n$$\nm_{21} = \\frac{a_{21}^{(0)}}{a_{11}^{(0)}} = \\frac{1}{1} = 1\n$$\nWe then update the second row ($R_2$) by subtracting $m_{21}$ times the first row ($R_1$) from it: $R_2 \\leftarrow R_2 - m_{21}R_1$.\n$$\nR_2 \\leftarrow R_2 - 1 \\cdot R_1\n$$\nThe new second row becomes:\n$$\n\\begin{pmatrix} 1 & 2 & 5 & 6 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} 1 & 2 & 3 & 0 \\end{pmatrix} = \\begin{pmatrix} 1-1 & 2-2 & 5-3 & 6-0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 2 & 6 \\end{pmatrix}\n$$\nThe rows $R_3$ and $R_4$ do not need to be modified in this step since $a_{31}^{(0)}=0$ and $a_{41}^{(0)}=0$, which means their corresponding multipliers $m_{31}$ and $m_{41}$ are zero.\n\nThe resulting matrix after step $k=1$ is $A^{(1)}$:\n$$\nA^{(1)} \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 & 0 \\\\\n0 & 0 & 2 & 6 \\\\\n0 & 4 & 1 & 7 \\\\\n0 & 0 & 8 & 1\n\\end{pmatrix}\n$$\n\n**Step $k=2$:**\nThe matrix at the beginning of this step is $A^{(1)}$. The second pivot is the element $a_{22}^{(1)}$, which is the $(2,2)$ entry of $A^{(1)}$.\n$$\n\\text{pivot}_2 = a_{22}^{(1)} = 0\n$$\nAccording to the definition provided, Gaussian elimination without pivoting breaks down at step $k$ if the $k$-th pivot is zero. In this case, at step $k=2$, the pivot is $a_{22}^{(1)} = 0$. The algorithm cannot proceed because the computation of the multipliers for eliminating elements below this pivot (e.g., $m_{32} = a_{32}^{(1)} / a_{22}^{(1)}$) would involve division by zero.\n\nTherefore, the earliest step at which Gaussian elimination without pivoting breaks down is $k=2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "While general methods may falter, the special structure of tridiagonal matrices often permits a deep and elegant analysis. This comprehensive exercise  guides you through a full spectral investigation of the symmetric tridiagonal Toeplitz matrix, a canonical model in scientific computing. You will derive its exact eigenvalues and eigenvectors, analyze the sensitivity of these eigenvectors using perturbation theory, and design an optimal spectral transformation to improve computational accuracy for nearly degenerate eigenvalues. This practice provides an integrated view of spectral theory, perturbation analysis, and algorithmic design.",
            "id": "3600010",
            "problem": "Let $n \\geq 3$ and let $T(a,b) \\in \\mathbb{R}^{n \\times n}$ be the real symmetric tridiagonal Toeplitz matrix with $a \\in \\mathbb{R}$ on the diagonal and $b \\in \\mathbb{R} \\setminus \\{0\\}$ on the subdiagonal and superdiagonal. That is, for $1 \\leq i \\leq n$, the diagonal entries are $a$, and for $1 \\leq i \\leq n-1$, the entries $(i,i+1)$ and $(i+1,i)$ are $b$, with all other entries $0$. Consider the following tasks:\n\n(i) Starting from the definition of an eigenpair and using only core properties of symmetric matrices and the tridiagonal structure of $T(a,b)$, derive explicit closed-form expressions for all eigenvalues and corresponding eigenvectors of $T(a,b)$ as functions of $a$, $b$, $n$, and the eigenpair index $k$.\n\n(ii) Using a well-tested perturbation bound for symmetric matrices that relates the change in an invariant subspace to the spectral separation (for example, an instance of the Davisâ€“Kahan angle theorem), define a normwise first-order eigenvector condition number $\\kappa_{k}$ in terms of the minimal separation between the eigenvalue $\\lambda_{k}$ and the rest of the spectrum. Specialize your expression to the case of a nearly repeated adjacent pair at indices $j$ and $j+1$ (with $1 \\leq j \\leq n-1$), and derive a sharp leading-order asymptotic approximation to $\\kappa_{j}$ for large $n$ with fixed $j$ in terms of $a$, $b$, $n$, and $j$.\n\n(iii) Consider the shift-invert spectral transformation with real shift $\\sigma$, mapping $T(a,b)$ to $(T(a,b)-\\sigma I)^{-1}$. The eigenvectors of $(T(a,b)-\\sigma I)^{-1}$ coincide with those of $T(a,b)$, while the transformed eigenvalues are $1/(\\lambda_{k}-\\sigma)$. To reduce the sensitivity of the computation of the eigenvector associated with the nearly repeated pair $(\\lambda_{j},\\lambda_{j+1})$ under normwise perturbations that are bounded relative to the operator norm of the transformed matrix, choose $\\sigma$ to solve the following minimax separation design problem: maximize, over real $\\sigma$ for which $(T(a,b)-\\sigma I)$ is invertible, the minimum distance $\\min\\{|\\lambda_{j}-\\sigma|,|\\lambda_{j+1}-\\sigma|\\}$, and thereby maximize the local separation $|1/(\\lambda_{j}-\\sigma)-1/(\\lambda_{j+1}-\\sigma)|$ in the transformed spectrum subject to a balanced conditioning of $(T(a,b)-\\sigma I)^{-1}$ with respect to the pair. Derive the unique optimal $\\sigma^{*}$ in closed form as a function of $a$, $b$, $n$, and $j$.\n\nProvide the final answer as the explicit expression for $\\sigma^{*}$. No rounding is required.",
            "solution": "The problem has been validated and is deemed a well-posed, scientifically grounded, and self-contained problem in numerical linear algebra. It requests the derivation of fundamental properties of a symmetric tridiagonal Toeplitz matrix and the solution of an optimization problem related to spectral transformations. The derivation will proceed in three parts as outlined in the problem statement.\n\n(i) Derivation of Eigenvalues and Eigenvectors\n\nLet $T(a,b) \\in \\mathbb{R}^{n \\times n}$ be the specified real symmetric tridiagonal Toeplitz matrix. Let $\\lambda$ be an eigenvalue and $v = [v_1, v_2, \\dots, v_n]^T$ be the corresponding eigenvector, where $v \\neq 0$. The eigenvalue equation is $T(a,b)v = \\lambda v$. This expands into a system of $n$ linear equations. For a generic row $i$, where $2 \\leq i \\leq n-1$, we have:\n$$ b v_{i-1} + a v_i + b v_{i+1} = \\lambda v_i $$\nThis can be rewritten as a second-order homogeneous linear difference equation:\n$$ b v_{i+1} + (a-\\lambda) v_i + b v_{i-1} = 0 $$\nThe equations for the first and last rows are:\n$$ a v_1 + b v_2 = \\lambda v_1 \\implies (a-\\lambda)v_1 + b v_2 = 0 $$\n$$ b v_{n-1} + a v_n = \\lambda v_n \\implies b v_{n-1} + (a-\\lambda)v_n = 0 $$\nTo make these two boundary equations fit the general recurrence relation form, we can define ghost components $v_0$ and $v_{n+1}$. The first equation becomes $b v_0 + (a-\\lambda)v_1 + b v_2 = 0$ if we impose the boundary condition $v_0 = 0$. The last equation becomes $b v_{n-1} + (a-\\lambda)v_n + b v_{n+1} = 0$ if we impose $v_{n+1} = 0$.\n\nWe seek a non-trivial solution to the recurrence $b v_{i+1} + (a-\\lambda) v_i + b v_{i-1} = 0$ subject to $v_0 = 0$ and $v_{n+1} = 0$. Let us propose a solution of the form $v_j = \\sin(j\\theta)$ for some parameter $\\theta$. The condition $v_0 = \\sin(0\\theta) = 0$ is automatically satisfied. Substituting this form into the recurrence relation gives:\n$$ b \\sin((i+1)\\theta) + (a-\\lambda)\\sin(i\\theta) + b\\sin((i-1)\\theta) = 0 $$\nUsing the trigonometric identity $\\sin(A\\pm B) = \\sin(A)\\cos(B) \\pm \\cos(A)\\sin(B)$, we get:\n$$ b(\\sin(i\\theta)\\cos(\\theta) + \\cos(i\\theta)\\sin(\\theta)) + (a-\\lambda)\\sin(i\\theta) + b(\\sin(i\\theta)\\cos(\\theta) - \\cos(i\\theta)\\sin(\\theta)) = 0 $$\n$$ 2b\\sin(i\\theta)\\cos(\\theta) + (a-\\lambda)\\sin(i\\theta) = 0 $$\n$$ (2b\\cos(\\theta) + a - \\lambda) \\sin(i\\theta) = 0 $$\nFor this equation to hold for all $i=1, \\dots, n$ for a non-trivial eigenvector (i.e., not all $\\sin(i\\theta)$ are zero), the first factor must be zero:\n$$ 2b\\cos(\\theta) + a - \\lambda = 0 \\implies \\lambda = a + 2b\\cos(\\theta) $$\nThis expression gives the eigenvalue $\\lambda$ in terms of the parameter $\\theta$. Now we must use the second boundary condition, $v_{n+1}=0$, to determine the permissible values of $\\theta$.\n$$ v_{n+1} = \\sin((n+1)\\theta) = 0 $$\nThis implies $(n+1)\\theta = k\\pi$ for some integer $k$. Thus, $\\theta = \\frac{k\\pi}{n+1}$.\nFor $k=1, 2, \\dots, n$, we obtain $n$ distinct sets of eigenvectors. For $k=0$ or $k=n+1$, $\\theta$ is $0$ or $\\pi$, leading to the trivial vector $v_j = 0$ for all $j$. Other integer values of $k$ yield eigenvectors that are linearly dependent on these $n$ vectors. We denote the distinct eigenvalues and eigenvectors by an index $k \\in \\{1, 2, \\dots, n\\}$.\nThe eigenvalues are:\n$$ \\lambda_k = a + 2b \\cos\\left(\\frac{k\\pi}{n+1}\\right), \\quad k = 1, 2, \\dots, n $$\nThe corresponding unnormalized eigenvectors $v^{(k)}$ have components:\n$$ v_j^{(k)} = \\sin\\left(\\frac{jk\\pi}{n+1}\\right), \\quad j = 1, 2, \\dots, n $$\nTo obtain the final expression, we normalize these eigenvectors to have a Euclidean norm of $1$. The squared norm is:\n$$ \\|v^{(k)}\\|_2^2 = \\sum_{j=1}^n \\sin^2\\left(\\frac{jk\\pi}{n+1}\\right) = \\sum_{j=1}^n \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2jk\\pi}{n+1}\\right)\\right) = \\frac{n}{2} - \\frac{1}{2}\\text{Re}\\left(\\sum_{j=1}^n \\exp\\left(i \\frac{2jk\\pi}{n+1}\\right)\\right) $$\nThe sum is a geometric series which evaluates to $-1$. Therefore, the squared norm is $\\frac{n}{2} - \\frac{1}{2}(-1) = \\frac{n+1}{2}$. The normalization constant is $\\sqrt{\\frac{2}{n+1}}$.\nThe components of the normalized eigenvector $v^{(k)}$ are:\n$$ v_j^{(k)} = \\sqrt{\\frac{2}{n+1}} \\sin\\left(\\frac{jk\\pi}{n+1}\\right), \\quad j = 1, 2, \\dots, n $$\n\n(ii) Eigenvector Condition Number\n\nFor a symmetric matrix, a first-order perturbation analysis (such as the Davis-Kahan theorem) shows that the sensitivity of an eigenvector $v_k$ to a normwise perturbation in the matrix is inversely proportional to the minimum separation between its eigenvalue $\\lambda_k$ and the rest of the spectrum. We define the normwise first-order eigenvector condition number $\\kappa_k$ as:\n$$ \\kappa_k = \\frac{1}{\\min_{m \\neq k} |\\lambda_k - \\lambda_m|} $$\nThe term \"nearly repeated adjacent pair at indices $j$ and $j+1$\" implies that the gap $|\\lambda_j - \\lambda_{j+1}|$ is small and is the dominant term in the conditioning of both $v_j$ and $v_{j+1}$. The gap between adjacent eigenvalues is:\n$$ |\\lambda_{k+1} - \\lambda_k| = \\left|\\left(a + 2b\\cos\\left(\\frac{(k+1)\\pi}{n+1}\\right)\\right) - \\left(a + 2b\\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right)\\right| = 2|b| \\left|\\cos\\left(\\frac{(k+1)\\pi}{n+1}\\right) - \\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right| $$\nUsing the identity $\\cos A - \\cos B = -2\\sin\\left(\\frac{A+B}{2}\\right)\\sin\\left(\\frac{A-B}{2}\\right)$, this becomes:\n$$ |\\lambda_{k+1} - \\lambda_k| = 4|b| \\sin\\left(\\frac{(2k+1)\\pi}{2(n+1)}\\right) \\sin\\left(\\frac{\\pi}{2(n+1)}\\right) $$\nFor large $n$ with fixed $k$, both sine arguments are small. Using the approximation $\\sin(x) \\approx x$ for small $x$:\n$$ |\\lambda_{k+1} - \\lambda_k| \\approx 4|b| \\left(\\frac{(2k+1)\\pi}{2(n+1)}\\right) \\left(\\frac{\\pi}{2(n+1)}\\right) = |b| \\frac{\\pi^2(2k+1)}{(n+1)^2} $$\nThe condition number $\\kappa_j$ depends on the minimal gap around $\\lambda_j$. The problem's context implies this minimal gap is with an adjacent eigenvalue, and the focus on the pair $(j, j+1)$ suggests we examine the gap $|\\lambda_j - \\lambda_{j+1}|$. If we take this as the minimal gap for $\\lambda_j$, then for large $n$ and fixed $j$:\n$$ \\kappa_j = \\frac{1}{|\\lambda_j - \\lambda_{j+1}|} \\approx \\frac{(n+1)^2}{|b|\\pi^2(2j+1)} $$\nThis condition number grows quadratically with $n$, highlighting the sensitivity of eigenvectors associated with eigenvalues at the spectral edges, where gaps are smallest.\n\n(iii) Optimal Shift for Shift-Invert Transformation\n\nThe shift-invert transformation maps $T(a,b)$ to $(T(a,b)-\\sigma I)^{-1}$. The eigenvectors are preserved, and the eigenvalues $\\lambda_k$ are transformed to $\\mu_k = 1/(\\lambda_k - \\sigma)$. To improve the numerical stability of computing the eigenvectors for the nearly repeated pair $(\\lambda_j, \\lambda_{j+1})$, we want to maximize their separation in the transformed spectrum, which is $|\\mu_j - \\mu_{j+1}|$.\nThe problem, however, poses the equivalent and more direct minimax problem: find the real shift $\\sigma$ that maximizes the minimum distance from $\\sigma$ to the pair of eigenvalues $\\lambda_j$ and $\\lambda_{j+1}$. Let this objective function be $f(\\sigma)$:\n$$ f(\\sigma) = \\min\\{|\\lambda_j - \\sigma|, |\\lambda_{j+1} - \\sigma|\\} $$\nLet's assume, without loss of generality, that $\\lambda_j \\le \\lambda_{j+1}$. The function $f(\\sigma)$ is the lower envelope of two V-shaped functions $|x-\\lambda_j|$ and $|x-\\lambda_{j+1}|$. The maximum value of this lower envelope occurs at the point where the two distances are equal:\n$$ |\\lambda_j - \\sigma| = |\\lambda_{j+1} - \\sigma| $$\nIf $\\sigma$ is between $\\lambda_j$ and $\\lambda_{j+1}$, this equality becomes $\\sigma - \\lambda_j = \\lambda_{j+1} - \\sigma$. Solving for $\\sigma$ yields the unique optimal shift $\\sigma^*$:\n$$ 2\\sigma = \\lambda_j + \\lambda_{j+1} \\implies \\sigma^* = \\frac{\\lambda_j + \\lambda_{j+1}}{2} $$\nThe optimal shift is the midpoint of the two eigenvalues. This choice also maximizes the separation $|\\mu_j - \\mu_{j+1}|$, since $|\\mu_j - \\mu_{j+1}| = \\frac{|\\lambda_{j+1}-\\lambda_j|}{|(\\lambda_j-\\sigma)(\\lambda_{j+1}-\\sigma)|}$ and $\\sigma^*$ minimizes the magnitude of the denominator. It also ensures that the conditioning of the matrix inversion with respect to this pair is balanced.\n\nTo find the explicit expression for $\\sigma^*$, we substitute the formulas for $\\lambda_j$ and $\\lambda_{j+1}$ from part (i):\n$$ \\sigma^* = \\frac{1}{2} \\left[ \\left(a + 2b \\cos\\left(\\frac{j\\pi}{n+1}\\right)\\right) + \\left(a + 2b \\cos\\left(\\frac{(j+1)\\pi}{n+1}\\right)\\right) \\right] $$\n$$ \\sigma^* = a + b \\left( \\cos\\left(\\frac{j\\pi}{n+1}\\right) + \\cos\\left(\\frac{(j+1)\\pi}{n+1}\\right) \\right) $$\nUsing the sum-to-product trigonometric identity $\\cos A + \\cos B = 2\\cos\\left(\\frac{A+B}{2}\\right)\\cos\\left(\\frac{A-B}{2}\\right)$, with $A = \\frac{(j+1)\\pi}{n+1}$ and $B = \\frac{j\\pi}{n+1}$, we get:\n$$ \\frac{A+B}{2} = \\frac{(2j+1)\\pi}{2(n+1)} \\quad \\text{and} \\quad \\frac{A-B}{2} = \\frac{\\pi}{2(n+1)} $$\nSubstituting this into the expression for $\\sigma^*$ yields the final closed-form solution:\n$$ \\sigma^* = a + 2b \\cos\\left(\\frac{(2j+1)\\pi}{2(n+1)}\\right) \\cos\\left(\\frac{\\pi}{2(n+1)}\\right) $$",
            "answer": "$$\\boxed{a + 2b \\cos\\left(\\frac{(2j+1)\\pi}{2(n+1)}\\right) \\cos\\left(\\frac{\\pi}{2(n+1)}\\right)}$$"
        },
        {
            "introduction": "Our final practice delves into the sophisticated world of modern, high-performance eigenvalue solvers like the Multiple Relatively Robust Representations (MRRR) algorithm. This method's success hinges on the subtle concept of \"relative robustness\" to compute eigenvectors with high relative accuracy, especially when eigenvalues are tightly clustered. This problem  challenges you to diagnose a failure of robustness caused by an ill-chosen matrix representation and then to derive a perturbation that restores stability. This exercise highlights the delicate engineering required to ensure the reliability of state-of-the-art numerical methods.",
            "id": "3600022",
            "problem": "Consider a symmetric tridiagonal matrix $T \\in \\mathbb{R}^{3 \\times 3}$ with diagonal entries all equal to $1$ and off-diagonal entries all equal to $\\beta > 0$, that is,\n$$\nT = \\begin{pmatrix}\n1 & \\beta & 0 \\\\\n\\beta & 1 & \\beta \\\\\n0 & \\beta & 1\n\\end{pmatrix}.\n$$\nThis matrix has a tightly clustered spectrum around $1$ when $\\beta$ is small, with one eigenvalue equal to $1$ and two others symmetrically placed around $1$. In the Multiple Relatively Robust Representations (MRRR) algorithm, one constructs a representation by shifting $T$ and computing a unit lower bidiagonal factorization $T - \\sigma I = L D L^{\\mathsf{T}}$, where $L$ is unit lower bidiagonal and $D$ is diagonal. Relative robustness for a cluster can fail if the conditional element growth in $L$ becomes large due to small pivots in $D$.\n\n(a) Using only fundamental definitions of the $L D L^{\\mathsf{T}}$ factorization for symmetric tridiagonal matrices, analyze the representation with the shift $\\sigma = 1$ and show that the representation $T - \\sigma I = L D L^{\\mathsf{T}}$ fails to be relatively robust because the first pivot in $D$ vanishes, leading to unbounded elements in $L$.\n\n(b) To restore robustness, consider a perturbed shift $\\sigma = 1 + \\alpha$ with $\\alpha \\in \\mathbb{R} \\setminus \\{0\\}$. Define the conditional element growth measure as $g(\\sigma) = \\max\\{ | \\ell_1 |, | \\ell_2 | \\}$, where $\\ell_1$ and $\\ell_2$ are the subdiagonal entries of $L$ in the $L D L^{\\mathsf{T}}$ factorization of $T - \\sigma I$. Impose the robustness criterion $g(\\sigma) \\leq \\rho$ for a fixed constant $\\rho$ satisfying $0 < \\rho < 1$.\n\nDerive, from first principles, a closed-form expression for the minimal magnitude of the perturbation $|\\alpha|$ in terms of $\\beta$ and $\\rho$ that guarantees $g(\\sigma) \\leq \\rho$. Your final answer must be a single closed-form analytic expression for the minimal $|\\alpha|$. No numerical evaluation is required and no rounding is necessary.",
            "solution": "The problem is analyzed in two parts. First, we validate the claim of robustness failure for a specific shift. Second, we derive the minimal perturbation to restore robustness under a given criterion.\n\n(a) Analysis of the shift $\\sigma = 1$.\n\nLet the given symmetric tridiagonal matrix be $T = \\begin{pmatrix} 1 & \\beta & 0 \\\\ \\beta & 1 & \\beta \\\\ 0 & \\beta & 1 \\end{pmatrix}$. For the shift $\\sigma = 1$, we consider the matrix $A = T - \\sigma I = T - 1 \\cdot I$.\n$$\nA = \\begin{pmatrix} 1 & \\beta & 0 \\\\ \\beta & 1 & \\beta \\\\ 0 & \\beta & 1 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & \\beta & 0 \\\\ \\beta & 0 & \\beta \\\\ 0 & \\beta & 0 \\end{pmatrix}.\n$$\nWe seek to compute the $L D L^{\\mathsf{T}}$ factorization of $A$, where $L$ is a unit lower bidiagonal matrix and $D$ is a diagonal matrix.\nFor a $3 \\times 3$ matrix, let $L = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\ell_1 & 1 & 0 \\\\ 0 & \\ell_2 & 1 \\end{pmatrix}$ and $D = \\text{diag}(d_1, d_2, d_3)$.\nThe factorization $A = L D L^{\\mathsf{T}}$ is given by:\n$$\n\\begin{pmatrix} 0 & \\beta & 0 \\\\ \\beta & 0 & \\beta \\\\ 0 & \\beta & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\ell_1 & 1 & 0 \\\\ 0 & \\ell_2 & 1 \\end{pmatrix} \\begin{pmatrix} d_1 & 0 & 0 \\\\ 0 & d_2 & 0 \\\\ 0 & 0 & d_3 \\end{pmatrix} \\begin{pmatrix} 1 & \\ell_1 & 0 \\\\ 0 & 1 & \\ell_2 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} d_1 & d_1 \\ell_1 & 0 \\\\ d_1 \\ell_1 & d_1 \\ell_1^2 + d_2 & d_2 \\ell_2 \\\\ 0 & d_2 \\ell_2 & d_2 \\ell_2^2 + d_3 \\end{pmatrix}.\n$$\nThe fundamental algorithm for this factorization proceeds by equating matrix elements sequentially.\nComparing the $(1,1)$ elements: $d_1 = 0$. The first pivot in $D$ is zero.\nComparing the $(2,1)$ elements: $d_1 \\ell_1 = \\beta$.\nSubstituting $d_1 = 0$, this becomes $0 \\cdot \\ell_1 = \\beta$. Since the problem states $\\beta > 0$, this equation has no solution for $\\ell_1$.\nAlternatively, the computation of $\\ell_1$ in the factorization algorithm is given by $\\ell_1 = A_{21} / d_1 = \\beta / d_1$. With $d_1=0$ and $\\beta>0$, the element $\\ell_1$ is undefined, corresponding to an infinite element growth. This violates the condition of relative robustness. The factorization fails because the shift $\\sigma=1$ is an eigenvalue of $T$, making $T-\\sigma I$ a singular matrix with a zero in the leading principal minor.\n\n(b) Analysis of the perturbed shift $\\sigma = 1 + \\alpha$.\n\nWith the perturbed shift $\\sigma = 1 + \\alpha$ where $\\alpha \\in \\mathbb{R} \\setminus \\{0\\}$, the shifted matrix is $A' = T - \\sigma I = T - (1 + \\alpha)I$.\n$$\nA' = \\begin{pmatrix} 1 - (1+\\alpha) & \\beta & 0 \\\\ \\beta & 1 - (1+\\alpha) & \\beta \\\\ 0 & \\beta & 1 - (1+\\alpha) \\end{pmatrix} = \\begin{pmatrix} -\\alpha & \\beta & 0 \\\\ \\beta & -\\alpha & \\beta \\\\ 0 & \\beta & -\\alpha \\end{pmatrix}.\n$$\nWe perform the $L D L^{\\mathsf{T}}$ factorization for $A'$. The diagonal entries of $A'$ are all $-\\alpha$ and the off-diagonal entries are all $\\beta$. The recursive formulas for the elements of $L$ and $D$ are derived by equating elements:\n1.  From the $(1,1)$ entry: $d_1 = -\\alpha$. Since $\\alpha \\neq 0$, $d_1 \\neq 0$.\n2.  From the $(2,1)$ entry, $d_1 \\ell_1 = \\beta$, so $\\ell_1 = \\frac{\\beta}{d_1} = -\\frac{\\beta}{\\alpha}$.\n3.  From the $(2,2)$ entry, $d_1 \\ell_1^2 + d_2 = -\\alpha$, so $d_2 = -\\alpha - d_1 \\ell_1^2 = -\\alpha - (-\\alpha) \\left(-\\frac{\\beta}{\\alpha}\\right)^2 = -\\alpha + \\frac{\\beta^2}{\\alpha} = \\frac{\\beta^2 - \\alpha^2}{\\alpha}$.\n4.  From the $(3,2)$ entry, $d_2 \\ell_2 = \\beta$, so $\\ell_2 = \\frac{\\beta}{d_2} = \\frac{\\beta}{(\\beta^2-\\alpha^2)/\\alpha} = \\frac{\\alpha\\beta}{\\beta^2 - \\alpha^2}$.\n\nThe conditional element growth measure is $g(\\sigma) = \\max\\{|\\ell_1|, |\\ell_2|\\}$. The robustness criterion is $g(\\sigma) \\leq \\rho$.\n$$\n\\max\\left\\{ \\left|-\\frac{\\beta}{\\alpha}\\right|, \\left|\\frac{\\alpha\\beta}{\\beta^2 - \\alpha^2}\\right| \\right\\} \\leq \\rho.\n$$\nThis is equivalent to satisfying two separate inequalities:\n(i) $|\\ell_1| = \\frac{\\beta}{|\\alpha|} \\leq \\rho$.\n(ii) $|\\ell_2| = \\frac{|\\alpha|\\beta}{|\\beta^2 - \\alpha^2|} \\leq \\rho$.\n\nLet's analyze the behavior of $g$ as a function of $|\\alpha|$. In the range $0 < |\\alpha| < \\beta$, the minimum value of $g$ occurs where $|\\ell_1|=|\\ell_2|$, which is at $|\\alpha| = \\beta/\\sqrt{2}$. The value is $g(\\sigma) = \\sqrt{2}$. Since the problem imposes $0 < \\rho < 1$, we have $\\rho  \\sqrt{2}$. This means the condition $g(\\sigma) \\leq \\rho$ cannot be satisfied for any $|\\alpha| \\in (0, \\beta)$, as the minimum value of $g$ in this interval is $\\sqrt{2}$. We must therefore seek a solution where $|\\alpha| > \\beta$.\n\nFor $|\\alpha| > \\beta$, let $x = |\\alpha|$. Then $x^2 - \\beta^2 > 0$. The two inequalities become:\n(i) $\\frac{\\beta}{x} \\leq \\rho \\implies x \\geq \\frac{\\beta}{\\rho}$.\n(ii) $\\frac{x\\beta}{x^2 - \\beta^2} \\leq \\rho$. Since all quantities are positive, we can rearrange:\n$$\nx\\beta \\leq \\rho(x^2 - \\beta^2) \\implies \\rho x^2 - \\beta x - \\rho \\beta^2 \\geq 0.\n$$\nThis is a quadratic inequality in $x$. The roots of the quadratic polynomial $f(x) = \\rho x^2 - \\beta x - \\rho \\beta^2$ are given by the quadratic formula:\n$$\nx = \\frac{-(-\\beta) \\pm \\sqrt{(-\\beta)^2 - 4(\\rho)(-\\rho\\beta^2)}}{2\\rho} = \\frac{\\beta \\pm \\sqrt{\\beta^2 + 4\\rho^2\\beta^2}}{2\\rho} = \\frac{\\beta \\pm \\beta\\sqrt{1+4\\rho^2}}{2\\rho}.\n$$\nSince $x = |\\alpha|  0$, we consider the positive root, $x_+ = \\frac{\\beta(1 + \\sqrt{1+4\\rho^2})}{2\\rho}$.\nThe quadratic $f(x)$ represents an upward-opening parabola, so $f(x) \\geq 0$ for $x$ outside the interval between the roots. As $x0$, this means $x \\geq x_+$. So, condition (ii) is satisfied if $x \\geq \\frac{\\beta(1 + \\sqrt{1+4\\rho^2})}{2\\rho}$.\n\nTo satisfy the overall robustness criterion, $|\\alpha|$ must satisfy both conditions:\n$$\n|\\alpha| \\geq \\frac{\\beta}{\\rho} \\quad \\text{and} \\quad |\\alpha| \\geq \\frac{\\beta(1 + \\sqrt{1+4\\rho^2})}{2\\rho}.\n$$\nThis is equivalent to $|\\alpha|$ being greater than or equal to the maximum of the two lower bounds:\n$$\n|\\alpha| \\geq \\max\\left(\\frac{\\beta}{\\rho}, \\frac{\\beta(1 + \\sqrt{1+4\\rho^2})}{2\\rho}\\right).\n$$\nWe compare the two quantities. Since $\\beta  0$ and $\\rho  0$, we only need to compare $\\frac{1}{\\rho}$ and $\\frac{1 + \\sqrt{1+4\\rho^2}}{2\\rho}$, which is equivalent to comparing $2$ and $1 + \\sqrt{1+4\\rho^2}$. This simplifies to comparing $1$ and $\\sqrt{1+4\\rho^2}$. Since $\\rho  0$, $4\\rho^2  0$, which implies $1+4\\rho^2  1$, and therefore $\\sqrt{1+4\\rho^2}  1$.\nThus, we have $\\frac{1 + \\sqrt{1+4\\rho^2}}{2\\rho}  \\frac{1}{\\rho}$. The maximum of the two bounds is the larger one.\nThe condition for $|\\alpha|$ simplifies to:\n$$\n|\\alpha| \\geq \\frac{\\beta(1 + \\sqrt{1+4\\rho^2})}{2\\rho}.\n$$\nThe problem asks for the minimal magnitude of the perturbation $|\\alpha|$ that guarantees robustness. This corresponds to the smallest value of $|\\alpha|$ satisfying the aformentioned inequality, which is the lower bound of the interval.",
            "answer": "$$\n\\boxed{\\frac{\\beta\\left(1 + \\sqrt{1 + 4\\rho^2}\\right)}{2\\rho}}\n$$"
        }
    ]
}