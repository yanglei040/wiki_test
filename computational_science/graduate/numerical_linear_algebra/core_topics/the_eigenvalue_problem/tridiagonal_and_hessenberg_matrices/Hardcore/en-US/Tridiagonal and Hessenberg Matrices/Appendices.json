{
    "hands_on_practices": [
        {
            "introduction": "While the eigenvalues of general matrices must be found using iterative numerical methods, certain highly structured matrices admit exact, closed-form solutions for their eigenpairs. This exercise guides you through the classic derivation for a symmetric tridiagonal Toeplitz matrix, a structure that appears frequently in discretizations of differential equations. By converting the eigenvalue problem into a second-order linear difference equation, you will develop a foundational understanding of the spectral properties that make these matrices both analytically tractable and computationally important. ",
            "id": "3600025",
            "problem": "Consider the $n \\times n$ Toeplitz tridiagonal matrix $T_n(\\alpha,\\beta)$ whose entries satisfy $(T_n)_{i,i}=\\alpha$ and $(T_n)_{i,i+1}=(T_n)_{i+1,i}=\\beta$ for $i=1,\\dots,n-1$, with all other entries equal to $0$. This matrix is a special case of an upper Hessenberg matrix (a matrix with zero entries below the first subdiagonal), and it is symmetric when $\\beta$ is real. Starting from the eigenvalue problem $T_n(\\alpha,\\beta)\\,v=\\lambda\\,v$ and from core definitions in linear algebra, derive the complete set of eigenpairs $(\\lambda_k,v^{(k)})$ for $k=1,\\dots,n$ by solving the resulting second-order linear finite-difference recurrence together with boundary constraints implied by the first and last rows. Then, specialize your derivation to the case $n=7$, $\\alpha=3$, and $\\beta=-1$, and determine the largest eigenvalue in closed form.\n\nProvide your final answer as a single closed-form analytic expression. No rounding is required.",
            "solution": "The problem requires finding the eigenvalues and eigenvectors of an $n \\times n$ Toeplitz tridiagonal matrix $T_n(\\alpha, \\beta)$ and then finding the largest eigenvalue for a specific case.\n\nFirst, we validate the problem statement. The givens are:\n- An $n \\times n$ matrix $T_n(\\alpha, \\beta)$ with entries $(T_n)_{i,i}=\\alpha$, $(T_n)_{i,i+1}=(T_n)_{i+1,i}=\\beta$, and all other entries being $0$.\n- The eigenvalue problem to be solved is $T_n(\\alpha,\\beta)\\,v=\\lambda\\,v$.\n- The method specified is to solve the resulting second-order linear finite-difference recurrence.\n- The specific case is $n=7$, $\\alpha=3$, and $\\beta=-1$.\n\nThe problem is scientifically grounded in numerical linear algebra, well-posed, and objective. It is a standard, solvable problem with a unique set of eigenvalues. The problem is valid.\n\nWe begin by writing the eigenvalue equation $T_n(\\alpha, \\beta) v = \\lambda v$ in component form. For a given eigenvector $v$ with components $v_i$, the $i$-th equation is:\n$$ (T_n v)_i = \\sum_{j=1}^n (T_n)_{i,j} v_j = \\lambda v_i $$\nBased on the definition of $T_n(\\alpha, \\beta)$, this system of equations can be written as:\nFor $i=1$: $\\alpha v_1 + \\beta v_2 = \\lambda v_1$\nFor $2 \\le i \\le n-1$: $\\beta v_{i-1} + \\alpha v_i + \\beta v_{i+1} = \\lambda v_i$\nFor $i=n$: $\\beta v_{n-1} + \\alpha v_n = \\lambda v_n$\n\nWe can unify these equations by defining fictitious components $v_0 = 0$ and $v_{n+1} = 0$.\nThe equation for $i=1$ can be written as $\\beta v_0 + (\\alpha - \\lambda)v_1 + \\beta v_2 = 0$, which is consistent with the first row of the matrix equation.\nThe equation for $i=n$ can be written as $\\beta v_{n-1} + (\\alpha - \\lambda)v_n + \\beta v_{n+1} = 0$, which is consistent with the last row.\nTherefore, the entire eigenvalue problem is equivalent to solving the second-order homogeneous linear finite-difference equation:\n$$ \\beta v_{i-1} + (\\alpha - \\lambda)v_i + \\beta v_{i+1} = 0 \\quad \\text{for } i=1, 2, \\dots, n $$\nwith the boundary conditions $v_0 = 0$ and $v_{n+1} = 0$.\n\nLet's assume a solution of the form $v_i = z^i$ for some constant $z$. Substituting this into the recurrence relation gives the characteristic equation:\n$$ \\beta z^{i-1} + (\\alpha - \\lambda)z^i + \\beta z^{i+1} = 0 $$\nAssuming $z \\ne 0$, we can divide by $z^{i-1}$:\n$$ \\beta + (\\alpha - \\lambda)z + \\beta z^2 = 0 $$\nFrom this quadratic equation in $z$, we can express $\\lambda$ in terms of $z$:\n$$ \\alpha - \\lambda = -\\beta \\left(z + \\frac{1}{z}\\right) \\implies \\lambda = \\alpha + \\beta \\left(z + \\frac{1}{z}\\right) $$\nSince the matrix $T_n(\\alpha, \\beta)$ is symmetric for real $\\beta$, its eigenvalues $\\lambda$ must be real. If $z$ were a complex number $z = r e^{i\\theta}$, then $\\lambda = \\alpha + \\beta(r e^{i\\theta} + \\frac{1}{r} e^{-i\\theta}) = \\alpha + \\beta \\left[ \\left(r+\\frac{1}{r}\\right)\\cos\\theta + i\\left(r-\\frac{1}{r}\\right)\\sin\\theta \\right]$. For $\\lambda$ to be real, we must have $(r-\\frac{1}{r})\\sin\\theta = 0$. This implies either $\\sin\\theta=0$ (so $z$ is real) or $r-\\frac{1}{r}=0$, i.e., $r=1$. The case where $z$ is real leads to $|\\alpha-\\lambda| \\ge 2|\\beta|$, while the case $|z|=1$ corresponds to $|\\alpha-\\lambda| \\le 2|\\beta|$. The boundary conditions will select the appropriate solutions. Let us proceed with the case $|z|=1$, so $z=e^{i\\theta}$.\nThe two roots of the characteristic equation are $z_1 = e^{i\\theta}$ and $z_2 = e^{-i\\theta}$. The general solution for $v_i$ is a linear combination of these fundamental solutions:\n$$ v_i = A (e^{i\\theta})^i + B (e^{-i\\theta})^i = A e^{i i \\theta} + B e^{-i i \\theta} $$\nThis can also be expressed as $v_i = C \\cos(i\\theta) + D \\sin(i\\theta)$.\n\nNow we apply the boundary conditions.\nFirst, $v_0 = 0$:\n$$ v_0 = C \\cos(0) + D \\sin(0) = C \\cdot 1 + D \\cdot 0 = C $$\nThus, $C=0$, and the solution simplifies to $v_i = D \\sin(i\\theta)$.\nSecond, $v_{n+1} = 0$:\n$$ v_{n+1} = D \\sin((n+1)\\theta) = 0 $$\nFor a non-trivial eigenvector, we must have $D \\ne 0$. This implies:\n$$ \\sin((n+1)\\theta) = 0 \\implies (n+1)\\theta = k\\pi $$\nfor some integer $k$. This gives the quantized values for $\\theta$:\n$$ \\theta_k = \\frac{k\\pi}{n+1} $$\nThe values $k=1, 2, \\dots, n$ yield distinct, non-trivial eigenvectors. For $k=0$ or $k=n+1$, $\\sin(i\\theta_k)=0$ for all $i$, resulting in the trivial zero vector.\n\nThe eigenvalues $\\lambda_k$ are found by substituting $z = e^{i\\theta_k}$ into the expression for $\\lambda$:\n$$ \\lambda_k = \\alpha + 2\\beta\\cos(\\theta_k) $$\nSubstituting the expression for $\\theta_k$:\n$$ \\lambda_k = \\alpha + 2\\beta\\cos\\left(\\frac{k\\pi}{n+1}\\right) \\quad \\text{for } k=1, 2, \\dots, n $$\nThe corresponding eigenvectors $v^{(k)}$ have components (up to a normalization constant $D$):\n$$ v_i^{(k)} = \\sin(i\\theta_k) = \\sin\\left(\\frac{ik\\pi}{n+1}\\right) \\quad \\text{for } i=1, 2, \\dots, n $$\nThis completes the derivation of the general eigenpairs.\n\nNext, we specialize to the case $n=7$, $\\alpha=3$, and $\\beta=-1$.\nThe eigenvalues are given by:\n$$ \\lambda_k = 3 + 2(-1)\\cos\\left(\\frac{k\\pi}{7+1}\\right) = 3 - 2\\cos\\left(\\frac{k\\pi}{8}\\right) \\quad \\text{for } k=1, 2, \\dots, 7 $$\nWe want to find the largest eigenvalue. This requires maximizing the expression for $\\lambda_k$ with respect to $k \\in \\{1, 2, \\dots, 7\\}$. To maximize $\\lambda_k = 3 - 2\\cos\\left(\\frac{k\\pi}{8}\\right)$, we must minimize the term $2\\cos\\left(\\frac{k\\pi}{8}\\right)$. Since $2 > 0$, this is equivalent to minimizing $\\cos\\left(\\frac{k\\pi}{8}\\right)$.\n\nThe argument of the cosine function, $\\frac{k\\pi}{8}$, lies in the interval $[\\frac{\\pi}{8}, \\frac{7\\pi}{8}]$. The cosine function is strictly decreasing on the interval $[0, \\pi]$. Since our interval of interest is a subset of $[0, \\pi]$, $\\cos(x)$ is minimized when its argument $x$ is maximized. The argument $\\frac{k\\pi}{8}$ is maximized when $k$ is maximized, i.e., at $k=7$.\n\nTherefore, the largest eigenvalue is $\\lambda_7$:\n$$ \\lambda_{\\max} = \\lambda_7 = 3 - 2\\cos\\left(\\frac{7\\pi}{8}\\right) $$\nTo express this in a more explicit closed form, we use the trigonometric identity $\\cos(\\pi - x) = -\\cos(x)$.\n$$ \\cos\\left(\\frac{7\\pi}{8}\\right) = \\cos\\left(\\pi - \\frac{\\pi}{8}\\right) = -\\cos\\left(\\frac{\\pi}{8}\\right) $$\nSubstituting this back into the expression for $\\lambda_7$:\n$$ \\lambda_{\\max} = 3 - 2\\left(-\\cos\\left(\\frac{\\pi}{8}\\right)\\right) = 3 + 2\\cos\\left(\\frac{\\pi}{8}\\right) $$\nFinally, we use the half-angle identity for cosine, $\\cos\\left(\\frac{x}{2}\\right) = \\sqrt{\\frac{1+\\cos(x)}{2}}$, for $x/2$ in the first quadrant. Here, we set $x=\\frac{\\pi}{4}$:\n$$ \\cos\\left(\\frac{\\pi}{8}\\right) = \\sqrt{\\frac{1+\\cos(\\pi/4)}{2}} = \\sqrt{\\frac{1+\\frac{\\sqrt{2}}{2}}{2}} = \\sqrt{\\frac{2+\\sqrt{2}}{4}} = \\frac{\\sqrt{2+\\sqrt{2}}}{2} $$\nSubstituting this into our expression for the largest eigenvalue gives the final answer:\n$$ \\lambda_{\\max} = 3 + 2\\left(\\frac{\\sqrt{2+\\sqrt{2}}}{2}\\right) = 3 + \\sqrt{2+\\sqrt{2}} $$\nThis is the largest eigenvalue in closed-form.",
            "answer": "$$\\boxed{3+\\sqrt{2+\\sqrt{2}}}$$"
        },
        {
            "introduction": "Upper Hessenberg matrices are central to numerical linear algebra, serving as a crucial intermediate form in the workhorse QR algorithm for eigenvalue computation. While their structure offers significant computational savings, it does not grant immunity to numerical instability. This practice problem provides a concrete demonstration of how a standard factorization method, Gaussian elimination without pivoting, can abruptly fail when applied to a simple Hessenberg matrix. Working through this example will highlight why numerically robust algorithms, which often rely on orthogonal transformations, are essential in practical scientific computing. ",
            "id": "3600001",
            "problem": "Consider an $n \\times n$ upper Hessenberg matrix $A$, defined by the property that $a_{ij} = 0$ whenever $i > j + 1$. Gaussian elimination (GE) without pivoting proceeds by successively selecting the diagonal element $a_{kk}^{(k-1)}$ of the partially reduced matrix $A^{(k-1)}$ as the $k$-th pivot, eliminating entries below it in column $k$ using a unit lower triangular multiplier, and updating trailing submatrices by rank-one transformations. A breakdown occurs at step $k$ if the pivot $a_{kk}^{(k-1)}$ is exactly zero, causing division by zero in the formation of multipliers.\n\nUse these definitions to analyze the following explicit counterexample. Let\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 & 0 \\\\\n1 & 2 & 5 & 6 \\\\\n0 & 4 & 1 & 7 \\\\\n0 & 0 & 8 & 1\n\\end{pmatrix},\n$$\nwhich is upper Hessenberg since $a_{31} = 0$, $a_{41} = 0$, and $a_{42} = 0$, while entries on and above the first subdiagonal are unrestricted.\n\nApply Gaussian elimination without pivoting to $A$, beginning with the pivot $a_{11}$ at step $k=1$, and proceed in the standard way to form the partially reduced matrices $A^{(1)}$, $A^{(2)}$, and so on, provided the pivot is nonzero at each step. Determine the earliest step index $k$ at which Gaussian elimination without pivoting breaks down due to a zero pivot for this matrix. Your final answer must be the single integer value of $k$.",
            "solution": "The problem asks for the earliest step $k$ at which Gaussian elimination (GE) without pivoting breaks down for a given $4 \\times 4$ upper Hessenberg matrix $A$. A breakdown occurs at step $k$ if the pivot element, which is the diagonal element $a_{kk}^{(k-1)}$ of the matrix $A^{(k-1)}$ obtained after step $k-1$, is zero. The initial matrix is denoted as $A^{(0)}$.\n\nThe given matrix is:\n$$\nA \\;=\\; A^{(0)} \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 & 0 \\\\\n1 & 2 & 5 & 6 \\\\\n0 & 4 & 1 & 7 \\\\\n0 & 0 & 8 & 1\n\\end{pmatrix}\n$$\nThis is an upper Hessenberg matrix as all entries $a_{ij}$ with $i > j+1$ are zero. Specifically, $a_{31}=0$, $a_{41}=0$, and $a_{42}=0$.\n\nWe proceed with Gaussian elimination step-by-step.\n\n**Step $k=1$:**\nThe matrix at the beginning of this step is $A^{(0)}$. The first pivot is the element $a_{11}^{(0)}$, which is the $(1,1)$ entry of $A^{(0)}$.\n$$\n\\text{pivot}_1 = a_{11}^{(0)} = 1\n$$\nSince the pivot is non-zero, GE does not break down at this step. The goal of this step is to introduce zeros below the pivot in the first column. The only non-zero element below the pivot is $a_{21}^{(0)}=1$. The elements $a_{31}^{(0)}$ and $a_{41}^{(0)}$ are already zero.\n\nTo eliminate $a_{21}^{(0)}$, we compute the multiplier $m_{21}$:\n$$\nm_{21} = \\frac{a_{21}^{(0)}}{a_{11}^{(0)}} = \\frac{1}{1} = 1\n$$\nWe then update the second row ($R_2$) by subtracting $m_{21}$ times the first row ($R_1$) from it: $R_2 \\leftarrow R_2 - m_{21}R_1$.\n$$\nR_2 \\leftarrow R_2 - 1 \\cdot R_1\n$$\nThe new second row becomes:\n$$\n\\begin{pmatrix} 1 & 2 & 5 & 6 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} 1 & 2 & 3 & 0 \\end{pmatrix} = \\begin{pmatrix} 1-1 & 2-2 & 5-3 & 6-0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 2 & 6 \\end{pmatrix}\n$$\nThe rows $R_3$ and $R_4$ do not need to be modified in this step since $a_{31}^{(0)}=0$ and $a_{41}^{(0)}=0$, which means their corresponding multipliers $m_{31}$ and $m_{41}$ are zero.\n\nThe resulting matrix after step $k=1$ is $A^{(1)}$:\n$$\nA^{(1)} \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 & 0 \\\\\n0 & 0 & 2 & 6 \\\\\n0 & 4 & 1 & 7 \\\\\n0 & 0 & 8 & 1\n\\end{pmatrix}\n$$\n\n**Step $k=2$:**\nThe matrix at the beginning of this step is $A^{(1)}$. The second pivot is the element $a_{22}^{(1)}$, which is the $(2,2)$ entry of $A^{(1)}$.\n$$\n\\text{pivot}_2 = a_{22}^{(1)} = 0\n$$\nAccording to the definition provided, Gaussian elimination without pivoting breaks down at step $k$ if the $k$-th pivot is zero. In this case, at step $k=2$, the pivot is $a_{22}^{(1)} = 0$. The algorithm cannot proceed because the computation of the multipliers for eliminating elements below this pivot (e.g., $m_{32} = a_{32}^{(1)} / a_{22}^{(1)}$) would involve division by zero.\n\nTherefore, the earliest step at which Gaussian elimination without pivoting breaks down is $k=2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Building on the need for numerical stability, we now explore a concept from a state-of-the-art algorithm designed for the tridiagonal eigenproblem. The Multiple Relatively Robust Representations (MRRR) algorithm achieves remarkable speed and accuracy by computing eigenvalues through carefully chosen factorizations of shifted matrices. This exercise provides a deep dive into the core of MRRR, asking you to diagnose a scenario where a chosen representation fails to be \"relatively robust\" and then to engineer a precise perturbation to restore it. Successfully completing this problem will give you insight into the sophisticated design principles behind high-performance eigenvalue solvers. ",
            "id": "3600022",
            "problem": "Consider a symmetric tridiagonal matrix $T \\in \\mathbb{R}^{3 \\times 3}$ with diagonal entries all equal to $1$ and off-diagonal entries all equal to $\\beta > 0$, that is,\n$$\nT = \\begin{pmatrix}\n1 & \\beta & 0 \\\\\n\\beta & 1 & \\beta \\\\\n0 & \\beta & 1\n\\end{pmatrix}.\n$$\nThis matrix has a tightly clustered spectrum around $1$ when $\\beta$ is small, with one eigenvalue equal to $1$ and two others symmetrically placed around $1$. In the Multiple Relatively Robust Representations (MRRR) algorithm, one constructs a representation by shifting $T$ and computing a unit lower bidiagonal factorization $T - \\sigma I = L D L^{\\mathsf{T}}$, where $L$ is unit lower bidiagonal and $D$ is diagonal. Relative robustness for a cluster can fail if the conditional element growth in $L$ becomes large due to small pivots in $D$.\n\n(a) Using only fundamental definitions of the $L D L^{\\mathsf{T}}$ factorization for symmetric tridiagonal matrices, analyze the representation with the shift $\\sigma = 1$ and show that the representation $T - \\sigma I = L D L^{\\mathsf{T}}$ fails to be relatively robust because the first pivot in $D$ vanishes, leading to unbounded elements in $L$.\n\n(b) To restore robustness, consider a perturbed shift $\\sigma = 1 + \\alpha$ with $\\alpha \\in \\mathbb{R} \\setminus \\{0\\}$. Define the conditional element growth measure as $g(\\sigma) = \\max\\{ | \\ell_1 |, | \\ell_2 | \\}$, where $\\ell_1$ and $\\ell_2$ are the subdiagonal entries of $L$ in the $L D L^{\\mathsf{T}}$ factorization of $T - \\sigma I$. Impose the robustness criterion $g(\\sigma) \\leq \\rho$ for a fixed constant $\\rho$ satisfying $0 < \\rho < 1$.\n\nDerive, from first principles, a closed-form expression for the minimal magnitude of the perturbation $|\\alpha|$ in terms of $\\beta$ and $\\rho$ that guarantees $g(\\sigma) \\leq \\rho$. Your final answer must be a single closed-form analytic expression for the minimal $|\\alpha|$. No numerical evaluation is required and no rounding is necessary.",
            "solution": "The problem is analyzed in two parts. First, we validate the claim of robustness failure for a specific shift. Second, we derive the minimal perturbation to restore robustness under a given criterion.\n\n(a) Analysis of the shift $\\sigma = 1$.\n\nLet the given symmetric tridiagonal matrix be $T = \\begin{pmatrix} 1 & \\beta & 0 \\\\ \\beta & 1 & \\beta \\\\ 0 & \\beta & 1 \\end{pmatrix}$. For the shift $\\sigma = 1$, we consider the matrix $A = T - \\sigma I = T - 1 \\cdot I$.\n$$\nA = \\begin{pmatrix} 1 & \\beta & 0 \\\\ \\beta & 1 & \\beta \\\\ 0 & \\beta & 1 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & \\beta & 0 \\\\ \\beta & 0 & \\beta \\\\ 0 & \\beta & 0 \\end{pmatrix}.\n$$\nWe seek to compute the $L D L^{\\mathsf{T}}$ factorization of $A$, where $L$ is a unit lower bidiagonal matrix and $D$ is a diagonal matrix.\nFor a $3 \\times 3$ matrix, let $L = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\ell_1 & 1 & 0 \\\\ 0 & \\ell_2 & 1 \\end{pmatrix}$ and $D = \\text{diag}(d_1, d_2, d_3)$.\nThe factorization $A = L D L^{\\mathsf{T}}$ is given by:\n$$\n\\begin{pmatrix} 0 & \\beta & 0 \\\\ \\beta & 0 & \\beta \\\\ 0 & \\beta & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\ell_1 & 1 & 0 \\\\ 0 & \\ell_2 & 1 \\end{pmatrix} \\begin{pmatrix} d_1 & 0 & 0 \\\\ 0 & d_2 & 0 \\\\ 0 & 0 & d_3 \\end{pmatrix} \\begin{pmatrix} 1 & \\ell_1 & 0 \\\\ 0 & 1 & \\ell_2 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} d_1 & d_1 \\ell_1 & 0 \\\\ d_1 \\ell_1 & d_1 \\ell_1^2 + d_2 & d_2 \\ell_2 \\\\ 0 & d_2 \\ell_2 & d_2 \\ell_2^2 + d_3 \\end{pmatrix}.\n$$\nThe fundamental algorithm for this factorization proceeds by equating matrix elements sequentially.\nComparing the $(1,1)$ elements: $d_1 = 0$. The first pivot in $D$ is zero.\nComparing the $(2,1)$ elements: $d_1 \\ell_1 = \\beta$.\nSubstituting $d_1 = 0$, this becomes $0 \\cdot \\ell_1 = \\beta$. Since the problem states $\\beta > 0$, this equation has no solution for $\\ell_1$.\nAlternatively, the computation of $\\ell_1$ in the factorization algorithm is given by $\\ell_1 = A_{21} / d_1 = \\beta / d_1$. With $d_1=0$ and $\\beta>0$, the element $\\ell_1$ is undefined, corresponding to an infinite element growth. This violates the condition of relative robustness. The factorization fails because the shift $\\sigma=1$ is an eigenvalue of $T$, making $T-\\sigma I$ a singular matrix with a zero in the leading principal minor.\n\n(b) Analysis of the perturbed shift $\\sigma = 1 + \\alpha$.\n\nWith the perturbed shift $\\sigma = 1 + \\alpha$ where $\\alpha \\in \\mathbb{R} \\setminus \\{0\\}$, the shifted matrix is $A' = T - \\sigma I = T - (1 + \\alpha)I$.\n$$\nA' = \\begin{pmatrix} 1 - (1+\\alpha) & \\beta & 0 \\\\ \\beta & 1 - (1+\\alpha) & \\beta \\\\ 0 & \\beta & 1 - (1+\\alpha) \\end{pmatrix} = \\begin{pmatrix} -\\alpha & \\beta & 0 \\\\ \\beta & -\\alpha & \\beta \\\\ 0 & \\beta & -\\alpha \\end{pmatrix}.\n$$\nWe perform the $L D L^{\\mathsf{T}}$ factorization for $A'$. The diagonal entries of $A'$ are all $-\\alpha$ and the off-diagonal entries are all $\\beta$. The recursive formulas for the elements of $L$ and $D$ are derived by equating elements:\n1.  From the $(1,1)$ entry: $d_1 = -\\alpha$. Since $\\alpha \\neq 0$, $d_1 \\neq 0$.\n2.  From the $(2,1)$ entry, $d_1 \\ell_1 = \\beta$, so $\\ell_1 = \\frac{\\beta}{d_1} = -\\frac{\\beta}{\\alpha}$.\n3.  From the $(2,2)$ entry, $d_1 \\ell_1^2 + d_2 = -\\alpha$, so $d_2 = -\\alpha - d_1 \\ell_1^2 = -\\alpha - (-\\alpha) \\left(-\\frac{\\beta}{\\alpha}\\right)^2 = -\\alpha + \\frac{\\beta^2}{\\alpha} = \\frac{\\beta^2 - \\alpha^2}{\\alpha}$.\n4.  From the $(3,2)$ entry, $d_2 \\ell_2 = \\beta$, so $\\ell_2 = \\frac{\\beta}{d_2} = \\frac{\\beta}{(\\beta^2-\\alpha^2)/\\alpha} = \\frac{\\alpha\\beta}{\\beta^2 - \\alpha^2}$.\n\nThe conditional element growth measure is $g(\\sigma) = \\max\\{|\\ell_1|, |\\ell_2|\\}$. The robustness criterion is $g(\\sigma) \\leq \\rho$.\n$$\n\\max\\left\\{ \\left|-\\frac{\\beta}{\\alpha}\\right|, \\left|\\frac{\\alpha\\beta}{\\beta^2 - \\alpha^2}\\right| \\right\\} \\leq \\rho.\n$$\nThis is equivalent to satisfying two separate inequalities:\n(i) $|\\ell_1| = \\frac{\\beta}{|\\alpha|} \\leq \\rho$.\n(ii) $|\\ell_2| = \\frac{|\\alpha|\\beta}{|\\beta^2 - \\alpha^2|} \\leq \\rho$.\n\nLet's analyze the behavior of $g$ as a function of $|\\alpha|$. In the range $0  |\\alpha|  \\beta$, the minimum value of $g$ occurs where $|\\ell_1|=|\\ell_2|$, which is at $|\\alpha| = \\beta/\\sqrt{2}$. The value is $g(\\sigma) = \\sqrt{2}$. Since the problem imposes $0  \\rho  1$, we have $\\rho  \\sqrt{2}$. This means the condition $g(\\sigma) \\leq \\rho$ cannot be satisfied for any $|\\alpha| \\in (0, \\beta)$, as the minimum value of $g$ in this interval is $\\sqrt{2}$. We must therefore seek a solution where $|\\alpha| > \\beta$.\n\nFor $|\\alpha| > \\beta$, let $x = |\\alpha|$. Then $x^2 - \\beta^2 > 0$. The two inequalities become:\n(i) $\\frac{\\beta}{x} \\leq \\rho \\implies x \\geq \\frac{\\beta}{\\rho}$.\n(ii) $\\frac{x\\beta}{x^2 - \\beta^2} \\leq \\rho$. Since all quantities are positive, we can rearrange:\n$$\nx\\beta \\leq \\rho(x^2 - \\beta^2) \\implies \\rho x^2 - \\beta x - \\rho \\beta^2 \\geq 0.\n$$\nThis is a quadratic inequality in $x$. The roots of the quadratic polynomial $f(x) = \\rho x^2 - \\beta x - \\rho \\beta^2$ are given by the quadratic formula:\n$$\nx = \\frac{-(-\\beta) \\pm \\sqrt{(-\\beta)^2 - 4(\\rho)(-\\rho\\beta^2)}}{2\\rho} = \\frac{\\beta \\pm \\sqrt{\\beta^2 + 4\\rho^2\\beta^2}}{2\\rho} = \\frac{\\beta \\pm \\beta\\sqrt{1+4\\rho^2}}{2\\rho}.\n$$\nSince $x = |\\alpha| > 0$, we consider the positive root, $x_+ = \\frac{\\beta(1 + \\sqrt{1+4\\rho^2})}{2\\rho}$.\nThe quadratic $f(x)$ represents an upward-opening parabola, so $f(x) \\geq 0$ for $x$ outside the interval between the roots. As $x>0$, this means $x \\geq x_+$. So, condition (ii) is satisfied if $x \\geq \\frac{\\beta(1 + \\sqrt{1+4\\rho^2})}{2\\rho}$.\n\nTo satisfy the overall robustness criterion, $|\\alpha|$ must satisfy both conditions:\n$$\n|\\alpha| \\geq \\frac{\\beta}{\\rho} \\quad \\text{and} \\quad |\\alpha| \\geq \\frac{\\beta(1 + \\sqrt{1+4\\rho^2})}{2\\rho}.\n$$\nThis is equivalent to $|\\alpha|$ being greater than or equal to the maximum of the two lower bounds:\n$$\n|\\alpha| \\geq \\max\\left(\\frac{\\beta}{\\rho}, \\frac{\\beta(1 + \\sqrt{1+4\\rho^2})}{2\\rho}\\right).\n$$\nWe compare the two quantities. Since $\\beta > 0$ and $\\rho > 0$, we only need to compare $\\frac{1}{\\rho}$ and $\\frac{1 + \\sqrt{1+4\\rho^2}}{2\\rho}$, which is equivalent to comparing $2$ and $1 + \\sqrt{1+4\\rho^2}$. This simplifies to comparing $1$ and $\\sqrt{1+4\\rho^2}$. Since $\\rho > 0$, $4\\rho^2 > 0$, which implies $1+4\\rho^2 > 1$, and therefore $\\sqrt{1+4\\rho^2} > 1$.\nThus, we have $\\frac{1 + \\sqrt{1+4\\rho^2}}{2\\rho} > \\frac{1}{\\rho}$. The maximum of the two bounds is the larger one.\nThe condition for $|\\alpha|$ simplifies to:\n$$\n|\\alpha| \\geq \\frac{\\beta(1 + \\sqrt{1+4\\rho^2})}{2\\rho}.\n$$\nThe problem asks for the minimal magnitude of the perturbation $|\\alpha|$ that guarantees robustness. This corresponds to the smallest value of $|\\alpha|$ satisfying the aforementioned inequality, which is the lower bound of the interval.",
            "answer": "$$\n\\boxed{\\frac{\\beta\\left(1 + \\sqrt{1 + 4\\rho^2}\\right)}{2\\rho}}\n$$"
        }
    ]
}