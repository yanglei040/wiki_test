{
    "hands_on_practices": [
        {
            "introduction": "Understanding a numerical algorithm begins with mastering its fundamental steps. This first practice is a concrete computational exercise designed to build intuition for the unshifted QR iteration. By manually performing two full steps of the algorithm on a simple $2 \\times 2$ symmetric matrix, you will trace the transformation of the matrix and explicitly verify the similarity property that preserves eigenvalues at each stage .",
            "id": "3598477",
            "problem": "Consider the unshifted orthogonal-triangular (QR) algorithm applied to the real symmetric matrix\n$$\nA_0=\\begin{pmatrix}2 & 1 \\\\ 1 & 2\\end{pmatrix}.\n$$\nThe unshifted QR iteration is defined as follows. At iteration index $k$, compute the orthogonal-triangular (QR) decomposition $A_k=Q_k R_k$, where $Q_k$ is orthogonal (so $Q_k^{\\top}Q_k=I$) and $R_k$ is upper triangular, and then form $A_{k+1}=R_k Q_k$. This procedure is known to produce a sequence of matrices connected by similarity transformations, and it is used to iteratively reveal eigenvalue information.\n\nUsing only the fundamental definition of the orthogonal-triangular (QR) decomposition for full-rank square matrices and the orthogonality property $Q_k^{\\top}Q_k=I$, perform exactly two unshifted QR iterations starting from $A_0$, thereby constructing $A_1$ and $A_2$. At each step, explicitly confirm the similarity relation that connects $A_{k+1}$ to $A_k$.\n\nYou must compute these decompositions and products exactly, without appeal to any pre-stated algorithmic shortcuts other than the defining properties of the orthogonal-triangular (QR) decomposition and orthogonality. Express your final result as an exact rational number.\n\nWhat is the exact value of the $(1,1)$ entry of $A_2$?",
            "solution": "The problem requires performing two iterations of the un-shifted QR algorithm on the matrix $A_0 = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$. The algorithm is defined by the sequence $A_k = Q_k R_k$ and $A_{k+1} = R_k Q_k$, where $A_k = Q_k R_k$ is the orthogonal-triangular (QR) decomposition of $A_k$.\n\nFirst, let's analyze the given matrix. The matrix $A_0$ is a $2 \\times 2$ real symmetric matrix.\n$$\nA_0 = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\nThe determinant is $\\det(A_0) = (2)(2) - (1)(1) = 3 \\neq 0$, so the matrix is full-rank and a unique QR decomposition (up to signs) exists.\n\n**Iteration 1: Computation of $A_1$**\n\nWe begin by computing the QR decomposition of $A_0$. Let the columns of $A_0$ be $a_1 = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ and $a_2 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$. We apply the Gram-Schmidt process to the column vectors of $A_0$ to find the orthogonal matrix $Q_0$.\n\nLet the columns of $Q_0$ be $q_1$ and $q_2$.\nThe first unnormalized orthogonal vector $u_1$ is simply $a_1$:\n$$\nu_1 = a_1 = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n$$\nThe squared norm is $\\|u_1\\|^2 = 2^2 + 1^2 = 5$. The norm is $\\|u_1\\| = \\sqrt{5}$.\nThe first column of $Q_0$ is the normalized vector $q_1$:\n$$\nq_1 = \\frac{u_1}{\\|u_1\\|} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n$$\nThe second unnormalized orthogonal vector $u_2$ is found by subtracting the projection of $a_2$ onto $u_1$:\n$$\nu_2 = a_2 - \\frac{a_2^{\\top} u_1}{u_1^{\\top} u_1} u_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{(1)(2) + (2)(1)}{5} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{4}{5} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{8}{5} \\\\ 2 - \\frac{4}{5} \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{5} \\\\ \\frac{6}{5} \\end{pmatrix}\n$$\nThe squared norm is $\\|u_2\\|^2 = (-\\frac{3}{5})^2 + (\\frac{6}{5})^2 = \\frac{9}{25} + \\frac{36}{25} = \\frac{45}{25} = \\frac{9}{5}$. The norm is $\\|u_2\\| = \\sqrt{\\frac{9}{5}} = \\frac{3}{\\sqrt{5}}$.\nThe second column of $Q_0$ is the normalized vector $q_2$:\n$$\nq_2 = \\frac{u_2}{\\|u_2\\|} = \\frac{1}{3/\\sqrt{5}} \\begin{pmatrix} -3/5 \\\\ 6/5 \\end{pmatrix} = \\frac{\\sqrt{5}}{3} \\frac{3}{5} \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}\n$$\nThus, the orthogonal matrix $Q_0$ is:\n$$\nQ_0 = [q_1 \\ q_2] = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 & -1 \\\\ 1 & 2 \\end{pmatrix}\n$$\nThe upper triangular matrix $R_0$ is obtained from $A_0 = Q_0 R_0 \\implies R_0 = Q_0^{\\top} A_0$, since $Q_0$ is orthogonal ($Q_0^{\\top}Q_0 = I$).\n$$\nR_0 = \\left(\\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 & 1 \\\\ -1 & 2 \\end{pmatrix}\\right) \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} (2)(2)+(1)(1) & (2)(1)+(1)(2) \\\\ (-1)(2)+(2)(1) & (-1)(1)+(2)(2) \\end{pmatrix} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 5 & 4 \\\\ 0 & 3 \\end{pmatrix}\n$$\nHaving found $Q_0$ and $R_0$, we compute the next matrix in the sequence, $A_1$:\n$$\nA_1 = R_0 Q_0 = \\left( \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 5 & 4 \\\\ 0 & 3 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 & -1 \\\\ 1 & 2 \\end{pmatrix} \\right) = \\frac{1}{5} \\begin{pmatrix} (5)(2)+(4)(1) & (5)(-1)+(4)(2) \\\\ (0)(2)+(3)(1) & (0)(-1)+(3)(2) \\end{pmatrix}\n$$\n$$\nA_1 = \\frac{1}{5} \\begin{pmatrix} 14 & 3 \\\\ 3 & 6 \\end{pmatrix}\n$$\nTo confirm the similarity relation, we show $A_1 = Q_0^{\\top} A_0 Q_0$. From the definitions, $A_1 = R_0 Q_0$ and $R_0 = Q_0^{\\top} A_0$. Substituting the second into the first gives $A_1 = (Q_0^{\\top} A_0) Q_0$, which is the desired relation. Since $A_0$ is symmetric, $A_1$ must also be symmetric, which our result confirms.\n\n**Iteration 2: Computation of $A_2$**\n\nWe now repeat the process for $A_1 = \\frac{1}{5} \\begin{pmatrix} 14 & 3 \\\\ 3 & 6 \\end{pmatrix}$. Let its columns be $a'_1 = \\frac{1}{5} \\begin{pmatrix} 14 \\\\ 3 \\end{pmatrix}$ and $a'_2 = \\frac{1}{5} \\begin{pmatrix} 3 \\\\ 6 \\end{pmatrix}$. We compute the QR decomposition $A_1 = Q_1 R_1$.\n\nFirst unnormalized vector $u'_1$:\n$$\nu'_1 = a'_1 = \\frac{1}{5} \\begin{pmatrix} 14 \\\\ 3 \\end{pmatrix}\n$$\nIts squared norm is $\\|u'_1\\|^2 = (\\frac{1}{5})^2 (14^2 + 3^2) = \\frac{1}{25}(196+9) = \\frac{205}{25} = \\frac{41}{5}$. The norm is $\\|u'_1\\|=\\sqrt{\\frac{41}{5}} = \\frac{\\sqrt{205}}{5}$.\nThe first column of $Q_1$ is $q'_1$:\n$$\nq'_1 = \\frac{u'_1}{\\|u'_1\\|} = \\frac{\\frac{1}{5} \\begin{pmatrix} 14 \\\\ 3 \\end{pmatrix}}{\\frac{\\sqrt{205}}{5}} = \\frac{1}{\\sqrt{205}} \\begin{pmatrix} 14 \\\\ 3 \\end{pmatrix}\n$$\nThe second unnormalized vector $u'_2$ is:\n$$\nu'_2 = a'_2 - \\frac{{a'_2}^{\\top} u'_1}{{u'_1}^{\\top} u'_1} u'_1\n$$\nThe dot product is ${a'_2}^{\\top} u'_1 = (\\frac{1}{5})^2 ((3)(14)+(6)(3)) = \\frac{1}{25}(42+18) = \\frac{60}{25} = \\frac{12}{5}$.\n$$\nu'_2 = \\frac{1}{5}\\begin{pmatrix} 3 \\\\ 6 \\end{pmatrix} - \\frac{12/5}{41/5} \\left( \\frac{1}{5}\\begin{pmatrix} 14 \\\\ 3 \\end{pmatrix} \\right) = \\frac{1}{5}\\begin{pmatrix} 3 \\\\ 6 \\end{pmatrix} - \\frac{12}{41} \\frac{1}{5}\\begin{pmatrix} 14 \\\\ 3 \\end{pmatrix} = \\frac{1}{205} \\left( 41\\begin{pmatrix} 3 \\\\ 6 \\end{pmatrix} - 12\\begin{pmatrix} 14 \\\\ 3 \\end{pmatrix} \\right)\n$$\n$$\nu'_2 = \\frac{1}{205} \\begin{pmatrix} 123-168 \\\\ 246-36 \\end{pmatrix} = \\frac{1}{205}\\begin{pmatrix} -45 \\\\ 210 \\end{pmatrix} = \\frac{15}{205}\\begin{pmatrix} -3 \\\\ 14 \\end{pmatrix} = \\frac{3}{41}\\begin{pmatrix} -3 \\\\ 14 \\end{pmatrix}\n$$\nIts squared norm is $\\|u'_2\\|^2 = (\\frac{3}{41})^2 ((-3)^2+14^2) = \\frac{9}{41^2}(9+196) = \\frac{9 \\cdot 205}{41^2} = \\frac{9 \\cdot 5 \\cdot 41}{41^2} = \\frac{45}{41}$. The norm is $\\|u'_2\\| = \\sqrt{\\frac{45}{41}} = \\frac{3\\sqrt{5}}{\\sqrt{41}} = \\frac{3\\sqrt{205}}{41}$.\nThe second column of $Q_1$ is $q'_2$:\n$$\nq'_2 = \\frac{u'_2}{\\|u'_2\\|} = \\frac{\\frac{3}{41}\\begin{pmatrix} -3 \\\\ 14 \\end{pmatrix}}{\\frac{3\\sqrt{205}}{41}} = \\frac{1}{\\sqrt{205}}\\begin{pmatrix} -3 \\\\ 14 \\end{pmatrix}\n$$\nThe orthogonal matrix $Q_1$ is:\n$$\nQ_1 = [q'_1 \\ q'_2] = \\frac{1}{\\sqrt{205}} \\begin{pmatrix} 14 & -3 \\\\ 3 & 14 \\end{pmatrix}\n$$\nThe upper triangular matrix $R_1 = Q_1^{\\top} A_1$:\n$$\nR_1 = \\left( \\frac{1}{\\sqrt{205}} \\begin{pmatrix} 14 & 3 \\\\ -3 & 14 \\end{pmatrix} \\right) \\left( \\frac{1}{5} \\begin{pmatrix} 14 & 3 \\\\ 3 & 6 \\end{pmatrix} \\right) = \\frac{1}{5\\sqrt{205}} \\begin{pmatrix} 14^2+3^2 & (14)(3)+(3)(6) \\\\ (-3)(14)+(14)(3) & (-3)(3)+(14)(6) \\end{pmatrix}\n$$\n$$\nR_1 = \\frac{1}{5\\sqrt{205}} \\begin{pmatrix} 196+9 & 42+18 \\\\ -42+42 & -9+84 \\end{pmatrix} = \\frac{1}{5\\sqrt{205}} \\begin{pmatrix} 205 & 60 \\\\ 0 & 75 \\end{pmatrix} = \\frac{1}{\\sqrt{205}} \\begin{pmatrix} 41 & 12 \\\\ 0 & 15 \\end{pmatrix}\n$$\nNow we compute $A_2 = R_1 Q_1$:\n$$\nA_2 = \\left( \\frac{1}{\\sqrt{205}} \\begin{pmatrix} 41 & 12 \\\\ 0 & 15 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{205}} \\begin{pmatrix} 14 & -3 \\\\ 3 & 14 \\end{pmatrix} \\right) = \\frac{1}{205} \\begin{pmatrix} (41)(14)+(12)(3) & (41)(-3)+(12)(14) \\\\ (0)(14)+(15)(3) & (0)(-3)+(15)(14) \\end{pmatrix}\n$$\n$$\nA_2 = \\frac{1}{205} \\begin{pmatrix} 574+36 & -123+168 \\\\ 45 & 210 \\end{pmatrix} = \\frac{1}{205} \\begin{pmatrix} 610 & 45 \\\\ 45 & 210 \\end{pmatrix}\n$$\nFactoring out common terms (all entries and the denominator $205=5 \\times 41$ are divisible by $5$):\n$$\nA_2 = \\frac{5}{205} \\begin{pmatrix} 122 & 9 \\\\ 9 & 42 \\end{pmatrix} = \\frac{1}{41} \\begin{pmatrix} 122 & 9 \\\\ 9 & 42 \\end{pmatrix}\n$$\nAs before, the similarity relation $A_2 = Q_1^{\\top} A_1 Q_1$ is guaranteed by the construction. Since $A_1$ is symmetric, $A_2$ must also be symmetric, which our result confirms.\n\nThe problem asks for the $(1,1)$ entry of $A_2$. From the matrix $A_2$ derived above, this value is:\n$$\n(A_2)_{11} = \\frac{122}{41}\n$$\nThis fraction is irreducible as $41$ is a prime number and $122 = 2 \\times 61$.",
            "answer": "$$\\boxed{\\frac{122}{41}}$$"
        },
        {
            "introduction": "Once the mechanics of the QR algorithm are clear, the next crucial question is about its performance: how quickly does it converge? This exercise moves beyond simple iteration to a quantitative analysis of the convergence rate. By linking the QR algorithm to the concept of simultaneous orthogonal iteration, you will derive an explicit expression for the decay of an off-diagonal element, revealing how the separation of eigenvalues governs the speed of convergence .",
            "id": "3598462",
            "problem": "Consider the un-shifted orthogonal-triangular (QR) algorithm applied to a real symmetric $3 \\times 3$ matrix $A$, with the iteration defined by $A^{(k)} = Q^{(k)} R^{(k)}$ and $A^{(k+1)} = R^{(k)} Q^{(k)}$, where $Q^{(k)}$ is orthogonal and $R^{(k)}$ is upper triangular. Denote by $a_{ij}^{(k)}$ the $(i,j)$-entry of $A^{(k)}$. Let\n$$\n\\Lambda = \\operatorname{diag}(2, 1, \\tfrac{1}{3}), \\quad\nV = \\begin{pmatrix}\n\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2} & 0 \\\\\n\\frac{\\sqrt{2}}{2} & \\phantom{-}\\frac{\\sqrt{2}}{2} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}, \\quad\nA = V \\Lambda V^{\\top}.\n$$\nStarting from $A^{(0)} = A$, use the equivalence of the un-shifted QR algorithm with simultaneous orthogonal iteration on a normal matrix to derive, from first principles, an explicit expression for $|a_{21}^{(k)}|$ in terms of $k$, and from it a geometric-rate bound of the form $|a_{21}^{(k)}| \\leq C r^{k}$ with $r \\in (0,1)$, where the constant $C$ and ratio $r$ must be determined from the spectral data and the initial basis induced by $V$. Then, using your derived bound, compute the smallest integer $k$ such that $|a_{21}^{(k)}| \\leq \\varepsilon$ with $\\varepsilon = 1.0 \\times 10^{-6}$. Your final answer must be the value of $k$ only; no rounding instructions are required for this integer output.",
            "solution": "The problem requires the analysis of the convergence of the $(2,1)$ entry of the matrices $A^{(k)}$ generated by the un-shifted QR algorithm. The core of the solution lies in leveraging the equivalence between the QR algorithm and simultaneous orthogonal iteration.\n\nThe un-shifted QR iteration is defined by $A^{(k)} = Q^{(k)}R^{(k)}$ and $A^{(k+1)} = R^{(k)}Q^{(k)}$, where $A^{(0)} = A$. This implies $A^{(k+1)} = (Q^{(k)})^{\\top} A^{(k)} Q^{(k)}$. By induction, if we define the accumulated orthogonal matrix $\\mathcal{Q}^{(k)} = Q^{(0)}Q^{(1)}\\cdots Q^{(k-1)}$, then $A^{(k)} = (\\mathcal{Q}^{(k)})^{\\top} A \\mathcal{Q}^{(k)}$. A key identity of the QR algorithm is $A^k = \\mathcal{Q}^{(k)} \\mathcal{R}^{(k)}$, where $\\mathcal{R}^{(k)} = R^{(k-1)}\\cdots R^{(0)}$. This means that $\\mathcal{Q}^{(k)}$ is the orthogonal factor from the QR factorization of the matrix power $A^k$.\n\nSimultaneous orthogonal iteration, starting with the standard basis (represented by the identity matrix $I$), generates a sequence of orthogonal matrices by applying the Gram-Schmidt process to the columns of $A^k$. The resulting orthogonal matrix is precisely $\\mathcal{Q}^{(k)}$. We will use this fact to derive an expression for the columns of $\\mathcal{Q}^{(k)}$.\n\nThe matrix $A$ is given by its spectral decomposition $A = V \\Lambda V^{\\top}$, where the eigenvalues are $\\lambda_1 = 2$, $\\lambda_2 = 1$, $\\lambda_3 = 1/3$, and the matrix of corresponding orthonormal eigenvectors is\n$$\nV = \\begin{pmatrix} v_1 & v_2 & v_3 \\end{pmatrix} = \\begin{pmatrix}\n\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2} & 0 \\\\\n\\frac{\\sqrt{2}}{2} & \\phantom{-}\\frac{\\sqrt{2}}{2} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nThe matrix $V$ is orthogonal, so $V^{-1}=V^{\\top}$. The matrix power $A^k$ is $A^k = (V \\Lambda V^{\\top})^k = V \\Lambda^k V^{\\top}$.\n\nLet $q_j^{(k)}$ denote the $j$-th column of $\\mathcal{Q}^{(k)}$. These columns are found by applying the Gram-Schmidt process to the columns of $A^k$, which are $A^k e_j$ for $j=1, 2, 3$.\n\nLet's find the first two columns of $A^k$:\nThe first column is $A^k e_1 = V \\Lambda^k V^{\\top} e_1$. The first column of $V^{\\top}$ is the first row of $V$, i.e., $(\\frac{\\sqrt{2}}{2}, -\\frac{\\sqrt{2}}{2}, 0)^{\\top}$.\n$$\nA^k e_1 = V \\begin{pmatrix} 2^k & 0 & 0 \\\\ 0 & 1^k & 0 \\\\ 0 & 0 & (1/3)^k \\end{pmatrix} \\begin{pmatrix} \\frac{\\sqrt{2}}{2} \\\\ -\\frac{\\sqrt{2}}{2} \\\\ 0 \\end{pmatrix} = V \\begin{pmatrix} \\frac{\\sqrt{2}}{2} 2^k \\\\ -\\frac{\\sqrt{2}}{2} 1^k \\\\ 0 \\end{pmatrix} = \\frac{\\sqrt{2}}{2} (2^k v_1 - 1^k v_2) = \\frac{\\sqrt{2}}{2} 2^k (v_1 - (\\tfrac{1}{2})^k v_2).\n$$\nThe first column $q_1^{(k)}$ of $\\mathcal{Q}^{(k)}$ is obtained by normalizing $A^k e_1$. Since $v_1$ and $v_2$ are orthogonal unit vectors,\n$$\nq_1^{(k)} = \\frac{v_1 - (\\tfrac{1}{2})^k v_2}{\\|v_1 - (\\tfrac{1}{2})^k v_2\\|} = \\frac{v_1 - (\\tfrac{1}{2})^k v_2}{\\sqrt{\\|v_1\\|^2 + ((\\tfrac{1}{2})^k)^2 \\|v_2\\|^2}} = \\frac{v_1 - (\\tfrac{1}{2})^k v_2}{\\sqrt{1 + (\\tfrac{1}{4})^k}}.\n$$\nThe second column of $A^k$ is $A^k e_2 = V \\Lambda^k V^{\\top} e_2$. The second column of $V^{\\top}$ is the second row of $V$, i.e., $(\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2}, 0)^{\\top}$.\n$$\nA^k e_2 = V \\begin{pmatrix} 2^k & 0 & 0 \\\\ 0 & 1^k & 0 \\\\ 0 & 0 & (1/3)^k \\end{pmatrix} \\begin{pmatrix} \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\\\ 0 \\end{pmatrix} = V \\begin{pmatrix} \\frac{\\sqrt{2}}{2} 2^k \\\\ \\frac{\\sqrt{2}}{2} 1^k \\\\ 0 \\end{pmatrix} = \\frac{\\sqrt{2}}{2} (2^k v_1 + 1^k v_2) = \\frac{\\sqrt{2}}{2} 2^k (v_1 + (\\tfrac{1}{2})^k v_2).\n$$\nTo find $q_2^{(k)}$, we first orthogonalize $A^k e_2$ against $q_1^{(k)}$. Let's denote $\\rho = \\tfrac{1}{2}$. The unnormalized vectors are proportional to $\\tilde{q}_1 = v_1 - \\rho^k v_2$ and $\\tilde{y}_2 = v_1 + \\rho^k v_2$. The orthogonalized vector is $\\tilde{q}_2 = \\tilde{y}_2 - \\frac{\\langle \\tilde{y}_2, \\tilde{q}_1 \\rangle}{\\langle \\tilde{q}_1, \\tilde{q}_1 \\rangle} \\tilde{q}_1$.\n$$\n\\langle \\tilde{y}_2, \\tilde{q}_1 \\rangle = \\langle v_1 + \\rho^k v_2, v_1 - \\rho^k v_2 \\rangle = \\|v_1\\|^2 - \\rho^{2k}\\|v_2\\|^2 = 1 - \\rho^{2k}.\n$$\n$$\n\\langle \\tilde{q}_1, \\tilde{q}_1 \\rangle = \\|v_1 - \\rho^k v_2\\|^2 = \\|v_1\\|^2 + \\rho^{2k}\\|v_2\\|^2 = 1 + \\rho^{2k}.\n$$\n$$\n\\tilde{q}_2 = (v_1 + \\rho^k v_2) - \\frac{1 - \\rho^{2k}}{1 + \\rho^{2k}}(v_1 - \\rho^k v_2) = \\frac{(1+\\rho^{2k}) - (1-\\rho^{2k})}{1+\\rho^{2k}}v_1 + \\frac{\\rho^k(1+\\rho^{2k}) + \\rho^k(1-\\rho^{2k})}{1+\\rho^{2k}}v_2 = \\frac{2\\rho^{2k}}{1+\\rho^{2k}}v_1 + \\frac{2\\rho^k}{1+\\rho^{2k}}v_2.\n$$\nThis vector is proportional to $\\rho^k v_1 + v_2$. Normalizing this vector gives $q_2^{(k)}$:\n$$\nq_2^{(k)} = \\frac{\\rho^k v_1 + v_2}{\\|\\rho^k v_1 + v_2\\|} = \\frac{(\\tfrac{1}{2})^k v_1 + v_2}{\\sqrt{(\\tfrac{1}{4})^k + 1}}.\n$$\nThe entry $a_{21}^{(k)}$ of $A^{(k)}$ is given by $a_{21}^{(k)} = (q_2^{(k)})^{\\top} A q_1^{(k)}$. First, we compute $A q_1^{(k)}$:\n$$\nA q_1^{(k)} = A \\frac{v_1 - \\rho^k v_2}{\\sqrt{1 + \\rho^{2k}}} = \\frac{A v_1 - \\rho^k A v_2}{\\sqrt{1 + \\rho^{2k}}} = \\frac{\\lambda_1 v_1 - \\rho^k \\lambda_2 v_2}{\\sqrt{1 + \\rho^{2k}}}.\n$$\nNow, we compute the dot product with $(q_2^{(k)})^{\\top}$:\n$$\na_{21}^{(k)} = \\frac{(\\rho^k v_1^{\\top} + v_2^{\\top})}{\\sqrt{1+\\rho^{2k}}} \\frac{(\\lambda_1 v_1 - \\rho^k \\lambda_2 v_2)}{\\sqrt{1 + \\rho^{2k}}} = \\frac{1}{1+\\rho^{2k}} (\\rho^k \\lambda_1 v_1^{\\top}v_1 - \\rho^{2k}\\lambda_2 v_1^{\\top}v_2 + \\lambda_1 v_2^{\\top}v_1 - \\rho^k\\lambda_2 v_2^{\\top}v_2).\n$$\nUsing $v_i^{\\top}v_j = \\delta_{ij}$, we get:\n$$\na_{21}^{(k)} = \\frac{1}{1+\\rho^{2k}} (\\rho^k \\lambda_1 - 0 + 0 - \\rho^k \\lambda_2) = \\frac{\\rho^k (\\lambda_1 - \\lambda_2)}{1+\\rho^{2k}}.\n$$\nSubstituting $\\lambda_1 = 2$, $\\lambda_2 = 1$, and $\\rho = 1/2$:\n$$\n|a_{21}^{(k)}| = a_{21}^{(k)} = \\frac{(\\tfrac{1}{2})^k (2 - 1)}{1+(\\tfrac{1}{2})^{2k}} = \\frac{(\\tfrac{1}{2})^k}{1+(\\tfrac{1}{4})^k}.\n$$\nThis is the required explicit expression for $|a_{21}^{(k)}|$.\n\nNext, we derive a geometric-rate bound $|a_{21}^{(k)}| \\leq C r^{k}$. From the expression above, we have $|a_{21}^{(k)}| = \\frac{1}{1+(1/4)^k} (\\frac{1}{2})^k$. Since $k \\geq 0$, $(1/4)^k > 0$, so $1+(1/4)^k > 1$. Therefore, $\\frac{1}{1+(1/4)^k} < 1$. We can establish the bound with $r=1/2$:\n$$\n|a_{21}^{(k)}| \\leq 1 \\cdot (\\tfrac{1}{2})^k.\n$$\nThus, we can choose the constant $C=1$ and the ratio $r=1/2$.\n\nFinally, we must find the smallest integer $k$ such that $|a_{21}^{(k)}| \\leq \\varepsilon = 1.0 \\times 10^{-6}$.\n$$\n\\frac{(\\tfrac{1}{2})^k}{1+(\\tfrac{1}{4})^k} \\leq 10^{-6}.\n$$\nSince $1+(1/4)^k > 1$, a slightly looser but sufficient condition is $(\\tfrac{1}{2})^k \\leq 10^{-6}$. Solving this gives a lower bound on $k$.\n$$\n-k \\ln(2) \\leq \\ln(10^{-6}) = -6 \\ln(10).\n$$\n$$\nk \\geq \\frac{6 \\ln(10)}{\\ln(2)}.\n$$\nUsing the values $\\ln(10) \\approx 2.302585$ and $\\ln(2) \\approx 0.693147$, we find:\n$$\nk \\geq \\frac{6 \\times 2.302585}{0.693147} \\approx 19.93156.\n$$\nSince $k$ must be an integer, the smallest possible value is $k=20$. We can verify this choice with the original inequality.\nFor $k=19$: $|a_{21}^{(19)}| = \\frac{(1/2)^{19}}{1+(1/4)^{19}} \\approx (1/2)^{19} \\approx 1.907 \\times 10^{-6} > 10^{-6}$.\nFor $k=20$: $|a_{21}^{(20)}| = \\frac{(1/2)^{20}}{1+(1/4)^{20}} \\approx (1/2)^{20} \\approx 0.953 \\times 10^{-6} < 10^{-6}$.\nThus, the smallest integer $k$ is $20$.",
            "answer": "$$\\boxed{20}$$"
        },
        {
            "introduction": "The convergence behavior of the QR algorithm becomes more nuanced for non-symmetric matrices, especially those with complex eigenvalues. This advanced problem explores such a case, using a companion matrix with a complex conjugate eigenpair. You will investigate why the algorithm converges to a block-triangular real Schur form instead of a diagonal one, and uncover the surprising possibility of a finite cycle, providing deep insight into the algorithm's behavior beyond the standard symmetric case .",
            "id": "3598473",
            "problem": "Consider the un-shifted orthogonal-triangular (QR) algorithm applied to a real, unreduced upper Hessenberg matrix, with the convention that at each iteration the upper triangular matrix has strictly positive diagonal entries. Let $A \\in \\mathbb{R}^{3 \\times 3}$ be the companion matrix of the monic polynomial $p(x) = x^{3} - x^{2} + x - 1$, namely\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0 & 0 & 1 \\\\\n1 & 0 & -1 \\\\\n0 & 1 & 1\n\\end{pmatrix}.\n$$\nThe un-shifted QR iteration is defined by $A_{0} = A$ and, for $k \\geq 0$, compute the unique QR factorization $A_{k} = Q_{k} R_{k}$ with $Q_{k}^{\\top} Q_{k} = I$ and $R_{k}$ upper triangular with strictly positive diagonal entries, and set $A_{k+1} = R_{k} Q_{k}$. Use only the facts that similarity transformations preserve eigenvalues, the real Schur theorem guarantees that any real matrix is orthogonally similar to an upper quasi-triangular matrix with $1 \\times 1$ and $2 \\times 2$ blocks on the diagonal, and the QR step $A_{k+1} = Q_{k}^{\\top} A_{k} Q_{k}$ is an orthogonal similarity.\n\nStarting from these foundations:\n\n1. Determine the eigenvalues of $A$ and characterize the spectral decomposition type (one-dimensional real eigenspace and a two-dimensional invariant subspace corresponding to a complex conjugate pair).\n\n2. Analyze, at a structural level, how the un-shifted QR iteration acts as a sequence of orthogonal similarities and argue why, for this $A$, convergence proceeds to the real Schur form with a persistent trailing $2 \\times 2$ block representing the complex conjugate pair. In particular, explain why deflation to a purely diagonal form is impossible in the invariant two-dimensional subspace.\n\n3. Investigate whether a nontrivial finite cycle (of period greater than one) can occur under the un-shifted QR iteration for this $A$ when the QR factorization is chosen with strictly positive diagonal entries in $R_{k}$. Your investigation should reason from the uniqueness of the QR step, orthogonal similarity invariants, and the structure of commuting transforms on the limiting real Schur form, rather than appealing to any prepackaged convergence theorem.\n\n4. Let the limiting trailing $2 \\times 2$ real Schur block be represented (after an appropriate orthogonal change of basis restricted to the invariant two-dimensional subspace) by a planar rotation matrix\n$$\n\\begin{pmatrix}\n\\cos(\\theta) & \\sin(\\theta) \\\\\n-\\sin(\\theta) & \\cos(\\theta)\n\\end{pmatrix}.\n$$\nCompute the exact value of the angle $\\theta$ associated with the complex conjugate eigenpair of $A$, expressed in radians.\n\nYour final answer must be the single value of $\\theta$ in radians, with no rounding.",
            "solution": "The solution is structured to address the four parts of the problem in sequence.\n\n**1. Eigenvalues of $A$ and Spectral Decomposition**\n\nThe eigenvalues of a companion matrix are the roots of its associated characteristic polynomial. The given matrix $A$ is the companion matrix for the polynomial $p(x) = x^{3} - x^{2} + x - 1$. The eigenvalues $\\lambda$ of $A$ are the roots of the equation $p(\\lambda) = 0$.\n\nWe solve $p(\\lambda) = \\lambda^{3} - \\lambda^{2} + \\lambda - 1 = 0$.\nFactoring the polynomial:\n$$ \\lambda^{2}(\\lambda - 1) + 1(\\lambda - 1) = 0 $$\n$$ (\\lambda^{2} + 1)(\\lambda - 1) = 0 $$\nThe roots are $\\lambda_1 = 1$, $\\lambda_2 = i$, and $\\lambda_3 = -i$.\n\nThe spectrum of $A$ consists of one real eigenvalue, $\\lambda_1 = 1$, and a complex conjugate pair, $\\{\\lambda_2, \\lambda_3\\} = \\{i, -i\\}$. For a real matrix such as $A$, the eigenspace corresponding to the real eigenvalue $\\lambda_1=1$ is a one-dimensional real subspace of $\\mathbb{R}^3$. The complex conjugate eigenpair $\\{i, -i\\}$ corresponds to a two-dimensional invariant subspace in $\\mathbb{R}^3$, spanned by the real and imaginary parts of an eigenvector associated with either $i$ or $-i$.\n\n**2. Convergence to Real Schur Form**\n\nThe un-shifted QR iteration generates a sequence of matrices $\\{A_k\\}$ where each matrix is orthogonally similar to the previous one: $A_{k+1} = R_k Q_k = (Q_k^{-1} A_k) Q_k = Q_k^{\\top} A_k Q_k$. By induction, all matrices $A_k$ are orthogonally similar to the original matrix $A_0 = A$. Therefore, they all share the same eigenvalues: $1$, $i$, and $-i$.\n\nIf the sequence $\\{A_k\\}$ converges to a matrix $A_{\\infty}$, then $A_{\\infty}$ must also be orthogonally similar to $A$. The real Schur theorem states that any real matrix is orthogonally similar to a real, upper quasi-triangular matrix, whose diagonal consists of $1 \\times 1$ blocks for real eigenvalues and $2 \\times 2$ blocks for complex conjugate pairs of eigenvalues.\n\nSince $A$ is a real matrix with complex eigenvalues ($i$ and $-i$), it cannot be diagonalized by a real orthogonal transformation. A real matrix is orthogonally diagonalizable if and only if it is symmetric. The given matrix $A$ is not symmetric (e.g., $A_{21}=1$ while $A_{12}=0$), so it cannot be orthogonally diagonalized over $\\mathbb{R}$.\n\nThe QR algorithm performs a sequence of real orthogonal similarities. Consequently, it cannot converge to a diagonal matrix. The \"best\" structure it can achieve is a real Schur form. For the matrix $A$, this form must contain a $1 \\times 1$ block for the eigenvalue $1$ and a $2 \\times 2$ block for the eigenvalues $\\{i, -i\\}$. Thus, the iteration, if it converges, must converge to a form with a persistent $2 \\times 2$ block, reflecting the two-dimensional invariant subspace associated with the complex eigenvalues. Deflation to a purely diagonal form is impossible within this framework. The convergence of the unshifted QR algorithm often sorts eigenvalues by magnitude, but here all eigenvalues have magnitude $1$. Nonetheless, the fundamental algebraic constraints imposed by the real orthogonal transformations dictate the structure of the limit, necessitating the $2 \\times 2$ block.\n\n**3. Investigation of Finite Cycles**\n\nWe investigate if a nontrivial finite cycle of period $p > 1$ can occur, meaning $A_{k+p} = A_k$ for some $k$. If such a cycle exists, the process is periodic from that point on. Let's assume the cycle starts at $k=0$ for simplicity, so $A_p = A_0$.\n\nFrom the similarity relation, $A_p = (Q_{p-1}^{\\top} \\dots Q_0^{\\top}) A_0 (Q_0 \\dots Q_{p-1})$. Let $\\mathbf{Q} = Q_0 Q_1 \\dots Q_{p-1}$. Then $A_p = \\mathbf{Q}^{\\top} A_0 \\mathbf{Q}$. For a cycle, $A_p=A_0$, which implies $A_0 = \\mathbf{Q}^{\\top} A_0 \\mathbf{Q}$, or $A_0 \\mathbf{Q} = \\mathbf{Q} A_0$. The orthogonal matrix $\\mathbf{Q}$ must commute with $A_0$.\n\nA key property of this specific matrix $A$ is that $A^4=I$. We can verify this from its characteristic polynomial $p(\\lambda) = \\lambda^3-\\lambda^2+\\lambda-1$. By the Cayley-Hamilton theorem, $p(A) = A^3 - A^2 + A - I = 0$. Multiplying by $(A+I)$ gives $(A+I)(A^3 - A^2 + A - I) = A^4 - A^3 + A^2 - A + A^3 - A^2 + A - I = A^4 - I = 0$. Thus, $A^4 = I$.\n\nThere is a fundamental result connecting the QR iteration to matrix powers: $A^p = (Q_0 \\dots Q_{p-1})(R_{p-1} \\dots R_0)$. Let $\\mathbf{Q}_p = Q_0 \\dots Q_{p-1}$ and $\\mathbf{R}_p = R_{p-1} \\dots R_0$. Then $A^p = \\mathbf{Q}_p \\mathbf{R}_p$ is the QR factorization of $A^p$, which is unique because $A$ is nonsingular ($\\det(A)=1$) and we require $R$ to have positive diagonal entries.\n\nLet's apply this for $p=4$. We have $A^4=I$. The unique QR factorization (with positive diagonal entries in the R-factor) of the identity matrix $I$ is $I = I \\cdot I$. Thus, we must have $\\mathbf{Q}_4 = Q_0 Q_1 Q_2 Q_3 = I$ and $\\mathbf{R}_4 = R_3 R_2 R_1 R_0 = I$.\n\nNow, let's look at the iterate $A_4$.\n$$ A_4 = \\mathbf{Q}_4^{\\top} A_0 \\mathbf{Q}_4 = I^{\\top} A_0 I = A_0 $$\nThis proves that $A_4=A_0$. The sequence is periodic with a period that divides $4$.\nThe period cannot be $p=1$, because that would imply $A_1 = A_0$, which for an unreduced Hessenberg matrix requires it to be triangular. $A$ is not triangular. Therefore, a nontrivial finite cycle (of period $2$ or $4$) must occur for this specific matrix $A$.\n\n**4. Computation of the Angle $\\theta$**\n\nThe limiting $2 \\times 2$ real Schur block, let's call it $B$, must have eigenvalues $i$ and $-i$. The characteristic polynomial of this block must be $(\\lambda-i)(\\lambda+i) = \\lambda^2+1=0$. This implies that $\\mathrm{tr}(B) = 0$ and $\\det(B) = 1$.\n\nThe problem states this block can be represented by a rotation matrix of the form:\n$$ C = \\begin{pmatrix} \\cos(\\theta) & \\sin(\\theta) \\\\ -\\sin(\\theta) & \\cos(\\theta) \\end{pmatrix} $$\nThe eigenvalues of this matrix $C$ are the roots of the characteristic equation $\\lambda^2 - \\mathrm{tr}(C)\\lambda + \\det(C) = 0$.\n$$ \\lambda^2 - (2\\cos(\\theta))\\lambda + 1 = 0 $$\nThe roots are $\\lambda = \\cos(\\theta) \\pm \\sqrt{\\cos^2(\\theta)-1} = \\cos(\\theta) \\pm i\\sin(\\theta) = e^{\\pm i\\theta}$.\n\nFor the eigenvalues of $C$ to match the required eigenvalues $\\{i, -i\\}$, we must have the set identity $\\{e^{i\\theta}, e^{-i\\theta}\\} = \\{i, -i\\}$.\nThis requires $\\cos(\\theta) = 0$ and $\\sin(\\theta) = \\pm 1$.\n\nThe angle $\\theta$ of a rotation associated with a complex conjugate pair of eigenvalues $re^{\\pm i\\phi}$ is fundamentally given by the argument $\\phi$ of the eigenvalues. Here, the eigenvalues are $i$ and $-i$. Their polar representations are:\n$$ i = 1 \\cdot \\exp\\left(i\\frac{\\pi}{2}\\right) $$\n$$ -i = 1 \\cdot \\exp\\left(-i\\frac{\\pi}{2}\\right) $$\nBy convention, the rotation angle is taken as the argument of the eigenvalue with the positive imaginary part. The argument of $i$ is $\\arg(i) = \\frac{\\pi}{2}$.\nThus, the angle of rotation $\\theta$ is $\\frac{\\pi}{2}$.\n\nLet's check this value with the matrix form. If $\\theta = \\frac{\\pi}{2}$, we have $\\cos(\\frac{\\pi}{2}) = 0$ and $\\sin(\\frac{\\pi}{2}) = 1$. The eigenvalues of $C$ are $e^{\\pm i\\pi/2}$, which are $\\pm i$. This is consistent.\nIf we had chosen $\\theta = -\\frac{\\pi}{2}$, the eigenvalues would be $e^{\\mp i\\pi/2}$, which are also $\\pm i$. However, the positive value $\\frac{\\pi}{2}$ is the standard choice for the angle corresponding to the pair.",
            "answer": "$$\\boxed{\\frac{\\pi}{2}}$$"
        }
    ]
}