{
    "hands_on_practices": [
        {
            "introduction": "The Singular Value Decomposition (SVD) provides the most stable and general method for computing the Moore-Penrose pseudoinverse. This first practice grounds your understanding in this fundamental connection by guiding you through a direct calculation. By starting with the SVD components of a matrix, you will construct the pseudoinverse $A^{+} = V \\Sigma^{+} U^{\\top}$, reinforcing the core definition and the crucial step of inverting only the non-zero singular values .",
            "id": "3592295",
            "problem": "Let $A \\in \\mathbb{R}^{3 \\times 2}$ have the Singular Value Decomposition (SVD) $A = U \\Sigma V^{\\top}$ with\n$$\nU \\;=\\;\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\\n0 & 0 & 1 \\\\\n-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0\n\\end{bmatrix},\n\\qquad\n\\Sigma \\;=\\;\n\\begin{bmatrix}\n6 & 0 \\\\\n0 & 2 \\\\\n0 & 0\n\\end{bmatrix},\n\\qquad\nV \\;=\\;\n\\begin{bmatrix}\n\\frac{3}{5} & \\frac{4}{5} \\\\\n-\\frac{4}{5} & \\frac{3}{5}\n\\end{bmatrix}.\n$$\nUsing only the defining properties of the Moore–Penrose pseudoinverse and the definition of the Singular Value Decomposition (SVD), derive the Moore–Penrose pseudoinverse $A^{+}$ explicitly from the provided factors. Then, report the exact value of the $(2,3)$ entry of $A^{+}$ as a single closed-form expression. Do not use any result that is not a direct consequence of these definitions. No rounding is required.",
            "solution": "The problem is valid as it is self-contained, scientifically grounded in numerical linear algebra, and mathematically well-posed. A unique solution exists and can be derived from the provided information.\n\nThe task is to find the $(2,3)$ entry of the Moore-Penrose pseudoinverse $A^{+}$ of a matrix $A$, given its Singular Value Decomposition (SVD), $A = U \\Sigma V^{\\top}$. The derivation must adhere strictly to the defining properties of the pseudoinverse.\n\nThe Moore-Penrose pseudoinverse of a matrix $A$, denoted $A^{+}$, is the unique matrix that satisfies the following four Penrose conditions:\n1.  $A A^{+} A = A$\n2.  $A^{+} A A^{+} = A^{+}$\n3.  $(A A^{+})^{\\top} = A A^{+}$\n4.  $(A^{+} A)^{\\top} = A^{+} A$\n\nWe are given the SVD components of $A \\in \\mathbb{R}^{3 \\times 2}$:\n$$\nU \\;=\\;\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\\n0 & 0 & 1 \\\\\n-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0\n\\end{bmatrix},\n\\qquad\n\\Sigma \\;=\\;\n\\begin{bmatrix}\n6 & 0 \\\\\n0 & 2 \\\\\n0 & 0\n\\end{bmatrix},\n\\qquad\nV \\;=\\;\n\\begin{bmatrix}\n\\frac{3}{5} & \\frac{4}{5} \\\\\n-\\frac{4}{5} & \\frac{3}{5}\n\\end{bmatrix}.\n$$\nHere, $U \\in \\mathbb{R}^{3 \\times 3}$ and $V \\in \\mathbb{R}^{2 \\times 2}$ are orthogonal matrices ($U^{\\top}U=I$ and $V^{\\top}V=I$), and $\\Sigma$ is a rectangular diagonal matrix.\n\nWe propose the candidate for the pseudoinverse as $A^{+} = V \\Sigma^{+} U^{\\top}$. The matrix $\\Sigma^{+} \\in \\mathbb{R}^{2 \\times 3}$ is the pseudoinverse of $\\Sigma$. It is constructed by taking the reciprocal of each non-zero singular value and transposing the resulting matrix. The non-zero singular values in $\\Sigma$ are $\\sigma_1 = 6$ and $\\sigma_2 = 2$.\n$$\n\\Sigma^{+} \\;=\\;\n\\begin{bmatrix}\n\\frac{1}{6} & 0 & 0 \\\\\n0 & \\frac{1}{2} & 0\n\\end{bmatrix}.\n$$\nTo rigorously justify the use of $A^{+} = V \\Sigma^{+} U^{\\top}$, we must demonstrate that it satisfies the four Penrose conditions.\n\n1.  $A A^{+} A = (U \\Sigma V^{\\top})(V \\Sigma^{+} U^{\\top})(U \\Sigma V^{\\top}) = U \\Sigma (V^{\\top}V) \\Sigma^{+} (U^{\\top}U) \\Sigma V^{\\top} = U (\\Sigma \\Sigma^{+} \\Sigma) V^{\\top}$.\n    The product $\\Sigma\\Sigma^{+}\\Sigma$ is:\n    $$\n    \\Sigma \\Sigma^{+} \\Sigma = \\left( \\begin{bmatrix} 6 & 0 \\\\ 0 & 2 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{6} & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 \\end{bmatrix} \\right) \\Sigma = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} 6 & 0 \\\\ 0 & 2 \\\\ 0 & 0 \\end{bmatrix} = \\begin{bmatrix} 6 & 0 \\\\ 0 & 2 \\\\ 0 & 0 \\end{bmatrix} = \\Sigma.\n    $$\n    Therefore, $A A^{+} A = U \\Sigma V^{\\top} = A$. The first condition is satisfied.\n\n2.  $A^{+} A A^{+} = (V \\Sigma^{+} U^{\\top})(U \\Sigma V^{\\top})(V \\Sigma^{+} U^{\\top}) = V \\Sigma^{+} (U^{\\top}U) \\Sigma (V^{\\top}V) \\Sigma^{+} U^{\\top} = V (\\Sigma^{+} \\Sigma \\Sigma^{+}) U^{\\top}$.\n    The product $\\Sigma^{+}\\Sigma\\Sigma^{+}$ is:\n    $$\n    \\Sigma^{+} \\Sigma \\Sigma^{+} = \\left( \\begin{bmatrix} \\frac{1}{6} & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 \\end{bmatrix} \\begin{bmatrix} 6 & 0 \\\\ 0 & 2 \\\\ 0 & 0 \\end{bmatrix} \\right) \\Sigma^{+} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{6} & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 \\end{bmatrix} = \\Sigma^{+}.\n    $$\n    Therefore, $A^{+} A A^{+} = V \\Sigma^{+} U^{\\top} = A^{+}$. The second condition is satisfied.\n\n3.  $(A A^{+})^{\\top} = A A^{+}$. The product is $A A^{+} = U \\Sigma V^{\\top}V \\Sigma^{+} U^{\\top} = U (\\Sigma \\Sigma^{+}) U^{\\top}$. The matrix $\\Sigma \\Sigma^{+} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$ is diagonal and thus symmetric, $(\\Sigma \\Sigma^{+})^{\\top} = \\Sigma \\Sigma^{+}$. So, $(A A^{+})^{\\top} = (U (\\Sigma \\Sigma^{+}) U^{\\top})^{\\top} = U (\\Sigma \\Sigma^{+})^{\\top} U^{\\top} = U (\\Sigma \\Sigma^{+}) U^{\\top} = A A^{+}$. The third condition is satisfied.\n\n4.  $(A^{+} A)^{\\top} = A^{+} A$. The product is $A^{+} A = V \\Sigma^{+} U^{\\top}U \\Sigma V^{\\top} = V (\\Sigma^{+} \\Sigma) V^{\\top}$. The matrix $\\Sigma^{+} \\Sigma = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$ is the identity matrix, which is symmetric. So, $(A^{+} A)^{\\top} = (V (\\Sigma^{+} \\Sigma) V^{\\top})^{\\top} = V (\\Sigma^{+} \\Sigma)^{\\top} V^{\\top} = V (\\Sigma^{+} \\Sigma) V^{\\top} = A^{+} A$. The fourth condition is satisfied.\n\nThe derivation confirms that $A^{+} = V \\Sigma^{+} U^{\\top}$ is indeed the Moore-Penrose pseudoinverse. We now compute its $(2,3)$ entry, denoted $A^{+}_{23}$.\n$A^{+} = V \\Sigma^{+} U^{\\top}$. The entry $A^{+}_{23}$ is the dot product of the second row of the product $V\\Sigma^{+}$ and the third column of $U^{\\top}$.\n\nFirst, we find the second row of $V\\Sigma^{+}$:\n$$\n\\left( V\\Sigma^{+} \\right)_{2,:} = \\begin{pmatrix} -\\frac{4}{5} & \\frac{3}{5} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{6} & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 \\end{pmatrix} = \\begin{pmatrix} \\left(-\\frac{4}{5}\\right)\\left(\\frac{1}{6}\\right) & \\left(-\\frac{4}{5}\\right)(0) + \\left(\\frac{3}{5}\\right)\\left(\\frac{1}{2}\\right) & 0 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} -\\frac{4}{30} & \\frac{3}{10} & 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{2}{15} & \\frac{3}{10} & 0 \\end{pmatrix}.\n$$\nNext, we determine the third column of $U^{\\top}$. Transposing $U$ gives:\n$$\nU^{\\top} \\;=\\;\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & 0 & -\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\\\\n0 & 1 & 0\n\\end{bmatrix}.\n$$\nThe third column of $U^{\\top}$ is $\\begin{pmatrix} -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix}^{\\top}$.\n\nFinally, we compute the dot product to find $A^{+}_{23}$:\n$$\nA^{+}_{23} = \\left(-\\frac{2}{15}\\right) \\left(-\\frac{1}{\\sqrt{2}}\\right) + \\left(\\frac{3}{10}\\right) \\left(\\frac{1}{\\sqrt{2}}\\right) + (0)(0)\n$$\n$$\nA^{+}_{23} = \\frac{2}{15\\sqrt{2}} + \\frac{3}{10\\sqrt{2}}.\n$$\nTo combine these terms, we use a common denominator of $30\\sqrt{2}$:\n$$\nA^{+}_{23} = \\frac{4}{30\\sqrt{2}} + \\frac{9}{30\\sqrt{2}} = \\frac{13}{30\\sqrt{2}}.\n$$\nRationalizing the denominator yields the final closed-form expression:\n$$\nA^{+}_{23} = \\frac{13\\sqrt{2}}{30 \\times 2} = \\frac{13\\sqrt{2}}{60}.\n$$",
            "answer": "$$\\boxed{\\frac{13\\sqrt{2}}{60}}$$"
        },
        {
            "introduction": "Beyond the mechanics of computation, the power of the pseudoinverse lies in its geometric interpretation. The matrix products $A A^{+}$ and $A^{+} A$ are not merely incidental; they are orthogonal projectors onto the fundamental subspaces of $A$. This exercise uses a concrete counterexample to illustrate that for non-normal matrices, these projectors are distinct, and it challenges you to characterize their respective range and nullspace, deepening your insight into how $A^{+}$ interacts with the geometry of the linear transformation defined by $A$ .",
            "id": "3592297",
            "problem": "Let $A \\in \\mathbb{R}^{2 \\times 2}$ be the singular matrix\n$$\nA \\;=\\; \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nUsing only the existence of the singular value decomposition (SVD) for any real matrix and the definition of the Moore–Penrose (MP) pseudoinverse via the SVD, carry out the following steps from first principles:\n\n1. Compute an SVD $A = U \\Sigma V^{\\top}$, identifying the nonzero singular value and corresponding singular vectors. Use this to construct the MP pseudoinverse $A^{+} = V \\Sigma^{+} U^{\\top}$.\n\n2. Compute explicitly the two matrices $A A^{+}$ and $A^{+} A$. Verify directly (by computation) that $A A^{+} \\neq A^{+} A$.\n\n3. Determine the ranges and nullspaces of $A A^{+}$ and $A^{+} A$ by direct linear-algebraic analysis (solve for the column spaces and the solutions to the associated homogeneous systems). State each as a span of vectors in $\\mathbb{R}^{2}$.\n\n4. Let $\\|\\cdot\\|_{F}$ denote the Frobenius norm, defined by $\\|M\\|_{F}^{2} = \\sum_{i,j} M_{ij}^{2}$ for any real matrix $M$. Compute the scalar quantity $\\|A A^{+} - A^{+} A\\|_{F}^{2}$.\n\nYour final answer must be the single real number obtained in step $4$. No rounding is required.",
            "solution": "We proceed from the existence of the singular value decomposition (SVD) and its use in defining the Moore–Penrose (MP) pseudoinverse. For any real matrix $A$, there exist orthogonal matrices $U$ and $V$ and a diagonal matrix $\\Sigma$ with nonnegative diagonal entries (the singular values) such that $A = U \\Sigma V^{\\top}$. If $\\Sigma$ has diagonal entries $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge 0$, then the MP pseudoinverse is defined by $A^{+} = V \\Sigma^{+} U^{\\top}$, where $\\Sigma^{+}$ replaces each nonzero $\\sigma_{i}$ by $\\sigma_{i}^{-1}$ and leaves zeros as zeros.\n\nStep 1: Compute an SVD of $A$ and then $A^{+}$.\n\nWe have\n$$\nA = \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nCompute $A A^{\\top}$ and $A^{\\top} A$:\n$$\nA A^{\\top} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 0 \\end{pmatrix}, \n\\qquad\nA^{\\top} A = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}.\n$$\nThe eigenvalues of $A A^{\\top}$ are $2$ and $0$, so the singular values are $\\sigma_{1} = \\sqrt{2}$ and $\\sigma_{2} = 0$. A unit eigenvector of $A A^{\\top}$ for the eigenvalue $2$ is $u_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, and a unit eigenvector for the eigenvalue $0$ is $u_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. Thus we can take\n$$\nU = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \n\\qquad\n\\Sigma = \\begin{pmatrix} \\sqrt{2} & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nThe corresponding right singular vector $v_{1}$ is obtained from $v_{1} = \\frac{1}{\\sigma_{1}} A^{\\top} u_{1}$:\n$$\nA^{\\top} u_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \n\\quad\nv_{1} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nChoose $v_{2}$ as a unit vector orthogonal to $v_{1}$, for instance\n$$\nv_{2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\nHence\n$$\nV = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\end{pmatrix}.\n$$\nThe pseudoinverse is then\n$$\n\\Sigma^{+} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 \\end{pmatrix},\n\\qquad\nA^{+} = V \\Sigma^{+} U^{\\top} = V \\Sigma^{+}.\n$$\nCompute $A^{+}$ explicitly:\n$$\nA^{+} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\end{pmatrix}\n\\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 \\end{pmatrix}\n=\n\\begin{pmatrix} \\frac{1}{2} & 0 \\\\ \\frac{1}{2} & 0 \\end{pmatrix}.\n$$\n\nStep 2: Compute $A A^{+}$ and $A^{+} A$ and verify they are unequal.\n\nFirst,\n$$\nA A^{+} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix}\n\\begin{pmatrix} \\frac{1}{2} & 0 \\\\ \\frac{1}{2} & 0 \\end{pmatrix}\n=\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nNext,\n$$\nA^{+} A = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ \\frac{1}{2} & 0 \\end{pmatrix}\n\\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix}\n=\n\\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}.\n$$\nThese two matrices are clearly not equal, so $A A^{+} \\neq A^{+} A$.\n\nStep 3: Determine ranges and nullspaces of $A A^{+}$ and $A^{+} A$.\n\nFor $A A^{+}$:\n$$\nA A^{+} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nThe range is the column space of $A A^{+}$, namely\n$$\n\\operatorname{range}(A A^{+}) = \\operatorname{span}\\!\\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\}.\n$$\nThe nullspace is the set of $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$ such that $(A A^{+}) x = 0$, i.e.,\n$$\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\;\\;\\Longrightarrow\\;\\; x_{1} = 0,\n$$\nso\n$$\n\\operatorname{null}(A A^{+}) = \\operatorname{span}\\!\\left\\{ \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right\\}.\n$$\n\nFor $A^{+} A$:\n$$\nA^{+} A = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}.\n$$\nIts range is the span of its columns; both columns are equal to $\\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, hence\n$$\n\\operatorname{range}(A^{+} A) = \\operatorname{span}\\!\\left\\{ \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right\\}.\n$$\nIts nullspace consists of $x$ such that $(A^{+} A) x = 0$, i.e.,\n$$\n\\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\;\\;\\Longrightarrow\\;\\; x_{1} + x_{2} = 0,\n$$\nso\n$$\n\\operatorname{null}(A^{+} A) = \\operatorname{span}\\!\\left\\{ \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\right\\}.\n$$\n\nStep 4: Compute $\\|A A^{+} - A^{+} A\\|_{F}^{2}$.\n\nForm the difference\n$$\nD \\;=\\; A A^{+} - A^{+} A \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{2} \\\\ -\\frac{1}{2} & -\\frac{1}{2} \\end{pmatrix}.\n$$\nThe squared Frobenius norm is the sum of squares of the entries:\n$$\n\\|D\\|_{F}^{2} \\;=\\; \\left(\\frac{1}{2}\\right)^{2} + \\left(-\\frac{1}{2}\\right)^{2} + \\left(-\\frac{1}{2}\\right)^{2} + \\left(-\\frac{1}{2}\\right)^{2}\n\\;=\\; \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4}\n\\;=\\; 1.\n$$\nTherefore, the requested scalar is $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Translating linear algebra theory into robust code requires confronting the realities of floating-point arithmetic. A key challenge in computing the pseudoinverse is determining the numerical rank of a matrix, as singular values that are theoretically zero may appear as small non-zero numbers due to roundoff error. This practice guides you through the design and validation of a numerically stable algorithm that implements a tolerance-based approach, a critical skill for any practitioner of computational linear algebra .",
            "id": "3592271",
            "problem": "Design a numerically stable algorithm to compute the Moore–Penrose pseudoinverse $A^{+}$ of a real matrix $A$ using the singular value decomposition, with an explicit tolerance $\\tau$ for numerical rank determination. Start from core definitions and well-tested facts in numerical linear algebra. Your algorithm must decide which singular values are treated as numerically nonzero via the inequality $\\,\\sigma_{i} > \\tau\\,$ and must construct $A^{+}$ accordingly without relying on any shortcut formulas in the problem statement. In your reasoning and implementation you must justify the choice of $\\tau$ in terms of floating-point unit roundoff $u$ and the spectral norm $\\lVert A \\rVert_{2}$. You must do the following:\n\n- Use the definition of the Moore–Penrose pseudoinverse via the Penrose conditions and the singular value decomposition factorization as the fundamental base.\n- Argue from the floating-point model with unit roundoff $u$ and standard bounds on backward error for the singular value decomposition and for matrix multiplication, and from norms induced by the Euclidean norm, to justify a tolerance of the form $\\tau = \\alpha\\,u\\,\\lVert A \\rVert_{2}$ for a dimension-dependent factor $\\alpha$.\n- Implement an algorithm that computes $A^{+}$ using singular value decomposition with this tolerance and verifies the Penrose conditions numerically.\n\nThe program must operate in purely mathematical terms. No physical units are involved. All angles, if any, are to be treated as dimensionless real numbers.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, you must output a boolean that is $\\texttt{True}$ exactly when all required numerical validations pass and $\\texttt{False}$ otherwise. The validations per test case are:\n\n- Verification of the four Penrose conditions using the spectral norm, namely $A A^{+} A = A$, $A^{+} A A^{+} = A^{+}$, $(A A^{+})^{\\mathsf{T}} = A A^{+}$, and $(A^{+} A)^{\\mathsf{T}} = A^{+} A$, within a tolerance derived from the floating-point unit roundoff. Concretely, define the residuals\n$$\n\\rho_{1} = \\frac{\\lVert A A^{+} A - A \\rVert_{2}}{\\max(1, \\lVert A \\rVert_{2})},\\quad\n\\rho_{2} = \\frac{\\lVert A^{+} A A^{+} - A^{+} \\rVert_{2}}{\\max(1, \\lVert A^{+} \\rVert_{2})},\n$$\n$$\n\\rho_{3} = \\frac{\\lVert (A A^{+})^{\\mathsf{T}} - A A^{+} \\rVert_{2}}{\\max(1, \\lVert A A^{+} \\rVert_{2})},\\quad\n\\rho_{4} = \\frac{\\lVert (A^{+} A)^{\\mathsf{T}} - A^{+} A \\rVert_{2}}{\\max(1, \\lVert A^{+} A \\rVert_{2})}.\n$$\nDeclare the Penrose conditions verified if $\\rho_{j} \\le c\\,u$ for all $j \\in \\{1,2,3,4\\}$, where $c = 100\\,\\max(m,n)$ and $m$ and $n$ are the matrix dimensions of $A$.\n\n- Verification that the numerical rank equals the count of singular values strictly larger than the tolerance $\\tau$ computed as $\\tau = \\max(m,n)\\,u\\,\\lVert A \\rVert_{2}$.\n\nTest Suite and construction details:\n\nYou must implement the following deterministic test suite. For each test case, construct $A$ with a prescribed singular spectrum by composing orthonormal factors and a diagonal singular value matrix. Use a fixed pseudorandom seed $2025$ everywhere that randomness is required so that all constructions are deterministic.\n\nFor a given dimension pair $(m,n)$ and a singular value list $\\{\\sigma_{1},\\dots,\\sigma_{r}\\}$ with $r \\le \\min(m,n)$, construct $A$ as $A = U_{m \\times r}\\,\\Sigma_{r \\times r}\\,V_{n \\times r}^{\\mathsf{T}}$, where $U_{m \\times r}$ and $V_{n \\times r}$ have orthonormal columns and $\\Sigma_{r \\times r} = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{r})$. When noise is specified, add a perturbation $\\Delta A$ with spectral norm prescribed below. In all cases, the program must compute $A^{+}$ via singular value decomposition with the tolerance rule above and validate the conditions described.\n\nProvide the following test cases:\n\n- Test case $1$ (happy path, square well-conditioned): $m = 5$, $n = 5$, singular values $\\{4, 3, 2, 1, 1/2\\}$, no noise.\n- Test case $2$ (tall, rank-deficient): $m = 6$, $n = 4$, singular values $\\{12, 8, 5\\}$, no noise. The exact rank is $3$.\n- Test case $3$ (wide, with small singular values near roundoff scale): $m = 4$, $n = 6$, singular values $\\{5, 1000\\,u, 10^{-16}, 0\\}$, no noise. The expected numerical rank under the tolerance rule is $2$ because $\\tau \\approx \\max(m,n)\\,u\\,\\lVert A \\rVert_{2} \\approx 6\\,u\\,5$, so $\\sigma_{1}$ and $\\sigma_{2}$ are above $\\tau$ and the rest are not.\n- Test case $4$ (zero matrix edge case): $m = 3$, $n = 7$, singular values $\\{\\}$, no noise. The numerical rank is $0$ and $A^{+}$ should be the zero matrix of size $n \\times m$.\n- Test case $5$ (noise below tolerance scale): $m = 7$, $n = 5$, singular values $\\{3, 2, 1\\}$, add a perturbation $\\Delta A$ with spectral norm $\\lVert \\Delta A \\rVert_{2} = 1 \\cdot u \\cdot \\lVert A \\rVert_{2}$. Under the tolerance rule with $\\tau = \\max(m,n)\\,u\\,\\lVert A \\rVert_{2}$, the numerical rank should remain $3$ because the noise level is strictly below $\\tau$.\n\nYour program must:\n\n- Compute $A^{+}$ by singular value decomposition using the tolerance $\\tau = \\max(m,n)\\,u\\,\\lVert A \\rVert_{2}$ for rank determination.\n- For each test case, report a boolean that is $\\texttt{True}$ if and only if all four Penrose residuals satisfy $\\rho_{j} \\le c\\,u$ with $c = 100\\,\\max(m,n)$ and if the numerical rank equals the count of singular values strictly larger than $\\tau$ computed from the ground-truth singular spectrum used in the construction of $A$.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\texttt{[True,False,True,True,True]}$.\n\nAll mathematical symbols and numbers in this problem statement, including dimension sizes, tolerances, and constants, are expressed in LaTeX as required.",
            "solution": "The problem requires the design, justification, and implementation of a numerically stable algorithm for computing the Moore-Penrose pseudoinverse $A^{+}$ of a real matrix $A \\in \\mathbb{R}^{m \\times n}$. The core of the method is the Singular Value Decomposition (SVD) combined with a carefully justified tolerance for determining the numerical rank. The solution must be validated against the defining Penrose conditions and a specific rank consistency check.\n\n### Theoretical Foundation: The Moore-Penrose Pseudoinverse\n\nFor any real matrix $A \\in \\mathbb{R}^{m \\times n}$, the Moore-Penrose pseudoinverse is the unique matrix $A^{+} \\in \\mathbb{R}^{n \\times m}$ that satisfies the following four Penrose conditions:\n1.  $A A^{+} A = A$\n2.  $A^{+} A A^{+} = A^{+}$\n3.  $(A A^{+})^{\\mathsf{T}} = A A^{+}$ (The product $A A^{+}$ is a symmetric matrix)\n4.  $(A^{+} A)^{\\mathsf{T}} = A^{+} A$ (The product $A^{+} A$ is a symmetric matrix)\n\nGeometrically, $A A^{+}$ is the orthogonal projector onto the range of $A$ (its column space), and $A^{+} A$ is the orthogonal projector onto the range of $A^{\\mathsf{T}}$ (its row space).\n\nThe Singular Value Decomposition (SVD) provides a direct and numerically stable route to construct $A^{+}$. The SVD of $A$ is the factorization $A = U \\Sigma V^{\\mathsf{T}}$, where:\n-   $U \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns are the left singular vectors of $A$.\n-   $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix whose columns are the right singular vectors of $A$.\n-   $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix whose diagonal entries $\\Sigma_{ii} = \\sigma_i$ are the singular values of $A$, ordered non-increasingly: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{\\min(m,n)} \\ge 0$.\n\nGiven this factorization, the pseudoinverse $A^{+}$ is defined as $A^{+} = V \\Sigma^{+} U^{\\mathsf{T}}$. The matrix $\\Sigma^{+} \\in \\mathbb{R}^{n \\times m}$ is constructed by transposing $\\Sigma$ and taking the reciprocal of its non-zero diagonal entries. Specifically, if the rank of $A$ is $r$, then $\\sigma_1, \\dots, \\sigma_r > 0$ and $\\sigma_{i}=0$ for $i > r$. The diagonal entries of $\\Sigma^{+}$ are then given by:\n$$\n(\\Sigma^{+})_{ii} = \\begin{cases} 1/\\sigma_i & \\text{if } \\sigma_i > 0 \\\\ 0 & \\text{if } \\sigma_i = 0 \\end{cases}\n$$\n\n### Numerical Stability and Rank Determination\n\nIn finite-precision floating-point arithmetic, the distinction between a very small non-zero singular value and a true zero is blurred by roundoff error. Standard backward error analysis for the SVD shows that the computed factorization $\\hat{U} \\hat{\\Sigma} \\hat{V}^{\\mathsf{T}}$ is the exact SVD of a nearby matrix $A+E$, where the norm of the perturbation $E$ is bounded by $\\|E\\|_2 \\le p(m,n) u \\|A\\|_2$. Here, $u$ is the unit roundoff of the floating-point system, and $p(m,n)$ is a low-degree polynomial in the matrix dimensions $m$ and $n$.\n\nBy the Weyl perturbation theorem for singular values, the computed singular values $\\hat{\\sigma}_i$ differ from the true singular values $\\sigma_i$ of $A$ by at most $\\|E\\|_2$. Thus, $|\\hat{\\sigma}_i - \\sigma_i| \\le p(m,n) u \\|A\\|_2$. This implies that any singular value smaller than this bound is computationally indistinguishable from zero. Inverting such small values would be numerically catastrophic, as it would amplify noise by a large factor ($1/\\sigma_i$).\n\nThis necessitates the concept of *numerical rank*: the number of singular values considered to be numerically distinct from zero. We define the numerical rank $k$ by counting the singular values that exceed a certain tolerance, $\\tau$. A standard, well-justified choice for this tolerance is of the form $\\tau = \\alpha u \\|A\\|_2$, where $\\|A\\|_2 = \\sigma_1$ is the largest singular value. The problem specifies the choice $\\tau = \\max(m,n) u \\|A\\|_2$. This choice sets the proportionality constant $\\alpha = \\max(m,n)$, which is a simple and effective heuristic for the dimension-dependent factor $p(m,n)$ in the backward error bound.\n\n### Algorithm Design for Computing $A^{+}$\n\nThe algorithm to compute $A^{+}$ proceeds as follows:\n1.  Given the input matrix $A \\in \\mathbb{R}^{m \\times n}$.\n2.  Compute the SVD of $A$, yielding $U$, the vector of singular values $s$, and $V^{\\mathsf{T}}$. For numerical efficiency, a \"thin\" SVD is sufficient, where $U$ is $m \\times k$, $s$ is a vector of length $k$, and $V^{\\mathsf{T}}$ is $k \\times n$, with $k=\\min(m,n)$.\n3.  Determine the spectral norm $\\|A\\|_2$. If $s$ is not empty, $\\|A\\|_2 = s_1$; otherwise, $\\|A\\|_2 = 0$.\n4.  Calculate the numerical tolerance $\\tau = \\max(m,n) u \\|A\\|_2$, where $u$ is the machine epsilon for the floating-point type.\n5.  Determine the numerical rank, denoted $k_{\\text{num}}$, which is the number of singular values $s_i$ such that $s_i > \\tau$.\n6.  Construct a new vector $s_{\\text{inv}}$ of the same size as $s$. For each $s_i$, the corresponding element $(s_{\\text{inv}})_i$ is $1/s_i$ if $s_i > \\tau$, and $0$ otherwise.\n7.  Construct the pseudoinverse $A^{+}$ using the components of the SVD and $s_{\\text{inv}}$: $A^{+} = V \\cdot \\text{diag}(s_{\\text{inv}}) \\cdot U^{\\mathsf{T}}$.\n\n### Verification Protocol\n\nThe problem mandates a strict verification protocol for each test case.\n1.  **Rank Verification**: The numerical rank computed by the algorithm, $k_{\\text{num}}$, must be equal to a reference rank. This reference rank is defined as the number of singular values from the ground-truth spectrum (used to construct the test matrix $A$) that are strictly greater than the tolerance $\\tau$ computed for the final matrix $A$. This test verifies that the rank-determination step is robust to perturbations introduced during matrix construction and (if applicable) noise addition.\n2.  **Penrose Conditions Verification**: The four Penrose conditions must be checked numerically. This is accomplished by computing the residuals $\\rho_1, \\rho_2, \\rho_3, \\rho_4$ as specified in the problem statement. Each residual is the spectral norm of the error, normalized by the norm of the expected quantity (or $1$ if the norm is small) to make the comparison scale-invariant. A test passes if all four residuals satisfy $\\rho_j \\le c u$, where the tolerance factor is $c = 100 \\max(m,n)$. This large factor accounts for the accumulation of floating-point errors during the matrix multiplications involved in forming the residuals themselves. A successful verification of these conditions confirms that the computed matrix $A^{+}$ indeed behaves as the true Moore-Penrose pseudoinverse within a reasonable numerical tolerance.\n\nThe logical union of these two verification outcomes (rank and Penrose) determines the boolean result for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for Moore-Penrose pseudoinverse computation.\n    \"\"\"\n    \n    # Unit roundoff for double precision floating-point arithmetic\n    # Note: np.finfo(float).eps is machine epsilon (2^-52), while unit roundoff is technically eps/2 (2^-53).\n    # Using eps is a common practice and is absorbed by the heuristic constants in the tolerances.\n    u = np.finfo(float).eps\n    \n    # Fixed seed for deterministic test case generation\n    rng = np.random.default_rng(2025)\n\n    test_cases = [\n        {'m': 5, 'n': 5, 'sv': [4, 3, 2, 1, 0.5], 'noise_factor': 0},\n        {'m': 6, 'n': 4, 'sv': [12, 8, 5], 'noise_factor': 0},\n        {'m': 4, 'n': 6, 'sv': [5, 1000 * u, 1e-16, 0], 'noise_factor': 0},\n        {'m': 3, 'n': 7, 'sv': [], 'noise_factor': 0},\n        {'m': 7, 'n': 5, 'sv': [3, 2, 1], 'noise_factor': 1.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        m, n, sv_list, noise_factor = case['m'], case['n'], case['sv'], case['noise_factor']\n        \n        # --- Matrix Construction ---\n        r = len(sv_list)\n        if r > 0:\n            # Generate U0 and V0 with orthonormal columns\n            U0_rand = rng.standard_normal((m, r))\n            U0, _ = np.linalg.qr(U0_rand)\n            \n            V0_rand = rng.standard_normal((n, r))\n            V0, _ = np.linalg.qr(V0_rand)\n            \n            Sigma0 = np.diag(sv_list)\n            A0 = U0 @ Sigma0 @ V0.T\n        else:\n            A0 = np.zeros((m, n))\n\n        # Add noise if specified\n        A = A0\n        if noise_factor > 0:\n            norm_A0 = sv_list[0] if r > 0 else 0\n            if norm_A0 > 0:\n                E = rng.standard_normal((m, n))\n                norm_E = np.linalg.norm(E, 2)\n                if norm_E > 1e-12: # Avoid division by zero for unlikely zero random matrix\n                    delta_A = E / norm_E * (noise_factor * u * norm_A0)\n                    A = A0 + delta_A\n        \n        # --- Pseudoinverse Calculation and Verification ---\n        is_valid = calculate_and_verify(A, m, n, np.array(sv_list), u, rng)\n        results.append(is_valid)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef calculate_and_verify(A, m, n, ground_truth_sv, u, rng):\n    \"\"\"\n    Computes the pseudoinverse of A and verifies the required conditions.\n    \n    Returns:\n        bool: True if all validations pass, False otherwise.\n    \"\"\"\n    \n    # --- 1. Compute Pseudoinverse A_plus and its Numerical Rank ---\n    # Use 'full_matrices=False' for thin SVD efficiency\n    U, s, Vh = np.linalg.svd(A, full_matrices=False)\n    \n    norm_A = s[0] if s.size > 0 else 0.0\n    tau = max(m, n) * u * norm_A\n    \n    computed_rank = np.sum(s > tau)\n    \n    s_inv = np.where(s > tau, 1.0 / s, 0.0)\n    \n    # Construct Sigma_plus of appropriate size k x k where k=len(s)\n    Sigma_plus = np.diag(s_inv)\n    \n    # A_plus = V @ Sigma_plus @ U.T\n    # Vh is V.T, so V = Vh.T\n    # Dimensions: Vh.T is n x k, Sigma_plus is k x k, U.T is k x m\n    k_svd = Vh.shape[0]\n    A_plus = (Vh.T[:, :k_svd]) @ Sigma_plus @ (U.T[:k_svd, :])\n\n    # --- 2. Rank Verification ---\n    # The reference rank is based on ground-truth singular values vs tolerance from final matrix A\n    reference_rank = np.sum(ground_truth_sv > tau)\n    rank_ok = (computed_rank == reference_rank)\n    \n    # --- 3. Penrose Conditions Verification ---\n    c_penrose = 100 * max(m, n)\n    tol_penrose = c_penrose * u\n    \n    # Pre-compute norms to avoid re-calculation and handle zero matrices\n    # norm_A is already computed\n    norm_A_plus = np.linalg.norm(A_plus, 2)\n    \n    # rho_1: || A A+ A - A ||_2 / max(1, ||A||_2)\n    res1 = A @ A_plus @ A - A\n    rho1 = np.linalg.norm(res1, 2) / max(1.0, norm_A)\n    \n    # rho_2: || A+ A A+ - A+ ||_2 / max(1, ||A+||_2)\n    res2 = A_plus @ A @ A_plus - A_plus\n    rho2 = np.linalg.norm(res2, 2) / max(1.0, norm_A_plus)\n    \n    # rho_3: || (A A+)^T - (A A+) ||_2 / max(1, ||A A+||_2)\n    P3 = A @ A_plus\n    res3 = P3.T - P3\n    rho3 = np.linalg.norm(res3, 2) / max(1.0, np.linalg.norm(P3, 2))\n\n    # rho_4: || (A+ A)^T - (A+ A) ||_2 / max(1, ||A+ A||_2)\n    P4 = A_plus @ A\n    res4 = P4.T - P4\n    rho4 = np.linalg.norm(res4, 2) / max(1.0, np.linalg.norm(P4, 2))\n    \n    penrose_ok = (rho1 = tol_penrose) and \\\n                 (rho2 = tol_penrose) and \\\n                 (rho3 = tol_penrose) and \\\n                 (rho4 = tol_penrose)\n                 \n    return rank_ok and penrose_ok\n    \nsolve()\n```"
        }
    ]
}