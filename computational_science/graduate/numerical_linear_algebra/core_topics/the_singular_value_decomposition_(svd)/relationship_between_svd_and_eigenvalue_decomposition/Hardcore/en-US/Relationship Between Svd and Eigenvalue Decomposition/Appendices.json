{
    "hands_on_practices": [
        {
            "introduction": "The determinant of a matrix offers a global perspective on its action as a linear transformation, representing the volume scaling factor. It is a fundamental result that the determinant equals the product of the eigenvalues, $\\det(A) = \\prod_i \\lambda_i$. This practice begins by guiding you to prove an equally fundamental relationship: the magnitude of the determinant is the product of the singular values, $|\\det(A)| = \\prod_i \\sigma_i$ . You will then explore a concrete example that highlights the crucial distinction between these two spectral views, demonstrating how a matrix can have a small determinant while simultaneously possessing large singular values, a hallmark of non-normal matrices that amplify some vectors significantly.",
            "id": "3573875",
            "problem": "Let $A \\in \\mathbb{C}^{n \\times n}$ be a square matrix. The singular value decomposition (SVD) writes $A = U \\Sigma V^{*}$, where $U \\in \\mathbb{C}^{n \\times n}$ and $V \\in \\mathbb{C}^{n \\times n}$ are unitary and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is diagonal with nonnegative entries, called the singular values. The eigenvalue decomposition (EVD) of a diagonalizable matrix writes $A = W \\Lambda W^{-1}$, where $W \\in \\mathbb{C}^{n \\times n}$ is invertible and $\\Lambda \\in \\mathbb{C}^{n \\times n}$ is diagonal with the eigenvalues $\\lambda_{i}$ on the diagonal. The determinant satisfies the multiplicativity property $\\det(AB) = \\det(A)\\det(B)$, and for any unitary matrix $Q$, $| \\det(Q) | = 1$. The Gram matrix $A^{*}A$ is Hermitian positive semidefinite, and its eigenvalues are the squares of the singular values of $A$.\n\nStarting from these foundational definitions and facts only, answer the following:\n\n1. Derive, without invoking any additional theorems, an expression linking $| \\det(A) |$ to the singular values of $A$. Your derivation must proceed from the properties stated above and must not assume any target relationship.\n2. Consider the $2 \\times 2$ matrix\n$$\nU = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & 1 \\\\ -1 & 1\\end{pmatrix}, \\qquad \\Sigma = \\begin{pmatrix}L & 0 \\\\ 0 & \\frac{\\epsilon}{L}\\end{pmatrix}, \\qquad V = I_{2},\n$$\nwith parameters $L > 1$ and $0 < \\epsilon \\ll 1$, and define\n$$\nA := U \\Sigma V^{*} = U \\Sigma.\n$$\nUsing only the definitions and facts stated at the beginning, compute the exact closed-form value of $| \\det(A) |$ in terms of $L$ and $\\epsilon$. Then, contrast this with the product $\\prod_{i}\\lambda_{i}$ by explicitly computing the eigenvalues of $A$ via its characteristic polynomial and characterizing their asymptotic magnitudes as $L \\to \\infty$ with $\\epsilon$ fixed. Your final reported answer must be the exact expression for $| \\det(A) |$ in terms of $L$ and $\\epsilon$.",
            "solution": "The problem is evaluated in two parts as requested. First, a general derivation is performed, followed by an analysis of a specific matrix.\n\n### Part 1: Derivation of the relationship between $| \\det(A) |$ and singular values\n\nLet $A \\in \\mathbb{C}^{n \\times n}$ be a square matrix. The problem provides the singular value decomposition (SVD) of $A$ as\n$$A = U \\Sigma V^{*}$$\nwhere $U, V \\in \\mathbb{C}^{n \\times n}$ are unitary matrices and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix whose diagonal entries $\\sigma_1, \\sigma_2, \\ldots, \\sigma_n$ are the singular values of $A$. By definition, $\\sigma_i \\ge 0$ for all $i=1, \\ldots, n$.\n\nTo find an expression for $|\\det(A)|$, we start by taking the determinant of the SVD equation. Using the multiplicative property of determinants, which is provided as $\\det(AB) = \\det(A)\\det(B)$, we have:\n$$\\det(A) = \\det(U \\Sigma V^{*}) = \\det(U) \\det(\\Sigma) \\det(V^{*})$$\nNow, we take the absolute value of both sides:\n$$|\\det(A)| = |\\det(U) \\det(\\Sigma) \\det(V^{*})|$$\nUsing the property that the absolute value of a product is the product of the absolute values, we get:\n$$|\\det(A)| = |\\det(U)| |\\det(\\Sigma)| |\\det(V^{*})|$$\nThe problem states that for any unitary matrix $Q$, $|\\det(Q)|=1$. Since $U$ and $V$ are unitary matrices, we have $|\\det(U)|=1$ and $|\\det(V)|=1$. The adjoint of a unitary matrix is also unitary, since $(V^{*})(V^{*})^{*} = V^{*}V = I$. Thus, $|\\det(V^{*})| = 1$. Substituting these values into the equation gives:\n$$|\\det(A)| = 1 \\cdot |\\det(\\Sigma)| \\cdot 1 = |\\det(\\Sigma)|$$\nThe matrix $\\Sigma$ is a diagonal matrix with the nonnegative singular values $\\sigma_i$ on its diagonal. The determinant of a diagonal matrix is the product of its diagonal entries:\n$$\\det(\\Sigma) = \\prod_{i=1}^{n} \\sigma_i$$\nSince all singular values are nonnegative, $\\sigma_i \\ge 0$, their product is also nonnegative, $\\prod_{i=1}^{n} \\sigma_i \\ge 0$. Therefore, the absolute value of the determinant of $\\Sigma$ is simply the determinant itself:\n$$|\\det(\\Sigma)| = \\det(\\Sigma) = \\prod_{i=1}^{n} \\sigma_i$$\nCombining the results, we arrive at the expression linking the absolute value of the determinant of $A$ to its singular values:\n$$|\\det(A)| = \\prod_{i=1}^{n} \\sigma_i$$\n\n### Part 2: Analysis of the specific $2 \\times 2$ matrix\n\nWe are given the matrix $A$ defined by its SVD components:\n$$U = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & 1 \\\\ -1 & 1\\end{pmatrix}, \\qquad \\Sigma = \\begin{pmatrix}L & 0 \\\\ 0 & \\frac{\\epsilon}{L}\\end{pmatrix}, \\qquad V = I_{2}$$\nand $A := U \\Sigma V^{*} = U \\Sigma$. The parameters are $L > 1$ and $0 < \\epsilon \\ll 1$.\n\nFirst, we compute the exact closed-form value of $|\\det(A)|$. From the derivation in Part 1, we know that $|\\det(A)|$ is the product of the singular values of $A$. The singular values are given as the diagonal entries of the matrix $\\Sigma$.\nThe singular values are $\\sigma_1 = L$ and $\\sigma_2 = \\frac{\\epsilon}{L}$.\nTherefore, the absolute value of the determinant of $A$ is:\n$$|\\det(A)| = \\sigma_1 \\sigma_2 = L \\cdot \\frac{\\epsilon}{L} = \\epsilon$$\n\nNext, we contrast this value with the product of the eigenvalues, $\\prod_{i}\\lambda_{i}$. A fundamental property of eigenvalues is that the determinant of a matrix is equal to the product of its eigenvalues, i.e., $\\det(A) = \\prod_{i}\\lambda_{i}$.\nTo verify this for the given matrix, we can compute $\\det(A)$ directly from its definition $A=U\\Sigma$.\n$$\\det(A) = \\det(U\\Sigma) = \\det(U)\\det(\\Sigma)$$\nWe compute the determinants of $U$ and $\\Sigma$:\n$$\\det(U) = \\det\\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & 1 \\\\ -1 & 1\\end{pmatrix}\\right) = \\left(\\frac{1}{\\sqrt{2}}\\right)^2 \\det\\begin{pmatrix}1 & 1 \\\\ -1 & 1\\end{pmatrix} = \\frac{1}{2}((1)(1) - (1)(-1)) = \\frac{1}{2}(1+1) = 1$$\n$$\\det(\\Sigma) = \\det\\begin{pmatrix}L & 0 \\\\ 0 & \\frac{\\epsilon}{L}\\end{pmatrix} = L \\cdot \\frac{\\epsilon}{L} - 0 \\cdot 0 = \\epsilon$$\nThus, $\\det(A) = 1 \\cdot \\epsilon = \\epsilon$.\nSince $\\prod_{i}\\lambda_{i} = \\det(A)$, we have $\\prod_{i}\\lambda_{i} = \\epsilon$.\nAs $\\epsilon > 0$, we have $|\\det(A)| = |\\epsilon| = \\epsilon$.\nThe contrast is that the value of $|\\det(A)|$ ($\\epsilon$) and the value of $\\prod_{i}\\lambda_{i}$ ($\\epsilon$) are identical.\n\nTo further characterize the relationship, we compute the eigenvalues of $A$ explicitly. First, we construct the matrix $A$:\n$$A = U\\Sigma = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & 1 \\\\ -1 & 1\\end{pmatrix} \\begin{pmatrix}L & 0 \\\\ 0 & \\frac{\\epsilon}{L}\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}L & \\frac{\\epsilon}{L} \\\\ -L & \\frac{\\epsilon}{L}\\end{pmatrix}$$\nThe eigenvalues $\\lambda$ are the roots of the characteristic polynomial $\\det(A - \\lambda I) = 0$.\n$$\\det\\left(\\begin{pmatrix}\\frac{L}{\\sqrt{2}} & \\frac{\\epsilon}{L\\sqrt{2}} \\\\ -\\frac{L}{\\sqrt{2}} & \\frac{\\epsilon}{L\\sqrt{2}}\\end{pmatrix} - \\begin{pmatrix}\\lambda & 0 \\\\ 0 & \\lambda\\end{pmatrix}\\right) = 0$$\n$$\\det\\begin{pmatrix}\\frac{L}{\\sqrt{2}} - \\lambda & \\frac{\\epsilon}{L\\sqrt{2}} \\\\ -\\frac{L}{\\sqrt{2}} & \\frac{\\epsilon}{L\\sqrt{2}} - \\lambda\\end{pmatrix} = 0$$\n$$ \\left(\\frac{L}{\\sqrt{2}} - \\lambda\\right)\\left(\\frac{\\epsilon}{L\\sqrt{2}} - \\lambda\\right) - \\left(\\frac{\\epsilon}{L\\sqrt{2}}\\right)\\left(-\\frac{L}{\\sqrt{2}}\\right) = 0 $$\n$$ \\frac{L\\epsilon}{2} - \\frac{L\\lambda}{\\sqrt{2}} - \\frac{\\epsilon\\lambda}{L\\sqrt{2}} + \\lambda^2 + \\frac{L\\epsilon}{2} = 0 $$\n$$ \\lambda^2 - \\left(\\frac{L}{\\sqrt{2}} + \\frac{\\epsilon}{L\\sqrt{2}}\\right)\\lambda + \\epsilon = 0 $$\n$$ \\lambda^2 - \\left(\\frac{L^2+\\epsilon}{L\\sqrt{2}}\\right)\\lambda + \\epsilon = 0 $$\nThe product of the roots of this quadratic equation is the constant term, $\\lambda_1\\lambda_2 = \\epsilon$, confirming our earlier finding. The sum of the roots is $\\lambda_1+\\lambda_2 = \\frac{L^2+\\epsilon}{L\\sqrt{2}}$.\nThe discriminant of this quadratic equation is $D = \\left(\\frac{L^2+\\epsilon}{L\\sqrt{2}}\\right)^2 - 4\\epsilon = \\frac{L^4 + 2L^2\\epsilon + \\epsilon^2}{2L^2} - 4\\epsilon = \\frac{L^2}{2} + \\epsilon + \\frac{\\epsilon^2}{2L^2} - 4\\epsilon = \\frac{L^2}{2} - 3\\epsilon + \\frac{\\epsilon^2}{2L^2}$.\nFor $L>1$ and $0 < \\epsilon \\ll 1$ (e.g., $L^2 > 6\\epsilon$), $D>0$, which means the eigenvalues $\\lambda_1$ and $\\lambda_2$ are real. Since their sum and product are positive, both eigenvalues are positive.\n\nWe characterize the asymptotic magnitudes of the eigenvalues as $L \\to \\infty$ with $\\epsilon$ fixed. The sum of eigenvalues $\\lambda_1+\\lambda_2 \\approx \\frac{L^2}{L\\sqrt{2}} = \\frac{L}{\\sqrt{2}}$, which tends to infinity. Since their product $\\lambda_1\\lambda_2 = \\epsilon$ is a small constant, one eigenvalue must be large and the other small. Let $\\lambda_1$ be the large eigenvalue.\n$\\lambda_1 \\approx \\lambda_1+\\lambda_2 = \\frac{L^2+\\epsilon}{L\\sqrt{2}} \\approx \\frac{L}{\\sqrt{2}}$.\nThe small eigenvalue is then $\\lambda_2 = \\frac{\\epsilon}{\\lambda_1} \\approx \\frac{\\epsilon}{L/\\sqrt{2}} = \\frac{\\epsilon\\sqrt{2}}{L}$.\n\nThe key contrast lies not between $|\\det(A)|$ and $\\prod\\lambda_i$, which are equal, but between the individual singular values and the magnitudes of the individual eigenvalues.\nSingular values: $\\sigma_1 = L$, $\\sigma_2 = \\frac{\\epsilon}{L}$.\nEigenvalue magnitudes (asymptotically for large $L$): $|\\lambda_1| = \\lambda_1 \\approx \\frac{L}{\\sqrt{2}}$, $|\\lambda_2| = \\lambda_2 \\approx \\frac{\\epsilon\\sqrt{2}}{L}$.\nThe matrix $A$ is not normal since $A^{*}A \\neq AA^{*}$, which is why its singular values do not coincide with the magnitudes of its eigenvalues. We observe that for large $L$:\n$$\\sigma_1 \\approx \\sqrt{2}|\\lambda_1| \\quad \\text{and} \\quad \\sigma_2 \\approx \\frac{1}{\\sqrt{2}}|\\lambda_2|$$\nDespite this disparity between the individual values, the product of the singular values remains equal to the product of the eigenvalues:\n$$\\sigma_1 \\sigma_2 = L \\cdot \\frac{\\epsilon}{L} = \\epsilon$$\n$$\\lambda_1 \\lambda_2 = \\epsilon$$\nThis is a manifestation of the general identities $|\\det(A)| = \\prod_{i=1}^n \\sigma_i$ and $\\det(A) = \\prod_{i=1}^n \\lambda_i$.\n\nThe final answer required is the exact closed-form value of $|\\det(A)|$. As computed, this is $\\epsilon$.",
            "answer": "$$\\boxed{\\epsilon}$$"
        },
        {
            "introduction": "Having established that individual singular values and eigenvalue magnitudes can diverge, we now push this concept to its limit with a fascinating thought experiment. This practice challenges you to construct and analyze a matrix that is nilpotent—meaning its eigenvalues are all zero and repeated application will eventually annihilate any vector—yet simultaneously acts as a perfect norm-preserving isometry on a specific subspace . Successfully building this matrix, known as a partial isometry, provides a profound insight into non-normality, where the spectral radius ($|\\lambda|_{\\max}=0$) offers no information about the matrix's norm ($\\|A\\|_2 = \\sigma_{\\max}=1$).",
            "id": "3573919",
            "problem": "Let $A \\in \\mathbb{C}^{n \\times n}$ satisfy that $A^{\\ast} A$ is idempotent, i.e., $(A^{\\ast} A)^{2} = A^{\\ast} A$. Use only foundational facts about the Singular Value Decomposition (SVD) and eigenvalue decomposition (EVD) to reason about the consequences for the singular values and eigenvalues of $A$.\n\n(a) Starting from the definition of the Singular Value Decomposition (SVD), $A = U \\Sigma V^{\\ast}$ with $U$ and $V$ unitary and $\\Sigma$ diagonal with nonnegative entries, and the eigenvalue decomposition (EVD) of the Hermitian positive semidefinite matrix $A^{\\ast} A$, derive what values the singular values of $A$ can take if $A^{\\ast} A$ is idempotent.\n\n(b) Construct an explicit nonzero $A \\in \\mathbb{C}^{4 \\times 4}$ such that $A^{\\ast} A$ is an orthogonal projector of rank $2$, yet $A$ is nilpotent of index $2$ (so all eigenvalues of $A$ are zero), and $A$ acts as a norm-preserving isometry on a two-dimensional subspace. Verify directly that $A^{2} = 0$, that $A^{\\ast} A$ is a projector, and determine the singular values of $A$ from first principles.\n\n(c) For your constructed matrix $A$ from part (b), define the Hermitian matrix $D := A^{\\ast} A - A A^{\\ast}$. Compute the characteristic polynomial of $D$ as a closed-form analytic expression in the variable $\\lambda$. Provide this characteristic polynomial as your final answer.",
            "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. We proceed with a detailed solution.\n\n**(a) Derivation of singular values**\n\nLet $A \\in \\mathbb{C}^{n \\times n}$ be a matrix. The Singular Value Decomposition (SVD) of $A$ is given by $A = U \\Sigma V^{\\ast}$, where $U, V \\in \\mathbb{C}^{n \\times n}$ are unitary matrices and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix with non-negative real entries, $\\Sigma = \\text{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_n)$, where $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_n \\geq 0$ are the singular values of $A$.\n\nWe consider the matrix $A^{\\ast} A$. Substituting the SVD of $A$:\n$$A^{\\ast} A = (U \\Sigma V^{\\ast})^{\\ast} (U \\Sigma V^{\\ast}) = (V \\Sigma^{\\ast} U^{\\ast})(U \\Sigma V^{\\ast})$$\nSince $\\Sigma$ is a real diagonal matrix, $\\Sigma^{\\ast} = \\Sigma$. As $U$ is unitary, $U^{\\ast}U = I$, where $I$ is the identity matrix. Therefore,\n$$A^{\\ast} A = V \\Sigma (U^{\\ast}U) \\Sigma V^{\\ast} = V \\Sigma^2 V^{\\ast}$$\nThis expression, $A^{\\ast} A = V \\Sigma^2 V^{\\ast}$, is the eigenvalue decomposition (EVD) of the matrix $A^{\\ast} A$. The matrix $A^{\\ast} A$ is Hermitian since $(A^{\\ast} A)^{\\ast} = A^{\\ast} (A^{\\ast})^{\\ast} = A^{\\ast} A$. Its eigenvalues are the diagonal entries of $\\Sigma^2$, which are $\\sigma_i^2$, and the corresponding eigenvectors are the columns of $V$. Since $\\sigma_i \\geq 0$, the eigenvalues $\\sigma_i^2$ are non-negative, so $A^{\\ast} A$ is a positive semidefinite matrix.\n\nThe problem states that $A^{\\ast} A$ is idempotent, which means $(A^{\\ast} A)^2 = A^{\\ast} A$.\nLet $\\lambda$ be an eigenvalue of an idempotent matrix $P$, with corresponding eigenvector $x \\neq 0$, such that $Px = \\lambda x$. Then $P^2 x = P(\\lambda x) = \\lambda(Px) = \\lambda(\\lambda x) = \\lambda^2 x$. Since $P^2=P$, we have $Px = \\lambda^2 x$.\nThus, $\\lambda x = \\lambda^2 x$, which implies $(\\lambda^2 - \\lambda)x = 0$. Since $x \\neq 0$, we must have $\\lambda^2 - \\lambda = 0$, which yields $\\lambda(\\lambda - 1) = 0$. The possible eigenvalues of any idempotent matrix are therefore $0$ and $1$.\n\nThe eigenvalues of $A^{\\ast} A$ are $\\lambda_i = \\sigma_i^2$. Applying the property of idempotent matrices, we must have $\\sigma_i^2 \\in \\{0, 1\\}$.\nSince the singular values $\\sigma_i$ are defined to be non-negative, we can find them by taking the square root:\n$$\\sigma_i = \\sqrt{0} = 0 \\quad \\text{or} \\quad \\sigma_i = \\sqrt{1} = 1$$\nTherefore, the singular values of any matrix $A$ for which $A^{\\ast} A$ is idempotent can only be $0$ or $1$.\n\n**(b) Construction and verification of matrix A**\n\nWe need to construct a nonzero matrix $A \\in \\mathbb{C}^{4 \\times 4}$ satisfying three conditions. Let us choose the following block matrix:\n$$A = \\begin{pmatrix} 0_{2} & I_2 \\\\ 0_{2} & 0_{2} \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}$$\nwhere $0_{2}$ is the $2 \\times 2$ zero matrix and $I_2$ is the $2 \\times 2$ identity matrix. This is a nonzero matrix.\n\n1.  **Nilpotency**: We check if $A$ is nilpotent of index $2$, which requires $A \\neq 0$ and $A^2 = 0$.\n    $$A^2 = \\begin{pmatrix} 0_{2} & I_2 \\\\ 0_{2} & 0_{2} \\end{pmatrix} \\begin{pmatrix} 0_{2} & I_2 \\\\ 0_{2} & 0_{2} \\end{pmatrix} = \\begin{pmatrix} 0_2 \\cdot 0_2 + I_2 \\cdot 0_2 & 0_2 \\cdot I_2 + I_2 \\cdot 0_2 \\\\ 0_2 \\cdot 0_2 + 0_2 \\cdot 0_2 & 0_2 \\cdot I_2 + 0_2 \\cdot 0_2 \\end{pmatrix} = \\begin{pmatrix} 0_{2} & 0_{2} \\\\ 0_{2} & 0_{2} \\end{pmatrix} = 0_4$$\n    Since $A \\neq 0$ and $A^2=0$, $A$ is nilpotent of index $2$. A consequence is that all its eigenvalues are $0$.\n\n2.  **$A^{\\ast} A$ as an orthogonal projector**: An orthogonal projector $P$ must be Hermitian ($P^{\\ast}=P$) and idempotent ($P^2=P$). We calculate $A^{\\ast}A$.\n    The conjugate transpose of A is $A^{\\ast} = \\begin{pmatrix} 0_{2} & I_2 \\\\ 0_{2} & 0_{2} \\end{pmatrix}^{\\ast} = \\begin{pmatrix} 0_{2}^{\\ast} & 0_{2}^{\\ast} \\\\ I_{2}^{\\ast} & 0_{2}^{\\ast} \\end{pmatrix} = \\begin{pmatrix} 0_{2} & 0_{2} \\\\ I_{2} & 0_{2} \\end{pmatrix}$.\n    $$A^{\\ast} A = \\begin{pmatrix} 0_{2} & 0_{2} \\\\ I_{2} & 0_{2} \\end{pmatrix} \\begin{pmatrix} 0_{2} & I_2 \\\\ 0_{2} & 0_{2} \\end{pmatrix} = \\begin{pmatrix} 0_{2} & 0_{2} \\\\ 0_{2} & I_2 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix}$$\n    Let's check the properties of $P = A^{\\ast} A$:\n    -   **Hermitian**: $P^{\\ast} = \\left(\\begin{smallmatrix} 0_{2} & 0_{2} \\\\ 0_{2} & I_2 \\end{smallmatrix}\\right)^{\\ast} = \\begin{pmatrix} 0_{2} & 0_{2} \\\\ 0_{2} & I_2 \\end{pmatrix} = P$. It is Hermitian.\n    -   **Idempotent**: $P^2 = \\left(\\begin{smallmatrix} 0_{2} & 0_{2} \\\\ 0_{2} & I_2 \\end{smallmatrix}\\right)^2 = \\begin{pmatrix} 0_{2} & 0_{2} \\\\ 0_{2} & I_2^2 \\end{pmatrix} = \\begin{pmatrix} 0_{2} & 0_{2} \\\\ 0_{2} & I_2 \\end{pmatrix} = P$. It is idempotent.\n    -   **Rank**: The rank of a projector is its trace. $\\text{rank}(P) = \\text{tr}(P) = 0+0+1+1 = 2$.\n    Thus, $A^{\\ast} A$ is an orthogonal projector of rank $2$.\n\n3.  **Norm-preserving isometry**: We must show that $A$ acts as a norm-preserving isometry on a two-dimensional subspace. A linear map acts as an isometry on a subspace $S$ if for all $x \\in S$, $\\|Ax\\| = \\|x\\|$.\n    The subspace where this holds is the span of the right singular vectors corresponding to singular values equal to $1$. From part (a), we know the singular values are $0$ or $1$. The number of singular values equal to $1$ is the rank of $A$, which is $\\text{rank}(A) = \\text{rank}(A^{\\ast}A) = 2$.\n    The singular values are the square roots of the eigenvalues of $A^{\\ast}A = \\text{diag}(0,0,1,1)$. The eigenvalues are $1, 1, 0, 0$.\n    The singular values are thus $\\sigma_1=1, \\sigma_2=1, \\sigma_3=0, \\sigma_4=0$.\n    The required subspace $S$ is the eigenspace of $A^{\\ast}A$ corresponding to the eigenvalue $1$. Let $x = (x_1, x_2, x_3, x_4)^T$.\n    $A^{\\ast}Ax = x \\implies \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix} \\implies \\begin{pmatrix} 0 \\\\ 0 \\\\ x_3 \\\\ x_4 \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix}$.\n    This implies $x_1=0$ and $x_2=0$. The subspace is $S = \\{ (0,0,x_3,x_4)^T \\in \\mathbb{C}^4 \\} = \\text{span}\\{e_3, e_4\\}$. This is a two-dimensional subspace.\n    For any vector $x \\in S$, $x = c_3 e_3 + c_4 e_4$ for some $c_3, c_4 \\in \\mathbb{C}$.\n    $\\|x\\|^2 = \\|c_3 e_3 + c_4 e_4\\|^2 = |c_3|^2 + |c_4|^2$.\n    Now we compute $Ax$:\n    $$Ax = \\begin{pmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ c_3 \\\\ c_4 \\end{pmatrix} = \\begin{pmatrix} c_3 \\\\ c_4 \\\\ 0 \\\\ 0 \\end{pmatrix} = c_3 e_1 + c_4 e_2$$\n    $\\|Ax\\|^2 = \\|c_3 e_1 + c_4 e_2\\|^2 = |c_3|^2 + |c_4|^2$.\n    Since $\\|Ax\\|^2 = \\|x\\|^2$, we have $\\|Ax\\| = \\|x\\|$ for all $x \\in S$. The property is verified.\n    \n    The singular values of $A$ are the non-negative square roots of the eigenvalues of $A^{\\ast}A$. Since the eigenvalues of $A^{\\ast}A=\\text{diag}(0,0,1,1)$ are $0, 0, 1, 1$, the singular values are $\\sqrt{1}, \\sqrt{1}, \\sqrt{0}, \\sqrt{0}$, which are $1, 1, 0, 0$.\n\n**(c) Characteristic polynomial of D**\n\nWe define the Hermitian matrix $D := A^{\\ast} A - A A^{\\ast}$. Using the matrix $A$ from part (b), we have already computed $A^{\\ast} A = \\begin{pmatrix} 0_2 & 0_2 \\\\ 0_2 & I_2 \\end{pmatrix}$. Now we compute $A A^{\\ast}$:\n$$A A^{\\ast} = \\begin{pmatrix} 0_{2} & I_2 \\\\ 0_{2} & 0_{2} \\end{pmatrix} \\begin{pmatrix} 0_{2} & 0_{2} \\\\ I_{2} & 0_{2} \\end{pmatrix} = \\begin{pmatrix} I_2 & 0_{2} \\\\ 0_{2} & 0_2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}$$\nNow we compute $D$:\n$$D = A^{\\ast} A - A A^{\\ast} = \\begin{pmatrix} 0_{2} & 0_{2} \\\\ 0_{2} & I_2 \\end{pmatrix} - \\begin{pmatrix} I_2 & 0_{2} \\\\ 0_{2} & 0_2 \\end{pmatrix} = \\begin{pmatrix} -I_2 & 0_{2} \\\\ 0_{2} & I_2 \\end{pmatrix}$$\nExplicitly, $D = \\text{diag}(-1, -1, 1, 1)$.\nThe characteristic polynomial of $D$ is $p(\\lambda) = \\det(D - \\lambda I)$.\n$$D - \\lambda I = \\begin{pmatrix} -1-\\lambda & 0 & 0 & 0 \\\\ 0 & -1-\\lambda & 0 & 0 \\\\ 0 & 0 & 1-\\lambda & 0 \\\\ 0 & 0 & 0 & 1-\\lambda \\end{pmatrix}$$\nThe determinant of this diagonal matrix is the product of its diagonal entries:\n$$p(\\lambda) = (-1-\\lambda)(-1-\\lambda)(1-\\lambda)(1-\\lambda)$$\n$$p(\\lambda) = (-(1+\\lambda))^2 (1-\\lambda)^2 = (1+\\lambda)^2 (1-\\lambda)^2$$\n$$p(\\lambda) = ((1+\\lambda)(1-\\lambda))^2 = (1-\\lambda^2)^2$$\nExpanding this expression gives:\n$$p(\\lambda) = 1 - 2\\lambda^2 + \\lambda^4$$\nThis is the characteristic polynomial of $D$.",
            "answer": "$$\\boxed{\\lambda^4 - 2\\lambda^2 + 1}$$"
        },
        {
            "introduction": "The conceptual differences between the eigenvalue decomposition and the singular value decomposition have deep, practical consequences in the design and analysis of numerical algorithms. This final practice bridges the gap from theory to application by examining a parametric family of non-normal matrices . You will discover how a matrix can possess well-separated eigenvalues, suggesting straightforward behavior, while its singular values are tightly clustered. This mismatch demonstrates why SVD-based metrics like the condition number can be misleading, as they may fail to predict the true difficulty that iterative solvers face on matrices with non-orthogonal eigenvectors.",
            "id": "3573887",
            "problem": "Let $A \\in \\mathbb{C}^{2 \\times 2}$ and consider the following parametric family of matrices\n$$\nA(L,\\alpha) \\;=\\; \\begin{pmatrix}\nL & \\alpha L \\\\\n0 & -L\n\\end{pmatrix},\n$$\nwhere $L > 0$ and $\\alpha > 0$ are real parameters. The matrix $A(L,\\alpha)$ is diagonalizable over $\\mathbb{C}$ because its eigenvalues are distinct. Let $\\lambda_1$ and $\\lambda_2$ denote the eigenvalues of $A(L,\\alpha)$, and let $\\sigma_1 \\geq \\sigma_2 \\geq 0$ denote the singular values obtained from the Singular Value Decomposition (SVD), that is, $A = U \\Sigma V^{\\ast}$ with $U,V$ unitary and $\\Sigma = \\operatorname{diag}(\\sigma_1,\\sigma_2)$. The squared singular values are the eigenvalues of $A^{\\ast}A$, where $A^{\\ast}$ denotes the conjugate transpose (Hermitian adjoint) of $A$.\n\nStarting from the definitions of eigenvalue decomposition and Singular Value Decomposition (SVD), derive the eigenvalues $\\lambda_1,\\lambda_2$ of $A(L,\\alpha)$ and the squared singular values $\\sigma_1^2,\\sigma_2^2$ of $A(L,\\alpha)$ by computing the eigenvalues of $A^{\\ast}A$. Then define the following dimensionless, $L$-normalized spectral-separation ratio\n$$\nR(L,\\alpha) \\;=\\; \\frac{\\left|\\sigma_1^2 - \\sigma_2^2\\right|/L^2}{\\left|\\lambda_1 - \\lambda_2\\right|/L}.\n$$\nThis ratio compares the $L$-normalized separation in the spectrum of $A^{\\ast}A$ (which underlies SVD-based constructions) to the $L$-normalized separation in the spectrum of $A$ itself (which underlies eigenvalue-based constructions).\n\nCompute $R(L,\\alpha)$ exactly as a closed-form analytical expression in terms of $\\alpha$ only. No rounding is required. Based on your derivation, briefly explain, using first principles, how small values of $\\alpha$ can lead to $A$ being diagonalizable with well-separated eigenvalues $\\lambda_i$, while $A^{\\ast}A$ has nearly repeated eigenvalues $\\sigma_i^2$, and articulate the implication of this mismatch for preconditioners that rely on SVD-derived spectral information and may therefore miss eigen-spectral separation. Your final answer must be the exact expression for $R(L,\\alpha)$ in terms of $\\alpha$ alone.",
            "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. All necessary parameters and definitions are provided, and the objectives are clear. The problem is a standard exercise in numerical linear algebra, exploring the relationship between eigenvalue decomposition and singular value decomposition for a non-normal matrix.\n\nFirst, we derive the eigenvalues of the matrix $A(L,\\alpha)$. The matrix is given by\n$$\nA \\;=\\; \\begin{pmatrix}\nL & \\alpha L \\\\\n0 & -L\n\\end{pmatrix}\n$$\nSince $A$ is an upper triangular matrix, its eigenvalues are its diagonal entries. Therefore, the eigenvalues are\n$$\n\\lambda_1 = L \\quad \\text{and} \\quad \\lambda_2 = -L\n$$\nThe problem states that $L > 0$, so the eigenvalues are distinct. We compute the $L$-normalized separation of the eigenvalues, which appears in the denominator of the ratio $R(L,\\alpha)$:\n$$\n\\frac{|\\lambda_1 - \\lambda_2|}{L} = \\frac{|L - (-L)|}{L} = \\frac{|2L|}{L} = \\frac{2L}{L} = 2\n$$\nThis separation is constant and independent of $\\alpha$.\n\nNext, we derive the squared singular values, $\\sigma_1^2$ and $\\sigma_2^2$. By definition, these are the eigenvalues of the matrix $A^{\\ast}A$, where $A^{\\ast}$ is the conjugate transpose of $A$. Since the entries of $A$ are real, $A^{\\ast}$ is simply the transpose $A^T$.\n$$\nA^{\\ast} = A^T = \\begin{pmatrix}\nL & 0 \\\\\n\\alpha L & -L\n\\end{pmatrix}\n$$\nNow, we compute the product $A^{\\ast}A$:\n$$\nA^{\\ast}A = \\begin{pmatrix}\nL & 0 \\\\\n\\alpha L & -L\n\\end{pmatrix}\n\\begin{pmatrix}\nL & \\alpha L \\\\\n0 & -L\n\\end{pmatrix} = \\begin{pmatrix}\nL^2 & \\alpha L^2 \\\\\n\\alpha L^2 & (\\alpha L)^2 + (-L)^2\n\\end{pmatrix} = \\begin{pmatrix}\nL^2 & \\alpha L^2 \\\\\n\\alpha L^2 & (\\alpha^2+1)L^2\n\\end{pmatrix}\n$$\nTo find the eigenvalues of $A^{\\ast}A$, we solve the characteristic equation $\\det(A^{\\ast}A - \\mu I) = 0$, where $\\mu = \\sigma^2$.\n$$\n\\det \\begin{pmatrix}\nL^2 - \\mu & \\alpha L^2 \\\\\n\\alpha L^2 & (\\alpha^2+1)L^2 - \\mu\n\\end{pmatrix} = 0\n$$\n$$\n(L^2 - \\mu)((\\alpha^2+1)L^2 - \\mu) - (\\alpha L^2)^2 = 0\n$$\nThis is a quadratic equation for $\\mu$. To simplify, we can factor out $L^2$ from the matrix $A^{\\ast}A = L^2 \\begin{pmatrix} 1 & \\alpha \\\\ \\alpha & \\alpha^2+1 \\end{pmatrix}$. Let the eigenvalues of the matrix $\\begin{pmatrix} 1 & \\alpha \\\\ \\alpha & \\alpha^2+1 \\end{pmatrix}$ be $\\mu'$. Then $\\mu = L^2 \\mu'$. The characteristic equation for $\\mu'$ is:\n$$\n(1 - \\mu')(\\alpha^2+1 - \\mu') - \\alpha^2 = 0\n$$\n$$\n\\mu'^2 - (\\alpha^2+1+1)\\mu' + (\\alpha^2+1) - \\alpha^2 = 0\n$$\n$$\n\\mu'^2 - (\\alpha^2+2)\\mu' + 1 = 0\n$$\nUsing the quadratic formula to solve for $\\mu'$:\n$$\n\\mu' = \\frac{(\\alpha^2+2) \\pm \\sqrt{(\\alpha^2+2)^2 - 4(1)(1)}}{2} = \\frac{(\\alpha^2+2) \\pm \\sqrt{\\alpha^4 + 4\\alpha^2 + 4 - 4}}{2}\n$$\n$$\n\\mu' = \\frac{(\\alpha^2+2) \\pm \\sqrt{\\alpha^4 + 4\\alpha^2}}{2} = \\frac{(\\alpha^2+2) \\pm \\sqrt{\\alpha^2(\\alpha^2+4)}}{2}\n$$\nSince $\\alpha > 0$, $\\sqrt{\\alpha^2} = \\alpha$.\n$$\n\\mu' = \\frac{\\alpha^2+2 \\pm \\alpha\\sqrt{\\alpha^2+4}}{2}\n$$\nThe squared singular values are $\\sigma_i^2 = L^2 \\mu'$. By convention, $\\sigma_1 \\geq \\sigma_2$, so $\\sigma_1^2 \\geq \\sigma_2^2$.\n$$\n\\sigma_1^2 = L^2 \\left(\\frac{\\alpha^2+2 + \\alpha\\sqrt{\\alpha^2+4}}{2}\\right)\n$$\n$$\n\\sigma_2^2 = L^2 \\left(\\frac{\\alpha^2+2 - \\alpha\\sqrt{\\alpha^2+4}}{2}\\right)\n$$\nNow we compute the quantity for the numerator of $R(L,\\alpha)$:\n$$\n\\sigma_1^2 - \\sigma_2^2 = L^2 \\left[ \\left(\\frac{\\alpha^2+2 + \\alpha\\sqrt{\\alpha^2+4}}{2}\\right) - \\left(\\frac{\\alpha^2+2 - \\alpha\\sqrt{\\alpha^2+4}}{2}\\right) \\right]\n$$\n$$\n\\sigma_1^2 - \\sigma_2^2 = L^2 \\left( \\frac{2\\alpha\\sqrt{\\alpha^2+4}}{2} \\right) = L^2 \\alpha\\sqrt{\\alpha^2+4}\n$$\nSince $L > 0$ and $\\alpha > 0$, this difference is positive, so $|\\sigma_1^2 - \\sigma_2^2| = L^2 \\alpha\\sqrt{\\alpha^2+4}$. The $L$-normalized separation is:\n$$\n\\frac{|\\sigma_1^2 - \\sigma_2^2|}{L^2} = \\alpha\\sqrt{\\alpha^2+4}\n$$\nFinally, we compute the ratio $R(L,\\alpha)$:\n$$\nR(L,\\alpha) = \\frac{\\left|\\sigma_1^2 - \\sigma_2^2\\right|/L^2}{\\left|\\lambda_1 - \\lambda_2\\right|/L} = \\frac{\\alpha\\sqrt{\\alpha^2+4}}{2}\n$$\nThis expression depends only on $\\alpha$, as required.\n\nFor the explanation:\nThe matrix $A$ is diagonalizable and has eigenvalues $\\lambda_1 = L$ and $\\lambda_2 = -L$. Their separation $|\\lambda_1 - \\lambda_2| = 2L$ is constant and substantial, as $L > 0$. The matrix is non-normal for any $\\alpha > 0$.\nThe squared singular values are $\\sigma_1^2$ and $\\sigma_2^2$. Their separation is $|\\sigma_1^2 - \\sigma_2^2| = L^2\\alpha\\sqrt{\\alpha^2+4}$. As $\\alpha \\to 0^+$, this separation approaches $0$. Specifically, $\\sigma_1^2 \\to L^2$ and $\\sigma_2^2 \\to L^2$.\nThe ratio $R(L,\\alpha) \\approx \\frac{\\alpha\\sqrt{4}}{2} = \\alpha$ for small $\\alpha$. So as $\\alpha \\to 0^+$, $R(L,\\alpha) \\to 0$. This quantifies the mismatch: the eigenvalue spectrum of $A$ is widely separated, whereas the eigenvalue spectrum of $A^{\\ast}A$ (i.e., the squared singular values) is tightly clustered.\n\nThe implication of this mismatch is significant for numerical methods. Preconditioners for iterative solvers are often designed based on spectral information. SVD-based analyses and preconditioners utilize the singular values. For small $\\alpha$, the singular values are clustered ($\\sigma_1 \\approx \\sigma_2 \\approx L$), and the condition number $\\kappa_2(A) = \\sigma_1/\\sigma_2 \\approx 1$. From an SVD perspective, the matrix $A$ appears well-conditioned and benign, behaving much like a simple scaling of an orthogonal matrix. A preconditioner built on this observation might be simple, for example, $P = L \\cdot I$, and would be expected to work well.\n\nHowever, the convergence of iterative solvers for non-normal matrices is not solely dictated by the condition number. The well-separated eigenvalues ($L$, $-L$), combined with a non-orthogonal eigenbasis (a hallmark of non-normality), can lead to transient growth phenomena that can severely slow the convergence of methods like GMRES. The SVD-based analysis, by focusing on the clustered singular values, completely misses this feature of the eigen-structure. It is this non-normal character, tied to the separated eigenvalues, that governs the true difficulty of solving systems with $A$. A preconditioner based on SVD spectral information is therefore \"missing\" the crucial information about the eigen-spectral properties and the non-normality of the matrix, leading to a potentially poor assessment of the problem's difficulty and an ineffective preconditioning strategy.",
            "answer": "$$\n\\boxed{\\frac{\\alpha\\sqrt{\\alpha^2+4}}{2}}\n$$"
        }
    ]
}