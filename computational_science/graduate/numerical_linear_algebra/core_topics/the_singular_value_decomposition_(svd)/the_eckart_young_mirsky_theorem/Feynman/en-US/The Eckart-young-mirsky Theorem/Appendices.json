{
    "hands_on_practices": [
        {
            "introduction": "The Eckart-Young-Mirsky theorem provides exact formulas for the minimum error achievable when approximating a matrix with one of lower rank. This first exercise is a direct application of this cornerstone result . By working with a concrete set of singular values, you will practice calculating the optimal approximation error in both the spectral and Frobenius norms, reinforcing the fundamental connection between singular value decay and compressibility.",
            "id": "3587148",
            "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ of rank $r$, and recall the singular value decomposition (SVD) defined as $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative diagonal entries $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{r} > 0$ and zeros elsewhere. The Frobenius norm $\\| \\cdot \\|_{F}$ is defined by $\\|X\\|_{F}^{2} = \\sum_{i=1}^{m} \\sum_{j=1}^{n} x_{ij}^{2}$, and the spectral norm $\\| \\cdot \\|_{2}$ is defined by $\\|X\\|_{2} = \\max_{\\|y\\|_{2} = 1} \\|X y\\|_{2}$, which equals the largest singular value of $X$. A norm $\\|\\cdot\\|$ is called unitarily invariant if $\\|Q_{1} X Q_{2}\\| = \\|X\\|$ for all orthogonal matrices $Q_{1}, Q_{2}$ and all $X$.\n\nYou will analyze best rank-$k$ approximations to $A$ from first principles using the SVD, the definitions above, and the fact that both $\\|\\cdot\\|_{F}$ and $\\|\\cdot\\|_{2}$ are unitarily invariant norms. Let $A$ have rank $r = 8$ and singular values\n$$\n\\sigma_{1} = 14,\\ \\sigma_{2} = 10,\\ \\sigma_{3} = 7,\\ \\sigma_{4} = 5,\\ \\sigma_{5} = 3,\\ \\sigma_{6} = 2,\\ \\sigma_{7} = 1,\\ \\sigma_{8} = 0.5.\n$$\nFor the target rank $k = 4$, compute the minimal Frobenius error and the minimal spectral error achievable by any rank-$4$ approximation to $A$. Then, for the uniform tolerance $\\varepsilon = 3.5$, determine the smallest integer $k$ such that there exists a rank-$k$ matrix $\\tilde{A}_{k}$ with $\\|A - \\tilde{A}_{k}\\|_{F} \\leq \\varepsilon$, and, independently, the smallest integer $k$ such that there exists a rank-$k$ matrix $A_{k}$ with $\\|A - A_{k}\\|_{2} \\leq \\varepsilon$.\n\nExpress both minimal errors for $k = 4$ exactly (do not round), and report the two minimal ranks as integers. Your final answer must be a single row matrix with four entries in the order: minimal Frobenius error for $k=4$, minimal spectral error for $k=4$, smallest $k$ for the Frobenius tolerance $\\varepsilon$, smallest $k$ for the spectral tolerance $\\varepsilon$.",
            "solution": "We begin from the singular value decomposition $A = U \\Sigma V^{\\top}$ and the definitions of the Frobenius norm and spectral norm. Both $\\|\\cdot\\|_{F}$ and $\\|\\cdot\\|_{2}$ are unitarily invariant norms, so the error $\\|A - X\\|$ for any candidate rank-$k$ approximation $X$ depends only on the singular values of $A - X$ and can be analyzed in the diagonal SVD coordinates.\n\nA rank-$k$ matrix $X$ can be written as $X = U Y V^{\\top}$ for some $Y$ of rank at most $k$. Because $U$ and $V$ are orthogonal and the norms are unitarily invariant, $\\|A - X\\| = \\|\\Sigma - Y\\|$ for both $\\|\\cdot\\|_{F}$ and $\\|\\cdot\\|_{2}$. Therefore, the problem reduces to approximating the diagonal matrix $\\Sigma$ by a rank-$k$ matrix $Y$ so as to minimize the chosen norm of $\\Sigma - Y$.\n\nFor the Frobenius norm, the squared norm decomposes additively over orthogonal directions: if $\\Sigma = \\operatorname{diag}(\\sigma_{1},\\dots,\\sigma_{r},0,\\dots)$ and $Y$ has singular values $\\tau_{1},\\dots,\\tau_{k}$ arranged on the diagonal (with all other diagonal entries zero), then\n$$\n\\|\\Sigma - Y\\|_{F}^{2} = \\sum_{i=1}^{m} \\sum_{j=1}^{n} (\\Sigma - Y)_{ij}^{2} = \\sum_{j=1}^{r} (\\sigma_{j} - \\tau_{j})^{2} + \\sum_{j>r} 0,\n$$\nwhen $Y$ is chosen diagonal in the same basis. The minimization over diagonal $Y$ of rank at most $k$ subject to $\\tau_{j} \\geq 0$ and at most $k$ nonzero entries is achieved by matching the $k$ largest diagonal entries and setting the remaining entries to zero, because the Frobenius norm reduces to the Euclidean distance in the diagonal coordinates and any deviation from $\\sigma_{j}$ for $j \\leq k$ increases the sum of squares. Hence, the optimal choice is $Y = \\operatorname{diag}(\\sigma_{1},\\dots,\\sigma_{k},0,\\dots)$, and the minimal Frobenius error is\n$$\n\\min_{\\operatorname{rank}(X) \\leq k} \\|A - X\\|_{F} = \\|\\Sigma - \\operatorname{diag}(\\sigma_{1},\\dots,\\sigma_{k},0,\\dots)\\|_{F} = \\left(\\sum_{j=k+1}^{r} \\sigma_{j}^{2}\\right)^{1/2}.\n$$\nThis conclusion is a direct consequence of orthogonal invariance and the Pythagorean decomposition of the Frobenius norm in the SVD basis.\n\nFor the spectral norm, $\\|A - X\\|_{2}$ equals the largest singular value of $A - X$. In the SVD coordinates, choosing $Y$ diagonal with the top $k$ singular values of $\\Sigma$ retained yields $A - X = U(\\Sigma - \\Sigma_{k})V^{\\top}$, where $\\Sigma_{k} = \\operatorname{diag}(\\sigma_{1},\\dots,\\sigma_{k},0,\\dots)$. The matrix $\\Sigma - \\Sigma_{k}$ is diagonal with entries $\\sigma_{k+1},\\dots,\\sigma_{r}$ (and zeros elsewhere), so its largest singular value is $\\sigma_{k+1}$. A standard majorization argument for unitarily invariant norms (viewed as symmetric gauge functions of singular values) shows that no other rank-$k$ choice of $Y$ can reduce the largest singular value below $\\sigma_{k+1}$ without increasing rank beyond $k$, because removing or attenuating any of the top $k$ singular directions leaves a singular value at least as large as $\\sigma_{k+1}$ in the residue. Thus,\n$$\n\\min_{\\operatorname{rank}(X) \\leq k} \\|A - X\\|_{2} = \\sigma_{k+1}.\n$$\nWe now apply these general conclusions to the given data. The singular values are\n$$\n\\sigma_{1} = 14,\\ \\sigma_{2} = 10,\\ \\sigma_{3} = 7,\\ \\sigma_{4} = 5,\\ \\sigma_{5} = 3,\\ \\sigma_{6} = 2,\\ \\sigma_{7} = 1,\\ \\sigma_{8} = 0.5,\n$$\nand $k = 4$.\n\n1. Minimal Frobenius error for $k = 4$:\n$$\n\\left(\\sum_{j=5}^{8} \\sigma_{j}^{2}\\right)^{1/2} = \\left(3^{2} + 2^{2} + 1^{2} + 0.5^{2}\\right)^{1/2} = \\left(9 + 4 + 1 + 0.25\\right)^{1/2} = \\left(\\frac{57}{4}\\right)^{1/2} = \\frac{\\sqrt{57}}{2}.\n$$\n\n2. Minimal spectral error for $k = 4$:\n$$\n\\sigma_{5} = 3.\n$$\n\n3. Smallest $k$ achieving the Frobenius tolerance $\\varepsilon = 3.5$:\nWe require\n$$\n\\left(\\sum_{j=k+1}^{8} \\sigma_{j}^{2}\\right)^{1/2} \\leq 3.5 \\quad \\Longleftrightarrow \\quad \\sum_{j=k+1}^{8} \\sigma_{j}^{2} \\leq 12.25.\n$$\nCompute tail sums:\n- For $k = 4$, $\\sum_{j=5}^{8} \\sigma_{j}^{2} = 9 + 4 + 1 + 0.25 = 14.25 > 12.25$.\n- For $k = 5$, $\\sum_{j=6}^{8} \\sigma_{j}^{2} = 4 + 1 + 0.25 = 5.25 \\leq 12.25$.\nThus the smallest $k$ is $k = 5$.\n\n4. Smallest $k$ achieving the spectral tolerance $\\varepsilon = 3.5$:\nWe require\n$$\n\\sigma_{k+1} \\leq 3.5.\n$$\nInspecting the sequence, $\\sigma_{4} = 5 > 3.5$ while $\\sigma_{5} = 3 \\leq 3.5$, so the smallest $k$ is $k = 4$.\n\nCollecting the four requested quantities in the specified order yields the final row matrix\n$$\n\\left( \\frac{\\sqrt{57}}{2},\\ 3,\\ 5,\\ 4 \\right).\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{\\sqrt{57}}{2} & 3 & 5 & 4\\end{pmatrix}}$$"
        },
        {
            "introduction": "A deep understanding of a theorem requires not only knowing when it applies, but also when it does not. The power of the Eckart-Young-Mirsky theorem is restricted to unitarily invariant norms. This exercise explores this crucial boundary condition by challenging you to construct a counterexample using the entrywise $\\ell_{\\infty}$ norm, which lacks unitary invariance . By discovering an approximation that is better than the one provided by the SVD truncation, you will gain a more robust and nuanced understanding of the theorem's scope.",
            "id": "3587141",
            "problem": "Consider the entrywise $\\ell_{\\infty}$ norm on $\\mathbb{R}^{m \\times n}$ defined by $\\|E\\|_{\\infty} = \\max_{i,j} |E_{i j}|$. Recall that the singular value decomposition (SVD) of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is $A = U \\Sigma V^{\\top}$ where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal (unitary in the complex case) and $\\Sigma$ is diagonal with nonnegative entries (the singular values). A norm $\\|\\cdot\\|$ on matrices is called unitarily invariant if $\\|U A V^{\\top}\\| = \\|A\\|$ for all orthogonal (unitary) $U$ and $V$. The Eckart-Young-Mirsky theorem asserts that, for any unitarily invariant norm, truncating the singular value decomposition of $A$ to rank $k$ produces a best rank-$k$ approximation of $A$.\n\nYour tasks are:\n\n- Construct a concrete counterexample showing that truncation can be suboptimal under the entrywise $\\ell_{\\infty}$ norm. Specifically, let $A \\in \\mathbb{R}^{2 \\times 2}$ be the identity matrix $A = I_{2}$ and fix the SVD $A = U \\Sigma V^{\\top}$ with $U = I_{2}$, $V = I_{2}$, and $\\Sigma = I_{2}$. Let $A_{1}$ denote the rank-$1$ truncation that keeps only the leading singular component under this fixed SVD.\n\n- Compute the value of $\\|A - A_{1}\\|_{\\infty}$.\n\n- Determine the minimum possible value of $\\|A - X\\|_{\\infty}$ over all rank-$1$ matrices $X \\in \\mathbb{R}^{2 \\times 2}$, and exhibit a rank-$1$ matrix achieving this minimum.\n\n- Report the numerical gap\n$$\n\\Delta \\;=\\; \\|A - A_{1}\\|_{\\infty} \\;-\\; \\min_{\\operatorname{rank}(X) = 1} \\|A - X\\|_{\\infty}.\n$$\n\n- Finally, starting from the definitions above and without invoking any pre-stated theorems, explain why unitary invariance is essential in the Eckart-Young-Mirsky conclusion and why that conclusion does not extend to the entrywise $\\ell_{\\infty}$ norm. Your explanation should reason from first principles about how orthogonal changes of basis affect entrywise norms versus unitarily invariant norms.\n\nProvide the value of $\\Delta$ as your final answer. No rounding is required.",
            "solution": "### Part 1: Compute $\\|A - A_{1}\\|_{\\infty}$\nThe matrix is $A = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$. The given SVD is $A = U \\Sigma V^{\\top}$ with $U=I_2$, $V=I_2$, and $\\Sigma=I_2$. The singular values are $\\sigma_1=1$ and $\\sigma_2=1$. The singular vectors are $u_1 = v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $u_2 = v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nThe full SVD expansion is $A = \\sigma_1 u_1 v_1^{\\top} + \\sigma_2 u_2 v_2^{\\top}$.\nThe rank-$1$ SVD truncation, $A_1$, is obtained by keeping only the leading term:\n$$\nA_1 = \\sigma_1 u_1 v_1^{\\top} = 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThe approximation error is the matrix $A - A_1$:\n$$\nA - A_1 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe entrywise $\\ell_{\\infty}$ norm of this error is the maximum absolute value of its entries:\n$$\n\\|A - A_1\\|_{\\infty} = \\max \\{|0|, |0|, |0|, |1|\\} = 1\n$$\n\n### Part 2: Determine $\\min_{\\operatorname{rank}(X) = 1} \\|A - X\\|_{\\infty}$\nWe seek to find the minimum value of $\\|I_2 - X\\|_{\\infty}$ over all rank-$1$ matrices $X \\in \\mathbb{R}^{2 \\times 2}$.\nLet $\\epsilon = \\min_{\\operatorname{rank}(X)=1} \\|I_2 - X\\|_{\\infty}$.\nThen for an optimal $X$, we have:\n$|1-x_{11}| \\le \\epsilon$, $|-x_{12}| \\le \\epsilon$, $|-x_{21}| \\le \\epsilon$, $|1-x_{22}| \\le \\epsilon$.\nThis implies $|x_{12}| \\le \\epsilon$ and $|x_{21}| \\le \\epsilon$.\nAlso, $1-\\epsilon \\le x_{11} \\le 1+\\epsilon$ and $1-\\epsilon \\le x_{22} \\le 1+\\epsilon$.\nThe minimum $\\epsilon$ must be less than $1$ (from our SVD result), so we can assume $\\epsilon < 1$. This implies $1-\\epsilon>0$, so $x_{11}$ and $x_{22}$ must be positive.\nThe rank-$1$ condition is $x_{11}x_{22} = x_{12}x_{21}$.\nTaking the magnitude and using the bounds, we get:\n$$\nx_{11}x_{22} = |x_{12}x_{21}| = |x_{12}| |x_{21}| \\le \\epsilon \\cdot \\epsilon = \\epsilon^2\n$$\nUsing the lower bounds for $x_{11}$ and $x_{22}$:\n$$\nx_{11}x_{22} \\ge (1-\\epsilon)(1-\\epsilon) = (1-\\epsilon)^2\n$$\nCombining these two inequalities:\n$$\n(1-\\epsilon)^2 \\le x_{11}x_{22} \\le \\epsilon^2\n$$\nThis requires $(1-\\epsilon)^2 \\le \\epsilon^2$. Since $1-\\epsilon > 0$ and $\\epsilon \\ge 0$, we can take the square root:\n$$\n1 - \\epsilon \\le \\epsilon \\implies 1 \\le 2\\epsilon \\implies \\epsilon \\ge \\frac{1}{2}\n$$\nSo the minimum possible error is at least $1/2$. We can show this minimum is achievable. Let's construct a rank-$1$ matrix $X$ such that $\\|I_2 - X\\|_{\\infty} = 1/2$.\nConsider $X = \\begin{pmatrix} 1/2 & 1/2 \\\\ 1/2 & 1/2 \\end{pmatrix}$. This matrix has rank $1$ since its rows are identical.\nThe error matrix is:\n$$\nI_2 - X = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 1/2 & 1/2 \\\\ 1/2 & 1/2 \\end{pmatrix} = \\begin{pmatrix} 1/2 & -1/2 \\\\ -1/2 & 1/2 \\end{pmatrix}\n$$\nThe $\\ell_{\\infty}$ norm of this error is $\\|I_2 - X\\|_{\\infty} = \\max\\{|1/2|, |-1/2|\\} = 1/2$.\nSince we proved a lower bound of $1/2$ and found a matrix that achieves it, the minimum value is exactly $1/2$.\n$$\n\\min_{\\operatorname{rank}(X) = 1} \\|A - X\\|_{\\infty} = \\frac{1}{2}\n$$\n\n### Part 3: Compute the Gap $\\Delta$\nThe gap is the difference between the error of the SVD truncation and the true minimum error.\n$$\n\\Delta = \\|A - A_{1}\\|_{\\infty} - \\min_{\\operatorname{rank}(X) = 1} \\|A - X\\|_{\\infty} = 1 - \\frac{1}{2} = \\frac{1}{2}\n$$\n\n### Part 4: Explanation of Unitary Invariance\nThe Eckart-Young-Mirsky theorem's conclusion relies fundamentally on the norm being unitarily invariant. A norm $\\|\\cdot\\|$ is unitarily invariant if it is invariant under orthogonal changes of basis, i.e., $\\|UAV^\\top\\| = \\|A\\|$ for all orthogonal matrices $U$ and $V$. Such norms, like the Frobenius norm $\\|\\cdot\\|_F$ and the spectral norm $\\|\\cdot\\|_2$, depend only on the singular values of the matrix, not on the singular vectors.\n\nThe proof of the EYM theorem exploits this property. For a matrix $A = U\\Sigma V^\\top$, the problem of minimizing $\\|A-X\\|$ over rank-$k$ matrices $X$ is transformed using unitary invariance:\n$$\n\\|A-X\\| = \\|U\\Sigma V^\\top - X\\| = \\|U^\\top(U\\Sigma V^\\top - X)V\\| = \\|\\Sigma - U^\\top XV\\|\n$$\nSince $X$ is any rank-$k$ matrix, $Y = U^\\top XV$ is also any rank-$k$ matrix. The problem is reduced to finding the best rank-$k$ approximation of a diagonal matrix $\\Sigma$. For a diagonal matrix, it is intuitive that the best approximation is to keep the $k$ largest entries and zero out the rest. This corresponds to $Y$ being a diagonal matrix with the first $k$ singular values, which in turn means $X=A_k$, the truncated SVD of $A$.\n\nThe entrywise $\\ell_{\\infty}$ norm is not unitarily invariant. It measures the largest entry in a matrix, $|A_{ij}| = |e_i^\\top A e_j|$, which explicitly depends on the standard basis $\\{e_i\\}$. An orthogonal transformation mixes the entries, and can change their maximum magnitude. For example, let $A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$, so $\\|A\\|_\\infty = 1$. Let $U=V=\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix}$ (a rotation). Then $B = UAV^\\top = \\frac{1}{2}\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$, and $\\|B\\|_\\infty = 1/2$. Since $\\|A\\|_\\infty \\neq \\|B\\|_\\infty$, the norm is not unitarily invariant.\n\nThis failure of invariance means we cannot simplify the minimization problem by transforming it to the basis of singular vectors. The problem of minimizing $\\|A-X\\|_\\infty$ must be solved in the original, standard basis.\n\nOur counterexample with $A = I_2$ illustrates this perfectly.\n- The SVD truncation $A_1 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$ results in an error matrix $A-A_1 = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$. The error is entirely concentrated in a single entry, leading to a maximal error of $\\|A-A_1\\|_\\infty = 1$.\n- The optimal rank-$1$ approximation is $X = \\frac{1}{2}\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$, which leads to an error matrix $A-X = \\begin{pmatrix} 1/2 & -1/2 \\\\ -1/2 & 1/2 \\end{pmatrix}$. Here, the total error is \"spread out\" among all entries. Because the $\\ell_\\infty$ norm only registers the single largest entry, this spreading strategy is effective, reducing the norm of the error to $1/2$.\n\nIn essence, the SVD provides an optimal low-rank approximation in a basis-independent sense. Norms that are not basis-independent (unitarily invariant) do not \"see\" this optimality. For the $\\ell_\\infty$ norm, it is better to find an approximation that contaminates all entries with small errors rather than one that perfectly zeros out some components but leaves a large, concentrated error in another.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "Theoretical guarantees are powerful, but their real-world utility depends on how they hold up under the constraints of computation. This practice bridges the gap between the exactness of the EYM theorem and the reality of finite-precision arithmetic . Starting from a standard backward stability model for the SVD, you will derive bounds on how much the error of a *computed* low-rank approximation can deviate from the theoretical optimum, a vital skill for assessing the quality of numerical results in practical applications.",
            "id": "3587135",
            "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ with $m \\geq n$ and consider computing its singular value decomposition (singular value decomposition (SVD)) in floating-point arithmetic with unit roundoff $u$. Assume a backward stability model for the computed SVD: there exists a perturbation $\\Delta A$ such that the computed factors $(\\widehat{U}, \\widehat{\\Sigma}, \\widehat{V})$ are an exact SVD of $A + \\Delta A$ and the perturbation satisfies $\\|\\Delta A\\|_{2} \\leq \\gamma_{mn}\\, u\\, \\|A\\|_{2}$ for a given modest constant $\\gamma_{mn}$. Define the computed rank-$k$ truncation $\\widehat{A}_{k} \\coloneqq \\widehat{U}_{:,1:k}\\,\\widehat{\\Sigma}_{1:k,1:k}\\,\\widehat{V}_{:,1:k}^{\\top}$. Let $\\|\\cdot\\|_{g}$ be any unitarily invariant norm. \n\nUsing only foundational properties of unitarily invariant norms (unitary invariance and the triangle inequality), the variational characterization of the best rank-$k$ approximation error, and standard perturbation bounds for singular values, address the following:\n\n1) Derive two-sided additive bounds that relate the computed truncation error $\\|A - \\widehat{A}_{k}\\|_{g}$ to the best rank-$k$ error of $A$ in the same norm. Your bounds should make explicit how $\\|\\Delta A\\|_{g}$ enters.\n\n2) Specialize your bounds to the spectral norm $\\|\\cdot\\|_{2}$ to obtain an explicit bound on the ratio $\\rho \\coloneqq \\dfrac{\\|A - \\widehat{A}_{k}\\|_{2}}{\\sigma_{k+1}(A)}$ in terms of $\\|\\Delta A\\|_{2}$ and $\\sigma_{k+1}(A)$, where $\\sigma_{k+1}(A)$ is the $(k+1)$-st singular value of $A$.\n\n3) For the concrete data $m = n = 100$, $\\gamma_{mn} = 10 n$, $u = 2^{-53}$, $\\|A\\|_{2} = 10$, and $\\sigma_{k+1}(A) = 5.5 \\times 10^{-11}$, compute the maximum possible value of $\\rho$ implied by your bound from part $2)$. Round your answer to four significant figures. Report only the numerical value of $\\rho$ with no units.",
            "solution": "Let $A \\in \\mathbb{R}^{m \\times n}$ with $m \\geq n$. The computed singular value decomposition (SVD) $(\\widehat{U}, \\widehat{\\Sigma}, \\widehat{V})$ is the exact SVD of a perturbed matrix $A + \\Delta A$, where the perturbation is bounded by $\\|\\Delta A\\|_{2} \\leq \\gamma_{mn}\\, u\\, \\|A\\|_{2}$. The computed rank-$k$ truncation is $\\widehat{A}_{k} \\coloneqq \\widehat{U}_{:,1:k}\\,\\widehat{\\Sigma}_{1:k,1:k}\\,\\widehat{V}_{:,1:k}^{\\top}$. By the Eckart-Young-Mirsky theorem, $\\widehat{A}_{k}$ is the best rank-$k$ approximation of $A + \\Delta A$ in any unitarily invariant norm $\\|\\cdot\\|_{g}$.\n\nLet $E_k(M)$ denote the error of the best rank-$k$ approximation to a matrix $M$ in the norm $\\|\\cdot\\|_g$. That is, $E_k(M) \\coloneqq \\min_{\\text{rank}(B)=k} \\|M - B\\|_g$. With this notation, the fact that $\\widehat{A}_k$ is the best rank-$k$ approximation of $A+\\Delta A$ is expressed as $\\|(A+\\Delta A) - \\widehat{A}_k\\|_g = E_k(A+\\Delta A)$. The best rank-$k$ error of the original matrix $A$ is $E_k(A)$.\n\n**1) Derivation of two-sided additive bounds**\n\nFirst, we establish a perturbation bound for the best approximation error $E_k(\\cdot)$. For any rank-$k$ matrix $B_k$, the triangle inequality gives\n$$ E_k(A+\\Delta A) = \\min_{\\text{rank}(B)=k} \\|(A+\\Delta A) - B\\|_g \\leq \\|(A+\\Delta A) - B_k\\|_g \\leq \\|A-B_k\\|_g + \\|\\Delta A\\|_g. $$\nSince this holds for any rank-$k$ matrix $B_k$, it must hold for the one that minimizes $\\|A-B_k\\|_g$. Thus,\n$$ E_k(A+\\Delta A) \\leq E_k(A) + \\|\\Delta A\\|_g. $$\nBy a symmetric argument, swapping the roles of $A$ and $A+\\Delta A$ (and using $-\\Delta A$ as the perturbation), we have\n$$ E_k(A) \\leq E_k(A+\\Delta A) + \\|-\\Delta A\\|_g = E_k(A+\\Delta A) + \\|\\Delta A\\|_g. $$\nThis implies $E_k(A+\\Delta A) \\geq E_k(A) - \\|\\Delta A\\|_g$. Combining these two inequalities gives a perturbation result for the best rank-$k$ approximation error:\n$$ |E_k(A+\\Delta A) - E_k(A)| \\leq \\|\\Delta A\\|_g. $$\nNow we bound the computed truncation error $\\|A - \\widehat{A}_k\\|_g$. We use the triangle inequality on the expression $A - \\widehat{A}_k = ((A+\\Delta A) - \\widehat{A}_k) - \\Delta A$.\n\nFor the upper bound:\n$$ \\|A - \\widehat{A}_k\\|_g \\leq \\|(A+\\Delta A) - \\widehat{A}_k\\|_g + \\|\\Delta A\\|_g $$\n$$ \\|A - \\widehat{A}_k\\|_g \\leq E_k(A+\\Delta A) + \\|\\Delta A\\|_g. $$\nUsing the perturbation result $E_k(A+\\Delta A) \\leq E_k(A) + \\|\\Delta A\\|_g$, we get:\n$$ \\|A - \\widehat{A}_k\\|_g \\leq (E_k(A) + \\|\\Delta A\\|_g) + \\|\\Delta A\\|_g = E_k(A) + 2\\|\\Delta A\\|_g. $$\nFor the lower bound, we use the reverse triangle inequality:\n$$ \\|A - \\widehat{A}_k\\|_g \\geq \\|(A+\\Delta A) - \\widehat{A}_k\\|_g - \\|\\Delta A\\|_g $$\n$$ \\|A - \\widehat{A}_k\\|_g \\geq E_k(A+\\Delta A) - \\|\\Delta A\\|_g. $$\nUsing the perturbation result $E_k(A+\\Delta A) \\geq E_k(A) - \\|\\Delta A\\|_g$, we get:\n$$ \\|A - \\widehat{A}_k\\|_g \\geq (E_k(A) - \\|\\Delta A\\|_g) - \\|\\Delta A\\|_g = E_k(A) - 2\\|\\Delta A\\|_g. $$\nCombining these results yields the two-sided additive bounds relating the computed truncation error to the best rank-$k$ error:\n$$ E_k(A) - 2\\|\\Delta A\\|_g \\leq \\|A - \\widehat{A}_k\\|_g \\leq E_k(A) + 2\\|\\Delta A\\|_g. $$\n\n**2) Specialization to the spectral norm**\n\nFor the spectral norm, $\\|\\cdot\\|_g = \\|\\cdot\\|_2$, the Eckart-Young-Mirsky theorem states that the best rank-$k$ approximation error is the $(k+1)$-st singular value. Thus, $E_k(A) = \\sigma_{k+1}(A)$.\nSubstituting this into the bounds from part 1 gives:\n$$ \\sigma_{k+1}(A) - 2\\|\\Delta A\\|_2 \\leq \\|A - \\widehat{A}_k\\|_2 \\leq \\sigma_{k+1}(A) + 2\\|\\Delta A\\|_2. $$\nThe problem defines the ratio $\\rho \\coloneqq \\dfrac{\\|A - \\widehat{A}_{k}\\|_{2}}{\\sigma_{k+1}(A)}$. Assuming $\\sigma_{k+1}(A) > 0$, we can divide the inequality by $\\sigma_{k+1}(A)$:\n$$ \\frac{\\sigma_{k+1}(A) - 2\\|\\Delta A\\|_2}{\\sigma_{k+1}(A)} \\leq \\rho \\leq \\frac{\\sigma_{k+1}(A) + 2\\|\\Delta A\\|_2}{\\sigma_{k+1}(A)}. $$\nThis simplifies to the explicit bound on $\\rho$:\n$$ 1 - \\frac{2\\|\\Delta A\\|_2}{\\sigma_{k+1}(A)} \\leq \\rho \\leq 1 + \\frac{2\\|\\Delta A\\|_2}{\\sigma_{k+1}(A)}. $$\n\n**3) Computation of the maximum possible value of $\\rho$**\n\nFrom the bound derived in part 2, the maximum possible value of $\\rho$ is given by the upper bound:\n$$ \\rho_{\\max} = 1 + \\frac{2\\|\\Delta A\\|_2}{\\sigma_{k+1}(A)}. $$\nTo find the maximum value of $\\rho$ implied by this bound, we must use the maximum possible value for $\\|\\Delta A\\|_2$ given by the backward stability model:\n$$ \\|\\Delta A\\|_2 \\leq \\gamma_{mn}\\, u\\, \\|A\\|_{2}. $$\nThe maximum value of the bound for $\\rho$ is therefore:\n$$ \\rho_{\\max} \\leq 1 + \\frac{2\\, \\gamma_{mn}\\, u\\, \\|A\\|_2}{\\sigma_{k+1}(A)}. $$\nWe are given the following data:\n$m = 100$, $n = 100$.\n$\\gamma_{mn} = 10n = 10 \\times 100 = 1000$.\n$u = 2^{-53}$.\n$\\|A\\|_2 = 10$.\n$\\sigma_{k+1}(A) = 5.5 \\times 10^{-11}$.\n\nSubstituting these values into the expression for the maximum $\\rho$:\n$$ \\rho_{\\max} \\leq 1 + \\frac{2 \\times 1000 \\times 2^{-53} \\times 10}{5.5 \\times 10^{-11}} $$\n$$ \\rho_{\\max} \\leq 1 + \\frac{2 \\times 10^4 \\times 2^{-53}}{5.5 \\times 10^{-11}}. $$\nWe compute the value of the fractional term. The unit roundoff $u = 2^{-53}$ is approximately $1.110223 \\times 10^{-16}$.\n$$ \\frac{2 \\times 10^4 \\times (1.110223 \\times 10^{-16})}{5.5 \\times 10^{-11}} = \\frac{2.220446 \\times 10^{-12}}{5.5 \\times 10^{-11}} = \\frac{2.220446}{55} \\approx 0.0403717. $$\nThus, the maximum value for $\\rho$ implied by the bound is:\n$$ \\rho_{\\max} \\approx 1 + 0.0403717 = 1.0403717. $$\nRounding this result to four significant figures gives $1.040$.",
            "answer": "$$\n\\boxed{1.040}\n$$"
        }
    ]
}