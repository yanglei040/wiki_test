{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in evaluating any numerical algorithm is to understand its computational cost. This practice guides you through a foundational analysis of randomized SVD, comparing its operational count against both its power-iterated variant and a classic deterministic method. By deriving the costs in terms of floating-point operations (flops) and data passes, you will gain a concrete understanding of the trade-offs involved and see precisely why randomized algorithms are so compelling for large-scale data .",
            "id": "3570685",
            "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$, a target rank $k \\ll \\min\\{m,n\\}$, an oversampling parameter $p \\geq 0$, and a nonnegative integer $q$ denoting the number of power iterations. Define the sketch size $\\ell = k + p$. The goal is to estimate algorithmic costs for two randomized range-finding procedures used in randomized Singular Value Decomposition (SVD), and to compare these costs to a deterministic truncated SVD computed by the Golub–Kahan–Lanczos bidiagonalization process.\n\nThe basic randomized range finder constructs an orthonormal basis $Q \\in \\mathbb{R}^{m \\times \\ell}$ for the approximate range of $A$ by drawing a test matrix $\\Omega \\in \\mathbb{R}^{n \\times \\ell}$, forming $Y = A \\Omega$, and computing a thin orthonormal–triangular factorization (QR) of $Y$, i.e., $Y = Q R$ with $Q^{\\top}Q = I_{\\ell}$. Its power-iterated variant forms $Y = (A A^{\\top})^{q} A \\Omega$ via a sequence of alternating multiplications by $A^{\\top}$ and $A$, starting with $Y_{0} = A \\Omega$ and iterating $Y_{i} = A \\big( A^{\\top} Y_{i-1} \\big)$ for $i = 1,2,\\dots,q$, then performs a thin QR of the final $Y$.\n\nFor the deterministic truncated SVD, consider the Golub–Kahan–Lanczos bidiagonalization process run for $k$ steps to obtain the leading $k$ singular triplets, which at each step applies one multiplication by $A$ and one by $A^{\\top}$ to vectors.\n\nAdopt the following operation-count model as the fundamental base for cost derivations:\n- The cost of multiplying an $m \\times n$ matrix by an $n \\times r$ matrix is $2 m n r$ floating-point operations (flops).\n- The cost of multiplying an $n \\times m$ matrix by an $m \\times r$ matrix is $2 m n r$ flops.\n- The cost of a thin Householder orthonormal–triangular factorization (QR) of an $m \\times r$ matrix with $m \\geq r$ is $2 m r^{2} - \\tfrac{2}{3} r^{3}$ flops.\n- A single pass over $A$ is defined as one full multiplication by $A$ or $A^{\\top}$ with any operand (matrix or vector); in other words, each application of $A$ and each application of $A^{\\top}$ counts as one pass.\n\nUnder this model, derive symbolic expressions for:\n1. The total flop count and the number of passes over $A$ required by the basic randomized range finder (one multiplication $Y = A \\Omega$ followed by a thin QR of $Y$).\n2. The total flop count and the number of passes over $A$ required by the power-iterated variant ($q$ power iterations as described, followed by a thin QR of $Y$).\n3. The total flop count and the number of passes over $A$ required by the deterministic truncated SVD via Golub–Kahan–Lanczos bidiagonalization run for $k$ steps.\n\nExpress each cost exactly in terms of $m$, $n$, $k$, $p$, and $q$ (with $\\ell = k + p$). Focus only on the dominant operations specified in the model above; neglect random matrix generation, small dense factorizations unrelated to the described steps, and any reorthogonalization overhead in the deterministic method beyond the stated matrix–vector multiplies. No rounding is required. Provide your final answer as a single row matrix with six entries in the order:\n$(\\text{flops}_{\\text{basic}}, \\text{passes}_{\\text{basic}}, \\text{flops}_{\\text{power}}, \\text{passes}_{\\text{power}}, \\text{flops}_{\\text{Lanczos}}, \\text{passes}_{\\text{Lanczos}})$.",
            "solution": "The problem statement is parsed and validated. All required data, definitions, and cost models are explicitly provided. The problem is scientifically grounded in the field of numerical linear algebra, specifically concerning randomized algorithms for matrix factorizations. It is well-posed, objective, and internally consistent. No flaws are identified. Therefore, a solution will be derived.\n\nThe task is to determine the floating-point operation (flop) counts and the number of passes over the matrix $A$ for three different algorithms, based on the provided cost model. Let $A \\in \\mathbb{R}^{m \\times n}$, the target rank be $k$, the oversampling parameter be $p$, and the sketch size be $\\ell = k + p$. The number of power iterations is $q$.\n\nThe cost model is as follows:\n- Matrix multiplication of an $m \\times n$ matrix by an $n \\times r$ matrix costs $2mnr$ flops.\n- Thin Householder QR factorization of an $m \\times r$ matrix ($m \\geq r$) costs $2mr^2 - \\frac{2}{3}r^3$ flops.\n- A pass over $A$ is one multiplication by $A$ or $A^\\top$.\n\n1. **Basic Randomized Range Finder**\nThis procedure consists of two stages:\na) Forming the sketched matrix $Y = A \\Omega$, where $A \\in \\mathbb{R}^{m \\times n}$ and $\\Omega \\in \\mathbb{R}^{n \\times \\ell}$.\nb) Computing the thin QR factorization of $Y \\in \\mathbb{R}^{m \\times \\ell}$.\n\na) The cost of forming $Y = A \\Omega$ is a matrix-matrix multiplication of an $m \\times n$ matrix by an $n \\times \\ell$ matrix. According to the model, with $r = \\ell$, the cost is:\n$$ \\text{flops}(A\\Omega) = 2mn\\ell $$\nThis step involves one multiplication by $A$, which constitutes $1$ pass over $A$.\n\nb) The cost of computing the thin QR factorization of $Y \\in \\mathbb{R}^{m \\times \\ell}$ is given by the model with $r = \\ell$:\n$$ \\text{flops}(QR(Y)) = 2m\\ell^2 - \\frac{2}{3}\\ell^3 $$\nThis step does not involve the matrix $A$.\n\nThe total flop count is the sum of these costs. Substituting $\\ell = k+p$:\n$$ \\text{flops}_{\\text{basic}} = 2mn\\ell + 2m\\ell^2 - \\frac{2}{3}\\ell^3 = 2mn(k+p) + 2m(k+p)^2 - \\frac{2}{3}(k+p)^3 $$\nThe total number of passes is:\n$$ \\text{passes}_{\\text{basic}} = 1 $$\n\n2. **Power-Iterated Randomized Range Finder**\nThis procedure involves three stages:\na) Forming the initial sketched matrix $Y_0 = A \\Omega$.\nb) Performing $q$ power iterations: $Y_i = A (A^{\\top} Y_{i-1})$ for $i = 1, \\dots, q$.\nc) Computing the thin QR factorization of the final matrix $Y = Y_q$.\n\na) The cost of forming $Y_0 = A \\Omega$ is identical to the first step of the basic method:\n$$ \\text{flops}(Y_0) = 2mn\\ell $$\nThis step uses $1$ pass over $A$.\n\nb) Each of the $q$ power iterations involves two matrix-matrix multiplications:\n- First, forming $Z_i = A^{\\top} Y_{i-1}$, where $A^{\\top} \\in \\mathbb{R}^{n \\times m}$ and $Y_{i-1} \\in \\mathbb{R}^{m \\times \\ell}$. The cost, per the model, is $2nm\\ell$ flops. This is $1$ pass over $A$ (via $A^\\top$).\n- Second, forming $Y_i = A Z_i$, where $A \\in \\mathbb{R}^{m \\times n}$ and $Z_i \\in \\mathbb{R}^{n \\times \\ell}$. The cost is $2mn\\ell$ flops. This is another pass over $A$.\nThe total cost for one power iteration is $2nm\\ell + 2mn\\ell = 4mn\\ell$ flops, and it requires $2$ passes over $A$.\nFor $q$ iterations, the total cost and passes are:\n$$ \\text{flops}(\\text{power iterations}) = q \\cdot (4mn\\ell) = 4qmn\\ell $$\n$$ \\text{passes}(\\text{power iterations}) = 2q $$\n\nc) The cost of the final QR factorization of $Y_q \\in \\mathbb{R}^{m \\times \\ell}$ is the same as in the basic method:\n$$ \\text{flops}(QR(Y_q)) = 2m\\ell^2 - \\frac{2}{3}\\ell^3 $$\n\nThe total flop count is the sum of the costs from all three stages. Substituting $\\ell = k+p$:\n$$ \\text{flops}_{\\text{power}} = 2mn\\ell + 4qmn\\ell + 2m\\ell^2 - \\frac{2}{3}\\ell^3 = (2+4q)mn\\ell + 2m\\ell^2 - \\frac{2}{3}\\ell^3 $$\n$$ \\text{flops}_{\\text{power}} = 2(1+2q)mn(k+p) + 2m(k+p)^2 - \\frac{2}{3}(k+p)^3 $$\nThe total number of passes is the sum of passes from stages (a) and (b):\n$$ \\text{passes}_{\\text{power}} = 1 + 2q $$\n\n3. **Deterministic Truncated SVD via Golub–Kahan–Lanczos Bidiagonalization**\nThis procedure is run for $k$ steps. The problem states that each step applies one multiplication by $A$ and one by $A^{\\top}$ to vectors. We neglect any other costs, as instructed.\n\nA matrix-vector multiplication is a special case of a matrix-matrix multiplication where one matrix has a single column ($r=1$).\n- The cost of multiplying $A \\in \\mathbb{R}^{m \\times n}$ by a vector $\\mathbf{v} \\in \\mathbb{R}^{n \\times 1}$ is $2mn(1) = 2mn$ flops. This is $1$ pass over $A$.\n- The cost of multiplying $A^{\\top} \\in \\mathbb{R}^{n \\times m}$ by a vector $\\mathbf{u} \\in \\mathbb{R}^{m \\times 1}$ is $2nm(1) = 2nm$ flops. This is $1$ pass over $A$ (via $A^\\top$).\n\nThe total cost for one step is the sum of these two multiplications:\n$$ \\text{flops}(\\text{1 step}) = 2mn + 2nm = 4mn $$\nThe number of passes for one step is $1 + 1 = 2$.\n\nFor $k$ steps, the total cost and number of passes are:\n$$ \\text{flops}_{\\text{Lanczos}} = k \\cdot (4mn) = 4mnk $$\n$$ \\text{passes}_{\\text{Lanczos}} = k \\cdot 2 = 2k $$\n\nSummary of Results:\n- Basic randomized range finder:\n  - Flops: $2mn(k+p) + 2m(k+p)^2 - \\frac{2}{3}(k+p)^3$\n  - Passes: $1$\n- Power-iterated variant:\n  - Flops: $2(1+2q)mn(k+p) + 2m(k+p)^2 - \\frac{2}{3}(k+p)^3$\n  - Passes: $1+2q$\n- Lanczos bidiagonalization:\n  - Flops: $4mnk$\n  - Passes: $2k$\n\nThese results will be compiled into a single row matrix for the final answer.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2mn(k+p) + 2m(k+p)^2 - \\frac{2}{3}(k+p)^3 & 1 & 2(1+2q)mn(k+p) + 2m(k+p)^2 - \\frac{2}{3}(k+p)^3 & 1+2q & 4mnk & 2k\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Once a low-rank approximation is computed, how can we efficiently verify its quality without computing the full, costly error term? This exercise delves into a powerful technique: using a random probe to estimate the approximation error. You will analyze the statistical properties of a single-probe Gaussian estimator, deriving its bias and variance to understand its reliability and underlying principles .",
            "id": "3570736",
            "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ be a fixed matrix and let $Q \\in \\mathbb{R}^{m \\times k}$ have orthonormal columns obtained from a randomized range finder in a randomized Singular Value Decomposition (SVD) algorithm. Define the residual operator $B \\in \\mathbb{R}^{m \\times n}$ by $B = (I_{m} - Q Q^{\\top}) A$, where $I_{m}$ is the $m \\times m$ identity. Consider a single-probe Gaussian residual norm estimator constructed by drawing $z \\sim \\mathcal{N}(0, I_{n})$ and computing the observable\n$$\n\\widehat{S} \\;=\\; \\|B z\\|_{2}^{2} \\;=\\; z^{\\top} B^{\\top} B z,\n$$\nwhich is used in practice to approximate the residual size $\\|A - Q Q^{\\top} A\\|$. In this problem, regard $\\widehat{S}$ as an estimator for the squared Frobenius residual\n$$\nS \\;=\\; \\|B\\|_{F}^{2} \\;=\\; \\operatorname{tr}(B^{\\top} B).\n$$\nStarting from first principles in probability and linear algebra, namely the isotropy of the standard normal distribution, properties of orthogonal projectors, and the moment identities for Gaussian quadratic forms, derive closed-form expressions for:\n- the bias of $\\widehat{S}$ as an estimator of $S$, namely $\\mathbb{E}[\\widehat{S}] - S$, and\n- the variance $\\operatorname{Var}(\\widehat{S})$,\nexpressed explicitly in terms of the singular values $\\{\\sigma_{i}(B)\\}_{i=1}^{\\rho}$ of $B$, where $\\rho = \\operatorname{rank}(B)$. Your final answer must be a single analytical expression containing both the bias and the variance in a single row matrix, in terms of $\\{\\sigma_{i}(B)\\}$ only. No numerical approximation is required.",
            "solution": "We begin by recalling the setting. The matrix $Q \\in \\mathbb{R}^{m \\times k}$ has orthonormal columns so that $Q^{\\top} Q = I_{k}$, and the matrix $I_{m} - Q Q^{\\top}$ is the orthogonal projector onto the orthogonal complement of the column space of $Q$. The residual operator is $B = (I_{m} - Q Q^{\\top}) A \\in \\mathbb{R}^{m \\times n}$. The Gaussian probe is $z \\sim \\mathcal{N}(0, I_{n})$, so $z$ has zero mean and identity covariance, i.e., $\\mathbb{E}[z] = 0$ and $\\mathbb{E}[z z^{\\top}] = I_{n}$. The estimator is $\\widehat{S} = \\|B z\\|_{2}^{2} = z^{\\top} B^{\\top} B z$.\n\nWe aim to compute the bias and variance of $\\widehat{S}$ as an estimator of $S = \\|B\\|_{F}^{2} = \\operatorname{tr}(B^{\\top} B)$, in closed form. The core probabilistic tool we use is the identity for quadratic forms in a standard normal vector. Let $M \\in \\mathbb{R}^{n \\times n}$ be a fixed symmetric matrix and $z \\sim \\mathcal{N}(0, I_{n})$. Then the following facts hold:\n1. $\\mathbb{E}[z^{\\top} M z] = \\operatorname{tr}(M)$.\n2. $\\operatorname{Var}(z^{\\top} M z) = 2 \\|M\\|_{F}^{2} = 2 \\operatorname{tr}(M^{2})$.\n\nThese follow from the isotropy of the standard normal distribution and Wick’s (Isserlis’) theorem for fourth moments of Gaussian variables:\n$$\n\\mathbb{E}[z_{i} z_{j} z_{k} z_{l}] = \\delta_{i j} \\delta_{k l} + \\delta_{i k} \\delta_{j l} + \\delta_{i l} \\delta_{j k},\n$$\nwhere $\\delta_{i j}$ is the Kronecker delta.\n\nWe apply these identities with $M = B^{\\top} B$. Note that $M$ is symmetric positive semidefinite. Then\n$$\n\\widehat{S} = z^{\\top} M z.\n$$\n\nFirst, compute the expectation:\n$$\n\\mathbb{E}[\\widehat{S}] = \\mathbb{E}[z^{\\top} M z] = \\operatorname{tr}(M) = \\operatorname{tr}(B^{\\top} B) = \\|B\\|_{F}^{2} = S.\n$$\nTherefore, the bias is\n$$\n\\mathbb{E}[\\widehat{S}] - S = 0.\n$$\n\nNext, compute the variance:\n$$\n\\operatorname{Var}(\\widehat{S}) = \\operatorname{Var}(z^{\\top} M z) = 2 \\operatorname{tr}(M^{2}) = 2 \\|M\\|_{F}^{2}.\n$$\nTo express this in terms of the singular values of $B$, recall that if $\\{\\sigma_{i}(B)\\}_{i=1}^{\\rho}$ are the nonzero singular values of $B$, then the eigenvalues of $M = B^{\\top} B$ are $\\{\\sigma_{i}(B)^{2}\\}_{i=1}^{\\rho}$ together with zeros. Hence,\n$$\n\\operatorname{tr}(M) = \\sum_{i=1}^{\\rho} \\sigma_{i}(B)^{2}, \\quad \\operatorname{tr}(M^{2}) = \\sum_{i=1}^{\\rho} \\sigma_{i}(B)^{4}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(\\widehat{S}) = 2 \\sum_{i=1}^{\\rho} \\sigma_{i}(B)^{4}.\n$$\n\nIn summary, for the single-probe Gaussian estimator of the squared residual norm,\n$$\n\\widehat{S} = \\|(I_{m} - Q Q^{\\top}) A z\\|_{2}^{2},\n$$\nthe bias equals zero and the variance equals $2 \\sum_{i=1}^{\\rho} \\sigma_{i}(B)^{4}$, where $B = (I_{m} - Q Q^{\\top}) A$ and $\\{\\sigma_{i}(B)\\}$ are its singular values. These identities are exact and follow from the fundamental properties of Gaussian quadratic forms and the Singular Value Decomposition (SVD).",
            "answer": "$$\\boxed{\\begin{pmatrix} 0 & 2 \\sum_{i=1}^{\\rho} \\sigma_{i}(B)^{4} \\end{pmatrix}}$$"
        },
        {
            "introduction": "The true power of randomized algorithms is realized when dealing with massive datasets that exceed a computer's main memory. This advanced practice challenges you to design a memory-aware randomized SVD algorithm, bridging the gap between theoretical error bounds and practical hardware constraints. By carefully analyzing the memory footprint of a blocked implementation, you will derive the optimal block size that respects a given memory budget while satisfying a target accuracy guarantee .",
            "id": "3570738",
            "problem": "Consider a blocked implementation of the Randomized Singular Value Decomposition (RSVD), where the objective is to approximate the leading $k$ singular components of a real matrix $A \\in \\mathbb{R}^{m \\times n}$ by constructing an orthonormal basis $Q \\in \\mathbb{R}^{m \\times r}$ with $r = k + p$, where $p$ is an oversampling parameter. You are given a memory budget of $B$ machine words for all in-core arrays. Assume the following algorithmic structure and implementation constraints, which you must use in your reasoning:\n\n- A Gaussian test matrix $\\Omega \\in \\mathbb{R}^{n \\times r}$ is used to build a sample matrix $Y = A \\Omega$. To respect the memory budget, $A$ is accessed in column blocks of size $s$, and the corresponding blocks of $\\Omega$ are applied so that $Y$ is accumulated in-core in a single pass over the columns of $A$.\n- The matrix $Y \\in \\mathbb{R}^{m \\times r}$ is orthonormalized to produce $Q \\in \\mathbb{R}^{m \\times r}$ using a numerically stable orthonormalization routine (for which you may assume an $O(r^{2})$ in-core scratch storage, treated explicitly in your memory count as an $r \\times r$ array).\n- Instead of forming $B = Q^{\\top} A \\in \\mathbb{R}^{r \\times n}$ explicitly, the algorithm forms the $r \\times r$ covariance matrix $F = Q^{\\top} A A^{\\top} Q$ by streaming $A$ again in column blocks of the same size $s$, accumulating $F \\leftarrow F + (Q^{\\top} A_{j})(Q^{\\top} A_{j})^{\\top}$ for each block $A_{j} \\in \\mathbb{R}^{m \\times s}$. This avoids storing $B$ or the full $A$ in-core.\n- You must ensure an expected spectral-norm relative error control of the form $\\mathbb{E}\\!\\left[\\lVert A - Q Q^{\\top} A \\rVert_{2}\\right] \\leq (1 + \\epsilon)\\, \\sigma_{k+1}$, where $\\sigma_{k+1}$ denotes the $(k+1)$-st singular value of $A$, using $q \\geq 0$ power iterations in the standard power scheme for RSVD and a Gaussian test matrix. You may assume that oversampling $p \\geq 4$ is permitted and that well-tested expectation bounds for Gaussian sketching and power iteration apply.\n\nTasks:\n- Starting from core definitions of the matrices involved, the blocked matrix products, and orthonormalization, derive the exact in-core memory footprint (in machine words) of the dominant steps of this blocked RSVD pipeline, as a function of $m$, $n$, $k$, $p$, and the block size $s$. Explicitly account for all arrays that must reside in memory simultaneously at each step in order to argue a single upper bound expression $M(s)$ that safely dominates the in-core usage across the entire algorithm.\n- Using well-tested error bounds for Gaussian RSVD with $q$ power iterations, derive a sufficient closed-form inequality on the oversampling $p$ in terms of $k$, $q$, and $\\epsilon$ that guarantees the stated error target. Convert this inequality into an explicit closed-form lower bound on $p$.\n- Combine the memory footprint and the oversampling requirement to produce a single, closed-form analytical expression for the largest admissible integer block size $s^{\\star}$ that satisfies the in-core memory budget $B$ and achieves the error target. Your design must enforce that $s^{\\star} \\leq n$ and that $s^{\\star} \\geq 0$.\n\nProvide your final answer as a single closed-form mathematical expression for $s^{\\star}$ in terms of $m$, $n$, $k$, $\\epsilon$, $q$, and $B$. Do not include any numerical evaluation. Do not include any units in the final expression. If you must use any non-elementary functions or integer-valued operators, define them implicitly in your derivation but express the final formula using standard mathematical symbols such as $\\lfloor \\cdot \\rfloor$, $\\lceil \\cdot \\rceil$, $\\min$, and $\\max$ only.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of numerical linear algebra, specifically randomized algorithms, and is well-posed, objective, and contains sufficient information for a unique solution.\n\nThe solution is derived in three parts: first, the memory footprint of the described algorithm is determined; second, the requirement on the oversampling parameter $p$ is derived from the given error bound; and third, these two results are combined to find the maximum allowed block size $s^{\\star}$.\n\n**Part 1: Derivation of the In-Core Memory Footprint**\n\nThe total memory footprint is the maximum memory required at any point during the algorithm. The algorithm consists of two main passes over the matrix $A$. We analyze the memory requirements for each pass. The sketch size is $r = k+p$.\n\nPass 1: Generation of the orthonormal basis $Q$.\nThis phase computes $Y = (A A^{\\top})^q A \\Omega$ and then orthonormalizes $Y$ to get $Q$.\n\nCase $q=0$: The sample matrix is $Y = A \\Omega$. The computation is blocked: $Y = \\sum_j A_j \\Omega_j$. At each step in this summation, the memory must hold the accumulator $Y \\in \\mathbb{R}^{m \\times r}$, the current matrix block $A_j \\in \\mathbb{R}^{m \\times s}$, and the corresponding test matrix block $\\Omega_j \\in \\mathbb{R}^{s \\times r}$. The memory required is $m r + m s + s r$.\n\nCase $q \\geq 1$: The sample matrix is $Y = (A A^{\\top})^q A \\Omega$. This is computed iteratively. Let $Y_{(0)} = A\\Omega$. The memory for this initial step is $m r + m s + s r$. Then, for $i=1, \\dots, q$, we compute $Y_{(i)} = A A^{\\top} Y_{(i-1)}$. This is performed in a blocked fashion as $Y_{(i)} = \\sum_j A_j (A_j^{\\top} Y_{(i-1)})$. To perform one update step in this summation, we need to hold in memory:\n1. The input vector from the previous iteration, $Y_{(i-1)} \\in \\mathbb{R}^{m \\times r}$ (size $m r$).\n2. The accumulator for the current iteration's output, $Y_{(i)} \\in \\mathbb{R}^{m \\times r}$ (size $m r$).\n3. The current block of the matrix, $A_j \\in \\mathbb{R}^{m \\times s}$ (size $m s$).\n4. The intermediate product $A_j^{\\top} Y_{(i-1)} \\in \\mathbb{R}^{s \\times r}$ (size $s r$).\nThe peak memory usage during a power iteration step is thus $m s + 2 m r + s r$. This is greater than the initial sampling step's memory. Thus, for $q \\ge 1$, the peak memory for sampling is $m s + 2 m r + s r$.\n\nWe can express the sampling memory requirement for any $q \\geq 0$ as $M_{samp}(s) = m s + s r + (1 + \\min(1, q)) m r$.\n\nAfter computing the final sample matrix $Y$, it is orthonormalized to $Q \\in \\mathbb{R}^{m \\times r}$. This step requires the matrix $Y$ (size $m r$) and an in-core scratch space of size $r \\times r$. The memory for orthonormalization is $M_{ortho} = m r + r^2$.\n\nThe total memory for the first pass is the maximum of the sampling and orthonormalization stages: $M_{pass1}(s) = \\max(M_{samp}(s), M_{ortho}) = \\max(m s + s r + (1 + \\min(1, q)) m r, m r + r^2)$.\n\nPass 2: Formation of the covariance matrix $F$.\nThis pass computes $F = Q^{\\top} A A^{\\top} Q$ by accumulating block-wise updates: $F \\leftarrow F + (Q^{\\top} A_j)(Q^{\\top} A_j)^{\\top}$. At each step, the memory must hold:\n1. The orthonormal basis $Q \\in \\mathbb{R}^{m \\times r}$ (size $m r$).\n2. The current matrix block $A_j \\in \\mathbb{R}^{m \\times s}$ (size $m s$).\n3. The accumulator for $F \\in \\mathbb{R}^{r \\times r}$ (size $r^2$).\n4. The intermediate product $Q^{\\top} A_j \\in \\mathbb{R}^{r \\times s}$ (size $r s$).\nThe total memory for this pass is $M_{pass2}(s) = m r + m s + r^2 + r s$.\n\nOverall Memory Footprint $M(s)$:\nThe peak memory for the entire algorithm is $M(s) = \\max(M_{pass1}(s), M_{pass2}(s))$.\n$M(s) = \\max(\\max(m s + s r + (1 + \\min(1, q)) m r, m r + r^2), m r + m s + r^2 + r s)$.\nSince $m r + m s + r^2 + r s = (m r + r^2) + s(m+r)$, the term for Pass 2 is strictly larger than the orthonormalization memory $m r + r^2$ (for $s > 0$).\nThus, the expression simplifies to:\n$M(s) = \\max(m s + s r + (1 + \\min(1, q)) m r, m s + s r + m r + r^2)$.\nThis can be rewritten by factoring out the common terms:\n$M(s) = s(m+r) + m r + \\max(\\min(1, q) m r, r^2)$.\n\n**Part 2: Derivation of the Oversampling Parameter analytical expression**\n\nThe problem requires an error bound of the form $\\mathbb{E}\\!\\left[\\lVert A - Q Q^{\\top} A \\rVert_{2}\\right] \\leq (1 + \\epsilon)\\, \\sigma_{k+1}$. Standard theoretical results for RSVD with $q$ power iterations and a Gaussian test matrix provide a bound which, under the assumption of a sufficient spectral gap (making tail-chasing terms negligible), can be expressed as:\n$$ \\mathbb{E}\\!\\left[\\lVert A - Q Q^{\\top} A \\rVert_{2}\\right] \\leq \\left(1 + \\sqrt{\\frac{k}{p-1}}\\right)^{\\frac{1}{2q+1}} \\sigma_{k+1} $$\nTo satisfy the problem's requirement, we must have:\n$$ \\left(1 + \\sqrt{\\frac{k}{p-1}}\\right)^{\\frac{1}{2q+1}} \\leq 1 + \\epsilon $$\nRaising both sides to the power of $2q+1$ gives:\n$$ 1 + \\sqrt{\\frac{k}{p-1}} \\leq (1+\\epsilon)^{2q+1} $$\n$$ \\sqrt{\\frac{k}{p-1}} \\leq (1+\\epsilon)^{2q+1} - 1 $$\nSquaring both sides (noting both are non-negative for $\\epsilon \\ge 0$):\n$$ \\frac{k}{p-1} \\leq \\left( (1+\\epsilon)^{2q+1} - 1 \\right)^2 $$\n$$ p-1 \\geq \\frac{k}{\\left( (1+\\epsilon)^{2q+1} - 1 \\right)^2} $$\n$$ p \\geq 1 + \\frac{k}{\\left( (1+\\epsilon)^{2q+1} - 1 \\right)^2} $$\nSince $p$ must be an integer and the problem states $p \\geq 4$ is permitted, the smallest sufficient integer value for $p$, which we denote $p^{\\star}$, is:\n$$ p^{\\star} = \\max\\left(4, \\left\\lceil 1 + \\frac{k}{\\left((1+\\epsilon)^{2q+1} - 1\\right)^2} \\right\\rceil\\right) $$\n\n**Part 3: Derivation of the Largest Admissible Block Size**\n\nWe combine the results from the previous parts. The total memory footprint $M(s)$ must not exceed the budget $B$. We use the minimal sufficient sketch size $r^{\\star} = k+p^{\\star}$.\n$$ s(m+r^{\\star}) + m r^{\\star} + \\max(\\min(1, q) m r^{\\star}, (r^{\\star})^2) \\leq B $$\nSolving for $s$:\n$$ s(m+r^{\\star}) \\leq B - m r^{\\star} - \\max(\\min(1, q) m r^{\\star}, (r^{\\star})^2) $$\n$$ s \\leq \\frac{B - m r^{\\star} - \\max(\\min(1, q) m r^{\\star}, (r^{\\star})^2)}{m+r^{\\star}} $$\nThe block size $s$ must be an integer, and is constrained by $0 \\leq s \\leq n$. The largest admissible integer block size $s^{\\star}$ is therefore the floor of the expression above, capped by $n$, and floored at $0$.\n$$ s^{\\star} = \\max\\left(0, \\min\\left(n, \\left\\lfloor \\frac{B - m r^{\\star} - \\max(\\min(1, q) m r^{\\star}, (r^{\\star})^2)}{m+r^{\\star}} \\right\\rfloor\\right)\\right) $$\nSubstituting $r^{\\star} = k+p^{\\star}$ and the full expression for $p^{\\star}$ yields the final closed-form expression for $s^{\\star}$ in terms of the given parameters.\nLet $P_0 = \\max\\left(4, \\left\\lceil 1 + \\frac{k}{\\left((1+\\epsilon)^{2q+1} - 1\\right)^2} \\right\\rceil\\right)$.\nThen $r^{\\star} = k + P_0$.\nThe expression for $s^{\\star}$ becomes:\n$$ s^{\\star} = \\max\\left(0, \\min\\left(n, \\left\\lfloor \\frac{B - m(k+P_0) - \\max(\\min(1, q) m(k+P_0), (k+P_0)^2)}{m+k+P_0} \\right\\rfloor\\right)\\right) $$\nThis expression combines all constraints into a single formula.",
            "answer": "$$ \\boxed{\\max\\left(0, \\min\\left(n, \\left\\lfloor \\frac{B - m\\left(k + \\max\\left(4, \\left\\lceil 1 + \\frac{k}{\\left((1+\\epsilon)^{2q+1} - 1\\right)^2} \\right\\rceil\\right)\\right) - \\max\\left(\\min(1,q)m\\left(k + \\max\\left(4, \\left\\lceil 1 + \\frac{k}{\\left((1+\\epsilon)^{2q+1} - 1\\right)^2} \\right\\rceil\\right)\\right), \\left(k + \\max\\left(4, \\left\\lceil 1 + \\frac{k}{\\left((1+\\epsilon)^{2q+1} - 1\\right)^2} \\right\\rceil\\right)\\right)^2\\right)}{m + k + \\max\\left(4, \\left\\lceil 1 + \\frac{k}{\\left((1+\\epsilon)^{2q+1} - 1\\right)^2} \\right\\rceil\\right)} \\right\\rfloor\\right)\\right)} $$"
        }
    ]
}