{
    "hands_on_practices": [
        {
            "introduction": "To bridge the gap between theoretical algorithm complexity and real-world performance, we must account for the memory hierarchy. This first exercise introduces tiling, or blocking, a fundamental technique for improving temporal locality. By analyzing a blocked matrix transpose, you will derive the optimal tile size that ensures the working set—the data actively being processed—fits within the cache, thereby minimizing capacity misses and reducing data movement from slower main memory. ",
            "id": "3542785",
            "problem": "Consider a blocked matrix transpose of a dense matrix from an input matrix $A \\in \\mathbb{R}^{n \\times n}$ to an output matrix $B \\in \\mathbb{R}^{n \\times n}$, both stored in row-major order. The algorithm processes square tiles of size $b \\times b$, reading a tile $\\mathcal{T}_A(i,j)$ from $A$ and writing its transpose into the corresponding tile $\\mathcal{T}_B(j,i)$ in $B$, until the entire matrix is transposed. Each matrix element occupies $s$ bytes. Assume an ideal two-level memory model with a single data cache that is fully associative, has Least Recently Used (LRU) replacement, and total capacity $M$ bytes. Assume a write-allocate, write-back policy, and neglect additional metadata space such as tag storage. The cache line size is irrelevant in this question; you may model capacity effects at the granularity of bytes, and you may ignore conflict misses because of full associativity.\n\nYou are to reason from first principles of the capacity model: a capacity miss occurs when the set of distinct data needed for reuse within a phase of computation exceeds the cache capacity $M$. For the blocked transpose, suppose the implementation is arranged so that, during the processing of a single pair of tiles $(i,j)$, all accesses remain within the two tiles $\\mathcal{T}_A(i,j)$ and $\\mathcal{T}_B(j,i)$ until that pair is completed.\n\nDerive, in closed form, the optimal integer tile size $b^{\\star}$ as a function of $M$ and $s$ that minimizes capacity misses by ensuring the two tiles can reside in cache simultaneously for the duration of their processing. Express your final answer as a single analytic expression. No numerical evaluation or rounding is required. The final answer must be a single symbolic mathematical expression.",
            "solution": "The blocked transpose processes $b \\times b$ tiles. Under the stated ideal cache model (fully associative with Least Recently Used (LRU) replacement and capacity $M$ bytes), capacity misses are governed by the working set size during the processing of a single tile pair. By assumption, the implementation confines its accesses to the two tiles $\\mathcal{T}_A(i,j)$ and $\\mathcal{T}_B(j,i)$ while transposing that pair.\n\nWe begin from the fundamental capacity principle: under LRU in a fully associative cache, all data that is reused without evictions will hit if the total distinct data referenced between uses does not exceed the cache capacity. Formally, if the reuse distance (measured in bytes) for any datum is at most $M$, then that datum will be present at its reuse and the access will be a cache hit.\n\nIn the present blocked transpose, the working set during one tile-pair processing is the union of the two tiles:\n- The tile $\\mathcal{T}_A(i,j)$ contains $b \\times b$ elements, occupying $b^{2} \\cdot s$ bytes.\n- The tile $\\mathcal{T}_B(j,i)$ contains $b \\times b$ elements, occupying $b^{2} \\cdot s$ bytes.\n\nTherefore, the total resident data required to keep both tiles in the cache simultaneously is\n$$\nW(b) \\;=\\; b^{2} s \\;+\\; b^{2} s \\;=\\; 2 b^{2} s.\n$$\nUnder the ideal capacity model, if $W(b) \\leq M$, then once the tiles are loaded into the cache, LRU will retain them until their processing completes, and the majority of intra-tile accesses will be served as hits (after the initial compulsory loads or write-allocate line fills). If $W(b) > M$, the cache cannot hold both tiles at once, and LRU will evict some portion of one tile while accessing the other, causing repeated capacity misses and thrashing.\n\nTo minimize capacity misses during the tile computation, one should choose the largest $b$ such that both tiles fit in the cache, i.e., such that $W(b) \\leq M$. This yields the inequality\n$$\n2 b^{2} s \\;\\leq\\; M.\n$$\nSolving for $b$ gives\n$$\nb^{2} \\;\\leq\\; \\frac{M}{2 s}\n\\qquad\\Longrightarrow\\qquad\nb \\;\\leq\\; \\sqrt{\\frac{M}{2 s}}.\n$$\nSince $b$ must be a positive integer, the optimal choice $b^{\\star}$ that maximizes $b$ under the capacity constraint is\n$$\nb^{\\star} \\;=\\; \\left\\lfloor \\sqrt{\\frac{M}{2 s}} \\right\\rfloor.\n$$\nThis expression ensures that the two $b \\times b$ tiles occupy at most $M$ bytes, so they can be retained in the cache throughout their processing under LRU, thereby minimizing capacity misses in the blocked transpose.",
            "answer": "$$\\boxed{\\left\\lfloor \\sqrt{\\frac{M}{2 s}} \\right\\rfloor}$$"
        },
        {
            "introduction": "While ensuring the working set fits in cache is necessary, it is not sufficient for high performance. The specific mapping of memory addresses to cache sets can cause conflict misses, where useful data is evicted even when the cache is not full. This practical exercise demonstrates how to diagnose and prevent such a scenario, known as cache-line thrashing, by strategically padding the rows of a matrix. You will apply first principles of cache set-indexing to determine the minimal padding needed to ensure consecutive matrix rows map to different cache sets, a common optimization in high-performance computing libraries. ",
            "id": "3542736",
            "problem": "Consider a computer system with a single-level data cache characterized by capacity $C$, associativity $A$, and cache line size $L$, all chosen as powers of two. For such a cache, the number of sets is $S = \\frac{C}{A L}$, and the set index for a memory address $a$ is defined by the widely used power-of-two mapping rule $s(a) = \\left( \\left\\lfloor \\frac{a}{L} \\right\\rfloor \\right) \\bmod S$. A dense matrix $M \\in \\mathbb{R}^{n \\times m}$ is stored in row-major order contiguous in memory with element size $b$ bytes, and the starting address of row $i$ is $\\alpha + i B$, where $B = m b + p$ is the row stride in bytes including an optional padding $p \\geq 0$. To ensure scientific realism and alignment-independent behavior, the padding $p$ must be chosen so that $B$ is an integer multiple of $L$.\n\nYou are given the specific parameters $C = 32 \\times 2^{10}$ bytes, $A = 8$, $L = 64$ bytes, $b = 8$ bytes (double precision), $m = 1024$, and a base address $\\alpha$ that is aligned to a cache line boundary so that $\\frac{\\alpha}{L} \\in \\mathbb{Z}$. Derive, from first principles of row-major layout and power-of-two set-indexing of caches, the minimal nonnegative padding $p$ in bytes so that consecutive rows $i$ and $i+1$ map to different cache sets, irrespective of $i$.\n\nExpress your final answer in bytes. No rounding is necessary.",
            "solution": "The problem requires finding the minimal non-negative padding $p$ for a row-major matrix such that consecutive rows do not map to the same cache sets. The core of the problem lies in understanding the relationship between memory addresses, the row stride, and the cache's set-indexing mechanism.\n\nFirst, we determine the cache geometry parameters from the givens. The cache capacity is $C = 32 \\times 2^{10}$ bytes, the associativity is $A = 8$, and the cache line size is $L = 64$ bytes. As all these are powers of two, we can write them as $C = 2^5 \\times 2^{10} = 2^{15}$ bytes, $A = 2^3$, and $L = 2^6$ bytes. The number of sets $S$ in the cache is given by the formula:\n$$S = \\frac{C}{A L} = \\frac{2^{15} \\text{ bytes}}{(8) \\cdot (64 \\text{ bytes})} = \\frac{2^{15}}{2^3 \\cdot 2^6} = \\frac{2^{15}}{2^9} = 2^6 = 64$$\nSo, there are $S=64$ sets in the cache.\n\nThe memory address $a$ is mapped to a set with index $s(a)$ given by the power-of-two mapping rule:\n$$s(a) = \\left( \\left\\lfloor \\frac{a}{L} \\right\\rfloor \\right) \\bmod S$$\nThe term $\\lfloor a/L \\rfloor$ represents the cache block address or block index of the memory address $a$.\n\nThe matrix is stored in row-major order. The starting memory address of row $i$ is $a_i = \\alpha + iB$, where $\\alpha$ is the base address of the matrix and $B$ is the row stride in bytes. The row stride is defined as $B = m b + p$, where $m=1024$ is the number of columns, $b=8$ bytes is the size of each element, and $p \\ge 0$ is the padding.\n\nThe problem imposes two critical constraints:\n1. The base address $\\alpha$ is aligned to a cache line boundary, which means $\\frac{\\alpha}{L}$ is an integer. Let $\\frac{\\alpha}{L} = k_{\\alpha}$ where $k_{\\alpha} \\in \\mathbb{Z}$.\n2. The row stride $B$ is an integer multiple of the cache line size $L$. This means $\\frac{B}{L}$ is an integer. Let $\\frac{B}{L} = k_{B}$ where $k_{B} \\in \\mathbb{Z}$.\n\nThe condition to be satisfied is that consecutive rows $i$ and $i+1$ map to different cache sets, for any row index $i$. This can be stated mathematically as $s(a_i) \\neq s(a_{i+1})$, where $a_i$ and $a_{i+1}$ are the starting addresses of row $i$ and row $i+1$, respectively.\n\nLet's analyze the set index for the start of row $i$, $s(a_i)$:\n$$s(a_i) = \\left( \\left\\lfloor \\frac{a_i}{L} \\right\\rfloor \\right) \\bmod S = \\left( \\left\\lfloor \\frac{\\alpha + iB}{L} \\right\\rfloor \\right) \\bmod S$$\nUsing the alignment constraints, we can simplify the floor expression:\n$$\\left\\lfloor \\frac{\\alpha + iB}{L} \\right\\rfloor = \\left\\lfloor \\frac{\\alpha}{L} + i\\frac{B}{L} \\right\\rfloor = \\lfloor k_{\\alpha} + i k_{B} \\rfloor$$\nSince $i$, $k_{\\alpha}$, and $k_{B}$ are all integers, their sum is an integer, so the floor operation is redundant.\n$$s(a_i) = (k_{\\alpha} + ik_{B}) \\bmod S$$\nSimilarly, for row $i+1$, the starting address is $a_{i+1} = \\alpha + (i+1)B$. The set index is:\n$$s(a_{i+1}) = (k_{\\alpha} + (i+1)k_{B}) \\bmod S = (k_{\\alpha} + ik_{B} + k_{B}) \\bmod S$$\n\nThe condition $s(a_i) \\neq s(a_{i+1})$ must hold for any $i$. This is equivalent to stating that the difference in set indices is non-zero (modulo $S$):\n$$(k_{\\alpha} + ik_{B} + k_{B}) - (k_{\\alpha} + ik_{B}) \\not\\equiv 0 \\pmod S$$\n$$k_{B} \\not\\equiv 0 \\pmod S$$\nThis simple condition—that the row stride in units of cache lines ($k_B$) must not be a multiple of the number of cache sets ($S$)—is the key to solving the problem.\n\nNow we must express $k_{B}$ in terms of the padding $p$ and apply the constraints.\n$$k_{B} = \\frac{B}{L} = \\frac{m b + p}{L}$$\nSubstituting the given values: $m=1024$, $b=8$, $L=64$.\n$$k_{B} = \\frac{1024 \\times 8 + p}{64} = \\frac{8192 + p}{64}$$\nThe constraint that $B$ must be a multiple of $L$ means $mb+p$ must be a multiple of $64$.\n$$1024 \\times 8 + p = 8192 + p$$\nSince $8192 = 128 \\times 64$, $8192$ is already a multiple of $64$. For the sum $8192+p$ to be a multiple of $64$, $p$ must also be a multiple of $64$.\nSo, we can write $p = q \\cdot L = 64q$ for some non-negative integer $q \\ge 0$.\n\nLet's substitute $p=64q$ back into the expression for $k_B$:\n$$k_{B} = \\frac{8192 + 64q}{64} = \\frac{8192}{64} + \\frac{64q}{64} = 128 + q$$\nNow we apply the condition $k_{B} \\not\\equiv 0 \\pmod S$ with $S=64$:\n$$128 + q \\not\\equiv 0 \\pmod{64}$$\nSince $128 = 2 \\times 64$, we have $128 \\equiv 0 \\pmod{64}$. The condition simplifies to:\n$$q \\not\\equiv 0 \\pmod{64}$$\nWe need to find the minimal non-negative padding $p$. This corresponds to finding the minimal non-negative integer $q$ that satisfies this condition. The possible values for $q$ are $0, 1, 2, \\dots$.\n- If $q=0$, then $q \\equiv 0 \\pmod{64}$. This violates the condition. This would correspond to $p=0$, which leads to consecutive rows mapping to the same set.\n- The smallest non-negative integer $q$ that is not a multiple of $64$ is $q=1$.\n\nTherefore, the minimal value for $q$ is $1$. The minimal padding $p$ is then:\n$$p_{min} = q_{min} \\cdot L = 1 \\cdot 64 = 64$$\nThe minimal non-negative padding required is $64$ bytes.",
            "answer": "$$\\boxed{64}$$"
        },
        {
            "introduction": "This final practice integrates the concepts of working sets and hierarchical blocking in the context of General Matrix-Matrix Multiplication (GEMM), a cornerstone of numerical linear algebra. You will model a more complex scenario involving three matrices and two levels of blocking: a larger block size $b$ for the cache and a smaller size $r$ for registers. By formulating and solving the capacity constraint for the L1 cache, you will determine the optimal block size that maximizes arithmetic intensity, providing a clear example of how performance is co-designed with the memory hierarchy in mind. ",
            "id": "3542772",
            "problem": "Consider the dense general matrix-matrix multiplication (GEMM) operation $C \\leftarrow C + A B$ on square matrices of dimension $n \\times n$, implemented by a three-loop algorithm enhanced with cache-aware blocking and a register-resident microkernel as follows. The computation is organized into outer tiles of size $b \\times b$ for the submatrix $C[I:I+b-1,\\,J:J+b-1]$, and the inner accumulation over the $k$-dimension proceeds in panels of width $r$ (register blocking of size $r$), so that, for each $k$-panel, an $A$-panel of size $b \\times r$ and a $B$-panel of size $r \\times b$ are reused across all invocations of an $r \\times r$ register-resident microkernel that updates $C$.\n\nAssume the following execution and memory model:\n- The Level-$1$ (L$1$) cache has capacity $M_1$ bytes, is fully associative with least-recently-used replacement, and exhibits no conflict or compulsory misses beyond what is implied by capacity.\n- Each matrix element occupies $s$ bytes (e.g., $s=8$ for double precision), and a fixed fraction $\\phi \\in (0,1)$ of the L$1$ cache is available for the three working data structures of the current $k$-panel update: the $A$-panel of size $b \\times r$, the $B$-panel of size $r \\times b$, and the $C$-tile of size $b \\times b$. The remaining fraction of the cache is reserved for code, stack, and unrelated data.\n- The $r \\times r$ microtiles of $C$ are held in registers during the innermost computation, but the full $b \\times b$ $C$-tile must remain resident in L$1$ over the sweep of microtiles within a given $k$-panel to avoid self-eviction and thereby maximize reuse of the $A$- and $B$-panels inside the L$1$ cache.\n- Ignore integrality constraints on $b$ and $r$ for the purpose of analysis, and neglect write-allocate overheads and cache line effects.\n\nStarting only from these assumptions and first principles about working-set capacity and reuse, derive the capacity constraint that guarantees simultaneous residency in L$1$ of the $A$-panel, the $B$-panel, and the $C$-tile during one $k$-panel update. Then, using this constraint, determine the value of the block size $b$ that maximizes L$1$-level reuse (equivalently, maximizes the arithmetic intensity at the L$1$ level) for a fixed register blocking size $r$.\n\nProvide your final answer as a closed-form analytic expression for the optimal block size $b^{\\star}$ in terms of $M_1$, $r$, $s$, and $\\phi$. Do not round your answer and do not include units in the final expression.",
            "solution": "The problem asks for the optimal block size $b^{\\star}$ for a blocked matrix-matrix multiplication algorithm, $C \\leftarrow C + A B$, that maximizes Level-$1$ (L$1$) cache reuse. The optimization is performed for a fixed register blocking size $r$.\n\nThe solution proceeds in three stages:\n1.  Formulate the L$1$ cache capacity constraint based on the memory footprint of the working set.\n2.  Formulate the objective function, which is the L$1$ arithmetic intensity, and analyze its behavior with respect to the block size $b$.\n3.  Solve the resulting constrained optimization problem to find the optimal block size $b^{\\star}$.\n\n**1. L$1$ Cache Capacity Constraint**\n\nAccording to the problem statement, a single $k$-panel update requires the simultaneous residency of three data structures in the L$1$ cache:\n- An $A$-panel of size $b \\times r$.\n- A $B$-panel of size $r \\times b$.\n- A $C$-tile of size $b \\times b$.\n\nEach matrix element occupies $s$ bytes. The memory footprint of each component is:\n- Memory for $A$-panel: $M_A = (b \\times r) \\cdot s = sbr$ bytes.\n- Memory for $B$-panel: $M_B = (r \\times b) \\cdot s = srb$ bytes.\n- Memory for $C$-tile: $M_C = (b \\times b) \\cdot s = sb^2$ bytes.\n\nThe total memory required for this working set is the sum of the footprints of these three components:\n$$W(b, r) = M_A + M_B + M_C = sbr + srb + sb^2 = s(b^2 + 2br)$$\nThe problem states that a fraction $\\phi$ of the total L$1$ cache capacity, $M_1$, is available for these data structures. Therefore, the total size of the working set cannot exceed this available capacity. This gives us the capacity constraint:\n$$s(b^2 + 2br) \\le \\phi M_1$$\n\n**2. Objective Function: L$1$ Arithmetic Intensity**\n\nThe goal is to maximize \"L$1$-level reuse\". This is equivalent to maximizing the L$1$ arithmetic intensity, $I_{L1}$, defined as the ratio of floating-point operations (FLOPS) performed to the amount of data transferred between main memory (or L$2$ cache) and the L$1$ cache.\n\nWe consider the computation for a single $b \\times b$ tile of the matrix $C$. This involves multiplying a $b \\times n$ sub-matrix of $A$ by an $n \\times b$ sub-matrix of $B$ and accumulating the result into the $C$ tile. The total number of FLOPS for this computation is $2nb^2$.\n\nThe L$1$ data traffic (misses) required to compute this $C$ tile is composed of:\n- Loading the $b \\times b$ $C$-tile from the next level of memory. This tile is then held resident. Traffic: $sb^2$ bytes.\n- Loading the necessary portions of matrices $A$ and $B$. The computation proceeds by sweeping through the $k$-dimension. The entire $b \\times n$ strip of $A$ and the $n \\times b$ strip of $B$ must be read. Traffic for $A$: $sbn$ bytes. Traffic for $B$: $snb$ bytes.\n- Writing the updated $b \\times b$ $C$-tile back to memory. Traffic: $sb^2$ bytes.\n\nThe total data movement to and from a higher level of memory for one $b \\times b$ tile is:\n$$D_{L1}(b) = \\text{reads} + \\text{writes} = (sb^2 + sbn + snb) + sb^2 = s(2b^2 + 2nb)$$\nThe L$1$ arithmetic intensity is therefore:\n$$I_{L1}(b) = \\frac{\\text{FLOPS}}{D_{L1}(b)} = \\frac{2nb^2}{s(2b^2 + 2nb)} = \\frac{2nb^2}{2s(b^2 + nb)} = \\frac{nb}{s(b+n)}$$\nTo find the block size $b$ that maximizes this function, we analyze its derivative with respect to $b$. The constant factors $\\frac{n}{s}$ can be ignored for finding the optimum. Let $f(b) = \\frac{b}{b+n}$.\n$$\\frac{df}{db} = \\frac{(b+n)(1) - b(1)}{(b+n)^2} = \\frac{n}{(b+n)^2}$$\nSince the matrix dimension $n$ is positive, $\\frac{df}{db}  0$ for all $b \\ge 0$. This demonstrates that the arithmetic intensity $I_{L1}(b)$ is a monotonically increasing function of the block size $b$.\n\n**3. Constrained Optimization**\n\nTo maximize the monotonically increasing objective function $I_{L1}(b)$, we must choose the largest possible value of $b$ that satisfies the capacity constraint derived in the first section. The optimal block size, $b^{\\star}$, will thus be the value of $b$ for which the constraint becomes an equality:\n$$s(b^2 + 2br) = \\phi M_1$$\nRearranging this equation gives a quadratic equation in $b$:\n$$b^2 + 2rb - \\frac{\\phi M_1}{s} = 0$$\nWe solve for $b$ using the quadratic formula, $b = \\frac{-B \\pm \\sqrt{B^2 - 4AC}}{2A}$, with coefficients $A=1$, $B=2r$, and $C = -\\frac{\\phi M_1}{s}$:\n$$b = \\frac{-2r \\pm \\sqrt{(2r)^2 - 4(1)\\left(-\\frac{\\phi M_1}{s}\\right)}}{2(1)}$$\n$$b = \\frac{-2r \\pm \\sqrt{4r^2 + \\frac{4\\phi M_1}{s}}}{2}$$\n$$b = -r \\pm \\sqrt{r^2 + \\frac{\\phi M_1}{s}}$$\nSince $b$ represents a physical block dimension, it must be a positive quantity. The term $\\sqrt{r^2 + \\frac{\\phi M_1}{s}}$ is strictly greater than $r$ (since $\\phi, M_1, s$ are positive). Therefore, we must choose the positive root to ensure $b  0$.\nThe optimal block size $b^{\\star}$ is:\n$$b^{\\star} = \\sqrt{r^2 + \\frac{\\phi M_1}{s}} - r$$\nThis expression represents the largest possible block size $b$ that allows the working set to fit within the available L$1$ cache, thereby maximizing L$1$ reuse for the given algorithmic structure.",
            "answer": "$$\\boxed{\\sqrt{r^{2} + \\frac{\\phi M_{1}}{s}} - r}$$"
        }
    ]
}