{
    "hands_on_practices": [
        {
            "introduction": "The effectiveness of the L-curve method relies on finding a distinct corner, but what happens when one doesn't exist? This exercise  invites you to analytically explore a limiting case where the operator's singular values are perfectly flat. By deriving the curvature expression from first principles, you will gain a fundamental insight into how the problem's spectral structure dictates the L-curve's shape and the potential ambiguity of the resulting parameter choice.",
            "id": "3554649",
            "problem": "Consider the linear inverse problem in $\\mathbb{R}^{n}$ defined by the Tikhonov-regularized least-squares objective\n$$\nJ_{\\lambda}(x) \\;=\\; \\|A x - b\\|_{2}^{2} \\;+\\; \\lambda^{2} \\|x\\|_{2}^{2},\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^{n}$ is nonzero, and $\\lambda > 0$ is the regularization parameter. The L-curve is the parametric log-log plot of the residual norm against the solution norm,\n$$\n\\big(\\ln \\rho(\\lambda), \\, \\ln \\eta(\\lambda)\\big), \\quad \\text{with} \\quad \\rho(\\lambda) := \\|A x_{\\lambda} - b\\|_{2}, \\quad \\eta(\\lambda) := \\|x_{\\lambda}\\|_{2},\n$$\nwhere $x_{\\lambda}$ minimizes $J_{\\lambda}(x)$. Construct a matrix $A$ whose singular values are nearly flat by taking the analytically tractable limiting case $A = \\sigma I_{n}$, with $\\sigma > 0$, and analyze the resulting L-curve.\n\nStarting only from fundamental definitions—namely, the normal equations for the minimizer $x_{\\lambda}$ and the definition of curvature for a plane curve—derive:\n1. The expressions for $\\rho(\\lambda)$ and $\\eta(\\lambda)$ in terms of $\\sigma$, $\\lambda$, and $\\|b\\|_{2}$.\n2. The parametric plane curve $c(\\lambda) = \\big(\\ln \\rho(\\lambda), \\ln \\eta(\\lambda)\\big)$ and its curvature $\\kappa(\\lambda)$ (as a function of $\\lambda$) computed via the standard curvature definition for a parametrically represented plane curve.\n3. The value $\\lambda_{\\star} > 0$ that maximizes $\\kappa(\\lambda)$, together with the maximal curvature $\\kappa_{\\max} := \\max_{\\lambda > 0} \\kappa(\\lambda)$.\n\nExplain, on the basis of your analytical expression for $\\kappa(\\lambda)$, why the L-curve lacks a sharp corner in this flat-singular-value setting and thus leads to ambiguous selection of $\\lambda$.\n\nProvide your final result as the pair $\\big(\\lambda_{\\star}, \\kappa_{\\max}\\big)$ in exact analytic form. No rounding is required.",
            "solution": "We begin from first principles for Tikhonov regularization. The minimizer $x_{\\lambda}$ of\n$$\nJ_{\\lambda}(x) \\;=\\; \\|A x - b\\|_{2}^{2} \\;+\\; \\lambda^{2} \\|x\\|_{2}^{2}\n$$\nsatisfies the normal equations\n$$\nA^{\\top} (A x_{\\lambda} - b) \\;+\\; \\lambda^{2} x_{\\lambda} \\;=\\; 0,\n$$\nwhich can be rearranged to\n$$\n\\big(A^{\\top} A + \\lambda^{2} I\\big) x_{\\lambda} \\;=\\; A^{\\top} b.\n$$\nIn the analytically tractable limiting case of nearly flat singular values, we set $A = \\sigma I_{n}$ with $\\sigma > 0$. Then $A^{\\top} A = \\sigma^{2} I_{n}$ and $A^{\\top} b = \\sigma b$, so the normal equations reduce to\n$$\n\\big(\\sigma^{2} I_{n} + \\lambda^{2} I_{n}\\big) x_{\\lambda} \\;=\\; \\sigma b\n\\quad\\Longrightarrow\\quad\nx_{\\lambda} \\;=\\; \\frac{\\sigma}{\\sigma^{2} + \\lambda^{2}} \\, b.\n$$\nThe residual is\n$$\nr_{\\lambda} \\;=\\; A x_{\\lambda} - b\n\\;=\\; \\sigma x_{\\lambda} - b\n\\;=\\; \\left( \\frac{\\sigma^{2}}{\\sigma^{2} + \\lambda^{2}} - 1 \\right) b\n\\;=\\; - \\frac{\\lambda^{2}}{\\sigma^{2} + \\lambda^{2}} \\, b.\n$$\nTherefore, the residual norm and solution norm are\n$$\n\\rho(\\lambda) \\;=\\; \\|r_{\\lambda}\\|_{2}\n\\;=\\; \\frac{\\lambda^{2}}{\\sigma^{2} + \\lambda^{2}} \\, \\|b\\|_{2},\n\\qquad\n\\eta(\\lambda) \\;=\\; \\|x_{\\lambda}\\|_{2}\n\\;=\\; \\frac{\\sigma}{\\sigma^{2} + \\lambda^{2}} \\, \\|b\\|_{2}.\n$$\nDefine the L-curve in log-log coordinates\n$$\ns(\\lambda) \\;=\\; \\ln \\rho(\\lambda)\n\\;=\\; \\ln \\|b\\|_{2} + \\ln \\lambda^{2} - \\ln (\\sigma^{2} + \\lambda^{2}),\n\\qquad\nt(\\lambda) \\;=\\; \\ln \\eta(\\lambda)\n\\;=\\; \\ln \\|b\\|_{2} + \\ln \\sigma - \\ln (\\sigma^{2} + \\lambda^{2}).\n$$\nNext, compute derivatives needed for the curvature. We have\n$$\ns'(\\lambda) \\;=\\; \\frac{2}{\\lambda} - \\frac{2\\lambda}{\\sigma^{2} + \\lambda^{2}}\n\\;=\\; \\frac{2 \\sigma^{2}}{\\lambda (\\sigma^{2} + \\lambda^{2})},\n$$\n$$\nt'(\\lambda) \\;=\\; - \\frac{2\\lambda}{\\sigma^{2} + \\lambda^{2}}.\n$$\nDifferentiating again gives\n$$\ns''(\\lambda) \\;=\\; - \\frac{2}{\\lambda^{2}} - \\frac{2(\\sigma^{2} - \\lambda^{2})}{(\\sigma^{2} + \\lambda^{2})^{2}},\n\\qquad\nt''(\\lambda) \\;=\\; - \\frac{2(\\sigma^{2} - \\lambda^{2})}{(\\sigma^{2} + \\lambda^{2})^{2}}.\n$$\nFor a planar parametric curve $(x(\\lambda), y(\\lambda))$, the curvature as a function of the parameter $\\lambda$ is\n$$\n\\kappa(\\lambda) \\;=\\;\n\\frac{\\left| x'(\\lambda) y''(\\lambda) - y'(\\lambda) x''(\\lambda) \\right|}\n{\\left( x'(\\lambda)^{2} + y'(\\lambda)^{2} \\right)^{3/2}}.\n$$\nApplying this to $x(\\lambda) = s(\\lambda)$ and $y(\\lambda) = t(\\lambda)$, we compute the numerator\n$$\nN(\\lambda) \\;=\\; s'(\\lambda) t''(\\lambda) - t'(\\lambda) s''(\\lambda).\n$$\nUsing the expressions above,\n\\begin{align*}\ns'(\\lambda) t''(\\lambda)\n&=\\; \\frac{2 \\sigma^{2}}{\\lambda (\\sigma^{2} + \\lambda^{2})} \\cdot \\left( - \\frac{2(\\sigma^{2} - \\lambda^{2})}{(\\sigma^{2} + \\lambda^{2})^{2}} \\right)\n\\;=\\; - \\frac{4 \\sigma^{2} (\\sigma^{2} - \\lambda^{2})}{\\lambda (\\sigma^{2} + \\lambda^{2})^{3}}, \\\\\nt'(\\lambda) s''(\\lambda)\n&=\\; \\left( - \\frac{2 \\lambda}{\\sigma^{2} + \\lambda^{2}} \\right) \\left( - \\frac{2}{\\lambda^{2}} - \\frac{2(\\sigma^{2} - \\lambda^{2})}{(\\sigma^{2} + \\lambda^{2})^{2}} \\right) \\\\\n&=\\; \\frac{4}{\\lambda (\\sigma^{2} + \\lambda^{2})} + \\frac{4 \\lambda (\\sigma^{2} - \\lambda^{2})}{(\\sigma^{2} + \\lambda^{2})^{3}}.\n\\end{align*}\nSubtracting yields\n\\begin{align*}\nN(\\lambda)\n&=\\; - \\frac{4 \\sigma^{2} (\\sigma^{2} - \\lambda^{2})}{\\lambda (\\sigma^{2} + \\lambda^{2})^{3}}\n\\;-\\; \\frac{4}{\\lambda (\\sigma^{2} + \\lambda^{2})}\n\\;-\\; \\frac{4 \\lambda (\\sigma^{2} - \\lambda^{2})}{(\\sigma^{2} + \\lambda^{2})^{3}} \\\\\n&=\\; - \\frac{8 \\sigma^{2}}{\\lambda (\\sigma^{2} + \\lambda^{2})^{2}}.\n\\end{align*}\nHence $\\left|N(\\lambda)\\right| = \\dfrac{8 \\sigma^{2}}{\\lambda (\\sigma^{2} + \\lambda^{2})^{2}}$.\n\nThe denominator is\n\\begin{align*}\nD(\\lambda)\n&=\\; \\left( s'(\\lambda)^{2} + t'(\\lambda)^{2} \\right)^{3/2}\n\\;=\\; \\left( \\frac{4 \\sigma^{4}}{\\lambda^{2} (\\sigma^{2} + \\lambda^{2})^{2}} + \\frac{4 \\lambda^{2}}{(\\sigma^{2} + \\lambda^{2})^{2}} \\right)^{3/2} \\\\\n&=\\; \\left( \\frac{4}{(\\sigma^{2} + \\lambda^{2})^{2}} \\left( \\frac{\\sigma^{4}}{\\lambda^{2}} + \\lambda^{2} \\right) \\right)^{3/2}\n\\;=\\; \\frac{8}{(\\sigma^{2} + \\lambda^{2})^{3}} \\left( \\frac{\\sigma^{4}}{\\lambda^{2}} + \\lambda^{2} \\right)^{3/2}.\n\\end{align*}\nTherefore, the curvature is\n\\begin{align*}\n\\kappa(\\lambda)\n&=\\; \\frac{\\left|N(\\lambda)\\right|}{D(\\lambda)}\n\\;=\\; \\frac{\\dfrac{8 \\sigma^{2}}{\\lambda (\\sigma^{2} + \\lambda^{2})^{2}}}{\\dfrac{8}{(\\sigma^{2} + \\lambda^{2})^{3}} \\left( \\dfrac{\\sigma^{4}}{\\lambda^{2}} + \\lambda^{2} \\right)^{3/2}} \\\\\n&=\\; \\frac{\\sigma^{2} (\\sigma^{2} + \\lambda^{2})}{\\lambda \\left( \\dfrac{\\sigma^{4}}{\\lambda^{2}} + \\lambda^{2} \\right)^{3/2}}\n\\;=\\; \\frac{\\sigma^{2} (\\sigma^{2} + \\lambda^{2}) \\lambda^{2}}{(\\sigma^{4} + \\lambda^{4})^{3/2}}.\n\\end{align*}\nIntroduce the dimensionless variable $y := \\lambda^{2} / \\sigma^{2}$ (with $y > 0$). Then\n$$\n\\kappa(\\lambda) \\;=\\; \\frac{y(1 + y)}{(1 + y^{2})^{3/2}}.\n$$\nTo maximize $\\kappa(\\lambda)$ over $\\lambda > 0$, we maximize $g(y) := \\dfrac{y(1 + y)}{(1 + y^{2})^{3/2}}$ over $y > 0$. Differentiate,\n\\begin{align*}\ng'(y)\n&=\\; (1 + 2 y) (1 + y^{2})^{-3/2} - 3 y (y + y^{2}) (1 + y^{2})^{-5/2} \\\\\n&=\\; (1 + y^{2})^{-5/2} \\left[ (1 + 2 y)(1 + y^{2}) - 3 y (y + y^{2}) \\right] \\\\\n&=\\; (1 + y^{2})^{-5/2} \\left( 1 + 2 y - 2 y^{2} - y^{3} \\right).\n\\end{align*}\nSetting $g'(y) = 0$ yields the cubic equation\n$$\n1 + 2 y - 2 y^{2} - y^{3} \\;=\\; 0,\n$$\nequivalently\n$$\ny^{3} + 2 y^{2} - 2 y - 1 \\;=\\; 0.\n$$\nOne root is $y = 1$, and the remaining roots of $y^{2} + 3 y + 1 = 0$ are negative. Since $y > 0$, the unique critical point is $y = 1$. As $y \\to 0^{+}$, $g(y) \\sim y \\to 0$, and as $y \\to \\infty$, $g(y) \\sim y^{2}/y^{3} = 1/y \\to 0$, so $y = 1$ is the global maximizer. Therefore,\n$$\n\\lambda_{\\star}^{2} / \\sigma^{2} \\;=\\; 1\n\\quad\\Longrightarrow\\quad\n\\lambda_{\\star} \\;=\\; \\sigma,\n\\qquad\n\\kappa_{\\max} \\;=\\; g(1) \\;=\\; \\frac{2}{(1 + 1)^{3/2}} \\;=\\; \\frac{1}{\\sqrt{2}}.\n$$\nThe curvature $\\kappa(\\lambda)$ is bounded by $\\dfrac{1}{\\sqrt{2}}$ and attains its maximum at $\\lambda = \\sigma$, but this maximum is modest and broad rather than sharply peaked. Consequently, the L-curve does not exhibit a pronounced corner in this flat-singular-value setting, leading to ambiguous selection of $\\lambda$ by the L-curve criterion: there is no sharply distinguished parameter value that stands out geometrically.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\sigma & \\frac{1}{\\sqrt{2}}\\end{pmatrix}}$$"
        },
        {
            "introduction": "While a flat spectrum can blur the L-curve's corner, a structured spectrum can create a different challenge: multiple corners. This practice  examines the conditions, rooted in the operator's spectral gaps and the data's alignment, that lead to several plausible regularization parameters. By analyzing these scenarios, you will develop a deeper intuition for interpreting complex L-curves and learn to apply principled methods to select the most meaningful solution.",
            "id": "3554620",
            "problem": "Consider a linear inverse problem in numerical linear algebra with a compact operator $A \\in \\mathbb{R}^{m \\times n}$ and observed data $b \\in \\mathbb{R}^{m}$. One seeks a stable solution $x \\in \\mathbb{R}^{n}$ via Tikhonov regularization with identity prior, defined as the minimizer of $\\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2$ for a regularization parameter $\\lambda > 0$. The L-curve is the parametric curve in the plane tracing $(\\log \\lVert A x_\\lambda - b \\rVert_2, \\log \\lVert x_\\lambda \\rVert_2)$ as $\\lambda$ varies. It is well-known that, for many ill-posed problems, the L-curve exhibits a single prominent corner where the trade-off between solution norm and residual norm transitions from under- to over-regularization. However, in some problems, multiple apparent corners may be observed, complicating parameter selection.\n\nWhich of the following statements correctly identify spectral/data conditions under which multiple apparent corners can arise on the L-curve, and propose a principled criterion to disambiguate among them?\n\nA. If the singular values $\\{\\sigma_i\\}$ of $A$ cluster into two well-separated bands and the data coefficients $\\{|u_i^\\top b|\\}$ have significant energy in both bands (where $A = U \\Sigma V^\\top$ is the singular value decomposition), then each cluster can induce its own transition in the filter factors as $\\lambda$ sweeps, producing two prominent bends on the L-curve. A principled disambiguation is to impose Morozov’s discrepancy principle, selecting the corner for which $\\lVert A x_\\lambda - b \\rVert_2$ matches the known noise level and the corresponding filter factors leave the large-$\\sigma$ band largely unattenuated while suppressing the small-$\\sigma$ band.\n\nB. Multiple corners are an artifact of plotting $\\log$ in base $10$ rather than natural logarithm; if one always uses the natural logarithm, the L-curve has at most one corner. The disambiguation is to replot with natural logarithm and take the unique corner.\n\nC. When $\\{\\sigma_i\\}$ follow a smooth power-law decay and $b$ is white noise, the L-curve generically has multiple corners. A robust disambiguation is to choose the corner with the smallest curvature to avoid overfitting.\n\nD. If $A$ is rank-deficient with repeated singular values and $b$ aligns with one repeated singular subspace, the L-curve must exhibit multiple corners. A principled choice is to set $\\lambda$ equal to the geometric mean of two adjacent singular values in the repeated block.\n\nE. If the right-hand side exhibits a multimodal Picard plot, meaning $\\{|u_i^\\top b|\\}$ has two dominant groups aligned with two separated spectral bands in $\\{\\sigma_i\\}$, and $A$ is severely ill-posed so that $\\{\\sigma_i\\}$ decay rapidly with a gap between the bands, then as $\\lambda$ passes through each band, the effective number of parameters changes abruptly, yielding multiple apparent corners. A principled disambiguation is to select the corner that coincides with the estimated Picard break index $k_*$, for which the noise begins to dominate, equivalently choosing $\\lambda$ such that the effective degrees of freedom $\\mathrm{df}(\\lambda) = \\sum_{i} \\sigma_i^2 / (\\sigma_i^2 + \\lambda^2)$ is approximately $k_*$.\n\nSelect all correct options.",
            "solution": "The problem asks for conditions under which the L-curve for Tikhonov regularization exhibits multiple corners and for principled methods to disambiguate them. The L-curve method is used to select a regularization parameter $\\lambda$ for the problem of minimizing $\\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2$. The solution to this minimization problem, denoted $x_\\lambda$, can be expressed using the Singular Value Decomposition (SVD) of the matrix $A$.\n\nLet the SVD of $A$ be $A = U \\Sigma V^\\top$, where $U = [u_1, u_2, \\dots, u_m]$ and $V = [v_1, v_2, \\dots, v_n]$ are orthogonal matrices, and $\\Sigma$ is a diagonal matrix with non-negative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$, with $r = \\text{rank}(A)$. The Tikhonov-regularized solution is given by:\n$$ x_\\lambda = \\sum_{i=1}^{r} \\phi_i(\\lambda) \\frac{u_i^\\top b}{\\sigma_i} v_i $$\nwhere $\\phi_i(\\lambda) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2}$ are the filter factors. These factors determine the degree to which each singular component is included in the solution. For a given component $i$, if $\\lambda \\ll \\sigma_i$, then $\\phi_i(\\lambda) \\approx 1$, and the component is preserved. If $\\lambda \\gg \\sigma_i$, then $\\phi_i(\\lambda) \\approx 0$, and the component is filtered out. The transition occurs around $\\lambda \\approx \\sigma_i$.\n\nThe L-curve is a log-log plot of the solution norm $\\lVert x_\\lambda \\rVert_2$ versus the residual norm $\\lVert A x_\\lambda - b \\rVert_2$. These norms can also be expressed using the SVD:\n$$ \\lVert x_\\lambda \\rVert_2^2 = \\sum_{i=1}^{r} \\left( \\frac{\\sigma_i (u_i^\\top b)}{\\sigma_i^2 + \\lambda^2} \\right)^2 = \\sum_{i=1}^{r} \\left( \\phi_i(\\lambda) \\frac{u_i^\\top b}{\\sigma_i} \\right)^2 $$\n$$ \\lVert A x_\\lambda - b \\rVert_2^2 = \\sum_{i=1}^{r} \\left( (\\phi_i(\\lambda)-1) (u_i^\\top b) \\right)^2 + \\sum_{i=r+1}^{m} (u_i^\\top b)^2 = \\sum_{i=1}^{r} \\left( \\frac{\\lambda^2 (u_i^\\top b)}{\\sigma_i^2 + \\lambda^2} \\right)^2 + \\sum_{i=r+1}^{m} (u_i^\\top b)^2 $$\n\nA \"corner\" on the L-curve corresponds to a value of $\\lambda$ where the character of the solution changes significantly, representing a transition in the trade-off. This occurs when $\\lambda$ sweeps across a singular value $\\sigma_i$. If the singular values $\\{\\sigma_i\\}$ are clustered into distinct, well-separated groups, and the data vector $b$ has significant energy projected onto the singular vectors $\\{u_i\\}$ corresponding to more than one of these groups, then the L-curve can exhibit multiple corners. As $\\lambda$ decreases, it sequentially crosses these clusters of singular values, causing abrupt changes in the solution and residual norms at each transition, each of which can manifest as a bend or corner on the log-log plot.\n\nLet's evaluate each option based on this principle.\n\n**A. If the singular values $\\{\\sigma_i\\}$ of $A$ cluster into two well-separated bands and the data coefficients $\\{|u_i^\\top b|\\}$ have significant energy in both bands (where $A = U \\Sigma V^\\top$ is the singular value decomposition), then each cluster can induce its own transition in the filter factors as $\\lambda$ sweeps, producing two prominent bends on the L-curve. A principled disambiguation is to impose Morozov’s discrepancy principle, selecting the corner for which $\\lVert A x_\\lambda - b \\rVert_2$ matches the known noise level and the corresponding filter factors leave the large-$\\sigma$ band largely unattenuated while suppressing the small-$\\sigma$ band.**\n\nThis statement accurately describes the primary mechanism for the formation of multiple corners. The condition of having separated clusters of singular values, combined with data that excites components in each cluster, is the classic scenario for this phenomenon. The proposed disambiguation method, Morozov's discrepancy principle, is a standard and principled approach in regularization theory. It requires an estimate of the noise level $\\delta$ in the data $b$ and selects $\\lambda$ such that the residual norm $\\lVert A x_\\lambda - b \\rVert_2$ is approximately equal to $\\delta$. This provides a clear, objective criterion for choosing among the possible $\\lambda$ values associated with the corners. The description of the filter factors' behavior for a good choice of $\\lambda$ is also correct: it should separate the \"signal\" (associated with large $\\sigma_i$) from the \"noise\" (associated with small $\\sigma_i$).\n\nVerdict: **Correct**.\n\n**B. Multiple corners are an artifact of plotting $\\log$ in base $10$ rather than natural logarithm; if one always uses the natural logarithm, the L-curve has at most one corner. The disambiguation is to replot with natural logarithm and take the unique corner.**\n\nThis statement is mathematically incorrect. The relationship between logarithms of different bases is a simple scaling factor: $\\log_{10}(z) = \\frac{\\ln(z)}{\\ln(10)}$. Plotting the L-curve using base-$10$ logarithms versus natural logarithms amounts to a uniform scaling of the coordinate axes. A uniform scaling of a parametric curve $(x(t), y(t))$ to $(cx(t), cy(t))$ scales its curvature by a factor of $1/c$ but does not change the parameter values $t$ where the curvature is maximized or minimized. Therefore, the number and location of the corners are independent of the choice of logarithm base.\n\nVerdict: **Incorrect**.\n\n**C. When $\\{\\sigma_i\\}$ follow a smooth power-law decay and $b$ is white noise, the L-curve generically has multiple corners. A robust disambiguation is to choose the corner with the smallest curvature to avoid overfitting.**\n\nThis statement is incorrect on two counts. First, a smooth decay of singular values, even for a \"difficult\" right-hand side like white noise, typically leads to a classic L-curve with a single, well-defined corner. Multiple corners are associated with *gaps* or *discontinuities* in the spectrum of singular values, not a smooth decay. Second, the L-curve criterion is precisely to select the point of *maximum* curvature, as this point represents the best compromise in the trade-off. Choosing the point of minimum curvature would correspond to the flat, non-informative parts of the curve, representing extreme under- or over-regularization. This is the antithesis of the method's purpose.\n\nVerdict: **Incorrect**.\n\n**D. If $A$ is rank-deficient with repeated singular values and $b$ aligns with one repeated singular subspace, the L-curve must exhibit multiple corners. A principled choice is to set $\\lambda$ equal to the geometric mean of two adjacent singular values in the repeated block.**\n\nThe premise of this statement is flawed. If a singular value $\\sigma_k$ is repeated (e.g., $\\sigma_k = \\sigma_{k+1}$), the corresponding filter factors are identical: $\\phi_k(\\lambda) = \\phi_{k+1}(\\lambda)$. These components are attenuated in exactly the same way as $\\lambda$ varies. The presence of a repeated singular value does not introduce a new transition scale for $\\lambda$ to cross; it is the *separation* between distinct singular values or clusters of them that causes multiple bends. Therefore, repeated singular values do not, in themselves, cause multiple corners. The proposed disambiguation is also moot since the geometric mean of two identical values is just the value itself.\n\nVerdict: **Incorrect**.\n\n**E. If the right-hand side exhibits a multimodal Picard plot, meaning $\\{|u_i^\\top b|\\}$ has two dominant groups aligned with two separated spectral bands in $\\{\\sigma_i\\}$, and $A$ is severely ill-posed so that $\\{\\sigma_i\\}$ decay rapidly with a gap between the bands, then as $\\lambda$ passes through each band, the effective number of parameters changes abruptly, yielding multiple apparent corners. A principled disambiguation is to select the corner that coincides with the estimated Picard break index $k_*$, for which the noise begins to dominate, equivalently choosing $\\lambda$ such that the effective degrees of freedom $\\mathrm{df}(\\lambda) = \\sum_{i} \\sigma_i^2 / (\\sigma_i^2 + \\lambda^2)$ is approximately $k_*$.**\n\nThis statement provides a detailed and accurate description of the phenomenon, consistent with the analysis for option A. The term \"multimodal Picard plot\" is an excellent way to characterize the data condition where the coefficients $|u_i^\\top b|$ have distinct bumps corresponding to separated spectral bands of $A$. The explanation that the effective number of parameters, or degrees of freedom $\\mathrm{df}(\\lambda) = \\sum_i \\phi_i(\\lambda)$, changes abruptly at each band is a precise and insightful physical interpretation. The proposed disambiguation method is also principled. It involves estimating the dimension of the \"signal\" subspace, denoted $k_*$, which is the number of coefficients that satisfy the discrete Picard condition before noise dominates. Then, $\\lambda$ is chosen to make the effective degrees of freedom of the solution match this dimension, i.e., $\\mathrm{df}(\\lambda) \\approx k_*$. This is a well-established statistical criterion, related to methods like GCV, for choosing the regularization parameter, especially when the noise level is unknown.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{AE}$$"
        },
        {
            "introduction": "So far, we have explored ambiguities arising from the problem's inherent structure. This final practice  addresses a more insidious issue: how noise in the data can create a sharp, yet entirely misleading, L-curve corner. You will investigate how adversarial noise aligned with dominant singular vectors can deceive the method and, most importantly, develop a diagnostic tool using principal angle analysis to flag these deceptive cases.",
            "id": "3554612",
            "problem": "Consider a linear inverse problem with Tikhonov regularization. Let $A \\in \\mathbb{R}^{m \\times n}$ with singular value decomposition $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ has diagonal entries $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_{\\min(m,n)} > 0$. Given data $b \\in \\mathbb{R}^{m}$, the Tikhonov-regularized solution for parameter $\\lambda > 0$ is defined by the minimizer of $\\|A x - b\\|_2^2 + \\lambda^2 \\|x\\|_2^2$, denoted $x_{\\lambda} \\in \\mathbb{R}^{n}$. The L-curve is the parametric plot of $(\\log \\|A x_{\\lambda} - b\\|_2, \\log \\|x_{\\lambda}\\|_2)$ as $\\lambda$ varies, and a commonly used parameter choice is the point of maximum curvature on this curve. You are to investigate adversarial noise that can create a misleading L-curve corner and design a diagnostic based on principal angle analysis between $x_{\\lambda}$ and dominant right-singular subspaces.\n\nStarting from core definitions in numerical linear algebra, complete the following tasks.\n\n1. Use the singular value decomposition as the base to express the Tikhonov solution in filter factor form. Let $b = \\sum_{i} \\beta_i u_i$ with $\\beta_i = u_i^{\\top} b$, and define the filter factors $\\phi_i(\\lambda) = \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda^2}$. Derive the representation\n$$\nx_{\\lambda} = \\sum_{i=1}^{\\min(m,n)} \\phi_i(\\lambda) \\beta_i v_i,\n$$\nand show that the residual $r_{\\lambda} = A x_{\\lambda} - b$ has coefficients\n$$\n\\gamma_i(\\lambda) = \\beta_i \\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2}\n$$\nin the left-singular vector basis, i.e., $r_{\\lambda} = \\sum_i \\gamma_i(\\lambda) u_i$.\n\n2. Define the L-curve parameterization $X(\\lambda) = \\log \\|r_{\\lambda}\\|_2$ and $Y(\\lambda) = \\log \\|x_{\\lambda}\\|_2$. Treating the curve $\\lambda \\mapsto (X(\\lambda), Y(\\lambda))$ as a planar curve, derive the curvature of the parametric curve with respect to a smooth parameter $t$, using the standard planar curvature formula. If you reparameterize by $t = \\log \\lambda$, then for $x(t) = X(\\mathrm{e}^{t})$ and $y(t) = Y(\\mathrm{e}^{t})$, show that a numerically stable curvature can be computed as\n$$\n\\kappa(t) = \\frac{|x'(t) y''(t) - y'(t) x''(t)|}{\\left(x'(t)^2 + y'(t)^2\\right)^{3/2}},\n$$\nusing numerical differentiation. Explain why reparameterization by $t = \\log \\lambda$ improves numerical stability in ill-posed problems.\n\n3. Consider adversarial noise aligned with the leading left-singular vectors. Suppose the data are $b = A x_{\\mathrm{true}} + e$ with $e = \\eta \\|A x_{\\mathrm{true}}\\|_2 u_1$ for some $\\eta > 0$, i.e., aligned with the first left-singular vector $u_1$. Using the expressions for $\\beta_i$ and $\\gamma_i(\\lambda)$, qualitatively argue why such alignment can produce an L-curve with a corner at a parameter $\\lambda$ that emphasizes the dominant singular subspace and can deviate from an optimal trade-off for reconstructing $x_{\\mathrm{true}}$.\n\n4. Propose a diagnostic based on principal angle analysis between $x_{\\lambda_{\\ast}}$ at the L-curve corner $\\lambda_{\\ast}$ and the dominant right-singular subspace $\\mathcal{V}_k = \\mathrm{span}\\{v_1,\\dots,v_k\\}$. The smallest principal angle $\\theta$ between a vector $x$ and the subspace $\\mathcal{V}_k$ is characterized by\n$$\n\\cos \\theta = \\frac{\\|P_{\\mathcal{V}_k} x\\|_2}{\\|x\\|_2},\n$$\nwhere $P_{\\mathcal{V}_k}$ is the orthogonal projector onto $\\mathcal{V}_k$. Additionally, define a data-dominance metric\n$$\nf_{\\mathrm{top}} = \\frac{\\sum_{i=1}^{k} \\beta_i^2}{\\sum_{i=1}^{\\min(m,n)} \\beta_i^2},\n$$\nmeasuring the fraction of data energy in the top $k$ left-singular directions. Propose a rule that flags a potentially misleading L-curve corner when both $\\cos \\theta$ is close to $1$ and $f_{\\mathrm{top}}$ is large, and specify quantitative thresholds suitable for numerical implementation. All angles, if computed, must be in radians.\n\n5. Implement a program that constructs synthetic problems, computes the L-curve corner by numerical curvature maximization, evaluates the proposed diagnostic, and outputs a boolean for each test case indicating whether the L-curve corner is flagged as potentially misleading by the diagnostic. Use the following test suite, which has been chosen to cover typical and adversarial conditions:\n\n- Matrix construction: set $m = n = 60$. Let the singular values be logarithmically decaying from $10^0$ to $10^{-6}$, i.e., $\\sigma_i = 10^{-\\alpha_i}$ with $\\alpha_i$ linearly spaced from $0$ to $6$ for $i = 1,\\dots,60$. Construct $U$ and $V$ as independent random orthogonal matrices obtained from the $\\mathrm{QR}$ factorization of Gaussian matrices with a fixed random seed. Set $A = U \\Sigma V^{\\top}$.\n- Truth construction: set $c_i = 0$ for $i \\le \\lfloor 0.4 n \\rfloor$, and $c_i = 1/(i - \\lfloor 0.4 n \\rfloor)$ for $i > \\lfloor 0.4 n \\rfloor$. Normalize $c$ to unit $\\ell_2$-norm and set $x_{\\mathrm{true}} = V c$. Define $b_{\\mathrm{clean}} = A x_{\\mathrm{true}}$.\n- L-curve computation: evaluate the L-curve on a grid of $\\lambda$ values logarithmically spaced over $[10^{-10}, 10^{2}]$. Compute curvature using the parameter $t = \\log \\lambda$ and finite differences. Choose the corner $\\lambda_{\\ast}$ as the $\\lambda$ that maximizes the curvature.\n- Diagnostic specifics: choose $k = 3$ for the dominant subspaces $\\mathcal{V}_k$ and $\\mathcal{U}_k$, use the thresholds $\\cos \\theta \\ge 0.995$ and $f_{\\mathrm{top}} \\ge 0.2$ to flag a suspect L-curve corner.\n- Noise models and levels (with fixed random seed for reproducibility):\n    1. Benign white noise: $e = \\delta \\|b_{\\mathrm{clean}}\\|_2 z$ where $z \\sim \\mathcal{N}(0, I_m)$ normalized to unit norm, with $\\delta = 0.05$.\n    2. Adversarial leading-direction noise: $e = \\delta \\|b_{\\mathrm{clean}}\\|_2 u_1$ with $\\delta = 0.5$.\n    3. Adversarial trailing-direction noise: $e = \\delta \\|b_{\\mathrm{clean}}\\|_2 u_{n}$ with $\\delta = 0.5$.\n    4. Near noise-free case: white noise with $\\delta = 10^{-8}$ as in case 1.\n\nFor each case, let $b = b_{\\mathrm{clean}} + e$, compute the L-curve corner $\\lambda_{\\ast}$, then evaluate $\\cos \\theta$ and $f_{\\mathrm{top}}$ at $x_{\\lambda_{\\ast}}$ and the corresponding $\\beta_i = u_i^{\\top} b$. Output a boolean indicating whether the diagnostic flags the case as suspect.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,False,True]\") corresponding respectively to the four cases above. Angles, if any are internally computed, must be in radians; the final outputs are booleans with no units. No user input is permitted.\n\nThe final output format must be exactly one line: a Python-style list of booleans with no spaces after commas.",
            "solution": "The provided problem is a well-posed and scientifically grounded exercise in numerical linear algebra, specifically concerning Tikhonov regularization and the L-curve method. It contains a minor inconsistency in its definition of the residual vector's coefficients, which will be noted but does not invalidate the problem's core logic. We will proceed with a full solution.\n\n### Part 1: Tikhonov Solution in Filter Factor Form\n\nThe Tikhonov regularized solution $x_{\\lambda}$ is the vector that minimizes the functional $J(x) = \\|A x - b\\|_2^2 + \\lambda^2 \\|x\\|_2^2$. The minimizer is found by setting the gradient of $J(x)$ with respect to $x$ to zero. This yields the normal equations:\n$$\n(A^{\\top} A + \\lambda^2 I) x = A^{\\top} b\n$$\nWe use the singular value decomposition $A = U \\Sigma V^{\\top}$, which implies $A^{\\top} A = V \\Sigma^{\\top} \\Sigma V^{\\top}$ and $A^{\\top} b = V \\Sigma^{\\top} U^{\\top} b$. The normal equations become:\n$$\n(V \\Sigma^{\\top} \\Sigma V^{\\top} + \\lambda^2 V I V^{\\top}) x_{\\lambda} = V \\Sigma^{\\top} U^{\\top} b\n$$\nLeveraging the orthogonality of $V$ (i.e., $V^{\\top}V = I$), we can multiply from the left by $V^{\\top}$:\n$$\n(\\Sigma^{\\top} \\Sigma + \\lambda^2 I) V^{\\top} x_{\\lambda} = \\Sigma^{\\top} U^{\\top} b\n$$\nLet us define the transformed solution vector $y = V^{\\top} x_{\\lambda}$ and the transformed data vector $d = U^{\\top} b$. The components of $d$ are $d_i = u_i^{\\top} b$, which are given as $\\beta_i$. The matrix $\\Sigma^{\\top} \\Sigma + \\lambda^2 I$ is a diagonal matrix of size $n \\times n$ with diagonal entries $\\sigma_i^2 + \\lambda^2$ for $i=1, \\dots, \\min(m,n)$. The equation for the $i$-th component of $y$ is:\n$$\n(\\sigma_i^2 + \\lambda^2) y_i = \\sigma_i \\beta_i\n$$\nSolving for $y_i$ gives $y_i = \\frac{\\sigma_i \\beta_i}{\\sigma_i^2 + \\lambda^2}$. To recover $x_{\\lambda}$, we transform back via $x_{\\lambda} = V y = \\sum_{i=1}^{\\min(m,n)} y_i v_i$. Substituting the expression for $y_i$:\n$$\nx_{\\lambda} = \\sum_{i=1}^{\\min(m,n)} \\frac{\\sigma_i \\beta_i}{\\sigma_i^2 + \\lambda^2} v_i\n$$\nThis can be expressed using the given filter factors $\\phi_i(\\lambda) = \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda^2}$, which yields the desired form:\n$$\nx_{\\lambda} = \\sum_{i=1}^{\\min(m,n)} \\phi_i(\\lambda) \\beta_i v_i\n$$\nNext, we derive the coefficients for the residual $r_{\\lambda} = A x_{\\lambda} - b$. We expand $A x_{\\lambda}$ in the basis of left-singular vectors $\\{u_i\\}$:\n$$\nA x_{\\lambda} = (U \\Sigma V^{\\top}) \\left( \\sum_{j=1}^{\\min(m,n)} y_j v_j \\right) = U \\Sigma \\left( \\sum_{j=1}^{\\min(m,n)} y_j (V^{\\top} v_j) \\right) = U \\Sigma y = \\sum_{i=1}^{\\min(m,n)} \\sigma_i y_i u_i\n$$\nSubstituting $y_i$:\n$$\nA x_{\\lambda} = \\sum_{i=1}^{\\min(m,n)} \\frac{\\sigma_i^2 \\beta_i}{\\sigma_i^2 + \\lambda^2} u_i\n$$\nThe data vector is $b = \\sum_{i=1}^{m} \\beta_i u_i$. The residual is therefore:\n$$\nr_{\\lambda} = A x_{\\lambda} - b = \\sum_{i=1}^{\\min(m,n)} \\frac{\\sigma_i^2 \\beta_i}{\\sigma_i^2 + \\lambda^2} u_i - \\sum_{i=1}^{m} \\beta_i u_i\n$$\nCombining terms under a common summation (assuming $\\sigma_i=0$ for $i > \\min(m,n)$):\n$$\nr_{\\lambda} = \\sum_{i=1}^{m} \\left( \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} - 1 \\right) \\beta_i u_i = \\sum_{i=1}^{m} \\left( \\frac{\\sigma_i^2 - (\\sigma_i^2 + \\lambda^2)}{\\sigma_i^2 + \\lambda^2} \\right) \\beta_i u_i = \\sum_{i=1}^{m} \\frac{-\\lambda^2}{\\sigma_i^2 + \\lambda^2} \\beta_i u_i\n$$\nThere is a formal inconsistency in the problem statement. It defines $r_{\\lambda} = A x_{\\lambda} - b$ but gives the coefficients as $\\gamma_i(\\lambda) = \\beta_i \\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2}$, which are positive. Our derivation shows the coefficients are $-\\gamma_i(\\lambda)$. The expression for $\\gamma_i(\\lambda)$ corresponds to the alternative residual definition $r=b - A x_{\\lambda}$. This likely sign error does not affect the norm $\\|r_{\\lambda}\\|_2$, and thus does not alter the L-curve's geometry. We proceed by using the magnitude of the derived coefficients.\n\n### Part 2: L-curve Curvature\n\nThe L-curve is the parametric plot of $(\\log \\|r_{\\lambda}\\|_2, \\log \\|x_{\\lambda}\\|_2)$. We define $\\rho(\\lambda) = \\|r_{\\lambda}\\|_2$ and $\\eta(\\lambda) = \\|x_{\\lambda}\\|_2$. The L-curve in log-log scale is parameterized by $\\lambda \\mapsto (X(\\lambda), Y(\\lambda))$, where $X(\\lambda) = \\log \\rho(\\lambda)$ and $Y(\\lambda) = \\log \\eta(\\lambda)$. For a general planar curve parameterized by $t$, $c(t) = (x(t), y(t))$, the curvature is given by the formula stated in the problem:\n$$\n\\kappa(t) = \\frac{|x'(t) y''(t) - y'(t) x''(t)|}{\\left(x'(t)^2 + y'(t)^2\\right)^{3/2}}\n$$\nThe reparameterization $t = \\log \\lambda$ (so $\\lambda = e^t$) is numerically advantageous for ill-posed problems. The singular values $\\sigma_i$ often span many orders of magnitude, causing the solution norm $\\eta(\\lambda)$ and residual norm $\\rho(\\lambda)$ to change rapidly for small $\\lambda$ and slowly for large $\\lambda$. A uniform grid in $\\lambda$ would poorly sample the region of rapid change. A logarithmic grid in $\\lambda$, which corresponds to a uniform grid in the parameter $t=\\log\\lambda$, distributes sample points more effectively across all scales. Numerical differentiation methods, such as the finite differences used in the implementation, provide more accurate derivative approximations when evaluated on a uniform grid. Thus, computing the derivatives $x'(t)$, $y'(t)$, $x''(t)$, and $y''(t)$ with respect to $t$ on this uniform grid produces a more stable and reliable curvature calculation.\n\n### Part 3: Adversarial Noise Analysis\n\nWhen the data is contaminated with noise aligned with a dominant left-singular vector, $b = A x_{\\mathrm{true}} + e$ with $e = \\eta \\|A x_{\\mathrm{true}}\\|_2 u_1$, the data coefficients $\\beta_i = u_i^{\\top} b$ are affected as follows:\n$$\n\\beta_i = u_i^{\\top}(A x_{\\mathrm{true}} + e) = u_i^{\\top} A x_{\\mathrm{true}} + \\eta \\|A x_{\\mathrm{true}}\\|_2 (u_i^{\\top} u_1) = (u_i^{\\top} A x_{\\mathrm{true}}) + (\\eta \\|A x_{\\mathrm{true}}\\|_2) \\delta_{i1}\n$$\nwhere $\\delta_{i1}$ is the Kronecker delta. The noise exclusively perturbs the first coefficient, $\\beta_1$, making it artificially large. Both the solution norm, $\\|x_{\\lambda}\\|_2^2 = \\sum_i (\\phi_i(\\lambda) \\beta_i)^2$, and the residual norm, $\\|r_{\\lambda}\\|_2^2 = \\sum_i (\\gamma_i(\\lambda))^2$, are now dominated by their first term involving $\\beta_1$.\nThe L-curve corner typically marks a transition point where $\\lambda$ balances data-fitting against solution size. With an oversized $\\beta_1$, this balance is skewed. The curve's dynamics are governed by the first SVD component, and the trade-off corner will be pushed towards $\\lambda_{\\ast} \\approx \\sigma_1$. This choice of $\\lambda_{\\ast}$ heavily filters all components with singular values $\\sigma_i \\ll \\sigma_1$. The resulting solution $x_{\\lambda_{\\ast}}$ will be dominated by the $v_1$ component, essentially projecting the large, noisy data component back into the solution space. This solution fails to accurately represent $x_{\\mathrm{true}}$, especially if its essential features are encoded in singular vectors $v_i$ with $i>1$. The L-curve corner is thus misleading because it identifies a parameter optimal for capturing dominant noise, not for reconstructing the true signal.\n\n### Part 4: Diagnostic Proposal\n\nThe analysis in Part 3 suggests that a misleading corner $\\lambda_{\\ast}$ produces a solution $x_{\\lambda_{\\ast}}$ that is highly aligned with the dominant right-singular subspace. This motivates a diagnostic to detect such an occurrence.\n\n1.  **Solution Alignment**: We measure the alignment of $x_{\\lambda_{\\ast}}$ with the dominant subspace $\\mathcal{V}_k = \\mathrm{span}\\{v_1, \\dots, v_k\\}$ using the cosine of the principal angle, $\\cos \\theta$:\n    $$\n    \\cos \\theta = \\frac{\\|P_{\\mathcal{V}_k} x_{\\lambda_{\\ast}}\\|_2}{\\|x_{\\lambda_{\\ast}}\\|_2}\n    $$\n    where $P_{\\mathcal{V}_k}$ is the orthogonal projector onto $\\mathcal{V}_k$. A value of $\\cos \\theta$ near $1$ indicates strong alignment.\n\n2.  **Data Dominance**: We measure the concentration of the data vector $b$'s energy in the corresponding dominant left-singular subspace $\\mathcal{U}_k = \\mathrm{span}\\{u_1, \\dots, u_k\\}$ using the metric $f_{\\mathrm{top}}$:\n    $$\n    f_{\\mathrm{top}} = \\frac{\\sum_{i=1}^{k} \\beta_i^2}{\\sum_{i=1}^{\\min(m,n)} \\beta_i^2} = \\frac{\\sum_{i=1}^{k} (u_i^{\\top}b)^2}{\\|b\\|_2^2}\n    $$\n    A large $f_{\\mathrm{top}}$ indicates that a few dominant components drive the data.\n\n**Diagnostic Rule**: An L-curve corner $\\lambda_{\\ast}$ is flagged as potentially misleading if both the solution alignment is high and the data energy is concentrated. For a practical implementation with $k=3$, we propose the thresholds:\nFlag as suspect if:\n$$\n\\cos \\theta \\ge 0.995 \\quad \\text{and} \\quad f_{\\mathrm{top}} \\ge 0.2\n$$\nThis rule identifies cases where the regularization parameter likely reflects dominant (and possibly corrupted) data components rather than finding a meaningful solution.",
            "answer": "```python\nimport numpy as np\n\ndef construct_problem(m, n, seed):\n    \"\"\"Constructs the test problem matrix A and true solution x_true.\"\"\"\n    rng = np.random.default_rng(seed=seed)\n    \n    # Singular values\n    s = np.logspace(0, -6, n)\n    \n    # Orthogonal matrices U and V\n    H1 = rng.standard_normal((m, m))\n    U, _ = np.linalg.qr(H1)\n    \n    H2 = rng.standard_normal((n, n))\n    V, _ = np.linalg.qr(H2)\n        \n    # Full A matrix\n    Sigma_mat = np.zeros((m, n))\n    np.fill_diagonal(Sigma_mat, s)\n    A = U @ Sigma_mat @ V.T\n    \n    # True solution x_true\n    n_cutoff = int(np.floor(0.4 * n))\n    c = np.zeros(n)\n    indices = np.arange(n_cutoff, n)\n    c[indices] = 1.0 / (indices - n_cutoff + 1)\n    c /= np.linalg.norm(c)\n    \n    x_true = V @ c\n    \n    return A, U, s, V, x_true\n\ndef get_lcurve_corner(U, s, b, lambda_grid):\n    \"\"\"Computes the L-curve and finds the lambda at maximum curvature.\"\"\"\n    m = U.shape[0]\n    n = len(s)\n    \n    # Data coefficients in U basis\n    beta_full = U.T @ b\n    beta = beta_full[:n]\n\n    log_res_norms = []\n    log_sol_norms = []\n    \n    s2 = s**2\n    beta2 = beta**2\n    \n    for lam in lambda_grid:\n        lam2 = lam**2\n        den = s2 + lam2\n        \n        # Solution norm calculation\n        sol_coeffs_sq = (s2 * beta2) / (den**2)\n        norm_x2 = np.sum(sol_coeffs_sq)\n        \n        # Residual norm calculation\n        res_coeffs_sq = (lam2**2 * beta2) / (den**2)\n        # Add residual from components of b orthogonal to U's first n columns\n        if m > n:\n            norm_r2 = np.sum(res_coeffs_sq) + np.sum(beta_full[n:]**2)\n        else:\n            norm_r2 = np.sum(res_coeffs_sq)\n        \n        # Avoid log(0) for extremely small norms\n        if norm_x2 > 1e-300 and norm_r2 > 1e-300:\n            log_sol_norms.append(0.5 * np.log(norm_x2))\n            log_res_norms.append(0.5 * np.log(norm_r2))\n        else: # Mark as invalid\n            log_sol_norms.append(np.nan) \n            log_res_norms.append(np.nan)\n\n    log_res_norms = np.array(log_res_norms)\n    log_sol_norms = np.array(log_sol_norms)\n    t = np.log(lambda_grid)\n    \n    # Filter out invalid (nan) values from numerical issues\n    valid_indices = ~np.isnan(log_sol_norms) & ~np.isnan(log_res_norms)\n    t_valid = t[valid_indices]\n    log_res_valid = log_res_norms[valid_indices]\n    log_sol_valid = log_sol_norms[valid_indices]\n    \n    # Compute derivatives w.r.t. t = log(lambda)\n    x_prime = np.gradient(log_res_valid, t_valid)\n    y_prime = np.gradient(log_sol_valid, t_valid)\n    \n    x_double_prime = np.gradient(x_prime, t_valid)\n    y_double_prime = np.gradient(y_prime, t_valid)\n    \n    # Curvature calculation\n    numerator = np.abs(x_prime * y_double_prime - y_prime * x_double_prime)\n    denominator = (x_prime**2 + y_prime**2)**1.5\n    \n    # Avoid division by zero\n    curvature = np.zeros_like(denominator)\n    valid_denom = denominator > 1e-12\n    curvature[valid_denom] = numerator[valid_denom] / denominator[valid_denom]\n    \n    idx_max_curv = np.argmax(curvature)\n    lambda_star = (lambda_grid[valid_indices])[idx_max_curv]\n    \n    return lambda_star\n\ndef evaluate_diagnostic(U, s, V, b, lambda_star, k, cos_theta_thresh, f_top_thresh):\n    \"\"\"Evaluates the diagnostic for a given b and lambda_star.\"\"\"\n    m, n = U.shape[0], V.shape[0]\n    \n    beta_full = U.T @ b\n    beta = beta_full[:n]\n        \n    s2 = s**2\n    lam_star2 = lambda_star**2\n    x_coeffs_V = (s * beta) / (s2 + lam_star2)\n    \n    # Calculate cos(theta)\n    norm_x = np.linalg.norm(x_coeffs_V)\n    norm_proj_x = np.linalg.norm(x_coeffs_V[:k])\n    \n    cos_theta = 0.0\n    if norm_x > 1e-16:\n      cos_theta = norm_proj_x / norm_x\n      \n    # Calculate f_top\n    norm_beta_full_sq = np.sum(beta_full**2)\n    norm_beta_top_k_sq = np.sum(beta[:k]**2)\n    \n    f_top = 0.0\n    if norm_beta_full_sq > 1e-16:\n      f_top = norm_beta_top_k_sq / norm_beta_full_sq\n\n    is_suspect = (cos_theta >= cos_theta_thresh) and (f_top >= f_top_thresh)\n    \n    return is_suspect\n\ndef solve():\n    \"\"\"Main function to run the test suite and print results.\"\"\"\n    m, n = 60, 60\n    k = 3\n    matrix_seed = 0\n    noise_seed = 1\n\n    cos_theta_thresh = 0.995\n    f_top_thresh = 0.2\n    \n    A, U, s, V, x_true = construct_problem(m, n, matrix_seed)\n    b_clean = A @ x_true\n    norm_b_clean = np.linalg.norm(b_clean)\n    \n    lambda_grid = np.logspace(-10, 2, 400)\n    \n    rng_noise = np.random.default_rng(noise_seed)\n    \n    e_white = rng_noise.standard_normal(m)\n    e_white /= np.linalg.norm(e_white)\n    \n    noise_cases = [\n        (0.05 * norm_b_clean * e_white),    # 1. Benign white noise\n        (0.5 * norm_b_clean * U[:, 0]),     # 2. Adversarial leading-direction\n        (0.5 * norm_b_clean * U[:, -1]),    # 3. Adversarial trailing-direction\n        (1e-8 * norm_b_clean * e_white)     # 4. Near noise-free\n    ]\n    \n    results = []\n    \n    for e in noise_cases:\n        b = b_clean + e\n        lambda_star = get_lcurve_corner(U, s, b, lambda_grid)\n        is_suspect = evaluate_diagnostic(U, s, V, b, lambda_star, k, cos_theta_thresh, f_top_thresh)\n        results.append(is_suspect)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}