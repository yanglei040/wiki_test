{
    "hands_on_practices": [
        {
            "introduction": "To truly appreciate Total Least Squares (TLS), it is essential to contrast it with the more familiar Ordinary Least Squares (OLS). This exercise  challenges you to derive the solutions for both methods from first principles, revealing why TLS is the appropriate choice for errors-in-variables models. You will discover the elegant connection between the TLS solution and the principal eigenvector of the data's covariance matrix, providing a deep geometric insight into how TLS captures the primary direction of variation.",
            "id": "3173554",
            "problem": "A laboratory records paired measurements $\\{(X_{i}, Y_{i})\\}_{i=1}^{n}$ of two physical quantities that jointly vary approximately along a straight relationship but both are measured with additive mean-zero noise. Assume the underlying signal is linear and that the measurement noises in $X$ and $Y$ are independent of the signal and of each other, with finite variances. The investigator wants to fit a straight line through the cloud of points. Two modeling choices are considered: Ordinary Least Squares (OLS) and Total Least Squares (TLS).\n\n- In Ordinary Least Squares (OLS), the model $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$ assumes that $X$ is measured without error and vertical residuals in $Y$ are minimized.\n- In Total Least Squares (TLS), also called errors-in-variables regression, the line is chosen to minimize the sum of squared orthogonal distances from the points to the line, acknowledging that both $X$ and $Y$ have measurement noise.\n\nYou are given the centered sample covariance matrix of the observed data,\n$$\n\\mathbf{S} \\;=\\; \\begin{pmatrix}\n3.2 & 2.4 \\\\\n2.4 & 5.0\n\\end{pmatrix},\n$$\ncomputed from $n$ observations, where $\\mathbf{S}$ has entries $s_{xx}$, $s_{xy}$, and $s_{yy}$ corresponding to the sample covariances of $X$ and $Y$.\n\nTasks:\n1. Starting from the definition of the OLS objective (minimizing the sum of squared vertical residuals) and the first-order optimality conditions, identify the structural assumption about errors and derive the OLS slope in terms of sample moments, explaining why the OLS line can differ from a line that minimizes orthogonal distances when $X$ is noisy.\n2. Starting from the definition of the TLS objective (minimizing the sum of squared orthogonal distances) and the requirement that the direction vector of the line has unit norm, derive the characterization of the TLS direction as an eigenvector of the sample covariance matrix $\\mathbf{S}$. Then express the TLS slope as the ratio of the components of that eigenvector in terms of $\\mathbf{S}$’s entries and its largest eigenvalue.\n3. Using the provided $\\mathbf{S}$, compute the TLS slope numerically.\n\nRound your numerical answer for the TLS slope to four significant figures. Express the slope as a unitless real number and report only the TLS slope for your final answer.",
            "solution": "We consider fitting a line to a cloud of points where both $X$ and $Y$ carry measurement noise. Let the centered data matrix be $\\mathbf{Z} \\in \\mathbb{R}^{n \\times 2}$, where each row is $\\mathbf{z}_{i} = (x_{i} - \\bar{x}, y_{i} - \\bar{y})$, and the sample covariance matrix is\n$$\n\\mathbf{S} \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{z}_{i} \\mathbf{z}_{i}^{\\top}\n\\;=\\;\n\\begin{pmatrix}\ns_{xx} & s_{xy} \\\\\ns_{xy} & s_{yy}\n\\end{pmatrix}.\n$$\n\nPart 1 (OLS): Ordinary Least Squares (OLS) assumes a model $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$ in which $X$ is treated as nonrandom or measured without error and the residual $\\varepsilon$ represents noise only in $Y$. The OLS objective is\n$$\n\\min_{\\beta_{0}, \\beta_{1}} \\sum_{i=1}^{n} \\left(y_{i} - \\beta_{0} - \\beta_{1} x_{i}\\right)^{2}.\n$$\nTaking partial derivatives with respect to $\\beta_{0}$ and $\\beta_{1}$ and setting them to zero yields the normal equations. After centering (so the intercept is $\\beta_{0} = \\bar{y} - \\beta_{1} \\bar{x}$), the first-order condition for $\\beta_{1}$ becomes\n$$\n\\sum_{i=1}^{n} (x_{i} - \\bar{x})(y_{i} - \\bar{y}) \\;=\\; \\beta_{1} \\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}.\n$$\nDividing by $n$ gives\n$$\ns_{xy} \\;=\\; \\beta_{1} s_{xx} \\quad \\Rightarrow \\quad \\beta_{1}^{\\mathrm{OLS}} \\;=\\; \\frac{s_{xy}}{s_{xx}}.\n$$\nThis formula presumes $X$ is measured without error, so the vertical residuals in $Y$ are the only errors penalized. When $X$ is noisy, minimizing only vertical residuals biases the slope toward underestimating the true direction of the data cloud, and hence the OLS line can differ markedly from a line that minimizes orthogonal distances.\n\nPart 2 (TLS): Total Least Squares (TLS), also called errors-in-variables, seeks a line that minimizes the sum of squared orthogonal distances from points to the line, acknowledging that both coordinates carry noise. Parameterize a line passing through the data centroid $(\\bar{x}, \\bar{y})$ with a unit direction vector $\\mathbf{v} = (v_{x}, v_{y})^{\\top}$, where $\\|\\mathbf{v}\\| = 1$. The orthogonal residual for a centered point $\\mathbf{z}_{i}$ is the component orthogonal to $\\mathbf{v}$:\n$$\n\\mathbf{r}_{i} \\;=\\; \\mathbf{z}_{i} - (\\mathbf{z}_{i}^{\\top}\\mathbf{v})\\,\\mathbf{v},\n\\quad\n\\|\\mathbf{r}_{i}\\|^{2} \\;=\\; \\|\\mathbf{z}_{i}\\|^{2} - (\\mathbf{z}_{i}^{\\top}\\mathbf{v})^{2}.\n$$\nThe TLS objective is\n$$\n\\min_{\\mathbf{v} \\in \\mathbb{R}^{2}} \\sum_{i=1}^{n} \\|\\mathbf{r}_{i}\\|^{2}\n\\quad \\text{subject to} \\quad \\|\\mathbf{v}\\| = 1.\n$$\nSince $\\sum_{i=1}^{n} \\|\\mathbf{z}_{i}\\|^{2}$ is constant with respect to $\\mathbf{v}$, minimizing the sum of squared orthogonal distances is equivalent to maximizing the projected variance along $\\mathbf{v}$:\n$$\n\\max_{\\|\\mathbf{v}\\| = 1} \\sum_{i=1}^{n} (\\mathbf{z}_{i}^{\\top}\\mathbf{v})^{2}\n\\;=\\;\n\\max_{\\|\\mathbf{v}\\| = 1} \\mathbf{v}^{\\top} \\left( \\sum_{i=1}^{n} \\mathbf{z}_{i}\\mathbf{z}_{i}^{\\top} \\right) \\mathbf{v}\n\\;=\\;\n\\max_{\\|\\mathbf{v}\\| = 1} \\mathbf{v}^{\\top} (n \\mathbf{S}) \\mathbf{v}.\n$$\nThis is a Rayleigh quotient maximization, whose maximizer $\\mathbf{v}$ is the eigenvector of $\\mathbf{S}$ associated with its largest eigenvalue $\\lambda_{1}$. Thus, the TLS direction is characterized by\n$$\n\\mathbf{S}\\,\\mathbf{v} \\;=\\; \\lambda_{1}\\,\\mathbf{v}, \\quad \\|\\mathbf{v}\\| = 1,\n$$\nand the TLS slope $m_{\\mathrm{TLS}}$ is the ratio of the components of $\\mathbf{v}$:\n$$\nm_{\\mathrm{TLS}} \\;=\\; \\frac{v_{y}}{v_{x}}.\n$$\nFrom the eigenvector equation,\n$$\n(s_{xx} - \\lambda_{1}) v_{x} + s_{xy} v_{y} \\;=\\; 0\n\\quad \\Rightarrow \\quad\n\\frac{v_{y}}{v_{x}} \\;=\\; \\frac{\\lambda_{1} - s_{xx}}{s_{xy}}.\n$$\nEquivalently,\n$$\n\\frac{v_{y}}{v_{x}} \\;=\\; \\frac{s_{xy}}{\\lambda_{1} - s_{yy}},\n$$\nand these are consistent for $\\lambda_{1}$ satisfying the characteristic equation of $\\mathbf{S}$.\n\nPart 3 (Numerical TLS slope): For\n$$\n\\mathbf{S} \\;=\\; \\begin{pmatrix}\n3.2 & 2.4 \\\\\n2.4 & 5.0\n\\end{pmatrix},\n$$\nthe eigenvalues are the roots of\n$$\n\\lambda^{2} - (s_{xx} + s_{yy}) \\lambda + (s_{xx}s_{yy} - s_{xy}^{2}) \\;=\\; 0.\n$$\nCompute the trace and determinant:\n$$\n\\operatorname{tr}(\\mathbf{S}) \\;=\\; 3.2 + 5.0 \\;=\\; 8.2,\n\\quad\n\\det(\\mathbf{S}) \\;=\\; 3.2 \\cdot 5.0 - 2.4^{2} \\;=\\; 16.0 - 5.76 \\;=\\; 10.24.\n$$\nThus,\n$$\n\\lambda_{1,2} \\;=\\; \\frac{ \\operatorname{tr}(\\mathbf{S}) \\pm \\sqrt{ \\operatorname{tr}(\\mathbf{S})^{2} - 4 \\det(\\mathbf{S}) } }{2}\n\\;=\\;\n\\frac{ 8.2 \\pm \\sqrt{ 67.24 - 40.96 } }{2}\n\\;=\\;\n\\frac{ 8.2 \\pm \\sqrt{ 26.28 } }{2}.\n$$\nThe larger eigenvalue is\n$$\n\\lambda_{1} \\;=\\; \\frac{ 8.2 + \\sqrt{26.28} }{2}.\n$$\nUsing this $\\lambda_{1}$ in the slope formula,\n$$\nm_{\\mathrm{TLS}} \\;=\\; \\frac{ \\lambda_{1} - s_{xx} }{ s_{xy} }\n\\;=\\;\n\\frac{ \\frac{ 8.2 + \\sqrt{26.28} }{2} - 3.2 }{ 2.4 }\n\\;=\\;\n\\frac{ 4.1 + \\frac{1}{2}\\sqrt{26.28} - 3.2 }{2.4}\n\\;=\\;\n\\frac{ 0.9 + \\frac{1}{2}\\sqrt{26.28} }{2.4}.\n$$\nNumerically, $\\sqrt{26.28} \\approx 5.1265$, hence\n$$\nm_{\\mathrm{TLS}} \\;\\approx\\; \\frac{0.9 + 2.56325}{2.4} \\;=\\; \\frac{3.46325}{2.4} \\;\\approx\\; 1.443.\n$$\nRounded to four significant figures, the TLS slope is $1.443$.\n\nFor contrast, the OLS slope from the same data would be\n$$\n\\beta_{1}^{\\mathrm{OLS}} \\;=\\; \\frac{s_{xy}}{s_{xx}} \\;=\\; \\frac{2.4}{3.2} \\;=\\; 0.75,\n$$\nwhich is notably smaller because OLS ignores noise in $X$, minimizing only vertical deviations. TLS, by minimizing orthogonal distances, aligns with the principal direction of the data cloud and produces a steeper slope here.\n\nThe requested final answer is the TLS slope.",
            "answer": "$$\\boxed{1.443}$$"
        },
        {
            "introduction": "Moving from conceptual understanding to practical implementation is a key step in mastering any numerical method. This practice  guides you through the process of formulating a line-fitting problem from its geometric definition—minimizing orthogonal distances—and deriving the complete solution using the Singular Value Decomposition (SVD). By implementing the algorithm and applying it to various datasets, you will gain hands-on experience in building a robust TLS solver from scratch.",
            "id": "3223301",
            "problem": "You are given sets of discrete planar data points where both the $x$- and $y$-coordinates are affected by measurement errors of comparable magnitude. Your task is to formulate the problem of fitting a straight line to these data using Total Least Squares (TLS), derive a computationally stable method from first principles, and implement it as a program that processes a specified test suite and outputs the results in a precise format.\n\nFundamental base and objective. Start from the geometric definition that a straight line in the plane can be written in the homogeneous form $a x + b y + c = 0$ with the normalization constraint $a^{2} + b^{2} = 1$. The orthogonal distance from a data point $(x_{i}, y_{i})$ to the line is given by $\\lvert a x_{i} + b y_{i} + c \\rvert$ because of the normalization. The Total Least Squares objective for line fitting with errors in both variables seeks the line parameters that minimize the sum of squared orthogonal distances to all points, subject to the normalization constraint. This is a discrete least squares problem where the residual is measured orthogonally to the model manifold rather than along a coordinate axis.\n\nDerivation scope and algorithmic requirements. From the above base, derive the constrained minimization formulation and reduce it to a numerically stable computation using linear algebra. Your derivation must not assume formulas specific to Total Least Squares or orthogonal regression as a starting point; instead, begin from the definition of the orthogonal distance and the normalization constraint, and use only widely known linear algebra tools such as eigenvalue problems or Singular Value Decomposition (SVD). Your final algorithm must:\n- Represent the fitted line in homogeneous form $a x + b y + c = 0$ with $a^{2} + b^{2} = 1$.\n- Impose the sign convention $a \\ge 0$, and if $a = 0$ then $b \\ge 0$, to ensure uniqueness.\n- Ensure the fitted line passes through the centroid $(\\bar{x}, \\bar{y})$ of the data that minimizes the orthogonal residuals under the normalization constraint.\n- Compute the root-mean-square (RMS) orthogonal distance $r = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (a x_{i} + b y_{i} + c)^{2}}$ for each dataset, where $n$ is the number of points.\n\nTest suite. Implement your algorithm for the following datasets. Each dataset is a list of ordered pairs $(x, y)$, with every number shown in decimal form:\n- Case $1$ (general sloped line with perturbations): $[(0.02, 1.12), (0.51, 1.93), (0.98, 2.92), (1.48, 4.06), (2.02, 5.01), (2.49, 6.07)]$.\n- Case $2$ (nearly vertical line): $[(2.95, -1.00), (3.02, 0.50), (3.05, 2.00), (2.97, 3.50), (3.01, 5.00)]$.\n- Case $3$ (negative slope near the origin with symmetry): $[(-4.00, 2.10), (-2.00, 1.02), (0.00, -0.02), (2.00, -1.05), (4.00, -1.95)]$.\n- Case $4$ (two-point minimal case): $[(1.00, 1.00), (3.01, 1.99)]$.\n\nNumerical and output requirements.\n- Use the homogeneous line parameters $(a, b, c)$ with $a^{2} + b^{2} = 1$ and the sign convention as stated.\n- For each dataset, compute and output the quadruple $[a, b, c, r]$, where $r$ is the RMS orthogonal distance as defined above.\n- Round every reported real number in the final output to exactly $6$ decimal places.\n- The final program output must be a single line containing a list of results for the four cases, in this exact format (no spaces are required, but they are permitted): a comma-separated list of the four quadruples enclosed in square brackets, for example, $[[a_{1},b_{1},c_{1},r_{1}],[a_{2},b_{2},c_{2},r_{2}],[a_{3},b_{3},c_{3},r_{3}],[a_{4},b_{4},c_{4},r_{4}]]$ with each $a_{k}, b_{k}, c_{k}, r_{k}$ rounded to $6$ decimals.\n- Angles are not involved; no physical units are required.\n- There must be no external inputs; the program must run as-is and print the required single line.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each case represented by its own list $[a, b, c, r]$ as described above. For example: $[[a_{1},b_{1},c_{1},r_{1}],[a_{2},b_{2},c_{2},r_{2}],[a_{3},b_{3},c_{3},r_{3}],[a_{4},b_{4},c_{4},r_{4}]]$.",
            "solution": "The problem requires the formulation and implementation of a total least squares (TLS) method to fit a straight line to a set of data points $(x_i, y_i)$, where both coordinates are subject to error. The derivation must start from first principles.\n\nLet the straight line be represented in the homogeneous form $a x + b y + c = 0$. The parameters $(a,b)$ are normalized such that $a^2 + b^2 = 1$. With this normalization, the orthogonal distance from a point $(x_i, y_i)$ to the line is given by $d_i = |a x_i + b y_i + c|$. The objective of TLS is to find the parameters $a, b, c$ that minimize the sum of the squared orthogonal distances for a set of $n$ data points:\n$$ \\text{Minimize } S(a, b, c) = \\sum_{i=1}^{n} (a x_i + b y_i + c)^2 \\quad \\text{subject to} \\quad g(a, b) = a^2 + b^2 - 1 = 0 $$\n\nTo find the optimal parameters, we can use calculus. A necessary condition for a minimum is that the partial derivative of $S$ with respect to $c$ must be zero:\n$$ \\frac{\\partial S}{\\partial c} = \\sum_{i=1}^{n} 2(a x_i + b y_i + c) \\cdot 1 = 0 $$\n$$ \\implies a \\sum_{i=1}^{n} x_i + b \\sum_{i=1}^{n} y_i + \\sum_{i=1}^{n} c = 0 $$\nLet $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$ and $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$ be the coordinates of the centroid of the data points. The condition becomes:\n$$ a \\bar{x} + b \\bar{y} + c = 0 $$\nThis demonstrates that the optimal line must pass through the centroid $(\\bar{x}, \\bar{y})$ of the data. This allows us to express $c$ in terms of $a$ and $b$:\n$$ c = -(a \\bar{x} + b \\bar{y}) $$\nSubstituting this expression for $c$ back into the objective function $S$, we simplify the problem. Let us define the centered coordinates $x'_i = x_i - \\bar{x}$ and $y'_i = y_i - \\bar{y}$. The minimization problem is now reduced to finding parameters $a$ and $b$ that:\n$$ \\text{Minimize } S(a, b) = \\sum_{i=1}^{n} (a x'_i + b y'_i)^2 \\quad \\text{subject to} \\quad a^2 + b^2 = 1 $$\nThis problem can be elegantly solved using linear algebra. Let us define a vector of parameters $\\mathbf{u} = \\begin{pmatrix} a \\\\ b \\end{pmatrix}$, so the constraint is $\\mathbf{u}^T \\mathbf{u} = 1$. Let us also define an $n \\times 2$ matrix $\\mathbf{A}$ containing the centered data points:\n$$ \\mathbf{A} = \\begin{pmatrix} x'_1 & y'_1 \\\\ x'_2 & y'_2 \\\\ \\vdots & \\vdots \\\\ x'_n & y'_n \\end{pmatrix} $$\nThe sum of squares can be written as the squared Euclidean norm of the vector $\\mathbf{A}\\mathbf{u}$:\n$$ S = \\| \\mathbf{A}\\mathbf{u} \\|_2^2 = (\\mathbf{A}\\mathbf{u})^T (\\mathbf{A}\\mathbf{u}) = \\mathbf{u}^T \\mathbf{A}^T \\mathbf{A} \\mathbf{u} $$\nThe problem is now to minimize the quadratic form $\\mathbf{u}^T (\\mathbf{A}^T \\mathbf{A}) \\mathbf{u}$ subject to the constraint that $\\mathbf{u}$ is a unit vector. The matrix $\\mathbf{C} = \\mathbf{A}^T \\mathbf{A}$ is the $2 \\times 2$ scatter matrix of the centered data:\n$$ \\mathbf{C} = \\begin{pmatrix} \\sum (x'_i)^2 & \\sum x'_i y'_i \\\\ \\sum x'_i y'_i & \\sum (y'_i)^2 \\end{pmatrix} $$\nThis formulation is a classic Rayleigh quotient problem. The minimum value of $\\mathbf{u}^T \\mathbf{C} \\mathbf{u}$ is the smallest eigenvalue of the symmetric matrix $\\mathbf{C}$, and the vector $\\mathbf{u}$ that achieves this minimum is the corresponding eigenvector.\n\nA numerically robust and computationally stable method to solve this is via the Singular Value Decomposition (SVD) of the matrix $\\mathbf{A}$. Let the SVD of $\\mathbf{A}$ be $\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T$, where $\\mathbf{U}$ is an $n \\times 2$ matrix with orthonormal columns, $\\mathbf{\\Sigma}$ is a $2 \\times 2$ diagonal matrix of singular values $\\sigma_1 \\ge \\sigma_2 \\ge 0$, and $\\mathbf{V}$ is a $2 \\times 2$ orthogonal matrix whose columns $\\mathbf{v}_1, \\mathbf{v}_2$ are the right-singular vectors.\n\nThe quantity to minimize is $\\|\\mathbf{A}\\mathbf{u}\\|_2^2$. Since $\\mathbf{V}$ is orthogonal, its columns form an orthonormal basis for $\\mathbb{R}^2$. Any unit vector $\\mathbf{u}$ can be written as a linear combination $\\mathbf{u} = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2$ with $c_1^2 + c_2^2 = 1$. The product $\\mathbf{A}\\mathbf{v}_j = \\sigma_j \\mathbf{u}_j$, where $\\mathbf{u}_j$ is the $j$-th column of $\\mathbf{U}$.\n$$ \\|\\mathbf{A}\\mathbf{u}\\|_2^2 = \\| \\mathbf{A} (c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2) \\|_2^2 = \\| c_1 \\mathbf{A}\\mathbf{v}_1 + c_2 \\mathbf{A}\\mathbf{v}_2 \\|_2^2 = \\| c_1 \\sigma_1 \\mathbf{u}_1 + c_2 \\sigma_2 \\mathbf{u}_2 \\|_2^2 $$\nSince $\\mathbf{u}_1$ and $\\mathbf{u}_2$ are orthonormal, this simplifies to:\n$$ \\|\\mathbf{A}\\mathbf{u}\\|_2^2 = c_1^2 \\sigma_1^2 + c_2^2 \\sigma_2^2 $$\nTo minimize this expression subject to $c_1^2 + c_2^2 = 1$ and given $\\sigma_1 \\ge \\sigma_2$, we must choose $c_1 = 0$ and $c_2 = 1$. This makes $\\mathbf{u} = \\mathbf{v}_2$, the right-singular vector corresponding to the smallest singular value $\\sigma_2$. The minimum value of the sum of squares is $S_{min} = \\sigma_2^2$.\n\nTherefore, the algorithm is as follows:\n1.  Compute the centroid $(\\bar{x}, \\bar{y})$ of the $n$ data points $(x_i, y_i)$.\n2.  Construct the centered data matrix $\\mathbf{A}$ with rows $(x_i - \\bar{x}, y_i - \\bar{y})$.\n3.  Compute the SVD of $\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T$. The SVD yields singular values $\\sigma_1 \\ge \\sigma_2$ and the matrix $\\mathbf{V}^T$.\n4.  The optimal parameter vector $\\begin{pmatrix} a \\\\ b \\end{pmatrix}$ is the right-singular vector corresponding to $\\sigma_2$. This vector is the second column of $\\mathbf{V}$, which is the second row of $\\mathbf{V}^T$.\n5.  Let $(a, b)$ be this vector. To ensure uniqueness, apply the sign convention: if $a < 0$, or if $a = 0$ and $b < 0$, negate both $a$ and $b$.\n6.  Compute $c = - (a \\bar{x} + b \\bar{y})$.\n7.  The sum of squared orthogonal distances is $S_{min} = \\sigma_2^2$. The root-mean-square (RMS) orthogonal distance is $r = \\sqrt{S_{min}/n} = \\sigma_2 / \\sqrt{n}$.\n8.  The solution is the quadruple $[a, b, c, r]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the total least squares line fitting problem for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general sloped line with perturbations)\n        [(0.02, 1.12), (0.51, 1.93), (0.98, 2.92), (1.48, 4.06), (2.02, 5.01), (2.49, 6.07)],\n        # Case 2 (nearly vertical line)\n        [(2.95, -1.00), (3.02, 0.50), (3.05, 2.00), (2.97, 3.50), (3.01, 5.00)],\n        # Case 3 (negative slope near the origin with symmetry)\n        [(-4.00, 2.10), (-2.00, 1.02), (0.00, -0.02), (2.00, -1.05), (4.00, -1.95)],\n        # Case 4 (two-point minimal case)\n        [(1.00, 1.00), (3.01, 1.99)],\n    ]\n\n    def fit_tls_line(points):\n        \"\"\"\n        Fits a line using Total Least Squares based on SVD.\n        \n        Args:\n            points: A list of (x, y) tuples.\n            \n        Returns:\n            A list [a, b, c, r] representing the line a*x + b*y + c = 0\n            and the RMS orthogonal distance r.\n        \"\"\"\n        data = np.array(points)\n        n = data.shape[0]\n\n        # 1. Compute the centroid of the data\n        centroid = np.mean(data, axis=0)\n        x_bar, y_bar = centroid\n\n        # 2. Form the centered data matrix A\n        centered_data = data - centroid\n\n        # 3. Compute the SVD of the centered data matrix\n        # U: Unitary matrix (left singular vectors)\n        # s: Singular values (sorted in descending order)\n        # Vt: Unitary matrix (right singular vectors, transposed)\n        _, s, Vt = np.linalg.svd(centered_data)\n\n        # 4. The parameters (a, b) are the components of the right singular vector\n        # corresponding to the smallest singular value. This is the last row of Vt.\n        a, b = Vt[1]\n\n        # 5. Apply the sign convention for uniqueness: a >= 0, and if a = 0, then b >= 0.\n        # We use a small tolerance for floating point comparisons.\n        if a  0.0 or (np.isclose(a, 0.0) and b  0.0):\n            a = -a\n            b = -b\n\n        # 6. Compute c using the fact that the line passes through the centroid\n        c = -(a * x_bar + b * y_bar)\n\n        # 7. Compute the RMS orthogonal distance\n        # The smallest singular value is s[1]. The sum of squared distances is s[1]**2.\n        sigma_2 = s[1]\n        r = sigma_2 / np.sqrt(n)\n        \n        return [a, b, c, r]\n\n    results = []\n    for case_data in test_cases:\n        result_quadruple = fit_tls_line(case_data)\n        results.append(result_quadruple)\n    \n    # Format the final output string as specified\n    formatted_results = []\n    for res in results:\n        # Format each number to 6 decimal places.\n        formatted_quad = [f\"{val:.6f}\" for val in res]\n        formatted_results.append(f\"[{','.join(formatted_quad)}]\")\n    \n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "For graduate-level studies, it is not enough to know *what* algorithm to use; one must also understand *how* to implement it in a numerically stable way. This advanced practice  explores a state-of-the-art method for solving the TLS problem using Golub-Kahan bidiagonalization, which avoids the potentially unstable step of forming the matrix product $D^T D$. This exercise will deepen your understanding of backward stability and the sophisticated techniques used in high-performance numerical libraries.",
            "id": "3599775",
            "problem": "Consider the Total Least Squares (TLS) problem for an overdetermined linear system. Given a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$ and a right-hand side vector $b \\in \\mathbb{R}^{m}$, define the augmented data matrix $D = [A\\ b] \\in \\mathbb{R}^{m \\times (n+1)}$. The TLS estimator seeks a vector $x \\in \\mathbb{R}^{n}$ and perturbations $\\Delta A \\in \\mathbb{R}^{m \\times n}$, $\\Delta b \\in \\mathbb{R}^{m}$ minimizing the Frobenius norm subject to consistency, namely\n$$\n\\min_{\\Delta A, \\Delta b, x} \\ \\|[\\Delta A\\ \\Delta b]\\|_F \\quad \\text{subject to} \\quad (A + \\Delta A)x = b + \\Delta b.\n$$\nYou must design a numerically stable algorithm to compute a TLS estimate of $x$ that adheres to the following constraints:\n\n- The method must be based on Golub–Kahan bidiagonalization (via orthogonal Householder transformations) of $D = [A\\ b]$, followed by a partial Singular Value Decomposition (SVD) of the resulting bidiagonal matrix. You must not form $D^T D$ at any point.\n- The derivation must start from core definitions and facts: the definition of TLS, the definition of Singular Value Decomposition, and the properties of orthogonal transformations and Householder reflectors. No other intermediate formulas are to be assumed.\n- You must justify, from first principles, why your algorithm is backward stable in floating-point arithmetic.\n\nYour program must implement this algorithm and evaluate it on the following test suite, using double-precision arithmetic:\n\n- Test case $1$ (near-consistent data with small perturbations):\n  - Dimensions: $m = 8$, $n = 3$.\n  - Random seed: $0$ for all random draws.\n  - Generate $A_{\\text{true}} \\in \\mathbb{R}^{8 \\times 3}$ and $x_{\\star} \\in \\mathbb{R}^3$ with independent standard normal entries.\n  - Set $b_{\\text{true}} = A_{\\text{true}} x_{\\star}$.\n  - Form $A = A_{\\text{true}} + \\eta_A$ and $b = b_{\\text{true}} + \\eta_b$, where entries of $\\eta_A$ and $\\eta_b$ are independent standard normal scaled by $10^{-3}$.\n  - Compute a TLS estimate $x_{\\text{TLS}}$ from $(A,b)$.\n  - Output $r_1 = \\|x_{\\text{TLS}} - x_{\\star}\\|_2 / \\|x_{\\star}\\|_2$ as a floating-point number.\n\n- Test case $2$ (left-orthogonal invariance of the TLS solution):\n  - Use the same $(A,b)$ from Test case $1$.\n  - Generate $Q \\in \\mathbb{R}^{8 \\times 8}$ as an orthogonal matrix by applying the $QR$ factorization (where $Q$ is orthogonal and $R$ is upper-triangular) to an $8 \\times 8$ standard normal matrix, with the same random seed $0$.\n  - Compute $x_{\\text{TLS}}^{(D)}$ from $D = [A\\ b]$ and $x_{\\text{TLS}}^{(QD)}$ from $QD = [QA\\ Qb]$.\n  - Output $r_2 = \\|x_{\\text{TLS}}^{(D)} - x_{\\text{TLS}}^{(QD)}\\|_2$ as a floating-point number.\n\n- Test case $3$ (exactly consistent data):\n  - Dimensions: $m = 6$, $n = 3$.\n  - Random seed: $123$ for all random draws in this test.\n  - Generate $A \\in \\mathbb{R}^{6 \\times 3}$ and $x_{\\star} \\in \\mathbb{R}^3$ with independent standard normal entries.\n  - Set $b = A x_{\\star}$ (no perturbations).\n  - Compute a TLS estimate $x_{\\text{TLS}}$ from $(A,b)$.\n  - Output $r_3 = \\|x_{\\text{TLS}} - x_{\\star}\\|_2 / \\|x_{\\star}\\|_2$ as a floating-point number.\n\n- Test case $4$ (singular triplet consistency check without forming $D^T D$):\n  - Use $D = [A\\ b]$ from Test case $1$.\n  - Let $v_{\\min} \\in \\mathbb{R}^{n+1}$ be the last right singular vector of $D$ recovered by your bidiagonalization-plus-bidiagonal-SVD pipeline, and let $\\sigma_{\\min}$ be the corresponding smallest singular value of $D$.\n  - Output $r_4 = \\left|\\|D v_{\\min}\\|_2 - \\sigma_{\\min}\\right|$ as a floating-point number.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[r_1,r_2,r_3,r_4]$. For example, the printed line must look like $[0.00123,4.56e-07,0.0,1.2e-15]$ with no additional text.\n\nAll random draws must use the specified seeds. Angles are not involved. There are no physical units. All outputs are real-valued floating-point scalars. Your implementation must be self-contained and must not require any user input, external files, or network access.",
            "solution": "We begin from the core definitions. The Total Least Squares (TLS) problem seeks perturbations $\\Delta A \\in \\mathbb{R}^{m \\times n}$, $\\Delta b \\in \\mathbb{R}^{m}$ and a vector $x \\in \\mathbb{R}^{n}$ that minimize the Frobenius norm $\\|[\\Delta A\\ \\Delta b]\\|_F$ subject to $(A + \\Delta A)x = b + \\Delta b$. Let $D = [A\\ b] \\in \\mathbb{R}^{m \\times (n+1)}$. The consistency constraint can be expressed as $D v = 0$ with $v = \\begin{bmatrix} x \\\\ -1 \\end{bmatrix}$ if there are no perturbations. In the presence of perturbations, the problem can be recast as finding a unit vector $v \\in \\mathbb{R}^{n+1}$ that minimizes $\\|D v\\|_2$ and then extracting $x$ by dehomogenization, provided the last component of $v$ is nonzero. This is rooted in the following standard fact about singular value decomposition.\n\nBy the definition of Singular Value Decomposition (SVD), any real matrix $D \\in \\mathbb{R}^{m \\times p}$ with $p = n+1$ admits a factorization $D = U \\Sigma V^T$ where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{p \\times p}$ are orthogonal and $\\Sigma \\in \\mathbb{R}^{m \\times p}$ is diagonal with nonnegative entries $\\sigma_1 \\ge \\cdots \\ge \\sigma_p \\ge 0$ on the diagonal (padding with zeros if necessary). The variational characterization of singular values states that\n$$\n\\sigma_p = \\min_{\\|v\\|_2=1} \\|D v\\|_2,\n$$\nwith a minimizer $v$ equal to the last right singular vector $v_p$ (the last column of $V$). Thus, if we find $v_{\\min} = v_p$ corresponding to $\\sigma_{\\min} = \\sigma_p$, then a TLS estimate is obtained by writing $v_{\\min} = \\alpha \\begin{bmatrix} x \\\\ -1 \\end{bmatrix}$ for some nonzero scalar $\\alpha$, which yields $x = - v_{1:n} / v_{n+1}$ provided $v_{n+1} \\ne 0$.\n\nA numerically stable way to compute the last right singular vector of $D$ avoids forming $D^T D$ because explicitly forming $D^T D$ squares the condition number and is known to be numerically unstable. Instead, we use Golub–Kahan bidiagonalization via orthogonal Householder transformations to reduce $D$ to upper bidiagonal form, followed by computing the SVD of the bidiagonal matrix. We outline the algorithmic steps.\n\n1. Householder bidiagonalization. For $D \\in \\mathbb{R}^{m \\times p}$ with $p = n+1$, we construct orthogonal Householder reflectors from the left and right that reduce $D$ to bidiagonal form. Specifically, we build a sequence of left reflectors $P_j = I - 2 u_j u_j^T$ acting on rows and right reflectors $Q_j = I - 2 w_j w_j^T$ acting on columns, with unit vectors $u_j$ and $w_j$ chosen so that\n$$\nB = P_k \\cdots P_1 \\, D \\, Q_1 \\cdots Q_k\n$$\nis upper bidiagonal (that is, nonzero entries appear only on the main diagonal and the first superdiagonal) for $k = \\min(m,p)$. Accumulating the right reflectors gives an orthogonal matrix $V = Q_1 \\cdots Q_k \\in \\mathbb{R}^{p \\times p}$. By orthogonality of Householder reflectors, we have $D = U B V^T$ with $U = P_1^T \\cdots P_k^T$ orthogonal.\n\n2. SVD of a bidiagonal matrix. Compute a thin SVD of the upper bidiagonal matrix $B$,\n$$\nB = \\widehat{U} \\, \\Sigma \\, \\widehat{V}^T,\n$$\nwhere $\\widehat{U} \\in \\mathbb{R}^{m \\times p}$ has orthonormal columns, $\\widehat{V} \\in \\mathbb{R}^{p \\times p}$ is orthogonal, and $\\Sigma = \\operatorname{diag}(\\sigma_1,\\ldots,\\sigma_p)$ with $\\sigma_1 \\ge \\cdots \\ge \\sigma_p \\ge 0$. Classical algorithms for the SVD of a bidiagonal matrix (for example, the implicit shifted QR iteration or divide-and-conquer) are backward stable.\n\n3. Assemble the right singular vector. From $D = U B V^T$ and $B = \\widehat{U} \\Sigma \\widehat{V}^T$, it follows that an SVD of $D$ is given by\n$$\nD = (U \\widehat{U}) \\, \\Sigma \\, (V \\widehat{V})^T.\n$$\nTherefore, the last right singular vector of $D$ is $v_{\\min} = V \\widehat{v}_{\\min}$, where $\\widehat{v}_{\\min}$ is the last column of $\\widehat{V}$. Since $V$ is orthogonal, $v_{\\min}$ has unit norm if $\\widehat{v}_{\\min}$ has unit norm.\n\n4. Extract the TLS estimate. Provided the last component of $v_{\\min}$ is nonzero, write $v_{\\min} = \\alpha \\begin{bmatrix} x \\\\ -1 \\end{bmatrix}$ for some nonzero $\\alpha$. Dehomogenizing gives the TLS estimate\n$$\nx_{\\text{TLS}} = -\\frac{v_{1:n}}{v_{n+1}}.\n$$\nIf $v_{n+1}$ is extremely small in magnitude, the TLS problem is ill-posed (the minimizing direction lies almost entirely in the column-space coordinates), and the dehomogenization is numerically unstable; this corresponds to a known boundary of TLS solvability.\n\nBackward stability justification. Householder reflectors are orthogonal transformations, and their application in floating-point arithmetic is known to be backward stable: each application can be interpreted as the exact application to a slightly perturbed matrix, with perturbations bounded by a small multiple of machine precision times the norm of the matrix. Specifically, if $\\mathrm{fl}(P^T D)$ denotes the computed result of applying a Householder reflector $P$ to $D$, then there exists a perturbation $\\Delta$ with $\\|\\Delta\\|_2 = \\mathcal{O}(\\epsilon) \\|D\\|_2$ such that $\\mathrm{fl}(P^T D) = P^T (D + \\Delta)$, where $\\epsilon$ is the unit roundoff. Composing finitely many such operations preserves a backward error bound of the form $\\|\\Delta D_{\\text{bidiag}}\\|_2 \\le \\mathrm{poly}(m,p) \\, \\epsilon \\, \\|D\\|_2$. The SVD of the bidiagonal matrix, when computed by a standard method such as the implicit shifted QR iteration or divide-and-conquer, is also backward stable: the computed singular triplets are exact for a nearby bidiagonal matrix $\\widehat{B} = B + \\Delta B$ with $\\|\\Delta B\\|_2 = \\mathcal{O}(\\epsilon) \\|B\\|_2$. Since pre- and postmultiplication by orthogonal matrices are norm-preserving, the assembled right singular vector $v_{\\min}$ is the exact last right singular vector of a nearby matrix $D + \\Delta D$ with $\\|\\Delta D\\|_2 \\le \\mathrm{poly}(m,p) \\, \\epsilon \\, \\|D\\|_2$. The final dehomogenization step $x_{\\text{TLS}} = -v_{1:n}/v_{n+1}$ is a continuous mapping provided $|v_{n+1}|$ is bounded away from zero; thus, the overall algorithm is backward stable whenever the TLS solution is well-posed. This approach deliberately avoids forming $D^T D$, which would square the condition number and generally destroy backward stability.\n\nAlgorithmic design for the program. We implement:\n\n- A Householder vector constructor for a given vector $z$ that returns a unit vector $u$ such that $(I - 2 u u^T) z$ is a multiple of the first canonical basis vector. We apply these from the left to zero elements below the diagonal and from the right to zero elements right of the superdiagonal.\n- Accumulation of the right orthogonal factor $V$ as the product of the right Householder reflectors. We do not need to accumulate the left orthogonal factor $U$.\n- An SVD of the resulting bidiagonal matrix $B$ to obtain $\\widehat{V}$ and the singular values; we then form $v_{\\min} = V \\widehat{v}_{\\min}$ and extract $x_{\\text{TLS}}$.\n- For Test case $4$, we verify $v_{\\min}$ and $\\sigma_{\\min}$ by computing $|\\|D v_{\\min}\\|_2 - \\sigma_{\\min}|$, which should be close to zero for a correctly computed singular triplet.\n\nTest suite outputs:\n\n- $r_1 = \\|x_{\\text{TLS}} - x_{\\star}\\|_2 / \\|x_{\\star}\\|_2$ for near-consistent data with small perturbations.\n- $r_2 = \\|x_{\\text{TLS}}^{(D)} - x_{\\text{TLS}}^{(QD)}\\|_2$ demonstrating left-orthogonal invariance.\n- $r_3 = \\|x_{\\text{TLS}} - x_{\\star}\\|_2 / \\|x_{\\star}\\|_2$ for exactly consistent data.\n- $r_4 = \\left|\\|D v_{\\min}\\|_2 - \\sigma_{\\min}\\right|$ checking singular triplet consistency.\n\nAll computations are done in double precision. The program produces a single bracketed line $[r_1,r_2,r_3,r_4]$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef householder_vector(x: np.ndarray):\n    \"\"\"\n    Compute a Householder vector v (unit 2-norm) such that\n    (I - 2 v v^T) x = +/- ||x|| e1.\n    Returns v as a 1-D numpy array. If x is the zero vector, returns None.\n    \"\"\"\n    sigma = np.linalg.norm(x)\n    if sigma == 0.0:\n        return None\n    v = x.copy()\n    sign = 1.0 if x[0] = 0 else -1.0\n    v[0] += sign * sigma\n    v_norm = np.linalg.norm(v)\n    if v_norm == 0.0:\n        return None\n    v /= v_norm\n    return v\n\ndef bidiagonalize_with_V(D: np.ndarray):\n    \"\"\"\n    Perform Golub–Kahan bidiagonalization of D via Householder reflections.\n    Returns B (bidiagonal form) and accumulated right orthogonal factor V,\n    such that D = U * B * V^T for some orthogonal U (not formed).\n    \"\"\"\n    m, p = D.shape\n    B = D.copy().astype(float)\n    V = np.eye(p, dtype=float)\n\n    k = min(m, p)\n    for j in range(k):\n        # Left reflector to zero out entries below the diagonal in column j\n        x_col = B[j:, j]\n        v = householder_vector(x_col)\n        if v is not None:\n            # Apply from the left to B[j:, j:]\n            # B[j:, j:] = (I - 2 v v^T) B[j:, j:]\n            B[j:, j:] -= 2.0 * np.outer(v, v @ B[j:, j:])\n\n        # Right reflector to zero out entries right of the superdiagonal in row j\n        if j  p - 1:\n            x_row = B[j, j+1:]\n            w = householder_vector(x_row)\n            if w is not None:\n                # Apply from the right to B[:, j+1:]\n                # B[:, j+1:] = B[:, j+1:] (I - 2 w w^T)\n                Bw = B[:, j+1:] @ w\n                B[:, j+1:] -= 2.0 * np.outer(Bw, w)\n                # Accumulate into V: V[:, j+1:] = V[:, j+1:] (I - 2 w w^T)\n                Vw = V[:, j+1:] @ w\n                V[:, j+1:] -= 2.0 * np.outer(Vw, w)\n\n    return B, V\n\ndef tls_from_D(D: np.ndarray, atol_den: float = 1e-14):\n    \"\"\"\n    Compute TLS solution x from augmented matrix D = [A b] using:\n    - Bidiagonalization D = U B V^T (only B, V needed),\n    - SVD of bidiagonal B to get last right singular vector,\n    - Back-transform to get v_min, then dehomogenize to x = -v[0:n]/v[n].\n    Returns x_tls, v_min (unit norm), sigma_min.\n    \"\"\"\n    m, p = D.shape\n    # Bidiagonalize D and accumulate right orthogonal factor V\n    B, V = bidiagonalize_with_V(D)\n    # SVD of bidiagonal B\n    # We only need singular values and right singular vectors\n    # Use thin SVD\n    Uhat, s, VhatT = svd(B, full_matrices=False, overwrite_a=False, check_finite=True)\n    Vhat = VhatT.T\n    # Last right singular vector of D\n    v_min = V @ Vhat[:, -1]\n    # Normalize for safety (should already be unit)\n    v_min /= np.linalg.norm(v_min)\n    sigma_min = s[-1]\n    # Dehomogenize\n    denom = v_min[-1]\n    if np.abs(denom)  atol_den:\n        # Ill-posed TLS (no finite solution); return NaNs to indicate failure\n        x_tls = np.full(p-1, np.nan)\n    else:\n        x_tls = -v_min[:-1] / denom\n    return x_tls, v_min, sigma_min\n\ndef solve():\n    results = []\n\n    # Test case 1: near-consistent data with small perturbations\n    rng = np.random.default_rng(0)\n    m1, n1 = 8, 3\n    A_true = rng.standard_normal((m1, n1))\n    x_star = rng.standard_normal(n1)\n    b_true = A_true @ x_star\n    noise_level = 1e-3\n    A1 = A_true + noise_level * rng.standard_normal((m1, n1))\n    b1 = b_true + noise_level * rng.standard_normal(m1)\n    D1 = np.hstack([A1, b1.reshape(-1, 1)])\n    x_tls1, vmin1, smin1 = tls_from_D(D1)\n    rel_err1 = np.linalg.norm(x_tls1 - x_star) / np.linalg.norm(x_star)\n    results.append(rel_err1)\n\n    # Test case 2: left-orthogonal invariance using Q from QR of Gaussian matrix\n    G = rng.standard_normal((m1, m1))\n    Q, _ = np.linalg.qr(G, mode='reduced')\n    D1Q = Q @ D1\n    x_tls1Q, _, _ = tls_from_D(D1Q)\n    diff2 = np.linalg.norm(x_tls1 - x_tls1Q)\n    results.append(diff2)\n\n    # Test case 3: exactly consistent data\n    rng3 = np.random.default_rng(123)\n    m3, n3 = 6, 3\n    A3 = rng3.standard_normal((m3, n3))\n    x3_star = rng3.standard_normal(n3)\n    b3 = A3 @ x3_star\n    D3 = np.hstack([A3, b3.reshape(-1, 1)])\n    x_tls3, _, _ = tls_from_D(D3)\n    rel_err3 = np.linalg.norm(x_tls3 - x3_star) / np.linalg.norm(x3_star)\n    results.append(rel_err3)\n\n    # Test case 4: singular triplet consistency check\n    # Using D1 from test case 1\n    # We already have vmin1 and smin1 from tls_from_D(D1)\n    # Verify that ||D1 vmin1||_2 approximately equals smin1\n    diff4 = abs(np.linalg.norm(D1 @ vmin1) - smin1)\n    results.append(diff4)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}