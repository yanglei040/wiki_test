{
    "hands_on_practices": [
        {
            "introduction": "A central goal of regularization is to stabilize the solution of an ill-posed problem against perturbations in the data. This practice moves beyond simply computing a solution and delves into its stability by asking: how does the Tikhonov solution $x_{\\lambda}$ change in response to small errors in the matrix $A$ and the vector $b$? By performing a first-order perturbation analysis, you will derive a fundamental expression for the solution's sensitivity, a core technique in numerical analysis that deepens the understanding of how regularization works .",
            "id": "3599467",
            "problem": "Consider the Tikhonov-regularized least-squares problem in numerical linear algebra: for a matrix $A \\in \\mathbb{R}^{m \\times n}$, a vector $b \\in \\mathbb{R}^{m}$, a regularization matrix $L \\in \\mathbb{R}^{p \\times n}$, and a parameter $\\lambda > 0$, the regularized solution $x_{\\lambda} \\in \\mathbb{R}^{n}$ is defined as the unique minimizer of the objective\n$$\n\\|A x - b\\|_{2}^{2} + \\lambda^2 \\|L x\\|_{2}^{2}.\n$$\nStart from the first-order optimality condition (obtained by setting the gradient of the objective to zero) and treat $L$ as fixed. Then consider small perturbations $\\delta A$ and $\\delta b$ to $A$ and $b$, respectively, and ignore second-order terms in $\\delta A$ and $\\delta b$. Derive the first-order approximation to the change $\\delta x$ in the solution $x_{\\lambda}$ caused by these perturbations, expressed in terms of $A$, $b$, $\\lambda$, $L$, $\\delta A$, and $\\delta b$.\n\nAfter deriving the expression, evaluate it for the following specific data:\n- $A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2}$,\n- $b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^{3}$,\n- $L = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 2}$,\n- $\\lambda = 1$,\n- $\\delta A = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2}$,\n- $\\delta b = \\begin{pmatrix} 0 \\\\ 3 \\\\ -3 \\end{pmatrix} \\in \\mathbb{R}^{3}$.\n\nCompute the corresponding first-order approximation $\\delta x$ using your derived formula. Express your final answer exactly as a $1 \\times 2$ row vector and do not round.",
            "solution": "The Tikhonov-regularized least-squares problem seeks to minimize the objective function $J(x)$:\n$$J(x) = \\|A x - b\\|_{2}^{2} + \\lambda^2 \\|L x\\|_{2}^{2}$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $L \\in \\mathbb{R}^{p \\times n}$, and $\\lambda > 0$. The objective function can be written using inner products as:\n$$J(x) = (A x - b)^T (A x - b) + \\lambda^2 (L x)^T (L x)$$\nExpanding this expression gives:\n$$J(x) = (x^T A^T - b^T)(A x - b) + \\lambda^2 x^T L^T L x$$\n$$J(x) = x^T A^T A x - x^T A^T b - b^T A x + b^T b + \\lambda^2 x^T L^T L x$$\nSince $b^T A x$ is a scalar, it is equal to its transpose $(b^T A x)^T = x^T A^T b$. Thus, we can combine the cross-terms:\n$$J(x) = x^T (A^T A + \\lambda^2 L^T L) x - 2 b^T A x + b^T b$$\nThe first-order optimality condition is found by taking the gradient of $J(x)$ with respect to $x$ and setting it to zero. Using standard matrix calculus identities, the gradient is:\n$$\\nabla_x J(x) = 2(A^T A + \\lambda^2 L^T L)x - 2 A^T b$$\nSetting $\\nabla_x J(x) = 0$ yields the normal equations for the regularized solution $x_{\\lambda}$:\n$$(A^T A + \\lambda^2 L^T L)x_{\\lambda} = A^T b$$\nThis is the required first-order optimality condition. Let us define the Hessian-like matrix $H = A^T A + \\lambda^2 L^T L$. The equation for $x_{\\lambda}$ is $H x_{\\lambda} = A^T b$.\n\nNext, we consider small perturbations $\\delta A$ and $\\delta b$ to $A$ and $b$. The perturbed matrix is $A' = A + \\delta A$ and the perturbed vector is $b' = b + \\delta b$. The corresponding solution is $x'_{\\lambda} = x_{\\lambda} + \\delta x$. The perturbed system must satisfy the same normal equations:\n$$((A + \\delta A)^T (A + \\delta A) + \\lambda^2 L^T L)(x_{\\lambda} + \\delta x) = (A + \\delta A)^T (b + \\delta b)$$\nWe expand both sides and ignore second-order terms, which are terms involving products of small quantities (e.g., $\\delta A^T \\delta A$, $\\delta A \\delta x$, $\\delta A^T \\delta b$).\n\nLeft-hand side (LHS) expansion:\n$$( (A^T + \\delta A^T)(A + \\delta A) + \\lambda^2 L^T L)(x_{\\lambda} + \\delta x)$$\n$$= (A^T A + A^T \\delta A + \\delta A^T A + \\delta A^T \\delta A + \\lambda^2 L^T L)(x_{\\lambda} + \\delta x)$$\nIgnoring the second-order term $\\delta A^T \\delta A$:\n$$\n\\approx (A^T A + \\lambda^2 L^T L + A^T \\delta A + \\delta A^T A)(x_{\\lambda} + \\delta x)\n$$\n$$= (H + A^T \\delta A + \\delta A^T A)(x_{\\lambda} + \\delta x)$$\n$$= H x_{\\lambda} + H \\delta x + (A^T \\delta A + \\delta A^T A)x_{\\lambda} + (A^T \\delta A + \\delta A^T A)\\delta x$$\nIgnoring the second-order term $(A^T \\delta A + \\delta A^T A)\\delta x$:\n$$ \\text{LHS} \\approx H x_{\\lambda} + H \\delta x + (A^T \\delta A + \\delta A^T A)x_{\\lambda} $$\n\nRight-hand side (RHS) expansion:\n$$(A^T + \\delta A^T)(b + \\delta b) = A^T b + A^T \\delta b + \\delta A^T b + \\delta A^T \\delta b$$\nIgnoring the second-order term $\\delta A^T \\delta b$:\n$$ \\text{RHS} \\approx A^T b + A^T \\delta b + \\delta A^T b $$\n\nEquating the first-order approximations of the LHS and RHS:\n$$H x_{\\lambda} + H \\delta x + (A^T \\delta A + \\delta A^T A)x_{\\lambda} = A^T b + A^T \\delta b + \\delta A^T b$$\nFrom the original unperturbed system, we know that $H x_{\\lambda} = A^T b$. These terms cancel out:\n$$H \\delta x + (A^T \\delta A + \\delta A^T A)x_{\\lambda} = A^T \\delta b + \\delta A^T b$$\nSolving for $\\delta x$, we get:\n$$H \\delta x = A^T \\delta b + \\delta A^T b - (A^T \\delta A + \\delta A^T A)x_{\\lambda}$$\n$$\\delta x = H^{-1} \\left( A^T \\delta b + \\delta A^T b - (A^T \\delta A + \\delta A^T A)x_{\\lambda} \\right)$$\n$$\\delta x = (A^T A + \\lambda^2 L^T L)^{-1} \\left( A^T \\delta b + \\delta A^T b - (A^T \\delta A + \\delta A^T A)x_{\\lambda} \\right)$$\nThis is the desired first-order approximation for the change $\\delta x$.\n\nNow, we evaluate this expression for the given data:\n$A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}$, $b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix}$, $L = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}$, $\\lambda = 1$, $\\delta A = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$, $\\delta b = \\begin{pmatrix} 0 \\\\ 3 \\\\ -3 \\end{pmatrix}$.\n\nFirst, we compute the unperturbed solution $x_{\\lambda}$. We need the matrix $H = A^T A + \\lambda^2 L^T L$.\n$A^T = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}$\n$A^T A = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$\n$L^T L = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix}$\nWith $\\lambda = 1$:\n$H = A^T A + L^T L = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} + \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 6 & 1 \\\\ 1 & 3 \\end{pmatrix}$\nThe RHS of the normal equation is $A^T b$:\n$A^T b = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$\nWe solve $H x_{\\lambda} = A^T b$. We need $H^{-1}$:\n$\\det(H) = (6)(3) - (1)(1) = 17$\n$H^{-1} = \\frac{1}{17} \\begin{pmatrix} 3 & -1 \\\\ -1 & 6 \\end{pmatrix}$\n$x_{\\lambda} = H^{-1} (A^T b) = \\frac{1}{17} \\begin{pmatrix} 3 & -1 \\\\ -1 & 6 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 3(1) - 1(2) \\\\ -1(1) + 6(2) \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 1 \\\\ 11 \\end{pmatrix}$\n\nNext, we compute the components of the expression for $\\delta x$.\nLet's compute the vector term $v = A^T \\delta b + \\delta A^T b - (A^T \\delta A + \\delta A^T A)x_{\\lambda}$.\n$A^T \\delta b = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ 0 \\end{pmatrix}$\n$\\delta A^T = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$\n$\\delta A^T b = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\nNow for the term involving $x_{\\lambda}$:\n$A^T \\delta A = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 1 \\end{pmatrix}$\n$\\delta A^T A = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 1 & 1 \\end{pmatrix}$\nLet $M = A^T \\delta A + \\delta A^T A = \\begin{pmatrix} 0 & 1 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 1 & 2 \\end{pmatrix}$\n$M x_{\\lambda} = \\begin{pmatrix} 0 & 1 \\\\ 1 & 2 \\end{pmatrix} \\frac{1}{17} \\begin{pmatrix} 1 \\\\ 11 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 0(1) + 1(11) \\\\ 1(1) + 2(11) \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 11 \\\\ 23 \\end{pmatrix}$\nNow we assemble $v$:\n$v = \\begin{pmatrix} -3 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\frac{1}{17} \\begin{pmatrix} 11 \\\\ 23 \\end{pmatrix} = \\begin{pmatrix} -3 - \\frac{11}{17} \\\\ -\\frac{23}{17} \\end{pmatrix} = \\begin{pmatrix} -\\frac{51}{17} - \\frac{11}{17} \\\\ -\\frac{23}{17} \\end{pmatrix} = \\begin{pmatrix} -\\frac{62}{17} \\\\ -\\frac{23}{17} \\end{pmatrix}$\n\nFinally, we compute $\\delta x = H^{-1} v$:\n$\\delta x = \\frac{1}{17} \\begin{pmatrix} 3 & -1 \\\\ -1 & 6 \\end{pmatrix} \\begin{pmatrix} -\\frac{62}{17} \\\\ -\\frac{23}{17} \\end{pmatrix} = \\frac{1}{17^2} \\begin{pmatrix} 3 & -1 \\\\ -1 & 6 \\end{pmatrix} \\begin{pmatrix} -62 \\\\ -23 \\end{pmatrix}$\n$\\delta x = \\frac{1}{289} \\begin{pmatrix} 3(-62) - 1(-23) \\\\ -1(-62) + 6(-23) \\end{pmatrix} = \\frac{1}{289} \\begin{pmatrix} -186 + 23 \\\\ 62 - 138 \\end{pmatrix} = \\frac{1}{289} \\begin{pmatrix} -163 \\\\ -76 \\end{pmatrix}$\n\nThe resulting change $\\delta x$ is the column vector $\\begin{pmatrix} -163/289 \\\\ -76/289 \\end{pmatrix}$. The problem requests the answer as a $1 \\times 2$ row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{163}{289} & -\\frac{76}{289}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Tikhonov regularization is one of several powerful techniques for ill-posed problems, and a practitioner must know how it compares to alternatives. This hands-on coding exercise pits Tikhonov regularization against another cornerstone method: Truncated Singular Value Decomposition (TSVD) . By implementing both and exploring their behavior on data with different spectral properties, you will gain direct insight into the 'filter functions' that underpin each method and learn to identify scenarios where one may be more effective than the other.",
            "id": "3599470",
            "problem": "Consider solving a linear inverse problem in the sense of numerical linear algebra: given a matrix $A \\in \\mathbb{R}^{n \\times n}$, an unknown vector $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$, deterministic data noise $\\eta \\in \\mathbb{R}^{n}$, and observed data $b = A x_{\\mathrm{true}} + \\eta$, compare two regularization strategies: Tikhonov regularization and truncated singular value decomposition. The goal is to compute the relative solution errors for both strategies over a set of test cases and to highlight a design where truncated singular value decomposition can outperform Tikhonov regularization due to spectral gaps.\n\nBase definitions and requirements:\n- For Tikhonov regularization with regularization parameter $\\lambda > 0$, the solution is the minimizer of the strictly convex objective $\\|A x - b\\|_{2}^{2} + \\lambda^2 \\|x\\|_{2}^{2}$, which is the unique solution to the normal equations $(A^{\\top} A + \\lambda^2 I) x_{\\mathrm{tik}} = A^{\\top} b$.\n- For truncated singular value decomposition (TSVD), define the singular value decomposition $A = U \\Sigma V^{\\top}$ with singular values $\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{n} \\ge 0$. For a truncation index $k \\in \\{0,1,\\dots,n\\}$, the TSVD solution is $x_{\\mathrm{tsvd}}^{(k)} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i}$. In this problem, the truncation index $k$ is determined by the rule $k = \\max\\{ i \\in \\{1,\\dots,n\\} : \\sigma_{i}^{2} \\ge \\lambda \\}$, with the convention that if the set is empty, then $k = 0$.\n- The relative error of a candidate solution $\\widehat{x}$ with respect to $x_{\\mathrm{true}}$ is $\\|\\widehat{x} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$.\n\nImplementation constraints and specialization for testability:\n- In all test cases, take $n = 10$ and choose $A$ to be diagonal with descending positive diagonal entries, so that $U = I$, $V = I$, and the singular values are the diagonal entries of $A$. This makes $A = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{n})$ and $A^{\\top} A = \\mathrm{diag}(\\sigma_{1}^{2},\\dots,\\sigma_{n}^{2})$.\n- For each test case, define the noise vector deterministically by $\\eta_{i} = \\mathrm{noise\\_level} \\cdot (-1)^{i}$ for $i = 1,2,\\dots,n$.\n- For Tikhonov, the explicit componentwise expression derived from the normal equations in this diagonal setting is $x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda^2} b_{i}$ for $i = 1,2,\\dots,n$.\n- For TSVD, given $k$ by the above selection rule, the explicit componentwise expression in this diagonal setting is $x_{\\mathrm{tsvd},i}^{(k)} = \\begin{cases} b_{i}/\\sigma_{i}, & i \\le k, \\\\ 0, & i > k. \\end{cases}$\n\nTest suite:\n- Test case $\\#1$ (spectral gap; designed to favor truncated singular value decomposition): \n  - $n = 10$.\n  - Singular values $\\sigma = [1.0, 0.6, 0.36, 0.216, 0.1296, 10^{-3}, 5 \\cdot 10^{-4}, 2 \\cdot 10^{-4}, 10^{-4}, 5 \\cdot 10^{-5}]$.\n  - True solution $x_{\\mathrm{true}} = [1, -\\tfrac{1}{2}, \\tfrac{1}{4}, -\\tfrac{1}{8}, \\tfrac{1}{16}, 0, 0, 0, 0, 0]$.\n  - Regularization parameter $\\lambda = 10^{-2.5}$ (which implies $\\lambda^2=10^{-5}$).\n  - Noise level $\\mathrm{noise\\_level} = 10^{-6}$.\n- Test case $\\#2$ (smooth spectrum; no pronounced gap):\n  - $n = 10$.\n  - Singular values $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$ for $i = 1,2,\\dots,10$.\n  - True solution $x_{\\mathrm{true},i} = 0.8^{i-1}$ for $i = 1,2,\\dots,10$.\n  - Regularization parameter $\\lambda = 10^{-2}$ (which implies $\\lambda^2=10^{-4}$).\n  - Noise level $\\mathrm{noise\\_level} = 10^{-6}$.\n- Test case $\\#3$ (boundary condition with very small regularization):\n  - $n = 10$.\n  - Singular values $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$ for $i = 1,2,\\dots,10$.\n  - True solution $x_{\\mathrm{true},i} = 0.8^{i-1}$ for $i = 1,2,\\dots,10$.\n  - Regularization parameter $\\lambda = 10^{-6}$ (which implies $\\lambda^2=10^{-12}$).\n  - Noise level $\\mathrm{noise\\_level} = 10^{-6}$.\n- Test case $\\#4$ (boundary condition with very large regularization):\n  - $n = 10$.\n  - Singular values $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$ for $i = 1,2,\\dots,10$.\n  - True solution $x_{\\mathrm{true},i} = 0.8^{i-1}$ for $i = 1,2,\\dots,10$.\n  - Regularization parameter $\\lambda = \\sqrt{10}$ (which implies $\\lambda^2=10^{1}$).\n  - Noise level $\\mathrm{noise\\_level} = 10^{-6}$.\n\nTasks to implement for each test case:\n- Construct $A = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{n})$, $x_{\\mathrm{true}}$, $\\eta$ with $\\eta_{i} = \\mathrm{noise\\_level} \\cdot (-1)^{i}$, and $b = A x_{\\mathrm{true}} + \\eta$.\n- Compute the Tikhonov solution $x_{\\mathrm{tik}}$ using $x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda^2} b_{i}$.\n- Compute the truncation index $k = \\max\\{ i : \\sigma_{i}^{2} \\ge \\lambda^2 \\}$ with the convention $k = 0$ if the set is empty.\n- Compute the truncated singular value decomposition solution $x_{\\mathrm{tsvd}}^{(k)}$ using $x_{\\mathrm{tsvd},i}^{(k)} = b_{i}/\\sigma_{i}$ for $i \\le k$ and $x_{\\mathrm{tsvd},i}^{(k)} = 0$ for $i > k$.\n- Compute the relative $2$-norm errors $e_{\\mathrm{tsvd}} = \\|x_{\\mathrm{tsvd}}^{(k)} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$ and $e_{\\mathrm{tik}} = \\|x_{\\mathrm{tik}} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$.\n- Also compute the boolean $b_{\\mathrm{adv}} = (e_{\\mathrm{tsvd}} < e_{\\mathrm{tik}})$ to indicate whether truncated singular value decomposition strictly outperforms Tikhonov in that test.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a list of the form $[k, e_{\\mathrm{tsvd}}, e_{\\mathrm{tik}}, b_{\\mathrm{adv}}]$. The final output should therefore be a list with four inner lists, one per test case, in order of test cases $\\#1$ through $\\#4$, for example, $[[k_{1}, e_{\\mathrm{tsvd},1}, e_{\\mathrm{tik},1}, b_{\\mathrm{adv},1}], [k_{2}, e_{\\mathrm{tsvd},2}, e_{\\mathrm{tik},2}, b_{\\mathrm{adv},2}], [k_{3}, e_{\\mathrm{tsvd},3}, e_{\\mathrm{tik},3}, b_{\\mathrm{adv},3}], [k_{4}, e_{\\mathrm{tsvd},4}, e_{\\mathrm{tik},4}, b_{\\mathrm{adv},4}]]$.\n\nNotes:\n- The problem description has been edited to use $\\lambda^2$ as the regularization coefficient to be consistent with the rest of the article. The numerical values of the coefficient are unchanged. The parameter name in the Python code will be changed from `lam` to `lam_sq` for clarity.\n- There are no physical units; all computations are in dimensionless floating-point arithmetic.\n- Angles are not used.\n- Express all booleans as language-native boolean values and all errors as floating-point numbers.",
            "solution": "The problem presented requires the comparison of two standard regularization techniques for solving ill-posed linear inverse problems: Tikhonov regularization and Truncated Singular Value Decomposition (TSVD). The validation confirms that the problem is scientifically sound, well-posed, and provides a clear, self-contained set of instructions and data for a numerical experiment. All definitions and formulas are consistent with the established literature in numerical linear algebra. We may therefore proceed with a solution.\n\nThe core of the problem lies in solving the linear system $A x = b$ where the matrix $A$ is ill-conditioned and the data vector $b$ is corrupted by noise $\\eta$. The model is given by $b = A x_{\\mathrm{true}} + \\eta$, where $x_{\\mathrm{true}}$ is the ground truth solution we seek to approximate. A naive attempt to solve for $x$ via $x = A^{-1} b$ would result in $x = x_{\\mathrm{true}} + A^{-1} \\eta$. Since $A$ is ill-conditioned, its inverse $A^{-1}$ has a very large norm, leading to extreme amplification of the noise term $\\eta$. Regularization methods are designed to counteract this by introducing a controlled bias to the solution in exchange for a dramatic reduction in variance due to noise.\n\nThe problem simplifies the analysis by considering a diagonal matrix $A = \\mathrm{diag}(\\sigma_{1}, \\dots, \\sigma_{n})$, where the diagonal entries $\\sigma_i > 0$ are the singular values of $A$. In the general case, any matrix $A$ has a Singular Value Decomposition (SVD) $A = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthogonal matrices and $\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$. The choice of a diagonal $A$ is equivalent to working in a basis where the SVD is trivial ($U=V=I$), allowing us to focus on how the singular values themselves are handled by each regularization method.\n\n**Tikhonov Regularization**\n\nTikhonov regularization recasts the problem as an optimization problem, seeking a solution $x$ that minimizes a combination of the data fidelity term and a penalty on the solution norm:\n$$ x_{\\mathrm{tik}} = \\arg\\min_{x} \\left( \\|A x - b\\|_{2}^{2} + \\lambda^2 \\|x\\|_{2}^{2} \\right) $$\nThe parameter $\\lambda > 0$ controls the trade-off. The unique minimizer $x_{\\mathrm{tik}}$ is found by solving the associated normal equations: $(A^{\\top} A + \\lambda^2 I) x_{\\mathrm{tik}} = A^{\\top} b$. For our diagonal matrix $A=\\mathrm{diag}(\\sigma_i)$, this system decouples into $n$ scalar equations:\n$$ (\\sigma_{i}^{2} + \\lambda^2) x_{\\mathrm{tik},i} = \\sigma_{i} b_{i} \\quad \\text{for } i \\in \\{1, \\dots, n\\} $$\nThis gives the explicit component-wise formula for the Tikhonov solution:\n$$ x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda^2} b_{i} $$\nThe term $f_{i}^{\\mathrm{tik}} = \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^2}$ can be viewed as a \"filter factor\". It smoothly attenuates components associated with small singular values. If $\\sigma_i^2 \\gg \\lambda^2$, then $f_{i}^{\\mathrm{tik}} \\approx 1$, and the component is largely unchanged. If $\\sigma_i^2 \\ll \\lambda^2$, then $f_{i}^{\\mathrm{tik}} \\approx 0$, and the component is suppressed.\n\n**Truncated Singular Value Decomposition (TSVD)**\n\nTSVD takes a more direct approach by constructing the solution using only the \"significant\" singular components. The general TSVD solution is given by a truncated sum:\n$$ x_{\\mathrm{tsvd}}^{(k)} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i} $$\nwhere $k$ is the truncation index, determining how many components are included. In our diagonal case ($U=I, V=I$), this simplifies to:\n$$ x_{\\mathrm{tsvd},i}^{(k)} = \\begin{cases} b_{i}/\\sigma_{i} & \\text{if } i \\le k \\\\ 0 & \\text{if } i > k \\end{cases} $$\nThis corresponds to filter factors $f_{i}^{\\mathrm{tsvd}}$ that form a step function: $f_{i}^{\\mathrm{tsvd}} = 1$ for $i \\le k$ and $f_{i}^{\\mathrm{tsvd}} = 0$ for $i > k$. The problem links the choice of $k$ to the Tikhonov parameter $\\lambda^2$ via the rule $k = \\max\\{ i \\in \\{1,\\dots,n\\} : \\sigma_{i}^{2} \\ge \\lambda^2 \\}$, with $k=0$ if the set is empty. This rule essentially keeps all components for which the Tikhonov filter factor would be at least $1/2$.\n\n**Comparison and the Role of Spectral Gaps**\n\nThe fundamental difference lies in their filter functions: Tikhonov's is smooth, while TSVD's is sharp. Test case $\\#1$ is specifically designed to highlight a scenario where TSVD's sharp cutoff is advantageous. It features a \"spectral gap,\" where the singular values show a large drop between $\\sigma_5$ and $\\sigma_6$. The regularization parameter $\\lambda^2=10^{-5}$ is chosen to lie within this gap (i.e., $\\sigma_5^2 \\gg \\lambda^2 \\gg \\sigma_6^2$). Furthermore, the true solution $x_{\\mathrm{true}}$ has its information content restricted to the first $5$ components.\n\nUnder these conditions, the truncation rule for TSVD yields $k=5$. TSVD therefore retains the first $5$ components (where the signal lies) and completely discards the remaining components (which contain only noise, as $x_{\\mathrm{true},i}=0$ for $i>5$). This acts as a perfect filter for this specific problem structure. In contrast, Tikhonov regularization applies its smooth filter to all components. While it heavily suppresses components $i > 5$, it still allows a small, filtered amount of noise to pass through. More importantly, it also slightly damps components $i \\le 5$, introducing a regularization error that TSVD does not have forthese components. This leads to TSVD outperforming Tikhonov.\n\nFor the other test cases with smoother spectral decay, the sharp cutoff of TSVD can be detrimental. If $x_{\\mathrm{true}}$ contains significant energy in components that TSVD truncates (because their $\\sigma_i$ are small), it will incur a large regularization error. Tikhonov's gentle damping of these components can result in a better overall approximation.\n\n**Computational Steps**\n\nFor each of the four test cases, the following procedure is implemented:\n1.  Initialize the parameters: $n=10$, singular values $\\sigma$, true solution $x_{\\mathrm{true}}$, regularization coefficient $\\lambda^2$, and noise level.\n2.  Construct the noise vector $\\eta$ where $\\eta_{j} = \\mathrm{noise\\_level} \\cdot (-1)^{j+1}$ for the $0$-based index $j \\in \\{0, \\dots, 9\\}$.\n3.  Compute the data vector $b = \\sigma \\odot x_{\\mathrm{true}} + \\eta$, where $\\odot$ denotes element-wise multiplication.\n4.  Calculate the Tikhonov solution vector $x_{\\mathrm{tik}}$ using its component-wise formula.\n5.  Determine the TSVD truncation index $k$ based on the provided rule.\n6.  Calculate the TSVD solution vector $x_{\\mathrm{tsvd}}^{(k)}$.\n7.  Compute the relative $2$-norm errors for both solutions: $e_{\\mathrm{tik}} = \\|x_{\\mathrm{tik}} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$ and $e_{\\mathrm{tsvd}} = \\|x_{\\mathrm{tsvd}}^{(k)} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$.\n8.  Evaluate the boolean condition $b_{\\mathrm{adv}} = (e_{\\mathrm{tsvd}} < e_{\\mathrm{tik}})$.\nThe collected results $[k, e_{\\mathrm{tsvd}}, e_{\\mathrm{tik}}, b_{\\mathrm{adv}}]$ for each case are then reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_test_case(sigma_vals, xtrue_vals, lam_sq, noise_level):\n    \"\"\"\n    Runs a single test case for comparing Tikhonov and TSVD regularization.\n    \"\"\"\n    n = 10\n    sigma = np.array(sigma_vals, dtype=float)\n    xtrue = np.array(xtrue_vals, dtype=float)\n\n    # Construct the noise vector eta and observed data b\n    # The problem uses 1-based indexing i=1,...,n. Python uses 0-based j=0,...,n-1.\n    # eta_i = noise_level * (-1)^i translates to eta[j] = noise_level * (-1)**(j+1)\n    indices_1_based = np.arange(1, n + 1)\n    eta = noise_level * ((-1) ** indices_1_based)\n    b = sigma * xtrue + eta\n\n    # Compute the Tikhonov solution\n    xtik = (sigma / (sigma**2 + lam_sq)) * b\n\n    # Determine the TSVD truncation index k\n    # k = max{ i in {1..n} : sigma_i^2 >= lam_sq }\n    # np.where returns 0-based indices. k needs to be a 1-based count.\n    valid_indices = np.where(sigma**2 >= lam_sq)[0]\n    if len(valid_indices) == 0:\n        k = 0\n    else:\n        k = int(np.max(valid_indices) + 1)\n\n    # Compute the TSVD solution\n    xtsvd = np.zeros(n)\n    if k > 0:\n        # Slicing with :k works correctly for 0-based index up to k-1.\n        xtsvd[:k] = b[:k] / sigma[:k]\n\n    # Compute the relative 2-norm errors\n    norm_xtrue = np.linalg.norm(xtrue)\n    \n    # This problem guarantees norm_xtrue > 0, so no division-by-zero check is needed.\n    e_tsvd = np.linalg.norm(xtsvd - xtrue) / norm_xtrue\n    e_tik = np.linalg.norm(xtik - xtrue) / norm_xtrue\n\n    # Determine if TSVD has a strictly smaller error\n    b_adv = bool(e_tsvd < e_tik)\n    \n    return [k, e_tsvd, e_tik, b_adv]\n\ndef solve():\n    \"\"\"\n    Defines and runs the four test cases, then prints the results.\n    \"\"\"\n    # Test case #1: Spectral gap\n    case1 = {\n        \"sigma_vals\": [1.0, 0.6, 0.36, 0.216, 0.1296, 1e-3, 5e-4, 2e-4, 1e-4, 5e-5],\n        \"xtrue_vals\": [1.0, -0.5, 0.25, -0.125, 0.0625, 0, 0, 0, 0, 0],\n        \"lam_sq\": 1e-5,\n        \"noise_level\": 1e-6\n    }\n    \n    # Test case #2: Smooth spectrum\n    n = 10\n    j_indices = np.arange(n)\n    sigma_smooth = 10**(-j_indices / 3.0)\n    xtrue_smooth = 0.8**j_indices\n    case2 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam_sq\": 1e-4,\n        \"noise_level\": 1e-6\n    }\n    \n    # Test case #3: Small regularization parameter\n    case3 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam_sq\": 1e-12,\n        \"noise_level\": 1e-6\n    }\n\n    # Test case #4: Large regularization parameter\n    case4 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam_sq\": 1e1,\n        \"noise_level\": 1e-6\n    }\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n    for case in test_cases:\n        result = run_test_case(\n            case[\"sigma_vals\"],\n            case[\"xtrue_vals\"],\n            case[\"lam_sq\"],\n            case[\"noise_level\"]\n        )\n        results.append(result)\n\n    # Print in the specified format: [[k1, e_tsvd1, e_tik1, b_adv1], [k2, ...]]\n    # Python's default string representation for a list of lists matches the required format.\n    print(results)\n\nsolve()\n```"
        },
        {
            "introduction": "The true power of Tikhonov regularization is unlocked by moving beyond the standard identity operator $L=I$ and choosing an $L$ that reflects prior knowledge about the solution's structure. This choice, however, introduces an assumption, and a mismatch between the assumption and reality leads to 'model bias'. This exercise  provides a striking demonstration of this effect by applying a standard norm penalty to a signal that is known to be sparse in a wavelet basis, allowing you to quantify the resulting error and appreciate the importance of selecting an appropriate regularizer.",
            "id": "3599516",
            "problem": "Consider the Tikhonov regularization problem in numerical linear algebra. Given a matrix $A \\in \\mathbb{R}^{n \\times n}$, a data vector $y \\in \\mathbb{R}^{n}$, and a regularization operator $L \\in \\mathbb{R}^{n \\times n}$, the Tikhonov estimator $\\hat{x}_{\\alpha}$ with parameter $\\alpha > 0$ is defined as the unique minimizer of the objective function\n$$\nJ_{\\alpha}(x) = \\|A x - y\\|_{2}^{2} + \\alpha^{2} \\|L x\\|_{2}^{2}.\n$$\nYou will investigate how a mismatch between the true regularity of the unknown vector $x$ and the chosen regularizer $L$ induces model bias. Construct a synthetic case where the ground-truth $x_{\\mathrm{true}}$ is sparse in a wavelet basis, but Tikhonov regularization is applied with $L = I$ (the identity operator), and quantify the induced misfit.\n\nYou must implement the following specifications exactly and compute the required outputs.\n\n1. Signal dimension and wavelet basis.\n   - Let $n = 64$ and consider the orthonormal Haar wavelet basis on $\\mathbb{R}^{n}$.\n   - Use the following canonical coefficient ordering for the forward Haar transform $W$: the vector of wavelet coefficients is ordered as $[a_{L}, d_{L}, d_{L-1}, \\dots, d_{1}]$ where $L = \\log_{2}(n)$, $a_{L} \\in \\mathbb{R}$ is the final coarse approximation coefficient, and $d_{j} \\in \\mathbb{R}^{2^{L-j}}$ is the detail block at scale $j$ for $j = L, L-1, \\dots, 1$.\n   - The inverse Haar transform $W^{-1}$ reconstructs a signal from this ordering.\n\n2. Ground-truth construction (sparse in the wavelet basis).\n   - Let $w_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ be all zeros except at exactly five entries located within detail blocks as follows (using the ordering above with $L = 6$ since $n = 64$):\n     - In $d_{6}$ at position $0$ set the value $+1.0$.\n     - In $d_{5}$ at position $1$ set the value $-0.8$.\n     - In $d_{3}$ at position $3$ set the value $+0.5$.\n     - In $d_{2}$ at position $5$ set the value $-1.1$.\n     - In $d_{1}$ at position $17$ set the value $+0.9$.\n   - All other coefficients, including $a_{6}$, are zero.\n   - Define $x_{\\mathrm{true}} = W^{-1} w_{\\mathrm{true}}$.\n\n3. Forward operator $A$ and data $y$.\n   - Consider two forward operators:\n     - Case $\\mathrm{blur}$: $A$ is a circulant convolution matrix generated by a discrete Gaussian kernel $g \\in \\mathbb{R}^{n}$ of odd truncation length $9$ with entries\n       $$\n       g_{k} = \\frac{\\exp\\!\\left(-\\frac{(k-0)^{2}}{2 \\sigma^{2}}\\right)}{\\sum_{j=-4}^{4} \\exp\\!\\left(-\\frac{j^{2}}{2 \\sigma^{2}}\\right)}, \\quad k \\in \\{-4,-3,\\dots,4\\},\n       $$\n       embedded periodically into length $n$ by placing these $9$ weights centered and zero elsewhere, with wrap-around defining the circulant structure. Use $\\sigma = 2.0$.\n     - Case $\\mathrm{id}$: $A = I$ (the identity).\n   - In all cases, generate noise-free data $y = A x_{\\mathrm{true}}$.\n\n4. Regularization and estimator.\n   - Fix $L = I$ and, for each test case, compute the Tikhonov estimator $\\hat{x}_{\\alpha}$ that minimizes $J_{\\alpha}(x)$ for the given $A$ and $y$.\n   - The setting is noise-free; the focus is on the deterministic model bias induced by the mismatch between the wavelet-sparse truth and the isotropic quadratic penalty.\n\n5. Misfit quantification metrics.\n   For each test case, compute the following two scalar quantities:\n   - Normalized squared model bias in the signal domain:\n     $$\n     b = \\frac{\\|\\hat{x}_{\\alpha} - x_{\\mathrm{true}}\\|_{2}^{2}}{\\|x_{\\mathrm{true}}\\|_{2}^{2}}.\n     $$\n   - Wavelet leakage ratio that quantifies energy placed on coefficients that are zero in $w_{\\mathrm{true}}$:\n     $$\n     \\ell = \\frac{\\| (W \\hat{x}_{\\alpha})_{S^{c}} \\|_{2}^{2}}{\\| w_{\\mathrm{true}} \\|_{2}^{2}},\n     $$\n     where $S$ is the support (index set) of the nonzero entries of $w_{\\mathrm{true}}$, and $S^{c}$ is its complement. The numerator uses the forward Haar transform of the reconstruction $\\hat{x}_{\\alpha}$.\n\n6. Test suite.\n   Use the following four test cases, each specified by $(A\\text{-type}, \\alpha)$:\n   - Test $1$: $(\\mathrm{blur}, 10^{-1})$.\n   - Test $2$: $(\\mathrm{blur}, 1)$.\n   - Test $3$: $(\\mathrm{blur}, 10^{-3})$.\n   - Test $4$: $(\\mathrm{id}, 3 \\times 10^{-1})$.\n\n7. Program output format.\n   - Your program must produce a single line of output containing the results aggregated as a list of lists. For each test case (in the order $1, 2, 3, 4$), output the pair $[b, \\ell]$ as floating-point numbers.\n   - The final output must therefore be a single line in the format\n     $$\n     [[b_{1}, \\ell_{1}], [b_{2}, \\ell_{2}], [b_{3}, \\ell_{3}], [b_{4}, \\ell_{4}]].\n     $$\n\nConstraints and guidance:\n- Use a correct implementation of the orthonormal Haar transform consistent with the ordering specified.\n- Construct $A$ explicitly as a dense matrix in both cases.\n- Solve for $\\hat{x}_{\\alpha}$ by minimizing $J_{\\alpha}(x)$ without using any iterative optimization; a direct linear algebra approach is expected.\n- No physical units are involved.\n- All randomization is forbidden; results must be deterministic.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in numerical linear algebra and signal processing, well-posed with all necessary parameters defined, and objective in its formulation. It requests a quantitative analysis of model bias in Tikhonov regularization, which is a standard topic. Therefore, a reasoned solution is provided.\n\nThe core of the problem is to compute the Tikhonov-regularized estimate $\\hat{x}_{\\alpha}$ for a given linear system and then to quantify the error, or model bias, with respect to a known ground truth $x_{\\mathrm{true}}$. The bias arises because the regularization term, $\\alpha^2 \\|L x\\|_2^2$, imposes a penalty that biases the solution towards vectors $x$ for which $\\|L x\\|_2^2$ is small. In this problem, the regularizer is $L=I$ (the identity), which favors solutions with small Euclidean norm. However, the true signal $x_{\\mathrm{true}}$ is constructed to be sparse in a wavelet basis, not to have an intrinsically small norm. This mismatch between the nature of the true signal and the assumption implicit in the regularizer is the source of the model bias we are tasked to investigate.\n\nThe solution proceeds through the following steps for each test case:\n1.  Construct the necessary mathematical objects: the orthonormal Haar wavelet transform matrix $W$, the ground-truth signal $x_{\\mathrm{true}}$, and the forward operator $A$.\n2.  Compute the Tikhonov estimator $\\hat{x}_{\\alpha}$ by solving the corresponding normal equations.\n3.  Calculate the two specified error metrics: the normalized squared model bias $b$ and the wavelet leakage ratio $\\ell$.\n\n### Step 1: Construction of Mathematical Objects\n\n**1.1. Orthonormal Haar Wavelet Transform Matrix**\nThe problem specifies an orthonormal Haar wavelet transform for signals of dimension $n=64$. The dimension $n=2^L$ with $L=6$ is a power of two, as required. The wavelet coefficients are ordered as $[a_L, d_L, d_{L-1}, \\dots, d_1]$, where $a_L$ is the final approximation coefficient (length $1$) and $d_j$ are the detail coefficients at scale $j$ (length $2^{L-j}$). This is a \"non-standard\" or unpacked ordering that can be generated by a recursive algorithm. The forward transform $w = Wx$ maps a signal $x \\in \\mathbb{R}^n$ to its wavelet coefficients $w \\in \\mathbb{R}^n$. The inverse transform is $x = W^{-1}w$. Since the basis is orthonormal, the transform matrix $W$ is orthogonal, meaning $W^{-1} = W^T$.\n\nThe matrix $W$ can be constructed by recognizing that its rows are the Haar basis vectors. Equivalently, the columns of its transpose $W^T$ are the basis vectors. The $i$-th basis vector is the signal that corresponds to a single non-zero coefficient at position $i$ in the wavelet domain. We can therefore construct $W^T$ by applying the inverse Haar transform to each of the canonical basis vectors $e_i \\in \\mathbb{R}^n$. The matrix $W$ is then obtained by transposing the result.\n\n**1.2. Ground-Truth Signal $x_{\\mathrm{true}}$**\nThe ground-truth signal $x_{\\mathrm{true}}$ is defined by its wavelet coefficients $w_{\\mathrm{true}}$. The vector $w_{\\mathrm{true}} \\in \\mathbb{R}^{64}$ is sparse, with five non-zero entries at specified locations within the detail blocks:\n- The coefficient vector is structured as $[a_6 (1), d_6 (1), d_5 (2), d_4 (4), d_3 (8), d_2 (16), d_1 (32)]$, where the numbers in parentheses are the lengths of the blocks.\n- The indices for these blocks are: $a_6: [0]$, $d_6: [1]$, $d_5: [2,3]$, $d_4: [4-7]$, $d_3: [8-15]$, $d_2: [16-31]$, $d_1: [32-63]$.\n- The non-zero values are set as:\n  - $w_{\\mathrm{true}}[1] = +1.0$ (in $d_6$)\n  - $w_{\\mathrm{true}}[3] = -0.8$ (in $d_5$)\n  - $w_{\\mathrm{true}}[11] = +0.5$ (in $d_3$)\n  - $w_{\\mathrm{true}}[21] = -1.1$ (in $d_2$)\n  - $w_{\\mathrm{true}}[49] = +0.9$ (in $d_1$)\nThe signal is then synthesized using the inverse Haar transform: $x_{\\mathrm{true}} = W^T w_{\\mathrm{true}}$.\n\n**1.3. Forward Operator $A$ and Data $y$**\nTwo types of forward operators are considered:\n- **Case `id`**: $A=I_{64}$, the $64 \\times 64$ identity matrix.\n- **Case `blur`**: $A$ is a circulant convolution matrix. The generating kernel is a discrete Gaussian of length $9$ with standard deviation $\\sigma=2.0$. The kernel weights $g_k$ for $k \\in \\{-4, \\dots, 4\\}$ are normalized to sum to $1$. This kernel is embedded into a periodic vector $c \\in \\mathbb{R}^{64}$ which becomes the first column of the circulant matrix $A$. Specifically, $c_0=g_0$, $c_k = g_k$ for $k \\in \\{1,2,3,4\\}$, and $c_{n+k} = g_k$ for $k \\in \\{-1,-2,-3,-4\\}$.\nThe noise-free data vector is generated as $y = A x_{\\mathrm{true}}$.\n\n### Step 2: Tikhonov Estimator\nThe Tikhonov estimator $\\hat{x}_{\\alpha}$ is the unique vector $x$ that minimizes the objective function $J_{\\alpha}(x) = \\|A x - y\\|_{2}^{2} + \\alpha^{2} \\|I x\\|_{2}^{2}$. The minimizer is found by setting the gradient $\\nabla_x J_{\\alpha}(x)$ to zero, which yields the normal equations:\n$$\n(A^T A + \\alpha^2 I) x = A^T y.\n$$\nThis is a linear system of equations, which can be solved for $\\hat{x}_{\\alpha}$:\n$$\n\\hat{x}_{\\alpha} = (A^T A + \\alpha^2 I)^{-1} A^T y.\n$$\nThis system is solved using a direct numerical linear algebra solver for robustness and to meet problem specifications.\n\n### Step 3: Misfit Quantification\nThe model bias is quantified using two metrics:\n\n**3.1. Normalized Squared Model Bias ($b$)**\nThis metric measures the squared error in the signal domain, normalized by the energy of the true signal:\n$$\nb = \\frac{\\|\\hat{x}_{\\alpha} - x_{\\mathrm{true}}\\|_{2}^{2}}{\\|x_{\\mathrm{true}}\\|_{2}^{2}}.\n$$\n\n**3.2. Wavelet Leakage Ratio ($\\ell$)**\nThis metric quantifies how much energy the regularized solution \"leaks\" into wavelet coefficients that should be zero. Let $S$ be the index set of the five non-zero entries in $w_{\\mathrm{true}}$, and $S^c$ be its complement. The estimated wavelet coefficients are $\\hat{w}_{\\alpha} = W \\hat{x}_{\\alpha}$. The leakage ratio is:\n$$\n\\ell = \\frac{\\| (\\hat{w}_{\\alpha})_{S^{c}} \\|_{2}^{2}}{\\| w_{\\mathrm{true}} \\|_{2}^{2}}.\n$$\nThe denominator $\\|w_{\\mathrm{true}}\\|_{2}^{2}$ is equal to $\\|x_{\\mathrm{true}}\\|_{2}^{2}$ because $W$ is an orthogonal matrix. The numerator is the squared norm of the components of $\\hat{w}_{\\alpha}$ at indices where $w_{\\mathrm{true}}$ is zero.\n\nThese calculations are performed for each of the four test cases specified by the pair $(A\\text{-type}, \\alpha)$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import circulant\n\ndef solve():\n    \"\"\"\n    Main function to solve the Tikhonov regularization problem and compute misfit metrics.\n    \"\"\"\n\n    def _inverse_haar_recursive(w: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Recursively computes the inverse Haar transform for a 1D vector.\n        Assumes the 'unpacked' coefficient ordering [a_L, d_L, d_{L-1}, ...].\n        \"\"\"\n        n = len(w)\n        if n == 1:\n            return w\n        \n        m = n // 2\n        w_approx_coeffs = w[:m]\n        w_detail = w[m:]\n        \n        x_approx_signal = _inverse_haar_recursive(w_approx_coeffs)\n        \n        x = np.zeros(n)\n        # Reconstruct from averages and differences\n        x[0::2] = (x_approx_signal + w_detail) / np.sqrt(2.0)\n        x[1::2] = (x_approx_signal - w_detail) / np.sqrt(2.0)\n        \n        return x\n\n    def get_haar_matrix(n: int) -> np.ndarray:\n        \"\"\"\n        Constructs the n x n orthonormal Haar wavelet transform matrix W.\n        The transform is defined as w = W @ x.\n        The rows of W are the wavelet basis vectors.\n        \"\"\"\n        if np.log2(n) % 1 != 0:\n            raise ValueError(\"n must be a power of 2.\")\n            \n        # W_T has the basis vectors as columns. Each basis vector is the synthesis\n        # of a canonical vector in the wavelet domain.\n        W_T = np.zeros((n, n))\n        I = np.identity(n)\n        for i in range(n):\n            W_T[:, i] = _inverse_haar_recursive(I[:, i])\n        \n        # W is the transpose of W_T, so its rows are the basis vectors.\n        W = W_T.T\n        return W\n\n    def construct_ground_truth(n: int, W: np.ndarray):\n        \"\"\"Constructs the ground-truth wavelet coefficients and signal.\"\"\"\n        w_true = np.zeros(n)\n        \n        # As per problem spec, n=64, L=6.\n        # Block structure: a6(1), d6(1), d5(2), d4(4), d3(8), d2(16), d1(32).\n        # Offsets: d6->1, d5->2, d4->4, d3->8, d2->16, d1->32.\n        \n        # In d6 at position 0\n        w_true[1] = 1.0\n        # In d5 at position 1\n        w_true[2 + 1] = -0.8\n        # In d3 at position 3\n        w_true[8 + 3] = 0.5\n        # In d2 at position 5\n        w_true[16 + 5] = -1.1\n        # In d1 at position 17\n        w_true[32 + 17] = 0.9\n        \n        # Synthesize the signal x_true = W^{-1} w_true = W^T w_true\n        x_true = W.T @ w_true\n        \n        return w_true, x_true\n\n    def construct_forward_operator(A_type: str, n: int):\n        \"\"\"Constructs the forward operator A.\"\"\"\n        if A_type == 'id':\n            return np.identity(n)\n        elif A_type == 'blur':\n            sigma = 2.0\n            kernel_len = 9\n            k = np.arange(-(kernel_len//2), (kernel_len//2) + 1)\n            \n            g_unnorm = np.exp(-k**2 / (2.0 * sigma**2))\n            g = g_unnorm / np.sum(g_unnorm)\n            \n            # Embed the kernel into the first column of the circulant matrix\n            c = np.zeros(n)\n            center_idx = kernel_len // 2\n            \n            # Positive shifts\n            c[0:center_idx + 1] = g[center_idx:]\n            # Negative shifts (wrap around)\n            c[n-center_idx:] = g[:center_idx]\n            \n            return circulant(c)\n        else:\n            raise ValueError(f\"Unknown A_type: {A_type}\")\n\n    # ===== Main execution logic =====\n    \n    n = 64\n    \n    # 1. Construct universal components\n    W = get_haar_matrix(n)\n    w_true, x_true = construct_ground_truth(n, W)\n    \n    x_true_norm_sq = np.linalg.norm(x_true)**2\n    w_true_norm_sq = np.linalg.norm(w_true)**2 # Should be equal to x_true_norm_sq\n    \n    support_mask = (w_true != 0)\n    complement_mask = ~support_mask\n    \n    # 2. Define test suite\n    test_cases = [\n        ('blur', 1e-1),\n        ('blur', 1.0),\n        ('blur', 1e-3),\n        ('id', 3e-1),\n    ]\n\n    results = []\n    \n    # 3. Iterate through test cases\n    for A_type, alpha in test_cases:\n        # Construct the forward operator\n        A = construct_forward_operator(A_type, n)\n        \n        # Generate the data\n        y = A @ x_true\n        \n        # Compute the Tikhonov estimator\n        # Solve (A^T A + alpha^2 I) x = A^T y\n        M = A.T @ A + (alpha**2) * np.identity(n)\n        rhs = A.T @ y\n        x_hat = np.linalg.solve(M, rhs)\n        \n        # Compute misfit metrics\n        # Metric b: Normalized squared model bias\n        bias_sq_norm = np.linalg.norm(x_hat - x_true)**2\n        b = bias_sq_norm / x_true_norm_sq\n        \n        # Metric l: Wavelet leakage ratio\n        w_hat = W @ x_hat\n        leakage_num = np.linalg.norm(w_hat[complement_mask])**2\n        l = leakage_num / w_true_norm_sq\n        \n        results.append([b, l])\n        \n    # 4. Format and print the final output\n    print(f\"{results}\")\n\n\nsolve()\n```"
        }
    ]
}