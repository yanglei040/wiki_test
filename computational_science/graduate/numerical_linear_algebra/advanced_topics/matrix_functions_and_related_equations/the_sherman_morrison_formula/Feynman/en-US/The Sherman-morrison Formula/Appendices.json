{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds our understanding by deriving the Sherman-Morrison formula for solving linear systems from fundamental principles. By algebraically manipulating the system $(A + u v^{T}) x = b$, you will see how the solution can be expressed using the inverse of the original matrix $A$, without needing to compute the inverse of the updated matrix. This exercise  is essential for revealing the structure of the solution and the computational savings the formula offers.",
            "id": "3596896",
            "problem": "Let $n$ be a positive integer. Let $A \\in \\mathbb{R}^{n \\times n}$ be an invertible matrix, and let $u,v \\in \\mathbb{R}^{n}$ and $b \\in \\mathbb{R}^{n}$. Consider the linear system $(A + u v^{T}) x = b$. Assume that $1 + v^{T} A^{-1} u \\neq 0$ so that the rank-one update $A + u v^{T}$ is invertible. Starting only from the definition of a matrix inverse and the basic properties of matrix and vector operations (linearity, associativity, and distributivity), derive an explicit analytic expression for the unique solution $x \\in \\mathbb{R}^{n}$ in terms of $A^{-1} b$, $A^{-1} u$, $v^{T} A^{-1} b$, and $v^{T} A^{-1} u$. Your final answer must be a single closed-form symbolic expression for $x$. No numerical approximation is required, and no rounding is needed.",
            "solution": "We are tasked with finding the solution $x \\in \\mathbb{R}^{n}$ to the linear system:\n$$ (A + u v^{T}) x = b $$\nHere, $A \\in \\mathbb{R}^{n \\times n}$ is an invertible matrix, and $u, v, b \\in \\mathbb{R}^{n}$ are column vectors. The term $u v^{T}$ represents the outer product of $u$ and $v$, which is an $n \\times n$ matrix of rank one.\n\nWe begin by applying the distributive property of matrix multiplication to the left-hand side of the equation:\n$$ Ax + (u v^{T}) x = b $$\nUsing the associativity of matrix-vector multiplication, the term $(u v^{T}) x$ can be regrouped. Note that $v^{T}x$ is the inner product of vectors $v$ and $x$, which results in a scalar value. Let's denote this scalar as $\\alpha$.\n$$ \\alpha = v^{T}x $$\nThe equation can then be written as:\n$$ Ax + u(v^{T}x) = Ax + u \\alpha = b $$\nOur goal is to isolate the vector $x$. We can rearrange the equation to place all terms involving $x$ on one side, but a more direct path is to first isolate the term $Ax$:\n$$ Ax = b - u \\alpha $$\nSince the matrix $A$ is invertible, its inverse, $A^{-1}$, exists. We can pre-multiply both sides of the equation by $A^{-1}$:\n$$ A^{-1}(Ax) = A^{-1}(b - u \\alpha) $$\nUsing the associative property on the left and the distributive property on the right, we get:\n$$ (A^{-1}A)x = A^{-1}b - A^{-1}(u \\alpha) $$\nBy definition of the inverse, $A^{-1}A = I$, where $I$ is the $n \\times n$ identity matrix. Also, since $\\alpha$ is a scalar, it can be factored out of the matrix-vector product: $A^{-1}(u \\alpha) = (A^{-1}u)\\alpha$. The equation simplifies to:\n$$ Ix = A^{-1}b - (A^{-1}u)\\alpha $$\n$$ x = A^{-1}b - (A^{-1}u)\\alpha $$\nThis expression gives $x$ in terms of the known quantities $A^{-1}b$ and $A^{-1}u$, but it still depends on the unknown scalar $\\alpha$. To find $\\alpha$, we use its definition, $\\alpha = v^{T}x$, and substitute the expression for $x$ we just derived:\n$$ \\alpha = v^{T} \\left( A^{-1}b - (A^{-1}u)\\alpha \\right) $$\nApplying the distributive property for the transpose-vector product:\n$$ \\alpha = v^{T}(A^{-1}b) - v^{T}((A^{-1}u)\\alpha) $$\nAgain, since $\\alpha$ is a scalar, we can factor it out of the expression on the right:\n$$ \\alpha = v^{T}A^{-1}b - (v^{T}A^{-1}u)\\alpha $$\nThis is a linear equation in the single variable $\\alpha$. The terms $v^{T}A^{-1}b$ and $v^{T}A^{-1}u$ are both scalars. We can now solve for $\\alpha$. First, we gather all terms involving $\\alpha$ on one side of the equation:\n$$ \\alpha + (v^{T}A^{-1}u)\\alpha = v^{T}A^{-1}b $$\nFactoring out $\\alpha$:\n$$ \\alpha (1 + v^{T}A^{-1}u) = v^{T}A^{-1}b $$\nThe problem statement provides the condition that $1 + v^{T}A^{-1}u \\neq 0$. This ensures that we can divide by the term $(1 + v^{T}A^{-1}u)$ to obtain a unique value for $\\alpha$:\n$$ \\alpha = \\frac{v^{T}A^{-1}b}{1 + v^{T}A^{-1}u} $$\nNow that we have an explicit expression for $\\alpha$, we can substitute it back into our equation for $x$:\n$$ x = A^{-1}b - (A^{-1}u)\\alpha = A^{-1}b - (A^{-1}u) \\left( \\frac{v^{T}A^{-1}b}{1 + v^{T}A^{-1}u} \\right) $$\nThis expression can be written more cleanly. The term $(A^{-1}u)(v^{T}A^{-1}b)$ is a product of a vector ($A^{-1}u$) and a scalar ($v^{T}A^{-1}b$). This is equivalent to the matrix product $A^{-1}uv^{T}A^{-1}b$.\n$$ x = A^{-1}b - \\frac{A^{-1}uv^{T}A^{-1}b}{1 + v^{T}A^{-1}u} $$\nThis is the explicit analytic expression for the unique solution $x$. It is expressed solely in terms of the required quantities: $A^{-1}b$, $A^{-1}u$, $v^{T}A^{-1}b$, and $v^{T}A^{-1}u$. This result is known as the Sherman-Morrison formula applied to solve a linear system.",
            "answer": "$$\n\\boxed{A^{-1}b - \\frac{A^{-1} u v^{T} A^{-1} b}{1 + v^{T} A^{-1} u}}\n$$"
        },
        {
            "introduction": "A crucial aspect of using any numerical formula is understanding its limitations. This exercise  challenges you to investigate the singular case of the Sherman-Morrison formula, where the denominator term $1 + v^{T} A^{-1} u$ becomes zero. You will construct a concrete example where this occurs and demonstrate that this condition corresponds precisely to the updated matrix $A + u v^{T}$ becoming singular, thereby providing a deep insight into why the formula fails.",
            "id": "3596884",
            "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$ be the symmetric, strictly diagonally dominant matrix\n$$\nA=\\begin{pmatrix}\n4 & 1 & 0 \\\\\n1 & 4 & 1 \\\\\n0 & 1 & 4\n\\end{pmatrix},\n$$\nand let $u=\\begin{pmatrix}1 \\\\ 0 \\\\ 1\\end{pmatrix}$. Choose $w=\\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$ and define $v \\in \\mathbb{R}^{3}$ by $v=-A^{T} w$. Consider the rank-one update $A+u v^{T}$ and the scalar\n$$\ns \\equiv 1+v^{T} A^{-1} u.\n$$\nUsing only fundamental properties of invertibility, kernels, and rank-one outer products, and without appealing to any specialized update identities, do the following:\n\n- Compute the exact value of $s$.\n- Using your computation, construct a nonzero vector in the kernel of $A+u v^{T}$ and justify why this proves $A+u v^{T}$ is not invertible.\n- Explain precisely how and why the Sherman–Morrison formula fails for this choice of $A$, $u$, and $v$, in terms of the linear-algebraic mechanism you have established.\n\nYour final reported answer must be the exact value of $s$. No rounding is required.",
            "solution": "The solution proceeds in three parts as requested by the problem statement.\n\n**Part 1: Compute the exact value of $s$.**\n\nThe scalar $s$ is defined as $s = 1+v^{T} A^{-1} u$. The vector $v$ is defined as $v=-A^{T} w$. We can substitute the definition of $v$ into the expression for $s$.\nFirst, we find the transpose of $v$:\n$$\nv^T = (-A^T w)^T = -w^T (A^T)^T = -w^T A\n$$\nNow, substitute this expression for $v^T$ into the formula for $s$:\n$$\ns = 1 + (-w^T A) A^{-1} u\n$$\nUsing the associative property of matrix multiplication, we can group the terms $A$ and $A^{-1}$:\n$$\ns = 1 - w^T (A A^{-1}) u\n$$\nSince $A A^{-1} = I$, where $I$ is the $3 \\times 3$ identity matrix, the expression simplifies to:\n$$\ns = 1 - w^T I u = 1 - w^T u\n$$\nThis simplification allows us to compute $s$ without needing to calculate $A^{-1}$. We are given the vectors $w$ and $u$:\n$$\nw = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad u = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nThe dot product $w^T u$ is:\n$$\nw^T u = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = (1)(1) + (0)(0) + (0)(1) = 1\n$$\nFinally, we substitute this value back into the expression for $s$:\n$$\ns = 1 - 1 = 0\n$$\nThe exact value of $s$ is $0$.\n\n**Part 2: Construct a nonzero vector in the kernel of $A+u v^{T}$ and justify its non-invertibility.**\n\nLet the updated matrix be $B = A+u v^{T}$. A vector $x$ is in the kernel of $B$ if $Bx=0$. We are seeking a nonzero vector $x$ that satisfies this condition.\nThe equation is:\n$$\n(A+u v^{T})x = 0\n$$\n$$\nAx + u(v^T x) = 0\n$$\nThe term $v^T x$ is a scalar. Let's denote it by $\\alpha = v^T x$. The equation becomes:\n$$\nAx + \\alpha u = 0\n$$\nSince $A$ is invertible, we can solve for $x$:\n$$\nAx = -\\alpha u \\implies x = A^{-1}(-\\alpha u) = -\\alpha (A^{-1}u)\n$$\nThis equation shows that any vector $x$ in the kernel of $B$ must be a scalar multiple of the vector $A^{-1}u$. Let's choose our candidate vector to be $x_k = A^{-1}u$. To be a valid kernel vector, it must be nonzero and satisfy the original equation. Since $A$ is invertible and $u \\neq 0$, $x_k = A^{-1}u$ must be nonzero.\n\nNow, we substitute $x_k=A^{-1}u$ back into the kernel equation $(A+u v^{T})x_k = 0$:\n$$\n(A+u v^{T})(A^{-1}u) = A(A^{-1}u) + u v^{T}(A^{-1}u) = Iu + u(v^{T}A^{-1}u) = u(1+v^{T}A^{-1}u)\n$$\nThe expression in the parentheses is precisely the scalar $s$. So, we have:\n$$\n(A+u v^{T})x_k = s u\n$$\nFrom Part 1, we calculated $s=0$. Therefore:\n$$\n(A+u v^{T})x_k = 0 \\cdot u = 0\n$$\nThis confirms that $x_k = A^{-1}u$ is indeed a vector in the kernel of $A+u v^{T}$. As argued, this vector is nonzero.\n\nTo explicitly construct this vector, we need to compute $A^{-1}$. The determinant of $A$ is $\\det(A) = 4(16-1) - 1(4-0) = 56$. The adjugate matrix of $A$ is:\n$$\n\\text{adj}(A) = \\begin{pmatrix}\n15 & -4 & 1 \\\\\n-4 & 16 & -4 \\\\\n1 & -4 & 15\n\\end{pmatrix}\n$$\nSo, the inverse is:\n$$\nA^{-1} = \\frac{1}{56}\\begin{pmatrix}\n15 & -4 & 1 \\\\\n-4 & 16 & -4 \\\\\n1 & -4 & 15\n\\end{pmatrix}\n$$\nNow we compute the kernel vector $x_k = A^{-1}u$:\n$$\nx_k = \\frac{1}{56}\\begin{pmatrix}\n15 & -4 & 1 \\\\\n-4 & 16 & -4 \\\\\n1 & -4 & 15\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{56}\\begin{pmatrix}\n15+1 \\\\\n-4-4 \\\\\n1+15\n\\end{pmatrix} = \\frac{1}{56}\\begin{pmatrix}\n16 \\\\\n-8 \\\\\n16\n\\end{pmatrix} = \\frac{8}{56}\\begin{pmatrix}\n2 \\\\\n-1 \\\\\n2\n\\end{pmatrix} = \\frac{1}{7}\\begin{pmatrix}\n2 \\\\\n-1 \\\\\n2\n\\end{pmatrix}\n$$\nA valid nonzero vector in the kernel is any non-zero scalar multiple of this, for instance, $\\begin{pmatrix} 2 \\\\ -1 \\\\ 2 \\end{pmatrix}$.\n\nA square matrix is invertible if and only if its kernel (or null space) contains only the zero vector. Since we have constructed a nonzero vector in the kernel of $A+u v^{T}$, this proves that the matrix $A+u v^{T}$ is singular (i.e., not invertible).\n\n**Part 3: Explain the failure of the Sherman–Morrison formula.**\n\nThe Sherman-Morrison formula provides an expression for the inverse of a rank-one update to an invertible matrix $A$. The formula is:\n$$\n(A+u v^{T})^{-1} = A^{-1} - \\frac{A^{-1}uv^{T}A^{-1}}{1+v^{T}A^{-1}u}\n$$\nThis formula is applicable only if the denominator is nonzero. The denominator is exactly the scalar $s = 1+v^{T}A^{-1}u$.\n\nIn Part 1, we computed the exact value of this scalar and found that $s=0$.\n$$\n1+v^{T}A^{-1}u = 0\n$$\nThe Sherman-Morrison formula thus fails for this specific choice of $A$, $u$, and $v$ because it would require division by zero, which is an undefined operation.\n\nThis failure is not a mere computational issue; it is a direct consequence of the linear-algebraic mechanism established in Part 2. The condition $1+v^{T}A^{-1}u = 0$ is precisely the condition that ensures the existence of a nontrivial kernel for the matrix $A+u v^{T}$, which implies the matrix is not invertible. The Sherman-Morrison formula, being a formula for the inverse, correctly reflects the non-existence of this inverse by becoming undefined. The vanishing denominator is the mathematical signal that the updated matrix has become singular.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "We now move from theory to a practical and powerful application: accelerating computations in machine learning. This practice  applies the Sherman-Morrison formula to the problem of streaming ridge regression, where a model is updated sequentially as new data arrives. You will design a numerically stable workflow that uses Cholesky factorizations to apply the inverse update, reinforcing the best practice of avoiding explicit matrix inversions in favor of stable triangular solves.",
            "id": "3596918",
            "problem": "You are given a symmetric positive definite matrix update scenario that arises in streaming ridge regression. Let $X \\in \\mathbb{R}^{m \\times n}$ be a data matrix and let $\\lambda \\in \\mathbb{R}$ with $\\lambda > 0$. Define $A = X^{\\top} X + \\lambda I_n \\in \\mathbb{R}^{n \\times n}$, where $I_n$ is the $n \\times n$ identity matrix. Suppose a new sample $x \\in \\mathbb{R}^{n}$ arrives, and you must compute the action of $(A + x x^{\\top})^{-1}$ on a given vector $b \\in \\mathbb{R}^{n}$ without ever forming $A^{-1}$ or $(A + x x^{\\top})^{-1}$ explicitly. You must use only triangular solves with the Cholesky factorization of $A$ (and, for verification only, the Cholesky factorization of $A + x x^{\\top}$), and you must not form or invert any dense matrix directly.\n\nYour task is to design a numerically stable workflow, justified from first principles, that computes $(A + x x^{\\top})^{-1} b$ using only triangular solves with the Cholesky factor of $A$, and to verify it numerically by comparing to a direct solve using the Cholesky factor of $A + x x^{\\top}$. Specifically, for each test case defined below, compute $y_{\\mathrm{SM}} = (A + x x^{\\top})^{-1} b$ using only solves with the Cholesky factor of $A$, and compute $y_{\\mathrm{ref}}$ by solving $(A + x x^{\\top}) y_{\\mathrm{ref}} = b$ via its Cholesky factorization. Report the Euclidean norm $\\lVert y_{\\mathrm{SM}} - y_{\\mathrm{ref}} \\rVert_2$.\n\nConstraints and requirements:\n- Use only triangular solves with the Cholesky factorization of $A$ when constructing $y_{\\mathrm{SM}}$.\n- For verification, you may also use a Cholesky factorization of $A + x x^{\\top}$ followed by triangular solves to obtain $y_{\\mathrm{ref}}$.\n- Do not compute $A^{-1}$, $(A + x x^{\\top})^{-1}$, or any matrix inverse explicitly.\n- All computations are purely mathematical; there are no physical units involved.\n\nTest suite:\nProvide outputs for the following three test cases.\n\nTest case $1$:\n- $n = 4$, $m = 6$, $\\lambda = 0.5$.\n- $X = \\begin{bmatrix}\n1 & 0 & -1 & 2 \\\\\n0 & 1 & 2 & -1 \\\\\n2 & -1 & 0 & 1 \\\\\n-1 & 2 & 1 & 0 \\\\\n1 & 1 & 1 & 1 \\\\\n3 & -2 & 1 & -1\n\\end{bmatrix}$, $x = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 2 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\\\ 0 \\end{bmatrix}$.\n\nTest case $2$ (nearly collinear columns, small regularization):\n- $n = 3$, $m = 3$, $\\lambda = 1 \\times 10^{-8}$.\n- $X = \\begin{bmatrix}\n1 & 1 & 1 \\\\\n2 & 2.000001 & 2 \\\\\n3 & 3.000002 & 3\n\\end{bmatrix}$, $x = \\begin{bmatrix} 1 \\\\ 1.000001 \\\\ 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}$.\n\nTest case $3$:\n- $n = 5$, $m = 7$, $\\lambda = 1$.\n- $X = \\begin{bmatrix}\n1 & 2 & -1 & 0 & 3 \\\\\n0 & -1 & 2 & 1 & -2 \\\\\n2 & 0 & 1 & -1 & 1 \\\\\n-1 & 3 & 0 & 2 & -3 \\\\\n1 & -2 & -1 & 1 & 0 \\\\\n2 & 1 & 3 & -2 & 1 \\\\\n-2 & 0 & 1 & 2 & -1\n\\end{bmatrix}$, $x = \\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ -2 \\end{bmatrix}$, $b = \\begin{bmatrix} -1 \\\\ 0 \\\\ 2 \\\\ 1 \\\\ 3 \\end{bmatrix}$.\n\nFinal output format:\nYour program should produce a single line of output containing the three results (one per test case) as a comma-separated list enclosed in square brackets, for example $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$, where each $\\text{result}_k$ is the Euclidean norm $\\lVert y_{\\mathrm{SM}} - y_{\\mathrm{ref}} \\rVert_2$ for test case $k$. The results must be floating-point numbers.",
            "solution": "### Principle-Based Solution\n\nThe objective is to compute $y = (A + x x^{\\top})^{-1} b$, where $A = X^{\\top} X + \\lambda I_n$, by leveraging the structure of the update. The matrix $A$ is altered by a rank-$1$ update, $x x^{\\top}$. The Sherman-Morrison formula is designed for precisely this situation.\n\nThe formula states that for an invertible matrix $B$ and vectors $u, v$, the inverse of a rank-$1$ update is:\n$$\n(B + u v^{\\top})^{-1} = B^{-1} - \\frac{B^{-1} u v^{\\top} B^{-1}}{1 + v^{\\top} B^{-1} u}\n$$\nThis formula is applicable provided that the denominator $1 + v^{\\top} B^{-1} u \\neq 0$.\n\nFor our problem, we set $B = A$, $u = x$, and $v = x$. The matrix $A$ is SPD, which implies that its inverse $A^{-1}$ is also SPD. For any non-zero vector $x$, the quadratic form $x^{\\top} A^{-1} x > 0$. Therefore, the denominator $1 + x^{\\top} A^{-1} x$ is always greater than $1$, ensuring the formula is well-defined.\n\nApplying the formula to our problem, we get:\n$$\n(A + x x^{\\top})^{-1} = A^{-1} - \\frac{A^{-1} x x^{\\top} A^{-1}}{1 + x^{\\top} A^{-1} x}\n$$\nWe need to compute the action of this inverse on the vector $b$:\n$$\ny_{\\mathrm{SM}} = (A + x x^{\\top})^{-1} b = \\left( A^{-1} - \\frac{A^{-1} x x^{\\top} A^{-1}}{1 + x^{\\top} A^{-1} x} \\right) b\n$$\nBy distributing $b$, we obtain:\n$$\ny_{\\mathrm{SM}} = A^{-1} b - \\frac{A^{-1} x (x^{\\top} A^{-1} b)}{1 + x^{\\top} A^{-1} x}\n$$\nThe problem explicitly forbids computing $A^{-1}$. We can reformulate the expression in terms of solutions to linear systems. Let's define two intermediate vectors, $z$ and $w$:\n1.  $z = A^{-1} b$, which is the solution to the linear system $Az = b$.\n2.  $w = A^{-1} x$, which is the solution to the linear system $Aw = x$.\n\nSubstituting $z$ and $w$ into the expression for $y_{\\mathrm{SM}}$ gives:\n$$\ny_{\\mathrm{SM}} = z - \\frac{w (x^{\\top} z)}{1 + x^{\\top} w}\n$$\nThis formulation avoids matrix inversion. The terms $x^{\\top} z$ and $x^{\\top} w$ are scalar dot products. The overall computation is a series of well-defined steps that adhere to the problem's constraints.\n\n**Algorithm for $y_{\\mathrm{SM}}$**:\n1.  Construct the matrix $A = X^{\\top} X + \\lambda I_n$.\n2.  Since $A$ is SPD, compute its Cholesky factorization $A = LL^{\\top}$, where $L$ is a lower triangular matrix.\n3.  Solve $Az = b$ for $z$. This is done efficiently using the Cholesky factor $L$, by first solving $Ly' = b$ (forward substitution) and then $L^{\\top}z = y'$ (backward substitution).\n4.  Solve $Aw = x$ for $w$ using the same factors $L$ and $L^{\\top}$.\n5.  Compute the scalar numerator $\\alpha = x^{\\top} z$.\n6.  Compute the scalar denominator $\\beta = 1 + x^{\\top} w$.\n7.  The final result is $y_{\\mathrm{SM}} = z - (\\alpha / \\beta) w$.\n\n**Algorithm for Verification ($y_{\\mathrm{ref}}$)**:\nTo verify the result, we solve the system $(A + x x^{\\top}) y = b$ directly, which provides a reference solution $y_{\\mathrm{ref}}$.\n1.  Construct the matrix $A = X^{\\top} X + \\lambda I_n$.\n2.  Construct the updated matrix $A_{\\mathrm{new}} = A + x x^{\\top}$.\n3.  Since $A_{\\mathrm{new}}$ is also SPD, compute its Cholesky factorization $A_{\\mathrm{new}} = L_{\\mathrm{new}}L_{\\mathrm{new}}^{\\top}$.\n4.  Solve $A_{\\mathrm{new}} y_{\\mathrm{ref}} = b$ using the factors $L_{\\mathrm{new}}$ and $L_{\\mathrm{new}}^{\\top}$.\n\nFinally, we compute the Euclidean norm of the difference, $\\lVert y_{\\mathrm{SM}} - y_{\\mathrm{ref}} \\rVert_2$. Due to the finite precision of floating-point arithmetic, this value will be very small (on the order of machine epsilon) but likely non-zero, confirming the numerical equivalence of the two methods.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, cho_solve\n\ndef solve():\n    \"\"\"\n    Solves the problem for all given test cases and prints the results.\n    \"\"\"\n    \n    # Test case 1\n    X1 = np.array([\n        [1, 0, -1, 2],\n        [0, 1, 2, -1],\n        [2, -1, 0, 1],\n        [-1, 2, 1, 0],\n        [1, 1, 1, 1],\n        [3, -2, 1, -1]\n    ])\n    x1 = np.array([1, -1, 0, 2])\n    b1 = np.array([1, 2, -1, 0])\n    lambda1 = 0.5\n    \n    # Test case 2\n    X2 = np.array([\n        [1, 1, 1],\n        [2, 2.000001, 2],\n        [3, 3.000002, 3]\n    ])\n    x2 = np.array([1, 1.000001, 1])\n    b2 = np.array([1, -1, 0])\n    lambda2 = 1e-8\n    \n    # Test case 3\n    X3 = np.array([\n        [1, 2, -1, 0, 3],\n        [0, -1, 2, 1, -2],\n        [2, 0, 1, -1, 1],\n        [-1, 3, 0, 2, -3],\n        [1, -2, -1, 1, 0],\n        [2, 1, 3, -2, 1],\n        [-2, 0, 1, 2, -1]\n    ])\n    x3 = np.array([2, -1, 0, 1, -2])\n    b3 = np.array([-1, 0, 2, 1, 3])\n    lambda3 = 1.0\n        \n    test_cases = [\n        (X1, x1, b1, lambda1),\n        (X2, x2, b2, lambda2),\n        (X3, x3, b3, lambda3),\n    ]\n\n    results = []\n    for case in test_cases:\n        X, x, b, lambda_val = case\n        result = compute_norm_diff(X, x, b, lambda_val)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_norm_diff(X, x, b, lambda_val):\n    \"\"\"\n    Computes the norm of the difference between the Sherman-Morrison\n    method and the direct Cholesky solve method.\n\n    Args:\n        X (np.ndarray): Data matrix of shape (m, n).\n        x (np.ndarray): New sample vector of shape (n,).\n        b (np.ndarray): Vector on which the inverse acts, shape (n,).\n        lambda_val (float): Regularization parameter.\n\n    Returns:\n        float: The Euclidean norm of the difference ||y_sm - y_ref||_2.\n    \"\"\"\n    n = X.shape[1]\n    I_n = np.identity(n)\n    \n    # Form the matrix A = X'X + lambda*I\n    A = X.T @ X + lambda_val * I_n\n    \n    # Method 1: Using Sherman-Morrison formula\n    # This method computes y_sm = (A + xx')^{-1} b\n    #\n    # y_sm = A^{-1}b - (A^{-1}x * (x' * A^{-1}b)) / (1 + x' * A^{-1}x)\n    #\n    # We define z = A^{-1}b and w = A^{-1}x and solve linear systems.\n    # y_sm = z - (w * (x'z)) / (1 + x'w)\n\n    # Compute Cholesky factorization of A, L where A = LL'\n    # Use lower=True to get the lower-triangular factor L\n    try:\n        L = cholesky(A, lower=True)\n    except np.linalg.LinAlgError:\n        # This should not happen since A is proven to be SPD\n        return float('nan')\n\n    # Solve Az = b for z using the Cholesky factor\n    z = cho_solve((L, True), b)\n    \n    # Solve Aw = x for w using the same Cholesky factor\n    w = cho_solve((L, True), x)\n    \n    # Compute the scalar components of the Sherman-Morrison formula\n    # alpha = x'z\n    alpha = x.T @ z\n    # beta = 1 + x'w\n    beta = 1.0 + x.T @ w\n    \n    # Compute y_sm\n    y_sm = z - (alpha / beta) * w\n\n    # Method 2: Direct solve for verification (y_ref)\n    # This involves forming the updated matrix A_new = A + xx' and solving\n    # A_new * y_ref = b\n    \n    # Form the updated matrix\n    A_new = A + np.outer(x, x)\n    \n    # Compute Cholesky factorization of A_new\n    try:\n        L_new = cholesky(A_new, lower=True)\n    except np.linalg.LinAlgError:\n        # A_new is also guaranteed to be SPD\n        return float('nan')\n\n    # Solve for the reference solution y_ref\n    y_ref = cho_solve((L_new, True), b)\n    \n    # Compute and return the Euclidean norm of the difference\n    return np.linalg.norm(y_sm - y_ref)\n\nsolve()\n\n```"
        }
    ]
}