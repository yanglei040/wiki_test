{
    "hands_on_practices": [
        {
            "introduction": "理解一个公式的最佳方式往往是从其推导过程开始。这项练习将引导你从第一性原理出发，为求解形如 $(A + u v^{T}) x = b$ 的线性方程组推导 Sherman-Morrison 公式的解形式。通过这个过程，你将牢固掌握其背后的代数操作和核心思想，为更复杂的应用打下坚实基础。",
            "id": "3596896",
            "problem": "设 $n$ 是一个正整数。设 $A \\in \\mathbb{R}^{n \\times n}$ 是一个可逆矩阵，且 $u,v \\in \\mathbb{R}^{n}$ 和 $b \\in \\mathbb{R}^{n}$。考虑线性系统 $(A + u v^{T}) x = b$。假设 $1 + v^{T} A^{-1} u \\neq 0$，从而秩一更新 $A + u v^{T}$ 是可逆的。仅从矩阵逆的定义以及矩阵和向量运算的基本性质（线性性、结合律和分配律）出发，推导唯一解 $x \\in \\mathbb{R}^{n}$ 的显式解析表达式，该表达式以 $A^{-1} b$、$A^{-1} u$、$v^{T} A^{-1} b$ 和 $v^{T} A^{-1} u$ 表示。你的最终答案必须是 $x$ 的单个封闭形式的符号表达式。不需要数值近似，也不需要四舍五入。",
            "solution": "该问题是有效的，因为它在科学上基于线性代数，其为唯一解提供了所有必要且一致的条件，因而问题是适定的，并且陈述客观。因此，我们可以进行推导。\n\n我们的任务是找到线性系统的解 $x \\in \\mathbb{R}^{n}$：\n$$ (A + u v^{T}) x = b $$\n其中，$A \\in \\mathbb{R}^{n \\times n}$ 是一个可逆矩阵， $u, v, b \\in \\mathbb{R}^{n}$ 是列向量。项 $u v^{T}$ 表示 $u$ 和 $v$ 的外积，它是一个秩为一的 $n \\times n$ 矩阵。\n\n我们首先将矩阵乘法的分配律应用于方程的左侧：\n$$ Ax + (u v^{T}) x = b $$\n利用矩阵-向量乘法的结合律，可以对项 $(u v^{T}) x$ 进行重组。注意，$v^{T}x$ 是向量 $v$ 和 $x$ 的内积，其结果是一个标量值。我们将这个标量表示为 $\\alpha$。\n$$ \\alpha = v^{T}x $$\n于是该方程可以写成：\n$$ Ax + u(v^{T}x) = Ax + u \\alpha = b $$\n我们的目标是分离出向量 $x$。我们可以重新整理方程，将所有含 $x$ 的项放在一边，但更直接的方法是先分离出项 $Ax$：\n$$ Ax = b - u \\alpha $$\n由于矩阵 $A$ 是可逆的，其逆矩阵 $A^{-1}$ 存在。我们可以在方程两边同时左乘 $A^{-1}$：\n$$ A^{-1}(Ax) = A^{-1}(b - u \\alpha) $$\n在左侧使用结合律，在右侧使用分配律，我们得到：\n$$ (A^{-1}A)x = A^{-1}b - A^{-1}(u \\alpha) $$\n根据逆矩阵的定义，$A^{-1}A = I$，其中 $I$ 是 $n \\times n$ 单位矩阵。此外，由于 $\\alpha$ 是一个标量，它可以从矩阵-向量乘积中提出：$A^{-1}(u \\alpha) = (A^{-1}u)\\alpha$。方程简化为：\n$$ Ix = A^{-1}b - (A^{-1}u)\\alpha $$\n$$ x = A^{-1}b - (A^{-1}u)\\alpha $$\n这个表达式用已知量 $A^{-1}b$ 和 $A^{-1}u$ 表示 $x$，但它仍然依赖于未知标量 $\\alpha$。为了求出 $\\alpha$，我们使用它的定义 $\\alpha = v^{T}x$，并代入我们刚刚推导出的 $x$ 的表达式：\n$$ \\alpha = v^{T} \\left( A^{-1}b - (A^{-1}u)\\alpha \\right) $$\n对转置-向量乘积应用分配律：\n$$ \\alpha = v^{T}(A^{-1}b) - v^{T}((A^{-1}u)\\alpha) $$\n同样，由于 $\\alpha$ 是一个标量，我们可以将其从右侧的表达式中提出：\n$$ \\alpha = v^{T}A^{-1}b - (v^{T}A^{-1}u)\\alpha $$\n这是一个关于单变量 $\\alpha$ 的线性方程。项 $v^{T}A^{-1}b$ 和 $v^{T}A^{-1}u$ 都是标量。我们现在可以解出 $\\alpha$。首先，我们将所有包含 $\\alpha$ 的项移到方程的一边：\n$$ \\alpha + (v^{T}A^{-1}u)\\alpha = v^{T}A^{-1}b $$\n提出因子 $\\alpha$：\n$$ \\alpha (1 + v^{T}A^{-1}u) = v^{T}A^{-1}b $$\n问题陈述中给出了条件 $1 + v^{T}A^{-1}u \\neq 0$。这确保了我们可以除以 $(1 + v^{T}A^{-1}u)$ 来获得 $\\alpha$ 的唯一值：\n$$ \\alpha = \\frac{v^{T}A^{-1}b}{1 + v^{T}A^{-1}u} $$\n现在我们有了 $\\alpha$ 的显式表达式，可以将其代回我们的 $x$ 方程中：\n$$ x = A^{-1}b - (A^{-1}u)\\alpha = A^{-1}b - (A^{-1}u) \\left( \\frac{v^{T}A^{-1}b}{1 + v^{T}A^{-1}u} \\right) $$\n这个表达式可以写得更简洁。项 $(A^{-1}u)(v^{T}A^{-1}b)$ 是一个向量（$A^{-1}u$）和一个标量（$v^{T}A^{-1}b$）的乘积。这等价于矩阵乘积 $A^{-1}uv^{T}A^{-1}b$。\n$$ x = A^{-1}b - \\frac{A^{-1}uv^{T}A^{-1}b}{1 + v^{T}A^{-1}u} $$\n这就是唯一解 $x$ 的显式解析表达式。它完全由所需的量表示：$A^{-1}b$、$A^{-1}u$、$v^{T}A^{-1}b$ 和 $v^{T}A^{-1}u$。这个结果被称为应用于求解线性系统的 Sherman-Morrison 公式。",
            "answer": "$$\n\\boxed{A^{-1}b - \\frac{A^{-1} u v^{T} A^{-1} b}{1 + v^{T} A^{-1} u}}\n$$"
        },
        {
            "introduction": "熟练使用任何数学工具都包括了解其适用范围和局限性。本练习将探讨 Sherman-Morrison 公式的奇异情况，即当分母项 $1 + v^{T} A^{-1} u$ 等于零时的情形。通过构造一个具体的例子，你将深入理解公式分母与更新后矩阵可逆性之间的深刻联系，并认识到为何此时公式会失效。",
            "id": "3596884",
            "problem": "设 $A \\in \\mathbb{R}^{3 \\times 3}$ 是对称、严格对角占优矩阵\n$$\nA=\\begin{pmatrix}\n4  1  0 \\\\\n1  4  1 \\\\\n0  1  4\n\\end{pmatrix},\n$$\n并设 $u=\\begin{pmatrix}1 \\\\ 0 \\\\ 1\\end{pmatrix}$。选择 $w=\\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$ 并定义 $v \\in \\mathbb{R}^{3}$ 为 $v=-A^{T} w$。考虑秩一更新 $A+u v^{T}$ 和标量\n$$\ns \\equiv 1+v^{T} A^{-1} u.\n$$\n仅使用可逆性、核、秩一外积的基本性质，并且不借助任何专门的更新恒等式，完成以下任务：\n\n- 计算 $s$ 的精确值。\n- 利用你的计算结果，构造一个在 $A+u v^{T}$ 的核中的非零向量，并说明为什么这证明了 $A+u v^{T}$ 是不可逆的。\n- 结合你已建立的线性代数机制，精确解释对于所选的 $A$、$u$ 和 $v$，Sherman–Morrison 公式为何以及如何失效。\n\n你最终报告的答案必须是 $s$ 的精确值。无需四舍五入。",
            "solution": "问题按如下方式进行验证。\n### 第1步：提取已知条件\n- 矩阵 $A \\in \\mathbb{R}^{3 \\times 3}$ 给定为 $A=\\begin{pmatrix} 4  1  0 \\\\ 1  4  1 \\\\ 0  1  4 \\end{pmatrix}$。题目说明该矩阵是对称且严格对角占优的。\n- 向量 $u \\in \\mathbb{R}^{3}$ 给定为 $u=\\begin{pmatrix}1 \\\\ 0 \\\\ 1\\end{pmatrix}$。\n- 向量 $w \\in \\mathbb{R}^{3}$ 给定为 $w=\\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$。\n- 向量 $v \\in \\mathbb{R}^{3}$ 由关系 $v=-A^{T} w$ 定义。\n- 标量 $s$ 由关系 $s \\equiv 1+v^{T} A^{-1} u$ 定义。\n\n### 第2步：使用提取的已知条件进行验证\n- **科学依据：** 该问题是数值线性代数中的一个标准练习。矩阵可逆性、核、秩一更新以及 Sherman-Morrison 公式等概念都是数学中基本且成熟的概念。\n- **适定性：** 矩阵 $A$ 是严格对角占优的，这保证了它是可逆的。因此，$A^{-1}$ 存在且唯一。向量 $u$、$v$、$w$ 都是良定义的，因此标量 $s$ 也是良定义的。任务陈述清晰，并能导出一个唯一且有意义的解。\n- **客观性：** 问题使用精确的数学语言表述，没有任何主观性或歧义。\n\n### 第3步：结论与行动\n该问题是有效的。它自洽、逻辑一致且在数学上是合理的。我们可以继续进行求解。\n\n解题过程按照题目要求分为三个部分。\n\n**第1部分：计算 $s$ 的精确值。**\n\n标量 $s$ 定义为 $s = 1+v^{T} A^{-1} u$。向量 $v$ 定义为 $v=-A^{T} w$。我们可以将 $v$ 的定义代入 $s$ 的表达式中。\n首先，我们求 $v$ 的转置：\n$$\nv^T = (-A^T w)^T = -w^T (A^T)^T = -w^T A\n$$\n现在，将 $v^T$ 的这个表达式代入 $s$ 的公式中：\n$$\ns = 1 + (-w^T A) A^{-1} u\n$$\n利用矩阵乘法的结合律，我们可以将项 $A$ 和 $A^{-1}$ 组合在一起：\n$$\ns = 1 - w^T (A A^{-1}) u\n$$\n因为 $A A^{-1} = I$，其中 $I$ 是 $3 \\times 3$ 单位矩阵，所以表达式简化为：\n$$\ns = 1 - w^T I u = 1 - w^T u\n$$\n这个简化使我们能够在不计算 $A^{-1}$ 的情况下计算 $s$。我们已知向量 $w$ 和 $u$：\n$$\nw = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad u = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\n点积 $w^T u$ 是：\n$$\nw^T u = \\begin{pmatrix} 1  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = (1)(1) + (0)(0) + (0)(1) = 1\n$$\n最后，我们将这个值代回 $s$ 的表达式中：\n$$\ns = 1 - 1 = 0\n$$\n$s$ 的精确值是 $0$。\n\n**第2部分：构造一个在 $A+u v^{T}$ 核中的非零向量，并证明其不可逆性。**\n\n设更新后的矩阵为 $B = A+u v^{T}$。如果 $Bx=0$，则向量 $x$ 在 $B$ 的核中。我们正在寻找一个满足此条件的非零向量 $x$。\n方程为：\n$$\n(A+u v^{T})x = 0\n$$\n$$\nAx + u(v^T x) = 0\n$$\n项 $v^T x$ 是一个标量。我们将其记为 $\\alpha = v^T x$。方程变为：\n$$\nAx + \\alpha u = 0\n$$\n由于 $A$ 是可逆的，我们可以解出 $x$：\n$$\nAx = -\\alpha u \\implies x = A^{-1}(-\\alpha u) = -\\alpha (A^{-1}u)\n$$\n这个方程表明，在 $B$ 的核中的任何向量 $x$ 都必须是向量 $A^{-1}u$ 的标量倍。我们选择候选向量为 $x_k = A^{-1}u$。要成为一个有效的核向量，它必须是非零的并且满足原始方程。由于 $A$ 是可逆的且 $u \\neq 0$，所以 $x_k = A^{-1}u$ 必须是非零的。\n\n现在，我们将 $x_k=A^{-1}u$ 代回核方程 $(A+u v^{T})x_k = 0$ 中：\n$$\n(A+u v^{T})(A^{-1}u) = A(A^{-1}u) + u v^{T}(A^{-1}u) = Iu + u(v^{T}A^{-1}u) = u(1+v^{T}A^{-1}u)\n$$\n括号中的表达式正是标量 $s$。所以，我们有：\n$$\n(A+u v^{T})x_k = s u\n$$\n在第1部分中，我们计算出 $s=0$。因此：\n$$\n(A+u v^{T})x_k = 0 \\cdot u = 0\n$$\n这证实了 $x_k = A^{-1}u$ 确实是 $A+u v^{T}$ 核中的一个向量。如前所述，这个向量是非零的。\n\n为了显式地构造这个向量，我们需要计算 $A^{-1}$。$A$ 的行列式是 $\\det(A) = 4(16-1) - 1(4-0) = 56$。$A$ 的伴随矩阵是：\n$$\n\\text{adj}(A) = \\begin{pmatrix}\n15  -4  1 \\\\\n-4  16  -4 \\\\\n1  -4  15\n\\end{pmatrix}\n$$\n所以，逆矩阵是：\n$$\nA^{-1} = \\frac{1}{56}\\begin{pmatrix}\n15  -4  1 \\\\\n-4  16  -4 \\\\\n1  -4  15\n\\end{pmatrix}\n$$\n现在我们计算核向量 $x_k = A^{-1}u$：\n$$\nx_k = \\frac{1}{56}\\begin{pmatrix}\n15  -4  1 \\\\\n-4  16  -4 \\\\\n1  -4  15\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{56}\\begin{pmatrix}\n15+1 \\\\\n-4-4 \\\\\n1+15\n\\end{pmatrix} = \\frac{1}{56}\\begin{pmatrix}\n16 \\\\\n-8 \\\\\n16\n\\end{pmatrix} = \\frac{8}{56}\\begin{pmatrix}\n2 \\\\\n-1 \\\\\n2\n\\end{pmatrix} = \\frac{1}{7}\\begin{pmatrix}\n2 \\\\\n-1 \\\\\n2\n\\end{pmatrix}\n$$\n核中的一个有效的非零向量是该向量的任何非零标量倍，例如 $\\begin{pmatrix} 2 \\\\ -1 \\\\ 2 \\end{pmatrix}$。\n\n一个方阵是可逆的，当且仅当它的核（或零空间）只包含零向量。由于我们已经在 $A+u v^{T}$ 的核中构造了一个非零向量，这证明了矩阵 $A+u v^{T}$ 是奇异的（即不可逆的）。\n\n**第3部分：解释 Sherman–Morrison 公式的失效。**\n\nSherman-Morrison 公式给出了一个可逆矩阵 $A$ 经过秩一更新后的逆矩阵的表达式。该公式为：\n$$\n(A+u v^{T})^{-1} = A^{-1} - \\frac{A^{-1}uv^{T}A^{-1}}{1+v^{T}A^{-1}u}\n$$\n这个公式仅在分母非零时适用。分母正是标量 $s = 1+v^{T}A^{-1}u$。\n\n在第1部分中，我们计算了该标量的精确值，并发现 $s=0$。\n$$\n1+v^{T}A^{-1}u = 0\n$$\n因此，对于 $A$、$u$ 和 $v$ 的这个特定选择，Sherman-Morrison 公式失效，因为它会需要除以零，而这是一个未定义的操作。\n\n这个失效不仅仅是一个计算问题；它是第2部分所建立的线性代数机制的直接结果。条件 $1+v^{T}A^{-1}u = 0$ 正是确保矩阵 $A+u v^{T}$ 存在非平凡核的条件，这意味着该矩阵不可逆。Sherman-Morrison 公式作为一个求逆公式，通过变得未定义，正确地反映了该逆矩阵的不存在性。分母为零是更新后的矩阵变为奇异的数学信号。",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "理论的价值最终体现在实践中，Sherman-Morrison 公式在机器学习等领域的数据流更新问题中展现了卓越的效率。本练习将演示如何在流式岭回归（streaming ridge regression）问题中，利用 Cholesky 分解来高效且数值稳定地实现该公式。这个实践向你展示了如何避免直接计算逆矩阵，而是通过求解线性系统的思想，将理论应用于实际的计算任务中。",
            "id": "3596918",
            "problem": "给定一个在流式岭回归中出现的对称正定矩阵更新场景。设 $X \\in \\mathbb{R}^{m \\times n}$ 为一个数据矩阵，$\\lambda \\in \\mathbb{R}$ 且 $\\lambda > 0$。定义 $A = X^{\\top} X + \\lambda I_n \\in \\mathbb{R}^{n \\times n}$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。假设一个新的样本 $x \\in \\mathbb{R}^{n}$ 到达，您必须计算 $(A + x x^{\\top})^{-1}$ 对给定向量 $b \\in \\mathbb{R}^{n}$ 的作用，而无需显式地构建 $A^{-1}$ 或 $(A + x x^{\\top})^{-1}$。您只能使用 $A$ 的 Cholesky 分解进行三角求解（并且，仅为了验证，使用 $A + x x^{\\top}$ 的 Cholesky 分解），且不得直接构建或求逆任何稠密矩阵。\n\n您的任务是设计一个从第一性原理出发、数值稳定的工作流程，该流程仅使用 $A$ 的 Cholesky 因子进行三角求解来计算 $y_{\\mathrm{SM}} = (A + x x^{\\top})^{-1} b$，并通过与使用 $A + x x^{\\top}$ 的 Cholesky 因子进行直接求解的结果进行比较，来数值验证您的结果。具体来说，对于下面定义的每个测试用例，您需要仅使用 $A$ 的 Cholesky 因子求解来计算 $y_{\\mathrm{SM}} = (A + x x^{\\top})^{-1} b$，并通过其 Cholesky 分解求解 $(A + x x^{\\top}) y_{\\mathrm{ref}} = b$ 来计算 $y_{\\mathrm{ref}}$。报告欧几里得范数 $\\lVert y_{\\mathrm{SM}} - y_{\\mathrm{ref}} \\rVert_2$。\n\n约束与要求：\n- 在构建 $y_{\\mathrm{SM}}$ 时，仅使用 $A$ 的 Cholesky 分解进行三角求解。\n- 为了验证，您也可以使用 $A + x x^{\\top}$ 的 Cholesky 分解，然后进行三角求解以获得 $y_{\\mathrm{ref}}$。\n- 不要显式计算 $A^{-1}$、$(A + x x^{\\top})^{-1}$ 或任何矩阵的逆。\n- 所有计算都是纯数学的；不涉及物理单位。\n\n测试套件：\n为以下三个测试用例提供输出。\n\n测试用例 1：\n- $n = 4$, $m = 6$, $\\lambda = 0.5$。\n- $X = \\begin{bmatrix}\n1  0  -1  2 \\\\\n0  1  2  -1 \\\\\n2  -1  0  1 \\\\\n-1  2  1  0 \\\\\n1  1  1  1 \\\\\n3  -2  1  -1\n\\end{bmatrix}$, $x = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 2 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\\\ 0 \\end{bmatrix}$。\n\n测试用例 2（近共线列，小正则化）：\n- $n = 3$, $m = 3$, $\\lambda = 1 \\times 10^{-8}$。\n- $X = \\begin{bmatrix}\n1  1  1 \\\\\n2  2.000001  2 \\\\\n3  3.000002  3\n\\end{bmatrix}$, $x = \\begin{bmatrix} 1 \\\\ 1.000001 \\\\ 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}$。\n\n测试用例 3：\n- $n = 5$, $m = 7$, $\\lambda = 1$。\n- $X = \\begin{bmatrix}\n1  2  -1  0  3 \\\\\n0  -1  2  1  -2 \\\\\n2  0  1  -1  1 \\\\\n-1  3  0  2  -3 \\\\\n1  -2  -1  1  0 \\\\\n2  1  3  -2  1 \\\\\n-2  0  1  2  -1\n\\end{bmatrix}$, $x = \\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ -2 \\end{bmatrix}$, $b = \\begin{bmatrix} -1 \\\\ 0 \\\\ 2 \\\\ 1 \\\\ 3 \\end{bmatrix}$。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含三个结果（每个测试用例一个），形式为用方括号括起来的逗号分隔列表，例如 $[\\text{结果}_1,\\text{结果}_2,\\text{结果}_3]$，其中每个 $\\text{结果}_k$ 是测试用例 $k$ 的欧几里得范数 $\\lVert y_{\\mathrm{SM}} - y_{\\mathrm{ref}} \\rVert_2$。结果必须是浮点数。",
            "solution": "用户提供了有效的问题陈述。\n\n### 第一步：提取已知条件\n- **矩阵定义**：$A = X^{\\top} X + \\lambda I_n$，其中 $X \\in \\mathbb{R}^{m \\times n}$，$\\lambda \\in \\mathbb{R}$ 且 $\\lambda > 0$，$I_n$ 是 $n \\times n$ 的单位矩阵。\n- **问题背景**：$A$ 是对称正定的。\n- **更新**：一个新的样本 $x \\in \\mathbb{R}^{n}$ 到达，得到更新后的矩阵 $A + x x^{\\top}$。\n- **目标**：对给定的向量 $b \\in \\mathbb{R}^{n}$，计算 $y = (A + x x^{\\top})^{-1} b$。\n- **方法 1 ($y_{\\mathrm{SM}}$)**：使用 Sherman-Morrison 公式，仅依赖于使用 $A$ 的 Cholesky 因子进行的三角求解。\n- **方法 2 ($y_{\\mathrm{ref}}$)**：为了验证，通过使用更新后矩阵 $A + x x^{\\top}$ 的 Cholesky 分解来求解 $(A + x x^{\\top}) y_{\\mathrm{ref}} = b$ 以计算 $y_{\\mathrm{ref}}$。\n- **约束**：\n    - 不显式构建 $A^{-1}$ 或 $(A + x x^{\\top})^{-1}$。\n    - 除执行 Cholesky 分解外，不直接构建或求逆任何稠密矩阵。\n    - 仅使用三角求解。\n- **输出**：报告每个测试用例的欧几里得范数 $\\lVert y_{\\mathrm{SM}} - y_{\\mathrm{ref}} \\rVert_2$。\n- **测试用例**：\n    - **用例 1**：$n = 4$, $m = 6$, $\\lambda = 0.5$。\n      $X = \\begin{bmatrix} 1  0  -1  2 \\\\ 0  1  2  -1 \\\\ 2  -1  0  1 \\\\ -1  2  1  0 \\\\ 1  1  1  1 \\\\ 3  -2  1  -1 \\end{bmatrix}$， $x = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 2 \\end{bmatrix}$， $b = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\\\ 0 \\end{bmatrix}$。\n    - **用例 2**：$n = 3$, $m = 3$, $\\lambda = 1 \\times 10^{-8}$。\n      $X = \\begin{bmatrix} 1  1  1 \\\\ 2  2.000001  2 \\\\ 3  3.000002  3 \\end{bmatrix}$， $x = \\begin{bmatrix} 1 \\\\ 1.000001 \\\\ 1 \\end{bmatrix}$， $b = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}$。\n    - **用例 3**：$n = 5$, $m = 7$, $\\lambda = 1$。\n      $X = \\begin{bmatrix} 1  2  -1  0  3 \\\\ 0  -1  2  1  -2 \\\\ 2  0  1  -1  1 \\\\ -1  3  0  2  -3 \\\\ 1  -2  -1  1  0 \\\\ 2  1  3  -2  1 \\\\ -2  0  1  2  -1 \\end{bmatrix}$， $x = \\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ -2 \\end{bmatrix}$， $b = \\begin{bmatrix} -1 \\\\ 0 \\\\ 2 \\\\ 1 \\\\ 3 \\end{bmatrix}$。\n\n### 第二步：使用提取的已知条件进行验证\n该问题具有科学依据、良态、客观且完整。\n\n- **科学依据**：该问题植根于数值线性代数，具体涉及机器学习中常见的线性系统有效更新（例如，流式岭回归）。矩阵 $A = X^T X + \\lambda I_n$ 是岭回归的正规方程矩阵。它保证是对称正定（SPD）的。\n    - **对称性**：$A^T = (X^T X + \\lambda I_n)^T = (X^T X)^T + (\\lambda I_n)^T = X^T(X^T)^T + \\lambda I_n^T = X^T X + \\lambda I_n = A$。\n    - **正定性**：对于任何非零向量 $v \\in \\mathbb{R}^n$，其二次型为 $v^T A v = v^T(X^T X + \\lambda I_n)v = v^T X^T X v + \\lambda v^T I_n v = (Xv)^T(Xv) + \\lambda v^T v = \\lVert Xv \\rVert_2^2 + \\lambda \\lVert v \\rVert_2^2$。由于 $\\lambda > 0$ 且 $v \\neq 0$，因此 $\\lambda \\lVert v \\rVert_2^2 > 0$。项 $\\lVert Xv \\rVert_2^2 \\ge 0$。因此，$v^T A v > 0$。\n由于 $A$ 是对称正定的，其 Cholesky 分解存在。秩-1 更新 $A + xx^T$ 也是对称正定的，因为 $v^T(A+xx^T)v = v^TAv + (v^Tx)^2 > 0$。Sherman-Morrison 公式是一个标准的、有效的数学恒等式。\n\n- **良态性**：所涉及的线性系统存在唯一解，因为矩阵 $A$ 和 $A+xx^T$ 都是可逆的。问题指定了明确的目标并提供了所有必要的数据。\n\n- **客观性**：问题使用精确的数学语言陈述，不包含主观因素。\n\n- **无其他缺陷**：该问题是一个标准的练习，并非微不足道，其要求是一致且可行的。\n\n### 第三步：结论与行动\n问题有效，我将继续进行解答。\n\n### 基于原理的解决方案\n\n目标是通过利用更新的结构来计算 $y = (A + x x^{\\top})^{-1} b$，其中 $A = X^{\\top} X + \\lambda I_n$。矩阵 $A$ 被一个秩-1 更新 $x x^{\\top}$ 所改变。Sherman-Morrison 公式正是为这种情况设计的。\n\n该公式指出，对于一个可逆矩阵 $B$ 和向量 $u, v$，秩-1 更新的逆为：\n$$\n(B + u v^{\\top})^{-1} = B^{-1} - \\frac{B^{-1} u v^{\\top} B^{-1}}{1 + v^{\\top} B^{-1} u}\n$$\n该公式适用的条件是分母 $1 + v^{\\top} B^{-1} u \\neq 0$。\n\n对于我们的问题，我们设置 $B = A$，$u = x$，$v = x$。矩阵 $A$ 是对称正定的，这意味着其逆 $A^{-1}$ 也是对称正定的。对于任何非零向量 $x$，二次型 $x^{\\top} A^{-1} x > 0$。因此，分母 $1 + x^{\\top} A^{-1} x$ 总是大于 1，确保该公式是良定义的。\n\n将此公式应用于我们的问题，我们得到：\n$$\n(A + x x^{\\top})^{-1} = A^{-1} - \\frac{A^{-1} x x^{\\top} A^{-1}}{1 + x^{\\top} A^{-1} x}\n$$\n我们需要计算这个逆对向量 $b$ 的作用：\n$$\ny_{\\mathrm{SM}} = (A + x x^{\\top})^{-1} b = \\left( A^{-1} - \\frac{A^{-1} x x^{\\top} A^{-1}}{1 + x^{\\top} A^{-1} x} \\right) b\n$$\n通过分配 $b$，我们得到：\n$$\ny_{\\mathrm{SM}} = A^{-1} b - \\frac{A^{-1} x (x^{\\top} A^{-1} b)}{1 + x^{\\top} A^{-1} x}\n$$\n问题明确禁止计算 $A^{-1}$。我们可以用线性系统解的形式重新表述该表达式。让我们定义两个中间向量，$z$ 和 $w$：\n1.  $z = A^{-1} b$，这是线性系统 $Az = b$ 的解。\n2.  $w = A^{-1} x$，这是线性系统 $Aw = x$ 的解。\n\n将 $z$ 和 $w$ 代入 $y_{\\mathrm{SM}}$ 的表达式中，得到：\n$$\ny_{\\mathrm{SM}} = z - \\frac{w (x^{\\top} z)}{1 + x^{\\top} w}\n$$\n这种表述方式避免了矩阵求逆。项 $x^{\\top} z$ 和 $x^{\\top} w$ 是标量点积。整个计算是一系列符合问题约束的良定义步骤。\n\n**$y_{\\mathrm{SM}}$ 的算法**：\n1.  构建矩阵 $A = X^{\\top} X + \\lambda I_n$。\n2.  由于 $A$ 是对称正定的，计算其 Cholesky 分解 $A = LL^{\\top}$，其中 $L$ 是一个下三角矩阵。\n3.  求解 $Az = b$ 得到 $z$。这可以通过使用 Cholesky 因子 $L$ 来高效完成，首先求解 $Ly' = b$（前向替换），然后求解 $L^{\\top}z = y'$（后向替换）。\n4.  使用相同的因子 $L$ 和 $L^{\\top}$ 求解 $Aw = x$ 得到 $w$。\n5.  计算标量分子 $\\alpha = x^{\\top} z$。\n6.  计算标量分母 $\\beta = 1 + x^{\\top} w$。\n7.  最终结果是 $y_{\\mathrm{SM}} = z - (\\alpha / \\beta) w$。\n\n**验证算法 ($y_{\\mathrm{ref}}$)**：\n为了验证结果，我们直接求解系统 $(A + x x^{\\top}) y = b$，这提供了参考解 $y_{\\mathrm{ref}}$。\n1.  构建矩阵 $A = X^{\\top} X + \\lambda I_n$。\n2.  构建更新后的矩阵 $A_{\\mathrm{new}} = A + x x^{\\top}$。\n3.  由于 $A_{\\mathrm{new}}$ 也是对称正定的，计算其 Cholesky 分解 $A_{\\mathrm{new}} = L_{\\mathrm{new}}L_{\\mathrm{new}}^{\\top}$。\n4.  使用因子 $L_{\\mathrm{new}}$ 和 $L_{\\mathrm{new}}^{\\top}$ 求解 $A_{\\mathrm{new}} y_{\\mathrm{ref}} = b$。\n\n最后，我们计算差值的欧几里得范数 $\\lVert y_{\\mathrm{SM}} - y_{\\mathrm{ref}} \\rVert_2$。由于浮点运算的有限精度，这个值会非常小（在机器精度的数量级上），但可能非零，从而证实了两种方法在数值上的等价性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, cho_solve\n\ndef solve():\n    \"\"\"\n    Solves the problem for all given test cases and prints the results.\n    \"\"\"\n    \n    # Test case 1\n    X1 = np.array([\n        [1, 0, -1, 2],\n        [0, 1, 2, -1],\n        [2, -1, 0, 1],\n        [-1, 2, 1, 0],\n        [1, 1, 1, 1],\n        [3, -2, 1, -1]\n    ])\n    x1 = np.array([1, -1, 0, 2])\n    b1 = np.array([1, 2, -1, 0])\n    lambda1 = 0.5\n    \n    # Test case 2\n    X2 = np.array([\n        [1, 1, 1],\n        [2, 2.000001, 2],\n        [3, 3.000002, 3]\n    ])\n    x2 = np.array([1, 1.000001, 1])\n    b2 = np.array([1, -1, 0])\n    lambda2 = 1e-8\n    \n    # Test case 3\n    X3 = np.array([\n        [1, 2, -1, 0, 3],\n        [0, -1, 2, 1, -2],\n        [2, 0, 1, -1, 1],\n        [-1, 3, 0, 2, -3],\n        [1, -2, -1, 1, 0],\n        [2, 1, 3, -2, 1],\n        [-2, 0, 1, 2, -1]\n    ])\n    x3 = np.array([2, -1, 0, 1, -2])\n    b3 = np.array([-1, 0, 2, 1, 3])\n    lambda3 = 1.0\n        \n    test_cases = [\n        (X1, x1, b1, lambda1),\n        (X2, x2, b2, lambda2),\n        (X3, x3, b3, lambda3),\n    ]\n\n    results = []\n    for case in test_cases:\n        X, x, b, lambda_val = case\n        result = compute_norm_diff(X, x, b, lambda_val)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_norm_diff(X, x, b, lambda_val):\n    \"\"\"\n    Computes the norm of the difference between the Sherman-Morrison\n    method and the direct Cholesky solve method.\n\n    Args:\n        X (np.ndarray): Data matrix of shape (m, n).\n        x (np.ndarray): New sample vector of shape (n,).\n        b (np.ndarray): Vector on which the inverse acts, shape (n,).\n        lambda_val (float): Regularization parameter.\n\n    Returns:\n        float: The Euclidean norm of the difference ||y_sm - y_ref||_2.\n    \"\"\"\n    n = X.shape[1]\n    I_n = np.identity(n)\n    \n    # Form the matrix A = X'X + lambda*I\n    A = X.T @ X + lambda_val * I_n\n    \n    # Method 1: Using Sherman-Morrison formula\n    # This method computes y_sm = (A + xx')^{-1} b\n    #\n    # y_sm = A^{-1}b - (A^{-1}x * (x' * A^{-1}b)) / (1 + x' * A^{-1}x)\n    #\n    # We define z = A^{-1}b and w = A^{-1}x and solve linear systems.\n    # y_sm = z - (w * (x'z)) / (1 + x'w)\n\n    # Compute Cholesky factorization of A, L where A = LL'\n    # Use lower=True to get the lower-triangular factor L\n    try:\n        L = cholesky(A, lower=True)\n    except np.linalg.LinAlgError:\n        # This should not happen since A is proven to be SPD\n        return float('nan')\n\n    # Solve Az = b for z using the Cholesky factor\n    z = cho_solve((L, True), b)\n    \n    # Solve Aw = x for w using the same Cholesky factor\n    w = cho_solve((L, True), x)\n    \n    # Compute the scalar components of the Sherman-Morrison formula\n    # alpha = x'z\n    alpha = x.T @ z\n    # beta = 1 + x'w\n    beta = 1.0 + x.T @ w\n    \n    # Compute y_sm\n    y_sm = z - (alpha / beta) * w\n\n    # Method 2: Direct solve for verification (y_ref)\n    # This involves forming the updated matrix A_new = A + xx' and solving\n    # A_new * y_ref = b\n    \n    # Form the updated matrix\n    A_new = A + np.outer(x, x)\n    \n    # Compute Cholesky factorization of A_new\n    try:\n        L_new = cholesky(A_new, lower=True)\n    except np.linalg.LinAlgError:\n        # A_new is also guaranteed to be SPD\n        return float('nan')\n\n    # Solve for the reference solution y_ref\n    y_ref = cho_solve((L_new, True), b)\n    \n    # Compute and return the Euclidean norm of the difference\n    return np.linalg.norm(y_sm - y_ref)\n\nsolve()\n\n```"
        }
    ]
}