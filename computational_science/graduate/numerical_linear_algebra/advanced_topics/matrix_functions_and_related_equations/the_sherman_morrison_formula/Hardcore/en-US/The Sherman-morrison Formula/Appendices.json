{
    "hands_on_practices": [
        {
            "introduction": "The Sherman-Morrison formula is powerful, but its applicability hinges on a crucial condition. This first exercise  is a hands-on exploration of the formula's breaking point. By constructing a specific scenario where the updated matrix becomes singular, you will gain a deeper understanding of why the denominator term $1 + v^T A^{-1} u$ is the key determinant of the formula's validity.",
            "id": "3596884",
            "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$ be the symmetric, strictly diagonally dominant matrix\n$$\nA=\\begin{pmatrix}\n4  1  0 \\\\\n1  4  1 \\\\\n0  1  4\n\\end{pmatrix},\n$$\nand let $u=\\begin{pmatrix}1 \\\\ 0 \\\\ 1\\end{pmatrix}$. Choose $w=\\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$ and define $v \\in \\mathbb{R}^{3}$ by $v=-A^{T} w$. Consider the rank-one update $A+u v^{T}$ and the scalar\n$$\ns \\equiv 1+v^{T} A^{-1} u.\n$$\nUsing only fundamental properties of invertibility, kernels, and rank-one outer products, and without appealing to any specialized update identities, do the following:\n\n- Compute the exact value of $s$.\n- Using your computation, construct a nonzero vector in the kernel of $A+u v^{T}$ and justify why this proves $A+u v^{T}$ is not invertible.\n- Explain precisely how and why the Sherman–Morrison formula fails for this choice of $A$, $u$, and $v$, in terms of the linear-algebraic mechanism you have established.\n\nYour final reported answer must be the exact value of $s$. No rounding is required.",
            "solution": "The problem is validated as follows.\n### Step 1: Extract Givens\n- The matrix $A \\in \\mathbb{R}^{3 \\times 3}$ is given as $A=\\begin{pmatrix} 4  1  0 \\\\ 1  4  1 \\\\ 0  1  4 \\end{pmatrix}$. It is stated to be symmetric and strictly diagonally dominant.\n- The vector $u \\in \\mathbb{R}^{3}$ is given as $u=\\begin{pmatrix}1 \\\\ 0 \\\\ 1\\end{pmatrix}$.\n- The vector $w \\in \\mathbb{R}^{3}$ is given as $w=\\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$.\n- The vector $v \\in \\mathbb{R}^{3}$ is defined by the relation $v=-A^{T} w$.\n- The scalar $s$ is defined by the relation $s \\equiv 1+v^{T} A^{-1} u$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a standard exercise in numerical linear algebra. The concepts of matrix invertibility, kernels, rank-one updates, and the Sherman-Morrison formula are all fundamental and well-established in mathematics.\n- **Well-Posed:** The matrix $A$ is strictly diagonally dominant, which guarantees that it is invertible. Therefore, $A^{-1}$ exists and is unique. The vectors $u$, $v$, $w$ are well-defined, and consequently, the scalar $s$ is also well-defined. The tasks are clearly stated and lead to a unique, meaningful solution.\n- **Objective:** The problem is formulated using precise mathematical language, free from any subjectivity or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is self-contained, logically consistent, and mathematically sound. We may proceed with the solution.\n\nThe solution proceeds in three parts as requested by the problem statement.\n\n**Part 1: Compute the exact value of $s$.**\n\nThe scalar $s$ is defined as $s = 1+v^{T} A^{-1} u$. The vector $v$ is defined as $v=-A^{T} w$. We can substitute the definition of $v$ into the expression for $s$.\nFirst, we find the transpose of $v$:\n$$\nv^T = (-A^T w)^T = -w^T (A^T)^T = -w^T A\n$$\nNow, substitute this expression for $v^T$ into the formula for $s$:\n$$\ns = 1 + (-w^T A) A^{-1} u\n$$\nUsing the associative property of matrix multiplication, we can group the terms $A$ and $A^{-1}$:\n$$\ns = 1 - w^T (A A^{-1}) u\n$$\nSince $A A^{-1} = I$, where $I$ is the $3 \\times 3$ identity matrix, the expression simplifies to:\n$$\ns = 1 - w^T I u = 1 - w^T u\n$$\nThis simplification allows us to compute $s$ without needing to calculate $A^{-1}$. We are given the vectors $w$ and $u$:\n$$\nw = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad u = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nThe dot product $w^T u$ is:\n$$\nw^T u = \\begin{pmatrix} 1  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = (1)(1) + (0)(0) + (0)(1) = 1\n$$\nFinally, we substitute this value back into the expression for $s$:\n$$\ns = 1 - 1 = 0\n$$\nThe exact value of $s$ is $0$.\n\n**Part 2: Construct a nonzero vector in the kernel of $A+u v^{T}$ and justify its non-invertibility.**\n\nLet the updated matrix be $B = A+u v^{T}$. A vector $x$ is in the kernel of $B$ if $Bx=0$. We are seeking a nonzero vector $x$ that satisfies this condition.\nThe equation is:\n$$\n(A+u v^{T})x = 0\n$$\n$$\nAx + u(v^T x) = 0\n$$\nThe term $v^T x$ is a scalar. Let's denote it by $\\alpha = v^T x$. The equation becomes:\n$$\nAx + \\alpha u = 0\n$$\nSince $A$ is invertible, we can solve for $x$:\n$$\nAx = -\\alpha u \\implies x = A^{-1}(-\\alpha u) = -\\alpha (A^{-1}u)\n$$\nThis equation shows that any vector $x$ in the kernel of $B$ must be a scalar multiple of the vector $A^{-1}u$. Let's choose our candidate vector to be $x_k = A^{-1}u$. To be a valid kernel vector, it must be nonzero and satisfy the original equation. Since $A$ is invertible and $u \\neq 0$, $x_k = A^{-1}u$ must be nonzero.\n\nNow, we substitute $x_k=A^{-1}u$ back into the kernel equation $(A+u v^{T})x_k = 0$:\n$$\n(A+u v^{T})(A^{-1}u) = A(A^{-1}u) + u v^{T}(A^{-1}u) = Iu + u(v^{T}A^{-1}u) = u(1+v^{T}A^{-1}u)\n$$\nThe expression in the parentheses is precisely the scalar $s$. So, we have:\n$$\n(A+u v^{T})x_k = s u\n$$\nFrom Part 1, we calculated $s=0$. Therefore:\n$$\n(A+u v^{T})x_k = 0 \\cdot u = 0\n$$\nThis confirms that $x_k = A^{-1}u$ is indeed a vector in the kernel of $A+u v^{T}$. As argued, this vector is nonzero.\n\nTo explicitly construct this vector, we need to compute $A^{-1}$. The determinant of $A$ is $\\det(A) = 4(16-1) - 1(4-0) = 56$. The adjugate matrix of $A$ is:\n$$\n\\text{adj}(A) = \\begin{pmatrix}\n15  -4  1 \\\\\n-4  16  -4 \\\\\n1  -4  15\n\\end{pmatrix}\n$$\nSo, the inverse is:\n$$\nA^{-1} = \\frac{1}{56}\\begin{pmatrix}\n15  -4  1 \\\\\n-4  16  -4 \\\\\n1  -4  15\n\\end{pmatrix}\n$$\nNow we compute the kernel vector $x_k = A^{-1}u$:\n$$\nx_k = \\frac{1}{56}\\begin{pmatrix}\n15  -4  1 \\\\\n-4  16  -4 \\\\\n1  -4  15\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{56}\\begin{pmatrix}\n15+1 \\\\\n-4-4 \\\\\n1+15\n\\end{pmatrix} = \\frac{1}{56}\\begin{pmatrix}\n16 \\\\\n-8 \\\\\n16\n\\end{pmatrix} = \\frac{8}{56}\\begin{pmatrix}\n2 \\\\\n-1 \\\\\n2\n\\end{pmatrix} = \\frac{1}{7}\\begin{pmatrix}\n2 \\\\\n-1 \\\\\n2\n\\end{pmatrix}\n$$\nA valid nonzero vector in the kernel is any non-zero scalar multiple of this, for instance, $\\begin{pmatrix} 2 \\\\ -1 \\\\ 2 \\end{pmatrix}$.\n\nA square matrix is invertible if and only if its kernel (or null space) contains only the zero vector. Since we have constructed a nonzero vector in the kernel of $A+u v^{T}$, this proves that the matrix $A+u v^{T}$ is singular (i.e., not invertible).\n\n**Part 3: Explain the failure of the Sherman–Morrison formula.**\n\nThe Sherman-Morrison formula provides an expression for the inverse of a rank-one update to an invertible matrix $A$. The formula is:\n$$\n(A+u v^{T})^{-1} = A^{-1} - \\frac{A^{-1}uv^{T}A^{-1}}{1+v^{T}A^{-1}u}\n$$\nThis formula is applicable only if the denominator is nonzero. The denominator is exactly the scalar $s = 1+v^{T}A^{-1}u$.\n\nIn Part 1, we computed the exact value of this scalar and found that $s=0$.\n$$\n1+v^{T}A^{-1}u = 0\n$$\nThe Sherman-Morrison formula thus fails for this specific choice of $A$, $u$, and $v$ because it would require division by zero, which is an undefined operation.\n\nThis failure is not a mere computational issue; it is a direct consequence of the linear-algebraic mechanism established in Part 2. The condition $1+v^{T}A^{-1}u = 0$ is precisely the condition that ensures the existence of a nontrivial kernel for the matrix $A+u v^{T}$, which implies the matrix is not invertible. The Sherman-Morrison formula, being a formula for the inverse, correctly reflects the non-existence of this inverse by becoming undefined. The vanishing denominator is the mathematical signal that the updated matrix has become singular.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Moving from theory to practice, the true utility of the Sherman-Morrison identity lies in its ability to update solutions to linear systems efficiently. This exercise  puts you in the role of implementing a key step in streaming ridge regression. Your task is to develop a numerically stable workflow that avoids explicit matrix inversion by using Cholesky factorizations, demonstrating how the formula's principle is applied in high-performance scientific computing.",
            "id": "3596918",
            "problem": "You are given a symmetric positive definite matrix update scenario that arises in streaming ridge regression. Let $X \\in \\mathbb{R}^{m \\times n}$ be a data matrix and let $\\lambda \\in \\mathbb{R}$ with $\\lambda  0$. Define $A = X^{\\top} X + \\lambda I_n \\in \\mathbb{R}^{n \\times n}$, where $I_n$ is the $n \\times n$ identity matrix. Suppose a new sample $x \\in \\mathbb{R}^{n}$ arrives, and you must compute the action of $(A + x x^{\\top})^{-1}$ on a given vector $b \\in \\mathbb{R}^{n}$ without ever forming $A^{-1}$ or $(A + x x^{\\top})^{-1}$ explicitly. You must use only triangular solves with the Cholesky factorization of $A$ (and, for verification only, the Cholesky factorization of $A + x x^{\\top}$), and you must not form or invert any dense matrix directly.\n\nYour task is to design a numerically stable workflow, justified from first principles, that computes $(A + x x^{\\top})^{-1} b$ using only triangular solves with the Cholesky factor of $A$, and to verify it numerically by comparing to a direct solve using the Cholesky factor of $A + x x^{\\top}$. Specifically, for each test case defined below, compute $y_{\\mathrm{SM}} = (A + x x^{\\top})^{-1} b$ using only solves with the Cholesky factor of $A$, and compute $y_{\\mathrm{ref}}$ by solving $(A + x x^{\\top}) y_{\\mathrm{ref}} = b$ via its Cholesky factorization. Report the Euclidean norm $\\lVert y_{\\mathrm{SM}} - y_{\\mathrm{ref}} \\rVert_2$.\n\nConstraints and requirements:\n- Use only triangular solves with the Cholesky factorization of $A$ when constructing $y_{\\mathrm{SM}}$.\n- For verification, you may also use a Cholesky factorization of $A + x x^{\\top}$ followed by triangular solves to obtain $y_{\\mathrm{ref}}$.\n- Do not compute $A^{-1}$, $(A + x x^{\\top})^{-1}$, or any matrix inverse explicitly.\n- All computations are purely mathematical; there are no physical units involved.\n\nTest suite:\nProvide outputs for the following three test cases.\n\nTest case $1$:\n- $n = 4$, $m = 6$, $\\lambda = 0.5$.\n$$\nX = \\begin{bmatrix}\n1  0  -1  2 \\\\\n0  1  2  -1 \\\\\n2  -1  0  1 \\\\\n-1  2  1  0 \\\\\n1  1  1  1 \\\\\n3  -2  1  -1\n\\end{bmatrix}, \\quad x = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 2 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\\\ 0 \\end{bmatrix}.\n$$\nTest case $2$ (nearly collinear columns, small regularization):\n- $n = 3$, $m = 3$, $\\lambda = 1 \\times 10^{-8}$.\n$$\nX = \\begin{bmatrix}\n1  1  1 \\\\\n2  2.000001  2 \\\\\n3  3.000002  3\n\\end{bmatrix}, \\quad x = \\begin{bmatrix} 1 \\\\ 1.000001 \\\\ 1 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}.\n$$\nTest case $3$:\n- $n = 5$, $m = 7$, $\\lambda = 1$.\n$$\nX = \\begin{bmatrix}\n1  2  -1  0  3 \\\\\n0  -1  2  1  -2 \\\\\n2  0  1  -1  1 \\\\\n-1  3  0  2  -3 \\\\\n1  -2  -1  1  0 \\\\\n2  1  3  -2  1 \\\\\n-2  0  1  2  -1\n\\end{bmatrix}, \\quad x = \\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ -2 \\end{bmatrix}, \\quad b = \\begin{bmatrix} -1 \\\\ 0 \\\\ 2 \\\\ 1 \\\\ 3 \\end{bmatrix}.\n$$\nFinal output format:\nYour program should produce a single line of output containing the three results (one per test case) as a comma-separated list enclosed in square brackets, for example $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$, where each $\\text{result}_k$ is the Euclidean norm $\\lVert y_{\\mathrm{SM}} - y_{\\mathrm{ref}} \\rVert_2$ for test case $k$. The results must be floating-point numbers.",
            "solution": "The user provided a valid problem statement.\n\n### Step 1: Extract Givens\n- **Matrix Definition**: $A = X^{\\top} X + \\lambda I_n$, where $X \\in \\mathbb{R}^{m \\times n}$, $\\lambda \\in \\mathbb{R}$ with $\\lambda  0$, and $I_n$ is the $n \\times n$ identity matrix.\n- **Problem Context**: $A$ is symmetric positive definite.\n- **Update**: A new sample $x \\in \\mathbb{R}^{n}$ arrives, leading to the updated matrix $A + x x^{\\top}$.\n- **Objective**: Compute $y = (A + x x^{\\top})^{-1} b$ for a given vector $b \\in \\mathbb{R}^{n}$.\n- **Method 1 ($y_{\\mathrm{SM}}$)**: Use the Sherman-Morrison formula, relying only on triangular solves with the Cholesky factor of $A$.\n- **Method 2 ($y_{\\mathrm{ref}}$)**: For verification, compute $y_{\\mathrm{ref}}$ by solving $(A + x x^{\\top}) y_{\\mathrm{ref}} = b$ using the Cholesky factorization of the updated matrix $A + x x^{\\top}$.\n- **Constraints**:\n    - Do not form $A^{-1}$ or $(A + x x^{\\top})^{-1}$ explicitly.\n    - Do not form or invert any dense matrix directly, other than performing Cholesky factorizations.\n    - Use only triangular solves.\n- **Output**: Report the Euclidean norm $\\lVert y_{\\mathrm{SM}} - y_{\\mathrm{ref}} \\rVert_2$ for each test case.\n- **Test Cases**:\n    - **Case 1**: $n = 4$, $m = 6$, $\\lambda = 0.5$.\n      $X = \\begin{bmatrix} 1  0  -1  2 \\\\ 0  1  2  -1 \\\\ 2  -1  0  1 \\\\ -1  2  1  0 \\\\ 1  1  1  1 \\\\ 3  -2  1  -1 \\end{bmatrix}$, $x = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 2 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\\\ 0 \\end{bmatrix}$.\n    - **Case 2**: $n = 3$, $m = 3$, $\\lambda = 1 \\times 10^{-8}$.\n      $X = \\begin{bmatrix} 1  1  1 \\\\ 2  2.000001  2 \\\\ 3  3.000002  3 \\end{bmatrix}$, $x = \\begin{bmatrix} 1 \\\\ 1.000001 \\\\ 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}$.\n    - **Case 3**: $n = 5$, $m = 7$, $\\lambda = 1$.\n      $X = \\begin{bmatrix} 1  2  -1  0  3 \\\\ 0  -1  2  1  -2 \\\\ 2  0  1  -1  1 \\\\ -1  3  0  2  -3 \\\\ 1  -2  -1  1  0 \\\\ 2  1  3  -2  1 \\\\ -2  0  1  2  -1 \\end{bmatrix}$, $x = \\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ -2 \\end{bmatrix}$, $b = \\begin{bmatrix} -1 \\\\ 0 \\\\ 2 \\\\ 1 \\\\ 3 \\end{bmatrix}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, objective, and complete.\n\n- **Scientific Grounding**: The problem is rooted in numerical linear algebra, specifically concerning efficient updates for linear systems common in machine learning (e.g., streaming ridge regression). The matrix $A = X^T X + \\lambda I_n$ is the normal equations matrix for ridge regression. It is guaranteed to be symmetric and positive definite (SPD).\n    - **Symmetry**: $A^T = (X^T X + \\lambda I_n)^T = (X^T X)^T + (\\lambda I_n)^T = X^T(X^T)^T + \\lambda I_n^T = X^T X + \\lambda I_n = A$.\n    - **Positive Definiteness**: For any non-zero vector $v \\in \\mathbb{R}^n$, the quadratic form is $v^T A v = v^T(X^T X + \\lambda I_n)v = v^T X^T X v + \\lambda v^T I_n v = (Xv)^T(Xv) + \\lambda v^T v = \\lVert Xv \\rVert_2^2 + \\lambda \\lVert v \\rVert_2^2$. Since $\\lambda > 0$ and $v \\neq 0$, $\\lambda \\lVert v \\rVert_2^2 > 0$. The term $\\lVert Xv \\rVert_2^2 \\ge 0$. Thus, $v^T A v > 0$.\nSince $A$ is SPD, its Cholesky factorization exists. The rank-$1$ update $A + xx^T$ is also SPD, as $v^T(A+xx^T)v = v^TAv + (v^Tx)^2 > 0$. The Sherman-Morrison formula is a standard, valid mathematical identity.\n\n- **Well-Posed**: A unique solution exists for the linear systems involved because the matrices $A$ and $A+xx^T$ are invertible. The problem specifies a clear objective and provides all necessary data.\n\n- **Objective**: The problem is stated using precise mathematical language and contains no subjective elements.\n\n- **No other flaws**: The problem is a standard exercise, not trivial, and its requirements are consistent and feasible.\n\n### Step 3: Verdict and Action\nThe problem is valid and I will proceed with the solution.\n\n### Principle-Based Solution\n\nThe objective is to compute $y = (A + x x^{\\top})^{-1} b$, where $A = X^{\\top} X + \\lambda I_n$, by leveraging the structure of the update. The matrix $A$ is altered by a rank-$1$ update, $x x^{\\top}$. The Sherman-Morrison formula is designed for precisely this situation.\n\nThe formula states that for an invertible matrix $B$ and vectors $u, v$, the inverse of a rank-$1$ update is:\n$$\n(B + u v^{\\top})^{-1} = B^{-1} - \\frac{B^{-1} u v^{\\top} B^{-1}}{1 + v^{\\top} B^{-1} u}\n$$\nThis formula is applicable provided that the denominator $1 + v^{\\top} B^{-1} u \\neq 0$.\n\nFor our problem, we set $B = A$, $u = x$, and $v = x$. The matrix $A$ is SPD, which implies that its inverse $A^{-1}$ is also SPD. For any non-zero vector $x$, the quadratic form $x^{\\top} A^{-1} x > 0$. Therefore, the denominator $1 + x^{\\top} A^{-1} x$ is always greater than $1$, ensuring the formula is well-defined.\n\nApplying the formula to our problem, we get:\n$$\n(A + x x^{\\top})^{-1} = A^{-1} - \\frac{A^{-1} x x^{\\top} A^{-1}}{1 + x^{\\top} A^{-1} x}\n$$\nWe need to compute the action of this inverse on the vector $b$:\n$$\ny_{\\mathrm{SM}} = (A + x x^{\\top})^{-1} b = \\left( A^{-1} - \\frac{A^{-1} x x^{\\top} A^{-1}}{1 + x^{\\top} A^{-1} x} \\right) b\n$$\nBy distributing $b$, we obtain:\n$$\ny_{\\mathrm{SM}} = A^{-1} b - \\frac{A^{-1} x (x^{\\top} A^{-1} b)}{1 + x^{\\top} A^{-1} x}\n$$\nThe problem explicitly forbids computing $A^{-1}$. We can reformulate the expression in terms of solutions to linear systems. Let's define two intermediate vectors, $z$ and $w$:\n1.  $z = A^{-1} b$, which is the solution to the linear system $Az = b$.\n2.  $w = A^{-1} x$, which is the solution to the linear system $Aw = x$.\n\nSubstituting $z$ and $w$ into the expression for $y_{\\mathrm{SM}}$ gives:\n$$\ny_{\\mathrm{SM}} = z - \\frac{w (x^{\\top} z)}{1 + x^{\\top} w}\n$$\nThis formulation avoids matrix inversion. The terms $x^{\\top} z$ and $x^{\\top} w$ are scalar dot products. The overall computation is a series of well-defined steps that adhere to the problem's constraints.\n\n**Algorithm for $y_{\\mathrm{SM}}$**:\n1.  Construct the matrix $A = X^{\\top} X + \\lambda I_n$.\n2.  Since $A$ is SPD, compute its Cholesky factorization $A = LL^{\\top}$, where $L$ is a lower triangular matrix.\n3.  Solve $Az = b$ for $z$. This is done efficiently using the Cholesky factor $L$, by first solving $Ly' = b$ (forward substitution) and then $L^{\\top}z = y'$ (backward substitution).\n4.  Solve $Aw = x$ for $w$ using the same factors $L$ and $L^{\\top}$.\n5.  Compute the scalar numerator $\\alpha = x^{\\top} z$.\n6.  Compute the scalar denominator $\\beta = 1 + x^{\\top} w$.\n7.  The final result is $y_{\\mathrm{SM}} = z - (\\alpha / \\beta) w$.\n\n**Algorithm for Verification ($y_{\\mathrm{ref}}$)**:\nTo verify the result, we solve the system $(A + x x^{\\top}) y = b$ directly, which provides a reference solution $y_{\\mathrm{ref}}$.\n1.  Construct the matrix $A = X^{\\top} X + \\lambda I_n$.\n2.  Construct the updated matrix $A_{\\mathrm{new}} = A + x x^{\\top}$.\n3.  Since $A_{\\mathrm{new}}$ is also SPD, compute its Cholesky factorization $A_{\\mathrm{new}} = L_{\\mathrm{new}}L_{\\mathrm{new}}^{\\top}$.\n4.  Solve $A_{\\mathrm{new}} y_{\\mathrm{ref}} = b$ using the factors $L_{\\mathrm{new}}$ and $L_{\\mathrm{new}}^{\\top}$.\n\nFinally, we compute the Euclidean norm of the difference, $\\lVert y_{\\mathrm{SM}} - y_{\\mathrm{ref}} \\rVert_2$. Due to the finite precision of floating-point arithmetic, this value will be very small (on the order of machine epsilon) but likely non-zero, confirming the numerical equivalence of the two methods.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, cho_solve\n\ndef solve():\n    \"\"\"\n    Solves the problem for all given test cases and prints the results.\n    \"\"\"\n    \n    # Test case 1\n    X1 = np.array([\n        [1, 0, -1, 2],\n        [0, 1, 2, -1],\n        [2, -1, 0, 1],\n        [-1, 2, 1, 0],\n        [1, 1, 1, 1],\n        [3, -2, 1, -1]\n    ])\n    x1 = np.array([1, -1, 0, 2])\n    b1 = np.array([1, 2, -1, 0])\n    lambda1 = 0.5\n    \n    # Test case 2\n    X2 = np.array([\n        [1, 1, 1],\n        [2, 2.000001, 2],\n        [3, 3.000002, 3]\n    ])\n    x2 = np.array([1, 1.000001, 1])\n    b2 = np.array([1, -1, 0])\n    lambda2 = 1e-8\n    \n    # Test case 3\n    X3 = np.array([\n        [1, 2, -1, 0, 3],\n        [0, -1, 2, 1, -2],\n        [2, 0, 1, -1, 1],\n        [-1, 3, 0, 2, -3],\n        [1, -2, -1, 1, 0],\n        [2, 1, 3, -2, 1],\n        [-2, 0, 1, 2, -1]\n    ])\n    x3 = np.array([2, -1, 0, 1, -2])\n    b3 = np.array([-1, 0, 2, 1, 3])\n    lambda3 = 1.0\n        \n    test_cases = [\n        (X1, x1, b1, lambda1),\n        (X2, x2, b2, lambda2),\n        (X3, x3, b3, lambda3),\n    ]\n\n    results = []\n    for case in test_cases:\n        X, x, b, lambda_val = case\n        result = compute_norm_diff(X, x, b, lambda_val)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_norm_diff(X, x, b, lambda_val):\n    \"\"\"\n    Computes the norm of the difference between the Sherman-Morrison\n    method and the direct Cholesky solve method.\n\n    Args:\n        X (np.ndarray): Data matrix of shape (m, n).\n        x (np.ndarray): New sample vector of shape (n,).\n        b (np.ndarray): Vector on which the inverse acts, shape (n,).\n        lambda_val (float): Regularization parameter.\n\n    Returns:\n        float: The Euclidean norm of the difference ||y_sm - y_ref||_2.\n    \"\"\"\n    n = X.shape[1]\n    I_n = np.identity(n)\n    \n    # Form the matrix A = X'X + lambda*I\n    A = X.T @ X + lambda_val * I_n\n    \n    # Method 1: Using Sherman-Morrison formula\n    # This method computes y_sm = (A + xx')^{-1} b\n    #\n    # y_sm = A^{-1}b - (A^{-1}x * (x' * A^{-1}b)) / (1 + x' * A^{-1}x)\n    #\n    # We define z = A^{-1}b and w = A^{-1}x and solve linear systems.\n    # y_sm = z - (w * (x'z)) / (1 + x'w)\n\n    # Compute Cholesky factorization of A, L where A = LL'\n    # Use lower=True to get the lower-triangular factor L\n    try:\n        L = cholesky(A, lower=True)\n    except np.linalg.LinAlgError:\n        # This should not happen since A is proven to be SPD\n        return float('nan')\n\n    # Solve Az = b for z using the Cholesky factor\n    z = cho_solve((L, True), b)\n    \n    # Solve Aw = x for w using the same Cholesky factor\n    w = cho_solve((L, True), x)\n    \n    # Compute the scalar components of the Sherman-Morrison formula\n    # alpha = x'z\n    alpha = x.T @ z\n    # beta = 1 + x'w\n    beta = 1.0 + x.T @ w\n    \n    # Compute y_sm\n    y_sm = z - (alpha / beta) * w\n\n    # Method 2: Direct solve for verification (y_ref)\n    # This involves forming the updated matrix A_new = A + xx' and solving\n    # A_new * y_ref = b\n    \n    # Form the updated matrix\n    A_new = A + np.outer(x, x)\n    \n    # Compute Cholesky factorization of A_new\n    try:\n        L_new = cholesky(A_new, lower=True)\n    except np.linalg.LinAlgError:\n        # A_new is also guaranteed to be SPD\n        return float('nan')\n\n    # Solve for the reference solution y_ref\n    y_ref = cho_solve((L_new, True), b)\n    \n    # Compute and return the Euclidean norm of the difference\n    return np.linalg.norm(y_sm - y_ref)\n\nsolve()\n\n```"
        },
        {
            "introduction": "In the world of finite-precision arithmetic, matrices are rarely perfectly singular; they are often ill-conditioned, and updates can degrade numerical accuracy. This final practice  delves into these practical challenges of numerical stability. You will design and implement a sophisticated acceptance criterion, grounded in floating-point error analysis, to decide whether a Sherman-Morrison update is trustworthy or if a full recalculation is the safer choice—a common dilemma in developing robust numerical software.",
            "id": "3596945",
            "problem": "You are given an invertible matrix $A \\in \\mathbb{R}^{n \\times n}$ and a proposed rank-one update $A + u v^{\\top}$, with $u,v \\in \\mathbb{R}^n$. The goal is to decide, using a principled floating-point error analysis, whether to accept applying a rank-one inverse update derived from the rank-one perturbation model, or to reject it and recompute a factorization of $A + u v^{\\top}$ instead.\n\nDesign a computable, sufficient acceptance criterion that is justified from first principles in floating-point arithmetic. Your design must rely only on the following fundamental starting points:\n\n- The standard floating-point rounding model for arithmetic operations: for basic operations $a \\circ b \\in \\{a+b, a-b, a \\cdot b, a/b\\}$ computed in floating-point as $\\mathrm{fl}(a \\circ b)$, there exists $\\delta$ with $|\\delta| \\leq u$ such that $\\mathrm{fl}(a \\circ b) = (a \\circ b)(1+\\delta)$, where $u$ is the unit roundoff. In binary64 (IEEE $754$ double precision), take $u$ to be half the machine epsilon, i.e., $u = 2^{-53}$.\n- A normwise error bound for dot products: for any $x,y \\in \\mathbb{R}^n$, a standard model yields $\\mathrm{fl}(x^{\\top} y) = x^{\\top} y \\,(1 + \\theta_n)$ with $|\\theta_n| \\leq \\gamma_n$, where $\\gamma_n = \\dfrac{n u}{1 - n u}$.\n- Backward error for linear solves: the computed solution $\\widehat{w}$ to $A w = u$ is the exact solution to $(A + \\Delta A_w)\\widehat{w} = u$, for some perturbation $\\Delta A_w$ with $\\|\\Delta A_w\\| \\lesssim u \\|A\\|$. A computable proxy for the backward error is the residual $r_w = A \\widehat{w} - u$. Similarly, for the computed solution $\\widehat{z}$ to $A^{\\top} z = v$, the residual is $r_z = A^{\\top} \\widehat{z} - v$.\n- The matrix determinant lemma: $\\det(A + u v^{\\top}) = \\det(A)\\left(1 + v^{\\top} A^{-1} u\\right)$. This characterizes potential singularity of the updated matrix when $1 + v^{\\top} A^{-1} u = 0$.\n- First-order perturbation theory for matrix inversion: for a small perturbation $E$, $(A+E)^{-1} - A^{-1} \\approx -A^{-1} E A^{-1}$ in norm, hence a rank-one perturbation $E = u v^{\\top}$ induces a first-order change with magnitude on the order of $\\|A^{-1} u\\| \\,\\|A^{-\\top} v\\|$.\n\nUsing these facts, do the following:\n\n1. Derive a sufficient, computable acceptance test for applying a rank-one inverse update to reflect $A \\mapsto A + u v^{\\top}$ that has two safeguards:\n   - A singularity proximity test, based on estimating the floating-point uncertainty in the scalar $d = 1 + v^{\\top} w$, where $w = A^{-1} u$. Construct a bound on the absolute error in the computed $\\widehat{d} = 1 + \\mathrm{fl}(v^{\\top} \\widehat{w})$ using the $\\gamma_n$-bound for the dot product and the residuals $r_w$ and $r_z$, together with a condition number factor $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. State clearly the bound you use and justify each term from the listed principles. Then require that $|\\widehat{d}|$ exceeds a prescribed multiple of this bound.\n   - A stability test for the magnitude of the induced inverse change, using the first-order bound $\\|A^{-1} u\\|_2 \\,\\|A^{-\\top} v\\|_2$ combined with the unit roundoff $u$ to form a predicted relative update size. Require that this prediction be below a user-chosen tolerance.\n2. Fix all constants numerically as follows:\n   - Use the binary64 unit roundoff $u = 2^{-53}$ and $\\gamma_n = \\dfrac{n u}{1 - n u}$.\n   - For the singularity proximity test, require $|\\widehat{d}|  c_{\\mathrm{den}} \\cdot \\mathrm{err}_d$ with $c_{\\mathrm{den}} = 10$, where $\\mathrm{err}_d$ is your absolute error bound for $d$.\n   - For the induced-update stability test, require $c_{\\mathrm{upd}} \\, u \\, \\|\\widehat{w}\\|_2 \\, \\|\\widehat{z}\\|_2 \\leq \\tau$ with $c_{\\mathrm{upd}} = 10$ and $\\tau = 10^{-2}$.\n3. Implement a program that, given a small suite of test matrices and vectors, evaluates your acceptance criterion and outputs a list of booleans indicating whether to accept the rank-one inverse update or to recompute a fresh factorization.\n\nImplementation details and computational choices:\n- Use the spectral norm $\\|\\cdot\\|_2$ for matrices and vectors wherever a norm is needed.\n- Estimate $\\|A^{-1}\\|_2$ via the condition number $\\kappa_2(A)$ computed in the spectral norm, i.e., $\\|A^{-1}\\|_2 = \\kappa_2(A)/\\|A\\|_2$.\n- Use the following absolute error bound for the scalar $d$, motivated by the listed principles:\n  $$\n  \\mathrm{err}_d \\;\\equiv\\; \\underbrace{\\gamma_n \\sum_{i=1}^n |v_i|\\,|\\widehat{w}_i|}_{\\text{dot-product rounding}} \\;+\\; \\underbrace{\\|v\\|_2 \\,\\|A^{-1}\\|_2 \\,\\|r_w\\|_2}_{\\text{solve for } w} \\;+\\; \\underbrace{\\|\\widehat{w}\\|_2 \\,\\|A^{-1}\\|_2 \\,\\|r_z\\|_2}_{\\text{solve for } z},\n  $$\n  where $r_w = A \\widehat{w} - u$ and $r_z = A^{\\top} \\widehat{z} - v$.\n- The final acceptance decision is the logical conjunction of the two safeguards.\n\nTest suite:\nProvide a decision for each of the following cases. All entries are real numbers and should be treated as dimensionless.\n\n- Case $1$ (well-conditioned, benign update):\n  - $A_1 = \\begin{bmatrix} 3  1  0 \\\\ 1  4  1 \\\\ 0  1  3 \\end{bmatrix}$,\n  - $u_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$,\n  - $v_1 = \\begin{bmatrix} 0.05 \\\\ 0.1 \\\\ -0.05 \\end{bmatrix}$.\n\n- Case $2$ (denominator near zero, constructed):\n  - Use the same $A_1$ as in Case $1$.\n  - $u_2 = \\begin{bmatrix} 10^{-4} \\\\ -2\\cdot 10^{-4} \\\\ 3\\cdot 10^{-4} \\end{bmatrix}$.\n  - Define $\\widehat{w}_2$ as the computed solution to $A_1 w = u_2$, and set\n    $$\n    v_2 \\;=\\; -\\,\\dfrac{\\widehat{w}_2}{\\widehat{w}_2^{\\top}\\widehat{w}_2} \\;+\\; 10^{-12} \\begin{bmatrix} -0.3 \\\\ 0.2 \\\\ -0.1 \\end{bmatrix}.\n    $$\n    This choice drives $1 + v_2^{\\top} \\widehat{w}_2$ close to zero in exact arithmetic.\n\n- Case $3$ (highly ill-conditioned matrix, moderate update):\n  - $A_3$ is the $10 \\times 10$ Hilbert matrix with entries $(A_3)_{ij} = \\dfrac{1}{i + j - 1}$ for $i,j \\in \\{1,\\dots,10\\}$.\n  - $u_3 = \\begin{bmatrix} 10^{-2} \\\\ -10^{-2} \\\\ 10^{-2} \\\\ -10^{-2} \\\\ 10^{-2} \\\\ -10^{-2} \\\\ 10^{-2} \\\\ -10^{-2} \\\\ 10^{-2} \\\\ -10^{-2} \\end{bmatrix}$,\n  - $v_3 = \\begin{bmatrix} 10^{-2} \\\\ 10^{-2} \\\\ -10^{-2} \\\\ -10^{-2} \\\\ 10^{-2} \\\\ 10^{-2} \\\\ -10^{-2} \\\\ -10^{-2} \\\\ 10^{-2} \\\\ 10^{-2} \\end{bmatrix}$.\n\n- Case $4$ (boundary case with identity, small but safe denominator):\n  - $A_4 = I_3$ (the $3 \\times 3$ identity matrix),\n  - $u_4 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $v_4 = \\begin{bmatrix} -1 + 2\\cdot 10^{-14} \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n\nRequired program behavior:\n- Implement the acceptance criterion exactly as specified.\n- For each case, compute the boolean decision: $\\,\\mathrm{True}$ if the update is accepted, $\\,\\mathrm{False}$ otherwise.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\") in the order of the cases $1$ through $4$.",
            "solution": "We design a principled, computable acceptance criterion for applying a rank-one inverse update under floating-point arithmetic, based on fundamentals. Let $A \\in \\mathbb{R}^{n \\times n}$ be nonsingular, and consider the rank-one update $A + u v^{\\top}$ with vectors $u,v \\in \\mathbb{R}^n$. The goal is to detect two potential numerical difficulties: (i) the updated matrix approaching singularity, which makes any explicit inverse update ill-conditioned or undefined, and (ii) excessive amplification of rounding due to a large first-order update magnitude.\n\nFloating-point model and dot-product error: In binary64, the unit roundoff is $u = 2^{-53}$. For a length-$n$ dot product, a standard model yields\n$$\n\\mathrm{fl}(x^{\\top} y) = x^{\\top} y \\,(1 + \\theta_n), \\quad |\\theta_n| \\le \\gamma_n, \\quad \\gamma_n = \\frac{n u}{1 - n u}.\n$$\nThus, for any $x,y \\in \\mathbb{R}^n$, the absolute rounding error in the computed dot product obeys\n$$\n|\\mathrm{fl}(x^{\\top} y) - x^{\\top} y| \\le \\gamma_n \\sum_{i=1}^n |x_i|\\,|y_i|.\n$$\n\nBackward error for linear solves: Let $\\widehat{w}$ be the computed solution to $A w = u$. There exists a perturbation $\\Delta A_w$ such that $(A + \\Delta A_w)\\widehat{w} = u$ with $\\|\\Delta A_w\\| \\lesssim u \\|A\\|$. The residual $r_w = A \\widehat{w} - u$ satisfies $r_w = -\\Delta A_w \\widehat{w}$. Hence, a bound on the effect on any linear functional can be obtained by\n$$\n|\\ell^{\\top} \\widehat{w} - \\ell^{\\top} w| = |\\ell^{\\top}(\\widehat{w} - w)| \\le \\|\\ell\\|_2 \\,\\|\\widehat{w} - w\\|_2 \\le \\|\\ell\\|_2 \\,\\|A^{-1}\\|_2 \\,\\|r_w\\|_2.\n$$\nWe apply this with $\\ell = v$ to bound the error in $v^{\\top} \\widehat{w}$. Similarly, for the computed solution $\\widehat{z}$ to $A^{\\top} z = v$ with residual $r_z = A^{\\top} \\widehat{z} - v$, we obtain a symmetric bound on the effect of the inexact $\\widehat{z}$ when used to approximate $A^{-\\top} v$.\n\nSingularity proximity via the matrix determinant lemma: The lemma states\n$$\n\\det(A + u v^{\\top}) = \\det(A)\\left(1 + v^{\\top} A^{-1} u\\right).\n$$\nHence, if $d \\equiv 1 + v^{\\top} w$ with $w = A^{-1} u$ is near zero, the updated matrix is near singular. In finite precision, we compute $\\widehat{w}$ and then $\\widehat{d} = 1 + \\mathrm{fl}(v^{\\top} \\widehat{w})$. The total absolute error in $\\widehat{d}$ can be bounded by three contributions:\n- Dot-product rounding:\n$$\n\\mathrm{err}_{\\mathrm{dot}} = \\gamma_n \\sum_{i=1}^n |v_i|\\,|\\widehat{w}_i|.\n$$\n- Inexact solve for $w$:\n$$\n\\mathrm{err}_{w} \\le \\|v\\|_2 \\,\\|A^{-1}\\|_2 \\,\\|r_w\\|_2, \\quad r_w = A \\widehat{w} - u.\n$$\n- Inexact solve for $z$ (symmetrized bound, accounting for the use of $A^{-\\top} v$):\n$$\n\\mathrm{err}_{z} \\le \\|\\widehat{w}\\|_2 \\,\\|A^{-1}\\|_2 \\,\\|r_z\\|_2, \\quad r_z = A^{\\top} \\widehat{z} - v.\n$$\nCombining them,\n$$\n\\mathrm{err}_d = \\mathrm{err}_{\\mathrm{dot}} + \\mathrm{err}_{w} + \\mathrm{err}_{z} \\;=\\; \\gamma_n \\sum_{i=1}^n |v_i|\\,|\\widehat{w}_i| \\;+\\; \\|A^{-1}\\|_2 \\left( \\|v\\|_2 \\,\\|r_w\\|_2 + \\|\\widehat{w}\\|_2 \\,\\|r_z\\|_2 \\right).\n$$\nWe estimate $\\|A^{-1}\\|_2$ from the spectral-norm condition number $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$, by computing $\\|A^{-1}\\|_2 = \\kappa_2(A)/\\|A\\|_2$.\n\nWe accept the update only if the magnitude of the computed $\\widehat{d}$ is well separated from zero relative to this bound, specifically\n$$\n|\\widehat{d}| > c_{\\mathrm{den}} \\,\\mathrm{err}_d, \\quad c_{\\mathrm{den}} = 10.\n$$\n\nMagnitude of the induced inverse update: First-order perturbation theory for inversion gives\n$$\n(A + u v^{\\top})^{-1} - A^{-1} \\;\\approx\\; -A^{-1} \\,(u v^{\\top}) \\,A^{-1},\n$$\nhence a normwise magnitude on the order of $\\|A^{-1} u\\|_2 \\,\\|A^{-\\top} v\\|_2$. In finite precision, forming an explicit rank-one correction suffers rounding on the order of the unit roundoff scaled by this magnitude. A conservative, computable predictor of the relative rounding impact is\n$$\n\\phi \\equiv c_{\\mathrm{upd}} \\, u \\,\\|\\widehat{w}\\|_2 \\,\\|\\widehat{z}\\|_2,\n$$\nwith $c_{\\mathrm{upd}} = 10$. We require $\\phi \\le \\tau$ with $\\tau = 10^{-2}$.\n\nAcceptance criterion: The update is accepted if and only if both\n$$\n|\\widehat{d}| > c_{\\mathrm{den}} \\,\\mathrm{err}_d\n\\quad \\text{and} \\quad\nc_{\\mathrm{upd}} \\, u \\,\\|\\widehat{w}\\|_2 \\,\\|\\widehat{z}\\|_2 \\le \\tau.\n$$\n\nAlgorithmic steps for each test case:\n1. Compute spectral norm $\\|A\\|_2$ and $\\kappa_2(A)$, then $\\|A^{-1}\\|_2 = \\kappa_2(A)/\\|A\\|_2$.\n2. Solve $A \\widehat{w} = u$ and $A^{\\top} \\widehat{z} = v$ (using stable direct solves), and residuals $r_w = A \\widehat{w} - u$, $r_z = A^{\\top} \\widehat{z} - v$.\n3. Compute $\\widehat{d} = 1 + \\mathrm{fl}(v^{\\top} \\widehat{w})$, $\\gamma_n = \\frac{n u}{1 - n u}$, and\n   $$\n   \\mathrm{err}_d = \\gamma_n \\sum_i |v_i|\\,|\\widehat{w}_i| + \\|A^{-1}\\|_2 \\big( \\|v\\|_2 \\,\\|r_w\\|_2 + \\|\\widehat{w}\\|_2 \\,\\|r_z\\|_2 \\big).\n   $$\n4. Form $\\phi = c_{\\mathrm{upd}} \\, u \\,\\|\\widehat{w}\\|_2 \\,\\|\\widehat{z}\\|_2$.\n5. Return $\\mathrm{True}$ if $|\\widehat{d}| > c_{\\mathrm{den}} \\,\\mathrm{err}_d$ and $\\phi \\le \\tau$, else $\\mathrm{False}$.\n\nTest suite analysis and expected behavior:\n- Case $1$: $A_1$ is moderately well-conditioned; $u_1, v_1$ are small. The scalar $d$ is not close to zero, $\\mathrm{err}_d$ is tiny, and $\\phi \\ll \\tau$. Accept.\n- Case $2$: By construction, $v_2^{\\top} \\widehat{w}_2 \\approx -1$, hence $\\widehat{d} \\approx 0$ and the first test fails. Reject.\n- Case $3$: The Hilbert matrix $A_3$ is extremely ill-conditioned. Even if $\\widehat{d}$ is not small, the product $\\|\\widehat{w}\\|_2 \\,\\|\\widehat{z}\\|_2$ is very large, making $\\phi \\gg \\tau$. Reject.\n- Case $4$: $A_4 = I_3$ and $u_4, v_4$ yield $\\widehat{d} \\approx 2 \\cdot 10^{-14}$, comfortably exceeding $c_{\\mathrm{den}} \\,\\mathrm{err}_d$ since $\\mathrm{err}_d \\approx \\gamma_3 \\approx \\mathcal{O}(10^{-16})$, and $\\phi$ is tiny. Accept.\n\nThe implementation follows these steps verbatim and outputs a single line \"[b1,b2,b3,b4]\" with booleans for the four cases in order.",
            "answer": "```python\nimport numpy as np\n\ndef hilbert(n: int) -> np.ndarray:\n    i = np.arange(1, n + 1, dtype=float)\n    j = i[:, None]\n    return 1.0 / (i[None, :] + j - 1.0)\n\ndef decide_accept(A: np.ndarray, u_vec: np.ndarray, v_vec: np.ndarray,\n                  c_den: float = 10.0, c_upd: float = 10.0, tau: float = 1e-2) -> bool:\n    n = A.shape[0]\n    # Binary64 unit roundoff u = 2^{-53}\n    unit_roundoff = np.power(2.0, -53)\n    gamma_n = (n * unit_roundoff) / (1.0 - n * unit_roundoff)\n\n    # Spectral norm and inverse norm estimate via condition number\n    normA2 = np.linalg.norm(A, 2)\n    # Guard in case norm is zero (should not happen for nonsingular A)\n    if not np.isfinite(normA2) or normA2 == 0.0:\n        return False\n    cond2 = np.linalg.cond(A, 2)\n    inv_norm2 = cond2 / normA2\n\n    # Stable solves\n    w_hat = np.linalg.solve(A, u_vec)\n    z_hat = np.linalg.solve(A.T, v_vec)\n\n    # Residuals for backward error proxies\n    r_w = A @ w_hat - u_vec\n    r_z = A.T @ z_hat - v_vec\n\n    # Compute d_hat = 1 + fl(v^T w_hat)\n    # In Python/NumPy, the dot is performed in floating point; we accept it as fl(.).\n    d_hat = 1.0 + float(v_vec @ w_hat)\n\n    # Error bound components for d\n    err_dot = gamma_n * float(np.sum(np.abs(v_vec) * np.abs(w_hat)))\n    err_w = inv_norm2 * (np.linalg.norm(v_vec, 2) * np.linalg.norm(r_w, 2))\n    err_z = inv_norm2 * (np.linalg.norm(w_hat, 2) * np.linalg.norm(r_z, 2))\n    err_d = err_dot + err_w + err_z\n\n    # Safeguard 1: denominator safety\n    safe_den = (abs(d_hat) > c_den * err_d)\n\n    # Safeguard 2: induced update magnitude predictor\n    alpha = np.linalg.norm(w_hat, 2) * np.linalg.norm(z_hat, 2)\n    phi = c_upd * unit_roundoff * alpha\n    stable_update = (phi = tau)\n\n    return bool(safe_den and stable_update)\n\ndef solve():\n    # Define test cases\n    # Case 1\n    A1 = np.array([[3.0, 1.0, 0.0],\n                   [1.0, 4.0, 1.0],\n                   [0.0, 1.0, 3.0]], dtype=float)\n    u1 = np.array([0.1, -0.2, 0.05], dtype=float)\n    v1 = np.array([0.05, 0.1, -0.05], dtype=float)\n\n    # Case 2 (constructed near-zero denominator)\n    u2 = np.array([1e-4, -2e-4, 3e-4], dtype=float)\n    w2 = np.linalg.solve(A1, u2)\n    # Construct v2 to make 1 + v^T w approximately zero, with a tiny perturbation\n    tiny = 1e-12\n    v2 = -w2 / float(w2 @ w2) + tiny * np.array([-0.3, 0.2, -0.1], dtype=float)\n\n    # Case 3 (Hilbert 10)\n    A3 = hilbert(10)\n    u3 = np.array([1e-2 if (i % 2 == 0) else -1e-2 for i in range(10)], dtype=float)\n    v3 = np.array([1e-2, 1e-2, -1e-2, -1e-2, 1e-2, 1e-2, -1e-2, -1e-2, 1e-2, 1e-2], dtype=float)\n\n    # Case 4 (identity, small but safe denominator)\n    A4 = np.eye(3, dtype=float)\n    u4 = np.array([1.0, 0.0, 0.0], dtype=float)\n    v4 = np.array([-1.0 + 2e-14, 0.0, 0.0], dtype=float)\n\n    test_cases = [\n        (A1, u1, v1),\n        (A1, u2, v2),\n        (A3, u3, v3),\n        (A4, u4, v4),\n    ]\n\n    results = []\n    for (A, u_vec, v_vec) in test_cases:\n        decision = decide_accept(A, u_vec, v_vec, c_den=10.0, c_upd=10.0, tau=1e-2)\n        results.append(decision)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}