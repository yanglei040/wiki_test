{
    "hands_on_practices": [
        {
            "introduction": "Understanding the Schur complement begins with its definition, which arises naturally from block Gaussian elimination. This first practice provides a direct, hands-on calculation for a small, concrete matrix . By working through the steps of partitioning a matrix and applying the formula $S = D - C A^{-1} B$, you will build a foundational understanding of how the Schur complement is constructed.",
            "id": "1074949",
            "problem": "Consider the $3 \\times 3$ real matrix $M$ defined as:\n$$\nM = \\begin{bmatrix}\n4 & 1 & 2 \\\\\n3 & 5 & 6 \\\\\n4 & 7 & 8\n\\end{bmatrix}.\n$$\nPartition $M$ into blocks where the top-left block $A$ is $1 \\times 1$, the top-right block $B$ is $1 \\times 2$, the bottom-left block $C$ is $2 \\times 1$, and the bottom-right block $D$ is $2 \\times 2$. Using block Gaussian elimination, compute the Schur complement of $A$ in $M$, denoted as $S = D - C A^{-1} B$. Report the entry in the second row and first column of the resulting $2 \\times 2$ Schur complement matrix $S$.",
            "solution": "1. Partition the matrix $M$ as\n$$\nM = \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix}, \\quad A=[4], \\quad B=\\begin{bmatrix} 1 & 2 \\end{bmatrix}, \\quad C=\\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}, \\quad D=\\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}.\n$$\n2. Since $A$ is the scalar $4$, its inverse is\n$$\nA^{-1} = \\frac1{4}.\n$$\n3. Compute the product $C A^{-1} B$:\n$$\nC A^{-1} B = \\frac{1}{4} C B = \\frac{1}{4} \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} \\begin{bmatrix} 1 & 2 \\end{bmatrix} = \\frac{1}{4} \\begin{bmatrix} 3\\cdot1 & 3\\cdot2 \\\\ 4\\cdot1 & 4\\cdot2 \\end{bmatrix} = \\frac{1}{4} \\begin{bmatrix} 3 & 6 \\\\ 4 & 8 \\end{bmatrix} = \\begin{bmatrix} \\frac{3}{4} & \\frac{3}{2} \\\\ 1 & 2 \\end{bmatrix}.\n$$\n4. The Schur complement is\n$$\nS = D - C A^{-1} B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} - \\begin{bmatrix} \\frac{3}{4} & \\frac{3}{2} \\\\ 1 & 2 \\end{bmatrix} = \\begin{bmatrix} 5-\\frac{3}{4} & 6-\\frac{3}{2} \\\\ 7-1 & 8-2 \\end{bmatrix} = \\begin{bmatrix} \\frac{17}{4} & \\frac{9}{2} \\\\ 6 & 6 \\end{bmatrix}.\n$$\n5. The entry in the second row, first column of $S$ is\n$$\nS_{2,1} = 6.\n$$",
            "answer": "$$\\boxed{6}$$"
        },
        {
            "introduction": "Beyond its algebraic definition, the practical utility of the Schur complement depends on its computational cost. This practice guides you through analyzing the floating-point operations (flops) required to explicitly form a Schur complement for a dense block matrix . This analysis is fundamental to numerical algorithm design and reveals why direct formation can be prohibitively expensive, motivating the need for more advanced techniques.",
            "id": "2160771",
            "problem": "A computational scientist is working on a large-scale simulation that requires solving a system of linear equations of the form $M\\mathbf{u} = \\mathbf{f}$. The matrix $M$ is a dense $2n \\times 2n$ matrix that has a natural block structure:\n$$\nM =\n\\begin{bmatrix}\nA & B \\\\\nC & D\n\\end{bmatrix}\n$$\nwhere $A, B, C,$ and $D$ are all dense $n \\times n$ matrices. The vector $\\mathbf{u}$ and $\\mathbf{f}$ are partitioned conformally as $\\mathbf{u} = (\\mathbf{u}_1^T, \\mathbf{u}_2^T)^T$ and $\\mathbf{f} = (\\mathbf{f}_1^T, \\mathbf{f}_2^T)^T$, where $\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{f}_1, \\mathbf{f}_2$ are vectors of length $n$.\n\nTo solve this system efficiently, the scientist decides to use a block elimination method, which involves forming the Schur complement of the block $A$. The Schur complement of $A$ in $M$, denoted as $S$, is defined by the matrix that appears in the reduced system for the unknown vector $\\mathbf{u}_2$.\n\nFor your analysis, assume the following standard costs for floating-point operations (flops), where one flop is defined as one multiplication and one addition:\n1.  The cost of a standard matrix-matrix multiplication of two dense $n \\times n$ matrices is $2n^3$ flops.\n2.  The cost of solving a dense $n \\times n$ linear system $A\\mathbf{y} = \\mathbf{z}$ for $\\mathbf{y}$ (which involves an LU factorization of $A$ followed by forward and backward substitution) is dominated by the factorization and costs $\\frac{2}{3}n^3$ flops.\n3.  When multiple systems with the same matrix $A$ need to be solved (e.g., $A\\mathbf{y}_i = \\mathbf{z}_i$ for $i=1, \\dots, k$), the LU factorization is performed once. Solving for each of the $k$ right-hand sides using forward/backward substitution costs $2n^2$ flops per right-hand side.\n\nAssuming that the matrix $A$ is invertible, determine the leading-order term for the total number of flops required to compute the Schur complement matrix $S$. Express your answer as an analytic expression in terms of $n$.",
            "solution": "We must compute the Schur complement of $A$ in $M$, which is the $n \\times n$ matrix\n$$\nS = D - C A^{-1} B.\n$$\nTo form $S$ efficiently, we avoid computing $A^{-1}$ explicitly and instead use an LU factorization of $A$ and triangular solves, as implied by the given cost model. The calculation can be broken into three main steps:\n\n1.  **LU Factorization of A:** First, we compute the LU factorization of the $n \\times n$ matrix $A$. According to the problem statement, this costs\n$$\n\\frac{2}{3} n^{3} \\text{ flops}.\n$$\n2.  **Solve for $A^{-1}B$:** Next, we need to compute the term $X = A^{-1} B$. This is equivalent to solving the matrix equation $A X = B$. Since $B$ is an $n \\times n$ matrix, this is equivalent to solving $n$ linear systems, where each column of $B$ is a right-hand side vector. Having already factored $A$, the cost for solving for each of the $n$ right-hand sides using forward/backward substitution is $2n^2$ flops. The total cost for this step is:\n$$\nn \\times (2n^2) = 2 n^{3} \\text{ flops}.\n$$\n3.  **Matrix Multiplication and Subtraction:** We then compute the product $C X = C (A^{-1} B)$. This is a multiplication of two dense $n \\times n$ matrices, which costs:\n$$\n2 n^{3} \\text{ flops}.\n$$\nFinally, we form $S = D - (C A^{-1} B)$. This is an $n \\times n$ matrix subtraction, which costs $n^2$ flops. This is a lower-order term and can be ignored when determining the leading-order cost.\n\nSumming the leading-order contributions from each step gives the total cost:\n$$\n\\text{Total Flops} = \\frac{2}{3} n^{3} + 2 n^{3} + 2 n^{3} = \\left(\\frac{2}{3} + 4\\right) n^3 = \\frac{14}{3} n^{3}.\n$$\nTherefore, the leading-order term for the flop count to compute $S$ is $\\frac{14}{3} n^{3}$.",
            "answer": "$$\\boxed{\\frac{14}{3}\\,n^{3}}$$"
        },
        {
            "introduction": "For truly large-scale problems, the cost of forming and storing the Schur complement matrix, as analyzed previously, is often prohibitive. The modern approach is to use it \"matrix-free\" as an operator within an iterative solver. This advanced practice explores how to compute the action of the Schur complement operator on a vector, which is the key step in solving a system like $Sy=v$ without ever forming $S$ explicitly . Mastering this concept is essential for applying Schur complement methods to the challenging linear systems that arise in science and engineering.",
            "id": "3595834",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be partitioned conformably as\n$$\nA \\;=\\; \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{bmatrix},\n$$\nwith $A_{11} \\in \\mathbb{R}^{n_1 \\times n_1}$ nonsingular and $A_{22} \\in \\mathbb{R}^{n_2 \\times n_2}$, where $n_1 + n_2 = n$. Suppose an $A_{11}$ factorization is available as a Lower–Upper decomposition (LU), $A_{11} = L_{11} U_{11}$, with $L_{11}$ and $U_{11}$ triangular and nonsingular. The Schur complement of $A_{11}$ in $A$ is denoted by $S_{22\\cdot 11} \\in \\mathbb{R}^{n_2 \\times n_2}$. You are given a vector $v \\in \\mathbb{R}^{n_2}$ and are asked to implement the action $y = S_{22\\cdot 11}^{-1} v$ without explicitly forming $S_{22\\cdot 11}$.\n\nYour allowed primitives are:\n- triangular solves with $L_{11}$ and $U_{11}$ (thus you may compute $w = A_{11}^{-1} t$ for a given $t \\in \\mathbb{R}^{n_1}$ by solving $L_{11} s = t$ and $U_{11} w = s$),\n- matrix–vector products with $A_{21}$ and $A_{12}$,\n- matrix–vector products with $A_{22}$,\n- standard iterative linear solvers that require only the ability to apply a linear operator to a vector.\n\nFrom first principles of block Gaussian elimination and operator-based iterative methods in numerical linear algebra, which of the following strategies correctly implements $y = S_{22\\cdot 11}^{-1} v$ under the stated constraints, without ever forming $S_{22\\cdot 11}$?\n\nA. Use a Krylov subspace method to solve $S_{22\\cdot 11} y = v$, where the operator application $z \\mapsto S_{22\\cdot 11} z$ is implemented as follows: given $z \\in \\mathbb{R}^{n_2}$, compute $t = A_{12} z$, solve $L_{11} s = t$ and $U_{11} w = s$ to obtain $w = A_{11}^{-1} t$, compute $r = A_{21} w$, and return $A_{22} z - r$.\n\nB. Use a Krylov subspace method to solve $S_{22\\cdot 11} y = v$, where the operator application $z \\mapsto S_{22\\cdot 11} z$ is implemented as follows: given $z \\in \\mathbb{R}^{n_2}$, compute $t = A_{12} z$, solve $L_{11} s = t$ and $U_{11} w = s$ to obtain $w = A_{11}^{-1} t$, compute $r = A_{21} w$, and return $A_{22} z + r$.\n\nC. Solve the bordered system $A \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ v \\end{bmatrix}$ by first computing $y = A_{22}^{-1} v$, then computing $x = -A_{11}^{-1} A_{12} y$ via triangular solves, and accept the resulting $y$ as $S_{22\\cdot 11}^{-1} v$.\n\nD. Use the Sherman–Morrison–Woodbury identity to compute $y = A_{22}^{-1} v - A_{22}^{-1} A_{21} A_{11}^{-1} A_{12} A_{22}^{-1} v$ exactly, using triangular solves with $A_{11}$ and matrix–vector products with $A_{21}$, $A_{12}$, and $A_{22}$.",
            "solution": "To solve the system $S_{22\\cdot 11} y = v$ without explicitly forming the Schur complement matrix $S_{22\\cdot 11}$, we can use an iterative method like a Krylov subspace method (e.g., GMRES or Conjugate Gradient if $S$ is symmetric positive definite). These methods only require a function that computes the matrix-vector product $S_{22\\cdot 11} z$ for any given vector $z \\in \\mathbb{R}^{n_2}$.\n\nLet's derive the steps to compute this product using the given primitives. By definition, the Schur complement is $S_{22\\cdot 11} = A_{22} - A_{21} A_{11}^{-1} A_{12}$. The action of this operator on a vector $z$ is:\n$$\nS_{22\\cdot 11} z = (A_{22} - A_{21} A_{11}^{-1} A_{12}) z = A_{22} z - A_{21} (A_{11}^{-1} (A_{12} z))\n$$\nTo compute this efficiently from right to left, we perform the following sequence of operations:\n1.  Compute the vector $t = A_{12} z$. This is a matrix-vector product.\n2.  Compute the vector $w = A_{11}^{-1} t$. Since we have the LU factorization of $A_{11}$, we can do this by solving the linear system $A_{11} w = t$ using forward and backward substitution (i.e., solving $L_{11}s=t$ and then $U_{11}w=s$).\n3.  Compute the vector $r = A_{21} w$. This is another matrix-vector product.\n4.  Compute the final result by subtracting $r$ from the product of $A_{22}$ and $z$: $A_{22} z - r$.\n\nNow, let's analyze the given options:\n\n- **A:** This option describes the exact sequence of operations derived above: compute $t=A_{12}z$, solve for $w=A_{11}^{-1}t$, compute $r=A_{21}w$, and return $A_{22}z - r$. This is the correct procedure.\n- **B:** This option suggests returning $A_{22} z + r$. The sign is incorrect, which would lead to a completely different operator.\n- **C:** This option suggests a procedure for solving a specific bordered system. While solving $A \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ v \\end{bmatrix}$ for $y$ is equivalent to finding $S_{22\\cdot 11}^{-1}v$, the proposed method for solving it is incorrect. It assumes $y$ can be found via $y = A_{22}^{-1} v$, which is only true if $A_{21}=0$. It also requires an inverse or factorization of $A_{22}$, which is not a given primitive.\n- **D:** This option invokes the Sherman-Morrison-Woodbury identity, which is not directly applicable here for computing $S_{22 \\cdot 11}^{-1}v$. The formula provided is also incorrect and requires inverting $A_{22}$, which is not a given primitive.\n\nTherefore, option A is the only one that correctly describes the matrix-free application of the Schur complement operator required for an iterative solver.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}