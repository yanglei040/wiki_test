{
    "hands_on_practices": [
        {
            "introduction": "The Schur complement is a fundamental concept in linear algebra, enabling the 'divide and conquer' strategy for solving large linear systems. This first practice provides a direct, hands-on calculation to build intuition. By working through the block matrix operations for a small, concrete example , you will solidify your understanding of the defining formula, $S = D - CA^{-1}B$, which is the cornerstone of all Schur complement applications.",
            "id": "1074949",
            "problem": "Consider the $3 \\times 3$ real matrix $M$ defined as:\n$$\nM = \\begin{bmatrix}\n4 & 1 & 2 \\\\\n3 & 5 & 6 \\\\\n4 & 7 & 8\n\\end{bmatrix}.\n$$\nPartition $M$ into blocks where the top-left block $A$ is $1 \\times 1$, the top-right block $B$ is $1 \\times 2$, the bottom-left block $C$ is $2 \\times 1$, and the bottom-right block $D$ is $2 \\times 2$. Using block Gaussian elimination, compute the Schur complement of $A$ in $M$, denoted as $S = D - C A^{-1} B$. Report the entry in the second row and first column of the resulting $2 \\times 2$ Schur complement matrix $S$.",
            "solution": "1. Partition the matrix $M$ as\n$$\nM=\\begin{pmatrix}\nA & B \\\\[6pt]\nC & D\n\\end{pmatrix},\n\\quad\nA=[4],\\;\nB=[\\,1\\;\\;2\\,],\\;\nC=\\begin{pmatrix}3\\\\4\\end{pmatrix},\\;\nD=\\begin{pmatrix}5&6\\\\7&8\\end{pmatrix}.\n$$\n2. Since $A$ is the scalar $4$, its inverse is\n$$\nA^{-1} = \\frac1{4}.\n$$\n3. Compute the product $C\\,A^{-1}B$:\n$$\nC\\,B = \\begin{pmatrix}3\\\\4\\end{pmatrix}\\,[\\,1\\;\\;2\\,]\n= \\begin{pmatrix}3\\cdot1 & 3\\cdot2 \\\\[4pt] 4\\cdot1 & 4\\cdot2\\end{pmatrix}\n= \\begin{pmatrix}3 & 6\\\\4 & 8\\end{pmatrix},\n$$\n$$\nC\\,A^{-1}B = \\frac1{4}\\begin{pmatrix}3 & 6\\\\4 & 8\\end{pmatrix}\n= \\begin{pmatrix}\\tfrac34 & \\tfrac32 \\\\[4pt] 1 & 2\\end{pmatrix}.\n$$\n4. The Schur complement is\n$$\nS = D - C\\,A^{-1}B\n= \\begin{pmatrix}5&6\\\\7&8\\end{pmatrix}\n- \\begin{pmatrix}\\tfrac34 & \\tfrac32 \\\\[4pt] 1 & 2\\end{pmatrix}\n= \\begin{pmatrix}5-\\tfrac34 & 6-\\tfrac32 \\\\[4pt] 7-1 & 8-2\\end{pmatrix}\n= \\begin{pmatrix}\\tfrac{17}{4} & \\tfrac{9}{2} \\\\[4pt] 6 & 6\\end{pmatrix}.\n$$\n5. The entry in the second row, first column of $S$ is\n$$\nS_{2,1} = 6.\n$$",
            "answer": "$$\\boxed{6}$$"
        },
        {
            "introduction": "While the definition of the Schur complement is straightforward, its practical utility lies in computational efficiency. This exercise moves from definition to application by analyzing the cost of forming the Schur complement in a large-scale setting. By performing a floating-point operation (flop) count, you will learn how to assess the computational viability of a block-elimination strategy, a key skill for designing efficient numerical algorithms .",
            "id": "2160771",
            "problem": "A computational scientist is working on a large-scale simulation that requires solving a system of linear equations of the form $M\\mathbf{u} = \\mathbf{f}$. The matrix $M$ is a dense $2n \\times 2n$ matrix that has a natural block structure:\n$$\nM =\n\\begin{pmatrix}\nA & B \\\\\nC & D\n\\end{pmatrix}\n$$\nwhere $A, B, C,$ and $D$ are all dense $n \\times n$ matrices. The vector $\\mathbf{u}$ and $\\mathbf{f}$ are partitioned conformally as $\\mathbf{u} = (\\mathbf{u}_1^T, \\mathbf{u}_2^T)^T$ and $\\mathbf{f} = (\\mathbf{f}_1^T, \\mathbf{f}_2^T)^T$, where $\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{f}_1, \\mathbf{f}_2$ are vectors of length $n$.\n\nTo solve this system efficiently, the scientist decides to use a block elimination method, which involves forming the Schur complement of the block $A$. The Schur complement of $A$ in $M$, denoted as $S$, is defined by the matrix that appears in the reduced system for the unknown vector $\\mathbf{u}_2$.\n\nFor your analysis, assume the following standard costs for floating-point operations (flops), where one flop is defined as one multiplication and one addition:\n1.  The cost of a standard matrix-matrix multiplication of two dense $n \\times n$ matrices is $2n^3$ flops.\n2.  The cost of solving a dense $n \\times n$ linear system $A\\mathbf{y} = \\mathbf{z}$ for $\\mathbf{y}$ (which involves an LU factorization of $A$ followed by forward and backward substitution) is dominated by the factorization and costs $\\frac{2}{3}n^3$ flops.\n3.  When multiple systems with the same matrix $A$ need to be solved (e.g., $A\\mathbf{y}_i = \\mathbf{z}_i$ for $i=1, \\dots, k$), the LU factorization is performed once. Solving for each of the $k$ right-hand sides using forward/backward substitution costs $2n^2$ flops per right-hand side.\n\nAssuming that the matrix $A$ is invertible, determine the leading-order term for the total number of flops required to compute the Schur complement matrix $S$. Express your answer as an analytic expression in terms of $n$.",
            "solution": "We must compute the Schur complement of $A$ in $M$, which is the $n \\times n$ matrix\n$$\nS = D - C A^{-1} B.\n$$\nTo form $S$ efficiently, we avoid computing $A^{-1}$ explicitly and instead use an LU factorization of $A$ and triangular solves, as implied by the given cost model.\n\nFirst, factor $A$ once using LU. By assumption, $A$ is invertible, and the factorization cost dominates the direct solve and is\n$$\n\\frac{2}{3} n^{3} \\text{ flops}.\n$$\nNext, compute $X = A^{-1} B$ by solving $A X = B$. The matrix $B$ has $n$ columns, so there are $k = n$ right-hand sides. After the factorization, each right-hand side costs $2 n^{2}$ flops for forward/backward substitution, so the total cost for all $n$ right-hand sides is\n$$\n2 n^{3} \\text{ flops}.\n$$\nThen compute the matrix product $C X$ to obtain $C A^{-1} B$. The cost of a dense $n \\times n$ by $n \\times n$ matrix-matrix multiplication is\n$$\n2 n^{3} \\text{ flops}.\n$$\nFinally, form $S = D - (C A^{-1} B)$, which requires an $n \\times n$ matrix subtraction. This costs $\\mathcal{O}(n^{2})$ flops, which is lower order than $n^{3}$ and does not contribute to the leading-order term.\n\nSumming the leading contributions gives\n$$\n\\frac{2}{3} n^{3} + 2 n^{3} + 2 n^{3} = \\frac{14}{3} n^{3}.\n$$\nTherefore, the leading-order term for the flop count to compute $S$ is $\\frac{14}{3} n^{3}$.",
            "answer": "$$\\boxed{\\frac{14}{3}\\,n^{3}}$$"
        },
        {
            "introduction": "An efficient algorithm is only useful if it is also numerically stable. This final practice explores a critical, and sometimes subtle, aspect of the Schur complement: its conditioning. Through a carefully chosen parametric example, you will investigate how the condition number of the Schur complement, $\\kappa_{2}(S)$, can behave quite differently from the original matrix blocks . This exercise highlights the crucial fact that block elimination can sometimes transform a well-conditioned problem into an ill-conditioned one, a vital consideration in robust scientific computing.",
            "id": "3222493",
            "problem": "Consider the $2 \\times 2$ block linear system with block matrix\n$$\nM \\;=\\; \\begin{pmatrix}\nA & B \\\\\nC & D\n\\end{pmatrix},\n$$\nwhere\n$$\nA \\;=\\; I_{2}, \\quad B \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & \\varepsilon \\end{pmatrix}, \\quad C \\;=\\; B^{\\top}, \\quad D \\;=\\; 0_{2},\n$$\nand $\\varepsilon \\in (0,1]$. By carrying out block Gaussian elimination that eliminates the variables associated with $A$, one forms the Schur complement $S$ of $A$ in $M$. Using fundamental definitions of the spectral norm and the $2$-norm condition number $\\kappa_{2}(\\cdot)$, compute the exact closed-form expression of $\\kappa_{2}(S)$ as a function of $\\varepsilon$. Provide your final answer as a single simplified analytic expression in $\\varepsilon$.",
            "solution": "The problem statement is evaluated for validity and is found to be self-contained, scientifically grounded in numerical linear algebra, and well-posed. All provided data and definitions are standard and consistent. Therefore, a complete solution is presented below.\n\nThe problem asks for the $2$-norm condition number of the Schur complement, $\\kappa_2(S)$, for a given $2 \\times 2$ block matrix $M$. The block matrix is\n$$\nM \\;=\\; \\begin{pmatrix}\nA & B \\\\\nC & D\n\\end{pmatrix}\n$$\nwhere the blocks are given as\n$$\nA \\;=\\; I_{2} \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad B \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & \\varepsilon \\end{pmatrix}, \\quad C \\;=\\; B^{\\top}, \\quad D \\;=\\; 0_{2} \\;=\\; \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nThe parameter $\\varepsilon$ is constrained to the interval $\\varepsilon \\in (0,1]$.\n\nThe Schur complement $S$ of the block $A$ in the matrix $M$ is formed by a block Gaussian elimination process. The general formula for the Schur complement of $A$ in $M$ is\n$$\nS \\;=\\; D - CA^{-1}B.\n$$\nThis is well-defined because the matrix $A = I_2$ is invertible, with its inverse being $A^{-1} = I_2^{-1} = I_2$.\n\nWe can now substitute the given matrices into the formula for $S$. The matrix $C$ is the transpose of $B$:\n$$\nC \\;=\\; B^{\\top} \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & \\varepsilon \\end{pmatrix}^{\\top} \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & \\varepsilon \\end{pmatrix}.\n$$\nNotice that $C=B$. Substituting the blocks into the definition of $S$:\n$$\nS \\;=\\; 0_{2} - B^{\\top} I_{2} B \\;=\\; -B^{\\top}B.\n$$\nNow, we compute the product $B^{\\top}B$:\n$$\nS \\;=\\; - \\begin{pmatrix} 1 & 0 \\\\ 0 & \\varepsilon \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\varepsilon \\end{pmatrix} \\;=\\; - \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 0 & 1 \\cdot 0 + 0 \\cdot \\varepsilon \\\\ 0 \\cdot 1 + \\varepsilon \\cdot 0 & 0 \\cdot 0 + \\varepsilon \\cdot \\varepsilon \\end{pmatrix} \\;=\\; - \\begin{pmatrix} 1 & 0 \\\\ 0 & \\varepsilon^2 \\end{pmatrix}.\n$$\nThe Schur complement is thus the diagonal matrix $S = \\begin{pmatrix} -1 & 0 \\\\ 0 & -\\varepsilon^2 \\end{pmatrix}$.\n\nThe next step is to compute the $2$-norm condition number of $S$, denoted $\\kappa_{2}(S)$. The definition of the condition number for an invertible matrix $S$ is\n$$\n\\kappa_{2}(S) \\;=\\; \\|S\\|_{2} \\|S^{-1}\\|_{2},\n$$\nwhere $\\| \\cdot \\|_{2}$ is the spectral norm (matrix $2$-norm). The matrix $S$ is invertible because its determinant is $\\det(S) = (-1)(-\\varepsilon^2) = \\varepsilon^2$, which is non-zero for $\\varepsilon \\in (0,1]$.\n\nFor any matrix $X$, the spectral norm $\\|X\\|_{2}$ is defined as the largest singular value of $X$, $\\sigma_{\\max}(X)$. The singular values are the square roots of the eigenvalues of $X^{\\top}X$.\n\nIn our case, the matrix $S$ is a real symmetric matrix. For a symmetric matrix, the singular values are the absolute values of its eigenvalues. The eigenvalues of a diagonal matrix are its diagonal entries. The eigenvalues of $S$ are therefore $\\lambda_1 = -1$ and $\\lambda_2 = -\\varepsilon^2$.\n\nThe spectral norm of $S$ is the maximum of the absolute values of its eigenvalues:\n$$\n\\|S\\|_{2} \\;=\\; \\max(|\\lambda_1|, |\\lambda_2|) \\;=\\; \\max(|-1|, |-\\varepsilon^2|) \\;=\\; \\max(1, \\varepsilon^2).\n$$\nGiven that $\\varepsilon \\in (0,1]$, we have $0 < \\varepsilon \\le 1$, which implies $0 < \\varepsilon^2 \\le 1$. Therefore,\n$$\n\\|S\\|_{2} \\;=\\; \\max(1, \\varepsilon^2) \\;=\\; 1.\n$$\n\nNext, we compute the norm of the inverse, $\\|S^{-1}\\|_{2}$. First, we find $S^{-1}$:\n$$\nS^{-1} \\;=\\; \\left(- \\begin{pmatrix} 1 & 0 \\\\ 0 & \\varepsilon^2 \\end{pmatrix}\\right)^{-1} \\;=\\; - \\begin{pmatrix} 1 & 0 \\\\ 0 & \\varepsilon^2 \\end{pmatrix}^{-1} \\;=\\; - \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{\\varepsilon^2} \\end{pmatrix} \\;=\\; \\begin{pmatrix} -1 & 0 \\\\ 0 & -\\frac{1}{\\varepsilon^2} \\end{pmatrix}.\n$$\nThe matrix $S^{-1}$ is also symmetric. Its eigenvalues are its diagonal entries, $\\mu_1 = -1$ and $\\mu_2 = -1/\\varepsilon^2$. The spectral norm of $S^{-1}$ is the maximum of the absolute values of its eigenvalues:\n$$\n\\|S^{-1}\\|_{2} \\;=\\; \\max(|\\mu_1|, |\\mu_2|) \\;=\\; \\max(|-1|, |-\\frac{1}{\\varepsilon^2}|) \\;=\\; \\max\\left(1, \\frac{1}{\\varepsilon^2}\\right).\n$$\nSince $\\varepsilon \\in (0,1]$, we know $0 < \\varepsilon^2 \\le 1$, which implies $\\frac{1}{\\varepsilon^2} \\ge 1$. Therefore,\n$$\n\\|S^{-1}\\|_{2} \\;=\\; \\max\\left(1, \\frac{1}{\\varepsilon^2}\\right) \\;=\\; \\frac{1}{\\varepsilon^2}.\n$$\nFinally, we compute the condition number by multiplying the norms:\n$$\n\\kappa_{2}(S) \\;=\\; \\|S\\|_{2} \\|S^{-1}\\|_{2} \\;=\\; 1 \\cdot \\frac{1}{\\varepsilon^2} \\;=\\; \\frac{1}{\\varepsilon^2}.\n$$\n\nAn alternative, more direct method for a symmetric matrix is to compute the condition number as the ratio of the largest to the smallest absolute eigenvalue:\n$$\n\\kappa_2(S) = \\frac{\\max_i |\\lambda_i|}{\\min_j |\\lambda_j|}.\n$$\nThe absolute values of the eigenvalues of $S$ are $1$ and $\\varepsilon^2$. The maximum and minimum of these values are:\n$$\n\\max(|\\lambda_1|, |\\lambda_2|) = \\max(1, \\varepsilon^2) = 1,\n$$\n$$\n\\min(|\\lambda_1|, |\\lambda_2|) = \\min(1, \\varepsilon^2) = \\varepsilon^2,\n$$\nsince $\\varepsilon \\in (0,1]$.\nThus, the condition number is\n$$\n\\kappa_{2}(S) \\;=\\; \\frac{1}{\\varepsilon^2}.\n$$\nBoth methods yield the same result. The final expression for the condition number is a function of $\\varepsilon$.",
            "answer": "$$\n\\boxed{\\frac{1}{\\varepsilon^2}}\n$$"
        }
    ]
}