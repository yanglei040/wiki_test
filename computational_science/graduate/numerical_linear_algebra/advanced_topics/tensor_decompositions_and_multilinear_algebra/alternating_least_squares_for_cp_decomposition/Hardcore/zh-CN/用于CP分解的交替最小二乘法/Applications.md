## 应用与[交叉](@entry_id:147634)学科联系

前序章节详细阐述了规范多元/平行因子 (CP) 分解的交替最小二乘 (ALS) 算法的核心原理与机制。我们已经了解，CP-ALS 通过迭代优化单个因子矩阵，同时固定其他因子，为[张量分解](@entry_id:173366)提供了一个优雅且易于实现的框架。然而，该算法的真正威力在于其强大的适应性，能够被扩展、约束和正则化，以解决横跨众多科学与工程领域的实际问题。

本章的目标并非重复介绍核心概念，而是展示这些基本原理如何在多样化的真实世界和[交叉](@entry_id:147634)学科背景下得到应用、扩展和整合。我们将探讨如何修改基础的 ALS 框架以应对不完整或带有特定结构的数据，如何通过引入先验知识来指导分解过程，以及如何将 CP-ALS 作为一种强大的工具，应用于从信号处理、机器学习到神经科学等多个领域。此外，我们还将深入探讨提升 CP-ALS [算法鲁棒性](@entry_id:635315)、效率和收敛速度的高级数值技术，并最终将其置于[模型验证](@entry_id:141140)和现代[可微编程](@entry_id:163801)的更广阔视角下。通过这些应用实例，我们将揭示 CP-ALS 不仅仅是一个数学构造，更是一个连接理论与实践、促进跨学科发现的强大引擎。

### 针对真实世界数据约束调整CP-ALS

原始的 CP-ALS 算法在无约束的最小二乘意义下寻找最佳因子，但这在许多实际应用中是不够的。真实世界的数据往往具有特定的内在结构和约束，必须在分解过程中予以考虑，以确保模型的物理意义和[可解释性](@entry_id:637759)。

#### 非负性约束

许多领域的数据本质上是非负的，例如图像的像素强度、化学物质的浓度、或文档中的词频计数。在这种情况下，分解出的因子矩阵也应具有非负性，以保持物理解释。例如，将高[光谱](@entry_id:185632)图像分解为纯物质[光谱](@entry_id:185632)（因子 B）及其在每个像素中的丰度（因子 A）时，[光谱](@entry_id:185632)和丰度都不能为负。

这就引出了非负 CP (NCP) 分解。通过在 ALS 的每个子问题中强制施加非负性约束，我们将标准的最小二乘问题转化为一系列非负最小二乘 (NNLS) 问题。一个常见的误区是认为可以通过求解无约束的普通正规方程，然后简单地将解中的负值“钳位”到零来解决 NNLS 问题。然而，这种[启发式方法](@entry_id:637904)通常无法得到真正的最优解，也无法保证算法的收敛性。

正确的做法是采用专门的 NNLS 求解器。从优化理论的角度看，非负性约束将问题转化为一个带[不等式约束](@entry_id:176084)的凸二次规划，其最优性由 [Karush-Kuhn-Tucker (KKT) 条件](@entry_id:176491)刻画，而不仅仅是梯度为零。简单的钳位操作无法满足 KKT 条件中的[互补松弛性](@entry_id:141017)。此外，从[数值稳定性](@entry_id:146550)的角度看，形成并求解[正规方程](@entry_id:142238) $(H^{\top} H) a = H^{\top} x$ 是一种不稳定的做法。在 CP 分解中，由克拉特里-雷奥（Khatri-Rao）积构成的矩阵 $H$ 往往是病态的（即[条件数](@entry_id:145150)很大）。计算 $H^{\top} H$ 会使其条件数平方，即 $\kappa(H^{\top} H) = \kappa(H)^{2}$，这会严重放大[数值误差](@entry_id:635587)，导致解对微小扰动异常敏感。因此，使用如主动集法、块主元旋转法或[投影梯度法](@entry_id:169354)等专业的 NNLS 算法至关重要，它们通常直接操作于矩阵 $H$，避免了[条件数](@entry_id:145150)的恶化，从而确保了数值的准确性和鲁棒性 。

#### 处理缺失数据

在实际数据收集中，数据缺失是一种常态而非例外，例如由于传感器故障、调查问卷未回答或实验限制。标准的 CP-ALS 算法无法直接处理含有缺失值的张量。幸运的是，ALS 框架可以被优雅地扩展来解决这个问题。

其核心思想是将标准的最小二乘[目标函数](@entry_id:267263)修改为加权最小二乘 (WLS) 目标函数。我们可以引入一个与数据张量 $\mathcal{X}$ 形状相同的权重张量 $\mathcal{W}$，其中观测到的元素对应权重为 1，缺失的元素对应权重为 0。[优化问题](@entry_id:266749)就变为：
$$ \min_{\{A^{(n)}\}} \; \|\mathcal{W} \odot (\mathcal{X} - \mathcal{\hat{X}})\|_F^2 $$
其中 $\odot$ 表示元素积。在 ALS 的每次迭代中，更新某个因子矩阵（例如 $A^{(k)}$）的子问题也相应地变为一个加权最小二乘问题。该问题可以进一步分解，对 $A^{(k)}$ 的每一行独立求解一个小的加权[岭回归](@entry_id:140984)问题。为了保证[数值稳定性](@entry_id:146550)，通常会加入一个小的吉洪诺夫（Tikhonov）正则化项。

这种方法的一个重要实践细节是如何处理数据中整行或整列都缺失的边缘情况。例如，如果更新因子 $A^{(k)}$ 时，其第 $i$ 行对应的所有数据点都缺失，那么该行的权重将全部为零。在这种情况下，没有信息可用于更新该行，一个合理的策略是保持该行不变或将其设置为零向量 。

#### 概率与结构化约束

除了非负性，某些应用场景还要求因子矩阵满足更复杂的结构化约束。一个典型的例子是，当使用[张量分解](@entry_id:173366)进行主题建模时，我们希望某些因子能够代表[离散概率分布](@entry_id:166565)。这意味着这些因子的元素不仅要非负，而且其每一列（代表一个“主题”）的元素之和必须为 1。

这类约束（例如，在因子矩阵 $C$ 的列上施加非负性和“和为一”的约束）可以将 CP 分解与[概率模型](@entry_id:265150)联系起来。为了在 ALS 框架内处理这些约束，一种灵活的方法是采用[罚函数法](@entry_id:636090)。我们可以将原始的最小二乘目标函数进行增广，加入惩罚项来惩罚对约束的违反。例如，对于和为一的约束 $\sum_{i} C_{ir} = 1$，可以加入形如 $\frac{\lambda_{S1}}{2} \sum_{r} (\sum_{i} C_{ir} - 1)^2$ 的惩罚项。同样，对于非负性约束 $C_{ir} \ge 0$，可以加入 $\frac{\lambda_{NN}}{2} \sum_{i,r} (\max(0, -C_{ir}))^2$ 的惩罚项。通过调整惩罚参数 $\lambda_{S1}$ 和 $\lambda_{NN}$，我们可以在拟合数据和满足约束之间取得平衡 。在实际算法中，更复杂的方法如[增广拉格朗日法](@entry_id:170637)或将子问题精确投影到[概率单纯形](@entry_id:635241)上，可以更严格地施加这些约束。

### 通过正则化增强CP-ALS以实现结构化发现

正则化是在模型中引入先验知识以[防止过拟合](@entry_id:635166)并引导分解朝向更有意义、更平滑或更稀疏的解的强大技术。通过向 ALS 目标函数添加惩罚项，我们可以将关于解的期望属性编码到[优化问题](@entry_id:266749)中。

一个经典的应用是在分析[时间序列数据](@entry_id:262935)时促进解的平滑性，这在神经科学、经济学和气候科学中非常普遍。例如，考虑一个记录了多个神经元在多次试验中随时间变化的活动张量（时间 $\times$ 神经元 $\times$ 试验）。我们有理由相信，与神经活动相关的潜在时间模式（由因子矩阵 $A$ 的列捕获）应该是平滑变化的，而不是充满高频噪声。

为了将这种平滑性先验知识融入模型，我们可以在更新因子 $A$ 的 ALS 子问题中加入一个[吉洪诺夫正则化](@entry_id:140094)项，该项惩罚 $A$ 中时间模式的“粗糙度”。一个典型的选择是使用一个惩罚 $A$ 的差分的范数的项，例如 $\frac{\gamma}{2} \| D A \|_F^2$，其中 $D$ 是一个一阶或二阶差分算子，$\gamma$ 是控制平滑程度的[正则化参数](@entry_id:162917)。加入此项后，更新 $A$ 的正规方程变为一个[西尔维斯特方程](@entry_id:155720)（Sylvester equation）：
$$ (\gamma D^{\top}D) A + A \left( (C^{\top}C) * (B^{\top}B) \right) = X_{(1)} (C \odot B) $$
其中 $*$ 表示[哈达玛积](@entry_id:182073)。这个正则化项的优美之处在于其在[频域](@entry_id:160070)中的直观解释。如果 $D$ 是一个循环差分算子，并且因子 $B$ 和 $C$ 的列近似正交，那么该正则化操作在傅里叶域中会解耦。可以证明，更新后的时间因子 $\hat{A}(\omega)$ 是通过将无约束的解乘以一个依赖于频率 $\omega$ 的衰减因子 $H(\omega)$ 得到的。对于[一阶差分](@entry_id:275675)，这个衰减因子形如 $H(\omega) \propto 1 / (k + \gamma(1 - \cos(\omega)))$。这个函数在低频（$\omega \approx 0$）时值较大，在高频（$\omega \approx \pi$）时值较小，因此它本质上是一个低通滤波器，能够有效地抑制时间因子中的高频[振荡](@entry_id:267781)，从而产生平滑的解 。

### 交叉学科应用与解释框架

CP-ALS 的灵活性使其成为众多学科中探索[多维数据](@entry_id:189051)集结构的强大工具。通过与特定领域的知识和约束相结合，CP 分解能够揭示出有意义的潜在模式。

#### 信号处理与复数数据

在信号处理、[无线通信](@entry_id:266253)和量子力学等领域，数据通常以复数形式出现，其中相位信息与幅度信息同样重要。例如，经过[傅里叶变换](@entry_id:142120)的[时间序列数据](@entry_id:262935)就是复值的。CP-ALS 框架可以自然地推广到复数张量。在这种情况下，最小二乘目标函数使用由厄米特[内积](@entry_id:158127)诱导的[弗罗贝尼乌斯范数](@entry_id:143384)。ALS 的更新法则也相应地变为使用[共轭转置](@entry_id:147909)（厄米特转置）和厄米特[格拉姆矩阵](@entry_id:203297)。

处理复数因子时会出现一个新的挑战：相位不确定性。对于一个秩为 1 的复数张量 $a \circ b \circ c$，我们可以将因子 $a$ 乘以一个相位因子 $e^{-i\psi}$，同时将因子 $c$ 乘以其共轭 $e^{i\psi}$，而张量本身保持不变，即 $(e^{-i\psi} a) \circ b \circ (e^{i\psi} c) = a \circ b \circ c$。为了得到唯一的分解，必须引入归一化约定来消除这种歧义。一种常见的做法是，在每次更新后，对每个分量的因子进行旋转，使得其中一个因子向量（例如 $a_r$）的某个元素（例如第一个元素）变为实数且非负 。

#### [话题建模](@entry_id:634705)与数据挖掘

CP 分解与机器学习中的话题模型（如[潜在狄利克雷分配](@entry_id:635270)，[LDA](@entry_id:138982)）之间存在着深刻的联系。一个文集可以表示为一个三阶张量，例如（词 $\times$ 词 $\times$ 文档）共现计数张量。对该张量进行带非负性和列和为一约束的 CP 分解，可以得到一个可解释为话题模型的分解结果。

在这种框架下，因子矩阵 $B$（词 $\times$ 主题）和 $C$（词 $\times$ 主题）的列可以被解释为每个主题下的词语[概率分布](@entry_id:146404)，而因子矩阵 $A$（文档 $\times$ 主题）的列则表示每个文档中不同主题的混合权重。对因子 $B$ 和 $C$ 施加单纯形约束（非负且列和为一）不仅赋予了模型概率解释，还解决了 CP 分解固有的尺度不确定性。根据克鲁斯卡尔（Kruskal）的唯一性定理，如果因子矩阵的[克鲁斯卡尔秩](@entry_id:751064)之和满足一定条件（例如，$k_A + k_B + k_C \ge 2R + 2$），那么分解在[置换](@entry_id:136432)不确定性之外是唯一的。通过约束消除了尺度不确定性，我们就能得到一个在主题[置换](@entry_id:136432)意义下唯一的分解，这对于模型解释和比较至关重要 。

#### 多主体/多任务分析

在许多研究中，我们会从多个主体、不同条件或多个相关任务中收集数据，从而得到一组结构相似的张量。例如，在神经科学中，我们可能有多名被试执行相同任务时的脑电图（EEG）数据（电极 $\times$ 时间 $\times$ 试验）。一个合理的假设是，某些潜在的神经模式（例如，与任务相关的特定脑力过程）在所有被试中是共享的，而其他模式则可能是个体特有的。

这种结构可以通过耦合或多块[张量分解](@entry_id:173366)来建模。我们可以假设所有张量 $\mathcal{X}^{(p)}$（其中 $p$ 是主体索引）共享一个或多个因子矩阵（例如，代表试验效应的因子 $C$），而其他因子（$A^{(p)}$ 和 $B^{(p)}$）则是主体特异的。通过最小化一个联合[目标函数](@entry_id:267263)，如所有张量重构误差的加权和，我们可以同时分解所有张量。

这种[耦合方法](@entry_id:195982)具有显著优势。在 ALS 更新共享因子 $C$ 时，其[正规方程](@entry_id:142238)的系数矩阵会汇集来自所有数据块的信息。具体来说，该矩阵是各[数据块](@entry_id:748187)对应格拉姆-[哈达玛积](@entry_id:182073)的加权和。与仅使用单个张量相比，这种信息汇集通常会使得正规方程的[条件数](@entry_id:145150)更好，从而提高共享因子的估计稳定性和可识别性。从本质上讲，耦合分解通过跨数据集[借力](@entry_id:167067)，增强了从噪声中提取共享结构的统计能力 。

#### [异常检测](@entry_id:635137)

CP 分解等低秩模型是强大的[无监督学习](@entry_id:160566)工具，尤其适用于[异常检测](@entry_id:635137)。其基本思想是，一个数据集中的绝大多数“正常”或“典型”行为可以被一个低秩结构很好地捕捉。而异常事件，如网络攻击、设备故障或欺诈行为，通常是局部的、非典型的，不符合这种低秩模式。

例如，一个记录了网络服务器日志的三阶张量（IP 地址 $\times$ URL $\times$ 小时），其正常流量模式（如周期性的访问高峰、热门页面的常规访问）具有低秩特性。我们可以使用 CP 或塔克（Tucker）分解来拟合一个低秩模型 $\mathcal{\hat{X}}$ 来代表这种[正常模式](@entry_id:139640)。然后，计算残差张量 $\mathcal{E} = \mathcal{X} - \mathcal{\hat{X}}$。正常的数据点应该能够被模型很好地重构，因此其残差会很小。相反，一个异常事件，例如一个[分布](@entry_id:182848)式[拒绝服务](@entry_id:748298) (DDoS) 攻击，表现为在某个特定时间点（小时）、针对某个特定 URL，来自大量 IP 地址的流量激增。这种模式不符合整体的低秩结构，因此会在残差张量中产生巨大的值。

通过分析残差的能量（例如，将残差的平方和沿时间维度聚合），我们可以定义一个异常分数。如果某个时间点的残差能量远高于平均水平，我们就可以将其标记为异常。这种基于分解的方法为检测[多维数据](@entry_id:189051)中的复杂异常模式提供了一个有效且可解释的框架 。

### 稳健高效CP-ALS的高级数值方法

尽管 CP-ALS 的概念简单，但在实践中，尤其是在处理大规模或病态问题时，其性能可能会受到数值问题的困扰。本节探讨一些旨在提高[算法鲁棒性](@entry_id:635315)、效率和收敛速度的高级数值技术。

#### 应对病态问题：“沼泽”现象

CP-ALS 算法的一个著名问题是其收敛可能会极其缓慢，有时会停滞在所谓的“沼泽”（swamp）区域，即使[目标函数](@entry_id:267263)值仍在缓慢下降。

从几何角度看，这种现象可以得到深刻的解释。ALS 算法可以被看作是在两个或多个因子块定义的[流形](@entry_id:153038)之间交替投影的过程。当算法接近一个解时，这些[流形](@entry_id:153038)的切空间可能变得近乎平行，即它们之间的主夹角非常小。一次完整的 ALS 迭代的线性化算子的范数由这些切空间之间最小主夹角（弗里德里希斯角 $\theta_F$）的余弦值 $\cos(\theta_F)$ 决定。当夹角 $\theta_F$ 趋于零时，$\cos(\theta_F)$ 趋于 1，这意味着误差在每次迭代中仅以非常小的因子衰减，导致收敛停滞。这种[切空间](@entry_id:199137)的[共线性](@entry_id:270224)通常与因子向量之间的[共线性](@entry_id:270224)（即“退化”）有关 。

为了解决这个问题，可以采用[正则化技术](@entry_id:261393)来改善 ALS 子问题的条件。莱文伯格-马夸特（Levenberg-Marquardt, LM）方法是一种有效的策略。它通过在每个 ALS 子问题的[正规方程](@entry_id:142238)中添加一个阻尼项（例如 $\mu I$ 或 $\mu \cdot \text{diag}(H)$）来对[系数矩阵](@entry_id:151473)进行正则化，从而使其可逆且条件更好。LM 方法的精髓在于其自适应调整阻尼参数 $\mu$ 的信赖域（trust-region）策略。该策略通过比较[目标函数](@entry_id:267263)的实际下降值与二次模型预测的下降值之比 $\rho$ 来评估当前步长的质量。如果 $\rho$ 较大（模型预测准确），则减小 $\mu$，使算法更接近[牛顿法](@entry_id:140116)，从而加速收敛；如果 $\rho$ 较小或为负（模型预测差），则增大 $\mu$，使算法更接近[梯度下降法](@entry_id:637322)，以确保稳定性 。

#### 提升效率与[可扩展性](@entry_id:636611)

当张量的维度或秩 $R$ 很大时，每个 ALS 子问题中[求解线性系统](@entry_id:146035)的计算成本可能成为瓶颈。

一种加速策略是使用预处理技术。与其直接求解正规方程 $Hx=b$，不如求解一个等价的、但[条件数](@entry_id:145150)更好的[预处理](@entry_id:141204)系统，例如 $P^{-1}Hx = P^{-1}b$。对于大型系统，通常使用共轭梯度等迭代求解器，而[预处理](@entry_id:141204)可以显著减少所需的迭代次数。[雅可比](@entry_id:264467)（Jacobi）[预处理器](@entry_id:753679)是一种简单而有效的选择，它仅使用[系数矩阵](@entry_id:151473) $H$ 的对角线元素来构造预处理器 $P = \text{diag}(H)$。对于 CP-ALS，矩阵 $H$ 是格拉姆矩阵的[哈达玛积](@entry_id:182073)，其对角[线元](@entry_id:196833)素易于计算。可以证明，如果因子矩阵的列是正交的，雅可比[预处理器](@entry_id:753679)将是 $H$ 的精确逆，此时[预处理](@entry_id:141204)后的系统[条件数](@entry_id:145150)为 1，一步即可求解 。

另一个提升性能的维度是优化模式更新的顺序。在某些问题中，不同模式的 ALS 子问题的条件数可能存在巨大差异。例如，在[线性逆问题](@entry_id:751313)中，如果前向算子本身具有可分离的克罗内克积结构，我们可以推导出每个模式更新子问题[条件数](@entry_id:145150)的[上界](@entry_id:274738)。基于这个界，可以设计一种自适应的展开策略：在每一步，选择并更新其子问题[条件数](@entry_id:145150)上界最小的那个模式。这种策略优先处理“最容易”或“最稳定”的子问题，有助于提高整个算法的[数值稳定性](@entry_id:146550)和收敛性能 。

#### 加速收敛

除了通过正则化和预处理改善单个子问题，还可以从宏观上加速整个 ALS 迭代过程。ALS 本质上是一个[不动点迭代](@entry_id:749443)过程 $y_{k+1} = G(y_k)$，其中 $y$ 是所有因子向量的堆叠，$G$ 代表一次完整的 ALS 扫描。

[安德森加速](@entry_id:178052)（Anderson Acceleration, AA）是一种强大的多割线方法，专门用于加速[不动点迭代](@entry_id:749443)。其核心思想是，不直接使用 $G(y_k)$ 作为下一次迭代的起点，而是利用过去 $m$ 次的迭代历史信息 $\{y_{k-j}\}$ 和残差 $\{r_{k-j} = y_{k-j} - G(y_{k-j})\}$，通过求解一个小的[最小二乘问题](@entry_id:164198)来计算一个外推的、更好的更新步。

然而，将 AA 应用于 CP-ALS 这样一个非凸问题时必须格外小心。由于[目标函数](@entry_id:267263)非凸且 ALS 映射 $G$ 通常不是一个压缩映射，AA 产生的外推步骤不保证会单调降低[目标函数](@entry_id:267263)值，甚至可能导致目标函数值上升。因此，为了实现稳健的加速，必须配备保护措施，例如：当[目标函数](@entry_id:267263)增加时进行“重启”（即放弃外推步，回退到标准的 ALS 步）；对小的[最小二乘问题](@entry_id:164198)进行正则化以处理残差向量近似共线时出现的[病态问题](@entry_id:137067)；以及在每次加速步后对因子进行归一化以控制 CP 分解的尺度不确定性  。

### 更广阔的视角：[模型验证](@entry_id:141140)与可微[张量网络](@entry_id:142149)

成功运行 CP-ALS 算法并获得因子只是分析过程的开始。我们还需要评估所得模型的有效性，并可以将整个分解过程视为一个更大的、可端到端优化的计算网络中的一个模块。

#### [模型验证](@entry_id:141140)：选择正确的秩

在 CP 分解中，一个至关重要且极具挑战性的任务是选择合适的模型秩 $R$。如果 $R$ 太小，模型将无法捕捉数据中的所有结构（[欠拟合](@entry_id:634904)）；如果 $R$ 太大，模型会开始拟合噪声（[过拟合](@entry_id:139093)），导致因子之间出现共线性，解释性变差，并且解不再稳定。仅仅依赖于重构误差（或“[拟合优度](@entry_id:637026)”）来选择 $R$ 是不可靠的，因为随着 $R$ 的增加，[拟合优度](@entry_id:637026)几乎总会提高，即使增加的秩是无意义的。

核心一致性诊断（Core Consistency Diagnostic, CORCONDIA）是一种用于判断给定秩 $R$ 的 CP 模型是否合适的有效工具。其原理基于 CP 模型与塔克（Tucker）模型之间的关系：一个秩为 $R$ 的 CP 模型在数学上等价于一个拥有特定“超对角”单位[核心张量](@entry_id:747891) $\mathcal{I}$ 的 $R \times R \times R$ 塔克模型。

CORCONDIA 的流程如下：首先，使用 ALS 算法计算出目标秩 $R$ 的 CP 因子 $A, B, C$。然后，将这些因子视为一个塔克模型的基，并将原始数据张量 $\mathcal{X}$ 投影到这个基上，计算出最优的塔克[核心张量](@entry_id:747891) $\mathcal{G}$。如果原始数据确实符合秩为 $R$ 的 CP 结构，那么计算出的[核心张量](@entry_id:747891) $\mathcal{G}$ 应该非常接近理想的超对角单位核心 $\mathcal{I}$。反之，如果 $\mathcal{G}$ 与 $\mathcal{I}$ 相差甚远（即具有很大的非对角元素），则表明该 CP 模型无法充分描述数据的多线性结构，可能需要一个更复杂的（非 CP）模型，或者秩 $R$ 的选择不当（通常是过高）。通过计算一个量化 $\mathcal{G}$ 与 $\mathcal{I}$ 接近程度的分数，CORCONDIA 能够帮助研究者识别出一个合理的秩范围，通常表现为分数随 $R$ 增加而突然下降的“拐点” 。

#### [可微编程](@entry_id:163801)与[超梯度](@entry_id:750478)

在现代机器学习的背景下，我们可以将整个包含 $K$ 次迭代的 ALS 算法视为一个单一的、复杂的、但可微的计算层。这意味着，我们可以计算一个最终[损失函数](@entry_id:634569) $\mathcal{L}$（它可能依赖于 ALS 的输出因子）关于 ALS 算法的输入（即原始数据张量 $X$）的梯度。这种通过优化算法本身进行反向传播计算出的梯度被称为“[超梯度](@entry_id:750478)”（hypergradient）。

这种能力开启了全新的可能性，例如，在[双层优化](@entry_id:637138)问题中，我们可能想要优化[数据采集](@entry_id:273490)过程（即优化 $X$）以产生更理想的分解结果。从理论上讲，这可以通过链式法则，将梯度从最终损失 $\mathcal{L}$ 一路[反向传播](@entry_id:199535)穿过 $K$ 次 ALS 迭代，直到输入 $X$。

然而，这种“展开”的求导过程在计算上具有挑战性。[前向模式自动微分](@entry_id:749523)（计算[雅可比-向量积](@entry_id:162748)）对于高维输入（如张量 $X$）来说成本过高。反向模式[自动微分](@entry_id:144512)（即[反向传播](@entry_id:199535)，计算向量-雅可比积）虽然在计算上更高效（其时间复杂度与输入维度无关），但通常需要存储[前向传播](@entry_id:193086)过程中的所有中间变量（即每一次迭代后的因子矩阵）。对于大量的迭代次数 $K$，这会导致巨大的内存开销。检查点（Checkpointing）技术为解决此问题提供了一个实用的折中方案。它通过在[前向传播](@entry_id:193086)时仅存储一小部分（“检查点”）中间状态，而在[反向传播](@entry_id:199535)需要某个非检查点状态时，从最近的检查点开始重新计算得到，从而用一些额外的计算时间换取了大量的内存节省。这一框架使得将像 CP-ALS 这样的传统[数值算法](@entry_id:752770)作为现代[深度学习架构](@entry_id:634549)中的可微模块成为可能 。