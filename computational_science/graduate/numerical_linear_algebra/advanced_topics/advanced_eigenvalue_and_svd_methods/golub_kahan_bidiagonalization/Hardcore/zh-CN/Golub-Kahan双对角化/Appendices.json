{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握一个算法，最好的方法莫过于在一个小例子上手动执行一遍。这项练习旨在通过对一个具体的 $4 \\times 4$ 矩阵应用 Golub-Kahan 算法，来巩固你对其中递推关系和所生成矩阵结构的理解。通过逐步计算，你将能亲眼见证正交基和双对角矩阵是如何被构建出来的，并验证它们之间所满足的核心恒等式。",
            "id": "3548819",
            "problem": "设 $A \\in \\mathbb{R}^{4 \\times 4}$ 是一个下双对角矩阵，其主对角线元素等于 $2$，下对角线元素等于 $1$，即\n$$\nA \\;=\\; \\begin{pmatrix}\n2  0  0  0 \\\\\n1  2  0  0 \\\\\n0  1  2  0 \\\\\n0  0  1  2\n\\end{pmatrix}.\n$$\n从单位向量 $u_1 = e_1 \\in \\mathbb{R}^4$ 开始，对矩阵 $A$ 应用三步 Golub–Kahan 双对角化，其中每一步交替地构造\n$$\nr_j \\;=\\; A^{T} u_j - \\beta_{j-1} v_{j-1}, \\quad \\alpha_j \\;=\\; \\|r_j\\|_2, \\quad v_j \\;=\\; r_j/\\alpha_j,\n$$\n随后\n$$\np_j \\;=\\; A v_j - \\alpha_j u_j, \\quad \\beta_j \\;=\\; \\|p_j\\|_2, \\quad u_{j+1} \\;=\\; p_j/\\beta_j,\n$$\n约定 $\\beta_0 = 0$ 且 $v_0$ 未被使用。构造矩阵 $U_3 = [\\,u_1 \\;\\; u_2 \\;\\; u_3\\,] \\in \\mathbb{R}^{4 \\times 3}$、$V_3 = [\\,v_1 \\;\\; v_2 \\;\\; v_3\\,] \\in \\mathbb{R}^{4 \\times 3}$ 和下双对角矩阵 $B_3 \\in \\mathbb{R}^{3 \\times 3}$，其对角线元素为 $\\alpha_1,\\alpha_2,\\alpha_3$，下对角线元素为 $\\beta_1,\\beta_2$。数值验证\n$$\nU_3^{T} A V_3 \\;=\\; B_3\n\\quad\\text{和}\\quad\nA V_3 \\;=\\; U_3 B_3 + \\beta_3\\, u_4 e_3^{T},\n$$\n其中 $u_4$ 是该过程产生的下一个左 Lanczos 向量，$e_3 \\in \\mathbb{R}^{3}$ 是第三个标准基向量。给出 $\\beta_3$ 的精确值作为最终答案。最终答案需为精确值，无需四舍五入。",
            "solution": "用户要求对给定的矩阵 $A \\in \\mathbb{R}^{4 \\times 4}$，从向量 $u_1 = e_1$ 开始，应用三步 Golub-Kahan 双对角化算法。目标是求出 $\\beta_3$ 的值。\n\n给定的矩阵为：\n$$\nA = \\begin{pmatrix}\n2  0  0  0 \\\\\n1  2  0  0 \\\\\n0  1  2  0 \\\\\n0  0  1  2\n\\end{pmatrix}\n$$\n其转置矩阵为：\n$$\nA^T = \\begin{pmatrix}\n2  1  0  0 \\\\\n0  2  1  0 \\\\\n0  0  2  1 \\\\\n0  0  0  2\n\\end{pmatrix}\n$$\n起始向量为 $u_1 = e_1 = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix}^T$。该算法按如下定义对 $j=1, 2, 3, \\ldots$ 进行迭代：\n$$\nr_j = A^{T} u_j - \\beta_{j-1} v_{j-1}, \\quad \\alpha_j = \\|r_j\\|_2, \\quad v_j = r_j/\\alpha_j\n$$\n$$\np_j = A v_j - \\alpha_j u_j, \\quad \\beta_j = \\|p_j\\|_2, \\quad u_{j+1} = p_j/\\beta_j\n$$\n我们从 $\\beta_0 = 0$ 开始。\n\n**步骤 1 ($j=1$)：**\n首先，我们计算 $r_1$、$\\alpha_1$ 和 $v_1$。\n$$\nr_1 = A^T u_1 - \\beta_0 v_0 = A^T e_1 - 0 = \\begin{pmatrix} 2  1  0  0 \\\\ 0  2  1  0 \\\\ 0  0  2  1 \\\\ 0  0  0  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\alpha_1 = \\|r_1\\|_2 = \\sqrt{2^2 + 0^2 + 0^2 + 0^2} = 2\n$$\n$$\nv_1 = \\frac{r_1}{\\alpha_1} = \\frac{1}{2} \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = e_1\n$$\n接着，我们计算 $p_1$、$\\beta_1$ 和 $u_2$。\n$$\np_1 = A v_1 - \\alpha_1 u_1 = A e_1 - 2 e_1 = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - 2 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\beta_1 = \\|p_1\\|_2 = \\sqrt{0^2 + 1^2 + 0^2 + 0^2} = 1\n$$\n$$\nu_2 = \\frac{p_1}{\\beta_1} = \\frac{1}{1} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = e_2\n$$\n\n**步骤 2 ($j=2$)：**\n我们使用步骤 1 的结果：$u_2=e_2$，$v_1=e_1$，$\\beta_1=1$。\n首先是 $r_2$、$\\alpha_2$ 和 $v_2$。\n$$\nr_2 = A^T u_2 - \\beta_1 v_1 = A^T e_2 - 1 e_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\alpha_2 = \\|r_2\\|_2 = \\sqrt{0^2 + 2^2 + 0^2 + 0^2} = 2\n$$\n$$\nv_2 = \\frac{r_2}{\\alpha_2} = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = e_2\n$$\n接着是 $p_2$、$\\beta_2$ 和 $u_3$。\n$$\np_2 = A v_2 - \\alpha_2 u_2 = A e_2 - 2 e_2 = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix} - 2 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\beta_2 = \\|p_2\\|_2 = \\sqrt{0^2 + 0^2 + 1^2 + 0^2} = 1\n$$\n$$\nu_3 = \\frac{p_2}{\\beta_2} = \\frac{1}{1} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = e_3\n$$\n\n**步骤 3 ($j=3$)：**\n我们使用步骤 2 的结果：$u_3=e_3$，$v_2=e_2$，$\\beta_2=1$。\n首先是 $r_3$、$\\alpha_3$ 和 $v_3$。\n$$\nr_3 = A^T u_3 - \\beta_2 v_2 = A^T e_3 - 1 e_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\alpha_3 = \\|r_3\\|_2 = \\sqrt{0^2 + 0^2 + 2^2 + 0^2} = 2\n$$\n$$\nv_3 = \\frac{r_3}{\\alpha_3} = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = e_3\n$$\n接着，我们计算 $p_3$、$\\beta_3$ 和 $u_4$。这一步将得到所要求的值 $\\beta_3$。\n$$\np_3 = A v_3 - \\alpha_3 u_3 = A e_3 - 2 e_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\\\ 1 \\end{pmatrix} - 2 \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\n$$\n\\beta_3 = \\|p_3\\|_2 = \\sqrt{0^2 + 0^2 + 0^2 + 1^2} = 1\n$$\n$$\nu_4 = \\frac{p_3}{\\beta_3} = \\frac{1}{1} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = e_4\n$$\n$\\beta_3$ 的值是 $1$。\n\n问题还要求构造矩阵并验证关系式。\n$U_3 = [u_1, u_2, u_3] = [e_1, e_2, e_3] = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix}$。\n$V_3 = [v_1, v_2, v_3] = [e_1, e_2, e_3] = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix}$。\n$B_3 = \\begin{pmatrix} \\alpha_1  0  0 \\\\ \\beta_1  \\alpha_2  0 \\\\ 0  \\beta_2  \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 2  0  0 \\\\ 1  2  0 \\\\ 0  1  2 \\end{pmatrix}$。\n\n验证 1：$U_3^T A V_3 = B_3$\n$$\nU_3^T A V_3 = \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  1  0 \\end{pmatrix} \\begin{pmatrix} 2  0  0  0 \\\\ 1  2  0  0 \\\\ 0  1  2  0 \\\\ 0  0  1  2 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 2  0  0  0 \\\\ 1  2  0  0 \\\\ 0  1  2  0 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} 2  0  0 \\\\ 1  2  0 \\\\ 0  1  2 \\end{pmatrix} = B_3\n$$\n第一个关系式成立。\n\n验证 2：$A V_3 = U_3 B_3 + \\beta_3 u_4 e_3^T$\n等式左边：\n$$\nA V_3 = \\begin{pmatrix} 2  0  0  0 \\\\ 1  2  0  0 \\\\ 0  1  2  0 \\\\ 0  0  1  2 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} 2  0  0 \\\\ 1  2  0 \\\\ 0  1  2 \\\\ 0  0  1 \\end{pmatrix}\n$$\n等式右边：\n$$\nU_3 B_3 + \\beta_3 u_4 e_3^T = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} \\begin{pmatrix} 2  0  0 \\\\ 1  2  0 \\\\ 0  1  2 \\end{pmatrix} + (1) \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0  0  1 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 2  0  0 \\\\ 1  2  0 \\\\ 0  1  2 \\\\ 0  0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  0 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 2  0  0 \\\\ 1  2  0 \\\\ 0  1  2 \\\\ 0  0  1 \\end{pmatrix}\n$$\n等式左边等于右边。第二个关系式也成立。\n\n计算证实了 $\\beta_3 = 1$。",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Golub-Kahan 双对角化过程的一个强大应用是求解大规模的正则化问题，例如吉洪诺夫（Tikhonov）正则化，这对于处理不适定或病态问题至关重要。该方法的核心思想是将原问题投影到一个维度小得多的克雷洛夫子空间中，从而高效求解。 这项练习将引导你推导这个投影后的正则化问题，并利用其奇异值分解（SVD）来根据 Morozov 差异原理选择合适的正则化参数 $\\lambda$。",
            "id": "3548852",
            "problem": "考虑一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和一个右端项 $b \\in \\mathbb{R}^{m}$，并回顾始于 $u_{1} = b / \\|b\\|_{2}$ 的 Golub-Kahan (GK) 双对角化过程。该过程构造了标准正交矩阵 $U_{k+1} \\in \\mathbb{R}^{m \\times (k+1)}$ 和 $V_{k} \\in \\mathbb{R}^{n \\times k}$，以及一个矩阵 $B_{k} \\in \\mathbb{R}^{(k+1) \\times k}$，使得 $A V_{k}$ 在 $U_{k+1}$ 上的表示为 $B_{k}$。在经典的吉洪诺夫正则化中，对于一个正则化参数 $\\lambda  0$，我们寻求一个 $x$ 来最小化二次泛函 $\\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}$。在基于 GK 的投影方法中，我们通过令 $x = V_{k} y$ 将 $x$ 限制在 $k$ 维子空间 $\\mathrm{span}(V_{k})$ 中，并研究关于 $y$ 变量的相应投影吉洪诺夫问题。\n\n从上述定义以及正交投影算子和最小二乘问题的标准性质出发：\n1. 推导以 $B_{k}$ 和 $\\beta e_{1}$ 表示的 $k$ 维投影吉洪诺夫问题，其中 $\\beta = \\|b\\|_{2}$ 且 $e_{1} \\in \\mathbb{R}^{k+1}$ 是第一个标准基向量。将该投影问题表示为一个具有适当分块矩阵和右端项的增广最小二乘问题。\n2. 仅使用关于奇异值分解 (SVD) 和正交变换的基本事实，推导投影吉洪诺夫解的谱形式，并根据 $B_{k}$ 的奇异值以及 $\\beta e_{1}$ 相对于 $B_{k}$ 的左奇异向量的谱系数，给出残差范数 $\\|B_{k} y(\\lambda) - \\beta e_{1}\\|_{2}$ 的显式表达式。\n\n现在，特殊情况取 $k = 2$，并假设经过两步 GK 过程后，矩阵为\n$$\nB_{2} = \\begin{pmatrix}\n3  0 \\\\\n4  0 \\\\\n0  5\n\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2},\n$$\n且数据范数为 $\\beta = \\|b\\|_{2} = 10$。通过对投影问题施加莫罗佐夫差异原理来研究参数选择：选择 $\\lambda  0$，使得投影残差满足 $\\|B_{2} y(\\lambda) - \\beta e_{1}\\|_{2} = \\delta$，其中给定的噪声水平为 $\\delta = \\sqrt{73}$。使用 $B_{2}$ 的奇异值分解，计算出对于给定的 $B_{2}$ 和 $\\beta$，满足投影问题差异原理的唯一正 $\\lambda$。\n\n将你最终的 $\\lambda$ 数值答案四舍五入到四位有效数字。不需要单位。",
            "solution": "1. 投影吉洪诺夫问题的推导。\n吉洪诺夫泛函为 $J(x) = \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}$。我们通过设置 $x = V_{k} y$（其中 $y \\in \\mathbb{R}^{k}$）将解 $x$ 投影到 $k$ 维子空间 $\\mathrm{span}(V_{k})$ 上。\n将此代入泛函，得到：\n$$ J(y) = \\|A V_{k} y - b\\|_{2}^{2} + \\lambda^{2} \\|V_{k} y\\|_{2}^{2} $$\n利用 Golub-Kahan 双对角化的性质，我们有 $A V_{k} = U_{k+1} B_{k}$。矩阵 $V_{k}$ 具有标准正交列，因此 $\\|V_{k} y\\|_{2}^{2} = y^{T} V_{k}^{T} V_{k} y = y^{T} I_{k} y = \\|y\\|_{2}^{2}$。双对角化的起始向量是 $u_{1} = b/\\|b\\|_{2}$，这意味着 $b = \\|b\\|_{2} u_{1}$。我们已知 $\\beta = \\|b\\|_{2}$。由于 $u_{1}$ 是标准正交矩阵 $U_{k+1}$ 的第一列，我们可以写成 $u_{1} = U_{k+1} e_{1}$，其中 $e_{1} \\in \\mathbb{R}^{k+1}$ 是第一个标准基向量。因此，$b = \\beta U_{k+1} e_{1}$。\n将这些关系代入泛函中：\n$$ J(y) = \\|U_{k+1} B_{k} y - \\beta U_{k+1} e_{1}\\|_{2}^{2} + \\lambda^{2} \\|y\\|_{2}^{2} $$\n由于 $U_{k+1}$ 是一个具有标准正交列的矩阵，左乘该矩阵保持欧几里得范数不变，即 $\\|U_{k+1}z\\|_{2} = \\|z\\|_{2}$。应用此性质，我们得到投影的吉洪诺夫泛函：\n$$ J(y) = \\|B_{k} y - \\beta e_{1}\\|_{2}^{2} + \\lambda^{2} \\|y\\|_{2}^{2} $$\n此表达式需要关于 $y \\in \\mathbb{R}^{k}$ 进行最小化。这是一个带吉洪诺夫正则化的线性最小二乘问题。它可以写成一个非正则化（或增广）的最小二乘问题：\n$$ \\min_{y} \\left\\| \\begin{pmatrix} B_{k} \\\\ \\lambda I_{k} \\end{pmatrix} y - \\begin{pmatrix} \\beta e_{1} \\\\ 0 \\end{pmatrix} \\right\\|_{2}^{2} $$\n其中增广矩阵属于 $\\mathbb{R}^{(k+1+k) \\times k}$，增广右端项属于 $\\mathbb{R}^{(k+1+k)}$。\n\n2. 解的谱形式与残差范数。\n最小化 $J(y)$ 的解 $y(\\lambda)$ 可通过求解正规方程组得到：\n$$ (B_{k}^{T} B_{k} + \\lambda^{2} I_{k}) y = B_{k}^{T} (\\beta e_{1}) $$\n设 $B_{k} \\in \\mathbb{R}^{(k+1) \\times k}$ 的奇异值分解 (SVD) 为 $B_{k} = P \\Sigma Q^{T}$，其中 $P \\in \\mathbb{R}^{(k+1) \\times (k+1)}$ 和 $Q \\in \\mathbb{R}^{k \\times k}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{(k+1) \\times k}$ 是一个矩形对角矩阵，其主对角线上的元素为奇异值 $\\sigma_{1}, \\dots, \\sigma_{k}$。\n则 $B_{k}^{T} B_{k} = Q \\Sigma^{T} P^{T} P \\Sigma Q^{T} = Q (\\Sigma^{T} \\Sigma) Q^{T}$。令 $\\Sigma_{d}^{2} = \\Sigma^{T} \\Sigma = \\mathrm{diag}(\\sigma_{1}^{2}, \\dots, \\sigma_{k}^{2})$。所以，$B_{k}^{T} B_{k} = Q \\Sigma_{d}^{2} Q^{T}$。\n正规方程组变为：\n$$ (Q \\Sigma_{d}^{2} Q^{T} + \\lambda^{2} I_{k}) y = Q \\Sigma^{T} P^{T} (\\beta e_{1}) $$\n$$ Q (\\Sigma_{d}^{2} + \\lambda^{2} I_{k}) Q^{T} y = Q \\Sigma^{T} P^{T} (\\beta e_{1}) $$\n从左侧乘以 $Q^{T}$，我们得到 $y(\\lambda) = Q (\\Sigma_{d}^{2} + \\lambda^{2} I_{k})^{-1} \\Sigma^{T} P^{T} (\\beta e_{1})$。\n令 $\\gamma = P^{T} (\\beta e_{1})$；其分量为 $\\gamma_{j} = \\beta p_{j}^{T} e_{1} = \\beta (p_{j})_{1}$，其中 $p_j$ 是 $P$ 的列向量。\n则 $y(\\lambda) = \\sum_{j=1}^{k} \\frac{\\sigma_{j} \\gamma_{j}}{\\sigma_{j}^{2} + \\lambda^{2}} q_{j}$，其中 $q_j$ 是 $Q$ 的列向量。\n残差为 $r(\\lambda) = B_{k} y(\\lambda) - \\beta e_{1}$。\n$$ B_{k} y(\\lambda) = P \\Sigma Q^{T} y(\\lambda) = P \\Sigma (\\Sigma_{d}^{2} + \\lambda^{2} I_{k})^{-1} \\Sigma^{T} P^{T} (\\beta e_{1}) = P \\Sigma (\\Sigma_{d}^{2} + \\lambda^{2} I_{k})^{-1} \\Sigma^{T} \\gamma $$\n$\\Sigma^{T} \\gamma$ 的第 $j$ 个分量是 $\\sigma_{j} \\gamma_{j}$，对于 $j=1, \\dots, k$。\n$(\\Sigma_{d}^{2} + \\lambda^{2} I_{k})^{-1} \\Sigma^{T} \\gamma$ 的第 $j$ 个分量是 $\\frac{\\sigma_{j} \\gamma_{j}}{\\sigma_{j}^{2} + \\lambda^{2}}$。\n乘积 $\\Sigma (\\dots)$ 是 $\\mathbb{R}^{k+1}$ 中的一个向量，其第 $j$ 个分量为 $\\frac{\\sigma_{j}^{2} \\gamma_{j}}{\\sigma_{j}^{2} + \\lambda^{2}}$（对于 $j=1, \\dots, k$），当 $j=k+1$ 时为 $0$。\n$$ B_{k} y(\\lambda) = P \\begin{pmatrix} \\frac{\\sigma_{1}^{2} \\gamma_{1}}{\\sigma_{1}^{2} + \\lambda^{2}} \\\\ \\vdots \\\\ \\frac{\\sigma_{k}^{2} \\gamma_{k}}{\\sigma_{k}^{2} + \\lambda^{2}} \\\\ 0 \\end{pmatrix} = \\sum_{j=1}^{k} \\frac{\\sigma_{j}^{2} \\gamma_{j}}{\\sigma_{j}^{2} + \\lambda^{2}} p_{j} $$\n右端项为 $\\beta e_{1} = P \\gamma = \\sum_{j=1}^{k+1} \\gamma_{j} p_{j}$。\n残差向量为：\n$$ r(\\lambda) = \\sum_{j=1}^{k} \\left( \\frac{\\sigma_{j}^{2}}{\\sigma_{j}^{2} + \\lambda^{2}} - 1 \\right) \\gamma_{j} p_{j} - \\gamma_{k+1} p_{k+1} = \\sum_{j=1}^{k} \\frac{-\\lambda^{2} \\gamma_{j}}{\\sigma_{j}^{2} + \\lambda^{2}} p_{j} - \\gamma_{k+1} p_{k+1} $$\n由于向量 $p_{j}$ 是标准正交的，残差的范数平方等于其系数的平方和：\n$$ \\|B_{k} y(\\lambda) - \\beta e_{1}\\|_{2}^{2} = \\sum_{j=1}^{k} \\left( \\frac{\\lambda^{2} \\gamma_{j}}{\\sigma_{j}^{2} + \\lambda^{2}} \\right)^{2} + \\gamma_{k+1}^{2} $$\n\n3. $\\lambda$ 的计算。\n我们特殊化到 $k=2$ 的情况，此时 $B_{2} = \\begin{pmatrix} 3  0 \\\\ 4  0 \\\\ 0  5 \\end{pmatrix}$，$\\beta = 10$，且 $\\delta = \\sqrt{73}$。差异原理要求 $\\|B_{2} y(\\lambda) - \\beta e_{1}\\|_{2}^{2} = \\delta^{2} = 73$。\n首先，我们计算 $B_{2}$ 的 SVD。我们需要 $B_{2}^{T} B_{2}$ 的特征值和特征向量：\n$$ B_{2}^{T} B_{2} = \\begin{pmatrix} 3  4  0 \\\\ 0  0  5 \\end{pmatrix} \\begin{pmatrix} 3  0 \\\\ 4  0 \\\\ 0  5 \\end{pmatrix} = \\begin{pmatrix} 9+16  0 \\\\ 0  25 \\end{pmatrix} = \\begin{pmatrix} 25  0 \\\\ 0  25 \\end{pmatrix} $$\n特征值为 $\\sigma_{1}^{2} = 25$ 和 $\\sigma_{2}^{2} = 25$，因此奇异值为 $\\sigma_{1} = 5$ 和 $\\sigma_{2} = 5$。因为 $B_{2}^{T} B_{2}$ 已经是对角矩阵，所以右奇异向量矩阵 $Q$ 是单位矩阵 $Q = I_{2}$。\n左奇异向量 $p_{1}, p_{2}$ 由 $p_{j} = \\frac{1}{\\sigma_{j}} B_{2} q_{j}$ 给出。\n$$ p_{1} = \\frac{1}{5} \\begin{pmatrix} 3  0 \\\\ 4  0 \\\\ 0  5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3/5 \\\\ 4/5 \\\\ 0 \\end{pmatrix} $$\n$$ p_{2} = \\frac{1}{5} \\begin{pmatrix} 3  0 \\\\ 4  0 \\\\ 0  5 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 0 \\\\ 0 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} $$\n第三个左奇异向量 $p_3$ 必须与 $p_1$ 和 $p_2$ 正交，并且具有单位范数。一个与 $p_1$ 正交且在 $xy$-平面内（因为 $p_2$ 沿着 $z$ 轴）的向量是 $(4/5, -3/5, 0)^{T}$。我们选择 $p_{3} = \\begin{pmatrix} 4/5 \\\\ -3/5 \\\\ 0 \\end{pmatrix}$。\n谱系数 $\\gamma_{j}$ 为 $\\gamma_{j} = \\beta (p_{j})_{1}$。当 $\\beta=10$ 时：\n$$ \\gamma_{1} = 10 \\cdot (p_{1})_{1} = 10 \\cdot \\frac{3}{5} = 6 $$\n$$ \\gamma_{2} = 10 \\cdot (p_{2})_{1} = 10 \\cdot 0 = 0 $$\n$$ \\gamma_{3} = 10 \\cdot (p_{3})_{1} = 10 \\cdot \\frac{4}{5} = 8 $$\n现在我们将这些值代入残差范数方程：\n$$ 73 = \\left( \\frac{\\lambda^{2} \\gamma_{1}}{\\sigma_{1}^{2} + \\lambda^{2}} \\right)^{2} + \\left( \\frac{\\lambda^{2} \\gamma_{2}}{\\sigma_{2}^{2} + \\lambda^{2}} \\right)^{2} + \\gamma_{3}^{2} $$\n$$ 73 = \\left( \\frac{\\lambda^{2} \\cdot 6}{25 + \\lambda^{2}} \\right)^{2} + \\left( \\frac{\\lambda^{2} \\cdot 0}{25 + \\lambda^{2}} \\right)^{2} + 8^{2} $$\n$$ 73 = \\frac{36\\lambda^{4}}{(25 + \\lambda^{2})^{2}} + 0 + 64 $$\n$$ 73 - 64 = 9 = \\frac{36\\lambda^{4}}{(25 + \\lambda^{2})^{2}} $$\n两边除以 $36$，我们得到：\n$$ \\frac{9}{36} = \\frac{1}{4} = \\left(\\frac{\\lambda^{2}}{25 + \\lambda^{2}}\\right)^{2} $$\n对两边取平方根（并注意由于 $\\lambda0$，括号中的项为正）：\n$$ \\frac{1}{2} = \\frac{\\lambda^{2}}{25 + \\lambda^{2}} $$\n$$ 25 + \\lambda^{2} = 2\\lambda^{2} $$\n$$ \\lambda^{2} = 25 $$\n由于正则化参数 $\\lambda$ 必须为正，我们取正根：\n$$ \\lambda = 5 $$\n问题要求答案四舍五入到四位有效数字。因此，$\\lambda = 5.000$。",
            "answer": "$$\\boxed{5.000}$$"
        },
        {
            "introduction": "在诸如 LSQR 这类基于 Golub-Kahan 过程的迭代算法中，我们需要高效地监控算法的收敛进程，这通常通过追踪残差范数 $\\|r_k\\|$ 和梯度范数 $\\|A^\\top r_k\\|$ 来实现。直接计算这些范数可能非常耗时，但幸运的是，我们可以仅利用双对角化过程中产生的量（即小规模的双对角矩阵 $B_k$ 和递推标量）来精确地估计它们。 这项练习要求你推导并实现这些估计量，并通过在具有挑战性的病态问题上进行测试，来验证其数值可靠性。",
            "id": "3548830",
            "problem": "考虑一个实矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和右端向量 $b \\in \\mathbb{R}^{m}$。对于由 $k$ 步 Golub-Kahan 双对角化（GKB）产生的迭代向量 $x_k \\in \\mathbb{R}^{n}$，定义最小二乘残差 $r_k = b - A x_k$ 及其正规方程梯度 $g_k = A^\\top r_k$。Golub-Kahan 双对角化（GKB）分别为 $\\mathbb{R}^m$ 和 $\\mathbb{R}^n$ 中的 Krylov 子空间构造标准正交基 $\\{u_i\\}$ 和 $\\{v_i\\}$，以及递推标量 $\\{\\alpha_i\\}$ 和 $\\{\\beta_i\\}$，从而得到一个下双对角矩阵 $B_k \\in \\mathbb{R}^{(k+1)\\times k}$，该矩阵编码了一个小的投影最小二乘问题。\n\n您的任务是：\n- 从 Golub-Kahan 双对角化过程的基本定义和标准正交变换的性质出发，推导范数 $\\|r_k\\|$ 和 $\\|A^\\top r_k\\|$ 的估计量。这些估计量应仅依赖于 GKB 产生的双对角矩阵 $B_k$ 和递推标量 $\\{\\alpha_i\\}$ 与 $\\{\\beta_i\\}$，而不在估计量公式中直接使用矩阵 $A$。\n- 实现一个程序，执行 $k$ 步 GKB 来构建 $B_k$，求解相关的小最小二乘问题以获得 $x_k$，然后仅根据 $B_k$ 和递推标量计算 $\\|r_k\\|$ 和 $\\|A^\\top r_k\\|$ 的估计量。为了验证，请使用 $A$ 计算实际范数，并将其与估计量进行比较。\n- 通过在一组高度病态的测试问题上报告估计量相对于实际范数的相对误差来量化其可靠性。每个报告的量必须是浮点数。不适用任何物理单位或角度规格。\n\n您必须使用以下矩阵和参数的测试套件，这些测试套件旨在探究一般情况、病態严重程度和边界迭代次数：\n1. $A$ 是 $20 \\times 20$ 的希尔伯特矩阵，其元素为 $A_{ij} = \\frac{1}{i+j-1}$，$b$ 是 $\\mathbb{R}^{20}$ 中的全1向量，$k = 10$。\n2. $A$ 是 $20 \\times 20$ 的希尔伯特矩阵，$b$ 是 $\\mathbb{R}^{20}$ 中的一个固定伪随机向量（确定性生成），$k = 5$。\n3. $A$ 是 $20 \\times 20$ 的范德蒙矩阵，节点 $t_i$ 线性分布于 $[0,1]$，列是 $t_i$ 的幂 $t_i^{j-1}$（$j=1,\\dots,20$），$b$ 是 $\\mathbb{R}^{20}$ 中的全1向量，$k = 10$。\n4. $A$ 具有指数衰减的奇异值：构造 $A = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{60 \\times 20}$ 和 $V \\in \\mathbb{R}^{20 \\times 20}$ 是正交矩阵（通过对高斯随机矩阵进行基于 Householder 的瘦 QR 分解得到 Q 因子），且 $\\Sigma = \\mathrm{diag}(\\sigma_1,\\dots,\\sigma_{20})$ 满足 $\\sigma_i = 10^{-i/3}$；$b$ 是 $\\mathbb{R}^{60}$ 中的一个固定伪随机向量，$k = 12$。\n5. 边界情况：$A$ 是 $10 \\times 10$ 的希尔伯特矩阵，$b$ 是 $\\mathbb{R}^{10}$ 中的全1向量，$k = 1$。\n\n对每个测试用例，计算：\n- 基于估计量的残差范数 $\\|r_k\\|$ 及其对应的实际值，并报告相对误差，其定义为 $\\left|\\|r_k\\|_{\\mathrm{est}} - \\|r_k\\|_{\\mathrm{act}}\\right| / \\max(\\|r_k\\|_{\\mathrm{act}}, \\varepsilon)$，其中 $\\varepsilon$ 是一个小的正常数，以避免除以零。\n- 基于估计量的梯度范数 $\\|A^\\top r_k\\|$ 及其对应的实际值，并报告类似的相对误差。\n\n您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序如下：\n$[\\mathrm{err}_{r}^{(1)}, \\mathrm{err}_{g}^{(1)}, \\mathrm{err}_{r}^{(2)}, \\mathrm{err}_{g}^{(2)}, \\mathrm{err}_{r}^{(3)}, \\mathrm{err}_{g}^{(3)}, \\mathrm{err}_{r}^{(4)}, \\mathrm{err}_{g}^{(4)}, \\mathrm{err}_{r}^{(5)}, \\mathrm{err}_{g}^{(5)}]$,\n其中 $\\mathrm{err}_{r}^{(i)}$ 是第 $i$ 个测试用例的残差范数相对误差，$\\mathrm{err}_{g}^{(i)}$ 是梯度范数相对误差。\n\n最终答案必须是可运行的代码，执行所有计算并按指定格式打印结果。不允许来自外部源的输入，且运行过程必须是自包含和确定性的。",
            "solution": "该问题要求使用 Golub-Kahan 双对角化（GKB）过程产生的量，推导并实现最小二乘残差范数 $\\|r_k\\|$ 及其对应的正规方程梯度范数 $\\|A^\\top r_k\\|$ 的估计量。\n\n### Golub-Kahan 双对角化（GKB）过程\n给定矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和向量 $b \\in \\mathbb{R}^{m}$，GKB 过程生成两个标准正交向量序列 $\\{u_i\\}_{i=1}^{k+1} \\subset \\mathbb{R}^m$ 和 $\\{v_i\\}_{i=1}^{k} \\subset \\mathbb{R}^n$，以及两个正标量序列 $\\{\\alpha_i\\}_{i=1}^{k}$ 和 $\\{\\beta_i\\}_{i=1}^{k+1}$。该过程初始化如下：\n$$\n\\beta_1 = \\|b\\|_2, \\quad u_1 = b / \\beta_1, \\quad v_0 = 0\n$$\n对于 $i=1, \\dots, k$，递推关系为：\n$$\n\\tilde{v}_i = A^\\top u_i - \\beta_i v_{i-1}, \\quad \\alpha_i = \\|\\tilde{v}_i\\|_2, \\quad v_i = \\tilde{v}_i / \\alpha_i\n$$\n$$\n\\tilde{u}_{i+1} = A v_i - \\alpha_i u_i, \\quad \\beta_{i+1} = \\|\\tilde{u}_{i+1}\\|_2, \\quad u_{i+1} = \\tilde{u}_{i+1} / \\beta_{i+1}\n$$\n令 $U_{k+1} = [u_1, \\dots, u_{k+1}] \\in \\mathbb{R}^{m \\times (k+1)}$ 且 $V_k = [v_1, \\dots, v_k] \\in \\mathbb{R}^{n \\times k}$。根据构造，有 $U_{k+1}^\\top U_{k+1} = I_{k+1}$ 和 $V_k^\\top V_k = I_k$。递推关系可以用矩阵形式表示：\n$$\nA V_k = U_{k+1} B_k\n$$\n其中 $B_k \\in \\mathbb{R}^{(k+1) \\times k}$ 是一个下双对角矩阵：\n$$\nB_k =\n\\begin{pmatrix}\n\\alpha_1    \\\\\n\\beta_2  \\alpha_2   \\\\\n \\beta_3  \\ddots  \\\\\n  \\ddots  \\alpha_k \\\\\n   \\beta_{k+1}\n\\end{pmatrix}\n$$\nGKB 过程为 Krylov 子空间 $\\mathcal{K}_k(A^\\top A, A^\\top b)$ 提供了一个标准正交基 $V_k$。最小二乘问题 $\\min_x \\|b-Ax\\|_2$ 的近似解 $x_k$ 在此子空间中寻找，即 $x_k = V_k y_k$，其中 $y_k \\in \\mathbb{R}^k$。\n\n### $\\|r_k\\|$ 估计量的推导\n残差 $r_k = b - Ax_k$ 可以用 GKB 的量来表示。代入 $x_k = V_k y_k$ 并使用 GKB 矩阵关系：\n$$\nr_k = b - A(V_k y_k) = b - (AV_k)y_k\n$$\n根据初始化，有 $b = \\beta_1 u_1$。由于 $U_{k+1}$ 是一个包含 $u_1$ 的标准正交基，我们可以写出 $b = U_{k+1}(\\beta_1 e_1)$，其中 $e_1 \\in \\mathbb{R}^{k+1}$ 是第一个标准基向量。\n$$\nr_k = U_{k+1}(\\beta_1 e_1) - (U_{k+1} B_k) y_k = U_{k+1} (\\beta_1 e_1 - B_k y_k)\n$$\n由于 $U_{k+1}$ 是一个列正交矩阵，取欧几里得范数会保持长度不变：\n$$\n\\|r_k\\|_2 = \\|U_{k+1} (\\beta_1 e_1 - B_k y_k)\\|_2 = \\|\\beta_1 e_1 - B_k y_k\\|_2\n$$\n选择向量 $y_k$ 来最小化 $\\|r_k\\|_2$，这等价于求解以下小的投影最小二乘问题：\n$$\ny_k = \\arg\\min_{y \\in \\mathbb{R}^k} \\|\\beta_1 e_1 - B_k y\\|_2\n$$\n因此，残差范数的估计量是这个小问题的残差的范数，它仅由 $B_k$ 和 $\\beta_1$ 计算得出。在理想算术中，这是一个精确的恒等式，而不是一个估计量。在有限精度下，它作为一个鲁棒的估计量，通常比直接计算 $\\|b-Ax_k\\|$ 更准确。\n$$\n\\|r_k\\|_{\\mathrm{est}} = \\min_{y \\in \\mathbb{R}^k} \\|\\beta_1 e_1 - B_k y\\|_2\n$$\n\n### $\\|A^\\top r_k\\|$ 估计量的推导\n令 $g_k = A^\\top r_k$ 为正规方程梯度。令 $s_k = \\beta_1 e_1 - B_k y_k$ 为投影问题的残差。我们有 $r_k = U_{k+1} s_k$。\n投影问题的最优性条件是其残差与 $B_k$ 的列正交，即 $B_k^\\top s_k = 0$。这给出了 $k$ 个线性方程：\n$$\n\\alpha_i (s_k)_i + \\beta_{i+1} (s_k)_{i+1} = 0 \\quad \\text{for } i=1, \\dots, k\n$$\n现在，我们使用 GKB 递推关系来表示 $g_k$：\n$$\ng_k = A^\\top r_k = A^\\top U_{k+1} s_k = \\sum_{i=1}^{k+1} (A^\\top u_i) (s_k)_i = \\sum_{i=1}^{k} (A^\\top u_i) (s_k)_i + (A^\\top u_{k+1}) (s_k)_{k+1}\n$$\n使用 $A^\\top u_i = \\alpha_i v_i + \\beta_i v_{i-1}$（其中 $v_0=0$）：\n$$\ng_k = \\sum_{i=1}^{k} (\\alpha_i v_i + \\beta_i v_{i-1}) (s_k)_i + (A^\\top u_{k+1}) (s_k)_{k+1}\n$$\n通过将具有相同 $v_i$ 的项分组来重新排列求和：\n$$\ng_k = v_1(\\alpha_1 (s_k)_1 + \\beta_2 (s_k)_2) + \\dots + v_{k-1}(\\alpha_{k-1} (s_k)_{k-1} + \\beta_k (s_k)_k) + v_k(\\alpha_k (s_k)_k) + (A^\\top u_{k+1}) (s_k)_{k+1}\n$$\n根据最优性条件 $B_k^\\top s_k = 0$，对于 $i=1, \\dots, k-1$，每一项 $(\\alpha_i (s_k)_i + \\beta_{i+1} (s_k)_{i+1})$ 都为零。表达式简化为：\n$$\ng_k = v_k(\\alpha_k (s_k)_k) + (A^\\top u_{k+1}) (s_k)_{k+1}\n$$\n来自 $B_k^\\top s_k = 0$ 的第 $k$ 个方程是 $\\alpha_k (s_k)_k + \\beta_{k+1} (s_k)_{k+1} = 0$，所以 $\\alpha_k (s_k)_k = -\\beta_{k+1} (s_k)_{k+1}$。\n$$\ng_k = v_k(-\\beta_{k+1} (s_k)_{k+1}) + (A^\\top u_{k+1}) (s_k)_{k+1} = (s_k)_{k+1} (A^\\top u_{k+1} - \\beta_{k+1} v_k)\n$$\n项 $(A^\\top u_{k+1} - \\beta_{k+1} v_k)$ 正是下一步 GKB 中将计算的未归一化向量 $\\tilde{v}_{k+1}$。因此，其范数为 $\\alpha_{k+1}$。\n$$\ng_k = (s_k)_{k+1} \\tilde{v}_{k+1} = (s_k)_{k+1} (\\alpha_{k+1} v_{k+1})\n$$\n取范数并使用 $\\|v_{k+1}\\|_2=1$：\n$$\n\\|g_k\\|_2 = \\|A^\\top r_k\\|_2 = |(s_k)_{k+1}| \\alpha_{k+1}\n$$\n项 $(s_k)_{k+1}$ 是 $s_k = \\beta_1 e_1 - B_k y_k$ 的第 $(k+1)$ 个分量。\n$$\n(s_k)_{k+1} = (\\beta_1 e_1)_{k+1} - (B_k y_k)_{k+1} = 0 - \\beta_{k+1} (y_k)_k = -\\beta_{k+1} (y_k)_k\n$$\n其中 $(y_k)_k$ 是解向量 $y_k$ 的最后一个元素。代入此式得到最终的估计量：\n$$\n\\|A^\\top r_k\\|_{\\mathrm{est}} = |-\\beta_{k+1} (y_k)_k| \\alpha_{k+1} = \\alpha_{k+1} \\beta_{k+1} |(y_k)_k|\n$$\n该估计量依赖于 $\\alpha_{k+1}$（需要超出第 $k$ 步的半步计算），$\\beta_{k+1}$（来自第 $k$ 步），以及投影问题解 $y_k$ 的最后一个分量。",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Small constant to prevent division by zero in relative error calculation.\n    EPSILON = np.finfo(float).eps\n\n    def create_hilbert_matrix(n):\n        \"\"\"Creates an n x n Hilbert matrix.\"\"\"\n        # A_ij = 1 / (i + j - 1) for 1-based indexing.\n        # This is 1 / (i_0 + j_0 + 1) for 0-based indexing.\n        return scipy.linalg.hilbert(n)\n\n    def create_vandermonde_matrix(n):\n        \"\"\"Creates an n x n Vandermonde matrix with nodes in [0, 1].\"\"\"\n        t = np.linspace(0, 1, n)\n        # increasing=True gives columns t^0, t^1, ..., t^(n-1)\n        return np.vander(t, increasing=True)\n\n    def create_exp_decay_sv_matrix(m, n, seed):\n        \"\"\"\n        Creates an m x n matrix with exponentially decaying singular values.\n        A = U @ Sigma @ V.T\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Create U (m x n) with orthonormal columns\n        rand_U_mat = rng.standard_normal((m, n))\n        U, _ = np.linalg.qr(rand_U_mat)\n        \n        # Create V (n x n) orthogonal matrix\n        rand_V_mat = rng.standard_normal((n, n))\n        V, _ = np.linalg.qr(rand_V_mat)\n        \n        # Create Sigma with singular values sigma_i = 10^(-i/3) for i=1,...,n\n        i = np.arange(1, n + 1)\n        sigmas = 10**(-i / 3.0)\n        Sigma = np.diag(sigmas)\n        \n        return U @ Sigma @ V.T\n\n    def run_gkb_test(A, b, k):\n        \"\"\"\n        Performs k steps of GKB, computes estimators and actual norms, \n        and returns the relative errors.\n        \"\"\"\n        m, n = A.shape\n        \n        # Storage for GKB quantities (using lists for dynamic length)\n        u_vectors = [np.zeros(m)]  # u_1, u_2, ..., u_{k+1}\n        v_vectors = []           # v_1, v_2, ..., v_k\n        alphas = []\n        betas = []\n\n        # --- Step 1: Golub-Kahan Bidiagonalization ---\n        # Initialization\n        beta_1 = np.linalg.norm(b)\n        if np.isclose(beta_1, 0.0):\n            return 0.0, 0.0 # Trivial case\n            \n        u_vectors[0] = b / beta_1\n        betas.append(beta_1)\n        v_prev = np.zeros(n)\n\n        # Main loop for k steps\n        for i in range(k):\n            # Update V\n            p = A.T @ u_vectors[i] - betas[i] * v_prev\n            alpha_i = np.linalg.norm(p)\n            v_i = p / alpha_i\n            \n            # Update U\n            q = A @ v_i - alpha_i * u_vectors[i]\n            beta_i_plus_1 = np.linalg.norm(q)\n            u_i_plus_1 = q / beta_i_plus_1\n\n            # Store results\n            alphas.append(alpha_i)\n            betas.append(beta_i_plus_1)\n            v_vectors.append(v_i)\n            u_vectors.append(u_i_plus_1)\n\n            v_prev = v_i\n\n        # Compute alpha_{k+1} for the gradient norm estimator\n        p_k_plus_1 = A.T @ u_vectors[k] - betas[k] * v_vectors[k-1]\n        alpha_k_plus_1 = np.linalg.norm(p_k_plus_1)\n\n        # --- Step 2: Solve Projected Least-Squares Problem ---\n        # Construct the (k+1) x k bidiagonal matrix B_k\n        B_k = np.zeros((k + 1, k))\n        np.fill_diagonal(B_k, alphas)\n        for i in range(k):\n            B_k[i+1, i] = betas[i+1]\n            \n        # Set up the right-hand side for the small problem\n        rhs_small = np.zeros(k + 1)\n        rhs_small[0] = beta_1\n        \n        # Solve min ||B_k * y - rhs_small||_2\n        y_k, residuals, _, _ = np.linalg.lstsq(B_k, rhs_small, rcond=None)\n\n        # --- Step 3: Compute Estimators ---\n        # ||r_k||_est is the residual norm of the small problem\n        # lstsq returns sum-of-squares, so take sqrt\n        norm_r_k_est = np.sqrt(residuals[0])\n        \n        # ||A^T r_k||_est = alpha_{k+1} * beta_{k+1} * |(y_k)_k|\n        beta_k_plus_1 = betas[k]\n        y_k_last = y_k[-1]\n        norm_g_k_est = alpha_k_plus_1 * beta_k_plus_1 * np.abs(y_k_last)\n        \n        # --- Step 4: Compute Actual Norms for Validation ---\n        V_k = np.array(v_vectors).T  # Shape: (n, k)\n        x_k = V_k @ y_k\n        \n        # Actual residual r_k = b - A*x_k\n        r_k_act = b - A @ x_k\n        norm_r_k_act = np.linalg.norm(r_k_act)\n        \n        # Actual gradient g_k = A^T * r_k\n        g_k_act = A.T @ r_k_act\n        norm_g_k_act = np.linalg.norm(g_k_act)\n\n        # --- Step 5: Compute Relative Errors ---\n        err_r = np.abs(norm_r_k_est - norm_r_k_act) / max(norm_r_k_act, EPSILON)\n        err_g = np.abs(norm_g_k_est - norm_g_k_act) / max(norm_g_k_act, EPSILON)\n        \n        return err_r, err_g\n\n    # --- Test Suite Definition ---\n    # Using a fixed seed for deterministic pseudorandom vectors\n    rng_seed = 42\n    rng = np.random.default_rng(rng_seed)\n\n    test_cases = [\n        {\n            \"A\": create_hilbert_matrix(20),\n            \"b\": np.ones(20),\n            \"k\": 10\n        },\n        {\n            \"A\": create_hilbert_matrix(20),\n            \"b\": rng.standard_normal(20),\n            \"k\": 5\n        },\n        {\n            \"A\": create_vandermonde_matrix(20),\n            \"b\": np.ones(20),\n            \"k\": 10\n        },\n        {\n            \"A\": create_exp_decay_sv_matrix(60, 20, seed=rng_seed),\n            \"b\": rng.standard_normal(60),\n            \"k\": 12\n        },\n        {\n            \"A\": create_hilbert_matrix(10),\n            \"b\": np.ones(10),\n            \"k\": 1\n        }\n    ]\n\n    # --- Run all tests and collect results ---\n    results = []\n    for case in test_cases:\n        err_r, err_g = run_gkb_test(case[\"A\"], case[\"b\"], case[\"k\"])\n        results.extend([err_r, err_g])\n    \n    # --- Final Output ---\n    # Convert results to string with specified precision format\n    result_strings = [f\"{res:.8e}\" for res in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n\n```"
        }
    ]
}