{
    "hands_on_practices": [
        {
            "introduction": "The best way to develop intuition for an abstract result like the Davis-Kahan theorem is to apply it to a simple, clean example. This practice guides you through a first-principles derivation for a fundamental $2 \\times 2$ Hermitian system, making the relationship between the perturbation norm $\\|E\\|_2$, the spectral gap, and the resulting subspace rotation $\\sin\\theta$ concrete. By calculating the exact angle and comparing it to the bound, you will see firsthand how and why the theorem provides a sharp estimate for worst-case perturbations.",
            "id": "3540476",
            "problem": "Let $A=\\mathrm{diag}(\\alpha,\\beta)\\in\\mathbb{C}^{2\\times 2}$ be a Hermitian matrix with $\\alpha,\\beta\\in\\mathbb{R}$ and $\\alpha>\\beta$, and let $E=\\epsilon\\begin{bmatrix}0 & 1 \\\\ 1 & 0\\end{bmatrix}$ with $\\epsilon\\in\\mathbb{R}$. Consider the perturbed Hermitian matrix $A+E=\\begin{bmatrix}\\alpha & \\epsilon \\\\ \\epsilon & \\beta\\end{bmatrix}$. Define the one-dimensional invariant subspace $\\mathcal{U}=\\mathrm{span}\\{u\\}$ of $A$ with $u=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$ (the eigenvector corresponding to $\\alpha$), and let $\\mathcal{V}=\\mathrm{span}\\{v\\}$ be the one-dimensional invariant subspace of $A+E$ associated with its larger eigenvalue, with $v$ a unit eigenvector. Let $\\theta\\in[0,\\pi/2]$ denote the principal angle (measured in radians) between $\\mathcal{U}$ and $\\mathcal{V}$, so that $\\sin\\theta=\\|P_{\\mathcal{U}^{\\perp}} v\\|_{2}$, where $P_{\\mathcal{U}^{\\perp}}$ is the orthogonal projector onto the orthogonal complement of $\\mathcal{U}$.\n\nStarting from the fundamental definitions of eigenvalues/eigenvectors of Hermitian matrices, principal angles of subspaces, and the spectral norm of a matrix (the largest singular value), perform the following:\n\n- Compute the exact closed-form expression for $\\sin\\theta$ in terms of $\\alpha$, $\\beta$, and $\\epsilon$.\n- Using first principles, deduce the inequality $\\sin\\theta\\le \\|E\\|_{2}/|\\alpha-\\beta|$ for this instance and explain why this example is near-tight by analyzing the limiting ratio $\\displaystyle\\lim_{\\epsilon\\to 0}\\frac{\\sin\\theta}{\\|E\\|_{2}/|\\alpha-\\beta|}$.\n\nExpress your final answer as the single closed-form analytic expression for $\\sin\\theta$ in terms of $\\alpha$, $\\beta$, and $\\epsilon$. No rounding is required and no physical units apply. Angles are to be interpreted in radians.",
            "solution": "The problem is first validated against the specified criteria.\n\n**Step 1: Extract Givens**\n-   Hermitian matrix $A=\\mathrm{diag}(\\alpha,\\beta)\\in\\mathbb{C}^{2\\times 2}$ with $\\alpha,\\beta\\in\\mathbb{R}$ and $\\alpha>\\beta$.\n-   Perturbation matrix $E=\\epsilon\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}$ with $\\epsilon\\in\\mathbb{R}$.\n-   Perturbed Hermitian matrix $A+E=\\begin{bmatrix}\\alpha&\\epsilon\\\\\\epsilon&\\beta\\end{bmatrix}$.\n-   One-dimensional invariant subspace of $A$, $\\mathcal{U}=\\mathrm{span}\\{u\\}$, where $u=\\begin{bmatrix}1\\\\0\\end{bmatrix}$ is the eigenvector corresponding to the eigenvalue $\\alpha$.\n-   One-dimensional invariant subspace of $A+E$, $\\mathcal{V}=\\mathrm{span}\\{v\\}$, where $v$ is a unit eigenvector associated with the larger eigenvalue of $A+E$.\n-   Principal angle $\\theta\\in[0,\\pi/2]$ between $\\mathcal{U}$ and $\\mathcal{V}$.\n-   Definition of the principal angle: $\\sin\\theta=\\|P_{\\mathcal{U}^{\\perp}} v\\|_{2}$, where $P_{\\mathcal{U}^{\\perp}}$ is the orthogonal projector onto $\\mathcal{U}^{\\perp}$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is a standard exercise in numerical linear algebra, specifically matrix perturbation theory. The matrices and concepts (Hermitian matrices, eigenvalues, eigenvectors, invariant subspaces, principal angles, spectral norm) are well-defined and fundamental in this field. All premises are factually correct.\n-   **Well-Posed:** The problem is clearly stated with all necessary information to find a unique solution for $\\sin\\theta$. The existence of eigenvalues and eigenvectors for a Hermitian matrix is guaranteed by the spectral theorem.\n-   **Objective:** The language is precise and mathematical, free of any subjective or opinion-based statements.\n-   The problem is self-contained and internally consistent. The definitions provided align with standard mathematical literature. The tasks are specific and formalizable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Part 1: Computation of $\\sin\\theta$**\n\nFirst, we find the eigenvalues and the relevant eigenvector of the perturbed matrix $A+E$. The characteristic equation is $\\det(A+E - \\lambda I) = 0$.\n$$\n\\det\\left(\\begin{bmatrix} \\alpha-\\lambda & \\epsilon \\\\ \\epsilon & \\beta-\\lambda \\end{bmatrix}\\right) = (\\alpha-\\lambda)(\\beta-\\lambda) - \\epsilon^2 = 0\n$$\nThis is a quadratic equation for $\\lambda$: $\\lambda^2 - (\\alpha+\\beta)\\lambda + \\alpha\\beta - \\epsilon^2 = 0$.\nThe solutions for $\\lambda$ are given by the quadratic formula:\n$$\n\\lambda = \\frac{(\\alpha+\\beta) \\pm \\sqrt{(\\alpha+\\beta)^2 - 4(\\alpha\\beta - \\epsilon^2)}}{2} = \\frac{(\\alpha+\\beta) \\pm \\sqrt{\\alpha^2+2\\alpha\\beta+\\beta^2 - 4\\alpha\\beta + 4\\epsilon^2}}{2}\n$$\n$$\n\\lambda = \\frac{(\\alpha+\\beta) \\pm \\sqrt{(\\alpha-\\beta)^2 + 4\\epsilon^2}}{2}\n$$\nThe problem specifies that $\\mathcal{V}$ is associated with the larger eigenvalue, which we denote $\\lambda_{+}$.\n$$\n\\lambda_{+} = \\frac{\\alpha+\\beta}{2} + \\frac{1}{2}\\sqrt{(\\alpha-\\beta)^2 + 4\\epsilon^2}\n$$\nNext, we find the eigenvector $v$ corresponding to $\\lambda_{+}$. Let $v = \\begin{bmatrix}v_1 \\\\ v_2\\end{bmatrix}$. The eigenvector equation is $(A+E)v = \\lambda_{+}v$, which gives the system:\n$$\n(\\alpha-\\lambda_{+})v_1 + \\epsilon v_2 = 0\n$$\n$$\n\\epsilon v_1 + (\\beta-\\lambda_{+})v_2 = 0\n$$\nThese two equations are linearly dependent. From the first equation, assuming $\\epsilon \\neq 0$, we have $v_2 = -\\frac{\\alpha-\\lambda_{+}}{\\epsilon}v_1$. The eigenvector is thus proportional to $\\begin{bmatrix} 1 \\\\ -(\\alpha-\\lambda_{+})/\\epsilon \\end{bmatrix}$. Let's define $c = -\\frac{\\alpha-\\lambda_{+}}{\\epsilon}$.\n$$\n\\alpha - \\lambda_{+} = \\alpha - \\left(\\frac{\\alpha+\\beta}{2} + \\frac{1}{2}\\sqrt{(\\alpha-\\beta)^2+4\\epsilon^2}\\right) = \\frac{\\alpha-\\beta}{2} - \\frac{1}{2}\\sqrt{(\\alpha-\\beta)^2+4\\epsilon^2}\n$$\nSo, $c = -\\frac{1}{\\epsilon}\\left(\\frac{\\alpha-\\beta}{2} - \\frac{1}{2}\\sqrt{(\\alpha-\\beta)^2+4\\epsilon^2}\\right) = \\frac{-(\\alpha-\\beta) + \\sqrt{(\\alpha-\\beta)^2+4\\epsilon^2}}{2\\epsilon}$.\nThe unit eigenvector $v$ is:\n$$\nv = \\frac{1}{\\sqrt{1+c^2}} \\begin{bmatrix} 1 \\\\ c \\end{bmatrix}\n$$\nThe invariant subspace $\\mathcal{U}$ is the span of $u = \\begin{bmatrix}1\\\\0\\end{bmatrix}$. The orthogonal complement $\\mathcal{U}^{\\perp}$ is spanned by $u^{\\perp} = \\begin{bmatrix}0\\\\1\\end{bmatrix}$. The orthogonal projector onto $\\mathcal{U}^{\\perp}$ is $P_{\\mathcal{U}^{\\perp}} = u^{\\perp}(u^{\\perp})^T = \\begin{bmatrix}0 & 0 \\\\ 0 & 1\\end{bmatrix}$.\nThe definition of $\\sin\\theta$ is $\\sin\\theta = \\|P_{\\mathcal{U}^{\\perp}} v\\|_{2}$.\n$$\nP_{\\mathcal{U}^{\\perp}} v = \\begin{bmatrix}0 & 0 \\\\ 0 & 1\\end{bmatrix} \\frac{1}{\\sqrt{1+c^2}} \\begin{bmatrix} 1 \\\\ c \\end{bmatrix} = \\frac{1}{\\sqrt{1+c^2}} \\begin{bmatrix} 0 \\\\ c \\end{bmatrix}\n$$\n$$\n\\sin\\theta = \\left\\| \\frac{1}{\\sqrt{1+c^2}} \\begin{bmatrix} 0 \\\\ c \\end{bmatrix} \\right\\|_{2} = \\frac{|c|}{\\sqrt{1+c^2}}\n$$\nThis can be simplified by a trigonometric substitution. Let $\\delta = \\alpha-\\beta > 0$. Then $c = \\frac{-\\delta + \\sqrt{\\delta^2+4\\epsilon^2}}{2\\epsilon}$.\nLet $\\phi \\in [0, \\pi/2)$ be an angle such that $\\tan\\phi = \\frac{2|\\epsilon|}{\\delta}$.\nThen $\\sqrt{\\delta^2+4\\epsilon^2} = \\sqrt{\\delta^2(1 + 4\\epsilon^2/\\delta^2)} = \\delta\\sqrt{1+\\tan^2\\phi} = \\delta\\sec\\phi$.\nSubstituting this into the expression for $c$:\n$$\nc = \\frac{-\\delta + \\delta\\sec\\phi}{2\\epsilon} = \\frac{\\delta}{2\\epsilon}(\\sec\\phi - 1)\n$$\nIf $\\epsilon > 0$, then $\\frac{\\delta}{2\\epsilon} = \\frac{1}{\\tan\\phi} = \\cot\\phi$.\n$c = \\cot\\phi(\\sec\\phi - 1) = \\frac{\\cos\\phi}{\\sin\\phi}\\left(\\frac{1}{\\cos\\phi}-1\\right) = \\frac{1-\\cos\\phi}{\\sin\\phi} = \\tan(\\phi/2)$.\nIf $\\epsilon < 0$, then $\\frac{\\delta}{2\\epsilon} = -\\cot\\phi$, so $c = -\\tan(\\phi/2)$.\nIn both cases, $|c| = \\tan(\\phi/2)$. Since $\\theta\\in[0,\\pi/2]$ and $\\phi/2\\in[0,\\pi/4)$, both $\\sin\\theta$ and $\\tan(\\phi/2)$ are non-negative.\n$$\n\\sin\\theta = \\frac{\\tan(\\phi/2)}{\\sqrt{1+\\tan^2(\\phi/2)}} = \\frac{\\tan(\\phi/2)}{\\sec(\\phi/2)} = \\sin(\\phi/2)\n$$\nWe can express $\\sin(\\phi/2)$ in terms of $\\alpha, \\beta, \\epsilon$ using the half-angle identity $\\sin^2(\\phi/2) = \\frac{1-\\cos\\phi}{2}$.\nFrom $\\tan\\phi = \\frac{2|\\epsilon|}{\\alpha-\\beta}$, we get $\\cos\\phi = \\frac{1}{\\sqrt{1+\\tan^2\\phi}} = \\frac{1}{\\sqrt{1 + \\frac{4\\epsilon^2}{(\\alpha-\\beta)^2}}} = \\frac{\\alpha-\\beta}{\\sqrt{(\\alpha-\\beta)^2+4\\epsilon^2}}$.\nTherefore,\n$$\n\\sin^2\\theta = \\sin^2(\\phi/2) = \\frac{1}{2}\\left(1 - \\frac{\\alpha-\\beta}{\\sqrt{(\\alpha-\\beta)^2+4\\epsilon^2}}\\right)\n$$\nSince $\\theta \\in [0, \\pi/2]$, $\\sin\\theta \\ge 0$. Taking the square root gives the final expression for $\\sin\\theta$:\n$$\n\\sin\\theta = \\sqrt{\\frac{1}{2}\\left(1 - \\frac{\\alpha-\\beta}{\\sqrt{(\\alpha-\\beta)^2+4\\epsilon^2}}\\right)}\n$$\n\n**Part 2: The Inequality and its Tightness**\n\nThis part concerns the inequality $\\sin\\theta \\le \\frac{\\|E\\|_{2}}{|\\alpha-\\beta|}$.\nFirst, we compute the spectral norm $\\|E\\|_2$. $\\|E\\|_2$ is the largest singular value of $E$, which is $\\sqrt{\\lambda_{\\max}(E^TE)}$.\n$E = \\epsilon\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}$, so $E^T = E$.\n$E^TE = E^2 = \\epsilon^2\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix} = \\epsilon^2\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix} = \\epsilon^2I$.\nThe eigenvalues of $E^2$ are both $\\epsilon^2$. The largest eigenvalue is $\\lambda_{\\max}(E^2) = \\epsilon^2$.\nSo, $\\|E\\|_2 = \\sqrt{\\epsilon^2} = |\\epsilon|$.\nThe inequality is $\\sin\\theta \\le \\frac{|\\epsilon|}{\\alpha-\\beta}$ (since $\\alpha>\\beta$, $|\\alpha-\\beta|=\\alpha-\\beta$).\nLet $x = \\frac{|\\epsilon|}{\\alpha-\\beta}$. The inequality becomes $\\sin\\theta \\le x$.\nIn Part 1, we found $\\sin\\theta = \\sin(\\phi/2)$ where $\\tan\\phi = \\frac{2|\\epsilon|}{\\alpha-\\beta} = 2x$.\nWe need to prove $\\sin(\\phi/2) \\le x$.\nLet $t = \\tan(\\phi/2)$. From the double angle identity for tangent, $\\tan\\phi = \\frac{2\\tan(\\phi/2)}{1-\\tan^2(\\phi/2)}$, we have $2x = \\frac{2t}{1-t^2}$, so $x = \\frac{t}{1-t^2}$.\nThe sine function can be expressed in terms of the tangent of the half-angle as $\\sin(\\phi/2) = \\frac{t}{\\sqrt{1+t^2}}$.\nThe inequality we wish to prove is $\\frac{t}{\\sqrt{1+t^2}} \\le \\frac{t}{1-t^2}$.\nSince $\\phi \\in [0, \\pi/2)$, we have $t = \\tan(\\phi/2) \\in [0, 1)$. Thus, $t \\ge 0$ and $1-t^2 > 0$.\nIf $t=0$ (i.e., $\\epsilon=0$), the inequality is $0 \\le 0$, which is true.\nIf $t > 0$, we can divide by $t$ to get $\\frac{1}{\\sqrt{1+t^2}} \\le \\frac{1}{1-t^2}$, which is equivalent to $1-t^2 \\le \\sqrt{1+t^2}$.\nSince both sides are positive for $t \\in (0,1)$, we can square both sides:\n$(1-t^2)^2 \\le 1+t^2$\n$1 - 2t^2 + t^4 \\le 1+t^2$\n$t^4 - 3t^2 \\le 0$\n$t^2(t^2 - 3) \\le 0$.\nSince $t^2 \\ge 0$, this inequality holds if and only if $t^2 - 3 \\le 0$, or $t^2 \\le 3$.\nSince $t = \\tan(\\phi/2)$ and $\\phi/2 \\in [0, \\pi/4)$, we have $t < 1$, so $t^2 < 1$. The condition $t^2 \\le 3$ is therefore always satisfied. This confirms the inequality.\n\nTo analyze the tightness, we evaluate the limit:\n$$\n\\lim_{\\epsilon\\to 0}\\frac{\\sin\\theta}{\\|E\\|_{2}/|\\alpha-\\beta|} = \\lim_{\\epsilon\\to 0}\\frac{\\sin\\theta}{|\\epsilon|/(\\alpha-\\beta)}\n$$\nUsing the notation from above, this is $\\lim_{x\\to 0} \\frac{\\sin(\\phi/2)}{x}$.\nAs $\\epsilon \\to 0$, $x \\to 0$, which implies $\\phi \\to 0$ and $t=\\tan(\\phi/2) \\to 0$.\nThe ratio is $\\frac{\\sin(\\phi/2)}{x} = \\frac{t/\\sqrt{1+t^2}}{t/(1-t^2)} = \\frac{1-t^2}{\\sqrt{1+t^2}}$.\n$$\n\\lim_{t\\to 0} \\frac{1-t^2}{\\sqrt{1+t^2}} = \\frac{1-0}{\\sqrt{1+0}} = 1\n$$\nSince the limit of the ratio is $1$, the inequality is asymptotically tight. For small $\\epsilon$, $\\sin\\theta$ is well approximated by $\\|E\\|_{2}/|\\alpha-\\beta|$. This example demonstrates that the bound provided by the Davis-Kahan $\\sin\\theta$ theorem can be sharp.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{1}{2}\\left(1 - \\frac{\\alpha-\\beta}{\\sqrt{(\\alpha-\\beta)^2+4\\epsilon^2}}\\right)}}\n$$"
        },
        {
            "introduction": "After seeing a specific instance where the Davis-Kahan bound is sharp , we can ask a more general question: what structural properties of a perturbation cause the maximum possible subspace rotation? This exercise shifts the focus from calculation to conceptual reasoning, challenging you to analyze how the interaction between spectral clusters and the perturbation's block structure governs the sensitivity of invariant subspaces. Identifying these \"worst-case\" scenarios is crucial for understanding when an invariant subspace problem is well-conditioned or ill-conditioned.",
            "id": "3540444",
            "problem": "Let $A \\in \\mathbb{C}^{n \\times n}$ be Hermitian. Suppose the spectrum of $A$ is partitioned into two sets, $\\Lambda_{1}$ and $\\Lambda_{2}$, corresponding to an orthogonal decomposition $\\mathbb{C}^{n} = \\mathcal{U} \\oplus \\mathcal{U}^{\\perp}$, where $\\mathcal{U}$ is an invariant subspace of $A$ associated to $\\Lambda_{1}$ and $\\mathcal{U}^{\\perp}$ to $\\Lambda_{2}$. Let $P$ be the orthogonal projector onto $\\mathcal{U}$. Define the spectral gap between the two sets by $g := \\inf\\{ |\\mu - \\lambda| : \\lambda \\in \\Lambda_{1}, \\mu \\in \\Lambda_{2} \\} > 0$. Consider a Hermitian perturbation $E$ such that $A + E$ has an invariant subspace $\\widehat{\\mathcal{U}}$ of the same dimension as $\\mathcal{U}$, and let $\\widehat{P}$ be the orthogonal projector onto $\\widehat{\\mathcal{U}}$. Define the sines of principal angles between $\\mathcal{U}$ and $\\widehat{\\mathcal{U}}$ by the singular values of $(I - P)\\widehat{P}$, and denote $\\|\\sin\\Theta\\| := \\|(I - P)\\widehat{P}\\|_{2}$.\n\nA bound known as the Davis-Kahan sin $\\theta$ inequality states that, under appropriate separation assumptions, the rotation of the invariant subspace is controlled by the ratio of the perturbation size to the spectral gap. In this problem, call the bound asymptotically sharp for a family $(A_{t}, E_{t})$ with spectral gap $g$ if the ratio $\\|\\sin\\Theta(A_{t}, E_{t})\\| \\, g / \\|E_{t}\\| \\to 1$ as $\\|E_{t}\\| \\to 0$.\n\nStarting from the fundamental notions above (Hermitian spectral decomposition, orthogonal projectors for invariant subspaces, principal angles, and spectral gap), analyze small Hermitian perturbations by reducing to a basis in which $A$ is block diagonal with respect to $\\mathcal{U}$ and $\\mathcal{U}^{\\perp}$, and model the perturbed invariant subspace as a graph of a linear map $X : \\mathcal{U} \\to \\mathcal{U}^{\\perp}$ determined to leading order by a Sylvester operator involving the gap. Use this to reason about first-order subspace rotation and identify families for which the Davis-Kahan bound is asymptotically sharp.\n\nWhich of the following families $(A_{t}, E_{t})$ produce asymptotically sharp behavior in the above sense? Select all that apply.\n\nA. For each $g > 0$, let $A_{t} = \\begin{bmatrix} 0 & 0 \\\\ 0 & g \\end{bmatrix}$ and $E_{t} = t \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$, with $t \\to 0$. Consider $\\mathcal{U} = \\operatorname{span}\\{e_{1}\\}$, the invariant subspace for the smaller eigenvalue of $A_{t}$, and $\\widehat{\\mathcal{U}}$ the corresponding invariant subspace of $A_{t} + E_{t}$.\n\nB. Fix integers $k, m \\geq 1$, choose $\\lambda \\in \\mathbb{R}$ and $g > 0$, and let $A = \\operatorname{diag}(\\lambda I_{k}, (\\lambda + g) I_{m})$. Let $E = \\begin{bmatrix} 0 & T \\\\ T^{*} & 0 \\end{bmatrix}$ where $T \\in \\mathbb{C}^{k \\times m}$ has spectral norm $\\|T\\| = t \\to 0$ and is rank-one with singular vectors aligned to coordinate axes. Take $\\mathcal{U}$ to be the $\\lambda$-eigenspace and $\\widehat{\\mathcal{U}}$ the corresponding invariant subspace of $A + E$.\n\nC. Let $A$ be diagonal with two clusters of eigenvalues separated by $g > 0$ as above, and take $E$ to be diagonal (so that $E$ commutes with $A$), with $\\|E\\| \\to 0$. Take $\\mathcal{U}$ as the invariant subspace for the first cluster.\n\nD. For each $g > 0$, let $A_{t} = \\begin{bmatrix} 0 & 0 \\\\ 0 & g \\end{bmatrix}$ and $E_{t} = t \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}$, with $t \\to 0$. Consider $\\mathcal{U} = \\operatorname{span}\\{e_{1}\\}$ and $\\widehat{\\mathcal{U}}$ as in option A.\n\nE. Let $A = \\operatorname{diag}(\\Lambda_{1}, \\Lambda_{2})$ with $\\Lambda_{1} = \\operatorname{diag}(\\lambda_{1}, \\dots, \\lambda_{k})$ and $\\Lambda_{2} = \\operatorname{diag}(\\mu_{1}, \\dots, \\mu_{m})$ real diagonal blocks. Let $g = \\min_{i, j} |\\mu_{j} - \\lambda_{i}| > 0$. Choose $E = \\begin{bmatrix} 0 & T \\\\ T^{*} & 0 \\end{bmatrix}$ with $T = t \\, u v^{*}$, $t \\to 0$, where $u$ and $v$ are unit coordinate vectors aligned with the pair $(i^{*}, j^{*})$ attaining the minimal separation $|\\mu_{j^{*}} - \\lambda_{i^{*}}| = g$. Take $\\mathcal{U}$ to be the invariant subspace for $\\Lambda_{1}$ and $\\widehat{\\mathcal{U}}$ for $A + E$.\n\nAnswer by selecting the correct option(s).",
            "solution": "We begin by setting the stage for small Hermitian perturbations of invariant subspaces. Let $A$ be Hermitian with an invariant subspace $\\mathcal{U}$ (dimension $k$) and orthogonal complement $\\mathcal{U}^{\\perp}$ (dimension $m = n - k$), with associated projectors $P$ and $I - P$. In an orthonormal basis adapted to this decomposition, we write\n$$\nA = \\begin{bmatrix} A_{11} & 0 \\\\ 0 & A_{22} \\end{bmatrix}, \\quad E = \\begin{bmatrix} E_{11} & E_{12} \\\\ E_{21} & E_{22} \\end{bmatrix},\n$$\nwith $A_{11} \\in \\mathbb{R}^{k \\times k}$, $A_{22} \\in \\mathbb{R}^{m \\times m}$ Hermitian, and $E$ Hermitian so $E_{21} = E_{12}^{*}$. The spectral gap between $\\Lambda_{1} = \\sigma(A_{11})$ and $\\Lambda_{2} = \\sigma(A_{22})$ is $g := \\inf\\{|\\mu - \\lambda| : \\lambda \\in \\Lambda_{1}, \\mu \\in \\Lambda_{2}\\} > 0$.\n\nFor small $\\|E\\|$, the invariant subspace $\\widehat{\\mathcal{U}}$ of $A + E$ can be represented as a graph of a linear map $X : \\mathcal{U} \\to \\mathcal{U}^{\\perp}$: the columns of $\\begin{bmatrix} I \\\\ X \\end{bmatrix}$ span $\\widehat{\\mathcal{U}}$. Subspace perturbation theory shows that $X$ satisfies a matrix Riccati equation\n$$\nA_{22} X - X A_{11} + E_{22} X - X E_{11} + E_{21} - X E_{12} X = 0.\n$$\nAt leading order in $\\|E\\|$, we neglect higher-order terms involving products of $X$ and $E$ and obtain the Sylvester equation\n$$\nA_{22} X - X A_{11} = - E_{21}.\n$$\nUnder the spectral separation assumption $g > 0$, the Sylvester operator is invertible and yields the bound\n$$\n\\|X\\| \\leq \\frac{\\|E_{21}\\|}{g}.\n$$\nMoreover, the largest sine of the principal angles between $\\mathcal{U}$ and the graph subspace is, to leading order, the operator norm of $X$. More precisely, writing the orthogonal projector onto the graph subspace yields\n$$\n\\widehat{P} = \\begin{bmatrix} (I + X^{*} X)^{-1} & (I + X^{*} X)^{-1} X^{*} \\\\ X (I + X^{*} X)^{-1} & X (I + X^{*} X)^{-1} X^{*} \\end{bmatrix},\n$$\nso\n$$\n(I - P) \\widehat{P} = \\begin{bmatrix} 0 & 0 \\\\ X (I + X^{*} X)^{-1} & X (I + X^{*} X)^{-1} X^{*} \\end{bmatrix}.\n$$\nHence\n$$\n\\|(I - P) \\widehat{P}\\| = \\|X (I + X^{*} X)^{-1/2}\\| = \\|X\\| + \\mathcal{O}(\\|X\\|^{3}).\n$$\nCombining these relations, we conclude that for small $\\|E\\|$\n$$\n\\|\\sin\\Theta\\| = \\|(I - P)\\widehat{P}\\| = \\frac{\\|E_{21}\\|}{g} + \\text{higher-order terms}.\n$$\nThis establishes the mechanism by which asymptotic sharpness occurs: when the perturbation is exclusively off-diagonal across the spectral partition and the Sylvester operator acts as scalar multiplication by $g$ on the dominant direction, the leading term $\\|X\\| = \\|E_{21}\\|/g$ is attained, so $\\|\\sin\\Theta\\| g / \\|E\\| \\to 1$ as $\\|E\\| \\to 0$. We now analyze each option.\n\nOption A. Here\n$$\nA_{t} = \\begin{bmatrix} 0 & 0 \\\\ 0 & g \\end{bmatrix}, \\quad E_{t} = t \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}, \\quad t \\to 0.\n$$\nWe consider $\\mathcal{U} = \\operatorname{span}\\{ e_{1} \\}$. The perturbed matrix is\n$$\nA_{t} + E_{t} = \\begin{bmatrix} 0 & t \\\\ t & g \\end{bmatrix}.\n$$\nThe eigenvalues are $\\lambda_{\\pm}(t) = \\frac{g \\pm \\sqrt{g^{2} + 4 t^{2}}}{2}$; the eigenvector corresponding to $\\lambda_{-}(t)$ is proportional to $\\begin{bmatrix} t / \\lambda_{-}(t) \\\\ 1 \\end{bmatrix}$. The angle $\\theta(t)$ between $\\operatorname{span}\\{e_{1}\\}$ and this eigenvector satisfies\n$$\n\\tan \\theta(t) = \\frac{|1|}{|t / \\lambda_{-}(t)|} = \\frac{|\\lambda_{-}(t)|}{|t|}.\n$$\nExpanding for small $t$,\n$$\n\\sqrt{g^{2} + 4 t^{2}} = g \\left( 1 + \\frac{2 t^{2}}{g^{2}} + \\mathcal{O}\\left(\\frac{t^{4}}{g^{4}}\\right) \\right), \\quad \\lambda_{-}(t) = \\frac{g - \\sqrt{g^{2} + 4 t^{2}}}{2} = - \\frac{t^{2}}{g} + \\mathcal{O}\\left(\\frac{t^{4}}{g^{3}}\\right).\n$$\nTherefore\n$$\n\\tan \\theta(t) = \\frac{t}{g} + \\mathcal{O}\\left(\\frac{t^{3}}{g^{3}}\\right), \\quad \\sin \\theta(t) = \\frac{t}{g} + \\mathcal{O}\\left(\\frac{t^{3}}{g^{3}}\\right).\n$$\nSince $\\|E_{t}\\|_{2} = |t|$, we obtain\n$$\n\\frac{\\|\\sin\\Theta\\| \\, g}{\\|E_{t}\\|} = \\frac{(\\sin \\theta(t)) g}{|t|} = 1 + \\mathcal{O}\\left(\\frac{t^{2}}{g^{2}}\\right) \\to 1 \\quad \\text{as } t \\to 0.\n$$\nThus option A exhibits asymptotically sharp behavior. Verdict: Correct.\n\nOption B. Here $A = \\operatorname{diag}(\\lambda I_{k}, (\\lambda + g) I_{m})$ and $E = \\begin{bmatrix} 0 & T \\\\ T^{*} & 0 \\end{bmatrix}$ with $\\|T\\| = t \\to 0$ and rank-one $T$ aligned to coordinate axes. In the block basis, the Sylvester equation for the leading-order graph map $X$ is\n$$\n(\\lambda + g) I_{m} \\, X - X \\, \\lambda I_{k} = - T^{*} \\quad \\Rightarrow \\quad g X = - T^{*} \\quad \\Rightarrow \\quad X = - \\frac{1}{g} T^{*}.\n$$\nHence $\\|X\\| = \\|T\\| / g = t / g$. As above,\n$$\n\\|\\sin\\Theta\\| = \\|X (I + X^{*} X)^{-1/2}\\| = \\frac{t}{g} + \\mathcal{O}\\left(\\frac{t^{3}}{g^{3}}\\right).\n$$\nThe higher-order terms stem from the Riccati correction $- X E_{12} X = - X T X$, which is of size $\\mathcal{O}(t^{3}/g^{2})$ because $X = \\mathcal{O}(t/g)$ and $T$ is $\\mathcal{O}(t)$. Since $\\|E\\| = \\|T\\| = t$, we conclude\n$$\n\\frac{\\|\\sin\\Theta\\| \\, g}{\\|E\\|} = 1 + \\mathcal{O}\\left(\\frac{t^{2}}{g^{2}}\\right) \\to 1 \\quad \\text{as } t \\to 0.\n$$\nThus option B is asymptotically sharp. Verdict: Correct.\n\nOption C. Here $A$ is diagonal with two clusters separated by $g > 0$, and $E$ is diagonal with $\\|E\\| \\to 0$, so $E$ commutes with $A$. In the block basis,\n$$\nE = \\begin{bmatrix} E_{11} & 0 \\\\ 0 & E_{22} \\end{bmatrix}, \\quad E_{21} = 0.\n$$\nThe Sylvester equation reduces to $A_{22} X - X A_{11} = - E_{21} = 0$, so to leading order $X = 0$. In fact, because $E$ is block diagonal, it does not couple $\\mathcal{U}$ with $\\mathcal{U}^{\\perp}$, and the invariant subspace $\\mathcal{U}$ of $A$ remains an invariant subspace of $A + E$; thus $\\widehat{\\mathcal{U}} = \\mathcal{U}$ and\n$$\n\\|\\sin\\Theta\\| = 0 \\quad \\Rightarrow \\quad \\frac{\\|\\sin\\Theta\\| \\, g}{\\|E\\|} = 0.\n$$\nThis does not approach $1$ unless $\\|E\\| \\to 0$ trivially yields $0$. Therefore option C is not asymptotically sharp. Verdict: Incorrect.\n\nOption D. With $A_{t} = \\begin{bmatrix} 0 & 0 \\\\ 0 & g \\end{bmatrix}$ and $E_{t} = t \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}$, we again have $E_{21} = 0$. The perturbation changes eigenvalues but leaves the eigenvectors $e_{1}, e_{2}$ unchanged, so $\\widehat{\\mathcal{U}} = \\mathcal{U}$ and $\\|\\sin\\Theta\\| = 0$. Consequently,\n$$\n\\frac{\\|\\sin\\Theta\\| \\, g}{\\|E_{t}\\|} = 0,\n$$\nand this family is not asymptotically sharp. Verdict: Incorrect.\n\nOption E. Here $A = \\operatorname{diag}(\\Lambda_{1}, \\Lambda_{2})$ with $\\Lambda_{1} = \\operatorname{diag}(\\lambda_{1}, \\dots, \\lambda_{k})$ and $\\Lambda_{2} = \\operatorname{diag}(\\mu_{1}, \\dots, \\mu_{m})$, and $g = \\min_{i, j} |\\mu_{j} - \\lambda_{i}| > 0$. The perturbation is $E = \\begin{bmatrix} 0 & T \\\\ T^{*} & 0 \\end{bmatrix}$ with $T = t \\, u v^{*}$, $u = e_{j^{*}} \\in \\mathbb{C}^{m}$, $v = e_{i^{*}} \\in \\mathbb{C}^{k}$ aligned with the pair $(i^{*}, j^{*})$ that attains the minimal gap $|\\mu_{j^{*}} - \\lambda_{i^{*}}| = g$. The Sylvester equation\n$$\n\\Lambda_{2} X - X \\Lambda_{1} = - T^{*}\n$$\ndecouples entrywise:\n$$\n(\\mu_{j} - \\lambda_{i}) X_{j i} = - \\overline{T_{i j}}.\n$$\nSince $T = t \\, u v^{*}$ has only $(i^{*}, j^{*})$-entry nonzero, we get\n$$\nX_{j i} = 0 \\quad \\text{for } (j, i) \\neq (j^{*}, i^{*}), \\quad X_{j^{*} i^{*}} = - \\frac{t}{\\mu_{j^{*}} - \\lambda_{i^{*}}} = - \\frac{t}{g}.\n$$\nThus $X$ is rank-one with $\\|X\\| = |t|/g$. As before,\n$$\n\\|\\sin\\Theta\\| = \\|X (I + X^{*} X)^{-1/2}\\| = \\frac{|t|}{g} + \\mathcal{O}\\left(\\frac{|t|^{3}}{g^{3}}\\right),\n$$\nwhile $\\|E\\| = \\|T\\| = |t|$. Hence\n$$\n\\frac{\\|\\sin\\Theta\\| \\, g}{\\|E\\|} = 1 + \\mathcal{O}\\left(\\frac{t^{2}}{g^{2}}\\right) \\to 1 \\quad \\text{as } t \\to 0,\n$$\ndemonstrating asymptotically sharp behavior even when the blocks $\\Lambda_{1}, \\Lambda_{2}$ are not scalar. The alignment of $u, v$ with the pair achieving the minimal separation ensures that the Sylvester operator acts as multiplication by $g$ on the perturbation mode, realizing the leading constant $1/g$. Verdict: Correct.\n\nIn summary, options A, B, and E construct families in which the perturbation is purely off-diagonal across the spectral partition and aligned to the direction of minimal separation, so the first-order solution of the Sylvester equation attains the constant $1/g$ and the Davis-Kahan bound is asymptotically sharp. Options C and D lack off-diagonal coupling and therefore produce no rotation of the invariant subspace.",
            "answer": "$$\\boxed{ABE}$$"
        },
        {
            "introduction": "While analytical exercises are key for building theoretical understanding, computational verification solidifies these concepts and prepares you for real-world applications. This practice bridges the gap between theory and code by asking you to numerically investigate the Davis-Kahan $\\sin\\Theta$ theorem. You will implement the core components of the theory—computing invariant subspaces, measuring principal angles via singular value decomposition, and calculating the spectral gap—to confirm that the inequality holds in practice.",
            "id": "3168152",
            "problem": "Consider real symmetric matrices and orthonormal subspaces arising from their eigen-decompositions. Use only the following fundamental definitions and facts as your base: (i) a real symmetric matrix has an orthonormal eigenbasis; (ii) an orthonormal basis for a subspace can be represented by a matrix with orthonormal columns; (iii) the orthogonal projector onto the column space of a matrix with orthonormal columns $U$ is $P = U U^\\top$; (iv) the singular values of $U^\\top V$ for two orthonormal bases $U$ and $V$ of the same dimension are the cosines of the principal angles between the subspaces; (v) the spectral norm $\\|M\\|_2$ of a matrix $M$ is its largest singular value; and (vi) the spectral gap associated with a split of the spectrum of a symmetric matrix $A$ into two groups is the minimum absolute difference between an eigenvalue in one group and an eigenvalue in the other group. Angles must be measured in radians.\n\nYour task is to write a complete program that, for each test case, performs all of the following steps using these definitions:\n\n1. Given a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, a symmetric perturbation $E \\in \\mathbb{R}^{n \\times n}$, and an integer $k$ with $1 \\le k \\le n-1$, form $\\tilde{A} = A + E$.\n2. Let $U_k \\in \\mathbb{R}^{n \\times k}$ be an orthonormal basis for the invariant subspace of $A$ spanned by the eigenvectors associated with the $k$ largest eigenvalues of $A$. Let $\\tilde{U}_k \\in \\mathbb{R}^{n \\times k}$ be defined analogously for $\\tilde{A}$.\n3. Compute the maximum principal angle $\\theta_{\\max}$ between the subspaces $\\mathcal{U}_k = \\mathrm{range}(U_k)$ and $\\tilde{\\mathcal{U}}_k = \\mathrm{range}(\\tilde{U}_k)$ by using the definition that the singular values of $U_k^\\top \\tilde{U}_k$ are the cosines of the principal angles. Extract the smallest singular value $\\sigma_{\\min}$ of $U_k^\\top \\tilde{U}_k$, then compute $\\sin(\\theta_{\\max}) = \\sqrt{1 - \\sigma_{\\min}^2}$.\n4. Compute the spectral norm $\\|E\\|_2$.\n5. Compute the spectral gap $\\delta$ for the split at $k$ using only the eigenvalues of $A$, defined as\n   $$ \\delta = \\min_{i \\in \\{1,\\dots,k\\},\\; j \\in \\{k+1,\\dots,n\\}} \\left| \\lambda_i - \\lambda_j \\right|, $$\n   where $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_n$ are the eigenvalues of $A$ in nonincreasing order.\n6. Compare the two nonnegative quantities $\\sin(\\theta_{\\max})$ and the ratio $r = \\|E\\|_2 / \\delta$. Produce a Boolean value that is true if and only if $\\sin(\\theta_{\\max}) \\le r$ holds within an absolute tolerance of $10^{-10}$ (that is, treat $\\sin(\\theta_{\\max}) \\le r + 10^{-10}$ as true), and false otherwise.\n\nTest Suite. Your program must hard-code and use exactly the following five test cases, each specified by $(A, E, k)$:\n\n- Test case 1: $n = 5$, $k = 2$,\n  $$ A = \\mathrm{diag}(5.0, 3.0, 1.0, -1.0, -2.0), $$\n  $$ E = 0.02 \\times \\begin{bmatrix}\n  0 & 1 & -1 & 0.5 & 0 \\\\\n  1 & 0 & 0.2 & -0.3 & 0.1 \\\\\n  -1 & 0.2 & 0 & 0.1 & 0.4 \\\\\n  0.5 & -0.3 & 0.1 & 0 & -0.2 \\\\\n  0 & 0.1 & 0.4 & -0.2 & 0\n  \\end{bmatrix}. $$\n\n- Test case 2: $n = 5$, $k = 2$,\n  $$ A = \\mathrm{diag}(1.0, 0.2, 0.0, -1.0, -2.0), $$\n  $$ E = 0.01 \\times \\begin{bmatrix}\n  0 & -0.5 & 0.3 & 0 & 0.1 \\\\\n  -0.5 & 0 & 0.2 & -0.1 & 0 \\\\\n  0.3 & 0.2 & 0 & 0.4 & -0.2 \\\\\n  0 & -0.1 & 0.4 & 0 & 0.3 \\\\\n  0.1 & 0 & -0.2 & 0.3 & 0\n  \\end{bmatrix}. $$\n\n- Test case 3: $n = 4$, $k = 2$,\n  $$ A = \\mathrm{diag}(3.0, 2.0, 1.0, -1.0), \\quad E = \\begin{bmatrix}\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0\n  \\end{bmatrix}. $$\n\n- Test case 4: $n = 6$, $k = 3$,\n  $$ A = \\mathrm{diag}(4.0, 3.0, 2.0, -1.0, -2.0, -3.0), $$\n  $$ E = 0.05 \\times \\begin{bmatrix}\n  0 & 0.3 & -0.2 & 0.1 & 0 & 0 \\\\\n  0.3 & 0 & 0.25 & -0.15 & 0.05 & 0 \\\\\n  -0.2 & 0.25 & 0 & 0.2 & -0.1 & 0.05 \\\\\n  0.1 & -0.15 & 0.2 & 0 & 0.3 & -0.2 \\\\\n  0 & 0.05 & -0.1 & 0.3 & 0 & 0.25 \\\\\n  0 & 0 & 0.05 & -0.2 & 0.25 & 0\n  \\end{bmatrix}. $$\n\n- Test case 5: $n = 4$, $k = 2$,\n  $$ A = \\mathrm{diag}(1.0, 0.001, 0.0, -1.0), $$\n  $$ E = 0.00005 \\times \\begin{bmatrix}\n  0 & 1 & 0 & 0.2 \\\\\n  1 & 0 & 0.3 & -0.1 \\\\\n  0 & 0.3 & 0 & 0.4 \\\\\n  0.2 & -0.1 & 0.4 & 0\n  \\end{bmatrix}. $$\n\nAngle unit specification: all angles are in radians. The final comparison uses $\\sin(\\theta_{\\max})$ directly, so no conversion is needed.\n\nFinal Output Format. Your program should produce a single line of output containing the results for the five test cases as a comma-separated list of Boolean values enclosed in square brackets, with no spaces, in the order of the test cases given above. For example, an output could look like \"[True,False,True,True,True]\" but with no spaces: \"[True,False,True,True,True]\".",
            "solution": "The problem requires the numerical verification of an inequality related to the perturbation of invariant subspaces of a symmetric matrix. This inequality is a form of the Davis-Kahan $\\sin\\Theta$ Theorem, a fundamental result in matrix perturbation theory. The theorem bounds the angle between an invariant subspace of a matrix $A$ and the corresponding invariant subspace of a perturbed matrix $\\tilde{A} = A + E$.\n\nThe procedure is executed for several test cases, each defined by a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, a symmetric perturbation matrix $E \\in \\mathbb{R}^{n \\times n}$, and an integer $k$ defining a partition of the spectrum. For each case, we must compute two quantities and compare them. The first quantity is $\\sin(\\theta_{\\max})$, where $\\theta_{\\max}$ is the largest principal angle between the invariant subspace associated with the $k$ largest eigenvalues of $A$ and the corresponding subspace of $\\tilde{A}$. The second quantity is the ratio $r = \\|E\\|_2 / \\delta$, where $\\|E\\|_2$ is the spectral norm of the perturbation and $\\delta$ is the spectral gap of $A$ at the partition point $k$. The final step is to verify if the inequality $\\sin(\\theta_{\\max}) \\le r$ holds, within a given numerical tolerance.\n\nThe algorithm proceeds as follows for each test case $(A, E, k)$:\n\n1.  **Form the Perturbed Matrix**:\n    Given the real symmetric matrix $A$ and the symmetric perturbation $E$, we form the perturbed matrix $\\tilde{A}$ as their sum:\n    $$ \\tilde{A} = A + E $$\n    Since $A$ and $E$ are symmetric, $\\tilde{A}$ is also a real symmetric matrix.\n\n2.  **Determine the Invariant Subspaces**:\n    We must find orthonormal bases for the invariant subspaces of $A$ and $\\tilde{A}$ corresponding to their $k$ largest eigenvalues.\n    First, for matrix $A$, we solve the eigenvalue problem $Av = \\lambda v$. Since $A$ is symmetric, it has a full set of $n$ real eigenvalues $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$ and a corresponding orthonormal basis of eigenvectors $v_1, v_2, \\dots, v_n$. We sort the eigenvalues in non-increasing order: $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_n$.\n    The invariant subspace $\\mathcal{U}_k$ is the one spanned by the eigenvectors $\\{v_1, \\dots, v_k\\}$ corresponding to the $k$ largest eigenvalues. An orthonormal basis for this subspace is given by the matrix $U_k \\in \\mathbb{R}^{n \\times k}$ whose columns are these eigenvectors:\n    $$ U_k = \\begin{bmatrix} v_1 & v_2 & \\cdots & v_k \\end{bmatrix} $$\n    Analogously, we compute the eigenvalues $\\tilde{\\lambda}_i$ and eigenvectors $\\tilde{v}_i$ of $\\tilde{A}$, sort them in non-increasing order, and form the matrix $\\tilde{U}_k \\in \\mathbb{R}^{n \\times k}$ from the eigenvectors corresponding to the $k$ largest eigenvalues of $\\tilde{A}$:\n    $$ \\tilde{U}_k = \\begin{bmatrix} \\tilde{v}_1 & \\tilde{v}_2 & \\cdots & \\tilde{v}_k \\end{bmatrix} $$\n    The subspace spanned by the columns of $\\tilde{U}_k$ is denoted $\\tilde{\\mathcal{U}}_k$.\n\n3.  **Compute the Sine of the Maximum Principal Angle**:\n    The principal angles between the subspaces $\\mathcal{U}_k$ and $\\tilde{\\mathcal{U}}_k$ are defined via the singular values of the matrix $U_k^\\top \\tilde{U}_k$. Let the singular values of this $k \\times k$ matrix be $s_1 \\ge s_2 \\ge \\cdots \\ge s_k$. These singular values are the cosines of the principal angles $\\theta_i$, so $s_i = \\cos(\\theta_i)$. The angles are sorted such that $0 \\le \\theta_1 \\le \\theta_2 \\le \\dots \\le \\theta_k \\le \\pi/2$.\n    The maximum principal angle, $\\theta_{\\max} = \\theta_k$, corresponds to the smallest singular value, $\\sigma_{\\min} = s_k = \\cos(\\theta_{\\max})$. Using the trigonometric identity $\\sin^2(\\theta) + \\cos^2(\\theta) = 1$, we can compute $\\sin(\\theta_{\\max})$:\n    $$ \\sin(\\theta_{\\max}) = \\sqrt{1 - \\cos^2(\\theta_{\\max})} = \\sqrt{1 - \\sigma_{\\min}^2} $$\n    This computation requires finding the singular value decomposition (SVD) of $U_k^\\top \\tilde{U}_k$ and identifying the minimum singular value.\n\n4.  **Compute the Spectral Norm of the Perturbation**:\n    The spectral norm of the matrix $E$, denoted $\\|E\\|_2$, is defined as its largest singular value. This value measures the maximum \"stretching\" effect of the linear transformation represented by $E$.\n\n5.  **Compute the Spectral Gap**:\n    The spectral gap $\\delta$ quantifies the separation between the set of the first $k$ eigenvalues of $A$ and the set of the remaining $n-k$ eigenvalues. It is defined as:\n    $$ \\delta = \\min_{i \\in \\{1,\\dots,k\\},\\; j \\in \\{k+1,\\dots,n\\}} \\left| \\lambda_i - \\lambda_j \\right| $$\n    Since the eigenvalues are sorted in non-increasing order ($\\lambda_1 \\ge \\cdots \\ge \\lambda_k \\ge \\lambda_{k+1} \\ge \\cdots \\ge \\lambda_n$), this minimum difference is simply the difference between the $k$-th and $(k+1)$-th eigenvalues:\n    $$ \\delta = \\lambda_k - \\lambda_{k+1} $$\n    A non-zero gap ($\\delta > 0$) is crucial for the stability of the invariant subspace $\\mathcal{U}_k$. The problem statement ensures $\\delta > 0$ for all test cases by providing matrices $A$ with distinct eigenvalues.\n\n6.  **Verify the Inequality**:\n    Finally, we compare the two computed non-negative quantities. We evaluate the Boolean expression:\n    $$ \\sin(\\theta_{\\max}) \\le \\frac{\\|E\\|_2}{\\delta} $$\n    Due to floating-point arithmetic, a direct comparison can be unreliable. Therefore, we check the inequality with a small absolute tolerance of $\\epsilon = 10^{-10}$:\n    $$ \\sin(\\theta_{\\max}) \\le \\frac{\\|E\\|_2}{\\delta} + \\epsilon $$\n    The result for each test case is `True` if this condition is met and `False` otherwise.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases by verifying a form of the\n    Davis-Kahan sin(Theta) theorem for each case.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1\n        (\n            np.diag([5.0, 3.0, 1.0, -1.0, -2.0]),\n            0.02 * np.array([\n                [0.0, 1.0, -1.0, 0.5, 0.0],\n                [1.0, 0.0, 0.2, -0.3, 0.1],\n                [-1.0, 0.2, 0.0, 0.1, 0.4],\n                [0.5, -0.3, 0.1, 0.0, -0.2],\n                [0.0, 0.1, 0.4, -0.2, 0.0]\n            ]),\n            2\n        ),\n        # Test case 2\n        (\n            np.diag([1.0, 0.2, 0.0, -1.0, -2.0]),\n            0.01 * np.array([\n                [0.0, -0.5, 0.3, 0.0, 0.1],\n                [-0.5, 0.0, 0.2, -0.1, 0.0],\n                [0.3, 0.2, 0.0, 0.4, -0.2],\n                [0.0, -0.1, 0.4, 0.0, 0.3],\n                [0.1, 0.0, -0.2, 0.3, 0.0]\n            ]),\n            2\n        ),\n        # Test case 3\n        (\n            np.diag([3.0, 2.0, 1.0, -1.0]),\n            np.zeros((4, 4)),\n            2\n        ),\n        # Test case 4\n        (\n            np.diag([4.0, 3.0, 2.0, -1.0, -2.0, -3.0]),\n            0.05 * np.array([\n                [0.0, 0.3, -0.2, 0.1, 0.0, 0.0],\n                [0.3, 0.0, 0.25, -0.15, 0.05, 0.0],\n                [-0.2, 0.25, 0.0, 0.2, -0.1, 0.05],\n                [0.1, -0.15, 0.2, 0.0, 0.3, -0.2],\n                [0.0, 0.05, -0.1, 0.3, 0.0, 0.25],\n                [0.0, 0.0, 0.05, -0.2, 0.25, 0.0]\n            ]),\n            3\n        ),\n        # Test case 5\n        (\n            np.diag([1.0, 0.001, 0.0, -1.0]),\n            0.00005 * np.array([\n                [0.0, 1.0, 0.0, 0.2],\n                [1.0, 0.0, 0.3, -0.1],\n                [0.0, 0.3, 0.0, 0.4],\n                [0.2, -0.1, 0.4, 0.0]\n            ]),\n            2\n        )\n    ]\n\n    results = []\n    \n    def get_invariant_subspace(matrix, k):\n        \"\"\"\n        Computes the orthonormal basis for the invariant subspace spanned by the\n        eigenvectors associated with the k largest eigenvalues of a symmetric matrix.\n        \"\"\"\n        # eigh is for symmetric matrices. It returns eigenvalues and eigenvectors.\n        # Eigenvalues are not guaranteed to be sorted.\n        eigenvalues, eigenvectors = np.linalg.eigh(matrix)\n        \n        # Sort eigenvalues in descending order and get the sorting indices\n        sorted_indices = np.argsort(eigenvalues)[::-1]\n        \n        # Reorder eigenvectors according to sorted eigenvalues\n        sorted_eigenvectors = eigenvectors[:, sorted_indices]\n        \n        # The basis for the subspace is the matrix of the first k eigenvectors\n        U_k = sorted_eigenvectors[:, :k]\n        \n        return U_k, eigenvalues[sorted_indices]\n\n    for A, E, k in test_cases:\n        # Step 1: Form the perturbed matrix\n        A_tilde = A + E\n\n        # Step 2: Determine the invariant subspaces\n        U_k, lambda_A = get_invariant_subspace(A, k)\n        U_tilde_k, _ = get_invariant_subspace(A_tilde, k)\n\n        # Step 3: Compute the sine of the maximum principal angle\n        # The cosines of principal angles are the singular values of U_k.T @ U_tilde_k\n        svd_vals = np.linalg.svd(U_k.T @ U_tilde_k, compute_uv=False)\n        sigma_min = np.min(svd_vals)\n        # sin(theta_max) = sqrt(1 - cos^2(theta_max)) = sqrt(1 - sigma_min^2)\n        # Handle potential floating point inaccuracies where sigma_min > 1\n        sin_theta_max = np.sqrt(max(0, 1 - sigma_min**2))\n        \n        # Step 4: Compute the spectral norm of the perturbation\n        norm_E = np.linalg.norm(E, ord=2)\n\n        # Step 5: Compute the spectral gap\n        # lambda_A is already sorted in descending order\n        # The problem states 1 <= k <= n-1, so lambda_A[k] is safe\n        delta = lambda_A[k-1] - lambda_A[k]\n\n        # Step 6: Verify the inequality\n        # Avoid division by zero if delta is very small or zero, although\n        # test cases are designed to have delta > 0.\n        if delta > np.finfo(float).eps:\n            r = norm_E / delta\n        else: # Handle case of zero or numerically zero gap\n            # If norm_E is also zero, 0 <= 0 is true.\n            # If norm_E > 0 and delta is 0, the bound is infinite. Inequality holds.\n            r = np.inf\n\n        # The inequality to check with an absolute tolerance of 1e-10\n        is_less_or_equal = (sin_theta_max <= r + 1e-10)\n        results.append(is_less_or_equal)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}