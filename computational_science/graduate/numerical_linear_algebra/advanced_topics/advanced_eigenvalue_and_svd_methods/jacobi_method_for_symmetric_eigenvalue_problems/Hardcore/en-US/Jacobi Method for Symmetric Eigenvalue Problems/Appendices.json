{
    "hands_on_practices": [
        {
            "introduction": "The performance of the Jacobi method is determined by the efficiency of its core operation: a single plane rotation. A naive implementation involving full matrix multiplication of the form $G^\\top A G$ is prohibitively expensive. This exercise focuses on the critical insight that a rotation only affects two rows and two columns, allowing for a direct update in $O(n)$ time, a crucial skill for implementing practical numerical algorithms .",
            "id": "3552566",
            "problem": "You are given a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and two distinct indices $(p,q)$ with $0 \\leq p  q \\leq n-1$. Consider one Jacobi rotation that annihilates the off-diagonal entry $a_{pq}$ by an orthogonal similarity transformation. Your task is to implement a single step of Jacobi's method that computes the updated matrix $A' \\in \\mathbb{R}^{n \\times n}$ corresponding to $A' \\leftarrow G^\\top A G$, where $G \\in \\mathbb{R}^{n \\times n}$ is an orthogonal rotation acting only on the $(p,q)$-plane and chosen to annihilate $a_{pq}$. The implementation must avoid forming the full product $G^\\top A G$ and must update $A$ in $O(n)$ by reusing temporaries for the affected rows and columns.\n\nFundamental base to be used:\n- A real symmetric matrix $A$ has real eigenvalues and can be orthogonally diagonalized. Orthogonal similarity transformations $A' = Q^\\top A Q$ preserve symmetry and eigenvalues.\n- A Jacobi rotation is an orthogonal matrix $G$ that is the identity except for a $2 \\times 2$ rotation acting on indices $(p,q)$; by an appropriate choice of rotation parameters, it can annihilate the single off-diagonal entry $a_{pq}$.\n\nDefinitions and requirements:\n- Let $a_{ij}$ denote the $(i,j)$-entry of $A$, and let $a'_{ij}$ denote the $(i,j)$-entry of $A'$.\n- The rotation parameters must be chosen to annihilate $a_{pq}$ in a numerically stable manner.\n- Efficiency requirement: update only the rows and columns indexed by $p$ and $q$, reusing temporaries to avoid unnecessary memory operations. Do not construct $G$ explicitly and do not perform dense matrix-matrix multiplication.\n- Correctness requirement: $A'$ must remain symmetric and satisfy $a'_{pq} \\approx 0$ up to Floating Point (FP) rounding error.\n\nFor each test case, compute and report the following three quantities:\n1. The integer $m$ equal to the number of entries $(i,j)$ for which $|a'_{ij} - a_{ij}| > \\varepsilon$, with $\\varepsilon = 10^{-12}$.\n2. The float $r$ equal to $|a'_{pq}|$.\n3. The float $f$ equal to the Frobenius norm $\\|A' - A\\|_F$, where $\\|X\\|_F = \\sqrt{\\sum_{i,j} x_{ij}^2}$.\n\nAngle units: Any internal rotation angle, if formed, must be in radians; however, no angle is part of the required output.\n\nTest suite:\n- Case $1$ (boundary, trivial annihilation): $n=2$, \n$$\nA = \\begin{bmatrix}\n2.0  0.0 \\\\\n0.0  3.0\n\\end{bmatrix},\\quad (p,q) = (0,1).\n$$\n- Case $2$ (general, mixed signs): $n=5$,\n$$\nA = \\begin{bmatrix}\n4  1  2  0  0 \\\\\n1  3  -1  2  0 \\\\\n2  -1  5  0  1 \\\\\n0  2  0  4  -2 \\\\\n0  0  1  -2  3\n\\end{bmatrix},\\quad (p,q) = (1,2).\n$$\n- Case $3$ (edge, equal diagonals): $n=8$,\n$$\nA = \\begin{bmatrix}\n7  2  0  1  0  0  3  0 \\\\\n2  6  1  0  0  2  0  0 \\\\\n0  1  5  0  2  0  0  1 \\\\\n1  0  0  6  0  0  2  0 \\\\\n0  0  2  0  4  1  0  0 \\\\\n0  2  0  0  1  3  1  0 \\\\\n3  0  0  2  0  1  6  0 \\\\\n0  0  1  0  0  0  0  5\n\\end{bmatrix},\\quad (p,q) = (3,6).\n$$\n\nYour program must apply one Jacobi rotation to each test case to produce the updated matrix $A'$, compute $(m,r,f)$ for each test case, and output a single line containing the results as a comma-separated list of lists in the following format:\n$$\n\\text{[}[m_1,r_1,f_1],[m_2,r_2,f_2],[m_3,r_3,f_3]\\text{]}.\n$$\nAll numeric outputs must be in standard Floating Point format (no scientific symbols other than the usual decimal representation) and there are no physical units. The tolerance is fixed at $\\varepsilon = 10^{-12}$.",
            "solution": "The user-provided problem is valid. It is scientifically sound, well-posed, objective, and contains all necessary information for a unique solution. The task is to implement a single, efficient step of the Jacobi method for a symmetric matrix.\n\n### Principle-Based Design\n\nThe core of the problem is to apply an orthogonal similarity transformation $A' = G^\\top A G$ to a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$. The transformation matrix $G$ is a Jacobi rotation (also known as a Givens rotation) designed to annihilate a specific off-diagonal element $a_{pq}$ (and its symmetric counterpart $a_{qp}$).\n\n**1. The Jacobi Rotation Matrix ($G$)**\n\nThe matrix $G$ is an orthogonal matrix that is identical to the identity matrix except for a $2 \\times 2$ rotation sub-block acting on the indices $(p,q)$, where $0 \\leq p  q \\leq n-1$. It is defined as:\n$$\nG_{ij} =\n\\begin{cases}\n    c = \\cos(\\theta)  \\text{if } i=j=p \\text{ or } i=j=q \\\\\n    s = \\sin(\\theta)  \\text{if } (i,j) = (p,q) \\\\\n    -s = -\\sin(\\theta)  \\text{if } (i,j) = (q,p) \\\\\n    1  \\text{if } i=j \\text{ and } i \\notin \\{p,q\\} \\\\\n    0  \\text{otherwise}\n\\end{cases}\n$$\nThe value of $\\theta$ is chosen to make the element $a'_{pq}$ of the transformed matrix $A'$ equal to zero. Orthogonality ($G^\\top G = I$) is guaranteed by the property $c^2 + s^2 = 1$.\n\n**2. Derivation of Rotation Parameters ($c, s$)**\n\nThe transformation affects the $2 \\times 2$ submatrix of $A$ at indices $(p,q)$ as follows:\n$$\n\\begin{pmatrix} a'_{pp}  a'_{pq} \\\\ a'_{qp}  a'_{qq} \\end{pmatrix} = \\begin{pmatrix} c  -s \\\\ s  c \\end{pmatrix} \\begin{pmatrix} a_{pp}  a_{pq} \\\\ a_{qp}  a_{qq} \\end{pmatrix} \\begin{pmatrix} c  s \\\\ -s  c \\end{pmatrix}\n$$\nPerforming this matrix multiplication yields the expression for the new off-diagonal element $a'_{pq}$:\n$$ a'_{pq} = (c^2 - s^2) a_{pq} + c s (a_{pp} - a_{qq}) $$\nTo annihilate this element (set $a'_{pq} = 0$), we must have:\n$$ (c^2 - s^2) a_{pq} = -c s (a_{pp} - a_{qq}) $$\nUsing the double-angle identities $c^2-s^2 = \\cos(2\\theta)$ and $2cs = \\sin(2\\theta)$, we find:\n$$ \\cot(2\\theta) = \\frac{\\cos(2\\theta)}{\\sin(2\\theta)} = \\frac{a_{qq} - a_{pp}}{2 a_{pq}} $$\nLet $\\tau = \\frac{a_{qq} - a_{pp}}{2 a_{pq}}$. To compute $c$ and $s$ in a numerically stable way without explicitly finding $\\theta$, we first find $t = \\tan(\\theta)$. The relationship $\\cot(2\\theta) = \\frac{1-t^2}{2t} = \\tau$ leads to the quadratic equation $t^2 + 2\\tau t - 1 = 0$. The smaller-magnitude root, which corresponds to a smaller rotation angle $|\\theta| \\leq \\pi/4$, is chosen for stability. This root is given by:\n$$ t = \\frac{\\text{sgn}(\\tau)}{|\\tau| + \\sqrt{\\tau^2 + 1}} $$\nA special case occurs if $\\tau = 0$ (i.e., $a_{pp}=a_{qq}$), for which we take $t=1$ (a rotation of $\\theta = \\pi/4$). If $a_{pq}=0$, no rotation is needed, so we set $\\theta=0$, which implies $t=0, c=1, s=0$.\nOnce $t$ is known, $c$ and $s$ are found by:\n$$ c = \\frac{1}{\\sqrt{1+t^2}}, \\quad s = t c $$\n\n**3. Efficient $O(n)$ Matrix Update**\n\nThe transformation $A' = G^\\top A G$ only modifies rows and columns $p$ and $q$. A naive implementation involving full matrix multiplication would be $O(n^3)$. An efficient $O(n)$ update is achieved by deriving explicit formulas for the changed elements.\n- For $i,j \\notin \\{p,q\\}$, the element $a_{ij}$ is unchanged: $a'_{ij} = a_{ij}$.\n- For elements in the affected rows and columns (but outside the $2 \\times 2$ block), for $i \\notin \\{p,q\\}$:\n  $$ a'_{pi} = a'_{ip} = c \\cdot a_{pi} - s \\cdot a_{qi} $$\n  $$ a'_{qi} = a'_{iq} = s \\cdot a_{pi} + c \\cdot a_{qi} $$\n  This requires $O(n)$ operations.\n- For the $2 \\times 2$ submatrix at indices $(p,q)$, the updated values are:\n  $$ a'_{pp} = c^2 a_{pp} - 2cs a_{pq} + s^2 a_{qq} $$\n  $$ a'_{qq} = s^2 a_{pp} + 2cs a_{pq} + c^2 a_{qq} $$\n  And $a'_{pq} = a'_{qp}$ is zero by construction. These updates require $O(1)$ operations.\n\n**4. Algorithmic Implementation and Output Calculation**\n\nThe algorithm proceeds as follows:\n1. For a given matrix $A$ and indices $(p,q)$, first handle the trivial case where $a_{pq}$ is already zero.\n2. If $a_{pq} \\neq 0$, calculate $\\tau$, then find $t, c, s$ using the stable formulas.\n3. Create a copy of the original matrix, $A_{orig}$, for later comparison. The working matrix $A$ (or a copy $A'$) is updated.\n4. Iterate from $i=0$ to $n-1$. For each $i \\notin \\{p,q\\}$, update the elements $a'_{pi}, a'_{ip}, a'_{qi}, a'_{iq}$ using the formulas above.\n5. Update the four elements of the $2 \\times 2$ submatrix $a'_{pp}, a'_{qq}, a'_{pq}, a'_{qp}$. The value of $a'_{pq}$ is calculated and stored to find $r$, and then can be set to exactly $0.0$ in the matrix to reflect perfect annihilation.\n6. The required metrics are then computed:\n   - $m$: Count of entries $(i,j)$ where $|a'_{ij} - a_{ij}| > \\varepsilon$. This is found by element-wise comparison between $A'$ and $A_{orig}$.\n   - $r$: The absolute value $|a'_{pq}|$ as computed from the transformation formula, which measures the numerical error in annihilation.\n   - $f$: The Frobenius norm of the difference, $\\|A' - A_{orig}\\|_F = \\sqrt{\\sum_{i,j} (a'_{ij} - a_{ij})^2}$.\n\nThis procedure correctly and efficiently implements one step of the Jacobi rotation, adhering to all specified constraints.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes one step of the Jacobi rotation for multiple test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([[2.0, 0.0], [0.0, 3.0]], dtype=float),\n            \"p\": 0, \"q\": 1\n        },\n        {\n            \"A\": np.array([\n                [4.0, 1.0, 2.0, 0.0, 0.0],\n                [1.0, 3.0, -1.0, 2.0, 0.0],\n                [2.0, -1.0, 5.0, 0.0, 1.0],\n                [0.0, 2.0, 0.0, 4.0, -2.0],\n                [0.0, 0.0, 1.0, -2.0, 3.0]\n            ], dtype=float),\n            \"p\": 1, \"q\": 2\n        },\n        {\n            \"A\": np.array([\n                [7.0, 2.0, 0.0, 1.0, 0.0, 0.0, 3.0, 0.0],\n                [2.0, 6.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0],\n                [0.0, 1.0, 5.0, 0.0, 2.0, 0.0, 0.0, 1.0],\n                [1.0, 0.0, 0.0, 6.0, 0.0, 0.0, 2.0, 0.0],\n                [0.0, 0.0, 2.0, 0.0, 4.0, 1.0, 0.0, 0.0],\n                [0.0, 2.0, 0.0, 0.0, 1.0, 3.0, 1.0, 0.0],\n                [3.0, 0.0, 0.0, 2.0, 0.0, 1.0, 6.0, 0.0],\n                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 5.0]\n            ], dtype=float),\n            \"p\": 3, \"q\": 6\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        p = case[\"p\"]\n        q = case[\"q\"]\n        results.append(jacobi_step(A, p, q))\n\n    # Format the final output string\n    result_strings = []\n    for res in results:\n        # Format list to string '[m,r,f]' without extra spaces\n        result_strings.append(f\"[{res[0]},{res[1]},{res[2]}]\")\n    \n    print(f\"[{','.join(result_strings)}]\")\n\ndef jacobi_step(A, p, q):\n    \"\"\"\n    Performs a single Jacobi rotation on matrix A to annihilate element (p,q).\n    \n    Args:\n        A (np.ndarray): The real symmetric matrix.\n        p (int): The first index (row/column).\n        q (int): The second index (row/column).\n\n    Returns:\n        list: A list containing [m, r, f].\n    \"\"\"\n    A_orig = A.copy()\n    A_prime = A.copy()\n    n = A.shape[0]\n\n    app = A_orig[p, p]\n    aqq = A_orig[q, q]\n    apq = A_orig[p, q]\n\n    # If the element is already zero, no rotation is necessary.\n    if np.isclose(apq, 0.0):\n        return [0, 0.0, 0.0]\n\n    # Calculate rotation parameters c and s\n    tau = (aqq - app) / (2.0 * apq)\n    if tau >= 0:\n        t = 1.0 / (tau + np.sqrt(1.0 + tau**2))\n    else:\n        t = -1.0 / (-tau + np.sqrt(1.0 + tau**2))\n    \n    c = 1.0 / np.sqrt(1.0 + t**2)\n    s = t * c\n\n    # Update the matrix A_prime in an O(n) fashion\n    # First, update elements in rows/cols p,q but not the 2x2 block\n    for i in range(n):\n        if i != p and i != q:\n            a_pi = A_orig[p, i]\n            a_qi = A_orig[q, i]\n            \n            val_pi_prime = c * a_pi - s * a_qi\n            val_qi_prime = s * a_pi + c * a_qi\n            \n            A_prime[p, i] = val_pi_prime\n            A_prime[i, p] = val_pi_prime\n            A_prime[q, i] = val_qi_prime\n            A_prime[i, q] = val_qi_prime\n\n    # Second, update the 2x2 submatrix\n    A_prime[p, p] = c**2 * app - 2 * c * s * apq + s**2 * aqq\n    A_prime[q, q] = s**2 * app + 2 * c * s * apq + c**2 * aqq\n    \n    apq_prime = (c**2 - s**2) * apq + c * s * (app - aqq)\n    A_prime[p, q] = apq_prime\n    A_prime[q, p] = apq_prime\n\n    # Compute the required metrics\n    eps = 1e-12\n    m = int(np.sum(np.abs(A_prime - A_orig) > eps))\n    r = float(np.abs(A_prime[p, q]))\n    f = float(np.linalg.norm(A_prime - A_orig, 'fro'))\n    \n    return [m, r, f]\n\n# Execute the main function\nsolve()\n```"
        },
        {
            "introduction": "The forward Jacobi process deconstructs a symmetric matrix $A$ into its eigenvalues $\\Lambda$ and an orthogonal matrix of eigenvectors $Q$. This exercise reverses the process, challenging you to reconstruct the original matrix $A$ from its diagonal form $\\Lambda$ and the sequence of rotations comprising $Q$. By working backward from the fundamental relationship $A = Q \\Lambda Q^\\top$, you will gain a deeper, more intuitive understanding of how the eigenvector basis transforms a simple diagonal structure into a general symmetric matrix .",
            "id": "2405315",
            "problem": "You are given a reverse formulation of the classical Jacobi method for symmetric eigenproblems. In the forward Jacobi process, a real symmetric matrix is diagonalized by a sequence of orthogonal plane rotations (Jacobi rotations). In this reverse problem, you are given a diagonal matrix and a specified sequence of Jacobi rotations, and you must reconstruct the dense symmetric matrix that would be diagonalized by the forward application of those rotations.\n\nFundamental base:\n- A real square matrix $Q$ is orthogonal if $Q^{\\mathsf{T}}Q=I$, where $I$ is the identity matrix and ${\\mathsf{T}}$ denotes transpose. Orthogonal similarity transformations map symmetric matrices to symmetric matrices and preserve eigenvalues.\n- A Jacobi rotation in the coordinate plane spanned by standard basis vectors indexed by $p$ and $q$ is an orthogonal matrix identical to the identity except in its $2\\times 2$ sub-block on rows and columns $(p,q)$, which equals a rotation by angle $\\theta$.\n\nDefinitions and conventions for this problem:\n- Angles are specified in radians.\n- Indices $p$ and $q$ are zero-based integers with $0 \\le p  q  n$, where $n$ is the matrix dimension.\n- For a given $n$, the elementary Jacobi rotation $G(p,q,\\theta)\\in\\mathbb{R}^{n\\times n}$ is defined as the identity matrix except for the entries\n  $$\n  G_{pp}=\\cos\\theta,\\quad G_{qq}=\\cos\\theta,\\quad G_{pq}=\\sin\\theta,\\quad G_{qp}=-\\sin\\theta,\n  $$\n  and $G_{ij}=0$ for all other off-diagonal entries in rows or columns $p$ and $q$. This specifies the standard plane rotation embedded in $\\mathbb{R}^n$.\n- Given a sequence of $k$ rotations $\\{(p_1,q_1,\\theta_1),\\ldots,(p_k,q_k,\\theta_k)\\}$, form the orthogonal matrix\n  $$\n  Q \\equiv G(p_1,q_1,\\theta_1)\\,G(p_2,q_2,\\theta_2)\\,\\cdots\\,G(p_k,q_k,\\theta_k).\n  $$\n- Let $\\Lambda=\\mathrm{diag}(\\lambda_0,\\ldots,\\lambda_{n-1})$ be a real diagonal matrix. Your task is to reconstruct the symmetric matrix $A$ that would be diagonalized by the forward Jacobi process using the given rotations, solely from $\\Lambda$ and the rotation sequence. You must derive from first principles how to compute $A$ using only the definitions above and general properties of orthogonal similarity transformations.\n\nProgram requirements:\n- Implement a program that, for each test case below, constructs the specified $Q$ from the rotation sequence and returns the upper-triangular entries of the reconstructed matrix $A$ in row-major order, including the diagonal. For example, for $n=3$, return the list $[A_{00},A_{01},A_{02},A_{11},A_{12},A_{22}]$.\n- For numerical reporting, round each returned number to $10^{-6}$ and format it with exactly six digits after the decimal point.\n- The final output must be a single line containing a single list that concatenates the per-test-case upper-triangular outputs, as a comma-separated list enclosed in square brackets, with no spaces.\n\nTest suite (angles in radians, indices zero-based):\n- Test case $1$ (boundary: empty rotation sequence): $n=3$, $\\Lambda=\\mathrm{diag}(1.5,-0.5,2.5)$, rotation sequence $\\varnothing$.\n- Test case $2$ (single rotation): $n=3$, $\\Lambda=\\mathrm{diag}(1,2,4)$, rotation sequence $\\{(0,1,\\pi/6)\\}$.\n- Test case $3$ (multiple rotations with shared indices): $n=4$, $\\Lambda=\\mathrm{diag}(0.5,1.5,2.5,3.5)$, rotation sequence $\\{(0,2,\\pi/4),(1,3,\\pi/3),(0,1,-\\pi/8)\\}$.\n- Test case $4$ (angles including zeros): $n=3$, $\\Lambda=\\mathrm{diag}(3,1,2)$, rotation sequence $\\{(0,2,0),(0,2,\\pi/2),(1,2,0)\\}$.\n\nYour program should produce a single line of output containing the concatenated results as a comma-separated list enclosed in square brackets (for example, $\\big[$result$1,$result$2,\\ldots\\big]$). No additional text should be printed.",
            "solution": "The problem requires the reconstruction of a real symmetric matrix $A$ from its diagonal form $\\Lambda$ and a sequence of Jacobi rotations that produce this diagonalization. This is the inverse of the Jacobi eigenvalue algorithm. We shall first derive the reconstruction formula from fundamental principles of linear algebra and then develop an algorithm to compute it.\n\nA fundamental result from spectral theory states that any real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ can be diagonalized by an orthogonal similarity transformation. That is, there exists an orthogonal matrix $P \\in \\mathbb{R}^{n \\times n}$ and a diagonal matrix $\\Lambda \\in \\mathbb{R}^{n \\times n}$ such that:\n$$ \\Lambda = P^{\\mathsf{T}} A P $$\nHere, the diagonal entries of $\\Lambda$ are the eigenvalues of $A$, and the columns of $P$ are the corresponding orthonormal eigenvectors. This relation can be inverted to express $A$ in terms of its eigenvalues and eigenvectors. Since $P$ is orthogonal, its transpose is its inverse, i.e., $P^{\\mathsf{T}} = P^{-1}$. By pre-multiplying by $P$ and post-multiplying by $P^{\\mathsf{T}}$, we obtain:\n$$ P \\Lambda P^{\\mathsf{T}} = P (P^{\\mathsf{T}} A P) P^{\\mathsf{T}} = (P P^{\\mathsf{T}}) A (P P^{\\mathsf{T}}) = I A I = A $$\nThus, the reconstruction formula is $A = P \\Lambda P^{\\mathsf{T}}$.\n\nThe problem states that the forward Jacobi process diagonalizes $A$ using a sequence of $k$ elementary rotations, $\\{(p_1,q_1,\\theta_1), \\ldots, (p_k,q_k,\\theta_k)\\}$. An elementary Jacobi rotation is an orthogonal transformation represented by a matrix $G(p,q,\\theta)$. The forward process applies these rotations as a sequence of similarity transformations. Let $A_0 = A$. The sequence is:\n$$ A_1 = G(p_1,q_1,\\theta_1)^{\\mathsf{T}} A_0 G(p_1,q_1,\\theta_1) $$\n$$ A_2 = G(p_2,q_2,\\theta_2)^{\\mathsf{T}} A_1 G(p_2,q_2,\\theta_2) $$\n$$ \\vdots $$\n$$ \\Lambda = A_k = G(p_k,q_k,\\theta_k)^{\\mathsf{T}} A_{k-1} G(p_k,q_k,\\theta_k) $$\nSubstituting recursively, we find the relation between $A$ and $\\Lambda$:\n$$ \\Lambda = \\left(G_k^{\\mathsf{T}} \\cdots G_2^{\\mathsf{T}} G_1^{\\mathsf{T}}\\right) A \\left(G_1 G_2 \\cdots G_k\\right) $$\nwhere we use the shorthand $G_i = G(p_i,q_i,\\theta_i)$. Using the property $(AB)^{\\mathsf{T}} = B^{\\mathsf{T}}A^{\\mathsf{T}}$, this simplifies to:\n$$ \\Lambda = (G_1 G_2 \\cdots G_k)^{\\mathsf{T}} A (G_1 G_2 \\cdots G_k) $$\nThe problem defines the total transformation matrix $Q$ as the ordered product of the elementary rotations:\n$$ Q \\equiv G_1 G_2 \\cdots G_k $$\nComparing this definition with the expression for $\\Lambda$, we see that the transformation matrix $P$ in the standard spectral decomposition corresponds exactly to $Q$.\n$$ \\Lambda = Q^{\\mathsf{T}} A Q $$\nTherefore, the formula to reconstruct $A$ is:\n$$ A = Q \\Lambda Q^{\\mathsf{T}} $$\nSince each $G_i$ is an orthogonal matrix, their product $Q$ is also an orthogonal matrix, validating this derivation.\n\nThe algorithm to solve the problem is as follows:\n$1$. For each test case, given the dimension $n$, the list of eigenvalues $(\\lambda_0, \\ldots, \\lambda_{n-1})$, and the rotation sequence $\\{(p_1,q_1,\\theta_1), \\ldots, (p_k,q_k,\\theta_k)\\}$.\n$2$. Construct the diagonal matrix $\\Lambda = \\mathrm{diag}(\\lambda_0, \\ldots, \\lambda_{n-1})$.\n$3$. Construct the total rotation matrix $Q$. This is done by first initializing a matrix and then multiplying it by each elementary Jacobi rotation matrix $G_i = G(p_i,q_i,\\theta_i)$ in the specified order.\n    - If the rotation sequence is empty ($k=0$), $Q$ is the identity matrix $I$ of size $n \\times n$.\n    - If the sequence is not empty, compute $Q = G_1 G_2 \\cdots G_k$. This can be done iteratively: start with $Q=G_1$, then compute $Q \\leftarrow Q G_2$, and so on, until $Q \\leftarrow Q G_k$. An elementary rotation matrix $G(p,q,\\theta)$ is an $n \\times n$ identity matrix except for four entries:\n    $$ G_{pp}=\\cos\\theta, \\quad G_{qq}=\\cos\\theta, \\quad G_{pq}=\\sin\\theta, \\quad G_{qp}=-\\sin\\theta $$\n$4$. Compute the symmetric matrix $A$ using the derived formula $A = Q \\Lambda Q^{\\mathsf{T}}$.\n$5$. Extract the upper-triangular elements of $A$, including the main diagonal, in row-major order. These are the elements $A_{ij}$ where $0 \\le i \\le j  n$.\n$6$. Format each extracted element by rounding to $10^{-6}$ and presenting it with exactly six digits after the decimal point. Concatenate all results from all test cases into a single list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the reverse Jacobi rotation problem for a suite of test cases.\n    \"\"\"\n    # Test suite (angles in radians, indices zero-based):\n    # - Case 1: n=3, Lambda=diag(1.5,-0.5,2.5), sequence []\n    # - Case 2: n=3, Lambda=diag(1,2,4), sequence [(0,1,pi/6)]\n    # - Case 3: n=4, Lambda=diag(0.5,1.5,2.5,3.5), sequence [(0,2,pi/4),(1,3,pi/3),(0,1,-pi/8)]\n    # - Case 4: n=3, Lambda=diag(3,1,2), sequence [(0,2,0),(0,2,pi/2),(1,2,0)]\n    test_cases = [\n        {'n': 3, 'lambdas': [1.5, -0.5, 2.5], 'rotations': []},\n        {'n': 3, 'lambdas': [1, 2, 4], 'rotations': [(0, 1, np.pi / 6)]},\n        {'n': 4, 'lambdas': [0.5, 1.5, 2.5, 3.5], 'rotations': [(0, 2, np.pi / 4), (1, 3, np.pi / 3), (0, 1, -np.pi / 8)]},\n        {'n': 3, 'lambdas': [3, 1, 2], 'rotations': [(0, 2, 0), (0, 2, np.pi / 2), (1, 2, 0)]}\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        n = case['n']\n        lambdas = case['lambdas']\n        rotations = case['rotations']\n\n        # Step 1: Construct the diagonal matrix Lambda\n        Lambda_mat = np.diag(lambdas)\n\n        # Step 2: Construct the orthogonal matrix Q from the sequence of rotations\n        if not rotations:\n            Q_mat = np.identity(n)\n        else:\n            # Initialize Q with the first rotation matrix G_1\n            p, q, theta = rotations[0]\n            c, s = np.cos(theta), np.sin(theta)\n            G = np.identity(n)\n            G[p, p], G[q, q] = c, c\n            G[p, q], G[q, p] = s, -s\n            Q_mat = G\n\n            # Accumulate the product Q = G_1 * G_2 * ... * G_k\n            for i in range(1, len(rotations)):\n                p, q, theta = rotations[i]\n                c, s = np.cos(theta), np.sin(theta)\n                G = np.identity(n)\n                G[p, p], G[q, q] = c, c\n                G[p, q], G[q, p] = s, -s\n                Q_mat = Q_mat @ G\n        \n        # Step 3: Reconstruct matrix A using A = Q * Lambda * Q^T\n        A_mat = Q_mat @ Lambda_mat @ Q_mat.T\n\n        # Step 4: Extract the upper-triangular entries in row-major order\n        case_results = []\n        for i in range(n):\n            for j in range(i, n):\n                # Format to 6 decimal places, including trailing zeros\n                formatted_val = f\"{A_mat[i, j]:.6f}\"\n                case_results.append(formatted_val)\n        \n        all_results.extend(case_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "An iterative algorithm is only as reliable as its stopping criterion. This practice explores the crucial question: what does it mean for a matrix to be \"nearly diagonal,\" and what accuracy does this guarantee for the computed eigenvalues and eigenvectors? You will apply foundational results from matrix perturbation theory, including the Hoffman-Wielandt and Davis-Kahan theorems, to connect the algorithm's termination condition to rigorous error bounds, bridging the gap between implementation and theoretical validation .",
            "id": "3552523",
            "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and the classical Jacobi method, which applies a sequence of plane rotations to produce an orthogonal matrix $Q_{k}$ after $k$ sweeps such that $D_{k} = Q_{k}^{\\top} A Q_{k}$ is nearly diagonal. Define the Frobenius norm $\\|A\\|_{F} = \\sqrt{\\sum_{i,j} a_{ij}^{2}}$ and the off-diagonal Frobenius seminorm $\\operatorname{off}(A) = \\sqrt{\\sum_{i \\neq j} a_{ij}^{2}}$. Let $D = D_{k}$ at termination, with diagonal entries $d_{ii}$, and let $q_{i}$ denote the $i$-th column of $Q_{k}$. Consider the stopping criterion\n$$\n\\operatorname{off}(D) \\le \\epsilon \\,\\|A\\|_{F},\n$$\nfor a prescribed tolerance $\\epsilon > 0$. The approximate eigenpairs reported by Jacobi at termination are $(d_{ii}, q_{i})$, i.e., Rayleigh–Ritz pairs with respect to the standard basis in the $D$–coordinates.\n\nUsing only the fundamental definitions of orthogonal similarity, Rayleigh quotient, residual, Frobenius norm, and well-tested facts for Hermitian (real symmetric) matrices (including the Hoffman–Wielandt theorem and the Davis–Kahan sin–$\\Theta$ perturbation bound), choose all statements that are valid consequences of the above stopping criterion and justify how they provide relative accuracy in the eigenvalues and computed eigenvectors.\n\nA. There exists a permutation $\\pi$ of $\\{1,\\dots,n\\}$ such that\n$$\n\\sum_{i=1}^{n} \\bigl(\\lambda_{i}(A) - d_{\\pi(i)\\pi(i)}\\bigr)^{2} \\le \\operatorname{off}(D)^{2} \\le \\epsilon^{2}\\,\\|A\\|_{F}^{2},\n$$\nso the root-mean-square eigenvalue error is at most $\\epsilon\\,\\|A\\|_{F}$. Moreover, for each $i$, letting $u_{i}$ be the unit eigenvector of $A$ associated with $\\lambda_{i}(A)$ and $q_{\\pi(i)}$ its matched computed vector, one has\n$$\n\\sin \\angle(u_{i}, q_{\\pi(i)}) \\le \\frac{\\|D - \\operatorname{diag}(d_{11},\\dots,d_{nn})\\|_{2}}{\\min_{j \\neq i} |\\lambda_{i}(A) - \\lambda_{j}(A)|} \\le \\frac{\\epsilon\\,\\|A\\|_{F}}{\\operatorname{sep}_{i}},\n$$\nwhere $\\operatorname{sep}_{i} = \\min_{j \\neq i} |\\lambda_{i}(A) - \\lambda_{j}(A)|$.\n\nB. For every $i$, one has $|\\lambda_{i}(A) - d_{ii}| \\le \\epsilon\\,\\|A\\|_{F}$ and $\\|A q_{i} - d_{ii} q_{i}\\|_{2} = 0$, so the computed vectors $q_{i}$ are exact eigenvectors of $A$.\n\nC. For all indices $i$ with $\\lambda_{i}(A) \\neq 0$, the relative eigenvalue error satisfies\n$$\n\\frac{|\\lambda_{i}(A) - d_{ii}|}{|\\lambda_{i}(A)|} \\le \\epsilon.\n$$\n\nD. Writing $R = D - \\operatorname{diag}(d_{11},\\dots,d_{nn})$ (the strictly off-diagonal part of $D$), one has $\\|R\\|_{2} \\le \\|R\\|_{F} = \\operatorname{off}(D) \\le \\epsilon\\,\\|A\\|_{F}$. Consequently, for any isolated eigenvalue cluster of $A$ separated from the remainder of the spectrum by a gap $g > 0$, the principal angle $\\Theta$ between the computed invariant subspace and the true invariant subspace obeys\n$$\n\\sin \\Theta \\le \\frac{\\|R\\|_{2}}{g} \\le \\frac{\\epsilon\\,\\|A\\|_{F}}{g}.\n$$\n\nE. Because the Jacobi method has quadratic convergence, the stopping criterion implies\n$$\n\\max_{i} |\\lambda_{i}(A) - d_{ii}| \\le \\epsilon^{2}\\,\\|A\\|_{F}.\n$$\n\nSelect all correct options.",
            "solution": "This problem analyzes the error bounds for eigenvalues and eigenvectors derived from the stopping criterion of a Jacobi-like method. The analysis relies on standard theorems from matrix perturbation theory.\n\nLet $A$ be the original symmetric matrix, $Q$ be the orthogonal matrix of approximate eigenvectors (the columns of $Q_k$), and $D = Q^\\top A Q$ be the nearly diagonal matrix at termination. We decompose $D$ into its diagonal part $\\Lambda_D = \\operatorname{diag}(d_{11}, \\dots, d_{nn})$ and its off-diagonal part $R = D - \\Lambda_D$.\nThe eigenvalues of $A$ are the same as the eigenvalues of $D$, which we denote $\\{\\lambda_i(A)\\}$. The eigenvalues of $\\Lambda_D$ are $\\{d_{ii}\\}$. The stopping criterion is $\\|R\\|_F = \\operatorname{off}(D) \\le \\epsilon \\|A\\|_F$.\n\n**Analysis of Option A:**\nThis option makes two claims, one about eigenvalue accuracy and one about eigenvector accuracy.\n1.  **Eigenvalue Accuracy:** The Hoffman-Wielandt theorem for symmetric matrices states that for any two symmetric matrices $M_1$ and $M_2$, there is a permutation $\\pi$ of their eigenvalues such that $\\sum_{i=1}^n (\\lambda_i(M_1) - \\lambda_{\\pi(i)}(M_2))^2 \\le \\|M_1 - M_2\\|_F^2$. We apply this theorem by letting $M_1 = D$ and $M_2 = \\Lambda_D$. The eigenvalues of $D$ are the eigenvalues of $A$, $\\{\\lambda_i(A)\\}$. The eigenvalues of $\\Lambda_D$ are its diagonal entries, $\\{d_{ii}\\}$. Therefore, there exists a permutation $\\pi$ such that:\n    $$ \\sum_{i=1}^n (\\lambda_i(A) - d_{\\pi(i)\\pi(i)})^2 \\le \\|D - \\Lambda_D\\|_F^2 = \\|R\\|_F^2 = \\operatorname{off}(D)^2 $$\n    Using the stopping criterion, $\\operatorname{off}(D) \\le \\epsilon \\|A\\|_F$, this becomes:\n    $$ \\sum_{i=1}^n (\\lambda_i(A) - d_{\\pi(i)\\pi(i)})^2 \\le \\epsilon^2 \\|A\\|_F^2 $$\n    This confirms the first part of the statement. The root-mean-square error is indeed bounded by $\\epsilon \\|A\\|_F$.\n2.  **Eigenvector Accuracy:** The computed eigenvector is $q_{\\pi(i)}$, and the true eigenvector is $u_i$. The angle between them is bounded by the Davis-Kahan theorem. The theorem, in one of its standard forms, states that for an eigenvalue $\\lambda_i(A)$ separated from the rest of the spectrum by a gap $\\operatorname{sep}_i = \\min_{j \\neq i} |\\lambda_i(A) - \\lambda_j(A)|$, the sine of the angle between the true eigenvector and the approximate one is bounded by the norm of the perturbation divided by the gap. The perturbation here is $R$. The bound is:\n    $$ \\sin \\angle(u_i, q_{\\pi(i)}) \\le \\frac{\\|R\\|_2}{\\operatorname{sep}_i} $$\n    We know that for any matrix, $\\|R\\|_2 \\le \\|R\\|_F$. And by the stopping criterion, $\\|R\\|_F = \\operatorname{off}(D) \\le \\epsilon \\|A\\|_F$. Thus, $\\|R\\|_2 \\le \\epsilon \\|A\\|_F$. Substituting this into the bound gives:\n    $$ \\sin \\angle(u_i, q_{\\pi(i)}) \\le \\frac{\\|R\\|_2}{\\operatorname{sep}_i} \\le \\frac{\\epsilon \\|A\\|_F}{\\operatorname{sep}_i} $$\n    The statement correctly identifies $\\|R\\|_2$ as $\\|D - \\operatorname{diag}(d_{11},\\dots,d_{nn})\\|_{2}$ and uses the correct gap. Thus, the second part is also correct.\n**Conclusion for A:** Correct.\n\n**Analysis of Option B:**\nThe statement claims $\\|A q_i - d_{ii} q_i\\|_2 = 0$. This implies that $(d_{ii}, q_i)$ is an exact eigenpair of $A$. Let's compute the residual norm:\n$$ \\|A q_i - d_{ii} q_i\\|_2 = \\|Q D Q^\\top q_i - d_{ii} q_i\\|_2 $$\nSince $Q$ is orthogonal, $\\|Qx\\|_2 = \\|x\\|_2$ and $Q^\\top q_i = e_i$ (the $i$-th standard basis vector).\n$$ \\|A q_i - d_{ii} q_i\\|_2 = \\|D e_i - d_{ii} e_i\\|_2 $$\nThe vector $De_i$ is the $i$-th column of $D$. Thus, $De_i - d_{ii} e_i$ is the $i$-th column of $D$ with its diagonal entry set to zero, which is exactly the $i$-th column of the off-diagonal part $R$. The squared norm is $\\sum_{j \\neq i} d_{ji}^2$. This is zero only if the $i$-th column of $D$ has no off-diagonal elements. The stopping criterion $\\operatorname{off}(D) \\le \\epsilon \\|A\\|_F$ does not guarantee this is zero. The claim is false.\n**Conclusion for B:** Incorrect.\n\n**Analysis of Option C:**\nThis claims a uniform relative error bound $\\frac{|\\lambda_i(A) - d_{ii}|}{|\\lambda_i(A)|} \\le \\epsilon$. From Option A, we have an absolute error bound $|\\lambda_i(A) - d_{\\pi(i)\\pi(i)}| \\le \\operatorname{off}(D) \\le \\epsilon \\|A\\|_F$. Assuming the best case where $\\pi$ is identity, the relative error is bounded by $\\frac{\\epsilon \\|A\\|_F}{|\\lambda_i(A)|}$. This is not guaranteed to be $\\le \\epsilon$, as it depends on the ratio $\\|A\\|_F / |\\lambda_i(A)|$, which can be arbitrarily large.\n**Conclusion for C:** Incorrect.\n\n**Analysis of Option D:**\nThis option makes a claim about the accuracy of an invariant subspace.\n1.  **Norm Inequality:** The statement begins with $\\|R\\|_2 \\le \\|R\\|_F = \\operatorname{off}(D) \\le \\epsilon \\|A\\|_F$. This is a chain of correct statements: the spectral norm is bounded by the Frobenius norm, the Frobenius norm of the off-diagonal part is by definition $\\operatorname{off}(D)$, and the last inequality is the stopping criterion. This part is correct.\n2.  **Subspace Error:** The statement invokes the Davis-Kahan $\\sin \\Theta$ theorem for invariant subspaces. If a set of eigenvalues is separated from the rest of the spectrum by a gap $g > 0$, the theorem provides a bound on the sine of the largest principal angle, $\\Theta$, between the true invariant subspace and the computed one. The bound is $\\sin \\Theta \\le \\frac{\\|R_{21}\\|_2}{g}$, where $R_{21}$ is the off-diagonal block of the perturbation matrix that couples the two subspaces. A simpler, more general bound is $\\sin \\Theta \\le \\frac{\\|R\\|_2}{g}$. The statement uses this form. Combining with the norm inequality from the first part gives:\n    $$ \\sin \\Theta \\le \\frac{\\|R\\|_2}{g} \\le \\frac{\\epsilon \\|A\\|_F}{g} $$\n    This is a correct application of the Davis-Kahan theorem.\n**Conclusion for D:** Correct.\n\n**Analysis of Option E:**\nThis statement claims that quadratic convergence implies an error of $\\epsilon^2$. The convergence rate describes how the error decreases from one iteration (sweep) to the next (e.g., $\\operatorname{off}_{k+1} \\le C \\cdot \\operatorname{off}_k^2$). The stopping criterion, however, is $\\operatorname{off}_k \\le \\epsilon \\|A\\|_F$. The accuracy of the result is determined by this criterion. The eigenvalue error is bounded by the final off-diagonal norm, $\\max_i |\\lambda_i(A) - d_{\\pi(i)\\pi(i)}| \\le \\operatorname{off}_k \\le \\epsilon \\|A\\|_F$. The error is proportional to $\\epsilon$, not $\\epsilon^2$. Quadratic convergence means the algorithm is very efficient in reaching this tolerance, but the final accuracy is dictated by the tolerance itself.\n**Conclusion for E:** Incorrect.\n\nFinal selection is A and D.",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}