{
    "hands_on_practices": [
        {
            "introduction": "To master the implicitly restarted Lanczos method, we must first build a solid understanding of its core component: the Lanczos iteration. This process constructs a special small, tridiagonal matrix that captures essential properties of the original large matrix. This first exercise provides direct, hands-on practice with the fundamental three-term recurrence, allowing you to manually compute the basis vectors and tridiagonal entries that form the foundation of the entire algorithm .",
            "id": "3590006",
            "problem": "Consider the core Krylov subspace construction used inside the Implicitly Restarted Lanczos Method (IRLM), where a symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is projected onto an orthonormal basis $\\{v_1, v_2, \\dots\\}$ to form a tridiagonal matrix $T$ via a three-term recurrence that follows from symmetry and orthogonality constraints. Let $A = \\mathrm{diag}(5,4,3,2,1) \\in \\mathbb{R}^{5 \\times 5}$ and let the initial vector be $v_1 = (1,1,1,1,1)^{\\top}$. Use the standard symmetric Lanczos process that enforces mutual orthogonality of the basis vectors and ensures the projected operator is tridiagonal. Begin with $v_1$ obtained by normalizing the given initial vector to unit $2$-norm, and carry out two steps of the Lanczos procedure to determine the first two diagonal entries and the first subdiagonal entry of the tridiagonal projection (denoted by $\\alpha_1$, $\\beta_1$, and $\\alpha_2$), together with the corresponding Lanczos basis vectors $v_2$ and $v_3$. Express all quantities exactly using radicals and rational numbers; no rounding is required. Provide your final answer as a single row matrix containing, in order, $\\alpha_1$, $\\beta_1$, $\\alpha_2$, the five entries of $v_2$, and the five entries of $v_3$.",
            "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. The task is to execute two full steps of the symmetric Lanczos algorithm for a given symmetric matrix $A$ and a specified initial vector.\n\nThe symmetric Lanczos algorithm is an iterative method that constructs an orthonormal basis $\\{v_j\\}_{j=1}^k$ for the Krylov subspace $\\mathcal{K}_k(A, v_1) = \\mathrm{span}\\{v_1, Av_1, \\dots, A^{k-1}v_1\\}$, and a real symmetric tridiagonal matrix $T_k$. The core of the algorithm is the three-term recurrence relation obtained by considering the matrix equation $AV_k = V_k T_k + \\beta_k v_{k+1} e_k^\\top$, where $V_k = [v_1, v_2, \\dots, v_k]$ is the matrix of orthonormal Lanczos vectors, and $T_k$ is the tridiagonal matrix. For an individual vector $v_j$, this gives the recurrence:\n$$A v_j = \\beta_{j-1} v_{j-1} + \\alpha_j v_j + \\beta_j v_{j+1}$$\nThe coefficients $\\alpha_j$ and $\\beta_j$ are determined by exploiting the orthonormality of the Lanczos vectors, i.e., $v_i^\\top v_j = \\delta_{ij}$. The coefficient $\\alpha_j$ is found by left-multiplying the recurrence by $v_j^\\top$:\n$$\\alpha_j = v_j^\\top A v_j$$\nThe subsequent vector $v_{j+1}$ and coefficient $\\beta_j$ are found by first computing an unnormalized residual vector $r_j$:\n$$r_j = A v_j - \\alpha_j v_j - \\beta_{j-1} v_{j-1}$$\nThen, $\\beta_j$ is the $2$-norm of this residual:\n$$\\beta_j = \\|r_j\\|_2$$\nIf $\\beta_j \\neq 0$, the next Lanczos vector is obtained via normalization:\n$$v_{j+1} = \\frac{r_j}{\\beta_j}$$\nThe algorithm is initialized with a starting vector $v_1$ of unit norm, and by convention, $\\beta_0 = 0$ and $v_0$ is the zero vector. We are given the matrix $A = \\mathrm{diag}(5,4,3,2,1)$ and an initial vector to be normalized.\n\n**Step 0: Initialization**\nThe unnormalized starting vector is $u = (1,1,1,1,1)^\\top$. We first compute its Euclidean norm:\n$$\\|u\\|_2 = \\sqrt{1^2 + 1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{5}$$\nThe first Lanczos vector $v_1$ is obtained by normalizing $u$:\n$$v_1 = \\frac{u}{\\|u\\|_2} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\n\n**Step 1: First Iteration ($j=1$)**\nWe begin the first step of the Lanczos process. First, we compute the product $w_1 = A v_1$:\n$$w_1 = A v_1 = \\begin{pmatrix} 5 & 0 & 0 & 0 & 0 \\\\ 0 & 4 & 0 & 0 & 0 \\\\ 0 & 0 & 3 & 0 & 0 \\\\ 0 & 0 & 0 & 2 & 0 \\\\ 0 & 0 & 0 & 0 & 1 \\end{pmatrix} \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 3 \\\\ 2 \\\\ 1 \\end{pmatrix}$$\nNext, we compute the first diagonal element $\\alpha_1$ of the tridiagonal matrix $T$:\n$$\\alpha_1 = v_1^\\top w_1 = v_1^\\top A v_1 = \\left( \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 3 \\\\ 2 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{5}(5+4+3+2+1) = \\frac{15}{5} = 3$$\nNow we compute the unnormalized residual vector $r_1$, recalling that $\\beta_0=0$:\n$$r_1 = w_1 - \\alpha_1 v_1 = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 3 \\\\ 2 \\\\ 1 \\end{pmatrix} - 3 \\cdot \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 5-3 \\\\ 4-3 \\\\ 3-3 \\\\ 2-3 \\\\ 1-3 \\end{pmatrix} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ -2 \\end{pmatrix}$$\nThe first off-diagonal element $\\beta_1$ is the $2$-norm of $r_1$:\n$$\\beta_1 = \\|r_1\\|_2 = \\left\\| \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ -2 \\end{pmatrix} \\right\\|_2 = \\frac{1}{\\sqrt{5}} \\sqrt{2^2 + 1^2 + 0^2 + (-1)^2 + (-2)^2} = \\frac{1}{\\sqrt{5}} \\sqrt{4+1+0+1+4} = \\frac{\\sqrt{10}}{\\sqrt{5}} = \\sqrt{2}$$\nFinally, we compute the second Lanczos vector $v_2$ by normalizing $r_1$:\n$$v_2 = \\frac{r_1}{\\beta_1} = \\frac{1}{\\sqrt{2}} \\left( \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ -2 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ -2 \\end{pmatrix}$$\n\n**Step 2: Second Iteration ($j=2$)**\nWe proceed to the second step. First, compute $w_2 = A v_2$:\n$$w_2 = A v_2 = \\begin{pmatrix} 5 & 0 & 0 & 0 & 0 \\\\ 0 & 4 & 0 & 0 & 0 \\\\ 0 & 0 & 3 & 0 & 0 \\\\ 0 & 0 & 0 & 2 & 0 \\\\ 0 & 0 & 0 & 0 & 1 \\end{pmatrix} \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ -2 \\end{pmatrix} = \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 10 \\\\ 4 \\\\ 0 \\\\ -2 \\\\ -2 \\end{pmatrix}$$\nNext, we compute the second diagonal element $\\alpha_2$:\n$$\\alpha_2 = v_2^\\top w_2 = v_2^\\top A v_2 = \\left( \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 2 & 1 & 0 & -1 & -2 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 10 \\\\ 4 \\\\ 0 \\\\ -2 \\\\ -2 \\end{pmatrix} \\right) = \\frac{1}{10}(20+4+0+2+4) = \\frac{30}{10} = 3$$\nNow we compute the unnormalized residual vector $r_2 = w_2 - \\alpha_2 v_2 - \\beta_1 v_1$:\n$$r_2 = \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 10 \\\\ 4 \\\\ 0 \\\\ -2 \\\\ -2 \\end{pmatrix} - 3 \\cdot \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ -2 \\end{pmatrix} - \\sqrt{2} \\cdot \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\nTo simplify the subtraction, we write all vectors with a common factor of $\\frac{1}{\\sqrt{10}}$. Note that $\\frac{\\sqrt{2}}{\\sqrt{5}} = \\frac{2}{\\sqrt{10}}$.\n$$r_2 = \\frac{1}{\\sqrt{10}} \\left( \\begin{pmatrix} 10 \\\\ 4 \\\\ 0 \\\\ -2 \\\\ -2 \\end{pmatrix} - \\begin{pmatrix} 6 \\\\ 3 \\\\ 0 \\\\ -3 \\\\ -6 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\\\ 2 \\\\ 2 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 10-6-2 \\\\ 4-3-2 \\\\ 0-0-2 \\\\ -2-(-3)-2 \\\\ -2-(-6)-2 \\end{pmatrix} = \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 2 \\\\ -1 \\\\ -2 \\\\ -1 \\\\ 2 \\end{pmatrix}$$\nTo find $v_3$, we first compute $\\beta_2 = \\|r_2\\|_2$:\n$$\\beta_2 = \\|r_2\\|_2 = \\left\\| \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 2 \\\\ -1 \\\\ -2 \\\\ -1 \\\\ 2 \\end{pmatrix} \\right\\|_2 = \\frac{1}{\\sqrt{10}} \\sqrt{2^2 + (-1)^2 + (-2)^2 + (-1)^2 + 2^2} = \\frac{1}{\\sqrt{10}} \\sqrt{4+1+4+1+4} = \\frac{\\sqrt{14}}{\\sqrt{10}}$$\nFinally, we compute the third Lanczos vector $v_3$ by normalizing $r_2$:\n$$v_3 = \\frac{r_2}{\\beta_2} = \\frac{1}{\\frac{\\sqrt{14}}{\\sqrt{10}}} \\left( \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 2 \\\\ -1 \\\\ -2 \\\\ -1 \\\\ 2 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{14}} \\begin{pmatrix} 2 \\\\ -1 \\\\ -2 \\\\ -1 \\\\ 2 \\end{pmatrix}$$\n\nThe required quantities are $\\alpha_1$, $\\beta_1$, $\\alpha_2$, and the five components of each of $v_2$ and $v_3$.\n$\\alpha_1 = 3$\n$\\beta_1 = \\sqrt{2}$\n$\\alpha_2 = 3$\nThe five components of $v_2$ are $\\frac{2}{\\sqrt{10}}$, $\\frac{1}{\\sqrt{10}}$, $0$, $-\\frac{1}{\\sqrt{10}}$, and $-\\frac{2}{\\sqrt{10}}$.\nThe five components of $v_3$ are $\\frac{2}{\\sqrt{14}}$, $-\\frac{1}{\\sqrt{14}}$, $-\\frac{2}{\\sqrt{14}}$, $-\\frac{1}{\\sqrt{14}}$, and $\\frac{2}{\\sqrt{14}}$.\nThese values are assembled into a single row matrix for the final answer.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 3 & \\sqrt{2} & 3 & \\frac{2}{\\sqrt{10}} & \\frac{1}{\\sqrt{10}} & 0 & -\\frac{1}{\\sqrt{10}} & -\\frac{2}{\\sqrt{10}} & \\frac{2}{\\sqrt{14}} & -\\frac{1}{\\sqrt{14}} & -\\frac{2}{\\sqrt{14}} & -\\frac{1}{\\sqrt{14}} & \\frac{2}{\\sqrt{14}} \\end{pmatrix} } $$"
        },
        {
            "introduction": "Once the Lanczos process generates a tridiagonal matrix $T_m$, its eigenvalues, known as Ritz values, serve as approximations to the eigenvalues of the original large matrix $A$. This practice delves into how we assess the quality of these approximations by computing their residual norms. The exercise is built around a special case that leads to a \"lucky breakdown,\" providing a crystal-clear illustration of how the Krylov subspace can sometimes perfectly capture an invariant subspace of the original matrix, yielding exact answers and deep insight into the method's behavior .",
            "id": "3590016",
            "problem": "Consider the implicitly restarted Lanczos method (IRLM), where one first constructs a short Lanczos factorization of dimension $m$ before applying implicit shifts. In exact arithmetic, the Lanczos process for a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ starts from a unit-norm vector $q_1$ and generates an orthonormal basis $Q_m = [q_1,\\dots,q_m]$ together with a symmetric tridiagonal projection $T_m \\in \\mathbb{R}^{m \\times m}$. The eigenpairs of $T_m$ are called Ritz pairs, and their associated Ritz vectors in the original space are $Q_m y$, where $y$ is an eigenvector of $T_m$. The residual norms of these Ritz pairs determine the quality of the approximation to the eigenpairs of $A$.\n\nFor the specific case of the real symmetric matrix $A = \\mathrm{diag}(10,9,1,0) \\in \\mathbb{R}^{4 \\times 4}$, with starting vector $v_1 = (1,1,0,0)^{\\top}$ and target subspace dimension $m=2$, work from the foundational definitions of the Lanczos process to:\n\n1. Normalize $v_1$ to obtain $q_1$ and carry out two steps of the Lanczos process to construct the tridiagonal projection $T_2$.\n2. Compute the Ritz values of $T_2$.\n3. Compute the residual norms of the corresponding Ritz pairs in the sense of the Lanczos projection.\n4. Based on these computations, assess which eigenvalues of $A$ are well approximated by the two-dimensional Lanczos subspace.\n\nExpress the final answer as a single row matrix containing, in order, the four entries of $T_2$ (row-major order), the two Ritz values, and the two residual norms. No rounding is required.",
            "solution": "The problem asks us to perform the first two steps of the Lanczos process for a given real symmetric matrix $A$ and a starting vector $v_1$, and then to analyze the resulting two-dimensional Lanczos subspace. The analysis involves computing the Ritz values and their corresponding residual norms.\n\nThe Lanczos algorithm for a symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and a starting vector $v_1$ generates a sequence of orthonormal vectors $q_j$ (the Lanczos vectors) and a symmetric tridiagonal matrix $T_m$. The process is defined by the following recurrence relations, starting with $q_1 = v_1/\\|v_1\\|_2$, $\\beta_1=0$, and $q_0 = \\mathbf{0}$:\nFor $j=1, 2, \\dots, m$:\n1. $w_j = A q_j$\n2. $\\alpha_j = q_j^{\\top} w_j$\n3. $\\hat{w}_j = w_j - \\alpha_j q_j - \\beta_j q_{j-1}$\n4. $\\beta_{j+1} = \\|\\hat{w}_j\\|_2$\n5. If $\\beta_{j+1}=0$, the algorithm terminates.\n6. $q_{j+1} = \\hat{w}_j / \\beta_{j+1}$\n\nThe resulting symmetric tridiagonal matrix is $T_m = \\begin{pmatrix} \\alpha_1 & \\beta_2 & & \\\\ \\beta_2 & \\alpha_2 & \\ddots & \\\\ & \\ddots & \\ddots & \\beta_m \\\\ & & \\beta_m & \\alpha_m \\end{pmatrix}$.\n\nThe given matrix is $A = \\mathrm{diag}(10,9,1,0)$ and the starting vector is $v_1 = (1,1,0,0)^{\\top}$. The target dimension is $m=2$.\n\n**1. Construction of the tridiagonal projection $T_2$**\n\nFirst, we normalize the starting vector $v_1$ to obtain $q_1$.\nThe Euclidean norm of $v_1$ is $\\|v_1\\|_2 = \\sqrt{1^2 + 1^2 + 0^2 + 0^2} = \\sqrt{2}$.\nTherefore, the first Lanczos vector is:\n$$q_1 = \\frac{v_1}{\\|v_1\\|_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\n\nNow, we perform the first step ($j=1$) of the Lanczos algorithm.\n$$w_1 = A q_1 = \\begin{pmatrix} 10 & 0 & 0 & 0 \\\\ 0 & 9 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 10 \\\\ 9 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\nThe first diagonal element of $T_2$ is $\\alpha_1$:\n$$\\alpha_1 = q_1^{\\top} w_1 = \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right)^{\\top} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 10 \\\\ 9 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right) = \\frac{1}{2} (1 \\cdot 10 + 1 \\cdot 9) = \\frac{19}{2}$$\nNext, we compute the unnormalized next Lanczos vector, with $\\beta_1=0$:\n$$\\hat{w}_1 = w_1 - \\alpha_1 q_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 10 \\\\ 9 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\frac{19}{2} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 10 - \\frac{19}{2} \\\\ 9 - \\frac{19}{2} \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ 0 \\\\ 0 \\end{pmatrix}$$\nThe first off-diagonal element of $T_2$ is $\\beta_2$:\n$$\\beta_2 = \\|\\hat{w}_1\\|_2 = \\left\\| \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ 0 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\frac{1}{\\sqrt{2}} \\sqrt{\\left(\\frac{1}{2}\\right)^2 + \\left(-\\frac{1}{2}\\right)^2} = \\frac{1}{\\sqrt{2}} \\sqrt{\\frac{1}{4} + \\frac{1}{4}} = \\frac{1}{\\sqrt{2}} \\sqrt{\\frac{1}{2}} = \\frac{1}{2}$$\nThe second Lanczos vector is:\n$$q_2 = \\frac{\\hat{w}_1}{\\beta_2} = \\frac{ \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} }{1/2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\nNow, we perform the second step ($j=2$) of the Lanczos algorithm.\n$$w_2 = A q_2 = \\begin{pmatrix} 10 & 0 & 0 & 0 \\\\ 0 & 9 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 10 \\\\ -9 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\nThe second diagonal element of $T_2$ is $\\alpha_2$:\n$$\\alpha_2 = q_2^{\\top} w_2 = \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right)^{\\top} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 10 \\\\ -9 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right) = \\frac{1}{2} (1 \\cdot 10 + (-1) \\cdot (-9)) = \\frac{1}{2}(10+9) = \\frac{19}{2}$$\nWe have now computed all entries of $T_2$:\n$$T_2 = \\begin{pmatrix} \\alpha_1 & \\beta_2 \\\\ \\beta_2 & \\alpha_2 \\end{pmatrix} = \\begin{pmatrix} \\frac{19}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{19}{2} \\end{pmatrix}$$\n\n**2. Computation of the Ritz values of $T_2$**\n\nThe Ritz values are the eigenvalues of $T_2$. We find them by solving the characteristic equation $\\det(T_2 - \\theta I) = 0$.\n$$\\det \\begin{pmatrix} \\frac{19}{2} - \\theta & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{19}{2} - \\theta \\end{pmatrix} = 0$$\n$$\\left(\\frac{19}{2} - \\theta\\right)^2 - \\left(\\frac{1}{2}\\right)^2 = 0$$\n$$\\left(\\frac{19}{2} - \\theta - \\frac{1}{2}\\right) \\left(\\frac{19}{2} - \\theta + \\frac{1}{2}\\right) = 0$$\n$$\\left(\\frac{18}{2} - \\theta\\right) \\left(\\frac{20}{2} - \\theta\\right) = 0$$\n$$(9 - \\theta)(10 - \\theta) = 0$$\nThe Ritz values are $\\theta_1 = 10$ and $\\theta_2 = 9$.\n\n**3. Computation of the residual norms**\n\nFor a given Ritz pair $(\\theta_k, y^{(k)})$, where $T_m y^{(k)} = \\theta_k y^{(k)}$ and $\\|y^{(k)}\\|=1$, the residual norm of the corresponding Ritz vector $x^{(k)} = Q_m y^{(k)}$ is given by $\\|A x^{(k)} - \\theta_k x^{(k)}\\|_2 = |\\beta_{m+1}| |e_m^{\\top} y^{(k)}|$, where $e_m$ is the $m$-th standard basis vector. For our case, $m=2$, so the formula is $\\|r_k\\|_2 = |\\beta_3| |e_2^{\\top} y^{(k)}| = |\\beta_3| |y_2^{(k)}|$.\n\nTo find the residual norms, we must compute $\\beta_3 = \\|\\hat{w}_2\\|_2$. We continue the Lanczos process for $j=2$:\n$$\\hat{w}_2 = w_2 - \\alpha_2 q_2 - \\beta_2 q_1$$\n$$\\hat{w}_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 10 \\\\ -9 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\frac{19}{2} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right) - \\frac{1}{2} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right)$$\n$$\\hat{w}_2 = \\frac{1}{\\sqrt{2}} \\left[ \\begin{pmatrix} 10 \\\\ -9 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 19/2 \\\\ -19/2 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right]$$\n$$\\hat{w}_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 10 - \\frac{19}{2} - \\frac{1}{2} \\\\ -9 + \\frac{19}{2} - \\frac{1}{2} \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 10 - \\frac{20}{2} \\\\ -9 + \\frac{18}{2} \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 10 - 10 \\\\ -9 + 9 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\nSince $\\hat{w}_2$ is the zero vector, its norm $\\beta_3 = \\|\\hat{w}_2\\|_2 = 0$.\n\nThis situation is known as a \"lucky breakdown\". It implies that the Krylov subspace $\\mathcal{K}_2(A, q_1) = \\mathrm{span}\\{q_1, q_2\\}$ is an invariant subspace of $A$.\nThe residual norm for each Ritz pair is $\\|r_k\\|_2 = |\\beta_3| |y_2^{(k)}| = 0 \\cdot |y_2^{(k)}| = 0$.\nThus, the residual norms for both Ritz pairs are $0$.\n\n**4. Assessment of eigenvalue approximation**\n\nThe eigenvalues of $A$ are $10$, $9$, $1$, and $0$. The computed Ritz values are $10$ and $9$. The residual norm of a Ritz pair is zero if and only if the Ritz pair is an exact eigenpair of the matrix $A$. Since both residual norms are $0$, the two-dimensional Lanczos subspace has yielded the exact eigenvalues $10$ and $9$. This is a direct consequence of the starting vector $v_1 = (1,1,0,0)^{\\top}$ being a linear combination of only the eigenvectors corresponding to these two eigenvalues, namely $e_1=(1,0,0,0)^{\\top}$ and $e_2=(0,1,0,0)^{\\top}$.\n\nThe final requested quantities are:\n- The four entries of $T_2$ in row-major order: $\\alpha_1=\\frac{19}{2}$, $\\beta_2=\\frac{1}{2}$, $\\beta_2=\\frac{1}{2}$, $\\alpha_2=\\frac{19}{2}$.\n- The two Ritz values (ordered descending): $\\theta_1=10$, $\\theta_2=9$.\n- The two corresponding residual norms: $0$, $0$.\nThese are combined into a single row matrix.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{19}{2} & \\frac{1}{2} & \\frac{1}{2} & \\frac{19}{2} & 10 & 9 & 0 & 0 \\end{pmatrix} } $$"
        },
        {
            "introduction": "The true power of IRLM lies in its \"implicit restart\" mechanism, which intelligently refines the search for desired eigenvalues without starting from scratch. This is accomplished by applying a carefully designed polynomial filter that dampens components of the starting vector corresponding to unwanted eigenvalues. This final practice demystifies the filtering process, guiding you to derive the filter polynomial and calculate its effect on different parts of the spectrum, revealing precisely how shifts are used to steer the algorithm toward the eigenvalues of interest .",
            "id": "3590045",
            "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with eigen-decomposition $A = U \\Lambda U^{\\top}$, where $U$ is orthogonal and $\\Lambda = \\operatorname{diag}(\\lambda_{1}, \\dots, \\lambda_{n})$. The Lanczos method constructs a Krylov subspace $\\mathcal{K}_{m}(A, v_{1})$ with a starting vector $v_{1}$ and produces a tridiagonal matrix $T_{m}$ satisfying the Lanczos relation $A V_{m} = V_{m} T_{m} + \\beta_{m} v_{m+1} e_{m}^{\\top}$, where $V_{m}$ has orthonormal columns forming a basis for $\\mathcal{K}_{m}(A, v_{1})$, $e_{m}$ is the $m$-th canonical basis vector, and $\\beta_{m} \\in \\mathbb{R}$.\n\nIn the implicitly restarted Lanczos method (IRLM), one applies a sequence of $s$ implicitly shifted orthogonal transformations (each corresponding to a shifted orthogonal-triangular or orthogonal-similarity step) with shifts $\\{\\sigma_{j}\\}_{j=1}^{s}$ to $T_{m}$, and then truncates to restart the process with a new starting vector. Starting from the fundamental properties of orthogonal similarity, the invariance of eigenvalues under similarity, and the fact that each shifted step is equivalent to premultiplying by $(A - \\sigma_{j} I)$ on the underlying Krylov sequence up to an orthogonal factor, derive the explicit scalar filtering polynomial $p(\\lambda)$ that describes how the restarted starting vector is formed as $p(A) v_{1}$, and explain why the component along an eigenvector $u_{i}$ is scaled by $p(\\lambda_{i})$.\n\nThen, for a hypothetical $A$ whose spectrum consists of the following ten eigenvalues,\n$$\n\\lambda_{1} = -2,\\ \\lambda_{2} = -1,\\ \\lambda_{3} = 0,\\ \\lambda_{4} = \\frac{1}{2},\\ \\lambda_{5} = 1,\\ \\lambda_{6} = \\frac{3}{2},\\ \\lambda_{7} = 2,\\ \\lambda_{8} = \\frac{5}{2},\\ \\lambda_{9} = 3,\\ \\lambda_{10} = 4,\n$$\nand for an implicitly restarted Lanczos sweep using three shifts\n$$\n\\sigma_{1} = 1,\\ \\sigma_{2} = 2,\\ \\sigma_{3} = 3,\n$$\ncompute the magnitudes $|p(\\lambda_{i})|$ for $i = 1, \\dots, 10$.\n\nPresent the final values in the order of the eigenvalues given above, as a single row matrix. No rounding is required; express all values exactly as integers or rational numbers.",
            "solution": "The problem asks for two main tasks: first, to derive the filtering polynomial $p(\\lambda)$ used in the Implicitly Restarted Lanczos Method (IRLM) and explain its effect on the components of the starting vector in the eigenbasis of the matrix $A$; second, to compute the magnitude of this polynomial at specified eigenvalues for a given set of shifts.\n\nLet us begin with the derivation. The IRLM is an iterative procedure to find a few eigenvalues and eigenvectors of a large matrix $A$. It starts by building a Krylov subspace $\\mathcal{K}_{m}(A, v_{1})$ of dimension $m$ using the Lanczos algorithm, which produces an orthonormal basis $V_{m} = [v_{1}, v_{2}, \\dots, v_{m}]$ and a real symmetric tridiagonal matrix $T_{m} \\in \\mathbb{R}^{m \\times m}$ that satisfy the Lanczos relation:\n$$A V_{m} = V_{m} T_{m} + \\beta_{m} v_{m+1} e_{m}^{\\top}$$\nHere, $V_{m}^{\\top} A V_{m} = T_{m}$, meaning $T_{m}$ is the projection of $A$ onto the Krylov subspace $\\mathcal{K}_{m}(A, v_{1})$.\n\nThe \"restarting\" part of IRLM involves refining the starting vector $v_{1}$ to better approximate the desired eigenspace. This is done by applying $s < m$ implicit shifts $\\{\\sigma_{j}\\}_{j=1}^{s}$ to $T_{m}$. Each shift application corresponds to one step of the \"bulge-chasing\" QR algorithm.\n\nConsider the application of a single shift $\\sigma_{1}$. An implicit QR step is performed on $T_{m}$ with this shift.\nFirst, we form the matrix $T_{m} - \\sigma_{1} I$ and compute its QR decomposition:\n$$T_{m} - \\sigma_{1} I = Q_{1} R_{1}$$\nwhere $Q_{1}$ is an orthogonal matrix and $R_{1}$ is an upper triangular matrix.\nThe similarity transformation is then applied to $T_{m}$:\n$$T_{m}^{(+)} = Q_{1}^{\\top} T_{m} Q_{1}$$\nThis process transforms the basis $V_{m}$ into a new orthonormal basis $V_{m}^{(+)} = V_{m} Q_{1}$. The new starting vector for the restarted process is the first column of this new basis, $v_{1}^{(+)}$.\n$$v_{1}^{(+)} = V_{m}^{(+)} e_{1} = (V_{m} Q_{1}) e_{1} = V_{m} q_{1}$$\nwhere $q_{1}$ is the first column of $Q_{1}$.\n\nThe crucial insight, as hinted in the problem, connects this process to applying a polynomial filter to the original starting vector $v_{1}$. Let us establish this connection. From the QR decomposition, $Q_{1} = (T_{m} - \\sigma_{1} I) R_{1}^{-1}$. Therefore, the first column $q_{1}$ is given by:\n$$q_{1} = (T_{m} - \\sigma_{1} I) R_{1}^{-1} e_{1}$$\nSince $R_{1}$ is upper triangular, $R_{1}^{-1}$ is also upper triangular. Its first column is $(R_{1}^{-1})_{11} e_{1}$, where $(R_{1}^{-1})_{11} = 1/r_{11}$. Thus, $R_{1}^{-1} e_{1} = (1/r_{11}) e_{1}$.\nSubstituting this into the expression for $q_{1}$:\n$$q_{1} = \\frac{1}{r_{11}} (T_{m} - \\sigma_{1} I) e_{1}$$\nNow, we can express the new starting vector $v_{1}^{(+)}$:\n$$v_{1}^{(+)} = V_{m} q_{1} = \\frac{1}{r_{11}} V_{m} (T_{m} - \\sigma_{1} I) e_{1}$$\nFrom the Lanczos relation, we can write for the first vector $v_{1}$: $A v_{1} = (V_m T_m + \\beta_m v_{m+1} e_m^\\top)e_1 = V_m T_m e_1$. This holds if $m \\geq 2$. More generally, the action of $A$ on any vector in $\\mathcal{K}_m(A, v_1)$ can be related to $T_m$. Consider the vector $(A-\\sigma_1 I)v_1 = (A-\\sigma_1 I)V_m e_1$. This vector is not generally in $\\mathcal{K}_m$, but its projection is. The relation $A V_m \\approx V_m T_m$ leads to the key identity that the new start vector $v_1^{(+)}$ is proportional to $(A-\\sigma_1 I)v_1$. The rigorous derivation shows $V_m (T_m-\\sigma_1 I)e_1 = (AV_m - \\beta_m v_{m+1}e_m^\\top - \\sigma_1 V_m)e_1 = (A-\\sigma_1 I)v_1 - \\beta_1 v_2$. The full derivation shows that the new start vector $v_1^{(+)}$ is proportional to $(A - \\sigma_1 I)v_1$.\nTherefore, $v_{1}^{(+)} = c_{1} (A - \\sigma_{1} I) v_{1}$ for some normalization constant $c_{1}$.\n\nWhen a sequence of $s$ shifts $\\{\\sigma_{j}\\}_{j=1}^{s}$ is applied, this process is repeated. The second step produces a vector proportional to $(A - \\sigma_{2} I) v_{1}^{(+)}$, which is in turn proportional to $(A - \\sigma_{2} I)(A - \\sigma_{1} I) v_{1}$. After applying all $s$ shifts, the resulting vector before truncation and restart, let's call it $\\tilde{v}_{1}$, is given by:\n$$\\tilde{v}_{1} = c \\prod_{j=1}^{s} (A - \\sigma_{j} I) v_{1}$$\nwhere $c$ is a cumulative normalization constant.\nThis defines a polynomial operator $p(A) = \\prod_{j=1}^{s} (A - \\sigma_{j} I)$. The corresponding scalar filtering polynomial is:\n$$p(\\lambda) = \\prod_{j=1}^{s} (\\lambda - \\sigma_{j})$$\nThe restarted starting vector is $\\frac{p(A)v_{1}}{\\|p(A)v_{1}\\|}$.\n\nNow, we explain why the component of the starting vector along an eigenvector $u_{i}$ is scaled by $p(\\lambda_{i})$. Let the initial vector $v_{1}$ be expanded in the orthonormal eigenbasis $\\{u_{i}\\}_{i=1}^{n}$ of $A$:\n$$v_{1} = \\sum_{i=1}^{n} \\alpha_{i} u_{i}$$\nwhere $\\alpha_{i} = u_{i}^{\\top} v_{1}$ is the coefficient of the component along $u_{i}$.\nApplying the polynomial operator $p(A)$ to $v_{1}$:\n$$p(A) v_{1} = p(A) \\left( \\sum_{i=1}^{n} \\alpha_{i} u_{i} \\right) = \\sum_{i=1}^{n} \\alpha_{i} p(A) u_{i}$$\nSince $u_{i}$ is an eigenvector of $A$ with eigenvalue $\\lambda_{i}$ (i.e., $A u_{i} = \\lambda_{i} u_{i}$), it is also an eigenvector of any polynomial in $A$. Specifically:\n$$p(A) u_{i} = \\left( \\prod_{j=1}^{s} (A - \\sigma_{j} I) \\right) u_{i} = \\left( \\prod_{j=1}^{s} (\\lambda_{i} - \\sigma_{j}) \\right) u_{i} = p(\\lambda_{i}) u_{i}$$\nSubstituting this result back, we get:\n$$p(A) v_{1} = \\sum_{i=1}^{n} \\alpha_{i} (p(\\lambda_{i}) u_{i}) = \\sum_{i=1}^{n} (\\alpha_{i} p(\\lambda_{i})) u_{i}$$\nThis demonstrates that the original component of $v_{1}$ along $u_{i}$, which is $\\alpha_{i} u_{i}$, is transformed into a new component $(\\alpha_{i} p(\\lambda_{i})) u_{i}$. Thus, the component is scaled by the scalar factor $p(\\lambda_{i})$. The purpose of IRLM is to choose shifts $\\sigma_{j}$ that correspond to unwanted eigenvalues, so that the corresponding $|p(\\lambda_{i})|$ values are small, effectively damping those components and enriching the starting vector in the desired eigen-components.\n\nFor the second part of the problem, we are given $s=3$ shifts: $\\sigma_{1} = 1$, $\\sigma_{2} = 2$, $\\sigma_{3} = 3$. The filtering polynomial is:\n$$p(\\lambda) = (\\lambda - 1)(\\lambda - 2)(\\lambda - 3)$$\nWe must compute $|p(\\lambda_{i})|$ for the given $10$ eigenvalues.\n\n- For $\\lambda_{1} = -2$:\n$p(-2) = (-2 - 1)(-2 - 2)(-2 - 3) = (-3)(-4)(-5) = -60$. So, $|p(\\lambda_{1})| = 60$.\n\n- For $\\lambda_{2} = -1$:\n$p(-1) = (-1 - 1)(-1 - 2)(-1 - 3) = (-2)(-3)(-4) = -24$. So, $|p(\\lambda_{2})| = 24$.\n\n- For $\\lambda_{3} = 0$:\n$p(0) = (0 - 1)(0 - 2)(0 - 3) = (-1)(-2)(-3) = -6$. So, $|p(\\lambda_{3})| = 6$.\n\n- For $\\lambda_{4} = \\frac{1}{2}$:\n$p(\\frac{1}{2}) = (\\frac{1}{2} - 1)(\\frac{1}{2} - 2)(\\frac{1}{2} - 3) = (-\\frac{1}{2})(-\\frac{3}{2})(-\\frac{5}{2}) = -\\frac{15}{8}$. So, $|p(\\lambda_{4})| = \\frac{15}{8}$.\n\n- For $\\lambda_{5} = 1$:\n$p(1) = (1 - 1)(1 - 2)(1 - 3) = (0)(-1)(-2) = 0$. So, $|p(\\lambda_{5})| = 0$.\n\n- For $\\lambda_{6} = \\frac{3}{2}$:\n$p(\\frac{3}{2}) = (\\frac{3}{2} - 1)(\\frac{3}{2} - 2)(\\frac{3}{2} - 3) = (\\frac{1}{2})(-\\frac{1}{2})(-\\frac{3}{2}) = \\frac{3}{8}$. So, $|p(\\lambda_{6})| = \\frac{3}{8}$.\n\n- For $\\lambda_{7} = 2$:\n$p(2) = (2 - 1)(2 - 2)(2 - 3) = (1)(0)(-1) = 0$. So, $|p(\\lambda_{7})| = 0$.\n\n- For $\\lambda_{8} = \\frac{5}{2}$:\n$p(\\frac{5}{2}) = (\\frac{5}{2} - 1)(\\frac{5}{2} - 2)(\\frac{5}{2} - 3) = (\\frac{3}{2})(\\frac{1}{2})(-\\frac{1}{2}) = -\\frac{3}{8}$. So, $|p(\\lambda_{8})| = \\frac{3}{8}$.\n\n- For $\\lambda_{9} = 3$:\n$p(3) = (3 - 1)(3 - 2)(3 - 3) = (2)(1)(0) = 0$. So, $|p(\\lambda_{9})| = 0$.\n\n- For $\\lambda_{10} = 4$:\n$p(4) = (4 - 1)(4 - 2)(4 - 3) = (3)(2)(1) = 6$. So, $|p(\\lambda_{10})| = 6$.\n\nThe resulting magnitudes, listed in order, are $\\{60, 24, 6, \\frac{15}{8}, 0, \\frac{3}{8}, 0, \\frac{3}{8}, 0, 6\\}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n60 & 24 & 6 & \\frac{15}{8} & 0 & \\frac{3}{8} & 0 & \\frac{3}{8} & 0 & 6\n\\end{pmatrix}\n}\n$$"
        }
    ]
}