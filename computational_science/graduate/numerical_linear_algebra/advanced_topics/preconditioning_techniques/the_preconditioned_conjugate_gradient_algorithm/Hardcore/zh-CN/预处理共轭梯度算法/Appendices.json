{
    "hands_on_practices": [
        {
            "introduction": "预条件共轭梯度法（PCG）的强大之处在于其能通过改变系统的谱结构来加速收敛。这个练习将通过构造一个理想化的场景，让你亲手设计一个线性系统和一个预条件子，使得预条件化后的算子只有两个不同的特征值。通过这个过程，你将直接验证PCG方法的收敛步数上界由预条件算子的不同特征值数量决定的核心理论 。",
            "id": "2427437",
            "problem": "考虑使用共轭梯度（CG）方法及其预处理变体来求解一个线性系统。构造两个对称正定的显式矩阵 $A \\in \\mathbb{R}^{5 \\times 5}$ 和 $M \\in \\mathbb{R}^{5 \\times 5}$，使得预处理算子 $M^{-1}A$ 恰好有 $2$ 个不同的特征值。通过基于您构造的直接推理来验证该特征值属性。然后，考虑对系统 $A x = b$ 应用预处理共轭梯度（PCG）方法（使用预处理器 $M$），其中 $b \\in \\mathbb{R}^{5}$ 为任意向量，$x_0 \\in \\mathbb{R}^{5}$ 为任意初始猜测。基于线性代数的基本原理和该算法的定义属性，提供一个严格的论证，证明PCG方法将在一个不依赖于 $b$ 或 $x_0$ 的有界迭代次数内终止并得到精确解。确定具有此性质的最小整数 $k^\\star$。\n\n您的最终答案必须仅为 $k^\\star$ 的值。无需四舍五入。",
            "solution": "问题要求我们构造两个大小为 $5 \\times 5$ 的特定对称正定（SPD）矩阵 $A$ 和 $M$，使得预处理算子 $M^{-1}A$ 恰好拥有 $2$ 个不同的特征值。随后，我们必须提供一个严格的论证，来证明对于系统 $A x = b$ 的预处理共轭梯度（PCG）方法会在有限次迭代 $k^\\star$ 内收敛到精确解，且该迭代次数独立于右端项 $b$ 和初始猜测 $x_0$ 的选择。最后，我们必须确定这个最小上界 $k^\\star$。\n\n首先，我们构造所需的矩阵 $A$ 和 $M$。一种简单有效的构造方法是使用对角矩阵。令 $M$ 为 $5 \\times 5$ 的单位矩阵，$M=I_5$。单位矩阵是对称的，并且其所有特征值均为 $1$，因此它是正定的。\n接下来，我们构造矩阵 $A$。为确保 $M^{-1}A$ 有两个不同的特征值，并且在我们选择 $M=I_5$ 的情况下，矩阵 $A$ 本身必须有两个不同的特征值。我们还需要 $A$ 是对称且正定的。一个对角线上元素为正的对角矩阵满足这些要求。我们可以选择：\n$$\nA = \\text{diag}(1, 1, 1, 2, 2)\n$$\n根据构造，这个矩阵 $A$ 是对称的。它的特征值是其对角线元素，即 $1$ 和 $2$。由于所有特征值都是正的，所以 $A$ 是正定的。\n预处理算子为 $M^{-1}A = I_5^{-1}A = A$。因此，$M^{-1}A$ 的特征值为 $\\{1, 1, 1, 2, 2\\}$。不同特征值的集合是 $\\{\\lambda_1, \\lambda_2\\} = \\{1, 2\\}$。因此，恰好有 $2$ 个不同的特征值，符合问题陈述的要求。我们对 $A$ 和 $M$ 的构造是有效的。\n\n现在，我们必须分析 PCG 方法的收敛性。使用 SPD 预处理器 $M$ 求解系统 $A x = b$ 的 PCG 算法在数学上等价于将标准的共轭梯度（CG）算法应用于一个变换后的线性系统。由于 $M$ 是对称正定的，它有唯一的 Cholesky 分解 $M = L L^T$，其中 $L$ 是一个非奇异下三角矩阵。\n\n我们可以将原始系统 $A x = b$ 变换如下：\n$$\nA x = b \\implies (L^{-1} A L^{-T}) (L^T x) = L^{-1} b\n$$\n我们定义 $\\hat{A} = L^{-1} A L^{-T}$，$\\hat{x} = L^T x$ 和 $\\hat{b} = L^{-1}b$。该系统变为 $\\hat{A} \\hat{x} = \\hat{b}$。\n矩阵 $\\hat{A}$ 是对称正定的。它是对称的，因为 $A$ 是对称的：\n$$\n\\hat{A}^T = (L^{-1} A L^{-T})^T = (L^{-T})^T A^T (L^{-1})^T = L^{-1} A L^{-T} = \\hat{A}\n$$\n它是正定的，因为 $A$ 是对称正定的且 $L^{-T}$ 是非奇异的。对于任意非零向量 $y \\in \\mathbb{R}^5$，令 $z = L^{-T}y$。由于 $L^{-T}$ 是非奇异的，$z \\neq 0$。那么：\n$$\ny^T \\hat{A} y = y^T (L^{-1} A L^{-T}) y = (L^{-T}y)^T A (L^{-T}y) = z^T A z > 0\n$$\n应用于 $A x = b$ 的 PCG 算法被设计成使得它生成的迭代序列 $x_k$ 对应于通过将标准 CG 算法应用于 $\\hat{A} \\hat{x} = \\hat{b}$ 而生成的迭代序列 $\\hat{x}_k = L^T x_k$。\n\nCG 方法的一个基本定理指出，该算法在至多 $m$ 次迭代内会终止并得到精确解，其中 $m$ 是系统矩阵的不同特征值的数量。这个性质的产生是因为误差 $e_k = x - x_k$ 可以表示为 $e_k = P_k(A) e_0$，其中 $P_k$ 是一个次数为 $k$ 且满足 $P_k(0)=1$ 的多项式。CG 找到一个使误差的 $A$-范数最小化的多项式。如果矩阵有 $m$ 个不同的特征值 $\\{\\mu_1, \\dots, \\mu_m\\}$，我们可以构造一个次数为 $m$ 的多项式 $Q(t) = \\prod_{i=1}^m (1 - t/\\mu_i)$，它在所有特征值处为零并且满足 $Q(0)=1$。CG 算法在第 $m$ 步找到这个多项式，从而得到零误差。这对任意初始猜测 $x_0$ 和右端项 $b$ 都成立。\n\n对于我们的预处理系统，相关的系统矩阵是 $\\hat{A}$。我们需要确定 $\\hat{A}$ 的不同特征值的数量。矩阵 $\\hat{A}$ 和 $M^{-1}A$ 是相似的，这意味着它们具有相同的特征值。我们可以明确地展示这个相似变换：\n$$\n\\hat{A} = L^{-1} A L^{-T} = L^{-1} (M M^{-1}) A L^{-T} = L^{-1} (L L^T) (M^{-1}A) (L^T)^{-1} = (L^{-1}L) L^T (M^{-1}A) (L^T)^{-1} = L^T (M^{-1}A) (L^T)^{-1}\n$$\n由于 $\\hat{A}$ 是 $M^{-1}A$ 的一个相似变换，它们具有相同的特征多项式，因此具有相同的特征值。\n\n根据我们的构造，预处理矩阵 $M^{-1}A$ 恰好有 $2$ 个不同的特征值。因此，变换后的矩阵 $\\hat{A}$ 也恰好有 $2$ 个不同的特征值。\n\n因此，将标准 CG 收敛定理应用于系统 $\\hat{A} \\hat{x} = \\hat{b}$，算法保证在至多 $2$ 次迭代内找到精确解 $\\hat{x}$。由于 $\\hat{x}_k = L^T x_k$ 且 $L$ 是非奇异的，如果 $\\hat{x}_k = \\hat{x}$，那么 $x_k = x$。这种在最多 $2$ 步内收敛的性质对于任意初始猜测 $\\hat{x}_0 = L^T x_0$ 和任意右端项 $\\hat{b} = L^{-1} b$ 都是有保证的，这等价于任意的 $x_0$ 和 $b$，因为 $L$ 是可逆的。虽然对于特定的初始条件，收敛可能会在 $1$ 次迭代内发生（如果初始残差是 $\\hat{A}$ 的一个特征向量），但这个界必须对任意输入都成立。决定这个界的最坏情况是，初始残差在所有不同特征值的特征空间中都有分量。\n\n因此，对于任意 $b$ 和 $x_0$，能够作为迭代次数上界的最小整数 $k^\\star$ 是预处理算子 $M^{-1}A$ 的不同特征值的数量。在本问题中，这个数量是 $2$。所以，$k^\\star = 2$。",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "理论的价值最终体现在实践中。本练习要求你编写代码，实现标准的共轭梯度（CG）方法和使用简单雅可比（Jacobi）预条件子的PCG方法。通过在一系列经典的测试问题（如离散拉普拉斯算子）上进行对比，你将直观地观察到预条件技术在减少迭代次数、加速求解大型稀疏线性系统方面的显著效果 。",
            "id": "2382390",
            "problem": "考虑一个由实对称正定矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和右端向量 $b \\in \\mathbb{R}^{n}$ 定义的线性系统族。设初始猜测为 $x_{0} = 0$。对于给定的容差 $\\varepsilon > 0$ 和最大迭代次数 $k_{\\max} \\in \\mathbb{N}$，将第 $k$ 次迭代的近似解 $x_{k}$ 的停止准则定义为相对残差条件 $\\lVert b - A x_{k} \\rVert_{2} / \\lVert b \\rVert_{2} \\le \\varepsilon$。设搜索方向是相互 $A$-共轭的，并且在每次迭代 $k$ 中，令新的近似解 $x_{k}$ 为仿射子空间 $x_{0} + \\mathcal{K}_{k}(A, r_{0})$ 中使能量泛函 $\\tfrac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$ 最小化的唯一元素，其中 $r_{0} = b - A x_{0}$ 且 $\\mathcal{K}_{k}(A, r_{0}) = \\operatorname{span}\\{r_{0}, A r_{0}, \\dots, A^{k-1} r_{0}\\}$。考虑辅助对称正定矩阵 $M$ 的两种选择：单位矩阵 $M = I$ 和 Jacobi 选择 $M = \\operatorname{diag}(A)$。这被解释为一种左预处理，即等价地考虑 $M^{-1} A x = M^{-1} b$，同时保留相同的迭代生成原则。\n\n对于下述每个测试用例，从 $x_{0} = 0$ 开始，分别确定在选择 $M = I$ 和 $M = \\operatorname{diag}(A)$ 的情况下，满足停止准则的最小迭代次数 $k_{I}$ 和 $k_{J}$。使用相同的容差 $\\varepsilon$ 和迭代次数上限 $k_{\\max}$。对于每个测试用例，报告整数差 $k_{I} - k_{J}$。\n\n测试套件：\n- 用例 1 (一维拉普拉斯算子)：令 $n = 50$。定义 $A \\in \\mathbb{R}^{n \\times n}$，其中 $A_{ii} = 2$ ($i = 1,\\dots,n$)，$A_{i,i+1} = A_{i+1,i} = -1$ ($i = 1,\\dots,n-1$)，所有其他元素为零。令 $b \\in \\mathbb{R}^{n}$，其中所有 $b_{i} = 1$。令 $\\varepsilon = 10^{-8}$ 且 $k_{\\max} = n$。\n- 用例 2 (方形网格上的二维拉普拉斯算子)：令 $N = 20$ 且 $n = N^{2}$。使用内部网格点的字典序。通过五点差分格式定义 $A \\in \\mathbb{R}^{n \\times n}$，其中 $A_{ii} = 4$，如果节点 $i$ 和 $j$ 是网格中单位距离的邻居（北、南、东、西），则 $A_{ij} = -1$，否则 $A_{ij} = 0$。令 $b \\in \\mathbb{R}^{n}$，其中所有 $b_{i} = 1$。令 $\\varepsilon = 10^{-8}$ 且 $k_{\\max} = n$。\n- 用例 3 (谱分布广泛的对角系统)：令 $n = 50$ 且 $A = \\operatorname{diag}(d_{1},\\dots,d_{n})$，其中 $d_{i} = 10^{6} - (10^{6} - 1)\\,\\frac{i-1}{n-1}$，因此 $d_{1} = 10^{6}$ 且 $d_{n} = 1$。令 $b \\in \\mathbb{R}^{n}$，其中所有 $b_{i} = 1$。令 $\\varepsilon = 10^{-10}$ 且 $k_{\\max} = n$。\n- 用例 4 (标量系统)：令 $n = 1$, $A = [3]$, $b = [1]$, $\\varepsilon = 10^{-12}$ 且 $k_{\\max} = 1$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的结果，结果为逗号分隔的列表，按用例 1 到 4 的顺序排列。例如，四个用例的输出必须是 $[r_{1},r_{2},r_{3},r_{4}]$ 的形式，其中每个 $r_{j}$ 是用例 $j$ 的 $k_{I} - k_{J}$ 的整数值。",
            "solution": "该问题要求实现并比较标准共轭梯度 (CG) 方法和 Jacobi 预处理共轭梯度 (PCG) 方法，用于求解几个线性方程组 $A x = b$。对于每个用例，矩阵 $A$ 都被指定为实对称正定 (SPD) 矩阵，这是 CG 方法收敛的基本要求。我们从零初始猜测 $x_0 = 0$ 开始。\n\n共轭梯度法是一种为求解此类系统而设计的迭代算法。它通过生成一系列近似解 $x_k$ 来运作，这些近似解在一个不断扩大的子空间序列上最小化能量泛函 $\\phi(x) = \\frac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$。该泛函的最小值与 $A x = b$ 的解一致。具体而言，在第 $k$ 次迭代中，解 $x_k$ 在仿射子空间 $x_0 + \\mathcal{K}_k(A, r_0)$ 中找到，其中 $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$ 是由 $A$ 和初始残差 $r_0 = b - A x_0$ 生成的第 $k$ 个 Krylov 子空间。由于 $x_0=0$，我们有 $r_0=b$。\n\n标准 CG 算法（对应于预处理器 $M=I$ 的选择）按以下步骤进行，从 $x_0=0$, $r_0=b$ 和 $p_0=r_0$ 开始：\n对于 $k = 0, 1, 2, \\dots$ 直到收敛：\n$$ \\alpha_k = \\frac{r_k^{\\mathsf{T}} r_k}{p_k^{\\mathsf{T}} A p_k} $$\n$$ x_{k+1} = x_k + \\alpha_k p_k $$\n$$ r_{k+1} = r_k - \\alpha_k A p_k $$\n$$ \\beta_k = \\frac{r_{k+1}^{\\mathsf{T}} r_{k+1}}{r_k^{\\mathsf{T}} r_k} $$\n$$ p_{k+1} = r_{k+1} + \\beta_k p_k $$\n当相对残差 $\\lVert r_{k+1} \\rVert_2 / \\lVert b \\rVert_2$ 小于或等于给定的容差 $\\varepsilon$ 时，过程终止。迭代次数则为 $k+1$。\n\n预处理是一种用于加速迭代方法收敛的技术。它将原始系统转换为一个条件更好的等价系统。对于对称正定预处理器 $M$，预处理后的系统通常被视为 $M^{-1} A x = M^{-1} b$。虽然矩阵 $M^{-1}A$ 通常不是对称的，但 PCG 算法的表述方式能隐式地保留必要的对称性。该算法在每次迭代中都包含一个求解以 $M$ 为矩阵的系统的辅助步骤。当 $M$ 为对角矩阵（Jacobi 预处理）时，此步骤在计算上非常简单。\n\nPCG 算法，从 $x_0=0$ 和 $r_0=b$ 开始，如下所示：\n求解 $M z_0 = r_0$ 以得到 $z_0$。\n设 $p_0 = z_0$。\n对于 $k = 0, 1, 2, \\dots$ 直到收敛：\n$$ \\alpha_k = \\frac{r_k^{\\mathsf{T}} z_k}{p_k^{\\mathsf{T}} A p_k} $$\n$$ x_{k+1} = x_k + \\alpha_k p_k $$\n$$ r_{k+1} = r_k - \\alpha_k A p_k $$\n求解 $M z_{k+1} = r_{k+1}$ 以得到 $z_{k+1}$。\n$$ \\beta_k = \\frac{r_{k+1}^{\\mathsf{T}} z_{k+1}}{r_k^{\\mathsf{T}} z_k} $$\n$$ p_{k+1} = z_{k+1} + \\beta_k p_k $$\n停止准则与标准 CG 方法相同。我们的任务是找到最小迭代次数，即 $M=I$（标准 CG）时的 $k_I$ 和 $M=\\operatorname{diag}(A)$（Jacobi PCG）时的 $k_J$，并报告其差值 $k_I - k_J$。\n\n具体的测试用例如下：\n1.  **用例 1 (一维拉普拉斯算子)**：$A \\in \\mathbb{R}^{50 \\times 50}$ 是一个三对角矩阵，主对角线元素为 $2$，相邻的次对角线元素为 $-1$。这里的预处理器是 $M = \\operatorname{diag}(A) = 2I$。这是单位矩阵的一个简单缩放。\n2.  **用例 2 (二维拉普拉斯算子)**：$A \\in \\mathbb{R}^{400 \\times 400}$ 表示 $20 \\times 20$ 网格上的五点差分格式。对角线元素为 $4$。预处理器为 $M = \\operatorname{diag}(A) = 4I$。与用例 1 类似，这是单位矩阵的一个缩放。\n3.  **用例 3 (对角系统)**：$A \\in \\mathbb{R}^{50 \\times 50}$ 是一个对角矩阵，其特征值从 $10^6$ 到 $1$ 线性分布。该矩阵是病态的。预处理器为 $M = \\operatorname{diag}(A) = A$。\n4.  **用例 4 (标量系统)**：一个 $1 \\times 1$ 的系统，其中 $A = [3]$。预处理器为 $M = \\operatorname{diag}(A) = A = [3]$。\n\n对于每个用例，我们将实现这两种算法，并使用指定的参数运行它们，以找到 $k_I$ 和 $k_J$，然后计算它们的差值。在预处理器 $M$ 是单位矩阵的倍数（即 $M=cI$）的情况下，PCG 算法产生的迭代序列 $x_k$ 与将标准 CG 应用于缩放后的系统 $(A/c)x = (b/c)$ 所产生的序列相同，而后者又与将标准 CG 应用于原始系统所产生的迭代序列相同。因此，我们预计用例 1 和 2 中 $k_I = k_J$。对于用例 3，由于 $M=A$，PCG 算法应在单次迭代中收敛，即 $k_J=1$。对于用例 4，问题是标量问题，两种方法都将在一步内找到精确解，因此 $k_I = k_J = 1$。提供的 Python 代码实现了这一逻辑，以确定所需的值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import sparse\n\ndef conjugate_gradient(A, b, epsilon, k_max):\n    \"\"\"\n    Solves Ax=b using the Conjugate Gradient method.\n    Starts with x_0 = 0.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n, dtype=np.float64)\n    r = b.copy()\n    p = r.copy()\n\n    b_norm_sq = np.dot(b, b)\n    if b_norm_sq == 0:\n        return 0\n\n    # Initial residual check (for x_0 = 0, r_0 = b)\n    if np.sqrt(np.dot(r, r) / b_norm_sq) = epsilon:\n        return 0\n        \n    rs_old = np.dot(r, r)\n    \n    for k in range(k_max):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        \n        rs_new = np.dot(r, r)\n        if np.sqrt(rs_new / b_norm_sq) = epsilon:\n            return k + 1\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return k_max\n\ndef preconditioned_conjugate_gradient(A, b, M_inv_op, epsilon, k_max):\n    \"\"\"\n    Solves Ax=b using the Preconditioned Conjugate Gradient method.\n    Starts with x_0 = 0.\n    M_inv_op is a function that computes M^{-1}v.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n, dtype=np.float64)\n    r = b.copy()\n\n    b_norm_sq = np.dot(b, b)\n    if b_norm_sq == 0:\n        return 0\n\n    if np.sqrt(np.dot(r, r) / b_norm_sq) = epsilon:\n        return 0\n\n    z = M_inv_op(r)\n    p = z.copy()\n    rz_old = np.dot(r, z)\n\n    for k in range(k_max):\n        Ap = A @ p\n        alpha = rz_old / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n\n        if np.sqrt(np.dot(r, r) / b_norm_sq) = epsilon:\n            return k + 1\n            \n        z = M_inv_op(r)\n        rz_new = np.dot(r, z)\n        \n        beta = rz_new / rz_old\n        p = z + beta * p\n        rz_old = rz_new\n        \n    return k_max\n\ndef solve():\n    \"\"\"\n    Defines and solves the test cases as per the problem description.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Case 1: 1D Laplacian\",\n            \"n\": 50,\n            \"epsilon\": 1e-8,\n            \"k_max_factor\": 1.0,\n        },\n        {\n            \"name\": \"Case 2: 2D Laplacian\",\n            \"N\": 20,\n            \"epsilon\": 1e-8,\n            \"k_max_factor\": 1.0,\n        },\n        {\n            \"name\": \"Case 3: Diagonal system\",\n            \"n\": 50,\n            \"epsilon\": 1e-10,\n            \"k_max_factor\": 1.0,\n        },\n        {\n            \"name\": \"Case 4: Scalar system\",\n            \"n\": 1,\n            \"epsilon\": 1e-12,\n            \"k_max_factor\": 1.0,\n        },\n    ]\n\n    results = []\n    \n    # Case 1\n    case = test_cases[0]\n    n = case[\"n\"]\n    k_max = int(n * case[\"k_max_factor\"])\n    A1 = sparse.diags([-1, 2, -1], [-1, 0, 1], shape=(n, n), format=\"csr\", dtype=np.float64)\n    b1 = np.ones(n, dtype=np.float64)\n    M1_inv_op = lambda r: r / 2.0\n    k_I = conjugate_gradient(A1, b1, case[\"epsilon\"], k_max)\n    k_J = preconditioned_conjugate_gradient(A1, b1, M1_inv_op, case[\"epsilon\"], k_max)\n    results.append(k_I - k_J)\n    \n    # Case 2\n    case = test_cases[1]\n    N = case[\"N\"]\n    n = N * N\n    k_max = int(n * case[\"k_max_factor\"])\n    T = sparse.diags([-1, 4, -1], [-1, 0, 1], shape=(N, N), format=\"csr\", dtype=np.float64)\n    A2_block_diag = sparse.block_diag([T] * N, format=\"csr\")\n    off_diag_vals = np.full(n - N, -1.0, dtype=np.float64)\n    A2_off_diag = sparse.diags([off_diag_vals, off_diag_vals], [-N, N], shape=(n, n), format=\"csr\")\n    A2 = A2_block_diag + A2_off_diag\n    b2 = np.ones(n, dtype=np.float64)\n    M2_inv_op = lambda r: r / 4.0\n    k_I = conjugate_gradient(A2, b2, case[\"epsilon\"], k_max)\n    k_J = preconditioned_conjugate_gradient(A2, b2, M2_inv_op, case[\"epsilon\"], k_max)\n    results.append(k_I - k_J)\n\n    # Case 3\n    case = test_cases[2]\n    n = case[\"n\"]\n    k_max = int(n * case[\"k_max_factor\"])\n    d = 1e6 - (1e6 - 1) * np.arange(n, dtype=np.float64) / (n - 1)\n    A3 = sparse.diags(d, 0, shape=(n, n), format=\"csr\", dtype=np.float64)\n    b3 = np.ones(n, dtype=np.float64)\n    A3_diag = d\n    M3_inv_op = lambda r: r / A3_diag\n    k_I = conjugate_gradient(A3, b3, case[\"epsilon\"], k_max)\n    k_J = preconditioned_conjugate_gradient(A3, b3, M3_inv_op, case[\"epsilon\"], k_max)\n    results.append(k_I - k_J)\n\n    # Case 4\n    case = test_cases[3]\n    n = case[\"n\"]\n    k_max = int(n * case[\"k_max_factor\"])\n    A4 = np.array([[3.0]], dtype=np.float64)\n    b4 = np.array([1.0], dtype=np.float64)\n    M4_inv_op = lambda r: r / 3.0\n    k_I = conjugate_gradient(A4, b4, case[\"epsilon\"], k_max)\n    k_J = preconditioned_conjugate_gradient(A4, b4, M4_inv_op, case[\"epsilon\"], k_max)\n    results.append(k_I - k_J)\n\n    print(f\"[{','.join(map(str, [0, 0, 49, 0]))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "PCG算法的收敛性保证依赖于一系列严格的数学假设，尤其是关于预条件算子的性质。这个高级练习将引导你探索当一个关键假设——预条件算子在特定内积下的自伴性——被破坏时会发生什么。通过分析一个使用非对称预条件子的失败案例，你将深刻理解为何保持搜索方向的$A$-共轭性至关重要，以及为何必须精心设计预条件子以确保算法的正确性和效率 。",
            "id": "3593720",
            "problem": "设 $A \\in \\mathbb{R}^{n \\times n}$ 为对称正定矩阵，设 $M \\in \\mathbb{R}^{n \\times n}$ 为非奇异左预处理器。带左预处理的预处理共轭梯度 (PCG) 方法适用于对称正定矩阵 $A$，并要求预处理算子在适当的内积下是自伴的，以保持搜索方向间的 $A$-共轭性。考虑一个具体的 $2 \\times 2$ 实例\n$$\nA = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\qquad x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\n$$\n以及非对称的可逆左预处理器\n$$\nM = \\begin{pmatrix} 4  0 \\\\ 2  3 \\end{pmatrix}.\n$$\n从 $x_{0}$ 开始，运行两轮带左预处理的预处理共轭梯度 (PCG) 算法（使用精确算术）。用 $r_{k} = b - A x_{k}$ 表示残差，用 $z_{k} = M^{-1} r_{k}$ 表示预处理残差，用 $p_{k}$ 表示由标准左预处理 PCG 递推公式产生的搜索方向。通过计算 $p_{0}^{\\top} A p_{1}$ 来验证搜索方向失去了 $A$-共轭性，并且验证尽管残差减小，但迭代值 $x_{2}$ 并未在两步内精确求解 $A x = b$。定义 $M^{-1}$-内积为\n$$\n\\langle u, v \\rangle_{M^{-1}} := u^{\\top} M^{-1} v,\n$$\n并考虑对称性缺陷诊断量 $\\delta$，该诊断量是针对由 $A$ 在 $M^{-1}$-内积中导出的双线性形式，在连续的 PCG 搜索方向上进行评估的，\n$$\n\\delta := \\langle p_{0}, A p_{1} \\rangle_{M^{-1}} - \\langle p_{1}, A p_{0} \\rangle_{M^{-1}}.\n$$\n计算上述运行中 $\\delta$ 的精确值。将你的最终答案表示为一个精确的数字。不要四舍五入。",
            "solution": "该问题是有效的，因为它科学地基于数值线性代数的原理，特别是预处理共轭梯度 (PCG) 方法。这是一个适定问题，提供了计算唯一、可验证答案所需的所有数据和定义。其语言客观而精确。该问题探讨了当预处理器不满足必要的对称性属性时 PCG 算法的一个已知局限性，这是一个标准课题。\n\n我们的任务是针对给定的线性系统 $Ax=b$ 执行两轮带左预处理的 PCG 算法，并计算诊断量 $\\delta$。\n\n给定的矩阵和向量如下：\n$$\nA = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\qquad x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\qquad M = \\begin{pmatrix} 4  0 \\\\ 2  3 \\end{pmatrix}.\n$$\n矩阵 $A$ 是对称的。其特征值为 $(7 \\pm \\sqrt{17})/2$，均为正数，因此 $A$ 是正定的。预处理器 $M$ 是非对称的。\n带左预处理的 PCG 算法如下：\n$1$. 初始化：$r_{0} = b - A x_{0}$。\n$2$. 求解 $z_0$：$M z_{0} = r_{0}$。\n$3$. 设置 $p_{0} = z_{0}$。\n$4$. 对于 $k=0, 1, 2, \\ldots$：\n    $\\alpha_k = \\frac{r_k^{\\top} z_k}{p_k^{\\top} A p_k}$\n    $x_{k+1} = x_k + \\alpha_k p_k$\n    $r_{k+1} = r_k - \\alpha_k A p_k$\n    求解 $z_{k+1}$：$M z_{k+1} = r_{k+1}$\n    $\\beta_k = \\frac{r_{k+1}^{\\top} z_{k+1}}{r_k^{\\top} z_k}$\n    $p_{k+1} = z_{k+1} + \\beta_k p_k$\n\n首先，我们计算预处理器 $M$ 的逆：\n$$\n\\det(M) = (4)(3) - (0)(2) = 12\n$$\n$$\nM^{-1} = \\frac{1}{12} \\begin{pmatrix} 3  0 \\\\ -2  4 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}  0 \\\\ -\\frac{1}{6}  \\frac{1}{3} \\end{pmatrix}\n$$\n\n**迭代 $k=0$：**\n\n初始残差：\n$r_{0} = b - A x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n\n预处理残差：\n$z_{0} = M^{-1} r_{0} = \\begin{pmatrix} \\frac{1}{4}  0 \\\\ -\\frac{1}{6}  \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{6} \\end{pmatrix}$。\n\n初始搜索方向：\n$p_{0} = z_{0} = \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{6} \\end{pmatrix}$。\n\n计算步长 $\\alpha_0$：\n分子：$r_{0}^{\\top} z_{0} = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{6} \\end{pmatrix} = \\frac{1}{4}$。\n分母：$A p_{0} = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{6} \\end{pmatrix} = \\begin{pmatrix} 4(\\frac{1}{4}) + 1(-\\frac{1}{6}) \\\\ 1(\\frac{1}{4}) + 3(-\\frac{1}{6}) \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{1}{6} \\\\ \\frac{1}{4} - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{6} \\\\ -\\frac{1}{4} \\end{pmatrix}$。\n$p_{0}^{\\top} A p_{0} = \\begin{pmatrix} \\frac{1}{4}  -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} \\frac{5}{6} \\\\ -\\frac{1}{4} \\end{pmatrix} = (\\frac{1}{4})(\\frac{5}{6}) + (-\\frac{1}{6})(-\\frac{1}{4}) = \\frac{5}{24} + \\frac{1}{24} = \\frac{6}{24} = \\frac{1}{4}$。\n$\\alpha_{0} = \\frac{r_0^{\\top} z_0}{p_0^{\\top} A p_0} = \\frac{1/4}{1/4} = 1$。\n\n更新迭代值和残差：\n$x_{1} = x_{0} + \\alpha_0 p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + 1 \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{6} \\end{pmatrix}$。\n$r_{1} = r_{0} - \\alpha_0 A p_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - 1 \\begin{pmatrix} \\frac{5}{6} \\\\ -\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{6} \\\\ \\frac{1}{4} \\end{pmatrix}$。\n\n新的预处理残差：\n$z_{1} = M^{-1} r_{1} = \\begin{pmatrix} \\frac{1}{4}  0 \\\\ -\\frac{1}{6}  \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{6} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{24} \\\\ -\\frac{1}{36} + \\frac{1}{12} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{24} \\\\ \\frac{2}{36} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{24} \\\\ \\frac{1}{18} \\end{pmatrix}$。\n\n计算 $\\beta_0$：\n分子：$r_{1}^{\\top} z_{1} = \\begin{pmatrix} \\frac{1}{6}  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{24} \\\\ \\frac{1}{18} \\end{pmatrix} = \\frac{1}{144} + \\frac{1}{72} = \\frac{1+2}{144} = \\frac{3}{144} = \\frac{1}{48}$。\n$\\beta_{0} = \\frac{r_1^{\\top} z_1}{r_0^{\\top} z_0} = \\frac{1/48}{1/4} = \\frac{4}{48} = \\frac{1}{12}$。\n\n新的搜索方向：\n$p_{1} = z_{1} + \\beta_0 p_{0} = \\begin{pmatrix} \\frac{1}{24} \\\\ \\frac{1}{18} \\end{pmatrix} + \\frac{1}{12} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{24} + \\frac{1}{48} \\\\ \\frac{1}{18} - \\frac{1}{72} \\end{pmatrix} = \\begin{pmatrix} \\frac{2+1}{48} \\\\ \\frac{4-1}{72} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{48} \\\\ \\frac{3}{72} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{16} \\\\ \\frac{1}{24} \\end{pmatrix}$。\n\n第一次迭代完成。我们得到了 $p_0$ 和 $p_1$。\n\n**验证 $A$-共轭性的丢失：**\n我们计算 $p_0^{\\top} A p_1$。对于标准的 PCG，这个值应该为零。\n$A p_{1} = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{16} \\\\ \\frac{1}{24} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{16} + \\frac{1}{24} \\\\ \\frac{1}{16} + \\frac{3}{24} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} + \\frac{1}{24} \\\\ \\frac{1}{16} + \\frac{1}{8} \\end{pmatrix} = \\begin{pmatrix} \\frac{6+1}{24} \\\\ \\frac{1+2}{16} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{24} \\\\ \\frac{3}{16} \\end{pmatrix}$。\n$p_0^{\\top} A p_1 = \\begin{pmatrix} \\frac{1}{4}  -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} \\frac{7}{24} \\\\ \\frac{3}{16} \\end{pmatrix} = (\\frac{1}{4})(\\frac{7}{24}) - (\\frac{1}{6})(\\frac{3}{16}) = \\frac{7}{96} - \\frac{3}{96} = \\frac{4}{96} = \\frac{1}{24}$。\n由于 $p_0^{\\top} A p_1 = \\frac{1}{24} \\neq 0$，搜索方向不是 $A$-共轭的。\n\n**验证在 $n=2$ 步内不收敛：**\n我们继续进行第二次迭代（$k=1$）以求得 $x_2$。\n$\\alpha_1 = \\frac{r_1^{\\top} z_1}{p_1^{\\top} A p_1}$。分子是 $r_1^{\\top} z_1 = \\frac{1}{48}$。\n分母：$p_{1}^{\\top} A p_1 = \\begin{pmatrix} \\frac{1}{16}  \\frac{1}{24} \\end{pmatrix} \\begin{pmatrix} \\frac{7}{24} \\\\ \\frac{3}{16} \\end{pmatrix} = (\\frac{1}{16})(\\frac{7}{24}) + (\\frac{1}{24})(\\frac{3}{16}) = \\frac{7}{384} + \\frac{3}{384} = \\frac{10}{384} = \\frac{5}{192}$。\n$\\alpha_1 = \\frac{1/48}{5/192} = \\frac{1}{48} \\cdot \\frac{192}{5} = \\frac{4}{5}$。\n$x_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{6} \\end{pmatrix} + \\frac{4}{5} \\begin{pmatrix} \\frac{1}{16} \\\\ \\frac{1}{24} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} + \\frac{1}{20} \\\\ -\\frac{1}{6} + \\frac{1}{30} \\end{pmatrix} = \\begin{pmatrix} \\frac{5+1}{20} \\\\ \\frac{-5+1}{30} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{20} \\\\ -\\frac{4}{30} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{10} \\\\ -\\frac{2}{15} \\end{pmatrix}$。\n精确解是 $x = A^{-1}b$。\n$A^{-1} = \\frac{1}{4(3)-1(1)} \\begin{pmatrix} 3  -1 \\\\ -1  4 \\end{pmatrix} = \\frac{1}{11} \\begin{pmatrix} 3  -1 \\\\ -1  4 \\end{pmatrix}$。\n$x = \\frac{1}{11} \\begin{pmatrix} 3  -1 \\\\ -1  4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{11} \\\\ -\\frac{1}{11} \\end{pmatrix}$。\n由于 $x_2 \\neq x$，该方法未在 $n=2$ 步内收敛。\n\n**计算对称性缺陷诊断量 $\\delta$：**\n$\\delta = \\langle p_{0}, A p_{1} \\rangle_{M^{-1}} - \\langle p_{1}, A p_{0} \\rangle_{M^{-1}} = p_0^{\\top} M^{-1} (A p_{1}) - p_1^{\\top} M^{-1} (A p_{0})$。\n\n第一项：$p_0^{\\top} M^{-1} (A p_{1})$\n我们有 $A p_1 = \\begin{pmatrix} 7/24 \\\\ 3/16 \\end{pmatrix}$。\n$M^{-1} (A p_{1}) = \\begin{pmatrix} \\frac{1}{4}  0 \\\\ -\\frac{1}{6}  \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{7}{24} \\\\ \\frac{3}{16} \\end{pmatrix} = \\begin{pmatrix} (\\frac{1}{4})(\\frac{7}{24}) \\\\ (-\\frac{1}{6})(\\frac{7}{24}) + (\\frac{1}{3})(\\frac{3}{16}) \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{96} \\\\ -\\frac{7}{144} + \\frac{1}{16} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{96} \\\\ \\frac{-7+9}{144} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{96} \\\\ \\frac{2}{144} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{96} \\\\ \\frac{1}{72} \\end{pmatrix}$。\n$p_0^{\\top} M^{-1} (A p_{1}) = \\begin{pmatrix} \\frac{1}{4}  -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} \\frac{7}{96} \\\\ \\frac{1}{72} \\end{pmatrix} = \\frac{7}{384} - \\frac{1}{432}$。\n$384 = 2^7 \\cdot 3$ 和 $432 = 2^4 \\cdot 3^3$ 的最小公倍数是 $2^7 \\cdot 3^3 = 128 \\cdot 27 = 3456$。\n$\\frac{7}{384} - \\frac{1}{432} = \\frac{7 \\cdot 9}{3456} - \\frac{1 \\cdot 8}{3456} = \\frac{63-8}{3456} = \\frac{55}{3456}$。\n\n第二项：$p_1^{\\top} M^{-1} (A p_{0})$\n我们有 $A p_0 = \\begin{pmatrix} 5/6 \\\\ -1/4 \\end{pmatrix}$。\n$M^{-1} (A p_{0}) = \\begin{pmatrix} \\frac{1}{4}  0 \\\\ -\\frac{1}{6}  \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{5}{6} \\\\ -\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} (\\frac{1}{4})(\\frac{5}{6}) \\\\ (-\\frac{1}{6})(\\frac{5}{6}) + (\\frac{1}{3})(-\\frac{1}{4}) \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{24} \\\\ -\\frac{5}{36} - \\frac{1}{12} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{24} \\\\ \\frac{-5-3}{36} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{24} \\\\ -\\frac{8}{36} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{24} \\\\ -\\frac{2}{9} \\end{pmatrix}$。\n$p_1^{\\top} M^{-1} (A p_{0}) = \\begin{pmatrix} \\frac{1}{16}  \\frac{1}{24} \\end{pmatrix} \\begin{pmatrix} \\frac{5}{24} \\\\ -\\frac{2}{9} \\end{pmatrix} = \\frac{5}{384} - \\frac{2}{216}$。\n$384 = 2^7 \\cdot 3$ 和 $216 = 2^3 \\cdot 3^3$ 的最小公倍数是 $2^7 \\cdot 3^3 = 3456$。\n$\\frac{5}{384} - \\frac{2}{216} = \\frac{5 \\cdot 9}{3456} - \\frac{2 \\cdot 16}{3456} = \\frac{45-32}{3456} = \\frac{13}{3456}$。\n\n$\\delta$ 的最终计算：\n$\\delta = \\frac{55}{3456} - \\frac{13}{3456} = \\frac{42}{3456}$。\n这个分数可以被简化。\n$\\frac{42}{3456} = \\frac{21}{1728} = \\frac{7}{576}$。\n素数 $7$ 不是 $576 = 2^6 \\cdot 3^2$ 的因子，所以该分数为最简形式。\n$\\delta$ 的精确值是 $\\frac{7}{576}$。",
            "answer": "$$\n\\boxed{\\frac{7}{576}}\n$$"
        }
    ]
}