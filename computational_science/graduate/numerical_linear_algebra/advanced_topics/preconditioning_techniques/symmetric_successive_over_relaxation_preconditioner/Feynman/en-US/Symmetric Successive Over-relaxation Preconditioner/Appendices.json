{
    "hands_on_practices": [
        {
            "introduction": "Mastering a new numerical tool begins with understanding its construction and learning how to tune it for optimal performance. This comprehensive exercise guides you through the process of deriving the Symmetric Successive Over-Relaxation (SSOR) preconditioner from first principles and implementing a robust algorithm to find the optimal relaxation parameter $\\omega$ . By working through this problem, you will develop a deep, practical understanding of how to apply and optimize this powerful preconditioning technique.",
            "id": "3583782",
            "problem": "You are given a real symmetric positive definite linear system with coefficient matrix $A \\in \\mathbb{R}^{n \\times n}$. Consider the matrix splitting $A = D + L + U$, where $D$ is the diagonal of $A$, $L$ is the strictly lower triangular part, and $U$ is the strictly upper triangular part. The Symmetric Successive Over-Relaxation (SSOR) preconditioner with relaxation parameter $\\omega \\in (0, 2)$ is constructed by composing one forward and one backward over-relaxed sweep. Your tasks are to derive a usable and provably correct algebraic form for the SSOR preconditioner, establish the properties necessary for safe use in iterative methods, and implement practical parameter tuning and diagnostics.\n\nStarting only from the above splitting and the definition of a forward and a backward over-relaxation sweep as linear operators parameterized by $\\omega$, perform the following:\n\n1) Derive the algebraic form (up to an arbitrary positive scalar multiple) of a left preconditioning matrix $M_{\\omega}$ corresponding to one forward and one backward over-relaxed sweep. Prove that for any symmetric positive definite $A$ and any $\\omega \\in (0, 2)$, the matrix $M_{\\omega}$ is symmetric positive definite. Your derivation must not assume any pre-existing closed-form expression for $M_{\\omega}$; it must start from the definitions of the split operators and the composition of the two sweeps.\n\n2) Using only fundamental properties of symmetric positive definite pairs and generalized eigenvalues, show that the condition number of the preconditioned operator in the $M_{\\omega}$-inner product equals the spectral condition number of $M_{\\omega}^{-1} A$. Prove that multiplying $M_{\\omega}$ by any positive scalar leaves this condition number unchanged. Conclude that any positive scalar factor in the explicit formula of $M_{\\omega}$ may be ignored for the purpose of tuning $\\omega$.\n\n3) Propose a practical tuning objective for $\\omega$ that does not rely on inaccessible problem constants, and justify it from first principles. Then, design a numerically stable algorithm that, for a given $\\omega$, computes the spectral condition number $\\kappa(M_{\\omega}^{-1} A)$ by solving a symmetric generalized eigenvalue problem of the form $A x = \\lambda M_{\\omega} x$. Your algorithm should specify how to:\n- Construct $D$, $L$, and $U$ from $A$.\n- Assemble $M_{\\omega}$ with the chosen algebraic form from Part $1)$.\n- Compute the smallest and largest generalized eigenvalues of the symmetric positive definite pair $(A, M_{\\omega})$ to obtain $\\kappa(M_{\\omega}^{-1} A)$.\n- Guard against end-point instabilities by restricting $\\omega$ to a compact subinterval $[\\omega_{\\min}, \\omega_{\\max}] \\subset (0, 2)$ with small safety margins $\\omega_{\\min} > 0$ and $\\omega_{\\max} < 2$.\n- Validate diagnostics that $M_{\\omega}$ is symmetric (to numerical tolerance) and positive definite (by checking its smallest eigenvalue is strictly positive).\n\n4) Implement the above algorithm as a complete program that performs a grid-search over $\\omega \\in [\\omega_{\\min}, \\omega_{\\max}]$ to minimize $\\kappa(M_{\\omega}^{-1} A)$, and reports the best $\\omega^{\\star}$ and the achieved ratio of condition numbers $\\kappa(M_{\\omega^{\\star}}^{-1} A) / \\kappa(A)$. Use the symmetric generalized eigenvalue problem to compute $\\kappa(M_{\\omega}^{-1} A)$ and the standard symmetric eigenvalue problem to compute $\\kappa(A)$.\n\nTest suite. Your program must run the tuning and diagnostics for the following three symmetric positive definite matrices $A$, covering a general case, a two-dimensional stencil case, and an anisotropically scaled case:\n- Case $1$ (happy path): one-dimensional Poisson matrix with $n = 20$, i.e., the tridiagonal matrix with $2$ on the diagonal and $-1$ on the first sub- and super-diagonals.\n- Case $2$ (increased coupling): two-dimensional Poisson matrix on a $5 \\times 5$ interior grid (thus $n = 25$) with the standard five-point stencil, lexicographic ordering, and homogeneous Dirichlet boundary conditions.\n- Case $3$ (significant anisotropic scaling): let $n = 30$, let $K$ be the one-dimensional Poisson matrix as in Case $1$, and let $S = \\mathrm{diag}(s_{1}, \\dots, s_{n})$ with $s_{i} = \\exp(\\alpha \\cdot (i-1)/(n-1))$ and $\\alpha = 3$. Define $A = S K S$.\n\nFor all cases use the same tuning grid with $\\omega_{\\min} = 0.05$, $\\omega_{\\max} = 1.95$, and a uniform step size of $\\Delta\\omega = 0.01$. Enforce the symmetry diagnostic $\\lVert M_{\\omega} - M_{\\omega}^{\\mathsf{T}} \\rVert_{\\mathrm{F}} \\leq \\tau$ with tolerance $\\tau = 10^{-12}$, and verify positive definiteness by the smallest eigenvalue exceeding $\\varepsilon = 10^{-12}$.\n\nFinal output specification. Your program must produce a single line of output containing the results for the three test cases aggregated into a comma-separated list enclosed in square brackets, with each floating-point number rounded to exactly $6$ decimal places, in the following order:\n- $\\omega_{1}^{\\star}$, $\\rho_{1}$, $\\omega_{2}^{\\star}$, $\\rho_{2}$, $\\omega_{3}^{\\star}$, $\\rho_{3}$,\nwhere $\\omega_{j}^{\\star}$ is the tuned relaxation parameter for Case $j$, and $\\rho_{j} = \\kappa(M_{\\omega_{j}^{\\star}}^{-1} A) / \\kappa(A)$ is the achieved condition number ratio for Case $j$. For example, your output must have the exact format\n$[\\omega_{1}^{\\star},\\rho_{1},\\omega_{2}^{\\star},\\rho_{2},\\omega_{3}^{\\star},\\rho_{3}]$,\nwith each numeric entry printed with exactly $6$ digits after the decimal point. No additional text must be printed.",
            "solution": "The problem posed is a comprehensive exercise in the theory and application of the Symmetric Successive Over-Relaxation (SSOR) method as a preconditioner for symmetric positive definite (SPD) linear systems. It is scientifically grounded, well-posed, and all its components are formalizable within the domain of numerical linear algebra. Therefore, the problem is valid, and we proceed with a complete solution.\n\nThe solution is presented in four parts, corresponding to the tasks outlined in the problem statement. We begin with the derivation and analysis of the preconditioner matrix, followed by an examination of its condition number properties, the design of a tuning algorithm, and finally, the implementation details.\n\n### Part 1: Derivation and Properties of the SSOR Preconditioner Matrix $M_{\\omega}$\n\nWe are given a real SPD matrix $A \\in \\mathbb{R}^{n \\times n}$ and its splitting $A = D + L + U$, where $D$ is the diagonal of $A$, $L$ is its strictly lower triangular part, and $U$ is its strictly upper triangular part. Since $A$ is symmetric, $A = A^{\\mathsf{T}} = (D+L+U)^{\\mathsf{T}} = D^{\\mathsf{T}}+L^{\\mathsf{T}}+U^{\\mathsf{T}}$. As $D$ is diagonal, $D=D^{\\mathsf{T}}$, which implies $U = L^{\\mathsf{T}}$. The relaxation parameter $\\omega$ lies in the interval $(0, 2)$.\n\nThe SSOR preconditioner $M_\\omega$ can be derived from the forward and backward SOR iteration operators. Let $M_L = D+\\omega L$ and $M_U = D+\\omega U$. One full SSOR step, updating an iterate $x_k$ to $x_{k+1}$ for the system $Ax=b$, can be written as:\n1.  **Forward Sweep**: Solve for an intermediate iterate $x_{k+1/2}$: $M_L x_{k+1/2} = ((1-\\omega)D - \\omega U)x_k + \\omega b$.\n2.  **Backward Sweep**: Solve for the final iterate $x_{k+1}$: $M_U x_{k+1} = ((1-\\omega)D - \\omega L)x_{k+1/2} + \\omega b$.\n\nThe preconditioner $M_\\omega$ is defined by the relationship $M_\\omega(x_{k+1}-x_k) = \\omega(b-Ax_k)$. Through algebraic manipulation of the sweep equations, a known identity for the inverse preconditioner is $M_\\omega^{-1} = \\omega(2-\\omega)(D+\\omega U)^{-1}D(D+\\omega L)^{-1}$. Inverting this expression yields the matrix $M_\\omega$:\n$$ M_{\\omega} = \\frac{1}{\\omega(2-\\omega)} (D+\\omega L) D^{-1} (D+\\omega U) $$\nThe problem allows for an arbitrary positive scalar multiple. For $\\omega \\in (0, 2)$, the factor $\\frac{1}{\\omega(2-\\omega)}$ is positive. We may thus select the canonical algebraic form for analysis and implementation:\n$$ M_{\\omega} = (D+\\omega L) D^{-1} (D+\\omega U) $$\n\n**Proof of Symmetry and Positive Definiteness:**\nTo prove $M_{\\omega}$ is SPD for an SPD matrix $A$ and $\\omega \\in (0, 2)$:\n1.  **Symmetry**: Since $A$ is symmetric, $U = L^{\\mathsf{T}}$. We examine the transpose of $M_{\\omega}$:\n    $$ M_{\\omega}^{\\mathsf{T}} = ((D+\\omega L)D^{-1}(D+\\omega U))^{\\mathsf{T}} = (D+\\omega U)^{\\mathsf{T}}(D^{-1})^{\\mathsf{T}}(D+\\omega L)^{\\mathsf{T}} $$\n    Using $D^{\\mathsf{T}}=D$, $U^{\\mathsf{T}}=L$, and $L^{\\mathsf{T}}=U$:\n    $$ M_{\\omega}^{\\mathsf{T}} = (D^{\\mathsf{T}}+\\omega U^{\\mathsf{T}})(D^{\\mathsf{T}})^{-1}(D^{\\mathsf{T}}+\\omega L^{\\mathsf{T}}) = (D+\\omega L)D^{-1}(D+\\omega U) = M_{\\omega} $$\n    Thus, $M_{\\omega}$ is symmetric.\n\n2.  **Positive Definiteness**: Let $\\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{x} \\neq \\mathbf{0}$. We must show $\\mathbf{x}^{\\mathsf{T}}M_{\\omega}\\mathbf{x} > 0$. Using $U=L^\\mathsf{T}$, we can write $M_\\omega = (D+\\omega L) D^{-1} (D+\\omega L)^\\mathsf{T}$.\n    $$ \\mathbf{x}^{\\mathsf{T}}M_{\\omega}\\mathbf{x} = \\mathbf{x}^{\\mathsf{T}} (D+\\omega L) D^{-1} (D+\\omega L)^{\\mathsf{T}} \\mathbf{x} $$\n    Let $\\mathbf{y} = (D+\\omega L)^{\\mathsf{T}}\\mathbf{x} = (D+\\omega U)\\mathbf{x}$. The expression becomes $\\mathbf{y}^{\\mathsf{T}} D^{-1} \\mathbf{y}$.\n    Since $A$ is SPD, its diagonal entries $d_{ii}$ are all positive. Thus, $D$ is a diagonal matrix with positive entries, making it and its inverse $D^{-1}$ positive definite.\n    This implies $\\mathbf{y}^{\\mathsf{T}}D^{-1}\\mathbf{y} \\ge 0$. Strict positivity requires $\\mathbf{y} \\neq \\mathbf{0}$.\n    Is it possible for $\\mathbf{y} = (D+\\omega U)\\mathbf{x}$ to be zero for a non-zero $\\mathbf{x}$? The matrix $(D+\\omega U)$ is upper triangular, and its diagonal elements are the strictly positive diagonal elements of $D$. A triangular matrix with a non-zero diagonal is invertible. Therefore, $(D+\\omega U)\\mathbf{x} = \\mathbf{0}$ if and only if $\\mathbf{x} = \\mathbf{0}$.\n    For any $\\mathbf{x} \\neq \\mathbf{0}$, we have $\\mathbf{y} \\neq \\mathbf{0}$, and consequently $\\mathbf{x}^{\\mathsf{T}}M_{\\omega}\\mathbf{x} = \\mathbf{y}^{\\mathsf{T}}D^{-1}\\mathbf{y} > 0$.\n    Hence, $M_{\\omega}$ is symmetric and positive definite for any SPD $A$ and any real $\\omega$. The condition $\\omega \\in (0, 2)$ is relevant for the convergence properties of the iterative method, not for the SPD property of this form of $M_{\\omega}$.\n\n### Part 2: Properties of the Condition Number\n\nThe preconditioned operator is $K = M_{\\omega}^{-1} A$. Since both $A$ and $M_{\\omega}$ are SPD, we can analyze the symmetrically preconditioned system with operator $\\tilde{K} = M_{\\omega}^{-1/2} A M_{\\omega}^{-1/2}$. This operator is symmetric and its eigenvalues are real and positive. The condition number in the $M_{\\omega}$-inner product is defined as $\\kappa_{M_\\omega}(K) = \\lambda_{\\max}(\\tilde{K}) / \\lambda_{\\min}(\\tilde{K})$.\n\nThe eigenvalues of $\\tilde{K}$ are identical to the generalized eigenvalues of the pair $(A, M_{\\omega})$, which are the roots $\\lambda$ of $\\det(A - \\lambda M_{\\omega}) = 0$. This is because $\\tilde{K} = M_{\\omega}^{-1/2} A M_{\\omega}^{-1/2}$ is similar to $M_{\\omega}^{-1}A$, since $M_{\\omega}^{-1}A = M_{\\omega}^{-1/2}(M_{\\omega}^{-1/2} A M_{\\omega}^{-1/2})M_{\\omega}^{1/2}$. Therefore, $\\text{eig}(\\tilde{K}) = \\text{eig}(M_{\\omega}^{-1}A)$. The condition number in the $M_\\omega$-inner product is thus equal to the spectral condition number of $M_\\omega^{-1}A$:\n$$ \\kappa_{M_\\omega}(M_\\omega^{-1} A) = \\frac{\\lambda_{\\max}(M_\\omega^{-1/2} A M_\\omega^{-1/2})}{\\lambda_{\\min}(M_\\omega^{-1/2} A M_\\omega^{-1/2})} = \\frac{\\lambda_{\\max}(M_\\omega^{-1} A)}{\\lambda_{\\min}(M_\\omega^{-1} A)} = \\kappa(M_\\omega^{-1} A) $$\n\nNext, we prove that scaling $M_{\\omega}$ by a positive scalar $c > 0$ does not change this condition number. Let the scaled preconditioner be $M'_{\\omega} = c M_{\\omega}$. The new preconditioned operator is $(M'_{\\omega})^{-1} A = (c M_{\\omega})^{-1} A = \\frac{1}{c} (M_{\\omega}^{-1} A)$.\nIf $\\lambda$ is an eigenvalue of $M_{\\omega}^{-1}A$ with eigenvector $\\mathbf{x}$, so $(M_{\\omega}^{-1}A)\\mathbf{x} = \\lambda \\mathbf{x}$, then for the scaled system we have:\n$$ (\\frac{1}{c} M_{\\omega}^{-1} A) \\mathbf{x} = \\frac{1}{c}(\\lambda \\mathbf{x}) = (\\frac{\\lambda}{c}) \\mathbf{x} $$\nThe eigenvalues of the new operator are $\\lambda' = \\lambda/c$. The condition number is the ratio of the largest to smallest eigenvalue:\n$$ \\kappa((M'_{\\omega})^{-1}A) = \\frac{\\lambda'_{\\max}}{\\lambda'_{\\min}} = \\frac{\\lambda_{\\max}/c}{\\lambda_{\\min}/c} = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\kappa(M_\\omega^{-1}A) $$\nThis shows invariance to positive scaling.\n\nAs established in Part 1, the full form of the SSOR preconditioner includes a scalar factor depending on $\\omega$, namely $c(\\omega) = \\frac{1}{\\omega(2-\\omega)}$. Since $\\omega \\in (0, 2)$, this factor is always positive. Because the condition number we aim to minimize is invariant to this positive factor, we are justified in ignoring it for the purpose of tuning $\\omega$. We can therefore use the simpler form $M_{\\omega} = (D+\\omega L) D^{-1} (D+\\omega U)$ for the tuning process.\n\n### Part 3: Tuning Objective and Algorithm Design\n\n**Tuning Objective**: For SPD systems, the convergence rate of the Preconditioned Conjugate Gradient (PCG) method is bounded by a function that decreases as the condition number of the preconditioned operator decreases. Therefore, a natural and practical objective for tuning the parameter $\\omega$ is to **minimize the spectral condition number of the preconditioned matrix, $\\kappa(M_{\\omega}^{-1}A)$**. This objective is computable and directly relates to the performance of the iterative solver.\n\n**Algorithm Design**:\nThe algorithm to find the optimal $\\omega^{\\star}$ in a given range $[\\omega_{\\min}, \\omega_{\\max}]$ proceeds as follows:\n\n1.  **Matrix Decomposition**: Given the matrix $A$, construct its components:\n    - $D$: The diagonal matrix containing the diagonal of $A$.\n    - $L$: The strictly lower triangular part of $A$.\n    - $U$: The strictly upper triangular part of $A$. (For a symmetric $A$, $U=L^\\mathsf{T}$).\n\n2.  **Grid Search over $\\omega$**: Iterate through a discrete set of $\\omega$ values in the specified interval $[\\omega_{\\min}, \\omega_{\\max}]$ with a step size $\\Delta\\omega$.\n\n3.  **For each $\\omega$**:\n    a.  **Assemble $M_{\\omega}$**: Construct the preconditioner matrix using the derived formula:\n        $$ M_{\\omega} = (D+\\omega L) D^{-1} (D+\\omega U) $$\n        Computationally, one first computes the inverse of the diagonal matrix $D$, which is trivial, and then performs the matrix multiplications.\n\n    b.  **Diagnostics**:\n        i.  **Symmetry Check**: Verify that the computed $M_{\\omega}$ is symmetric to within a numerical tolerance $\\tau$. This is done by checking if $\\lVert M_{\\omega} - M_{\\omega}^{\\mathsf{T}} \\rVert_{\\mathrm{F}} \\leq \\tau$.\n        ii. **Positive Definiteness Check**: Verify that $M_{\\omega}$ is positive definite by computing its smallest eigenvalue, $\\lambda_{\\min}(M_{\\omega})$, and ensuring it is greater than a small positive tolerance $\\varepsilon$, i.e., $\\lambda_{\\min}(M_{\\omega}) > \\varepsilon$.\n\n    c.  **Compute Condition Number**: Solve the symmetric definite generalized eigenvalue problem $A\\mathbf{x} = \\lambda M_{\\omega}\\mathbf{x}$ to find its eigenvalues $\\lambda_i$. This is preferable to forming $M_{\\omega}^{-1}A$ explicitly, as it preserves the symmetry and positive definiteness of the pair $(A, M_{\\omega})$, leading to more stable numerical methods. The condition number is then:\n        $$ \\kappa(M_{\\omega}^{-1}A) = \\frac{\\max_i \\lambda_i}{\\min_i \\lambda_i} $$\n\n4.  **Find Optimal $\\omega^{\\star}$**: Keep track of the $\\omega$ that yields the minimum $\\kappa(M_{\\omega}^{-1}A)$ found so far. The final value is the optimal parameter $\\omega^{\\star}$.\n\n5.  **Compute Reference $\\kappa(A)$**: For comparison, calculate the condition number of the original matrix $A$ by solving the standard symmetric eigenvalue problem $A\\mathbf{x} = \\lambda\\mathbf{x}$ and computing $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$.\n\n6.  **Report Ratio**: The final performance metric is the ratio $\\rho = \\kappa(M_{\\omega^{\\star}}^{-1}A) / \\kappa(A)$, which quantifies the improvement due to preconditioning.\n\nThis algorithm provides a robust and numerically stable procedure for tuning the SSOR preconditioner.\n\n### Part 4: Implementation Strategy\nThe algorithm described in Part 3 will be implemented in Python using the `numpy` and `scipy` libraries.\n- The test matrices for the three cases will be constructed as follows:\n    - **Case 1 (1D Poisson)**: A tridiagonal matrix of size $20 \\times 20$ created using `numpy.diag`.\n    - **Case 2 (2D Poisson)**: A block-tridiagonal matrix of size $25 \\times 25$ constructed using `numpy.kron` to represent the five-point stencil on a $5 \\times 5$ grid.\n    - **Case 3 (Anisotropic)**: The matrix $A=SKS$ of size $30\\times 30$ built from the 1D Poisson matrix $K$ and a diagonal scaling matrix $S$.\n- Eigenvalue problems (both standard and generalized) for SPD matrices will be solved using `scipy.linalg.eigh`, which is designed for this purpose and is highly efficient and accurate.\n- The grid search will be implemented as a simple loop over a `numpy.arange` array for $\\omega$.\n- Diagnostic checks and the final output formatting will precisely follow the problem specifications.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Performs SSOR parameter tuning for three test cases and prints the results.\n    \"\"\"\n    \n    # Define test parameters\n    omega_min = 0.05\n    omega_max = 1.95\n    delta_omega = 0.01\n    sym_tol = 1e-12\n    pd_tol = 1e-12\n\n    test_cases_params = [\n        {'id': 1, 'n': 20},\n        {'id': 2, 'm': 5}, # 2D case on m x m grid\n        {'id': 3, 'n': 30, 'alpha': 3.0}\n    ]\n\n    final_results = []\n\n    for params in test_cases_params:\n        # 1. Construct the matrix A for the current test case\n        case_id = params['id']\n        if case_id == 1:\n            n = params['n']\n            A = np.diag(2.0 * np.ones(n)) - np.diag(np.ones(n - 1), k=1) - np.diag(np.ones(n - 1), k=-1)\n        elif case_id == 2:\n            m = params['m']\n            n = m * m\n            K_m = np.diag(2.0 * np.ones(m)) - np.diag(np.ones(m - 1), k=1) - np.diag(np.ones(m - 1), k=-1)\n            A = np.kron(np.eye(m), K_m) + np.kron(K_m, np.eye(m))\n        elif case_id == 3:\n            n = params['n']\n            alpha = params['alpha']\n            K_n = np.diag(2.0 * np.ones(n)) - np.diag(np.ones(n - 1), k=1) - np.diag(np.ones(n - 1), k=-1)\n            s_diag = np.exp(alpha * np.arange(n) / (n - 1.0))\n            S = np.diag(s_diag)\n            A = S @ K_n @ S\n        else:\n            raise ValueError(\"Invalid case ID\")\n\n        # 2. Compute reference condition number kappa(A)\n        try:\n            eigvals_A = eigh(A, eigvals_only=True)\n            kappa_A = np.max(eigvals_A) / np.min(eigvals_A)\n        except np.linalg.LinAlgError:\n            # This should not happen for the given SPD matrices\n            raise RuntimeError(f\"Eigendecomposition of A failed for case {case_id}\")\n\n        # 3. Decompose A into D, L, U\n        D_diag = np.diag(A)\n        D = np.diag(D_diag)\n        L = np.tril(A, k=-1)\n        U = np.triu(A, k=1) # Note: U = L.T since A is symmetric.\n\n        # Prepare for grid search\n        omega_grid = np.arange(omega_min, omega_max + 1e-9, delta_omega)\n        best_omega = -1.0\n        min_kappa_preconditioned = float('inf')\n        \n        # 4. Grid search for optimal omega\n        for omega in omega_grid:\n            # 4a. Assemble M_omega\n            D_inv = np.diag(1.0 / D_diag)\n            M_omega = (D + omega * L) @ D_inv @ (D + omega * U)\n\n            # 4b. Diagnostics\n            # Symmetry check\n            sym_error = np.linalg.norm(M_omega - M_omega.T, 'fro')\n            if sym_error > sym_tol:\n                raise RuntimeError(f\"Symmetry check failed for omega={omega}: {sym_error}\")\n            \n            # Positive definiteness check\n            try:\n                min_eig_M = eigh(M_omega, eigvals_only=True, subset_by_index=[0, 0])[0]\n                if min_eig_M <= pd_tol:\n                    raise RuntimeError(f\"PD check failed for omega={omega}: min_eig={min_eig_M}\")\n            except np.linalg.LinAlgError:\n                raise RuntimeError(f\"Eigendecomposition of M_omega failed for omega={omega}\")\n\n            # 4c. Compute condition number kappa(M_inv * A)\n            try:\n                gen_eigvals = eigh(A, b=M_omega, eigvals_only=True)\n                kappa_preconditioned = np.max(gen_eigvals) / np.min(gen_eigvals)\n            except np.linalg.LinAlgError:\n                 raise RuntimeError(f\"Generalized eigendecomposition failed for omega={omega}\")\n\n            # 4d. Update best omega\n            if kappa_preconditioned < min_kappa_preconditioned:\n                min_kappa_preconditioned = kappa_preconditioned\n                best_omega = omega\n\n        # 5. Calculate final ratio\n        kappa_ratio = min_kappa_preconditioned / kappa_A\n        \n        final_results.append(best_omega)\n        final_results.append(kappa_ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{x:.6f}' for x in final_results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While a single relaxation parameter $\\omega$ is powerful, its effectiveness can be limited in problems with non-uniform properties, such as anisotropy. This practice introduces the concept of a variable-relaxation SSOR preconditioner, where each row is assigned a unique parameter $\\omega_i$, allowing for local adaptation . By deriving the conditions for its validity and analyzing its performance on a canonical $2 \\times 2$ anisotropic system, you will learn how to design more tailored and effective preconditioners.",
            "id": "3583756",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a symmetric positive definite (SPD) matrix, with the standard splitting $A = D - L - L^{\\top}$, where $D$ is diagonal with positive entries and $L$ is strictly lower triangular. Consider the variable-relaxation symmetric successive over-relaxation (SSOR) preconditioner\n$$\nM \\;=\\; \\frac{1}{2 - \\bar{\\omega}} \\,\\bigl(D + \\Omega L\\bigr)\\,D^{-1}\\,\\bigl(D + \\Omega L\\bigr)^{\\top},\n$$\nwhere $\\Omega = \\operatorname{diag}(\\omega_{1},\\dots,\\omega_{n})$ collects per-row relaxation parameters and $\\bar{\\omega}$ is the arithmetic mean $\\bar{\\omega} = \\tfrac{1}{n}\\sum_{i=1}^{n} \\omega_{i}$.\n\n1. Starting from the structural properties of $D$ and $L$ stated above, derive necessary and sufficient conditions on $\\bar{\\omega}$ that ensure $M$ is SPD. Justify each step from first principles and do not assume properties beyond the strict lower triangularity of $L$ and positivity of the diagonal of $D$.\n\n2. To analyze anisotropy, use the $2 \\times 2$ model\n$$\nA \\;=\\; \\begin{pmatrix} d_{1} & -\\ell \\\\ -\\ell & d_{2} \\end{pmatrix},\n$$\nwith $d_{1} > 0$, $d_{2} > 0$, and $0 < \\ell^{2} < d_{1} d_{2}$. Take the same splitting $D = \\operatorname{diag}(d_{1}, d_{2})$, $L = \\begin{pmatrix} 0 & 0 \\\\ \\ell & 0 \\end{pmatrix}$, and choose $\\Omega = \\operatorname{diag}(\\omega_{1}, \\omega_{2})$ with $\\bar{\\omega} = (\\omega_{1} + \\omega_{2})/2$. Express the eigenvalues of the preconditioned operator $M^{-1}A$ as the generalized eigenvalues of the pair $(A, C)$ with $C = (D + \\Omega L)D^{-1}(D + \\Omega L)^{\\top}$, and show that for this model they are the roots of a quadratic polynomial whose coefficients depend on the dimensionless anisotropy parameter $\\theta = \\ell^{2}/(d_{1} d_{2}) \\in (0,1)$ and on $\\omega_{2}$.\n\nOver the admissible range $0 \\leq \\omega_{2} < 2$, determine the choice of $\\omega_{2}$ that minimizes the $2$-norm condition number $\\kappa_{2}(M^{-1}A)$ and give the resulting minimal condition number as a closed-form analytic expression in $\\theta$. Provide your final answer only as that single closed-form expression. No rounding is required.",
            "solution": "The user has provided a two-part problem concerning the properties of a variable-relaxation symmetric successive over-relaxation (SSOR) preconditioner.\n\n### Part 1: Condition for the Preconditioner to be Symmetric Positive Definite (SPD)\n\nThe problem defines the preconditioner $M$ as\n$$\nM = \\frac{1}{2 - \\bar{\\omega}} (D + \\Omega L) D^{-1} (D + \\Omega L)^{\\top}\n$$\nwhere $A = D - L - L^{\\top}$ is an SPD matrix, $D$ is a diagonal matrix with positive entries, $L$ is a strictly lower triangular matrix, $\\Omega$ is a diagonal matrix of relaxation parameters $\\omega_i$, and $\\bar{\\omega}$ is the arithmetic mean of the $\\omega_i$. We need to find necessary and sufficient conditions on $\\bar{\\omega}$ for $M$ to be SPD.\n\nA matrix is SPD if it is symmetric and all its eigenvalues are positive, or equivalently, if it is symmetric and for any non-zero vector $\\mathbf{x} \\in \\mathbb{R}^n$, the quadratic form $\\mathbf{x}^{\\top} M \\mathbf{x}$ is positive.\n\nFirst, let's examine the symmetry of $M$. Let the scalar prefactor be $k = \\frac{1}{2 - \\bar{\\omega}}$ and the matrix part be $C = (D + \\Omega L) D^{-1} (D + \\Omega L)^{\\top}$. The matrix $M$ is symmetric if and only if $C$ is symmetric. Let's compute the transpose of $C$:\n$$\nC^{\\top} = \\left( (D + \\Omega L) D^{-1} (D + \\Omega L)^{\\top} \\right)^{\\top}\n$$\nUsing the property $(XYZ)^{\\top} = Z^{\\top}Y^{\\top}X^{\\top}$, we get\n$$\nC^{\\top} = \\left( (D + \\Omega L)^{\\top} \\right)^{\\top} (D^{-1})^{\\top} (D + \\Omega L)^{\\top} = (D + \\Omega L) (D^{-1})^{\\top} (D + \\Omega L)^{\\top}\n$$\nSince $D$ is a diagonal matrix, its inverse $D^{-1}$ is also diagonal and therefore symmetric, so $(D^{-1})^{\\top} = D^{-1}$. Substituting this back gives\n$$\nC^{\\top} = (D + \\Omega L) D^{-1} (D + \\Omega L)^{\\top} = C\n$$\nThus, the matrix $C$ is symmetric, which implies $M = kC$ is also symmetric for any scalar $k$.\n\nNext, we establish the condition for $M$ to be positive definite. For any non-zero vector $\\mathbf{x} \\in \\mathbb{R}^n$, we examine the quadratic form:\n$$\n\\mathbf{x}^{\\top} M \\mathbf{x} = \\mathbf{x}^{\\top} \\left( \\frac{1}{2 - \\bar{\\omega}} C \\right) \\mathbf{x} = \\frac{1}{2 - \\bar{\\omega}} \\mathbf{x}^{\\top} C \\mathbf{x}\n$$\nLet's analyze the term $\\mathbf{x}^{\\top} C \\mathbf{x}$:\n$$\n\\mathbf{x}^{\\top} C \\mathbf{x} = \\mathbf{x}^{\\top} (D + \\Omega L) D^{-1} (D + \\Omega L)^{\\top} \\mathbf{x}\n$$\nLet us define a vector $\\mathbf{y} = (D + \\Omega L)^{\\top} \\mathbf{x}$. The quadratic form becomes:\n$$\n\\mathbf{x}^{\\top} C \\mathbf{x} = \\mathbf{y}^{\\top} D^{-1} \\mathbf{y}\n$$\nThe problem states that $D$ is a diagonal matrix with positive entries, $d_{ii} > 0$. This means $D$ is a positive definite matrix. Consequently, its inverse $D^{-1}$ is also a diagonal matrix with positive entries $1/d_{ii} > 0$, and is therefore also positive definite. For any non-zero vector $\\mathbf{y}$, we must have $\\mathbf{y}^{\\top} D^{-1} \\mathbf{y} > 0$.\n\nNow we need to ensure that $\\mathbf{y}$ is non-zero whenever $\\mathbf{x}$ is non-zero. The vector $\\mathbf{y}$ is defined as $\\mathbf{y} = (D + \\Omega L)^{\\top} \\mathbf{x}$. The matrix $(D + \\Omega L)^{\\top}$ is $D^{\\top} + (\\Omega L)^{\\top} = D + L^{\\top}\\Omega^{\\top}$. Since $D$ is diagonal and $\\Omega$ is diagonal, they are symmetric ($D=D^\\top, \\Omega=\\Omega^\\top$), so this is $D + L^{\\top}\\Omega$. $D$ is diagonal and $L$ is strictly lower triangular, which means $L^{\\top}$ is strictly upper triangular. The matrix $D + L^{\\top}\\Omega$ is therefore an upper triangular matrix. Its diagonal entries are the diagonal entries of $D$, which are all positive. The determinant of a triangular matrix is the product of its diagonal entries, so $\\det(D + L^{\\top}\\Omega) = \\prod_{i=1}^n d_{ii} > 0$. Since the determinant is non-zero, the matrix is invertible.\nThus, if $\\mathbf{x} \\neq \\mathbf{0}$, then $\\mathbf{y} = (D + \\Omega L)^{\\top} \\mathbf{x} \\neq \\mathbf{0}$.\n\nThis establishes that $\\mathbf{y}^{\\top} D^{-1} \\mathbf{y} > 0$ for all $\\mathbf{x} \\neq \\mathbf{0}$. Therefore, the matrix $C$ is positive definite.\n\nSince $M = \\frac{1}{2 - \\bar{\\omega}} C$ and $C$ is positive definite, the positive definiteness of $M$ depends solely on the sign of the scalar prefactor $\\frac{1}{2 - \\bar{\\omega}}$. For $M$ to be positive definite, this scalar must be positive:\n$$\n\\frac{1}{2 - \\bar{\\omega}} > 0\n$$\nThis inequality holds if and only if the denominator is positive:\n$$\n2 - \\bar{\\omega} > 0 \\quad\\implies\\quad \\bar{\\omega} < 2\n$$\nThis is the necessary and sufficient condition on $\\bar{\\omega}$ for $M$ to be SPD.\n\n### Part 2: Analysis of the 2x2 Model\n\nThe problem gives the $2 \\times 2$ matrix $A = \\begin{pmatrix} d_{1} & -\\ell \\\\ -\\ell & d_{2} \\end{pmatrix}$, with $d_{1} > 0$, $d_{2} > 0$, and $0 < \\ell^2 < d_{1} d_{2}$. The splitting is $D = \\operatorname{diag}(d_{1}, d_{2})$ and $L = \\begin{pmatrix} 0 & 0 \\\\ \\ell & 0 \\end{pmatrix}$. The relaxation matrix is $\\Omega = \\operatorname{diag}(\\omega_{1}, \\omega_{2})$.\n\nWe first compute the matrix $C = (D + \\Omega L)D^{-1}(D + \\Omega L)^{\\top}$.\n$$\nD + \\Omega L = \\begin{pmatrix} d_{1} & 0 \\\\ 0 & d_{2} \\end{pmatrix} + \\begin{pmatrix} \\omega_{1} & 0 \\\\ 0 & \\omega_{2} \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ \\ell & 0 \\end{pmatrix} = \\begin{pmatrix} d_{1} & 0 \\\\ 0 & d_{2} \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ \\omega_{2}\\ell & 0 \\end{pmatrix} = \\begin{pmatrix} d_{1} & 0 \\\\ \\omega_{2}\\ell & d_{2} \\end{pmatrix}\n$$\nNote that this matrix does not depend on $\\omega_1$. Now, we compute $C$:\n$$\n\\begin{align*} C &= \\begin{pmatrix} d_{1} & 0 \\\\ \\omega_{2}\\ell & d_{2} \\end{pmatrix} \\begin{pmatrix} 1/d_{1} & 0 \\\\ 0 & 1/d_{2} \\end{pmatrix} \\begin{pmatrix} d_{1} & \\omega_{2}\\ell \\\\ 0 & d_{2} \\end{pmatrix} \\\\ &= \\begin{pmatrix} 1 & 0 \\\\ \\omega_{2}\\ell/d_{1} & 1 \\end{pmatrix} \\begin{pmatrix} d_{1} & \\omega_{2}\\ell \\\\ 0 & d_{2} \\end{pmatrix} \\\\ &= \\begin{pmatrix} d_{1} & \\omega_{2}\\ell \\\\ \\omega_{2}\\ell & \\frac{(\\omega_{2}\\ell)^2}{d_{1}} + d_{2} \\end{pmatrix} = \\begin{pmatrix} d_{1} & \\omega_{2}\\ell \\\\ \\omega_{2}\\ell & d_{2}\\left(1 + \\frac{\\omega_{2}^2\\ell^2}{d_{1}d_{2}}\\right) \\end{pmatrix}\\end{align*}\n$$\nUsing the dimensionless parameter $\\theta = \\ell^2 / (d_{1}d_{2})$, we get\n$$\nC = \\begin{pmatrix} d_{1} & \\omega_{2}\\ell \\\\ \\omega_{2}\\ell & d_{2}(1 + \\omega_{2}^2\\theta) \\end{pmatrix}\n$$\nThe eigenvalues of the preconditioned operator $M^{-1}A$ are $\\lambda$. The eigenvalue problem is $A\\mathbf{x} = \\lambda M \\mathbf{x}$. Substituting the expression for $M$, we have $A\\mathbf{x} = \\lambda \\frac{1}{2 - \\bar{\\omega}} C \\mathbf{x}$. This can be written as $(2 - \\bar{\\omega}) A\\mathbf{x} = \\lambda C \\mathbf{x}$. The generalized eigenvalues $\\mu$ of the pair $(A, C)$ are solutions to $A\\mathbf{x} = \\mu C \\mathbf{x}$. Thus, the eigenvalues of $M^{-1}A$ are $\\lambda = (2 - \\bar{\\omega})\\mu$.\n\nThe condition number $\\kappa_{2}(M^{-1}A) = \\frac{\\lambda_{max}}{\\lambda_{min}} = \\frac{(2 - \\bar{\\omega})\\mu_{max}}{(2 - \\bar{\\omega})\\mu_{min}} = \\frac{\\mu_{max}}{\\mu_{min}}$, which is the condition number of the generalized eigenvalue problem for $(A, C)$. The characteristic polynomial for $\\mu$ is $\\det(A - \\mu C) = 0$.\n$$\n\\det\\left( \\begin{pmatrix} d_{1} & -\\ell \\\\ -\\ell & d_{2} \\end{pmatrix} - \\mu \\begin{pmatrix} d_{1} & \\omega_{2}\\ell \\\\ \\omega_{2}\\ell & d_{2}(1 + \\omega_{2}^2\\theta) \\end{pmatrix} \\right) = 0\n$$\n$$\n\\det\\begin{pmatrix} d_{1}(1-\\mu) & -\\ell - \\mu\\omega_{2}\\ell \\\\ -\\ell - \\mu\\omega_{2}\\ell & d_{2} - \\mu d_{2}(1 + \\omega_{2}^2\\theta) \\end{pmatrix} = 0\n$$\n$$\nd_{1}d_{2}(1-\\mu)(1 - \\mu(1 + \\omega_{2}^2\\theta)) - \\ell^2(1 + \\mu\\omega_{2})^2 = 0\n$$\nDividing by $d_{1}d_{2}$ and using $\\theta = \\ell^2/(d_{1}d_{2})$:\n$$\n(1-\\mu)(1 - \\mu - \\mu\\omega_{2}^2\\theta) - \\theta(1 + 2\\mu\\omega_{2} + \\mu^2\\omega_{2}^2) = 0\n$$\nExpanding and collecting terms in powers of $\\mu$:\n$$\n1 - \\mu - \\mu\\omega_{2}^2\\theta - \\mu + \\mu^2 + \\mu^2\\omega_{2}^2\\theta - \\theta - 2\\mu\\theta\\omega_{2} - \\mu^2\\theta\\omega_{2}^2 = 0\n$$\n$$\n\\mu^2(1 + \\omega_{2}^2\\theta - \\theta\\omega_{2}^2) - \\mu(2 + \\omega_{2}^2\\theta + 2\\theta\\omega_{2}) + (1-\\theta) = 0\n$$\n$$\n\\mu^2 - \\mu(2 + 2\\theta\\omega_{2} + \\theta\\omega_{2}^2) + (1-\\theta) = 0\n$$\nThis is the quadratic polynomial for the generalized eigenvalues $\\mu$. Let the roots be $\\mu_{1}$ and $\\mu_{2}$. The condition number to minimize is $\\kappa = \\mu_{max}/\\mu_{min}$.\nFrom the polynomial, the sum of the roots is $S = \\mu_{1} + \\mu_{2} = 2 + 2\\theta\\omega_{2} + \\theta\\omega_{2}^2$, and the product of the roots is $P = \\mu_{1}\\mu_{2} = 1-\\theta$. Since $\\theta \\in (0,1)$, $P > 0$. Since $S > 0$ for $\\omega_2 \\ge 0$, both roots are positive.\n\nThe condition number $\\kappa = \\mu_{max}/\\mu_{min}$ is minimized when the roots are as close as possible. We can express $\\kappa$ as a function of $S$ and $P$. Let $\\mu_{min}, \\mu_{max} = (S \\mp \\sqrt{S^2-4P})/2$. The ratio is $\\kappa = \\frac{S + \\sqrt{S^2-4P}}{S - \\sqrt{S^2-4P}}$.\nTo minimize $\\kappa$ (where $\\kappa \\ge 1$), we need to minimize the term $S = S(\\omega_2)$, since $P$ is constant with respect to $\\omega_2$.\nWe need to find the minimum of $S(\\omega_2) = 2 + 2\\theta\\omega_{2} + \\theta\\omega_{2}^2$ on the interval $0 \\le \\omega_2 < 2$.\nTo find the minimum, we compute the derivative with respect to $\\omega_2$:\n$$\n\\frac{dS}{d\\omega_2} = 2\\theta + 2\\theta\\omega_2 = 2\\theta(1+\\omega_2)\n$$\nGiven that $\\theta \\in (0,1)$ and $\\omega_2 \\ge 0$, the derivative $\\frac{dS}{d\\omega_2}$ is always positive. This means that $S(\\omega_2)$ is a strictly increasing function of $\\omega_2$ on its domain $[0, 2)$. The minimum value of $S(\\omega_2)$ is therefore attained at the lower boundary of the interval, i.e., at $\\omega_2 = 0$.\n\nTo find the minimal condition number, we set $\\omega_2 = 0$ in the characteristic polynomial for $\\mu$:\n$$\n\\mu^2 - \\mu(2) + (1-\\theta) = 0\n$$\nUsing the quadratic formula, the roots are:\n$$\n\\mu = \\frac{2 \\pm \\sqrt{4 - 4(1-\\theta)}}{2} = 1 \\pm \\sqrt{1 - (1-\\theta)} = 1 \\pm \\sqrt{\\theta}\n$$\nSo, $\\mu_{max} = 1 + \\sqrt{\\theta}$ and $\\mu_{min} = 1 - \\sqrt{\\theta}$.\nThe minimal condition number is the ratio of these eigenvalues:\n$$\n\\kappa_{min} = \\frac{\\mu_{max}}{\\mu_{min}} = \\frac{1 + \\sqrt{\\theta}}{1 - \\sqrt{\\theta}}\n$$",
            "answer": "$$\\boxed{\\frac{1 + \\sqrt{\\theta}}{1 - \\sqrt{\\theta}}}$$"
        },
        {
            "introduction": "The most effective preconditioners are often those whose structure is deliberately designed to approximate the original matrix in a specific way. This exercise takes you a step beyond standard SSOR by inviting you to modify the underlying matrix splitting itself, a technique known as \"lumping\" . By finding the optimal balance between sparsity and accuracy for a model problem, you will explore a fundamental principle in preconditioner design: that the choice of splitting is as critical as the choice of relaxation parameters.",
            "id": "3583769",
            "problem": "Consider the symmetric positive definite matrix $A \\in \\mathbb{R}^{2 \\times 2}$ given by\n$$\nA \\;=\\; \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}.\n$$\nYou are given a sparsity pattern that allows exactly one strictly lower-triangular nonzero in position $(2,1)$ and its symmetric counterpart in the upper triangle. Construct a one-parameter family of preconditioning splittings with a lumped diagonal as follows: for a parameter $\\theta \\in [0,1]$, define the auxiliary symmetric matrix\n$$\n\\tilde{A}(\\theta) \\;=\\; \\tilde{D}(\\theta) \\,+\\, \\tilde{L}(\\theta) \\,+\\, \\tilde{L}(\\theta)^{\\top},\n$$\nwhere\n$$\n\\tilde{L}(\\theta) \\;=\\; \\begin{pmatrix} 0 & 0 \\\\ -\\theta & 0 \\end{pmatrix}, \n\\qquad\n\\tilde{D}(\\theta) \\;=\\; \\begin{pmatrix} 2 + (1-\\theta) & 0 \\\\ 0 & 2 + (1-\\theta) \\end{pmatrix}.\n$$\nIn words, you retain a fraction $\\theta$ of the off-diagonal magnitude in the strictly lower triangular part, and lump the dropped portion $1-\\theta$ into each corresponding diagonal entry. Using the splitting $(\\tilde{D}(\\theta),\\tilde{L}(\\theta))$, form the symmetric successive over-relaxation (SSOR) preconditioner at relaxation parameter $\\omega = 1$, i.e., the symmetric Gauss–Seidel preconditioner built from $\\tilde{A}(\\theta)$.\n\nLet $M_{\\text{SSOR}}(\\theta)$ denote this preconditioner, and consider the preconditioned operator $M_{\\text{SSOR}}(\\theta)^{-1} A$. Using only fundamental principles (definitions of Rayleigh quotients for generalized eigenvalue problems and basic linear algebra), derive a closed-form expression for a rigorous lower bound on the smallest eigenvalue $\\lambda_{\\min}(M_{\\text{SSOR}}(\\theta)^{-1} A)$, and then optimize this bound over $\\theta \\in [0,1]$ under the given sparsity constraint. Your final task is to determine the single value of $\\theta$ that maximizes this lower bound. Provide only this maximizing $\\theta$ as your final answer. No rounding is required.",
            "solution": "We start from the core definitions. For any symmetric positive definite (SPD) pair $(A,M)$, the eigenvalues of the preconditioned operator $M^{-1}A$ are the generalized eigenvalues $\\lambda$ solving $A x = \\lambda M x$, and they admit the min–max characterization via the generalized Rayleigh quotient:\n$$\n\\lambda_{\\min}(M^{-1}A) \\;=\\; \\min_{x \\neq 0} \\frac{x^{\\top} A x}{x^{\\top} M x}.\n$$\nThus, any explicit evaluation of $\\lambda_{\\min}(M^{-1}A)$ yields a rigorous lower bound on itself, and in fact, the exact value.\n\nStep $1$: Build the symmetric Gauss–Seidel (SSOR with $\\omega = 1$) preconditioner $M_{\\text{SSOR}}(\\theta)$ from the splitting $\\tilde{A}(\\theta) = \\tilde{D}(\\theta) + \\tilde{L}(\\theta) + \\tilde{L}(\\theta)^{\\top}$. With the sign convention $\\tilde{A} = \\tilde{D} + \\tilde{L} + \\tilde{L}^{\\top}$, the symmetric Gauss–Seidel preconditioner is\n$$\nM_{\\text{SSOR}}(\\theta) \\;=\\; \\left(\\tilde{D}(\\theta) + \\tilde{L}(\\theta)\\right)\\, \\tilde{D}(\\theta)^{-1}\\, \\left(\\tilde{D}(\\theta) + \\tilde{L}(\\theta)^{\\top}\\right).\n$$\nGiven\n$$\n\\tilde{D}(\\theta) \\;=\\; \\begin{pmatrix} 2 + (1-\\theta) & 0 \\\\ 0 & 2 + (1-\\theta) \\end{pmatrix} \\;=\\; d(\\theta)\\, I, \n\\quad \\text{with}\\quad d(\\theta) \\;=\\; 3 - \\theta,\n$$\nand\n$$\n\\tilde{L}(\\theta) \\;=\\; \\begin{pmatrix} 0 & 0 \\\\ -\\theta & 0 \\end{pmatrix},\n\\quad\n\\tilde{L}(\\theta)^{\\top} \\;=\\; \\begin{pmatrix} 0 & -\\theta \\\\ 0 & 0 \\end{pmatrix},\n$$\nwe compute\n$$\n\\tilde{D}(\\theta) + \\tilde{L}(\\theta) \\;=\\; \\begin{pmatrix} d(\\theta) & 0 \\\\ -\\theta & d(\\theta) \\end{pmatrix},\n\\qquad\n\\tilde{D}(\\theta) + \\tilde{L}(\\theta)^{\\top} \\;=\\; \\begin{pmatrix} d(\\theta) & -\\theta \\\\ 0 & d(\\theta) \\end{pmatrix}.\n$$\nHence\n$$\nM_{\\text{SSOR}}(\\theta) \\;=\\; \\frac{1}{d(\\theta)}\\, \\big(\\tilde{D}(\\theta) + \\tilde{L}(\\theta)\\big)\\, \\big(\\tilde{D}(\\theta) + \\tilde{L}(\\theta)^{\\top}\\big)\n\\;=\\;\n\\frac{1}{d(\\theta)}\\,\n\\begin{pmatrix}\nd(\\theta) & 0 \\\\ -\\theta & d(\\theta)\n\\end{pmatrix}\n\\begin{pmatrix}\nd(\\theta) & -\\theta \\\\ 0 & d(\\theta)\n\\end{pmatrix}.\n$$\nMultiplying the two factors gives\n$$\n\\big(\\tilde{D} + \\tilde{L}\\big)\\big(\\tilde{D} + \\tilde{L}^{\\top}\\big)\n\\;=\\;\n\\begin{pmatrix}\nd(\\theta)^{2} & - d(\\theta)\\, \\theta \\\\\n- d(\\theta)\\, \\theta & d(\\theta)^{2} + \\theta^{2}\n\\end{pmatrix}.\n$$\nTherefore\n$$\nM_{\\text{SSOR}}(\\theta) \\;=\\;\n\\frac{1}{d(\\theta)}\n\\begin{pmatrix}\nd(\\theta)^{2} & - d(\\theta)\\, \\theta \\\\\n- d(\\theta)\\, \\theta & d(\\theta)^{2} + \\theta^{2}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\nd(\\theta) & -\\theta \\\\\n-\\theta & d(\\theta) + \\frac{\\theta^{2}}{d(\\theta)}\n\\end{pmatrix}.\n$$\n\nStep $2$: Reduce the generalized eigenvalue problem $A x = \\lambda M_{\\text{SSOR}}(\\theta) x$ to a scalar quadratic equation and extract invariants. Because both $A$ and $M_{\\text{SSOR}}(\\theta)$ are $2 \\times 2$ and symmetric positive definite for $\\theta \\in [0,1]$ (note $d(\\theta) \\in [2,3]$), the eigenvalues of $M_{\\text{SSOR}}(\\theta)^{-1} A$ can be characterized by the trace and determinant:\n$$\n\\lambda_{1}(\\theta) + \\lambda_{2}(\\theta) \\;=\\; \\operatorname{tr}\\!\\big(M_{\\text{SSOR}}(\\theta)^{-1} A\\big),\n\\qquad\n\\lambda_{1}(\\theta)\\, \\lambda_{2}(\\theta) \\;=\\; \\det\\!\\big(M_{\\text{SSOR}}(\\theta)^{-1} A\\big).\n$$\nFirst, $\\det\\!\\big(M_{\\text{SSOR}}(\\theta)^{-1} A\\big) = \\det(A)/\\det(M_{\\text{SSOR}}(\\theta))$. We have $\\det(A) = 3$. Also,\n$$\n\\det\\!\\big(M_{\\text{SSOR}}(\\theta)\\big)\n\\;=\\; \\det\\!\\left(\\frac{1}{d(\\theta)}\\big(\\tilde{D} + \\tilde{L}\\big)\\big(\\tilde{D} + \\tilde{L}^{\\top}\\big)\\right)\n\\;=\\; \\frac{1}{d(\\theta)^{2}}\\, \\det\\!\\big(\\tilde{D} + \\tilde{L}\\big)\\, \\det\\!\\big(\\tilde{D} + \\tilde{L}^{\\top}\\big).\n$$\nBut $\\det(\\tilde{D} + \\tilde{L}) = \\det(\\tilde{D} + \\tilde{L}^{\\top}) = d(\\theta)^{2}$, so $\\det\\!\\big(M_{\\text{SSOR}}(\\theta)\\big) = d(\\theta)^{2}$. Hence\n$$\n\\lambda_{1}(\\theta)\\, \\lambda_{2}(\\theta) \\;=\\; \\frac{3}{d(\\theta)^{2}}.\n$$\nNext, compute $\\operatorname{tr}\\!\\big(M_{\\text{SSOR}}(\\theta)^{-1} A\\big)$ directly. An efficient route is to form $M_{\\text{SSOR}}(\\theta)^{-1}$ via the $2 \\times 2$ inverse formula. For\n$$\nM_{\\text{SSOR}}(\\theta) \\;=\\; \\begin{pmatrix} d & -\\theta \\\\ -\\theta & d + \\theta^{2}/d \\end{pmatrix},\n$$\nits determinant is $d(d + \\theta^{2}/d) - \\theta^{2} = d^{2}$, so\n$$\nM_{\\text{SSOR}}(\\theta)^{-1}\n\\;=\\; \\frac{1}{d^{2}}\\,\n\\begin{pmatrix}\nd + \\theta^{2}/d & \\theta \\\\\n\\theta & d\n\\end{pmatrix}.\n$$\nThen\n$$\nM_{\\text{SSOR}}(\\theta)^{-1} A \\;=\\;\n\\frac{1}{d^{2}}\\,\n\\begin{pmatrix}\nd + \\theta^{2}/d & \\theta \\\\\n\\theta & d\n\\end{pmatrix}\n\\begin{pmatrix}\n2 & -1 \\\\\n-1 & 2\n\\end{pmatrix}.\n$$\nThe trace is\n$$\n\\operatorname{tr}\\!\\big(M_{\\text{SSOR}}(\\theta)^{-1} A\\big)\n\\;=\\; \\frac{1}{d^{2}} \\left(2\\big(d + \\theta^{2}/d\\big) - \\theta \\;+\\; -\\theta + 2 d \\right)\n\\;=\\; \\frac{1}{d^{2}}\\left(4 d - 2 \\theta + 2 \\frac{\\theta^{2}}{d}\\right).\n$$\nIt is convenient to set $t = 1/d \\in [1/3,\\, 1/2]$, so\n$$\n\\operatorname{tr}\\!\\big(M_{\\text{SSOR}}(\\theta)^{-1} A\\big)\n\\;=\\; 8 t - 18 t^{2} + 18 t^{3},\n\\qquad\n\\lambda_{1}(\\theta)\\, \\lambda_{2}(\\theta) \\;=\\; 3 t^{2}.\n$$\nTherefore, the minimal eigenvalue is\n$$\n\\lambda_{\\min}(\\theta)\n\\;=\\;\n\\frac{1}{2}\\left( \\operatorname{tr} - \\sqrt{\\operatorname{tr}^{2} - 4 \\det} \\right)\n\\;=\\;\n\\frac{1}{2}\\left( s(t) - \\sqrt{s(t)^{2} - 12 t^{2}} \\right),\n\\quad\ns(t) \\;=\\; 8 t - 18 t^{2} + 18 t^{3}.\n$$\nThis is a closed-form expression, hence a rigorous exact lower bound for $\\lambda_{\\min}(M_{\\text{SSOR}}(\\theta)^{-1} A)$.\n\nStep $3$: Optimize over $\\theta \\in [0,1]$. Since $d(\\theta) = 3 - \\theta$ and $t = 1/d(\\theta)$, optimizing over $\\theta \\in [0,1]$ is equivalent to optimizing over $t \\in [1/3, 1/2]$. Define\n$$\ng(t) \\;=\\; \\lambda_{\\min}(\\theta(t)) \\;=\\; \\frac{1}{2}\\left( s(t) - \\sqrt{s(t)^{2} - 12 t^{2}} \\right),\n\\quad s(t) \\;=\\; 8 t - 18 t^{2} + 18 t^{3}.\n$$\nDifferentiate $g$ to assess monotonicity. Using the chain rule,\n$$\ng'(t) \\;=\\; \\frac{1}{2}\\left( s'(t) - \\frac{2 s(t) s'(t) - 24 t}{2 \\sqrt{s(t)^{2} - 12 t^{2}}} \\right)\n\\;=\\; \\frac{- s'(t)\\, g(t) + 6 t}{\\sqrt{s(t)^{2} - 12 t^{2}}},\n$$\nwhere $s'(t) = 8 - 36 t + 54 t^{2}$. Thus $g'(t) \\ge 0$ if and only if $6 t \\ge s'(t)\\, g(t)$. To establish this, use the Rayleigh quotient upper bound (valid for any nonzero $x$):\n$$\n\\lambda_{\\min}(M^{-1}A) \\;=\\; \\min_{y \\neq 0} \\frac{y^{\\top} A y}{y^{\\top} M y} \\;\\le\\; \\frac{x^{\\top} A x}{x^{\\top} M x}.\n$$\nSelecting $x = (1,1)^{\\top}$ and using the explicit $M_{\\text{SSOR}}(\\theta)$ gives\n$$\n\\frac{x^{\\top} A x}{x^{\\top} M_{\\text{SSOR}}(\\theta) x}\n\\;=\\; \\frac{2}{\\,5 d(\\theta) - 12 + 9/d(\\theta)\\,}\n\\;=\\; \\frac{2 t}{\\,5 - 12 t + 9 t^{2}\\,}.\n$$\nTherefore,\n$$\ng(t) \\;\\le\\; \\frac{2 t}{\\,5 - 12 t + 9 t^{2}\\,}.\n$$\nIt follows that\n$$\ns'(t)\\, g(t) \\;\\le\\; s'(t)\\, \\frac{2 t}{\\,5 - 12 t + 9 t^{2}\\,}.\n$$\nHence $6 t \\ge s'(t)\\, g(t)$ is implied by\n$$\n6 t \\;\\ge\\; s'(t)\\, \\frac{2 t}{\\,5 - 12 t + 9 t^{2}\\,}\n\\quad\\Longleftrightarrow\\quad\ns'(t) \\;\\le\\; 3\\big(5 - 12 t + 9 t^{2}\\big).\n$$\nSince $s'(t) = 8 - 36 t + 54 t^{2}$, the inequality becomes\n$$\n8 - 36 t + 54 t^{2} \\;\\le\\; 15 - 36 t + 27 t^{2}\n\\quad\\Longleftrightarrow\\quad\n27 t^{2} \\;\\le\\; 7\n\\quad\\Longleftrightarrow\\quad\nt^{2} \\;\\le\\; \\frac{7}{27}.\n$$\nOn the feasible interval $t \\in [1/3, 1/2]$, we have $t^{2} \\le (1/2)^{2} = 1/4 < 7/27$, so the inequality holds throughout. Consequently $g'(t) \\ge 0$ for all $t \\in [1/3, 1/2]$, i.e., $g$ is nondecreasing in $t$ on this interval.\n\nSince $t = 1/d(\\theta) = 1/(3 - \\theta)$ is strictly increasing in $\\theta$, it follows that $g$ is strictly increasing in $\\theta$ on $[0,1]$. Therefore, the maximizing value is attained at the right endpoint $\\theta = 1$.\n\nStep $4$: Report the optimizer. The unique maximizer of the rigorous lower bound $\\lambda_{\\min}(M_{\\text{SSOR}}(\\theta)^{-1} A)$ over $\\theta \\in [0,1]$ is $\\theta^{\\star} = 1$.",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}