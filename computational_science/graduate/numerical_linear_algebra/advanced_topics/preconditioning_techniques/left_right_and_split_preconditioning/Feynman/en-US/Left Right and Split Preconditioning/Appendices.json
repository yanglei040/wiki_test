{
    "hands_on_practices": [
        {
            "introduction": "The choice between left, right, and split preconditioning extends beyond mere algebraic rearrangement; it fundamentally alters how an iterative method perceives and reports convergence. This exercise  challenges you to dissect these differences by analyzing the relationship between the true residual, $r = b - A x$, and the internally monitored residual of the preconditioned system. By working through a concrete $2 \\times 2$ example, you will develop the critical skill of setting appropriate stopping criteria to guarantee a desired level of accuracy for each preconditioning strategy.",
            "id": "3555570",
            "problem": "Consider the linear system $A x = b$ with\n$$\nA = \\begin{bmatrix}4 & 1 \\\\ 2 & 3\\end{bmatrix}, \\quad b = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n$$\nand a classical triangular split of $A$ into diagonal, strictly lower, and strictly upper parts, $A = D + L + U$, where\n$$\nD = \\begin{bmatrix}4 & 0 \\\\ 0 & 3\\end{bmatrix}, \\quad L = \\begin{bmatrix}0 & 0 \\\\ 2 & 0\\end{bmatrix}, \\quad U = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}.\n$$\nDefine the split preconditioners $M_L = D + L = \\begin{bmatrix}4 & 0 \\\\ 2 & 3\\end{bmatrix}$ and $M_R = D + U = \\begin{bmatrix}4 & 1 \\\\ 0 & 3\\end{bmatrix}$. Consider three preconditioning strategies for $A x = b$:\n(1) left preconditioning with $M_L$, which uses the system $M_L^{-1} A x = M_L^{-1} b$,\n(2) right preconditioning with $M_R$, which uses the system $A M_R^{-1} y = b$ and sets $x = M_R^{-1} y$,\nand (3) split preconditioning, which uses the system $M_L^{-1} A M_R^{-1} z = M_L^{-1} b$ and sets $x = M_R^{-1} z$.\n\nLet $\\|\\cdot\\|_{\\infty}$ denote the vector infinity norm and the induced matrix infinity norm. Suppose one runs a Krylov subspace iterative method, such as the Generalized Minimal Residual (GMRES) method, in each preconditioning regime. In left and split preconditioning, the method’s internally monitored residual is that of the preconditioned system, whereas in right preconditioning it is the residual with respect to the original $A x = b$ system. Two accuracy goals are considered:\n(i) an absolute backward-error goal $\\|r\\|_{\\infty} \\le \\tau$ with $\\tau = 10^{-6}$, where $r = b - A x$ is the true (unpreconditioned) residual,\nand\n(ii) a relative backward-error goal $\\|r\\|_{\\infty} / \\|b\\|_{\\infty} \\le \\varepsilon$ with $\\varepsilon = 10^{-6}$.\n\nAnalyze how stopping criteria in the internally monitored residual should be chosen in each strategy to achieve these goals and how to correctly recover the physical solution $x$ from the preconditioned variables. Then, select all statements below that are valid for this test problem and these goals.\n\nA. In right preconditioning with $M_R$, the method’s residual equals the true residual $r = b - A x$, so requiring the internally monitored $\\|r\\|_{\\infty} \\le 10^{-6}$ directly enforces the absolute goal; moreover, the physical solution is recovered by $x = M_R^{-1} y$.\n\nB. In left preconditioning with $M_L$, ensuring the internally monitored preconditioned residual $\\|\\hat{r}\\|_{\\infty} \\le 10^{-6}$, where $\\hat{r} = M_L^{-1}(b - A x)$, guarantees $\\|r\\|_{\\infty} \\le 10^{-6}$, so no scaling is needed to meet the absolute goal.\n\nC. In split preconditioning, the internally monitored residual is $\\hat{r} = M_L^{-1}(b - A x)$ in the system $M_L^{-1} A M_R^{-1} z = M_L^{-1} b$, and to ensure $\\|r\\|_{\\infty} \\le \\tau$ it suffices to require $\\|\\hat{r}\\|_{\\infty} \\le \\tau / \\|M_L\\|_{\\infty}$; the physical solution is recovered as $x = M_R^{-1} z$.\n\nD. For the relative backward-error goal $\\|r\\|_{\\infty} / \\|b\\|_{\\infty} \\le \\varepsilon$ under left preconditioning with $M_L$, if the method monitors $\\|\\hat{r}\\|_{\\infty} / \\|\\hat{b}\\|_{\\infty}$ with $\\hat{b} = M_L^{-1} b$, then one should stop when $\\|\\hat{r}\\|_{\\infty} / \\|\\hat{b}\\|_{\\infty} \\le \\varepsilon / \\kappa_{\\infty}(M_L)$, where $\\kappa_{\\infty}(M_L) = \\|M_L\\|_{\\infty} \\|M_L^{-1}\\|_{\\infty}$; for this test problem, $\\kappa_{\\infty}(M_L) = 2.5$.\n\nE. Right preconditioning corrupts absolute backward-error measurement, so it cannot be used when an absolute residual tolerance is specified.\n\nF. In split preconditioning, the physical solution must be recovered via $x = M_L^{-1} z$ rather than $x = M_R^{-1} z$; this is essential to maintain consistency with the monitored residual.\n\nSelect all correct options.",
            "solution": "The user has requested a detailed analysis of a problem concerning preconditioned iterative methods for solving a linear system. The primary task is to validate the problem statement and, if valid, evaluate several claims about different preconditioning strategies.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem provides the following information:\n- The linear system is $A x = b$.\n- The matrix $A$ is given by $A = \\begin{bmatrix}4 & 1 \\\\ 2 & 3\\end{bmatrix}$.\n- The vector $b$ is given by $b = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$.\n- A triangular splitting of $A$ is defined as $A = D + L + U$, where:\n  - $D = \\begin{bmatrix}4 & 0 \\\\ 0 & 3\\end{bmatrix}$ (diagonal part)\n  - $L = \\begin{bmatrix}0 & 0 \\\\ 2 & 0\\end{bmatrix}$ (strictly lower part)\n  - $U = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}$ (strictly upper part)\n- The left preconditioner is $M_L = D + L = \\begin{bmatrix}4 & 0 \\\\ 2 & 3\\end{bmatrix}$.\n- The right preconditioner is $M_R = D + U = \\begin{bmatrix}4 & 1 \\\\ 0 & 3\\end{bmatrix}$.\n- Three preconditioning strategies are considered for a Krylov subspace method:\n  1.  **Left preconditioning**: Solve $M_L^{-1} A x = M_L^{-1} b$. The internally monitored residual is that of this preconditioned system.\n  2.  **Right preconditioning**: Solve $A M_R^{-1} y = b$ and set $x = M_R^{-1} y$. The internally monitored residual is that of the original system, $r = b - A x$.\n  3.  **Split preconditioning**: Solve $M_L^{-1} A M_R^{-1} z = M_L^{-1} b$ and set $x = M_R^{-1} z$. The internally monitored residual is that of this preconditioned system.\n- The norm used is the infinity norm, $\\|\\cdot\\|_{\\infty}$.\n- The true (unpreconditioned) residual is defined as $r = b - A x$.\n- Two accuracy goals for the true residual are considered:\n  - (i) Absolute backward-error: $\\|r\\|_{\\infty} \\le \\tau$, with $\\tau = 10^{-6}$.\n  - (ii) Relative backward-error: $\\|r\\|_{\\infty} / \\|b\\|_{\\infty} \\le \\varepsilon$, with $\\varepsilon = 10^{-6}$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem statement is analyzed against the validation criteria.\n- **Scientifically Grounded**: The problem is set firmly within the established theory of numerical linear algebra. Left, right, and split preconditioning are standard techniques for accelerating Krylov subspace methods like GMRES. The concepts of true and preconditioned residuals, as well as norms and condition numbers, are fundamental to this field.\n- **Well-Posed**: The problem is well-posed. The matrices $A$, $M_L$, and $M_R$ are all nonsingular. The questions asked are about the formal relationships between different quantities, which are mathematically derivable. A unique and meaningful analysis is possible.\n- **Objective**: The problem is stated in precise, objective, and unambiguous mathematical language. All terms are standard or explicitly defined.\n- **Flaw Analysis**:\n  1.  **Scientific/Factual Unsoundness**: None. The matrix splittings and definitions are correct.\n  2.  **Non-Formalizable/Irrelevant**: The problem is directly formalizable and central to the topic of preconditioning.\n  3.  **Incomplete/Contradictory Setup**: The setup is complete and internally consistent. It provides all necessary matrices, definitions, and goals.\n  4.  **Unrealistic/Infeasible**: The problem uses a simple $2 \\times 2$ system, which is a common and valid approach for illustrating theoretical concepts.\n  5.  **Ill-Posed/Poorly Structured**: The structure is clear and logical, leading to a well-defined analysis task.\n  6.  **Pseudo-Profound/Trivial**: The problem requires a careful application of norm inequalities and definitions of preconditioning, testing conceptual understanding rather than being trivial.\n  7.  **Outside Scientific Verifiability**: All claims are mathematically verifiable.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. The analysis can proceed.\n\n### Derivation and Option Analysis\n\nLet $x_k$ be the $k$-th iterate of the solution. The true residual is $r_k = b - A x_k$. We analyze the relationship between the true residual and the monitored residual for each strategy.\n\n**Strategy 1: Left Preconditioning**\nThe system solved is $(M_L^{-1} A) x = M_L^{-1} b$.\nThe iterative method generates iterates $x_k$ and the corresponding monitored (preconditioned) residual is\n$$\n\\hat{r}_k = (M_L^{-1} b) - (M_L^{-1} A) x_k = M_L^{-1} (b - A x_k) = M_L^{-1} r_k.\n$$\nFrom this relationship, we have $r_k = M_L \\hat{r}_k$. Applying norms, we get the bounds:\n$$\n\\|r_k\\|_{\\infty} = \\|M_L \\hat{r}_k\\|_{\\infty} \\le \\|M_L\\|_{\\infty} \\|\\hat{r}_k\\|_{\\infty}\n$$\nand\n$$\n\\|\\hat{r}_k\\|_{\\infty} = \\|M_L^{-1} r_k\\|_{\\infty} \\le \\|M_L^{-1}\\|_{\\infty} \\|r_k\\|_{\\infty}.\n$$\n\n**Strategy 2: Right Preconditioning**\nThe system solved is $(A M_R^{-1}) y = b$. The iterative method generates iterates $y_k$. The physical solution iterate is $x_k = M_R^{-1} y_k$.\nThe problem states that the monitored residual is that of the original system. Let's verify this is consistent. The residual of the transformed system is $b - (A M_R^{-1}) y_k$.\nSubstituting $y_k = M_R x_k$, this becomes $b - (A M_R^{-1}) (M_R x_k) = b - A x_k = r_k$.\nThus, the monitored residual is indeed the true residual, $\\hat{r}_k = r_k$. This is a key advantage of right preconditioning. The solution is recovered via $x = M_R^{-1} y$.\n\n**Strategy 3: Split Preconditioning**\nThe system solved is $(M_L^{-1} A M_R^{-1}) z = M_L^{-1} b$. The iterative method generates iterates $z_k$. The physical solution is recovered by setting $x_k = M_R^{-1} z_k$.\nThe preconditioned residual is\n$$\n\\hat{r}_k = (M_L^{-1} b) - (M_L^{-1} A M_R^{-1}) z_k = M_L^{-1} (b - A (M_R^{-1} z_k)).\n$$\nSubstituting $x_k = M_R^{-1} z_k$, we get\n$$\n\\hat{r}_k = M_L^{-1} (b - A x_k) = M_L^{-1} r_k.\n$$\nThis is the same relationship as in left preconditioning: $r_k = M_L \\hat{r}_k$.\n\nWith these principles established, we evaluate each option.\n\n**A. In right preconditioning with $M_R$, the method’s residual equals the true residual $r = b - A x$, so requiring the internally monitored $\\|r\\|_{\\infty} \\le 10^{-6}$ directly enforces the absolute goal; moreover, the physical solution is recovered by $x = M_R^{-1} y$.**\nOur analysis of right preconditioning shows that the monitored residual is identical to the true residual, $r = b - A x$. Therefore, a stopping criterion based on the monitored residual, $\\|\\hat{r}\\|_{\\infty} \\le \\tau$, directly enforces the goal on the true residual, $\\|r\\|_{\\infty} \\le \\tau$. The variable transformation is correctly stated as $x = M_R^{-1} y$. The statement is entirely accurate.\n**Verdict: Correct.**\n\n**B. In left preconditioning with $M_L$, ensuring the internally monitored preconditioned residual $\\|\\hat{r}\\|_{\\infty} \\le 10^{-6}$, where $\\hat{r} = M_L^{-1}(b - A x)$, guarantees $\\|r\\|_{\\infty} \\le 10^{-6}$, so no scaling is needed to meet the absolute goal.**\nThe relationship between the true and preconditioned residuals is $r = M_L \\hat{r}$. Taking norms, $\\|r\\|_{\\infty} \\le \\|M_L\\|_{\\infty} \\|\\hat{r}\\|_{\\infty}$. The statement implies that $\\|M_L\\|_{\\infty} \\le 1$. Let's calculate $\\|M_L\\|_{\\infty}$.\n$$\nM_L = \\begin{bmatrix}4 & 0 \\\\ 2 & 3\\end{bmatrix}\n$$\nThe infinity norm of a matrix is the maximum absolute row sum.\n- Row 1 sum: $|4| + |0| = 4$.\n- Row 2 sum: $|2| + |3| = 5$.\nThus, $\\|M_L\\|_{\\infty} = 5$. Since $\\|M_L\\|_{\\infty} > 1$, the condition $\\|\\hat{r}\\|_{\\infty} \\le 10^{-6}$ only guarantees $\\|r\\|_{\\infty} \\le 5 \\times 10^{-6}$. It does not guarantee $\\|r\\|_{\\infty} \\le 10^{-6}$. In fact, the true residual could be larger than the monitored residual. Scaling is necessary.\n**Verdict: Incorrect.**\n\n**C. In split preconditioning, the internally monitored residual is $\\hat{r} = M_L^{-1}(b - A x)$ in the system $M_L^{-1} A M_R^{-1} z = M_L^{-1} b$, and to ensure $\\|r\\|_{\\infty} \\le \\tau$ it suffices to require $\\|\\hat{r}\\|_{\\infty} \\le \\tau / \\|M_L\\|_{\\infty}$; the physical solution is recovered as $x = M_R^{-1} z$.**\nThis statement has three parts.\n1.  **Physical solution recovery**: The system being solved is $A'z = b'$ where $A' = M_L^{-1} A M_R^{-1}$ and $b' = M_L^{-1} b$. Left-multiplying by $M_L$ gives $A M_R^{-1} z = b$. Comparing this with the original system $Ax=b$, we see that $x=M_R^{-1}z$. This is correct.\n2.  **Monitored residual**: As derived earlier for split preconditioning, the monitored residual $\\hat{r}$ (for the system in $z$) is related to the true residual $r = b - Ax$ by $\\hat{r} = M_L^{-1} r$. The statement expresses this as $\\hat{r} = M_L^{-1}(b-Ax)$, which is correct.\n3.  **Stopping criterion**: From $r = M_L \\hat{r}$, we have $\\|r\\|_{\\infty} = \\|M_L \\hat{r}\\|_{\\infty} \\le \\|M_L\\|_{\\infty} \\|\\hat{r}\\|_{\\infty}$. To ensure $\\|r\\|_{\\infty} \\le \\tau$, it is sufficient to enforce $\\|M_L\\|_{\\infty} \\|\\hat{r}\\|_{\\infty} \\le \\tau$, which rearranges to $\\|\\hat{r}\\|_{\\infty} \\le \\tau / \\|M_L\\|_{\\infty}$. This part is also correct.\nAll parts of the statement are correct.\n**Verdict: Correct.**\n\n**D. For the relative backward-error goal $\\|r\\|_{\\infty} / \\|b\\|_{\\infty} \\le \\varepsilon$ under left preconditioning with $M_L$, if the method monitors $\\|\\hat{r}\\|_{\\infty} / \\|\\hat{b}\\|_{\\infty}$ with $\\hat{b} = M_L^{-1} b$, then one should stop when $\\|\\hat{r}\\|_{\\infty} / \\|\\hat{b}\\|_{\\infty} \\le \\varepsilon / \\kappa_{\\infty}(M_L)$, where $\\kappa_{\\infty}(M_L) = \\|M_L\\|_{\\infty} \\|M_L^{-1}\\|_{\\infty}$; for this test problem, $\\kappa_{\\infty}(M_L) = 2.5$.**\nLet's analyze the relative error propagation. The true relative residual is $\\|r\\|_{\\infty} / \\|b\\|_{\\infty}$. The monitored relative residual is $\\|\\hat{r}\\|_{\\infty} / \\|\\hat{b}\\|_{\\infty}$.\nWe have the relationships $r = M_L \\hat{r}$ and $b = M_L \\hat{b}$. From these, we get the inequalities:\n$\\|r\\|_{\\infty} \\le \\|M_L\\|_{\\infty} \\|\\hat{r}\\|_{\\infty}$\n$\\|\\hat{b}\\|_{\\infty} = \\|M_L^{-1} b\\|_{\\infty} \\le \\|M_L^{-1}\\|_{\\infty} \\|b\\|_{\\infty} \\implies \\|b\\|_{\\infty} \\ge \\frac{\\|\\hat{b}\\|_{\\infty}}{\\|M_L^{-1}\\|_{\\infty}}$.\nCombining these to bound the true relative error:\n$$\n\\frac{\\|r\\|_{\\infty}}{\\|b\\|_{\\infty}} \\le \\frac{\\|M_L\\|_{\\infty} \\|\\hat{r}\\|_{\\infty}}{\\|\\hat{b}\\|_{\\infty} / \\|M_L^{-1}\\|_{\\infty}} = \\left(\\|M_L\\|_{\\infty} \\|M_L^{-1}\\|_{\\infty}\\right) \\frac{\\|\\hat{r}\\|_{\\infty}}{\\|\\hat{b}\\|_{\\infty}} = \\kappa_{\\infty}(M_L) \\frac{\\|\\hat{r}\\|_{\\infty}}{\\|\\hat{b}\\|_{\\infty}}.\n$$\nTo guarantee $\\|r\\|_{\\infty} / \\|b\\|_{\\infty} \\le \\varepsilon$, it is sufficient to require $\\kappa_{\\infty}(M_L) \\frac{\\|\\hat{r}\\|_{\\infty}}{\\|\\hat{b}\\|_{\\infty}} \\le \\varepsilon$, which means the stopping criterion on the monitored relative residual should be $\\frac{\\|\\hat{r}\\|_{\\infty}}{\\|\\hat{b}\\|_{\\infty}} \\le \\frac{\\varepsilon}{\\kappa_{\\infty}(M_L)}$. This logic is correct.\nNow, we calculate $\\kappa_{\\infty}(M_L) = \\|M_L\\|_{\\infty} \\|M_L^{-1}\\|_{\\infty}$.\nWe already found $\\|M_L\\|_{\\infty} = 5$. We need $M_L^{-1}$.\nThe determinant is $\\det(M_L) = (4)(3) - (0)(2) = 12$.\nThe inverse is $M_L^{-1} = \\frac{1}{12}\\begin{bmatrix}3 & 0 \\\\ -2 & 4\\end{bmatrix} = \\begin{bmatrix}1/4 & 0 \\\\ -1/6 & 1/3\\end{bmatrix}$.\nThe infinity norm of the inverse is the maximum absolute row sum:\n- Row 1 sum: $|1/4| + |0| = 1/4 = 0.25$.\n- Row 2 sum: $|-1/6| + |1/3| = 1/6 + 2/6 = 3/6 = 1/2 = 0.5$.\nSo, $\\|M_L^{-1}\\|_{\\infty} = 1/2$.\nThe condition number is $\\kappa_{\\infty}(M_L) = \\|M_L\\|_{\\infty} \\|M_L^{-1}\\|_{\\infty} = 5 \\times (1/2) = 2.5$.\nThe calculated value matches the value given in the statement. Both parts of the statement are correct.\n**Verdict: Correct.**\n\n**E. Right preconditioning corrupts absolute backward-error measurement, so it cannot be used when an absolute residual tolerance is specified.**\nThis statement is the opposite of the truth. As established in the analysis of option A, the monitored residual in right preconditioning IS the true residual. It provides a direct, uncorrupted measure of the backward error. It is the most reliable of the three methods for enforcing a specific tolerance on the true residual.\n**Verdict: Incorrect.**\n\n**F. In split preconditioning, the physical solution must be recovered via $x = M_L^{-1} z$ rather than $x = M_R^{-1} z$; this is essential to maintain consistency with the monitored residual.**\nAs derived in the analysis for option C, the system being solved is $M_L^{-1} A M_R^{-1} z = M_L^{-1} b$. Multiplying by $M_L$ yields $A (M_R^{-1} z) = b$. Comparing this to the original system $Ax=b$, the correct transformation is $x = M_R^{-1} z$. The statement suggests $x = M_L^{-1} z$, which is incorrect.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "While understanding residual monitoring is crucial, the practical utility of a preconditioner begins with its successful construction. This practice  delves into the creation of preconditioners using Incomplete LU (ILU) factorization, a powerful but delicate technique. You will confront a common failure mode—a zero pivot—and see how row pivoting resolves the issue, allowing a stable factorization to proceed. The exercise then illuminates the non-trivial consequences of this pivoting, revealing how it fundamentally changes the resulting left- and right-preconditioned operators.",
            "id": "3555596",
            "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$ be the sparse matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}.\n$$\nConsider performing an Incomplete Lower-Upper (ILU) factorization with zero fill-in, denoted $\\mathrm{ILU}(0)$, under the natural row ordering without pivoting, so that the incomplete factors $L$ and $U$ inherit the sparsity pattern of the strictly lower and upper triangular parts of $A$ respectively, with $L$ unit lower triangular. Define the preconditioning matrix $M$ by $M = L U$.\n\nTasks:\n1. Using only the fundamental definitions of factorization and pivoting, argue whether the $\\mathrm{ILU}(0)$ process on $A$ without pivoting can maintain nonsingularity of $M$ and justify your conclusion by analyzing the first pivot.\n2. Let $P \\in \\mathbb{R}^{3 \\times 3}$ be the permutation matrix that swaps the first and second rows. Perform $\\mathrm{ILU}(0)$ on the permuted matrix $P A$ and construct explicit $L$ and $U$ such that $P A = L U$ with the same sparsity constraints as above. Define the corresponding preconditioner $M := L U$.\n3. Using only the definitions of left and right preconditioning, analyze the consequences of this pivoting for the preconditioned operators. Specifically, define the left-preconditioned operator $T_{\\mathrm{left}} := M^{-1} A$ and the right-preconditioned operator $T_{\\mathrm{right}} := A M^{-1}$, and derive their exact forms in terms of $P$.\n4. Compute the determinant of the left-preconditioned operator $T_{\\mathrm{left}}$. Express your final answer as a single real number. No rounding is required.",
            "solution": "The problem will be addressed by sequentially completing the four tasks as stated.\n\nThe given sparse matrix is\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 3}.\n$$\n\nTask 1: Analysis of $\\mathrm{ILU}(0)$ on $A$ without pivoting.\n\nThe $\\mathrm{ILU}(0)$ factorization constructs an approximation $M=LU \\approx A$, where $L$ is a unit lower triangular matrix and $U$ is an upper triangular matrix. The sparsity pattern of $L$ is constrained to that of the strictly lower triangular part of $A$, and the sparsity pattern of $U$ is constrained to that of the upper triangular part of $A$.\n\nThe factors $L$ and $U$ must have the forms:\n$$\nL = \\begin{pmatrix}\n1 & 0 & 0 \\\\\nl_{21} & 1 & 0 \\\\\n0 & l_{32} & 1\n\\end{pmatrix}, \\quad\nU = \\begin{pmatrix}\nu_{11} & u_{12} & 0 \\\\\n0 & u_{22} & u_{23} \\\\\n0 & 0 & u_{33}\n\\end{pmatrix}\n$$\nThe ILU factorization process is a variant of Gaussian elimination. The first step involves the pivot element $A_{11}$. To compute the entries of the factors, we would typically set $u_{11} = A_{11}$. In this problem, $A_{11} = 0$.\n\nThe algorithm for computing the incomplete factorization would require calculating the multiplier $l_{21}$ as $l_{21} = A_{21} / u_{11} = A_{21} / A_{11}$. Since $A_{21}=1$ and $A_{11}=0$, this computation involves division by zero. Therefore, the $\\mathrm{ILU}(0)$ process on $A$ without pivoting fails at the very first step.\n\nIf the factorization could proceed, the preconditioner would be $M = LU$. The determinant of $M$ is $\\det(M) = \\det(L)\\det(U)$. Since $L$ is unit lower triangular, $\\det(L)=1$. The determinant of $U$ is the product of its diagonal elements, $\\det(U) = u_{11} u_{22} u_{33}$. As established, the algorithm would set $u_{11} = A_{11} = 0$. Consequently, $\\det(U) = 0$, which implies $\\det(M) = 0$.\nA singular preconditioner $M$ is computationally undesirable as its inverse $M^{-1}$ does not exist, making it impossible to solve the preconditioned system.\nConclusion for Task 1: The $\\mathrm{ILU}(0)$ process on $A$ without pivoting fails due to a zero pivot at position $(1,1)$. This failure means a nonsingular preconditioner $M$ cannot be formed; if it were formally constructed, it would be singular because its first diagonal element $u_{11}$ would be zero.\n\nTask 2: $\\mathrm{ILU}(0)$ on the permuted matrix $PA$.\n\nThe permutation matrix $P$ that swaps the first and second rows is:\n$$\nP = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nThe permuted matrix $PA$ is:\n$$\nPA = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix}.\n$$\nLet us denote this permuted matrix as $A' = PA$. We perform $\\mathrm{ILU}(0)$ on $A'$. The sparsity pattern of $L$ and $U$ is inherited from $A'$.\nThe matrix $A'$ has non-zero entries in its strictly lower part only at position $(3,2)$. Its upper part has non-zeroes at $(1,1), (1,3), (2,2), (3,3)$.\nThe factors $L$ and $U$ for $A'$ will have the forms:\n$$\nL = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & l_{32} & 1\n\\end{pmatrix}, \\quad\nU = \\begin{pmatrix}\nu_{11} & 0 & u_{13} \\\\\n0 & u_{22} & 0 \\\\\n0 & 0 & u_{33}\n\\end{pmatrix}\n$$\nbecause $A'_{21}=0, A'_{31}=0, A'_{12}=0, A'_{23}=0$.\nThe product $LU$ is:\n$$\nLU = \\begin{pmatrix}\nu_{11} & 0 & u_{13} \\\\\n0 & u_{22} & 0 \\\\\n0 & l_{32}u_{22} & u_{33}\n\\end{pmatrix}.\n$$\nWe equate the non-zero entries of $LU$ to the corresponding entries of $A'$:\n$$\n\\begin{pmatrix}\nu_{11} & 0 & u_{13} \\\\\n0 & u_{22} & 0 \\\\\n0 & l_{32}u_{22} & u_{33}\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix}.\n$$\nFrom this equality, we find the coefficients:\n$u_{11} = 1$\n$u_{13} = 1$\n$u_{22} = 1$\n$l_{32}u_{22} = 1 \\implies l_{32}(1) = 1 \\implies l_{32} = 1$\n$u_{33} = 1$\n\nThe resulting factors are:\n$$\nL = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix}, \\quad\nU = \\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nThe preconditioner is $M=LU$. Let's compute it:\n$$\nM = LU = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix}.\n$$\nWe observe that $M=PA$. In this case, the $\\mathrm{ILU}(0)$ factorization of the permuted matrix $PA$ is exact.\n\nTask 3: Analysis of the preconditioned operators.\n\nWe have the preconditioner $M = LU = PA$. The preconditioned operators are $T_{\\mathrm{left}} = M^{-1}A$ and $T_{\\mathrm{right}} = AM^{-1}$.\n\nFor the right-preconditioned operator $T_{\\mathrm{right}}$:\nWe substitute $M=PA$:\n$T_{\\mathrm{right}} = A M^{-1} = A (PA)^{-1}$.\nUsing the property $(XY)^{-1} = Y^{-1}X^{-1}$, we get:\n$T_{\\mathrm{right}} = A (A^{-1}P^{-1}) = (AA^{-1})P^{-1} = I P^{-1} = P^{-1}$.\nThe permutation matrix $P$ is an involution, meaning $P^2=I$, so $P^{-1}=P$.\nThus, the right-preconditioned operator is exactly the permutation matrix $P$:\n$T_{\\mathrm{right}} = P^{-1} = P$.\n\nFor the left-preconditioned operator $T_{\\mathrm{left}}$:\nWe substitute $M=PA$:\n$T_{\\mathrm{left}} = M^{-1}A = (PA)^{-1}A$.\nAgain, using $(XY)^{-1} = Y^{-1}X^{-1}$:\n$T_{\\mathrm{left}} = (A^{-1}P^{-1})A = A^{-1}P^{-1}A$.\nUsing $P^{-1}=P$, we have:\n$T_{\\mathrm{left}} = A^{-1}PA$.\nThis shows that the left-preconditioned operator $T_{\\mathrm{left}}$ is a similarity transformation of the permutation matrix $P$.\nThe exact forms in terms of $P$ (and $A$) are $T_{\\mathrm{right}} = P^{-1}$ and $T_{\\mathrm{left}} = A^{-1}P^{-1}A$.\n\nTask 4: Determinant of the left-preconditioned operator $T_{\\mathrm{left}}$.\n\nWe use the derived form $T_{\\mathrm{left}} = A^{-1}P^{-1}A$. The determinant of a product of matrices is the product of their determinants:\n$$\n\\det(T_{\\mathrm{left}}) = \\det(A^{-1}P^{-1}A) = \\det(A^{-1}) \\det(P^{-1}) \\det(A).\n$$\nSince $\\det(A^{-1}) = 1/\\det(A)$ and $\\det(P^{-1}) = 1/\\det(P)$, we have:\n$$\n\\det(T_{\\mathrm{left}}) = \\frac{1}{\\det(A)} \\frac{1}{\\det(P)} \\det(A) = \\frac{1}{\\det(P)}.\n$$\nAlternatively and more simply, the determinant is invariant under similarity transformations:\n$$\n\\det(T_{\\mathrm{left}}) = \\det(A^{-1}(P^{-1})A) = \\det(P^{-1}).\n$$\nThe matrix $P$ is obtained from the identity matrix $I$ by swapping two rows (the first and second). A single row swap negates the determinant. Since $\\det(I)=1$, the determinant of $P$ is:\n$$\n\\det(P) = -1.\n$$\nTherefore, $\\det(P^{-1}) = 1/\\det(P) = 1/(-1) = -1$.\nSo, the determinant of the left-preconditioned operator is:\n$$\n\\det(T_{\\mathrm{left}}) = -1.\n$$\nThis result is independent of the matrix $A$, provided $A$ is invertible, which it is ($\\det(A) = 0(..) - 1(1-0) + 0(..) = -1 \\neq 0$).",
            "answer": "$$\n\\boxed{-1}\n$$"
        },
        {
            "introduction": "Our final practice moves from algebraic structure to the bedrock of numerical computation: finite precision arithmetic. Even with a well-defined problem and a stable factorization, the limitations of floating-point numbers can introduce subtle yet significant errors, especially when the preconditioner $M$ is ill-conditioned. This hands-on coding exercise  guides you to implement a left-preconditioned GMRES solver, observe the loss of accuracy in computing the preconditioned residual $M^{-1} r_k$, and implement a compensated summation technique to robustly track the true residual norm.",
            "id": "3555547",
            "problem": "Consider solving a linear system $A x = b$ with left preconditioning in finite precision arithmetic. In left preconditioning, one solves the transformed system $M^{-1} A x = M^{-1} b$ using the Generalized Minimal Residual method (GMRES). The Arnoldi process builds an orthonormal basis for the Krylov subspace generated by the operator $K = M^{-1} A$ acting on the preconditioned residual. In exact arithmetic, the norm of the preconditioned residual $\\|M^{-1} r_k\\|_2$ is minimized at each iteration $k$, and the GMRES algorithm tracks this quantity using a small least-squares problem derived from the Arnoldi relation. However, in finite precision arithmetic, explicitly computing $M^{-1} r_k$ can suffer loss of accuracy when the condition number $\\kappa(M)$ is large.\n\nYour task is threefold:\n- Implement left-preconditioned GMRES from first principles using the Arnoldi process on the operator $K = M^{-1} A$ and Givens rotations to maintain the triangularization of the small least-squares problem. The initial guess must be $x_0 = 0$, so that $r_0 = b$. At iteration $k$, track the GMRES internal estimate of the preconditioned residual norm $\\|M^{-1} r_k\\|_2$ from the least-squares structure, without explicitly forming $M^{-1} r_k$.\n- Demonstrate a finite precision scenario in which the explicitly computed $M^{-1} r_k$ loses accuracy by emulating the $M^{-1}$ application in reduced precision only for the residual formation. Specifically, use single-precision arithmetic to apply $M^{-1}$ to $r_k$ when computing the explicit preconditioned residual norm, while performing the Arnoldi process and GMRES algebra in double precision. Quantify the discrepancy as a relative error between the explicit single-precision $\\|M^{-1} r_k\\|_2$ and the GMRES-tracked $\\|M^{-1} r_k\\|_2$.\n- Propose and implement a compensated update for the unpreconditioned residual that maintains accurate tracking of $\\|r_k\\|_2$ even when $\\kappa(M)$ is large. Use the fundamental identity $r_{k+1} = r_k - A \\Delta x_k$, where $\\Delta x_k$ is the change in the GMRES iterate between iterations $k$ and $k+1$, and apply a vector form of compensated summation (Kahan-style) to update $r_k$ componentwise so as to reduce cancellation error. Quantify the accuracy by comparing the compensated $\\|r_k\\|_2$ to the directly recomputed $\\|b - A x_k\\|_2$.\n\nStart from the following fundamental bases:\n- The definition of the residual $r_k = b - A x_k$.\n- The definition of the left-preconditioned operator $K = M^{-1} A$ and the preconditioned residual $\\hat{r}_k = M^{-1} r_k$.\n- The Arnoldi process relation for $K$, which yields an orthonormal basis and a small upper-Hessenberg linear system used to define the GMRES iterate.\n- Standard backward error reasoning for linear solves, which implies that when applying $M^{-1}$ in finite precision arithmetic the computed result is the exact solution for a perturbed system $(M + \\Delta M) z = r_k + \\delta r$, where the relative perturbations are bounded proportionally to the unit roundoff and the condition number $\\kappa(M)$.\n\nDesign the following experimental setting to ensure scientific realism while being self-contained:\n- Construct $A \\in \\mathbb{R}^{n \\times n}$ as $A = Q^\\top D Q + \\gamma S$, where $Q$ is orthonormal obtained from a thin $QR$ factorization of a Gaussian random matrix, $D$ is diagonal with entries spanning a moderate range to keep $A$ well-conditioned, $S$ is a skew-symmetric matrix with small entries, and $\\gamma$ is a small scalar to introduce mild nonnormality.\n- Construct the left preconditioner $M$ as a diagonal matrix with entries spanning a prescribed range so that $\\kappa(M)$ takes on controlled values (e.g., $10^2$, $10^4$, $10^8$). This keeps $M$ invertible but allows exploration of loss of accuracy in $M^{-1} r_k$ formation as $\\kappa(M)$ grows.\n- Use $x_0 = 0$ and a random vector $b$ with entries drawn from a normal distribution. All quantities must be dimensionless, and all norms must be the Euclidean norm $\\|\\cdot\\|_2$.\n\nImplement the compensated update for the unpreconditioned residual as follows:\n- Maintain a pair $(r^{\\mathrm{comp}}, c)$ of vectors, where $r^{\\mathrm{comp}}$ is the compensated residual estimate and $c$ is a compensation vector initially $0$.\n- At each iteration when the GMRES least-squares solution vector changes from $y_{k-1}$ to $y_k$, compute $\\Delta x_k = V_k (y_k - y_{k-1})$ using the current Arnoldi basis $V_k$ for the operator $K = M^{-1} A$ and update $r^{\\mathrm{comp}} \\leftarrow r^{\\mathrm{comp}} - A \\Delta x_k$ using componentwise compensated summation (Kahan-style) to mitigate cancellation.\n\nTest suite and output specification:\n- Use three test cases with parameters $(n, \\kappa(M), k_{\\max})$:\n    1. Case A: $n = 40$, target $\\kappa(M) = 10^2$, $k_{\\max} = 20$.\n    2. Case B: $n = 40$, target $\\kappa(M) = 10^8$, $k_{\\max} = 20$.\n    3. Case C: $n = 40$, target $\\kappa(M) = 10^4$, $k_{\\max} = 30$.\n- For each case, run left-preconditioned GMRES for $k_{\\max}$ iterations and produce two floats:\n    1. The relative error between the explicit single-precision preconditioned residual norm $\\|M^{-1} r_k\\|_2$ and the GMRES-tracked preconditioned residual norm at the final iteration $k$: $$E_{\\mathrm{hat}} = \\frac{\\left|\\|M^{-1} r_k\\|_2^{\\mathrm{(single)}} - \\|M^{-1} r_k\\|_2^{\\mathrm{(GMRES)}}\\right|}{\\|M^{-1} r_k\\|_2^{\\mathrm{(GMRES)}}}.$$\n    2. The relative error between the compensated unpreconditioned residual norm and the directly recomputed residual norm at the final iteration $k$: $$E_{\\mathrm{comp}} = \\frac{\\left|\\|r_k^{\\mathrm{(comp)}}\\|_2 - \\|b - A x_k\\|_2\\right|}{\\|b - A x_k\\|_2}.$$\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of the two floats. For example, the output should have the form `[[E1_hat,E1_comp],[E2_hat,E2_comp],[E3_hat,E3_comp]]`. No additional text should be printed.\n\nAll random draws must use a fixed seed to ensure reproducibility. All computations unrelated to the explicit residual formation and the Arnoldi process must use double precision. The explicit formation of $M^{-1} r_k$ used to demonstrate loss of accuracy must be performed in single precision. The compensated update must be implemented with componentwise compensated summation and must not use any external high-precision libraries.",
            "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded problem in numerical linear algebra that requires the implementation and analysis of the left-preconditioned Generalized Minimal Residual (GMRES) method under specific finite-precision arithmetic conditions. The problem is clear, internally consistent, and requires a non-trivial implementation based on established numerical principles.\n\n### Principle-Based Solution Design\n\nThe task is to solve the linear system $A x = b$ using left-preconditioned GMRES, where the system is transformed to $M^{-1} A x = M^{-1} b$. The solution involves three primary components: implementing the core algorithm, demonstrating a specific finite-precision instability, and implementing a compensated summation technique to mitigate a related accuracy issue.\n\n#### 1. Left-Preconditioned GMRES with Arnoldi and Givens Rotations\n\nThe GMRES method finds an approximate solution $x_k$ from an affine subspace $x_0 + \\mathcal{K}_k$ that minimizes the Euclidean norm of the residual. For the left-preconditioned system, the operator is $K = M^{-1} A$ and the initial residual is $\\hat{r}_0 = M^{-1} r_0$. With an initial guess of $x_0 = 0$, we have $r_0 = b$, so $\\hat{r}_0 = M^{-1} b$. GMRES minimizes the norm of the preconditioned residual, $\\|M^{-1} r_k\\|_2 = \\|M^{-1}(b - A x_k)\\|_2$.\n\nThe core of the algorithm is the Arnoldi process, which generates an orthonormal basis $V_{k+1} = [v_0, v_1, \\dots, v_k]$ for the Krylov subspace $\\mathcal{K}_{k+1}(K, \\hat{r}_0) = \\text{span}\\{\\hat{r}_0, K\\hat{r}_0, \\dots, K^k \\hat{r}_0\\}$. The process produces the relation:\n$$\nK V_k = V_{k+1} \\bar{H}_k\n$$\nwhere $V_k = [v_0, \\dots, v_{k-1}]$, $V_{k+1} = [v_0, \\dots, v_k]$, and $\\bar{H}_k$ is a $(k+1) \\times k$ upper-Hessenberg matrix. The implementation will use a modified Gram-Schmidt process for its superior numerical stability.\n\nThe GMRES iterate $x_k$ is expressed as $x_k = x_0 + z_k = V_k y_k$, where $y_k$ is a vector of coefficients. The minimization problem becomes:\n$$\n\\min_{y_k} \\| \\hat{r}_0 - K z_k \\|_2 = \\min_{y_k} \\| \\beta v_0 - K V_k y_k \\|_2 = \\min_{y_k} \\| \\beta v_0 - V_{k+1} \\bar{H}_k y_k \\|_2\n$$\nwhere $\\beta = \\|\\hat{r}_0\\|_2$ and $v_0 = \\hat{r}_0 / \\beta$. Since the columns of $V_{k+1}$ are orthonormal, this is equivalent to solving the small least-squares problem:\n$$\ny_k = \\arg\\min_{y \\in \\mathbb{R}^k} \\| \\beta e_1 - \\bar{H}_k y \\|_2\n$$\nwhere $e_1 = [1, 0, \\dots, 0]^\\top \\in \\mathbb{R}^{k+1}$.\n\nTo solve this efficiently at each iteration, we apply a sequence of Givens rotations $G_0, G_1, \\dots, G_{k-1}$ to transform $\\bar{H}_k$ into an upper-triangular matrix $R_k$. Let $\\Omega_k = G_{k-1} \\dots G_0$. Applying $\\Omega_k$ to the least-squares problem yields:\n$$\n\\min_{y} \\| \\Omega_k (\\beta e_1) - \\Omega_k \\bar{H}_k y \\|_2 = \\min_{y} \\| g_k - \\begin{pmatrix} R_k \\\\ 0 \\end{pmatrix} y \\|_2\n$$\nThe solution $y_k$ is found by solving the triangular system $R_k y_k = g_k(1:k)$. The norm of the preconditioned residual is then given directly by the magnitude of the last element of the transformed right-hand side vector, $\\|M^{-1} r_k\\|_2 = |g_k(k+1)|$. This is the GMRES-tracked residual norm, calculated without explicitly forming $r_k$ or $M^{-1} r_k$.\n\n#### 2. Demonstrating Finite-Precision Error\n\nWhen the preconditioner $M$ is ill-conditioned (i.e., $\\kappa(M)$ is large), the explicit computation of $\\hat{r}_k = M^{-1} r_k$ in finite-precision arithmetic can suffer from a significant loss of accuracy. Standard backward error analysis for solving $M z = r_k$ shows that the computed solution $\\tilde{z}$ satisfies $(M + \\Delta M) \\tilde{z} = r_k$, where $\\|\\Delta M\\| / \\|M\\|$ is proportional to machine epsilon $\\epsilon_{\\text{mach}}$ and $\\kappa(M)$. This means the computed preconditioned residual may be far from the true value.\n\nTo demonstrate this, we will perform the main GMRES algorithm in double precision ($\\epsilon_{\\text{mach}} \\approx 10^{-16}$), but when we need to explicitly compute $\\|M^{-1} r_k\\|_2$ for comparison, we will simulate a less accurate computation. This will be done by casting the unpreconditioned residual $r_k$ and the diagonal of $M$ to single precision ($\\epsilon_{\\text{mach}} \\approx 10^{-7}$) before performing the division that represents the action of $M^{-1}$. The resulting single-precision norm, $\\|M^{-1} r_k\\|_2^{\\mathrm{(single)}}$, is then compared to the stable, internally tracked GMRES norm, $\\|M^{-1} r_k\\|_2^{\\mathrm{(GMRES)}}$. The relative error $E_{\\mathrm{hat}}$ is expected to be large when $\\kappa(M)$ is large.\n\n#### 3. Compensated Residual Update\n\nA related accuracy issue is the update of the unpreconditioned residual $r_k$. Naively updating it as $r_k \\leftarrow r_{k-1} - A \\Delta x_k$ can lead to catastrophic cancellation, as for a converging method, $A \\Delta x_k$ will be very close to $r_{k-1}$. The problem proposes to implement a compensated summation scheme, specifically a vector-form of Kahan's algorithm, to update the residual.\n\nWe maintain the residual $r^{\\mathrm{comp}}$ and a compensation vector $c$, both initialized to zero (with $r^{\\mathrm{comp}}$ set to $b$ initially). At each iteration $k$, we first compute the change in the solution iterate, $\\Delta x_k = x_k - x_{k-1}$. The update term is $u = -A \\Delta x_k$. The compensated update proceeds component-wise for each vector element:\n1. $y_{\\text{kahan}} = u - c$\n2. $t = r^{\\mathrm{comp}} + y_{\\text{kahan}}$\n3. $c = (t - r^{\\mathrm{comp}}) - y_{\\text{kahan}}$\n4. $r^{\\mathrm{comp}} = t$\n\nThis procedure stores the low-order bits lost during the addition of $r^{\\mathrm{comp}}$ and $y_{\\text{kahan}}$ into the compensation vector $c$, and re-introduces them in the next iteration. This maintains high accuracy in the computed $r^{\\mathrm{comp}}$. The accuracy of this method is assessed by computing the relative error $E_{\\mathrm{comp}}$ between the norm of the compensated residual, $\\|r_k^{\\mathrm{(comp)}}\\|_2$, and the norm of a directly recomputed residual, $\\|b - A x_k\\|_2$, which serves as a high-accuracy benchmark.\n\nThe experimental setup with a controlled matrix structure ($A = Q^\\top D Q + \\gamma S$) and a diagonal preconditioner $M$ with prescribed $\\kappa(M)$ provides a scientifically sound and reproducible environment to test these numerical phenomena.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import qr, solve_triangular\n\n# Fixed seed for reproducibility\nRNG = np.random.default_rng(12345)\n\ndef create_problem(n, kappa_M, gamma=1e-2):\n    \"\"\"\n    Constructs the matrix A, preconditioner M, and vector b for the test problem.\n    \"\"\"\n    # Use double precision for construction\n    Z = RNG.standard_normal((n, n), dtype=np.float64)\n    Q, _ = qr(Z, mode='economic')\n    Q = Q.astype(np.float64)\n\n    # D with moderate conditioning (kappa ~ 10)\n    d = np.logspace(0, 1, n, dtype=np.float64)\n    D = np.diag(d)\n\n    # Skew-symmetric S with small entries\n    S_rand = RNG.standard_normal((n, n), dtype=np.float64)\n    S = (S_rand - S_rand.T) / 2.0\n\n    A = Q.T @ D @ Q + gamma * S\n\n    # Diagonal preconditioner M with target condition number\n    m_diag = np.logspace(0, np.log10(kappa_M), n, dtype=np.float64)\n    M = np.diag(m_diag)\n\n    # Right-hand side vector b\n    b = RNG.standard_normal(n, dtype=np.float64)\n\n    return A, M, b\n\ndef kahan_sum(v_sum, v_add, c):\n    \"\"\"\n    Performs a component-wise Kahan summation for two vectors.\n    v_sum = v_sum + v_add\n    \"\"\"\n    y = v_add - c\n    t = v_sum + y\n    c = (t - v_sum) - y\n    v_sum = t\n    return v_sum, c\n\ndef left_gmres(A, M, b, k_max):\n    \"\"\"\n    Implements left-preconditioned GMRES with specified numerical analyses.\n    \"\"\"\n    n = A.shape[0]\n\n    # ---- Initialization ----\n    # x_0 = 0, so r_0 = b\n    r = b.copy()\n    \n    # Compensated residual initialization\n    r_comp = b.copy()\n    c_comp = np.zeros(n, dtype=np.float64)\n    \n    # Preconditioned residual r_hat_0 = M^{-1} r_0. Since M is diagonal, this is an element-wise division.\n    M_diag = np.diag(M)\n    M_inv_r = r / M_diag\n\n    beta = np.linalg.norm(M_inv_r)\n    \n    # Arnoldi basis V and Hessenberg H\n    V = np.zeros((n, k_max + 1), dtype=np.float64)\n    V[:, 0] = M_inv_r / beta\n    H = np.zeros((k_max + 1, k_max), dtype=np.float64)\n    \n    # Givens rotation data\n    cs = np.zeros(k_max, dtype=np.float64)\n    sn = np.zeros(k_max, dtype=np.float64)\n    \n    # RHS for least-squares problem, g\n    g = np.zeros(k_max + 1, dtype=np.float64)\n    g[0] = beta\n    \n    x_prev = np.zeros(n, dtype=np.float64)\n    \n    iterations_run = k_max\n\n    # ---- GMRES Iteration Loop ----\n    for k in range(k_max):\n        # -- Arnoldi Step --\n        # Form w = K * v_k = M^{-1} * A * v_k\n        v_k = V[:, k]\n        Av_k = A @ v_k\n        w = Av_k / M_diag\n\n        # Modified Gram-Schmidt orthogonalization\n        for j in range(k + 1):\n            h_jk = np.dot(V[:, j], w)\n            H[j, k] = h_jk\n            w -= h_jk * V[:, j]\n            \n        H[k + 1, k] = np.linalg.norm(w)\n        \n        # Check for lucky breakdown (exact solution found)\n        if H[k + 1, k]  1e-12:\n            iterations_run = k + 1\n            break\n            \n        V[:, k + 1] = w / H[k + 1, k]\n\n        # -- Update Least-Squares Problem via Givens Rotations --\n        # Apply previous k rotations to the new column of H\n        for j in range(k):\n            h_temp = cs[j] * H[j, k] + sn[j] * H[j + 1, k]\n            H[j + 1, k] = -sn[j] * H[j, k] + cs[j] * H[j + 1, k]\n            H[j, k] = h_temp\n            \n        # Generate and apply new rotation for the current column k\n        h_kk, h_kp1_k = H[k, k], H[k + 1, k]\n        rot_norm = np.sqrt(h_kk**2 + h_kp1_k**2)\n        cs[k] = h_kk / rot_norm\n        sn[k] = h_kp1_k / rot_norm\n        \n        H[k, k] = cs[k] * h_kk + sn[k] * h_kp1_k\n        H[k + 1, k] = 0.0\n        \n        # Apply the new rotation to the RHS vector g\n        g_k = g[k]\n        g[k] = cs[k] * g_k\n        g[k + 1] = -sn[k] * g_k\n\n        # -- Compensated Residual Update --\n        # 1. Solve the LS problem R_k * y_k = g_{1:k+1}\n        R_k = H[:k+1, :k+1]\n        g_sub = g[:k+1]\n        y_k = solve_triangular(R_k, g_sub, check_finite=False)\n        \n        # 2. Compute current solution x_k = V_{k+1} * y_k\n        # Note: This is an intermediate solution, not the final one.\n        # It's better to compute the change in the LS solution y and map that up.\n        # However, for simplicity of delta_x, we compute the full x_curr.\n        x_curr = V[:, :k+1] @ y_k\n        \n        # 3. Compute change in solution\n        delta_x = x_curr - x_prev\n        \n        # 4. Update compensated residual using Kahan summation\n        update_term = - (A @ delta_x)\n        r_comp, c_comp = kahan_sum(r_comp, update_term, c_comp)\n        \n        # 5. Store current solution for the next iteration's delta\n        x_prev = x_curr\n\n    # ---- Final Calculations (after k_max or breakdown) ----\n    k_final = iterations_run\n    \n    # Final LS solution y and solution iterate x\n    R_final = H[:k_final, :k_final]\n    g_final = g[:k_final]\n    y_final = solve_triangular(R_final, g_final, check_finite=False)\n    x_final = V[:, :k_final] @ y_final\n\n    # -- Error E_hat: GMRES-tracked vs. explicit single-precision preconditioned residual norm --\n    # 1. GMRES-tracked preconditioned residual norm is |g_{k_final}|\n    norm_res_precond_gmres = abs(g[k_final])\n\n    # 2. Explicitly compute unpreconditioned residual r_k = b - A*x_k in double precision\n    r_final_explicit = b - (A @ x_final)\n    \n    # 3. Compute M^{-1}r_k in single precision to demonstrate error\n    r_final_explicit_sp = r_final_explicit.astype(np.float32)\n    M_diag_sp = M_diag.astype(np.float32)\n    M_inv_r_sp = r_final_explicit_sp / M_diag_sp\n    norm_res_precond_single = np.linalg.norm(M_inv_r_sp)\n    \n    # 4. E_hat relative error\n    if norm_res_precond_gmres  1e-15:\n        E_hat = np.abs(norm_res_precond_single - norm_res_precond_gmres)\n    else:\n        E_hat = np.abs(norm_res_precond_single - norm_res_precond_gmres) / norm_res_precond_gmres\n    \n    # -- Error E_comp: Compensated vs. directly recomputed unpreconditioned residual norm --\n    # 1. Norm of the compensated residual\n    norm_r_comp = np.linalg.norm(r_comp)\n    \n    # 2. Norm of the directly recomputed residual (benchmark)\n    norm_r_recomputed = np.linalg.norm(r_final_explicit)\n    \n    # 3. E_comp relative error\n    if norm_r_recomputed  1e-15:\n        E_comp = np.abs(norm_r_comp - norm_r_recomputed)\n    else:    \n        E_comp = np.abs(norm_r_comp - norm_r_recomputed) / norm_r_recomputed\n    \n    return [E_hat, E_comp]\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        (40, 1e2, 20),\n        (40, 1e8, 20),\n        (40, 1e4, 30),\n    ]\n\n    results = []\n    for n, kappa_M, k_max in test_cases:\n        A, M, b = create_problem(n, kappa_M)\n        case_results = left_gmres(A, M, b, k_max)\n        results.append(case_results)\n\n    # Format output as specified: [[E1_hat,E1_comp],[E2_hat,E2_comp],[E3_hat,E3_comp]]\n    # str() on a list gives '[v1, v2]', join with commas, then wrap in brackets.\n    # .replace(\" \", \"\") to remove spaces for exact output matching.\n    output_str = f\"[{','.join(map(str, results))}]\".replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n\n```"
        }
    ]
}