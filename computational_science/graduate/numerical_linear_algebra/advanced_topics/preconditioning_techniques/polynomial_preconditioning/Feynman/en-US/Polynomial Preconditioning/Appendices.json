{
    "hands_on_practices": [
        {
            "introduction": "To build a strong foundation in polynomial preconditioning, we begin with the fundamental design problem. This exercise challenges you to derive the theoretically optimal polynomial preconditioner from first principles, assuming only that the spectrum of the matrix $A$ is contained within a known interval $[a, b]$ . By leveraging the minimax property of Chebyshev polynomials, you will construct a polynomial $p_m(A)$ that minimizes the spectral radius of the residual operator $I - p_m(A)A$, providing the best possible uniform approximation of $A^{-1}$ on the spectral interval.",
            "id": "3565775",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be symmetric positive definite (SPD), and suppose only spectral bounds are available, namely $0 < a \\le \\lambda_{\\min}(A) \\le \\lambda_{\\max}(A) \\le b$. Consider polynomial preconditioning, where one seeks a polynomial $p$ such that the linear operator $I - A p(A)$ has small operator $2$-norm. Using the spectral theorem for symmetric matrices, the spectral mapping property for polynomial functions, and the minimax extremal property of Chebyshev polynomials of the first kind on $[-1,1]$, construct a robust recipe for a degree-$m$ polynomial preconditioner $p_{m}$ based solely on the bounds $a$ and $b$ that guarantees a prescribed reduction factor $\\delta \\in (0,1)$ in the operator $2$-norm, that is,\n$$\n\\|I - A p_{m}(A)\\|_{2} \\le \\delta.\n$$\nYour derivation must begin from the stated foundational facts and proceed by mapping the interval $[a,b]$ to $[-1,1]$ via an affine transformation and scaling a Chebyshev polynomial to impose the constraint $q(0)=0$ on the auxiliary polynomial $q(\\lambda) = \\lambda p_{m}(\\lambda)$. Explicitly produce:\n- a closed-form expression for the preconditioner $p_{m}(\\lambda)$ valid for all $\\lambda \\in [a,b]$, expressed in terms of Chebyshev polynomials of the first kind $T_{k}$, and\n- a closed-form expression for the minimal degree $m$ (as a function of $a$, $b$, and $\\delta$) that guarantees $\\|I - A p_{m}(A)\\|_{2} \\le \\delta$.\n\nThe final answer must be a single closed-form analytical expression. If multiple quantities are reported, present them as a single row matrix using the $\\mathrm{pmatrix}$ environment. No numerical evaluation is required.",
            "solution": "The user wants me to solve a numerical analysis problem.\nThe user wants me to solve a problem in numerical linear algebra.\nThe user wants me to solve a problem regarding polynomial preconditioning.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n-   $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix.\n-   The spectrum of $A$, $\\sigma(A)$, is contained within the interval $[a, b]$, where $0 < a \\le \\lambda_{\\min}(A) \\le \\lambda_{\\max}(A) \\le b$.\n-   A degree-$m$ polynomial preconditioner $p_m$ is sought.\n-   The goal is to construct $p_m$ such that the operator $2$-norm of the preconditioned matrix satisfies $\\|I - A p_{m}(A)\\|_{2} \\le \\delta$ for a given reduction factor $\\delta \\in (0,1)$.\n-   The derivation must use: the spectral theorem, the spectral mapping property, and the minimax property of Chebyshev polynomials of the first kind $T_k(x)$ on $[-1,1]$.\n-   The derivation procedure must involve an affine map from $[a,b]$ to $[-1,1]$ and imposing the constraint $q(0)=0$ on an auxiliary polynomial $q(\\lambda) = \\lambda p_m(\\lambda)$.\n-   The required outputs are closed-form expressions for the preconditioner polynomial $p_m(\\lambda)$ and the minimal degree $m$ that guarantees the norm reduction.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is a standard, fundamental topic in numerical linear algebra, specifically in the construction of iterative solvers for linear systems. All concepts—SPD matrices, eigenvalues, polynomial preconditioning, Chebyshev polynomials, and spectral bounds—are well-established and rigorously defined. The proposed methodology reflects the classical Chebyshev iteration and preconditioning theory. The problem is scientifically and mathematically sound.\n-   **Well-Posed**: The problem is well-posed. It seeks to minimize a maximum value (a minimax problem) under clear constraints. The minimax property of Chebyshev polynomials ensures that a unique optimal polynomial solution exists for this problem structure.\n-   **Objective**: The problem is stated in precise mathematical language, free of any subjectivity, ambiguity, or non-technical terms.\n-   The problem is complete, consistent, and formalizable. No flaws are detected.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full, reasoned solution will be provided.\n\n**Solution**\n\nThe objective is to find a polynomial $p_m$ of degree $m$ that minimizes the operator $2$-norm of the residual operator $R(A) = I - A p_m(A)$. Since $A$ is symmetric, it is unitarily diagonalizable, and the operator $2$-norm of any polynomial function of $A$ is determined by the maximum absolute value of the polynomial on the spectrum of $A$, $\\sigma(A)$.\n$$\n\\|I - A p_m(A)\\|_{2} = \\max_{\\lambda \\in \\sigma(A)} |1 - \\lambda p_m(\\lambda)|\n$$\nWe define the residual polynomial $R_{m+1}(\\lambda) = 1 - \\lambda p_m(\\lambda)$. Since $p_m(\\lambda)$ is a polynomial of degree $m$, $R_{m+1}(\\lambda)$ is a polynomial of degree at most $m+1$. Furthermore, by its definition, $R_{m+1}(0) = 1 - 0 \\cdot p_m(0) = 1$. The problem requires $q(\\lambda) = \\lambda p_m(\\lambda)$ to satisfy $q(0)=0$, which implies $R_{m+1}(0) = 1 - q(0) = 1$. So the degree of $R_{m+1}(\\lambda)$ is exactly $m+1$.\n\nSince the spectrum $\\sigma(A)$ is unknown beyond the fact that $\\sigma(A) \\subseteq [a,b]$, we must solve a more conservative problem by minimizing the maximum of $|R_{m+1}(\\lambda)|$ over the entire interval $[a,b]$:\n$$\n\\min_{p_m \\in \\mathcal{P}_m} \\max_{\\lambda \\in [a,b]} |1 - \\lambda p_m(\\lambda)| = \\min_{R_{m+1} \\in \\mathcal{P}_{m+1}, R_{m+1}(0)=1} \\max_{\\lambda \\in [a,b]} |R_{m+1}(\\lambda)|\n$$\nwhere $\\mathcal{P}_k$ is the space of polynomials of degree at most $k$.\n\nThis is a classic minimax problem. The solution is found by relating it to the Chebyshev polynomials of the first kind, $T_k(x)$, which are optimal in a similar sense on the interval $[-1,1]$. We first perform an affine transformation to map the interval $\\lambda \\in [a,b]$ to $x \\in [-1,1]$. Let $x = c_1 \\lambda + c_2$.\nFor $\\lambda=a$, we set $x=-1$: $-1 = c_1 a + c_2$.\nFor $\\lambda=b$, we set $x=1$: $1 = c_1 b + c_2$.\nSolving this system of linear equations for $c_1$ and $c_2$ yields:\n$c_1 = \\frac{2}{b-a}$ and $c_2 = -\\frac{a+b}{b-a}$.\nSo, the transformation is $x(\\lambda) = \\frac{2\\lambda - (a+b)}{b-a}$. The inverse transformation is $\\lambda(x) = \\frac{b-a}{2}x + \\frac{a+b}{2}$.\n\nThe constraint $R_{m+1}(0)=1$ is on a point $\\lambda=0$ which is outside the interval $[a,b]$ since $A$ is SPD ($a>0$). We must find the corresponding point $x_0$ in the transformed coordinate system:\n$$\nx_0 = x(0) = \\frac{-(a+b)}{b-a}\n$$\nSince $b>a>0$, we have $a+b > 0$ and $b-a > 0$, so $x_0 < -1$.\n\nThe problem is now transformed into finding a polynomial $P_{m+1}(x)$ of degree $m+1$ such that $P_{m+1}(x_0)=1$ and its maximum absolute value on $[-1,1]$ is minimized. The minimax property of Chebyshev polynomials states that the solution to this problem is given by a scaled Chebyshev polynomial:\n$$\nP_{m+1}(x) = \\frac{T_{m+1}(x)}{T_{m+1}(x_0)}\n$$\nThe maximum value of this polynomial on $[-1,1]$ is $\\frac{\\max_{x \\in [-1,1]}|T_{m+1}(x)|}{|T_{m+1}(x_0)|} = \\frac{1}{|T_{m+1}(x_0)|}$.\n\nTransforming back to the $\\lambda$ variable, the optimal residual polynomial is:\n$$\nR_{m+1}(\\lambda) = P_{m+1}(x(\\lambda)) = \\frac{T_{m+1}\\left(\\frac{2\\lambda - (a+b)}{b-a}\\right)}{T_{m+1}\\left(\\frac{-(a+b)}{b-a}\\right)}\n$$\nFrom the definition $R_{m+1}(\\lambda) = 1 - \\lambda p_m(\\lambda)$, we can solve for the preconditioner polynomial $p_m(\\lambda)$. Since $R_{m+1}(0)=1$, the polynomial $1-R_{m+1}(\\lambda)$ has a root at $\\lambda=0$, ensuring that $p_m(\\lambda)$ is a polynomial of degree $m$.\n$$\np_m(\\lambda) = \\frac{1 - R_{m+1}(\\lambda)}{\\lambda} = \\frac{1}{\\lambda} \\left( 1 - \\frac{T_{m+1}\\left(\\frac{2\\lambda - (a+b)}{b-a}\\right)}{T_{m+1}\\left(-\\frac{a+b}{b-a}\\right)} \\right)\n$$\nThis is the closed-form expression for the preconditioner polynomial of degree $m$.\n\nNext, we find the minimal degree $m$ that guarantees the norm reduction $\\|I - A p_m(A)\\|_2 \\le \\delta$.\nThe guaranteed reduction is given by the maximum of the optimal residual polynomial on $[a,b]$:\n$$\n\\|I - A p_m(A)\\|_2 \\le \\max_{\\lambda \\in [a,b]} |R_{m+1}(\\lambda)| = \\frac{1}{|T_{m+1}(x_0)|} = \\frac{1}{\\left|T_{m+1}\\left(-\\frac{a+b}{b-a}\\right)\\right|}\n$$\nWe require this bound to be less than or equal to $\\delta$:\n$$\n\\frac{1}{\\left|T_{m+1}\\left(-\\frac{a+b}{b-a}\\right)\\right|} \\le \\delta \\quad \\implies \\quad \\left|T_{m+1}\\left(-\\frac{a+b}{b-a}\\right)\\right| \\ge \\frac{1}{\\delta}\n$$\nFor $|x| \\ge 1$, the Chebyshev polynomial has the representation $T_k(x) = \\cosh(k \\cdot \\mathrm{arccosh}(x))$. Since $x_0 = -\\frac{a+b}{b-a} < -1$, we use the identity $T_k(x) = (-1)^k \\cosh(k \\cdot \\mathrm{arccosh}(-x))$ for $x \\le -1$. Thus, $|T_{m+1}(x_0)| = \\cosh((m+1) \\cdot \\mathrm{arccosh}(-x_0))$.\nThe inequality becomes:\n$$\n\\cosh\\left((m+1) \\cdot \\mathrm{arccosh}\\left(\\frac{a+b}{b-a}\\right)\\right) \\ge \\frac{1}{\\delta}\n$$\nSince $\\cosh$ is an increasing function for non-negative arguments, and $\\mathrm{arccosh}(y) \\ge 0$ for $y \\ge 1$, we can take $\\mathrm{arccosh}$ of both sides:\n$$\n(m+1) \\cdot \\mathrm{arccosh}\\left(\\frac{a+b}{b-a}\\right) \\ge \\mathrm{arccosh}\\left(\\frac{1}{\\delta}\\right)\n$$\nSolving for $m+1$:\n$$\nm+1 \\ge \\frac{\\mathrm{arccosh}(1/\\delta)}{\\mathrm{arccosh}\\left(\\frac{a+b}{b-a}\\right)}\n$$\nSince the degree $m$ must be an integer, $m+1$ must be the smallest integer satisfying this condition. The minimal integer value for $m+1$ is therefore the ceiling of the right-hand side.\n$$\nm+1 = \\left\\lceil \\frac{\\mathrm{arccosh}(1/\\delta)}{\\mathrm{arccosh}\\left(\\frac{a+b}{b-a}\\right)} \\right\\rceil\n$$\nThe minimal degree $m$ is then:\n$$\nm = \\left\\lceil \\frac{\\mathrm{arccosh}(1/\\delta)}{\\mathrm{arccosh}\\left(\\frac{a+b}{b-a}\\right)} \\right\\rceil - 1\n$$\nThis provides the minimal non-negative integer degree required to achieve the desired reduction factor $\\delta$.\n\nThe two requested expressions are the formula for $p_m(\\lambda)$ and the minimal degree $m$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{\\lambda} \\left( 1 - \\frac{T_{m+1}\\left(\\frac{2\\lambda - a - b}{b-a}\\right)}{T_{m+1}\\left(-\\frac{a+b}{b-a}\\right)} \\right) & \\left\\lceil \\frac{\\mathrm{arccosh}(1/\\delta)}{\\mathrm{arccosh}\\left(\\frac{a+b}{b-a}\\right)} \\right\\rceil - 1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A powerful preconditioner is one that significantly accelerates convergence. This practice bridges theory and application by asking you to analyze the concrete performance gains achieved by using the Chebyshev polynomial preconditioner with the Conjugate Gradient (CG) method . Starting from the established convergence bounds for CG, you will derive the effective condition number of the preconditioned system and determine the minimal polynomial degree required to achieve a specific speedup, such as halving the number of iterations.",
            "id": "3565795",
            "problem": "Consider a real, symmetric positive definite (SPD) matrix $A \\in \\mathbb{R}^{n \\times n}$ with spectrum contained in the interval $[\\lambda_{\\min}, \\lambda_{\\max}] \\subset (0,\\infty)$, and write the condition number as $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$. Let the Conjugate Gradient (CG) method be applied to the linear system $Ax=b$ and suppose you are allowed to use a left polynomial preconditioner $M_m = p_m(A)$, where $p_m$ is a polynomial of degree $m$ chosen to approximate $A^{-1}$ on $[\\lambda_{\\min},\\lambda_{\\max}]$ in the uniform norm via the minimax (Chebyshev) construction.\n\nStarting from the standard worst-case error bound for CG on SPD matrices in terms of the Chebyshev polynomials of the first kind and from the extremal properties of Chebyshev polynomials for uniform approximation on an interval, derive:\n\n1. The uniform relative error $E_m$ of the preconditioned spectrum clustering near $1$, defined by $E_m = \\max_{\\lambda \\in [\\lambda_{\\min},\\lambda_{\\max}]} |1 - \\lambda p_m(\\lambda)|$, expressed in closed form in terms of $\\kappa$ and $m$.\n2. The effective condition number $\\kappa_{\\mathrm{eff}}$ of the preconditioned operator $M_m A$ as a function of $E_m$ (and hence of $\\kappa$ and $m$).\n3. The associated CG contraction factor parameter $\\alpha_{\\mathrm{eff}}$ for $M_m A$, analogous to the unpreconditioned parameter $\\alpha = (\\sqrt{\\kappa}-1)/(\\sqrt{\\kappa}+1)$.\n\nUsing these derivations, determine the minimal polynomial degree $m$ required so that, for a fixed tolerance and in the same worst-case sense, the number of CG iterations needed with the preconditioner is halved compared to running CG without preconditioning. Provide the explicit degree $m$ as your final answer, and, in addition, present the explicit scaling laws that relate $E_m$, $\\kappa_{\\mathrm{eff}}$, and $\\alpha_{\\mathrm{eff}}$ to $\\kappa$ and $m$. No rounding is required; give an exact integer value for the degree $m$ in your final answer. All angles, if any appear, should be expressed in radians. No physical units are involved in this problem.",
            "solution": "The problem statement has been validated and is deemed sound. It is a well-posed problem in numerical linear algebra, specifically concerning the analysis of polynomial preconditioning for the Conjugate Gradient (CG) method. All terms are standard and the premises are scientifically grounded.\n\nThe problem asks for the analysis of a polynomial preconditioner $M_m = p_m(A)$ for a symmetric positive definite (SPD) matrix $A$. The polynomial $p_m(\\lambda)$ of degree $m$ is chosen to minimize the uniform norm of the residual polynomial on the spectral interval $[\\lambda_{\\min}, \\lambda_{\\max}]$. We will derive the key performance metrics and then determine the minimal degree $m$ to achieve a specified improvement in convergence speed.\n\nLet the eigenvalues of $A$ be denoted by $\\lambda$, which are all contained in the interval $[\\lambda_{\\min}, \\lambda_{\\max}] \\subset (0,\\infty)$. The condition number is $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$. The preconditioned matrix is $M_m A = p_m(A)A$. Its eigenvalues are $\\lambda' = \\lambda p_m(\\lambda)$. The convergence rate of CG applied to the preconditioned system $M_m A x = M_m b$ depends on the spectrum of $M_m A$. An effective preconditioner clusters the eigenvalues of $M_m A$ around $1$. This is achieved by making $\\lambda p_m(\\lambda)$ close to $1$ for all $\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]$.\n\nThe polynomial $p_m(\\lambda)$ is chosen to solve the minimax problem:\n$$ \\min_{p_m \\in \\mathcal{P}_m} \\max_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} |1 - \\lambda p_m(\\lambda)| $$\nwhere $\\mathcal{P}_m$ is the space of polynomials of degree at most $m$. Let $q_{m+1}(\\lambda) = 1 - \\lambda p_m(\\lambda)$. Since $p_m(\\lambda)$ is a polynomial of degree $m$, $q_{m+1}(\\lambda)$ is a polynomial of degree at most $m+1$. Furthermore, evaluating at $\\lambda=0$, we find $q_{m+1}(0) = 1 - 0 \\cdot p_m(0) = 1$. The problem is therefore equivalent to finding the polynomial $q_{m+1}(\\lambda)$ of degree at most $m+1$ that satisfies $q_{m+1}(0)=1$ and has the smallest possible maximum absolute value on the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$.\n\nThis is a classic problem in approximation theory, whose solution is given by a scaled and shifted Chebyshev polynomial of the first kind, $T_k(x)$. The optimal polynomial is given by:\n$$ q_{m+1}(\\lambda) = \\frac{T_{m+1}\\left( \\frac{2\\lambda - (\\lambda_{\\max}+\\lambda_{\\min})}{\\lambda_{\\max}-\\lambda_{\\min}} \\right)}{T_{m+1}\\left( \\frac{-( \\lambda_{\\max}+\\lambda_{\\min})}{\\lambda_{\\max}-\\lambda_{\\min}} \\right)} $$\nThe linear transformation $x(\\lambda) = \\frac{2\\lambda - (\\lambda_{\\max}+\\lambda_{\\min})}{\\lambda_{\\max}-\\lambda_{\\min}}$ maps the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$ to $[-1, 1]$. The point $\\lambda=0$ is mapped to $x_0 = \\frac{-(\\lambda_{\\max}+\\lambda_{\\min})}{\\lambda_{\\max}-\\lambda_{\\min}} = -\\frac{\\kappa+1}{\\kappa-1}$.\n\n**1. Derivation of the uniform relative error $E_m$**\n\nThe uniform relative error is $E_m = \\max_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} |q_{m+1}(\\lambda)|$. Since $\\max_{x \\in [-1,1]} |T_{m+1}(x)| = 1$, we have:\n$$ E_m = \\frac{1}{\\left|T_{m+1}(x_0)\\right|} = \\frac{1}{\\left|T_{m+1}\\left(-\\frac{\\kappa+1}{\\kappa-1}\\right)\\right|} $$\nUsing the property $T_k(-x) = (-1)^k T_k(x)$, and noting that for $z>1$, $T_k(z)>0$, we get $|T_{m+1}(-\\frac{\\kappa+1}{\\kappa-1})| = T_{m+1}(\\frac{\\kappa+1}{\\kappa-1})$.\nFor arguments $|z| \\ge 1$, $T_k(z) = \\cosh(k \\cdot \\text{arccosh}(z))$. We need to evaluate $\\text{arccosh}(\\frac{\\kappa+1}{\\kappa-1})$.\nLet $\\alpha = \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}$. Then $\\frac{1}{\\alpha} = \\frac{\\sqrt{\\kappa}+1}{\\sqrt{\\kappa}-1}$. We can write $\\frac{\\kappa+1}{\\kappa-1} = \\frac{(\\sqrt{\\kappa}+1)^2+(\\sqrt{\\kappa}-1)^2}{(\\sqrt{\\kappa}+1)^2-(\\sqrt{\\kappa}-1)^2} = \\frac{\\alpha^{-2}+1}{\\alpha^{-2}-1}$, which is not the identity I was looking for.\nA more direct calculation:\n$$ \\frac{\\kappa+1}{\\kappa-1} = \\frac{\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}+1}{\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}-1} = \\frac{\\lambda_{\\max}+\\lambda_{\\min}}{\\lambda_{\\max}-\\lambda_{\\min}} $$\nA standard identity relates $\\text{arccosh}(\\frac{a+b}{a-b})$ to $\\text{arctanh}(\\sqrt{b/a})$. Another useful identity is $\\text{arccosh}(z) = \\ln(z+\\sqrt{z^2-1})$.\nLet $z = \\frac{\\kappa+1}{\\kappa-1}$. Then $z^2-1 = \\left(\\frac{\\kappa+1}{\\kappa-1}\\right)^2 - 1 = \\frac{(\\kappa+1)^2 - (\\kappa-1)^2}{(\\kappa-1)^2} = \\frac{4\\kappa}{(\\kappa-1)^2}$. So $\\sqrt{z^2-1} = \\frac{2\\sqrt{\\kappa}}{\\kappa-1}$.\n$$ \\text{arccosh}\\left(\\frac{\\kappa+1}{\\kappa-1}\\right) = \\ln\\left(\\frac{\\kappa+1}{\\kappa-1} + \\frac{2\\sqrt{\\kappa}}{\\kappa-1}\\right) = \\ln\\left(\\frac{\\kappa+2\\sqrt{\\kappa}+1}{\\kappa-1}\\right) = \\ln\\left(\\frac{(\\sqrt{\\kappa}+1)^2}{(\\sqrt{\\kappa}-1)(\\sqrt{\\kappa}+1)}\\right) = \\ln\\left(\\frac{\\sqrt{\\kappa}+1}{\\sqrt{\\kappa}-1}\\right) $$\nThis quantity is $-\\ln(\\alpha)$. Since we need a positive value for arccosh, we use $\\ln(1/\\alpha)$.\nSo, $T_{m+1}(\\frac{\\kappa+1}{\\kappa-1}) = \\cosh((m+1)\\ln(1/\\alpha))$. Using $\\cosh(x) = (\\exp(x)+\\exp(-x))/2$:\n$$ T_{m+1}\\left(\\frac{\\kappa+1}{\\kappa-1}\\right) = \\frac{\\exp((m+1)\\ln(1/\\alpha)) + \\exp(-(m+1)\\ln(1/\\alpha))}{2} = \\frac{(1/\\alpha)^{m+1} + \\alpha^{m+1}}{2} = \\frac{\\alpha^{-(m+1)} + \\alpha^{m+1}}{2} $$\nThe uniform relative error is therefore:\n$$ E_m = \\frac{1}{T_{m+1}\\left(\\frac{\\kappa+1}{\\kappa-1}\\right)} = \\frac{2}{\\alpha^{-(m+1)} + \\alpha^{m+1}} = \\frac{2\\alpha^{m+1}}{1+\\alpha^{2(m+1)}} $$\n\n**2. Derivation of the effective condition number $\\kappa_{\\mathrm{eff}}$**\n\nThe eigenvalues $\\lambda'$ of the preconditioned matrix $M_m A$ are given by $\\lambda'_i = \\lambda_i p_m(\\lambda_i) = 1-q_{m+1}(\\lambda_i)$. Since $\\lambda_i \\in [\\lambda_{\\min}, \\lambda_{\\max}]$, we have $q_{m+1}(\\lambda_i) \\in [-E_m, E_m]$.\nTherefore, the eigenvalues $\\lambda'$ are contained in the interval $[1-E_m, 1+E_m]$.\nThe preconditioned matrix $M_m A$ is symmetric. For it to be positive definite, its eigenvalues must be positive. This requires $1-E_m > 0$. Since $\\frac{\\kappa+1}{\\kappa-1}>1$, $T_{m+1}(\\frac{\\kappa+1}{\\kappa-1}) > 1$, which means $E_m < 1$, ensuring positive definiteness.\nThe effective condition number is the ratio of the largest to the smallest eigenvalue in this bounding interval:\n$$ \\kappa_{\\mathrm{eff}} = \\frac{\\max(\\lambda')}{\\min(\\lambda')} = \\frac{1+E_m}{1-E_m} $$\n\n**3. Derivation of the associated CG contraction factor parameter $\\alpha_{\\mathrm{eff}}$**\n\nThe standard worst-case convergence factor for CG on an SPD matrix with condition number $\\kappa$ is given by $\\alpha = (\\sqrt{\\kappa}-1)/(\\sqrt{\\kappa}+1)$. For the preconditioned system, the effective condition number is $\\kappa_{\\mathrm{eff}}$. The new contraction factor parameter is:\n$$ \\alpha_{\\mathrm{eff}} = \\frac{\\sqrt{\\kappa_{\\mathrm{eff}}}-1}{\\sqrt{\\kappa_{\\mathrm{eff}}}+1} = \\frac{\\sqrt{\\frac{1+E_m}{1-E_m}}-1}{\\sqrt{\\frac{1+E_m}{1-E_m}}+1} $$\nThis expression can be simplified by multiplying the numerator and denominator by $\\sqrt{1-E_m}$:\n$$ \\alpha_{\\mathrm{eff}} = \\frac{\\sqrt{1+E_m}-\\sqrt{1-E_m}}{\\sqrt{1+E_m}+\\sqrt{1-E_m}} $$\n\n**4. Determination of the minimal polynomial degree $m$**\n\nThe number of CG iterations, $k$, required to reduce the initial error by a factor $\\tau$ is approximately proportional to $1/(-\\ln(\\alpha))$. The problem asks to find the minimal degree $m$ such that the number of iterations is halved. This implies:\n$$ k_{\\mathrm{prec}} = \\frac{1}{2} k_{\\mathrm{unprec}} \\implies \\frac{1}{-\\ln(\\alpha_{\\mathrm{eff}})} \\approx \\frac{1}{2} \\frac{1}{-\\ln(\\alpha)} $$\nThis leads to the condition $\\ln(\\alpha_{\\mathrm{eff}}) = 2\\ln(\\alpha) = \\ln(\\alpha^2)$, and thus $\\alpha_{\\mathrm{eff}} = \\alpha^2$.\nWe now solve for $m$. Let's first express $E_m$ in terms of $\\alpha_{\\mathrm{eff}}$. From the expression for $\\alpha_{\\mathrm{eff}}$, we can solve for $E_m$. Rationalizing the expression gives:\n$$ \\alpha_{\\mathrm{eff}} = \\frac{(\\sqrt{1+E_m}-\\sqrt{1-E_m})^2}{(1+E_m)-(1-E_m)} = \\frac{1+E_m - 2\\sqrt{1-E_m^2} + 1-E_m}{2E_m} = \\frac{1-\\sqrt{1-E_m^2}}{E_m} $$\nSetting $\\alpha_{\\mathrm{eff}} = \\alpha^2$, we have $\\alpha^2 = \\frac{1-\\sqrt{1-E_m^2}}{E_m}$.\n$E_m \\alpha^2 = 1 - \\sqrt{1-E_m^2} \\implies 1-E_m \\alpha^2 = \\sqrt{1-E_m^2}$.\nSquaring both sides: $(1-E_m \\alpha^2)^2 = 1-E_m^2 \\implies 1 - 2E_m \\alpha^2 + E_m^2 \\alpha^4 = 1-E_m^2$.\nAssuming $E_m \\neq 0$, we can divide by $E_m$: $-2\\alpha^2 + E_m \\alpha^4 = -E_m \\implies E_m(1+\\alpha^4) = 2\\alpha^2$.\n$$ E_m = \\frac{2\\alpha^2}{1+\\alpha^4} $$\nWe now equate this required value of $E_m$ with the expression we derived for $E_m$ in terms of $m$:\n$$ \\frac{2\\alpha^{m+1}}{1+\\alpha^{2(m+1)}} = \\frac{2\\alpha^2}{1+\\alpha^4} $$\nLet $f(x) = \\frac{x}{1+x^2}$. The equation is $f(\\alpha^{m+1}) = f(\\alpha^2)$. For $x \\in (0,1)$, the function $f(x)$ is strictly increasing, since $f'(x) = \\frac{1-x^2}{(1+x^2)^2} > 0$. Thus, $f$ is injective on this domain. Since $\\alpha \\in(0,1)$, both $\\alpha^{m+1}$ and $\\alpha^2$ are in $(0,1)$.\nTherefore, the equality holds if and only if the arguments are equal:\n$$ \\alpha^{m+1} = \\alpha^2 $$\nSince $\\alpha \\in (0,1)$, we can conclude that the exponents must be equal:\n$$ m+1 = 2 \\implies m = 1 $$\nThe minimal polynomial degree required is $m=1$.\n\nThe explicit scaling laws requested are the exact relations derived above:\n1. $E_m = \\frac{2\\alpha^{m+1}}{1+\\alpha^{2(m+1)}}$, where $\\alpha = \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}$.\n2. $\\kappa_{\\mathrm{eff}} = \\frac{1+E_m}{1-E_m}$.\n3. $\\alpha_{\\mathrm{eff}} = \\frac{\\sqrt{\\kappa_{\\mathrm{eff}}}-1}{\\sqrt{\\kappa_{\\mathrm{eff}}}+1}$.\nThese three equations collectively relate $E_m$, $\\kappa_{\\mathrm{eff}}$, and $\\alpha_{\\mathrm{eff}}$ to the original problem parameters $\\kappa$ and $m$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "In many practical scenarios, a few outlier eigenvalues are the primary culprits for slow convergence, while the bulk of the spectrum is well-behaved. This exercise introduces an alternative, targeted approach to preconditioning that is highly effective in such cases . Instead of seeking a uniform approximation over the entire spectrum, you will construct a low-degree polynomial designed specifically to place roots at known outlier locations (approximated by Ritz values), thereby damping the associated error components.",
            "id": "3565786",
            "problem": "Consider a large, sparse, real symmetric positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$ whose spectrum is known from application context to be mostly contained in the interval $[0.8, 1.2]$, with a few outlier eigenvalues outside this bulk. A short run of the Lanczos process yields two approximate Ritz values $\\theta_{1} = 0.2$ and $\\theta_{2} = 1.8$ that indicate directions associated with slow convergence if untreated. You intend to apply a single pass of polynomial preconditioning in the form of a residual transform $r_{\\text{new}} = s(A) r_{\\text{old}}$ prior to launching a Krylov method such as the Conjugate Gradient (CG) method or the Generalized Minimal Residual (GMRES) method, where $s$ is a real polynomial. \n\nStarting from the following fundamental bases:\n- In a stationary residual transform induced by a polynomial $s$, the new residual is $r_{\\text{new}} = s(A) r_{\\text{old}}$, so the action on each eigencomponent is scalar multiplication by $s(\\lambda)$, where $\\lambda$ is an eigenvalue of $A$.\n- Ritz values computed by the Lanczos process approximate extremal eigenvalues and indicate directions where targeted damping (via roots of $s$ at those approximate eigenvalues) can be effective.\n- Choosing $s(0) = 1$ is consistent with a product of Richardson-like damping factors, $s(\\lambda) = \\prod_{j} (1 - \\alpha_{j} \\lambda)$, and avoids the trivial $s \\equiv 0$ that would correspond to a non-polynomial inverse.\n\nConstruct a degree-$3$ residual polynomial $s(\\lambda)$ that enforces $s(0) = 1$ and places simple roots at the two given Ritz values $\\theta_{1}$ and $\\theta_{2}$ to damp the associated directions. Use the remaining degree of freedom to determine $s$ by minimizing a discrete least-squares objective over two representative bulk points $\\lambda_{3} = 0.9$ and $\\lambda_{4} = 1.1$ with equal weights. That is, among polynomials of the form\n$$\ns(\\lambda) = \\bigl(1 - \\tfrac{\\lambda}{\\theta_{1}}\\bigr)\\bigl(1 - \\tfrac{\\lambda}{\\theta_{2}}\\bigr)\\bigl(1 + \\mu \\lambda\\bigr),\n$$\nchoose the scalar $\\mu \\in \\mathbb{R}$ that minimizes\n$$\nJ(\\mu) = \\bigl(s(0.9)\\bigr)^{2} + \\bigl(s(1.1)\\bigr)^{2}.\n$$\n\nExplain and justify your choice of multiplicities for the roots at $\\theta_{1}$ and $\\theta_{2}$ based on the degree budget and robustness to Ritz value inaccuracy. Then, compute and report the final closed-form expression for the optimal polynomial $s(\\lambda)$ as your answer. No rounding is required; provide an exact analytic expression.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the established theory of polynomial preconditioning for iterative methods in numerical linear algebra. It is well-posed, providing sufficient constraints to uniquely determine the polynomial, and is expressed in objective, unambiguous language. All necessary data are provided.\n\nThe problem asks for the construction of a degree-$3$ polynomial preconditioner $s(\\lambda)$ for a linear system involving a symmetric positive definite matrix $A$. The polynomial is to be used as a residual transform, $r_{\\text{new}} = s(A) r_{\\text{old}}$. The construction is guided by several constraints:\n$1$. The polynomial $s(\\lambda)$ must have degree $3$.\n$2$. It must satisfy the normalization condition $s(0) = 1$.\n$3$. It must have roots at the locations of two approximate outlier eigenvalues (Ritz values), $\\theta_{1} = 0.2$ and $\\theta_{2} = 1.8$, to damp the corresponding slowly converging eigenmodes.\n$4$. The remaining degree of freedom must be used to minimize the polynomial's magnitude over the bulk of the spectrum, approximated by two points $\\lambda_{3} = 0.9$ and $\\lambda_{4} = 1.1$.\n\nFirst, we address the choice of root multiplicities. The polynomial has a total degree of $3$. We are given two specific locations, $\\theta_{1}$ and $\\theta_{2}$, where roots are desired to suppress error components. Allocating a simple root to each of these locations, i.e., having factors $(\\lambda - \\theta_{1})$ and $(\\lambda - \\theta_{2})$, consumes two of the available three degrees. This is the most balanced approach as it treats both identified outlier modes equally. It leaves one remaining degree of freedom, which can then be used to further shape the polynomial, in this case, to minimize its magnitude over the interval where most eigenvalues are presumed to lie. Using a higher multiplicity, for example a double root at $\\theta_{1}$ and no root at $\\theta_{2}$, would be an unbalanced strategy, ignoring one of the known problematic modes. Furthermore, from a robustness standpoint, simple roots are often preferred. Ritz values are approximations to true eigenvalues. A higher-order root creates a very sharp and deep minimum in $|s(\\lambda)|$ at the Ritz value, which might be less effective if the Ritz value is not a highly accurate approximation of the true eigenvalue. A broader null provided by simple roots can be more forgiving to inaccuracies in the Ritz values.\n\nThe problem proposes a form for $s(\\lambda)$ that already incorporates the root conditions and the normalization $s(0)=1$:\n$$\ns(\\lambda) = \\left(1 - \\frac{\\lambda}{\\theta_{1}}\\right)\\left(1 - \\frac{\\lambda}{\\theta_{2}}\\right)\\left(1 + \\mu \\lambda\\right)\n$$\nThis is a degree-$3$ polynomial. Evaluating at $\\lambda=0$ gives $s(0)=(1-0)(1-0)(1+0) = 1$, satisfying the normalization. The first two factors ensure that $s(\\theta_{1}) = 0$ and $s(\\theta_{2}) = 0$. The parameter $\\mu$ corresponds to the remaining degree of freedom.\n\nWe are given the values $\\theta_{1} = 0.2$ and $\\theta_{2} = 1.8$. Substituting these into the expression for $s(\\lambda)$:\n$$\ns(\\lambda) = \\left(1 - \\frac{\\lambda}{0.2}\\right)\\left(1 - \\frac{\\lambda}{1.8}\\right)\\left(1 + \\mu \\lambda\\right) = (1 - 5\\lambda)\\left(1 - \\frac{10\\lambda}{18}\\right)(1 + \\mu\\lambda) = (1 - 5\\lambda)\\left(1 - \\frac{5}{9}\\lambda\\right)(1 + \\mu\\lambda)\n$$\nThe parameter $\\mu$ is to be chosen to minimize the objective function $J(\\mu) = (s(0.9))^{2} + (s(1.1))^{2}$. We evaluate $s(\\lambda)$ at the points $\\lambda = 0.9$ and $\\lambda = 1.1$.\n\nFor $\\lambda = 0.9$:\n$$\ns(0.9) = \\left(1 - 5(0.9)\\right)\\left(1 - \\frac{5}{9}(0.9)\\right)\\left(1 + \\mu(0.9)\\right) = (1 - 4.5)(1 - 0.5)(1 + 0.9\\mu) = (-3.5)(0.5)(1 + 0.9\\mu) = -1.75(1 + 0.9\\mu)\n$$\nWriting the coefficients as fractions, $-1.75 = -\\frac{7}{4}$ and $0.9 = \\frac{9}{10}$. So, $s(0.9) = -\\frac{7}{4}(1 + \\frac{9}{10}\\mu)$.\n\nFor $\\lambda = 1.1$:\n$$\ns(1.1) = \\left(1 - 5(1.1)\\right)\\left(1 - \\frac{5}{9}(1.1)\\right)\\left(1 + \\mu(1.1)\\right) = (1 - 5.5)\\left(1 - \\frac{5.5}{9}\\right)(1 + 1.1\\mu)\n$$\n$$\ns(1.1) = (-4.5)\\left(\\frac{9 - 5.5}{9}\\right)(1 + 1.1\\mu) = (-4.5)\\left(\\frac{3.5}{9}\\right)(1 + 1.1\\mu) = (-0.5)(3.5)(1 + 1.1\\mu) = -1.75(1 + 1.1\\mu)\n$$\nUsing fractions, $-1.75 = -\\frac{7}{4}$ and $1.1 = \\frac{11}{10}$. So, $s(1.1) = -\\frac{7}{4}(1 + \\frac{11}{10}\\mu)$.\n\nThe objective function is:\n$$\nJ(\\mu) = \\left(-\\frac{7}{4}\\left(1 + \\frac{9}{10}\\mu\\right)\\right)^{2} + \\left(-\\frac{7}{4}\\left(1 + \\frac{11}{10}\\mu\\right)\\right)^{2} = \\left(\\frac{7}{4}\\right)^{2} \\left[ \\left(1 + \\frac{9}{10}\\mu\\right)^{2} + \\left(1 + \\frac{11}{10}\\mu\\right)^{2} \\right]\n$$\nTo find the minimum, we can ignore the positive constant factor $(\\frac{7}{4})^{2}$ and minimize the function $\\tilde{J}(\\mu) = (1 + \\frac{9}{10}\\mu)^{2} + (1 + \\frac{11}{10}\\mu)^{2}$. We find the critical point by setting the derivative with respect to $\\mu$ to zero:\n$$\n\\frac{d\\tilde{J}}{d\\mu} = 2\\left(1 + \\frac{9}{10}\\mu\\right)\\left(\\frac{9}{10}\\right) + 2\\left(1 + \\frac{11}{10}\\mu\\right)\\left(\\frac{11}{10}\\right) = 0\n$$\nDividing by $2$:\n$$\n\\frac{9}{10}\\left(1 + \\frac{9}{10}\\mu\\right) + \\frac{11}{10}\\left(1 + \\frac{11}{10}\\mu\\right) = 0\n$$\n$$\n\\frac{9}{10} + \\frac{81}{100}\\mu + \\frac{11}{10} + \\frac{121}{100}\\mu = 0\n$$\nCombining terms:\n$$\n\\left(\\frac{9}{10} + \\frac{11}{10}\\right) + \\left(\\frac{81}{100} + \\frac{121}{100}\\right)\\mu = 0\n$$\n$$\n\\frac{20}{10} + \\frac{202}{100}\\mu = 0\n$$\n$$\n2 + \\frac{202}{100}\\mu = 0\n$$\n$$\n\\frac{202}{100}\\mu = -2 \\implies \\mu = -2 \\cdot \\frac{100}{202} = -\\frac{200}{202} = -\\frac{100}{101}\n$$\nThe second derivative is $\\frac{d^{2}\\tilde{J}}{d\\mu^{2}} = 2(\\frac{9}{10})^2 + 2(\\frac{11}{10})^2 > 0$, confirming that this value of $\\mu$ corresponds to a minimum.\n\nNow we substitute this optimal value of $\\mu = -\\frac{100}{101}$ back into the expression for $s(\\lambda)$:\n$$\ns(\\lambda) = (1 - 5\\lambda)\\left(1 - \\frac{5}{9}\\lambda\\right)\\left(1 - \\frac{100}{101}\\lambda\\right)\n$$\nThis is the final closed-form expression for the optimal degree-$3$ residual polynomial that satisfies all the given conditions.",
            "answer": "$$\\boxed{s(\\lambda) = (1 - 5\\lambda)\\left(1 - \\frac{5}{9}\\lambda\\right)\\left(1 - \\frac{100}{101}\\lambda\\right)}$$"
        }
    ]
}