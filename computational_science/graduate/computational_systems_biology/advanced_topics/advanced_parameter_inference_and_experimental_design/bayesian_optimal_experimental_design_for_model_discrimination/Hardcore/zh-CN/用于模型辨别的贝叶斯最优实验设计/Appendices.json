{
    "hands_on_practices": [
        {
            "introduction": "为了区分相互竞争的科学模型，我们需要设计信息量最大的实验。这项首个练习 () 介绍了贝叶斯优化实验设计中的核心效用函数——互信息，并指导您针对一个假设的生物钟系统直接计算该效用。通过实现这一过程，您将对如何量化实验的期望价值建立起基础性的理解。",
            "id": "3290038",
            "problem": "考虑一个细胞群体中生物钟生物发光报告基因的两个竞争性机理模型：一个无转录反馈抑制的模型和一个有转录反馈抑制的模型。设二元模型指示器为 $M \\in \\{0,1\\}$，其中 $M=0$ 表示无抑制模型，$M=1$ 表示有抑制模型。一个光暗（LD）循环由一个设计向量 $d=(T,\\delta)$ 指定，其中 $T$ 是LD周期（单位为小时），$\\delta \\in [0,1]$ 是占空比（光照时间占周期的比例，以小数表示）。实验产生一个平均生物发光读数 $Y$，该读数是在固定的LD循环 $d$ 下，对 $n$ 次重复测量取样本均值得到的，每次重复都是对潜在的确定性平均响应的带噪声观测。\n\n假设模型的先验为 $p(M)$，其中 $p(M=0)=p_0$，$p(M=1)=1-p_0$。假设一个与中心极限定理一致的高斯噪声模型用于样本均值：在给定 $M$ 和 $d$ 的条件下，读数 $Y$ 服从高斯分布，其均值为 $\\mu_M(T,\\delta)$，方差为 $\\sigma^2/n$，即 $Y \\mid M,d \\sim \\mathcal{N}(\\mu_M(T,\\delta), \\sigma^2/n)$，其中 $\\sigma^2$ 是重复测量层面的方差。两个模型的平均响应编码了对生物钟共振的同步以及对光的不同敏感性：\n- 共振因子为 $R(T) = \\exp\\left(-\\frac{1}{2}\\left(\\frac{T-\\tau_0}{\\sigma_T}\\right)^2\\right)$，其中 $\\tau_0$ 是内源周期（单位为小时），$\\sigma_T$ 是一个尺度参数（单位为小时），表示同步共振的宽度。\n- 对于 $M=0$（无抑制），均值为 $\\mu_0(T,\\delta) = A_0 \\, R(T) \\, \\delta$，其中 $A_0$ 是一个敏感性参数（单位任意）。\n- 对于 $M=1$（有抑制），均值为 $\\mu_1(T,\\delta) = A_1 \\, R(T) \\, \\frac{\\delta}{1 + k \\delta}$，其中 $A_1$ 是一个敏感性参数（单位任意），$k>0$ 控制抑制强度（无量纲）。\n\n设实验设计目标为期望信息增益，在贝叶斯优化实验设计中，这即为模型指示器 $M$ 和观测读数 $Y$ 在给定 $d$ 下的互信息（MI），记为 $U(d)=I(M;Y \\mid d)$。该量由核心的香农互信息恒等式定义：\n$$\nI(M;Y \\mid d) = \\mathbb{E}_{p(m,y \\mid d)}\\left[\\log \\frac{p(m \\mid y,d)}{p(m)}\\right],\n$$\n其中 $p(m,y \\mid d) = p(m) \\, p(y \\mid m,d)$，$p(m \\mid y,d)$ 表示在给定观测读数和设计下的模型后验。\n\n您的任务是实现一个程序，对于每个提供的参数集，为一组有限的候选设计 $d=(T,\\delta)$ 评估目标 $U(d)$，并选择使 $U(d)$ 最大的设计。程序必须根据互信息的基本定义精确计算 $U(d)$，不使用任何快捷公式，通过对由 $M$ 的先验和条件分布 $p(y \\mid m,d)$ 导出的预测分布 $p(y \\mid d)$ 进行正确的边缘化，并按要求对 $y$ 取期望。程序应使用科学上现实的参数值，采用数值稳定的方法计算所需的期望，并遵守下文的单位和输出格式规范。\n\n单位和量：\n- LD周期 $T$ 必须以小时表示。\n- 占空比 $\\delta$ 必须以小数表示（无百分号）。\n- 互信息 $U(d)$ 必须使用自然对数计算，因此以奈特（nats）为单位。\n- 最终报告的 $T$ 值单位为小时，最终报告的 $U(d)$ 值单位为奈特。\n- 在最终输出中，将 $T$ 和 $\\delta$ 四舍五入到三位小数，将 $U(d)$ 四舍五入到六位小数。\n\n测试套件：\n对于每个测试用例，给定 $(p_0, \\tau_0, \\sigma_T, A_0, A_1, k, \\sigma, n)$ 和一个由指定的 $T$ 值（单位为小时）和占空比（以小数表示）列表构建的有限候选集 $\\mathcal{D} = \\{(T_i, \\delta_j)\\}$。对每个用例，为所有 $d \\in \\mathcal{D}$ 评估 $U(d)$，然后返回具有最大 $U(d)$ 的单个设计以及该最大值 $U(d)$。\n\n- 测试用例 $1$（一般情况，平衡先验）：\n  - 参数：$p_0=0.5$, $\\tau_0=24.0 \\, \\text{小时}$, $\\sigma_T=2.0 \\, \\text{小时}$, $A_0=1.0$, $A_1=1.2$, $k=2.0$, $\\sigma=0.2$, $n=50$。\n  - 候选周期：$T \\in \\{20.0, 24.0, 28.0\\}$ 小时。\n  - 候选占空比：$\\delta \\in \\{0.3, 0.5, 0.8\\}$。\n- 测试用例 $2$（边界占空比，弱光条件）：\n  - 参数：$p_0=0.5$, $\\tau_0=24.0 \\, \\text{小时}$, $\\sigma_T=2.0 \\, \\text{小时}$, $A_0=1.0$, $A_1=1.2$, $k=2.0$, $\\sigma=0.2$, $n=50$。\n  - 候选周期：$T \\in \\{22.0, 24.0, 26.0\\}$ 小时。\n  - 候选占空比：$\\delta \\in \\{0.01, 0.05, 0.1\\}$。\n- 测试用例 $3$（模型几乎无法区分）：\n  - 参数：$p_0=0.5$, $\\tau_0=24.0 \\, \\text{小时}$, $\\sigma_T=2.0 \\, \\text{小时}$, $A_0=1.0$, $A_1=1.0$, $k=0.0$, $\\sigma=0.2$, $n=50$。\n  - 候选周期：$T \\in \\{20.0, 24.0, 28.0\\}$ 小时。\n  - 候选占空比：$\\delta \\in \\{0.3, 0.5, 0.8\\}$。\n\n最终输出规范：\n- 对于每个测试用例，返回三元组 $[T^\\star, \\delta^\\star, U^\\star]$，其中 $(T^\\star, \\delta^\\star)$ 是在候选集中使 $U(d)$ 最大化的设计（单位如上所述），$U^\\star = \\max_{d \\in \\mathcal{D}} U(d)$（单位为奈特）。\n- 您的程序应生成单行输出，包含三个结果，格式完全如下：一个由方括号括起来的逗号分隔列表，其中每个元素本身是一个列表 $[T^\\star,\\delta^\\star,U^\\star]$，并应用了舍入规则，例如 $\\big[ [T^\\star_1,\\delta^\\star_1,U^\\star_1],[T^\\star_2,\\delta^\\star_2,U^\\star_2],[T^\\star_3,\\delta^\\star_3,U^\\star_3] \\big]$。",
            "solution": "该问题要求从一个有限的候选集 $\\mathcal{D}$ 中确定一个最优实验设计 $d^{\\star}=(T^{\\star},\\delta^{\\star})$。最优性准则为最大化期望信息增益，这等价于在给定设计 $d=(T,\\delta)$ 的情况下，模型指示器 $M$ 和实验读数 $Y$ 之间的香农互信息 $U(d) = I(M;Y \\mid d)$。\n\n互信息的基本定义由期望给出\n$$\nU(d) = I(M;Y \\mid d) = \\mathbb{E}_{p(m,y \\mid d)}\\left[\\log \\frac{p(m \\mid y,d)}{p(m)}\\right]\n$$\n该期望是对模型指示器 $M$ 和数据 $Y$ 的联合分布 $p(m,y \\mid d) = p(m) p(y \\mid m, d)$ 取的。我们可以将期望展开为对离散模型变量 $M \\in \\{0,1\\}$ 的求和以及对连续数据变量 $Y$ 的积分：\n$$\nU(d) = \\sum_{m \\in \\{0,1\\}} \\int_{-\\infty}^{\\infty} p(m) p(y \\mid m, d) \\log \\frac{p(m \\mid y,d)}{p(m)} \\, dy\n$$\n使用后验概率的定义 $p(m \\mid y,d) = \\frac{p(y \\mid m,d) p(m)}{p(y \\mid d)}$，其中 $p(y \\mid d) = \\sum_{m'} p(m') p(y \\mid m', d)$ 是数据的边缘预测分布，我们可以重写对数中的项：\n$$\n\\frac{p(m \\mid y,d)}{p(m)} = \\frac{p(y \\mid m,d) p(m)}{p(y \\mid d) p(m)} = \\frac{p(y \\mid m,d)}{p(y \\mid d)}\n$$\n将此代入 $U(d)$ 的表达式中得到：\n$$\nU(d) = \\sum_{m \\in \\{0,1\\}} p(m) \\int_{-\\infty}^{\\infty} p(y \\mid m, d) \\log \\frac{p(y \\mid m,d)}{p(y \\mid d)} \\, dy\n$$\n这种形式揭示了互信息是从边缘预测分布 $p(y \\mid d)$ 到模型条件分布 $p(y \\mid m, d)$ 的期望库尔贝克-莱布勒散度，该期望是在先验模型概率上平均的。\n\n此问题的具体组成部分是：\n1.  **先验模型概率**：$p(M=0) = p_0$ 和 $p(M=1) = 1-p_0$。\n2.  **条件数据分布（似然）**：读数 $Y$ 是 $n$ 次重复测量的样本均值。根据中心极限定理，其分布近似为高斯分布。问题将其指定为精确的：$Y \\mid M=m, d \\sim \\mathcal{N}(y; \\mu_m(d), \\sigma^2/n)$。我们将有效方差记为 $\\sigma_{\\text{eff}}^2 = \\sigma^2/n$。平均响应为：\n    $$\n    \\mu_0(d) = \\mu_0(T,\\delta) = A_0 \\, R(T) \\, \\delta\n    $$\n    $$\n    \\mu_1(d) = \\mu_1(T,\\delta) = A_1 \\, R(T) \\, \\frac{\\delta}{1 + k \\delta}\n    $$\n    其中共振因子为 $R(T) = \\exp\\left(-\\frac{1}{2}\\left(\\frac{T-\\tau_0}{\\sigma_T}\\right)^2\\right)$。\n3.  **边缘预测分布**：这是一个高斯混合模型（GMM）：\n    $$\n    p(y \\mid d) = p_0 \\cdot \\mathcal{N}(y; \\mu_0(d), \\sigma_{\\text{eff}}^2) + (1-p_0) \\cdot \\mathcal{N}(y; \\mu_1(d), \\sigma_{\\text{eff}}^2)\n    $$\n\n目标函数可以写成两项之和：\n$U(d) = p_0 \\cdot \\text{Term}_0 + (1-p_0) \\cdot \\text{Term}_1$，其中\n$$\n\\text{Term}_0 = \\int_{-\\infty}^{\\infty} \\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2) \\log \\frac{\\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2)}{p(y \\mid d)} \\, dy = \\mathbb{E}_{Y \\sim \\mathcal{N}(\\mu_0, \\sigma_{\\text{eff}}^2)}\\left[ \\log \\frac{\\mathcal{N}(Y; \\mu_0, \\sigma_{\\text{eff}}^2)}{p(Y \\mid d)} \\right]\n$$\n$$\n\\text{Term}_1 = \\int_{-\\infty}^{\\infty} \\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2) \\log \\frac{\\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2)}{p(y \\mid d)} \\, dy = \\mathbb{E}_{Y \\sim \\mathcal{N}(\\mu_1, \\sigma_{\\text{eff}}^2)}\\left[ \\log \\frac{\\mathcal{N}(Y; \\mu_1, \\sigma_{\\text{eff}}^2)}{p(Y \\mid d)} \\right]\n$$\n这些积分没有通用的闭式解，必须进行数值计算。一种用于计算形如 $\\int_{-\\infty}^{\\infty} e^{-x^2}g(x)dx$ 的积分的标准且数值稳定的方法是高斯-埃尔米特求积。上述期望的形式为 $\\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}[f(\\mu + \\sigma Z)]$，可以使用求积法进行评估。我们使用概率论学家的埃尔米特多项式，它们对于权重函数 $e^{-z^2/2}$ 是正交的，这与标准正态分布的核相匹配。期望 $\\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}[f(Z)] = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} f(z) dz$ 可以近似为 $\\frac{1}{\\sqrt{2\\pi}} \\sum_{i=1}^{N_q} w_i f(z_i)$，其中 $(z_i, w_i)$ 是针对权重函数 $e^{-z^2/2}$ 的求积点和权重。\n\n让我们定义期望内的被积函数。对于 $\\text{Term}_0$，对数的参数是：\n$$\n\\frac{\\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2)}{p_0 \\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2) + (1-p_0) \\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2)} = \\frac{1}{p_0 + (1-p_0) \\frac{\\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2)}{\\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2)}}\n$$\n高斯概率密度函数的比值可以简化为：\n$$\n\\frac{\\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2)}{\\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2)} = \\exp\\left( \\frac{(\\mu_1-\\mu_0)(y - (\\mu_0+\\mu_1)/2)}{\\sigma_{\\text{eff}}^2} \\right)\n$$\n这种表述方式通过避免计算单个PDF值时可能出现的下溢或上溢，显著提高了数值稳定性。然后使用求积法对包含此指数项的函数进行计算，从而得到期望值。\n\n当模型的参数无法区分时，会出现一个重要的特殊情况。在测试用例3中，参数 $A_1=1.0$ 和 $k=0.0$ 导致模型 $M=1$ 的平均响应与模型 $M=0$ 的完全相同：\n$$\n\\mu_1(d) = A_1 R(T) \\frac{\\delta}{1+k\\delta} = 1.0 \\cdot R(T) \\frac{\\delta}{1+0 \\cdot \\delta} = R(T) \\delta = A_0 R(T) \\delta = \\mu_0(d)\n$$\n当 $\\mu_0(d) = \\mu_1(d)$ 时，条件分布 $p(y \\mid M=0,d)$ 和 $p(y \\mid M=1,d)$ 是相同的。因此，边缘分布 $p(y \\mid d)$ 也是同一个分布。比率 $\\frac{p(y \\mid m,d)}{p(y \\mid d)}$ 变为 $1$，其对数为 $0$，因此互信息 $U(d)$ 必然为 $0$。任何实验都无法区分两个相同的模型，所以期望信息增益为零。\n\n总体算法如下：\n1.  对于每个测试用例，定义模型参数和候选设计集 $\\mathcal{D}$。\n2.  初始化变量以存储最优设计 $(T^{\\star}, \\delta^{\\star})$ 和最大效用 $U^{\\star}=-\\infty$。\n3.  对于 $\\mathcal{D}$ 中的每个设计 $d=(T, \\delta)$：\n    a. 计算平均响应 $\\mu_0(d)$ 和 $\\mu_1(d)$。\n    b. 如果 $\\mu_0(d) = \\mu_1(d)$，则 $U(d) = 0$。否则，继续。\n    c. 使用足够高阶的高斯-埃尔米特求积法计算两个期望积分以确保精度。\n    d. 计算 $U(d) = p_0 \\cdot \\text{Term}_0 + (1-p_0) \\cdot \\text{Term}_1$。\n    e. 如果 $U(d) > U^{\\star}$，则更新 $U^{\\star} = U(d)$，$T^{\\star} = T$，和 $\\delta^{\\star} = \\delta$。\n4.  在评估完 $\\mathcal{D}$ 中的所有设计后，最终的 $(T^{\\star}, \\delta^{\\star}, U^{\\star})$ 即为该测试用例的结果。\n5.  将所有测试用例的结果整理成指定的输出格式，并按要求对值进行四舍五入。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import roots_hermitenorm\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian optimal experimental design problem for model discrimination.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"params\": (0.5, 24.0, 2.0, 1.0, 1.2, 2.0, 0.2, 50),\n            \"T_candidates\": [20.0, 24.0, 28.0],\n            \"delta_candidates\": [0.3, 0.5, 0.8],\n        },\n        {\n            \"params\": (0.5, 24.0, 2.0, 1.0, 1.2, 2.0, 0.2, 50),\n            \"T_candidates\": [22.0, 24.0, 26.0],\n            \"delta_candidates\": [0.01, 0.05, 0.1],\n        },\n        {\n            \"params\": (0.5, 24.0, 2.0, 1.0, 1.0, 0.0, 0.2, 50),\n            \"T_candidates\": [20.0, 24.0, 28.0],\n            \"delta_candidates\": [0.3, 0.5, 0.8],\n        },\n    ]\n\n    all_results = []\n    \n    # Quadrature points and weights for numerical integration\n    # Using probabilist's Hermite polynomials (weight func exp(-x^2/2))\n    # A degree of 100 provides high accuracy.\n    quad_deg = 100\n    z_i, w_i = roots_hermitenorm(quad_deg)\n\n    for case in test_cases:\n        p0, tau0, sigma_T, A0, A1, k, sigma, n = case[\"params\"]\n        T_candidates = case[\"T_candidates\"]\n        delta_candidates = case[\"delta_candidates\"]\n\n        best_T = -1.0\n        best_delta = -1.0\n        max_U = -1.0\n\n        sigma_eff_sq = sigma**2 / n\n        sigma_eff = np.sqrt(sigma_eff_sq)\n\n        for T in T_candidates:\n            for delta in delta_candidates:\n                # Calculate resonance factor\n                R_T = np.exp(-0.5 * ((T - tau0) / sigma_T)**2)\n\n                # Calculate mean responses for the two models\n                mu0 = A0 * R_T * delta\n                mu1 = A1 * R_T * (delta / (1.0 + k * delta))\n                \n                # If models are indistinguishable, information gain is zero\n                if np.isclose(mu0, mu1):\n                    U_d = 0.0\n                else:\n                    # Define integrands for expectation calculation\n                    # The value y for the integrands will be sampled from the respective Gaussians\n                    # via change of variables in the quadrature.\n                    \n                    # Log-integrand for the expectation with respect to model 0\n                    def log_integrand_0(y):\n                        exponent = ((mu1 - mu0) * (y - (mu0 + mu1) / 2.0)) / sigma_eff_sq\n                        # Use log-sum-exp trick for stability, though direct computation is fine here\n                        # log(1 / (p0 + (1-p0) * exp(exponent))) = -log(p0 + (1-p0) * exp(exponent))\n                        return -np.log(p0 + (1.0 - p0) * np.exp(exponent))\n\n                    # Log-integrand for the expectation with respect to model 1\n                    def log_integrand_1(y):\n                        exponent = ((mu0 - mu1) * (y - (mu0 + mu1) / 2.0)) / sigma_eff_sq\n                        return -np.log(p0 * np.exp(exponent) + (1.0 - p0))\n\n                    # Perform numerical integration using Gauss-Hermite quadrature\n                    # E[f(Y)] where Y ~ N(mu, sigma^2) is E[f(mu + sigma*Z)] where Z ~ N(0,1)\n                    # The quadrature points z_i are effectively samples of Z.\n                    \n                    # Expectation w.r.t. model 0\n                    y_samples_0 = mu0 + sigma_eff * z_i\n                    integrand_vals_0 = log_integrand_0(y_samples_0)\n                    term_0 = np.sum(w_i * integrand_vals_0) / np.sqrt(2.0 * np.pi)\n\n                    # Expectation w.r.t. model 1\n                    y_samples_1 = mu1 + sigma_eff * z_i\n                    integrand_vals_1 = log_integrand_1(y_samples_1)\n                    term_1 = np.sum(w_i * integrand_vals_1) / np.sqrt(2.0 * np.pi)\n                    \n                    U_d = p0 * term_0 + (1.0 - p0) * term_1\n\n                if U_d > max_U:\n                    max_U = U_d\n                    best_T = T\n                    best_delta = delta\n\n        # Round to specified precision for output\n        T_star = round(best_T, 3)\n        delta_star = round(best_delta, 3)\n        U_star = round(max_U, 6)\n        \n        all_results.append(f\"[{T_star:.3f},{delta_star:.3f},{U_star:.6f}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现实世界中的实验总是受到资源的限制，这迫使我们在数据质量和数量之间做出权衡。本练习 () 在单分子成像 (smFISH) 的背景下探讨了这种权衡，您必须决定如何将固定预算分配于提高测量灵敏度和增加分析的细胞数量之间。它还提出了一个常见的挑战：区分那些预测均值相同但噪声水平不同的模型，这要求您关注更高阶的统计矩。",
            "id": "3290026",
            "problem": "考虑对单细胞中通过单分子荧光原位杂交 (smFISH) 测量的转录本计数，在两个相互竞争的观测模型之间进行判别。这两个模型在期望计数上一致，但在方差上有所不同。设随机选择的细胞的潜在真实转录本计数为 $X$，其总体均值为 $\\mu$。smFISH 测量被建模为分子计数灵敏度为 $q \\in (0,1)$ 的二项式稀疏，因此观测计数 $Y$ 是检测到的转录本数量。两个候选的 $Y$ 观测模型是：\n\n- 模型 $\\mathcal{M}_P$（泊松稀疏）：$X \\sim \\mathrm{Poisson}(\\mu)$ 且 $Y \\mid X \\sim \\mathrm{Binomial}(X,q)$，这意味着 $Y \\sim \\mathrm{Poisson}(q\\mu)$，其中 $\\mathbb{E}[Y] = q\\mu$ 且 $\\mathrm{Var}[Y] = q\\mu$。\n- 模型 $\\mathcal{M}_{NB}$（负二项稀疏）：$X \\sim \\mathrm{Negative\\ Binomial}(\\mu,k)$，其中 $k$ 是离散（大小）参数，且 $Y \\mid X \\sim \\mathrm{Binomial}(X,q)$。在负二项 (NB) 模型下，通过泊松-伽马混合稀疏，$Y$ 仍然服从 NB 分布，其中 $\\mathbb{E}[Y] = q\\mu$ 且 $\\mathrm{Var}[Y] = q\\mu + \\dfrac{q^2\\mu^2}{k}$。\n\n假设先验概率相等，即 $\\pi(\\mathcal{M}_P) = \\pi(\\mathcal{M}_{NB}) = 1/2$。对于单个细胞，模型的边缘预测分布为：\n- 在模型 $\\mathcal{M}_P$ 下，概率质量函数 (pmf) 为 $p_P(y \\mid q,\\mu) = \\dfrac{e^{-q\\mu}(q\\mu)^y}{y!}$，对于 $y \\in \\{0,1,2,\\dots\\}$。\n- 在模型 $\\mathcal{M}_{NB}$ 下，均值为 $q\\mu$，离散参数为 $k$ 时，其 pmf 为\n$$\np_{NB}(y \\mid q,\\mu,k) = \\frac{\\Gamma(y+k)}{\\Gamma(k)\\,\\Gamma(y+1)}\\left(\\frac{k}{k+q\\mu}\\right)^{k}\\left(\\frac{q\\mu}{k+q\\mu}\\right)^{y},\\quad y \\in \\{0,1,2,\\dots\\}.\n$$\n\n您需要分配独立重复实验的次数 $N \\in \\mathbb{N}$（测量的细胞数），并从一个离散网格中选择分子计数灵敏度 $q$，以在预算约束下最大化一个贝叶斯模型判别准则。总成本为\n$$\nC(N,q) = c_r N + c_s\\, \\frac{q}{1-q},\n$$\n其中 $c_r$ 是单次重复实验的成本，$c_s \\frac{q}{1-q}$ 模拟了实现更高灵敏度 $q$（例如，更多的探针组）所带来的急剧增加的成本。给定预算 $B$，可行设计是那些满足 $C(N,q) \\le B$ 和 $N \\ge 1$ 的设计。\n\n作为模型判别的贝叶斯效用，使用模型指示符 $M \\in \\{\\mathcal{M}_P,\\mathcal{M}_{NB}\\}$ 和数据 $Y$ 在相等先验下的互信息，对于单次重复实验，这等于 $p_P$ 和 $p_{NB}$ 之间的 Jensen-Shannon 散度：\n$$\nI_1(q) = \\sum_{y=0}^{\\infty} \\left[ \\frac{1}{2}\\, p_P(y\\mid q,\\mu) \\log \\frac{p_P(y\\mid q,\\mu)}{\\frac{1}{2}p_P(y\\mid q,\\mu)+\\frac{1}{2}p_{NB}(y\\mid q,\\mu,k)} + \\frac{1}{2}\\, p_{NB}(y\\mid q,\\mu,k) \\log \\frac{p_{NB}(y\\mid q,\\mu,k)}{\\frac{1}{2}p_P(y\\mid q,\\mu)+\\frac{1}{2}p_{NB}(y\\mid q,\\mu,k)} \\right].\n$$\n对于 $N$ 次独立重复实验，互信息线性增长：\n$$\nI_N(q) = N\\, I_1(q).\n$$\n\n您的任务是编写一个完整、可运行的程序。对于每个指定的测试用例，该程序应在提供的 $q$ 值网格上进行搜索，通过对级数求和来计算 $I_1(q)$，求和上限应足够大以确保尾部概率可忽略不计，然后在给定预算和选定的 $q$ 的情况下，选择 $N$ 为最大可行整数（因为 $I_N(q)$ 随 $N$ 线性增加）。对于每个测试用例，程序应输出三元组 $[N^*, q^*, I_N(q^*)]$，其中 $[N^*, q^*, I_N(q^*)]$ 是在可行 $q$ 网格中，满足 $C(N,q) \\le B$ 和 $N \\ge 1$ 约束下 $I_N(q)$ 的最大化者。\n\n使用以下测试套件。在每种情况下，$q$ 必须从网格 $\\{0.3,0.5,0.7,0.85,0.95\\}$ 中选择。\n\n- 测试用例 1：$B=50$, $c_r=1.0$, $c_s=10.0$, $\\mu=8.0$, $k=20.0$。\n- 测试用例 2：$B=8$, $c_r=1.0$, $c_s=10.0$, $\\mu=8.0$, $k=20.0$。\n- 测试用例 3：$B=50$, $c_r=2.0$, $c_s=5.0$, $\\mu=5.0$, $k=2.0$。\n- 测试用例 4：$B=50$, $c_r=1.0$, $c_s=10.0$, $\\mu=10.0$, $k=10^6$。\n- 测试用例 5：$B=50$, $c_r=0.5$, $c_s=50.0$, $\\mu=12.0$, $k=5.0$。\n\n算法要求：\n- 为保证数值稳定性，请使用自然对数和对数伽马函数计算 pmf 的对数，然后取幂以获得概率。\n- 将 $y$ 的无穷和截断在 $y_{\\max}$，其选择方式为\n$$\ny_{\\max} = \\left\\lceil q\\mu + 10\\, \\sqrt{\\max\\left(q\\mu, q\\mu + \\frac{q^2\\mu^2}{k}\\right)} + 10 \\right\\rceil\n$$\n并且强制要求 $y_{\\max} \\ge 100$，以确保所有网格值的尾部贡献可以忽略不计。\n- 对于每个可行的 $q$，计算 $N_{\\max} = \\left\\lfloor \\dfrac{B - c_s\\, \\frac{q}{1-q}}{c_r} \\right\\rfloor$。如果 $N_{\\max} \\ge 1$，则设 $N=N_{\\max}$；否则，将该 $q$ 视为不可行而舍弃。\n- 在可行的 $q$ 中，选择具有最大 $I_N(q)$ 的设计 $[N^*, q^*, I_N(q^*)]$，若出现平局，则优先选择较大的 $N$，其次选择较大的 $q$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个元素本身是对应测试用例（按上述顺序）的结果列表，形式为 $[N^*, q^*, I_N(q^*)]$。\n- 将 $N^*$ 打印为整数，$q^*$ 四舍五入到三位小数，$I_N(q^*)$ 四舍五入到六位小数。",
            "solution": "该问题要求我们找到最优的实验设计，该设计由重复实验次数（细胞数）$N$ 和测量灵敏度 $q$ 定义，以便在两个相互竞争的单细胞转录本计数统计模型之间进行判别。该设计必须在总预算约束下最大化一个贝叶斯效用函数。\n\n问题的核心在于用于模型判别的贝叶斯最优实验设计 (OED)。效用函数是模型指示符 $M \\in \\{\\mathcal{M}_P, \\mathcal{M}_{NB}\\}$ 与观测数据 $Y$ 之间的互信息 $I(M; Y)$。对于 $N$ 次独立同分布的观测，此效用为 $I_N(q) = N \\cdot I_1(q)$，其中 $I_1(q)$ 是单次观测的互信息。如问题所述，对于两个具有相等先验概率 $\\pi(\\mathcal{M}_P) = \\pi(\\mathcal{M}_{NB}) = 1/2$ 的模型，这种单次重复实验的效用等同于模型边缘预测分布 $p_P(y)$ 和 $p_{NB}(y)$ 之间的 Jensen-Shannon 散度 (JSD)：\n$$\nI_1(q) = \\mathrm{JSD}(p_P, p_{NB}) = \\sum_{y=0}^{\\infty} \\left[ \\frac{1}{2}p_P(y) \\ln \\frac{p_P(y)}{p_{\\text{mix}}(y)} + \\frac{1}{2}p_{NB}(y) \\ln \\frac{p_{NB}(y)}{p_{\\text{mix}}(y)} \\right]\n$$\n其中 $p_{\\text{mix}}(y) = \\frac{1}{2}p_P(y) + \\frac{1}{2}p_{NB}(y)$ 是数据在模型上的边缘分布。按规定使用自然对数。\n\n优化目标是在可能的 $q$ 值的离散网格上以及所有可行的整数 $N \\ge 1$ 上最大化总效用 $I_N(q)$。约束由总预算 $B$ 定义，成本为 $C(N,q) = c_r N + c_s \\frac{q}{1-q}$。\n\n算法流程如下：\n1.  对于选定的灵敏度 $q$，实验装置的成本是 $C_s(q) = c_s \\frac{q}{1-q}$。用于重复实验的剩余预算是 $B - C_s(q)$。\n2.  由于效用 $I_N(q) = N \\cdot I_1(q)$ 随 $N$ 线性增加，对于固定的 $q$，我们应始终选择预算允许的最大重复实验次数。这由 $N = N_{\\max}(q) = \\left\\lfloor \\frac{B - C_s(q)}{c_r} \\right\\rfloor$ 给出。\n3.  一个设计 $(N,q)$ 只有在 $C_s(q) \\le B$ 且 $N_{\\max}(q) \\ge 1$ 时才是可行的。\n4.  因此，问题简化为在给定的 $q$ 值离散网格上进行搜索。对于每个可行的 $q$，我们计算其对应的 $N_{\\max}(q)$ 和总效用 $I_N(q) = N_{\\max}(q) \\cdot I_1(q)$。\n5.  最优设计 $(N^*, q^*)$ 是在所有可行设计中产生最大总效用的那个。问题规定了决胜规则：如果两个设计产生相同的最大效用，则优先选择重复实验次数 $N$ 较大的那个，其次是灵敏度 $q$ 较大的那个。\n\n主要的计算挑战是计算 $I_1(q)$，这涉及到一个无穷级数。问题指定了一种稳健的截断策略。级数在由两个分布的均值和方差确定的上界 $y_{\\max}$ 处截断：\n$$\ny_{\\max} = \\left\\lceil q\\mu + 10 \\sqrt{\\max\\left(\\mathrm{Var}_P[Y], \\mathrm{Var}_{NB}[Y]\\right)} + 10 \\right\\rceil, \\quad \\text{with } y_{\\max} \\ge 100\n$$\n其中 $\\mathrm{Var}_P[Y] = q\\mu$ 且 $\\mathrm{Var}_{NB}[Y] = q\\mu + \\frac{q^2\\mu^2}{k}$。这确保了求和覆盖了分布具有显著概率质量的区域。\n\n为了保持数值稳定性，特别是对于小概率和大的组合数（如负二项 PMF 中），所有的概率质量函数 (PMF) 都在对数空间中计算。泊松 PMF $p_P(y \\mid q,\\mu)$ 和负二项 PMF $p_{NB}(y \\mid q,\\mu,k)$ 使用它们的对数-PMF 公式计算，这涉及到对数伽马函数（来自 SciPy 的 `gammaln`）。\n混合概率的对数 $\\ln(p_{\\text{mix}}(y))$ 使用 `logsumexp` 函数稳定地计算：\n$$\n\\ln(p_{\\text{mix}}(y)) = \\ln\\left(\\frac{1}{2}e^{\\ln p_P(y)} + \\frac{1}{2}e^{\\ln p_{NB}(y)}\\right) = \\ln(0.5) + \\mathrm{logsumexp}(\\ln p_P(y), \\ln p_{NB}(y))\n$$\n然后将 JSD 的各项从 $y=0$ 到 $y_{\\max}$ 求和，得到 $I_1(q)$。\n\n总体算法遍历每个测试用例的参数。对于每个案例，它评估来自给定网格的所有可行设计 $(N_{\\max}(q), q)$，存储它们的效用，然后根据指定的标准（最大效用，然后是最大 $N$，然后是最大 $q$）选择最优设计。最终输出格式化为三元组列表 $[N^*, q^*, I_{N^*}(q^*)]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln, logsumexp\n\ndef calculate_log_pmfs(y_values, q, mu, k):\n    \"\"\"\n    Calculates the log-PMFs for Poisson and Negative Binomial distributions\n    in a numerically stable and vectorized way.\n    \"\"\"\n    lam = q * mu\n    ys = np.asarray(y_values, dtype=np.float64)\n\n    # Poisson log PMF: log(e^(-lam) * lam^y / y!) = y*log(lam) - lam - lgamma(y+1)\n    if lam == 0:\n        log_pmf_p = np.full_like(ys, -np.inf)\n        log_pmf_p[ys == 0] = 0.0\n    else:\n        log_pmf_p = ys * np.log(lam) - lam - gammaln(ys + 1)\n\n    # Negative Binomial log PMF from the problem statement\n    # log(pmf) = lgamma(y+k) - lgamma(k) - lgamma(y+1) + k*log(k/(k+lam)) + y*log(lam/(k+lam))\n    if lam == 0:\n        log_pmf_nb = np.full_like(ys, -np.inf)\n        log_pmf_nb[ys == 0] = 0.0\n    else:\n        log_term_k = k * (np.log(k) - np.log(k + lam))\n        log_term_y = ys * (np.log(lam) - np.log(k + lam))\n        log_pmf_nb = gammaln(ys + k) - gammaln(k) - gammaln(ys + 1) + log_term_k + log_term_y\n        \n    return log_pmf_p, log_pmf_nb\n\ndef calculate_i1(q, mu, k):\n    \"\"\"\n    Calculates the single-replicate mutual information I_1(q), which is the\n    Jensen-Shannon divergence between the two predictive distributions.\n    \"\"\"\n    # Determine truncation limit y_max\n    mean_val = q * mu\n    var_p = mean_val\n    var_nb = mean_val + (q**2 * mu**2) / k\n    \n    y_max = np.ceil(mean_val + 10 * np.sqrt(max(var_p, var_nb)) + 10)\n    y_max = int(max(y_max, 100))\n    \n    y_values = np.arange(0, y_max + 1)\n    \n    log_p_p_vals, log_p_nb_vals = calculate_log_pmfs(y_values, q, mu, k)\n    \n    I1 = 0.0\n    for i in range(len(y_values)):\n        log_p_p = log_p_p_vals[i]\n        log_p_nb = log_p_nb_vals[i]\n        \n        # log of the mixture probability: log(0.5 * (p_p + p_nb))\n        log_p_mix = np.log(0.5) + logsumexp([log_p_p, log_p_nb])\n        \n        # Add JSD term for model P, handling p_p = 0 case\n        p_p = np.exp(log_p_p)\n        if p_p > 0:\n            term_p = 0.5 * p_p * (log_p_p - log_p_mix)\n            I1 += term_p\n            \n        # Add JSD term for model NB, handling p_nb = 0 case\n        p_nb = np.exp(log_p_nb)\n        if p_nb > 0:\n            term_nb = 0.5 * p_nb * (log_p_nb - log_p_mix)\n            I1 += term_nb\n            \n    return I1\n\ndef find_optimal_design(B, cr, cs, mu, k, q_grid):\n    \"\"\"\n    Finds the optimal design (N*, q*) that maximizes total utility I_N(q)\n    subject to budget and feasibility constraints.\n    \"\"\"\n    candidates = []\n\n    for q in q_grid:\n        if q >= 1.0: continue\n        \n        cost_q = cs * q / (1.0 - q)\n        \n        if cost_q > B:\n            continue\n            \n        N_max = np.floor((B - cost_q) / cr)\n        \n        if N_max  1:\n            continue\n        \n        N = int(N_max)\n        i1_q = calculate_i1(q, mu, k)\n        i_total = N * i1_q\n        \n        candidates.append({'N': N, 'q': q, 'I': i_total})\n        \n    if not candidates:\n        return [0, 0.0, 0.0]\n\n    # Find the best candidate according to the tie-breaking rules:\n    # 1. Maximize utility 'I'\n    # 2. Maximize number of replicates 'N'\n    # 3. Maximize sensitivity 'q'\n    candidates.sort(key=lambda x: (-x['I'], -x['N'], -x['q']))\n    \n    best_design = candidates[0]\n    return [best_design['N'], best_design['q'], best_design['I']]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'B': 50, 'cr': 1.0, 'cs': 10.0, 'mu': 8.0, 'k': 20.0},\n        {'B': 8, 'cr': 1.0, 'cs': 10.0, 'mu': 8.0, 'k': 20.0},\n        {'B': 50, 'cr': 2.0, 'cs': 5.0, 'mu': 5.0, 'k': 2.0},\n        {'B': 50, 'cr': 1.0, 'cs': 10.0, 'mu': 10.0, 'k': 1e6},\n        {'B': 50, 'cr': 0.5, 'cs': 50.0, 'mu': 12.0, 'k': 5.0},\n    ]\n    q_grid = [0.3, 0.5, 0.7, 0.85, 0.95]\n\n    results = []\n    for params in test_cases:\n        result = find_optimal_design(\n            params['B'], params['cr'], params['cs'], params['mu'], params['k'], q_grid\n        )\n        results.append(result)\n\n    # Format the results for the final print statement\n    formatted_results = []\n    for N_star, q_star, I_star in results:\n        formatted_results.append(f\"[{N_star},{q_star:.3f},{I_star:.6f}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "通常，我们的不确定性不仅仅在于哪个模型是正确的；我们对每个模型内部的具体参数也同样不确定。本练习 () 正面解决了这一挑战，要求您在考虑模型参数先验不确定性的同时，设计一个实验来区分两种动力学模型。您将使用拉普拉斯近似法，这是一种功能强大且广泛应用的方法，来将参数不确定性传播到模型预测中，并计算设计效用。",
            "id": "3290050",
            "problem": "您的任务是构建一个完整的程序，用于计算一个近似的贝叶斯最优实验设计准则，以区分在温度变化条件下，一级生化转化的两种竞争性动力学模型。这两种模型分别是 Arrhenius 速率模型和一种基于温度系数公式的非 Arrhenius 速率模型。实验包含一个两步、分段恒温方案，该方案由一个基准绝对温度和一系列温度变化来表征。可观测量是两步方案结束后的转化分数。您必须使用第一性原理和经过充分检验的近似方法来构建和实现设计准则的计算。\n\n该问题的基本基础如下。\n\n- Arrhenius 模型下随温度变化的速率：对于绝对温度 $T$，速率为 $k_{\\text{Arr}}(T;\\theta_{\\text{Arr}}) = A \\exp\\!\\left(-E_{a}/(R T)\\right)$，其中 $A$ 是指前因子，$E_{a}$ 是活化能，$R$ 是普适气体常数。\n- 非 Arrhenius 模型（温度系数模型）下随温度变化的速率：对于绝对温度 $T$，速率为 $k_{\\text{non}}(T;\\theta_{\\text{non}}) = k_{\\text{ref}} \\, Q_{10}^{(T - T_{\\text{ref}})/10}$，其中 $k_{\\text{ref}}$ 是在 $T_{\\text{ref}}$ 时的参考速率，$Q_{10}$ 是温度系数值。\n- 分段恒定速率下的一级转化：对于在温度 $T_{i}$ 下持续时间为 $\\tau_{i}$ 的步骤 $i$，速率为 $k(T_{i})$，则两步方案结束时的转化分数为 $y = 1 - \\exp\\!\\left(-\\sum_{i=1}^{2} k(T_{i}) \\tau_{i}\\right)$。\n- 模型 $m \\in \\{\\text{Arr}, \\text{non}\\}$ 和设计 $\\Delta T$ 下可观测量的贝叶斯预测分布：$p(y \\mid m, \\Delta T) = \\int p(y \\mid \\theta_{m}, m, \\Delta T) \\, p(\\theta_{m} \\mid m) \\, d\\theta_{m}$，其中 $p(y \\mid \\theta_{m}, m, \\Delta T)$ 是测量的似然，$p(\\theta_{m} \\mid m)$ 是参数先验。\n- 测量模型：加性高斯测量噪声，$y_{\\text{obs}} = y_{\\text{true}} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_{y}^{2})$。\n- 用于模型区分的 Kullback–Leibler 散度：对于给定的 $\\Delta T$，设计目标是最大化 $D_{\\mathrm{KL}}\\!\\left(p(y \\mid m_{\\text{Arr}}, \\Delta T) \\| p(y \\mid m_{\\text{non}}, \\Delta T)\\right)$。\n\n您的任务是实现以下基于上述基础的计算过程。\n\n- 使用 Laplace (delta) 方法，通过将从参数到可观测量的确定性映射在每个模型参数的先验均值附近进行线性化，来将预测分布 $p(y \\mid m, \\Delta T)$ 近似为单变量高斯分布。每个模型下得到的预测分布由一个均值和一个方差表征，该均值和方差同时包含了参数不确定性（通过线性化和参数先验协方差）和测量噪声方差 $\\sigma_{y}^{2}$。\n- 使用得到的两个高斯预测分布（每个模型一个），计算 Kullback–Leibler 散度 $D_{\\mathrm{KL}}\\!\\left(p(y \\mid m_{\\text{Arr}}, \\Delta T) \\| p(y \\mid m_{\\text{non}}, \\Delta T)\\right)$。\n- 对于一个有限的候选两步温变序列集合 $\\Delta T = [\\Delta T_{1}, \\Delta T_{2}]$，以及指定的步长 $[\\tau_{1}, \\tau_{2}]$，计算每个候选方案的散度，并选择使散度最大化的序列的索引（零基），以及相应的最大散度值。\n\n所有温度必须以开尔文（K）为单位。所有时间必须以秒（s）为单位。所有能量必须以焦耳每摩尔（J/mol）为单位。不使用角度。Kullback–Leibler 散度是无量纲的。除非在测试用例中另有规定，普适气体常数 $R = 8.314 \\,\\text{J}\\,\\text{mol}^{-1}\\,\\text{K}^{-1}$。序列索引使用零基索引。对于非 Arrhenius 模型，使用 $T_{\\text{ref}}$ 等于相应测试用例的基准绝对温度 $T_{0}$。\n\n用于线性化的参数化和先验：\n\n- Arrhenius 模型参数为 $\\theta_{\\text{Arr}} = [\\ln A, E_{a}]^{\\top}$，服从高斯先验 $\\mathcal{N}(\\mu_{\\text{Arr}}, \\Sigma_{\\text{Arr}})$。\n- 非 Arrhenius 模型参数为 $\\theta_{\\text{non}} = [\\ln k_{\\text{ref}}, \\ln Q_{10}]^{\\top}$，服从高斯先验 $\\mathcal{N}(\\mu_{\\text{non}}, \\Sigma_{\\text{non}})$。\n\n您的程序必须实现上述过程，并将其应用于以下测试套件。每个测试用例提供一个基准绝对温度 $T_{0}$，一对步长 $[\\tau_{1}, \\tau_{2}]$，一个候选温变序列列表 $[\\Delta T_{1}, \\Delta T_{2}]$，两个模型的参数先验均值和协方差，测量噪声标准差 $\\sigma_{y}$，普适气体常数 $R$，以及参考温度 $T_{\\text{ref}} = T_{0}$。\n\n测试套件：\n\n- 测试用例 1（正常路径）：\n  - $T_{0} = 310.0 \\,\\text{K}$，$[\\tau_{1}, \\tau_{2}] = [300.0, 300.0] \\,\\text{s}$。\n  - 候选序列（开尔文）：$[[-5.0, 5.0], [5.0, -5.0], [0.0, 0.0], [8.0, 8.0]]$。\n  - Arrhenius 先验：$\\mu_{\\text{Arr}} = [\\ln(10^{5}), 60000.0]$，$\\Sigma_{\\text{Arr}} = \\mathrm{diag}([0.3^{2}, 5000.0^{2}])$。\n  - 非 Arrhenius 先验：$\\mu_{\\text{non}} = [\\ln(0.1), \\ln(2.0)]$，$\\Sigma_{\\text{non}} = \\mathrm{diag}([0.2^{2}, 0.1^{2}])$。\n  - $\\sigma_{y} = 0.02$，$R = 8.314$，$T_{\\text{ref}} = T_{0}$。\n\n- 测试用例 2（零持续时间边界）：\n  - $T_{0} = 310.0 \\,\\text{K}$，$[\\tau_{1}, \\tau_{2}] = [0.0, 0.0] \\,\\text{s}$。\n  - 候选序列（开尔文）：$[[-5.0, 5.0], [5.0, -5.0]]$。\n  - Arrhenius 先验：$\\mu_{\\text{Arr}} = [\\ln(10^{5}), 60000.0]$，$\\Sigma_{\\text{Arr}} = \\mathrm{diag}([0.3^{2}, 5000.0^{2}])$。\n  - 非 Arrhenius 先验：$\\mu_{\\text{non}} = [\\ln(0.1), \\ln(2.0)]$，$\\Sigma_{\\text{non}} = \\mathrm{diag}([0.2^{2}, 0.1^{2}])$。\n  - $\\sigma_{y} = 0.02$，$R = 8.314$，$T_{\\text{ref}} = T_{0}$。\n\n- 测试用例 3（通过更长持续时间和更小噪声实现更清晰的区分）：\n  - $T_{0} = 305.0 \\,\\text{K}$，$[\\tau_{1}, \\tau_{2}] = [600.0, 600.0] \\,\\text{s}$。\n  - 候选序列（开尔文）：$[[-10.0, 10.0], [10.0, -10.0], [10.0, 10.0], [-10.0, -10.0], [0.0, 0.0], [5.0, -5.0]]$。\n  - Arrhenius 先验：$\\mu_{\\text{Arr}} = [\\ln(5.0 \\times 10^{6}), 80000.0]$，$\\Sigma_{\\text{Arr}} = \\mathrm{diag}([0.25^{2}, 4000.0^{2}])$。\n  - 非 Arrhenius 先验：$\\mu_{\\text{non}} = [\\ln(0.05), \\ln(1.8)]$，$\\Sigma_{\\text{non}} = \\mathrm{diag}([0.15^{2}, 0.08^{2}])$。\n  - $\\sigma_{y} = 0.005$，$R = 8.314$，$T_{\\text{ref}} = T_{0}$。\n\n- 测试用例 4（非对角参数协方差）：\n  - $T_{0} = 315.0 \\,\\text{K}$，$[\\tau_{1}, \\tau_{2}] = [400.0, 800.0] \\,\\text{s}$。\n  - 候选序列（开尔文）：$[[-7.0, 3.0], [3.0, -7.0], [7.0, 7.0], [0.0, 0.0]]$。\n  - Arrhenius 先验：$\\mu_{\\text{Arr}} = [\\ln(2.0 \\times 10^{5}), 70000.0]$，标准差为 $\\sigma_{\\ln A} = 0.3$, $\\sigma_{E_{a}} = 6000.0$，相关系数 $\\rho = 0.25$，因此\n    $\\Sigma_{\\text{Arr}} = \\begin{bmatrix} \\sigma_{\\ln A}^{2}  \\rho \\, \\sigma_{\\ln A} \\, \\sigma_{E_{a}} \\\\ \\rho \\, \\sigma_{\\ln A} \\, \\sigma_{E_{a}}  \\sigma_{E_{a}}^{2} \\end{bmatrix}$。\n  - 非 Arrhenius 先验：$\\mu_{\\text{non}} = [\\ln(0.08), \\ln(2.1)]$，标准差为 $\\sigma_{\\ln k_{\\text{ref}}} = 0.2$, $\\sigma_{\\ln Q_{10}} = 0.1$，相关系数 $\\rho = -0.2$，因此\n    $\\Sigma_{\\text{non}} = \\begin{bmatrix} \\sigma_{\\ln k_{\\text{ref}}}^{2}  \\rho \\, \\sigma_{\\ln k_{\\text{ref}}} \\, \\sigma_{\\ln Q_{10}} \\\\ \\rho \\, \\sigma_{\\ln k_{\\text{ref}}} \\, \\sigma_{\\ln Q_{10}}  \\sigma_{\\ln Q_{10}}^{2} \\end{bmatrix}$。\n  - $\\sigma_{y} = 0.01$，$R = 8.314$，$T_{\\text{ref}} = T_{0}$。\n\n对于每个测试用例，您的程序必须：\n- 按照提供的顺序枚举所有候选序列，并使用通过 Laplace (delta) 方法推导出的高斯近似，为每个候选方案计算 $D_{\\mathrm{KL}}\\!\\left(p(y \\mid m_{\\text{Arr}}, \\Delta T) \\| p(y \\mid m_{\\text{non}}, \\Delta T)\\right)$。\n- 选择具有最大散度的候选序列的零基索引。如果出现平局，选择最小的索引。\n- 为每个测试用例报告一个由所选索引（一个整数）和相应的最大散度值（一个四舍五入到小数点后恰好六位的浮点数）组成的对。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含所有测试用例的结果。结果是一个逗号分隔的对列表，用方括号括起来，每个对的格式为 $[i, d]$，其中 $i$ 是所选的零基索引，$d$ 是四舍五入到六位小数的最大散度。例如：$[[0,0.123456],[2,0.000000],\\ldots]$。",
            "solution": "该问题是有效的，因为它在科学上基于化学动力学和贝叶斯统计，定义明确，具有清晰的目标和所有必要信息，并且在计算上是可行的。解决方案首先推导必要的数学公式，然后进行数值实现。\n\n目标是从一组有限的候选设计中找到最优实验设计 $\\Delta T = [\\Delta T_{1}, \\Delta T_{2}]^{\\top}$。最优设计是使两个竞争模型（一个 Arrhenius 模型 $m_{\\text{Arr}}$ 和一个非 Arrhenius 模型 $m_{\\text{non}}$）的可观测量 $y$ 的贝叶斯预测分布之间的 Kullback-Leibler (KL) 散度最大化的设计。\n目标函数是：\n$$\n\\Delta T^* = \\arg\\max_{\\Delta T} D_{\\mathrm{KL}}\\!\\left(p(y \\mid m_{\\text{Arr}}, \\Delta T) \\| p(y \\mid m_{\\text{non}}, \\Delta T)\\right)\n$$\n\n问题的核心在于近似每个模型的预测分布 $p(y \\mid m, \\Delta T)$。问题指定使用 Laplace (或 delta) 方法，这涉及到将模型输出对其参数进行线性化。\n\n令 $g_m(\\theta_m; \\Delta T)$ 表示给定模型 $m$、参数向量 $\\theta_m$ 和实验设计 $\\Delta T$ 时可观测量 $y$ 的确定性预测。每个模型的参数先验分布为高斯分布，$p(\\theta_m) = \\mathcal{N}(\\theta_m \\mid \\mu_m, \\Sigma_m)$。测量模型为 $y_{\\text{obs}} = y_{\\text{true}} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_y^2)$。\n\nLaplace 近似将 $g_m(\\theta_m)$ 在先验均值 $\\mu_m$ 附近进行线性化：\n$$\ny \\approx g_m(\\mu_m) + \\nabla_{\\theta_m} g_m(\\mu_m)^{\\top} (\\theta_m - \\mu_m)\n$$\n在此线性化下，真实模型输出 $y$ 在参数 $\\theta_m$ 的先验分布上边缘化后的分布也是高斯分布。其均值为 $\\mathbb{E}[y] = g_m(\\mu_m)$，其方差为 $\\text{Var}[y] = \\nabla_{\\theta_m} g_m(\\mu_m)^{\\top} \\Sigma_m \\nabla_{\\theta_m} g_m(\\mu_m)$。我们把这个参数诱导的方差记为 $\\sigma_{m, \\text{param}}^2$。\n\n最终的可观测量 $y_{\\text{obs}}$ 包含测量噪声。近似预测分布 $p(y \\mid m, \\Delta T)$ 是线性化模型输出与测量噪声之和的分布。由于两者都是独立的高斯分布，得到的预测分布也是高斯分布，$p(y \\mid m, \\Delta T) \\approx \\mathcal{N}(y \\mid \\mu_{y,m}, \\sigma_{y,m}^2)$，其中：\n\\begin{itemize}\n    \\item 预测均值：$\\mu_{y,m} = g_m(\\mu_m; \\Delta T)$\n    \\item 预测方差：$\\sigma_{y,m}^2 = \\sigma_{m, \\text{param}}^2 + \\sigma_y^2 = \\nabla_{\\theta_m} g_m(\\mu_m)^{\\top} \\Sigma_m \\nabla_{\\theta_m} g_m(\\mu_m) + \\sigma_y^2$\n\\end{itemize}\n\n接下来，我们为两个模型指定函数 $g_m$ 及其梯度 $\\nabla_{\\theta_m} g_m$。\n可观测量是转化分数，$y = 1 - \\exp(-\\lambda_m)$，其中 $\\lambda_m = \\sum_{i=1}^{2} k_m(T_i) \\tau_i$。两个步骤的温度分别是 $T_1 = T_0 + \\Delta T_1$ 和 $T_2 = T_0 + \\Delta T_2$。\n使用链式法则，$y = g_m(\\theta_m)$ 相对于 $\\theta_m$ 的梯度为：\n$$\n\\nabla_{\\theta_m} g_m = \\frac{\\partial g_m}{\\partial \\lambda_m} \\nabla_{\\theta_m} \\lambda_m = \\exp(-\\lambda_m) \\nabla_{\\theta_m} \\lambda_m\n$$\n其中 $\\nabla_{\\theta_m} \\lambda_m = \\tau_1 \\nabla_{\\theta_m} k_m(T_1; \\theta_m) + \\tau_2 \\nabla_{\\theta_m} k_m(T_2; \\theta_m)$。所有导数都在先验均值 $\\theta_m = \\mu_m$ 处求值。\n\n**Arrhenius 模型 ($m_{\\text{Arr}}$)**\n- 参数：$\\theta_{\\text{Arr}} = [\\ln A, E_a]^{\\top}$。\n- 速率：$k_{\\text{Arr}}(T) = \\exp(\\ln A - E_a/(RT))$。\n- 速率关于参数的梯度：\n  - $\\frac{\\partial k_{\\text{Arr}}}{\\partial (\\ln A)} = k_{\\text{Arr}}(T)$\n  - $\\frac{\\partial k_{\\text{Arr}}}{\\partial E_a} = -\\frac{k_{\\text{Arr}}(T)}{RT}$\n- 梯度向量为 $\\nabla_{\\theta_{\\text{Arr}}} k_{\\text{Arr}} = [k_{\\text{Arr}}, -k_{\\text{Arr}}/(RT)]^{\\top}$。\n\n**非 Arrhenius 模型 ($m_{\\text{non}}$)**\n- 参数：$\\theta_{\\text{non}} = [\\ln k_{\\text{ref}}, \\ln Q_{10}]^{\\top}$。\n- 速率：$k_{\\text{non}}(T) = k_{\\text{ref}} Q_{10}^{(T-T_{\\text{ref}})/10} = \\exp\\left(\\ln k_{\\text{ref}} + (\\ln Q_{10}) \\frac{T-T_{\\text{ref}}}{10}\\right)$。\n- 速率关于参数的梯度：\n  - $\\frac{\\partial k_{\\text{non}}}{\\partial (\\ln k_{\\text{ref}})} = k_{\\text{non}}(T)$\n  - $\\frac{\\partial k_{\\text{non}}}{\\partial (\\ln Q_{10})} = k_{\\text{non}}(T) \\frac{T-T_{\\text{ref}}}{10}$\n- 梯度向量为 $\\nabla_{\\theta_{\\text{non}}} k_{\\text{non}} = [k_{\\text{non}}, k_{\\text{non}}\\frac{T-T_{\\text{ref}}}{10}]^{\\top}$。\n\n有了两个高斯预测分布的均值和方差，即 $(\\mu_{y, \\text{Arr}}, \\sigma_{y, \\text{Arr}}^2)$ 和 $(\\mu_{y, \\text{non}}, \\sigma_{y, \\text{non}}^2)$，我们就可以计算 Kullback-Leibler 散度。对于两个单变量高斯分布 $P_1 = \\mathcal{N}(\\mu_1, \\sigma_1^2)$ 和 $P_2 = \\mathcal{N}(\\mu_2, \\sigma_2^2)$，KL 散度由以下公式给出：\n$$\nD_{\\mathrm{KL}}(P_1 \\| P_2) = \\ln\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n$$\n在我们的情境中，$P_1$ 对应 Arrhenius 模型的预测分布，$P_2$ 对应非 Arrhenius 模型的预测分布。\n\n每个测试用例的计算步骤如下：\n1. 对于每个候选设计 $\\Delta T = [\\Delta T_1, \\Delta T_2]$，计算 $T_1 = T_0 + \\Delta T_1$ 和 $T_2 = T_0 + \\Delta T_2$。\n2. 对于 Arrhenius 模型：\n   a. 在先验均值 $\\mu_{\\text{Arr}}$ 处评估速率 $k_{\\text{Arr}}(T_1)$ 和 $k_{\\text{Arr}}(T_2)$。\n   b. 计算预测均值 $\\mu_{y, \\text{Arr}}$。\n   c. 在 $\\mu_{\\text{Arr}}$ 处计算梯度 $\\nabla_{\\theta_{\\text{Arr}}} g_{\\text{Arr}}$。\n   d. 计算预测方差 $\\sigma_{y, \\text{Arr}}^2 = (\\nabla_{\\theta_{\\text{Arr}}} g_{\\text{Arr}})^{\\top} \\Sigma_{\\text{Arr}} (\\nabla_{\\theta_{\\text{Arr}}} g_{\\text{Arr}}) + \\sigma_y^2$。\n3. 对于非 Arrhenius 模型：\n   a. 在先验均值 $\\mu_{\\text{non}}$ 处评估速率 $k_{\\text{non}}(T_1)$ 和 $k_{\\text{non}}(T_2)$。\n   b. 计算预测均值 $\\mu_{y, \\text{non}}$。\n   c. 在 $\\mu_{\\text{non}}$ 处计算梯度 $\\nabla_{\\theta_{\\text{non}}} g_{\\text{non}}$。\n   d. 计算预测方差 $\\sigma_{y, \\text{non}}^2 = (\\nabla_{\\theta_{\\text{non}}} g_{\\text{non}})^{\\top} \\Sigma_{\\text{non}} (\\nabla_{\\theta_{\\text{non}}} g_{\\text{non}}) + \\sigma_y^2$。\n4. 使用推导出的均值和方差计算 $D_{\\mathrm{KL}}$。\n5. 在计算完所有候选设计的散度后，找出产生最大散度的设计的索引。报告该索引和相应的最大散度值，四舍五入到六位小数。\n\n该过程将对每个提供的测试用例实施。对于测试用例 2，其中 $\\tau_1 = \\tau_2 = 0$，对于所有模型和参数，$\\lambda_m$ 均为 0，因此 $y=0$。梯度为零向量，导致参数诱导的方差为零。两个预测分布变得相同，即 $\\mathcal{N}(0, \\sigma_y^2)$，因此所有设计的 KL 散度都为 0。对于测试用例 4，首先根据提供的标准差和相关系数构建非对角协方差矩阵。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Bayesian optimal experimental design criterion for model discrimination\n    between Arrhenius and non-Arrhenius kinetic models.\n    \"\"\"\n\n    def get_predictive_dist_params(\n        model_type, T0, delta_T, taus, mu_theta, Sigma_theta, sigma_y, R\n    ):\n        \"\"\"\n        Calculates the mean and variance of the Gaussian-approximated predictive distribution.\n        \"\"\"\n        T_ref = T0\n        T1 = T0 + delta_T[0]\n        T2 = T0 + delta_T[1]\n        tau1, tau2 = taus\n\n        if model_type == 'arrhenius':\n            lnA, Ea = mu_theta\n            \n            def k_rate(T, p):\n                lnA_p, Ea_p = p\n                return np.exp(lnA_p - Ea_p / (R * T))\n\n            k1 = k_rate(T1, mu_theta)\n            k2 = k_rate(T2, mu_theta)\n            \n            # Gradient of k w.r.t theta = [lnA, Ea]\n            grad_k1 = np.array([k1, -k1 / (R * T1)])\n            grad_k2 = np.array([k2, -k2 / (R * T2)])\n\n        elif model_type == 'non_arrhenius':\n            ln_k_ref, ln_Q10 = mu_theta\n\n            def k_rate(T, p):\n                ln_k_ref_p, ln_Q10_p = p\n                return np.exp(ln_k_ref_p + ln_Q10_p * (T - T_ref) / 10.0)\n\n            k1 = k_rate(T1, mu_theta)\n            k2 = k_rate(T2, mu_theta)\n\n            # Gradient of k w.r.t theta = [ln_k_ref, ln_Q10]\n            grad_k1 = np.array([k1, k1 * (T1 - T_ref) / 10.0])\n            grad_k2 = np.array([k2, k2 * (T2 - T_ref) / 10.0])\n        else:\n            raise ValueError(\"Unknown model type\")\n\n        # Total integrated rate argument\n        lambda_val = k1 * tau1 + k2 * tau2\n        \n        # Predictive mean\n        mu_y = 1.0 - np.exp(-lambda_val)\n\n        # Gradient of lambda w.r.t theta\n        grad_lambda = tau1 * grad_k1 + tau2 * grad_k2\n        \n        # Gradient of y w.r.t theta\n        grad_y = np.exp(-lambda_val) * grad_lambda\n        \n        # Parameter-induced variance\n        var_param = grad_y.T @ Sigma_theta @ grad_y\n        \n        # Total predictive variance\n        var_y = var_param + sigma_y**2\n        \n        return mu_y, var_y\n\n    def kl_divergence_gaussians(mu1, var1, mu2, var2):\n        \"\"\"\n        Computes KL divergence D_KL(N(mu1, var1) || N(mu2, var2)).\n        \"\"\"\n        if var1 == 0 or var2 == 0:\n            return 0.0 # Should not happen with sigma_y > 0\n        \n        # Using formula with variances to avoid sqrt\n        # D_KL = 0.5 * (log(var2/var1) + (var1 + (mu1-mu2)^2)/var2 - 1)\n        term1 = np.log(var2 / var1)\n        term2 = (var1 + (mu1 - mu2)**2) / var2\n        term3 = -1.0\n        \n        kl_div = 0.5 * (term1 + term2 + term3)\n        return kl_div if kl_div > 0 else 0.0\n\n    test_cases = [\n        {\n            \"T0\": 310.0, \"taus\": [300.0, 300.0],\n            \"candidates\": [[-5.0, 5.0], [5.0, -5.0], [0.0, 0.0], [8.0, 8.0]],\n            \"mu_arr\": [np.log(1e5), 60000.0],\n            \"Sigma_arr\": np.diag([0.3**2, 5000.0**2]),\n            \"mu_non\": [np.log(0.1), np.log(2.0)],\n            \"Sigma_non\": np.diag([0.2**2, 0.1**2]),\n            \"sigma_y\": 0.02, \"R\": 8.314\n        },\n        {\n            \"T0\": 310.0, \"taus\": [0.0, 0.0],\n            \"candidates\": [[-5.0, 5.0], [5.0, -5.0]],\n            \"mu_arr\": [np.log(1e5), 60000.0],\n            \"Sigma_arr\": np.diag([0.3**2, 5000.0**2]),\n            \"mu_non\": [np.log(0.1), np.log(2.0)],\n            \"Sigma_non\": np.diag([0.2**2, 0.1**2]),\n            \"sigma_y\": 0.02, \"R\": 8.314\n        },\n        {\n            \"T0\": 305.0, \"taus\": [600.0, 600.0],\n            \"candidates\": [[-10.0, 10.0], [10.0, -10.0], [10.0, 10.0], [-10.0, -10.0], [0.0, 0.0], [5.0, -5.0]],\n            \"mu_arr\": [np.log(5e6), 80000.0],\n            \"Sigma_arr\": np.diag([0.25**2, 4000.0**2]),\n            \"mu_non\": [np.log(0.05), np.log(1.8)],\n            \"Sigma_non\": np.diag([0.15**2, 0.08**2]),\n            \"sigma_y\": 0.005, \"R\": 8.314\n        },\n        {\n            \"T0\": 315.0, \"taus\": [400.0, 800.0],\n            \"candidates\": [[-7.0, 3.0], [3.0, -7.0], [7.0, 7.0], [0.0, 0.0]],\n            \"mu_arr\": [np.log(2e5), 70000.0],\n            \"Sigma_arr\": None, # Will be constructed\n            \"mu_non\": [np.log(0.08), np.log(2.1)],\n            \"Sigma_non\": None, # Will be constructed\n            \"sigma_y\": 0.01, \"R\": 8.314\n        },\n    ]\n\n    # Construct covariance matrices for test case 4\n    s_lnA, s_Ea, rho_arr = 0.3, 6000.0, 0.25\n    cov_arr = rho_arr * s_lnA * s_Ea\n    test_cases[3][\"Sigma_arr\"] = np.array([[s_lnA**2, cov_arr], [cov_arr, s_Ea**2]])\n    \n    s_lnk, s_lnQ, rho_non = 0.2, 0.1, -0.2\n    cov_non = rho_non * s_lnk * s_lnQ\n    test_cases[3][\"Sigma_non\"] = np.array([[s_lnk**2, cov_non], [cov_non, s_lnQ**2]])\n\n    \n    final_results = []\n    for case in test_cases:\n        divergences = []\n        for delta_T in case[\"candidates\"]:\n            mu_arr, var_arr = get_predictive_dist_params(\n                'arrhenius', case[\"T0\"], delta_T, case[\"taus\"], \n                case[\"mu_arr\"], case[\"Sigma_arr\"], case[\"sigma_y\"], case[\"R\"]\n            )\n            mu_non, var_non = get_predictive_dist_params(\n                'non_arrhenius', case[\"T0\"], delta_T, case[\"taus\"], \n                case[\"mu_non\"], case[\"Sigma_non\"], case[\"sigma_y\"], case[\"R\"]\n            )\n            \n            kl_div = kl_divergence_gaussians(mu_arr, var_arr, mu_non, var_non)\n            divergences.append(kl_div)\n            \n        max_div = -1.0\n        best_idx = -1\n        for i, div in enumerate(divergences):\n            if div > max_div:\n                max_div = div\n                best_idx = i\n        \n        final_results.append([best_idx, max_div])\n\n    # Format output as specified\n    formatted_results = [f\"[{res[0]},{res[1]:.6f}]\" for res in final_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}