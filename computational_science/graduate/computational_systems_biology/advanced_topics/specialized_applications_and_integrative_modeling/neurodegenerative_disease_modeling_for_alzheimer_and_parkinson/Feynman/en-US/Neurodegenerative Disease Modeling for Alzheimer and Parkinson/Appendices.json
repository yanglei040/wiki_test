{
    "hands_on_practices": [
        {
            "introduction": "At the heart of modeling neurodegenerative diseases lies the ability to translate molecular mechanisms into a mathematical framework. This first practice challenges you to construct a model of amyloid aggregation from the ground up, using the law of mass action and the principle of mass conservation . By deriving a system of ordinary differential equations for monomer, oligomer, and fibril species, you will master the foundational skill of building a biophysically consistent kinetic model, a cornerstone of computational systems biology.",
            "id": "3333662",
            "problem": "Consider amyloid aggregation relevant to Alzheimer’s disease and Parkinson’s disease, where monomeric protein (e.g., amyloid-beta or alpha-synuclein) self-assembles into soluble oligomers and insoluble fibrils. Let $m(t)$ denote the monomer concentration measured in monomer equivalents, $o(t)$ denote the oligomer mass concentration measured in monomer equivalents, and $f(t)$ denote the fibril mass concentration measured in monomer equivalents. Assume a closed, well-mixed system with no synthesis or degradation and the following biophysically grounded mechanisms derived from the law of mass action and stoichiometric mass balance:\n- Primary nucleation: $n_{c}$ monomers collide to form an oligomeric nucleus. Model this as a reaction of overall order $n_c \\ge 2$ in $m(t)$ with rate constant $k_{n}$.\n- Elongation: monomers add to fibril ends to increase fibril mass. Use a coarse-grained closure that the total end concentration is proportional to fibril mass, yielding an effective bimolecular mass flux proportional to $m(t)f(t)$ with rate constant $k_{e}$.\n- Fragmentation-induced shedding: fibrils shed soluble oligomeric fragments at a rate proportional to fibril mass $f(t)$ with rate constant $k_{f}$.\n\nStarting only from the law of mass action, stoichiometric mass balance, and the stated mechanisms, derive an ordinary differential equation (ODE) system for $m(t)$, $o(t)$, and $f(t)$ that exactly conserves total monomer-equivalent mass. For each reaction term in each equation, state its unit consistency by specifying the required physical units of $k_{n}$, $k_{e}$, and $k_{f}$ to ensure that each time derivative has units of concentration per unit time. Assume time is measured in seconds and concentrations in moles per liter. No additional modeling shortcuts are permitted beyond the closure stated above.\n\nThen, prove that the total monomer-equivalent mass is conserved and identify the associated invariant. Express the final conserved quantity as a closed-form analytic expression in terms of the initial conditions $m(0)$, $o(0)$, and $f(0)$. Do not include units in your final boxed answer. The final answer must be a single analytic expression.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of chemical kinetics and mass balance, well-posed with sufficient information for a unique derivation, and objective in its language. It represents a standard, albeit simplified, modeling exercise in computational systems biology, free from scientific flaws, ambiguities, or contradictions.\n\nWe begin by deriving the system of ordinary differential equations (ODEs) for the monomer concentration $m(t)$, oligomer mass concentration $o(t)$, and fibril mass concentration $f(t)$, all measured in monomer equivalents. The change in each concentration over time is the sum of the rates of production and consumption from the specified reaction mechanisms.\n\n1.  **Primary Nucleation:** The process involves $n_c$ monomers forming an oligomeric nucleus: $n_c m \\xrightarrow{k_{n}} o$. The rate of this reaction is given to be of order $n_c$ in the monomer concentration $m$, so the rate of reaction events is $k_{n} [m(t)]^{n_c}$. For each reaction event, $n_c$ monomers are consumed, and one oligomer containing $n_c$ monomer equivalents is formed.\n    -   The rate of change of monomer concentration due to nucleation is the consumption of $n_c$ monomers per reaction event:\n        $$R_{n,m} = -n_c k_{n} [m(t)]^{n_c}$$\n    -   The rate of change of oligomer concentration (in monomer equivalents) is the production of $n_c$ monomer equivalents per reaction event:\n        $$R_{n,o} = +n_c k_{n} [m(t)]^{n_c}$$\n    -   This process does not directly involve fibrils, so their contribution is zero.\n\n2.  **Elongation:** Monomers add to fibril ends, increasing fibril mass: $m + f \\xrightarrow{k_{e}} f$. The problem states that the effective bimolecular mass flux is proportional to $m(t)f(t)$. This flux represents the rate at which monomer mass is converted into fibril mass.\n    -   The rate of change of monomer concentration due to elongation is the consumption of monomer mass:\n        $$R_{e,m} = -k_{e} m(t) f(t)$$\n    -   The rate of change of fibril concentration (in monomer equivalents) is the addition of this mass:\n        $$R_{e,f} = +k_{e} m(t) f(t)$$\n    -   This process does not directly involve oligomers, so their contribution is zero.\n\n3.  **Fragmentation-induced Shedding:** Fibrils shed oligomeric fragments: $f \\xrightarrow{k_{f}} o$. The rate of this process is given as being proportional to the fibril mass $f(t)$. This rate represents the conversion of fibril mass into oligomer mass.\n    -   The rate of change of fibril concentration due to fragmentation is the loss of fibril mass:\n        $$R_{f,f} = -k_{f} f(t)$$\n    -   The rate of change of oligomer concentration (in monomer equivalents) is the gain of this mass:\n        $$R_{f,o} = +k_{f} f(t)$$\n    -   This process does not directly involve monomers, so their contribution is zero.\n\nCombining these terms, we construct the full ODE system:\n$$\n\\frac{dm}{dt} = -n_c k_{n} m^{n_c} - k_{e} m f\n$$\n$$\n\\frac{do}{dt} = n_c k_{n} m^{n_c} + k_{f} f\n$$\n$$\n\\frac{df}{dt} = k_{e} m f - k_{f} f\n$$\n\nNext, we establish the physical units of the rate constants to ensure dimensional consistency. Time $t$ is in seconds ($s$) and concentrations $m$, $o$, and $f$ are in moles per liter ($M$). The units of the time derivatives (e.g., $\\frac{dm}{dt}$) must be $M \\cdot s^{-1}$.\n-   For the nucleation term, $n_c k_{n} m^{n_c}$, the units must be $M \\cdot s^{-1}$. Since $n_c$ is a dimensionless stoichiometric coefficient, we have:\n    $$[\\text{units of } k_{n}] \\cdot M^{n_c} = M \\cdot s^{-1} \\implies [\\text{units of } k_{n}] = M^{1-n_c} \\cdot s^{-1}$$\n-   For the elongation term, $k_{e} m f$, the units must be $M \\cdot s^{-1}$:\n    $$[\\text{units of } k_{e}] \\cdot M \\cdot M = M \\cdot s^{-1} \\implies [\\text{units of } k_{e}] = M^{-1} \\cdot s^{-1}$$\n-   For the fragmentation term, $k_{f} f$, the units must be $M \\cdot s^{-1}$:\n    $$[\\text{units of } k_{f}] \\cdot M = M \\cdot s^{-1} \\implies [\\text{units of } k_{f}] = s^{-1}$$\n\nFinally, we prove that the total monomer-equivalent mass is conserved. Let the total mass be $M_{\\text{total}}(t) = m(t) + o(t) + f(t)$. To prove conservation, we show that its time derivative is zero.\n$$\n\\frac{d M_{\\text{total}}}{dt} = \\frac{d}{dt} \\left( m(t) + o(t) + f(t) \\right) = \\frac{dm}{dt} + \\frac{do}{dt} + \\frac{df}{dt}\n$$\nSubstituting the derived ODEs into this sum:\n$$\n\\frac{d M_{\\text{total}}}{dt} = \\left( -n_c k_{n} m^{n_c} - k_{e} m f \\right) + \\left( n_c k_{n} m^{n_c} + k_{f} f \\right) + \\left( k_{e} m f - k_{f} f \\right)\n$$\nCollecting like terms:\n$$\n\\frac{d M_{\\text{total}}}{dt} = (-n_c k_{n} m^{n_c} + n_c k_{n} m^{n_c}) + (-k_{e} m f + k_{e} m f) + (k_{f} f - k_{f} f)\n$$\n$$\n\\frac{d M_{\\text{total}}}{dt} = 0 + 0 + 0 = 0\n$$\nSince $\\frac{d M_{\\text{total}}}{dt} = 0$, the total mass $M_{\\text{total}}(t)$ is a constant for all time $t \\geq 0$. This constant is the invariant of the system, and its value is determined by the initial conditions at $t=0$. The conserved quantity is therefore:\n$$\nM_{\\text{total}} = m(t) + o(t) + f(t) = m(0) + o(0) + f(0)\n$$\nThe problem asks for the closed-form analytic expression for this conserved quantity in terms of the initial conditions. This expression is $m(0) + o(0) + f(0)$.",
            "answer": "$$\n\\boxed{m(0) + o(0) + f(0)}\n$$"
        },
        {
            "introduction": "Pathological proteins in neurodegenerative diseases spread through the brain in a prion-like manner, following the network of anatomical connections. This exercise guides you through the process of coarse-graining a continuous reaction-diffusion model onto a discrete brain network, a cornerstone of modern disease progression modeling . By deriving the network ODEs using the graph Laplacian and performing a stability analysis, you will gain insight into the critical conditions that permit the onset and propagation of pathology across the connectome.",
            "id": "3333677",
            "problem": "In modeling the prion-like spread of misfolded proteins implicated in Alzheimer’s disease (AD) and Parkinson’s disease (PD) across the brain’s structural connectivity network, a common starting point is the Kolmogorov–Petrovsky–Piskunov (KPP) reaction–diffusion Partial Differential Equation (PDE) for a concentration field $u(\\mathbf{x}, t)$ over a spatial domain:\n$$\\partial_t u = D \\nabla^2 u + r u \\left(1 - \\frac{u}{K}\\right),$$\nwhere $D$ is the diffusion coefficient, $r$ is the intrinsic growth rate, and $K$ is the carrying capacity. Consider a network coarse-graining of the brain into $N$ regions (nodes), with symmetric weighted adjacency matrix $W \\in \\mathbb{R}^{N \\times N}$ describing axonal connectivity strengths and combinatorial graph Laplacian $L = \\mathrm{diag}(W \\mathbf{1}) - W$, where $\\mathbf{1}$ is the all-ones vector. Assume the network is connected and undirected, and work under no-flux exchange to the exterior (closed system). Let $\\mathbf{u}(t) \\in \\mathbb{R}^N$ collect regional concentrations.\n\nUsing conservation of mass and Fick’s law as the fundamental base for diffusion on a network, and standard logistic growth for local reaction kinetics, derive from first principles the system of Ordinary Differential Equations (ODEs) governing $\\dot{\\mathbf{u}}(t)$ on the network. Then, by linearizing the network dynamics at the disease-free equilibrium $\\mathbf{u} = \\mathbf{0}$, determine the dominant (largest real-part) eigenvalue of the Jacobian in terms of $D$, $r$, and the spectrum of the graph Laplacian $L$.\n\nProvide your final answer as a single closed-form analytic expression for this dominant eigenvalue. Do not include units. If you introduce any acronyms such as Ordinary Differential Equation (ODE) or Partial Differential Equation (PDE), define them upon first use.",
            "solution": "The problem asks for the derivation of a network-based model for protein misfolding propagation and the analysis of its stability at the disease-free equilibrium. The derivation proceeds from first principles, and the analysis requires linearizing the system and finding the dominant eigenvalue of the resulting Jacobian matrix.\n\nFirst, we define acronyms used in this solution: Partial Differential Equation (PDE) and Ordinary Differential Equation (ODE).\n\n**1. Derivation of the System of Ordinary Differential Equations (ODEs)**\n\nThe dynamics of the concentration of misfolded protein in each brain region (node) $i$, denoted by $u_i(t)$, are governed by the principle of conservation of mass. The rate of change of concentration, $\\frac{d u_i}{dt}$, is the sum of the rate of change due to local reaction kinetics and the rate of change due to diffusion between connected regions.\n$$\n\\frac{d u_i}{dt} = \\left(\\frac{d u_i}{dt}\\right)_{\\text{reaction}} + \\left(\\frac{d u_i}{dt}\\right)_{\\text{diffusion}}\n$$\nThe reaction term is described by logistic growth, as specified in the continuous KPP model. For a discrete node $i$, this local term depends only on $u_i$:\n$$\n\\left(\\frac{d u_i}{dt}\\right)_{\\text{reaction}} = r u_i \\left(1 - \\frac{u_i}{K}\\right)\n$$\nThe diffusion term is derived from Fick's first law, which states that flux is proportional to the negative gradient of concentration. On a network, the flux of protein from node $j$ to node $i$, $J_{j \\to i}$, is proportional to the concentration difference $(u_j - u_i)$. The proportionality constant includes the diffusion coefficient $D$ and the connectivity strength $W_{ij}$ between the nodes.\n$$\nJ_{j \\to i} = D W_{ij} (u_j - u_i)\n$$\nThe net change in concentration at node $i$ due to diffusion is the sum of fluxes from all other nodes $j$ in the network. Since the problem specifies a closed system (no-flux exchange to the exterior), we sum over all nodes $j=1, \\dots, N$.\n$$\n\\left(\\frac{d u_i}{dt}\\right)_{\\text{diffusion}} = \\sum_{j=1}^{N} J_{j \\to i} = \\sum_{j=1}^{N} D W_{ij} (u_j - u_i)\n$$\nWe can expand this sum:\n$$\n\\left(\\frac{d u_i}{dt}\\right)_{\\text{diffusion}} = D \\left( \\sum_{j=1}^{N} W_{ij} u_j - \\sum_{j=1}^{N} W_{ij} u_i \\right) = D \\left( \\sum_{j=1}^{N} W_{ij} u_j - u_i \\sum_{j=1}^{N} W_{ij} \\right)\n$$\nLet $d_i = \\sum_{j=1}^{N} W_{ij}$ be the weighted degree of node $i$. The first term in the parenthesis is the $i$-th component of the matrix-vector product $W\\mathbf{u}$. The second term is $d_i u_i$. In matrix notation, the vector of all weighted degrees is the diagonal of the degree matrix, $\\mathrm{diag}(\\mathbf{d}) = \\mathrm{diag}(W\\mathbf{1})$. Let's denote this matrix by $\\mathrm{Deg}$.\nThe diffusion term for node $i$ can then be written as:\n$$\n\\left(\\frac{d u_i}{dt}\\right)_{\\text{diffusion}} = D \\left( (W\\mathbf{u})_i - (\\mathrm{Deg}\\,\\mathbf{u})_i \\right) = -D \\left( (\\mathrm{Deg}\\,\\mathbf{u})_i - (W\\mathbf{u})_i \\right)\n$$\nThis expression corresponds to the $i$-th component of the vector $-D(\\mathrm{Deg} - W)\\mathbf{u}$. The problem defines the combinatorial graph Laplacian as $L = \\mathrm{diag}(W \\mathbf{1}) - W = \\mathrm{Deg} - W$. Therefore, the diffusion term for the entire system is:\n$$\n\\left(\\frac{d\\mathbf{u}}{dt}\\right)_{\\text{diffusion}} = -D L \\mathbf{u}\n$$\nCombining the reaction and diffusion terms, we obtain the system of $N$ coupled ODEs governing the concentration vector $\\mathbf{u}(t)$:\n$$\n\\frac{d\\mathbf{u}}{dt} = -D L \\mathbf{u} + \\mathbf{f}(\\mathbf{u})\n$$\nwhere $\\mathbf{f}(\\mathbf{u})$ is the vector of local reaction kinetics, with components $f_i(u_i) = r u_i (1 - u_i/K)$. This can be written using element-wise (Hadamard) product $\\circ$ as $\\mathbf{f}(\\mathbf{u}) = r \\mathbf{u} \\circ (\\mathbf{1} - \\mathbf{u}/K)$, where $\\mathbf{1}$ is the all-ones vector.\n\n**2. Linearization and Stability Analysis**\n\nThe next step is to analyze the stability of the disease-free equilibrium, which is $\\mathbf{u} = \\mathbf{0}$. An equilibrium point must satisfy $\\frac{d\\mathbf{u}}{dt} = \\mathbf{0}$. At $\\mathbf{u}=\\mathbf{0}$, we have $-D L (\\mathbf{0}) + \\mathbf{f}(\\mathbf{0}) = \\mathbf{0} + \\mathbf{0} = \\mathbf{0}$, confirming it is an equilibrium.\n\nTo study the local stability, we linearize the system by computing the Jacobian matrix $J$ of the right-hand side, which we denote by $\\mathbf{F}(\\mathbf{u}) = -D L \\mathbf{u} + \\mathbf{f}(\\mathbf{u})$, evaluated at $\\mathbf{u}=\\mathbf{0}$. The components of the vector field $\\mathbf{F}(\\mathbf{u})$ are:\n$$\nF_i(\\mathbf{u}) = -D \\sum_{j=1}^{N} L_{ij} u_j + r u_i - \\frac{r}{K} u_i^2\n$$\nThe entries of the Jacobian matrix are $J_{ij} = \\frac{\\partial F_i}{\\partial u_j}$.\n\nFor $i \\neq j$:\n$$\nJ_{ij} = \\frac{\\partial}{\\partial u_j} \\left( -D \\sum_{k=1}^{N} L_{ik} u_k + r u_i - \\frac{r}{K} u_i^2 \\right) = -D L_{ij}\n$$\nFor $i = j$:\n$$\nJ_{ii} = \\frac{\\partial}{\\partial u_i} \\left( -D \\sum_{k=1}^{N} L_{ik} u_k + r u_i - \\frac{r}{K} u_i^2 \\right) = -D L_{ii} + r - \\frac{2r}{K} u_i\n$$\nEvaluating the Jacobian at the equilibrium $\\mathbf{u} = \\mathbf{0}$:\n$$\nJ_{ij}(\\mathbf{0}) = -D L_{ij} \\quad \\text{for } i \\neq j\n$$\n$$\nJ_{ii}(\\mathbf{0}) = -D L_{ii} + r\n$$\nThis can be expressed in matrix form. The matrix with entries $-D L_{ij}$ is simply $-D L$. The additional term $+r$ on the diagonal corresponds to adding the matrix $rI$, where $I$ is the $N \\times N$ identity matrix. Thus, the Jacobian at the disease-free equilibrium is:\n$$\nJ(\\mathbf{0}) = rI - D L\n$$\n\n**3. Determining the Dominant Eigenvalue**\n\nThe dominant eigenvalue of the Jacobian is the eigenvalue with the largest real part, as it determines the stability of the equilibrium. Let $\\{\\mu_k\\}_{k=1}^N$ be the set of eigenvalues of the graph Laplacian $L$, and let $\\{\\mathbf{v}_k\\}_{k=1}^N$ be the corresponding set of eigenvectors.\n$$\nL \\mathbf{v}_k = \\mu_k \\mathbf{v}_k\n$$\nThe eigenvectors of $L$ are also the eigenvectors of the Jacobian $J(\\mathbf{0})$:\n$$\nJ(\\mathbf{0}) \\mathbf{v}_k = (rI - D L) \\mathbf{v}_k = r(I\\mathbf{v}_k) - D(L\\mathbf{v}_k) = r\\mathbf{v}_k - D(\\mu_k \\mathbf{v}_k) = (r - D\\mu_k)\\mathbf{v}_k\n$$\nTherefore, the eigenvalues of $J(\\mathbf{0})$, denoted by $\\{\\lambda_k\\}_{k=1}^N$, are related to the eigenvalues of $L$ by the expression:\n$$\n\\lambda_k = r - D\\mu_k\n$$\nThe problem states that the network is connected and undirected, and the adjacency matrix $W$ is symmetric. This implies that the Laplacian $L$ is a real symmetric matrix, and thus all its eigenvalues $\\mu_k$ are real. Consequently, all Jacobian eigenvalues $\\lambda_k$ are also real.\n\nThe dominant eigenvalue is $\\lambda_{\\text{dom}} = \\max_{k} \\{\\lambda_k\\}$.\n$$\n\\lambda_{\\text{dom}} = \\max_{k} \\{r - D\\mu_k\\}\n$$\nSince $r$ is a constant and the diffusion coefficient $D$ is positive ($D > 0$), maximizing this expression is equivalent to minimizing the eigenvalue $\\mu_k$ of the Laplacian.\n$$\n\\lambda_{\\text{dom}} = r - D \\min_{k} \\{\\mu_k\\}\n$$\nFor any connected, undirected graph, the spectrum of the combinatorial Laplacian $L$ is well-characterized. Its eigenvalues are real and non-negative, and they are typically ordered as $0 = \\mu_1 \\le \\mu_2 \\le \\dots \\le \\mu_N$. The smallest eigenvalue, $\\mu_1$, is always $0$, and for a connected graph, it has a multiplicity of one. The corresponding eigenvector is the all-ones vector $\\mathbf{1}$.\n\nTherefore, the minimum eigenvalue of the Laplacian is $\\min_{k} \\{\\mu_k\\} = \\mu_1 = 0$.\n\nSubstituting this value into the expression for the dominant eigenvalue:\n$$\n\\lambda_{\\text{dom}} = r - D \\cdot 0 = r\n$$\nThe dominant eigenvalue of the Jacobian matrix, linearized at the disease-free equilibrium, is equal to the intrinsic growth rate $r$.",
            "answer": "$$\n\\boxed{r}\n$$"
        },
        {
            "introduction": "A predictive model is only as good as the parameters that define it, but can these parameters be reliably estimated from experimental data? This hands-on computational exercise tackles the crucial concept of practical parameter identifiability, a common challenge in systems biology . By working with a model of longitudinal PET imaging data, you will implement and compare two powerful techniques—the Fisher Information Matrix and profile likelihoods—to determine if a model's parameters can be constrained by data, a critical step in model validation.",
            "id": "3333602",
            "problem": "Consider a simplified longitudinal Positron Emission Tomography (PET) model of misfolded protein burden in neurodegenerative disease, where aggregated species arise from production and are cleared via first-order kinetics. Let $B(t)$ denote the burden of aggregated protein at time $t$ measured in arbitrary PET units. Assume production occurs at a constant rate $k_{agg}$ and clearance occurs at rate $k_{clear}$ such that the burden obeys the ordinary differential equation (ODE)\n$$\\frac{dB}{dt} = k_{agg} - k_{clear}\\,B(t), \\quad B(0)=0,$$\nwhere $k_{agg} > 0$ and $k_{clear} > 0$ with units of day$^{-1}$. The PET signal is modeled as\n$$S(t;\\theta) = \\alpha\\,B(t),$$\nwhere $\\alpha>0$ is a known proportionality constant and the parameter vector is $\\theta = (k_{agg}, k_{clear})$. Observations $y_i$ at times $t_i$ satisfy the Gaussian measurement model\n$$y_i = S(t_i;\\theta) + \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),$$\nwith known noise standard deviation $\\sigma>0$. For the given ODE and initial condition, the burden explicitly is\n$$B(t) = \\frac{k_{agg}}{k_{clear}} \\left(1 - e^{-k_{clear} t}\\right),$$\nso that\n$$S(t;\\theta) = \\alpha \\frac{k_{agg}}{k_{clear}} \\left(1 - e^{-k_{clear} t}\\right).$$\n\nYour task is to assess practical identifiability of the parameters $k_{agg}$ and $k_{clear}$ using two complementary approaches:\n\n1. Fisher Information Matrix (FIM) computed at the true parameter values under the Gaussian noise model. The Fisher information for $\\theta=(k_{agg},k_{clear})$ is \n$$I(\\theta) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n} J(t_i;\\theta)^\\top J(t_i;\\theta),$$\nwhere $J(t;\\theta) = \\left(\\frac{\\partial S(t;\\theta)}{\\partial k_{agg}},\\,\\frac{\\partial S(t;\\theta)}{\\partial k_{clear}}\\right)$ is the gradient of the signal with respect to the parameters. Use the analytical partial derivatives\n$$\\frac{\\partial S(t;\\theta)}{\\partial k_{agg}} = \\alpha\\,\\frac{1 - e^{-k_{clear} t}}{k_{clear}},$$\n$$\\frac{\\partial S(t;\\theta)}{\\partial k_{clear}} = \\alpha\\,k_{agg}\\left[-\\frac{1 - e^{-k_{clear} t}}{k_{clear}^2} + \\frac{t\\,e^{-k_{clear} t}}{k_{clear}}\\right].$$\nDeclare the FIM-based identifiability criterion satisfied if $I(\\theta)$ is positive definite (all eigenvalues strictly positive) and its spectral condition number is strictly less than a chosen threshold $10^6$.\n\n2. Profile likelihood for each parameter. Define the sum of squared errors (SSE) for data $\\{(t_i,y_i)\\}_{i=1}^n$ as \n$$\\mathrm{SSE}(\\theta) = \\sum_{i=1}^n \\left[y_i - S(t_i;\\theta)\\right]^2.$$\nLet $\\hat{\\theta}$ be the joint minimizer of $\\mathrm{SSE}(\\theta)$ over $k_{agg}>0$, $k_{clear}>0$ (within reasonable bounds). For each parameter $\\theta_j \\in \\{k_{agg},k_{clear}\\}$, compute the profile likelihood by fixing $\\theta_j$ across a grid of values and optimizing $\\mathrm{SSE}$ over the remaining parameter, then form the profile of $-2\\log\\mathcal{L}$ up to an additive constant via \n$$\\Delta(\\theta_j) = \\frac{\\mathrm{SSE}(\\theta_j, \\hat{\\theta}_{-j}(\\theta_j)) - \\mathrm{SSE}(\\hat{\\theta})}{\\sigma^2},$$\nwhere $\\hat{\\theta}_{-j}(\\theta_j)$ denotes the optimizer of the remaining parameter given fixed $\\theta_j$. Using the chi-square threshold $\\Delta^* = 3.841459$ (corresponding to $95\\%$ confidence for $1$ degree of freedom), declare the profile-based identifiability criterion for $\\theta_j$ satisfied if the set $\\{\\theta_j: \\Delta(\\theta_j) \\le \\Delta^*\\}$ is bounded on both sides within the scanned grid.\n\nCombine both criteria to decide identifiability: a parameter is practically identifiable if both the FIM criterion and the profile likelihood criterion are satisfied.\n\nImplement a program that:\n- Generates synthetic data by evaluating $S(t;\\theta)$ at specified times and adding Gaussian noise with standard deviation $\\sigma$.\n- Computes $I(\\theta)$, checks positive definiteness and the spectral condition number.\n- Computes the joint least-squares estimate $\\hat{\\theta}$ and then the profile likelihoods for both $k_{agg}$ and $k_{clear}$, checking boundedness of the $95\\%$ confidence set within the grid.\n- Returns, for each test case, a list of two booleans $[b_{agg}, b_{clear}]$ indicating practical identifiability for $k_{agg}$ and $k_{clear}$ respectively.\n\nUse the following test suite of parameter sets and sampling schedules, ensuring all times are in days and all rates are in day$^{-1}$:\n- Test Case $1$ (well-sampled, moderate noise): $t_i = 0,2,4,\\ldots,60$; $\\alpha = 1$; $\\sigma = 0.02$; true $\\theta^\\star = (k_{agg}, k_{clear}) = (0.02, 0.1)$.\n- Test Case $2$ (well-sampled, high noise): $t_i = 0,2,4,\\ldots,60$; $\\alpha = 1$; $\\sigma = 0.10$; true $\\theta^\\star = (0.02, 0.1)$.\n- Test Case $3$ (sparse early-time sampling): $t_i = 0,1,2$; $\\alpha = 1$; $\\sigma = 0.02$; true $\\theta^\\star = (0.02, 0.1)$.\n- Test Case $4$ (slow clearance): $t_i = 0,1,2,\\ldots,30$; $\\alpha = 1$; $\\sigma = 0.02$; true $\\theta^\\star = (0.02, 0.005)$.\n\nFor all optimization tasks use parameter bounds $k_{agg}\\in[10^{-6},1.0]$ and $k_{clear}\\in[10^{-6},1.0]$. For profile likelihood grids, scan $200$ logarithmically spaced values in $[10^{-4},1.0]$ for each parameter.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3,result4]$), where each $resultj$ is itself a two-element list $[b_{agg},b_{clear}]$ of booleans for the corresponding test case.",
            "solution": "The model starts from production-clearance kinetics for aggregated protein burden $B(t)$, which is standard in computational systems biology when modeling imbalance between formation and removal of molecular species. Under first-order clearance and constant aggregation rate, the ordinary differential equation is $\\frac{dB}{dt} = k_{agg} - k_{clear} B(t)$ with $B(0) = 0$. This ODE follows directly from conservation of mass: the net rate of change is production minus the rate of removal proportional to $B(t)$. Solving the linear ODE yields \n$$B(t) = \\frac{k_{agg}}{k_{clear}} \\left(1 - e^{-k_{clear} t}\\right),$$ \nobtained by standard integrating factor methods for $\\frac{dB}{dt} + k_{clear} B = k_{agg}$ with homogeneous solution $B_h(t) = C e^{-k_{clear} t}$ and particular solution $B_p(t) = \\frac{k_{agg}}{k_{clear}}$. Enforcing $B(0) = 0$ gives the stated expression.\n\nThe PET signal is modeled as $S(t;\\theta) = \\alpha B(t)$, assuming a known linear proportionality $\\alpha$ between burden and measured signal. The Gaussian noise model $y_i = S(t_i;\\theta) + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ leverages well-tested assumptions for PET measurement error across repeated frames in longitudinal scans.\n\nFor identifiability, we use two classical approaches. The Fisher Information Matrix (FIM) under Gaussian noise for independent samples is \n$$I(\\theta) = \\frac{1}{\\sigma^2}\\sum_{i=1}^n J(t_i;\\theta)^\\top J(t_i;\\theta),$$\nwhere $J(t;\\theta)$ is the gradient of $S(t;\\theta)$ with respect to the parameters. Starting from \n$$S(t;\\theta) = \\alpha \\frac{k_{agg}}{k_{clear}}(1 - e^{-k_{clear} t}),$$ \nwe differentiate to obtain \n$$\\frac{\\partial S}{\\partial k_{agg}} = \\alpha \\frac{1 - e^{-k_{clear} t}}{k_{clear}},$$\nand \n$$\\frac{\\partial S}{\\partial k_{clear}} = \\alpha\\,k_{agg}\\left[-\\frac{1 - e^{-k_{clear} t}}{k_{clear}^2} + \\frac{t\\,e^{-k_{clear} t}}{k_{clear}}\\right],$$\nby applying the product rule to $k_{agg} k_{clear}^{-1}(1 - e^{-k_{clear} t})$ and using $\\frac{d}{dk_{clear}} e^{-k_{clear} t} = -t e^{-k_{clear} t}$. These expressions provide $J(t;\\theta)$; then $I(\\theta)$ is a $2\\times 2$ matrix aggregated over samples. For the FIM criterion, we test positive definiteness by ensuring both eigenvalues of $I(\\theta)$ are strictly positive; we also compute the spectral condition number $\\kappa(I) = \\lambda_{\\max}/\\lambda_{\\min}$ and require $\\kappa(I)  10^6$ to avoid near-singularity that would imply severe parameter correlation and poor practical identifiability.\n\nThe profile likelihood approach assesses identifiability by examining whether the likelihood surface provides bounded confidence intervals for each parameter when the other is optimized. The sum of squared errors is \n$$\\mathrm{SSE}(\\theta) = \\sum_{i=1}^n [y_i - S(t_i;\\theta)]^2.$$\nWe obtain a joint least-squares estimate $\\hat{\\theta}$ by minimizing $\\mathrm{SSE}(\\theta)$ under $k_{agg},k_{clear} > 0$ with practical bounds $[10^{-6},1.0]$ for numerical stability. For each parameter $\\theta_j$, we compute the profile \n$$\\Delta(\\theta_j) = \\frac{\\mathrm{SSE}(\\theta_j, \\hat{\\theta}_{-j}(\\theta_j)) - \\mathrm{SSE}(\\hat{\\theta})}{\\sigma^2},$$\nwhich approximates $-2\\log\\mathcal{L}$ differences under Gaussian noise up to an additive constant. Using the chi-square threshold $\\Delta^* = 3.841459$ for $95\\%$ confidence with $1$ degree of freedom, we inspect whether the set $\\{\\theta_j: \\Delta(\\theta_j) \\le \\Delta^*\\}$ is bounded within the grid scan. If the interval is bounded on both sides, the parameter exhibits a finite confidence interval, indicating practical identifiability; if it is unbounded (e.g., confidence set stretches to the boundary of the scanned grid), the parameter is not practically identifiable.\n\nAlgorithmic design:\n- Generate synthetic data $y_i$ via $y_i = S(t_i;\\theta^\\star) + \\varepsilon_i$ for each test case, using a fixed random seed for reproducibility.\n- Compute $I(\\theta^\\star)$ using the analytical gradients; check positive definiteness via eigenvalues and condition number $\\kappa(I)$.\n- Fit $\\hat{\\theta}$ by nonlinear least squares on residuals $r_i(\\theta) = y_i - S(t_i;\\theta)$ with positivity bounds $[10^{-6},1.0]$.\n- For each parameter, evaluate the profile likelihood over a logarithmic grid in $[10^{-4},1.0]$: fix $\\theta_j$, optimize $\\mathrm{SSE}$ over the other parameter, compute $\\Delta(\\theta_j)$, and determine if the $95\\%$ confidence set is bounded.\n- Combine the FIM and profile criteria to produce booleans per parameter.\n\nThe test suite covers diverse regimes:\n- Test Case $1$ provides rich sampling and moderate noise, expected to yield identifiable $k_{agg}$ and $k_{clear}$.\n- Test Case $2$ increases $\\sigma$, potentially degrading identifiability as the likelihood surface flattens.\n- Test Case $3$ uses sparse early-time samples where $S(t) \\approx \\alpha k_{agg} t$ for small $t$, making $k_{clear}$ difficult to identify since early dynamics depend weakly on $k_{clear}$.\n- Test Case $4$ employs very slow clearance $k_{clear}$, rendering the dynamic close to linear over the window; this often prevents tight identification of $k_{clear}$.\n\nThe program outputs a single line: a list of the four $[b_{agg}, b_{clear}]$ results, in order of the test cases, suitable for automated verification. Each boolean directly encodes whether the parameter is practically identifiable under the combined criteria.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import least_squares, minimize_scalar\n\ndef model_signal(t, k_agg, k_clear, alpha):\n    # S(t) = alpha * (k_agg / k_clear) * (1 - exp(-k_clear * t)), B0 = 0\n    t = np.asarray(t, dtype=float)\n    return alpha * (k_agg / k_clear) * (1.0 - np.exp(-k_clear * t))\n\ndef gradients(t, k_agg, k_clear, alpha):\n    # Analytical gradients of S(t;theta) wrt k_agg and k_clear\n    t = np.asarray(t, dtype=float)\n    exp_term = np.exp(-k_clear * t)\n    dS_dkagg = alpha * (1.0 - exp_term) / k_clear\n    # d/dk_clear of k_agg/k_clear * (1 - exp(-k_clear t))\n    term1 = -(1.0 - exp_term) / (k_clear**2)\n    term2 = (t * exp_term) / k_clear\n    dS_dkclear = alpha * k_agg * (term1 + term2)\n    return dS_dkagg, dS_dkclear\n\ndef fisher_information(t, theta, alpha, sigma):\n    k_agg, k_clear = theta\n    dS_dkagg, dS_dkclear = gradients(t, k_agg, k_clear, alpha)\n    # Build I = (1/sigma^2) sum J^T J\n    I11 = np.sum(dS_dkagg**2) / (sigma**2)\n    I22 = np.sum(dS_dkclear**2) / (sigma**2)\n    I12 = np.sum(dS_dkagg * dS_dkclear) / (sigma**2)\n    I = np.array([[I11, I12],\n                  [I12, I22]], dtype=float)\n    return I\n\ndef is_fim_identifiable(I, cond_threshold=1e6, eps=1e-12):\n    # Check positive definiteness and condition number\n    eigvals = np.linalg.eigvalsh(I)\n    pd = np.all(eigvals > eps)\n    # Avoid division by zero in condition number\n    if eigvals.min() = 0:\n        cond = np.inf\n    else:\n        cond = eigvals.max() / eigvals.min()\n    return pd and (cond  cond_threshold)\n\ndef sse(theta, t, y, alpha):\n    k_agg, k_clear = theta\n    # Clip to positive small values to avoid division by zero in model\n    if k_agg = 0 or k_clear = 0:\n        return np.inf\n    pred = model_signal(t, k_agg, k_clear, alpha)\n    res = y - pred\n    return float(np.sum(res**2))\n\ndef fit_theta(t, y, alpha, bounds=(1e-6, 1.0)):\n    # Nonlinear least squares on residuals with bounds\n    def residuals(theta):\n        return y - model_signal(t, theta[0], theta[1], alpha)\n    lb = np.array([bounds[0], bounds[0]], dtype=float)\n    ub = np.array([bounds[1], bounds[1]], dtype=float)\n    # Initial guess: moderate values\n    x0 = np.array([0.02, 0.1], dtype=float)\n    res = least_squares(residuals, x0=x0, bounds=(lb, ub), method='trf', jac='2-point', max_nfev=5000)\n    theta_hat = res.x\n    sse_hat = float(np.sum(res.fun**2))\n    return theta_hat, sse_hat\n\ndef profile_likelihood_param(param_name, t, y, alpha, sigma, theta_hat, sse_hat,\n                             grid_min=1e-4, grid_max=1.0, n_grid=200, bounds=(1e-6, 1.0)):\n    # Build logarithmic grid for the profiled parameter\n    grid = np.exp(np.linspace(np.log(grid_min), np.log(grid_max), n_grid))\n    deltas = np.empty_like(grid)\n    # Optimize the other parameter for each fixed value\n    for i, val in enumerate(grid):\n        if param_name == 'k_agg':\n            fixed_k_agg = val\n            # Optimize k_clear with bounds\n            def obj(k_clear):\n                # k_clear is scalar\n                k_clear = float(k_clear)\n                if k_clear = 0:\n                    return np.inf\n                return sse((fixed_k_agg, k_clear), t, y, alpha)\n            res = minimize_scalar(obj, bounds=bounds, method='bounded', options={'xatol': 1e-6, 'maxiter': 500})\n            sse_val = float(res.fun)\n        elif param_name == 'k_clear':\n            fixed_k_clear = val\n            def obj(k_agg):\n                k_agg = float(k_agg)\n                if k_agg = 0:\n                    return np.inf\n                return sse((k_agg, fixed_k_clear), t, y, alpha)\n            res = minimize_scalar(obj, bounds=bounds, method='bounded', options={'xatol': 1e-6, 'maxiter': 500})\n            sse_val = float(res.fun)\n        else:\n            raise ValueError(\"Unknown parameter name for profiling\")\n        deltas[i] = (sse_val - sse_hat) / (sigma**2)\n    # Identify bounded 95% CI set within the grid using chi-square threshold\n    threshold = 3.841459  # 95% for 1 dof\n    min_idx = int(np.argmin(deltas))\n    # Search left and right for threshold crossings\n    left_cross = None\n    for i in range(min_idx, -1, -1):\n        if deltas[i] >= threshold:\n            left_cross = i\n            break\n    right_cross = None\n    for i in range(min_idx, len(deltas)):\n        if deltas[i] >= threshold:\n            right_cross = i\n            break\n    # Bounded if both crossings exist and there is at least one point below threshold between them\n    bounded = (left_cross is not None) and (right_cross is not None) and (left_cross  right_cross)\n    return bounded, grid, deltas\n\ndef assess_identifiability(t, theta_true, alpha, sigma, rng):\n    # Generate synthetic data\n    t = np.asarray(t, dtype=float)\n    y_true = model_signal(t, theta_true[0], theta_true[1], alpha)\n    noise = rng.normal(loc=0.0, scale=sigma, size=t.shape)\n    y = y_true + noise\n\n    # Fisher Information at true parameters\n    I = fisher_information(t, theta_true, alpha, sigma)\n    fim_ok = is_fim_identifiable(I, cond_threshold=1e6, eps=1e-12)\n\n    # Joint fit\n    theta_hat, sse_hat = fit_theta(t, y, alpha, bounds=(1e-6, 1.0))\n\n    # Profile likelihoods\n    bounded_kagg, _, _ = profile_likelihood_param('k_agg', t, y, alpha, sigma, theta_hat, sse_hat,\n                                                  grid_min=1e-4, grid_max=1.0, n_grid=200, bounds=(1e-6, 1.0))\n    bounded_kclear, _, _ = profile_likelihood_param('k_clear', t, y, alpha, sigma, theta_hat, sse_hat,\n                                                    grid_min=1e-4, grid_max=1.0, n_grid=200, bounds=(1e-6, 1.0))\n    # Combined criterion: FIM criterion must hold AND profile boundedness must hold for each parameter\n    ident_kagg = bool(fim_ok and bounded_kagg)\n    ident_kclear = bool(fim_ok and bounded_kclear)\n    return [ident_kagg, ident_kclear]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (t_values, theta_true, alpha, sigma)\n        (np.arange(0.0, 60.0 + 1e-9, 2.0), (0.02, 0.10), 1.0, 0.02),   # Test Case 1\n        (np.arange(0.0, 60.0 + 1e-9, 2.0), (0.02, 0.10), 1.0, 0.10),   # Test Case 2\n        (np.array([0.0, 1.0, 2.0]), (0.02, 0.10), 1.0, 0.02),          # Test Case 3\n        (np.arange(0.0, 30.0 + 1e-9, 1.0), (0.02, 0.005), 1.0, 0.02),  # Test Case 4\n    ]\n\n    rng = np.random.default_rng(seed=0)\n    results = []\n    for t_vals, theta_true, alpha, sigma in test_cases:\n        res = assess_identifiability(t_vals, theta_true, alpha, sigma, rng)\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    # Each result is a list [bool, bool]; we need a single-line string with brackets and commas.\n    # Convert booleans to Python's True/False textual representation.\n    def format_result(r):\n        return \"[\" + \",\".join([\"True\" if x else \"False\" for x in r]) + \"]\"\n    print(\"[\" + \",\".join(format_result(r) for r in results) + \"]\")\n\nsolve()\n```"
        }
    ]
}