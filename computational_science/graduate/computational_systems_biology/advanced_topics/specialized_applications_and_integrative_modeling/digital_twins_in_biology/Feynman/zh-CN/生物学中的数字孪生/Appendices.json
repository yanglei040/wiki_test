{
    "hands_on_practices": [
        {
            "introduction": "数字孪生的一个基本前提是其内部状态必须是可观测的。如果无法从外部测量中推断出系统的完整状态，那么这个孪生模型在预测和控制方面将毫无用处。本练习将引导你运用非线性控制理论中的可观测性概念，为一个急性炎症模型设计一个最小化的传感器组合。你将通过符号计算发现，有时仅测量一个关键组件，就足以揭示整个系统的动态，这个看似反直觉的结论凸显了理论分析在指导高效实验设计中的强大威力。",
            "id": "3301928",
            "problem": "考虑为一个急性炎症的连续时间生物数字孪生设计一个最小传感器套件。该孪生的内部状态是三个可测量的生物量：病原体载量 $x_1$、炎症介质浓度 $x_2$ 和组织损伤 $x_3$。其内部动力学由以下平滑常微分方程（ODE）系统建模，该系统基于宿主-病原体-炎症动力学的标准相互作用规则：\n$$\n\\begin{aligned}\n\\dot{x}_1 = a x_1 \\left(1 - \\frac{x_1}{K}\\right) - k x_2 x_1, \\\\\n\\dot{x}_2 = b x_1 - g x_2 - e x_3 x_2, \\\\\n\\dot{x}_3 = d x_2 - z x_3,\n\\end{aligned}\n$$\n其中 $a, K, k, b, g, e, d, z$ 是正常数，代表具有生物学可解释性的速率和容量（例如，$a$ 是病原体生长速率，$K$ 是承载能力，$k$ 是由 $x_2$ 介导的清除率，$b$ 是病原体对介质的刺激，$g$ 是介质衰减率，$e$ 是损伤介导的介质移除，$d$ 是由 $x_2$ 驱动的损伤累积，$z$ 是损伤修复）。假设在生理学相关的操作范围内，$x_1, x_2, x_3 > 0$。\n\n您可以从标量测量函数集合 $\\{h_1(x) = x_1, \\; h_2(x) = x_2, \\; h_3(x) = x_3\\}$ 中选择输出；多传感器套件对应于这些标量输出的串联。使用基于李导数（Hermann–Krener 框架）的非线性局部可观测性秩条件，提出一个能够实现该三状态系统结构可观测性的最小传感器套件，并通过以下方式证明该选择的合理性：从所选输出的逐次李导数构造非线性可观测性矩阵，直至达到秩检验所需的阶数，并以符号方式执行秩或行列式检验。将“结构可观测性”视为一种属性，即在所声明的正值域内，对于几乎所有的状态和参数值，局部可观测性秩条件都得到满足（也就是说，失效集是一个低维流形，并且秩条件不会被模型结构恒等地违反）。\n\n将实现结构可观测性所需的最小标量输出通道数作为您的最终答案。您的最终答案应表示为一个不带单位的整数。无需四舍五入。",
            "solution": "用户希望找到实现一个三状态生物系统结构可观测性所需的最小传感器数量。\n\n### 问题验证\n该问题提供了一个非线性三维常微分方程（ODE）系统，用于模拟宿主-病原体动力学，一组可能​​的标量测量，以及一个明确的目标。所指定的方法是 Hermann-Krener 可观测性秩条件。该系统由以下方程描述：\n$$\n\\begin{aligned}\n\\dot{x}_1 = f_1(x) = a x_1 \\left(1 - \\frac{x_1}{K}\\right) - k x_2 x_1 \\\\\n\\dot{x}_2 = f_2(x) = b x_1 - g x_2 - e x_3 x_2 \\\\\n\\dot{x}_3 = f_3(x) = d x_2 - z x_3\n\\end{aligned}\n$$\n其中 $x = \\begin{pmatrix} x_1 & x_2 & x_3 \\end{pmatrix}^T$ 是状态向量。参数 $a, K, k, b, g, e, d, z$ 是正常数，且状态为正，$x_1 > 0$, $x_2 > 0$, $x_3 > 0$。可能的传感器输出是 $h_1(x) = x_1$、$h_2(x) = x_2$ 和 $h_3(x) = x_3$。\n\n该问题具有科学依据，因为该模型是数学生物学中的一个标准表示。这是一个适定问题，提供了应用指定控制理论框架所需的所有必要信息。语言客观而精确。该问题是有效的。\n\n### 求解推导\n为了确定系统的可观测性，我们应用非线性可观测性秩条件。对于一个输出为 $y = h(x)$ 且状态空间维度为 $n$ 的系统 $\\dot{x} = f(x)$，如果可观测性矩阵 $\\mathcal{O}(x)$ 的秩为 $n$，则该系统是局部可观测的。对于我们的 $n=3$ 系统，可观测性矩阵是一个 $3 \\times 3$ 的矩阵，由输出函数 $h(x)$ 的逐次李导数的梯度构成：\n$$\n\\mathcal{O}(x) = \\begin{pmatrix} \\nabla (L_f^0 h(x)) \\\\ \\nabla (L_f^1 h(x)) \\\\ \\nabla (L_f^2 h(x)) \\end{pmatrix}\n$$\n其中 $L_f^0 h(x) = h(x)$，李导数递归定义为 $L_f^i h(x) = L_f(L_f^{i-1} h(x)) = \\nabla (L_f^{i-1} h(x)) \\cdot f(x)$。结构可观测性要求对于某个 $h(x)$ 的选择，$\\det(\\mathcal{O}(x))$ 不恒等于零。\n\n我们寻求最小的传感器套件。最小可能的配置是单个传感器。我们将测试单个标量输出 $h_1(x)$、$h_2(x)$ 或 $h_3(x)$ 是否能实现可观测性。我们将分析输出为 $y = h_3(x) = x_3$ 的情况。\n\n**步骤1：零阶李导数**\n零阶李导数就是输出函数本身：\n$$L_f^0 h_3(x) = h_3(x) = x_3$$\n该函数相对于状态向量 $x = \\begin{pmatrix} x_1 & x_2 & x_3 \\end{pmatrix}^T$ 的梯度是：\n$$\\nabla (L_f^0 h_3(x)) = \\begin{pmatrix} \\frac{\\partial x_3}{\\partial x_1} & \\frac{\\partial x_3}{\\partial x_2} & \\frac{\\partial x_3}{\\partial x_3} \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}$$\n这构成了可观测性矩阵 $\\mathcal{O}_{h_3}(x)$ 的第一行。\n\n**步骤2：一阶李导数**\n一阶李导数是：\n$$L_f^1 h_3(x) = L_f(x_3) = \\nabla x_3 \\cdot f(x) = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} f_1(x) \\\\ f_2(x) \\\\ f_3(x) \\end{pmatrix} = f_3(x)$$\n代入 $f_3(x)$ 的表达式：\n$$L_f^1 h_3(x) = d x_2 - z x_3$$\n梯度是：\n$$\\nabla (L_f^1 h_3(x)) = \\begin{pmatrix} \\frac{\\partial f_3}{\\partial x_1} & \\frac{\\partial f_3}{\\partial x_2} & \\frac{\\partial f_3}{\\partial x_3} \\end{pmatrix} = \\begin{pmatrix} 0 & d & -z \\end{pmatrix}$$\n这构成了 $\\mathcal{O}_{h_3}(x)$ 的第二行。\n\n**步骤3：二阶李导数**\n二阶李导数是：\n$$L_f^2 h_3(x) = L_f(L_f^1 h_3(x)) = L_f(f_3(x)) = \\nabla f_3(x) \\cdot f(x)$$\n使用上一步计算出的梯度：\n$$L_f^2 h_3(x) = \\begin{pmatrix} 0 & d & -z \\end{pmatrix} \\begin{pmatrix} f_1(x) \\\\ f_2(x) \\\\ f_3(x) \\end{pmatrix} = d f_2(x) - z f_3(x)$$\n代入 $f_2(x)$ 和 $f_3(x)$ 的表达式：\n$$L_f^2 h_3(x) = d(b x_1 - g x_2 - e x_3 x_2) - z(d x_2 - z x_3)$$\n$$L_f^2 h_3(x) = db x_1 - dg x_2 - de x_2 x_3 - zd x_2 + z^2 x_3$$\n$$L_f^2 h_3(x) = db x_1 - (dg + zd)x_2 - de x_2 x_3 + z^2 x_3$$\n该函数的梯度是：\n$$\\nabla(L_f^2 h_3(x)) = \\begin{pmatrix} \\frac{\\partial}{\\partial x_1}(L_f^2 h_3) & \\frac{\\partial}{\\partial x_2}(L_f^2 h_3) & \\frac{\\partial}{\\partial x_3}(L_f^2 h_3) \\end{pmatrix}$$\n偏导数是：\n$$\\frac{\\partial}{\\partial x_1}(L_f^2 h_3(x)) = db$$\n$$\\frac{\\partial}{\\partial x_2}(L_f^2 h_3(x)) = -(dg + zd) - de x_3 = -(dg + zd + de x_3)$$\n$$\\frac{\\partial}{\\partial x_3}(L_f^2 h_3(x)) = -de x_2 + z^2$$\n这构成了 $\\mathcal{O}_{h_3}(x)$ 的第三行。\n\n**步骤4：构造可观测性矩阵并检验其秩**\n我们使用计算出的梯度来组装可观测性矩阵：\n$$\n\\mathcal{O}_{h_3}(x) = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & d & -z \\\\ db & -(dg + zd + de x_3) & z^2 - de x_2 \\end{pmatrix}\n$$\n为了检查秩是否为 $3$，我们计算其行列式。我们沿第一行进行余子式展开：\n$$\n\\det(\\mathcal{O}_{h_3}(x)) = 0 \\cdot \\det(\\dots) - 0 \\cdot \\det(\\dots) + 1 \\cdot \\det\\begin{pmatrix} 0 & d \\\\ db & -(dg + zd + de x_3) \\end{pmatrix}\n$$\n$$\n\\det(\\mathcal{O}_{h_3}(x)) = (0) \\cdot (-(dg + zd + de x_3)) - (d) \\cdot (db) = -d^2 b\n$$\n参数 $b$ 和 $d$ 均是正常数。因此，行列式 $\\det(\\mathcal{O}_{h_3}(x)) = -d^2 b$ 是一个非零常数。由于行列式处处非零，因此对于状态变量 $x$ 和参数的所有值，可观测性矩阵 $\\mathcal{O}_{h_3}(x)$ 都具有满秩（$3$）。\n\n这个结果表明，仅测量组织损伤 $x_3$ 就足以使系统结构上（在本例中也是全局上）可观测。由于单个传感器就足够了，所以不可能使用更少的传感器。因此，所需的最小标量输出通道数为 $1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "在确认系统可观测之后，下一步便是利用实验数据来构建和校准模型。CRISPR Perturb-seq等现代高通量技术能够产生包含因果信息的干预性数据，是推断生物网络内部因果关系的理想选择。在本练习中，你将模拟一名计算生物学家，利用模拟的Perturb-seq数据来校准一个信号网络数字孪生。你将通过对不同基因敲除和外部刺激组合下的数据进行线性回归，来估计网络的交互参数，并最终测试你所构建的孪生模型对未见过的干预组合的预测能力，这是检验任何模型泛化能力的关键一步。",
            "id": "3301917",
            "problem": "您的任务是校准一个简化的细胞信号网络数字孪生，方法是使用汇集的成簇规律间隔短回文重复序列 (CRISPR) 扰动和单细胞 RNA 测序 (Perturb-seq) 干预，然后评估其外推到未见过的干预组合的能力。该数字孪生由信号网络的常微分方程的线性化稳态近似构建而成。该网络有 $n = 4$ 个基因活动节点，并由 $m = 2$ 个外源输入通道驱动。线性化稳态结构因果模型为\n$$\n\\mathbf{x} = W \\mathbf{x} + S \\mathbf{u},\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^{4}$ 是基因 $\\{1,2,3,4\\}$ 的稳态活动向量，$\\mathbf{u} \\in \\mathbb{R}^{2}$ 是输入刺激向量，$W \\in \\mathbb{R}^{4 \\times 4}$ 是有向相互作用矩阵（对角线为零），$S \\in \\mathbb{R}^{4 \\times 2}$ 将输入映射到基因节点。稳定性要求 $W$ 的谱半径小于 $1$，以确保在没有干预的情况下 $(I - W)$ 是可逆的。\n\n干预是完美的基因敲除，形式化为 Pearl 的 do-算子。对于一个敲除集 $K \\subset \\{1,2,3,4\\}$，受干预的系统通过将 $K$ 中节点的结构方程替换为将这些变量钳位到零。对于未受干预的节点，其稳态满足\n$$\n\\mathbf{x}_{R} = W_{R,R}\\mathbf{x}_{R} + S_{R} \\mathbf{u},\n$$\n其中 $R = \\{1,2,3,4\\} \\setminus K$，$W_{R,R}$ 是限于 $R$ 中行和列的 $W$ 的子矩阵，$S_{R}$ 是 $S$ 限于 $R$ 中的行。求解可得\n$$\n\\mathbf{x}_{R} = (I_{|R|} - W_{R,R})^{-1} S_{R} \\mathbf{u},\n$$\n并且 $\\mathbf{x}_{K} = \\mathbf{0}$。\n\n用于生成观测值的合成孪生的真实参数为\n$$\nW^\\star =\n\\begin{bmatrix}\n0 & 0.15 & 0 & 0.10 \\\\\n0.05 & 0 & 0.20 & 0 \\\\\n0.10 & 0 & 0 & 0.05 \\\\\n0 & 0.10 & 0.10 & 0\n\\end{bmatrix}, \\quad\nS^\\star =\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n0.2 & 0.5 \\\\\n0.0 & 1.0 \\\\\n0.3 & 0.0\n\\end{bmatrix}.\n$$\n\n训练实验：对于\n$$\n\\mathcal{U}_{\\text{train}} = \\left\\{ \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}, \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix} \\right\\}\n$$\n中的每个输入向量，收集以下干预集的稳态观测值：\n$$\n\\mathcal{K}_{\\text{train}} = \\left\\{ \\emptyset, \\{1\\}, \\{2\\}, \\{3\\}, \\{4\\} \\right\\}.\n$$\n也就是说，对于每个 $\\mathbf{u} \\in \\mathcal{U}_{\\text{train}}$，在野生型（无敲除）和每个单基因敲除下测量 $\\mathbf{x}$。测量的训练输出由上述真实模型生成，然后被标准差为 $\\sigma = 10^{-3}$ 的独立零均值高斯测量噪声所破坏。为了可复现性，使用固定的随机种子 0。在校准期间，当构建回归特征时，无论这些通道中是否存在测量噪声，都应在设计矩阵中将被敲除的变量视为精确的零。\n\n校准目标：使用训练数据，通过最小化所有适用方程的总平方误差来估计 $\\widehat{W}$ 和 $\\widehat{S}$。对于每个节点 $i \\in \\{1,2,3,4\\}$，当节点 $i$ 在给定的训练实验中未被敲除时，强制执行线性关系\n$$\nx_i = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{4} W_{i j} x_j + \\sum_{\\ell=1}^{2} S_{i \\ell} u_\\ell,\n$$\n其中，对于在该实验中被敲除的任何 $j$，都有 $x_j = 0$。不要使用节点 $i$ 被敲除的实验中的方程。对所有 $i$ 强制施加 $W_{i i} = 0$。\n\n测试套件：校准后，通过预测三种未见过的干预和输入的稳态来评估校准后的孪生。对于每种情况，通过使用 $\\widehat{W}$ 和 $\\widehat{S}$ 求解受干预的系统来计算预测的 $\\widehat{\\mathbf{x}}$，如上所述，并将其与使用 $W^\\star$ 和 $S^\\star$ 计算的相应无噪声真实值 $\\mathbf{x}^\\star$ 进行比较。\n\n- 案例 A（未见过的双基因敲除）：$K = \\{1,3\\}$，$\\mathbf{u} = \\begin{bmatrix} 0.7 \\\\ 0.1 \\end{bmatrix}$。\n- 案例 B（未见过的输入，无敲除）：$K = \\emptyset$，$\\mathbf{u} = \\begin{bmatrix} 0.2 \\\\ 0.9 \\end{bmatrix}$。\n- 案例 C（未见过的三重组合）：$K = \\{2,4\\}$，$\\mathbf{u} = \\begin{bmatrix} 0.4 \\\\ 0.4 \\end{bmatrix}$。\n\n对于每种情况，报告均方根误差\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{4} \\sum_{i=1}^{4} \\left( \\widehat{x}_i - x_i^\\star \\right)^2 }.\n$$\n\n您的程序必须按照规定实现校准和评估，仅使用提供的训练和测试配置以及固定的噪声种子 0。最终输出必须是单行，包含案例 A、B 和 C 的三个 RMSE 值，按此顺序排列，形式为逗号分隔的列表，并用方括号括起，例如 $\\left[ r_A, r_B, r_C \\right]$，其中每个 $r_\\cdot$ 是一个浮点数。此问题不涉及物理单位。不涉及角度。不使用百分比。输出必须是指定格式的单行，不含任何额外文本。程序必须是一个完整的、可运行的程序。",
            "solution": "该问题要求校准一个简化的细胞信号网络数字孪生，并评估其预测能力。该过程包括三个主要阶段：生成合成训练数据、使用这些数据校准模型参数，以及在未见过的测试案例上评估校准后的模型。\n\n该数字孪生由一个线性化稳态结构因果模型表示：\n$$\n\\mathbf{x} = W \\mathbf{x} + S \\mathbf{u}\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^{4}$ 是基因活动向量，$\\mathbf{u} \\in \\mathbb{R}^{2}$ 是外部刺激向量，$W \\in \\mathbb{R}^{4 \\times 4}$ 是基因-基因相互作用矩阵，其中对于所有 $i \\in \\{1,2,3,4\\}$ 都有 $W_{ii} = 0$，$S \\in \\mathbb{R}^{4 \\times 2}$ 是将输入映射到基因的矩阵。为保证稳定性，$W$ 的谱半径必须小于 1。\n\n干预使用 Pearl 的 `do`-算子建模为完美的基因敲除。对于一组被敲除的基因 $K \\subset \\{1,2,3,4\\}$，这些基因的活动被钳位到零，即 $\\mathbf{x}_{K} = \\mathbf{0}$。其余基因（由集合 $R = \\{1,2,3,4\\} \\setminus K$ 索引）的活动通过求解简化系统来确定：\n$$\n\\mathbf{x}_{R} = W_{R,R}\\mathbf{x}_{R} + S_{R} \\mathbf{u}\n$$\n其中 $W_{R,R}$ 和 $S_{R}$ 是对应于 $R$ 中基因的行和列的子矩阵。这得出的解为：\n$$\n\\mathbf{x}_{R} = (I_{|R|} - W_{R,R})^{-1} S_{R} \\mathbf{u}\n$$\n其中 $I_{|R|}$ 是大小为 $|R|$ 的单位矩阵。\n\n**第 1 部分：训练数据生成**\n\n首先，我们生成一个用于训练的合成数据集。该数据集源自真实模型参数：\n$$\nW^\\star =\n\\begin{bmatrix}\n0 & 0.15 & 0 & 0.10 \\\\\n0.05 & 0 & 0.20 & 0 \\\\\n0.10 & 0 & 0 & 0.05 \\\\\n0 & 0.10 & 0.10 & 0\n\\end{bmatrix}, \\quad\nS^\\star =\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n0.2 & 0.5 \\\\\n0.0 & 1.0 \\\\\n0.3 & 0.0\n\\end{bmatrix}\n$$\n训练实验包括来自 $\\mathcal{U}_{\\text{train}}$ 的所有输入向量和来自 $\\mathcal{K}_{\\text{train}}$ 的所有干预集的组合：\n$$\n\\mathcal{U}_{\\text{train}} = \\left\\{ \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}, \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix} \\right\\}, \\quad\n\\mathcal{K}_{\\text{train}} = \\left\\{ \\emptyset, \\{1\\}, \\{2\\}, \\{3\\}, \\{4\\} \\right\\}\n$$\n这产生了 $3 \\times 5 = 15$ 个独特的实验。对于每一对 $(\\mathbf{u}, K)$，我们使用带有 $W^\\star$ 和 $S^\\star$ 的干预解方程计算无噪声的稳态向量 $\\mathbf{x}^\\star$。然后，为了模拟测量误差，我们向 $\\mathbf{x}^\\star$ 的每个分量添加标准差为 $\\sigma = 10^{-3}$ 的独立高斯噪声 $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$。得到的噪声向量 $\\mathbf{x}_{\\text{obs}} = \\mathbf{x}^\\star + \\boldsymbol{\\epsilon}$ 构成了我们训练集中的一个观测值。使用固定的随机种子 0 来确保噪声生成的可复现性。\n\n**第 2 部分：模型校准**\n\n校准的目标是根据生成的训练数据估计模型参数 $\\widehat{W}$ 和 $\\widehat{S}$。估计是通过最小化所有适用结构方程的总平方误差来进行的。由于方程是解耦的，我们可以为每个基因 $i \\in \\{1,2,3,4\\}$ 单独进行线性回归。\n\n对于每个基因 $i$，我们拟合线性模型：\n$$\nx_i = \\sum_{j=1, j \\neq i}^{4} W_{i j} x_j + \\sum_{\\ell=1}^{2} S_{i \\ell} u_\\ell\n$$\n基因 $i$ 需要估计的参数是 $W$ 的第 $i$ 行的 3 个非对角线元素和 $S$ 的第 $i$ 行的 2 个元素，形成一个参数向量 $\\boldsymbol{\\theta}_i \\in \\mathbb{R}^5$。\n\n此回归的数据来源于所有基因 $i$ *未*被敲除的训练实验。对于每个这样的实验 $(\\mathbf{u}^{(k)}, K^{(k)}, \\mathbf{x}_{\\text{obs}}^{(k)})$，其中 $i \\notin K^{(k)}$，我们形成一个数据点。回归目标是基因 $i$ 的观测活动，$y_k = (\\mathbf{x}_{\\text{obs}}^{(k)})_i$。特征向量 $\\boldsymbol{\\phi}_k$ 由其他基因的活动和输入构成。按照规定，在特征向量中，任何被敲除基因 $j \\in K^{(k)}$ 的活动都被视为精确的 $0$，无论由于噪声其测量值是否为零。因此，我们构建一个特征专用状态向量 $\\mathbf{x}_{\\text{feat}}^{(k)}$，其中如果 $j \\notin K^{(k)}$，则 $(\\mathbf{x}_{\\text{feat}}^{(k)})_j = (\\mathbf{x}_{\\text{obs}}^{(k)})_j$，否则为 $0$。然后特征向量为 $\\boldsymbol{\\phi}_k = [(\\mathbf{x}_{\\text{feat}}^{(k)})_{j \\neq i}, \\mathbf{u}^{(k)}]$。\n\n对于每个基因 $i$，我们收集所有这些目标-特征对以形成矩阵方程 $Y_i = \\Phi_i \\boldsymbol{\\theta}_i + \\text{errors}$。参数向量 $\\widehat{\\boldsymbol{\\theta}}_i$ 使用普通最小二乘法估计：\n$$\n\\widehat{\\boldsymbol{\\theta}}_i = (\\Phi_i^T \\Phi_i)^{-1} \\Phi_i^T Y_i\n$$\n然后通过用 $\\widehat{\\boldsymbol{\\theta}}_i$ 中的相应元素填充其第 $i$ 行来组装估计矩阵 $\\widehat{W}$ 和 $\\widehat{S}$，并确保 $\\widehat{W}_{ii} = 0$。\n\n**第 3 部分：在未见过的测试案例上进行评估**\n\n由 $(\\widehat{W}, \\widehat{S})$ 定义的校准后数字孪生在三个未见过的测试案例上进行评估：\n- 案例 A：双基因敲除 $K = \\{1,3\\}$，输入为 $\\mathbf{u} = [0.7, 0.1]^T$。\n- 案例 B：未见过的输入 $\\mathbf{u} = [0.2, 0.9]^T$，无敲除，$K = \\emptyset$。\n- 案例 C：双基因敲除 $K = \\{2,4\\}$，输入为 $\\mathbf{u} = [0.4, 0.4]^T$。\n\n对于每个测试案例，我们执行两次计算：\n1.  预测状态 $\\widehat{\\mathbf{x}}$ 是通过使用校准参数 $(\\widehat{W}, \\widehat{S})$ 求解受干预系统来计算的。\n2.  真实状态 $\\mathbf{x}^\\star$ 是通过使用真实参数 $(W^\\star, S^\\star)$ 求解同一系统来计算的，代表了无噪声的现实情况。\n\n性能通过预测与真实值之间的均方根误差 (RMSE) 来量化：\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{4} \\sum_{i=1}^{4} \\left( \\widehat{x}_i - x_i^\\star \\right)^2 }\n$$\n最终输出包含为案例 A、B 和 C 计算的三个 RMSE 值。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calibrates and evaluates a digital twin of a cell signaling network.\n    The process involves:\n    1. Generating noisy training data from a ground-truth model.\n    2. Calibrating the model parameters (W, S) using least-squares regression.\n    3. Evaluating the calibrated model's predictions on unseen test cases against the noise-free ground truth.\n    \"\"\"\n    \n    # --- Ground-truth parameters and constants ---\n    W_star = np.array([\n        [0.0, 0.15, 0.0, 0.10],\n        [0.05, 0.0, 0.20, 0.0],\n        [0.10, 0.0, 0.0, 0.05],\n        [0.0, 0.10, 0.10, 0.0]\n    ])\n    \n    S_star = np.array([\n        [1.0, 0.0],\n        [0.2, 0.5],\n        [0.0, 1.0],\n        [0.3, 0.0]\n    ])\n    \n    sigma = 1e-3\n    num_genes = 4\n    num_inputs = 2\n    \n    # --- Training and test configurations ---\n    u_train = [\n        np.array([1.0, 0.0]),\n        np.array([0.0, 1.0]),\n        np.array([0.5, 0.5])\n    ]\n    # Intervention sets (1-based), will be converted to 0-based sets.\n    k_train_tuples = [(), (1,), (2,), (3,), (4,)]\n    \n    test_cases = [\n        {'K': {1, 3}, 'u': np.array([0.7, 0.1])},  # Case A\n        {'K': set(),  'u': np.array([0.2, 0.9])},  # Case B\n        {'K': {2, 4}, 'u': np.array([0.4, 0.4])}   # Case C\n    ]\n\n    def solve_intervened_system(W, S, u, K_0based):\n        \"\"\"\n        Solves for the steady-state vector x for a given intervened system.\n        \"\"\"\n        all_nodes = set(range(num_genes))\n        R_0based = sorted(list(all_nodes - K_0based))\n        \n        if not R_0based:\n            return np.zeros(num_genes)\n        \n        R_indices = np.array(R_0based)\n        W_RR = W[np.ix_(R_indices, R_indices)]\n        S_R = S[R_indices, :]\n        \n        I_R = np.identity(len(R_0based))\n        \n        try:\n            # Solve (I_R - W_RR) * x_R = S_R * u for x_R\n            x_R = np.linalg.solve(I_R - W_RR, S_R @ u)\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix, though unlikely with stable W\n            return np.full(num_genes, np.nan)\n        \n        x = np.zeros(num_genes)\n        x[R_indices] = x_R\n        return x\n\n    # --- 1. Generate training data ---\n    rng = np.random.default_rng(0)\n    training_data = []\n    for u in u_train:\n        for k_tuple in k_train_tuples:\n            K_0based = {k - 1 for k in k_tuple}\n            \n            x_star = solve_intervened_system(W_star, S_star, u, K_0based)\n            noise = rng.normal(loc=0.0, scale=sigma, size=num_genes)\n            x_obs = x_star + noise\n            training_data.append({'u': u, 'K': K_0based, 'x_obs': x_obs})\n            \n    # --- 2. Calibrate model (estimate W_hat, S_hat) ---\n    W_hat = np.zeros((num_genes, num_genes))\n    S_hat = np.zeros((num_genes, num_inputs))\n    \n    for i in range(num_genes): # For each gene i (0 to 3)\n        Phi_list = []\n        Y_list = []\n        \n        for exp in training_data:\n            if i not in exp['K']: # Use experiments where gene i is not knocked out\n                # Target is the observed value of gene i\n                Y_list.append(exp['x_obs'][i])\n                \n                # Feature vector construction\n                x_feat = exp['x_obs'].copy()\n                for k_node in exp['K']:\n                    x_feat[k_node] = 0.0 # Enforce knockout to be exactly zero\n                \n                x_features = np.delete(x_feat, i)\n                u_features = exp['u']\n                phi = np.concatenate((x_features, u_features))\n                Phi_list.append(phi)\n                \n        Phi = np.array(Phi_list)\n        Y = np.array(Y_list)\n        \n        # Solve the least squares problem: Phi * theta = Y\n        theta, _, _, _ = np.linalg.lstsq(Phi, Y, rcond=None)\n        \n        # Unpack theta into W_hat and S_hat rows\n        w_row_hat_flat = theta[:num_genes - 1]\n        s_row_hat = theta[num_genes - 1:]\n        \n        W_hat[i, :] = np.insert(w_row_hat_flat, i, 0.0)\n        S_hat[i, :] = s_row_hat\n\n    # --- 3. Evaluate on test suite ---\n    rmse_results = []\n    for case in test_cases:\n        K_test_1based = case['K']\n        u_test = case['u']\n        \n        K_test_0based = {k - 1 for k in K_test_1based}\n        \n        # Prediction from calibrated model\n        x_hat = solve_intervened_system(W_hat, S_hat, u_test, K_test_0based)\n        \n        # Ground truth (noise-free)\n        x_star_test = solve_intervened_system(W_star, S_star, u_test, K_test_0based)\n        \n        # Compute RMSE\n        rmse = np.sqrt(np.mean((x_hat - x_star_test)**2))\n        rmse_results.append(rmse)\n        \n    # --- Final Output ---\n    print(f\"[{','.join(f'{r:.10f}' for r in rmse_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个真正先进的数字孪生不仅能模拟生物系统，更能主动指导未来的研究方向。当面对关于生物机制的多种竞争性假说时，我们可以利用数字孪生来设计一个信息量最大的实验，以最有效的方式区分这些假说。本练习将带你进入贝叶斯最优实验设计（Bayesian Optimal Experimental Design, OED）的前沿领域，这是一个旨在最大化实验信息收益的强大框架。你将通过最大化预期贝叶斯因子（Expected Bayes Factor）来选择最优实验条件，从而高效地辨别两个竞争的信号通路模型，深刻体会数字孪生在驱动“模型-预测-实验”这一科学发现闭环中的核心价值。",
            "id": "3301904",
            "problem": "生物学中的数字孪生（DTB）是一个生物系统的计算表示，它会用数据持续更新。考虑一个 DTB，其任务是通过选择一个实验刺激浓度 $u$（单位为纳摩尔，nM）来区分两个竞争性的机理通路假说 $\\mathcal{M}_1$ 和 $\\mathcal{M}_2$。选择的依据是，当 $\\mathcal{M}_1$ 是数据生成模型时，最大化支持 $\\mathcal{M}_1$ 的贝叶斯因子（Bayes factor (BF)）的期望对数。在模型 $\\mathcal{M}_i$ 下，可观测量 $y$（无量纲，例如归一化的报告基因强度）遵循一个具有已知方差的高斯噪声模型和一个单参数、参数线性的机理映射。具体来说，设机理映射为 $f_i(u,\\theta_i) = \\theta_i \\, g(u)$，其中 $g(u)$ 是一个固定的、已知的灵敏度函数，用于表征通路在刺激 $u$ 下的输入-输出映射，而 $\\theta_i$ 是一个特定于模型的标量参数。假设高斯先验为 $\\theta_i \\sim \\mathcal{N}(\\mu_{0,i}, \\tau_i^2)$，条件高斯测量模型为 $y \\mid \\theta_i, \\mathcal{M}_i \\sim \\mathcal{N}(f_i(u,\\theta_i), \\sigma^2)$，其中测量方差 $\\sigma^2$ 已知。对于每个指定的测试用例，DTB 必须计算实验刺激 $u^\\star$，以最大化期望对数贝叶斯因子 $\\mathbb{E}_{y \\sim p(y \\mid \\mathcal{M}_1)}\\left[\\log \\mathrm{BF}_{1,2}(u)\\right]$，其中 $\\mathrm{BF}_{1,2}(u) = \\dfrac{p(y \\mid \\mathcal{M}_1)}{p(y \\mid \\mathcal{M}_2)}$，而 $p(y \\mid \\mathcal{M}_i)$ 表示模型 $\\mathcal{M}_i$ 下的模型证据（边际似然）。\n\n您的程序必须：\n- 从基本贝叶斯原理出发，推导如何将 $\\mathbb{E}_{y \\sim p(y \\mid \\mathcal{M}_1)}\\left[\\log \\mathrm{BF}_{1,2}(u)\\right]$ 计算为 $u$、$\\sigma^2$、$\\mu_{0,i}$、$\\tau_i^2$ 和 $g(u)$ 的函数，推导过程需从高斯先验、高斯似然和边际化的定义开始。\n- 在闭区间 $[0, u_{\\max}]$ 上，使用指定的网格分辨率 $\\Delta u$进行搜索，以找到 $u^\\star = \\arg\\max_{u \\in \\{0, \\Delta u, 2 \\Delta u, \\ldots, u_{\\max}\\}} \\mathbb{E}_{y \\sim p(y \\mid \\mathcal{M}_1)}\\left[\\log \\mathrm{BF}_{1,2}(u)\\right]$。如果多个 $u$ 值达到相同的最大值，则选择其中最小的 $u$。\n- 输出以纳摩尔 (nM) 为单位的 $u^\\star$，并四舍五入到三位小数。\n\n使用以下测试套件，该套件通过改变灵敏度函数 $g(u)$ 和概率参数来探究一般情况、边界情况和边缘情况。对于每种情况，灵敏度函数 $g(u)$ 要么是线性的 $g(u) = u$，要么是类似 Michaelis–Menten 的饱和形式 $g(u) = \\dfrac{u}{K + u}$，其中半饱和常数 $K$ 的单位为 nM。以下所有量和常数均以其规定单位表示，所有数值常数均为实值。\n\n- 测试用例 1（一般情况，线性灵敏度）：\n  - $\\sigma^2 = 1.0$, $\\mu_{0,1} = 1.0$, $\\tau_1^2 = 0.5$, $\\mu_{0,2} = 0.2$, $\\tau_2^2 = 0.5$。\n  - $g(u) = u$。\n  - $u_{\\max} = 10.0$ nM, $\\Delta u = 0.001$ nM。\n\n- 测试用例 2（边界情况，无法区分的先验和均值）：\n  - $\\sigma^2 = 1.0$, $\\mu_{0,1} = 0.0$, $\\tau_1^2 = 0.5$, $\\mu_{0,2} = 0.0$, $\\tau_2^2 = 0.5$。\n  - $g(u) = u$。\n  - $u_{\\max} = 10.0$ nM, $\\Delta u = 0.001$ nM。\n\n- 测试用例 3（边缘情况，饱和灵敏度及不同先验信息量）：\n  - $\\sigma^2 = 0.5$, $\\mu_{0,1} = 2.0$, $\\tau_1^2 = 0.01$, $\\mu_{0,2} = 1.0$, $\\tau_2^2 = 1.0$。\n  - $g(u) = \\dfrac{u}{K + u}$，其中 $K = 1.0$ nM。\n  - $u_{\\max} = 100.0$ nM, $\\Delta u = 0.1$ nM。\n\n- 测试用例 4（仅方差区分，均值相等，先验方差不同）：\n  - $\\sigma^2 = 0.5$, $\\mu_{0,1} = 0.0$, $\\tau_1^2 = 0.01$, $\\mu_{0,2} = 0.0$, $\\tau_2^2 = 1.0$。\n  - $g(u) = u$。\n  - $u_{\\max} = 20.0$ nM, $\\Delta u = 0.01$ nM。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按上述测试用例的顺序列出结果。每个条目必须是一个浮点数，给出以 nM 为单位的 $u^\\star$，四舍五入到三位小数，例如 $\\left[\\text{case}_1,\\text{case}_2,\\text{case}_3,\\text{case}_4\\right]$，其中每个 $\\text{case}_i$ 是对应情况计算出的最优刺激。",
            "solution": "该问题是有效的。它在科学上基于贝叶斯统计和最优实验设计，问题定义清晰，具有明确的目标和约束，并使用精确、客观的语言进行表述。\n\n任务是找到刺激 $u^\\star$，以最大化模型 $\\mathcal{M}_1$ 相对于 $\\mathcal{M}_2$ 的期望对数贝叶斯因子，其中期望是基于由模型 $\\mathcal{M}_1$ 生成的数据计算的。该目标函数等价于两个模型预测分布之间的 Kullback-Leibler (KL) 散度。\n\n$u^\\star = \\arg\\max_{u} \\mathbb{E}_{y \\sim p(y \\mid \\mathcal{M}_1)}\\left[\\log \\frac{p(y \\mid u, \\mathcal{M}_1)}{p(y \\mid u, \\mathcal{M}_2)}\\right] = \\arg\\max_{u} D_{KL}\\left(p(y \\mid u, \\mathcal{M}_1) \\Vert p(y \\mid u, \\mathcal{M}_2)\\right)$\n\n首先，我们必须为每个模型 $\\mathcal{M}_i$ 推导其边际似然，即模型证据 $p(y \\mid u, \\mathcal{M}_i)$。该模型由参数 $\\theta_i$ 的高斯先验和观测值 $y$ 的高斯似然定义。\n\n对于模型 $\\mathcal{M}_i$：\n1.  **先验**：参数 $\\theta_i$ 具有高斯先验分布：\n    $$p(\\theta_i \\mid \\mathcal{M}_i) = \\mathcal{N}(\\theta_i; \\mu_{0,i}, \\tau_i^2)$$\n    其中 $\\mu_{0,i}$ 是先验均值，$\\tau_i^2$ 是先验方差。\n\n2.  **似然**：在给定 $\\theta_i$ 和刺激 $u$ 的情况下，可观测量 $y$ 遵循高斯分布：\n    $$p(y \\mid \\theta_i, u, \\mathcal{M}_i) = \\mathcal{N}(y; f_i(u, \\theta_i), \\sigma^2)$$\n    其中均值是机理映射 $f_i(u, \\theta_i) = \\theta_i g(u)$，$\\sigma^2$ 是已知的测量方差。\n\n通过对参数 $\\theta_i$ 进行积分，可以得到边际似然 $p(y \\mid u, \\mathcal{M}_i)$：\n$$p(y \\mid u, \\mathcal{M}_i) = \\int p(y \\mid \\theta_i, u, \\mathcal{M}_i) p(\\theta_i \\mid \\mathcal{M}_i) d\\theta_i$$\n这是一个标准的高斯卷积，其结果是 $y$ 的一个高斯分布。我们来确定其均值和方差。\n\n边际预测分布的均值为：\n$$\\mu_{y,i} = \\mathbb{E}[y \\mid u, \\mathcal{M}_i] = \\mathbb{E}_{\\theta_i \\sim p(\\theta_i \\mid \\mathcal{M}_i)}[\\mathbb{E}[y \\mid \\theta_i, u, \\mathcal{M}_i]]$$\n$$\\mu_{y,i} = \\mathbb{E}_{\\theta_i}[\\theta_i g(u)] = g(u) \\mathbb{E}_{\\theta_i}[\\theta_i] = \\mu_{0,i} g(u)$$\n\n边际预测分布的方差可使用全方差定律求得：\n$$\\sigma_{y,i}^2 = \\text{Var}(y \\mid u, \\mathcal{M}_i) = \\mathbb{E}_{\\theta_i}[\\text{Var}(y \\mid \\theta_i, u, \\mathcal{M}_i)] + \\text{Var}_{\\theta_i}(\\mathbb{E}[y \\mid \\theta_i, u, \\mathcal{M}_i])$$\n$$\\sigma_{y,i}^2 = \\mathbb{E}_{\\theta_i}[\\sigma^2] + \\text{Var}_{\\theta_i}(\\theta_i g(u))$$\n$$\\sigma_{y,i}^2 = \\sigma^2 + g(u)^2 \\text{Var}_{\\theta_i}(\\theta_i) = \\sigma^2 + g(u)^2 \\tau_i^2$$\n\n因此，模型 $\\mathcal{M}_i$ 的边际似然是一个高斯分布：\n$$p(y \\mid u, \\mathcal{M}_i) = \\mathcal{N}(y; \\mu_{y,i}, \\sigma_{y,i}^2) = \\mathcal{N}(y; \\mu_{0,i} g(u), \\sigma^2 + \\tau_i^2 g(u)^2)$$\n\n现在我们可以写出目标函数，即两个高斯预测分布 $p(y \\mid u, \\mathcal{M}_1)$ 和 $p(y \\mid u, \\mathcal{M}_2)$ 之间的 KL 散度。令 $p_1 = \\mathcal{N}(\\mu_{y,1}, \\sigma_{y,1}^2)$ 和 $p_2 = \\mathcal{N}(\\mu_{y,2}, \\sigma_{y,2}^2)$。KL 散度由下式给出：\n$$D_{KL}(p_1 \\Vert p_2) = \\log\\frac{\\sigma_{y,2}}{\\sigma_{y,1}} + \\frac{\\sigma_{y,1}^2 + (\\mu_{y,1} - \\mu_{y,2})^2}{2\\sigma_{y,2}^2} - \\frac{1}{2}$$\n代入均值和方差的表达式：\n- $\\mu_{y,1} = \\mu_{0,1} g(u)$\n- $\\sigma_{y,1}^2 = \\sigma^2 + \\tau_1^2 g(u)^2$\n- $\\mu_{y,2} = \\mu_{0,2} g(u)$\n- $\\sigma_{y,2}^2 = \\sigma^2 + \\tau_2^2 g(u)^2$\n\n需要最大化的目标函数 $F(u)$ 变为：\n$$F(u) = \\frac{1}{2} \\log\\left(\\frac{\\sigma^2 + \\tau_2^2 g(u)^2}{\\sigma^2 + \\tau_1^2 g(u)^2}\\right) + \\frac{(\\sigma^2 + \\tau_1^2 g(u)^2) + (\\mu_{0,1} g(u) - \\mu_{0,2} g(u))^2}{2(\\sigma^2 + \\tau_2^2 g(u)^2)} - \\frac{1}{2}$$\n简化第二项：\n$$F(u) = \\frac{1}{2} \\log\\left(\\frac{\\sigma^2 + \\tau_2^2 g(u)^2}{\\sigma^2 + \\tau_1^2 g(u)^2}\\right) + \\frac{\\sigma^2 + (\\tau_1^2 + (\\mu_{0,1} - \\mu_{0,2})^2) g(u)^2}{2(\\sigma^2 + \\tau_2^2 g(u)^2)} - \\frac{1}{2}$$\n\n这就是期望对数贝叶斯因子的最终解析表达式。问题要求在一个指定的离散集合中找到使该函数 $F(u)$ 最大化的 $u$ 值。\n\n求解策略如下：\n1. 对每个测试用例，定义参数 $\\sigma^2, \\mu_{0,1}, \\tau_1^2, \\mu_{0,2}, \\tau_2^2$ 和函数 $g(u)$。\n2. 创建一个从 $0$ 到 $u_{\\max}$ 的离散刺激值网格 $u$，步长为 $\\Delta u$。\n3. 对于网格中的每个 $u$ 值，使用推导出的公式计算目标函数 $F(u)$ 的值。\n4. 在网格上找到 $F(u)$ 的最大值。\n5. 最优刺激 $u^\\star$ 是与该最大值对应的 $u$ 值。如果多个 $u$ 值产生相同的最大值，则按问题陈述选择最小的 $u$。通过找到第一个最大值的索引可以自然地处理此问题。\n6. 每个用例的最终结果四舍五入到三位小数。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal experimental stimulus u* for model discrimination\n    based on maximizing the expected log Bayes factor.\n    \"\"\"\n    test_cases = [\n        # Test Case 1\n        {'sigma2': 1.0, 'mu01': 1.0, 'tau1_2': 0.5, 'mu02': 0.2, 'tau2_2': 0.5,\n         'g_type': 'linear', 'K': None, 'u_max': 10.0, 'delta_u': 0.001},\n        # Test Case 2\n        {'sigma2': 1.0, 'mu01': 0.0, 'tau1_2': 0.5, 'mu02': 0.0, 'tau2_2': 0.5,\n         'g_type': 'linear', 'K': None, 'u_max': 10.0, 'delta_u': 0.001},\n        # Test Case 3\n        {'sigma2': 0.5, 'mu01': 2.0, 'tau1_2': 0.01, 'mu02': 1.0, 'tau2_2': 1.0,\n         'g_type': 'saturating', 'K': 1.0, 'u_max': 100.0, 'delta_u': 0.1},\n        # Test Case 4\n        {'sigma2': 0.5, 'mu01': 0.0, 'tau1_2': 0.01, 'mu02': 0.0, 'tau2_2': 1.0,\n         'g_type': 'linear', 'K': None, 'u_max': 20.0, 'delta_u': 0.01},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        sigma2 = case['sigma2']\n        mu01 = case['mu01']\n        tau1_2 = case['tau1_2']\n        mu02 = case['mu02']\n        tau2_2 = case['tau2_2']\n        u_max = case['u_max']\n        delta_u = case['delta_u']\n\n        if case['g_type'] == 'linear':\n            # Sensitivity function g(u) = u\n            g_func = lambda u: u\n        else:  # 'saturating'\n            # Sensitivity function g(u) = u / (K + u)\n            K = case['K']\n            g_func = lambda u: u / (K + u)\n\n        # Create the grid of stimulus values u.\n        # Use np.linspace for robust handling of floating-point steps.\n        num_points = int(np.round(u_max / delta_u)) + 1\n        u_grid = np.linspace(0.0, u_max, num_points)\n\n        # Evaluate g(u) over the grid.\n        g_vals = g_func(u_grid)\n        g_vals_sq = g_vals**2\n\n        # Calculate the components of the objective function (KL divergence)\n        # in a vectorized manner.\n\n        # Variances of the predictive distributions\n        sigma_y1_sq = sigma2 + tau1_2 * g_vals_sq\n        sigma_y2_sq = sigma2 + tau2_2 * g_vals_sq\n\n        # The objective function F(u) is the KL-divergence D_KL( p(y|M1) || p(y|M2) )\n        \n        # Term 1: 0.5 * log(sigma_y2^2 / sigma_y1^2)\n        log_term = 0.5 * np.log(sigma_y2_sq / sigma_y1_sq)\n\n        # Term 2: (sigma_y1^2 + (mu_y1-mu_y2)^2) / (2*sigma_y2^2)\n        mu_diff_sq = (mu01 - mu02)**2\n        frac_numerator = sigma2 + (tau1_2 + mu_diff_sq) * g_vals_sq\n        frac_term = frac_numerator / (2.0 * sigma_y2_sq)\n\n        # Full objective function\n        objective_values = log_term + frac_term - 0.5\n\n        # Find the index of the maximum value.\n        # np.argmax returns the index of the first occurrence of the maximum,\n        # which satisfies the tie-breaking rule (choose the smallest u).\n        best_idx = np.argmax(objective_values)\n        u_star = u_grid[best_idx]\n\n        results.append(f\"{u_star:.3f}\")\n\n    # Print results in the required format: [case1,case2,case3,case4]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}