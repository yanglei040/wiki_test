{
    "hands_on_practices": [
        {
            "introduction": "一个有意义的表征应该能够清晰地反映数据的内在规律，然而，变分自编码器（VAE）的潜在空间可能存在固有的模糊性。本练习将通过一个思想实验，构建两个可以生成完全相同数据分布但其潜在表征含义完全相反的模型 ()。这个实践旨在揭示潜在变量模型的符号不确定性问题，并强调在解释 VAE 学习到的表征时需要谨慎。",
            "id": "3357963",
            "problem": "考虑一个计算系统生物学中的合成单基因表达情景，其中一个潜通路活性变量 $z$ 影响一个对数尺度的表达测量值 $x$。变分自编码器 (VAE; Variational Autoencoder) 定义了一个生成模型，该模型包含先验 $p(z)$、解码器 $p_{\\theta}(x \\mid z)$ 以及一个近似真实后验 $p_{\\theta}(z \\mid x)$ 的编码器 $q_{\\phi}(z \\mid x)$。假设 $z$ 是一个一维连续潜变量。假设测量噪声是加性的高斯噪声，反映了经过适当归一化后的技术和生物变异性。\n\n构建两个编码器-解码器对（模型 $1$ 和模型 $2$），它们产生相同的边缘分布 $p_{\\theta}(x)$，但对于相同的 $x$ 会导出不同的潜表示 $z$。具体来说，对于一个固定的斜率参数 $w \\neq 0$，一个固定的均值水平 $\\mu$，以及一个固定的噪声方差 $\\sigma_{x}^{2} > 0$，定义如下：\n\n- 模型 $1$：先验 $p_{1}(z) = \\mathcal{N}(0, 1)$，解码器 $p_{1}(x \\mid z) = \\mathcal{N}(w z + \\mu, \\sigma_{x}^{2})$。编码器被约束为高斯分布 $q_{1}(z \\mid x) = \\mathcal{N}(m_{1}(x), s^{2})$，其方差 $s^{2}$ 等于模型 $1$ 下的精确后验方差，均值 $m_{1}(x)$ 等于模型 $1$ 下的精确后验均值。\n- 模型 $2$：先验 $p_{2}(z') = \\mathcal{N}(0, 1)$，解码器 $p_{2}(x \\mid z') = \\mathcal{N}((-w) z' + \\mu, \\sigma_{x}^{2})$。编码器被约束为高斯分布 $q_{2}(z' \\mid x) = \\mathcal{N}(m_{2}(x), s^{2})$，其方差 $s^{2}$ 与模型 $1$ 相同，等于模型 $2$ 下的精确后验方差，均值 $m_{2}(x)$ 等于模型 $2$ 下的精确后验均值。\n\n从高斯先验、高斯似然以及线性高斯模型的贝叶斯法则的定义出发，执行以下操作：\n\n- 推导模型 $1$ 的边缘分布 $p_{1}(x)$ 和模型 $2$ 的边缘分布 $p_{2}(x)$，并证明 $p_{1}(x) = p_{2}(x)$。\n- 在各自的模型下，推导精确后验均值 $m_{1}(x)$ 和 $m_{2}(x)$，以及共享的后验方差 $s^{2}$。\n- 对于一个固定的观测值 $x$，推导从 $q_{1}(z \\mid x)$ 到 $q_{2}(z' \\mid x)$ 的 Kullback–Leibler 散度 (KL; Kullback–Leibler divergence)。\n- 计算该 KL 散度相对于共同边缘分布 $p_{\\theta}(x)$ 的期望，并将其简化为关于 $w$ 和 $\\sigma_{x}^{2}$ 的闭式解析表达式。\n\n用文字讨论您的构建对于潜变量 $z$ 在通路活性方向性和可识别性方面的生物学可解释性有何影响。作为您的最终答案，仅提供期望 KL 散度 $\\mathbb{E}_{p_{\\theta}(x)}\\left[ \\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big) \\right]$ 的闭式解析表达式。无需四舍五入，最终答案中不报告任何单位。",
            "solution": "问题陈述已经过验证，被认为是有效的。这是一个在统计机器学习及其在计算系统生物学中应用的适定、有科学依据的问题。它是自洽、一致的，没有任何逻辑或事实上的缺陷。现在将提供一个完整的解决方案。\n\n该问题要求构建和分析两个不同的生成模型，它们产生相同的可观测数据分布，但在其潜表示上有所不同。这探讨了潜变量模型中的一个根本性的不可识别问题。我们将一步步地进行所需的推导。\n\n**$1$. 边缘分布 $p_{1}(x)$ 和 $p_{2}(x)$ 的推导**\n\n观测值 $x$ 的边缘分布是通过对联合分布 $p(x, z) = p(x \\mid z)p(z)$ 关于潜变量 $z$ 进行积分得到的。对于这两个模型，这对应于两个高斯分布的卷积，其结果也是一个高斯分布。\n\n对于一个一般的层级模型，其中 $z \\sim \\mathcal{N}(\\mu_z, \\sigma_z^2)$ 且 $x \\mid z \\sim \\mathcal{N}(az+b, \\sigma_x^2)$， $x$ 的边缘分布也是高斯的，即 $x \\sim \\mathcal{N}(\\mu_x, \\sigma_{total}^2)$，其均值和方差由以下公式给出：\n$\\mu_x = \\mathbb{E}[x] = \\mathbb{E}[\\mathbb{E}[x \\mid z]] = \\mathbb{E}[az+b] = a\\mu_z + b$\n$\\sigma_{total}^2 = \\mathrm{Var}(x) = \\mathbb{E}[\\mathrm{Var}(x \\mid z)] + \\mathrm{Var}(\\mathbb{E}[x \\mid z]) = \\mathbb{E}[\\sigma_x^2] + \\mathrm{Var}(az+b) = \\sigma_x^2 + a^2\\sigma_z^2$\n\n对于模型 $1$：\n先验是 $p_{1}(z) = \\mathcal{N}(z \\mid 0, 1)$，所以 $\\mu_z = 0$ 且 $\\sigma_z^2 = 1$。\n解码器是 $p_{1}(x \\mid z) = \\mathcal{N}(x \\mid wz + \\mu, \\sigma_{x}^{2})$，所以 $a=w$ 且 $b=\\mu$。\n\n$x$ 的边缘均值是：\n$$\\mathbb{E}_{p_1}[x] = w(0) + \\mu = \\mu$$\n$x$ 的边缘方差是：\n$$\\mathrm{Var}_{p_1}(x) = \\sigma_{x}^{2} + w^2(1) = w^2 + \\sigma_{x}^{2}$$\n因此，模型 $1$ 的边缘分布是：\n$$p_{1}(x) = \\mathcal{N}(x \\mid \\mu, w^2 + \\sigma_{x}^{2})$$\n\n对于模型 $2$：\n先验是 $p_{2}(z') = \\mathcal{N}(z' \\mid 0, 1)$，所以 $\\mu_{z'} = 0$ 且 $\\sigma_{z'}^2 = 1$。\n解码器是 $p_{2}(x \\mid z') = \\mathcal{N}(x \\mid (-w)z' + \\mu, \\sigma_{x}^{2})$，所以 $a=-w$ 且 $b=\\mu$。\n\n$x$ 的边缘均值是：\n$$\\mathbb{E}_{p_2}[x] = (-w)(0) + \\mu = \\mu$$\n$x$ 的边缘方差是：\n$$\\mathrm{Var}_{p_2}(x) = \\sigma_{x}^{2} + (-w)^2(1) = w^2 + \\sigma_{x}^{2}$$\n因此，模型 $2$ 的边缘分布是：\n$$p_{2}(x) = \\mathcal{N}(x \\mid \\mu, w^2 + \\sigma_{x}^{2})$$\n\n比较这两个边缘分布，我们发现 $p_{1}(x) = p_{2}(x)$。两个模型都从相同的分布生成数据，我们将其表示为 $p_{\\theta}(x) = \\mathcal{N}(\\mu, w^2 + \\sigma_{x}^{2})$。这证实了仅凭观测值 $x$ 无法区分这两个模型。\n\n**$2$. 后验均值和方差的推导**\n\n对于一个具有先验 $p(z) = \\mathcal{N}(z \\mid \\mu_z, \\Sigma_z)$ 和似然 $p(x \\mid z) = \\mathcal{N}(x \\mid Az+b, \\Sigma_x)$ 的线性高斯模型，其后验 $p(z \\mid x)$ 也是高斯分布，即 $p(z \\mid x) = \\mathcal{N}(z \\mid \\mu_{z \\mid x}, \\Sigma_{z \\mid x})$，参数为：\n$$\\Sigma_{z \\mid x} = (\\Sigma_z^{-1} + A^T \\Sigma_x^{-1} A)^{-1}$$\n$$\\mu_{z \\mid x} = \\Sigma_{z \\mid x} (A^T \\Sigma_x^{-1}(x-b) + \\Sigma_z^{-1}\\mu_z)$$\n在我们的一维情况下，矩阵变为标量。\n\n对于模型 $1$：\n$p_{1}(z) = \\mathcal{N}(z \\mid 0, 1) \\implies \\mu_z = 0, \\Sigma_z = 1$。\n$p_{1}(x \\mid z) = \\mathcal{N}(x \\mid wz + \\mu, \\sigma_x^2) \\implies A = w, b = \\mu, \\Sigma_x = \\sigma_x^2$。\n\n后验方差 $s^2$ 是：\n$$s^2 = (1^{-1} + w (\\sigma_x^2)^{-1} w)^{-1} = (1 + \\frac{w^2}{\\sigma_x^2})^{-1} = \\left(\\frac{\\sigma_x^2 + w^2}{\\sigma_x^2}\\right)^{-1} = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2}$$\n后验均值 $m_{1}(x)$ 是：\n$$m_{1}(x) = s^2 (w (\\sigma_x^2)^{-1} (x - \\mu) + 1^{-1} \\cdot 0) = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2} \\left( \\frac{w}{\\sigma_x^2} (x - \\mu) \\right) = \\frac{w(x - \\mu)}{w^2 + \\sigma_x^2}$$\n所以，$q_{1}(z \\mid x) = \\mathcal{N}(z \\mid m_{1}(x), s^2)$。\n\n对于模型 $2$：\n$p_{2}(z') = \\mathcal{N}(z' \\mid 0, 1) \\implies \\mu_{z'} = 0, \\Sigma_{z'} = 1$。\n$p_{2}(x \\mid z') = \\mathcal{N}(x \\mid (-w)z' + \\mu, \\sigma_x^2) \\implies A = -w, b = \\mu, \\Sigma_x = \\sigma_x^2$。\n\n后验方差是：\n$$(1^{-1} + (-w) (\\sigma_x^2)^{-1} (-w))^{-1} = (1 + \\frac{w^2}{\\sigma_x^2})^{-1} = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2}$$\n这与模型 $1$ 的后验方差相同，因此共享的后验方差确实是 $s^2 = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2}$。\n后验均值 $m_{2}(x)$ 是：\n$$m_{2}(x) = s^2 ((-w) (\\sigma_x^2)^{-1} (x - \\mu) + 1^{-1} \\cdot 0) = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2} \\left( \\frac{-w}{\\sigma_x^2} (x - \\mu) \\right) = \\frac{-w(x - \\mu)}{w^2 + \\sigma_x^2}$$\n所以，$q_{2}(z' \\mid x) = \\mathcal{N}(z' \\mid m_{2}(x), s^2)$。注意 $m_{2}(x) = -m_{1}(x)$。\n\n**$3$. Kullback-Leibler 散度的推导**\n\n两个一维高斯分布 $q_a = \\mathcal{N}(\\mu_a, \\sigma_a^2)$ 和 $q_b = \\mathcal{N}(\\mu_b, \\sigma_b^2)$ 之间的 KL 散度由以下公式给出：\n$$\\mathrm{KL}(q_a \\,\\|\\, q_b) = \\ln\\left(\\frac{\\sigma_b}{\\sigma_a}\\right) + \\frac{\\sigma_a^2 + (\\mu_a - \\mu_b)^2}{2\\sigma_b^2} - \\frac{1}{2}$$\n在我们的情况下，我们计算的是 $\\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big)$。这两个分布是 $q_{1} = \\mathcal{N}(m_{1}(x), s^2)$ 和 $q_{2} = \\mathcal{N}(m_{2}(x), s^2)$。\n由于方差相等，$\\sigma_a^2 = \\sigma_b^2 = s^2$，公式可以显著简化：\n$$\\mathrm{KL}(q_1 \\,\\|\\, q_2) = \\ln(1) + \\frac{s^2 + (m_{1}(x) - m_{2}(x))^2}{2s^2} - \\frac{1}{2} = \\frac{1}{2} + \\frac{(m_{1}(x) - m_{2}(x))^2}{2s^2} - \\frac{1}{2} = \\frac{(m_{1}(x) - m_{2}(x))^2}{2s^2}$$\n我们代入均值的表达式：\n$$m_{1}(x) - m_{2}(x) = \\frac{w(x - \\mu)}{w^2 + \\sigma_x^2} - \\left( \\frac{-w(x - \\mu)}{w^2 + \\sigma_x^2} \\right) = \\frac{2w(x - \\mu)}{w^2 + \\sigma_x^2}$$\n现在，我们将这个差值和方差 $s^2$ 代入 KL 散度表达式中：\n$$\\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big) = \\frac{1}{2s^2} \\left( \\frac{2w(x - \\mu)}{w^2 + \\sigma_x^2} \\right)^2 = \\frac{1}{2 \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2}} \\frac{4w^2(x - \\mu)^2}{(w^2 + \\sigma_x^2)^2}$$\n$$\\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big) = \\frac{w^2 + \\sigma_x^2}{2\\sigma_x^2} \\frac{4w^2(x - \\mu)^2}{(w^2 + \\sigma_x^2)^2} = \\frac{2w^2(x-\\mu)^2}{\\sigma_x^2(w^2 + \\sigma_x^2)}$$\n\n**$4$. 期望 KL 散度的推导**\n\n我们必须计算 KL 散度相对于共同边缘数据分布 $p_{\\theta}(x) = \\mathcal{N}(x \\mid \\mu, w^2 + \\sigma_{x}^{2})$ 的期望。\n$$\\mathbb{E}_{p_{\\theta}(x)}\\left[ \\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big) \\right] = \\mathbb{E}_{p_{\\theta}(x)}\\left[ \\frac{2w^2(x-\\mu)^2}{\\sigma_x^2(w^2 + \\sigma_x^2)} \\right]$$\n项 $\\frac{2w^2}{\\sigma_x^2(w^2 + \\sigma_x^2)}$ 是一个相对于 $x$ 的常数，所以我们可以将其从期望中提出来：\n$$= \\frac{2w^2}{\\sigma_x^2(w^2 + \\sigma_x^2)} \\mathbb{E}_{p_{\\theta}(x)}\\left[ (x-\\mu)^2 \\right]$$\n根据定义，项 $\\mathbb{E}_{p_{\\theta}(x)}\\left[ (x-\\mu)^2 \\right]$ 是随机变量 $x$ 在分布 $p_{\\theta}(x)$ 下的方差，因为 $\\mu$ 是 $x$ 的均值。我们已经确定了 $\\mathrm{Var}_{p_\\theta}(x) = w^2 + \\sigma_x^2$。\n将这个结果代回：\n$$\\mathbb{E}_{p_{\\theta}(x)}\\left[ \\mathrm{KL} \\right] = \\frac{2w^2}{\\sigma_x^2(w^2 + \\sigma_x^2)} (w^2 + \\sigma_x^2) = \\frac{2w^2}{\\sigma_x^2}$$\n\n**$5$. 对生物学可解释性影响的讨论**\n\n这个结果揭示了像 VAEs 这样的潜变量模型在用于系统生物学中的表示学习时存在一个关键的不可识别性问题。\n这两个模型，一个是由 $wz$ 驱动的基因激活，另一个是由 $(-w)z'$ 驱动，在观测上是等价的。它们产生完全相同的对数表达值 $x$ 的分布。这意味着无论由该过程生成多少数据，都无法区分一个高的潜变量值 $z$ 是对应于高基因表达（模型 $1$，假设 $w>0$）还是低基因表达（模型 $2$）。\n\n对于一个给定的观测值 $x$，这两个模型为潜状态提供了相反的解释：在一个模型中，推断出的平均活性是 $m_1(x)$，而在另一个模型中是 $m_2(x) = -m_1(x)$。如果一个生物学家要将 $z$ 解释为“通路活性”，一个模型会认为该通路被激活，而另一个模型则认为它被抑制，用以解释同一个数据点。潜变量对观测值影响的方向性仅从数据本身是无法确定的。\n\n这种模糊性，或称符号不确定性，是模型中对称性的直接后果：先验 $p(z)=\\mathcal{N}(0,1)$ 关于 $0$ 是对称的，并且将潜变量映射到观测均值的函数存在一个符号模糊性 $w \\leftrightarrow -w$。在这些数据上训练的任何 VAE 都可能收敛到这两种解中的任意一种（或其混合），这使得对学习到的潜维度符号的解释变得任意。\n\n最终结果 $\\mathbb{E}[\\mathrm{KL}] = \\frac{2w^2}{\\sigma_x^2}$ 量化了这两种同样有效的后验信念之间的平均差异性。这个值与信噪比的平方 $\\frac{w^2}{\\sigma_x^2}$ 成正比。当信号（$w$）相对于噪声（$\\sigma_x$）很强时，后验分布变得非常确定（方差 $s^2$ 很低），但其中心却在非常不同的均值上（$m_1(x)$ vs. $-m_1(x)$）。这导致了一个大的 KL 散度，表明这两个模型提供了尖锐冲突但同样合理的解释。解决这种模糊性需要打破模型的对称性，例如，通过融入先验的生物学知识，即已知某个特定通路是所研究基因的激活剂或抑制剂。",
            "answer": "$$\n\\boxed{\\frac{2w^{2}}{\\sigma_{x}^{2}}}\n$$"
        },
        {
            "introduction": "后验坍塌是 VAE 训练中一种常见的失败模式，即模型学会忽略输入数据，导致潜在表征失去信息量。本练习将让您亲手处理稀疏计数数据，通过数值优化寻找变分参数，并实现一套关键的诊断指标来检测后验坍塌的发生 ()。通过这个过程，您将深入理解 KL 散度惩罚项在模型优化中的作用，以及它如何可能导致表征质量的退化。",
            "id": "3358032",
            "problem": "给定一个简化的概率模型，该模型专为计算系统生物学中常见的稀疏计数数据（如单细胞基因表达计数）而设计。考虑一个带有权重因子的变分自编码器（VAE）变体，称为$\\beta$-变分自编码器（$\\beta$-VAE），其中标量参数$\\beta \\in \\mathbb{R}_{\\ge 0}$缩放证据下界（ELBO）中的Kullback–Leibler散度项。目标是分析调整$\\beta$如何影响ELBO优化，并设计诊断指标以检测稀疏计数数据下编码器的后验坍塌现象。\n\n模型和推导的基本基础：\n- 设观测计数 $x \\in \\mathbb{N}_0$ 由泊松分布生成，其率是潜变量 $z \\in \\mathbb{R}$ 的指数函数：$p(x \\mid z) = \\mathrm{Poisson}(\\lambda)$，其中 $\\lambda = \\exp(w z + b)$，$w \\in \\mathbb{R}$ 和 $b \\in \\mathbb{R}$ 是固定的解码器参数。泊松概率质量函数为 $p(x \\mid \\lambda) = \\exp(-\\lambda) \\lambda^{x} / x!$，适用于 $x \\in \\mathbb{N}_0$。\n- 潜先验是标准高斯分布：$p(z) = \\mathcal{N}(0, 1)$。\n- 编码器的变分族是具有对角协方差的高斯分布：$q(z \\mid x) = \\mathcal{N}(\\mu(x), \\sigma^2(x))$，其通过均值 $\\mu(x) \\in \\mathbb{R}$ 和标准差 $\\sigma(x) \\in \\mathbb{R}_{>0}$ 对每个观测值进行参数化。\n- 单个观测值的证据下界（ELBO）定义为重构项（在变分分布下的期望对数似然）与变分后验和先验之间的Kullback–Leibler散度（由$\\beta$缩放）之差：$\\mathrm{ELBO}(x;\\beta) = \\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] - \\beta \\, \\mathrm{KL}(q(z \\mid x) \\Vert p(z))$。两个单变量高斯分布之间的Kullback–Leibler散度为 $\\mathrm{KL}(\\mathcal{N}(\\mu,\\sigma^2) \\Vert \\mathcal{N}(0,1)) = \\frac{1}{2}\\left(\\mu^2 + \\sigma^2 - \\log \\sigma^2 - 1\\right)$。\n- 对于具有对数连接函数和高斯变分后验的泊松解码器，重构项在$q(z \\mid x)$下存在闭式期望，因为$\\log \\lambda = w z + b$是$z$的线性函数，而$\\lambda = \\exp(w z + b)$在$q(z \\mid x)$下是对数正态分布的。请仅使用这些经过充分测试的事实和定义作为您计算和推断的起点。\n\n您的程序必须针对下述每个测试用例，纯粹以数学和算法术语执行以下操作：\n1. 对于给定数据集中的每个观测值 $x_i$，使用给定的固定解码器参数 $(w,b)$ 和指定的 $\\beta$，关于属于 $q(z \\mid x_i) = \\mathcal{N}(\\mu_i, \\sigma_i^2)$ 的变分参数 $(\\mu_i, \\sigma_i)$ 最大化ELBO。使用数值优化来找到使观测值 $x_i$ 的ELBO最大化的 $(\\mu_i^\\star, \\sigma_i^\\star)$。独立处理每个观测值，并假设所有观测值的权重均等。\n2. 使用优化后的参数计算诊断指标以检测后验坍塌：\n   - 平均Kullback–Leibler散度 $\\overline{\\mathrm{KL}} = \\frac{1}{n}\\sum_{i=1}^{n} \\mathrm{KL}(\\mathcal{N}(\\mu_i^\\star,\\sigma_i^{\\star 2}) \\Vert \\mathcal{N}(0,1))$。\n   - 观测值$X$和潜变量$Z$在变分族下的互信息近似值，定义为 $I(X;Z) \\approx \\frac{1}{n} \\sum_{i=1}^{n} \\mathrm{KL}\\!\\left(\\mathcal{N}(\\mu_i^\\star,\\sigma_i^{\\star 2}) \\Vert \\mathcal{N}(\\bar{\\mu}, \\bar{v})\\right)$，其中聚合后验$q(z)$由一个高斯分布近似，其均值 $\\bar{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} \\mu_i^\\star$ 和方差 $\\bar{v} = \\frac{1}{n}\\sum_{i=1}^{n} \\sigma_i^{\\star 2} + \\mathrm{Var}(\\{\\mu_i^\\star\\}_{i=1}^{n})$ 是使用混合分布的全方差定律计算的。\n   - 数据集中编码器均值的方差 $\\mathrm{Var}(\\{\\mu_i^\\star\\}_{i=1}^{n})$。\n3. 当且仅当以下所有条件同时成立时，将测试用例的后验坍塌声明为布尔值结果：\n   - $\\overline{\\mathrm{KL}}  \\varepsilon_{\\mathrm{KL}}$,\n   - $I(X;Z)  \\varepsilon_{\\mathrm{MI}}$,\n   - $\\mathrm{Var}(\\{\\mu_i^\\star\\})  \\varepsilon_{\\mu}$,\n   其中阈值固定为 $\\varepsilon_{\\mathrm{KL}} = 0.05$，$\\varepsilon_{\\mathrm{MI}} = 0.01$ 和 $\\varepsilon_{\\mu} = 0.005$。这些诊断指标旨在捕捉一个状态，在该状态下，变分后验接近先验，潜变量携带的关于观测值的信息可忽略不计，并且编码器的输出在不同输入之间几乎不变。\n\n待计算检验的假设：随着$\\beta$的增加，Kullback–Leibler散度惩罚项在ELBO中逐渐占据主导地位，导致大多数$x_i$的$\\mu_i^\\star \\to 0$和$\\sigma_i^\\star \\to 1$（后验坍塌），特别是当解码器灵敏度$w$较弱或数据高度稀疏（许多零值）时。相反，较小的$\\beta$允许后验中有信息量的偏差以改善重构。\n\n测试套件：\n- 测试用例1（正常路径，信息丰富状态）：数据集计数 $[0,0,1,0,2,0,0,1,0]$，解码器参数 $(w,b) = (1.0,\\log(0.3))$，以及 $\\beta = 0.1$。\n- 测试用例2（高$\\beta$状态）：与测试用例1相同的数据集计数和解码器参数，但 $\\beta = 10.0$。\n- 测试用例3（边界情况：极端稀疏）：数据集计数 $[0,0,0,0,0,0,0,0,0,0,0,0]$，解码器参数 $(w,b) = (1.0,\\log(0.5))$，以及 $\\beta = 1.0$。\n- 测试用例4（边缘情况：解码器灵敏度弱）：数据集计数 $[0,1,3,0,2,1,0,4,0,1]$，解码器参数 $(w,b) = (0.05,0.0)$，以及 $\\beta = 0.5$。\n\n要求的最终输出格式：\n- 您的程序应生成一行输出，其中包含按上述顺序列出的四个测试用例的后验坍塌判定，格式为逗号分隔的布尔值Python列表，并用方括号括起，例如“[False,True,False,True]”。\n- 不应打印任何额外文本。\n\n本问题描述中的所有变量、函数、运算符和数字都是数学性的，必须以LaTeX格式编写。此处不适用角度和物理单位。请遵循指定的最终输出格式，将所有阈值和输出以布尔值、整数、浮点数或这些类型的列表形式进行数值表示。",
            "solution": "该问题是有效的，因为它在科学上基于变分推断和生成模型的理论，问题陈述清晰（well-posed），具有明确的目标和定义的参数，并且表达客观。任务是分析带有泊松解码器的$\\beta$-变分自编码器（$\\beta$-VAE）的行为，特别是在不同条件下检测后验坍塌。\n\n解决方案分为以下几个步骤：\n1.  推导单个观测值 $x$ 的证据下界（ELBO）目标函数。\n2.  定义数值优化程序，为每个观测值找到最优变分参数 $(\\mu^\\star, \\sigma^\\star)$。\n3.  定义用于检测后验坍塌的诊断指标。\n4.  实现一个算法来处理每个测试用例，执行优化，计算诊断指标，并应用坍塌标准。\n\n**1. 证据下界（ELBO）推导**\n\n单个观测值 $x$ 的ELBO由下式给出：\n$$\n\\mathrm{ELBO}(x;\\beta) = \\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] - \\beta \\, \\mathrm{KL}(q(z \\mid x) \\Vert p(z))\n$$\n变分后验为 $q(z \\mid x) = \\mathcal{N}(z; \\mu, \\sigma^2)$，先验为 $p(z) = \\mathcal{N}(z; 0, 1)$。ELBO的两个项分别进行分析。\n\n**Kullback-Leibler (KL) 散度项**\n变分后验与标准高斯先验之间的KL散度由以下公式给出：\n$$\n\\mathrm{KL}(q(z \\mid x) \\Vert p(z)) = \\mathrm{KL}(\\mathcal{N}(\\mu, \\sigma^2) \\Vert \\mathcal{N}(0,1)) = \\frac{1}{2}\\left(\\mu^2 + \\sigma^2 - \\log \\sigma^2 - 1\\right)\n$$\n利用属性 $\\log \\sigma^2 = 2\\log\\sigma$，可以写成：\n$$\n\\mathrm{KL}(q(z \\mid x) \\Vert p(z)) = \\frac{1}{2}\\left(\\mu^2 + \\sigma^2 - 2\\log\\sigma - 1\\right)\n$$\n\n**重构项**\n重构项是数据在变分后验下的期望对数似然。解码器为 $p(x \\mid z) = \\mathrm{Poisson}(\\lambda)$，其率为 $\\lambda = \\exp(w z + b)$。对数似然为：\n$$\n\\log p(x \\mid z) = \\log\\left( \\frac{e^{-\\lambda} \\lambda^x}{x!} \\right) = -\\lambda + x \\log\\lambda - \\log(x!)\n$$\n代入 $\\lambda = \\exp(w z + b)$：\n$$\n\\log p(x \\mid z) = -\\exp(w z + b) + x(w z + b) - \\log(x!)\n$$\n我们对 $z \\sim q(z \\mid x) = \\mathcal{N}(\\mu, \\sigma^2)$ 取期望：\n$$\n\\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] = \\mathbb{E}[-\\exp(w z + b)] + \\mathbb{E}[x(w z + b)] - \\mathbb{E}[\\log(x!)]\n$$\n利用期望的线性性质以及 $x$ 和 $\\log(x!)$ 相对于 $z$ 是常数：\n$$\n= -\\mathbb{E}[\\exp(w z + b)] + x(w \\mathbb{E}[z] + b) - \\log(x!)\n$$\n由于 $z \\sim \\mathcal{N}(\\mu, \\sigma^2)$，我们有 $\\mathbb{E}[z] = \\mu$。变量 $y = w z + b$ 也是高斯分布的，其均值为 $\\mathbb{E}[y] = w\\mu + b$，方差为 $\\mathrm{Var}(y) = w^2\\sigma^2$。期望 $\\mathbb{E}[\\exp(y)]$ 是高斯变量 $y$ 在 $t=1$ 处评估的矩生成函数，即 $\\exp(\\mathbb{E}[y] + \\frac{1}{2}\\mathrm{Var}(y))$。\n$$\n\\mathbb{E}[\\exp(w z + b)] = \\exp\\left(w\\mu + b + \\frac{1}{2}w^2\\sigma^2\\right)\n$$\n综上所述，重构项为：\n$$\n\\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] = x w \\mu + xb - \\log(x!) - \\exp\\left(w\\mu + b + \\frac{1}{2}w^2\\sigma^2\\right)\n$$\n\n**完整ELBO目标**\n合并各项，ELBO为：\n$$\n\\mathrm{ELBO} = \\left( x w \\mu + xb - \\log(x!) - \\exp\\left(w\\mu + b + \\frac{1}{2}w^2\\sigma^2\\right) \\right) - \\frac{\\beta}{2}\\left(\\mu^2 + \\sigma^2 - 2\\log\\sigma - 1\\right)\n$$\n\n**2. 优化程序**\n\n对于每个观测值 $x_i$，我们必须找到最大化ELBO的变分参数 $(\\mu_i^\\star, \\sigma_i^\\star)$。这是一个数值优化问题。相对于 $(\\mu, \\sigma)$ 为常数的项，即 $xb$、$-\\log(x!)$ 和 $-\\beta/2(-1)$，可以从目标函数中去掉而不改变最大值的位置。因此，需要最大化的函数为：\n$$\nL(\\mu, \\sigma) = x w \\mu - \\exp\\left(w\\mu + b + \\frac{1}{2}w^2\\sigma^2\\right) - \\frac{\\beta}{2}\\left(\\mu^2 + \\sigma^2 - 2\\log\\sigma\\right)\n$$\n优化在 $\\mu \\in \\mathbb{R}$ 和 $\\sigma \\in \\mathbb{R}_{0}$ 上进行。为了处理约束 $\\sigma  0$，我们使用重参数化 $\\sigma = \\exp(\\rho)$，其中 $\\rho \\in \\mathbb{R}$。目标函数变成一个关于 $(\\mu, \\rho)$ 的无约束问题：\n$$\nL(\\mu, \\rho) = x w \\mu - \\exp\\left(w\\mu + b + \\frac{1}{2}w^2\\exp(2\\rho)\\right) - \\frac{\\beta}{2}\\left(\\mu^2 + \\exp(2\\rho) - 2\\rho\\right)\n$$\n我们将使用像BFGS这样的数值优化算法来最小化 $-L(\\mu, \\rho)$，从初始猜测 $(\\mu, \\rho) = (0, 0)$ 开始，这对应于 $(\\mu, \\sigma) = (0, 1)$（即先验）。\n\n**3. 后验坍塌的诊断指标**\n\n在为数据集（大小为 $n$）中的每个观测值 $x_i$ 获得最优参数 $(\\mu_i^\\star, \\sigma_i^\\star)$ 后，我们计算以下诊断指标：\n\n- **平均KL散度**：该指标衡量学习到的后验与先验的平均距离。一个较小的值表明后验已经“坍塌”到先验。\n  $$\n  \\overline{\\mathrm{KL}} = \\frac{1}{n}\\sum_{i=1}^{n} \\mathrm{KL}(\\mathcal{N}(\\mu_i^\\star, \\sigma_i^{\\star 2}) \\Vert \\mathcal{N}(0,1)) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mu_i^{\\star 2} + \\sigma_i^{\\star 2} - 2\\log\\sigma_i^\\star - 1\\right)\n  $$\n\n- **互信息 $I(X;Z)$ 的近似**：该指标衡量潜变量$Z$包含多少关于观测值$X$的信息。一个较低的值表明表示是无信息量的。它通过计算从每个后验 $q(z \\mid x_i)$ 到聚合后验 $q_{agg}(z) = \\mathcal{N}(\\bar{\\mu}, \\bar{v})$ 的KL散度的平均值来近似。聚合后验的参数为：\n  $$\n  \\bar{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} \\mu_i^\\star \\quad \\text{和} \\quad \\bar{v} = \\frac{1}{n}\\sum_{i=1}^{n} \\sigma_i^{\\star 2} + \\mathrm{Var}(\\{\\mu_i^\\star\\}_{i=1}^{n})\n  $$\n  两个一般单变量高斯分布 $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ 和 $\\mathcal{N}(\\mu_2, \\sigma_2^2)$ 之间的KL散度为：\n  $$\n  \\mathrm{KL}(\\mathcal{N}_1 \\Vert \\mathcal{N}_2) = \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n  $$\n  因此，互信息的近似值为：\n  $$\n  I(X;Z) \\approx \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\log\\frac{\\sqrt{\\bar{v}}}{\\sigma_i^\\star} + \\frac{\\sigma_i^{\\star 2} + (\\mu_i^\\star - \\bar{\\mu})^2}{2\\bar{v}} - \\frac{1}{2} \\right)\n  $$\n\n- **编码器均值的方差**：该指标衡量编码器的输出均值在整个数据集上的变化程度。一个非常低的方差表明编码器正在忽略输入。\n  $$\n  \\mathrm{Var}(\\{\\mu_i^\\star\\}_{i=1}^{n})\n  $$\n\n**4. 后验坍塌条件**\n\n如果以下三个条件全部满足，则声明发生后验坍塌，使用给定的阈值 $\\varepsilon_{\\mathrm{KL}} = 0.05$，$\\varepsilon_{\\mathrm{MI}} = 0.01$ 和 $\\varepsilon_{\\mu} = 0.005$：\n1.  $\\overline{\\mathrm{KL}}  \\varepsilon_{\\mathrm{KL}}$\n2.  $I(X;Z)  \\varepsilon_{\\mathrm{MI}}$\n3.  $\\mathrm{Var}(\\{\\mu_i^\\star\\})  \\varepsilon_{\\mu}$\n\n最终答案中实现的算法遵循这些步骤为每个测试用例生成所需的布尔值输出。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the Variational Autoencoder analysis problem for the given test cases.\n    \"\"\"\n    \n    # Define thresholds for posterior collapse detection.\n    EPS_KL = 0.05\n    EPS_MI = 0.01\n    EPS_MU = 0.005\n\n    def analyze_case(counts, w, b, beta):\n        \"\"\"\n        Processes a single test case: performs optimization for each data point,\n        computes diagnostics, and determines if posterior collapse has occurred.\n        \n        Args:\n            counts (np.ndarray): The dataset of observed counts.\n            w (float): The decoder weight parameter.\n            b (float): The decoder bias parameter.\n            beta (float): The beta parameter for the beta-VAE.\n            \n        Returns:\n            bool: True if posterior collapse is detected, False otherwise.\n        \"\"\"\n        \n        def objective_factory(x_i):\n            \"\"\"\n            Factory to create the objective function to be minimized (-ELBO) for a\n            given data point x_i.\n            \"\"\"\n            def neg_elbo(params):\n                \"\"\"\n                The negative of the ELBO, excluding constant terms, as a function\n                of the variational parameters mu and log_sigma. We minimize this.\n                \"\"\"\n                mu, log_sigma = params\n                sigma = np.exp(log_sigma)\n                sigma_sq = sigma**2\n                \n                # The ELBO to be maximized is:\n                # E[log p(x|z)] - beta * KL(q||p)\n                # E[log p(x|z)] (variable part) = x*w*mu - exp(w*mu + b + 0.5*w^2*sigma^2)\n                # KL(q||p) (full) = 0.5 * (mu^2 + sigma^2 - 2*log(sigma) - 1)\n                \n                recon_term = w * x_i * mu - np.exp(w * mu + b + 0.5 * w**2 * sigma_sq)\n                kl_div = 0.5 * (mu**2 + sigma_sq - 2 * log_sigma - 1.0)\n                \n                # We minimize the negative ELBO\n                return -(recon_term - beta * kl_div)\n            \n            return neg_elbo\n\n        def kld_general_gaussians(mu1, sigma1, mu2, sigma2_val):\n            \"\"\"\n            Calculates KL(N(mu1, sigma1^2) || N(mu2, sigma2_val)).\n            Note: sigma1 is std dev, sigma2_val is variance.\n            \"\"\"\n            sigma1_sq = sigma1**2\n            sigma2 = np.sqrt(sigma2_val)\n            # Formula: log(sigma2/sigma1) + (sigma1^2 + (mu1-mu2)^2) / (2*sigma2^2) - 0.5\n            return np.log(sigma2 / sigma1) + (sigma1_sq + (mu1 - mu2)**2) / (2 * sigma2_val) - 0.5\n\n        mu_stars = []\n        sigma_stars = []\n        n = len(counts)\n\n        # Optimize an ELBO for each data point to find optimal variational parameters\n        for x in counts:\n            obj_func = objective_factory(x)\n            initial_guess = np.array([0.0, 0.0])  # mu=0, log_sigma=0 (sigma=1)\n            result = minimize(obj_func, initial_guess, method='BFGS')\n            mu_star, log_sigma_star = result.x\n            sigma_star = np.exp(log_sigma_star)\n            \n            mu_stars.append(mu_star)\n            sigma_stars.append(sigma_star)\n\n        mu_stars = np.array(mu_stars)\n        sigma_stars = np.array(sigma_stars)\n        sigma_sq_stars = sigma_stars**2\n        \n        # Diagnostic 1: Average KL divergence from posterior to prior\n        kl_divs = 0.5 * (mu_stars**2 + sigma_sq_stars - 2 * np.log(sigma_stars) - 1.0)\n        avg_kl = np.mean(kl_divs)\n\n        # Diagnostic 2: Variance of encoder means\n        mu_var = np.var(mu_stars) # Population variance (ddof=0)\n\n        # Diagnostic 3: Mutual information approximation\n        agg_posterior_mean = np.mean(mu_stars)\n        agg_posterior_var = np.mean(sigma_sq_stars) + mu_var\n        \n        kls_to_agg = kld_general_gaussians(mu_stars, sigma_stars, agg_posterior_mean, agg_posterior_var)\n        mi_approx = np.mean(kls_to_agg)\n        \n        # Check for posterior collapse\n        is_collapsed = (avg_kl  EPS_KL) and (mi_approx  EPS_MI) and (mu_var  EPS_MU)\n        \n        return is_collapsed\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (happy path, informative regime)\n        {\"counts\": np.array([0, 0, 1, 0, 2, 0, 0, 1, 0]), \"w\": 1.0, \"b\": np.log(0.3), \"beta\": 0.1},\n        # Test case 2 (high-beta regime)\n        {\"counts\": np.array([0, 0, 1, 0, 2, 0, 0, 1, 0]), \"w\": 1.0, \"b\": np.log(0.3), \"beta\": 10.0},\n        # Test case 3 (boundary case: extreme sparsity)\n        {\"counts\": np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), \"w\": 1.0, \"b\": np.log(0.5), \"beta\": 1.0},\n        # Test case 4 (edge case: weak decoder sensitivity)\n        {\"counts\": np.array([0, 1, 3, 0, 2, 1, 0, 4, 0, 1]), \"w\": 0.05, \"b\": 0.0, \"beta\": 0.5},\n    ]\n\n    results = []\n    for case in test_cases:\n        collapse_result = analyze_case(case[\"counts\"], case[\"w\"], case[\"b\"], case[\"beta\"])\n        results.append(collapse_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在理解了后验坍塌的整体现象之后，一个更深入的问题是，这种坍塌是否均匀地影响所有潜在维度。本练习将引入一种更精细的诊断工具，通过计算逐维度的 KL 散度来识别未被充分利用的潜在维度 ()。此外，它还引导您设计一种针对性的训练干预策略——维度权重 $\\beta$-VAE，旨在“拯救”坍塌的维度，从而提升表征的整体质量。",
            "id": "3357966",
            "problem": "您正在计算系统生物学领域中研究一种用于单细胞转录组表示学习的变分自编码器 (VAE)。该潜变量模型使用一个分解的近似后验和一个各向同性标准正态先验。具体来说，对于每个输入 $x$，编码器输出一个对角高斯近似后验 $q_{\\phi}(z \\mid x)$ 的参数，其中 $q_{\\phi}(z \\mid x) = \\mathcal{N}\\!\\left(\\mu(x), \\mathrm{diag}\\left(\\sigma^{2}(x)\\right)\\right)$，$z \\in \\mathbb{R}^{d}$ 的坐标为 $z_{j}$ 且 $q_{\\phi}(z_{j} \\mid x) = \\mathcal{N}\\!\\left(\\mu_{j}(x), \\sigma_{j}^{2}(x)\\right)$，先验为 $p(z) = \\mathcal{N}(0, I)$。这里 $\\phi$ 表示编码器参数，$\\mu(x) \\in \\mathbb{R}^{d}$，且 $\\sigma^{2}(x) \\in \\mathbb{R}_{+}^{d}$。\n\n您的目标是构建一种有原则的、逐维度的 Kullback–Leibler (KL) 诊断方法，以检测后验坍塌，并提出一种有针对性的训练干预措施。您将通过推导和计算逐维度的 KL 散度 $\\mathrm{KL}\\!\\left(q_{\\phi}(z_{j} \\mid x) \\,\\|\\, p(z_{j})\\right)$，在数据集上对其进行聚合，然后为 KL 项提出一种逐维度加权方案以对抗坍塌。\n\n用作基础的基本定义：\n- 对于一个 VAE，数据点 $x$ 的证据下界 (ELBO) 由 $\\mathcal{L}(x) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right)$ 给出。\n- 连续分布 $q$ 和 $p$ 之间的 Kullback–Leibler 散度为 $\\mathrm{KL}(q \\,\\|\\, p) = \\int q(u)\\,\\log\\frac{q(u)}{p(u)}\\,du$。\n- 对于独立坐标，$\\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right)$ 可加性分解为 $\\sum_{j=1}^{d} \\mathrm{KL}\\!\\left(q_{\\phi}(z_{j} \\mid x) \\,\\|\\, p(z_{j})\\right)$。\n\n任务：\n1. 从第一性原理出发，推导单变量高斯 KL 散度 $\\mathrm{KL}\\!\\left(\\mathcal{N}(\\mu_{j}, \\sigma_{j}^{2}) \\,\\|\\, \\mathcal{N}(0, 1)\\right)$ 关于 $\\mu_{j}$ 和 $\\sigma_{j}^{2}$ 的闭式表达式，然后将其特化到编码器输出 $\\mu_{j}(x)$ 和 $\\log \\sigma_{j}^{2}(x)$。\n2. 给定一个数据集 $\\mathcal{D} = \\{x_{i}\\}_{i=1}^{n}$ 以及每个 $x_{i}$ 对应的编码器输出 $\\mu(x_{i})$ 和 $\\log \\sigma^{2}(x_{i})$，计算逐维度的、数据集平均的 KL 值 $\\overline{\\mathrm{KL}}_{j} = \\frac{1}{n}\\sum_{i=1}^{n} \\mathrm{KL}\\!\\left(q_{\\phi}(z_{j} \\mid x_{i}) \\,\\|\\, p(z_{j})\\right)$。\n3. 如果 $\\overline{\\mathrm{KL}}_{j}  \\varepsilon$，则定义维度 $j$ 为坍塌维度，其中 $\\varepsilon$ 是用户指定的非负阈值（单位为奈特）。检测并报告坍塌的维度。\n4. 通过为 $\\beta$-VAE 风格的目标函数 $\\mathcal{L}_{\\text{weighted}}(x) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - \\sum_{j=1}^{d} \\beta_{j}\\,\\mathrm{KL}\\!\\left(q_{\\phi}(z_{j} \\mid x) \\,\\|\\, p(z_{j})\\right)$ 设计逐维度权重 $\\beta_{j}$ 来提出一种有针对性的训练干预措施，该措施旨在增加坍塌维度的信息使用量，同时不破坏训练的稳定性。使用以下规则根据诊断结果设置 $\\beta_{j}$：\n   - 选择一个逐维度的目标信息水平 $\\tau$（单位为奈特）。设置 $\\beta_{j} = \\mathrm{clip}\\!\\left(\\frac{\\overline{\\mathrm{KL}}_{j}}{\\tau}, \\beta_{\\min}, 1\\right)$，其中如果 $u  a$，$\\mathrm{clip}(u, a, b)$ 返回 $a$；如果 $u > b$，返回 $b$；否则返回 $u$。$\\beta_{\\min} \\in (0, 1]$ 是一个下限，以避免权重为零。这会减少满足 $\\overline{\\mathrm{KL}}_{j}  \\tau$ 的维度的 KL 惩罚（较小的 $\\beta_{j}$），从而鼓励它们携带更多信息，同时保持 $\\beta_{j} \\le 1$ 以避免过度惩罚信息丰富的维度。\n\n您的程序必须为以下测试套件实现上述步骤。每个测试用例提供了编码器输出，即在 $n$ 个样本和 $d$ 个潜维度上的 $\\mu$ 和 $\\log \\sigma^{2}$ 值的矩阵，以及阈值 $(\\varepsilon, \\tau, \\beta_{\\min})$。\n\n测试套件：\n- 用例 1：\n  - $n = 4$, $d = 3$。\n  - $\\mu = \\begin{bmatrix}\n  0.0  0.5  1.0 \\\\\n  0.0  0.4  1.2 \\\\\n  0.0  0.6  0.9 \\\\\n  0.0  0.45  1.1\n  \\end{bmatrix}$，\n    $\\log \\sigma^{2} = \\begin{bmatrix}\n  0.0  -0.2  0.3 \\\\\n  0.0  -0.2  0.3 \\\\\n  0.0  -0.2  0.3 \\\\\n  0.0  -0.2  0.3\n  \\end{bmatrix}$。\n  - $\\varepsilon = 0.05$，$\\tau = 0.2$，$\\beta_{\\min} = 0.1$。\n- 用例 2 (边界条件)：\n  - $n = 2$, $d = 2$。\n  - $\\mu = \\begin{bmatrix}\n  0.4472136  0.0 \\\\\n  0.4472136  0.0\n  \\end{bmatrix}$，\n    $\\log \\sigma^{2} = \\begin{bmatrix}\n  0.0  0.0 \\\\\n  0.0  0.0\n  \\end{bmatrix}$。\n  - $\\varepsilon = 0.1$，$\\tau = 0.15$，$\\beta_{\\min} = 0.1$。\n- 用例 3 (异方差极端情况)：\n  - $n = 3$, $d = 4$。\n  - $\\mu = \\begin{bmatrix}\n  0.0  0.3  0.2  0.0 \\\\\n  0.0  0.2  0.25  0.0 \\\\\n  0.0  0.4  0.15  0.0\n  \\end{bmatrix}$，\n    $\\log \\sigma^{2} = \\begin{bmatrix}\n  0.0  -0.1  -3.0  1.5 \\\\\n  0.0  -0.1  -3.0  1.5 \\\\\n  0.0  -0.1  -3.0  1.5\n  \\end{bmatrix}$。\n  - $\\varepsilon = 0.02$，$\\tau = 0.5$，$\\beta_{\\min} = 0.2$。\n- 用例 4 (所有维度均为信息维度)：\n  - $n = 3$, $d = 2$。\n  - $\\mu = \\begin{bmatrix}\n  0.4  0.7 \\\\\n  0.35  0.6 \\\\\n  0.45  0.65\n  \\end{bmatrix}$，\n    $\\log \\sigma^{2} = \\begin{bmatrix}\n  -0.2  -0.1 \\\\\n  -0.2  -0.1 \\\\\n  -0.2  -0.1\n  \\end{bmatrix}$。\n  - $\\varepsilon = 0.05$，$\\tau = 0.2$，$\\beta_{\\min} = 0.1$。\n\n每个用例所需的计算和输出：\n- 使用您推导的闭式表达式计算逐样本、逐维度的 KL 值。\n- 计算数据集平均的逐维度 KL 值 $\\overline{\\mathrm{KL}}_{j}$。\n- 使用严格不等式确定坍塌维度的索引集合 $\\{j : \\overline{\\mathrm{KL}}_{j}  \\varepsilon\\}$。\n- 使用 $\\beta_{j} = \\mathrm{clip}\\!\\left(\\frac{\\overline{\\mathrm{KL}}_{j}}{\\tau}, \\beta_{\\min}, 1\\right)$ 计算推荐的 $\\beta_{j}$。\n- 计算数据集上的平均总 KL，定义为 $\\overline{\\mathrm{KL}}_{\\text{total}} = \\sum_{j=1}^{d} \\overline{\\mathrm{KL}}_{j}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素对应一个测试用例，并且本身也是一个包含以下元素的列表：\n  - 坍塌维度的整数计数，\n  - 四舍五入到 $6$ 位小数的浮点数 $\\overline{\\mathrm{KL}}_{\\text{total}}$，\n  - 坍塌维度索引的列表（整数），\n  - 按维度顺序列出的、四舍五入到 $4$ 位小数的推荐 $\\beta_{j}$ 值列表。\n- 例如，单个测试用例可能会产生类似 $[1,0.123456,[0],[0.5,1.0,0.8]]$ 的输出片段。最终输出必须是包含所有四个测试用例的列表，例如 $[[\\ldots],[\\ldots],[\\ldots],[\\ldots]]$。",
            "solution": "该问题要求推导并应用一种针对变分自编码器 (VAE) 的逐维度 Kullback–Leibler (KL) 散度诊断方法，以检测和对抗后验坍塌。VAE 模型被指定为具有一个分解的对角高斯近似后验 $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\mu(x), \\mathrm{diag}(\\sigma^{2}(x)))$ 和一个标准正态先验 $p(z) = \\mathcal{N}(0, I)$。\n\n解决方案主要分为两个部分。首先，我们推导单个潜维度下单变量后验与先验之间 KL 散度的闭式表达式。其次，我们建立算法，用于根据给定的编码器输出数据集计算所需的诊断指标和干预权重。\n\n**1. 逐维度 KL 散度的推导**\n\n主要任务是推导单个潜维度 $j$ 的近似后验 $q_{\\phi}(z_j \\mid x) = \\mathcal{N}(z_j; \\mu_j(x), \\sigma_j^2(x))$ 与先验 $p(z_j) = \\mathcal{N}(z_j; 0, 1)$ 之间 KL 散度的闭式表达式。为简化符号，我们将省略对 $x$ 和 $\\phi$ 的依赖，并将分布表示为 $q(z_j) = \\mathcal{N}(\\mu_j, \\sigma_j^2)$ 和 $p(z_j) = \\mathcal{N}(0, 1)$。\n\nKL 散度的定义为：\n$$\n\\mathrm{KL}(q(z_j) \\,\\|\\, p(z_j)) = \\int_{-\\infty}^{\\infty} q(z_j) \\log \\frac{q(z_j)}{p(z_j)} dz_j\n$$\n这可以展开为两个涉及关于 $q(z_j)$ 的期望的项：\n$$\n\\mathrm{KL}(q(z_j) \\,\\|\\, p(z_j)) = \\mathbb{E}_{q(z_j)}[\\log q(z_j)] - \\mathbb{E}_{q(z_j)}[\\log p(z_j)]\n$$\n这是 $q(z_j)$ 的微分熵的负数（记为 $-H(q(z_j))$）减去 $q(z_j)$ 和 $p(z_j)$ 之间的交叉熵。\n\n$q(z_j)$ 和 $p(z_j)$ 的概率密度函数 (PDF) 分别为：\n$$\nq(z_j) = \\frac{1}{\\sqrt{2\\pi\\sigma_j^2}} \\exp\\left(-\\frac{(z_j - \\mu_j)^2}{2\\sigma_j^2}\\right)\n$$\n$$\np(z_j) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z_j^2}{2}\\right)\n$$\n它们的对数形式为：\n$$\n\\log q(z_j) = -\\frac{(z_j - \\mu_j)^2}{2\\sigma_j^2} - \\frac{1}{2}\\log(2\\pi\\sigma_j^2)\n$$\n$$\n\\log p(z_j) = -\\frac{z_j^2}{2} - \\frac{1}{2}\\log(2\\pi)\n$$\n\n现在我们计算这两个期望项。\n\n第一项 $\\mathbb{E}_{q(z_j)}[\\log q(z_j)]$ 与高斯分布的熵有关。\n$$\n\\mathbb{E}_{q(z_j)}[\\log q(z_j)] = \\mathbb{E}_{q(z_j)}\\left[-\\frac{(z_j - \\mu_j)^2}{2\\sigma_j^2} - \\frac{1}{2}\\log(2\\pi\\sigma_j^2)\\right]\n$$\n$$\n= -\\frac{1}{2\\sigma_j^2} \\mathbb{E}_{q(z_j)}[(z_j - \\mu_j)^2] - \\frac{1}{2}\\log(2\\pi\\sigma_j^2)\n$$\n根据定义，$\\mathbb{E}_{q(z_j)}[(z_j - \\mu_j)^2]$ 是 $q(z_j)$ 的方差，即 $\\sigma_j^2$。\n$$\n\\mathbb{E}_{q(z_j)}[\\log q(z_j)] = -\\frac{\\sigma_j^2}{2\\sigma_j^2} - \\frac{1}{2}\\log(2\\pi\\sigma_j^2) = -\\frac{1}{2} - \\frac{1}{2}\\log(2\\pi\\sigma_j^2)\n$$\n\n第二项是 $\\log p(z_j)$ 的期望：\n$$\n\\mathbb{E}_{q(z_j)}[\\log p(z_j)] = \\mathbb{E}_{q(z_j)}\\left[-\\frac{z_j^2}{2} - \\frac{1}{2}\\log(2\\pi)\\right]\n$$\n$$\n= -\\frac{1}{2} \\mathbb{E}_{q(z_j)}[z_j^2] - \\frac{1}{2}\\log(2\\pi)\n$$\n对于一个随机变量 $Z_j \\sim \\mathcal{N}(\\mu_j, \\sigma_j^2)$，我们有 $\\mathrm{Var}(Z_j) = \\mathbb{E}[Z_j^2] - (\\mathbb{E}[Z_j])^2$。因此，$\\mathbb{E}[Z_j^2] = \\mathrm{Var}(Z_j) + (\\mathbb{E}[Z_j])^2 = \\sigma_j^2 + \\mu_j^2$。\n$$\n\\mathbb{E}_{q(z_j)}[\\log p(z_j)] = -\\frac{1}{2}(\\sigma_j^2 + \\mu_j^2) - \\frac{1}{2}\\log(2\\pi)\n$$\n\n合并各项以求得 KL 散度：\n$$\n\\mathrm{KL}(q(z_j) \\,\\|\\, p(z_j)) = \\left(-\\frac{1}{2} - \\frac{1}{2}\\log(2\\pi\\sigma_j^2)\\right) - \\left(-\\frac{1}{2}(\\sigma_j^2 + \\mu_j^2) - \\frac{1}{2}\\log(2\\pi)\\right)\n$$\n$$\n= -\\frac{1}{2} - \\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\sigma_j^2) + \\frac{1}{2}\\sigma_j^2 + \\frac{1}{2}\\mu_j^2 + \\frac{1}{2}\\log(2\\pi)\n$$\n$$\n= \\frac{1}{2} \\left( \\mu_j^2 + \\sigma_j^2 - \\log(\\sigma_j^2) - 1 \\right)\n$$\n这就是单变量高斯分布 $\\mathcal{N}(\\mu_j, \\sigma_j^2)$ 与标准正态分布 $\\mathcal{N}(0, 1)$ 之间 KL 散度的闭式表达式。\n\n问题指明 VAE 编码器输出 $\\mu_j(x)$ 和 $\\log \\sigma_j^2(x)$。我们记 $v_j(x) = \\log \\sigma_j^2(x)$，这意味着 $\\sigma_j^2(x) = \\exp(v_j(x))$。将这些代入我们推导出的公式，得到用于计算的最终表达式：\n$$\n\\mathrm{KL}(q_{\\phi}(z_j \\mid x) \\,\\|\\, p(z_j)) = \\frac{1}{2}\\left( \\mu_j(x)^2 + \\exp(v_j(x)) - v_j(x) - 1 \\right)\n$$\n\n**2. 用于诊断和干预的算法流程**\n\n给定一个数据集 $\\mathcal{D} = \\{x_i\\}_{i=1}^{n}$ 以及每个数据点对应的编码器输出 $(\\mu(x_i), \\log \\sigma^2(x_i))$，我们对每个测试用例执行以下计算序列。\n\n1.  **计算逐样本、逐维度的 KL 散度**：对于每个样本 $x_i$ 和每个潜维度 $j \\in \\{1, \\dots, d\\}$，我们使用推导出的公式计算 KL 散度，记为 $\\mathrm{KL}_{ij}$：\n    $$\n    \\mathrm{KL}_{ij} = \\frac{1}{2}\\left( \\mu_j(x_i)^2 + \\exp(\\log \\sigma_j^2(x_i)) - \\log \\sigma_j^2(x_i) - 1 \\right)\n    $$\n2.  **计算数据集平均的逐维度 KL 散度**：对于每个维度 $j$，我们将数据集中所有 $n$ 个样本的 KL 值进行平均：\n    $$\n    \\overline{\\mathrm{KL}}_{j} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathrm{KL}_{ij}\n    $$\n    这个值 $\\overline{\\mathrm{KL}}_{j}$ 量化了维度 $j$ 相对于先验被强制编码的平均信息量（单位为奈特）。\n3.  **检测坍塌维度**：如果一个维度 $j$ 的平均 KL 散度接近于零，则该维度被识别为“坍塌”。这意味着后验 $q_{\\phi}(z_j \\mid x)$ 与先验 $p(z_j)$ 几乎无法区分，因此该维度未被用于编码关于数据的信息。我们使用给定的阈值 $\\varepsilon$ 来形式化这一点：如果 $\\overline{\\mathrm{KL}}_{j}  \\varepsilon$，则维度 $j$ 坍塌。我们将报告所有这些维度的数量和索引。索引是基于 0 的。\n4.  **提出针对性干预措施**：为了对抗坍塌，提出了一个带有维度特定权重 $\\beta_j$ 的改进版 $\\beta$-VAE 目标函数。这些权重旨在减少对坍塌或接近坍塌维度的 KL 惩罚，从而鼓励模型使用它们。规则如下：\n    $$\n    \\beta_j = \\mathrm{clip}\\left(\\frac{\\overline{\\mathrm{KL}}_{j}}{\\tau}, \\beta_{\\min}, 1\\right)\n    $$\n    其中 $\\tau$ 是一个目标信息水平，$\\beta_{\\min}$ 是一个最小权重，$\\mathrm{clip}(u, a, b)$ 将值 $u$ 限制在范围 $[a, b]$ 内。这将对所有维度 $j \\in \\{1, \\dots, d\\}$ 进行计算。\n5.  **计算平均总 KL 散度**：在数据集上平均的总 KL 惩罚是逐维度平均 KL 的总和：\n    $$\n    \\overline{\\mathrm{KL}}_{\\text{total}} = \\sum_{j=1}^{d} \\overline{\\mathrm{KL}}_{j}\n    $$\n这些步骤在提供的程序中实现，用于分析测试用例。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the VAE posterior collapse diagnostic problem for a given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"mu\": np.array([\n                [0.0, 0.5, 1.0],\n                [0.0, 0.4, 1.2],\n                [0.0, 0.6, 0.9],\n                [0.0, 0.45, 1.1]\n            ]),\n            \"log_var\": np.array([\n                [0.0, -0.2, 0.3],\n                [0.0, -0.2, 0.3],\n                [0.0, -0.2, 0.3],\n                [0.0, -0.2, 0.3]\n            ]),\n            \"params\": (0.05, 0.2, 0.1)  # (epsilon, tau, beta_min)\n        },\n        {\n            \"mu\": np.array([\n                [0.4472136, 0.0],\n                [0.4472136, 0.0]\n            ]),\n            \"log_var\": np.array([\n                [0.0, 0.0],\n                [0.0, 0.0]\n            ]),\n            \"params\": (0.1, 0.15, 0.1)\n        },\n        {\n            \"mu\": np.array([\n                [0.0, 0.3, 0.2, 0.0],\n                [0.0, 0.2, 0.25, 0.0],\n                [0.0, 0.4, 0.15, 0.0]\n            ]),\n            \"log_var\": np.array([\n                [0.0, -0.1, -3.0, 1.5],\n                [0.0, -0.1, -3.0, 1.5],\n                [0.0, -0.1, -3.0, 1.5]\n            ]),\n            \"params\": (0.02, 0.5, 0.2)\n        },\n        {\n            \"mu\": np.array([\n                [0.4, 0.7],\n                [0.35, 0.6],\n                [0.45, 0.65]\n            ]),\n            \"log_var\": np.array([\n                [-0.2, -0.1],\n                [-0.2, -0.1],\n                [-0.2, -0.1]\n            ]),\n            \"params\": (0.05, 0.2, 0.1)\n        }\n    ]\n\n    all_results_str = []\n\n    for case in test_cases:\n        mu_matrix = case[\"mu\"]\n        log_var_matrix = case[\"log_var\"]\n        epsilon, tau, beta_min = case[\"params\"]\n\n        # 1. Compute per-sample, per-dimension KL divergence\n        # KL = 0.5 * (mu^2 + exp(log_var) - log_var - 1)\n        var_matrix = np.exp(log_var_matrix)\n        kl_matrix = 0.5 * (np.square(mu_matrix) + var_matrix - log_var_matrix - 1)\n\n        # 2. Compute dataset-averaged per-dimension KL divergence\n        avg_kl_per_dim = np.mean(kl_matrix, axis=0)\n\n        # 3. Detect collapsed dimensions\n        collapsed_indices = np.where(avg_kl_per_dim  epsilon)[0].tolist()\n        num_collapsed = len(collapsed_indices)\n\n        # 4. Propose targeted intervention\n        # beta_j = clip(avg_kl_j / tau, beta_min, 1)\n        beta_ratios = avg_kl_per_dim / tau\n        beta_values = np.clip(beta_ratios, beta_min, 1.0).tolist()\n        \n        # 5. Compute average total KL divergence\n        total_avg_kl = np.sum(avg_kl_per_dim)\n\n        # Format the output string for the current case\n        s_c = str(num_collapsed)\n        s_t = f\"{total_avg_kl:.6f}\"\n        s_i = f\"[{','.join(map(str, collapsed_indices))}]\"\n        s_b = f\"[{','.join([f'{b:.4f}' for b in beta_values])}]\"\n        \n        case_str = f\"[{s_c},{s_t},{s_i},{s_b}]\"\n        all_results_str.append(case_str)\n        \n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        }
    ]
}