{
    "hands_on_practices": [
        {
            "introduction": "支持向量机（SVM）的核心思想根植于一个优雅的几何概念：寻找一个能以最大“间隔”将不同类别的数据点分开的决策边界。这个“最大间隔”原则是构建鲁棒分类器的基础。这项基础练习将引导您在一个理想化的、线性可分的数据集上手动计算这个最大间隔超平面的参数，从而帮助您深入理解SVM优化问题的本质。通过解决这个经典问题 ，您将掌握将几何直觉转化为数学公式的关键技能。",
            "id": "3353372",
            "problem": "一个计算系统生物学小组正在训练一个线性支持向量机 (SVM) 分类器，该分类器根据两个转录组学特征来区分两种细胞命运：转录因子 $X_{1}$ 和 $X_{2}$ 相对于基线的对数转换表达偏差。他们收集了 $6$ 个标记样本：正标记类别 ($+1$) 的特征向量为 $(2,2)$、$(2,0)$ 和 $(0,2)$，负标记类别 ($-1$) 的特征向量为 $(-2,-2)$、$(-2,0)$ 和 $(0,-2)$。假设数据是严格线性可分的，并且分类器是 $\\mathbb{R}^{2}$ 中的硬间隔最大间隔线性SVM。\n\n仅使用硬间隔线性SVM的核心定义（在单位函数间隔下正确分类的约束条件下，最小化权重向量的欧几里得范数的平方）以及关于欧几里得范数和间隔的标准几何事实，确定唯一最大间隔分离超平面参数 $(w,b)$，其中 $w \\in \\mathbb{R}^{2}$ 且 $b \\in \\mathbb{R}$，以及几何间隔宽度 $\\gamma=2/\\|w\\|$。将您的最终结果报告为单个行向量 $(w_{1},w_{2},b,\\gamma)$，其中 $\\gamma=2/\\|w\\|$。\n\n提供精确值，不要四舍五入。",
            "solution": "该问题被评估为有效。这是一个在机器学习领域，具体涉及支持向量机 (SVM) 的适定、自洽且具有科学依据的问题。\n\n### 第一步：提取已知条件\n- **任务**：确定硬间隔线性SVM的参数 $(w, b)$ 和几何间隔宽度 $\\gamma$。\n- **特征空间**：$\\mathbb{R}^{2}$，特征为 $X_1$ 和 $X_2$。\n- **数据点**：$N=6$ 个标记样本 $(x_i, y_i)$ 的集合。\n- **正类别 ($y_i = +1$)**：\n  - $x_1 = (2, 2)$\n  - $x_2 = (2, 0)$\n  - $x_3 = (0, 2)$\n- **负类别 ($y_i = -1$)**：\n  - $x_4 = (-2, -2)$\n  - $x_5 = (-2, 0)$\n  - $x_6 = (0, -2)$\n- **模型**：硬间隔最大间隔线性SVM。分离超平面由 $w \\cdot x + b = 0$ 定义，其中 $w = (w_1, w_2) \\in \\mathbb{R}^2$ 且 $b \\in \\mathbb{R}$。\n- **输出格式**：将结果报告为单个行向量 $(w_1, w_2, b, \\gamma)$，其中 $\\gamma = 2/\\|w\\|$。\n\n### 第二步：使用提取的已知条件进行验证\n该问题有效。\n- **科学依据**：该问题是应用硬间隔SVM的一个典型例子，这是机器学习和计算生物学中的一个基本概念。所有原理和定义都是标准的。\n- **适定性**：数据已指定，通过目视检查，它们是线性可分的。对于线性可分数据集，硬间隔SVM的优化问题是一个凸优化问题，这保证了最大间隔超平面存在唯一解。\n- **客观性**：该问题使用精确的数学语言陈述，没有歧义或主观因素。\n- **完整性**：提供了所有必要的数据点及其标签。\n\n### 第三步：求解推导\n硬间隔线性SVM旨在找到一个分离超平面 $w \\cdot x + b = 0$，以最大化几何间隔，这等同于最小化权重向量的欧几里得范数的平方 $\\|w\\|^2$。该优化问题可表述为：\n$$\n\\begin{aligned}\n \\underset{w, b}{\\text{minimize}}\n  \\frac{1}{2} \\|w\\|^2 \\\\\n \\text{subject to}\n  y_i(w \\cdot x_i + b) \\ge 1, \\quad \\text{for } i = 1, \\dots, 6\n\\end{aligned}\n$$\n这些约束确保所有数据点都被正确分类，并且函数间隔至少为 $1$。\n\n设 $w = (w_1, w_2)$。这 $6$ 个约束条件是：\n对于 $y_i = +1$：\n1. $1 \\cdot (w_1(2) + w_2(2) + b) \\ge 1 \\implies 2w_1 + 2w_2 + b \\ge 1$\n2. $1 \\cdot (w_1(2) + w_2(0) + b) \\ge 1 \\implies 2w_1 + b \\ge 1$\n3. $1 \\cdot (w_1(0) + w_2(2) + b) \\ge 1 \\implies 2w_2 + b \\ge 1$\n\n对于 $y_i = -1$：\n4. $-1 \\cdot (w_1(-2) + w_2(-2) + b) \\ge 1 \\implies 2w_1 + 2w_2 - b \\ge 1$\n5. $-1 \\cdot (w_1(-2) + w_2(0) + b) \\ge 1 \\implies 2w_1 - b \\ge 1$\n6. $-1 \\cdot (w_1(0) + w_2(-2) + b) \\ge 1 \\implies 2w_2 - b \\ge 1$\n\n我们可以利用数据的对称性来简化问题。数据集关于交换坐标 $X_1$ 和 $X_2$ 是对称的。这表明最优权重向量 $w$ 的分量应该相等，即 $w_1 = w_2$。\n数据集也关于原点对称：对于类别 $y_i$ 中的每个点 $x_i$，点 $-x_i$ 都在类别 $-y_i$ 中。这表明分离超平面应该通过原点，这意味着偏置项 $b=0$。\n\n让我们通过设置 $w_1 = w_2 = w_c$ 和 $b=0$ 来检验这个假设。优化问题变为在简化的约束条件下最小化 $\\|w\\|^2 = w_c^2 + w_c^2 = 2w_c^2$。\n将 $w_1 = w_2 = w_c$ 和 $b=0$ 代入约束条件：\n1. $2w_c + 2w_c \\ge 1 \\implies 4w_c \\ge 1$\n2. $2w_c \\ge 1$\n3. $2w_c \\ge 1$\n4. $2w_c + 2w_c \\ge 1 \\implies 4w_c \\ge 1$\n5. $2w_c \\ge 1$\n6. $2w_c \\ge 1$\n\n所有约束都简化为单一条件 $2w_c \\ge 1$，即 $w_c \\ge \\frac{1}{2}$。约束 $4w_c \\ge 1$ (即 $w_c \\ge \\frac{1}{4}$) 是冗余的。\n我们必须在 $w_c \\ge \\frac{1}{2}$ 的约束下最小化目标函数 $2w_c^2$。由于当 $w_c  0$ 时目标函数是单调递增的，最小值在可行域的边界处取得，即当 $w_c$ 取其可能的最小值时。\n因此，最优值为 $w_c = \\frac{1}{2}$。\n\n这给出了权重向量和偏置的唯一解：\n$w_1 = \\frac{1}{2}$\n$w_2 = \\frac{1}{2}$\n$b = 0$\n\n权重向量为 $w = (\\frac{1}{2}, \\frac{1}{2})$。分离超平面为 $\\frac{1}{2}X_1 + \\frac{1}{2}X_2 = 0$，或 $X_1 + X_2 = 0$。\n\n让我们根据原始约束来验证这个解。\n当 $w=(\\frac{1}{2}, \\frac{1}{2})$ 且 $b=0$ 时：\n1. 对于 $x_1=(2,2)$: $y_1(w \\cdot x_1 + b) = 1(\\frac{1}{2}(2) + \\frac{1}{2}(2) + 0) = 1(1+1) = 2 \\ge 1$。(正确)\n2. 对于 $x_2=(2,0)$: $y_2(w \\cdot x_2 + b) = 1(\\frac{1}{2}(2) + \\frac{1}{2}(0) + 0) = 1(1) = 1 \\ge 1$。(正确，位于间隔边界上)\n3. 对于 $x_3=(0,2)$: $y_3(w \\cdot x_3 + b) = 1(\\frac{1}{2}(0) + \\frac{1}{2}(2) + 0) = 1(1) = 1 \\ge 1$。(正确，位于间隔边界上)\n4. 对于 $x_4=(-2,-2)$: $y_4(w \\cdot x_4 + b) = -1(\\frac{1}{2}(-2) + \\frac{1}{2}(-2) + 0) = -1(-1-1) = 2 \\ge 1$。(正确)\n5. 对于 $x_5=(-2,0)$: $y_5(w \\cdot x_5 + b) = -1(\\frac{1}{2}(-2) + \\frac{1}{2}(0) + 0) = -1(-1) = 1 \\ge 1$。(正确，位于间隔边界上)\n6. 对于 $x_6=(0,-2)$: $y_6(w \\cdot x_6 + b) = -1(\\frac{1}{2}(0) + \\frac{1}{2}(-2) + 0) = -1(-1) = 1 \\ge 1$。(正确，位于间隔边界上)\n\n等式 $y_i(w \\cdot x_i + b) = 1$ 成立的点是支持向量：$(2,0), (0,2), (-2,0), (0,-2)$。该解是一致的。\n\n最后一步是计算几何间隔宽度 $\\gamma = \\frac{2}{\\|w\\|}$。\n首先，计算 $w$ 的欧几里得范数：\n$$ \\|w\\| = \\sqrt{w_1^2 + w_2^2} = \\sqrt{(\\frac{1}{2})^2 + (\\frac{1}{2})^2} = \\sqrt{\\frac{1}{4} + \\frac{1}{4}} = \\sqrt{\\frac{2}{4}} = \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2} $$\n现在，计算间隔宽度 $\\gamma$：\n$$ \\gamma = \\frac{2}{\\|w\\|} = \\frac{2}{\\frac{\\sqrt{2}}{2}} = \\frac{4}{\\sqrt{2}} = \\frac{4\\sqrt{2}}{2} = 2\\sqrt{2} $$\n\n所需的参数是 $w_1 = \\frac{1}{2}$，$w_2 = \\frac{1}{2}$，$b=0$ 和 $\\gamma = 2\\sqrt{2}$。最终结果是行向量 $(w_1, w_2, b, \\gamma)$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2}  \\frac{1}{2}  0  2\\sqrt{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在真实的生物学研究中，数据往往是嘈杂且不完全线性可分的。为了应对这一挑战，我们需要从理想化的硬间隔SVM模型过渡到更具实用性的软间隔SVM。软间隔方法通过引入松弛变量 $\\xi_i$ 和正则化参数 $C$，允许分类器在一定程度上容忍错分样本，以换取更宽的间隔和更好的泛化能力。这项编程实践  要求您实现一个软间隔SVM分类器，并探索不同 $C$ 值对决策边界和松弛变量的影响，从而深刻理解模型在“最大化间隔”与“最小化误差”之间的权衡。",
            "id": "3353409",
            "problem": "您的任务是构建一个完整、可运行的程序，用于确定通过训练支持向量机（SVM）在一个双基因生物分类问题中获得的最优线性分类器。背景是计算系统生物学，其中使用基因表达特征将样本分为阳性表型和阴性表型。特征是双基因表达向量 $x = (G_1, G_2)$，其中 $G_1$ 和 $G_2$ 代表归一化的基因表达测量值，由于高通量分析中典型的中心化和缩放，这些值可能为正也可能为负。\n\n您必须实现的推导的基础是带间隔损失的正则化经验风险最小化框架、线性分类中的间隔定义以及通过松弛变量惩罚错分类的思想。必须从这些原则出发推导出线性决策边界，而不是依赖题目中给出的简化公式。\n\n您将获得两类样本：阳性表型和阴性表型。所有测试用例的数据点都是固定的，并按以下顺序排列：\n- 标记为 $+1$ 的阳性表型样本：$x_1 = (2, 2)$，$x_2 = (1, 0)$，$x_3 = (2, 1)$。\n- 标记为 $-1$ 的阴性表型样本：$x_4 = (-2, -2)$，$x_5 = (-1, 0.5)$，$x_6 = (-2, -1)$。\n\n样本顺序严格为 $\\{x_1, x_2, x_3, x_4, x_5, x_6\\}$，在报告松弛变量时必须保持不变。分类规则必须是形式为 $f(x) = w_1 G_1 + w_2 G_2 + b$ 的线性决策函数，并且松弛变量定义为 $\\{\\xi_i\\}_{i=1}^6$ 以量化对间隔的违反程度。\n\n您的程序必须针对以下每个测试用例，计算最优分离超平面参数 $(w_1, w_2, b)$ 和最优松弛变量 $\\xi_i$，这些参数是在具有指定正则化参数 $C$ 的线性软间隔支持向量机下得到的。底层优化问题必须足够精确地求解，以揭示决策边界和松弛变量。所有数值输出必须四舍五入到 $6$ 位小数。\n\n测试套件：\n- 测试用例 1：$C = 1$。\n- 测试用例 2：$C = 0.25$。\n- 测试用例 3：$C = 5$。\n\n对于每个测试用例，按顺序计算并报告以下内容：\n- 首先是决策边界系数 $w_1$、$w_2$ 和 $b$（实数）。\n- 然后是对应于有序样本 $\\{x_1, x_2, x_3, x_4, x_5, x_6\\}$ 的六个松弛变量 $\\xi_1, \\xi_2, \\xi_3, \\xi_4, \\xi_5, \\xi_6$。\n\n所有报告的量都必须是四舍五入到 $6$ 位小数的实数。不涉及物理单位、角度单位或百分比。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按所列顺序汇总三个测试用例的输出。具体来说，最终输出应该是一个包含 $27$ 个数字的扁平列表：\n$[w_1^{(1)}, w_2^{(1)}, b^{(1)}, \\xi_1^{(1)}, \\ldots, \\xi_6^{(1)}, w_1^{(2)}, w_2^{(2)}, b^{(2)}, \\xi_1^{(2)}, \\ldots, \\xi_6^{(2)}, w_1^{(3)}, w_2^{(3)}, b^{(3)}, \\xi_1^{(3)}, \\ldots, \\xi_6^{(3)}]$，\n其中上标 $(k)$ 表示测试用例 $k$。\n\n在整个过程中必须保持科学真实性和合理性，并且算法应从上述基本原理推导得出，而不是使用本问题陈述中提供的简化公式。所有数学实体，包括所有数字，在您的推理中必须以 LaTeX 格式书写。",
            "solution": "我们从二元分类的正则化经验风险最小化原则开始，寻求一个线性决策函数 $f(x) = w_1 G_1 + w_2 G_2 + b$ 来分离两个类别。对于一个带标签的样本 $(x_i, y_i)$，其中 $y_i \\in \\{+1, -1\\}$，其间隔定义为 $y_i f(x_i) = y_i (w^\\top x_i + b)$，其中 $w = (w_1, w_2)$ 且 $x_i = (G_{1,i}, G_{2,i})$。间隔的概念鼓励正确分类的样本获得较大的正值，并惩罚那些未达到足够间隔的样本。\n\n为了对不可分数据具有鲁棒性，引入了松弛变量 $\\xi_i \\geq 0$ 以允许对间隔的违反。经典的线性软间隔支持向量机（SVM）是通过在间隔最大化与铰链损失（hinge loss）惩罚之间进行权衡而推导出来的。样本 $i$ 的铰链损失为 $\\max(0, 1 - y_i (w^\\top x_i + b))$，总的正则化经验风险将此损失与对 $w$ 的二次正则化相结合，以控制模型容量。\n\n线性软间隔SVM的原始优化问题是\n$$\n\\min_{w, b, \\xi} \\quad \\frac{1}{2} \\| w \\|_2^2 + C \\sum_{i=1}^n \\xi_i\n$$\n服从约束\n$$\ny_i (w^\\top x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad i = 1, \\ldots, n,\n$$\n其中 $C  0$ 是正则化参数，用于控制间隔宽度和铰链损失惩罚之间的权衡，$n$ 是训练样本的数量。在我们的设定中，$n = 6$，样本顺序固定为 $\\{x_1, x_2, x_3, x_4, x_5, x_6\\}$，标签为 $y_1 = y_2 = y_3 = +1$ 和 $y_4 = y_5 = y_6 = -1$。\n\n为了高效、精确地计算最优解，我们使用拉格朗日乘子对不等式约束推导其对偶问题。为 $y_i (w^\\top x_i + b) \\geq 1 - \\xi_i$ 引入乘子 $\\alpha_i \\geq 0$，为 $\\xi_i \\geq 0$ 引入乘子 $\\mu_i \\geq 0$，Karush-Kuhn-Tucker（KKT）条件导出变量 $\\alpha_i$ 约束在 $0 \\leq \\alpha_i \\leq C$ 的对偶形式。通过驻点条件消去 $w$ 和 $b$，对偶目标函数变为：\n$$\n\\max_{\\alpha \\in \\mathbb{R}^n} \\quad \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\langle x_i, x_j \\rangle\n$$\n服从等式约束\n$$\n\\sum_{i=1}^n \\alpha_i y_i = 0\n$$\n以及箱形约束\n$$\n0 \\leq \\alpha_i \\leq C, \\quad i = 1, \\ldots, n.\n$$\n\n定义格拉姆矩阵（Gram matrix）$K \\in \\mathbb{R}^{n \\times n}$，其元素为 $K_{ij} = \\langle x_i, x_j \\rangle$，以及 $Q_{ij} = y_i y_j K_{ij}$，对偶目标函数可以紧凑地写为：\n$$\n\\max_{\\alpha} \\quad \\mathbf{1}^\\top \\alpha - \\frac{1}{2} \\alpha^\\top Q \\alpha\n$$\n服从约束\n$$\ny^\\top \\alpha = 0, \\quad 0 \\leq \\alpha \\leq C \\mathbf{1}.\n$$\n我们等价地可以最小化\n$$\n\\min_{\\alpha} \\quad \\frac{1}{2} \\alpha^\\top Q \\alpha - \\mathbf{1}^\\top \\alpha\n$$\n在相同的约束下。\n\n一旦找到最优的 $\\alpha^\\star$，原始参数可以通过以下方式恢复：\n$$\nw^\\star = \\sum_{i=1}^n \\alpha_i^\\star y_i x_i,\n$$\n偏置项 $b^\\star$ 根据KKT条件计算。对于任何满足 $0  \\alpha_i^\\star  C$ 的样本（一个“自由”支持向量），KKT互补松弛条件意味着：\n$$\ny_i (w^{\\star\\top} x_i + b^\\star) = 1,\n$$\n因此对于这类样本，\n$$\nb^\\star = y_i - w^{\\star\\top} x_i.\n$$\n如果存在多个自由支持向量，对它们所隐含的 $b^\\star$ 值取平均在数值上是鲁棒的。如果没有自由支持向量（所有 $\\alpha_i^\\star$ 都在边界 $0$ 或 $C$ 上），可以从所有样本的间隔约束中推导出 $b^\\star$ 的一个一致区间：\n$$\nb^\\star \\in \\bigcap_{i: y_i = +1} (-\\infty, 1 - w^{\\star\\top} x_i] \\ \\cap \\ \\bigcap_{i: y_i = -1} [-1 - w^{\\star\\top} x_i, \\infty).\n$$\n选择区间 $[b_{\\text{low}}, b_{\\text{high}}]$ 的中点，其中\n$$\nb_{\\text{low}} = \\max_{i: y_i = -1} \\left(-1 - w^{\\star\\top} x_i \\right), \\quad b_{\\text{high}} = \\min_{i: y_i = +1} \\left(1 - w^{\\star\\top} x_i \\right),\n$$\n即使在退化情况下也能确保数值稳定性。\n\n最后，松弛变量可以直接根据铰链损失的定义计算得出：\n$$\n\\xi_i^\\star = \\max \\left( 0, 1 - y_i \\left( w^{\\star\\top} x_i + b^\\star \\right) \\right), \\quad i = 1, \\ldots, n.\n$$\n\n算法规划：\n- 为固定的样本顺序构建 $X \\in \\mathbb{R}^{6 \\times 2}$ 和 $y \\in \\mathbb{R}^6$。\n- 对每个给定 $C$ 的测试用例：\n  - 构建格拉姆矩阵 $K$ 和矩阵 $Q$，其中 $Q_{ij} = y_i y_j K_{ij}$。\n  - 使用带有解析梯度 $\\nabla f(\\alpha) = Q \\alpha - \\mathbf{1}$ 和等式约束雅可比矩阵 $\\nabla (y^\\top \\alpha) = y$ 的约束优化器，在 $y^\\top \\alpha = 0$ 和 $0 \\leq \\alpha \\leq C \\mathbf{1}$ 的约束下，最小化对偶目标函数 $\\frac{1}{2} \\alpha^\\top Q \\alpha - \\mathbf{1}^\\top \\alpha$。\n  - 通过 $w^\\star = \\sum_i \\alpha_i^\\star y_i x_i$ 从 $\\alpha^\\star$ 恢复 $w^\\star$。\n  - 如果存在自由支持向量，则使用它们计算 $b^\\star$；否则使用上述的中点区间法。\n  - 按照固定的样本顺序计算 $\\xi_i^\\star = \\max(0, 1 - y_i (w^{\\star\\top} x_i + b^\\star))$。\n- 将所有报告值四舍五入到 $6$ 位小数。\n- 按 $(w_1, w_2, b, \\xi_1, \\xi_2, \\xi_3, \\xi_4, \\xi_5, \\xi_6)$ 汇总每个测试用例的输出，并跨三个测试用例连接结果。\n- 打印最终的单行、方括号括起的逗号分隔列表。\n\n此过程基于间隔、铰链损失、正则化和凸优化对偶理论的定义，确保了科学的严谨性和数学的一致性。它遵守问题描述中说明的约束和预期输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef svm_linear_soft_margin(X, y, C):\n    \"\"\"\n    Train a linear soft-margin SVM using the dual problem with a linear kernel.\n    Returns w (2,), b (scalar), xi (n,), alpha (n,).\n    \"\"\"\n    n, d = X.shape\n    # Gram matrix K_ij = x_i . x_j\n    K = X @ X.T\n    # Q_ij = y_i y_j K_ij\n    Q = (y[:, None] * y[None, :]) * K\n\n    # Objective: 0.5 * alpha^T Q alpha - 1^T alpha\n    def objective(alpha):\n        return 0.5 * alpha.dot(Q).dot(alpha) - np.sum(alpha)\n\n    # Gradient: Q alpha - 1\n    def grad(alpha):\n        return Q.dot(alpha) - np.ones(n)\n\n    # Equality constraint: y^T alpha = 0\n    constraints = {\n        'type': 'eq',\n        'fun': lambda a: float(np.dot(y, a)),\n        'jac': lambda a: y.astype(float)\n    }\n\n    # Bounds: 0 = alpha_i = C\n    bounds = [(0.0, float(C))] * n\n\n    # Initial guess: small values satisfying equality constraint\n    alpha0 = np.zeros(n, dtype=float)\n    pos_idx = np.where(y  0)[0]\n    neg_idx = np.where(y  0)[0]\n    if len(pos_idx)  0 and len(neg_idx)  0:\n        a0 = min(0.1, C / 10.0) if C > 0 else 0.1\n        alpha0[pos_idx[0]] = a0\n        alpha0[neg_idx[0]] = a0\n\n    res = minimize(\n        objective, alpha0, jac=grad, bounds=bounds, constraints=constraints,\n        method='SLSQP', options={'maxiter': 1000, 'ftol': 1e-9, 'disp': False}\n    )\n    alpha = res.x\n\n    # Compute w = sum_i alpha_i y_i x_i\n    w = (alpha * y) @ X  # shape (2,)\n\n    # Compute b from KKT:\n    tol = 1e-6\n    free_sv = np.where((alpha  tol)  (alpha  C - tol))[0]\n    if free_sv.size  0:\n        b_vals = y[free_sv] - X[free_sv].dot(w)\n        b = float(np.mean(b_vals))\n    else:\n        # Interval method over all samples\n        b_low = -np.inf\n        b_high = np.inf\n        margins_no_b = X.dot(w)\n        for i in range(n):\n            if y[i] == 1: # y_i(w.x_i + b) >= 1 -> w.x_i + b >= 1 -> b >= 1 - w.x_i\n                if alpha[i]  C - tol: # i is not a bounded SV violating margin\n                    b_high = min(b_high, 1 - margins_no_b[i])\n            else: # y_i == -1, y_i(w.x_i + b) >= 1 -> -(w.x_i + b) >= 1 -> w.x_i + b = -1 -> b = -1 - w.x_i\n                if alpha[i]  C - tol:\n                    b_low = max(b_low, -1 - margins_no_b[i])\n\n        if not np.isfinite(b_low) and np.isfinite(b_high):\n            b_low = b_high - 1.0\n        elif not np.isfinite(b_high) and np.isfinite(b_low):\n            b_high = b_low + 1.0\n        elif not np.isfinite(b_low) and not np.isfinite(b_high):\n             # this case is unlikely but let's be safe\n             pos_margins = margins_no_b[y == 1]\n             neg_margins = margins_no_b[y == -1]\n             b = -0.5 * (np.median(pos_margins) + np.median(neg_margins))\n        else:\n             b = 0.5 * (b_low + b_high)\n\n    # Slack variables: xi_i = max(0, 1 - y_i (w^T x_i + b))\n    margins = y * (X.dot(w) + b)\n    xi = np.maximum(0.0, 1.0 - margins)\n\n    return w, b, xi, alpha\n\ndef solve():\n    # Fixed dataset in specified order:\n    # Positive: (2,2), (1,0), (2,1); Negative: (-2,-2), (-1,0.5), (-2,-1)\n    X = np.array([\n        [2.0, 2.0],\n        [1.0, 0.0],\n        [2.0, 1.0],\n        [-2.0, -2.0],\n        [-1.0, 0.5],\n        [-2.0, -1.0],\n    ], dtype=float)\n    y = np.array([1, 1, 1, -1, -1, -1], dtype=float)\n\n    # Test cases: C values\n    test_cases = [1.0, 0.25, 5.0]\n\n    results = []\n    for C in test_cases:\n        w, b, xi, _ = svm_linear_soft_margin(X, y, C)\n        # Order: w1, w2, b, xi1..xi6\n        case_values = [w[0], w[1], b] + list(xi)\n        # Round to 6 decimals for output stability\n        case_values = [float(f\"{v:.6f}\") for v in case_values]\n        results.extend(case_values)\n\n    # Final print statement in the exact required format.\n    print(\"[\" + \",\".join(f\"{v:.6f}\" for v in results) + \"]\")\n\nsolve()\n```"
        },
        {
            "introduction": "将机器学习应用于真实的生物学问题，尤其是在整合来自多个实验批次（cohorts）的组学数据时，我们面临的挑战远不止分类本身。系统性的技术性偏差，即“批次效应”，常常会掩盖或扭曲真实的生物学信号，导致模型失效。这项高级实践  模拟了一个完整的计算系统生物学分析流程，您需要设计并实现一个严谨的交叉验证流程，在该流程内部正确地执行批次效应校正。这个练习旨在强调数据预处理与模型验证相结合的重要性，特别是如何防止“数据泄露”，从而获得对模型泛化性能的无偏估计，这是构建可靠生物医学预测模型的关键。",
            "id": "3353416",
            "problem": "给定一个计算系统生物学中的场景，其中多队列组学测量数据用于使用支持向量机 (SVM) 进行二元生物学分类。队列对应于实验室批次，每个批次都可能在测量的特征上留下系统性的位置和尺度偏差（批次效应）。您的任务是形式化批次效应如何产生，从参数化经验贝叶斯 (EB) 的角度推导一个位置-尺度批次校正程序，并实现一个严格的训练流程，该流程严格在训练折内执行批次校正，以防止在 SVM 拟合前发生数据泄露。然后，在多种测试条件下，对合成的多队列组学数据评估此流程。\n\n基本基础和生成模型：\n- 组学特征是受生物学条件和技术效应影响的分子丰度的测量值。设 $p$ 表示特征数量，$n$ 表示样本数量。每个样本 $i$ 属于一个批次 $B_i \\in \\{1,\\dots, B\\}$，并有一个二元类别标签 $Y_i \\in \\{-1, +1\\}$。\n- 对于特征 $g \\in \\{1,\\dots, p\\}$ 在样本 $i$ 中的一个简化且有科学依据的测量模型是\n$$\nX_{g,i} \\;=\\; \\mu_g \\;+\\; s_g Y_i \\;+\\; \\delta_{g, B_i} \\;+\\; \\gamma_{g, B_i} \\,\\varepsilon_{g,i},\n$$\n其中 $\\mu_g$ 是特征 $g$ 的基线丰度，$s_g$ 是特征 $g$ 的生物信号载荷，$\\delta_{g,b}$ 是特征 $g$ 在批次 $b$ 中的批次特异性位置（加性）效应，$\\gamma_{g,b}$ 是批次特异性尺度（乘性）效应，$\\varepsilon_{g,i}$ 是均值为零、方差有限的独立噪声。该模型捕捉了批次效应如何产生：$\\delta_{g,b}$ 和 $\\gamma_{g,b}$ 系统地改变了跨批次的特征分布，如果不进行校正，可能会混淆下游分类器。\n\n批次校正原理：\n- 参数化经验贝叶斯 ComBat 风格的校正通过以下步骤移除批次效应：标准化特征，在训练数据内部估计批次特异性的位置和尺度参数，并使用这些从训练中得出的参数来调整训练和测试特征。对于每个特征 $g$，定义全局训练均值和标准差：\n$$\n\\hat{\\mu}_g \\;=\\; \\frac{1}{n_{\\text{train}}} \\sum_{i \\in \\text{train}} X_{g,i}, \\qquad\n\\hat{\\sigma}_g \\;=\\; \\sqrt{ \\frac{1}{n_{\\text{train}}-1} \\sum_{i \\in \\text{train}} (X_{g,i} - \\hat{\\mu}_g)^2 }.\n$$\n通过 $Z_{g,i} = (X_{g,i} - \\hat{\\mu}_g)/\\hat{\\sigma}_g$ 对训练集和测试集进行标准化。对于训练集中存在的每个批次 $b$，估计特征 $g$ 的批次级标准化位置和尺度：\n$$\n\\hat{m}_{g,b} \\;=\\; \\frac{1}{n_{b,\\text{train}}} \\sum_{i \\in \\text{train}, B_i=b} Z_{g,i}, \\qquad\n\\hat{s}_{g,b} \\;=\\; \\sqrt{ \\frac{1}{n_{b,\\text{train}}-1} \\sum_{i \\in \\text{train}, B_i=b} (Z_{g,i} - \\hat{m}_{g,b})^2 }.\n$$\n对于训练集中存在的批次，通过 $Z_{g,i}^{\\text{adj}} = (Z_{g,i} - \\hat{m}_{g,B_i}) / \\hat{s}_{g,B_i}$ 调整训练集和测试集的标准化值；如果一个测试批次在训练集中不存在，则不对该批次进行调整（恒等变换），或使用一个不依赖于测试数据的明确定义的回退方案。最后，使用全局训练统计量重新缩放回去：$X_{g,i}^{\\text{adj}} = Z_{g,i}^{\\text{adj}} \\hat{\\sigma}_g + \\hat{\\mu}_g$。这是一个位置-尺度 ComBat 风格的校正，没有经验贝叶斯收缩，严格在训练折内执行以防止数据泄露。\n\n分类器和优化目标：\n- 使用一个在线性 SVM 分类器，在原问题中拟合，最小化正则化经验风险。设 $w \\in \\mathbb{R}^p$ 和 $b \\in \\mathbb{R}$。对于训练样本 $(x_i, y_i)$，平方合页损失目标函数为\n$$\n\\min_{w,b} \\quad \\frac{\\lambda}{2}\\|w\\|_2^2 \\;+\\; \\frac{1}{n_{\\text{train}}} \\sum_{i \\in \\text{train}} \\left( \\max\\{0, 1 - y_i (w^\\top x_i + b)\\} \\right)^2,\n$$\n其中 $\\lambda  0$ 是正则化参数。通过对小批量数据进行随机梯度下降 (SGD) 来优化此目标。对于一个训练样本 $(x_i, y_i)$，其间隔为 $m_i = y_i (w^\\top x_i + b)$，梯度贡献为\n$$\n\\nabla_w \\;\\leftarrow\\; \\lambda w \\;-\\; \\frac{2}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{1}\\{m_i  1\\} \\, y_i x_i (1 - m_i), \\qquad\n\\nabla_b \\;\\leftarrow\\; - \\frac{2}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{1}\\{m_i  1\\} \\, y_i (1 - m_i).\n$$\n使用学习率 $\\eta  0$ 进行更新：$(w, b) \\leftarrow (w, b) - \\eta (\\nabla_w, \\nabla_b)$。\n\n数据泄露考量：\n- 一个依赖于整个数据集（包括测试样本）的预处理算子是一种数据依赖的变换。如果在交叉验证之前对所有样本估计批次校正参数，测试集信息会影响训练变换，从而打破训练集和测试集之间的独立性假设。形式上，设 $\\mathcal{T}$ 为训练集，$\\mathcal{S}$ 为测试集，一个依赖于 $\\mathcal{T} \\cup \\mathcal{S}$ 的数据依赖映射 $\\mathcal{P}$ 会改变以 $\\mathcal{S}$ 为条件的变换后 $\\mathcal{T}$ 的分布，这违反了标准的泛化设置并会夸大性能。\n\n任务：\n- 严格在训练折内实现上述 ComBat 风格的批次校正。\n- 使用所提供的目标函数，通过随机梯度下降拟合一个使用平方合页损失的线性 SVM。\n- 使用 $K=5$ 的 $K$ 折交叉验证，并计算各折的平均分类准确率，其中准确率是正确分类的测试样本的比例，以小数形式表示（而非百分比）。\n\n测试套件和要求输出：\n- 所有随机抽取必须由固定的伪随机种子控制，以确保确定性。对数据生成和任何随机优化使用种子 $2025$。\n- 使用 $p=50$ 个特征，二元标签 $Y_i \\in \\{-1,+1\\}$ 在每个批次内大致平均分配。\n- 对于所有情况，设置以下生成参数：特征基线 $\\mu_g = 0$ 对所有 $g$，信号特征数量 $p_{\\text{sig}} = 20$（特征 $1$ 到 $20$），信号强度 $s_g = s = 1.0$ 对 $g \\le p_{\\text{sig}}$ 且 $s_g = 0$ 对其他情况，噪声 $\\varepsilon_{g,i} \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$ 且 $\\sigma_\\varepsilon = 0.5$。对于每个批次 $b$ 和特征 $g$，从 $\\mathcal{N}(0, \\sigma_\\delta^2)$（其中 $\\sigma_\\delta = 1.0$）中抽取位置偏移 $\\delta_{g,b}$，并从对数正态分布中抽取尺度效应 $\\gamma_{g,b}$，其对数尺度标准差为 $\\sigma_\\gamma = 0.3$，对数均值为零，即 $\\log \\gamma_{g,b} \\sim \\mathcal{N}(0, \\sigma_\\gamma^2)$。\n- 使用 SVM 超参数：正则化 $\\lambda = 10^{-3}$，学习率 $\\eta = 0.05$，轮次 $E = 20$，小批量大小 $m = 32$。\n- 除非另有规定，通过对类别标签进行分层来构建折，并保持各折不相交。当整个批次在某个训练折中缺失时，对于相应测试集中的该批次，不进行调整（恒等变换），而不是使用任何从测试中得出的量。\n\n提供三个测试案例：\n1. 案例1（均衡多队列）：$B=3$ 个批次，每个批次样本大小为 $[40, 40, 40]$。执行 $5$ 折交叉验证，在每个训练折内进行 ComBat 风格的校正，拟合 SVM，并报告各折的平均测试准确率，以四舍五入到四位小数的小数形式表示。\n2. 案例2（边缘情况：训练集中缺少批次）：$B=3$ 个批次，样本大小为 $[50, 50, 10]$。构建 $5$ 折交叉验证，使得第 $1$ 折的测试集中包含来自小批次的所有 $10$ 个样本，而其训练集中没有；对于其他折，使用分层分割。执行折内批次校正，并报告各折的平均测试准确率，以四舍五入到四位小数的小数形式表示。\n3. 案例3（数据泄露敏感性）：重用案例1的数据。计算两个流程：流程 $\\mathcal{P}_{\\text{leak}}$，其中批次校正参数在整个数据集上拟合一次，并在交叉验证之前应用；以及流程 $\\mathcal{P}_{\\text{strict}}$，其中参数严格在训练折内拟合。报告在相同折上的平均测试准确率差异（流程 $\\mathcal{P}_{\\text{leak}}$ 减去流程 $\\mathcal{P}_{\\text{strict}}$），以四舍五入到四位小数的小数形式表示。\n\n最终输出格式：\n- 您的程序应产生单行输出，包含三个结果，形式为用方括号括起来的逗号分隔列表（例如，$[0.8123,0.7450,0.0360]$）。条目必须按案例1、案例2和案例3的顺序排列。所有值都必须是四舍五入到四位小数的小数。不应打印其他任何文本。",
            "solution": "该问题要求开发和评估一个用于多队列组学数据二元分类的计算流程。核心任务包括：基于一个包含生物信号和批次效应的特定生成模型来生成合成数据，实现一个有原则的批次校正程序，训练一个线性支持向量机 (SVM) 分类器，并使用交叉验证来评估整个流程。问题的一个关键方面是在交叉验证框架内正确处理批次校正，以防止测试集数据泄露到训练过程中。解决方案分为三部分：数据模型和算法的形式化，交叉验证和评估策略的描述，以及三个指定测试案例的实现细节。\n\n首先，我们对底层的科学和数学原理进行形式化。对于特征 $g \\in \\{1, \\dots, p\\}$ 在样本 $i \\in \\{1, \\dots, n\\}$ 中的测量值 $X_{g,i}$ 的生成模型由下式给出：\n$$\nX_{g,i} \\;=\\; \\mu_g \\;+\\; s_g Y_i \\;+\\; \\delta_{g, B_i} \\;+\\; \\gamma_{g, B_i} \\,\\varepsilon_{g,i}\n$$\n在这里，$\\mu_g$ 是特征 $g$ 的基线丰度，$Y_i \\in \\{-1, +1\\}$ 是二元类别标签，$s_g$ 代表该特征与类别的关联（生物信号）。项 $\\delta_{g, B_i}$ 和 $\\gamma_{g, B_i}$ 分别是样本 $i$ 所属批次 $B_i$ 的加性（位置）和乘性（尺度）批次效应。项 $\\varepsilon_{g,i}$ 代表从均值为0、方差有限的分布中抽取的独立同分布噪声。根据问题规范，参数实例化如下：$\\mu_g=0$，$s_g=1.0$ 用于前 $p_{\\text{sig}}=20$ 个特征，其他特征 $s_g=0$；$\\delta_{g,b} \\sim \\mathcal{N}(0, \\sigma_\\delta^2)$ 且 $\\sigma_\\delta=1.0$；$\\log \\gamma_{g,b} \\sim \\mathcal{N}(0, \\sigma_\\gamma^2)$ 且 $\\sigma_\\gamma=0.3$；以及 $\\varepsilon_{g,i} \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$ 且 $\\sigma_\\varepsilon=0.5$。\n\n为了减轻批次效应的混淆影响，应用了参数化经验贝叶斯 (EB) ComBat 风格的校正。为防止数据泄露，此校正的所有参数必须完全从每个交叉验证折的训练数据中估计。对每个特征 $g$ 的程序如下：\n1.  仅使用训练样本计算全局均值 $\\hat{\\mu}_g$ 和标准差 $\\hat{\\sigma}_g$。\n    $$\n    \\hat{\\mu}_g \\;=\\; \\frac{1}{n_{\\text{train}}} \\sum_{i \\in \\text{train}} X_{g,i}, \\qquad\n    \\hat{\\sigma}_g \\;=\\; \\sqrt{ \\frac{1}{n_{\\text{train}}-1} \\sum_{i \\in \\text{train}} (X_{g,i} - \\hat{\\mu}_g)^2 }\n    $$\n2.  使用这些从训练中得出的参数对训练数据和测试数据进行标准化：$Z_{g,i} = (X_{g,i} - \\hat{\\mu}_g) / \\hat{\\sigma}_g$。为了数值稳定性，向 $\\hat{\\sigma}_g$ 中加入一个小的常数 $\\epsilon$。\n3.  对于训练集中存在的每个批次 $b$，估计其在标准化数据上的特定位置和尺度效应。注意：问题陈述在 $\\hat{s}_{g,b}$ 的定义中包含一个排版错误，即递归的自我引用。将要实现的正确公式，是以估计的均值 $\\hat{m}_{g,b}$ 为中心来计算方差。\n    $$\n    \\hat{m}_{g,b} \\;=\\; \\frac{1}{n_{b,\\text{train}}} \\sum_{i \\in \\text{train}, B_i=b} Z_{g,i}, \\qquad\n    \\hat{s}_{g,b} \\;=\\; \\sqrt{ \\frac{1}{n_{b,\\text{train}}-1} \\sum_{i \\in \\text{train}, B_i=b} (Z_{g,i} - \\hat{m}_{g,b})^2 }\n    $$\n    同样为 $\\hat{s}_{g,b}$ 的分母使用一个稳定性常数。如果一个批次在训练折中只包含一个样本，其尺度校正因子 $\\hat{s}_{g,b}$ 设为 $1$。\n4.  调整标准化值。对于在训练期间存在的批次 $B_i$ 中的样本 $i$，调整为 $Z_{g,i}^{\\text{adj}} = (Z_{g,i} - \\hat{m}_{g,B_i}) / \\hat{s}_{g,B_i}$。如果批次 $B_i$（例如，来自测试样本）不在训练集中，则不进行调整（即 $Z_{g,i}^{\\text{adj}} = Z_{g,i}$）。\n5.  使用全局训练参数将调整后的数据重新缩放回原始特征空间维度：$X_{g,i}^{\\text{adj}} = Z_{g,i}^{\\text{adj}} \\hat{\\sigma}_g + \\hat{\\mu}_g$。\n\n然后，使用校正后的数据 $x_i^{\\text{adj}} \\in \\mathbb{R}^p$ 和相应的标签 $y_i$ 来训练线性 SVM 分类器。该分类器试图通过最小化带 $L_2$ 正则化的平方合页损失来找到权重向量 $w \\in \\mathbb{R}^p$ 和偏置项 $b \\in \\mathbb{R}$：\n$$\n\\min_{w,b} \\quad \\frac{\\lambda}{2}\\|w\\|_2^2 \\;+\\; \\frac{1}{n_{\\text{train}}} \\sum_{i \\in \\text{train}} \\left( \\max\\{0, 1 - y_i (w^\\top x_i^{\\text{adj}} + b)\\} \\right)^2\n$$\n该目标函数使用随机梯度下降 (SGD) 进行优化。对于每个训练样本的小批量 $\\mathcal{B}$，权重和偏置被迭代更新。使用学习率 $\\eta  0$ 和正则化参数 $\\lambda  0$，更新使用以下梯度进行：\n$$\n\\nabla_w J = \\lambda w \\;-\\; \\frac{2}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{1}\\{m_i  1\\} \\, y_i x_i^{\\text{adj}} (1 - m_i)\n$$\n$$\n\\nabla_b J = - \\frac{2}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{1}\\{m_i  1\\} \\, y_i (1 - m_i)\n$$\n其中 $m_i = y_i (w^\\top x_i^{\\text{adj}} + b)$ 是样本 $i$ 的间隔，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数。参数按 $(w,b) \\leftarrow (w,b) - \\eta (\\nabla_w J, \\nabla_b J)$ 进行更新。\n\n评估将使用 $K=5$ 折交叉验证进行。数据集被划分为5个不相交的折。在5次迭代中的每一次，一折用作测试集，其余4折用作训练集。5个测试折的平均分类准确率是最终的性能指标。\n\n案例1涉及一个均衡设计，有 $B=3$ 个批次，大小为 $[40, 40, 40]$。使用标准的分层5折分割。批次校正严格在每个训练折内进行。\n\n案例2研究一个边缘情况，批次大小为 $[50, 50, 10]$。构建5个折，使得对于第一折，包含10个样本的小批次完全成为测试集的一部分，确保它在相应的训练集中缺失。这测试了指定的回退机制（对未知批次进行恒等变换）。其余的折对数据的剩余部分进行划分。\n\n案例3直接量化了数据泄露的影响。使用与案例1相同的数据和折，我们比较两个流程。严格流程 $\\mathcal{P}_{\\text{strict}}$ 在每个折内进行校正，如上所述。泄露流程 $\\mathcal{P}_{\\text{leak}}$ 在交叉验证开始前，在整个数据集上一次性估计批次校正参数。报告的结果是平均准确率的差异，$\\text{Acc}(\\mathcal{P}_{\\text{leak}}) - \\text{Acc}(\\mathcal{P}_{\\text{strict}})$，预计该值为正，因测试集信息泄露到训练变换中而人为地夸大了性能。所有计算均遵守指定的随机种子 $2025$ 以实现完全的可复现性。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the execution of the three test cases.\n    \"\"\"\n\n    def generate_data(batch_sizes, p, p_sig, s, sigma_eps, sigma_delta, sigma_gamma, seed):\n        \"\"\"Generates synthetic omics data based on the specified model.\"\"\"\n        rng = np.random.default_rng(seed)\n        n_samples = sum(batch_sizes)\n        n_batches = len(batch_sizes)\n\n        # Assign samples to batches and create class labels (Y)\n        batches = np.repeat(np.arange(1, n_batches + 1), batch_sizes)\n        y = np.ones(n_samples, dtype=int)\n        start_idx = 0\n        for size in batch_sizes:\n            # Assign labels approximately equally within each batch\n            n_pos = size // 2\n            y[start_idx : start_idx + n_pos] = 1\n            y[start_idx + n_pos : start_idx + size] = -1\n            rng.shuffle(y[start_idx : start_idx + size])\n            start_idx += size\n        \n        # Generate model parameters\n        mu = np.zeros(p)\n        s_vec = np.zeros(p)\n        s_vec[:p_sig] = s\n\n        # Batch effects\n        delta = rng.normal(0, sigma_delta, size=(p, n_batches))\n        log_gamma = rng.normal(0, sigma_gamma, size=(p, n_batches))\n        gamma = np.exp(log_gamma)\n\n        # Noise\n        epsilon = rng.normal(0, sigma_eps, size=(n_samples, p))\n\n        # Construct data matrix X\n        X = np.zeros((n_samples, p))\n        for i in range(n_samples):\n            batch_idx = batches[i] - 1\n            X[i, :] = mu + s_vec * y[i] + delta[:, batch_idx] + gamma[:, batch_idx] * epsilon[i, :]\n        \n        return X, y, batches\n\n    def combat_fit(X_train, batches_train):\n        \"\"\"Estimates ComBat parameters from training data.\"\"\"\n        train_batches_unique = np.unique(batches_train)\n        p = X_train.shape[1]\n\n        # Global stats\n        mu_g = np.mean(X_train, axis=0)\n        sigma_g = np.std(X_train, axis=0, ddof=1)\n        # Add a small epsilon for stability if std dev is zero\n        sigma_g[sigma_g == 0] = 1e-8\n\n        # Standardize data\n        Z = (X_train - mu_g) / sigma_g\n\n        # Batch-specific stats\n        batch_map = {batch_id: i for i, batch_id in enumerate(train_batches_unique)}\n        m_gb = np.zeros((p, len(train_batches_unique)))\n        s_gb = np.ones((p, len(train_batches_unique)))\n        \n        for batch_id, i in batch_map.items():\n            idx = (batches_train == batch_id)\n            n_b = np.sum(idx)\n            if n_b  0:\n                Z_b = Z[idx, :]\n                m_gb[:, i] = np.mean(Z_b, axis=0)\n                if n_b  1:\n                    s_gb[:, i] = np.std(Z_b, axis=0, ddof=1)\n        # Add a small epsilon for stability\n        s_gb[s_gb == 0] = 1e-8\n\n        return {'mu_g': mu_g, 'sigma_g': sigma_g, 'm_gb': m_gb, 's_gb': s_gb, 'batch_map': batch_map}\n    \n    def combat_transform(X, batches, params):\n        \"\"\"Applies ComBat correction using pre-fitted parameters.\"\"\"\n        mu_g, sigma_g, m_gb, s_gb, batch_map = params['mu_g'], params['sigma_g'], params['m_gb'], params['s_gb'], params['batch_map']\n        \n        X_adj = np.copy(X)\n        Z = (X - mu_g) / sigma_g\n\n        for b_id in np.unique(batches):\n            if b_id in batch_map:\n                idx = (batches == b_id)\n                batch_col_idx = batch_map[b_id]\n                \n                Z_adj_b = (Z[idx, :] - m_gb[:, batch_col_idx]) / s_gb[:, batch_col_idx]\n                X_adj[idx, :] = Z_adj_b * sigma_g + mu_g\n        \n        return X_adj\n\n    def svm_sgd(X_train, y_train, lambda_, eta, epochs, batch_size, seed):\n        \"\"\"Trains a linear SVM with squared hinge loss via SGD.\"\"\"\n        rng = np.random.default_rng(seed)\n        n, p = X_train.shape\n        w = np.zeros(p)\n        b = 0.0\n\n        indices = np.arange(n)\n        for _ in range(epochs):\n            rng.shuffle(indices)\n            for i in range(0, n, batch_size):\n                batch_indices = indices[i:i + batch_size]\n                X_batch, y_batch = X_train[batch_indices], y_train[batch_indices]\n                \n                margins = y_batch * (X_batch @ w + b)\n                \n                # Identify samples with margin  1\n                misclassified_mask = margins  1\n                \n                grad_w = lambda_ * w\n                grad_b = 0.0\n\n                if np.any(misclassified_mask):\n                    y_mis = y_batch[misclassified_mask]\n                    X_mis = X_batch[misclassified_mask]\n                    one_minus_m = 1 - margins[misclassified_mask]\n\n                    grad_w -= (2 / len(X_batch)) * np.sum( (y_mis * one_minus_m)[:, np.newaxis] * X_mis, axis=0)\n                    grad_b -= (2 / len(X_batch)) * np.sum(y_mis * one_minus_m)\n\n                w -= eta * grad_w\n                b -= eta * grad_b\n        return w, b\n\n    def predict(X, w, b):\n        \"\"\"Makes predictions using the trained SVM.\"\"\"\n        return np.sign(X @ w + b)\n\n    def accuracy(y_true, y_pred):\n        \"\"\"Calculates classification accuracy.\"\"\"\n        return np.mean(y_true == y_pred)\n\n    def create_stratified_folds(y, k, seed):\n        \"\"\"Creates stratified K-folds.\"\"\"\n        rng = np.random.default_rng(seed)\n        indices = np.arange(len(y))\n        pos_indices = indices[y == 1]\n        neg_indices = indices[y == -1]\n        rng.shuffle(pos_indices)\n        rng.shuffle(neg_indices)\n        \n        pos_folds = np.array_split(pos_indices, k)\n        neg_folds = np.array_split(neg_indices, k)\n        \n        folds = []\n        for i in range(k):\n            test_indices = np.concatenate((pos_folds[i], neg_folds[i]))\n            rng.shuffle(test_indices)\n            folds.append(test_indices)\n            \n        return folds\n\n    def run_cv_pipeline(X, y, batches, folds, lambda_, eta, epochs, batch_size, cv_seed, pipeline_mode, full_data_params=None):\n        \"\"\"Runs the full CV pipeline.\"\"\"\n        accuracies = []\n        n_samples = len(y)\n        all_indices = np.arange(n_samples)\n\n        for test_indices in folds:\n            train_indices = np.setdiff1d(all_indices, test_indices)\n\n            X_train, y_train, batches_train = X[train_indices], y[train_indices], batches[train_indices]\n            X_test, y_test, batches_test = X[test_indices], y[test_indices], batches[test_indices]\n            \n            if pipeline_mode == 'strict':\n                params = combat_fit(X_train, batches_train)\n            elif pipeline_mode == 'leaky':\n                params = full_data_params\n            else:\n                raise ValueError(\"Invalid pipeline mode\")\n\n            X_train_adj = combat_transform(X_train, batches_train, params)\n            X_test_adj = combat_transform(X_test, batches_test, params)\n            \n            # Train SVM\n            w, b = svm_sgd(X_train_adj, y_train, lambda_, eta, epochs, batch_size, cv_seed)\n            \n            # Evaluate\n            y_pred = predict(X_test_adj, w, b)\n            acc = accuracy(y_test, y_pred)\n            accuracies.append(acc)\n            \n        return np.mean(accuracies)\n\n    # Common parameters\n    p = 50\n    p_sig = 20\n    s_val = 1.0\n    sigma_eps = 0.5\n    sigma_delta = 1.0\n    sigma_gamma = 0.3\n    data_seed = 2025\n    \n    lambda_ = 1e-3\n    eta = 0.05\n    epochs = 20\n    batch_size = 32\n    k_folds = 5\n\n    results = []\n\n    # --- Case 1 ---\n    batch_sizes_1 = [40, 40, 40]\n    X1, y1, batches1 = generate_data(batch_sizes_1, p, p_sig, s_val, sigma_eps, sigma_delta, sigma_gamma, data_seed)\n    folds1 = create_stratified_folds(y1, k_folds, data_seed)\n    acc1 = run_cv_pipeline(X1, y1, batches1, folds1, lambda_, eta, epochs, batch_size, data_seed, 'strict')\n    results.append(acc1)\n\n    # --- Case 2 ---\n    batch_sizes_2 = [50, 50, 10]\n    X2, y2, batches2 = generate_data(batch_sizes_2, p, p_sig, s_val, sigma_eps, sigma_delta, sigma_gamma, data_seed)\n    \n    # Custom fold generation for Case 2\n    b3_indices = np.where(batches2 == 3)[0]\n    b12_indices = np.where(batches2 != 3)[0]\n    \n    test_fold1 = b3_indices\n    remaining_indices = b12_indices\n    y_remaining = y2[remaining_indices]\n\n    other_folds_relative = create_stratified_folds(y_remaining, k_folds - 1, data_seed)\n    \n    folds2 = [test_fold1]\n    for fold in other_folds_relative:\n        folds2.append(remaining_indices[fold])\n\n    acc2 = run_cv_pipeline(X2, y2, batches2, folds2, lambda_, eta, epochs, batch_size, data_seed, 'strict')\n    results.append(acc2)\n\n    # --- Case 3 ---\n    # Re-use data and folds from Case 1\n    # Pipeline_strict is just acc1\n    acc_strict = acc1\n    \n    # Pipeline_leak\n    leaky_params = combat_fit(X1, batches1)\n    acc_leak = run_cv_pipeline(X1, y1, batches1, folds1, lambda_, eta, epochs, batch_size, data_seed, 'leaky', full_data_params=leaky_params)\n    \n    diff = acc_leak - acc_strict\n    results.append(diff)\n    \n    # Print final output\n    print(f\"[{','.join([f'{r:.4f}' for r in results])}]\")\n\nsolve()\n\n```"
        }
    ]
}