{
    "hands_on_practices": [
        {
            "introduction": "要理解卷积神经网络（CNN）为何能有效发现序列基序（motif），关键在于揭示其核心操作——卷积——与经典生物信息学方法之间的深刻联系。本练习将引导您从第一性原理出发，通过编程实践来证明，对单热编码（one-hot encoded）的DNA序列进行卷积操作，在数学上等同于使用位置权重矩阵（PWM）进行扫描并计算对数几率得分（log-odds score）。通过亲手实现这一过程，您将不再将CNN视为一个黑箱，而是能深刻理解其在基序发现任务中的生物学基础和工作原理。",
            "id": "3297925",
            "problem": "您的任务是实现一个严格的位置权重矩阵（PWM）扫描，并在序列基序发现的背景下，证明其等同于卷积神经网络（CNN）的单通道、多滤波器操作。其科学基础应从概率模型定义和对数似然比开始。位置权重矩阵（PWM）通过一组关于脱氧核糖核酸（DNA）核苷酸的位置特异性多项分布来表示一个长度为 $L$ 的基序。背景模型给出了独立的核苷酸概率。您必须计算整个DNA序列上的窗口化对数优势比分数，并识别出所有分数严格超过给定阈值的起始索引。\n\n基本原理：\n- 设基序由一个PWM表示，其条目为 $p_{i,b}$，其中 $i \\in \\{0,1,\\dots,L-1\\}$ 是位置索引，$b \\in \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$ 是核苷酸索引。设背景分布为 $q_b$，满足 $q_{\\mathrm{A}} + q_{\\mathrm{C}} + q_{\\mathrm{G}} + q_{\\mathrm{T}} = 1$ 且每个 $q_b > 0$。\n- 在独立位置假设下，基序模型为长度为 $L$ 的子序列 $x_{t:t+L-1}$ 分配的概率为 $\\prod_{i=0}^{L-1} p_{i, x_{t+i}}$，而背景模型分配的概率为 $\\prod_{i=0}^{L-1} q_{x_{t+i}}$。对数优势比（对数似然比）分数定义为\n$$\nS(t) = \\log \\frac{\\prod_{i=0}^{L-1} p_{i, x_{t+i}}}{\\prod_{i=0}^{L-1} q_{x_{t+i}}}\n= \\sum_{i=0}^{L-1} \\log \\frac{p_{i, x_{t+i}}}{q_{x_{t+i}}}.\n$$\n- 定义对数优势比权重矩阵 $W$ 为 $W_{i,b} = \\log\\left(\\frac{p_{i,b}}{q_b}\\right)$，使用自然对数。将DNA序列编码为一个独热矩阵 $X \\in \\{0,1\\}^{T \\times 4}$，其中 $T$ 是序列长度，通道按顺序对应 $\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}$。对于任何未知的核苷酸符号（如 $N$），编码为零向量 $(0,0,0,0)$，使其对对数优势比总和的贡献为零。那么分数可以写成\n$$\nS(t) = \\sum_{i=0}^{L-1} \\sum_{b \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}} W_{i,b} \\, X_{t+i,b},\n$$\n这正是在卷积神经网络（CNN）中使用的有效模式多通道卷积，其中滤波器 $W$ 跨越 $L$ 个位置和 $4$ 个通道。\n\n程序要求：\n- 实现一个函数，给定一个DNA序列、由每个位置的核苷酸概率 $p_{i,b}$ 指定的PWM、一个背景分布 $q_b$ 和一个实值阈值 $\\tau$，计算所有有效起始位置 $t \\in \\{0,1,\\dots,T-L\\}$ 的 $S(t)$，并返回满足 $S(t) > \\tau$ 的索引 $t$ 的列表。\n- 在所有对数优势比计算中使用自然对数。按照上述规定，通过在未知核苷酸 $N$ 的位置贡献 $0$ 来处理它们。\n- 严格遵循“超过”标准：仅当 $S(t)$ 严格大于 $\\tau$ 时才包括该位置。\n- 仅在正向链上操作；不考虑反向互补链。\n- 所有数学量必须严格按照上述定义处理。\n\n测试套件：\n您必须硬编码并评估以下五个测试用例。对于每个用例，报告分数严格超过指定阈值的起始索引列表。\n\n- 用例 1（理想情况，均匀背景，精确基序重复出现）：\n    - 序列：$\\texttt{ACGTACGTACGT}$。\n    - PWM长度 $L = 4$，每个位置的概率（顺序为 $\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}$）：\n        - 位置 0：$(0.7, 0.1, 0.1, 0.1)$。\n        - 位置 1：$(0.1, 0.7, 0.1, 0.1)$。\n        - 位置 2：$(0.1, 0.1, 0.7, 0.1)$。\n        - 位置 3：$(0.1, 0.1, 0.1, 0.7)$。\n    - 背景 $q = (0.25, 0.25, 0.25, 0.25)$。\n    - 阈值 $\\tau = 3.5$。\n\n- 用例 2（边界情况，没有窗口超过高阈值）：\n    - 序列：$\\texttt{AAAAACCCCC}$。\n    - PWM和背景与用例1相同。\n    - 阈值 $\\tau = 4.0$。\n\n- 用例 3（未知符号 $N$ 减少贡献，但精确匹配出现一次）：\n    - 序列：$\\texttt{NNNACGTNNN}$。\n    - PWM和背景与用例1相同。\n    - 阈值 $\\tau = 3.5$。\n\n- 用例 4（非均匀背景，不同的PWM长度和组成）：\n    - 序列：$\\texttt{ACGACGTTAC}$。\n    - PWM长度 $L = 3$，概率（顺序为 $\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}$）：\n        - 位置 0：$(0.6, 0.2, 0.1, 0.1)$。\n        - 位置 1：$(0.1, 0.6, 0.2, 0.1)$。\n        - 位置 2：$(0.1, 0.2, 0.6, 0.1)$。\n    - 背景 $q = (0.1, 0.4, 0.4, 0.1)$。\n    - 阈值 $\\tau = 2.4$。\n\n- 用例 5（精确等于阈值不应被包括）：\n    - 序列：$\\texttt{ACGT}$。\n    - PWM和背景与用例1相同。\n    - 阈值 $\\tau = 4.118477668724633$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素对应一个测试用例，并且本身是严格超过阈值的位置的整数列表，顺序与上述测试套件一致。例如，输出可能看起来像 $[\\,[0,4,8],[],[3],[0,3],[]\\,]$。",
            "solution": "我们从基序的独立位置模型以及独立同分布的背景模型开始。设一个长度为 $L$ 的基序由一个位置权重矩阵（PWM）描述，其条目为 $p_{i,b}$，其中 $i \\in \\{0,\\dots,L-1\\}$ 是基序位置的索引，$b \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$ 是核苷酸的索引。设背景模型由 $q_b$ 参数化，使得 $q_{\\mathrm{A}} + q_{\\mathrm{C}} + q_{\\mathrm{G}} + q_{\\mathrm{T}} = 1$ 且每个 $q_b > 0$。对于一个DNA序列 $x_0 x_1 \\dots x_{T-1}$，考虑任何窗口起始索引 $t$ 且 $0 \\le t \\le T-L$。基序模型为子序列 $x_{t:t+L-1}$ 分配的概率是 $\\prod_{i=0}^{L-1} p_{i, x_{t+i}}$，而背景模型分配的概率是 $\\prod_{i=0}^{L-1} q_{x_{t+i}}$。经典的扫描分数是自然对数似然比\n$$\nS(t) = \\log \\frac{\\prod_{i=0}^{L-1} p_{i, x_{t+i}}}{\\prod_{i=0}^{L-1} q_{x_{t+i}}}\n= \\sum_{i=0}^{L-1} \\log \\frac{p_{i, x_{t+i}}}{q_{x_{t+i}}}.\n$$\n定义对数优势比权重矩阵 $W$ 为 $W_{i,b} = \\log\\left(\\frac{p_{i,b}}{q_b}\\right)$，我们有\n$$\nS(t) = \\sum_{i=0}^{L-1} W_{i, x_{t+i}}.\n$$\n引入独热编码 $X \\in \\{0,1\\}^{T \\times 4}$，通道顺序为 $(\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T})$，即如果 $x_j=b$ 则 $X_{j,b} = 1$，否则为 $0$；对于未知符号（例如 $N$），对所有 $b$ 设置 $X_{j,b}=0$。然后，对于任何起始位置 $t$，分数可以写成\n$$\nS(t) = \\sum_{i=0}^{L-1} \\sum_{b} W_{i,b} \\, X_{t+i,b},\n$$\n这与在卷积神经网络（CNN）中用于序列基序发现的、跨序列长度维度使用滤波器 $W$（跨越 $L$ 个位置和 $4$ 个通道）的有效模式卷积是相同的。用CNN的术语来说，$S(t)$ 是在位置 $t$ 处将单个滤波器应用于独热编码输入所得到的激活值。\n\n算法设计：\n- 通过 $W_{i,b} = \\log\\left(\\frac{p_{i,b}}{q_b}\\right)$ 使用自然对数计算 $W$，由于所提供的测试套件中每个 $p_{i,b}$ 和 $q_b$ 都是严格为正的，因此可以保证得到有限值。\n- 对序列进行独热编码，映射关系为 $\\mathrm{A} \\mapsto (1,0,0,0)$，$\\mathrm{C} \\mapsto (0,1,0,0)$，$\\mathrm{G} \\mapsto (0,0,1,0)$，$\\mathrm{T} \\mapsto (0,0,0,1)$，以及 $N \\mapsto (0,0,0,0)$。\n- 对于每个有效的起始索引 $t$，计算 $S(t)$ 作为张量缩并 $S(t) = \\sum_{i,b} W_{i,b} X_{t+i,b}$，并收集所有满足 $S(t) > \\tau$ 的 $t$。\n- 对五个测试用例中的每一个都执行此操作，并按顺序汇总结果。\n\n所提供测试套件的分析预期：\n- 用例 1：该基序对模式 $\\texttt{ACGT}$ 在各个位置上具有高度特异性。在均匀背景 $q_b = 0.25$ 的情况下，每个位置匹配的对数优势比权重为 $\\log(0.7/0.25) = \\log(2.8) \\approx 1.029619417$，错配权重为 $\\log(0.1/0.25) = \\log(0.4) \\approx -0.916290732$。一个完美匹配的子序列 $\\texttt{ACGT}$ 产生的总分数为 $4 \\times \\log(2.8) \\approx 4.118477669$，这严格超过了 $\\tau = 3.5$。在序列 $\\texttt{ACGTACGTACGT}$ 中，完美匹配出现在位置 $0$、$4$ 和 $8$。所有其他长度为 $4$ 的窗口都是导致全部错配的移位，产生负分，因此结果是 $[0,4,8]$。\n- 用例 2：使用相同的PWM和背景，序列 $\\texttt{AAAAACCCCC}$ 不包含 $\\texttt{ACGT}$ 窗口。即使是最好的窗口也无法超过 $\\tau = 4.0$。例如，$\\texttt{AAAA}$ 产生的 $S \\approx 1.029619417 + 3 \\times (-0.916290732) \\approx -1.719252779$。因此，没有位置超过阈值，结果是 $[]$。\n- 用例 3：根据指定的编码和求和规则，未知符号 $N$ 贡献为零。序列 $\\texttt{NNNACGTNNN}$ 在位置 $3$ 处包含一个完美匹配 $\\texttt{ACGT}$，其 $S \\approx 4.118477669 > 3.5$，而所有与 $N$ 重叠但没有精确基序的窗口累积的分数不足或为负。因此，结果是 $[3]$。\n- 用例 4：非均匀背景 $q = (0.1, 0.4, 0.4, 0.1)$ 和对 $\\texttt{ACG}$ 特异的PWM长度 $L=3$。完美匹配的分数为 $\\log(0.6/0.1) + \\log(0.6/0.4) + \\log(0.6/0.4) = \\log(6) + 2 \\log(1.5) \\approx 2.602689685$，超过了 $\\tau = 2.4$。在序列 $\\texttt{ACGACGTTAC}$ 中，完美匹配出现在位置 $0$ 和 $3$，其他窗口不是最优的，分数低于阈值。因此结果是 $[0,3]$。\n- 用例 5：阈值被设置为 $\\tau = 4.118477668724633$，这略大于从PWM和均匀背景中获得的确切完美匹配分数。对于序列 $\\texttt{ACGT}$，唯一的窗口是整个序列，其得分约为 $4.118477668724632$，由于严格不等式，它没有严格超过阈值。因此，结果是 $[]$。\n\n实现说明：\n- 计算中一致使用自然对数。\n- 程序生成单行输出，按顺序汇总所有五个用例的结果，格式为 $[\\,[0,4,8],[],[3],[0,3],[]\\,]$。\n- 该方法展示了统计PWM扫描与计算系统生物学中用于基序发现的卷积神经网络（CNN）核心的有效模式多通道卷积之间的联系。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nNUC_ORDER = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n\ndef one_hot_encode(seq: str) -> np.ndarray:\n    \"\"\"\n    Encode DNA sequence into one-hot with channels in order A, C, G, T.\n    Unknowns (e.g., 'N') are encoded as all zeros.\n    \"\"\"\n    X = np.zeros((len(seq), 4), dtype=float)\n    for i, ch in enumerate(seq.upper()):\n        idx = NUC_ORDER.get(ch, None)\n        if idx is not None:\n            X[i, idx] = 1.0\n        # else: unknown contributes 0 vector\n    return X\n\ndef log_odds_weights(pwm_probs: np.ndarray, background: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute log-odds weight matrix W_{i,b} = ln(p_{i,b} / q_b).\n    pwm_probs: shape (L, 4)\n    background: shape (4,)\n    \"\"\"\n    # Ensure arrays are float for division and log\n    pwm = np.array(pwm_probs, dtype=float)\n    q = np.array(background, dtype=float)\n    # Avoid division by zero: test suite guarantees q_b > 0 and p_{i,b} > 0\n    W = np.log(pwm / q[None, :])\n    return W\n\ndef pwm_scan_positions(seq: str, pwm_probs: np.ndarray, background: np.ndarray, threshold: float) -> list:\n    \"\"\"\n    Perform PWM scanning: compute windowed log-odds scores and return positions exceeding threshold.\n    Uses natural logarithm and strict inequality.\n    \"\"\"\n    X = one_hot_encode(seq)\n    W = log_odds_weights(pwm_probs, background)\n    L = W.shape[0]\n    T = X.shape[0]\n    results = []\n    # Slide window and compute score S(t) = sum_{i,b} W[i,b] * X[t+i,b]\n    # Equivalent to valid-mode multi-channel convolution\n    for t in range(T - L + 1):\n        window = X[t:t+L, :]  # shape (L, 4)\n        score = float(np.sum(window * W))\n        if score > threshold:\n            results.append(t)\n    return results\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case 1\n    seq1 = \"ACGTACGTACGT\"\n    pwm1 = np.array([\n        [0.7, 0.1, 0.1, 0.1],  # position 0\n        [0.1, 0.7, 0.1, 0.1],  # position 1\n        [0.1, 0.1, 0.7, 0.1],  # position 2\n        [0.1, 0.1, 0.1, 0.7],  # position 3\n    ], dtype=float)\n    bg_uniform = np.array([0.25, 0.25, 0.25, 0.25], dtype=float)\n    tau1 = 3.5\n\n    # Case 2\n    seq2 = \"AAAAACCCCC\"\n    tau2 = 4.0\n\n    # Case 3\n    seq3 = \"NNNACGTNNN\"\n    tau3 = 3.5\n\n    # Case 4\n    seq4 = \"ACGACGTTAC\"\n    pwm4 = np.array([\n        [0.6, 0.2, 0.1, 0.1],  # position 0\n        [0.1, 0.6, 0.2, 0.1],  # position 1\n        [0.1, 0.2, 0.6, 0.1],  # position 2\n    ], dtype=float)\n    bg_nonuniform = np.array([0.1, 0.4, 0.4, 0.1], dtype=float)\n    tau4 = 2.4\n\n    # Case 5\n    seq5 = \"ACGT\"\n    tau5 = 4.118477668724633  # Slightly above the perfect match sum for PWM1 under uniform bg\n\n    test_cases = [\n        (seq1, pwm1, bg_uniform, tau1),\n        (seq2, pwm1, bg_uniform, tau2),\n        (seq3, pwm1, bg_uniform, tau3),\n        (seq4, pwm4, bg_nonuniform, tau4),\n        (seq5, pwm1, bg_uniform, tau5),\n    ]\n\n    results = []\n    for seq, pwm, bg, tau in test_cases:\n        positions = pwm_scan_positions(seq, pwm, bg, tau)\n        results.append(positions)\n\n    # Final print statement in the exact required format.\n    # Ensure a single line: list of lists of integers\n    print(f\"[{','.join([str(lst) for lst in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在卷积层之后，池化（pooling）是CNN架构中另一个至关重要的组成部分，它并非一个简单的降采样操作。本练习旨在剖析最大池化（max-pooling）层在基序发现中的作用，您将首先对一个给定的激活图进行最大池化计算，然后通过一个概率模型，从理论上分析池化如何影响在噪声背景下检测微弱基序信号的能力。这项实践将帮助您理解最大池化为何能增强信号并提供位置不变性（positional invariance），这是CNN在基因组学中取得成功的关键因素之一。",
            "id": "3297865",
            "problem": "将卷积神经网络（CNN; Convolutional Neural Network）应用于脱氧核糖核酸（DNA; deoxyribonucleic acid）序列，通过一维卷积和随后的最大池化来发现转录因子结合基序。考虑一个单个卷积滤波器，在一个长度为 $14$ 的序列上生成一维激活图，其有序值为 $a_{1:14} = \\{a_{1}, a_{2}, \\dots, a_{14}\\}$，\n其中\n$a_{1} = 0.10$, $a_{2} = 0.05$, $a_{3} = 0.12$, $a_{4} = 0.03$, $a_{5} = 0.20$, $a_{6} = 0.11$, $a_{7} = 0.09$, $a_{8} = 0.08$, $a_{9} = 0.15$, $a_{10} = 0.10$, $a_{11} = 0.07$, $a_{12} = 0.13$, $a_{13} = 0.04$, $a_{14} = 0.06$。\n\n最大池化层使用窗口大小 $W = 3$ 和步长 $S = 2$ 进行有效池化（无填充，且仅考虑完全位于激活图内部的窗口）。首先，通过在每个起始于索引 $1, 1+S, 1+2S, \\dots$ 的有效窗口内取最大值，计算池化输出向量。\n\n接下来，在池化前阶段，根据以下概率模型分析弱基序信号的可检测性。在任何给定的大小为 $W$ 的池化窗口内，假设恰好有一个位置包含弱基序信号，而其余 $W-1$ 个位置仅包含噪声。将基序位置的池化前激活值建模为一个高斯随机变量 $X_{\\text{motif}} \\sim \\mathcal{N}(\\mu_{w}, \\sigma^{2})$，其均值偏移 $\\mu_{w} > 0$，方差 $\\sigma^{2} > 0$，而非基序位置的激活值是独立同分布的高斯随机变量 $X_{\\text{noise}} \\sim \\mathcal{N}(0, \\sigma^{2})$。设该窗口的池化后输出为 $Y = \\max\\{X_{1}, X_{2}, \\dots, X_{W}\\}$，其中一个 $X_{i}$ 是 $X_{\\text{motif}}$，其余是 $X_{\\text{noise}}$，并假设所有 $X_{i}$ 相互独立。\n\n推导出一个关于 $\\mu_{w}$、$\\sigma$、$W$ 和检测阈值 $\\tau \\in \\mathbb{R}$ 的闭式解析表达式，用于表示最大池化产生超过阈值的可检测输出的概率，即 $\\mathbb{P}(Y > \\tau)$，该表达式需使用高斯累积分布函数 $\\Phi(\\cdot)$ 表示。将你的最终答案表示为单个闭式表达式。不需要进行数值近似或四舍五入。所要求的最终答案应仅为 $\\mathbb{P}(Y > \\tau)$ 的解析表达式；最终答案中不要包含中间池化向量。",
            "solution": "用户提供的问题被评估为有效，因为它科学上基于卷积神经网络和概率论的原理，问题设定清晰完整，并且表达客观。该问题由两个不同的部分组成：一个数值计算和一个解析推导。\n\n第一部分：计算池化输出向量\n\n问题的第一部分要求计算将最大池化层应用于给定的一维激活图的输出。\n\n输入是长度为 $L=14$ 的激活图 $a_{1:14} = \\{a_{1}, a_{2}, \\dots, a_{14}\\}$。\n最大池化的参数是窗口大小 $W=3$ 和步长 $S=2$。\n池化操作是'有效'池化，意味着不使用填充，并且只考虑完全适合输入序列内部的窗口。\n\n第 $k$ 个窗口的起始索引（使用基于1的索引）由 $1 + (k-1)S$ 给出。窗口必须在序列长度 $L$ 之内。窗口的最后一个元素位于索引 $1 + (k-1)S + W - 1$。该值必须小于或等于 $L$。\n$1 + (k-1)S + W - 1 \\le L$\n$(k-1)S + W \\le L$\n$(k-1)2 + 3 \\le 14$\n$(k-1)2 \\le 11$\n$k-1 \\le 5.5$\n$k \\le 6.5$\n由于 $k$ 必须是整数，因此 $k$ 的最大值为 $6$。因此，有 $6$ 个有效池化窗口。\n\n设 $p_k$ 是第 $k$ 个池化窗口的输出。我们计算 $k=1, \\dots, 6$ 时的 $p_k$。\n\n1.  对于 $k=1$：窗口从索引 $1$ 开始，覆盖索引 $\\{1, 2, 3\\}$。输入值为 $\\{a_1, a_2, a_3\\} = \\{0.10, 0.05, 0.12\\}$。\n    $p_1 = \\max(0.10, 0.05, 0.12) = 0.12$。\n\n2.  对于 $k=2$：窗口从索引 $1 + S = 3$ 开始，覆盖索引 $\\{3, 4, 5\\}$。输入值为 $\\{a_3, a_4, a_5\\} = \\{0.12, 0.03, 0.20\\}$。\n    $p_2 = \\max(0.12, 0.03, 0.20) = 0.20$。\n\n3.  对于 $k=3$：窗口从索引 $1 + 2S = 5$ 开始，覆盖索引 $\\{5, 6, 7\\}$。输入值为 $\\{a_5, a_6, a_7\\} = \\{0.20, 0.11, 0.09\\}$。\n    $p_3 = \\max(0.20, 0.11, 0.09) = 0.20$。\n\n4.  对于 $k=4$：窗口从索引 $1 + 3S = 7$ 开始，覆盖索引 $\\{7, 8, 9\\}$。输入值为 $\\{a_7, a_8, a_9\\} = \\{0.09, 0.08, 0.15\\}$。\n    $p_4 = \\max(0.09, 0.08, 0.15) = 0.15$。\n\n5.  对于 $k=5$：窗口从索引 $1 + 4S = 9$ 开始，覆盖索引 $\\{9, 10, 11\\}$。输入值为 $\\{a_9, a_{10}, a_{11}\\} = \\{0.15, 0.10, 0.07\\}$。\n    $p_5 = \\max(0.15, 0.10, 0.07) = 0.15$。\n\n6.  对于 $k=6$：窗口从索引 $1 + 5S = 11$ 开始，覆盖索引 $\\{11, 12, 13\\}$。输入值为 $\\{a_{11}, a_{12}, a_{13}\\} = \\{0.07, 0.13, 0.04\\}$。\n    $p_6 = \\max(0.07, 0.13, 0.04) = 0.13$。\n\n池化输出向量为 $\\{p_1, p_2, p_3, p_4, p_5, p_6\\} = \\{0.12, 0.20, 0.20, 0.15, 0.15, 0.13\\}$。\n\n第二部分：检测概率的解析推导\n\n问题的第二部分要求推导一个闭式表达式，表示最大池化窗口的输出 $Y$ 超过阈值 $\\tau$ 的概率。\n\n池化窗口包含 $W$ 个随机变量，$\\{X_1, X_2, \\dots, X_W\\}$。其中一个变量对应于基序信号 $X_{\\text{motif}}$，其余 $W-1$ 个变量对应于噪声 $X_{\\text{noise}}$。\n分布给定如下：\n- 基序信号：$X_{\\text{motif}} \\sim \\mathcal{N}(\\mu_{w}, \\sigma^{2})$\n- 噪声信号：$X_{\\text{noise}} \\sim \\mathcal{N}(0, \\sigma^{2})$\n\n窗口内的激活值 $X_i$ 被假设为独立的。最大池化操作的输出是 $Y = \\max\\{X_1, X_2, \\dots, X_W\\}$。我们想要找到概率 $\\mathbb{P}(Y > \\tau)$。\n\n首先计算其互补概率 $\\mathbb{P}(Y \\le \\tau)$ 会更方便，这是 $Y$ 在 $\\tau$ 处的累积分布函数（CDF）。\n事件 $Y \\le \\tau$ 等价于所有单个激活值都小于或等于 $\\tau$ 的事件。\n$$ \\mathbb{P}(Y \\le \\tau) = \\mathbb{P}(\\max\\{X_1, X_2, \\dots, X_W\\} \\le \\tau) = \\mathbb{P}(X_1 \\le \\tau, X_2 \\le \\tau, \\dots, X_W \\le \\tau) $$\n由于随机变量 $X_i$ 是独立的，我们可以将联合概率写为边缘概率的乘积：\n$$ \\mathbb{P}(Y \\le \\tau) = \\prod_{i=1}^{W} \\mathbb{P}(X_i \\le \\tau) $$\n随机变量集合 $\\{X_1, \\dots, X_W\\}$ 由一个 $X_{\\text{motif}}$ 实例和 $W-1$ 个 $X_{\\text{noise}}$ 实例组成。由于乘法是可交换的，基序的具体索引不会改变乘积的最终表达式。该乘积由一个基序项和 $W-1$ 个相同的噪声变量项构成。\n$$ \\mathbb{P}(Y \\le \\tau) = \\mathbb{P}(X_{\\text{motif}} \\le \\tau) \\cdot \\left[ \\mathbb{P}(X_{\\text{noise}} \\le \\tau) \\right]^{W-1} $$\n设 $\\Phi(\\cdot)$ 是标准正态分布 $\\mathcal{N}(0, 1)$ 的累积分布函数。对于一个一般正态随机变量 $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$，其累积分布函数由 $\\mathbb{P}(X \\le x) = \\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)$ 给出。\n\n使用这个公式，我们可以表示我们变量的概率：\n1.  对于基序信号，$X_{\\text{motif}} \\sim \\mathcal{N}(\\mu_{w}, \\sigma^{2})$：\n    $$ \\mathbb{P}(X_{\\text{motif}} \\le \\tau) = \\Phi\\left(\\frac{\\tau - \\mu_{w}}{\\sigma}\\right) $$\n2.  对于噪声信号，$X_{\\text{noise}} \\sim \\mathcal{N}(0, \\sigma^{2})$：\n    $$ \\mathbb{P}(X_{\\text{noise}} \\le \\tau) = \\Phi\\left(\\frac{\\tau - 0}{\\sigma}\\right) = \\Phi\\left(\\frac{\\tau}{\\sigma}\\right) $$\n将这些表达式代入 $\\mathbb{P}(Y \\le \\tau)$ 的方程中：\n$$ \\mathbb{P}(Y \\le \\tau) = \\Phi\\left(\\frac{\\tau - \\mu_{w}}{\\sigma}\\right) \\left[ \\Phi\\left(\\frac{\\tau}{\\sigma}\\right) \\right]^{W-1} $$\n所求概率为 $\\mathbb{P}(Y > \\tau)$，可以通过全概率定律得到：\n$$ \\mathbb{P}(Y > \\tau) = 1 - \\mathbb{P}(Y \\le \\tau) $$\n因此，检测概率的最终解析表达式为：\n$$ \\mathbb{P}(Y > \\tau) = 1 - \\Phi\\left(\\frac{\\tau - \\mu_{w}}{\\sigma}\\right) \\left[ \\Phi\\left(\\frac{\\tau}{\\sigma}\\right) \\right]^{W-1} $$\n该表达式是根据题目要求，用给定的参数 $\\mu_{w}$、$\\sigma$、$W$、$\\tau$ 和标准正态累积分布函数 $\\Phi(\\cdot)$ 表示的。",
            "answer": "$$\n\\boxed{1 - \\Phi\\left(\\frac{\\tau - \\mu_{w}}{\\sigma}\\right) \\left[ \\Phi\\left(\\frac{\\tau}{\\sigma}\\right) \\right]^{W-1}}\n$$"
        },
        {
            "introduction": "当一个CNN模型训练完成后，我们如何验证其学到的滤波器（filter）是否对应着我们感兴趣的、具有生物学意义的基序？本练习将指导您设计并实现一个量化的基序恢复评估指标，用于严格比较CNN滤波器与已知的基序PWM。您将使用基于信息论的对称Kullback-Leibler（KL）散度来度量二者之间的差异，并学习如何处理不同长度基序的比对以及如何根据信息含量对不同位置进行加权。这项高级实践对于模型解释和验证至关重要，它使我们能够从“黑箱”中提取并验证有意义的生物学模式。",
            "id": "3297894",
            "problem": "给定一个已知的位置权重矩阵（Position Weight Matrix, PWM）和一个由实值权重表示的卷积神经网络（Convolutional Neural Network, CNN）滤波器。目标是设计、实现并评估一个定量的基序恢复（motif recovery）度量，该度量使用已知PWM和从滤波器派生的PWM之间的对称Kullback–Leibler散度（KLD）。该度量必须基于脱氧核糖核酸（DNA）核苷酸上的概率分布和信息论的第一性原理，并且必须包括一个有原则的跨基序位置的聚合方法和对齐处理。核苷酸的字母表为 $\\{A, C, G, T\\}$，PWM的行按此固定字母表顺序排列。\n\n使用的基本原理和定义：\n- 对于长度为 $L$ 的基序，其PWM $P \\in [0,1]^{L \\times 4}$ 是一个关于核苷酸的逐行概率分布，其中对于每个位置 $j \\in \\{1,\\dots,L\\}$，行 $P_{j,\\cdot}$ 满足 $\\sum_{a \\in \\{A,C,G,T\\}} P_{j,a} = 1$ 且 $P_{j,a} \\ge 0$。\n- 一个CNN滤波器由一个长度为 $L_f$ 的实值矩阵 $W \\in \\mathbb{R}^{L_f \\times 4}$ 给出；由此，通过带有温度 $\\tau > 0$ 的softmax变换在每个位置派生出概率分布。对于每个位置 $j$ 和核苷酸 $a$，定义 $$Q_{j,a} = \\frac{\\exp\\left(W_{j,a} / \\tau\\right)}{\\sum_{b \\in \\{A,C,G,T\\}} \\exp\\left(W_{j,b} / \\tau\\right)}.$$\n- 为避免当任何概率为零时出现未定义的对数，对每个PWM行独立应用带有参数 $\\alpha > 0$ 的Dirichlet式伪计数平滑：对于任何和为1的行向量 $R_{j,\\cdot}$，定义平滑后的行 $$\\widehat{R}_{j,a} = \\frac{R_{j,a} + \\alpha}{\\sum_{b \\in \\{A,C,G,T\\}} \\left(R_{j,b} + \\alpha\\right)} = \\frac{R_{j,a} + \\alpha}{1 + 4\\alpha}.$$\n- 两个在相同支撑集上的离散分布 $p$ 和 $q$ 之间的Kullback–Leibler散度为 $$D_{\\mathrm{KL}}(p \\,\\|\\, q) = \\sum_{a} p_a \\log\\left(\\frac{p_a}{q_a}\\right),$$ 所有对数均为自然对数（单位为奈特，nats）。对称Kullback–Leibler散度为 $$D_{\\mathrm{sym}}(p, q) = D_{\\mathrm{KL}}(p \\,\\|\\, q) + D_{\\mathrm{KL}}(q \\,\\|\\, p).$$\n- 对于一个PWM行 $p = P_{j,\\cdot}$，位置 $j$ 上的信息含量使用自然对数定义为 $$I_j = \\log(4) - H(p), \\quad\\text{其中}\\quad H(p) = -\\sum_{a} p_a \\log p_a.$$ 这使得 $I_j \\in [0, \\log(4)]$。\n\n跨位置聚合与对齐：\n- 如果已知PWM和从滤波器派生的PWM的长度不同，则通过将较短的PWM在较长的PWM内滑动到所有可能的偏移位置，并在每个对齐位置评估恢复度量，来进行对齐。设 $P$ 的长度为 $L_P$，$Q$ 的长度为 $L_Q$，并令 $S = \\min(L_P, L_Q)$ 表示将较短基序与较长基序对齐时的重叠长度。对于每个长度允许的偏移量 $s$，定义重叠位置的集合 $\\{0,\\dots,S-1\\}$，并计算相应平滑行之间的逐位置对称KLD。使用以下两种方案之一跨位置进行聚合：\n    1. 均匀平均：为 $j \\in \\{0,\\dots,S-1\\}$ 分配权重 $w_j = \\frac{1}{S}$，并计算 $$D_{\\mathrm{agg}}(s) = \\sum_{j=0}^{S-1} w_j \\, D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right),$$ 其中 $j'$ 和 $k'$ 是在偏移量 $s$ 和所选对齐方向下 $P$ 和 $Q$ 内的适当索引。\n    2. 信息含量加权：根据重叠区域内已知的PWM行分配权重，$w_j = \\frac{I_{j'}}{\\sum_{u=0}^{S-1} I_{u'}}$，约定如果 $\\sum I_{u'} = 0$ 则使用均匀权重。计算 $$D_{\\mathrm{agg}}(s) = \\sum_{j=0}^{S-1} w_j \\, D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right).$$\n- 将基序恢复度量定义为所有有效对齐中最小的聚合对称KLD：$$M = \\min_{s} D_{\\mathrm{agg}}(s).$$ 值越低表示恢复效果越好。结果以奈特（nats）为单位报告。\n\n实现要求：\n- 完全按照所述方式实现该度量，包括从滤波器到PWM的softmax转换、每行伪计数平滑、对称KLD、两种聚合模式，以及选择较短基序在较长基序上滑动的对齐方式。\n- 一致地使用固定的核苷酸顺序 $\\{A, C, G, T\\}$。\n- 所有对数必须是自然对数；结果以奈特（nats）为单位，作为十进制浮点数报告。在最后一行将最终输出四舍五入到六位小数。\n\n测试套件：\n实现您的程序，为以下四个测试用例中的每一个计算 $M$。不使用外部输入；测试用例嵌入在程序中。在所有情况下，字母表顺序为 $\\{A, C, G, T\\}$。\n\n测试用例1（正常路径，长度匹配，信息含量加权）：\n- 已知PWM $P^{(1)}$，长度 $L_P = 6$:\n$$\n\\begin{bmatrix}\n0.10  0.40  0.40  0.10 \\\\\n0.05  0.05  0.85  0.05 \\\\\n0.25  0.25  0.25  0.25 \\\\\n0.70  0.10  0.10  0.10 \\\\\n0.10  0.10  0.10  0.70 \\\\\n0.40  0.10  0.40  0.10\n\\end{bmatrix}\n$$\n- 滤波器权重 $W^{(1)}$，长度 $L_Q = 6$，在 $\\log(P^{(1)}_{j,\\cdot})$ 的基础上每行都添加了扰动：\n行1: $\\log(P^{(1)}_{1,\\cdot}) + [0.00, -0.10, +0.10, 0.00]$; 行2: $\\log(P^{(1)}_{2,\\cdot}) + [-0.05, +0.20, -0.15, 0.00]$; 行3: $\\log(P^{(1)}_{3,\\cdot}) + [+0.30, -0.10, -0.10, -0.10]$; 行4: $\\log(P^{(1)}_{4,\\cdot}) + [-0.20, +0.05, +0.05, +0.10]$; 行5: $\\log(P^{(1)}_{5,\\cdot}) + [0.00, 0.00, 0.00, 0.00]$; 行6: $\\log(P^{(1)}_{6,\\cdot}) + [+0.05, -0.05, +0.05, -0.05]$。\n- 温度 $\\tau = 1.0$，伪计数 $\\alpha = 10^{-4}$，聚合模式：信息含量加权。\n\n测试用例2（边界情况，精确匹配，均匀平均）：\n- 已知PWM $P^{(2)}$，长度 $L_P = 4$:\n$$\n\\begin{bmatrix}\n0.30  0.20  0.40  0.10 \\\\\n0.10  0.10  0.70  0.10 \\\\\n0.25  0.25  0.25  0.25 \\\\\n0.20  0.50  0.20  0.10\n\\end{bmatrix}\n$$\n- 滤波器权重 $W^{(2)}$，长度 $L_Q = 4$：对于每一行 $j$，设置 $W^{(2)}_{j,a} = \\log\\left(P^{(2)}_{j,a}\\right)$，使得从滤波器派生的PWM在softmax下与已知PWM完全匹配。\n- 温度 $\\tau = 1.0$，伪计数 $\\alpha = 10^{-9}$，聚合模式：均匀平均。\n\n测试用例3（长度不匹配，子基序比对，信息含量加权）：\n- 已知PWM $P^{(3)}$，长度 $L_P = 8$:\n$$\n\\begin{bmatrix}\n0.25  0.25  0.25  0.25 \\\\\n0.80  0.05  0.05  0.10 \\\\\n0.05  0.85  0.05  0.05 \\\\\n0.05  0.05  0.85  0.05 \\\\\n0.10  0.05  0.05  0.80 \\\\\n0.70  0.10  0.10  0.10 \\\\\n0.25  0.25  0.25  0.25 \\\\\n0.25  0.25  0.25  0.25\n\\end{bmatrix}\n$$\n- 滤波器权重 $W^{(3)}$，长度 $L_Q = 5$：对于与$P^{(3)}$中位置2到6相对应的行，设置 $W^{(3)}_{j,a} = \\log\\left(P^{(3)}_{j+1,a}\\right) + \\delta_{j,a}$，其中 $\\delta_{j,a}$ 是小的扰动，例如，对每一行 $j$ 使用 $[+0.01, -0.01, 0.00, 0.00]$。\n- 温度 $\\tau = 1.0$，伪计数 $\\alpha = 10^{-5}$，聚合模式：信息含量加权。\n\n测试用例4（带有零的边缘情况，均匀平均）：\n- 已知PWM $P^{(4)}$，长度 $L_P = 4$，具有确定性行：\n$$\n\\begin{bmatrix}\n1.00  0.00  0.00  0.00 \\\\\n0.00  1.00  0.00  0.00 \\\\\n0.00  0.00  1.00  0.00 \\\\\n0.00  0.00  0.00  1.00 \\\\\n\\end{bmatrix}\n$$\n- 滤波器权重 $W^{(4)}$，长度 $L_Q = 4$，通过大的logit值偏好不同的核苷酸：\n行1: $[-2.0, -2.0, -2.0, +2.0]$; 行2: $[+2.0, -2.0, -2.0, -2.0]$; 行3: $[-2.0, +2.0, -2.0, -2.0]$; 行4: $[-2.0, -2.0, +2.0, -2.0]$。\n- 温度 $\\tau = 1.0$，伪计数 $\\alpha = 10^{-3}$，聚合模式：均匀平均。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含四个结果 $M^{(1)}, M^{(2)}, M^{(3)}, M^{(4)}$，形式为一个用方括号括起来的逗号分隔列表，四舍五入到六位小数，例如 $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4\\right]$。单位是奈特（自然对数底）。不应打印任何其他文本。",
            "solution": "任务是构建一个定量的基序恢复度量，用于评估卷积神经网络（CNN）滤波器派生的基序与已知位置权重矩阵（PWM）的匹配程度。该度量植根于概率分布和信息论的原理，并随后用代码实现，并附带一个小型测试套件。推导过程从以下基本要素开始。\n\n第一性原理和核心定义：\n1. 一个位置权重矩阵（PWM）$P$由行 $P_{j,\\cdot}$ 构成，这些行是在核苷酸 $\\{A,C,G,T\\}$ 上的有效离散概率分布，即对于每个位置 $j$，有 $\\sum_{a} P_{j,a} = 1$ 且 $P_{j,a} \\ge 0$。\n2. 一个CNN滤波器 $W$ 是一个实值矩阵；为了将其与PWM进行比较，必须将其每一行映射为一个有效的分布。softmax函数是将实值logit值转换为概率的典型方法。给定温度 $\\tau > 0$，派生出的PWM $Q$ 为\n$$\nQ_{j,a} = \\frac{\\exp\\left(W_{j,a} / \\tau\\right)}{\\sum_{b \\in \\{A,C,G,T\\}} \\exp\\left(W_{j,b} / \\tau\\right)}.\n$$\n这遵循了概率的公理：对每个 $j$，$Q_{j,a} \\ge 0$ 且 $\\sum_{a} Q_{j,a} = 1$。\n\n3. 为确保数值稳定性并避免当某个概率分量为零时出现未定义的对数，对每一行应用由标量 $\\alpha > 0$ 参数化的Dirichlet式伪计数平滑。对于任何满足 $\\sum_{a} R_{j,a} = 1$ 的行分布 $R_{j,\\cdot}$，我们定义\n$$\n\\widehat{R}_{j,a} = \\frac{R_{j,a} + \\alpha}{1 + 4\\alpha},\n$$\n这保留了归一化性质并强制所有分量严格为正 $\\widehat{R}_{j,a} > 0$。\n\n4. 在相同支撑集上，分布 $p$ 和 $q$ 之间的Kullback–Leibler散度（使用自然对数）为\n$$\nD_{\\mathrm{KL}}(p \\,\\|\\, q) = \\sum_{a} p_a \\log\\left(\\frac{p_a}{q_a}\\right).\n$$\n其对称对应项定义为\n$$\nD_{\\mathrm{sym}}(p, q) = D_{\\mathrm{KL}}(p \\,\\|\\, q) + D_{\\mathrm{KL}}(q \\,\\|\\, p).\n$$\n这个对称版本非负，当且仅当 $p = q$ 时为零，并且从两个方向惩罚差异。\n\n5. 基序位置上的信息含量量化了与均匀分布的偏差。使用自然对数，令\n$$\nH(p) = -\\sum_{a} p_a \\log p_a, \\quad I_j = \\log(4) - H(P_{j,\\cdot}).\n$$\n这里，$\\log(4)$ 是四个等可能核苷酸的最大熵，因此 $I_j \\in [0, \\log(4)]$。\n\n跨位置聚合与对齐：\n一个长度为 $L_P$ 的基序和一个长度为 $L_Q$ 的滤波器派生PWM可能长度不同。为了比较，将较短的基序在较长的基序上滑动，并在每个有效的对齐位置计算一个聚合散度。设较短基序的长度为 $S$，较长基序的长度为 $L_{\\mathrm{long}}$。对于每个偏移量 $s \\in \\{0, 1, \\dots, L_{\\mathrm{long}} - S\\}$，通过重叠区域内的直接索引对应，在较短和较长的PWM之间定义一个行映射。对于重叠的行，计算\n$$\nD_{\\mathrm{agg}}(s) = \\sum_{j=0}^{S-1} w_j \\, D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right),\n$$\n其中 $(j',k')$ 表示在已知PWM和滤波器派生PWM中对齐的行，权重 $w_j$ 遵循以下两种方案之一：\n- 均匀平均：$w_j = \\frac{1}{S}$，反映所有位置同等重要。\n- 信息含量加权：$w_j = \\frac{I_{j'}}{\\sum_{u=0}^{S-1} I_{u'}}$，其中 $I_{j'}$ 是根据重叠区域内的已知PWM行计算的。如果 $\\sum I_{u'} = 0$（例如，所有行都是均匀分布），则使用 $w_j = \\frac{1}{S}$。\n\n恢复度量是所有有效偏移中最小的聚合散度：\n$$\nM = \\min_s D_{\\mathrm{agg}}(s).\n$$\n较低的 $M$ 值表示滤波器派生的PWM在其最佳对齐位置上与已知PWM更接近，这正是我们期望当滤波器捕获到该基序时所发生的情况。\n\n算法设计：\n1. 输入准备：构建已知的PWM矩阵 $P$ 和滤波器权重矩阵 $W$，以及标量参数 $\\tau$（softmax温度）、$\\alpha$（伪计数）和聚合模式选择（均匀或信息含量加权）。\n2. Softmax转换：对 $W$ 中的每一行 $j$，使用温度为 $\\tau$ 的softmax计算 $Q_{j,\\cdot}$，通过在求幂前减去行最大值以避免溢出，从而以数值稳定的方式实现。这将产生一个合法的PWM $Q$。\n3. 平滑：对 $P$ 和 $Q$，使用 $\\alpha$ 对每一行独立应用伪计数平滑，得到 $\\widehat{P}$ 和 $\\widehat{Q}$。\n4. 对齐处理：在 $\\widehat{P}$ 和 $\\widehat{Q}$ 中确定较长和较短者。将较短的在较长的上滑动所有有效偏移量 $s$，对于每个对齐，通过以下方式计算 $D_{\\mathrm{agg}}(s)$：\n   - 逐位置对称KLD：在每个重叠索引处计算 $D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right)$。\n   - 权重 $w_j$：可以是均匀的 $\\frac{1}{S}$，也可以是基于重叠区域内已知PWM行的信息含量 $I_{j'}$。在计算 $I_{j'}$ 时，使用平滑后的已知PWM以确保条目严格为正（这可以防止未定义的对数并保持度量的稳定性）。\n5. 最小化：选择 $M = \\min_s D_{\\mathrm{agg}}(s)$ 作为该案例的度量。\n6. 输出：生成一行，包含所有测试用例的度量，四舍五入到六位小数，单位为奈特。\n\n选择的理由：\n- 从CNN滤波器logit值到逐位置分布的softmax转换是机器学习中的标准做法，并确保了与PWM语义的兼容性。\n- 使用 $\\alpha$ 进行平滑对于数值稳定性是必要的，因为 $D_{\\mathrm{KL}}$ 需要严格为正的支撑集。选择一个小的 $\\alpha$ 可以保持原始分布占主导地位，同时避免未定义的对数。\n- 对称KLD对称地惩罚不匹配，适用于比较两个分布而无需偏袒某个散度方向。\n- 信息含量加权通过强调具有更高特异性（更低熵）的位置来反映生物学和统计学的相关性。当不打算或不需要这种加权时，均匀平均可作为基线。\n- 滑动对齐处理了滤波器捕获子基序或训练时使用的感受野大小与基序长度不同的现实情况。\n\n测试套件覆盖范围：\n- 测试用例1检验了带有中等扰动和信息含量加权的一般情况。\n- 测试用例2检查了完全相等的边界情况，期望结果接近于零。\n- 测试用例3测试了长度不匹配和对齐敏感性，其中包含高信息的子基序。\n- 测试用例4检验了包含零的确定性行，确认平滑处理能导致有限的散度和稳定的计算。\n\n该实现严格遵守自然对数；因此，所有报告的值都以奈特为单位。最终输出是一个用方括号括起来的逗号分隔列表，包含四个四舍五入到六位小数的浮点数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef softmax_rows(logits: np.ndarray, tau: float) -> np.ndarray:\n    \"\"\"\n    Compute row-wise softmax with temperature tau in a numerically stable way.\n    \"\"\"\n    # Subtract max per row for numerical stability.\n    z = logits / tau\n    z = z - np.max(z, axis=1, keepdims=True)\n    exp_z = np.exp(z)\n    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\ndef smooth_pwm(pwm: np.ndarray, alpha: float) -> np.ndarray:\n    \"\"\"\n    Apply Dirichlet-style pseudocount smoothing per row:\n    (p + alpha) / (1 + 4*alpha), assuming each row sums to 1.\n    \"\"\"\n    # Each row normalization denominator is 1.0 + 4.0 * alpha because sum(row) == 1.\n    denom = 1.0 + 4.0 * alpha\n    return (pwm + alpha) / denom\n\ndef symmetric_kl(p: np.ndarray, q: np.ndarray) -> float:\n    \"\"\"\n    Compute symmetric KL divergence between two distributions p and q\n    using natural logarithms. Assumes p,q > 0 and sum to 1.\n    \"\"\"\n    # Small epsilon guard (should not be necessary after smoothing,\n    # but we include minimal safeguard).\n    eps = 1e-300\n    p = np.clip(p, eps, 1.0)\n    q = np.clip(q, eps, 1.0)\n    d1 = np.sum(p * np.log(p / q))\n    d2 = np.sum(q * np.log(q / p))\n    return float(d1 + d2)\n\ndef information_content_rows(pwm: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute information content per row: I_j = log(4) - H(p_j),\n    where H(p) = -sum p log p with natural logarithms.\n    \"\"\"\n    # Ensure strictly positive entries to avoid log(0)\n    eps = 1e-300\n    p = np.clip(p, eps, 1.0)\n    entropy = -np.sum(p * np.log(p), axis=1)  # nats\n    I = np.log(4.0) - entropy\n    return I\n\ndef aggregate_divergence(P_hat: np.ndarray, Q_hat: np.ndarray,\n                         mode: str, P_for_weights: np.ndarray) -> float:\n    \"\"\"\n    Align shorter across longer and compute minimum aggregated symmetric KL divergence.\n    mode: 'uniform' or 'ic' (information-content weighting using P_for_weights).\n    P_for_weights: PWM to compute information content for weights (use smoothed known PWM).\n    \"\"\"\n    len_P = P_hat.shape[0]\n    len_Q = Q_hat.shape[0]\n    # Determine which to slide: slide the shorter across the longer\n    if len_P <= len_Q:\n        shorter = P_hat\n        longer = Q_hat\n        slide_P_over_Q = True\n    else:\n        shorter = Q_hat\n        longer = P_hat\n        slide_P_over_Q = False\n\n    S = shorter.shape[0]\n    L = longer.shape[0]\n    best = np.inf\n    # Precompute info content of known PWM rows as needed\n    info_weights = information_content_rows(P_for_weights)\n    for s in range(L - S + 1):\n        # Build weights for this overlapped region\n        if mode == 'uniform':\n            w = np.ones(S, dtype=float) / float(S)\n        elif mode == 'ic':\n            # Use information content from the known PWM rows in the overlap\n            if slide_P_over_Q:\n                # P is shorter and is the known PWM. Overlap indices in P are 0..S-1.\n                I = info_weights\n            else:\n                # Q is shorter. P is longer and is the known PWM.\n                # Overlap indices in P are s..s+S-1.\n                I = info_weights[s:s+S]\n            total_I = np.sum(I)\n            if total_I == 0.0:\n                w = np.ones(S, dtype=float) / float(S)\n            else:\n                w = I / total_I\n        else:\n            raise ValueError(\"Unknown aggregation mode\")\n\n        # Compute weighted symmetric KL over the overlap at offset s\n        total = 0.0\n        for j in range(S):\n            if slide_P_over_Q: # P is shorter, Q is longer\n                p_row = shorter[j]    # Row from P_hat\n                q_row = longer[s + j] # Row from Q_hat\n            else: # Q is shorter, P is longer\n                p_row = longer[s + j] # Row from P_hat\n                q_row = shorter[j]    # Row from Q_hat\n            d_sym = symmetric_kl(p_row, q_row)\n            total += w[j] * d_sym\n        if total < best:\n            best = total\n    return float(best)\n\ndef motif_recovery_metric(P_known: np.ndarray, W_filter: np.ndarray,\n                          tau: float, alpha: float, agg_mode: str) -> float:\n    \"\"\"\n    Compute the motif recovery metric M between known PWM P_known and filter weights W_filter.\n    \"\"\"\n    Q_filter = softmax_rows(W_filter, tau)\n    # Smooth both PWMs\n    P_hat = smooth_pwm(P_known, alpha)\n    Q_hat = smooth_pwm(Q_filter, alpha)\n    # For information-content weighting, use the smoothed known PWM\n    M = aggregate_divergence(P_hat, Q_hat, agg_mode, P_hat)\n    return M\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Test case 1\n    P1 = np.array([\n        [0.10, 0.40, 0.40, 0.10],\n        [0.05, 0.05, 0.85, 0.05],\n        [0.25, 0.25, 0.25, 0.25],\n        [0.70, 0.10, 0.10, 0.10],\n        [0.10, 0.10, 0.10, 0.70],\n        [0.40, 0.10, 0.40, 0.10],\n    ], dtype=float)\n    # Build W1 as log(P1) plus perturbations per row\n    logP1 = np.log(P1)\n    perturbations1 = np.array([\n        [0.00, -0.10, +0.10, 0.00],\n        [-0.05, +0.20, -0.15, 0.00],\n        [+0.30, -0.10, -0.10, -0.10],\n        [-0.20, +0.05, +0.05, +0.10],\n        [0.00, 0.00, 0.00, 0.00],\n        [+0.05, -0.05, +0.05, -0.05],\n    ], dtype=float)\n    W1 = logP1 + perturbations1\n    tau1 = 1.0\n    alpha1 = 1e-4\n    agg1 = 'ic'\n\n    # Test case 2\n    P2 = np.array([\n        [0.30, 0.20, 0.40, 0.10],\n        [0.10, 0.10, 0.70, 0.10],\n        [0.25, 0.25, 0.25, 0.25],\n        [0.20, 0.50, 0.20, 0.10],\n    ], dtype=float)\n    W2 = np.log(P2)  # exact match under softmax\n    tau2 = 1.0\n    alpha2 = 1e-9\n    agg2 = 'uniform'\n\n    # Test case 3\n    P3 = np.array([\n        [0.25, 0.25, 0.25, 0.25],\n        [0.80, 0.05, 0.05, 0.10],\n        [0.05, 0.85, 0.05, 0.05],\n        [0.05, 0.05, 0.85, 0.05],\n        [0.10, 0.05, 0.05, 0.80],\n        [0.70, 0.10, 0.10, 0.10],\n        [0.25, 0.25, 0.25, 0.25],\n        [0.25, 0.25, 0.25, 0.25],\n    ], dtype=float)\n    # Filter corresponds to positions 2..6 of P3 with slight perturbations\n    logP3 = np.log(P3)\n    W3_core = logP3[1:6, :]  # rows 2..6\n    perturbations3 = np.array([\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n    ], dtype=float)\n    W3 = W3_core + perturbations3\n    tau3 = 1.0\n    alpha3 = 1e-5\n    agg3 = 'ic'\n\n    # Test case 4\n    P4 = np.array([\n        [1.00, 0.00, 0.00, 0.00],\n        [0.00, 1.00, 0.00, 0.00],\n        [0.00, 0.00, 1.00, 0.00],\n        [0.00, 0.00, 0.00, 1.00],\n    ], dtype=float)\n    W4 = np.array([\n        [-2.0, -2.0, -2.0, +2.0],  # favors T\n        [+2.0, -2.0, -2.0, -2.0],  # favors A\n        [-2.0, +2.0, -2.0, -2.0],  # favors C\n        [-2.0, -2.0, +2.0, -2.0],  # favors G\n    ], dtype=float)\n    tau4 = 1.0\n    alpha4 = 1e-3\n    agg4 = 'uniform'\n\n    test_cases = [\n        (P1, W1, tau1, alpha1, agg1),\n        (P2, W2, tau2, alpha2, agg2),\n        (P3, W3, tau3, alpha3, agg3),\n        (P4, W4, tau4, alpha4, agg4),\n    ]\n\n    results = []\n    for P_known, W_filter, tau, alpha, agg_mode in test_cases:\n        M = motif_recovery_metric(P_known, W_filter, tau, alpha, agg_mode)\n        # Round to 6 decimal places\n        results.append(f\"{M:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}