{
    "hands_on_practices": [
        {
            "introduction": "The journey of a CRISPR-Cas system begins with the transcription and processing of its guide RNA. This first practice invites you to model this crucial biogenesis step using the principles of mass-action kinetics. By deriving and solving a system of ordinary differential equations, you will determine the steady-state concentrations of precursor and mature crRNA, providing a quantitative foundation for understanding how the cell maintains its arsenal of guide molecules .",
            "id": "3298925",
            "problem": "In a prokaryotic Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)-CRISPR-associated (Cas) locus, precursor CRISPR RNA (pre-CRISPR RNA, pre-crRNA) is transcribed by RNA polymerase and subsequently processed by a Cas endoribonuclease into mature CRISPR RNA (crRNA). Consider a single transcriptional unit in a well-mixed, constant-volume cell, operating in the linear, unsaturated regime for processing. Assume the following:\n\n- Transcription of pre-CRISPR RNA occurs at a constant zero-order rate $k_{t}$ (e.g., due to a constitutive promoter).\n- Processing of pre-CRISPR RNA into crRNA is pseudo-first-order with respect to pre-CRISPR RNA, with effective rate constant $k_{p}$, reflecting excess and constant Cas processing activity.\n- Both pre-CRISPR RNA and crRNA are removed by first-order processes with the same rate constant $k_{d}$ (e.g., ribonuclease degradation and dilution by growth).\n- Each processing event yields exactly one crRNA molecule from one pre-CRISPR RNA molecule, with no branching or reverse reactions.\n- There is no feedback of crRNA on transcription or processing, and no other sources or sinks.\n\nUsing mass-action kinetics and the above assumptions as the only modeling base, derive the ordinary differential equations governing the temporal dynamics of the pre-CRISPR RNA concentration $P(t)$ and the crRNA concentration $C(t)$. Then, compute the unique steady-state concentrations $(P^{*}, C^{*})$ in closed form as functions of $k_{t}$, $k_{p}$, and $k_{d}$. Express your final answer symbolically in terms of $k_{t}$, $k_{p}$, and $k_{d}$, without substituting numerical values. If you were to assign units, take $k_{t}$ in $\\text{concentration}\\cdot\\text{time}^{-1}$ and $k_{p}, k_{d}$ in $\\text{time}^{-1}$, so that $P^{*}$ and $C^{*}$ have units of concentration. Provide the steady-state pair as a single row vector.",
            "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- **System**: A single transcriptional unit in a prokaryotic CRISPR-Cas locus.\n- **Components**: Precursor CRISPR RNA (pre-crRNA), denoted by $P(t)$, and mature CRISPR RNA (crRNA), denoted by $C(t)$.\n- **Process 1 (Transcription)**: Production of pre-CRISPR RNA $P$ occurs at a constant zero-order rate $k_t$.\n- **Process 2 (Processing)**: Conversion of pre-CRISPR RNA $P$ into crRNA $C$ is a pseudo-first-order reaction with respect to $P$, with an effective rate constant $k_p$.\n- **Process 3 (Removal)**: Both $P$ and $C$ are removed through first-order processes (degradation and dilution) with the same rate constant $k_d$.\n- **Stoichiometry**: Each processing event converts one molecule of $P$ into one molecule of $C$ ($P \\to C$).\n- **Assumptions**: The system is well-mixed, has constant volume, operates in the linear/unsaturated regime for processing, has no feedback, no reverse reactions, and no other sources or sinks for $P$ and $C$.\n- **Objective**:\n    1.  Derive the ordinary differential equations (ODEs) for the concentrations $P(t)$ and $C(t)$.\n    2.  Compute the unique steady-state concentrations $(P^*, C^*)$ as functions of the parameters $k_t$, $k_p$, and $k_d$.\n- **Required Format**: Final answer as a symbolic row vector.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem describes a simplified kinetic model of RNA biogenesis, a core process in molecular and systems biology. The use of zero-order production (for constitutive gene expression), first-order degradation, and pseudo-first-order processing (assuming an enzyme is in excess and not saturated) are standard and valid simplifying assumptions for constructing mathematical models in this field. The biological context, CRISPR RNA processing, is real and accurately depicted in this simplified framework.\n2.  **Well-Posed**: The problem is well-posed. It provides a complete set of reactions and corresponding rate laws, defines all parameters ($k_t, k_p, k_d$), and specifies a clear, achievable objective (deriving ODEs and finding the steady-state solution). The resulting system of linear ODEs is guaranteed to have a unique and stable steady-state solution, given that the physical rate constants are positive.\n3.  **Objective**: The problem is stated in precise, objective language. The terms \"zero-order\", \"first-order\", and \"mass-action kinetics\" have unambiguous meanings in chemical kinetics and systems biology.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a standard exercise in mathematical modeling of biochemical networks.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be derived.\n\n### Solution Derivation\nThe temporal dynamics of the concentrations of pre-CRISPR RNA, $P(t)$, and mature crRNA, $C(t)$, are described by a system of ordinary differential equations. These equations are formulated based on the principle of mass-action kinetics, where the rate of change of a species' concentration is the sum of the rates of all reactions producing it minus the sum of the rates of all reactions consuming it.\n\n**1. Derivation of the ODE for Pre-CRISPR RNA, $P(t)$**\nThe concentration of pre-CRISPR RNA, $P$, changes due to three processes:\n- **Production**: $P$ is transcribed at a constant, zero-order rate $k_t$. The rate of this process is simply $k_t$. This contributes a positive term to the rate of change of $P$.\n- **Processing**: $P$ is processed into crRNA, $C$. This is a pseudo-first-order reaction with respect to $P$, so its rate is $k_p P(t)$. This is a consumption pathway for $P$, contributing a negative term.\n- **Removal**: $P$ is removed (degraded/diluted) via a first-order process. The rate of this reaction is $k_d P(t)$. This is another consumption pathway for $P$, contributing a negative term.\n\nCombining these terms gives the ODE for $P(t)$:\n$$\n\\frac{dP}{dt} = (\\text{rate of production}) - (\\text{rate of processing}) - (\\text{rate of removal})\n$$\n$$\n\\frac{dP}{dt} = k_t - k_p P(t) - k_d P(t)\n$$\nThis can be simplified to:\n$$\n\\frac{dP}{dt} = k_t - (k_p + k_d)P(t)\n$$\n\n**2. Derivation of the ODE for crRNA, $C(t)$**\nThe concentration of mature crRNA, $C$, changes due to two processes:\n- **Production**: $C$ is produced from the processing of $P$. Since the stoichiometry is $1:1$, the rate of production of $C$ is equal to the rate of processing of $P$, which is $k_p P(t)$. This contributes a positive term to the rate of change of $C$.\n- **Removal**: $C$ is removed via a first-order process with rate constant $k_d$. The rate of this reaction is $k_d C(t)$. This is a consumption pathway for $C$, contributing a negative term.\n\nCombining these terms gives the ODE for $C(t)$:\n$$\n\\frac{dC}{dt} = (\\text{rate of production}) - (\\text{rate of removal})\n$$\n$$\n\\frac{dC}{dt} = k_p P(t) - k_d C(t)\n$$\n\n**3. Calculation of Steady-State Concentrations**\nThe steady state of the system is defined by the condition where the concentrations of all species are no longer changing over time. Mathematically, this corresponds to setting all time derivatives to zero:\n$$\n\\frac{dP}{dt} = 0 \\quad \\text{and} \\quad \\frac{dC}{dt} = 0\n$$\nLet $P^*$ and $C^*$ denote the steady-state concentrations. The ODEs become a system of algebraic equations:\n1. $0 = k_t - (k_p + k_d)P^*$\n2. $0 = k_p P^* - k_d C^*$\n\nWe can solve this system sequentially. First, we solve Equation 1 for $P^*$:\n$$\n(k_p + k_d)P^* = k_t\n$$\n$$\nP^* = \\frac{k_t}{k_p + k_d}\n$$\nNext, we solve Equation 2 for $C^*$, using the expression for $P^*$ we just found:\n$$\nk_d C^* = k_p P^*\n$$\n$$\nC^* = \\frac{k_p}{k_d} P^*\n$$\nSubstituting the expression for $P^*$:\n$$\nC^* = \\frac{k_p}{k_d} \\left( \\frac{k_t}{k_p + k_d} \\right)\n$$\n$$\nC^* = \\frac{k_p k_t}{k_d (k_p + k_d)}\n$$\n\nThe steady-state concentrations are therefore $(P^*, C^*) = \\left(\\frac{k_t}{k_p + k_d}, \\frac{k_p k_t}{k_d(k_p + k_d)}\\right)$. These expressions are physically meaningful, as all rate constants ($k_t, k_p, k_d$) are inherently positive, ensuring that the concentrations $P^*$ and $C^*$ are also positive.\n\nThe final answer is the pair of steady-state concentrations presented as a row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{k_{t}}{k_{p} + k_{d}}  \\frac{k_{p} k_{t}}{k_{d}(k_{p} + k_{d})}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Once a CRISPR-Cas effector is assembled, its next challenge is to efficiently locate a short target sequence within a vast genome. This exercise explores the biophysical model of facilitated diffusion, a process combining 3D exploration with 1D sliding along DNA . You will analyze a key trade-off between search efficiency and non-specific binding, revealing the non-intuitive principle that being too \"sticky\" can be just as detrimental as not being sticky enough.",
            "id": "3298945",
            "problem": "A Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)–CRISPR-associated (Cas) effector searches for its target on a bacterial chromosome by alternating between three-dimensional diffusion in the cytoplasm and one-dimensional sliding along non-specific DNA. Consider a two-state kinetic model in which the effector alternates between a free state and a non-specifically bound state. The free-to-bound transition is a Poisson process with rate constant $k_a$ (effective per-second association to any non-specific DNA segment), and the bound-to-free transition is a Poisson process with rate constant $k_d$ (dissociation rate). While bound, the effector performs one-dimensional diffusion with coefficient $D_1$ and scans an average segment of DNA of length $\\ell$ (in base pairs) per bound bout. The genome accessible length is $L$ (in base pairs), and the target is assumed to be unique and uniformly distributed along the accessible DNA.\n\nUse the following fundamental bases:\n- Exponential waiting times for Poisson processes have mean equal to the inverse of the rate, so the mean bound time is $1/k_d$ and the mean free time is $1/k_a$.\n- For one-dimensional diffusion over a bound interval of mean duration $1/k_d$, the root-mean-square displacement scales as $\\sqrt{2 D_1/k_d}$, and thus the mean scanned length scales as $\\ell \\propto \\sqrt{1/k_d}$; consequently, under a change from $k_d$ to $k_d'$, the scanned length transforms as $\\ell' = \\ell \\sqrt{k_d/k_d'}$.\n- Under independence of bouts and uniform target placement, the expected number of scanning bouts required to encounter the target is $L/\\ell$.\n\nDerive an expression for the mean search time $T$ in terms of $k_a$, $k_d$, $\\ell$, and $L$, then compute the fold-change in search time $F = T'/T$ when the non-specific binding affinity is doubled by halving the dissociation rate (i.e., $k_d' = k_d/2$) while holding $k_a$ fixed. Use the scaling $\\ell' = \\ell \\sqrt{k_d/k_d'}$ to account for the change in the scanned length. Finally, evaluate $F$ numerically for the parameter values $k_a = 2.0 \\times 10^{2}~\\text{s}^{-1}$, $k_d = 1.0 \\times 10^{-1}~\\text{s}^{-1}$, $\\ell = 1.0 \\times 10^{4}~\\text{bp}$, and $L = 4.0 \\times 10^{6}~\\text{bp}$. Express the fold-change as a dimensionless decimal number and round your final answer to $3$ significant figures. Briefly interpret the tradeoff between search efficiency and sequestration implied by these parameter values, explicitly stating why the fold-change increases or decreases under the affinity doubling.",
            "solution": "The problem statement is analyzed and found to be valid. It is scientifically grounded in the principles of biophysical modeling of protein-DNA interactions (facilitated diffusion), well-posed with all necessary parameters and relationships, and objective in its language. The problem is self-contained, consistent, and solvable as posed.\n\nThe first step is to derive an expression for the mean search time, $T$. The search process is modeled as a sequence of cycles, where each cycle consists of a period of three-dimensional diffusion in the cytoplasm (the \"free\" state) followed by a period of one-dimensional sliding on non-specific DNA (the \"bound\" state).\n\nThe mean time spent in the free state, $\\tau_f$, is the inverse of the association rate constant, $k_a$:\n$$ \\tau_f = \\frac{1}{k_a} $$\nThe mean time spent in the bound state, $\\tau_b$, is the inverse of the dissociation rate constant, $k_d$:\n$$ \\tau_b = \\frac{1}{k_d} $$\nThe total mean duration of a single search cycle (one free phase plus one bound phase) is $\\tau_{cycle}$:\n$$ \\tau_{cycle} = \\tau_f + \\tau_b = \\frac{1}{k_a} + \\frac{1}{k_d} $$\nThe problem states that the expected number of scanning bouts, $N_{bouts}$, required to encounter the unique target is the ratio of the total accessible genome length, $L$, to the mean segment length scanned per bout, $\\ell$:\n$$ N_{bouts} = \\frac{L}{\\ell} $$\nThe total mean search time, $T$, is the product of the expected number of bouts and the mean duration of each cycle. This is a valid approximation when $N_{bouts} \\gg 1$.\n$$ T = N_{bouts} \\times \\tau_{cycle} = \\frac{L}{\\ell} \\left( \\frac{1}{k_a} + \\frac{1}{k_d} \\right) $$\n\nNext, we must compute the fold-change in search time, $F = T'/T$, under a specific perturbation. The non-specific binding affinity is doubled by halving the dissociation rate, while the association rate is held constant. The new parameters are:\n$$ k_a' = k_a $$\n$$ k_d' = \\frac{k_d}{2} $$\nThe change in dissociation rate affects the mean scanned length per bout. According to the provided scaling relationship:\n$$ \\ell' = \\ell \\sqrt{\\frac{k_d}{k_d'}} = \\ell \\sqrt{\\frac{k_d}{k_d/2}} = \\ell \\sqrt{2} $$\nThe new search time, $T'$, is given by the same general formula but with the new parameters:\n$$ T' = \\frac{L}{\\ell'} \\left( \\frac{1}{k_a'} + \\frac{1}{k_d'} \\right) = \\frac{L}{\\ell\\sqrt{2}} \\left( \\frac{1}{k_a} + \\frac{1}{k_d/2} \\right) = \\frac{L}{\\ell\\sqrt{2}} \\left( \\frac{1}{k_a} + \\frac{2}{k_d} \\right) $$\nThe fold-change $F$ is the ratio $T'/T$:\n$$ F = \\frac{T'}{T} = \\frac{\\frac{L}{\\ell\\sqrt{2}} \\left( \\frac{1}{k_a} + \\frac{2}{k_d} \\right)}{\\frac{L}{\\ell} \\left( \\frac{1}{k_a} + \\frac{1}{k_d} \\right)} $$\nThe term $L/\\ell$ cancels out, leaving:\n$$ F = \\frac{1}{\\sqrt{2}} \\frac{\\frac{1}{k_a} + \\frac{2}{k_d}}{\\frac{1}{k_a} + \\frac{1}{k_d}} $$\nThis can be written with a common denominator as:\n$$ F = \\frac{1}{\\sqrt{2}} \\frac{\\frac{k_d + 2k_a}{k_a k_d}}{\\frac{k_d + k_a}{k_a k_d}} = \\frac{1}{\\sqrt{2}} \\frac{k_d + 2k_a}{k_d + k_a} $$\n\nNow, we evaluate $F$ numerically using the given parameter values:\n$k_a = 2.0 \\times 10^{2}~\\text{s}^{-1} = 200~\\text{s}^{-1}$\n$k_d = 1.0 \\times 10^{-1}~\\text{s}^{-1} = 0.1~\\text{s}^{-1}$\n\nSubstituting these values into the expression for $F$:\n$$ F = \\frac{1}{\\sqrt{2}} \\frac{0.1 + 2(200)}{0.1 + 200} = \\frac{1}{\\sqrt{2}} \\frac{0.1 + 400}{200.1} = \\frac{1}{\\sqrt{2}} \\frac{400.1}{200.1} $$\nFirst, we compute the ratio:\n$$ \\frac{400.1}{200.1} \\approx 1.99950025 $$\nThen, we divide by $\\sqrt{2}$:\n$$ F \\approx \\frac{1.99950025}{\\sqrt{2}} \\approx \\frac{1.99950025}{1.41421356} \\approx 1.4137644 $$\nRounding to $3$ significant figures, we get $F = 1.41$.\n\nFinally, we interpret the result. A fold-change of $F \\approx 1.41$ indicates that doubling the non-specific binding affinity increases the mean search time by approximately $41\\%$. This reveals a fundamental tradeoff in the search process.\nOn one hand, increasing the binding affinity (by decreasing $k_d$) makes each one-dimensional sliding bout more productive. The effector remains on the DNA longer, scanning a greater length $\\ell' = \\ell\\sqrt{2}$. This reduces the total number of binding/unbinding cycles needed to cover the entire genome by a factor of $1/\\sqrt{2}$. This effect, in isolation, would speed up the search.\nOn the other hand, the time spent in each binding event, $\\tau_b = 1/k_d$, doubles. With the given parameters, the initial mean free time is $\\tau_f = 1/k_a = 1/200~\\text{s} = 0.005~\\text{s}$, while the initial mean bound time is $\\tau_b = 1/k_d = 1/0.1~\\text{s} = 10~\\text{s}$. The search is overwhelmingly dominated by the time spent non-specifically bound ($\\tau_b \\gg \\tau_f$). Doubling the affinity causes the new bound time to become $\\tau_b' = 20~\\text{s}$. The benefit of needing fewer cycles (a factor of $1/\\sqrt{2} \\approx 0.707$) is outweighed by the cost of each cycle taking roughly twice as long. The increase in sequestration time on non-specific DNA is the dominant effect, leading to a net slowdown in the overall search. This illustrates that for a given system, there is an optimal non-specific binding affinity that minimizes search time; being too \"sticky\" is as detrimental as not being sticky enough. For these parameters, the system becomes overly sequestered on non-target DNA, hindering efficient target location.\nThe overall effect can be approximated by noting that since $\\tau_f \\ll \\tau_b$, the time per cycle is $\\tau_{cycle} \\approx \\tau_b$. The total search time is $T \\approx (L/\\ell) \\tau_b$. The new time is $T' \\approx (L/\\ell') \\tau_b' = (L/(\\ell\\sqrt{2})) (2\\tau_b) = (L/\\ell)\\tau_b \\times (2/\\sqrt{2}) = T\\sqrt{2}$. The fold-change is thus approximately $\\sqrt{2} \\approx 1.414$, which matches our precise calculation.",
            "answer": "$$\\boxed{1.41}$$"
        },
        {
            "introduction": "Moving from understanding to engineering, the effective use of CRISPR as a tool depends on designing guide RNAs that are both potent and specific. This final practice places you in the role of a designer, tasked with balancing the competing objectives of maximizing on-target activity and minimizing off-target risk . By calculating guide performance metrics and identifying the Pareto-optimal set, you will learn to navigate the complex trade-offs that define real-world therapeutic and synthetic biology design.",
            "id": "3298966",
            "problem": "Consider modeling endonuclease activity of Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) and CRISPR-associated (Cas) ribonucleoproteins acting on a genomic locus. A single guide ribonucleic acid (sgRNA) of length $20$ nucleotides recognizes a genomic protospacer adjacent to a Protospacer Adjacent Motif (PAM). We model guide performance using the following foundational bases:\n\n- A binding-and-cleavage propensity that is monotone with respect to an additive sequence-derived score, transformed into a probability by a logistic mapping. Concretely, there exists an internal latent score $S$ such that the probability of cleavage at a site is $p = \\sigma(S)$, where the logistic function is $\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$. This reflects a coarse-grained view of multistep recognition as a soft-threshold process.\n- For an on-target site, the latent score $S_{\\mathrm{on}}$ is approximated to first order by a linear combination of guide features: $S_{\\mathrm{on}} = w_0 + w_{\\mathrm{GC}}\\,x_{\\mathrm{GC}} + w_{\\mathrm{PAM}}\\,x_{\\mathrm{PAM}} + w_{\\mathrm{H}}\\,x_{\\mathrm{H}}$, where $x_{\\mathrm{GC}} \\in [0,1]$ is the guanine-cytosine fraction of the guide, $x_{\\mathrm{PAM}} \\in \\{0,1\\}$ indicates a canonical PAM context, and $x_{\\mathrm{H}}$ is the length of the longest homopolymer run in the guide. Signs and magnitudes of coefficients encode empirically observed trends such as decreased activity with long homopolymers.\n- For an off-target site that differs from the on-target at certain protospacer positions, mismatches reduce the score additively by position-specific penalties. If $\\mathcal{M} \\subset \\{1,2,\\dots,20\\}$ is the set of mismatch positions, and $\\delta_i \\ge 0$ is the penalty for a mismatch at position $i$, then the off-target latent score is $S_{\\mathrm{off}} = S_{\\mathrm{on}} - \\sum_{i \\in \\mathcal{M}} \\delta_i$, and the off-target cleavage probability is $p_{\\mathrm{off}} = \\sigma(S_{\\mathrm{off}})$. This implements a position-dependent \"seed\" sensitivity without prescribing a specific biophysical micro-model beyond additivity to first order.\n- The total off-target risk for a guide is the sum of expected cleavage probabilities across its enumerated candidate off-target sites: $R = \\sum_{j=1}^{N_{\\mathrm{off}}} p_{\\mathrm{off},j}$, where $N_{\\mathrm{off}}$ is the number of off-target sites considered for that guide.\n- A composite utility score that trades off benefit (on-target cleavage) against cost (aggregate off-target cleavage) is $U = p_{\\mathrm{on}} - c \\, R$, where $c \\ge 0$ is a supplied trade-off coefficient quantifying the relative penalty of off-target cleavage compared to on-target cleavage.\n\nYour task is to write a program that, for a set of test cases, computes the composite utility scores $U$ for each candidate guide, and selects the Pareto-optimal set of guides under the bi-objective dominance relation that maximizes $p_{\\mathrm{on}}$ and minimizes $R$. A guide $a$ dominates guide $b$ if and only if $p_{\\mathrm{on},a} \\ge p_{\\mathrm{on},b}$ and $R_a \\le R_b$, with at least one strict inequality. The Pareto-optimal set contains all guides not dominated by any other.\n\nYour program must implement the above principles using the exact parameter values below as a test suite. All numbers that appear here are dimensionless.\n\nTest Suite Parameters:\n\n- Case $1$:\n  - Linear coefficients: $w_0 = -1.0$, $w_{\\mathrm{GC}} = 2.5$, $w_{\\mathrm{PAM}} = 1.0$, $w_{\\mathrm{H}} = -0.3$.\n  - Positional mismatch penalties $(\\delta_i)_{i=1}^{20}$: positions $1$ to $5$: $1.6$ each; positions $6$ to $10$: $1.0$ each; positions $11$ to $15$: $0.6$ each; positions $16$ to $20$: $0.4$ each.\n  - Trade-off coefficient: $c = 0.5$.\n  - Guides (each item is $(x_{\\mathrm{GC}}, x_{\\mathrm{PAM}}, x_{\\mathrm{H}}, \\{\\text{off-target mismatches}\\})$), with mismatch positions listed as $1$-based indices within $\\{1,\\dots,20\\}$:\n    - Guide $0$: $(0.45, 1, 2, \\{\\{3,12\\}, \\{5\\}\\})$.\n    - Guide $1$: $(0.60, 1, 3, \\{\\{1,2\\}, \\{15,16,17\\}\\})$.\n    - Guide $2$: $(0.55, 0, 1, \\{\\{7\\}\\})$.\n    - Guide $3$: $(0.35, 1, 4, \\{\\{10,11\\}, \\{2\\}, \\{19\\}\\})$.\n- Case $2$:\n  - Linear coefficients: $w_0 = -1.0$, $w_{\\mathrm{GC}} = 2.5$, $w_{\\mathrm{PAM}} = 1.0$, $w_{\\mathrm{H}} = -0.3$.\n  - Positional mismatch penalties $(\\delta_i)_{i=1}^{20}$: positions $1$ to $5$: $1.6$ each; positions $6$ to $10$: $1.0$ each; positions $11$ to $15$: $0.6$ each; positions $16$ to $20$: $0.4$ each.\n  - Trade-off coefficient: $c = 0.2$.\n  - Guides:\n    - Guide $0$: $(0.50, 1, 1, \\{\\})$.\n    - Guide $1$: $(0.65, 1, 2, \\{\\}\\,)$.\n    - Guide $2$: $(0.40, 0, 1, \\{\\{1,5,10\\}\\})$.\n- Case $3$:\n  - Linear coefficients: $w_0 = -1.0$, $w_{\\mathrm{GC}} = 2.5$, $w_{\\mathrm{PAM}} = 1.0$, $w_{\\mathrm{H}} = -0.3$.\n  - Positional mismatch penalties $(\\delta_i)_{i=1}^{20}$: positions $1$ to $5$: $1.6$ each; positions $6$ to $10$: $1.0$ each; positions $11$ to $15$: $0.6$ each; positions $16$ to $20$: $0.4$ each.\n  - Trade-off coefficient: $c = 0.8$.\n  - Guides:\n    - Guide $0$: $(0.62, 1, 2, \\{\\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{4,5\\}\\})$.\n    - Guide $1$: $(0.50, 1, 2, \\{\\{18,19,20\\}\\})$.\n    - Guide $2$: $(0.48, 0, 1, \\{\\{12\\}, \\{13\\}\\})$.\n    - Guide $3$: $(0.36, 1, 1, \\{\\{19\\}\\})$.\n\nImplementation requirements:\n\n- For each guide in each case, compute $S_{\\mathrm{on}}$, $p_{\\mathrm{on}} = \\sigma(S_{\\mathrm{on}})$, off-target probabilities for each mismatch set $\\mathcal{M}$ via $p_{\\mathrm{off}} = \\sigma\\!\\left(S_{\\mathrm{on}} - \\sum_{i \\in \\mathcal{M}} \\delta_i\\right)$, the total risk $R = \\sum p_{\\mathrm{off}}$, and the composite utility $U = p_{\\mathrm{on}} - c\\,R$.\n- Determine the Pareto-optimal set by the dominance rule above, using $p_{\\mathrm{on}}$ to be maximized and $R$ to be minimized. Report the set as $0$-based indices in ascending order.\n- Numerical outputs must be floats for $U$ and integers for indices. Round each $U$ to $6$ decimal places before reporting.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results aggregated across cases as a comma-separated list enclosed in square brackets, where for each case you output two elements in order: a list of rounded composite utilities for all guides in that case, and a list of Pareto-optimal guide indices. The overall output therefore has the form $[\\,[U^{(1)}_0,\\dots,U^{(1)}_{n_1-1}],\\,[I^{(1)}_0,\\dots],\\,[U^{(2)}_0,\\dots],\\,[I^{(2)}_0,\\dots],\\,[U^{(3)}_0,\\dots],\\,[I^{(3)}_0,\\dots]\\,]$ with no additional text.",
            "solution": "The problem is valid as it is scientifically grounded in established, albeit simplified, principles of CRISPR-Cas modeling, is mathematically well-posed, and provides a complete and consistent set of data and definitions for a computational solution.\n\nThe task is to compute a composite utility score, $U$, for several candidate single guide RNAs (sgRNAs) and to identify the Pareto-optimal set of guides based on on-target activity and off-target risk. The solution proceeds by first implementing the provided mathematical model and then applying a standard algorithm for finding a Pareto front.\n\n**1. Mathematical Model Implementation**\n\nThe core of the model is based on a latent score, $S$, which is transformed into a cleavage probability, $p$, using the logistic sigmoid function, $\\sigma(x) = \\frac{1}{1 + e^{-x}}$.\n\nFor each guide, characterized by its GC-content fraction $x_{\\mathrm{GC}}$, canonical PAM context indicator $x_{\\mathrm{PAM}}$, and longest homopolymer run length $x_{\\mathrm{H}}$, we first compute the on-target latent score, $S_{\\mathrm{on}}$. This score is a linear combination of the guide's features:\n$$S_{\\mathrm{on}} = w_0 + w_{\\mathrm{GC}}\\,x_{\\mathrm{GC}} + w_{\\mathrm{PAM}}\\,x_{\\mathrm{PAM}} + w_{\\mathrm{H}}\\,x_{\\mathrm{H}}$$\nwhere $w_0$, $w_{\\mathrm{GC}}$, $w_{\\mathrm{PAM}}$, and $w_{\\mathrm{H}}$ are given coefficients.\n\nThe on-target cleavage probability, $p_{\\mathrm{on}}$, is then calculated as:\n$$p_{\\mathrm{on}} = \\sigma(S_{\\mathrm{on}})$$\n\nNext, we assess the off-target effects. For each of the $N_{\\mathrm{off}}$ potential off-target sites associated with a guide, we are given a set of mismatch positions, $\\mathcal{M}_j \\subset \\{1, 2, \\dots, 20\\}$ for $j \\in \\{1, \\dots, N_{\\mathrm{off}}\\}$. The model posits that mismatches additively penalize the latent score. The total penalty for an off-target site $j$ is the sum of position-specific penalties, $\\delta_i$, for each mismatch position $i \\in \\mathcal{M}_j$:\n$$\\Delta_j = \\sum_{i \\in \\mathcal{M}_j} \\delta_i$$\nThe off-target latent score for site $j$, $S_{\\mathrm{off},j}$, is the on-target score reduced by this penalty:\n$$S_{\\mathrm{off},j} = S_{\\mathrm{on}} - \\Delta_j$$\nThe corresponding off-target cleavage probability is $p_{\\mathrm{off},j} = \\sigma(S_{\\mathrm{off},j})$.\n\nThe total off-target risk, $R$, for a guide is the sum of these probabilities over all its considered off-target sites:\n$$R = \\sum_{j=1}^{N_{\\mathrm{off}}} p_{\\mathrm{off},j}$$\nIf a guide has no specified off-target sites, its risk $R$ is $0$.\n\nFinally, the composite utility score, $U$, is a linear trade-off between the benefit of on-target cleavage ($p_{\\mathrm{on}}$) and the cost of off-target risk ($R$), weighted by a coefficient $c$:\n$$U = p_{\\mathrm{on}} - c \\, R$$\nThis value is computed for every guide in a test case and rounded to $6$ decimal places as required.\n\n**2. Pareto-Optimality Analysis**\n\nThe second part of the task is to identify the set of non-dominated guides. This is a bi-objective optimization problem where we seek to simultaneously maximize $p_{\\mathrm{on}}$ and minimize $R$.\n\nA guide $a$ is said to dominate a guide $b$ if it is at least as good as $b$ on both objectives and strictly better on at least one. Formally, guide $a$ dominates guide $b$ if:\n$$(p_{\\mathrm{on},a} \\ge p_{\\mathrm{on},b} \\text{ and } R_a \\le R_b) \\text{ and } (p_{\\mathrm{on},a}  p_{\\mathrm{on},b} \\text{ or } R_a  R_b)$$\n\nTo find the Pareto-optimal set, we perform a pairwise comparison. For each guide $g_i$ in the set of all guides, we check if it is dominated by any other guide $g_j$. If no such dominating guide $g_j$ exists, then $g_i$ is part of the Pareto-optimal set.\n\nThe algorithm is as follows:\n1. For each guide $i$, compute and store the pair of metrics $(p_{\\mathrm{on},i}, R_i)$.\n2. Initialize an empty Pareto set, $\\mathcal{P}$.\n3. For each guide $i$:\n    a. Assume guide $i$ is not dominated.\n    b. For each other guide $j$ (where $j \\neq i$):\n        i. Check if guide $j$ dominates guide $i$ using the defined condition.\n        ii. If dominance is found, mark guide $i$ as dominated and break the inner loop.\n    c. If, after checking all other guides, guide $i$ is not marked as dominated, add its index to the set $\\mathcal{P}$.\n4. The final result is a list of the 0-based indices in $\\mathcal{P}$, sorted in ascending order.\n\nThis two-stage process—first, the systematic calculation of $(p_{\\mathrm{on}},R)$ pairs and utility scores $U$ for all guides, and second, the exhaustive pairwise comparison to identify the non-dominated subset—provides the complete solution for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the CRISPR guide utility and Pareto optimality problem.\n    \"\"\"\n\n    # Helper function for the logistic sigmoid\n    def sigma(x: float) - float:\n        \"\"\"Computes the logistic sigmoid function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-x))\n\n    def process_case(case_data: dict) - tuple[list[float], list[int]]:\n        \"\"\"\n        Processes a single test case to compute utilities and the Pareto-optimal set.\n\n        Args:\n            case_data: A dictionary containing parameters for the test case.\n\n        Returns:\n            A tuple containing:\n            - A list of rounded composite utility scores for all guides.\n            - A sorted list of 0-based indices of the Pareto-optimal guides.\n        \"\"\"\n        # Unpack case data\n        w0, w_gc, w_pam, w_h = case_data[\"coeffs\"]\n        c = case_data[\"c\"]\n        guides_data = case_data[\"guides\"]\n        delta_penalties = case_data[\"penalties\"]\n\n        utilities = []\n        guide_metrics = []  # Stores {'id', 'p_on', 'R'} for Pareto analysis\n\n        # Step 1: Calculate metrics for each guide\n        for idx, guide in enumerate(guides_data):\n            x_gc, x_pam, x_h, off_target_mismatch_sets = guide\n            \n            # Calculate On-Target Score and Probability\n            s_on = w0 + w_gc * x_gc + w_pam * x_pam + w_h * x_h\n            p_on = sigma(s_on)\n\n            # Calculate Off-Target Risk\n            total_risk = 0.0\n            for mismatch_set in off_target_mismatch_sets:\n                penalty_sum = 0.0\n                for pos in mismatch_set:\n                    # Problem uses 1-based indexing for mismatch positions\n                    penalty_sum += delta_penalties[pos - 1]\n                \n                s_off = s_on - penalty_sum\n                p_off = sigma(s_off)\n                total_risk += p_off\n            \n            # Store metrics for Pareto analysis\n            guide_metrics.append({'id': idx, 'p_on': p_on, 'R': total_risk})\n\n            # Calculate and store Composite Utility, rounded to 6 decimal places\n            utility = p_on - c * total_risk\n            utilities.append(round(utility, 6))\n\n        # Step 2: Determine the Pareto-optimal set\n        pareto_indices = []\n        for i, guide_i_metrics in enumerate(guide_metrics):\n            is_dominated = False\n            for j, guide_j_metrics in enumerate(guide_metrics):\n                if i == j:\n                    continue\n                \n                # Guide j dominates guide i if it is at least as good on all objectives\n                # and strictly better on at least one.\n                p_on_i, R_i = guide_i_metrics['p_on'], guide_i_metrics['R']\n                p_on_j, R_j = guide_j_metrics['p_on'], guide_j_metrics['R']\n\n                # Objectives: maximize p_on, minimize R\n                if (p_on_j = p_on_i and R_j = R_i) and \\\n                   (p_on_j  p_on_i or R_j  R_i):\n                    is_dominated = True\n                    break\n            \n            if not is_dominated:\n                pareto_indices.append(guide_i_metrics['id'])\n        \n        pareto_indices.sort()\n\n        return utilities, pareto_indices\n\n    # Define the penalty array, which is constant across all test cases\n    # Positions 1-5: 1.6; 6-10: 1.0; 11-15: 0.6; 16-20: 0.4\n    delta_penalties = np.array([1.6] * 5 + [1.0] * 5 + [0.6] * 5 + [0.4] * 5)\n    \n    # Define the test cases based on the problem statement\n    test_cases = [\n        # Case 1\n        {\n            \"coeffs\": (-1.0, 2.5, 1.0, -0.3), \"c\": 0.5, \"penalties\": delta_penalties,\n            \"guides\": [\n                (0.45, 1, 2, [{3, 12}, {5}]),\n                (0.60, 1, 3, [{1, 2}, {15, 16, 17}]),\n                (0.55, 0, 1, [{7}]),\n                (0.35, 1, 4, [{10, 11}, {2}, {19}])\n            ]\n        },\n        # Case 2\n        {\n            \"coeffs\": (-1.0, 2.5, 1.0, -0.3), \"c\": 0.2, \"penalties\": delta_penalties,\n            \"guides\": [\n                (0.50, 1, 1, []),\n                (0.65, 1, 2, []),\n                (0.40, 0, 1, [{1, 5, 10}])\n            ]\n        },\n        # Case 3\n        {\n            \"coeffs\": (-1.0, 2.5, 1.0, -0.3), \"c\": 0.8, \"penalties\": delta_penalties,\n            \"guides\": [\n                (0.62, 1, 2, [{1}, {2}, {3}, {1, 2}, {4, 5}]),\n                (0.50, 1, 2, [{18, 19, 20}]),\n                (0.48, 0, 1, [{12}, {13}]),\n                (0.36, 1, 1, [{19}])\n            ]\n        }\n    ]\n\n    # Aggregate results from all cases\n    all_results = []\n    for case_data in test_cases:\n        utilities, pareto_set = process_case(case_data)\n        all_results.append(utilities)\n        all_results.append(pareto_set)\n\n    # Format the final output string as a list of lists, with no spaces\n    # Example: [[0.1,0.2],[0],[-0.5],[1]]\n    string_pieces = [str(item).replace(\" \", \"\") for item in all_results]\n    print(f\"[{','.join(string_pieces)}]\")\n\nsolve()\n```"
        }
    ]
}