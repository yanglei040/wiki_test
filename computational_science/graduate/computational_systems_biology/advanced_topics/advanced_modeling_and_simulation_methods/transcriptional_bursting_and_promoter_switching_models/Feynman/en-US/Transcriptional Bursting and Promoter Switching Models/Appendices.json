{
    "hands_on_practices": [
        {
            "introduction": "The most direct way to connect a theoretical model to experimental data is by comparing statistical moments. This first practice challenges you to use the \"method of moments\" to link the experimentally measurable mean and Fano factor of an mRNA distribution back to the underlying promoter switching rates, $k_{\\text{on}}$ and $k_{\\text{off}}$ . By deriving these relationships from first principles, you will gain a fundamental understanding of how promoter kinetics shape the statistics of gene expression.",
            "id": "3356589",
            "problem": "Consider the standard two-state promoter switching (telegraph) model for messenger ribonucleic acid (mRNA) transcription in a single gene. The promoter toggles between an inactive state ($0$, \"OFF\") and an active state ($1$, \"ON\") according to a continuous-time Markov process with transition rate $k_{\\text{on}}$ from $0$ to $1$ and transition rate $k_{\\text{off}}$ from $1$ to $0$. When the promoter is ON, mRNA is synthesized at constant rate $s$; when the promoter is OFF, no mRNA is synthesized. Each mRNA molecule degrades independently at rate $\\gamma$. Assume the system is at steady state and that $s$ and $\\gamma$ are known positive constants.\n\nYou measure the ensemble steady-state mean $\\,\\mu\\,$ and the Fano factor $\\,F\\,$ (defined as the steady-state variance divided by the steady-state mean) of the mRNA copy number across cells. Working from first principles (starting from the telegraph model transitions and the definitions of the mean, second moment, and cross-moment), derive the steady-state relationships linking $\\,\\mu\\,$ and $\\,F\\,$ to $\\,k_{\\text{on}}\\,$ and $\\,k_{\\text{off}}\\,$. Using these relationships:\n\n- Determine the necessary and sufficient conditions on the measured $\\,\\mu\\,$ and $\\,F\\,$ (given known $s$ and $\\gamma$) under which $\\,k_{\\text{on}}\\,$ and $\\,k_{\\text{off}}\\,$ are identifiable as positive, finite rates.\n- Provide closed-form analytic expressions for the method-of-moments estimates of $\\,k_{\\text{on}}\\,$ and $\\,k_{\\text{off}}\\,$ in terms of $\\,\\mu\\,$, $\\,F\\,$, $\\,s\\,$, and $\\,\\gamma\\,$.\n\nYour final answer should contain only the analytic expressions for $\\,k_{\\text{on}}\\,$ and $\\,k_{\\text{off}}\\,$. No numerical evaluation is required.",
            "solution": "The problem is valid as it is scientifically grounded in the standard telegraph model of transcription, is well-posed, objective, and contains all necessary information for a formal derivation.\n\nLet $z(t)$ be the state of the promoter, where $z=1$ for the ON state and $z=0$ for the OFF state. Let $n(t)$ be the number of mRNA molecules. The system dynamics are described by the following transitions and rates:\n\\begin{itemize}\n    \\item Promoter activation: $0 \\xrightarrow{k_{\\text{on}}} 1$\n    \\item Promoter deactivation: $1 \\xrightarrow{k_{\\text{off}}} 0$\n    \\item mRNA synthesis: $\\emptyset \\xrightarrow{s} n$, occurs only when $z=1$\n    \\item mRNA degradation: $n \\xrightarrow{\\gamma} \\emptyset$, for each molecule\n\\end{itemize}\nWe proceed from first principles by deriving the time evolution of the moments of the joint probability distribution $P(z, n, t)$. We are interested in the steady state, where all time derivatives are zero.\n\nFirst, we consider the first moments: the mean promoter state $\\langle z \\rangle$ and the mean mRNA number $\\langle n \\rangle$. The dynamics of the expected values are given by:\n$$ \\frac{d\\langle z \\rangle}{dt} = k_{\\text{on}} \\langle 1-z \\rangle - k_{\\text{off}} \\langle z \\rangle = k_{\\text{on}} - (k_{\\text{on}} + k_{\\text{off}})\\langle z \\rangle $$\n$$ \\frac{d\\langle n \\rangle}{dt} = \\langle s \\cdot z - \\gamma n \\rangle = s\\langle z \\rangle - \\gamma\\langle n \\rangle $$\nAt steady state ($\\frac{d}{dt}=0$), we denote the steady-state probability of the promoter being ON as $p_{\\text{on}} = \\langle z \\rangle_{ss}$ and the mean mRNA number as $\\mu = \\langle n \\rangle_{ss}$. From the first equation:\n$$ 0 = k_{\\text{on}} - (k_{\\text{on}} + k_{\\text{off}})p_{\\text{on}} \\implies p_{\\text{on}} = \\frac{k_{\\text{on}}}{k_{\\text{on}} + k_{\\text{off}}} $$\nFrom the second equation:\n$$ 0 = s p_{\\text{on}} - \\gamma \\mu \\implies \\mu = \\frac{s}{\\gamma} p_{\\text{on}} $$\nCombining these gives the first relationship between the mean and the kinetic rates:\n$$ \\mu = \\frac{s}{\\gamma} \\frac{k_{\\text{on}}}{k_{\\text{on}} + k_{\\text{off}}} $$\n\nNext, we derive the dynamics of the second moments to find the variance, $\\sigma^2 = \\langle n^2 \\rangle_{ss} - \\mu^2$. We need the time evolution of $\\langle n^2 \\rangle$ and the cross-moment $\\langle nz \\rangle$.\nThe change in $n^2$ per reaction is:\n\\begin{itemize}\n    \\item Synthesis ($n \\to n+1$): $(n+1)^2 - n^2 = 2n+1$. This occurs with rate $s$ when $z=1$.\n    \\item Degradation ($n \\to n-1$): $(n-1)^2 - n^2 = -2n+1$. This occurs with rate $\\gamma n$.\n\\end{itemize}\nThus, the dynamics of $\\langle n^2 \\rangle$ are:\n$$ \\frac{d\\langle n^2 \\rangle}{dt} = \\langle (2n+1)sz \\rangle + \\langle (-2n+1)\\gamma n \\rangle = 2s\\langle nz \\rangle + s\\langle z \\rangle - 2\\gamma\\langle n^2 \\rangle + \\gamma\\langle n \\rangle $$\nThe change in the product $nz$ per reaction is:\n\\begin{itemize}\n    \\item Synthesis ($n \\to n+1$): $(n+1)z - nz = z$. Rate is $sz$. Since $z$ is $0$ or $1$, $z^2=z$. Contribution is $s\\langle z^2 \\rangle = s\\langle z \\rangle$.\n    \\item Degradation ($n \\to n-1$): $(n-1)z - nz = -z$. Rate is $\\gamma n$. Contribution is $\\langle -z(\\gamma n) \\rangle = -\\gamma \\langle nz \\rangle$.\n    \\item Activation ($z=0 \\to z=1$): $n(1) - n(0) = n$. Rate is $k_{\\text{on}}$ when $z=0$. Contribution is $k_{\\text{on}}\\langle n(1-z) \\rangle = k_{\\text{on}}(\\langle n \\rangle - \\langle nz \\rangle)$.\n    \\item Deactivation ($z=1 \\to z=0$): $n(0) - n(1) = -n$. Rate is $k_{\\text{off}}$ when $z=1$. Contribution is $k_{\\text{off}}\\langle -n z \\rangle = -k_{\\text{off}}\\langle nz \\rangle$.\n\\end{itemize}\nSumming these contributions gives the dynamics of $\\langle nz \\rangle$:\n$$ \\frac{d\\langle nz \\rangle}{dt} = s\\langle z \\rangle - \\gamma\\langle nz \\rangle + k_{\\text{on}}\\langle n \\rangle - k_{\\text{on}}\\langle nz \\rangle - k_{\\text{off}}\\langle nz \\rangle = s\\langle z \\rangle + k_{\\text{on}}\\langle n \\rangle - (k_{\\text{on}}+k_{\\text{off}}+\\gamma)\\langle nz \\rangle $$\nAt steady state, setting the derivatives to zero:\n$$ 2s\\langle nz \\rangle_{ss} + s p_{\\text{on}} - 2\\gamma\\langle n^2 \\rangle_{ss} + \\gamma\\mu = 0 \\implies \\langle n^2 \\rangle_{ss} = \\frac{1}{2\\gamma} (2s\\langle nz \\rangle_{ss} + s p_{\\text{on}} + \\gamma\\mu) $$\n$$ s p_{\\text{on}} + k_{\\text{on}}\\mu - (k_{\\text{on}}+k_{\\text{off}}+\\gamma)\\langle nz \\rangle_{ss} = 0 \\implies \\langle nz \\rangle_{ss} = \\frac{s p_{\\text{on}} + k_{\\text{on}}\\mu}{k_{\\text{on}}+k_{\\text{off}}+\\gamma} $$\nSubstituting the expression for $\\langle nz \\rangle_{ss}$ into the one for $\\langle n^2 \\rangle_{ss}$:\n$$ \\langle n^2 \\rangle_{ss} = \\frac{s}{\\gamma} \\left( \\frac{s p_{\\text{on}} + k_{\\text{on}}\\mu}{k_{\\text{on}}+k_{\\text{off}}+\\gamma} \\right) + \\frac{s p_{\\text{on}}}{2\\gamma} + \\frac{\\mu}{2} $$\nUsing $\\mu = sp_{\\text{on}}/\\gamma$, we have $s p_{\\text{on}} = \\gamma\\mu$.\n$$ \\langle n^2 \\rangle_{ss} = \\frac{s}{\\gamma} \\left( \\frac{\\gamma\\mu + k_{\\text{on}}\\mu}{k_{\\text{on}}+k_{\\text{off}}+\\gamma} \\right) + \\frac{\\gamma\\mu}{2\\gamma} + \\frac{\\mu}{2} = \\frac{s\\mu(k_{\\text{on}}+\\gamma)}{\\gamma(k_{\\text{on}}+k_{\\text{off}}+\\gamma)} + \\mu $$\nThe variance is $\\sigma^2 = \\langle n^2 \\rangle_{ss} - \\mu^2$:\n$$ \\sigma^2 = \\mu + \\frac{s\\mu(k_{\\text{on}}+\\gamma)}{\\gamma(k_{\\text{on}}+k_{\\text{off}}+\\gamma)} - \\mu^2 $$\nUsing $\\mu = \\frac{s}{\\gamma} \\frac{k_{\\text{on}}}{k_{\\text{on}}+k_{\\text{off}}}$, we can rearrange the variance to get a more insightful form:\n$$ \\sigma^2 = \\mu + \\left(\\frac{s}{\\gamma}\\right)^2 \\frac{k_{\\text{on}}k_{\\text{off}}}{(k_{\\text{on}}+k_{\\text{off}})^2(k_{\\text{on}}+k_{\\text{off}}+\\gamma)} $$\nThe Fano factor is $F = \\sigma^2/\\mu$.\n$$ F = 1 + \\frac{1}{\\mu} \\left(\\frac{s}{\\gamma}\\right)^2 \\frac{k_{\\text{on}}k_{\\text{off}}}{(k_{\\text{on}}+k_{\\text{off}})^2(k_{\\text{on}}+k_{\\text{off}}+\\gamma)} $$\nSubstituting $\\mu = \\frac{s}{\\gamma} \\frac{k_{\\text{on}}}{k_{\\text{on}}+k_{\\text{off}}}$:\n$$ F = 1 + \\frac{\\gamma(k_{\\text{on}}+k_{\\text{off}})}{s k_{\\text{on}}} \\left(\\frac{s}{\\gamma}\\right)^2 \\frac{k_{\\text{on}}k_{\\text{off}}}{(k_{\\text{on}}+k_{\\text{off}})^2(k_{\\text{on}}+k_{\\text{off}}+\\gamma)} = 1 + \\frac{s k_{\\text{off}}}{(k_{\\text{on}}+k_{\\text{off}})(k_{\\text{on}}+k_{\\text{off}}+\\gamma)} $$\nThis gives our second key relationship. Now we solve for $k_{\\text{on}}$ and $k_{\\text{off}}$ using the two relationships:\n1. $\\mu = \\frac{s}{\\gamma} \\frac{k_{\\text{on}}}{k_{\\text{on}}+k_{\\text{off}}}$\n2. $F-1 = \\frac{s k_{\\text{off}}}{(k_{\\text{on}}+k_{\\text{off}})(k_{\\text{on}}+k_{\\text{off}}+\\gamma)}$\n\nFrom (1), we have $\\frac{\\gamma\\mu}{s} = \\frac{k_{\\text{on}}}{k_{\\text{on}}+k_{\\text{off}}} = p_{\\text{on}}$.\nThis implies $1 - \\frac{\\gamma\\mu}{s} = \\frac{k_{\\text{off}}}{k_{\\text{on}}+k_{\\text{off}}} = p_{\\text{off}}$. A necessary condition for $k_{\\text{on}}>0, k_{\\text{off}}>0$ is $0 < p_{\\text{on}} < 1$, which requires $0 < \\mu < s/\\gamma$.\nLet $K = k_{\\text{on}}+k_{\\text{off}}$. Substitute into (2):\n$$ F-1 = \\frac{s(K p_{\\text{off}})}{K(K+\\gamma)} = \\frac{s(1-\\frac{\\gamma\\mu}{s})}{K+\\gamma} = \\frac{s-\\gamma\\mu}{K+\\gamma} $$\nSolving for $K$:\n$$ K+\\gamma = \\frac{s-\\gamma\\mu}{F-1} \\implies K = \\frac{s-\\gamma\\mu}{F-1} - \\gamma = \\frac{s-\\gamma\\mu - \\gamma(F-1)}{F-1} = \\frac{s+\\gamma-\\gamma(\\mu+F)}{F-1} $$\nFor $K$ to be positive, given $F>1$ (which must hold if $k_{\\text{off}}>0$), the numerator must be positive: $s+\\gamma-\\gamma(\\mu+F) > 0$, which simplifies to $F < 1 + s/\\gamma - \\mu$.\nThus, the necessary and sufficient conditions on the measured $(\\mu, F)$ for identifiable positive finite rates are:\n- $F > 1$\n- $0 < \\mu < s/\\gamma$\n- $F < 1 + s/\\gamma - \\mu$\n\nUnder these conditions, the rate constants are given by:\n$k_{\\text{on}} = K \\cdot p_{\\text{on}} = K \\cdot \\frac{\\gamma\\mu}{s}$\n$k_{\\text{off}} = K \\cdot p_{\\text{off}} = K \\cdot \\left(1-\\frac{\\gamma\\mu}{s}\\right)$\n\nSubstituting the expression for $K$:\n$$ k_{\\text{on}} = \\frac{\\gamma\\mu}{s} \\left(\\frac{s+\\gamma-\\gamma(\\mu+F)}{F-1}\\right) $$\n$$ k_{\\text{off}} = \\frac{s-\\gamma\\mu}{s} \\left(\\frac{s+\\gamma-\\gamma(\\mu+F)}{F-1}\\right) $$\nThese are the closed-form analytic expressions for the method-of-moments estimates of $k_{\\text{on}}$ and $k_{\\text{off}}$.",
            "answer": "$$ \\boxed{ \\begin{aligned} k_{\\text{on}} &= \\frac{\\gamma\\mu}{s} \\left(\\frac{s+\\gamma-\\gamma(\\mu+F)}{F-1}\\right) \\\\ k_{\\text{off}} &= \\frac{s-\\gamma\\mu}{s} \\left(\\frac{s+\\gamma-\\gamma(\\mu+F)}{F-1}\\right) \\end{aligned} } $$"
        },
        {
            "introduction": "While the method of moments is useful, it often requires knowing some parameters to estimate others. This raises a deeper question: what is the absolute limit of what we can learn from a given type of data? This practice explores this question of structural identifiability for snapshot mRNA counts . You will use the formalisms of maximum likelihood and the Fisher Information Matrix to demonstrate that static, single-timepoint data is fundamentally insufficient to uniquely determine all parameters of the telegraph model, a crucial insight for any modeling endeavor.",
            "id": "3356617",
            "problem": "A gene is modeled by a two-state promoter switching process (commonly called the telegraph model) with promoter activation rate $k_{\\text{on}}$, promoter inactivation rate $k_{\\text{off}}$, active-state transcription initiation rate $s$ (molecules per unit time), and messenger ribonucleic acid (mRNA) degradation rate $\\gamma$. At steady state, and in the bursty limit where transcription occurs in short active episodes interspersed with longer inactive periods, single-cell snapshot mRNA counts measured by Single-molecule Fluorescence In Situ Hybridization (smFISH) can be well-approximated by a negative binomial distribution. In this regime, the distribution of counts $n \\in \\{0,1,2,\\dots\\}$ has probability mass function\n$$\nP(n \\mid r,p) \\;=\\; \\binom{n + r - 1}{n} \\,(1-p)^{r}\\, p^{n},\n$$\nwith shape parameter $r$ and success probability $p$ related to the underlying biophysical parameters by\n$$\nr \\;=\\; \\frac{k_{\\text{on}}}{\\gamma}, \n\\qquad\np \\;=\\; \\frac{b}{1+b},\n\\qquad\nb \\;=\\; \\frac{s}{k_{\\text{off}}}.\n$$\nYou observe $N$ independent single-cell counts $\\{n_{i}\\}_{i=1}^{N}$ at steady state. Assume no additional measurements or external priors are available beyond these snapshot counts and the negative binomial approximation above.\n\nTasks:\n1) Starting from the definition of the negative binomial probability mass function and independence of the $N$ cells, write down the log-likelihood $\\ell(k_{\\text{on}},k_{\\text{off}},s,\\gamma \\mid \\{n_{i}\\})$ as a function of the biophysical parameter vector $\\theta = (k_{\\text{on}},k_{\\text{off}},s,\\gamma)$, using only the relationships given above between $(r,p)$ and $\\theta$.\n\n2) Derive the score with respect to $(r,p)$, and express the score with respect to $\\theta$ via the chain rule using the Jacobian $\\partial(r,p)/\\partial\\theta$.\n\n3) Using the standard definition of the Fisher information matrix as the negative expectation of the Hessian of the log-likelihood, express the Fisher information for $\\theta$ in terms of the Fisher information of $(r,p)$ and the Jacobian of the parameter mapping. Do not assume any special values of the parameters beyond the bursty-limit mapping stated above.\n\n4) Under the measurement scenario described (smFISH snapshots only) and the negative binomial approximation above, determine whether $\\theta$ is structurally identifiable from the data. In particular, determine the rank of the Fisher information matrix with respect to $\\theta$ at generic parameter values, and state the dimension of the asymptotic confidence region for $\\theta$ in the large-$N$ limit. Report as your final answer the rank of the Fisher information matrix for $\\theta$ under these conditions. No rounding is required, and no physical units are needed for this rank.",
            "solution": "The problem asks for an analysis of the structural identifiability of a four-parameter biophysical model of transcription, given that the observable mRNA counts follow a two-parameter negative binomial distribution. The analysis proceeds in four parts as requested.\n\n### Part 1: Log-Likelihood Function\n\nThe probability mass function (PMF) of the negative binomial distribution for a single cell's mRNA count $n$ is given as\n$$P(n \\mid r,p) = \\binom{n + r - 1}{n} (1-p)^{r} p^{n}$$\nwhere $r$ is the shape parameter and $p$ is the success probability. We are given $N$ independent single-cell counts $\\{n_{i}\\}_{i=1}^{N}$. The likelihood function is the product of the individual probabilities:\n$$L(r,p \\mid \\{n_{i}\\}) = \\prod_{i=1}^{N} P(n_i \\mid r,p) = \\prod_{i=1}^{N} \\binom{n_i + r - 1}{n_i} (1-p)^{r} p^{n_i}$$\nThe log-likelihood, denoted $\\ell(r,p \\mid \\{n_{i}\\})$, is the natural logarithm of the likelihood:\n$$\\ell(r,p \\mid \\{n_{i}\\}) = \\ln L(r,p \\mid \\{n_{i}\\}) = \\sum_{i=1}^{N} \\left[ \\ln\\binom{n_i + r - 1}{n_i} + r\\ln(1-p) + n_i\\ln p \\right]$$\nUsing the identity $\\binom{n+k-1}{k} = \\frac{\\Gamma(n+k)}{\\Gamma(n+1)\\Gamma(k)}$, where $\\Gamma(\\cdot)$ is the gamma function, we can rewrite the log-likelihood as:\n$$\\ell(r,p \\mid \\{n_{i}\\}) = \\sum_{i=1}^{N} \\left[ \\ln\\Gamma(n_i + r) - \\ln\\Gamma(r) - \\ln\\Gamma(n_i + 1) + r\\ln(1-p) + n_i\\ln p \\right]$$\nThe problem specifies the relationship between the statistical parameters $(r, p)$ and the biophysical parameters $\\theta = (k_{\\text{on}}, k_{\\text{off}}, s, \\gamma)$:\n$$r = \\frac{k_{\\text{on}}}{\\gamma}$$\n$$p = \\frac{b}{1+b} \\quad \\text{with} \\quad b = \\frac{s}{k_{\\text{off}}}$$\nSubstituting for $b$, we get $p = \\frac{s/k_{\\text{off}}}{1+s/k_{\\text{off}}} = \\frac{s}{k_{\\text{off}}+s}$. Consequently, $1-p = 1 - \\frac{s}{k_{\\text{off}}+s} = \\frac{k_{\\text{off}}}{k_{\\text{off}}+s}$.\n\nBy substituting these expressions for $r$, $p$, and $1-p$ into the log-likelihood function for $(r,p)$, we obtain the log-likelihood as a function of the biophysical parameter vector $\\theta$:\n$$\\ell(\\theta \\mid \\{n_i\\}) = \\sum_{i=1}^{N} \\left[ \\ln\\Gamma\\left(n_i + \\frac{k_{\\text{on}}}{\\gamma}\\right) - \\ln\\Gamma\\left(\\frac{k_{\\text{on}}}{\\gamma}\\right) - \\ln\\Gamma(n_i + 1) + \\frac{k_{\\text{on}}}{\\gamma}\\ln\\left(\\frac{k_{\\text{off}}}{k_{\\text{off}}+s}\\right) + n_i\\ln\\left(\\frac{s}{k_{\\text{off}}+s}\\right) \\right]$$\n\n### Part 2: Score Vector\n\nThe score vector is the gradient of the log-likelihood function with respect to the parameters. Let $\\psi(\\cdot)$ denote the digamma function, $\\psi(z) = \\frac{d}{dz}\\ln\\Gamma(z)$. The score with respect to $(r,p)$ is $S_{r,p} = \\nabla_{r,p} \\ell = \\left(\\frac{\\partial \\ell}{\\partial r}, \\frac{\\partial \\ell}{\\partial p}\\right)^T$.\n$$\\frac{\\partial \\ell}{\\partial r} = \\sum_{i=1}^{N} \\left[ \\psi(n_i + r) - \\psi(r) + \\ln(1-p) \\right]$$\n$$\\frac{\\partial \\ell}{\\partial p} = \\sum_{i=1}^{N} \\left[ -\\frac{r}{1-p} + \\frac{n_i}{p} \\right] = -\\frac{Nr}{1-p} + \\frac{\\sum_{i=1}^{N} n_i}{p}$$\nThe score with respect to $\\theta$ can be found using the chain rule:\n$$S_{\\theta} = \\nabla_{\\theta} \\ell(\\theta) = J^T S_{r,p} = J^T \\nabla_{r,p} \\ell(r(\\theta), p(\\theta))$$\nwhere $J$ is the Jacobian matrix of the transformation from $\\theta$ to $(r,p)$:\n$$J = \\frac{\\partial(r,p)}{\\partial\\theta} = \\begin{pmatrix} \\frac{\\partial r}{\\partial k_{\\text{on}}} & \\frac{\\partial r}{\\partial k_{\\text{off}}} & \\frac{\\partial r}{\\partial s} & \\frac{\\partial r}{\\partial \\gamma} \\\\ \\frac{\\partial p}{\\partial k_{\\text{on}}} & \\frac{\\partial p}{\\partial k_{\\text{off}}} & \\frac{\\partial p}{\\partial s} & \\frac{\\partial p}{\\partial \\gamma} \\end{pmatrix}$$\nWe compute the elements of the Jacobian:\n- $\\frac{\\partial r}{\\partial k_{\\text{on}}} = \\frac{1}{\\gamma}$\n- $\\frac{\\partial r}{\\partial k_{\\text{off}}} = 0$\n- $\\frac{\\partial r}{\\partial s} = 0$\n- $\\frac{\\partial r}{\\partial \\gamma} = -\\frac{k_{\\text{on}}}{\\gamma^2}$\n- $\\frac{\\partial p}{\\partial k_{\\text{on}}} = 0$\n- $\\frac{\\partial p}{\\partial k_{\\text{off}}} = \\frac{\\partial}{\\partial k_{\\text{off}}}\\left(\\frac{s}{k_{\\text{off}}+s}\\right) = -\\frac{s}{(k_{\\text{off}}+s)^2}$\n- $\\frac{\\partial p}{\\partial s} = \\frac{\\partial}{\\partial s}\\left(\\frac{s}{k_{\\text{off}}+s}\\right) = \\frac{(k_{\\text{off}}+s) - s}{(k_{\\text{off}}+s)^2} = \\frac{k_{\\text{off}}}{(k_{\\text{off}}+s)^2}$\n- $\\frac{\\partial p}{\\partial \\gamma} = 0$\n\nThe Jacobian matrix is:\n$$J = \\begin{pmatrix} 1/\\gamma & 0 & 0 & -k_{\\text{on}}/\\gamma^2 \\\\ 0 & -s/(k_{\\text{off}}+s)^2 & k_{\\text{off}}/(k_{\\text{off}}+s)^2 & 0 \\end{pmatrix}$$\n\n### Part 3: Fisher Information Matrix\n\nThe Fisher Information Matrix (FIM) for a parameter vector $\\phi$ is defined as $I(\\phi) = -E[\\nabla_{\\phi} \\nabla_{\\phi}^T \\ell(\\phi)]$. An equivalent definition is $I(\\phi) = E[S_{\\phi} S_{\\phi}^T]$. For a reparameterization $\\phi = f(\\theta)$, the FIM for $\\theta$ is related to the FIM for $\\phi$ by the formula $I(\\theta) = J^T I(\\phi) J$, where $J$ is the Jacobian $\\partial\\phi/\\partial\\theta$.\n\nIn our case, the observable parameters are $\\phi=(r,p)$, and the biophysical parameters are $\\theta=(k_{\\text{on}}, k_{\\text{off}}, s, \\gamma)$. Using the formula with the Jacobian $J$ derived in Part 2, the FIM for $\\theta$ is expressed in terms of the FIM for $(r,p)$ as:\n$$I(\\theta) = J^T I(r,p) J$$\n\n### Part 4: Structural Identifiability and FIM Rank\n\nA parameter vector $\\theta$ is structurally identifiable if its value can be uniquely determined from ideal (infinite and noise-free) data. In the context of maximum likelihood estimation, local structural identifiability requires that the Fisher Information Matrix $I(\\theta)$ be non-singular, i.e., of full rank.\n\nThe parameter vector $\\theta$ has dimension $4$. Thus, $I(\\theta)$ is a $4 \\times 4$ matrix. The parameter vector $(r,p)$ has dimension $2$, and for the negative binomial distribution, its FIM, $I(r,p)$, is a $2 \\times 2$ positive definite matrix for any valid parameter values ($r>0, 0<p<1$). This means $I(r,p)$ is invertible and has rank $2$.\n\nThe rank of $I(\\theta)$ is given by the rank of the matrix product $J^T I(r,p) J$. A general property of matrix ranks is that $\\text{rank}(AB) \\le \\min(\\text{rank}(A), \\text{rank}(B))$.\n$$\\text{rank}(I(\\theta)) = \\text{rank}(J^T I(r,p) J) \\le \\min(\\text{rank}(J^T), \\text{rank}(I(r,p)), \\text{rank}(J))$$\nSince $\\text{rank}(I(r,p)) = 2$, the rank of $I(\\theta)$ can be at most $2$. Furthermore, because $I(r,p)$ is positive definite (and thus full rank), the rank of the product is determined solely by the rank of the Jacobian:\n$$\\text{rank}(I(\\theta)) = \\text{rank}(J)$$\nWe must now determine the rank of the $2 \\times 4$ Jacobian matrix $J$:\n$$J = \\begin{pmatrix} 1/\\gamma & 0 & 0 & -k_{\\text{on}}/\\gamma^2 \\\\ 0 & -s/(k_{\\text{off}}+s)^2 & k_{\\text{off}}/(k_{\\text{off}}+s)^2 & 0 \\end{pmatrix}$$\nThe rank of a matrix is the dimension of the vector space spanned by its rows (or columns). The two rows of $J$ are:\n$$R_1 = (1/\\gamma, \\quad 0, \\quad 0, \\quad -k_{\\text{on}}/\\gamma^2)$$\n$$R_2 = (0, \\quad -s/(k_{\\text{off}}+s)^2, \\quad k_{\\text{off}}/(k_{\\text{off}}+s)^2, \\quad 0)$$\nFor the rows to be linearly dependent, one must be a scalar multiple of the other, i.e., $R_1 = c R_2$ for some constant $c$. By inspecting the first elements, we would need $1/\\gamma = c \\cdot 0$, which is impossible for any finite $c$ since $1/\\gamma \\neq 0$ for physically meaningful parameters. Similarly, by inspecting the fourth elements, $-k_{\\text{on}}/\\gamma^2 = c \\cdot 0$, which implies $k_{\\text{on}}=0$ (a trivial case) or $c$ is undefined. Since the non-zero elements of the two rows are in different positions, the rows are linearly independent for any generic, non-trivial choice of parameters ($k_{\\text{on}}, s, \\gamma > 0$ and finite).\n\nTherefore, the rank of the Jacobian matrix $J$ is $2$.\n$$\\text{rank}(J) = 2$$\nThis implies that the rank of the Fisher Information Matrix for the biophysical parameters is also $2$.\n$$\\text{rank}(I(\\theta)) = 2$$\nSince the number of parameters is $4$ and the rank of the FIM is $2$, the FIM is singular (rank-deficient). The rank deficiency is $4 - 2 = 2$. This proves that the parameter vector $\\theta$ is structurally unidentifiable from the given data. The data can only constrain $2$ effective parameters, which are the combinations $r=k_{\\text{on}}/\\gamma$ and $b=s/k_{\\text{off}}$. The dimension of the identifiable manifold of parameters is $2$. In the large-$N$ limit, the asymptotic confidence region for $\\theta$ is unbounded in $2$ directions in the $4$-dimensional parameter space. The problem asks for the rank of the Fisher information matrix.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "The previous exercises highlight the inherent limitations of using static snapshot data to infer dynamic processes. To fully characterize promoter kinetics, we must capture the system's evolution in time. This final, computational practice guides you in developing a Hidden Markov Model (HMM) to analyze time-series data of transcriptional activity . By implementing the forward-backward algorithm, you will see how richer datasets, coupled with appropriate statistical tools, can overcome the ambiguities of snapshot measurements and provide a more complete picture of gene regulation.",
            "id": "3356569",
            "problem": "Consider a two-state promoter model for transcriptional bursting, where the promoter switches between an inactive state and an active state in continuous time, modeled as a Continuous-Time Markov Chain (CTMC). Let the inactive state be labeled $0$ and the active state be labeled $1$. The CTMC is defined by a generator matrix with off-diagonal transition rates $k_{\\text{on}}$ from state $0$ to state $1$ and $k_{\\text{off}}$ from state $1$ to state $0$. Transcriptional initiation events occur according to a state-dependent rate, with rate $\\lambda_0$ in state $0$ and rate $\\lambda_1$ in state $1$. Observations are obtained as time-binned counts. In each bin of duration $\\Delta$, each initiation event is detected independently with probability $p$, and the observed count in the bin is the number of detected events.\n\nStarting from the following fundamental definitions and facts: the CTMC transition matrix over time $\\Delta$ is given by the matrix exponential of the generator, the Poisson process governs event initiation in each promoter state, and independent Bernoulli detection thins the Poisson process, derive from first principles the likelihood for a sequence of observed time-binned counts under binomial detection and promoter switching. Based on your derivation, design an exact inference algorithm using the forward-backward procedure for a Hidden Markov Model (HMM) that computes the log-likelihood of the observed count sequence and the posterior probabilities of the promoter state at each bin. Assume the initial promoter state distribution is the stationary distribution of the CTMC.\n\nMathematical and modeling setup:\n- The CTMC generator matrix is\n$$\nQ = \\begin{pmatrix}\n- k_{\\text{on}} & k_{\\text{on}} \\\\\nk_{\\text{off}} & - k_{\\text{off}}\n\\end{pmatrix},\n$$\nwith $k_{\\text{on}} > 0$ and $k_{\\text{off}} > 0$, measured in $s^{-1}$.\n- The bin duration is $\\Delta$ in $s$.\n- The transcription initiation rates are $\\lambda_0$ and $\\lambda_1$, both in $s^{-1}$.\n- The detection probability $p$ is dimensionless and satisfies $0 < p \\leq 1$.\n- Let $y_t$ be the observed count in bin $t$ for $t = 1, \\dots, T$.\n\nYou must:\n1. Derive the emission probability for an observed count $y_t$ given the promoter state, under the assumption that initiation events in state $s \\in \\{0,1\\}$ occur as a Poisson process with rate $\\lambda_s$, and each event is detected independently with probability $p$. Use only fundamental properties of Poisson processes and independent Bernoulli trials to establish the emission model.\n2. Derive the exact two-state CTMC transition matrix over time $\\Delta$ for the promoter state, expressing it in terms of $k_{\\text{on}}$, $k_{\\text{off}}$, and $\\Delta$.\n3. Construct the HMM likelihood of the observed sequence $\\{y_t\\}_{t=1}^T$ by marginalizing over hidden promoter states, and express how the forward-backward algorithm computes the log-likelihood and posterior probabilities of states at each time bin in a numerically stable manner.\n4. Implement a complete program that, for each test case below, computes the log-likelihood of the provided observed count sequence using the exact forward-backward algorithm, with the initial promoter state distribution set to the stationary distribution of the CTMC. The final program output must be a single line containing a comma-separated list of the log-likelihood values for each test case, enclosed in square brackets.\n\nUnits and output specification:\n- Rates $k_{\\text{on}}$, $k_{\\text{off}}$, $\\lambda_0$, and $\\lambda_1$ are provided in $s^{-1}$.\n- The bin duration $\\Delta$ is provided in $s$.\n- Observed counts $y_t$ are integers per bin and dimensionless.\n- The final log-likelihood values are dimensionless floats and must be output with no unit.\n\nTest suite:\n- Case 1 (general case): $k_{\\text{on}} = 0.06\\, s^{-1}$, $k_{\\text{off}} = 0.09\\, s^{-1}$, $\\Delta = 20\\, s$, $\\lambda_0 = 0.05\\, s^{-1}$, $\\lambda_1 = 0.5\\, s^{-1}$, $p = 0.6$, observed counts $y = [5,7,1,0,2,6]$.\n- Case 2 (perfect detection): $k_{\\text{on}} = 0.2\\, s^{-1}$, $k_{\\text{off}} = 0.05\\, s^{-1}$, $\\Delta = 15\\, s$, $\\lambda_0 = 0.02\\, s^{-1}$, $\\lambda_1 = 0.4\\, s^{-1}$, $p = 1.0$, observed counts $y = [7,6,1,0,5]$.\n- Case 3 (low detection probability): $k_{\\text{on}} = 0.08\\, s^{-1}$, $k_{\\text{off}} = 0.12\\, s^{-1}$, $\\Delta = 30\\, s$, $\\lambda_0 = 0.1\\, s^{-1}$, $\\lambda_1 = 1.0\\, s^{-1}$, $p = 0.05$, observed counts $y = [2,0,1,0,1,3]$.\n- Case 4 (no activation, absorbing inactive promoter): $k_{\\text{on}} = 0.0\\, s^{-1}$, $k_{\\text{off}} = 0.2\\, s^{-1}$, $\\Delta = 25\\, s$, $\\lambda_0 = 0.02\\, s^{-1}$, $\\lambda_1 = 0.6\\, s^{-1}$, $p = 0.7$, observed counts $y = [0,0,1,0,0,1]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where $r_i$ is the log-likelihood (a float) for Case $i$ computed by the exact forward-backward algorithm with appropriate numerical scaling for stability.",
            "solution": "The posed problem is valid as it is scientifically grounded in the standard two-state model of transcriptional regulation, a cornerstone of computational systems biology. It is well-posed, providing all necessary parameters and a clear objective. The framework relies on established mathematical principles: Continuous-Time Markov Chains (CTMCs), Poisson processes, and Hidden Markov Models (HMMs). The formulation is objective and free of ambiguity, permitting a unique and verifiable solution.\n\nThe solution involves constructing a Hidden Markov Model (HMM) to represent the stochastic transcription process. The HMM discretizes the continuous-time process into bins of duration $\\Delta$. The hidden states of the HMM, $z_t \\in \\{0, 1\\}$, correspond to the promoter state (inactive or active) during the $t$-th time bin. We assume the promoter state is constant within each bin. The observed variables are the time-binned transcript counts, $\\{y_t\\}_{t=1}^T$. The HMM is fully specified by its initial state distribution, transition matrix, and emission probabilities.\n\n**1. Derivation of Emission Probabilities**\n\nThe emission probability is the probability of observing $y_t$ counts in a bin of duration $\\Delta$, given the promoter is in a fixed state $s \\in \\{0, 1\\}$.\nAccording to the problem, transcription initiation in state $s$ is a Poisson process with rate $\\lambda_s$. Therefore, the number of initiation events, $N_s$, occurring in an interval of duration $\\Delta$ follows a Poisson distribution:\n$$\nP(N_s=k | \\text{state}=s) = \\frac{(\\lambda_s \\Delta)^k e^{-\\lambda_s \\Delta}}{k!}\n$$\nEach of these $k$ events is detected independently with a Bernoulli trial of success probability $p$. The number of observed counts, $y_t$, given $k$ initiation events, follows a Binomial distribution:\n$$\nP(y_t = j | N_s=k) = \\binom{k}{j} p^j (1-p)^{k-j}\n$$\nTo find the marginal probability of observing $j$ counts, we sum over all possible numbers of initiation events $k \\ge j$:\n$$\nP(y_t=j | \\text{state}=s) = \\sum_{k=j}^{\\infty} P(y_t=j | N_s=k) P(N_s=k)\n$$\n$$\nP(y_t=j | \\text{state}=s) = \\sum_{k=j}^{\\infty} \\left[ \\frac{k!}{j!(k-j)!} p^j (1-p)^{k-j} \\right] \\left[ \\frac{(\\lambda_s \\Delta)^k e^{-\\lambda_s \\Delta}}{k!} \\right]\n$$\nRearranging terms, we get:\n$$\nP(y_t=j | \\text{state}=s) = \\frac{(p\\lambda_s \\Delta)^j}{j!} e^{-\\lambda_s \\Delta} \\sum_{k=j}^{\\infty} \\frac{((1-p)\\lambda_s \\Delta)^{k-j}}{(k-j)!}\n$$\nLet $m = k-j$. The summation becomes the Taylor series for an exponential function:\n$$\n\\sum_{m=0}^{\\infty} \\frac{((1-p)\\lambda_s \\Delta)^m}{m!} = e^{(1-p)\\lambda_s \\Delta}\n$$\nSubstituting this back, we obtain:\n$$\nP(y_t=j | \\text{state}=s) = \\frac{(p\\lambda_s \\Delta)^j}{j!} e^{-\\lambda_s \\Delta} e^{(1-p)\\lambda_s \\Delta} = \\frac{(p\\lambda_s \\Delta)^j e^{-p\\lambda_s \\Delta}}{j!}\n$$\nThis is the probability mass function of a Poisson distribution with mean $p \\lambda_s \\Delta$. Thus, the emission probabilities for observing $y_t$ counts from state $s \\in \\{0, 1\\}$ are:\n$$\nB_s(y_t) = P(y_t | z_t=s) = \\text{Poisson}(y_t; p \\lambda_s \\Delta)\n$$\n\n**2. Derivation of the Transition Matrix**\n\nThe transition matrix $A$ for the HMM describes the probability of the promoter state changing from bin to bin. This is governed by the evolution of the underlying CTMC over the bin duration $\\Delta$. The transition probability matrix is the matrix exponential of the generator matrix $Q$ multiplied by $\\Delta$: $A = e^{Q\\Delta}$.\nThe generator matrix is $Q = \\begin{pmatrix} -k_{\\text{on}} & k_{\\text{on}} \\\\ k_{\\text{off}} & -k_{\\text{off}} \\end{pmatrix}$.\nTo compute $e^{Q\\Delta}$, we diagonalize $Q$. The eigenvalues $\\mu$ are the roots of the characteristic equation $\\det(Q - \\mu I) = 0$, which yields $\\mu(\\mu + (k_{\\text{on}}+k_{\\text{off}})) = 0$. The eigenvalues are $\\mu_1=0$ and $\\mu_2=-(k_{\\text{on}}+k_{\\text{off}})$. Let $k_s = k_{\\text{on}}+k_{\\text{off}}$.\nThe corresponding right eigenvectors are $v_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ for $\\mu_1=0$ and $v_2 = \\begin{pmatrix} k_{\\text{on}} \\\\ -k_{\\text{off}} \\end{pmatrix}$ for $\\mu_2=-k_s$.\nThe matrix of eigenvectors is $V = \\begin{pmatrix} 1 & k_{\\text{on}} \\\\ 1 & -k_{\\text{off}} \\end{pmatrix}$, and its inverse is $V^{-1} = \\frac{1}{k_s} \\begin{pmatrix} k_{\\text{off}} & k_{\\text{on}} \\\\ 1 & -1 \\end{pmatrix}$.\nThe matrix exponential is then $A = V e^{D\\Delta} V^{-1}$, where $D = \\text{diag}(0, -k_s)$.\n$$\nA = \\begin{pmatrix} 1 & k_{\\text{on}} \\\\ 1 & -k_{\\text{off}} \\end{pmatrix} \\begin{pmatrix} e^0 & 0 \\\\ 0 & e^{-k_s\\Delta} \\end{pmatrix} \\frac{1}{k_s} \\begin{pmatrix} k_{\\text{off}} & k_{\\text{on}} \\\\ 1 & -1 \\end{pmatrix}\n$$\nPerforming the matrix multiplication gives the transition matrix elements $A_{ij} = P(z_{t+1}=j | z_t=i)$:\n$$\nA = \\begin{pmatrix}\n\\frac{k_{\\text{off}}}{k_s} + \\frac{k_{\\text{on}}}{k_s}e^{-k_s\\Delta} & \\frac{k_{\\text{on}}}{k_s} - \\frac{k_{\\text{on}}}{k_s}e^{-k_s\\Delta} \\\\\n\\frac{k_{\\text{off}}}{k_s} - \\frac{k_{\\text{off}}}{k_s}e^{-k_s\\Delta} & \\frac{k_{\\text{on}}}{k_s} + \\frac{k_{\\text{off}}}{k_s}e^{-k_s\\Delta}\n\\end{pmatrix}\n$$\nThis matrix is valid for $k_s > 0$. If $k_s=0$, the matrix becomes the identity matrix.\n\n**3. HMM Likelihood and the Forward-Backward Algorithm**\n\nThe HMM is defined by:\n- **Initial state distribution $\\pi$**: The stationary distribution of the CTMC, given by the normalized left eigenvector of $Q$ for the eigenvalue $0$. This yields $\\pi = (\\pi_0, \\pi_1)$, where $\\pi_0 = \\frac{k_{\\text{off}}}{k_{\\text{on}}+k_{\\text{off}}}$ and $\\pi_1 = \\frac{k_{\\text{on}}}{k_{\\text{on}}+k_{\\text{off}}}$. If $k_{\\text{on}} = 0$, the state is absorbed in state 0, so $\\pi = (1,0)$.\n- **Transition matrix $A$**: As derived above.\n- **Emission probabilities $B_s(y_t)$**: As derived above.\n\nThe likelihood of an observed sequence $Y = \\{y_1, \\dots, y_T\\}$ is $P(Y) = \\sum_Z P(Y, Z)$, where the sum is over all possible hidden state sequences $Z = \\{z_1, \\dots, z_T\\}$. A direct summation is computationally infeasible. The forward algorithm computes this efficiently.\n\n**The Forward Algorithm**: We define the forward variable $\\alpha_t(i) = P(y_1, \\dots, y_t, z_t=i)$.\n- **Initialization ($t=1$)**: $\\alpha_1(i) = \\pi_i B_i(y_1)$.\n- **Recursion ($t=2, \\dots, T$)**: $\\alpha_t(j) = \\left[ \\sum_{i=0}^1 \\alpha_{t-1}(i) A_{ij} \\right] B_j(y_t)$.\n- **Termination**: The total likelihood is $L = \\sum_{i=0}^1 \\alpha_T(i)$.\n\nFor numerical stability with long sequences, these probabilities can underflow. A scaling procedure is employed. We define scaled forward variables $\\hat{\\alpha}_t(i) = P(z_t=i | y_1, \\dots, y_t)$.\n- **Initialization**: $\\alpha'_1(i) = \\pi_i B_i(y_1)$. The first scaling factor is $c_1 = \\sum_i \\alpha'_1(i)$. The scaled variable is $\\hat{\\alpha}_1(i) = \\alpha'_1(i) / c_1$.\n- **Recursion**: For $t=2, \\dots, T$, we first compute the unscaled variables $\\alpha'_t(j) = \\left[ \\sum_{i=0}^1 \\hat{\\alpha}_{t-1}(i) A_{ij} \\right] B_j(y_t)$. The scaling factor is $c_t = \\sum_j \\alpha'_t(j)$. The scaled variable is $\\hat{\\alpha}_t(j) = \\alpha'_t(j) / c_t$.\n- **Log-Likelihood**: The log-likelihood of the sequence is the sum of the logarithms of the scaling factors: $\\log L = \\sum_{t=1}^T \\log c_t$.\n\n**Posterior Probabilities**: To compute the posterior (smoothed) probabilities $\\gamma_t(i) = P(z_t=i | y_1, \\dots, y_T)$, we also need the backward algorithm. The backward variable is $\\beta_t(i) = P(y_{t+1}, \\dots, y_T | z_t=i)$. It is computed via a recursion from $t=T-1$ down to $1$. The posterior is then given by $\\gamma_t(i) = \\frac{\\alpha_t(i)\\beta_t(i)}{L}$. Using the scaled forward variables, this becomes $\\gamma_t(i) \\propto \\hat{\\alpha}_t(i)\\beta_t(i)$, where the result is normalized to sum to $1$. The implementation below focuses on the log-likelihood calculation as required.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef log_poisson_pmf(k, mu):\n    \"\"\"\n    Computes the log of the Poisson probability mass function.\n    Avoids underflow/overflow by working in log space.\n    \"\"\"\n    # handle mu=0 case\n    if mu == 0:\n        return 0.0 if k == 0 else -np.inf\n    return k * np.log(mu) - mu - gammaln(k + 1)\n\ndef forward_algorithm(y, pi, A, emission_params):\n    \"\"\"\n    Computes the log-likelihood of an observation sequence using the\n    forward algorithm with scaling for numerical stability.\n    \n    Args:\n        y (list[int]): The sequence of observed counts.\n        pi (np.ndarray): The initial state distribution (1xN).\n        A (np.ndarray): The state transition matrix (NxN).\n        emission_params (list[float]): List of means for the Poisson emission distributions.\n        \n    Returns:\n        float: The log-likelihood of the observation sequence.\n    \"\"\"\n    T = len(y)\n    N = len(pi)\n    \n    # Pre-compute log emission probabilities for the whole sequence\n    log_emission_probs = np.zeros((T, N))\n    for t in range(T):\n        for i in range(N):\n            log_emission_probs[t, i] = log_poisson_pmf(y[t], emission_params[i])\n            \n    log_likelihood = 0.0\n    \n    # Initialization (t=1)\n    # alpha is in log-space, but we sum in normal space using log-sum-exp\n    # To use the scaling factor method, we convert to normal space briefly\n    log_alpha = np.log(pi) + log_emission_probs[0, :]\n    \n    # Scaling step\n    max_log_alpha = np.max(log_alpha)\n    if max_log_alpha == -np.inf: # Sequence is impossible\n        return -np.inf\n    \n    alpha_scaled = np.exp(log_alpha - max_log_alpha)\n    scale_factor = np.sum(alpha_scaled)\n    log_scale_factor = np.log(scale_factor) + max_log_alpha\n    \n    log_likelihood += log_scale_factor\n    alpha = alpha_scaled / scale_factor\n\n    # Recursion (t=2...T)\n    for t in range(1, T):\n        # Propagate forward\n        alpha_prev = alpha\n        alpha_unscaled = (alpha_prev @ A)\n        \n        # Check for zero probability, which can happen in absorbing states\n        if np.sum(alpha_unscaled) == 0:\n            return -np.inf\n\n        log_alpha = np.log(alpha_unscaled) + log_emission_probs[t, :]\n        \n        # Scaling step\n        max_log_alpha = np.max(log_alpha)\n        if max_log_alpha == -np.inf:\n            return -np.inf\n\n        alpha_scaled = np.exp(log_alpha - max_log_alpha)\n        scale_factor = np.sum(alpha_scaled)\n        log_scale_factor = np.log(scale_factor) + max_log_alpha\n        \n        log_likelihood += log_scale_factor\n        alpha = alpha_scaled / scale_factor\n        \n    return log_likelihood\n\n\ndef solve():\n    test_cases = [\n        {'k_on': 0.06, 'k_off': 0.09, 'delta': 20.0, 'lambda_0': 0.05, 'lambda_1': 0.5, 'p': 0.6, 'y': [5, 7, 1, 0, 2, 6]},\n        {'k_on': 0.2, 'k_off': 0.05, 'delta': 15.0, 'lambda_0': 0.02, 'lambda_1': 0.4, 'p': 1.0, 'y': [7, 6, 1, 0, 5]},\n        {'k_on': 0.08, 'k_off': 0.12, 'delta': 30.0, 'lambda_0': 0.1, 'lambda_1': 1.0, 'p': 0.05, 'y': [2, 0, 1, 0, 1, 3]},\n        {'k_on': 0.0, 'k_off': 0.2, 'delta': 25.0, 'lambda_0': 0.02, 'lambda_1': 0.6, 'p': 0.7, 'y': [0, 0, 1, 0, 0, 1]}\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        k_on, k_off, delta = case['k_on'], case['k_off'], case['delta']\n        lambda_0, lambda_1, p = case['lambda_0'], case['lambda_1'], case['p']\n        y = case['y']\n        \n        # 1. HMM Parameters\n        # Initial state distribution (stationary distribution of CTMC)\n        k_sum = k_on + k_off\n        if k_on == 0 and k_off > 0: # Absorbing in state 0\n            pi = np.array([1.0, 0.0])\n        elif k_sum > 0:\n            pi = np.array([k_off / k_sum, k_on / k_sum])\n        else: # k_on = 0, k_off = 0\n            # Both states absorbing. Assume starts in one. Problem is ill-defined, but we can assume pi=[0.5, 0.5]\n            pi = np.array([0.5, 0.5])\n            \n        # Transition matrix A = exp(Q*delta)\n        A = np.zeros((2, 2))\n        if k_sum > 0:\n            e_ksd = np.exp(-k_sum * delta)\n            A[0, 0] = (k_off + k_on * e_ksd) / k_sum\n            A[0, 1] = (k_on - k_on * e_ksd) / k_sum\n            A[1, 0] = (k_off - k_off * e_ksd) / k_sum\n            A[1, 1] = (k_on + k_off * e_ksd) / k_sum\n        else: # k_on = 0, k_off = 0\n            A = np.array([[1.0, 0.0], [0.0, 1.0]])\n            \n        # Emission parameters (means of the Poisson distributions)\n        mu_0 = p * lambda_0 * delta\n        mu_1 = p * lambda_1 * delta\n        emission_params = [mu_0, mu_1]\n        \n        # 2. Run Forward Algorithm\n        log_likelihood = forward_algorithm(y, pi, A, emission_params)\n        results.append(log_likelihood)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}