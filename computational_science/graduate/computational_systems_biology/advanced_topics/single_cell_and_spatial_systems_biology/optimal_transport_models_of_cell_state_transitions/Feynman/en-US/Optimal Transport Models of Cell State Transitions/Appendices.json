{
    "hands_on_practices": [
        {
            "introduction": "Before applying computational models to biological data, it's crucial to understand the assumptions that justify the mathematical formulation. This exercise  explores a foundational step in modeling cell state transitions: normalizing raw cell counts from different time points into probability measures. By formalizing the biological assumptions required for this normalization, you will develop a deeper appreciation for how the abstract framework of balanced optimal transport can be responsibly applied to real-world single-cell data, where population sizes often change over time.",
            "id": "3335584",
            "problem": "Consider a developmental process in which a single cell population evolves in gene expression state space $\\mathcal{X} \\subset \\mathbb{R}^{d}$ from time $t_{0}$ to time $t_{1}$. At each time point, single-cell ribonucleic acid sequencing (scRNA-seq) yields a snapshot: at time $t_{0}$, $n$ cells with states $\\{x_{i}\\}_{i=1}^{n} \\subset \\mathcal{X}$, and at time $t_{1}$, $m$ cells with states $\\{y_{j}\\}_{j=1}^{m} \\subset \\mathcal{X}$. Assume the observed samples are generated by independent sampling from latent state distributions $\\mu^{\\star}$ and $\\nu^{\\star}$ over $\\mathcal{X}$ at times $t_{0}$ and $t_{1}$, respectively, but the absolute population sizes at these times may differ due to proliferation and death. You intend to use optimal transport (OT) to infer a minimal-cost coupling of states between $t_{0}$ and $t_{1}$.\n\nStarting from foundational definitions of a probability measure and the balanced OT requirement that the input measures have equal total mass, formulate biologically sound assumptions under which rescaling the empirical measures at $t_{0}$ and $t_{1}$ to have total mass $1$ preserves interpretable relative state frequencies and does not confound state-dependent dynamics. Your assumptions should be expressed in terms of state-independent sampling and state-independent net mass change across $\\mathcal{X}$ over the interval $[t_{0}, t_{1}]$.\n\nThen, under these assumptions and given $n \\neq m$, construct the empirical measures at $t_{0}$ and $t_{1}$ as discrete probability measures on $\\mathcal{X}$ supported on the observed cells. Compute the normalization constants that ensure each empirical measure has total mass $1$ in terms of $n$ and $m$.\n\nYour final answer must be the pair of normalization constants written as a single row matrix. No rounding is required, and no physical units apply.",
            "solution": "The problem addresses a foundational issue in the application of optimal transport (OT) to computational systems biology, specifically for modeling cell state transitions from time-course single-cell data. Standard (balanced) OT, as formulated in the Monge-Kantorovich problem, requires the two input measures to be probability measures, i.e., they must have equal total mass, normalized to $1$. However, experimental snapshots of a cell population at two different times, $t_0$ and $t_1$, typically yield different numbers of cells, $n$ and $m$ respectively, due to biological processes like proliferation and death, as well as sampling variability. This implies that the raw empirical measures have unequal total mass ($n$ and $m$). The task is to formulate the assumptions under which it is valid to simply rescale these empirical measures to have unit mass and then to determine the corresponding normalization constants.\n\nLet the true, unobserved population measure at time $t_0$ be $P_{t_0}$ and at time $t_1$ be $P_{t_1}$. These are non-negative measures on the state space $\\mathcal{X} \\subset \\mathbb{R}^{d}$. The total mass of each measure corresponds to the total number of cells in the population at that time: $\\int_{\\mathcal{X}} dP_{t_0} = N_0$ and $\\int_{\\mathcal{X}} dP_{t_1} = N_1$. The latent state distributions, $\\mu^{\\star}$ and $\\nu^{\\star}$, mentioned in the problem are the normalized versions of these population measures:\n$$\n\\mu^{\\star}(A) = \\frac{P_{t_0}(A)}{N_0} \\quad \\text{and} \\quad \\nu^{\\star}(A) = \\frac{P_{t_1}(A)}{N_1}\n$$\nfor any measurable set $A \\subset \\mathcal{X}$. By definition, $\\mu^{\\star}$ and $\\nu^{\\star}$ are probability measures. A dynamical process, which includes cell differentiation, proliferation, and death, maps the initial population to the final one. We can represent this by an operator $\\mathcal{T}$ such that $P_{t_1} = \\mathcal{T}(P_{t_0})$.\n\nTo justify the rescaling of empirical measures, we must formulate assumptions that decouple the change in total mass from the change in state distributions.\n\n**Formulation of Assumptions**\n\n1.  **State-Independent Sampling:** The sampling of $n$ cells at time $t_0$ and $m$ cells at time $t_1$ by scRNA-seq is a uniform random process. This means that each cell in the population at a given time has an equal probability of being measured, irrespective of its gene expression state $x \\in \\mathcal{X}$. This assumption ensures that the collected samples $\\{x_i\\}_{i=1}^n$ and $\\{y_j\\}_{j=1}^m$ are representative of the underlying populations. Consequently, the empirical probability distributions derived from these samples are consistent estimators of the true latent state distributions $\\mu^{\\star}$ and $\\nu^{\\star}$. That is, the normalized empirical measures converge to the true distributions as the sample sizes grow: $\\frac{1}{n}\\sum_{i=1}^n \\delta_{x_i} \\to \\mu^{\\star}$ and $\\frac{1}{m}\\sum_{j=1}^m \\delta_{y_j} \\to \\nu^{\\star}$.\n\n2.  **State-Independent Net Mass Change:** The net change in cell population size over the interval $[t_0, t_1]$ is due to a growth or death rate that is uniform across all cell states. This idealization means the operator $\\mathcal{T}$ describing the population dynamics can be decomposed into a product of a global scaling factor $g$ and a mass-preserving transport operator $\\mathcal{T}_{\\text{map}}$. The operator $\\mathcal{T}_{\\text{map}}$ describes only the transitions in state space (differentiation), while $g = N_1/N_0$ accounts for the total change in population size. Formally, for any initial measure $P$, we assume $\\mathcal{T}(P) = g \\cdot \\mathcal{T}_{\\text{map}}(P)$, where $\\mathcal{T}_{\\text{map}}$ preserves total mass: $\\int_{\\mathcal{X}} d(\\mathcal{T}_{\\text{map}}(P)) = \\int_{\\mathcal{X}} dP$.\n\nUnder these two assumptions, we can analyze the relationship between the true latent probability distributions $\\mu^{\\star}$ and $\\nu^{\\star}$:\n$$\n\\nu^{\\star} = \\frac{P_{t_1}}{N_1} = \\frac{\\mathcal{T}(P_{t_0})}{N_1} = \\frac{g \\cdot \\mathcal{T}_{\\text{map}}(P_{t_0})}{g \\cdot N_0} = \\frac{\\mathcal{T}_{\\text{map}}(N_0 \\cdot \\mu^{\\star})}{N_0} = \\mathcal{T}_{\\text{map}}(\\mu^{\\star})\n$$\nThe final equality holds because $\\mathcal{T}_{\\text{map}}$ is a linear operator. This derivation shows that the dynamics relating the latent probability distributions are governed solely by the mass-preserving transport map $\\mathcal{T}_{\\text{map}}$. Therefore, the problem of inferring cellular trajectories can be posed as a standard balanced OT problem between $\\mu^{\\star}$ and $\\nu^{\\star}$. Since state-independent sampling allows us to approximate $\\mu^{\\star}$ and $\\nu^{\\star}$ with normalized empirical measures, rescaling is a valid procedure to infer the relative state-transition dynamics embodied by $\\mathcal{T}_{\\text{map}}$, while information about the global growth factor $g$ is discarded.\n\n**Construction of Empirical Measures and Normalization Constants**\n\nGiven the assumptions above, we proceed to construct the discrete probability measures supported on the observed cells.\nThe data at time $t_0$ consists of $n$ cell states $\\{x_i\\}_{i=1}^n$. The empirical measure is a sum of Dirac delta measures, one at each observed state. To form a probability measure, which we denote $\\mu_n$, we assign a weight to each observation. Assuming each observation is equally representative, all weights are identical. Let this weight be $c_0$.\nThe empirical measure is thus:\n$$\n\\mu_n = \\sum_{i=1}^{n} c_0 \\delta_{x_i}\n$$\nwhere $\\delta_{x_i}$ is the Dirac measure supported at point $x_i$. For $\\mu_n$ to be a probability measure, its total mass must be $1$:\n$$\n\\int_{\\mathcal{X}} d\\mu_n(x) = \\int_{\\mathcal{X}} \\left( \\sum_{i=1}^{n} c_0 \\delta_{x_i} \\right) dx = \\sum_{i=1}^{n} c_0 \\int_{\\mathcal{X}} d\\delta_{x_i}(x) = \\sum_{i=1}^{n} c_0 = n c_0\n$$\nSetting the total mass to $1$ yields the equation for the normalization constant $c_0$:\n$$\nn c_0 = 1 \\implies c_0 = \\frac{1}{n}\n$$\nSimilarly, for the data at time $t_1$, consisting of $m$ cell states $\\{y_j\\}_{j=1}^m$, the empirical probability measure $\\nu_m$ is constructed by assigning an equal weight $c_1$ to each observation:\n$$\n\\nu_m = \\sum_{j=1}^{m} c_1 \\delta_{y_j}\n$$\nThe total mass of $\\nu_m$ must also be $1$:\n$$\n\\int_{\\mathcalX} d\\nu_m(y) = \\int_{\\mathcal{X}} \\left( \\sum_{j=1}^{m} c_1 \\delta_{y_j} \\right) dy = \\sum_{j=1}^{m} c_1 \\int_{\\mathcal{X}} d\\delta_{y_j}(y) = \\sum_{j=1}^{m} c_1 = m c_1\n$$\nSetting this to $1$ gives the normalization constant $c_1$:\n$$\nm c_1 = 1 \\implies c_1 = \\frac{1}{m}\n$$\nThe pair of normalization constants are therefore $\\frac{1}{n}$ for the measure at $t_0$ and $\\frac{1}{m}$ for the measure at $t_1$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{n}  \\frac{1}{m}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "This exercise  strips the optimal transport problem down to its essential components, using a minimal $2 \\times 2$ model of cell state transitions. By manually deriving the optimal transport plan, you will gain direct insight into how the relative positions of cell states, encoded in the cost matrix, determine the most efficient transition pathways. This practice illuminates the critical threshold at which a deterministic cell fate mapping (one-to-one) becomes degenerate with a stochastic one (one-to-many), a key concept in understanding cellular plasticity.",
            "id": "3335628",
            "problem": "Consider a minimal model of cell state transitions along a one-dimensional pseudotime axis in which two initial cell states at positions $x_{1} = 0$ and $x_{2} = 1$ evolve into two terminal states at positions $y_{1} = a$ and $y_{2} = 1 - a$, with $a \\in (0,1)$. Assume a balanced population: each initial state carries mass $1/2$, and each terminal state requires mass $1/2$. The transport cost between an initial state $x_{i}$ and a terminal state $y_{j}$ is the squared Euclidean distance $c_{ij} = (x_{i} - y_{j})^{2}$, modeling an energy-like penalty for large transcriptomic displacements.\n\nFormulate the problem as a balanced Kantorovich optimal transport problem: minimize the total transport cost $\\sum_{i=1}^{2} \\sum_{j=1}^{2} c_{ij} \\gamma_{ij}$ over couplings $\\gamma = (\\gamma_{ij})$ with row sums $\\sum_{j} \\gamma_{ij} = 1/2$ for each $i \\in \\{1,2\\}$ and column sums $\\sum_{i} \\gamma_{ij} = 1/2$ for each $j \\in \\{1,2\\}$.\n\nStarting only from the definitions of the balanced Kantorovich optimal transport problem, linearity of the objective, and feasibility constraints, derive the optimal coupling $\\gamma^{\\star}$ as a function of $a$, and characterize when the optimizer is induced by a single map (i.e., a deterministic plan supported on a permutation) versus when split couplings (i.e., non-deterministic plans that split mass) are also optimal.\n\nWhat is the exact critical value $a^{\\star}$ at which the optimal plan ceases to be uniquely induced by a single map and becomes non-unique, admitting split couplings as optimizers as well? Provide your answer as a single exact number with no units. Do not round.",
            "solution": "The problem requires us to find a critical value $a^{\\star}$ for a parameter in a one-dimensional optimal transport problem. The setup involves two initial states at positions $x_{1} = 0$ and $x_{2} = 1$, each with an initial mass of $1/2$. These states evolve into two terminal states at positions $y_{1} = a$ and $y_{2} = 1 - a$, each requiring a final mass of $1/2$. The parameter $a$ is defined on the open interval $(0,1)$. The transport cost between an initial point $x_i$ and a terminal point $y_j$ is the squared Euclidean distance, $c_{ij} = (x_i - y_j)^2$.\n\nFirst, we compute the entries of the $2 \\times 2$ cost matrix $C = (c_{ij})$:\n$c_{11} = (x_1 - y_1)^2 = (0 - a)^2 = a^2$\n$c_{12} = (x_1 - y_2)^2 = (0 - (1-a))^2 = (1-a)^2$\n$c_{21} = (x_2 - y_1)^2 = (1 - a)^2$\n$c_{22} = (x_2 - y_2)^2 = (1 - (1-a))^2 = a^2$\nThe cost matrix is therefore:\n$$ C = \\begin{pmatrix} a^2  (1-a)^2 \\\\ (1-a)^2  a^2 \\end{pmatrix} $$\nThe problem is to find a coupling matrix $\\gamma = (\\gamma_{ij})$ that minimizes the total transport cost, which is given by the linear objective function $C(\\gamma) = \\sum_{i=1}^{2} \\sum_{j=1}^{2} c_{ij} \\gamma_{ij}$. The coupling $\\gamma$ must satisfy a set of constraints defining a balanced transport plan:\n1. Non-negativity: $\\gamma_{ij} \\ge 0$ for all $i,j \\in \\{1,2\\}$.\n2. Row sum constraints (mass conservation at sources):\n   $\\gamma_{11} + \\gamma_{12} = 1/2$\n   $\\gamma_{21} + \\gamma_{22} = 1/2$\n3. Column sum constraints (mass conservation at targets):\n   $\\gamma_{11} + \\gamma_{21} = 1/2$\n   $\\gamma_{12} + \\gamma_{22} = 1/2$\n\nThese four linear equality constraints on the four variables $\\gamma_{ij}$ are not independent. The space of feasible couplings can be parameterized by a single variable. Let us set $\\gamma_{11} = \\epsilon$.\nFrom the first row constraint, $\\gamma_{12} = 1/2 - \\epsilon$.\nFrom the first column constraint, $\\gamma_{21} = 1/2 - \\epsilon$.\nFrom the second row constraint, $\\gamma_{22} = 1/2 - \\gamma_{21} = 1/2 - (1/2 - \\epsilon) = \\epsilon$.\nThis is consistent with the second column constraint: $\\gamma_{12} + \\gamma_{22} = (1/2 - \\epsilon) + \\epsilon = 1/2$.\nSo, any feasible coupling matrix can be expressed as:\n$$ \\gamma(\\epsilon) = \\begin{pmatrix} \\epsilon  \\frac{1}{2} - \\epsilon \\\\ \\frac{1}{2} - \\epsilon  \\epsilon \\end{pmatrix} $$\nThe non-negativity constraint $\\gamma_{ij} \\ge 0$ implies that $\\epsilon \\ge 0$ and $1/2 - \\epsilon \\ge 0$, which restricts $\\epsilon$ to the closed interval $[0, 1/2]$.\n\nThe total cost can now be written as a function of $\\epsilon$:\n$$ C(\\epsilon) = \\sum_{i,j} c_{ij} \\gamma_{ij}(\\epsilon) = c_{11}\\epsilon + c_{12}(\\frac{1}{2} - \\epsilon) + c_{21}(\\frac{1}{2} - \\epsilon) + c_{22}\\epsilon $$\nSubstituting the cost values:\n$$ C(\\epsilon) = a^2 \\epsilon + (1-a)^2 (\\frac{1}{2} - \\epsilon) + (1-a)^2 (\\frac{1}{2} - \\epsilon) + a^2 \\epsilon $$\n$$ C(\\epsilon) = 2a^2 \\epsilon + 2(1-a)^2 (\\frac{1}{2} - \\epsilon) = 2a^2 \\epsilon + (1-a)^2 - 2(1-a)^2 \\epsilon $$\n$$ C(\\epsilon) = [2a^2 - 2(1-a)^2] \\epsilon + (1-a)^2 $$\nThis is a linear function of $\\epsilon$. Let's analyze the coefficient of $\\epsilon$, which determines the slope of this function:\n$$ \\text{Slope} = 2a^2 - 2(1-a)^2 = 2(a^2 - (1-2a+a^2)) = 2(a^2 - 1 + 2a - a^2) = 2(2a - 1) $$\nThe total cost function is $C(\\epsilon) = 2(2a - 1)\\epsilon + (1-a)^2$.\nTo find the optimal coupling, we must minimize this linear function over the feasible domain $\\epsilon \\in [0, 1/2]$. The minimum of a linear function on a closed interval occurs at an endpoint, unless the function is constant.\n\nCase 1: The slope is positive. $2(2a - 1)  0 \\implies 2a  1 \\implies a  1/2$.\nIn this case, $C(\\epsilon)$ is minimized at the lower bound of the domain, $\\epsilon = 0$. The optimizer is unique, $\\epsilon^{\\star} = 0$, giving the coupling $\\gamma^{\\star} = \\begin{pmatrix} 0  1/2 \\\\ 1/2  0 \\end{pmatrix}$. This corresponds to a deterministic plan (an off-diagonal permutation) where $x_1 \\to y_2$ and $x_2 \\to y_1$.\n\nCase 2: The slope is negative. $2(2a - 1)  0 \\implies 2a  1 \\implies a  1/2$.\nIn this case, $C(\\epsilon)$ is minimized at the upper bound of the domain, $\\epsilon = 1/2$. The optimizer is unique, $\\epsilon^{\\star} = 1/2$, giving the coupling $\\gamma^{\\star} = \\begin{pmatrix} 1/2  0 \\\\ 0  1/2 \\end{pmatrix}$. This also corresponds to a deterministic plan (a diagonal permutation) where $x_1 \\to y_1$ and $x_2 \\to y_2$.\n\nCase 3: The slope is zero. $2(2a - 1) = 0 \\implies 2a = 1 \\implies a = 1/2$.\nIn this case, the cost function $C(\\epsilon) = (1-1/2)^2 = 1/4$ becomes constant for all $\\epsilon \\in [0, 1/2]$. Thus, any feasible coupling $\\gamma(\\epsilon)$ is an optimal plan. This includes the two deterministic plans at the endpoints ($\\epsilon=0$ and $\\epsilon=1/2$) as well as a continuum of \"split couplings\" for any $\\epsilon \\in (0, 1/2)$.\n\nThe problem asks for the critical value $a^{\\star}$ at which the optimal plan ceases to be uniquely induced by a single map. For $a \\neq 1/2$, the optimal plan is unique and deterministic. At $a=1/2$, the optimal plan becomes non-unique and admits non-deterministic solutions (split couplings). Therefore, this transition point is the critical value we seek.\n$$ a^{\\star} = \\frac{1}{2} $$",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "Biological systems are dynamic, with cell populations constantly changing due to proliferation and death. This exercise  moves beyond the constraint of mass conservation by introducing unbalanced optimal transport (UOT), a powerful extension for modeling such open systems. You will implement the generalized Sinkhorn algorithm, a cornerstone of modern computational OT, to find a transport plan that explicitly accounts for changes in total cell mass, providing a more faithful model of cell state transitions.",
            "id": "3335646",
            "problem": "You are given two discrete measures representing cell state distributions before and after a biological process in computational systems biology. The pre-process distribution has total mass $1$ (e.g., representing normalized cell counts at initial time), and the post-process distribution has total mass $1.5$ (e.g., including cell proliferation). The goal is to compute an unbalanced optimal transport plan that allows mass creation and destruction, using Kullback–Leibler divergence (KL) relaxation on the marginals and entropic regularization to ensure numerical stability, and then to quantify the fraction of created or destroyed mass.\n\nStart from the foundational definitions of optimal transport and its unbalanced extension:\n- Optimal transport (OT) seeks a nonnegative transport plan $\\gamma \\in \\mathbb{R}_{+}^{m \\times n}$ minimizing the expected cost of moving mass between two measures under mass conservation constraints.\n- Unbalanced optimal transport (UOT) relaxes the mass conservation using Kullback–Leibler divergence penalties to allow discrepancies between the plan marginals and the input measures, which is appropriate for biological processes involving cell death or proliferation.\n- Entropic regularization adds a negative entropy term to the objective to produce a strictly positive and numerically stable plan.\n\nIn this problem, the measures are discrete histograms of cell states supported on $m = 5$ and $n = 5$ points with coordinates $\\{x_i\\}_{i=1}^{5}$ and $\\{y_j\\}_{j=1}^{5}$ on the real line (representing a one-dimensional embedding of cell states along pseudotime). The ground cost between states is the squared Euclidean distance $c_{ij} = (x_i - y_j)^2$. Define the Gibbs kernel $K$ by $K_{ij} = \\exp\\left(-c_{ij}/\\varepsilon\\right)$, where $\\varepsilon  0$ is the entropic regularization parameter. Use the standard unbalanced entropic OT formulation with KL penalties on both marginals and relaxation parameter $\\tau  0$, yielding a plan of the form $\\gamma = \\mathrm{diag}(u)\\, K\\, \\mathrm{diag}(v)$ with positive scaling vectors $u \\in \\mathbb{R}_{+}^{m}$ and $v \\in \\mathbb{R}_{+}^{n}$ obtained by generalized Sinkhorn iterations.\n\nYour program must:\n1. Implement the generalized Sinkhorn algorithm for unbalanced OT with Kullback–Leibler divergence penalties on both marginals using the following fixed-point iterations. Given $K$, input measures $a \\in \\mathbb{R}_{+}^{m}$ and $b \\in \\mathbb{R}_{+}^{n}$, entropic parameter $\\varepsilon  0$, and relaxation parameter $\\tau  0$, update scaling vectors $u$ and $v$ according to\n   $$u \\leftarrow \\left(\\frac{a}{K v}\\right)^{\\frac{\\tau}{\\tau + \\varepsilon}}, \\quad v \\leftarrow \\left(\\frac{b}{K^{\\top} u}\\right)^{\\frac{\\tau}{\\tau + \\varepsilon}},$$\n   where the divisions are elementwise and the exponent is applied elementwise. Iterate until convergence to a user-defined tolerance or until a maximum iteration count is reached.\n2. Compute the plan $\\gamma = \\mathrm{diag}(u)\\, K\\, \\mathrm{diag}(v)$ implicitly via its marginals. The transported mass leaving the source is $s = \\gamma \\mathbf{1}_n = u \\odot (K v)$ and the transported mass arriving to the target is $t = \\gamma^{\\top} \\mathbf{1}_m = v \\odot (K^{\\top} u)$, where $\\odot$ denotes elementwise multiplication and $\\mathbf{1}_k$ denotes the $k$-dimensional all-ones vector.\n3. Let the total transported mass be $M = \\sum_{i=1}^{m} s_i = \\sum_{j=1}^{n} t_j$. Quantify mass creation relative to the source as\n   $$f_{\\mathrm{create,source}} = \\max\\left(0, \\frac{M - \\sum_{i=1}^{m} a_i}{\\sum_{i=1}^{m} a_i}\\right),$$\n   and mass destruction relative to the target as\n   $$f_{\\mathrm{destroy,target}} = \\max\\left(0, \\frac{\\sum_{j=1}^{n} b_j - M}{\\sum_{j=1}^{n} b_j}\\right).$$\n   Both quantities must be expressed as decimals (floats).\n\nUse the following test suite, which specifies the measures and parameters:\n- Test case 1 (general \"happy path\"): \n  - Source support: $x = [0.0, 0.25, 0.5, 0.75, 1.0]$\n  - Target support: $y = [0.1, 0.35, 0.6, 0.85, 1.1]$\n  - Source measure: $a = [0.05, 0.15, 0.4, 0.3, 0.1]$ (sums to $1$)\n  - Target measure: $b = [0.1, 0.2, 0.5, 0.5, 0.2]$ (sums to $1.5$)\n  - Entropic parameter: $\\varepsilon = 0.5$\n  - KL relaxation parameter: $\\tau = 0.5$\n- Test case 2 (near-balanced relaxation boundary):\n  - Same $x$, $y$, $a$, $b$, and $\\varepsilon$ as Test case 1\n  - KL relaxation parameter: $\\tau = 10.0$\n- Test case 3 (edge case with large inter-state separation):\n  - Source support: $x = [0.0, 0.1, 0.2, 0.3, 0.4]$\n  - Target support: $y = [2.0, 2.1, 2.2, 2.3, 2.4]$\n  - Source measure: $a = [0.1, 0.2, 0.3, 0.25, 0.15]$ (sums to $1$)\n  - Target measure: $b = [0.3, 0.35, 0.4, 0.25, 0.2]$ (sums to $1.5$)\n  - Entropic parameter: $\\varepsilon = 0.5$\n  - KL relaxation parameter: $\\tau = 0.01$\n\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, where each element is itself a list of three floats $[M, f_{\\mathrm{create,source}}, f_{\\mathrm{destroy,target}}]$ for the corresponding test case. For example, the output format must be\n$$[ [M_1, f_{\\mathrm{create,source},1}, f_{\\mathrm{destroy,target},1}], [M_2, f_{\\mathrm{create,source},2}, f_{\\mathrm{destroy,target},2}], [M_3, f_{\\mathrm{create,source},3}, f_{\\mathrm{destroy,target},3}] ].$$\nNo physical units or angle units are involved; all quantities are dimensionless. All printed numbers must be floats.",
            "solution": "The problem is valid. It is scientifically grounded in the established mathematical theory of unbalanced optimal transport, well-posed with all necessary information provided, and objective in its formulation.\n\nThe task is to compute the total transported mass and the fractions of created and destroyed mass for an unbalanced optimal transport (UOT) problem between two discrete distributions. This problem is framed in the context of computational systems biology, where such distributions represent cell states, and UOT models transitions involving cell proliferation or death. The solution is found using a generalized Sinkhorn algorithm.\n\nThe foundational UOT problem seeks to find a transport plan $\\gamma \\in \\mathbb{R}_{+}^{m \\times n}$ that minimizes an objective function incorporating a transport cost, an entropic regularizer, and penalties for deviation from the input marginal measures $a \\in \\mathbb{R}_{+}^{m}$ and $b \\in \\mathbb{R}_{+}^{n}$. The objective function is:\n$$ \\mathcal{L}(\\gamma) = \\sum_{i=1}^m \\sum_{j=1}^n \\gamma_{ij} c_{ij} - \\varepsilon H(\\gamma) + \\tau \\mathrm{KL}(\\gamma \\mathbf{1}_n | a) + \\tau \\mathrm{KL}(\\gamma^{\\top} \\mathbf{1}_m | b) $$\nHere, $c_{ij}$ is the ground cost, $\\varepsilon > 0$ is the entropic regularization parameter, $\\tau > 0$ is the relaxation parameter for the Kullback–Leibler (KL) divergence penalty, $H(\\gamma) = -\\sum_{ij} \\gamma_{ij}(\\log \\gamma_{ij} - 1)$ is the entropy, and $\\gamma \\mathbf{1}_n$ and $\\gamma^{\\top} \\mathbf{1}_m$ are the marginals of the plan $\\gamma$.\n\nThe solution to this minimization problem has a specific structure: $\\gamma_{ij} = u_i K_{ij} v_j$, or in matrix form, $\\gamma = \\mathrm{diag}(u) K \\mathrm{diag}(v)$, where $u \\in \\mathbb{R}_{+}^{m}$ and $v \\in \\mathbb{R}_{+}^{n}$ are positive scaling vectors and $K$ is the Gibbs kernel, defined as $K_{ij} = \\exp(-c_{ij}/\\varepsilon)$. The scaling vectors $u$ and $v$ are found by iterating a set of fixed-point update rules, an approach known as the generalized Sinkhorn algorithm.\n\nThe solution process is as follows:\n\n1.  **Compute the Cost Matrix and Gibbs Kernel**:\n    First, we construct the cost matrix $C \\in \\mathbb{R}^{m \\times n}$. The cost $c_{ij}$ represents the effort to move a unit of mass from state $x_i$ to state $y_j$. As specified, the cost is the squared Euclidean distance: $c_{ij} = (x_i - y_j)^2$. Given the one-dimensional supports $x = \\{x_i\\}_{i=1}^{5}$ and $y = \\{y_j\\}_{j=1}^{5}$, this is computed for all pairs $(i, j)$.\n    Next, the Gibbs kernel $K$ is computed from the cost matrix $C$ and the entropic regularization parameter $\\varepsilon$:\n    $$ K_{ij} = \\exp\\left(-\\frac{c_{ij}}{\\varepsilon}\\right) $$\n    The kernel $K$ transforms the costs into similarities, where larger costs result in smaller kernel entries.\n\n2.  **Generalized Sinkhorn Iterations**:\n    The core of the solution is to find the scaling vectors $u$ and $v$. We initialize them, for instance, as vectors of ones. The vectors are then updated iteratively until they converge. The update rules, as given in the problem, are derived from the Karush-Kuhn-Tucker (KKT) conditions of the UOT optimization problem:\n    $$ u \\leftarrow \\left(\\frac{a}{K v}\\right)^{\\frac{\\tau}{\\tau + \\varepsilon}} $$\n    $$ v \\leftarrow \\left(\\frac{b}{K^{\\top} u}\\right)^{\\frac{\\tau}{\\tau + \\varepsilon}} $$\n    In these expressions, $Kv$ and $K^{\\top}u$ are matrix-vector products, divisions are performed elementwise, and the exponent is applied elementwise to the resulting vectors. It is convenient to pre-compute the exponent $p = \\frac{\\tau}{\\tau + \\varepsilon}$. The iterations continue until the change between successive updates of a scaling vector (e.g., the relative L2-norm of the difference) falls below a predefined tolerance, or a maximum number of iterations is reached.\n\n3.  **Compute Final Quantities**:\n    Upon convergence of $u$ and $v$, the optimal transport plan $\\gamma$ is implicitly defined. We do not need to construct this potentially large matrix explicitly. Instead, we can directly compute its row and column sums (the plan's marginals).\n    The mass leaving the source states is the vector $s = u \\odot (K v)$, where $\\odot$ denotes elementwise (Hadamard) product.\n    The mass arriving at the target states is the vector $t = v \\odot (K^{\\top} u)$.\n    The total mass transported by the plan, $M$, is the sum of all elements in $\\gamma$, which is equivalent to summing the elements of either marginal: $M = \\sum_{i=1}^{m} s_i = \\sum_{j=1}^{n} t_j$.\n\n    With the total transported mass $M$ computed, we can quantify the mass creation and destruction as relative fractions:\n    -   The fraction of mass created, relative to the total mass of the source distribution $a$, is:\n        $$ f_{\\mathrm{create,source}} = \\max\\left(0, \\frac{M - \\sum_{i=1}^{m} a_i}{\\sum_{i=1}^{m} a_i}\\right) $$\n    -   The fraction of mass destroyed, relative to the total mass of the target distribution $b$, is:\n        $$ f_{\\mathrm{destroy,target}} = \\max\\left(0, \\frac{\\sum_{j=1}^{n} b_j - M}{\\sum_{j=1}^{n} b_j}\\right) $$\n    These formulas capture the net change in mass facilitated by the transport plan.\n\nThis entire procedure is applied to each of the three provided test cases, which differ in their parameters and state locations, to obtain the final results.",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef unbalanced_ot_sinkhorn(x, y, a, b, epsilon, tau, max_iter=1000, tol=1e-9):\n    \"\"\"\n    Computes the unbalanced optimal transport plan using the generalized Sinkhorn algorithm.\n\n    Args:\n        x (np.ndarray): Source support points (m,).\n        y (np.ndarray): Target support points (n,).\n        a (np.ndarray): Source measure (m,).\n        b (np.ndarray): Target measure (n,).\n        epsilon (float): Entropic regularization parameter.\n        tau (float): KL relaxation parameter.\n        max_iter (int): Maximum number of iterations.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        list: A list containing [M, f_create_source, f_destroy_target].\n    \"\"\"\n    m, n = a.shape[0], b.shape[0]\n\n    # Reshape for cdist\n    x_col = x.reshape(-1, 1)\n    y_col = y.reshape(-1, 1)\n\n    # 1. Compute Cost Matrix and Gibbs Kernel\n    C = cdist(x_col, y_col, 'sqeuclidean')\n    K = np.exp(-C / epsilon)\n\n    # 2. Generalized Sinkhorn Iterations\n    u = np.ones(m)\n    v = np.ones(n)\n    p = tau / (tau + epsilon)\n\n    for i in range(max_iter):\n        u_old = u.copy()\n        \n        # Update u\n        Kv = K @ v\n        # Add a small constant to avoid division by zero\n        Kv[Kv == 0] = 1e-16\n        u = (a / Kv)**p\n\n        # Update v\n        KTu = K.T @ u\n        # Add a small constant to avoid division by zero\n        KTu[KTu == 0] = 1e-16\n        v = (b / KTu)**p\n        \n        # Check for convergence\n        err = np.linalg.norm(u - u_old) / (np.linalg.norm(u) + 1e-9)\n        if err  tol:\n            break\n\n    # 3. Compute Final Quantities\n    # Transported mass leaving the source: s = u * (K @ v)\n    s = u * (K @ v)\n    \n    # Total transported mass\n    M = np.sum(s)\n\n    sum_a = np.sum(a)\n    sum_b = np.sum(b)\n    \n    # Mass creation relative to source\n    f_create_source = max(0.0, (M - sum_a) / sum_a)\n    \n    # Mass destruction relative to target\n    f_destroy_target = max(0.0, (sum_b - M) / sum_b)\n\n    return [M, f_create_source, f_destroy_target]\n\n\ndef solve():\n    \"\"\"\n    Solves the unbalanced optimal transport problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"x\": np.array([0.0, 0.25, 0.5, 0.75, 1.0]),\n            \"y\": np.array([0.1, 0.35, 0.6, 0.85, 1.1]),\n            \"a\": np.array([0.05, 0.15, 0.4, 0.3, 0.1]),\n            \"b\": np.array([0.1, 0.2, 0.5, 0.5, 0.2]),\n            \"epsilon\": 0.5,\n            \"tau\": 0.5\n        },\n        {\n            \"x\": np.array([0.0, 0.25, 0.5, 0.75, 1.0]),\n            \"y\": np.array([0.1, 0.35, 0.6, 0.85, 1.1]),\n            \"a\": np.array([0.05, 0.15, 0.4, 0.3, 0.1]),\n            \"b\": np.array([0.1, 0.2, 0.5, 0.5, 0.2]),\n            \"epsilon\": 0.5,\n            \"tau\": 10.0\n        },\n        {\n            \"x\": np.array([0.0, 0.1, 0.2, 0.3, 0.4]),\n            \"y\": np.array([2.0, 2.1, 2.2, 2.3, 2.4]),\n            \"a\": np.array([0.1, 0.2, 0.3, 0.25, 0.15]),\n            \"b\": np.array([0.3, 0.35, 0.4, 0.25, 0.2]),\n            \"epsilon\": 0.5,\n            \"tau\": 0.01\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = unbalanced_ot_sinkhorn(\n            case[\"x\"],\n            case[\"y\"],\n            case[\"a\"],\n            case[\"b\"],\n            case[\"epsilon\"],\n            case[\"tau\"]\n        )\n        results.append(result)\n\n    # Format the output string as a list of lists of floats\n    # e.g., [[M1, f1_c, f1_d], [M2, f2_c, f2_d], [M3, f3_c, f3_d]]\n    # The template's format is a bit tricky, this builds it robustly.\n    # `str(list)` introduces spaces, which are present in the example.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}