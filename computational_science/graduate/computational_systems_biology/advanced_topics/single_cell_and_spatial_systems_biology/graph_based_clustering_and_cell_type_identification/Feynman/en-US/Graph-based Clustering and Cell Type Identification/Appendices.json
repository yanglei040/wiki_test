{
    "hands_on_practices": [
        {
            "introduction": "A central challenge in identifying cell types from a similarity graph is to define what constitutes a \"good\" cluster. Modularity provides a powerful, quantitative answer to this question by measuring how densely connected the nodes within a community are, compared to a random configuration. This exercise  makes this abstract concept concrete by asking you to calculate the modularity gain of separating a suspected rare cell population, a foundational skill for interpreting the output of many community detection algorithms.",
            "id": "3317980",
            "problem": "A single-cell RNA sequencing (scRNA-seq) experiment yields a cell-cell similarity graph built by $k$-Nearest Neighbors ($k$-NN) followed by symmetrization into an undirected, unweighted graph. You suspect a rare cell state forming a small, dense subgraph that is connected to a larger, more heterogeneous community. Consider the following abstracted graph-level summary, which is consistent with this scenario and derived from quality-controlled data:\n\n- The graph is undirected with a total of $m = 100$ edges.\n- A candidate rare subgraph $S$ contains $l_{S} = 30$ edges internal to $S$.\n- The large community $L$ contains $l_{L} = 60$ edges internal to $L$.\n- There are $e_{SL} = 10$ edges that connect nodes in $S$ to nodes in $L$.\n\nAssume the Newmanâ€“Girvan modularity under the configuration-model null for undirected graphs. Compute the modularity gain $\\Delta Q$ when partitioning the graph into two communities $\\{S, L\\}$ compared to treating all nodes as a single community. Based on the sign of $\\Delta Q$, briefly decide whether the rare subgraph $S$ forms its own cluster under modularity maximization. Express your final answer for $\\Delta Q$ as an exact fraction with no rounding and no units.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- The graph is undirected and unweighted.\n- Total number of edges: $m = 100$.\n- The graph is partitioned into two communities, a candidate rare subgraph $S$ and a large community $L$.\n- Number of edges internal to community $S$: $l_{S} = 30$.\n- Number of edges internal to community $L$: $l_{L} = 60$.\n- Number of edges connecting community $S$ to community $L$: $e_{SL} = 10$.\n- The null model is the configuration model for undirected graphs.\n- The task is to compute the modularity gain $\\Delta Q$ for the partition $\\{S, L\\}$ relative to a single community.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Groundedness**: The problem is well-grounded in computational systems biology and network science. Modularity, as defined by Newman and Girvan, is a standard metric for community detection, and the configuration model is a standard null model. The scenario described is a common problem in the analysis of scRNA-seq data.\n- **Well-Posedness**: The problem is well-posed. It provides all necessary information to calculate the modularity for the specified partition. The question is unambiguous.\n- **Consistency**: The problem's givens are internally consistent. The total number of edges $m$ must be the sum of edges internal to each community and the edges connecting them. We can verify this: $l_{S} + l_{L} + e_{SL} = 30 + 60 + 10 = 100$, which matches the given total $m=100$.\n- **Objectivity**: The problem is stated in precise, objective language.\n- **Conclusion**: The problem is valid, as it is scientifically sound, self-contained, consistent, and well-posed.\n\n### Solution Derivation\nThe modularity, $Q$, of a partitioned graph is defined as the fraction of edges that fall within the given communities minus the expected value of the same fraction in a randomized graph with the same degree sequence (the configuration model). The formula for a partition into a set of communities $\\{c\\}$ is:\n$$Q = \\sum_{c} \\left[ \\frac{l_c}{m} - \\left(\\frac{d_c}{2m}\\right)^2 \\right]$$\nwhere:\n- $m$ is the total number of edges in the graph.\n- $l_c$ is the number of edges completely within community $c$.\n- $d_c$ is the sum of the degrees of all nodes in community $c$.\n\nThe problem asks for the modularity gain, $\\Delta Q$, when partitioning the graph into two communities, $\\{S, L\\}$, compared to the baseline of treating all nodes as a single community, $\\{U\\}$.\n$$\\Delta Q = Q_{\\{S,L\\}} - Q_{\\{U\\}}$$\nFirst, we consider the case of a single community $U$ containing all nodes. In this case, all edges are internal, so $l_U = m$. The sum of degrees for all nodes is $d_U = 2m$. The modularity is:\n$$Q_{\\{U\\}} = \\frac{l_U}{m} - \\left(\\frac{d_U}{2m}\\right)^2 = \\frac{m}{m} - \\left(\\frac{2m}{2m}\\right)^2 = 1 - 1^2 = 0$$\nThe modularity of an unpartitioned graph is always $0$. Therefore, the modularity gain is simply the modularity of the proposed partition:\n$$\\Delta Q = Q_{\\{S,L\\}} - 0 = Q_{\\{S,L\\}}$$\nNow, we calculate $Q_{\\{S,L\\}}$. The formula expands to:\n$$Q_{\\{S,L\\}} = \\left[ \\frac{l_S}{m} - \\left(\\frac{d_S}{2m}\\right)^2 \\right] + \\left[ \\frac{l_L}{m} - \\left(\\frac{d_L}{2m}\\right)^2 \\right]$$\nWe are given $m=100$, $l_S=30$, and $l_L=60$. We need to find $d_S$ and $d_L$. The sum of degrees for a community's nodes, $d_c$, can be calculated by summing the degrees of its member nodes. This sum is equal to twice the number of internal edges plus the number of a community's edges that connect to other communities.\n\nFor community $S$:\nThe number of internal edges is $l_S = 30$. The number of external edges is $e_{SL} = 10$.\nThe sum of degrees is $d_S = 2l_S + e_{SL} = 2(30) + 10 = 60 + 10 = 70$.\n\nFor community $L$:\nThe number of internal edges is $l_L = 60$. The number of external edges is also $e_{SL} = 10$.\nThe sum of degrees is $d_L = 2l_L + e_{SL} = 2(60) + 10 = 120 + 10 = 130$.\n\nAs a consistency check, the total sum of degrees in the graph must be $d_S + d_L = 70 + 130 = 200$. This is equal to $2m = 2(100) = 200$, so our calculations are correct.\n\nNow, we substitute these values into the modularity formula:\n$$\\Delta Q = Q_{\\{S,L\\}} = \\left[ \\frac{30}{100} - \\left(\\frac{70}{2 \\cdot 100}\\right)^2 \\right] + \\left[ \\frac{60}{100} - \\left(\\frac{130}{2 \\cdot 100}\\right)^2 \\right]$$\n$$\\Delta Q = \\left[ \\frac{3}{10} - \\left(\\frac{70}{200}\\right)^2 \\right] + \\left[ \\frac{6}{10} - \\left(\\frac{130}{200}\\right)^2 \\right]$$\n$$\\Delta Q = \\left[ \\frac{3}{10} - \\left(\\frac{7}{20}\\right)^2 \\right] + \\left[ \\frac{6}{10} - \\left(\\frac{13}{20}\\right)^2 \\right]$$\n$$\\Delta Q = \\left[ \\frac{3}{10} - \\frac{49}{400} \\right] + \\left[ \\frac{6}{10} - \\frac{169}{400} \\right]$$\nCombine terms:\n$$\\Delta Q = \\frac{3}{10} + \\frac{6}{10} - \\frac{49}{400} - \\frac{169}{400}$$\n$$\\Delta Q = \\frac{9}{10} - \\frac{49 + 169}{400}$$\n$$\\Delta Q = \\frac{9}{10} - \\frac{218}{400}$$\nTo subtract, we find a common denominator, which is $400$:\n$$\\Delta Q = \\frac{9 \\cdot 40}{10 \\cdot 40} - \\frac{218}{400} = \\frac{360}{400} - \\frac{218}{400}$$\n$$\\Delta Q = \\frac{360 - 218}{400} = \\frac{142}{400}$$\nFinally, we simplify the fraction by dividing the numerator and denominator by their greatest common divisor, which is $2$:\n$$\\Delta Q = \\frac{71}{200}$$\nThe problem also asks to decide whether $S$ forms its own cluster based on the sign of $\\Delta Q$. Since $\\Delta Q = \\frac{71}{200} > 0$, the modularity of the partition $\\{S, L\\}$ is positive. This means that the density of edges within communities $S$ and $L$ is greater than what would be expected by random chance under the configuration model. Modularity maximization algorithms seek partitions with high positive $Q$ values. A positive $\\Delta Q$ relative to the unpartitioned state indicates that the proposed partition is a structurally meaningful improvement. Therefore, based on this metric, the subgraph $S$ does form its own distinct cluster.",
            "answer": "$$\\boxed{\\frac{71}{200}}$$"
        },
        {
            "introduction": "While modularity gives us a score for a given partition, biological reality is often multi-scale; we might be interested in broad cell families or highly specific subtypes. The resolution parameter $\\gamma$ provides a \"tuning knob\" to control the granularity of clusters detected by modularity optimization, allowing us to explore this hierarchy. This practice  delves into this crucial concept, challenging you to derive the critical resolution required to resolve a small, rare cluster from a larger one.",
            "id": "3317960",
            "problem": "A common approach to cell type identification in single-cell RNA sequencing (scRNA-seq) is to construct a cell-cell similarity graph and then perform community detection using modularity maximization with a tunable resolution parameter $\\gamma$. Consider an undirected, unweighted toy graph modeling a scRNA-seq k-nearest neighbor similarity: a larger component is a complete graph (clique) of size $L$ and a rare component is a smaller complete graph of size $S$. These two components are connected by exactly one inter-component edge between a single node in the large clique and a single node in the small clique. Let $L=12$ and $S=4$. Let $A_{ij}$ be the adjacency matrix, $k_{i}$ the degree of node $i$, $m$ the total number of edges, and $\\delta(c_{i},c_{j})$ the Kronecker delta indicating whether nodes $i$ and $j$ belong to the same community. The generalized Newmanâ€“Girvan modularity with resolution parameter $\\gamma$ is defined by\n$$\nQ(\\gamma)=\\frac{1}{2m}\\sum_{i}\\sum_{j}\\left(A_{ij}-\\gamma\\frac{k_i k_j}{2m}\\right)\\delta(c_i,c_j).\n$$\nAssume no self-loops and symmetry $A_{ij}=A_{ji}$. Using only the definition above and first principles about degrees and edge counts in cliques, compute $Q(\\gamma)$ for two partitions: (i) all $L+S$ nodes in a single community, and (ii) two communities where the rare $S$-node clique is separated from the $L$-node clique. Derive the exact critical resolution value $\\gamma_{c}$ at which the two-community partition has strictly larger modularity than the single-community partition. Provide your final answer for $\\gamma_{c}$ as a single exact rational expression. State clearly the inequality that characterizes the range of $\\gamma$ that separates the rare clique, but report only $\\gamma_{c}$ in your final answer.",
            "solution": "The problem statement is a valid, well-posed question in the domain of graph theory as applied to computational systems biology. It is scientifically grounded, objective, and provides all necessary information to derive a unique solution.\n\nThe problem asks for the critical resolution parameter $\\gamma_c$ at which a two-community partition of a specific graph becomes more favorable (i.e., has a higher modularity score) than a single-community partition. The graph consists of a complete graph (clique) of size $L=12$ and a smaller clique of size $S=4$, connected by a single edge.\n\nFirst, we characterize the graph's properties.\nLet the large clique be $C_L'$ and the small clique be $C_S'$.\nThe number of nodes is $N = L+S = 12+4=16$.\nThe number of edges within the $L$-clique is $\\frac{L(L-1)}{2} = \\frac{12(11)}{2} = 66$.\nThe number of edges within the $S$-clique is $\\frac{S(S-1)}{2} = \\frac{4(3)}{2} = 6$.\nThere is one inter-component edge connecting the two cliques.\nThe total number of edges, $m$, in the graph is the sum of these, so $m = 66 + 6 + 1 = 73$.\n\nNext, we determine the degrees $k_i$ of all nodes $i$.\nFor the nodes in the larger clique of size $L$:\n- $L-1 = 11$ nodes are connected only to other nodes within the clique. Their degree is $k_i = L-1 = 11$.\n- One node is connected to the other $L-1$ nodes in its clique and also to one node in the smaller clique. Its degree is $k_i = (L-1) + 1 = L = 12$.\n\nFor the nodes in the smaller clique of size $S$:\n- $S-1 = 3$ nodes are connected only to other nodes within the clique. Their degree is $k_i = S-1 = 3$.\n- One node is connected to the other $S-1$ nodes in its clique and also to the node in the larger clique. Its degree is $k_i = (S-1) + 1 = S = 4$.\n\nThe sum of all degrees is $\\sum_i k_i = (11 \\times 11) + (1 \\times 12) + (3 \\times 3) + (1 \\times 4) = 121 + 12 + 9 + 4 = 146$. This matches $2m = 2 \\times 73 = 146$, as expected.\n\nThe modularity $Q(\\gamma)$ is given by:\n$$Q(\\gamma)=\\frac{1}{2m}\\sum_{i}\\sum_{j}\\left(A_{ij}-\\gamma\\frac{k_i k_j}{2m}\\right)\\delta(c_i,c_j)$$\nThis can be rewritten by summing over communities $c$:\n$$Q(\\gamma) = \\sum_c \\left[ \\frac{e_c}{m} - \\gamma \\left(\\frac{d_c}{2m}\\right)^2 \\right]$$\nwhere $e_c$ is the number of edges within community $c$, and $d_c = \\sum_{i \\in c} k_i$ is the sum of the degrees of the nodes in community $c$.\n\nWe will now compute the modularity for the two specified partitions.\n\nCase (i): All $L+S$ nodes in a single community.\nLet this partition be $P_1$. There is only one community, $c_1$, which is the entire graph.\n- The number of edges within this community is the total number of edges, so $e_1 = m = 73$.\n- The sum of degrees of nodes in this community is the sum of all degrees, so $d_1 = \\sum_i k_i = 2m = 146$.\n\nThe modularity $Q_1(\\gamma)$ for this partition is:\n$$Q_1(\\gamma) = \\frac{e_1}{m} - \\gamma \\left(\\frac{d_1}{2m}\\right)^2 = \\frac{m}{m} - \\gamma \\left(\\frac{2m}{2m}\\right)^2 = 1 - \\gamma$$\n\nCase (ii): Two communities, one for the $L$-node clique ($C_L$) and one for the $S$-node clique ($C_S$).\nLet this partition be $P_2$. The single inter-component edge is cut by this partition.\n- Community $C_L$ consists of the $L=12$ nodes of the larger clique. The number of edges entirely within this community, $e_L$, is the number of edges in a complete graph of size $L$.\n$e_L = \\frac{L(L-1)}{2} = \\frac{12(11)}{2} = 66$.\n- Community $C_S$ consists of the $S=4$ nodes of the smaller clique. The number of edges entirely within this community, $e_S$, is the number of edges in a complete graph of size $S$.\n$e_S = \\frac{S(S-1)}{2} = \\frac{4(3)}{2} = 6$.\n\nThe sum of degrees for each community, $d_L$ and $d_S$, uses the degrees $k_i$ from the full graph.\n- For $C_L$: $d_L = \\sum_{i \\in C_L} k_i = (L-1)(L-1) + L = (11)(11) + 12 = 121 + 12 = 133$.\n- For $C_S$: $d_S = \\sum_{i \\in C_S} k_i = (S-1)(S-1) + S = (3)(3) + 4 = 9 + 4 = 13$.\nAs a check, $d_L + d_S = 133 + 13 = 146 = 2m$.\n\nThe modularity $Q_2(\\gamma)$ for this partition is the sum of contributions from each community:\n$$Q_2(\\gamma) = \\left[ \\frac{e_L}{m} - \\gamma \\left(\\frac{d_L}{2m}\\right)^2 \\right] + \\left[ \\frac{e_S}{m} - \\gamma \\left(\\frac{d_S}{2m}\\right)^2 \\right]$$\n$$Q_2(\\gamma) = \\frac{e_L + e_S}{m} - \\gamma \\left[ \\left(\\frac{d_L}{2m}\\right)^2 + \\left(\\fracd_S}{2m}\\right)^2 \\right]$$\nSubstituting the values we calculated:\n$e_L + e_S = 66 + 6 = 72$.\n$m=73$.\n$d_L = 133$.\n$d_S = 13$.\n$2m = 146$.\n$$Q_2(\\gamma) = \\frac{72}{73} - \\gamma \\left[ \\frac{133^2 + 13^2}{146^2} \\right]$$\n\nThe two-community partition is preferred when $Q_2(\\gamma) > Q_1(\\gamma)$. The critical resolution $\\gamma_c$ is the value where the two modularities are equal, i.e., $Q_2(\\gamma_c) = Q_1(\\gamma_c)$.\n$$ \\frac{e_L + e_S}{m} - \\gamma_c \\frac{d_L^2 + d_S^2}{(2m)^2} = 1 - \\gamma_c $$\nRearranging to solve for $\\gamma_c$:\n$$ \\gamma_c \\left( 1 - \\frac{d_L^2 + d_S^2}{(2m)^2} \\right) = 1 - \\frac{e_L + e_S}{m} $$\nThe left side term in the parenthesis can be simplified using $2m = d_L + d_S$:\n$$ 1 - \\frac{d_L^2 + d_S^2}{(d_L+d_S)^2} = \\frac{(d_L+d_S)^2 - (d_L^2 + d_S^2)}{(d_L+d_S)^2} = \\frac{2d_L d_S}{(d_L+d_S)^2} = \\frac{2d_L d_S}{(2m)^2} $$\nThe right side of the equation is:\n$$ 1 - \\frac{e_L + e_S}{m} = \\frac{m - (e_L + e_S)}{m} $$\nSince $e_L+e_S$ represents all edges except the one connecting edge, $m = (e_L+e_S)+1$, so $m - (e_L+e_S) = 1$. The right side is $\\frac{1}{m}$.\nSubstituting these back into the equation for $\\gamma_c$:\n$$ \\gamma_c \\left( \\frac{2d_L d_S}{(2m)^2} \\right) = \\frac{1}{m} $$\n$$ \\gamma_c = \\frac{1}{m} \\frac{(2m)^2}{2d_L d_S} = \\frac{4m^2}{2m d_L d_S} = \\frac{2m}{d_L d_S} $$\nThis provides a general formula for the critical resolution in this graph topology. The condition to separate the rare clique is $\\gamma > \\gamma_c = \\frac{2m}{d_L d_S}$.\n\nNow, we substitute the numerical values:\n$m=73$, $d_L=133$, $d_S=13$.\n$$ \\gamma_c = \\frac{2 \\times 73}{133 \\times 13} = \\frac{146}{1729} $$\nThe numerator is $2 \\times 73$. The denominator is not divisible by $2$ or $73$ ($1729 = 23 \\times 73 + 20$). Therefore, the fraction is irreducible.\n\nThe range of $\\gamma$ for which the two-community partition has strictly larger modularity is $\\gamma > \\frac{146}{1729}$. The critical value is $\\gamma_c$.",
            "answer": "$$\\boxed{\\frac{146}{1729}}$$"
        },
        {
            "introduction": "Beyond modularity optimization, spectral clustering offers another powerful framework for dissecting a graph's structure, using the eigenvalues of the graph Laplacian to reveal its underlying connectivity. This approach is particularly useful for automatically estimating the number of clusters by identifying significant \"spectral gaps.\" This hands-on coding exercise  guides you through a complete analysis pipeline, from constructing a $k$-NN graph and selecting an optimal $k$ to evaluating clustering performance on datasets that model both discrete cell types and continuous developmental lineages.",
            "id": "3317968",
            "problem": "You are given the task of designing a program that, for synthetic datasets modeling single-cell RNA sequencing (scRNA-seq) embeddings, constructs $k$-nearest neighbor graphs, evaluates the symmetric normalized graph Laplacian spectrum, and selects an appropriate $k$ by maximizing a well-defined \"first nontrivial spectral gap.\" The goal is to analyze how the selected $k$ trades off between over-clustering and under-clustering on datasets that include known lineage continua. The problem must be solved by implementing a complete, runnable program that produces the required final output without any user input.\n\nStart from the following fundamental definitions and well-tested facts:\n\n1. A $k$-nearest neighbor (k-NN) graph on $n$ points in $\\mathbb{R}^d$ connects each point $i$ to the set of $k$ nearest points by Euclidean distance. To obtain an undirected graph, use the symmetrized union rule: if $i$ is among the $k$ nearest neighbors of $j$ or $j$ is among the $k$ nearest neighbors of $i$, then connect $i$ and $j$.\n\n2. Let $W \\in \\mathbb{R}^{n \\times n}$ be a symmetric, nonnegative weighted adjacency matrix for the k-NN graph, and let $D \\in \\mathbb{R}^{n \\times n}$ be the diagonal degree matrix with $D_{ii} = \\sum_{j=1}^n W_{ij}$. The symmetric normalized Laplacian is defined as\n$$\nL_{\\mathrm{sym}} = I - D^{-\\frac{1}{2}} W D^{-\\frac{1}{2}}.\n$$\nIt is known that $L_{\\mathrm{sym}}$ is symmetric positive semidefinite with eigenvalues $0 = \\lambda_1 \\le \\lambda_2 \\le \\cdots \\le \\lambda_n \\le 2$. The multiplicity of $\\lambda = 0$ equals the number of connected components of the graph. Spectral clustering uses gaps in the ordered spectrum $\\{\\lambda_i\\}$ to infer the number of clusters via stability of partitions.\n\n3. For a given $k$, let $\\lambda_1 \\le \\lambda_2 \\le \\cdots \\le \\lambda_n$ be the ordered eigenvalues of $L_{\\mathrm{sym}}$. Define the \"first nontrivial spectral gap\" for that $k$ as follows. Fix\n$$\nr_{\\max} = \\min(10, n - 2).\n$$\nFor $i \\in \\{1,2,\\dots,r_{\\max}\\}$, define gaps $g_i = \\lambda_{i+1} - \\lambda_i$. Identify\n$$\ni^\\star(k) = \\arg\\min\\{ i \\in \\{1,\\dots,r_{\\max}\\} \\,:\\, g_i = \\max_{1 \\le j \\le r_{\\max}} g_j \\},\n$$\nthe smallest index at which the maximum gap is achieved. The predicted number of clusters at this $k$ is defined as $c(k) = i^\\star(k)$. Define the \"gap magnitude\" as $G(k) = g_{i^\\star(k)}$.\n\n4. Select\n$$\nk^\\star = \\arg\\min\\{ k \\in \\mathcal{K} \\,:\\, G(k) = \\max_{k' \\in \\mathcal{K}} G(k') \\},\n$$\nwhere $\\mathcal{K}$ is a specified finite set of candidate $k$ values, and the tie-breaker selects the smallest $k$ achieving the maximum gap. The final predicted number of clusters is $c^\\star = c(k^\\star)$.\n\n5. Evaluate over-clustering and under-clustering using ground-truth discrete cluster counts for each dataset. Let $c_{\\mathrm{true}}$ be the ground-truth count (for a pure lineage continuum scenario, set $c_{\\mathrm{true}} = 1$). Define boolean indicators:\n- Over-clustering: $c^\\star > c_{\\mathrm{true}}$.\n- Under-clustering: $c^\\star < c_{\\mathrm{true}}$.\n\nGraph construction and weights. Use Euclidean distance in $\\mathbb{R}^2$. For each $k$, construct the undirected symmetrized k-NN graph and weight each edge $(i,j)$ by a Gaussian kernel\n$$\nW_{ij} = \\exp\\!\\left(-\\frac{\\|x_i - x_j\\|^2}{2 \\sigma^2}\\right),\n$$\nwhere $\\sigma$ is the median of the Euclidean distances from each point to its $k$ nearest neighbors, aggregated across all points. If any node has zero degree, use the convention that $(D^{-\\frac{1}{2}})_{ii} = 0$ for that node in $L_{\\mathrm{sym}}$.\n\nYour program must implement the above procedure and apply it to the following test suite of synthetic datasets (all in $\\mathbb{R}^2$), with a fixed random seed to ensure determinism. The datasets are intended to mimic diverse scRNA-seq embeddings, including discrete types and lineage continua.\n\nDataset A (happy path: well-separated discrete clusters):\n- Total points $n = 90$, generated as a mixture of $3$ isotropic Gaussian blobs of equal size $30$ with standard deviation $0.4$, centered at $(-4, 0)$, $(0, 0)$, and $(4, 0)$.\n- Ground truth $c_{\\mathrm{true}} = 3$.\n- Candidate set $\\mathcal{K} = \\{3, 5, 7, 9, 11, 13, 15\\}$.\n\nDataset B (continuum: linear lineage):\n- Total points $n = 120$, placed along a line segment with additive noise: take $t$ evenly spaced in $[-5, 5]$ and set $x = (t, \\epsilon)$ where $\\epsilon$ are independent Gaussian deviations with standard deviation $0.1$.\n- Ground truth $c_{\\mathrm{true}} = 1$.\n- Candidate set $\\mathcal{K} = \\{3, 5, 7, 9, 11, 13, 15\\}$.\n\nDataset C (branching continuum: Y-shaped manifold):\n- Total points $n = 150$, formed by three noisy rays from the origin with angles $0$, $2\\pi/3$, and $4\\pi/3$. For each ray, sample radii uniformly over $[0.3, 5.0]$ at $50$ evenly spaced values, add isotropic Gaussian noise with standard deviation $0.15$ to both coordinates, and concatenate all three rays.\n- Ground truth $c_{\\mathrm{true}} = 3$.\n- Candidate set $\\mathcal{K} = \\{3, 5, 7, 9, 11, 13, 15, 17, 19\\}$.\n\nProgram requirements:\n- Use a fixed random seed $42$ for all stochastic components.\n- For each dataset, perform the $k$-selection as defined above and compute the predicted cluster count $c^\\star$ at the selected $k^\\star$.\n- For each dataset, compute booleans for over-clustering and under-clustering relative to $c_{\\mathrm{true}}$.\n- The final output must be a single line containing a list of results for the three datasets in order A, B, C. Each result is a list of the form $[k^\\star, c^\\star, \\text{over}, \\text{under}]$ where $k^\\star$ and $c^\\star$ are integers, and $\\text{over}$ and $\\text{under}$ are booleans.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a list in the form $[k^\\star, c^\\star, \\text{over}, \\text{under}]$. For example, the structure must be exactly like $[[3,2,False,True],[\\dots],[\\dots]]$ with no extra characters or text.",
            "solution": "The problem posed is a well-defined computational task in the domain of graph-based data analysis, specifically tailored to clustering problems reminiscent of single-cell RNA sequencing data analysis. It requires the implementation of a specific algorithm based on spectral graph theory to estimate the number of clusters in several synthetic datasets. The procedure is scientifically sound, mathematically unambiguous, and programmatically formalizable.\n\nThe core principle is to leverage the spectrum of the graph Laplacian, a matrix that encodes the connectivity structure of a graph. The number of eigenvalues close to zero is related to the number of connected components or \"clusters\" in the data. By systematically varying the connectivity of the graph (by changing the neighborhood size $k$) and observing the stability of the spectral gaps, one can infer an optimal number of clusters.\n\nThe detailed, step-by-step procedure is as follows:\n\nFirst, for each of the three specified datasets (A, B, and C), we generate the point coordinates $X \\in \\mathbb{R}^{n \\times 2}$ according to the provided stochastic models. A fixed random seed of $42$ ensures the reproducibility of these datasets. The ground-truth number of clusters, $c_{\\mathrm{true}}$, is given for each case.\n\nThe main analytical procedure iterates through a predefined set of candidate neighborhood sizes, $\\mathcal{K}$. For each candidate $k \\in \\mathcal{K}$, the following sequence of computations is executed:\n\n1.  **k-NN Graph Construction**: A pairwise Euclidean distance matrix is computed for all $n$ points. For each point $x_i$, its $k$ nearest neighbors are identified. An undirected graph is then formed by creating an edge $(i, j)$ if point $x_j$ is among the $k$ nearest neighbors of $x_i$, or if $x_i$ is among the $k$ nearest neighbors of $x_j$. This defines the graph's topology.\n\n2.  **Adaptive Edge Weighting**: The connections in the graph are weighted to form a symmetric adjacency matrix $W \\in \\mathbb{R}^{n \\times n}$. The weight $W_{ij}$ for an existing edge $(i, j)$ is calculated using a Gaussian kernel:\n    $$ W_{ij} = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2 \\sigma^2}\\right) $$\n    The scaling factor $\\sigma$ is determined adaptively for each $k$. It is set to the median of the collected distances from every point in the dataset to each of its $k$ nearest neighbors. This local density normalization is a common technique to handle clusters of varying shapes and densities. For pairs $(i, j)$ not connected by an edge, $W_{ij} = 0$.\n\n3.  **Symmetric Normalized Laplacian**: From the weighted adjacency matrix $W$, the diagonal degree matrix $D$ is computed, where $D_{ii} = \\sum_{j=1}^n W_{ij}$. The symmetric normalized Laplacian, $L_{\\mathrm{sym}}$, is then constructed:\n    $$ L_{\\mathrm{sym}} = I - D^{-\\frac{1}{2}} W D^{-\\frac{1}{2}} $$\n    where $I$ is the identity matrix. A special convention handles nodes with degree zero (isolated points): if $D_{ii} = 0$, the corresponding entry $(D^{-\\frac{1}{2}})_{ii}$ is also set to $0$.\n\n4.  **Eigenvalue Computation and Gap Analysis**: The eigenvalues of the symmetric, positive semidefinite matrix $L_{\\mathrm{sym}}$ are computed and sorted in non-decreasing order: $0 \\le \\lambda_1 \\le \\lambda_2 \\le \\cdots \\le \\lambda_n$. The number of clusters is estimated using the \"eigengap heuristic.\" We analyze the gaps between the first few consecutive eigenvalues. The gaps are defined as $g_i = \\lambda_{i+1} - \\lambda_i$ for $i \\in \\{1, 2, \\dots, r_{\\max}\\}$, where $r_{\\max} = \\min(10, n - 2)$. The index $i^\\star(k)$ corresponding to the largest gap is identified:\n    $$ i^\\star(k) = \\arg\\min\\{ i \\in \\{1,\\dots,r_{\\max}\\} \\,:\\, g_i = \\max_{1 \\le j \\le r_{\\max}} g_j \\} $$\n    The use of $\\arg\\min$ ensures that if the maximum gap occurs at multiple indices, the smallest such index is chosen. This index $i^\\star(k)$ is the predicted number of clusters for the given $k$, denoted $c(k)$. The magnitude of this maximum gap is denoted $G(k) = g_{i^\\^\\star(k)}$.\n\nAfter this process is completed for all $k \\in \\mathcal{K}$, the optimal neighborhood size, $k^\\star$, is selected. It is defined as the value of $k$ that maximizes the gap magnitude $G(k)$:\n$$ k^\\star = \\arg\\min\\{ k \\in \\mathcal{K} \\,:\\, G(k) = \\max_{k' \\in \\mathcal{K}} G(k') \\} $$\nThe tie-breaking rule here selects the smallest $k$ that achieves the maximum gap, favoring simpler graph models. The final predicted number of clusters for the dataset is $c^\\star = c(k^\\star)$.\n\nFinally, this prediction is compared to the ground-truth number of clusters $c_{\\mathrm{true}}$. Two boolean indicators are computed:\n-   Over-clustering: True if $c^\\star > c_{\\mathrm{true}}$.\n-   Under-clustering: True if $c^\\star < c_{\\mathrm{true}}$.\n\nThis entire procedure is applied independently to each of the three datasets, and the results, in the format $[k^\\star, c^\\star, \\text{Over-clustering}, \\text{Under-clustering}]$, are aggregated for the final output.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all datasets and print the final result.\n    \"\"\"\n    np.random.seed(42)\n\n    test_cases_params = [\n        {\n            \"name\": \"A\",\n            \"n\": 90,\n            \"c_true\": 3,\n            \"K_set\": [3, 5, 7, 9, 11, 13, 15],\n            \"generator\": \"gaussians\"\n        },\n        {\n            \"name\": \"B\",\n            \"n\": 120,\n            \"c_true\": 1,\n            \"K_set\": [3, 5, 7, 9, 11, 13, 15],\n            \"generator\": \"line\"\n        },\n        {\n            \"name\": \"C\",\n            \"n\": 150,\n            \"c_true\": 3,\n            \"K_set\": [3, 5, 7, 9, 11, 13, 15, 17, 19],\n            \"generator\": \"y_shape\"\n        },\n    ]\n\n    final_results = []\n    for params in test_cases_params:\n        if params[\"generator\"] == \"gaussians\":\n            centers = np.array([[-4, 0], [0, 0], [4, 0]])\n            n_per_cluster = 30\n            std_dev = 0.4\n            points = np.vstack([\n                np.random.randn(n_per_cluster, 2) * std_dev + center\n                for center in centers\n            ])\n        elif params[\"generator\"] == \"line\":\n            n = params[\"n\"]\n            t = np.linspace(-5, 5, n)\n            epsilon = np.random.randn(n) * 0.1\n            points = np.vstack((t, epsilon)).T\n        elif params[\"generator\"] == \"y_shape\":\n            n_per_ray = 50\n            radii = np.linspace(0.3, 5.0, n_per_ray)\n            angles = [0, 2 * np.pi / 3, 4 * np.pi / 3]\n            std_dev = 0.15\n            all_rays = []\n            for angle in angles:\n                x = radii * np.cos(angle)\n                y = radii * np.sin(angle)\n                ray = np.vstack((x, y)).T\n                noise = np.random.randn(n_per_ray, 2) * std_dev\n                all_rays.append(ray + noise)\n            points = np.concatenate(all_rays, axis=0)\n\n        result = analyze_dataset(points, params[\"K_set\"], params[\"c_true\"])\n        final_results.append(result)\n        \n    # Format the final output string\n    result_str = \",\".join(map(str, final_results))\n    print(f\"[{result_str}]\")\n\n\ndef analyze_dataset(X, K_set, c_true):\n    \"\"\"\n    Performs the full k-selection and clustering analysis for a single dataset.\n    \"\"\"\n    n, d = X.shape\n\n    # Pre-compute pairwise distances\n    dist_matrix = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            dist = np.linalg.norm(X[i] - X[j])\n            dist_matrix[i, j] = dist\n            dist_matrix[j, i] = dist\n\n    results_per_k = []\n\n    for k in K_set:\n        # 1. Find k-NN for each point\n        # Add a large value to the diagonal to prevent selecting self\n        dist_matrix_no_self = dist_matrix + np.identity(n) * 1e9\n        knn_indices = np.argsort(dist_matrix_no_self, axis=1)[:, :k]\n        \n        # 2. Collect distances to k-NNs to compute sigma\n        knn_distances_flat = dist_matrix[np.arange(n)[:, None], knn_indices].flatten()\n        sigma = np.median(knn_distances_flat)\n        if sigma == 0: sigma = 1e-9 # Avoid division by zero\n\n        # 3. Construct symmetrized adjacency matrix `A`\n        A = np.zeros((n, n), dtype=bool)\n        for i in range(n):\n            A[i, knn_indices[i]] = True\n        A = np.logical_or(A, A.T)\n        \n        # 4. Construct weighted matrix `W`\n        W = np.zeros((n, n))\n        edge_indices = np.where(A)\n        edge_dists = dist_matrix[edge_indices]\n        W[edge_indices] = np.exp(-edge_dists**2 / (2 * sigma**2))\n\n        # 5. Compute symmetric normalized Laplacian `L_sym`\n        D_diag = np.sum(W, axis=1)\n        D_inv_sqrt_diag = np.zeros_like(D_diag)\n        non_zero_mask = D_diag > 0\n        D_inv_sqrt_diag[non_zero_mask] = 1.0 / np.sqrt(D_diag[non_zero_mask])\n        D_inv_sqrt = np.diag(D_inv_sqrt_diag)\n        \n        L_sym = np.identity(n) - D_inv_sqrt @ W @ D_inv_sqrt\n        \n        # 6. Compute eigenvalues and find the largest gap\n        # We need at most r_max+1 eigenvalues.\n        r_max = min(10, n - 2)\n        eigvals = linalg.eigh(L_sym, eigvals_only=True, subset_by_index=[0, r_max + 1])\n\n        # 7. Analyze gaps\n        gaps = eigvals[1:] - eigvals[:-1]\n        \n        if len(gaps) == 0:\n            # This can happen if n is very small and r_max=0\n            G_k = 0\n            c_k = 1\n        else:\n            max_gap_index_0based = np.argmax(gaps)\n            # The cluster count is the 1-based index of the largest gap\n            c_k = max_gap_index_0based + 1\n            G_k = gaps[max_gap_index_0based]\n\n        results_per_k.append({\"k\": k, \"G\": G_k, \"c\": c_k})\n\n    # 8. Select optimal k*\n    max_G = -1.0\n    for res in results_per_k:\n        if res[\"G\"] > max_G:\n            max_G = res[\"G\"]\n\n    best_k_candidates = [res[\"k\"] for res in results_per_k if np.isclose(res[\"G\"], max_G)]\n    k_star = min(best_k_candidates)\n\n    # 9. Find final cluster count c*\n    c_star = 0\n    for res in results_per_k:\n        if res[\"k\"] == k_star:\n            c_star = res[\"c\"]\n            break\n\n    # 10. Evaluate clustering\n    over_clustering = c_star > c_true\n    under_clustering = c_star < c_true\n\n    return [k_star, c_star, over_clustering, under_clustering]\n\nsolve()\n```"
        }
    ]
}