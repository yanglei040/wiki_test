{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds our exploration in the bedrock of statistical inference. By deriving the maximum likelihood estimates for the mean $\\mu$ and variance $\\sigma^2$ of a normal distribution from first principles , you will build a foundational understanding of score equations and Fisher information, concepts that are central to parameter estimation and confidence assessment.",
            "id": "3322901",
            "problem": "In a quantitative transcriptomics assay, suppose replicated measurements of messenger ribonucleic acid (mRNA) abundance for a single gene across $n$ technical replicates are modeled as independent and identically distributed (i.i.d.) as $y_{1},\\dots,y_{n} \\sim \\mathcal{N}(\\mu,\\sigma^{2})$, where $\\mu$ is the unknown mean abundance and $\\sigma^{2}$ is the unknown measurement variance. Using the principles of maximum likelihood estimation and least-squares inference, proceed from first principles by starting with the probability density function of the normal distribution and the independence assumption to construct the likelihood. Then:\n- Derive the score equations for $\\mu$ and $\\sigma^{2}$ by differentiating the log-likelihood with respect to each parameter.\n- Solve the score equations to obtain the maximum likelihood estimates (MLEs) of $\\mu$ and $\\sigma^{2}$ in terms of the data.\n- Compute the observed Fisher information matrix, defined as the negative Hessian of the log-likelihood with respect to $(\\mu,\\sigma^{2})$, evaluated at the MLEs.\n\nTo ensure a closed-form single final answer, report the determinant of the observed Fisher information matrix evaluated at the MLEs as your final result. Express the final answer as a symbolic expression; no numerical rounding is required. No physical units are needed. Use the notation $\\hat{\\mu}$ and $\\hat{\\sigma}^{2}$ to denote the MLEs of $\\mu$ and $\\sigma^{2}$ respectively, and $n$ for the number of replicates.",
            "solution": "The user has provided a problem statement that requires validation before a solution can be attempted.\n\n### Step 1: Extract Givens\n- The data are $n$ independent and identically distributed (i.i.d.) measurements, $y_1, \\dots, y_n$.\n- The data are drawn from a normal distribution: $y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n- The parameters of the distribution, the mean $\\mu$ and the variance $\\sigma^2$, are unknown.\n- The task is to:\n    1.  Derive the score equations for $\\mu$ and $\\sigma^2$ from the log-likelihood.\n    2.  Solve these equations to find the maximum likelihood estimates (MLEs), denoted $\\hat{\\mu}$ and $\\hat{\\sigma}^2$.\n    3.  Compute the observed Fisher information matrix, defined as the negative of the Hessian of the log-likelihood, evaluated at the MLEs.\n- The final result to be reported is the determinant of this observed Fisher information matrix, expressed symbolically in terms of $n$ and $\\hat{\\sigma}^2$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly rooted in the fundamental principles of statistical inference, specifically maximum likelihood estimation. The normal distribution is a cornerstone of statistical modeling in the sciences, and the concepts of likelihood, score functions, and Fisher information are central to the theory and practice of estimation. The application context of quantitative transcriptomics is appropriate.\n- **Well-Posed:** The problem provides a clear objective and all the necessary information to achieve it. The steps are logically sequenced, leading to a unique, well-defined mathematical expression.\n- **Objective:** The problem is stated using precise, unambiguous mathematical and statistical terminology. It is free of any subjective or opinion-based content.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically sound, well-posed, objective, and requires a standard, albeit detailed, derivation from first principles. I will now proceed with the solution.\n\n***\n\nThe solution proceeds by first constructing the log-likelihood function, then finding its critical points to determine the MLEs, and finally calculating the curvature at the maximum to find the observed Fisher information.\n\nThe probability density function (PDF) for a single observation $y_i$ from a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is:\n$$ f(y_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right) $$\nGiven that the observations $y_1, \\dots, y_n$ are i.i.d., the likelihood function $L(\\mu, \\sigma^2 | \\mathbf{y})$ is the product of the individual PDFs:\n$$ L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} f(y_i | \\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right) $$\n$$ L(\\mu, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right) $$\nIt is more convenient to work with the log-likelihood function, $\\ell(\\mu, \\sigma^2) = \\ln(L(\\mu, \\sigma^2))$:\n$$ \\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 $$\n\n**1. Derivation of the Score Equations**\nThe score equations are obtained by setting the first partial derivatives of the log-likelihood function with respect to the parameters $\\mu$ and $\\sigma^2$ to zero.\n\nThe partial derivative with respect to $\\mu$ is:\n$$ \\frac{\\partial \\ell}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) $$\n$$ \\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(y_i - \\mu)(-1) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) $$\nThe first score equation is $\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) = 0$.\n\nThe partial derivative with respect to $\\sigma^2$ is:\n$$ \\frac{\\partial \\ell}{\\partial (\\sigma^2)} = \\frac{\\partial}{\\partial (\\sigma^2)} \\left( -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) $$\n$$ \\frac{\\partial \\ell}{\\partial (\\sigma^2)} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 $$\nThe second score equation is $-\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 = 0$.\n\n**2. Solving for the Maximum Likelihood Estimates (MLEs)**\nTo find the MLEs, $\\hat{\\mu}$ and $\\hat{\\sigma}^2$, we solve the score equations. From the first equation:\n$$ \\frac{1}{\\hat{\\sigma}^2} \\sum_{i=1}^{n} (y_i - \\hat{\\mu}) = 0 \\implies \\sum_{i=1}^{n} y_i - n\\hat{\\mu} = 0 $$\n$$ \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} y_i $$\nThe MLE for the mean is the sample mean.\n\nSubstituting $\\hat{\\mu}$ into the second score equation:\n$$ -\\frac{n}{2\\hat{\\sigma}^2} + \\frac{1}{2(\\hat{\\sigma}^2)^2} \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0 $$\nMultiplying by $2(\\hat{\\sigma}^2)^2$ yields:\n$$ -n\\hat{\\sigma}^2 + \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0 $$\n$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 $$\nThe MLE for the variance is the sample variance with a denominator of $n$.\n\n**3. Computing the Observed Fisher Information Matrix**\nThe observed Fisher information matrix $I(\\mu, \\sigma^2)$ is the negative of the Hessian matrix of the log-likelihood, $\\mathbf{H}$. The elements of the Hessian are the second partial derivatives of $\\ell$.\n$$ \\mathbf{H} = \\begin{pmatrix} \\frac{\\partial^2 \\ell}{\\partial \\mu^2} & \\frac{\\partial^2 \\ell}{\\partial \\mu \\partial (\\sigma^2)} \\\\ \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2) \\partial \\mu} & \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} \\end{pmatrix} $$\nWe compute each element:\n$$ \\frac{\\partial^2 \\ell}{\\partial \\mu^2} = \\frac{\\partial}{\\partial \\mu} \\left( \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) \\right) = -\\frac{n}{\\sigma^2} $$\n$$ \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2) \\partial \\mu} = \\frac{\\partial}{\\partial (\\sigma^2)} \\left( \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) \\right) = -\\frac{1}{(\\sigma^2)^2} \\sum_{i=1}^{n} (y_i - \\mu) $$\n$$ \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} = \\frac{\\partial}{\\partial (\\sigma^2)} \\left( -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) = \\frac{n}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3} \\sum_{i=1}^{n} (y_i - \\mu)^2 $$\nNext, we evaluate the Hessian at the MLEs $(\\hat{\\mu}, \\hat{\\sigma}^2)$.\n$$ \\left. \\frac{\\partial^2 \\ell}{\\partial \\mu^2} \\right|_{(\\hat{\\mu}, \\hat{\\sigma}^2)} = -\\frac{n}{\\hat{\\sigma}^2} $$\nThe off-diagonal term becomes zero because $\\sum_{i=1}^{n} (y_i - \\hat{\\mu}) = 0$:\n$$ \\left. \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2) \\partial \\mu} \\right|_{(\\hat{\\mu}, \\hat{\\sigma}^2)} = -\\frac{1}{(\\hat{\\sigma}^2)^2} \\sum_{i=1}^{n} (y_i - \\hat{\\mu}) = 0 $$\nFor the final element, we substitute $\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = n\\hat{\\sigma}^2$:\n$$ \\left. \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} \\right|_{(\\hat{\\mu}, \\hat{\\sigma}^2)} = \\frac{n}{2(\\hat{\\sigma}^2)^2} - \\frac{1}{(\\hat{\\sigma}^2)^3} \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = \\frac{n}{2(\\hat{\\sigma}^2)^2} - \\frac{n\\hat{\\sigma}^2}{(\\hat{\\sigma}^2)^3} = \\frac{n}{2(\\hat{\\sigma}^2)^2} - \\frac{n}{(\\hat{\\sigma}^2)^2} = -\\frac{n}{2(\\hat{\\sigma}^2)^2} $$\nThe Hessian matrix evaluated at the MLEs is:\n$$ \\mathbf{H}(\\hat{\\mu}, \\hat{\\sigma}^2) = \\begin{pmatrix} -\\frac{n}{\\hat{\\sigma}^2} & 0 \\\\ 0 & -\\frac{n}{2(\\hat{\\sigma}^2)^2} \\end{pmatrix} $$\nThe observed Fisher information matrix is $I(\\hat{\\mu}, \\hat{\\sigma}^2) = -\\mathbf{H}(\\hat{\\mu}, \\hat{\\sigma}^2)$:\n$$ I(\\hat{\\mu}, \\hat{\\sigma}^2) = \\begin{pmatrix} \\frac{n}{\\hat{\\sigma}^2} & 0 \\\\ 0 & \\frac{n}{2(\\hat{\\sigma}^2)^2} \\end{pmatrix} $$\nFinally, we compute the determinant of the observed Fisher information matrix. As the matrix is diagonal, the determinant is the product of its diagonal elements:\n$$ \\det(I(\\hat{\\mu}, \\hat{\\sigma}^2)) = \\left(\\frac{n}{\\hat{\\sigma}^2}\\right) \\left(\\frac{n}{2(\\hat{\\sigma}^2)^2}\\right) = \\frac{n^2}{2(\\hat{\\sigma}^2)^3} $$",
            "answer": "$$\\boxed{\\frac{n^2}{2(\\hat{\\sigma}^2)^{3}}}$$"
        },
        {
            "introduction": "Building upon the foundational principles, this exercise moves into the more realistic domain of nonlinear biological models. You will apply the Gauss-Newton method to the classic Michaelis-Menten kinetics model , providing hands-on experience with the iterative algorithms required to solve the nonlinear least-squares problems that are ubiquitous in systems biology. This practice bridges the gap between the theory of maximum likelihood and its practical implementation for complex models.",
            "id": "3322866",
            "problem": "You are modeling initial reaction velocity measurements from an enzyme-catalyzed reaction following Michaelis–Menten kinetics in a computational systems biology study. For substrate concentrations $\\{x_i\\}_{i=1}^{n}$ and measured initial velocities $\\{y_i\\}_{i=1}^{n}$, assume the observation model\n$$\ny_i \\;=\\; \\frac{V_{\\max}\\, x_i}{K_m + x_i} \\;+\\; \\epsilon_i,\n$$\nwhere $\\epsilon_i$ are independent and identically distributed (i.i.d.) Gaussian noise terms with mean $0$ and variance $\\sigma^2$, and $V_{\\max} \\gt 0$, $K_m \\gt 0$. Under the Gaussian noise assumption, the maximum likelihood estimator coincides with the nonlinear least-squares estimator that minimizes the sum of squared residuals. Let the residual for the $i$-th datum be\n$$\nr_i(\\theta) \\;=\\; y_i \\;-\\; f(x_i;\\theta), \\quad \\text{with} \\quad \\theta \\equiv (V_{\\max}, K_m), \\quad f(x;\\theta) \\equiv \\frac{V_{\\max} x}{K_m + x}.\n$$\nConsider the Gauss–Newton method for solving the nonlinear least-squares problem. Starting from the first principles of maximum likelihood for Gaussian noise and the definition of the Jacobian as the matrix of first derivatives of the model with respect to the parameters, derive:\n- the Jacobian row for a single observation $x_i$, i.e., the $1 \\times 2$ vector of partial derivatives of $f(x_i;\\theta)$ with respect to $(V_{\\max}, K_m)$; and\n- the explicit Gauss–Newton normal equations for the parameter increment $\\Delta \\theta$ in terms of sums over $i$, written via the components of the symmetric $2 \\times 2$ matrix $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta)$ and the $2 \\times 1$ vector $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta)$, where $J_i(\\theta)$ denotes the Jacobian row at $x_i$.\n\nReport your final answer as a single row matrix with three entries: first, the Jacobian row expressed in terms of $x_i$, $V_{\\max}$, and $K_m$; second, the $2 \\times 2$ symmetric matrix of the Gauss–Newton normal matrix written as explicit sums over $i$; and third, the $2 \\times 1$ right-hand side vector written as explicit sums over $i$ in terms of the residuals $r_i(\\theta) = y_i - f(x_i;\\theta)$. Do not include an equality sign in the final answer. No numerical evaluation is required, and no rounding is needed. Express the final answer symbolically.",
            "solution": "The user has provided a valid problem statement. The task is to derive the components of the normal equations for the Gauss–Newton method as applied to a Michaelis–Menten kinetic model. The problem is scientifically grounded in enzyme kinetics and statistical inference, is well-posed, and all necessary definitions are provided. I will proceed with the derivation.\n\nThe problem concerns the estimation of parameters $\\theta = (V_{\\max}, K_m)$ for the Michaelis–Menten model function\n$$\nf(x; \\theta) = \\frac{V_{\\max} x}{K_m + x}\n$$\nThe Gauss–Newton method provides an iterative update $\\Delta\\theta$ to the parameter vector by solving the linear system known as the normal equations:\n$$\n\\left( \\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta) \\right) \\Delta \\theta = \\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta)\n$$\nwhere $J_i(\\theta)$ is the Jacobian row vector for the $i$-th observation and $r_i(\\theta)$ is the corresponding residual. Our objective is to derive the analytical expressions for the Jacobian row $J_i(\\theta)$, the matrix $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta)$, and the vector $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta)$.\n\nFirst, we derive the Jacobian row for a single observation $x_i$. The Jacobian $J_i(\\theta)$ is a $1 \\times 2$ row vector containing the partial derivatives of the model function $f(x_i; \\theta)$ with respect to the parameters $V_{\\max}$ and $K_m$:\n$$\nJ_i(\\theta) = \\begin{pmatrix} \\frac{\\partial f(x_i; \\theta)}{\\partial V_{\\max}} & \\frac{\\partial f(x_i; \\theta)}{\\partial K_m} \\end{pmatrix}\n$$\nWe compute each partial derivative separately. The derivative with respect to $V_{\\max}$ is:\n$$\n\\frac{\\partial f}{\\partial V_{\\max}} = \\frac{\\partial}{\\partial V_{\\max}} \\left( \\frac{V_{\\max} x_i}{K_m + x_i} \\right) = \\frac{x_i}{K_m + x_i}\n$$\nThe derivative with respect to $K_m$ is found using the quotient rule:\n$$\n\\frac{\\partial f}{\\partial K_m} = \\frac{\\partial}{\\partial K_m} \\left( \\frac{V_{\\max} x_i}{K_m + x_i} \\right) = \\frac{0 \\cdot (K_m + x_i) - (V_{\\max} x_i) \\cdot \\frac{\\partial}{\\partial K_m}(K_m + x_i)}{(K_m + x_i)^2} = \\frac{-V_{\\max} x_i}{(K_m + x_i)^2}\n$$\nCombining these results, the Jacobian row for the $i$-th measurement is:\n$$\nJ_i(\\theta) = \\begin{pmatrix} \\frac{x_i}{K_m + x_i} & - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix}\n$$\n\nNext, we derive the $2 \\times 2$ symmetric matrix for the normal equations, which is the sum of the outer products of the Jacobian rows with their transposes: $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta)$. For a single observation $i$, the outer product $J_i(\\theta)^{\\top} J_i(\\theta)$ is:\n$$\nJ_i(\\theta)^{\\top} J_i(\\theta) = \\begin{pmatrix} \\frac{x_i}{K_m + x_i} \\\\ - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix} \\begin{pmatrix} \\frac{x_i}{K_m + x_i} & - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix}\n= \\begin{pmatrix} \\left(\\frac{x_i}{K_m + x_i}\\right)^2 & -\\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} \\\\ -\\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} & \\left(\\frac{V_{\\max} x_i}{(K_m + x_i)^2}\\right)^2 \\end{pmatrix}\n$$\nSumming these matrices over all observations $i=1, \\dots, n$ yields the Gauss–Newton normal matrix:\n$$\n\\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta) = \\begin{pmatrix}\n\\sum_{i=1}^{n} \\frac{x_i^2}{(K_m + x_i)^2} & - \\sum_{i=1}^{n} \\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} \\\\\n- \\sum_{i=1}^{n} \\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} & \\sum_{i=1}^{n} \\frac{V_{\\max}^2 x_i^2}{(K_m + x_i)^4}\n\\end{pmatrix}\n$$\n\nFinally, we derive the $2 \\times 1$ right-hand side vector of the normal equations, $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta)$, where $r_i(\\theta) = y_i - f(x_i; \\theta)$ is the residual for the $i$-th observation. This vector is the sum of the Jacobian transposes, each scaled by the corresponding scalar residual:\n$$\n\\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta) = \\sum_{i=1}^{n} \\begin{pmatrix} \\frac{x_i}{K_m + x_i} \\\\ - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix} r_i(\\theta) = \\begin{pmatrix}\n\\sum_{i=1}^{n} r_i(\\theta) \\frac{x_i}{K_m + x_i} \\\\\n- \\sum_{i=1}^{n} r_i(\\theta) \\frac{V_{\\max} x_i}{(K_m + x_i)^2}\n\\end{pmatrix}\n$$\nThese derived components constitute the necessary elements for one iteration of the Gauss–Newton algorithm for this specific parameter estimation problem.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\begin{pmatrix} \\frac{x_i}{K_m + x_i} & - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix} & \\begin{pmatrix} \\sum_{i=1}^{n} \\frac{x_i^2}{(K_m + x_i)^2} & - \\sum_{i=1}^{n} \\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} \\\\ - \\sum_{i=1}^{n} \\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} & \\sum_{i=1}^{n} \\frac{V_{\\max}^2 x_i^2}{(K_m + x_i)^4} \\end{pmatrix} & \\begin{pmatrix} \\sum_{i=1}^{n} r_i(\\theta) \\frac{x_i}{K_m + x_i} \\\\ - \\sum_{i=1}^{n} r_i(\\theta) \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Our final practice challenges the common assumption of Gaussian noise and explores the concept of robust regression. By comparing the influence of outliers under a Gaussian (least-squares) versus a Student-$t$ error model , you will learn how the choice of likelihood function can dramatically improve parameter inference in the presence of noisy, real-world data like that from metabolomics. This exercise highlights the flexibility and power of the maximum likelihood framework to create estimators tailored to specific data-generating processes.",
            "id": "3322854",
            "problem": "A metabolomics laboratory quantifies metabolite abundances across experimental conditions and fits a parametric regression model to predict the measured response from experimental covariates. Let the data be $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$ with $n$ independent samples, where $y_{i}$ is the measured log-abundance and $x_{i}$ is a vector of covariates. The model posits\n$y_{i} = f(x_{i}; \\theta) + \\varepsilon_{i}$,\nwhere $f(x_{i}; \\theta)$ is a smooth predictor and $\\varepsilon_{i}$ are independent, mean-zero errors. In practice, metabolomics data can contain outliers due to sporadic ion suppression or misalignment events, motivating heavy-tailed noise models.\n\nAssume the following two error models with a common scale parameter $\\sigma > 0$:\n\n- Student-$t$ model: $\\varepsilon_{i} \\sim t_{\\nu}(0, \\sigma)$ with degrees of freedom $\\nu > 0$ and density\n$$\np_{\\nu,\\sigma}(\\varepsilon) = \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\,\\sigma} \\left(1 + \\frac{\\varepsilon^{2}}{\\nu \\sigma^{2}}\\right)^{-\\frac{\\nu+1}{2}}.\n$$\n\n- Gaussian model: $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$ with density\n$$\np_{\\sigma}(\\varepsilon) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\!\\left(-\\frac{\\varepsilon^{2}}{2\\sigma^{2}}\\right).\n$$\n\nLet the residual for sample $i$ be $r_{i}(\\theta) = y_{i} - f(x_{i}; \\theta)$.\n\nTasks:\n\n1. Using the independence of errors and the given density for the Student-$t$ model, derive the negative log-likelihood as a function of $\\theta$ and $\\sigma$, expressed in terms of the residuals $r_{i}(\\theta)$. You may drop additive terms that do not depend on $\\theta$ or $\\sigma$.\n\n2. In robust regression, the influence on the estimating equations is governed by the derivative of the per-sample negative log-likelihood with respect to the residual, i.e., the function $\\psi(r) = \\frac{d}{dr}\\rho(r)$ where $\\rho(r)$ is the per-sample negative log-likelihood contribution. Derive the corresponding $\\psi$-functions for:\n   - the Student-$t$ error model with parameters $\\nu$ and $\\sigma$,\n   - the Gaussian error model with scale $\\sigma$.\n\n3. Consider the presence of a small fraction of outliers in the metabolomics dataset. To compare robustness, define the ratio\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{\\psi_{\\text{Student-}t}(r)}{\\psi_{\\text{Gaussian}}(r)}.\n$$\nProvide a single, closed-form analytic expression for $\\Xi(r; \\nu, \\sigma)$ in terms of $r$, $\\nu$, and $\\sigma$.\n\nGive your final answer as the simplified expression for $\\Xi(r; \\nu, \\sigma)$. No numerical evaluation is required. Do not include any units. All other derivations are to support the final expression. The final answer must be a single analytic expression.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. The tasks involve standard derivations in statistical modeling and robust regression, based on correct mathematical definitions. We proceed with the solution.\n\nThe solution is structured by addressing the three tasks in sequence.\n\n### Task 1: Negative Log-Likelihood for the Student-$t$ Model\n\nThe model for the data is $y_{i} = f(x_{i}; \\theta) + \\varepsilon_{i}$, where the errors $\\varepsilon_{i}$ are assumed to be independent and identically distributed according to a Student-$t$ distribution, $\\varepsilon_{i} \\sim t_{\\nu}(0, \\sigma)$. The residual for the $i$-th sample is $r_{i}(\\theta) = y_{i} - f(x_{i}; \\theta)$, which corresponds to the unobserved error $\\varepsilon_{i}$.\n\nThe probability density function for each error $\\varepsilon_i = r_i(\\theta)$ is given by\n$$\np_{\\nu,\\sigma}(r_i(\\theta)) = \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\,\\sigma} \\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right)^{-\\frac{\\nu+1}{2}}.\n$$\nDue to the independence of the errors, the likelihood function for the entire dataset, $L(\\theta, \\sigma)$, is the product of the individual probability densities:\n$$\nL(\\theta, \\sigma) = \\prod_{i=1}^{n} p_{\\nu,\\sigma}(r_i(\\theta)) = \\prod_{i=1}^{n} \\left[ \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\,\\sigma} \\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right)^{-\\frac{\\nu+1}{2}} \\right].\n$$\nThe log-likelihood, $\\ell(\\theta, \\sigma) = \\ln(L(\\theta, \\sigma))$, is the sum of the logarithms of the individual densities:\n$$\n\\ell(\\theta, \\sigma) = \\sum_{i=1}^{n} \\ln \\left[ p_{\\nu,\\sigma}(r_i(\\theta)) \\right] = \\sum_{i=1}^{n} \\left[ \\ln\\left(\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}}\\right) - \\ln(\\sigma) - \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right) \\right].\n$$\nThis can be expanded to:\n$$\n\\ell(\\theta, \\sigma) = n \\ln\\left(\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}}\\right) - n \\ln(\\sigma) - \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\ln\\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right).\n$$\nThe negative log-likelihood, $-\\ell(\\theta, \\sigma)$, is:\n$$\n-\\ell(\\theta, \\sigma) = -n \\ln\\left(\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}}\\right) + n \\ln(\\sigma) + \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\ln\\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right).\n$$\nAs instructed, we drop additive terms that do not depend on the parameters $\\theta$ or $\\sigma$. The parameter $\\nu$ is considered fixed. Therefore, the term $-n \\ln\\left(\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}}\\right)$ is dropped. The resulting expression for the negative log-likelihood, which we aim to minimize, is:\n$$\n\\mathcal{L}_{\\text{Student-}t}(\\theta, \\sigma) = n \\ln(\\sigma) + \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\ln\\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right).\n$$\n\n### Task 2: Derivation of the $\\psi$-functions\n\nThe function $\\psi(r)$ is defined as the derivative of the per-sample negative log-likelihood contribution, $\\rho(r)$, with respect to the residual $r$.\n\n**For the Student-$t$ model:**\nThe per-sample contribution to the negative log-likelihood that depends on the residual $r$ is denoted $\\rho_{\\text{Student-}t}(r)$. From the full negative log-likelihood derived above, we identify $\\rho(r)$ by taking the term inside the summation and treating $r_i(\\theta)$ as $r$. Additive terms that are constant with respect to $r$ (like $\\ln(\\sigma)$) are omitted as their derivative is zero.\n$$\n\\rho_{\\text{Student-}t}(r) = \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{r^{2}}{\\nu \\sigma^{2}}\\right).\n$$\nNow, we compute the derivative $\\psi_{\\text{Student-}t}(r) = \\frac{d}{dr}\\rho_{\\text{Student-}t}(r)$:\n$$\n\\psi_{\\text{Student-}t}(r) = \\frac{d}{dr} \\left[ \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{r^{2}}{\\nu \\sigma^{2}}\\right) \\right].\n$$\nUsing the chain rule for derivatives, we get:\n$$\n\\psi_{\\text{Student-}t}(r) = \\frac{\\nu+1}{2} \\cdot \\frac{1}{1 + \\frac{r^{2}}{\\nu \\sigma^{2}}} \\cdot \\frac{d}{dr}\\left(\\frac{r^{2}}{\\nu \\sigma^{2}}\\right) = \\frac{\\nu+1}{2} \\cdot \\frac{1}{1 + \\frac{r^{2}}{\\nu \\sigma^{2}}} \\cdot \\frac{2r}{\\nu \\sigma^{2}}.\n$$\nSimplifying the expression:\n$$\n\\psi_{\\text{Student-}t}(r) = (\\nu+1) \\cdot \\frac{r}{\\nu \\sigma^{2} \\left(1 + \\frac{r^{2}}{\\nu \\sigma^{2}}\\right)} = \\frac{(\\nu+1)r}{\\nu \\sigma^{2} + r^{2}}.\n$$\n\n**For the Gaussian model:**\nThe probability density function is $p_{\\sigma}(\\varepsilon) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\!\\left(-\\frac{\\varepsilon^{2}}{2\\sigma^{2}}\\right)$.\nThe per-sample negative log-likelihood for a residual $r$ is:\n$$\n-\\ln(p_{\\sigma}(r)) = -\\ln\\left(\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\right) - \\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right) = \\ln(\\sqrt{2\\pi}) + \\ln(\\sigma) + \\frac{r^{2}}{2\\sigma^{2}}.\n$$\nThe term that depends on the residual $r$ is $\\rho_{\\text{Gaussian}}(r)$:\n$$\n\\rho_{\\text{Gaussian}}(r) = \\frac{r^{2}}{2\\sigma^{2}}.\n$$\nThis corresponds to the ordinary least-squares objective function, scaled by $1/(2\\sigma^2)$.\nNow, we compute the derivative $\\psi_{\\text{Gaussian}}(r) = \\frac{d}{dr}\\rho_{\\text{Gaussian}}(r)$:\n$$\n\\psi_{\\text{Gaussian}}(r) = \\frac{d}{dr}\\left(\\frac{r^{2}}{2\\sigma^{2}}\\right) = \\frac{2r}{2\\sigma^{2}} = \\frac{r}{\\sigma^{2}}.\n$$\n\n### Task 3: The Ratio $\\Xi(r; \\nu, \\sigma)$\n\nThe ratio $\\Xi(r; \\nu, \\sigma)$ is defined as the quotient of the two $\\psi$-functions:\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{\\psi_{\\text{Student-}t}(r)}{\\psi_{\\text{Gaussian}}(r)}.\n$$\nSubstituting the expressions derived in Task $2$:\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{\\frac{(\\nu+1)r}{\\nu \\sigma^{2} + r^{2}}}{\\frac{r}{\\sigma^{2}}}.\n$$\nAssuming $r \\neq 0$, we can simplify this expression by multiplying by the reciprocal of the denominator:\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{(\\nu+1)r}{\\nu \\sigma^{2} + r^{2}} \\cdot \\frac{\\sigma^{2}}{r}.\n$$\nCanceling the term $r$ from the numerator and denominator gives the final closed-form expression:\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{(\\nu+1)\\sigma^{2}}{\\nu \\sigma^{2} + r^{2}}.\n$$\nThis ratio represents the weight assigned to a residual $r$ by the Student-$t$ model relative to the Gaussian model. For large residuals ($|r| \\to \\infty$), $\\Xi \\to 0$, demonstrating the robustness of the Student-$t$ model by down-weighting the influence of outliers. For small residuals ($r \\to 0$), $\\Xi \\to \\frac{\\nu+1}{\\nu}$, showing a behavior similar to the Gaussian model but with a scaling factor.",
            "answer": "$$\n\\boxed{\\frac{(\\nu+1)\\sigma^{2}}{\\nu \\sigma^{2} + r^{2}}}\n$$"
        }
    ]
}