{
    "hands_on_practices": [
        {
            "introduction": "最大似然估计是统计推断的基石。本练习将引导你从第一性原理出发，推导正态分布均值和方差的最大似然估计，这是无数统计检验和模型的基础。通过这个过程，你将深刻理解对数似然、得分方程以及费雪信息矩阵等核心概念，并明白后者如何量化估计的精度。",
            "id": "3322901",
            "problem": "在一项定量转录组学分析中，假设对单个基因在 $n$ 个技术重复中的信使核糖核酸 (mRNA) 丰度进行重复测量，建模为独立同分布 (i.i.d.) 的随机变量 $y_{1},\\dots,y_{n} \\sim \\mathcal{N}(\\mu,\\sigma^{2})$，其中 $\\mu$ 是未知的平均丰度，$\\sigma^{2}$ 是未知的测量方差。请使用最大似然估计和最小二乘推断的原理，从正态分布的概率密度函数和独立性假设等基本原理出发，构建似然函数。然后：\n- 通过对对数似然函数关于每个参数求导，推导出 $\\mu$ 和 $\\sigma^{2}$ 的得分方程。\n- 求解得分方程，以获得 $\\mu$ 和 $\\sigma^{2}$ 的最大似然估计 (MLE)，并用数据表示。\n- 计算观测Fisher信息矩阵，其定义为对数似然函数关于 $(\\mu,\\sigma^{2})$ 的负Hessian矩阵在最大似然估计值处的取值。\n\n为确保得到单一闭合形式的最终答案，请将观测Fisher信息矩阵在最大似然估计值处计算的行列式作为最终结果报告。最终答案需以符号表达式形式呈现，无需数值舍入，也不需要物理单位。使用记号 $\\hat{\\mu}$ 和 $\\hat{\\sigma}^{2}$ 分别表示 $\\mu$ 和 $\\sigma^{2}$ 的最大似然估计，用 $n$ 表示重复次数。",
            "solution": "用户提供了一个问题陈述，在尝试解答之前需要对其进行验证。\n\n### 第1步：提取已知条件\n- 数据为 $n$ 个独立同分布 (i.i.d.) 的测量值，$y_1, \\dots, y_n$。\n- 数据服从正态分布：$y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$。\n- 分布的参数，即均值 $\\mu$ 和方差 $\\sigma^2$，是未知的。\n- 任务是：\n    1.  从对数似然函数推导出 $\\mu$ 和 $\\sigma^2$ 的得分方程。\n    2.  求解这些方程，找到最大似然估计 (MLE)，记为 $\\hat{\\mu}$ 和 $\\hat{\\sigma}^2$。\n    3.  计算观测Fisher信息矩阵，其定义为对数似然函数的负Hessian矩阵在最大似然估计值处的取值。\n- 需要报告的最终结果是该观测Fisher信息矩阵的行列式，用符号 $n$ 和 $\\hat{\\sigma}^2$ 表示。\n\n### 第2步：使用提取的已知条件进行验证\n- **科学性：** 该问题牢固地植根于统计推断的基本原理，特别是最大似然估计。正态分布是科学领域统计建模的基石，而似然、得分函数和Fisher信息等概念是估计理论与实践的核心。其在定量转录组学中的应用背景是恰当的。\n- **适定性：** 该问题提供了明确的目标和所有必需的信息来实现它。步骤逻辑清晰，导向一个唯一的、定义明确的数学表达式。\n- **客观性：** 问题使用精确、无歧义的数学和统计术语进行陈述，不含任何主观或基于意见的内容。\n\n### 第3步：结论与行动\n该问题是有效的。它在科学上是合理的，问题是适定的、客观的，并且需要从基本原理出发进行标准的、尽管详尽的推导。我现在将开始解答。\n\n***\n\n求解过程首先构建对数似然函数，然后找到其临界点以确定最大似然估计，最后计算在最大值点的曲率以求得观测Fisher信息。\n\n单个观测值 $y_i$ 来自正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 的概率密度函数 (PDF) 为：\n$$ f(y_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right) $$\n鉴于观测值 $y_1, \\dots, y_n$ 是独立同分布的，似然函数 $L(\\mu, \\sigma^2 | \\mathbf{y})$ 是各单个PDF的乘积：\n$$ L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} f(y_i | \\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right) $$\n$$ L(\\mu, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right) $$\n处理对数似然函数 $\\ell(\\mu, \\sigma^2) = \\ln(L(\\mu, \\sigma^2))$ 会更方便：\n$$ \\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 $$\n\n**1. 推导得分方程**\n得分方程是通过将对数似然函数对参数 $\\mu$ 和 $\\sigma^2$ 的一阶偏导数置为零得到的。\n\n关于 $\\mu$ 的偏导数为：\n$$ \\frac{\\partial \\ell}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) $$\n$$ \\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(y_i - \\mu)(-1) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) $$\n第一个得分方程是 $\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) = 0$。\n\n关于 $\\sigma^2$ 的偏导数为：\n$$ \\frac{\\partial \\ell}{\\partial (\\sigma^2)} = \\frac{\\partial}{\\partial (\\sigma^2)} \\left( -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) $$\n$$ \\frac{\\partial \\ell}{\\partial (\\sigma^2)} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 $$\n第二个得分方程是 $-\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 = 0$。\n\n**2. 求解最大似然估计 (MLE)**\n为了找到最大似然估计 $\\hat{\\mu}$ 和 $\\hat{\\sigma}^2$，我们求解得分方程。从第一个方程得出：\n$$ \\frac{1}{\\hat{\\sigma}^2} \\sum_{i=1}^{n} (y_i - \\hat{\\mu}) = 0 \\implies \\sum_{i=1}^{n} y_i - n\\hat{\\mu} = 0 $$\n$$ \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} y_i $$\n均值的最大似然估计是样本均值。\n\n将 $\\hat{\\mu}$ 代入第二个得分方程：\n$$ -\\frac{n}{2\\hat{\\sigma}^2} + \\frac{1}{2(\\hat{\\sigma}^2)^2} \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0 $$\n乘以 $2(\\hat{\\sigma}^2)^2$ 得到：\n$$ -n\\hat{\\sigma}^2 + \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0 $$\n$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 $$\n方差的最大似然估计是分母为 $n$ 的样本方差。\n\n**3. 计算观测Fisher信息矩阵**\n观测Fisher信息矩阵 $I(\\mu, \\sigma^2)$ 是对数似然函数Hessian矩阵 $\\mathbf{H}$ 的负值。Hessian矩阵的元素是 $\\ell$ 的二阶偏导数。\n$$ \\mathbf{H} = \\begin{pmatrix} \\frac{\\partial^2 \\ell}{\\partial \\mu^2} & \\frac{\\partial^2 \\ell}{\\partial \\mu \\partial (\\sigma^2)} \\\\ \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2) \\partial \\mu} & \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} \\end{pmatrix} $$\n我们计算每个元素：\n$$ \\frac{\\partial^2 \\ell}{\\partial \\mu^2} = \\frac{\\partial}{\\partial \\mu} \\left( \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) \\right) = -\\frac{n}{\\sigma^2} $$\n$$ \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2) \\partial \\mu} = \\frac{\\partial}{\\partial (\\sigma^2)} \\left( \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) \\right) = -\\frac{1}{(\\sigma^2)^2} \\sum_{i=1}^{n} (y_i - \\mu) $$\n$$ \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} = \\frac{\\partial}{\\partial (\\sigma^2)} \\left( -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) = \\frac{n}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3} \\sum_{i=1}^{n} (y_i - \\mu)^2 $$\n接下来，我们在最大似然估计值 $(\\hat{\\mu}, \\hat{\\sigma}^2)$ 处计算Hessian矩阵。\n$$ \\left. \\frac{\\partial^2 \\ell}{\\partial \\mu^2} \\right|_{(\\hat{\\mu}, \\hat{\\sigma}^2)} = -\\frac{n}{\\hat{\\sigma}^2} $$\n非对角线项变为零，因为 $\\sum_{i=1}^{n} (y_i - \\hat{\\mu}) = 0$：\n$$ \\left. \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2) \\partial \\mu} \\right|_{(\\hat{\\mu}, \\hat{\\sigma}^2)} = -\\frac{1}{(\\hat{\\sigma}^2)^2} \\sum_{i=1}^{n} (y_i - \\hat{\\mu}) = 0 $$\n对于最后一个元素，我们代入 $\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = n\\hat{\\sigma}^2$：\n$$ \\left. \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} \\right|_{(\\hat{\\mu}, \\hat{\\sigma}^2)} = \\frac{n}{2(\\hat{\\sigma}^2)^2} - \\frac{1}{(\\hat{\\sigma}^2)^3} \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = \\frac{n}{2(\\hat{\\sigma}^2)^2} - \\frac{n\\hat{\\sigma}^2}{(\\hat{\\sigma}^2)^3} = \\frac{n}{2(\\hat{\\sigma}^2)^2} - \\frac{n}{(\\hat{\\sigma}^2)^2} = -\\frac{n}{2(\\hat{\\sigma}^2)^2} $$\n在最大似然估计值处计算的Hessian矩阵为：\n$$ \\mathbf{H}(\\hat{\\mu}, \\hat{\\sigma}^2) = \\begin{pmatrix} -\\frac{n}{\\hat{\\sigma}^2} & 0 \\\\ 0 & -\\frac{n}{2(\\hat{\\sigma}^2)^2} \\end{pmatrix} $$\n观测Fisher信息矩阵为 $I(\\hat{\\mu}, \\hat{\\sigma}^2) = -\\mathbf{H}(\\hat{\\mu}, \\hat{\\sigma}^2)$：\n$$ I(\\hat{\\mu}, \\hat{\\sigma}^2) = \\begin{pmatrix} \\frac{n}{\\hat{\\sigma}^2} & 0 \\\\ 0 & \\frac{n}{2(\\hat{\\sigma}^2)^2} \\end{pmatrix} $$\n最后，我们计算观测Fisher信息矩阵的行列式。由于该矩阵是对角矩阵，其行列式是对角元素的乘积：\n$$ \\det(I(\\hat{\\mu}, \\hat{\\sigma}^2)) = \\left(\\frac{n}{\\hat{\\sigma}^2}\\right) \\left(\\frac{n}{2(\\hat{\\sigma}^2)^2}\\right) = \\frac{n^2}{2(\\hat{\\sigma}^2)^3} $$",
            "answer": "$$\\boxed{\\frac{n^2}{2(\\hat{\\sigma}^2)^{3}}}$$"
        },
        {
            "introduction": "生物系统通常由非线性模型描述，例如著名的米氏动力学模型。本练习将最小二乘法拟合的概念推广到此类非线性情况，并展示其在高斯噪声假设下与最大似然估计的等价性。你将推导高斯-牛顿算法的核心组件，这是一种强大的迭代参数估计算法，从而在优化理论与系统生物学的实际建模之间建立起坚实的桥梁。",
            "id": "3322866",
            "problem": "在一项计算系统生物学研究中，您正在对遵循 Michaelis–Menten 动力学的酶催化反应的初始反应速率测量值进行建模。对于底物浓度 $\\{x_i\\}_{i=1}^{n}$ 和测量的初始速率 $\\{y_i\\}_{i=1}^{n}$，假设观测模型为\n$$\ny_i \\;=\\; \\frac{V_{\\max}\\, x_i}{K_m + x_i} \\;+\\; \\epsilon_i,\n$$\n其中 $\\epsilon_i$ 是均值为 $0$、方差为 $\\sigma^2$ 的独立同分布 (i.i.d.) 高斯噪声项，且 $V_{\\max} \\gt 0$, $K_m \\gt 0$。在高斯噪声假设下，最大似然估计量与最小化残差平方和的非线性最小二乘估计量一致。设第 $i$ 个数据点的残差为\n$$\nr_i(\\theta) \\;=\\; y_i \\;-\\; f(x_i;\\theta), \\quad \\text{with} \\quad \\theta \\equiv (V_{\\max}, K_m), \\quad f(x;\\theta) \\equiv \\frac{V_{\\max} x}{K_m + x}.\n$$\n考虑使用 Gauss–Newton 方法求解非线性最小二乘问题。从高斯噪声最大似然的第一性原理以及雅可比矩阵作为模型关于参数的一阶导数矩阵的定义出发，推导：\n- 单个观测值 $x_i$ 的雅可比行向量，即 $f(x_i;\\theta)$ 关于 $(V_{\\max}, K_m)$ 的偏导数的 $1 \\times 2$ 向量；以及\n- 参数增量 $\\Delta \\theta$ 的显式 Gauss–Newton 正规方程组，以关于 $i$ 的求和形式，通过对称 $2 \\times 2$ 矩阵 $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta)$ 和 $2 \\times 1$ 向量 $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta)$ 的分量来书写，其中 $J_i(\\theta)$ 表示在 $x_i$ 处的雅可比行向量。\n\n将您的最终答案以一个包含三个条目的单行矩阵形式报告：第一项，用 $x_i$, $V_{\\max}$, 和 $K_m$ 表示的雅可比行向量；第二项，写作关于 $i$ 的显式求和形式的 Gauss–Newton 正规方程的 $2 \\times 2$ 对称矩阵；第三项，用残差 $r_i(\\theta) = y_i - f(x_i;\\theta)$ 表示的、写作关于 $i$ 的显式求和形式的 $2 \\times 1$ 右端向量。最终答案中不要包含等号。无需进行数值计算，也无需四舍五入。以符号形式表示最终答案。",
            "solution": "用户提供了一个有效的问题陈述。任务是推导应用于 Michaelis–Menten 动力学模型的 Gauss–Newton 方法的正规方程组的各组成部分。该问题在酶动力学和统计推断方面具有科学依据，是适定的，并且提供了所有必要的定义。我将继续进行推导。\n\n该问题涉及对 Michaelis–Menten 模型函数\n$$\nf(x; \\theta) = \\frac{V_{\\max} x}{K_m + x}\n$$\n的参数 $\\theta = (V_{\\max}, K_m)$ 进行估计。Gauss–Newton 方法通过求解称为正规方程组的线性系统，为参数向量提供迭代更新量 $\\Delta\\theta$：\n$$\n\\left( \\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta) \\right) \\Delta \\theta = \\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta)\n$$\n其中 $J_i(\\theta)$ 是第 $i$ 次观测的雅可比行向量，$r_i(\\theta)$ 是相应的残差。我们的目标是推导雅可比行向量 $J_i(\\theta)$、矩阵 $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta)$ 和向量 $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta)$ 的解析表达式。\n\n首先，我们推导单个观测值 $x_i$ 的雅可比行向量。雅可比矩阵 $J_i(\\theta)$ 是一个 $1 \\times 2$ 的行向量，包含模型函数 $f(x_i; \\theta)$ 关于参数 $V_{\\max}$ 和 $K_m$ 的偏导数：\n$$\nJ_i(\\theta) = \\begin{pmatrix} \\frac{\\partial f(x_i; \\theta)}{\\partial V_{\\max}} & \\frac{\\partial f(x_i; \\theta)}{\\partial K_m} \\end{pmatrix}\n$$\n我们分别计算每个偏导数。关于 $V_{\\max}$ 的导数是：\n$$\n\\frac{\\partial f}{\\partial V_{\\max}} = \\frac{\\partial}{\\partial V_{\\max}} \\left( \\frac{V_{\\max} x_i}{K_m + x_i} \\right) = \\frac{x_i}{K_m + x_i}\n$$\n关于 $K_m$ 的导数使用商法则求得：\n$$\n\\frac{\\partial f}{\\partial K_m} = \\frac{\\partial}{\\partial K_m} \\left( \\frac{V_{\\max} x_i}{K_m + x_i} \\right) = \\frac{0 \\cdot (K_m + x_i) - (V_{\\max} x_i) \\cdot \\frac{\\partial}{\\partial K_m}(K_m + x_i)}{(K_m + x_i)^2} = \\frac{-V_{\\max} x_i}{(K_m + x_i)^2}\n$$\n结合这些结果，第 $i$ 次测量的雅可比行向量为：\n$$\nJ_i(\\theta) = \\begin{pmatrix} \\frac{x_i}{K_m + x_i} & - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix}\n$$\n\n接下来，我们推导正规方程组的 $2 \\times 2$ 对称矩阵，它是雅可比行向量与其转置的外积之和：$\\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta)$。对于单个观测值 $i$，外积 $J_i(\\theta)^{\\top} J_i(\\theta)$ 为：\n$$\nJ_i(\\theta)^{\\top} J_i(\\theta) = \\begin{pmatrix} \\frac{x_i}{K_m + x_i} \\\\ - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix} \\begin{pmatrix} \\frac{x_i}{K_m + x_i} & - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix}\n= \\begin{pmatrix} \\left(\\frac{x_i}{K_m + x_i}\\right)^2 & -\\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} \\\\ -\\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} & \\left(\\frac{V_{\\max} x_i}{(K_m + x_i)^2}\\right)^2 \\end{pmatrix}\n$$\n将这些矩阵对所有观测值 $i=1, \\dots, n$ 求和，得到 Gauss–Newton 正规矩阵：\n$$\n\\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta) = \\begin{pmatrix}\n\\sum_{i=1}^{n} \\frac{x_i^2}{(K_m + x_i)^2} & - \\sum_{i=1}^{n} \\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} \\\\\n- \\sum_{i=1}^{n} \\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} & \\sum_{i=1}^{n} \\frac{V_{\\max}^2 x_i^2}{(K_m + x_i)^4}\n\\end{pmatrix}\n$$\n\n最后，我们推导正规方程组的 $2 \\times 1$ 右端向量 $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta)$，其中 $r_i(\\theta) = y_i - f(x_i; \\theta)$ 是第 $i$ 次观测的残差。该向量是雅可比矩阵转置的和，每一项都乘以相应的标量残差：\n$$\n\\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta) = \\sum_{i=1}^{n} \\begin{pmatrix} \\frac{x_i}{K_m + x_i} \\\\ - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix} r_i(\\theta) = \\begin{pmatrix}\n\\sum_{i=1}^{n} r_i(\\theta) \\frac{x_i}{K_m + x_i} \\\\\n- \\sum_{i=1}^{n} r_i(\\theta) \\frac{V_{\\max} x_i}{(K_m + x_i)^2}\n\\end{pmatrix}\n$$\n这些推导出的组成部分构成了针对此特定参数估计问题的 Gauss–Newton 算法单次迭代所需的必要元素。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\begin{pmatrix} \\frac{x_i}{K_m + x_i} & - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix} & \\begin{pmatrix} \\sum_{i=1}^{n} \\frac{x_i^2}{(K_m + x_i)^2} & - \\sum_{i=1}^{n} \\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} \\\\ - \\sum_{i=1}^{n} \\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} & \\sum_{i=1}^{n} \\frac{V_{\\max}^2 x_i^2}{(K_m + x_i)^4} \\end{pmatrix} & \\begin{pmatrix} \\sum_{i=1}^{n} r_i(\\theta) \\frac{x_i}{K_m + x_i} \\\\ - \\sum_{i=1}^{n} r_i(\\theta) \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "尽管高斯噪声模型很方便，但现实世界中的生物学数据（如代谢组学数据）常含有会扭曲估计结果的离群值。本练习通过用重尾的Student-t分布替代高斯误差模型来探索稳健回归。通过推导和比较两种模型的影响函数，你将定量地理解稳健方法如何自动降低离群值的影响，从而在面对不完美数据时获得更可靠的参数估计。",
            "id": "3322854",
            "problem": "一个代谢组学实验室量化了不同实验条件下的代谢物丰度，并拟合了一个参数回归模型，以根据实验协变量预测测量的响应。设数据为 $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$，包含 $n$ 个独立样本，其中 $y_{i}$ 是测量的对数丰度，$x_{i}$ 是一个协变量向量。该模型假定\n$y_{i} = f(x_{i}; \\theta) + \\varepsilon_{i}$，\n其中 $f(x_{i}; \\theta)$ 是一个平滑预测变量，$\\varepsilon_{i}$ 是独立的、均值为零的误差。在实践中，由于零星的离子抑制或未对准事件，代谢组学数据可能包含异常值，这促使我们使用重尾噪声模型。\n\n假设以下两种误差模型，它们具有一个公共尺度参数 $\\sigma > 0$：\n\n- 学生t模型 (Student-$t$ model): $\\varepsilon_{i} \\sim t_{\\nu}(0, \\sigma)$，自由度为 $\\nu > 0$，其密度为\n$$\np_{\\nu,\\sigma}(\\varepsilon) = \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\,\\sigma} \\left(1 + \\frac{\\varepsilon^{2}}{\\nu \\sigma^{2}}\\right)^{-\\frac{\\nu+1}{2}}.\n$$\n\n- 高斯模型 (Gaussian model): $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$，其密度为\n$$\np_{\\sigma}(\\varepsilon) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\!\\left(-\\frac{\\varepsilon^{2}}{2\\sigma^{2}}\\right).\n$$\n\n令样本 $i$ 的残差为 $r_{i}(\\theta) = y_{i} - f(x_{i}; \\theta)$。\n\n任务：\n\n1. 使用误差的独立性和给定的学生t模型的密度，推导负对数似然作为 $\\theta$ 和 $\\sigma$ 的函数，并用残差 $r_{i}(\\theta)$ 表示。您可以省略不依赖于 $\\theta$ 或 $\\sigma$ 的加性项。\n\n2. 在稳健回归中，对估计方程的影响由单个样本的负对数似然贡献对残差的导数决定，即函数 $\\psi(r) = \\frac{d}{dr}\\rho(r)$，其中 $\\rho(r)$ 是单个样本的负对数似然贡献。请推导以下模型的相应 $\\psi$ 函数：\n   - 具有参数 $\\nu$ 和 $\\sigma$ 的学生t误差模型，\n   - 具有尺度参数 $\\sigma$ 的高斯误差模型。\n\n3. 考虑代谢组学数据集中存在一小部分异常值。为了比较稳健性，定义比率\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{\\psi_{\\text{Student-}t}(r)}{\\psi_{\\text{Gaussian}}(r)}.\n$$\n请提供 $\\Xi(r; \\nu, \\sigma)$ 的一个单一、闭式的解析表达式，用 $r$、$\\nu$ 和 $\\sigma$ 表示。\n\n请给出您对 $\\Xi(r; \\nu, \\sigma)$ 的简化表达式作为最终答案。无需进行数值评估。不要包含任何单位。所有其他推导都是为了支持最终表达式。最终答案必须是单个解析表达式。",
            "solution": "该问题经核实具有科学依据、问题适定且客观。这些任务涉及统计建模和稳健回归中的标准推导，并基于正确的数学定义。我们开始进行解答。\n\n解答的结构是按顺序完成这三个任务。\n\n### 任务 1：学生t模型的负对数似然\n\n数据模型为 $y_{i} = f(x_{i}; \\theta) + \\varepsilon_{i}$，其中误差 $\\varepsilon_{i}$ 假设为独立同分布，服从学生t分布，即 $\\varepsilon_{i} \\sim t_{\\nu}(0, \\sigma)$。第 $i$ 个样本的残差为 $r_{i}(\\theta) = y_{i} - f(x_{i}; \\theta)$，这对应于未观测到的误差 $\\varepsilon_{i}$。\n\n每个误差 $\\varepsilon_i = r_i(\\theta)$ 的概率密度函数由下式给出\n$$\np_{\\nu,\\sigma}(r_i(\\theta)) = \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\,\\sigma} \\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right)^{-\\frac{\\nu+1}{2}}.\n$$\n由于误差是独立的，整个数据集的似然函数 $L(\\theta, \\sigma)$ 是各个概率密度的乘积：\n$$\nL(\\theta, \\sigma) = \\prod_{i=1}^{n} p_{\\nu,\\sigma}(r_i(\\theta)) = \\prod_{i=1}^{n} \\left[ \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\,\\sigma} \\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right)^{-\\frac{\\nu+1}{2}} \\right].\n$$\n对数似然 $\\ell(\\theta, \\sigma) = \\ln(L(\\theta, \\sigma))$ 是各个密度对数的总和：\n$$\n\\ell(\\theta, \\sigma) = \\sum_{i=1}^{n} \\ln \\left[ p_{\\nu,\\sigma}(r_i(\\theta)) \\right] = \\sum_{i=1}^{n} \\left[ \\ln\\left(\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}}\\right) - \\ln(\\sigma) - \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right) \\right].\n$$\n这可以展开为：\n$$\n\\ell(\\theta, \\sigma) = n \\ln\\left(\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}}\\right) - n \\ln(\\sigma) - \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\ln\\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right).\n$$\n负对数似然 $-\\ell(\\theta, \\sigma)$ 是：\n$$\n-\\ell(\\theta, \\sigma) = -n \\ln\\left(\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}}\\right) + n \\ln(\\sigma) + \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\ln\\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right).\n$$\n按照指示，我们去掉不依赖于参数 $\\theta$ 或 $\\sigma$ 的加性项。参数 $\\nu$ 被认为是固定的。因此，项 $-n \\ln\\left(\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}}\\right)$ 被去掉。我们旨在最小化的负对数似然的最终表达式是：\n$$\n\\mathcal{L}_{\\text{Student-}t}(\\theta, \\sigma) = n \\ln(\\sigma) + \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\ln\\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right).\n$$\n\n### 任务 2：$\\psi$ 函数的推导\n\n函数 $\\psi(r)$ 定义为单个样本的负对数似然贡献 $\\rho(r)$ 对残差 $r$ 的导数。\n\n**对于学生t模型：**\n单个样本对负对数似然的贡献中依赖于残差 $r$ 的部分记为 $\\rho_{\\text{Student-}t}(r)$。从上面推导的完整负对数似然中，我们通过取求和内的项并将 $r_i(\\theta)$ 视为 $r$ 来识别 $\\rho(r)$。对于 $r$ 是常数的加性项（如 $\\ln(\\sigma)$）被省略，因为它们的导数为零。\n$$\n\\rho_{\\text{Student-}t}(r) = \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{r^{2}}{\\nu \\sigma^{2}}\\right).\n$$\n现在，我们计算导数 $\\psi_{\\text{Student-}t}(r) = \\frac{d}{dr}\\rho_{\\text{Student-}t}(r)$:\n$$\n\\psi_{\\text{Student-}t}(r) = \\frac{d}{dr} \\left[ \\frac{\\nu+1}{2} \\ln\\left(1 + \\fracr^{2}}{\\nu \\sigma^{2}}\\right) \\right].\n$$\n使用链式求导法则，我们得到：\n$$\n\\psi_{\\text{Student-}t}(r) = \\frac{\\nu+1}{2} \\cdot \\frac{1}{1 + \\frac{r^{2}}{\\nu \\sigma^{2}}} \\cdot \\frac{d}{dr}\\left(\\frac{r^{2}}{\\nu \\sigma^{2}}\\right) = \\frac{\\nu+1}{2} \\cdot \\frac{1}{1 + \\frac{r^{2}}{\\nu \\sigma^{2}}} \\cdot \\frac{2r}{\\nu \\sigma^{2}}.\n$$\n化简表达式：\n$$\n\\psi_{\\text{Student-}t}(r) = (\\nu+1) \\cdot \\frac{r}{\\nu \\sigma^{2} \\left(1 + \\frac{r^{2}}{\\nu \\sigma^{2}}\\right)} = \\frac{(\\nu+1)r}{\\nu \\sigma^{2} + r^{2}}.\n$$\n\n**对于高斯模型：**\n概率密度函数为 $p_{\\sigma}(\\varepsilon) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\!\\left(-\\frac{\\varepsilon^{2}}{2\\sigma^{2}}\\right)$。\n单个样本残差为 $r$ 时的负对数似然为：\n$$\n-\\ln(p_{\\sigma}(r)) = -\\ln\\left(\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\right) - \\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right) = \\ln(\\sqrt{2\\pi}) + \\ln(\\sigma) + \\frac{r^{2}}{2\\sigma^{2}}.\n$$\n依赖于残差 $r$ 的项是 $\\rho_{\\text{Gaussian}}(r)$:\n$$\n\\rho_{\\text{Gaussian}}(r) = \\frac{r^{2}}{2\\sigma^{2}}.\n$$\n这对应于普通最小二乘目标函数，并按 $1/(2\\sigma^2)$ 进行了缩放。\n现在，我们计算导数 $\\psi_{\\text{Gaussian}}(r) = \\frac{d}{dr}\\rho_{\\text{Gaussian}}(r)$:\n$$\n\\psi_{\\text{Gaussian}}(r) = \\frac{d}{dr}\\left(\\frac{r^{2}}{2\\sigma^{2}}\\right) = \\frac{2r}{2\\sigma^{2}} = \\frac{r}{\\sigma^{2}}.\n$$\n\n### 任务 3：比率 $\\Xi(r; \\nu, \\sigma)$\n\n比率 $\\Xi(r; \\nu, \\sigma)$ 定义为两个 $\\psi$ 函数的商：\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{\\psi_{\\text{Student-}t}(r)}{\\psi_{\\text{Gaussian}}(r)}.\n$$\n代入任务2中推导出的表达式：\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{\\frac{(\\nu+1)r}{\\nu \\sigma^{2} + r^{2}}}{\\frac{r}{\\sigma^{2}}}.\n$$\n假设 $r \\neq 0$，我们可以通过乘以分母的倒数来简化此表达式：\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{(\\nu+1)r}{\\nu \\sigma^{2} + r^{2}} \\cdot \\frac{\\sigma^{2}}{r}.\n$$\n从分子和分母中消去项 $r$，得到最终的闭式表达式：\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{(\\nu+1)\\sigma^{2}}{\\nu \\sigma^{2} + r^{2}}.\n$$\n这个比率表示学生t模型相对于高斯模型赋给残差 $r$ 的权重。对于大的残差（$|r| \\to \\infty$），$\\Xi \\to 0$，这通过降低异常值的影响证明了学生t模型的稳健性。对于小的残差（$r \\to 0$），$\\Xi \\to \\frac{\\nu+1}{\\nu}$，表现出与高斯模型类似的行为，但带有一个缩放因子。",
            "answer": "$$\n\\boxed{\\frac{(\\nu+1)\\sigma^{2}}{\\nu \\sigma^{2} + r^{2}}}\n$$"
        }
    ]
}