{
    "hands_on_practices": [
        {
            "introduction": "We begin with a foundational case of structural non-identifiability, where parameters are \"confounded\" because they only appear in the model equations as an inseparable combination. This exercise  first asks you to demonstrate this issue in a simple linear system by deriving its input-output map, revealing that only the product of two parameters, $k_a k_b$, can be determined. It then provides a powerful lesson in experimental design, guiding you to devise and analyze new experiments that successfully isolate and identify each parameter individually.",
            "id": "3352677",
            "problem": "Consider a single-compartment reporter governed by a linear ordinary differential equation with a production term that is the product of two unknown parameters. The state $x(t)$ evolves according to $x'(t) = k_a k_b u(t) - \\gamma x(t)$ and the measured output is $y(t) = x(t)$. Here, $k_a > 0$ and $k_b > 0$ are unknown constants, $u(t)$ is a known, user-programmable input, and $\\gamma > 0$ is a known degradation rate constant. Assume that $x(0)$ is known and that $y(t)$ is measured continuously and noiselessly on a finite interval $[0,T]$.\n\nUsing only the foundational principles of linear systems and the definition of structural identifiability, derive the input-output map and determine which parameter combination is structurally identifiable from the single-input experiment with $u(t)$. Then, to break the product confounding, you are allowed to perform two additional, orthogonal experiments that independently address the two modules underlying $k_a$ and $k_b$:\n\n- Experiment A: reconfigure the system such that the production term isolates $k_a$ with an independently programmable input $u_a(t)$, yielding the dynamics $x'(t) = k_a u_a(t) - \\gamma x(t)$ with $y(t) = x(t)$.\n- Experiment B: reconfigure the system such that the production term isolates $k_b$ with an independently programmable input $u_b(t)$, yielding the dynamics $x'(t) = k_b u_b(t) - \\gamma x(t)$ with $y(t) = x(t)$.\n\nIn both experiments, apply a step input with known amplitudes $A_a > 0$ and $A_b > 0$, respectively: $u_a(t) = A_a$ for $t \\ge 0$ and $u_b(t) = A_b$ for $t \\ge 0$, and use the same initial condition $x(0)=0$. Let $y_{\\infty}^{(A)}$ and $y_{\\infty}^{(B)}$ denote the steady-state outputs observed in Experiments A and B, respectively. Using first principles of linear differential equations, derive closed-form expressions for $k_a$ and $k_b$ in terms of $\\gamma$, $A_a$, $A_b$, $y_{\\infty}^{(A)}$, and $y_{\\infty}^{(B)}$.\n\nProvide your final answer as a single row matrix containing the expressions for $k_a$ and $k_b$. No numerical evaluation is required, and you should report symbolic expressions only. Do not include units in your final boxed answer.",
            "solution": "The user has provided a problem concerning parameter identifiability in a simple linear ordinary differential equation (ODE) model, a common topic in computational systems biology. I will first validate the problem statement and then proceed to a full solution.\n\n### Step 1: Problem Validation\n\nI will begin by extracting the given information and validating the problem's integrity.\n\n**Extracted Givens:**\n1.  **Initial System:**\n    -   Dynamics: $x'(t) = k_a k_b u(t) - \\gamma x(t)$\n    -   Output: $y(t) = x(t)$\n    -   Parameters: $k_a > 0$, $k_b > 0$ (unknown constants); $\\gamma > 0$ (known constant).\n    -   Input: $u(t)$ is a known, programmable function.\n    -   Initial Condition: $x(0)$ is known.\n    -   Observation: $y(t)$ is measured continuously and without noise on $[0,T]$.\n\n2.  **Experiment A:**\n    -   Dynamics: $x'(t) = k_a u_a(t) - \\gamma x(t)$\n    -   Output: $y(t) = x(t)$\n    -   Input: $u_a(t) = A_a$ for $t \\ge 0$, where $A_a > 0$ is a known constant.\n    -   Initial Condition: $x(0) = 0$.\n    -   Observation: Steady-state output $y_{\\infty}^{(A)}$ is measured.\n\n3.  **Experiment B:**\n    -   Dynamics: $x'(t) = k_b u_b(t) - \\gamma x(t)$\n    -   Output: $y(t) = x(t)$\n    -   Input: $u_b(t) = A_b$ for $t \\ge 0$, where $A_b > 0$ is a known constant.\n    -   Initial Condition: $x(0) = 0$.\n    -   Observation: Steady-state output $y_{\\infty}^{(B)}$ is measured.\n\n**Validation Verdict:**\nThe problem is **valid**.\n-   It is **scientifically grounded** in the principles of linear systems theory and differential equations, which are fundamental to modeling in STEM fields.\n-   It is **well-posed**, providing all necessary information to uniquely determine the quantities requested.\n-   It is **objective**, stated in precise mathematical language without subjective or ambiguous terms.\n-   The scenario describes a classic case of structural non-identifiability and how it can be resolved through targeted experimental design, which is a core concept in systems biology. The problem does not violate any of the invalidity criteria.\n\n### Step 2: Derivation of the Solution\n\nHaving validated the problem, I will now derive the solution in three parts as requested.\n\n**Part 1: Identifiability Analysis of the Initial System**\n\nThe initial system is described by the linear first-order ODE:\n$$\nx'(t) + \\gamma x(t) = k_a k_b u(t)\n$$\nThe output is given by $y(t) = x(t)$. To determine the input-output relationship, we can use the Laplace transform. Let $X(s) = \\mathcal{L}\\{x(t)\\}$ and $U(s) = \\mathcal{L}\\{u(t)\\}$. Applying the transform to the ODE yields:\n$$\nsX(s) - x(0) = k_a k_b U(s) - \\gamma X(s)\n$$\nSolving for $X(s)$, which is equal to the Laplace transform of the output $Y(s)$, we get:\n$$\n(s + \\gamma) X(s) = k_a k_b U(s) + x(0)\n$$\n$$\nY(s) = X(s) = \\frac{k_a k_b}{s + \\gamma} U(s) + \\frac{x(0)}{s + \\gamma}\n$$\nThe input-output map, independent of the initial condition, is captured by the transfer function $G(s)$, defined as the ratio of the output transform to the input transform assuming a zero initial state, $G(s) = \\frac{Y(s)}{U(s)}|_{x(0)=0}$.\n$$\nG(s) = \\frac{k_a k_b}{s + \\gamma}\n$$\nFrom the experimental data (measurements of $y(t)$ for a given input $u(t)$), one can, in principle, identify the transfer function $G(s)$. The problem states that $\\gamma$ is a known constant. Therefore, the pole of the transfer function at $s = -\\gamma$ is known. The only parameter of the model that can be determined from this experiment is the numerator of the transfer function, which is the product $p = k_a k_b$. The individual parameters $k_a$ and $k_b$ cannot be distinguished. Any pair of parameters $(k_a', k_b')$ such that $k_a'k_b' = p$ would yield the exact same input-output behavior. This situation is known as structural non-identifiability. The identifiable parameter combination is the product $k_a k_b$.\n\n**Part 2: Derivation of $k_a$ from Experiment A**\n\nIn Experiment A, the system is reconfigured to isolate $k_a$. The dynamics are governed by:\n$$\nx'(t) = k_a u_a(t) - \\gamma x(t)\n$$\nThe input is a step function of known amplitude $A_a > 0$, so $u_a(t) = A_a$ for $t \\ge 0$. The initial condition is $x(0) = 0$. The equation becomes:\n$$\nx'(t) + \\gamma x(t) = k_a A_a\n$$\nWe are asked to use the steady-state output $y_{\\infty}^{(A)}$ to find $k_a$. By definition, the system is at steady state when its state variables are no longer changing with time. Mathematically, this corresponds to the limit $t \\to \\infty$, where $x'(t) \\to 0$. Let $x_{\\infty}^{(A)}$ be the steady-state value of $x(t)$. The ODE at steady state reduces to an algebraic equation:\n$$\n0 + \\gamma x_{\\infty}^{(A)} = k_a A_a\n$$\nThe measured output is $y(t) = x(t)$, so the measured steady-state output is $y_{\\infty}^{(A)} = x_{\\infty}^{(A)}$. Substituting this into the equation:\n$$\n\\gamma y_{\\infty}^{(A)} = k_a A_a\n$$\nThe problem states that $\\gamma$, $A_a$ are known, and $y_{\\infty}^{(A)}$ is measured. Since $A_a > 0$, we can solve for $k_a$:\n$$\nk_a = \\frac{\\gamma y_{\\infty}^{(A)}}{A_a}\n$$\nThis experiment successfully breaks the confounding from the initial setup and allows for the unique determination of $k_a$.\n\n**Part 3: Derivation of $k_b$ from Experiment B**\n\nExperiment B is designed to isolate $k_b$ in a perfectly analogous manner. The dynamics are:\n$$\nx'(t) = k_b u_b(t) - \\gamma x(t)\n$$\nThe input is a step function $u_b(t) = A_b$ for $t \\ge 0$, with known amplitude $A_b > 0$. The initial condition is $x(0) = 0$. The equation becomes:\n$$\nx'(t) + \\gamma x(t) = k_b A_b\n$$\nWe again consider the steady-state condition where $t \\to \\infty$ and $x'(t) \\to 0$. Let $x_{\\infty}^{(B)}$ be the steady-state value of $x(t)$. The differential equation simplifies to:\n$$\n0 + \\gamma x_{\\infty}^{(B)} = k_b A_b\n$$\nThe measured steady-state output is $y_{\\infty}^{(B)} = x_{\\infty}^{(B)}$. Substituting this gives:\n$$\n\\gamma y_{\\infty}^{(B)} = k_b A_b\n$$\nSince $\\gamma$ and $A_b$ are known ($A_b>0$) and $y_{\\infty}^{(B)}$ is measured, we can solve for $k_b$:\n$$\nk_b = \\frac{\\gamma y_{\\infty}^{(B)}}{A_b}\n$$\nThis experiment allows for the unique determination of $k_b$. By performing Experiments A and B, the structural non-identifiability of the original system is fully resolved.\n\nThe final answer requires the expressions for $k_a$ and $k_b$ to be presented in a single row matrix.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\gamma y_{\\infty}^{(A)}}{A_a}  \\frac{\\gamma y_{\\infty}^{(B)}}{A_b} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Building upon the concept of parameter confounding, we now investigate how identifiability issues manifest in a more complex, nonlinear biological network. This practice  uses a canonical two-site phosphorylation cycle and challenges you to determine what can be learned from steady-state measurements alone—a common scenario in experimental biology. By working through the algebraic steady-state conditions, you will find that individual rate constants are unidentifiable, but their ratios, corresponding to equilibrium constants, can be uniquely determined.",
            "id": "3352624",
            "problem": "Consider a two-site phosphorylation cycle for a single substrate in a well-mixed cellular compartment. The substrate exists in three modification states: unphosphorylated $S_{0}$, singly phosphorylated $S_{1}$, and doubly phosphorylated $S_{2}$. Effective first-order mass-action transitions describe phosphorylation and dephosphorylation between adjacent states:\n$$S_{0} \\xrightleftharpoons[k_{-1}]{k_{1}} S_{1} \\xrightleftharpoons[k_{-2}]{k_{2}} S_{2},$$\nwhere $k_{1}$ and $k_{2}$ are forward rate constants for phosphorylation and $k_{-1}$ and $k_{-2}$ are reverse rate constants for dephosphorylation. Assume all rate constants are strictly positive. The total substrate is conserved:\n$$S_{T} = S_{0} + S_{1} + S_{2}.$$\nAt steady state, $S_{1}^{\\ast}$ and $S_{2}^{\\ast}$ are experimentally measurable, and $S_{T}$ is known. No time-course data is available; only steady-state is observed. Using the definitions of mass-action kinetics, the steady-state condition, and the conservation law, derive which combinations of the forward and reverse rate constants are structurally identifiable from the steady-state measurements $(S_{1}^{\\ast}, S_{2}^{\\ast}, S_{T})$. Express your final answer as analytic expressions in terms of the measurable steady-state fractions\n$$x_{1} = \\frac{S_{1}^{\\ast}}{S_{T}}, \\quad x_{2} = \\frac{S_{2}^{\\ast}}{S_{T}},$$\nand provide the identifiable combinations as a single row matrix containing closed-form expressions. No numerical approximation is required, and no units are to be included in the final answer box.",
            "solution": "The user wants me to identify which combinations of rate constants are structurally identifiable from steady-state measurements in a two-site phosphorylation model.\n\n### Step 1: Extract Givens\n- **Model:** A two-site phosphorylation cycle with states $S_{0}$, $S_{1}$, and $S_{2}$.\n- **Reactions:** $S_{0} \\xrightleftharpoons[k_{-1}]{k_{1}} S_{1} \\xrightleftharpoons[k_{-2}]{k_{2}} S_{2}$.\n- **Kinetics:** Effective first-order mass-action.\n- **Rate Constants:** $k_{1}, k_{-1}, k_{2}, k_{-2}$ are all strictly positive real numbers.\n- **Conservation Law:** The total substrate concentration $S_{T}$ is conserved: $S_{T} = S_{0} + S_{1} + S_{2}$.\n- **Measurements:** Steady-state concentrations $S_{1}^{\\ast}$ and $S_{2}^{\\ast}$ are measurable, and $S_{T}$ is known.\n- **Data Condition:** Only steady-state data is available; no time-course data.\n- **Objective:** Derive the structurally identifiable combinations of the rate constants in terms of the measurable steady-state fractions $x_{1} = \\frac{S_{1}^{\\ast}}{S_{T}}$ and $x_{2} = \\frac{S_{2}^{\\ast}}{S_{T}}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n\n- **Scientifically Grounded:** The problem describes a canonical model in systems biology, the futile cycle or phosphorylation chain. The use of mass-action kinetics and conservation laws are fundamental principles. The model is scientifically sound and widely used.\n- **Well-Posed:** The problem is well-posed. It asks for structurally identifiable parameter combinations, which is a standard and well-defined question in parameter estimation and systems identification. A unique solution for these combinations exists based on the provided information.\n- **Objective:** The problem is stated using precise, objective, and formal mathematical language, free from ambiguity or subjective claims.\n- **Other Flaws:**\n  - The problem does not violate any scientific principles.\n  - It is directly relevant to the topic of structural identifiability in computational systems biology.\n  - The setup is complete and consistent, providing all necessary information (reactions, kinetics, conservation law, and measurements) to solve the problem.\n  - The assumptions are standard in the field and do not represent unrealistic or infeasible conditions.\n  - The terminology is clearly defined, and the structure of the problem is logical.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed to derive the solution.\n\nTo determine the structurally identifiable parameter combinations, we begin by writing the system of ordinary differential equations (ODEs) that describe the dynamics according to the law of mass-action. Let $S_{0}(t)$, $S_{1}(t)$, and $S_{2}(t)$ be the concentrations of the respective species at time $t$.\n\nThe system of ODEs is:\n$$\n\\frac{dS_{0}}{dt} = -k_{1}S_{0} + k_{-1}S_{1} \\\\\n\\frac{dS_{1}}{dt} = k_{1}S_{0} - k_{-1}S_{1} - k_{2}S_{1} + k_{-2}S_{2} \\\\\n\\frac{dS_{2}}{dt} = k_{2}S_{1} - k_{-2}S_{2}\n$$\nAt steady state, the concentrations of all species are constant, so their time derivatives are zero. Let the steady-state concentrations be denoted by $S_{0}^{\\ast}$, $S_{1}^{\\ast}$, and $S_{2}^{\\ast}$.\n$$\n\\frac{dS_{0}}{dt} = 0 \\implies -k_{1}S_{0}^{\\ast} + k_{-1}S_{1}^{\\ast} = 0 \\quad (1) \\\\\n\\frac{dS_{1}}{dt} = 0 \\implies k_{1}S_{0}^{\\ast} - k_{-1}S_{1}^{\\ast} - k_{2}S_{1}^{\\ast} + k_{-2}S_{2}^{\\ast} = 0 \\quad (2) \\\\\n\\frac{dS_{2}}{dt} = 0 \\implies k_{2}S_{1}^{\\ast} - k_{-2}S_{2}^{\\ast} = 0 \\quad (3)\n$$\nNote that the sum of these three equations is zero, $\\frac{d(S_{0}+S_{1}+S_{2})}{dt} = 0$, which is consistent with the conservation law. This means that only two of the three equations are algebraically independent. We will use equations $(1)$ and $(3)$.\n\nFrom equation $(1)$, we can express a ratio of rate constants in terms of steady-state concentrations:\n$$k_{1}S_{0}^{\\ast} = k_{-1}S_{1}^{\\ast} \\implies \\frac{k_{1}}{k_{-1}} = \\frac{S_{1}^{\\ast}}{S_{0}^{\\ast}}$$\nFrom equation $(3)$, we find another ratio of rate constants:\n$$k_{2}S_{1}^{\\ast} = k_{-2}S_{2}^{\\ast} \\implies \\frac{k_{2}}{k_{-2}} = \\frac{S_{2}^{\\ast}}{S_{1}^{\\ast}}$$\nThe problem states that $S_{1}^{\\ast}$, $S_{2}^{\\ast}$, and the total concentration $S_{T}$ are known quantities. The concentration $S_{0}^{\\ast}$ is not directly measured but can be determined from the conservation law:\n$$S_{T} = S_{0}^{\\ast} + S_{1}^{\\ast} + S_{2}^{\\ast} \\implies S_{0}^{\\ast} = S_{T} - S_{1}^{\\ast} - S_{2}^{\\ast}$$\nNow we substitute this expression for $S_{0}^{\\ast}$ into our equation for the first rate constant ratio:\n$$\\frac{k_{1}}{k_{-1}} = \\frac{S_{1}^{\\ast}}{S_{T} - S_{1}^{\\ast} - S_{2}^{\\ast}}$$\nThe problem requires the answer to be in terms of the measurable fractions $x_{1} = \\frac{S_{1}^{\\ast}}{S_{T}}$ and $x_{2} = \\frac{S_{2}^{\\ast}}{S_{T}}$. To achieve this, we divide the numerator and denominator of our expressions by the known total concentration $S_{T}$.\n\nFor the first ratio:\n$$\\frac{k_{1}}{k_{-1}} = \\frac{S_{1}^{\\ast}/S_{T}}{(S_{T} - S_{1}^{\\ast} - S_{2}^{\\ast})/S_{T}} = \\frac{S_{1}^{\\ast}/S_{T}}{1 - S_{1}^{\\ast}/S_{T} - S_{2}^{\\ast}/S_{T}} = \\frac{x_{1}}{1 - x_{1} - x_{2}}$$\nFor the second ratio:\n$$\\frac{k_{2}}{k_{-2}} = \\frac{S_{2}^{\\ast}}{S_{1}^{\\ast}} = \\frac{S_{2}^{\\ast}/S_{T}}{S_{1}^{\\ast}/S_{T}} = \\frac{x_{2}}{x_{1}}$$\nThese two expressions, $\\frac{k_{1}}{k_{-1}}$ and $\\frac{k_{2}}{k_{-2}}$, relate the parameters to the measurable quantities. Since we have two independent equations and four unknown parameters ($k_{1}, k_{-1}, k_{2}, k_{-2}$), we cannot solve for each parameter individually. However, we can uniquely determine the values of these two specific combinations of parameters. Therefore, the structurally identifiable combinations are precisely these two ratios. For any non-trivial steady state where $x_1 > 0$ and $1-x_1-x_2>0$, these expressions are well-defined. The problem stating that all rate constants are strictly positive ensures such a non-trivial steady state can exist.\n\nThe identifiable combinations are:\n$$\n\\text{Combination 1: } \\frac{k_{1}}{k_{-1}} = \\frac{x_{1}}{1 - x_{1} - x_{2}} \\\\\n\\text{Combination 2: } \\frac{k_{2}}{k_{-2}} = \\frac{x_{2}}{x_{1}}\n$$\nThese represent the equilibrium constants for the two reaction steps.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{x_{1}}{1 - x_{1} - x_{2}}  \\frac{x_{2}}{x_{1}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Our focus now shifts from the intrinsic structure of a model to the practical challenge of estimating parameters from finite, noisy data. This exercise  introduces a cornerstone of practical identifiability analysis: the Fisher Information Matrix ($FIM$). You will derive the $FIM$ for a simple time-dependent model, establishing a direct link between model sensitivities, measurement noise, and the best possible precision of parameter estimates. This practice reveals how experimental design, such as the choice of sampling times, critically impacts our ability to confidently determine parameter values.",
            "id": "3352673",
            "problem": "Consider a two-parameter linear-in-parameters model for a measured output in computational systems biology, given by $y(t;\\theta)=\\theta_1 \\exp(-t)+\\theta_2 \\exp(-2 t)$, where $\\theta=\\begin{pmatrix}\\theta_1  \\theta_2\\end{pmatrix}^{\\top}$ denotes unknown parameters. Suppose one collects $n$ independent observations at sampling times $\\{t_i\\}_{i=1}^{n}$, with measurement model $z_i=y(t_i;\\theta)+\\varepsilon_i$, where the noise $\\varepsilon_i$ is independent and identically distributed Gaussian with zero mean and variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nStarting from the definition of the Fisher Information Matrix (FIM) as the expected negative Hessian of the log-likelihood under independent Gaussian observations with known variance, derive the Fisher Information Matrix with respect to $\\theta$ for the described model. Then, using the relationship between the Fisher Information Matrix and the Cramér–Rao lower bound (CRLB), state a clear condition on the sampling times $\\{t_i\\}$ that ensures practical identifiability of $\\theta$ under this measurement design.\n\nAnswer specification: Your final reported quantity must be a single closed-form analytic expression for the determinant of the Fisher Information Matrix in terms of $\\{t_i\\}_{i=1}^{n}$ and $\\sigma^2$. No rounding is required, and no units are to be reported. Provide the identifiability condition in words within your derivation, but do not include it in the final boxed answer.",
            "solution": "The problem requires the derivation of the Fisher Information Matrix (FIM) for a given two-parameter model, the statement of the condition for practical parameter identifiability, and the calculation of the determinant of the FIM.\n\nThe model for the measured output is given by $y(t;\\theta) = \\theta_1 \\exp(-t) + \\theta_2 \\exp(-2t)$, where the parameter vector is $\\theta = \\begin{pmatrix} \\theta_1  \\theta_2 \\end{pmatrix}^{\\top}$. The measurement model for $n$ independent observations at times $\\{t_i\\}_{i=1}^{n}$ is $z_i = y(t_i;\\theta) + \\varepsilon_i$, with noise $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ being independent and identically distributed Gaussian with zero mean and known variance $\\sigma^2$.\n\nThe probability density function for a single measurement $z_i$ is given by the Gaussian distribution:\n$$p(z_i|\\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z_i - y(t_i;\\theta))^2}{2\\sigma^2}\\right)$$\nDue to the independence of the measurements, the likelihood function for the entire dataset $\\mathbf{z} = \\{z_i\\}_{i=1}^{n}$ is the product of the individual probabilities:\n$$\\mathcal{L}(\\theta; \\mathbf{z}) = \\prod_{i=1}^{n} p(z_i|\\theta, \\sigma^2) = \\left(2\\pi\\sigma^2\\right)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (z_i - y(t_i;\\theta))^2\\right)$$\nThe log-likelihood function, denoted $\\ell(\\theta)$, is:\n$$\\ell(\\theta) = \\ln \\mathcal{L}(\\theta; \\mathbf{z}) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (z_i - y(t_i;\\theta))^2$$\nThe Fisher Information Matrix, $F$, is defined as the negative of the expectation of the Hessian matrix of the log-likelihood. The elements of the FIM are given by $F_{jk} = -E\\left[\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta_j \\partial \\theta_k}\\right]$.\n\nFirst, we find the first partial derivatives of $\\ell(\\theta)$ with respect to the parameters $\\theta_j$:\n$$\\frac{\\partial \\ell}{\\partial \\theta_j} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(z_i - y(t_i;\\theta)) \\left(-\\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_j}\\right) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (z_i - y(t_i;\\theta)) \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_j}$$\nNext, we find the second partial derivatives:\n$$\\frac{\\partial^2 \\ell}{\\partial \\theta_k \\partial \\theta_j} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ \\left(-\\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_k}\\right) \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_j} + (z_i - y(t_i;\\theta)) \\frac{\\partial^2 y(t_i;\\theta)}{\\partial \\theta_k \\partial \\theta_j} \\right]$$\nTaking the expectation, we use the fact that $E[z_i] = y(t_i;\\theta)$, which implies $E[z_i - y(t_i;\\theta)] = E[\\varepsilon_i] = 0$. This makes the second term in the summation vanish:\n$$E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta_k \\partial \\theta_j}\\right] = -\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_k} \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_j}$$\nTherefore, the elements of the FIM are:\n$$F_{jk} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta_k \\partial \\theta_j}\\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_j} \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_k}$$\nThis is the general formula for the FIM for a model with additive, i.i.d. Gaussian noise.\nFor our specific model, we calculate the partial derivatives of $y(t;\\theta)$ with respect to $\\theta_1$ and $\\theta_2$, which are the model sensitivities:\n$$\\frac{\\partial y(t;\\theta)}{\\partial \\theta_1} = \\exp(-t)$$\n$$\\frac{\\partial y(t;\\theta)}{\\partial \\theta_2} = \\exp(-2t)$$\nNote that since the model is linear in the parameters, these sensitivities are independent of $\\theta$.\n\nWe can now construct the $2 \\times 2$ FIM, $F$:\n$$F_{11} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left(\\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_1}\\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (\\exp(-t_i))^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-2t_i)$$\n$$F_{22} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left(\\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_2}\\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (\\exp(-2t_i))^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-4t_i)$$\n$$F_{12} = F_{21} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_1} \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-t_i) \\exp(-2t_i) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-3t_i)$$\nThe Fisher Information Matrix is thus:\n$$F = \\frac{1}{\\sigma^2} \\begin{pmatrix} \\sum_{i=1}^{n} \\exp(-2t_i)  \\sum_{i=1}^{n} \\exp(-3t_i) \\\\ \\sum_{i=1}^{n} \\exp(-3t_i)  \\sum_{i=1}^{n} \\exp(-4t_i) \\end{pmatrix}$$\n\nThe Cramér-Rao Lower Bound (CRLB) establishes that the covariance matrix of any unbiased estimator $\\hat{\\theta}$ is bounded below by the inverse of the FIM, i.e., $\\mathrm{Cov}(\\hat{\\theta}) \\ge F^{-1}$. Practical identifiability requires that parameters can be estimated with finite variance. This implies that the CRLB, $F^{-1}$, must exist and be finite. For $F^{-1}$ to exist, the FIM, $F$, must be invertible (non-singular). A matrix is invertible if and only if its determinant is non-zero. Since $\\sigma^2 > 0$, the condition $\\det(F) \\ne 0$ is equivalent to the matrix $S^\\top S$ being invertible, where $S$ is the $n \\times 2$ sensitivity matrix with columns corresponding to the sensitivities with respect to $\\theta_1$ and $\\theta_2$. Invertibility of $S^\\top S$ requires the columns of $S$ to be linearly independent.\nThe columns of $S$ are the vectors $(\\exp(-t_1), \\dots, \\exp(-t_n))^\\top$ and $(\\exp(-2t_1), \\dots, \\exp(-2t_n))^\\top$. These vectors are linearly dependent if and only if one is a scalar multiple of the other, i.e., $\\exp(-2t_i) = c \\cdot \\exp(-t_i)$ for some constant $c$ and for all $i=1, \\dots, n$. This simplifies to $\\exp(-t_i) = c$. Since $\\exp(-t)$ is a strictly monotonic function, this can only hold for all $i$ if all sampling times $t_i$ are identical. If all $t_i$ are the same, $\\det(F) = 0$, and the parameters are not practically identifiable. Therefore, a clear condition on the sampling times that ensures practical identifiability of $\\theta$ is that the set of sampling times $\\{t_i\\}_{i=1}^{n}$ must contain at least two distinct values.\n\nFinally, we compute the determinant of the FIM as requested. Using the property $\\det(kA) = k^d \\det(A)$ for a $d \\times d$ matrix, with $d=2$:\n$$\\det(F) = \\left(\\frac{1}{\\sigma^2}\\right)^2 \\det \\begin{pmatrix} \\sum_{i=1}^{n} \\exp(-2t_i)  \\sum_{i=1}^{n} \\exp(-3t_i) \\\\ \\sum_{i=1}^{n} \\exp(-3t_i)  \\sum_{i=1}^{n} \\exp(-4t_i) \\end{pmatrix}$$\n$$\\det(F) = \\frac{1}{\\sigma^4} \\left[ \\left(\\sum_{i=1}^{n} \\exp(-2t_i)\\right) \\left(\\sum_{i=1}^{n} \\exp(-4t_i)\\right) - \\left(\\sum_{i=1}^{n} \\exp(-3t_i)\\right)^2 \\right]$$\nThis is the required closed-form analytic expression for the determinant of the Fisher Information Matrix.",
            "answer": "$$\\boxed{\\frac{1}{\\sigma^4} \\left[ \\left(\\sum_{i=1}^{n} \\exp(-2t_i)\\right) \\left(\\sum_{i=1}^{n} \\exp(-4t_i)\\right) - \\left(\\sum_{i=1}^{n} \\exp(-3t_i)\\right)^2 \\right]}$$"
        }
    ]
}