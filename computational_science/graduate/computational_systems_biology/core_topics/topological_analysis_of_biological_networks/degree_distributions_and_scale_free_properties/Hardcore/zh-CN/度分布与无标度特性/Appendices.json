{
    "hands_on_practices": [
        {
            "introduction": "对任何网络的分析都始于描述其基本结构。本练习旨在提供关键的第一步的实践：处理原始且可能混杂的蛋白质相互作用数据，并计算其度分布。通过亲手实现从数据清洗到计算直方图、归一化分布（$P(k)$）以及互补累积分布（$\\bar{F}(k)$）的完整流程，你将构建一个用于定量网络生物学分析的基础工具。",
            "id": "3299639",
            "problem": "给定代表无向蛋白质-蛋白质相互作用网络的邻接表，其中每个键是一个蛋白质标识符，其关联的列表包含邻居标识符。网络可能包含重复的邻居条目、缺失的邻居条目以及自环。请将每个网络视为一个无向简单图：节点集是所有键和邻居标识符的并集；必须移除自环；必须合并平行边；邻接关系必须对称化。根据离散随机变量经验分布的基本原理，构建度直方图、归一化度分布和互补累积分布函数，并提供具体的计算输出。\n\n需使用的定义：\n- 节点的度是在形成无向简单图后，其连接的唯一邻居的数量。\n- 经验度直方图是每个度的节点计数。\n- 归一化度分布 $P(k)$ 是度为 $k$ 的节点的经验比例。\n- 互补累积分布函数 $\\bar{F}(k)$ 是概率 $\\Pr(K \\ge k)$。\n\n您的程序必须：\n- 实现基于核心定义的精确步骤，将每个邻接表转换为无向简单图，计算节点度，构建度直方图（从 $0$ 到 $k_{\\max}$ 的每个度 $k$ 的计数），计算归一化分布 $P(k)$，并计算互补累积分布函数 $\\bar{F}(k)$。\n- 通过遵循以下解释来证明归一化选择的合理性：均匀随机地选择一个节点定义了度随机变量 $K$；因此，$P(k)$ 在所有观察到的度上的总和必须为 $1$，并且 $\\bar{F}(k)$ 必须是关于 $k$ 的非增函数，且 $\\bar{F}(0) = 1$。\n\n测试套件：\n- 案例 A (混合连通性): P1: [P2,P3], P2: [P1,P3,P4], P3: [P1,P2,P5], P4: [P2], P5: [P3,P6], P6: [P5]。\n- 案例 B (中心辐射型): P0: [P1,P2,P3,P4,P5], P1: [P0], P2: [P0], P3: [P0], P4: [P0], P5: [P0]。\n- 案例 C (孤立节点): P1: [], P2: [], P3: []。\n- 案例 D (包含重复、自环和缺失对称性的噪声列表): P1: [P2,P2,P1], P2: [P1], P3: [P4], P4: []。\n\n对于每个测试案例，您的程序必须生成一个包含三个列表的三元组：\n- 度直方图，作为一个整数列表，对应 $k = 0, 1, \\dots, k_{\\max}$。\n- 归一化分布 $P(k)$，作为一个实数列表，对应相同的范围。\n- 互补累积分布 $\\bar{F}(k)$，作为一个实数列表，对应相同的范围。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[result\\_A,result\\_B,result\\_C,result\\_D]$），其中每个 $result\\_*$ 是上述的三个列表组成的三元组。最终输出必须是单行，并且只包含此括号表达式。此问题不涉及任何物理单位或角度。所有结果必须是根据上述定义派生出的纯数值列表。",
            "solution": "任务是为几个原始的蛋白质-蛋白质相互作用网络计算度直方图、归一化度分布和互补累积分布函数（CCDF）。该过程基于图论和经验概率分布的基本原理。它涉及两个主要阶段：首先，从提供的邻接表构建一个规范的无向简单图；其次，对构建的图中节点的度进行统计分析。\n\n### 阶段 1：构建无向简单图\n\n原始输入是一组邻接表，其中可能包含不一致之处，例如自环、重复边和不对称连接。为了进行有效的网络分析，必须将这些原始数据标准化为无向简单图。\n\n1.  **节点集识别**：完整的节点集（表示为 $V$）是所有作为邻接表中的键出现的蛋白质标识符与所有作为值列表中的邻居出现的标识符的并集。设输入邻接表为 $A_{raw}$。节点集为 $V = \\{u \\mid u \\in \\text{keys}(A_{raw})\\} \\cup \\{v \\mid v \\in \\text{values}(A_{raw}[u]) \\text{ for some } u\\}$。节点总数为 $N = |V|$。\n\n2.  **规范邻接表示**：我们构建一个最终的、对称的邻接表 $A_{final}$，它代表无向简单图。一个以节点为键、以邻居集合为值的字典是一种理想的数据结构，因为集合能自动处理重复的条目。\n    - 对于每个节点 $u \\in V$，我们初始化一个空的邻居集合。\n    - 我们遍历原始邻接表 $A_{raw}$ 中的每个条目 $(u, \\text{neighbors}_u)$。\n    - 对于每个邻居 $v \\in \\text{neighbors}_u$，我们处理潜在的边 $(u, v)$：\n        - **移除自环**：如果 $u = v$，该边是自环，应被丢弃。\n        - **对称化并合并平行边**：如果 $u \\ne v$，该边是有效的。我们将 $v$ 添加到 $u$ 的邻居集合中，并且为了确保对称性，我们将 $u$ 添加到 $v$ 的邻居集合中。使用集合可以确保如果边 $(u, v)$ 或 $(v, u)$ 多次出现，它只会被存储一次，从而有效地合并了平行边。\n\n此过程之后，$A_{final}$ 包含了一个清晰、对称的网络连接表示。任何节点 $u$ 的度，记为 $k_u$，就是其集合中唯一邻居的数量，即 $k_u = |A_{final}[u]|$。\n\n### 阶段 2：计算度分布\n\n在正确计算出节点度之后，我们可以定义一个离散随机变量 $K$，表示从集合 $V$ 中均匀随机选择的一个节点的度。统计分布是从所有 $N$ 个节点的观察度中得出的经验估计。\n\n1.  **度直方图**：度直方图 $H(k)$ 是一个函数，它将度 $k$ 映射到网络中具有该度的节点数量。\n    $$H(k) = |\\{u \\in V \\mid k_u = k\\}|$$\n    该直方图是为从 $k=0$ 到最大观察度 $k_{\\max} = \\max_{u \\in V}(k_u)$ 的所有整数度计算的。所有直方图计数的总和等于节点总数：$\\sum_{k=0}^{k_{\\max}} H(k) = N$。\n\n2.  **归一化度分布 $P(k)$**：归一化度分布是随机变量 $K$ 的经验概率质量函数（PMF）。它给出了具有度 $k$ 的节点的比例。\n    $$P(k) = \\Pr(K=k) = \\frac{H(k)}{N}$$\n    作为一个 PMF，它必须满足归一化条件。其合理性直接源于定义：\n    $$\\sum_{k=0}^{k_{\\max}} P(k) = \\sum_{k=0}^{k_{\\max}} \\frac{H(k)}{N} = \\frac{1}{N} \\sum_{k=0}^{k_{\\max}} H(k) = \\frac{N}{N} = 1$$\n\n3.  **互补累积分布函数 (CCDF) $\\bar{F}(k)$**：CCDF，也称为生存函数，给出了一个随机选择的节点度至少为 $k$ 的概率。\n    $$\\bar{F}(k) = \\Pr(K \\ge k) = \\sum_{j=k}^{k_{\\max}} P(j)$$\n    该函数有两个关键属性：\n    - 它必须随 $k$ 的增加而非增，因为度 $\\ge k+1$ 的节点集是度 $\\ge k$ 的节点集的子集。\n    - 它必须满足 $\\bar{F}(0) = 1$，因为每个节点的度都为 $0$ 或更大。这可以通过 $\\bar{F}(0) = \\sum_{j=0}^{k_{\\max}} P(j) = 1$ 来证明。\n    一种计算上高效的计算 CCDF 的方法是使用反向累积和。我们从分布的尾部开始：\n    - $\\bar{F}(k_{\\max}) = P(k_{\\max})$\n    - 对于 $k = k_{\\max}-1, \\dots, 0$，我们有 $\\bar{F}(k) = \\bar{F}(k+1) + P(k)$。\n\n### 算法综合\n\n需要实现的完整算法如下：\n1.  **输入解析**：对于给定的原始邻接表，确定唯一的节点标识符的完整集合 $V$。\n2.  **图构建**：为 $V$ 中的每个节点初始化一个集合字典。遍历原始邻接表，向字典中添加对称边，同时忽略自环。\n3.  **度计算**：计算一个度列表，其中每个元素是一个节点的邻居集合的大小。\n4.  **直方图生成**：确定最大度 $k_{\\max}$。创建一个大小为 $k_{\\max}+1$ 的整数数组，并初始化为零。通过计算每个度值的出现次数来填充此数组。\n5.  **分布计算**：\n    - 将直方图数组除以节点总数 $N$，以获得归一化分布 $P(k)$。\n    - 通过对 $P(k)$ 数组执行反向累积和来计算 CCDF $\\bar{F}(k)$。\n6.  **输出格式化**：将整数直方图、浮点数 $P(k)$ 列表和浮点数 $\\bar{F}(k)$ 列表打包成给定网络的单个结果元组。对所有测试案例重复此过程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by processing a series of network adjacency lists\n    to compute their degree distributions.\n    \"\"\"\n\n    def process_network(adj_raw: dict) - list:\n        \"\"\"\n        Transforms a raw adjacency list into an undirected simple graph and\n        computes its degree histogram, normalized distribution, and CCDF.\n\n        Args:\n            adj_raw: A dictionary representing a raw adjacency list.\n\n        Returns:\n            A list containing three lists: the degree histogram, the\n            normalized degree distribution P(k), and the CCDF F_bar(k).\n        \"\"\"\n        # Phase 1: Construct the Undirected Simple Graph\n        \n        # 1. Node Set Identification\n        nodes = set(adj_raw.keys())\n        for u in adj_raw:\n            nodes.update(adj_raw[u])\n        \n        num_nodes = len(nodes)\n        \n        if num_nodes == 0:\n            # Handle empty graph case: k_max=0, one node of degree 0 (convention)\n            # This case is not in test suite, but is good practice.\n            # Here we assume at least one node exists as per test cases.\n            # For a graph with no nodes, num_nodes = 0, division by zero occurs.\n            # But test cases imply nodes always exist.\n            # Case C has 3 nodes.\n            return [[0], [0.0], [0.0]] # Or handle as error/special value\n\n        # 2. Canonical Adjacency Representation\n        adj_final = {node: set() for node in nodes}\n        for u, neighbors in adj_raw.items():\n            for v in neighbors:\n                if u != v:  # Ignore self-loops\n                    adj_final[u].add(v)\n                    adj_final[v].add(u) # Symmetrize\n\n        # Phase 2: Compute Degree Distributions\n\n        # 3. Degree Calculation\n        degrees = [len(neighbors) for neighbors in adj_final.values()]\n\n        # 4. Degree Histogram\n        k_max = max(degrees) if degrees else 0\n        histogram = np.zeros(k_max + 1, dtype=int)\n        \n        unique_degrees, counts = np.unique(degrees, return_counts=True)\n        histogram[unique_degrees] = counts\n        \n        # 5. Normalized Distribution P(k) and CCDF F_bar(k)\n        p_k = histogram / num_nodes\n        f_bar_k = np.cumsum(p_k[::-1])[::-1]\n\n        # 6. Ouput Formatting\n        return [\n            histogram.tolist(),\n            p_k.tolist(),\n            f_bar_k.tolist()\n        ]\n\n    test_cases = [\n        # Case A: Mixed connectivity\n        {'P1': ['P2','P3'], 'P2': ['P1','P3','P4'], 'P3': ['P1','P2','P5'], 'P4': ['P2'], 'P5': ['P3','P6'], 'P6': ['P5']},\n        # Case B: Hub-and-spoke\n        {'P0': ['P1','P2','P3','P4','P5'], 'P1': ['P0'], 'P2': ['P0'], 'P3': ['P0'], 'P4': ['P0'], 'P5': ['P0']},\n        # Case C: Isolated nodes\n        {'P1': [], 'P2': [], 'P3': []},\n        # Case D: Noisy list with duplicates, self-loop, and missing symmetry\n        {'P1': ['P2','P2','P1'], 'P2': ['P1'], 'P3': ['P4'], 'P4': []},\n    ]\n\n    results = [process_network(case) for case in test_cases]\n\n    # Manual string formatting to avoid spaces and match problem spec precisely.\n    result_strings = []\n    for res in results:\n        hist_str = f\"[{','.join(map(str, res[0]))}]\"\n        pk_str = f\"[{','.join(map(str, res[1]))}]\"\n        fbar_str = f\"[{','.join(map(str, res[2]))}]\"\n        result_strings.append(f\"[{hist_str},{pk_str},{fbar_str}]\")\n\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "获得度分布后，关键的一步是检验其是否遵循幂律，这是无标度网络的一个标志。本练习深入探讨了该问题的统计核心，要求你推导幂律指数 $\\gamma$ 的最大似然估计（MLE）。这一基本推导不仅为你提供了参数估计的工具，还强调了连续近似与网络数据离散性这一现实之间的重要理论差异。",
            "id": "3299654",
            "problem": "在对一个大型蛋白质-蛋白质相互作用网络的计算分析中，假设观测到的高于一个下限截止值的度值被建模为来自一个重尾定律，这与无标度假设一致。具体来说，设 $k_{1},\\dots,k_{n}$ 是独立的度样本，满足 $k_{i}\\geq k_{\\min}$，它们从一个连续的帕累托型尾部分布中抽取，其概率密度函数为 $p(k\\mid \\gamma)$，支撑集为 $[k_{\\min},\\infty)$，尾部指数 $\\gamma1$。仅使用以下基本原理：(i) 概率密度函数在其支撑集上的积分必须归一化为1，以及 (ii) 最大似然估计 (MLE) 选择在模型下使观测数据似然性最大化的参数值，从第一性原理出发，推导连续模型下尾部指数的MLE $\\hat{\\gamma}$ 的闭式表达式。\n\n然后，在不引用任何预先给定的估计量的情况下，解释为什么真实网络的度是整数这一事实需要离散处理，并说明两种基于原理的修正方法是如何产生的：一种是连续性校正，它在连续推导中用 $k_{\\min}-\\tfrac{1}{2}$ 替换 $k_{\\min}$；另一种是针对 $k\\in\\{k_{\\min},k_{\\min}+1,\\dots\\}$ 的精确离散似然，其归一化涉及一个级数。描述定义精确离散MLE的得分方程的根，并解释为什么它通常需要数值解。\n\n你最终报告的答案必须是推导出的连续模型 $\\hat{\\gamma}$ 的闭式表达式。无需四舍五入，最终答案不带物理单位。",
            "solution": "首先将对问题陈述进行严格的验证过程。\n\n### 第1步：提取已知条件\n- **数据**：一组 $n$ 个独立的度样本，$k_{1}, \\dots, k_{n}$。\n- **约束**：每个样本满足 $k_{i} \\geq k_{\\min}$。\n- **模型**：数据从一个连续的帕累托型尾部分布中抽取。\n- **概率密度函数 (PDF)**：PDF表示为 $p(k\\mid \\gamma)$，其支撑集为区间 $[k_{\\min},\\infty)$。\n- **参数**：模型有一个单一参数，即尾部指数 $\\gamma$，约束条件为 $\\gamma1$。\n- **原理 (i)**：PDF必须归一化为1：$\\int_{k_{\\min}}^{\\infty} p(k\\mid \\gamma) \\, dk = 1$。\n- **原理 (ii)**：将使用最大似然估计 (MLE) 方法。\n- **任务1**：从第一性原理出发，为连续模型推导尾部指数的MLE $\\hat{\\gamma}$ 的闭式表达式。\n- **任务2**：解释为什么网络度的离散性质需要一个离散的统计模型。\n- **任务3**：描述处理离散数据的两种基于原理的修正方法：连续性校正和精确离散似然函数。\n- **任务4**：描述精确离散MLE的得分方程，并解释为什么它通常需要数值解。\n\n### 第2步：使用提取的已知条件进行验证\n根据既定标准评估问题的有效性。\n\n- **科学基础**：该问题在统计物理学、网络科学和计算生物学领域有坚实的理论基础。研究网络度的幂律或帕累托分布（“无标度”假说）是一个核心课题。最大似然估计的使用是一种标准且严谨的统计方法。连续近似与精确离散处理之间的区别是该领域一个关键且被充分理解的问题。该问题在科学上和数学上都是合理的。\n- **适定性**：该问题是适定的。它提供了所有必要的信息和第一性原理——PDF的归一化和MLE的定义——以推导所需的估计量。问题的结构导致了连续估计量的一个唯一且有意义的解。\n- **客观性**：问题以精确、客观、正式的语言陈述，没有歧义或主观内容。\n\n### 第3步：结论与行动\n问题被判定为**有效**，因为它具有科学基础、适定性和客观性。没有可识别的缺陷。我现在将进行一个完整的、有理有据的解答。\n\n推导和解释将按照问题陈述中概述的分段进行。\n\n首先，我们推导连续帕累托型模型的MLE。帕累托型尾部的函数形式为 $p(k) \\propto k^{-\\gamma}$。我们为 $k \\in [k_{\\min}, \\infty)$ 定义概率密度函数 (PDF) 如下：\n$$p(k \\mid \\gamma) = C k^{-\\gamma}$$\n其中 $C$ 是一个归一化常数。根据原理 (i)，PDF在其支撑集上的积分必须为 $1$。\n$$ \\int_{k_{\\min}}^{\\infty} C k^{-\\gamma} \\, dk = 1 $$\n我们计算该积分：\n$$ C \\left[ \\frac{k^{-\\gamma+1}}{-\\gamma+1} \\right]_{k_{\\min}}^{\\infty} = C \\left( \\lim_{k \\to \\infty} \\frac{k^{1-\\gamma}}{1-\\gamma} - \\frac{k_{\\min}^{1-\\gamma}}{1-\\gamma} \\right) = 1 $$\n给定约束条件 $\\gamma  1$，指数 $1-\\gamma$ 为负。因此，$\\lim_{k \\to \\infty} k^{1-\\gamma} = 0$。方程变为：\n$$ C \\left( 0 - \\frac{k_{\\min}^{1-\\gamma}}{1-\\gamma} \\right) = C \\frac{k_{\\min}^{1-\\gamma}}{\\gamma-1} = 1 $$\n解出归一化常数 $C$，我们得到：\n$$ C = (\\gamma-1) k_{\\min}^{\\gamma-1} $$\n因此，连续帕累托分布的归一化PDF为：\n$$ p(k \\mid \\gamma) = (\\gamma-1) k_{\\min}^{\\gamma-1} k^{-\\gamma} $$\n接下来，我们应用原理 (ii)，即最大似然估计。对于一组 $n$ 个独立同分布的观测值 $k_1, k_2, \\dots, k_n$，似然函数 $L(\\gamma)$ 是各个概率的乘积：\n$$ L(\\gamma \\mid \\{k_i\\}) = \\prod_{i=1}^{n} p(k_i \\mid \\gamma) = \\prod_{i=1}^{n} \\left( (\\gamma-1) k_{\\min}^{\\gamma-1} k_i^{-\\gamma} \\right) $$\n$$ L(\\gamma) = (\\gamma-1)^n (k_{\\min}^{\\gamma-1})^n \\left( \\prod_{i=1}^{n} k_i \\right)^{-\\gamma} = (\\gamma-1)^n k_{\\min}^{n(\\gamma-1)} \\exp\\left(-\\gamma \\sum_{i=1}^{n} \\ln(k_i)\\right) $$\n为了找到使 $L(\\gamma)$ 最大化的 $\\gamma$ 值，从数学上讲，最大化对数似然函数 $\\ell(\\gamma) = \\ln L(\\gamma)$ 更为方便，因为对数是一个严格单调函数。\n$$ \\ell(\\gamma) = \\ln\\left( (\\gamma-1)^n k_{\\min}^{n(\\gamma-1)} \\left( \\prod_{i=1}^{n} k_i \\right)^{-\\gamma} \\right) $$\n$$ \\ell(\\gamma) = n \\ln(\\gamma-1) + n(\\gamma-1)\\ln(k_{\\min}) - \\gamma \\sum_{i=1}^{n} \\ln(k_i) $$\n为了找到最大值，我们计算 $\\ell(\\gamma)$ 关于 $\\gamma$ 的导数并将其设为零。这就定义了得分方程。\n$$ \\frac{d\\ell}{d\\gamma} = \\frac{n}{\\gamma-1} + n\\ln(k_{\\min}) - \\sum_{i=1}^{n} \\ln(k_i) = 0 $$\n设 $\\hat{\\gamma}$ 是解此方程的 $\\gamma$ 值。\n$$ \\frac{n}{\\hat{\\gamma}-1} = \\sum_{i=1}^{n} \\ln(k_i) - n\\ln(k_{\\min}) $$\n$$ \\frac{n}{\\hat{\\gamma}-1} = \\sum_{i=1}^{n} \\left( \\ln(k_i) - \\ln(k_{\\min}) \\right) $$\n$$ \\frac{n}{\\hat{\\gamma}-1} = \\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right) $$\n解出 $\\hat{\\gamma}-1$：\n$$ \\hat{\\gamma}-1 = \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right)} $$\n最后，我们得到尾部指数MLE的闭式表达式：\n$$ \\hat{\\gamma} = 1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right)} $$\n为了确认这对应于一个最大值，我们检查二阶导数：\n$$ \\frac{d^2\\ell}{d\\gamma^2} = -\\frac{n}{(\\gamma-1)^2} $$\n由于 $n  0$，对于 $\\gamma1$，二阶导数恒为负，这证实了 $\\hat{\\gamma}$ 确实是一个最大似然估计量。\n\n第二，我们讨论为什么网络度的离散性质需要离散的统计处理。根据定义，网络度是表示连接计数的整数。像上面推导的帕累托PDF这样的连续概率密度函数，是定义在连续的实数范围上的。任何连续PDF $p(x)$ 的一个基本性质是，观测到任何单个特定值 $x_0$ 的概率为零，即 $P(X=x_0) = \\int_{x_0}^{x_0} p(x)dx = 0$。这与数据的经验现实产生了直接矛盾，因为数据由特定的整数计数组成。虽然对于较大的度值 $k$，连续近似 $P(k-0.5 \\leq X  k+0.5) \\approx p(k)$ 可能足够好，但它是一种近似，可能会引入系统性偏差，特别是对于接近 $k_{\\min}$ 的较小 $k$ 值。一个有原则的统计模型必须尊重数据的样本空间，在这种情况下，即整数集合 $\\{k_{\\min}, k_{\\min}+1, \\dots\\}$。处理这个问题的恰当工具是概率质量函数 (PMF)，而不是PDF。\n\n第三，我们描述两种基于原理的修正方法。\n1.  **连续性校正**：这是一种标准的统计启发式方法，用于用连续分布来近似离散分布。其核心思想是将整数值 $k$ 视为代表连续区间 $[k - \\frac{1}{2}, k + \\frac{1}{2})$。因此，一个每个观测值 $k_i \\geq k_{\\min}$ 的离散数据集在概念上被映射到一个连续问题，其中样本从范围 $[k_{\\min} - \\frac{1}{2}, \\infty)$ 中抽取。这种修正方法的一个直接应用是在连续推导中用 $k_{\\min} - \\frac{1}{2}$ 替换 $k_{\\min}$。这会修改用于归一化的积分下限，并因此调整对数似然函数。最终得到的 $\\hat{\\gamma}$ 估计量将与连续模型的形式相同，但在整个表达式中用 $k_{\\min}-\\frac{1}{2}$ 替换 $k_{\\min}$：$\\hat{\\gamma} = 1 + n \\left( \\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}-1/2}\\right) \\right)^{-1}$。这种方法比朴素的连续模型有所改进，但仍然是一种近似。\n\n2.  **精确离散似然**：最严谨的方法是直接在整数上定义一个离散幂律分布。概率质量函数 (PMF) 定义为 $P(k \\mid \\gamma) = C' k^{-\\gamma}$，对于整数 $k \\in \\{k_{\\min}, k_{\\min}+1, \\dots\\}$。归一化常数 $C'$ 由支撑集上概率总和必须为 $1$ 的条件确定：\n    $$ \\sum_{k=k_{\\min}}^{\\infty} P(k \\mid \\gamma) = C' \\sum_{k=k_{\\min}}^{\\infty} k^{-\\gamma} = 1 $$\n    该无穷级数是一个称为赫尔维茨zeta函数 (Hurwitz zeta function) 的特殊函数，$\\zeta(\\gamma, q) = \\sum_{j=0}^{\\infty} (j+q)^{-\\gamma} = \\sum_{k=q}^{\\infty} k^{-\\gamma}$。因此，$C' = 1 / \\zeta(\\gamma, k_{\\min})$，精确的PMF为：\n    $$ P(k \\mid \\gamma) = \\frac{k^{-\\gamma}}{\\zeta(\\gamma, k_{\\min})} $$\n    然后使用此PMF构建似然函数。\n\n第四，我们描述精确离散MLE的得分方程。离散模型的对数似然函数为：\n$$ \\ell_{\\text{discrete}}(\\gamma) = \\ln \\left( \\prod_{i=1}^n \\frac{k_i^{-\\gamma}}{\\zeta(\\gamma, k_{\\min})} \\right) = \\sum_{i=1}^n \\left( -\\gamma \\ln(k_i) - \\ln(\\zeta(\\gamma, k_{\\min})) \\right) $$\n$$ \\ell_{\\text{discrete}}(\\gamma) = -\\gamma \\sum_{i=1}^n \\ln(k_i) - n \\ln(\\zeta(\\gamma, k_{\\min})) $$\n得分方程通过将关于 $\\gamma$ 的导数设为零来找到：\n$$ \\frac{d\\ell_{\\text{discrete}}}{d\\gamma} = - \\sum_{i=1}^n \\ln(k_i) - n \\frac{1}{\\zeta(\\gamma, k_{\\min})} \\frac{d\\zeta(\\gamma, k_{\\min})}{d\\gamma} = 0 $$\n赫尔维茨zeta函数的导数为 $\\frac{d}{d\\gamma} \\zeta(\\gamma, k_{\\min}) = -\\sum_{k=k_{\\min}}^{\\infty} k^{-\\gamma} \\ln(k)$。代入此式可得离散MLE $\\hat{\\gamma}_{\\text{discrete}}$ 的得分方程：\n$$ \\sum_{i=1}^n \\ln(k_i) - n \\frac{\\sum_{k=k_{\\min}}^{\\infty} k^{-\\hat{\\gamma}_{\\text{discrete}}} \\ln(k)}{\\sum_{k=k_{\\min}}^{\\infty} k^{-\\hat{\\gamma}_{\\text{discrete}}}} = 0 $$\n这个方程是超越方程，因为它涉及到由无穷级数定义的赫尔维茨zeta函数及其导数。对于 $\\hat{\\gamma}_{\\text{discrete}}$，没有通用的闭式代数解。因此，必须使用数值求根算法（如牛顿法或二分法）来求解特定数据集 $\\{k_i\\}$ 和参数 $k_{\\min}$ 的得分方程，从而找到其值。",
            "answer": "$$\\boxed{1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right)}}$$"
        },
        {
            "introduction": "为什么如此多的生物网络表现出无标度特性？这项高级练习将挑战你从描述转向解释，探索一种潜在的生成机制。你将实现一个超图模型，其中蛋白质复合物是基本单元，以研究将这种高阶结构投影到简单的成对图上是否能自然产生重尾度分布。这个练习体现了以假设驱动、基于模拟的研究方法，这正是现代计算系统生物学的核心。",
            "id": "3299720",
            "problem": "考虑一个将蛋白质复合物建模为超图的模型，其中每个复合物是一个超边，连接所有参与该复合物的蛋白质。设该超图表示为 $H = (V, E)$，其中 $V$ 是 $n$ 个蛋白质（节点）的集合，而 $E = \\{e_1, e_2, \\ldots, e_m\\}$ 是超边（复合物）的多重集。每个超边 $e_j \\subseteq V$ 的基数为 $s_j = |e_j|$，解释为复合物的大小。对于每个节点 $i \\in V$，定义其超度 $d_i^{H}$ 为包含节点 $i$ 的超边数量，即 $d_i^{H} = |\\{e_j \\in E : i \\in e_j\\}|$。该超图到成对网络 $G = (V, A)$ 的投影是通过以下方式构建的：只要存在至少一个超边 $e_j$ 使得 $\\{u, v\\} \\subseteq e_j$，就在 $A$ 中添加一条无向边 $(u, v)$。将节点 $i$ 的投影度定义为 $d_i^{G}$，即 $i$ 在 $G$ 中的不同邻居的数量，每个邻居无论在多少个超边中共同出现都只计数一次。\n\n在计算系统生物学中，重尾度分布和无标度特性通常在相互作用网络的背景下进行讨论。本问题的目标是，从第一性原理出发，研究代表蛋白质复合物的超边是否能在投影的成对网络度分布中诱导出明显的重尾现象，即使超度分布本身不是重尾的。这项研究必须通过使用明确、可复现的规则和稳健的统计检验来生成合成数据并进行。\n\n使用的基本概念和定义：\n- 超图 $H = (V, E)$ 是图的一种推广，其中一条超边可以连接任意数量的节点。不允许自环或多个相同的超边；只有当两个超边被视为一个时，它们才可以具有相同的节点集。\n- 度分布定义为所有节点 $i \\in V$ 的 $d_i^{H}$ 和 $d_i^{G}$ 的经验频率分布。\n- 度分布尾部的重尾行为可以通过在连续幂律模型下进行最大似然估计，并使用似然比将拟合优度与指数模型进行比较来评估。设候选尾部区域为 $X = \\{x : x \\ge x_{\\min}\\}$，并假设尾部的数据点为 $\\{x_\\ell\\}_{\\ell=1}^n$，其中 $x_\\ell \\in \\mathbb{R}$。在我们的案例中，$x_\\ell$ 是整数度，但为了估计而用连续分布建模。对于 $x \\ge x_{\\min}$，连续幂律密度为 $p(x \\mid \\alpha, x_{\\min}) = (\\alpha - 1) x_{\\min}^{\\alpha-1} x^{-\\alpha}$，其中 $\\alpha  1$。对于 $x \\ge x_{\\min}$，相应的指数密度为 $q(x \\mid \\lambda, x_{\\min}) = \\lambda e^{-\\lambda (x - x_{\\min})}$，其中 $\\lambda  0$。\n- 对于一个候选的 $x_{\\min}$，Kolmogorov–Smirnov (KS) 统计量是经验累积分布函数与模型累积分布函数在尾部区域上绝对差的上确界。\n\n需要实现的任务：\n1. 超图生成。对于每个测试用例，使用以下步骤生成 $H = (V, E)$：\n   - 固定节点数 $n$ 和超边数 $m$。\n   - 从指定的分布中抽样每个超边的大小 $s_j$，该分布限制为整数，并裁剪以确保 $2 \\le s_j \\le n$。\n   - 从指定的分布中抽取节点倾向性 $w_i  0$，并将其转换为选择概率 $p_i = w_i / \\sum_{u \\in V} w_u$。\n   - 对于每个超边 $e_j$，根据概率 $\\{p_i\\}_{i=1}^n$ 从 $V$ 中无放回地抽样 $s_j$ 个不同的节点以形成 $e_j$。\n   - 如果生成了任何重复的超边，则将其移除。\n2. 投影到成对网络。通过在至少一个超边中共同出现的所有无序节点对之间添加一条无向边来构建 $G = (V, A)$。计算所有 $i \\in V$ 的 $d_i^{G}$。\n3. 超度分布。计算所有 $i \\in V$ 的 $d_i^{H}$，并获得超度的经验分布（每个唯一值的计数）。\n4. 对投影度分布进行重尾检验。使用经验投影度 $\\{d_i^{G}\\}_{i=1}^n$，执行以下操作：\n   - 通过最小化经验尾部累积分布与连续幂律模型累积分布之间的 Kolmogorov–Smirnov 距离来选择一个候选尾部阈值 $x_{\\min}$。将候选 $x_{\\min}$ 限制为具有至少最小尾部观测数量的观测度值。\n   - 通过最大似然估计幂律尾部指数 $\\hat{\\alpha}$：$$\\hat{\\alpha} = 1 + \\frac{n}{\\sum_{\\ell=1}^n \\ln(x_\\ell/x_{\\min})}.$$ \n   - 通过最大似然估计尾部的指数率参数 $\\hat{\\lambda}$：$$\\hat{\\lambda} = \\frac{1}{\\frac{1}{n} \\sum_{\\ell=1}^n (x_\\ell - x_{\\min})}.$$\n   - 计算尾部数据在两种模型下的对数似然以及对数似然比 $R = \\log L_{\\text{PL}} - \\log L_{\\text{EXP}}$，其中 $\\log L_{\\text{PL}}$ 是幂律模型下的对数似然，$\\log L_{\\text{EXP}}$ 是指数模型下的对数似然。\n   - 如果满足以下所有条件，则声明投影度分布表现出明显的重尾：尾部观测数 $n$ 至少达到指定的最小值，幂律拟合的 Kolmogorov–Smirnov 统计量低于指定的阈值，估计的指数 $\\hat{\\alpha}$ 位于生物学上合理的范围内，且对数似然比 $R$ 为正。\n5. 最终输出。对于每个测试用例，输出一个布尔值，如果投影网络根据任务4中的标准表现出明显的重尾，则为 true，否则为 false。\n\n测试套件：\n- 测试用例 1（一般情况，异构复合物大小和节点倾向性）：$n = 1000$， $m = 800$，超边大小 $s_j$ 从离散 Zipf 分布中抽样，指数参数 $\\tau = 2.2$，限制为 $2 \\le s_j \\le 30$，节点倾向性 $w_i$ 从对数正态分布中抽样，尺度参数 $\\sigma = 1.0$，随机种子 $42$。\n- 测试用例 2（边界情况，近乎同构的复合物）：$n = 1000$， $m = 800$，所有 $j$ 的超边大小固定为 $s_j = 3$，节点倾向性均匀（所有 $w_i$ 相等），随机种子 $43$。\n- 测试用例 3（边缘情况，大量具有中等指数大小和高度异构倾向性的超边）：$n = 300$， $m = 2000$，超边大小从均值为 $6$ 的泊松分布中抽样，然后平移和裁剪以确保 $s_j \\ge 2$，节点倾向性 $w_i$ 从对数正态分布中抽样，尺度参数 $\\sigma = 2.0$，随机种子 $44$。\n- 测试用例 4（小系统规模，均匀超边大小）：$n = 80$， $m = 60$，超边大小从 $2$ 到 $5$（含）的整数中均匀抽样，节点倾向性均匀，随机种子 $45$。\n\n所有用例统一使用的重尾检测参数：\n- 最小尾部样本量 $n_{\\text{min}} = 50$。\n- Kolmogorov–Smirnov 阈值 $D_{\\text{max}} = 0.1$。\n- 合理指数范围 $[\\alpha_{\\text{min}}, \\alpha_{\\text{max}}] = [1.5, 5.0]$。\n\n您的程序必须按程序实现所有任务，并生成单行输出，其中包含四个测试用例的检测结果，格式为方括号括起来的逗号分隔列表（例如，“[true,false,true,false]”）。布尔值必须为小写，并且必须反映每个测试用例中投影度分布的重尾指示符，顺序与上面列出的测试用例相同。不涉及物理单位；角度和百分比不适用于此问题。程序必须是完整的，无需外部输入或文件即可运行，并且必须严格遵守所述的输出格式。",
            "solution": "问题陈述已经过严格验证，被认为是有效的。它提出了一个定义明确、有科学依据且客观的计算任务。该问题要求研究超图成对投影中重尾度分布的出现，即使在底层超图的属性不一定是重尾的情况下。它为合成数据生成提供了一个明确、可复现的流程，并为检验重尾假设提供了一个稳健的统计框架。问题文本中存在一个微小的歧义，即超边集 $E$ 是一个“多重集”，而程序性指令是“移除重复的超边”。程序性指令是一个明确的指令，解决了这个歧义；最终的超边集将是唯一的。所有测试用例的参数和分布都已明确指定。\n\n解决方案通过遵循问题中概述的任务顺序来实现。\n\n步骤 1：超图生成\n对于每个测试用例，根据其特定参数生成一个超图 $H = (V, E)$。节点集 $V$ 的大小为 $n$。生成过程包括创建一个包含 $m$ 个超边的集合，之后移除任何重复项以形成最终的集合 $E$。这意味着最终唯一超边的数量可能小于或等于 $m$。每个超边 $e_j$ 的生成过程如下：\n1.  超边的大小 $s_j = |e_j|$ 从指定的概率分布中抽样，并裁剪到范围 $[2, n]$ 内。\n2.  节点倾向性 $\\{w_i\\}_{i=1}^n$ 从指定的分布中抽取。这些倾向性被归一化以形成选择概率 $p_i = w_i / \\sum_{k=1}^n w_k$。\n3.  使用概率 $\\{p_i\\}$ 从 $V$ 中无放回地抽样 $s_j$ 个不同的节点，以构成超边 $e_j$。\n为了可复现性，每个测试用例的生成过程都使用独立的随机数生成器并以特定的随机种子进行播种。每个测试用例的具体分布如下：\n-   **测试用例 1**：节点数 $n=1000$，初始超边数 $m=800$。超边大小 $s_j$ 从整数域 $[2, 30]$ 上的离散 Zipf 分布中抽样，指数为 $\\tau = 2.2$。节点倾向性 $w_i$ 来自尺度为 $\\sigma=1.0$ 的对数正态分布。\n-   **测试用例 2**：$n=1000$， $m=800$。所有超边大小固定为 $s_j=3$。节点倾向性 $w_i$ 均匀，实际上使节点选择成为均匀随机。\n-   **测试用例 3**：$n=300$， $m=2000$。超边大小 $s_j$ 从均值为 $\\lambda=6$ 的泊松分布中抽样，然后裁剪到范围 $[2, n]$ 内。节点倾向性 $w_i$ 来自尺度为 $\\sigma=2.0$ 的对数正态分布。\n-   **测试用例 4**：$n=80$， $m=60$。超边大小 $s_j$ 从整数 $\\{2, 3, 4, 5\\}$ 中均匀抽样。节点倾向性 $w_i$ 均匀。\n\n步骤 2：网络投影和度计算\n从生成的超图 $H=(V, E)$ 构建一个简单（成对）图 $G=(V, A)$。当且仅当存在至少一个超边 $e_j \\in E$ 使得 $u$ 和 $v$ 都是 $e_j$ 的成员时（即 $\\{u, v\\} \\subseteq e_j$），才将边 $(u, v)$ 添加到边集 $A$ 中。构建 $G$ 后，计算每个节点 $i \\in V$ 的投影度 $d_i^G$，作为其在 $G$ 中的不同邻居的数量。同时计算超度 $d_i^H$，定义为包含节点 $i$ 的唯一超边的数量。主要分析集中在 $\\{d_i^G\\}$ 的分布上。\n\n步骤 3：重尾检验\n问题的核心是确定投影度 $\\{d_i^G\\}_{i=1}^n$ 的经验分布是否表现出明显的重尾。这通过指定的严格统计程序进行评估，该程序涉及将幂律模型与指数模型对分布的尾部进行比较。\n\n1.  **寻找最优尾部阈值 $x_{\\min}$**：分析仅限于分布的尾部，定义为所有度 $d \\ge x_{\\min}$。最优 $x_{\\min}$ 的值是通过搜索所有唯一观测到的度值来选择的。对于每个候选 $x_{\\min}$，我们考虑构成尾部的数据子集。只考虑那些导致尾部至少包含 $n_{\\min} = 50$ 个数据点的候选值。对于每个有效的候选值，通过最大似然估计幂律指数 $\\hat{\\alpha}$。计算 Kolmogorov-Smirnov (KS) 统计量 $D$，它衡量尾部数据的经验累积分布函数 (CDF) 与拟合的幂律模型的 CDF 之间的最大距离。选择使该 KS 统计量 $D$ 最小的候选 $x_{\\min}$ 用于后续测试。连续幂律模型的 CDF 为 $P(x) = 1 - (x/x_{\\min})^{1-\\alpha}$。双边 KS 统计量的计算公式为 $D = \\max(\\sup_{x \\ge x_{\\min}} |S(x) - P(x)|)$，其中 $S(x)$ 是经验 CDF。\n\n2.  **应用判定标准**：使用最优 $x_{\\min}$ 及其对应的尾部数据，必须满足一组四个条件才能宣布该分布具有明显的重尾：\n    -   **最小尾部样本量**：尾部中的数据点数 $n_{\\text{tail}}$ 必须至少为 $n_{\\min} = 50$。如果找到了有效的 $x_{\\min}$，则此条件已隐式满足。\n    -   **拟合优度**：在最优 $x_{\\min}$ 处的幂律拟合的 KS 统计量，我们称之为 $D_{\\text{opt}}$，必须低于阈值 $D_{\\text{max}} = 0.1$。\n    -   **合理指数**：幂律指数的最大似然估计值 $\\hat{\\alpha}$ 必须在生物学上合理的范围 $[\\alpha_{\\min}, \\alpha_{\\text{max}}] = [1.5, 5.0]$ 内。公式为 $\\hat{\\alpha} = 1 + n_{\\text{tail}} \\left(\\sum_{i \\in \\text{tail}} \\ln(d_i^G/x_{\\min})\\right)^{-1}$。\n    -   **似然比检验**：幂律模型必须比与之竞争的指数模型更拟合。我们使用其最大似然公式估计拟合相同尾部数据的指数分布的率参数 $\\hat{\\lambda}$：$\\hat{\\lambda} = \\left(\\frac{1}{n_{\\text{tail}}} \\sum_{i \\in \\text{tail}} (d_i^G - x_{\\min})\\right)^{-1}$。然后我们计算尾部数据在幂律模型 ($\\log L_{\\text{PL}}$) 和指数模型 ($\\log L_{\\text{EXP}}$) 下的对数似然。对数似然比 $R = \\log L_{\\text{PL}} - \\log L_{\\text{EXP}}$ 必须为正，表示数据在幂律模型下更可能出现。\n\n如果所有四个条件都满足，则该测试用例的结果为 `true`；否则为 `false`。整个过程被编码到一个程序中，该程序处理四个测试用例中的每一个，并以指定格式输出布尔结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print the final output.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"n\": 1000, \"m\": 800,\n            \"size_dist\": {\"type\": \"zipf\", \"params\": {\"tau\": 2.2, \"min\": 2, \"max\": 30}},\n            \"prop_dist\": {\"type\": \"lognormal\", \"params\": {\"sigma\": 1.0}},\n            \"seed\": 42\n        },\n        {\n            \"n\": 1000, \"m\": 800,\n            \"size_dist\": {\"type\": \"fixed\", \"params\": {\"val\": 3}},\n            \"prop_dist\": {\"type\": \"uniform\", \"params\": {}},\n            \"seed\": 43\n        },\n        {\n            \"n\": 300, \"m\": 2000,\n            \"size_dist\": {\"type\": \"poisson\", \"params\": {\"mean\": 6, \"min\": 2}},\n            \"prop_dist\": {\"type\": \"lognormal\", \"params\": {\"sigma\": 2.0}},\n            \"seed\": 44\n        },\n        {\n            \"n\": 80, \"m\": 60,\n            \"size_dist\": {\"type\": \"uniform_int\", \"params\": {\"min\": 2, \"max\": 5}},\n            \"prop_dist\": {\"type\": \"uniform\", \"params\": {}},\n            \"seed\": 45\n        }\n    ]\n\n    heavy_tail_params = {\n        \"n_min\": 50,\n        \"D_max\": 0.1,\n        \"alpha_range\": [1.5, 5.0]\n    }\n\n    results = []\n    for case in test_cases:\n        result = run_simulation_and_test(case, heavy_tail_params)\n        results.append(str(result).lower())\n\n    print(f\"[{','.join(results)}]\")\n\n\ndef get_hyperedge_sizes(dist_info, m, n, rng):\n    \"\"\"Generates hyperedge sizes based on distribution info.\"\"\"\n    dist_type = dist_info[\"type\"]\n    params = dist_info[\"params\"]\n    if dist_type == \"zipf\":\n        tau, s_min, s_max = params[\"tau\"], params[\"min\"], params[\"max\"]\n        k = np.arange(s_min, s_max + 1)\n        weights = k ** (-tau)\n        p = weights / np.sum(weights)\n        return rng.choice(k, size=m, p=p)\n    elif dist_type == \"fixed\":\n        return np.full(m, params[\"val\"], dtype=int)\n    elif dist_type == \"poisson\":\n        vals = rng.poisson(params[\"mean\"], size=m)\n        return np.clip(vals, params[\"min\"], n).astype(int)\n    elif dist_type == \"uniform_int\":\n        return rng.integers(params[\"min\"], params[\"max\"] + 1, size=m)\n    return None\n\ndef get_node_propensities(dist_info, n, rng):\n    \"\"\"Generates node propensities based on distribution info.\"\"\"\n    dist_type = dist_info[\"type\"]\n    if dist_type == \"lognormal\":\n        return rng.lognormal(mean=0.0, sigma=dist_info[\"params\"][\"sigma\"], size=n)\n    elif dist_type == \"uniform\":\n        return np.ones(n)\n    return None\n\ndef test_heavy_tail(degrees, n_min, D_max, alpha_range):\n    \"\"\"\n    Performs heavy-tail testing on a degree distribution.\n    \"\"\"\n    unique_degrees = np.unique(degrees)\n    if len(unique_degrees) == 0:\n        return False\n        \n    best_D = np.inf\n    best_xmin = -1\n\n    for xmin_cand in unique_degrees:\n        tail_data = degrees[degrees >= xmin_cand]\n        n_tail = len(tail_data)\n\n        if n_tail  n_min:\n            continue\n\n        # Calculate MLE for alpha\n        if xmin_cand == 0: continue # Avoid log(0)\n        alpha_hat = 1.0 + n_tail / np.sum(np.log(tail_data / xmin_cand))\n\n        # Calculate KS statistic\n        sorted_tail = np.sort(tail_data)\n        \n        # Model CDF\n        model_cdf = 1 - (sorted_tail / xmin_cand) ** (1 - alpha_hat)\n        \n        # Empirical CDF\n        emp_cdf_at = (np.arange(1, n_tail + 1)) / n_tail\n        emp_cdf_before = (np.arange(0, n_tail)) / n_tail\n        \n        d_plus = np.max(emp_cdf_at - model_cdf) if emp_cdf_at.size > 0 else 0\n        d_minus = np.max(model_cdf - emp_cdf_before) if model_cdf.size > 0 else 0\n        \n        D = max(d_plus, d_minus)\n        \n        if D  best_D:\n            best_D = D\n            best_xmin = xmin_cand\n\n    if best_xmin == -1:\n        return False\n\n    # Final checks with the best x_min\n    final_tail = degrees[degrees >= best_xmin]\n    n_tail_final = len(final_tail)\n\n    # Condition 1: Minimum tail size (guaranteed if best_xmin != -1)\n    if n_tail_final  n_min:\n        return False\n\n    # Condition 2: KS statistic threshold\n    if best_D >= D_max:\n        return False\n\n    # Condition 3: Plausible exponent\n    alpha_final = 1.0 + n_tail_final / np.sum(np.log(final_tail / best_xmin))\n    if not (alpha_range[0] = alpha_final = alpha_range[1]):\n        return False\n\n    # Condition 4: Likelihood ratio\n    # MLE for exponential\n    mean_excess = np.mean(final_tail - best_xmin)\n    if mean_excess = 0: # Avoid division by zero or log of non-positive\n        return False\n    lambda_hat = 1.0 / mean_excess\n\n    # Log-likelihood for power law\n    log_L_PL = n_tail_final * np.log(alpha_final - 1) + n_tail_final * (alpha_final - 1) * np.log(best_xmin) - alpha_final * np.sum(np.log(final_tail))\n\n    # Log-likelihood for exponential\n    log_L_EXP = n_tail_final * np.log(lambda_hat) - lambda_hat * np.sum(final_tail - best_xmin)\n\n    R = log_L_PL - log_L_EXP\n    if R = 0:\n        return False\n    \n    return True\n\n\ndef run_simulation_and_test(case_params, heavy_tail_params):\n    \"\"\"\n    Runs one full test case: generate hypergraph, project, and test distribution.\n    \"\"\"\n    n, m, seed = case_params[\"n\"], case_params[\"m\"], case_params[\"seed\"]\n    size_dist, prop_dist = case_params[\"size_dist\"], case_params[\"prop_dist\"]\n    \n    rng = np.random.default_rng(seed)\n\n    # 1. Hypergraph generation\n    s_j = get_hyperedge_sizes(size_dist, m, n, rng)\n    w_i = get_node_propensities(prop_dist, n, rng)\n    p_i = w_i / np.sum(w_i)\n    \n    nodes = np.arange(n)\n    hyperedges = set()\n    for size in s_j:\n        if size > n: size = n\n        if size  2: continue\n        sampled_nodes = rng.choice(nodes, size=int(size), replace=False, p=p_i)\n        hyperedges.add(frozenset(sampled_nodes))\n        \n    # 2. Projection to pairwise network and degree calculation\n    adj = [set() for _ in range(n)]\n    for h in hyperedges:\n        h_list = list(h)\n        for i in range(len(h_list)):\n            for j in range(i + 1, len(h_list)):\n                u, v = h_list[i], h_list[j]\n                adj[u].add(v)\n                adj[v].add(u)\n    \n    d_G = np.array([len(neighbors) for neighbors in adj])\n    \n    # 4. Heavy-tail testing\n    return test_heavy_tail(\n        d_G,\n        heavy_tail_params[\"n_min\"],\n        heavy_tail_params[\"D_max\"],\n        heavy_tail_params[\"alpha_range\"]\n    )\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}