{
    "hands_on_practices": [
        {
            "introduction": "Functional enrichment analysis involves simultaneously testing thousands of gene sets, which dramatically increases the chance of finding false positives. This exercise provides a foundational hands-on experience with multiple hypothesis testing correction, a non-negotiable step in any analysis pipeline. By applying both the conservative Bonferroni correction and the widely-used Benjamini-Hochberg (BH) procedure, you will directly compare how controlling the Family-Wise Error Rate ($FWER$) versus the False Discovery Rate ($FDR$) impacts the set of significant findings .",
            "id": "3312231",
            "problem": "A researcher performs gene set enrichment analysis for a differentially expressed gene list using three standardized annotation resources: Gene Ontology Biological Process (GO BP), Kyoto Encyclopedia of Genes and Genomes (KEGG), and Reactome Pathway Knowledgebase (Reactome). For a combined panel of $m=12$ candidate categories across these resources, each tested by a one-sided hypergeometric enrichment test under standard assumptions, the resulting raw $p$-values (already corrected for the combinatorial sample space of each test but not for multiple hypotheses across categories) are:\n$0.0008$, $0.0015$, $0.0042$, $0.0070$, $0.0110$, $0.0190$, $0.0280$, $0.0370$, $0.0490$, $0.1200$, $0.3300$, $0.6200$.\n\nAssume that exactly $m_0=9$ of the $m=12$ tested category-level hypotheses are true nulls and that the null $p$-values are independent and identically distributed as $\\mathrm{Uniform}(0,1)$.\n\nUsing only first-principles definitions of family-wise error control and false discovery rate control:\n- Compute the Bonferroni-adjusted $p$-values for all $m=12$ tests and determine the number of rejected hypotheses at family-wise significance threshold $\\alpha_{\\text{FWER}}=0.05$.\n- Compute the Benjamini–Hochberg (BH) adjusted $p$-values for all $m=12$ tests and determine the number of rejected hypotheses at target false discovery rate $q=0.10$.\n\nThen, under the independence and uniformity assumptions specified:\n- Derive the expected number of false rejections for the Bonferroni procedure at $\\alpha_{\\text{FWER}}=0.05$.\n- Derive the bound on the expected proportion of false discoveries (false discovery rate) attained by the Benjamini–Hochberg procedure at $q=0.10$.\n\nFinally, define the dimensionless comparison metric\n$$\\rho \\equiv \\frac{E[V_{\\text{Bonf}}]/m}{\\left(\\frac{m_0}{m}\\right) q}$$,\nwhere $E[V_{\\text{Bonf}}]$ is the expected number of false rejections under Bonferroni. Compute $\\rho$ exactly and report the single value of $\\rho$ as an exact analytic fraction. Do not include any units, and do not round.",
            "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, objective, and self-contained. All necessary data, definitions, and assumptions for a unique and meaningful solution are provided. The problem is therefore deemed **valid**.\n\nThe problem requires the application of multiple hypothesis testing corrections (Bonferroni and Benjamini-Hochberg) to a set of $p$-values, followed by the derivation and calculation of related statistical quantities. Let the total number of tests be $m=12$, and let the provided raw $p$-values, already sorted in non-decreasing order, be denoted by $p_{(i)}$ for $i=1, 2, \\dots, 12$. The family-wise significance threshold is $\\alpha_{\\text{FWER}}=0.05$, and the target false discovery rate is $q=0.10$. The number of true null hypotheses is given as $m_0=9$.\n\n**1. Bonferroni Correction**\n\nThe Bonferroni correction controls the family-wise error rate (FWER) by adjusting each raw $p$-value, $p_i$. The Bonferroni-adjusted $p$-value, $p_{i, \\text{adj}}^{\\text{Bonf}}$, is defined as:\n$$p_{i, \\text{adj}}^{\\text{Bonf}} = \\min(m \\cdot p_i, 1)$$\nA hypothesis is rejected if its adjusted $p$-value is less than or equal to the FWER threshold $\\alpha_{\\text{FWER}}$. This is equivalent to rejecting any hypothesis for which the raw $p$-value $p_i \\le \\alpha_{\\text{FWER}} / m$.\n\nGiven $m=12$ and $\\alpha_{\\text{FWER}}=0.05$, the rejection threshold for raw $p$-values is $0.05/12 \\approx 0.004167$.\nThe provided raw $p$-values are:\n$p_{(1)} = 0.0008$, $p_{(2)} = 0.0015$, $p_{(3)} = 0.0042$, $p_{(4)} = 0.0070$, $p_{(5)} = 0.0110$, $p_{(6)} = 0.0190$, $p_{(7)} = 0.0280$, $p_{(8)} = 0.0370$, $p_{(9)} = 0.0490$, $p_{(10)} = 0.1200$, $p_{(11)} = 0.3300$, $p_{(12)} = 0.6200$.\n\nThe Bonferroni-adjusted $p$-values are calculated as $p_{(i), \\text{adj}}^{\\text{Bonf}} = \\min(12 \\cdot p_{(i)}, 1)$:\n$p_{(1), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0008 = 0.0096$\n$p_{(2), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0015 = 0.0180$\n$p_{(3), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0042 = 0.0504$\n$p_{(4), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0070 = 0.0840$\n$p_{(5), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0110 = 0.1320$\n$p_{(6), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0190 = 0.2280$\n$p_{(7), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0280 = 0.3360$\n$p_{(8), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0370 = 0.4440$\n$p_{(9), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0490 = 0.5880$\n$p_{(10), \\text{adj}}^{\\text{Bonf}} = \\min(12 \\times 0.1200, 1) = \\min(1.44, 1) = 1.0$\n$p_{(11), \\text{adj}}^{\\text{Bonf}} = \\min(12 \\times 0.3300, 1) = \\min(3.96, 1) = 1.0$\n$p_{(12), \\text{adj}}^{\\text{Bonf}} = \\min(12 \\times 0.6200, 1) = \\min(7.44, 1) = 1.0$\n\nTo find the number of rejected hypotheses, we compare each adjusted $p$-value to $\\alpha_{\\text{FWER}} = 0.05$:\n$p_{(1), \\text{adj}}^{\\text{Bonf}} = 0.0096 \\le 0.05$ (Reject)\n$p_{(2), \\text{adj}}^{\\text{Bonf}} = 0.0180 \\le 0.05$ (Reject)\n$p_{(3), \\text{adj}}^{\\text{Bonf}} = 0.0504 > 0.05$ (Do not reject)\nAll subsequent adjusted $p$-values are also greater than $0.05$.\nThus, the Bonferroni procedure rejects $2$ hypotheses.\n\n**2. Benjamini-Hochberg (BH) Correction**\n\nThe Benjamini-Hochberg procedure controls the False Discovery Rate (FDR). The BH-adjusted $p$-value, $p_{(i), \\text{adj}}^{\\text{BH}}$, for the $i$-th sorted raw $p$-value $p_{(i)}$ is computed using a step-down procedure that enforces monotonicity. The adjusted value for the largest raw $p$-value is the raw $p$-value itself: $p_{(m), \\text{adj}}^{\\text{BH}}=p_{(m)}$. For all other $p$-values, the adjustment is made recursively:\n$$p_{(i), \\text{adj}}^{\\text{BH}} = \\min\\left(p_{(i+1), \\text{adj}}^{\\text{BH}}, \\frac{m \\cdot p_{(i)}}{i}\\right) \\quad \\text{for } i=m-1, \\dots, 1$$\nThis is equivalent to $p_{(i), \\text{adj}}^{\\text{BH}} = \\min_{j \\ge i}\\left(\\frac{m \\cdot p_{(j)}}{j}\\right)$.\n\nLet's compute the BH-adjusted $p$-values:\n$p_{(12), \\text{adj}}^{\\text{BH}} = p_{(12)} = 0.6200$\n$p_{(11), \\text{adj}}^{\\text{BH}} = \\min(p_{(12), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.3300}{11}) = \\min(0.6200, 0.3600) = 0.3600$\n$p_{(10), \\text{adj}}^{\\text{BH}} = \\min(p_{(11), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.1200}{10}) = \\min(0.3600, 0.1440) = 0.1440$\n$p_{(9), \\text{adj}}^{\\text{BH}} = \\min(p_{(10), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0490}{9}) = \\min(0.1440, 0.0653\\overline{3}) = 0.0653\\overline{3}$\n$p_{(8), \\text{adj}}^{\\text{BH}} = \\min(p_{(9), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0370}{8}) = \\min(0.0653\\overline{3}, 0.0555) = 0.0555$\n$p_{(7), \\text{adj}}^{\\text{BH}} = \\min(p_{(8), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0280}{7}) = \\min(0.0555, 0.0480) = 0.0480$\n$p_{(6), \\text{adj}}^{\\text{BH}} = \\min(p_{(7), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0190}{6}) = \\min(0.0480, 0.0380) = 0.0380$\n$p_{(5), \\text{adj}}^{\\text{BH}} = \\min(p_{(6), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0110}{5}) = \\min(0.0380, 0.0264) = 0.0264$\n$p_{(4), \\text{adj}}^{\\text{BH}} = \\min(p_{(5), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0070}{4}) = \\min(0.0264, 0.0210) = 0.0210$\n$p_{(3), \\text{adj}}^{\\text{BH}} = \\min(p_{(4), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0042}{3}) = \\min(0.0210, 0.0168) = 0.0168$\n$p_{(2), \\text{adj}}^{\\text{BH}} = \\min(p_{(3), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0015}{2}) = \\min(0.0168, 0.0090) = 0.0090$\n$p_{(1), \\text{adj}}^{\\text{BH}} = \\min(p_{(2), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0008}{1}) = \\min(0.0090, 0.0096) = 0.0090$\n\nA hypothesis is rejected if its BH-adjusted $p$-value is less than or equal to the target FDR $q=0.10$.\n$p_{(1), \\text{adj}}^{\\text{BH}}$ through $p_{(9), \\text{adj}}^{\\text{BH}}$ are all less than or equal to $0.10$. $p_{(10), \\text{adj}}^{\\text{BH}} = 0.1440 > 0.10$.\nThus, the Benjamini-Hochberg procedure rejects $9$ hypotheses.\n\n**3. Expected Number of False Rejections (Bonferroni)**\n\nLet $V_{\\text{Bonf}}$ be the number of false rejections (Type I errors) under the Bonferroni procedure. These are true null hypotheses that are incorrectly rejected. Let $H_0$ be the set of $m_0$ true null hypotheses.\n$$V_{\\text{Bonf}} = \\sum_{i \\in H_0} \\mathbb{I}\\left(p_i \\le \\frac{\\alpha_{\\text{FWER}}}{m}\\right)$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. By the linearity of expectation, the expected number of false rejections is:\n$$E[V_{\\text{Bonf}}] = E\\left[\\sum_{i \\in H_0} \\mathbb{I}\\left(p_i \\le \\frac{\\alpha_{\\text{FWER}}}{m}\\right)\\right] = \\sum_{i \\in H_0} E\\left[\\mathbb{I}\\left(p_i \\le \\frac{\\alpha_{\\text{FWER}}}{m}\\right)\\right] = \\sum_{i \\in H_0} P\\left(p_i \\le \\frac{\\alpha_{\\text{FWER}}}{m}\\right)$$\nUnder the assumption that $p$-values for true null hypotheses are independently and identically distributed as $\\mathrm{Uniform}(0,1)$, the probability of rejecting a single true null is $P(p_i \\le \\frac{\\alpha_{\\text{FWER}}}{m}) = \\frac{\\alpha_{\\text{FWER}}}{m}$.\nSince there are $m_0$ such hypotheses, the expected number of false rejections is:\n$$E[V_{\\text{Bonf}}] = m_0 \\cdot \\frac{\\alpha_{\\text{FWER}}}{m}$$\nSubstituting the given values: $m_0=9$, $m=12$, and $\\alpha_{\\text{FWER}}=0.05$:\n$$E[V_{\\text{Bonf}}] = 9 \\times \\frac{0.05}{12} = \\frac{9 \\times 0.05}{12} = \\frac{0.45}{12} = 0.0375 = \\frac{3}{80}$$\n\n**4. Bound on False Discovery Rate (Benjamini-Hochberg)**\n\nThe False Discovery Rate (FDR) is defined as the expected proportion of false discoveries among all discoveries, $E[V/R]$, where $V$ is the number of false rejections and $R$ is the total number of rejections ($V/R$ is defined as $0$ if $R=0$). The Benjamini-Hochberg procedure, when applied at a level $q$, guarantees that for independent tests (as assumed here), the FDR is controlled:\n$$\\text{FDR} = E\\left[\\frac{V}{R}\\right] \\le \\frac{m_0}{m}q$$\nThe problem asks for the bound attained by the procedure. This bound is the right-hand side of the inequality.\nSubstituting the given values: $m_0=9$, $m=12$, and $q=0.10$:\n$$\\text{Bound} = \\frac{9}{12} \\times 0.10 = \\frac{3}{4} \\times 0.10 = 0.75 \\times 0.10 = 0.075 = \\frac{3}{40}$$\n\n**5. Calculation of the Comparison Metric $\\rho$**\n\nThe dimensionless comparison metric $\\rho$ is defined as:\n$$\\rho \\equiv \\frac{E[V_{\\text{Bonf}}]/m}{\\left(\\frac{m_0}{m}\\right) q}$$\nWe substitute the derived expression for $E[V_{\\text{Bonf}}]$ from part 3:\n$$E[V_{\\text{Bonf}}] = m_0 \\frac{\\alpha_{\\text{FWER}}}{m}$$\nThe expression for $\\rho$ becomes:\n$$\\rho = \\frac{\\left(m_0 \\frac{\\alpha_{\\text{FWER}}}{m}\\right) / m}{\\left(\\frac{m_0}{m}\\right) q} = \\frac{\\frac{m_0 \\alpha_{\\text{FWER}}}{m^2}}{\\frac{m_0 q}{m}}$$\nSimplifying the expression by canceling terms:\n$$\\rho = \\frac{m_0 \\alpha_{\\text{FWER}}}{m^2} \\cdot \\frac{m}{m_0 q} = \\frac{\\alpha_{\\text{FWER}}}{m \\cdot q}$$\nNow, we compute the exact value of $\\rho$ using the given constants $\\alpha_{\\text{FWER}}=0.05$, $m=12$, and $q=0.10$:\n$$\\rho = \\frac{0.05}{12 \\times 0.10} = \\frac{0.05}{1.2} = \\frac{5/100}{12/10} = \\frac{5}{100} \\times \\frac{10}{12} = \\frac{50}{1200} = \\frac{1}{24}$$",
            "answer": "$$\\boxed{\\frac{1}{24}}$$"
        },
        {
            "introduction": "Once you have a list of significant terms, a second challenge emerges that is unique to hierarchical databases like the Gene Ontology (GO). Because annotations propagate up the Directed Acyclic Graph (DAG), a specific, enriched child term can cause its general ancestor terms to also appear significant, leading to redundant and uninformative results. This practice guides you through implementing the classic `elim` and `weight` algorithms, which directly address this by adjusting a parent term's significance based on the evidence already provided by its more specific children .",
            "id": "3312287",
            "problem": "You are given the task of formalizing and implementing structured traversal strategies over a Directed Acyclic Graph (DAG) representing Gene Ontology (GO) terms to address inherited annotation dependencies in statistical enrichment testing. The fundamental base for this problem is the definition of the hypergeometric model for enrichment testing, the DAG inheritance of GO annotations, and the logic of attenuation of evidence along hierarchical structures observed in Gene Ontology, Kyoto Encyclopedia of Genes and Genomes (KEGG), and Reactome. The goal is to construct programmatic versions of two traversal strategies, commonly referred to as “elim” and “weight,” to mitigate the influence of broad ancestor GO terms on the significance of their child terms. You must justify these strategies through implementation and calculation using a well-defined test suite.\n\nFundamental base and definitions:\n- Let the background universe of genes be a finite set $U$ with cardinality $|U| = M$.\n- Let $I \\subseteq U$ be a fixed \"interest\" set of genes (e.g., differentially expressed), with cardinality $|I| = n$.\n- For each GO term $t$, let $A_t \\subseteq U$ denote the set of genes annotated directly to term $t$.\n- The GO structure is a Directed Acyclic Graph (DAG), where edges point from parent terms to child terms, and child terms represent more specific biological functions than their parents.\n- For enrichment of a term $t$ under the hypergeometric model, define the overlap $k_t = |I \\cap A_t|$, and $K_t = |A_t|$. The one-sided overrepresentation $p$-value for observing $k_t$ or more interest genes annotated to $t$ is given by the hypergeometric tail:\n$$\np_t = \\Pr\\{X \\ge k_t\\} \\quad \\text{where } X \\sim \\text{Hypergeometric}(M, K_t, n).\n$$\n\nAttenuation strategies to implement:\n- Elimination strategy (“elim”): Traversal proceeds bottom-up from leaves to root. When a child term $c$ is deemed significant under threshold $\\alpha$ (i.e., $p_c \\le \\alpha$), the genes annotated to $c$ are excluded from the annotation sets of all its ancestors before testing those ancestors. Formally, for a parent term $t$, define the effective annotation set after elimination as $A_t^{\\text{elim}} = A_t \\setminus \\bigcup_{c \\in \\mathcal{D}_t^{\\text{sig}}} A_c$, where $\\mathcal{D}_t^{\\text{sig}}$ are the descendants of $t$ that have been found significant at level $\\alpha$ in the bottom-up traversal. The parent is then tested using $k_t^{\\text{elim}} = |I \\cap A_t^{\\text{elim}}|$ and $K_t^{\\text{elim}} = |A_t^{\\text{elim}}|$ in the same hypergeometric tail.\n- Weighting strategy (“weight”): Traversal computes raw $p$-values for all terms, then for each parent $t$, the parent’s raw $p_t$ is increased proportionally to the fraction of interest evidence already concentrated in significant children. Define for each child $c$ of $t$ the overlap fraction\n$$\nf_{t,c} = \\frac{|I \\cap A_t \\cap A_c|}{\\max(1, |I \\cap A_t|)}.\n$$\nThen define the cumulative child influence\n$$\nS_t = \\sum_{c \\in \\text{child}(t)} f_{t,c} \\cdot \\mathbf{1}[p_c \\le \\alpha],\n$$\nand define the weighted parent value as\n$$\np_t^{\\text{weight}} = \\min\\{1, \\, p_t \\cdot (1 + \\lambda S_t)\\},\n$$\nwith scaling parameter $\\lambda = 1$. This strategy ensures an attenuation of ancestor significance proportional to the concentration of interest genes already explained by significant children. The choice of multiplicative adjustment is a principled monotone penalty that raises $p$-values when child terms capture overlapping evidence, thereby discouraging double-counting across the DAG.\n\nTest suite specification:\nAll test cases use the same GO DAG and annotation map, but vary the interest set $I$. The DAG, background universe, and annotations are specified as follows:\n- Universe $U = \\{g_1, g_2, g_3, g_4, g_5, g_6, g_7, g_8, g_9, g_{10}, g_{11}, g_{12}\\}$, with $|U| = M = 12$.\n- Terms: $R$ (root), $A$, $B$, $C$ with edges $R \\to A$, $R \\to B$, $A \\to C$.\n- Annotations:\n    - $A_R = \\{g_1, g_2, g_3, g_4, g_5, g_6, g_7, g_8, g_9, g_{10}\\}$ with $|A_R| = 10$.\n    - $A_A = \\{g_1, g_2, g_3, g_4, g_5, g_6\\}$ with $|A_A| = 6$.\n    - $A_B = \\{g_7, g_8, g_9, g_{10}\\}$ with $|A_B| = 4$.\n    - $A_C = \\{g_1, g_2, g_3\\}$ with $|A_C| = 3$.\nAll test cases use $\\alpha = 0.05$ and $\\lambda = 1$, and focus on the parent term $A$ to evaluate the effect of the strategies relative to its child term $C$.\n\nTest cases:\n- Case $1$ (happy path, child term concentration): $I = \\{g_1, g_2, g_3, g_{11}\\}$ with $|I| = n = 4$. Expectation: $C$ is significant, and elimination reduces the apparent significance of $A$ by removing $C$’s genes from $A$, while weighting raises the parent’s $p$ proportionally.\n- Case $2$ (boundary, child not significant): $I = \\{g_4, g_5, g_7, g_8\\}$ with $|I| = n = 4$. Expectation: $C$ not significant, and both elim and weight reduce to the raw parent value, showing no attenuation.\n- Case $3$ (edge, no overlap with parent): $I = \\{g_{11}, g_{12}\\}$ with $|I| = n = 2$. Expectation: parent shows no enrichment; both strategies keep the parent non-significant.\n\nRequired program behavior:\n- Implement the hypergeometric one-sided tail $p$-value for overrepresentation, the elimination strategy as defined, and the weighting strategy as defined. Treat all set cardinalities and operations in exact integer terms, and be precise with the DAG relationships. The program must compute for each test case the triple $[p_A, p_A^{\\text{elim}}, p_A^{\\text{weight}}]$ as floating-point values.\n- Final output format: Your program should produce a single line containing the results for the three test cases as a comma-separated list of three lists of floats, enclosed in square brackets, in the order of Case $1$, Case $2$, Case $3$. For example, an output of the form `[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3]]`, where each $x_i$, $y_i$, $z_i$ is a float. No other text should be printed.\n- There are no physical units involved. Angles are not used. Percentages must be represented as decimals in $[0,1]$ when they arise (e.g., $0.05$ for $\\alpha$).\n\nScientific realism:\nThe GO DAG inheritance induces dependencies because if a specific child term $c$ is enriched, its ancestor $t$ may appear enriched by inheritance of $A_c \\subseteq A_t$. The constructed strategies are justified to attenuate evidence that would otherwise be double-counted. The principles extend conceptually to pathway hierarchies and graphs in Kyoto Encyclopedia of Genes and Genomes (KEGG) and Reactome, though the exact topology and statistical structure differ (e.g., Reactome’s event hierarchy and KEGG’s pathway graphs are not strictly isomorphic to the GO DAG).",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded, well-posed, objective, and internally consistent. It presents a formal and solvable problem in computational systems biology concerning the correction of statistical dependencies in functional enrichment analysis on a Directed Acyclic Graph (DAG).\n\nThe objective is to implement and evaluate two strategies, \"elim\" and \"weight\", which are designed to mitigate the statistical artifacts arising from the hierarchical structure of ontologies like the Gene Ontology (GO). These artifacts often manifest as broad, less informative parent terms appearing significant merely due to the strong signal from one or more of their specific, more informative child terms. We will use a defined toy model of a GO-like DAG, gene annotations, and specific sets of \"interest genes\" to calculate the effects of these strategies.\n\nThe statistical foundation for enrichment analysis is the one-sided hypergeometric test. Given a universe of $M$ genes, an interest set of $n$ genes, and a GO term $t$ annotated with $K_t$ genes, the probability ($p$-value) of observing $k_t$ or more genes from the interest set annotated to term $t$ by chance is given by the survival function of the hypergeometric distribution:\n$$\np_t = \\Pr\\{X \\ge k_t\\} = \\sum_{i=k_t}^{\\min(n, K_t)} \\frac{\\binom{K_t}{i} \\binom{M - K_t}{n - i}}{\\binom{M}{n}}, \\quad \\text{where } X \\sim \\text{Hypergeometric}(M, K_t, n)\n$$\nHere, $k_t$ is the number of genes in the intersection of the interest set and the term's annotation set.\n\nThe provided test suite uses a consistent background and structure:\n- Gene universe size: $M = |U| = 12$.\n- GO terms and DAG structure: Edges $R \\to A$, $R \\to B$, $A \\to C$. Term $C$ is a child of $A$.\n- Gene annotations:\n    - $A_A = \\{g_1, g_2, g_3, g_4, g_5, g_6\\}$, so $K_A = |A_A| = 6$.\n    - $A_C = \\{g_1, g_2, g_3\\}$, so $K_C = |A_C| = 3$. We note that $A_C \\subset A_A$, consistent with the true-path rule in GO.\n- Significance level for identifying significant children: $\\alpha = 0.05$.\n- Weighting parameter: $\\lambda = 1$.\n\nWe will now analyze each of the three test cases by first calculating the raw $p$-values for terms $A$ and $C$, and then applying the `elim` and `weight` algorithms to derive the adjusted $p$-values for the parent term $A$.\n\n**Case 1: Child term concentration**\n- Interest set: $I = \\{g_1, g_2, g_3, g_{11}\\}$, with size $n = |I| = 4$.\n\n1.  **Test Child Term C**:\n    - Intersection with $A_C$: $I \\cap A_C = \\{g_1, g_2, g_3\\}$. Cardinality $k_C = 3$.\n    - $p_C = \\Pr\\{X \\ge 3\\}$ for $X \\sim \\text{Hypergeometric}(M=12, K_C=3, n=4)$.\n    - $p_C = \\frac{\\binom{3}{3}\\binom{12-3}{4-3}}{\\binom{12}{4}} = \\frac{1 \\cdot 9}{495} \\approx 0.01818$.\n    - Since $p_C \\approx 0.01818 \\le \\alpha=0.05$, term $C$ is significant.\n\n2.  **Test Parent Term A (Raw)**:\n    - Intersection with $A_A$: $I \\cap A_A = \\{g_1, g_2, g_3\\}$. Cardinality $k_A = 3$.\n    - $p_A = \\Pr\\{X \\ge 3\\}$ for $X \\sim \\text{Hypergeometric}(M=12, K_A=6, n=4)$.\n    - $p_A = \\frac{\\binom{6}{3}\\binom{12-6}{4-3}}{\\binom{12}{4}} + \\frac{\\binom{6}{4}\\binom{12-6}{4-4}}{\\binom{12}{4}} = \\frac{20 \\cdot 6}{495} + \\frac{15 \\cdot 1}{495} = \\frac{120+15}{495} = \\frac{135}{495} \\approx 0.27273$.\n\n3.  **Apply `elim` Strategy to A**:\n    - Since child $C$ is significant, its genes are removed from parent $A$.\n    - Effective annotation set: $A_A^{\\text{elim}} = A_A \\setminus A_C = \\{g_4, g_5, g_6\\}$.\n    - Effective set size: $K_A^{\\text{elim}} = |A_A^{\\text{elim}}| = 3$.\n    - Effective intersection: $I \\cap A_A^{\\text{elim}} = \\emptyset$. Cardinality $k_A^{\\text{elim}} = 0$.\n    - $p_A^{\\text{elim}} = \\Pr\\{X \\ge 0\\}$ for $X \\sim \\text{Hypergeometric}(M=12, K_A^{\\text{elim}}=3, n=4)$. This probability is $1$.\n\n4.  **Apply `weight` Strategy to A**:\n    - Since child $C$ is significant, we calculate the penalty.\n    - Child influence fraction: $f_{A,C} = \\frac{|I \\cap A_C|}{\\max(1, |I \\cap A_A|)} = \\frac{k_C}{\\max(1, k_A)} = \\frac{3}{\\max(1, 3)} = 1$.\n    - Cumulative influence: $S_A = f_{A,C} \\cdot \\mathbf{1}[p_C \\le \\alpha] = 1 \\cdot 1 = 1$.\n    - Weighted $p$-value: $p_A^{\\text{weight}} = \\min\\{1, p_A \\cdot (1 + \\lambda S_A)\\} = \\min\\{1, 0.27273 \\cdot (1 + 1 \\cdot 1)\\} = \\min\\{1, 0.54545\\} = 0.54545$.\n\n- **Result for Case 1**: $[p_A, p_A^{\\text{elim}}, p_A^{\\text{weight}}] \\approx [0.27273, 1.0, 0.54545]$.\n\n**Case 2: Child not significant**\n- Interest set: $I = \\{g_4, g_5, g_7, g_8\\}$, with size $n = |I| = 4$.\n\n1.  **Test Child Term C**:\n    - Intersection with $A_C$: $I \\cap A_C = \\emptyset$. Cardinality $k_C = 0$.\n    - $p_C = \\Pr\\{X \\ge 0\\}$ for $X \\sim \\text{Hypergeometric}(M=12, K_C=3, n=4)$. This probability is $1$.\n    - Since $p_C = 1 > \\alpha=0.05$, term $C$ is not significant.\n\n2.  **Test Parent Term A (Raw)**:\n    - Intersection with $A_A$: $I \\cap A_A = \\{g_4, g_5\\}$. Cardinality $k_A = 2$.\n    - $p_A = \\Pr\\{X \\ge 2\\}$ for $X \\sim \\text{Hypergeometric}(M=12, K_A=6, n=4)$.\n    - $p_A = \\frac{\\binom{6}{2}\\binom{6}{2}}{\\binom{12}{4}} + \\frac{\\binom{6}{3}\\binom{6}{1}}{\\binom{12}{4}} + \\frac{\\binom{6}{4}\\binom{6}{0}}{\\binom{12}{4}} = \\frac{15 \\cdot 15}{495} + \\frac{20 \\cdot 6}{495} + \\frac{15 \\cdot 1}{495} = \\frac{225+120+15}{495} = \\frac{360}{495} \\approx 0.72727$.\n\n3.  **Apply `elim` and `weight` Strategies to A**:\n    - Since child $C$ is not significant, neither strategy applies an adjustment.\n    - $A_A^{\\text{elim}} = A_A$, so $p_A^{\\text{elim}} = p_A \\approx 0.72727$.\n    - The indicator function $\\mathbf{1}[p_C \\le \\alpha]$ is $0$, so $S_A = 0$, and $p_A^{\\text{weight}} = p_A \\approx 0.72727$.\n\n- **Result for Case 2**: $[p_A, p_A^{\\text{elim}}, p_A^{\\text{weight}}] \\approx [0.72727, 0.72727, 0.72727]$.\n\n**Case 3: No overlap with parent**\n- Interest set: $I = \\{g_{11}, g_{12}\\}$, with size $n = |I| = 2$.\n\n1.  **Test Child Term C**:\n    - Intersection with $A_C$: $I \\cap A_C = \\emptyset$. Cardinality $k_C = 0$.\n    - $p_C = \\Pr\\{X \\ge 0\\}$ for $X \\sim \\text{Hypergeometric}(M=12, K_C=3, n=2)$. This probability is $1$.\n    - Term $C$ is not significant.\n\n2.  **Test Parent Term A (Raw)**:\n    - Intersection with $A_A$: $I \\cap A_A = \\emptyset$. Cardinality $k_A = 0$.\n    - $p_A = \\Pr\\{X \\ge 0\\}$ for $X \\sim \\text{Hypergeometric}(M=12, K_A=6, n=2)$. This probability is $1$.\n\n3.  **Apply `elim` and `weight` Strategies to A**:\n    - Child $C$ is not significant, so no adjustments are made.\n    - $p_A^{\\text{elim}} = p_A = 1$.\n    - $p_A^{\\text{weight}} = p_A = 1$.\n\n- **Result for Case 3**: $[p_A, p_A^{\\text{elim}}, p_A^{\\text{weight}}] = [1.0, 1.0, 1.0]$.\n\nThese calculations confirm the expected behavior of the algorithms. In Case 1, where the child term captures all the evidence, both strategies correctly attenuate the parent term's apparent significance. In Cases 2 and 3, where the child term is not significant, the parent term's $p$-value is correctly left unaltered.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import hypergeom\n\ndef solve():\n    \"\"\"\n    Implements and evaluates 'elim' and 'weight' strategies for GO enrichment.\n    \"\"\"\n\n    # --- Static data from the problem statement ---\n    # Universe of genes\n    U = {f'g_{i}' for i in range(1, 13)}\n    M = len(U)\n\n    # GO term annotations (full sets including inheritance)\n    annotations = {\n        'R': {f'g_{i}' for i in range(1, 11)},\n        'A': {f'g_{i}' for i in range(1, 7)},\n        'B': {f'g_{i}' for i in range(7, 11)},\n        'C': {f'g_{i}' for i in range(1, 4)},\n    }\n    \n    # DAG structure: map parent to a list of its children\n    dag_children = {\n        'R': ['A', 'B'],\n        'A': ['C'],\n        'B': [],\n        'C': []\n    }\n    # For bottom-up traversal (elim strategy), we need parent information\n    dag_parents = {\n        'A': ['R'],\n        'B': ['R'],\n        'C': ['A'],\n        'R': []\n    }\n\n    # Parameters\n    alpha = 0.05\n    lambda_param = 1.0\n\n    # --- Test cases ---\n    test_cases = [\n        # Case 1: Happy path, child term concentration\n        {'I': {'g_1', 'g_2', 'g_3', 'g_11'}},\n        # Case 2: Boundary, child not significant\n        {'I': {'g_4', 'g_5', 'g_7', 'g_8'}},\n        # Case 3: Edge, no overlap with parent\n        {'I': {'g_11', 'g_12'}},\n    ]\n    \n    # Helper function for hypergeometric p-value\n    def hypergeometric_p_value(M_total, K_annotated, n_interest, k_overlap):\n        \"\"\"\n        Calculates the one-sided overrepresentation p-value.\n        Uses scipy.stats.hypergeom.sf (survival function), which is 1 - cdf.\n        sf(k-1, M, n, N) is Pr(X >= k).\n        - M (total) is M in problem\n        - n (type I) is K in problem\n        - N (draws) is n in problem\n        \"\"\"\n        if k_overlap == 0:\n            return 1.0\n        # sf(k-1, ...) computes P(X >= k)\n        return hypergeom.sf(k_overlap - 1, M_total, K_annotated, n_interest)\n\n    final_results = []\n    \n    # --- Main processing loop ---\n    for case in test_cases:\n        I = case['I']\n        n = len(I)\n\n        # We focus on the parent term A and its child C.\n        \n        # 1. Calculate raw p-values for C and A\n        K_C = len(annotations['C'])\n        k_C = len(I.intersection(annotations['C']))\n        p_C = hypergeometric_p_value(M, K_C, n, k_C)\n\n        K_A = len(annotations['A'])\n        k_A = len(I.intersection(annotations['A']))\n        p_A = hypergeometric_p_value(M, K_A, n, k_A)\n\n        # Determine if child C is significant\n        is_C_significant = (p_C = alpha)\n\n        # 2. Calculate p_A_elim\n        p_A_elim = p_A\n        if is_C_significant:\n            # The only descendant of A is C.\n            # Perform elimination: remove genes of significant children\n            A_elim_set = annotations['A'].difference(annotations['C'])\n            K_A_elim = len(A_elim_set)\n            k_A_elim = len(I.intersection(A_elim_set))\n            p_A_elim = hypergeometric_p_value(M, K_A_elim, n, k_A_elim)\n\n        # 3. Calculate p_A_weight\n        p_A_weight = p_A\n        if is_C_significant:\n            # The only child of A is C\n            # Calculate overlap fraction f_AC\n            f_AC_num = len(I.intersection(annotations['A']).intersection(annotations['C']))\n            # Since C is child of A, A_A intersect A_C is A_C\n            # f_AC_num = len(I.intersection(annotations['C'])) = k_C\n            f_AC_den = max(1, k_A)\n            f_AC = f_AC_num / f_AC_den\n            \n            # Calculate cumulative child influence S_A\n            S_A = f_AC # Since C is the only child of A\n\n            # Calculate weighted p-value\n            p_A_weight = min(1.0, p_A * (1.0 + lambda_param * S_A))\n\n        final_results.append([p_A, p_A_elim, p_A_weight])\n\n    # Final print statement in the exact required format.\n    output_str = f\"[{','.join([f'[{\",\".join(map(str, res))}]' for res in final_results])}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The final step in functional annotation is often the most difficult: interpreting a long list of statistically significant terms. To transform this list into biological insight, we must identify and summarize the core biological themes. This exercise simulates that process by having you quantify the relationships between GO terms using semantic similarity, a measure based on shared information content . You will then use these similarity scores to cluster the terms and select a concise set of representatives, a key skill for generating clear and focused biological hypotheses from high-throughput data.",
            "id": "3312268",
            "problem": "You are given a simplified Directed Acyclic Graph (DAG) that models a subset of the Gene Ontology (GO; Gene Ontology), intended for computational functional annotation. For completeness and context, the Kyoto Encyclopedia of Genes and Genomes (KEGG) and Reactome are other pathway resources, but this problem focuses on the Gene Ontology DAG semantics. The graph encodes the Biological Process branch with integer-labeled terms from $1$ to $12$, where each term may have zero or more parents. The DAG obeys the True Path Rule, meaning annotations to child terms also imply annotations to all ancestor terms. The problem requires you to construct a semantic similarity matrix for a set of GO terms under the Resnik definition, normalize it to a distance, and then propose a clustering with representative selection that reduces redundancy while preserving coverage of biological processes, measured as coverage of leaf processes. All quantities and steps must be derived from first principles given below and the provided data; no external databases are permitted.\n\nFoundational definitions to be used:\n- The Gene Ontology DAG is specified by parent relationships. Each term $t$ has a set of parents $\\mathrm{Pa}(t)$, and ancestors $\\mathrm{Anc}(t)$ defined recursively as $t$ itself plus ancestors of its parents. The DAG is acyclic and has a unique root at $t=1$.\n- The True Path Rule defines the count $c(t)$ for a term $t$ as the total number of annotations to all leaf descendants of $t$ (including $t$ if it is a leaf). The total annotation count is $C=\\sum_{\\ell \\in L}c(\\ell)$, where $L$ is the set of leaf terms.\n- The probability of a term is $p(t)=\\frac{c(t)}{C}$, and its Information Content is $IC(t)=-\\ln p(t)$ using the natural logarithm.\n- For two terms $a$ and $b$, the Most Informative Common Ancestor (MICA) is the ancestor in $\\mathrm{Anc}(a)\\cap \\mathrm{Anc}(b)$ with maximal $IC$; the Resnik semantic similarity is $s_{\\mathrm{Resnik}}(a,b)=IC(\\mathrm{MICA}(a,b))$.\n- To bound similarity to $[0,1]$ for clustering, use the normalized similarity $s(a,b)=\\frac{s_{\\mathrm{Resnik}}(a,b)}{\\max(IC(a),IC(b))}$, with the convention that $s(a,b)=0$ if $\\max(IC(a),IC(b))=0$.\n- Define a distance $d(a,b)=1-s(a,b)$.\n- Form a threshold graph on a given set $S$ of terms by connecting $a,b\\in S$ with an undirected edge if $d(a,b)\\le \\theta$, where $\\theta$ is a supplied threshold. Clusters are the connected components of this threshold graph.\n\nCoverage of biological processes:\n- Let $L$ be the leaves of the DAG. The leaf coverage set for any set of terms $X$ is $L(X)=\\bigcup_{t\\in X}L(t)$ where $L(t)$ is the set of leaves reachable from $t$ along child edges.\n- The coverage fraction of a representative set $R$ relative to the original set $S$ is $\\gamma(R,S)=\\frac{|L(R)|}{|L(S)|}$, with the convention $\\gamma(R,S)=1$ if $|L(S)|=0$.\n- Representative selection must reduce redundancy while preserving coverage: choose at most one representative per cluster initially to maximize coverage, and then greedily add additional terms across any clusters if needed until $\\gamma(R,S)\\ge \\alpha$, for a supplied coverage threshold $\\alpha$ expressed as a decimal. Within a cluster, choose the initial representative that maximizes $|L(\\{t\\})|$ intersected with $L(S)$; break ties by larger $IC$, then by smaller term index. For subsequent greedy additions, always choose the remaining term with largest marginal increase in $|L(R)|$; break ties by larger $IC$, then smaller index. The final representative list for each test case must be sorted in increasing term index.\n\nThe DAG and base leaf annotation counts are specified as follows:\n- Parent relationships (each pair $(u,v)$ means $u$ is the parent of $v$):\n  $(1,2)$, $(1,3)$, $(2,4)$, $(2,5)$, $(3,6)$, $(3,7)$, $(3,12)$, $(4,8)$, $(5,9)$, $(6,10)$, $(7,11)$.\n- Leaves are $8$, $9$, $10$, $11$, $12$.\n- Base leaf annotation counts are:\n  $c(8)=50$, $c(9)=70$, $c(10)=120$, $c(11)=90$, $c(12)=80$.\n- The total count is $C=50+70+120+90+80$.\n\nYour program must:\n1. Compute $c(t)$ for all $t\\in\\{1,\\ldots,12\\}$ by summing leaf counts over descendants.\n2. Compute $p(t)$ and $IC(t)$ for all terms.\n3. For each test case set $S$, build the normalized Resnik similarity matrix $[s(a,b)]_{a,b\\in S}$ and the corresponding distance matrix $[d(a,b)]_{a,b\\in S}$.\n4. Cluster $S$ into connected components of the threshold graph using the supplied $\\theta$.\n5. Select representatives as defined above to achieve coverage fraction at least $\\alpha$.\n6. Produce the final output as a single line containing the results for all test cases as a comma-separated list enclosed in square brackets. Each result must be a list of integers representing the selected representatives for that test case, sorted in increasing order. No spaces are allowed anywhere in this output.\n\nTest suite:\n- Test case $1$: $S=[3,4,5,6,7]$, $\\theta=0.8$, $\\alpha=0.8$.\n- Test case $2$: $S=[4,6]$, $\\theta=0.5$, $\\alpha=1.0$.\n- Test case $3$: $S=[3,6,10]$, $\\theta=0.8$, $\\alpha=0.67$.\n- Test case $4$: $S=[4,4,5,5]$, $\\theta=0.42$, $\\alpha=1.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the sorted list of representative term indices for the corresponding test case. For example, a valid output format is `[[a_1,a_2],[b_1],[c_1,c_2,c_3],[d_1]]`. The output must contain no spaces.",
            "solution": "The problem requires the implementation of a multi-step bioinformatic algorithm to cluster Gene Ontology (GO) terms and select representative terms based on semantic similarity and process coverage. The problem is well-posed, scientifically grounded in the principles of computational functional genomics, and algorithmically specified. We will proceed with a systematic, principle-based solution.\n\nThe overall approach involves four main stages:\n1.  **System-wide Pre-computation**: We first process the given Directed Acyclic Graph (DAG) structure and annotation counts to compute properties for all terms, which are independent of the specific test cases.\n2.  **Pairwise Similarity and Distance Calculation**: For each test case, we compute a distance matrix for the given set of terms $S$.\n3.  **Clustering**: We use the distance matrix and a threshold $\\theta$ to group the terms in $S$ into clusters.\n4.  **Representative Selection**: We apply the specified two-phase algorithm to select a subset of representative terms $R \\subseteq S$ that satisfies a given coverage threshold $\\alpha$.\n\n**1. System-wide Pre-computation**\n\nFirst, we represent the GO hierarchy as a graph. The parent relationships define a graph with terms $\\{1, \\dots, 12\\}$ as nodes. For each term $t$, we can determine its set of parents $\\mathrm{Pa}(t)$, children $\\mathrm{Ch}(t)$, ancestors $\\mathrm{Anc}(t)$, and leaf descendants $L(t)$. The set of ancestors $\\mathrm{Anc}(t)$ is defined as $\\{t\\} \\cup \\bigcup_{p \\in \\mathrm{Pa}(t)} \\mathrm{Anc}(p)$. The set of leaf descendants $L(t)$ is found by traversing child-edges from $t$ until leaf nodes are reached. The set of all leaf terms in the graph is $L = \\{8, 9, 10, 11, 12\\}$.\n\nThe base annotation counts $c(\\ell)$ are given for each leaf term $\\ell \\in L$: $c(8)=50$, $c(9)=70$, $c(10)=120$, $c(11)=90$, $c(12)=80$. The total annotation count is $C = \\sum_{\\ell \\in L} c(\\ell) = 50+70+120+90+80 = 410$.\n\nAccording to the True Path Rule, the count for any term $t$, $c(t)$, is the sum of the base counts of all its leaf descendants:\n$$c(t) = \\sum_{\\ell \\in L(t)} c(\\ell)$$\nUsing this rule, we compute the counts for all non-leaf terms:\n- $c(7) = c(11) = 90$\n- $c(6) = c(10) = 120$\n- $c(5) = c(9) = 70$\n- $c(4) = c(8) = 50$\n- $c(3) = c(6) + c(7) + c(12) = 120 + 90 + 80 = 290$\n- $c(2) = c(4) + c(5) = 50 + 70 = 120$\n- $c(1) = c(2) + c(3) = 120 + 290 = 410$\n\nThe probability of a term $t$ is $p(t) = c(t)/C$. The Information Content (IC), a measure of specificity, is then calculated using the natural logarithm:\n$$IC(t) = -\\ln p(t) = -\\ln\\left(\\frac{c(t)}{C}\\right)$$\nFor example, $IC(1) = -\\ln(410/410) = 0$, and $IC(4) = -\\ln(50/410) \\approx 2.1041$. These IC values are pre-computed for all $12$ terms.\n\n**2. Pairwise Semantic Similarity and Distance Calculation**\n\nFor each pair of terms $(a, b)$ in a given set $S$, we calculate their semantic similarity. The Resnik similarity is defined by the Information Content of their Most Informative Common Ancestor (MICA). The MICA is the term $t_{\\mathrm{MICA}}$ in the set of common ancestors $\\mathrm{Anc}(a) \\cap \\mathrm{Anc}(b)$ that has the highest IC value.\n$$t_{\\mathrm{MICA}}(a,b) = \\arg\\max_{t \\in \\mathrm{Anc}(a)\\cap \\mathrm{Anc}(b)} IC(t)$$\nThe provided DAG is a tree, where each node other than the root has a single parent. In a tree, the MICA is uniquely identified as the Lowest Common Ancestor (LCA), since IC values strictly decrease as one ascends the tree from the LCA towards the root. The Resnik similarity is:\n$$s_{\\mathrm{Resnik}}(a,b) = IC(t_{\\mathrm{MICA}}(a,b))$$\nTo facilitate comparison and clustering, this similarity is normalized:\n$$s(a,b) = \\frac{s_{\\mathrm{Resnik}}(a,b)}{\\max(IC(a), IC(b))}$$\nwith the convention $s(a,b)=0$ if the denominator is zero. Finally, a distance metric is defined as:\n$$d(a,b) = 1 - s(a,b)$$\nThese distances are computed for all pairs in the set $S$ for a given test case, forming a symmetric distance matrix.\n\n**3. Clustering**\n\nUsing the computed distance matrix for a set $S$, we construct a threshold graph. An undirected edge is created between any two distinct terms $a,b \\in S$ if their distance $d(a,b)$ is less than or equal to a given threshold $\\theta$.\nThe clusters are then defined as the connected components of this threshold graph. We can find these components using standard graph traversal algorithms like Breadth-First Search (BFS) or Depth-First Search (DFS).\n\n**4. Representative Selection**\n\nThe goal is to select a representative subset $R \\subseteq S$ that reduces redundancy while maintaining biological process coverage. The process is as follows:\n\nFirst, calculate the total leaf coverage of the input set $S$, $L(S) = \\bigcup_{t \\in S} L(t)$.\n\nThe selection proceeds in two phases:\n-   **Initial Selection**: For each cluster found in the previous step, exactly one representative term is chosen. The selected representative $t^*$ from a cluster $C_i$ is the one that maximizes the size of its leaf set intersection with the total leaf set of S: $t^* = \\arg\\max_{t \\in C_i} |L(t) \\cap L(S)|$. Ties are resolved by selecting the term with the larger $IC$, and further ties by the smaller term index. This yields an initial representative set $R$.\n-   **Greedy Addition**: We calculate the coverage fraction $\\gamma(R,S) = |L(R)| / |L(S)|$, where $L(R) = \\bigcup_{t \\in R} L(t)$. If $\\gamma(R,S)$ is less than the required coverage threshold $\\alpha$, we iteratively add more terms to $R$. In each step, we select a term $t' \\in S \\setminus R$ that provides the largest marginal gain in leaf coverage, i.e., maximizes $|L(t') \\setminus L(R)|$. Ties are again broken by larger $IC$, and then by smaller term index. This continues until $\\gamma(R,S) \\ge \\alpha$ or no more terms can be added to increase coverage.\n\nThe final set of representatives $R$ for each test case is then sorted in increasing order of term index to produce the final result. This entire procedure is applied to each test case provided in the problem statement.",
            "answer": "```python\nimport numpy as np\n\nclass GOSimilarityProcessor:\n    \"\"\"\n    Handles GO semantic similarity calculations, clustering, and representative selection.\n    \"\"\"\n    def __init__(self, parent_rels, leaf_counts):\n        self.num_terms = 12\n        self.terms = list(range(1, self.num_terms + 1))\n        self.leaf_counts_base = leaf_counts\n        self.leaves = set(self.leaf_counts_base.keys())\n\n        # Build graph structure\n        self.parents = {t: set() for t in self.terms}\n        self.children = {t: set() for t in self.terms}\n        for u, v in parent_rels:\n            self.parents[v].add(u)\n            self.children[u].add(v)\n\n        # Pre-compute graph properties and information content\n        self._memo_anc = {}\n        self.ancestors = {t: self._get_ancestors(t) for t in self.terms}\n\n        self._memo_ld = {}\n        self.leaf_descendants = {t: self._get_leaf_descendants(t) for t in self.terms}\n        \n        self.counts = self._compute_counts()\n        self.total_count = sum(self.leaf_counts_base.values())\n        self.ic = self._compute_ic()\n\n    def _get_ancestors(self, t):\n        if t in self._memo_anc:\n            return self._memo_anc[t]\n        \n        anc = {t}\n        for p in self.parents.get(t, []):\n            anc.update(self._get_ancestors(p))\n        self._memo_anc[t] = anc\n        return anc\n\n    def _get_leaf_descendants(self, t):\n        if t in self._memo_ld:\n            return self._memo_ld[t]\n        \n        if t in self.leaves:\n            self._memo_ld[t] = {t}\n            return {t}\n        \n        desc_leaves = set()\n        for c in self.children.get(t, []):\n            desc_leaves.update(self._get_leaf_descendants(c))\n        self._memo_ld[t] = desc_leaves\n        return desc_leaves\n\n    def _compute_counts(self):\n        counts = {t: 0 for t in self.terms}\n        for t in self.terms:\n            leaf_desc = self.leaf_descendants[t]\n            if leaf_desc:\n                counts[t] = sum(self.leaf_counts_base[l] for l in leaf_desc)\n        return counts\n\n    def _compute_ic(self):\n        ic = {t: 0.0 for t in self.terms}\n        for t in self.terms:\n            if self.counts[t]  0 and self.total_count  0:\n                p_t = self.counts[t] / self.total_count\n                ic[t] = -np.log(p_t)\n        return ic\n\n    def _get_mica(self, t1, t2):\n        common_ancestors = self.ancestors[t1]  self.ancestors[t2]\n        if not common_ancestors:\n            return None\n        \n        max_ic = -1.0\n        mica = None\n        # The problem statement implies a unique MICA, which is true for the given tree data.\n        # Iteration order over a set is not guaranteed, but for a unique max, it's fine.\n        for anc in common_ancestors:\n            if self.ic[anc]  max_ic:\n                max_ic = self.ic[anc]\n                mica = anc\n        return mica\n\n    def solve_case(self, S_list, theta, alpha):\n        # Use a sorted set of unique terms from the input list S\n        S = sorted(list(set(S_list)))\n\n        # 1. Compute Distance Matrix\n        dist_matrix = {t1: {t2: 1.0 for t2 in S} for t1 in S}\n        for i in range(len(S)):\n            for j in range(i, len(S)):\n                t1, t2 = S[i], S[j]\n                if t1 == t2:\n                    dist_matrix[t1][t1] = 0.0\n                    continue\n\n                mica = self._get_mica(t1, t2)\n                s_resnik = self.ic.get(mica, 0.0)\n                \n                max_ic_val = max(self.ic[t1], self.ic[t2])\n                s_norm = s_resnik / max_ic_val if max_ic_val  0 else 0.0\n                \n                dist = 1.0 - s_norm\n                dist_matrix[t1][t2] = dist\n                dist_matrix[t2][t1] = dist\n\n        # 2. Find Clusters via Connected Components of Threshold Graph\n        adj = {t: [] for t in S}\n        for i in range(len(S)):\n            for j in range(i + 1, len(S)):\n                t1, t2 = S[i], S[j]\n                if dist_matrix[t1][t2] = theta:\n                    adj[t1].append(t2)\n                    adj[t2].append(t1)\n\n        visited = set()\n        clusters = []\n        for term in S:\n            if term not in visited:\n                component = []\n                q = [term]\n                visited.add(term)\n                component.append(term)\n                head = 0\n                while head  len(q):\n                    u = q[head]\n                    head += 1\n                    for v in adj[u]:\n                        if v not in visited:\n                            visited.add(v)\n                            q.append(v)\n                            component.append(v)\n                clusters.append(component)\n\n        # 3. Representative Selection\n        L_S = set().union(*(self.leaf_descendants[t] for t in S))\n        \n        if not L_S: # Handle case where S covers no leaves\n            return []\n\n        # Phase 1: Initial Selection\n        R = set()\n        for cluster in clusters:\n            candidates = []\n            for t in cluster:\n                score = len(self.leaf_descendants[t]  L_S)\n                candidates.append((score, self.ic[t], t))\n            \n            # Tie-breaking: 1. score (desc), 2. IC (desc), 3. term index (asc)\n            candidates.sort(key=lambda x: (x[0], x[1], -x[2]), reverse=True)\n            if candidates:\n                best_rep = candidates[0][2]\n                R.add(best_rep)\n\n        # Phase 2: Greedy Addition\n        L_R = set().union(*(self.leaf_descendants[t] for t in R))\n\n        while len(L_R) / len(L_S)  alpha:\n            remaining_terms = [t for t in S if t not in R]\n            if not remaining_terms:\n                break\n            \n            candidates = []\n            for t in remaining_terms:\n                marginal_gain = len(self.leaf_descendants[t] - L_R)\n                candidates.append((marginal_gain, self.ic[t], t))\n            \n            # Tie-breaking: 1. gain (desc), 2. IC (desc), 3. term index (asc)\n            candidates.sort(key=lambda x: (x[0], x[1], -x[2]), reverse=True)\n            \n            if not candidates or candidates[0][0] == 0: # No more progress possible\n                break\n                \n            best_term_to_add = candidates[0][2]\n            R.add(best_term_to_add)\n            L_R.update(self.leaf_descendants[best_term_to_add])\n            \n        return sorted(list(R))\n\ndef solve():\n    # Define problem constants\n    parent_relationships = [\n        (1, 2), (1, 3), (2, 4), (2, 5), (3, 6), (3, 7), (3, 12),\n        (4, 8), (5, 9), (6, 10), (7, 11)\n    ]\n    base_leaf_counts = {8: 50, 9: 70, 10: 120, 11: 90, 12: 80}\n\n    # Initialize the processor once with all static data\n    processor = GOSimilarityProcessor(parent_relationships, base_leaf_counts)\n\n    # Define the test cases\n    test_cases = [\n        {'S': [3, 4, 5, 6, 7], 'theta': 0.8, 'alpha': 0.8},\n        {'S': [4, 6], 'theta': 0.5, 'alpha': 1.0},\n        {'S': [3, 6, 10], 'theta': 0.8, 'alpha': 0.67},\n        {'S': [4, 4, 5, 5], 'theta': 0.42, 'alpha': 1.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = processor.solve_case(case['S'], case['theta'], case['alpha'])\n        results.append(result)\n\n    # Format output as specified: [[a1,a2],[b1],[c1,c2,c3],[d1]] with no spaces\n    result_str = \",\".join([str(r).replace(\" \", \"\") for r in results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        }
    ]
}