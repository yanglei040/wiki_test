{
    "hands_on_practices": [
        {
            "introduction": "Principal Component Analysis is famously used to reduce the dimensionality of \"wide\" datasets, where the number of features $p$ vastly exceeds the number of samples $n$. This is the standard scenario in fields like transcriptomics. This first practice invites you to derive a fundamental limit on the effective dimensionality of such data from first principles. By exploring the rank of the covariance matrix, you will uncover why the number of informative principal components is constrained by the number of samples, not the number of features .",
            "id": "3302515",
            "problem": "You are analyzing a transcriptomics dataset in computational systems biology in which gene expression levels for $p$ genes are measured across $n$ biological samples, with $p \\gg n$. Let $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ denote the sample-by-gene data matrix whose $(i,j)$ entry is the expression level of gene $j$ in sample $i$. Let $\\mathbf{X}_{c}$ denote the column-centered matrix obtained by subtracting, for each gene $j$, its sample mean from $\\mathbf{X}$ so that each column of $\\mathbf{X}_{c}$ has mean $0$ across samples. Consider the feature-feature sample covariance matrix\n$$\n\\mathbf{S} \\;=\\; \\frac{1}{n-1}\\,\\mathbf{X}_{c}^{\\top}\\mathbf{X}_{c} \\;\\in\\; \\mathbb{R}^{p \\times p},\n$$\nthe object that is diagonalized in Principal Component Analysis (PCA), where Principal Component Analysis (PCA) is the orthogonal decomposition of feature variance along principal directions defined by the eigenvectors of $\\mathbf{S}$. Using only fundamental linear algebraic definitions and properties of centering and covariance, determine the maximum possible number of nonzero eigenvalues of $\\mathbf{S}$ in the regime $p \\gg n$. Justify the limitation by explicitly explaining the linear dependence structure among the columns of $\\mathbf{X}_{c}$ induced by centering across $n$ samples. Provide your final answer as a single closed-form expression in terms of $n$ and $p$ specialized to the regime $p \\gg n$. No rounding is required, and no units are involved.",
            "solution": "The objective is to determine the maximum possible number of nonzero eigenvalues of the feature-feature sample covariance matrix $\\mathbf{S} = \\frac{1}{n-1}\\mathbf{X}_{c}^{\\top}\\mathbf{X}_{c}$.\n\nA fundamental theorem in linear algebra states that the number of nonzero eigenvalues of any square matrix is equal to its rank. Therefore, the problem is equivalent to determining the maximum possible rank of the matrix $\\mathbf{S}$.\n\nThe matrix $\\mathbf{S}$ is defined as $\\mathbf{S} = \\frac{1}{n-1}\\mathbf{X}_{c}^{\\top}\\mathbf{X}_{c}$. The scalar factor $\\frac{1}{n-1}$ is nonzero (assuming $n > 1$, which is necessary to compute a covariance) and does not affect the rank of the matrix. Thus, we have:\n$$\n\\text{rank}(\\mathbf{S}) = \\text{rank}\\left(\\frac{1}{n-1}\\mathbf{X}_{c}^{\\top}\\mathbf{X}_{c}\\right) = \\text{rank}(\\mathbf{X}_{c}^{\\top}\\mathbf{X}_{c})\n$$\nAnother fundamental property of linear algebra states that for any matrix $\\mathbf{A}$, the rank of $\\mathbf{A}^{\\top}\\mathbf{A}$ is equal to the rank of $\\mathbf{A}$. That is, $\\text{rank}(\\mathbf{A}^{\\top}\\mathbf{A}) = \\text{rank}(\\mathbf{A})$. Applying this property to the matrix $\\mathbf{X}_{c} \\in \\mathbb{R}^{n \\times p}$, we get:\n$$\n\\text{rank}(\\mathbf{S}) = \\text{rank}(\\mathbf{X}_{c}^{\\top}\\mathbf{X}_{c}) = \\text{rank}(\\mathbf{X}_{c})\n$$\nThe problem is now reduced to finding the maximum possible rank of the column-centered data matrix $\\mathbf{X}_{c}$.\n\nThe matrix $\\mathbf{X}_{c}$ has dimensions $n \\times p$. The rank of any matrix cannot exceed the minimum of its number of rows and columns. Therefore, $\\text{rank}(\\mathbf{X}_{c}) \\le \\min(n, p)$. Given the specified regime $p \\gg n$, this implies $\\min(n, p) = n$, so we have an initial upper bound:\n$$\n\\text{rank}(\\mathbf{X}_{c}) \\le n\n$$\nHowever, we must account for the constraint imposed by the column-centering operation. The problem states that $\\mathbf{X}_{c}$ is obtained by subtracting the sample mean from each column of the original data matrix $\\mathbf{X}$. Let $\\mathbf{x}_{c,j}$ denote the $j$-th column of $\\mathbf{X}_{c}$, which is a vector in $\\mathbb{R}^n$. The centering condition means that the sum of the elements in each column vector is zero:\n$$\n\\sum_{i=1}^{n} (\\mathbf{X}_{c})_{ij} = 0 \\quad \\text{for all } j \\in \\{1, 2, \\dots, p\\}\n$$\nThis can be expressed using vector notation. Let $\\mathbf{1}_{n} \\in \\mathbb{R}^n$ be the column vector consisting of $n$ ones. The centering condition is equivalent to the statement that each column vector $\\mathbf{x}_{c,j}$ is orthogonal to $\\mathbf{1}_{n}$:\n$$\n\\mathbf{1}_{n}^{\\top}\\mathbf{x}_{c,j} = 0 \\quad \\text{for all } j \\in \\{1, 2, \\dots, p\\}\n$$\nThis constraint has a crucial geometric interpretation: all $p$ column vectors of the matrix $\\mathbf{X}_{c}$ must lie within the subspace of $\\mathbb{R}^n$ that is orthogonal to the vector $\\mathbf{1}_{n}$. This subspace, often denoted as $\\mathbf{1}_{n}^{\\perp}$, has a dimension of $n-1$.\n\nThe rank of a matrix is defined as the dimension of its column space. The column space of $\\mathbf{X}_{c}$ is the vector space spanned by its $p$ column vectors, $\\text{span}\\{\\mathbf{x}_{c,1}, \\mathbf{x}_{c,2}, \\dots, \\mathbf{x}_{c,p}\\}$. Since every one of these vectors is confined to an $(n-1)$-dimensional subspace, the dimension of their span cannot exceed $n-1$. Accordingly, the rank of $\\mathbf{X}_{c}$ is bounded from above by $n-1$:\n$$\n\\text{rank}(\\mathbf{X}_{c}) \\le n-1\n$$\nThis upper bound is achievable if the original data in $\\mathbf{X}$ is sufficiently general. For instance, if we take $n-1$ columns of $\\mathbf{X}$ to be linearly independent and not lying entirely in the span of $\\mathbf{1}_n$, their projection onto the subspace $\\mathbf{1}_{n}^{\\perp}$ will also be linearly independent. Therefore, the *maximum possible* rank of $\\mathbf{X}_{c}$ is exactly $n-1$.\n\nThe regime $p \\gg n$ is critical. Since $p > n-1$, we have a set of $p$ vectors that reside in an $(n-1)$-dimensional space. This guarantees that the set of column vectors of $\\mathbf{X}_{c}$ must be linearly dependent, and the rank of the matrix is limited by the dimension of this subspace ($n-1$) rather than the number of features ($p$).\n\nIn summary:\n1. The number of nonzero eigenvalues of $\\mathbf{S}$ is equal to $\\text{rank}(\\mathbf{S})$.\n2. $\\text{rank}(\\mathbf{S}) = \\text{rank}(\\mathbf{X}_{c})$.\n3. The centering operation constrains the columns of $\\mathbf{X}_{c}$ to an $(n-1)$-dimensional subspace of $\\mathbb{R}^n$.\n4. Therefore, the maximum possible rank of $\\mathbf{X}_{c}$ is $n-1$.\n\nThe maximum possible number of nonzero eigenvalues of $\\mathbf{S}$ in the regime $p \\gg n$ is $n-1$.",
            "answer": "$$\n\\boxed{n-1}\n$$"
        },
        {
            "introduction": "Standard PCA operates under the assumption of a Euclidean geometry, treating variance identically in all feature directions. However, in many biological systems, features (like genes) have known, differing noise levels or correlations that should be accounted for. This practice introduces Generalized PCA, which extends the classical framework by incorporating a custom metric via a Mahalanobis distance. You will derive the underlying optimization problem and see how it elegantly transforms into a generalized eigenvalue problem, allowing for a more nuanced dimensionality reduction that respects the data's inherent structure .",
            "id": "3302534",
            "problem": "In a study of a three-gene signaling module assayed by single-cell RNA sequencing (scRNA-seq), suppose the sample covariance of log-expression across cells for the genes is modeled by a symmetric positive definite matrix $S \\in \\mathbb{R}^{3 \\times 3}$, and that known, gene-specific measurement noise levels define a positive definite Mahalanobis metric $M \\in \\mathbb{R}^{3 \\times 3}$. The first generalized principal component under the Mahalanobis metric is defined as the direction $v \\in \\mathbb{R}^{3}$ that maximizes the sample variance $v^{\\top} S v$ subject to the normalization constraint $v^{\\top} M v = 1$.\n\nStarting only from the definitions of variance, covariance, and the Mahalanobis norm, derive the constrained maximization principle for generalized principal component analysis (PCA) under a Mahalanobis metric, and then reduce it to a standard eigenvalue problem via an appropriate whitening transformation. Conclude by computing the exact maximum generalized variance (i.e., the largest generalized principal component variance) for the specific matrices\n$$\nS \\;=\\; \\begin{pmatrix}\n3.2 & 1.2 & 0.0 \\\\\n1.2 & 2.5 & 0.0 \\\\\n0.0 & 0.0 & 1.6\n\\end{pmatrix},\n\\qquad\nM \\;=\\; \\begin{pmatrix}\n1.0 & 0.0 & 0.0 \\\\\n0.0 & 0.5 & 0.0 \\\\\n0.0 & 0.0 & 2.0\n\\end{pmatrix}.\n$$\nProvide your final answer as a single closed-form analytic expression for the maximum generalized variance. Do not include units, and do not round.",
            "solution": "The objective is to find a vector $v \\in \\mathbb{R}^3$ that maximizes the generalized variance, defined by the quadratic form $f(v) = v^{\\top} S v$, subject to the normalization constraint $g(v) = v^{\\top} M v - 1 = 0$. This is a constrained optimization problem that can be solved using the method of Lagrange multipliers.\n\n**1. Derivation of the Constrained Maximization Principle**\n\nWe define the Lagrangian function $\\mathcal{L}(v, \\lambda)$:\n$$\n\\mathcal{L}(v, \\lambda) = v^{\\top} S v - \\lambda(v^{\\top} M v - 1)\n$$\nTo find the extrema, we set the gradient with respect to $v$ to zero. Given that $S$ and $M$ are symmetric, $\\nabla_v(v^{\\top} A v) = 2Av$. Thus:\n$$\n\\nabla_v \\mathcal{L}(v, \\lambda) = 2Sv - \\lambda (2Mv) = 0 \\implies Sv = \\lambda Mv\n$$\nThis is a generalized eigenvalue problem. The vectors $v$ that satisfy this equation are the generalized eigenvectors, and the scalars $\\lambda$ are the generalized eigenvalues of the matrix pair $(S, M)$. To find the value of the objective function at these stationary points, we left-multiply by $v^{\\top}$:\n$$\nv^{\\top} S v = v^{\\top} (\\lambda Mv) = \\lambda (v^{\\top} M v)\n$$\nApplying the constraint $v^{\\top} M v = 1$, we find that the variance is equal to the generalized eigenvalue:\n$$\nv^{\\top} S v = \\lambda\n$$\nTherefore, to maximize the variance, we must find the largest generalized eigenvalue, $\\lambda_{\\text{max}}$.\n\n**2. Reduction to a Standard Eigenvalue Problem**\n\nSince $M$ is positive definite, it has an invertible Cholesky decomposition $M = LL^{\\top}$. Substituting this into the generalized eigenvalue equation gives $Sv = \\lambda LL^{\\top}v$. Pre-multiplying by $L^{-1}$ yields $L^{-1}Sv = \\lambda L^{\\top}v$. We define a \"whitened\" vector $u = L^{\\top}v$, which implies $v = L^{-\\top}u$. Substituting this gives:\n$$\nL^{-1}S(L^{-\\top}u) = \\lambda u \\implies (L^{-1}SL^{-\\top})u = \\lambda u\n$$\nThis is a standard eigenvalue problem for the symmetric matrix $S' = L^{-1}SL^{-\\top}$, whose eigenvalues $\\lambda$ are the same as the generalized eigenvalues of $(S,M)$.\n\n**3. Computation for the Specific Case**\n\nWe solve the characteristic equation $\\det(S - \\lambda M) = 0$.\n$$\nS - \\lambda M = \\begin{pmatrix}\n3.2 - 1.0\\lambda & 1.2 & 0.0 \\\\\n1.2 & 2.5 - 0.5\\lambda & 0.0 \\\\\n0.0 & 0.0 & 1.6 - 2.0\\lambda\n\\end{pmatrix}\n$$\nThe determinant is block-diagonal, so we have:\n$$\n\\det(S - \\lambda M) = (1.6 - 2.0\\lambda) \\left| \\begin{matrix} 3.2 - \\lambda & 1.2 \\\\ 1.2 & 2.5 - 0.5\\lambda \\end{matrix} \\right| = 0\n$$\nThis gives one eigenvalue from the linear factor: $1.6 - 2.0\\lambda = 0 \\implies \\lambda_1 = 0.8$.\nThe other two eigenvalues come from the quadratic factor:\n$$\n(3.2 - \\lambda)(2.5 - 0.5\\lambda) - (1.2)^2 = 0\n$$\n$$\n8.0 - 1.6\\lambda - 2.5\\lambda + 0.5\\lambda^2 - 1.44 = 0\n$$\n$$\n0.5\\lambda^2 - 4.1\\lambda + 6.56 = 0 \\implies \\lambda^2 - 8.2\\lambda + 13.12 = 0\n$$\nUsing the quadratic formula:\n$$\n\\lambda = \\frac{8.2 \\pm \\sqrt{(-8.2)^2 - 4(1)(13.12)}}{2} = \\frac{8.2 \\pm \\sqrt{67.24 - 52.48}}{2} = \\frac{8.2 \\pm \\sqrt{14.76}}{2}\n$$\nSince $\\sqrt{14.76} = \\sqrt{1476/100} = \\sqrt{36 \\times 41}/10 = 6\\sqrt{41}/10 = 3\\sqrt{41}/5$, we have:\n$$\n\\lambda = \\frac{8.2 \\pm 3\\sqrt{41}/5}{2} = 4.1 \\pm \\frac{3\\sqrt{41}}{10} = \\frac{41 \\pm 3\\sqrt{41}}{10}\n$$\nThe three generalized eigenvalues are $\\lambda_1 = 0.8$, $\\lambda_2 = \\frac{41 + 3\\sqrt{41}}{10}$, and $\\lambda_3 = \\frac{41 - 3\\sqrt{41}}{10}$. The largest of these is $\\lambda_2$. The maximum generalized variance is therefore $\\lambda_{\\text{max}} = \\frac{41 + 3\\sqrt{41}}{10}$.",
            "answer": "$$\\boxed{\\frac{41 + 3\\sqrt{41}}{10}}$$"
        },
        {
            "introduction": "The results of PCA, particularly the loading vectors, are often used to infer biological meaning. But how sensitive are these interpretations to single outlier samples, which are common in single-cell data? This advanced practice delves into the robustness of PCA by asking you to derive the influence function for the leading principal component loading. This powerful tool from robust statistics allows you to quantify the impact of an infinitesimal contamination and even construct a theoretical \"adversarial\" cell that maximally perturbs the analysis, providing a rigorous way to assess the stability of your findings .",
            "id": "3302561",
            "problem": "Consider a centered single-cell gene expression matrix representing $n$ cells across $p$ genes, modeled as realizations of a random vector $x \\in \\mathbb{R}^p$ with distribution $F$ satisfying zero mean, that is $E[x] = 0$. The sample covariance functional is defined by $S(F) = E[x x^\\top]$, and Principal Component Analysis (PCA) estimates loadings as the eigenvectors of the covariance. Let $\\lambda_1 > \\lambda_2 \\ge \\cdots \\ge \\lambda_p$ denote the distinct eigenvalues of a symmetric positive semi-definite matrix $S \\in \\mathbb{R}^{p \\times p}$, with corresponding orthonormal eigenvectors $v_1, v_2, \\ldots, v_p \\in \\mathbb{R}^p$. The leading PCA loading is $v_1$.\n\nRobustness to outlier cells may be quantified via the Influence Function (IF) from robust statistics for a functional $T(F)$, defined by the GÃ¢teaux derivative at $F$ under infinitesimal contamination at a point $a \\in \\mathbb{R}^p$, namely $F_\\varepsilon = (1 - \\varepsilon) F + \\varepsilon \\Delta_a$, as\n$$\n\\mathrm{IF}_T(a; F) = \\left. \\frac{d}{d\\varepsilon} T(F_\\varepsilon) \\right|_{\\varepsilon = 0}.\n$$\nIn this setting, we consider $T(F) = v_1(F)$, the leading PCA loading viewed as a functional of the covariance functional. Starting from the core definitions of the covariance, eigen-decomposition, and first principles of differentiating the eigenvalue equation, derive the first-order variation of $v_1$ under a symmetric perturbation to $S$ induced by an infinitesimal contamination at $a$, expressed entirely in terms of the spectral quantities of $S$ and the point $a$. Use this to obtain the corresponding Influence Function for the leading PCA loading.\n\nNext, define an adversarial cell $a^\\star \\in \\mathbb{R}^p$ subject to an energy constraint $\\|a^\\star\\|_2 \\le R$ that maximizes, to first order in $\\varepsilon$, the angular deviation of $v_1$ caused by contamination at $a^\\star$. Explicitly derive $a^\\star$ and the resulting first-order angular deviation in radians as a function of $R$, $\\varepsilon$, and the eigenvalues of $S$.\n\nYour program must implement the following algorithmic steps for each test case:\n- Center the provided cell-by-gene matrix $X \\in \\mathbb{R}^{n \\times p}$ by subtracting its column-wise mean to enforce $E[x] = 0$ in the empirical distribution.\n- Compute the sample covariance $S = \\frac{1}{n} X^\\top X$.\n- Compute the eigenvalues and eigenvectors of $S$, ordered so that $\\lambda_1$ is the largest eigenvalue, with corresponding orthonormal eigenvectors $v_1, \\ldots, v_p$.\n- Construct the adversarial cell $a^\\star$ that maximizes the first-order angular deviation of $v_1$ under the constraint $\\|a^\\star\\|_2 \\le R$, and compute the first-order predicted angular deviation $\\theta_{\\mathrm{IF}}$ in radians for contamination mass $\\varepsilon$.\n- Compute the exact leading eigenvector $w_1$ of the contaminated covariance $S_\\varepsilon = (1 - \\varepsilon) S + \\varepsilon a^\\star (a^\\star)^\\top$ and the exact angular deviation $\\theta_{\\mathrm{exact}} = \\arccos\\left( |v_1^\\top w_1| \\right)$ in radians.\n- Return the triple of floats $\\left[\\theta_{\\mathrm{exact}}, \\theta_{\\mathrm{IF}}, |\\theta_{\\mathrm{exact}} - \\theta_{\\mathrm{IF}}| \\right]$ in that order for each test case.\n\nAngle measures must be expressed in radians. The final program output must be a single line containing a list of the three test case results as comma-separated lists enclosed in square brackets, with no surrounding explanatory text, exactly in the format\n$$\n\\texttt{[[}\\theta_{\\mathrm{exact}}^{(1)}, \\theta_{\\mathrm{IF}}^{(1)}, |\\theta_{\\mathrm{exact}}^{(1)} - \\theta_{\\mathrm{IF}}^{(1)}| \\texttt{],[}\\theta_{\\mathrm{exact}}^{(2)}, \\theta_{\\mathrm{IF}}^{(2)}, |\\theta_{\\mathrm{exact}}^{(2)} - \\theta_{\\mathrm{IF}}^{(2)}| \\texttt{],[}\\theta_{\\mathrm{exact}}^{(3)}, \\theta_{\\mathrm{IF}}^{(3)}, |\\theta_{\\mathrm{exact}}^{(3)} - \\theta_{\\mathrm{IF}}^{(3)}| \\texttt{]]}.\n$$\n\nUse the following test suite, with each case specified by $(X, R, \\varepsilon)$:\n\n- Case A (well-separated leading eigenvalue):\n$$\nX^{(A)} =\n\\begin{bmatrix}\n2.0 & 1.0 & 0.2 & -0.1 \\\\\n-1.5 & 0.5 & -0.2 & 0.0 \\\\\n3.0 & -0.5 & 0.1 & 0.2 \\\\\n-2.2 & -0.6 & 0.0 & 0.1 \\\\\n1.8 & 0.9 & -0.1 & -0.2 \\\\\n-0.7 & -1.1 & 0.3 & 0.0 \\\\\n2.4 & 0.2 & 0.0 & -0.1 \\\\\n-1.9 & 0.8 & -0.3 & 0.0\n\\end{bmatrix}, \\quad R^{(A)} = 2.0, \\quad \\varepsilon^{(A)} = 0.01.\n$$\n\n- Case B (nearly equal top two eigenvalues):\n$$\nX^{(B)} =\n\\begin{bmatrix}\n1.5 & 1.4 & 0.2 & 0.0 \\\\\n-1.2 & -1.1 & -0.1 & 0.1 \\\\\n1.8 & -1.7 & 0.0 & -0.2 \\\\\n-1.6 & 1.5 & 0.1 & 0.2 \\\\\n1.3 & 1.2 & -0.2 & 0.0 \\\\\n-1.4 & -1.5 & 0.2 & 0.0 \\\\\n1.7 & -1.6 & 0.0 & 0.1 \\\\\n-1.3 & 1.4 & -0.2 & -0.1\n\\end{bmatrix}, \\quad R^{(B)} = 2.0, \\quad \\varepsilon^{(B)} = 0.05.\n$$\n\n- Case C (near-degenerate leading eigenvalues, stronger contamination):\n$$\nX^{(C)} =\n\\begin{bmatrix}\n1.00 & 1.02 & 0.05 & 0.00 \\\\\n-1.00 & -0.98 & -0.04 & 0.00 \\\\\n1.00 & -0.98 & 0.00 & -0.03 \\\\\n-1.00 & 1.02 & -0.01 & 0.02 \\\\\n0.95 & 0.93 & 0.02 & 0.01 \\\\\n-0.95 & -0.93 & -0.02 & -0.01 \\\\\n0.98 & -0.96 & 0.03 & -0.02 \\\\\n-0.98 & 0.96 & -0.03 & 0.02\n\\end{bmatrix}, \\quad R^{(C)} = 1.5, \\quad \\varepsilon^{(C)} = 0.20.\n$$\n\nScientific realism and consistency constraints:\n- Treat $X$ as centered log-transformed expression approximations; centering will be performed explicitly by subtracting column means.\n- Ensure that all computations adhere strictly to the mathematical statements above without empirical shortcuts.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $\\left[\\text{Case A}, \\text{Case B}, \\text{Case C}\\right]$, where each case is itself a list of three floats $\\left[\\theta_{\\mathrm{exact}}, \\theta_{\\mathrm{IF}}, |\\theta_{\\mathrm{exact}} - \\theta_{\\mathrm{IF}}|\\right]$.",
            "solution": "The core of this problem is to derive the influence function ($\\mathrm{IF}$) for the leading principal component (PC) loading vector, use it to find an optimal adversarial data point, and compare the first-order approximation of the resulting perturbation to the exact numerical result.\n\n### Part 1: Derivation of the Influence Function for the Leading PC Loading\n\nLet $S \\in \\mathbb{R}^{p \\times p}$ be a symmetric positive semi-definite covariance matrix with distinct leading eigenvalue $\\lambda_1 > \\lambda_2 \\ge \\cdots \\ge \\lambda_p \\ge 0$ and corresponding orthonormal eigenvectors $v_1, v_2, \\ldots, v_p$. The defining eigendecomposition for the $k$-th eigenpair is:\n$$\nS v_k = \\lambda_k v_k\n$$\n\nThe data distribution $F$ is contaminated at a point $a \\in \\mathbb{R}^p$, yielding a mixture distribution $F_\\varepsilon = (1 - \\varepsilon) F + \\varepsilon \\Delta_a$. The covariance matrix for this contaminated distribution, denoted $S_\\varepsilon$, is:\n$$\nS_\\varepsilon = E_{x \\sim F_\\varepsilon}[x x^\\top] = (1 - \\varepsilon) E_{x \\sim F}[x x^\\top] + \\varepsilon E_{x \\sim \\Delta_a}[x x^\\top] = (1 - \\varepsilon) S + \\varepsilon a a^\\top\n$$\nWe can express this as a perturbation of $S$:\n$$\nS_\\varepsilon = S + \\varepsilon(a a^\\top - S)\n$$\nThe perturbation to $S$ is $P = a a^\\top - S$. The influence function is the derivative of the leading eigenvector functional $v_1(F)$ with respect to $\\varepsilon$ at $\\varepsilon=0$. Let $v_1(\\varepsilon)$ and $\\lambda_1(\\varepsilon)$ be the perturbed leading eigenvector and eigenvalue of $S_\\varepsilon$. The eigenvalue equation is $S_\\varepsilon v_1(\\varepsilon) = \\lambda_1(\\varepsilon) v_1(\\varepsilon)$. Differentiating with respect to $\\varepsilon$ and evaluating at $\\varepsilon=0$ yields:\n$$\n\\left. \\frac{d S_\\varepsilon}{d \\varepsilon} \\right|_{\\varepsilon=0} v_1(0) + S_\\varepsilon(0) \\left. \\frac{d v_1(\\varepsilon)}{d \\varepsilon} \\right|_{\\varepsilon=0} = \\left. \\frac{d \\lambda_1(\\varepsilon)}{d \\varepsilon} \\right|_{\\varepsilon=0} v_1(0) + \\lambda_1(0) \\left. \\frac{d v_1(\\varepsilon)}{d \\varepsilon} \\right|_{\\varepsilon=0}\n$$\nLet $\\dot{v}_1 = \\left. \\frac{d v_1(\\varepsilon)}{d \\varepsilon} \\right|_{\\varepsilon=0} = \\mathrm{IF}_{v_1}(a; F)$, $\\dot{\\lambda}_1 = \\left. \\frac{d \\lambda_1(\\varepsilon)}{d \\varepsilon} \\right|_{\\varepsilon=0}$, and $\\dot{S} = \\left. \\frac{d S_\\varepsilon}{d \\varepsilon} \\right|_{\\varepsilon=0} = a a^\\top - S$. The equation becomes:\n$$\n\\dot{S} v_1 + S \\dot{v}_1 = \\dot{\\lambda}_1 v_1 + \\lambda_1 \\dot{v}_1\n$$\nUsing $S v_1 = \\lambda_1 v_1$ and rearranging gives:\n$$\n(S - \\lambda_1 I) \\dot{v}_1 = (\\dot{\\lambda}_1 v_1 - \\dot{S} v_1)\n$$\nThe derivative $\\dot{v}_1$ can be expressed in the basis of eigenvectors $\\{v_k\\}_{k=1}^p$ as $\\dot{v}_1 = \\sum_{k=1}^p c_k v_k$. Since $v_1(\\varepsilon)$ must remain a unit vector, $\\|v_1(\\varepsilon)\\|_2^2 = 1$, its derivative is orthogonal to $v_1(\\varepsilon)$. At $\\varepsilon=0$, this implies $\\dot{v}_1^\\top v_1 = 0$, so $c_1=0$. Thus, $\\dot{v}_1 = \\sum_{k=2}^p c_k v_k$.\n\nTo find the coefficients $c_j$ for $j \\ge 2$, we left-multiply the rearranged equation by $v_j^\\top$:\n$$\nv_j^\\top (S - \\lambda_1 I) \\dot{v}_1 = v_j^\\top (\\dot{\\lambda}_1 v_1 - \\dot{S} v_1)\n$$\nThe left-hand side (LHS) becomes:\n$$\nv_j^\\top (S - \\lambda_1 I) \\sum_{k=2}^p c_k v_k = v_j^\\top \\sum_{k=2}^p c_k (\\lambda_k - \\lambda_1) v_k = c_j (\\lambda_j - \\lambda_1)\n$$\nThe right-hand side (RHS) becomes:\n$$\n\\dot{\\lambda}_1 (v_j^\\top v_1) - v_j^\\top \\dot{S} v_1 = -v_j^\\top \\dot{S} v_1 = -v_j^\\top (a a^\\top - S) v_1\n$$\nSince $v_j^\\top S v_1 = v_j^\\top (\\lambda_1 v_1) = \\lambda_1 (v_j^\\top v_1) = 0$ for $j \\ge 2$, the RHS simplifies to:\n$$\n-v_j^\\top (a a^\\top) v_1 = -(v_j^\\top a)(a^\\top v_1)\n$$\nEquating LHS and RHS, and solving for $c_j$ (assuming $\\lambda_1 \\neq \\lambda_j$):\n$$\nc_j = \\frac{-(v_j^\\top a)(a^\\top v_1)}{\\lambda_j - \\lambda_1} = \\frac{(a^\\top v_1)(a^\\top v_j)}{\\lambda_1 - \\lambda_j}\n$$\nSubstituting back, we obtain the influence function for the leading PC loading:\n$$\n\\mathrm{IF}_{v_1}(a; F) = \\dot{v}_1 = \\sum_{k=2}^p \\frac{(a^\\top v_1)(a^\\top v_k)}{\\lambda_1 - \\lambda_k} v_k\n$$\n\n### Part 2: Derivation of the Adversarial Cell\n\nThe adversarial cell $a^\\star$ is defined as the point $a$ with energy constraint $\\|a\\|_2 \\le R$ that maximizes the first-order angular deviation of $v_1$. For a small perturbation, the new vector is $v_1(\\varepsilon) \\approx v_1 + \\varepsilon \\dot{v}_1$. Since $\\dot{v}_1$ is orthogonal to $v_1$, the angle of deviation $\\theta$ for small $\\varepsilon$ is approximately $\\tan(\\theta) = \\|\\varepsilon \\dot{v}_1\\| / \\|v_1\\| = \\varepsilon \\|\\dot{v}_1\\|$. Thus, we seek to maximize $\\|\\dot{v}_1\\| = \\|\\mathrm{IF}_{v_1}(a; F)\\|$.\n\nUsing the orthogonality of the eigenvectors $\\{v_k\\}$, the squared norm of the influence function is:\n$$\n\\|\\dot{v}_1\\|^2 = \\left\\| \\sum_{k=2}^p c_k v_k \\right\\|^2 = \\sum_{k=2}^p c_k^2 = (a^\\top v_1)^2 \\sum_{k=2}^p \\frac{(a^\\top v_k)^2}{(\\lambda_1 - \\lambda_k)^2}\n$$\nLet $\\alpha_k = a^\\top v_k$ be the projection of $a$ onto $v_k$. Then $a = \\sum_{k=1}^p \\alpha_k v_k$, and the constraint $\\|a\\|_2^2 \\le R^2$ becomes $\\sum_{k=1}^p \\alpha_k^2 \\le R^2$. We want to maximize:\n$$\nJ(\\alpha_1, \\ldots, \\alpha_p) = \\alpha_1^2 \\sum_{k=2}^p \\frac{\\alpha_k^2}{(\\lambda_1 - \\lambda_k)^2}\n$$\nThe maximization occurs on the boundary, so $\\sum_{k=1}^p \\alpha_k^2 = R^2$. The term $1/(\\lambda_1 - \\lambda_k)^2$ is largest for the eigenvalue $\\lambda_k$ closest to $\\lambda_1$, which is $\\lambda_2$. To maximize the sum, we should place all the \"energy\" of the non-$v_1$ components of $a$ onto the $v_2$ direction. This means we set $\\alpha_k = 0$ for all $k > 2$. The problem reduces to maximizing $\\alpha_1^2 \\frac{\\alpha_2^2}{(\\lambda_1 - \\lambda_2)^2}$ subject to $\\alpha_1^2 + \\alpha_2^2 = R^2$. This is equivalent to maximizing the product $\\alpha_1^2 \\alpha_2^2$ subject to their sum being constant. The maximum is achieved when $\\alpha_1^2 = \\alpha_2^2 = R^2/2$.\n\nThus, $|\\alpha_1| = R/\\sqrt{2}$ and $|\\alpha_2| = R/\\sqrt{2}$. The optimal adversarial cell $a^\\star$ is a linear combination of $v_1$ and $v_2$. We can choose the signs arbitrarily; for simplicity, we set:\n$$\na^\\star = \\frac{R}{\\sqrt{2}} v_1 + \\frac{R}{\\sqrt{2}} v_2\n$$\n\n### Part 3: First-Order Angular Deviation\n\nThe first-order angular deviation is $\\theta_{\\mathrm{IF}} = \\varepsilon \\|\\mathrm{IF}_{v_1}(a^\\star; F)\\|$. We calculate this norm:\n$$\n\\mathrm{IF}_{v_1}(a^\\star; F) = \\sum_{k=2}^p \\frac{((a^\\star)^\\top v_1)((a^\\star)^\\top v_k)}{\\lambda_1 - \\lambda_k} v_k\n$$\nWith our choice of $a^\\star$, we have $(a^\\star)^\\top v_1 = R/\\sqrt{2}$, $(a^\\star)^\\top v_2 = R/\\sqrt{2}$, and $(a^\\star)^\\top v_k = 0$ for $k > 2$. The sum has only one non-zero term (at $k=2$):\n$$\n\\mathrm{IF}_{v_1}(a^\\star; F) = \\frac{(R/\\sqrt{2})(R/\\sqrt{2})}{\\lambda_1 - \\lambda_2} v_2 = \\frac{R^2}{2(\\lambda_1 - \\lambda_2)} v_2\n$$\nThe norm is:\n$$\n\\|\\mathrm{IF}_{v_1}(a^\\star; F)\\| = \\left\\| \\frac{R^2}{2(\\lambda_1 - \\lambda_2)} v_2 \\right\\| = \\frac{R^2}{2(\\lambda_1 - \\lambda_2)}\n$$\nTherefore, the maximum first-order angular deviation is:\n$$\n\\theta_{\\mathrm{IF}} = \\varepsilon \\frac{R^2}{2(\\lambda_1 - \\lambda_2)}\n$$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the final result.\n    \"\"\"\n\n    def calculate_deviations(X, R, epsilon):\n        \"\"\"\n        Performs the analysis for a single test case.\n\n        Args:\n            X (np.ndarray): The n x p cell-by-gene matrix.\n            R (float): The energy constraint for the adversarial cell.\n            epsilon (float): The contamination mass.\n\n        Returns:\n            list: A list of three floats: [theta_exact, theta_IF, error].\n        \"\"\"\n        # Step 1: Center the provided cell-by-gene matrix X.\n        n, p = X.shape\n        X_centered = X - np.mean(X, axis=0)\n\n        # Step 2: Compute the sample covariance S.\n        # S = (1/n) * X_centered.T @ X_centered\n        S = np.cov(X_centered, rowvar=False, bias=True)\n\n        # Step 3: Compute the eigenvalues and eigenvectors of S, ordered descending.\n        # np.linalg.eigh returns eigenvalues in ascending order.\n        eigenvalues, eigenvectors = np.linalg.eigh(S)\n        \n        # Sort in descending order\n        desc_indices = np.argsort(eigenvalues)[::-1]\n        lambdas = eigenvalues[desc_indices]\n        vectors = eigenvectors[:, desc_indices]\n\n        # Extract leading spectral quantities\n        lambda1 = lambdas[0]\n        lambda2 = lambdas[1]\n        v1 = vectors[:, 0]\n        v2 = vectors[:, 1]\n        \n        # Guard against division by zero if eigenvalues are identical, though problem implies lambda1 > lambda2.\n        if np.isclose(lambda1, lambda2):\n            # In this degenerate case, the IF is theoretically infinite.\n            # We can return infinity or handle as an error. For this problem,\n            # this indicates a breakdown of the first-order approximation's premise.\n            # We will let the division proceed, resulting in np.inf as per standard float behavior.\n            pass\n\n        # Step 4: Construct the adversarial cell a_star and compute theta_IF.\n        a_star = (R / np.sqrt(2)) * (v1 + v2)\n        \n        # First-order predicted angular deviation from derived formula\n        theta_IF = epsilon * R**2 / (2 * (lambda1 - lambda2))\n\n        # Step 5: Compute the exact leading eigenvector w1 of the contaminated covariance S_epsilon.\n        S_epsilon = (1 - epsilon) * S + epsilon * np.outer(a_star, a_star)\n        \n        # Eigendecomposition of the contaminated matrix\n        eigvals_eps, eigvecs_eps = np.linalg.eigh(S_epsilon)\n        \n        # Sort in descending order\n        desc_indices_eps = np.argsort(eigvals_eps)[::-1]\n        w1 = eigvecs_eps[:, desc_indices_eps[0]]\n\n        # Compute the exact angular deviation theta_exact.\n        # The absolute value handles the arbitrary sign of eigenvectors.\n        # Clipping handles potential floating point inaccuracies where the dot product might be slightly > 1.\n        dot_product = np.clip(np.abs(np.dot(v1, w1)), -1.0, 1.0)\n        theta_exact = np.arccos(dot_product)\n        \n        # Step 6: Return the triple of floats.\n        error = np.abs(theta_exact - theta_IF)\n        \n        return [theta_exact, theta_IF, error]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([\n                [2.0, 1.0, 0.2, -0.1],\n                [-1.5, 0.5, -0.2, 0.0],\n                [3.0, -0.5, 0.1, 0.2],\n                [-2.2, -0.6, 0.0, 0.1],\n                [1.8, 0.9, -0.1, -0.2],\n                [-0.7, -1.1, 0.3, 0.0],\n                [2.4, 0.2, 0.0, -0.1],\n                [-1.9, 0.8, -0.3, 0.0]\n            ]), 2.0, 0.01  # Case A: R=2.0, epsilon=0.01\n        ),\n        (\n            np.array([\n                [1.5, 1.4, 0.2, 0.0],\n                [-1.2, -1.1, -0.1, 0.1],\n                [1.8, -1.7, 0.0, -0.2],\n                [-1.6, 1.5, 0.1, 0.2],\n                [1.3, 1.2, -0.2, 0.0],\n                [-1.4, -1.5, 0.2, 0.0],\n                [1.7, -1.6, 0.0, 0.1],\n                [-1.3, 1.4, -0.2, -0.1]\n            ]), 2.0, 0.05  # Case B: R=2.0, epsilon=0.05\n        ),\n        (\n            np.array([\n                [1.00, 1.02, 0.05, 0.00],\n                [-1.00, -0.98, -0.04, 0.00],\n                [1.00, -0.98, 0.00, -0.03],\n                [-1.00, 1.02, -0.01, 0.02],\n                [0.95, 0.93, 0.02, 0.01],\n                [-0.95, -0.93, -0.02, -0.01],\n                [0.98, -0.96, 0.03, -0.02],\n                [-0.98, 0.96, -0.03, 0.02]\n            ]), 1.5, 0.20 # Case C: R=1.5, epsilon=0.20\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        X, R, epsilon = case\n        result_tuple = calculate_deviations(X, R, epsilon)\n        results.append(result_tuple)\n\n    # Format the final output string exactly as specified.\n    # Create strings for each sublist without spaces, then join them.\n    formatted_results = []\n    for res in results:\n        # Using a list comprehension and join to avoid spaces from default list str()\n        s = f\"[{','.join(map(str, res))}]\"\n        formatted_results.append(s)\n    \n    final_string = f\"[{','.join(formatted_results)}]\"\n    print(final_string)\n\nsolve()\n```"
        }
    ]
}