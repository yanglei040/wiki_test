{
    "hands_on_practices": [
        {
            "introduction": "The first step in any sequencing analysis is converting the raw, analog signals from the sequencing instrument into discrete base calls (A, C, G, T) with associated quality scores. This exercise  challenges you to implement a base caller based on a probabilistic model of Illumina sequencing signals. You will use a Gaussian model for signal noise and apply Bayesian inference to determine the most likely base and the confidence in that call, which is fundamental to all downstream analyses.",
            "id": "3310871",
            "problem": "You are given a probabilistic model for sequencing-by-synthesis signals in a four-channel Illumina system, where each cycle emits an intensity vector in the channel space corresponding to the nucleotides Adenine (A), Cytosine (C), Guanine (G), and Thymine (T). At cycle index $i$, the observed intensity vector is denoted by $\\mathbf{y}_i \\in \\mathbb{R}^4$, the per-cycle scalar scale factor is $s_i \\in \\mathbb{R}_{\\ge 0}$, the background baseline vector is $\\boldsymbol{\\beta} \\in \\mathbb{R}^4$, and the channel-wise noise variances are collected into the diagonal covariance $\\operatorname{diag}(\\boldsymbol{\\sigma}^2)$ with $\\boldsymbol{\\sigma}^2 \\in \\mathbb{R}^4_{>0}$. The calibration (cross-talk) matrix $\\mathbf{C} \\in \\mathbb{R}^{4 \\times 4}$ maps a one-hot base vector to expected normalized channel means, such that the expected mean intensity for base $b \\in \\{A,C,G,T\\}$ is the column $\\boldsymbol{\\mu}_b = \\mathbf{C}_{:,b}$, and the generative signal model is given by the sum of scaled mean and baseline plus additive noise:\n$$\n\\mathbf{y}_i = s_i \\boldsymbol{\\mu}_b + \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}_i, \\quad \\boldsymbol{\\varepsilon}_i \\sim \\mathcal{N}\\!\\left(\\mathbf{0}, \\operatorname{diag}(\\boldsymbol{\\sigma}^2)\\right).\n$$\nAssume channel noises are independent and Gaussian, and that cycles are independent conditioned on the base at each cycle. The set of prior probabilities over bases is $\\mathbf{p} = [p(A),p(C),p(G),p(T)]$ with $p(A)+p(C)+p(G)+p(T)=1$ and $p(b)>0$ for each base $b$.\n\nYour tasks are to compute, for each cycle $i$:\n1. The maximum likelihood base call, defined as the base $\\hat{b}_i$ that maximizes the likelihood $p(\\mathbf{y}_i \\mid b)$ under the stated Gaussian model. In case of ties within a tolerance threshold $\\tau$, select the base with the smallest index in lexicographic order $A \\rightarrow 0$, $C \\rightarrow 1$, $G \\rightarrow 2$, $T \\rightarrow 3$.\n2. The Phred quality score $Q_i$ for the chosen base call, defined by the standard transformation from the posterior error probability,\n$$\nQ_i = -10 \\log_{10}\\!\\left(P_{\\text{err},i}\\right),\n$$\nwhere $P_{\\text{err},i} = 1 - P(\\hat{b}_i \\mid \\mathbf{y}_i)$, and $P(b \\mid \\mathbf{y}_i)$ is computed using Bayes' theorem with the provided prior $\\mathbf{p}$ and the Gaussian likelihood $p(\\mathbf{y}_i \\mid b)$ implied by the generative model above. Use numerically stable computations for probabilities, and when $P_{\\text{err},i}$ is numerically zero, treat it as $10^{-300}$ to avoid undefined logarithms. Express all quality scores as floats rounded to two decimal places.\n\nBase indexing is as follows: $A \\rightarrow 0$, $C \\rightarrow 1$, $G \\rightarrow 2$, $T \\rightarrow 3$. There are no physical units for intensities in this problem.\n\nImplement a complete program that, given the following test suite of parameter sets, produces the specified outputs. For each test case, output a pair of lists: the first list contains the base calls per cycle as integers in $\\{0,1,2,3\\}$, and the second list contains the corresponding $Q$-scores as floats rounded to two decimal places. Aggregate the results of all provided test cases into a single line as a comma-separated list enclosed in square brackets, where each element of the top-level list corresponds to one test case and is itself a two-element list as described.\n\nUse the tie tolerance $\\tau = 10^{-12}$ for all test cases.\n\nTest Suite:\n- Test case $1$ (happy path, low noise):\n  - Calibration matrix:\n    $$\n    \\mathbf{C}^{(1)} = \\begin{bmatrix}\n    1.0 & 0.1 & 0.1 & 0.1 \\\\\n    0.1 & 1.0 & 0.1 & 0.1 \\\\\n    0.1 & 0.1 & 1.0 & 0.1 \\\\\n    0.1 & 0.1 & 0.1 & 1.0\n    \\end{bmatrix}.\n    $$\n  - Variances: $\\boldsymbol{\\sigma}^{2\\,(1)} = [0.05, 0.05, 0.05, 0.05]$.\n  - Baseline: $\\boldsymbol{\\beta}^{(1)} = [0.0, 0.0, 0.0, 0.0]$.\n  - Scale per cycle: $\\mathbf{s}^{(1)} = [1.0, 1.0, 1.0, 1.0, 1.0]$.\n  - Priors: $\\mathbf{p}^{(1)} = [0.25, 0.25, 0.25, 0.25]$.\n  - Observations:\n    $$\n    \\mathbf{Y}^{(1)} = \\begin{bmatrix}\n    1.20 & 0.10 & 0.10 & 0.10 \\\\\n    0.10 & 1.10 & 0.10 & 0.10 \\\\\n    0.20 & 0.10 & 1.30 & 0.10 \\\\\n    0.10 & 0.10 & 0.10 & 1.25 \\\\\n    0.30 & 0.20 & 0.90 & 0.20\n    \\end{bmatrix}.\n    $$\n- Test case $2$ (high noise, ambiguous signals):\n  - Calibration matrix:\n    $$\n    \\mathbf{C}^{(2)} = \\begin{bmatrix}\n    1.0 & 0.1 & 0.1 & 0.1 \\\\\n    0.1 & 1.0 & 0.1 & 0.1 \\\\\n    0.1 & 0.1 & 1.0 & 0.1 \\\\\n    0.1 & 0.1 & 0.1 & 1.0\n    \\end{bmatrix}.\n    $$\n  - Variances: $\\boldsymbol{\\sigma}^{2\\,(2)} = [0.5, 0.5, 0.5, 0.5]$.\n  - Baseline: $\\boldsymbol{\\beta}^{(2)} = [0.0, 0.0, 0.0, 0.0]$.\n  - Scale per cycle: $\\mathbf{s}^{(2)} = [1.0, 1.0, 1.0]$.\n  - Priors: $\\mathbf{p}^{(2)} = [0.25, 0.25, 0.25, 0.25]$.\n  - Observations:\n    $$\n    \\mathbf{Y}^{(2)} = \\begin{bmatrix}\n    0.50 & 0.50 & 0.50 & 0.50 \\\\\n    0.60 & 0.50 & 0.50 & 0.40 \\\\\n    0.50 & 0.40 & 0.60 & 0.50\n    \\end{bmatrix}.\n    $$\n- Test case $3$ (non-uniform priors, heterogeneous calibration and variances):\n  - Calibration matrix:\n    $$\n    \\mathbf{C}^{(3)} = \\begin{bmatrix}\n    0.9 & 0.2 & 0.2 & 0.2 \\\\\n    0.2 & 0.9 & 0.3 & 0.2 \\\\\n    0.2 & 0.3 & 0.9 & 0.2 \\\\\n    0.2 & 0.2 & 0.2 & 0.9\n    \\end{bmatrix}.\n    $$\n  - Variances: $\\boldsymbol{\\sigma}^{2\\,(3)} = [0.1, 0.2, 0.1, 0.2]$.\n  - Baseline: $\\boldsymbol{\\beta}^{(3)} = [0.05, 0.05, 0.05, 0.05]$.\n  - Scale per cycle: $\\mathbf{s}^{(3)} = [1.0, 0.8, 1.2]$.\n  - Priors: $\\mathbf{p}^{(3)} = [0.10, 0.40, 0.40, 0.10]$.\n  - Observations:\n    $$\n    \\mathbf{Y}^{(3)} = \\begin{bmatrix}\n    0.95 & 0.13 & 0.10 & 0.10 \\\\\n    0.20 & 0.75 & 0.30 & 0.20 \\\\\n    0.20 & 0.25 & 1.10 & 0.20\n    \\end{bmatrix}.\n    $$\n- Test case $4$ (boundary case: zero intensities, low noise):\n  - Calibration matrix:\n    $$\n    \\mathbf{C}^{(4)} = \\begin{bmatrix}\n    1.0 & 0.1 & 0.1 & 0.1 \\\\\n    0.1 & 1.0 & 0.1 & 0.1 \\\\\n    0.1 & 0.1 & 1.0 & 0.1 \\\\\n    0.1 & 0.1 & 0.1 & 1.0\n    \\end{bmatrix}.\n    $$\n  - Variances: $\\boldsymbol{\\sigma}^{2\\,(4)} = [0.05, 0.05, 0.05, 0.05]$.\n  - Baseline: $\\boldsymbol{\\beta}^{(4)} = [0.0, 0.0, 0.0, 0.0]$.\n  - Scale per cycle: $\\mathbf{s}^{(4)} = [1.0, 1.0]$.\n  - Priors: $\\mathbf{p}^{(4)} = [0.25, 0.25, 0.25, 0.25]$.\n  - Observations:\n    $$\n    \\mathbf{Y}^{(4)} = \\begin{bmatrix}\n    0.00 & 0.00 & 0.00 & 0.00 \\\\\n    0.00 & 0.00 & 0.00 & 0.00\n    \\end{bmatrix}.\n    $$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list: the first list contains the integer base calls per cycle using the mapping $A \\rightarrow 0$, $C \\rightarrow 1$, $G \\rightarrow 2$, $T \\rightarrow 3$, and the second list contains the corresponding Phred quality scores per cycle rounded to two decimal places. For example, the output format must be\n$$\n[\\,[\\,[\\text{calls}^{(1)}],\\,[\\text{qscores}^{(1)}]\\,],\\,\\ldots,\\,[\\,[\\text{calls}^{(4)}],\\,[\\text{qscores}^{(4)}]\\,]\\,].\n$$",
            "solution": "The problem requires the implementation of a statistical base-calling algorithm for a simplified four-channel sequencing-by-synthesis system. We are asked to determine the most likely DNA base for each sequencing cycle and to quantify the confidence in that call using a Phred quality score. This will be accomplished by applying principles of probability theory, specifically maximum likelihood estimation and Bayesian inference, to the provided generative signal model.\n\nThe process for each cycle $i$ involves two main computations:\n1.  **Maximum Likelihood (ML) Base Calling**: To find the base $\\hat{b}_i$ that is most likely to have generated the observed intensity vector $\\mathbf{y}_i$.\n2.  **Phred Quality Score ($Q_i$) Calculation**: To compute a quality score for the call $\\hat{b}_i$ based on its posterior error probability.\n\nWe will analyze each step based on fundamental principles.\n\n**Principle 1: The Gaussian Signal Model and Likelihood**\n\nThe generative model for the observed intensity vector $\\mathbf{y}_i \\in \\mathbb{R}^4$ at cycle $i$ for a given base $b \\in \\{A,C,G,T\\}$ is:\n$$\n\\mathbf{y}_i = s_i \\boldsymbol{\\mu}_b + \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}_i, \\quad \\boldsymbol{\\varepsilon}_i \\sim \\mathcal{N}\\!\\left(\\mathbf{0}, \\operatorname{diag}(\\boldsymbol{\\sigma}^2)\\right)\n$$\nwhere $\\boldsymbol{\\mu}_b$ is the column of the calibration matrix $\\mathbf{C}$ corresponding to base $b$. This equation states that the observed signal is the sum of a scaled ideal signal $s_i \\boldsymbol{\\mu}_b$, a constant background baseline $\\boldsymbol{\\beta}$, and additive Gaussian noise $\\boldsymbol{\\varepsilon}_i$.\n\nThe model implies that the conditional probability of observing $\\mathbf{y}_i$ given base $b$ follows a multivariate normal distribution:\n$$\n\\mathbf{y}_i \\mid b \\sim \\mathcal{N}\\!\\left(s_i \\boldsymbol{\\mu}_b + \\boldsymbol{\\beta}, \\operatorname{diag}(\\boldsymbol{\\sigma}^2)\\right)\n$$\nThe probability density function (PDF), which serves as the likelihood $p(\\mathbf{y}_i \\mid b)$, is:\n$$\np(\\mathbf{y}_i \\mid b) = \\frac{1}{\\sqrt{(2\\pi)^4 \\det(\\operatorname{diag}(\\boldsymbol{\\sigma}^2))}} \\exp\\left(-\\frac{1}{2} (\\mathbf{y}_i - (s_i \\boldsymbol{\\mu}_b + \\boldsymbol{\\beta}))^T (\\operatorname{diag}(\\boldsymbol{\\sigma}^2))^{-1} (\\mathbf{y}_i - (s_i \\boldsymbol{\\mu}_b + \\boldsymbol{\\beta}))\\right)\n$$\nSince the noise covariance matrix $\\boldsymbol{\\Sigma} = \\operatorname{diag}(\\boldsymbol{\\sigma}^2)$ is diagonal, the channels are independent. The exponent's argument, known as the squared Mahalanobis distance, simplifies to a weighted sum of squared errors (WSSE):\n$$\n\\chi^2(\\mathbf{y}_i, b) = \\sum_{j=0}^{3} \\frac{(y_{i,j} - (s_i C_{j,b} + \\beta_j))^2}{\\sigma_j^2}\n$$\nwhere the base index $b$ maps to the corresponding column of $\\mathbf{C}$, and $j \\in \\{0,1,2,3\\}$ indexes the four channels.\n\n**Principle 2: Maximum Likelihood Base Calling**\n\nThe ML base call $\\hat{b}_i$ is the base that maximizes the likelihood function:\n$$\n\\hat{b}_i = \\arg\\max_{b \\in \\{A,C,G,T\\}} p(\\mathbf{y}_i \\mid b)\n$$\nMaximizing the likelihood $p(\\mathbf{y}_i \\mid b)$ is equivalent to maximizing its natural logarithm, $\\ln p(\\mathbf{y}_i \\mid b)$, which is computationally more stable.\n$$\n\\ln p(\\mathbf{y}_i \\mid b) = -\\frac{1}{2} \\sum_{j=0}^{3} \\ln(2\\pi \\sigma_j^2) - \\frac{1}{2} \\sum_{j=0}^{3} \\frac{(y_{i,j} - (s_i C_{j,b} + \\beta_j))^2}{\\sigma_j^2}\n$$\nSince the first term is constant with respect to the base $b$, maximizing the log-likelihood is equivalent to minimizing the WSSE term, $\\chi^2(\\mathbf{y}_i, b)$.\n\nThe algorithm for ML base calling is as follows:\n1.  For each cycle $i$ and for each of the four bases $b$, calculate the log-likelihood $\\ln p(\\mathbf{y}_i \\mid b)$.\n2.  Convert the log-likelihoods to linear-scale likelihoods: $L_b = \\exp(\\ln p(\\mathbf{y}_i \\mid b))$.\n3.  Find the maximum likelihood, $L_{max} = \\max_{b} L_b$.\n4.  Identify the set of candidate bases $S_{cand} = \\{ b \\mid L_{max} - L_b < \\tau \\}$, where $\\tau=10^{-12}$ is the given tolerance.\n5.  The final base call $\\hat{b}_i$ is the base in $S_{cand}$ with the smallest lexicographical index ($A \\rightarrow 0, C \\rightarrow 1, G \\rightarrow 2, T \\rightarrow 3$).\n\n**Principle 3: Bayesian Inference and Phred Quality Score**\n\nThe Phred quality score $Q_i$ is derived from the posterior probability of error, $P_{\\text{err},i}$. To calculate this, we first need the posterior probability of each base $b$ given the observation $\\mathbf{y}_i$, denoted $P(b \\mid \\mathbf{y}_i)$. Using Bayes' theorem:\n$$\nP(b \\mid \\mathbf{y}_i) = \\frac{p(\\mathbf{y}_i \\mid b) p(b)}{p(\\mathbf{y}_i)} = \\frac{p(\\mathbf{y}_i \\mid b) p(b)}{\\sum_{b'} p(\\mathbf{y}_i \\mid b') p(b')}\n$$\nwhere $p(b)$ are the given prior probabilities for each base, and the denominator is the marginal likelihood (evidence), a normalization constant ensuring the posterior probabilities sum to $1$.\n\nThe posterior error probability for the call $\\hat{b}_i$ is the probability that the true base was any other base:\n$$\nP_{\\text{err},i} = P(\\text{base} \\neq \\hat{b}_i \\mid \\mathbf{y}_i) = 1 - P(\\hat{b}_i \\mid \\mathbf{y}_i) = \\sum_{b \\neq \\hat{b}_i} P(b \\mid \\mathbf{y}_i)\n$$\nThe Phred score is then defined as:\n$$\nQ_i = -10 \\log_{10}(P_{\\text{err},i})\n$$\n\n**Principle 4: Numerically Stable Computation**\n\nDirect computation of likelihoods and posterior probabilities can lead to numerical underflow, as these values can be extremely small. It is standard practice to perform calculations in log-space. Let $\\lambda_b = \\ln(p(\\mathbf{y}_i \\mid b) p(b)) = \\ln p(\\mathbf{y}_i \\mid b) + \\ln p(b)$ be the log-joint probability. The log of the posterior is:\n$$\n\\ln P(b \\mid \\mathbf{y}_i) = \\lambda_b - \\ln\\left(\\sum_{b'} \\exp(\\lambda_{b'})\\right)\n$$\nThe summation term is computed stably using the log-sum-exp trick. Let $\\lambda_{max} = \\max_{b'} \\lambda_{b'}$. Then:\n$$\n\\ln\\left(\\sum_{b'} \\exp(\\lambda_{b'})\\right) = \\lambda_{max} + \\ln\\left(\\sum_{b'} \\exp(\\lambda_{b'} - \\lambda_{max})\\right)\n$$\nThis avoids overflow in the exponentiation and underflow in the sum. Once the log-posteriors $\\ln P(b \\mid \\mathbf{y}_i)$ are computed, we can find $P(\\hat{b}_i \\mid \\mathbf{y}_i) = \\exp(\\ln P(\\hat{b}_i \\mid \\mathbf{y}_i))$ and subsequently $P_{\\text{err},i}$. The problem specifies that if $P_{\\text{err},i}$ is numerically zero, it should be treated as $10^{-300}$ to prevent an undefined logarithm when calculating $Q_i$.\n\nBy combining these principles, we can construct an algorithm to process the provided test suite and generate the required base calls and quality scores.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the DNA sequencing base calling problem for all test cases.\n    \"\"\"\n    \n    # Base mapping and problem constants\n    base_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    bases = ['A', 'C', 'G', 'T']\n    num_bases = 4\n    tau = 1e-12\n    p_err_floor = 1e-300\n\n    # Test Suite\n    test_cases = [\n        # Test case 1\n        {\n            \"C\": np.array([\n                [1.0, 0.1, 0.1, 0.1],\n                [0.1, 1.0, 0.1, 0.1],\n                [0.1, 0.1, 1.0, 0.1],\n                [0.1, 0.1, 0.1, 1.0]\n            ]),\n            \"sigma_sq\": np.array([0.05, 0.05, 0.05, 0.05]),\n            \"beta\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"s\": np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"p\": np.array([0.25, 0.25, 0.25, 0.25]),\n            \"Y\": np.array([\n                [1.20, 0.10, 0.10, 0.10],\n                [0.10, 1.10, 0.10, 0.10],\n                [0.20, 0.10, 1.30, 0.10],\n                [0.10, 0.10, 0.10, 1.25],\n                [0.30, 0.20, 0.90, 0.20]\n            ])\n        },\n        # Test case 2\n        {\n            \"C\": np.array([\n                [1.0, 0.1, 0.1, 0.1],\n                [0.1, 1.0, 0.1, 0.1],\n                [0.1, 0.1, 1.0, 0.1],\n                [0.1, 0.1, 0.1, 1.0]\n            ]),\n            \"sigma_sq\": np.array([0.5, 0.5, 0.5, 0.5]),\n            \"beta\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"s\": np.array([1.0, 1.0, 1.0]),\n            \"p\": np.array([0.25, 0.25, 0.25, 0.25]),\n            \"Y\": np.array([\n                [0.50, 0.50, 0.50, 0.50],\n                [0.60, 0.50, 0.50, 0.40],\n                [0.50, 0.40, 0.60, 0.50]\n            ])\n        },\n        # Test case 3\n        {\n            \"C\": np.array([\n                [0.9, 0.2, 0.2, 0.2],\n                [0.2, 0.9, 0.3, 0.2],\n                [0.2, 0.3, 0.9, 0.2],\n                [0.2, 0.2, 0.2, 0.9]\n            ]),\n            \"sigma_sq\": np.array([0.1, 0.2, 0.1, 0.2]),\n            \"beta\": np.array([0.05, 0.05, 0.05, 0.05]),\n            \"s\": np.array([1.0, 0.8, 1.2]),\n            \"p\": np.array([0.10, 0.40, 0.40, 0.10]),\n            \"Y\": np.array([\n                [0.95, 0.13, 0.10, 0.10],\n                [0.20, 0.75, 0.30, 0.20],\n                [0.20, 0.25, 1.10, 0.20]\n            ])\n        },\n        # Test case 4\n        {\n            \"C\": np.array([\n                [1.0, 0.1, 0.1, 0.1],\n                [0.1, 1.0, 0.1, 0.1],\n                [0.1, 0.1, 1.0, 0.1],\n                [0.1, 0.1, 0.1, 1.0]\n            ]),\n            \"sigma_sq\": np.array([0.05, 0.05, 0.05, 0.05]),\n            \"beta\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"s\": np.array([1.0, 1.0]),\n            \"p\": np.array([0.25, 0.25, 0.25, 0.25]),\n            \"Y\": np.array([\n                [0.00, 0.00, 0.00, 0.00],\n                [0.00, 0.00, 0.00, 0.00]\n            ])\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        C, sigma_sq, beta, s_vec, p, Y = case[\"C\"], case[\"sigma_sq\"], case[\"beta\"], case[\"s\"], case[\"p\"], case[\"Y\"]\n        \n        calls_per_case = []\n        qscores_per_case = []\n        \n        num_cycles = Y.shape[0]\n        \n        log_priors = np.log(p)\n        log_likelihood_const = -0.5 * np.sum(np.log(2 * np.pi * sigma_sq))\n\n        for i in range(num_cycles):\n            y_i = Y[i, :]\n            s_i = s_vec[i]\n            \n            log_likelihoods = np.zeros(num_bases)\n            \n            for b_idx in range(num_bases):\n                mu_b = C[:, b_idx]\n                expected_mean = s_i * mu_b + beta\n                \n                # Calculate WSSE (chi-squared)\n                wsse = np.sum(((y_i - expected_mean)**2) / sigma_sq)\n                \n                # Log likelihood\n                log_likelihoods[b_idx] = log_likelihood_const - 0.5 * wsse\n            \n            # --- Maximum Likelihood Base Call ---\n            likelihoods = np.exp(log_likelihoods)\n            max_likelihood = np.max(likelihoods)\n            tied_indices = np.where(max_likelihood - likelihoods  tau)[0]\n            ml_base_call = np.min(tied_indices)\n            calls_per_case.append(ml_base_call)\n            \n            # --- Phred Quality Score Calculation ---\n            # Log joint probabilities\n            log_joint = log_likelihoods + log_priors\n            \n            # Log-sum-exp for normalization\n            log_joint_max = np.max(log_joint)\n            log_marginal = log_joint_max + np.log(np.sum(np.exp(log_joint - log_joint_max)))\n            \n            # Log posteriors\n            log_posteriors = log_joint - log_marginal\n            posteriors = np.exp(log_posteriors)\n            \n            p_correct = posteriors[ml_base_call]\n            p_err = 1.0 - p_correct\n            \n            # Handle numerical zero\n            if p_err = p_err_floor:\n                p_err = p_err_floor\n            \n            q_score = -10 * np.log10(p_err)\n            qscores_per_case.append(round(q_score, 2))\n            \n        all_results.append([calls_per_case, qscores_per_case])\n\n    # Format the final output string\n    result_strings = []\n    for calls, qscores in all_results:\n        calls_str = f\"[{','.join(map(str, calls))}]\"\n        qscores_str = f\"[{','.join([f'{q:.2f}' for q in qscores])}]\"\n        result_strings.append(f\"[{calls_str},{qscores_str}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "After generating reads, a fundamental question is how completely they cover the genome. This theoretical exercise  guides you through a first-principles derivation of the classic Lander-Waterman model for random shotgun sequencing. By modeling read placement as a Poisson process, you will develop an intuition for the relationship between sequencing depth and genome completeness, a critical concept for experimental design.",
            "id": "3310820",
            "problem": "Consider a linear reference genome of length $L$ measured in base pairs (bp). A whole-genome random shotgun sequencing experiment produces short reads of fixed length $\\ell$ whose start positions are modeled as a homogeneous Poisson point process along the genome with rate $\\lambda$ (reads per bp). The mean per-base coverage is defined as $c = \\lambda \\ell$. Assume reads are independently and uniformly placed and that coverage events are independent across positions given the Poisson model.\n\nStarting solely from these definitions and the properties of the homogeneous Poisson process and its renewal structure, perform the following derivations:\n1. Derive the expected fraction of bases that remain uncovered as a function of the coverage $c$.\n2. Derive the probability distribution of uncovered gap lengths (contiguous stretches of bp that receive zero coverage). State clearly whether your result is conditional on the gap being positive and, if so, give the unconditional mixture structure as well.\n3. Using extreme value theory for independent and identically distributed samples, derive an approximation for the expected maximum uncovered gap length along the genome in terms of $L$, $\\ell$, $c$, and universal constants.\n\nThen, evaluate your approximation numerically for the parameters $L = 1.0 \\times 10^{7}$ bp, $\\ell = 100$ bp, and $c = 5.0$. Round your final numerical answer to $4$ significant figures and express it in base pairs (bp). Your final answer must be a single real number.",
            "solution": "The problem asks for derivations related to uncovered regions in a genome sequenced by a random shotgun method, based on the Lander-Waterman model. The model assumes read start positions follow a homogeneous Poisson point process. We will address each part of the problem sequentially.\n\nLet $L$ be the length of the genome, $\\ell$ be the fixed read length, and $\\lambda$ be the rate of the Poisson process for read starts. The mean coverage is given by $c = \\lambda \\ell$.\n\n**1. Expected fraction of uncovered bases**\n\nTo find the expected fraction of the genome that is uncovered, we can calculate the probability that an arbitrary single base pair is uncovered and then invoke the homogeneity of the process.\n\nConsider a single base at an arbitrary position $x$ in the genome. This base is covered if at least one read overlaps it. A read of length $\\ell$ starting at position $s$ covers the interval of bases $[s, s+\\ell-1]$ (using integer coordinates). For a continuous coordinate system, a read starting at $s$ covers the interval $[s, s+\\ell)$.\n\nA read starting at $s$ covers position $x$ if $s \\le x  s+\\ell$, which is equivalent to $x-\\ell  s \\le x$. This defines an interval of length $\\ell$ for the starting positions of reads that can cover position $x$.\n\nThe number of read starts, $K$, in any interval of length $I$ is a random variable following a Poisson distribution with mean $\\lambda I$. Therefore, the number of reads covering position $x$, let's call it $K_x$, is a Poisson-distributed random variable with mean $\\lambda \\times \\ell = c$.\n$$P(K_x = k) = \\frac{c^k e^{-c}}{k!}$$\nA base at position $x$ is uncovered if and only if the number of reads covering it is zero, i.e., $K_x = 0$. The probability of this event is:\n$$P(\\text{base at } x \\text{ is uncovered}) = P(K_x = 0) = \\frac{c^0 e^{-c}}{0!} = e^{-c}$$\nSince the process is homogeneous (uniform placement of reads), this probability is the same for all positions $x$ (ignoring end effects for a large genome $L \\gg \\ell$). By linearity of expectation, the expected number of uncovered bases is $L \\times P(\\text{uncovered}) = L e^{-c}$.\nThe expected fraction of bases that remain uncovered, $F_u$, is the expected number of uncovered bases divided by the total length $L$:\n$$F_u = \\frac{L e^{-c}}{L} = e^{-c}$$\n\n**2. Probability distribution of uncovered gap lengths**\n\nAn uncovered gap is a contiguous stretch of bases with zero coverage. We can model the length of these gaps by considering the spacing between the start positions of consecutive reads. The locations of read starts, $\\{s_i\\}$, form a Poisson point process of rate $\\lambda$. The distances between consecutive events in such a process, $X_i = s_{i+1} - s_i$, are independent and identically distributed (i.i.d.) exponential random variables with rate $\\lambda$. The probability density function (PDF) is $f_X(x) = \\lambda e^{-\\lambda x}$ for $x \\ge 0$.\n\nA read starting at $s_i$ covers the interval $[s_i, s_i+\\ell)$. If the next read starts at $s_{i+1}$, it covers $[s_{i+1}, s_{i+1}+\\ell)$. A gap in coverage occurs between these two reads if and only if the end of the first read's coverage, $s_i+\\ell$, is less than the start of the next read, $s_{i+1}$. This condition is $s_i+\\ell  s_{i+1}$, or $s_{i+1}-s_i  \\ell$. In terms of the inter-read spacing $X = s_{i+1}-s_i$, a gap is formed if $X  \\ell$.\n\nThe length of the uncovered continuous segment is $G = s_{i+1} - (s_i+\\ell) = X - \\ell$. This length is positive only if $X  \\ell$.\n\nFirst, we find the probability that a positive-length gap is formed between any two consecutive reads. This is:\n$$P(G  0) = P(X  \\ell) = \\int_{\\ell}^{\\infty} \\lambda e^{-\\lambda x} dx = [-e^{-\\lambda x}]_{\\ell}^{\\infty} = e^{-\\lambda \\ell} = e^{-c}$$\nThe probability that no gap is formed (i.e., the reads overlap) is $P(G \\le 0) = P(X \\le \\ell) = 1 - e^{-c}$.\n\nNow, we derive the distribution of the gap length $G$, conditional on it being positive ($G0$). This is the distribution of $X-\\ell$ given $X\\ell$. The cumulative distribution function (CDF) for $g \\ge 0$ is:\n$$P(G \\le g | G  0) = P(X - \\ell \\le g | X  \\ell) = \\frac{P(\\ell  X \\le \\ell + g)}{P(X  \\ell)}$$\nThe numerator is calculated as:\n$$P(\\ell  X \\le \\ell + g) = F_X(\\ell+g) - F_X(\\ell) = (1 - e^{-\\lambda(\\ell+g)}) - (1 - e^{-\\lambda\\ell}) = e^{-\\lambda\\ell} - e^{-\\lambda\\ell}e^{-\\lambda g} = e^{-c}(1 - e^{-\\lambda g})$$\nThe denominator is $P(X  \\ell) = e^{-c}$.\nTherefore, the conditional CDF of the gap length is:\n$$F_{G|G0}(g) = \\frac{e^{-c}(1 - e^{-\\lambda g})}{e^{-c}} = 1 - e^{-\\lambda g}$$\nThis is the CDF of an exponential distribution with rate $\\lambda$. Thus, conditional on being positive, uncovered gap lengths are exponentially distributed with rate $\\lambda = c/\\ell$. The conditional PDF is $f_{G|G0}(g) = \\lambda e^{-\\lambda g}$ for $g \\ge 0$.\n\nThe unconditional distribution of gap lengths $G$ is a mixture model. It has a discrete mass at $G=0$ (representing no gap) and a continuous part for $G0$:\n- With probability $P(G \\le 0) = 1 - e^{-c}$, the gap length is zero.\n- With probability $P(G  0) = e^{-c}$, the gap length is positive and follows an exponential distribution with rate $\\lambda$.\n\nThe unconditional \"PDF\" can be written using the Dirac delta function $\\delta(g)$:\n$$f_G(g) = (1-e^{-c})\\delta(g) + e^{-c} \\lambda e^{-\\lambda g} \\cdot I(g0)$$\nwhere $I(g0)$ is the indicator function for positive $g$.\n\n**3. Expected maximum uncovered gap length**\n\nWe need to find the expected maximum among all uncovered gaps across the genome. The number of opportunities for a gap to form is the number of inter-read intervals, which is approximately the number of reads, $N_{reads}$. The expected number of reads is $E[N_{reads}] = \\lambda L$. The number of positive-length gaps, $n$, is the number of reads multiplied by the probability of a gap forming, $e^{-c}$. So, the expected number of gaps is:\n$$n = E[N_{gaps}] = (\\lambda L) e^{-c}$$\nThe lengths of these $n$ gaps, $G_1, G_2, \\ldots, G_n$, are i.i.d. samples from an exponential distribution with rate $\\lambda$. We want to find the expected value of the maximum of these samples, $G_{max} = \\max(G_1, \\ldots, G_n)$.\n\nThe exact expected value of the maximum of $n$ i.i.d. $\\text{Exp}(\\lambda)$ variables is given by:\n$$E[G_{max}] = \\frac{1}{\\lambda} H_n = \\frac{1}{\\lambda} \\sum_{k=1}^{n} \\frac{1}{k}$$\nwhere $H_n$ is the $n$-th harmonic number. For large $n$, the harmonic number can be approximated by $H_n \\approx \\ln(n) + \\gamma$, where $\\gamma \\approx 0.5772$ is the Euler-Mascheroni constant. This approximation arises from extreme value theory, where the centralized and scaled maximum converges to a Gumbel distribution.\n\nUsing this approximation, we have:\n$$E[G_{max}] \\approx \\frac{1}{\\lambda}(\\ln(n) + \\gamma)$$\nSubstituting $n = \\lambda L e^{-c}$ and $\\lambda = c/\\ell$:\n$$E[G_{max}] \\approx \\frac{\\ell}{c} \\left( \\ln(\\lambda L e^{-c}) + \\gamma \\right)$$\nWe can expand the logarithm:\n$$E[G_{max}] \\approx \\frac{\\ell}{c} \\left( \\ln(\\lambda) + \\ln(L) - c + \\gamma \\right)$$\nSubstituting $\\lambda=c/\\ell$ again:\n$$E[G_{max}] \\approx \\frac{\\ell}{c} \\left( \\ln\\left(\\frac{c}{\\ell}\\right) + \\ln(L) - c + \\gamma \\right) = \\frac{\\ell}{c} \\left( \\ln(c) - \\ln(\\ell) + \\ln(L) - c + \\gamma \\right)$$\n$$E[G_{max}] \\approx \\frac{\\ell}{c} \\left( \\ln\\left(\\frac{L}{\\ell}\\right) + \\ln(c) - c + \\gamma \\right)$$\nThis expression provides the approximation for the expected maximum uncovered gap length.\n\n**Numerical Evaluation**\n\nWe are given the parameters:\n- $L = 1.0 \\times 10^{7}$ bp\n- $\\ell = 100$ bp\n- $c = 5.0$\n- We use the constant $\\gamma \\approx 0.57721566$\n\nFirst, let's compute the terms inside the parenthesis:\n- $\\frac{L}{\\ell} = \\frac{1.0 \\times 10^{7}}{100} = 1.0 \\times 10^{5}$\n- $\\ln\\left(\\frac{L}{\\ell}\\right) = \\ln(10^5) = 5 \\ln(10) \\approx 5 \\times 2.302585 = 11.512925$\n- $\\ln(c) = \\ln(5.0) \\approx 1.609438$\n- The value of $c$ is $5.0$.\n\nNow, we substitute these values into the derived formula for $E[G_{max}]$:\n$$E[G_{max}] \\approx \\frac{100}{5.0} \\left( 11.512925 + 1.609438 - 5.0 + 0.577216 \\right)$$\n$$E[G_{max}] \\approx 20 \\left( 13.122363 - 5.0 + 0.577216 \\right)$$\n$$E[G_{max}] \\approx 20 \\left( 8.122363 + 0.577216 \\right)$$\n$$E[G_{max}] \\approx 20 \\left( 8.699579 \\right)$$\n$$E[G_{max}] \\approx 173.99158$$\nThe problem asks to round the final answer to $4$ significant figures.\n$$E[G_{max}] \\approx 174.0 \\text{ bp}$$\nThis is the expected length of the largest contiguous region of the genome that receives no sequence coverage.",
            "answer": "$$\\boxed{174.0}$$"
        },
        {
            "introduction": "A primary task in genomics is determining the origin of each sequencing read within a reference genome, a process known as read mapping. This exercise  has you implement a widely-used and highly efficient seed-and-extend mapping algorithm powered by the Ferragina-Manzini (FM) index. This practice provides hands-on experience with the core data structures that make large-scale genome alignment computationally tractable.",
            "id": "3310867",
            "problem": "You are given a reference deoxyribonucleic acid (DNA) string and are asked to implement a principled seed-and-extend candidate mapping enumerator using the Ferragina–Manzini (FM) index. The task is to compute, for each provided read, all candidate start positions in the reference at which the read can align with at most $k$ mismatches, subject to a seed filtration defined by an exact-seed match constraint.\n\nFoundational base and definitions to use:\n- DNA is modeled as a string over the alphabet $\\Sigma = \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$.\n- The Central Dogma of Molecular Biology establishes that DNA sequences encode genetic information; at the computational level, read mapping is formulated as searching substrings within a reference genome.\n- The FM index is derived from the Burrows–Wheeler Transform (BWT) and allows efficient exact substring search via backward search. Construct the FM index on the string $S\\$$ where $S$ is the reference and $\\$$ is a sentinel character lexicographically smaller than any character in $\\Sigma$.\n- Let $n = |S\\$|$ denote the length of the text after appending the sentinel. The FM index comprises:\n  1. The BWT string $B$, defined by $B[i] = S\\$\\big[(\\mathrm{SA}[i] - 1) \\bmod n\\big]$, where $\\mathrm{SA}$ is the suffix array of $S\\$$.\n  2. The array $C(c)$ giving the total number of characters in $S\\$$ that are lexicographically smaller than $c$.\n  3. The occurrence function $\\mathrm{Occ}(c, i)$ returning the number of occurrences of character $c$ in the prefix $B[0:i)$, with $0 \\le i \\le n$.\n- Backward search is performed using the last-first (LF) mapping. For a pattern $P$ and an interval $[l, r)$ of suffix array ranks, updating with character $c$ yields a new interval\n  $$l' = C(c) + \\mathrm{Occ}(c, l), \\quad r' = C(c) + \\mathrm{Occ}(c, r),$$\n  iterated from the last character of $P$ to the first. If at any step $l' \\ge r'$, the pattern does not occur. Otherwise, the exact occurrences of $P$ are given by the suffix array positions $\\mathrm{SA}[l:r)$.\n\nSeed-and-extend filtration:\n- Given a read $R$ of length $L$, a seed length $m$, and an integer $f \\ge 1$, form disjoint seeds by taking substrings $R[o_j : o_j + m]$ at offsets $o_j = jm$ for all integers $j$ such that $o_j + m \\le L$. Let $t = \\left\\lfloor \\frac{L}{m} \\right\\rfloor$ denote the number of such seeds.\n- A candidate start position $x$ in $S$ passes the filtration if at least $f$ of these seeds have exact matches in $S$ consistent with alignment at $x$. Formally, for a seed at offset $o$, any exact match position $p$ of that seed in $S$ implies a candidate alignment start $x = p - o$. Count how many seeds support a given $x$, and retain $x$ only if the count is at least $f$.\n- After filtration, perform extension: compute the number of mismatches between $R$ and $S[x : x + L]$ and retain $x$ if this number is at most $k$. Valid starts must satisfy $0 \\le x \\le |S| - L$.\n\nPrincipled guarantee to use for parameter selection:\n- For allowing up to $k$ mismatches, a classical pigeonhole principle argument ensures that if the read is partitioned into $k + 1$ disjoint segments, at least one segment must be mismatch-free. This motivates taking $m \\approx \\left\\lfloor \\frac{L}{k + 1} \\right\\rfloor$ and a filtration requirement $f = 1$ for completeness. Stronger filtration criteria with $f  1$ are valid but may exclude true mappings when mismatches exist.\n\nAlgorithmic requirements:\n- Construct the FM index for the provided reference $S$.\n- For each read and parameter set $(L, k, m, f)$, compute the set of candidate start positions in $S$ that satisfy the filtration and mismatch constraint, using only exact FM-index seed search plus extension by direct comparison.\n- All intervals and indexing must be self-consistent. Use zero-based indexing for positions in $S$. The sentinel index and out-of-range candidates must be discarded.\n\nTest suite:\n- Use the reference $S = \\text{\"ACGTACGTACGTACGTACGTACGT\"}$, with $|S| = 24$.\n- For each test case, the program should produce the sorted list of valid candidate starts in ascending order.\n- Test cases:\n  1. Case A (happy path): $R_1 = \\text{\"ACGTACGTACG\"}$, $L = 11$, $k = 1$, $m = 5$, $f = 1$.\n  2. Case B (boundary $k=0$): $R_2 = \\text{\"GTACGT\"}$, $L = 6$, $k = 0$, $m = 3$, $f = 2$.\n  3. Case C (boundary near ends): $R_3 = \\text{\"ACGTACG\"}$, $L = 7$, $k = 2$, $m = 3$, $f = 1$.\n  4. Case D (strict filtration edge): $R_4 = \\text{\"AAGTACGTA\"}$, $L = 9$, $k = 1$, $m = 4$, $f = 2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\text{[result1,result2,result3,result4]}$), where each $\\text{result}$ is itself a list of integers denoting the candidate start positions for the corresponding test case.\n- The lists must be sorted in ascending order, and the overall output must be exactly one line with no additional text.",
            "solution": "The task is to implement a seed-and-extend read mapping algorithm that enumerates all candidate start positions for a given DNA read within a reference sequence. The solution must adhere to specified algorithmic principles, employing the Ferragina–Manzini (FM) index for efficient seed searching. The process involves three main stages: FM-index construction, seed-based filtration, and extension-based verification.\n\n### 1. FM-Index Construction\n\nThe FM-index is a compressed full-text index that allows for efficient counting and locating of arbitrary patterns in a text. Its construction is a prerequisite for the mapping algorithm. We begin with the reference DNA string $S$.\n\n_Step 1: Text Preparation_\nFirst, a sentinel character, $\\$$, which is lexicographically smaller than any character in the DNA alphabet $\\Sigma = \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$, is appended to the reference string $S$. This creates the text $T = S\\$$ of length $n = |S| + 1$. The sentinel ensures that every suffix of $T$ is unique and that the suffix array corresponds to a permutation of indices from $0$ to $n-1$.\n\n_Step 2: Suffix Array (SA)_\nThe suffix array, $\\mathrm{SA}$, of $T$ is an array of integers of length $n$. $\\mathrm{SA}[i]$ stores the starting position of the $i$-th lexicographically smallest suffix of $T$. For a small text like the one provided, the SA can be constructed by generating all suffixes, pairing them with their starting indices, and sorting these pairs.\n\n_Step 3: Burrows–Wheeler Transform (BWT)_\nThe BWT generates a permutation of the characters of $T$, denoted as the string $B$. It is defined by the relation $B[i] = T[(\\mathrm{SA}[i] - 1) \\bmod n]$ for $i \\in [0, n-1]$. Conceptually, $B$ is the last column of a matrix whose rows are all cyclic shifts of $T$ sorted lexicographically. This string $B$ has the crucial property of tending to group identical characters together, making it highly compressible and suitable for efficient queries.\n\n_Step 4: C-Table and Occurrence Function_\nTo enable efficient searching, two auxiliary data structures are built:\n1.  **C-Table**: The array $C(c)$ stores the total count of characters in $T$ that are lexicographically smaller than character $c$. This table allows us to instantly find the starting-rank block in the suffix array for all suffixes beginning with a specific character $c$.\n2.  **Occurrence Function ($\\mathrm{Occ}$)**: The function $\\mathrm{Occ}(c, i)$ returns the number of times character $c$ appears in the prefix of the BWT string, $B[0:i)$. For computational efficiency, this is pre-calculated and stored in a $2$D-array, where one dimension represents the characters of the alphabet and the other represents the positions in $B$.\n\n### 2. Exact Seed Search via Backward Search\n\nWith the FM-index constructed, we can perform highly efficient exact string matching using an algorithm known as backward search. This algorithm finds the suffix array interval $[l, r)$ corresponding to all suffixes that start with a given pattern $P$.\n\nThe search proceeds by iterating through the characters of the pattern $P$ from right to left. Starting with the full SA interval $[l, r) = [0, n)$, each character $c$ from $P$ updates the interval according to the last-first (LF) mapping property:\n$$l_{\\text{new}} = C(c) + \\mathrm{Occ}(c, l_{\\text{old}})$$\n$$r_{\\text{new}} = C(c) + \\mathrm{Occ}(c, r_{\\text{old}})$$\nIf at any point $l_{\\text{new}} \\ge r_{\\text{new}}$, the pattern $P$ does not exist in the text $T$. If the loop completes, the final interval $[l, r)$ identifies the range of suffixes, $\\mathrm{SA}[l \\dots r-1]$, that are prefixed by $P$. The values $\\mathrm{SA}[i]$ for $i \\in [l, r)$ are the starting positions of the exact matches of $P$ in $T$.\n\n### 3. Seed-and-Extend Filtration and Verification\n\nThis strategy uses short, exact matches (seeds) to rapidly identify a small set of promising candidate alignment locations, which are then verified in a more costly extension step.\n\n_Step 1: Seeding_\nGiven a read $R$ of length $L$, a seed length $m$, and a filtration threshold $f$, the read is partitioned into $t = \\lfloor L/m \\rfloor$ disjoint seeds. The $j$-th seed (using $0$-based indexing) is the substring $R[o_j : o_j + m]$, where the offset is $o_j = jm$ for $j \\in [0, t-1]$.\n\n_Step 2: Filtration_\nFor each of the $t$ seeds, we use the backward search algorithm on the FM-index to find all its exact match positions $\\{p_0, p_1, \\ldots\\}$ in the reference string $S$. Each match position $p$ for a seed at offset $o_j$ in the read implies a candidate start position for the entire read, calculated as $x = p - o_j$.\nA counter is maintained for each potential start position $x$. We iterate through all seeds and all their matches, incrementing the counter for the corresponding $x$ each time it is implied. After processing all seeds, we apply the filtration criterion: a candidate start position $x$ is retained only if its counter is at least $f$. This means at least $f$ distinct seeds from the read support an alignment starting at $x$.\n\n_Step 3: Extension and Verification_\nThe final stage verifies the filtered candidates. For each candidate start position $x$ that passes filtration, we perform two checks:\n1.  **Boundary Check**: The alignment must lie entirely within the reference $S$. This requires $0 \\le x \\le |S| - L$. Candidates outside this range are discarded.\n2.  **Mismatch Count**: The number of mismatches between the read $R$ and the corresponding reference substring $S[x : x + L]$ is computed. If this count is less than or equal to the permissible maximum, $k$, the position $x$ is accepted as a valid mapping location.\n\nThe final output for each read is the sorted list of all such validated start positions. This principled combination of compressed indexing and heuristic filtration allows for an efficient yet sensitive read mapping process.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import Counter\n\nclass FMIndex:\n    \"\"\"\n    An implementation of the Ferragina–Manzini (FM) index.\n    \"\"\"\n    def __init__(self, text, alphabet):\n        \"\"\"\n        Constructs the FM index for a given text.\n        Args:\n            text (str): The input string, ending with a sentinel '$'.\n            alphabet (list): A list of characters in the alphabet, sorted lexicographically.\n        \"\"\"\n        self.text = text\n        self.alphabet = alphabet\n        self.n = len(text)\n        self.char_map = {c: i for i, c in enumerate(self.alphabet)}\n\n        # 1. Suffix Array (SA) construction\n        suffixes = sorted([(self.text[i:], i) for i in range(self.n)])\n        self.sa = np.array([s[1] for s in suffixes], dtype=np.int32)\n\n        # 2. Burrows–Wheeler Transform (BWT) string B\n        bwt_chars = []\n        for i in range(self.n):\n            bwt_chars.append(self.text[(self.sa[i] - 1) % self.n])\n        self.bwt = \"\".join(bwt_chars)\n\n        # 3. C-Table (counts of chars lexicographically smaller than c)\n        self.c_table = {}\n        counts = Counter(self.text)\n        total = 0\n        for char in self.alphabet:\n            self.c_table[char] = total\n            total += counts.get(char, 0)\n        \n        # 4. Occurrence (Occ) table\n        self.occ = np.zeros((len(self.alphabet), self.n + 1), dtype=np.int32)\n        for i in range(self.n):\n            self.occ[:, i + 1] = self.occ[:, i]\n            if self.bwt[i] in self.char_map:\n                char_idx = self.char_map[self.bwt[i]]\n                self.occ[char_idx, i + 1] += 1\n\n    def _get_occ(self, char, index):\n        \"\"\" Helper to query the Occ table. \"\"\"\n        return self.occ[self.char_map[char], index]\n\n    def search(self, pattern):\n        \"\"\"\n        Performs backward search to find exact matches of a pattern.\n        Args:\n            pattern (str): The pattern to search for.\n        Returns:\n            list: A sorted list of starting positions of the pattern in the original text.\n        \"\"\"\n        if not pattern:\n            return []\n        \n        l, r = 0, self.n\n        for char in reversed(pattern):\n            if char not in self.char_map:\n                return []\n            \n            l = self.c_table[char] + self._get_occ(char, l)\n            r = self.c_table[char] + self._get_occ(char, r)\n            \n            if l >= r:\n                return []\n        \n        return sorted([self.sa[i] for i in range(l, r)])\n\ndef find_candidates(fm_index, S, R, k, m, f):\n    \"\"\"\n    Implements the seed-and-extend mapping algorithm.\n    Args:\n        fm_index (FMIndex): The pre-computed FM index of the reference.\n        S (str): The reference DNA string.\n        R (str): The read DNA string.\n        k (int): Maximum allowed mismatches.\n        m (int): Seed length.\n        f (int): Minimum number of seeds required to support a candidate.\n    Returns:\n        list: A sorted list of valid start positions.\n    \"\"\"\n    L = len(R)\n    s_len = len(S)\n    \n    candidate_supports = Counter()\n    num_seeds = L // m\n    \n    for j in range(num_seeds):\n        offset = j * m\n        seed = R[offset : offset + m]\n        \n        match_positions = fm_index.search(seed)\n        \n        for p in match_positions:\n            candidate_start = p - offset\n            candidate_supports[candidate_start] += 1\n            \n    filtered_starts = []\n    for start_pos, count in candidate_supports.items():\n        if count >= f:\n            filtered_starts.append(start_pos)\n            \n    valid_starts = []\n    for x in sorted(filtered_starts):\n        if 0 = x = s_len - L:\n            mismatches = 0\n            ref_substring = S[x : x + L]\n            for i in range(L):\n                if R[i] != ref_substring[i]:\n                    mismatches += 1\n            if mismatches = k:\n                valid_starts.append(x)\n                \n    return valid_starts\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    S = \"ACGTACGTACGTACGTACGTACGT\"\n    alphabet = ['$', 'A', 'C', 'G', 'T']\n    \n    text_with_sentinel = S + '$'\n    fm_index = FMIndex(text_with_sentinel, alphabet)\n\n    test_cases = [\n        # (R, L, k, m, f) - L is provided but len(R) is used as it's equivalent\n        (\"ACGTACGTACG\", 11, 1, 5, 1), # Case A\n        (\"GTACGT\", 6, 0, 3, 2),       # Case B\n        (\"ACGTACG\", 7, 2, 3, 1),       # Case C\n        (\"AAGTACGTA\", 9, 1, 4, 2)     # Case D\n    ]\n\n    results = []\n    for R, L, k, m, f in test_cases:\n        result = find_candidates(fm_index, S, R, k, m, f)\n        results.append(result)\n\n    # Format the output string to be exactly as required\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}