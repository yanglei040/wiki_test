{
    "hands_on_practices": [
        {
            "introduction": "Adjacency lists are a fundamental data structure for representing the sparse networks commonly found in biology, offering significant memory and computational advantages over dense matrices. This practice solidifies your ability to work directly with this representation to compute essential topological properties, such as node in-degrees, out-degrees, and the prevalence of reciprocal feedback motifs . Mastering these algorithmic primitives is a crucial first step in characterizing the structure and function of any network model.",
            "id": "3332690",
            "problem": "Consider a directed gene-regulatory network modeled as a graph with node set $V$ and directed edge set $E$. Each node represents a gene, and a directed edge from node $i$ to node $j$ indicates that gene $i$ transcriptionally regulates gene $j$. The graph is represented by an adjacency list, where for each node $i$, the adjacency list $A[i]$ contains the set of out-neighbors $N_{\\mathrm{out}}(i)$, i.e., nodes $j$ such that $(i,j) \\in E$. For computational correctness, duplicated edges in $A[i]$ are to be ignored (treated as a single edge), and self-loops $(i,i)$ are permitted.\n\nDefinitions:\n- The out-degree of node $i$ is $d_{\\mathrm{out}}(i) = |N_{\\mathrm{out}}(i)|$.\n- The in-neighbor set of node $i$ is $N_{\\mathrm{in}}(i) = \\{j \\in V \\mid (j,i) \\in E\\}$, and the in-degree is $d_{\\mathrm{in}}(i) = |N_{\\mathrm{in}}(i)|$.\n- The reciprocal neighbor overlap size for node $i$ is $s(i) = |N_{\\mathrm{out}}(i) \\cap N_{\\mathrm{in}}(i)|$, which counts how many nodes both regulate $i$ and are regulated by $i$.\n- The total number of directed edges is $m = |E|$.\n- The maximum degree is $\\Delta_{\\max} = \\max_{i \\in V} \\{\\max(d_{\\mathrm{out}}(i), d_{\\mathrm{in}}(i))\\}$.\n\nTask:\n- Design and implement an algorithm that, given an adjacency list representation $A$ of a directed graph on $n$ nodes $V = \\{0,1,\\dots,n-1\\}$, computes for all nodes $i \\in V$ the in-degree $d_{\\mathrm{in}}(i)$, out-degree $d_{\\mathrm{out}}(i)$, and reciprocal neighbor overlap size $s(i)$ as defined above.\n- The algorithm must operate using only adjacency lists, not adjacency matrices, for its core computations.\n- Analyze the time complexity of your algorithm in terms of the total number of edges $m$ and the maximum degree $\\Delta_{\\max}$, starting from fundamental definitions of adjacency lists and degrees.\n\nTo ensure universal applicability and testability, use the following test suite of graphs. Each test case specifies the number of nodes $n$ and an adjacency list $A$ with $n$ entries. For each node index $i$ with no outgoing edges, include an empty list. All node indices are zero-based.\n\nTest suite:\n- Test case $1$ (general case with mixed reciprocal regulation and unidirectional regulation):\n  - $n = 5$\n  - $A[0] = [1, 2]$, $A[1] = [2, 3]$, $A[2] = [0, 3]$, $A[3] = [2]$, $A[4] = [0, 3]$\n- Test case $2$ (boundary case with no edges):\n  - $n = 3$\n  - $A[0] = []$, $A[1] = []$, $A[2] = []$\n- Test case $3$ (edge case with self-loops and an isolated node):\n  - $n = 4$\n  - $A[0] = [0, 1]$, $A[1] = [1, 2]$, $A[2] = []$, $A[3] = [3]$\n- Test case $4$ (high-degree hub with reciprocal edges to test $\\Delta_{\\max}$ impact):\n  - $n = 6$\n  - $A[0] = [1, 2, 3, 4, 5]$, $A[1] = [0]$, $A[2] = [0]$, $A[3] = [0]$, $A[4] = [0]$, $A[5] = [0]$\n\nRequired final output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list of the form $[D_{\\mathrm{in}}, D_{\\mathrm{out}}, S, m, \\Delta_{\\max}]$.\n- Here, $D_{\\mathrm{in}}$ is the list of in-degrees $[d_{\\mathrm{in}}(0), d_{\\mathrm{in}}(1), \\dots, d_{\\mathrm{in}}(n-1)]$, $D_{\\mathrm{out}}$ is the list of out-degrees $[d_{\\mathrm{out}}(0), d_{\\mathrm{out}}(1), \\dots, d_{\\mathrm{out}}(n-1)]$, $S$ is the list of reciprocal neighbor overlap sizes $[s(0), s(1), \\dots, s(n-1)]$, $m$ is the integer edge count, and $\\Delta_{\\max}$ is the integer maximum degree.\n- For example, the program should print a single line with the structure $[[\\dots],[\\dots],[\\dots],[\\dots]]$ where each inner list is a test case result in the specified format.\n\nNote:\n- No physical units are involved.\n- The angle unit is not applicable.\n- Express all numeric results as integers in standard decimal notation.",
            "solution": "The user-provided problem is subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n\n- **Graph Model**: A directed gene-regulatory network is modeled as a graph with node set $V = \\{0, 1, \\dots, n-1\\}$ and directed edge set $E$.\n- **Representation**: The graph is given by an adjacency list $A$, where $A[i]$ contains the set of out-neighbors $N_{\\mathrm{out}}(i)$ for each node $i$.\n- **Edge Handling**: Duplicated edges in $A[i]$ are to be ignored. Self-loops $(i,i)$ are permitted.\n- **Definitions**:\n    - Out-degree: $d_{\\mathrm{out}}(i) = |N_{\\mathrm{out}}(i)|$.\n    - In-neighbor set: $N_{\\mathrm{in}}(i) = \\{j \\in V \\mid (j,i) \\in E\\}$.\n    - In-degree: $d_{\\mathrm{in}}(i) = |N_{\\mathrm{in}}(i)|$.\n    - Reciprocal neighbor overlap size: $s(i) = |N_{\\mathrm{out}}(i) \\cap N_{\\mathrm{in}}(i)|$.\n    - Total number of directed edges: $m = |E|$.\n    - Maximum degree: $\\Delta_{\\max} = \\max_{i \\in V} \\{\\max(d_{\\mathrm{out}}(i), d_{\\mathrm{in}}(i))\\}$.\n- **Task**:\n    1.  Design an algorithm to compute $d_{\\mathrm{in}}(i)$, $d_{\\mathrm{out}}(i)$, and $s(i)$ for all $i \\in V$, as well as $m$ and $\\Delta_{\\max}$, from the adjacency list $A$.\n    2.  The algorithm's core computations must use adjacency lists, not adjacency matrices.\n    3.  Analyze the algorithm's time complexity in terms of $m$ and $\\Delta_{\\max}$.\n- **Test Suite**: Four specific test cases are provided, each with a given number of nodes $n$ and an adjacency list $A$.\n- **Output Format**: A single line containing a comma-separated list of results for each test case. Each test case result must be a list of the form $[D_{\\mathrm{in}}, D_{\\mathrm{out}}, S, m, \\Delta_{\\max}]$, where $D_{\\mathrm{in}}$, $D_{\\mathrm{out}}$, and $S$ are lists of the per-node metrics.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is assessed against the validation criteria.\n\n- **Scientifically Grounded**: The problem is well-grounded. Graph-based models of regulatory networks are a cornerstone of computational systems biology. All definitions ($d_{\\mathrm{in}}, d_{\\mathrm{out}}$, etc.) are standard in graph theory. This criterion is met.\n- **Well-Posed**: The problem is well-posed. The inputs are clearly defined (adjacency list $A$, number of nodes $n$), and the required outputs ($D_{\\mathrm{in}}$, $D_{\\mathrm{out}}$, $S$, $m$, $\\Delta_{\\max}$) are unambiguously specified by mathematical definitions. A unique solution exists for any given input. This criterion is met.\n- **Objective**: The problem is stated in objective, mathematical language, free from ambiguity or subjective claims. This criterion is met.\n\nThe problem does not exhibit any invalidating flaws. It is scientifically sound, formalizable, complete, and algorithmically solvable.\n\n### Step 3: Verdict and Action\n\nThe problem is deemed **valid**. A solution will be provided.\n\n### Algorithmic Design and Analysis\n\nThe objective is to compute five graph-theoretic metrics from a given adjacency list representation $A$ of a directed graph on $n$ nodes. The core of the task is to design an efficient algorithm that avoids the $O(n^2)$ space complexity of an adjacency matrix, as mandated.\n\n**1. Data Structure Pre-processing**\n\nThe input is an adjacency list $A$, where each $A[i]$ is a list of out-neighbors. The problem states that duplicate edges should be ignored. The most effective way to enforce this uniqueness and enable efficient lookups is to convert each list $A[i]$ into a hash set. We construct a new data structure, `A_sets`, which is a list of sets, where `A_sets[i]` stores the unique out-neighbors of node $i$. This pre-processing step iterates through all entries in the original adjacency lists. If the total number of entries (including duplicates) is $M_{raw}$, this step takes $O(M_{raw})$ time.\n\n**2. Computation of Out-Degrees ($d_{\\mathrm{out}}$) and Total Edges ($m$)**\n\nOnce we have the list of sets `A_sets`, the out-degree $d_{\\mathrm{out}}(i)$ for each node $i$ is simply the size of the set `A_sets[i]`. This can be computed for all nodes in $O(n)$ time. The total number of unique directed edges, $m$, is the sum of all out-degrees: $m = \\sum_{i=0}^{n-1} d_{\\mathrm{out}}(i)$. This sum can be computed in $O(n)$ time after all out-degrees are known.\n\n**3. Computation of In-Degrees ($d_{\\mathrm{in}}$)**\n\nThe given adjacency list directly provides out-neighbors. To compute in-degrees, we must determine for each node $j$ how many other nodes $i$ have an edge $(i,j)$. Instead of constructing a full reverse graph, we can compute all in-degrees in a single pass over the edges. We initialize an array for in-degrees, `d_in`, of size $n$ with zeros. Then, we iterate through each node $i$ from $0$ to $n-1$ and for each of its out-neighbors $j \\in \\text{A\\_sets}[i]$, we increment the in-degree counter for $j$, i.e., `d_in[j]++`. This process touches each unique edge exactly once, so its time complexity is $O(m)$.\n\n**4. Computation of Reciprocal Neighbor Overlap Size ($s(i)$)**\n\nThe reciprocal neighbor overlap size for node $i$ is defined as $s(i) = |N_{\\mathrm{out}}(i) \\cap N_{\\mathrm{in}}(i)|$. An equivalent definition is the count of nodes $j$ for which both edge $(i, j)$ and edge $(j, i)$ exist. A direct and efficient algorithm can be designed based on this. We initialize an array for the overlap sizes, `s`, of size $n$ with zeros. We then iterate through each node $i$ and for each of its out-neighbors $j \\in \\text{A\\_sets}[i]$, we check if the reverse edge $(j, i)$ exists. This check is equivalent to testing for the membership of $i$ in the set of out-neighbors of $j$, i.e., `i in A_sets[j]`. Because `A_sets[j]` is a hash set, this check takes $O(1)$ time on average. If the condition is true, we increment the counter for `s[i]`. This process also involves iterating through all unique edges, leading to a total time complexity of $O(m)$ for computing all $s(i)$ values.\n\n**5. Computation of Maximum Degree ($\\Delta_{\\max}$)**\n\nAfter computing the lists of all in-degrees $D_{\\mathrm{in}}$ and out-degrees $D_{\\mathrm{out}}$, the maximum degree $\\Delta_{\\max}$ is found by taking the maximum value present in either of these two lists. This requires a single pass through both lists, which takes $O(n)$ time.\n\n**6. Time Complexity Analysis**\n\nThe overall time complexity of the algorithm is the sum of the complexities of its constituent steps:\n- Pre-processing $A$ into `A_sets`: $O(M_{raw})$, where $M_{raw}$ is the number of raw entries in $A$.\n- Computing all $d_{\\mathrm{out}}(i)$: $O(n)$.\n- Computing all $d_{\\mathrm{in}}(i)$: $O(m)$.\n- Computing all $s(i)$: $O(m)$ on average.\n- Computing $m$ and $\\Delta_{\\max}$: $O(n)$.\n\nThe total time complexity is $O(M_{raw} + n + m)$. Since the number of unique edges $m$ is at most $M_{raw}$, this simplifies to $O(M_{raw} + n)$. If the input is assumed to be free of duplicates, $M_{raw} = m$, and the complexity is $O(n+m)$.\n\nThe problem asks for the complexity in terms of $m$ and $\\Delta_{\\max}$. The number of edges $m$ is related to the maximum degree $\\Delta_{\\max}$ by the inequality $m = \\sum_{i \\in V} d_{\\mathrm{out}}(i) \\le n \\cdot \\Delta_{\\max}$. Therefore, the $O(n+m)$ complexity can be expressed with an upper bound of $O(n + n \\cdot \\Delta_{\\max}) = O(n \\cdot \\Delta_{\\max})$. This bound correctly captures the algorithm's dependence on the number of nodes and the graph's density as characterized by $\\Delta_{\\max}$. The space complexity is $O(n+m)$ to store the sanitized adjacency sets and the result arrays. This adheres to the constraint of avoiding an adjacency matrix.",
            "answer": "```python\nimport numpy as np\n\ndef calculate_metrics(n, adj_list):\n    \"\"\"\n    Computes graph metrics for a given directed graph.\n\n    Args:\n        n (int): The number of nodes in the graph.\n        adj_list (list of lists): The adjacency list representation of the graph.\n\n    Returns:\n        list: A list containing [D_in, D_out, S, m, Delta_max].\n    \"\"\"\n    if n == 0:\n        return [[], [], [], 0, 0]\n\n    # Step 1: Pre-process adjacency list to handle duplicates and for efficient lookups.\n    # A_sets[i] will store the unique out-neighbors of node i.\n    A_sets = [set(neighbors) for neighbors in adj_list]\n\n    # Step 2: Compute out-degrees.\n    d_out = [len(s) for s in A_sets]\n\n    # Initialize arrays for in-degrees and reciprocal overlap sizes.\n    d_in = [0] * n\n    s = [0] * n\n    \n    # Step 3 & 4: Compute in-degrees and reciprocal overlaps.\n    # Iterate through each edge (i,j) to update d_in for j and s for i.\n    for i in range(n):\n        for j in A_sets[i]:\n            # Each j is an out-neighbor of i, so this is an edge (i, j).\n            # This contributes to the in-degree of j.\n            d_in[j] += 1\n            \n            # Check for the reciprocal edge (j, i) to calculate s(i).\n            # This check is efficient due to the use of sets.\n            if i in A_sets[j]:\n                s[i] += 1\n\n    # Step 5: Compute total number of edges m.\n    m = sum(d_out)\n\n    # Step 6: Compute maximum degree Delta_max.\n    delta_max = 0\n    if n > 0:\n        max_out = max(d_out) if d_out else 0\n        max_in = max(d_in) if d_in else 0\n        delta_max = max(max_out, max_in)\n\n    return [d_in, d_out, s, m, delta_max]\n\ndef solve():\n    \"\"\"\n    Runs the validation and computation for the problem's test suite.\n    \"\"\"\n    test_cases = [\n        (5, [[1, 2], [2, 3], [0, 3], [2], [0, 3]]),\n        (3, [[], [], []]),\n        (4, [[0, 1], [1, 2], [], [3]]),\n        (6, [[1, 2, 3, 4, 5], [0], [0], [0], [0], [0]])\n    ]\n\n    results = []\n    for n, A in test_cases:\n        result = calculate_metrics(n, A)\n        results.append(result)\n\n    # Format the final output string exactly as required.\n    # The default str() representation of a list is used, e.g., '[1, 2, 3]'.\n    # A comma is used to join the string representations of each test case result.\n    final_output = f\"[{','.join(map(str, results))}]\"\n    \n    # Python's str() adds spaces after commas in lists. Let's remove them for a compact representation.\n    final_output = final_output.replace(\" \", \"\")\n    print(final_output)\n\nsolve()\n\n```"
        },
        {
            "introduction": "In practice, raw data from high-throughput experiments often yields dense, weighted adjacency matrices where many interactions may be weak or noisy. This exercise walks you through the common and essential task of network sparsification, which involves converting a dense matrix into a more manageable and interpretable sparse list by applying a significance threshold . Importantly, it also introduces the rigorous practice of quantifying the approximation error using matrix norms like the Frobenius norm $\\lVert \\cdot \\rVert_F$, a key skill in computational model validation.",
            "id": "3332692",
            "problem": "Consider a directed, weighted biomolecular interaction network (for example, transcription factor to gene influences) represented by a dense adjacency matrix $W \\in \\mathbb{R}^{n \\times n}$, where each entry $w_{ij}$ encodes the signed influence strength from node $i$ to node $j$. Each ordered pair $(i,j)$ also carries a categorical edge attribute $a_{ij} \\in \\mathbb{Z}$ (for example, $a_{ij} = 1$ for activation, $a_{ij} = -1$ for inhibition, and $a_{ij} = 0$ for absence or unknown), supplied in a parallel attribute matrix $A \\in \\mathbb{Z}^{n \\times n}$. The network is to be sparsified by dropping small-magnitude influences using a threshold $\\tau \\in \\mathbb{R}_{\\ge 0}$.\n\nStarting from the foundational definitions of adjacency matrices for networks and matrix norms, design and implement a procedure that:\n1. Converts $W$ and $A$ into a compressed adjacency list that contains only the retained edges after sparsification, preserving the attributes. Use the rule \"drop if $|w_{ij}|  \\tau$ and $w_{ij} \\neq 0$; keep otherwise,\" so that ties at exactly $\\tau$ are kept and zeros are not listed.\n2. Forms the sparsified matrix $W^{(\\tau)}$ by zeroing all entries dropped by the threshold, i.e., $w^{(\\tau)}_{ij} = w_{ij}$ if $|w_{ij}| \\ge \\tau$, and $w^{(\\tau)}_{ij} = 0$ if $|w_{ij}|  \\tau$.\n3. Derives and computes an upper bound on the matrix approximation error introduced by threshold-based sparsification in the Frobenius norm, and verifies a general spectral norm inequality. Specifically, let $E = W - W^{(\\tau)}$ denote the error matrix. Let $m$ denote the number of dropped nonzero entries (the count of $(i,j)$ such that $w_{ij} \\neq 0$ and $|w_{ij}|  \\tau$). Show that\n$$\\lVert E \\rVert_F^2 = \\sum_{(i,j):\\, |w_{ij}|  \\tau} w_{ij}^2 \\le m \\tau^2,$$\nand hence\n$$\\lVert E \\rVert_F \\le \\sqrt{m}\\,\\tau.$$\nAlso justify and compute that\n$$\\lVert E \\rVert_2 \\le \\lVert E \\rVert_F,$$\nwhere $\\lVert \\cdot \\rVert_2$ is the spectral norm induced by the Euclidean vector norm and $\\lVert \\cdot \\rVert_F$ is the Frobenius norm.\n\nYour program must construct the compressed adjacency list of quadruples $(i,j,w_{ij},a_{ij})$ for retained edges, compute $m$, $\\lVert E \\rVert_F$, the bound $\\sqrt{m}\\,\\tau$, and $\\lVert E \\rVert_2$, and verify the two inequalities by producing boolean values. The attribute preservation must be validated by checking that the attribute $a_{ij}$ in the adjacency list equals the corresponding entry in $A$ for every retained edge.\n\nUse the following test suite. Each case provides $(W,A,\\tau)$ explicitly. All numeric entries are dimensionless and angles are not involved.\n\nTest case $1$ (happy path, mixed signs, moderate threshold):\n$$\nW_1 =\n\\begin{bmatrix}\n0  0.21  -0.05  0.11 \\\\\n0.03  0  0.41  -0.12 \\\\\n-0.21  0.08  0  0.07 \\\\\n0.15  -0.09  0.20  0\n\\end{bmatrix},\\quad\nA_1 =\n\\begin{bmatrix}\n0  1  -1  1 \\\\\n1  0  1  -1 \\\\\n-1  1  0  1 \\\\\n1  -1  1  0\n\\end{bmatrix},\\quad\n\\tau_1 = 0.15.\n$$\n\nTest case $2$ (boundary case, $\\tau = 0$ keeps all nonzero edges):\n$$\nW_2 =\n\\begin{bmatrix}\n0  -0.20  0 \\\\\n0.18  0  0.05 \\\\\n0  -0.04  0\n\\end{bmatrix},\\quad\nA_2 =\n\\begin{bmatrix}\n0  -1  0 \\\\\n1  0  1 \\\\\n0  -1  0\n\\end{bmatrix},\\quad\n\\tau_2 = 0.\n$$\n\nTest case $3$ (edge case, threshold exceeding all weights, drops all nonzeros):\n$$\nW_3 =\n\\begin{bmatrix}\n0  0.30  -0.10  0  0.20 \\\\\n-0.25  0  0.05  -0.40  0 \\\\\n0.10  -0.20  0  0.30  -0.05 \\\\\n0  0  -0.15  0  0.10 \\\\\n-0.20  0.25  0  -0.05  0\n\\end{bmatrix},\\quad\nA_3 =\n\\begin{bmatrix}\n0  1  -1  0  1 \\\\\n-1  0  1  -1  0 \\\\\n1  -1  0  1  -1 \\\\\n0  0  -1  0  1 \\\\\n-1  1  0  -1  0\n\\end{bmatrix},\\quad\n\\tau_3 = 0.5.\n$$\n\nFor each test case, your program must output a list with the following entries, in order:\n- the integer number of retained edges $k$,\n- the integer number of dropped nonzero entries $m$,\n- the float value $\\lVert E \\rVert_F$,\n- the float upper bound $\\sqrt{m}\\,\\tau$,\n- the float value $\\lVert E \\rVert_2$,\n- a boolean indicating whether $\\lVert E \\rVert_F \\le \\sqrt{m}\\,\\tau$,\n- a boolean indicating whether $\\lVert E \\rVert_2 \\le \\lVert E \\rVert_F$,\n- a boolean indicating whether attributes are exactly preserved in the compressed adjacency list.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one list per test case in order, for example, $[\\text{case1},\\text{case2},\\text{case3}]$, where each $\\text{case}$ is itself a list as specified above.",
            "solution": "The problem presented is a well-posed exercise in computational network analysis and linear algebra, grounded in the established practices of systems biology. It requires the implementation of a threshold-based network sparsification procedure and the verification of standard mathematical inequalities related to matrix norms. The problem is self-contained, scientifically sound, and all provided data and conditions are consistent. Thus, we may proceed with a formal solution.\n\nThe core of the problem is to analyze the consequences of sparsifying a weighted, directed network represented by an adjacency matrix $W \\in \\mathbb{R}^{n \\times n}$. Sparsification is achieved by eliminating edges whose influence strength $w_{ij}$ falls below a certain magnitude threshold $\\tau \\in \\mathbb{R}_{\\ge 0}$. An associated attribute matrix $A \\in \\mathbb{Z}^{n \\times n}$ must also be handled.\n\nThe procedure can be broken down into the following logical steps:\n\n**1. Network Sparsification and Adjacency List Generation**\n\nThe network is defined by the adjacency matrix $W$ and attribute matrix $A$. The sparsification rule is to discard any edge $(i, j)$ if its weight $w_{ij}$ satisfies $0  |w_{ij}|  \\tau$. Edges with $|w_{ij}| \\ge \\tau$ are retained, as are entries where $w_{ij} = 0$ (which represent non-edges and remain as such).\n\nFirst, we construct the sparsified adjacency list. This list will contain only the retained edges, which are those whose weights satisfy $|w_{ij}| \\ge \\tau$. For each such edge, we store a quadruple $(i, j, w_{ij}, a_{ij})$, where $i$ is the source node index, $j$ is the target node index, $w_{ij}$ is the weight from $W$, and $a_{ij}$ is the corresponding attribute from $A$. The total number of retained edges is denoted by $k$.\n\nSimultaneously, we identify the set of dropped nonzero edges. These are the edges for which $0  |w_{ij}|  \\tau$. We count these edges, and this count is denoted by $m$.\n\n**2. Error Matrix Formulation**\n\nThe sparsification process transforms the original matrix $W$ into a sparsified matrix $W^{(\\tau)}$. The entries of $W^{(\\tau)}$ are defined as:\n$$\nw^{(\\tau)}_{ij} =\n\\begin{cases}\nw_{ij}  \\text{if } |w_{ij}| \\ge \\tau \\\\\n0  \\text{if } |w_{ij}|  \\tau\n\\end{cases}\n$$\nThis definition correctly handles the case of $w_{ij} = 0$, which remains $0$ in $W^{(\\tau)}$. The approximation error introduced by this process is captured by the error matrix $E = W - W^{(\\tau)}$. The entries of $E$ are $e_{ij} = w_{ij} - w^{(\\tau)}_{ij}$. Based on the definition of $W^{(\\tau)}$, the entries of the error matrix are:\n$$\ne_{ij} =\n\\begin{cases}\n0  \\text{if } |w_{ij}| \\ge \\tau \\text{ or } w_{ij} = 0 \\\\\nw_{ij}  \\text{if } 0  |w_{ij}|  \\tau\n\\end{cases}\n$$\nThus, the non-zero entries of $E$ are precisely the weights of the dropped edges.\n\n**3. Derivation of the Frobenius Norm Error Bound**\n\nThe problem requires us to show that $\\lVert E \\rVert_F \\le \\sqrt{m}\\,\\tau$, where $\\lVert \\cdot \\rVert_F$ is the Frobenius norm. The Frobenius norm of a matrix $E$ is defined as the square root of the sum of the squares of its elements: $\\lVert E \\rVert_F = \\sqrt{\\sum_{i,j} |e_{ij}|^2}$.\n\nLet's start with the squared Frobenius norm, $\\lVert E \\rVert_F^2$:\n$$\n\\lVert E \\rVert_F^2 = \\sum_{i=1}^n \\sum_{j=1}^n e_{ij}^2\n$$\nFrom the definition of $E$, the sum is non-zero only for the set of $m$ dropped edges, where $e_{ij} = w_{ij}$.\n$$\n\\lVert E \\rVert_F^2 = \\sum_{(i,j):\\, 0  |w_{ij}|  \\tau} w_{ij}^2\n$$\nThis establishes the first part of the identity we were asked to show. For each term in this sum, the condition for being a dropped edge is $0  |w_{ij}|  \\tau$, which implies $w_{ij}^2  \\tau^2$. By replacing each term $w_{ij}^2$ with the larger value $\\tau^2$, we obtain an upper bound on the sum. Since there are exactly $m$ such terms:\n$$\n\\sum_{(i,j):\\, 0  |w_{ij}|  \\tau} w_{ij}^2  \\sum_{k=1}^m \\tau^2 = m\\tau^2\n$$\nThe problem statement uses a non-strict inequality, $\\lVert E \\rVert_F^2 \\le m\\tau^2$, which is a valid and more general upper bound. Taking the square root of both sides (and since norms are non-negative) yields the desired result:\n$$\n\\lVert E \\rVert_F \\le \\sqrt{m\\tau^2} = \\sqrt{m}\\,\\tau\n$$\n\n**4. Justification of the Spectral Norm Inequality**\n\nWe must also justify the well-known matrix norm inequality $\\lVert E \\rVert_2 \\le \\lVert E \\rVert_F$, where $\\lVert \\cdot \\rVert_2$ is the spectral norm. The spectral norm of a matrix $E$ is defined as its largest singular value, $\\sigma_{\\max}(E)$. The Frobenius norm can also be expressed in terms of the singular values $\\{\\sigma_k\\}$ of the matrix:\n$$\n\\lVert E \\rVert_F^2 = \\sum_{k=1}^{\\text{rank}(E)} \\sigma_k(E)^2\n$$\nThe squared spectral norm is simply the square of the largest singular value:\n$$\n\\lVert E \\rVert_2^2 = (\\sigma_{\\max}(E))^2\n$$\nSince all singular values are non-negative, the sum of their squares must be greater than or equal to the square of any single singular value, including the largest one:\n$$\n(\\sigma_{\\max}(E))^2 \\le \\sum_{k=1}^{\\text{rank}(E)} \\sigma_k(E)^2\n$$\nSubstituting the norm definitions back gives:\n$$\n\\lVert E \\rVert_2^2 \\le \\lVert E \\rVert_F^2\n$$\nAs norms are non-negative, we can take the square root of both sides to arrive at the final inequality:\n$$\n\\lVert E \\rVert_2 \\le \\lVert E \\rVert_F\n$$\nThis inequality holds for any matrix $E$, and we will verify it computationally for the error matrices generated from the test cases.\n\n**5. Attribute Preservation Check**\n\nThe final task is to validate that the attributes of the retained edges are preserved correctly in the generated adjacency list. This is a crucial data integrity check. For every quadruple $(i, j, w_{ij}, a'_{ij})$ in the generated list, we will verify that the stored attribute $a'_{ij}$ is identical to the original attribute $A_{ij}$ from the input attribute matrix. The check is successful if this holds true for all $k$ retained edges.\n\nThe implementation will perform these calculations for each test case and report the requested quantities: $k$, $m$, $\\lVert E \\rVert_F$, $\\sqrt{m}\\,\\tau$, $\\lVert E \\rVert_2$, and the boolean results of the two inequality checks and the attribute preservation validation.",
            "answer": "```python\nimport numpy as np\n\ndef solve_case(W: np.ndarray, A: np.ndarray, tau: float):\n    \"\"\"\n    Processes a single test case for network sparsification and error analysis.\n\n    Args:\n        W: The weight matrix (n x n).\n        A: The attribute matrix (n x n).\n        tau: The sparsification threshold.\n\n    Returns:\n        A list containing the eight required output values for the case.\n    \"\"\"\n    n = W.shape[0]\n    adj_list = []\n    k = 0  # Number of retained edges\n    m = 0  # Number of dropped nonzero entries\n    E = np.zeros_like(W, dtype=float)\n\n    for i in range(n):\n        for j in range(n):\n            weight = W[i, j]\n            # An edge is a non-zero weight.\n            if weight != 0:\n                abs_weight = np.abs(weight)\n                if abs_weight = tau:\n                    # Retained edge\n                    k += 1\n                    adj_list.append((i, j, weight, A[i, j]))\n                else: # 0  abs_weight  tau\n                    # Dropped nonzero edge\n                    m += 1\n                    E[i, j] = weight\n    \n    # Compute norms\n    frobenius_norm_E = np.linalg.norm(E, 'fro')\n    spectral_norm_E = np.linalg.norm(E, 2)\n    \n    # Compute upper bound for the Frobenius norm\n    bound_frobenius = np.sqrt(m) * tau if m  0 else 0.0\n\n    # Verify inequalities\n    # Using np.isclose for robust floating-point comparison\n    inequality1_verified = frobenius_norm_E = bound_frobenius or np.isclose(frobenius_norm_E, bound_frobenius)\n    inequality2_verified = spectral_norm_E = frobenius_norm_E or np.isclose(spectral_norm_E, frobenius_norm_E)\n\n    # Verify attribute preservation\n    attributes_preserved = True\n    if not adj_list: # Vacuously true if no edges are retained\n        attributes_preserved = True\n    else:\n        for src, dst, w, attr in adj_list:\n            if attr != A[src, dst]:\n                attributes_preserved = False\n                break\n    \n    return [\n        k, \n        m, \n        float(frobenius_norm_E), \n        float(bound_frobenius), \n        float(spectral_norm_E), \n        inequality1_verified, \n        inequality2_verified, \n        attributes_preserved\n    ]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        (\n            np.array([\n                [0, 0.21, -0.05, 0.11],\n                [0.03, 0, 0.41, -0.12],\n                [-0.21, 0.08, 0, 0.07],\n                [0.15, -0.09, 0.20, 0]\n            ]),\n            np.array([\n                [0, 1, -1, 1],\n                [1, 0, 1, -1],\n                [-1, 1, 0, 1],\n                [1, -1, 1, 0]\n            ]),\n            0.15\n        ),\n        (\n            np.array([\n                [0, -0.20, 0],\n                [0.18, 0, 0.05],\n                [0, -0.04, 0]\n            ]),\n            np.array([\n                [0, -1, 0],\n                [1, 0, 1],\n                [0, -1, 0]\n            ]),\n            0.0\n        ),\n        (\n            np.array([\n                [0, 0.30, -0.10, 0, 0.20],\n                [-0.25, 0, 0.05, -0.40, 0],\n                [0.10, -0.20, 0, 0.30, -0.05],\n                [0, 0, -0.15, 0, 0.10],\n                [-0.20, 0.25, 0, -0.05, 0]\n            ]),\n            np.array([\n                [0, 1, -1, 0, 1],\n                [-1, 0, 1, -1, 0],\n                [1, -1, 0, 1, -1],\n                [0, 0, -1, 0, 1],\n                [-1, 1, 0, -1, 0]\n            ]),\n            0.5\n        )\n    ]\n\n    results = []\n    for W, A, tau in test_cases:\n        result = solve_case(W, A, tau)\n        results.append(result)\n\n    # Format the final output string exactly as required\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A central challenge in systems biology is to build comprehensive models by integrating data from diverse experimental sources, which are often incomplete or conflicting. This advanced practice tackles this problem head-on, guiding you to develop a principled method for reconciling multiple datasets into a single consensus network . You will derive and implement an algorithm that optimizes a consistency objective, learning to balance evidence from differently weighted sources against a sparsity penalty ($\\lambda$) to produce a robust, data-driven model.",
            "id": "3332735",
            "problem": "Consider a signed, directed molecular interaction network with $n$ nodes (molecules), where each data source $k \\in \\{1,\\dots,K\\}$ provides a signed edge set encoded either as an adjacency list (AL) or a signed adjacency matrix (AM). In a signed network, an edge $(i,j)$ carries a value $A_{ij}^{(k)} \\in \\{-1,0,+1\\}$ representing inhibition ($-1$), absence or unknown ($0$), or activation ($+1$). Each source is assigned a nonnegative reliability weight $w_k \\in \\mathbb{R}_{\\ge 0}$. The goal is to reconcile conflicting edge signs across sources into a single signed adjacency matrix $S \\in \\{-1,0,+1\\}^{n \\times n}$ by optimizing a consistency objective that rewards agreement with sources while penalizing the inclusion of edges to enforce sparsity.\n\nFundamental base and definitions:\n- A signed adjacency matrix $A^{(k)} \\in \\{-1,0,+1\\}^{n \\times n}$ represents a directed signed graph for source $k$, where $A^{(k)}_{ij}$ is the sign of the edge from node $i$ to node $j$, and $A^{(k)}_{ii} = 0$ for all $i$.\n- An adjacency list (AL) for source $k$ is a set of ordered triples $(i,j,s)$ with $i \\neq j$, $0 \\le i,j  n$, and $s \\in \\{-1,+1\\}$, denoting that $A^{(k)}_{ij} = s$. Any $(i,j)$ not listed has $A^{(k)}_{ij} = 0$.\n- Let the aggregated vote for edge $(i,j)$ be $v_{ij} = \\sum_{k=1}^K w_k A^{(k)}_{ij}$.\n- Let the reconciled signed adjacency matrix be $S$, with $S_{ij} \\in \\{-1,0,+1\\}$ and $S_{ii} = 0$ for all $i$.\n\nConsistency objective to be optimized:\nDefine the objective\n$$\nJ(S) = \\sum_{i \\neq j} \\left( \\sum_{k=1}^K w_k \\, S_{ij} \\, A^{(k)}_{ij} \\;-\\; \\lambda \\, |S_{ij}| \\right),\n$$\nwhere $\\lambda \\in \\mathbb{R}_{\\ge 0}$ is a sparsity parameter penalizing nonzero reconciled edges. The problem is to design an algorithm that, given $n$, the weights $w_k$, and the source edge sets (ALs or AMs), constructs $S$ that optimizes $J(S)$ over $S \\in \\{-1,0,+1\\}^{n \\times n}$, and to analyze the computational complexity of the algorithm in terms of $n$, $K$, and the total number of provided edges.\n\nAlgorithmic requirements and output specification:\n- Your algorithm must work from first principles using the above definitions, constructing $S$ directly from the mathematical objective without relying on any pre-specified shortcut formulae.\n- You must base your derivation on the explicit structure of $J(S)$ and on the properties of signed adjacency matrices and lists.\n- The final program must implement the algorithm for a fixed test suite defined below and produce the reconciled $S$ for each test case, flattened into a list of $n^2$ integers in row-major order. Each integer must be one of $-1$, $0$, or $+1$.\n- Diagonal entries must be $0$ in every output, consistent with $S_{ii} = 0$.\n\nTest suite:\nUse the following four test cases, with node indices starting at $0$ and strictly $i \\neq j$ for edges. For adjacency lists, any edge not listed is treated as $0$.\n\n1. Happy path with conflicting signs and moderate sparsity:\n   - $n = 4$, $K = 3$, $\\lambda = 0.2$, weights $w = [0.6, 0.3, 0.1]$,\n   - Source $1$ AL: $(0,1,+1)$, $(1,2,-1)$, $(2,3,+1)$, $(0,3,-1)$,\n   - Source $2$ AL: $(0,1,-1)$, $(1,2,-1)$, $(3,2,-1)$, $(0,3,+1)$,\n   - Source $3$ AL: $(0,1,+1)$, $(2,3,+1)$, $(0,3,-1)$.\n\n2. Boundary condition with strong sparsity:\n   - $n = 4$, $K = 3$, $\\lambda = 0.75$, weights $w = [0.6, 0.3, 0.1]$,\n   - Sources identical to Test $1$.\n\n3. Edge-case tie at the threshold:\n   - $n = 4$, $K = 3$, $\\lambda = 0.4$, weights $w = [0.6, 0.3, 0.1]$,\n   - Sources identical to Test $1$.\n\n4. High-reliability dominance across two conflicting sources:\n   - $n = 3$, $K = 2$, $\\lambda = 0.0$, weights $w = [0.9, 0.1]$,\n   - Source $1$ AL: $(0,1,-1)$, $(1,2,+1)$,\n   - Source $2$ AL: $(0,1,+1)$, $(1,2,-1)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the flattened reconciled adjacency matrix $S$ for one test case, represented as a bracket-enclosed comma-separated list of $n^2$ integers. For example, the output must have the form\n$$\n[\\,[s^{(1)}_0,\\dots,s^{(1)}_{n^2-1}],\\,[s^{(2)}_0,\\dots,s^{(2)}_{n^2-1}],\\,\\dots\\,],\n$$\nwith no spaces in the printed line. Each $s^{(t)}_m$ is an integer in $\\{-1,0,+1\\}$. No physical units or angles are involved in this problem. The program must not read any external input.",
            "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded problem in computational systems biology. It is self-contained, with all necessary parameters and definitions provided. The objective function is mathematically sound, and its optimization is tractable.\n\nThe core of the problem is to determine the reconciled signed adjacency matrix $S \\in \\{-1, 0, +1\\}^{n \\times n}$ that maximizes the objective function:\n$$\nJ(S) = \\sum_{i \\neq j} \\left( \\sum_{k=1}^K w_k \\, S_{ij} \\, A^{(k)}_{ij} \\;-\\; \\lambda \\, |S_{ij}| \\right)\n$$\nThe problem statement defines the aggregated vote for an edge $(i, j)$ as $v_{ij} = \\sum_{k=1}^K w_k A^{(k)}_{ij}$. Substituting this into the objective function, we get:\n$$\nJ(S) = \\sum_{i \\neq j} \\left( v_{ij} \\, S_{ij} - \\lambda \\, |S_{ij}| \\right)\n$$\nThe structure of this objective function is a sum of terms, where each term $J_{ij}(S_{ij}) = v_{ij} S_{ij} - \\lambda |S_{ij}|$ depends only on a single entry $S_{ij}$ of the matrix $S$. Therefore, we can maximize the total objective $J(S)$ by independently maximizing each term $J_{ij}(S_{ij})$ for all pairs $(i, j)$ with $i \\neq j$. The constraint $S_{ii} = 0$ is fixed for all diagonal elements.\n\nFor each off-diagonal entry $S_{ij}$, we must choose the value from the set $\\{-1, 0, +1\\}$ that maximizes $J_{ij}(S_{ij})$. We can analyze this by evaluating $J_{ij}$ for each of the three possibilities:\n$1$. If we choose $S_{ij} = +1$:\n$$\nJ_{ij}(+1) = v_{ij}(+1) - \\lambda |+1| = v_{ij} - \\lambda\n$$\n$2$. If we choose $S_{ij} = -1$:\n$$\nJ_{ij}(-1) = v_{ij}(-1) - \\lambda |-1| = -v_{ij} - \\lambda\n$$\n$3$. If we choose $S_{ij} = 0$:\n$$\nJ_{ij}(0) = v_{ij}(0) - \\lambda |0| = 0\n$$\nTo find the optimal value for $S_{ij}$, we compare these three outcomes.\nFirst, consider the case where $v_{ij}  0$. In this scenario, $v_{ij}  -v_{ij}$, which implies $v_{ij} - \\lambda  -v_{ij} - \\lambda$. Thus, $S_{ij}=+1$ is preferred over $S_{ij}=-1$. The choice is now between $S_{ij}=+1$ and $S_{ij}=0$. We choose $S_{ij}=+1$ if $J_{ij}(+1)  J_{ij}(0)$, which means $v_{ij} - \\lambda  0$, or $v_{ij}  \\lambda$. Otherwise, we choose $S_{ij}=0$.\n\nNext, consider the case where $v_{ij}  0$. Here, $-v_{ij}  v_{ij}$, implying $-v_{ij} - \\lambda  v_{ij} - \\lambda$. Thus, $S_{ij}=-1$ is preferred over $S_{ij}=+1$. The choice is between $S_{ij}=-1$ and $S_{ij}=0$. We choose $S_{ij}=-1$ if $J_{ij}(-1)  J_{ij}(0)$, which means $-v_{ij} - \\lambda  0$, or $v_{ij}  -\\lambda$. Otherwise, we choose $S_{ij}=0$.\n\nFinally, if $v_{ij}=0$, then $J_{ij}(+1) = -\\lambda$, $J_{ij}(-1) = -\\lambda$, and $J_{ij}(0)=0$. Since the problem specifies $\\lambda \\ge 0$, the maximum value is $0$, achieved when $S_{ij}=0$.\n\nA special consideration arises in tie-breaking. If $v_{ij} = \\lambda$, then $J_{ij}(+1) = \\lambda - \\lambda = 0$, which equals $J_{ij}(0)$. Similarly, if $v_{ij} = -\\lambda$, then $J_{ij}(-1) = -(-\\lambda) - \\lambda = 0$, which also equals $J_{ij}(0)$. In these cases of a tie, choosing $S_{ij}=0$ is the standard convention, promoting sparsity as intended by the penalty term $-\\lambda|S_{ij}|$.\n\nCombining these observations, we can formulate a clear decision rule for each $S_{ij}$ where $i \\neq j$:\n$$\nS_{ij} =\n\\begin{cases}\n+1  \\text{if } v_{ij}  \\lambda \\\\\n-1  \\text{if } v_{ij}  -\\lambda \\\\\n0   \\text{if } -\\lambda \\le v_{ij} \\le \\lambda\n\\end{cases}\n$$\n\nBased on this derivation, the algorithm to construct the matrix $S$ is as follows:\n$1$. Initialize an $n \\times n$ matrix of aggregated votes, denoted $V$, to all zeros. The entry $V_{ij}$ will store the value $v_{ij}$.\n$2$. For each data source $k \\in \\{1, \\dots, K\\}$ with weight $w_k$:\n   a. If the source is given as an adjacency list, iterate through each specified edge $(i, j, s)$.\n   b. Update the vote matrix: $V_{ij} \\leftarrow V_{ij} + w_k \\cdot s$.\n$3$. Initialize the $n \\times n$ reconciled matrix $S$ to all zeros. This automatically satisfies the $S_{ii}=0$ constraint.\n$4$. Iterate through each off-diagonal entry $(i, j)$ of the vote matrix $V$ (i.e., for all $i, j$ where $i \\neq j$):\n   a. Apply the derived decision rule: if $V_{ij}  \\lambda$, set $S_{ij} = +1$; if $V_{ij}  -\\lambda$, set $S_{ij} = -1$. Otherwise, $S_{ij}$ remains $0$.\n$5$. The resulting matrix $S$ is the solution that optimizes the objective function $J(S)$.\n\nThe computational complexity of this algorithm can be analyzed as follows. Let $E_k$ be the number of edges provided by source $k$, and let $E_{total} = \\sum_{k=1}^K E_k$ be the total number of edges across all sources.\n- Step $1$: Initialization of the $n \\times n$ vote matrix $V$ takes $O(n^2)$ time.\n- Step $2$: Aggregating votes from all sources, given as adjacency lists, requires a single pass over all provided edges. This takes $O(E_{total})$ time.\n- Step $3$: Initialization of the $n \\times n$ solution matrix $S$ takes $O(n^2)$ time.\n- Step $4$: Applying the thresholding rule requires iterating over all $n^2-n$ off-diagonal elements. This step takes $O(n^2)$ time.\nThe dominant steps are initialization and thresholding on the matrices, and vote aggregation. Therefore, the total time complexity is $O(n^2 + E_{total})$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the network reconciliation problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 4, \"K\": 3, \"lambda\": 0.2, \"weights\": [0.6, 0.3, 0.1],\n            \"sources\": [\n                [(0, 1, 1), (1, 2, -1), (2, 3, 1), (0, 3, -1)],  # Source 1 AL\n                [(0, 1, -1), (1, 2, -1), (3, 2, -1), (0, 3, 1)], # Source 2 AL\n                [(0, 1, 1), (2, 3, 1), (0, 3, -1)]              # Source 3 AL\n            ]\n        },\n        {\n            \"n\": 4, \"K\": 3, \"lambda\": 0.75, \"weights\": [0.6, 0.3, 0.1],\n            \"sources\": [\n                [(0, 1, 1), (1, 2, -1), (2, 3, 1), (0, 3, -1)],\n                [(0, 1, -1), (1, 2, -1), (3, 2, -1), (0, 3, 1)],\n                [(0, 1, 1), (2, 3, 1), (0, 3, -1)]\n            ]\n        },\n        {\n            \"n\": 4, \"K\": 3, \"lambda\": 0.4, \"weights\": [0.6, 0.3, 0.1],\n            \"sources\": [\n                [(0, 1, 1), (1, 2, -1), (2, 3, 1), (0, 3, -1)],\n                [(0, 1, -1), (1, 2, -1), (3, 2, -1), (0, 3, 1)],\n                [(0, 1, 1), (2, 3, 1), (0, 3, -1)]\n            ]\n        },\n        {\n            \"n\": 3, \"K\": 2, \"lambda\": 0.0, \"weights\": [0.9, 0.1],\n            \"sources\": [\n                [(0, 1, -1), (1, 2, 1)],\n                [(0, 1, 1), (1, 2, -1)]\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        lambda_param = case[\"lambda\"]\n        weights = case[\"weights\"]\n        sources = case[\"sources\"]\n\n        # Step 1  2: Initialize vote matrix and aggregate votes\n        # V_ij will store the aggregated vote v_ij\n        V = np.zeros((n, n), dtype=float)\n        for k, source_al in enumerate(sources):\n            w_k = weights[k]\n            for i, j, s in source_al:\n                V[i, j] += w_k * s\n\n        # Step 3  4: Initialize reconciled matrix S and apply decision rule\n        # S is initialized to zeros, satisfying S_ii = 0\n        S = np.zeros((n, n), dtype=int)\n        \n        # Apply the derived decision rule using vectorized numpy operations\n        # S_ij = +1 if v_ij  lambda\n        S[V  lambda_param] = 1\n        \n        # S_ij = -1 if v_ij  -lambda\n        S[V  -lambda_param] = -1\n        \n        # S_ij = 0 if -lambda = v_ij = lambda (already set by np.zeros)\n\n        # Flatten the resulting matrix S into a list in row-major order\n        flat_S = S.flatten().tolist()\n        results.append(flat_S)\n\n    # Format the final output according to the problem specification\n    # e.g., [[-1,0,...],[0,1,...]]\n    output_str = f\"[{','.join([f'[{\",\".join(map(str, r))}]' for r in results])}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}