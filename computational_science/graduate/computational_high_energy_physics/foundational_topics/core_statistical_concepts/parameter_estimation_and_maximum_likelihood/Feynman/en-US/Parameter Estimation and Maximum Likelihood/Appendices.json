{
    "hands_on_practices": [
        {
            "introduction": "Before committing enormous resources to building an experiment, physicists must project its discovery potential. This practice introduces the Asimov dataset, a key conceptual tool for calculating the median expected significance of a search. By working through this derivation, you will connect the likelihood function directly to the famous \"Asimov formula\" for a counting experiment, gaining a fundamental understanding of how experimental sensitivity is quantified. ",
            "id": "3526337",
            "problem": "A single-bin counting experiment is performed in a search for a new high-energy physics signal. The observed event count is modeled as a Poisson random variable with mean $\\nu(\\mu)=\\mu s + b$, where $\\mu \\geq 0$ is the signal strength parameter that scales the nominal expected signal yield $s$, and $b$ is the known expected background yield. The likelihood for an observed count $n$ is the Poisson probability mass function $L(n \\mid \\mu)=\\text{Pois}\\!\\left(n \\mid \\mu s + b\\right)$. For testing the discovery hypothesis, the null hypothesis is $H_{0}:\\mu=0$, and the alternative is $H_{1}:\\mu>0$. The one-sided profile likelihood ratio test statistic for discovery is defined as $q_{0}=-2\\ln\\lambda(0)$ with $\\lambda(0)=\\dfrac{L(n \\mid \\mu=0)}{L(n \\mid \\hat{\\mu})}$, where $\\hat{\\mu}$ is the maximum likelihood estimate of $\\mu$ subject to the physical constraint $\\mu\\geq 0$.\n\nDefine the Asimov dataset in this context. Then, under the Asimov assumption for the signal-plus-background hypothesis (i.e., the data are taken to be equal to their expectation with $\\mu=1$), derive the closed-form analytic expression for the median expected discovery test statistic $q_{0}$ and the corresponding median expected significance $Z=\\sqrt{q_{0}}$ as functions of $s$ and $b$. Your final answer must be a single closed-form expression or a pair of closed-form expressions presented together, and must not include any numerical substitution. No rounding is required. Express the final significance as a pure number without units.",
            "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, objective, and complete. We may proceed with the solution.\n\nThe problem asks for the median expected discovery test statistic, $q_{0}$, and the corresponding median expected significance, $Z$, under the Asimov assumption for the signal-plus-background hypothesis ($\\mu=1$). The derivation proceeds in four steps: (1) defining the Asimov dataset, (2) finding the maximum likelihood estimate ($\\hat{\\mu}$) for this dataset, (3) calculating the test statistic $q_{0}$, and (4) deriving the significance $Z$.\n\nFirst, we define the Asimov dataset. The Asimov dataset is a representative dataset where the observed data are set to their expectation values under a specific hypothesis. For this problem, the hypothesis is the signal-plus-background hypothesis with signal strength $\\mu=1$. The expected number of events, $\\nu(\\mu)$, is given by $\\nu(\\mu) = \\mu s + b$. Under the hypothesis $\\mu=1$, the expected event count is $\\nu(1) = (1)s + b = s+b$. Therefore, the Asimov dataset consists of a single observed count $n_{A} = s+b$. Note that while an observed count must be an integer, the Asimov dataset is a theoretical construct for which $n_A$ can be a non-integer real number, as the Poisson probability mass function can be generalized to non-integer arguments using the Gamma function.\n\nSecond, we determine the maximum likelihood estimate (MLE), $\\hat{\\mu}$, for the signal strength parameter $\\mu$ given the Asimov observation $n = n_{A} = s+b$. The likelihood function is the Poisson probability $L(n \\mid \\mu) = \\frac{(\\mu s + b)^{n} \\exp(-(\\mu s + b))}{n!}$. It is more convenient to work with the log-likelihood function, $\\ln L$:\n$$ \\ln L(n \\mid \\mu) = n \\ln(\\mu s + b) - (\\mu s + b) - \\ln(n!) $$\nTo find the unconstrained MLE, $\\tilde{\\mu}$, we take the derivative of $\\ln L$ with respect to $\\mu$ and set it to zero:\n$$ \\frac{\\partial}{\\partial \\mu} \\ln L(n \\mid \\mu) = \\frac{ns}{\\mu s + b} - s = 0 $$\nAssuming $s>0$, we can solve for $\\mu$:\n$$ \\frac{ns}{\\tilde{\\mu} s + b} = s \\implies n = \\tilde{\\mu} s + b \\implies \\tilde{\\mu} = \\frac{n-b}{s} $$\nSubstituting the Asimov data $n = n_{A} = s+b$ into this expression gives the unconstrained MLE for the Asimov dataset:\n$$ \\tilde{\\mu}_{A} = \\frac{(s+b) - b}{s} = \\frac{s}{s} = 1 $$\nThe problem states the physical constraint $\\mu \\geq 0$. The constrained MLE, $\\hat{\\mu}$, is therefore given by $\\hat{\\mu} = \\max(0, \\tilde{\\mu})$. Since we found $\\tilde{\\mu}_{A}=1$, which is greater than $0$, the constrained MLE for the Asimov dataset is $\\hat{\\mu}_{A} = 1$.\n\nThird, we calculate the test statistic $q_{0}$. The test statistic is defined as $q_{0} = -2\\ln\\lambda(0)$, where the profile likelihood ratio $\\lambda(0)$ is given by:\n$$ \\lambda(0) = \\frac{L(n \\mid \\mu=0)}{L(n \\mid \\hat{\\mu})} $$\nWe evaluate this for the Asimov dataset $n=n_A=s+b$ and its corresponding MLE $\\hat{\\mu}=\\hat{\\mu}_A=1$.\nThe likelihood in the numerator is evaluated under the null hypothesis ($H_{0}: \\mu=0$):\n$$ L(n_{A} \\mid \\mu=0) = \\text{Pois}(s+b \\mid b) = \\frac{b^{s+b} \\exp(-b)}{(s+b)!} $$\nThe likelihood in the denominator is evaluated at the MLE, $\\hat{\\mu}=1$:\n$$ L(n_{A} \\mid \\hat{\\mu}=1) = \\text{Pois}(s+b \\mid s+b) = \\frac{(s+b)^{s+b} \\exp(-(s+b))}{(s+b)!} $$\nThe ratio $\\lambda(0)$ is then:\n$$ \\lambda(0) = \\frac{b^{s+b} \\exp(-b)}{(s+b)^{s+b} \\exp(-(s+b))} = \\left(\\frac{b}{s+b}\\right)^{s+b} \\exp(-(b-(s+b))) = \\left(\\frac{b}{s+b}\\right)^{s+b} \\exp(s) $$\nNow, we compute $q_{0}$ by taking $-2\\ln$ of this expression:\n$$ q_{0} = -2\\ln\\left[ \\left(\\frac{b}{s+b}\\right)^{s+b} \\exp(s) \\right] = -2 \\left[ (s+b)\\ln\\left(\\frac{b}{s+b}\\right) + s \\right] $$\nUsing the property $\\ln(x/y) = -\\ln(y/x)$, we can rewrite this as:\n$$ q_{0} = -2 \\left[ -(s+b)\\ln\\left(\\frac{s+b}{b}\\right) + s \\right] = 2(s+b)\\ln\\left(\\frac{s+b}{b}\\right) - 2s $$\nFactoring out the $2$ and rewriting the argument of the logarithm gives the final expression for the median expected test statistic, which is often denoted as $q_{0,A}$:\n$$ q_{0} = 2 \\left[ (s+b)\\ln\\left(1+\\frac{s}{b}\\right) - s \\right] $$\nThis expression is valid for $s>0$ and $b>0$.\n\nFinally, the median expected significance $Z$ is defined as $Z = \\sqrt{q_{0}}$. Taking the square root of the expression for $q_{0}$ gives:\n$$ Z = \\sqrt{2 \\left[ (s+b)\\ln\\left(1+\\frac{s}{b}\\right) - s \\right]} $$\nThis is the well-known \"Asimov formula\" for the median significance of a simple counting experiment.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 2 \\left[ (s+b)\\ln\\left(1+\\frac{s}{b}\\right) - s \\right]  &  \\sqrt{2 \\left[ (s+b)\\ln\\left(1+\\frac{s}{b}\\right) - s \\right]} \\end{pmatrix} } $$"
        },
        {
            "introduction": "Moving from design to analysis, this exercise tackles a central task in particle physics: fitting the mass of a resonance. You will construct an unbinned likelihood for a Voigt profile, which models a particle's natural width convolved with detector resolution effects. This practice is crucial for understanding how to handle realistic data and, more importantly, for investigating how a misspecified model—a common source of systematic uncertainty—can introduce a tangible bias in your final measurement. ",
            "id": "3526389",
            "problem": "Consider a dataset of invariant mass measurements $\\{x_i\\}_{i=1}^N$ of a single resonance produced in a high-energy physics experiment, where each measured $x_i$ is the true resonance mass subject to intrinsic line shape and detector resolution. Assume the following foundational model components, which are standard in the field and grounded in well-tested physical and statistical principles:\n\n1. The intrinsic line shape of the resonance is described by the non-relativistic Breit–Wigner distribution, which corresponds to the Cauchy distribution with location parameter $m_0$ and scale (half-width at half-maximum) $\\gamma = \\Gamma/2$, where $\\Gamma$ is the natural width. The normalized intrinsic probability density function is\n$$\nf_{\\mathrm{BW}}(m; m_0, \\gamma) = \\frac{1}{\\pi} \\cdot \\frac{\\gamma}{(m - m_0)^2 + \\gamma^2}.\n$$\n\n2. The detector resolution is modeled as a Gaussian response with standard deviation $\\sigma$ and mean zero, so that the measured mass $x$ is the sum of the true sampled mass $m$ from the Breit–Wigner distribution and an independent Gaussian noise term $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$. Therefore, the observed distribution of $x$ is the convolution of $f_{\\mathrm{BW}}$ and the Gaussian resolution kernel:\n$$\nf(x; m, \\sigma, \\gamma) = \\int_{-\\infty}^{\\infty} f_{\\mathrm{BW}}(u; m, \\gamma) \\cdot \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\frac{(x-u)^2}{2\\sigma^2}\\right) \\, du,\n$$\nwhere $m$ is the resonance mass parameter to be estimated and $\\gamma$ is assumed known.\n\n3. The unbinned likelihood for a sample $\\{x_i\\}_{i=1}^N$ under parameters $(m, \\sigma)$ is the product of the convolution density evaluated at each observation:\n$$\nL(m, \\sigma) = \\prod_{i=1}^N f(x_i; m, \\sigma, \\gamma),\n$$\nand the log-likelihood is\n$$\n\\ell(m, \\sigma) = \\sum_{i=1}^N \\log f(x_i; m, \\sigma, \\gamma).\n$$\n\nYour tasks:\n\nA. Construct the unbinned likelihood $L(m,\\sigma)$ starting from the definitions above. Present the explicit normalized form of $f(x; m, \\sigma, \\gamma)$ as a mathematically well-defined convolution whose evaluation is numerically stable for all realistic parameter values in high-energy physics. Base your construction strictly on the definitions and properties of convolution and known special functions as needed, without introducing ad hoc or nonstandard approximations.\n\nB. Implement a maximum likelihood estimator (MLE) $\\hat{m}$ under model misspecification of the resolution: you are to fix the resolution model at $\\sigma_{\\mathrm{model}} = \\sigma_{\\mathrm{true}} + \\Delta\\sigma$, while keeping $\\gamma$ known and constant, and treat $m$ as the only free parameter to be estimated by maximizing $\\ell(m, \\sigma_{\\mathrm{model}})$. The resulting $\\hat{m}$ is the quasi-MLE of $m$ under a misspecified resolution model. Use a numerically robust optimization strategy appropriate for this one-dimensional continuous parameter.\n\nC. Quantify the bias in $\\hat{m}$ as $\\hat{m} - m_{\\mathrm{true}}$ for the following scientifically realistic and self-consistent test suite of parameter configurations. In each case, generate synthetic data by sampling $u \\sim f_{\\mathrm{BW}}(u; m_{\\mathrm{true}}, \\gamma)$ and independent $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{true}}^2)$, then form $x = u + \\epsilon$. This sampling method is exactly consistent with the convolution model. For reproducibility, use the specified random seeds. All masses must be expressed in $\\mathrm{GeV}$ (with natural units where $c=1$), and your final outputs must be in $\\mathrm{GeV}$ as decimal floats.\n\nTest suite (each case is $(N, m_{\\mathrm{true}}, \\Gamma, \\sigma_{\\mathrm{true}}, \\Delta\\sigma, \\text{seed})$ with $\\gamma=\\Gamma/2$):\n- Case 1 (happy path, correctly specified resolution): $(20000, 91.1876, 2.4952, 1.2, 0.0, 31415)$\n- Case 2 (moderate positive misspecification): $(20000, 91.1876, 2.4952, 1.2, +0.3, 27182)$\n- Case 3 (moderate negative misspecification): $(20000, 91.1876, 2.4952, 1.2, -0.3, 16180)$\n- Case 4 (small sample, positive misspecification): $(500, 91.1876, 2.4952, 1.2, +0.3, 57721)$\n- Case 5 (Gaussian-dominated regime): $(15000, 125.0, 1.2, 2.0, +0.5, 42424)$\n- Case 6 (Breit–Wigner-dominated regime): $(15000, 3.1, 0.1, 0.02, -0.01, 99999)$\n\nProgram requirements:\n\n1. Implement the construction and evaluation of $f(x; m, \\sigma, \\gamma)$ using a numerically stable special function representation appropriate for the convolution of a Cauchy distribution with a Gaussian. Ensure the function is properly normalized.\n\n2. For each test case, generate the synthetic dataset and compute the quasi-MLE $\\hat{m}$ under the misspecified resolution $\\sigma_{\\mathrm{model}} = \\sigma_{\\mathrm{true}} + \\Delta\\sigma$. Compute the bias $\\hat{m} - m_{\\mathrm{true}}$ in $\\mathrm{GeV}$ as a decimal float.\n\n3. Final output format: Your program should produce a single line of output containing the biases for the test cases as a comma-separated list enclosed in square brackets (e.g., \"[b1,b2,b3,b4,b5,b6]\"), with each $b_k$ in $\\mathrm{GeV}$ as a decimal float. No other output should be produced.\n\nAngles do not appear in this problem. Percentages must not be used. All masses must be treated and output in $\\mathrm{GeV}$.",
            "solution": "The posed problem is valid. It is scientifically grounded in the principles of high-energy physics and statistical inference, well-posed, and provides a complete and consistent set of definitions and data for a formal solution. The task is to quantify the bias in the maximum likelihood estimate of a resonance mass when the detector resolution model is misspecified.\n\n### A. Construction of the Probability Density Function (PDF)\n\nThe problem requires the construction of the PDF for the observed invariant mass $x$. This PDF, denoted $f(x; m, \\sigma, \\gamma)$, results from the convolution of the intrinsic resonance line shape and the detector's Gaussian resolution function.\n\nThe intrinsic line shape is the non-relativistic Breit–Wigner distribution, which is a Cauchy distribution with location parameter $m_0$ (the true mass of the resonance, which we denote as $m$ in the context of estimation) and scale parameter $\\gamma = \\Gamma/2$. Its PDF is:\n$$\nf_{\\mathrm{BW}}(u; m, \\gamma) = \\frac{1}{\\pi} \\frac{\\gamma}{(u - m)^2 + \\gamma^2}\n$$\n\nThe detector resolution is a Gaussian function with mean $0$ and standard deviation $\\sigma$:\n$$\nf_G(\\epsilon; \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{\\epsilon^2}{2\\sigma^2}\\right)\n$$\n\nThe observed measurement $x$ is the sum of a random variable $u$ drawn from $f_{\\mathrm{BW}}$ and an independent random variable $\\epsilon$ drawn from $f_G$. Therefore, the PDF of $x$ is the convolution of these two distributions:\n$$\nf(x; m, \\sigma, \\gamma) = (f_{\\mathrm{BW}} * f_G)(x) = \\int_{-\\infty}^{\\infty} f_{\\mathrm{BW}}(u; m, \\gamma) f_G(x-u; \\sigma) \\, du\n$$\nSubstituting the expressions for the PDFs gives the explicit integral from the problem statement. This convolution is known as the Voigt profile.\n\nWhile the integral can be computed numerically, a more direct and stable method involves its known analytical form, which uses the Faddeeva function (or complex error function), $w(z)$. The Faddeeva function is defined as $w(z) = e^{-z^2} \\text{erfc}(-iz)$, where $\\text{erfc}$ is the complementary error function.\n\nThe Voigt profile PDF is given by:\n$$\nf(x; m, \\sigma, \\gamma) = \\frac{\\text{Re}[w(z)]}{\\sigma \\sqrt{2\\pi}}\n$$\nwhere the complex argument $z$ is:\n$$\nz = \\frac{(x-m) + i\\gamma}{\\sigma\\sqrt{2}}\n$$\nThis form is properly normalized, i.e., $\\int_{-\\infty}^{\\infty} f(x; m, \\sigma, \\gamma) \\, dx = 1$. The expression is numerically stable and efficiently computable using standard scientific libraries, such as `scipy.special.wofz`, which provides an implementation of $w(z)$. This analytical form will be used in the subsequent steps.\n\n### B. Maximum Likelihood Estimator Implementation\n\nThe goal is to find the maximum likelihood estimate (MLE) of the mass parameter $m$, denoted $\\hat{m}$, given a dataset $\\{x_i\\}_{i=1}^N$ and a potentially misspecified resolution model. The model parameters are the unknown mass $m$, the fixed known width $\\gamma$, and a fixed, possibly incorrect, resolution $\\sigma_{\\mathrm{model}}$.\n\nThe unbinned log-likelihood function is:\n$$\n\\ell(m) = \\sum_{i=1}^N \\log f(x_i; m, \\sigma_{\\mathrm{model}}, \\gamma)\n$$\nTo find the MLE $\\hat{m}$, we need to find the value of $m$ that maximizes $\\ell(m)$. This is equivalent to minimizing the negative log-likelihood function, $-\\ell(m)$:\n$$\n\\hat{m} = \\arg\\min_m \\left[ -\\sum_{i=1}^N \\log f(x_i; m, \\sigma_{\\mathrm{model}}, \\gamma) \\right]\n$$\nThis is a one-dimensional continuous optimization problem. A robust algorithm suitable for this task is `scipy.optimize.minimize_scalar`. This function finds a local minimum of a scalar function of one variable within a given bracket. For the negative log-likelihood function of a Voigt profile, a global minimum is typically unique and found reliably.\n\nThe algorithm is as follows:\n1. Define a function that computes the negative log-likelihood, $-\\ell(m)$, for a given value of $m$, using the dataset $\\{x_i\\}$, the known $\\gamma$, and the model resolution $\\sigma_{\\mathrm{model}}$.\n2. Use `scipy.optimize.minimize_scalar` to find the value of $m$ that minimizes this function. A reasonable search bracket, such as $[m_{\\mathrm{true}} - c, m_{\\mathrm{true}} + c]$ for some constant $c$, must be provided to bound the search space.\n\n### C. Bias Quantification Procedure\n\nThe final task is to quantify the bias, $\\hat{m} - m_{\\mathrm{true}}$, for a suite of test cases. Each case involves generating a synthetic dataset according to the true physical process and then applying the MLE procedure with a misspecified model.\n\nFor each test case specified by $(N, m_{\\mathrm{true}}, \\Gamma, \\sigma_{\\mathrm{true}}, \\Delta\\sigma, \\text{seed})$:\n1.  **Set up parameters**: Calculate $\\gamma = \\Gamma/2$ and the model resolution $\\sigma_{\\mathrm{model}} = \\sigma_{\\mathrm{true}} + \\Delta\\sigma$.\n2.  **Initialize RNG**: A random number generator is initialized with the specified `seed` to ensure reproducibility.\n3.  **Generate Data**: A synthetic dataset $\\{x_i\\}_{i=1}^N$ is generated.\n    -   First, $N$ random values $\\{u_i\\}$ are drawn from the Breit-Wigner distribution $f_{\\mathrm{BW}}(u; m_{\\mathrm{true}}, \\gamma)$. This can be achieved by transforming samples from a standard uniform or standard Cauchy distribution. For a standard Cauchy variable $C$, a sample is given by $u_i = m_{\\mathrm{true}} + \\gamma C_i$.\n    -   Second, $N$ independent random noise values $\\{\\epsilon_i\\}$ are drawn from the Gaussian distribution $\\mathcal{N}(0, \\sigma_{\\mathrm{true}}^2)$.\n    -   The final observed values are $x_i = u_i + \\epsilon_i$.\n4.  **Estimate Mass**: The MLE procedure from part B is applied to the generated dataset $\\{x_i\\}$ using the model resolution $\\sigma_{\\mathrm{model}}$. This yields the quasi-MLE $\\hat{m}$.\n5.  **Calculate Bias**: The bias is computed as the difference between the estimated mass and the true mass: $\\text{Bias} = \\hat{m} - m_{\\mathrm{true}}$.\n\nThis procedure is repeated for all test cases, and the resulting biases are collected. The implementation will follow this logic to produce the required output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import wofz\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Solves the problem of estimating bias in resonance mass measurement\n    due to misspecified detector resolution.\n    \"\"\"\n    \n    # Test suite: (N, m_true, Gamma, sigma_true, delta_sigma, seed)\n    test_cases = [\n        # Case 1 (happy path, correctly specified resolution):\n        (20000, 91.1876, 2.4952, 1.2, 0.0, 31415),\n        # Case 2 (moderate positive misspecification):\n        (20000, 91.1876, 2.4952, 1.2, 0.3, 27182),\n        # Case 3 (moderate negative misspecification):\n        (20000, 91.1876, 2.4952, 1.2, -0.3, 16180),\n        # Case 4 (small sample, positive misspecification):\n        (500, 91.1876, 2.4952, 1.2, 0.3, 57721),\n        # Case 5 (Gaussian-dominated regime):\n        (15000, 125.0, 1.2, 2.0, 0.5, 42424),\n        # Case 6 (Breit–Wigner-dominated regime):\n        (15000, 3.1, 0.1, 0.02, -0.01, 99999),\n    ]\n\n    def voigt_pdf(x, m, sigma, gamma):\n        \"\"\"\n        Computes the Voigt profile PDF.\n        The PDF is the convolution of a Cauchy(m, gamma) and a Gaussian(0, sigma).\n        \"\"\"\n        # The Faddeeva function `wofz` provides a fast and stable implementation.\n        # z = (x - m + i*gamma) / (sigma * sqrt(2))\n        z = ((x - m) + 1j * gamma) / (sigma * np.sqrt(2.0))\n        \n        # PDF = Re(wofz(z)) / (sigma * sqrt(2*pi))\n        pdf = np.real(wofz(z)) / (sigma * np.sqrt(2.0 * np.pi))\n        return pdf\n\n    def neg_log_likelihood(m, x_data, sigma, gamma):\n        \"\"\"\n        Computes the negative log-likelihood for the Voigt distribution.\n        \"\"\"\n        pdf_values = voigt_pdf(x_data, m, sigma, gamma)\n        \n        # To avoid issues with log(0), although Voigt is always positive.\n        # A small constant can be added, but np.log handles -inf correctly\n        # and the optimizer will move away from such regions.\n        log_likelihood_sum = np.sum(np.log(pdf_values))\n        \n        # We want to maximize the likelihood, which is equivalent to\n        # minimizing the negative log-likelihood.\n        return -log_likelihood_sum\n\n    results = []\n    \n    for case in test_cases:\n        N, m_true, Gamma, sigma_true, delta_sigma, seed = case\n        \n        # Initialize the random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n        \n        # Calculate derived parameters\n        gamma = Gamma / 2.0\n        sigma_model = sigma_true + delta_sigma\n\n        # Generate synthetic data\n        # 1. Sample from a standard Cauchy distribution and scale/shift.\n        # u ~ Breit-Wigner(m_true, gamma)\n        u_samples = m_true + gamma * rng.standard_cauchy(N)\n        \n        # 2. Sample from a Gaussian distribution.\n        # eps ~ Normal(0, sigma_true^2)\n        eps_samples = rng.normal(loc=0.0, scale=sigma_true, size=N)\n        \n        # 3. The observed data is the sum.\n        x_data = u_samples + eps_samples\n\n        # Create the objective function for the optimizer.\n        # It must be a function of a single variable, `m`.\n        objective_func = lambda m: neg_log_likelihood(m, x_data, sigma_model, gamma)\n        \n        # Perform the minimization to find the MLE for m.\n        # A bracket around the true mass is a safe choice.\n        # For the J/psi case (m=3.1), a bracket of +/- 1 is wide but safe.\n        # For all cases, this bracket is sufficient.\n        res = minimize_scalar(\n            objective_func, \n            bracket=(m_true - 1.0, m_true + 1.0),\n            method='brent'\n        )\n        \n        m_hat = res.x\n        \n        # Calculate the bias\n        bias = m_hat - m_true\n        results.append(bias)\n\n    # Format the final output as specified.\n    print(f\"[{','.join(f'{b:.8f}' for b in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The final step of many analyses is to report a parameter estimate with a confidence interval, but standard methods can be misleading in challenging situations. This practice explores a scenario where the likelihood function is nearly flat, causing simple curvature-based error estimates to fail spectacularly. By comparing this fragile method to the robust likelihood-ratio scan and parametric bootstrap, you will gain practical experience with the gold-standard techniques used in high-energy physics to ensure reliable interval estimation. ",
            "id": "3526375",
            "problem": "You will design and implement a complete, runnable program that constructs and compares three interval estimators for a single signal-strength parameter in a simplified high-energy physics counting experiment. The context is a weakly coupled interaction whose event yield depends quadratically on a real nonnegative amplitude parameter. The key challenge is the regime near a likelihood saddle where the second derivative of the log-likelihood with respect to the parameter is approximately zero, causing curvature-based uncertainty estimates to fail.\n\nThe physical setup is idealized but standard in computational high-energy physics: a single-bin counting experiment with known background and a signal yield that scales with the square of a nonnegative amplitude. The data are a single integer count $n$ drawn from a Poisson distribution with mean\n$$\n\\lambda(\\theta) \\equiv b + s\\,\\theta^2,\n$$\nwhere $b \\ge 0$ is the known expected background count, $s > 0$ is a known signal normalization, and $\\theta \\ge 0$ is the real amplitude parameter of interest. No physical units are required because all quantities are dimensionless event counts. Your program must implement three interval constructions for $\\theta$ at a central probability mass of $0.6827$ (commonly called one standard deviation for a Gaussian), and compare their behaviors, particularly near the saddle regime where the observed curvature of the log-likelihood at its maximum is close to zero.\n\nFundamental base and definitions:\n- The likelihood for observing $n$ given $\\theta$ is\n$$\nL(\\theta; n, b, s) = \\mathrm{Pois}\\big(n \\mid \\lambda(\\theta)\\big) = \\frac{\\lambda(\\theta)^n e^{-\\lambda(\\theta)}}{n!}.\n$$\n- The log-likelihood (up to an additive constant independent of $\\theta$) is\n$$\n\\ell(\\theta) = n \\log \\lambda(\\theta) - \\lambda(\\theta).\n$$\n- The maximum likelihood estimate (MLE) $\\hat{\\theta}$ is any maximizer of $\\ell(\\theta)$ subject to $\\theta \\ge 0$.\n- The observed Fisher information is defined as $J(\\theta) \\equiv -\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta^2}$ evaluated at the MLE, namely $J(\\hat{\\theta})$.\n- The likelihood-ratio statistic for a given $\\theta$ is\n$$\nq(\\theta) \\equiv 2\\left[\\ell(\\hat{\\theta}) - \\ell(\\theta)\\right].\n$$\n\nYour program must:\n1. Compute the MLE $\\hat{\\theta}$ by maximizing $\\ell(\\theta)$ over $\\theta \\ge 0$.\n2. Construct the following three intervals for $\\theta$ at central probability mass $0.6827$:\n   - Curvature-based (Wald) interval: Use the observed Fisher information $J(\\hat{\\theta})$ to form a normal-approximation interval. Treat the target as a two-sided central interval. If $J(\\hat{\\theta}) \\le 0$ or numerically indistinguishable from zero (below a strict positive tolerance), the curvature-based interval is undefined; in that case, you must report the curvature-based interval as $[0, +\\infty)$.\n   - Likelihood-ratio scan interval: Invert the test based on $q(\\theta)$, finding the connected interval of $\\theta \\ge 0$ for which $q(\\theta) \\le c$, where $c$ corresponds to the one-dimensional central probability mass of $0.6827$ under the Gaussian approximation. Use the canonical choice $c = 1$. Implement numerical root finding by scanning $q(\\theta)$ and bracketing each endpoint rigorously before invoking a robust solver. If the lower endpoint would be negative, report it as $0$.\n   - Parametric bootstrap interval: Generate pseudo-experiments $n^\\star \\sim \\mathrm{Pois}\\big(b + s \\hat{\\theta}^2\\big)$ with a fixed random seed for reproducibility. For each pseudo-experiment, compute the bootstrap MLE $\\hat{\\theta}^\\star$. Report the empirical central interval defined by the lower and upper quantiles at $0.15865$ and $0.84135$. Use at least $10000$ bootstrap replicates for each test case to ensure stability.\n\n3. Provide results for the following test suite of distinct regimes:\n   - Case A (saddle regime): $n = 25$, $b = 25.0$, $s = 100.0$.\n   - Case B (slight excess near saddle): $n = 27$, $b = 25.0$, $s = 100.0$.\n   - Case C (deficit with boundary MLE): $n = 20$, $b = 25.0$, $s = 100.0$.\n   - Case D (low-count regime): $n = 1$, $b = 0.5$, $s = 5.0$.\n\nAlgorithmic and numerical requirements:\n- Start from the fundamental definitions above and compute derivatives as needed from $\\ell(\\theta)$. Do not assume any pre-supplied formulas; derive what you need from first principles.\n- All numerical solvers must handle the constraint $\\theta \\ge 0$. Use a clear finite tolerance when deciding whether $J(\\hat{\\theta})$ is nonpositive.\n- Use a fixed seed for any pseudo-random generation to ensure determinism.\n- Report each interval as an ordered pair $[\\text{lower}, \\text{upper}]$ with the lower bound truncated at $0$ when necessary. If the upper bound is $+\\infty$, that must be reported explicitly as an infinite floating-point value.\n\nFinal output format:\n- For each case in the specified order (A, B, C, D), produce a list containing the three intervals in the order curvature-based, likelihood-ratio scan, parametric bootstrap, each interval represented as two floating-point numbers rounded to $6$ decimal places. The final output must therefore be a single line containing a list of four elements (one per test case), where each element is a list of six floats in the order\n$$\n[\\text{wald\\_low}, \\text{wald\\_high}, \\text{lr\\_low}, \\text{lr\\_high}, \\text{boot\\_low}, \\text{boot\\_high}].\n$$\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\,[\\cdots], [\\cdots], [\\cdots], [\\cdots]\\,]$.\n\nAll quantities are dimensionless, and no physical units are required. All angles, if any appear, must be in radians; however, this problem does not involve angles.",
            "solution": "We begin from the maximum likelihood principle and the Poisson model appropriate for a single-bin counting experiment in high-energy physics. The experiment observes a count $n$ modeled as Poisson with mean $\\lambda(\\theta) = b + s \\theta^2$ for nonnegative $\\theta$. The likelihood function is $L(\\theta) = \\mathrm{Pois}(n \\mid \\lambda(\\theta))$ and the log-likelihood, dropping additive constants independent of $\\theta$, is\n$$\n\\ell(\\theta) = n \\log \\lambda(\\theta) - \\lambda(\\theta), \\quad \\lambda(\\theta) \\equiv b + s \\theta^2.\n$$\n\nMaximum likelihood estimate:\nBecause the Poisson likelihood with respect to its mean is maximized at $\\hat{\\lambda} = n$, the constrained maximizer over $\\theta \\ge 0$ must satisfy\n$$\n\\hat{\\theta}^2 = \\max\\left( \\frac{n - b}{s}, 0 \\right), \\quad \\hat{\\theta} = \\sqrt{\\max\\left( \\frac{n - b}{s}, 0 \\right)}.\n$$\nThis follows by maximizing $\\ell(\\theta)$ as a function of $\\lambda(\\theta)$ with respect to $\\theta$, observing monotonicity and the nonnegativity constraint. If $n \\le b$ then $\\hat{\\theta} = 0$ (boundary MLE). If $n > b$ then $\\hat{\\theta} = \\sqrt{(n - b)/s}$ (interior MLE), for which the corresponding $\\hat{\\lambda} = n$.\n\nObserved Fisher information and curvature-based (Wald) interval:\nThe observed Fisher information is $J(\\theta) \\equiv -\\ell''(\\theta)$, where the prime denotes differentiation with respect to $\\theta$. We compute derivatives from first principles. First,\n$$\n\\ell'(\\theta) = \\frac{n}{\\lambda(\\theta)} \\cdot \\lambda'(\\theta) - \\lambda'(\\theta) = \\left(\\frac{n - \\lambda(\\theta)}{\\lambda(\\theta)}\\right) \\lambda'(\\theta).\n$$\nSince $\\lambda'(\\theta) = 2 s \\theta$, we have\n$$\n\\ell'(\\theta) = \\left(\\frac{n - \\lambda(\\theta)}{\\lambda(\\theta)}\\right) 2 s \\theta.\n$$\nDifferentiating again, using the product rule and that\n$$\n\\frac{d}{d\\theta}\\left(\\frac{n - \\lambda(\\theta)}{\\lambda(\\theta)}\\right) = -\\lambda'(\\theta)\\left(\\frac{1}{\\lambda(\\theta)} + \\frac{n - \\lambda(\\theta)}{\\lambda(\\theta)^2}\\right) = -\\lambda'(\\theta) \\frac{n}{\\lambda(\\theta)^2},\n$$\nwe obtain\n$$\n\\ell''(\\theta) = 2 s \\frac{n - \\lambda(\\theta)}{\\lambda(\\theta)} - 4 s^2 \\theta^2 \\frac{n}{\\lambda(\\theta)^2}.\n$$\nEvaluated at the interior MLE where $\\lambda(\\hat{\\theta}) = n$, this simplifies to\n$$\n\\ell''(\\hat{\\theta}) = - \\frac{4 s (n - b)}{n}, \\quad J(\\hat{\\theta}) = -\\ell''(\\hat{\\theta}) = \\frac{4 s (n - b)}{n}.\n$$\nAt the boundary MLE $\\hat{\\theta} = 0$, we have $\\lambda(0) = b$ and\n$$\n\\ell''(0) = 2 s \\frac{n - b}{b}, \\quad J(0) = -\\ell''(0) = 2 s \\frac{b - n}{b}.\n$$\nThe saddle regime occurs when $n \\approx b$, for which $\\ell''(\\hat{\\theta}) \\approx 0$ and hence $J(\\hat{\\theta}) \\approx 0$. In that case the quadratic (Gaussian) approximation breaks down because it predicts an arbitrarily large variance or fails to define one when $J(\\hat{\\theta}) \\le 0$ under numerical noise.\n\nThe curvature-based (Wald) $0.6827$-mass interval is the two-sided normal-approximation interval\n$$\n\\left[ \\max\\left(0, \\hat{\\theta} - \\frac{1}{\\sqrt{J(\\hat{\\theta})}}\\right), \\ \\hat{\\theta} + \\frac{1}{\\sqrt{J(\\hat{\\theta})}} \\right],\n$$\nwith the understanding that if $J(\\hat{\\theta}) \\le 0$ or is numerically indistinguishable from zero within a strict tolerance, this construction is invalid and we report $[0, +\\infty)$.\n\nLikelihood-ratio scan interval:\nDefine the likelihood-ratio statistic\n$$\nq(\\theta) \\equiv 2 \\left[\\ell(\\hat{\\theta}) - \\ell(\\theta)\\right].\n$$\nWe construct the $0.6827$-mass interval by inverting the test at threshold $c = 1$ (the canonical one-dimensional threshold corresponding to one standard deviation under a Gaussian approximation). The interval is the set\n$$\n\\{\\theta \\ge 0 \\mid q(\\theta) \\le 1\\}.\n$$\nOperationally, we find the connected interval around $\\hat{\\theta}$ by root finding on $q(\\theta) - 1 = 0$. The lower endpoint is either the unique root in $(0, \\hat{\\theta})$ when it exists or $0$ otherwise (e.g., if $q(0) \\le 1$ or $\\hat{\\theta} = 0$). The upper endpoint is the unique root in $(\\hat{\\theta}, \\infty)$, which exists because $\\ell(\\theta) \\to -\\infty$ as $\\theta \\to \\infty$ and hence $q(\\theta) \\to \\infty$. We bracket each root by scanning outward and apply a robust bisection-based solver.\n\nParametric bootstrap interval:\nWe perform a parametric bootstrap at the plug-in parameter value $\\hat{\\theta}$. Generate $N_{\\mathrm{boot}}$ pseudo-experiments $n^\\star \\sim \\mathrm{Pois}(b + s \\hat{\\theta}^2)$ with a fixed seed. For each $n^\\star$, compute the bootstrap MLE\n$$\n\\hat{\\theta}^\\star = \\sqrt{\\max\\left(\\frac{n^\\star - b}{s}, 0\\right)}.\n$$\nThe bootstrap $0.6827$-mass central interval is given by the empirical quantiles at probabilities $0.15865$ and $0.84135$. Because $\\theta \\ge 0$, it is common that the lower quantile may be $0$ in the boundary regime.\n\nTest suite and implementation details:\nWe apply the above constructions to four cases: $(n, b, s) \\in \\{(25, 25.0, 100.0), (27, 25.0, 100.0), (20, 25.0, 100.0), (1, 0.5, 5.0)\\}$. For each, we compute\n- the MLE $\\hat{\\theta}$ from first principles as above;\n- the curvature-based interval using $J(\\hat{\\theta})$ derived from $\\ell''(\\theta)$;\n- the likelihood-ratio interval by solving $q(\\theta) = 1$ with bracketing and a robust root finder;\n- the parametric bootstrap interval with $N_{\\mathrm{boot}} \\ge 10000$ replicates and fixed seed.\n\nNumerical stability considerations:\n- We use the analytic expression for $\\ell''(\\theta)$ to evaluate $J(\\hat{\\theta})$ accurately at the MLE. When $b$ is very small, we avoid evaluating divisions by zero by ensuring $b > 0$ in the test suite.\n- We implement a strict positive tolerance for $J(\\hat{\\theta})$. If $J(\\hat{\\theta})$ is nonpositive or below tolerance, we declare the Wald interval undefined and report $[0, +\\infty)$.\n- For likelihood-ratio bracketing, we exponentially increase the bracket until $q(\\theta)$ exceeds $1$ to guarantee existence of the upper root. The lower root is sought only if $q(0) > 1$ and $\\hat{\\theta} > 0$.\n\nFinal output:\nFor each case, we report the six numbers\n$$\n[\\text{wald\\_low}, \\text{wald\\_high}, \\text{lr\\_low}, \\text{lr\\_high}, \\text{boot\\_low}, \\text{boot\\_high}],\n$$\nrounded to $6$ decimal places, producing a single-line list of four such lists. This design demonstrates the failure of curvature-based intervals near the saddle ($J(\\hat{\\theta}) \\approx 0$), the robustness of likelihood-ratio intervals obtained by scanning the exact likelihood, and the data-driven behavior of bootstrap intervals, especially in boundary and low-count regimes.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef loglik(n, b, s, theta):\n    # Log-likelihood up to additive constant independent of theta\n    # Ensure theta >= 0\n    if theta < 0:\n        return -np.inf\n    lam = b + s * theta**2\n    if lam <= 0:\n        return -np.inf\n    return n * np.log(lam) - lam\n\ndef mle_theta(n, b, s):\n    # Constrained MLE theta >= 0\n    if n > b:\n        val = (n - b) / s\n        # numerical safeguard\n        val = max(val, 0.0)\n        return np.sqrt(val)\n    else:\n        return 0.0\n\ndef ell_pp(n, b, s, theta):\n    # Second derivative of log-likelihood wrt theta\n    lam = b + s * theta**2\n    if lam <= 0:\n        return np.nan\n    term1 = 2.0 * s * (n - lam) / lam\n    term2 = 4.0 * s**2 * theta**2 * n / (lam**2)\n    return term1 - term2\n\ndef observed_info(n, b, s, theta_hat):\n    # Observed Fisher information J = -ell''\n    lpp = ell_pp(n, b, s, theta_hat)\n    if not np.isfinite(lpp):\n        return np.nan\n    return -lpp\n\ndef wald_interval(n, b, s, theta_hat, tol=1e-12):\n    J = observed_info(n, b, s, theta_hat)\n    if not np.isfinite(J) or J <= tol:\n        # Undefined curvature interval near saddle or negative curvature\n        return (0.0, float('inf'))\n    sigma = 1.0 / np.sqrt(J)\n    low = max(0.0, theta_hat - sigma)\n    high = theta_hat + sigma\n    return (low, high)\n\ndef q_stat(n, b, s, theta, theta_hat):\n    return 2.0 * (loglik(n, b, s, theta_hat) - loglik(n, b, s, theta))\n\ndef lr_interval(n, b, s, theta_hat, c=1.0):\n    # Find interval {theta >= 0 | q(theta) <= c}\n    # Lower endpoint\n    def f(th):\n        return q_stat(n, b, s, th, theta_hat) - c\n\n    # Handle lower endpoint\n    if theta_hat <= 0.0:\n        lower = 0.0\n    else:\n        q0 = q_stat(n, b, s, 0.0, theta_hat)\n        if q0 <= c:\n            lower = 0.0\n        else:\n            # Find root in (0, theta_hat)\n            a, bnd = 0.0, theta_hat\n            # f(bnd) = -c <= 0; f(a) >= 0\n            # For numerical stability, ensure sign change\n            fa = f(a + 1e-16)  # slightly above zero\n            fb = f(bnd)\n            # Ensure fb < 0 (should be true)\n            if fb >= 0:\n                # fallback: shrink bnd slightly\n                bnd = theta_hat * 0.999999\n                fb = f(bnd)\n            if fa <= 0 or fb >= 0:\n                # If still not a valid bracket due to numerical issues, set lower to 0\n                lower = 0.0\n            else:\n                lower = brentq(f, a + 1e-16, bnd, maxiter=1000, xtol=1e-12, rtol=1e-10)\n    # Upper endpoint\n    # Bracket by expanding until f(high) >= 0 (i.e., q(high) >= c)\n    lo = max(theta_hat, 0.0)\n    hi = max(lo * 2.0, 1e-9) if lo > 0 else 1e-6\n    # Ensure f(lo) < 0 (since q(lo) = 0 at theta_hat)\n    flo = f(lo)\n    # For boundary MLE lo = 0, q(0) may be 0; adjust slightly above 0\n    if lo == 0.0:\n        lo = 1e-12\n        flo = f(lo)\n    # Increase hi until sign change\n    fhi = f(hi)\n    iters = 0\n    while not np.isfinite(fhi) or fhi < 0.0:\n        hi *= 2.0\n        fhi = f(hi)\n        iters += 1\n        if iters > 200:\n            break\n    if not np.isfinite(fhi) or fhi < 0.0:\n        # As a last resort, use a very large upper bound\n        hi = max(1.0, hi)\n        for _ in range(1000):\n            hi *= 1.5\n            fhi = f(hi)\n            if np.isfinite(fhi) and fhi >= 0.0:\n                break\n    if np.isfinite(fhi) and flo < 0.0 and fhi >= 0.0:\n        upper = brentq(f, lo, hi, maxiter=1000, xtol=1e-12, rtol=1e-10)\n    else:\n        upper = float('inf')\n    return (max(0.0, lower), upper)\n\ndef bootstrap_interval(n, b, s, theta_hat, n_boot=10000, seed=12345, qlow=0.15865, qhigh=0.84135):\n    rng = np.random.default_rng(seed)\n    lam_hat = b + s * theta_hat**2\n    # Draw Poisson pseudo data\n    samples = rng.poisson(lam=lam_hat, size=n_boot)\n    # Compute bootstrap MLEs\n    thetas = np.sqrt(np.maximum(samples - b, 0.0) / s)\n    # Empirical quantiles\n    low = float(np.quantile(thetas, qlow, method='linear'))\n    high = float(np.quantile(thetas, qhigh, method='linear'))\n    return (max(0.0, low), max(0.0, high))\n\ndef format_float(x):\n    if np.isinf(x):\n        return \"inf\"\n    if np.isnan(x):\n        return \"nan\"\n    return f\"{x:.6f}\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is (n, b, s)\n    test_cases = [\n        (25, 25.0, 100.0),  # Case A: saddle regime (n ~ b)\n        (27, 25.0, 100.0),  # Case B: slight excess near saddle\n        (20, 25.0, 100.0),  # Case C: deficit with boundary MLE\n        (1, 0.5, 5.0),      # Case D: low-count regime\n    ]\n\n    results = []\n    for (n, b, s) in test_cases:\n        theta_hat = mle_theta(n, b, s)\n        w_low, w_high = wald_interval(n, b, s, theta_hat)\n        lr_low, lr_high = lr_interval(n, b, s, theta_hat, c=1.0)\n        # Increase bootstrap replicates modestly in low-count regimes to stabilize\n        n_boot = 20000 if n <= 1 else 10000\n        b_low, b_high = bootstrap_interval(n, b, s, theta_hat, n_boot=n_boot, seed=123456)\n        # Append formatted results\n        results.append([\n            format_float(w_low), format_float(w_high),\n            format_float(lr_low), format_float(lr_high),\n            format_float(b_low), format_float(b_high)\n        ])\n\n    # Final print statement in the exact required format.\n    # We need a single-line list of lists of 6 floats (as strings).\n    # Construct the string manually to ensure exact formatting.\n    inner_strs = []\n    for res in results:\n        inner_strs.append(\"[\" + \",\".join(res) + \"]\")\n    print(\"[\" + \",\".join(inner_strs) + \"]\")\n\nsolve()\n```"
        }
    ]
}