{
    "hands_on_practices": [
        {
            "introduction": "Before conducting any physics analysis, it is crucial to ensure the integrity and self-consistency of the underlying data. An event record is more than a simple list of particles; it is a structured graph representing a causal history of interactions and decays. This practice challenges you to build a comprehensive validator that enforces the fundamental 'grammar' of an event record, checking for logical status transitions, consistent vertex-particle links, local four-momentum conservation, and causal acyclicity . Mastering these checks provides a deep understanding of how physical principles are encoded in data.",
            "id": "3513428",
            "problem": "You are given the task of designing and implementing a programmatic validator for high-energy physics event records that represent particle histories as a graph of vertices (interactions or decays) connected by particles. The validator must enforce logically and physically consistent status transitions and related constraints. The event data model follows a simplified but standard model inspired by formats such as HepMC and HEPEVT:\n\n- Each particle has:\n  - A unique integer identifier $p \\in \\mathbb{Z}$.\n  - An integer status $s \\in \\{-1,1,2\\}$, where $s=-1$ denotes an incoming beam particle, $s=1$ denotes a final-state stable particle, and $s=2$ denotes an intermediate decaying or propagating particle.\n  - A production vertex identifier $v_{\\mathrm{prod}} \\in \\mathbb{Z} \\cup \\{\\varnothing\\}$ and an end vertex identifier $v_{\\mathrm{end}} \\in \\mathbb{Z} \\cup \\{\\varnothing\\}$.\n  - A four-momentum $p^{\\mu}=(E,p_x,p_y,p_z)$ in natural units with the speed of light $c$ set to $1$, where all components are given in gigaelectronvolts (GeV). That is, $E$ is in $\\mathrm{GeV}$ and $(p_x,p_y,p_z)$ are in $\\mathrm{GeV}$.\n\n- Each vertex has:\n  - A unique integer identifier $v \\in \\mathbb{Z}$.\n  - A list of incoming particle identifiers and a list of outgoing particle identifiers.\n\nThe validator must enforce the following constraints derived from fundamental principles:\n\n1. Status-vertex incidence rules:\n   - If $s=-1$ (incoming), then $v_{\\mathrm{prod}}=\\varnothing$ and $v_{\\mathrm{end}}\\neq \\varnothing$.\n   - If $s=1$ (final-state), then $v_{\\mathrm{prod}}\\neq \\varnothing$ and $v_{\\mathrm{end}}=\\varnothing$.\n   - If $s=2$ (intermediate), then $v_{\\mathrm{prod}}\\neq \\varnothing$ and $v_{\\mathrm{end}}\\neq \\varnothing$.\n\n2. Vertex-particle consistency:\n   - For every vertex $v$, the declared incoming particle set must equal the set $\\{p \\mid v_{\\mathrm{end}}(p)=v\\}$, and the declared outgoing particle set must equal the set $\\{p \\mid v_{\\mathrm{prod}}(p)=v\\}$.\n   - For every vertex $v$, incoming particles must have $s \\in \\{-1,2\\}$ and outgoing particles must have $s \\in \\{1,2\\}$.\n   - Every vertex must have at least one incoming and at least one outgoing particle.\n\n3. Four-momentum conservation at each vertex $v$:\n   - Let $\\sum_{\\mathrm{in}} p^{\\mu}$ be the sum over incoming four-momenta and $\\sum_{\\mathrm{out}} p^{\\mu}$ be the sum over outgoing four-momenta. The conservation condition is\n     $$\\left|\\sum_{\\mathrm{in}} p^{\\mu} - \\sum_{\\mathrm{out}} p^{\\mu}\\right| \\le \\varepsilon \\quad \\text{component-wise},$$\n     for a given tolerance $\\varepsilon > 0$, interpreted as a bound in $\\mathrm{GeV}$ for each component. Use $\\varepsilon = 10^{-9}\\,\\mathrm{GeV}$.\n\n4. Causal acyclicity of the particle-vertex graph:\n   - Construct a directed graph where for each vertex $v$, for each incoming particle $p_{\\mathrm{in}}$ and each outgoing particle $p_{\\mathrm{out}}$, add an edge $p_{\\mathrm{in}} \\to p_{\\mathrm{out}}$. The graph must be a directed acyclic graph (DAG). Any cycle violates causality.\n\nYour program must implement these checks and output whether each provided event is valid. The output for each event must be a boolean, with $ \\mathrm{True}$ indicating that the event passes all checks and $ \\mathrm{False}$ otherwise.\n\nAll energies and momentum components are to be interpreted and validated in $\\mathrm{GeV}$, using natural units with $c=1$.\n\nTest Suite:\nImplement your validator against the following concrete events. Each event is specified by a list of particles and a list of vertices. Each particle record is a tuple $(\\text{id}, s, v_{\\mathrm{prod}}, v_{\\mathrm{end}}, (E,p_x,p_y,p_z))$, and each vertex record is a tuple $(\\text{id}, [\\text{incoming}], [\\text{outgoing}])$.\n\n- Event $1$ (happy path, valid):\n  - Particles:\n    - $(1,-1,\\varnothing,101,(100,0,0,100))$\n    - $(2,-1,\\varnothing,101,(100,0,0,-100))$\n    - $(3,2,101,102,(120,0,0,10))$\n    - $(4,1,101,\\varnothing,(80,0,0,-10))$\n    - $(5,1,102,\\varnothing,(60,10,0,5))$\n    - $(6,1,102,\\varnothing,(60,-10,0,5))$\n  - Vertices:\n    - $(101,[1,2],[3,4])$\n    - $(102,[3],[5,6])$\n\n- Event $2$ (invalid: final-state particle has an end vertex and participates as incoming at that vertex):\n  - Particles:\n    - $(11,-1,\\varnothing,201,(10,0,0,0))$\n    - $(10,1,201,202,(10,0,0,0))$\n  - Vertices:\n    - $(201,[11],[10])$\n    - $(202,[10],[])$\n\n- Event $3$ (invalid: intermediate without end vertex):\n  - Particles:\n    - $(20,-1,\\varnothing,301,(50,0,0,0))$\n    - $(21,2,301,\\varnothing,(50,0,0,0))$\n  - Vertices:\n    - $(301,[20],[21])$\n\n- Event $4$ (invalid: incoming with a production vertex):\n  - Particles:\n    - $(30,-1,401,402,(5,0,0,0))$\n  - Vertices:\n    - $(401,[],[30])$\n    - $(402,[30],[])$\n\n- Event $5$ (invalid: vertex-particle mismatch and inconsistent production vertex for an outgoing particle):\n  - Particles:\n    - $(40,-1,\\varnothing,501,(10,0,0,0))$\n    - $(41,2,501,502,(10,0,0,0))$\n    - $(42,1,503,\\varnothing,(10,0,0,0))$\n  - Vertices:\n    - $(501,[40],[41])$\n    - $(502,[41],[42])$\n    - $(503,[],[42])$\n\n- Event $6$ (invalid: four-momentum not conserved at a vertex beyond tolerance $\\varepsilon$):\n  - Particles:\n    - $(50,-1,\\varnothing,601,(10,0,0,0))$\n    - $(51,1,601,\\varnothing,(9,0,0,0))$\n  - Vertices:\n    - $(601,[50],[51])$\n\n- Event $7$ (invalid: causal cycle in the graph):\n  - Particles:\n    - $(60,2,701,702,(5,0,0,0))$\n    - $(61,2,702,701,(5,0,0,0))$\n  - Vertices:\n    - $(701,[61],[60])$\n    - $(702,[60],[61])$\n\nYour program must evaluate these $7$ events and produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, the output format must be exactly of the form $[\\mathrm{result}_1,\\mathrm{result}_2,\\ldots,\\mathrm{result}_7]$, where each $\\mathrm{result}_i$ is a boolean.",
            "solution": "The problem statement has been evaluated and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to construct a unique and verifiable solution. We will now proceed with the design and explanation of the validator.\n\nThe programmatic validation of a high-energy physics event record requires a series of checks to ensure the event's logical self-consistency and its adherence to fundamental physical laws. We will design a validator that systematically applies four categories of constraints: (1) status-vertex incidence rules, (2) vertex-particle graph consistency, (3) four-momentum conservation at each vertex, and (4) causal acyclicity. An event is deemed valid only if it passes all checks.\n\nFirst, for efficient access, the event data, provided as lists of particles and vertices, will be parsed into dictionaries mapping unique identifiers to their respective data structures. A particle with identifier $p$ is denoted as $P_p$, and a vertex with identifier $v$ as $V_v$. The four-momentum of particle $p$ is $p^{\\mu}$. The null vertex identifier, $\\varnothing$, is represented by a null value in the implementation.\n\n**1. Status-Vertex Incidence Rules**\n\nThis set of rules formalizes the lifecycle of a particle within an event. A particle's status, $s$, dictates its position in the event's history graph.\n\n*   **Principle**: Particles are classified by their role. An incoming beam particle ($s=-1$) originates from outside the event, thus it has no production vertex ($v_{\\mathrm{prod}}=\\varnothing$), but it must interact at some vertex ($v_{\\mathrm{end}}\\neq \\varnothing$). A final-state particle ($s=1$) is stable and leaves the interaction region, so it must be produced at a vertex ($v_{\\mathrm{prod}}\\neq \\varnothing$) but has no subsequent decay or interaction vertex ($v_{\\mathrm{end}}=\\varnothing$). An intermediate particle ($s=2$) is produced and subsequently decays or interacts, so it must be bracketed by both a production and an end vertex ($v_{\\mathrm{prod}}\\neq \\varnothing$ and $v_{\\mathrm{end}}\\neq \\varnothing$).\n\n*   **Algorithm**: The validator iterates through every particle $P_p$ in the event. For each particle, it checks the following logical conditions:\n    *   If $s(P_p) = -1$, it must be that $v_{\\mathrm{prod}}(P_p) = \\varnothing$ and $v_{\\mathrm{end}}(P_p) \\neq \\varnothing$.\n    *   If $s(P_p) = 1$, it must be that $v_{\\mathrm{prod}}(P_p) \\neq \\varnothing$ and $v_{\\mathrm{end}}(P_p) = \\varnothing$.\n    *   If $s(P_p) = 2$, it must be that $v_{\\mathrm{prod}}(P_p) \\neq \\varnothing$ and $v_{\\mathrm{end}}(P_p) \\neq \\varnothing$.\n    A failure of any of these conditions for any particle invalidates the entire event.\n\n**2. Vertex-Particle Consistency**\n\nThis check ensures that the particle and vertex records provide a consistent and complete description of the event's graph topology.\n\n*   **Principle**: The event record contains two perspectives on the event graph: the particles' vertex pointers ($v_{\\mathrm{prod}}, v_{\\mathrm{end}}$) and the vertices' particle lists (incoming, outgoing). These two perspectives must be mutually consistent. Furthermore, a vertex represents a physical interaction, which must involve particles both entering and exiting, and the status of these particles must be appropriate for their role.\n\n*   **Algorithm**: The validator iterates through every vertex $V_v$. For each vertex, it performs four checks:\n    1.  **Topological Consistency**: It constructs the set of actual incoming particles, $I_{actual} = \\{p \\mid v_{\\mathrm{end}}(P_p) = v\\}$, and the set of actual outgoing particles, $O_{actual} = \\{p \\mid v_{\\mathrm{prod}}(P_p) = v\\}$. It then verifies that these sets are identical to the declared incoming and outgoing particle lists provided with the vertex data, $I_{declared}$ and $O_{declared}$. That is, $I_{actual} = \\text{set}(I_{declared})$ and $O_{actual} = \\text{set}(O_{declared})$.\n    2.  **Particle Status at Vertex**: For every particle $p \\in I_{declared}$, the validator checks that its status $s(P_p) \\in \\{-1, 2\\}$. For every particle $p' \\in O_{declared}$, it checks that its status $s(P_{p'}) \\in \\{1, 2\\}$. A final-state particle ($s=1$) cannot be incoming to a vertex, and an initial-state particle ($s=-1$) cannot be outgoing from one.\n    3.  **Vertex Population**: A physical vertex must represent an interaction or decay. Therefore, it must have at least one incoming and at least one outgoing particle. The validator checks if $|I_{declared}| \\ge 1$ and $|O_{declared}| \\ge 1$.\n    Any discrepancy found in these checks for any vertex marks the event as invalid.\n\n**3. Four-Momentum Conservation**\n\nThis is a stringent test based on one of the most fundamental conservation laws in physics.\n\n*   **Principle**: In the absence of external fields, the total four-momentum of an isolated system is conserved. At each vertex, which represents a local interaction, the sum of the four-momenta of all incoming particles must equal the sum of the four-momenta of all outgoing particles. This stems from the invariance of physical laws under spacetime translation (Noether's theorem).\n\n*   **Algorithm**: For each vertex $V_v$, the validator calculates the sum of four-momenta for all incoming particles, $\\sum_{\\mathrm{in}} p^{\\mu} = \\sum_{p \\in I_{declared}} p^{\\mu}(P_p)$, and for all outgoing particles, $\\sum_{\\mathrm{out}} p^{\\mu} = \\sum_{p' \\in O_{declared}} p^{\\mu}(P_{p'})$. Due to finite floating-point precision, a direct equality check is inappropriate. Instead, the condition is that the absolute difference for each of the four components (energy and three momentum components) must be less than or equal to a small tolerance, $\\varepsilon$. The problem specifies $\\varepsilon = 10^{-9} \\, \\mathrm{GeV}$.\n    $$ \\left| \\left(\\sum_{\\mathrm{in}} p^{\\mu}\\right)_k - \\left(\\sum_{\\mathrm{out}} p^{\\mu}\\right)_k \\right| \\le \\varepsilon $$\n    for each component $k \\in \\{0, 1, 2, 3\\}$. A violation of this condition at any vertex invalidates the event.\n\n**4. Causal Acyclicity**\n\nThis constraint ensures that the event history is causally possible.\n\n*   **Principle**: Causality dictates that an effect cannot precede its cause. In the context of a particle history graph, this means a particle cannot be its own ancestor. For example, a particle cannot decay into products which then interact to re-form the original particle. The sequence of productions and decays must form a Directed Acyclic Graph (DAG).\n\n*   **Algorithm**: A directed graph is constructed where the nodes represent the particles. An edge $p_{\\mathrm{in}} \\to p_{\\mathrm{out}}$ is added if there exists a vertex $V_v$ where $p_{\\mathrm{in}}$ is an incoming particle and $p_{\\mathrm{out}}$ is an outgoing particle. This graph represents the flow of causality and lineage. To check for cycles, a Depth-First Search (DFS) algorithm is employed. During the traversal, nodes are tracked using three states: unvisited, visiting (currently in the recursion stack), and visited (all descendants explored). If the DFS encounters a node that is currently in the 'visiting' state, a back edge has been found, which indicates a cycle. The presence of any cycle violates causality and invalidates the event.\n\nBy methodically applying these four layers of validation, the program can rigorously determine the physical and logical validity of any given event record.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the validator.\n    \"\"\"\n    \n    # Represents the null vertex identifier ø.\n    NULL_VERTEX = None\n\n    # Test cases as defined in the problem statement.\n    test_cases = [\n        # Event 1 (valid)\n        {\n            \"particles\": [\n                (1, -1, NULL_VERTEX, 101, (100, 0, 0, 100)),\n                (2, -1, NULL_VERTEX, 101, (100, 0, 0, -100)),\n                (3, 2, 101, 102, (120, 0, 0, 10)),\n                (4, 1, 101, NULL_VERTEX, (80, 0, 0, -10)),\n                (5, 1, 102, NULL_VERTEX, (60, 10, 0, 5)),\n                (6, 1, 102, NULL_VERTEX, (60, -10, 0, 5)),\n            ],\n            \"vertices\": [\n                (101, [1, 2], [3, 4]),\n                (102, [3], [5, 6]),\n            ]\n        },\n        # Event 2 (invalid: final-state particle has end vertex)\n        {\n            \"particles\": [\n                (11, -1, NULL_VERTEX, 201, (10, 0, 0, 0)),\n                (10, 1, 201, 202, (10, 0, 0, 0)),\n            ],\n            \"vertices\": [\n                (201, [11], [10]),\n                (202, [10], []),\n            ]\n        },\n        # Event 3 (invalid: intermediate without end vertex)\n        {\n            \"particles\": [\n                (20, -1, NULL_VERTEX, 301, (50, 0, 0, 0)),\n                (21, 2, 301, NULL_VERTEX, (50, 0, 0, 0)),\n            ],\n            \"vertices\": [\n                (301, [20], [21]),\n            ]\n        },\n        # Event 4 (invalid: incoming with production vertex)\n        {\n            \"particles\": [\n                (30, -1, 401, 402, (5, 0, 0, 0)),\n            ],\n            \"vertices\": [\n                (401, [], [30]),\n                (402, [30], []),\n            ]\n        },\n        # Event 5 (invalid: vertex-particle mismatch)\n        {\n            \"particles\": [\n                (40, -1, NULL_VERTEX, 501, (10, 0, 0, 0)),\n                (41, 2, 501, 502, (10, 0, 0, 0)),\n                (42, 1, 503, NULL_VERTEX, (10, 0, 0, 0)),\n            ],\n            \"vertices\": [\n                (501, [40], [41]),\n                (502, [41], [42]),\n                (503, [], [42]),\n            ]\n        },\n        # Event 6 (invalid: four-momentum not conserved)\n        {\n            \"particles\": [\n                (50, -1, NULL_VERTEX, 601, (10, 0, 0, 0)),\n                (51, 1, 601, NULL_VERTEX, (9, 0, 0, 0)),\n            ],\n            \"vertices\": [\n                (601, [50], [51]),\n            ]\n        },\n        # Event 7 (invalid: causal cycle)\n        {\n            \"particles\": [\n                (60, 2, 701, 702, (5, 0, 0, 0)),\n                (61, 2, 702, 701, (5, 0, 0, 0)),\n            ],\n            \"vertices\": [\n                (701, [61], [60]),\n                (702, [60], [61]),\n            ]\n        }\n    ]\n\n    results = [validate_event(case[\"particles\"], case[\"vertices\"], NULL_VERTEX) for case in test_cases]\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef validate_event(particles_list, vertices_list, null_vertex_id):\n    \"\"\"\n    Performs all validation checks for a single event.\n    \"\"\"\n    \n    # Pre-processing: Create efficient look-up maps\n    particles_map = {p[0]: {\"id\": p[0], \"s\": p[1], \"v_prod\": p[2], \"v_end\": p[3], \"pmu\": np.array(p[4])} for p in particles_list}\n    vertices_map = {v[0]: {\"id\": v[0], \"in\": v[1], \"out\": v[2]} for v in vertices_list}\n\n    # 1. Status-vertex incidence rules\n    for p_id, p_data in particles_map.items():\n        s, v_prod, v_end = p_data[\"s\"], p_data[\"v_prod\"], p_data[\"v_end\"]\n        if s == -1 and not (v_prod == null_vertex_id and v_end != null_vertex_id): return False\n        if s == 1 and not (v_prod != null_vertex_id and v_end == null_vertex_id): return False\n        if s == 2 and not (v_prod != null_vertex_id and v_end != null_vertex_id): return False\n\n    # 2. Vertex-particle consistency\n    for v_id, v_data in vertices_map.items():\n        # Check vertex population\n        if not (v_data[\"in\"] and v_data[\"out\"]): return False\n\n        # Compare declared vs actual particle sets at vertex\n        actual_in = {p_id for p_id, p in particles_map.items() if p[\"v_end\"] == v_id}\n        actual_out = {p_id for p_id, p in particles_map.items() if p[\"v_prod\"] == v_id}\n        if set(v_data[\"in\"]) != actual_in or set(v_data[\"out\"]) != actual_out: return False\n\n        # Check status of particles at vertex\n        for p_id in v_data[\"in\"]:\n            if particles_map[p_id][\"s\"] not in [-1, 2]: return False\n        for p_id in v_data[\"out\"]:\n            if particles_map[p_id][\"s\"] not in [1, 2]: return False\n    \n    # 3. Four-momentum conservation\n    epsilon = 1e-9\n    for v_id, v_data in vertices_map.items():\n        sum_in_pmu = np.sum([particles_map[p_id][\"pmu\"] for p_id in v_data[\"in\"]], axis=0)\n        sum_out_pmu = np.sum([particles_map[p_id][\"pmu\"] for p_id in v_data[\"out\"]], axis=0)\n        if not np.all(np.abs(sum_in_pmu - sum_out_pmu) <= epsilon): return False\n\n    # 4. Causal acyclicity\n    # Build adjacency list for the particle causality graph\n    adj = {p_id: [] for p_id in particles_map}\n    for v_data in vertices_map.values():\n        for p_in in v_data[\"in\"]:\n            for p_out in v_data[\"out\"]:\n                adj[p_in].append(p_out)\n    \n    # DFS-based cycle detection\n    # States: 0 = unvisited, 1 = visiting, 2 = visited\n    path = set()  # Corresponds to state 1 (visiting)\n    visited = set() # Corresponds to state 2 (visited)\n\n    def has_cycle(node):\n        path.add(node)\n        for neighbor in adj.get(node, []):\n            if neighbor in path:\n                return True\n            if neighbor not in visited:\n                if has_cycle(neighbor):\n                    return True\n        path.remove(node)\n        visited.add(node)\n        return False\n\n    for node in particles_map:\n        if node not in visited:\n            if has_cycle(node):\n                return False\n\n    return True\n\nsolve()\n```"
        },
        {
            "introduction": "In collaborative high-energy physics research, it is common to encounter event data from various generators and experiments, often stored in different formats or versions. This exercise simulates the practical and essential task of creating a converter to bridge these different representations, inspired by the transition between HepMC versions. You will implement logic to handle unit conversions, map particle status codes between conventions, and, most importantly, preserve the event's directed graph structure that encodes the particle history . This practice develops key software engineering skills required for building robust physics analysis workflows.",
            "id": "3513391",
            "problem": "You are given the task of implementing a converter between two in-memory representations of high-energy physics event records that abstract the common features of the High Energy Physics Monte Carlo (HepMC) version $2$ and version $3$ formats. Each event is a directed bipartite graph with vertex nodes (production or decay points) and particle nodes (edges connecting vertices). The input representation (HepMC$2$-like) declares its units for momentum-energy and space-time separately. The output representation (HepMC$3$-like) must preserve the graph structure and explicitly convert all numerical quantities into fixed target units. The converter must also map integer particle status codes from the input convention to the output convention via a well-defined function. You must implement a runnable program that performs this conversion for a small test suite of events, checks three invariants for each event, and outputs the results.\n\nFoundational base and definitions to use:\n- A HepMC event can be idealized as a directed bipartite graph $G = (V \\cup P, E)$ with $V$ the set of vertices, $P$ the set of particles, and $E$ edges connecting $V$ to $P$ and $P$ to $V$ such that there are no edges between two $V$ or two $P$ nodes. Each particle $p \\in P$ has at most one production vertex $\\mathrm{prod}(p) \\in V$ and at most one end vertex $\\mathrm{end}(p) \\in V$. Each vertex $v \\in V$ has an incoming set $\\mathrm{in}(v) \\subseteq P$ and an outgoing set $\\mathrm{out}(v) \\subseteq P$ that satisfy set equalities induced by particle endpoints: $\\mathrm{in}(v) = \\{ p \\in P \\mid \\mathrm{end}(p) = v \\}$ and $\\mathrm{out}(v) = \\{ p \\in P \\mid \\mathrm{prod}(p) = v \\}$.\n- Four-momentum is represented as a tuple $(p_x, p_y, p_z, E)$ and four-position as $(x, y, z, t)$. In natural units with speed of light $c = 1$, the unit for energy and momentum is common. Unit conversions are pure scalings: if the input momentum-energy unit is $\\mathsf{MEV}$ and the target is $\\mathsf{GEV}$ then the scale factor $s_p$ is $s_p = 10^{-3}$; if the input is $\\mathsf{GEV}$, then $s_p = 1$. For space-time, if the input length unit is $\\mathsf{CM}$ and the target is $\\mathsf{MM}$ then the scale factor $s_\\ell$ is $s_\\ell = 10$, and if the input is $\\mathsf{MM}$ then $s_\\ell = 1$. Apply $s_p$ to all four-momentum components and $s_\\ell$ to all four-position components.\n- Particle status codes are integers. You must define a deterministic mapping function $f: \\mathbb{Z} \\to \\mathbb{Z}$ for the conversion. You are required to choose a convention that resolves the ambiguity of nonstandard codes while preserving typical semantics such as final-state and decayed/intermediate categories. You must document your choice inside your code as comments.\n\nYour target output representation must:\n- Use fixed target units: $\\mathsf{GEV}$ for momentum-energy and $\\mathsf{MM}$ for space-time.\n- Preserve the event graph structure exactly: for every particle and vertex, the relationships $\\mathrm{prod}(\\cdot)$ and $\\mathrm{end}(\\cdot)$ and the derived sets $\\mathrm{in}(\\cdot)$, $\\mathrm{out}(\\cdot)$ must match between input and output modulo units and status code values.\n- Map status codes using your defined $f(s)$ and apply this to every particle.\n\nFor each event in the test suite below, your program must compute and append to a single flat list three boolean results in this order:\n- Units-correct: a boolean indicating whether all four-momenta and four-positions have been converted to the specified target units by the correct scale factors, within a relative tolerance of $10^{-12}$.\n- Status-correct: a boolean indicating whether every particle status in the output equals $f(s)$ applied to the input status $s$.\n- Graph-preserved: a boolean indicating whether the input and output graphs have identical adjacency in terms of production and end vertices for every particle, and incoming and outgoing particle sets for every vertex.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,...]\"). The elements must be booleans.\n\nTest suite specification:\nImplement the test suite as three concrete HepMC$2$-like events with the following exact content. All numerical values must be interpreted in their declared input units and converted to the target units in the output.\n\nEvent A:\n- Input units: momentum-energy unit $\\mathsf{MEV}$, length unit $\\mathsf{MM}$.\n- Vertices:\n  - Vertex $v_1$: identifier $1$, position $(x,y,z,t) = (0.0, 0.0, 0.0, 0.0)$.\n  - Vertex $v_2$: identifier $2$, position $(x,y,z,t) = (0.0, 0.0, 1.0, 1.0)$.\n- Particles:\n  - Particle $p_1$: identifier $1$, status $2$, four-momentum $(p_x,p_y,p_z,E) = (0.0, 0.0, 1000.0, 1100.0)$, $\\mathrm{prod}(p_1) = v_1$, $\\mathrm{end}(p_1) = v_2$.\n  - Particle $p_2$: identifier $2$, status $1$, four-momentum $(100.0, 0.0, 400.0, 412.3105626)$, $\\mathrm{prod}(p_2) = v_2$, $\\mathrm{end}(p_2) = \\varnothing$.\n  - Particle $p_3$: identifier $3$, status $1$, four-momentum $(-100.0, 0.0, 600.0, 687.6894374)$, $\\mathrm{prod}(p_3) = v_2$, $\\mathrm{end}(p_3) = \\varnothing$.\n\nEvent B:\n- Input units: momentum-energy unit $\\mathsf{GEV}$, length unit $\\mathsf{CM}$.\n- Vertices:\n  - Vertex $v_{10}$: identifier $10$, position $(0.0, 0.0, 0.0, 0.0)$.\n  - Vertex $v_{11}$: identifier $11$, position $(0.1, -0.2, 0.3, 0.0)$.\n- Particles:\n  - Particle $p_{12}$: identifier $12$, status $2$, four-momentum $(0.3, 0.4, 0.0, 0.5)$, $\\mathrm{prod}(p_{12}) = v_{10}$, $\\mathrm{end}(p_{12}) = v_{11}$.\n  - Particle $p_{13}$: identifier $13$, status $1$, four-momentum $(0.1, 0.2, 0.0, 0.2236067977)$, $\\mathrm{prod}(p_{13}) = v_{11}$, $\\mathrm{end}(p_{13}) = \\varnothing$.\n  - Particle $p_{14}$: identifier $14$, status $1$, four-momentum $(0.2, 0.2, 0.0, 0.2763932023)$, $\\mathrm{prod}(p_{14}) = v_{11}$, $\\mathrm{end}(p_{14}) = \\varnothing$.\n  - Particle $p_{10}$: identifier $10$, status $3$, four-momentum $(0.0, 0.0, 0.0, 0.0)$, with no production or end vertex attached.\n\nEvent C:\n- Input units: momentum-energy unit $\\mathsf{MEV}$, length unit $\\mathsf{CM}$.\n- Vertices:\n  - Vertex $v_{100}$: identifier $100$, position $(0.0, 0.0, 0.0, 0.0)$.\n  - Vertex $v_{101}$: identifier $101$, position $(0.0, 0.0, 0.01, 0.01)$.\n  - Vertex $v_{102}$: identifier $102$, position $(0.0, 0.0, 0.02, 0.02)$.\n- Particles:\n  - Particle $p_{100}$: identifier $100$, status $2$, four-momentum $(0.0, 0.0, 1500.0, 1600.0)$, $\\mathrm{prod}(p_{100}) = v_{100}$, $\\mathrm{end}(p_{100}) = v_{101}$.\n  - Particle $p_{101}$: identifier $101$, status $2$, four-momentum $(0.0, 0.0, 1000.0, 1100.0)$, $\\mathrm{prod}(p_{101}) = v_{101}$, $\\mathrm{end}(p_{101}) = v_{102}$.\n  - Particle $p_{102}$: identifier $102$, status $1$, four-momentum $(0.0, 0.0, 500.0, 500.0)$, $\\mathrm{prod}(p_{102}) = v_{102}$, $\\mathrm{end}(p_{102}) = \\varnothing$.\n  - Particle $p_{103}$: identifier $103$, status $1$, four-momentum $(0.0, 0.0, 500.0, 600.0)$, $\\mathrm{prod}(p_{103}) = v_{102}$, $\\mathrm{end}(p_{103}) = \\varnothing$.\n\nUnit conversion targets and tolerances:\n- Convert all $(p_x, p_y, p_z, E)$ to $\\mathsf{GEV}$.\n- Convert all $(x, y, z, t)$ to $\\mathsf{MM}$.\n- Use absolute or relative tolerance of $10^{-12}$ when comparing floating-point quantities.\n\nStatus mapping convention:\n- Define and use a mapping $f(s)$ that preserves the meaning of final-state and decayed/intermediate codes where $s \\in \\{1,2\\}$ and assigns a consistent code for documentation or other non-propagating entries. For all other $s$, provide a well-documented conventional choice. You must describe $f$ in your source code comments.\n\nOutput format requirement:\n- Your program must produce a single line containing one flat list with $9$ booleans in the exact order: for Event A $(\\text{Units-correct}, \\text{Status-correct}, \\text{Graph-preserved})$, followed by Event B in the same order, followed by Event C in the same order. The printed line must be in the format \"[val1,val2,...,val9]\".\n\nAll computations must be performed with internal floating-point arithmetic consistent with the unit rules stated. No file input or output is permitted; construct the test events inside the program. No external libraries beyond the specified ones are allowed.",
            "solution": "The problem requires the implementation of a converter for high-energy physics event records. This task can be deconstructed into three primary components: data structure definition, data transformation, and verification. The solution is designed following a principle-based approach, ensuring correctness and clarity at each step.\n\nFirst, to formalize the problem, we establish a robust in-memory representation for the event data. An event is a directed bipartite graph $G = (V \\cup P, E)$, where $V$ is a set of vertices and $P$ is a set of particles. We define Python classes to model these entities. The `Vertex` class stores a unique integer identifier, a $4$-dimensional position vector $(x, y, z, t)$, and lists of incoming and outgoing particles. The `Particle` class stores a unique integer identifier, a particle status code, a $4$-dimensional momentum vector $(p_x, p_y, p_z, E)$, and references to its production and end vertices. An `Event` class aggregates the vertices and particles, along with the event-wide momentum-energy and length units. Using NumPy arrays for the $4$-vectors provides a computationally efficient way to handle numerical operations.\n\nSecond, we implement the conversion logic. This process generates a new `Event` object in the target representation from an input `Event`. The conversion involves three distinct transformations applied to the data:\n\n1.  **Unit Conversion:** All numerical quantities must be converted to the target units of Giga-electron Volts ($\\mathsf{GEV}$) for momentum-energy and millimeters ($\\mathsf{MM}$) for space-time. This is achieved by applying scaling factors. The momentum-energy scaling factor, $s_p$, is $10^{-3}$ if converting from $\\mathsf{MEV}$ to $\\mathsf{GEV}$, and $1$ if the input is already in $\\mathsf{GEV}$. The length scaling factor, $s_\\ell$, is $10$ if converting from $\\mathsf{CM}$ to $\\mathsf{MM}$, and $1$ if the input is already in $\\mathsf{MM}$. For each particle, its four-momentum vector is multiplied by $s_p$. For each vertex, its four-position vector is multiplied by $s_\\ell$.\n\n2.  **Status Code Mapping:** Particle status codes are integers that denote the particle's role in the event history. We must define a deterministic mapping function $f: \\mathbb{Z} \\to \\mathbb{Z}$ to convert from the input convention to the output convention. A standard and logical choice for $f(s)$ is to preserve the conventional meanings of common codes while handling others gracefully. We define $f(1) = 1$ for final-state particles, $f(2) = 2$ for decayed intermediate particles, and $f(3) = 0$ for documentation entries. All other non-standard codes are also mapped to $0$, a common practice for codes without a direct physical interpretation in the target scheme. This function $f(s)$ is applied to the status code of every particle during the conversion.\n\n3.  **Graph Structure Preservation:** The event's history is encoded in its graph topology, specifically the connections between particles and vertices. The relationships $\\mathrm{prod}(p)$ (the production vertex of particle $p$) and $\\mathrm{end}(p)$ (the end vertex of particle $p$) must be preserved. The conversion process achieves this by first creating a complete set of new particles and vertices with transformed numerical data, and then re-establishing the links between them. For each new particle, its production and end vertex references are set to the new vertices that correspond (by unique identifier) to the original vertices. Since the sets of incoming particles $\\mathrm{in}(v)$ and outgoing particles $\\mathrm{out}(v)$ for a vertex $v$ are fully determined by the $\\mathrm{prod}(\\cdot)$ and $\\mathrm{end}(\\cdot)$ relations of all particles, preserving these relations guarantees the entire graph structure is preserved.\n\nThird, a set of three invariants is checked for each converted event to validate the correctness of the transformation. The results of these checks, which are boolean values, form the final output.\n\n1.  **Units-correct:** This invariant is verified by iterating through all particles and vertices of the output event. For each entity, we check if its four-vector is numerically equal to the original four-vector multiplied by the appropriate scaling factor ($s_p$ or $s_\\ell$). The comparison is performed using `numpy.allclose`, which robustly handles floating-point arithmetic with a specified relative tolerance of $10^{-12}$. The invariant holds only if the check passes for all entities.\n\n2.  **Status-correct:** This invariant is confirmed by iterating through all particles in the output event and ensuring that each particle's status code is equal to the result of applying the mapping function $f(s)$ to the status code of the corresponding particle in the input event.\n\n3.  **Graph-preserved:** This invariant is tested by verifying the topological integrity. For each particle in the input event, we find its counterpart in the output event by its identifier. We then compare the identifiers of their respective production and end vertices. The graph is considered preserved if, for every particle, the production and end vertex identifiers in the output event match those in the input event. This includes correctly handling particles that may lack a production or end vertex (e.g., initial or final state particles without a recorded decay vertex).\n\nThe overall implementation processes the three test events from the provided suite, performs the conversion and verification for each, and aggregates the nine resulting boolean values into a single flat list for the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# A deterministic function to map particle status codes.\n# This function must be documented as per the problem description.\n# Convention chosen:\n# - Status 1 (final-state) maps to 1.\n# - Status 2 (intermediate/decayed) maps to 2.\n# - Status 3 (documentation) maps to 0, a common convention for non-physical entries.\n# - All other status codes are mapped to 0 as a catch-all for undefined/non-standard codes.\ndef status_map(s: int) -> int:\n    \"\"\"Maps an input status code to an output status code.\"\"\"\n    if s == 1:\n        return 1\n    if s == 2:\n        return 2\n    if s == 3:\n        return 0\n    return 0\n\nclass Vertex:\n    \"\"\"Represents a vertex in the event graph.\"\"\"\n    def __init__(self, id_val: int, pos: tuple):\n        self.id = id_val\n        self.pos = np.array(pos, dtype=float)\n        self.incoming_particles = []\n        self.outgoing_particles = []\n\n    def __repr__(self) -> str:\n        return f\"Vertex(id={self.id}, pos={self.pos})\"\n\nclass Particle:\n    \"\"\"Represents a particle in the event graph.\"\"\"\n    def __init__(self, id_val: int, status: int, mom: tuple):\n        self.id = id_val\n        self.status = status\n        self.mom = np.array(mom, dtype=float)\n        self.prod_vertex = None\n        self.end_vertex = None\n\n    def __repr__(self) -> str:\n        prod_id = self.prod_vertex.id if self.prod_vertex else None\n        end_id = self.end_vertex.id if self.end_vertex else None\n        return f\"Particle(id={self.id}, status={self.status}, mom={self.mom}, prod={prod_id}, end={end_id})\"\n\nclass Event:\n    \"\"\"Represents a single event, containing vertices and particles.\"\"\"\n    def __init__(self, mom_unit: str, len_unit: str):\n        self.mom_unit = mom_unit\n        self.len_unit = len_unit\n        self.vertices = {}  # map id -> Vertex\n        self.particles = {} # map id -> Particle\n\n    def add_vertex(self, vertex: Vertex):\n        self.vertices[vertex.id] = vertex\n\n    def add_particle(self, particle: Particle):\n        self.particles[particle.id] = particle\n\ndef convert_event(input_event: Event) -> Event:\n    \"\"\"Converts an event to the target representation.\"\"\"\n    # Determine scaling factors\n    s_p = 1e-3 if input_event.mom_unit == 'MEV' else 1.0\n    s_l = 10.0 if input_event.len_unit == 'CM' else 1.0\n    \n    # Create new event with target units\n    output_event = Event('GEV', 'MM')\n\n    # Convert vertices\n    for v_id, v_in in input_event.vertices.items():\n        new_pos = v_in.pos * s_l\n        output_event.add_vertex(Vertex(v_id, tuple(new_pos)))\n\n    # Convert particles\n    for p_id, p_in in input_event.particles.items():\n        new_mom = p_in.mom * s_p\n        new_status = status_map(p_in.status)\n        output_event.add_particle(Particle(p_id, new_status, tuple(new_mom)))\n\n    # Re-establish graph links\n    for p_id, p_in in input_event.particles.items():\n        p_out = output_event.particles[p_id]\n        if p_in.prod_vertex:\n            prod_v_out = output_event.vertices[p_in.prod_vertex.id]\n            p_out.prod_vertex = prod_v_out\n            prod_v_out.outgoing_particles.append(p_out)\n        if p_in.end_vertex:\n            end_v_out = output_event.vertices[p_in.end_vertex.id]\n            p_out.end_vertex = end_v_out\n            end_v_out.incoming_particles.append(p_out)\n            \n    return output_event\n\ndef run_checks(input_event: Event, output_event: Event):\n    \"\"\"Performs the three required checks and returns the boolean results.\"\"\"\n    # Determine scaling factors again for verification\n    s_p = 1e-3 if input_event.mom_unit == 'MEV' else 1.0\n    s_l = 10.0 if input_event.len_unit == 'CM' else 1.0\n    \n    # 1. Units-correct check\n    units_correct = True\n    # Check particle momenta\n    for p_id, p_in in input_event.particles.items():\n        p_out = output_event.particles[p_id]\n        expected_mom = p_in.mom * s_p\n        if not np.allclose(p_out.mom, expected_mom, rtol=1e-12, atol=1e-12):\n            units_correct = False\n            break\n    # Check vertex positions\n    if units_correct:\n        for v_id, v_in in input_event.vertices.items():\n            v_out = output_event.vertices[v_id]\n            expected_pos = v_in.pos * s_l\n            if not np.allclose(v_out.pos, expected_pos, rtol=1e-12, atol=1e-12):\n                units_correct = False\n                break\n\n    # 2. Status-correct check\n    status_correct = True\n    for p_id, p_in in input_event.particles.items():\n        p_out = output_event.particles[p_id]\n        if p_out.status != status_map(p_in.status):\n            status_correct = False\n            break\n\n    # 3. Graph-preserved check\n    graph_preserved = True\n    for p_id, p_in in input_event.particles.items():\n        p_out = output_event.particles[p_id]\n        \n        in_prod_id = p_in.prod_vertex.id if p_in.prod_vertex else None\n        out_prod_id = p_out.prod_vertex.id if p_out.prod_vertex else None\n        \n        in_end_id = p_in.end_vertex.id if p_in.end_vertex else None\n        out_end_id = p_out.end_vertex.id if p_out.end_vertex else None\n        \n        if in_prod_id != out_prod_id or in_end_id != out_end_id:\n            graph_preserved = False\n            break\n\n    return units_correct, status_correct, graph_preserved\n\ndef solve():\n    \"\"\"Main function to define tests, run conversion and checks, and print results.\"\"\"\n    test_cases = [\n        # Event A\n        {\n            \"units\": (\"MEV\", \"MM\"),\n            \"vertices\": [\n                (1, (0.0, 0.0, 0.0, 0.0)),\n                (2, (0.0, 0.0, 1.0, 1.0)),\n            ],\n            \"particles\": [\n                (1, 2, (0.0, 0.0, 1000.0, 1100.0)),\n                (2, 1, (100.0, 0.0, 400.0, 412.3105626)),\n                (3, 1, (-100.0, 0.0, 600.0, 687.6894374)),\n            ],\n            \"links\": [\n                (1, 1, 2),\n                (2, 2, None),\n                (3, 2, None),\n            ]\n        },\n        # Event B\n        {\n            \"units\": (\"GEV\", \"CM\"),\n            \"vertices\": [\n                (10, (0.0, 0.0, 0.0, 0.0)),\n                (11, (0.1, -0.2, 0.3, 0.0)),\n            ],\n            \"particles\": [\n                (12, 2, (0.3, 0.4, 0.0, 0.5)),\n                (13, 1, (0.1, 0.2, 0.0, 0.2236067977)),\n                (14, 1, (0.2, 0.2, 0.0, 0.2763932023)),\n                (10, 3, (0.0, 0.0, 0.0, 0.0)),\n            ],\n            \"links\": [\n                (12, 10, 11),\n                (13, 11, None),\n                (14, 11, None),\n                (10, None, None),\n            ]\n        },\n        # Event C\n        {\n            \"units\": (\"MEV\", \"CM\"),\n            \"vertices\": [\n                (100, (0.0, 0.0, 0.0, 0.0)),\n                (101, (0.0, 0.0, 0.01, 0.01)),\n                (102, (0.0, 0.0, 0.02, 0.02)),\n            ],\n            \"particles\": [\n                (100, 2, (0.0, 0.0, 1500.0, 1600.0)),\n                (101, 2, (0.0, 0.0, 1000.0, 1100.0)),\n                (102, 1, (0.0, 0.0, 500.0, 500.0)),\n                (103, 1, (0.0, 0.0, 500.0, 600.0)),\n            ],\n            \"links\": [\n                (100, 100, 101),\n                (101, 101, 102),\n                (102, 102, None),\n                (103, 102, None),\n            ]\n        }\n    ]\n\n    results = []\n    for case_data in test_cases:\n        # Build input event from data\n        input_event = Event(*case_data[\"units\"])\n        for v_data in case_data[\"vertices\"]:\n            input_event.add_vertex(Vertex(*v_data))\n        for p_data in case_data[\"particles\"]:\n            input_event.add_particle(Particle(*p_data))\n        \n        for p_id, prod_v_id, end_v_id in case_data[\"links\"]:\n            particle = input_event.particles[p_id]\n            if prod_v_id is not None:\n                prod_v = input_event.vertices[prod_v_id]\n                particle.prod_vertex = prod_v\n                prod_v.outgoing_particles.append(particle)\n            if end_v_id is not None:\n                end_v = input_event.vertices[end_v_id]\n                particle.end_vertex = end_v\n                end_v.incoming_particles.append(particle)\n        \n        # Perform conversion\n        output_event = convert_event(input_event)\n        \n        # Run checks and store results\n        check_results = run_checks(input_event, output_event)\n        results.extend(check_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Modern analyses at colliders like the LHC contend with complex realities such as negative event weights from Next-to-Leading Order (NLO) calculations and high levels of pileup. This advanced exercise delves into a subtle but critical failure mode where missing data provenance—the inability to distinguish primary-process particles from pileup—can lead to pathological cancellations in histograms when combined with negative weights. You will implement a simulation to demonstrate this effect and use a quantitative metric to detect it , highlighting how the detailed structure of an event record is essential for obtaining correct physics results in a high-luminosity environment.",
            "id": "3513432",
            "problem": "You are given the task of designing and implementing a self-contained program that stress-tests negative-weight semantics in simplified high-energy physics event records with pileup overlay and detects histogram cancellation pathologies attributable to missing provenance fields in the event record. The program must generate synthetic event data with Next-to-Leading Order (NLO) style mixed positive and negative event weights, overlay pileup, and quantify a cancellation severity metric in a pileup-dominated region. The detection is grounded in the additivity of histogram weights and the triangle inequality for absolute values.\n\nFundamental base and modeling assumptions:\n- An event record is represented as a multiset of contributions, each with an observable value denoted by $x$ and an associated signed event weight denoted by $w$. Histogram bin contents are linear in the event weights; that is, bin integrals are sums of weights of contributions that fall into each bin.\n- Negative-weighted events appear at Next-to-Leading Order (NLO) due to subtraction methods that ensure infrared safety; thus, a fraction $f_{-}$ of primary events have $w = -1$, while the complementary fraction have $w = +1$. Pileup interactions are modeled as additional independent soft collisions overlaid on the same event.\n- Provenance fields in a real event record disambiguate the origin of each contribution (for example, primary hard-scatter versus pileup). If these provenance fields are missing, a naive aggregator may erroneously propagate the primary event weight $w_{\\mathrm{prim}}$ to all overlaid contributions, including pileup, which should have independent positive contributions. This can induce pathological cancellations in histogram regions dominated by pileup.\n\nDetection principle:\n- For any collection of contributions with signed weights $\\{w_i\\}$ in a selected region of observable space, define the signed sum $S = \\sum_i w_i$ and the absolute sum $A = \\sum_i |w_i|$. The triangle inequality implies $|S| \\le A$. If most contributions share the same sign, then $|S|/A$ is close to $1$; if there are many cancellations, then $|S|/A$ is significantly less than $1$.\n- Define the cancellation severity metric $C$ in a chosen region $\\mathcal{R}$ as $C = 1 - \\frac{|S|}{A}$ with the convention that $C = 0$ if $A = 0$. Large $C$ indicates strong cancellations. We target a pileup-dominated region $x \\le x_{\\mathrm{cut}}$ to isolate cancellation patterns inconsistent with correct provenance handling. A pathology is detected if $C > \\tau$, where $\\tau$ is a user-specified threshold.\n\nSimulation model your program must implement:\n- For each test case, generate $n$ primary hard-scatter events. For each event:\n  - Draw the primary event weight $w_{\\mathrm{prim}} \\in \\{-1, +1\\}$ with $\\mathbb{P}(w_{\\mathrm{prim}} = -1) = f_{-}$. The primary contribution has an observable value $x_{\\mathrm{prim}}$ drawn from a normal distribution with mean $\\mu_x = 100.0$ and standard deviation $\\sigma_x = 20.0$, truncated to $x_{\\mathrm{prim}} > 0$.\n  - Draw the number of pileup collisions $N_{\\mathrm{PU}}$ from a Poisson distribution with mean $\\mu$. For each pileup collision, generate one contribution with $x_{\\mathrm{PU}}$ drawn from an exponential distribution with mean $\\lambda^{-1} = 20.0$ and weight assignment as follows:\n    - If provenance is present, set $w_{\\mathrm{PU}} = +1$ for all pileup contributions.\n    - If provenance is missing, set $w_{\\mathrm{PU}} = w_{\\mathrm{prim}}$, which encodes erroneous weight propagation from the primary.\n- Build the global collection of contributions by aggregating the primary and pileup contributions across all $n$ events. No mother-daughter links are needed beyond the weight propagation rule above. The histogram binning is defined by $B$ uniform bins on $[0, x_{\\max}]$, but the cancellation metric $C$ is evaluated directly on contributions with $x \\le x_{\\mathrm{cut}}$.\n\nDetection output your program must compute:\n- For each test case, compute the cancellation severity $C = 1 - \\frac{|S|}{A}$ using all contributions with $x \\le x_{\\mathrm{cut}}$. If $C > \\tau$, return the boolean value True; otherwise return False.\n- The final program output must be a single line containing a Python-style list of the booleans for the provided test suite in the same order.\n\nImplementation constraints:\n- The program must be deterministic and must not read any input. Use a fixed pseudo-random number generator seed specified per test case.\n- All observable values $x$ should be interpreted in gigaelectronvolts, but no unit conversion is required in the output since only boolean results are returned.\n\nTest suite to implement and evaluate, given as ordered tuples $\\left(\\text{seed}, n, \\mu, f_{-}, \\text{missing}, x_{\\mathrm{cut}}, B, x_{\\max}, \\tau\\right)$:\n- Case A: $\\left(12345, 4000, 3.0, 0.3, \\text{False}, 60.0, 40, 200.0, 0.5\\right)$\n- Case B: $\\left(12345, 4000, 3.0, 0.3, \\text{True}, 60.0, 40, 200.0, 0.5\\right)$\n- Case C: $\\left(23456, 4000, 3.0, 0.0, \\text{True}, 60.0, 40, 200.0, 0.5\\right)$\n- Case D: $\\left(34567, 4000, 20.0, 0.2, \\text{False}, 60.0, 40, 200.0, 0.5\\right)$\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\left[\\text{result}_A,\\text{result}_B,\\text{result}_C,\\text{result}_D\\right]$, where each $\\text{result}$ is either True or False and appears in the same order as the cases above.",
            "solution": "The problem is subjected to validation against the established criteria.\n\n### Step 1: Extracted Givens\n\n**Fundamental Assumptions:**\n- An event record is a multiset of contributions, each with an observable value `$x$` and a signed event weight `$w$`.\n- Histogram bin contents are `$\\sum_i w_i$` for contributions in a bin.\n- A fraction `$f_{-}$` of primary events have weight `$w = -1$`, while the fraction `$1-f_{-}$` have `$w = +1$`.\n- Pileup interactions are independent soft collisions overlaid on the event.\n- Missing provenance fields cause the primary event weight `$w_{\\mathrm{prim}}$` to be erroneously propagated to all associated pileup contributions.\n\n**Detection Principle:**\n- Signed sum: `$S = \\sum_i w_i$`.\n- Absolute sum: `$A = \\sum_i |w_i|$`.\n- Cancellation severity metric: `$C = 1 - \\frac{|S|}{A}$`, with `$C = 0$` if `$A = 0$`.\n- The metric is evaluated in a region `$\\mathcal{R}$` defined by `$x \\le x_{\\mathrm{cut}}$`.\n- A pathology is detected if `$C > \\tau$`, where `$\\tau$` is a given threshold.\n\n**Simulation Model:**\n- Generate `$n$` primary events.\n- For each event:\n    - Primary weight `$w_{\\mathrm{prim}} \\in \\{-1, +1\\}$` is drawn with probability `$\\mathbb{P}(w_{\\mathrm{prim}} = -1) = f_{-}$`.\n    - Primary observable `$x_{\\mathrm{prim}}$` is drawn from a normal distribution `$\\mathcal{N}(\\mu_x, \\sigma_x)$` with `$\\mu_x = 100.0$` and `$\\sigma_x = 20.0$`, truncated to ensure `$x_{\\mathrm{prim}} > 0$`.\n    - Number of pileup collisions `$N_{\\mathrm{PU}}$` is drawn from a Poisson distribution `$\\text{Poisson}(\\mu)$`.\n    - For each of the `$N_{\\mathrm{PU}}$` collisions, an observable `$x_{\\mathrm{PU}}$` is drawn from an exponential distribution with mean `$\\lambda^{-1} = 20.0$`.\n    - The pileup weight `$w_{\\mathrm{PU}}$` is assigned based on the `missing` provenance flag:\n        - If `missing` is `False` (provenance present): `$w_{\\mathrm{PU}} = +1$`.\n        - If `missing` is `True` (provenance missing): `$w_{\\mathrm{PU}} = w_{\\mathrm{prim}}$`.\n\n**Task:**\n- Compute `$C$` for all contributions with `$x \\le x_{\\mathrm{cut}}$` for each test case.\n- Return `True` if `$C > \\tau$`, `False` otherwise.\n- The program must be deterministic, using a specified seed for each case.\n\n**Test Suite:**\n- Case A: `$(12345, 4000, 3.0, 0.3, \\text{False}, 60.0, 40, 200.0, 0.5)$`\n- Case B: `$(12345, 4000, 3.0, 0.3, \\text{True}, 60.0, 40, 200.0, 0.5)$`\n- Case C: `$(23456, 4000, 3.0, 0.0, \\text{True}, 60.0, 40, 200.0, 0.5)$`\n- Case D: `$(34567, 4000, 20.0, 0.2, \\text{False}, 60.0, 40, 200.0, 0.5)$`\nThe tuple elements are `$(\\text{seed}, n, \\mu, f_{-}, \\text{missing}, x_{\\mathrm{cut}}, B, x_{\\max}, \\tau)$`.\n\n### Step 2: Validation Using Extracted Givens\n\nThe problem statement is analyzed for validity:\n- **Scientific Grounding**: The problem is built upon established concepts in computational high-energy physics, namely Next-to-Leading Order (NLO) calculations which can produce negative weights, the phenomenon of pileup in hadron colliders, and the importance of data provenance in event records. The statistical models (Poisson, Normal, Exponential distributions) are standard for such simulations. The detection metric is a direct application of the triangle inequality, a fundamental mathematical principle. The problem is scientifically and mathematically sound.\n- **Well-Posedness**: All parameters required for the simulation (`$n, \\mu, f_{-}, \\mu_x, \\sigma_x, \\lambda$`) and the detection logic (`$x_{\\mathrm{cut}}, \\tau$`) are explicitly provided for each test case. The use of a fixed pseudo-random number generator seed for each case ensures a deterministic and unique outcome. The output is a clearly defined boolean value. The problem is well-posed.\n- **Objectivity**: The problem statement is formulated using precise, objective language and mathematical definitions. It is free from subjective claims or ambiguity.\n- **Completeness and Consistency**: The problem is self-contained. While it provides histogram binning parameters (`$B, x_{\\max}$`), it clarifies that the cancellation metric `$C$` is computed directly on contributions, rendering those parameters contextually relevant but not required for the specific calculation, which is a consistent specification. There are no contradictions.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. It is a well-defined, scientifically grounded computational task. A solution will be developed.\n\n### Algorithmic Design and Rationale\n\nThe solution requires implementing the specified Monte Carlo simulation. For each test case defined in the suite, we will perform the following steps:\n\n1.  **Initialization**: A pseudo-random number generator (PRNG) is initialized with the seed provided for the specific test case. This ensures reproducibility. We also initialize two accumulators, `$S_{\\mathcal{R}}$` for the signed sum and `$A_{\\mathcal{R}}$` for the absolute sum of weights of contributions within the target region `$\\mathcal{R}$`, which is defined by `$x \\le x_{\\mathrm{cut}}$`.\n\n2.  **Event Generation Loop**: We iterate `$n$` times to generate each primary event and its associated pileup.\n    - **Primary Event**:\n        - The primary event weight, `$w_{\\mathrm{prim}}$`, is drawn from a Bernoulli distribution where the outcome `$-1$` has probability `$f_{-}$` and `$+1$` has probability `$1-f_{-}$`.\n        - The primary observable, `$x_{\\mathrm{prim}}$`, is drawn from `$\\mathcal{N}(\\mu_x=100.0, \\sigma_x=20.0)$`. We must enforce the constraint `$x_{\\mathrm{prim}} > 0$`. Given that the mean is `$5\\sigma$` away from `$0$`, the probability of a non-positive value is negligible, so a simple rejection sampling (redrawing if non-positive) is a correct and efficient method.\n        - We check if this primary contribution falls within the analysis region: if `$x_{\\mathrm{prim}} \\le x_{\\mathrm{cut}}$`, we update the sums: `$S_{\\mathcal{R}} \\leftarrow S_{\\mathcal{R}} + w_{\\mathrm{prim}}$` and `$A_{\\mathcal{R}} \\leftarrow A_{\\mathcal{R}} + |w_{\\mathrm{prim}}|$`.\n    - **Pileup Generation**:\n        - The number of pileup contributions, `$N_{\\mathrm{PU}}$`, is drawn from a Poisson distribution with mean `$\\mu$`.\n        - We loop `$N_{\\mathrm{PU}}$` times to generate each pileup contribution.\n            - The pileup observable, `$x_{\\mathrm{PU}}$`, is drawn from an exponential distribution with mean `$20.0$`. The scale parameter for the random number generator corresponds to the mean.\n            - The pileup weight, `$w_{\\mathrm{PU}}$`, is determined by the `missing` provenance flag. If `missing` is `False`, `$w_{\\mathrm{PU}} = +1$`. If `missing` is `True`, `$w_{\\mathrm{PU}} = w_{\\mathrm{prim}}$`.\n            - We check if this pileup contribution falls within the analysis region: if `$x_{\\mathrm{PU}} \\le x_{\\mathrm{cut}}$`, we update the sums: `$S_{\\mathcal{R}} \\leftarrow S_{\\mathcal{R}} + w_{\\mathrm{PU}}$` and `$A_{\\mathcal{R}} \\leftarrow A_{\\mathcal{R}} + |w_{\\mathrm{PU}}|$`.\n\n3.  **Cancellation Metric Calculation**: After processing all `$n$` events, we calculate the cancellation severity `$C$`. To prevent division by zero, we first check if `$A_{\\mathcal{R}} > 0$`. If it is, `$C = 1 - |S_{\\mathcal{R}}| / A_{\\mathcal{R}}$`. Otherwise, as per the problem definition, `$C = 0$`.\n\n4.  **Pathology Detection**: We compare the computed `$C$` with the threshold `$\\tau$`. The result for the test case is `True` if `$C > \\tau$` and `False` otherwise.\n\nThis entire process is repeated for each of the four test cases.\n\n**Analysis of Expected Results:**\n- **Case A**: `missing=False`. All pileup weights are `$w_{\\mathrm{PU}}=+1$`. The region `$x \\le 60.0$` is dominated by pileup events (exponential with mean `$20.0$`). Primary events (`$\\mathcal{N}(100, 20)$`) are rare in this region. The overwhelming majority of contributions will have a positive weight. Any negative contributions from the few primary events will be negligible compared to the sum of positive pileup weights. Thus, `$A_{\\mathcal{R}} \\approx S_{\\mathcal{R}}$`, making `$C \\approx 0$`. We expect the result to be `False`.\n- **Case B**: `missing=True`, `$f_{-}=0.3$`. Pileup weights are propagated from the primary, so `$w_{\\mathrm{PU}} = w_{\\mathrm{prim}}$`. Approximately `$30\\%$` of events will generate a cascade of negative-weighted pileup contributions. In the pileup-dominated region `$x \\le 60.0$`, we will have a large sum of positive weights (from `$70\\%$` of events) and a large sum of negative weights (from `$30\\%$` of events). This will lead to significant cancellation, so `$|S_{\\mathcal{R}}| \\ll A_{\\mathcal{R}}$` and `$C$` will be large. Since `$f_{-}=0.3$`, we might expect `$|S_{\\mathcal{R}}|/A_{\\mathcal{R}} \\approx |0.7 - 0.3| / (0.7 + 0.3) = 0.4$`, leading to `$C \\approx 0.6$`. This is greater than `$\\tau=0.5$`. We expect the result to be `True`.\n- **Case C**: `missing=True`, `$f_{-}=0.0$`. Although provenance is missing, all primary weights are `$w_{\\mathrm{prim}}=+1$`. Therefore, all propagated pileup weights will also be `$w_{\\mathrm{PU}}=+1$`. Every single contribution in the entire simulation will have a weight of `$+1$`. Cancellation is impossible. `$S_{\\mathcal{R}} = A_{\\mathcal{R}}$`, so `$C = 0$`. We expect the result to be `False`.\n- **Case D**: `missing=False`, `$\\mu=20.0$`. Similar to Case A, provenance is handled correctly, so all `$w_{\\mathrm{PU}}=+1$`. The pileup is much higher (`$\\mu=20.0$` vs `$\\mu=3.0$`), meaning the region `$x \\le 60.0$` will be even more dominated by positive-weight pileup contributions. Any cancellations from the rare primary events will be completely washed out. `$C$` will be extremely close to `$0$`. We expect the result to be `False`.\n\nThe logic holds, and the implementation will proceed as designed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of detecting histogram cancellation pathologies\n    in simulated high-energy physics events.\n    \"\"\"\n\n    # Test suite defined as ordered tuples:\n    # (seed, n, mu, f_minus, missing, x_cut, B, x_max, tau)\n    # B and x_max are provided but not used in the cancellation metric calculation.\n    test_cases = [\n        # Case A\n        (12345, 4000, 3.0, 0.3, False, 60.0, 40, 200.0, 0.5),\n        # Case B\n        (12345, 4000, 3.0, 0.3, True, 60.0, 40, 200.0, 0.5),\n        # Case C\n        (23456, 4000, 3.0, 0.0, True, 60.0, 40, 200.0, 0.5),\n        # Case D\n        (34567, 4000, 20.0, 0.2, False, 60.0, 40, 200.0, 0.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, n, mu, f_minus, missing, x_cut, _, _, tau = case\n\n        # Initialize a dedicated random number generator for determinism\n        rng = np.random.default_rng(seed)\n\n        # Parameters for the distributions\n        mu_x = 100.0\n        sigma_x = 20.0\n        lambda_inv = 20.0 # Mean of the exponential distribution\n\n        # Accumulators for the sums in the region x <= x_cut\n        s_region = 0.0  # Signed sum S\n        a_region = 0.0  # Absolute sum A\n\n        # Main simulation loop over n primary events\n        for _ in range(n):\n            # 1. Generate primary event\n            # Draw primary weight w_prim from {-1, +1}\n            w_prim = -1 if rng.random() < f_minus else 1\n\n            # Draw primary observable x_prim from a truncated normal distribution\n            # Rejection sampling is efficient as P(x <= 0) is tiny\n            x_prim = -1.0\n            while x_prim <= 0:\n                x_prim = rng.normal(loc=mu_x, scale=sigma_x)\n\n            # Check if primary contribution is in the analysis region\n            if x_prim <= x_cut:\n                s_region += w_prim\n                a_region += abs(w_prim)\n\n            # 2. Generate pileup events\n            # Draw number of pileup collisions\n            n_pu = rng.poisson(lam=mu)\n\n            if n_pu > 0:\n                # Draw all pileup observables at once for efficiency\n                x_pu_values = rng.exponential(scale=lambda_inv, size=n_pu)\n\n                # Determine pileup weight based on provenance flag\n                if missing:\n                    w_pu = w_prim\n                else:\n                    w_pu = 1\n\n                # Sum contributions from pileup events in the analysis region\n                # Vectorized operation for performance\n                in_region_mask = x_pu_values <= x_cut\n                num_in_region = np.sum(in_region_mask)\n\n                if num_in_region > 0:\n                    s_region += num_in_region * w_pu\n                    a_region += num_in_region * abs(w_pu)\n        \n        # 3. Calculate cancellation severity metric C\n        if a_region == 0:\n            cancellation_severity = 0.0\n        else:\n            cancellation_severity = 1.0 - abs(s_region) / a_region\n            \n        # 4. Detect pathology\n        pathology_detected = cancellation_severity > tau\n        results.append(pathology_detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}